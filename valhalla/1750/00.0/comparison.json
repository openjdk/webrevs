{"files":[{"patch":"@@ -4,0 +4,1 @@\n+\/\/ Copyright 2025 Arm Limited and\/or its affiliates.\n@@ -1197,1 +1198,0 @@\n-  static int emit_exception_handler(C2_MacroAssembler *masm);\n@@ -1200,5 +1200,1 @@\n-  static uint size_exception_handler() {\n-    return MacroAssembler::far_codestub_branch_size();\n-  }\n-\n-    \/\/ count one adr and one far branch instruction\n+    \/\/ count one branch instruction and one far call instruction sequence\n@@ -2255,19 +2251,0 @@\n-\/\/ Emit exception handler code.\n-int HandlerImpl::emit_exception_handler(C2_MacroAssembler* masm)\n-{\n-  \/\/ mov rscratch1 #exception_blob_entry_point\n-  \/\/ br rscratch1\n-  \/\/ Note that the code buffer's insts_mark is always relative to insts.\n-  \/\/ That's why we must use the macroassembler to generate a handler.\n-  address base = __ start_a_stub(size_exception_handler());\n-  if (base == nullptr) {\n-    ciEnv::current()->record_failure(\"CodeCache is full\");\n-    return 0;  \/\/ CodeBuffer::expand failed\n-  }\n-  int offset = __ offset();\n-  __ far_jump(RuntimeAddress(OptoRuntime::exception_blob()->entry_point()));\n-  assert(__ offset() - offset <= (int) size_exception_handler(), \"overflow\");\n-  __ end_a_stub();\n-  return offset;\n-}\n-\n@@ -2284,0 +2261,1 @@\n+\n@@ -2285,0 +2263,3 @@\n+  Label start;\n+  __ bind(start);\n+  __ far_call(RuntimeAddress(SharedRuntime::deopt_blob()->unpack()));\n@@ -2286,2 +2267,2 @@\n-  __ adr(lr, __ pc());\n-  __ far_jump(RuntimeAddress(SharedRuntime::deopt_blob()->unpack()));\n+  int entry_offset = __ offset();\n+  __ b(start);\n@@ -2291,1 +2272,1 @@\n-  return offset;\n+  return entry_offset;\n","filename":"src\/hotspot\/cpu\/aarch64\/aarch64.ad","additions":9,"deletions":28,"binary":false,"changes":37,"status":"modified"},{"patch":"@@ -455,2 +455,8 @@\n-  __ adr(lr, pc());\n-  __ far_jump(RuntimeAddress(SharedRuntime::deopt_blob()->unpack()));\n+  Label start;\n+  __ bind(start);\n+\n+  __ far_call(RuntimeAddress(SharedRuntime::deopt_blob()->unpack()));\n+\n+  int entry_offset = __ offset();\n+  __ b(start);\n+\n@@ -460,1 +466,1 @@\n-  return offset;\n+  return entry_offset;\n","filename":"src\/hotspot\/cpu\/aarch64\/c1_LIRAssembler_aarch64.cpp","additions":9,"deletions":3,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -74,1 +74,1 @@\n-    _deopt_handler_size = 7 * NativeInstruction::instruction_size\n+    _deopt_handler_size = 4 * NativeInstruction::instruction_size\n","filename":"src\/hotspot\/cpu\/aarch64\/c1_LIRAssembler_aarch64.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -2746,3 +2746,2 @@\n-  \/\/ - UseSVE = 0, vector_length_in_bytes = 8 or 16\n-  \/\/ - UseSVE = 1, vector_length_in_bytes = 8 or 16\n-  \/\/ - UseSVE = 2, vector_length_in_bytes >= 8\n+  \/\/ - UseSVE = 0\/1, vector_length_in_bytes = 8 or 16, excluding double and long types\n+  \/\/ - UseSVE = 2, vector_length_in_bytes >= 8, for all types\n","filename":"src\/hotspot\/cpu\/aarch64\/c2_MacroAssembler_aarch64.cpp","additions":2,"deletions":3,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -210,0 +210,35 @@\n+inline intptr_t* AnchorMark::anchor_mark_set_pd() {\n+  intptr_t* sp = _top_frame.sp();\n+  if (_top_frame.is_interpreted_frame()) {\n+    \/\/ In case the top frame is interpreted we need to set up the anchor using\n+    \/\/ the last_sp saved in the frame (remove possible alignment added while\n+    \/\/ thawing, see ThawBase::finish_thaw()). We also clear last_sp to match\n+    \/\/ the behavior when calling the VM from the interpreter (we check for this\n+    \/\/ in FreezeBase::prepare_freeze_interpreted_top_frame, which can be reached\n+    \/\/ if preempting again at redo_vmcall()).\n+    _last_sp_from_frame = _top_frame.interpreter_frame_last_sp();\n+    assert(_last_sp_from_frame != nullptr, \"\");\n+    _top_frame.interpreter_frame_set_last_sp(nullptr);\n+    if (sp != _last_sp_from_frame) {\n+      \/\/ We need to move up return pc and fp. They will be read next in\n+      \/\/ set_anchor() and set as _last_Java_pc and _last_Java_fp respectively.\n+      _last_sp_from_frame[-1] = (intptr_t)_top_frame.pc();\n+      _last_sp_from_frame[-2] = (intptr_t)_top_frame.fp();\n+    }\n+    _is_interpreted = true;\n+    sp = _last_sp_from_frame;\n+  }\n+  return sp;\n+}\n+\n+inline void AnchorMark::anchor_mark_clear_pd() {\n+  if (_is_interpreted) {\n+    \/\/ Restore last_sp_from_frame and possibly overwritten pc.\n+    _top_frame.interpreter_frame_set_last_sp(_last_sp_from_frame);\n+    intptr_t* sp = _top_frame.sp();\n+    if (sp != _last_sp_from_frame) {\n+      sp[-1] = (intptr_t)_top_frame.pc();\n+    }\n+  }\n+}\n+\n@@ -313,0 +348,1 @@\n+  \/\/ We only need to set the return pc. rfp will be restored back in gen_continuation_enter().\n@@ -314,1 +350,6 @@\n-  sp[-2] = (intptr_t)enterSpecial.fp();\n+  return sp;\n+}\n+\n+inline intptr_t* ThawBase::push_preempt_adapter() {\n+  frame enterSpecial = new_entry_frame();\n+  intptr_t* sp = enterSpecial.sp();\n@@ -316,1 +357,2 @@\n-  log_develop_trace(continuations, preempt)(\"push_cleanup_continuation initial sp: \" INTPTR_FORMAT \" final sp: \" INTPTR_FORMAT, p2i(sp + 2 * frame::metadata_words), p2i(sp));\n+  \/\/ We only need to set the return pc. rfp will be restored back in generate_cont_preempt_stub().\n+  sp[-1] = (intptr_t)StubRoutines::cont_preempt_stub();\n","filename":"src\/hotspot\/cpu\/aarch64\/continuationFreezeThaw_aarch64.inline.hpp","additions":44,"deletions":2,"binary":false,"changes":46,"status":"modified"},{"patch":"@@ -55,0 +55,3 @@\n+    DEBUG_ONLY(Method* m = f.is_interpreted_frame() ? f.interpreter_frame_method() : f.cb()->as_nmethod()->method();)\n+    assert(m->is_object_wait0() || thread->interp_at_preemptable_vmcall_cnt() > 0,\n+           \"preemptable VM call not using call_VM_preemptable\");\n","filename":"src\/hotspot\/cpu\/aarch64\/continuationHelper_aarch64.inline.hpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -476,1 +476,1 @@\n-    assert(sender_pc == nm->deopt_handler_begin(), \"unexpected sender pc\");\n+    assert(sender_pc == nm->deopt_handler_entry(), \"unexpected sender pc\");\n","filename":"src\/hotspot\/cpu\/aarch64\/frame_aarch64.inline.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -363,1 +363,1 @@\n-  ModRefBarrierSetAssembler::load_at(masm, decorators, type, dst, src, tmp1, tmp2);\n+  CardTableBarrierSetAssembler::load_at(masm, decorators, type, dst, src, tmp1, tmp2);\n","filename":"src\/hotspot\/cpu\/aarch64\/gc\/g1\/g1BarrierSetAssembler_aarch64.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -35,0 +35,25 @@\n+void CardTableBarrierSetAssembler::arraycopy_prologue(MacroAssembler* masm, DecoratorSet decorators, bool is_oop,\n+                                                      Register src, Register dst, Register count, RegSet saved_regs) {\n+\n+  if (is_oop) {\n+    gen_write_ref_array_pre_barrier(masm, decorators, dst, count, saved_regs);\n+  }\n+}\n+\n+void CardTableBarrierSetAssembler::arraycopy_epilogue(MacroAssembler* masm, DecoratorSet decorators, bool is_oop,\n+                                                      Register start, Register count, Register tmp,\n+                                                      RegSet saved_regs) {\n+  if (is_oop) {\n+    gen_write_ref_array_post_barrier(masm, decorators, start, count, tmp, saved_regs);\n+  }\n+}\n+\n+void CardTableBarrierSetAssembler::store_at(MacroAssembler* masm, DecoratorSet decorators, BasicType type,\n+                                            Address dst, Register val, Register tmp1, Register tmp2, Register tmp3) {\n+  if (is_reference_type(type)) {\n+    oop_store_at(masm, decorators, type, dst, val, tmp1, tmp2, tmp3);\n+  } else {\n+    BarrierSetAssembler::store_at(masm, decorators, type, dst, val, tmp1, tmp2, tmp3);\n+  }\n+}\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/gc\/shared\/cardTableBarrierSetAssembler_aarch64.cpp","additions":25,"deletions":0,"binary":false,"changes":25,"status":"modified"},{"patch":"@@ -573,0 +573,8 @@\n+#ifdef ASSERT\n+  Label not_preempted;\n+  ldr(rscratch1, Address(rthread, JavaThread::preempt_alternate_return_offset()));\n+  cbz(rscratch1, not_preempted);\n+  stop(\"remove_activation: should not have alternate return address set\");\n+  bind(not_preempted);\n+#endif \/* ASSERT *\/\n+\n@@ -1624,0 +1632,1 @@\n+                                             Label*   return_pc,\n@@ -1647,2 +1656,2 @@\n-                               entry_point, number_of_arguments,\n-                     check_exceptions);\n+                               return_pc, entry_point,\n+                               number_of_arguments, check_exceptions);\n@@ -1654,4 +1663,5 @@\n-void InterpreterMacroAssembler::call_VM_preemptable(Register oop_result,\n-                                                    address entry_point,\n-                                                    Register arg_1) {\n-  assert(arg_1 == c_rarg1, \"\");\n+void InterpreterMacroAssembler::call_VM_preemptable_helper(Register oop_result,\n+                                                           address entry_point,\n+                                                           int number_of_arguments,\n+                                                           bool check_exceptions) {\n+  assert(InterpreterRuntime::is_preemptable_call(entry_point), \"VM call not preemptable, should use call_VM()\");\n@@ -1662,1 +1672,1 @@\n-    Label L;\n+    Label L1, L2;\n@@ -1664,3 +1674,10 @@\n-    cbz(rscratch1, L);\n-    stop(\"Should not have alternate return address set\");\n-    bind(L);\n+    cbz(rscratch1, L1);\n+    stop(\"call_VM_preemptable_helper: Should not have alternate return address set\");\n+    bind(L1);\n+    \/\/ We check this counter in patch_return_pc_with_preempt_stub() during freeze.\n+    incrementw(Address(rthread, JavaThread::interp_at_preemptable_vmcall_cnt_offset()));\n+    ldrw(rscratch1, Address(rthread, JavaThread::interp_at_preemptable_vmcall_cnt_offset()));\n+    cmpw(rscratch1, 0);\n+    br(Assembler::GT, L2);\n+    stop(\"call_VM_preemptable_helper: should be > 0\");\n+    bind(L2);\n@@ -1674,3 +1691,2 @@\n-  adr(rscratch1, resume_pc);\n-  str(rscratch1, Address(rthread, JavaThread::last_Java_pc_offset()));\n-  call_VM_base(oop_result, noreg, noreg, entry_point, 1, false \/*check_exceptions*\/);\n+  \/\/ Note: call_VM_base will use resume_pc label to set last_Java_pc.\n+  call_VM_base(noreg, noreg, noreg, &resume_pc, entry_point, number_of_arguments, false \/*check_exceptions*\/);\n@@ -1680,0 +1696,12 @@\n+#ifdef ASSERT\n+  {\n+    Label L;\n+    decrementw(Address(rthread, JavaThread::interp_at_preemptable_vmcall_cnt_offset()));\n+    ldrw(rscratch1, Address(rthread, JavaThread::interp_at_preemptable_vmcall_cnt_offset()));\n+    cmpw(rscratch1, 0);\n+    br(Assembler::GE, L);\n+    stop(\"call_VM_preemptable_helper: should be >= 0\");\n+    bind(L);\n+  }\n+#endif \/* ASSERT *\/\n+\n@@ -1691,0 +1719,45 @@\n+  if (check_exceptions) {\n+    \/\/ check for pending exceptions\n+    ldr(rscratch1, Address(rthread, in_bytes(Thread::pending_exception_offset())));\n+    Label ok;\n+    cbz(rscratch1, ok);\n+    lea(rscratch1, RuntimeAddress(StubRoutines::forward_exception_entry()));\n+    br(rscratch1);\n+    bind(ok);\n+  }\n+\n+  \/\/ get oop result if there is one and reset the value in the thread\n+  if (oop_result->is_valid()) {\n+    get_vm_result_oop(oop_result, rthread);\n+  }\n+}\n+\n+static void pass_arg1(MacroAssembler* masm, Register arg) {\n+  if (c_rarg1 != arg ) {\n+    masm->mov(c_rarg1, arg);\n+  }\n+}\n+\n+static void pass_arg2(MacroAssembler* masm, Register arg) {\n+  if (c_rarg2 != arg ) {\n+    masm->mov(c_rarg2, arg);\n+  }\n+}\n+\n+void InterpreterMacroAssembler::call_VM_preemptable(Register oop_result,\n+                                         address entry_point,\n+                                         Register arg_1,\n+                                         bool check_exceptions) {\n+  pass_arg1(this, arg_1);\n+  call_VM_preemptable_helper(oop_result, entry_point, 1, check_exceptions);\n+}\n+\n+void InterpreterMacroAssembler::call_VM_preemptable(Register oop_result,\n+                                         address entry_point,\n+                                         Register arg_1,\n+                                         Register arg_2,\n+                                         bool check_exceptions) {\n+  LP64_ONLY(assert_different_registers(arg_1, c_rarg2));\n+  pass_arg2(this, arg_2);\n+  pass_arg1(this, arg_1);\n+  call_VM_preemptable_helper(oop_result, entry_point, 2, check_exceptions);\n","filename":"src\/hotspot\/cpu\/aarch64\/interp_masm_aarch64.cpp","additions":86,"deletions":13,"binary":false,"changes":99,"status":"modified"},{"patch":"@@ -48,0 +48,1 @@\n+                            Label*   return_pc,\n@@ -61,0 +62,1 @@\n+  \/\/ Use for vthread preemption\n@@ -63,1 +65,7 @@\n-                           Register arg_1);\n+                           Register arg_1,\n+                           bool check_exceptions = true);\n+  void call_VM_preemptable(Register oop_result,\n+                           address entry_point,\n+                           Register arg_1,\n+                           Register arg_2,\n+                           bool check_exceptions = true);\n@@ -65,0 +73,5 @@\n+ private:\n+  void call_VM_preemptable_helper(Register oop_result,\n+                                  address entry_point,\n+                                  int number_of_arguments,\n+                                  bool check_exceptions);\n@@ -66,0 +79,1 @@\n+ public:\n","filename":"src\/hotspot\/cpu\/aarch64\/interp_masm_aarch64.hpp","additions":15,"deletions":1,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -751,4 +751,0 @@\n-static bool is_preemptable(address entry_point) {\n-  return entry_point == CAST_FROM_FN_PTR(address, InterpreterRuntime::monitorenter);\n-}\n-\n@@ -758,0 +754,1 @@\n+                                  Label*   return_pc,\n@@ -790,6 +787,1 @@\n-  if (is_preemptable(entry_point)) {\n-    \/\/ skip setting last_pc since we already set it to desired value.\n-    set_last_Java_frame(last_java_sp, rfp, noreg, rscratch1);\n-  } else {\n-    set_last_Java_frame(last_java_sp, rfp, l, rscratch1);\n-  }\n+  set_last_Java_frame(last_java_sp, rfp, return_pc != nullptr ? *return_pc : l, rscratch1);\n@@ -830,1 +822,1 @@\n-  call_VM_base(oop_result, noreg, noreg, entry_point, number_of_arguments, check_exceptions);\n+  call_VM_base(oop_result, noreg, noreg, nullptr, entry_point, number_of_arguments, check_exceptions);\n@@ -1088,1 +1080,1 @@\n-  call_VM_base(oop_result, rthread, last_java_sp, entry_point, number_of_arguments, check_exceptions);\n+  call_VM_base(oop_result, rthread, last_java_sp, nullptr, entry_point, number_of_arguments, check_exceptions);\n@@ -3431,91 +3423,0 @@\n-\/\/ this simulates the behaviour of the x86 cmpxchg instruction using a\n-\/\/ load linked\/store conditional pair. we use the acquire\/release\n-\/\/ versions of these instructions so that we flush pending writes as\n-\/\/ per Java semantics.\n-\n-\/\/ n.b the x86 version assumes the old value to be compared against is\n-\/\/ in rax and updates rax with the value located in memory if the\n-\/\/ cmpxchg fails. we supply a register for the old value explicitly\n-\n-\/\/ the aarch64 load linked\/store conditional instructions do not\n-\/\/ accept an offset. so, unlike x86, we must provide a plain register\n-\/\/ to identify the memory word to be compared\/exchanged rather than a\n-\/\/ register+offset Address.\n-\n-void MacroAssembler::cmpxchgptr(Register oldv, Register newv, Register addr, Register tmp,\n-                                Label &succeed, Label *fail) {\n-  \/\/ oldv holds comparison value\n-  \/\/ newv holds value to write in exchange\n-  \/\/ addr identifies memory word to compare against\/update\n-  if (UseLSE) {\n-    mov(tmp, oldv);\n-    casal(Assembler::xword, oldv, newv, addr);\n-    cmp(tmp, oldv);\n-    br(Assembler::EQ, succeed);\n-    membar(AnyAny);\n-  } else {\n-    Label retry_load, nope;\n-    prfm(Address(addr), PSTL1STRM);\n-    bind(retry_load);\n-    \/\/ flush and load exclusive from the memory location\n-    \/\/ and fail if it is not what we expect\n-    ldaxr(tmp, addr);\n-    cmp(tmp, oldv);\n-    br(Assembler::NE, nope);\n-    \/\/ if we store+flush with no intervening write tmp will be zero\n-    stlxr(tmp, newv, addr);\n-    cbzw(tmp, succeed);\n-    \/\/ retry so we only ever return after a load fails to compare\n-    \/\/ ensures we don't return a stale value after a failed write.\n-    b(retry_load);\n-    \/\/ if the memory word differs we return it in oldv and signal a fail\n-    bind(nope);\n-    membar(AnyAny);\n-    mov(oldv, tmp);\n-  }\n-  if (fail)\n-    b(*fail);\n-}\n-\n-void MacroAssembler::cmpxchg_obj_header(Register oldv, Register newv, Register obj, Register tmp,\n-                                        Label &succeed, Label *fail) {\n-  assert(oopDesc::mark_offset_in_bytes() == 0, \"assumption\");\n-  cmpxchgptr(oldv, newv, obj, tmp, succeed, fail);\n-}\n-\n-void MacroAssembler::cmpxchgw(Register oldv, Register newv, Register addr, Register tmp,\n-                                Label &succeed, Label *fail) {\n-  \/\/ oldv holds comparison value\n-  \/\/ newv holds value to write in exchange\n-  \/\/ addr identifies memory word to compare against\/update\n-  \/\/ tmp returns 0\/1 for success\/failure\n-  if (UseLSE) {\n-    mov(tmp, oldv);\n-    casal(Assembler::word, oldv, newv, addr);\n-    cmp(tmp, oldv);\n-    br(Assembler::EQ, succeed);\n-    membar(AnyAny);\n-  } else {\n-    Label retry_load, nope;\n-    prfm(Address(addr), PSTL1STRM);\n-    bind(retry_load);\n-    \/\/ flush and load exclusive from the memory location\n-    \/\/ and fail if it is not what we expect\n-    ldaxrw(tmp, addr);\n-    cmp(tmp, oldv);\n-    br(Assembler::NE, nope);\n-    \/\/ if we store+flush with no intervening write tmp will be zero\n-    stlxrw(tmp, newv, addr);\n-    cbzw(tmp, succeed);\n-    \/\/ retry so we only ever return after a load fails to compare\n-    \/\/ ensures we don't return a stale value after a failed write.\n-    b(retry_load);\n-    \/\/ if the memory word differs we return it in oldv and signal a fail\n-    bind(nope);\n-    membar(AnyAny);\n-    mov(oldv, tmp);\n-  }\n-  if (fail)\n-    b(*fail);\n-}\n-\n","filename":"src\/hotspot\/cpu\/aarch64\/macroAssembler_aarch64.cpp","additions":4,"deletions":103,"binary":false,"changes":107,"status":"modified"},{"patch":"@@ -89,0 +89,1 @@\n+    Label*   return_pc,                \/\/ to set up last_Java_frame; use nullptr otherwise\n@@ -1256,10 +1257,0 @@\n-  \/\/ Various forms of CAS\n-\n-  void cmpxchg_obj_header(Register oldv, Register newv, Register obj, Register tmp,\n-                          Label &succeed, Label *fail);\n-  void cmpxchgptr(Register oldv, Register newv, Register addr, Register tmp,\n-                  Label &succeed, Label *fail);\n-\n-  void cmpxchgw(Register oldv, Register newv, Register addr, Register tmp,\n-                  Label &succeed, Label *fail);\n-\n","filename":"src\/hotspot\/cpu\/aarch64\/macroAssembler_aarch64.hpp","additions":1,"deletions":10,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -139,1 +139,2 @@\n-inline int StackChunkFrameStream<frame_kind>::interpreter_frame_num_oops() const {\n+template <typename RegisterMapT>\n+inline int StackChunkFrameStream<frame_kind>::interpreter_frame_num_oops(RegisterMapT* map) const {\n@@ -142,7 +143,3 @@\n-  InterpreterOopMap mask;\n-  f.interpreted_frame_oop_map(&mask);\n-  return  mask.num_oops()\n-        + 1 \/\/ for the mirror oop\n-        + (f.interpreter_frame_method()->is_native() ? 1 : 0) \/\/ temp oop slot\n-        + pointer_delta_as_int((intptr_t*)f.interpreter_frame_monitor_begin(),\n-              (intptr_t*)f.interpreter_frame_monitor_end())\/BasicObjectLock::size();\n+  InterpreterOopCount closure;\n+  f.oops_interpreted_do(&closure, map);\n+  return closure.count();\n","filename":"src\/hotspot\/cpu\/aarch64\/stackChunkFrameStream_aarch64.inline.hpp","additions":5,"deletions":8,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -2411,1 +2411,1 @@\n-  __ call_VM(noreg, entry, temp);\n+  __ call_VM_preemptable(noreg, entry, temp);\n@@ -2463,1 +2463,1 @@\n-  __ call_VM(noreg, entry, temp);\n+  __ call_VM_preemptable(noreg, entry, temp);\n@@ -3848,1 +3848,1 @@\n-  call_VM(r0, CAST_FROM_FN_PTR(address, InterpreterRuntime::_new), c_rarg1, c_rarg2);\n+  __ call_VM_preemptable(r0, CAST_FROM_FN_PTR(address, InterpreterRuntime::_new), c_rarg1, c_rarg2);\n","filename":"src\/hotspot\/cpu\/aarch64\/templateTable_aarch64.cpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -275,0 +275,6 @@\n+  Label start;\n+  __ bind(start);\n+\n+  __ jump(SharedRuntime::deopt_blob()->unpack(), relocInfo::runtime_call_type, noreg);\n+\n+  int entry_offset = __ offset();\n@@ -277,1 +283,1 @@\n-  __ jump(SharedRuntime::deopt_blob()->unpack(), relocInfo::runtime_call_type, noreg);\n+  __ b(start);\n@@ -282,1 +288,1 @@\n-  return offset;\n+  return entry_offset;\n","filename":"src\/hotspot\/cpu\/arm\/c1_LIRAssembler_arm.cpp","additions":8,"deletions":2,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -267,0 +267,3 @@\n+  Label start;\n+\n+  __ bind(start);\n@@ -268,0 +271,2 @@\n+  int entry_offset = __ offset();\n+  __ b(start);\n@@ -272,1 +277,1 @@\n-  return offset;\n+  return entry_offset;\n","filename":"src\/hotspot\/cpu\/ppc\/c1_LIRAssembler_ppc.cpp","additions":6,"deletions":1,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -40,0 +40,3 @@\n+    DEBUG_ONLY(Method* m = f.is_interpreted_frame() ? f.interpreter_frame_method() : f.cb()->as_nmethod()->method();)\n+    assert(m->is_object_wait0() || thread->interp_at_preemptable_vmcall_cnt() > 0,\n+           \"preemptable VM call not using call_VM_preemptable\");\n","filename":"src\/hotspot\/cpu\/ppc\/continuationHelper_ppc.inline.hpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -111,0 +111,2 @@\n+  assert(nonvolatile_accross_vthread_preemtion(R24_dispatch_addr),\n+         \"Requirement of field accesses (e.g. putstatic)\");\n@@ -865,0 +867,3 @@\n+  asm_assert_mem8_is_zero(in_bytes(JavaThread::preempt_alternate_return_offset()), R16_thread,\n+                          \"remove_activation: should not have alternate return address set\");\n+\n@@ -2017,1 +2022,2 @@\n-                                        Register arg_1, bool check_exceptions) {\n+                                                    Register arg_1,\n+                                                    bool check_exceptions) {\n@@ -2022,0 +2028,10 @@\n+  call_VM_preemptable(oop_result, entry_point, arg_1, noreg \/* arg_2 *\/, check_exceptions);\n+}\n+\n+void InterpreterMacroAssembler::call_VM_preemptable(Register oop_result, address entry_point,\n+                                                    Register arg_1, Register arg_2,\n+                                                    bool check_exceptions) {\n+  if (!Continuations::enabled()) {\n+    call_VM(oop_result, entry_point, arg_1, arg_2, check_exceptions);\n+    return;\n+  }\n@@ -2024,0 +2040,3 @@\n+  Register tmp = R11_scratch1;\n+  assert_different_registers(arg_1, tmp);\n+  assert_different_registers(arg_2, tmp);\n@@ -2025,3 +2044,10 @@\n-  DEBUG_ONLY(ld(R0, in_bytes(JavaThread::preempt_alternate_return_offset()), R16_thread));\n-  DEBUG_ONLY(cmpdi(CR0, R0, 0));\n-  asm_assert_eq(\"Should not have alternate return address set\");\n+#ifdef ASSERT\n+  asm_assert_mem8_is_zero(in_bytes(JavaThread::preempt_alternate_return_offset()), R16_thread,\n+                          \"Should not have alternate return address set\");\n+  \/\/ We check this counter in patch_return_pc_with_preempt_stub() during freeze.\n+  lwa(tmp, in_bytes(JavaThread::interp_at_preemptable_vmcall_cnt_offset()), R16_thread);\n+  addi(tmp, tmp, 1);\n+  cmpwi(CR0, tmp, 0);\n+  stw(tmp, in_bytes(JavaThread::interp_at_preemptable_vmcall_cnt_offset()), R16_thread);\n+  asm_assert(gt, \"call_VM_preemptable: should be > 0\");\n+#endif \/\/ ASSERT\n@@ -2030,1 +2056,1 @@\n-  assert(nonvolatile_accross_vthread_preemtion(R31) && nonvolatile_accross_vthread_preemtion(R22), \"\");\n+  assert(nonvolatile_accross_vthread_preemtion(R31) && nonvolatile_accross_vthread_preemtion(R24), \"\");\n@@ -2033,1 +2059,1 @@\n-  std(R22, _ijava_state_neg(fresult), R3_ARG1);\n+  std(R24, _ijava_state_neg(fresult), R3_ARG1);\n@@ -2038,0 +2064,2 @@\n+  assert(arg_2 != R4_ARG2, \"smashed argument\");\n+  mr_if_needed(R5_ARG3, arg_2, true \/* allow_noreg *\/);\n@@ -2039,1 +2067,1 @@\n-  call_VM(oop_result, entry_point, false \/*check_exceptions*\/, &resume_pc \/* last_java_pc *\/);\n+  call_VM(noreg \/* oop_result *\/, entry_point, false \/*check_exceptions*\/, &resume_pc \/* last_java_pc *\/);\n@@ -2042,0 +2070,8 @@\n+#ifdef ASSERT\n+  lwa(tmp, in_bytes(JavaThread::interp_at_preemptable_vmcall_cnt_offset()), R16_thread);\n+  addi(tmp, tmp, -1);\n+  cmpwi(CR0, tmp, 0);\n+  stw(tmp, in_bytes(JavaThread::interp_at_preemptable_vmcall_cnt_offset()), R16_thread);\n+  asm_assert(ge, \"call_VM_preemptable: should be >= 0\");\n+#endif \/\/ ASSERT\n+\n@@ -2046,0 +2082,1 @@\n+  \/\/ Preempted. Frames are already frozen on heap.\n@@ -2053,0 +2090,1 @@\n+\n@@ -2054,0 +2092,6 @@\n+  if (check_exceptions) {\n+    check_and_forward_exception(R11_scratch1, R12_scratch2);\n+  }\n+  if (oop_result->is_valid()) {\n+    get_vm_result_oop(oop_result);\n+  }\n@@ -2057,2 +2101,0 @@\n-  if (!Continuations::enabled()) return;\n-\n@@ -2063,5 +2105,0 @@\n-  \/\/ Restore registers that are preserved across vthread preemption\n-  assert(nonvolatile_accross_vthread_preemtion(R31) && nonvolatile_accross_vthread_preemtion(R22), \"\");\n-  ld(R3_ARG1, _abi0(callers_sp), R1_SP); \/\/ load FP\n-  ld(R31, _ijava_state_neg(lresult), R3_ARG1);\n-  ld(R22, _ijava_state_neg(fresult), R3_ARG1);\n","filename":"src\/hotspot\/cpu\/ppc\/interp_masm_ppc_64.cpp","additions":51,"deletions":14,"binary":false,"changes":65,"status":"modified"},{"patch":"@@ -705,0 +705,5 @@\n+  \/\/ Restore registers that are preserved across vthread preemption\n+  assert(__ nonvolatile_accross_vthread_preemtion(R31) && __ nonvolatile_accross_vthread_preemtion(R24), \"\");\n+  __ ld(R3_ARG1, _abi0(callers_sp), R1_SP); \/\/ load FP\n+  __ ld(R31, _ijava_state_neg(lresult), R3_ARG1);\n+  __ ld(R24, _ijava_state_neg(fresult), R3_ARG1);\n@@ -1252,1 +1257,1 @@\n-  const Register access_flags         = R22_tmp2;\n+  const Register access_flags         = R24_tmp4;\n","filename":"src\/hotspot\/cpu\/ppc\/templateInterpreterGenerator_ppc.cpp","additions":6,"deletions":1,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -55,0 +55,3 @@\n+    DEBUG_ONLY(Method* m = f.is_interpreted_frame() ? f.interpreter_frame_method() : f.cb()->as_nmethod()->method();)\n+    assert(m->is_object_wait0() || thread->interp_at_preemptable_vmcall_cnt() > 0,\n+           \"preemptable VM call not using call_VM_preemptable\");\n","filename":"src\/hotspot\/cpu\/riscv\/continuationHelper_riscv.inline.hpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -2210,1 +2210,1 @@\n-  __ call_VM(noreg, entry, temp);\n+  __ call_VM_preemptable(noreg, entry, temp);\n@@ -2263,1 +2263,1 @@\n-  __ call_VM(noreg, entry, temp);\n+  __ call_VM_preemptable(noreg, entry, temp);\n@@ -3616,1 +3616,1 @@\n-  call_VM(x10, CAST_FROM_FN_PTR(address, InterpreterRuntime::_new), c_rarg1, c_rarg2);\n+  __ call_VM_preemptable(x10, CAST_FROM_FN_PTR(address, InterpreterRuntime::_new), c_rarg1, c_rarg2);\n","filename":"src\/hotspot\/cpu\/riscv\/templateTable_riscv.cpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -275,1 +275,7 @@\n-  }  int offset = code_offset();\n+  }\n+\n+  int offset = code_offset();\n+\n+  Label start;\n+  __ bind(start);\n+\n@@ -279,0 +285,5 @@\n+\n+  int entry_offset = __ offset();\n+\n+  __ z_bru(start);\n+\n@@ -282,1 +293,1 @@\n-  return offset;\n+  return entry_offset;\n","filename":"src\/hotspot\/cpu\/s390\/c1_LIRAssembler_s390.cpp","additions":13,"deletions":2,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -578,5 +578,11 @@\n-  \/\/ Record top_frame_sp, because the callee might modify it, if it's compiled.\n-  assert_different_registers(Z_R1, method);\n-  z_sgrk(Z_R1, Z_SP, Z_fp);\n-  z_srag(Z_R1, Z_R1, Interpreter::logStackElementSize);\n-  z_stg(Z_R1, _z_ijava_state_neg(top_frame_sp), Z_fp);\n+#ifdef ASSERT\n+  NearLabel ok;\n+  Register tmp = Z_R1;\n+  z_lg(tmp, Address(Z_fp, _z_ijava_state_neg(top_frame_sp)));\n+  z_slag(tmp, tmp, Interpreter::logStackElementSize);\n+  z_agr(tmp, Z_fp);\n+  z_cgr(tmp, Z_SP);\n+  z_bre(ok);\n+  stop(\"corrupted top_frame_sp\");\n+  bind(ok);\n+#endif\n@@ -1921,0 +1927,5 @@\n+  \/\/ Rtemp3 is free at this point, use it to store top_frame_sp\n+  z_sgrk(Rtemp3, Z_SP, Z_fp);\n+  z_srag(Rtemp3, Rtemp3, Interpreter::logStackElementSize);\n+  reg2mem_opt(Rtemp3, Address(Z_fp, _z_ijava_state_neg(top_frame_sp)));\n+\n","filename":"src\/hotspot\/cpu\/s390\/interp_masm_s390.cpp","additions":16,"deletions":5,"binary":false,"changes":21,"status":"modified"},{"patch":"@@ -1103,0 +1103,5 @@\n+\n+  __ z_lcgr(top_frame_size);  \/\/ negate\n+  __ z_srag(top_frame_size, top_frame_size, Interpreter::logStackElementSize);\n+  \/\/ Store relativized top_frame_sp\n+  __ z_stg(top_frame_size, _z_ijava_state_neg(top_frame_sp), fp);\n@@ -2077,0 +2082,8 @@\n+  {\n+    Register top_frame_sp = Z_R1_scratch; \/\/ anyway going to load it with correct value\n+    __ z_lg(top_frame_sp, Address(Z_fp, _z_ijava_state_neg(top_frame_sp)));\n+    __ z_slag(top_frame_sp, top_frame_sp, Interpreter::logStackElementSize);\n+    __ z_agr(top_frame_sp, Z_fp);\n+\n+    __ resize_frame_absolute(top_frame_sp, \/* temp = *\/ Z_R0, \/* load_fp = *\/ true);\n+  }\n@@ -2184,0 +2197,8 @@\n+  {\n+    Register top_frame_sp = Z_R1_scratch;\n+    __ z_lg(top_frame_sp, Address(Z_fp, _z_ijava_state_neg(top_frame_sp)));\n+    __ z_slag(top_frame_sp, top_frame_sp, Interpreter::logStackElementSize);\n+    __ z_agr(top_frame_sp, Z_fp);\n+\n+    __ resize_frame_absolute(top_frame_sp, \/* temp = *\/ Z_R0, \/* load_fp = *\/ true);\n+  }\n","filename":"src\/hotspot\/cpu\/s390\/templateInterpreterGenerator_s390.cpp","additions":21,"deletions":0,"binary":false,"changes":21,"status":"modified"},{"patch":"@@ -456,3 +456,9 @@\n-  InternalAddress here(__ pc());\n-  __ pushptr(here.addr(), rscratch1);\n-  __ jump(RuntimeAddress(SharedRuntime::deopt_blob()->unpack()));\n+  Label start;\n+  __ bind(start);\n+\n+  __ call(RuntimeAddress(SharedRuntime::deopt_blob()->unpack()));\n+\n+  int entry_offset = __ offset();\n+\n+  __ jmp(start);\n+\n@@ -463,1 +469,1 @@\n-  return offset;\n+  return entry_offset;\n","filename":"src\/hotspot\/cpu\/x86\/c1_LIRAssembler_x86.cpp","additions":10,"deletions":4,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -51,1 +51,1 @@\n-    _deopt_handler_size = 17\n+    _deopt_handler_size = 10\n","filename":"src\/hotspot\/cpu\/x86\/c1_LIRAssembler_x86.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -201,0 +201,35 @@\n+inline intptr_t* AnchorMark::anchor_mark_set_pd() {\n+  intptr_t* sp = _top_frame.sp();\n+  if (_top_frame.is_interpreted_frame()) {\n+    \/\/ In case the top frame is interpreted we need to set up the anchor using\n+    \/\/ the last_sp saved in the frame (remove possible alignment added while\n+    \/\/ thawing, see ThawBase::finish_thaw()). We also clear last_sp to match\n+    \/\/ the behavior when calling the VM from the interpreter (we check for this\n+    \/\/ in FreezeBase::prepare_freeze_interpreted_top_frame, which can be reached\n+    \/\/ if preempting again at redo_vmcall()).\n+    _last_sp_from_frame = _top_frame.interpreter_frame_last_sp();\n+    assert(_last_sp_from_frame != nullptr, \"\");\n+    _top_frame.interpreter_frame_set_last_sp(nullptr);\n+    if (sp != _last_sp_from_frame) {\n+      \/\/ We need to move up return pc and fp. They will be read next in\n+      \/\/ set_anchor() and set as _last_Java_pc and _last_Java_fp respectively.\n+      _last_sp_from_frame[-1] = (intptr_t)_top_frame.pc();\n+      _last_sp_from_frame[-2] = (intptr_t)_top_frame.fp();\n+    }\n+    _is_interpreted = true;\n+    sp = _last_sp_from_frame;\n+  }\n+  return sp;\n+}\n+\n+inline void AnchorMark::anchor_mark_clear_pd() {\n+  if (_is_interpreted) {\n+    \/\/ Restore last_sp_from_frame and possibly overwritten pc.\n+    _top_frame.interpreter_frame_set_last_sp(_last_sp_from_frame);\n+    intptr_t* sp = _top_frame.sp();\n+    if (sp != _last_sp_from_frame) {\n+      sp[-1] = (intptr_t)_top_frame.pc();\n+    }\n+  }\n+}\n+\n@@ -301,0 +336,1 @@\n+  \/\/ We only need to set the return pc. rbp will be restored back in gen_continuation_enter().\n@@ -302,1 +338,6 @@\n-  sp[-2] = (intptr_t)enterSpecial.fp();\n+  return sp;\n+}\n+\n+inline intptr_t* ThawBase::push_preempt_adapter() {\n+  frame enterSpecial = new_entry_frame();\n+  intptr_t* sp = enterSpecial.sp();\n@@ -304,1 +345,2 @@\n-  log_develop_trace(continuations, preempt)(\"push_cleanup_continuation initial sp: \" INTPTR_FORMAT \" final sp: \" INTPTR_FORMAT, p2i(sp + 2 * frame::metadata_words), p2i(sp));\n+  \/\/ We only need to set the return pc. rbp will be restored back in generate_cont_preempt_stub().\n+  sp[-1] = (intptr_t)StubRoutines::cont_preempt_stub();\n","filename":"src\/hotspot\/cpu\/x86\/continuationFreezeThaw_x86.inline.hpp","additions":44,"deletions":2,"binary":false,"changes":46,"status":"modified"},{"patch":"@@ -53,0 +53,3 @@\n+    DEBUG_ONLY(Method* m = f.is_interpreted_frame() ? f.interpreter_frame_method() : f.cb()->as_nmethod()->method();)\n+    assert(m->is_object_wait0() || thread->interp_at_preemptable_vmcall_cnt() > 0,\n+           \"preemptable VM call not using call_VM_preemptable\");\n","filename":"src\/hotspot\/cpu\/x86\/continuationHelper_x86.inline.hpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -542,2 +542,0 @@\n-    \/\/ QQQ seems like this code is equivalent on the two platforms\n-#ifdef AMD64\n@@ -547,3 +545,0 @@\n-#else\n-      tos_addr += 2;\n-#endif \/\/ AMD64\n@@ -575,13 +570,1 @@\n-    case T_FLOAT   : {\n-#ifdef AMD64\n-        value_result->f = *(jfloat*)tos_addr;\n-#else\n-      if (method->is_native()) {\n-        jdouble d = *(jdouble*)tos_addr;  \/\/ Result was in ST0 so need to convert to jfloat\n-        value_result->f = (jfloat)d;\n-      } else {\n-        value_result->f = *(jfloat*)tos_addr;\n-      }\n-#endif \/\/ AMD64\n-      break;\n-    }\n+    case T_FLOAT   : value_result->f = *(jfloat*)tos_addr; break;\n@@ -617,1 +600,0 @@\n-#ifdef AMD64\n@@ -625,1 +607,0 @@\n-#endif \/\/ AMD64\n","filename":"src\/hotspot\/cpu\/x86\/frame_x86.cpp","additions":1,"deletions":20,"binary":false,"changes":21,"status":"modified"},{"patch":"@@ -83,2 +83,1 @@\n-#ifdef AMD64\n-#ifdef _WIN64\n+#ifdef _WINDOWS\n@@ -94,4 +93,1 @@\n-#endif \/\/ _WIN64\n-#else\n-    entry_frame_call_wrapper_offset                  =  2,\n-#endif \/\/ AMD64\n+#endif \/\/ _WINDOWS\n","filename":"src\/hotspot\/cpu\/x86\/frame_x86.hpp","additions":2,"deletions":6,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -456,1 +456,1 @@\n-    assert(sender_pc == nm->deopt_handler_begin(), \"unexpected sender pc\");\n+    assert(sender_pc == nm->deopt_handler_entry(), \"unexpected sender pc\");\n@@ -525,1 +525,0 @@\n-#ifdef AMD64\n@@ -533,1 +532,0 @@\n-#endif \/\/ AMD64\n","filename":"src\/hotspot\/cpu\/x86\/frame_x86.inline.hpp","additions":1,"deletions":3,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -147,1 +147,1 @@\n-  ModRefBarrierSetAssembler::load_at(masm, decorators, type, dst, src, tmp1);\n+  CardTableBarrierSetAssembler::load_at(masm, decorators, type, dst, src, tmp1);\n","filename":"src\/hotspot\/cpu\/x86\/gc\/g1\/g1BarrierSetAssembler_x86.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -44,0 +44,52 @@\n+void CardTableBarrierSetAssembler::arraycopy_prologue(MacroAssembler* masm, DecoratorSet decorators, BasicType type,\n+                                                      Register src, Register dst, Register count) {\n+  bool checkcast = (decorators & ARRAYCOPY_CHECKCAST) != 0;\n+  bool disjoint = (decorators & ARRAYCOPY_DISJOINT) != 0;\n+  bool obj_int = (type == T_OBJECT) && UseCompressedOops;\n+\n+  if (is_reference_type(type)) {\n+    if (!checkcast) {\n+      if (!obj_int) {\n+        \/\/ Save count for barrier\n+        __ movptr(r11, count);\n+      } else if (disjoint) {\n+        \/\/ Save dst in r11 in the disjoint case\n+        __ movq(r11, dst);\n+      }\n+    }\n+    gen_write_ref_array_pre_barrier(masm, decorators, dst, count);\n+  }\n+}\n+\n+void CardTableBarrierSetAssembler::arraycopy_epilogue(MacroAssembler* masm, DecoratorSet decorators, BasicType type,\n+                                                      Register src, Register dst, Register count) {\n+  bool checkcast = (decorators & ARRAYCOPY_CHECKCAST) != 0;\n+  bool disjoint = (decorators & ARRAYCOPY_DISJOINT) != 0;\n+  bool obj_int = (type == T_OBJECT) && UseCompressedOops;\n+  Register tmp = rax;\n+\n+  if (is_reference_type(type)) {\n+    if (!checkcast) {\n+      if (!obj_int) {\n+        \/\/ Save count for barrier\n+        count = r11;\n+      } else if (disjoint) {\n+        \/\/ Use the saved dst in the disjoint case\n+        dst = r11;\n+      }\n+    } else {\n+      tmp = rscratch1;\n+    }\n+    gen_write_ref_array_post_barrier(masm, decorators, dst, count, tmp);\n+  }\n+}\n+\n+void CardTableBarrierSetAssembler::store_at(MacroAssembler* masm, DecoratorSet decorators, BasicType type,\n+                                            Address dst, Register val, Register tmp1, Register tmp2, Register tmp3) {\n+  if (is_reference_type(type)) {\n+    oop_store_at(masm, decorators, type, dst, val, tmp1, tmp2, tmp3);\n+  } else {\n+    BarrierSetAssembler::store_at(masm, decorators, type, dst, val, tmp1, tmp2, tmp3);\n+  }\n+}\n+\n","filename":"src\/hotspot\/cpu\/x86\/gc\/shared\/cardTableBarrierSetAssembler_x86.cpp","additions":52,"deletions":0,"binary":false,"changes":52,"status":"modified"},{"patch":"@@ -29,1 +29,1 @@\n-#include \"gc\/shared\/modRefBarrierSetAssembler.hpp\"\n+#include \"gc\/shared\/barrierSetAssembler.hpp\"\n@@ -31,1 +31,1 @@\n-class CardTableBarrierSetAssembler: public ModRefBarrierSetAssembler {\n+class CardTableBarrierSetAssembler: public BarrierSetAssembler {\n@@ -33,0 +33,3 @@\n+  virtual void gen_write_ref_array_pre_barrier(MacroAssembler* masm, DecoratorSet decorators,\n+                                               Register addr, Register count) {}\n+\n@@ -39,0 +42,11 @@\n+\n+public:\n+  virtual void arraycopy_prologue(MacroAssembler* masm, DecoratorSet decorators, BasicType type,\n+                                  Register src, Register dst, Register count);\n+\n+  virtual void arraycopy_epilogue(MacroAssembler* masm, DecoratorSet decorators, BasicType type,\n+                                  Register src, Register dst, Register count);\n+\n+  virtual void store_at(MacroAssembler* masm, DecoratorSet decorators, BasicType type,\n+                        Address dst, Register val, Register tmp1, Register tmp2, Register tmp3);\n+\n","filename":"src\/hotspot\/cpu\/x86\/gc\/shared\/cardTableBarrierSetAssembler_x86.hpp","additions":16,"deletions":2,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -331,4 +331,5 @@\n-void InterpreterMacroAssembler::call_VM_preemptable(Register oop_result,\n-                                                    address entry_point,\n-                                                    Register arg_1) {\n-  assert(arg_1 == c_rarg1, \"\");\n+void InterpreterMacroAssembler::call_VM_preemptable_helper(Register oop_result,\n+                                                           address entry_point,\n+                                                           int number_of_arguments,\n+                                                           bool check_exceptions) {\n+  assert(InterpreterRuntime::is_preemptable_call(entry_point), \"VM call not preemptable, should use call_VM()\");\n@@ -339,1 +340,1 @@\n-    Label L;\n+    Label L1, L2;\n@@ -341,3 +342,9 @@\n-    jcc(Assembler::equal, L);\n-    stop(\"Should not have alternate return address set\");\n-    bind(L);\n+    jcc(Assembler::equal, L1);\n+    stop(\"call_VM_preemptable_helper: should not have alternate return address set\");\n+    bind(L1);\n+    \/\/ We check this counter in patch_return_pc_with_preempt_stub() during freeze.\n+    incrementl(Address(r15_thread, JavaThread::interp_at_preemptable_vmcall_cnt_offset()));\n+    cmpl(Address(r15_thread, JavaThread::interp_at_preemptable_vmcall_cnt_offset()), 0);\n+    jcc(Assembler::greater, L2);\n+    stop(\"call_VM_preemptable_helper: should be > 0\");\n+    bind(L2);\n@@ -351,1 +358,0 @@\n-  \/\/ Note: call_VM_helper requires last_Java_pc for anchor to be at the top of the stack.\n@@ -354,1 +360,2 @@\n-  MacroAssembler::call_VM_helper(oop_result, entry_point, 1, false \/*check_exceptions*\/);\n+  lea(rax, Address(rsp, wordSize));\n+  call_VM_base(noreg, rax, entry_point, number_of_arguments, false);\n@@ -359,0 +366,11 @@\n+#ifdef ASSERT\n+  {\n+    Label L;\n+    decrementl(Address(r15_thread, JavaThread::interp_at_preemptable_vmcall_cnt_offset()));\n+    cmpl(Address(r15_thread, JavaThread::interp_at_preemptable_vmcall_cnt_offset()), 0);\n+    jcc(Assembler::greaterEqual, L);\n+    stop(\"call_VM_preemptable_helper: should be >= 0\");\n+    bind(L);\n+  }\n+#endif \/* ASSERT *\/\n+\n@@ -371,0 +389,48 @@\n+  if (check_exceptions) {\n+    \/\/ check for pending exceptions\n+    cmpptr(Address(r15_thread, Thread::pending_exception_offset()), NULL_WORD);\n+    Label ok;\n+    jcc(Assembler::equal, ok);\n+    \/\/ Exception stub expects return pc to be at top of stack. We only need\n+    \/\/ it to check Interpreter::contains(return_address) so anything will do.\n+    lea(rscratch1, resume_pc);\n+    push(rscratch1);\n+    jump(RuntimeAddress(StubRoutines::forward_exception_entry()));\n+    bind(ok);\n+  }\n+\n+  \/\/ get oop result if there is one and reset the value in the thread\n+  if (oop_result->is_valid()) {\n+    get_vm_result_oop(oop_result);\n+  }\n+}\n+\n+static void pass_arg1(MacroAssembler* masm, Register arg) {\n+  if (c_rarg1 != arg ) {\n+    masm->mov(c_rarg1, arg);\n+  }\n+}\n+\n+static void pass_arg2(MacroAssembler* masm, Register arg) {\n+  if (c_rarg2 != arg ) {\n+    masm->mov(c_rarg2, arg);\n+  }\n+}\n+\n+void InterpreterMacroAssembler::call_VM_preemptable(Register oop_result,\n+                                         address entry_point,\n+                                         Register arg_1,\n+                                         bool check_exceptions) {\n+  pass_arg1(this, arg_1);\n+  call_VM_preemptable_helper(oop_result, entry_point, 1, check_exceptions);\n+}\n+\n+void InterpreterMacroAssembler::call_VM_preemptable(Register oop_result,\n+                                         address entry_point,\n+                                         Register arg_1,\n+                                         Register arg_2,\n+                                         bool check_exceptions) {\n+  LP64_ONLY(assert_different_registers(arg_1, c_rarg2));\n+  pass_arg2(this, arg_2);\n+  pass_arg1(this, arg_1);\n+  call_VM_preemptable_helper(oop_result, entry_point, 2, check_exceptions);\n@@ -524,2 +590,2 @@\n-  LP64_ONLY(assert(Rsub_klass != r14, \"r14 holds locals\");)\n-  LP64_ONLY(assert(Rsub_klass != r13, \"r13 holds bcp\");)\n+  assert(Rsub_klass != r14, \"r14 holds locals\");\n+  assert(Rsub_klass != r13, \"r13 holds bcp\");\n@@ -808,0 +874,8 @@\n+#ifdef ASSERT\n+  Label not_preempted;\n+  cmpptr(Address(r15_thread, JavaThread::preempt_alternate_return_offset()), NULL_WORD);\n+  jcc(Assembler::equal, not_preempted);\n+  stop(\"remove_activation: should not have alternate return address set\");\n+  bind(not_preempted);\n+#endif \/* ASSERT *\/\n+\n","filename":"src\/hotspot\/cpu\/x86\/interp_masm_x86.cpp","additions":86,"deletions":12,"binary":false,"changes":98,"status":"modified"},{"patch":"@@ -65,0 +65,1 @@\n+  \/\/ Use for vthread preemption\n@@ -67,1 +68,7 @@\n-                           Register arg_1);\n+                           Register arg_1,\n+                           bool check_exceptions = true);\n+  void call_VM_preemptable(Register oop_result,\n+                           address entry_point,\n+                           Register arg_1,\n+                           Register arg_2,\n+                           bool check_exceptions = true);\n@@ -69,0 +76,5 @@\n+ private:\n+  void call_VM_preemptable_helper(Register oop_result,\n+                                  address entry_point,\n+                                  int number_of_arguments,\n+                                  bool check_exceptions);\n@@ -70,0 +82,1 @@\n+ public:\n","filename":"src\/hotspot\/cpu\/x86\/interp_masm_x86.hpp","additions":14,"deletions":1,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -568,1 +568,0 @@\n-#ifdef AMD64\n@@ -577,3 +576,0 @@\n-#else\n-      ls.print(\"%3s=\" PTR_FORMAT, r->name(), saved_regs[((saved_regs_count - 1) - i)]);\n-#endif\n","filename":"src\/hotspot\/cpu\/x86\/methodHandles_x86.cpp","additions":0,"deletions":4,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -131,1 +131,2 @@\n-inline int StackChunkFrameStream<frame_kind>::interpreter_frame_num_oops() const {\n+template <typename RegisterMapT>\n+inline int StackChunkFrameStream<frame_kind>::interpreter_frame_num_oops(RegisterMapT* map) const {\n@@ -134,7 +135,3 @@\n-  InterpreterOopMap mask;\n-  f.interpreted_frame_oop_map(&mask);\n-  return  mask.num_oops()\n-        + 1 \/\/ for the mirror oop\n-        + (f.interpreter_frame_method()->is_native() ? 1 : 0) \/\/ temp oop slot\n-        + pointer_delta_as_int((intptr_t*)f.interpreter_frame_monitor_begin(),\n-              (intptr_t*)f.interpreter_frame_monitor_end())\/BasicObjectLock::size();\n+  InterpreterOopCount closure;\n+  f.oops_interpreted_do(&closure, map);\n+  return closure.count();\n","filename":"src\/hotspot\/cpu\/x86\/stackChunkFrameStream_x86.inline.hpp","additions":5,"deletions":8,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -2343,1 +2343,1 @@\n-  __ call_VM(noreg, entry, temp);\n+  __ call_VM_preemptable(noreg, entry, temp);\n@@ -2390,1 +2390,1 @@\n-  __ call_VM(noreg, entry, temp);\n+  __ call_VM_preemptable(noreg, entry, temp);\n@@ -3761,2 +3761,2 @@\n-  call_VM(rax, CAST_FROM_FN_PTR(address, InterpreterRuntime::_new), c_rarg1, c_rarg2);\n-   __ verify_oop(rax);\n+  __ call_VM_preemptable(rax, CAST_FROM_FN_PTR(address, InterpreterRuntime::_new), c_rarg1, c_rarg2);\n+  __ verify_oop(rax);\n","filename":"src\/hotspot\/cpu\/x86\/templateTable_x86.cpp","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -1263,1 +1263,0 @@\n-#ifdef _LP64\n@@ -1269,1 +1268,0 @@\n-#endif\n","filename":"src\/hotspot\/cpu\/x86\/vm_version_x86.cpp","additions":0,"deletions":2,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -25,1 +25,1 @@\n-\/\/ X86 Common Architecture Description File\n+\/\/ X86 AMD64 Architecture Description File\n@@ -62,0 +62,160 @@\n+\/\/ General Registers\n+\/\/ R8-R15 must be encoded with REX.  (RSP, RBP, RSI, RDI need REX when\n+\/\/ used as byte registers)\n+\n+\/\/ Previously set RBX, RSI, and RDI as save-on-entry for java code\n+\/\/ Turn off SOE in java-code due to frequent use of uncommon-traps.\n+\/\/ Now that allocator is better, turn on RSI and RDI as SOE registers.\n+\n+reg_def RAX  (SOC, SOC, Op_RegI,  0, rax->as_VMReg());\n+reg_def RAX_H(SOC, SOC, Op_RegI,  0, rax->as_VMReg()->next());\n+\n+reg_def RCX  (SOC, SOC, Op_RegI,  1, rcx->as_VMReg());\n+reg_def RCX_H(SOC, SOC, Op_RegI,  1, rcx->as_VMReg()->next());\n+\n+reg_def RDX  (SOC, SOC, Op_RegI,  2, rdx->as_VMReg());\n+reg_def RDX_H(SOC, SOC, Op_RegI,  2, rdx->as_VMReg()->next());\n+\n+reg_def RBX  (SOC, SOE, Op_RegI,  3, rbx->as_VMReg());\n+reg_def RBX_H(SOC, SOE, Op_RegI,  3, rbx->as_VMReg()->next());\n+\n+reg_def RSP  (NS,  NS,  Op_RegI,  4, rsp->as_VMReg());\n+reg_def RSP_H(NS,  NS,  Op_RegI,  4, rsp->as_VMReg()->next());\n+\n+\/\/ now that adapter frames are gone RBP is always saved and restored by the prolog\/epilog code\n+reg_def RBP  (NS, SOE, Op_RegI,  5, rbp->as_VMReg());\n+reg_def RBP_H(NS, SOE, Op_RegI,  5, rbp->as_VMReg()->next());\n+\n+#ifdef _WIN64\n+\n+reg_def RSI  (SOC, SOE, Op_RegI,  6, rsi->as_VMReg());\n+reg_def RSI_H(SOC, SOE, Op_RegI,  6, rsi->as_VMReg()->next());\n+\n+reg_def RDI  (SOC, SOE, Op_RegI,  7, rdi->as_VMReg());\n+reg_def RDI_H(SOC, SOE, Op_RegI,  7, rdi->as_VMReg()->next());\n+\n+#else\n+\n+reg_def RSI  (SOC, SOC, Op_RegI,  6, rsi->as_VMReg());\n+reg_def RSI_H(SOC, SOC, Op_RegI,  6, rsi->as_VMReg()->next());\n+\n+reg_def RDI  (SOC, SOC, Op_RegI,  7, rdi->as_VMReg());\n+reg_def RDI_H(SOC, SOC, Op_RegI,  7, rdi->as_VMReg()->next());\n+\n+#endif\n+\n+reg_def R8   (SOC, SOC, Op_RegI,  8, r8->as_VMReg());\n+reg_def R8_H (SOC, SOC, Op_RegI,  8, r8->as_VMReg()->next());\n+\n+reg_def R9   (SOC, SOC, Op_RegI,  9, r9->as_VMReg());\n+reg_def R9_H (SOC, SOC, Op_RegI,  9, r9->as_VMReg()->next());\n+\n+reg_def R10  (SOC, SOC, Op_RegI, 10, r10->as_VMReg());\n+reg_def R10_H(SOC, SOC, Op_RegI, 10, r10->as_VMReg()->next());\n+\n+reg_def R11  (SOC, SOC, Op_RegI, 11, r11->as_VMReg());\n+reg_def R11_H(SOC, SOC, Op_RegI, 11, r11->as_VMReg()->next());\n+\n+reg_def R12  (SOC, SOE, Op_RegI, 12, r12->as_VMReg());\n+reg_def R12_H(SOC, SOE, Op_RegI, 12, r12->as_VMReg()->next());\n+\n+reg_def R13  (SOC, SOE, Op_RegI, 13, r13->as_VMReg());\n+reg_def R13_H(SOC, SOE, Op_RegI, 13, r13->as_VMReg()->next());\n+\n+reg_def R14  (SOC, SOE, Op_RegI, 14, r14->as_VMReg());\n+reg_def R14_H(SOC, SOE, Op_RegI, 14, r14->as_VMReg()->next());\n+\n+reg_def R15  (SOC, SOE, Op_RegI, 15, r15->as_VMReg());\n+reg_def R15_H(SOC, SOE, Op_RegI, 15, r15->as_VMReg()->next());\n+\n+reg_def R16  (SOC, SOC, Op_RegI, 16, r16->as_VMReg());\n+reg_def R16_H(SOC, SOC, Op_RegI, 16, r16->as_VMReg()->next());\n+\n+reg_def R17  (SOC, SOC, Op_RegI, 17, r17->as_VMReg());\n+reg_def R17_H(SOC, SOC, Op_RegI, 17, r17->as_VMReg()->next());\n+\n+reg_def R18  (SOC, SOC, Op_RegI, 18, r18->as_VMReg());\n+reg_def R18_H(SOC, SOC, Op_RegI, 18, r18->as_VMReg()->next());\n+\n+reg_def R19  (SOC, SOC, Op_RegI, 19, r19->as_VMReg());\n+reg_def R19_H(SOC, SOC, Op_RegI, 19, r19->as_VMReg()->next());\n+\n+reg_def R20  (SOC, SOC, Op_RegI, 20, r20->as_VMReg());\n+reg_def R20_H(SOC, SOC, Op_RegI, 20, r20->as_VMReg()->next());\n+\n+reg_def R21  (SOC, SOC, Op_RegI, 21, r21->as_VMReg());\n+reg_def R21_H(SOC, SOC, Op_RegI, 21, r21->as_VMReg()->next());\n+\n+reg_def R22  (SOC, SOC, Op_RegI, 22, r22->as_VMReg());\n+reg_def R22_H(SOC, SOC, Op_RegI, 22, r22->as_VMReg()->next());\n+\n+reg_def R23  (SOC, SOC, Op_RegI, 23, r23->as_VMReg());\n+reg_def R23_H(SOC, SOC, Op_RegI, 23, r23->as_VMReg()->next());\n+\n+reg_def R24  (SOC, SOC, Op_RegI, 24, r24->as_VMReg());\n+reg_def R24_H(SOC, SOC, Op_RegI, 24, r24->as_VMReg()->next());\n+\n+reg_def R25  (SOC, SOC, Op_RegI, 25, r25->as_VMReg());\n+reg_def R25_H(SOC, SOC, Op_RegI, 25, r25->as_VMReg()->next());\n+\n+reg_def R26  (SOC, SOC, Op_RegI, 26, r26->as_VMReg());\n+reg_def R26_H(SOC, SOC, Op_RegI, 26, r26->as_VMReg()->next());\n+\n+reg_def R27  (SOC, SOC, Op_RegI, 27, r27->as_VMReg());\n+reg_def R27_H(SOC, SOC, Op_RegI, 27, r27->as_VMReg()->next());\n+\n+reg_def R28  (SOC, SOC, Op_RegI, 28, r28->as_VMReg());\n+reg_def R28_H(SOC, SOC, Op_RegI, 28, r28->as_VMReg()->next());\n+\n+reg_def R29  (SOC, SOC, Op_RegI, 29, r29->as_VMReg());\n+reg_def R29_H(SOC, SOC, Op_RegI, 29, r29->as_VMReg()->next());\n+\n+reg_def R30  (SOC, SOC, Op_RegI, 30, r30->as_VMReg());\n+reg_def R30_H(SOC, SOC, Op_RegI, 30, r30->as_VMReg()->next());\n+\n+reg_def R31  (SOC, SOC, Op_RegI, 31, r31->as_VMReg());\n+reg_def R31_H(SOC, SOC, Op_RegI, 31, r31->as_VMReg()->next());\n+\n+\/\/ Floating Point Registers\n+\n+\/\/ Specify priority of register selection within phases of register\n+\/\/ allocation.  Highest priority is first.  A useful heuristic is to\n+\/\/ give registers a low priority when they are required by machine\n+\/\/ instructions, like EAX and EDX on I486, and choose no-save registers\n+\/\/ before save-on-call, & save-on-call before save-on-entry.  Registers\n+\/\/ which participate in fixed calling sequences should come last.\n+\/\/ Registers which are used as pairs must fall on an even boundary.\n+\n+alloc_class chunk0(R10,         R10_H,\n+                   R11,         R11_H,\n+                   R8,          R8_H,\n+                   R9,          R9_H,\n+                   R12,         R12_H,\n+                   RCX,         RCX_H,\n+                   RBX,         RBX_H,\n+                   RDI,         RDI_H,\n+                   RDX,         RDX_H,\n+                   RSI,         RSI_H,\n+                   RAX,         RAX_H,\n+                   RBP,         RBP_H,\n+                   R13,         R13_H,\n+                   R14,         R14_H,\n+                   R15,         R15_H,\n+                   R16,         R16_H,\n+                   R17,         R17_H,\n+                   R18,         R18_H,\n+                   R19,         R19_H,\n+                   R20,         R20_H,\n+                   R21,         R21_H,\n+                   R22,         R22_H,\n+                   R23,         R23_H,\n+                   R24,         R24_H,\n+                   R25,         R25_H,\n+                   R26,         R26_H,\n+                   R27,         R27_H,\n+                   R28,         R28_H,\n+                   R29,         R29_H,\n+                   R30,         R30_H,\n+                   R31,         R31_H,\n+                   RSP,         RSP_H);\n+\n@@ -646,0 +806,192 @@\n+\/\/----------Architecture Description Register Classes--------------------------\n+\/\/ Several register classes are automatically defined based upon information in\n+\/\/ this architecture description.\n+\/\/ 1) reg_class inline_cache_reg           ( \/* as def'd in frame section *\/ )\n+\/\/ 2) reg_class stack_slots( \/* one chunk of stack-based \"registers\" *\/ )\n+\/\/\n+\n+\/\/ Empty register class.\n+reg_class no_reg();\n+\n+\/\/ Class for all pointer\/long registers including APX extended GPRs.\n+reg_class all_reg(RAX, RAX_H,\n+                  RDX, RDX_H,\n+                  RBP, RBP_H,\n+                  RDI, RDI_H,\n+                  RSI, RSI_H,\n+                  RCX, RCX_H,\n+                  RBX, RBX_H,\n+                  RSP, RSP_H,\n+                  R8,  R8_H,\n+                  R9,  R9_H,\n+                  R10, R10_H,\n+                  R11, R11_H,\n+                  R12, R12_H,\n+                  R13, R13_H,\n+                  R14, R14_H,\n+                  R15, R15_H,\n+                  R16, R16_H,\n+                  R17, R17_H,\n+                  R18, R18_H,\n+                  R19, R19_H,\n+                  R20, R20_H,\n+                  R21, R21_H,\n+                  R22, R22_H,\n+                  R23, R23_H,\n+                  R24, R24_H,\n+                  R25, R25_H,\n+                  R26, R26_H,\n+                  R27, R27_H,\n+                  R28, R28_H,\n+                  R29, R29_H,\n+                  R30, R30_H,\n+                  R31, R31_H);\n+\n+\/\/ Class for all int registers including APX extended GPRs.\n+reg_class all_int_reg(RAX\n+                      RDX,\n+                      RBP,\n+                      RDI,\n+                      RSI,\n+                      RCX,\n+                      RBX,\n+                      R8,\n+                      R9,\n+                      R10,\n+                      R11,\n+                      R12,\n+                      R13,\n+                      R14,\n+                      R16,\n+                      R17,\n+                      R18,\n+                      R19,\n+                      R20,\n+                      R21,\n+                      R22,\n+                      R23,\n+                      R24,\n+                      R25,\n+                      R26,\n+                      R27,\n+                      R28,\n+                      R29,\n+                      R30,\n+                      R31);\n+\n+\/\/ Class for all pointer registers\n+reg_class any_reg %{\n+  return _ANY_REG_mask;\n+%}\n+\n+\/\/ Class for all pointer registers (excluding RSP)\n+reg_class ptr_reg %{\n+  return _PTR_REG_mask;\n+%}\n+\n+\/\/ Class for all pointer registers (excluding RSP and RBP)\n+reg_class ptr_reg_no_rbp %{\n+  return _PTR_REG_NO_RBP_mask;\n+%}\n+\n+\/\/ Class for all pointer registers (excluding RAX and RSP)\n+reg_class ptr_no_rax_reg %{\n+  return _PTR_NO_RAX_REG_mask;\n+%}\n+\n+\/\/ Class for all pointer registers (excluding RAX, RBX, and RSP)\n+reg_class ptr_no_rax_rbx_reg %{\n+  return _PTR_NO_RAX_RBX_REG_mask;\n+%}\n+\n+\/\/ Class for all long registers (excluding RSP)\n+reg_class long_reg %{\n+  return _LONG_REG_mask;\n+%}\n+\n+\/\/ Class for all long registers (excluding RAX, RDX and RSP)\n+reg_class long_no_rax_rdx_reg %{\n+  return _LONG_NO_RAX_RDX_REG_mask;\n+%}\n+\n+\/\/ Class for all long registers (excluding RCX and RSP)\n+reg_class long_no_rcx_reg %{\n+  return _LONG_NO_RCX_REG_mask;\n+%}\n+\n+\/\/ Class for all long registers (excluding RBP and R13)\n+reg_class long_no_rbp_r13_reg %{\n+  return _LONG_NO_RBP_R13_REG_mask;\n+%}\n+\n+\/\/ Class for all int registers (excluding RSP)\n+reg_class int_reg %{\n+  return _INT_REG_mask;\n+%}\n+\n+\/\/ Class for all int registers (excluding RAX, RDX, and RSP)\n+reg_class int_no_rax_rdx_reg %{\n+  return _INT_NO_RAX_RDX_REG_mask;\n+%}\n+\n+\/\/ Class for all int registers (excluding RCX and RSP)\n+reg_class int_no_rcx_reg %{\n+  return _INT_NO_RCX_REG_mask;\n+%}\n+\n+\/\/ Class for all int registers (excluding RBP and R13)\n+reg_class int_no_rbp_r13_reg %{\n+  return _INT_NO_RBP_R13_REG_mask;\n+%}\n+\n+\/\/ Singleton class for RAX pointer register\n+reg_class ptr_rax_reg(RAX, RAX_H);\n+\n+\/\/ Singleton class for RBX pointer register\n+reg_class ptr_rbx_reg(RBX, RBX_H);\n+\n+\/\/ Singleton class for RSI pointer register\n+reg_class ptr_rsi_reg(RSI, RSI_H);\n+\n+\/\/ Singleton class for RBP pointer register\n+reg_class ptr_rbp_reg(RBP, RBP_H);\n+\n+\/\/ Singleton class for RDI pointer register\n+reg_class ptr_rdi_reg(RDI, RDI_H);\n+\n+\/\/ Singleton class for stack pointer\n+reg_class ptr_rsp_reg(RSP, RSP_H);\n+\n+\/\/ Singleton class for TLS pointer\n+reg_class ptr_r15_reg(R15, R15_H);\n+\n+\/\/ Singleton class for RAX long register\n+reg_class long_rax_reg(RAX, RAX_H);\n+\n+\/\/ Singleton class for RCX long register\n+reg_class long_rcx_reg(RCX, RCX_H);\n+\n+\/\/ Singleton class for RDX long register\n+reg_class long_rdx_reg(RDX, RDX_H);\n+\n+\/\/ Singleton class for R11 long register\n+reg_class long_r11_reg(R11, R11_H);\n+\n+\/\/ Singleton class for RAX int register\n+reg_class int_rax_reg(RAX);\n+\n+\/\/ Singleton class for RBX int register\n+reg_class int_rbx_reg(RBX);\n+\n+\/\/ Singleton class for RCX int register\n+reg_class int_rcx_reg(RCX);\n+\n+\/\/ Singleton class for RDX int register\n+reg_class int_rdx_reg(RDX);\n+\n+\/\/ Singleton class for RDI int register\n+reg_class int_rdi_reg(RDI);\n+\n+\/\/ Singleton class for instruction pointer\n+\/\/ reg_class ip_reg(RIP);\n+\n@@ -706,1 +1058,0 @@\n-\n@@ -1096,0 +1447,1 @@\n+\n@@ -1104,7 +1456,1 @@\n-\/\/ Header information of the source block.\n-\/\/ Method declarations\/definitions which are used outside\n-\/\/ the ad-scope can conveniently be defined here.\n-\/\/\n-\/\/ To keep related declarations\/definitions\/uses close together,\n-\/\/ we switch between source %{ }% and source_hpp %{ }% freely as needed.\n-#include \"runtime\/vm_version.hpp\"\n+#include \"peephole_x86_64.hpp\"\n@@ -1113,1 +1459,1 @@\n-class NativeJump;\n+bool castLL_is_imm32(const Node* n);\n@@ -1115,1 +1461,1 @@\n-class CallStubImpl {\n+%}\n@@ -1117,3 +1463,1 @@\n-  \/\/--------------------------------------------------------------\n-  \/\/---<  Used for optimization in Compile::shorten_branches  >---\n-  \/\/--------------------------------------------------------------\n+source %{\n@@ -1121,5 +1465,5 @@\n- public:\n-  \/\/ Size of call trampoline stub.\n-  static uint size_call_trampoline() {\n-    return 0; \/\/ no call trampolines on this platform\n-  }\n+bool castLL_is_imm32(const Node* n) {\n+  assert(n->is_CastLL(), \"must be a CastLL\");\n+  const TypeLong* t = n->bottom_type()->is_long();\n+  return (t->_lo == min_jlong || Assembler::is_simm32(t->_lo)) && (t->_hi == max_jlong || Assembler::is_simm32(t->_hi));\n+}\n@@ -1127,5 +1471,1 @@\n-  \/\/ number of relocations needed by a call trampoline stub\n-  static uint reloc_call_trampoline() {\n-    return 0; \/\/ no call trampolines on this platform\n-  }\n-};\n+%}\n@@ -1133,1 +1473,2 @@\n-class HandlerImpl {\n+\/\/ Register masks\n+source_hpp %{\n@@ -1135,1 +1476,14 @@\n- public:\n+extern RegMask _ANY_REG_mask;\n+extern RegMask _PTR_REG_mask;\n+extern RegMask _PTR_REG_NO_RBP_mask;\n+extern RegMask _PTR_NO_RAX_REG_mask;\n+extern RegMask _PTR_NO_RAX_RBX_REG_mask;\n+extern RegMask _LONG_REG_mask;\n+extern RegMask _LONG_NO_RAX_RDX_REG_mask;\n+extern RegMask _LONG_NO_RCX_REG_mask;\n+extern RegMask _LONG_NO_RBP_R13_REG_mask;\n+extern RegMask _INT_REG_mask;\n+extern RegMask _INT_NO_RAX_RDX_REG_mask;\n+extern RegMask _INT_NO_RCX_REG_mask;\n+extern RegMask _INT_NO_RBP_R13_REG_mask;\n+extern RegMask _FLOAT_REG_mask;\n@@ -1137,2 +1491,37 @@\n-  static int emit_exception_handler(C2_MacroAssembler *masm);\n-  static int emit_deopt_handler(C2_MacroAssembler* masm);\n+extern RegMask _STACK_OR_PTR_REG_mask;\n+extern RegMask _STACK_OR_LONG_REG_mask;\n+extern RegMask _STACK_OR_INT_REG_mask;\n+\n+inline const RegMask& STACK_OR_PTR_REG_mask()  { return _STACK_OR_PTR_REG_mask;  }\n+inline const RegMask& STACK_OR_LONG_REG_mask() { return _STACK_OR_LONG_REG_mask; }\n+inline const RegMask& STACK_OR_INT_REG_mask()  { return _STACK_OR_INT_REG_mask;  }\n+\n+%}\n+\n+source %{\n+#define   RELOC_IMM64    Assembler::imm_operand\n+#define   RELOC_DISP32   Assembler::disp32_operand\n+\n+#define __ masm->\n+\n+RegMask _ANY_REG_mask;\n+RegMask _PTR_REG_mask;\n+RegMask _PTR_REG_NO_RBP_mask;\n+RegMask _PTR_NO_RAX_REG_mask;\n+RegMask _PTR_NO_RAX_RBX_REG_mask;\n+RegMask _LONG_REG_mask;\n+RegMask _LONG_NO_RAX_RDX_REG_mask;\n+RegMask _LONG_NO_RCX_REG_mask;\n+RegMask _LONG_NO_RBP_R13_REG_mask;\n+RegMask _INT_REG_mask;\n+RegMask _INT_NO_RAX_RDX_REG_mask;\n+RegMask _INT_NO_RCX_REG_mask;\n+RegMask _INT_NO_RBP_R13_REG_mask;\n+RegMask _FLOAT_REG_mask;\n+RegMask _STACK_OR_PTR_REG_mask;\n+RegMask _STACK_OR_LONG_REG_mask;\n+RegMask _STACK_OR_INT_REG_mask;\n+\n+static bool need_r12_heapbase() {\n+  return UseCompressedOops;\n+}\n@@ -1140,7 +1529,14 @@\n-  static uint size_exception_handler() {\n-    \/\/ NativeCall instruction size is the same as NativeJump.\n-    \/\/ exception handler starts out as jump and can be patched to\n-    \/\/ a call be deoptimization.  (4932387)\n-    \/\/ Note that this value is also credited (in output.cpp) to\n-    \/\/ the size of the code section.\n-    return NativeJump::instruction_size;\n+void reg_mask_init() {\n+  constexpr Register egprs[] = {r16, r17, r18, r19, r20, r21, r22, r23, r24, r25, r26, r27, r28, r29, r30, r31};\n+\n+  \/\/ _ALL_REG_mask is generated by adlc from the all_reg register class below.\n+  \/\/ We derive a number of subsets from it.\n+  _ANY_REG_mask.assignFrom(_ALL_REG_mask);\n+\n+  if (PreserveFramePointer) {\n+    _ANY_REG_mask.remove(OptoReg::as_OptoReg(rbp->as_VMReg()));\n+    _ANY_REG_mask.remove(OptoReg::as_OptoReg(rbp->as_VMReg()->next()));\n+  }\n+  if (need_r12_heapbase()) {\n+    _ANY_REG_mask.remove(OptoReg::as_OptoReg(r12->as_VMReg()));\n+    _ANY_REG_mask.remove(OptoReg::as_OptoReg(r12->as_VMReg()->next()));\n@@ -1149,3 +1545,10 @@\n-  static uint size_deopt_handler() {\n-    \/\/ three 5 byte instructions plus one move for unreachable address.\n-    return 15+3;\n+  _PTR_REG_mask.assignFrom(_ANY_REG_mask);\n+  _PTR_REG_mask.remove(OptoReg::as_OptoReg(rsp->as_VMReg()));\n+  _PTR_REG_mask.remove(OptoReg::as_OptoReg(rsp->as_VMReg()->next()));\n+  _PTR_REG_mask.remove(OptoReg::as_OptoReg(r15->as_VMReg()));\n+  _PTR_REG_mask.remove(OptoReg::as_OptoReg(r15->as_VMReg()->next()));\n+  if (!UseAPX) {\n+    for (uint i = 0; i < sizeof(egprs)\/sizeof(Register); i++) {\n+      _PTR_REG_mask.remove(OptoReg::as_OptoReg(egprs[i]->as_VMReg()));\n+      _PTR_REG_mask.remove(OptoReg::as_OptoReg(egprs[i]->as_VMReg()->next()));\n+    }\n@@ -1153,8 +1556,2 @@\n-};\n-inline Assembler::AvxVectorLen vector_length_encoding(int bytes) {\n-  switch(bytes) {\n-    case  4: \/\/ fall-through\n-    case  8: \/\/ fall-through\n-    case 16: return Assembler::AVX_128bit;\n-    case 32: return Assembler::AVX_256bit;\n-    case 64: return Assembler::AVX_512bit;\n+  _STACK_OR_PTR_REG_mask.assignFrom(_PTR_REG_mask);\n+  _STACK_OR_PTR_REG_mask.or_with(STACK_OR_STACK_SLOTS_mask());\n@@ -1163,3 +1560,37 @@\n-    default: {\n-      ShouldNotReachHere();\n-      return Assembler::AVX_NoVec;\n+  _PTR_REG_NO_RBP_mask.assignFrom(_PTR_REG_mask);\n+  _PTR_REG_NO_RBP_mask.remove(OptoReg::as_OptoReg(rbp->as_VMReg()));\n+  _PTR_REG_NO_RBP_mask.remove(OptoReg::as_OptoReg(rbp->as_VMReg()->next()));\n+\n+  _PTR_NO_RAX_REG_mask.assignFrom(_PTR_REG_mask);\n+  _PTR_NO_RAX_REG_mask.remove(OptoReg::as_OptoReg(rax->as_VMReg()));\n+  _PTR_NO_RAX_REG_mask.remove(OptoReg::as_OptoReg(rax->as_VMReg()->next()));\n+\n+  _PTR_NO_RAX_RBX_REG_mask.assignFrom(_PTR_NO_RAX_REG_mask);\n+  _PTR_NO_RAX_RBX_REG_mask.remove(OptoReg::as_OptoReg(rbx->as_VMReg()));\n+  _PTR_NO_RAX_RBX_REG_mask.remove(OptoReg::as_OptoReg(rbx->as_VMReg()->next()));\n+\n+\n+  _LONG_REG_mask.assignFrom(_PTR_REG_mask);\n+  _STACK_OR_LONG_REG_mask.assignFrom(_LONG_REG_mask);\n+  _STACK_OR_LONG_REG_mask.or_with(STACK_OR_STACK_SLOTS_mask());\n+\n+  _LONG_NO_RAX_RDX_REG_mask.assignFrom(_LONG_REG_mask);\n+  _LONG_NO_RAX_RDX_REG_mask.remove(OptoReg::as_OptoReg(rax->as_VMReg()));\n+  _LONG_NO_RAX_RDX_REG_mask.remove(OptoReg::as_OptoReg(rax->as_VMReg()->next()));\n+  _LONG_NO_RAX_RDX_REG_mask.remove(OptoReg::as_OptoReg(rdx->as_VMReg()));\n+  _LONG_NO_RAX_RDX_REG_mask.remove(OptoReg::as_OptoReg(rdx->as_VMReg()->next()));\n+\n+  _LONG_NO_RCX_REG_mask.assignFrom(_LONG_REG_mask);\n+  _LONG_NO_RCX_REG_mask.remove(OptoReg::as_OptoReg(rcx->as_VMReg()));\n+  _LONG_NO_RCX_REG_mask.remove(OptoReg::as_OptoReg(rcx->as_VMReg()->next()));\n+\n+  _LONG_NO_RBP_R13_REG_mask.assignFrom(_LONG_REG_mask);\n+  _LONG_NO_RBP_R13_REG_mask.remove(OptoReg::as_OptoReg(rbp->as_VMReg()));\n+  _LONG_NO_RBP_R13_REG_mask.remove(OptoReg::as_OptoReg(rbp->as_VMReg()->next()));\n+  _LONG_NO_RBP_R13_REG_mask.remove(OptoReg::as_OptoReg(r13->as_VMReg()));\n+  _LONG_NO_RBP_R13_REG_mask.remove(OptoReg::as_OptoReg(r13->as_VMReg()->next()));\n+\n+  _INT_REG_mask.assignFrom(_ALL_INT_REG_mask);\n+  if (!UseAPX) {\n+    for (uint i = 0; i < sizeof(egprs)\/sizeof(Register); i++) {\n+      _INT_REG_mask.remove(OptoReg::as_OptoReg(egprs[i]->as_VMReg()));\n@@ -1168,0 +1599,25 @@\n+\n+  if (PreserveFramePointer) {\n+    _INT_REG_mask.remove(OptoReg::as_OptoReg(rbp->as_VMReg()));\n+  }\n+  if (need_r12_heapbase()) {\n+    _INT_REG_mask.remove(OptoReg::as_OptoReg(r12->as_VMReg()));\n+  }\n+\n+  _STACK_OR_INT_REG_mask.assignFrom(_INT_REG_mask);\n+  _STACK_OR_INT_REG_mask.or_with(STACK_OR_STACK_SLOTS_mask());\n+\n+  _INT_NO_RAX_RDX_REG_mask.assignFrom(_INT_REG_mask);\n+  _INT_NO_RAX_RDX_REG_mask.remove(OptoReg::as_OptoReg(rax->as_VMReg()));\n+  _INT_NO_RAX_RDX_REG_mask.remove(OptoReg::as_OptoReg(rdx->as_VMReg()));\n+\n+  _INT_NO_RCX_REG_mask.assignFrom(_INT_REG_mask);\n+  _INT_NO_RCX_REG_mask.remove(OptoReg::as_OptoReg(rcx->as_VMReg()));\n+\n+  _INT_NO_RBP_R13_REG_mask.assignFrom(_INT_REG_mask);\n+  _INT_NO_RBP_R13_REG_mask.remove(OptoReg::as_OptoReg(rbp->as_VMReg()));\n+  _INT_NO_RBP_R13_REG_mask.remove(OptoReg::as_OptoReg(r13->as_VMReg()));\n+\n+  \/\/ _FLOAT_REG_LEGACY_mask\/_FLOAT_REG_EVEX_mask is generated by adlc\n+  \/\/ from the float_reg_legacy\/float_reg_evex register class.\n+  _FLOAT_REG_mask.assignFrom(VM_Version::supports_evex() ? _FLOAT_REG_EVEX_mask : _FLOAT_REG_LEGACY_mask);\n@@ -1170,2 +1626,2 @@\n-static inline Assembler::AvxVectorLen vector_length_encoding(const Node* n) {\n-  return vector_length_encoding(Matcher::vector_length_in_bytes(n));\n+static bool generate_vzeroupper(Compile* C) {\n+  return (VM_Version::supports_vzeroupper() && (C->max_vector_size() > 16 || C->clear_upper_avx() == true)) ? true: false;  \/\/ Generate vzeroupper\n@@ -1174,4 +1630,2 @@\n-static inline Assembler::AvxVectorLen vector_length_encoding(const MachNode* use, MachOper* opnd) {\n-  uint def_idx = use->operand_index(opnd);\n-  Node* def = use->in(def_idx);\n-  return vector_length_encoding(def);\n+static int clear_avx_size() {\n+  return generate_vzeroupper(Compile::current()) ? 3: 0;  \/\/ vzeroupper\n@@ -1180,3 +1634,8 @@\n-static inline bool is_vector_popcount_predicate(BasicType bt) {\n-  return (is_subword_type(bt) && VM_Version::supports_avx512_bitalg()) ||\n-         (is_non_subword_integral_type(bt) && VM_Version::supports_avx512_vpopcntdq());\n+\/\/ !!!!! Special hack to get all types of calls to specify the byte offset\n+\/\/       from the start of the call to the point where the return address\n+\/\/       will point.\n+int MachCallStaticJavaNode::ret_addr_offset()\n+{\n+  int offset = 5; \/\/ 5 bytes from start of call to where return address points\n+  offset += clear_avx_size();\n+  return offset;\n@@ -1185,3 +1644,5 @@\n-static inline bool is_clz_non_subword_predicate_evex(BasicType bt, int vlen_bytes) {\n-  return is_non_subword_integral_type(bt) && VM_Version::supports_avx512cd() &&\n-           (VM_Version::supports_avx512vl() || vlen_bytes == 64);\n+int MachCallDynamicJavaNode::ret_addr_offset()\n+{\n+  int offset = 15; \/\/ 15 bytes from start of call to where return address points\n+  offset += clear_avx_size();\n+  return offset;\n@@ -1190,17 +1651,11 @@\n-class Node::PD {\n-public:\n-  enum NodeFlags {\n-    Flag_intel_jcc_erratum    = Node::_last_flag << 1,\n-    Flag_sets_carry_flag      = Node::_last_flag << 2,\n-    Flag_sets_parity_flag     = Node::_last_flag << 3,\n-    Flag_sets_zero_flag       = Node::_last_flag << 4,\n-    Flag_sets_overflow_flag   = Node::_last_flag << 5,\n-    Flag_sets_sign_flag       = Node::_last_flag << 6,\n-    Flag_clears_carry_flag    = Node::_last_flag << 7,\n-    Flag_clears_parity_flag   = Node::_last_flag << 8,\n-    Flag_clears_zero_flag     = Node::_last_flag << 9,\n-    Flag_clears_overflow_flag = Node::_last_flag << 10,\n-    Flag_clears_sign_flag     = Node::_last_flag << 11,\n-    _last_flag                = Flag_clears_sign_flag\n-  };\n-};\n+int MachCallRuntimeNode::ret_addr_offset() {\n+  if (_entry_point == nullptr) {\n+    \/\/ CallLeafNoFPInDirect\n+    return 3; \/\/ callq (register)\n+  }\n+  int offset = 13; \/\/ movq r10,#addr; callq (r10)\n+  if (this->ideal_Opcode() != Op_CallLeafVector) {\n+    offset += clear_avx_size();\n+  }\n+  return offset;\n+}\n@@ -1208,1 +1663,3 @@\n-%} \/\/ end source_hpp\n+\/\/\n+\/\/ Compute padding required for nodes which need alignment\n+\/\/\n@@ -1210,1 +1667,8 @@\n-source %{\n+\/\/ The address of the call instruction needs to be 4-byte aligned to\n+\/\/ ensure that it does not span a cache line so that it can be patched.\n+int CallStaticJavaDirectNode::compute_padding(int current_offset) const\n+{\n+  current_offset += clear_avx_size(); \/\/ skip vzeroupper\n+  current_offset += 1; \/\/ skip call opcode byte\n+  return align_up(current_offset, alignment_required()) - current_offset;\n+}\n@@ -1212,2 +1676,8 @@\n-#include \"opto\/addnode.hpp\"\n-#include \"c2_intelJccErratum_x86.hpp\"\n+\/\/ The address of the call instruction needs to be 4-byte aligned to\n+\/\/ ensure that it does not span a cache line so that it can be patched.\n+int CallDynamicJavaDirectNode::compute_padding(int current_offset) const\n+{\n+  current_offset += clear_avx_size(); \/\/ skip vzeroupper\n+  current_offset += 11; \/\/ skip movq instruction + call opcode byte\n+  return align_up(current_offset, alignment_required()) - current_offset;\n+}\n@@ -1215,5 +1685,19 @@\n-void PhaseOutput::pd_perform_mach_node_analysis() {\n-  if (VM_Version::has_intel_jcc_erratum()) {\n-    int extra_padding = IntelJccErratum::tag_affected_machnodes(C, C->cfg(), C->regalloc());\n-    _buf_sizes._code += extra_padding;\n-  }\n+\/\/ This could be in MacroAssembler but it's fairly C2 specific\n+static void emit_cmpfp_fixup(MacroAssembler* masm) {\n+  Label exit;\n+  __ jccb(Assembler::noParity, exit);\n+  __ pushf();\n+  \/\/\n+  \/\/ comiss\/ucomiss instructions set ZF,PF,CF flags and\n+  \/\/ zero OF,AF,SF for NaN values.\n+  \/\/ Fixup flags by zeroing ZF,PF so that compare of NaN\n+  \/\/ values returns 'less than' result (CF is set).\n+  \/\/ Leave the rest of flags unchanged.\n+  \/\/\n+  \/\/    7 6 5 4 3 2 1 0\n+  \/\/   |S|Z|r|A|r|P|r|C|  (r - reserved bit)\n+  \/\/    0 0 1 0 1 0 1 1   (0x2B)\n+  \/\/\n+  __ andq(Address(rsp, 0), 0xffffff2b);\n+  __ popf();\n+  __ bind(exit);\n@@ -1222,7 +1706,7 @@\n-int MachNode::pd_alignment_required() const {\n-  if (VM_Version::has_intel_jcc_erratum() && IntelJccErratum::is_jcc_erratum_branch(this)) {\n-    \/\/ Conservatively add worst case padding. We assume that relocInfo::addr_unit() is 1 on x86.\n-    return IntelJccErratum::largest_jcc_size() + 1;\n-  } else {\n-    return 1;\n-  }\n+static void emit_cmpfp3(MacroAssembler* masm, Register dst) {\n+  Label done;\n+  __ movl(dst, -1);\n+  __ jcc(Assembler::parity, done);\n+  __ jcc(Assembler::below, done);\n+  __ setcc(Assembler::notEqual, dst);\n+  __ bind(done);\n@@ -1231,9 +1715,37 @@\n-int MachNode::compute_padding(int current_offset) const {\n-  if (flags() & Node::PD::Flag_intel_jcc_erratum) {\n-    Compile* C = Compile::current();\n-    PhaseOutput* output = C->output();\n-    Block* block = output->block();\n-    int index = output->index();\n-    return IntelJccErratum::compute_padding(current_offset, this, block, index, C->regalloc());\n-  } else {\n-    return 0;\n+\/\/ Math.min()    # Math.max()\n+\/\/ --------------------------\n+\/\/ ucomis[s\/d]   #\n+\/\/ ja   -> b     # a\n+\/\/ jp   -> NaN   # NaN\n+\/\/ jb   -> a     # b\n+\/\/ je            #\n+\/\/ |-jz -> a | b # a & b\n+\/\/ |    -> a     #\n+static void emit_fp_min_max(MacroAssembler* masm, XMMRegister dst,\n+                            XMMRegister a, XMMRegister b,\n+                            XMMRegister xmmt, Register rt,\n+                            bool min, bool single) {\n+\n+  Label nan, zero, below, above, done;\n+\n+  if (single)\n+    __ ucomiss(a, b);\n+  else\n+    __ ucomisd(a, b);\n+\n+  if (dst->encoding() != (min ? b : a)->encoding())\n+    __ jccb(Assembler::above, above); \/\/ CF=0 & ZF=0\n+  else\n+    __ jccb(Assembler::above, done);\n+\n+  __ jccb(Assembler::parity, nan);  \/\/ PF=1\n+  __ jccb(Assembler::below, below); \/\/ CF=1\n+\n+  \/\/ equal\n+  __ vpxor(xmmt, xmmt, xmmt, Assembler::AVX_128bit);\n+  if (single) {\n+    __ ucomiss(a, xmmt);\n+    __ jccb(Assembler::equal, zero);\n+\n+    __ movflt(dst, a);\n+    __ jmp(done);\n@@ -1241,1 +1753,3 @@\n-}\n+  else {\n+    __ ucomisd(a, xmmt);\n+    __ jccb(Assembler::equal, zero);\n@@ -1243,3 +1757,3 @@\n-\/\/ Emit exception handler code.\n-\/\/ Stuff framesize into a register and call a VM stub routine.\n-int HandlerImpl::emit_exception_handler(C2_MacroAssembler* masm) {\n+    __ movdbl(dst, a);\n+    __ jmp(done);\n+  }\n@@ -1247,6 +1761,20 @@\n-  \/\/ Note that the code buffer's insts_mark is always relative to insts.\n-  \/\/ That's why we must use the macroassembler to generate a handler.\n-  address base = __ start_a_stub(size_exception_handler());\n-  if (base == nullptr) {\n-    ciEnv::current()->record_failure(\"CodeCache is full\");\n-    return 0;  \/\/ CodeBuffer::expand failed\n+  __ bind(zero);\n+  if (min)\n+    __ vpor(dst, a, b, Assembler::AVX_128bit);\n+  else\n+    __ vpand(dst, a, b, Assembler::AVX_128bit);\n+\n+  __ jmp(done);\n+\n+  __ bind(above);\n+  if (single)\n+    __ movflt(dst, min ? b : a);\n+  else\n+    __ movdbl(dst, min ? b : a);\n+\n+  __ jmp(done);\n+\n+  __ bind(nan);\n+  if (single) {\n+    __ movl(rt, 0x7fc00000); \/\/ Float.NaN\n+    __ movdl(dst, rt);\n@@ -1254,5 +1782,13 @@\n-  int offset = __ offset();\n-  __ jump(RuntimeAddress(OptoRuntime::exception_blob()->entry_point()));\n-  assert(__ offset() - offset <= (int) size_exception_handler(), \"overflow\");\n-  __ end_a_stub();\n-  return offset;\n+  else {\n+    __ mov64(rt, 0x7ff8000000000000L); \/\/ Double.NaN\n+    __ movdq(dst, rt);\n+  }\n+  __ jmp(done);\n+\n+  __ bind(below);\n+  if (single)\n+    __ movflt(dst, min ? a : b);\n+  else\n+    __ movdbl(dst, min ? a : b);\n+\n+  __ bind(done);\n@@ -1261,2 +1797,2 @@\n-\/\/ Emit deopt handler code.\n-int HandlerImpl::emit_deopt_handler(C2_MacroAssembler* masm) {\n+\/\/=============================================================================\n+const RegMask& MachConstantBaseNode::_out_RegMask = RegMask::EMPTY;\n@@ -1264,8 +1800,3 @@\n-  \/\/ Note that the code buffer's insts_mark is always relative to insts.\n-  \/\/ That's why we must use the macroassembler to generate a handler.\n-  address base = __ start_a_stub(size_deopt_handler());\n-  if (base == nullptr) {\n-    ciEnv::current()->record_failure(\"CodeCache is full\");\n-    return 0;  \/\/ CodeBuffer::expand failed\n-  }\n-  int offset = __ offset();\n+int ConstantTable::calculate_table_base_offset() const {\n+  return 0;  \/\/ absolute addressing, no offset\n+}\n@@ -1273,4 +1804,4 @@\n-  address the_pc = (address) __ pc();\n-  Label next;\n-  \/\/ push a \"the_pc\" on the stack without destroying any registers\n-  \/\/ as they all may be live.\n+bool MachConstantBaseNode::requires_postalloc_expand() const { return false; }\n+void MachConstantBaseNode::postalloc_expand(GrowableArray <Node *> *nodes, PhaseRegAlloc *ra_) {\n+  ShouldNotReachHere();\n+}\n@@ -1278,5 +1809,3 @@\n-  \/\/ push address of \"next\"\n-  __ call(next, relocInfo::none); \/\/ reloc none is fine since it is a disp32\n-  __ bind(next);\n-  \/\/ adjust it so it matches \"the_pc\"\n-  __ subptr(Address(rsp, 0), __ offset() - offset);\n+void MachConstantBaseNode::emit(C2_MacroAssembler* masm, PhaseRegAlloc* ra_) const {\n+  \/\/ Empty encoding\n+}\n@@ -1284,4 +1813,2 @@\n-  __ jump(RuntimeAddress(SharedRuntime::deopt_blob()->unpack()));\n-  assert(__ offset() - offset <= (int) size_deopt_handler(), \"overflow %d\", (__ offset() - offset));\n-  __ end_a_stub();\n-  return offset;\n+uint MachConstantBaseNode::size(PhaseRegAlloc* ra_) const {\n+  return 0;\n@@ -1290,11 +1817,3 @@\n-static Assembler::Width widthForType(BasicType bt) {\n-  if (bt == T_BYTE) {\n-    return Assembler::B;\n-  } else if (bt == T_SHORT) {\n-    return Assembler::W;\n-  } else if (bt == T_INT) {\n-    return Assembler::D;\n-  } else {\n-    assert(bt == T_LONG, \"not a long: %s\", type2name(bt));\n-    return Assembler::Q;\n-  }\n+#ifndef PRODUCT\n+void MachConstantBaseNode::format(PhaseRegAlloc* ra_, outputStream* st) const {\n+  st->print(\"# MachConstantBaseNode (empty encoding)\");\n@@ -1302,0 +1821,1 @@\n+#endif\n@@ -1303,21 +1823,89 @@\n-\/\/=============================================================================\n-  \/\/ Float masks come from different places depending on platform.\n-  static address float_signmask()  { return StubRoutines::x86::float_sign_mask(); }\n-  static address float_signflip()  { return StubRoutines::x86::float_sign_flip(); }\n-  static address double_signmask() { return StubRoutines::x86::double_sign_mask(); }\n-  static address double_signflip() { return StubRoutines::x86::double_sign_flip(); }\n-  static address vector_short_to_byte_mask() { return StubRoutines::x86::vector_short_to_byte_mask(); }\n-  static address vector_int_to_byte_mask() { return StubRoutines::x86::vector_int_to_byte_mask(); }\n-  static address vector_byte_perm_mask() { return StubRoutines::x86::vector_byte_perm_mask(); }\n-  static address vector_long_sign_mask() { return StubRoutines::x86::vector_long_sign_mask(); }\n-  static address vector_all_bits_set() { return StubRoutines::x86::vector_all_bits_set(); }\n-  static address vector_int_mask_cmp_bits() { return StubRoutines::x86::vector_int_mask_cmp_bits(); }\n-  static address vector_int_to_short_mask() { return StubRoutines::x86::vector_int_to_short_mask(); }\n-  static address vector_byte_shufflemask() { return StubRoutines::x86::vector_byte_shuffle_mask(); }\n-  static address vector_short_shufflemask() { return StubRoutines::x86::vector_short_shuffle_mask(); }\n-  static address vector_int_shufflemask() { return StubRoutines::x86::vector_int_shuffle_mask(); }\n-  static address vector_long_shufflemask() { return StubRoutines::x86::vector_long_shuffle_mask(); }\n-  static address vector_32_bit_mask() { return StubRoutines::x86::vector_32_bit_mask(); }\n-  static address vector_64_bit_mask() { return StubRoutines::x86::vector_64_bit_mask(); }\n-  static address vector_float_signflip() { return StubRoutines::x86::vector_float_sign_flip();}\n-  static address vector_double_signflip() { return StubRoutines::x86::vector_double_sign_flip();}\n+\/\/=============================================================================\n+#ifndef PRODUCT\n+void MachPrologNode::format(PhaseRegAlloc* ra_, outputStream* st) const {\n+  Compile* C = ra_->C;\n+\n+  int framesize = C->output()->frame_size_in_bytes();\n+  int bangsize = C->output()->bang_size_in_bytes();\n+  assert((framesize & (StackAlignmentInBytes-1)) == 0, \"frame size not aligned\");\n+  \/\/ Remove wordSize for return addr which is already pushed.\n+  framesize -= wordSize;\n+\n+  if (C->output()->need_stack_bang(bangsize)) {\n+    framesize -= wordSize;\n+    st->print(\"# stack bang (%d bytes)\", bangsize);\n+    st->print(\"\\n\\t\");\n+    st->print(\"pushq   rbp\\t# Save rbp\");\n+    if (PreserveFramePointer) {\n+        st->print(\"\\n\\t\");\n+        st->print(\"movq    rbp, rsp\\t# Save the caller's SP into rbp\");\n+    }\n+    if (framesize) {\n+      st->print(\"\\n\\t\");\n+      st->print(\"subq    rsp, #%d\\t# Create frame\",framesize);\n+    }\n+  } else {\n+    st->print(\"subq    rsp, #%d\\t# Create frame\",framesize);\n+    st->print(\"\\n\\t\");\n+    framesize -= wordSize;\n+    st->print(\"movq    [rsp + #%d], rbp\\t# Save rbp\",framesize);\n+    if (PreserveFramePointer) {\n+      st->print(\"\\n\\t\");\n+      st->print(\"movq    rbp, rsp\\t# Save the caller's SP into rbp\");\n+      if (framesize > 0) {\n+        st->print(\"\\n\\t\");\n+        st->print(\"addq    rbp, #%d\", framesize);\n+      }\n+    }\n+  }\n+\n+  if (VerifyStackAtCalls) {\n+    st->print(\"\\n\\t\");\n+    framesize -= wordSize;\n+    st->print(\"movq    [rsp + #%d], 0xbadb100d\\t# Majik cookie for stack depth check\",framesize);\n+#ifdef ASSERT\n+    st->print(\"\\n\\t\");\n+    st->print(\"# stack alignment check\");\n+#endif\n+  }\n+  if (C->stub_function() != nullptr) {\n+    st->print(\"\\n\\t\");\n+    st->print(\"cmpl    [r15_thread + #disarmed_guard_value_offset], #disarmed_guard_value\\t\");\n+    st->print(\"\\n\\t\");\n+    st->print(\"je      fast_entry\\t\");\n+    st->print(\"\\n\\t\");\n+    st->print(\"call    #nmethod_entry_barrier_stub\\t\");\n+    st->print(\"\\n\\tfast_entry:\");\n+  }\n+  st->cr();\n+}\n+#endif\n+\n+void MachPrologNode::emit(C2_MacroAssembler *masm, PhaseRegAlloc *ra_) const {\n+  Compile* C = ra_->C;\n+\n+  __ verified_entry(C);\n+\n+  if (ra_->C->stub_function() == nullptr) {\n+    __ entry_barrier();\n+  }\n+\n+  if (!Compile::current()->output()->in_scratch_emit_size()) {\n+    __ bind(*_verified_entry);\n+  }\n+\n+  C->output()->set_frame_complete(__ offset());\n+\n+  if (C->has_mach_constant_base_node()) {\n+    \/\/ NOTE: We set the table base offset here because users might be\n+    \/\/ emitted before MachConstantBaseNode.\n+    ConstantTable& constant_table = C->output()->constant_table();\n+    constant_table.set_table_base_offset(constant_table.calculate_table_base_offset());\n+  }\n+}\n+\n+\n+int MachPrologNode::reloc() const\n+{\n+  return 0; \/\/ a large enough number\n+}\n@@ -1327,3 +1915,7 @@\n-bool Matcher::match_rule_supported(int opcode) {\n-  if (!has_match_rule(opcode)) {\n-    return false; \/\/ no match rule present\n+#ifndef PRODUCT\n+void MachEpilogNode::format(PhaseRegAlloc* ra_, outputStream* st) const\n+{\n+  Compile* C = ra_->C;\n+  if (generate_vzeroupper(C)) {\n+    st->print(\"vzeroupper\");\n+    st->cr(); st->print(\"\\t\");\n@@ -1331,6 +1923,105 @@\n-  switch (opcode) {\n-    case Op_AbsVL:\n-    case Op_StoreVectorScatter:\n-      if (UseAVX < 3) {\n-        return false;\n-      }\n+\n+  int framesize = C->output()->frame_size_in_bytes();\n+  assert((framesize & (StackAlignmentInBytes-1)) == 0, \"frame size not aligned\");\n+  \/\/ Remove word for return adr already pushed\n+  \/\/ and RBP\n+  framesize -= 2*wordSize;\n+\n+  if (framesize) {\n+    st->print_cr(\"addq    rsp, %d\\t# Destroy frame\", framesize);\n+    st->print(\"\\t\");\n+  }\n+\n+  st->print_cr(\"popq    rbp\");\n+  if (do_polling() && C->is_method_compilation()) {\n+    st->print(\"\\t\");\n+    st->print_cr(\"cmpq    rsp, poll_offset[r15_thread] \\n\\t\"\n+                 \"ja      #safepoint_stub\\t\"\n+                 \"# Safepoint: poll for GC\");\n+  }\n+}\n+#endif\n+\n+void MachEpilogNode::emit(C2_MacroAssembler* masm, PhaseRegAlloc* ra_) const\n+{\n+  Compile* C = ra_->C;\n+\n+  if (generate_vzeroupper(C)) {\n+    \/\/ Clear upper bits of YMM registers when current compiled code uses\n+    \/\/ wide vectors to avoid AVX <-> SSE transition penalty during call.\n+    __ vzeroupper();\n+  }\n+\n+  \/\/ Subtract two words to account for return address and rbp\n+  int initial_framesize = C->output()->frame_size_in_bytes() - 2*wordSize;\n+  __ remove_frame(initial_framesize, C->needs_stack_repair());\n+\n+  if (StackReservedPages > 0 && C->has_reserved_stack_access()) {\n+    __ reserved_stack_check();\n+  }\n+\n+  if (do_polling() && C->is_method_compilation()) {\n+    Label dummy_label;\n+    Label* code_stub = &dummy_label;\n+    if (!C->output()->in_scratch_emit_size()) {\n+      C2SafepointPollStub* stub = new (C->comp_arena()) C2SafepointPollStub(__ offset());\n+      C->output()->add_stub(stub);\n+      code_stub = &stub->entry();\n+    }\n+    __ relocate(relocInfo::poll_return_type);\n+    __ safepoint_poll(*code_stub, true \/* at_return *\/, true \/* in_nmethod *\/);\n+  }\n+}\n+\n+int MachEpilogNode::reloc() const\n+{\n+  return 2; \/\/ a large enough number\n+}\n+\n+const Pipeline* MachEpilogNode::pipeline() const\n+{\n+  return MachNode::pipeline_class();\n+}\n+\n+\/\/=============================================================================\n+\n+enum RC {\n+  rc_bad,\n+  rc_int,\n+  rc_kreg,\n+  rc_float,\n+  rc_stack\n+};\n+\n+static enum RC rc_class(OptoReg::Name reg)\n+{\n+  if( !OptoReg::is_valid(reg)  ) return rc_bad;\n+\n+  if (OptoReg::is_stack(reg)) return rc_stack;\n+\n+  VMReg r = OptoReg::as_VMReg(reg);\n+\n+  if (r->is_Register()) return rc_int;\n+\n+  if (r->is_KRegister()) return rc_kreg;\n+\n+  assert(r->is_XMMRegister(), \"must be\");\n+  return rc_float;\n+}\n+\n+\/\/ Next two methods are shared by 32- and 64-bit VM. They are defined in x86.ad.\n+static void vec_mov_helper(C2_MacroAssembler *masm, int src_lo, int dst_lo,\n+                          int src_hi, int dst_hi, uint ireg, outputStream* st);\n+\n+void vec_spill_helper(C2_MacroAssembler *masm, bool is_load,\n+                     int stack_offset, int reg, uint ireg, outputStream* st);\n+\n+static void vec_stack_to_stack_helper(C2_MacroAssembler *masm, int src_offset,\n+                                      int dst_offset, uint ireg, outputStream* st) {\n+  if (masm) {\n+    switch (ireg) {\n+    case Op_VecS:\n+      __ movq(Address(rsp, -8), rax);\n+      __ movl(rax, Address(rsp, src_offset));\n+      __ movl(Address(rsp, dst_offset), rax);\n+      __ movq(rax, Address(rsp, -8));\n@@ -1338,5 +2029,3 @@\n-    case Op_PopCountI:\n-    case Op_PopCountL:\n-      if (!UsePopCountInstruction) {\n-        return false;\n-      }\n+    case Op_VecD:\n+      __ pushq(Address(rsp, src_offset));\n+      __ popq (Address(rsp, dst_offset));\n@@ -1344,4 +2033,5 @@\n-    case Op_PopCountVI:\n-      if (UseAVX < 2) {\n-        return false;\n-      }\n+    case Op_VecX:\n+      __ pushq(Address(rsp, src_offset));\n+      __ popq (Address(rsp, dst_offset));\n+      __ pushq(Address(rsp, src_offset+8));\n+      __ popq (Address(rsp, dst_offset+8));\n@@ -1349,6 +2039,5 @@\n-    case Op_CompressV:\n-    case Op_ExpandV:\n-    case Op_PopCountVL:\n-      if (UseAVX < 2) {\n-        return false;\n-      }\n+    case Op_VecY:\n+      __ vmovdqu(Address(rsp, -32), xmm0);\n+      __ vmovdqu(xmm0, Address(rsp, src_offset));\n+      __ vmovdqu(Address(rsp, dst_offset), xmm0);\n+      __ vmovdqu(xmm0, Address(rsp, -32));\n@@ -1356,4 +2045,5 @@\n-    case Op_MulVI:\n-      if ((UseSSE < 4) && (UseAVX < 1)) { \/\/ only with SSE4_1 or AVX\n-        return false;\n-      }\n+    case Op_VecZ:\n+      __ evmovdquq(Address(rsp, -64), xmm0, 2);\n+      __ evmovdquq(xmm0, Address(rsp, src_offset), 2);\n+      __ evmovdquq(Address(rsp, dst_offset), xmm0, 2);\n+      __ evmovdquq(xmm0, Address(rsp, -64), 2);\n@@ -1361,4 +2051,12 @@\n-    case Op_MulVL:\n-      if (UseSSE < 4) { \/\/ only with SSE4_1 or AVX\n-        return false;\n-      }\n+    default:\n+      ShouldNotReachHere();\n+    }\n+#ifndef PRODUCT\n+  } else {\n+    switch (ireg) {\n+    case Op_VecS:\n+      st->print(\"movq    [rsp - #8], rax\\t# 32-bit mem-mem spill\\n\\t\"\n+                \"movl    rax, [rsp + #%d]\\n\\t\"\n+                \"movl    [rsp + #%d], rax\\n\\t\"\n+                \"movq    rax, [rsp - #8]\",\n+                src_offset, dst_offset);\n@@ -1366,4 +2064,4 @@\n-    case Op_MulReductionVL:\n-      if (VM_Version::supports_avx512dq() == false) {\n-        return false;\n-      }\n+    case Op_VecD:\n+      st->print(\"pushq   [rsp + #%d]\\t# 64-bit mem-mem spill\\n\\t\"\n+                \"popq    [rsp + #%d]\",\n+                src_offset, dst_offset);\n@@ -1371,10 +2069,6 @@\n-    case Op_AbsVB:\n-    case Op_AbsVS:\n-    case Op_AbsVI:\n-    case Op_AddReductionVI:\n-    case Op_AndReductionV:\n-    case Op_OrReductionV:\n-    case Op_XorReductionV:\n-      if (UseSSE < 3) { \/\/ requires at least SSSE3\n-        return false;\n-      }\n+     case Op_VecX:\n+      st->print(\"pushq   [rsp + #%d]\\t# 128-bit mem-mem spill\\n\\t\"\n+                \"popq    [rsp + #%d]\\n\\t\"\n+                \"pushq   [rsp + #%d]\\n\\t\"\n+                \"popq    [rsp + #%d]\",\n+                src_offset, dst_offset, src_offset+8, dst_offset+8);\n@@ -1382,16 +2076,6 @@\n-    case Op_MaxHF:\n-    case Op_MinHF:\n-      if (!VM_Version::supports_avx512vlbw()) {\n-        return false;\n-      }  \/\/ fallthrough\n-    case Op_AddHF:\n-    case Op_DivHF:\n-    case Op_FmaHF:\n-    case Op_MulHF:\n-    case Op_ReinterpretS2HF:\n-    case Op_ReinterpretHF2S:\n-    case Op_SubHF:\n-    case Op_SqrtHF:\n-      if (!VM_Version::supports_avx512_fp16()) {\n-        return false;\n-      }\n+    case Op_VecY:\n+      st->print(\"vmovdqu [rsp - #32], xmm0\\t# 256-bit mem-mem spill\\n\\t\"\n+                \"vmovdqu xmm0, [rsp + #%d]\\n\\t\"\n+                \"vmovdqu [rsp + #%d], xmm0\\n\\t\"\n+                \"vmovdqu xmm0, [rsp - #32]\",\n+                src_offset, dst_offset);\n@@ -1399,6 +2083,6 @@\n-    case Op_VectorLoadShuffle:\n-    case Op_VectorRearrange:\n-    case Op_MulReductionVI:\n-      if (UseSSE < 4) { \/\/ requires at least SSE4\n-        return false;\n-      }\n+    case Op_VecZ:\n+      st->print(\"vmovdqu [rsp - #64], xmm0\\t# 512-bit mem-mem spill\\n\\t\"\n+                \"vmovdqu xmm0, [rsp + #%d]\\n\\t\"\n+                \"vmovdqu [rsp + #%d], xmm0\\n\\t\"\n+                \"vmovdqu xmm0, [rsp - #64]\",\n+                src_offset, dst_offset);\n@@ -1406,4 +2090,93 @@\n-    case Op_IsInfiniteF:\n-    case Op_IsInfiniteD:\n-      if (!VM_Version::supports_avx512dq()) {\n-        return false;\n+    default:\n+      ShouldNotReachHere();\n+    }\n+#endif\n+  }\n+}\n+\n+uint MachSpillCopyNode::implementation(C2_MacroAssembler* masm,\n+                                       PhaseRegAlloc* ra_,\n+                                       bool do_size,\n+                                       outputStream* st) const {\n+  assert(masm != nullptr || st  != nullptr, \"sanity\");\n+  \/\/ Get registers to move\n+  OptoReg::Name src_second = ra_->get_reg_second(in(1));\n+  OptoReg::Name src_first = ra_->get_reg_first(in(1));\n+  OptoReg::Name dst_second = ra_->get_reg_second(this);\n+  OptoReg::Name dst_first = ra_->get_reg_first(this);\n+\n+  enum RC src_second_rc = rc_class(src_second);\n+  enum RC src_first_rc = rc_class(src_first);\n+  enum RC dst_second_rc = rc_class(dst_second);\n+  enum RC dst_first_rc = rc_class(dst_first);\n+\n+  assert(OptoReg::is_valid(src_first) && OptoReg::is_valid(dst_first),\n+         \"must move at least 1 register\" );\n+\n+  if (src_first == dst_first && src_second == dst_second) {\n+    \/\/ Self copy, no move\n+    return 0;\n+  }\n+  if (bottom_type()->isa_vect() != nullptr && bottom_type()->isa_vectmask() == nullptr) {\n+    uint ireg = ideal_reg();\n+    assert((src_first_rc != rc_int && dst_first_rc != rc_int), \"sanity\");\n+    assert((ireg == Op_VecS || ireg == Op_VecD || ireg == Op_VecX || ireg == Op_VecY || ireg == Op_VecZ ), \"sanity\");\n+    if( src_first_rc == rc_stack && dst_first_rc == rc_stack ) {\n+      \/\/ mem -> mem\n+      int src_offset = ra_->reg2offset(src_first);\n+      int dst_offset = ra_->reg2offset(dst_first);\n+      vec_stack_to_stack_helper(masm, src_offset, dst_offset, ireg, st);\n+    } else if (src_first_rc == rc_float && dst_first_rc == rc_float ) {\n+      vec_mov_helper(masm, src_first, dst_first, src_second, dst_second, ireg, st);\n+    } else if (src_first_rc == rc_float && dst_first_rc == rc_stack ) {\n+      int stack_offset = ra_->reg2offset(dst_first);\n+      vec_spill_helper(masm, false, stack_offset, src_first, ireg, st);\n+    } else if (src_first_rc == rc_stack && dst_first_rc == rc_float ) {\n+      int stack_offset = ra_->reg2offset(src_first);\n+      vec_spill_helper(masm, true,  stack_offset, dst_first, ireg, st);\n+    } else {\n+      ShouldNotReachHere();\n+    }\n+    return 0;\n+  }\n+  if (src_first_rc == rc_stack) {\n+    \/\/ mem ->\n+    if (dst_first_rc == rc_stack) {\n+      \/\/ mem -> mem\n+      assert(src_second != dst_first, \"overlap\");\n+      if ((src_first & 1) == 0 && src_first + 1 == src_second &&\n+          (dst_first & 1) == 0 && dst_first + 1 == dst_second) {\n+        \/\/ 64-bit\n+        int src_offset = ra_->reg2offset(src_first);\n+        int dst_offset = ra_->reg2offset(dst_first);\n+        if (masm) {\n+          __ pushq(Address(rsp, src_offset));\n+          __ popq (Address(rsp, dst_offset));\n+#ifndef PRODUCT\n+        } else {\n+          st->print(\"pushq   [rsp + #%d]\\t# 64-bit mem-mem spill\\n\\t\"\n+                    \"popq    [rsp + #%d]\",\n+                     src_offset, dst_offset);\n+#endif\n+        }\n+      } else {\n+        \/\/ 32-bit\n+        assert(!((src_first & 1) == 0 && src_first + 1 == src_second), \"no transform\");\n+        assert(!((dst_first & 1) == 0 && dst_first + 1 == dst_second), \"no transform\");\n+        \/\/ No pushl\/popl, so:\n+        int src_offset = ra_->reg2offset(src_first);\n+        int dst_offset = ra_->reg2offset(dst_first);\n+        if (masm) {\n+          __ movq(Address(rsp, -8), rax);\n+          __ movl(rax, Address(rsp, src_offset));\n+          __ movl(Address(rsp, dst_offset), rax);\n+          __ movq(rax, Address(rsp, -8));\n+#ifndef PRODUCT\n+        } else {\n+          st->print(\"movq    [rsp - #8], rax\\t# 32-bit mem-mem spill\\n\\t\"\n+                    \"movl    rax, [rsp + #%d]\\n\\t\"\n+                    \"movl    [rsp + #%d], rax\\n\\t\"\n+                    \"movq    rax, [rsp - #8]\",\n+                     src_offset, dst_offset);\n+#endif\n+        }\n@@ -1411,16 +2184,30 @@\n-      break;\n-    case Op_SqrtVD:\n-    case Op_SqrtVF:\n-    case Op_VectorMaskCmp:\n-    case Op_VectorCastB2X:\n-    case Op_VectorCastS2X:\n-    case Op_VectorCastI2X:\n-    case Op_VectorCastL2X:\n-    case Op_VectorCastF2X:\n-    case Op_VectorCastD2X:\n-    case Op_VectorUCastB2X:\n-    case Op_VectorUCastS2X:\n-    case Op_VectorUCastI2X:\n-    case Op_VectorMaskCast:\n-      if (UseAVX < 1) { \/\/ enabled for AVX only\n-        return false;\n+      return 0;\n+    } else if (dst_first_rc == rc_int) {\n+      \/\/ mem -> gpr\n+      if ((src_first & 1) == 0 && src_first + 1 == src_second &&\n+          (dst_first & 1) == 0 && dst_first + 1 == dst_second) {\n+        \/\/ 64-bit\n+        int offset = ra_->reg2offset(src_first);\n+        if (masm) {\n+          __ movq(as_Register(Matcher::_regEncode[dst_first]), Address(rsp, offset));\n+#ifndef PRODUCT\n+        } else {\n+          st->print(\"movq    %s, [rsp + #%d]\\t# spill\",\n+                     Matcher::regName[dst_first],\n+                     offset);\n+#endif\n+        }\n+      } else {\n+        \/\/ 32-bit\n+        assert(!((src_first & 1) == 0 && src_first + 1 == src_second), \"no transform\");\n+        assert(!((dst_first & 1) == 0 && dst_first + 1 == dst_second), \"no transform\");\n+        int offset = ra_->reg2offset(src_first);\n+        if (masm) {\n+          __ movl(as_Register(Matcher::_regEncode[dst_first]), Address(rsp, offset));\n+#ifndef PRODUCT\n+        } else {\n+          st->print(\"movl    %s, [rsp + #%d]\\t# spill\",\n+                     Matcher::regName[dst_first],\n+                     offset);\n+#endif\n+        }\n@@ -1428,4 +2215,31 @@\n-      break;\n-    case Op_PopulateIndex:\n-      if (UseAVX < 2) {\n-        return false;\n+      return 0;\n+    } else if (dst_first_rc == rc_float) {\n+      \/\/ mem-> xmm\n+      if ((src_first & 1) == 0 && src_first + 1 == src_second &&\n+          (dst_first & 1) == 0 && dst_first + 1 == dst_second) {\n+        \/\/ 64-bit\n+        int offset = ra_->reg2offset(src_first);\n+        if (masm) {\n+          __ movdbl( as_XMMRegister(Matcher::_regEncode[dst_first]), Address(rsp, offset));\n+#ifndef PRODUCT\n+        } else {\n+          st->print(\"%s  %s, [rsp + #%d]\\t# spill\",\n+                     UseXmmLoadAndClearUpper ? \"movsd \" : \"movlpd\",\n+                     Matcher::regName[dst_first],\n+                     offset);\n+#endif\n+        }\n+      } else {\n+        \/\/ 32-bit\n+        assert(!((src_first & 1) == 0 && src_first + 1 == src_second), \"no transform\");\n+        assert(!((dst_first & 1) == 0 && dst_first + 1 == dst_second), \"no transform\");\n+        int offset = ra_->reg2offset(src_first);\n+        if (masm) {\n+          __ movflt( as_XMMRegister(Matcher::_regEncode[dst_first]), Address(rsp, offset));\n+#ifndef PRODUCT\n+        } else {\n+          st->print(\"movss   %s, [rsp + #%d]\\t# spill\",\n+                     Matcher::regName[dst_first],\n+                     offset);\n+#endif\n+        }\n@@ -1433,4 +2247,16 @@\n-      break;\n-    case Op_RoundVF:\n-      if (UseAVX < 2) { \/\/ enabled for AVX2 only\n-        return false;\n+      return 0;\n+    } else if (dst_first_rc == rc_kreg) {\n+      \/\/ mem -> kreg\n+      if ((src_first & 1) == 0 && src_first + 1 == src_second &&\n+          (dst_first & 1) == 0 && dst_first + 1 == dst_second) {\n+        \/\/ 64-bit\n+        int offset = ra_->reg2offset(src_first);\n+        if (masm) {\n+          __ kmov(as_KRegister(Matcher::_regEncode[dst_first]), Address(rsp, offset));\n+#ifndef PRODUCT\n+        } else {\n+          st->print(\"kmovq   %s, [rsp + #%d]\\t# spill\",\n+                     Matcher::regName[dst_first],\n+                     offset);\n+#endif\n+        }\n@@ -1438,4 +2264,33 @@\n-      break;\n-    case Op_RoundVD:\n-      if (UseAVX < 3) {\n-        return false;  \/\/ enabled for AVX3 only\n+      return 0;\n+    }\n+  } else if (src_first_rc == rc_int) {\n+    \/\/ gpr ->\n+    if (dst_first_rc == rc_stack) {\n+      \/\/ gpr -> mem\n+      if ((src_first & 1) == 0 && src_first + 1 == src_second &&\n+          (dst_first & 1) == 0 && dst_first + 1 == dst_second) {\n+        \/\/ 64-bit\n+        int offset = ra_->reg2offset(dst_first);\n+        if (masm) {\n+          __ movq(Address(rsp, offset), as_Register(Matcher::_regEncode[src_first]));\n+#ifndef PRODUCT\n+        } else {\n+          st->print(\"movq    [rsp + #%d], %s\\t# spill\",\n+                     offset,\n+                     Matcher::regName[src_first]);\n+#endif\n+        }\n+      } else {\n+        \/\/ 32-bit\n+        assert(!((src_first & 1) == 0 && src_first + 1 == src_second), \"no transform\");\n+        assert(!((dst_first & 1) == 0 && dst_first + 1 == dst_second), \"no transform\");\n+        int offset = ra_->reg2offset(dst_first);\n+        if (masm) {\n+          __ movl(Address(rsp, offset), as_Register(Matcher::_regEncode[src_first]));\n+#ifndef PRODUCT\n+        } else {\n+          st->print(\"movl    [rsp + #%d], %s\\t# spill\",\n+                     offset,\n+                     Matcher::regName[src_first]);\n+#endif\n+        }\n@@ -1443,76 +2298,32 @@\n-      break;\n-    case Op_CompareAndSwapL:\n-    case Op_CompareAndSwapP:\n-      break;\n-    case Op_StrIndexOf:\n-      if (!UseSSE42Intrinsics) {\n-        return false;\n-      }\n-      break;\n-    case Op_StrIndexOfChar:\n-      if (!UseSSE42Intrinsics) {\n-        return false;\n-      }\n-      break;\n-    case Op_OnSpinWait:\n-      if (VM_Version::supports_on_spin_wait() == false) {\n-        return false;\n-      }\n-      break;\n-    case Op_MulVB:\n-    case Op_LShiftVB:\n-    case Op_RShiftVB:\n-    case Op_URShiftVB:\n-    case Op_VectorInsert:\n-    case Op_VectorLoadMask:\n-    case Op_VectorStoreMask:\n-    case Op_VectorBlend:\n-      if (UseSSE < 4) {\n-        return false;\n-      }\n-      break;\n-    case Op_MaxD:\n-    case Op_MaxF:\n-    case Op_MinD:\n-    case Op_MinF:\n-      if (UseAVX < 1) { \/\/ enabled for AVX only\n-        return false;\n-      }\n-      break;\n-    case Op_CacheWB:\n-    case Op_CacheWBPreSync:\n-    case Op_CacheWBPostSync:\n-      if (!VM_Version::supports_data_cache_line_flush()) {\n-        return false;\n-      }\n-      break;\n-    case Op_ExtractB:\n-    case Op_ExtractL:\n-    case Op_ExtractI:\n-    case Op_RoundDoubleMode:\n-      if (UseSSE < 4) {\n-        return false;\n-      }\n-      break;\n-    case Op_RoundDoubleModeV:\n-      if (VM_Version::supports_avx() == false) {\n-        return false; \/\/ 128bit vroundpd is not available\n-      }\n-      break;\n-    case Op_LoadVectorGather:\n-    case Op_LoadVectorGatherMasked:\n-      if (UseAVX < 2) {\n-        return false;\n-      }\n-      break;\n-    case Op_FmaF:\n-    case Op_FmaD:\n-    case Op_FmaVD:\n-    case Op_FmaVF:\n-      if (!UseFMA) {\n-        return false;\n-      }\n-      break;\n-    case Op_MacroLogicV:\n-      if (UseAVX < 3 || !UseVectorMacroLogic) {\n-        return false;\n+      return 0;\n+    } else if (dst_first_rc == rc_int) {\n+      \/\/ gpr -> gpr\n+      if ((src_first & 1) == 0 && src_first + 1 == src_second &&\n+          (dst_first & 1) == 0 && dst_first + 1 == dst_second) {\n+        \/\/ 64-bit\n+        if (masm) {\n+          __ movq(as_Register(Matcher::_regEncode[dst_first]),\n+                  as_Register(Matcher::_regEncode[src_first]));\n+#ifndef PRODUCT\n+        } else {\n+          st->print(\"movq    %s, %s\\t# spill\",\n+                     Matcher::regName[dst_first],\n+                     Matcher::regName[src_first]);\n+#endif\n+        }\n+        return 0;\n+      } else {\n+        \/\/ 32-bit\n+        assert(!((src_first & 1) == 0 && src_first + 1 == src_second), \"no transform\");\n+        assert(!((dst_first & 1) == 0 && dst_first + 1 == dst_second), \"no transform\");\n+        if (masm) {\n+          __ movl(as_Register(Matcher::_regEncode[dst_first]),\n+                  as_Register(Matcher::_regEncode[src_first]));\n+#ifndef PRODUCT\n+        } else {\n+          st->print(\"movl    %s, %s\\t# spill\",\n+                     Matcher::regName[dst_first],\n+                     Matcher::regName[src_first]);\n+#endif\n+        }\n+        return 0;\n@@ -1520,6 +2331,27 @@\n-      break;\n-\n-    case Op_VectorCmpMasked:\n-    case Op_VectorMaskGen:\n-      if (UseAVX < 3 || !VM_Version::supports_bmi2()) {\n-        return false;\n+    } else if (dst_first_rc == rc_float) {\n+      \/\/ gpr -> xmm\n+      if ((src_first & 1) == 0 && src_first + 1 == src_second &&\n+          (dst_first & 1) == 0 && dst_first + 1 == dst_second) {\n+        \/\/ 64-bit\n+        if (masm) {\n+          __ movdq( as_XMMRegister(Matcher::_regEncode[dst_first]), as_Register(Matcher::_regEncode[src_first]));\n+#ifndef PRODUCT\n+        } else {\n+          st->print(\"movdq   %s, %s\\t# spill\",\n+                     Matcher::regName[dst_first],\n+                     Matcher::regName[src_first]);\n+#endif\n+        }\n+      } else {\n+        \/\/ 32-bit\n+        assert(!((src_first & 1) == 0 && src_first + 1 == src_second), \"no transform\");\n+        assert(!((dst_first & 1) == 0 && dst_first + 1 == dst_second), \"no transform\");\n+        if (masm) {\n+          __ movdl( as_XMMRegister(Matcher::_regEncode[dst_first]), as_Register(Matcher::_regEncode[src_first]));\n+#ifndef PRODUCT\n+        } else {\n+          st->print(\"movdl   %s, %s\\t# spill\",\n+                     Matcher::regName[dst_first],\n+                     Matcher::regName[src_first]);\n+#endif\n+        }\n@@ -1527,7 +2359,14 @@\n-      break;\n-    case Op_VectorMaskFirstTrue:\n-    case Op_VectorMaskLastTrue:\n-    case Op_VectorMaskTrueCount:\n-    case Op_VectorMaskToLong:\n-      if (UseAVX < 1) {\n-         return false;\n+      return 0;\n+    } else if (dst_first_rc == rc_kreg) {\n+      if ((src_first & 1) == 0 && src_first + 1 == src_second &&\n+          (dst_first & 1) == 0 && dst_first + 1 == dst_second) {\n+        \/\/ 64-bit\n+        if (masm) {\n+          __ kmov(as_KRegister(Matcher::_regEncode[dst_first]), as_Register(Matcher::_regEncode[src_first]));\n+  #ifndef PRODUCT\n+        } else {\n+           st->print(\"kmovq   %s, %s\\t# spill\",\n+                       Matcher::regName[dst_first],\n+                       Matcher::regName[src_first]);\n+  #endif\n+        }\n@@ -1535,8 +2374,34 @@\n-      break;\n-    case Op_RoundF:\n-    case Op_RoundD:\n-      break;\n-    case Op_CopySignD:\n-    case Op_CopySignF:\n-      if (UseAVX < 3)  {\n-        return false;\n+      Unimplemented();\n+      return 0;\n+    }\n+  } else if (src_first_rc == rc_float) {\n+    \/\/ xmm ->\n+    if (dst_first_rc == rc_stack) {\n+      \/\/ xmm -> mem\n+      if ((src_first & 1) == 0 && src_first + 1 == src_second &&\n+          (dst_first & 1) == 0 && dst_first + 1 == dst_second) {\n+        \/\/ 64-bit\n+        int offset = ra_->reg2offset(dst_first);\n+        if (masm) {\n+          __ movdbl( Address(rsp, offset), as_XMMRegister(Matcher::_regEncode[src_first]));\n+#ifndef PRODUCT\n+        } else {\n+          st->print(\"movsd   [rsp + #%d], %s\\t# spill\",\n+                     offset,\n+                     Matcher::regName[src_first]);\n+#endif\n+        }\n+      } else {\n+        \/\/ 32-bit\n+        assert(!((src_first & 1) == 0 && src_first + 1 == src_second), \"no transform\");\n+        assert(!((dst_first & 1) == 0 && dst_first + 1 == dst_second), \"no transform\");\n+        int offset = ra_->reg2offset(dst_first);\n+        if (masm) {\n+          __ movflt(Address(rsp, offset), as_XMMRegister(Matcher::_regEncode[src_first]));\n+#ifndef PRODUCT\n+        } else {\n+          st->print(\"movss   [rsp + #%d], %s\\t# spill\",\n+                     offset,\n+                     Matcher::regName[src_first]);\n+#endif\n+        }\n@@ -1544,2 +2409,28 @@\n-      if (!VM_Version::supports_avx512vl()) {\n-        return false;\n+      return 0;\n+    } else if (dst_first_rc == rc_int) {\n+      \/\/ xmm -> gpr\n+      if ((src_first & 1) == 0 && src_first + 1 == src_second &&\n+          (dst_first & 1) == 0 && dst_first + 1 == dst_second) {\n+        \/\/ 64-bit\n+        if (masm) {\n+          __ movdq( as_Register(Matcher::_regEncode[dst_first]), as_XMMRegister(Matcher::_regEncode[src_first]));\n+#ifndef PRODUCT\n+        } else {\n+          st->print(\"movdq   %s, %s\\t# spill\",\n+                     Matcher::regName[dst_first],\n+                     Matcher::regName[src_first]);\n+#endif\n+        }\n+      } else {\n+        \/\/ 32-bit\n+        assert(!((src_first & 1) == 0 && src_first + 1 == src_second), \"no transform\");\n+        assert(!((dst_first & 1) == 0 && dst_first + 1 == dst_second), \"no transform\");\n+        if (masm) {\n+          __ movdl( as_Register(Matcher::_regEncode[dst_first]), as_XMMRegister(Matcher::_regEncode[src_first]));\n+#ifndef PRODUCT\n+        } else {\n+          st->print(\"movdl   %s, %s\\t# spill\",\n+                     Matcher::regName[dst_first],\n+                     Matcher::regName[src_first]);\n+#endif\n+        }\n@@ -1547,5 +2438,30 @@\n-      break;\n-    case Op_CompressBits:\n-    case Op_ExpandBits:\n-      if (!VM_Version::supports_bmi2()) {\n-        return false;\n+      return 0;\n+    } else if (dst_first_rc == rc_float) {\n+      \/\/ xmm -> xmm\n+      if ((src_first & 1) == 0 && src_first + 1 == src_second &&\n+          (dst_first & 1) == 0 && dst_first + 1 == dst_second) {\n+        \/\/ 64-bit\n+        if (masm) {\n+          __ movdbl( as_XMMRegister(Matcher::_regEncode[dst_first]), as_XMMRegister(Matcher::_regEncode[src_first]));\n+#ifndef PRODUCT\n+        } else {\n+          st->print(\"%s  %s, %s\\t# spill\",\n+                     UseXmmRegToRegMoveAll ? \"movapd\" : \"movsd \",\n+                     Matcher::regName[dst_first],\n+                     Matcher::regName[src_first]);\n+#endif\n+        }\n+      } else {\n+        \/\/ 32-bit\n+        assert(!((src_first & 1) == 0 && src_first + 1 == src_second), \"no transform\");\n+        assert(!((dst_first & 1) == 0 && dst_first + 1 == dst_second), \"no transform\");\n+        if (masm) {\n+          __ movflt( as_XMMRegister(Matcher::_regEncode[dst_first]), as_XMMRegister(Matcher::_regEncode[src_first]));\n+#ifndef PRODUCT\n+        } else {\n+          st->print(\"%s  %s, %s\\t# spill\",\n+                     UseXmmRegToRegMoveAll ? \"movaps\" : \"movss \",\n+                     Matcher::regName[dst_first],\n+                     Matcher::regName[src_first]);\n+#endif\n+        }\n@@ -1553,4 +2469,21 @@\n-      break;\n-    case Op_CompressM:\n-      if (!VM_Version::supports_avx512vl() || !VM_Version::supports_bmi2()) {\n-        return false;\n+      return 0;\n+    } else if (dst_first_rc == rc_kreg) {\n+      assert(false, \"Illegal spilling\");\n+      return 0;\n+    }\n+  } else if (src_first_rc == rc_kreg) {\n+    if (dst_first_rc == rc_stack) {\n+      \/\/ mem -> kreg\n+      if ((src_first & 1) == 0 && src_first + 1 == src_second &&\n+          (dst_first & 1) == 0 && dst_first + 1 == dst_second) {\n+        \/\/ 64-bit\n+        int offset = ra_->reg2offset(dst_first);\n+        if (masm) {\n+          __ kmov(Address(rsp, offset), as_KRegister(Matcher::_regEncode[src_first]));\n+#ifndef PRODUCT\n+        } else {\n+          st->print(\"kmovq   [rsp + #%d] , %s\\t# spill\",\n+                     offset,\n+                     Matcher::regName[src_first]);\n+#endif\n+        }\n@@ -1558,5 +2491,14 @@\n-      break;\n-    case Op_ConvF2HF:\n-    case Op_ConvHF2F:\n-      if (!VM_Version::supports_float16()) {\n-        return false;\n+      return 0;\n+    } else if (dst_first_rc == rc_int) {\n+      if ((src_first & 1) == 0 && src_first + 1 == src_second &&\n+          (dst_first & 1) == 0 && dst_first + 1 == dst_second) {\n+        \/\/ 64-bit\n+        if (masm) {\n+          __ kmov(as_Register(Matcher::_regEncode[dst_first]), as_KRegister(Matcher::_regEncode[src_first]));\n+#ifndef PRODUCT\n+        } else {\n+         st->print(\"kmovq   %s, %s\\t# spill\",\n+                     Matcher::regName[dst_first],\n+                     Matcher::regName[src_first]);\n+#endif\n+        }\n@@ -1564,5 +2506,15 @@\n-      break;\n-    case Op_VectorCastF2HF:\n-    case Op_VectorCastHF2F:\n-      if (!VM_Version::supports_f16c() && !VM_Version::supports_evex()) {\n-        return false;\n+      Unimplemented();\n+      return 0;\n+    } else if (dst_first_rc == rc_kreg) {\n+      if ((src_first & 1) == 0 && src_first + 1 == src_second &&\n+          (dst_first & 1) == 0 && dst_first + 1 == dst_second) {\n+        \/\/ 64-bit\n+        if (masm) {\n+          __ kmov(as_KRegister(Matcher::_regEncode[dst_first]), as_KRegister(Matcher::_regEncode[src_first]));\n+#ifndef PRODUCT\n+        } else {\n+         st->print(\"kmovq   %s, %s\\t# spill\",\n+                     Matcher::regName[dst_first],\n+                     Matcher::regName[src_first]);\n+#endif\n+        }\n@@ -1570,1 +2522,5 @@\n-      break;\n+      return 0;\n+    } else if (dst_first_rc == rc_float) {\n+      assert(false, \"Illegal spill\");\n+      return 0;\n+    }\n@@ -1572,1 +2528,4 @@\n-  return true;  \/\/ Match rules are supported by default.\n+\n+  assert(0,\" foo \");\n+  Unimplemented();\n+  return 0;\n@@ -1575,1 +2534,5 @@\n-\/\/------------------------------------------------------------------------\n+#ifndef PRODUCT\n+void MachSpillCopyNode::format(PhaseRegAlloc *ra_, outputStream* st) const {\n+  implementation(nullptr, ra_, false, st);\n+}\n+#endif\n@@ -1577,3 +2540,2 @@\n-static inline bool is_pop_count_instr_target(BasicType bt) {\n-  return (is_subword_type(bt) && VM_Version::supports_avx512_bitalg()) ||\n-         (is_non_subword_integral_type(bt) && VM_Version::supports_avx512_vpopcntdq());\n+void MachSpillCopyNode::emit(C2_MacroAssembler *masm, PhaseRegAlloc *ra_) const {\n+  implementation(masm, ra_, false, nullptr);\n@@ -1582,2 +2544,2 @@\n-bool Matcher::match_rule_supported_auto_vectorization(int opcode, int vlen, BasicType bt) {\n-  return match_rule_supported_vector(opcode, vlen, bt);\n+uint MachSpillCopyNode::size(PhaseRegAlloc *ra_) const {\n+  return MachNode::size(ra_);\n@@ -1586,5 +2548,26 @@\n-\/\/ Identify extra cases that we might want to provide match rules for vector nodes and\n-\/\/ other intrinsics guarded with vector length (vlen) and element type (bt).\n-bool Matcher::match_rule_supported_vector(int opcode, int vlen, BasicType bt) {\n-  if (!match_rule_supported(opcode)) {\n-    return false;\n+\/\/=============================================================================\n+#ifndef PRODUCT\n+void BoxLockNode::format(PhaseRegAlloc* ra_, outputStream* st) const\n+{\n+  int offset = ra_->reg2offset(in_RegMask(0).find_first_elem());\n+  int reg = ra_->get_reg_first(this);\n+  st->print(\"leaq    %s, [rsp + #%d]\\t# box lock\",\n+            Matcher::regName[reg], offset);\n+}\n+#endif\n+\n+void BoxLockNode::emit(C2_MacroAssembler* masm, PhaseRegAlloc* ra_) const\n+{\n+  int offset = ra_->reg2offset(in_RegMask(0).find_first_elem());\n+  int reg = ra_->get_encode(this);\n+\n+  __ lea(as_Register(reg), Address(rsp, offset));\n+}\n+\n+uint BoxLockNode::size(PhaseRegAlloc *ra_) const\n+{\n+  int offset = ra_->reg2offset(in_RegMask(0).find_first_elem());\n+  if (ra_->get_encode(this) > 15) {\n+    return (offset < 0x80) ? 6 : 9; \/\/ REX2\n+  } else {\n+    return (offset < 0x80) ? 5 : 8; \/\/ REX\n@@ -1592,10 +2575,35 @@\n-  \/\/ Matcher::vector_size_supported() restricts vector sizes in the following way (see Matcher::vector_width_in_bytes):\n-  \/\/   * SSE2 supports 128bit vectors for all types;\n-  \/\/   * AVX1 supports 256bit vectors only for FLOAT and DOUBLE types;\n-  \/\/   * AVX2 supports 256bit vectors for all types;\n-  \/\/   * AVX512F supports 512bit vectors only for INT, FLOAT, and DOUBLE types;\n-  \/\/   * AVX512BW supports 512bit vectors for BYTE, SHORT, and CHAR types.\n-  \/\/ There's also a limit on minimum vector size supported: 2 elements (or 4 bytes for BYTE).\n-  \/\/ And MaxVectorSize is taken into account as well.\n-  if (!vector_size_supported(bt, vlen)) {\n-    return false;\n+}\n+\n+\/\/=============================================================================\n+#ifndef PRODUCT\n+void MachVEPNode::format(PhaseRegAlloc* ra_, outputStream* st) const\n+{\n+  st->print_cr(\"MachVEPNode\");\n+}\n+#endif\n+\n+void MachVEPNode::emit(C2_MacroAssembler* masm, PhaseRegAlloc* ra_) const\n+{\n+  CodeBuffer* cbuf = masm->code();\n+  uint insts_size = cbuf->insts_size();\n+  if (!_verified) {\n+    __ ic_check(1);\n+  } else {\n+    \/\/ TODO 8284443 Avoid creation of temporary frame\n+    if (ra_->C->stub_function() == nullptr) {\n+      __ verified_entry(ra_->C, 0);\n+      __ entry_barrier();\n+      int initial_framesize = ra_->C->output()->frame_size_in_bytes() - 2*wordSize;\n+      __ remove_frame(initial_framesize, false);\n+    }\n+    \/\/ Unpack inline type args passed as oop and then jump to\n+    \/\/ the verified entry point (skipping the unverified entry).\n+    int sp_inc = __ unpack_inline_args(ra_->C, _receiver_only);\n+    \/\/ Emit code for verified entry and save increment for stack repair on return\n+    __ verified_entry(ra_->C, sp_inc);\n+    if (Compile::current()->output()->in_scratch_emit_size()) {\n+      Label dummy_verified_entry;\n+      __ jmp(dummy_verified_entry);\n+    } else {\n+      __ jmp(*_verified_entry);\n+    }\n@@ -1603,21 +2611,337 @@\n-  \/\/ Special cases which require vector length follow:\n-  \/\/   * implementation limitations\n-  \/\/   * some 512bit vector operations on FLOAT and DOUBLE types require AVX512DQ\n-  \/\/   * 128bit vroundpd instruction is present only in AVX1\n-  int size_in_bits = vlen * type2aelembytes(bt) * BitsPerByte;\n-  switch (opcode) {\n-    case Op_MaxVHF:\n-    case Op_MinVHF:\n-      if (!VM_Version::supports_avx512bw()) {\n-        return false;\n-      }\n-    case Op_AddVHF:\n-    case Op_DivVHF:\n-    case Op_FmaVHF:\n-    case Op_MulVHF:\n-    case Op_SubVHF:\n-    case Op_SqrtVHF:\n-      if (size_in_bits < 512 && !VM_Version::supports_avx512vl()) {\n-        return false;\n-      }\n-      if (!VM_Version::supports_avx512_fp16()) {\n+  \/* WARNING these NOPs are critical so that verified entry point is properly\n+     4 bytes aligned for patching by NativeJump::patch_verified_entry() *\/\n+  int nops_cnt = 4 - ((cbuf->insts_size() - insts_size) & 0x3);\n+  nops_cnt &= 0x3; \/\/ Do not add nops if code is aligned.\n+  if (nops_cnt > 0) {\n+    __ nop(nops_cnt);\n+  }\n+}\n+\n+\/\/=============================================================================\n+#ifndef PRODUCT\n+void MachUEPNode::format(PhaseRegAlloc* ra_, outputStream* st) const\n+{\n+  if (UseCompressedClassPointers) {\n+    st->print_cr(\"movl    rscratch1, [j_rarg0 + oopDesc::klass_offset_in_bytes()]\\t# compressed klass\");\n+    st->print_cr(\"\\tcmpl    rscratch1, [rax + CompiledICData::speculated_klass_offset()]\\t # Inline cache check\");\n+  } else {\n+    st->print_cr(\"movq    rscratch1, [j_rarg0 + oopDesc::klass_offset_in_bytes()]\\t# compressed klass\");\n+    st->print_cr(\"\\tcmpq    rscratch1, [rax + CompiledICData::speculated_klass_offset()]\\t # Inline cache check\");\n+  }\n+  st->print_cr(\"\\tjne     SharedRuntime::_ic_miss_stub\");\n+}\n+#endif\n+\n+void MachUEPNode::emit(C2_MacroAssembler* masm, PhaseRegAlloc* ra_) const\n+{\n+  __ ic_check(InteriorEntryAlignment);\n+}\n+\n+\n+\/\/=============================================================================\n+\n+bool Matcher::supports_vector_calling_convention(void) {\n+  return EnableVectorSupport;\n+}\n+\n+OptoRegPair Matcher::vector_return_value(uint ideal_reg) {\n+  assert(EnableVectorSupport, \"sanity\");\n+  int lo = XMM0_num;\n+  int hi = XMM0b_num;\n+  if (ideal_reg == Op_VecX) hi = XMM0d_num;\n+  else if (ideal_reg == Op_VecY) hi = XMM0h_num;\n+  else if (ideal_reg == Op_VecZ) hi = XMM0p_num;\n+  return OptoRegPair(hi, lo);\n+}\n+\n+\/\/ Is this branch offset short enough that a short branch can be used?\n+\/\/\n+\/\/ NOTE: If the platform does not provide any short branch variants, then\n+\/\/       this method should return false for offset 0.\n+bool Matcher::is_short_branch_offset(int rule, int br_size, int offset) {\n+  \/\/ The passed offset is relative to address of the branch.\n+  \/\/ On 86 a branch displacement is calculated relative to address\n+  \/\/ of a next instruction.\n+  offset -= br_size;\n+\n+  \/\/ the short version of jmpConUCF2 contains multiple branches,\n+  \/\/ making the reach slightly less\n+  if (rule == jmpConUCF2_rule)\n+    return (-126 <= offset && offset <= 125);\n+  return (-128 <= offset && offset <= 127);\n+}\n+\n+\/\/ Return whether or not this register is ever used as an argument.\n+\/\/ This function is used on startup to build the trampoline stubs in\n+\/\/ generateOptoStub.  Registers not mentioned will be killed by the VM\n+\/\/ call in the trampoline, and arguments in those registers not be\n+\/\/ available to the callee.\n+bool Matcher::can_be_java_arg(int reg)\n+{\n+  return\n+    reg ==  RDI_num || reg == RDI_H_num ||\n+    reg ==  RSI_num || reg == RSI_H_num ||\n+    reg ==  RDX_num || reg == RDX_H_num ||\n+    reg ==  RCX_num || reg == RCX_H_num ||\n+    reg ==   R8_num || reg ==  R8_H_num ||\n+    reg ==   R9_num || reg ==  R9_H_num ||\n+    reg ==  R12_num || reg == R12_H_num ||\n+    reg == XMM0_num || reg == XMM0b_num ||\n+    reg == XMM1_num || reg == XMM1b_num ||\n+    reg == XMM2_num || reg == XMM2b_num ||\n+    reg == XMM3_num || reg == XMM3b_num ||\n+    reg == XMM4_num || reg == XMM4b_num ||\n+    reg == XMM5_num || reg == XMM5b_num ||\n+    reg == XMM6_num || reg == XMM6b_num ||\n+    reg == XMM7_num || reg == XMM7b_num;\n+}\n+\n+bool Matcher::is_spillable_arg(int reg)\n+{\n+  return can_be_java_arg(reg);\n+}\n+\n+uint Matcher::int_pressure_limit()\n+{\n+  return (INTPRESSURE == -1) ? _INT_REG_mask.size() : INTPRESSURE;\n+}\n+\n+uint Matcher::float_pressure_limit()\n+{\n+  \/\/ After experiment around with different values, the following default threshold\n+  \/\/ works best for LCM's register pressure scheduling on x64.\n+  uint dec_count  = VM_Version::supports_evex() ? 4 : 2;\n+  uint default_float_pressure_threshold = _FLOAT_REG_mask.size() - dec_count;\n+  return (FLOATPRESSURE == -1) ? default_float_pressure_threshold : FLOATPRESSURE;\n+}\n+\n+bool Matcher::use_asm_for_ldiv_by_con( jlong divisor ) {\n+  \/\/ In 64 bit mode a code which use multiply when\n+  \/\/ devisor is constant is faster than hardware\n+  \/\/ DIV instruction (it uses MulHiL).\n+  return false;\n+}\n+\n+\/\/ Register for DIVI projection of divmodI\n+const RegMask& Matcher::divI_proj_mask() {\n+  return INT_RAX_REG_mask();\n+}\n+\n+\/\/ Register for MODI projection of divmodI\n+const RegMask& Matcher::modI_proj_mask() {\n+  return INT_RDX_REG_mask();\n+}\n+\n+\/\/ Register for DIVL projection of divmodL\n+const RegMask& Matcher::divL_proj_mask() {\n+  return LONG_RAX_REG_mask();\n+}\n+\n+\/\/ Register for MODL projection of divmodL\n+const RegMask& Matcher::modL_proj_mask() {\n+  return LONG_RDX_REG_mask();\n+}\n+\n+%}\n+\n+source_hpp %{\n+\/\/ Header information of the source block.\n+\/\/ Method declarations\/definitions which are used outside\n+\/\/ the ad-scope can conveniently be defined here.\n+\/\/\n+\/\/ To keep related declarations\/definitions\/uses close together,\n+\/\/ we switch between source %{ }% and source_hpp %{ }% freely as needed.\n+\n+#include \"runtime\/vm_version.hpp\"\n+\n+class NativeJump;\n+\n+class CallStubImpl {\n+\n+  \/\/--------------------------------------------------------------\n+  \/\/---<  Used for optimization in Compile::shorten_branches  >---\n+  \/\/--------------------------------------------------------------\n+\n+ public:\n+  \/\/ Size of call trampoline stub.\n+  static uint size_call_trampoline() {\n+    return 0; \/\/ no call trampolines on this platform\n+  }\n+\n+  \/\/ number of relocations needed by a call trampoline stub\n+  static uint reloc_call_trampoline() {\n+    return 0; \/\/ no call trampolines on this platform\n+  }\n+};\n+\n+class HandlerImpl {\n+\n+ public:\n+\n+  static int emit_deopt_handler(C2_MacroAssembler* masm);\n+\n+  static uint size_deopt_handler() {\n+    \/\/ one call and one jmp.\n+    return 10;\n+  }\n+};\n+\n+inline Assembler::AvxVectorLen vector_length_encoding(int bytes) {\n+  switch(bytes) {\n+    case  4: \/\/ fall-through\n+    case  8: \/\/ fall-through\n+    case 16: return Assembler::AVX_128bit;\n+    case 32: return Assembler::AVX_256bit;\n+    case 64: return Assembler::AVX_512bit;\n+\n+    default: {\n+      ShouldNotReachHere();\n+      return Assembler::AVX_NoVec;\n+    }\n+  }\n+}\n+\n+static inline Assembler::AvxVectorLen vector_length_encoding(const Node* n) {\n+  return vector_length_encoding(Matcher::vector_length_in_bytes(n));\n+}\n+\n+static inline Assembler::AvxVectorLen vector_length_encoding(const MachNode* use, MachOper* opnd) {\n+  uint def_idx = use->operand_index(opnd);\n+  Node* def = use->in(def_idx);\n+  return vector_length_encoding(def);\n+}\n+\n+static inline bool is_vector_popcount_predicate(BasicType bt) {\n+  return (is_subword_type(bt) && VM_Version::supports_avx512_bitalg()) ||\n+         (is_non_subword_integral_type(bt) && VM_Version::supports_avx512_vpopcntdq());\n+}\n+\n+static inline bool is_clz_non_subword_predicate_evex(BasicType bt, int vlen_bytes) {\n+  return is_non_subword_integral_type(bt) && VM_Version::supports_avx512cd() &&\n+           (VM_Version::supports_avx512vl() || vlen_bytes == 64);\n+}\n+\n+class Node::PD {\n+public:\n+  enum NodeFlags {\n+    Flag_intel_jcc_erratum    = Node::_last_flag << 1,\n+    Flag_sets_carry_flag      = Node::_last_flag << 2,\n+    Flag_sets_parity_flag     = Node::_last_flag << 3,\n+    Flag_sets_zero_flag       = Node::_last_flag << 4,\n+    Flag_sets_overflow_flag   = Node::_last_flag << 5,\n+    Flag_sets_sign_flag       = Node::_last_flag << 6,\n+    Flag_clears_carry_flag    = Node::_last_flag << 7,\n+    Flag_clears_parity_flag   = Node::_last_flag << 8,\n+    Flag_clears_zero_flag     = Node::_last_flag << 9,\n+    Flag_clears_overflow_flag = Node::_last_flag << 10,\n+    Flag_clears_sign_flag     = Node::_last_flag << 11,\n+    _last_flag                = Flag_clears_sign_flag\n+  };\n+};\n+\n+%} \/\/ end source_hpp\n+\n+source %{\n+\n+#include \"opto\/addnode.hpp\"\n+#include \"c2_intelJccErratum_x86.hpp\"\n+\n+void PhaseOutput::pd_perform_mach_node_analysis() {\n+  if (VM_Version::has_intel_jcc_erratum()) {\n+    int extra_padding = IntelJccErratum::tag_affected_machnodes(C, C->cfg(), C->regalloc());\n+    _buf_sizes._code += extra_padding;\n+  }\n+}\n+\n+int MachNode::pd_alignment_required() const {\n+  if (VM_Version::has_intel_jcc_erratum() && IntelJccErratum::is_jcc_erratum_branch(this)) {\n+    \/\/ Conservatively add worst case padding. We assume that relocInfo::addr_unit() is 1 on x86.\n+    return IntelJccErratum::largest_jcc_size() + 1;\n+  } else {\n+    return 1;\n+  }\n+}\n+\n+int MachNode::compute_padding(int current_offset) const {\n+  if (flags() & Node::PD::Flag_intel_jcc_erratum) {\n+    Compile* C = Compile::current();\n+    PhaseOutput* output = C->output();\n+    Block* block = output->block();\n+    int index = output->index();\n+    return IntelJccErratum::compute_padding(current_offset, this, block, index, C->regalloc());\n+  } else {\n+    return 0;\n+  }\n+}\n+\n+\/\/ Emit deopt handler code.\n+int HandlerImpl::emit_deopt_handler(C2_MacroAssembler* masm) {\n+\n+  \/\/ Note that the code buffer's insts_mark is always relative to insts.\n+  \/\/ That's why we must use the macroassembler to generate a handler.\n+  address base = __ start_a_stub(size_deopt_handler());\n+  if (base == nullptr) {\n+    ciEnv::current()->record_failure(\"CodeCache is full\");\n+    return 0;  \/\/ CodeBuffer::expand failed\n+  }\n+  int offset = __ offset();\n+\n+  Label start;\n+  __ bind(start);\n+\n+  __ call(RuntimeAddress(SharedRuntime::deopt_blob()->unpack()));\n+\n+  int entry_offset = __ offset();\n+\n+  __ jmp(start);\n+\n+  assert(__ offset() - offset <= (int) size_deopt_handler(), \"overflow %d\", (__ offset() - offset));\n+  __ end_a_stub();\n+  return entry_offset;\n+}\n+\n+static Assembler::Width widthForType(BasicType bt) {\n+  if (bt == T_BYTE) {\n+    return Assembler::B;\n+  } else if (bt == T_SHORT) {\n+    return Assembler::W;\n+  } else if (bt == T_INT) {\n+    return Assembler::D;\n+  } else {\n+    assert(bt == T_LONG, \"not a long: %s\", type2name(bt));\n+    return Assembler::Q;\n+  }\n+}\n+\n+\/\/=============================================================================\n+\n+  \/\/ Float masks come from different places depending on platform.\n+  static address float_signmask()  { return StubRoutines::x86::float_sign_mask(); }\n+  static address float_signflip()  { return StubRoutines::x86::float_sign_flip(); }\n+  static address double_signmask() { return StubRoutines::x86::double_sign_mask(); }\n+  static address double_signflip() { return StubRoutines::x86::double_sign_flip(); }\n+  static address vector_short_to_byte_mask() { return StubRoutines::x86::vector_short_to_byte_mask(); }\n+  static address vector_int_to_byte_mask() { return StubRoutines::x86::vector_int_to_byte_mask(); }\n+  static address vector_byte_perm_mask() { return StubRoutines::x86::vector_byte_perm_mask(); }\n+  static address vector_long_sign_mask() { return StubRoutines::x86::vector_long_sign_mask(); }\n+  static address vector_all_bits_set() { return StubRoutines::x86::vector_all_bits_set(); }\n+  static address vector_int_mask_cmp_bits() { return StubRoutines::x86::vector_int_mask_cmp_bits(); }\n+  static address vector_int_to_short_mask() { return StubRoutines::x86::vector_int_to_short_mask(); }\n+  static address vector_byte_shufflemask() { return StubRoutines::x86::vector_byte_shuffle_mask(); }\n+  static address vector_short_shufflemask() { return StubRoutines::x86::vector_short_shuffle_mask(); }\n+  static address vector_int_shufflemask() { return StubRoutines::x86::vector_int_shuffle_mask(); }\n+  static address vector_long_shufflemask() { return StubRoutines::x86::vector_long_shuffle_mask(); }\n+  static address vector_32_bit_mask() { return StubRoutines::x86::vector_32_bit_mask(); }\n+  static address vector_64_bit_mask() { return StubRoutines::x86::vector_64_bit_mask(); }\n+  static address vector_float_signflip() { return StubRoutines::x86::vector_float_sign_flip();}\n+  static address vector_double_signflip() { return StubRoutines::x86::vector_double_sign_flip();}\n+\n+\/\/=============================================================================\n+bool Matcher::match_rule_supported(int opcode) {\n+  if (!has_match_rule(opcode)) {\n+    return false; \/\/ no match rule present\n+  }\n+  switch (opcode) {\n+    case Op_AbsVL:\n+    case Op_StoreVectorScatter:\n+      if (UseAVX < 3) {\n@@ -1627,4 +2951,4 @@\n-    case Op_AbsVF:\n-    case Op_NegVF:\n-      if ((vlen == 16) && (VM_Version::supports_avx512dq() == false)) {\n-        return false; \/\/ 512bit vandps and vxorps are not available\n+    case Op_PopCountI:\n+    case Op_PopCountL:\n+      if (!UsePopCountInstruction) {\n+        return false;\n@@ -1633,4 +2957,3 @@\n-    case Op_AbsVD:\n-    case Op_NegVD:\n-      if ((vlen == 8) && (VM_Version::supports_avx512dq() == false)) {\n-        return false; \/\/ 512bit vpmullq, vandpd and vxorpd are not available\n+    case Op_PopCountVI:\n+      if (UseAVX < 2) {\n+        return false;\n@@ -1639,24 +2962,4 @@\n-    case Op_RotateRightV:\n-    case Op_RotateLeftV:\n-      if (bt != T_INT && bt != T_LONG) {\n-        return false;\n-      } \/\/ fallthrough\n-    case Op_MacroLogicV:\n-      if (!VM_Version::supports_evex() ||\n-          ((size_in_bits != 512) && !VM_Version::supports_avx512vl())) {\n-        return false;\n-      }\n-      break;\n-    case Op_ClearArray:\n-    case Op_VectorMaskGen:\n-    case Op_VectorCmpMasked:\n-      if (!VM_Version::supports_avx512bw()) {\n-        return false;\n-      }\n-      if ((size_in_bits != 512) && !VM_Version::supports_avx512vl()) {\n-        return false;\n-      }\n-      break;\n-    case Op_LoadVectorMasked:\n-    case Op_StoreVectorMasked:\n-      if (!VM_Version::supports_avx512bw() && (is_subword_type(bt) || UseAVX < 1)) {\n+    case Op_CompressV:\n+    case Op_ExpandV:\n+    case Op_PopCountVL:\n+      if (UseAVX < 2) {\n@@ -1666,3 +2969,2 @@\n-    case Op_UMinV:\n-    case Op_UMaxV:\n-      if (UseAVX == 0) {\n+    case Op_MulVI:\n+      if ((UseSSE < 4) && (UseAVX < 1)) { \/\/ only with SSE4_1 or AVX\n@@ -1672,3 +2974,2 @@\n-    case Op_MaxV:\n-    case Op_MinV:\n-      if (UseSSE < 4 && is_integral_type(bt)) {\n+    case Op_MulVL:\n+      if (UseSSE < 4) { \/\/ only with SSE4_1 or AVX\n@@ -1677,11 +2978,2 @@\n-      if ((bt == T_FLOAT || bt == T_DOUBLE)) {\n-          \/\/ Float\/Double intrinsics are enabled for AVX family currently.\n-          if (UseAVX == 0) {\n-            return false;\n-          }\n-          if (UseAVX > 2 && (!VM_Version::supports_avx512dq() && size_in_bits == 512)) { \/\/ 512 bit Float\/Double intrinsics need AVX512DQ\n-            return false;\n-          }\n-      }\n-    case Op_CallLeafVector:\n-      if (size_in_bits == 512 && !VM_Version::supports_avx512vlbwdq()) {\n+    case Op_MulReductionVL:\n+      if (VM_Version::supports_avx512dq() == false) {\n@@ -1692,0 +2984,3 @@\n+    case Op_AbsVB:\n+    case Op_AbsVS:\n+    case Op_AbsVI:\n@@ -1693,4 +2988,0 @@\n-      if (bt == T_INT && (UseSSE < 3 || !VM_Version::supports_ssse3())) {\n-        return false;\n-      }\n-      \/\/ fallthrough\n@@ -1700,1 +2991,1 @@\n-      if (is_subword_type(bt) && (UseSSE < 4)) {\n+      if (UseSSE < 3) { \/\/ requires at least SSSE3\n@@ -1704,9 +2995,3 @@\n-    case Op_MinReductionV:\n-    case Op_MaxReductionV:\n-      if ((bt == T_INT || is_subword_type(bt)) && UseSSE < 4) {\n-        return false;\n-      } else if (bt == T_LONG && (UseAVX < 3 || !VM_Version::supports_avx512vlbwdq())) {\n-        return false;\n-      }\n-      \/\/ Float\/Double intrinsics enabled for AVX family.\n-      if (UseAVX == 0 && (bt == T_FLOAT || bt == T_DOUBLE)) {\n+    case Op_MaxHF:\n+    case Op_MinHF:\n+      if (!VM_Version::supports_avx512vlbw()) {\n@@ -1714,2 +2999,10 @@\n-      }\n-      if (UseAVX > 2 && (!VM_Version::supports_avx512dq() && size_in_bits == 512)) {\n+      }  \/\/ fallthrough\n+    case Op_AddHF:\n+    case Op_DivHF:\n+    case Op_FmaHF:\n+    case Op_MulHF:\n+    case Op_ReinterpretS2HF:\n+    case Op_ReinterpretHF2S:\n+    case Op_SubHF:\n+    case Op_SqrtHF:\n+      if (!VM_Version::supports_avx512_fp16()) {\n@@ -1719,7 +3012,0 @@\n-    case Op_VectorTest:\n-      if (UseSSE < 4) {\n-        return false; \/\/ Implementation limitation\n-      } else if (size_in_bits < 32) {\n-        return false; \/\/ Implementation limitation\n-      }\n-      break;\n@@ -1728,15 +3014,3 @@\n-      if(vlen == 2) {\n-        return false; \/\/ Implementation limitation due to how shuffle is loaded\n-      } else if (size_in_bits == 256 && UseAVX < 2) {\n-        return false; \/\/ Implementation limitation\n-      }\n-      break;\n-    case Op_VectorLoadMask:\n-    case Op_VectorMaskCast:\n-      if (size_in_bits == 256 && UseAVX < 2) {\n-        return false; \/\/ Implementation limitation\n-      }\n-      \/\/ fallthrough\n-    case Op_VectorStoreMask:\n-      if (vlen == 2) {\n-        return false; \/\/ Implementation limitation\n+    case Op_MulReductionVI:\n+      if (UseSSE < 4) { \/\/ requires at least SSE4\n+        return false;\n@@ -1745,2 +3019,3 @@\n-    case Op_PopulateIndex:\n-      if (size_in_bits > 256 && !VM_Version::supports_avx512bw()) {\n+    case Op_IsInfiniteF:\n+    case Op_IsInfiniteD:\n+      if (!VM_Version::supports_avx512dq()) {\n@@ -1750,0 +3025,3 @@\n+    case Op_SqrtVD:\n+    case Op_SqrtVF:\n+    case Op_VectorMaskCmp:\n@@ -1753,7 +3031,7 @@\n-      if (bt != T_DOUBLE && size_in_bits == 256 && UseAVX < 2) {\n-        return false;\n-      }\n-      break;\n-      if (is_integral_type(bt) && size_in_bits == 256 && UseAVX < 2) {\n-        return false;\n-      } else if (!is_integral_type(bt) && !VM_Version::supports_avx512dq()) {\n+    case Op_VectorCastF2X:\n+    case Op_VectorCastD2X:\n+    case Op_VectorUCastB2X:\n+    case Op_VectorUCastS2X:\n+    case Op_VectorUCastI2X:\n+    case Op_VectorMaskCast:\n+      if (UseAVX < 1) { \/\/ enabled for AVX only\n@@ -1764,12 +3042,2 @@\n-    case Op_VectorCastF2X: {\n-        \/\/ As per JLS section 5.1.3 narrowing conversion to sub-word types\n-        \/\/ happen after intermediate conversion to integer and special handling\n-        \/\/ code needs AVX2 vpcmpeqd instruction for 256 bit vectors.\n-        int src_size_in_bits = type2aelembytes(T_FLOAT) * vlen * BitsPerByte;\n-        if (is_integral_type(bt) && src_size_in_bits == 256 && UseAVX < 2) {\n-          return false;\n-        }\n-      }\n-      \/\/ fallthrough\n-    case Op_VectorCastD2X:\n-      if (bt == T_LONG && !VM_Version::supports_avx512dq()) {\n+    case Op_PopulateIndex:\n+      if (UseAVX < 2) {\n@@ -1779,5 +3047,2 @@\n-    case Op_VectorCastF2HF:\n-    case Op_VectorCastHF2F:\n-      if (!VM_Version::supports_f16c() &&\n-         ((!VM_Version::supports_evex() ||\n-         ((size_in_bits != 512) && !VM_Version::supports_avx512vl())))) {\n+    case Op_RoundVF:\n+      if (UseAVX < 2) { \/\/ enabled for AVX2 only\n@@ -1788,2 +3053,2 @@\n-      if (!VM_Version::supports_avx512dq()) {\n-        return false;\n+      if (UseAVX < 3) {\n+        return false;  \/\/ enabled for AVX3 only\n@@ -1792,2 +3057,5 @@\n-    case Op_MulReductionVI:\n-      if (bt == T_BYTE && size_in_bits == 512 && !VM_Version::supports_avx512bw()) {\n+    case Op_CompareAndSwapL:\n+    case Op_CompareAndSwapP:\n+      break;\n+    case Op_StrIndexOf:\n+      if (!UseSSE42Intrinsics) {\n@@ -1797,2 +3065,2 @@\n-    case Op_LoadVectorGatherMasked:\n-      if (!is_subword_type(bt) && size_in_bits < 512 && !VM_Version::supports_avx512vl()) {\n+    case Op_StrIndexOfChar:\n+      if (!UseSSE42Intrinsics) {\n@@ -1801,4 +3069,3 @@\n-      if (is_subword_type(bt) &&\n-         ((size_in_bits > 256 && !VM_Version::supports_avx512bw()) ||\n-          (size_in_bits < 64)                                      ||\n-          (bt == T_SHORT && !VM_Version::supports_bmi2()))) {\n+      break;\n+    case Op_OnSpinWait:\n+      if (VM_Version::supports_on_spin_wait() == false) {\n@@ -1808,5 +3075,9 @@\n-    case Op_StoreVectorScatterMasked:\n-    case Op_StoreVectorScatter:\n-      if (is_subword_type(bt)) {\n-        return false;\n-      } else if (size_in_bits < 512 && !VM_Version::supports_avx512vl()) {\n+    case Op_MulVB:\n+    case Op_LShiftVB:\n+    case Op_RShiftVB:\n+    case Op_URShiftVB:\n+    case Op_VectorInsert:\n+    case Op_VectorLoadMask:\n+    case Op_VectorStoreMask:\n+    case Op_VectorBlend:\n+      if (UseSSE < 4) {\n@@ -1815,3 +3086,6 @@\n-      \/\/ fallthrough\n-    case Op_LoadVectorGather:\n-      if (!is_subword_type(bt) && size_in_bits == 64) {\n+      break;\n+    case Op_MaxD:\n+    case Op_MaxF:\n+    case Op_MinD:\n+    case Op_MinF:\n+      if (UseAVX < 1) { \/\/ enabled for AVX only\n@@ -1820,1 +3094,5 @@\n-      if (is_subword_type(bt) && size_in_bits < 64) {\n+      break;\n+    case Op_CacheWB:\n+    case Op_CacheWBPreSync:\n+    case Op_CacheWBPostSync:\n+      if (!VM_Version::supports_data_cache_line_flush()) {\n@@ -1824,6 +3102,5 @@\n-    case Op_SaturatingAddV:\n-    case Op_SaturatingSubV:\n-      if (UseAVX < 1) {\n-        return false; \/\/ Implementation limitation\n-      }\n-      if (is_subword_type(bt) && size_in_bits == 512 && !VM_Version::supports_avx512bw()) {\n+    case Op_ExtractB:\n+    case Op_ExtractL:\n+    case Op_ExtractI:\n+    case Op_RoundDoubleMode:\n+      if (UseSSE < 4) {\n@@ -1833,20 +3110,3 @@\n-    case Op_SelectFromTwoVector:\n-       if (size_in_bits < 128) {\n-         return false;\n-       }\n-       if ((size_in_bits < 512 && !VM_Version::supports_avx512vl())) {\n-         return false;\n-       }\n-       if (bt == T_SHORT && !VM_Version::supports_avx512bw()) {\n-         return false;\n-       }\n-       if (bt == T_BYTE && !VM_Version::supports_avx512_vbmi()) {\n-         return false;\n-       }\n-       if ((bt == T_INT || bt == T_FLOAT || bt == T_DOUBLE) && !VM_Version::supports_evex()) {\n-         return false;\n-       }\n-       break;\n-    case Op_MaskAll:\n-      if (!VM_Version::supports_evex()) {\n-        return false;\n+    case Op_RoundDoubleModeV:\n+      if (VM_Version::supports_avx() == false) {\n+        return false; \/\/ 128bit vroundpd is not available\n@@ -1854,1 +3114,4 @@\n-      if ((vlen > 16 || is_subword_type(bt)) && !VM_Version::supports_avx512bw()) {\n+      break;\n+    case Op_LoadVectorGather:\n+    case Op_LoadVectorGatherMasked:\n+      if (UseAVX < 2) {\n@@ -1857,1 +3120,6 @@\n-      if (size_in_bits < 512 && !VM_Version::supports_avx512vl()) {\n+      break;\n+    case Op_FmaF:\n+    case Op_FmaD:\n+    case Op_FmaVD:\n+    case Op_FmaVF:\n+      if (!UseFMA) {\n@@ -1861,2 +3129,2 @@\n-    case Op_VectorMaskCmp:\n-      if (vlen < 2 || size_in_bits < 32) {\n+    case Op_MacroLogicV:\n+      if (UseAVX < 3 || !UseVectorMacroLogic) {\n@@ -1866,1 +3134,3 @@\n-    case Op_CompressM:\n+\n+    case Op_VectorCmpMasked:\n+    case Op_VectorMaskGen:\n@@ -1871,3 +3141,14 @@\n-    case Op_CompressV:\n-    case Op_ExpandV:\n-      if (is_subword_type(bt) && !VM_Version::supports_avx512_vbmi2()) {\n+    case Op_VectorMaskFirstTrue:\n+    case Op_VectorMaskLastTrue:\n+    case Op_VectorMaskTrueCount:\n+    case Op_VectorMaskToLong:\n+      if (UseAVX < 1) {\n+         return false;\n+      }\n+      break;\n+    case Op_RoundF:\n+    case Op_RoundD:\n+      break;\n+    case Op_CopySignD:\n+    case Op_CopySignF:\n+      if (UseAVX < 3)  {\n@@ -1876,1 +3157,1 @@\n-      if (size_in_bits < 128 ) {\n+      if (!VM_Version::supports_avx512vl()) {\n@@ -1879,2 +3160,4 @@\n-    case Op_VectorLongToMask:\n-      if (UseAVX < 1) {\n+      break;\n+    case Op_CompressBits:\n+    case Op_ExpandBits:\n+      if (!VM_Version::supports_bmi2()) {\n@@ -1883,1 +3166,3 @@\n-      if (UseAVX < 3 && !VM_Version::supports_bmi2()) {\n+      break;\n+    case Op_CompressM:\n+      if (!VM_Version::supports_avx512vl() || !VM_Version::supports_bmi2()) {\n@@ -1887,3 +3172,3 @@\n-    case Op_SignumVD:\n-    case Op_SignumVF:\n-      if (UseAVX < 1) {\n+    case Op_ConvF2HF:\n+    case Op_ConvHF2F:\n+      if (!VM_Version::supports_float16()) {\n@@ -1893,6 +3178,4 @@\n-    case Op_PopCountVI:\n-    case Op_PopCountVL: {\n-        if (!is_pop_count_instr_target(bt) &&\n-            (size_in_bits == 512) && !VM_Version::supports_avx512bw()) {\n-          return false;\n-        }\n+    case Op_VectorCastF2HF:\n+    case Op_VectorCastHF2F:\n+      if (!VM_Version::supports_f16c() && !VM_Version::supports_evex()) {\n+        return false;\n@@ -1901,3 +3184,53 @@\n-    case Op_ReverseV:\n-    case Op_ReverseBytesV:\n-      if (UseAVX < 2) {\n+  }\n+  return true;  \/\/ Match rules are supported by default.\n+}\n+\n+\/\/------------------------------------------------------------------------\n+\n+static inline bool is_pop_count_instr_target(BasicType bt) {\n+  return (is_subword_type(bt) && VM_Version::supports_avx512_bitalg()) ||\n+         (is_non_subword_integral_type(bt) && VM_Version::supports_avx512_vpopcntdq());\n+}\n+\n+bool Matcher::match_rule_supported_auto_vectorization(int opcode, int vlen, BasicType bt) {\n+  return match_rule_supported_vector(opcode, vlen, bt);\n+}\n+\n+\/\/ Identify extra cases that we might want to provide match rules for vector nodes and\n+\/\/ other intrinsics guarded with vector length (vlen) and element type (bt).\n+bool Matcher::match_rule_supported_vector(int opcode, int vlen, BasicType bt) {\n+  if (!match_rule_supported(opcode)) {\n+    return false;\n+  }\n+  \/\/ Matcher::vector_size_supported() restricts vector sizes in the following way (see Matcher::vector_width_in_bytes):\n+  \/\/   * SSE2 supports 128bit vectors for all types;\n+  \/\/   * AVX1 supports 256bit vectors only for FLOAT and DOUBLE types;\n+  \/\/   * AVX2 supports 256bit vectors for all types;\n+  \/\/   * AVX512F supports 512bit vectors only for INT, FLOAT, and DOUBLE types;\n+  \/\/   * AVX512BW supports 512bit vectors for BYTE, SHORT, and CHAR types.\n+  \/\/ There's also a limit on minimum vector size supported: 2 elements (or 4 bytes for BYTE).\n+  \/\/ And MaxVectorSize is taken into account as well.\n+  if (!vector_size_supported(bt, vlen)) {\n+    return false;\n+  }\n+  \/\/ Special cases which require vector length follow:\n+  \/\/   * implementation limitations\n+  \/\/   * some 512bit vector operations on FLOAT and DOUBLE types require AVX512DQ\n+  \/\/   * 128bit vroundpd instruction is present only in AVX1\n+  int size_in_bits = vlen * type2aelembytes(bt) * BitsPerByte;\n+  switch (opcode) {\n+    case Op_MaxVHF:\n+    case Op_MinVHF:\n+      if (!VM_Version::supports_avx512bw()) {\n+        return false;\n+      }\n+    case Op_AddVHF:\n+    case Op_DivVHF:\n+    case Op_FmaVHF:\n+    case Op_MulVHF:\n+    case Op_SubVHF:\n+    case Op_SqrtVHF:\n+      if (size_in_bits < 512 && !VM_Version::supports_avx512vl()) {\n+        return false;\n+      }\n+      if (!VM_Version::supports_avx512_fp16()) {\n@@ -1907,3 +3240,20 @@\n-    case Op_CountTrailingZerosV:\n-    case Op_CountLeadingZerosV:\n-      if (UseAVX < 2) {\n+    case Op_AbsVF:\n+    case Op_NegVF:\n+      if ((vlen == 16) && (VM_Version::supports_avx512dq() == false)) {\n+        return false; \/\/ 512bit vandps and vxorps are not available\n+      }\n+      break;\n+    case Op_AbsVD:\n+    case Op_NegVD:\n+      if ((vlen == 8) && (VM_Version::supports_avx512dq() == false)) {\n+        return false; \/\/ 512bit vpmullq, vandpd and vxorpd are not available\n+      }\n+      break;\n+    case Op_RotateRightV:\n+    case Op_RotateLeftV:\n+      if (bt != T_INT && bt != T_LONG) {\n+        return false;\n+      } \/\/ fallthrough\n+    case Op_MacroLogicV:\n+      if (!VM_Version::supports_evex() ||\n+          ((size_in_bits != 512) && !VM_Version::supports_avx512vl())) {\n@@ -1913,3 +3263,13239 @@\n-  }\n-  return true;  \/\/ Per default match rules are supported.\n-}\n+    case Op_ClearArray:\n+    case Op_VectorMaskGen:\n+    case Op_VectorCmpMasked:\n+      if (!VM_Version::supports_avx512bw()) {\n+        return false;\n+      }\n+      if ((size_in_bits != 512) && !VM_Version::supports_avx512vl()) {\n+        return false;\n+      }\n+      break;\n+    case Op_LoadVectorMasked:\n+    case Op_StoreVectorMasked:\n+      if (!VM_Version::supports_avx512bw() && (is_subword_type(bt) || UseAVX < 1)) {\n+        return false;\n+      }\n+      break;\n+    case Op_UMinV:\n+    case Op_UMaxV:\n+      if (UseAVX == 0) {\n+        return false;\n+      }\n+      break;\n+    case Op_MaxV:\n+    case Op_MinV:\n+      if (UseSSE < 4 && is_integral_type(bt)) {\n+        return false;\n+      }\n+      if ((bt == T_FLOAT || bt == T_DOUBLE)) {\n+          \/\/ Float\/Double intrinsics are enabled for AVX family currently.\n+          if (UseAVX == 0) {\n+            return false;\n+          }\n+          if (UseAVX > 2 && (!VM_Version::supports_avx512dq() && size_in_bits == 512)) { \/\/ 512 bit Float\/Double intrinsics need AVX512DQ\n+            return false;\n+          }\n+      }\n+      break;\n+    case Op_CallLeafVector:\n+      if (size_in_bits == 512 && !VM_Version::supports_avx512vlbwdq()) {\n+        return false;\n+      }\n+      break;\n+    case Op_AddReductionVI:\n+      if (bt == T_INT && (UseSSE < 3 || !VM_Version::supports_ssse3())) {\n+        return false;\n+      }\n+      \/\/ fallthrough\n+    case Op_AndReductionV:\n+    case Op_OrReductionV:\n+    case Op_XorReductionV:\n+      if (is_subword_type(bt) && (UseSSE < 4)) {\n+        return false;\n+      }\n+      break;\n+    case Op_MinReductionV:\n+    case Op_MaxReductionV:\n+      if ((bt == T_INT || is_subword_type(bt)) && UseSSE < 4) {\n+        return false;\n+      } else if (bt == T_LONG && (UseAVX < 3 || !VM_Version::supports_avx512vlbwdq())) {\n+        return false;\n+      }\n+      \/\/ Float\/Double intrinsics enabled for AVX family.\n+      if (UseAVX == 0 && (bt == T_FLOAT || bt == T_DOUBLE)) {\n+        return false;\n+      }\n+      if (UseAVX > 2 && (!VM_Version::supports_avx512dq() && size_in_bits == 512)) {\n+        return false;\n+      }\n+      break;\n+    case Op_VectorTest:\n+      if (UseSSE < 4) {\n+        return false; \/\/ Implementation limitation\n+      } else if (size_in_bits < 32) {\n+        return false; \/\/ Implementation limitation\n+      }\n+      break;\n+    case Op_VectorLoadShuffle:\n+    case Op_VectorRearrange:\n+      if(vlen == 2) {\n+        return false; \/\/ Implementation limitation due to how shuffle is loaded\n+      } else if (size_in_bits == 256 && UseAVX < 2) {\n+        return false; \/\/ Implementation limitation\n+      }\n+      break;\n+    case Op_VectorLoadMask:\n+    case Op_VectorMaskCast:\n+      if (size_in_bits == 256 && UseAVX < 2) {\n+        return false; \/\/ Implementation limitation\n+      }\n+      \/\/ fallthrough\n+    case Op_VectorStoreMask:\n+      if (vlen == 2) {\n+        return false; \/\/ Implementation limitation\n+      }\n+      break;\n+    case Op_PopulateIndex:\n+      if (size_in_bits > 256 && !VM_Version::supports_avx512bw()) {\n+        return false;\n+      }\n+      break;\n+    case Op_VectorCastB2X:\n+    case Op_VectorCastS2X:\n+    case Op_VectorCastI2X:\n+      if (bt != T_DOUBLE && size_in_bits == 256 && UseAVX < 2) {\n+        return false;\n+      }\n+      break;\n+    case Op_VectorCastL2X:\n+      if (is_integral_type(bt) && size_in_bits == 256 && UseAVX < 2) {\n+        return false;\n+      } else if (!is_integral_type(bt) && !VM_Version::supports_avx512dq()) {\n+        return false;\n+      }\n+      break;\n+    case Op_VectorCastF2X: {\n+        \/\/ As per JLS section 5.1.3 narrowing conversion to sub-word types\n+        \/\/ happen after intermediate conversion to integer and special handling\n+        \/\/ code needs AVX2 vpcmpeqd instruction for 256 bit vectors.\n+        int src_size_in_bits = type2aelembytes(T_FLOAT) * vlen * BitsPerByte;\n+        if (is_integral_type(bt) && src_size_in_bits == 256 && UseAVX < 2) {\n+          return false;\n+        }\n+      }\n+      \/\/ fallthrough\n+    case Op_VectorCastD2X:\n+      if (bt == T_LONG && !VM_Version::supports_avx512dq()) {\n+        return false;\n+      }\n+      break;\n+    case Op_VectorCastF2HF:\n+    case Op_VectorCastHF2F:\n+      if (!VM_Version::supports_f16c() &&\n+         ((!VM_Version::supports_evex() ||\n+         ((size_in_bits != 512) && !VM_Version::supports_avx512vl())))) {\n+        return false;\n+      }\n+      break;\n+    case Op_RoundVD:\n+      if (!VM_Version::supports_avx512dq()) {\n+        return false;\n+      }\n+      break;\n+    case Op_MulReductionVI:\n+      if (bt == T_BYTE && size_in_bits == 512 && !VM_Version::supports_avx512bw()) {\n+        return false;\n+      }\n+      break;\n+    case Op_LoadVectorGatherMasked:\n+      if (!is_subword_type(bt) && size_in_bits < 512 && !VM_Version::supports_avx512vl()) {\n+        return false;\n+      }\n+      if (is_subword_type(bt) &&\n+         ((size_in_bits > 256 && !VM_Version::supports_avx512bw()) ||\n+          (size_in_bits < 64)                                      ||\n+          (bt == T_SHORT && !VM_Version::supports_bmi2()))) {\n+        return false;\n+      }\n+      break;\n+    case Op_StoreVectorScatterMasked:\n+    case Op_StoreVectorScatter:\n+      if (is_subword_type(bt)) {\n+        return false;\n+      } else if (size_in_bits < 512 && !VM_Version::supports_avx512vl()) {\n+        return false;\n+      }\n+      \/\/ fallthrough\n+    case Op_LoadVectorGather:\n+      if (!is_subword_type(bt) && size_in_bits == 64) {\n+        return false;\n+      }\n+      if (is_subword_type(bt) && size_in_bits < 64) {\n+        return false;\n+      }\n+      break;\n+    case Op_SaturatingAddV:\n+    case Op_SaturatingSubV:\n+      if (UseAVX < 1) {\n+        return false; \/\/ Implementation limitation\n+      }\n+      if (is_subword_type(bt) && size_in_bits == 512 && !VM_Version::supports_avx512bw()) {\n+        return false;\n+      }\n+      break;\n+    case Op_SelectFromTwoVector:\n+       if (size_in_bits < 128) {\n+         return false;\n+       }\n+       if (size_in_bits < 512 && !VM_Version::supports_avx512vl()) {\n+         return false;\n+       }\n+       if (bt == T_SHORT && !VM_Version::supports_avx512bw()) {\n+         return false;\n+       }\n+       if (bt == T_BYTE && !VM_Version::supports_avx512_vbmi()) {\n+         return false;\n+       }\n+       if ((bt == T_INT || bt == T_FLOAT || bt == T_DOUBLE) && !VM_Version::supports_evex()) {\n+         return false;\n+       }\n+       break;\n+    case Op_MaskAll:\n+      if (!VM_Version::supports_evex()) {\n+        return false;\n+      }\n+      if ((vlen > 16 || is_subword_type(bt)) && !VM_Version::supports_avx512bw()) {\n+        return false;\n+      }\n+      if (size_in_bits < 512 && !VM_Version::supports_avx512vl()) {\n+        return false;\n+      }\n+      break;\n+    case Op_VectorMaskCmp:\n+      if (vlen < 2 || size_in_bits < 32) {\n+        return false;\n+      }\n+      break;\n+    case Op_CompressM:\n+      if (UseAVX < 3 || !VM_Version::supports_bmi2()) {\n+        return false;\n+      }\n+      break;\n+    case Op_CompressV:\n+    case Op_ExpandV:\n+      if (is_subword_type(bt) && !VM_Version::supports_avx512_vbmi2()) {\n+        return false;\n+      }\n+      if (size_in_bits < 128 ) {\n+        return false;\n+      }\n+    case Op_VectorLongToMask:\n+      if (UseAVX < 1) {\n+        return false;\n+      }\n+      if (UseAVX < 3 && !VM_Version::supports_bmi2()) {\n+        return false;\n+      }\n+      break;\n+    case Op_SignumVD:\n+    case Op_SignumVF:\n+      if (UseAVX < 1) {\n+        return false;\n+      }\n+      break;\n+    case Op_PopCountVI:\n+    case Op_PopCountVL: {\n+        if (!is_pop_count_instr_target(bt) &&\n+            (size_in_bits == 512) && !VM_Version::supports_avx512bw()) {\n+          return false;\n+        }\n+      }\n+      break;\n+    case Op_ReverseV:\n+    case Op_ReverseBytesV:\n+      if (UseAVX < 2) {\n+        return false;\n+      }\n+      break;\n+    case Op_CountTrailingZerosV:\n+    case Op_CountLeadingZerosV:\n+      if (UseAVX < 2) {\n+        return false;\n+      }\n+      break;\n+  }\n+  return true;  \/\/ Per default match rules are supported.\n+}\n+\n+bool Matcher::match_rule_supported_vector_masked(int opcode, int vlen, BasicType bt) {\n+  \/\/ ADLC based match_rule_supported routine checks for the existence of pattern based\n+  \/\/ on IR opcode. Most of the unary\/binary\/ternary masked operation share the IR nodes\n+  \/\/ of their non-masked counterpart with mask edge being the differentiator.\n+  \/\/ This routine does a strict check on the existence of masked operation patterns\n+  \/\/ by returning a default false value for all the other opcodes apart from the\n+  \/\/ ones whose masked instruction patterns are defined in this file.\n+  if (!match_rule_supported_vector(opcode, vlen, bt)) {\n+    return false;\n+  }\n+\n+  int size_in_bits = vlen * type2aelembytes(bt) * BitsPerByte;\n+  if (size_in_bits != 512 && !VM_Version::supports_avx512vl()) {\n+    return false;\n+  }\n+  switch(opcode) {\n+    \/\/ Unary masked operations\n+    case Op_AbsVB:\n+    case Op_AbsVS:\n+      if(!VM_Version::supports_avx512bw()) {\n+        return false;  \/\/ Implementation limitation\n+      }\n+    case Op_AbsVI:\n+    case Op_AbsVL:\n+      return true;\n+\n+    \/\/ Ternary masked operations\n+    case Op_FmaVF:\n+    case Op_FmaVD:\n+      return true;\n+\n+    case Op_MacroLogicV:\n+      if(bt != T_INT && bt != T_LONG) {\n+        return false;\n+      }\n+      return true;\n+\n+    \/\/ Binary masked operations\n+    case Op_AddVB:\n+    case Op_AddVS:\n+    case Op_SubVB:\n+    case Op_SubVS:\n+    case Op_MulVS:\n+    case Op_LShiftVS:\n+    case Op_RShiftVS:\n+    case Op_URShiftVS:\n+      assert(size_in_bits == 512 || VM_Version::supports_avx512vl(), \"\");\n+      if (!VM_Version::supports_avx512bw()) {\n+        return false;  \/\/ Implementation limitation\n+      }\n+      return true;\n+\n+    case Op_MulVL:\n+      assert(size_in_bits == 512 || VM_Version::supports_avx512vl(), \"\");\n+      if (!VM_Version::supports_avx512dq()) {\n+        return false;  \/\/ Implementation limitation\n+      }\n+      return true;\n+\n+    case Op_AndV:\n+    case Op_OrV:\n+    case Op_XorV:\n+    case Op_RotateRightV:\n+    case Op_RotateLeftV:\n+      if (bt != T_INT && bt != T_LONG) {\n+        return false; \/\/ Implementation limitation\n+      }\n+      return true;\n+\n+    case Op_VectorLoadMask:\n+      assert(size_in_bits == 512 || VM_Version::supports_avx512vl(), \"\");\n+      if (is_subword_type(bt) && !VM_Version::supports_avx512bw()) {\n+        return false;\n+      }\n+      return true;\n+\n+    case Op_AddVI:\n+    case Op_AddVL:\n+    case Op_AddVF:\n+    case Op_AddVD:\n+    case Op_SubVI:\n+    case Op_SubVL:\n+    case Op_SubVF:\n+    case Op_SubVD:\n+    case Op_MulVI:\n+    case Op_MulVF:\n+    case Op_MulVD:\n+    case Op_DivVF:\n+    case Op_DivVD:\n+    case Op_SqrtVF:\n+    case Op_SqrtVD:\n+    case Op_LShiftVI:\n+    case Op_LShiftVL:\n+    case Op_RShiftVI:\n+    case Op_RShiftVL:\n+    case Op_URShiftVI:\n+    case Op_URShiftVL:\n+    case Op_LoadVectorMasked:\n+    case Op_StoreVectorMasked:\n+    case Op_LoadVectorGatherMasked:\n+    case Op_StoreVectorScatterMasked:\n+      return true;\n+\n+    case Op_UMinV:\n+    case Op_UMaxV:\n+      if (size_in_bits < 512 && !VM_Version::supports_avx512vl()) {\n+        return false;\n+      } \/\/ fallthrough\n+    case Op_MaxV:\n+    case Op_MinV:\n+      if (is_subword_type(bt) && !VM_Version::supports_avx512bw()) {\n+        return false; \/\/ Implementation limitation\n+      }\n+      if (is_floating_point_type(bt) && !VM_Version::supports_avx10_2()) {\n+        return false; \/\/ Implementation limitation\n+      }\n+      return true;\n+    case Op_SaturatingAddV:\n+    case Op_SaturatingSubV:\n+      if (!is_subword_type(bt)) {\n+        return false;\n+      }\n+      if (size_in_bits < 128 || !VM_Version::supports_avx512bw()) {\n+        return false; \/\/ Implementation limitation\n+      }\n+      return true;\n+\n+    case Op_VectorMaskCmp:\n+      if (is_subword_type(bt) && !VM_Version::supports_avx512bw()) {\n+        return false; \/\/ Implementation limitation\n+      }\n+      return true;\n+\n+    case Op_VectorRearrange:\n+      if (bt == T_SHORT && !VM_Version::supports_avx512bw()) {\n+        return false; \/\/ Implementation limitation\n+      }\n+      if (bt == T_BYTE && !VM_Version::supports_avx512_vbmi()) {\n+        return false; \/\/ Implementation limitation\n+      } else if ((bt == T_INT || bt == T_FLOAT) && size_in_bits < 256) {\n+        return false; \/\/ Implementation limitation\n+      }\n+      return true;\n+\n+    \/\/ Binary Logical operations\n+    case Op_AndVMask:\n+    case Op_OrVMask:\n+    case Op_XorVMask:\n+      if (vlen > 16 && !VM_Version::supports_avx512bw()) {\n+        return false; \/\/ Implementation limitation\n+      }\n+      return true;\n+\n+    case Op_PopCountVI:\n+    case Op_PopCountVL:\n+      if (!is_pop_count_instr_target(bt)) {\n+        return false;\n+      }\n+      return true;\n+\n+    case Op_MaskAll:\n+      return true;\n+\n+    case Op_CountLeadingZerosV:\n+      if (is_non_subword_integral_type(bt) && VM_Version::supports_avx512cd()) {\n+        return true;\n+      }\n+    default:\n+      return false;\n+  }\n+}\n+\n+bool Matcher::vector_needs_partial_operations(Node* node, const TypeVect* vt) {\n+  return false;\n+}\n+\n+\/\/ Return true if Vector::rearrange needs preparation of the shuffle argument\n+bool Matcher::vector_rearrange_requires_load_shuffle(BasicType elem_bt, int vlen) {\n+  switch (elem_bt) {\n+    case T_BYTE:  return false;\n+    case T_SHORT: return !VM_Version::supports_avx512bw();\n+    case T_INT:   return !VM_Version::supports_avx();\n+    case T_LONG:  return vlen < 8 && !VM_Version::supports_avx512vl();\n+    default:\n+      ShouldNotReachHere();\n+      return false;\n+  }\n+}\n+\n+MachOper* Matcher::pd_specialize_generic_vector_operand(MachOper* generic_opnd, uint ideal_reg, bool is_temp) {\n+  assert(Matcher::is_generic_vector(generic_opnd), \"not generic\");\n+  bool legacy = (generic_opnd->opcode() == LEGVEC);\n+  if (!VM_Version::supports_avx512vlbwdq() && \/\/ KNL\n+      is_temp && !legacy && (ideal_reg == Op_VecZ)) {\n+    \/\/ Conservatively specialize 512bit vec TEMP operands to legVecZ (zmm0-15) on KNL.\n+    return new legVecZOper();\n+  }\n+  if (legacy) {\n+    switch (ideal_reg) {\n+      case Op_VecS: return new legVecSOper();\n+      case Op_VecD: return new legVecDOper();\n+      case Op_VecX: return new legVecXOper();\n+      case Op_VecY: return new legVecYOper();\n+      case Op_VecZ: return new legVecZOper();\n+    }\n+  } else {\n+    switch (ideal_reg) {\n+      case Op_VecS: return new vecSOper();\n+      case Op_VecD: return new vecDOper();\n+      case Op_VecX: return new vecXOper();\n+      case Op_VecY: return new vecYOper();\n+      case Op_VecZ: return new vecZOper();\n+    }\n+  }\n+  ShouldNotReachHere();\n+  return nullptr;\n+}\n+\n+bool Matcher::is_reg2reg_move(MachNode* m) {\n+  switch (m->rule()) {\n+    case MoveVec2Leg_rule:\n+    case MoveLeg2Vec_rule:\n+    case MoveF2VL_rule:\n+    case MoveF2LEG_rule:\n+    case MoveVL2F_rule:\n+    case MoveLEG2F_rule:\n+    case MoveD2VL_rule:\n+    case MoveD2LEG_rule:\n+    case MoveVL2D_rule:\n+    case MoveLEG2D_rule:\n+      return true;\n+    default:\n+      return false;\n+  }\n+}\n+\n+bool Matcher::is_generic_vector(MachOper* opnd) {\n+  switch (opnd->opcode()) {\n+    case VEC:\n+    case LEGVEC:\n+      return true;\n+    default:\n+      return false;\n+  }\n+}\n+\n+\/\/------------------------------------------------------------------------\n+\n+const RegMask* Matcher::predicate_reg_mask(void) {\n+  return &_VECTMASK_REG_mask;\n+}\n+\n+\/\/ Max vector size in bytes. 0 if not supported.\n+int Matcher::vector_width_in_bytes(BasicType bt) {\n+  assert(is_java_primitive(bt), \"only primitive type vectors\");\n+  \/\/ SSE2 supports 128bit vectors for all types.\n+  \/\/ AVX2 supports 256bit vectors for all types.\n+  \/\/ AVX2\/EVEX supports 512bit vectors for all types.\n+  int size = (UseAVX > 1) ? (1 << UseAVX) * 8 : 16;\n+  \/\/ AVX1 supports 256bit vectors only for FLOAT and DOUBLE.\n+  if (UseAVX > 0 && (bt == T_FLOAT || bt == T_DOUBLE))\n+    size = (UseAVX > 2) ? 64 : 32;\n+  if (UseAVX > 2 && (bt == T_BYTE || bt == T_SHORT || bt == T_CHAR))\n+    size = (VM_Version::supports_avx512bw()) ? 64 : 32;\n+  \/\/ Use flag to limit vector size.\n+  size = MIN2(size,(int)MaxVectorSize);\n+  \/\/ Minimum 2 values in vector (or 4 for bytes).\n+  switch (bt) {\n+  case T_DOUBLE:\n+  case T_LONG:\n+    if (size < 16) return 0;\n+    break;\n+  case T_FLOAT:\n+  case T_INT:\n+    if (size < 8) return 0;\n+    break;\n+  case T_BOOLEAN:\n+    if (size < 4) return 0;\n+    break;\n+  case T_CHAR:\n+    if (size < 4) return 0;\n+    break;\n+  case T_BYTE:\n+    if (size < 4) return 0;\n+    break;\n+  case T_SHORT:\n+    if (size < 4) return 0;\n+    break;\n+  default:\n+    ShouldNotReachHere();\n+  }\n+  return size;\n+}\n+\n+\/\/ Limits on vector size (number of elements) loaded into vector.\n+int Matcher::max_vector_size(const BasicType bt) {\n+  return vector_width_in_bytes(bt)\/type2aelembytes(bt);\n+}\n+int Matcher::min_vector_size(const BasicType bt) {\n+  int max_size = max_vector_size(bt);\n+  \/\/ Min size which can be loaded into vector is 4 bytes.\n+  int size = (type2aelembytes(bt) == 1) ? 4 : 2;\n+  \/\/ Support for calling svml double64 vectors\n+  if (bt == T_DOUBLE) {\n+    size = 1;\n+  }\n+  return MIN2(size,max_size);\n+}\n+\n+int Matcher::max_vector_size_auto_vectorization(const BasicType bt) {\n+  \/\/ Limit the max vector size for auto vectorization to 256 bits (32 bytes)\n+  \/\/ by default on Cascade Lake\n+  if (VM_Version::is_default_intel_cascade_lake()) {\n+    return MIN2(Matcher::max_vector_size(bt), 32 \/ type2aelembytes(bt));\n+  }\n+  return Matcher::max_vector_size(bt);\n+}\n+\n+int Matcher::scalable_vector_reg_size(const BasicType bt) {\n+  return -1;\n+}\n+\n+\/\/ Vector ideal reg corresponding to specified size in bytes\n+uint Matcher::vector_ideal_reg(int size) {\n+  assert(MaxVectorSize >= size, \"\");\n+  switch(size) {\n+    case  4: return Op_VecS;\n+    case  8: return Op_VecD;\n+    case 16: return Op_VecX;\n+    case 32: return Op_VecY;\n+    case 64: return Op_VecZ;\n+  }\n+  ShouldNotReachHere();\n+  return 0;\n+}\n+\n+\/\/ Check for shift by small constant as well\n+static bool clone_shift(Node* shift, Matcher* matcher, Matcher::MStack& mstack, VectorSet& address_visited) {\n+  if (shift->Opcode() == Op_LShiftX && shift->in(2)->is_Con() &&\n+      shift->in(2)->get_int() <= 3 &&\n+      \/\/ Are there other uses besides address expressions?\n+      !matcher->is_visited(shift)) {\n+    address_visited.set(shift->_idx); \/\/ Flag as address_visited\n+    mstack.push(shift->in(2), Matcher::Visit);\n+    Node *conv = shift->in(1);\n+    \/\/ Allow Matcher to match the rule which bypass\n+    \/\/ ConvI2L operation for an array index on LP64\n+    \/\/ if the index value is positive.\n+    if (conv->Opcode() == Op_ConvI2L &&\n+        conv->as_Type()->type()->is_long()->_lo >= 0 &&\n+        \/\/ Are there other uses besides address expressions?\n+        !matcher->is_visited(conv)) {\n+      address_visited.set(conv->_idx); \/\/ Flag as address_visited\n+      mstack.push(conv->in(1), Matcher::Pre_Visit);\n+    } else {\n+      mstack.push(conv, Matcher::Pre_Visit);\n+    }\n+    return true;\n+  }\n+  return false;\n+}\n+\n+\/\/ This function identifies sub-graphs in which a 'load' node is\n+\/\/ input to two different nodes, and such that it can be matched\n+\/\/ with BMI instructions like blsi, blsr, etc.\n+\/\/ Example : for b = -a[i] & a[i] can be matched to blsi r32, m32.\n+\/\/ The graph is (AndL (SubL Con0 LoadL*) LoadL*), where LoadL*\n+\/\/ refers to the same node.\n+\/\/\n+\/\/ Match the generic fused operations pattern (op1 (op2 Con{ConType} mop) mop)\n+\/\/ This is a temporary solution until we make DAGs expressible in ADL.\n+template<typename ConType>\n+class FusedPatternMatcher {\n+  Node* _op1_node;\n+  Node* _mop_node;\n+  int _con_op;\n+\n+  static int match_next(Node* n, int next_op, int next_op_idx) {\n+    if (n->in(1) == nullptr || n->in(2) == nullptr) {\n+      return -1;\n+    }\n+\n+    if (next_op_idx == -1) { \/\/ n is commutative, try rotations\n+      if (n->in(1)->Opcode() == next_op) {\n+        return 1;\n+      } else if (n->in(2)->Opcode() == next_op) {\n+        return 2;\n+      }\n+    } else {\n+      assert(next_op_idx > 0 && next_op_idx <= 2, \"Bad argument index\");\n+      if (n->in(next_op_idx)->Opcode() == next_op) {\n+        return next_op_idx;\n+      }\n+    }\n+    return -1;\n+  }\n+\n+ public:\n+  FusedPatternMatcher(Node* op1_node, Node* mop_node, int con_op) :\n+    _op1_node(op1_node), _mop_node(mop_node), _con_op(con_op) { }\n+\n+  bool match(int op1, int op1_op2_idx,  \/\/ op1 and the index of the op1->op2 edge, -1 if op1 is commutative\n+             int op2, int op2_con_idx,  \/\/ op2 and the index of the op2->con edge, -1 if op2 is commutative\n+             typename ConType::NativeType con_value) {\n+    if (_op1_node->Opcode() != op1) {\n+      return false;\n+    }\n+    if (_mop_node->outcnt() > 2) {\n+      return false;\n+    }\n+    op1_op2_idx = match_next(_op1_node, op2, op1_op2_idx);\n+    if (op1_op2_idx == -1) {\n+      return false;\n+    }\n+    \/\/ Memory operation must be the other edge\n+    int op1_mop_idx = (op1_op2_idx & 1) + 1;\n+\n+    \/\/ Check that the mop node is really what we want\n+    if (_op1_node->in(op1_mop_idx) == _mop_node) {\n+      Node* op2_node = _op1_node->in(op1_op2_idx);\n+      if (op2_node->outcnt() > 1) {\n+        return false;\n+      }\n+      assert(op2_node->Opcode() == op2, \"Should be\");\n+      op2_con_idx = match_next(op2_node, _con_op, op2_con_idx);\n+      if (op2_con_idx == -1) {\n+        return false;\n+      }\n+      \/\/ Memory operation must be the other edge\n+      int op2_mop_idx = (op2_con_idx & 1) + 1;\n+      \/\/ Check that the memory operation is the same node\n+      if (op2_node->in(op2_mop_idx) == _mop_node) {\n+        \/\/ Now check the constant\n+        const Type* con_type = op2_node->in(op2_con_idx)->bottom_type();\n+        if (con_type != Type::TOP && ConType::as_self(con_type)->get_con() == con_value) {\n+          return true;\n+        }\n+      }\n+    }\n+    return false;\n+  }\n+};\n+\n+static bool is_bmi_pattern(Node* n, Node* m) {\n+  assert(UseBMI1Instructions, \"sanity\");\n+  if (n != nullptr && m != nullptr) {\n+    if (m->Opcode() == Op_LoadI) {\n+      FusedPatternMatcher<TypeInt> bmii(n, m, Op_ConI);\n+      return bmii.match(Op_AndI, -1, Op_SubI,  1,  0)  ||\n+             bmii.match(Op_AndI, -1, Op_AddI, -1, -1)  ||\n+             bmii.match(Op_XorI, -1, Op_AddI, -1, -1);\n+    } else if (m->Opcode() == Op_LoadL) {\n+      FusedPatternMatcher<TypeLong> bmil(n, m, Op_ConL);\n+      return bmil.match(Op_AndL, -1, Op_SubL,  1,  0) ||\n+             bmil.match(Op_AndL, -1, Op_AddL, -1, -1) ||\n+             bmil.match(Op_XorL, -1, Op_AddL, -1, -1);\n+    }\n+  }\n+  return false;\n+}\n+\n+\/\/ Should the matcher clone input 'm' of node 'n'?\n+bool Matcher::pd_clone_node(Node* n, Node* m, Matcher::MStack& mstack) {\n+  \/\/ If 'n' and 'm' are part of a graph for BMI instruction, clone the input 'm'.\n+  if (UseBMI1Instructions && is_bmi_pattern(n, m)) {\n+    mstack.push(m, Visit);\n+    return true;\n+  }\n+  if (is_vshift_con_pattern(n, m)) { \/\/ ShiftV src (ShiftCntV con)\n+    mstack.push(m, Visit);           \/\/ m = ShiftCntV\n+    return true;\n+  }\n+  if (is_encode_and_store_pattern(n, m)) {\n+    mstack.push(m, Visit);\n+    return true;\n+  }\n+  return false;\n+}\n+\n+\/\/ Should the Matcher clone shifts on addressing modes, expecting them\n+\/\/ to be subsumed into complex addressing expressions or compute them\n+\/\/ into registers?\n+bool Matcher::pd_clone_address_expressions(AddPNode* m, Matcher::MStack& mstack, VectorSet& address_visited) {\n+  Node *off = m->in(AddPNode::Offset);\n+  if (off->is_Con()) {\n+    address_visited.test_set(m->_idx); \/\/ Flag as address_visited\n+    Node *adr = m->in(AddPNode::Address);\n+\n+    \/\/ Intel can handle 2 adds in addressing mode, with one of them using an immediate offset.\n+    \/\/ AtomicAdd is not an addressing expression.\n+    \/\/ Cheap to find it by looking for screwy base.\n+    if (adr->is_AddP() &&\n+        !adr->in(AddPNode::Base)->is_top() &&\n+        !adr->in(AddPNode::Offset)->is_Con() &&\n+        off->get_long() == (int) (off->get_long()) && \/\/ immL32\n+        \/\/ Are there other uses besides address expressions?\n+        !is_visited(adr)) {\n+      address_visited.set(adr->_idx); \/\/ Flag as address_visited\n+      Node *shift = adr->in(AddPNode::Offset);\n+      if (!clone_shift(shift, this, mstack, address_visited)) {\n+        mstack.push(shift, Pre_Visit);\n+      }\n+      mstack.push(adr->in(AddPNode::Address), Pre_Visit);\n+      mstack.push(adr->in(AddPNode::Base), Pre_Visit);\n+    } else {\n+      mstack.push(adr, Pre_Visit);\n+    }\n+\n+    \/\/ Clone X+offset as it also folds into most addressing expressions\n+    mstack.push(off, Visit);\n+    mstack.push(m->in(AddPNode::Base), Pre_Visit);\n+    return true;\n+  } else if (clone_shift(off, this, mstack, address_visited)) {\n+    address_visited.test_set(m->_idx); \/\/ Flag as address_visited\n+    mstack.push(m->in(AddPNode::Address), Pre_Visit);\n+    mstack.push(m->in(AddPNode::Base), Pre_Visit);\n+    return true;\n+  }\n+  return false;\n+}\n+\n+static inline Assembler::ComparisonPredicate booltest_pred_to_comparison_pred(int bt) {\n+  switch (bt) {\n+    case BoolTest::eq:\n+      return Assembler::eq;\n+    case BoolTest::ne:\n+      return Assembler::neq;\n+    case BoolTest::le:\n+    case BoolTest::ule:\n+      return Assembler::le;\n+    case BoolTest::ge:\n+    case BoolTest::uge:\n+      return Assembler::nlt;\n+    case BoolTest::lt:\n+    case BoolTest::ult:\n+      return Assembler::lt;\n+    case BoolTest::gt:\n+    case BoolTest::ugt:\n+      return Assembler::nle;\n+    default : ShouldNotReachHere(); return Assembler::_false;\n+  }\n+}\n+\n+static inline Assembler::ComparisonPredicateFP booltest_pred_to_comparison_pred_fp(int bt) {\n+  switch (bt) {\n+  case BoolTest::eq: return Assembler::EQ_OQ;  \/\/ ordered non-signaling\n+  \/\/ As per JLS 15.21.1, != of NaNs is true. Thus use unordered compare.\n+  case BoolTest::ne: return Assembler::NEQ_UQ; \/\/ unordered non-signaling\n+  case BoolTest::le: return Assembler::LE_OQ;  \/\/ ordered non-signaling\n+  case BoolTest::ge: return Assembler::GE_OQ;  \/\/ ordered non-signaling\n+  case BoolTest::lt: return Assembler::LT_OQ;  \/\/ ordered non-signaling\n+  case BoolTest::gt: return Assembler::GT_OQ;  \/\/ ordered non-signaling\n+  default: ShouldNotReachHere(); return Assembler::FALSE_OS;\n+  }\n+}\n+\n+\/\/ Helper methods for MachSpillCopyNode::implementation().\n+static void vec_mov_helper(C2_MacroAssembler *masm, int src_lo, int dst_lo,\n+                          int src_hi, int dst_hi, uint ireg, outputStream* st) {\n+  assert(ireg == Op_VecS || \/\/ 32bit vector\n+         ((src_lo & 1) == 0 && (src_lo + 1) == src_hi &&\n+          (dst_lo & 1) == 0 && (dst_lo + 1) == dst_hi),\n+         \"no non-adjacent vector moves\" );\n+  if (masm) {\n+    switch (ireg) {\n+    case Op_VecS: \/\/ copy whole register\n+    case Op_VecD:\n+    case Op_VecX:\n+      if ((UseAVX < 3) || VM_Version::supports_avx512vl()) {\n+        __ movdqu(as_XMMRegister(Matcher::_regEncode[dst_lo]), as_XMMRegister(Matcher::_regEncode[src_lo]));\n+      } else {\n+        __ vextractf32x4(as_XMMRegister(Matcher::_regEncode[dst_lo]), as_XMMRegister(Matcher::_regEncode[src_lo]), 0x0);\n+     }\n+      break;\n+    case Op_VecY:\n+      if ((UseAVX < 3) || VM_Version::supports_avx512vl()) {\n+        __ vmovdqu(as_XMMRegister(Matcher::_regEncode[dst_lo]), as_XMMRegister(Matcher::_regEncode[src_lo]));\n+      } else {\n+        __ vextractf64x4(as_XMMRegister(Matcher::_regEncode[dst_lo]), as_XMMRegister(Matcher::_regEncode[src_lo]), 0x0);\n+     }\n+      break;\n+    case Op_VecZ:\n+      __ evmovdquq(as_XMMRegister(Matcher::_regEncode[dst_lo]), as_XMMRegister(Matcher::_regEncode[src_lo]), 2);\n+      break;\n+    default:\n+      ShouldNotReachHere();\n+    }\n+#ifndef PRODUCT\n+  } else {\n+    switch (ireg) {\n+    case Op_VecS:\n+    case Op_VecD:\n+    case Op_VecX:\n+      st->print(\"movdqu  %s,%s\\t# spill\",Matcher::regName[dst_lo],Matcher::regName[src_lo]);\n+      break;\n+    case Op_VecY:\n+    case Op_VecZ:\n+      st->print(\"vmovdqu %s,%s\\t# spill\",Matcher::regName[dst_lo],Matcher::regName[src_lo]);\n+      break;\n+    default:\n+      ShouldNotReachHere();\n+    }\n+#endif\n+  }\n+}\n+\n+void vec_spill_helper(C2_MacroAssembler *masm, bool is_load,\n+                     int stack_offset, int reg, uint ireg, outputStream* st) {\n+  if (masm) {\n+    if (is_load) {\n+      switch (ireg) {\n+      case Op_VecS:\n+        __ movdl(as_XMMRegister(Matcher::_regEncode[reg]), Address(rsp, stack_offset));\n+        break;\n+      case Op_VecD:\n+        __ movq(as_XMMRegister(Matcher::_regEncode[reg]), Address(rsp, stack_offset));\n+        break;\n+      case Op_VecX:\n+        if ((UseAVX < 3) || VM_Version::supports_avx512vl()) {\n+          __ movdqu(as_XMMRegister(Matcher::_regEncode[reg]), Address(rsp, stack_offset));\n+        } else {\n+          __ vpxor(as_XMMRegister(Matcher::_regEncode[reg]), as_XMMRegister(Matcher::_regEncode[reg]), as_XMMRegister(Matcher::_regEncode[reg]), 2);\n+          __ vinsertf32x4(as_XMMRegister(Matcher::_regEncode[reg]), as_XMMRegister(Matcher::_regEncode[reg]), Address(rsp, stack_offset),0x0);\n+        }\n+        break;\n+      case Op_VecY:\n+        if ((UseAVX < 3) || VM_Version::supports_avx512vl()) {\n+          __ vmovdqu(as_XMMRegister(Matcher::_regEncode[reg]), Address(rsp, stack_offset));\n+        } else {\n+          __ vpxor(as_XMMRegister(Matcher::_regEncode[reg]), as_XMMRegister(Matcher::_regEncode[reg]), as_XMMRegister(Matcher::_regEncode[reg]), 2);\n+          __ vinsertf64x4(as_XMMRegister(Matcher::_regEncode[reg]), as_XMMRegister(Matcher::_regEncode[reg]), Address(rsp, stack_offset),0x0);\n+        }\n+        break;\n+      case Op_VecZ:\n+        __ evmovdquq(as_XMMRegister(Matcher::_regEncode[reg]), Address(rsp, stack_offset), 2);\n+        break;\n+      default:\n+        ShouldNotReachHere();\n+      }\n+    } else { \/\/ store\n+      switch (ireg) {\n+      case Op_VecS:\n+        __ movdl(Address(rsp, stack_offset), as_XMMRegister(Matcher::_regEncode[reg]));\n+        break;\n+      case Op_VecD:\n+        __ movq(Address(rsp, stack_offset), as_XMMRegister(Matcher::_regEncode[reg]));\n+        break;\n+      case Op_VecX:\n+        if ((UseAVX < 3) || VM_Version::supports_avx512vl()) {\n+          __ movdqu(Address(rsp, stack_offset), as_XMMRegister(Matcher::_regEncode[reg]));\n+        }\n+        else {\n+          __ vextractf32x4(Address(rsp, stack_offset), as_XMMRegister(Matcher::_regEncode[reg]), 0x0);\n+        }\n+        break;\n+      case Op_VecY:\n+        if ((UseAVX < 3) || VM_Version::supports_avx512vl()) {\n+          __ vmovdqu(Address(rsp, stack_offset), as_XMMRegister(Matcher::_regEncode[reg]));\n+        }\n+        else {\n+          __ vextractf64x4(Address(rsp, stack_offset), as_XMMRegister(Matcher::_regEncode[reg]), 0x0);\n+        }\n+        break;\n+      case Op_VecZ:\n+        __ evmovdquq(Address(rsp, stack_offset), as_XMMRegister(Matcher::_regEncode[reg]), 2);\n+        break;\n+      default:\n+        ShouldNotReachHere();\n+      }\n+    }\n+#ifndef PRODUCT\n+  } else {\n+    if (is_load) {\n+      switch (ireg) {\n+      case Op_VecS:\n+        st->print(\"movd    %s,[rsp + %d]\\t# spill\", Matcher::regName[reg], stack_offset);\n+        break;\n+      case Op_VecD:\n+        st->print(\"movq    %s,[rsp + %d]\\t# spill\", Matcher::regName[reg], stack_offset);\n+        break;\n+       case Op_VecX:\n+        st->print(\"movdqu  %s,[rsp + %d]\\t# spill\", Matcher::regName[reg], stack_offset);\n+        break;\n+      case Op_VecY:\n+      case Op_VecZ:\n+        st->print(\"vmovdqu %s,[rsp + %d]\\t# spill\", Matcher::regName[reg], stack_offset);\n+        break;\n+      default:\n+        ShouldNotReachHere();\n+      }\n+    } else { \/\/ store\n+      switch (ireg) {\n+      case Op_VecS:\n+        st->print(\"movd    [rsp + %d],%s\\t# spill\", stack_offset, Matcher::regName[reg]);\n+        break;\n+      case Op_VecD:\n+        st->print(\"movq    [rsp + %d],%s\\t# spill\", stack_offset, Matcher::regName[reg]);\n+        break;\n+       case Op_VecX:\n+        st->print(\"movdqu  [rsp + %d],%s\\t# spill\", stack_offset, Matcher::regName[reg]);\n+        break;\n+      case Op_VecY:\n+      case Op_VecZ:\n+        st->print(\"vmovdqu [rsp + %d],%s\\t# spill\", stack_offset, Matcher::regName[reg]);\n+        break;\n+      default:\n+        ShouldNotReachHere();\n+      }\n+    }\n+#endif\n+  }\n+}\n+\n+template <class T>\n+static inline GrowableArray<jbyte>* vreplicate_imm(BasicType bt, T con, int len) {\n+  int size = type2aelembytes(bt) * len;\n+  GrowableArray<jbyte>* val = new GrowableArray<jbyte>(size, size, 0);\n+  for (int i = 0; i < len; i++) {\n+    int offset = i * type2aelembytes(bt);\n+    switch (bt) {\n+      case T_BYTE: val->at(i) = con; break;\n+      case T_SHORT: {\n+        jshort c = con;\n+        memcpy(val->adr_at(offset), &c, sizeof(jshort));\n+        break;\n+      }\n+      case T_INT: {\n+        jint c = con;\n+        memcpy(val->adr_at(offset), &c, sizeof(jint));\n+        break;\n+      }\n+      case T_LONG: {\n+        jlong c = con;\n+        memcpy(val->adr_at(offset), &c, sizeof(jlong));\n+        break;\n+      }\n+      case T_FLOAT: {\n+        jfloat c = con;\n+        memcpy(val->adr_at(offset), &c, sizeof(jfloat));\n+        break;\n+      }\n+      case T_DOUBLE: {\n+        jdouble c = con;\n+        memcpy(val->adr_at(offset), &c, sizeof(jdouble));\n+        break;\n+      }\n+      default: assert(false, \"%s\", type2name(bt));\n+    }\n+  }\n+  return val;\n+}\n+\n+static inline jlong high_bit_set(BasicType bt) {\n+  switch (bt) {\n+    case T_BYTE:  return 0x8080808080808080;\n+    case T_SHORT: return 0x8000800080008000;\n+    case T_INT:   return 0x8000000080000000;\n+    case T_LONG:  return 0x8000000000000000;\n+    default:\n+      ShouldNotReachHere();\n+      return 0;\n+  }\n+}\n+\n+#ifndef PRODUCT\n+  void MachNopNode::format(PhaseRegAlloc*, outputStream* st) const {\n+    st->print(\"nop \\t# %d bytes pad for loops and calls\", _count);\n+  }\n+#endif\n+\n+  void MachNopNode::emit(C2_MacroAssembler *masm, PhaseRegAlloc*) const {\n+    __ nop(_count);\n+  }\n+\n+  uint MachNopNode::size(PhaseRegAlloc*) const {\n+    return _count;\n+  }\n+\n+#ifndef PRODUCT\n+  void MachBreakpointNode::format(PhaseRegAlloc*, outputStream* st) const {\n+    st->print(\"# breakpoint\");\n+  }\n+#endif\n+\n+  void MachBreakpointNode::emit(C2_MacroAssembler *masm, PhaseRegAlloc* ra_) const {\n+    __ int3();\n+  }\n+\n+  uint MachBreakpointNode::size(PhaseRegAlloc* ra_) const {\n+    return MachNode::size(ra_);\n+  }\n+\n+%}\n+\n+\/\/----------ENCODING BLOCK-----------------------------------------------------\n+\/\/ This block specifies the encoding classes used by the compiler to\n+\/\/ output byte streams.  Encoding classes are parameterized macros\n+\/\/ used by Machine Instruction Nodes in order to generate the bit\n+\/\/ encoding of the instruction.  Operands specify their base encoding\n+\/\/ interface with the interface keyword.  There are currently\n+\/\/ supported four interfaces, REG_INTER, CONST_INTER, MEMORY_INTER, &\n+\/\/ COND_INTER.  REG_INTER causes an operand to generate a function\n+\/\/ which returns its register number when queried.  CONST_INTER causes\n+\/\/ an operand to generate a function which returns the value of the\n+\/\/ constant when queried.  MEMORY_INTER causes an operand to generate\n+\/\/ four functions which return the Base Register, the Index Register,\n+\/\/ the Scale Value, and the Offset Value of the operand when queried.\n+\/\/ COND_INTER causes an operand to generate six functions which return\n+\/\/ the encoding code (ie - encoding bits for the instruction)\n+\/\/ associated with each basic boolean condition for a conditional\n+\/\/ instruction.\n+\/\/\n+\/\/ Instructions specify two basic values for encoding.  Again, a\n+\/\/ function is available to check if the constant displacement is an\n+\/\/ oop. They use the ins_encode keyword to specify their encoding\n+\/\/ classes (which must be a sequence of enc_class names, and their\n+\/\/ parameters, specified in the encoding block), and they use the\n+\/\/ opcode keyword to specify, in order, their primary, secondary, and\n+\/\/ tertiary opcode.  Only the opcode sections which a particular\n+\/\/ instruction needs for encoding need to be specified.\n+encode %{\n+  enc_class cdql_enc(no_rax_rdx_RegI div)\n+  %{\n+    \/\/ Full implementation of Java idiv and irem; checks for\n+    \/\/ special case as described in JVM spec., p.243 & p.271.\n+    \/\/\n+    \/\/         normal case                           special case\n+    \/\/\n+    \/\/ input : rax: dividend                         min_int\n+    \/\/         reg: divisor                          -1\n+    \/\/\n+    \/\/ output: rax: quotient  (= rax idiv reg)       min_int\n+    \/\/         rdx: remainder (= rax irem reg)       0\n+    \/\/\n+    \/\/  Code sequnce:\n+    \/\/\n+    \/\/    0:   3d 00 00 00 80          cmp    $0x80000000,%eax\n+    \/\/    5:   75 07\/08                jne    e <normal>\n+    \/\/    7:   33 d2                   xor    %edx,%edx\n+    \/\/  [div >= 8 -> offset + 1]\n+    \/\/  [REX_B]\n+    \/\/    9:   83 f9 ff                cmp    $0xffffffffffffffff,$div\n+    \/\/    c:   74 03\/04                je     11 <done>\n+    \/\/ 000000000000000e <normal>:\n+    \/\/    e:   99                      cltd\n+    \/\/  [div >= 8 -> offset + 1]\n+    \/\/  [REX_B]\n+    \/\/    f:   f7 f9                   idiv   $div\n+    \/\/ 0000000000000011 <done>:\n+    Label normal;\n+    Label done;\n+\n+    \/\/ cmp    $0x80000000,%eax\n+    __ cmpl(as_Register(RAX_enc), 0x80000000);\n+\n+    \/\/ jne    e <normal>\n+    __ jccb(Assembler::notEqual, normal);\n+\n+    \/\/ xor    %edx,%edx\n+    __ xorl(as_Register(RDX_enc), as_Register(RDX_enc));\n+\n+    \/\/ cmp    $0xffffffffffffffff,%ecx\n+    __ cmpl($div$$Register, -1);\n+\n+    \/\/ je     11 <done>\n+    __ jccb(Assembler::equal, done);\n+\n+    \/\/ <normal>\n+    \/\/ cltd\n+    __ bind(normal);\n+    __ cdql();\n+\n+    \/\/ idivl\n+    \/\/ <done>\n+    __ idivl($div$$Register);\n+    __ bind(done);\n+  %}\n+\n+  enc_class cdqq_enc(no_rax_rdx_RegL div)\n+  %{\n+    \/\/ Full implementation of Java ldiv and lrem; checks for\n+    \/\/ special case as described in JVM spec., p.243 & p.271.\n+    \/\/\n+    \/\/         normal case                           special case\n+    \/\/\n+    \/\/ input : rax: dividend                         min_long\n+    \/\/         reg: divisor                          -1\n+    \/\/\n+    \/\/ output: rax: quotient  (= rax idiv reg)       min_long\n+    \/\/         rdx: remainder (= rax irem reg)       0\n+    \/\/\n+    \/\/  Code sequnce:\n+    \/\/\n+    \/\/    0:   48 ba 00 00 00 00 00    mov    $0x8000000000000000,%rdx\n+    \/\/    7:   00 00 80\n+    \/\/    a:   48 39 d0                cmp    %rdx,%rax\n+    \/\/    d:   75 08                   jne    17 <normal>\n+    \/\/    f:   33 d2                   xor    %edx,%edx\n+    \/\/   11:   48 83 f9 ff             cmp    $0xffffffffffffffff,$div\n+    \/\/   15:   74 05                   je     1c <done>\n+    \/\/ 0000000000000017 <normal>:\n+    \/\/   17:   48 99                   cqto\n+    \/\/   19:   48 f7 f9                idiv   $div\n+    \/\/ 000000000000001c <done>:\n+    Label normal;\n+    Label done;\n+\n+    \/\/ mov    $0x8000000000000000,%rdx\n+    __ mov64(as_Register(RDX_enc), 0x8000000000000000);\n+\n+    \/\/ cmp    %rdx,%rax\n+    __ cmpq(as_Register(RAX_enc), as_Register(RDX_enc));\n+\n+    \/\/ jne    17 <normal>\n+    __ jccb(Assembler::notEqual, normal);\n+\n+    \/\/ xor    %edx,%edx\n+    __ xorl(as_Register(RDX_enc), as_Register(RDX_enc));\n+\n+    \/\/ cmp    $0xffffffffffffffff,$div\n+    __ cmpq($div$$Register, -1);\n+\n+    \/\/ je     1e <done>\n+    __ jccb(Assembler::equal, done);\n+\n+    \/\/ <normal>\n+    \/\/ cqto\n+    __ bind(normal);\n+    __ cdqq();\n+\n+    \/\/ idivq (note: must be emitted by the user of this rule)\n+    \/\/ <done>\n+    __ idivq($div$$Register);\n+    __ bind(done);\n+  %}\n+\n+  enc_class clear_avx %{\n+    DEBUG_ONLY(int off0 = __ offset());\n+    if (generate_vzeroupper(Compile::current())) {\n+      \/\/ Clear upper bits of YMM registers to avoid AVX <-> SSE transition penalty\n+      \/\/ Clear upper bits of YMM registers when current compiled code uses\n+      \/\/ wide vectors to avoid AVX <-> SSE transition penalty during call.\n+      __ vzeroupper();\n+    }\n+    DEBUG_ONLY(int off1 = __ offset());\n+    assert(off1 - off0 == clear_avx_size(), \"correct size prediction\");\n+  %}\n+\n+  enc_class Java_To_Runtime(method meth) %{\n+    __ lea(r10, RuntimeAddress((address)$meth$$method));\n+    __ call(r10);\n+    __ post_call_nop();\n+  %}\n+\n+  enc_class Java_Static_Call(method meth)\n+  %{\n+    \/\/ JAVA STATIC CALL\n+    \/\/ CALL to fixup routine.  Fixup routine uses ScopeDesc info to\n+    \/\/ determine who we intended to call.\n+    if (!_method) {\n+      __ call(RuntimeAddress(CAST_FROM_FN_PTR(address, $meth$$method)));\n+    } else if (_method->intrinsic_id() == vmIntrinsicID::_ensureMaterializedForStackWalk) {\n+      \/\/ The NOP here is purely to ensure that eliding a call to\n+      \/\/ JVM_EnsureMaterializedForStackWalk doesn't change the code size.\n+      __ addr_nop_5();\n+      __ block_comment(\"call JVM_EnsureMaterializedForStackWalk (elided)\");\n+    } else {\n+      int method_index = resolved_method_index(masm);\n+      RelocationHolder rspec = _optimized_virtual ? opt_virtual_call_Relocation::spec(method_index)\n+                                                  : static_call_Relocation::spec(method_index);\n+      address mark = __ pc();\n+      int call_offset = __ offset();\n+      __ call(AddressLiteral(CAST_FROM_FN_PTR(address, $meth$$method), rspec));\n+      if (CodeBuffer::supports_shared_stubs() && _method->can_be_statically_bound()) {\n+        \/\/ Calls of the same statically bound method can share\n+        \/\/ a stub to the interpreter.\n+        __ code()->shared_stub_to_interp_for(_method, call_offset);\n+      } else {\n+        \/\/ Emit stubs for static call.\n+        address stub = CompiledDirectCall::emit_to_interp_stub(masm, mark);\n+        __ clear_inst_mark();\n+        if (stub == nullptr) {\n+          ciEnv::current()->record_failure(\"CodeCache is full\");\n+          return;\n+        }\n+      }\n+    }\n+    __ post_call_nop();\n+  %}\n+\n+  enc_class Java_Dynamic_Call(method meth) %{\n+    __ ic_call((address)$meth$$method, resolved_method_index(masm));\n+    __ post_call_nop();\n+  %}\n+\n+  enc_class call_epilog %{\n+    if (VerifyStackAtCalls) {\n+      \/\/ Check that stack depth is unchanged: find majik cookie on stack\n+      int framesize = ra_->reg2offset_unchecked(OptoReg::add(ra_->_matcher._old_SP, -3*VMRegImpl::slots_per_word));\n+      Label L;\n+      __ cmpptr(Address(rsp, framesize), (int32_t)0xbadb100d);\n+      __ jccb(Assembler::equal, L);\n+      \/\/ Die if stack mismatch\n+      __ int3();\n+      __ bind(L);\n+    }\n+    if (tf()->returns_inline_type_as_fields() && !_method->is_method_handle_intrinsic() && _method->return_type()->is_loaded()) {\n+      \/\/ The last return value is not set by the callee but used to pass the null marker to compiled code.\n+      \/\/ Search for the corresponding projection, get the register and emit code that initialized it.\n+      uint con = (tf()->range_cc()->cnt() - 1);\n+      for (DUIterator_Fast imax, i = fast_outs(imax); i < imax; i++) {\n+        ProjNode* proj = fast_out(i)->as_Proj();\n+        if (proj->_con == con) {\n+          \/\/ Set null marker if rax is non-null (a non-null value is returned buffered or scalarized)\n+          OptoReg::Name optoReg = ra_->get_reg_first(proj);\n+          VMReg reg = OptoReg::as_VMReg(optoReg, ra_->_framesize, OptoReg::reg2stack(ra_->_matcher._new_SP));\n+          Register toReg = reg->is_reg() ? reg->as_Register() : rscratch1;\n+          __ testq(rax, rax);\n+          __ setb(Assembler::notZero, toReg);\n+          __ movzbl(toReg, toReg);\n+          if (reg->is_stack()) {\n+            int st_off = reg->reg2stack() * VMRegImpl::stack_slot_size;\n+            __ movq(Address(rsp, st_off), toReg);\n+          }\n+          break;\n+        }\n+      }\n+      if (return_value_is_used()) {\n+        \/\/ An inline type is returned as fields in multiple registers.\n+        \/\/ Rax either contains an oop if the inline type is buffered or a pointer\n+        \/\/ to the corresponding InlineKlass with the lowest bit set to 1. Zero rax\n+        \/\/ if the lowest bit is set to allow C2 to use the oop after null checking.\n+        \/\/ rax &= (rax & 1) - 1\n+        __ movptr(rscratch1, rax);\n+        __ andptr(rscratch1, 0x1);\n+        __ subptr(rscratch1, 0x1);\n+        __ andptr(rax, rscratch1);\n+      }\n+    }\n+  %}\n+\n+%}\n+\n+\/\/----------FRAME--------------------------------------------------------------\n+\/\/ Definition of frame structure and management information.\n+\/\/\n+\/\/  S T A C K   L A Y O U T    Allocators stack-slot number\n+\/\/                             |   (to get allocators register number\n+\/\/  G  Owned by    |        |  v    add OptoReg::stack0())\n+\/\/  r   CALLER     |        |\n+\/\/  o     |        +--------+      pad to even-align allocators stack-slot\n+\/\/  w     V        |  pad0  |        numbers; owned by CALLER\n+\/\/  t   -----------+--------+----> Matcher::_in_arg_limit, unaligned\n+\/\/  h     ^        |   in   |  5\n+\/\/        |        |  args  |  4   Holes in incoming args owned by SELF\n+\/\/  |     |        |        |  3\n+\/\/  |     |        +--------+\n+\/\/  V     |        | old out|      Empty on Intel, window on Sparc\n+\/\/        |    old |preserve|      Must be even aligned.\n+\/\/        |     SP-+--------+----> Matcher::_old_SP, even aligned\n+\/\/        |        |   in   |  3   area for Intel ret address\n+\/\/     Owned by    |preserve|      Empty on Sparc.\n+\/\/       SELF      +--------+\n+\/\/        |        |  pad2  |  2   pad to align old SP\n+\/\/        |        +--------+  1\n+\/\/        |        | locks  |  0\n+\/\/        |        +--------+----> OptoReg::stack0(), even aligned\n+\/\/        |        |  pad1  | 11   pad to align new SP\n+\/\/        |        +--------+\n+\/\/        |        |        | 10\n+\/\/        |        | spills |  9   spills\n+\/\/        V        |        |  8   (pad0 slot for callee)\n+\/\/      -----------+--------+----> Matcher::_out_arg_limit, unaligned\n+\/\/        ^        |  out   |  7\n+\/\/        |        |  args  |  6   Holes in outgoing args owned by CALLEE\n+\/\/     Owned by    +--------+\n+\/\/      CALLEE     | new out|  6   Empty on Intel, window on Sparc\n+\/\/        |    new |preserve|      Must be even-aligned.\n+\/\/        |     SP-+--------+----> Matcher::_new_SP, even aligned\n+\/\/        |        |        |\n+\/\/\n+\/\/ Note 1: Only region 8-11 is determined by the allocator.  Region 0-5 is\n+\/\/         known from SELF's arguments and the Java calling convention.\n+\/\/         Region 6-7 is determined per call site.\n+\/\/ Note 2: If the calling convention leaves holes in the incoming argument\n+\/\/         area, those holes are owned by SELF.  Holes in the outgoing area\n+\/\/         are owned by the CALLEE.  Holes should not be necessary in the\n+\/\/         incoming area, as the Java calling convention is completely under\n+\/\/         the control of the AD file.  Doubles can be sorted and packed to\n+\/\/         avoid holes.  Holes in the outgoing arguments may be necessary for\n+\/\/         varargs C calling conventions.\n+\/\/ Note 3: Region 0-3 is even aligned, with pad2 as needed.  Region 3-5 is\n+\/\/         even aligned with pad0 as needed.\n+\/\/         Region 6 is even aligned.  Region 6-7 is NOT even aligned;\n+\/\/         region 6-11 is even aligned; it may be padded out more so that\n+\/\/         the region from SP to FP meets the minimum stack alignment.\n+\/\/ Note 4: For I2C adapters, the incoming FP may not meet the minimum stack\n+\/\/         alignment.  Region 11, pad1, may be dynamically extended so that\n+\/\/         SP meets the minimum alignment.\n+\n+frame\n+%{\n+  \/\/ These three registers define part of the calling convention\n+  \/\/ between compiled code and the interpreter.\n+  inline_cache_reg(RAX);                \/\/ Inline Cache Register\n+\n+  \/\/ Optional: name the operand used by cisc-spilling to access\n+  \/\/ [stack_pointer + offset]\n+  cisc_spilling_operand_name(indOffset32);\n+\n+  \/\/ Number of stack slots consumed by locking an object\n+  sync_stack_slots(2);\n+\n+  \/\/ Compiled code's Frame Pointer\n+  frame_pointer(RSP);\n+\n+  \/\/ Interpreter stores its frame pointer in a register which is\n+  \/\/ stored to the stack by I2CAdaptors.\n+  \/\/ I2CAdaptors convert from interpreted java to compiled java.\n+  interpreter_frame_pointer(RBP);\n+\n+  \/\/ Stack alignment requirement\n+  stack_alignment(StackAlignmentInBytes); \/\/ Alignment size in bytes (128-bit -> 16 bytes)\n+\n+  \/\/ Number of outgoing stack slots killed above the out_preserve_stack_slots\n+  \/\/ for calls to C.  Supports the var-args backing area for register parms.\n+  varargs_C_out_slots_killed(frame::arg_reg_save_area_bytes\/BytesPerInt);\n+\n+  \/\/ The after-PROLOG location of the return address.  Location of\n+  \/\/ return address specifies a type (REG or STACK) and a number\n+  \/\/ representing the register number (i.e. - use a register name) or\n+  \/\/ stack slot.\n+  \/\/ Ret Addr is on stack in slot 0 if no locks or verification or alignment.\n+  \/\/ Otherwise, it is above the locks and verification slot and alignment word\n+  return_addr(STACK - 2 +\n+              align_up((Compile::current()->in_preserve_stack_slots() +\n+                        Compile::current()->fixed_slots()),\n+                       stack_alignment_in_slots()));\n+\n+  \/\/ Location of compiled Java return values.  Same as C for now.\n+  return_value\n+  %{\n+    assert(ideal_reg >= Op_RegI && ideal_reg <= Op_RegL,\n+           \"only return normal values\");\n+\n+    static const int lo[Op_RegL + 1] = {\n+      0,\n+      0,\n+      RAX_num,  \/\/ Op_RegN\n+      RAX_num,  \/\/ Op_RegI\n+      RAX_num,  \/\/ Op_RegP\n+      XMM0_num, \/\/ Op_RegF\n+      XMM0_num, \/\/ Op_RegD\n+      RAX_num   \/\/ Op_RegL\n+    };\n+    static const int hi[Op_RegL + 1] = {\n+      0,\n+      0,\n+      OptoReg::Bad, \/\/ Op_RegN\n+      OptoReg::Bad, \/\/ Op_RegI\n+      RAX_H_num,    \/\/ Op_RegP\n+      OptoReg::Bad, \/\/ Op_RegF\n+      XMM0b_num,    \/\/ Op_RegD\n+      RAX_H_num     \/\/ Op_RegL\n+    };\n+    \/\/ Excluded flags and vector registers.\n+    assert(ARRAY_SIZE(hi) == _last_machine_leaf - 8, \"missing type\");\n+    return OptoRegPair(hi[ideal_reg], lo[ideal_reg]);\n+  %}\n+%}\n+\n+\/\/----------ATTRIBUTES---------------------------------------------------------\n+\/\/----------Operand Attributes-------------------------------------------------\n+op_attrib op_cost(0);        \/\/ Required cost attribute\n+\n+\/\/----------Instruction Attributes---------------------------------------------\n+ins_attrib ins_cost(100);       \/\/ Required cost attribute\n+ins_attrib ins_size(8);         \/\/ Required size attribute (in bits)\n+ins_attrib ins_short_branch(0); \/\/ Required flag: is this instruction\n+                                \/\/ a non-matching short branch variant\n+                                \/\/ of some long branch?\n+ins_attrib ins_alignment(1);    \/\/ Required alignment attribute (must\n+                                \/\/ be a power of 2) specifies the\n+                                \/\/ alignment that some part of the\n+                                \/\/ instruction (not necessarily the\n+                                \/\/ start) requires.  If > 1, a\n+                                \/\/ compute_padding() function must be\n+                                \/\/ provided for the instruction\n+\n+\/\/ Whether this node is expanded during code emission into a sequence of\n+\/\/ instructions and the first instruction can perform an implicit null check.\n+ins_attrib ins_is_late_expanded_null_check_candidate(false);\n+\n+\/\/----------OPERANDS-----------------------------------------------------------\n+\/\/ Operand definitions must precede instruction definitions for correct parsing\n+\/\/ in the ADLC because operands constitute user defined types which are used in\n+\/\/ instruction definitions.\n+\n+\/\/----------Simple Operands----------------------------------------------------\n+\/\/ Immediate Operands\n+\/\/ Integer Immediate\n+operand immI()\n+%{\n+  match(ConI);\n+\n+  op_cost(10);\n+  format %{ %}\n+  interface(CONST_INTER);\n+%}\n+\n+\/\/ Constant for test vs zero\n+operand immI_0()\n+%{\n+  predicate(n->get_int() == 0);\n+  match(ConI);\n+\n+  op_cost(0);\n+  format %{ %}\n+  interface(CONST_INTER);\n+%}\n+\n+\/\/ Constant for increment\n+operand immI_1()\n+%{\n+  predicate(n->get_int() == 1);\n+  match(ConI);\n+\n+  op_cost(0);\n+  format %{ %}\n+  interface(CONST_INTER);\n+%}\n+\n+\/\/ Constant for decrement\n+operand immI_M1()\n+%{\n+  predicate(n->get_int() == -1);\n+  match(ConI);\n+\n+  op_cost(0);\n+  format %{ %}\n+  interface(CONST_INTER);\n+%}\n+\n+operand immI_2()\n+%{\n+  predicate(n->get_int() == 2);\n+  match(ConI);\n+\n+  op_cost(0);\n+  format %{ %}\n+  interface(CONST_INTER);\n+%}\n+\n+operand immI_4()\n+%{\n+  predicate(n->get_int() == 4);\n+  match(ConI);\n+\n+  op_cost(0);\n+  format %{ %}\n+  interface(CONST_INTER);\n+%}\n+\n+operand immI_8()\n+%{\n+  predicate(n->get_int() == 8);\n+  match(ConI);\n+\n+  op_cost(0);\n+  format %{ %}\n+  interface(CONST_INTER);\n+%}\n+\n+\/\/ Valid scale values for addressing modes\n+operand immI2()\n+%{\n+  predicate(0 <= n->get_int() && (n->get_int() <= 3));\n+  match(ConI);\n+\n+  format %{ %}\n+  interface(CONST_INTER);\n+%}\n+\n+operand immU7()\n+%{\n+  predicate((0 <= n->get_int()) && (n->get_int() <= 0x7F));\n+  match(ConI);\n+\n+  op_cost(5);\n+  format %{ %}\n+  interface(CONST_INTER);\n+%}\n+\n+operand immI8()\n+%{\n+  predicate((-0x80 <= n->get_int()) && (n->get_int() < 0x80));\n+  match(ConI);\n+\n+  op_cost(5);\n+  format %{ %}\n+  interface(CONST_INTER);\n+%}\n+\n+operand immU8()\n+%{\n+  predicate((0 <= n->get_int()) && (n->get_int() <= 255));\n+  match(ConI);\n+\n+  op_cost(5);\n+  format %{ %}\n+  interface(CONST_INTER);\n+%}\n+\n+operand immI16()\n+%{\n+  predicate((-32768 <= n->get_int()) && (n->get_int() <= 32767));\n+  match(ConI);\n+\n+  op_cost(10);\n+  format %{ %}\n+  interface(CONST_INTER);\n+%}\n+\n+\/\/ Int Immediate non-negative\n+operand immU31()\n+%{\n+  predicate(n->get_int() >= 0);\n+  match(ConI);\n+\n+  op_cost(0);\n+  format %{ %}\n+  interface(CONST_INTER);\n+%}\n+\n+\/\/ Pointer Immediate\n+operand immP()\n+%{\n+  match(ConP);\n+\n+  op_cost(10);\n+  format %{ %}\n+  interface(CONST_INTER);\n+%}\n+\n+\/\/ Null Pointer Immediate\n+operand immP0()\n+%{\n+  predicate(n->get_ptr() == 0);\n+  match(ConP);\n+\n+  op_cost(5);\n+  format %{ %}\n+  interface(CONST_INTER);\n+%}\n+\n+\/\/ Pointer Immediate\n+operand immN() %{\n+  match(ConN);\n+\n+  op_cost(10);\n+  format %{ %}\n+  interface(CONST_INTER);\n+%}\n+\n+operand immNKlass() %{\n+  match(ConNKlass);\n+\n+  op_cost(10);\n+  format %{ %}\n+  interface(CONST_INTER);\n+%}\n+\n+\/\/ Null Pointer Immediate\n+operand immN0() %{\n+  predicate(n->get_narrowcon() == 0);\n+  match(ConN);\n+\n+  op_cost(5);\n+  format %{ %}\n+  interface(CONST_INTER);\n+%}\n+\n+operand immP31()\n+%{\n+  predicate(n->as_Type()->type()->reloc() == relocInfo::none\n+            && (n->get_ptr() >> 31) == 0);\n+  match(ConP);\n+\n+  op_cost(5);\n+  format %{ %}\n+  interface(CONST_INTER);\n+%}\n+\n+\n+\/\/ Long Immediate\n+operand immL()\n+%{\n+  match(ConL);\n+\n+  op_cost(20);\n+  format %{ %}\n+  interface(CONST_INTER);\n+%}\n+\n+\/\/ Long Immediate 8-bit\n+operand immL8()\n+%{\n+  predicate(-0x80L <= n->get_long() && n->get_long() < 0x80L);\n+  match(ConL);\n+\n+  op_cost(5);\n+  format %{ %}\n+  interface(CONST_INTER);\n+%}\n+\n+\/\/ Long Immediate 32-bit unsigned\n+operand immUL32()\n+%{\n+  predicate(n->get_long() == (unsigned int) (n->get_long()));\n+  match(ConL);\n+\n+  op_cost(10);\n+  format %{ %}\n+  interface(CONST_INTER);\n+%}\n+\n+\/\/ Long Immediate 32-bit signed\n+operand immL32()\n+%{\n+  predicate(n->get_long() == (int) (n->get_long()));\n+  match(ConL);\n+\n+  op_cost(15);\n+  format %{ %}\n+  interface(CONST_INTER);\n+%}\n+\n+operand immL_Pow2()\n+%{\n+  predicate(is_power_of_2((julong)n->get_long()));\n+  match(ConL);\n+\n+  op_cost(15);\n+  format %{ %}\n+  interface(CONST_INTER);\n+%}\n+\n+operand immL_NotPow2()\n+%{\n+  predicate(is_power_of_2((julong)~n->get_long()));\n+  match(ConL);\n+\n+  op_cost(15);\n+  format %{ %}\n+  interface(CONST_INTER);\n+%}\n+\n+\/\/ Long Immediate zero\n+operand immL0()\n+%{\n+  predicate(n->get_long() == 0L);\n+  match(ConL);\n+\n+  op_cost(10);\n+  format %{ %}\n+  interface(CONST_INTER);\n+%}\n+\n+\/\/ Constant for increment\n+operand immL1()\n+%{\n+  predicate(n->get_long() == 1);\n+  match(ConL);\n+\n+  format %{ %}\n+  interface(CONST_INTER);\n+%}\n+\n+\/\/ Constant for decrement\n+operand immL_M1()\n+%{\n+  predicate(n->get_long() == -1);\n+  match(ConL);\n+\n+  format %{ %}\n+  interface(CONST_INTER);\n+%}\n+\n+\/\/ Long Immediate: low 32-bit mask\n+operand immL_32bits()\n+%{\n+  predicate(n->get_long() == 0xFFFFFFFFL);\n+  match(ConL);\n+  op_cost(20);\n+\n+  format %{ %}\n+  interface(CONST_INTER);\n+%}\n+\n+\/\/ Int Immediate: 2^n-1, positive\n+operand immI_Pow2M1()\n+%{\n+  predicate((n->get_int() > 0)\n+            && is_power_of_2((juint)n->get_int() + 1));\n+  match(ConI);\n+\n+  op_cost(20);\n+  format %{ %}\n+  interface(CONST_INTER);\n+%}\n+\n+\/\/ Float Immediate zero\n+operand immF0()\n+%{\n+  predicate(jint_cast(n->getf()) == 0);\n+  match(ConF);\n+\n+  op_cost(5);\n+  format %{ %}\n+  interface(CONST_INTER);\n+%}\n+\n+\/\/ Float Immediate\n+operand immF()\n+%{\n+  match(ConF);\n+\n+  op_cost(15);\n+  format %{ %}\n+  interface(CONST_INTER);\n+%}\n+\n+\/\/ Half Float Immediate\n+operand immH()\n+%{\n+  match(ConH);\n+\n+  op_cost(15);\n+  format %{ %}\n+  interface(CONST_INTER);\n+%}\n+\n+\/\/ Double Immediate zero\n+operand immD0()\n+%{\n+  predicate(jlong_cast(n->getd()) == 0);\n+  match(ConD);\n+\n+  op_cost(5);\n+  format %{ %}\n+  interface(CONST_INTER);\n+%}\n+\n+\/\/ Double Immediate\n+operand immD()\n+%{\n+  match(ConD);\n+\n+  op_cost(15);\n+  format %{ %}\n+  interface(CONST_INTER);\n+%}\n+\n+\/\/ Immediates for special shifts (sign extend)\n+\n+\/\/ Constants for increment\n+operand immI_16()\n+%{\n+  predicate(n->get_int() == 16);\n+  match(ConI);\n+\n+  format %{ %}\n+  interface(CONST_INTER);\n+%}\n+\n+operand immI_24()\n+%{\n+  predicate(n->get_int() == 24);\n+  match(ConI);\n+\n+  format %{ %}\n+  interface(CONST_INTER);\n+%}\n+\n+\/\/ Constant for byte-wide masking\n+operand immI_255()\n+%{\n+  predicate(n->get_int() == 255);\n+  match(ConI);\n+\n+  format %{ %}\n+  interface(CONST_INTER);\n+%}\n+\n+\/\/ Constant for short-wide masking\n+operand immI_65535()\n+%{\n+  predicate(n->get_int() == 65535);\n+  match(ConI);\n+\n+  format %{ %}\n+  interface(CONST_INTER);\n+%}\n+\n+\/\/ Constant for byte-wide masking\n+operand immL_255()\n+%{\n+  predicate(n->get_long() == 255);\n+  match(ConL);\n+\n+  format %{ %}\n+  interface(CONST_INTER);\n+%}\n+\n+\/\/ Constant for short-wide masking\n+operand immL_65535()\n+%{\n+  predicate(n->get_long() == 65535);\n+  match(ConL);\n+\n+  format %{ %}\n+  interface(CONST_INTER);\n+%}\n+\n+operand kReg()\n+%{\n+  constraint(ALLOC_IN_RC(vectmask_reg));\n+  match(RegVectMask);\n+  format %{%}\n+  interface(REG_INTER);\n+%}\n+\n+\/\/ Register Operands\n+\/\/ Integer Register\n+operand rRegI()\n+%{\n+  constraint(ALLOC_IN_RC(int_reg));\n+  match(RegI);\n+\n+  match(rax_RegI);\n+  match(rbx_RegI);\n+  match(rcx_RegI);\n+  match(rdx_RegI);\n+  match(rdi_RegI);\n+\n+  format %{ %}\n+  interface(REG_INTER);\n+%}\n+\n+\/\/ Special Registers\n+operand rax_RegI()\n+%{\n+  constraint(ALLOC_IN_RC(int_rax_reg));\n+  match(RegI);\n+  match(rRegI);\n+\n+  format %{ \"RAX\" %}\n+  interface(REG_INTER);\n+%}\n+\n+\/\/ Special Registers\n+operand rbx_RegI()\n+%{\n+  constraint(ALLOC_IN_RC(int_rbx_reg));\n+  match(RegI);\n+  match(rRegI);\n+\n+  format %{ \"RBX\" %}\n+  interface(REG_INTER);\n+%}\n+\n+operand rcx_RegI()\n+%{\n+  constraint(ALLOC_IN_RC(int_rcx_reg));\n+  match(RegI);\n+  match(rRegI);\n+\n+  format %{ \"RCX\" %}\n+  interface(REG_INTER);\n+%}\n+\n+operand rdx_RegI()\n+%{\n+  constraint(ALLOC_IN_RC(int_rdx_reg));\n+  match(RegI);\n+  match(rRegI);\n+\n+  format %{ \"RDX\" %}\n+  interface(REG_INTER);\n+%}\n+\n+operand rdi_RegI()\n+%{\n+  constraint(ALLOC_IN_RC(int_rdi_reg));\n+  match(RegI);\n+  match(rRegI);\n+\n+  format %{ \"RDI\" %}\n+  interface(REG_INTER);\n+%}\n+\n+operand no_rax_rdx_RegI()\n+%{\n+  constraint(ALLOC_IN_RC(int_no_rax_rdx_reg));\n+  match(RegI);\n+  match(rbx_RegI);\n+  match(rcx_RegI);\n+  match(rdi_RegI);\n+\n+  format %{ %}\n+  interface(REG_INTER);\n+%}\n+\n+operand no_rbp_r13_RegI()\n+%{\n+  constraint(ALLOC_IN_RC(int_no_rbp_r13_reg));\n+  match(RegI);\n+  match(rRegI);\n+  match(rax_RegI);\n+  match(rbx_RegI);\n+  match(rcx_RegI);\n+  match(rdx_RegI);\n+  match(rdi_RegI);\n+\n+  format %{ %}\n+  interface(REG_INTER);\n+%}\n+\n+\/\/ Pointer Register\n+operand any_RegP()\n+%{\n+  constraint(ALLOC_IN_RC(any_reg));\n+  match(RegP);\n+  match(rax_RegP);\n+  match(rbx_RegP);\n+  match(rdi_RegP);\n+  match(rsi_RegP);\n+  match(rbp_RegP);\n+  match(r15_RegP);\n+  match(rRegP);\n+\n+  format %{ %}\n+  interface(REG_INTER);\n+%}\n+\n+operand rRegP()\n+%{\n+  constraint(ALLOC_IN_RC(ptr_reg));\n+  match(RegP);\n+  match(rax_RegP);\n+  match(rbx_RegP);\n+  match(rdi_RegP);\n+  match(rsi_RegP);\n+  match(rbp_RegP);  \/\/ See Q&A below about\n+  match(r15_RegP);  \/\/ r15_RegP and rbp_RegP.\n+\n+  format %{ %}\n+  interface(REG_INTER);\n+%}\n+\n+operand rRegN() %{\n+  constraint(ALLOC_IN_RC(int_reg));\n+  match(RegN);\n+\n+  format %{ %}\n+  interface(REG_INTER);\n+%}\n+\n+\/\/ Question: Why is r15_RegP (the read-only TLS register) a match for rRegP?\n+\/\/ Answer: Operand match rules govern the DFA as it processes instruction inputs.\n+\/\/ It's fine for an instruction input that expects rRegP to match a r15_RegP.\n+\/\/ The output of an instruction is controlled by the allocator, which respects\n+\/\/ register class masks, not match rules.  Unless an instruction mentions\n+\/\/ r15_RegP or any_RegP explicitly as its output, r15 will not be considered\n+\/\/ by the allocator as an input.\n+\/\/ The same logic applies to rbp_RegP being a match for rRegP: If PreserveFramePointer==true,\n+\/\/ the RBP is used as a proper frame pointer and is not included in ptr_reg. As a\n+\/\/ result, RBP is not included in the output of the instruction either.\n+\n+\/\/ This operand is not allowed to use RBP even if\n+\/\/ RBP is not used to hold the frame pointer.\n+operand no_rbp_RegP()\n+%{\n+  constraint(ALLOC_IN_RC(ptr_reg_no_rbp));\n+  match(RegP);\n+  match(rbx_RegP);\n+  match(rsi_RegP);\n+  match(rdi_RegP);\n+\n+  format %{ %}\n+  interface(REG_INTER);\n+%}\n+\n+\/\/ Special Registers\n+\/\/ Return a pointer value\n+operand rax_RegP()\n+%{\n+  constraint(ALLOC_IN_RC(ptr_rax_reg));\n+  match(RegP);\n+  match(rRegP);\n+\n+  format %{ %}\n+  interface(REG_INTER);\n+%}\n+\n+\/\/ Special Registers\n+\/\/ Return a compressed pointer value\n+operand rax_RegN()\n+%{\n+  constraint(ALLOC_IN_RC(int_rax_reg));\n+  match(RegN);\n+  match(rRegN);\n+\n+  format %{ %}\n+  interface(REG_INTER);\n+%}\n+\n+\/\/ Used in AtomicAdd\n+operand rbx_RegP()\n+%{\n+  constraint(ALLOC_IN_RC(ptr_rbx_reg));\n+  match(RegP);\n+  match(rRegP);\n+\n+  format %{ %}\n+  interface(REG_INTER);\n+%}\n+\n+operand rsi_RegP()\n+%{\n+  constraint(ALLOC_IN_RC(ptr_rsi_reg));\n+  match(RegP);\n+  match(rRegP);\n+\n+  format %{ %}\n+  interface(REG_INTER);\n+%}\n+\n+operand rbp_RegP()\n+%{\n+  constraint(ALLOC_IN_RC(ptr_rbp_reg));\n+  match(RegP);\n+  match(rRegP);\n+\n+  format %{ %}\n+  interface(REG_INTER);\n+%}\n+\n+\/\/ Used in rep stosq\n+operand rdi_RegP()\n+%{\n+  constraint(ALLOC_IN_RC(ptr_rdi_reg));\n+  match(RegP);\n+  match(rRegP);\n+\n+  format %{ %}\n+  interface(REG_INTER);\n+%}\n+\n+operand r15_RegP()\n+%{\n+  constraint(ALLOC_IN_RC(ptr_r15_reg));\n+  match(RegP);\n+  match(rRegP);\n+\n+  format %{ %}\n+  interface(REG_INTER);\n+%}\n+\n+operand rRegL()\n+%{\n+  constraint(ALLOC_IN_RC(long_reg));\n+  match(RegL);\n+  match(rax_RegL);\n+  match(rdx_RegL);\n+\n+  format %{ %}\n+  interface(REG_INTER);\n+%}\n+\n+\/\/ Special Registers\n+operand no_rax_rdx_RegL()\n+%{\n+  constraint(ALLOC_IN_RC(long_no_rax_rdx_reg));\n+  match(RegL);\n+  match(rRegL);\n+\n+  format %{ %}\n+  interface(REG_INTER);\n+%}\n+\n+operand rax_RegL()\n+%{\n+  constraint(ALLOC_IN_RC(long_rax_reg));\n+  match(RegL);\n+  match(rRegL);\n+\n+  format %{ \"RAX\" %}\n+  interface(REG_INTER);\n+%}\n+\n+operand rcx_RegL()\n+%{\n+  constraint(ALLOC_IN_RC(long_rcx_reg));\n+  match(RegL);\n+  match(rRegL);\n+\n+  format %{ %}\n+  interface(REG_INTER);\n+%}\n+\n+operand rdx_RegL()\n+%{\n+  constraint(ALLOC_IN_RC(long_rdx_reg));\n+  match(RegL);\n+  match(rRegL);\n+\n+  format %{ %}\n+  interface(REG_INTER);\n+%}\n+\n+operand r11_RegL()\n+%{\n+  constraint(ALLOC_IN_RC(long_r11_reg));\n+  match(RegL);\n+  match(rRegL);\n+\n+  format %{ %}\n+  interface(REG_INTER);\n+%}\n+\n+operand no_rbp_r13_RegL()\n+%{\n+  constraint(ALLOC_IN_RC(long_no_rbp_r13_reg));\n+  match(RegL);\n+  match(rRegL);\n+  match(rax_RegL);\n+  match(rcx_RegL);\n+  match(rdx_RegL);\n+\n+  format %{ %}\n+  interface(REG_INTER);\n+%}\n+\n+\/\/ Flags register, used as output of compare instructions\n+operand rFlagsReg()\n+%{\n+  constraint(ALLOC_IN_RC(int_flags));\n+  match(RegFlags);\n+\n+  format %{ \"RFLAGS\" %}\n+  interface(REG_INTER);\n+%}\n+\n+\/\/ Flags register, used as output of FLOATING POINT compare instructions\n+operand rFlagsRegU()\n+%{\n+  constraint(ALLOC_IN_RC(int_flags));\n+  match(RegFlags);\n+\n+  format %{ \"RFLAGS_U\" %}\n+  interface(REG_INTER);\n+%}\n+\n+operand rFlagsRegUCF() %{\n+  constraint(ALLOC_IN_RC(int_flags));\n+  match(RegFlags);\n+  predicate(false);\n+\n+  format %{ \"RFLAGS_U_CF\" %}\n+  interface(REG_INTER);\n+%}\n+\n+\/\/ Float register operands\n+operand regF() %{\n+   constraint(ALLOC_IN_RC(float_reg));\n+   match(RegF);\n+\n+   format %{ %}\n+   interface(REG_INTER);\n+%}\n+\n+\/\/ Float register operands\n+operand legRegF() %{\n+   constraint(ALLOC_IN_RC(float_reg_legacy));\n+   match(RegF);\n+\n+   format %{ %}\n+   interface(REG_INTER);\n+%}\n+\n+\/\/ Float register operands\n+operand vlRegF() %{\n+   constraint(ALLOC_IN_RC(float_reg_vl));\n+   match(RegF);\n+\n+   format %{ %}\n+   interface(REG_INTER);\n+%}\n+\n+\/\/ Double register operands\n+operand regD() %{\n+   constraint(ALLOC_IN_RC(double_reg));\n+   match(RegD);\n+\n+   format %{ %}\n+   interface(REG_INTER);\n+%}\n+\n+\/\/ Double register operands\n+operand legRegD() %{\n+   constraint(ALLOC_IN_RC(double_reg_legacy));\n+   match(RegD);\n+\n+   format %{ %}\n+   interface(REG_INTER);\n+%}\n+\n+\/\/ Double register operands\n+operand vlRegD() %{\n+   constraint(ALLOC_IN_RC(double_reg_vl));\n+   match(RegD);\n+\n+   format %{ %}\n+   interface(REG_INTER);\n+%}\n+\n+\/\/----------Memory Operands----------------------------------------------------\n+\/\/ Direct Memory Operand\n+\/\/ operand direct(immP addr)\n+\/\/ %{\n+\/\/   match(addr);\n+\n+\/\/   format %{ \"[$addr]\" %}\n+\/\/   interface(MEMORY_INTER) %{\n+\/\/     base(0xFFFFFFFF);\n+\/\/     index(0x4);\n+\/\/     scale(0x0);\n+\/\/     disp($addr);\n+\/\/   %}\n+\/\/ %}\n+\n+\/\/ Indirect Memory Operand\n+operand indirect(any_RegP reg)\n+%{\n+  constraint(ALLOC_IN_RC(ptr_reg));\n+  match(reg);\n+\n+  format %{ \"[$reg]\" %}\n+  interface(MEMORY_INTER) %{\n+    base($reg);\n+    index(0x4);\n+    scale(0x0);\n+    disp(0x0);\n+  %}\n+%}\n+\n+\/\/ Indirect Memory Plus Short Offset Operand\n+operand indOffset8(any_RegP reg, immL8 off)\n+%{\n+  constraint(ALLOC_IN_RC(ptr_reg));\n+  match(AddP reg off);\n+\n+  format %{ \"[$reg + $off (8-bit)]\" %}\n+  interface(MEMORY_INTER) %{\n+    base($reg);\n+    index(0x4);\n+    scale(0x0);\n+    disp($off);\n+  %}\n+%}\n+\n+\/\/ Indirect Memory Plus Long Offset Operand\n+operand indOffset32(any_RegP reg, immL32 off)\n+%{\n+  constraint(ALLOC_IN_RC(ptr_reg));\n+  match(AddP reg off);\n+\n+  format %{ \"[$reg + $off (32-bit)]\" %}\n+  interface(MEMORY_INTER) %{\n+    base($reg);\n+    index(0x4);\n+    scale(0x0);\n+    disp($off);\n+  %}\n+%}\n+\n+\/\/ Indirect Memory Plus Index Register Plus Offset Operand\n+operand indIndexOffset(any_RegP reg, rRegL lreg, immL32 off)\n+%{\n+  constraint(ALLOC_IN_RC(ptr_reg));\n+  match(AddP (AddP reg lreg) off);\n+\n+  op_cost(10);\n+  format %{\"[$reg + $off + $lreg]\" %}\n+  interface(MEMORY_INTER) %{\n+    base($reg);\n+    index($lreg);\n+    scale(0x0);\n+    disp($off);\n+  %}\n+%}\n+\n+\/\/ Indirect Memory Plus Index Register Plus Offset Operand\n+operand indIndex(any_RegP reg, rRegL lreg)\n+%{\n+  constraint(ALLOC_IN_RC(ptr_reg));\n+  match(AddP reg lreg);\n+\n+  op_cost(10);\n+  format %{\"[$reg + $lreg]\" %}\n+  interface(MEMORY_INTER) %{\n+    base($reg);\n+    index($lreg);\n+    scale(0x0);\n+    disp(0x0);\n+  %}\n+%}\n+\n+\/\/ Indirect Memory Times Scale Plus Index Register\n+operand indIndexScale(any_RegP reg, rRegL lreg, immI2 scale)\n+%{\n+  constraint(ALLOC_IN_RC(ptr_reg));\n+  match(AddP reg (LShiftL lreg scale));\n+\n+  op_cost(10);\n+  format %{\"[$reg + $lreg << $scale]\" %}\n+  interface(MEMORY_INTER) %{\n+    base($reg);\n+    index($lreg);\n+    scale($scale);\n+    disp(0x0);\n+  %}\n+%}\n+\n+operand indPosIndexScale(any_RegP reg, rRegI idx, immI2 scale)\n+%{\n+  constraint(ALLOC_IN_RC(ptr_reg));\n+  predicate(n->in(3)->in(1)->as_Type()->type()->is_long()->_lo >= 0);\n+  match(AddP reg (LShiftL (ConvI2L idx) scale));\n+\n+  op_cost(10);\n+  format %{\"[$reg + pos $idx << $scale]\" %}\n+  interface(MEMORY_INTER) %{\n+    base($reg);\n+    index($idx);\n+    scale($scale);\n+    disp(0x0);\n+  %}\n+%}\n+\n+\/\/ Indirect Memory Times Scale Plus Index Register Plus Offset Operand\n+operand indIndexScaleOffset(any_RegP reg, immL32 off, rRegL lreg, immI2 scale)\n+%{\n+  constraint(ALLOC_IN_RC(ptr_reg));\n+  match(AddP (AddP reg (LShiftL lreg scale)) off);\n+\n+  op_cost(10);\n+  format %{\"[$reg + $off + $lreg << $scale]\" %}\n+  interface(MEMORY_INTER) %{\n+    base($reg);\n+    index($lreg);\n+    scale($scale);\n+    disp($off);\n+  %}\n+%}\n+\n+\/\/ Indirect Memory Plus Positive Index Register Plus Offset Operand\n+operand indPosIndexOffset(any_RegP reg, immL32 off, rRegI idx)\n+%{\n+  constraint(ALLOC_IN_RC(ptr_reg));\n+  predicate(n->in(2)->in(3)->as_Type()->type()->is_long()->_lo >= 0);\n+  match(AddP (AddP reg (ConvI2L idx)) off);\n+\n+  op_cost(10);\n+  format %{\"[$reg + $off + $idx]\" %}\n+  interface(MEMORY_INTER) %{\n+    base($reg);\n+    index($idx);\n+    scale(0x0);\n+    disp($off);\n+  %}\n+%}\n+\n+\/\/ Indirect Memory Times Scale Plus Positive Index Register Plus Offset Operand\n+operand indPosIndexScaleOffset(any_RegP reg, immL32 off, rRegI idx, immI2 scale)\n+%{\n+  constraint(ALLOC_IN_RC(ptr_reg));\n+  predicate(n->in(2)->in(3)->in(1)->as_Type()->type()->is_long()->_lo >= 0);\n+  match(AddP (AddP reg (LShiftL (ConvI2L idx) scale)) off);\n+\n+  op_cost(10);\n+  format %{\"[$reg + $off + $idx << $scale]\" %}\n+  interface(MEMORY_INTER) %{\n+    base($reg);\n+    index($idx);\n+    scale($scale);\n+    disp($off);\n+  %}\n+%}\n+\n+\/\/ Indirect Narrow Oop Operand\n+operand indCompressedOop(rRegN reg) %{\n+  predicate(UseCompressedOops && (CompressedOops::shift() == Address::times_8));\n+  constraint(ALLOC_IN_RC(ptr_reg));\n+  match(DecodeN reg);\n+\n+  op_cost(10);\n+  format %{\"[R12 + $reg << 3] (compressed oop addressing)\" %}\n+  interface(MEMORY_INTER) %{\n+    base(0xc); \/\/ R12\n+    index($reg);\n+    scale(0x3);\n+    disp(0x0);\n+  %}\n+%}\n+\n+\/\/ Indirect Narrow Oop Plus Offset Operand\n+\/\/ Note: x86 architecture doesn't support \"scale * index + offset\" without a base\n+\/\/ we can't free r12 even with CompressedOops::base() == nullptr.\n+operand indCompressedOopOffset(rRegN reg, immL32 off) %{\n+  predicate(UseCompressedOops && (CompressedOops::shift() == Address::times_8));\n+  constraint(ALLOC_IN_RC(ptr_reg));\n+  match(AddP (DecodeN reg) off);\n+\n+  op_cost(10);\n+  format %{\"[R12 + $reg << 3 + $off] (compressed oop addressing)\" %}\n+  interface(MEMORY_INTER) %{\n+    base(0xc); \/\/ R12\n+    index($reg);\n+    scale(0x3);\n+    disp($off);\n+  %}\n+%}\n+\n+\/\/ Indirect Memory Operand\n+operand indirectNarrow(rRegN reg)\n+%{\n+  predicate(CompressedOops::shift() == 0);\n+  constraint(ALLOC_IN_RC(ptr_reg));\n+  match(DecodeN reg);\n+\n+  format %{ \"[$reg]\" %}\n+  interface(MEMORY_INTER) %{\n+    base($reg);\n+    index(0x4);\n+    scale(0x0);\n+    disp(0x0);\n+  %}\n+%}\n+\n+\/\/ Indirect Memory Plus Short Offset Operand\n+operand indOffset8Narrow(rRegN reg, immL8 off)\n+%{\n+  predicate(CompressedOops::shift() == 0);\n+  constraint(ALLOC_IN_RC(ptr_reg));\n+  match(AddP (DecodeN reg) off);\n+\n+  format %{ \"[$reg + $off (8-bit)]\" %}\n+  interface(MEMORY_INTER) %{\n+    base($reg);\n+    index(0x4);\n+    scale(0x0);\n+    disp($off);\n+  %}\n+%}\n+\n+\/\/ Indirect Memory Plus Long Offset Operand\n+operand indOffset32Narrow(rRegN reg, immL32 off)\n+%{\n+  predicate(CompressedOops::shift() == 0);\n+  constraint(ALLOC_IN_RC(ptr_reg));\n+  match(AddP (DecodeN reg) off);\n+\n+  format %{ \"[$reg + $off (32-bit)]\" %}\n+  interface(MEMORY_INTER) %{\n+    base($reg);\n+    index(0x4);\n+    scale(0x0);\n+    disp($off);\n+  %}\n+%}\n+\n+\/\/ Indirect Memory Plus Index Register Plus Offset Operand\n+operand indIndexOffsetNarrow(rRegN reg, rRegL lreg, immL32 off)\n+%{\n+  predicate(CompressedOops::shift() == 0);\n+  constraint(ALLOC_IN_RC(ptr_reg));\n+  match(AddP (AddP (DecodeN reg) lreg) off);\n+\n+  op_cost(10);\n+  format %{\"[$reg + $off + $lreg]\" %}\n+  interface(MEMORY_INTER) %{\n+    base($reg);\n+    index($lreg);\n+    scale(0x0);\n+    disp($off);\n+  %}\n+%}\n+\n+\/\/ Indirect Memory Plus Index Register Plus Offset Operand\n+operand indIndexNarrow(rRegN reg, rRegL lreg)\n+%{\n+  predicate(CompressedOops::shift() == 0);\n+  constraint(ALLOC_IN_RC(ptr_reg));\n+  match(AddP (DecodeN reg) lreg);\n+\n+  op_cost(10);\n+  format %{\"[$reg + $lreg]\" %}\n+  interface(MEMORY_INTER) %{\n+    base($reg);\n+    index($lreg);\n+    scale(0x0);\n+    disp(0x0);\n+  %}\n+%}\n+\n+\/\/ Indirect Memory Times Scale Plus Index Register\n+operand indIndexScaleNarrow(rRegN reg, rRegL lreg, immI2 scale)\n+%{\n+  predicate(CompressedOops::shift() == 0);\n+  constraint(ALLOC_IN_RC(ptr_reg));\n+  match(AddP (DecodeN reg) (LShiftL lreg scale));\n+\n+  op_cost(10);\n+  format %{\"[$reg + $lreg << $scale]\" %}\n+  interface(MEMORY_INTER) %{\n+    base($reg);\n+    index($lreg);\n+    scale($scale);\n+    disp(0x0);\n+  %}\n+%}\n+\n+\/\/ Indirect Memory Times Scale Plus Index Register Plus Offset Operand\n+operand indIndexScaleOffsetNarrow(rRegN reg, immL32 off, rRegL lreg, immI2 scale)\n+%{\n+  predicate(CompressedOops::shift() == 0);\n+  constraint(ALLOC_IN_RC(ptr_reg));\n+  match(AddP (AddP (DecodeN reg) (LShiftL lreg scale)) off);\n+\n+  op_cost(10);\n+  format %{\"[$reg + $off + $lreg << $scale]\" %}\n+  interface(MEMORY_INTER) %{\n+    base($reg);\n+    index($lreg);\n+    scale($scale);\n+    disp($off);\n+  %}\n+%}\n+\n+\/\/ Indirect Memory Times Plus Positive Index Register Plus Offset Operand\n+operand indPosIndexOffsetNarrow(rRegN reg, immL32 off, rRegI idx)\n+%{\n+  constraint(ALLOC_IN_RC(ptr_reg));\n+  predicate(CompressedOops::shift() == 0 && n->in(2)->in(3)->as_Type()->type()->is_long()->_lo >= 0);\n+  match(AddP (AddP (DecodeN reg) (ConvI2L idx)) off);\n+\n+  op_cost(10);\n+  format %{\"[$reg + $off + $idx]\" %}\n+  interface(MEMORY_INTER) %{\n+    base($reg);\n+    index($idx);\n+    scale(0x0);\n+    disp($off);\n+  %}\n+%}\n+\n+\/\/ Indirect Memory Times Scale Plus Positive Index Register Plus Offset Operand\n+operand indPosIndexScaleOffsetNarrow(rRegN reg, immL32 off, rRegI idx, immI2 scale)\n+%{\n+  constraint(ALLOC_IN_RC(ptr_reg));\n+  predicate(CompressedOops::shift() == 0 && n->in(2)->in(3)->in(1)->as_Type()->type()->is_long()->_lo >= 0);\n+  match(AddP (AddP (DecodeN reg) (LShiftL (ConvI2L idx) scale)) off);\n+\n+  op_cost(10);\n+  format %{\"[$reg + $off + $idx << $scale]\" %}\n+  interface(MEMORY_INTER) %{\n+    base($reg);\n+    index($idx);\n+    scale($scale);\n+    disp($off);\n+  %}\n+%}\n+\n+\/\/----------Special Memory Operands--------------------------------------------\n+\/\/ Stack Slot Operand - This operand is used for loading and storing temporary\n+\/\/                      values on the stack where a match requires a value to\n+\/\/                      flow through memory.\n+operand stackSlotP(sRegP reg)\n+%{\n+  constraint(ALLOC_IN_RC(stack_slots));\n+  \/\/ No match rule because this operand is only generated in matching\n+\n+  format %{ \"[$reg]\" %}\n+  interface(MEMORY_INTER) %{\n+    base(0x4);   \/\/ RSP\n+    index(0x4);  \/\/ No Index\n+    scale(0x0);  \/\/ No Scale\n+    disp($reg);  \/\/ Stack Offset\n+  %}\n+%}\n+\n+operand stackSlotI(sRegI reg)\n+%{\n+  constraint(ALLOC_IN_RC(stack_slots));\n+  \/\/ No match rule because this operand is only generated in matching\n+\n+  format %{ \"[$reg]\" %}\n+  interface(MEMORY_INTER) %{\n+    base(0x4);   \/\/ RSP\n+    index(0x4);  \/\/ No Index\n+    scale(0x0);  \/\/ No Scale\n+    disp($reg);  \/\/ Stack Offset\n+  %}\n+%}\n+\n+operand stackSlotF(sRegF reg)\n+%{\n+  constraint(ALLOC_IN_RC(stack_slots));\n+  \/\/ No match rule because this operand is only generated in matching\n+\n+  format %{ \"[$reg]\" %}\n+  interface(MEMORY_INTER) %{\n+    base(0x4);   \/\/ RSP\n+    index(0x4);  \/\/ No Index\n+    scale(0x0);  \/\/ No Scale\n+    disp($reg);  \/\/ Stack Offset\n+  %}\n+%}\n+\n+operand stackSlotD(sRegD reg)\n+%{\n+  constraint(ALLOC_IN_RC(stack_slots));\n+  \/\/ No match rule because this operand is only generated in matching\n+\n+  format %{ \"[$reg]\" %}\n+  interface(MEMORY_INTER) %{\n+    base(0x4);   \/\/ RSP\n+    index(0x4);  \/\/ No Index\n+    scale(0x0);  \/\/ No Scale\n+    disp($reg);  \/\/ Stack Offset\n+  %}\n+%}\n+operand stackSlotL(sRegL reg)\n+%{\n+  constraint(ALLOC_IN_RC(stack_slots));\n+  \/\/ No match rule because this operand is only generated in matching\n+\n+  format %{ \"[$reg]\" %}\n+  interface(MEMORY_INTER) %{\n+    base(0x4);   \/\/ RSP\n+    index(0x4);  \/\/ No Index\n+    scale(0x0);  \/\/ No Scale\n+    disp($reg);  \/\/ Stack Offset\n+  %}\n+%}\n+\n+\/\/----------Conditional Branch Operands----------------------------------------\n+\/\/ Comparison Op  - This is the operation of the comparison, and is limited to\n+\/\/                  the following set of codes:\n+\/\/                  L (<), LE (<=), G (>), GE (>=), E (==), NE (!=)\n+\/\/\n+\/\/ Other attributes of the comparison, such as unsignedness, are specified\n+\/\/ by the comparison instruction that sets a condition code flags register.\n+\/\/ That result is represented by a flags operand whose subtype is appropriate\n+\/\/ to the unsignedness (etc.) of the comparison.\n+\/\/\n+\/\/ Later, the instruction which matches both the Comparison Op (a Bool) and\n+\/\/ the flags (produced by the Cmp) specifies the coding of the comparison op\n+\/\/ by matching a specific subtype of Bool operand below, such as cmpOpU.\n+\n+\/\/ Comparison Code\n+operand cmpOp()\n+%{\n+  match(Bool);\n+\n+  format %{ \"\" %}\n+  interface(COND_INTER) %{\n+    equal(0x4, \"e\");\n+    not_equal(0x5, \"ne\");\n+    less(0xC, \"l\");\n+    greater_equal(0xD, \"ge\");\n+    less_equal(0xE, \"le\");\n+    greater(0xF, \"g\");\n+    overflow(0x0, \"o\");\n+    no_overflow(0x1, \"no\");\n+  %}\n+%}\n+\n+\/\/ Comparison Code, unsigned compare.  Used by FP also, with\n+\/\/ C2 (unordered) turned into GT or LT already.  The other bits\n+\/\/ C0 and C3 are turned into Carry & Zero flags.\n+operand cmpOpU()\n+%{\n+  match(Bool);\n+\n+  format %{ \"\" %}\n+  interface(COND_INTER) %{\n+    equal(0x4, \"e\");\n+    not_equal(0x5, \"ne\");\n+    less(0x2, \"b\");\n+    greater_equal(0x3, \"ae\");\n+    less_equal(0x6, \"be\");\n+    greater(0x7, \"a\");\n+    overflow(0x0, \"o\");\n+    no_overflow(0x1, \"no\");\n+  %}\n+%}\n+\n+\n+\/\/ Floating comparisons that don't require any fixup for the unordered case,\n+\/\/ If both inputs of the comparison are the same, ZF is always set so we\n+\/\/ don't need to use cmpOpUCF2 for eq\/ne\n+operand cmpOpUCF() %{\n+  match(Bool);\n+  predicate(n->as_Bool()->_test._test == BoolTest::lt ||\n+            n->as_Bool()->_test._test == BoolTest::ge ||\n+            n->as_Bool()->_test._test == BoolTest::le ||\n+            n->as_Bool()->_test._test == BoolTest::gt ||\n+            n->in(1)->in(1) == n->in(1)->in(2));\n+  format %{ \"\" %}\n+  interface(COND_INTER) %{\n+    equal(0xb, \"np\");\n+    not_equal(0xa, \"p\");\n+    less(0x2, \"b\");\n+    greater_equal(0x3, \"ae\");\n+    less_equal(0x6, \"be\");\n+    greater(0x7, \"a\");\n+    overflow(0x0, \"o\");\n+    no_overflow(0x1, \"no\");\n+  %}\n+%}\n+\n+\n+\/\/ Floating comparisons that can be fixed up with extra conditional jumps\n+operand cmpOpUCF2() %{\n+  match(Bool);\n+  predicate((n->as_Bool()->_test._test == BoolTest::ne ||\n+             n->as_Bool()->_test._test == BoolTest::eq) &&\n+            n->in(1)->in(1) != n->in(1)->in(2));\n+  format %{ \"\" %}\n+  interface(COND_INTER) %{\n+    equal(0x4, \"e\");\n+    not_equal(0x5, \"ne\");\n+    less(0x2, \"b\");\n+    greater_equal(0x3, \"ae\");\n+    less_equal(0x6, \"be\");\n+    greater(0x7, \"a\");\n+    overflow(0x0, \"o\");\n+    no_overflow(0x1, \"no\");\n+  %}\n+%}\n+\n+\/\/ Operands for bound floating pointer register arguments\n+operand rxmm0() %{\n+  constraint(ALLOC_IN_RC(xmm0_reg));\n+  match(VecX);\n+  format%{%}\n+  interface(REG_INTER);\n+%}\n+\n+\/\/ Vectors\n+\n+\/\/ Dummy generic vector class. Should be used for all vector operands.\n+\/\/ Replaced with vec[SDXYZ] during post-selection pass.\n+operand vec() %{\n+  constraint(ALLOC_IN_RC(dynamic));\n+  match(VecX);\n+  match(VecY);\n+  match(VecZ);\n+  match(VecS);\n+  match(VecD);\n+\n+  format %{ %}\n+  interface(REG_INTER);\n+%}\n+\n+\/\/ Dummy generic legacy vector class. Should be used for all legacy vector operands.\n+\/\/ Replaced with legVec[SDXYZ] during post-selection cleanup.\n+\/\/ Note: legacy register class is used to avoid extra (unneeded in 32-bit VM)\n+\/\/ runtime code generation via reg_class_dynamic.\n+operand legVec() %{\n+  constraint(ALLOC_IN_RC(dynamic));\n+  match(VecX);\n+  match(VecY);\n+  match(VecZ);\n+  match(VecS);\n+  match(VecD);\n+\n+  format %{ %}\n+  interface(REG_INTER);\n+%}\n+\n+\/\/ Replaces vec during post-selection cleanup. See above.\n+operand vecS() %{\n+  constraint(ALLOC_IN_RC(vectors_reg_vlbwdq));\n+  match(VecS);\n+\n+  format %{ %}\n+  interface(REG_INTER);\n+%}\n+\n+\/\/ Replaces legVec during post-selection cleanup. See above.\n+operand legVecS() %{\n+  constraint(ALLOC_IN_RC(vectors_reg_legacy));\n+  match(VecS);\n+\n+  format %{ %}\n+  interface(REG_INTER);\n+%}\n+\n+\/\/ Replaces vec during post-selection cleanup. See above.\n+operand vecD() %{\n+  constraint(ALLOC_IN_RC(vectord_reg_vlbwdq));\n+  match(VecD);\n+\n+  format %{ %}\n+  interface(REG_INTER);\n+%}\n+\n+\/\/ Replaces legVec during post-selection cleanup. See above.\n+operand legVecD() %{\n+  constraint(ALLOC_IN_RC(vectord_reg_legacy));\n+  match(VecD);\n+\n+  format %{ %}\n+  interface(REG_INTER);\n+%}\n+\n+\/\/ Replaces vec during post-selection cleanup. See above.\n+operand vecX() %{\n+  constraint(ALLOC_IN_RC(vectorx_reg_vlbwdq));\n+  match(VecX);\n+\n+  format %{ %}\n+  interface(REG_INTER);\n+%}\n+\n+\/\/ Replaces legVec during post-selection cleanup. See above.\n+operand legVecX() %{\n+  constraint(ALLOC_IN_RC(vectorx_reg_legacy));\n+  match(VecX);\n+\n+  format %{ %}\n+  interface(REG_INTER);\n+%}\n+\n+\/\/ Replaces vec during post-selection cleanup. See above.\n+operand vecY() %{\n+  constraint(ALLOC_IN_RC(vectory_reg_vlbwdq));\n+  match(VecY);\n+\n+  format %{ %}\n+  interface(REG_INTER);\n+%}\n+\n+\/\/ Replaces legVec during post-selection cleanup. See above.\n+operand legVecY() %{\n+  constraint(ALLOC_IN_RC(vectory_reg_legacy));\n+  match(VecY);\n+\n+  format %{ %}\n+  interface(REG_INTER);\n+%}\n+\n+\/\/ Replaces vec during post-selection cleanup. See above.\n+operand vecZ() %{\n+  constraint(ALLOC_IN_RC(vectorz_reg));\n+  match(VecZ);\n+\n+  format %{ %}\n+  interface(REG_INTER);\n+%}\n+\n+\/\/ Replaces legVec during post-selection cleanup. See above.\n+operand legVecZ() %{\n+  constraint(ALLOC_IN_RC(vectorz_reg_legacy));\n+  match(VecZ);\n+\n+  format %{ %}\n+  interface(REG_INTER);\n+%}\n+\n+\/\/----------OPERAND CLASSES----------------------------------------------------\n+\/\/ Operand Classes are groups of operands that are used as to simplify\n+\/\/ instruction definitions by not requiring the AD writer to specify separate\n+\/\/ instructions for every form of operand when the instruction accepts\n+\/\/ multiple operand types with the same basic encoding and format.  The classic\n+\/\/ case of this is memory operands.\n+\n+opclass memory(indirect, indOffset8, indOffset32, indIndexOffset, indIndex,\n+               indIndexScale, indPosIndexScale, indIndexScaleOffset, indPosIndexOffset, indPosIndexScaleOffset,\n+               indCompressedOop, indCompressedOopOffset,\n+               indirectNarrow, indOffset8Narrow, indOffset32Narrow,\n+               indIndexOffsetNarrow, indIndexNarrow, indIndexScaleNarrow,\n+               indIndexScaleOffsetNarrow, indPosIndexOffsetNarrow, indPosIndexScaleOffsetNarrow);\n+\n+\/\/----------PIPELINE-----------------------------------------------------------\n+\/\/ Rules which define the behavior of the target architectures pipeline.\n+pipeline %{\n+\n+\/\/----------ATTRIBUTES---------------------------------------------------------\n+attributes %{\n+  variable_size_instructions;        \/\/ Fixed size instructions\n+  max_instructions_per_bundle = 3;   \/\/ Up to 3 instructions per bundle\n+  instruction_unit_size = 1;         \/\/ An instruction is 1 bytes long\n+  instruction_fetch_unit_size = 16;  \/\/ The processor fetches one line\n+  instruction_fetch_units = 1;       \/\/ of 16 bytes\n+%}\n+\n+\/\/----------RESOURCES----------------------------------------------------------\n+\/\/ Resources are the functional units available to the machine\n+\n+\/\/ Generic P2\/P3 pipeline\n+\/\/ 3 decoders, only D0 handles big operands; a \"bundle\" is the limit of\n+\/\/ 3 instructions decoded per cycle.\n+\/\/ 2 load\/store ops per cycle, 1 branch, 1 FPU,\n+\/\/ 3 ALU op, only ALU0 handles mul instructions.\n+resources( D0, D1, D2, DECODE = D0 | D1 | D2,\n+           MS0, MS1, MS2, MEM = MS0 | MS1 | MS2,\n+           BR, FPU,\n+           ALU0, ALU1, ALU2, ALU = ALU0 | ALU1 | ALU2);\n+\n+\/\/----------PIPELINE DESCRIPTION-----------------------------------------------\n+\/\/ Pipeline Description specifies the stages in the machine's pipeline\n+\n+\/\/ Generic P2\/P3 pipeline\n+pipe_desc(S0, S1, S2, S3, S4, S5);\n+\n+\/\/----------PIPELINE CLASSES---------------------------------------------------\n+\/\/ Pipeline Classes describe the stages in which input and output are\n+\/\/ referenced by the hardware pipeline.\n+\n+\/\/ Naming convention: ialu or fpu\n+\/\/ Then: _reg\n+\/\/ Then: _reg if there is a 2nd register\n+\/\/ Then: _long if it's a pair of instructions implementing a long\n+\/\/ Then: _fat if it requires the big decoder\n+\/\/   Or: _mem if it requires the big decoder and a memory unit.\n+\n+\/\/ Integer ALU reg operation\n+pipe_class ialu_reg(rRegI dst)\n+%{\n+    single_instruction;\n+    dst    : S4(write);\n+    dst    : S3(read);\n+    DECODE : S0;        \/\/ any decoder\n+    ALU    : S3;        \/\/ any alu\n+%}\n+\n+\/\/ Long ALU reg operation\n+pipe_class ialu_reg_long(rRegL dst)\n+%{\n+    instruction_count(2);\n+    dst    : S4(write);\n+    dst    : S3(read);\n+    DECODE : S0(2);     \/\/ any 2 decoders\n+    ALU    : S3(2);     \/\/ both alus\n+%}\n+\n+\/\/ Integer ALU reg operation using big decoder\n+pipe_class ialu_reg_fat(rRegI dst)\n+%{\n+    single_instruction;\n+    dst    : S4(write);\n+    dst    : S3(read);\n+    D0     : S0;        \/\/ big decoder only\n+    ALU    : S3;        \/\/ any alu\n+%}\n+\n+\/\/ Integer ALU reg-reg operation\n+pipe_class ialu_reg_reg(rRegI dst, rRegI src)\n+%{\n+    single_instruction;\n+    dst    : S4(write);\n+    src    : S3(read);\n+    DECODE : S0;        \/\/ any decoder\n+    ALU    : S3;        \/\/ any alu\n+%}\n+\n+\/\/ Integer ALU reg-reg operation\n+pipe_class ialu_reg_reg_fat(rRegI dst, memory src)\n+%{\n+    single_instruction;\n+    dst    : S4(write);\n+    src    : S3(read);\n+    D0     : S0;        \/\/ big decoder only\n+    ALU    : S3;        \/\/ any alu\n+%}\n+\n+\/\/ Integer ALU reg-mem operation\n+pipe_class ialu_reg_mem(rRegI dst, memory mem)\n+%{\n+    single_instruction;\n+    dst    : S5(write);\n+    mem    : S3(read);\n+    D0     : S0;        \/\/ big decoder only\n+    ALU    : S4;        \/\/ any alu\n+    MEM    : S3;        \/\/ any mem\n+%}\n+\n+\/\/ Integer mem operation (prefetch)\n+pipe_class ialu_mem(memory mem)\n+%{\n+    single_instruction;\n+    mem    : S3(read);\n+    D0     : S0;        \/\/ big decoder only\n+    MEM    : S3;        \/\/ any mem\n+%}\n+\n+\/\/ Integer Store to Memory\n+pipe_class ialu_mem_reg(memory mem, rRegI src)\n+%{\n+    single_instruction;\n+    mem    : S3(read);\n+    src    : S5(read);\n+    D0     : S0;        \/\/ big decoder only\n+    ALU    : S4;        \/\/ any alu\n+    MEM    : S3;\n+%}\n+\n+\/\/ \/\/ Long Store to Memory\n+\/\/ pipe_class ialu_mem_long_reg(memory mem, rRegL src)\n+\/\/ %{\n+\/\/     instruction_count(2);\n+\/\/     mem    : S3(read);\n+\/\/     src    : S5(read);\n+\/\/     D0     : S0(2);          \/\/ big decoder only; twice\n+\/\/     ALU    : S4(2);     \/\/ any 2 alus\n+\/\/     MEM    : S3(2);  \/\/ Both mems\n+\/\/ %}\n+\n+\/\/ Integer Store to Memory\n+pipe_class ialu_mem_imm(memory mem)\n+%{\n+    single_instruction;\n+    mem    : S3(read);\n+    D0     : S0;        \/\/ big decoder only\n+    ALU    : S4;        \/\/ any alu\n+    MEM    : S3;\n+%}\n+\n+\/\/ Integer ALU0 reg-reg operation\n+pipe_class ialu_reg_reg_alu0(rRegI dst, rRegI src)\n+%{\n+    single_instruction;\n+    dst    : S4(write);\n+    src    : S3(read);\n+    D0     : S0;        \/\/ Big decoder only\n+    ALU0   : S3;        \/\/ only alu0\n+%}\n+\n+\/\/ Integer ALU0 reg-mem operation\n+pipe_class ialu_reg_mem_alu0(rRegI dst, memory mem)\n+%{\n+    single_instruction;\n+    dst    : S5(write);\n+    mem    : S3(read);\n+    D0     : S0;        \/\/ big decoder only\n+    ALU0   : S4;        \/\/ ALU0 only\n+    MEM    : S3;        \/\/ any mem\n+%}\n+\n+\/\/ Integer ALU reg-reg operation\n+pipe_class ialu_cr_reg_reg(rFlagsReg cr, rRegI src1, rRegI src2)\n+%{\n+    single_instruction;\n+    cr     : S4(write);\n+    src1   : S3(read);\n+    src2   : S3(read);\n+    DECODE : S0;        \/\/ any decoder\n+    ALU    : S3;        \/\/ any alu\n+%}\n+\n+\/\/ Integer ALU reg-imm operation\n+pipe_class ialu_cr_reg_imm(rFlagsReg cr, rRegI src1)\n+%{\n+    single_instruction;\n+    cr     : S4(write);\n+    src1   : S3(read);\n+    DECODE : S0;        \/\/ any decoder\n+    ALU    : S3;        \/\/ any alu\n+%}\n+\n+\/\/ Integer ALU reg-mem operation\n+pipe_class ialu_cr_reg_mem(rFlagsReg cr, rRegI src1, memory src2)\n+%{\n+    single_instruction;\n+    cr     : S4(write);\n+    src1   : S3(read);\n+    src2   : S3(read);\n+    D0     : S0;        \/\/ big decoder only\n+    ALU    : S4;        \/\/ any alu\n+    MEM    : S3;\n+%}\n+\n+\/\/ Conditional move reg-reg\n+pipe_class pipe_cmplt( rRegI p, rRegI q, rRegI y)\n+%{\n+    instruction_count(4);\n+    y      : S4(read);\n+    q      : S3(read);\n+    p      : S3(read);\n+    DECODE : S0(4);     \/\/ any decoder\n+%}\n+\n+\/\/ Conditional move reg-reg\n+pipe_class pipe_cmov_reg( rRegI dst, rRegI src, rFlagsReg cr)\n+%{\n+    single_instruction;\n+    dst    : S4(write);\n+    src    : S3(read);\n+    cr     : S3(read);\n+    DECODE : S0;        \/\/ any decoder\n+%}\n+\n+\/\/ Conditional move reg-mem\n+pipe_class pipe_cmov_mem( rFlagsReg cr, rRegI dst, memory src)\n+%{\n+    single_instruction;\n+    dst    : S4(write);\n+    src    : S3(read);\n+    cr     : S3(read);\n+    DECODE : S0;        \/\/ any decoder\n+    MEM    : S3;\n+%}\n+\n+\/\/ Conditional move reg-reg long\n+pipe_class pipe_cmov_reg_long( rFlagsReg cr, rRegL dst, rRegL src)\n+%{\n+    single_instruction;\n+    dst    : S4(write);\n+    src    : S3(read);\n+    cr     : S3(read);\n+    DECODE : S0(2);     \/\/ any 2 decoders\n+%}\n+\n+\/\/ Float reg-reg operation\n+pipe_class fpu_reg(regD dst)\n+%{\n+    instruction_count(2);\n+    dst    : S3(read);\n+    DECODE : S0(2);     \/\/ any 2 decoders\n+    FPU    : S3;\n+%}\n+\n+\/\/ Float reg-reg operation\n+pipe_class fpu_reg_reg(regD dst, regD src)\n+%{\n+    instruction_count(2);\n+    dst    : S4(write);\n+    src    : S3(read);\n+    DECODE : S0(2);     \/\/ any 2 decoders\n+    FPU    : S3;\n+%}\n+\n+\/\/ Float reg-reg operation\n+pipe_class fpu_reg_reg_reg(regD dst, regD src1, regD src2)\n+%{\n+    instruction_count(3);\n+    dst    : S4(write);\n+    src1   : S3(read);\n+    src2   : S3(read);\n+    DECODE : S0(3);     \/\/ any 3 decoders\n+    FPU    : S3(2);\n+%}\n+\n+\/\/ Float reg-reg operation\n+pipe_class fpu_reg_reg_reg_reg(regD dst, regD src1, regD src2, regD src3)\n+%{\n+    instruction_count(4);\n+    dst    : S4(write);\n+    src1   : S3(read);\n+    src2   : S3(read);\n+    src3   : S3(read);\n+    DECODE : S0(4);     \/\/ any 3 decoders\n+    FPU    : S3(2);\n+%}\n+\n+\/\/ Float reg-reg operation\n+pipe_class fpu_reg_mem_reg_reg(regD dst, memory src1, regD src2, regD src3)\n+%{\n+    instruction_count(4);\n+    dst    : S4(write);\n+    src1   : S3(read);\n+    src2   : S3(read);\n+    src3   : S3(read);\n+    DECODE : S1(3);     \/\/ any 3 decoders\n+    D0     : S0;        \/\/ Big decoder only\n+    FPU    : S3(2);\n+    MEM    : S3;\n+%}\n+\n+\/\/ Float reg-mem operation\n+pipe_class fpu_reg_mem(regD dst, memory mem)\n+%{\n+    instruction_count(2);\n+    dst    : S5(write);\n+    mem    : S3(read);\n+    D0     : S0;        \/\/ big decoder only\n+    DECODE : S1;        \/\/ any decoder for FPU POP\n+    FPU    : S4;\n+    MEM    : S3;        \/\/ any mem\n+%}\n+\n+\/\/ Float reg-mem operation\n+pipe_class fpu_reg_reg_mem(regD dst, regD src1, memory mem)\n+%{\n+    instruction_count(3);\n+    dst    : S5(write);\n+    src1   : S3(read);\n+    mem    : S3(read);\n+    D0     : S0;        \/\/ big decoder only\n+    DECODE : S1(2);     \/\/ any decoder for FPU POP\n+    FPU    : S4;\n+    MEM    : S3;        \/\/ any mem\n+%}\n+\n+\/\/ Float mem-reg operation\n+pipe_class fpu_mem_reg(memory mem, regD src)\n+%{\n+    instruction_count(2);\n+    src    : S5(read);\n+    mem    : S3(read);\n+    DECODE : S0;        \/\/ any decoder for FPU PUSH\n+    D0     : S1;        \/\/ big decoder only\n+    FPU    : S4;\n+    MEM    : S3;        \/\/ any mem\n+%}\n+\n+pipe_class fpu_mem_reg_reg(memory mem, regD src1, regD src2)\n+%{\n+    instruction_count(3);\n+    src1   : S3(read);\n+    src2   : S3(read);\n+    mem    : S3(read);\n+    DECODE : S0(2);     \/\/ any decoder for FPU PUSH\n+    D0     : S1;        \/\/ big decoder only\n+    FPU    : S4;\n+    MEM    : S3;        \/\/ any mem\n+%}\n+\n+pipe_class fpu_mem_reg_mem(memory mem, regD src1, memory src2)\n+%{\n+    instruction_count(3);\n+    src1   : S3(read);\n+    src2   : S3(read);\n+    mem    : S4(read);\n+    DECODE : S0;        \/\/ any decoder for FPU PUSH\n+    D0     : S0(2);     \/\/ big decoder only\n+    FPU    : S4;\n+    MEM    : S3(2);     \/\/ any mem\n+%}\n+\n+pipe_class fpu_mem_mem(memory dst, memory src1)\n+%{\n+    instruction_count(2);\n+    src1   : S3(read);\n+    dst    : S4(read);\n+    D0     : S0(2);     \/\/ big decoder only\n+    MEM    : S3(2);     \/\/ any mem\n+%}\n+\n+pipe_class fpu_mem_mem_mem(memory dst, memory src1, memory src2)\n+%{\n+    instruction_count(3);\n+    src1   : S3(read);\n+    src2   : S3(read);\n+    dst    : S4(read);\n+    D0     : S0(3);     \/\/ big decoder only\n+    FPU    : S4;\n+    MEM    : S3(3);     \/\/ any mem\n+%}\n+\n+pipe_class fpu_mem_reg_con(memory mem, regD src1)\n+%{\n+    instruction_count(3);\n+    src1   : S4(read);\n+    mem    : S4(read);\n+    DECODE : S0;        \/\/ any decoder for FPU PUSH\n+    D0     : S0(2);     \/\/ big decoder only\n+    FPU    : S4;\n+    MEM    : S3(2);     \/\/ any mem\n+%}\n+\n+\/\/ Float load constant\n+pipe_class fpu_reg_con(regD dst)\n+%{\n+    instruction_count(2);\n+    dst    : S5(write);\n+    D0     : S0;        \/\/ big decoder only for the load\n+    DECODE : S1;        \/\/ any decoder for FPU POP\n+    FPU    : S4;\n+    MEM    : S3;        \/\/ any mem\n+%}\n+\n+\/\/ Float load constant\n+pipe_class fpu_reg_reg_con(regD dst, regD src)\n+%{\n+    instruction_count(3);\n+    dst    : S5(write);\n+    src    : S3(read);\n+    D0     : S0;        \/\/ big decoder only for the load\n+    DECODE : S1(2);     \/\/ any decoder for FPU POP\n+    FPU    : S4;\n+    MEM    : S3;        \/\/ any mem\n+%}\n+\n+\/\/ UnConditional branch\n+pipe_class pipe_jmp(label labl)\n+%{\n+    single_instruction;\n+    BR   : S3;\n+%}\n+\n+\/\/ Conditional branch\n+pipe_class pipe_jcc(cmpOp cmp, rFlagsReg cr, label labl)\n+%{\n+    single_instruction;\n+    cr    : S1(read);\n+    BR    : S3;\n+%}\n+\n+\/\/ Allocation idiom\n+pipe_class pipe_cmpxchg(rRegP dst, rRegP heap_ptr)\n+%{\n+    instruction_count(1); force_serialization;\n+    fixed_latency(6);\n+    heap_ptr : S3(read);\n+    DECODE   : S0(3);\n+    D0       : S2;\n+    MEM      : S3;\n+    ALU      : S3(2);\n+    dst      : S5(write);\n+    BR       : S5;\n+%}\n+\n+\/\/ Generic big\/slow expanded idiom\n+pipe_class pipe_slow()\n+%{\n+    instruction_count(10); multiple_bundles; force_serialization;\n+    fixed_latency(100);\n+    D0  : S0(2);\n+    MEM : S3(2);\n+%}\n+\n+\/\/ The real do-nothing guy\n+pipe_class empty()\n+%{\n+    instruction_count(0);\n+%}\n+\n+\/\/ Define the class for the Nop node\n+define\n+%{\n+   MachNop = empty;\n+%}\n+\n+%}\n+\n+\/\/----------INSTRUCTIONS-------------------------------------------------------\n+\/\/\n+\/\/ match      -- States which machine-independent subtree may be replaced\n+\/\/               by this instruction.\n+\/\/ ins_cost   -- The estimated cost of this instruction is used by instruction\n+\/\/               selection to identify a minimum cost tree of machine\n+\/\/               instructions that matches a tree of machine-independent\n+\/\/               instructions.\n+\/\/ format     -- A string providing the disassembly for this instruction.\n+\/\/               The value of an instruction's operand may be inserted\n+\/\/               by referring to it with a '$' prefix.\n+\/\/ opcode     -- Three instruction opcodes may be provided.  These are referred\n+\/\/               to within an encode class as $primary, $secondary, and $tertiary\n+\/\/               rrspectively.  The primary opcode is commonly used to\n+\/\/               indicate the type of machine instruction, while secondary\n+\/\/               and tertiary are often used for prefix options or addressing\n+\/\/               modes.\n+\/\/ ins_encode -- A list of encode classes with parameters. The encode class\n+\/\/               name must have been defined in an 'enc_class' specification\n+\/\/               in the encode section of the architecture description.\n+\n+\/\/ ============================================================================\n+\n+instruct ShouldNotReachHere() %{\n+  match(Halt);\n+  format %{ \"stop\\t# ShouldNotReachHere\" %}\n+  ins_encode %{\n+    if (is_reachable()) {\n+      const char* str = __ code_string(_halt_reason);\n+      __ stop(str);\n+    }\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ ============================================================================\n+\n+\/\/ Dummy reg-to-reg vector moves. Removed during post-selection cleanup.\n+\/\/ Load Float\n+instruct MoveF2VL(vlRegF dst, regF src) %{\n+  match(Set dst src);\n+  format %{ \"movss $dst,$src\\t! load float (4 bytes)\" %}\n+  ins_encode %{\n+    ShouldNotReachHere();\n+  %}\n+  ins_pipe( fpu_reg_reg );\n+%}\n+\n+\/\/ Load Float\n+instruct MoveF2LEG(legRegF dst, regF src) %{\n+  match(Set dst src);\n+  format %{ \"movss $dst,$src\\t# if src != dst load float (4 bytes)\" %}\n+  ins_encode %{\n+    ShouldNotReachHere();\n+  %}\n+  ins_pipe( fpu_reg_reg );\n+%}\n+\n+\/\/ Load Float\n+instruct MoveVL2F(regF dst, vlRegF src) %{\n+  match(Set dst src);\n+  format %{ \"movss $dst,$src\\t! load float (4 bytes)\" %}\n+  ins_encode %{\n+    ShouldNotReachHere();\n+  %}\n+  ins_pipe( fpu_reg_reg );\n+%}\n+\n+\/\/ Load Float\n+instruct MoveLEG2F(regF dst, legRegF src) %{\n+  match(Set dst src);\n+  format %{ \"movss $dst,$src\\t# if src != dst load float (4 bytes)\" %}\n+  ins_encode %{\n+    ShouldNotReachHere();\n+  %}\n+  ins_pipe( fpu_reg_reg );\n+%}\n+\n+\/\/ Load Double\n+instruct MoveD2VL(vlRegD dst, regD src) %{\n+  match(Set dst src);\n+  format %{ \"movsd $dst,$src\\t! load double (8 bytes)\" %}\n+  ins_encode %{\n+    ShouldNotReachHere();\n+  %}\n+  ins_pipe( fpu_reg_reg );\n+%}\n+\n+\/\/ Load Double\n+instruct MoveD2LEG(legRegD dst, regD src) %{\n+  match(Set dst src);\n+  format %{ \"movsd $dst,$src\\t# if src != dst load double (8 bytes)\" %}\n+  ins_encode %{\n+    ShouldNotReachHere();\n+  %}\n+  ins_pipe( fpu_reg_reg );\n+%}\n+\n+\/\/ Load Double\n+instruct MoveVL2D(regD dst, vlRegD src) %{\n+  match(Set dst src);\n+  format %{ \"movsd $dst,$src\\t! load double (8 bytes)\" %}\n+  ins_encode %{\n+    ShouldNotReachHere();\n+  %}\n+  ins_pipe( fpu_reg_reg );\n+%}\n+\n+\/\/ Load Double\n+instruct MoveLEG2D(regD dst, legRegD src) %{\n+  match(Set dst src);\n+  format %{ \"movsd $dst,$src\\t# if src != dst load double (8 bytes)\" %}\n+  ins_encode %{\n+    ShouldNotReachHere();\n+  %}\n+  ins_pipe( fpu_reg_reg );\n+%}\n+\n+\/\/----------Load\/Store\/Move Instructions---------------------------------------\n+\/\/----------Load Instructions--------------------------------------------------\n+\n+\/\/ Load Byte (8 bit signed)\n+instruct loadB(rRegI dst, memory mem)\n+%{\n+  match(Set dst (LoadB mem));\n+\n+  ins_cost(125);\n+  format %{ \"movsbl  $dst, $mem\\t# byte\" %}\n+\n+  ins_encode %{\n+    __ movsbl($dst$$Register, $mem$$Address);\n+  %}\n+\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+\/\/ Load Byte (8 bit signed) into Long Register\n+instruct loadB2L(rRegL dst, memory mem)\n+%{\n+  match(Set dst (ConvI2L (LoadB mem)));\n+\n+  ins_cost(125);\n+  format %{ \"movsbq  $dst, $mem\\t# byte -> long\" %}\n+\n+  ins_encode %{\n+    __ movsbq($dst$$Register, $mem$$Address);\n+  %}\n+\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+\/\/ Load Unsigned Byte (8 bit UNsigned)\n+instruct loadUB(rRegI dst, memory mem)\n+%{\n+  match(Set dst (LoadUB mem));\n+\n+  ins_cost(125);\n+  format %{ \"movzbl  $dst, $mem\\t# ubyte\" %}\n+\n+  ins_encode %{\n+    __ movzbl($dst$$Register, $mem$$Address);\n+  %}\n+\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+\/\/ Load Unsigned Byte (8 bit UNsigned) into Long Register\n+instruct loadUB2L(rRegL dst, memory mem)\n+%{\n+  match(Set dst (ConvI2L (LoadUB mem)));\n+\n+  ins_cost(125);\n+  format %{ \"movzbq  $dst, $mem\\t# ubyte -> long\" %}\n+\n+  ins_encode %{\n+    __ movzbq($dst$$Register, $mem$$Address);\n+  %}\n+\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+\/\/ Load Unsigned Byte (8 bit UNsigned) with 32-bit mask into Long Register\n+instruct loadUB2L_immI(rRegL dst, memory mem, immI mask, rFlagsReg cr) %{\n+  match(Set dst (ConvI2L (AndI (LoadUB mem) mask)));\n+  effect(KILL cr);\n+\n+  format %{ \"movzbq  $dst, $mem\\t# ubyte & 32-bit mask -> long\\n\\t\"\n+            \"andl    $dst, right_n_bits($mask, 8)\" %}\n+  ins_encode %{\n+    Register Rdst = $dst$$Register;\n+    __ movzbq(Rdst, $mem$$Address);\n+    __ andl(Rdst, $mask$$constant & right_n_bits(8));\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+\/\/ Load Short (16 bit signed)\n+instruct loadS(rRegI dst, memory mem)\n+%{\n+  match(Set dst (LoadS mem));\n+\n+  ins_cost(125);\n+  format %{ \"movswl $dst, $mem\\t# short\" %}\n+\n+  ins_encode %{\n+    __ movswl($dst$$Register, $mem$$Address);\n+  %}\n+\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+\/\/ Load Short (16 bit signed) to Byte (8 bit signed)\n+instruct loadS2B(rRegI dst, memory mem, immI_24 twentyfour) %{\n+  match(Set dst (RShiftI (LShiftI (LoadS mem) twentyfour) twentyfour));\n+\n+  ins_cost(125);\n+  format %{ \"movsbl $dst, $mem\\t# short -> byte\" %}\n+  ins_encode %{\n+    __ movsbl($dst$$Register, $mem$$Address);\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+\/\/ Load Short (16 bit signed) into Long Register\n+instruct loadS2L(rRegL dst, memory mem)\n+%{\n+  match(Set dst (ConvI2L (LoadS mem)));\n+\n+  ins_cost(125);\n+  format %{ \"movswq $dst, $mem\\t# short -> long\" %}\n+\n+  ins_encode %{\n+    __ movswq($dst$$Register, $mem$$Address);\n+  %}\n+\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+\/\/ Load Unsigned Short\/Char (16 bit UNsigned)\n+instruct loadUS(rRegI dst, memory mem)\n+%{\n+  match(Set dst (LoadUS mem));\n+\n+  ins_cost(125);\n+  format %{ \"movzwl  $dst, $mem\\t# ushort\/char\" %}\n+\n+  ins_encode %{\n+    __ movzwl($dst$$Register, $mem$$Address);\n+  %}\n+\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+\/\/ Load Unsigned Short\/Char (16 bit UNsigned) to Byte (8 bit signed)\n+instruct loadUS2B(rRegI dst, memory mem, immI_24 twentyfour) %{\n+  match(Set dst (RShiftI (LShiftI (LoadUS mem) twentyfour) twentyfour));\n+\n+  ins_cost(125);\n+  format %{ \"movsbl $dst, $mem\\t# ushort -> byte\" %}\n+  ins_encode %{\n+    __ movsbl($dst$$Register, $mem$$Address);\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+\/\/ Load Unsigned Short\/Char (16 bit UNsigned) into Long Register\n+instruct loadUS2L(rRegL dst, memory mem)\n+%{\n+  match(Set dst (ConvI2L (LoadUS mem)));\n+\n+  ins_cost(125);\n+  format %{ \"movzwq  $dst, $mem\\t# ushort\/char -> long\" %}\n+\n+  ins_encode %{\n+    __ movzwq($dst$$Register, $mem$$Address);\n+  %}\n+\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+\/\/ Load Unsigned Short\/Char (16 bit UNsigned) with mask 0xFF into Long Register\n+instruct loadUS2L_immI_255(rRegL dst, memory mem, immI_255 mask) %{\n+  match(Set dst (ConvI2L (AndI (LoadUS mem) mask)));\n+\n+  format %{ \"movzbq  $dst, $mem\\t# ushort\/char & 0xFF -> long\" %}\n+  ins_encode %{\n+    __ movzbq($dst$$Register, $mem$$Address);\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+\/\/ Load Unsigned Short\/Char (16 bit UNsigned) with 32-bit mask into Long Register\n+instruct loadUS2L_immI(rRegL dst, memory mem, immI mask, rFlagsReg cr) %{\n+  match(Set dst (ConvI2L (AndI (LoadUS mem) mask)));\n+  effect(KILL cr);\n+\n+  format %{ \"movzwq  $dst, $mem\\t# ushort\/char & 32-bit mask -> long\\n\\t\"\n+            \"andl    $dst, right_n_bits($mask, 16)\" %}\n+  ins_encode %{\n+    Register Rdst = $dst$$Register;\n+    __ movzwq(Rdst, $mem$$Address);\n+    __ andl(Rdst, $mask$$constant & right_n_bits(16));\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+\/\/ Load Integer\n+instruct loadI(rRegI dst, memory mem)\n+%{\n+  match(Set dst (LoadI mem));\n+\n+  ins_cost(125);\n+  format %{ \"movl    $dst, $mem\\t# int\" %}\n+\n+  ins_encode %{\n+    __ movl($dst$$Register, $mem$$Address);\n+  %}\n+\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+\/\/ Load Integer (32 bit signed) to Byte (8 bit signed)\n+instruct loadI2B(rRegI dst, memory mem, immI_24 twentyfour) %{\n+  match(Set dst (RShiftI (LShiftI (LoadI mem) twentyfour) twentyfour));\n+\n+  ins_cost(125);\n+  format %{ \"movsbl  $dst, $mem\\t# int -> byte\" %}\n+  ins_encode %{\n+    __ movsbl($dst$$Register, $mem$$Address);\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+\/\/ Load Integer (32 bit signed) to Unsigned Byte (8 bit UNsigned)\n+instruct loadI2UB(rRegI dst, memory mem, immI_255 mask) %{\n+  match(Set dst (AndI (LoadI mem) mask));\n+\n+  ins_cost(125);\n+  format %{ \"movzbl  $dst, $mem\\t# int -> ubyte\" %}\n+  ins_encode %{\n+    __ movzbl($dst$$Register, $mem$$Address);\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+\/\/ Load Integer (32 bit signed) to Short (16 bit signed)\n+instruct loadI2S(rRegI dst, memory mem, immI_16 sixteen) %{\n+  match(Set dst (RShiftI (LShiftI (LoadI mem) sixteen) sixteen));\n+\n+  ins_cost(125);\n+  format %{ \"movswl  $dst, $mem\\t# int -> short\" %}\n+  ins_encode %{\n+    __ movswl($dst$$Register, $mem$$Address);\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+\/\/ Load Integer (32 bit signed) to Unsigned Short\/Char (16 bit UNsigned)\n+instruct loadI2US(rRegI dst, memory mem, immI_65535 mask) %{\n+  match(Set dst (AndI (LoadI mem) mask));\n+\n+  ins_cost(125);\n+  format %{ \"movzwl  $dst, $mem\\t# int -> ushort\/char\" %}\n+  ins_encode %{\n+    __ movzwl($dst$$Register, $mem$$Address);\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+\/\/ Load Integer into Long Register\n+instruct loadI2L(rRegL dst, memory mem)\n+%{\n+  match(Set dst (ConvI2L (LoadI mem)));\n+\n+  ins_cost(125);\n+  format %{ \"movslq  $dst, $mem\\t# int -> long\" %}\n+\n+  ins_encode %{\n+    __ movslq($dst$$Register, $mem$$Address);\n+  %}\n+\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+\/\/ Load Integer with mask 0xFF into Long Register\n+instruct loadI2L_immI_255(rRegL dst, memory mem, immI_255 mask) %{\n+  match(Set dst (ConvI2L (AndI (LoadI mem) mask)));\n+\n+  format %{ \"movzbq  $dst, $mem\\t# int & 0xFF -> long\" %}\n+  ins_encode %{\n+    __ movzbq($dst$$Register, $mem$$Address);\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+\/\/ Load Integer with mask 0xFFFF into Long Register\n+instruct loadI2L_immI_65535(rRegL dst, memory mem, immI_65535 mask) %{\n+  match(Set dst (ConvI2L (AndI (LoadI mem) mask)));\n+\n+  format %{ \"movzwq  $dst, $mem\\t# int & 0xFFFF -> long\" %}\n+  ins_encode %{\n+    __ movzwq($dst$$Register, $mem$$Address);\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+\/\/ Load Integer with a 31-bit mask into Long Register\n+instruct loadI2L_immU31(rRegL dst, memory mem, immU31 mask, rFlagsReg cr) %{\n+  match(Set dst (ConvI2L (AndI (LoadI mem) mask)));\n+  effect(KILL cr);\n+\n+  format %{ \"movl    $dst, $mem\\t# int & 31-bit mask -> long\\n\\t\"\n+            \"andl    $dst, $mask\" %}\n+  ins_encode %{\n+    Register Rdst = $dst$$Register;\n+    __ movl(Rdst, $mem$$Address);\n+    __ andl(Rdst, $mask$$constant);\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+\/\/ Load Unsigned Integer into Long Register\n+instruct loadUI2L(rRegL dst, memory mem, immL_32bits mask)\n+%{\n+  match(Set dst (AndL (ConvI2L (LoadI mem)) mask));\n+\n+  ins_cost(125);\n+  format %{ \"movl    $dst, $mem\\t# uint -> long\" %}\n+\n+  ins_encode %{\n+    __ movl($dst$$Register, $mem$$Address);\n+  %}\n+\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+\/\/ Load Long\n+instruct loadL(rRegL dst, memory mem)\n+%{\n+  match(Set dst (LoadL mem));\n+\n+  ins_cost(125);\n+  format %{ \"movq    $dst, $mem\\t# long\" %}\n+\n+  ins_encode %{\n+    __ movq($dst$$Register, $mem$$Address);\n+  %}\n+\n+  ins_pipe(ialu_reg_mem); \/\/ XXX\n+%}\n+\n+\/\/ Load Range\n+instruct loadRange(rRegI dst, memory mem)\n+%{\n+  match(Set dst (LoadRange mem));\n+\n+  ins_cost(125); \/\/ XXX\n+  format %{ \"movl    $dst, $mem\\t# range\" %}\n+  ins_encode %{\n+    __ movl($dst$$Register, $mem$$Address);\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+\/\/ Load Pointer\n+instruct loadP(rRegP dst, memory mem)\n+%{\n+  match(Set dst (LoadP mem));\n+  predicate(n->as_Load()->barrier_data() == 0);\n+\n+  ins_cost(125); \/\/ XXX\n+  format %{ \"movq    $dst, $mem\\t# ptr\" %}\n+  ins_encode %{\n+    __ movq($dst$$Register, $mem$$Address);\n+  %}\n+  ins_pipe(ialu_reg_mem); \/\/ XXX\n+%}\n+\n+\/\/ Load Compressed Pointer\n+instruct loadN(rRegN dst, memory mem)\n+%{\n+   predicate(n->as_Load()->barrier_data() == 0);\n+   match(Set dst (LoadN mem));\n+\n+   ins_cost(125); \/\/ XXX\n+   format %{ \"movl    $dst, $mem\\t# compressed ptr\" %}\n+   ins_encode %{\n+     __ movl($dst$$Register, $mem$$Address);\n+   %}\n+   ins_pipe(ialu_reg_mem); \/\/ XXX\n+%}\n+\n+\n+\/\/ Load Klass Pointer\n+instruct loadKlass(rRegP dst, memory mem)\n+%{\n+  match(Set dst (LoadKlass mem));\n+\n+  ins_cost(125); \/\/ XXX\n+  format %{ \"movq    $dst, $mem\\t# class\" %}\n+  ins_encode %{\n+    __ movq($dst$$Register, $mem$$Address);\n+  %}\n+  ins_pipe(ialu_reg_mem); \/\/ XXX\n+%}\n+\n+\/\/ Load narrow Klass Pointer\n+instruct loadNKlass(rRegN dst, memory mem)\n+%{\n+  predicate(!UseCompactObjectHeaders);\n+  match(Set dst (LoadNKlass mem));\n+\n+  ins_cost(125); \/\/ XXX\n+  format %{ \"movl    $dst, $mem\\t# compressed klass ptr\" %}\n+  ins_encode %{\n+    __ movl($dst$$Register, $mem$$Address);\n+  %}\n+  ins_pipe(ialu_reg_mem); \/\/ XXX\n+%}\n+\n+instruct loadNKlassCompactHeaders(rRegN dst, memory mem, rFlagsReg cr)\n+%{\n+  predicate(UseCompactObjectHeaders);\n+  match(Set dst (LoadNKlass mem));\n+  effect(KILL cr);\n+  ins_cost(125);\n+  format %{\n+    \"movl    $dst, $mem\\t# compressed klass ptr, shifted\\n\\t\"\n+    \"shrl    $dst, markWord::klass_shift_at_offset\"\n+  %}\n+  ins_encode %{\n+    if (UseAPX) {\n+      __ eshrl($dst$$Register, $mem$$Address, markWord::klass_shift_at_offset, false);\n+    }\n+    else {\n+      __ movl($dst$$Register, $mem$$Address);\n+      __ shrl($dst$$Register, markWord::klass_shift_at_offset);\n+    }\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+\/\/ Load Float\n+instruct loadF(regF dst, memory mem)\n+%{\n+  match(Set dst (LoadF mem));\n+\n+  ins_cost(145); \/\/ XXX\n+  format %{ \"movss   $dst, $mem\\t# float\" %}\n+  ins_encode %{\n+    __ movflt($dst$$XMMRegister, $mem$$Address);\n+  %}\n+  ins_pipe(pipe_slow); \/\/ XXX\n+%}\n+\n+\/\/ Load Double\n+instruct loadD_partial(regD dst, memory mem)\n+%{\n+  predicate(!UseXmmLoadAndClearUpper);\n+  match(Set dst (LoadD mem));\n+\n+  ins_cost(145); \/\/ XXX\n+  format %{ \"movlpd  $dst, $mem\\t# double\" %}\n+  ins_encode %{\n+    __ movdbl($dst$$XMMRegister, $mem$$Address);\n+  %}\n+  ins_pipe(pipe_slow); \/\/ XXX\n+%}\n+\n+instruct loadD(regD dst, memory mem)\n+%{\n+  predicate(UseXmmLoadAndClearUpper);\n+  match(Set dst (LoadD mem));\n+\n+  ins_cost(145); \/\/ XXX\n+  format %{ \"movsd   $dst, $mem\\t# double\" %}\n+  ins_encode %{\n+    __ movdbl($dst$$XMMRegister, $mem$$Address);\n+  %}\n+  ins_pipe(pipe_slow); \/\/ XXX\n+%}\n+\n+\/\/ max = java.lang.Math.max(float a, float b)\n+instruct maxF_avx10_reg(regF dst, regF a, regF b) %{\n+  predicate(VM_Version::supports_avx10_2());\n+  match(Set dst (MaxF a b));\n+  format %{ \"maxF $dst, $a, $b\" %}\n+  ins_encode %{\n+    __ eminmaxss($dst$$XMMRegister, $a$$XMMRegister, $b$$XMMRegister, AVX10_MINMAX_MAX_COMPARE_SIGN);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+\/\/ max = java.lang.Math.max(float a, float b)\n+instruct maxF_reg(legRegF dst, legRegF a, legRegF b, legRegF tmp, legRegF atmp, legRegF btmp) %{\n+  predicate(!VM_Version::supports_avx10_2() && UseAVX > 0 && !VLoopReductions::is_reduction(n));\n+  match(Set dst (MaxF a b));\n+  effect(USE a, USE b, TEMP tmp, TEMP atmp, TEMP btmp);\n+  format %{ \"maxF $dst, $a, $b \\t! using $tmp, $atmp and $btmp as TEMP\" %}\n+  ins_encode %{\n+    __ vminmax_fp(Op_MaxV, T_FLOAT, $dst$$XMMRegister, $a$$XMMRegister, $b$$XMMRegister, $tmp$$XMMRegister, $atmp$$XMMRegister, $btmp$$XMMRegister, Assembler::AVX_128bit);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct maxF_reduction_reg(legRegF dst, legRegF a, legRegF b, legRegF xtmp, rRegI rtmp, rFlagsReg cr) %{\n+  predicate(!VM_Version::supports_avx10_2() && UseAVX > 0 && VLoopReductions::is_reduction(n));\n+  match(Set dst (MaxF a b));\n+  effect(USE a, USE b, TEMP xtmp, TEMP rtmp, KILL cr);\n+\n+  format %{ \"maxF_reduction $dst, $a, $b \\t!using $xtmp and $rtmp as TEMP\" %}\n+  ins_encode %{\n+    emit_fp_min_max(masm, $dst$$XMMRegister, $a$$XMMRegister, $b$$XMMRegister, $xtmp$$XMMRegister, $rtmp$$Register,\n+                    false \/*min*\/, true \/*single*\/);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+\/\/ max = java.lang.Math.max(double a, double b)\n+instruct maxD_avx10_reg(regD dst, regD a, regD b) %{\n+  predicate(VM_Version::supports_avx10_2());\n+  match(Set dst (MaxD a b));\n+  format %{ \"maxD $dst, $a, $b\" %}\n+  ins_encode %{\n+    __ eminmaxsd($dst$$XMMRegister, $a$$XMMRegister, $b$$XMMRegister, AVX10_MINMAX_MAX_COMPARE_SIGN);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+\/\/ max = java.lang.Math.max(double a, double b)\n+instruct maxD_reg(legRegD dst, legRegD a, legRegD b, legRegD tmp, legRegD atmp, legRegD btmp) %{\n+  predicate(!VM_Version::supports_avx10_2() && UseAVX > 0 && !VLoopReductions::is_reduction(n));\n+  match(Set dst (MaxD a b));\n+  effect(USE a, USE b, TEMP atmp, TEMP btmp, TEMP tmp);\n+  format %{ \"maxD $dst, $a, $b \\t! using $tmp, $atmp and $btmp as TEMP\" %}\n+  ins_encode %{\n+    __ vminmax_fp(Op_MaxV, T_DOUBLE, $dst$$XMMRegister, $a$$XMMRegister, $b$$XMMRegister, $tmp$$XMMRegister, $atmp$$XMMRegister, $btmp$$XMMRegister, Assembler::AVX_128bit);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct maxD_reduction_reg(legRegD dst, legRegD a, legRegD b, legRegD xtmp, rRegL rtmp, rFlagsReg cr) %{\n+  predicate(!VM_Version::supports_avx10_2() && UseAVX > 0 && VLoopReductions::is_reduction(n));\n+  match(Set dst (MaxD a b));\n+  effect(USE a, USE b, TEMP xtmp, TEMP rtmp, KILL cr);\n+\n+  format %{ \"maxD_reduction $dst, $a, $b \\t! using $xtmp and $rtmp as TEMP\" %}\n+  ins_encode %{\n+    emit_fp_min_max(masm, $dst$$XMMRegister, $a$$XMMRegister, $b$$XMMRegister, $xtmp$$XMMRegister, $rtmp$$Register,\n+                    false \/*min*\/, false \/*single*\/);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+\/\/ max = java.lang.Math.min(float a, float b)\n+instruct minF_avx10_reg(regF dst, regF a, regF b) %{\n+  predicate(VM_Version::supports_avx10_2());\n+  match(Set dst (MinF a b));\n+  format %{ \"minF $dst, $a, $b\" %}\n+  ins_encode %{\n+    __ eminmaxss($dst$$XMMRegister, $a$$XMMRegister, $b$$XMMRegister, AVX10_MINMAX_MIN_COMPARE_SIGN);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+\/\/ min = java.lang.Math.min(float a, float b)\n+instruct minF_reg(legRegF dst, legRegF a, legRegF b, legRegF tmp, legRegF atmp, legRegF btmp) %{\n+  predicate(!VM_Version::supports_avx10_2() && UseAVX > 0 && !VLoopReductions::is_reduction(n));\n+  match(Set dst (MinF a b));\n+  effect(USE a, USE b, TEMP tmp, TEMP atmp, TEMP btmp);\n+  format %{ \"minF $dst, $a, $b \\t! using $tmp, $atmp and $btmp as TEMP\" %}\n+  ins_encode %{\n+    __ vminmax_fp(Op_MinV, T_FLOAT, $dst$$XMMRegister, $a$$XMMRegister, $b$$XMMRegister, $tmp$$XMMRegister, $atmp$$XMMRegister, $btmp$$XMMRegister, Assembler::AVX_128bit);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct minF_reduction_reg(legRegF dst, legRegF a, legRegF b, legRegF xtmp, rRegI rtmp, rFlagsReg cr) %{\n+  predicate(!VM_Version::supports_avx10_2() && UseAVX > 0 && VLoopReductions::is_reduction(n));\n+  match(Set dst (MinF a b));\n+  effect(USE a, USE b, TEMP xtmp, TEMP rtmp, KILL cr);\n+\n+  format %{ \"minF_reduction $dst, $a, $b \\t! using $xtmp and $rtmp as TEMP\" %}\n+  ins_encode %{\n+    emit_fp_min_max(masm, $dst$$XMMRegister, $a$$XMMRegister, $b$$XMMRegister, $xtmp$$XMMRegister, $rtmp$$Register,\n+                    true \/*min*\/, true \/*single*\/);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+\/\/ max = java.lang.Math.min(double a, double b)\n+instruct minD_avx10_reg(regD dst, regD a, regD b) %{\n+  predicate(VM_Version::supports_avx10_2());\n+  match(Set dst (MinD a b));\n+  format %{ \"minD $dst, $a, $b\" %}\n+  ins_encode %{\n+    __ eminmaxsd($dst$$XMMRegister, $a$$XMMRegister, $b$$XMMRegister, AVX10_MINMAX_MIN_COMPARE_SIGN);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+\/\/ min = java.lang.Math.min(double a, double b)\n+instruct minD_reg(legRegD dst, legRegD a, legRegD b, legRegD tmp, legRegD atmp, legRegD btmp) %{\n+  predicate(!VM_Version::supports_avx10_2() && UseAVX > 0 && !VLoopReductions::is_reduction(n));\n+  match(Set dst (MinD a b));\n+  effect(USE a, USE b, TEMP tmp, TEMP atmp, TEMP btmp);\n+    format %{ \"minD $dst, $a, $b \\t! using $tmp, $atmp and $btmp as TEMP\" %}\n+  ins_encode %{\n+    __ vminmax_fp(Op_MinV, T_DOUBLE, $dst$$XMMRegister, $a$$XMMRegister, $b$$XMMRegister, $tmp$$XMMRegister, $atmp$$XMMRegister, $btmp$$XMMRegister, Assembler::AVX_128bit);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct minD_reduction_reg(legRegD dst, legRegD a, legRegD b, legRegD xtmp, rRegL rtmp, rFlagsReg cr) %{\n+  predicate(!VM_Version::supports_avx10_2() && UseAVX > 0 && VLoopReductions::is_reduction(n));\n+  match(Set dst (MinD a b));\n+  effect(USE a, USE b, TEMP xtmp, TEMP rtmp, KILL cr);\n+\n+  format %{ \"maxD_reduction $dst, $a, $b \\t! using $xtmp and $rtmp as TEMP\" %}\n+  ins_encode %{\n+    emit_fp_min_max(masm, $dst$$XMMRegister, $a$$XMMRegister, $b$$XMMRegister, $xtmp$$XMMRegister, $rtmp$$Register,\n+                    true \/*min*\/, false \/*single*\/);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+\/\/ Load Effective Address\n+instruct leaP8(rRegP dst, indOffset8 mem)\n+%{\n+  match(Set dst mem);\n+\n+  ins_cost(110); \/\/ XXX\n+  format %{ \"leaq    $dst, $mem\\t# ptr 8\" %}\n+  ins_encode %{\n+    __ leaq($dst$$Register, $mem$$Address);\n+  %}\n+  ins_pipe(ialu_reg_reg_fat);\n+%}\n+\n+instruct leaP32(rRegP dst, indOffset32 mem)\n+%{\n+  match(Set dst mem);\n+\n+  ins_cost(110);\n+  format %{ \"leaq    $dst, $mem\\t# ptr 32\" %}\n+  ins_encode %{\n+    __ leaq($dst$$Register, $mem$$Address);\n+  %}\n+  ins_pipe(ialu_reg_reg_fat);\n+%}\n+\n+instruct leaPIdxOff(rRegP dst, indIndexOffset mem)\n+%{\n+  match(Set dst mem);\n+\n+  ins_cost(110);\n+  format %{ \"leaq    $dst, $mem\\t# ptr idxoff\" %}\n+  ins_encode %{\n+    __ leaq($dst$$Register, $mem$$Address);\n+  %}\n+  ins_pipe(ialu_reg_reg_fat);\n+%}\n+\n+instruct leaPIdxScale(rRegP dst, indIndexScale mem)\n+%{\n+  match(Set dst mem);\n+\n+  ins_cost(110);\n+  format %{ \"leaq    $dst, $mem\\t# ptr idxscale\" %}\n+  ins_encode %{\n+    __ leaq($dst$$Register, $mem$$Address);\n+  %}\n+  ins_pipe(ialu_reg_reg_fat);\n+%}\n+\n+instruct leaPPosIdxScale(rRegP dst, indPosIndexScale mem)\n+%{\n+  match(Set dst mem);\n+\n+  ins_cost(110);\n+  format %{ \"leaq    $dst, $mem\\t# ptr idxscale\" %}\n+  ins_encode %{\n+    __ leaq($dst$$Register, $mem$$Address);\n+  %}\n+  ins_pipe(ialu_reg_reg_fat);\n+%}\n+\n+instruct leaPIdxScaleOff(rRegP dst, indIndexScaleOffset mem)\n+%{\n+  match(Set dst mem);\n+\n+  ins_cost(110);\n+  format %{ \"leaq    $dst, $mem\\t# ptr idxscaleoff\" %}\n+  ins_encode %{\n+    __ leaq($dst$$Register, $mem$$Address);\n+  %}\n+  ins_pipe(ialu_reg_reg_fat);\n+%}\n+\n+instruct leaPPosIdxOff(rRegP dst, indPosIndexOffset mem)\n+%{\n+  match(Set dst mem);\n+\n+  ins_cost(110);\n+  format %{ \"leaq    $dst, $mem\\t# ptr posidxoff\" %}\n+  ins_encode %{\n+    __ leaq($dst$$Register, $mem$$Address);\n+  %}\n+  ins_pipe(ialu_reg_reg_fat);\n+%}\n+\n+instruct leaPPosIdxScaleOff(rRegP dst, indPosIndexScaleOffset mem)\n+%{\n+  match(Set dst mem);\n+\n+  ins_cost(110);\n+  format %{ \"leaq    $dst, $mem\\t# ptr posidxscaleoff\" %}\n+  ins_encode %{\n+    __ leaq($dst$$Register, $mem$$Address);\n+  %}\n+  ins_pipe(ialu_reg_reg_fat);\n+%}\n+\n+\/\/ Load Effective Address which uses Narrow (32-bits) oop\n+instruct leaPCompressedOopOffset(rRegP dst, indCompressedOopOffset mem)\n+%{\n+  predicate(UseCompressedOops && (CompressedOops::shift() != 0));\n+  match(Set dst mem);\n+\n+  ins_cost(110);\n+  format %{ \"leaq    $dst, $mem\\t# ptr compressedoopoff32\" %}\n+  ins_encode %{\n+    __ leaq($dst$$Register, $mem$$Address);\n+  %}\n+  ins_pipe(ialu_reg_reg_fat);\n+%}\n+\n+instruct leaP8Narrow(rRegP dst, indOffset8Narrow mem)\n+%{\n+  predicate(CompressedOops::shift() == 0);\n+  match(Set dst mem);\n+\n+  ins_cost(110); \/\/ XXX\n+  format %{ \"leaq    $dst, $mem\\t# ptr off8narrow\" %}\n+  ins_encode %{\n+    __ leaq($dst$$Register, $mem$$Address);\n+  %}\n+  ins_pipe(ialu_reg_reg_fat);\n+%}\n+\n+instruct leaP32Narrow(rRegP dst, indOffset32Narrow mem)\n+%{\n+  predicate(CompressedOops::shift() == 0);\n+  match(Set dst mem);\n+\n+  ins_cost(110);\n+  format %{ \"leaq    $dst, $mem\\t# ptr off32narrow\" %}\n+  ins_encode %{\n+    __ leaq($dst$$Register, $mem$$Address);\n+  %}\n+  ins_pipe(ialu_reg_reg_fat);\n+%}\n+\n+instruct leaPIdxOffNarrow(rRegP dst, indIndexOffsetNarrow mem)\n+%{\n+  predicate(CompressedOops::shift() == 0);\n+  match(Set dst mem);\n+\n+  ins_cost(110);\n+  format %{ \"leaq    $dst, $mem\\t# ptr idxoffnarrow\" %}\n+  ins_encode %{\n+    __ leaq($dst$$Register, $mem$$Address);\n+  %}\n+  ins_pipe(ialu_reg_reg_fat);\n+%}\n+\n+instruct leaPIdxScaleNarrow(rRegP dst, indIndexScaleNarrow mem)\n+%{\n+  predicate(CompressedOops::shift() == 0);\n+  match(Set dst mem);\n+\n+  ins_cost(110);\n+  format %{ \"leaq    $dst, $mem\\t# ptr idxscalenarrow\" %}\n+  ins_encode %{\n+    __ leaq($dst$$Register, $mem$$Address);\n+  %}\n+  ins_pipe(ialu_reg_reg_fat);\n+%}\n+\n+instruct leaPIdxScaleOffNarrow(rRegP dst, indIndexScaleOffsetNarrow mem)\n+%{\n+  predicate(CompressedOops::shift() == 0);\n+  match(Set dst mem);\n+\n+  ins_cost(110);\n+  format %{ \"leaq    $dst, $mem\\t# ptr idxscaleoffnarrow\" %}\n+  ins_encode %{\n+    __ leaq($dst$$Register, $mem$$Address);\n+  %}\n+  ins_pipe(ialu_reg_reg_fat);\n+%}\n+\n+instruct leaPPosIdxOffNarrow(rRegP dst, indPosIndexOffsetNarrow mem)\n+%{\n+  predicate(CompressedOops::shift() == 0);\n+  match(Set dst mem);\n+\n+  ins_cost(110);\n+  format %{ \"leaq    $dst, $mem\\t# ptr posidxoffnarrow\" %}\n+  ins_encode %{\n+    __ leaq($dst$$Register, $mem$$Address);\n+  %}\n+  ins_pipe(ialu_reg_reg_fat);\n+%}\n+\n+instruct leaPPosIdxScaleOffNarrow(rRegP dst, indPosIndexScaleOffsetNarrow mem)\n+%{\n+  predicate(CompressedOops::shift() == 0);\n+  match(Set dst mem);\n+\n+  ins_cost(110);\n+  format %{ \"leaq    $dst, $mem\\t# ptr posidxscaleoffnarrow\" %}\n+  ins_encode %{\n+    __ leaq($dst$$Register, $mem$$Address);\n+  %}\n+  ins_pipe(ialu_reg_reg_fat);\n+%}\n+\n+instruct loadConI(rRegI dst, immI src)\n+%{\n+  match(Set dst src);\n+\n+  format %{ \"movl    $dst, $src\\t# int\" %}\n+  ins_encode %{\n+    __ movl($dst$$Register, $src$$constant);\n+  %}\n+  ins_pipe(ialu_reg_fat); \/\/ XXX\n+%}\n+\n+instruct loadConI0(rRegI dst, immI_0 src, rFlagsReg cr)\n+%{\n+  match(Set dst src);\n+  effect(KILL cr);\n+\n+  ins_cost(50);\n+  format %{ \"xorl    $dst, $dst\\t# int\" %}\n+  ins_encode %{\n+    __ xorl($dst$$Register, $dst$$Register);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct loadConL(rRegL dst, immL src)\n+%{\n+  match(Set dst src);\n+\n+  ins_cost(150);\n+  format %{ \"movq    $dst, $src\\t# long\" %}\n+  ins_encode %{\n+    __ mov64($dst$$Register, $src$$constant);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct loadConL0(rRegL dst, immL0 src, rFlagsReg cr)\n+%{\n+  match(Set dst src);\n+  effect(KILL cr);\n+\n+  ins_cost(50);\n+  format %{ \"xorl    $dst, $dst\\t# long\" %}\n+  ins_encode %{\n+    __ xorl($dst$$Register, $dst$$Register);\n+  %}\n+  ins_pipe(ialu_reg); \/\/ XXX\n+%}\n+\n+instruct loadConUL32(rRegL dst, immUL32 src)\n+%{\n+  match(Set dst src);\n+\n+  ins_cost(60);\n+  format %{ \"movl    $dst, $src\\t# long (unsigned 32-bit)\" %}\n+  ins_encode %{\n+    __ movl($dst$$Register, $src$$constant);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct loadConL32(rRegL dst, immL32 src)\n+%{\n+  match(Set dst src);\n+\n+  ins_cost(70);\n+  format %{ \"movq    $dst, $src\\t# long (32-bit)\" %}\n+  ins_encode %{\n+    __ movq($dst$$Register, $src$$constant);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct loadConP(rRegP dst, immP con) %{\n+  match(Set dst con);\n+\n+  format %{ \"movq    $dst, $con\\t# ptr\" %}\n+  ins_encode %{\n+    __ mov64($dst$$Register, $con$$constant, $con->constant_reloc(), RELOC_IMM64);\n+  %}\n+  ins_pipe(ialu_reg_fat); \/\/ XXX\n+%}\n+\n+instruct loadConP0(rRegP dst, immP0 src, rFlagsReg cr)\n+%{\n+  match(Set dst src);\n+  effect(KILL cr);\n+\n+  ins_cost(50);\n+  format %{ \"xorl    $dst, $dst\\t# ptr\" %}\n+  ins_encode %{\n+    __ xorl($dst$$Register, $dst$$Register);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct loadConP31(rRegP dst, immP31 src, rFlagsReg cr)\n+%{\n+  match(Set dst src);\n+  effect(KILL cr);\n+\n+  ins_cost(60);\n+  format %{ \"movl    $dst, $src\\t# ptr (positive 32-bit)\" %}\n+  ins_encode %{\n+    __ movl($dst$$Register, $src$$constant);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct loadConF(regF dst, immF con) %{\n+  match(Set dst con);\n+  ins_cost(125);\n+  format %{ \"movss   $dst, [$constantaddress]\\t# load from constant table: float=$con\" %}\n+  ins_encode %{\n+    __ movflt($dst$$XMMRegister, $constantaddress($con));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct loadConH(regF dst, immH con) %{\n+  match(Set dst con);\n+  ins_cost(125);\n+  format %{ \"movss   $dst, [$constantaddress]\\t# load from constant table: halffloat=$con\" %}\n+  ins_encode %{\n+    __ movflt($dst$$XMMRegister, $constantaddress($con));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct loadConN0(rRegN dst, immN0 src, rFlagsReg cr) %{\n+  match(Set dst src);\n+  effect(KILL cr);\n+  format %{ \"xorq    $dst, $src\\t# compressed null pointer\" %}\n+  ins_encode %{\n+    __ xorq($dst$$Register, $dst$$Register);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct loadConN(rRegN dst, immN src) %{\n+  match(Set dst src);\n+\n+  ins_cost(125);\n+  format %{ \"movl    $dst, $src\\t# compressed ptr\" %}\n+  ins_encode %{\n+    address con = (address)$src$$constant;\n+    if (con == nullptr) {\n+      ShouldNotReachHere();\n+    } else {\n+      __ set_narrow_oop($dst$$Register, (jobject)$src$$constant);\n+    }\n+  %}\n+  ins_pipe(ialu_reg_fat); \/\/ XXX\n+%}\n+\n+instruct loadConNKlass(rRegN dst, immNKlass src) %{\n+  match(Set dst src);\n+\n+  ins_cost(125);\n+  format %{ \"movl    $dst, $src\\t# compressed klass ptr\" %}\n+  ins_encode %{\n+    address con = (address)$src$$constant;\n+    if (con == nullptr) {\n+      ShouldNotReachHere();\n+    } else {\n+      __ set_narrow_klass($dst$$Register, (Klass*)$src$$constant);\n+    }\n+  %}\n+  ins_pipe(ialu_reg_fat); \/\/ XXX\n+%}\n+\n+instruct loadConF0(regF dst, immF0 src)\n+%{\n+  match(Set dst src);\n+  ins_cost(100);\n+\n+  format %{ \"xorps   $dst, $dst\\t# float 0.0\" %}\n+  ins_encode %{\n+    __ xorps($dst$$XMMRegister, $dst$$XMMRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ Use the same format since predicate() can not be used here.\n+instruct loadConD(regD dst, immD con) %{\n+  match(Set dst con);\n+  ins_cost(125);\n+  format %{ \"movsd   $dst, [$constantaddress]\\t# load from constant table: double=$con\" %}\n+  ins_encode %{\n+    __ movdbl($dst$$XMMRegister, $constantaddress($con));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct loadConD0(regD dst, immD0 src)\n+%{\n+  match(Set dst src);\n+  ins_cost(100);\n+\n+  format %{ \"xorpd   $dst, $dst\\t# double 0.0\" %}\n+  ins_encode %{\n+    __ xorpd($dst$$XMMRegister, $dst$$XMMRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct loadSSI(rRegI dst, stackSlotI src)\n+%{\n+  match(Set dst src);\n+\n+  ins_cost(125);\n+  format %{ \"movl    $dst, $src\\t# int stk\" %}\n+  ins_encode %{\n+    __ movl($dst$$Register, $src$$Address);\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+instruct loadSSL(rRegL dst, stackSlotL src)\n+%{\n+  match(Set dst src);\n+\n+  ins_cost(125);\n+  format %{ \"movq    $dst, $src\\t# long stk\" %}\n+  ins_encode %{\n+    __ movq($dst$$Register, $src$$Address);\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+instruct loadSSP(rRegP dst, stackSlotP src)\n+%{\n+  match(Set dst src);\n+\n+  ins_cost(125);\n+  format %{ \"movq    $dst, $src\\t# ptr stk\" %}\n+  ins_encode %{\n+    __ movq($dst$$Register, $src$$Address);\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+instruct loadSSF(regF dst, stackSlotF src)\n+%{\n+  match(Set dst src);\n+\n+  ins_cost(125);\n+  format %{ \"movss   $dst, $src\\t# float stk\" %}\n+  ins_encode %{\n+    __ movflt($dst$$XMMRegister, Address(rsp, $src$$disp));\n+  %}\n+  ins_pipe(pipe_slow); \/\/ XXX\n+%}\n+\n+\/\/ Use the same format since predicate() can not be used here.\n+instruct loadSSD(regD dst, stackSlotD src)\n+%{\n+  match(Set dst src);\n+\n+  ins_cost(125);\n+  format %{ \"movsd   $dst, $src\\t# double stk\" %}\n+  ins_encode  %{\n+    __ movdbl($dst$$XMMRegister, Address(rsp, $src$$disp));\n+  %}\n+  ins_pipe(pipe_slow); \/\/ XXX\n+%}\n+\n+\/\/ Prefetch instructions for allocation.\n+\/\/ Must be safe to execute with invalid address (cannot fault).\n+\n+instruct prefetchAlloc( memory mem ) %{\n+  predicate(AllocatePrefetchInstr==3);\n+  match(PrefetchAllocation mem);\n+  ins_cost(125);\n+\n+  format %{ \"PREFETCHW $mem\\t# Prefetch allocation into level 1 cache and mark modified\" %}\n+  ins_encode %{\n+    __ prefetchw($mem$$Address);\n+  %}\n+  ins_pipe(ialu_mem);\n+%}\n+\n+instruct prefetchAllocNTA( memory mem ) %{\n+  predicate(AllocatePrefetchInstr==0);\n+  match(PrefetchAllocation mem);\n+  ins_cost(125);\n+\n+  format %{ \"PREFETCHNTA $mem\\t# Prefetch allocation to non-temporal cache for write\" %}\n+  ins_encode %{\n+    __ prefetchnta($mem$$Address);\n+  %}\n+  ins_pipe(ialu_mem);\n+%}\n+\n+instruct prefetchAllocT0( memory mem ) %{\n+  predicate(AllocatePrefetchInstr==1);\n+  match(PrefetchAllocation mem);\n+  ins_cost(125);\n+\n+  format %{ \"PREFETCHT0 $mem\\t# Prefetch allocation to level 1 and 2 caches for write\" %}\n+  ins_encode %{\n+    __ prefetcht0($mem$$Address);\n+  %}\n+  ins_pipe(ialu_mem);\n+%}\n+\n+instruct prefetchAllocT2( memory mem ) %{\n+  predicate(AllocatePrefetchInstr==2);\n+  match(PrefetchAllocation mem);\n+  ins_cost(125);\n+\n+  format %{ \"PREFETCHT2 $mem\\t# Prefetch allocation to level 2 cache for write\" %}\n+  ins_encode %{\n+    __ prefetcht2($mem$$Address);\n+  %}\n+  ins_pipe(ialu_mem);\n+%}\n+\n+\/\/----------Store Instructions-------------------------------------------------\n+\n+\/\/ Store Byte\n+instruct storeB(memory mem, rRegI src)\n+%{\n+  match(Set mem (StoreB mem src));\n+\n+  ins_cost(125); \/\/ XXX\n+  format %{ \"movb    $mem, $src\\t# byte\" %}\n+  ins_encode %{\n+    __ movb($mem$$Address, $src$$Register);\n+  %}\n+  ins_pipe(ialu_mem_reg);\n+%}\n+\n+\/\/ Store Char\/Short\n+instruct storeC(memory mem, rRegI src)\n+%{\n+  match(Set mem (StoreC mem src));\n+\n+  ins_cost(125); \/\/ XXX\n+  format %{ \"movw    $mem, $src\\t# char\/short\" %}\n+  ins_encode %{\n+    __ movw($mem$$Address, $src$$Register);\n+  %}\n+  ins_pipe(ialu_mem_reg);\n+%}\n+\n+\/\/ Store Integer\n+instruct storeI(memory mem, rRegI src)\n+%{\n+  match(Set mem (StoreI mem src));\n+\n+  ins_cost(125); \/\/ XXX\n+  format %{ \"movl    $mem, $src\\t# int\" %}\n+  ins_encode %{\n+    __ movl($mem$$Address, $src$$Register);\n+  %}\n+  ins_pipe(ialu_mem_reg);\n+%}\n+\n+\/\/ Store Long\n+instruct storeL(memory mem, rRegL src)\n+%{\n+  match(Set mem (StoreL mem src));\n+\n+  ins_cost(125); \/\/ XXX\n+  format %{ \"movq    $mem, $src\\t# long\" %}\n+  ins_encode %{\n+    __ movq($mem$$Address, $src$$Register);\n+  %}\n+  ins_pipe(ialu_mem_reg); \/\/ XXX\n+%}\n+\n+\/\/ Store Pointer\n+instruct storeP(memory mem, any_RegP src)\n+%{\n+  predicate(n->as_Store()->barrier_data() == 0);\n+  match(Set mem (StoreP mem src));\n+\n+  ins_cost(125); \/\/ XXX\n+  format %{ \"movq    $mem, $src\\t# ptr\" %}\n+  ins_encode %{\n+    __ movq($mem$$Address, $src$$Register);\n+  %}\n+  ins_pipe(ialu_mem_reg);\n+%}\n+\n+instruct storeImmP0(memory mem, immP0 zero)\n+%{\n+  predicate(UseCompressedOops && (CompressedOops::base() == nullptr) && n->as_Store()->barrier_data() == 0);\n+  match(Set mem (StoreP mem zero));\n+\n+  ins_cost(125); \/\/ XXX\n+  format %{ \"movq    $mem, R12\\t# ptr (R12_heapbase==0)\" %}\n+  ins_encode %{\n+    __ movq($mem$$Address, r12);\n+  %}\n+  ins_pipe(ialu_mem_reg);\n+%}\n+\n+\/\/ Store Null Pointer, mark word, or other simple pointer constant.\n+instruct storeImmP(memory mem, immP31 src)\n+%{\n+  predicate(n->as_Store()->barrier_data() == 0);\n+  match(Set mem (StoreP mem src));\n+\n+  ins_cost(150); \/\/ XXX\n+  format %{ \"movq    $mem, $src\\t# ptr\" %}\n+  ins_encode %{\n+    __ movq($mem$$Address, $src$$constant);\n+  %}\n+  ins_pipe(ialu_mem_imm);\n+%}\n+\n+\/\/ Store Compressed Pointer\n+instruct storeN(memory mem, rRegN src)\n+%{\n+  predicate(n->as_Store()->barrier_data() == 0);\n+  match(Set mem (StoreN mem src));\n+\n+  ins_cost(125); \/\/ XXX\n+  format %{ \"movl    $mem, $src\\t# compressed ptr\" %}\n+  ins_encode %{\n+    __ movl($mem$$Address, $src$$Register);\n+  %}\n+  ins_pipe(ialu_mem_reg);\n+%}\n+\n+instruct storeNKlass(memory mem, rRegN src)\n+%{\n+  match(Set mem (StoreNKlass mem src));\n+\n+  ins_cost(125); \/\/ XXX\n+  format %{ \"movl    $mem, $src\\t# compressed klass ptr\" %}\n+  ins_encode %{\n+    __ movl($mem$$Address, $src$$Register);\n+  %}\n+  ins_pipe(ialu_mem_reg);\n+%}\n+\n+instruct storeImmN0(memory mem, immN0 zero)\n+%{\n+  predicate(CompressedOops::base() == nullptr && n->as_Store()->barrier_data() == 0);\n+  match(Set mem (StoreN mem zero));\n+\n+  ins_cost(125); \/\/ XXX\n+  format %{ \"movl    $mem, R12\\t# compressed ptr (R12_heapbase==0)\" %}\n+  ins_encode %{\n+    __ movl($mem$$Address, r12);\n+  %}\n+  ins_pipe(ialu_mem_reg);\n+%}\n+\n+instruct storeImmN(memory mem, immN src)\n+%{\n+  predicate(n->as_Store()->barrier_data() == 0);\n+  match(Set mem (StoreN mem src));\n+\n+  ins_cost(150); \/\/ XXX\n+  format %{ \"movl    $mem, $src\\t# compressed ptr\" %}\n+  ins_encode %{\n+    address con = (address)$src$$constant;\n+    if (con == nullptr) {\n+      __ movl($mem$$Address, 0);\n+    } else {\n+      __ set_narrow_oop($mem$$Address, (jobject)$src$$constant);\n+    }\n+  %}\n+  ins_pipe(ialu_mem_imm);\n+%}\n+\n+instruct storeImmNKlass(memory mem, immNKlass src)\n+%{\n+  match(Set mem (StoreNKlass mem src));\n+\n+  ins_cost(150); \/\/ XXX\n+  format %{ \"movl    $mem, $src\\t# compressed klass ptr\" %}\n+  ins_encode %{\n+    __ set_narrow_klass($mem$$Address, (Klass*)$src$$constant);\n+  %}\n+  ins_pipe(ialu_mem_imm);\n+%}\n+\n+\/\/ Store Integer Immediate\n+instruct storeImmI0(memory mem, immI_0 zero)\n+%{\n+  predicate(UseCompressedOops && (CompressedOops::base() == nullptr));\n+  match(Set mem (StoreI mem zero));\n+\n+  ins_cost(125); \/\/ XXX\n+  format %{ \"movl    $mem, R12\\t# int (R12_heapbase==0)\" %}\n+  ins_encode %{\n+    __ movl($mem$$Address, r12);\n+  %}\n+  ins_pipe(ialu_mem_reg);\n+%}\n+\n+instruct storeImmI(memory mem, immI src)\n+%{\n+  match(Set mem (StoreI mem src));\n+\n+  ins_cost(150);\n+  format %{ \"movl    $mem, $src\\t# int\" %}\n+  ins_encode %{\n+    __ movl($mem$$Address, $src$$constant);\n+  %}\n+  ins_pipe(ialu_mem_imm);\n+%}\n+\n+\/\/ Store Long Immediate\n+instruct storeImmL0(memory mem, immL0 zero)\n+%{\n+  predicate(UseCompressedOops && (CompressedOops::base() == nullptr));\n+  match(Set mem (StoreL mem zero));\n+\n+  ins_cost(125); \/\/ XXX\n+  format %{ \"movq    $mem, R12\\t# long (R12_heapbase==0)\" %}\n+  ins_encode %{\n+    __ movq($mem$$Address, r12);\n+  %}\n+  ins_pipe(ialu_mem_reg);\n+%}\n+\n+instruct storeImmL(memory mem, immL32 src)\n+%{\n+  match(Set mem (StoreL mem src));\n+\n+  ins_cost(150);\n+  format %{ \"movq    $mem, $src\\t# long\" %}\n+  ins_encode %{\n+    __ movq($mem$$Address, $src$$constant);\n+  %}\n+  ins_pipe(ialu_mem_imm);\n+%}\n+\n+\/\/ Store Short\/Char Immediate\n+instruct storeImmC0(memory mem, immI_0 zero)\n+%{\n+  predicate(UseCompressedOops && (CompressedOops::base() == nullptr));\n+  match(Set mem (StoreC mem zero));\n+\n+  ins_cost(125); \/\/ XXX\n+  format %{ \"movw    $mem, R12\\t# short\/char (R12_heapbase==0)\" %}\n+  ins_encode %{\n+    __ movw($mem$$Address, r12);\n+  %}\n+  ins_pipe(ialu_mem_reg);\n+%}\n+\n+instruct storeImmI16(memory mem, immI16 src)\n+%{\n+  predicate(UseStoreImmI16);\n+  match(Set mem (StoreC mem src));\n+\n+  ins_cost(150);\n+  format %{ \"movw    $mem, $src\\t# short\/char\" %}\n+  ins_encode %{\n+    __ movw($mem$$Address, $src$$constant);\n+  %}\n+  ins_pipe(ialu_mem_imm);\n+%}\n+\n+\/\/ Store Byte Immediate\n+instruct storeImmB0(memory mem, immI_0 zero)\n+%{\n+  predicate(UseCompressedOops && (CompressedOops::base() == nullptr));\n+  match(Set mem (StoreB mem zero));\n+\n+  ins_cost(125); \/\/ XXX\n+  format %{ \"movb    $mem, R12\\t# short\/char (R12_heapbase==0)\" %}\n+  ins_encode %{\n+    __ movb($mem$$Address, r12);\n+  %}\n+  ins_pipe(ialu_mem_reg);\n+%}\n+\n+instruct storeImmB(memory mem, immI8 src)\n+%{\n+  match(Set mem (StoreB mem src));\n+\n+  ins_cost(150); \/\/ XXX\n+  format %{ \"movb    $mem, $src\\t# byte\" %}\n+  ins_encode %{\n+    __ movb($mem$$Address, $src$$constant);\n+  %}\n+  ins_pipe(ialu_mem_imm);\n+%}\n+\n+\/\/ Store Float\n+instruct storeF(memory mem, regF src)\n+%{\n+  match(Set mem (StoreF mem src));\n+\n+  ins_cost(95); \/\/ XXX\n+  format %{ \"movss   $mem, $src\\t# float\" %}\n+  ins_encode %{\n+    __ movflt($mem$$Address, $src$$XMMRegister);\n+  %}\n+  ins_pipe(pipe_slow); \/\/ XXX\n+%}\n+\n+\/\/ Store immediate Float value (it is faster than store from XMM register)\n+instruct storeF0(memory mem, immF0 zero)\n+%{\n+  predicate(UseCompressedOops && (CompressedOops::base() == nullptr));\n+  match(Set mem (StoreF mem zero));\n+\n+  ins_cost(25); \/\/ XXX\n+  format %{ \"movl    $mem, R12\\t# float 0. (R12_heapbase==0)\" %}\n+  ins_encode %{\n+    __ movl($mem$$Address, r12);\n+  %}\n+  ins_pipe(ialu_mem_reg);\n+%}\n+\n+instruct storeF_imm(memory mem, immF src)\n+%{\n+  match(Set mem (StoreF mem src));\n+\n+  ins_cost(50);\n+  format %{ \"movl    $mem, $src\\t# float\" %}\n+  ins_encode %{\n+    __ movl($mem$$Address, jint_cast($src$$constant));\n+  %}\n+  ins_pipe(ialu_mem_imm);\n+%}\n+\n+\/\/ Store Double\n+instruct storeD(memory mem, regD src)\n+%{\n+  match(Set mem (StoreD mem src));\n+\n+  ins_cost(95); \/\/ XXX\n+  format %{ \"movsd   $mem, $src\\t# double\" %}\n+  ins_encode %{\n+    __ movdbl($mem$$Address, $src$$XMMRegister);\n+  %}\n+  ins_pipe(pipe_slow); \/\/ XXX\n+%}\n+\n+\/\/ Store immediate double 0.0 (it is faster than store from XMM register)\n+instruct storeD0_imm(memory mem, immD0 src)\n+%{\n+  predicate(!UseCompressedOops || (CompressedOops::base() != nullptr));\n+  match(Set mem (StoreD mem src));\n+\n+  ins_cost(50);\n+  format %{ \"movq    $mem, $src\\t# double 0.\" %}\n+  ins_encode %{\n+    __ movq($mem$$Address, $src$$constant);\n+  %}\n+  ins_pipe(ialu_mem_imm);\n+%}\n+\n+instruct storeD0(memory mem, immD0 zero)\n+%{\n+  predicate(UseCompressedOops && (CompressedOops::base() == nullptr));\n+  match(Set mem (StoreD mem zero));\n+\n+  ins_cost(25); \/\/ XXX\n+  format %{ \"movq    $mem, R12\\t# double 0. (R12_heapbase==0)\" %}\n+  ins_encode %{\n+    __ movq($mem$$Address, r12);\n+  %}\n+  ins_pipe(ialu_mem_reg);\n+%}\n+\n+instruct storeSSI(stackSlotI dst, rRegI src)\n+%{\n+  match(Set dst src);\n+\n+  ins_cost(100);\n+  format %{ \"movl    $dst, $src\\t# int stk\" %}\n+  ins_encode %{\n+    __ movl($dst$$Address, $src$$Register);\n+  %}\n+  ins_pipe( ialu_mem_reg );\n+%}\n+\n+instruct storeSSL(stackSlotL dst, rRegL src)\n+%{\n+  match(Set dst src);\n+\n+  ins_cost(100);\n+  format %{ \"movq    $dst, $src\\t# long stk\" %}\n+  ins_encode %{\n+    __ movq($dst$$Address, $src$$Register);\n+  %}\n+  ins_pipe(ialu_mem_reg);\n+%}\n+\n+instruct storeSSP(stackSlotP dst, rRegP src)\n+%{\n+  match(Set dst src);\n+\n+  ins_cost(100);\n+  format %{ \"movq    $dst, $src\\t# ptr stk\" %}\n+  ins_encode %{\n+    __ movq($dst$$Address, $src$$Register);\n+  %}\n+  ins_pipe(ialu_mem_reg);\n+%}\n+\n+instruct storeSSF(stackSlotF dst, regF src)\n+%{\n+  match(Set dst src);\n+\n+  ins_cost(95); \/\/ XXX\n+  format %{ \"movss   $dst, $src\\t# float stk\" %}\n+  ins_encode %{\n+    __ movflt(Address(rsp, $dst$$disp), $src$$XMMRegister);\n+  %}\n+  ins_pipe(pipe_slow); \/\/ XXX\n+%}\n+\n+instruct storeSSD(stackSlotD dst, regD src)\n+%{\n+  match(Set dst src);\n+\n+  ins_cost(95); \/\/ XXX\n+  format %{ \"movsd   $dst, $src\\t# double stk\" %}\n+  ins_encode %{\n+    __ movdbl(Address(rsp, $dst$$disp), $src$$XMMRegister);\n+  %}\n+  ins_pipe(pipe_slow); \/\/ XXX\n+%}\n+\n+instruct cacheWB(indirect addr)\n+%{\n+  predicate(VM_Version::supports_data_cache_line_flush());\n+  match(CacheWB addr);\n+\n+  ins_cost(100);\n+  format %{\"cache wb $addr\" %}\n+  ins_encode %{\n+    assert($addr->index_position() < 0, \"should be\");\n+    assert($addr$$disp == 0, \"should be\");\n+    __ cache_wb(Address($addr$$base$$Register, 0));\n+  %}\n+  ins_pipe(pipe_slow); \/\/ XXX\n+%}\n+\n+instruct cacheWBPreSync()\n+%{\n+  predicate(VM_Version::supports_data_cache_line_flush());\n+  match(CacheWBPreSync);\n+\n+  ins_cost(100);\n+  format %{\"cache wb presync\" %}\n+  ins_encode %{\n+    __ cache_wbsync(true);\n+  %}\n+  ins_pipe(pipe_slow); \/\/ XXX\n+%}\n+\n+instruct cacheWBPostSync()\n+%{\n+  predicate(VM_Version::supports_data_cache_line_flush());\n+  match(CacheWBPostSync);\n+\n+  ins_cost(100);\n+  format %{\"cache wb postsync\" %}\n+  ins_encode %{\n+    __ cache_wbsync(false);\n+  %}\n+  ins_pipe(pipe_slow); \/\/ XXX\n+%}\n+\n+\/\/----------BSWAP Instructions-------------------------------------------------\n+instruct bytes_reverse_int(rRegI dst) %{\n+  match(Set dst (ReverseBytesI dst));\n+\n+  format %{ \"bswapl  $dst\" %}\n+  ins_encode %{\n+    __ bswapl($dst$$Register);\n+  %}\n+  ins_pipe( ialu_reg );\n+%}\n+\n+instruct bytes_reverse_long(rRegL dst) %{\n+  match(Set dst (ReverseBytesL dst));\n+\n+  format %{ \"bswapq  $dst\" %}\n+  ins_encode %{\n+    __ bswapq($dst$$Register);\n+  %}\n+  ins_pipe( ialu_reg);\n+%}\n+\n+instruct bytes_reverse_unsigned_short(rRegI dst, rFlagsReg cr) %{\n+  match(Set dst (ReverseBytesUS dst));\n+  effect(KILL cr);\n+\n+  format %{ \"bswapl  $dst\\n\\t\"\n+            \"shrl    $dst,16\\n\\t\" %}\n+  ins_encode %{\n+    __ bswapl($dst$$Register);\n+    __ shrl($dst$$Register, 16);\n+  %}\n+  ins_pipe( ialu_reg );\n+%}\n+\n+instruct bytes_reverse_short(rRegI dst, rFlagsReg cr) %{\n+  match(Set dst (ReverseBytesS dst));\n+  effect(KILL cr);\n+\n+  format %{ \"bswapl  $dst\\n\\t\"\n+            \"sar     $dst,16\\n\\t\" %}\n+  ins_encode %{\n+    __ bswapl($dst$$Register);\n+    __ sarl($dst$$Register, 16);\n+  %}\n+  ins_pipe( ialu_reg );\n+%}\n+\n+\/\/---------- Zeros Count Instructions ------------------------------------------\n+\n+instruct countLeadingZerosI(rRegI dst, rRegI src, rFlagsReg cr) %{\n+  predicate(UseCountLeadingZerosInstruction);\n+  match(Set dst (CountLeadingZerosI src));\n+  effect(KILL cr);\n+\n+  format %{ \"lzcntl  $dst, $src\\t# count leading zeros (int)\" %}\n+  ins_encode %{\n+    __ lzcntl($dst$$Register, $src$$Register);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct countLeadingZerosI_mem(rRegI dst, memory src, rFlagsReg cr) %{\n+  predicate(UseCountLeadingZerosInstruction);\n+  match(Set dst (CountLeadingZerosI (LoadI src)));\n+  effect(KILL cr);\n+  ins_cost(175);\n+  format %{ \"lzcntl  $dst, $src\\t# count leading zeros (int)\" %}\n+  ins_encode %{\n+    __ lzcntl($dst$$Register, $src$$Address);\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+instruct countLeadingZerosI_bsr(rRegI dst, rRegI src, rFlagsReg cr) %{\n+  predicate(!UseCountLeadingZerosInstruction);\n+  match(Set dst (CountLeadingZerosI src));\n+  effect(KILL cr);\n+\n+  format %{ \"bsrl    $dst, $src\\t# count leading zeros (int)\\n\\t\"\n+            \"jnz     skip\\n\\t\"\n+            \"movl    $dst, -1\\n\"\n+      \"skip:\\n\\t\"\n+            \"negl    $dst\\n\\t\"\n+            \"addl    $dst, 31\" %}\n+  ins_encode %{\n+    Register Rdst = $dst$$Register;\n+    Register Rsrc = $src$$Register;\n+    Label skip;\n+    __ bsrl(Rdst, Rsrc);\n+    __ jccb(Assembler::notZero, skip);\n+    __ movl(Rdst, -1);\n+    __ bind(skip);\n+    __ negl(Rdst);\n+    __ addl(Rdst, BitsPerInt - 1);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct countLeadingZerosL(rRegI dst, rRegL src, rFlagsReg cr) %{\n+  predicate(UseCountLeadingZerosInstruction);\n+  match(Set dst (CountLeadingZerosL src));\n+  effect(KILL cr);\n+\n+  format %{ \"lzcntq  $dst, $src\\t# count leading zeros (long)\" %}\n+  ins_encode %{\n+    __ lzcntq($dst$$Register, $src$$Register);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct countLeadingZerosL_mem(rRegI dst, memory src, rFlagsReg cr) %{\n+  predicate(UseCountLeadingZerosInstruction);\n+  match(Set dst (CountLeadingZerosL (LoadL src)));\n+  effect(KILL cr);\n+  ins_cost(175);\n+  format %{ \"lzcntq  $dst, $src\\t# count leading zeros (long)\" %}\n+  ins_encode %{\n+    __ lzcntq($dst$$Register, $src$$Address);\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+instruct countLeadingZerosL_bsr(rRegI dst, rRegL src, rFlagsReg cr) %{\n+  predicate(!UseCountLeadingZerosInstruction);\n+  match(Set dst (CountLeadingZerosL src));\n+  effect(KILL cr);\n+\n+  format %{ \"bsrq    $dst, $src\\t# count leading zeros (long)\\n\\t\"\n+            \"jnz     skip\\n\\t\"\n+            \"movl    $dst, -1\\n\"\n+      \"skip:\\n\\t\"\n+            \"negl    $dst\\n\\t\"\n+            \"addl    $dst, 63\" %}\n+  ins_encode %{\n+    Register Rdst = $dst$$Register;\n+    Register Rsrc = $src$$Register;\n+    Label skip;\n+    __ bsrq(Rdst, Rsrc);\n+    __ jccb(Assembler::notZero, skip);\n+    __ movl(Rdst, -1);\n+    __ bind(skip);\n+    __ negl(Rdst);\n+    __ addl(Rdst, BitsPerLong - 1);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct countTrailingZerosI(rRegI dst, rRegI src, rFlagsReg cr) %{\n+  predicate(UseCountTrailingZerosInstruction);\n+  match(Set dst (CountTrailingZerosI src));\n+  effect(KILL cr);\n+\n+  format %{ \"tzcntl    $dst, $src\\t# count trailing zeros (int)\" %}\n+  ins_encode %{\n+    __ tzcntl($dst$$Register, $src$$Register);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct countTrailingZerosI_mem(rRegI dst, memory src, rFlagsReg cr) %{\n+  predicate(UseCountTrailingZerosInstruction);\n+  match(Set dst (CountTrailingZerosI (LoadI src)));\n+  effect(KILL cr);\n+  ins_cost(175);\n+  format %{ \"tzcntl    $dst, $src\\t# count trailing zeros (int)\" %}\n+  ins_encode %{\n+    __ tzcntl($dst$$Register, $src$$Address);\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+instruct countTrailingZerosI_bsf(rRegI dst, rRegI src, rFlagsReg cr) %{\n+  predicate(!UseCountTrailingZerosInstruction);\n+  match(Set dst (CountTrailingZerosI src));\n+  effect(KILL cr);\n+\n+  format %{ \"bsfl    $dst, $src\\t# count trailing zeros (int)\\n\\t\"\n+            \"jnz     done\\n\\t\"\n+            \"movl    $dst, 32\\n\"\n+      \"done:\" %}\n+  ins_encode %{\n+    Register Rdst = $dst$$Register;\n+    Label done;\n+    __ bsfl(Rdst, $src$$Register);\n+    __ jccb(Assembler::notZero, done);\n+    __ movl(Rdst, BitsPerInt);\n+    __ bind(done);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct countTrailingZerosL(rRegI dst, rRegL src, rFlagsReg cr) %{\n+  predicate(UseCountTrailingZerosInstruction);\n+  match(Set dst (CountTrailingZerosL src));\n+  effect(KILL cr);\n+\n+  format %{ \"tzcntq    $dst, $src\\t# count trailing zeros (long)\" %}\n+  ins_encode %{\n+    __ tzcntq($dst$$Register, $src$$Register);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct countTrailingZerosL_mem(rRegI dst, memory src, rFlagsReg cr) %{\n+  predicate(UseCountTrailingZerosInstruction);\n+  match(Set dst (CountTrailingZerosL (LoadL src)));\n+  effect(KILL cr);\n+  ins_cost(175);\n+  format %{ \"tzcntq    $dst, $src\\t# count trailing zeros (long)\" %}\n+  ins_encode %{\n+    __ tzcntq($dst$$Register, $src$$Address);\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+instruct countTrailingZerosL_bsf(rRegI dst, rRegL src, rFlagsReg cr) %{\n+  predicate(!UseCountTrailingZerosInstruction);\n+  match(Set dst (CountTrailingZerosL src));\n+  effect(KILL cr);\n+\n+  format %{ \"bsfq    $dst, $src\\t# count trailing zeros (long)\\n\\t\"\n+            \"jnz     done\\n\\t\"\n+            \"movl    $dst, 64\\n\"\n+      \"done:\" %}\n+  ins_encode %{\n+    Register Rdst = $dst$$Register;\n+    Label done;\n+    __ bsfq(Rdst, $src$$Register);\n+    __ jccb(Assembler::notZero, done);\n+    __ movl(Rdst, BitsPerLong);\n+    __ bind(done);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+\/\/--------------- Reverse Operation Instructions ----------------\n+instruct bytes_reversebit_int(rRegI dst, rRegI src, rRegI rtmp, rFlagsReg cr) %{\n+  predicate(!VM_Version::supports_gfni());\n+  match(Set dst (ReverseI src));\n+  effect(TEMP dst, TEMP rtmp, KILL cr);\n+  format %{ \"reverse_int $dst $src\\t! using $rtmp as TEMP\" %}\n+  ins_encode %{\n+    __ reverseI($dst$$Register, $src$$Register, xnoreg, xnoreg, $rtmp$$Register);\n+  %}\n+  ins_pipe( ialu_reg );\n+%}\n+\n+instruct bytes_reversebit_int_gfni(rRegI dst, rRegI src, vlRegF xtmp1, vlRegF xtmp2, rRegL rtmp, rFlagsReg cr) %{\n+  predicate(VM_Version::supports_gfni());\n+  match(Set dst (ReverseI src));\n+  effect(TEMP dst, TEMP xtmp1, TEMP xtmp2, TEMP rtmp, KILL cr);\n+  format %{ \"reverse_int $dst $src\\t! using $rtmp, $xtmp1 and $xtmp2 as TEMP\" %}\n+  ins_encode %{\n+    __ reverseI($dst$$Register, $src$$Register, $xtmp1$$XMMRegister, $xtmp2$$XMMRegister, $rtmp$$Register);\n+  %}\n+  ins_pipe( ialu_reg );\n+%}\n+\n+instruct bytes_reversebit_long(rRegL dst, rRegL src, rRegL rtmp1, rRegL rtmp2, rFlagsReg cr) %{\n+  predicate(!VM_Version::supports_gfni());\n+  match(Set dst (ReverseL src));\n+  effect(TEMP dst, TEMP rtmp1, TEMP rtmp2, KILL cr);\n+  format %{ \"reverse_long $dst $src\\t! using $rtmp1 and $rtmp2 as TEMP\" %}\n+  ins_encode %{\n+    __ reverseL($dst$$Register, $src$$Register, xnoreg, xnoreg, $rtmp1$$Register, $rtmp2$$Register);\n+  %}\n+  ins_pipe( ialu_reg );\n+%}\n+\n+instruct bytes_reversebit_long_gfni(rRegL dst, rRegL src, vlRegD xtmp1, vlRegD xtmp2, rRegL rtmp, rFlagsReg cr) %{\n+  predicate(VM_Version::supports_gfni());\n+  match(Set dst (ReverseL src));\n+  effect(TEMP dst, TEMP xtmp1, TEMP xtmp2, TEMP rtmp, KILL cr);\n+  format %{ \"reverse_long $dst $src\\t! using $rtmp, $xtmp1 and $xtmp2 as TEMP\" %}\n+  ins_encode %{\n+    __ reverseL($dst$$Register, $src$$Register, $xtmp1$$XMMRegister, $xtmp2$$XMMRegister, $rtmp$$Register, noreg);\n+  %}\n+  ins_pipe( ialu_reg );\n+%}\n+\n+\/\/---------- Population Count Instructions -------------------------------------\n+\n+instruct popCountI(rRegI dst, rRegI src, rFlagsReg cr) %{\n+  predicate(UsePopCountInstruction);\n+  match(Set dst (PopCountI src));\n+  effect(KILL cr);\n+\n+  format %{ \"popcnt  $dst, $src\" %}\n+  ins_encode %{\n+    __ popcntl($dst$$Register, $src$$Register);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct popCountI_mem(rRegI dst, memory mem, rFlagsReg cr) %{\n+  predicate(UsePopCountInstruction);\n+  match(Set dst (PopCountI (LoadI mem)));\n+  effect(KILL cr);\n+\n+  format %{ \"popcnt  $dst, $mem\" %}\n+  ins_encode %{\n+    __ popcntl($dst$$Register, $mem$$Address);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+\/\/ Note: Long.bitCount(long) returns an int.\n+instruct popCountL(rRegI dst, rRegL src, rFlagsReg cr) %{\n+  predicate(UsePopCountInstruction);\n+  match(Set dst (PopCountL src));\n+  effect(KILL cr);\n+\n+  format %{ \"popcnt  $dst, $src\" %}\n+  ins_encode %{\n+    __ popcntq($dst$$Register, $src$$Register);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+\/\/ Note: Long.bitCount(long) returns an int.\n+instruct popCountL_mem(rRegI dst, memory mem, rFlagsReg cr) %{\n+  predicate(UsePopCountInstruction);\n+  match(Set dst (PopCountL (LoadL mem)));\n+  effect(KILL cr);\n+\n+  format %{ \"popcnt  $dst, $mem\" %}\n+  ins_encode %{\n+    __ popcntq($dst$$Register, $mem$$Address);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+\n+\/\/----------MemBar Instructions-----------------------------------------------\n+\/\/ Memory barrier flavors\n+\n+instruct membar_acquire()\n+%{\n+  match(MemBarAcquire);\n+  match(LoadFence);\n+  ins_cost(0);\n+\n+  size(0);\n+  format %{ \"MEMBAR-acquire ! (empty encoding)\" %}\n+  ins_encode();\n+  ins_pipe(empty);\n+%}\n+\n+instruct membar_acquire_lock()\n+%{\n+  match(MemBarAcquireLock);\n+  ins_cost(0);\n+\n+  size(0);\n+  format %{ \"MEMBAR-acquire (prior CMPXCHG in FastLock so empty encoding)\" %}\n+  ins_encode();\n+  ins_pipe(empty);\n+%}\n+\n+instruct membar_release()\n+%{\n+  match(MemBarRelease);\n+  match(StoreFence);\n+  ins_cost(0);\n+\n+  size(0);\n+  format %{ \"MEMBAR-release ! (empty encoding)\" %}\n+  ins_encode();\n+  ins_pipe(empty);\n+%}\n+\n+instruct membar_release_lock()\n+%{\n+  match(MemBarReleaseLock);\n+  ins_cost(0);\n+\n+  size(0);\n+  format %{ \"MEMBAR-release (a FastUnlock follows so empty encoding)\" %}\n+  ins_encode();\n+  ins_pipe(empty);\n+%}\n+\n+instruct membar_volatile(rFlagsReg cr) %{\n+  match(MemBarVolatile);\n+  effect(KILL cr);\n+  ins_cost(400);\n+\n+  format %{\n+    $$template\n+    $$emit$$\"lock addl [rsp + #0], 0\\t! membar_volatile\"\n+  %}\n+  ins_encode %{\n+    __ membar(Assembler::StoreLoad);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct unnecessary_membar_volatile()\n+%{\n+  match(MemBarVolatile);\n+  predicate(Matcher::post_store_load_barrier(n));\n+  ins_cost(0);\n+\n+  size(0);\n+  format %{ \"MEMBAR-volatile (unnecessary so empty encoding)\" %}\n+  ins_encode();\n+  ins_pipe(empty);\n+%}\n+\n+instruct membar_storestore() %{\n+  match(MemBarStoreStore);\n+  match(StoreStoreFence);\n+  ins_cost(0);\n+\n+  size(0);\n+  format %{ \"MEMBAR-storestore (empty encoding)\" %}\n+  ins_encode( );\n+  ins_pipe(empty);\n+%}\n+\n+\/\/----------Move Instructions--------------------------------------------------\n+\n+instruct castX2P(rRegP dst, rRegL src)\n+%{\n+  match(Set dst (CastX2P src));\n+\n+  format %{ \"movq    $dst, $src\\t# long->ptr\" %}\n+  ins_encode %{\n+    if ($dst$$reg != $src$$reg) {\n+      __ movptr($dst$$Register, $src$$Register);\n+    }\n+  %}\n+  ins_pipe(ialu_reg_reg); \/\/ XXX\n+%}\n+\n+instruct castI2N(rRegN dst, rRegI src)\n+%{\n+  match(Set dst (CastI2N src));\n+\n+  format %{ \"movq    $dst, $src\\t# int -> narrow ptr\" %}\n+  ins_encode %{\n+    if ($dst$$reg != $src$$reg) {\n+      __ movl($dst$$Register, $src$$Register);\n+    }\n+  %}\n+  ins_pipe(ialu_reg_reg); \/\/ XXX\n+%}\n+\n+instruct castN2X(rRegL dst, rRegN src)\n+%{\n+  match(Set dst (CastP2X src));\n+\n+  format %{ \"movq    $dst, $src\\t# ptr -> long\" %}\n+  ins_encode %{\n+    if ($dst$$reg != $src$$reg) {\n+      __ movptr($dst$$Register, $src$$Register);\n+    }\n+  %}\n+  ins_pipe(ialu_reg_reg); \/\/ XXX\n+%}\n+\n+instruct castP2X(rRegL dst, rRegP src)\n+%{\n+  match(Set dst (CastP2X src));\n+\n+  format %{ \"movq    $dst, $src\\t# ptr -> long\" %}\n+  ins_encode %{\n+    if ($dst$$reg != $src$$reg) {\n+      __ movptr($dst$$Register, $src$$Register);\n+    }\n+  %}\n+  ins_pipe(ialu_reg_reg); \/\/ XXX\n+%}\n+\n+\/\/ Convert oop into int for vectors alignment masking\n+instruct convP2I(rRegI dst, rRegP src)\n+%{\n+  match(Set dst (ConvL2I (CastP2X src)));\n+\n+  format %{ \"movl    $dst, $src\\t# ptr -> int\" %}\n+  ins_encode %{\n+    __ movl($dst$$Register, $src$$Register);\n+  %}\n+  ins_pipe(ialu_reg_reg); \/\/ XXX\n+%}\n+\n+\/\/ Convert compressed oop into int for vectors alignment masking\n+\/\/ in case of 32bit oops (heap < 4Gb).\n+instruct convN2I(rRegI dst, rRegN src)\n+%{\n+  predicate(CompressedOops::shift() == 0);\n+  match(Set dst (ConvL2I (CastP2X (DecodeN src))));\n+\n+  format %{ \"movl    $dst, $src\\t# compressed ptr -> int\" %}\n+  ins_encode %{\n+    __ movl($dst$$Register, $src$$Register);\n+  %}\n+  ins_pipe(ialu_reg_reg); \/\/ XXX\n+%}\n+\n+\/\/ Convert oop pointer into compressed form\n+instruct encodeHeapOop(rRegN dst, rRegP src, rFlagsReg cr) %{\n+  predicate(n->bottom_type()->make_ptr()->ptr() != TypePtr::NotNull);\n+  match(Set dst (EncodeP src));\n+  effect(KILL cr);\n+  format %{ \"encode_heap_oop $dst,$src\" %}\n+  ins_encode %{\n+    Register s = $src$$Register;\n+    Register d = $dst$$Register;\n+    if (s != d) {\n+      __ movq(d, s);\n+    }\n+    __ encode_heap_oop(d);\n+  %}\n+  ins_pipe(ialu_reg_long);\n+%}\n+\n+instruct encodeHeapOop_not_null(rRegN dst, rRegP src, rFlagsReg cr) %{\n+  predicate(n->bottom_type()->make_ptr()->ptr() == TypePtr::NotNull);\n+  match(Set dst (EncodeP src));\n+  effect(KILL cr);\n+  format %{ \"encode_heap_oop_not_null $dst,$src\" %}\n+  ins_encode %{\n+    __ encode_heap_oop_not_null($dst$$Register, $src$$Register);\n+  %}\n+  ins_pipe(ialu_reg_long);\n+%}\n+\n+instruct decodeHeapOop(rRegP dst, rRegN src, rFlagsReg cr) %{\n+  predicate(n->bottom_type()->is_ptr()->ptr() != TypePtr::NotNull &&\n+            n->bottom_type()->is_ptr()->ptr() != TypePtr::Constant);\n+  match(Set dst (DecodeN src));\n+  effect(KILL cr);\n+  format %{ \"decode_heap_oop $dst,$src\" %}\n+  ins_encode %{\n+    Register s = $src$$Register;\n+    Register d = $dst$$Register;\n+    if (s != d) {\n+      __ movq(d, s);\n+    }\n+    __ decode_heap_oop(d);\n+  %}\n+  ins_pipe(ialu_reg_long);\n+%}\n+\n+instruct decodeHeapOop_not_null(rRegP dst, rRegN src, rFlagsReg cr) %{\n+  predicate(n->bottom_type()->is_ptr()->ptr() == TypePtr::NotNull ||\n+            n->bottom_type()->is_ptr()->ptr() == TypePtr::Constant);\n+  match(Set dst (DecodeN src));\n+  effect(KILL cr);\n+  format %{ \"decode_heap_oop_not_null $dst,$src\" %}\n+  ins_encode %{\n+    Register s = $src$$Register;\n+    Register d = $dst$$Register;\n+    if (s != d) {\n+      __ decode_heap_oop_not_null(d, s);\n+    } else {\n+      __ decode_heap_oop_not_null(d);\n+    }\n+  %}\n+  ins_pipe(ialu_reg_long);\n+%}\n+\n+instruct encodeKlass_not_null(rRegN dst, rRegP src, rFlagsReg cr) %{\n+  match(Set dst (EncodePKlass src));\n+  effect(TEMP dst, KILL cr);\n+  format %{ \"encode_and_move_klass_not_null $dst,$src\" %}\n+  ins_encode %{\n+    __ encode_and_move_klass_not_null($dst$$Register, $src$$Register);\n+  %}\n+  ins_pipe(ialu_reg_long);\n+%}\n+\n+instruct decodeKlass_not_null(rRegP dst, rRegN src, rFlagsReg cr) %{\n+  match(Set dst (DecodeNKlass src));\n+  effect(TEMP dst, KILL cr);\n+  format %{ \"decode_and_move_klass_not_null $dst,$src\" %}\n+  ins_encode %{\n+    __ decode_and_move_klass_not_null($dst$$Register, $src$$Register);\n+  %}\n+  ins_pipe(ialu_reg_long);\n+%}\n+\n+\/\/----------Conditional Move---------------------------------------------------\n+\/\/ Jump\n+\/\/ dummy instruction for generating temp registers\n+instruct jumpXtnd_offset(rRegL switch_val, immI2 shift, rRegI dest) %{\n+  match(Jump (LShiftL switch_val shift));\n+  ins_cost(350);\n+  predicate(false);\n+  effect(TEMP dest);\n+\n+  format %{ \"leaq    $dest, [$constantaddress]\\n\\t\"\n+            \"jmp     [$dest + $switch_val << $shift]\\n\\t\" %}\n+  ins_encode %{\n+    \/\/ We could use jump(ArrayAddress) except that the macro assembler needs to use r10\n+    \/\/ to do that and the compiler is using that register as one it can allocate.\n+    \/\/ So we build it all by hand.\n+    \/\/ Address index(noreg, switch_reg, (Address::ScaleFactor)$shift$$constant);\n+    \/\/ ArrayAddress dispatch(table, index);\n+    Address dispatch($dest$$Register, $switch_val$$Register, (Address::ScaleFactor) $shift$$constant);\n+    __ lea($dest$$Register, $constantaddress);\n+    __ jmp(dispatch);\n+  %}\n+  ins_pipe(pipe_jmp);\n+%}\n+\n+instruct jumpXtnd_addr(rRegL switch_val, immI2 shift, immL32 offset, rRegI dest) %{\n+  match(Jump (AddL (LShiftL switch_val shift) offset));\n+  ins_cost(350);\n+  effect(TEMP dest);\n+\n+  format %{ \"leaq    $dest, [$constantaddress]\\n\\t\"\n+            \"jmp     [$dest + $switch_val << $shift + $offset]\\n\\t\" %}\n+  ins_encode %{\n+    \/\/ We could use jump(ArrayAddress) except that the macro assembler needs to use r10\n+    \/\/ to do that and the compiler is using that register as one it can allocate.\n+    \/\/ So we build it all by hand.\n+    \/\/ Address index(noreg, switch_reg, (Address::ScaleFactor) $shift$$constant, (int) $offset$$constant);\n+    \/\/ ArrayAddress dispatch(table, index);\n+    Address dispatch($dest$$Register, $switch_val$$Register, (Address::ScaleFactor) $shift$$constant, (int) $offset$$constant);\n+    __ lea($dest$$Register, $constantaddress);\n+    __ jmp(dispatch);\n+  %}\n+  ins_pipe(pipe_jmp);\n+%}\n+\n+instruct jumpXtnd(rRegL switch_val, rRegI dest) %{\n+  match(Jump switch_val);\n+  ins_cost(350);\n+  effect(TEMP dest);\n+\n+  format %{ \"leaq    $dest, [$constantaddress]\\n\\t\"\n+            \"jmp     [$dest + $switch_val]\\n\\t\" %}\n+  ins_encode %{\n+    \/\/ We could use jump(ArrayAddress) except that the macro assembler needs to use r10\n+    \/\/ to do that and the compiler is using that register as one it can allocate.\n+    \/\/ So we build it all by hand.\n+    \/\/ Address index(noreg, switch_reg, Address::times_1);\n+    \/\/ ArrayAddress dispatch(table, index);\n+    Address dispatch($dest$$Register, $switch_val$$Register, Address::times_1);\n+    __ lea($dest$$Register, $constantaddress);\n+    __ jmp(dispatch);\n+  %}\n+  ins_pipe(pipe_jmp);\n+%}\n+\n+\/\/ Conditional move\n+instruct cmovI_imm_01(rRegI dst, immI_1 src, rFlagsReg cr, cmpOp cop)\n+%{\n+  predicate(n->in(2)->in(2)->is_Con() && n->in(2)->in(2)->get_int() == 0);\n+  match(Set dst (CMoveI (Binary cop cr) (Binary src dst)));\n+\n+  ins_cost(100); \/\/ XXX\n+  format %{ \"setbn$cop $dst\\t# signed, int\" %}\n+  ins_encode %{\n+    Assembler::Condition cond = (Assembler::Condition)($cop$$cmpcode);\n+    __ setb(MacroAssembler::negate_condition(cond), $dst$$Register);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct cmovI_reg(rRegI dst, rRegI src, rFlagsReg cr, cmpOp cop)\n+%{\n+  predicate(!UseAPX);\n+  match(Set dst (CMoveI (Binary cop cr) (Binary dst src)));\n+\n+  ins_cost(200); \/\/ XXX\n+  format %{ \"cmovl$cop $dst, $src\\t# signed, int\" %}\n+  ins_encode %{\n+    __ cmovl((Assembler::Condition)($cop$$cmpcode), $dst$$Register, $src$$Register);\n+  %}\n+  ins_pipe(pipe_cmov_reg);\n+%}\n+\n+instruct cmovI_reg_ndd(rRegI dst, rRegI src1, rRegI src2, rFlagsReg cr, cmpOp cop)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (CMoveI (Binary cop cr) (Binary src1 src2)));\n+\n+  ins_cost(200);\n+  format %{ \"ecmovl$cop $dst, $src1, $src2\\t# signed, int ndd\" %}\n+  ins_encode %{\n+    __ ecmovl((Assembler::Condition)($cop$$cmpcode), $dst$$Register, $src1$$Register, $src2$$Register);\n+  %}\n+  ins_pipe(pipe_cmov_reg);\n+%}\n+\n+instruct cmovI_imm_01U(rRegI dst, immI_1 src, rFlagsRegU cr, cmpOpU cop)\n+%{\n+  predicate(n->in(2)->in(2)->is_Con() && n->in(2)->in(2)->get_int() == 0);\n+  match(Set dst (CMoveI (Binary cop cr) (Binary src dst)));\n+\n+  ins_cost(100); \/\/ XXX\n+  format %{ \"setbn$cop $dst\\t# unsigned, int\" %}\n+  ins_encode %{\n+    Assembler::Condition cond = (Assembler::Condition)($cop$$cmpcode);\n+    __ setb(MacroAssembler::negate_condition(cond), $dst$$Register);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct cmovI_regU(cmpOpU cop, rFlagsRegU cr, rRegI dst, rRegI src) %{\n+  predicate(!UseAPX);\n+  match(Set dst (CMoveI (Binary cop cr) (Binary dst src)));\n+\n+  ins_cost(200); \/\/ XXX\n+  format %{ \"cmovl$cop $dst, $src\\t# unsigned, int\" %}\n+  ins_encode %{\n+    __ cmovl((Assembler::Condition)($cop$$cmpcode), $dst$$Register, $src$$Register);\n+  %}\n+  ins_pipe(pipe_cmov_reg);\n+%}\n+\n+instruct cmovI_regU_ndd(rRegI dst, cmpOpU cop, rFlagsRegU cr, rRegI src1, rRegI src2) %{\n+  predicate(UseAPX);\n+  match(Set dst (CMoveI (Binary cop cr) (Binary src1 src2)));\n+\n+  ins_cost(200);\n+  format %{ \"ecmovl$cop $dst, $src1, $src2\\t# unsigned, int ndd\" %}\n+  ins_encode %{\n+    __ ecmovl((Assembler::Condition)($cop$$cmpcode), $dst$$Register, $src1$$Register, $src2$$Register);\n+  %}\n+  ins_pipe(pipe_cmov_reg);\n+%}\n+\n+instruct cmovI_imm_01UCF(rRegI dst, immI_1 src, rFlagsRegUCF cr, cmpOpUCF cop)\n+%{\n+  predicate(n->in(2)->in(2)->is_Con() && n->in(2)->in(2)->get_int() == 0);\n+  match(Set dst (CMoveI (Binary cop cr) (Binary src dst)));\n+\n+  ins_cost(100); \/\/ XXX\n+  format %{ \"setbn$cop $dst\\t# unsigned, int\" %}\n+  ins_encode %{\n+    Assembler::Condition cond = (Assembler::Condition)($cop$$cmpcode);\n+    __ setb(MacroAssembler::negate_condition(cond), $dst$$Register);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct cmovI_regUCF(cmpOpUCF cop, rFlagsRegUCF cr, rRegI dst, rRegI src) %{\n+  predicate(!UseAPX);\n+  match(Set dst (CMoveI (Binary cop cr) (Binary dst src)));\n+  ins_cost(200);\n+  expand %{\n+    cmovI_regU(cop, cr, dst, src);\n+  %}\n+%}\n+\n+instruct cmovI_regUCF_ndd(rRegI dst, cmpOpUCF cop, rFlagsRegUCF cr, rRegI src1, rRegI src2) %{\n+  predicate(UseAPX);\n+  match(Set dst (CMoveI (Binary cop cr) (Binary src1 src2)));\n+  ins_cost(200);\n+  format %{ \"ecmovl$cop $dst, $src1, $src2\\t# unsigned, int ndd\" %}\n+  ins_encode %{\n+    __ ecmovl((Assembler::Condition)($cop$$cmpcode), $dst$$Register, $src1$$Register, $src2$$Register);\n+  %}\n+  ins_pipe(pipe_cmov_reg);\n+%}\n+\n+instruct cmovI_regUCF2_ne(cmpOpUCF2 cop, rFlagsRegUCF cr, rRegI dst, rRegI src) %{\n+  predicate(!UseAPX && n->in(1)->in(1)->as_Bool()->_test._test == BoolTest::ne);\n+  match(Set dst (CMoveI (Binary cop cr) (Binary dst src)));\n+\n+  ins_cost(200); \/\/ XXX\n+  format %{ \"cmovpl  $dst, $src\\n\\t\"\n+            \"cmovnel $dst, $src\" %}\n+  ins_encode %{\n+    __ cmovl(Assembler::parity, $dst$$Register, $src$$Register);\n+    __ cmovl(Assembler::notEqual, $dst$$Register, $src$$Register);\n+  %}\n+  ins_pipe(pipe_cmov_reg);\n+%}\n+\n+instruct cmovI_regUCF2_ne_ndd(cmpOpUCF2 cop, rFlagsRegUCF cr, rRegI dst, rRegI src1, rRegI src2) %{\n+  predicate(UseAPX && n->in(1)->in(1)->as_Bool()->_test._test == BoolTest::ne);\n+  match(Set dst (CMoveI (Binary cop cr) (Binary src1 src2)));\n+  effect(TEMP dst);\n+\n+  ins_cost(200);\n+  format %{ \"ecmovpl  $dst, $src1, $src2\\n\\t\"\n+            \"cmovnel  $dst, $src2\" %}\n+  ins_encode %{\n+    __ ecmovl(Assembler::parity, $dst$$Register, $src1$$Register, $src2$$Register);\n+    __ cmovl(Assembler::notEqual, $dst$$Register, $src2$$Register);\n+  %}\n+  ins_pipe(pipe_cmov_reg);\n+%}\n+\n+\/\/ Since (x == y) == !(x != y), we can flip the sense of the test by flipping the\n+\/\/ inputs of the CMove\n+instruct cmovI_regUCF2_eq(cmpOpUCF2 cop, rFlagsRegUCF cr, rRegI dst, rRegI src) %{\n+  predicate(!UseAPX && n->in(1)->in(1)->as_Bool()->_test._test == BoolTest::eq);\n+  match(Set dst (CMoveI (Binary cop cr) (Binary src dst)));\n+  effect(TEMP dst);\n+\n+  ins_cost(200); \/\/ XXX\n+  format %{ \"cmovpl  $dst, $src\\n\\t\"\n+            \"cmovnel $dst, $src\" %}\n+  ins_encode %{\n+    __ cmovl(Assembler::parity, $dst$$Register, $src$$Register);\n+    __ cmovl(Assembler::notEqual, $dst$$Register, $src$$Register);\n+  %}\n+  ins_pipe(pipe_cmov_reg);\n+%}\n+\n+\/\/ We need this special handling for only eq \/ neq comparison since NaN == NaN is false,\n+\/\/ and parity flag bit is set if any of the operand is a NaN.\n+instruct cmovI_regUCF2_eq_ndd(cmpOpUCF2 cop, rFlagsRegUCF cr, rRegI dst, rRegI src1, rRegI src2) %{\n+  predicate(UseAPX && n->in(1)->in(1)->as_Bool()->_test._test == BoolTest::eq);\n+  match(Set dst (CMoveI (Binary cop cr) (Binary src2 src1)));\n+  effect(TEMP dst);\n+\n+  ins_cost(200);\n+  format %{ \"ecmovpl  $dst, $src1, $src2\\n\\t\"\n+            \"cmovnel  $dst, $src2\" %}\n+  ins_encode %{\n+    __ ecmovl(Assembler::parity, $dst$$Register, $src1$$Register, $src2$$Register);\n+    __ cmovl(Assembler::notEqual, $dst$$Register, $src2$$Register);\n+  %}\n+  ins_pipe(pipe_cmov_reg);\n+%}\n+\n+\/\/ Conditional move\n+instruct cmovI_mem(cmpOp cop, rFlagsReg cr, rRegI dst, memory src) %{\n+  predicate(!UseAPX);\n+  match(Set dst (CMoveI (Binary cop cr) (Binary dst (LoadI src))));\n+\n+  ins_cost(250); \/\/ XXX\n+  format %{ \"cmovl$cop $dst, $src\\t# signed, int\" %}\n+  ins_encode %{\n+    __ cmovl((Assembler::Condition)($cop$$cmpcode), $dst$$Register, $src$$Address);\n+  %}\n+  ins_pipe(pipe_cmov_mem);\n+%}\n+\n+\/\/ Conditional move\n+instruct cmovI_rReg_rReg_mem_ndd(rRegI dst, cmpOp cop, rFlagsReg cr, rRegI src1, memory src2)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (CMoveI (Binary cop cr) (Binary src1 (LoadI src2))));\n+\n+  ins_cost(250);\n+  format %{ \"ecmovl$cop $dst, $src1, $src2\\t# signed, int ndd\" %}\n+  ins_encode %{\n+    __ ecmovl((Assembler::Condition)($cop$$cmpcode), $dst$$Register, $src1$$Register, $src2$$Address);\n+  %}\n+  ins_pipe(pipe_cmov_mem);\n+%}\n+\n+\/\/ Conditional move\n+instruct cmovI_memU(cmpOpU cop, rFlagsRegU cr, rRegI dst, memory src)\n+%{\n+  predicate(!UseAPX);\n+  match(Set dst (CMoveI (Binary cop cr) (Binary dst (LoadI src))));\n+\n+  ins_cost(250); \/\/ XXX\n+  format %{ \"cmovl$cop $dst, $src\\t# unsigned, int\" %}\n+  ins_encode %{\n+    __ cmovl((Assembler::Condition)($cop$$cmpcode), $dst$$Register, $src$$Address);\n+  %}\n+  ins_pipe(pipe_cmov_mem);\n+%}\n+\n+instruct cmovI_memUCF(cmpOpUCF cop, rFlagsRegUCF cr, rRegI dst, memory src) %{\n+  predicate(!UseAPX);\n+  match(Set dst (CMoveI (Binary cop cr) (Binary dst (LoadI src))));\n+  ins_cost(250);\n+  expand %{\n+    cmovI_memU(cop, cr, dst, src);\n+  %}\n+%}\n+\n+instruct cmovI_rReg_rReg_memU_ndd(rRegI dst, cmpOpU cop, rFlagsRegU cr, rRegI src1, memory src2)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (CMoveI (Binary cop cr) (Binary src1 (LoadI src2))));\n+\n+  ins_cost(250);\n+  format %{ \"ecmovl$cop $dst, $src1, $src2\\t# unsigned, int ndd\" %}\n+  ins_encode %{\n+    __ ecmovl((Assembler::Condition)($cop$$cmpcode), $dst$$Register, $src1$$Register, $src2$$Address);\n+  %}\n+  ins_pipe(pipe_cmov_mem);\n+%}\n+\n+instruct cmovI_rReg_rReg_memUCF_ndd(rRegI dst, cmpOpUCF cop, rFlagsRegUCF cr, rRegI src1, memory src2)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (CMoveI (Binary cop cr) (Binary src1 (LoadI src2))));\n+  ins_cost(250);\n+  format %{ \"ecmovl$cop $dst, $src1, $src2\\t# unsigned, int ndd\" %}\n+  ins_encode %{\n+    __ ecmovl((Assembler::Condition)($cop$$cmpcode), $dst$$Register, $src1$$Register, $src2$$Address);\n+  %}\n+  ins_pipe(pipe_cmov_mem);\n+%}\n+\n+\/\/ Conditional move\n+instruct cmovN_reg(rRegN dst, rRegN src, rFlagsReg cr, cmpOp cop)\n+%{\n+  predicate(!UseAPX);\n+  match(Set dst (CMoveN (Binary cop cr) (Binary dst src)));\n+\n+  ins_cost(200); \/\/ XXX\n+  format %{ \"cmovl$cop $dst, $src\\t# signed, compressed ptr\" %}\n+  ins_encode %{\n+    __ cmovl((Assembler::Condition)($cop$$cmpcode), $dst$$Register, $src$$Register);\n+  %}\n+  ins_pipe(pipe_cmov_reg);\n+%}\n+\n+\/\/ Conditional move ndd\n+instruct cmovN_reg_ndd(rRegN dst, rRegN src1, rRegN src2, rFlagsReg cr, cmpOp cop)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (CMoveN (Binary cop cr) (Binary src1 src2)));\n+\n+  ins_cost(200);\n+  format %{ \"ecmovl$cop $dst, $src1, $src2\\t# signed, compressed ptr ndd\" %}\n+  ins_encode %{\n+    __ ecmovl((Assembler::Condition)($cop$$cmpcode), $dst$$Register, $src1$$Register, $src2$$Register);\n+  %}\n+  ins_pipe(pipe_cmov_reg);\n+%}\n+\n+\/\/ Conditional move\n+instruct cmovN_regU(cmpOpU cop, rFlagsRegU cr, rRegN dst, rRegN src)\n+%{\n+  predicate(!UseAPX);\n+  match(Set dst (CMoveN (Binary cop cr) (Binary dst src)));\n+\n+  ins_cost(200); \/\/ XXX\n+  format %{ \"cmovl$cop $dst, $src\\t# unsigned, compressed ptr\" %}\n+  ins_encode %{\n+    __ cmovl((Assembler::Condition)($cop$$cmpcode), $dst$$Register, $src$$Register);\n+  %}\n+  ins_pipe(pipe_cmov_reg);\n+%}\n+\n+instruct cmovN_regUCF(cmpOpUCF cop, rFlagsRegUCF cr, rRegN dst, rRegN src) %{\n+  predicate(!UseAPX);\n+  match(Set dst (CMoveN (Binary cop cr) (Binary dst src)));\n+  ins_cost(200);\n+  expand %{\n+    cmovN_regU(cop, cr, dst, src);\n+  %}\n+%}\n+\n+\/\/ Conditional move ndd\n+instruct cmovN_regU_ndd(rRegN dst, cmpOpU cop, rFlagsRegU cr, rRegN src1, rRegN src2)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (CMoveN (Binary cop cr) (Binary src1 src2)));\n+\n+  ins_cost(200);\n+  format %{ \"ecmovl$cop $dst, $src1, $src2\\t# unsigned, compressed ptr ndd\" %}\n+  ins_encode %{\n+    __ ecmovl((Assembler::Condition)($cop$$cmpcode), $dst$$Register, $src1$$Register, $src2$$Register);\n+  %}\n+  ins_pipe(pipe_cmov_reg);\n+%}\n+\n+instruct cmovN_regUCF_ndd(rRegN dst, cmpOpUCF cop, rFlagsRegUCF cr, rRegN src1, rRegN src2) %{\n+  predicate(UseAPX);\n+  match(Set dst (CMoveN (Binary cop cr) (Binary src1 src2)));\n+  ins_cost(200);\n+  format %{ \"ecmovl$cop $dst, $src1, $src2\\t# unsigned, compressed ptr ndd\" %}\n+  ins_encode %{\n+    __ ecmovl((Assembler::Condition)($cop$$cmpcode), $dst$$Register, $src1$$Register, $src2$$Register);\n+  %}\n+  ins_pipe(pipe_cmov_reg);\n+%}\n+\n+instruct cmovN_regUCF2_ne(cmpOpUCF2 cop, rFlagsRegUCF cr, rRegN dst, rRegN src) %{\n+  predicate(n->in(1)->in(1)->as_Bool()->_test._test == BoolTest::ne);\n+  match(Set dst (CMoveN (Binary cop cr) (Binary dst src)));\n+\n+  ins_cost(200); \/\/ XXX\n+  format %{ \"cmovpl  $dst, $src\\n\\t\"\n+            \"cmovnel $dst, $src\" %}\n+  ins_encode %{\n+    __ cmovl(Assembler::parity, $dst$$Register, $src$$Register);\n+    __ cmovl(Assembler::notEqual, $dst$$Register, $src$$Register);\n+  %}\n+  ins_pipe(pipe_cmov_reg);\n+%}\n+\n+\/\/ Since (x == y) == !(x != y), we can flip the sense of the test by flipping the\n+\/\/ inputs of the CMove\n+instruct cmovN_regUCF2_eq(cmpOpUCF2 cop, rFlagsRegUCF cr, rRegN dst, rRegN src) %{\n+  predicate(n->in(1)->in(1)->as_Bool()->_test._test == BoolTest::eq);\n+  match(Set dst (CMoveN (Binary cop cr) (Binary src dst)));\n+\n+  ins_cost(200); \/\/ XXX\n+  format %{ \"cmovpl  $dst, $src\\n\\t\"\n+            \"cmovnel $dst, $src\" %}\n+  ins_encode %{\n+    __ cmovl(Assembler::parity, $dst$$Register, $src$$Register);\n+    __ cmovl(Assembler::notEqual, $dst$$Register, $src$$Register);\n+  %}\n+  ins_pipe(pipe_cmov_reg);\n+%}\n+\n+\/\/ Conditional move\n+instruct cmovP_reg(rRegP dst, rRegP src, rFlagsReg cr, cmpOp cop)\n+%{\n+  predicate(!UseAPX);\n+  match(Set dst (CMoveP (Binary cop cr) (Binary dst src)));\n+\n+  ins_cost(200); \/\/ XXX\n+  format %{ \"cmovq$cop $dst, $src\\t# signed, ptr\" %}\n+  ins_encode %{\n+    __ cmovq((Assembler::Condition)($cop$$cmpcode), $dst$$Register, $src$$Register);\n+  %}\n+  ins_pipe(pipe_cmov_reg);  \/\/ XXX\n+%}\n+\n+\/\/ Conditional move ndd\n+instruct cmovP_reg_ndd(rRegP dst, rRegP src1, rRegP src2, rFlagsReg cr, cmpOp cop)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (CMoveP (Binary cop cr) (Binary src1 src2)));\n+\n+  ins_cost(200);\n+  format %{ \"ecmovq$cop $dst, $src1, $src2\\t# signed, ptr ndd\" %}\n+  ins_encode %{\n+    __ ecmovq((Assembler::Condition)($cop$$cmpcode), $dst$$Register, $src1$$Register, $src2$$Register);\n+  %}\n+  ins_pipe(pipe_cmov_reg);\n+%}\n+\n+\/\/ Conditional move\n+instruct cmovP_regU(cmpOpU cop, rFlagsRegU cr, rRegP dst, rRegP src)\n+%{\n+  predicate(!UseAPX);\n+  match(Set dst (CMoveP (Binary cop cr) (Binary dst src)));\n+\n+  ins_cost(200); \/\/ XXX\n+  format %{ \"cmovq$cop $dst, $src\\t# unsigned, ptr\" %}\n+  ins_encode %{\n+    __ cmovq((Assembler::Condition)($cop$$cmpcode), $dst$$Register, $src$$Register);\n+  %}\n+  ins_pipe(pipe_cmov_reg); \/\/ XXX\n+%}\n+\n+\/\/ Conditional move ndd\n+instruct cmovP_regU_ndd(rRegP dst, cmpOpU cop, rFlagsRegU cr, rRegP src1, rRegP src2)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (CMoveP (Binary cop cr) (Binary src1 src2)));\n+\n+  ins_cost(200);\n+  format %{ \"ecmovq$cop $dst, $src1, $src2\\t# unsigned, ptr ndd\" %}\n+  ins_encode %{\n+    __ ecmovq((Assembler::Condition)($cop$$cmpcode), $dst$$Register, $src1$$Register, $src2$$Register);\n+  %}\n+  ins_pipe(pipe_cmov_reg);\n+%}\n+\n+instruct cmovP_regUCF(cmpOpUCF cop, rFlagsRegUCF cr, rRegP dst, rRegP src) %{\n+  predicate(!UseAPX);\n+  match(Set dst (CMoveP (Binary cop cr) (Binary dst src)));\n+  ins_cost(200);\n+  expand %{\n+    cmovP_regU(cop, cr, dst, src);\n+  %}\n+%}\n+\n+instruct cmovP_regUCF_ndd(rRegP dst, cmpOpUCF cop, rFlagsRegUCF cr, rRegP src1, rRegP src2) %{\n+  predicate(UseAPX);\n+  match(Set dst (CMoveP (Binary cop cr) (Binary src1 src2)));\n+  ins_cost(200);\n+  format %{ \"ecmovq$cop $dst, $src1, $src2\\t# unsigned, ptr ndd\" %}\n+  ins_encode %{\n+    __ ecmovq((Assembler::Condition)($cop$$cmpcode), $dst$$Register, $src1$$Register, $src2$$Register);\n+  %}\n+  ins_pipe(pipe_cmov_reg);\n+%}\n+\n+instruct cmovP_regUCF2_ne(cmpOpUCF2 cop, rFlagsRegUCF cr, rRegP dst, rRegP src) %{\n+  predicate(!UseAPX && n->in(1)->in(1)->as_Bool()->_test._test == BoolTest::ne);\n+  match(Set dst (CMoveP (Binary cop cr) (Binary dst src)));\n+\n+  ins_cost(200); \/\/ XXX\n+  format %{ \"cmovpq  $dst, $src\\n\\t\"\n+            \"cmovneq $dst, $src\" %}\n+  ins_encode %{\n+    __ cmovq(Assembler::parity, $dst$$Register, $src$$Register);\n+    __ cmovq(Assembler::notEqual, $dst$$Register, $src$$Register);\n+  %}\n+  ins_pipe(pipe_cmov_reg);\n+%}\n+\n+instruct cmovP_regUCF2_ne_ndd(cmpOpUCF2 cop, rFlagsRegUCF cr, rRegP dst, rRegP src1, rRegP src2) %{\n+  predicate(UseAPX && n->in(1)->in(1)->as_Bool()->_test._test == BoolTest::ne);\n+  match(Set dst (CMoveP (Binary cop cr) (Binary src1 src2)));\n+  effect(TEMP dst);\n+\n+  ins_cost(200);\n+  format %{ \"ecmovpq  $dst, $src1, $src2\\n\\t\"\n+            \"cmovneq  $dst, $src2\" %}\n+  ins_encode %{\n+    __ ecmovq(Assembler::parity, $dst$$Register, $src1$$Register, $src2$$Register);\n+    __ cmovq(Assembler::notEqual, $dst$$Register, $src2$$Register);\n+  %}\n+  ins_pipe(pipe_cmov_reg);\n+%}\n+\n+\/\/ Since (x == y) == !(x != y), we can flip the sense of the test by flipping the\n+\/\/ inputs of the CMove\n+instruct cmovP_regUCF2_eq(cmpOpUCF2 cop, rFlagsRegUCF cr, rRegP dst, rRegP src) %{\n+  predicate(!UseAPX && n->in(1)->in(1)->as_Bool()->_test._test == BoolTest::eq);\n+  match(Set dst (CMoveP (Binary cop cr) (Binary src dst)));\n+\n+  ins_cost(200); \/\/ XXX\n+  format %{ \"cmovpq  $dst, $src\\n\\t\"\n+            \"cmovneq $dst, $src\" %}\n+  ins_encode %{\n+    __ cmovq(Assembler::parity, $dst$$Register, $src$$Register);\n+    __ cmovq(Assembler::notEqual, $dst$$Register, $src$$Register);\n+  %}\n+  ins_pipe(pipe_cmov_reg);\n+%}\n+\n+instruct cmovP_regUCF2_eq_ndd(cmpOpUCF2 cop, rFlagsRegUCF cr, rRegP dst, rRegP src1, rRegP src2) %{\n+  predicate(UseAPX && n->in(1)->in(1)->as_Bool()->_test._test == BoolTest::eq);\n+  match(Set dst (CMoveP (Binary cop cr) (Binary src2 src1)));\n+  effect(TEMP dst);\n+\n+  ins_cost(200);\n+  format %{ \"ecmovpq  $dst, $src1, $src2\\n\\t\"\n+            \"cmovneq  $dst, $src2\" %}\n+  ins_encode %{\n+    __ ecmovq(Assembler::parity, $dst$$Register, $src1$$Register, $src2$$Register);\n+    __ cmovq(Assembler::notEqual, $dst$$Register, $src2$$Register);\n+  %}\n+  ins_pipe(pipe_cmov_reg);\n+%}\n+\n+instruct cmovL_imm_01(rRegL dst, immL1 src, rFlagsReg cr, cmpOp cop)\n+%{\n+  predicate(n->in(2)->in(2)->is_Con() && n->in(2)->in(2)->get_long() == 0);\n+  match(Set dst (CMoveL (Binary cop cr) (Binary src dst)));\n+\n+  ins_cost(100); \/\/ XXX\n+  format %{ \"setbn$cop $dst\\t# signed, long\" %}\n+  ins_encode %{\n+    Assembler::Condition cond = (Assembler::Condition)($cop$$cmpcode);\n+    __ setb(MacroAssembler::negate_condition(cond), $dst$$Register);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct cmovL_reg(cmpOp cop, rFlagsReg cr, rRegL dst, rRegL src)\n+%{\n+  predicate(!UseAPX);\n+  match(Set dst (CMoveL (Binary cop cr) (Binary dst src)));\n+\n+  ins_cost(200); \/\/ XXX\n+  format %{ \"cmovq$cop $dst, $src\\t# signed, long\" %}\n+  ins_encode %{\n+    __ cmovq((Assembler::Condition)($cop$$cmpcode), $dst$$Register, $src$$Register);\n+  %}\n+  ins_pipe(pipe_cmov_reg);  \/\/ XXX\n+%}\n+\n+instruct cmovL_reg_ndd(rRegL dst, cmpOp cop, rFlagsReg cr, rRegL src1, rRegL src2)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (CMoveL (Binary cop cr) (Binary src1 src2)));\n+\n+  ins_cost(200);\n+  format %{ \"ecmovq$cop $dst, $src1, $src2\\t# signed, long ndd\" %}\n+  ins_encode %{\n+    __ ecmovq((Assembler::Condition)($cop$$cmpcode), $dst$$Register, $src1$$Register, $src2$$Register);\n+  %}\n+  ins_pipe(pipe_cmov_reg);\n+%}\n+\n+instruct cmovL_mem(cmpOp cop, rFlagsReg cr, rRegL dst, memory src)\n+%{\n+  predicate(!UseAPX);\n+  match(Set dst (CMoveL (Binary cop cr) (Binary dst (LoadL src))));\n+\n+  ins_cost(200); \/\/ XXX\n+  format %{ \"cmovq$cop $dst, $src\\t# signed, long\" %}\n+  ins_encode %{\n+    __ cmovq((Assembler::Condition)($cop$$cmpcode), $dst$$Register, $src$$Address);\n+  %}\n+  ins_pipe(pipe_cmov_mem);  \/\/ XXX\n+%}\n+\n+instruct cmovL_rReg_rReg_mem_ndd(rRegL dst, cmpOp cop, rFlagsReg cr, rRegL src1, memory src2)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (CMoveL (Binary cop cr) (Binary src1 (LoadL src2))));\n+\n+  ins_cost(200);\n+  format %{ \"ecmovq$cop $dst, $src1, $src2\\t# signed, long ndd\" %}\n+  ins_encode %{\n+    __ ecmovq((Assembler::Condition)($cop$$cmpcode), $dst$$Register, $src1$$Register, $src2$$Address);\n+  %}\n+  ins_pipe(pipe_cmov_mem);\n+%}\n+\n+instruct cmovL_imm_01U(rRegL dst, immL1 src, rFlagsRegU cr, cmpOpU cop)\n+%{\n+  predicate(n->in(2)->in(2)->is_Con() && n->in(2)->in(2)->get_long() == 0);\n+  match(Set dst (CMoveL (Binary cop cr) (Binary src dst)));\n+\n+  ins_cost(100); \/\/ XXX\n+  format %{ \"setbn$cop $dst\\t# unsigned, long\" %}\n+  ins_encode %{\n+    Assembler::Condition cond = (Assembler::Condition)($cop$$cmpcode);\n+    __ setb(MacroAssembler::negate_condition(cond), $dst$$Register);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct cmovL_regU(cmpOpU cop, rFlagsRegU cr, rRegL dst, rRegL src)\n+%{\n+  predicate(!UseAPX);\n+  match(Set dst (CMoveL (Binary cop cr) (Binary dst src)));\n+\n+  ins_cost(200); \/\/ XXX\n+  format %{ \"cmovq$cop $dst, $src\\t# unsigned, long\" %}\n+  ins_encode %{\n+    __ cmovq((Assembler::Condition)($cop$$cmpcode), $dst$$Register, $src$$Register);\n+  %}\n+  ins_pipe(pipe_cmov_reg); \/\/ XXX\n+%}\n+\n+instruct cmovL_regU_ndd(rRegL dst, cmpOpU cop, rFlagsRegU cr, rRegL src1, rRegL src2)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (CMoveL (Binary cop cr) (Binary src1 src2)));\n+\n+  ins_cost(200);\n+  format %{ \"ecmovq$cop $dst, $src1, $src2\\t# unsigned, long ndd\" %}\n+  ins_encode %{\n+    __ ecmovq((Assembler::Condition)($cop$$cmpcode), $dst$$Register, $src1$$Register, $src2$$Register);\n+  %}\n+  ins_pipe(pipe_cmov_reg);\n+%}\n+\n+instruct cmovL_imm_01UCF(rRegL dst, immL1 src, rFlagsRegUCF cr, cmpOpUCF cop)\n+%{\n+  predicate(n->in(2)->in(2)->is_Con() && n->in(2)->in(2)->get_long() == 0);\n+  match(Set dst (CMoveL (Binary cop cr) (Binary src dst)));\n+\n+  ins_cost(100); \/\/ XXX\n+  format %{ \"setbn$cop $dst\\t# unsigned, long\" %}\n+  ins_encode %{\n+    Assembler::Condition cond = (Assembler::Condition)($cop$$cmpcode);\n+    __ setb(MacroAssembler::negate_condition(cond), $dst$$Register);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct cmovL_regUCF(cmpOpUCF cop, rFlagsRegUCF cr, rRegL dst, rRegL src) %{\n+  predicate(!UseAPX);\n+  match(Set dst (CMoveL (Binary cop cr) (Binary dst src)));\n+  ins_cost(200);\n+  expand %{\n+    cmovL_regU(cop, cr, dst, src);\n+  %}\n+%}\n+\n+instruct cmovL_regUCF_ndd(rRegL dst, cmpOpUCF cop, rFlagsRegUCF cr, rRegL src1, rRegL src2)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (CMoveL (Binary cop cr) (Binary src1 src2)));\n+  ins_cost(200);\n+  format %{ \"ecmovq$cop $dst, $src1, $src2\\t# unsigned, long ndd\" %}\n+  ins_encode %{\n+    __ ecmovq((Assembler::Condition)($cop$$cmpcode), $dst$$Register, $src1$$Register, $src2$$Register);\n+  %}\n+  ins_pipe(pipe_cmov_reg);\n+%}\n+\n+instruct cmovL_regUCF2_ne(cmpOpUCF2 cop, rFlagsRegUCF cr, rRegL dst, rRegL src) %{\n+  predicate(!UseAPX && n->in(1)->in(1)->as_Bool()->_test._test == BoolTest::ne);\n+  match(Set dst (CMoveL (Binary cop cr) (Binary dst src)));\n+\n+  ins_cost(200); \/\/ XXX\n+  format %{ \"cmovpq  $dst, $src\\n\\t\"\n+            \"cmovneq $dst, $src\" %}\n+  ins_encode %{\n+    __ cmovq(Assembler::parity, $dst$$Register, $src$$Register);\n+    __ cmovq(Assembler::notEqual, $dst$$Register, $src$$Register);\n+  %}\n+  ins_pipe(pipe_cmov_reg);\n+%}\n+\n+instruct cmovL_regUCF2_ne_ndd(cmpOpUCF2 cop, rFlagsRegUCF cr, rRegL dst, rRegL src1, rRegL src2) %{\n+  predicate(UseAPX && n->in(1)->in(1)->as_Bool()->_test._test == BoolTest::ne);\n+  match(Set dst (CMoveL (Binary cop cr) (Binary src1 src2)));\n+  effect(TEMP dst);\n+\n+  ins_cost(200);\n+  format %{ \"ecmovpq  $dst, $src1, $src2\\n\\t\"\n+            \"cmovneq  $dst, $src2\" %}\n+  ins_encode %{\n+    __ ecmovq(Assembler::parity, $dst$$Register, $src1$$Register, $src2$$Register);\n+    __ cmovq(Assembler::notEqual, $dst$$Register, $src2$$Register);\n+  %}\n+  ins_pipe(pipe_cmov_reg);\n+%}\n+\n+\/\/ Since (x == y) == !(x != y), we can flip the sense of the test by flipping the\n+\/\/ inputs of the CMove\n+instruct cmovL_regUCF2_eq(cmpOpUCF2 cop, rFlagsRegUCF cr, rRegL dst, rRegL src) %{\n+  predicate(!UseAPX && n->in(1)->in(1)->as_Bool()->_test._test == BoolTest::eq);\n+  match(Set dst (CMoveL (Binary cop cr) (Binary src dst)));\n+\n+  ins_cost(200); \/\/ XXX\n+  format %{ \"cmovpq  $dst, $src\\n\\t\"\n+            \"cmovneq $dst, $src\" %}\n+  ins_encode %{\n+    __ cmovq(Assembler::parity, $dst$$Register, $src$$Register);\n+    __ cmovq(Assembler::notEqual, $dst$$Register, $src$$Register);\n+  %}\n+  ins_pipe(pipe_cmov_reg);\n+%}\n+\n+instruct cmovL_regUCF2_eq_ndd(cmpOpUCF2 cop, rFlagsRegUCF cr, rRegL dst, rRegL src1, rRegL src2) %{\n+  predicate(UseAPX && n->in(1)->in(1)->as_Bool()->_test._test == BoolTest::eq);\n+  match(Set dst (CMoveL (Binary cop cr) (Binary src2 src1)));\n+  effect(TEMP dst);\n+\n+  ins_cost(200);\n+  format %{ \"ecmovpq  $dst, $src1, $src2\\n\\t\"\n+            \"cmovneq $dst, $src2\" %}\n+  ins_encode %{\n+    __ ecmovq(Assembler::parity, $dst$$Register, $src1$$Register, $src2$$Register);\n+    __ cmovq(Assembler::notEqual, $dst$$Register, $src2$$Register);\n+  %}\n+  ins_pipe(pipe_cmov_reg);\n+%}\n+\n+instruct cmovL_memU(cmpOpU cop, rFlagsRegU cr, rRegL dst, memory src)\n+%{\n+  predicate(!UseAPX);\n+  match(Set dst (CMoveL (Binary cop cr) (Binary dst (LoadL src))));\n+\n+  ins_cost(200); \/\/ XXX\n+  format %{ \"cmovq$cop $dst, $src\\t# unsigned, long\" %}\n+  ins_encode %{\n+    __ cmovq((Assembler::Condition)($cop$$cmpcode), $dst$$Register, $src$$Address);\n+  %}\n+  ins_pipe(pipe_cmov_mem); \/\/ XXX\n+%}\n+\n+instruct cmovL_memUCF(cmpOpUCF cop, rFlagsRegUCF cr, rRegL dst, memory src) %{\n+  predicate(!UseAPX);\n+  match(Set dst (CMoveL (Binary cop cr) (Binary dst (LoadL src))));\n+  ins_cost(200);\n+  expand %{\n+    cmovL_memU(cop, cr, dst, src);\n+  %}\n+%}\n+\n+instruct cmovL_rReg_rReg_memU_ndd(rRegL dst, cmpOpU cop, rFlagsRegU cr, rRegL src1, memory src2)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (CMoveL (Binary cop cr) (Binary src1 (LoadL src2))));\n+\n+  ins_cost(200);\n+  format %{ \"ecmovq$cop $dst, $src1, $src2\\t# unsigned, long ndd\" %}\n+  ins_encode %{\n+    __ ecmovq((Assembler::Condition)($cop$$cmpcode), $dst$$Register, $src1$$Register, $src2$$Address);\n+  %}\n+  ins_pipe(pipe_cmov_mem);\n+%}\n+\n+instruct cmovL_rReg_rReg_memUCF_ndd(rRegL dst, cmpOpUCF cop, rFlagsRegUCF cr, rRegL src1, memory src2)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (CMoveL (Binary cop cr) (Binary src1 (LoadL src2))));\n+  ins_cost(200);\n+  format %{ \"ecmovq$cop $dst, $src1, $src2\\t# unsigned, long ndd\" %}\n+  ins_encode %{\n+    __ ecmovq((Assembler::Condition)($cop$$cmpcode), $dst$$Register, $src1$$Register, $src2$$Address);\n+  %}\n+  ins_pipe(pipe_cmov_mem);\n+%}\n+\n+instruct cmovF_reg(cmpOp cop, rFlagsReg cr, regF dst, regF src)\n+%{\n+  match(Set dst (CMoveF (Binary cop cr) (Binary dst src)));\n+\n+  ins_cost(200); \/\/ XXX\n+  format %{ \"jn$cop    skip\\t# signed cmove float\\n\\t\"\n+            \"movss     $dst, $src\\n\"\n+    \"skip:\" %}\n+  ins_encode %{\n+    Label Lskip;\n+    \/\/ Invert sense of branch from sense of CMOV\n+    __ jccb((Assembler::Condition)($cop$$cmpcode^1), Lskip);\n+    __ movflt($dst$$XMMRegister, $src$$XMMRegister);\n+    __ bind(Lskip);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct cmovF_regU(cmpOpU cop, rFlagsRegU cr, regF dst, regF src)\n+%{\n+  match(Set dst (CMoveF (Binary cop cr) (Binary dst src)));\n+\n+  ins_cost(200); \/\/ XXX\n+  format %{ \"jn$cop    skip\\t# unsigned cmove float\\n\\t\"\n+            \"movss     $dst, $src\\n\"\n+    \"skip:\" %}\n+  ins_encode %{\n+    Label Lskip;\n+    \/\/ Invert sense of branch from sense of CMOV\n+    __ jccb((Assembler::Condition)($cop$$cmpcode^1), Lskip);\n+    __ movflt($dst$$XMMRegister, $src$$XMMRegister);\n+    __ bind(Lskip);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct cmovF_regUCF(cmpOpUCF cop, rFlagsRegUCF cr, regF dst, regF src) %{\n+  match(Set dst (CMoveF (Binary cop cr) (Binary dst src)));\n+  ins_cost(200);\n+  expand %{\n+    cmovF_regU(cop, cr, dst, src);\n+  %}\n+%}\n+\n+instruct cmovD_reg(cmpOp cop, rFlagsReg cr, regD dst, regD src)\n+%{\n+  match(Set dst (CMoveD (Binary cop cr) (Binary dst src)));\n+\n+  ins_cost(200); \/\/ XXX\n+  format %{ \"jn$cop    skip\\t# signed cmove double\\n\\t\"\n+            \"movsd     $dst, $src\\n\"\n+    \"skip:\" %}\n+  ins_encode %{\n+    Label Lskip;\n+    \/\/ Invert sense of branch from sense of CMOV\n+    __ jccb((Assembler::Condition)($cop$$cmpcode^1), Lskip);\n+    __ movdbl($dst$$XMMRegister, $src$$XMMRegister);\n+    __ bind(Lskip);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct cmovD_regU(cmpOpU cop, rFlagsRegU cr, regD dst, regD src)\n+%{\n+  match(Set dst (CMoveD (Binary cop cr) (Binary dst src)));\n+\n+  ins_cost(200); \/\/ XXX\n+  format %{ \"jn$cop    skip\\t# unsigned cmove double\\n\\t\"\n+            \"movsd     $dst, $src\\n\"\n+    \"skip:\" %}\n+  ins_encode %{\n+    Label Lskip;\n+    \/\/ Invert sense of branch from sense of CMOV\n+    __ jccb((Assembler::Condition)($cop$$cmpcode^1), Lskip);\n+    __ movdbl($dst$$XMMRegister, $src$$XMMRegister);\n+    __ bind(Lskip);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct cmovD_regUCF(cmpOpUCF cop, rFlagsRegUCF cr, regD dst, regD src) %{\n+  match(Set dst (CMoveD (Binary cop cr) (Binary dst src)));\n+  ins_cost(200);\n+  expand %{\n+    cmovD_regU(cop, cr, dst, src);\n+  %}\n+%}\n+\n+\/\/----------Arithmetic Instructions--------------------------------------------\n+\/\/----------Addition Instructions----------------------------------------------\n+\n+instruct addI_rReg(rRegI dst, rRegI src, rFlagsReg cr)\n+%{\n+  predicate(!UseAPX);\n+  match(Set dst (AddI dst src));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_carry_flag, PD::Flag_sets_parity_flag);\n+  format %{ \"addl    $dst, $src\\t# int\" %}\n+  ins_encode %{\n+    __ addl($dst$$Register, $src$$Register);\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+instruct addI_rReg_ndd(rRegI dst, rRegI src1, rRegI src2, rFlagsReg cr)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (AddI src1 src2));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_carry_flag, PD::Flag_sets_parity_flag);\n+\n+  format %{ \"eaddl    $dst, $src1, $src2\\t# int ndd\" %}\n+  ins_encode %{\n+    __ eaddl($dst$$Register, $src1$$Register, $src2$$Register, false);\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+instruct addI_rReg_imm(rRegI dst, immI src, rFlagsReg cr)\n+%{\n+  predicate(!UseAPX);\n+  match(Set dst (AddI dst src));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_carry_flag, PD::Flag_sets_parity_flag);\n+\n+  format %{ \"addl    $dst, $src\\t# int\" %}\n+  ins_encode %{\n+    __ addl($dst$$Register, $src$$constant);\n+  %}\n+  ins_pipe( ialu_reg );\n+%}\n+\n+instruct addI_rReg_rReg_imm_ndd(rRegI dst, rRegI src1, immI src2, rFlagsReg cr)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (AddI src1 src2));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_carry_flag, PD::Flag_sets_parity_flag);\n+\n+  format %{ \"eaddl    $dst, $src1, $src2\\t# int ndd\" %}\n+  ins_encode %{\n+    __ eaddl($dst$$Register, $src1$$Register, $src2$$constant, false);\n+  %}\n+  ins_pipe( ialu_reg );\n+%}\n+\n+instruct addI_rReg_mem_imm_ndd(rRegI dst, memory src1, immI src2, rFlagsReg cr)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (AddI (LoadI src1) src2));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_carry_flag, PD::Flag_sets_parity_flag);\n+\n+  format %{ \"eaddl    $dst, $src1, $src2\\t# int ndd\" %}\n+  ins_encode %{\n+    __ eaddl($dst$$Register, $src1$$Address, $src2$$constant, false);\n+  %}\n+  ins_pipe( ialu_reg );\n+%}\n+\n+instruct addI_rReg_mem(rRegI dst, memory src, rFlagsReg cr)\n+%{\n+  predicate(!UseAPX);\n+  match(Set dst (AddI dst (LoadI src)));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_carry_flag, PD::Flag_sets_parity_flag);\n+\n+  ins_cost(150); \/\/ XXX\n+  format %{ \"addl    $dst, $src\\t# int\" %}\n+  ins_encode %{\n+    __ addl($dst$$Register, $src$$Address);\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+instruct addI_rReg_rReg_mem_ndd(rRegI dst, rRegI src1, memory src2, rFlagsReg cr)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (AddI src1 (LoadI src2)));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_carry_flag, PD::Flag_sets_parity_flag);\n+\n+  ins_cost(150);\n+  format %{ \"eaddl    $dst, $src1, $src2\\t# int ndd\" %}\n+  ins_encode %{\n+    __ eaddl($dst$$Register, $src1$$Register, $src2$$Address, false);\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+instruct addI_mem_rReg(memory dst, rRegI src, rFlagsReg cr)\n+%{\n+  match(Set dst (StoreI dst (AddI (LoadI dst) src)));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_carry_flag, PD::Flag_sets_parity_flag);\n+\n+  ins_cost(150); \/\/ XXX\n+  format %{ \"addl    $dst, $src\\t# int\" %}\n+  ins_encode %{\n+    __ addl($dst$$Address, $src$$Register);\n+  %}\n+  ins_pipe(ialu_mem_reg);\n+%}\n+\n+instruct addI_mem_imm(memory dst, immI src, rFlagsReg cr)\n+%{\n+  match(Set dst (StoreI dst (AddI (LoadI dst) src)));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_carry_flag, PD::Flag_sets_parity_flag);\n+\n+\n+  ins_cost(125); \/\/ XXX\n+  format %{ \"addl    $dst, $src\\t# int\" %}\n+  ins_encode %{\n+    __ addl($dst$$Address, $src$$constant);\n+  %}\n+  ins_pipe(ialu_mem_imm);\n+%}\n+\n+instruct incI_rReg(rRegI dst, immI_1 src, rFlagsReg cr)\n+%{\n+  predicate(!UseAPX && UseIncDec);\n+  match(Set dst (AddI dst src));\n+  effect(KILL cr);\n+\n+  format %{ \"incl    $dst\\t# int\" %}\n+  ins_encode %{\n+    __ incrementl($dst$$Register);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct incI_rReg_ndd(rRegI dst, rRegI src, immI_1 val, rFlagsReg cr)\n+%{\n+  predicate(UseAPX && UseIncDec);\n+  match(Set dst (AddI src val));\n+  effect(KILL cr);\n+\n+  format %{ \"eincl    $dst, $src\\t# int ndd\" %}\n+  ins_encode %{\n+    __ eincl($dst$$Register, $src$$Register, false);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct incI_rReg_mem_ndd(rRegI dst, memory src, immI_1 val, rFlagsReg cr)\n+%{\n+  predicate(UseAPX && UseIncDec);\n+  match(Set dst (AddI (LoadI src) val));\n+  effect(KILL cr);\n+\n+  format %{ \"eincl    $dst, $src\\t# int ndd\" %}\n+  ins_encode %{\n+    __ eincl($dst$$Register, $src$$Address, false);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct incI_mem(memory dst, immI_1 src, rFlagsReg cr)\n+%{\n+  predicate(UseIncDec);\n+  match(Set dst (StoreI dst (AddI (LoadI dst) src)));\n+  effect(KILL cr);\n+\n+  ins_cost(125); \/\/ XXX\n+  format %{ \"incl    $dst\\t# int\" %}\n+  ins_encode %{\n+    __ incrementl($dst$$Address);\n+  %}\n+  ins_pipe(ialu_mem_imm);\n+%}\n+\n+\/\/ XXX why does that use AddI\n+instruct decI_rReg(rRegI dst, immI_M1 src, rFlagsReg cr)\n+%{\n+  predicate(!UseAPX && UseIncDec);\n+  match(Set dst (AddI dst src));\n+  effect(KILL cr);\n+\n+  format %{ \"decl    $dst\\t# int\" %}\n+  ins_encode %{\n+    __ decrementl($dst$$Register);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct decI_rReg_ndd(rRegI dst, rRegI src, immI_M1 val, rFlagsReg cr)\n+%{\n+  predicate(UseAPX && UseIncDec);\n+  match(Set dst (AddI src val));\n+  effect(KILL cr);\n+\n+  format %{ \"edecl    $dst, $src\\t# int ndd\" %}\n+  ins_encode %{\n+    __ edecl($dst$$Register, $src$$Register, false);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct decI_rReg_mem_ndd(rRegI dst, memory src, immI_M1 val, rFlagsReg cr)\n+%{\n+  predicate(UseAPX && UseIncDec);\n+  match(Set dst (AddI (LoadI src) val));\n+  effect(KILL cr);\n+\n+  format %{ \"edecl    $dst, $src\\t# int ndd\" %}\n+  ins_encode %{\n+    __ edecl($dst$$Register, $src$$Address, false);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+\/\/ XXX why does that use AddI\n+instruct decI_mem(memory dst, immI_M1 src, rFlagsReg cr)\n+%{\n+  predicate(UseIncDec);\n+  match(Set dst (StoreI dst (AddI (LoadI dst) src)));\n+  effect(KILL cr);\n+\n+  ins_cost(125); \/\/ XXX\n+  format %{ \"decl    $dst\\t# int\" %}\n+  ins_encode %{\n+    __ decrementl($dst$$Address);\n+  %}\n+  ins_pipe(ialu_mem_imm);\n+%}\n+\n+instruct leaI_rReg_immI2_immI(rRegI dst, rRegI index, immI2 scale, immI disp)\n+%{\n+  predicate(VM_Version::supports_fast_2op_lea());\n+  match(Set dst (AddI (LShiftI index scale) disp));\n+\n+  format %{ \"leal $dst, [$index << $scale + $disp]\\t# int\" %}\n+  ins_encode %{\n+    Address::ScaleFactor scale = static_cast<Address::ScaleFactor>($scale$$constant);\n+    __ leal($dst$$Register, Address(noreg, $index$$Register, scale, $disp$$constant));\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+instruct leaI_rReg_rReg_immI(rRegI dst, rRegI base, rRegI index, immI disp)\n+%{\n+  predicate(VM_Version::supports_fast_3op_lea());\n+  match(Set dst (AddI (AddI base index) disp));\n+\n+  format %{ \"leal $dst, [$base + $index + $disp]\\t# int\" %}\n+  ins_encode %{\n+    __ leal($dst$$Register, Address($base$$Register, $index$$Register, Address::times_1, $disp$$constant));\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+instruct leaI_rReg_rReg_immI2(rRegI dst, no_rbp_r13_RegI base, rRegI index, immI2 scale)\n+%{\n+  predicate(VM_Version::supports_fast_2op_lea());\n+  match(Set dst (AddI base (LShiftI index scale)));\n+\n+  format %{ \"leal $dst, [$base + $index << $scale]\\t# int\" %}\n+  ins_encode %{\n+    Address::ScaleFactor scale = static_cast<Address::ScaleFactor>($scale$$constant);\n+    __ leal($dst$$Register, Address($base$$Register, $index$$Register, scale));\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+instruct leaI_rReg_rReg_immI2_immI(rRegI dst, rRegI base, rRegI index, immI2 scale, immI disp)\n+%{\n+  predicate(VM_Version::supports_fast_3op_lea());\n+  match(Set dst (AddI (AddI base (LShiftI index scale)) disp));\n+\n+  format %{ \"leal $dst, [$base + $index << $scale + $disp]\\t# int\" %}\n+  ins_encode %{\n+    Address::ScaleFactor scale = static_cast<Address::ScaleFactor>($scale$$constant);\n+    __ leal($dst$$Register, Address($base$$Register, $index$$Register, scale, $disp$$constant));\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+instruct addL_rReg(rRegL dst, rRegL src, rFlagsReg cr)\n+%{\n+  predicate(!UseAPX);\n+  match(Set dst (AddL dst src));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_carry_flag, PD::Flag_sets_parity_flag);\n+\n+  format %{ \"addq    $dst, $src\\t# long\" %}\n+  ins_encode %{\n+    __ addq($dst$$Register, $src$$Register);\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+instruct addL_rReg_ndd(rRegL dst, rRegL src1, rRegL src2, rFlagsReg cr)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (AddL src1 src2));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_carry_flag, PD::Flag_sets_parity_flag);\n+\n+  format %{ \"eaddq    $dst, $src1, $src2\\t# long ndd\" %}\n+  ins_encode %{\n+    __ eaddq($dst$$Register, $src1$$Register, $src2$$Register, false);\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+instruct addL_rReg_imm(rRegL dst, immL32 src, rFlagsReg cr)\n+%{\n+  predicate(!UseAPX);\n+  match(Set dst (AddL dst src));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_carry_flag, PD::Flag_sets_parity_flag);\n+\n+  format %{ \"addq    $dst, $src\\t# long\" %}\n+  ins_encode %{\n+    __ addq($dst$$Register, $src$$constant);\n+  %}\n+  ins_pipe( ialu_reg );\n+%}\n+\n+instruct addL_rReg_rReg_imm_ndd(rRegL dst, rRegL src1, immL32 src2, rFlagsReg cr)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (AddL src1 src2));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_carry_flag, PD::Flag_sets_parity_flag);\n+\n+  format %{ \"eaddq    $dst, $src1, $src2\\t# long ndd\" %}\n+  ins_encode %{\n+    __ eaddq($dst$$Register, $src1$$Register, $src2$$constant, false);\n+  %}\n+  ins_pipe( ialu_reg );\n+%}\n+\n+instruct addL_rReg_mem_imm_ndd(rRegL dst, memory src1, immL32 src2, rFlagsReg cr)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (AddL (LoadL src1) src2));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_carry_flag, PD::Flag_sets_parity_flag);\n+\n+  format %{ \"eaddq    $dst, $src1, $src2\\t# long ndd\" %}\n+  ins_encode %{\n+    __ eaddq($dst$$Register, $src1$$Address, $src2$$constant, false);\n+  %}\n+  ins_pipe( ialu_reg );\n+%}\n+\n+instruct addL_rReg_mem(rRegL dst, memory src, rFlagsReg cr)\n+%{\n+  predicate(!UseAPX);\n+  match(Set dst (AddL dst (LoadL src)));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_carry_flag, PD::Flag_sets_parity_flag);\n+\n+  ins_cost(150); \/\/ XXX\n+  format %{ \"addq    $dst, $src\\t# long\" %}\n+  ins_encode %{\n+    __ addq($dst$$Register, $src$$Address);\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+instruct addL_rReg_rReg_mem_ndd(rRegL dst, rRegL src1, memory src2, rFlagsReg cr)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (AddL src1 (LoadL src2)));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_carry_flag, PD::Flag_sets_parity_flag);\n+\n+  ins_cost(150);\n+  format %{ \"eaddq    $dst, $src1, $src2\\t# long ndd\" %}\n+  ins_encode %{\n+    __ eaddq($dst$$Register, $src1$$Register, $src2$$Address, false);\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+instruct addL_mem_rReg(memory dst, rRegL src, rFlagsReg cr)\n+%{\n+  match(Set dst (StoreL dst (AddL (LoadL dst) src)));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_carry_flag, PD::Flag_sets_parity_flag);\n+\n+  ins_cost(150); \/\/ XXX\n+  format %{ \"addq    $dst, $src\\t# long\" %}\n+  ins_encode %{\n+    __ addq($dst$$Address, $src$$Register);\n+  %}\n+  ins_pipe(ialu_mem_reg);\n+%}\n+\n+instruct addL_mem_imm(memory dst, immL32 src, rFlagsReg cr)\n+%{\n+  match(Set dst (StoreL dst (AddL (LoadL dst) src)));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_carry_flag, PD::Flag_sets_parity_flag);\n+\n+  ins_cost(125); \/\/ XXX\n+  format %{ \"addq    $dst, $src\\t# long\" %}\n+  ins_encode %{\n+    __ addq($dst$$Address, $src$$constant);\n+  %}\n+  ins_pipe(ialu_mem_imm);\n+%}\n+\n+instruct incL_rReg(rRegL dst, immL1 src, rFlagsReg cr)\n+%{\n+  predicate(!UseAPX && UseIncDec);\n+  match(Set dst (AddL dst src));\n+  effect(KILL cr);\n+\n+  format %{ \"incq    $dst\\t# long\" %}\n+  ins_encode %{\n+    __ incrementq($dst$$Register);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct incL_rReg_ndd(rRegL dst, rRegI src, immL1 val, rFlagsReg cr)\n+%{\n+  predicate(UseAPX && UseIncDec);\n+  match(Set dst (AddL src val));\n+  effect(KILL cr);\n+\n+  format %{ \"eincq    $dst, $src\\t# long ndd\" %}\n+  ins_encode %{\n+    __ eincq($dst$$Register, $src$$Register, false);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct incL_rReg_mem_ndd(rRegL dst, memory src, immL1 val, rFlagsReg cr)\n+%{\n+  predicate(UseAPX && UseIncDec);\n+  match(Set dst (AddL (LoadL src) val));\n+  effect(KILL cr);\n+\n+  format %{ \"eincq    $dst, $src\\t# long ndd\" %}\n+  ins_encode %{\n+    __ eincq($dst$$Register, $src$$Address, false);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct incL_mem(memory dst, immL1 src, rFlagsReg cr)\n+%{\n+  predicate(UseIncDec);\n+  match(Set dst (StoreL dst (AddL (LoadL dst) src)));\n+  effect(KILL cr);\n+\n+  ins_cost(125); \/\/ XXX\n+  format %{ \"incq    $dst\\t# long\" %}\n+  ins_encode %{\n+    __ incrementq($dst$$Address);\n+  %}\n+  ins_pipe(ialu_mem_imm);\n+%}\n+\n+\/\/ XXX why does that use AddL\n+instruct decL_rReg(rRegL dst, immL_M1 src, rFlagsReg cr)\n+%{\n+  predicate(!UseAPX && UseIncDec);\n+  match(Set dst (AddL dst src));\n+  effect(KILL cr);\n+\n+  format %{ \"decq    $dst\\t# long\" %}\n+  ins_encode %{\n+    __ decrementq($dst$$Register);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct decL_rReg_ndd(rRegL dst, rRegL src, immL_M1 val, rFlagsReg cr)\n+%{\n+  predicate(UseAPX && UseIncDec);\n+  match(Set dst (AddL src val));\n+  effect(KILL cr);\n+\n+  format %{ \"edecq    $dst, $src\\t# long ndd\" %}\n+  ins_encode %{\n+    __ edecq($dst$$Register, $src$$Register, false);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct decL_rReg_mem_ndd(rRegL dst, memory src, immL_M1 val, rFlagsReg cr)\n+%{\n+  predicate(UseAPX && UseIncDec);\n+  match(Set dst (AddL (LoadL src) val));\n+  effect(KILL cr);\n+\n+  format %{ \"edecq    $dst, $src\\t# long ndd\" %}\n+  ins_encode %{\n+    __ edecq($dst$$Register, $src$$Address, false);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+\/\/ XXX why does that use AddL\n+instruct decL_mem(memory dst, immL_M1 src, rFlagsReg cr)\n+%{\n+  predicate(UseIncDec);\n+  match(Set dst (StoreL dst (AddL (LoadL dst) src)));\n+  effect(KILL cr);\n+\n+  ins_cost(125); \/\/ XXX\n+  format %{ \"decq    $dst\\t# long\" %}\n+  ins_encode %{\n+    __ decrementq($dst$$Address);\n+  %}\n+  ins_pipe(ialu_mem_imm);\n+%}\n+\n+instruct leaL_rReg_immI2_immL32(rRegL dst, rRegL index, immI2 scale, immL32 disp)\n+%{\n+  predicate(VM_Version::supports_fast_2op_lea());\n+  match(Set dst (AddL (LShiftL index scale) disp));\n+\n+  format %{ \"leaq $dst, [$index << $scale + $disp]\\t# long\" %}\n+  ins_encode %{\n+    Address::ScaleFactor scale = static_cast<Address::ScaleFactor>($scale$$constant);\n+    __ leaq($dst$$Register, Address(noreg, $index$$Register, scale, $disp$$constant));\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+instruct leaL_rReg_rReg_immL32(rRegL dst, rRegL base, rRegL index, immL32 disp)\n+%{\n+  predicate(VM_Version::supports_fast_3op_lea());\n+  match(Set dst (AddL (AddL base index) disp));\n+\n+  format %{ \"leaq $dst, [$base + $index + $disp]\\t# long\" %}\n+  ins_encode %{\n+    __ leaq($dst$$Register, Address($base$$Register, $index$$Register, Address::times_1, $disp$$constant));\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+instruct leaL_rReg_rReg_immI2(rRegL dst, no_rbp_r13_RegL base, rRegL index, immI2 scale)\n+%{\n+  predicate(VM_Version::supports_fast_2op_lea());\n+  match(Set dst (AddL base (LShiftL index scale)));\n+\n+  format %{ \"leaq $dst, [$base + $index << $scale]\\t# long\" %}\n+  ins_encode %{\n+    Address::ScaleFactor scale = static_cast<Address::ScaleFactor>($scale$$constant);\n+    __ leaq($dst$$Register, Address($base$$Register, $index$$Register, scale));\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+instruct leaL_rReg_rReg_immI2_immL32(rRegL dst, rRegL base, rRegL index, immI2 scale, immL32 disp)\n+%{\n+  predicate(VM_Version::supports_fast_3op_lea());\n+  match(Set dst (AddL (AddL base (LShiftL index scale)) disp));\n+\n+  format %{ \"leaq $dst, [$base + $index << $scale + $disp]\\t# long\" %}\n+  ins_encode %{\n+    Address::ScaleFactor scale = static_cast<Address::ScaleFactor>($scale$$constant);\n+    __ leaq($dst$$Register, Address($base$$Register, $index$$Register, scale, $disp$$constant));\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+instruct addP_rReg(rRegP dst, rRegL src, rFlagsReg cr)\n+%{\n+  match(Set dst (AddP dst src));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_carry_flag, PD::Flag_sets_parity_flag);\n+\n+  format %{ \"addq    $dst, $src\\t# ptr\" %}\n+  ins_encode %{\n+    __ addq($dst$$Register, $src$$Register);\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+instruct addP_rReg_imm(rRegP dst, immL32 src, rFlagsReg cr)\n+%{\n+  match(Set dst (AddP dst src));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_carry_flag, PD::Flag_sets_parity_flag);\n+\n+  format %{ \"addq    $dst, $src\\t# ptr\" %}\n+  ins_encode %{\n+    __ addq($dst$$Register, $src$$constant);\n+  %}\n+  ins_pipe( ialu_reg );\n+%}\n+\n+\/\/ XXX addP mem ops ????\n+\n+instruct checkCastPP(rRegP dst)\n+%{\n+  match(Set dst (CheckCastPP dst));\n+\n+  size(0);\n+  format %{ \"# checkcastPP of $dst\" %}\n+  ins_encode(\/* empty encoding *\/);\n+  ins_pipe(empty);\n+%}\n+\n+instruct castPP(rRegP dst)\n+%{\n+  match(Set dst (CastPP dst));\n+\n+  size(0);\n+  format %{ \"# castPP of $dst\" %}\n+  ins_encode(\/* empty encoding *\/);\n+  ins_pipe(empty);\n+%}\n+\n+instruct castII(rRegI dst)\n+%{\n+  predicate(VerifyConstraintCasts == 0);\n+  match(Set dst (CastII dst));\n+\n+  size(0);\n+  format %{ \"# castII of $dst\" %}\n+  ins_encode(\/* empty encoding *\/);\n+  ins_cost(0);\n+  ins_pipe(empty);\n+%}\n+\n+instruct castII_checked(rRegI dst, rFlagsReg cr)\n+%{\n+  predicate(VerifyConstraintCasts > 0);\n+  match(Set dst (CastII dst));\n+\n+  effect(KILL cr);\n+  format %{ \"# cast_checked_II $dst\" %}\n+  ins_encode %{\n+    __ verify_int_in_range(_idx, bottom_type()->is_int(), $dst$$Register);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct castLL(rRegL dst)\n+%{\n+  predicate(VerifyConstraintCasts == 0);\n+  match(Set dst (CastLL dst));\n+\n+  size(0);\n+  format %{ \"# castLL of $dst\" %}\n+  ins_encode(\/* empty encoding *\/);\n+  ins_cost(0);\n+  ins_pipe(empty);\n+%}\n+\n+instruct castLL_checked_L32(rRegL dst, rFlagsReg cr)\n+%{\n+  predicate(VerifyConstraintCasts > 0 && castLL_is_imm32(n));\n+  match(Set dst (CastLL dst));\n+\n+  effect(KILL cr);\n+  format %{ \"# cast_checked_LL $dst\" %}\n+  ins_encode %{\n+    __ verify_long_in_range(_idx, bottom_type()->is_long(), $dst$$Register, noreg);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct castLL_checked(rRegL dst, rRegL tmp, rFlagsReg cr)\n+%{\n+  predicate(VerifyConstraintCasts > 0 && !castLL_is_imm32(n));\n+  match(Set dst (CastLL dst));\n+\n+  effect(KILL cr, TEMP tmp);\n+  format %{ \"# cast_checked_LL $dst\\tusing $tmp as TEMP\" %}\n+  ins_encode %{\n+    __ verify_long_in_range(_idx, bottom_type()->is_long(), $dst$$Register, $tmp$$Register);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct castFF(regF dst)\n+%{\n+  match(Set dst (CastFF dst));\n+\n+  size(0);\n+  format %{ \"# castFF of $dst\" %}\n+  ins_encode(\/* empty encoding *\/);\n+  ins_cost(0);\n+  ins_pipe(empty);\n+%}\n+\n+instruct castHH(regF dst)\n+%{\n+  match(Set dst (CastHH dst));\n+\n+  size(0);\n+  format %{ \"# castHH of $dst\" %}\n+  ins_encode(\/* empty encoding *\/);\n+  ins_cost(0);\n+  ins_pipe(empty);\n+%}\n+\n+instruct castDD(regD dst)\n+%{\n+  match(Set dst (CastDD dst));\n+\n+  size(0);\n+  format %{ \"# castDD of $dst\" %}\n+  ins_encode(\/* empty encoding *\/);\n+  ins_cost(0);\n+  ins_pipe(empty);\n+%}\n+\n+\/\/ XXX No flag versions for CompareAndSwap{P,I,L} because matcher can't match them\n+instruct compareAndSwapP(rRegI res,\n+                         memory mem_ptr,\n+                         rax_RegP oldval, rRegP newval,\n+                         rFlagsReg cr)\n+%{\n+  predicate(n->as_LoadStore()->barrier_data() == 0);\n+  match(Set res (CompareAndSwapP mem_ptr (Binary oldval newval)));\n+  match(Set res (WeakCompareAndSwapP mem_ptr (Binary oldval newval)));\n+  effect(KILL cr, KILL oldval);\n+\n+  format %{ \"cmpxchgq $mem_ptr,$newval\\t# \"\n+            \"If rax == $mem_ptr then store $newval into $mem_ptr\\n\\t\"\n+            \"setcc $res \\t# emits sete + movzbl or setzue for APX\" %}\n+  ins_encode %{\n+    __ lock();\n+    __ cmpxchgq($newval$$Register, $mem_ptr$$Address);\n+    __ setcc(Assembler::equal, $res$$Register);\n+  %}\n+  ins_pipe( pipe_cmpxchg );\n+%}\n+\n+instruct compareAndSwapL(rRegI res,\n+                         memory mem_ptr,\n+                         rax_RegL oldval, rRegL newval,\n+                         rFlagsReg cr)\n+%{\n+  match(Set res (CompareAndSwapL mem_ptr (Binary oldval newval)));\n+  match(Set res (WeakCompareAndSwapL mem_ptr (Binary oldval newval)));\n+  effect(KILL cr, KILL oldval);\n+\n+  format %{ \"cmpxchgq $mem_ptr,$newval\\t# \"\n+            \"If rax == $mem_ptr then store $newval into $mem_ptr\\n\\t\"\n+            \"setcc $res \\t# emits sete + movzbl or setzue for APX\" %}\n+  ins_encode %{\n+    __ lock();\n+    __ cmpxchgq($newval$$Register, $mem_ptr$$Address);\n+    __ setcc(Assembler::equal, $res$$Register);\n+  %}\n+  ins_pipe( pipe_cmpxchg );\n+%}\n+\n+instruct compareAndSwapI(rRegI res,\n+                         memory mem_ptr,\n+                         rax_RegI oldval, rRegI newval,\n+                         rFlagsReg cr)\n+%{\n+  match(Set res (CompareAndSwapI mem_ptr (Binary oldval newval)));\n+  match(Set res (WeakCompareAndSwapI mem_ptr (Binary oldval newval)));\n+  effect(KILL cr, KILL oldval);\n+\n+  format %{ \"cmpxchgl $mem_ptr,$newval\\t# \"\n+            \"If rax == $mem_ptr then store $newval into $mem_ptr\\n\\t\"\n+            \"setcc $res \\t# emits sete + movzbl or setzue for APX\" %}\n+  ins_encode %{\n+    __ lock();\n+    __ cmpxchgl($newval$$Register, $mem_ptr$$Address);\n+    __ setcc(Assembler::equal, $res$$Register);\n+  %}\n+  ins_pipe( pipe_cmpxchg );\n+%}\n+\n+instruct compareAndSwapB(rRegI res,\n+                         memory mem_ptr,\n+                         rax_RegI oldval, rRegI newval,\n+                         rFlagsReg cr)\n+%{\n+  match(Set res (CompareAndSwapB mem_ptr (Binary oldval newval)));\n+  match(Set res (WeakCompareAndSwapB mem_ptr (Binary oldval newval)));\n+  effect(KILL cr, KILL oldval);\n+\n+  format %{ \"cmpxchgb $mem_ptr,$newval\\t# \"\n+            \"If rax == $mem_ptr then store $newval into $mem_ptr\\n\\t\"\n+            \"setcc $res \\t# emits sete + movzbl or setzue for APX\" %}\n+  ins_encode %{\n+    __ lock();\n+    __ cmpxchgb($newval$$Register, $mem_ptr$$Address);\n+    __ setcc(Assembler::equal, $res$$Register);\n+  %}\n+  ins_pipe( pipe_cmpxchg );\n+%}\n+\n+instruct compareAndSwapS(rRegI res,\n+                         memory mem_ptr,\n+                         rax_RegI oldval, rRegI newval,\n+                         rFlagsReg cr)\n+%{\n+  match(Set res (CompareAndSwapS mem_ptr (Binary oldval newval)));\n+  match(Set res (WeakCompareAndSwapS mem_ptr (Binary oldval newval)));\n+  effect(KILL cr, KILL oldval);\n+\n+  format %{ \"cmpxchgw $mem_ptr,$newval\\t# \"\n+            \"If rax == $mem_ptr then store $newval into $mem_ptr\\n\\t\"\n+            \"setcc $res \\t# emits sete + movzbl or setzue for APX\" %}\n+  ins_encode %{\n+    __ lock();\n+    __ cmpxchgw($newval$$Register, $mem_ptr$$Address);\n+    __ setcc(Assembler::equal, $res$$Register);\n+  %}\n+  ins_pipe( pipe_cmpxchg );\n+%}\n+\n+instruct compareAndSwapN(rRegI res,\n+                          memory mem_ptr,\n+                          rax_RegN oldval, rRegN newval,\n+                          rFlagsReg cr) %{\n+  predicate(n->as_LoadStore()->barrier_data() == 0);\n+  match(Set res (CompareAndSwapN mem_ptr (Binary oldval newval)));\n+  match(Set res (WeakCompareAndSwapN mem_ptr (Binary oldval newval)));\n+  effect(KILL cr, KILL oldval);\n+\n+  format %{ \"cmpxchgl $mem_ptr,$newval\\t# \"\n+            \"If rax == $mem_ptr then store $newval into $mem_ptr\\n\\t\"\n+            \"setcc $res \\t# emits sete + movzbl or setzue for APX\" %}\n+  ins_encode %{\n+    __ lock();\n+    __ cmpxchgl($newval$$Register, $mem_ptr$$Address);\n+    __ setcc(Assembler::equal, $res$$Register);\n+  %}\n+  ins_pipe( pipe_cmpxchg );\n+%}\n+\n+instruct compareAndExchangeB(\n+                         memory mem_ptr,\n+                         rax_RegI oldval, rRegI newval,\n+                         rFlagsReg cr)\n+%{\n+  match(Set oldval (CompareAndExchangeB mem_ptr (Binary oldval newval)));\n+  effect(KILL cr);\n+\n+  format %{ \"cmpxchgb $mem_ptr,$newval\\t# \"\n+            \"If rax == $mem_ptr then store $newval into $mem_ptr\\n\\t\"  %}\n+  ins_encode %{\n+    __ lock();\n+    __ cmpxchgb($newval$$Register, $mem_ptr$$Address);\n+  %}\n+  ins_pipe( pipe_cmpxchg );\n+%}\n+\n+instruct compareAndExchangeS(\n+                         memory mem_ptr,\n+                         rax_RegI oldval, rRegI newval,\n+                         rFlagsReg cr)\n+%{\n+  match(Set oldval (CompareAndExchangeS mem_ptr (Binary oldval newval)));\n+  effect(KILL cr);\n+\n+  format %{ \"cmpxchgw $mem_ptr,$newval\\t# \"\n+            \"If rax == $mem_ptr then store $newval into $mem_ptr\\n\\t\"  %}\n+  ins_encode %{\n+    __ lock();\n+    __ cmpxchgw($newval$$Register, $mem_ptr$$Address);\n+  %}\n+  ins_pipe( pipe_cmpxchg );\n+%}\n+\n+instruct compareAndExchangeI(\n+                         memory mem_ptr,\n+                         rax_RegI oldval, rRegI newval,\n+                         rFlagsReg cr)\n+%{\n+  match(Set oldval (CompareAndExchangeI mem_ptr (Binary oldval newval)));\n+  effect(KILL cr);\n+\n+  format %{ \"cmpxchgl $mem_ptr,$newval\\t# \"\n+            \"If rax == $mem_ptr then store $newval into $mem_ptr\\n\\t\"  %}\n+  ins_encode %{\n+    __ lock();\n+    __ cmpxchgl($newval$$Register, $mem_ptr$$Address);\n+  %}\n+  ins_pipe( pipe_cmpxchg );\n+%}\n+\n+instruct compareAndExchangeL(\n+                         memory mem_ptr,\n+                         rax_RegL oldval, rRegL newval,\n+                         rFlagsReg cr)\n+%{\n+  match(Set oldval (CompareAndExchangeL mem_ptr (Binary oldval newval)));\n+  effect(KILL cr);\n+\n+  format %{ \"cmpxchgq $mem_ptr,$newval\\t# \"\n+            \"If rax == $mem_ptr then store $newval into $mem_ptr\\n\\t\"  %}\n+  ins_encode %{\n+    __ lock();\n+    __ cmpxchgq($newval$$Register, $mem_ptr$$Address);\n+  %}\n+  ins_pipe( pipe_cmpxchg );\n+%}\n+\n+instruct compareAndExchangeN(\n+                          memory mem_ptr,\n+                          rax_RegN oldval, rRegN newval,\n+                          rFlagsReg cr) %{\n+  predicate(n->as_LoadStore()->barrier_data() == 0);\n+  match(Set oldval (CompareAndExchangeN mem_ptr (Binary oldval newval)));\n+  effect(KILL cr);\n+\n+  format %{ \"cmpxchgl $mem_ptr,$newval\\t# \"\n+            \"If rax == $mem_ptr then store $newval into $mem_ptr\\n\\t\" %}\n+  ins_encode %{\n+    __ lock();\n+    __ cmpxchgl($newval$$Register, $mem_ptr$$Address);\n+  %}\n+  ins_pipe( pipe_cmpxchg );\n+%}\n+\n+instruct compareAndExchangeP(\n+                         memory mem_ptr,\n+                         rax_RegP oldval, rRegP newval,\n+                         rFlagsReg cr)\n+%{\n+  predicate(n->as_LoadStore()->barrier_data() == 0);\n+  match(Set oldval (CompareAndExchangeP mem_ptr (Binary oldval newval)));\n+  effect(KILL cr);\n+\n+  format %{ \"cmpxchgq $mem_ptr,$newval\\t# \"\n+            \"If rax == $mem_ptr then store $newval into $mem_ptr\\n\\t\" %}\n+  ins_encode %{\n+    __ lock();\n+    __ cmpxchgq($newval$$Register, $mem_ptr$$Address);\n+  %}\n+  ins_pipe( pipe_cmpxchg );\n+%}\n+\n+instruct xaddB_reg_no_res(memory mem, Universe dummy, rRegI add, rFlagsReg cr) %{\n+  predicate(n->as_LoadStore()->result_not_used());\n+  match(Set dummy (GetAndAddB mem add));\n+  effect(KILL cr);\n+  format %{ \"addb_lock   $mem, $add\" %}\n+  ins_encode %{\n+    __ lock();\n+    __ addb($mem$$Address, $add$$Register);\n+  %}\n+  ins_pipe(pipe_cmpxchg);\n+%}\n+\n+instruct xaddB_imm_no_res(memory mem, Universe dummy, immI add, rFlagsReg cr) %{\n+  predicate(n->as_LoadStore()->result_not_used());\n+  match(Set dummy (GetAndAddB mem add));\n+  effect(KILL cr);\n+  format %{ \"addb_lock   $mem, $add\" %}\n+  ins_encode %{\n+    __ lock();\n+    __ addb($mem$$Address, $add$$constant);\n+  %}\n+  ins_pipe(pipe_cmpxchg);\n+%}\n+\n+instruct xaddB(memory mem, rRegI newval, rFlagsReg cr) %{\n+  predicate(!n->as_LoadStore()->result_not_used());\n+  match(Set newval (GetAndAddB mem newval));\n+  effect(KILL cr);\n+  format %{ \"xaddb_lock  $mem, $newval\" %}\n+  ins_encode %{\n+    __ lock();\n+    __ xaddb($mem$$Address, $newval$$Register);\n+  %}\n+  ins_pipe(pipe_cmpxchg);\n+%}\n+\n+instruct xaddS_reg_no_res(memory mem, Universe dummy, rRegI add, rFlagsReg cr) %{\n+  predicate(n->as_LoadStore()->result_not_used());\n+  match(Set dummy (GetAndAddS mem add));\n+  effect(KILL cr);\n+  format %{ \"addw_lock   $mem, $add\" %}\n+  ins_encode %{\n+    __ lock();\n+    __ addw($mem$$Address, $add$$Register);\n+  %}\n+  ins_pipe(pipe_cmpxchg);\n+%}\n+\n+instruct xaddS_imm_no_res(memory mem, Universe dummy, immI add, rFlagsReg cr) %{\n+  predicate(UseStoreImmI16 && n->as_LoadStore()->result_not_used());\n+  match(Set dummy (GetAndAddS mem add));\n+  effect(KILL cr);\n+  format %{ \"addw_lock   $mem, $add\" %}\n+  ins_encode %{\n+    __ lock();\n+    __ addw($mem$$Address, $add$$constant);\n+  %}\n+  ins_pipe(pipe_cmpxchg);\n+%}\n+\n+instruct xaddS(memory mem, rRegI newval, rFlagsReg cr) %{\n+  predicate(!n->as_LoadStore()->result_not_used());\n+  match(Set newval (GetAndAddS mem newval));\n+  effect(KILL cr);\n+  format %{ \"xaddw_lock  $mem, $newval\" %}\n+  ins_encode %{\n+    __ lock();\n+    __ xaddw($mem$$Address, $newval$$Register);\n+  %}\n+  ins_pipe(pipe_cmpxchg);\n+%}\n+\n+instruct xaddI_reg_no_res(memory mem, Universe dummy, rRegI add, rFlagsReg cr) %{\n+  predicate(n->as_LoadStore()->result_not_used());\n+  match(Set dummy (GetAndAddI mem add));\n+  effect(KILL cr);\n+  format %{ \"addl_lock   $mem, $add\" %}\n+  ins_encode %{\n+    __ lock();\n+    __ addl($mem$$Address, $add$$Register);\n+  %}\n+  ins_pipe(pipe_cmpxchg);\n+%}\n+\n+instruct xaddI_imm_no_res(memory mem, Universe dummy, immI add, rFlagsReg cr) %{\n+  predicate(n->as_LoadStore()->result_not_used());\n+  match(Set dummy (GetAndAddI mem add));\n+  effect(KILL cr);\n+  format %{ \"addl_lock   $mem, $add\" %}\n+  ins_encode %{\n+    __ lock();\n+    __ addl($mem$$Address, $add$$constant);\n+  %}\n+  ins_pipe(pipe_cmpxchg);\n+%}\n+\n+instruct xaddI(memory mem, rRegI newval, rFlagsReg cr) %{\n+  predicate(!n->as_LoadStore()->result_not_used());\n+  match(Set newval (GetAndAddI mem newval));\n+  effect(KILL cr);\n+  format %{ \"xaddl_lock  $mem, $newval\" %}\n+  ins_encode %{\n+    __ lock();\n+    __ xaddl($mem$$Address, $newval$$Register);\n+  %}\n+  ins_pipe(pipe_cmpxchg);\n+%}\n+\n+instruct xaddL_reg_no_res(memory mem, Universe dummy, rRegL add, rFlagsReg cr) %{\n+  predicate(n->as_LoadStore()->result_not_used());\n+  match(Set dummy (GetAndAddL mem add));\n+  effect(KILL cr);\n+  format %{ \"addq_lock   $mem, $add\" %}\n+  ins_encode %{\n+    __ lock();\n+    __ addq($mem$$Address, $add$$Register);\n+  %}\n+  ins_pipe(pipe_cmpxchg);\n+%}\n+\n+instruct xaddL_imm_no_res(memory mem, Universe dummy, immL32 add, rFlagsReg cr) %{\n+  predicate(n->as_LoadStore()->result_not_used());\n+  match(Set dummy (GetAndAddL mem add));\n+  effect(KILL cr);\n+  format %{ \"addq_lock   $mem, $add\" %}\n+  ins_encode %{\n+    __ lock();\n+    __ addq($mem$$Address, $add$$constant);\n+  %}\n+  ins_pipe(pipe_cmpxchg);\n+%}\n+\n+instruct xaddL(memory mem, rRegL newval, rFlagsReg cr) %{\n+  predicate(!n->as_LoadStore()->result_not_used());\n+  match(Set newval (GetAndAddL mem newval));\n+  effect(KILL cr);\n+  format %{ \"xaddq_lock  $mem, $newval\" %}\n+  ins_encode %{\n+    __ lock();\n+    __ xaddq($mem$$Address, $newval$$Register);\n+  %}\n+  ins_pipe(pipe_cmpxchg);\n+%}\n+\n+instruct xchgB( memory mem, rRegI newval) %{\n+  match(Set newval (GetAndSetB mem newval));\n+  format %{ \"XCHGB  $newval,[$mem]\" %}\n+  ins_encode %{\n+    __ xchgb($newval$$Register, $mem$$Address);\n+  %}\n+  ins_pipe( pipe_cmpxchg );\n+%}\n+\n+instruct xchgS( memory mem, rRegI newval) %{\n+  match(Set newval (GetAndSetS mem newval));\n+  format %{ \"XCHGW  $newval,[$mem]\" %}\n+  ins_encode %{\n+    __ xchgw($newval$$Register, $mem$$Address);\n+  %}\n+  ins_pipe( pipe_cmpxchg );\n+%}\n+\n+instruct xchgI( memory mem, rRegI newval) %{\n+  match(Set newval (GetAndSetI mem newval));\n+  format %{ \"XCHGL  $newval,[$mem]\" %}\n+  ins_encode %{\n+    __ xchgl($newval$$Register, $mem$$Address);\n+  %}\n+  ins_pipe( pipe_cmpxchg );\n+%}\n+\n+instruct xchgL( memory mem, rRegL newval) %{\n+  match(Set newval (GetAndSetL mem newval));\n+  format %{ \"XCHGL  $newval,[$mem]\" %}\n+  ins_encode %{\n+    __ xchgq($newval$$Register, $mem$$Address);\n+  %}\n+  ins_pipe( pipe_cmpxchg );\n+%}\n+\n+instruct xchgP( memory mem, rRegP newval) %{\n+  match(Set newval (GetAndSetP mem newval));\n+  predicate(n->as_LoadStore()->barrier_data() == 0);\n+  format %{ \"XCHGQ  $newval,[$mem]\" %}\n+  ins_encode %{\n+    __ xchgq($newval$$Register, $mem$$Address);\n+  %}\n+  ins_pipe( pipe_cmpxchg );\n+%}\n+\n+instruct xchgN( memory mem, rRegN newval) %{\n+  predicate(n->as_LoadStore()->barrier_data() == 0);\n+  match(Set newval (GetAndSetN mem newval));\n+  format %{ \"XCHGL  $newval,$mem]\" %}\n+  ins_encode %{\n+    __ xchgl($newval$$Register, $mem$$Address);\n+  %}\n+  ins_pipe( pipe_cmpxchg );\n+%}\n+\n+\/\/----------Abs Instructions-------------------------------------------\n+\n+\/\/ Integer Absolute Instructions\n+instruct absI_rReg(rRegI dst, rRegI src, rFlagsReg cr)\n+%{\n+  match(Set dst (AbsI src));\n+  effect(TEMP dst, KILL cr);\n+  format %{ \"xorl    $dst, $dst\\t# abs int\\n\\t\"\n+            \"subl    $dst, $src\\n\\t\"\n+            \"cmovll  $dst, $src\" %}\n+  ins_encode %{\n+    __ xorl($dst$$Register, $dst$$Register);\n+    __ subl($dst$$Register, $src$$Register);\n+    __ cmovl(Assembler::less, $dst$$Register, $src$$Register);\n+  %}\n+\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+\/\/ Long Absolute Instructions\n+instruct absL_rReg(rRegL dst, rRegL src, rFlagsReg cr)\n+%{\n+  match(Set dst (AbsL src));\n+  effect(TEMP dst, KILL cr);\n+  format %{ \"xorl    $dst, $dst\\t# abs long\\n\\t\"\n+            \"subq    $dst, $src\\n\\t\"\n+            \"cmovlq  $dst, $src\" %}\n+  ins_encode %{\n+    __ xorl($dst$$Register, $dst$$Register);\n+    __ subq($dst$$Register, $src$$Register);\n+    __ cmovq(Assembler::less, $dst$$Register, $src$$Register);\n+  %}\n+\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+\/\/----------Subtraction Instructions-------------------------------------------\n+\n+\/\/ Integer Subtraction Instructions\n+instruct subI_rReg(rRegI dst, rRegI src, rFlagsReg cr)\n+%{\n+  predicate(!UseAPX);\n+  match(Set dst (SubI dst src));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_carry_flag, PD::Flag_sets_parity_flag);\n+\n+  format %{ \"subl    $dst, $src\\t# int\" %}\n+  ins_encode %{\n+    __ subl($dst$$Register, $src$$Register);\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+instruct subI_rReg_ndd(rRegI dst, rRegI src1, rRegI src2, rFlagsReg cr)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (SubI src1 src2));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_carry_flag, PD::Flag_sets_parity_flag);\n+\n+  format %{ \"esubl    $dst, $src1, $src2\\t# int ndd\" %}\n+  ins_encode %{\n+    __ esubl($dst$$Register, $src1$$Register, $src2$$Register, false);\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+instruct subI_rReg_rReg_imm_ndd(rRegI dst, rRegI src1, immI src2, rFlagsReg cr)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (SubI src1 src2));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_carry_flag, PD::Flag_sets_parity_flag);\n+\n+  format %{ \"esubl    $dst, $src1, $src2\\t# int ndd\" %}\n+  ins_encode %{\n+    __ esubl($dst$$Register, $src1$$Register, $src2$$constant, false);\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+instruct subI_rReg_mem_imm_ndd(rRegI dst, memory src1, immI src2, rFlagsReg cr)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (SubI (LoadI src1) src2));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_carry_flag, PD::Flag_sets_parity_flag);\n+\n+  format %{ \"esubl    $dst, $src1, $src2\\t# int ndd\" %}\n+  ins_encode %{\n+    __ esubl($dst$$Register, $src1$$Address, $src2$$constant, false);\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+instruct subI_rReg_mem(rRegI dst, memory src, rFlagsReg cr)\n+%{\n+  predicate(!UseAPX);\n+  match(Set dst (SubI dst (LoadI src)));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_carry_flag, PD::Flag_sets_parity_flag);\n+\n+  ins_cost(150);\n+  format %{ \"subl    $dst, $src\\t# int\" %}\n+  ins_encode %{\n+    __ subl($dst$$Register, $src$$Address);\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+instruct subI_rReg_rReg_mem_ndd(rRegI dst, rRegI src1, memory src2, rFlagsReg cr)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (SubI src1 (LoadI src2)));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_carry_flag, PD::Flag_sets_parity_flag);\n+\n+  ins_cost(150);\n+  format %{ \"esubl    $dst, $src1, $src2\\t# int ndd\" %}\n+  ins_encode %{\n+    __ esubl($dst$$Register, $src1$$Register, $src2$$Address, false);\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+instruct subI_rReg_mem_rReg_ndd(rRegI dst, memory src1, rRegI src2, rFlagsReg cr)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (SubI (LoadI src1) src2));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_carry_flag, PD::Flag_sets_parity_flag);\n+\n+  ins_cost(150);\n+  format %{ \"esubl    $dst, $src1, $src2\\t# int ndd\" %}\n+  ins_encode %{\n+    __ esubl($dst$$Register, $src1$$Address, $src2$$Register, false);\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+instruct subI_mem_rReg(memory dst, rRegI src, rFlagsReg cr)\n+%{\n+  match(Set dst (StoreI dst (SubI (LoadI dst) src)));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_carry_flag, PD::Flag_sets_parity_flag);\n+\n+  ins_cost(150);\n+  format %{ \"subl    $dst, $src\\t# int\" %}\n+  ins_encode %{\n+    __ subl($dst$$Address, $src$$Register);\n+  %}\n+  ins_pipe(ialu_mem_reg);\n+%}\n+\n+instruct subL_rReg(rRegL dst, rRegL src, rFlagsReg cr)\n+%{\n+  predicate(!UseAPX);\n+  match(Set dst (SubL dst src));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_carry_flag, PD::Flag_sets_parity_flag);\n+\n+  format %{ \"subq    $dst, $src\\t# long\" %}\n+  ins_encode %{\n+    __ subq($dst$$Register, $src$$Register);\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+instruct subL_rReg_ndd(rRegL dst, rRegL src1, rRegL src2, rFlagsReg cr)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (SubL src1 src2));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_carry_flag, PD::Flag_sets_parity_flag);\n+\n+  format %{ \"esubq    $dst, $src1, $src2\\t# long ndd\" %}\n+  ins_encode %{\n+    __ esubq($dst$$Register, $src1$$Register, $src2$$Register, false);\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+instruct subL_rReg_rReg_imm_ndd(rRegL dst, rRegL src1, immL32 src2, rFlagsReg cr)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (SubL src1 src2));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_carry_flag, PD::Flag_sets_parity_flag);\n+\n+  format %{ \"esubq    $dst, $src1, $src2\\t# long ndd\" %}\n+  ins_encode %{\n+    __ esubq($dst$$Register, $src1$$Register, $src2$$constant, false);\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+instruct subL_rReg_mem_imm_ndd(rRegL dst, memory src1, immL32 src2, rFlagsReg cr)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (SubL (LoadL src1) src2));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_carry_flag, PD::Flag_sets_parity_flag);\n+\n+  format %{ \"esubq    $dst, $src1, $src2\\t# long ndd\" %}\n+  ins_encode %{\n+    __ esubq($dst$$Register, $src1$$Address, $src2$$constant, false);\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+instruct subL_rReg_mem(rRegL dst, memory src, rFlagsReg cr)\n+%{\n+  predicate(!UseAPX);\n+  match(Set dst (SubL dst (LoadL src)));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_carry_flag, PD::Flag_sets_parity_flag);\n+\n+  ins_cost(150);\n+  format %{ \"subq    $dst, $src\\t# long\" %}\n+  ins_encode %{\n+    __ subq($dst$$Register, $src$$Address);\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+instruct subL_rReg_rReg_mem_ndd(rRegL dst, rRegL src1, memory src2, rFlagsReg cr)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (SubL src1 (LoadL src2)));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_carry_flag, PD::Flag_sets_parity_flag);\n+\n+  ins_cost(150);\n+  format %{ \"esubq    $dst, $src1, $src2\\t# long ndd\" %}\n+  ins_encode %{\n+    __ esubq($dst$$Register, $src1$$Register, $src2$$Address, false);\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+instruct subL_rReg_mem_rReg_ndd(rRegL dst, memory src1, rRegL src2, rFlagsReg cr)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (SubL (LoadL src1) src2));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_carry_flag, PD::Flag_sets_parity_flag);\n+\n+  ins_cost(150);\n+  format %{ \"esubq    $dst, $src1, $src2\\t# long ndd\" %}\n+  ins_encode %{\n+    __ esubq($dst$$Register, $src1$$Address, $src2$$Register, false);\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+instruct subL_mem_rReg(memory dst, rRegL src, rFlagsReg cr)\n+%{\n+  match(Set dst (StoreL dst (SubL (LoadL dst) src)));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_carry_flag, PD::Flag_sets_parity_flag);\n+\n+  ins_cost(150);\n+  format %{ \"subq    $dst, $src\\t# long\" %}\n+  ins_encode %{\n+    __ subq($dst$$Address, $src$$Register);\n+  %}\n+  ins_pipe(ialu_mem_reg);\n+%}\n+\n+\/\/ Subtract from a pointer\n+\/\/ XXX hmpf???\n+instruct subP_rReg(rRegP dst, rRegI src, immI_0 zero, rFlagsReg cr)\n+%{\n+  match(Set dst (AddP dst (SubI zero src)));\n+  effect(KILL cr);\n+\n+  format %{ \"subq    $dst, $src\\t# ptr - int\" %}\n+  ins_encode %{\n+    __ subq($dst$$Register, $src$$Register);\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+instruct negI_rReg(rRegI dst, immI_0 zero, rFlagsReg cr)\n+%{\n+  predicate(!UseAPX);\n+  match(Set dst (SubI zero dst));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag);\n+\n+  format %{ \"negl    $dst\\t# int\" %}\n+  ins_encode %{\n+    __ negl($dst$$Register);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct negI_rReg_ndd(rRegI dst, rRegI src, immI_0 zero, rFlagsReg cr)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (SubI zero src));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag);\n+\n+  format %{ \"enegl    $dst, $src\\t# int ndd\" %}\n+  ins_encode %{\n+    __ enegl($dst$$Register, $src$$Register, false);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct negI_rReg_2(rRegI dst, rFlagsReg cr)\n+%{\n+  predicate(!UseAPX);\n+  match(Set dst (NegI dst));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag);\n+\n+  format %{ \"negl    $dst\\t# int\" %}\n+  ins_encode %{\n+    __ negl($dst$$Register);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct negI_rReg_2_ndd(rRegI dst, rRegI src, rFlagsReg cr)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (NegI src));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag);\n+\n+  format %{ \"enegl    $dst, $src\\t# int ndd\" %}\n+  ins_encode %{\n+    __ enegl($dst$$Register, $src$$Register, false);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct negI_mem(memory dst, immI_0 zero, rFlagsReg cr)\n+%{\n+  match(Set dst (StoreI dst (SubI zero (LoadI dst))));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag);\n+\n+  format %{ \"negl    $dst\\t# int\" %}\n+  ins_encode %{\n+    __ negl($dst$$Address);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct negL_rReg(rRegL dst, immL0 zero, rFlagsReg cr)\n+%{\n+  predicate(!UseAPX);\n+  match(Set dst (SubL zero dst));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag);\n+\n+  format %{ \"negq    $dst\\t# long\" %}\n+  ins_encode %{\n+    __ negq($dst$$Register);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct negL_rReg_ndd(rRegL dst, rRegL src, immL0 zero, rFlagsReg cr)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (SubL zero src));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag);\n+\n+  format %{ \"enegq    $dst, $src\\t# long ndd\" %}\n+  ins_encode %{\n+    __ enegq($dst$$Register, $src$$Register, false);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct negL_rReg_2(rRegL dst, rFlagsReg cr)\n+%{\n+  predicate(!UseAPX);\n+  match(Set dst (NegL dst));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag);\n+\n+  format %{ \"negq    $dst\\t# int\" %}\n+  ins_encode %{\n+    __ negq($dst$$Register);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct negL_rReg_2_ndd(rRegL dst, rRegL src, rFlagsReg cr)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (NegL src));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag);\n+\n+  format %{ \"enegq    $dst, $src\\t# long ndd\" %}\n+  ins_encode %{\n+    __ enegq($dst$$Register, $src$$Register, false);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct negL_mem(memory dst, immL0 zero, rFlagsReg cr)\n+%{\n+  match(Set dst (StoreL dst (SubL zero (LoadL dst))));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag);\n+\n+  format %{ \"negq    $dst\\t# long\" %}\n+  ins_encode %{\n+    __ negq($dst$$Address);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+\/\/----------Multiplication\/Division Instructions-------------------------------\n+\/\/ Integer Multiplication Instructions\n+\/\/ Multiply Register\n+\n+instruct mulI_rReg(rRegI dst, rRegI src, rFlagsReg cr)\n+%{\n+  predicate(!UseAPX);\n+  match(Set dst (MulI dst src));\n+  effect(KILL cr);\n+\n+  ins_cost(300);\n+  format %{ \"imull   $dst, $src\\t# int\" %}\n+  ins_encode %{\n+    __ imull($dst$$Register, $src$$Register);\n+  %}\n+  ins_pipe(ialu_reg_reg_alu0);\n+%}\n+\n+instruct mulI_rReg_ndd(rRegI dst, rRegI src1, rRegI src2, rFlagsReg cr)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (MulI src1 src2));\n+  effect(KILL cr);\n+\n+  ins_cost(300);\n+  format %{ \"eimull   $dst, $src1, $src2\\t# int ndd\" %}\n+  ins_encode %{\n+    __ eimull($dst$$Register, $src1$$Register, $src2$$Register, false);\n+  %}\n+  ins_pipe(ialu_reg_reg_alu0);\n+%}\n+\n+instruct mulI_rReg_imm(rRegI dst, rRegI src, immI imm, rFlagsReg cr)\n+%{\n+  match(Set dst (MulI src imm));\n+  effect(KILL cr);\n+\n+  ins_cost(300);\n+  format %{ \"imull   $dst, $src, $imm\\t# int\" %}\n+  ins_encode %{\n+    __ imull($dst$$Register, $src$$Register, $imm$$constant);\n+  %}\n+  ins_pipe(ialu_reg_reg_alu0);\n+%}\n+\n+instruct mulI_mem(rRegI dst, memory src, rFlagsReg cr)\n+%{\n+  predicate(!UseAPX);\n+  match(Set dst (MulI dst (LoadI src)));\n+  effect(KILL cr);\n+\n+  ins_cost(350);\n+  format %{ \"imull   $dst, $src\\t# int\" %}\n+  ins_encode %{\n+    __ imull($dst$$Register, $src$$Address);\n+  %}\n+  ins_pipe(ialu_reg_mem_alu0);\n+%}\n+\n+instruct mulI_rReg_rReg_mem_ndd(rRegI dst, rRegI src1, memory src2, rFlagsReg cr)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (MulI src1 (LoadI src2)));\n+  effect(KILL cr);\n+\n+  ins_cost(350);\n+  format %{ \"eimull   $dst, $src1, $src2\\t# int ndd\" %}\n+  ins_encode %{\n+    __ eimull($dst$$Register, $src1$$Register, $src2$$Address, false);\n+  %}\n+  ins_pipe(ialu_reg_mem_alu0);\n+%}\n+\n+instruct mulI_mem_imm(rRegI dst, memory src, immI imm, rFlagsReg cr)\n+%{\n+  match(Set dst (MulI (LoadI src) imm));\n+  effect(KILL cr);\n+\n+  ins_cost(300);\n+  format %{ \"imull   $dst, $src, $imm\\t# int\" %}\n+  ins_encode %{\n+    __ imull($dst$$Register, $src$$Address, $imm$$constant);\n+  %}\n+  ins_pipe(ialu_reg_mem_alu0);\n+%}\n+\n+instruct mulAddS2I_rReg(rRegI dst, rRegI src1, rRegI src2, rRegI src3, rFlagsReg cr)\n+%{\n+  match(Set dst (MulAddS2I (Binary dst src1) (Binary src2 src3)));\n+  effect(KILL cr, KILL src2);\n+\n+  expand %{ mulI_rReg(dst, src1, cr);\n+           mulI_rReg(src2, src3, cr);\n+           addI_rReg(dst, src2, cr); %}\n+%}\n+\n+instruct mulL_rReg(rRegL dst, rRegL src, rFlagsReg cr)\n+%{\n+  predicate(!UseAPX);\n+  match(Set dst (MulL dst src));\n+  effect(KILL cr);\n+\n+  ins_cost(300);\n+  format %{ \"imulq   $dst, $src\\t# long\" %}\n+  ins_encode %{\n+    __ imulq($dst$$Register, $src$$Register);\n+  %}\n+  ins_pipe(ialu_reg_reg_alu0);\n+%}\n+\n+instruct mulL_rReg_ndd(rRegL dst, rRegL src1, rRegL src2, rFlagsReg cr)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (MulL src1 src2));\n+  effect(KILL cr);\n+\n+  ins_cost(300);\n+  format %{ \"eimulq   $dst, $src1, $src2\\t# long ndd\" %}\n+  ins_encode %{\n+    __ eimulq($dst$$Register, $src1$$Register, $src2$$Register, false);\n+  %}\n+  ins_pipe(ialu_reg_reg_alu0);\n+%}\n+\n+instruct mulL_rReg_imm(rRegL dst, rRegL src, immL32 imm, rFlagsReg cr)\n+%{\n+  match(Set dst (MulL src imm));\n+  effect(KILL cr);\n+\n+  ins_cost(300);\n+  format %{ \"imulq   $dst, $src, $imm\\t# long\" %}\n+  ins_encode %{\n+    __ imulq($dst$$Register, $src$$Register, $imm$$constant);\n+  %}\n+  ins_pipe(ialu_reg_reg_alu0);\n+%}\n+\n+instruct mulL_mem(rRegL dst, memory src, rFlagsReg cr)\n+%{\n+  predicate(!UseAPX);\n+  match(Set dst (MulL dst (LoadL src)));\n+  effect(KILL cr);\n+\n+  ins_cost(350);\n+  format %{ \"imulq   $dst, $src\\t# long\" %}\n+  ins_encode %{\n+    __ imulq($dst$$Register, $src$$Address);\n+  %}\n+  ins_pipe(ialu_reg_mem_alu0);\n+%}\n+\n+instruct mulL_rReg_rReg_mem_ndd(rRegL dst, rRegL src1, memory src2, rFlagsReg cr)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (MulL src1 (LoadL src2)));\n+  effect(KILL cr);\n+\n+  ins_cost(350);\n+  format %{ \"eimulq   $dst, $src1, $src2 \\t# long\" %}\n+  ins_encode %{\n+    __ eimulq($dst$$Register, $src1$$Register, $src2$$Address, false);\n+  %}\n+  ins_pipe(ialu_reg_mem_alu0);\n+%}\n+\n+instruct mulL_mem_imm(rRegL dst, memory src, immL32 imm, rFlagsReg cr)\n+%{\n+  match(Set dst (MulL (LoadL src) imm));\n+  effect(KILL cr);\n+\n+  ins_cost(300);\n+  format %{ \"imulq   $dst, $src, $imm\\t# long\" %}\n+  ins_encode %{\n+    __ imulq($dst$$Register, $src$$Address, $imm$$constant);\n+  %}\n+  ins_pipe(ialu_reg_mem_alu0);\n+%}\n+\n+instruct mulHiL_rReg(rdx_RegL dst, rRegL src, rax_RegL rax, rFlagsReg cr)\n+%{\n+  match(Set dst (MulHiL src rax));\n+  effect(USE_KILL rax, KILL cr);\n+\n+  ins_cost(300);\n+  format %{ \"imulq   RDX:RAX, RAX, $src\\t# mulhi\" %}\n+  ins_encode %{\n+    __ imulq($src$$Register);\n+  %}\n+  ins_pipe(ialu_reg_reg_alu0);\n+%}\n+\n+instruct umulHiL_rReg(rdx_RegL dst, rRegL src, rax_RegL rax, rFlagsReg cr)\n+%{\n+  match(Set dst (UMulHiL src rax));\n+  effect(USE_KILL rax, KILL cr);\n+\n+  ins_cost(300);\n+  format %{ \"mulq   RDX:RAX, RAX, $src\\t# umulhi\" %}\n+  ins_encode %{\n+    __ mulq($src$$Register);\n+  %}\n+  ins_pipe(ialu_reg_reg_alu0);\n+%}\n+\n+instruct divI_rReg(rax_RegI rax, rdx_RegI rdx, no_rax_rdx_RegI div,\n+                   rFlagsReg cr)\n+%{\n+  match(Set rax (DivI rax div));\n+  effect(KILL rdx, KILL cr);\n+\n+  ins_cost(30*100+10*100); \/\/ XXX\n+  format %{ \"cmpl    rax, 0x80000000\\t# idiv\\n\\t\"\n+            \"jne,s   normal\\n\\t\"\n+            \"xorl    rdx, rdx\\n\\t\"\n+            \"cmpl    $div, -1\\n\\t\"\n+            \"je,s    done\\n\"\n+    \"normal: cdql\\n\\t\"\n+            \"idivl   $div\\n\"\n+    \"done:\"        %}\n+  ins_encode(cdql_enc(div));\n+  ins_pipe(ialu_reg_reg_alu0);\n+%}\n+\n+instruct divL_rReg(rax_RegL rax, rdx_RegL rdx, no_rax_rdx_RegL div,\n+                   rFlagsReg cr)\n+%{\n+  match(Set rax (DivL rax div));\n+  effect(KILL rdx, KILL cr);\n+\n+  ins_cost(30*100+10*100); \/\/ XXX\n+  format %{ \"movq    rdx, 0x8000000000000000\\t# ldiv\\n\\t\"\n+            \"cmpq    rax, rdx\\n\\t\"\n+            \"jne,s   normal\\n\\t\"\n+            \"xorl    rdx, rdx\\n\\t\"\n+            \"cmpq    $div, -1\\n\\t\"\n+            \"je,s    done\\n\"\n+    \"normal: cdqq\\n\\t\"\n+            \"idivq   $div\\n\"\n+    \"done:\"        %}\n+  ins_encode(cdqq_enc(div));\n+  ins_pipe(ialu_reg_reg_alu0);\n+%}\n+\n+instruct udivI_rReg(rax_RegI rax, rdx_RegI rdx, no_rax_rdx_RegI div, rFlagsReg cr)\n+%{\n+  match(Set rax (UDivI rax div));\n+  effect(KILL rdx, KILL cr);\n+\n+  ins_cost(300);\n+  format %{ \"udivl $rax,$rax,$div\\t# UDivI\\n\" %}\n+  ins_encode %{\n+    __ udivI($rax$$Register, $div$$Register, $rdx$$Register);\n+  %}\n+  ins_pipe(ialu_reg_reg_alu0);\n+%}\n+\n+instruct udivL_rReg(rax_RegL rax, rdx_RegL rdx, no_rax_rdx_RegL div, rFlagsReg cr)\n+%{\n+  match(Set rax (UDivL rax div));\n+  effect(KILL rdx, KILL cr);\n+\n+  ins_cost(300);\n+  format %{ \"udivq $rax,$rax,$div\\t# UDivL\\n\" %}\n+  ins_encode %{\n+     __ udivL($rax$$Register, $div$$Register, $rdx$$Register);\n+  %}\n+  ins_pipe(ialu_reg_reg_alu0);\n+%}\n+\n+\/\/ Integer DIVMOD with Register, both quotient and mod results\n+instruct divModI_rReg_divmod(rax_RegI rax, rdx_RegI rdx, no_rax_rdx_RegI div,\n+                             rFlagsReg cr)\n+%{\n+  match(DivModI rax div);\n+  effect(KILL cr);\n+\n+  ins_cost(30*100+10*100); \/\/ XXX\n+  format %{ \"cmpl    rax, 0x80000000\\t# idiv\\n\\t\"\n+            \"jne,s   normal\\n\\t\"\n+            \"xorl    rdx, rdx\\n\\t\"\n+            \"cmpl    $div, -1\\n\\t\"\n+            \"je,s    done\\n\"\n+    \"normal: cdql\\n\\t\"\n+            \"idivl   $div\\n\"\n+    \"done:\"        %}\n+  ins_encode(cdql_enc(div));\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ Long DIVMOD with Register, both quotient and mod results\n+instruct divModL_rReg_divmod(rax_RegL rax, rdx_RegL rdx, no_rax_rdx_RegL div,\n+                             rFlagsReg cr)\n+%{\n+  match(DivModL rax div);\n+  effect(KILL cr);\n+\n+  ins_cost(30*100+10*100); \/\/ XXX\n+  format %{ \"movq    rdx, 0x8000000000000000\\t# ldiv\\n\\t\"\n+            \"cmpq    rax, rdx\\n\\t\"\n+            \"jne,s   normal\\n\\t\"\n+            \"xorl    rdx, rdx\\n\\t\"\n+            \"cmpq    $div, -1\\n\\t\"\n+            \"je,s    done\\n\"\n+    \"normal: cdqq\\n\\t\"\n+            \"idivq   $div\\n\"\n+    \"done:\"        %}\n+  ins_encode(cdqq_enc(div));\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ Unsigned integer DIVMOD with Register, both quotient and mod results\n+instruct udivModI_rReg_divmod(rax_RegI rax, no_rax_rdx_RegI tmp, rdx_RegI rdx,\n+                              no_rax_rdx_RegI div, rFlagsReg cr)\n+%{\n+  match(UDivModI rax div);\n+  effect(TEMP tmp, KILL cr);\n+\n+  ins_cost(300);\n+  format %{ \"udivl $rax,$rax,$div\\t# begin UDivModI\\n\\t\"\n+            \"umodl $rdx,$rax,$div\\t! using $tmp as TEMP # end UDivModI\\n\"\n+          %}\n+  ins_encode %{\n+    __ udivmodI($rax$$Register, $div$$Register, $rdx$$Register, $tmp$$Register);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ Unsigned long DIVMOD with Register, both quotient and mod results\n+instruct udivModL_rReg_divmod(rax_RegL rax, no_rax_rdx_RegL tmp, rdx_RegL rdx,\n+                              no_rax_rdx_RegL div, rFlagsReg cr)\n+%{\n+  match(UDivModL rax div);\n+  effect(TEMP tmp, KILL cr);\n+\n+  ins_cost(300);\n+  format %{ \"udivq $rax,$rax,$div\\t# begin UDivModL\\n\\t\"\n+            \"umodq $rdx,$rax,$div\\t! using $tmp as TEMP # end UDivModL\\n\"\n+          %}\n+  ins_encode %{\n+    __ udivmodL($rax$$Register, $div$$Register, $rdx$$Register, $tmp$$Register);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct modI_rReg(rdx_RegI rdx, rax_RegI rax, no_rax_rdx_RegI div,\n+                   rFlagsReg cr)\n+%{\n+  match(Set rdx (ModI rax div));\n+  effect(KILL rax, KILL cr);\n+\n+  ins_cost(300); \/\/ XXX\n+  format %{ \"cmpl    rax, 0x80000000\\t# irem\\n\\t\"\n+            \"jne,s   normal\\n\\t\"\n+            \"xorl    rdx, rdx\\n\\t\"\n+            \"cmpl    $div, -1\\n\\t\"\n+            \"je,s    done\\n\"\n+    \"normal: cdql\\n\\t\"\n+            \"idivl   $div\\n\"\n+    \"done:\"        %}\n+  ins_encode(cdql_enc(div));\n+  ins_pipe(ialu_reg_reg_alu0);\n+%}\n+\n+instruct modL_rReg(rdx_RegL rdx, rax_RegL rax, no_rax_rdx_RegL div,\n+                   rFlagsReg cr)\n+%{\n+  match(Set rdx (ModL rax div));\n+  effect(KILL rax, KILL cr);\n+\n+  ins_cost(300); \/\/ XXX\n+  format %{ \"movq    rdx, 0x8000000000000000\\t# lrem\\n\\t\"\n+            \"cmpq    rax, rdx\\n\\t\"\n+            \"jne,s   normal\\n\\t\"\n+            \"xorl    rdx, rdx\\n\\t\"\n+            \"cmpq    $div, -1\\n\\t\"\n+            \"je,s    done\\n\"\n+    \"normal: cdqq\\n\\t\"\n+            \"idivq   $div\\n\"\n+    \"done:\"        %}\n+  ins_encode(cdqq_enc(div));\n+  ins_pipe(ialu_reg_reg_alu0);\n+%}\n+\n+instruct umodI_rReg(rdx_RegI rdx, rax_RegI rax, no_rax_rdx_RegI div, rFlagsReg cr)\n+%{\n+  match(Set rdx (UModI rax div));\n+  effect(KILL rax, KILL cr);\n+\n+  ins_cost(300);\n+  format %{ \"umodl $rdx,$rax,$div\\t# UModI\\n\" %}\n+  ins_encode %{\n+    __ umodI($rax$$Register, $div$$Register, $rdx$$Register);\n+  %}\n+  ins_pipe(ialu_reg_reg_alu0);\n+%}\n+\n+instruct umodL_rReg(rdx_RegL rdx, rax_RegL rax, no_rax_rdx_RegL div, rFlagsReg cr)\n+%{\n+  match(Set rdx (UModL rax div));\n+  effect(KILL rax, KILL cr);\n+\n+  ins_cost(300);\n+  format %{ \"umodq $rdx,$rax,$div\\t# UModL\\n\" %}\n+  ins_encode %{\n+    __ umodL($rax$$Register, $div$$Register, $rdx$$Register);\n+  %}\n+  ins_pipe(ialu_reg_reg_alu0);\n+%}\n+\n+\/\/ Integer Shift Instructions\n+\/\/ Shift Left by one, two, three\n+instruct salI_rReg_immI2(rRegI dst, immI2 shift, rFlagsReg cr)\n+%{\n+  predicate(!UseAPX);\n+  match(Set dst (LShiftI dst shift));\n+  effect(KILL cr);\n+\n+  format %{ \"sall    $dst, $shift\" %}\n+  ins_encode %{\n+    __ sall($dst$$Register, $shift$$constant);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+\/\/ Shift Left by one, two, three\n+instruct salI_rReg_immI2_ndd(rRegI dst, rRegI src, immI2 shift, rFlagsReg cr)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (LShiftI src shift));\n+  effect(KILL cr);\n+\n+  format %{ \"esall    $dst, $src, $shift\\t# int(ndd)\" %}\n+  ins_encode %{\n+    __ esall($dst$$Register, $src$$Register, $shift$$constant, false);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+\/\/ Shift Left by 8-bit immediate\n+instruct salI_rReg_imm(rRegI dst, immI8 shift, rFlagsReg cr)\n+%{\n+  predicate(!UseAPX);\n+  match(Set dst (LShiftI dst shift));\n+  effect(KILL cr);\n+\n+  format %{ \"sall    $dst, $shift\" %}\n+  ins_encode %{\n+    __ sall($dst$$Register, $shift$$constant);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+\/\/ Shift Left by 8-bit immediate\n+instruct salI_rReg_imm_ndd(rRegI dst, rRegI src, immI8 shift, rFlagsReg cr)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (LShiftI src shift));\n+  effect(KILL cr);\n+\n+  format %{ \"esall    $dst, $src, $shift\\t# int (ndd)\" %}\n+  ins_encode %{\n+    __ esall($dst$$Register, $src$$Register, $shift$$constant, false);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct salI_rReg_mem_imm_ndd(rRegI dst, memory src, immI8 shift, rFlagsReg cr)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (LShiftI (LoadI src) shift));\n+  effect(KILL cr);\n+\n+  format %{ \"esall    $dst, $src, $shift\\t# int (ndd)\" %}\n+  ins_encode %{\n+    __ esall($dst$$Register, $src$$Address, $shift$$constant, false);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+\/\/ Shift Left by 8-bit immediate\n+instruct salI_mem_imm(memory dst, immI8 shift, rFlagsReg cr)\n+%{\n+  match(Set dst (StoreI dst (LShiftI (LoadI dst) shift)));\n+  effect(KILL cr);\n+\n+  format %{ \"sall    $dst, $shift\" %}\n+  ins_encode %{\n+    __ sall($dst$$Address, $shift$$constant);\n+  %}\n+  ins_pipe(ialu_mem_imm);\n+%}\n+\n+\/\/ Shift Left by variable\n+instruct salI_rReg_CL(rRegI dst, rcx_RegI shift, rFlagsReg cr)\n+%{\n+  predicate(!VM_Version::supports_bmi2());\n+  match(Set dst (LShiftI dst shift));\n+  effect(KILL cr);\n+\n+  format %{ \"sall    $dst, $shift\" %}\n+  ins_encode %{\n+    __ sall($dst$$Register);\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+\/\/ Shift Left by variable\n+instruct salI_mem_CL(memory dst, rcx_RegI shift, rFlagsReg cr)\n+%{\n+  predicate(!VM_Version::supports_bmi2());\n+  match(Set dst (StoreI dst (LShiftI (LoadI dst) shift)));\n+  effect(KILL cr);\n+\n+  format %{ \"sall    $dst, $shift\" %}\n+  ins_encode %{\n+    __ sall($dst$$Address);\n+  %}\n+  ins_pipe(ialu_mem_reg);\n+%}\n+\n+instruct salI_rReg_rReg(rRegI dst, rRegI src, rRegI shift)\n+%{\n+  predicate(VM_Version::supports_bmi2());\n+  match(Set dst (LShiftI src shift));\n+\n+  format %{ \"shlxl   $dst, $src, $shift\" %}\n+  ins_encode %{\n+    __ shlxl($dst$$Register, $src$$Register, $shift$$Register);\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+instruct salI_mem_rReg(rRegI dst, memory src, rRegI shift)\n+%{\n+  predicate(VM_Version::supports_bmi2());\n+  match(Set dst (LShiftI (LoadI src) shift));\n+  ins_cost(175);\n+  format %{ \"shlxl   $dst, $src, $shift\" %}\n+  ins_encode %{\n+    __ shlxl($dst$$Register, $src$$Address, $shift$$Register);\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+\/\/ Arithmetic Shift Right by 8-bit immediate\n+instruct sarI_rReg_imm(rRegI dst, immI8 shift, rFlagsReg cr)\n+%{\n+  predicate(!UseAPX);\n+  match(Set dst (RShiftI dst shift));\n+  effect(KILL cr);\n+\n+  format %{ \"sarl    $dst, $shift\" %}\n+  ins_encode %{\n+    __ sarl($dst$$Register, $shift$$constant);\n+  %}\n+  ins_pipe(ialu_mem_imm);\n+%}\n+\n+\/\/ Arithmetic Shift Right by 8-bit immediate\n+instruct sarI_rReg_imm_ndd(rRegI dst, rRegI src, immI8 shift, rFlagsReg cr)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (RShiftI src shift));\n+  effect(KILL cr);\n+\n+  format %{ \"esarl    $dst, $src, $shift\\t# int (ndd)\" %}\n+  ins_encode %{\n+    __ esarl($dst$$Register, $src$$Register, $shift$$constant, false);\n+  %}\n+  ins_pipe(ialu_mem_imm);\n+%}\n+\n+instruct sarI_rReg_mem_imm_ndd(rRegI dst, memory src, immI8 shift, rFlagsReg cr)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (RShiftI (LoadI src) shift));\n+  effect(KILL cr);\n+\n+  format %{ \"esarl    $dst, $src, $shift\\t# int (ndd)\" %}\n+  ins_encode %{\n+    __ esarl($dst$$Register, $src$$Address, $shift$$constant, false);\n+  %}\n+  ins_pipe(ialu_mem_imm);\n+%}\n+\n+\/\/ Arithmetic Shift Right by 8-bit immediate\n+instruct sarI_mem_imm(memory dst, immI8 shift, rFlagsReg cr)\n+%{\n+  match(Set dst (StoreI dst (RShiftI (LoadI dst) shift)));\n+  effect(KILL cr);\n+\n+  format %{ \"sarl    $dst, $shift\" %}\n+  ins_encode %{\n+    __ sarl($dst$$Address, $shift$$constant);\n+  %}\n+  ins_pipe(ialu_mem_imm);\n+%}\n+\n+\/\/ Arithmetic Shift Right by variable\n+instruct sarI_rReg_CL(rRegI dst, rcx_RegI shift, rFlagsReg cr)\n+%{\n+  predicate(!VM_Version::supports_bmi2());\n+  match(Set dst (RShiftI dst shift));\n+  effect(KILL cr);\n+\n+  format %{ \"sarl    $dst, $shift\" %}\n+  ins_encode %{\n+    __ sarl($dst$$Register);\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+\/\/ Arithmetic Shift Right by variable\n+instruct sarI_mem_CL(memory dst, rcx_RegI shift, rFlagsReg cr)\n+%{\n+  predicate(!VM_Version::supports_bmi2());\n+  match(Set dst (StoreI dst (RShiftI (LoadI dst) shift)));\n+  effect(KILL cr);\n+\n+  format %{ \"sarl    $dst, $shift\" %}\n+  ins_encode %{\n+    __ sarl($dst$$Address);\n+  %}\n+  ins_pipe(ialu_mem_reg);\n+%}\n+\n+instruct sarI_rReg_rReg(rRegI dst, rRegI src, rRegI shift)\n+%{\n+  predicate(VM_Version::supports_bmi2());\n+  match(Set dst (RShiftI src shift));\n+\n+  format %{ \"sarxl   $dst, $src, $shift\" %}\n+  ins_encode %{\n+    __ sarxl($dst$$Register, $src$$Register, $shift$$Register);\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+instruct sarI_mem_rReg(rRegI dst, memory src, rRegI shift)\n+%{\n+  predicate(VM_Version::supports_bmi2());\n+  match(Set dst (RShiftI (LoadI src) shift));\n+  ins_cost(175);\n+  format %{ \"sarxl   $dst, $src, $shift\" %}\n+  ins_encode %{\n+    __ sarxl($dst$$Register, $src$$Address, $shift$$Register);\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+\/\/ Logical Shift Right by 8-bit immediate\n+instruct shrI_rReg_imm(rRegI dst, immI8 shift, rFlagsReg cr)\n+%{\n+  predicate(!UseAPX);\n+  match(Set dst (URShiftI dst shift));\n+  effect(KILL cr);\n+\n+  format %{ \"shrl    $dst, $shift\" %}\n+  ins_encode %{\n+    __ shrl($dst$$Register, $shift$$constant);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+\/\/ Logical Shift Right by 8-bit immediate\n+instruct shrI_rReg_imm_ndd(rRegI dst, rRegI src, immI8 shift, rFlagsReg cr)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (URShiftI src shift));\n+  effect(KILL cr);\n+\n+  format %{ \"eshrl    $dst, $src, $shift\\t # int (ndd)\" %}\n+  ins_encode %{\n+    __ eshrl($dst$$Register, $src$$Register, $shift$$constant, false);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct shrI_rReg_mem_imm_ndd(rRegI dst, memory src, immI8 shift, rFlagsReg cr)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (URShiftI (LoadI src) shift));\n+  effect(KILL cr);\n+\n+  format %{ \"eshrl    $dst, $src, $shift\\t # int (ndd)\" %}\n+  ins_encode %{\n+    __ eshrl($dst$$Register, $src$$Address, $shift$$constant, false);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+\/\/ Logical Shift Right by 8-bit immediate\n+instruct shrI_mem_imm(memory dst, immI8 shift, rFlagsReg cr)\n+%{\n+  match(Set dst (StoreI dst (URShiftI (LoadI dst) shift)));\n+  effect(KILL cr);\n+\n+  format %{ \"shrl    $dst, $shift\" %}\n+  ins_encode %{\n+    __ shrl($dst$$Address, $shift$$constant);\n+  %}\n+  ins_pipe(ialu_mem_imm);\n+%}\n+\n+\/\/ Logical Shift Right by variable\n+instruct shrI_rReg_CL(rRegI dst, rcx_RegI shift, rFlagsReg cr)\n+%{\n+  predicate(!VM_Version::supports_bmi2());\n+  match(Set dst (URShiftI dst shift));\n+  effect(KILL cr);\n+\n+  format %{ \"shrl    $dst, $shift\" %}\n+  ins_encode %{\n+    __ shrl($dst$$Register);\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+\/\/ Logical Shift Right by variable\n+instruct shrI_mem_CL(memory dst, rcx_RegI shift, rFlagsReg cr)\n+%{\n+  predicate(!VM_Version::supports_bmi2());\n+  match(Set dst (StoreI dst (URShiftI (LoadI dst) shift)));\n+  effect(KILL cr);\n+\n+  format %{ \"shrl    $dst, $shift\" %}\n+  ins_encode %{\n+    __ shrl($dst$$Address);\n+  %}\n+  ins_pipe(ialu_mem_reg);\n+%}\n+\n+instruct shrI_rReg_rReg(rRegI dst, rRegI src, rRegI shift)\n+%{\n+  predicate(VM_Version::supports_bmi2());\n+  match(Set dst (URShiftI src shift));\n+\n+  format %{ \"shrxl   $dst, $src, $shift\" %}\n+  ins_encode %{\n+    __ shrxl($dst$$Register, $src$$Register, $shift$$Register);\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+instruct shrI_mem_rReg(rRegI dst, memory src, rRegI shift)\n+%{\n+  predicate(VM_Version::supports_bmi2());\n+  match(Set dst (URShiftI (LoadI src) shift));\n+  ins_cost(175);\n+  format %{ \"shrxl   $dst, $src, $shift\" %}\n+  ins_encode %{\n+    __ shrxl($dst$$Register, $src$$Address, $shift$$Register);\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+\/\/ Long Shift Instructions\n+\/\/ Shift Left by one, two, three\n+instruct salL_rReg_immI2(rRegL dst, immI2 shift, rFlagsReg cr)\n+%{\n+  predicate(!UseAPX);\n+  match(Set dst (LShiftL dst shift));\n+  effect(KILL cr);\n+\n+  format %{ \"salq    $dst, $shift\" %}\n+  ins_encode %{\n+    __ salq($dst$$Register, $shift$$constant);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+\/\/ Shift Left by one, two, three\n+instruct salL_rReg_immI2_ndd(rRegL dst, rRegL src, immI2 shift, rFlagsReg cr)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (LShiftL src shift));\n+  effect(KILL cr);\n+\n+  format %{ \"esalq    $dst, $src, $shift\\t# long (ndd)\" %}\n+  ins_encode %{\n+    __ esalq($dst$$Register, $src$$Register, $shift$$constant, false);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+\/\/ Shift Left by 8-bit immediate\n+instruct salL_rReg_imm(rRegL dst, immI8 shift, rFlagsReg cr)\n+%{\n+  predicate(!UseAPX);\n+  match(Set dst (LShiftL dst shift));\n+  effect(KILL cr);\n+\n+  format %{ \"salq    $dst, $shift\" %}\n+  ins_encode %{\n+    __ salq($dst$$Register, $shift$$constant);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+\/\/ Shift Left by 8-bit immediate\n+instruct salL_rReg_imm_ndd(rRegL dst, rRegL src, immI8 shift, rFlagsReg cr)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (LShiftL src shift));\n+  effect(KILL cr);\n+\n+  format %{ \"esalq    $dst, $src, $shift\\t# long (ndd)\" %}\n+  ins_encode %{\n+    __ esalq($dst$$Register, $src$$Register, $shift$$constant, false);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct salL_rReg_mem_imm_ndd(rRegL dst, memory src, immI8 shift, rFlagsReg cr)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (LShiftL (LoadL src) shift));\n+  effect(KILL cr);\n+\n+  format %{ \"esalq    $dst, $src, $shift\\t# long (ndd)\" %}\n+  ins_encode %{\n+    __ esalq($dst$$Register, $src$$Address, $shift$$constant, false);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+\/\/ Shift Left by 8-bit immediate\n+instruct salL_mem_imm(memory dst, immI8 shift, rFlagsReg cr)\n+%{\n+  match(Set dst (StoreL dst (LShiftL (LoadL dst) shift)));\n+  effect(KILL cr);\n+\n+  format %{ \"salq    $dst, $shift\" %}\n+  ins_encode %{\n+    __ salq($dst$$Address, $shift$$constant);\n+  %}\n+  ins_pipe(ialu_mem_imm);\n+%}\n+\n+\/\/ Shift Left by variable\n+instruct salL_rReg_CL(rRegL dst, rcx_RegI shift, rFlagsReg cr)\n+%{\n+  predicate(!VM_Version::supports_bmi2());\n+  match(Set dst (LShiftL dst shift));\n+  effect(KILL cr);\n+\n+  format %{ \"salq    $dst, $shift\" %}\n+  ins_encode %{\n+    __ salq($dst$$Register);\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+\/\/ Shift Left by variable\n+instruct salL_mem_CL(memory dst, rcx_RegI shift, rFlagsReg cr)\n+%{\n+  predicate(!VM_Version::supports_bmi2());\n+  match(Set dst (StoreL dst (LShiftL (LoadL dst) shift)));\n+  effect(KILL cr);\n+\n+  format %{ \"salq    $dst, $shift\" %}\n+  ins_encode %{\n+    __ salq($dst$$Address);\n+  %}\n+  ins_pipe(ialu_mem_reg);\n+%}\n+\n+instruct salL_rReg_rReg(rRegL dst, rRegL src, rRegI shift)\n+%{\n+  predicate(VM_Version::supports_bmi2());\n+  match(Set dst (LShiftL src shift));\n+\n+  format %{ \"shlxq   $dst, $src, $shift\" %}\n+  ins_encode %{\n+    __ shlxq($dst$$Register, $src$$Register, $shift$$Register);\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+instruct salL_mem_rReg(rRegL dst, memory src, rRegI shift)\n+%{\n+  predicate(VM_Version::supports_bmi2());\n+  match(Set dst (LShiftL (LoadL src) shift));\n+  ins_cost(175);\n+  format %{ \"shlxq   $dst, $src, $shift\" %}\n+  ins_encode %{\n+    __ shlxq($dst$$Register, $src$$Address, $shift$$Register);\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+\/\/ Arithmetic Shift Right by 8-bit immediate\n+instruct sarL_rReg_imm(rRegL dst, immI shift, rFlagsReg cr)\n+%{\n+  predicate(!UseAPX);\n+  match(Set dst (RShiftL dst shift));\n+  effect(KILL cr);\n+\n+  format %{ \"sarq    $dst, $shift\" %}\n+  ins_encode %{\n+    __ sarq($dst$$Register, (unsigned char)($shift$$constant & 0x3F));\n+  %}\n+  ins_pipe(ialu_mem_imm);\n+%}\n+\n+\/\/ Arithmetic Shift Right by 8-bit immediate\n+instruct sarL_rReg_imm_ndd(rRegL dst, rRegL src, immI shift, rFlagsReg cr)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (RShiftL src shift));\n+  effect(KILL cr);\n+\n+  format %{ \"esarq    $dst, $src, $shift\\t# long (ndd)\" %}\n+  ins_encode %{\n+    __ esarq($dst$$Register, $src$$Register, (unsigned char)($shift$$constant & 0x3F), false);\n+  %}\n+  ins_pipe(ialu_mem_imm);\n+%}\n+\n+instruct sarL_rReg_mem_imm_ndd(rRegL dst, memory src, immI shift, rFlagsReg cr)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (RShiftL (LoadL src) shift));\n+  effect(KILL cr);\n+\n+  format %{ \"esarq    $dst, $src, $shift\\t# long (ndd)\" %}\n+  ins_encode %{\n+    __ esarq($dst$$Register, $src$$Address, (unsigned char)($shift$$constant & 0x3F), false);\n+  %}\n+  ins_pipe(ialu_mem_imm);\n+%}\n+\n+\/\/ Arithmetic Shift Right by 8-bit immediate\n+instruct sarL_mem_imm(memory dst, immI shift, rFlagsReg cr)\n+%{\n+  match(Set dst (StoreL dst (RShiftL (LoadL dst) shift)));\n+  effect(KILL cr);\n+\n+  format %{ \"sarq    $dst, $shift\" %}\n+  ins_encode %{\n+    __ sarq($dst$$Address, (unsigned char)($shift$$constant & 0x3F));\n+  %}\n+  ins_pipe(ialu_mem_imm);\n+%}\n+\n+\/\/ Arithmetic Shift Right by variable\n+instruct sarL_rReg_CL(rRegL dst, rcx_RegI shift, rFlagsReg cr)\n+%{\n+  predicate(!VM_Version::supports_bmi2());\n+  match(Set dst (RShiftL dst shift));\n+  effect(KILL cr);\n+\n+  format %{ \"sarq    $dst, $shift\" %}\n+  ins_encode %{\n+    __ sarq($dst$$Register);\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+\/\/ Arithmetic Shift Right by variable\n+instruct sarL_mem_CL(memory dst, rcx_RegI shift, rFlagsReg cr)\n+%{\n+  predicate(!VM_Version::supports_bmi2());\n+  match(Set dst (StoreL dst (RShiftL (LoadL dst) shift)));\n+  effect(KILL cr);\n+\n+  format %{ \"sarq    $dst, $shift\" %}\n+  ins_encode %{\n+    __ sarq($dst$$Address);\n+  %}\n+  ins_pipe(ialu_mem_reg);\n+%}\n+\n+instruct sarL_rReg_rReg(rRegL dst, rRegL src, rRegI shift)\n+%{\n+  predicate(VM_Version::supports_bmi2());\n+  match(Set dst (RShiftL src shift));\n+\n+  format %{ \"sarxq   $dst, $src, $shift\" %}\n+  ins_encode %{\n+    __ sarxq($dst$$Register, $src$$Register, $shift$$Register);\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+instruct sarL_mem_rReg(rRegL dst, memory src, rRegI shift)\n+%{\n+  predicate(VM_Version::supports_bmi2());\n+  match(Set dst (RShiftL (LoadL src) shift));\n+  ins_cost(175);\n+  format %{ \"sarxq   $dst, $src, $shift\" %}\n+  ins_encode %{\n+    __ sarxq($dst$$Register, $src$$Address, $shift$$Register);\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+\/\/ Logical Shift Right by 8-bit immediate\n+instruct shrL_rReg_imm(rRegL dst, immI8 shift, rFlagsReg cr)\n+%{\n+  predicate(!UseAPX);\n+  match(Set dst (URShiftL dst shift));\n+  effect(KILL cr);\n+\n+  format %{ \"shrq    $dst, $shift\" %}\n+  ins_encode %{\n+    __ shrq($dst$$Register, $shift$$constant);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+\/\/ Logical Shift Right by 8-bit immediate\n+instruct shrL_rReg_imm_ndd(rRegL dst, rRegL src, immI8 shift, rFlagsReg cr)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (URShiftL src shift));\n+  effect(KILL cr);\n+\n+  format %{ \"eshrq    $dst, $src, $shift\\t# long (ndd)\" %}\n+  ins_encode %{\n+    __ eshrq($dst$$Register, $src$$Register, $shift$$constant, false);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct shrL_rReg_mem_imm_ndd(rRegL dst, memory src, immI8 shift, rFlagsReg cr)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (URShiftL (LoadL src) shift));\n+  effect(KILL cr);\n+\n+  format %{ \"eshrq    $dst, $src, $shift\\t# long (ndd)\" %}\n+  ins_encode %{\n+    __ eshrq($dst$$Register, $src$$Address, $shift$$constant, false);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+\/\/ Logical Shift Right by 8-bit immediate\n+instruct shrL_mem_imm(memory dst, immI8 shift, rFlagsReg cr)\n+%{\n+  match(Set dst (StoreL dst (URShiftL (LoadL dst) shift)));\n+  effect(KILL cr);\n+\n+  format %{ \"shrq    $dst, $shift\" %}\n+  ins_encode %{\n+    __ shrq($dst$$Address, $shift$$constant);\n+  %}\n+  ins_pipe(ialu_mem_imm);\n+%}\n+\n+\/\/ Logical Shift Right by variable\n+instruct shrL_rReg_CL(rRegL dst, rcx_RegI shift, rFlagsReg cr)\n+%{\n+  predicate(!VM_Version::supports_bmi2());\n+  match(Set dst (URShiftL dst shift));\n+  effect(KILL cr);\n+\n+  format %{ \"shrq    $dst, $shift\" %}\n+  ins_encode %{\n+    __ shrq($dst$$Register);\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+\/\/ Logical Shift Right by variable\n+instruct shrL_mem_CL(memory dst, rcx_RegI shift, rFlagsReg cr)\n+%{\n+  predicate(!VM_Version::supports_bmi2());\n+  match(Set dst (StoreL dst (URShiftL (LoadL dst) shift)));\n+  effect(KILL cr);\n+\n+  format %{ \"shrq    $dst, $shift\" %}\n+  ins_encode %{\n+    __ shrq($dst$$Address);\n+  %}\n+  ins_pipe(ialu_mem_reg);\n+%}\n+\n+instruct shrL_rReg_rReg(rRegL dst, rRegL src, rRegI shift)\n+%{\n+  predicate(VM_Version::supports_bmi2());\n+  match(Set dst (URShiftL src shift));\n+\n+  format %{ \"shrxq   $dst, $src, $shift\" %}\n+  ins_encode %{\n+    __ shrxq($dst$$Register, $src$$Register, $shift$$Register);\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+instruct shrL_mem_rReg(rRegL dst, memory src, rRegI shift)\n+%{\n+  predicate(VM_Version::supports_bmi2());\n+  match(Set dst (URShiftL (LoadL src) shift));\n+  ins_cost(175);\n+  format %{ \"shrxq   $dst, $src, $shift\" %}\n+  ins_encode %{\n+    __ shrxq($dst$$Register, $src$$Address, $shift$$Register);\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+\/\/ Logical Shift Right by 24, followed by Arithmetic Shift Left by 24.\n+\/\/ This idiom is used by the compiler for the i2b bytecode.\n+instruct i2b(rRegI dst, rRegI src, immI_24 twentyfour)\n+%{\n+  match(Set dst (RShiftI (LShiftI src twentyfour) twentyfour));\n+\n+  format %{ \"movsbl  $dst, $src\\t# i2b\" %}\n+  ins_encode %{\n+    __ movsbl($dst$$Register, $src$$Register);\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+\/\/ Logical Shift Right by 16, followed by Arithmetic Shift Left by 16.\n+\/\/ This idiom is used by the compiler the i2s bytecode.\n+instruct i2s(rRegI dst, rRegI src, immI_16 sixteen)\n+%{\n+  match(Set dst (RShiftI (LShiftI src sixteen) sixteen));\n+\n+  format %{ \"movswl  $dst, $src\\t# i2s\" %}\n+  ins_encode %{\n+    __ movswl($dst$$Register, $src$$Register);\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+\/\/ ROL\/ROR instructions\n+\n+\/\/ Rotate left by constant.\n+instruct rolI_immI8_legacy(rRegI dst, immI8 shift, rFlagsReg cr)\n+%{\n+  predicate(!VM_Version::supports_bmi2() && n->bottom_type()->basic_type() == T_INT);\n+  match(Set dst (RotateLeft dst shift));\n+  effect(KILL cr);\n+  format %{ \"roll    $dst, $shift\" %}\n+  ins_encode %{\n+    __ roll($dst$$Register, $shift$$constant);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct rolI_immI8(rRegI dst, rRegI src, immI8 shift)\n+%{\n+  predicate(!UseAPX && VM_Version::supports_bmi2() && n->bottom_type()->basic_type() == T_INT);\n+  match(Set dst (RotateLeft src shift));\n+  format %{ \"rolxl   $dst, $src, $shift\" %}\n+  ins_encode %{\n+    int shift = 32 - ($shift$$constant & 31);\n+    __ rorxl($dst$$Register, $src$$Register, shift);\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+instruct rolI_mem_immI8(rRegI dst, memory src, immI8 shift)\n+%{\n+  predicate(VM_Version::supports_bmi2() && n->bottom_type()->basic_type() == T_INT);\n+  match(Set dst (RotateLeft (LoadI src) shift));\n+  ins_cost(175);\n+  format %{ \"rolxl   $dst, $src, $shift\" %}\n+  ins_encode %{\n+    int shift = 32 - ($shift$$constant & 31);\n+    __ rorxl($dst$$Register, $src$$Address, shift);\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+\/\/ Rotate Left by variable\n+instruct rolI_rReg_Var(rRegI dst, rcx_RegI shift, rFlagsReg cr)\n+%{\n+  predicate(!UseAPX && n->bottom_type()->basic_type() == T_INT);\n+  match(Set dst (RotateLeft dst shift));\n+  effect(KILL cr);\n+  format %{ \"roll    $dst, $shift\" %}\n+  ins_encode %{\n+    __ roll($dst$$Register);\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+\/\/ Rotate Left by variable\n+instruct rolI_rReg_Var_ndd(rRegI dst, rRegI src, rcx_RegI shift, rFlagsReg cr)\n+%{\n+  predicate(UseAPX && n->bottom_type()->basic_type() == T_INT);\n+  match(Set dst (RotateLeft src shift));\n+  effect(KILL cr);\n+\n+  format %{ \"eroll    $dst, $src, $shift\\t# rotate left (int ndd)\" %}\n+  ins_encode %{\n+    __ eroll($dst$$Register, $src$$Register, false);\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+\/\/ Rotate Right by constant.\n+instruct rorI_immI8_legacy(rRegI dst, immI8 shift, rFlagsReg cr)\n+%{\n+  predicate(!VM_Version::supports_bmi2() && n->bottom_type()->basic_type() == T_INT);\n+  match(Set dst (RotateRight dst shift));\n+  effect(KILL cr);\n+  format %{ \"rorl    $dst, $shift\" %}\n+  ins_encode %{\n+    __ rorl($dst$$Register, $shift$$constant);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+\/\/ Rotate Right by constant.\n+instruct rorI_immI8(rRegI dst, rRegI src, immI8 shift)\n+%{\n+  predicate(!UseAPX && VM_Version::supports_bmi2() && n->bottom_type()->basic_type() == T_INT);\n+  match(Set dst (RotateRight src shift));\n+  format %{ \"rorxl   $dst, $src, $shift\" %}\n+  ins_encode %{\n+    __ rorxl($dst$$Register, $src$$Register, $shift$$constant);\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+instruct rorI_mem_immI8(rRegI dst, memory src, immI8 shift)\n+%{\n+  predicate(VM_Version::supports_bmi2() && n->bottom_type()->basic_type() == T_INT);\n+  match(Set dst (RotateRight (LoadI src) shift));\n+  ins_cost(175);\n+  format %{ \"rorxl   $dst, $src, $shift\" %}\n+  ins_encode %{\n+    __ rorxl($dst$$Register, $src$$Address, $shift$$constant);\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+\/\/ Rotate Right by variable\n+instruct rorI_rReg_Var(rRegI dst, rcx_RegI shift, rFlagsReg cr)\n+%{\n+  predicate(!UseAPX && n->bottom_type()->basic_type() == T_INT);\n+  match(Set dst (RotateRight dst shift));\n+  effect(KILL cr);\n+  format %{ \"rorl    $dst, $shift\" %}\n+  ins_encode %{\n+    __ rorl($dst$$Register);\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+\/\/ Rotate Right by variable\n+instruct rorI_rReg_Var_ndd(rRegI dst, rRegI src, rcx_RegI shift, rFlagsReg cr)\n+%{\n+  predicate(UseAPX && n->bottom_type()->basic_type() == T_INT);\n+  match(Set dst (RotateRight src shift));\n+  effect(KILL cr);\n+\n+  format %{ \"erorl    $dst, $src, $shift\\t# rotate right(int ndd)\" %}\n+  ins_encode %{\n+    __ erorl($dst$$Register, $src$$Register, false);\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+\/\/ Rotate Left by constant.\n+instruct rolL_immI8_legacy(rRegL dst, immI8 shift, rFlagsReg cr)\n+%{\n+  predicate(!VM_Version::supports_bmi2() && n->bottom_type()->basic_type() == T_LONG);\n+  match(Set dst (RotateLeft dst shift));\n+  effect(KILL cr);\n+  format %{ \"rolq    $dst, $shift\" %}\n+  ins_encode %{\n+    __ rolq($dst$$Register, $shift$$constant);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct rolL_immI8(rRegL dst, rRegL src, immI8 shift)\n+%{\n+  predicate(!UseAPX && VM_Version::supports_bmi2() && n->bottom_type()->basic_type() == T_LONG);\n+  match(Set dst (RotateLeft src shift));\n+  format %{ \"rolxq   $dst, $src, $shift\" %}\n+  ins_encode %{\n+    int shift = 64 - ($shift$$constant & 63);\n+    __ rorxq($dst$$Register, $src$$Register, shift);\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+instruct rolL_mem_immI8(rRegL dst, memory src, immI8 shift)\n+%{\n+  predicate(VM_Version::supports_bmi2() && n->bottom_type()->basic_type() == T_LONG);\n+  match(Set dst (RotateLeft (LoadL src) shift));\n+  ins_cost(175);\n+  format %{ \"rolxq   $dst, $src, $shift\" %}\n+  ins_encode %{\n+    int shift = 64 - ($shift$$constant & 63);\n+    __ rorxq($dst$$Register, $src$$Address, shift);\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+\/\/ Rotate Left by variable\n+instruct rolL_rReg_Var(rRegL dst, rcx_RegI shift, rFlagsReg cr)\n+%{\n+  predicate(!UseAPX && n->bottom_type()->basic_type() == T_LONG);\n+  match(Set dst (RotateLeft dst shift));\n+  effect(KILL cr);\n+  format %{ \"rolq    $dst, $shift\" %}\n+  ins_encode %{\n+    __ rolq($dst$$Register);\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+\/\/ Rotate Left by variable\n+instruct rolL_rReg_Var_ndd(rRegL dst, rRegL src, rcx_RegI shift, rFlagsReg cr)\n+%{\n+  predicate(UseAPX && n->bottom_type()->basic_type() == T_LONG);\n+  match(Set dst (RotateLeft src shift));\n+  effect(KILL cr);\n+\n+  format %{ \"erolq    $dst, $src, $shift\\t# rotate left(long ndd)\" %}\n+  ins_encode %{\n+    __ erolq($dst$$Register, $src$$Register, false);\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+\/\/ Rotate Right by constant.\n+instruct rorL_immI8_legacy(rRegL dst, immI8 shift, rFlagsReg cr)\n+%{\n+  predicate(!VM_Version::supports_bmi2() && n->bottom_type()->basic_type() == T_LONG);\n+  match(Set dst (RotateRight dst shift));\n+  effect(KILL cr);\n+  format %{ \"rorq    $dst, $shift\" %}\n+  ins_encode %{\n+    __ rorq($dst$$Register, $shift$$constant);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+\/\/ Rotate Right by constant\n+instruct rorL_immI8(rRegL dst, rRegL src, immI8 shift)\n+%{\n+  predicate(VM_Version::supports_bmi2() && n->bottom_type()->basic_type() == T_LONG);\n+  match(Set dst (RotateRight src shift));\n+  format %{ \"rorxq   $dst, $src, $shift\" %}\n+  ins_encode %{\n+    __ rorxq($dst$$Register, $src$$Register, $shift$$constant);\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+instruct rorL_mem_immI8(rRegL dst, memory src, immI8 shift)\n+%{\n+  predicate(VM_Version::supports_bmi2() && n->bottom_type()->basic_type() == T_LONG);\n+  match(Set dst (RotateRight (LoadL src) shift));\n+  ins_cost(175);\n+  format %{ \"rorxq   $dst, $src, $shift\" %}\n+  ins_encode %{\n+    __ rorxq($dst$$Register, $src$$Address, $shift$$constant);\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+\/\/ Rotate Right by variable\n+instruct rorL_rReg_Var(rRegL dst, rcx_RegI shift, rFlagsReg cr)\n+%{\n+  predicate(!UseAPX && n->bottom_type()->basic_type() == T_LONG);\n+  match(Set dst (RotateRight dst shift));\n+  effect(KILL cr);\n+  format %{ \"rorq    $dst, $shift\" %}\n+  ins_encode %{\n+    __ rorq($dst$$Register);\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+\/\/ Rotate Right by variable\n+instruct rorL_rReg_Var_ndd(rRegL dst, rRegL src, rcx_RegI shift, rFlagsReg cr)\n+%{\n+  predicate(UseAPX && n->bottom_type()->basic_type() == T_LONG);\n+  match(Set dst (RotateRight src shift));\n+  effect(KILL cr);\n+\n+  format %{ \"erorq    $dst, $src, $shift\\t# rotate right(long ndd)\" %}\n+  ins_encode %{\n+    __ erorq($dst$$Register, $src$$Register, false);\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+\/\/----------------------------- CompressBits\/ExpandBits ------------------------\n+\n+instruct compressBitsL_reg(rRegL dst, rRegL src, rRegL mask) %{\n+  predicate(n->bottom_type()->isa_long());\n+  match(Set dst (CompressBits src mask));\n+  format %{ \"pextq  $dst, $src, $mask\\t! parallel bit extract\" %}\n+  ins_encode %{\n+    __ pextq($dst$$Register, $src$$Register, $mask$$Register);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct expandBitsL_reg(rRegL dst, rRegL src, rRegL mask) %{\n+  predicate(n->bottom_type()->isa_long());\n+  match(Set dst (ExpandBits src mask));\n+  format %{ \"pdepq  $dst, $src, $mask\\t! parallel bit deposit\" %}\n+  ins_encode %{\n+    __ pdepq($dst$$Register, $src$$Register, $mask$$Register);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct compressBitsL_mem(rRegL dst, rRegL src, memory mask) %{\n+  predicate(n->bottom_type()->isa_long());\n+  match(Set dst (CompressBits src (LoadL mask)));\n+  format %{ \"pextq  $dst, $src, $mask\\t! parallel bit extract\" %}\n+  ins_encode %{\n+    __ pextq($dst$$Register, $src$$Register, $mask$$Address);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct expandBitsL_mem(rRegL dst, rRegL src, memory mask) %{\n+  predicate(n->bottom_type()->isa_long());\n+  match(Set dst (ExpandBits src (LoadL mask)));\n+  format %{ \"pdepq  $dst, $src, $mask\\t! parallel bit deposit\" %}\n+  ins_encode %{\n+    __ pdepq($dst$$Register, $src$$Register, $mask$$Address);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+\n+\/\/ Logical Instructions\n+\n+\/\/ Integer Logical Instructions\n+\n+\/\/ And Instructions\n+\/\/ And Register with Register\n+instruct andI_rReg(rRegI dst, rRegI src, rFlagsReg cr)\n+%{\n+  predicate(!UseAPX);\n+  match(Set dst (AndI dst src));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n+\n+  format %{ \"andl    $dst, $src\\t# int\" %}\n+  ins_encode %{\n+    __ andl($dst$$Register, $src$$Register);\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+\/\/ And Register with Register using New Data Destination (NDD)\n+instruct andI_rReg_ndd(rRegI dst, rRegI src1, rRegI src2, rFlagsReg cr)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (AndI src1 src2));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n+\n+  format %{ \"eandl     $dst, $src1, $src2\\t# int ndd\" %}\n+  ins_encode %{\n+    __ eandl($dst$$Register, $src1$$Register, $src2$$Register, false);\n+\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+\/\/ And Register with Immediate 255\n+instruct andI_rReg_imm255(rRegI dst, rRegI src, immI_255 mask)\n+%{\n+  match(Set dst (AndI src mask));\n+\n+  format %{ \"movzbl  $dst, $src\\t# int & 0xFF\" %}\n+  ins_encode %{\n+    __ movzbl($dst$$Register, $src$$Register);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+\/\/ And Register with Immediate 255 and promote to long\n+instruct andI2L_rReg_imm255(rRegL dst, rRegI src, immI_255 mask)\n+%{\n+  match(Set dst (ConvI2L (AndI src mask)));\n+\n+  format %{ \"movzbl  $dst, $src\\t# int & 0xFF -> long\" %}\n+  ins_encode %{\n+    __ movzbl($dst$$Register, $src$$Register);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+\/\/ And Register with Immediate 65535\n+instruct andI_rReg_imm65535(rRegI dst, rRegI src, immI_65535 mask)\n+%{\n+  match(Set dst (AndI src mask));\n+\n+  format %{ \"movzwl  $dst, $src\\t# int & 0xFFFF\" %}\n+  ins_encode %{\n+    __ movzwl($dst$$Register, $src$$Register);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+\/\/ And Register with Immediate 65535 and promote to long\n+instruct andI2L_rReg_imm65535(rRegL dst, rRegI src, immI_65535 mask)\n+%{\n+  match(Set dst (ConvI2L (AndI src mask)));\n+\n+  format %{ \"movzwl  $dst, $src\\t# int & 0xFFFF -> long\" %}\n+  ins_encode %{\n+    __ movzwl($dst$$Register, $src$$Register);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+\/\/ Can skip int2long conversions after AND with small bitmask\n+instruct convI2LAndI_reg_immIbitmask(rRegL dst, rRegI src,  immI_Pow2M1 mask, rRegI tmp, rFlagsReg cr)\n+%{\n+  predicate(VM_Version::supports_bmi2());\n+  ins_cost(125);\n+  effect(TEMP tmp, KILL cr);\n+  match(Set dst (ConvI2L (AndI src mask)));\n+  format %{ \"bzhiq $dst, $src, $mask \\t# using $tmp as TEMP, int &  immI_Pow2M1 -> long\" %}\n+  ins_encode %{\n+    __ movl($tmp$$Register, exact_log2($mask$$constant + 1));\n+    __ bzhiq($dst$$Register, $src$$Register, $tmp$$Register);\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+\/\/ And Register with Immediate\n+instruct andI_rReg_imm(rRegI dst, immI src, rFlagsReg cr)\n+%{\n+  predicate(!UseAPX);\n+  match(Set dst (AndI dst src));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n+\n+  format %{ \"andl    $dst, $src\\t# int\" %}\n+  ins_encode %{\n+    __ andl($dst$$Register, $src$$constant);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct andI_rReg_rReg_imm_ndd(rRegI dst, rRegI src1, immI src2, rFlagsReg cr)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (AndI src1 src2));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n+\n+  format %{ \"eandl    $dst, $src1, $src2\\t# int ndd\" %}\n+  ins_encode %{\n+    __ eandl($dst$$Register, $src1$$Register, $src2$$constant, false);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct andI_rReg_mem_imm_ndd(rRegI dst, memory src1, immI src2, rFlagsReg cr)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (AndI (LoadI src1) src2));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n+\n+  format %{ \"eandl    $dst, $src1, $src2\\t# int ndd\" %}\n+  ins_encode %{\n+    __ eandl($dst$$Register, $src1$$Address, $src2$$constant, false);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+\/\/ And Register with Memory\n+instruct andI_rReg_mem(rRegI dst, memory src, rFlagsReg cr)\n+%{\n+  predicate(!UseAPX);\n+  match(Set dst (AndI dst (LoadI src)));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n+\n+  ins_cost(150);\n+  format %{ \"andl    $dst, $src\\t# int\" %}\n+  ins_encode %{\n+    __ andl($dst$$Register, $src$$Address);\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+instruct andI_rReg_rReg_mem_ndd(rRegI dst, rRegI src1, memory src2, rFlagsReg cr)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (AndI src1 (LoadI src2)));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n+\n+  ins_cost(150);\n+  format %{ \"eandl    $dst, $src1, $src2\\t# int ndd\" %}\n+  ins_encode %{\n+    __ eandl($dst$$Register, $src1$$Register, $src2$$Address, false);\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+\/\/ And Memory with Register\n+instruct andB_mem_rReg(memory dst, rRegI src, rFlagsReg cr)\n+%{\n+  match(Set dst (StoreB dst (AndI (LoadB dst) src)));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n+\n+  ins_cost(150);\n+  format %{ \"andb    $dst, $src\\t# byte\" %}\n+  ins_encode %{\n+    __ andb($dst$$Address, $src$$Register);\n+  %}\n+  ins_pipe(ialu_mem_reg);\n+%}\n+\n+instruct andI_mem_rReg(memory dst, rRegI src, rFlagsReg cr)\n+%{\n+  match(Set dst (StoreI dst (AndI (LoadI dst) src)));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n+\n+  ins_cost(150);\n+  format %{ \"andl    $dst, $src\\t# int\" %}\n+  ins_encode %{\n+    __ andl($dst$$Address, $src$$Register);\n+  %}\n+  ins_pipe(ialu_mem_reg);\n+%}\n+\n+\/\/ And Memory with Immediate\n+instruct andI_mem_imm(memory dst, immI src, rFlagsReg cr)\n+%{\n+  match(Set dst (StoreI dst (AndI (LoadI dst) src)));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n+\n+  ins_cost(125);\n+  format %{ \"andl    $dst, $src\\t# int\" %}\n+  ins_encode %{\n+    __ andl($dst$$Address, $src$$constant);\n+  %}\n+  ins_pipe(ialu_mem_imm);\n+%}\n+\n+\/\/ BMI1 instructions\n+instruct andnI_rReg_rReg_mem(rRegI dst, rRegI src1, memory src2, immI_M1 minus_1, rFlagsReg cr) %{\n+  match(Set dst (AndI (XorI src1 minus_1) (LoadI src2)));\n+  predicate(UseBMI1Instructions);\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n+\n+  ins_cost(125);\n+  format %{ \"andnl  $dst, $src1, $src2\" %}\n+\n+  ins_encode %{\n+    __ andnl($dst$$Register, $src1$$Register, $src2$$Address);\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+instruct andnI_rReg_rReg_rReg(rRegI dst, rRegI src1, rRegI src2, immI_M1 minus_1, rFlagsReg cr) %{\n+  match(Set dst (AndI (XorI src1 minus_1) src2));\n+  predicate(UseBMI1Instructions);\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n+\n+  format %{ \"andnl  $dst, $src1, $src2\" %}\n+\n+  ins_encode %{\n+    __ andnl($dst$$Register, $src1$$Register, $src2$$Register);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct blsiI_rReg_rReg(rRegI dst, rRegI src, immI_0 imm_zero, rFlagsReg cr) %{\n+  match(Set dst (AndI (SubI imm_zero src) src));\n+  predicate(UseBMI1Instructions);\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_clears_overflow_flag);\n+\n+  format %{ \"blsil  $dst, $src\" %}\n+\n+  ins_encode %{\n+    __ blsil($dst$$Register, $src$$Register);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct blsiI_rReg_mem(rRegI dst, memory src, immI_0 imm_zero, rFlagsReg cr) %{\n+  match(Set dst (AndI (SubI imm_zero (LoadI src) ) (LoadI src) ));\n+  predicate(UseBMI1Instructions);\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_clears_overflow_flag);\n+\n+  ins_cost(125);\n+  format %{ \"blsil  $dst, $src\" %}\n+\n+  ins_encode %{\n+    __ blsil($dst$$Register, $src$$Address);\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+instruct blsmskI_rReg_mem(rRegI dst, memory src, immI_M1 minus_1, rFlagsReg cr)\n+%{\n+  match(Set dst (XorI (AddI (LoadI src) minus_1) (LoadI src) ) );\n+  predicate(UseBMI1Instructions);\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_clears_zero_flag, PD::Flag_clears_overflow_flag);\n+\n+  ins_cost(125);\n+  format %{ \"blsmskl $dst, $src\" %}\n+\n+  ins_encode %{\n+    __ blsmskl($dst$$Register, $src$$Address);\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+instruct blsmskI_rReg_rReg(rRegI dst, rRegI src, immI_M1 minus_1, rFlagsReg cr)\n+%{\n+  match(Set dst (XorI (AddI src minus_1) src));\n+  predicate(UseBMI1Instructions);\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_clears_zero_flag, PD::Flag_clears_overflow_flag);\n+\n+  format %{ \"blsmskl $dst, $src\" %}\n+\n+  ins_encode %{\n+    __ blsmskl($dst$$Register, $src$$Register);\n+  %}\n+\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct blsrI_rReg_rReg(rRegI dst, rRegI src, immI_M1 minus_1, rFlagsReg cr)\n+%{\n+  match(Set dst (AndI (AddI src minus_1) src) );\n+  predicate(UseBMI1Instructions);\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_clears_overflow_flag);\n+\n+  format %{ \"blsrl  $dst, $src\" %}\n+\n+  ins_encode %{\n+    __ blsrl($dst$$Register, $src$$Register);\n+  %}\n+\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+instruct blsrI_rReg_mem(rRegI dst, memory src, immI_M1 minus_1, rFlagsReg cr)\n+%{\n+  match(Set dst (AndI (AddI (LoadI src) minus_1) (LoadI src) ) );\n+  predicate(UseBMI1Instructions);\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_clears_overflow_flag);\n+\n+  ins_cost(125);\n+  format %{ \"blsrl  $dst, $src\" %}\n+\n+  ins_encode %{\n+    __ blsrl($dst$$Register, $src$$Address);\n+  %}\n+\n+  ins_pipe(ialu_reg);\n+%}\n+\n+\/\/ Or Instructions\n+\/\/ Or Register with Register\n+instruct orI_rReg(rRegI dst, rRegI src, rFlagsReg cr)\n+%{\n+  predicate(!UseAPX);\n+  match(Set dst (OrI dst src));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n+\n+  format %{ \"orl     $dst, $src\\t# int\" %}\n+  ins_encode %{\n+    __ orl($dst$$Register, $src$$Register);\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+\/\/ Or Register with Register using New Data Destination (NDD)\n+instruct orI_rReg_ndd(rRegI dst, rRegI src1, rRegI src2, rFlagsReg cr)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (OrI src1 src2));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n+\n+  format %{ \"eorl     $dst, $src1, $src2\\t# int ndd\" %}\n+  ins_encode %{\n+    __ eorl($dst$$Register, $src1$$Register, $src2$$Register, false);\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+\/\/ Or Register with Immediate\n+instruct orI_rReg_imm(rRegI dst, immI src, rFlagsReg cr)\n+%{\n+  predicate(!UseAPX);\n+  match(Set dst (OrI dst src));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n+\n+  format %{ \"orl     $dst, $src\\t# int\" %}\n+  ins_encode %{\n+    __ orl($dst$$Register, $src$$constant);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct orI_rReg_rReg_imm_ndd(rRegI dst, rRegI src1, immI src2, rFlagsReg cr)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (OrI src1 src2));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n+\n+  format %{ \"eorl     $dst, $src1, $src2\\t# int ndd\" %}\n+  ins_encode %{\n+    __ eorl($dst$$Register, $src1$$Register, $src2$$constant, false);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct orI_rReg_imm_rReg_ndd(rRegI dst, immI src1, rRegI src2, rFlagsReg cr)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (OrI src1 src2));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n+\n+  format %{ \"eorl     $dst, $src2, $src1\\t# int ndd\" %}\n+  ins_encode %{\n+    __ eorl($dst$$Register, $src2$$Register, $src1$$constant, false);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct orI_rReg_mem_imm_ndd(rRegI dst, memory src1, immI src2, rFlagsReg cr)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (OrI (LoadI src1) src2));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n+\n+  format %{ \"eorl     $dst, $src1, $src2\\t# int ndd\" %}\n+  ins_encode %{\n+    __ eorl($dst$$Register, $src1$$Address, $src2$$constant, false);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+\/\/ Or Register with Memory\n+instruct orI_rReg_mem(rRegI dst, memory src, rFlagsReg cr)\n+%{\n+  predicate(!UseAPX);\n+  match(Set dst (OrI dst (LoadI src)));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n+\n+  ins_cost(150);\n+  format %{ \"orl     $dst, $src\\t# int\" %}\n+  ins_encode %{\n+    __ orl($dst$$Register, $src$$Address);\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+instruct orI_rReg_rReg_mem_ndd(rRegI dst, rRegI src1, memory src2, rFlagsReg cr)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (OrI src1 (LoadI src2)));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n+\n+  ins_cost(150);\n+  format %{ \"eorl     $dst, $src1, $src2\\t# int ndd\" %}\n+  ins_encode %{\n+    __ eorl($dst$$Register, $src1$$Register, $src2$$Address, false);\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+\/\/ Or Memory with Register\n+instruct orB_mem_rReg(memory dst, rRegI src, rFlagsReg cr)\n+%{\n+  match(Set dst (StoreB dst (OrI (LoadB dst) src)));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n+\n+  ins_cost(150);\n+  format %{ \"orb    $dst, $src\\t# byte\" %}\n+  ins_encode %{\n+    __ orb($dst$$Address, $src$$Register);\n+  %}\n+  ins_pipe(ialu_mem_reg);\n+%}\n+\n+instruct orI_mem_rReg(memory dst, rRegI src, rFlagsReg cr)\n+%{\n+  match(Set dst (StoreI dst (OrI (LoadI dst) src)));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n+\n+  ins_cost(150);\n+  format %{ \"orl     $dst, $src\\t# int\" %}\n+  ins_encode %{\n+    __ orl($dst$$Address, $src$$Register);\n+  %}\n+  ins_pipe(ialu_mem_reg);\n+%}\n+\n+\/\/ Or Memory with Immediate\n+instruct orI_mem_imm(memory dst, immI src, rFlagsReg cr)\n+%{\n+  match(Set dst (StoreI dst (OrI (LoadI dst) src)));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n+\n+  ins_cost(125);\n+  format %{ \"orl     $dst, $src\\t# int\" %}\n+  ins_encode %{\n+    __ orl($dst$$Address, $src$$constant);\n+  %}\n+  ins_pipe(ialu_mem_imm);\n+%}\n+\n+\/\/ Xor Instructions\n+\/\/ Xor Register with Register\n+instruct xorI_rReg(rRegI dst, rRegI src, rFlagsReg cr)\n+%{\n+  predicate(!UseAPX);\n+  match(Set dst (XorI dst src));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n+\n+  format %{ \"xorl    $dst, $src\\t# int\" %}\n+  ins_encode %{\n+    __ xorl($dst$$Register, $src$$Register);\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+\/\/ Xor Register with Register using New Data Destination (NDD)\n+instruct xorI_rReg_ndd(rRegI dst, rRegI src1, rRegI src2, rFlagsReg cr)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (XorI src1 src2));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n+\n+  format %{ \"exorl    $dst, $src1, $src2\\t# int ndd\" %}\n+  ins_encode %{\n+    __ exorl($dst$$Register, $src1$$Register, $src2$$Register, false);\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+\/\/ Xor Register with Immediate -1\n+instruct xorI_rReg_im1(rRegI dst, immI_M1 imm)\n+%{\n+  predicate(!UseAPX);\n+  match(Set dst (XorI dst imm));\n+\n+  format %{ \"notl    $dst\" %}\n+  ins_encode %{\n+     __ notl($dst$$Register);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct xorI_rReg_im1_ndd(rRegI dst, rRegI src, immI_M1 imm)\n+%{\n+  match(Set dst (XorI src imm));\n+  predicate(UseAPX);\n+\n+  format %{ \"enotl    $dst, $src\" %}\n+  ins_encode %{\n+     __ enotl($dst$$Register, $src$$Register);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+\/\/ Xor Register with Immediate\n+instruct xorI_rReg_imm(rRegI dst, immI src, rFlagsReg cr)\n+%{\n+  \/\/ Strict predicate check to make selection of xorI_rReg_im1 cost agnostic if immI src is -1.\n+  predicate(!UseAPX && n->in(2)->bottom_type()->is_int()->get_con() != -1);\n+  match(Set dst (XorI dst src));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n+\n+  format %{ \"xorl    $dst, $src\\t# int\" %}\n+  ins_encode %{\n+    __ xorl($dst$$Register, $src$$constant);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct xorI_rReg_rReg_imm_ndd(rRegI dst, rRegI src1, immI src2, rFlagsReg cr)\n+%{\n+  \/\/ Strict predicate check to make selection of xorI_rReg_im1_ndd cost agnostic if immI src2 is -1.\n+  predicate(UseAPX && n->in(2)->bottom_type()->is_int()->get_con() != -1);\n+  match(Set dst (XorI src1 src2));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n+\n+  format %{ \"exorl    $dst, $src1, $src2\\t# int ndd\" %}\n+  ins_encode %{\n+    __ exorl($dst$$Register, $src1$$Register, $src2$$constant, false);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+\/\/ Xor Memory with Immediate\n+instruct xorI_rReg_mem_imm_ndd(rRegI dst, memory src1, immI src2, rFlagsReg cr)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (XorI (LoadI src1) src2));\n+  effect(KILL cr);\n+  ins_cost(150);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n+\n+  format %{ \"exorl    $dst, $src1, $src2\\t# int ndd\" %}\n+  ins_encode %{\n+    __ exorl($dst$$Register, $src1$$Address, $src2$$constant, false);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+\/\/ Xor Register with Memory\n+instruct xorI_rReg_mem(rRegI dst, memory src, rFlagsReg cr)\n+%{\n+  predicate(!UseAPX);\n+  match(Set dst (XorI dst (LoadI src)));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n+\n+  ins_cost(150);\n+  format %{ \"xorl    $dst, $src\\t# int\" %}\n+  ins_encode %{\n+    __ xorl($dst$$Register, $src$$Address);\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+instruct xorI_rReg_rReg_mem_ndd(rRegI dst, rRegI src1, memory src2, rFlagsReg cr)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (XorI src1 (LoadI src2)));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n+\n+  ins_cost(150);\n+  format %{ \"exorl    $dst, $src1, $src2\\t# int ndd\" %}\n+  ins_encode %{\n+    __ exorl($dst$$Register, $src1$$Register, $src2$$Address, false);\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+\/\/ Xor Memory with Register\n+instruct xorB_mem_rReg(memory dst, rRegI src, rFlagsReg cr)\n+%{\n+  match(Set dst (StoreB dst (XorI (LoadB dst) src)));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n+\n+  ins_cost(150);\n+  format %{ \"xorb    $dst, $src\\t# byte\" %}\n+  ins_encode %{\n+    __ xorb($dst$$Address, $src$$Register);\n+  %}\n+  ins_pipe(ialu_mem_reg);\n+%}\n+\n+instruct xorI_mem_rReg(memory dst, rRegI src, rFlagsReg cr)\n+%{\n+  match(Set dst (StoreI dst (XorI (LoadI dst) src)));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n+\n+  ins_cost(150);\n+  format %{ \"xorl    $dst, $src\\t# int\" %}\n+  ins_encode %{\n+    __ xorl($dst$$Address, $src$$Register);\n+  %}\n+  ins_pipe(ialu_mem_reg);\n+%}\n+\n+\/\/ Xor Memory with Immediate\n+instruct xorI_mem_imm(memory dst, immI src, rFlagsReg cr)\n+%{\n+  match(Set dst (StoreI dst (XorI (LoadI dst) src)));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n+\n+  ins_cost(125);\n+  format %{ \"xorl    $dst, $src\\t# int\" %}\n+  ins_encode %{\n+    __ xorl($dst$$Address, $src$$constant);\n+  %}\n+  ins_pipe(ialu_mem_imm);\n+%}\n+\n+\n+\/\/ Long Logical Instructions\n+\n+\/\/ And Instructions\n+\/\/ And Register with Register\n+instruct andL_rReg(rRegL dst, rRegL src, rFlagsReg cr)\n+%{\n+  predicate(!UseAPX);\n+  match(Set dst (AndL dst src));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n+\n+  format %{ \"andq    $dst, $src\\t# long\" %}\n+  ins_encode %{\n+    __ andq($dst$$Register, $src$$Register);\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+\/\/ And Register with Register using New Data Destination (NDD)\n+instruct andL_rReg_ndd(rRegL dst, rRegL src1, rRegL src2, rFlagsReg cr)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (AndL src1 src2));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n+\n+  format %{ \"eandq     $dst, $src1, $src2\\t# long ndd\" %}\n+  ins_encode %{\n+    __ eandq($dst$$Register, $src1$$Register, $src2$$Register, false);\n+\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+\/\/ And Register with Immediate 255\n+instruct andL_rReg_imm255(rRegL dst, rRegL src, immL_255 mask)\n+%{\n+  match(Set dst (AndL src mask));\n+\n+  format %{ \"movzbl  $dst, $src\\t# long & 0xFF\" %}\n+  ins_encode %{\n+    \/\/ movzbl zeroes out the upper 32-bit and does not need REX.W\n+    __ movzbl($dst$$Register, $src$$Register);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+\/\/ And Register with Immediate 65535\n+instruct andL_rReg_imm65535(rRegL dst, rRegL src, immL_65535 mask)\n+%{\n+  match(Set dst (AndL src mask));\n+\n+  format %{ \"movzwl  $dst, $src\\t# long & 0xFFFF\" %}\n+  ins_encode %{\n+    \/\/ movzwl zeroes out the upper 32-bit and does not need REX.W\n+    __ movzwl($dst$$Register, $src$$Register);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+\/\/ And Register with Immediate\n+instruct andL_rReg_imm(rRegL dst, immL32 src, rFlagsReg cr)\n+%{\n+  predicate(!UseAPX);\n+  match(Set dst (AndL dst src));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n+\n+  format %{ \"andq    $dst, $src\\t# long\" %}\n+  ins_encode %{\n+    __ andq($dst$$Register, $src$$constant);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct andL_rReg_rReg_imm_ndd(rRegL dst, rRegL src1, immL32 src2, rFlagsReg cr)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (AndL src1 src2));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n+\n+  format %{ \"eandq    $dst, $src1, $src2\\t# long ndd\" %}\n+  ins_encode %{\n+    __ eandq($dst$$Register, $src1$$Register, $src2$$constant, false);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct andL_rReg_mem_imm_ndd(rRegL dst, memory src1, immL32 src2, rFlagsReg cr)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (AndL (LoadL src1) src2));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n+\n+  format %{ \"eandq    $dst, $src1, $src2\\t# long ndd\" %}\n+  ins_encode %{\n+    __ eandq($dst$$Register, $src1$$Address, $src2$$constant, false);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+\/\/ And Register with Memory\n+instruct andL_rReg_mem(rRegL dst, memory src, rFlagsReg cr)\n+%{\n+  predicate(!UseAPX);\n+  match(Set dst (AndL dst (LoadL src)));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n+\n+  ins_cost(150);\n+  format %{ \"andq    $dst, $src\\t# long\" %}\n+  ins_encode %{\n+    __ andq($dst$$Register, $src$$Address);\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+instruct andL_rReg_rReg_mem_ndd(rRegL dst, rRegL src1, memory src2, rFlagsReg cr)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (AndL src1 (LoadL src2)));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n+\n+  ins_cost(150);\n+  format %{ \"eandq    $dst, $src1, $src2\\t# long ndd\" %}\n+  ins_encode %{\n+    __ eandq($dst$$Register, $src1$$Register, $src2$$Address, false);\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+\/\/ And Memory with Register\n+instruct andL_mem_rReg(memory dst, rRegL src, rFlagsReg cr)\n+%{\n+  match(Set dst (StoreL dst (AndL (LoadL dst) src)));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n+\n+  ins_cost(150);\n+  format %{ \"andq    $dst, $src\\t# long\" %}\n+  ins_encode %{\n+    __ andq($dst$$Address, $src$$Register);\n+  %}\n+  ins_pipe(ialu_mem_reg);\n+%}\n+\n+\/\/ And Memory with Immediate\n+instruct andL_mem_imm(memory dst, immL32 src, rFlagsReg cr)\n+%{\n+  match(Set dst (StoreL dst (AndL (LoadL dst) src)));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n+\n+  ins_cost(125);\n+  format %{ \"andq    $dst, $src\\t# long\" %}\n+  ins_encode %{\n+    __ andq($dst$$Address, $src$$constant);\n+  %}\n+  ins_pipe(ialu_mem_imm);\n+%}\n+\n+instruct btrL_mem_imm(memory dst, immL_NotPow2 con, rFlagsReg cr)\n+%{\n+  \/\/ con should be a pure 64-bit immediate given that not(con) is a power of 2\n+  \/\/ because AND\/OR works well enough for 8\/32-bit values.\n+  predicate(log2i_graceful(~n->in(3)->in(2)->get_long()) > 30);\n+\n+  match(Set dst (StoreL dst (AndL (LoadL dst) con)));\n+  effect(KILL cr);\n+\n+  ins_cost(125);\n+  format %{ \"btrq    $dst, log2(not($con))\\t# long\" %}\n+  ins_encode %{\n+    __ btrq($dst$$Address, log2i_exact((julong)~$con$$constant));\n+  %}\n+  ins_pipe(ialu_mem_imm);\n+%}\n+\n+\/\/ BMI1 instructions\n+instruct andnL_rReg_rReg_mem(rRegL dst, rRegL src1, memory src2, immL_M1 minus_1, rFlagsReg cr) %{\n+  match(Set dst (AndL (XorL src1 minus_1) (LoadL src2)));\n+  predicate(UseBMI1Instructions);\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n+\n+  ins_cost(125);\n+  format %{ \"andnq  $dst, $src1, $src2\" %}\n+\n+  ins_encode %{\n+    __ andnq($dst$$Register, $src1$$Register, $src2$$Address);\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+instruct andnL_rReg_rReg_rReg(rRegL dst, rRegL src1, rRegL src2, immL_M1 minus_1, rFlagsReg cr) %{\n+  match(Set dst (AndL (XorL src1 minus_1) src2));\n+  predicate(UseBMI1Instructions);\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n+\n+  format %{ \"andnq  $dst, $src1, $src2\" %}\n+\n+  ins_encode %{\n+  __ andnq($dst$$Register, $src1$$Register, $src2$$Register);\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+instruct blsiL_rReg_rReg(rRegL dst, rRegL src, immL0 imm_zero, rFlagsReg cr) %{\n+  match(Set dst (AndL (SubL imm_zero src) src));\n+  predicate(UseBMI1Instructions);\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_clears_overflow_flag);\n+\n+  format %{ \"blsiq  $dst, $src\" %}\n+\n+  ins_encode %{\n+    __ blsiq($dst$$Register, $src$$Register);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct blsiL_rReg_mem(rRegL dst, memory src, immL0 imm_zero, rFlagsReg cr) %{\n+  match(Set dst (AndL (SubL imm_zero (LoadL src) ) (LoadL src) ));\n+  predicate(UseBMI1Instructions);\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_clears_overflow_flag);\n+\n+  ins_cost(125);\n+  format %{ \"blsiq  $dst, $src\" %}\n+\n+  ins_encode %{\n+    __ blsiq($dst$$Register, $src$$Address);\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+instruct blsmskL_rReg_mem(rRegL dst, memory src, immL_M1 minus_1, rFlagsReg cr)\n+%{\n+  match(Set dst (XorL (AddL (LoadL src) minus_1) (LoadL src) ) );\n+  predicate(UseBMI1Instructions);\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_clears_zero_flag, PD::Flag_clears_overflow_flag);\n+\n+  ins_cost(125);\n+  format %{ \"blsmskq $dst, $src\" %}\n+\n+  ins_encode %{\n+    __ blsmskq($dst$$Register, $src$$Address);\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+instruct blsmskL_rReg_rReg(rRegL dst, rRegL src, immL_M1 minus_1, rFlagsReg cr)\n+%{\n+  match(Set dst (XorL (AddL src minus_1) src));\n+  predicate(UseBMI1Instructions);\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_clears_zero_flag, PD::Flag_clears_overflow_flag);\n+\n+  format %{ \"blsmskq $dst, $src\" %}\n+\n+  ins_encode %{\n+    __ blsmskq($dst$$Register, $src$$Register);\n+  %}\n+\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct blsrL_rReg_rReg(rRegL dst, rRegL src, immL_M1 minus_1, rFlagsReg cr)\n+%{\n+  match(Set dst (AndL (AddL src minus_1) src) );\n+  predicate(UseBMI1Instructions);\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_clears_overflow_flag);\n+\n+  format %{ \"blsrq  $dst, $src\" %}\n+\n+  ins_encode %{\n+    __ blsrq($dst$$Register, $src$$Register);\n+  %}\n+\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct blsrL_rReg_mem(rRegL dst, memory src, immL_M1 minus_1, rFlagsReg cr)\n+%{\n+  match(Set dst (AndL (AddL (LoadL src) minus_1) (LoadL src)) );\n+  predicate(UseBMI1Instructions);\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_clears_overflow_flag);\n+\n+  ins_cost(125);\n+  format %{ \"blsrq  $dst, $src\" %}\n+\n+  ins_encode %{\n+    __ blsrq($dst$$Register, $src$$Address);\n+  %}\n+\n+  ins_pipe(ialu_reg);\n+%}\n+\n+\/\/ Or Instructions\n+\/\/ Or Register with Register\n+instruct orL_rReg(rRegL dst, rRegL src, rFlagsReg cr)\n+%{\n+  predicate(!UseAPX);\n+  match(Set dst (OrL dst src));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n+\n+  format %{ \"orq     $dst, $src\\t# long\" %}\n+  ins_encode %{\n+    __ orq($dst$$Register, $src$$Register);\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+\/\/ Or Register with Register using New Data Destination (NDD)\n+instruct orL_rReg_ndd(rRegL dst, rRegL src1, rRegL src2, rFlagsReg cr)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (OrL src1 src2));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n+\n+  format %{ \"eorq     $dst, $src1, $src2\\t# long ndd\" %}\n+  ins_encode %{\n+    __ eorq($dst$$Register, $src1$$Register, $src2$$Register, false);\n+\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+\/\/ Use any_RegP to match R15 (TLS register) without spilling.\n+instruct orL_rReg_castP2X(rRegL dst, any_RegP src, rFlagsReg cr) %{\n+  match(Set dst (OrL dst (CastP2X src)));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n+\n+  format %{ \"orq     $dst, $src\\t# long\" %}\n+  ins_encode %{\n+    __ orq($dst$$Register, $src$$Register);\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+instruct orL_rReg_castP2X_ndd(rRegL dst, any_RegP src1, any_RegP src2, rFlagsReg cr) %{\n+  match(Set dst (OrL src1 (CastP2X src2)));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n+\n+  format %{ \"eorq     $dst, $src1, $src2\\t# long ndd\" %}\n+  ins_encode %{\n+    __ eorq($dst$$Register, $src1$$Register, $src2$$Register, false);\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+\/\/ Or Register with Immediate\n+instruct orL_rReg_imm(rRegL dst, immL32 src, rFlagsReg cr)\n+%{\n+  predicate(!UseAPX);\n+  match(Set dst (OrL dst src));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n+\n+  format %{ \"orq     $dst, $src\\t# long\" %}\n+  ins_encode %{\n+    __ orq($dst$$Register, $src$$constant);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct orL_rReg_rReg_imm_ndd(rRegL dst, rRegL src1, immL32 src2, rFlagsReg cr)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (OrL src1 src2));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n+\n+  format %{ \"eorq     $dst, $src1, $src2\\t# long ndd\" %}\n+  ins_encode %{\n+    __ eorq($dst$$Register, $src1$$Register, $src2$$constant, false);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct orL_rReg_imm_rReg_ndd(rRegL dst, immL32 src1, rRegL src2, rFlagsReg cr)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (OrL src1 src2));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n+\n+  format %{ \"eorq     $dst, $src2, $src1\\t# long ndd\" %}\n+  ins_encode %{\n+    __ eorq($dst$$Register, $src2$$Register, $src1$$constant, false);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+\/\/ Or Memory with Immediate\n+instruct orL_rReg_mem_imm_ndd(rRegL dst, memory src1, immL32 src2, rFlagsReg cr)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (OrL (LoadL src1) src2));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n+\n+  format %{ \"eorq     $dst, $src1, $src2\\t# long ndd\" %}\n+  ins_encode %{\n+    __ eorq($dst$$Register, $src1$$Address, $src2$$constant, false);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+\/\/ Or Register with Memory\n+instruct orL_rReg_mem(rRegL dst, memory src, rFlagsReg cr)\n+%{\n+  predicate(!UseAPX);\n+  match(Set dst (OrL dst (LoadL src)));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n+\n+  ins_cost(150);\n+  format %{ \"orq     $dst, $src\\t# long\" %}\n+  ins_encode %{\n+    __ orq($dst$$Register, $src$$Address);\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+instruct orL_rReg_rReg_mem_ndd(rRegL dst, rRegL src1, memory src2, rFlagsReg cr)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (OrL src1 (LoadL src2)));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n+\n+  ins_cost(150);\n+  format %{ \"eorq     $dst, $src1, $src2\\t# long ndd\" %}\n+  ins_encode %{\n+    __ eorq($dst$$Register, $src1$$Register, $src2$$Address, false);\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+\/\/ Or Memory with Register\n+instruct orL_mem_rReg(memory dst, rRegL src, rFlagsReg cr)\n+%{\n+  match(Set dst (StoreL dst (OrL (LoadL dst) src)));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n+\n+  ins_cost(150);\n+  format %{ \"orq     $dst, $src\\t# long\" %}\n+  ins_encode %{\n+    __ orq($dst$$Address, $src$$Register);\n+  %}\n+  ins_pipe(ialu_mem_reg);\n+%}\n+\n+\/\/ Or Memory with Immediate\n+instruct orL_mem_imm(memory dst, immL32 src, rFlagsReg cr)\n+%{\n+  match(Set dst (StoreL dst (OrL (LoadL dst) src)));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n+\n+  ins_cost(125);\n+  format %{ \"orq     $dst, $src\\t# long\" %}\n+  ins_encode %{\n+    __ orq($dst$$Address, $src$$constant);\n+  %}\n+  ins_pipe(ialu_mem_imm);\n+%}\n+\n+instruct btsL_mem_imm(memory dst, immL_Pow2 con, rFlagsReg cr)\n+%{\n+  \/\/ con should be a pure 64-bit power of 2 immediate\n+  \/\/ because AND\/OR works well enough for 8\/32-bit values.\n+  predicate(log2i_graceful(n->in(3)->in(2)->get_long()) > 31);\n+\n+  match(Set dst (StoreL dst (OrL (LoadL dst) con)));\n+  effect(KILL cr);\n+\n+  ins_cost(125);\n+  format %{ \"btsq    $dst, log2($con)\\t# long\" %}\n+  ins_encode %{\n+    __ btsq($dst$$Address, log2i_exact((julong)$con$$constant));\n+  %}\n+  ins_pipe(ialu_mem_imm);\n+%}\n+\n+\/\/ Xor Instructions\n+\/\/ Xor Register with Register\n+instruct xorL_rReg(rRegL dst, rRegL src, rFlagsReg cr)\n+%{\n+  predicate(!UseAPX);\n+  match(Set dst (XorL dst src));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n+\n+  format %{ \"xorq    $dst, $src\\t# long\" %}\n+  ins_encode %{\n+    __ xorq($dst$$Register, $src$$Register);\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+\/\/ Xor Register with Register using New Data Destination (NDD)\n+instruct xorL_rReg_ndd(rRegL dst, rRegL src1, rRegL src2, rFlagsReg cr)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (XorL src1 src2));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n+\n+  format %{ \"exorq    $dst, $src1, $src2\\t# long ndd\" %}\n+  ins_encode %{\n+    __ exorq($dst$$Register, $src1$$Register, $src2$$Register, false);\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+\/\/ Xor Register with Immediate -1\n+instruct xorL_rReg_im1(rRegL dst, immL_M1 imm)\n+%{\n+  predicate(!UseAPX);\n+  match(Set dst (XorL dst imm));\n+\n+  format %{ \"notq   $dst\" %}\n+  ins_encode %{\n+     __ notq($dst$$Register);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct xorL_rReg_im1_ndd(rRegL dst,rRegL src, immL_M1 imm)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (XorL src imm));\n+\n+  format %{ \"enotq   $dst, $src\" %}\n+  ins_encode %{\n+    __ enotq($dst$$Register, $src$$Register);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+\/\/ Xor Register with Immediate\n+instruct xorL_rReg_imm(rRegL dst, immL32 src, rFlagsReg cr)\n+%{\n+  \/\/ Strict predicate check to make selection of xorL_rReg_im1 cost agnostic if immL32 src is -1.\n+  predicate(!UseAPX && n->in(2)->bottom_type()->is_long()->get_con() != -1L);\n+  match(Set dst (XorL dst src));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n+\n+  format %{ \"xorq    $dst, $src\\t# long\" %}\n+  ins_encode %{\n+    __ xorq($dst$$Register, $src$$constant);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct xorL_rReg_rReg_imm(rRegL dst, rRegL src1, immL32 src2, rFlagsReg cr)\n+%{\n+  \/\/ Strict predicate check to make selection of xorL_rReg_im1_ndd cost agnostic if immL32 src2 is -1.\n+  predicate(UseAPX && n->in(2)->bottom_type()->is_long()->get_con() != -1L);\n+  match(Set dst (XorL src1 src2));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n+\n+  format %{ \"exorq    $dst, $src1, $src2\\t# long ndd\" %}\n+  ins_encode %{\n+    __ exorq($dst$$Register, $src1$$Register, $src2$$constant, false);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+\/\/ Xor Memory with Immediate\n+instruct xorL_rReg_mem_imm(rRegL dst, memory src1, immL32 src2, rFlagsReg cr)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (XorL (LoadL src1) src2));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n+  ins_cost(150);\n+\n+  format %{ \"exorq    $dst, $src1, $src2\\t# long ndd\" %}\n+  ins_encode %{\n+    __ exorq($dst$$Register, $src1$$Address, $src2$$constant, false);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+\/\/ Xor Register with Memory\n+instruct xorL_rReg_mem(rRegL dst, memory src, rFlagsReg cr)\n+%{\n+  predicate(!UseAPX);\n+  match(Set dst (XorL dst (LoadL src)));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n+\n+  ins_cost(150);\n+  format %{ \"xorq    $dst, $src\\t# long\" %}\n+  ins_encode %{\n+    __ xorq($dst$$Register, $src$$Address);\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+instruct xorL_rReg_rReg_mem_ndd(rRegL dst, rRegL src1, memory src2, rFlagsReg cr)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (XorL src1 (LoadL src2)));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n+\n+  ins_cost(150);\n+  format %{ \"exorq    $dst, $src1, $src2\\t# long ndd\" %}\n+  ins_encode %{\n+    __ exorq($dst$$Register, $src1$$Register, $src2$$Address, false);\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+\/\/ Xor Memory with Register\n+instruct xorL_mem_rReg(memory dst, rRegL src, rFlagsReg cr)\n+%{\n+  match(Set dst (StoreL dst (XorL (LoadL dst) src)));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n+\n+  ins_cost(150);\n+  format %{ \"xorq    $dst, $src\\t# long\" %}\n+  ins_encode %{\n+    __ xorq($dst$$Address, $src$$Register);\n+  %}\n+  ins_pipe(ialu_mem_reg);\n+%}\n+\n+\/\/ Xor Memory with Immediate\n+instruct xorL_mem_imm(memory dst, immL32 src, rFlagsReg cr)\n+%{\n+  match(Set dst (StoreL dst (XorL (LoadL dst) src)));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n+\n+  ins_cost(125);\n+  format %{ \"xorq    $dst, $src\\t# long\" %}\n+  ins_encode %{\n+    __ xorq($dst$$Address, $src$$constant);\n+  %}\n+  ins_pipe(ialu_mem_imm);\n+%}\n+\n+instruct cmpLTMask(rRegI dst, rRegI p, rRegI q, rFlagsReg cr)\n+%{\n+  match(Set dst (CmpLTMask p q));\n+  effect(KILL cr);\n+\n+  ins_cost(400);\n+  format %{ \"cmpl    $p, $q\\t# cmpLTMask\\n\\t\"\n+            \"setcc   $dst \\t# emits setlt + movzbl or setzul for APX\"\n+            \"negl    $dst\" %}\n+  ins_encode %{\n+    __ cmpl($p$$Register, $q$$Register);\n+    __ setcc(Assembler::less, $dst$$Register);\n+    __ negl($dst$$Register);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct cmpLTMask0(rRegI dst, immI_0 zero, rFlagsReg cr)\n+%{\n+  match(Set dst (CmpLTMask dst zero));\n+  effect(KILL cr);\n+\n+  ins_cost(100);\n+  format %{ \"sarl    $dst, #31\\t# cmpLTMask0\" %}\n+  ins_encode %{\n+    __ sarl($dst$$Register, 31);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+\/* Better to save a register than avoid a branch *\/\n+instruct cadd_cmpLTMask(rRegI p, rRegI q, rRegI y, rFlagsReg cr)\n+%{\n+  match(Set p (AddI (AndI (CmpLTMask p q) y) (SubI p q)));\n+  effect(KILL cr);\n+  ins_cost(300);\n+  format %{ \"subl    $p,$q\\t# cadd_cmpLTMask\\n\\t\"\n+            \"jge     done\\n\\t\"\n+            \"addl    $p,$y\\n\"\n+            \"done:   \" %}\n+  ins_encode %{\n+    Register Rp = $p$$Register;\n+    Register Rq = $q$$Register;\n+    Register Ry = $y$$Register;\n+    Label done;\n+    __ subl(Rp, Rq);\n+    __ jccb(Assembler::greaterEqual, done);\n+    __ addl(Rp, Ry);\n+    __ bind(done);\n+  %}\n+  ins_pipe(pipe_cmplt);\n+%}\n+\n+\/* Better to save a register than avoid a branch *\/\n+instruct and_cmpLTMask(rRegI p, rRegI q, rRegI y, rFlagsReg cr)\n+%{\n+  match(Set y (AndI (CmpLTMask p q) y));\n+  effect(KILL cr);\n+\n+  ins_cost(300);\n+\n+  format %{ \"cmpl    $p, $q\\t# and_cmpLTMask\\n\\t\"\n+            \"jlt     done\\n\\t\"\n+            \"xorl    $y, $y\\n\"\n+            \"done:   \" %}\n+  ins_encode %{\n+    Register Rp = $p$$Register;\n+    Register Rq = $q$$Register;\n+    Register Ry = $y$$Register;\n+    Label done;\n+    __ cmpl(Rp, Rq);\n+    __ jccb(Assembler::less, done);\n+    __ xorl(Ry, Ry);\n+    __ bind(done);\n+  %}\n+  ins_pipe(pipe_cmplt);\n+%}\n+\n+\n+\/\/---------- FP Instructions------------------------------------------------\n+\n+\/\/ Really expensive, avoid\n+instruct cmpF_cc_reg(rFlagsRegU cr, regF src1, regF src2)\n+%{\n+  match(Set cr (CmpF src1 src2));\n+\n+  ins_cost(500);\n+  format %{ \"ucomiss $src1, $src2\\n\\t\"\n+            \"jnp,s   exit\\n\\t\"\n+            \"pushfq\\t# saw NaN, set CF\\n\\t\"\n+            \"andq    [rsp], #0xffffff2b\\n\\t\"\n+            \"popfq\\n\"\n+    \"exit:\" %}\n+  ins_encode %{\n+    __ ucomiss($src1$$XMMRegister, $src2$$XMMRegister);\n+    emit_cmpfp_fixup(masm);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct cmpF_cc_reg_CF(rFlagsRegUCF cr, regF src1, regF src2) %{\n+  match(Set cr (CmpF src1 src2));\n+\n+  ins_cost(100);\n+  format %{ \"ucomiss $src1, $src2\" %}\n+  ins_encode %{\n+    __ ucomiss($src1$$XMMRegister, $src2$$XMMRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct cmpF_cc_memCF(rFlagsRegUCF cr, regF src1, memory src2) %{\n+  match(Set cr (CmpF src1 (LoadF src2)));\n+\n+  ins_cost(100);\n+  format %{ \"ucomiss $src1, $src2\" %}\n+  ins_encode %{\n+    __ ucomiss($src1$$XMMRegister, $src2$$Address);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct cmpF_cc_immCF(rFlagsRegUCF cr, regF src, immF con) %{\n+  match(Set cr (CmpF src con));\n+  ins_cost(100);\n+  format %{ \"ucomiss $src, [$constantaddress]\\t# load from constant table: float=$con\" %}\n+  ins_encode %{\n+    __ ucomiss($src$$XMMRegister, $constantaddress($con));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ Really expensive, avoid\n+instruct cmpD_cc_reg(rFlagsRegU cr, regD src1, regD src2)\n+%{\n+  match(Set cr (CmpD src1 src2));\n+\n+  ins_cost(500);\n+  format %{ \"ucomisd $src1, $src2\\n\\t\"\n+            \"jnp,s   exit\\n\\t\"\n+            \"pushfq\\t# saw NaN, set CF\\n\\t\"\n+            \"andq    [rsp], #0xffffff2b\\n\\t\"\n+            \"popfq\\n\"\n+    \"exit:\" %}\n+  ins_encode %{\n+    __ ucomisd($src1$$XMMRegister, $src2$$XMMRegister);\n+    emit_cmpfp_fixup(masm);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct cmpD_cc_reg_CF(rFlagsRegUCF cr, regD src1, regD src2) %{\n+  match(Set cr (CmpD src1 src2));\n+\n+  ins_cost(100);\n+  format %{ \"ucomisd $src1, $src2 test\" %}\n+  ins_encode %{\n+    __ ucomisd($src1$$XMMRegister, $src2$$XMMRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct cmpD_cc_memCF(rFlagsRegUCF cr, regD src1, memory src2) %{\n+  match(Set cr (CmpD src1 (LoadD src2)));\n+\n+  ins_cost(100);\n+  format %{ \"ucomisd $src1, $src2\" %}\n+  ins_encode %{\n+    __ ucomisd($src1$$XMMRegister, $src2$$Address);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct cmpD_cc_immCF(rFlagsRegUCF cr, regD src, immD con) %{\n+  match(Set cr (CmpD src con));\n+  ins_cost(100);\n+  format %{ \"ucomisd $src, [$constantaddress]\\t# load from constant table: double=$con\" %}\n+  ins_encode %{\n+    __ ucomisd($src$$XMMRegister, $constantaddress($con));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ Compare into -1,0,1\n+instruct cmpF_reg(rRegI dst, regF src1, regF src2, rFlagsReg cr)\n+%{\n+  match(Set dst (CmpF3 src1 src2));\n+  effect(KILL cr);\n+\n+  ins_cost(275);\n+  format %{ \"ucomiss $src1, $src2\\n\\t\"\n+            \"movl    $dst, #-1\\n\\t\"\n+            \"jp,s    done\\n\\t\"\n+            \"jb,s    done\\n\\t\"\n+            \"setne   $dst\\n\\t\"\n+            \"movzbl  $dst, $dst\\n\"\n+    \"done:\" %}\n+  ins_encode %{\n+    __ ucomiss($src1$$XMMRegister, $src2$$XMMRegister);\n+    emit_cmpfp3(masm, $dst$$Register);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ Compare into -1,0,1\n+instruct cmpF_mem(rRegI dst, regF src1, memory src2, rFlagsReg cr)\n+%{\n+  match(Set dst (CmpF3 src1 (LoadF src2)));\n+  effect(KILL cr);\n+\n+  ins_cost(275);\n+  format %{ \"ucomiss $src1, $src2\\n\\t\"\n+            \"movl    $dst, #-1\\n\\t\"\n+            \"jp,s    done\\n\\t\"\n+            \"jb,s    done\\n\\t\"\n+            \"setne   $dst\\n\\t\"\n+            \"movzbl  $dst, $dst\\n\"\n+    \"done:\" %}\n+  ins_encode %{\n+    __ ucomiss($src1$$XMMRegister, $src2$$Address);\n+    emit_cmpfp3(masm, $dst$$Register);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ Compare into -1,0,1\n+instruct cmpF_imm(rRegI dst, regF src, immF con, rFlagsReg cr) %{\n+  match(Set dst (CmpF3 src con));\n+  effect(KILL cr);\n+\n+  ins_cost(275);\n+  format %{ \"ucomiss $src, [$constantaddress]\\t# load from constant table: float=$con\\n\\t\"\n+            \"movl    $dst, #-1\\n\\t\"\n+            \"jp,s    done\\n\\t\"\n+            \"jb,s    done\\n\\t\"\n+            \"setne   $dst\\n\\t\"\n+            \"movzbl  $dst, $dst\\n\"\n+    \"done:\" %}\n+  ins_encode %{\n+    __ ucomiss($src$$XMMRegister, $constantaddress($con));\n+    emit_cmpfp3(masm, $dst$$Register);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ Compare into -1,0,1\n+instruct cmpD_reg(rRegI dst, regD src1, regD src2, rFlagsReg cr)\n+%{\n+  match(Set dst (CmpD3 src1 src2));\n+  effect(KILL cr);\n+\n+  ins_cost(275);\n+  format %{ \"ucomisd $src1, $src2\\n\\t\"\n+            \"movl    $dst, #-1\\n\\t\"\n+            \"jp,s    done\\n\\t\"\n+            \"jb,s    done\\n\\t\"\n+            \"setne   $dst\\n\\t\"\n+            \"movzbl  $dst, $dst\\n\"\n+    \"done:\" %}\n+  ins_encode %{\n+    __ ucomisd($src1$$XMMRegister, $src2$$XMMRegister);\n+    emit_cmpfp3(masm, $dst$$Register);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ Compare into -1,0,1\n+instruct cmpD_mem(rRegI dst, regD src1, memory src2, rFlagsReg cr)\n+%{\n+  match(Set dst (CmpD3 src1 (LoadD src2)));\n+  effect(KILL cr);\n+\n+  ins_cost(275);\n+  format %{ \"ucomisd $src1, $src2\\n\\t\"\n+            \"movl    $dst, #-1\\n\\t\"\n+            \"jp,s    done\\n\\t\"\n+            \"jb,s    done\\n\\t\"\n+            \"setne   $dst\\n\\t\"\n+            \"movzbl  $dst, $dst\\n\"\n+    \"done:\" %}\n+  ins_encode %{\n+    __ ucomisd($src1$$XMMRegister, $src2$$Address);\n+    emit_cmpfp3(masm, $dst$$Register);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ Compare into -1,0,1\n+instruct cmpD_imm(rRegI dst, regD src, immD con, rFlagsReg cr) %{\n+  match(Set dst (CmpD3 src con));\n+  effect(KILL cr);\n+\n+  ins_cost(275);\n+  format %{ \"ucomisd $src, [$constantaddress]\\t# load from constant table: double=$con\\n\\t\"\n+            \"movl    $dst, #-1\\n\\t\"\n+            \"jp,s    done\\n\\t\"\n+            \"jb,s    done\\n\\t\"\n+            \"setne   $dst\\n\\t\"\n+            \"movzbl  $dst, $dst\\n\"\n+    \"done:\" %}\n+  ins_encode %{\n+    __ ucomisd($src$$XMMRegister, $constantaddress($con));\n+    emit_cmpfp3(masm, $dst$$Register);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/----------Arithmetic Conversion Instructions---------------------------------\n+\n+instruct convF2D_reg_reg(regD dst, regF src)\n+%{\n+  match(Set dst (ConvF2D src));\n+\n+  format %{ \"cvtss2sd $dst, $src\" %}\n+  ins_encode %{\n+    __ cvtss2sd ($dst$$XMMRegister, $src$$XMMRegister);\n+  %}\n+  ins_pipe(pipe_slow); \/\/ XXX\n+%}\n+\n+instruct convF2D_reg_mem(regD dst, memory src)\n+%{\n+  predicate(UseAVX == 0);\n+  match(Set dst (ConvF2D (LoadF src)));\n+\n+  format %{ \"cvtss2sd $dst, $src\" %}\n+  ins_encode %{\n+    __ cvtss2sd ($dst$$XMMRegister, $src$$Address);\n+  %}\n+  ins_pipe(pipe_slow); \/\/ XXX\n+%}\n+\n+instruct convD2F_reg_reg(regF dst, regD src)\n+%{\n+  match(Set dst (ConvD2F src));\n+\n+  format %{ \"cvtsd2ss $dst, $src\" %}\n+  ins_encode %{\n+    __ cvtsd2ss ($dst$$XMMRegister, $src$$XMMRegister);\n+  %}\n+  ins_pipe(pipe_slow); \/\/ XXX\n+%}\n+\n+instruct convD2F_reg_mem(regF dst, memory src)\n+%{\n+  predicate(UseAVX == 0);\n+  match(Set dst (ConvD2F (LoadD src)));\n+\n+  format %{ \"cvtsd2ss $dst, $src\" %}\n+  ins_encode %{\n+    __ cvtsd2ss ($dst$$XMMRegister, $src$$Address);\n+  %}\n+  ins_pipe(pipe_slow); \/\/ XXX\n+%}\n+\n+\/\/ XXX do mem variants\n+instruct convF2I_reg_reg(rRegI dst, regF src, rFlagsReg cr)\n+%{\n+  predicate(!VM_Version::supports_avx10_2());\n+  match(Set dst (ConvF2I src));\n+  effect(KILL cr);\n+  format %{ \"convert_f2i $dst, $src\" %}\n+  ins_encode %{\n+    __ convertF2I(T_INT, T_FLOAT, $dst$$Register, $src$$XMMRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct convF2I_reg_reg_avx10(rRegI dst, regF src)\n+%{\n+  predicate(VM_Version::supports_avx10_2());\n+  match(Set dst (ConvF2I src));\n+  format %{ \"evcvttss2sisl $dst, $src\" %}\n+  ins_encode %{\n+    __ evcvttss2sisl($dst$$Register, $src$$XMMRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct convF2I_reg_mem_avx10(rRegI dst, memory src)\n+%{\n+  predicate(VM_Version::supports_avx10_2());\n+  match(Set dst (ConvF2I (LoadF src)));\n+  format %{ \"evcvttss2sisl $dst, $src\" %}\n+  ins_encode %{\n+    __ evcvttss2sisl($dst$$Register, $src$$Address);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct convF2L_reg_reg(rRegL dst, regF src, rFlagsReg cr)\n+%{\n+  predicate(!VM_Version::supports_avx10_2());\n+  match(Set dst (ConvF2L src));\n+  effect(KILL cr);\n+  format %{ \"convert_f2l $dst, $src\"%}\n+  ins_encode %{\n+    __ convertF2I(T_LONG, T_FLOAT, $dst$$Register, $src$$XMMRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct convF2L_reg_reg_avx10(rRegL dst, regF src)\n+%{\n+  predicate(VM_Version::supports_avx10_2());\n+  match(Set dst (ConvF2L src));\n+  format %{ \"evcvttss2sisq $dst, $src\" %}\n+  ins_encode %{\n+    __ evcvttss2sisq($dst$$Register, $src$$XMMRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct convF2L_reg_mem_avx10(rRegL dst, memory src)\n+%{\n+  predicate(VM_Version::supports_avx10_2());\n+  match(Set dst (ConvF2L (LoadF src)));\n+  format %{ \"evcvttss2sisq $dst, $src\" %}\n+  ins_encode %{\n+    __ evcvttss2sisq($dst$$Register, $src$$Address);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct convD2I_reg_reg(rRegI dst, regD src, rFlagsReg cr)\n+%{\n+  predicate(!VM_Version::supports_avx10_2());\n+  match(Set dst (ConvD2I src));\n+  effect(KILL cr);\n+  format %{ \"convert_d2i $dst, $src\"%}\n+  ins_encode %{\n+    __ convertF2I(T_INT, T_DOUBLE, $dst$$Register, $src$$XMMRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct convD2I_reg_reg_avx10(rRegI dst, regD src)\n+%{\n+  predicate(VM_Version::supports_avx10_2());\n+  match(Set dst (ConvD2I src));\n+  format %{ \"evcvttsd2sisl $dst, $src\" %}\n+  ins_encode %{\n+    __ evcvttsd2sisl($dst$$Register, $src$$XMMRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct convD2I_reg_mem_avx10(rRegI dst, memory src)\n+%{\n+  predicate(VM_Version::supports_avx10_2());\n+  match(Set dst (ConvD2I (LoadD src)));\n+  format %{ \"evcvttsd2sisl $dst, $src\" %}\n+  ins_encode %{\n+    __ evcvttsd2sisl($dst$$Register, $src$$Address);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct convD2L_reg_reg(rRegL dst, regD src, rFlagsReg cr)\n+%{\n+  predicate(!VM_Version::supports_avx10_2());\n+  match(Set dst (ConvD2L src));\n+  effect(KILL cr);\n+  format %{ \"convert_d2l $dst, $src\"%}\n+  ins_encode %{\n+    __ convertF2I(T_LONG, T_DOUBLE, $dst$$Register, $src$$XMMRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct convD2L_reg_reg_avx10(rRegL dst, regD src)\n+%{\n+  predicate(VM_Version::supports_avx10_2());\n+  match(Set dst (ConvD2L src));\n+  format %{ \"evcvttsd2sisq $dst, $src\" %}\n+  ins_encode %{\n+    __ evcvttsd2sisq($dst$$Register, $src$$XMMRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct convD2L_reg_mem_avx10(rRegL dst, memory src)\n+%{\n+  predicate(VM_Version::supports_avx10_2());\n+  match(Set dst (ConvD2L (LoadD src)));\n+  format %{ \"evcvttsd2sisq $dst, $src\" %}\n+  ins_encode %{\n+    __ evcvttsd2sisq($dst$$Register, $src$$Address);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct round_double_reg(rRegL dst, regD src, rRegL rtmp, rcx_RegL rcx, rFlagsReg cr)\n+%{\n+  match(Set dst (RoundD src));\n+  effect(TEMP dst, TEMP rtmp, TEMP rcx, KILL cr);\n+  format %{ \"round_double $dst,$src \\t! using $rtmp and $rcx as TEMP\"%}\n+  ins_encode %{\n+    __ round_double($dst$$Register, $src$$XMMRegister, $rtmp$$Register, $rcx$$Register);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct round_float_reg(rRegI dst, regF src, rRegL rtmp, rcx_RegL rcx, rFlagsReg cr)\n+%{\n+  match(Set dst (RoundF src));\n+  effect(TEMP dst, TEMP rtmp, TEMP rcx, KILL cr);\n+  format %{ \"round_float $dst,$src\" %}\n+  ins_encode %{\n+    __ round_float($dst$$Register, $src$$XMMRegister, $rtmp$$Register, $rcx$$Register);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct convI2F_reg_reg(vlRegF dst, rRegI src)\n+%{\n+  predicate(!UseXmmI2F);\n+  match(Set dst (ConvI2F src));\n+\n+  format %{ \"cvtsi2ssl $dst, $src\\t# i2f\" %}\n+  ins_encode %{\n+    if (UseAVX > 0) {\n+      __ pxor($dst$$XMMRegister, $dst$$XMMRegister);\n+    }\n+    __ cvtsi2ssl ($dst$$XMMRegister, $src$$Register);\n+  %}\n+  ins_pipe(pipe_slow); \/\/ XXX\n+%}\n+\n+instruct convI2F_reg_mem(regF dst, memory src)\n+%{\n+  predicate(UseAVX == 0);\n+  match(Set dst (ConvI2F (LoadI src)));\n+\n+  format %{ \"cvtsi2ssl $dst, $src\\t# i2f\" %}\n+  ins_encode %{\n+    __ cvtsi2ssl ($dst$$XMMRegister, $src$$Address);\n+  %}\n+  ins_pipe(pipe_slow); \/\/ XXX\n+%}\n+\n+instruct convI2D_reg_reg(vlRegD dst, rRegI src)\n+%{\n+  predicate(!UseXmmI2D);\n+  match(Set dst (ConvI2D src));\n+\n+  format %{ \"cvtsi2sdl $dst, $src\\t# i2d\" %}\n+  ins_encode %{\n+    if (UseAVX > 0) {\n+      __ pxor($dst$$XMMRegister, $dst$$XMMRegister);\n+    }\n+    __ cvtsi2sdl ($dst$$XMMRegister, $src$$Register);\n+  %}\n+  ins_pipe(pipe_slow); \/\/ XXX\n+%}\n+\n+instruct convI2D_reg_mem(regD dst, memory src)\n+%{\n+  predicate(UseAVX == 0);\n+  match(Set dst (ConvI2D (LoadI src)));\n+\n+  format %{ \"cvtsi2sdl $dst, $src\\t# i2d\" %}\n+  ins_encode %{\n+    __ cvtsi2sdl ($dst$$XMMRegister, $src$$Address);\n+  %}\n+  ins_pipe(pipe_slow); \/\/ XXX\n+%}\n+\n+instruct convXI2F_reg(regF dst, rRegI src)\n+%{\n+  predicate(UseXmmI2F);\n+  match(Set dst (ConvI2F src));\n+\n+  format %{ \"movdl $dst, $src\\n\\t\"\n+            \"cvtdq2psl $dst, $dst\\t# i2f\" %}\n+  ins_encode %{\n+    __ movdl($dst$$XMMRegister, $src$$Register);\n+    __ cvtdq2ps($dst$$XMMRegister, $dst$$XMMRegister);\n+  %}\n+  ins_pipe(pipe_slow); \/\/ XXX\n+%}\n+\n+instruct convXI2D_reg(regD dst, rRegI src)\n+%{\n+  predicate(UseXmmI2D);\n+  match(Set dst (ConvI2D src));\n+\n+  format %{ \"movdl $dst, $src\\n\\t\"\n+            \"cvtdq2pdl $dst, $dst\\t# i2d\" %}\n+  ins_encode %{\n+    __ movdl($dst$$XMMRegister, $src$$Register);\n+    __ cvtdq2pd($dst$$XMMRegister, $dst$$XMMRegister);\n+  %}\n+  ins_pipe(pipe_slow); \/\/ XXX\n+%}\n+\n+instruct convL2F_reg_reg(vlRegF dst, rRegL src)\n+%{\n+  match(Set dst (ConvL2F src));\n+\n+  format %{ \"cvtsi2ssq $dst, $src\\t# l2f\" %}\n+  ins_encode %{\n+    if (UseAVX > 0) {\n+      __ pxor($dst$$XMMRegister, $dst$$XMMRegister);\n+    }\n+    __ cvtsi2ssq ($dst$$XMMRegister, $src$$Register);\n+  %}\n+  ins_pipe(pipe_slow); \/\/ XXX\n+%}\n+\n+instruct convL2F_reg_mem(regF dst, memory src)\n+%{\n+  predicate(UseAVX == 0);\n+  match(Set dst (ConvL2F (LoadL src)));\n+\n+  format %{ \"cvtsi2ssq $dst, $src\\t# l2f\" %}\n+  ins_encode %{\n+    __ cvtsi2ssq ($dst$$XMMRegister, $src$$Address);\n+  %}\n+  ins_pipe(pipe_slow); \/\/ XXX\n+%}\n+\n+instruct convL2D_reg_reg(vlRegD dst, rRegL src)\n+%{\n+  match(Set dst (ConvL2D src));\n+\n+  format %{ \"cvtsi2sdq $dst, $src\\t# l2d\" %}\n+  ins_encode %{\n+    if (UseAVX > 0) {\n+      __ pxor($dst$$XMMRegister, $dst$$XMMRegister);\n+    }\n+    __ cvtsi2sdq ($dst$$XMMRegister, $src$$Register);\n+  %}\n+  ins_pipe(pipe_slow); \/\/ XXX\n+%}\n+\n+instruct convL2D_reg_mem(regD dst, memory src)\n+%{\n+  predicate(UseAVX == 0);\n+  match(Set dst (ConvL2D (LoadL src)));\n+\n+  format %{ \"cvtsi2sdq $dst, $src\\t# l2d\" %}\n+  ins_encode %{\n+    __ cvtsi2sdq ($dst$$XMMRegister, $src$$Address);\n+  %}\n+  ins_pipe(pipe_slow); \/\/ XXX\n+%}\n+\n+instruct convI2L_reg_reg(rRegL dst, rRegI src)\n+%{\n+  match(Set dst (ConvI2L src));\n+\n+  ins_cost(125);\n+  format %{ \"movslq  $dst, $src\\t# i2l\" %}\n+  ins_encode %{\n+    __ movslq($dst$$Register, $src$$Register);\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+\/\/ Zero-extend convert int to long\n+instruct convI2L_reg_reg_zex(rRegL dst, rRegI src, immL_32bits mask)\n+%{\n+  match(Set dst (AndL (ConvI2L src) mask));\n+\n+  format %{ \"movl    $dst, $src\\t# i2l zero-extend\\n\\t\" %}\n+  ins_encode %{\n+    if ($dst$$reg != $src$$reg) {\n+      __ movl($dst$$Register, $src$$Register);\n+    }\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+\/\/ Zero-extend convert int to long\n+instruct convI2L_reg_mem_zex(rRegL dst, memory src, immL_32bits mask)\n+%{\n+  match(Set dst (AndL (ConvI2L (LoadI src)) mask));\n+\n+  format %{ \"movl    $dst, $src\\t# i2l zero-extend\\n\\t\" %}\n+  ins_encode %{\n+    __ movl($dst$$Register, $src$$Address);\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+instruct zerox_long_reg_reg(rRegL dst, rRegL src, immL_32bits mask)\n+%{\n+  match(Set dst (AndL src mask));\n+\n+  format %{ \"movl    $dst, $src\\t# zero-extend long\" %}\n+  ins_encode %{\n+    __ movl($dst$$Register, $src$$Register);\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+instruct convL2I_reg_reg(rRegI dst, rRegL src)\n+%{\n+  match(Set dst (ConvL2I src));\n+\n+  format %{ \"movl    $dst, $src\\t# l2i\" %}\n+  ins_encode %{\n+    __ movl($dst$$Register, $src$$Register);\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+\n+instruct MoveF2I_stack_reg(rRegI dst, stackSlotF src) %{\n+  match(Set dst (MoveF2I src));\n+  effect(DEF dst, USE src);\n+\n+  ins_cost(125);\n+  format %{ \"movl    $dst, $src\\t# MoveF2I_stack_reg\" %}\n+  ins_encode %{\n+    __ movl($dst$$Register, Address(rsp, $src$$disp));\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+instruct MoveI2F_stack_reg(regF dst, stackSlotI src) %{\n+  match(Set dst (MoveI2F src));\n+  effect(DEF dst, USE src);\n+\n+  ins_cost(125);\n+  format %{ \"movss   $dst, $src\\t# MoveI2F_stack_reg\" %}\n+  ins_encode %{\n+    __ movflt($dst$$XMMRegister, Address(rsp, $src$$disp));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct MoveD2L_stack_reg(rRegL dst, stackSlotD src) %{\n+  match(Set dst (MoveD2L src));\n+  effect(DEF dst, USE src);\n+\n+  ins_cost(125);\n+  format %{ \"movq    $dst, $src\\t# MoveD2L_stack_reg\" %}\n+  ins_encode %{\n+    __ movq($dst$$Register, Address(rsp, $src$$disp));\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+instruct MoveL2D_stack_reg_partial(regD dst, stackSlotL src) %{\n+  predicate(!UseXmmLoadAndClearUpper);\n+  match(Set dst (MoveL2D src));\n+  effect(DEF dst, USE src);\n+\n+  ins_cost(125);\n+  format %{ \"movlpd  $dst, $src\\t# MoveL2D_stack_reg\" %}\n+  ins_encode %{\n+    __ movdbl($dst$$XMMRegister, Address(rsp, $src$$disp));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct MoveL2D_stack_reg(regD dst, stackSlotL src) %{\n+  predicate(UseXmmLoadAndClearUpper);\n+  match(Set dst (MoveL2D src));\n+  effect(DEF dst, USE src);\n+\n+  ins_cost(125);\n+  format %{ \"movsd   $dst, $src\\t# MoveL2D_stack_reg\" %}\n+  ins_encode %{\n+    __ movdbl($dst$$XMMRegister, Address(rsp, $src$$disp));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\n+instruct MoveF2I_reg_stack(stackSlotI dst, regF src) %{\n+  match(Set dst (MoveF2I src));\n+  effect(DEF dst, USE src);\n+\n+  ins_cost(95); \/\/ XXX\n+  format %{ \"movss   $dst, $src\\t# MoveF2I_reg_stack\" %}\n+  ins_encode %{\n+    __ movflt(Address(rsp, $dst$$disp), $src$$XMMRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct MoveI2F_reg_stack(stackSlotF dst, rRegI src) %{\n+  match(Set dst (MoveI2F src));\n+  effect(DEF dst, USE src);\n+\n+  ins_cost(100);\n+  format %{ \"movl    $dst, $src\\t# MoveI2F_reg_stack\" %}\n+  ins_encode %{\n+    __ movl(Address(rsp, $dst$$disp), $src$$Register);\n+  %}\n+  ins_pipe( ialu_mem_reg );\n+%}\n+\n+instruct MoveD2L_reg_stack(stackSlotL dst, regD src) %{\n+  match(Set dst (MoveD2L src));\n+  effect(DEF dst, USE src);\n+\n+  ins_cost(95); \/\/ XXX\n+  format %{ \"movsd   $dst, $src\\t# MoveL2D_reg_stack\" %}\n+  ins_encode %{\n+    __ movdbl(Address(rsp, $dst$$disp), $src$$XMMRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct MoveL2D_reg_stack(stackSlotD dst, rRegL src) %{\n+  match(Set dst (MoveL2D src));\n+  effect(DEF dst, USE src);\n+\n+  ins_cost(100);\n+  format %{ \"movq    $dst, $src\\t# MoveL2D_reg_stack\" %}\n+  ins_encode %{\n+    __ movq(Address(rsp, $dst$$disp), $src$$Register);\n+  %}\n+  ins_pipe(ialu_mem_reg);\n+%}\n+\n+instruct MoveF2I_reg_reg(rRegI dst, regF src) %{\n+  match(Set dst (MoveF2I src));\n+  effect(DEF dst, USE src);\n+  ins_cost(85);\n+  format %{ \"movd    $dst,$src\\t# MoveF2I\" %}\n+  ins_encode %{\n+    __ movdl($dst$$Register, $src$$XMMRegister);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct MoveD2L_reg_reg(rRegL dst, regD src) %{\n+  match(Set dst (MoveD2L src));\n+  effect(DEF dst, USE src);\n+  ins_cost(85);\n+  format %{ \"movd    $dst,$src\\t# MoveD2L\" %}\n+  ins_encode %{\n+    __ movdq($dst$$Register, $src$$XMMRegister);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct MoveI2F_reg_reg(regF dst, rRegI src) %{\n+  match(Set dst (MoveI2F src));\n+  effect(DEF dst, USE src);\n+  ins_cost(100);\n+  format %{ \"movd    $dst,$src\\t# MoveI2F\" %}\n+  ins_encode %{\n+    __ movdl($dst$$XMMRegister, $src$$Register);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct MoveL2D_reg_reg(regD dst, rRegL src) %{\n+  match(Set dst (MoveL2D src));\n+  effect(DEF dst, USE src);\n+  ins_cost(100);\n+  format %{ \"movd    $dst,$src\\t# MoveL2D\" %}\n+  ins_encode %{\n+     __ movdq($dst$$XMMRegister, $src$$Register);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+\n+\/\/ Fast clearing of an array\n+\/\/ Small non-constant lenght ClearArray for non-AVX512 targets.\n+instruct rep_stos(rcx_RegL cnt, rdi_RegP base, regD tmp, rax_RegL val,\n+                  Universe dummy, rFlagsReg cr)\n+%{\n+  predicate(!((ClearArrayNode*)n)->is_large() && !((ClearArrayNode*)n)->word_copy_only() && (UseAVX <= 2));\n+  match(Set dummy (ClearArray (Binary cnt base) val));\n+  effect(USE_KILL cnt, USE_KILL base, TEMP tmp, USE_KILL val, KILL cr);\n+\n+  format %{ $$template\n+    $$emit$$\"cmp     InitArrayShortSize,rcx\\n\\t\"\n+    $$emit$$\"jg      LARGE\\n\\t\"\n+    $$emit$$\"dec     rcx\\n\\t\"\n+    $$emit$$\"js      DONE\\t# Zero length\\n\\t\"\n+    $$emit$$\"mov     rax,(rdi,rcx,8)\\t# LOOP\\n\\t\"\n+    $$emit$$\"dec     rcx\\n\\t\"\n+    $$emit$$\"jge     LOOP\\n\\t\"\n+    $$emit$$\"jmp     DONE\\n\\t\"\n+    $$emit$$\"# LARGE:\\n\\t\"\n+    if (UseFastStosb) {\n+       $$emit$$\"shlq    rcx,3\\t# Convert doublewords to bytes\\n\\t\"\n+       $$emit$$\"rep     stosb\\t# Store rax to *rdi++ while rcx--\\n\\t\"\n+    } else if (UseXMMForObjInit) {\n+       $$emit$$\"movdq   $tmp, $val\\n\\t\"\n+       $$emit$$\"punpcklqdq $tmp, $tmp\\n\\t\"\n+       $$emit$$\"vinserti128_high $tmp, $tmp\\n\\t\"\n+       $$emit$$\"jmpq    L_zero_64_bytes\\n\\t\"\n+       $$emit$$\"# L_loop:\\t# 64-byte LOOP\\n\\t\"\n+       $$emit$$\"vmovdqu $tmp,(rax)\\n\\t\"\n+       $$emit$$\"vmovdqu $tmp,0x20(rax)\\n\\t\"\n+       $$emit$$\"add     0x40,rax\\n\\t\"\n+       $$emit$$\"# L_zero_64_bytes:\\n\\t\"\n+       $$emit$$\"sub     0x8,rcx\\n\\t\"\n+       $$emit$$\"jge     L_loop\\n\\t\"\n+       $$emit$$\"add     0x4,rcx\\n\\t\"\n+       $$emit$$\"jl      L_tail\\n\\t\"\n+       $$emit$$\"vmovdqu $tmp,(rax)\\n\\t\"\n+       $$emit$$\"add     0x20,rax\\n\\t\"\n+       $$emit$$\"sub     0x4,rcx\\n\\t\"\n+       $$emit$$\"# L_tail:\\t# Clearing tail bytes\\n\\t\"\n+       $$emit$$\"add     0x4,rcx\\n\\t\"\n+       $$emit$$\"jle     L_end\\n\\t\"\n+       $$emit$$\"dec     rcx\\n\\t\"\n+       $$emit$$\"# L_sloop:\\t# 8-byte short loop\\n\\t\"\n+       $$emit$$\"vmovq   xmm0,(rax)\\n\\t\"\n+       $$emit$$\"add     0x8,rax\\n\\t\"\n+       $$emit$$\"dec     rcx\\n\\t\"\n+       $$emit$$\"jge     L_sloop\\n\\t\"\n+       $$emit$$\"# L_end:\\n\\t\"\n+    } else {\n+       $$emit$$\"rep     stosq\\t# Store rax to *rdi++ while rcx--\\n\\t\"\n+    }\n+    $$emit$$\"# DONE\"\n+  %}\n+  ins_encode %{\n+    __ clear_mem($base$$Register, $cnt$$Register, $val$$Register,\n+                 $tmp$$XMMRegister, false, false);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct rep_stos_word_copy(rcx_RegL cnt, rdi_RegP base, regD tmp, rax_RegL val,\n+                            Universe dummy, rFlagsReg cr)\n+%{\n+  predicate(!((ClearArrayNode*)n)->is_large() && ((ClearArrayNode*)n)->word_copy_only() && (UseAVX <= 2));\n+  match(Set dummy (ClearArray (Binary cnt base) val));\n+  effect(USE_KILL cnt, USE_KILL base, TEMP tmp, USE_KILL val, KILL cr);\n+\n+  format %{ $$template\n+    $$emit$$\"cmp     InitArrayShortSize,rcx\\n\\t\"\n+    $$emit$$\"jg      LARGE\\n\\t\"\n+    $$emit$$\"dec     rcx\\n\\t\"\n+    $$emit$$\"js      DONE\\t# Zero length\\n\\t\"\n+    $$emit$$\"mov     rax,(rdi,rcx,8)\\t# LOOP\\n\\t\"\n+    $$emit$$\"dec     rcx\\n\\t\"\n+    $$emit$$\"jge     LOOP\\n\\t\"\n+    $$emit$$\"jmp     DONE\\n\\t\"\n+    $$emit$$\"# LARGE:\\n\\t\"\n+    if (UseXMMForObjInit) {\n+       $$emit$$\"movdq   $tmp, $val\\n\\t\"\n+       $$emit$$\"punpcklqdq $tmp, $tmp\\n\\t\"\n+       $$emit$$\"vinserti128_high $tmp, $tmp\\n\\t\"\n+       $$emit$$\"jmpq    L_zero_64_bytes\\n\\t\"\n+       $$emit$$\"# L_loop:\\t# 64-byte LOOP\\n\\t\"\n+       $$emit$$\"vmovdqu $tmp,(rax)\\n\\t\"\n+       $$emit$$\"vmovdqu $tmp,0x20(rax)\\n\\t\"\n+       $$emit$$\"add     0x40,rax\\n\\t\"\n+       $$emit$$\"# L_zero_64_bytes:\\n\\t\"\n+       $$emit$$\"sub     0x8,rcx\\n\\t\"\n+       $$emit$$\"jge     L_loop\\n\\t\"\n+       $$emit$$\"add     0x4,rcx\\n\\t\"\n+       $$emit$$\"jl      L_tail\\n\\t\"\n+       $$emit$$\"vmovdqu $tmp,(rax)\\n\\t\"\n+       $$emit$$\"add     0x20,rax\\n\\t\"\n+       $$emit$$\"sub     0x4,rcx\\n\\t\"\n+       $$emit$$\"# L_tail:\\t# Clearing tail bytes\\n\\t\"\n+       $$emit$$\"add     0x4,rcx\\n\\t\"\n+       $$emit$$\"jle     L_end\\n\\t\"\n+       $$emit$$\"dec     rcx\\n\\t\"\n+       $$emit$$\"# L_sloop:\\t# 8-byte short loop\\n\\t\"\n+       $$emit$$\"vmovq   xmm0,(rax)\\n\\t\"\n+       $$emit$$\"add     0x8,rax\\n\\t\"\n+       $$emit$$\"dec     rcx\\n\\t\"\n+       $$emit$$\"jge     L_sloop\\n\\t\"\n+       $$emit$$\"# L_end:\\n\\t\"\n+    } else {\n+       $$emit$$\"rep     stosq\\t# Store rax to *rdi++ while rcx--\\n\\t\"\n+    }\n+    $$emit$$\"# DONE\"\n+  %}\n+  ins_encode %{\n+    __ clear_mem($base$$Register, $cnt$$Register, $val$$Register,\n+                 $tmp$$XMMRegister, false, true);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ Small non-constant length ClearArray for AVX512 targets.\n+instruct rep_stos_evex(rcx_RegL cnt, rdi_RegP base, legRegD tmp, kReg ktmp, rax_RegL val,\n+                       Universe dummy, rFlagsReg cr)\n+%{\n+  predicate(!((ClearArrayNode*)n)->is_large() && !((ClearArrayNode*)n)->word_copy_only() && (UseAVX > 2));\n+  match(Set dummy (ClearArray (Binary cnt base) val));\n+  ins_cost(125);\n+  effect(USE_KILL cnt, USE_KILL base, TEMP tmp, TEMP ktmp, USE_KILL val, KILL cr);\n+\n+  format %{ $$template\n+    $$emit$$\"xorq    rax, rax\\t# ClearArray:\\n\\t\"\n+    $$emit$$\"cmp     InitArrayShortSize,rcx\\n\\t\"\n+    $$emit$$\"jg      LARGE\\n\\t\"\n+    $$emit$$\"dec     rcx\\n\\t\"\n+    $$emit$$\"js      DONE\\t# Zero length\\n\\t\"\n+    $$emit$$\"mov     rax,(rdi,rcx,8)\\t# LOOP\\n\\t\"\n+    $$emit$$\"dec     rcx\\n\\t\"\n+    $$emit$$\"jge     LOOP\\n\\t\"\n+    $$emit$$\"jmp     DONE\\n\\t\"\n+    $$emit$$\"# LARGE:\\n\\t\"\n+    if (UseFastStosb) {\n+       $$emit$$\"shlq    rcx,3\\t# Convert doublewords to bytes\\n\\t\"\n+       $$emit$$\"rep     stosb\\t# Store rax to *rdi++ while rcx--\\n\\t\"\n+    } else if (UseXMMForObjInit) {\n+       $$emit$$\"mov     rdi,rax\\n\\t\"\n+       $$emit$$\"vpxor   ymm0,ymm0,ymm0\\n\\t\"\n+       $$emit$$\"jmpq    L_zero_64_bytes\\n\\t\"\n+       $$emit$$\"# L_loop:\\t# 64-byte LOOP\\n\\t\"\n+       $$emit$$\"vmovdqu ymm0,(rax)\\n\\t\"\n+       $$emit$$\"vmovdqu ymm0,0x20(rax)\\n\\t\"\n+       $$emit$$\"add     0x40,rax\\n\\t\"\n+       $$emit$$\"# L_zero_64_bytes:\\n\\t\"\n+       $$emit$$\"sub     0x8,rcx\\n\\t\"\n+       $$emit$$\"jge     L_loop\\n\\t\"\n+       $$emit$$\"add     0x4,rcx\\n\\t\"\n+       $$emit$$\"jl      L_tail\\n\\t\"\n+       $$emit$$\"vmovdqu ymm0,(rax)\\n\\t\"\n+       $$emit$$\"add     0x20,rax\\n\\t\"\n+       $$emit$$\"sub     0x4,rcx\\n\\t\"\n+       $$emit$$\"# L_tail:\\t# Clearing tail bytes\\n\\t\"\n+       $$emit$$\"add     0x4,rcx\\n\\t\"\n+       $$emit$$\"jle     L_end\\n\\t\"\n+       $$emit$$\"dec     rcx\\n\\t\"\n+       $$emit$$\"# L_sloop:\\t# 8-byte short loop\\n\\t\"\n+       $$emit$$\"vmovq   xmm0,(rax)\\n\\t\"\n+       $$emit$$\"add     0x8,rax\\n\\t\"\n+       $$emit$$\"dec     rcx\\n\\t\"\n+       $$emit$$\"jge     L_sloop\\n\\t\"\n+       $$emit$$\"# L_end:\\n\\t\"\n+    } else {\n+       $$emit$$\"rep     stosq\\t# Store rax to *rdi++ while rcx--\\n\\t\"\n+    }\n+    $$emit$$\"# DONE\"\n+  %}\n+  ins_encode %{\n+    __ clear_mem($base$$Register, $cnt$$Register, $val$$Register,\n+                 $tmp$$XMMRegister, false, false, $ktmp$$KRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct rep_stos_evex_word_copy(rcx_RegL cnt, rdi_RegP base, legRegD tmp, kReg ktmp, rax_RegL val,\n+                                 Universe dummy, rFlagsReg cr)\n+%{\n+  predicate(!((ClearArrayNode*)n)->is_large() && ((ClearArrayNode*)n)->word_copy_only() && (UseAVX > 2));\n+  match(Set dummy (ClearArray (Binary cnt base) val));\n+  ins_cost(125);\n+  effect(USE_KILL cnt, USE_KILL base, TEMP tmp, TEMP ktmp, USE_KILL val, KILL cr);\n+\n+  format %{ $$template\n+    $$emit$$\"xorq    rax, rax\\t# ClearArray:\\n\\t\"\n+    $$emit$$\"cmp     InitArrayShortSize,rcx\\n\\t\"\n+    $$emit$$\"jg      LARGE\\n\\t\"\n+    $$emit$$\"dec     rcx\\n\\t\"\n+    $$emit$$\"js      DONE\\t# Zero length\\n\\t\"\n+    $$emit$$\"mov     rax,(rdi,rcx,8)\\t# LOOP\\n\\t\"\n+    $$emit$$\"dec     rcx\\n\\t\"\n+    $$emit$$\"jge     LOOP\\n\\t\"\n+    $$emit$$\"jmp     DONE\\n\\t\"\n+    $$emit$$\"# LARGE:\\n\\t\"\n+    if (UseFastStosb) {\n+       $$emit$$\"shlq    rcx,3\\t# Convert doublewords to bytes\\n\\t\"\n+       $$emit$$\"rep     stosb\\t# Store rax to *rdi++ while rcx--\\n\\t\"\n+    } else if (UseXMMForObjInit) {\n+       $$emit$$\"mov     rdi,rax\\n\\t\"\n+       $$emit$$\"vpxor   ymm0,ymm0,ymm0\\n\\t\"\n+       $$emit$$\"jmpq    L_zero_64_bytes\\n\\t\"\n+       $$emit$$\"# L_loop:\\t# 64-byte LOOP\\n\\t\"\n+       $$emit$$\"vmovdqu ymm0,(rax)\\n\\t\"\n+       $$emit$$\"vmovdqu ymm0,0x20(rax)\\n\\t\"\n+       $$emit$$\"add     0x40,rax\\n\\t\"\n+       $$emit$$\"# L_zero_64_bytes:\\n\\t\"\n+       $$emit$$\"sub     0x8,rcx\\n\\t\"\n+       $$emit$$\"jge     L_loop\\n\\t\"\n+       $$emit$$\"add     0x4,rcx\\n\\t\"\n+       $$emit$$\"jl      L_tail\\n\\t\"\n+       $$emit$$\"vmovdqu ymm0,(rax)\\n\\t\"\n+       $$emit$$\"add     0x20,rax\\n\\t\"\n+       $$emit$$\"sub     0x4,rcx\\n\\t\"\n+       $$emit$$\"# L_tail:\\t# Clearing tail bytes\\n\\t\"\n+       $$emit$$\"add     0x4,rcx\\n\\t\"\n+       $$emit$$\"jle     L_end\\n\\t\"\n+       $$emit$$\"dec     rcx\\n\\t\"\n+       $$emit$$\"# L_sloop:\\t# 8-byte short loop\\n\\t\"\n+       $$emit$$\"vmovq   xmm0,(rax)\\n\\t\"\n+       $$emit$$\"add     0x8,rax\\n\\t\"\n+       $$emit$$\"dec     rcx\\n\\t\"\n+       $$emit$$\"jge     L_sloop\\n\\t\"\n+       $$emit$$\"# L_end:\\n\\t\"\n+    } else {\n+       $$emit$$\"rep     stosq\\t# Store rax to *rdi++ while rcx--\\n\\t\"\n+    }\n+    $$emit$$\"# DONE\"\n+  %}\n+  ins_encode %{\n+    __ clear_mem($base$$Register, $cnt$$Register, $val$$Register,\n+                 $tmp$$XMMRegister, false, true, $ktmp$$KRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ Large non-constant length ClearArray for non-AVX512 targets.\n+instruct rep_stos_large(rcx_RegL cnt, rdi_RegP base, regD tmp, rax_RegL val,\n+                        Universe dummy, rFlagsReg cr)\n+%{\n+  predicate(((ClearArrayNode*)n)->is_large() && !((ClearArrayNode*)n)->word_copy_only() && (UseAVX <= 2));\n+  match(Set dummy (ClearArray (Binary cnt base) val));\n+  effect(USE_KILL cnt, USE_KILL base, TEMP tmp, USE_KILL val, KILL cr);\n+\n+  format %{ $$template\n+    if (UseFastStosb) {\n+       $$emit$$\"shlq    rcx,3\\t# Convert doublewords to bytes\\n\\t\"\n+       $$emit$$\"rep     stosb\\t# Store rax to *rdi++ while rcx--\"\n+    } else if (UseXMMForObjInit) {\n+       $$emit$$\"movdq   $tmp, $val\\n\\t\"\n+       $$emit$$\"punpcklqdq $tmp, $tmp\\n\\t\"\n+       $$emit$$\"vinserti128_high $tmp, $tmp\\n\\t\"\n+       $$emit$$\"jmpq    L_zero_64_bytes\\n\\t\"\n+       $$emit$$\"# L_loop:\\t# 64-byte LOOP\\n\\t\"\n+       $$emit$$\"vmovdqu $tmp,(rax)\\n\\t\"\n+       $$emit$$\"vmovdqu $tmp,0x20(rax)\\n\\t\"\n+       $$emit$$\"add     0x40,rax\\n\\t\"\n+       $$emit$$\"# L_zero_64_bytes:\\n\\t\"\n+       $$emit$$\"sub     0x8,rcx\\n\\t\"\n+       $$emit$$\"jge     L_loop\\n\\t\"\n+       $$emit$$\"add     0x4,rcx\\n\\t\"\n+       $$emit$$\"jl      L_tail\\n\\t\"\n+       $$emit$$\"vmovdqu $tmp,(rax)\\n\\t\"\n+       $$emit$$\"add     0x20,rax\\n\\t\"\n+       $$emit$$\"sub     0x4,rcx\\n\\t\"\n+       $$emit$$\"# L_tail:\\t# Clearing tail bytes\\n\\t\"\n+       $$emit$$\"add     0x4,rcx\\n\\t\"\n+       $$emit$$\"jle     L_end\\n\\t\"\n+       $$emit$$\"dec     rcx\\n\\t\"\n+       $$emit$$\"# L_sloop:\\t# 8-byte short loop\\n\\t\"\n+       $$emit$$\"vmovq   xmm0,(rax)\\n\\t\"\n+       $$emit$$\"add     0x8,rax\\n\\t\"\n+       $$emit$$\"dec     rcx\\n\\t\"\n+       $$emit$$\"jge     L_sloop\\n\\t\"\n+       $$emit$$\"# L_end:\\n\\t\"\n+    } else {\n+       $$emit$$\"rep     stosq\\t# Store rax to *rdi++ while rcx--\"\n+    }\n+  %}\n+  ins_encode %{\n+    __ clear_mem($base$$Register, $cnt$$Register, $val$$Register,\n+                 $tmp$$XMMRegister, true, false);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct rep_stos_large_word_copy(rcx_RegL cnt, rdi_RegP base, regD tmp, rax_RegL val,\n+                                  Universe dummy, rFlagsReg cr)\n+%{\n+  predicate(((ClearArrayNode*)n)->is_large() && ((ClearArrayNode*)n)->word_copy_only() && (UseAVX <= 2));\n+  match(Set dummy (ClearArray (Binary cnt base) val));\n+  effect(USE_KILL cnt, USE_KILL base, TEMP tmp, USE_KILL val, KILL cr);\n+\n+  format %{ $$template\n+    if (UseXMMForObjInit) {\n+       $$emit$$\"movdq   $tmp, $val\\n\\t\"\n+       $$emit$$\"punpcklqdq $tmp, $tmp\\n\\t\"\n+       $$emit$$\"vinserti128_high $tmp, $tmp\\n\\t\"\n+       $$emit$$\"jmpq    L_zero_64_bytes\\n\\t\"\n+       $$emit$$\"# L_loop:\\t# 64-byte LOOP\\n\\t\"\n+       $$emit$$\"vmovdqu $tmp,(rax)\\n\\t\"\n+       $$emit$$\"vmovdqu $tmp,0x20(rax)\\n\\t\"\n+       $$emit$$\"add     0x40,rax\\n\\t\"\n+       $$emit$$\"# L_zero_64_bytes:\\n\\t\"\n+       $$emit$$\"sub     0x8,rcx\\n\\t\"\n+       $$emit$$\"jge     L_loop\\n\\t\"\n+       $$emit$$\"add     0x4,rcx\\n\\t\"\n+       $$emit$$\"jl      L_tail\\n\\t\"\n+       $$emit$$\"vmovdqu $tmp,(rax)\\n\\t\"\n+       $$emit$$\"add     0x20,rax\\n\\t\"\n+       $$emit$$\"sub     0x4,rcx\\n\\t\"\n+       $$emit$$\"# L_tail:\\t# Clearing tail bytes\\n\\t\"\n+       $$emit$$\"add     0x4,rcx\\n\\t\"\n+       $$emit$$\"jle     L_end\\n\\t\"\n+       $$emit$$\"dec     rcx\\n\\t\"\n+       $$emit$$\"# L_sloop:\\t# 8-byte short loop\\n\\t\"\n+       $$emit$$\"vmovq   xmm0,(rax)\\n\\t\"\n+       $$emit$$\"add     0x8,rax\\n\\t\"\n+       $$emit$$\"dec     rcx\\n\\t\"\n+       $$emit$$\"jge     L_sloop\\n\\t\"\n+       $$emit$$\"# L_end:\\n\\t\"\n+    } else {\n+       $$emit$$\"rep     stosq\\t# Store rax to *rdi++ while rcx--\"\n+    }\n+  %}\n+  ins_encode %{\n+    __ clear_mem($base$$Register, $cnt$$Register, $val$$Register,\n+                 $tmp$$XMMRegister, true, true);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ Large non-constant length ClearArray for AVX512 targets.\n+instruct rep_stos_large_evex(rcx_RegL cnt, rdi_RegP base, legRegD tmp, kReg ktmp, rax_RegL val,\n+                             Universe dummy, rFlagsReg cr)\n+%{\n+  predicate(((ClearArrayNode*)n)->is_large() && !((ClearArrayNode*)n)->word_copy_only() && (UseAVX > 2));\n+  match(Set dummy (ClearArray (Binary cnt base) val));\n+  effect(USE_KILL cnt, USE_KILL base, TEMP tmp, TEMP ktmp, USE_KILL val, KILL cr);\n+\n+  format %{ $$template\n+    if (UseFastStosb) {\n+       $$emit$$\"xorq    rax, rax\\t# ClearArray:\\n\\t\"\n+       $$emit$$\"shlq    rcx,3\\t# Convert doublewords to bytes\\n\\t\"\n+       $$emit$$\"rep     stosb\\t# Store rax to *rdi++ while rcx--\"\n+    } else if (UseXMMForObjInit) {\n+       $$emit$$\"mov     rdi,rax\\t# ClearArray:\\n\\t\"\n+       $$emit$$\"vpxor   ymm0,ymm0,ymm0\\n\\t\"\n+       $$emit$$\"jmpq    L_zero_64_bytes\\n\\t\"\n+       $$emit$$\"# L_loop:\\t# 64-byte LOOP\\n\\t\"\n+       $$emit$$\"vmovdqu ymm0,(rax)\\n\\t\"\n+       $$emit$$\"vmovdqu ymm0,0x20(rax)\\n\\t\"\n+       $$emit$$\"add     0x40,rax\\n\\t\"\n+       $$emit$$\"# L_zero_64_bytes:\\n\\t\"\n+       $$emit$$\"sub     0x8,rcx\\n\\t\"\n+       $$emit$$\"jge     L_loop\\n\\t\"\n+       $$emit$$\"add     0x4,rcx\\n\\t\"\n+       $$emit$$\"jl      L_tail\\n\\t\"\n+       $$emit$$\"vmovdqu ymm0,(rax)\\n\\t\"\n+       $$emit$$\"add     0x20,rax\\n\\t\"\n+       $$emit$$\"sub     0x4,rcx\\n\\t\"\n+       $$emit$$\"# L_tail:\\t# Clearing tail bytes\\n\\t\"\n+       $$emit$$\"add     0x4,rcx\\n\\t\"\n+       $$emit$$\"jle     L_end\\n\\t\"\n+       $$emit$$\"dec     rcx\\n\\t\"\n+       $$emit$$\"# L_sloop:\\t# 8-byte short loop\\n\\t\"\n+       $$emit$$\"vmovq   xmm0,(rax)\\n\\t\"\n+       $$emit$$\"add     0x8,rax\\n\\t\"\n+       $$emit$$\"dec     rcx\\n\\t\"\n+       $$emit$$\"jge     L_sloop\\n\\t\"\n+       $$emit$$\"# L_end:\\n\\t\"\n+    } else {\n+       $$emit$$\"xorq    rax, rax\\t# ClearArray:\\n\\t\"\n+       $$emit$$\"rep     stosq\\t# Store rax to *rdi++ while rcx--\"\n+    }\n+  %}\n+  ins_encode %{\n+    __ clear_mem($base$$Register, $cnt$$Register, $val$$Register,\n+                 $tmp$$XMMRegister, true, false, $ktmp$$KRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct rep_stos_large_evex_word_copy(rcx_RegL cnt, rdi_RegP base, legRegD tmp, kReg ktmp, rax_RegL val,\n+                                       Universe dummy, rFlagsReg cr)\n+%{\n+  predicate(((ClearArrayNode*)n)->is_large() && ((ClearArrayNode*)n)->word_copy_only() && (UseAVX > 2));\n+  match(Set dummy (ClearArray (Binary cnt base) val));\n+  effect(USE_KILL cnt, USE_KILL base, TEMP tmp, TEMP ktmp, USE_KILL val, KILL cr);\n+\n+  format %{ $$template\n+    if (UseFastStosb) {\n+       $$emit$$\"xorq    rax, rax\\t# ClearArray:\\n\\t\"\n+       $$emit$$\"shlq    rcx,3\\t# Convert doublewords to bytes\\n\\t\"\n+       $$emit$$\"rep     stosb\\t# Store rax to *rdi++ while rcx--\"\n+    } else if (UseXMMForObjInit) {\n+       $$emit$$\"mov     rdi,rax\\t# ClearArray:\\n\\t\"\n+       $$emit$$\"vpxor   ymm0,ymm0,ymm0\\n\\t\"\n+       $$emit$$\"jmpq    L_zero_64_bytes\\n\\t\"\n+       $$emit$$\"# L_loop:\\t# 64-byte LOOP\\n\\t\"\n+       $$emit$$\"vmovdqu ymm0,(rax)\\n\\t\"\n+       $$emit$$\"vmovdqu ymm0,0x20(rax)\\n\\t\"\n+       $$emit$$\"add     0x40,rax\\n\\t\"\n+       $$emit$$\"# L_zero_64_bytes:\\n\\t\"\n+       $$emit$$\"sub     0x8,rcx\\n\\t\"\n+       $$emit$$\"jge     L_loop\\n\\t\"\n+       $$emit$$\"add     0x4,rcx\\n\\t\"\n+       $$emit$$\"jl      L_tail\\n\\t\"\n+       $$emit$$\"vmovdqu ymm0,(rax)\\n\\t\"\n+       $$emit$$\"add     0x20,rax\\n\\t\"\n+       $$emit$$\"sub     0x4,rcx\\n\\t\"\n+       $$emit$$\"# L_tail:\\t# Clearing tail bytes\\n\\t\"\n+       $$emit$$\"add     0x4,rcx\\n\\t\"\n+       $$emit$$\"jle     L_end\\n\\t\"\n+       $$emit$$\"dec     rcx\\n\\t\"\n+       $$emit$$\"# L_sloop:\\t# 8-byte short loop\\n\\t\"\n+       $$emit$$\"vmovq   xmm0,(rax)\\n\\t\"\n+       $$emit$$\"add     0x8,rax\\n\\t\"\n+       $$emit$$\"dec     rcx\\n\\t\"\n+       $$emit$$\"jge     L_sloop\\n\\t\"\n+       $$emit$$\"# L_end:\\n\\t\"\n+    } else {\n+       $$emit$$\"xorq    rax, rax\\t# ClearArray:\\n\\t\"\n+       $$emit$$\"rep     stosq\\t# Store rax to *rdi++ while rcx--\"\n+    }\n+  %}\n+  ins_encode %{\n+    __ clear_mem($base$$Register, $cnt$$Register, $val$$Register,\n+                 $tmp$$XMMRegister, true, true, $ktmp$$KRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ Small constant length ClearArray for AVX512 targets.\n+instruct rep_stos_im(immL cnt, rRegP base, regD tmp, rax_RegL val, kReg ktmp, Universe dummy, rFlagsReg cr)\n+%{\n+  predicate(!((ClearArrayNode*)n)->is_large() && !((ClearArrayNode*)n)->word_copy_only() &&\n+            ((MaxVectorSize >= 32) && VM_Version::supports_avx512vl()));\n+  match(Set dummy (ClearArray (Binary cnt base) val));\n+  ins_cost(100);\n+  effect(TEMP tmp, USE_KILL val, TEMP ktmp, KILL cr);\n+  format %{ \"clear_mem_imm $base , $cnt  \\n\\t\" %}\n+  ins_encode %{\n+    __ clear_mem($base$$Register, $cnt$$constant, $val$$Register, $tmp$$XMMRegister, $ktmp$$KRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct string_compareL(rdi_RegP str1, rcx_RegI cnt1, rsi_RegP str2, rdx_RegI cnt2,\n+                         rax_RegI result, legRegD tmp1, rFlagsReg cr)\n+%{\n+  predicate(!VM_Version::supports_avx512vlbw() && ((StrCompNode*)n)->encoding() == StrIntrinsicNode::LL);\n+  match(Set result (StrComp (Binary str1 cnt1) (Binary str2 cnt2)));\n+  effect(TEMP tmp1, USE_KILL str1, USE_KILL str2, USE_KILL cnt1, USE_KILL cnt2, KILL cr);\n+\n+  format %{ \"String Compare byte[] $str1,$cnt1,$str2,$cnt2 -> $result   \/\/ KILL $tmp1\" %}\n+  ins_encode %{\n+    __ string_compare($str1$$Register, $str2$$Register,\n+                      $cnt1$$Register, $cnt2$$Register, $result$$Register,\n+                      $tmp1$$XMMRegister, StrIntrinsicNode::LL, knoreg);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct string_compareL_evex(rdi_RegP str1, rcx_RegI cnt1, rsi_RegP str2, rdx_RegI cnt2,\n+                              rax_RegI result, legRegD tmp1, kReg ktmp, rFlagsReg cr)\n+%{\n+  predicate(VM_Version::supports_avx512vlbw() && ((StrCompNode*)n)->encoding() == StrIntrinsicNode::LL);\n+  match(Set result (StrComp (Binary str1 cnt1) (Binary str2 cnt2)));\n+  effect(TEMP tmp1, TEMP ktmp, USE_KILL str1, USE_KILL str2, USE_KILL cnt1, USE_KILL cnt2, KILL cr);\n+\n+  format %{ \"String Compare byte[] $str1,$cnt1,$str2,$cnt2 -> $result   \/\/ KILL $tmp1\" %}\n+  ins_encode %{\n+    __ string_compare($str1$$Register, $str2$$Register,\n+                      $cnt1$$Register, $cnt2$$Register, $result$$Register,\n+                      $tmp1$$XMMRegister, StrIntrinsicNode::LL, $ktmp$$KRegister);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct string_compareU(rdi_RegP str1, rcx_RegI cnt1, rsi_RegP str2, rdx_RegI cnt2,\n+                         rax_RegI result, legRegD tmp1, rFlagsReg cr)\n+%{\n+  predicate(!VM_Version::supports_avx512vlbw() && ((StrCompNode*)n)->encoding() == StrIntrinsicNode::UU);\n+  match(Set result (StrComp (Binary str1 cnt1) (Binary str2 cnt2)));\n+  effect(TEMP tmp1, USE_KILL str1, USE_KILL str2, USE_KILL cnt1, USE_KILL cnt2, KILL cr);\n+\n+  format %{ \"String Compare char[] $str1,$cnt1,$str2,$cnt2 -> $result   \/\/ KILL $tmp1\" %}\n+  ins_encode %{\n+    __ string_compare($str1$$Register, $str2$$Register,\n+                      $cnt1$$Register, $cnt2$$Register, $result$$Register,\n+                      $tmp1$$XMMRegister, StrIntrinsicNode::UU, knoreg);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct string_compareU_evex(rdi_RegP str1, rcx_RegI cnt1, rsi_RegP str2, rdx_RegI cnt2,\n+                              rax_RegI result, legRegD tmp1, kReg ktmp, rFlagsReg cr)\n+%{\n+  predicate(VM_Version::supports_avx512vlbw() && ((StrCompNode*)n)->encoding() == StrIntrinsicNode::UU);\n+  match(Set result (StrComp (Binary str1 cnt1) (Binary str2 cnt2)));\n+  effect(TEMP tmp1, TEMP ktmp, USE_KILL str1, USE_KILL str2, USE_KILL cnt1, USE_KILL cnt2, KILL cr);\n+\n+  format %{ \"String Compare char[] $str1,$cnt1,$str2,$cnt2 -> $result   \/\/ KILL $tmp1\" %}\n+  ins_encode %{\n+    __ string_compare($str1$$Register, $str2$$Register,\n+                      $cnt1$$Register, $cnt2$$Register, $result$$Register,\n+                      $tmp1$$XMMRegister, StrIntrinsicNode::UU, $ktmp$$KRegister);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct string_compareLU(rdi_RegP str1, rcx_RegI cnt1, rsi_RegP str2, rdx_RegI cnt2,\n+                          rax_RegI result, legRegD tmp1, rFlagsReg cr)\n+%{\n+  predicate(!VM_Version::supports_avx512vlbw() && ((StrCompNode*)n)->encoding() == StrIntrinsicNode::LU);\n+  match(Set result (StrComp (Binary str1 cnt1) (Binary str2 cnt2)));\n+  effect(TEMP tmp1, USE_KILL str1, USE_KILL str2, USE_KILL cnt1, USE_KILL cnt2, KILL cr);\n+\n+  format %{ \"String Compare byte[] $str1,$cnt1,$str2,$cnt2 -> $result   \/\/ KILL $tmp1\" %}\n+  ins_encode %{\n+    __ string_compare($str1$$Register, $str2$$Register,\n+                      $cnt1$$Register, $cnt2$$Register, $result$$Register,\n+                      $tmp1$$XMMRegister, StrIntrinsicNode::LU, knoreg);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct string_compareLU_evex(rdi_RegP str1, rcx_RegI cnt1, rsi_RegP str2, rdx_RegI cnt2,\n+                               rax_RegI result, legRegD tmp1, kReg ktmp, rFlagsReg cr)\n+%{\n+  predicate(VM_Version::supports_avx512vlbw() && ((StrCompNode*)n)->encoding() == StrIntrinsicNode::LU);\n+  match(Set result (StrComp (Binary str1 cnt1) (Binary str2 cnt2)));\n+  effect(TEMP tmp1, TEMP ktmp, USE_KILL str1, USE_KILL str2, USE_KILL cnt1, USE_KILL cnt2, KILL cr);\n+\n+  format %{ \"String Compare byte[] $str1,$cnt1,$str2,$cnt2 -> $result   \/\/ KILL $tmp1\" %}\n+  ins_encode %{\n+    __ string_compare($str1$$Register, $str2$$Register,\n+                      $cnt1$$Register, $cnt2$$Register, $result$$Register,\n+                      $tmp1$$XMMRegister, StrIntrinsicNode::LU, $ktmp$$KRegister);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct string_compareUL(rsi_RegP str1, rdx_RegI cnt1, rdi_RegP str2, rcx_RegI cnt2,\n+                          rax_RegI result, legRegD tmp1, rFlagsReg cr)\n+%{\n+  predicate(!VM_Version::supports_avx512vlbw() && ((StrCompNode*)n)->encoding() == StrIntrinsicNode::UL);\n+  match(Set result (StrComp (Binary str1 cnt1) (Binary str2 cnt2)));\n+  effect(TEMP tmp1, USE_KILL str1, USE_KILL str2, USE_KILL cnt1, USE_KILL cnt2, KILL cr);\n+\n+  format %{ \"String Compare byte[] $str1,$cnt1,$str2,$cnt2 -> $result   \/\/ KILL $tmp1\" %}\n+  ins_encode %{\n+    __ string_compare($str2$$Register, $str1$$Register,\n+                      $cnt2$$Register, $cnt1$$Register, $result$$Register,\n+                      $tmp1$$XMMRegister, StrIntrinsicNode::UL, knoreg);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct string_compareUL_evex(rsi_RegP str1, rdx_RegI cnt1, rdi_RegP str2, rcx_RegI cnt2,\n+                               rax_RegI result, legRegD tmp1, kReg ktmp, rFlagsReg cr)\n+%{\n+  predicate(VM_Version::supports_avx512vlbw() && ((StrCompNode*)n)->encoding() == StrIntrinsicNode::UL);\n+  match(Set result (StrComp (Binary str1 cnt1) (Binary str2 cnt2)));\n+  effect(TEMP tmp1, TEMP ktmp, USE_KILL str1, USE_KILL str2, USE_KILL cnt1, USE_KILL cnt2, KILL cr);\n+\n+  format %{ \"String Compare byte[] $str1,$cnt1,$str2,$cnt2 -> $result   \/\/ KILL $tmp1\" %}\n+  ins_encode %{\n+    __ string_compare($str2$$Register, $str1$$Register,\n+                      $cnt2$$Register, $cnt1$$Register, $result$$Register,\n+                      $tmp1$$XMMRegister, StrIntrinsicNode::UL, $ktmp$$KRegister);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+\/\/ fast search of substring with known size.\n+instruct string_indexof_conL(rdi_RegP str1, rdx_RegI cnt1, rsi_RegP str2, immI int_cnt2,\n+                             rbx_RegI result, legRegD tmp_vec, rax_RegI cnt2, rcx_RegI tmp, rFlagsReg cr)\n+%{\n+  predicate(UseSSE42Intrinsics && (((StrIndexOfNode*)n)->encoding() == StrIntrinsicNode::LL));\n+  match(Set result (StrIndexOf (Binary str1 cnt1) (Binary str2 int_cnt2)));\n+  effect(TEMP tmp_vec, USE_KILL str1, USE_KILL str2, USE_KILL cnt1, KILL cnt2, KILL tmp, KILL cr);\n+\n+  format %{ \"String IndexOf byte[] $str1,$cnt1,$str2,$int_cnt2 -> $result   \/\/ KILL $tmp_vec, $cnt1, $cnt2, $tmp\" %}\n+  ins_encode %{\n+    int icnt2 = (int)$int_cnt2$$constant;\n+    if (icnt2 >= 16) {\n+      \/\/ IndexOf for constant substrings with size >= 16 elements\n+      \/\/ which don't need to be loaded through stack.\n+      __ string_indexofC8($str1$$Register, $str2$$Register,\n+                          $cnt1$$Register, $cnt2$$Register,\n+                          icnt2, $result$$Register,\n+                          $tmp_vec$$XMMRegister, $tmp$$Register, StrIntrinsicNode::LL);\n+    } else {\n+      \/\/ Small strings are loaded through stack if they cross page boundary.\n+      __ string_indexof($str1$$Register, $str2$$Register,\n+                        $cnt1$$Register, $cnt2$$Register,\n+                        icnt2, $result$$Register,\n+                        $tmp_vec$$XMMRegister, $tmp$$Register, StrIntrinsicNode::LL);\n+    }\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+\/\/ fast search of substring with known size.\n+instruct string_indexof_conU(rdi_RegP str1, rdx_RegI cnt1, rsi_RegP str2, immI int_cnt2,\n+                             rbx_RegI result, legRegD tmp_vec, rax_RegI cnt2, rcx_RegI tmp, rFlagsReg cr)\n+%{\n+  predicate(UseSSE42Intrinsics && (((StrIndexOfNode*)n)->encoding() == StrIntrinsicNode::UU));\n+  match(Set result (StrIndexOf (Binary str1 cnt1) (Binary str2 int_cnt2)));\n+  effect(TEMP tmp_vec, USE_KILL str1, USE_KILL str2, USE_KILL cnt1, KILL cnt2, KILL tmp, KILL cr);\n+\n+  format %{ \"String IndexOf char[] $str1,$cnt1,$str2,$int_cnt2 -> $result   \/\/ KILL $tmp_vec, $cnt1, $cnt2, $tmp\" %}\n+  ins_encode %{\n+    int icnt2 = (int)$int_cnt2$$constant;\n+    if (icnt2 >= 8) {\n+      \/\/ IndexOf for constant substrings with size >= 8 elements\n+      \/\/ which don't need to be loaded through stack.\n+      __ string_indexofC8($str1$$Register, $str2$$Register,\n+                          $cnt1$$Register, $cnt2$$Register,\n+                          icnt2, $result$$Register,\n+                          $tmp_vec$$XMMRegister, $tmp$$Register, StrIntrinsicNode::UU);\n+    } else {\n+      \/\/ Small strings are loaded through stack if they cross page boundary.\n+      __ string_indexof($str1$$Register, $str2$$Register,\n+                        $cnt1$$Register, $cnt2$$Register,\n+                        icnt2, $result$$Register,\n+                        $tmp_vec$$XMMRegister, $tmp$$Register, StrIntrinsicNode::UU);\n+    }\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+\/\/ fast search of substring with known size.\n+instruct string_indexof_conUL(rdi_RegP str1, rdx_RegI cnt1, rsi_RegP str2, immI int_cnt2,\n+                              rbx_RegI result, legRegD tmp_vec, rax_RegI cnt2, rcx_RegI tmp, rFlagsReg cr)\n+%{\n+  predicate(UseSSE42Intrinsics && (((StrIndexOfNode*)n)->encoding() == StrIntrinsicNode::UL));\n+  match(Set result (StrIndexOf (Binary str1 cnt1) (Binary str2 int_cnt2)));\n+  effect(TEMP tmp_vec, USE_KILL str1, USE_KILL str2, USE_KILL cnt1, KILL cnt2, KILL tmp, KILL cr);\n+\n+  format %{ \"String IndexOf char[] $str1,$cnt1,$str2,$int_cnt2 -> $result   \/\/ KILL $tmp_vec, $cnt1, $cnt2, $tmp\" %}\n+  ins_encode %{\n+    int icnt2 = (int)$int_cnt2$$constant;\n+    if (icnt2 >= 8) {\n+      \/\/ IndexOf for constant substrings with size >= 8 elements\n+      \/\/ which don't need to be loaded through stack.\n+      __ string_indexofC8($str1$$Register, $str2$$Register,\n+                          $cnt1$$Register, $cnt2$$Register,\n+                          icnt2, $result$$Register,\n+                          $tmp_vec$$XMMRegister, $tmp$$Register, StrIntrinsicNode::UL);\n+    } else {\n+      \/\/ Small strings are loaded through stack if they cross page boundary.\n+      __ string_indexof($str1$$Register, $str2$$Register,\n+                        $cnt1$$Register, $cnt2$$Register,\n+                        icnt2, $result$$Register,\n+                        $tmp_vec$$XMMRegister, $tmp$$Register, StrIntrinsicNode::UL);\n+    }\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct string_indexofL(rdi_RegP str1, rdx_RegI cnt1, rsi_RegP str2, rax_RegI cnt2,\n+                         rbx_RegI result, legRegD tmp_vec, rcx_RegI tmp, rFlagsReg cr)\n+%{\n+  predicate(UseSSE42Intrinsics && (((StrIndexOfNode*)n)->encoding() == StrIntrinsicNode::LL));\n+  match(Set result (StrIndexOf (Binary str1 cnt1) (Binary str2 cnt2)));\n+  effect(TEMP tmp_vec, USE_KILL str1, USE_KILL str2, USE_KILL cnt1, USE_KILL cnt2, KILL tmp, KILL cr);\n+\n+  format %{ \"String IndexOf byte[] $str1,$cnt1,$str2,$cnt2 -> $result   \/\/ KILL all\" %}\n+  ins_encode %{\n+    __ string_indexof($str1$$Register, $str2$$Register,\n+                      $cnt1$$Register, $cnt2$$Register,\n+                      (-1), $result$$Register,\n+                      $tmp_vec$$XMMRegister, $tmp$$Register, StrIntrinsicNode::LL);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct string_indexofU(rdi_RegP str1, rdx_RegI cnt1, rsi_RegP str2, rax_RegI cnt2,\n+                         rbx_RegI result, legRegD tmp_vec, rcx_RegI tmp, rFlagsReg cr)\n+%{\n+  predicate(UseSSE42Intrinsics && (((StrIndexOfNode*)n)->encoding() == StrIntrinsicNode::UU));\n+  match(Set result (StrIndexOf (Binary str1 cnt1) (Binary str2 cnt2)));\n+  effect(TEMP tmp_vec, USE_KILL str1, USE_KILL str2, USE_KILL cnt1, USE_KILL cnt2, KILL tmp, KILL cr);\n+\n+  format %{ \"String IndexOf char[] $str1,$cnt1,$str2,$cnt2 -> $result   \/\/ KILL all\" %}\n+  ins_encode %{\n+    __ string_indexof($str1$$Register, $str2$$Register,\n+                      $cnt1$$Register, $cnt2$$Register,\n+                      (-1), $result$$Register,\n+                      $tmp_vec$$XMMRegister, $tmp$$Register, StrIntrinsicNode::UU);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct string_indexofUL(rdi_RegP str1, rdx_RegI cnt1, rsi_RegP str2, rax_RegI cnt2,\n+                          rbx_RegI result, legRegD tmp_vec, rcx_RegI tmp, rFlagsReg cr)\n+%{\n+  predicate(UseSSE42Intrinsics && (((StrIndexOfNode*)n)->encoding() == StrIntrinsicNode::UL));\n+  match(Set result (StrIndexOf (Binary str1 cnt1) (Binary str2 cnt2)));\n+  effect(TEMP tmp_vec, USE_KILL str1, USE_KILL str2, USE_KILL cnt1, USE_KILL cnt2, KILL tmp, KILL cr);\n+\n+  format %{ \"String IndexOf char[] $str1,$cnt1,$str2,$cnt2 -> $result   \/\/ KILL all\" %}\n+  ins_encode %{\n+    __ string_indexof($str1$$Register, $str2$$Register,\n+                      $cnt1$$Register, $cnt2$$Register,\n+                      (-1), $result$$Register,\n+                      $tmp_vec$$XMMRegister, $tmp$$Register, StrIntrinsicNode::UL);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct string_indexof_char(rdi_RegP str1, rdx_RegI cnt1, rax_RegI ch,\n+                              rbx_RegI result, legRegD tmp_vec1, legRegD tmp_vec2, legRegD tmp_vec3, rcx_RegI tmp, rFlagsReg cr)\n+%{\n+  predicate(UseSSE42Intrinsics && (((StrIndexOfCharNode*)n)->encoding() == StrIntrinsicNode::U));\n+  match(Set result (StrIndexOfChar (Binary str1 cnt1) ch));\n+  effect(TEMP tmp_vec1, TEMP tmp_vec2, TEMP tmp_vec3, USE_KILL str1, USE_KILL cnt1, USE_KILL ch, TEMP tmp, KILL cr);\n+  format %{ \"StringUTF16 IndexOf char[] $str1,$cnt1,$ch -> $result   \/\/ KILL all\" %}\n+  ins_encode %{\n+    __ string_indexof_char($str1$$Register, $cnt1$$Register, $ch$$Register, $result$$Register,\n+                           $tmp_vec1$$XMMRegister, $tmp_vec2$$XMMRegister, $tmp_vec3$$XMMRegister, $tmp$$Register);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct stringL_indexof_char(rdi_RegP str1, rdx_RegI cnt1, rax_RegI ch,\n+                              rbx_RegI result, legRegD tmp_vec1, legRegD tmp_vec2, legRegD tmp_vec3, rcx_RegI tmp, rFlagsReg cr)\n+%{\n+  predicate(UseSSE42Intrinsics && (((StrIndexOfCharNode*)n)->encoding() == StrIntrinsicNode::L));\n+  match(Set result (StrIndexOfChar (Binary str1 cnt1) ch));\n+  effect(TEMP tmp_vec1, TEMP tmp_vec2, TEMP tmp_vec3, USE_KILL str1, USE_KILL cnt1, USE_KILL ch, TEMP tmp, KILL cr);\n+  format %{ \"StringLatin1 IndexOf char[] $str1,$cnt1,$ch -> $result   \/\/ KILL all\" %}\n+  ins_encode %{\n+    __ stringL_indexof_char($str1$$Register, $cnt1$$Register, $ch$$Register, $result$$Register,\n+                           $tmp_vec1$$XMMRegister, $tmp_vec2$$XMMRegister, $tmp_vec3$$XMMRegister, $tmp$$Register);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+\/\/ fast string equals\n+instruct string_equals(rdi_RegP str1, rsi_RegP str2, rcx_RegI cnt, rax_RegI result,\n+                       legRegD tmp1, legRegD tmp2, rbx_RegI tmp3, rFlagsReg cr)\n+%{\n+  predicate(!VM_Version::supports_avx512vlbw());\n+  match(Set result (StrEquals (Binary str1 str2) cnt));\n+  effect(TEMP tmp1, TEMP tmp2, USE_KILL str1, USE_KILL str2, USE_KILL cnt, KILL tmp3, KILL cr);\n+\n+  format %{ \"String Equals $str1,$str2,$cnt -> $result    \/\/ KILL $tmp1, $tmp2, $tmp3\" %}\n+  ins_encode %{\n+    __ arrays_equals(false, $str1$$Register, $str2$$Register,\n+                     $cnt$$Register, $result$$Register, $tmp3$$Register,\n+                     $tmp1$$XMMRegister, $tmp2$$XMMRegister, false \/* char *\/, knoreg);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct string_equals_evex(rdi_RegP str1, rsi_RegP str2, rcx_RegI cnt, rax_RegI result,\n+                           legRegD tmp1, legRegD tmp2, kReg ktmp, rbx_RegI tmp3, rFlagsReg cr)\n+%{\n+  predicate(VM_Version::supports_avx512vlbw());\n+  match(Set result (StrEquals (Binary str1 str2) cnt));\n+  effect(TEMP tmp1, TEMP tmp2, TEMP ktmp, USE_KILL str1, USE_KILL str2, USE_KILL cnt, KILL tmp3, KILL cr);\n+\n+  format %{ \"String Equals $str1,$str2,$cnt -> $result    \/\/ KILL $tmp1, $tmp2, $tmp3\" %}\n+  ins_encode %{\n+    __ arrays_equals(false, $str1$$Register, $str2$$Register,\n+                     $cnt$$Register, $result$$Register, $tmp3$$Register,\n+                     $tmp1$$XMMRegister, $tmp2$$XMMRegister, false \/* char *\/, $ktmp$$KRegister);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+\/\/ fast array equals\n+instruct array_equalsB(rdi_RegP ary1, rsi_RegP ary2, rax_RegI result,\n+                       legRegD tmp1, legRegD tmp2, rcx_RegI tmp3, rbx_RegI tmp4, rFlagsReg cr)\n+%{\n+  predicate(!VM_Version::supports_avx512vlbw() && ((AryEqNode*)n)->encoding() == StrIntrinsicNode::LL);\n+  match(Set result (AryEq ary1 ary2));\n+  effect(TEMP tmp1, TEMP tmp2, USE_KILL ary1, USE_KILL ary2, KILL tmp3, KILL tmp4, KILL cr);\n+\n+  format %{ \"Array Equals byte[] $ary1,$ary2 -> $result   \/\/ KILL $tmp1, $tmp2, $tmp3, $tmp4\" %}\n+  ins_encode %{\n+    __ arrays_equals(true, $ary1$$Register, $ary2$$Register,\n+                     $tmp3$$Register, $result$$Register, $tmp4$$Register,\n+                     $tmp1$$XMMRegister, $tmp2$$XMMRegister, false \/* char *\/, knoreg);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct array_equalsB_evex(rdi_RegP ary1, rsi_RegP ary2, rax_RegI result,\n+                            legRegD tmp1, legRegD tmp2, kReg ktmp, rcx_RegI tmp3, rbx_RegI tmp4, rFlagsReg cr)\n+%{\n+  predicate(VM_Version::supports_avx512vlbw() && ((AryEqNode*)n)->encoding() == StrIntrinsicNode::LL);\n+  match(Set result (AryEq ary1 ary2));\n+  effect(TEMP tmp1, TEMP tmp2, TEMP ktmp, USE_KILL ary1, USE_KILL ary2, KILL tmp3, KILL tmp4, KILL cr);\n+\n+  format %{ \"Array Equals byte[] $ary1,$ary2 -> $result   \/\/ KILL $tmp1, $tmp2, $tmp3, $tmp4\" %}\n+  ins_encode %{\n+    __ arrays_equals(true, $ary1$$Register, $ary2$$Register,\n+                     $tmp3$$Register, $result$$Register, $tmp4$$Register,\n+                     $tmp1$$XMMRegister, $tmp2$$XMMRegister, false \/* char *\/, $ktmp$$KRegister);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct array_equalsC(rdi_RegP ary1, rsi_RegP ary2, rax_RegI result,\n+                       legRegD tmp1, legRegD tmp2, rcx_RegI tmp3, rbx_RegI tmp4, rFlagsReg cr)\n+%{\n+  predicate(!VM_Version::supports_avx512vlbw() && ((AryEqNode*)n)->encoding() == StrIntrinsicNode::UU);\n+  match(Set result (AryEq ary1 ary2));\n+  effect(TEMP tmp1, TEMP tmp2, USE_KILL ary1, USE_KILL ary2, KILL tmp3, KILL tmp4, KILL cr);\n+\n+  format %{ \"Array Equals char[] $ary1,$ary2 -> $result   \/\/ KILL $tmp1, $tmp2, $tmp3, $tmp4\" %}\n+  ins_encode %{\n+    __ arrays_equals(true, $ary1$$Register, $ary2$$Register,\n+                     $tmp3$$Register, $result$$Register, $tmp4$$Register,\n+                     $tmp1$$XMMRegister, $tmp2$$XMMRegister, true \/* char *\/, knoreg);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct array_equalsC_evex(rdi_RegP ary1, rsi_RegP ary2, rax_RegI result,\n+                            legRegD tmp1, legRegD tmp2, kReg ktmp, rcx_RegI tmp3, rbx_RegI tmp4, rFlagsReg cr)\n+%{\n+  predicate(VM_Version::supports_avx512vlbw() && ((AryEqNode*)n)->encoding() == StrIntrinsicNode::UU);\n+  match(Set result (AryEq ary1 ary2));\n+  effect(TEMP tmp1, TEMP tmp2, TEMP ktmp, USE_KILL ary1, USE_KILL ary2, KILL tmp3, KILL tmp4, KILL cr);\n+\n+  format %{ \"Array Equals char[] $ary1,$ary2 -> $result   \/\/ KILL $tmp1, $tmp2, $tmp3, $tmp4\" %}\n+  ins_encode %{\n+    __ arrays_equals(true, $ary1$$Register, $ary2$$Register,\n+                     $tmp3$$Register, $result$$Register, $tmp4$$Register,\n+                     $tmp1$$XMMRegister, $tmp2$$XMMRegister, true \/* char *\/, $ktmp$$KRegister);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct arrays_hashcode(rdi_RegP ary1, rdx_RegI cnt1, rbx_RegI result, immU8 basic_type,\n+                         legRegD tmp_vec1, legRegD tmp_vec2, legRegD tmp_vec3, legRegD tmp_vec4,\n+                         legRegD tmp_vec5, legRegD tmp_vec6, legRegD tmp_vec7, legRegD tmp_vec8,\n+                         legRegD tmp_vec9, legRegD tmp_vec10, legRegD tmp_vec11, legRegD tmp_vec12,\n+                         legRegD tmp_vec13, rRegI tmp1, rRegI tmp2, rRegI tmp3, rFlagsReg cr)\n+%{\n+  predicate(UseAVX >= 2);\n+  match(Set result (VectorizedHashCode (Binary ary1 cnt1) (Binary result basic_type)));\n+  effect(TEMP tmp_vec1, TEMP tmp_vec2, TEMP tmp_vec3, TEMP tmp_vec4, TEMP tmp_vec5, TEMP tmp_vec6,\n+         TEMP tmp_vec7, TEMP tmp_vec8, TEMP tmp_vec9, TEMP tmp_vec10, TEMP tmp_vec11, TEMP tmp_vec12,\n+         TEMP tmp_vec13, TEMP tmp1, TEMP tmp2, TEMP tmp3, USE_KILL ary1, USE_KILL cnt1,\n+         USE basic_type, KILL cr);\n+\n+  format %{ \"Array HashCode array[] $ary1,$cnt1,$result,$basic_type -> $result   \/\/ KILL all\" %}\n+  ins_encode %{\n+    __ arrays_hashcode($ary1$$Register, $cnt1$$Register, $result$$Register,\n+                       $tmp1$$Register, $tmp2$$Register, $tmp3$$Register,\n+                       $tmp_vec1$$XMMRegister, $tmp_vec2$$XMMRegister, $tmp_vec3$$XMMRegister,\n+                       $tmp_vec4$$XMMRegister, $tmp_vec5$$XMMRegister, $tmp_vec6$$XMMRegister,\n+                       $tmp_vec7$$XMMRegister, $tmp_vec8$$XMMRegister, $tmp_vec9$$XMMRegister,\n+                       $tmp_vec10$$XMMRegister, $tmp_vec11$$XMMRegister, $tmp_vec12$$XMMRegister,\n+                       $tmp_vec13$$XMMRegister, (BasicType)$basic_type$$constant);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct count_positives(rsi_RegP ary1, rcx_RegI len, rax_RegI result,\n+                         legRegD tmp1, legRegD tmp2, rbx_RegI tmp3, rFlagsReg cr,)\n+%{\n+  predicate(!VM_Version::supports_avx512vlbw() || !VM_Version::supports_bmi2());\n+  match(Set result (CountPositives ary1 len));\n+  effect(TEMP tmp1, TEMP tmp2, USE_KILL ary1, USE_KILL len, KILL tmp3, KILL cr);\n+\n+  format %{ \"countPositives byte[] $ary1,$len -> $result   \/\/ KILL $tmp1, $tmp2, $tmp3\" %}\n+  ins_encode %{\n+    __ count_positives($ary1$$Register, $len$$Register,\n+                       $result$$Register, $tmp3$$Register,\n+                       $tmp1$$XMMRegister, $tmp2$$XMMRegister, knoreg, knoreg);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct count_positives_evex(rsi_RegP ary1, rcx_RegI len, rax_RegI result,\n+                              legRegD tmp1, legRegD tmp2, kReg ktmp1, kReg ktmp2, rbx_RegI tmp3, rFlagsReg cr,)\n+%{\n+  predicate(VM_Version::supports_avx512vlbw() && VM_Version::supports_bmi2());\n+  match(Set result (CountPositives ary1 len));\n+  effect(TEMP tmp1, TEMP tmp2, TEMP ktmp1, TEMP ktmp2, USE_KILL ary1, USE_KILL len, KILL tmp3, KILL cr);\n+\n+  format %{ \"countPositives byte[] $ary1,$len -> $result   \/\/ KILL $tmp1, $tmp2, $tmp3\" %}\n+  ins_encode %{\n+    __ count_positives($ary1$$Register, $len$$Register,\n+                       $result$$Register, $tmp3$$Register,\n+                       $tmp1$$XMMRegister, $tmp2$$XMMRegister, $ktmp1$$KRegister, $ktmp2$$KRegister);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+\/\/ fast char[] to byte[] compression\n+instruct string_compress(rsi_RegP src, rdi_RegP dst, rdx_RegI len, legRegD tmp1, legRegD tmp2, legRegD tmp3,\n+                         legRegD tmp4, rcx_RegI tmp5, rax_RegI result, rFlagsReg cr) %{\n+  predicate(!VM_Version::supports_avx512vlbw() || !VM_Version::supports_bmi2());\n+  match(Set result (StrCompressedCopy src (Binary dst len)));\n+  effect(TEMP tmp1, TEMP tmp2, TEMP tmp3, TEMP tmp4, USE_KILL src, USE_KILL dst,\n+         USE_KILL len, KILL tmp5, KILL cr);\n+\n+  format %{ \"String Compress $src,$dst -> $result    \/\/ KILL RAX, RCX, RDX\" %}\n+  ins_encode %{\n+    __ char_array_compress($src$$Register, $dst$$Register, $len$$Register,\n+                           $tmp1$$XMMRegister, $tmp2$$XMMRegister, $tmp3$$XMMRegister,\n+                           $tmp4$$XMMRegister, $tmp5$$Register, $result$$Register,\n+                           knoreg, knoreg);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct string_compress_evex(rsi_RegP src, rdi_RegP dst, rdx_RegI len, legRegD tmp1, legRegD tmp2, legRegD tmp3,\n+                              legRegD tmp4, kReg ktmp1, kReg ktmp2, rcx_RegI tmp5, rax_RegI result, rFlagsReg cr) %{\n+  predicate(VM_Version::supports_avx512vlbw() && VM_Version::supports_bmi2());\n+  match(Set result (StrCompressedCopy src (Binary dst len)));\n+  effect(TEMP tmp1, TEMP tmp2, TEMP tmp3, TEMP tmp4, TEMP ktmp1, TEMP ktmp2, USE_KILL src, USE_KILL dst,\n+         USE_KILL len, KILL tmp5, KILL cr);\n+\n+  format %{ \"String Compress $src,$dst -> $result    \/\/ KILL RAX, RCX, RDX\" %}\n+  ins_encode %{\n+    __ char_array_compress($src$$Register, $dst$$Register, $len$$Register,\n+                           $tmp1$$XMMRegister, $tmp2$$XMMRegister, $tmp3$$XMMRegister,\n+                           $tmp4$$XMMRegister, $tmp5$$Register, $result$$Register,\n+                           $ktmp1$$KRegister, $ktmp2$$KRegister);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\/\/ fast byte[] to char[] inflation\n+instruct string_inflate(Universe dummy, rsi_RegP src, rdi_RegP dst, rdx_RegI len,\n+                        legRegD tmp1, rcx_RegI tmp2, rFlagsReg cr) %{\n+  predicate(!VM_Version::supports_avx512vlbw() || !VM_Version::supports_bmi2());\n+  match(Set dummy (StrInflatedCopy src (Binary dst len)));\n+  effect(TEMP tmp1, TEMP tmp2, USE_KILL src, USE_KILL dst, USE_KILL len, KILL cr);\n+\n+  format %{ \"String Inflate $src,$dst    \/\/ KILL $tmp1, $tmp2\" %}\n+  ins_encode %{\n+    __ byte_array_inflate($src$$Register, $dst$$Register, $len$$Register,\n+                          $tmp1$$XMMRegister, $tmp2$$Register, knoreg);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct string_inflate_evex(Universe dummy, rsi_RegP src, rdi_RegP dst, rdx_RegI len,\n+                             legRegD tmp1, kReg ktmp, rcx_RegI tmp2, rFlagsReg cr) %{\n+  predicate(VM_Version::supports_avx512vlbw() && VM_Version::supports_bmi2());\n+  match(Set dummy (StrInflatedCopy src (Binary dst len)));\n+  effect(TEMP tmp1, TEMP tmp2, TEMP ktmp, USE_KILL src, USE_KILL dst, USE_KILL len, KILL cr);\n+\n+  format %{ \"String Inflate $src,$dst    \/\/ KILL $tmp1, $tmp2\" %}\n+  ins_encode %{\n+    __ byte_array_inflate($src$$Register, $dst$$Register, $len$$Register,\n+                          $tmp1$$XMMRegister, $tmp2$$Register, $ktmp$$KRegister);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+\/\/ encode char[] to byte[] in ISO_8859_1\n+instruct encode_iso_array(rsi_RegP src, rdi_RegP dst, rdx_RegI len,\n+                          legRegD tmp1, legRegD tmp2, legRegD tmp3, legRegD tmp4,\n+                          rcx_RegI tmp5, rax_RegI result, rFlagsReg cr) %{\n+  predicate(!((EncodeISOArrayNode*)n)->is_ascii());\n+  match(Set result (EncodeISOArray src (Binary dst len)));\n+  effect(TEMP tmp1, TEMP tmp2, TEMP tmp3, TEMP tmp4, USE_KILL src, USE_KILL dst, USE_KILL len, KILL tmp5, KILL cr);\n+\n+  format %{ \"Encode iso array $src,$dst,$len -> $result    \/\/ KILL RCX, RDX, $tmp1, $tmp2, $tmp3, $tmp4, RSI, RDI \" %}\n+  ins_encode %{\n+    __ encode_iso_array($src$$Register, $dst$$Register, $len$$Register,\n+                        $tmp1$$XMMRegister, $tmp2$$XMMRegister, $tmp3$$XMMRegister,\n+                        $tmp4$$XMMRegister, $tmp5$$Register, $result$$Register, false);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+\/\/ encode char[] to byte[] in ASCII\n+instruct encode_ascii_array(rsi_RegP src, rdi_RegP dst, rdx_RegI len,\n+                            legRegD tmp1, legRegD tmp2, legRegD tmp3, legRegD tmp4,\n+                            rcx_RegI tmp5, rax_RegI result, rFlagsReg cr) %{\n+  predicate(((EncodeISOArrayNode*)n)->is_ascii());\n+  match(Set result (EncodeISOArray src (Binary dst len)));\n+  effect(TEMP tmp1, TEMP tmp2, TEMP tmp3, TEMP tmp4, USE_KILL src, USE_KILL dst, USE_KILL len, KILL tmp5, KILL cr);\n+\n+  format %{ \"Encode ascii array $src,$dst,$len -> $result    \/\/ KILL RCX, RDX, $tmp1, $tmp2, $tmp3, $tmp4, RSI, RDI \" %}\n+  ins_encode %{\n+    __ encode_iso_array($src$$Register, $dst$$Register, $len$$Register,\n+                        $tmp1$$XMMRegister, $tmp2$$XMMRegister, $tmp3$$XMMRegister,\n+                        $tmp4$$XMMRegister, $tmp5$$Register, $result$$Register, true);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+\/\/----------Overflow Math Instructions-----------------------------------------\n+\n+instruct overflowAddI_rReg(rFlagsReg cr, rax_RegI op1, rRegI op2)\n+%{\n+  match(Set cr (OverflowAddI op1 op2));\n+  effect(DEF cr, USE_KILL op1, USE op2);\n+\n+  format %{ \"addl    $op1, $op2\\t# overflow check int\" %}\n+\n+  ins_encode %{\n+    __ addl($op1$$Register, $op2$$Register);\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+instruct overflowAddI_rReg_imm(rFlagsReg cr, rax_RegI op1, immI op2)\n+%{\n+  match(Set cr (OverflowAddI op1 op2));\n+  effect(DEF cr, USE_KILL op1, USE op2);\n+\n+  format %{ \"addl    $op1, $op2\\t# overflow check int\" %}\n+\n+  ins_encode %{\n+    __ addl($op1$$Register, $op2$$constant);\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+instruct overflowAddL_rReg(rFlagsReg cr, rax_RegL op1, rRegL op2)\n+%{\n+  match(Set cr (OverflowAddL op1 op2));\n+  effect(DEF cr, USE_KILL op1, USE op2);\n+\n+  format %{ \"addq    $op1, $op2\\t# overflow check long\" %}\n+  ins_encode %{\n+    __ addq($op1$$Register, $op2$$Register);\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+instruct overflowAddL_rReg_imm(rFlagsReg cr, rax_RegL op1, immL32 op2)\n+%{\n+  match(Set cr (OverflowAddL op1 op2));\n+  effect(DEF cr, USE_KILL op1, USE op2);\n+\n+  format %{ \"addq    $op1, $op2\\t# overflow check long\" %}\n+  ins_encode %{\n+    __ addq($op1$$Register, $op2$$constant);\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+instruct overflowSubI_rReg(rFlagsReg cr, rRegI op1, rRegI op2)\n+%{\n+  match(Set cr (OverflowSubI op1 op2));\n+\n+  format %{ \"cmpl    $op1, $op2\\t# overflow check int\" %}\n+  ins_encode %{\n+    __ cmpl($op1$$Register, $op2$$Register);\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+instruct overflowSubI_rReg_imm(rFlagsReg cr, rRegI op1, immI op2)\n+%{\n+  match(Set cr (OverflowSubI op1 op2));\n+\n+  format %{ \"cmpl    $op1, $op2\\t# overflow check int\" %}\n+  ins_encode %{\n+    __ cmpl($op1$$Register, $op2$$constant);\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+instruct overflowSubL_rReg(rFlagsReg cr, rRegL op1, rRegL op2)\n+%{\n+  match(Set cr (OverflowSubL op1 op2));\n+\n+  format %{ \"cmpq    $op1, $op2\\t# overflow check long\" %}\n+  ins_encode %{\n+    __ cmpq($op1$$Register, $op2$$Register);\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+instruct overflowSubL_rReg_imm(rFlagsReg cr, rRegL op1, immL32 op2)\n+%{\n+  match(Set cr (OverflowSubL op1 op2));\n+\n+  format %{ \"cmpq    $op1, $op2\\t# overflow check long\" %}\n+  ins_encode %{\n+    __ cmpq($op1$$Register, $op2$$constant);\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+instruct overflowNegI_rReg(rFlagsReg cr, immI_0 zero, rax_RegI op2)\n+%{\n+  match(Set cr (OverflowSubI zero op2));\n+  effect(DEF cr, USE_KILL op2);\n+\n+  format %{ \"negl    $op2\\t# overflow check int\" %}\n+  ins_encode %{\n+    __ negl($op2$$Register);\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+instruct overflowNegL_rReg(rFlagsReg cr, immL0 zero, rax_RegL op2)\n+%{\n+  match(Set cr (OverflowSubL zero op2));\n+  effect(DEF cr, USE_KILL op2);\n+\n+  format %{ \"negq    $op2\\t# overflow check long\" %}\n+  ins_encode %{\n+    __ negq($op2$$Register);\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+instruct overflowMulI_rReg(rFlagsReg cr, rax_RegI op1, rRegI op2)\n+%{\n+  match(Set cr (OverflowMulI op1 op2));\n+  effect(DEF cr, USE_KILL op1, USE op2);\n+\n+  format %{ \"imull    $op1, $op2\\t# overflow check int\" %}\n+  ins_encode %{\n+    __ imull($op1$$Register, $op2$$Register);\n+  %}\n+  ins_pipe(ialu_reg_reg_alu0);\n+%}\n+\n+instruct overflowMulI_rReg_imm(rFlagsReg cr, rRegI op1, immI op2, rRegI tmp)\n+%{\n+  match(Set cr (OverflowMulI op1 op2));\n+  effect(DEF cr, TEMP tmp, USE op1, USE op2);\n+\n+  format %{ \"imull    $tmp, $op1, $op2\\t# overflow check int\" %}\n+  ins_encode %{\n+    __ imull($tmp$$Register, $op1$$Register, $op2$$constant);\n+  %}\n+  ins_pipe(ialu_reg_reg_alu0);\n+%}\n+\n+instruct overflowMulL_rReg(rFlagsReg cr, rax_RegL op1, rRegL op2)\n+%{\n+  match(Set cr (OverflowMulL op1 op2));\n+  effect(DEF cr, USE_KILL op1, USE op2);\n+\n+  format %{ \"imulq    $op1, $op2\\t# overflow check long\" %}\n+  ins_encode %{\n+    __ imulq($op1$$Register, $op2$$Register);\n+  %}\n+  ins_pipe(ialu_reg_reg_alu0);\n+%}\n+\n+instruct overflowMulL_rReg_imm(rFlagsReg cr, rRegL op1, immL32 op2, rRegL tmp)\n+%{\n+  match(Set cr (OverflowMulL op1 op2));\n+  effect(DEF cr, TEMP tmp, USE op1, USE op2);\n+\n+  format %{ \"imulq    $tmp, $op1, $op2\\t# overflow check long\" %}\n+  ins_encode %{\n+    __ imulq($tmp$$Register, $op1$$Register, $op2$$constant);\n+  %}\n+  ins_pipe(ialu_reg_reg_alu0);\n+%}\n+\n+\n+\/\/----------Control Flow Instructions------------------------------------------\n+\/\/ Signed compare Instructions\n+\n+\/\/ XXX more variants!!\n+instruct compI_rReg(rFlagsReg cr, rRegI op1, rRegI op2)\n+%{\n+  match(Set cr (CmpI op1 op2));\n+  effect(DEF cr, USE op1, USE op2);\n+\n+  format %{ \"cmpl    $op1, $op2\" %}\n+  ins_encode %{\n+    __ cmpl($op1$$Register, $op2$$Register);\n+  %}\n+  ins_pipe(ialu_cr_reg_reg);\n+%}\n+\n+instruct compI_rReg_imm(rFlagsReg cr, rRegI op1, immI op2)\n+%{\n+  match(Set cr (CmpI op1 op2));\n+\n+  format %{ \"cmpl    $op1, $op2\" %}\n+  ins_encode %{\n+    __ cmpl($op1$$Register, $op2$$constant);\n+  %}\n+  ins_pipe(ialu_cr_reg_imm);\n+%}\n+\n+instruct compI_rReg_mem(rFlagsReg cr, rRegI op1, memory op2)\n+%{\n+  match(Set cr (CmpI op1 (LoadI op2)));\n+\n+  ins_cost(500); \/\/ XXX\n+  format %{ \"cmpl    $op1, $op2\" %}\n+  ins_encode %{\n+    __ cmpl($op1$$Register, $op2$$Address);\n+  %}\n+  ins_pipe(ialu_cr_reg_mem);\n+%}\n+\n+instruct testI_reg(rFlagsReg cr, rRegI src, immI_0 zero)\n+%{\n+  match(Set cr (CmpI src zero));\n+\n+  format %{ \"testl   $src, $src\" %}\n+  ins_encode %{\n+    __ testl($src$$Register, $src$$Register);\n+  %}\n+  ins_pipe(ialu_cr_reg_imm);\n+%}\n+\n+instruct testI_reg_imm(rFlagsReg cr, rRegI src, immI con, immI_0 zero)\n+%{\n+  match(Set cr (CmpI (AndI src con) zero));\n+\n+  format %{ \"testl   $src, $con\" %}\n+  ins_encode %{\n+    __ testl($src$$Register, $con$$constant);\n+  %}\n+  ins_pipe(ialu_cr_reg_imm);\n+%}\n+\n+instruct testI_reg_reg(rFlagsReg cr, rRegI src1, rRegI src2, immI_0 zero)\n+%{\n+  match(Set cr (CmpI (AndI src1 src2) zero));\n+\n+  format %{ \"testl   $src1, $src2\" %}\n+  ins_encode %{\n+    __ testl($src1$$Register, $src2$$Register);\n+  %}\n+  ins_pipe(ialu_cr_reg_imm);\n+%}\n+\n+instruct testI_reg_mem(rFlagsReg cr, rRegI src, memory mem, immI_0 zero)\n+%{\n+  match(Set cr (CmpI (AndI src (LoadI mem)) zero));\n+\n+  format %{ \"testl   $src, $mem\" %}\n+  ins_encode %{\n+    __ testl($src$$Register, $mem$$Address);\n+  %}\n+  ins_pipe(ialu_cr_reg_mem);\n+%}\n+\n+\/\/ Unsigned compare Instructions; really, same as signed except they\n+\/\/ produce an rFlagsRegU instead of rFlagsReg.\n+instruct compU_rReg(rFlagsRegU cr, rRegI op1, rRegI op2)\n+%{\n+  match(Set cr (CmpU op1 op2));\n+\n+  format %{ \"cmpl    $op1, $op2\\t# unsigned\" %}\n+  ins_encode %{\n+    __ cmpl($op1$$Register, $op2$$Register);\n+  %}\n+  ins_pipe(ialu_cr_reg_reg);\n+%}\n+\n+instruct compU_rReg_imm(rFlagsRegU cr, rRegI op1, immI op2)\n+%{\n+  match(Set cr (CmpU op1 op2));\n+\n+  format %{ \"cmpl    $op1, $op2\\t# unsigned\" %}\n+  ins_encode %{\n+    __ cmpl($op1$$Register, $op2$$constant);\n+  %}\n+  ins_pipe(ialu_cr_reg_imm);\n+%}\n+\n+instruct compU_rReg_mem(rFlagsRegU cr, rRegI op1, memory op2)\n+%{\n+  match(Set cr (CmpU op1 (LoadI op2)));\n+\n+  ins_cost(500); \/\/ XXX\n+  format %{ \"cmpl    $op1, $op2\\t# unsigned\" %}\n+  ins_encode %{\n+    __ cmpl($op1$$Register, $op2$$Address);\n+  %}\n+  ins_pipe(ialu_cr_reg_mem);\n+%}\n+\n+instruct testU_reg(rFlagsRegU cr, rRegI src, immI_0 zero)\n+%{\n+  match(Set cr (CmpU src zero));\n+\n+  format %{ \"testl   $src, $src\\t# unsigned\" %}\n+  ins_encode %{\n+    __ testl($src$$Register, $src$$Register);\n+  %}\n+  ins_pipe(ialu_cr_reg_imm);\n+%}\n+\n+instruct compP_rReg(rFlagsRegU cr, rRegP op1, rRegP op2)\n+%{\n+  match(Set cr (CmpP op1 op2));\n+\n+  format %{ \"cmpq    $op1, $op2\\t# ptr\" %}\n+  ins_encode %{\n+    __ cmpq($op1$$Register, $op2$$Register);\n+  %}\n+  ins_pipe(ialu_cr_reg_reg);\n+%}\n+\n+instruct compP_rReg_mem(rFlagsRegU cr, rRegP op1, memory op2)\n+%{\n+  match(Set cr (CmpP op1 (LoadP op2)));\n+  predicate(n->in(2)->as_Load()->barrier_data() == 0);\n+\n+  ins_cost(500); \/\/ XXX\n+  format %{ \"cmpq    $op1, $op2\\t# ptr\" %}\n+  ins_encode %{\n+    __ cmpq($op1$$Register, $op2$$Address);\n+  %}\n+  ins_pipe(ialu_cr_reg_mem);\n+%}\n+\n+\/\/ XXX this is generalized by compP_rReg_mem???\n+\/\/ Compare raw pointer (used in out-of-heap check).\n+\/\/ Only works because non-oop pointers must be raw pointers\n+\/\/ and raw pointers have no anti-dependencies.\n+instruct compP_mem_rReg(rFlagsRegU cr, rRegP op1, memory op2)\n+%{\n+  predicate(n->in(2)->in(2)->bottom_type()->reloc() == relocInfo::none &&\n+            n->in(2)->as_Load()->barrier_data() == 0);\n+  match(Set cr (CmpP op1 (LoadP op2)));\n+\n+  format %{ \"cmpq    $op1, $op2\\t# raw ptr\" %}\n+  ins_encode %{\n+    __ cmpq($op1$$Register, $op2$$Address);\n+  %}\n+  ins_pipe(ialu_cr_reg_mem);\n+%}\n+\n+\/\/ This will generate a signed flags result. This should be OK since\n+\/\/ any compare to a zero should be eq\/neq.\n+instruct testP_reg(rFlagsReg cr, rRegP src, immP0 zero)\n+%{\n+  match(Set cr (CmpP src zero));\n+\n+  format %{ \"testq   $src, $src\\t# ptr\" %}\n+  ins_encode %{\n+    __ testq($src$$Register, $src$$Register);\n+  %}\n+  ins_pipe(ialu_cr_reg_imm);\n+%}\n+\n+\/\/ This will generate a signed flags result. This should be OK since\n+\/\/ any compare to a zero should be eq\/neq.\n+instruct testP_mem(rFlagsReg cr, memory op, immP0 zero)\n+%{\n+  predicate((!UseCompressedOops || (CompressedOops::base() != nullptr)) &&\n+            n->in(1)->as_Load()->barrier_data() == 0);\n+  match(Set cr (CmpP (LoadP op) zero));\n+\n+  ins_cost(500); \/\/ XXX\n+  format %{ \"testq   $op, 0xffffffffffffffff\\t# ptr\" %}\n+  ins_encode %{\n+    __ testq($op$$Address, 0xFFFFFFFF);\n+  %}\n+  ins_pipe(ialu_cr_reg_imm);\n+%}\n+\n+instruct testP_mem_reg0(rFlagsReg cr, memory mem, immP0 zero)\n+%{\n+  predicate(UseCompressedOops && (CompressedOops::base() == nullptr) &&\n+            n->in(1)->as_Load()->barrier_data() == 0);\n+  match(Set cr (CmpP (LoadP mem) zero));\n+\n+  format %{ \"cmpq    R12, $mem\\t# ptr (R12_heapbase==0)\" %}\n+  ins_encode %{\n+    __ cmpq(r12, $mem$$Address);\n+  %}\n+  ins_pipe(ialu_cr_reg_mem);\n+%}\n+\n+instruct compN_rReg(rFlagsRegU cr, rRegN op1, rRegN op2)\n+%{\n+  match(Set cr (CmpN op1 op2));\n+\n+  format %{ \"cmpl    $op1, $op2\\t# compressed ptr\" %}\n+  ins_encode %{ __ cmpl($op1$$Register, $op2$$Register); %}\n+  ins_pipe(ialu_cr_reg_reg);\n+%}\n+\n+instruct compN_rReg_mem(rFlagsRegU cr, rRegN src, memory mem)\n+%{\n+  predicate(n->in(2)->as_Load()->barrier_data() == 0);\n+  match(Set cr (CmpN src (LoadN mem)));\n+\n+  format %{ \"cmpl    $src, $mem\\t# compressed ptr\" %}\n+  ins_encode %{\n+    __ cmpl($src$$Register, $mem$$Address);\n+  %}\n+  ins_pipe(ialu_cr_reg_mem);\n+%}\n+\n+instruct compN_rReg_imm(rFlagsRegU cr, rRegN op1, immN op2) %{\n+  match(Set cr (CmpN op1 op2));\n+\n+  format %{ \"cmpl    $op1, $op2\\t# compressed ptr\" %}\n+  ins_encode %{\n+    __ cmp_narrow_oop($op1$$Register, (jobject)$op2$$constant);\n+  %}\n+  ins_pipe(ialu_cr_reg_imm);\n+%}\n+\n+instruct compN_mem_imm(rFlagsRegU cr, memory mem, immN src)\n+%{\n+  predicate(n->in(2)->as_Load()->barrier_data() == 0);\n+  match(Set cr (CmpN src (LoadN mem)));\n+\n+  format %{ \"cmpl    $mem, $src\\t# compressed ptr\" %}\n+  ins_encode %{\n+    __ cmp_narrow_oop($mem$$Address, (jobject)$src$$constant);\n+  %}\n+  ins_pipe(ialu_cr_reg_mem);\n+%}\n+\n+instruct compN_rReg_imm_klass(rFlagsRegU cr, rRegN op1, immNKlass op2) %{\n+  match(Set cr (CmpN op1 op2));\n+\n+  format %{ \"cmpl    $op1, $op2\\t# compressed klass ptr\" %}\n+  ins_encode %{\n+    __ cmp_narrow_klass($op1$$Register, (Klass*)$op2$$constant);\n+  %}\n+  ins_pipe(ialu_cr_reg_imm);\n+%}\n@@ -1917,10 +16503,4 @@\n-bool Matcher::match_rule_supported_vector_masked(int opcode, int vlen, BasicType bt) {\n-  \/\/ ADLC based match_rule_supported routine checks for the existence of pattern based\n-  \/\/ on IR opcode. Most of the unary\/binary\/ternary masked operation share the IR nodes\n-  \/\/ of their non-masked counterpart with mask edge being the differentiator.\n-  \/\/ This routine does a strict check on the existence of masked operation patterns\n-  \/\/ by returning a default false value for all the other opcodes apart from the\n-  \/\/ ones whose masked instruction patterns are defined in this file.\n-  if (!match_rule_supported_vector(opcode, vlen, bt)) {\n-    return false;\n-  }\n+instruct compN_mem_imm_klass(rFlagsRegU cr, memory mem, immNKlass src)\n+%{\n+  predicate(!UseCompactObjectHeaders);\n+  match(Set cr (CmpN src (LoadNKlass mem)));\n@@ -1928,14 +16508,6 @@\n-  int size_in_bits = vlen * type2aelembytes(bt) * BitsPerByte;\n-  if (size_in_bits != 512 && !VM_Version::supports_avx512vl()) {\n-    return false;\n-  }\n-  switch(opcode) {\n-    \/\/ Unary masked operations\n-    case Op_AbsVB:\n-    case Op_AbsVS:\n-      if(!VM_Version::supports_avx512bw()) {\n-        return false;  \/\/ Implementation limitation\n-      }\n-    case Op_AbsVI:\n-    case Op_AbsVL:\n-      return true;\n+  format %{ \"cmpl    $mem, $src\\t# compressed klass ptr\" %}\n+  ins_encode %{\n+    __ cmp_narrow_klass($mem$$Address, (Klass*)$src$$constant);\n+  %}\n+  ins_pipe(ialu_cr_reg_mem);\n+%}\n@@ -1943,4 +16515,2 @@\n-    \/\/ Ternary masked operations\n-    case Op_FmaVF:\n-    case Op_FmaVD:\n-      return true;\n+instruct testN_reg(rFlagsReg cr, rRegN src, immN0 zero) %{\n+  match(Set cr (CmpN src zero));\n@@ -1948,5 +16518,4 @@\n-    case Op_MacroLogicV:\n-      if(bt != T_INT && bt != T_LONG) {\n-        return false;\n-      }\n-      return true;\n+  format %{ \"testl   $src, $src\\t# compressed ptr\" %}\n+  ins_encode %{ __ testl($src$$Register, $src$$Register); %}\n+  ins_pipe(ialu_cr_reg_imm);\n+%}\n@@ -1954,14 +16523,5 @@\n-    \/\/ Binary masked operations\n-    case Op_AddVB:\n-    case Op_AddVS:\n-    case Op_SubVB:\n-    case Op_SubVS:\n-    case Op_MulVS:\n-    case Op_LShiftVS:\n-    case Op_RShiftVS:\n-    case Op_URShiftVS:\n-      assert(size_in_bits == 512 || VM_Version::supports_avx512vl(), \"\");\n-      if (!VM_Version::supports_avx512bw()) {\n-        return false;  \/\/ Implementation limitation\n-      }\n-      return true;\n+instruct testN_mem(rFlagsReg cr, memory mem, immN0 zero)\n+%{\n+  predicate(CompressedOops::base() != nullptr &&\n+            n->in(1)->as_Load()->barrier_data() == 0);\n+  match(Set cr (CmpN (LoadN mem) zero));\n@@ -1969,6 +16529,7 @@\n-    case Op_MulVL:\n-      assert(size_in_bits == 512 || VM_Version::supports_avx512vl(), \"\");\n-      if (!VM_Version::supports_avx512dq()) {\n-        return false;  \/\/ Implementation limitation\n-      }\n-      return true;\n+  ins_cost(500); \/\/ XXX\n+  format %{ \"testl   $mem, 0xffffffff\\t# compressed ptr\" %}\n+  ins_encode %{\n+    __ cmpl($mem$$Address, (int)0xFFFFFFFF);\n+  %}\n+  ins_pipe(ialu_cr_reg_mem);\n+%}\n@@ -1976,9 +16537,5 @@\n-    case Op_AndV:\n-    case Op_OrV:\n-    case Op_XorV:\n-    case Op_RotateRightV:\n-    case Op_RotateLeftV:\n-      if (bt != T_INT && bt != T_LONG) {\n-        return false; \/\/ Implementation limitation\n-      }\n-      return true;\n+instruct testN_mem_reg0(rFlagsReg cr, memory mem, immN0 zero)\n+%{\n+  predicate(CompressedOops::base() == nullptr &&\n+            n->in(1)->as_Load()->barrier_data() == 0);\n+  match(Set cr (CmpN (LoadN mem) zero));\n@@ -1986,6 +16543,6 @@\n-    case Op_VectorLoadMask:\n-      assert(size_in_bits == 512 || VM_Version::supports_avx512vl(), \"\");\n-      if (is_subword_type(bt) && !VM_Version::supports_avx512bw()) {\n-        return false;\n-      }\n-      return true;\n+  format %{ \"cmpl    R12, $mem\\t# compressed ptr (R12_heapbase==0)\" %}\n+  ins_encode %{\n+    __ cmpl(r12, $mem$$Address);\n+  %}\n+  ins_pipe(ialu_cr_reg_mem);\n+%}\n@@ -1993,26 +16550,2 @@\n-    case Op_AddVI:\n-    case Op_AddVL:\n-    case Op_AddVF:\n-    case Op_AddVD:\n-    case Op_SubVI:\n-    case Op_SubVL:\n-    case Op_SubVF:\n-    case Op_SubVD:\n-    case Op_MulVI:\n-    case Op_MulVF:\n-    case Op_MulVD:\n-    case Op_DivVF:\n-    case Op_DivVD:\n-    case Op_SqrtVF:\n-    case Op_SqrtVD:\n-    case Op_LShiftVI:\n-    case Op_LShiftVL:\n-    case Op_RShiftVI:\n-    case Op_RShiftVL:\n-    case Op_URShiftVI:\n-    case Op_URShiftVL:\n-    case Op_LoadVectorMasked:\n-    case Op_StoreVectorMasked:\n-    case Op_LoadVectorGatherMasked:\n-    case Op_StoreVectorScatterMasked:\n-      return true;\n+\/\/ Yanked all unsigned pointer compare operations.\n+\/\/ Pointer compares are done with CmpP which is already unsigned.\n@@ -2020,23 +16553,3 @@\n-    case Op_UMinV:\n-    case Op_UMaxV:\n-      if (size_in_bits < 512 && !VM_Version::supports_avx512vl()) {\n-        return false;\n-      } \/\/ fallthrough\n-    case Op_MaxV:\n-    case Op_MinV:\n-      if (is_subword_type(bt) && !VM_Version::supports_avx512bw()) {\n-        return false; \/\/ Implementation limitation\n-      }\n-      if (is_floating_point_type(bt) && !VM_Version::supports_avx10_2()) {\n-        return false; \/\/ Implementation limitation\n-      }\n-      return true;\n-    case Op_SaturatingAddV:\n-    case Op_SaturatingSubV:\n-      if (!is_subword_type(bt)) {\n-        return false;\n-      }\n-      if (size_in_bits < 128 || !VM_Version::supports_avx512bw()) {\n-        return false; \/\/ Implementation limitation\n-      }\n-      return true;\n+instruct compL_rReg(rFlagsReg cr, rRegL op1, rRegL op2)\n+%{\n+  match(Set cr (CmpL op1 op2));\n@@ -2044,5 +16557,6 @@\n-    case Op_VectorMaskCmp:\n-      if (is_subword_type(bt) && !VM_Version::supports_avx512bw()) {\n-        return false; \/\/ Implementation limitation\n-      }\n-      return true;\n+  format %{ \"cmpq    $op1, $op2\" %}\n+  ins_encode %{\n+    __ cmpq($op1$$Register, $op2$$Register);\n+  %}\n+  ins_pipe(ialu_cr_reg_reg);\n+%}\n@@ -2050,10 +16564,3 @@\n-    case Op_VectorRearrange:\n-      if (bt == T_SHORT && !VM_Version::supports_avx512bw()) {\n-        return false; \/\/ Implementation limitation\n-      }\n-      if (bt == T_BYTE && !VM_Version::supports_avx512_vbmi()) {\n-        return false; \/\/ Implementation limitation\n-      } else if ((bt == T_INT || bt == T_FLOAT) && size_in_bits < 256) {\n-        return false; \/\/ Implementation limitation\n-      }\n-      return true;\n+instruct compL_rReg_imm(rFlagsReg cr, rRegL op1, immL32 op2)\n+%{\n+  match(Set cr (CmpL op1 op2));\n@@ -2061,8 +16568,6 @@\n-    \/\/ Binary Logical operations\n-    case Op_AndVMask:\n-    case Op_OrVMask:\n-    case Op_XorVMask:\n-      if (vlen > 16 && !VM_Version::supports_avx512bw()) {\n-        return false; \/\/ Implementation limitation\n-      }\n-      return true;\n+  format %{ \"cmpq    $op1, $op2\" %}\n+  ins_encode %{\n+    __ cmpq($op1$$Register, $op2$$constant);\n+  %}\n+  ins_pipe(ialu_cr_reg_imm);\n+%}\n@@ -2070,6 +16575,3 @@\n-    case Op_PopCountVI:\n-    case Op_PopCountVL:\n-      if (!is_pop_count_instr_target(bt)) {\n-        return false;\n-      }\n-      return true;\n+instruct compL_rReg_mem(rFlagsReg cr, rRegL op1, memory op2)\n+%{\n+  match(Set cr (CmpL op1 (LoadL op2)));\n@@ -2077,2 +16579,6 @@\n-    case Op_MaskAll:\n-      return true;\n+  format %{ \"cmpq    $op1, $op2\" %}\n+  ins_encode %{\n+    __ cmpq($op1$$Register, $op2$$Address);\n+  %}\n+  ins_pipe(ialu_cr_reg_mem);\n+%}\n@@ -2080,8 +16586,3 @@\n-    case Op_CountLeadingZerosV:\n-      if (is_non_subword_integral_type(bt) && VM_Version::supports_avx512cd()) {\n-        return true;\n-      }\n-    default:\n-      return false;\n-  }\n-}\n+instruct testL_reg(rFlagsReg cr, rRegL src, immL0 zero)\n+%{\n+  match(Set cr (CmpL src zero));\n@@ -2089,3 +16590,6 @@\n-bool Matcher::vector_needs_partial_operations(Node* node, const TypeVect* vt) {\n-  return false;\n-}\n+  format %{ \"testq   $src, $src\" %}\n+  ins_encode %{\n+    __ testq($src$$Register, $src$$Register);\n+  %}\n+  ins_pipe(ialu_cr_reg_imm);\n+%}\n@@ -2093,12 +16597,3 @@\n-\/\/ Return true if Vector::rearrange needs preparation of the shuffle argument\n-bool Matcher::vector_rearrange_requires_load_shuffle(BasicType elem_bt, int vlen) {\n-  switch (elem_bt) {\n-    case T_BYTE:  return false;\n-    case T_SHORT: return !VM_Version::supports_avx512bw();\n-    case T_INT:   return !VM_Version::supports_avx();\n-    case T_LONG:  return vlen < 8 && !VM_Version::supports_avx512vl();\n-    default:\n-      ShouldNotReachHere();\n-      return false;\n-  }\n-}\n+instruct testL_reg_imm(rFlagsReg cr, rRegL src, immL32 con, immL0 zero)\n+%{\n+  match(Set cr (CmpL (AndL src con) zero));\n@@ -2106,28 +16601,63 @@\n-MachOper* Matcher::pd_specialize_generic_vector_operand(MachOper* generic_opnd, uint ideal_reg, bool is_temp) {\n-  assert(Matcher::is_generic_vector(generic_opnd), \"not generic\");\n-  bool legacy = (generic_opnd->opcode() == LEGVEC);\n-  if (!VM_Version::supports_avx512vlbwdq() && \/\/ KNL\n-      is_temp && !legacy && (ideal_reg == Op_VecZ)) {\n-    \/\/ Conservatively specialize 512bit vec TEMP operands to legVecZ (zmm0-15) on KNL.\n-    return new legVecZOper();\n-  }\n-  if (legacy) {\n-    switch (ideal_reg) {\n-      case Op_VecS: return new legVecSOper();\n-      case Op_VecD: return new legVecDOper();\n-      case Op_VecX: return new legVecXOper();\n-      case Op_VecY: return new legVecYOper();\n-      case Op_VecZ: return new legVecZOper();\n-    }\n-  } else {\n-    switch (ideal_reg) {\n-      case Op_VecS: return new vecSOper();\n-      case Op_VecD: return new vecDOper();\n-      case Op_VecX: return new vecXOper();\n-      case Op_VecY: return new vecYOper();\n-      case Op_VecZ: return new vecZOper();\n-    }\n-  }\n-  ShouldNotReachHere();\n-  return nullptr;\n-}\n+  format %{ \"testq   $src, $con\\t# long\" %}\n+  ins_encode %{\n+    __ testq($src$$Register, $con$$constant);\n+  %}\n+  ins_pipe(ialu_cr_reg_imm);\n+%}\n+\n+instruct testL_reg_reg(rFlagsReg cr, rRegL src1, rRegL src2, immL0 zero)\n+%{\n+  match(Set cr (CmpL (AndL src1 src2) zero));\n+\n+  format %{ \"testq   $src1, $src2\\t# long\" %}\n+  ins_encode %{\n+    __ testq($src1$$Register, $src2$$Register);\n+  %}\n+  ins_pipe(ialu_cr_reg_imm);\n+%}\n+\n+instruct testL_reg_mem(rFlagsReg cr, rRegL src, memory mem, immL0 zero)\n+%{\n+  match(Set cr (CmpL (AndL src (LoadL mem)) zero));\n+\n+  format %{ \"testq   $src, $mem\" %}\n+  ins_encode %{\n+    __ testq($src$$Register, $mem$$Address);\n+  %}\n+  ins_pipe(ialu_cr_reg_mem);\n+%}\n+\n+instruct testL_reg_mem2(rFlagsReg cr, rRegP src, memory mem, immL0 zero)\n+%{\n+  match(Set cr (CmpL (AndL (CastP2X src) (LoadL mem)) zero));\n+\n+  format %{ \"testq   $src, $mem\" %}\n+  ins_encode %{\n+    __ testq($src$$Register, $mem$$Address);\n+  %}\n+  ins_pipe(ialu_cr_reg_mem);\n+%}\n+\n+\/\/ Manifest a CmpU result in an integer register.  Very painful.\n+\/\/ This is the test to avoid.\n+instruct cmpU3_reg_reg(rRegI dst, rRegI src1, rRegI src2, rFlagsReg flags)\n+%{\n+  match(Set dst (CmpU3 src1 src2));\n+  effect(KILL flags);\n+\n+  ins_cost(275); \/\/ XXX\n+  format %{ \"cmpl    $src1, $src2\\t# CmpL3\\n\\t\"\n+            \"movl    $dst, -1\\n\\t\"\n+            \"jb,u    done\\n\\t\"\n+            \"setcc   $dst \\t# emits setne + movzbl or setzune for APX\"\n+    \"done:\" %}\n+  ins_encode %{\n+    Label done;\n+    __ cmpl($src1$$Register, $src2$$Register);\n+    __ movl($dst$$Register, -1);\n+    __ jccb(Assembler::below, done);\n+    __ setcc(Assembler::notZero, $dst$$Register);\n+    __ bind(done);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n@@ -2135,17 +16665,23 @@\n-bool Matcher::is_reg2reg_move(MachNode* m) {\n-  switch (m->rule()) {\n-    case MoveVec2Leg_rule:\n-    case MoveLeg2Vec_rule:\n-    case MoveF2VL_rule:\n-    case MoveF2LEG_rule:\n-    case MoveVL2F_rule:\n-    case MoveLEG2F_rule:\n-    case MoveD2VL_rule:\n-    case MoveD2LEG_rule:\n-    case MoveVL2D_rule:\n-    case MoveLEG2D_rule:\n-      return true;\n-    default:\n-      return false;\n-  }\n-}\n+\/\/ Manifest a CmpL result in an integer register.  Very painful.\n+\/\/ This is the test to avoid.\n+instruct cmpL3_reg_reg(rRegI dst, rRegL src1, rRegL src2, rFlagsReg flags)\n+%{\n+  match(Set dst (CmpL3 src1 src2));\n+  effect(KILL flags);\n+\n+  ins_cost(275); \/\/ XXX\n+  format %{ \"cmpq    $src1, $src2\\t# CmpL3\\n\\t\"\n+            \"movl    $dst, -1\\n\\t\"\n+            \"jl,s    done\\n\\t\"\n+            \"setcc   $dst \\t# emits setne + movzbl or setzune for APX\"\n+    \"done:\" %}\n+  ins_encode %{\n+    Label done;\n+    __ cmpq($src1$$Register, $src2$$Register);\n+    __ movl($dst$$Register, -1);\n+    __ jccb(Assembler::less, done);\n+    __ setcc(Assembler::notZero, $dst$$Register);\n+    __ bind(done);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n@@ -2153,9 +16689,23 @@\n-bool Matcher::is_generic_vector(MachOper* opnd) {\n-  switch (opnd->opcode()) {\n-    case VEC:\n-    case LEGVEC:\n-      return true;\n-    default:\n-      return false;\n-  }\n-}\n+\/\/ Manifest a CmpUL result in an integer register.  Very painful.\n+\/\/ This is the test to avoid.\n+instruct cmpUL3_reg_reg(rRegI dst, rRegL src1, rRegL src2, rFlagsReg flags)\n+%{\n+  match(Set dst (CmpUL3 src1 src2));\n+  effect(KILL flags);\n+\n+  ins_cost(275); \/\/ XXX\n+  format %{ \"cmpq    $src1, $src2\\t# CmpL3\\n\\t\"\n+            \"movl    $dst, -1\\n\\t\"\n+            \"jb,u    done\\n\\t\"\n+            \"setcc   $dst \\t# emits setne + movzbl or setzune for APX\"\n+    \"done:\" %}\n+  ins_encode %{\n+    Label done;\n+    __ cmpq($src1$$Register, $src2$$Register);\n+    __ movl($dst$$Register, -1);\n+    __ jccb(Assembler::below, done);\n+    __ setcc(Assembler::notZero, $dst$$Register);\n+    __ bind(done);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n@@ -2163,1 +16713,5 @@\n-\/\/------------------------------------------------------------------------\n+\/\/ Unsigned long compare Instructions; really, same as signed long except they\n+\/\/ produce an rFlagsRegU instead of rFlagsReg.\n+instruct compUL_rReg(rFlagsRegU cr, rRegL op1, rRegL op2)\n+%{\n+  match(Set cr (CmpUL op1 op2));\n@@ -2165,3 +16719,6 @@\n-const RegMask* Matcher::predicate_reg_mask(void) {\n-  return &_VECTMASK_REG_mask;\n-}\n+  format %{ \"cmpq    $op1, $op2\\t# unsigned\" %}\n+  ins_encode %{\n+    __ cmpq($op1$$Register, $op2$$Register);\n+  %}\n+  ins_pipe(ialu_cr_reg_reg);\n+%}\n@@ -2169,41 +16726,3 @@\n-\/\/ Max vector size in bytes. 0 if not supported.\n-int Matcher::vector_width_in_bytes(BasicType bt) {\n-  assert(is_java_primitive(bt), \"only primitive type vectors\");\n-  \/\/ SSE2 supports 128bit vectors for all types.\n-  \/\/ AVX2 supports 256bit vectors for all types.\n-  \/\/ AVX2\/EVEX supports 512bit vectors for all types.\n-  int size = (UseAVX > 1) ? (1 << UseAVX) * 8 : 16;\n-  \/\/ AVX1 supports 256bit vectors only for FLOAT and DOUBLE.\n-  if (UseAVX > 0 && (bt == T_FLOAT || bt == T_DOUBLE))\n-    size = (UseAVX > 2) ? 64 : 32;\n-  if (UseAVX > 2 && (bt == T_BYTE || bt == T_SHORT || bt == T_CHAR))\n-    size = (VM_Version::supports_avx512bw()) ? 64 : 32;\n-  \/\/ Use flag to limit vector size.\n-  size = MIN2(size,(int)MaxVectorSize);\n-  \/\/ Minimum 2 values in vector (or 4 for bytes).\n-  switch (bt) {\n-  case T_DOUBLE:\n-  case T_LONG:\n-    if (size < 16) return 0;\n-    break;\n-  case T_FLOAT:\n-  case T_INT:\n-    if (size < 8) return 0;\n-    break;\n-  case T_BOOLEAN:\n-    if (size < 4) return 0;\n-    break;\n-  case T_CHAR:\n-    if (size < 4) return 0;\n-    break;\n-  case T_BYTE:\n-    if (size < 4) return 0;\n-    break;\n-  case T_SHORT:\n-    if (size < 4) return 0;\n-    break;\n-  default:\n-    ShouldNotReachHere();\n-  }\n-  return size;\n-}\n+instruct compUL_rReg_imm(rFlagsRegU cr, rRegL op1, immL32 op2)\n+%{\n+  match(Set cr (CmpUL op1 op2));\n@@ -2211,14 +16730,6 @@\n-\/\/ Limits on vector size (number of elements) loaded into vector.\n-int Matcher::max_vector_size(const BasicType bt) {\n-  return vector_width_in_bytes(bt)\/type2aelembytes(bt);\n-}\n-int Matcher::min_vector_size(const BasicType bt) {\n-  int max_size = max_vector_size(bt);\n-  \/\/ Min size which can be loaded into vector is 4 bytes.\n-  int size = (type2aelembytes(bt) == 1) ? 4 : 2;\n-  \/\/ Support for calling svml double64 vectors\n-  if (bt == T_DOUBLE) {\n-    size = 1;\n-  }\n-  return MIN2(size,max_size);\n-}\n+  format %{ \"cmpq    $op1, $op2\\t# unsigned\" %}\n+  ins_encode %{\n+    __ cmpq($op1$$Register, $op2$$constant);\n+  %}\n+  ins_pipe(ialu_cr_reg_imm);\n+%}\n@@ -2226,8 +16737,3 @@\n-int Matcher::max_vector_size_auto_vectorization(const BasicType bt) {\n-  \/\/ Limit the max vector size for auto vectorization to 256 bits (32 bytes)\n-  \/\/ by default on Cascade Lake\n-  if (VM_Version::is_default_intel_cascade_lake()) {\n-    return MIN2(Matcher::max_vector_size(bt), 32 \/ type2aelembytes(bt));\n-  }\n-  return Matcher::max_vector_size(bt);\n-}\n+instruct compUL_rReg_mem(rFlagsRegU cr, rRegL op1, memory op2)\n+%{\n+  match(Set cr (CmpUL op1 (LoadL op2)));\n@@ -2235,3 +16741,6 @@\n-int Matcher::scalable_vector_reg_size(const BasicType bt) {\n-  return -1;\n-}\n+  format %{ \"cmpq    $op1, $op2\\t# unsigned\" %}\n+  ins_encode %{\n+    __ cmpq($op1$$Register, $op2$$Address);\n+  %}\n+  ins_pipe(ialu_cr_reg_mem);\n+%}\n@@ -2239,13 +16748,3 @@\n-\/\/ Vector ideal reg corresponding to specified size in bytes\n-uint Matcher::vector_ideal_reg(int size) {\n-  assert(MaxVectorSize >= size, \"\");\n-  switch(size) {\n-    case  4: return Op_VecS;\n-    case  8: return Op_VecD;\n-    case 16: return Op_VecX;\n-    case 32: return Op_VecY;\n-    case 64: return Op_VecZ;\n-  }\n-  ShouldNotReachHere();\n-  return 0;\n-}\n+instruct testUL_reg(rFlagsRegU cr, rRegL src, immL0 zero)\n+%{\n+  match(Set cr (CmpUL src zero));\n@@ -2253,25 +16752,6 @@\n-\/\/ Check for shift by small constant as well\n-static bool clone_shift(Node* shift, Matcher* matcher, Matcher::MStack& mstack, VectorSet& address_visited) {\n-  if (shift->Opcode() == Op_LShiftX && shift->in(2)->is_Con() &&\n-      shift->in(2)->get_int() <= 3 &&\n-      \/\/ Are there other uses besides address expressions?\n-      !matcher->is_visited(shift)) {\n-    address_visited.set(shift->_idx); \/\/ Flag as address_visited\n-    mstack.push(shift->in(2), Matcher::Visit);\n-    Node *conv = shift->in(1);\n-    \/\/ Allow Matcher to match the rule which bypass\n-    \/\/ ConvI2L operation for an array index on LP64\n-    \/\/ if the index value is positive.\n-    if (conv->Opcode() == Op_ConvI2L &&\n-        conv->as_Type()->type()->is_long()->_lo >= 0 &&\n-        \/\/ Are there other uses besides address expressions?\n-        !matcher->is_visited(conv)) {\n-      address_visited.set(conv->_idx); \/\/ Flag as address_visited\n-      mstack.push(conv->in(1), Matcher::Pre_Visit);\n-    } else {\n-      mstack.push(conv, Matcher::Pre_Visit);\n-    }\n-    return true;\n-  }\n-  return false;\n-}\n+  format %{ \"testq   $src, $src\\t# unsigned\" %}\n+  ins_encode %{\n+    __ testq($src$$Register, $src$$Register);\n+  %}\n+  ins_pipe(ialu_cr_reg_imm);\n+%}\n@@ -2279,14 +16759,3 @@\n-\/\/ This function identifies sub-graphs in which a 'load' node is\n-\/\/ input to two different nodes, and such that it can be matched\n-\/\/ with BMI instructions like blsi, blsr, etc.\n-\/\/ Example : for b = -a[i] & a[i] can be matched to blsi r32, m32.\n-\/\/ The graph is (AndL (SubL Con0 LoadL*) LoadL*), where LoadL*\n-\/\/ refers to the same node.\n-\/\/\n-\/\/ Match the generic fused operations pattern (op1 (op2 Con{ConType} mop) mop)\n-\/\/ This is a temporary solution until we make DAGs expressible in ADL.\n-template<typename ConType>\n-class FusedPatternMatcher {\n-  Node* _op1_node;\n-  Node* _mop_node;\n-  int _con_op;\n+instruct compB_mem_imm(rFlagsReg cr, memory mem, immI8 imm)\n+%{\n+  match(Set cr (CmpI (LoadB mem) imm));\n@@ -2294,4 +16763,5 @@\n-  static int match_next(Node* n, int next_op, int next_op_idx) {\n-    if (n->in(1) == nullptr || n->in(2) == nullptr) {\n-      return -1;\n-    }\n+  ins_cost(125);\n+  format %{ \"cmpb    $mem, $imm\" %}\n+  ins_encode %{ __ cmpb($mem$$Address, $imm$$constant); %}\n+  ins_pipe(ialu_cr_reg_mem);\n+%}\n@@ -2299,14 +16769,3 @@\n-    if (next_op_idx == -1) { \/\/ n is commutative, try rotations\n-      if (n->in(1)->Opcode() == next_op) {\n-        return 1;\n-      } else if (n->in(2)->Opcode() == next_op) {\n-        return 2;\n-      }\n-    } else {\n-      assert(next_op_idx > 0 && next_op_idx <= 2, \"Bad argument index\");\n-      if (n->in(next_op_idx)->Opcode() == next_op) {\n-        return next_op_idx;\n-      }\n-    }\n-    return -1;\n-  }\n+instruct testUB_mem_imm(rFlagsReg cr, memory mem, immU7 imm, immI_0 zero)\n+%{\n+  match(Set cr (CmpI (AndI (LoadUB mem) imm) zero));\n@@ -2314,3 +16773,5 @@\n- public:\n-  FusedPatternMatcher(Node* op1_node, Node* mop_node, int con_op) :\n-    _op1_node(op1_node), _mop_node(mop_node), _con_op(con_op) { }\n+  ins_cost(125);\n+  format %{ \"testb   $mem, $imm\\t# ubyte\" %}\n+  ins_encode %{ __ testb($mem$$Address, $imm$$constant); %}\n+  ins_pipe(ialu_cr_reg_mem);\n+%}\n@@ -2318,15 +16779,3 @@\n-  bool match(int op1, int op1_op2_idx,  \/\/ op1 and the index of the op1->op2 edge, -1 if op1 is commutative\n-             int op2, int op2_con_idx,  \/\/ op2 and the index of the op2->con edge, -1 if op2 is commutative\n-             typename ConType::NativeType con_value) {\n-    if (_op1_node->Opcode() != op1) {\n-      return false;\n-    }\n-    if (_mop_node->outcnt() > 2) {\n-      return false;\n-    }\n-    op1_op2_idx = match_next(_op1_node, op2, op1_op2_idx);\n-    if (op1_op2_idx == -1) {\n-      return false;\n-    }\n-    \/\/ Memory operation must be the other edge\n-    int op1_mop_idx = (op1_op2_idx & 1) + 1;\n+instruct testB_mem_imm(rFlagsReg cr, memory mem, immI8 imm, immI_0 zero)\n+%{\n+  match(Set cr (CmpI (AndI (LoadB mem) imm) zero));\n@@ -2334,25 +16783,37 @@\n-    \/\/ Check that the mop node is really what we want\n-    if (_op1_node->in(op1_mop_idx) == _mop_node) {\n-      Node* op2_node = _op1_node->in(op1_op2_idx);\n-      if (op2_node->outcnt() > 1) {\n-        return false;\n-      }\n-      assert(op2_node->Opcode() == op2, \"Should be\");\n-      op2_con_idx = match_next(op2_node, _con_op, op2_con_idx);\n-      if (op2_con_idx == -1) {\n-        return false;\n-      }\n-      \/\/ Memory operation must be the other edge\n-      int op2_mop_idx = (op2_con_idx & 1) + 1;\n-      \/\/ Check that the memory operation is the same node\n-      if (op2_node->in(op2_mop_idx) == _mop_node) {\n-        \/\/ Now check the constant\n-        const Type* con_type = op2_node->in(op2_con_idx)->bottom_type();\n-        if (con_type != Type::TOP && ConType::as_self(con_type)->get_con() == con_value) {\n-          return true;\n-        }\n-      }\n-    }\n-    return false;\n-  }\n-};\n+  ins_cost(125);\n+  format %{ \"testb   $mem, $imm\\t# byte\" %}\n+  ins_encode %{ __ testb($mem$$Address, $imm$$constant); %}\n+  ins_pipe(ialu_cr_reg_mem);\n+%}\n+\n+\/\/----------Max and Min--------------------------------------------------------\n+\/\/ Min Instructions\n+\n+instruct cmovI_reg_g(rRegI dst, rRegI src, rFlagsReg cr)\n+%{\n+  predicate(!UseAPX);\n+  effect(USE_DEF dst, USE src, USE cr);\n+\n+  format %{ \"cmovlgt $dst, $src\\t# min\" %}\n+  ins_encode %{\n+    __ cmovl(Assembler::greater, $dst$$Register, $src$$Register);\n+  %}\n+  ins_pipe(pipe_cmov_reg);\n+%}\n+\n+instruct cmovI_reg_g_ndd(rRegI dst, rRegI src1, rRegI src2, rFlagsReg cr)\n+%{\n+  predicate(UseAPX);\n+  effect(DEF dst, USE src1, USE src2, USE cr);\n+\n+  format %{ \"ecmovlgt $dst, $src1, $src2\\t# min ndd\" %}\n+  ins_encode %{\n+    __ ecmovl(Assembler::greater, $dst$$Register, $src1$$Register, $src2$$Register);\n+  %}\n+  ins_pipe(pipe_cmov_reg);\n+%}\n+\n+instruct minI_rReg(rRegI dst, rRegI src)\n+%{\n+  predicate(!UseAPX);\n+  match(Set dst (MinI dst src));\n@@ -2360,17 +16821,7 @@\n-static bool is_bmi_pattern(Node* n, Node* m) {\n-  assert(UseBMI1Instructions, \"sanity\");\n-  if (n != nullptr && m != nullptr) {\n-    if (m->Opcode() == Op_LoadI) {\n-      FusedPatternMatcher<TypeInt> bmii(n, m, Op_ConI);\n-      return bmii.match(Op_AndI, -1, Op_SubI,  1,  0)  ||\n-             bmii.match(Op_AndI, -1, Op_AddI, -1, -1)  ||\n-             bmii.match(Op_XorI, -1, Op_AddI, -1, -1);\n-    } else if (m->Opcode() == Op_LoadL) {\n-      FusedPatternMatcher<TypeLong> bmil(n, m, Op_ConL);\n-      return bmil.match(Op_AndL, -1, Op_SubL,  1,  0) ||\n-             bmil.match(Op_AndL, -1, Op_AddL, -1, -1) ||\n-             bmil.match(Op_XorL, -1, Op_AddL, -1, -1);\n-    }\n-  }\n-  return false;\n-}\n+  ins_cost(200);\n+  expand %{\n+    rFlagsReg cr;\n+    compI_rReg(cr, dst, src);\n+    cmovI_reg_g(dst, src, cr);\n+  %}\n+%}\n@@ -2378,17 +16829,5 @@\n-\/\/ Should the matcher clone input 'm' of node 'n'?\n-bool Matcher::pd_clone_node(Node* n, Node* m, Matcher::MStack& mstack) {\n-  \/\/ If 'n' and 'm' are part of a graph for BMI instruction, clone the input 'm'.\n-  if (UseBMI1Instructions && is_bmi_pattern(n, m)) {\n-    mstack.push(m, Visit);\n-    return true;\n-  }\n-  if (is_vshift_con_pattern(n, m)) { \/\/ ShiftV src (ShiftCntV con)\n-    mstack.push(m, Visit);           \/\/ m = ShiftCntV\n-    return true;\n-  }\n-  if (is_encode_and_store_pattern(n, m)) {\n-    mstack.push(m, Visit);\n-    return true;\n-  }\n-  return false;\n-}\n+instruct minI_rReg_ndd(rRegI dst, rRegI src1, rRegI src2)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (MinI src1 src2));\n+  effect(DEF dst, USE src1, USE src2);\n@@ -2396,8 +16835,7 @@\n-\/\/ Should the Matcher clone shifts on addressing modes, expecting them\n-\/\/ to be subsumed into complex addressing expressions or compute them\n-\/\/ into registers?\n-bool Matcher::pd_clone_address_expressions(AddPNode* m, Matcher::MStack& mstack, VectorSet& address_visited) {\n-  Node *off = m->in(AddPNode::Offset);\n-  if (off->is_Con()) {\n-    address_visited.test_set(m->_idx); \/\/ Flag as address_visited\n-    Node *adr = m->in(AddPNode::Address);\n+  ins_cost(200);\n+  expand %{\n+    rFlagsReg cr;\n+    compI_rReg(cr, src1, src2);\n+    cmovI_reg_g_ndd(dst, src1, src2, cr);\n+  %}\n+%}\n@@ -2405,19 +16843,4 @@\n-    \/\/ Intel can handle 2 adds in addressing mode, with one of them using an immediate offset.\n-    \/\/ AtomicAdd is not an addressing expression.\n-    \/\/ Cheap to find it by looking for screwy base.\n-    if (adr->is_AddP() &&\n-        !adr->in(AddPNode::Base)->is_top() &&\n-        !adr->in(AddPNode::Offset)->is_Con() &&\n-        off->get_long() == (int) (off->get_long()) && \/\/ immL32\n-        \/\/ Are there other uses besides address expressions?\n-        !is_visited(adr)) {\n-      address_visited.set(adr->_idx); \/\/ Flag as address_visited\n-      Node *shift = adr->in(AddPNode::Offset);\n-      if (!clone_shift(shift, this, mstack, address_visited)) {\n-        mstack.push(shift, Pre_Visit);\n-      }\n-      mstack.push(adr->in(AddPNode::Address), Pre_Visit);\n-      mstack.push(adr->in(AddPNode::Base), Pre_Visit);\n-    } else {\n-      mstack.push(adr, Pre_Visit);\n-    }\n+instruct cmovI_reg_l(rRegI dst, rRegI src, rFlagsReg cr)\n+%{\n+  predicate(!UseAPX);\n+  effect(USE_DEF dst, USE src, USE cr);\n@@ -2425,12 +16848,6 @@\n-    \/\/ Clone X+offset as it also folds into most addressing expressions\n-    mstack.push(off, Visit);\n-    mstack.push(m->in(AddPNode::Base), Pre_Visit);\n-    return true;\n-  } else if (clone_shift(off, this, mstack, address_visited)) {\n-    address_visited.test_set(m->_idx); \/\/ Flag as address_visited\n-    mstack.push(m->in(AddPNode::Address), Pre_Visit);\n-    mstack.push(m->in(AddPNode::Base), Pre_Visit);\n-    return true;\n-  }\n-  return false;\n-}\n+  format %{ \"cmovllt $dst, $src\\t# max\" %}\n+  ins_encode %{\n+    __ cmovl(Assembler::less, $dst$$Register, $src$$Register);\n+  %}\n+  ins_pipe(pipe_cmov_reg);\n+%}\n@@ -2438,21 +16855,4 @@\n-static inline Assembler::ComparisonPredicate booltest_pred_to_comparison_pred(int bt) {\n-  switch (bt) {\n-    case BoolTest::eq:\n-      return Assembler::eq;\n-    case BoolTest::ne:\n-      return Assembler::neq;\n-    case BoolTest::le:\n-    case BoolTest::ule:\n-      return Assembler::le;\n-    case BoolTest::ge:\n-    case BoolTest::uge:\n-      return Assembler::nlt;\n-    case BoolTest::lt:\n-    case BoolTest::ult:\n-      return Assembler::lt;\n-    case BoolTest::gt:\n-    case BoolTest::ugt:\n-      return Assembler::nle;\n-    default : ShouldNotReachHere(); return Assembler::_false;\n-  }\n-}\n+instruct cmovI_reg_l_ndd(rRegI dst, rRegI src1, rRegI src2, rFlagsReg cr)\n+%{\n+  predicate(UseAPX);\n+  effect(DEF dst, USE src1, USE src2, USE cr);\n@@ -2460,12 +16860,6 @@\n-static inline Assembler::ComparisonPredicateFP booltest_pred_to_comparison_pred_fp(int bt) {\n-  switch (bt) {\n-  case BoolTest::eq: return Assembler::EQ_OQ;  \/\/ ordered non-signaling\n-  \/\/ As per JLS 15.21.1, != of NaNs is true. Thus use unordered compare.\n-  case BoolTest::ne: return Assembler::NEQ_UQ; \/\/ unordered non-signaling\n-  case BoolTest::le: return Assembler::LE_OQ;  \/\/ ordered non-signaling\n-  case BoolTest::ge: return Assembler::GE_OQ;  \/\/ ordered non-signaling\n-  case BoolTest::lt: return Assembler::LT_OQ;  \/\/ ordered non-signaling\n-  case BoolTest::gt: return Assembler::GT_OQ;  \/\/ ordered non-signaling\n-  default: ShouldNotReachHere(); return Assembler::FALSE_OS;\n-  }\n-}\n+  format %{ \"ecmovllt $dst, $src1, $src2\\t# max ndd\" %}\n+  ins_encode %{\n+    __ ecmovl(Assembler::less, $dst$$Register, $src1$$Register, $src2$$Register);\n+  %}\n+  ins_pipe(pipe_cmov_reg);\n+%}\n@@ -2473,49 +16867,4 @@\n-\/\/ Helper methods for MachSpillCopyNode::implementation().\n-static void vec_mov_helper(C2_MacroAssembler *masm, int src_lo, int dst_lo,\n-                          int src_hi, int dst_hi, uint ireg, outputStream* st) {\n-  assert(ireg == Op_VecS || \/\/ 32bit vector\n-         ((src_lo & 1) == 0 && (src_lo + 1) == src_hi &&\n-          (dst_lo & 1) == 0 && (dst_lo + 1) == dst_hi),\n-         \"no non-adjacent vector moves\" );\n-  if (masm) {\n-    switch (ireg) {\n-    case Op_VecS: \/\/ copy whole register\n-    case Op_VecD:\n-    case Op_VecX:\n-      if ((UseAVX < 3) || VM_Version::supports_avx512vl()) {\n-        __ movdqu(as_XMMRegister(Matcher::_regEncode[dst_lo]), as_XMMRegister(Matcher::_regEncode[src_lo]));\n-      } else {\n-        __ vextractf32x4(as_XMMRegister(Matcher::_regEncode[dst_lo]), as_XMMRegister(Matcher::_regEncode[src_lo]), 0x0);\n-     }\n-      break;\n-    case Op_VecY:\n-      if ((UseAVX < 3) || VM_Version::supports_avx512vl()) {\n-        __ vmovdqu(as_XMMRegister(Matcher::_regEncode[dst_lo]), as_XMMRegister(Matcher::_regEncode[src_lo]));\n-      } else {\n-        __ vextractf64x4(as_XMMRegister(Matcher::_regEncode[dst_lo]), as_XMMRegister(Matcher::_regEncode[src_lo]), 0x0);\n-     }\n-      break;\n-    case Op_VecZ:\n-      __ evmovdquq(as_XMMRegister(Matcher::_regEncode[dst_lo]), as_XMMRegister(Matcher::_regEncode[src_lo]), 2);\n-      break;\n-    default:\n-      ShouldNotReachHere();\n-    }\n-#ifndef PRODUCT\n-  } else {\n-    switch (ireg) {\n-    case Op_VecS:\n-    case Op_VecD:\n-    case Op_VecX:\n-      st->print(\"movdqu  %s,%s\\t# spill\",Matcher::regName[dst_lo],Matcher::regName[src_lo]);\n-      break;\n-    case Op_VecY:\n-    case Op_VecZ:\n-      st->print(\"vmovdqu %s,%s\\t# spill\",Matcher::regName[dst_lo],Matcher::regName[src_lo]);\n-      break;\n-    default:\n-      ShouldNotReachHere();\n-    }\n-#endif\n-  }\n-}\n+instruct maxI_rReg(rRegI dst, rRegI src)\n+%{\n+  predicate(!UseAPX);\n+  match(Set dst (MaxI dst src));\n@@ -2523,63 +16872,115 @@\n-void vec_spill_helper(C2_MacroAssembler *masm, bool is_load,\n-                     int stack_offset, int reg, uint ireg, outputStream* st) {\n-  if (masm) {\n-    if (is_load) {\n-      switch (ireg) {\n-      case Op_VecS:\n-        __ movdl(as_XMMRegister(Matcher::_regEncode[reg]), Address(rsp, stack_offset));\n-        break;\n-      case Op_VecD:\n-        __ movq(as_XMMRegister(Matcher::_regEncode[reg]), Address(rsp, stack_offset));\n-        break;\n-      case Op_VecX:\n-        if ((UseAVX < 3) || VM_Version::supports_avx512vl()) {\n-          __ movdqu(as_XMMRegister(Matcher::_regEncode[reg]), Address(rsp, stack_offset));\n-        } else {\n-          __ vpxor(as_XMMRegister(Matcher::_regEncode[reg]), as_XMMRegister(Matcher::_regEncode[reg]), as_XMMRegister(Matcher::_regEncode[reg]), 2);\n-          __ vinsertf32x4(as_XMMRegister(Matcher::_regEncode[reg]), as_XMMRegister(Matcher::_regEncode[reg]), Address(rsp, stack_offset),0x0);\n-        }\n-        break;\n-      case Op_VecY:\n-        if ((UseAVX < 3) || VM_Version::supports_avx512vl()) {\n-          __ vmovdqu(as_XMMRegister(Matcher::_regEncode[reg]), Address(rsp, stack_offset));\n-        } else {\n-          __ vpxor(as_XMMRegister(Matcher::_regEncode[reg]), as_XMMRegister(Matcher::_regEncode[reg]), as_XMMRegister(Matcher::_regEncode[reg]), 2);\n-          __ vinsertf64x4(as_XMMRegister(Matcher::_regEncode[reg]), as_XMMRegister(Matcher::_regEncode[reg]), Address(rsp, stack_offset),0x0);\n-        }\n-        break;\n-      case Op_VecZ:\n-        __ evmovdquq(as_XMMRegister(Matcher::_regEncode[reg]), Address(rsp, stack_offset), 2);\n-        break;\n-      default:\n-        ShouldNotReachHere();\n-      }\n-    } else { \/\/ store\n-      switch (ireg) {\n-      case Op_VecS:\n-        __ movdl(Address(rsp, stack_offset), as_XMMRegister(Matcher::_regEncode[reg]));\n-        break;\n-      case Op_VecD:\n-        __ movq(Address(rsp, stack_offset), as_XMMRegister(Matcher::_regEncode[reg]));\n-        break;\n-      case Op_VecX:\n-        if ((UseAVX < 3) || VM_Version::supports_avx512vl()) {\n-          __ movdqu(Address(rsp, stack_offset), as_XMMRegister(Matcher::_regEncode[reg]));\n-        }\n-        else {\n-          __ vextractf32x4(Address(rsp, stack_offset), as_XMMRegister(Matcher::_regEncode[reg]), 0x0);\n-        }\n-        break;\n-      case Op_VecY:\n-        if ((UseAVX < 3) || VM_Version::supports_avx512vl()) {\n-          __ vmovdqu(Address(rsp, stack_offset), as_XMMRegister(Matcher::_regEncode[reg]));\n-        }\n-        else {\n-          __ vextractf64x4(Address(rsp, stack_offset), as_XMMRegister(Matcher::_regEncode[reg]), 0x0);\n-        }\n-        break;\n-      case Op_VecZ:\n-        __ evmovdquq(Address(rsp, stack_offset), as_XMMRegister(Matcher::_regEncode[reg]), 2);\n-        break;\n-      default:\n-        ShouldNotReachHere();\n-      }\n+  ins_cost(200);\n+  expand %{\n+    rFlagsReg cr;\n+    compI_rReg(cr, dst, src);\n+    cmovI_reg_l(dst, src, cr);\n+  %}\n+%}\n+\n+instruct maxI_rReg_ndd(rRegI dst, rRegI src1, rRegI src2)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (MaxI src1 src2));\n+  effect(DEF dst, USE src1, USE src2);\n+\n+  ins_cost(200);\n+  expand %{\n+    rFlagsReg cr;\n+    compI_rReg(cr, src1, src2);\n+    cmovI_reg_l_ndd(dst, src1, src2, cr);\n+  %}\n+%}\n+\n+\/\/ ============================================================================\n+\/\/ Branch Instructions\n+\n+\/\/ Jump Direct - Label defines a relative address from JMP+1\n+instruct jmpDir(label labl)\n+%{\n+  match(Goto);\n+  effect(USE labl);\n+\n+  ins_cost(300);\n+  format %{ \"jmp     $labl\" %}\n+  size(5);\n+  ins_encode %{\n+    Label* L = $labl$$label;\n+    __ jmp(*L, false); \/\/ Always long jump\n+  %}\n+  ins_pipe(pipe_jmp);\n+%}\n+\n+\/\/ Jump Direct Conditional - Label defines a relative address from Jcc+1\n+instruct jmpCon(cmpOp cop, rFlagsReg cr, label labl)\n+%{\n+  match(If cop cr);\n+  effect(USE labl);\n+\n+  ins_cost(300);\n+  format %{ \"j$cop     $labl\" %}\n+  size(6);\n+  ins_encode %{\n+    Label* L = $labl$$label;\n+    __ jcc((Assembler::Condition)($cop$$cmpcode), *L, false); \/\/ Always long jump\n+  %}\n+  ins_pipe(pipe_jcc);\n+%}\n+\n+\/\/ Jump Direct Conditional - Label defines a relative address from Jcc+1\n+instruct jmpLoopEnd(cmpOp cop, rFlagsReg cr, label labl)\n+%{\n+  match(CountedLoopEnd cop cr);\n+  effect(USE labl);\n+\n+  ins_cost(300);\n+  format %{ \"j$cop     $labl\\t# loop end\" %}\n+  size(6);\n+  ins_encode %{\n+    Label* L = $labl$$label;\n+    __ jcc((Assembler::Condition)($cop$$cmpcode), *L, false); \/\/ Always long jump\n+  %}\n+  ins_pipe(pipe_jcc);\n+%}\n+\n+\/\/ Jump Direct Conditional - using unsigned comparison\n+instruct jmpConU(cmpOpU cop, rFlagsRegU cmp, label labl) %{\n+  match(If cop cmp);\n+  effect(USE labl);\n+\n+  ins_cost(300);\n+  format %{ \"j$cop,u   $labl\" %}\n+  size(6);\n+  ins_encode %{\n+    Label* L = $labl$$label;\n+    __ jcc((Assembler::Condition)($cop$$cmpcode), *L, false); \/\/ Always long jump\n+  %}\n+  ins_pipe(pipe_jcc);\n+%}\n+\n+instruct jmpConUCF(cmpOpUCF cop, rFlagsRegUCF cmp, label labl) %{\n+  match(If cop cmp);\n+  effect(USE labl);\n+\n+  ins_cost(200);\n+  format %{ \"j$cop,u   $labl\" %}\n+  size(6);\n+  ins_encode %{\n+    Label* L = $labl$$label;\n+    __ jcc((Assembler::Condition)($cop$$cmpcode), *L, false); \/\/ Always long jump\n+  %}\n+  ins_pipe(pipe_jcc);\n+%}\n+\n+instruct jmpConUCF2(cmpOpUCF2 cop, rFlagsRegUCF cmp, label labl) %{\n+  match(If cop cmp);\n+  effect(USE labl);\n+\n+  ins_cost(200);\n+  format %{ $$template\n+    if ($cop$$cmpcode == Assembler::notEqual) {\n+      $$emit$$\"jp,u    $labl\\n\\t\"\n+      $$emit$$\"j$cop,u   $labl\"\n+    } else {\n+      $$emit$$\"jp,u    done\\n\\t\"\n+      $$emit$$\"j$cop,u   $labl\\n\\t\"\n+      $$emit$$\"done:\"\n@@ -2587,38 +16988,13 @@\n-#ifndef PRODUCT\n-  } else {\n-    if (is_load) {\n-      switch (ireg) {\n-      case Op_VecS:\n-        st->print(\"movd    %s,[rsp + %d]\\t# spill\", Matcher::regName[reg], stack_offset);\n-        break;\n-      case Op_VecD:\n-        st->print(\"movq    %s,[rsp + %d]\\t# spill\", Matcher::regName[reg], stack_offset);\n-        break;\n-       case Op_VecX:\n-        st->print(\"movdqu  %s,[rsp + %d]\\t# spill\", Matcher::regName[reg], stack_offset);\n-        break;\n-      case Op_VecY:\n-      case Op_VecZ:\n-        st->print(\"vmovdqu %s,[rsp + %d]\\t# spill\", Matcher::regName[reg], stack_offset);\n-        break;\n-      default:\n-        ShouldNotReachHere();\n-      }\n-    } else { \/\/ store\n-      switch (ireg) {\n-      case Op_VecS:\n-        st->print(\"movd    [rsp + %d],%s\\t# spill\", stack_offset, Matcher::regName[reg]);\n-        break;\n-      case Op_VecD:\n-        st->print(\"movq    [rsp + %d],%s\\t# spill\", stack_offset, Matcher::regName[reg]);\n-        break;\n-       case Op_VecX:\n-        st->print(\"movdqu  [rsp + %d],%s\\t# spill\", stack_offset, Matcher::regName[reg]);\n-        break;\n-      case Op_VecY:\n-      case Op_VecZ:\n-        st->print(\"vmovdqu [rsp + %d],%s\\t# spill\", stack_offset, Matcher::regName[reg]);\n-        break;\n-      default:\n-        ShouldNotReachHere();\n-      }\n+  %}\n+  ins_encode %{\n+    Label* l = $labl$$label;\n+    if ($cop$$cmpcode == Assembler::notEqual) {\n+      __ jcc(Assembler::parity, *l, false);\n+      __ jcc(Assembler::notEqual, *l, false);\n+    } else if ($cop$$cmpcode == Assembler::equal) {\n+      Label done;\n+      __ jccb(Assembler::parity, done);\n+      __ jcc(Assembler::equal, *l, false);\n+      __ bind(done);\n+    } else {\n+       ShouldNotReachHere();\n@@ -2626,3 +17002,3 @@\n-#endif\n-  }\n-}\n+  %}\n+  ins_pipe(pipe_jcc);\n+%}\n@@ -2630,34 +17006,87 @@\n-template <class T>\n-static inline GrowableArray<jbyte>* vreplicate_imm(BasicType bt, T con, int len) {\n-  int size = type2aelembytes(bt) * len;\n-  GrowableArray<jbyte>* val = new GrowableArray<jbyte>(size, size, 0);\n-  for (int i = 0; i < len; i++) {\n-    int offset = i * type2aelembytes(bt);\n-    switch (bt) {\n-      case T_BYTE: val->at(i) = con; break;\n-      case T_SHORT: {\n-        jshort c = con;\n-        memcpy(val->adr_at(offset), &c, sizeof(jshort));\n-        break;\n-      }\n-      case T_INT: {\n-        jint c = con;\n-        memcpy(val->adr_at(offset), &c, sizeof(jint));\n-        break;\n-      }\n-      case T_LONG: {\n-        jlong c = con;\n-        memcpy(val->adr_at(offset), &c, sizeof(jlong));\n-        break;\n-      }\n-      case T_FLOAT: {\n-        jfloat c = con;\n-        memcpy(val->adr_at(offset), &c, sizeof(jfloat));\n-        break;\n-      }\n-      case T_DOUBLE: {\n-        jdouble c = con;\n-        memcpy(val->adr_at(offset), &c, sizeof(jdouble));\n-        break;\n-      }\n-      default: assert(false, \"%s\", type2name(bt));\n+\/\/ ============================================================================\n+\/\/ The 2nd slow-half of a subtype check.  Scan the subklass's 2ndary\n+\/\/ superklass array for an instance of the superklass.  Set a hidden\n+\/\/ internal cache on a hit (cache is checked with exposed code in\n+\/\/ gen_subtype_check()).  Return NZ for a miss or zero for a hit.  The\n+\/\/ encoding ALSO sets flags.\n+\n+instruct partialSubtypeCheck(rdi_RegP result,\n+                             rsi_RegP sub, rax_RegP super, rcx_RegI rcx,\n+                             rFlagsReg cr)\n+%{\n+  match(Set result (PartialSubtypeCheck sub super));\n+  predicate(!UseSecondarySupersTable);\n+  effect(KILL rcx, KILL cr);\n+\n+  ins_cost(1100);  \/\/ slightly larger than the next version\n+  format %{ \"movq    rdi, [$sub + in_bytes(Klass::secondary_supers_offset())]\\n\\t\"\n+            \"movl    rcx, [rdi + Array<Klass*>::length_offset_in_bytes()]\\t# length to scan\\n\\t\"\n+            \"addq    rdi, Array<Klass*>::base_offset_in_bytes()\\t# Skip to start of data; set NZ in case count is zero\\n\\t\"\n+            \"repne   scasq\\t# Scan *rdi++ for a match with rax while rcx--\\n\\t\"\n+            \"jne,s   miss\\t\\t# Missed: rdi not-zero\\n\\t\"\n+            \"movq    [$sub + in_bytes(Klass::secondary_super_cache_offset())], $super\\t# Hit: update cache\\n\\t\"\n+            \"xorq    $result, $result\\t\\t Hit: rdi zero\\n\\t\"\n+    \"miss:\\t\" %}\n+\n+  ins_encode %{\n+    Label miss;\n+    \/\/ NB: Callers may assume that, when $result is a valid register,\n+    \/\/ check_klass_subtype_slow_path_linear sets it to a nonzero\n+    \/\/ value.\n+    __ check_klass_subtype_slow_path_linear($sub$$Register, $super$$Register,\n+                                            $rcx$$Register, $result$$Register,\n+                                            nullptr, &miss,\n+                                            \/*set_cond_codes:*\/ true);\n+    __ xorptr($result$$Register, $result$$Register);\n+    __ bind(miss);\n+  %}\n+\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ ============================================================================\n+\/\/ Two versions of hashtable-based partialSubtypeCheck, both used when\n+\/\/ we need to search for a super class in the secondary supers array.\n+\/\/ The first is used when we don't know _a priori_ the class being\n+\/\/ searched for. The second, far more common, is used when we do know:\n+\/\/ this is used for instanceof, checkcast, and any case where C2 can\n+\/\/ determine it by constant propagation.\n+\n+instruct partialSubtypeCheckVarSuper(rsi_RegP sub, rax_RegP super, rdi_RegP result,\n+                                       rdx_RegL temp1, rcx_RegL temp2, rbx_RegP temp3, r11_RegL temp4,\n+                                       rFlagsReg cr)\n+%{\n+  match(Set result (PartialSubtypeCheck sub super));\n+  predicate(UseSecondarySupersTable);\n+  effect(KILL cr, TEMP temp1, TEMP temp2, TEMP temp3, TEMP temp4);\n+\n+  ins_cost(1000);\n+  format %{ \"partialSubtypeCheck $result, $sub, $super\" %}\n+\n+  ins_encode %{\n+    __ lookup_secondary_supers_table_var($sub$$Register, $super$$Register, $temp1$$Register, $temp2$$Register,\n+\t\t\t\t\t $temp3$$Register, $temp4$$Register, $result$$Register);\n+  %}\n+\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct partialSubtypeCheckConstSuper(rsi_RegP sub, rax_RegP super_reg, immP super_con, rdi_RegP result,\n+                                       rdx_RegL temp1, rcx_RegL temp2, rbx_RegP temp3, r11_RegL temp4,\n+                                       rFlagsReg cr)\n+%{\n+  match(Set result (PartialSubtypeCheck sub (Binary super_reg super_con)));\n+  predicate(UseSecondarySupersTable);\n+  effect(KILL cr, TEMP temp1, TEMP temp2, TEMP temp3, TEMP temp4);\n+\n+  ins_cost(700);  \/\/ smaller than the next version\n+  format %{ \"partialSubtypeCheck $result, $sub, $super_reg, $super_con\" %}\n+\n+  ins_encode %{\n+    u1 super_klass_slot = ((Klass*)$super_con$$constant)->hash_slot();\n+    if (InlineSecondarySupersTest) {\n+      __ lookup_secondary_supers_table_const($sub$$Register, $super_reg$$Register, $temp1$$Register, $temp2$$Register,\n+                                       $temp3$$Register, $temp4$$Register, $result$$Register,\n+                                       super_klass_slot);\n+    } else {\n+      __ call(RuntimeAddress(StubRoutines::lookup_secondary_supers_table_stub(super_klass_slot)));\n@@ -2665,3 +17094,1 @@\n-  }\n-  return val;\n-}\n+  %}\n@@ -2669,11 +17096,2 @@\n-static inline jlong high_bit_set(BasicType bt) {\n-  switch (bt) {\n-    case T_BYTE:  return 0x8080808080808080;\n-    case T_SHORT: return 0x8000800080008000;\n-    case T_INT:   return 0x8000000080000000;\n-    case T_LONG:  return 0x8000000000000000;\n-    default:\n-      ShouldNotReachHere();\n-      return 0;\n-  }\n-}\n+  ins_pipe(pipe_slow);\n+%}\n@@ -2681,5 +17099,11 @@\n-#ifndef PRODUCT\n-  void MachNopNode::format(PhaseRegAlloc*, outputStream* st) const {\n-    st->print(\"nop \\t# %d bytes pad for loops and calls\", _count);\n-  }\n-#endif\n+\/\/ ============================================================================\n+\/\/ Branch Instructions -- short offset versions\n+\/\/\n+\/\/ These instructions are used to replace jumps of a long offset (the default\n+\/\/ match) with jumps of a shorter offset.  These instructions are all tagged\n+\/\/ with the ins_short_branch attribute, which causes the ADLC to suppress the\n+\/\/ match rules in general matching.  Instead, the ADLC generates a conversion\n+\/\/ method in the MachNode which can be used to do in-place replacement of the\n+\/\/ long variant with the shorter variant.  The compiler will determine if a\n+\/\/ branch can be taken by the is_short_branch_offset() predicate in the machine\n+\/\/ specific code section of the file.\n@@ -2687,3 +17111,4 @@\n-  void MachNopNode::emit(C2_MacroAssembler *masm, PhaseRegAlloc*) const {\n-    __ nop(_count);\n-  }\n+\/\/ Jump Direct - Label defines a relative address from JMP+1\n+instruct jmpDir_short(label labl) %{\n+  match(Goto);\n+  effect(USE labl);\n@@ -2691,3 +17116,10 @@\n-  uint MachNopNode::size(PhaseRegAlloc*) const {\n-    return _count;\n-  }\n+  ins_cost(300);\n+  format %{ \"jmp,s   $labl\" %}\n+  size(2);\n+  ins_encode %{\n+    Label* L = $labl$$label;\n+    __ jmpb(*L);\n+  %}\n+  ins_pipe(pipe_jmp);\n+  ins_short_branch(1);\n+%}\n@@ -2695,5 +17127,4 @@\n-#ifndef PRODUCT\n-  void MachBreakpointNode::format(PhaseRegAlloc*, outputStream* st) const {\n-    st->print(\"# breakpoint\");\n-  }\n-#endif\n+\/\/ Jump Direct Conditional - Label defines a relative address from Jcc+1\n+instruct jmpCon_short(cmpOp cop, rFlagsReg cr, label labl) %{\n+  match(If cop cr);\n+  effect(USE labl);\n@@ -2701,3 +17132,10 @@\n-  void MachBreakpointNode::emit(C2_MacroAssembler *masm, PhaseRegAlloc* ra_) const {\n-    __ int3();\n-  }\n+  ins_cost(300);\n+  format %{ \"j$cop,s   $labl\" %}\n+  size(2);\n+  ins_encode %{\n+    Label* L = $labl$$label;\n+    __ jccb((Assembler::Condition)($cop$$cmpcode), *L);\n+  %}\n+  ins_pipe(pipe_jcc);\n+  ins_short_branch(1);\n+%}\n@@ -2705,3 +17143,4 @@\n-  uint MachBreakpointNode::size(PhaseRegAlloc* ra_) const {\n-    return MachNode::size(ra_);\n-  }\n+\/\/ Jump Direct Conditional - Label defines a relative address from Jcc+1\n+instruct jmpLoopEnd_short(cmpOp cop, rFlagsReg cr, label labl) %{\n+  match(CountedLoopEnd cop cr);\n+  effect(USE labl);\n@@ -2709,0 +17148,9 @@\n+  ins_cost(300);\n+  format %{ \"j$cop,s   $labl\\t# loop end\" %}\n+  size(2);\n+  ins_encode %{\n+    Label* L = $labl$$label;\n+    __ jccb((Assembler::Condition)($cop$$cmpcode), *L);\n+  %}\n+  ins_pipe(pipe_jcc);\n+  ins_short_branch(1);\n@@ -2711,1 +17159,4 @@\n-encode %{\n+\/\/ Jump Direct Conditional - using unsigned comparison\n+instruct jmpConU_short(cmpOpU cop, rFlagsRegU cmp, label labl) %{\n+  match(If cop cmp);\n+  effect(USE labl);\n@@ -2713,10 +17164,39 @@\n-  enc_class call_epilog %{\n-    if (VerifyStackAtCalls) {\n-      \/\/ Check that stack depth is unchanged: find majik cookie on stack\n-      int framesize = ra_->reg2offset_unchecked(OptoReg::add(ra_->_matcher._old_SP, -3*VMRegImpl::slots_per_word));\n-      Label L;\n-      __ cmpptr(Address(rsp, framesize), (int32_t)0xbadb100d);\n-      __ jccb(Assembler::equal, L);\n-      \/\/ Die if stack mismatch\n-      __ int3();\n-      __ bind(L);\n+  ins_cost(300);\n+  format %{ \"j$cop,us  $labl\" %}\n+  size(2);\n+  ins_encode %{\n+    Label* L = $labl$$label;\n+    __ jccb((Assembler::Condition)($cop$$cmpcode), *L);\n+  %}\n+  ins_pipe(pipe_jcc);\n+  ins_short_branch(1);\n+%}\n+\n+instruct jmpConUCF_short(cmpOpUCF cop, rFlagsRegUCF cmp, label labl) %{\n+  match(If cop cmp);\n+  effect(USE labl);\n+\n+  ins_cost(300);\n+  format %{ \"j$cop,us  $labl\" %}\n+  size(2);\n+  ins_encode %{\n+    Label* L = $labl$$label;\n+    __ jccb((Assembler::Condition)($cop$$cmpcode), *L);\n+  %}\n+  ins_pipe(pipe_jcc);\n+  ins_short_branch(1);\n+%}\n+\n+instruct jmpConUCF2_short(cmpOpUCF2 cop, rFlagsRegUCF cmp, label labl) %{\n+  match(If cop cmp);\n+  effect(USE labl);\n+\n+  ins_cost(300);\n+  format %{ $$template\n+    if ($cop$$cmpcode == Assembler::notEqual) {\n+      $$emit$$\"jp,u,s  $labl\\n\\t\"\n+      $$emit$$\"j$cop,u,s  $labl\"\n+    } else {\n+      $$emit$$\"jp,u,s  done\\n\\t\"\n+      $$emit$$\"j$cop,u,s  $labl\\n\\t\"\n+      $$emit$$\"done:\"\n@@ -2724,32 +17204,14 @@\n-    if (tf()->returns_inline_type_as_fields() && !_method->is_method_handle_intrinsic() && _method->return_type()->is_loaded()) {\n-      \/\/ The last return value is not set by the callee but used to pass the null marker to compiled code.\n-      \/\/ Search for the corresponding projection, get the register and emit code that initialized it.\n-      uint con = (tf()->range_cc()->cnt() - 1);\n-      for (DUIterator_Fast imax, i = fast_outs(imax); i < imax; i++) {\n-        ProjNode* proj = fast_out(i)->as_Proj();\n-        if (proj->_con == con) {\n-          \/\/ Set null marker if rax is non-null (a non-null value is returned buffered or scalarized)\n-          OptoReg::Name optoReg = ra_->get_reg_first(proj);\n-          VMReg reg = OptoReg::as_VMReg(optoReg, ra_->_framesize, OptoReg::reg2stack(ra_->_matcher._new_SP));\n-          Register toReg = reg->is_reg() ? reg->as_Register() : rscratch1;\n-          __ testq(rax, rax);\n-          __ setb(Assembler::notZero, toReg);\n-          __ movzbl(toReg, toReg);\n-          if (reg->is_stack()) {\n-            int st_off = reg->reg2stack() * VMRegImpl::stack_slot_size;\n-            __ movq(Address(rsp, st_off), toReg);\n-          }\n-          break;\n-        }\n-      }\n-      if (return_value_is_used()) {\n-        \/\/ An inline type is returned as fields in multiple registers.\n-        \/\/ Rax either contains an oop if the inline type is buffered or a pointer\n-        \/\/ to the corresponding InlineKlass with the lowest bit set to 1. Zero rax\n-        \/\/ if the lowest bit is set to allow C2 to use the oop after null checking.\n-        \/\/ rax &= (rax & 1) - 1\n-        __ movptr(rscratch1, rax);\n-        __ andptr(rscratch1, 0x1);\n-        __ subptr(rscratch1, 0x1);\n-        __ andptr(rax, rscratch1);\n-      }\n+  %}\n+  size(4);\n+  ins_encode %{\n+    Label* l = $labl$$label;\n+    if ($cop$$cmpcode == Assembler::notEqual) {\n+      __ jccb(Assembler::parity, *l);\n+      __ jccb(Assembler::notEqual, *l);\n+    } else if ($cop$$cmpcode == Assembler::equal) {\n+      Label done;\n+      __ jccb(Assembler::parity, done);\n+      __ jccb(Assembler::equal, *l);\n+      __ bind(done);\n+    } else {\n+       ShouldNotReachHere();\n@@ -2758,1 +17220,57 @@\n-\n+  ins_pipe(pipe_jcc);\n+  ins_short_branch(1);\n+%}\n+\n+\/\/ ============================================================================\n+\/\/ inlined locking and unlocking\n+\n+instruct cmpFastLockLightweight(rFlagsReg cr, rRegP object, rbx_RegP box, rax_RegI rax_reg, rRegP tmp) %{\n+  match(Set cr (FastLock object box));\n+  effect(TEMP rax_reg, TEMP tmp, USE_KILL box);\n+  ins_cost(300);\n+  format %{ \"fastlock $object,$box\\t! kills $box,$rax_reg,$tmp\" %}\n+  ins_encode %{\n+    __ fast_lock_lightweight($object$$Register, $box$$Register, $rax_reg$$Register, $tmp$$Register, r15_thread);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct cmpFastUnlockLightweight(rFlagsReg cr, rRegP object, rax_RegP rax_reg, rRegP tmp) %{\n+  match(Set cr (FastUnlock object rax_reg));\n+  effect(TEMP tmp, USE_KILL rax_reg);\n+  ins_cost(300);\n+  format %{ \"fastunlock $object,$rax_reg\\t! kills $rax_reg,$tmp\" %}\n+  ins_encode %{\n+    __ fast_unlock_lightweight($object$$Register, $rax_reg$$Register, $tmp$$Register, r15_thread);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\n+\/\/ ============================================================================\n+\/\/ Safepoint Instructions\n+instruct safePoint_poll_tls(rFlagsReg cr, rRegP poll)\n+%{\n+  match(SafePoint poll);\n+  effect(KILL cr, USE poll);\n+\n+  format %{ \"testl   rax, [$poll]\\t\"\n+            \"# Safepoint: poll for GC\" %}\n+  ins_cost(125);\n+  ins_encode %{\n+    __ relocate(relocInfo::poll_type);\n+    address pre_pc = __ pc();\n+    __ testl(rax, Address($poll$$Register, 0));\n+    assert(nativeInstruction_at(pre_pc)->is_safepoint_poll(), \"must emit test %%eax [reg]\");\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+instruct mask_all_evexL(kReg dst, rRegL src) %{\n+  match(Set dst (MaskAll src));\n+  format %{ \"mask_all_evexL $dst, $src \\t! mask all operation\" %}\n+  ins_encode %{\n+    int mask_len = Matcher::vector_length(this);\n+    __ vector_maskall_operation($dst$$KRegister, $src$$Register, mask_len);\n+  %}\n+  ins_pipe( pipe_slow );\n@@ -2761,6 +17279,11 @@\n-\/\/ Operands for bound floating pointer register arguments\n-operand rxmm0() %{\n-  constraint(ALLOC_IN_RC(xmm0_reg));\n-  match(VecX);\n-  format%{%}\n-  interface(REG_INTER);\n+instruct mask_all_evexI_GT32(kReg dst, rRegI src, rRegL tmp) %{\n+  predicate(Matcher::vector_length(n) > 32);\n+  match(Set dst (MaskAll src));\n+  effect(TEMP tmp);\n+  format %{ \"mask_all_evexI_GT32 $dst, $src \\t! using $tmp as TEMP\" %}\n+  ins_encode %{\n+    int mask_len = Matcher::vector_length(this);\n+    __ movslq($tmp$$Register, $src$$Register);\n+    __ vector_maskall_operation($dst$$KRegister, $tmp$$Register, mask_len);\n+  %}\n+  ins_pipe( pipe_slow );\n@@ -2769,19 +17292,15 @@\n-\/\/----------OPERANDS-----------------------------------------------------------\n-\/\/ Operand definitions must precede instruction definitions for correct parsing\n-\/\/ in the ADLC because operands constitute user defined types which are used in\n-\/\/ instruction definitions.\n-\n-\/\/ Vectors\n-\n-\/\/ Dummy generic vector class. Should be used for all vector operands.\n-\/\/ Replaced with vec[SDXYZ] during post-selection pass.\n-operand vec() %{\n-  constraint(ALLOC_IN_RC(dynamic));\n-  match(VecX);\n-  match(VecY);\n-  match(VecZ);\n-  match(VecS);\n-  match(VecD);\n-\n-  format %{ %}\n-  interface(REG_INTER);\n+\/\/ ============================================================================\n+\/\/ Procedure Call\/Return Instructions\n+\/\/ Call Java Static Instruction\n+\/\/ Note: If this code changes, the corresponding ret_addr_offset() and\n+\/\/       compute_padding() functions will have to be adjusted.\n+instruct CallStaticJavaDirect(method meth) %{\n+  match(CallStaticJava);\n+  effect(USE meth);\n+\n+  ins_cost(300);\n+  format %{ \"call,static \" %}\n+  opcode(0xE8); \/* E8 cd *\/\n+  ins_encode(clear_avx, Java_Static_Call(meth), call_epilog);\n+  ins_pipe(pipe_slow);\n+  ins_alignment(4);\n@@ -2790,11 +17309,7 @@\n-\/\/ Dummy generic legacy vector class. Should be used for all legacy vector operands.\n-\/\/ Replaced with legVec[SDXYZ] during post-selection cleanup.\n-\/\/ Note: legacy register class is used to avoid extra (unneeded in 32-bit VM)\n-\/\/ runtime code generation via reg_class_dynamic.\n-operand legVec() %{\n-  constraint(ALLOC_IN_RC(dynamic));\n-  match(VecX);\n-  match(VecY);\n-  match(VecZ);\n-  match(VecS);\n-  match(VecD);\n+\/\/ Call Java Dynamic Instruction\n+\/\/ Note: If this code changes, the corresponding ret_addr_offset() and\n+\/\/       compute_padding() functions will have to be adjusted.\n+instruct CallDynamicJavaDirect(method meth)\n+%{\n+  match(CallDynamicJava);\n+  effect(USE meth);\n@@ -2802,2 +17317,6 @@\n-  format %{ %}\n-  interface(REG_INTER);\n+  ins_cost(300);\n+  format %{ \"movq    rax, #Universe::non_oop_word()\\n\\t\"\n+            \"call,dynamic \" %}\n+  ins_encode(clear_avx, Java_Dynamic_Call(meth), call_epilog);\n+  ins_pipe(pipe_slow);\n+  ins_alignment(4);\n@@ -2806,4 +17325,5 @@\n-\/\/ Replaces vec during post-selection cleanup. See above.\n-operand vecS() %{\n-  constraint(ALLOC_IN_RC(vectors_reg_vlbwdq));\n-  match(VecS);\n+\/\/ Call Runtime Instruction\n+instruct CallRuntimeDirect(method meth)\n+%{\n+  match(CallRuntime);\n+  effect(USE meth);\n@@ -2811,2 +17331,4 @@\n-  format %{ %}\n-  interface(REG_INTER);\n+  ins_cost(300);\n+  format %{ \"call,runtime \" %}\n+  ins_encode(clear_avx, Java_To_Runtime(meth));\n+  ins_pipe(pipe_slow);\n@@ -2815,4 +17337,5 @@\n-\/\/ Replaces legVec during post-selection cleanup. See above.\n-operand legVecS() %{\n-  constraint(ALLOC_IN_RC(vectors_reg_legacy));\n-  match(VecS);\n+\/\/ Call runtime without safepoint\n+instruct CallLeafDirect(method meth)\n+%{\n+  match(CallLeaf);\n+  effect(USE meth);\n@@ -2820,2 +17343,4 @@\n-  format %{ %}\n-  interface(REG_INTER);\n+  ins_cost(300);\n+  format %{ \"call_leaf,runtime \" %}\n+  ins_encode(clear_avx, Java_To_Runtime(meth));\n+  ins_pipe(pipe_slow);\n@@ -2824,4 +17349,5 @@\n-\/\/ Replaces vec during post-selection cleanup. See above.\n-operand vecD() %{\n-  constraint(ALLOC_IN_RC(vectord_reg_vlbwdq));\n-  match(VecD);\n+\/\/ Call runtime without safepoint and with vector arguments\n+instruct CallLeafDirectVector(method meth)\n+%{\n+  match(CallLeafVector);\n+  effect(USE meth);\n@@ -2829,2 +17355,4 @@\n-  format %{ %}\n-  interface(REG_INTER);\n+  ins_cost(300);\n+  format %{ \"call_leaf,vector \" %}\n+  ins_encode(Java_To_Runtime(meth));\n+  ins_pipe(pipe_slow);\n@@ -2833,4 +17361,6 @@\n-\/\/ Replaces legVec during post-selection cleanup. See above.\n-operand legVecD() %{\n-  constraint(ALLOC_IN_RC(vectord_reg_legacy));\n-  match(VecD);\n+\/\/ Call runtime without safepoint\n+\/\/ entry point is null, target holds the address to call\n+instruct CallLeafNoFPInDirect(rRegP target)\n+%{\n+  predicate(n->as_Call()->entry_point() == nullptr);\n+  match(CallLeafNoFP target);\n@@ -2838,2 +17368,7 @@\n-  format %{ %}\n-  interface(REG_INTER);\n+  ins_cost(300);\n+  format %{ \"call_leaf_nofp,runtime indirect \" %}\n+  ins_encode %{\n+     __ call($target$$Register);\n+  %}\n+\n+  ins_pipe(pipe_slow);\n@@ -2842,4 +17377,6 @@\n-\/\/ Replaces vec during post-selection cleanup. See above.\n-operand vecX() %{\n-  constraint(ALLOC_IN_RC(vectorx_reg_vlbwdq));\n-  match(VecX);\n+\/\/ Call runtime without safepoint\n+instruct CallLeafNoFPDirect(method meth)\n+%{\n+  predicate(n->as_Call()->entry_point() != nullptr);\n+  match(CallLeafNoFP);\n+  effect(USE meth);\n@@ -2847,2 +17384,4 @@\n-  format %{ %}\n-  interface(REG_INTER);\n+  ins_cost(300);\n+  format %{ \"call_leaf_nofp,runtime \" %}\n+  ins_encode(clear_avx, Java_To_Runtime(meth));\n+  ins_pipe(pipe_slow);\n@@ -2851,4 +17390,7 @@\n-\/\/ Replaces legVec during post-selection cleanup. See above.\n-operand legVecX() %{\n-  constraint(ALLOC_IN_RC(vectorx_reg_legacy));\n-  match(VecX);\n+\/\/ Return Instruction\n+\/\/ Remove the return address & jump to it.\n+\/\/ Notice: We always emit a nop after a ret to make sure there is room\n+\/\/ for safepoint patching\n+instruct Ret()\n+%{\n+  match(Return);\n@@ -2856,2 +17398,5 @@\n-  format %{ %}\n-  interface(REG_INTER);\n+  format %{ \"ret\" %}\n+  ins_encode %{\n+    __ ret(0);\n+  %}\n+  ins_pipe(pipe_jmp);\n@@ -2860,4 +17405,9 @@\n-\/\/ Replaces vec during post-selection cleanup. See above.\n-operand vecY() %{\n-  constraint(ALLOC_IN_RC(vectory_reg_vlbwdq));\n-  match(VecY);\n+\/\/ Tail Call; Jump from runtime stub to Java code.\n+\/\/ Also known as an 'interprocedural jump'.\n+\/\/ Target of jump will eventually return to caller.\n+\/\/ TailJump below removes the return address.\n+\/\/ Don't use rbp for 'jump_target' because a MachEpilogNode has already been\n+\/\/ emitted just above the TailCall which has reset rbp to the caller state.\n+instruct TailCalljmpInd(no_rbp_RegP jump_target, rbx_RegP method_ptr)\n+%{\n+  match(TailCall jump_target method_ptr);\n@@ -2865,2 +17415,6 @@\n-  format %{ %}\n-  interface(REG_INTER);\n+  ins_cost(300);\n+  format %{ \"jmp     $jump_target\\t# rbx holds method\" %}\n+  ins_encode %{\n+    __ jmp($jump_target$$Register);\n+  %}\n+  ins_pipe(pipe_jmp);\n@@ -2869,4 +17423,5 @@\n-\/\/ Replaces legVec during post-selection cleanup. See above.\n-operand legVecY() %{\n-  constraint(ALLOC_IN_RC(vectory_reg_legacy));\n-  match(VecY);\n+\/\/ Tail Jump; remove the return address; jump to target.\n+\/\/ TailCall above leaves the return address around.\n+instruct tailjmpInd(no_rbp_RegP jump_target, rax_RegP ex_oop)\n+%{\n+  match(TailJump jump_target ex_oop);\n@@ -2874,2 +17429,8 @@\n-  format %{ %}\n-  interface(REG_INTER);\n+  ins_cost(300);\n+  format %{ \"popq    rdx\\t# pop return address\\n\\t\"\n+            \"jmp     $jump_target\" %}\n+  ins_encode %{\n+    __ popq(as_Register(RDX_enc));\n+    __ jmp($jump_target$$Register);\n+  %}\n+  ins_pipe(pipe_jmp);\n@@ -2878,4 +17439,4 @@\n-\/\/ Replaces vec during post-selection cleanup. See above.\n-operand vecZ() %{\n-  constraint(ALLOC_IN_RC(vectorz_reg));\n-  match(VecZ);\n+\/\/ Forward exception.\n+instruct ForwardExceptionjmp()\n+%{\n+  match(ForwardException);\n@@ -2883,2 +17444,5 @@\n-  format %{ %}\n-  interface(REG_INTER);\n+  format %{ \"jmp     forward_exception_stub\" %}\n+  ins_encode %{\n+    __ jump(RuntimeAddress(StubRoutines::forward_exception_entry()), noreg);\n+  %}\n+  ins_pipe(pipe_jmp);\n@@ -2887,4 +17451,6 @@\n-\/\/ Replaces legVec during post-selection cleanup. See above.\n-operand legVecZ() %{\n-  constraint(ALLOC_IN_RC(vectorz_reg_legacy));\n-  match(VecZ);\n+\/\/ Create exception oop: created by stack-crawling runtime code.\n+\/\/ Created exception is now available to this handler, and is setup\n+\/\/ just prior to jumping to this handler.  No code emitted.\n+instruct CreateException(rax_RegP ex_oop)\n+%{\n+  match(Set ex_oop (CreateEx));\n@@ -2892,2 +17458,5 @@\n-  format %{ %}\n-  interface(REG_INTER);\n+  size(0);\n+  \/\/ use the following format syntax\n+  format %{ \"# exception oop is in rax; no code emitted\" %}\n+  ins_encode();\n+  ins_pipe(empty);\n@@ -2896,3 +17465,6 @@\n-\/\/ INSTRUCTIONS -- Platform independent definitions (same for 32- and 64-bit)\n-\n-\/\/ ============================================================================\n+\/\/ Rethrow exception:\n+\/\/ The exception oop will come in the first argument position.\n+\/\/ Then JUMP (not call) to the rethrow stub code.\n+instruct RethrowException()\n+%{\n+  match(Rethrow);\n@@ -2900,3 +17472,2 @@\n-instruct ShouldNotReachHere() %{\n-  match(Halt);\n-  format %{ \"stop\\t# ShouldNotReachHere\" %}\n+  \/\/ use the following format syntax\n+  format %{ \"jmp     rethrow_stub\" %}\n@@ -2904,4 +17475,1 @@\n-    if (is_reachable()) {\n-      const char* str = __ code_string(_halt_reason);\n-      __ stop(str);\n-    }\n+    __ jump(RuntimeAddress(OptoRuntime::rethrow_stub()), noreg);\n@@ -2909,1 +17477,1 @@\n-  ins_pipe(pipe_slow);\n+  ins_pipe(pipe_jmp);\n@@ -2913,0 +17481,12 @@\n+\/\/ This name is KNOWN by the ADLC and cannot be changed.\n+\/\/ The ADLC forces a 'TypeRawPtr::BOTTOM' output type\n+\/\/ for this guy.\n+instruct tlsLoadP(r15_RegP dst) %{\n+  match(Set dst (ThreadLocal));\n+  effect(DEF dst);\n+\n+  size(0);\n+  format %{ \"# TLS is in R15\" %}\n+  ins_encode( \/*empty encoding*\/ );\n+  ins_pipe(ialu_reg_reg);\n+%}\n@@ -10975,0 +25555,326 @@\n+\n+\/\/----------PEEPHOLE RULES-----------------------------------------------------\n+\/\/ These must follow all instruction definitions as they use the names\n+\/\/ defined in the instructions definitions.\n+\/\/\n+\/\/ peeppredicate ( rule_predicate );\n+\/\/ \/\/ the predicate unless which the peephole rule will be ignored\n+\/\/\n+\/\/ peepmatch ( root_instr_name [preceding_instruction]* );\n+\/\/\n+\/\/ peepprocedure ( procedure_name );\n+\/\/ \/\/ provide a procedure name to perform the optimization, the procedure should\n+\/\/ \/\/ reside in the architecture dependent peephole file, the method has the\n+\/\/ \/\/ signature of MachNode* (Block*, int, PhaseRegAlloc*, (MachNode*)(*)(), int...)\n+\/\/ \/\/ with the arguments being the basic block, the current node index inside the\n+\/\/ \/\/ block, the register allocator, the functions upon invoked return a new node\n+\/\/ \/\/ defined in peepreplace, and the rules of the nodes appearing in the\n+\/\/ \/\/ corresponding peepmatch, the function return true if successful, else\n+\/\/ \/\/ return false\n+\/\/\n+\/\/ peepconstraint %{\n+\/\/ (instruction_number.operand_name relational_op instruction_number.operand_name\n+\/\/  [, ...] );\n+\/\/ \/\/ instruction numbers are zero-based using left to right order in peepmatch\n+\/\/\n+\/\/ peepreplace ( instr_name  ( [instruction_number.operand_name]* ) );\n+\/\/ \/\/ provide an instruction_number.operand_name for each operand that appears\n+\/\/ \/\/ in the replacement instruction's match rule\n+\/\/\n+\/\/ ---------VM FLAGS---------------------------------------------------------\n+\/\/\n+\/\/ All peephole optimizations can be turned off using -XX:-OptoPeephole\n+\/\/\n+\/\/ Each peephole rule is given an identifying number starting with zero and\n+\/\/ increasing by one in the order seen by the parser.  An individual peephole\n+\/\/ can be enabled, and all others disabled, by using -XX:OptoPeepholeAt=#\n+\/\/ on the command-line.\n+\/\/\n+\/\/ ---------CURRENT LIMITATIONS----------------------------------------------\n+\/\/\n+\/\/ Only transformations inside a basic block (do we need more for peephole)\n+\/\/\n+\/\/ ---------EXAMPLE----------------------------------------------------------\n+\/\/\n+\/\/ \/\/ pertinent parts of existing instructions in architecture description\n+\/\/ instruct movI(rRegI dst, rRegI src)\n+\/\/ %{\n+\/\/   match(Set dst (CopyI src));\n+\/\/ %}\n+\/\/\n+\/\/ instruct incI_rReg(rRegI dst, immI_1 src, rFlagsReg cr)\n+\/\/ %{\n+\/\/   match(Set dst (AddI dst src));\n+\/\/   effect(KILL cr);\n+\/\/ %}\n+\/\/\n+\/\/ instruct leaI_rReg_immI(rRegI dst, immI_1 src)\n+\/\/ %{\n+\/\/   match(Set dst (AddI dst src));\n+\/\/ %}\n+\/\/\n+\/\/ 1. Simple replacement\n+\/\/ - Only match adjacent instructions in same basic block\n+\/\/ - Only equality constraints\n+\/\/ - Only constraints between operands, not (0.dest_reg == RAX_enc)\n+\/\/ - Only one replacement instruction\n+\/\/\n+\/\/ \/\/ Change (inc mov) to lea\n+\/\/ peephole %{\n+\/\/   \/\/ lea should only be emitted when beneficial\n+\/\/   peeppredicate( VM_Version::supports_fast_2op_lea() );\n+\/\/   \/\/ increment preceded by register-register move\n+\/\/   peepmatch ( incI_rReg movI );\n+\/\/   \/\/ require that the destination register of the increment\n+\/\/   \/\/ match the destination register of the move\n+\/\/   peepconstraint ( 0.dst == 1.dst );\n+\/\/   \/\/ construct a replacement instruction that sets\n+\/\/   \/\/ the destination to ( move's source register + one )\n+\/\/   peepreplace ( leaI_rReg_immI( 0.dst 1.src 0.src ) );\n+\/\/ %}\n+\/\/\n+\/\/ 2. Procedural replacement\n+\/\/ - More flexible finding relevent nodes\n+\/\/ - More flexible constraints\n+\/\/ - More flexible transformations\n+\/\/ - May utilise architecture-dependent API more effectively\n+\/\/ - Currently only one replacement instruction due to adlc parsing capabilities\n+\/\/\n+\/\/ \/\/ Change (inc mov) to lea\n+\/\/ peephole %{\n+\/\/   \/\/ lea should only be emitted when beneficial\n+\/\/   peeppredicate( VM_Version::supports_fast_2op_lea() );\n+\/\/   \/\/ the rule numbers of these nodes inside are passed into the function below\n+\/\/   peepmatch ( incI_rReg movI );\n+\/\/   \/\/ the method that takes the responsibility of transformation\n+\/\/   peepprocedure ( inc_mov_to_lea );\n+\/\/   \/\/ the replacement is a leaI_rReg_immI, a lambda upon invoked creating this\n+\/\/   \/\/ node is passed into the function above\n+\/\/   peepreplace ( leaI_rReg_immI() );\n+\/\/ %}\n+\n+\/\/ These instructions is not matched by the matcher but used by the peephole\n+instruct leaI_rReg_rReg_peep(rRegI dst, rRegI src1, rRegI src2)\n+%{\n+  predicate(false);\n+  match(Set dst (AddI src1 src2));\n+  format %{ \"leal    $dst, [$src1 + $src2]\" %}\n+  ins_encode %{\n+    Register dst = $dst$$Register;\n+    Register src1 = $src1$$Register;\n+    Register src2 = $src2$$Register;\n+    if (src1 != rbp && src1 != r13) {\n+      __ leal(dst, Address(src1, src2, Address::times_1));\n+    } else {\n+      assert(src2 != rbp && src2 != r13, \"\");\n+      __ leal(dst, Address(src2, src1, Address::times_1));\n+    }\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+instruct leaI_rReg_immI_peep(rRegI dst, rRegI src1, immI src2)\n+%{\n+  predicate(false);\n+  match(Set dst (AddI src1 src2));\n+  format %{ \"leal    $dst, [$src1 + $src2]\" %}\n+  ins_encode %{\n+    __ leal($dst$$Register, Address($src1$$Register, $src2$$constant));\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+instruct leaI_rReg_immI2_peep(rRegI dst, rRegI src, immI2 shift)\n+%{\n+  predicate(false);\n+  match(Set dst (LShiftI src shift));\n+  format %{ \"leal    $dst, [$src << $shift]\" %}\n+  ins_encode %{\n+    Address::ScaleFactor scale = static_cast<Address::ScaleFactor>($shift$$constant);\n+    Register src = $src$$Register;\n+    if (scale == Address::times_2 && src != rbp && src != r13) {\n+      __ leal($dst$$Register, Address(src, src, Address::times_1));\n+    } else {\n+      __ leal($dst$$Register, Address(noreg, src, scale));\n+    }\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+instruct leaL_rReg_rReg_peep(rRegL dst, rRegL src1, rRegL src2)\n+%{\n+  predicate(false);\n+  match(Set dst (AddL src1 src2));\n+  format %{ \"leaq    $dst, [$src1 + $src2]\" %}\n+  ins_encode %{\n+    Register dst = $dst$$Register;\n+    Register src1 = $src1$$Register;\n+    Register src2 = $src2$$Register;\n+    if (src1 != rbp && src1 != r13) {\n+      __ leaq(dst, Address(src1, src2, Address::times_1));\n+    } else {\n+      assert(src2 != rbp && src2 != r13, \"\");\n+      __ leaq(dst, Address(src2, src1, Address::times_1));\n+    }\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+instruct leaL_rReg_immL32_peep(rRegL dst, rRegL src1, immL32 src2)\n+%{\n+  predicate(false);\n+  match(Set dst (AddL src1 src2));\n+  format %{ \"leaq    $dst, [$src1 + $src2]\" %}\n+  ins_encode %{\n+    __ leaq($dst$$Register, Address($src1$$Register, $src2$$constant));\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+instruct leaL_rReg_immI2_peep(rRegL dst, rRegL src, immI2 shift)\n+%{\n+  predicate(false);\n+  match(Set dst (LShiftL src shift));\n+  format %{ \"leaq    $dst, [$src << $shift]\" %}\n+  ins_encode %{\n+    Address::ScaleFactor scale = static_cast<Address::ScaleFactor>($shift$$constant);\n+    Register src = $src$$Register;\n+    if (scale == Address::times_2 && src != rbp && src != r13) {\n+      __ leaq($dst$$Register, Address(src, src, Address::times_1));\n+    } else {\n+      __ leaq($dst$$Register, Address(noreg, src, scale));\n+    }\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+\/\/ These peephole rules replace mov + I pairs (where I is one of {add, inc, dec,\n+\/\/ sal}) with lea instructions. The {add, sal} rules are beneficial in\n+\/\/ processors with at least partial ALU support for lea\n+\/\/ (supports_fast_2op_lea()), whereas the {inc, dec} rules are only generally\n+\/\/ beneficial for processors with full ALU support\n+\/\/ (VM_Version::supports_fast_3op_lea()) and Intel Cascade Lake.\n+\n+peephole\n+%{\n+  peeppredicate(VM_Version::supports_fast_2op_lea());\n+  peepmatch (addI_rReg);\n+  peepprocedure (lea_coalesce_reg);\n+  peepreplace (leaI_rReg_rReg_peep());\n+%}\n+\n+peephole\n+%{\n+  peeppredicate(VM_Version::supports_fast_2op_lea());\n+  peepmatch (addI_rReg_imm);\n+  peepprocedure (lea_coalesce_imm);\n+  peepreplace (leaI_rReg_immI_peep());\n+%}\n+\n+peephole\n+%{\n+  peeppredicate(VM_Version::supports_fast_3op_lea() ||\n+                VM_Version::is_intel_cascade_lake());\n+  peepmatch (incI_rReg);\n+  peepprocedure (lea_coalesce_imm);\n+  peepreplace (leaI_rReg_immI_peep());\n+%}\n+\n+peephole\n+%{\n+  peeppredicate(VM_Version::supports_fast_3op_lea() ||\n+                VM_Version::is_intel_cascade_lake());\n+  peepmatch (decI_rReg);\n+  peepprocedure (lea_coalesce_imm);\n+  peepreplace (leaI_rReg_immI_peep());\n+%}\n+\n+peephole\n+%{\n+  peeppredicate(VM_Version::supports_fast_2op_lea());\n+  peepmatch (salI_rReg_immI2);\n+  peepprocedure (lea_coalesce_imm);\n+  peepreplace (leaI_rReg_immI2_peep());\n+%}\n+\n+peephole\n+%{\n+  peeppredicate(VM_Version::supports_fast_2op_lea());\n+  peepmatch (addL_rReg);\n+  peepprocedure (lea_coalesce_reg);\n+  peepreplace (leaL_rReg_rReg_peep());\n+%}\n+\n+peephole\n+%{\n+  peeppredicate(VM_Version::supports_fast_2op_lea());\n+  peepmatch (addL_rReg_imm);\n+  peepprocedure (lea_coalesce_imm);\n+  peepreplace (leaL_rReg_immL32_peep());\n+%}\n+\n+peephole\n+%{\n+  peeppredicate(VM_Version::supports_fast_3op_lea() ||\n+                VM_Version::is_intel_cascade_lake());\n+  peepmatch (incL_rReg);\n+  peepprocedure (lea_coalesce_imm);\n+  peepreplace (leaL_rReg_immL32_peep());\n+%}\n+\n+peephole\n+%{\n+  peeppredicate(VM_Version::supports_fast_3op_lea() ||\n+                VM_Version::is_intel_cascade_lake());\n+  peepmatch (decL_rReg);\n+  peepprocedure (lea_coalesce_imm);\n+  peepreplace (leaL_rReg_immL32_peep());\n+%}\n+\n+peephole\n+%{\n+  peeppredicate(VM_Version::supports_fast_2op_lea());\n+  peepmatch (salL_rReg_immI2);\n+  peepprocedure (lea_coalesce_imm);\n+  peepreplace (leaL_rReg_immI2_peep());\n+%}\n+\n+peephole\n+%{\n+  peepmatch (leaPCompressedOopOffset);\n+  peepprocedure (lea_remove_redundant);\n+%}\n+\n+peephole\n+%{\n+  peepmatch (leaP8Narrow);\n+  peepprocedure (lea_remove_redundant);\n+%}\n+\n+peephole\n+%{\n+  peepmatch (leaP32Narrow);\n+  peepprocedure (lea_remove_redundant);\n+%}\n+\n+\/\/ These peephole rules matches instructions which set flags and are followed by a testI\/L_reg\n+\/\/ The test instruction is redudanent in case the downstream instuctions (like JCC or CMOV) only use flags that are already set by the previous instruction\n+\n+\/\/int variant\n+peephole\n+%{\n+  peepmatch (testI_reg);\n+  peepprocedure (test_may_remove);\n+%}\n+\n+\/\/long variant\n+peephole\n+%{\n+  peepmatch (testL_reg);\n+  peepprocedure (test_may_remove);\n+%}\n+\n+\n+\/\/----------SMARTSPILL RULES---------------------------------------------------\n+\/\/ These must follow all instruction definitions as they use the names\n+\/\/ defined in the instructions definitions.\n","filename":"src\/hotspot\/cpu\/x86\/x86.ad","additions":16419,"deletions":1513,"binary":false,"changes":17932,"status":"modified"},{"patch":"@@ -80,0 +80,1 @@\n+#include \"oops\/constantPool.inline.hpp\"\n@@ -87,0 +88,1 @@\n+#include \"oops\/resolvedFieldEntry.hpp\"\n@@ -524,1 +526,13 @@\n-static void rewrite_nofast_bytecode(const methodHandle& method) {\n+\/\/ In AOTCache workflow, when dumping preimage, the constant pool entries are stored in unresolved state.\n+\/\/ So the fast version of getfield\/putfield needs to be converted to nofast version.\n+\/\/ When dumping the final image in the assembly phase, these nofast versions are converted back to fast versions\n+\/\/ if the constant pool entry refered by these bytecodes is stored in resolved state.\n+\/\/ Same principle applies to static and dynamic archives. If the constant pool entry is in resolved state, then\n+\/\/ the fast version of the bytecodes can be preserved, else use the nofast version.\n+\/\/\n+\/\/ The fast versions of aload_0 (i.e. _fast_Xaccess_0) merges the bytecode pair (aload_0, fast_Xgetfield).\n+\/\/ If the fast version of aload_0 is preserved in AOTCache, then the JVMTI notifications for field access and\n+\/\/ breakpoint events will be skipped for the second bytecode (fast_Xgetfield) in the pair.\n+\/\/ Same holds for fast versions of iload_0. So for these bytecodes, nofast version is used.\n+static void rewrite_bytecodes(const methodHandle& method) {\n+  ConstantPool* cp = method->constants();\n@@ -526,0 +540,9 @@\n+  Bytecodes::Code new_code;\n+\n+  LogStreamHandle(Trace, aot, resolve) lsh;\n+  if (lsh.is_enabled()) {\n+    lsh.print(\"Rewriting bytecodes for \");\n+    method()->print_external_name(&lsh);\n+    lsh.print(\"\\n\");\n+  }\n+\n@@ -528,5 +551,59 @@\n-    switch (opcode) {\n-    case Bytecodes::_getfield:      *bcs.bcp() = Bytecodes::_nofast_getfield;      break;\n-    case Bytecodes::_putfield:      *bcs.bcp() = Bytecodes::_nofast_putfield;      break;\n-    case Bytecodes::_aload_0:       *bcs.bcp() = Bytecodes::_nofast_aload_0;       break;\n-    case Bytecodes::_iload: {\n+    \/\/ Use current opcode as the default value of new_code\n+    new_code = opcode;\n+    switch(opcode) {\n+    case Bytecodes::_getfield: {\n+      uint rfe_index = bcs.get_index_u2();\n+      bool is_resolved = cp->is_resolved(rfe_index, opcode);\n+      if (is_resolved) {\n+        assert(!CDSConfig::is_dumping_preimage_static_archive(), \"preimage should not have resolved field references\");\n+        ResolvedFieldEntry* rfe = cp->resolved_field_entry_at(bcs.get_index_u2());\n+        switch(rfe->tos_state()) {\n+        case btos:\n+          \/\/ fallthrough\n+        case ztos: new_code = Bytecodes::_fast_bgetfield; break;\n+        case atos: new_code = Bytecodes::_fast_agetfield; break;\n+        case itos: new_code = Bytecodes::_fast_igetfield; break;\n+        case ctos: new_code = Bytecodes::_fast_cgetfield; break;\n+        case stos: new_code = Bytecodes::_fast_sgetfield; break;\n+        case ltos: new_code = Bytecodes::_fast_lgetfield; break;\n+        case ftos: new_code = Bytecodes::_fast_fgetfield; break;\n+        case dtos: new_code = Bytecodes::_fast_dgetfield; break;\n+        default:\n+          ShouldNotReachHere();\n+          break;\n+        }\n+      } else {\n+        new_code = Bytecodes::_nofast_getfield;\n+      }\n+      break;\n+    }\n+    case Bytecodes::_putfield: {\n+      uint rfe_index = bcs.get_index_u2();\n+      bool is_resolved = cp->is_resolved(rfe_index, opcode);\n+      if (is_resolved) {\n+        assert(!CDSConfig::is_dumping_preimage_static_archive(), \"preimage should not have resolved field references\");\n+        ResolvedFieldEntry* rfe = cp->resolved_field_entry_at(bcs.get_index_u2());\n+        switch(rfe->tos_state()) {\n+        case btos: new_code = Bytecodes::_fast_bputfield; break;\n+        case ztos: new_code = Bytecodes::_fast_zputfield; break;\n+        case atos: new_code = Bytecodes::_fast_aputfield; break;\n+        case itos: new_code = Bytecodes::_fast_iputfield; break;\n+        case ctos: new_code = Bytecodes::_fast_cputfield; break;\n+        case stos: new_code = Bytecodes::_fast_sputfield; break;\n+        case ltos: new_code = Bytecodes::_fast_lputfield; break;\n+        case ftos: new_code = Bytecodes::_fast_fputfield; break;\n+        case dtos: new_code = Bytecodes::_fast_dputfield; break;\n+        default:\n+          ShouldNotReachHere();\n+          break;\n+        }\n+      } else {\n+        new_code = Bytecodes::_nofast_putfield;\n+      }\n+      break;\n+    }\n+    case Bytecodes::_aload_0:\n+      \/\/ Revert _fast_Xaccess_0 or _aload_0 to _nofast_aload_0\n+      new_code = Bytecodes::_nofast_aload_0;\n+      break;\n+    case Bytecodes::_iload:\n@@ -534,1 +611,1 @@\n-        *bcs.bcp() = Bytecodes::_nofast_iload;\n+        new_code = Bytecodes::_nofast_iload;\n@@ -537,0 +614,2 @@\n+    default:\n+      break;\n@@ -538,1 +617,5 @@\n-    default: break;\n+    if (opcode != new_code) {\n+      *bcs.bcp() = new_code;\n+      if (lsh.is_enabled()) {\n+        lsh.print_cr(\"%d:%s -> %s\", bcs.bci(), Bytecodes::name(opcode), Bytecodes::name(new_code));\n+      }\n@@ -546,1 +629,1 @@\n-void AOTMetaspace::rewrite_nofast_bytecodes_and_calculate_fingerprints(Thread* thread, InstanceKlass* ik) {\n+void AOTMetaspace::rewrite_bytecodes_and_calculate_fingerprints(Thread* thread, InstanceKlass* ik) {\n@@ -550,1 +633,1 @@\n-      rewrite_nofast_bytecode(m);\n+      rewrite_bytecodes(m);\n","filename":"src\/hotspot\/share\/cds\/aotMetaspace.cpp","additions":93,"deletions":10,"binary":false,"changes":103,"status":"modified"},{"patch":"@@ -961,1 +961,1 @@\n-      AOTMetaspace::rewrite_nofast_bytecodes_and_calculate_fingerprints(Thread::current(), ik);\n+      AOTMetaspace::rewrite_bytecodes_and_calculate_fingerprints(Thread::current(), ik);\n","filename":"src\/hotspot\/share\/cds\/archiveBuilder.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -1817,1 +1817,2 @@\n-          name != vmSymbols::java_lang_NullPointerException()) {\n+          name != vmSymbols::java_lang_NullPointerException() &&\n+          name != vmSymbols::jdk_internal_vm_PreemptedException()) {\n","filename":"src\/hotspot\/share\/cds\/heapShared.cpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -1066,1 +1066,3 @@\n-    assert(offsets->value(CodeOffsets::Exceptions) != -1, \"must have exception entry\");\n+\n+    assert(compiler->type() == compiler_c2 ||\n+           offsets->value(CodeOffsets::Exceptions) != -1, \"must have exception entry\");\n","filename":"src\/hotspot\/share\/ci\/ciEnv.cpp","additions":3,"deletions":1,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -480,1 +480,1 @@\n-  LinkResolver::resolve_field(result, link_info, bc, false, CHECK_AND_CLEAR_(false));\n+  LinkResolver::resolve_field(result, link_info, bc, ClassInitMode::dont_init, CHECK_AND_CLEAR_(false));\n","filename":"src\/hotspot\/share\/ci\/ciField.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -5869,0 +5869,1 @@\n+    guarantee_property((u4)cp_size < 0xffff, \"Overflow in constant pool size for hidden class %s\", CHECK);\n","filename":"src\/hotspot\/share\/classfile\/classFileParser.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -2153,0 +2153,1 @@\n+int java_lang_VirtualThread::_interruptible_wait_offset;\n@@ -2164,0 +2165,1 @@\n+  macro(_interruptible_wait_offset,        k, \"interruptibleWait\",  bool_signature,              false); \\\n@@ -2233,0 +2235,4 @@\n+void java_lang_VirtualThread::set_interruptible_wait(oop vthread, jboolean value) {\n+  vthread->bool_field_put_volatile(_interruptible_wait_offset, value);\n+}\n+\n","filename":"src\/hotspot\/share\/classfile\/javaClasses.cpp","additions":6,"deletions":0,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -571,0 +571,1 @@\n+  static int _interruptible_wait_offset;\n@@ -623,0 +624,1 @@\n+  static void set_interruptible_wait(oop vthread, jboolean value);\n","filename":"src\/hotspot\/share\/classfile\/javaClasses.hpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -78,0 +78,1 @@\n+  do_klass(PreemptedException_klass,                    jdk_internal_vm_PreemptedException                    ) \\\n","filename":"src\/hotspot\/share\/classfile\/vmClassMacros.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -249,0 +249,1 @@\n+  template(jdk_internal_vm_PreemptedException,        \"jdk\/internal\/vm\/PreemptedException\")       \\\n@@ -545,0 +546,2 @@\n+  template(atKlassInit_name,                          \"atKlassInit\")                              \\\n+  template(hasArgsAtTop_name,                         \"hasArgsAtTop\")                             \\\n","filename":"src\/hotspot\/share\/classfile\/vmSymbols.hpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -31,0 +31,1 @@\n+#include \"cppstdlib\/type_traits.hpp\"\n@@ -56,2 +57,0 @@\n-#include <type_traits>\n-\n","filename":"src\/hotspot\/share\/code\/codeBlob.cpp","additions":1,"deletions":2,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -1161,2 +1161,1 @@\n-    + align_up(debug_info->data_size()           , oopSize)\n-    + ImmutableDataReferencesCounterSize;\n+    + align_up(debug_info->data_size()           , oopSize);\n@@ -1167,0 +1166,1 @@\n+    immutable_data_size += ImmutableDataRefCountSize;\n@@ -1320,1 +1320,1 @@\n-    _deopt_handler_offset    = 0;\n+    _deopt_handler_entry_offset    = 0;\n@@ -1341,1 +1341,1 @@\n-    _immutable_data_reference_counter_offset = 0;\n+    _immutable_data_ref_count_offset = 0;\n@@ -1460,1 +1460,1 @@\n-  _deopt_handler_offset         = nm._deopt_handler_offset;\n+  _deopt_handler_entry_offset   = nm._deopt_handler_entry_offset;\n@@ -1474,1 +1474,1 @@\n-  _immutable_data_reference_counter_offset = nm._immutable_data_reference_counter_offset;\n+  _immutable_data_ref_count_offset = nm._immutable_data_ref_count_offset;\n@@ -1479,1 +1479,1 @@\n-    set_immutable_data_references_counter(get_immutable_data_references_counter() + 1);\n+    inc_immutable_data_ref_count();\n@@ -1722,1 +1722,1 @@\n-        _deopt_handler_offset    = code_offset() + offsets->value(CodeOffsets::Deopt);\n+        _deopt_handler_entry_offset    = code_offset() + offsets->value(CodeOffsets::Deopt);\n@@ -1724,1 +1724,1 @@\n-        _deopt_handler_offset    = -1;\n+        _deopt_handler_entry_offset    = -1;\n@@ -1730,1 +1730,0 @@\n-      assert(offsets->value(CodeOffsets::Exceptions) != -1, \"must be set\");\n@@ -1733,2 +1732,10 @@\n-      _exception_offset          = _stub_offset + offsets->value(CodeOffsets::Exceptions);\n-      _deopt_handler_offset      = _stub_offset + offsets->value(CodeOffsets::Deopt);\n+      bool has_exception_handler = (offsets->value(CodeOffsets::Exceptions) != -1);\n+      assert(has_exception_handler == (compiler->type() != compiler_c2),\n+             \"C2 compiler doesn't provide exception handler stub code.\");\n+      if (has_exception_handler) {\n+        _exception_offset = _stub_offset + offsets->value(CodeOffsets::Exceptions);\n+      } else {\n+        _exception_offset = -1;\n+      }\n+\n+      _deopt_handler_entry_offset = _stub_offset + offsets->value(CodeOffsets::Deopt);\n@@ -1776,2 +1783,1 @@\n-    _immutable_data_reference_counter_offset = _speculations_offset + align_up(speculations_len, oopSize);\n-    DEBUG_ONLY( int immutable_data_end_offset = _immutable_data_reference_counter_offset + ImmutableDataReferencesCounterSize; )\n+    _immutable_data_ref_count_offset = _speculations_offset + align_up(speculations_len, oopSize);\n@@ -1779,2 +1785,1 @@\n-    _immutable_data_reference_counter_offset =  _scopes_data_offset + align_up(debug_info->data_size(), oopSize);\n-    DEBUG_ONLY( int immutable_data_end_offset = _immutable_data_reference_counter_offset + ImmutableDataReferencesCounterSize; )\n+    _immutable_data_ref_count_offset = _scopes_data_offset + align_up(debug_info->data_size(), oopSize);\n@@ -1782,0 +1787,1 @@\n+    DEBUG_ONLY( int immutable_data_end_offset = _immutable_data_ref_count_offset + ImmutableDataRefCountSize; )\n@@ -1813,1 +1819,1 @@\n-    set_immutable_data_references_counter(1);\n+    init_immutable_data_ref_count();\n@@ -2446,6 +2452,2 @@\n-    int reference_count = get_immutable_data_references_counter();\n-    assert(reference_count > 0, \"immutable data has no references\");\n-\n-    set_immutable_data_references_counter(reference_count - 1);\n-    \/\/ Free memory if this is the last nmethod referencing immutable data\n-    if (reference_count == 0) {\n+    \/\/ Free memory if this was the last nmethod referencing immutable data\n+    if (dec_immutable_data_ref_count() == 0) {\n@@ -4054,1 +4056,1 @@\n-  if (JVMCI_ONLY(_deopt_handler_offset != -1 &&) pos == deopt_handler_begin()) label = \"[Deopt Handler Code]\";\n+  if (JVMCI_ONLY(_deopt_handler_entry_offset != -1 &&) pos == deopt_handler_entry()) label = \"[Deopt Handler Entry Point]\";\n","filename":"src\/hotspot\/share\/code\/nmethod.cpp","additions":26,"deletions":24,"binary":false,"changes":50,"status":"modified"},{"patch":"@@ -33,0 +33,1 @@\n+#include \"runtime\/mutexLocker.hpp\"\n@@ -172,1 +173,1 @@\n-  #define ImmutableDataReferencesCounterSize ((int)sizeof(int))\n+  #define ImmutableDataRefCountSize ((int)sizeof(int))\n@@ -236,1 +237,1 @@\n-  int _deopt_handler_offset;\n+  int _deopt_handler_entry_offset;\n@@ -258,1 +259,1 @@\n-  int      _immutable_data_reference_counter_offset;\n+  int      _immutable_data_ref_count_offset;\n@@ -624,1 +625,1 @@\n-  address deopt_handler_begin   () const { return           header_begin() + _deopt_handler_offset    ; }\n+  address deopt_handler_entry   () const { return           header_begin() + _deopt_handler_entry_offset    ; }\n@@ -655,1 +656,1 @@\n-  address speculations_end      () const { return           _immutable_data + _immutable_data_reference_counter_offset ; }\n+  address speculations_end      () const { return           _immutable_data + _immutable_data_ref_count_offset ; }\n@@ -657,1 +658,1 @@\n-  address scopes_data_end       () const { return           _immutable_data + _immutable_data_reference_counter_offset ; }\n+  address scopes_data_end       () const { return           _immutable_data + _immutable_data_ref_count_offset ; }\n@@ -659,1 +660,1 @@\n-  address immutable_data_references_counter_begin () const { return _immutable_data + _immutable_data_reference_counter_offset ; }\n+  address immutable_data_ref_count_begin () const { return  _immutable_data + _immutable_data_ref_count_offset ; }\n@@ -983,2 +984,18 @@\n-  inline int  get_immutable_data_references_counter()           { return *((int*)immutable_data_references_counter_begin());  }\n-  inline void set_immutable_data_references_counter(int count)  { *((int*)immutable_data_references_counter_begin()) = count; }\n+  inline void init_immutable_data_ref_count() {\n+    assert(is_not_installed(), \"should be called in nmethod constructor\");\n+    *((int*)immutable_data_ref_count_begin()) = 1;\n+  }\n+\n+  inline int inc_immutable_data_ref_count() {\n+    assert_lock_strong(CodeCache_lock);\n+    int* ref_count = (int*)immutable_data_ref_count_begin();\n+    assert(*ref_count > 0, \"Must be positive\");\n+    return ++(*ref_count);\n+  }\n+\n+  inline int dec_immutable_data_ref_count() {\n+    assert_lock_strong(CodeCache_lock);\n+    int* ref_count = (int*)immutable_data_ref_count_begin();\n+    assert(*ref_count > 0, \"Must be positive\");\n+    return --(*ref_count);\n+  }\n","filename":"src\/hotspot\/share\/code\/nmethod.hpp","additions":26,"deletions":9,"binary":false,"changes":35,"status":"modified"},{"patch":"@@ -1367,1 +1367,0 @@\n-  SubTasksDone                               _sub_tasks;\n@@ -1371,0 +1370,1 @@\n+  volatile bool                              _code_cache_claimed;\n@@ -1373,5 +1373,4 @@\n-  enum PSAdjustSubTask {\n-    PSAdjustSubTask_code_cache,\n-\n-    PSAdjustSubTask_num_elements\n-  };\n+  bool try_claim_code_cache_task() {\n+    return AtomicAccess::load(&_code_cache_claimed) == false\n+        && AtomicAccess::cmpxchg(&_code_cache_claimed, false, true) == false;\n+  }\n@@ -1383,2 +1382,3 @@\n-    _sub_tasks(PSAdjustSubTask_num_elements),\n-    _nworkers(nworkers) {\n+    _oop_storage_iter(),\n+    _nworkers(nworkers),\n+    _code_cache_claimed(false) {\n@@ -1391,3 +1391,4 @@\n-    ParCompactionManager* cm = ParCompactionManager::gc_thread_compaction_manager(worker_id);\n-    cm->preserved_marks()->adjust_during_full_gc();\n-      \/\/ adjust pointers in all spaces\n+      \/\/ Pointers in heap.\n+      ParCompactionManager* cm = ParCompactionManager::gc_thread_compaction_manager(worker_id);\n+      cm->preserved_marks()->adjust_during_full_gc();\n+\n@@ -1397,0 +1398,1 @@\n+\n@@ -1398,5 +1400,1 @@\n-      ResourceMark rm;\n-      Threads::possibly_parallel_oops_do(_nworkers > 1, &pc_adjust_pointer_closure, nullptr);\n-    }\n-    _oop_storage_iter.oops_do(&pc_adjust_pointer_closure);\n-    {\n+      \/\/ All (strong and weak) CLDs.\n@@ -1406,0 +1404,1 @@\n+\n@@ -1407,0 +1406,13 @@\n+      \/\/ Threads stack frames. No need to visit on-stack nmethods, because all\n+      \/\/ nmethods are visited in one go via CodeCache::nmethods_do.\n+      ResourceMark rm;\n+      Threads::possibly_parallel_oops_do(_nworkers > 1, &pc_adjust_pointer_closure, nullptr);\n+      if (try_claim_code_cache_task()) {\n+        NMethodToOopClosure adjust_code(&pc_adjust_pointer_closure, NMethodToOopClosure::FixRelocations);\n+        CodeCache::nmethods_do(&adjust_code);\n+      }\n+    }\n+\n+    {\n+      \/\/ VM internal strong and weak roots.\n+      _oop_storage_iter.oops_do(&pc_adjust_pointer_closure);\n@@ -1410,5 +1422,0 @@\n-    if (_sub_tasks.try_claim_task(PSAdjustSubTask_code_cache)) {\n-      NMethodToOopClosure adjust_code(&pc_adjust_pointer_closure, NMethodToOopClosure::FixRelocations);\n-      CodeCache::nmethods_do(&adjust_code);\n-    }\n-    _sub_tasks.all_tasks_claimed();\n","filename":"src\/hotspot\/share\/gc\/parallel\/psParallelCompact.cpp","additions":28,"deletions":21,"binary":false,"changes":49,"status":"modified"},{"patch":"@@ -51,1 +51,0 @@\n-#include \"gc\/shared\/modRefBarrierSet.hpp\"\n","filename":"src\/hotspot\/share\/gc\/serial\/serialFullGC.cpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -25,0 +25,1 @@\n+#include \"ci\/ciInlineKlass.hpp\"\n@@ -37,0 +38,102 @@\n+void CardTableBarrierSetC1::store_at_resolved(LIRAccess& access, LIR_Opr value) {\n+  DecoratorSet decorators = access.decorators();\n+  bool is_array = (decorators & IS_ARRAY) != 0;\n+  bool on_anonymous = (decorators & ON_UNKNOWN_OOP_REF) != 0;\n+\n+  \/\/ Is this a flat, atomic access that might require gc barriers on oop fields?\n+  ciInlineKlass* vk = access.vk();\n+  if (vk != nullptr && vk->has_object_fields()) {\n+    \/\/ Add pre-barriers for oop fields\n+    for (int i = 0; i < vk->nof_nonstatic_fields(); i++) {\n+      ciField* field = vk->nonstatic_field_at(i);\n+      if (!field->type()->is_primitive_type()) {\n+        int off = access.offset().opr().as_jint() + field->offset_in_bytes() - vk->payload_offset();\n+        LIRAccess inner_access(access.gen(), decorators, access.base(), LIR_OprFact::intConst(off), field->type()->basic_type(), access.patch_emit_info(), access.access_emit_info());\n+        pre_barrier(inner_access, resolve_address(inner_access, false),\n+                    LIR_OprFact::illegalOpr \/* pre_val *\/, inner_access.patch_emit_info());\n+      }\n+    }\n+  }\n+\n+  if (access.is_oop()) {\n+    pre_barrier(access, access.resolved_addr(),\n+                LIR_OprFact::illegalOpr \/* pre_val *\/, access.patch_emit_info());\n+  }\n+\n+  BarrierSetC1::store_at_resolved(access, value);\n+\n+  if (access.is_oop()) {\n+    bool precise = is_array || on_anonymous;\n+    LIR_Opr post_addr = precise ? access.resolved_addr() : access.base().opr();\n+    post_barrier(access, post_addr, value);\n+  }\n+\n+  if (vk != nullptr && vk->has_object_fields()) {\n+    \/\/ Add post-barriers for oop fields\n+    for (int i = 0; i < vk->nof_nonstatic_fields(); i++) {\n+      ciField* field = vk->nonstatic_field_at(i);\n+      if (!field->type()->is_primitive_type()) {\n+        int inner_off = field->offset_in_bytes() - vk->payload_offset();\n+        int off = access.offset().opr().as_jint() + inner_off;\n+        LIRAccess inner_access(access.gen(), decorators, access.base(), LIR_OprFact::intConst(off), field->type()->basic_type(), access.patch_emit_info(), access.access_emit_info());\n+\n+        \/\/ Shift long value to extract the narrow oop field value and zero-extend\n+        LIR_Opr field_val = access.gen()->new_register(T_LONG);\n+        access.gen()->lir()->unsigned_shift_right(value,\n+                                                  LIR_OprFact::intConst(inner_off << LogBitsPerByte),\n+                                                  field_val, LIR_Opr::illegalOpr());\n+        LIR_Opr mask = access.gen()->load_immediate((julong) max_juint, T_LONG);\n+        access.gen()->lir()->logical_and(field_val, mask, field_val);\n+        LIR_Opr oop_val = access.gen()->new_register(T_OBJECT);\n+        access.gen()->lir()->move(field_val, oop_val);\n+\n+        assert(!is_array && !on_anonymous, \"not suppported\");\n+        post_barrier(inner_access, access.base().opr(), oop_val);\n+      }\n+    }\n+  }\n+}\n+\n+LIR_Opr CardTableBarrierSetC1::atomic_cmpxchg_at_resolved(LIRAccess& access, LIRItem& cmp_value, LIRItem& new_value) {\n+  if (access.is_oop()) {\n+    pre_barrier(access, access.resolved_addr(),\n+                LIR_OprFact::illegalOpr \/* pre_val *\/, nullptr);\n+  }\n+\n+  LIR_Opr result = BarrierSetC1::atomic_cmpxchg_at_resolved(access, cmp_value, new_value);\n+\n+  if (access.is_oop()) {\n+    post_barrier(access, access.resolved_addr(), new_value.result());\n+  }\n+\n+  return result;\n+}\n+\n+LIR_Opr CardTableBarrierSetC1::atomic_xchg_at_resolved(LIRAccess& access, LIRItem& value) {\n+  if (access.is_oop()) {\n+    pre_barrier(access, access.resolved_addr(),\n+                LIR_OprFact::illegalOpr \/* pre_val *\/, nullptr);\n+  }\n+\n+  LIR_Opr result = BarrierSetC1::atomic_xchg_at_resolved(access, value);\n+\n+  if (access.is_oop()) {\n+    post_barrier(access, access.resolved_addr(), value.result());\n+  }\n+\n+  return result;\n+}\n+\n+\/\/ This overrides the default to resolve the address into a register,\n+\/\/ assuming it will be used by a write barrier anyway.\n+LIR_Opr CardTableBarrierSetC1::resolve_address(LIRAccess& access, bool resolve_in_register) {\n+  DecoratorSet decorators = access.decorators();\n+  bool needs_patching = (decorators & C1_NEEDS_PATCHING) != 0;\n+  bool is_write = (decorators & ACCESS_WRITE) != 0;\n+  bool is_array = (decorators & IS_ARRAY) != 0;\n+  bool on_anonymous = (decorators & ON_UNKNOWN_OOP_REF) != 0;\n+  bool precise = is_array || on_anonymous;\n+  resolve_in_register |= !needs_patching && is_write && access.is_oop() && precise;\n+  return BarrierSetC1::resolve_address(access, resolve_in_register);\n+}\n+\n","filename":"src\/hotspot\/share\/gc\/shared\/c1\/cardTableBarrierSetC1.cpp","additions":103,"deletions":0,"binary":false,"changes":103,"status":"modified"},{"patch":"@@ -38,0 +38,101 @@\n+Node* CardTableBarrierSetC2::store_at_resolved(C2Access& access, C2AccessValue& val) const {\n+  DecoratorSet decorators = access.decorators();\n+\n+  Node* adr = access.addr().node();\n+\n+  bool is_array = (decorators & IS_ARRAY) != 0;\n+  bool anonymous = (decorators & ON_UNKNOWN_OOP_REF) != 0;\n+  bool in_heap = (decorators & IN_HEAP) != 0;\n+  bool use_precise = is_array || anonymous;\n+  bool tightly_coupled_alloc = (decorators & C2_TIGHTLY_COUPLED_ALLOC) != 0;\n+\n+  const InlineTypeNode* vt = nullptr;\n+  if (access.is_parse_access() && static_cast<C2ParseAccess&>(access).vt() != nullptr) {\n+    vt = static_cast<C2ParseAccess&>(access).vt();\n+  }\n+\n+  if (vt == nullptr && (!access.is_oop() || tightly_coupled_alloc || (!in_heap && !anonymous))) {\n+    return BarrierSetC2::store_at_resolved(access, val);\n+  }\n+\n+  assert(access.is_parse_access(), \"entry not supported at optimization time\");\n+  C2ParseAccess& parse_access = static_cast<C2ParseAccess&>(access);\n+\n+  Node* store = BarrierSetC2::store_at_resolved(access, val);\n+  \/\/ TODO 8350865\n+  \/\/ - We actually only need the post barrier once for non-arrays (same for C1, right)?\n+  \/\/ - Value is only needed to determine if we are storing null. Maybe we can go with a simple boolean?\n+  GraphKit* kit = parse_access.kit();\n+  if (vt != nullptr) {\n+    for (uint i = 0; i < vt->field_count(); ++i) {\n+      ciType* type = vt->field_type(i);\n+      if (!type->is_primitive_type()) {\n+        ciInlineKlass* vk = vt->bottom_type()->inline_klass();\n+        int field_offset = vt->field_offset(i) - vk->payload_offset();\n+        Node* value = vt->field_value(i);\n+        Node* field_adr = kit->basic_plus_adr(access.base(), adr, field_offset);\n+        post_barrier(kit, access.base(), field_adr, value, use_precise);\n+      }\n+    }\n+  } else {\n+    post_barrier(kit, access.base(), adr, val.node(), use_precise);\n+  }\n+\n+  return store;\n+}\n+\n+Node* CardTableBarrierSetC2::atomic_cmpxchg_val_at_resolved(C2AtomicParseAccess& access, Node* expected_val,\n+                                                            Node* new_val, const Type* value_type) const {\n+  if (!access.is_oop()) {\n+    return BarrierSetC2::atomic_cmpxchg_val_at_resolved(access, expected_val, new_val, value_type);\n+  }\n+\n+  Node* result = BarrierSetC2::atomic_cmpxchg_val_at_resolved(access, expected_val, new_val, value_type);\n+\n+  post_barrier(access.kit(), access.base(), access.addr().node(), new_val, true);\n+\n+  return result;\n+}\n+\n+Node* CardTableBarrierSetC2::atomic_cmpxchg_bool_at_resolved(C2AtomicParseAccess& access, Node* expected_val,\n+                                                             Node* new_val, const Type* value_type) const {\n+  GraphKit* kit = access.kit();\n+\n+  if (!access.is_oop()) {\n+    return BarrierSetC2::atomic_cmpxchg_bool_at_resolved(access, expected_val, new_val, value_type);\n+  }\n+\n+  Node* load_store = BarrierSetC2::atomic_cmpxchg_bool_at_resolved(access, expected_val, new_val, value_type);\n+\n+  \/\/ Emit the post barrier only when the actual store happened. This makes sense\n+  \/\/ to check only for LS_cmp_* that can fail to set the value.\n+  \/\/ LS_cmp_exchange does not produce any branches by default, so there is no\n+  \/\/ boolean result to piggyback on. TODO: When we merge CompareAndSwap with\n+  \/\/ CompareAndExchange and move branches here, it would make sense to conditionalize\n+  \/\/ post_barriers for LS_cmp_exchange as well.\n+  \/\/\n+  \/\/ CAS success path is marked more likely since we anticipate this is a performance\n+  \/\/ critical path, while CAS failure path can use the penalty for going through unlikely\n+  \/\/ path as backoff. Which is still better than doing a store barrier there.\n+  IdealKit ideal(kit);\n+  ideal.if_then(load_store, BoolTest::ne, ideal.ConI(0), PROB_STATIC_FREQUENT); {\n+    kit->sync_kit(ideal);\n+    post_barrier(kit, access.base(), access.addr().node(), new_val, true);\n+    ideal.sync_kit(kit);\n+  } ideal.end_if();\n+  kit->final_sync(ideal);\n+\n+  return load_store;\n+}\n+\n+Node* CardTableBarrierSetC2::atomic_xchg_at_resolved(C2AtomicParseAccess& access, Node* new_val, const Type* value_type) const {\n+  Node* result = BarrierSetC2::atomic_xchg_at_resolved(access, new_val, value_type);\n+  if (!access.is_oop()) {\n+    return result;\n+  }\n+\n+  post_barrier(access.kit(), access.base(), access.addr().node(), new_val, true);\n+\n+  return result;\n+}\n+\n","filename":"src\/hotspot\/share\/gc\/shared\/c2\/cardTableBarrierSetC2.cpp","additions":101,"deletions":0,"binary":false,"changes":101,"status":"modified"},{"patch":"@@ -28,1 +28,1 @@\n-#include \"gc\/shared\/c2\/modRefBarrierSetC2.hpp\"\n+#include \"gc\/shared\/c2\/barrierSetC2.hpp\"\n@@ -30,1 +30,1 @@\n-class CardTableBarrierSetC2: public ModRefBarrierSetC2 {\n+class CardTableBarrierSetC2: public BarrierSetC2 {\n@@ -38,0 +38,8 @@\n+  virtual Node* store_at_resolved(C2Access& access, C2AccessValue& val) const;\n+\n+  virtual Node* atomic_cmpxchg_val_at_resolved(C2AtomicParseAccess& access, Node* expected_val,\n+                                               Node* new_val, const Type* value_type) const;\n+  virtual Node* atomic_cmpxchg_bool_at_resolved(C2AtomicParseAccess& access, Node* expected_val,\n+                                                Node* new_val, const Type* value_type) const;\n+  virtual Node* atomic_xchg_at_resolved(C2AtomicParseAccess& access, Node* new_val, const Type* value_type) const;\n+\n","filename":"src\/hotspot\/share\/gc\/shared\/c2\/cardTableBarrierSetC2.hpp","additions":10,"deletions":2,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -28,0 +28,1 @@\n+#include \"gc\/shared\/barrierSet.hpp\"\n@@ -29,1 +30,1 @@\n-#include \"gc\/shared\/modRefBarrierSet.hpp\"\n+#include \"memory\/memRegion.hpp\"\n@@ -44,1 +45,1 @@\n-class CardTableBarrierSet: public ModRefBarrierSet {\n+class CardTableBarrierSet: public BarrierSet {\n@@ -62,1 +63,2 @@\n-  CardTable* card_table() const { return _card_table; }\n+  template <DecoratorSet decorators, typename T>\n+  inline void write_ref_field_pre(T* addr) {}\n@@ -69,1 +71,1 @@\n-  void write_ref_field_post(T* field);\n+  inline void write_ref_field_post(T *addr);\n@@ -71,0 +73,1 @@\n+  \/\/ Causes all refs in \"mr\" to be assumed to be modified (by this JavaThread).\n@@ -73,0 +76,14 @@\n+  \/\/ Operations on arrays, or general regions (e.g., for \"clone\") may be\n+  \/\/ optimized by some barriers.\n+\n+  \/\/ Below length is the # array elements being written\n+  virtual void write_ref_array_pre(oop* dst, size_t length,\n+                                   bool dest_uninitialized) {}\n+  virtual void write_ref_array_pre(narrowOop* dst, size_t length,\n+                                   bool dest_uninitialized) {}\n+  \/\/ Below count is the # array elements being written, starting\n+  \/\/ at the address \"start\", which may not necessarily be HeapWord-aligned\n+  inline void write_ref_array(HeapWord* start, size_t count);\n+\n+  CardTable* card_table() const { return _card_table; }\n+\n@@ -78,1 +95,37 @@\n-  class AccessBarrier: public ModRefBarrierSet::AccessBarrier<decorators, BarrierSetT> {};\n+  class AccessBarrier: public BarrierSet::AccessBarrier<decorators, BarrierSetT> {\n+    typedef BarrierSet::AccessBarrier<decorators, BarrierSetT> Raw;\n+\n+  public:\n+    template <typename T>\n+    static void oop_store_in_heap(T* addr, oop value);\n+    template <typename T>\n+    static oop oop_atomic_cmpxchg_in_heap(T* addr, oop compare_value, oop new_value);\n+    template <typename T>\n+    static oop oop_atomic_xchg_in_heap(T* addr, oop new_value);\n+\n+    template <typename T>\n+    static void oop_arraycopy_in_heap(arrayOop src_obj, size_t src_offset_in_bytes, T* src_raw,\n+                                      arrayOop dst_obj, size_t dst_offset_in_bytes, T* dst_raw,\n+                                      size_t length);\n+  private:\n+    \/\/ Failing checkcast or check null during copy, still needs barrier\n+    template <typename T>\n+    static inline void oop_arraycopy_partial_barrier(BarrierSetT *bs, T* dst_raw, T* p);\n+  public:\n+\n+    static void clone_in_heap(oop src, oop dst, size_t size);\n+\n+    static void oop_store_in_heap_at(oop base, ptrdiff_t offset, oop value) {\n+      oop_store_in_heap(AccessInternal::oop_field_addr<decorators>(base, offset), value);\n+    }\n+\n+    static oop oop_atomic_xchg_in_heap_at(oop base, ptrdiff_t offset, oop new_value) {\n+      return oop_atomic_xchg_in_heap(AccessInternal::oop_field_addr<decorators>(base, offset), new_value);\n+    }\n+\n+    static oop oop_atomic_cmpxchg_in_heap_at(oop base, ptrdiff_t offset, oop compare_value, oop new_value) {\n+      return oop_atomic_cmpxchg_in_heap(AccessInternal::oop_field_addr<decorators>(base, offset), compare_value, new_value);\n+    }\n+\n+    static void value_copy_in_heap(void* src, void* dst, InlineKlass* md, LayoutKind lk);\n+  };\n","filename":"src\/hotspot\/share\/gc\/shared\/cardTableBarrierSet.hpp","additions":58,"deletions":5,"binary":false,"changes":63,"status":"modified"},{"patch":"@@ -30,0 +30,1 @@\n+#include \"gc\/shared\/barrierSet.hpp\"\n@@ -31,1 +32,4 @@\n-#include \"runtime\/atomicAccess.hpp\"\n+#include \"oops\/compressedOops.inline.hpp\"\n+#include \"oops\/inlineKlass.inline.hpp\"\n+#include \"oops\/objArrayOop.hpp\"\n+#include \"oops\/oop.hpp\"\n@@ -39,0 +43,160 @@\n+class Klass;\n+\n+\/\/ count is number of array elements being written\n+void CardTableBarrierSet::write_ref_array(HeapWord* start, size_t count) {\n+  HeapWord* end = (HeapWord*)((char*)start + (count*heapOopSize));\n+  \/\/ In the case of compressed oops, start and end may potentially be misaligned;\n+  \/\/ so we need to conservatively align the first downward (this is not\n+  \/\/ strictly necessary for current uses, but a case of good hygiene and,\n+  \/\/ if you will, aesthetics) and the second upward (this is essential for\n+  \/\/ current uses) to a HeapWord boundary, so we mark all cards overlapping\n+  \/\/ this write. If this evolves in the future to calling a\n+  \/\/ logging barrier of narrow oop granularity, like the pre-barrier for G1\n+  \/\/ (mentioned here merely by way of example), we will need to change this\n+  \/\/ interface, so it is \"exactly precise\" (if i may be allowed the adverbial\n+  \/\/ redundancy for emphasis) and does not include narrow oop slots not\n+  \/\/ included in the original write interval.\n+  HeapWord* aligned_start = align_down(start, HeapWordSize);\n+  HeapWord* aligned_end   = align_up  (end,   HeapWordSize);\n+  \/\/ If compressed oops were not being used, these should already be aligned\n+  assert(UseCompressedOops || (aligned_start == start && aligned_end == end),\n+         \"Expected heap word alignment of start and end\");\n+  write_region(MemRegion(aligned_start, aligned_end));\n+}\n+\n+template <DecoratorSet decorators, typename BarrierSetT>\n+template <typename T>\n+inline void CardTableBarrierSet::AccessBarrier<decorators, BarrierSetT>::\n+oop_store_in_heap(T* addr, oop value) {\n+  BarrierSetT *bs = barrier_set_cast<BarrierSetT>(barrier_set());\n+  bs->template write_ref_field_pre<decorators>(addr);\n+  Raw::oop_store(addr, value);\n+  bs->template write_ref_field_post<decorators>(addr);\n+}\n+\n+template <DecoratorSet decorators, typename BarrierSetT>\n+template <typename T>\n+inline oop CardTableBarrierSet::AccessBarrier<decorators, BarrierSetT>::\n+oop_atomic_cmpxchg_in_heap(T* addr, oop compare_value, oop new_value) {\n+  BarrierSetT *bs = barrier_set_cast<BarrierSetT>(barrier_set());\n+  bs->template write_ref_field_pre<decorators>(addr);\n+  oop result = Raw::oop_atomic_cmpxchg(addr, compare_value, new_value);\n+  if (result == compare_value) {\n+    bs->template write_ref_field_post<decorators>(addr);\n+  }\n+  return result;\n+}\n+\n+template <DecoratorSet decorators, typename BarrierSetT>\n+template <typename T>\n+inline oop CardTableBarrierSet::AccessBarrier<decorators, BarrierSetT>::\n+oop_atomic_xchg_in_heap(T* addr, oop new_value) {\n+  BarrierSetT *bs = barrier_set_cast<BarrierSetT>(barrier_set());\n+  bs->template write_ref_field_pre<decorators>(addr);\n+  oop result = Raw::oop_atomic_xchg(addr, new_value);\n+  bs->template write_ref_field_post<decorators>(addr);\n+  return result;\n+}\n+\n+template <DecoratorSet decorators, typename BarrierSetT>\n+template <typename T>\n+inline void CardTableBarrierSet::AccessBarrier<decorators, BarrierSetT>::\n+oop_arraycopy_partial_barrier(BarrierSetT *bs, T* dst_raw, T* p) {\n+  const size_t pd = pointer_delta(p, dst_raw, (size_t)heapOopSize);\n+  \/\/ pointer delta is scaled to number of elements (length field in\n+  \/\/ objArrayOop) which we assume is 32 bit.\n+  assert(pd == (size_t)(int)pd, \"length field overflow\");\n+  bs->write_ref_array((HeapWord*)dst_raw, pd);\n+}\n+\n+template <DecoratorSet decorators, typename BarrierSetT>\n+template <typename T>\n+inline void CardTableBarrierSet::AccessBarrier<decorators, BarrierSetT>::\n+oop_arraycopy_in_heap(arrayOop src_obj, size_t src_offset_in_bytes, T* src_raw,\n+                      arrayOop dst_obj, size_t dst_offset_in_bytes, T* dst_raw,\n+                      size_t length) {\n+  BarrierSetT *bs = barrier_set_cast<BarrierSetT>(barrier_set());\n+\n+  src_raw = arrayOopDesc::obj_offset_to_raw(src_obj, src_offset_in_bytes, src_raw);\n+  dst_raw = arrayOopDesc::obj_offset_to_raw(dst_obj, dst_offset_in_bytes, dst_raw);\n+\n+  if ((!HasDecorator<decorators, ARRAYCOPY_CHECKCAST>::value) &&\n+      (!HasDecorator<decorators, ARRAYCOPY_NOTNULL>::value)) {\n+    \/\/ Optimized covariant case\n+    bs->write_ref_array_pre(dst_raw, length,\n+                            HasDecorator<decorators, IS_DEST_UNINITIALIZED>::value);\n+    Raw::oop_arraycopy(nullptr, 0, src_raw, nullptr, 0, dst_raw, length);\n+    bs->write_ref_array((HeapWord*)dst_raw, length);\n+  } else {\n+    assert(dst_obj != nullptr, \"better have an actual oop\");\n+    Klass* bound = objArrayOop(dst_obj)->element_klass();\n+    T* from = const_cast<T*>(src_raw);\n+    T* end = from + length;\n+    for (T* p = dst_raw; from < end; from++, p++) {\n+      T element = *from;\n+      \/\/ Apply any required checks\n+      if (HasDecorator<decorators, ARRAYCOPY_NOTNULL>::value && CompressedOops::is_null(element)) {\n+        oop_arraycopy_partial_barrier(bs, dst_raw, p);\n+        throw_array_null_pointer_store_exception(src_obj, dst_obj, JavaThread::current());\n+        return;\n+      }\n+      if (HasDecorator<decorators, ARRAYCOPY_CHECKCAST>::value &&\n+          (!oopDesc::is_instanceof_or_null(CompressedOops::decode(element), bound))) {\n+        oop_arraycopy_partial_barrier(bs, dst_raw, p);\n+        throw_array_store_exception(src_obj, dst_obj, JavaThread::current());\n+        return;\n+      }\n+      \/\/ write\n+      bs->template write_ref_field_pre<decorators>(p);\n+      *p = element;\n+    }\n+    bs->write_ref_array((HeapWord*)dst_raw, length);\n+  }\n+}\n+\n+template <DecoratorSet decorators, typename BarrierSetT>\n+inline void CardTableBarrierSet::AccessBarrier<decorators, BarrierSetT>::\n+clone_in_heap(oop src, oop dst, size_t size) {\n+  Raw::clone(src, dst, size);\n+  BarrierSetT *bs = barrier_set_cast<BarrierSetT>(barrier_set());\n+  bs->write_region(MemRegion((HeapWord*)(void*)dst, size));\n+}\n+\n+template <DecoratorSet decorators, typename BarrierSetT>\n+inline void CardTableBarrierSet::AccessBarrier<decorators, BarrierSetT>::\n+value_copy_in_heap(void* src, void* dst, InlineKlass* md, LayoutKind lk) {\n+  if (!md->contains_oops()) {\n+    \/\/ If we do not have oops in the flat array, we can just do a raw copy.\n+    Raw::value_copy(src, dst, md, lk);\n+  } else {\n+    BarrierSetT* bs = barrier_set_cast<BarrierSetT>(BarrierSet::barrier_set());\n+    \/\/ src\/dst aren't oops, need offset to adjust oop map offset\n+    const address dst_oop_addr_offset = ((address) dst) - md->payload_offset();\n+    typedef typename ValueOopType<decorators>::type OopType;\n+\n+    \/\/ Pre-barriers...\n+    OopMapBlock* map = md->start_of_nonstatic_oop_maps();\n+    OopMapBlock* const end = map + md->nonstatic_oop_map_count();\n+    bool is_uninitialized = HasDecorator<decorators, IS_DEST_UNINITIALIZED>::value;\n+    while (map != end) {\n+      address doop_address = dst_oop_addr_offset + map->offset();\n+      \/\/ The pre-barrier only impacts G1, which will emit a barrier if the destination is\n+      \/\/ initialized. Note that we should not emit a barrier if the destination is uninitialized,\n+      \/\/ as doing so will fill the SATB queue with garbage data.\n+      bs->write_ref_array_pre((OopType*) doop_address, map->count(), is_uninitialized);\n+      map++;\n+    }\n+\n+    Raw::value_copy(src, dst, md, lk);\n+\n+    \/\/ Post-barriers...\n+    map = md->start_of_nonstatic_oop_maps();\n+    while (map != end) {\n+      address doop_address = dst_oop_addr_offset + map->offset();\n+      \/\/ The post-barrier needs to be called for initialized and uninitialized destinations.\n+      bs->write_ref_array((HeapWord*) doop_address, map->count());\n+      map++;\n+    }\n+  }\n+}\n+\n","filename":"src\/hotspot\/share\/gc\/shared\/cardTableBarrierSet.inline.hpp","additions":165,"deletions":1,"binary":false,"changes":166,"status":"modified"},{"patch":"@@ -171,1 +171,0 @@\n-  DEBUG_ONLY(static void fill_args_check(HeapWord* start, size_t words);)\n@@ -314,3 +313,0 @@\n-  static void fill_with_object(MemRegion region, bool zap = true) {\n-    fill_with_object(region.start(), region.word_size(), zap);\n-  }\n@@ -349,1 +345,1 @@\n-  virtual size_t tlab_capacity(Thread *thr) const = 0;\n+  virtual size_t tlab_capacity() const = 0;\n@@ -351,2 +347,2 @@\n-  \/\/ The amount of used space for thread-local allocation buffers for the given thread.\n-  virtual size_t tlab_used(Thread *thr) const = 0;\n+  \/\/ The amount of space used for thread-local allocation buffers.\n+  virtual size_t tlab_used() const = 0;\n@@ -359,1 +355,1 @@\n-  virtual size_t unsafe_max_tlab_alloc(Thread *thr) const = 0;\n+  virtual size_t unsafe_max_tlab_alloc() const = 0;\n","filename":"src\/hotspot\/share\/gc\/shared\/collectedHeap.hpp","additions":4,"deletions":8,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -227,1 +227,1 @@\n-  klass->initialize(CHECK);\n+  klass->initialize_preemptable(CHECK_AND_CLEAR_PREEMPTED);\n@@ -726,1 +726,2 @@\n-void InterpreterRuntime::resolve_get_put(JavaThread* current, Bytecodes::Code bytecode) {\n+void InterpreterRuntime::resolve_get_put(Bytecodes::Code bytecode, TRAPS) {\n+  JavaThread* current = THREAD;\n@@ -731,1 +732,1 @@\n-  resolve_get_put(bytecode, last_frame.get_index_u2(bytecode), m, pool, true \/*initialize_holder*\/, current);\n+  resolve_get_put(bytecode, last_frame.get_index_u2(bytecode), m, pool, ClassInitMode::init_preemptable, THREAD);\n@@ -737,1 +738,1 @@\n-                                         bool initialize_holder, TRAPS) {\n+                                         ClassInitMode init_mode, TRAPS) {\n@@ -745,2 +746,1 @@\n-    LinkResolver::resolve_field_access(info, pool, field_index,\n-                                       m, bytecode, initialize_holder, CHECK);\n+    LinkResolver::resolve_field_access(info, pool, field_index, m, bytecode, init_mode, CHECK);\n@@ -904,1 +904,2 @@\n-void InterpreterRuntime::resolve_invoke(JavaThread* current, Bytecodes::Code bytecode) {\n+void InterpreterRuntime::resolve_invoke(Bytecodes::Code bytecode, TRAPS) {\n+  JavaThread* current = THREAD;\n@@ -932,1 +933,0 @@\n-    JavaThread* THREAD = current; \/\/ For exception macros.\n@@ -935,1 +935,1 @@\n-                                 THREAD);\n+                                 ClassInitMode::init_preemptable, THREAD);\n@@ -1043,1 +1043,2 @@\n-void InterpreterRuntime::resolve_invokehandle(JavaThread* current) {\n+void InterpreterRuntime::resolve_invokehandle(TRAPS) {\n+  JavaThread* current = THREAD;\n@@ -1072,1 +1073,2 @@\n-void InterpreterRuntime::resolve_invokedynamic(JavaThread* current) {\n+void InterpreterRuntime::resolve_invokedynamic(TRAPS) {\n+  JavaThread* current = THREAD;\n@@ -1107,1 +1109,1 @@\n-    resolve_get_put(current, bytecode);\n+    resolve_get_put(bytecode, CHECK_AND_CLEAR_PREEMPTED);\n@@ -1113,1 +1115,1 @@\n-    resolve_invoke(current, bytecode);\n+    resolve_invoke(bytecode, CHECK_AND_CLEAR_PREEMPTED);\n@@ -1116,1 +1118,1 @@\n-    resolve_invokehandle(current);\n+    resolve_invokehandle(THREAD);\n@@ -1119,1 +1121,1 @@\n-    resolve_invokedynamic(current);\n+    resolve_invokedynamic(THREAD);\n@@ -1639,0 +1641,8 @@\n+\n+#ifdef ASSERT\n+bool InterpreterRuntime::is_preemptable_call(address entry_point) {\n+  return entry_point == CAST_FROM_FN_PTR(address, InterpreterRuntime::monitorenter) ||\n+         entry_point == CAST_FROM_FN_PTR(address, InterpreterRuntime::resolve_from_cache) ||\n+         entry_point == CAST_FROM_FN_PTR(address, InterpreterRuntime::_new);\n+}\n+#endif \/\/ ASSERT\n","filename":"src\/hotspot\/share\/interpreter\/interpreterRuntime.cpp","additions":25,"deletions":15,"binary":false,"changes":40,"status":"modified"},{"patch":"@@ -106,1 +106,1 @@\n-                              methodHandle& m, constantPoolHandle& pool, bool initialize_holder, TRAPS);\n+                              methodHandle& m, constantPoolHandle& pool, ClassInitMode init_mode, TRAPS);\n@@ -115,1 +115,1 @@\n-  static void resolve_get_put(JavaThread* current, Bytecodes::Code bytecode);\n+  static void resolve_get_put(Bytecodes::Code bytecode, TRAPS);\n@@ -118,3 +118,3 @@\n-  static void resolve_invoke(JavaThread* current, Bytecodes::Code bytecode);\n-  static void resolve_invokehandle (JavaThread* current);\n-  static void resolve_invokedynamic(JavaThread* current);\n+  static void resolve_invoke(Bytecodes::Code bytecode, TRAPS);\n+  static void resolve_invokehandle (TRAPS);\n+  static void resolve_invokedynamic(TRAPS);\n@@ -183,0 +183,3 @@\n+\n+  \/\/ Virtual Thread Preemption\n+  DEBUG_ONLY(static bool is_preemptable_call(address entry_point);)\n","filename":"src\/hotspot\/share\/interpreter\/interpreterRuntime.hpp","additions":8,"deletions":5,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -989,1 +989,1 @@\n-                                        bool initialize_class, TRAPS) {\n+                                        ClassInitMode init_mode, TRAPS) {\n@@ -991,1 +991,1 @@\n-  resolve_field(fd, link_info, byte, initialize_class, CHECK);\n+  resolve_field(fd, link_info, byte, init_mode, CHECK);\n@@ -996,1 +996,1 @@\n-                                 Bytecodes::Code byte, bool initialize_class,\n+                                 Bytecodes::Code byte, ClassInitMode init_mode,\n@@ -1081,2 +1081,6 @@\n-    if (is_static && initialize_class) {\n-      sel_klass->initialize(CHECK);\n+    if (is_static) {\n+      if (init_mode == ClassInitMode::init) {\n+        sel_klass->initialize(CHECK);\n+      } else if (init_mode == ClassInitMode::init_preemptable) {\n+        sel_klass->initialize_preemptable(CHECK);\n+      }\n@@ -1108,1 +1112,1 @@\n-                                       bool initialize_class, TRAPS) {\n+                                       ClassInitMode init_mode, TRAPS) {\n@@ -1115,2 +1119,6 @@\n-  if (initialize_class && resolved_klass->should_be_initialized()) {\n-    resolved_klass->initialize(CHECK);\n+  if (init_mode != ClassInitMode::dont_init && resolved_klass->should_be_initialized()) {\n+    if (init_mode == ClassInitMode::init) {\n+      resolved_klass->initialize(CHECK);\n+    } else if (init_mode == ClassInitMode::init_preemptable) {\n+      resolved_klass->initialize_preemptable(CHECK);\n+    }\n@@ -1131,1 +1139,1 @@\n-  resolve_static_call(result, link_info, \/*initialize_class*\/false, CHECK);\n+  resolve_static_call(result, link_info, ClassInitMode::dont_init, CHECK);\n@@ -1684,1 +1692,1 @@\n-  resolve_static_call(info, link_info, \/*initialize_class*\/false, THREAD);\n+  resolve_static_call(info, link_info, ClassInitMode::dont_init, THREAD);\n@@ -1708,1 +1716,1 @@\n-void LinkResolver::resolve_invoke(CallInfo& result, Handle recv, const constantPoolHandle& pool, int index, Bytecodes::Code byte, TRAPS) {\n+void LinkResolver::resolve_invoke(CallInfo& result, Handle recv, const constantPoolHandle& pool, int index, Bytecodes::Code byte, ClassInitMode init_mode, TRAPS) {\n@@ -1710,7 +1718,7 @@\n-    case Bytecodes::_invokestatic   : resolve_invokestatic   (result,       pool, index, CHECK); break;\n-    case Bytecodes::_invokespecial  : resolve_invokespecial  (result, recv, pool, index, CHECK); break;\n-    case Bytecodes::_invokevirtual  : resolve_invokevirtual  (result, recv, pool, index, CHECK); break;\n-    case Bytecodes::_invokehandle   : resolve_invokehandle   (result,       pool, index, CHECK); break;\n-    case Bytecodes::_invokedynamic  : resolve_invokedynamic  (result,       pool, index, CHECK); break;\n-    case Bytecodes::_invokeinterface: resolve_invokeinterface(result, recv, pool, index, CHECK); break;\n-    default                         :                                                            break;\n+    case Bytecodes::_invokestatic   : resolve_invokestatic   (result,       pool, index, init_mode, CHECK); break;\n+    case Bytecodes::_invokespecial  : resolve_invokespecial  (result, recv, pool, index,            CHECK); break;\n+    case Bytecodes::_invokevirtual  : resolve_invokevirtual  (result, recv, pool, index,            CHECK); break;\n+    case Bytecodes::_invokehandle   : resolve_invokehandle   (result,       pool, index,            CHECK); break;\n+    case Bytecodes::_invokedynamic  : resolve_invokedynamic  (result,       pool, index,            CHECK); break;\n+    case Bytecodes::_invokeinterface: resolve_invokeinterface(result, recv, pool, index,            CHECK); break;\n+    default                         :                                                                       break;\n@@ -1739,1 +1747,1 @@\n-      resolve_static_call(result, link_info, \/*initialize_class=*\/false, CHECK);\n+      resolve_static_call(result, link_info, ClassInitMode::dont_init, CHECK);\n@@ -1750,1 +1758,1 @@\n-void LinkResolver::resolve_invokestatic(CallInfo& result, const constantPoolHandle& pool, int index, TRAPS) {\n+void LinkResolver::resolve_invokestatic(CallInfo& result, const constantPoolHandle& pool, int index, ClassInitMode init_mode, TRAPS) {\n@@ -1752,1 +1760,1 @@\n-  resolve_static_call(result, link_info, \/*initialize_class*\/true, CHECK);\n+  resolve_static_call(result, link_info, init_mode, CHECK);\n","filename":"src\/hotspot\/share\/interpreter\/linkResolver.cpp","additions":29,"deletions":21,"binary":false,"changes":50,"status":"modified"},{"patch":"@@ -192,0 +192,6 @@\n+enum class ClassInitMode {\n+  dont_init,\n+  init,\n+  init_preemptable\n+};\n+\n@@ -270,1 +276,1 @@\n-                                      const constantPoolHandle& pool, int index, TRAPS);\n+                                      const constantPoolHandle& pool, int index, ClassInitMode mode, TRAPS);\n@@ -298,1 +304,1 @@\n-                                   bool initialize_class, TRAPS);\n+                                   ClassInitMode mode, TRAPS);\n@@ -304,2 +310,1 @@\n-    resolve_field_access(result, pool, index, method, byte,\n-                         \/* initialize_class*\/true, THREAD);\n+    resolve_field_access(result, pool, index, method, byte, ClassInitMode::init, THREAD);\n@@ -309,1 +314,1 @@\n-                            bool initialize_class, TRAPS);\n+                            ClassInitMode mode, TRAPS);\n@@ -313,1 +318,1 @@\n-                                     bool initialize_klass, TRAPS);\n+                                     ClassInitMode mode, TRAPS);\n@@ -356,1 +361,6 @@\n-                             Bytecodes::Code byte, TRAPS);\n+                             Bytecodes::Code byte, ClassInitMode static_mode, TRAPS);\n+  static void resolve_invoke(CallInfo& result, Handle recv,\n+                             const constantPoolHandle& pool, int index,\n+                             Bytecodes::Code byte, TRAPS) {\n+    resolve_invoke(result, recv, pool, index, byte, ClassInitMode::init, THREAD);\n+  }\n","filename":"src\/hotspot\/share\/interpreter\/linkResolver.hpp","additions":17,"deletions":7,"binary":false,"changes":24,"status":"modified"},{"patch":"@@ -1011,1 +1011,1 @@\n-  LinkResolver::resolve_field(fd, link_info, Bytecodes::java_code(code), false, CHECK_NULL);\n+  LinkResolver::resolve_field(fd, link_info, Bytecodes::java_code(code), ClassInitMode::dont_init, CHECK_NULL);\n","filename":"src\/hotspot\/share\/jvmci\/jvmciCompilerToVM.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -245,0 +245,1 @@\n+static BuiltinException _preempted_exception;\n@@ -265,0 +266,1 @@\n+oop Universe::preempted_exception_instance()      { return _preempted_exception.instance(); }\n@@ -324,0 +326,1 @@\n+  _preempted_exception.store_in_cds();\n@@ -343,0 +346,1 @@\n+    _preempted_exception.load_from_cds();\n@@ -362,0 +366,1 @@\n+  _preempted_exception.serialize(f);\n@@ -1164,0 +1169,1 @@\n+  _preempted_exception.init_if_empty(vmSymbols::jdk_internal_vm_PreemptedException(), CHECK_false);\n","filename":"src\/hotspot\/share\/memory\/universe.cpp","additions":6,"deletions":0,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -248,0 +248,1 @@\n+  static oop          preempted_exception_instance();\n","filename":"src\/hotspot\/share\/memory\/universe.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -228,1 +228,1 @@\n-        FOR_EACH_CONCRETE_BARRIER_SET_DO(BARRIER_SET_RESOLVE_BARRIER_CLOSURE)\n+        FOR_EACH_BARRIER_SET_DO(BARRIER_SET_RESOLVE_BARRIER_CLOSURE)\n@@ -251,1 +251,1 @@\n-        FOR_EACH_CONCRETE_BARRIER_SET_DO(BARRIER_SET_RESOLVE_BARRIER_CLOSURE)\n+        FOR_EACH_BARRIER_SET_DO(BARRIER_SET_RESOLVE_BARRIER_CLOSURE)\n","filename":"src\/hotspot\/share\/oops\/access.inline.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2017, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2017, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -28,0 +28,1 @@\n+#include \"cppstdlib\/type_traits.hpp\"\n@@ -38,2 +39,0 @@\n-#include <type_traits>\n-\n","filename":"src\/hotspot\/share\/oops\/accessBackend.hpp","additions":2,"deletions":3,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -30,0 +30,1 @@\n+#include \"cppstdlib\/type_traits.hpp\"\n@@ -38,2 +39,0 @@\n-#include <type_traits>\n-\n","filename":"src\/hotspot\/share\/oops\/accessBackend.inline.hpp","additions":1,"deletions":2,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2018, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2018, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -28,0 +28,1 @@\n+#include \"cppstdlib\/type_traits.hpp\"\n@@ -32,2 +33,0 @@\n-#include <type_traits>\n-\n","filename":"src\/hotspot\/share\/oops\/accessDecorators.hpp","additions":2,"deletions":3,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -869,1 +869,1 @@\n-\/\/ Set the initialization lock to null so the object can be GC'ed.  Any racing\n+\/\/ Set the initialization lock to null so the object can be GC'ed. Any racing\n@@ -872,1 +872,2 @@\n-\/\/ the lock and return.\n+\/\/ the lock and return. For preempted vthreads we keep the oop protected\n+\/\/ in the ObjectMonitor (see ObjectMonitor::set_object_strong()).\n@@ -880,0 +881,25 @@\n+class PreemptableInitCall {\n+  JavaThread* _thread;\n+  bool _previous;\n+  DEBUG_ONLY(InstanceKlass* _previous_klass;)\n+ public:\n+  PreemptableInitCall(JavaThread* thread, InstanceKlass* ik) : _thread(thread) {\n+    _previous = thread->at_preemptable_init();\n+    _thread->set_at_preemptable_init(true);\n+    DEBUG_ONLY(_previous_klass = _thread->preempt_init_klass();)\n+    DEBUG_ONLY(_thread->set_preempt_init_klass(ik));\n+  }\n+  ~PreemptableInitCall() {\n+    _thread->set_at_preemptable_init(_previous);\n+    DEBUG_ONLY(_thread->set_preempt_init_klass(_previous_klass));\n+  }\n+};\n+\n+void InstanceKlass::initialize_preemptable(TRAPS) {\n+  if (this->should_be_initialized()) {\n+    PreemptableInitCall pic(THREAD, this);\n+    initialize_impl(THREAD);\n+  } else {\n+    assert(is_initialized(), \"sanity check\");\n+  }\n+}\n@@ -1101,1 +1127,6 @@\n-    ObjectLocker ol(h_init_lock, jt);\n+    ObjectLocker ol(h_init_lock, CHECK_PREEMPTABLE_false);\n+    \/\/ Don't allow preemption if we link\/initialize classes below,\n+    \/\/ since that would release this monitor while we are in the\n+    \/\/ middle of linking this class.\n+    NoPreemptMark npm(THREAD);\n+\n@@ -1295,0 +1326,11 @@\n+class ThreadWaitingForClassInit : public StackObj {\n+  JavaThread* _thread;\n+ public:\n+  ThreadWaitingForClassInit(JavaThread* thread, InstanceKlass* ik) : _thread(thread) {\n+    _thread->set_class_to_be_initialized(ik);\n+  }\n+  ~ThreadWaitingForClassInit() {\n+    _thread->set_class_to_be_initialized(nullptr);\n+  }\n+};\n+\n@@ -1314,1 +1356,1 @@\n-    ObjectLocker ol(h_init_lock, jt);\n+    ObjectLocker ol(h_init_lock, CHECK_PREEMPTABLE);\n@@ -1327,3 +1369,2 @@\n-      jt->set_class_to_be_initialized(this);\n-      ol.wait_uninterruptibly(jt);\n-      jt->set_class_to_be_initialized(nullptr);\n+      ThreadWaitingForClassInit twcl(THREAD, this);\n+      ol.wait_uninterruptibly(CHECK_PREEMPTABLE);\n@@ -1387,0 +1428,4 @@\n+  \/\/ Block preemption once we are the initializer thread. Unmounting now\n+  \/\/ would complicate the reentrant case (identity is platform thread).\n+  NoPreemptMark npm(THREAD);\n+\n","filename":"src\/hotspot\/share\/oops\/instanceKlass.cpp","additions":52,"deletions":7,"binary":false,"changes":59,"status":"modified"},{"patch":"@@ -640,0 +640,1 @@\n+  void initialize_preemptable(TRAPS);\n","filename":"src\/hotspot\/share\/oops\/instanceKlass.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -255,0 +255,4 @@\n+void Klass::initialize_preemptable(TRAPS) {\n+  ShouldNotReachHere();\n+}\n+\n","filename":"src\/hotspot\/share\/oops\/klass.cpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -605,0 +605,1 @@\n+  virtual void initialize_preemptable(TRAPS);\n","filename":"src\/hotspot\/share\/oops\/klass.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -28,1 +28,1 @@\n-#include \"layoutKind.hpp\"\n+#include \"cppstdlib\/type_traits.hpp\"\n@@ -31,0 +31,1 @@\n+#include \"oops\/layoutKind.hpp\"\n@@ -35,2 +36,0 @@\n-#include <type_traits>\n-\n","filename":"src\/hotspot\/share\/oops\/markWord.hpp","additions":2,"deletions":3,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -28,0 +28,1 @@\n+#include \"cppstdlib\/type_traits.hpp\"\n@@ -31,2 +32,0 @@\n-#include <type_traits>\n-\n","filename":"src\/hotspot\/share\/oops\/objArrayOop.hpp","additions":1,"deletions":2,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -28,0 +28,1 @@\n+#include \"cppstdlib\/type_traits.hpp\"\n@@ -39,2 +40,0 @@\n-#include <type_traits>\n-\n","filename":"src\/hotspot\/share\/oops\/oop.hpp","additions":1,"deletions":2,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -28,0 +28,1 @@\n+#include \"cppstdlib\/type_traits.hpp\"\n@@ -31,2 +32,0 @@\n-#include <type_traits>\n-\n","filename":"src\/hotspot\/share\/oops\/oopsHierarchy.hpp","additions":2,"deletions":3,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -28,0 +28,1 @@\n+#include \"cppstdlib\/type_traits.hpp\"\n@@ -31,2 +32,0 @@\n-#include <type_traits>\n-\n","filename":"src\/hotspot\/share\/oops\/typeArrayOop.hpp","additions":2,"deletions":3,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -2411,1 +2411,1 @@\n-        tty->print_cr(\"0xBADB100D   +VerifyStackAtCalls\");\n+        tty->print_cr(\"<Majik cookie>   +VerifyStackAtCalls\");\n","filename":"src\/hotspot\/share\/opto\/chaitin.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -279,0 +279,1 @@\n+macro(NarrowMemProj)\n","filename":"src\/hotspot\/share\/opto\/classes.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -328,1 +328,10 @@\n-  int                   _major_progress;        \/\/ Count of something big happening\n+  \/* If major progress is set:\n+   *   Marks that the loop tree information (get_ctrl, idom, get_loop, etc.) could be invalid, and we need to rebuild the loop tree.\n+   *   It also indicates that the graph was changed in a way that is promising to be able to apply more loop optimization.\n+   * If major progress is not set:\n+   *   Loop tree information is valid.\n+   *   If major progress is not set at the end of a loop opts phase, then we can stop loop opts, because we do not expect any further progress if we did more loop opts phases.\n+   *\n+   * This is not 100% accurate, the semantics of major progress has become less clear over time, but this is the general idea.\n+   *\/\n+  bool                  _major_progress;\n@@ -595,1 +604,0 @@\n-  int               major_progress() const      { return _major_progress; }\n@@ -602,3 +610,4 @@\n-  void          set_major_progress()            { _major_progress++; }\n-  void          restore_major_progress(int progress) { _major_progress += progress; }\n-  void        clear_major_progress()            { _major_progress = 0; }\n+  bool              major_progress() const      { return _major_progress; }\n+  void          set_major_progress()            { _major_progress = true; }\n+  void          restore_major_progress(bool progress) { _major_progress = _major_progress || progress; }\n+  void        clear_major_progress()            { _major_progress = false; }\n","filename":"src\/hotspot\/share\/opto\/compile.hpp","additions":14,"deletions":5,"binary":false,"changes":19,"status":"modified"},{"patch":"@@ -886,1 +886,1 @@\n-void ConnectionGraph::reduce_phi_on_castpp_field_load(Node* curr_castpp, GrowableArray<Node *>  &alloc_worklist, GrowableArray<Node *>  &memnode_worklist) {\n+void ConnectionGraph::reduce_phi_on_castpp_field_load(Node* curr_castpp, GrowableArray<Node*> &alloc_worklist) {\n@@ -1320,1 +1320,1 @@\n-void ConnectionGraph::reduce_phi(PhiNode* ophi, GrowableArray<Node *>  &alloc_worklist, GrowableArray<Node *>  &memnode_worklist) {\n+void ConnectionGraph::reduce_phi(PhiNode* ophi, GrowableArray<Node*> &alloc_worklist) {\n@@ -1347,1 +1347,1 @@\n-    reduce_phi_on_castpp_field_load(castpps.at(i), alloc_worklist, memnode_worklist);\n+    reduce_phi_on_castpp_field_load(castpps.at(i), alloc_worklist);\n@@ -4320,0 +4320,8 @@\n+#if 0  \/\/ TODO: Fix 8372259\n+        } else if (C->get_alias_index(result->adr_type()) != alias_idx) {\n+          assert(C->get_general_index(alias_idx) == C->get_alias_index(result->adr_type()), \"should be projection for the same field\/array element\");\n+          result = get_map(result->_idx);\n+          assert(result != nullptr, \"new projection should have been allocated\");\n+          break;\n+        }\n+#else\n@@ -4321,0 +4329,1 @@\n+#endif\n@@ -4616,0 +4625,16 @@\n+        \/\/ Add a new NarrowMem projection for each existing NarrowMem projection with new adr type\n+        InitializeNode* init = alloc->as_Allocate()->initialization();\n+        assert(init != nullptr, \"can't find Initialization node for this Allocate node\");\n+        auto process_narrow_proj = [&](NarrowMemProjNode* proj) {\n+          const TypePtr* adr_type = proj->adr_type();\n+          const TypePtr* new_adr_type = tinst->add_offset(adr_type->offset());\n+          if (adr_type != new_adr_type && !init->already_has_narrow_mem_proj_with_adr_type(new_adr_type)) {\n+            DEBUG_ONLY( uint alias_idx = _compile->get_alias_index(new_adr_type); )\n+            assert(_compile->get_general_index(alias_idx) == _compile->get_alias_index(adr_type), \"new adr type should be narrowed down from existing adr type\");\n+            NarrowMemProjNode* new_proj = new NarrowMemProjNode(init, new_adr_type);\n+            igvn->set_type(new_proj, new_proj->bottom_type());\n+            record_for_optimizer(new_proj);\n+            set_map(proj, new_proj); \/\/ record it so ConnectionGraph::find_inst_mem() can find it\n+          }\n+        };\n+        init->for_each_narrow_mem_proj_with_new_uses(process_narrow_proj);\n@@ -4687,1 +4712,1 @@\n-        reduce_phi(n->as_Phi(), alloc_worklist, memnode_worklist);\n+        reduce_phi(n->as_Phi(), alloc_worklist);\n@@ -4889,5 +4914,7 @@\n-    } else if (n->is_MemBar()) { \/\/ Initialize, MemBar nodes\n-      \/\/ we don't need to do anything, but the users must be pushed\n-      n = n->as_MemBar()->proj_out_or_null(TypeFunc::Memory);\n-      if (n == nullptr) {\n-        continue;\n+    } else if (n->is_MemBar()) { \/\/ MemBar nodes\n+      if (!n->is_Initialize()) { \/\/ memory projections for Initialize pushed below (so we get to all their uses)\n+        \/\/ we don't need to do anything, but the users must be pushed\n+        n = n->as_MemBar()->proj_out_or_null(TypeFunc::Memory);\n+        if (n == nullptr) {\n+          continue;\n+        }\n@@ -4913,0 +4940,2 @@\n+    } else if (n->is_Proj()) {\n+      assert(n->in(0)->is_Initialize(), \"we only push memory projections for Initialize\");\n@@ -4956,0 +4985,5 @@\n+      } else if (use->is_Proj()) {\n+        assert(n->is_Initialize(), \"We only push projections of Initialize\");\n+        if (use->as_Proj()->_con == TypeFunc::Memory) { \/\/ Ignore precedent edge\n+          memnode_worklist.append_if_missing(use);\n+        }\n@@ -5011,1 +5045,1 @@\n-        const Type *at = igvn->type(mem->in(MemNode::Address));\n+        const Type* at = igvn->type(mem->in(MemNode::Address));\n@@ -5131,1 +5165,1 @@\n-             n->is_AddP() || n->is_Phi(), \"unknown node used for set_map()\");\n+             n->is_AddP() || n->is_Phi() || n->is_NarrowMemProj(), \"unknown node used for set_map()\");\n","filename":"src\/hotspot\/share\/opto\/escape.cpp","additions":45,"deletions":11,"binary":false,"changes":56,"status":"modified"},{"patch":"@@ -568,1 +568,1 @@\n-                        \/\/ ChecCastPP    - allocation that this is a cast of\n+                        \/\/ CheckCastPP   - allocation that this is a cast of\n@@ -570,0 +570,2 @@\n+                        \/\/ NarrowMem     - newly created projection (type includes instance_id) from projection created\n+                        \/\/                 before EA\n@@ -614,1 +616,1 @@\n-  void reduce_phi_on_castpp_field_load(Node* castpp, GrowableArray<Node *>  &alloc_worklist, GrowableArray<Node *>  &memnode_worklist);\n+  void reduce_phi_on_castpp_field_load(Node* castpp, GrowableArray<Node*> &alloc_worklist);\n@@ -618,1 +620,1 @@\n-  void reduce_phi(PhiNode* ophi, GrowableArray<Node *>  &alloc_worklist, GrowableArray<Node *>  &memnode_worklist);\n+  void reduce_phi(PhiNode* ophi, GrowableArray<Node*> &alloc_worklist);\n","filename":"src\/hotspot\/share\/opto\/escape.hpp","additions":5,"deletions":3,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -4165,4 +4165,7 @@\n-    \/\/ Add an edge in the MergeMem for the header fields so an access\n-    \/\/ to one of those has correct memory state\n-    set_memory(minit_out, C->get_alias_index(oop_type->add_offset(oopDesc::mark_offset_in_bytes())));\n-    set_memory(minit_out, C->get_alias_index(oop_type->add_offset(oopDesc::klass_offset_in_bytes())));\n+    int mark_idx = C->get_alias_index(oop_type->add_offset(oopDesc::mark_offset_in_bytes()));\n+    \/\/ Add an edge in the MergeMem for the header fields so an access to one of those has correct memory state.\n+    \/\/ Use one NarrowMemProjNode per slice to properly record the adr type of each slice. The Initialize node will have\n+    \/\/ multiple projections as a result.\n+    set_memory(_gvn.transform(new NarrowMemProjNode(init, C->get_adr_type(mark_idx))), mark_idx);\n+    int klass_idx = C->get_alias_index(oop_type->add_offset(oopDesc::klass_offset_in_bytes()));\n+    set_memory(_gvn.transform(new NarrowMemProjNode(init, C->get_adr_type(klass_idx))), klass_idx);\n@@ -4196,1 +4199,1 @@\n-        hook_memory_on_init(*this, elemidx, minit_in, minit_out);\n+        hook_memory_on_init(*this, elemidx, minit_in, _gvn.transform(new NarrowMemProjNode(init, C->get_adr_type(elemidx))));\n@@ -4207,1 +4210,1 @@\n-        hook_memory_on_init(*this, fieldidx, minit_in, minit_out);\n+        hook_memory_on_init(*this, fieldidx, minit_in, _gvn.transform(new NarrowMemProjNode(init, C->get_adr_type(fieldidx))));\n","filename":"src\/hotspot\/share\/opto\/graphKit.cpp","additions":9,"deletions":6,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -6288,1 +6288,1 @@\n-    C->gvn_replace_by(init->proj_out(TypeFunc::Memory), alloc_mem);\n+    init->replace_mem_projs_by(alloc_mem, C);\n@@ -6339,2 +6339,14 @@\n-    set_memory(init->proj_out_or_null(TypeFunc::Memory), Compile::AliasIdxRaw);\n-    set_memory(init->proj_out_or_null(TypeFunc::Memory), elemidx);\n+    \/\/ Need to properly move every memory projection for the Initialize\n+#ifdef ASSERT\n+    int mark_idx = C->get_alias_index(ary_type->add_offset(oopDesc::mark_offset_in_bytes()));\n+    int klass_idx = C->get_alias_index(ary_type->add_offset(oopDesc::klass_offset_in_bytes()));\n+#endif\n+    auto move_proj = [&](ProjNode* proj) {\n+      int alias_idx = C->get_alias_index(proj->adr_type());\n+      assert(alias_idx == Compile::AliasIdxRaw ||\n+             alias_idx == elemidx ||\n+             alias_idx == mark_idx ||\n+             alias_idx == klass_idx, \"should be raw memory or array element type\");\n+      set_memory(proj, alias_idx);\n+    };\n+    init->for_each_proj(move_proj, TypeFunc::Memory);\n","filename":"src\/hotspot\/share\/opto\/library_call.cpp","additions":15,"deletions":3,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -1751,0 +1751,3 @@\n+\n+  void split_thru_phi_yank_old_nodes(Node* n, Node* region);\n+\n","filename":"src\/hotspot\/share\/opto\/loopnode.hpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -238,0 +238,3 @@\n+  split_thru_phi_yank_old_nodes(n, region);\n+  _igvn.replace_node(n, phi);\n+\n@@ -248,0 +251,22 @@\n+\/\/ If the region is a Loop, we are removing the old n,\n+\/\/ and need to yank it from the _body. If any phi we\n+\/\/ just split through now has no use any more, it also\n+\/\/ has to be removed.\n+void PhaseIdealLoop::split_thru_phi_yank_old_nodes(Node* n, Node* region) {\n+  IdealLoopTree* region_loop = get_loop(region);\n+  if (region->is_Loop() && region_loop->is_innermost()) {\n+    region_loop->_body.yank(n);\n+    for (uint j = 1; j < n->req(); j++) {\n+      PhiNode* phi = n->in(j)->isa_Phi();\n+      \/\/ Check that phi belongs to the region and only has n as a use.\n+      if (phi != nullptr &&\n+          phi->in(0) == region &&\n+          phi->unique_multiple_edges_out_or_null() == n) {\n+        assert(get_ctrl(phi) == region, \"sanity\");\n+        assert(get_ctrl(n) == region, \"sanity\");\n+        region_loop->_body.yank(phi);\n+      }\n+    }\n+  }\n+}\n+\n@@ -1275,3 +1300,3 @@\n-  \/\/ Split 'n' through the merge point if it is profitable\n-  Node *phi = split_thru_phi( n, n_blk, policy );\n-  if (!phi) return n;\n+  \/\/ Split 'n' through the merge point if it is profitable, replacing it with a new phi.\n+  Node* phi = split_thru_phi(n, n_blk, policy);\n+  if (phi == nullptr) { return n; }\n@@ -1279,3 +1304,0 @@\n-  \/\/ Found a Phi to split thru!\n-  \/\/ Replace 'n' with the new phi\n-  _igvn.replace_node( n, phi );\n@@ -1614,5 +1636,1 @@\n-    \/\/ Found a Phi to split thru!\n-    \/\/ Replace 'n' with the new phi\n-    _igvn.replace_node(n, phi);\n-\n-    Node *bolphi = split_thru_phi(bol, n_ctrl, -1);\n+    Node* bolphi = split_thru_phi(bol, n_ctrl, -1);\n@@ -1621,2 +1639,0 @@\n-\n-    _igvn.replace_node(bol, bolphi);\n@@ -1631,2 +1647,1 @@\n-      Node *cmovphi = split_thru_phi(iff, n_ctrl, -1);\n-      _igvn.replace_node(iff, cmovphi);\n+      Node* cmovphi = split_thru_phi(iff, n_ctrl, -1);\n@@ -2861,2 +2876,3 @@\n-          Node* cle_out = cle->proj_out_or_null(false);\n-          if (use == cle_out) {\n+          \/\/ is use the projection that exits the loop from the CountedLoopEndNode?\n+          if (use->in(0) == cle) {\n+            IfFalseNode* cle_out = use->as_IfFalse();\n","filename":"src\/hotspot\/share\/opto\/loopopts.cpp","additions":33,"deletions":17,"binary":false,"changes":50,"status":"modified"},{"patch":"@@ -528,0 +528,5 @@\n+    } else if (mem->is_top()) {\n+      \/\/ The slice is on a dead path. Returning nullptr would lead to elimination\n+      \/\/ bailout, but we want to prevent that. Just forwarding the top is also legal,\n+      \/\/ and IGVN can just clean things up, and remove whatever receives top.\n+      return mem;\n@@ -878,0 +883,35 @@\n+#ifdef ASSERT\n+  \/\/ Verify if a value can be written into a field.\n+  void verify_type_compatability(const Type* value_type, const Type* field_type) {\n+    BasicType value_bt = value_type->basic_type();\n+    BasicType field_bt = field_type->basic_type();\n+\n+    \/\/ Primitive types must match.\n+    if (is_java_primitive(value_bt) && value_bt == field_bt) { return; }\n+\n+    \/\/ I have been struggling to make a similar assert for non-primitive\n+    \/\/ types. I we can add one in the future. For now, I just let them\n+    \/\/ pass without checks.\n+    \/\/ In particular, I was struggling with a value that came from a call,\n+    \/\/ and had only a non-null check CastPP. There was also a checkcast\n+    \/\/ in the graph to verify the interface, but the corresponding\n+    \/\/ CheckCastPP result was not updated in the stack slot, and so\n+    \/\/ we ended up using the CastPP. That means that the field knows\n+    \/\/ that it should get an oop from an interface, but the value lost\n+    \/\/ that information, and so it is not a subtype.\n+    \/\/ There may be other issues, feel free to investigate further!\n+    if (!is_java_primitive(value_bt)) { return; }\n+\n+    tty->print_cr(\"value not compatible for field: %s vs %s\",\n+                  type2name(value_bt),\n+                  type2name(field_bt));\n+    tty->print(\"value_type: \");\n+    value_type->dump();\n+    tty->cr();\n+    tty->print(\"field_type: \");\n+    field_type->dump();\n+    tty->cr();\n+    assert(false, \"value_type does not fit field_type\");\n+  }\n+#endif\n+\n@@ -901,0 +941,1 @@\n+  DEBUG_ONLY(verify_type_compatability(field_val->bottom_type(), field_type);)\n@@ -1267,1 +1308,0 @@\n-        assert(init->outcnt() <= 2, \"only a control and memory projection expected\");\n@@ -1277,3 +1317,1 @@\n-        Node *mem_proj = init->proj_out_or_null(TypeFunc::Memory);\n-        if (mem_proj != nullptr) {\n-          Node *mem = init->in(TypeFunc::Memory);\n+        Node* mem = init->in(TypeFunc::Memory);\n@@ -1281,0 +1319,1 @@\n+        if (init->number_of_projs(TypeFunc::Memory) > 0) {\n@@ -1282,1 +1321,1 @@\n-            assert(mem->in(TypeFunc::Memory) == _callprojs->fallthrough_memproj, \"allocation memory projection\");\n+            assert(mem->as_MergeMem()->memory_at(Compile::AliasIdxRaw) == _callprojs->fallthrough_memproj, \"allocation memory projection\");\n@@ -1286,2 +1325,3 @@\n-#endif\n-          _igvn.replace_node(mem_proj, mem);\n+#endif\n+        init->replace_mem_projs_by(mem, &_igvn);\n+        assert(init->outcnt() == 0, \"should only have had a control and some memory projections, and we removed them\");\n@@ -1895,1 +1935,10 @@\n-      MemBarNode* mb = MemBarNode::make(C, Op_MemBarStoreStore, Compile::AliasIdxBot);\n+      \/\/ What we want is to prevent the compiler and the CPU from re-ordering the stores that initialize this object\n+      \/\/ with subsequent stores to any slice. As a consequence, this MemBar should capture the entire memory state at\n+      \/\/ this point in the IR and produce a new memory state that should cover all slices. However, the Initialize node\n+      \/\/ only captures\/produces a partial memory state making it complicated to insert such a MemBar. Because\n+      \/\/ re-ordering by the compiler can't happen by construction (a later Store that publishes the just allocated\n+      \/\/ object reference is indirectly control dependent on the Initialize node), preventing reordering by the CPU is\n+      \/\/ sufficient. For that a MemBar on the raw memory slice is good enough.\n+      \/\/ If init is null, this allocation does have an InitializeNode but this logic can't locate it (see comment in\n+      \/\/ PhaseMacroExpand::initialize_object()).\n+      MemBarNode* mb = MemBarNode::make(C, Op_MemBarStoreStore, Compile::AliasIdxRaw);\n@@ -1911,2 +1960,2 @@\n-      Node* init_mem = init->proj_out_or_null(TypeFunc::Memory);\n-      MemBarNode* mb = MemBarNode::make(C, Op_MemBarStoreStore, Compile::AliasIdxBot);\n+      \/\/ See comment above that explains why a raw memory MemBar is good enough.\n+      MemBarNode* mb = MemBarNode::make(C, Op_MemBarStoreStore, Compile::AliasIdxRaw);\n@@ -1918,2 +1967,11 @@\n-      Node* mem = new ProjNode(init, TypeFunc::Memory);\n-      transform_later(mem);\n+      Node* old_raw_mem_proj = nullptr;\n+      auto find_raw_mem = [&](ProjNode* proj) {\n+        if (C->get_alias_index(proj->adr_type()) == Compile::AliasIdxRaw) {\n+          assert(old_raw_mem_proj == nullptr, \"only one expected\");\n+          old_raw_mem_proj = proj;\n+        }\n+      };\n+      init->for_each_proj(find_raw_mem, TypeFunc::Memory);\n+      assert(old_raw_mem_proj != nullptr, \"should have found raw mem Proj\");\n+      Node* raw_mem_proj = new ProjNode(init, TypeFunc::Memory);\n+      transform_later(raw_mem_proj);\n@@ -1923,1 +1981,1 @@\n-      mb->init_req(TypeFunc::Memory, mem);\n+      mb->init_req(TypeFunc::Memory, raw_mem_proj);\n@@ -1928,1 +1986,1 @@\n-      mem = new ProjNode(mb, TypeFunc::Memory);\n+      Node* mem = new ProjNode(mb, TypeFunc::Memory);\n@@ -1937,3 +1995,1 @@\n-      if (init_mem != nullptr) {\n-        _igvn.replace_node(init_mem, mem);\n-      }\n+      _igvn.replace_node(old_raw_mem_proj, mem);\n","filename":"src\/hotspot\/share\/opto\/macro.cpp","additions":73,"deletions":17,"binary":false,"changes":90,"status":"modified"},{"patch":"@@ -154,0 +154,2 @@\n+    assert(!n->is_Initialize() || n->as_Initialize()->number_of_projs(TypeFunc::Memory) == 1,\n+           \"after matching, Initialize should have a single memory projection\");\n@@ -1064,1 +1066,1 @@\n-        Node* m;\n+        Node* m = nullptr;\n@@ -1079,4 +1081,18 @@\n-              \/\/ Convert to machine-dependent projection\n-              RegMask* mask = nullptr;\n-              if (n->in(0)->is_Call() && n->in(0)->as_Call()->tf()->returns_inline_type_as_fields()) {\n-                mask = return_values_mask(n->in(0)->as_Call()->tf());\n+              if (n->in(0)->is_Initialize() && n->as_Proj()->_con == TypeFunc::Memory) {\n+                \/\/ Initialize may have multiple NarrowMem projections. They would all match to identical raw mem MachProjs.\n+                \/\/ We don't need multiple MachProjs. Create one if none already exist, otherwise use existing one.\n+                m = n->in(0)->as_Initialize()->mem_mach_proj();\n+                if (m == nullptr && has_new_node(n->in(0))) {\n+                  InitializeNode* new_init = new_node(n->in(0))->as_Initialize();\n+                  m = new_init->mem_mach_proj();\n+                }\n+                assert(m == nullptr || m->is_MachProj(), \"no mem projection yet or a MachProj created during matching\");\n+              }\n+              if (m == nullptr) {\n+                \/\/ Convert to machine-dependent projection\n+                RegMask* mask = nullptr;\n+                if (n->in(0)->is_Call() && n->in(0)->as_Call()->tf()->returns_inline_type_as_fields()) {\n+                  mask = return_values_mask(n->in(0)->as_Call()->tf());\n+                }\n+                m = n->in(0)->as_Multi()->match(n->as_Proj(), this, mask);\n+                NOT_PRODUCT(record_new2old(m, n);)\n@@ -1084,2 +1100,0 @@\n-              m = n->in(0)->as_Multi()->match(n->as_Proj(), this, mask);\n-              NOT_PRODUCT(record_new2old(m, n);)\n","filename":"src\/hotspot\/share\/opto\/matcher.cpp","additions":21,"deletions":7,"binary":false,"changes":28,"status":"modified"},{"patch":"@@ -99,1 +99,1 @@\n-  dump_adr_type(this, _adr_type, st);\n+  dump_adr_type(_adr_type, st);\n@@ -116,1 +116,1 @@\n-void MemNode::dump_adr_type(const Node* mem, const TypePtr* adr_type, outputStream *st) {\n+void MemNode::dump_adr_type(const TypePtr* adr_type, outputStream* st) {\n@@ -4467,1 +4467,1 @@\n-  assert(outcnt() > 0 && outcnt() <= 2, \"Only one or two out edges allowed\");\n+  assert(outcnt() > 0 && (outcnt() <= 2 || Opcode() == Op_Initialize), \"Only one or two out edges allowed\");\n@@ -4475,3 +4475,0 @@\n-  if (proj_out_or_null(TypeFunc::Memory) != nullptr) {\n-    igvn->replace_node(proj_out(TypeFunc::Memory), in(TypeFunc::Memory));\n-  }\n@@ -4481,0 +4478,7 @@\n+  if (is_Initialize()) {\n+    as_Initialize()->replace_mem_projs_by(in(TypeFunc::Memory), igvn);\n+  } else {\n+    if (proj_out_or_null(TypeFunc::Memory) != nullptr) {\n+      igvn->replace_node(proj_out(TypeFunc::Memory), in(TypeFunc::Memory));\n+    }\n+  }\n@@ -5695,0 +5699,42 @@\n+void InitializeNode::replace_mem_projs_by(Node* mem, Compile* C) {\n+  auto replace_proj = [&](ProjNode* proj) {\n+    C->gvn_replace_by(proj, mem);\n+    return CONTINUE;\n+  };\n+  apply_to_projs(replace_proj, TypeFunc::Memory);\n+}\n+\n+void InitializeNode::replace_mem_projs_by(Node* mem, PhaseIterGVN* igvn) {\n+  DUIterator_Fast imax, i = fast_outs(imax);\n+  auto replace_proj = [&](ProjNode* proj) {\n+    igvn->replace_node(proj, mem);\n+    --i; --imax;\n+    return CONTINUE;\n+  };\n+  apply_to_projs(imax, i, replace_proj, TypeFunc::Memory);\n+}\n+\n+bool InitializeNode::already_has_narrow_mem_proj_with_adr_type(const TypePtr* adr_type) const {\n+  auto find_proj = [&](ProjNode* proj) {\n+    if (proj->adr_type() == adr_type) {\n+      return BREAK_AND_RETURN_CURRENT_PROJ;\n+    }\n+    return CONTINUE;\n+  };\n+  DUIterator_Fast imax, i = fast_outs(imax);\n+  return apply_to_narrow_mem_projs_any_iterator(UsesIteratorFast(imax, i, this), find_proj) != nullptr;\n+}\n+\n+MachProjNode* InitializeNode::mem_mach_proj() const {\n+  auto find_proj = [](ProjNode* proj) {\n+    if (proj->is_MachProj()) {\n+      return BREAK_AND_RETURN_CURRENT_PROJ;\n+    }\n+    return CONTINUE;\n+  };\n+  ProjNode* proj = apply_to_projs(find_proj, TypeFunc::Memory);\n+  if (proj == nullptr) {\n+    return nullptr;\n+  }\n+  return proj->as_MachProj();\n+}\n@@ -6117,0 +6163,1 @@\n+           || n->is_NarrowMemProj()\n","filename":"src\/hotspot\/share\/opto\/memnode.cpp","additions":53,"deletions":6,"binary":false,"changes":59,"status":"modified"},{"patch":"@@ -174,1 +174,1 @@\n-  static void dump_adr_type(const Node* mem, const TypePtr* adr_type, outputStream *st);\n+  static void dump_adr_type(const TypePtr* adr_type, outputStream* st);\n@@ -1405,1 +1405,14 @@\n- private:\n+  \/\/ An Initialize node has multiple memory projections. Helper methods used when the node is removed.\n+  \/\/ For use at parse time\n+  void replace_mem_projs_by(Node* mem, Compile* C);\n+  \/\/ For use with IGVN\n+  void replace_mem_projs_by(Node* mem, PhaseIterGVN* igvn);\n+\n+  \/\/ Does a NarrowMemProj with this adr_type and this node as input already exist?\n+  bool already_has_narrow_mem_proj_with_adr_type(const TypePtr* adr_type) const;\n+\n+  \/\/ Used during matching: find the MachProj memory projection if there's one. Expectation is that there should be at\n+  \/\/ most one.\n+  MachProjNode* mem_mach_proj() const;\n+\n+private:\n@@ -1422,0 +1435,27 @@\n+\n+  \/\/ Iterate with i over all NarrowMemProj uses calling callback\n+  template <class Callback, class Iterator> NarrowMemProjNode* apply_to_narrow_mem_projs_any_iterator(Iterator i, Callback callback) const {\n+    auto filter = [&](ProjNode* proj) {\n+      if (proj->is_NarrowMemProj() && callback(proj->as_NarrowMemProj()) == BREAK_AND_RETURN_CURRENT_PROJ) {\n+        return BREAK_AND_RETURN_CURRENT_PROJ;\n+      }\n+      return CONTINUE;\n+    };\n+    ProjNode* res = apply_to_projs_any_iterator(i, filter);\n+    if (res == nullptr) {\n+      return nullptr;\n+    }\n+    return res->as_NarrowMemProj();\n+  }\n+\n+public:\n+\n+  \/\/ callback is allowed to add new uses that will then be iterated over\n+  template <class Callback> void for_each_narrow_mem_proj_with_new_uses(Callback callback) const {\n+    auto callback_always_continue = [&](NarrowMemProjNode* proj) {\n+      callback(proj);\n+      return MultiNode::CONTINUE;\n+    };\n+    DUIterator i = outs();\n+    apply_to_narrow_mem_projs_any_iterator(UsesIterator(i, this), callback_always_continue);\n+  }\n","filename":"src\/hotspot\/share\/opto\/memnode.hpp","additions":42,"deletions":2,"binary":false,"changes":44,"status":"modified"},{"patch":"@@ -1136,0 +1136,13 @@\n+  \/\/ Check for \"(con0 - X) << con1\"\n+  \/\/ Transform is legal, but check for profit.  Avoid breaking 'i2s'\n+  \/\/ and 'i2b' patterns which typically fold into 'StoreC\/StoreB'.\n+  if (add1_op == Op_Sub(bt) && (bt != T_INT || con < 16)) {    \/\/ Left input is a sub?\n+    \/\/ Left input is a sub from a constant?\n+    const TypeInteger* t11 = phase->type(add1->in(1))->isa_integer(bt);\n+    if (t11 != nullptr && t11->is_con()) {\n+      \/\/ Compute X << con0\n+      Node* lsh = phase->transform(LShiftNode::make(add1->in(2), in(2), bt));\n+      \/\/ Compute (con1<<con0) - (X<<con0)\n+      return SubNode::make(phase->integercon(java_shift_left(t11->get_con_as_long(bt), con, bt), bt), lsh, bt);\n+    }\n+  }\n","filename":"src\/hotspot\/share\/opto\/mulnode.cpp","additions":13,"deletions":0,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -48,14 +48,5 @@\n-  for( DUIterator_Fast imax, i = fast_outs(imax); i < imax; i++ ) {\n-    Node *p = fast_out(i);\n-    if (p->is_Proj()) {\n-      ProjNode *proj = p->as_Proj();\n-      if (proj->_con == which_proj) {\n-        assert((Opcode() != Op_If && Opcode() != Op_RangeCheck) || proj->Opcode() == (which_proj ? Op_IfTrue : Op_IfFalse), \"bad if #2\");\n-        return proj;\n-      }\n-    } else {\n-      assert(p == this && this->is_Start(), \"else must be proj\");\n-      continue;\n-    }\n-  }\n-  return nullptr;\n+  assert(number_of_projs(which_proj) <= 1, \"only when there's a single projection\");\n+  ProjNode* proj = find_first(which_proj);\n+  assert(proj == nullptr || (Opcode() != Op_If && Opcode() != Op_RangeCheck) || proj->Opcode() == (which_proj ? Op_IfTrue : Op_IfFalse),\n+         \"incorrect projection node at If\/RangeCheck: IfTrue on false path or IfFalse on true path\");\n+  return proj;\n@@ -65,4 +56,8 @@\n-  for (DUIterator_Fast imax, i = fast_outs(imax); i < imax; i++) {\n-    ProjNode* proj = fast_out(i)->isa_Proj();\n-    if (proj != nullptr && (proj->_con == which_proj) && (proj->_is_io_use == is_io_use)) {\n-      return proj;\n+  assert(number_of_projs(which_proj, is_io_use) <= 1, \"only when there's a single projection\");\n+  return find_first(which_proj, is_io_use);\n+}\n+\n+template<class Callback> ProjNode* MultiNode::apply_to_projs(Callback callback, uint which_proj, bool is_io_use) const {\n+  auto filter = [&](ProjNode* proj) {\n+    if (proj->_is_io_use == is_io_use && callback(proj) == BREAK_AND_RETURN_CURRENT_PROJ) {\n+      return BREAK_AND_RETURN_CURRENT_PROJ;\n@@ -70,2 +65,35 @@\n-  }\n-  return nullptr;\n+    return CONTINUE;\n+  };\n+  return apply_to_projs(filter, which_proj);\n+}\n+\n+uint MultiNode::number_of_projs(uint which_proj) const {\n+  uint cnt = 0;\n+  auto count_projs = [&](ProjNode* proj) {\n+    cnt++;\n+  };\n+  for_each_proj(count_projs, which_proj);\n+  return cnt;\n+}\n+\n+uint MultiNode::number_of_projs(uint which_proj, bool is_io_use) const {\n+  uint cnt = 0;\n+  auto count_projs = [&](ProjNode* proj) {\n+    cnt++;\n+  };\n+  for_each_proj(count_projs, which_proj, is_io_use);\n+  return cnt;\n+}\n+\n+ProjNode* MultiNode::find_first(uint which_proj) const {\n+  auto find_proj = [&](ProjNode* proj) {\n+    return BREAK_AND_RETURN_CURRENT_PROJ;\n+  };\n+  return apply_to_projs(find_proj, which_proj);\n+}\n+\n+ProjNode* MultiNode::find_first(uint which_proj, bool is_io_use) const {\n+  auto find_proj = [](ProjNode* proj) {\n+    return BREAK_AND_RETURN_CURRENT_PROJ;\n+  };\n+  return apply_to_projs(find_proj, which_proj, is_io_use);\n@@ -242,0 +270,5 @@\n+\n+NarrowMemProjNode::NarrowMemProjNode(InitializeNode* src, const TypePtr* adr_type)\n+  : ProjNode(src, TypeFunc::Memory), _adr_type(adr_type) {\n+  init_class_id(Class_NarrowMemProj);\n+}\n","filename":"src\/hotspot\/share\/opto\/multnode.cpp","additions":53,"deletions":20,"binary":false,"changes":73,"status":"modified"},{"patch":"@@ -52,0 +52,99 @@\n+  uint number_of_projs(uint which_proj) const;\n+  uint number_of_projs(uint which_proj, bool is_io_use) const;\n+\n+protected:\n+\n+  \/\/ Provide single interface for DUIterator_Fast\/DUIterator for template method below\n+  class UsesIteratorFast {\n+    DUIterator_Fast& _imax;\n+    DUIterator_Fast& _i;\n+    const Node* _node;\n+\n+  public:\n+    bool cont() {\n+      return _i < _imax;\n+    }\n+    void next() {\n+      _i++;\n+    }\n+    Node* current() {\n+      return _node->fast_out(_i);\n+    }\n+    UsesIteratorFast(DUIterator_Fast& imax, DUIterator_Fast& i, const Node* node)\n+      : _imax(imax), _i(i), _node(node) {\n+    }\n+  };\n+\n+  class UsesIterator {\n+    DUIterator& _i;\n+    const Node* _node;\n+\n+  public:\n+    bool cont() {\n+      return _node->has_out(_i);\n+    }\n+    void next() {\n+      _i++;\n+    }\n+    Node* current() {\n+      return _node->out(_i);\n+    }\n+    UsesIterator(DUIterator& i, const Node* node)\n+      : _i(i), _node(node) {\n+    }\n+  };\n+\n+  \/\/ Iterate with i over all Proj uses calling callback\n+  template<class Callback, class Iterator> ProjNode* apply_to_projs_any_iterator(Iterator i, Callback callback) const {\n+    for (; i.cont(); i.next()) {\n+      Node* p = i.current();\n+      if (p->is_Proj()) {\n+        ProjNode* proj = p->as_Proj();\n+        ApplyToProjs result = callback(proj);\n+        if (result == BREAK_AND_RETURN_CURRENT_PROJ) {\n+          return proj;\n+        }\n+        assert(result == CONTINUE, \"should be either break or continue\");\n+      } else {\n+        assert(p == this && is_Start(), \"else must be proj\");\n+      }\n+    }\n+    return nullptr;\n+  }\n+  enum ApplyToProjs {\n+    CONTINUE,\n+    BREAK_AND_RETURN_CURRENT_PROJ\n+  };\n+\n+  \/\/ Run callback on projections with iterator passed as argument\n+  template <class Callback> ProjNode* apply_to_projs(DUIterator_Fast& imax, DUIterator_Fast& i, Callback callback, uint which_proj) const;\n+\n+  \/\/ Same but with default iterator and for matching _con\n+  template<class Callback> ProjNode* apply_to_projs(Callback callback, uint which_proj) const {\n+    DUIterator_Fast imax, i = fast_outs(imax);\n+    return apply_to_projs(imax, i, callback, which_proj);\n+  }\n+\n+  \/\/ Same but for matching _con and _is_io_use\n+  template <class Callback> ProjNode* apply_to_projs(Callback callback, uint which_proj, bool is_io_use) const;\n+\n+public:\n+  template<class Callback> void for_each_proj(Callback callback, uint which_proj) const {\n+    auto callback_always_continue = [&](ProjNode* proj) {\n+      callback(proj);\n+      return MultiNode::CONTINUE;\n+    };\n+    apply_to_projs(callback_always_continue, which_proj);\n+  }\n+\n+  template <class Callback> void for_each_proj(Callback callback, uint which_proj, bool is_io_use) const {\n+    auto callback_always_continue = [&](ProjNode* proj) {\n+      callback(proj);\n+      return MultiNode::CONTINUE;\n+    };\n+    apply_to_projs(callback_always_continue, which_proj, is_io_use);\n+  }\n+\n+\n+  ProjNode* find_first(uint which_proj) const;\n+  ProjNode* find_first(uint which_proj, bool is_io_use) const;\n@@ -109,0 +208,35 @@\n+\/\/ A ProjNode variant that captures an adr_type(). Used as a projection of InitializeNode to have the right adr_type()\n+\/\/ for array elements\/fields.\n+class NarrowMemProjNode : public ProjNode {\n+private:\n+  const TypePtr* const _adr_type;\n+protected:\n+  virtual uint hash() const {\n+    return ProjNode::hash() + _adr_type->hash();\n+  }\n+  virtual bool cmp(const Node& n) const {\n+    return ProjNode::cmp(n) && ((NarrowMemProjNode&)n)._adr_type == _adr_type;\n+  }\n+  virtual uint size_of() const {\n+    return sizeof(*this);\n+  }\n+public:\n+  NarrowMemProjNode(InitializeNode* src, const TypePtr* adr_type);\n+\n+  virtual const TypePtr* adr_type() const {\n+    return _adr_type;\n+  }\n+\n+  virtual int Opcode() const;\n+};\n+\n+template <class Callback> ProjNode* MultiNode::apply_to_projs(DUIterator_Fast& imax, DUIterator_Fast& i, Callback callback, uint which_proj) const {\n+  auto filter = [&](ProjNode* proj) {\n+    if (proj->_con == which_proj && callback(proj) == BREAK_AND_RETURN_CURRENT_PROJ) {\n+      return BREAK_AND_RETURN_CURRENT_PROJ;\n+    }\n+    return CONTINUE;\n+  };\n+  return apply_to_projs_any_iterator(UsesIteratorFast(imax, i, this), filter);\n+}\n+\n","filename":"src\/hotspot\/share\/opto\/multnode.hpp","additions":134,"deletions":0,"binary":false,"changes":134,"status":"modified"},{"patch":"@@ -2615,1 +2615,1 @@\n-    MemNode::dump_adr_type(this, adr_type(), st);\n+    MemNode::dump_adr_type(adr_type(), st);\n@@ -2902,0 +2902,14 @@\n+Node* Node::unique_multiple_edges_out_or_null() const {\n+  Node* use = nullptr;\n+  for (DUIterator_Fast kmax, k = fast_outs(kmax); k < kmax; k++) {\n+    Node* u = fast_out(k);\n+    if (use == nullptr) {\n+      use = u; \/\/ first use\n+    } else if (u != use) {\n+      return nullptr; \/\/ not unique\n+    } else {\n+      \/\/ secondary use\n+    }\n+  }\n+  return use;\n+}\n","filename":"src\/hotspot\/share\/opto\/node.cpp","additions":15,"deletions":1,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -140,0 +140,1 @@\n+class NarrowMemProjNode;\n@@ -431,0 +432,7 @@\n+\n+  \/\/ In some cases, a node n is only used by a single use, but the use may use\n+  \/\/ n once or multiple times:\n+  \/\/   use = ConvF2I(this)\n+  \/\/   use = AddI(this, this)\n+  Node* unique_multiple_edges_out_or_null() const;\n+\n@@ -779,0 +787,1 @@\n+      DEFINE_CLASS_ID(NarrowMemProj, Proj, 6)\n@@ -1002,0 +1011,1 @@\n+  DEFINE_CLASS_QUERY(NarrowMemProj)\n","filename":"src\/hotspot\/share\/opto\/node.hpp","additions":10,"deletions":0,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -1443,1 +1443,0 @@\n-  int exception_handler_req = HandlerImpl::size_exception_handler() + MAX_stubs_size; \/\/ add marginal slop for handler\n@@ -1449,1 +1448,1 @@\n-    code_req = const_req = stub_req = exception_handler_req = deopt_handler_req = 0x10;  \/\/ force expansion\n+    code_req = const_req = stub_req = deopt_handler_req = 0x10;  \/\/ force expansion\n@@ -1456,1 +1455,0 @@\n-          exception_handler_req +\n@@ -1886,2 +1884,0 @@\n-    \/\/ Emit the exception handler code.\n-    _code_offsets.set_value(CodeOffsets::Exceptions, HandlerImpl::emit_exception_handler(masm));\n","filename":"src\/hotspot\/share\/opto\/output.cpp","additions":1,"deletions":5,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -2627,0 +2627,3 @@\n+  auto enqueue_init_mem_projs = [&](ProjNode* proj) {\n+    add_users_to_worklist0(proj, worklist);\n+  };\n@@ -2631,2 +2634,1 @@\n-      Node* imem = init->proj_out_or_null(TypeFunc::Memory);\n-      if (imem != nullptr) add_users_to_worklist0(imem, worklist);\n+      init->for_each_proj(enqueue_init_mem_projs, TypeFunc::Memory);\n@@ -2646,2 +2648,2 @@\n-    Node* imem = use->as_Initialize()->proj_out_or_null(TypeFunc::Memory);\n-    if (imem != nullptr) add_users_to_worklist0(imem, worklist);\n+    InitializeNode* init = use->as_Initialize();\n+    init->for_each_proj(enqueue_init_mem_projs, TypeFunc::Memory);\n","filename":"src\/hotspot\/share\/opto\/phaseX.cpp","additions":6,"deletions":4,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -383,1 +383,0 @@\n-  assert(init->outcnt() <= 2, \"only a control and memory projection expected\");\n@@ -389,5 +388,2 @@\n-  Node *mem_proj = init->proj_out_or_null(TypeFunc::Memory);\n-  if (mem_proj != nullptr) {\n-    Node *mem = init->in(TypeFunc::Memory);\n-    C->gvn_replace_by(mem_proj, mem);\n-  }\n+  Node* mem = init->in(TypeFunc::Memory);\n+  init->replace_mem_projs_by(mem, C);\n","filename":"src\/hotspot\/share\/opto\/stringopts.cpp","additions":2,"deletions":6,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -1524,0 +1524,15 @@\n+BoolTest::mask BoolTest::unsigned_mask(BoolTest::mask btm) {\n+  switch(btm) {\n+    case eq:\n+    case ne:\n+      return btm;\n+    case lt:\n+    case le:\n+    case gt:\n+    case ge:\n+      return mask(btm | unsigned_compare);\n+    default:\n+      ShouldNotReachHere();\n+  }\n+}\n+\n","filename":"src\/hotspot\/share\/opto\/subnode.cpp","additions":15,"deletions":0,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -356,1 +356,1 @@\n-  static mask unsigned_mask(mask btm) { return mask(btm | unsigned_compare); }\n+  static mask unsigned_mask(mask btm);\n","filename":"src\/hotspot\/share\/opto\/subnode.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -783,1 +783,1 @@\n-                        link_info, false, THREAD);\n+                        link_info, ClassInitMode::dont_init, THREAD);\n@@ -845,1 +845,1 @@\n-        LinkResolver::resolve_field(result, link_info, Bytecodes::_nop, false, THREAD);\n+        LinkResolver::resolve_field(result, link_info, Bytecodes::_nop, ClassInitMode::dont_init, THREAD);\n","filename":"src\/hotspot\/share\/prims\/methodHandles.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -34,0 +34,1 @@\n+#include \"cppstdlib\/limits.hpp\"\n@@ -78,1 +79,0 @@\n-#include <limits>\n","filename":"src\/hotspot\/share\/runtime\/arguments.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -31,0 +31,1 @@\n+#include \"cppstdlib\/type_traits.hpp\"\n@@ -36,0 +37,1 @@\n+#include \"interpreter\/bytecodeStream.hpp\"\n@@ -37,0 +39,1 @@\n+#include \"interpreter\/interpreterRuntime.hpp\"\n@@ -41,0 +44,1 @@\n+#include \"oops\/constantPool.inline.hpp\"\n@@ -66,0 +70,2 @@\n+#include \"runtime\/vframe.inline.hpp\"\n+#include \"runtime\/vframe_hp.hpp\"\n@@ -76,2 +82,6 @@\n-\n-#include <type_traits>\n+#ifdef COMPILER1\n+#include \"c1\/c1_Runtime1.hpp\"\n+#endif\n+#ifdef COMPILER2\n+#include \"opto\/runtime.hpp\"\n+#endif\n@@ -186,1 +196,1 @@\n-static void log_frames_after_thaw(JavaThread* thread, ContinuationWrapper& cont, intptr_t* sp, bool preempted);\n+static void log_frames_after_thaw(JavaThread* thread, ContinuationWrapper& cont, intptr_t* sp);\n@@ -188,0 +198,1 @@\n+static void verify_frame_kind(frame& top, Continuation::preempt_kind preempt_kind, Method** m_ptr = nullptr, const char** code_name_ptr = nullptr, int* bci_ptr = nullptr, stackChunkOop chunk = nullptr);\n@@ -465,1 +476,1 @@\n-  static inline void prepare_freeze_interpreted_top_frame(frame& f);\n+  inline void prepare_freeze_interpreted_top_frame(frame& f);\n@@ -1093,2 +1104,2 @@\n-    frame f = _thread->last_frame();\n-    if (f.is_interpreted_frame()) {\n+    frame top_frame = _thread->last_frame();\n+    if (top_frame.is_interpreted_frame()) {\n@@ -1099,1 +1110,15 @@\n-      prepare_freeze_interpreted_top_frame(f);\n+      prepare_freeze_interpreted_top_frame(top_frame);\n+    }\n+\n+    \/\/ Do this now so should_process_args_at_top() is set before calling finish_freeze\n+    \/\/ in case we might need to apply GC barriers to frames in this stackChunk.\n+    if (_thread->at_preemptable_init()) {\n+      assert(top_frame.is_interpreted_frame(), \"only InterpreterRuntime::_new\/resolve_from_cache allowed\");\n+      chunk->set_at_klass_init(true);\n+      methodHandle m(_thread, top_frame.interpreter_frame_method());\n+      Bytecode_invoke call = Bytecode_invoke_check(m, top_frame.interpreter_frame_bci());\n+      assert(!call.is_valid() || call.is_invokestatic(), \"only invokestatic allowed\");\n+      if (call.is_invokestatic() && call.size_of_parameters() > 0) {\n+        assert(top_frame.interpreter_frame_expression_stack_size() > 0, \"should have parameters in exp stack\");\n+        chunk->set_has_args_at_top(true);\n+      }\n@@ -1634,0 +1659,19 @@\n+class AnchorMark : public StackObj {\n+  JavaThread* _current;\n+  frame& _top_frame;\n+  intptr_t* _last_sp_from_frame;\n+  bool _is_interpreted;\n+\n+ public:\n+  AnchorMark(JavaThread* current, frame& f) : _current(current), _top_frame(f), _is_interpreted(false) {\n+    intptr_t* sp = anchor_mark_set_pd();\n+    set_anchor(_current, sp);\n+  }\n+  ~AnchorMark() {\n+    clear_anchor(_current);\n+    anchor_mark_clear_pd();\n+  }\n+  inline intptr_t* anchor_mark_set_pd();\n+  inline void anchor_mark_clear_pd();\n+};\n+\n@@ -1652,2 +1696,3 @@\n-  if (!cont.entry()->is_virtual_thread() && JvmtiExport::has_frame_pops(thread)) {\n-    int num_frames = num_java_frames(cont);\n+  if (!cont.entry()->is_virtual_thread()) {\n+    if (JvmtiExport::has_frame_pops(thread)) {\n+      int num_frames = num_java_frames(cont);\n@@ -1655,2 +1700,4 @@\n-    ContinuationWrapper::SafepointOp so(Thread::current(), cont);\n-    JvmtiExport::continuation_yield_cleanup(JavaThread::current(), num_frames);\n+      ContinuationWrapper::SafepointOp so(Thread::current(), cont);\n+      JvmtiExport::continuation_yield_cleanup(thread, num_frames);\n+    }\n+    invalidate_jvmti_stack(thread);\n@@ -1658,1 +1705,0 @@\n-  invalidate_jvmti_stack(thread);\n@@ -1661,1 +1707,1 @@\n-static void jvmti_mount_end(JavaThread* current, ContinuationWrapper& cont, frame top) {\n+static void jvmti_mount_end(JavaThread* current, ContinuationWrapper& cont, frame top, Continuation::preempt_kind pk) {\n@@ -1664,1 +1710,1 @@\n-  HandleMarkCleaner hm(current);\n+  HandleMarkCleaner hm(current);  \/\/ Cleanup all handles (including so._conth) before returning to Java.\n@@ -1666,4 +1712,1 @@\n-\n-\n-  \/\/ Since we might safepoint set the anchor so that the stack can be walked.\n-  set_anchor(current, top.sp());\n+  AnchorMark am(current, top);  \/\/ Set anchor so that the stack is walkable.\n@@ -1676,1 +1719,4 @@\n-      JvmtiExport::post_monitor_contended_entered(current, current->contended_entered_monitor());\n+      \/\/ No monitor JVMTI events for ObjectLocker case.\n+      if (pk != Continuation::object_locker) {\n+        JvmtiExport::post_monitor_contended_entered(current, current->contended_entered_monitor());\n+      }\n@@ -1680,2 +1726,0 @@\n-\n-  clear_anchor(current);\n@@ -1704,0 +1748,103 @@\n+\n+static void verify_frame_kind(frame& top, Continuation::preempt_kind preempt_kind, Method** m_ptr, const char** code_name_ptr, int* bci_ptr, stackChunkOop chunk) {\n+  Method* m;\n+  const char* code_name;\n+  int bci;\n+  if (preempt_kind == Continuation::monitorenter) {\n+    assert(top.is_interpreted_frame() || top.is_runtime_frame(), \"unexpected %sframe\",\n+      top.is_compiled_frame() ? \"compiled \" : top.is_native_frame() ? \"native \" : \"\");\n+    bool at_sync_method;\n+    if (top.is_interpreted_frame()) {\n+      m = top.interpreter_frame_method();\n+      assert(!m->is_native() || m->is_synchronized(), \"invalid method %s\", m->external_name());\n+      address bcp = top.interpreter_frame_bcp();\n+      assert(bcp != 0 || m->is_native(), \"\");\n+      at_sync_method = m->is_synchronized() && (bcp == 0 || bcp == m->code_base());\n+      \/\/ bcp is advanced on monitorenter before making the VM call, adjust for that.\n+      bool at_sync_bytecode = bcp > m->code_base() && Bytecode(m, bcp - 1).code() == Bytecodes::Code::_monitorenter;\n+      assert(at_sync_method || at_sync_bytecode, \"\");\n+      bci = at_sync_method ? -1 : top.interpreter_frame_bci();\n+    } else {\n+      JavaThread* current = JavaThread::current();\n+      ResourceMark rm(current);\n+      CodeBlob* cb = top.cb();\n+      RegisterMap reg_map(current,\n+                  RegisterMap::UpdateMap::skip,\n+                  RegisterMap::ProcessFrames::skip,\n+                  RegisterMap::WalkContinuation::include);\n+      if (top.is_heap_frame()) {\n+        assert(chunk != nullptr, \"\");\n+        reg_map.set_stack_chunk(chunk);\n+        top = chunk->relativize(top);\n+        top.set_frame_index(0);\n+      }\n+      frame fr = top.sender(&reg_map);\n+      vframe*  vf  = vframe::new_vframe(&fr, &reg_map, current);\n+      compiledVFrame* cvf = compiledVFrame::cast(vf);\n+      m = cvf->method();\n+      bci = cvf->scope()->bci();\n+      at_sync_method = bci == SynchronizationEntryBCI;\n+      assert(!at_sync_method || m->is_synchronized(), \"bci is %d but method %s is not synchronized\", bci, m->external_name());\n+      bool is_c1_monitorenter = false, is_c2_monitorenter = false;\n+      COMPILER1_PRESENT(is_c1_monitorenter = cb == Runtime1::blob_for(StubId::c1_monitorenter_id) ||\n+                                             cb == Runtime1::blob_for(StubId::c1_monitorenter_nofpu_id);)\n+      COMPILER2_PRESENT(is_c2_monitorenter = cb == CodeCache::find_blob(OptoRuntime::complete_monitor_locking_Java());)\n+      assert(is_c1_monitorenter || is_c2_monitorenter, \"wrong runtime stub frame\");\n+    }\n+    code_name = at_sync_method ? \"synchronized method\" : \"monitorenter\";\n+  } else if (preempt_kind == Continuation::object_wait) {\n+    assert(top.is_interpreted_frame() || top.is_native_frame(), \"\");\n+    m  = top.is_interpreted_frame() ? top.interpreter_frame_method() : top.cb()->as_nmethod()->method();\n+    assert(m->is_object_wait0(), \"\");\n+    bci = 0;\n+    code_name = \"\";\n+  } else {\n+    assert(preempt_kind == Continuation::object_locker, \"invalid preempt kind\");\n+    assert(top.is_interpreted_frame(), \"\");\n+    m = top.interpreter_frame_method();\n+    Bytecode current_bytecode = Bytecode(m, top.interpreter_frame_bcp());\n+    Bytecodes::Code code = current_bytecode.code();\n+    assert(code == Bytecodes::Code::_new || code == Bytecodes::Code::_invokestatic ||\n+           (code == Bytecodes::Code::_getstatic || code == Bytecodes::Code::_putstatic), \"invalid bytecode\");\n+    bci = top.interpreter_frame_bci();\n+    code_name = Bytecodes::name(current_bytecode.code());\n+  }\n+  assert(bci >= 0 || m->is_synchronized(), \"invalid bci:%d at method %s\", bci, m->external_name());\n+\n+  if (m_ptr != nullptr) {\n+    *m_ptr = m;\n+    *code_name_ptr = code_name;\n+    *bci_ptr = bci;\n+  }\n+}\n+\n+static void log_preempt_after_freeze(const ContinuationWrapper& cont) {\n+  JavaThread* current = cont.thread();\n+  int64_t tid = current->monitor_owner_id();\n+\n+  StackChunkFrameStream<ChunkFrames::Mixed> sfs(cont.tail());\n+  frame top_frame = sfs.to_frame();\n+  bool at_init = current->at_preemptable_init();\n+  bool at_enter = current->current_pending_monitor() != nullptr;\n+  bool at_wait = current->current_waiting_monitor() != nullptr;\n+  assert((at_enter && !at_wait) || (!at_enter && at_wait), \"\");\n+  Continuation::preempt_kind pk = at_init ? Continuation::object_locker : at_enter ? Continuation::monitorenter : Continuation::object_wait;\n+\n+  Method* m = nullptr;\n+  const char* code_name = nullptr;\n+  int bci = InvalidFrameStateBci;\n+  verify_frame_kind(top_frame, pk, &m, &code_name, &bci, cont.tail());\n+  assert(m != nullptr && code_name != nullptr && bci != InvalidFrameStateBci, \"should be set\");\n+\n+  ResourceMark rm(current);\n+  if (bci < 0) {\n+    log_trace(continuations, preempt)(\"Preempted \" INT64_FORMAT \" while synchronizing on %smethod %s\", tid, m->is_native() ? \"native \" : \"\", m->external_name());\n+  } else if (m->is_object_wait0()) {\n+    log_trace(continuations, preempt)(\"Preempted \" INT64_FORMAT \" at native method %s\", tid, m->external_name());\n+  } else {\n+    Klass* k = current->preempt_init_klass();\n+    assert(k != nullptr || !at_init, \"\");\n+    log_trace(continuations, preempt)(\"Preempted \" INT64_FORMAT \" at %s(bci:%d) in method %s %s%s\", tid, code_name, bci,\n+            m->external_name(), at_init ? \"trying to initialize klass \" : \"\", at_init ? k->external_name() : \"\");\n+  }\n+}\n@@ -1733,0 +1880,1 @@\n+  \/\/ Set up things so that on return to Java we jump to preempt stub.\n@@ -1735,1 +1883,1 @@\n-\n+  DEBUG_ONLY(log_preempt_after_freeze(cont);)\n@@ -1931,0 +2079,1 @@\n+  bool _process_args_at_top;\n@@ -1935,0 +2084,3 @@\n+  \/\/ Only used for preemption on ObjectLocker\n+  ObjectMonitor* _init_lock;\n+\n@@ -1962,0 +2114,2 @@\n+  inline intptr_t* push_preempt_adapter();\n+  intptr_t* redo_vmcall(JavaThread* current, frame& top);\n@@ -1978,1 +2132,1 @@\n-  NOINLINE void recurse_thaw_interpreted_frame(const frame& hf, frame& caller, int num_frames);\n+  NOINLINE void recurse_thaw_interpreted_frame(const frame& hf, frame& caller, int num_frames, bool is_top);\n@@ -1983,1 +2137,1 @@\n-  void push_return_frame(frame& f);\n+  void push_return_frame(const frame& f);\n@@ -2066,1 +2220,1 @@\n-    f.next(SmallRegisterMap::instance(), false \/* stop *\/);\n+    f.next(SmallRegisterMap::instance_no_args(), false \/* stop *\/);\n@@ -2092,1 +2246,1 @@\n-    f.next(SmallRegisterMap::instance(), true \/* stop *\/);\n+    f.next(SmallRegisterMap::instance_no_args(), true \/* stop *\/);\n@@ -2114,1 +2268,1 @@\n-  f.next(SmallRegisterMap::instance(), true \/* stop *\/);\n+  f.next(SmallRegisterMap::instance_no_args(), true \/* stop *\/);\n@@ -2246,2 +2400,3 @@\n-  set_anchor(_thread, rs.sp());\n-  log_frames(_thread);\n+    frame top(rs.sp());\n+    AnchorMark am(_thread, top);\n+    log_frames(_thread);\n@@ -2251,1 +2406,0 @@\n-  clear_anchor(_thread);\n@@ -2274,0 +2428,1 @@\n+  _process_args_at_top = false;\n@@ -2276,0 +2431,1 @@\n+    ObjectMonitor* mon = nullptr;\n@@ -2280,2 +2436,2 @@\n-      ObjectMonitor* mon = waiter->monitor();\n-      preempt_kind = waiter->is_wait() ? Continuation::freeze_on_wait : Continuation::freeze_on_monitorenter;\n+      mon = waiter->monitor();\n+      preempt_kind = waiter->is_wait() ? Continuation::object_wait : Continuation::monitorenter;\n@@ -2287,0 +2443,1 @@\n+        log_develop_trace(continuations, preempt)(\"Failed to acquire monitor, unmounting again\");\n@@ -2290,0 +2447,1 @@\n+      JVMTI_ONLY(assert(_thread->contended_entered_monitor() == nullptr || _thread->contended_entered_monitor() == mon, \"\"));\n@@ -2291,3 +2449,6 @@\n-      \/\/ Preemption cancelled in moniterenter case. We actually acquired\n-      \/\/ the monitor after freezing all frames so nothing to do.\n-      preempt_kind = Continuation::freeze_on_monitorenter;\n+      \/\/ Preemption cancelled on moniterenter or ObjectLocker case. We\n+      \/\/ actually acquired the monitor after freezing all frames so no\n+      \/\/ need to call resume_operation. If this is the ObjectLocker case\n+      \/\/ we released the monitor already at ~ObjectLocker, so _init_lock\n+      \/\/ will be set to nullptr below since there is no monitor to release.\n+      preempt_kind = Continuation::monitorenter;\n@@ -2295,0 +2456,1 @@\n+\n@@ -2297,0 +2459,12 @@\n+\n+    if (chunk->at_klass_init()) {\n+      preempt_kind = Continuation::object_locker;\n+      chunk->set_at_klass_init(false);\n+      _process_args_at_top = chunk->has_args_at_top();\n+      if (_process_args_at_top) {\n+        \/\/ Only needed for the top frame which will be thawed.\n+        chunk->set_has_args_at_top(false);\n+      }\n+      assert(waiter == nullptr || mon != nullptr, \"should have a monitor\");\n+      _init_lock = mon;  \/\/ remember monitor since we will need it on handle_preempted_continuation()\n+    }\n@@ -2357,1 +2531,1 @@\n-  JVMTI_ONLY(invalidate_jvmti_stack(_thread));\n+  JVMTI_ONLY(if (!_cont.entry()->is_virtual_thread()) invalidate_jvmti_stack(_thread));\n@@ -2380,1 +2554,1 @@\n-    recurse_thaw_interpreted_frame(heap_frame, caller, num_frames);\n+    recurse_thaw_interpreted_frame(heap_frame, caller, num_frames, top_on_preempt_case);\n@@ -2393,1 +2567,1 @@\n-  _stream.next(SmallRegisterMap::instance());\n+  _stream.next(SmallRegisterMap::instance_no_args());\n@@ -2503,1 +2677,0 @@\n-  assert(preempt_kind == Continuation::freeze_on_wait || preempt_kind == Continuation::freeze_on_monitorenter, \"\");\n@@ -2506,0 +2679,2 @@\n+  DEBUG_ONLY(verify_frame_kind(top, preempt_kind);)\n+  NOT_PRODUCT(int64_t tid = _thread->monitor_owner_id();)\n@@ -2513,1 +2688,1 @@\n-      jvmti_mount_end(_thread, _cont, top);\n+      jvmti_mount_end(_thread, _cont, top, preempt_kind);\n@@ -2530,1 +2705,1 @@\n-  if (preempt_kind == Continuation::freeze_on_wait) {\n+  if (preempt_kind == Continuation::object_wait) {\n@@ -2532,1 +2707,2 @@\n-    if (_thread->pending_interrupted_exception()) {\n+    bool throw_ie = _thread->pending_interrupted_exception();\n+    if (throw_ie) {\n@@ -2536,5 +2712,68 @@\n-  } else if (top.is_runtime_frame()) {\n-    \/\/ The continuation might now run on a different platform thread than the previous time so\n-    \/\/ we need to adjust the current thread saved in the stub frame before restoring registers.\n-    JavaThread** thread_addr = frame::saved_thread_address(top);\n-    if (thread_addr != nullptr) *thread_addr = _thread;\n+    log_develop_trace(continuations, preempt)(\"Resuming \" INT64_FORMAT\" after preemption on Object.wait%s\", tid, throw_ie ? \"(throwing IE)\" : \"\");\n+  } else if (preempt_kind == Continuation::monitorenter) {\n+    if (top.is_runtime_frame()) {\n+      \/\/ The continuation might now run on a different platform thread than the previous time so\n+      \/\/ we need to adjust the current thread saved in the stub frame before restoring registers.\n+      JavaThread** thread_addr = frame::saved_thread_address(top);\n+      if (thread_addr != nullptr) *thread_addr = _thread;\n+    }\n+    log_develop_trace(continuations, preempt)(\"Resuming \" INT64_FORMAT \" after preemption on monitorenter\", tid);\n+  } else {\n+    \/\/ We need to redo the original call into the VM. First though, we need\n+    \/\/ to exit the monitor we just acquired (except on preemption cancelled\n+    \/\/ case where it was already released).\n+    assert(preempt_kind == Continuation::object_locker, \"\");\n+    if (_init_lock != nullptr) _init_lock->exit(_thread);\n+    sp = redo_vmcall(_thread, top);\n+  }\n+  return sp;\n+}\n+\n+intptr_t* ThawBase::redo_vmcall(JavaThread* current, frame& top) {\n+  assert(!current->preempting(), \"\");\n+  NOT_PRODUCT(int64_t tid = current->monitor_owner_id();)\n+  intptr_t* sp = top.sp();\n+\n+  {\n+    HandleMarkCleaner hmc(current);  \/\/ Cleanup all handles (including so._conth) before returning to Java.\n+    ContinuationWrapper::SafepointOp so(current, _cont);\n+    AnchorMark am(current, top);    \/\/ Set the anchor so that the stack is walkable.\n+\n+    Method* m = top.interpreter_frame_method();\n+    Bytecode current_bytecode = Bytecode(m, top.interpreter_frame_bcp());\n+    Bytecodes::Code code = current_bytecode.code();\n+    log_develop_trace(continuations, preempt)(\"Redoing InterpreterRuntime::%s for \" INT64_FORMAT, code == Bytecodes::Code::_new ? \"_new\" : \"resolve_from_cache\", tid);\n+\n+    \/\/ These InterpreterRuntime entry points use JRT_ENTRY which uses a HandleMarkCleaner.\n+    \/\/ Create a HandeMark to avoid destroying so._conth.\n+    HandleMark hm(current);\n+    DEBUG_ONLY(JavaThread::AtRedoVMCall apvmc(current);)\n+    if (code == Bytecodes::Code::_new) {\n+      InterpreterRuntime::_new(current, m->constants(), current_bytecode.get_index_u2(code));\n+    } else {\n+      InterpreterRuntime::resolve_from_cache(current, code);\n+    }\n+  }\n+\n+  if (current->preempting()) {\n+    \/\/ Preempted again so we just arrange to return to preempt stub to unmount.\n+    sp = push_preempt_adapter();\n+    current->set_preempt_alternate_return(nullptr);\n+    bool cancelled = current->preemption_cancelled();\n+    if (cancelled) {\n+      \/\/ Since preemption was cancelled, the thread will call thaw again from the preempt\n+      \/\/ stub. These retries could happen several times due to contention on the init_lock,\n+      \/\/ so just let the vthread umount to give a chance for other vthreads to run.\n+      current->set_preemption_cancelled(false);\n+      oop vthread = current->vthread();\n+      assert(java_lang_VirtualThread::state(vthread) == java_lang_VirtualThread::RUNNING, \"wrong state for vthread\");\n+      java_lang_VirtualThread::set_state(vthread, java_lang_VirtualThread::YIELDING);\n+#if INCLUDE_JVMTI\n+      if (current->contended_entered_monitor() != nullptr) {\n+        current->set_contended_entered_monitor(nullptr);\n+      }\n+#endif\n+    }\n+    log_develop_trace(continuations, preempt)(\"Preempted \" INT64_FORMAT \" again%s\", tid, cancelled ? \"(preemption cancelled, setting state to YIELDING)\" : \"\");\n+  } else {\n+    log_develop_trace(continuations, preempt)(\"Call succesful, resuming \" INT64_FORMAT, tid);\n@@ -2546,0 +2785,1 @@\n+  HandleMarkCleaner hm(current);  \/\/ Cleanup all handles (including so._conth) before returning to Java.\n@@ -2547,2 +2787,1 @@\n-  \/\/ Since we might safepoint set the anchor so that the stack can be walked.\n-  set_anchor(current, top.sp());\n+  AnchorMark am(current, top);  \/\/ Set the anchor so that the stack is walkable.\n@@ -2552,1 +2791,0 @@\n-  clear_anchor(current);\n@@ -2555,1 +2793,1 @@\n-NOINLINE void ThawBase::recurse_thaw_interpreted_frame(const frame& hf, frame& caller, int num_frames) {\n+NOINLINE void ThawBase::recurse_thaw_interpreted_frame(const frame& hf, frame& caller, int num_frames, bool is_top) {\n@@ -2559,1 +2797,5 @@\n-    _cont.tail()->do_barriers<stackChunkOopDesc::BarrierType::Store>(_stream, SmallRegisterMap::instance());\n+    if (is_top && _process_args_at_top) {\n+      _cont.tail()->do_barriers<stackChunkOopDesc::BarrierType::Store>(_stream, SmallRegisterMap::instance_with_args());\n+    } else {\n+      _cont.tail()->do_barriers<stackChunkOopDesc::BarrierType::Store>(_stream, SmallRegisterMap::instance_no_args());\n+    }\n@@ -2604,1 +2846,1 @@\n-    _cont.tail()->fix_thawed_frame(caller, SmallRegisterMap::instance());\n+    _cont.tail()->fix_thawed_frame(caller, SmallRegisterMap::instance_no_args());\n@@ -2621,1 +2863,1 @@\n-    _cont.tail()->do_barriers<stackChunkOopDesc::BarrierType::Store>(_stream, SmallRegisterMap::instance());\n+    _cont.tail()->do_barriers<stackChunkOopDesc::BarrierType::Store>(_stream, SmallRegisterMap::instance_no_args());\n@@ -2687,1 +2929,1 @@\n-    _cont.tail()->fix_thawed_frame(caller, SmallRegisterMap::instance());\n+    _cont.tail()->fix_thawed_frame(caller, SmallRegisterMap::instance_no_args());\n@@ -2713,1 +2955,1 @@\n-    _stream.next(SmallRegisterMap::instance());\n+    _stream.next(SmallRegisterMap::instance_no_args());\n@@ -2753,1 +2995,1 @@\n-    _cont.tail()->do_barriers<stackChunkOopDesc::BarrierType::Store>(_stream, SmallRegisterMap::instance());\n+    _cont.tail()->do_barriers<stackChunkOopDesc::BarrierType::Store>(_stream, SmallRegisterMap::instance_no_args());\n@@ -2791,1 +3033,1 @@\n-  _cont.tail()->fix_thawed_frame(caller, SmallRegisterMap::instance());\n+  _cont.tail()->fix_thawed_frame(caller, SmallRegisterMap::instance_no_args());\n@@ -2818,1 +3060,6 @@\n-  chunk->fix_thawed_frame(f, SmallRegisterMap::instance()); \/\/ can only fix caller after push_return_frame (due to callee saved regs)\n+   \/\/ can only fix caller after push_return_frame (due to callee saved regs)\n+  if (_process_args_at_top) {\n+    chunk->fix_thawed_frame(f, SmallRegisterMap::instance_with_args());\n+  } else {\n+    chunk->fix_thawed_frame(f, SmallRegisterMap::instance_no_args());\n+  }\n@@ -2832,1 +3079,1 @@\n-void ThawBase::push_return_frame(frame& f) { \/\/ see generate_cont_thaw\n+void ThawBase::push_return_frame(const frame& f) { \/\/ see generate_cont_thaw\n@@ -2880,1 +3127,0 @@\n-  DEBUG_ONLY(bool preempted = cont.tail()->preempted();)\n@@ -2884,1 +3130,1 @@\n-  DEBUG_ONLY(log_frames_after_thaw(thread, cont, sp, preempted);)\n+  DEBUG_ONLY(log_frames_after_thaw(thread, cont, sp);)\n@@ -3026,1 +3272,1 @@\n-static void log_frames_after_thaw(JavaThread* thread, ContinuationWrapper& cont, intptr_t* sp, bool preempted) {\n+static void log_frames_after_thaw(JavaThread* thread, ContinuationWrapper& cont, intptr_t* sp) {\n@@ -3030,1 +3276,3 @@\n-  if (preempted && sp0 == cont.entrySP()) {\n+  bool preempted = false;\n+  stackChunkOop tail = cont.tail();\n+  if (tail != nullptr && tail->preempted()) {\n@@ -3032,1 +3280,1 @@\n-    assert(cont.tail()->preempted(), \"\");\n+    preempted = true;\n@@ -3042,1 +3290,1 @@\n-  assert(ContinuationEntry::assert_entry_frame_laid_out(thread), \"\");\n+  assert(preempted || ContinuationEntry::assert_entry_frame_laid_out(thread), \"\");\n@@ -3113,1 +3361,1 @@\n-      FOR_EACH_CONCRETE_BARRIER_SET_DO(BARRIER_SET_RESOLVE_BARRIER_CLOSURE)\n+      FOR_EACH_BARRIER_SET_DO(BARRIER_SET_RESOLVE_BARRIER_CLOSURE)\n","filename":"src\/hotspot\/share\/runtime\/continuationFreezeThaw.cpp","additions":315,"deletions":67,"binary":false,"changes":382,"status":"modified"},{"patch":"@@ -549,0 +549,3 @@\n+  if (exec_mode == Unpack_deopt) {\n+    assert(deoptee.is_deoptimized_frame(), \"frame is not marked for deoptimization\");\n+  }\n@@ -1455,0 +1458,3 @@\n+#if INCLUDE_JVMCI\n+      \/\/ big_value allows encoding double\/long value as e.g. [int = 0, long], and storing\n+      \/\/ the value in two array elements.\n@@ -1482,0 +1488,3 @@\n+#else \/\/ not INCLUDE_JVMCI\n+      obj->int_at_put(index, value->get_jint());\n+#endif \/\/ INCLUDE_JVMCI\n","filename":"src\/hotspot\/share\/runtime\/deoptimization.cpp","additions":9,"deletions":0,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -213,1 +213,1 @@\n-    return nm->deopt_handler_begin() - pc_return_offset;\n+    return nm->deopt_handler_entry() - pc_return_offset;\n@@ -362,1 +362,1 @@\n-  address deopt = nm->deopt_handler_begin();\n+  address deopt = nm->deopt_handler_entry();\n@@ -921,1 +921,2 @@\n-void frame::oops_interpreted_do(OopClosure* f, const RegisterMap* map, bool query_oop_map_cache) const {\n+template <typename RegisterMapT>\n+void frame::oops_interpreted_do(OopClosure* f, const RegisterMapT* map, bool query_oop_map_cache) const {\n@@ -958,3 +959,0 @@\n-  Symbol* signature = nullptr;\n-  bool has_receiver = false;\n-\n@@ -965,1 +963,1 @@\n-  if (!m->is_native()) {\n+  if (!m->is_native() && map != nullptr && map->include_argument_oops()) {\n@@ -967,18 +965,7 @@\n-    if (map != nullptr && call.is_valid()) {\n-      signature = call.signature();\n-      has_receiver = call.has_receiver();\n-      if (map->include_argument_oops() &&\n-          interpreter_frame_expression_stack_size() > 0) {\n-        ResourceMark rm(thread);  \/\/ is this right ???\n-        \/\/ we are at a call site & the expression stack is not empty\n-        \/\/ => process callee's arguments\n-        \/\/\n-        \/\/ Note: The expression stack can be empty if an exception\n-        \/\/       occurred during method resolution\/execution. In all\n-        \/\/       cases we empty the expression stack completely be-\n-        \/\/       fore handling the exception (the exception handling\n-        \/\/       code in the interpreter calls a blocking runtime\n-        \/\/       routine which can cause this code to be executed).\n-        \/\/       (was bug gri 7\/27\/98)\n-        oops_interpreted_arguments_do(signature, has_receiver, f);\n-      }\n+    if (call.is_valid() && interpreter_frame_expression_stack_size() > 0) {\n+      ResourceMark rm(thread);  \/\/ is this right ???\n+      Symbol* signature = call.signature();\n+      bool has_receiver = call.has_receiver();\n+      \/\/ We are at a call site & the expression stack is not empty\n+      \/\/ so we might have callee arguments we need to process.\n+      oops_interpreted_arguments_do(signature, has_receiver, f);\n@@ -1018,0 +1005,4 @@\n+template void frame::oops_interpreted_do(OopClosure* f, const RegisterMap* map, bool query_oop_map_cache) const;\n+template void frame::oops_interpreted_do(OopClosure* f, const SmallRegisterMapNoArgs* map, bool query_oop_map_cache) const;\n+template void frame::oops_interpreted_do(OopClosure* f, const SmallRegisterMapWithArgs* map, bool query_oop_map_cache) const;\n+\n","filename":"src\/hotspot\/share\/runtime\/frame.cpp","additions":16,"deletions":25,"binary":false,"changes":41,"status":"modified"},{"patch":"@@ -459,1 +459,2 @@\n-  void oops_interpreted_do(OopClosure* f, const RegisterMap* map, bool query_oop_map_cache = true) const;\n+  template <typename RegisterMapT>\n+  void oops_interpreted_do(OopClosure* f, const RegisterMapT* map, bool query_oop_map_cache = true) const;\n","filename":"src\/hotspot\/share\/runtime\/frame.hpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -62,0 +62,1 @@\n+  assert(!thread->preempting(), \"Unexpected Java upcall whilst processing preemption\");\n@@ -247,1 +248,1 @@\n-  LinkResolver::resolve_static_call(callinfo, link_info, true, CHECK);\n+  LinkResolver::resolve_static_call(callinfo, link_info, ClassInitMode::init, CHECK);\n","filename":"src\/hotspot\/share\/runtime\/javaCalls.cpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -448,0 +448,1 @@\n+  _throwing_unsafe_access_error(false),\n@@ -497,0 +498,4 @@\n+  _at_preemptable_init(false),\n+  DEBUG_ONLY(_preempt_init_klass(nullptr) COMMA)\n+  DEBUG_ONLY(_interp_at_preemptable_vmcall_cnt(0) COMMA)\n+  DEBUG_ONLY(_interp_redoing_vm_call(false) COMMA)\n","filename":"src\/hotspot\/share\/runtime\/javaThread.cpp","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -321,0 +321,1 @@\n+  volatile bool         _throwing_unsafe_access_error;   \/\/ Thread has faulted and is throwing an exception\n@@ -488,0 +489,3 @@\n+  \/\/ We allow preemption on some klass initialization calls.\n+  \/\/ We use this boolean to mark such calls.\n+  bool _at_preemptable_init;\n@@ -496,1 +500,1 @@\n-  bool preempting()           { return _preempt_alternate_return != nullptr; }\n+  bool preempting()                              { return _preempt_alternate_return != nullptr; }\n@@ -499,1 +503,35 @@\n-private:\n+  bool at_preemptable_init()           { return _at_preemptable_init; }\n+  void set_at_preemptable_init(bool b) { _at_preemptable_init = b; }\n+\n+#ifdef ASSERT\n+  \/\/ Used for extra logging with -Xlog:continuation+preempt\n+  InstanceKlass* _preempt_init_klass;\n+\n+  InstanceKlass* preempt_init_klass() { return _preempt_init_klass; }\n+  void set_preempt_init_klass(InstanceKlass* ik) { _preempt_init_klass = ik; }\n+\n+  int _interp_at_preemptable_vmcall_cnt;\n+  int interp_at_preemptable_vmcall_cnt() { return _interp_at_preemptable_vmcall_cnt; }\n+\n+  bool _interp_redoing_vm_call;\n+  bool interp_redoing_vm_call() const { return _interp_redoing_vm_call; };\n+\n+  class AtRedoVMCall : public StackObj {\n+    JavaThread* _thread;\n+   public:\n+    AtRedoVMCall(JavaThread* t) : _thread(t) {\n+      assert(!_thread->_interp_redoing_vm_call, \"\");\n+      _thread->_interp_redoing_vm_call = true;\n+      _thread->_interp_at_preemptable_vmcall_cnt++;\n+      assert(_thread->_interp_at_preemptable_vmcall_cnt > 0, \"Unexpected count: %d\",\n+             _thread->_interp_at_preemptable_vmcall_cnt);\n+    }\n+    ~AtRedoVMCall() {\n+      assert(_thread->_interp_redoing_vm_call, \"\");\n+      _thread->_interp_redoing_vm_call = false;\n+      _thread->_interp_at_preemptable_vmcall_cnt--;\n+      assert(_thread->_interp_at_preemptable_vmcall_cnt >= 0, \"Unexpected count: %d\",\n+             _thread->_interp_at_preemptable_vmcall_cnt);\n+    }\n+  };\n+#endif \/\/ ASSERT\n@@ -501,0 +539,1 @@\n+private:\n@@ -626,0 +665,3 @@\n+  bool is_throwing_unsafe_access_error()          { return _throwing_unsafe_access_error; }\n+  void set_throwing_unsafe_access_error(bool val) { _throwing_unsafe_access_error = val; }\n+\n@@ -889,0 +931,1 @@\n+  DEBUG_ONLY(static ByteSize interp_at_preemptable_vmcall_cnt_offset() { return byte_offset_of(JavaThread, _interp_at_preemptable_vmcall_cnt); })\n@@ -1334,2 +1377,2 @@\n-  NoPreemptMark(JavaThread* thread) : _ce(thread->last_continuation()), _unpin(false) {\n-    if (_ce != nullptr) _unpin = _ce->pin();\n+  NoPreemptMark(JavaThread* thread, bool ignore_mark = false) : _ce(thread->last_continuation()), _unpin(false) {\n+    if (_ce != nullptr && !ignore_mark) _unpin = _ce->pin();\n@@ -1362,0 +1405,14 @@\n+class ThrowingUnsafeAccessError : public StackObj {\n+  JavaThread* _thread;\n+  bool _prev;\n+public:\n+  ThrowingUnsafeAccessError(JavaThread* thread) :\n+      _thread(thread),\n+      _prev(thread->is_throwing_unsafe_access_error()) {\n+    _thread->set_throwing_unsafe_access_error(true);\n+  }\n+  ~ThrowingUnsafeAccessError() {\n+    _thread->set_throwing_unsafe_access_error(_prev);\n+  }\n+};\n+\n","filename":"src\/hotspot\/share\/runtime\/javaThread.hpp","additions":61,"deletions":4,"binary":false,"changes":65,"status":"modified"},{"patch":"@@ -96,0 +96,3 @@\n+#ifdef COMPILER2\n+#include \"opto\/runtime.hpp\"\n+#endif\n@@ -610,0 +613,5 @@\n+#ifdef COMPILER2\n+      if (nm->compiler_type() == compiler_c2) {\n+        return OptoRuntime::exception_blob()->entry_point();\n+      }\n+#endif \/\/ COMPILER2\n","filename":"src\/hotspot\/share\/runtime\/sharedRuntime.cpp","additions":8,"deletions":0,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -29,0 +29,1 @@\n+#include \"memory\/iterator.hpp\"\n@@ -83,1 +84,2 @@\n-  inline int num_oops() const;\n+  template <typename RegisterMapT>\n+  inline int num_oops(RegisterMapT* map) const;\n@@ -105,1 +107,2 @@\n-  inline int interpreter_frame_num_oops() const;\n+  template <typename RegisterMapT>\n+  inline int interpreter_frame_num_oops(RegisterMapT* map) const;\n@@ -127,0 +130,9 @@\n+class InterpreterOopCount : public OopClosure {\n+  int _count;\n+public:\n+  InterpreterOopCount() : _count(0) {}\n+  void do_oop(oop* p) override { _count++; }\n+  void do_oop(narrowOop* p) override { _count++; }\n+  int count() { return _count; }\n+};\n+\n","filename":"src\/hotspot\/share\/runtime\/stackChunkFrameStream.hpp","additions":14,"deletions":2,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -202,1 +202,2 @@\n-inline int StackChunkFrameStream<frame_kind>::num_oops() const {\n+template <typename RegisterMapT>\n+inline int StackChunkFrameStream<frame_kind>::num_oops(RegisterMapT* map) const {\n@@ -204,1 +205,1 @@\n-    return interpreter_frame_num_oops();\n+    return interpreter_frame_num_oops(map);\n@@ -386,1 +387,1 @@\n-    f.oops_interpreted_do(closure, nullptr, true);\n+    f.oops_interpreted_do(closure, map, true);\n","filename":"src\/hotspot\/share\/runtime\/stackChunkFrameStream.inline.hpp","additions":4,"deletions":3,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -142,1 +142,1 @@\n-template StackValue* StackValue::create_stack_value(const frame* fr, const SmallRegisterMap* reg_map, ScopeValue* sv);\n+template StackValue* StackValue::create_stack_value(const frame* fr, const SmallRegisterMapNoArgs* reg_map, ScopeValue* sv);\n@@ -267,1 +267,1 @@\n-template address StackValue::stack_value_address(const frame* fr, const SmallRegisterMap* reg_map, ScopeValue* sv);\n+template address StackValue::stack_value_address(const frame* fr, const SmallRegisterMapNoArgs* reg_map, ScopeValue* sv);\n","filename":"src\/hotspot\/share\/runtime\/stackValue.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -508,2 +508,4 @@\n-ObjectLocker::ObjectLocker(Handle obj, JavaThread* thread) : _npm(thread) {\n-  _thread = thread;\n+ObjectLocker::ObjectLocker(Handle obj, TRAPS) : _thread(THREAD), _obj(obj),\n+  _npm(_thread, _thread->at_preemptable_init() \/* ignore_mark *\/), _skip_exit(false) {\n+  assert(!_thread->preempting(), \"\");\n+\n@@ -511,1 +513,0 @@\n-  _obj = obj;\n@@ -515,0 +516,14 @@\n+\n+    if (_thread->preempting()) {\n+      \/\/ If preemption was cancelled we acquired the monitor after freezing\n+      \/\/ the frames. Redoing the vm call laterin thaw will require us to\n+      \/\/ release it since the call should look like the original one. We\n+      \/\/ do it in ~ObjectLocker to reduce the window of time we hold the\n+      \/\/ monitor since we can't do anything useful with it now, and would\n+      \/\/ otherwise just force other vthreads to preempt in case they try\n+      \/\/ to acquire this monitor.\n+      _skip_exit = !_thread->preemption_cancelled();\n+      ObjectSynchronizer::read_monitor(_thread, _obj())->set_object_strong();\n+      _thread->set_pending_preempted_exception();\n+\n+    }\n@@ -519,1 +534,1 @@\n-  if (_obj() != nullptr) {\n+  if (_obj() != nullptr && !_skip_exit) {\n@@ -524,0 +539,8 @@\n+void ObjectLocker::wait_uninterruptibly(TRAPS) {\n+  ObjectSynchronizer::waitUninterruptibly(_obj, 0, _thread);\n+  if (_thread->preempting()) {\n+    _skip_exit = true;\n+    ObjectSynchronizer::read_monitor(_thread, _obj())->set_object_strong();\n+    _thread->set_pending_preempted_exception();\n+  }\n+}\n@@ -551,3 +574,1 @@\n-  if (millis < 0) {\n-    THROW_MSG(vmSymbols::java_lang_IllegalArgumentException(), \"timeout value is negative\");\n-  }\n+  assert(millis >= 0, \"timeout value is negative\");\n","filename":"src\/hotspot\/share\/runtime\/synchronizer.cpp","additions":28,"deletions":7,"binary":false,"changes":35,"status":"modified"},{"patch":"@@ -413,0 +413,1 @@\n+  initialize_class(vmSymbols::jdk_internal_vm_PreemptedException(), CHECK);\n","filename":"src\/hotspot\/share\/runtime\/threads.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -540,1 +540,1 @@\n-  nonstatic_field(nmethod,                     _deopt_handler_offset,                         int)                                   \\\n+  nonstatic_field(nmethod,                     _deopt_handler_entry_offset,                   int)                                   \\\n@@ -543,0 +543,1 @@\n+  nonstatic_field(nmethod,                     _immutable_data_ref_count_offset,              int)                                   \\\n","filename":"src\/hotspot\/share\/runtime\/vmStructs.cpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -97,0 +97,4 @@\n+  void set_pending_preempted_exception();\n+  void clear_pending_preempted_exception();\n+  void check_preempted_exception() NOT_DEBUG_RETURN;\n+\n@@ -233,0 +237,2 @@\n+#define CHECK_PREEMPTABLE                        THREAD); if (HAS_PENDING_EXCEPTION) { THREAD->check_preempted_exception(); return;       } (void)(0\n+#define CHECK_PREEMPTABLE_false                  THREAD); if (HAS_PENDING_EXCEPTION) { THREAD->check_preempted_exception(); return false; } (void)(0\n@@ -252,0 +258,4 @@\n+#define CLEAR_PENDING_PREEMPTED_EXCEPTION       (((ThreadShadow*)THREAD)->clear_pending_preempted_exception())\n+#define CHECK_AND_CLEAR_PREEMPTED               THREAD); if (HAS_PENDING_EXCEPTION) { CLEAR_PENDING_PREEMPTED_EXCEPTION; return; } (void)(0\n+\n+\n","filename":"src\/hotspot\/share\/utilities\/exceptions.hpp","additions":10,"deletions":0,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -29,0 +29,3 @@\n+#include \"cppstdlib\/cstddef.hpp\"\n+#include \"cppstdlib\/limits.hpp\"\n+#include \"cppstdlib\/type_traits.hpp\"\n@@ -37,3 +40,0 @@\n-#include <cstddef>\n-#include <limits>\n-#include <type_traits>\n","filename":"src\/hotspot\/share\/utilities\/globalDefinitions.hpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -70,4 +70,17 @@\n-        \/\/ while building the interim javac, the ClassReader will produce a warning when loading a class\n-        \/\/ keeping the constant of a feature that has been integrated or dropped, serves the purpose of muting such warnings.\n-        IMPLICIT_CLASSES, \/\/to be removed when boot JDK is 25\n-        SCOPED_VALUES,\n+        \/\/ The JDK build process involves creating an interim javac which is then\n+        \/\/ used to compile the rest of the JDK. The jdk.internal.javac.PreviewFeature\n+        \/\/ annotation from the current sources is used when compiling interim javac.\n+        \/\/ That's because the javac APIs of the current sources may be annotated with\n+        \/\/ this annotation and they may be using the enum constants of the current sources.\n+        \/\/ Furthermore, when compiling interim javac, the class files from the bootstrap JDK get\n+        \/\/ used and those may also contain the PreviewFeature annotation. However, they may be\n+        \/\/ using the enum constants of the bootstrap JDK's PreviewFeature annotation.\n+        \/\/ If javac sees an annotation with an unknown enum constant, it produces a warning,\n+        \/\/ and that in turn fails the build.\n+        \/\/ So, in the current sources, we need to preserve the PreviewFeature enum constants\n+        \/\/ for as long as the interim javac build needs it. As a result, we retain PreviewFeature\n+        \/\/ enum constants for preview features that are present in the bootstrap JDK.\n+        \/\/ Older constants can be removed.\n+        \/\/\n+        \/\/ For example, Class-File API became final in JDK 24. As soon as JDK 23 was dropped as\n+        \/\/ the bootstrap JDK, the CLASSFILE_API enum constant became eligible for removal.\n@@ -77,4 +90,0 @@\n-        CLASSFILE_API,\n-        STREAM_GATHERERS,\n-        MODULE_IMPORTS, \/\/remove when the boot JDK is JDK 25\n-        KEY_DERIVATION, \/\/remove when the boot JDK is JDK 25\n","filename":"src\/java.base\/share\/classes\/jdk\/internal\/javac\/PreviewFeature.java","additions":17,"deletions":8,"binary":false,"changes":25,"status":"modified"},{"patch":"@@ -1355,1 +1355,1 @@\n-        TypeRelation isSameTypeVisitor = new TypeRelation() {\n+        abstract class TypeEqualityVisitor extends TypeRelation {\n@@ -1394,1 +1394,1 @@\n-                            isSameType(t.type, t2.type);\n+                            sameTypeComparator(t.type, t2.type);\n@@ -1398,0 +1398,2 @@\n+            abstract boolean sameTypeComparator(Type t, Type s);\n+\n@@ -1428,1 +1430,1 @@\n-                    && containsTypeEquivalent(t.getTypeArguments(), s.getTypeArguments());\n+                    && sameTypeArguments(t.getTypeArguments(), s.getTypeArguments());\n@@ -1431,0 +1433,2 @@\n+            abstract boolean sameTypeArguments(List<Type> ts, List<Type> ss);\n+\n@@ -1486,0 +1490,10 @@\n+        }\n+\n+        TypeEqualityVisitor isSameTypeVisitor = new TypeEqualityVisitor() {\n+            boolean sameTypeComparator(Type t, Type s) {\n+                return isSameType(t, s);\n+            }\n+\n+            boolean sameTypeArguments(List<Type> ts, List<Type> ss) {\n+                return containsTypeEquivalent(ts, ss);\n+            }\n@@ -3879,1 +3893,1 @@\n-            final Type t2;;\n+            final Type t2;\n@@ -3892,2 +3906,2 @@\n-                        && isSameType(t1, typePair.t1)\n-                        && isSameType(t2, typePair.t2);\n+                        && exactTypeVisitor.visit(t1, typePair.t1)\n+                        && exactTypeVisitor.visit(t2, typePair.t2);\n@@ -3896,0 +3910,18 @@\n+\n+        TypeEqualityVisitor exactTypeVisitor = new TypeEqualityVisitor() {\n+            @Override\n+            boolean sameTypeArguments(List<Type> ts, List<Type> ss) {\n+                while (ts.nonEmpty() && ss.nonEmpty()\n+                        && sameTypeComparator(ts.head, ss.head)) {\n+                    ts = ts.tail;\n+                    ss = ss.tail;\n+                }\n+                return ts.isEmpty() && ss.isEmpty();\n+            }\n+\n+            @Override\n+            boolean sameTypeComparator(Type t, Type s) {\n+                return exactTypeVisitor.visit(t, s);\n+            }\n+        };\n+\n@@ -4107,9 +4139,8 @@\n-    \/\/ where\n-        List<Type> erasedSupertypes(Type t) {\n-            ListBuffer<Type> buf = new ListBuffer<>();\n-            for (Type sup : closure(t)) {\n-                if (sup.hasTag(TYPEVAR)) {\n-                    buf.append(sup);\n-                } else {\n-                    buf.append(erasure(sup));\n-                }\n+\n+    public List<Type> erasedSupertypes(Type t) {\n+        ListBuffer<Type> buf = new ListBuffer<>();\n+        for (Type sup : closure(t)) {\n+            if (sup.hasTag(TYPEVAR)) {\n+                buf.append(sup);\n+            } else {\n+                buf.append(erasure(sup));\n@@ -4117,1 +4148,2 @@\n-            return buf.toList();\n+        return buf.toList();\n+    }\n@@ -4120,0 +4152,1 @@\n+    \/\/ where\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/code\/Types.java","additions":49,"deletions":16,"binary":false,"changes":65,"status":"modified"},{"patch":"@@ -35,0 +35,1 @@\n+import static com.sun.tools.javac.code.TypeTag.ARRAY;\n@@ -40,0 +41,1 @@\n+import static com.sun.tools.javac.code.TypeTag.TYPEVAR;\n@@ -1860,3 +1862,9 @@\n-                Type lub = types.lub(t1, t2);\n-\n-                if (lub.hasTag(BOT)) {\n+                \/* the most semantically correct approach here would be to invoke Types::lub\n+                 * and then erase the result.\n+                 * But this approach can be too slow for some complex cases, see JDK-8369654.\n+                 * This is why the method below leverages the fact that the result\n+                 * will be erased to produce a correct supertype using a simpler approach compared\n+                 * to a full blown lub.\n+                 *\/\n+                Type es = erasedSuper(t1, t2);\n+                if (es == null || es.hasTag(BOT)) {\n@@ -1866,0 +1874,3 @@\n+                return es;\n+            }\n+        }\n@@ -1867,1 +1878,20 @@\n-                return types.erasure(lub);\n+        private Type erasedSuper(Type t1, Type t2) {\n+            if (t1.hasTag(ARRAY) && t2.hasTag(ARRAY)) {\n+                Type elem1 = types.elemtype(t1);\n+                Type elem2 = types.elemtype(t2);\n+                if (elem1.isPrimitive() || elem2.isPrimitive()) {\n+                    return (elem1.tsym == elem2.tsym) ? t1 : syms.serializableType;\n+                } else { \/\/ both are arrays of references\n+                    return new ArrayType(erasedSuper(elem1, elem2), syms.arrayClass);\n+                }\n+            } else {\n+                t1 = types.skipTypeVars(t1, false);\n+                t2 = types.skipTypeVars(t2, false);\n+                List<Type> intersection = types.intersect(\n+                        t1.hasTag(ARRAY) ?\n+                                List.of(syms.serializableType, syms.cloneableType, syms.objectType) :\n+                                types.erasedSupertypes(t1),\n+                        t2.hasTag(ARRAY) ?\n+                                List.of(syms.serializableType, syms.cloneableType, syms.objectType) :\n+                                types.erasedSupertypes(t2));\n+                return intersection.head;\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/jvm\/Code.java","additions":34,"deletions":4,"binary":false,"changes":38,"status":"modified"},{"patch":"@@ -1557,1 +1557,11 @@\n-                    mref.expr = toP(F.at(pos).AnnotatedType(typeAnnos, mref.expr));\n+                    if (TreeInfo.isType(mref.expr, names)) {\n+                        mref.expr = insertAnnotationsToMostInner(mref.expr, typeAnnos, false);\n+                    } else {\n+                        \/\/the selector is not a type, error recovery:\n+                        JCAnnotatedType annotatedType =\n+                                toP(F.at(pos).AnnotatedType(typeAnnos, mref.expr));\n+                        int termStart = getStartPos(mref.expr);\n+                        mref.expr = syntaxError(termStart, List.of(annotatedType),\n+                                                Errors.IllegalStartOfType);\n+                    }\n+                    mref.pos = getStartPos(mref.expr);\n@@ -4965,1 +4975,2 @@\n-            if (!isVoid && typarams.isEmpty() && (token.kind == EQ || token.kind == SEMI)) {\n+            if (!isVoid && typarams.isEmpty() &&\n+                (token.kind == EQ || token.kind == SEMI || token.kind == COMMA)) {\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/parser\/JavacParser.java","additions":13,"deletions":2,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -455,0 +455,13 @@\n+        return isTypeSelector(base, names, TreeInfo::isStaticSym);\n+    }\n+    \/\/where\n+        private static boolean isStaticSym(JCTree tree) {\n+            Symbol sym = symbol(tree);\n+            return (sym.kind == TYP || sym.kind == PCK);\n+        }\n+\n+    public static boolean isType(JCTree base, Names names) {\n+        return isTypeSelector(base, names, _ -> true);\n+    }\n+\n+    private static boolean isTypeSelector(JCTree base, Names names, Predicate<JCTree> checkStaticSym) {\n@@ -462,1 +475,1 @@\n-                        isStaticSym(base);\n+                        checkStaticSym.test(base);\n@@ -464,1 +477,1 @@\n-                return isStaticSym(base) &&\n+                return checkStaticSym.test(base) &&\n@@ -475,5 +488,0 @@\n-    \/\/where\n-        private static boolean isStaticSym(JCTree tree) {\n-            Symbol sym = symbol(tree);\n-            return (sym.kind == TYP || sym.kind == PCK);\n-        }\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/tree\/TreeInfo.java","additions":15,"deletions":7,"binary":false,"changes":22,"status":"modified"},{"patch":"@@ -111,1 +111,1 @@\n-    ol.wait(THREAD);\n+    ol.wait_uninterruptibly(THREAD);\n","filename":"test\/hotspot\/gtest\/oops\/test_markWord.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -121,0 +121,3 @@\n+gc\/shenandoah\/TestRetainObjects.java#no-tlab 8361099 generic-all\n+gc\/shenandoah\/TestSieveObjects.java#no-tlab 8361099 generic-all\n+gc\/shenandoah\/TestSieveObjects.java#no-tlab-genshen 8361099 generic-all\n@@ -184,0 +187,1 @@\n+serviceability\/sa\/TestJhsdbJstackMixedWithXComp.java 8371194 linux-x64\n@@ -267,0 +271,1 @@\n+vmTestbase\/nsk\/jvmti\/scenarios\/events\/EM02\/em02t006\/TestDescription.java 8371103 generic-all\n","filename":"test\/hotspot\/jtreg\/ProblemList.txt","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -1959,0 +1959,5 @@\n+    public static final String SAFEPOINT_SCALAROBJECT_OF = COMPOSITE_PREFIX + \"SAFEPOINT_SCALAROBJECT_OF\" + POSTFIX;\n+    static {\n+        safepointScalarobjectOfNodes(SAFEPOINT_SCALAROBJECT_OF, \"SafePointScalarObject\");\n+    }\n+\n@@ -3274,0 +3279,5 @@\n+    private static void safepointScalarobjectOfNodes(String irNodePlaceholder, String irNodeRegex) {\n+        String regex = START + irNodeRegex + MID + \".*\" + IS_REPLACED + \".*\" + END;\n+        beforeMatching(irNodePlaceholder, regex);\n+    }\n+\n","filename":"test\/hotspot\/jtreg\/compiler\/lib\/ir_framework\/IRNode.java","additions":10,"deletions":0,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -37,5 +37,0 @@\n-javax\/management\/remote\/mandatory\/connection\/DeadLockTest.java 8309069 windows-x64\n-\n-javax\/management\/remote\/mandatory\/connection\/ConnectionTest.java 8308352 windows-x64\n-\n-\n","filename":"test\/jdk\/ProblemList-Virtual.txt","additions":0,"deletions":5,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -648,2 +648,0 @@\n-sun\/security\/ssl\/SSLLogger\/DebugPropertyValuesTest.java         8370852 generic-all\n-\n","filename":"test\/jdk\/ProblemList.txt","additions":0,"deletions":2,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -29,1 +29,1 @@\n- *          4802647 7123424 8024709 8193128 8327858 8346307\n+ *          4802647 7123424 8024709 8193128 8327858 8346307 8368178\n@@ -476,0 +476,1 @@\n+        THROWS(NoSuchElementException.class, c::getFirst, c::getLast);\n@@ -478,0 +479,1 @@\n+        equal2(c, c.reversed());\n@@ -1236,0 +1238,4 @@\n+        if (!l.isEmpty()) {\n+            equal(l.getFirst(), l.get(0));\n+            equal(l.getLast(), l.get(l.size() - 1));\n+        }\n","filename":"test\/jdk\/java\/util\/Collection\/MOAT.java","additions":7,"deletions":1,"binary":false,"changes":8,"status":"modified"}]}