{"files":[{"patch":"@@ -1693,0 +1693,3 @@\n+  } else if (_entry_point == nullptr) {\n+    \/\/ See CallLeafNoFPIndirect\n+    return 1 * NativeInstruction::instruction_size;\n@@ -1801,12 +1804,1 @@\n-  \/\/ n.b. frame size includes space for return pc and rfp\n-  const int framesize = C->output()->frame_size_in_bytes();\n-\n-  if (C->clinit_barrier_on_entry()) {\n-    assert(!C->method()->holder()->is_not_initialized(), \"initialization should have been started\");\n-\n-    Label L_skip_barrier;\n-    __ mov_metadata(rscratch2, C->method()->holder()->constant_encoding());\n-    __ clinit_barrier(rscratch2, rscratch1, &L_skip_barrier);\n-    __ far_jump(RuntimeAddress(SharedRuntime::get_handle_wrong_method_stub()));\n-    __ bind(L_skip_barrier);\n-  }\n+  __ verified_entry(C, 0);\n@@ -1815,2 +1807,2 @@\n-  if (C->max_vector_size() > 0) {\n-    __ reinitialize_ptrue();\n+  if (C->stub_function() == nullptr) {\n+    __ entry_barrier();\n@@ -1819,25 +1811,2 @@\n-  int bangsize = C->output()->bang_size_in_bytes();\n-  if (C->output()->need_stack_bang(bangsize))\n-    __ generate_stack_overflow_check(bangsize);\n-\n-  __ build_frame(framesize);\n-\n-  if (C->stub_function() == nullptr) {\n-    BarrierSetAssembler* bs = BarrierSet::barrier_set()->barrier_set_assembler();\n-    \/\/ Dummy labels for just measuring the code size\n-    Label dummy_slow_path;\n-    Label dummy_continuation;\n-    Label dummy_guard;\n-    Label* slow_path = &dummy_slow_path;\n-    Label* continuation = &dummy_continuation;\n-    Label* guard = &dummy_guard;\n-    if (!Compile::current()->output()->in_scratch_emit_size()) {\n-      \/\/ Use real labels from actual stub when not emitting code for the purpose of measuring its size\n-      C2EntryBarrierStub* stub = new (Compile::current()->comp_arena()) C2EntryBarrierStub();\n-      Compile::current()->output()->add_stub(stub);\n-      slow_path = &stub->entry();\n-      continuation = &stub->continuation();\n-      guard = &stub->guard();\n-    }\n-    \/\/ In the C2 code, we move the non-hot part of nmethod entry barriers out-of-line to a stub.\n-    bs->nmethod_entry_barrier(masm, slow_path, continuation, guard);\n+  if (!Compile::current()->output()->in_scratch_emit_size()) {\n+    __ bind(*_verified_entry);\n@@ -1860,6 +1829,0 @@\n-uint MachPrologNode::size(PhaseRegAlloc* ra_) const\n-{\n-  return MachNode::size(ra_); \/\/ too many variables; just compute it\n-                              \/\/ the hard way\n-}\n-\n@@ -1908,1 +1871,1 @@\n-  __ remove_frame(framesize);\n+  __ remove_frame(framesize, C->needs_stack_repair());\n@@ -1927,5 +1890,0 @@\n-uint MachEpilogNode::size(PhaseRegAlloc *ra_) const {\n-  \/\/ Variable size. Determine dynamically.\n-  return MachNode::size(ra_);\n-}\n-\n@@ -2227,1 +2185,12 @@\n-\/\/=============================================================================\n+\/\/\/=============================================================================\n+#ifndef PRODUCT\n+void MachVEPNode::format(PhaseRegAlloc* ra_, outputStream* st) const\n+{\n+  st->print_cr(\"# MachVEPNode\");\n+  if (!_verified) {\n+    st->print_cr(\"\\t load_class\");\n+  } else {\n+    st->print_cr(\"\\t unpack_inline_arg\");\n+  }\n+}\n+#endif\n@@ -2229,0 +2198,27 @@\n+void MachVEPNode::emit(C2_MacroAssembler *masm, PhaseRegAlloc* ra_) const\n+{\n+  if (!_verified) {\n+    __ ic_check(1);\n+  } else {\n+    \/\/ TODO 8284443 Avoid creation of temporary frame\n+    if (ra_->C->stub_function() == nullptr) {\n+      __ verified_entry(ra_->C, 0);\n+      __ entry_barrier();\n+      int framesize = ra_->C->output()->frame_slots() << LogBytesPerInt;\n+      __ remove_frame(framesize, false);\n+    }\n+    \/\/ Unpack inline type args passed as oop and then jump to\n+    \/\/ the verified entry point (skipping the unverified entry).\n+    int sp_inc = __ unpack_inline_args(ra_->C, _receiver_only);\n+    \/\/ Emit code for verified entry and save increment for stack repair on return\n+    __ verified_entry(ra_->C, sp_inc);\n+    if (Compile::current()->output()->in_scratch_emit_size()) {\n+      Label dummy_verified_entry;\n+      __ b(dummy_verified_entry);\n+    } else {\n+      __ b(*_verified_entry);\n+    }\n+  }\n+}\n+\n+\/\/=============================================================================\n@@ -2251,5 +2247,0 @@\n-uint MachUEPNode::size(PhaseRegAlloc* ra_) const\n-{\n-  return MachNode::size(ra_);\n-}\n-\n@@ -3742,0 +3733,31 @@\n+    if (tf()->returns_inline_type_as_fields() && !_method->is_method_handle_intrinsic() && _method->return_type()->is_loaded()) {\n+      \/\/ The last return value is not set by the callee but used to pass the null marker to compiled code.\n+      \/\/ Search for the corresponding projection, get the register and emit code that initialized it.\n+      uint con = (tf()->range_cc()->cnt() - 1);\n+      for (DUIterator_Fast imax, i = fast_outs(imax); i < imax; i++) {\n+        ProjNode* proj = fast_out(i)->as_Proj();\n+        if (proj->_con == con) {\n+          \/\/ Set null marker if r0 is non-null (a non-null value is returned buffered or scalarized)\n+          OptoReg::Name optoReg = ra_->get_reg_first(proj);\n+          VMReg reg = OptoReg::as_VMReg(optoReg, ra_->_framesize, OptoReg::reg2stack(ra_->_matcher._new_SP));\n+          Register toReg = reg->is_reg() ? reg->as_Register() : rscratch1;\n+          __ cmp(r0, zr);\n+          __ cset(toReg, Assembler::NE);\n+          if (reg->is_stack()) {\n+            int st_off = reg->reg2stack() * VMRegImpl::stack_slot_size;\n+            __ str(toReg, Address(sp, st_off));\n+          }\n+          break;\n+        }\n+      }\n+      if (return_value_is_used()) {\n+        \/\/ An inline type is returned as fields in multiple registers.\n+        \/\/ R0 either contains an oop if the inline type is buffered or a pointer\n+        \/\/ to the corresponding InlineKlass with the lowest bit set to 1. Zero r0\n+        \/\/ if the lowest bit is set to allow C2 to use the oop after null checking.\n+        \/\/ r0 &= (r0 & 1) - 1\n+        __ andr(rscratch1, r0, 0x1);\n+        __ sub(rscratch1, rscratch1, 0x1);\n+        __ andr(r0, r0, rscratch1);\n+      }\n+    }\n@@ -6923,1 +6945,1 @@\n-    \"mov  $dst, $con\\t# ptr\\n\\t\"\n+    \"mov  $dst, $con\\t# ptr\"\n@@ -8116,0 +8138,30 @@\n+instruct castI2N(iRegNNoSp dst, iRegI src) %{\n+  match(Set dst (CastI2N src));\n+\n+  ins_cost(INSN_COST);\n+  format %{ \"mov $dst, $src\\t# int -> narrow ptr\" %}\n+\n+  ins_encode %{\n+    if ($dst$$reg != $src$$reg) {\n+      __ mov(as_Register($dst$$reg), as_Register($src$$reg));\n+    }\n+  %}\n+\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct castN2X(iRegLNoSp dst, iRegN src) %{\n+  match(Set dst (CastP2X src));\n+\n+  ins_cost(INSN_COST);\n+  format %{ \"mov $dst, $src\\t# ptr -> long\" %}\n+\n+  ins_encode %{\n+    if ($dst$$reg != $src$$reg) {\n+      __ mov(as_Register($dst$$reg), as_Register($src$$reg));\n+    }\n+  %}\n+\n+  ins_pipe(ialu_reg);\n+%}\n+\n@@ -15069,1 +15121,1 @@\n-instruct clearArray_reg_reg(iRegL_R11 cnt, iRegP_R10 base, Universe dummy, rFlagsReg cr)\n+instruct clearArray_reg_reg_immL0(iRegL_R11 cnt, iRegP_R10 base, immL0 zero, Universe dummy, rFlagsReg cr)\n@@ -15071,1 +15123,1 @@\n-  match(Set dummy (ClearArray cnt base));\n+  match(Set dummy (ClearArray (Binary cnt base) zero));\n@@ -15088,0 +15140,16 @@\n+instruct clearArray_reg_reg(iRegL_R11 cnt, iRegP_R10 base, iRegL val, Universe dummy, rFlagsReg cr)\n+%{\n+  predicate(((ClearArrayNode*)n)->word_copy_only());\n+  match(Set dummy (ClearArray (Binary cnt base) val));\n+  effect(USE_KILL cnt, USE_KILL base, KILL cr);\n+\n+  ins_cost(4 * INSN_COST);\n+  format %{ \"ClearArray $cnt, $base, $val\" %}\n+\n+  ins_encode %{\n+    __ fill_words($base$$Register, $cnt$$Register, $val$$Register);\n+  %}\n+\n+  ins_pipe(pipe_class_memory);\n+%}\n+\n@@ -15091,1 +15159,2 @@\n-            < (uint64_t)(BlockZeroingLowLimit >> LogBytesPerWord));\n+            < (uint64_t)(BlockZeroingLowLimit >> LogBytesPerWord)\n+            && !((ClearArrayNode*)n)->word_copy_only());\n@@ -16408,0 +16477,18 @@\n+\/\/ entry point is null, target holds the address to call\n+instruct CallLeafNoFPIndirect(iRegP target)\n+%{\n+  predicate(n->as_Call()->entry_point() == nullptr);\n+\n+  match(CallLeafNoFP target);\n+\n+  ins_cost(CALL_COST);\n+\n+  format %{ \"CALL, runtime leaf nofp indirect $target\" %}\n+\n+  ins_encode %{\n+    __ blr($target$$Register);\n+  %}\n+\n+  ins_pipe(pipe_class_call);\n+%}\n+\n@@ -16410,0 +16497,2 @@\n+  predicate(n->as_Call()->entry_point() != nullptr);\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/aarch64.ad","additions":150,"deletions":61,"binary":false,"changes":211,"status":"modified"},{"patch":"@@ -35,0 +35,1 @@\n+#include \"ci\/ciInlineKlass.hpp\"\n@@ -36,0 +37,1 @@\n+#include \"ci\/ciObjArrayKlass.hpp\"\n@@ -41,0 +43,1 @@\n+#include \"oops\/oop.inline.hpp\"\n@@ -429,1 +432,1 @@\n-  __ remove_frame(initial_frame_size_in_bytes());\n+  __ remove_frame(initial_frame_size_in_bytes(), needs_stack_repair());\n@@ -479,0 +482,42 @@\n+  if (InlineTypeReturnedAsFields) {\n+    \/\/ Check if we are returning an non-null inline type and load its fields into registers\n+    ciType* return_type = compilation()->method()->return_type();\n+    if (return_type->is_inlinetype()) {\n+      ciInlineKlass* vk = return_type->as_inline_klass();\n+      if (vk->can_be_returned_as_fields()) {\n+        address unpack_handler = vk->unpack_handler();\n+        assert(unpack_handler != nullptr, \"must be\");\n+        __ far_call(RuntimeAddress(unpack_handler));\n+      }\n+    } else if (return_type->is_instance_klass() && (!return_type->is_loaded() || StressCallingConvention)) {\n+      Label skip;\n+      Label not_null;\n+      __ cbnz(r0, not_null);\n+      \/\/ Returned value is null, zero all return registers because they may belong to oop fields\n+      __ mov(j_rarg1, zr);\n+      __ mov(j_rarg2, zr);\n+      __ mov(j_rarg3, zr);\n+      __ mov(j_rarg4, zr);\n+      __ mov(j_rarg5, zr);\n+      __ mov(j_rarg6, zr);\n+      __ mov(j_rarg7, zr);\n+      __ b(skip);\n+      __ bind(not_null);\n+\n+      \/\/ Check if we are returning an non-null inline type and load its fields into registers\n+      __ test_oop_is_not_inline_type(r0, rscratch2, skip, \/* can_be_null= *\/ false);\n+\n+      \/\/ Load fields from a buffered value with an inline class specific handler\n+      __ load_klass(rscratch1 \/*dst*\/, r0 \/*src*\/);\n+      __ ldr(rscratch1, Address(rscratch1, InstanceKlass::adr_inlineklass_fixed_block_offset()));\n+      __ ldr(rscratch1, Address(rscratch1, InlineKlass::unpack_handler_offset()));\n+      \/\/ Unpack handler can be null if inline type is not scalarizable in returns\n+      __ cbz(rscratch1, skip);\n+      __ blr(rscratch1);\n+\n+      __ bind(skip);\n+    }\n+    \/\/ At this point, r0 points to the value object (for interpreter or C1 caller).\n+    \/\/ The fields of the object are copied into registers (for C2 caller).\n+  }\n+\n@@ -480,1 +525,1 @@\n-  __ remove_frame(initial_frame_size_in_bytes());\n+  __ remove_frame(initial_frame_size_in_bytes(), needs_stack_repair());\n@@ -492,0 +537,4 @@\n+int LIR_Assembler::store_inline_type_fields_to_buf(ciInlineKlass* vk) {\n+  return (__ store_inline_type_fields_to_buf(vk, false));\n+}\n+\n@@ -538,3 +587,1 @@\n-        if (patch_code == lir_patch_none) {\n-          jobject2reg(c->as_jobject(), dest->as_register());\n-        } else {\n+        if (patch_code != lir_patch_none) {\n@@ -542,0 +589,2 @@\n+        } else {\n+          jobject2reg(c->as_jobject(), dest->as_register());\n@@ -651,0 +700,2 @@\n+    \/\/ Non-null case is not handled on aarch64 but handled on x86\n+    \/\/ FIXME: do we need to add it here?\n@@ -998,0 +1049,14 @@\n+void LIR_Assembler::move(LIR_Opr src, LIR_Opr dst) {\n+  assert(dst->is_cpu_register(), \"must be\");\n+  assert(dst->type() == src->type(), \"must be\");\n+\n+  if (src->is_cpu_register()) {\n+    reg2reg(src, dst);\n+  } else if (src->is_stack()) {\n+    stack2reg(src, dst, dst->type());\n+  } else if (src->is_constant()) {\n+    const2reg(src, dst, lir_patch_none, nullptr);\n+  } else {\n+    ShouldNotReachHere();\n+  }\n+}\n@@ -1189,1 +1254,1 @@\n-  if (UseSlowPath ||\n+  if (UseSlowPath || op->always_slow_path() ||\n@@ -1301,22 +1366,16 @@\n-  if (should_profile) {\n-    Register mdo  = klass_RInfo;\n-    __ mov_metadata(mdo, md->constant_encoding());\n-    Label not_null;\n-    __ cbnz(obj, not_null);\n-    \/\/ Object is null; update MDO and exit\n-    Address data_addr\n-      = __ form_address(rscratch2, mdo,\n-                        md->byte_offset_of_slot(data, DataLayout::flags_offset()),\n-                        0);\n-    __ ldrb(rscratch1, data_addr);\n-    __ orr(rscratch1, rscratch1, BitData::null_seen_byte_constant());\n-    __ strb(rscratch1, data_addr);\n-    __ b(*obj_is_null);\n-    __ bind(not_null);\n-\n-    Label update_done;\n-    Register recv = k_RInfo;\n-    __ load_klass(recv, obj);\n-    type_profile_helper(mdo, md, data, recv, &update_done);\n-    Address counter_addr(mdo, md->byte_offset_of_slot(data, CounterData::count_offset()));\n-    __ addptr(counter_addr, DataLayout::counter_increment);\n+  if (op->need_null_check()) {\n+    if (should_profile) {\n+      Register mdo  = klass_RInfo;\n+      __ mov_metadata(mdo, md->constant_encoding());\n+      Label not_null;\n+      __ cbnz(obj, not_null);\n+      \/\/ Object is null; update MDO and exit\n+      Address data_addr\n+        = __ form_address(rscratch2, mdo,\n+                          md->byte_offset_of_slot(data, DataLayout::flags_offset()),\n+                          0);\n+      __ ldrb(rscratch1, data_addr);\n+      __ orr(rscratch1, rscratch1, BitData::null_seen_byte_constant());\n+      __ strb(rscratch1, data_addr);\n+      __ b(*obj_is_null);\n+      __ bind(not_null);\n@@ -1324,3 +1383,11 @@\n-    __ bind(update_done);\n-  } else {\n-    __ cbz(obj, *obj_is_null);\n+      Label update_done;\n+      Register recv = k_RInfo;\n+      __ load_klass(recv, obj);\n+      type_profile_helper(mdo, md, data, recv, &update_done);\n+      Address counter_addr(mdo, md->byte_offset_of_slot(data, CounterData::count_offset()));\n+      __ addptr(counter_addr, DataLayout::counter_increment);\n+\n+      __ bind(update_done);\n+    } else {\n+      __ cbz(obj, *obj_is_null);\n+    }\n@@ -1337,0 +1404,1 @@\n+    assert(!k->is_loaded() || !k->is_obj_array_klass(), \"Use refined array for a direct pointer comparison\");\n@@ -1359,1 +1427,8 @@\n-        __ cmp(klass_RInfo, k_RInfo);\n+        if (k->is_loaded() && k->is_obj_array_klass()) {\n+          \/\/ For a direct pointer comparison, we need the refined array klass pointer\n+          ciKlass* k_refined = ciObjArrayKlass::make(k->as_obj_array_klass()->element_klass());\n+          __ mov_metadata(rscratch1, k_refined->constant_encoding());\n+          __ cmp(klass_RInfo, rscratch1);\n+        } else {\n+          __ cmp(klass_RInfo, k_RInfo);\n+        }\n@@ -1484,0 +1559,106 @@\n+void LIR_Assembler::emit_opFlattenedArrayCheck(LIR_OpFlattenedArrayCheck* op) {\n+  \/\/ We are loading\/storing from\/to an array that *may* be a flat array (the\n+  \/\/ declared type is Object[], abstract[], interface[] or VT.ref[]).\n+  \/\/ If this array is a flat array, take the slow path.\n+  __ test_flat_array_oop(op->array()->as_register(), op->tmp()->as_register(), *op->stub()->entry());\n+  if (!op->value()->is_illegal()) {\n+    \/\/ The array is not a flat array, but it might be null-free. If we are storing\n+    \/\/ a null into a null-free array, take the slow path (which will throw NPE).\n+    Label skip;\n+    __ cbnz(op->value()->as_register(), skip);\n+    __ test_null_free_array_oop(op->array()->as_register(), op->tmp()->as_register(), *op->stub()->entry());\n+    __ bind(skip);\n+  }\n+}\n+\n+void LIR_Assembler::emit_opNullFreeArrayCheck(LIR_OpNullFreeArrayCheck* op) {\n+  \/\/ We are storing into an array that *may* be null-free (the declared type is\n+  \/\/ Object[], abstract[], interface[] or VT.ref[]).\n+  Label test_mark_word;\n+  Register tmp = op->tmp()->as_register();\n+  __ ldr(tmp, Address(op->array()->as_register(), oopDesc::mark_offset_in_bytes()));\n+  __ tst(tmp, markWord::unlocked_value);\n+  __ br(Assembler::NE, test_mark_word);\n+  __ load_prototype_header(tmp, op->array()->as_register());\n+  __ bind(test_mark_word);\n+  __ tst(tmp, markWord::null_free_array_bit_in_place);\n+}\n+\n+void LIR_Assembler::emit_opSubstitutabilityCheck(LIR_OpSubstitutabilityCheck* op) {\n+  Label L_oops_equal;\n+  Label L_oops_not_equal;\n+  Label L_end;\n+\n+  Register left  = op->left()->as_register();\n+  Register right = op->right()->as_register();\n+\n+  __ cmp(left, right);\n+  __ br(Assembler::EQ, L_oops_equal);\n+\n+  \/\/ (1) Null check -- if one of the operands is null, the other must not be null (because\n+  \/\/     the two references are not equal), so they are not substitutable,\n+  \/\/     FIXME: do null check only if the operand is nullable\n+  {\n+    __ cbz(left, L_oops_not_equal);\n+    __ cbz(right, L_oops_not_equal);\n+  }\n+\n+  ciKlass* left_klass = op->left_klass();\n+  ciKlass* right_klass = op->right_klass();\n+\n+  \/\/ (2) Inline type check -- if either of the operands is not a inline type,\n+  \/\/     they are not substitutable. We do this only if we are not sure that the\n+  \/\/     operands are inline type\n+  if ((left_klass == nullptr || right_klass == nullptr) ||\/\/ The klass is still unloaded, or came from a Phi node.\n+      !left_klass->is_inlinetype() || !right_klass->is_inlinetype()) {\n+    Register tmp1  = op->tmp1()->as_register();\n+    __ mov(tmp1, markWord::inline_type_pattern);\n+    __ ldr(rscratch1, Address(left, oopDesc::mark_offset_in_bytes()));\n+    __ andr(tmp1, tmp1, rscratch1);\n+    __ ldr(rscratch1, Address(right, oopDesc::mark_offset_in_bytes()));\n+    __ andr(tmp1, tmp1, rscratch1);\n+    __ cmp(tmp1, (u1)markWord::inline_type_pattern);\n+    __ br(Assembler::NE, L_oops_not_equal);\n+  }\n+\n+  \/\/ (3) Same klass check: if the operands are of different klasses, they are not substitutable.\n+  if (left_klass != nullptr && left_klass->is_inlinetype() && left_klass == right_klass) {\n+    \/\/ No need to load klass -- the operands are statically known to be the same inline klass.\n+    __ b(*op->stub()->entry());\n+  } else {\n+    Register left_klass_op = op->left_klass_op()->as_register();\n+    Register right_klass_op = op->right_klass_op()->as_register();\n+\n+    if (UseCompressedClassPointers) {\n+      __ ldrw(left_klass_op,  Address(left,  oopDesc::klass_offset_in_bytes()));\n+      __ ldrw(right_klass_op, Address(right, oopDesc::klass_offset_in_bytes()));\n+      __ cmpw(left_klass_op, right_klass_op);\n+    } else {\n+      __ ldr(left_klass_op,  Address(left,  oopDesc::klass_offset_in_bytes()));\n+      __ ldr(right_klass_op, Address(right, oopDesc::klass_offset_in_bytes()));\n+      __ cmp(left_klass_op, right_klass_op);\n+    }\n+\n+    __ br(Assembler::EQ, *op->stub()->entry()); \/\/ same klass -> do slow check\n+    \/\/ fall through to L_oops_not_equal\n+  }\n+\n+  __ bind(L_oops_not_equal);\n+  move(op->not_equal_result(), op->result_opr());\n+  __ b(L_end);\n+\n+  __ bind(L_oops_equal);\n+  move(op->equal_result(), op->result_opr());\n+  __ b(L_end);\n+\n+  \/\/ We've returned from the stub. R0 contains 0x0 IFF the two\n+  \/\/ operands are not substitutable. (Don't compare against 0x1 in case the\n+  \/\/ C compiler is naughty)\n+  __ bind(*op->stub()->continuation());\n+  __ cbz(r0, L_oops_not_equal); \/\/ (call_stub() == 0x0) -> not_equal\n+  move(op->equal_result(), op->result_opr()); \/\/ (call_stub() != 0x0) -> equal\n+  \/\/ fall-through\n+  __ bind(L_end);\n+}\n+\n+\n@@ -1997,1 +2178,1 @@\n-  add_call_info(code_offset(), op->info());\n+  add_call_info(code_offset(), op->info(), op->maybe_return_as_fields());\n@@ -2008,1 +2189,1 @@\n-  add_call_info(code_offset(), op->info());\n+  add_call_info(code_offset(), op->info(), op->maybe_return_as_fields());\n@@ -2171,0 +2352,12 @@\n+void LIR_Assembler::arraycopy_inlinetype_check(Register obj, Register tmp, CodeStub* slow_path, bool is_dest, bool null_check) {\n+  if (null_check) {\n+    __ cbz(obj, *slow_path->entry());\n+  }\n+  if (is_dest) {\n+    __ test_null_free_array_oop(obj, tmp, *slow_path->entry());\n+    \/\/ TODO 8350865 Flat no longer implies null-free, so we need to check for flat dest. Can we do better here?\n+    __ test_flat_array_oop(obj, tmp, *slow_path->entry());\n+  } else {\n+    __ test_flat_array_oop(obj, tmp, *slow_path->entry());\n+  }\n+}\n@@ -2189,0 +2382,6 @@\n+  if (flags & LIR_OpArrayCopy::always_slow_path) {\n+    __ b(*stub->entry());\n+    __ bind(*stub->continuation());\n+    return;\n+  }\n+\n@@ -2242,0 +2441,8 @@\n+  \/\/ Handle inline type arrays\n+  if (flags & LIR_OpArrayCopy::src_inlinetype_check) {\n+    arraycopy_inlinetype_check(src, tmp, stub, false, (flags & LIR_OpArrayCopy::src_null_check));\n+  }\n+  if (flags & LIR_OpArrayCopy::dst_inlinetype_check) {\n+    arraycopy_inlinetype_check(dst, tmp, stub, true, (flags & LIR_OpArrayCopy::dst_null_check));\n+  }\n+\n@@ -2783,0 +2990,20 @@\n+void LIR_Assembler::emit_profile_inline_type(LIR_OpProfileInlineType* op) {\n+  Register obj = op->obj()->as_register();\n+  Register tmp = op->tmp()->as_pointer_register();\n+  bool not_null = op->not_null();\n+  int flag = op->flag();\n+\n+  Label not_inline_type;\n+  if (!not_null) {\n+    __ cbz(obj, not_inline_type);\n+  }\n+\n+  __ test_oop_is_not_inline_type(obj, tmp, not_inline_type);\n+\n+  Address mdo_addr = as_Address(op->mdp()->as_address_ptr(), rscratch2);\n+  __ ldrb(rscratch1, mdo_addr);\n+  __ orr(rscratch1, rscratch1, flag);\n+  __ strb(rscratch1, mdo_addr);\n+\n+  __ bind(not_inline_type);\n+}\n@@ -2922,0 +3149,4 @@\n+void LIR_Assembler::check_orig_pc() {\n+  __ ldr(rscratch2, frame_map()->address_for_orig_pc_addr());\n+  __ cmp(rscratch2, (u1)NULL_WORD);\n+}\n","filename":"src\/hotspot\/cpu\/aarch64\/c1_LIRAssembler_aarch64.cpp","additions":265,"deletions":34,"binary":false,"changes":299,"status":"modified"},{"patch":"@@ -77,0 +77,3 @@\n+  void arraycopy_inlinetype_check(Register obj, Register tmp, CodeStub* slow_path, bool is_dest, bool null_check);\n+  void move(LIR_Opr src, LIR_Opr dst);\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/c1_LIRAssembler_aarch64.hpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -49,0 +49,21 @@\n+void C2_MacroAssembler::entry_barrier() {\n+  BarrierSetAssembler* bs = BarrierSet::barrier_set()->barrier_set_assembler();\n+  \/\/ Dummy labels for just measuring the code size\n+  Label dummy_slow_path;\n+  Label dummy_continuation;\n+  Label dummy_guard;\n+  Label* slow_path = &dummy_slow_path;\n+  Label* continuation = &dummy_continuation;\n+  Label* guard = &dummy_guard;\n+  if (!Compile::current()->output()->in_scratch_emit_size()) {\n+    \/\/ Use real labels from actual stub when not emitting code for the purpose of measuring its size\n+    C2EntryBarrierStub* stub = new (Compile::current()->comp_arena()) C2EntryBarrierStub();\n+    Compile::current()->output()->add_stub(stub);\n+    slow_path = &stub->entry();\n+    continuation = &stub->continuation();\n+    guard = &stub->guard();\n+  }\n+  \/\/ In the C2 code, we move the non-hot part of nmethod entry barriers out-of-line to a stub.\n+  bs->nmethod_entry_barrier(this, slow_path, continuation, guard);\n+}\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/c2_MacroAssembler_aarch64.cpp","additions":21,"deletions":0,"binary":false,"changes":21,"status":"modified"},{"patch":"@@ -70,0 +70,6 @@\n+\n+  \/\/ Repair the sender sp if the frame has been extended\n+  if (sender_cb->is_nmethod()) {\n+    sender_sp = f.repair_sender_sp(sender_sp, link_addr);\n+  }\n+\n@@ -78,1 +84,1 @@\n-frame FreezeBase::new_heap_frame(frame& f, frame& caller) {\n+frame FreezeBase::new_heap_frame(frame& f, frame& caller, int size_adjust) {\n@@ -112,2 +118,2 @@\n-    sp = caller.unextended_sp() - fsize;\n-    if (caller.is_interpreted_frame()) {\n+    sp = caller.unextended_sp() - fsize - size_adjust;\n+    if (caller.is_interpreted_frame() && size_adjust == 0) {\n@@ -186,1 +192,1 @@\n-inline void FreezeBase::patch_pd(frame& hf, const frame& caller) {\n+inline void FreezeBase::patch_pd(frame& hf, const frame& caller, bool is_bottom_frame) {\n@@ -190,1 +196,2 @@\n-  } else {\n+  } else if (is_bottom_frame && caller.pc() != nullptr) {\n+    assert(caller.is_compiled_frame(), \"\");\n@@ -261,1 +268,1 @@\n-template<typename FKind> frame ThawBase::new_stack_frame(const frame& hf, frame& caller, bool bottom) {\n+template<typename FKind> frame ThawBase::new_stack_frame(const frame& hf, frame& caller, bool bottom, int size_adjust) {\n@@ -289,1 +296,1 @@\n-    intptr_t* frame_sp = caller.unextended_sp() - fsize;\n+    intptr_t* frame_sp = caller.unextended_sp() - fsize - size_adjust;\n@@ -291,7 +298,4 @@\n-      int argsize = FKind::stack_argsize(hf);\n-\n-      fsize += argsize;\n-      frame_sp   -= argsize;\n-      caller.set_sp(caller.sp() - argsize);\n-      assert(caller.sp() == frame_sp + (fsize-argsize), \"\");\n-\n+      if (size_adjust == 0) {\n+        int argsize = FKind::stack_argsize(hf);\n+        frame_sp -= argsize;\n+      }\n@@ -300,0 +304,2 @@\n+    caller.set_sp(frame_sp + fsize);\n+    assert(is_aligned(frame_sp, frame::frame_alignment), \"\");\n@@ -306,1 +312,1 @@\n-      fp = frame_sp + FKind::size(hf) - frame::sender_sp_offset;\n+      fp = frame_sp + fsize - frame::sender_sp_offset;\n@@ -321,1 +327,0 @@\n-    caller.set_sp(caller.sp() - 1);\n@@ -325,1 +330,0 @@\n-\n@@ -330,1 +334,3 @@\n-  patch_callee_link(caller, caller.fp());\n+  if (caller.is_interpreted_frame() || PreserveFramePointer) {\n+    patch_callee_link(caller, caller.fp());\n+  }\n","filename":"src\/hotspot\/cpu\/aarch64\/continuationFreezeThaw_aarch64.inline.hpp","additions":24,"deletions":18,"binary":false,"changes":42,"status":"modified"},{"patch":"@@ -147,2 +147,2 @@\n-inline void ContinuationHelper::Frame::patch_pc(const frame& f, address pc) {\n-  address* pc_addr = &(((address*) f.sp())[-1]);\n+inline void ContinuationHelper::Frame::patch_pc(const frame& f, address pc, bool callee_augmented) {\n+  address* pc_addr = &(((address*) (callee_augmented ? f.unextended_sp() : f.sp()))[-1]);\n","filename":"src\/hotspot\/cpu\/aarch64\/continuationHelper_aarch64.inline.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -35,0 +35,3 @@\n+#ifdef COMPILER1\n+#include \"c1\/c1_Runtime1.hpp\"\n+#endif\n@@ -451,0 +454,4 @@\n+#ifdef ASSERT\n+   address sender_pc_copy = pauth_strip_verifiable((address) *(l_sender_sp-1));\n+#endif\n+\n@@ -453,0 +460,5 @@\n+  intptr_t** saved_fp_addr = (intptr_t**) (l_sender_sp - frame::sender_sp_offset);\n+\n+  \/\/ Repair the sender sp if the frame has been extended\n+  l_sender_sp = repair_sender_sp(l_sender_sp, saved_fp_addr);\n+\n@@ -458,1 +470,9 @@\n-  intptr_t** saved_fp_addr = (intptr_t**) (l_sender_sp - frame::sender_sp_offset);\n+#ifdef ASSERT\n+  if (sender_pc != sender_pc_copy) {\n+    \/\/ When extending the stack in the callee method entry to make room for unpacking of value\n+    \/\/ type args, we keep a copy of the sender pc at the expected location in the callee frame.\n+    \/\/ If the sender pc is patched due to deoptimization, the copy is not consistent anymore.\n+    nmethod* nm = CodeCache::find_blob(sender_pc)->as_nmethod();\n+    assert(sender_pc == nm->deopt_handler_entry(), \"unexpected sender pc\");\n+  }\n+#endif\n@@ -464,2 +484,15 @@\n-    if (!_cb->is_nmethod()) { \/\/ compiled frames do not use callee-saved registers\n-      map->set_include_argument_oops(_cb->caller_must_gc_arguments(map->thread()));\n+    bool c1_buffering = false;\n+#ifdef COMPILER1\n+    nmethod* nm = _cb->as_nmethod_or_null();\n+    if (nm != nullptr && nm->is_compiled_by_c1() && nm->method()->has_scalarized_args() &&\n+        pc() < nm->verified_inline_entry_point()) {\n+      \/\/ TODO 8284443 Can't we do that by not passing 'dont_gc_arguments' in case 'StubId::c1_buffer_inline_args_id' in 'Runtime1::generate_code_for'?\n+      \/\/ The VEP and VIEP(RO) of C1-compiled methods call buffer_inline_args_xxx\n+      \/\/ before doing any argument shuffling, so we need to scan the oops\n+      \/\/ as the caller passes them.\n+      c1_buffering = true;\n+    }\n+#endif\n+    if (!_cb->is_nmethod() || c1_buffering) { \/\/ compiled frames do not use callee-saved registers\n+      bool caller_args = _cb->caller_must_gc_arguments(map->thread()) || c1_buffering;\n+      map->set_include_argument_oops(caller_args);\n","filename":"src\/hotspot\/cpu\/aarch64\/frame_aarch64.inline.hpp","additions":36,"deletions":3,"binary":false,"changes":39,"status":"modified"},{"patch":"@@ -135,0 +135,1 @@\n+  assert_different_registers(value, temp1, temp2);\n@@ -208,1 +209,13 @@\n-  __ push_call_clobbered_registers();\n+  \/\/ save the live input values\n+  RegSet saved = RegSet::of(pre_val);\n+  FloatRegSet fsaved;\n+\n+  \/\/ Barriers might be emitted when converting between (scalarized) calling\n+  \/\/ conventions for inline types. Save all argument registers before calling\n+  \/\/ into the runtime.\n+\n+  \/\/ TODO 8366717 This came with 8284161: Implementation of Virtual Threads (Preview) later in May 2022\n+  \/\/ Check if it's sufficient\n+  \/\/__ push_call_clobbered_registers();\n+  assert_different_registers(rscratch1, pre_val); \/\/ push_CPU_state trashes rscratch1\n+  __ push_CPU_state(true);\n@@ -229,1 +242,1 @@\n-  __ pop_call_clobbered_registers();\n+  __ pop_CPU_state(true);\n@@ -370,0 +383,10 @@\n+\n+  bool in_heap = (decorators & IN_HEAP) != 0;\n+  bool as_normal = (decorators & AS_NORMAL) != 0;\n+  bool dest_uninitialized = (decorators & IS_DEST_UNINITIALIZED) != 0;\n+\n+  bool needs_pre_barrier = as_normal && !dest_uninitialized;\n+  bool needs_post_barrier = (val != noreg && in_heap);\n+\n+  assert_different_registers(val, tmp1, tmp2, tmp3);\n+\n@@ -379,8 +402,10 @@\n-  g1_write_barrier_pre(masm,\n-                       tmp3 \/* obj *\/,\n-                       tmp2 \/* pre_val *\/,\n-                       rthread \/* thread *\/,\n-                       tmp1  \/* tmp1 *\/,\n-                       rscratch2  \/* tmp2 *\/,\n-                       val != noreg \/* tosca_live *\/,\n-                       false \/* expand_call *\/);\n+  if (needs_pre_barrier) {\n+    g1_write_barrier_pre(masm,\n+                         tmp3 \/* obj *\/,\n+                         tmp2 \/* pre_val *\/,\n+                         rthread \/* thread *\/,\n+                         tmp1  \/* tmp1 *\/,\n+                         rscratch2  \/* tmp2 *\/,\n+                         val != noreg \/* tosca_live *\/,\n+                         false \/* expand_call *\/);\n+  }\n@@ -393,3 +418,5 @@\n-    if (UseCompressedOops) {\n-      new_val = rscratch2;\n-      __ mov(new_val, val);\n+    if (needs_post_barrier) {\n+      if (UseCompressedOops) {\n+        new_val = rscratch2;\n+        __ mov(new_val, val);\n+      }\n@@ -397,0 +424,1 @@\n+\n@@ -398,6 +426,8 @@\n-    g1_write_barrier_post(masm,\n-                          tmp3 \/* store_adr *\/,\n-                          new_val \/* new_val *\/,\n-                          rthread \/* thread *\/,\n-                          tmp1 \/* tmp1 *\/,\n-                          tmp2 \/* tmp2 *\/);\n+    if (needs_post_barrier) {\n+      g1_write_barrier_post(masm,\n+                            tmp3 \/* store_adr *\/,\n+                            new_val \/* new_val *\/,\n+                            rthread \/* thread *\/,\n+                            tmp1 \/* tmp1 *\/,\n+                            tmp2 \/* tmp2 *\/);\n+    }\n","filename":"src\/hotspot\/cpu\/aarch64\/gc\/g1\/g1BarrierSetAssembler_aarch64.cpp","additions":49,"deletions":19,"binary":false,"changes":68,"status":"modified"},{"patch":"@@ -112,0 +112,1 @@\n+\n@@ -116,1 +117,9 @@\n-      store_check(masm, dst.base(), dst);\n+      if (tmp3 != noreg) {\n+        \/\/ TODO 8366717 This change is from before the 'tmp3' arg was added to mainline, check if it's still needed. Same on x64. Also, this should be a __ lea\n+        \/\/ Called by MacroAssembler::pack_inline_helper. We cannot corrupt the dst.base() register\n+        __ mov(tmp3, dst.base());\n+        store_check(masm, tmp3, dst);\n+      } else {\n+        \/\/ It's OK to corrupt the dst.base() register.\n+        store_check(masm, dst.base(), dst);\n+      }\n","filename":"src\/hotspot\/cpu\/aarch64\/gc\/shared\/cardTableBarrierSetAssembler_aarch64.cpp","additions":10,"deletions":1,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -35,0 +35,1 @@\n+#include \"oops\/constMethodFlags.hpp\"\n@@ -38,0 +39,1 @@\n+#include \"oops\/inlineKlass.hpp\"\n@@ -211,0 +213,86 @@\n+void InterpreterMacroAssembler::allocate_instance(Register klass, Register new_obj,\n+                                                  Register t1, Register t2,\n+                                                  bool clear_fields, Label& alloc_failed) {\n+  MacroAssembler::allocate_instance(klass, new_obj, t1, t2, clear_fields, alloc_failed);\n+  if (DTraceAllocProbes) {\n+    \/\/ Trigger dtrace event for fastpath\n+    push(atos);\n+    call_VM_leaf(CAST_FROM_FN_PTR(address, static_cast<int (*)(oopDesc*)>(SharedRuntime::dtrace_object_alloc)), new_obj);\n+    pop(atos);\n+  }\n+}\n+\n+void InterpreterMacroAssembler::read_flat_field(Register entry,\n+                                                Register field_index, Register field_offset,\n+                                                Register temp, Register obj) {\n+  Label failed_alloc, slow_path, done;\n+  const Register src = field_offset;\n+  const Register alloc_temp = r10;\n+  const Register dst_temp   = field_index;\n+  const Register layout_info = temp;\n+  assert_different_registers(obj, entry, field_index, field_offset, temp, alloc_temp, rscratch1);\n+\n+  load_unsigned_byte(temp, Address(entry, in_bytes(ResolvedFieldEntry::flags_offset())));\n+  \/\/ If the field is nullable, jump to slow path\n+  tbz(temp, ResolvedFieldEntry::is_null_free_inline_type_shift, slow_path);\n+\n+  \/\/ Grab the inline field klass\n+  ldr(rscratch1, Address(entry, in_bytes(ResolvedFieldEntry::field_holder_offset())));\n+  inline_layout_info(rscratch1, field_index, layout_info);\n+\n+  const Register field_klass = dst_temp;\n+  ldr(field_klass, Address(layout_info, in_bytes(InlineLayoutInfo::klass_offset())));\n+\n+  \/\/ allocate buffer\n+  push(obj); \/\/ save holder\n+  allocate_instance(field_klass, obj, alloc_temp, rscratch2, false, failed_alloc);\n+\n+  \/\/ Have an oop instance buffer, copy into it\n+  payload_address(obj, dst_temp, field_klass);  \/\/ danger, uses rscratch1\n+  pop(alloc_temp);             \/\/ restore holder\n+  lea(src, Address(alloc_temp, field_offset));\n+  \/\/ call_VM_leaf, clobbers a few regs, save restore new obj\n+  push(obj);\n+  flat_field_copy(IS_DEST_UNINITIALIZED, src, dst_temp, layout_info);\n+  pop(obj);\n+  b(done);\n+\n+  bind(failed_alloc);\n+  pop(obj);\n+  bind(slow_path);\n+  call_VM(obj, CAST_FROM_FN_PTR(address, InterpreterRuntime::read_flat_field),\n+          obj, entry);\n+\n+  bind(done);\n+  membar(Assembler::StoreStore);\n+}\n+\n+void InterpreterMacroAssembler::write_flat_field(Register entry, Register field_offset,\n+                                                 Register tmp1, Register tmp2,\n+                                                 Register obj) {\n+  assert_different_registers(entry, field_offset, tmp1, tmp2, obj);\n+  Label slow_path, done;\n+\n+  load_unsigned_byte(tmp1, Address(entry, in_bytes(ResolvedFieldEntry::flags_offset())));\n+  test_field_is_not_null_free_inline_type(tmp1, noreg \/* temp *\/, slow_path);\n+\n+  null_check(r0); \/\/ FIXME JDK-8341120\n+\n+  add(obj, obj, field_offset);\n+\n+  load_klass(tmp1, r0);\n+  payload_address(r0, r0, tmp1);\n+\n+  Register layout_info = field_offset;\n+  load_unsigned_short(tmp1, Address(entry, in_bytes(ResolvedFieldEntry::field_index_offset())));\n+  ldr(tmp2, Address(entry, in_bytes(ResolvedFieldEntry::field_holder_offset())));\n+  inline_layout_info(tmp2, tmp1, layout_info);\n+\n+  flat_field_copy(IN_HEAP, r0, obj, layout_info);\n+  b(done);\n+\n+  bind(slow_path);\n+  call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::write_flat_field), obj, r0, entry);\n+  bind(done);\n+}\n+\n@@ -245,1 +333,2 @@\n-                                                  Label& ok_is_subtype) {\n+                                                  Label& ok_is_subtype,\n+                                                  bool profile) {\n@@ -251,1 +340,3 @@\n-  profile_typecheck(r2, Rsub_klass, r5); \/\/ blows r2, reloads r5\n+  if (profile) {\n+    profile_typecheck(r2, Rsub_klass, r5); \/\/ blows r2, reloads r5\n+  }\n@@ -635,0 +726,1 @@\n+\n@@ -662,0 +754,44 @@\n+  if (state == atos && InlineTypeReturnedAsFields) {\n+    Label skip;\n+    Label not_null;\n+    cbnz(r0, not_null);\n+    \/\/ Returned value is null, zero all return registers because they may belong to oop fields\n+    mov(j_rarg1, zr);\n+    mov(j_rarg2, zr);\n+    mov(j_rarg3, zr);\n+    mov(j_rarg4, zr);\n+    mov(j_rarg5, zr);\n+    mov(j_rarg6, zr);\n+    mov(j_rarg7, zr);\n+    b(skip);\n+    bind(not_null);\n+\n+    \/\/ Check if we are returning an non-null inline type and load its fields into registers\n+    test_oop_is_not_inline_type(r0, rscratch2, skip, \/* can_be_null= *\/ false);\n+\n+    \/\/ Load fields from a buffered value with an inline class specific handler\n+    load_klass(rscratch1 \/*dst*\/, r0 \/*src*\/);\n+    ldr(rscratch1, Address(rscratch1, InstanceKlass::adr_inlineklass_fixed_block_offset()));\n+    ldr(rscratch1, Address(rscratch1, InlineKlass::unpack_handler_offset()));\n+    \/\/ Unpack handler can be null if inline type is not scalarizable in returns\n+    cbz(rscratch1, skip);\n+\n+    blr(rscratch1);\n+#ifdef ASSERT\n+    \/\/ TODO 8284443 Enable\n+    if (StressCallingConvention && false) {\n+      Label skip_stress;\n+      ldr(rscratch1, Address(rfp, frame::interpreter_frame_method_offset * wordSize));\n+      ldrw(rscratch1, Address(rscratch1, Method::flags_offset()));\n+      tstw(rscratch1, MethodFlags::has_scalarized_return_flag());\n+      br(Assembler::EQ, skip_stress);\n+      load_klass(r0, r0);\n+      orr(r0, r0, 1);\n+      bind(skip_stress);\n+    }\n+#endif\n+    bind(skip);\n+    \/\/ Check above kills sender esp in rscratch2. Reload it.\n+    ldr(rscratch2, Address(rfp, frame::interpreter_frame_sender_sp_offset * wordSize));\n+  }\n+\n@@ -939,1 +1075,1 @@\n-void InterpreterMacroAssembler::profile_not_taken_branch(Register mdp) {\n+void InterpreterMacroAssembler::profile_not_taken_branch(Register mdp, bool acmp) {\n@@ -951,1 +1087,1 @@\n-    update_mdp_by_constant(mdp, in_bytes(BranchData::branch_data_size()));\n+    update_mdp_by_constant(mdp, acmp ? in_bytes(ACmpData::acmp_data_size()) : in_bytes(BranchData::branch_data_size()));\n@@ -1274,0 +1410,114 @@\n+template <class ArrayData> void InterpreterMacroAssembler::profile_array_type(Register mdp,\n+                                                                              Register array,\n+                                                                              Register tmp) {\n+  if (ProfileInterpreter) {\n+    Label profile_continue;\n+\n+    \/\/ If no method data exists, go to profile_continue.\n+    test_method_data_pointer(mdp, profile_continue);\n+\n+    mov(tmp, array);\n+    profile_obj_type(tmp, Address(mdp, in_bytes(ArrayData::array_offset())));\n+\n+    Label not_flat;\n+    test_non_flat_array_oop(array, tmp, not_flat);\n+\n+    set_mdp_flag_at(mdp, ArrayData::flat_array_byte_constant());\n+\n+    bind(not_flat);\n+\n+    Label not_null_free;\n+    test_non_null_free_array_oop(array, tmp, not_null_free);\n+\n+    set_mdp_flag_at(mdp, ArrayData::null_free_array_byte_constant());\n+\n+    bind(not_null_free);\n+\n+    bind(profile_continue);\n+  }\n+}\n+\n+template void InterpreterMacroAssembler::profile_array_type<ArrayLoadData>(Register mdp,\n+                                                                           Register array,\n+                                                                           Register tmp);\n+template void InterpreterMacroAssembler::profile_array_type<ArrayStoreData>(Register mdp,\n+                                                                            Register array,\n+                                                                            Register tmp);\n+\n+void InterpreterMacroAssembler::profile_multiple_element_types(Register mdp, Register element, Register tmp, const Register tmp2) {\n+  if (ProfileInterpreter) {\n+    Label profile_continue;\n+\n+    \/\/ If no method data exists, go to profile_continue.\n+    test_method_data_pointer(mdp, profile_continue);\n+\n+    Label done, update;\n+    cbnz(element, update);\n+    set_mdp_flag_at(mdp, BitData::null_seen_byte_constant());\n+    b(done);\n+\n+    bind(update);\n+    load_klass(tmp, element);\n+\n+    \/\/ Record the object type.\n+    record_klass_in_profile(tmp, mdp, tmp2);\n+\n+    bind(done);\n+\n+    \/\/ The method data pointer needs to be updated.\n+    update_mdp_by_constant(mdp, in_bytes(ArrayStoreData::array_store_data_size()));\n+\n+    bind(profile_continue);\n+  }\n+}\n+\n+\n+void InterpreterMacroAssembler::profile_element_type(Register mdp,\n+                                                     Register element,\n+                                                     Register tmp) {\n+  if (ProfileInterpreter) {\n+    Label profile_continue;\n+\n+    \/\/ If no method data exists, go to profile_continue.\n+    test_method_data_pointer(mdp, profile_continue);\n+\n+    mov(tmp, element);\n+    profile_obj_type(tmp, Address(mdp, in_bytes(ArrayLoadData::element_offset())));\n+\n+    \/\/ The method data pointer needs to be updated.\n+    update_mdp_by_constant(mdp, in_bytes(ArrayLoadData::array_load_data_size()));\n+\n+    bind(profile_continue);\n+  }\n+}\n+\n+void InterpreterMacroAssembler::profile_acmp(Register mdp,\n+                                             Register left,\n+                                             Register right,\n+                                             Register tmp) {\n+  if (ProfileInterpreter) {\n+    Label profile_continue;\n+\n+    \/\/ If no method data exists, go to profile_continue.\n+    test_method_data_pointer(mdp, profile_continue);\n+\n+    mov(tmp, left);\n+    profile_obj_type(tmp, Address(mdp, in_bytes(ACmpData::left_offset())));\n+\n+    Label left_not_inline_type;\n+    test_oop_is_not_inline_type(left, tmp, left_not_inline_type);\n+    set_mdp_flag_at(mdp, ACmpData::left_inline_type_byte_constant());\n+    bind(left_not_inline_type);\n+\n+    mov(tmp, right);\n+    profile_obj_type(tmp, Address(mdp, in_bytes(ACmpData::right_offset())));\n+\n+    Label right_not_inline_type;\n+    test_oop_is_not_inline_type(right, tmp, right_not_inline_type);\n+    set_mdp_flag_at(mdp, ACmpData::right_inline_type_byte_constant());\n+    bind(right_not_inline_type);\n+\n+    bind(profile_continue);\n+  }\n+}\n+\n@@ -1637,1 +1887,1 @@\n-        assert(ReturnTypeEntry::static_cell_count() < TypeStackSlotEntries::per_arg_count(), \"can't move past ret type\");\n+        assert(SingleTypeEntry::static_cell_count() < TypeStackSlotEntries::per_arg_count(), \"can't move past ret type\");\n@@ -1683,1 +1933,1 @@\n-    Address mdo_ret_addr(mdp, -in_bytes(ReturnTypeEntry::size()));\n+    Address mdo_ret_addr(mdp, -in_bytes(SingleTypeEntry::size()));\n","filename":"src\/hotspot\/cpu\/aarch64\/interp_masm_aarch64.cpp","additions":256,"deletions":6,"binary":false,"changes":262,"status":"modified"},{"patch":"@@ -161,0 +161,26 @@\n+  \/\/ Kills t1 and t2, perserves klass, return allocation in new_obj\n+  void allocate_instance(Register klass, Register new_obj,\n+                         Register t1, Register t2,\n+                         bool clear_fields, Label& alloc_failed);\n+\n+  \/\/ Allocate instance in \"obj\" and read in the content of the inline field\n+  \/\/ NOTES:\n+  \/\/   - input holder object via \"obj\", which must be r0,\n+  \/\/     will return new instance via the same reg\n+  \/\/   - assumes holder_klass and valueKlass field klass have both been resolved\n+  void read_flat_field(Register entry,\n+                       Register field_index, Register field_offset,\n+                       Register temp, Register obj);\n+\n+  void write_flat_field(Register entry, Register field_offset,\n+                        Register tmp1, Register tmp2,\n+                        Register obj);\n+\n+  \/\/ Allocate value buffer in \"obj\" and read in flat element at the given index\n+  \/\/ NOTES:\n+  \/\/   - Return via \"obj\" must be r0\n+  \/\/   - kills all given regs\n+  void read_flat_element(Register array, Register index,\n+                         Register t1, Register t2,\n+                         Register obj = r0);\n+\n@@ -205,1 +231,1 @@\n-  void gen_subtype_check( Register sub_klass, Label &ok_is_subtype );\n+  void gen_subtype_check( Register sub_klass, Label &ok_is_subtype, bool profile = true);\n@@ -294,1 +320,1 @@\n-  void profile_not_taken_branch(Register mdp);\n+  void profile_not_taken_branch(Register mdp, bool acmp = false);\n@@ -307,0 +333,4 @@\n+  template <class ArrayData> void profile_array_type(Register mdp, Register array, Register tmp);\n+  void profile_multiple_element_types(Register mdp, Register element, Register tmp, Register tmp2);\n+  void profile_element_type(Register mdp, Register element, Register tmp);\n+  void profile_acmp(Register mdp, Register left, Register right, Register tmp);\n","filename":"src\/hotspot\/cpu\/aarch64\/interp_masm_aarch64.hpp","additions":32,"deletions":2,"binary":false,"changes":34,"status":"modified"},{"patch":"@@ -29,0 +29,1 @@\n+#include \"ci\/ciInlineKlass.hpp\"\n@@ -50,0 +51,1 @@\n+#include \"oops\/resolvedFieldEntry.hpp\"\n@@ -51,0 +53,1 @@\n+#include \"runtime\/globals.hpp\"\n@@ -56,0 +59,1 @@\n+#include \"runtime\/signature_cc.hpp\"\n@@ -59,0 +63,1 @@\n+#include \"vmreg_aarch64.inline.hpp\"\n@@ -2031,1 +2036,5 @@\n-  if (!VerifyOops) return;\n+  if (!VerifyOops || VerifyAdapterSharing) {\n+    \/\/ Below address of the code string confuses VerifyAdapterSharing\n+    \/\/ because it may differ between otherwise equivalent adapters.\n+    return;\n+  }\n@@ -2064,1 +2073,5 @@\n-  if (!VerifyOops) return;\n+  if (!VerifyOops || VerifyAdapterSharing) {\n+    \/\/ Below address of the code string confuses VerifyAdapterSharing\n+    \/\/ because it may differ between otherwise equivalent adapters.\n+    return;\n+  }\n@@ -2162,0 +2175,4 @@\n+void MacroAssembler::super_call_VM_leaf(address entry_point) {\n+  MacroAssembler::call_VM_leaf_base(entry_point, 1);\n+}\n+\n@@ -2207,0 +2224,88 @@\n+void MacroAssembler::test_markword_is_inline_type(Register markword, Label& is_inline_type) {\n+  assert_different_registers(markword, rscratch2);\n+  mov(rscratch2, markWord::inline_type_mask_in_place);\n+  andr(markword, markword, rscratch2);\n+  mov(rscratch2, markWord::inline_type_pattern);\n+  cmp(markword, rscratch2);\n+  br(Assembler::EQ, is_inline_type);\n+}\n+\n+void MacroAssembler::test_oop_is_not_inline_type(Register object, Register tmp, Label& not_inline_type, bool can_be_null) {\n+  assert_different_registers(tmp, rscratch1);\n+  if (can_be_null) {\n+    cbz(object, not_inline_type);\n+  }\n+  const int is_inline_type_mask = markWord::inline_type_pattern;\n+  ldr(tmp, Address(object, oopDesc::mark_offset_in_bytes()));\n+  mov(rscratch1, is_inline_type_mask);\n+  andr(tmp, tmp, rscratch1);\n+  cmp(tmp, rscratch1);\n+  br(Assembler::NE, not_inline_type);\n+}\n+\n+void MacroAssembler::test_field_is_null_free_inline_type(Register flags, Register temp_reg, Label& is_null_free_inline_type) {\n+  assert(temp_reg == noreg, \"not needed\"); \/\/ keep signature uniform with x86\n+  tbnz(flags, ResolvedFieldEntry::is_null_free_inline_type_shift, is_null_free_inline_type);\n+}\n+\n+void MacroAssembler::test_field_is_not_null_free_inline_type(Register flags, Register temp_reg, Label& not_null_free_inline_type) {\n+  assert(temp_reg == noreg, \"not needed\"); \/\/ keep signature uniform with x86\n+  tbz(flags, ResolvedFieldEntry::is_null_free_inline_type_shift, not_null_free_inline_type);\n+}\n+\n+void MacroAssembler::test_field_is_flat(Register flags, Register temp_reg, Label& is_flat) {\n+  assert(temp_reg == noreg, \"not needed\"); \/\/ keep signature uniform with x86\n+  tbnz(flags, ResolvedFieldEntry::is_flat_shift, is_flat);\n+}\n+\n+void MacroAssembler::test_field_has_null_marker(Register flags, Register temp_reg, Label& has_null_marker) {\n+  assert(temp_reg == noreg, \"not needed\"); \/\/ keep signature uniform with x86\n+  tbnz(flags, ResolvedFieldEntry::has_null_marker_shift, has_null_marker);\n+}\n+\n+void MacroAssembler::test_oop_prototype_bit(Register oop, Register temp_reg, int32_t test_bit, bool jmp_set, Label& jmp_label) {\n+  Label test_mark_word;\n+  \/\/ load mark word\n+  ldr(temp_reg, Address(oop, oopDesc::mark_offset_in_bytes()));\n+  \/\/ check displaced\n+  tst(temp_reg, markWord::unlocked_value);\n+  br(Assembler::NE, test_mark_word);\n+  \/\/ slow path use klass prototype\n+  load_prototype_header(temp_reg, oop);\n+\n+  bind(test_mark_word);\n+  andr(temp_reg, temp_reg, test_bit);\n+  if (jmp_set) {\n+    cbnz(temp_reg, jmp_label);\n+  } else {\n+    cbz(temp_reg, jmp_label);\n+  }\n+}\n+\n+void MacroAssembler::test_flat_array_oop(Register oop, Register temp_reg, Label& is_flat_array) {\n+  test_oop_prototype_bit(oop, temp_reg, markWord::flat_array_bit_in_place, true, is_flat_array);\n+}\n+\n+void MacroAssembler::test_non_flat_array_oop(Register oop, Register temp_reg,\n+                                                  Label&is_non_flat_array) {\n+  test_oop_prototype_bit(oop, temp_reg, markWord::flat_array_bit_in_place, false, is_non_flat_array);\n+}\n+\n+void MacroAssembler::test_null_free_array_oop(Register oop, Register temp_reg, Label& is_null_free_array) {\n+  test_oop_prototype_bit(oop, temp_reg, markWord::null_free_array_bit_in_place, true, is_null_free_array);\n+}\n+\n+void MacroAssembler::test_non_null_free_array_oop(Register oop, Register temp_reg, Label&is_non_null_free_array) {\n+  test_oop_prototype_bit(oop, temp_reg, markWord::null_free_array_bit_in_place, false, is_non_null_free_array);\n+}\n+\n+void MacroAssembler::test_flat_array_layout(Register lh, Label& is_flat_array) {\n+  tst(lh, Klass::_lh_array_tag_flat_value_bit_inplace);\n+  br(Assembler::NE, is_flat_array);\n+}\n+\n+void MacroAssembler::test_non_flat_array_layout(Register lh, Label& is_non_flat_array) {\n+  tst(lh, Klass::_lh_array_tag_flat_value_bit_inplace);\n+  br(Assembler::EQ, is_non_flat_array);\n+}\n+\n@@ -4907,0 +5012,10 @@\n+void MacroAssembler::load_metadata(Register dst, Register src) {\n+  if (UseCompactObjectHeaders) {\n+    load_narrow_klass_compact(dst, src);\n+  } else if (UseCompressedClassPointers) {\n+    ldrw(dst, Address(src, oopDesc::klass_offset_in_bytes()));\n+  } else {\n+    ldr(dst, Address(src, oopDesc::klass_offset_in_bytes()));\n+  }\n+}\n+\n@@ -5017,0 +5132,5 @@\n+void MacroAssembler::load_prototype_header(Register dst, Register src) {\n+  load_klass(dst, src);\n+  ldr(dst, Address(dst, Klass::prototype_header_offset()));\n+}\n+\n@@ -5418,0 +5538,40 @@\n+void MacroAssembler::flat_field_copy(DecoratorSet decorators, Register src, Register dst,\n+                                     Register inline_layout_info) {\n+  BarrierSetAssembler* bs = BarrierSet::barrier_set()->barrier_set_assembler();\n+  bs->flat_field_copy(this, decorators, src, dst, inline_layout_info);\n+}\n+\n+void MacroAssembler::payload_offset(Register inline_klass, Register offset) {\n+  ldr(offset, Address(inline_klass, InstanceKlass::adr_inlineklass_fixed_block_offset()));\n+  ldrw(offset, Address(offset, InlineKlass::payload_offset_offset()));\n+}\n+\n+void MacroAssembler::payload_address(Register oop, Register data, Register inline_klass) {\n+  \/\/ ((address) (void*) o) + vk->payload_offset();\n+  Register offset = (data == oop) ? rscratch1 : data;\n+  payload_offset(inline_klass, offset);\n+  if (data == oop) {\n+    add(data, data, offset);\n+  } else {\n+    lea(data, Address(oop, offset));\n+  }\n+}\n+\n+void MacroAssembler::data_for_value_array_index(Register array, Register array_klass,\n+                                                Register index, Register data) {\n+  assert_different_registers(array, array_klass, index);\n+  assert_different_registers(rscratch1, array, index);\n+\n+  \/\/ array->base() + (index << Klass::layout_helper_log2_element_size(lh));\n+  ldrw(rscratch1, Address(array_klass, Klass::layout_helper_offset()));\n+\n+  \/\/ Klass::layout_helper_log2_element_size(lh)\n+  \/\/ (lh >> _lh_log2_element_size_shift) & _lh_log2_element_size_mask;\n+  lsr(rscratch1, rscratch1, Klass::_lh_log2_element_size_shift);\n+  andr(rscratch1, rscratch1, Klass::_lh_log2_element_size_mask);\n+  lslv(index, index, rscratch1);\n+\n+  add(data, array, index);\n+  add(data, data, arrayOopDesc::base_offset_in_bytes(T_FLAT_ELEMENT));\n+}\n+\n@@ -5493,0 +5653,102 @@\n+\/\/ Object \/ value buffer allocation...\n+void MacroAssembler::allocate_instance(Register klass, Register new_obj,\n+                                       Register t1, Register t2,\n+                                       bool clear_fields, Label& alloc_failed)\n+{\n+  Label done, initialize_header, initialize_object, slow_case, slow_case_no_pop;\n+  Register layout_size = t1;\n+  assert(new_obj == r0, \"needs to be r0\");\n+  assert_different_registers(klass, new_obj, t1, t2);\n+\n+  \/\/ get instance_size in InstanceKlass (scaled to a count of bytes)\n+  ldrw(layout_size, Address(klass, Klass::layout_helper_offset()));\n+  \/\/ test to see if it is malformed in some way\n+  tst(layout_size, Klass::_lh_instance_slow_path_bit);\n+  br(Assembler::NE, slow_case_no_pop);\n+\n+  \/\/ Allocate the instance:\n+  \/\/  If TLAB is enabled:\n+  \/\/    Try to allocate in the TLAB.\n+  \/\/    If fails, go to the slow path.\n+  \/\/    Initialize the allocation.\n+  \/\/    Exit.\n+  \/\/\n+  \/\/  Go to slow path.\n+\n+  if (UseTLAB) {\n+    push(klass);\n+    tlab_allocate(new_obj, layout_size, 0, klass, t2, slow_case);\n+    if (ZeroTLAB || (!clear_fields)) {\n+      \/\/ the fields have been already cleared\n+      b(initialize_header);\n+    } else {\n+      \/\/ initialize both the header and fields\n+      b(initialize_object);\n+    }\n+\n+    if (clear_fields) {\n+      \/\/ The object is initialized before the header.  If the object size is\n+      \/\/ zero, go directly to the header initialization.\n+      bind(initialize_object);\n+      int header_size = oopDesc::header_size() * HeapWordSize;\n+      assert(is_aligned(header_size, BytesPerLong), \"oop header size must be 8-byte-aligned\");\n+      subs(layout_size, layout_size, header_size);\n+      br(Assembler::EQ, initialize_header);\n+\n+      \/\/ Initialize topmost object field, divide size by 8, check if odd and\n+      \/\/ test if zero.\n+\n+  #ifdef ASSERT\n+      \/\/ make sure instance_size was multiple of 8\n+      Label L;\n+      tst(layout_size, 7);\n+      br(Assembler::EQ, L);\n+      stop(\"object size is not multiple of 8 - adjust this code\");\n+      bind(L);\n+      \/\/ must be > 0, no extra check needed here\n+  #endif\n+\n+      lsr(layout_size, layout_size, LogBytesPerLong);\n+\n+      \/\/ initialize remaining object fields: instance_size was a multiple of 8\n+      {\n+        Label loop;\n+        Register base = t2;\n+\n+        bind(loop);\n+        add(rscratch1, new_obj, layout_size, Assembler::LSL, LogBytesPerLong);\n+        str(zr, Address(rscratch1, header_size - 1*oopSize));\n+        subs(layout_size, layout_size, 1);\n+        br(Assembler::NE, loop);\n+      }\n+    } \/\/ clear_fields\n+\n+    \/\/ initialize object header only.\n+    bind(initialize_header);\n+    pop(klass);\n+    Register mark_word = t2;\n+    if (UseCompactObjectHeaders || EnableValhalla) {\n+      ldr(mark_word, Address(klass, Klass::prototype_header_offset()));\n+      str(mark_word, Address(new_obj, oopDesc::mark_offset_in_bytes()));\n+    } else {\n+      mov(mark_word, (intptr_t)markWord::prototype().value());\n+      str(mark_word, Address(new_obj, oopDesc::mark_offset_in_bytes()));\n+    }\n+    if (!UseCompactObjectHeaders) {\n+      store_klass_gap(new_obj, zr);  \/\/ zero klass gap for compressed oops\n+      mov(t2, klass);                \/\/ preserve klass\n+      store_klass(new_obj, t2);      \/\/ src klass reg is potentially compressed\n+    }\n+    b(done);\n+  }\n+\n+  if (UseTLAB) {\n+    bind(slow_case);\n+    pop(klass);\n+  }\n+  bind(slow_case_no_pop);\n+  b(alloc_failed);\n+\n+  bind(done);\n+}\n+\n@@ -5532,0 +5794,20 @@\n+void MacroAssembler::get_inline_type_field_klass(Register holder_klass, Register index, Register inline_klass) {\n+  inline_layout_info(holder_klass, index, inline_klass);\n+  ldr(inline_klass, Address(inline_klass, InlineLayoutInfo::klass_offset()));\n+}\n+\n+void MacroAssembler::inline_layout_info(Register holder_klass, Register index, Register layout_info) {\n+  assert_different_registers(holder_klass, index, layout_info);\n+  InlineLayoutInfo array[2];\n+  int size = (char*)&array[1] - (char*)&array[0]; \/\/ computing size of array elements\n+  if (is_power_of_2(size)) {\n+    lsl(index, index, log2i_exact(size)); \/\/ Scale index by power of 2\n+  } else {\n+    mov(layout_info, size);\n+    mul(index, index, layout_info); \/\/ Scale the index to be the entry index * array_element_size\n+  }\n+  ldr(layout_info, Address(holder_klass, InstanceKlass::inline_layout_info_array_offset()));\n+  add(layout_info, layout_info, Array<InlineLayoutInfo>::base_offset_in_bytes());\n+  lea(layout_info, Address(layout_info, index));\n+}\n+\n@@ -5657,0 +5939,62 @@\n+void MacroAssembler::remove_frame(int initial_framesize, bool needs_stack_repair) {\n+  if (needs_stack_repair) {\n+    \/\/ Remove the extension of the caller's frame used for inline type unpacking\n+    \/\/\n+    \/\/ Right now the stack looks like this:\n+    \/\/\n+    \/\/ | Arguments from caller     |\n+    \/\/ |---------------------------|  <-- caller's SP\n+    \/\/ | Saved LR #1               |\n+    \/\/ | Saved FP #1               |\n+    \/\/ |---------------------------|\n+    \/\/ | Extension space for       |\n+    \/\/ |   inline arg (un)packing  |\n+    \/\/ |---------------------------|  <-- start of this method's frame\n+    \/\/ | Saved LR #2               |\n+    \/\/ | Saved FP #2               |\n+    \/\/ |---------------------------|  <-- FP\n+    \/\/ | sp_inc                    |\n+    \/\/ | method locals             |\n+    \/\/ |---------------------------|  <-- SP\n+    \/\/\n+    \/\/ There are two copies of FP and LR on the stack. They will be identical at\n+    \/\/ first, but that can change.\n+    \/\/ If the caller has been deoptimized, LR #1 will be patched to point at the\n+    \/\/ deopt blob, and LR #2 will still point into the old method.\n+    \/\/ If the saved FP (x29) was not used as the frame pointer, but to store an\n+    \/\/ oop, the GC will be aware only of FP #2 as the spilled location of x29 and\n+    \/\/ will fix only this one.\n+    \/\/\n+    \/\/ When restoring, one must then load FP #2 into x29, and LR #1 into x30,\n+    \/\/ while keeping in mind that from the scalarized entry point, there will be\n+    \/\/ only one copy of each.\n+    \/\/\n+    \/\/ The sp_inc stack slot holds the total size of the frame including the\n+    \/\/ extension space minus two words for the saved FP and LR. That is how to\n+    \/\/ find LR #1. FP #2 is always located just after sp_inc.\n+\n+    int sp_inc_offset = initial_framesize - 3 * wordSize;  \/\/ Immediately below saved LR and FP\n+\n+    ldr(rscratch1, Address(sp, sp_inc_offset));\n+    ldr(rfp, Address(sp, sp_inc_offset + wordSize));\n+    add(sp, sp, rscratch1);\n+    ldr(lr, Address(sp, wordSize));\n+    add(sp, sp, 2 * wordSize);\n+  } else {\n+    remove_frame(initial_framesize);\n+  }\n+}\n+\n+void MacroAssembler::save_stack_increment(int sp_inc, int frame_size) {\n+  int real_frame_size = frame_size + sp_inc;\n+  assert(sp_inc == 0 || sp_inc > 2*wordSize, \"invalid sp_inc value\");\n+  assert(real_frame_size >= 2*wordSize, \"frame size must include FP\/LR space\");\n+  assert((real_frame_size & (StackAlignmentInBytes-1)) == 0, \"frame size not aligned\");\n+\n+  int sp_inc_offset = frame_size - 3 * wordSize;  \/\/ Immediately below saved LR and FP\n+\n+  \/\/ Subtract two words for the saved FP and LR as these will be popped\n+  \/\/ separately. See remove_frame above.\n+  mov(rscratch1, real_frame_size - 2*wordSize);\n+  str(rscratch1, Address(sp, sp_inc_offset));\n+}\n@@ -6574,0 +6918,448 @@\n+#ifdef COMPILER2\n+\/\/ C2 compiled method's prolog code\n+\/\/ Moved here from aarch64.ad to support Valhalla code belows\n+void MacroAssembler::verified_entry(Compile* C, int sp_inc) {\n+  if (C->clinit_barrier_on_entry()) {\n+    assert(!C->method()->holder()->is_not_initialized(), \"initialization should have been started\");\n+\n+    Label L_skip_barrier;\n+\n+    mov_metadata(rscratch2, C->method()->holder()->constant_encoding());\n+    clinit_barrier(rscratch2, rscratch1, &L_skip_barrier);\n+    far_jump(RuntimeAddress(SharedRuntime::get_handle_wrong_method_stub()));\n+    bind(L_skip_barrier);\n+  }\n+\n+  if (C->max_vector_size() > 0) {\n+    reinitialize_ptrue();\n+  }\n+\n+  int bangsize = C->output()->bang_size_in_bytes();\n+  if (C->output()->need_stack_bang(bangsize))\n+    generate_stack_overflow_check(bangsize);\n+\n+  \/\/ n.b. frame size includes space for return pc and rfp\n+  const long framesize = C->output()->frame_size_in_bytes();\n+  build_frame(framesize);\n+\n+  if (C->needs_stack_repair()) {\n+    save_stack_increment(sp_inc, framesize);\n+  }\n+\n+  if (VerifyStackAtCalls) {\n+    Unimplemented();\n+  }\n+}\n+#endif \/\/ COMPILER2\n+\n+int MacroAssembler::store_inline_type_fields_to_buf(ciInlineKlass* vk, bool from_interpreter) {\n+  assert(InlineTypeReturnedAsFields, \"Inline types should never be returned as fields\");\n+  \/\/ An inline type might be returned. If fields are in registers we\n+  \/\/ need to allocate an inline type instance and initialize it with\n+  \/\/ the value of the fields.\n+  Label skip;\n+  \/\/ We only need a new buffered inline type if a new one is not returned\n+  tbz(r0, 0, skip);\n+  int call_offset = -1;\n+\n+  \/\/ Be careful not to clobber r1-7 which hold returned fields\n+  \/\/ Also do not use callee-saved registers as these may be live in the interpreter\n+  Register tmp1 = r13, tmp2 = r14, klass = r15, r0_preserved = r12;\n+\n+  \/\/ The following code is similar to allocate_instance but has some slight differences,\n+  \/\/ e.g. object size is always not zero, sometimes it's constant; storing klass ptr after\n+  \/\/ allocating is not necessary if vk != nullptr, etc. allocate_instance is not aware of these.\n+  Label slow_case;\n+  \/\/ 1. Try to allocate a new buffered inline instance either from TLAB or eden space\n+  mov(r0_preserved, r0); \/\/ save r0 for slow_case since *_allocate may corrupt it when allocation failed\n+\n+  if (vk != nullptr) {\n+    \/\/ Called from C1, where the return type is statically known.\n+    movptr(klass, (intptr_t)vk->get_InlineKlass());\n+    jint lh = vk->layout_helper();\n+    assert(lh != Klass::_lh_neutral_value, \"inline class in return type must have been resolved\");\n+    if (UseTLAB && !Klass::layout_helper_needs_slow_path(lh)) {\n+      tlab_allocate(r0, noreg, lh, tmp1, tmp2, slow_case);\n+    } else {\n+      b(slow_case);\n+    }\n+  } else {\n+    \/\/ Call from interpreter. R0 contains ((the InlineKlass* of the return type) | 0x01)\n+    andr(klass, r0, -2);\n+    if (UseTLAB) {\n+      ldrw(tmp2, Address(klass, Klass::layout_helper_offset()));\n+      tst(tmp2, Klass::_lh_instance_slow_path_bit);\n+      br(Assembler::NE, slow_case);\n+      tlab_allocate(r0, tmp2, 0, tmp1, tmp2, slow_case);\n+    } else {\n+      b(slow_case);\n+    }\n+  }\n+  if (UseTLAB) {\n+    \/\/ 2. Initialize buffered inline instance header\n+    Register buffer_obj = r0;\n+    if (UseCompactObjectHeaders) {\n+      ldr(rscratch1, Address(klass, Klass::prototype_header_offset()));\n+      str(rscratch1, Address(buffer_obj, oopDesc::mark_offset_in_bytes()));\n+    } else {\n+      mov(rscratch1, (intptr_t)markWord::inline_type_prototype().value());\n+      str(rscratch1, Address(buffer_obj, oopDesc::mark_offset_in_bytes()));\n+      store_klass_gap(buffer_obj, zr);\n+      if (vk == nullptr) {\n+        \/\/ store_klass corrupts klass, so save it for later use (interpreter case only).\n+        mov(tmp1, klass);\n+      }\n+      store_klass(buffer_obj, klass);\n+      klass = tmp1;\n+    }\n+    \/\/ 3. Initialize its fields with an inline class specific handler\n+    if (vk != nullptr) {\n+      far_call(RuntimeAddress(vk->pack_handler())); \/\/ no need for call info as this will not safepoint.\n+    } else {\n+      ldr(tmp1, Address(klass, InstanceKlass::adr_inlineklass_fixed_block_offset()));\n+      ldr(tmp1, Address(tmp1, InlineKlass::pack_handler_offset()));\n+      blr(tmp1);\n+    }\n+\n+    membar(Assembler::StoreStore);\n+    b(skip);\n+  } else {\n+    \/\/ Must have already branched to slow_case above.\n+    DEBUG_ONLY(should_not_reach_here());\n+  }\n+  bind(slow_case);\n+  \/\/ We failed to allocate a new inline type, fall back to a runtime\n+  \/\/ call. Some oop field may be live in some registers but we can't\n+  \/\/ tell. That runtime call will take care of preserving them\n+  \/\/ across a GC if there's one.\n+  mov(r0, r0_preserved);\n+\n+  if (from_interpreter) {\n+    super_call_VM_leaf(StubRoutines::store_inline_type_fields_to_buf());\n+  } else {\n+    far_call(RuntimeAddress(StubRoutines::store_inline_type_fields_to_buf()));\n+    call_offset = offset();\n+  }\n+  membar(Assembler::StoreStore);\n+\n+  bind(skip);\n+  return call_offset;\n+}\n+\n+\/\/ Move a value between registers\/stack slots and update the reg_state\n+bool MacroAssembler::move_helper(VMReg from, VMReg to, BasicType bt, RegState reg_state[]) {\n+  assert(from->is_valid() && to->is_valid(), \"source and destination must be valid\");\n+  if (reg_state[to->value()] == reg_written) {\n+    return true; \/\/ Already written\n+  }\n+\n+  if (from != to && bt != T_VOID) {\n+    if (reg_state[to->value()] == reg_readonly) {\n+      return false; \/\/ Not yet writable\n+    }\n+    if (from->is_reg()) {\n+      if (to->is_reg()) {\n+        if (from->is_Register() && to->is_Register()) {\n+          mov(to->as_Register(), from->as_Register());\n+        } else if (from->is_FloatRegister() && to->is_FloatRegister()) {\n+          fmovd(to->as_FloatRegister(), from->as_FloatRegister());\n+        } else {\n+          ShouldNotReachHere();\n+        }\n+      } else {\n+        int st_off = to->reg2stack() * VMRegImpl::stack_slot_size;\n+        Address to_addr = Address(sp, st_off);\n+        if (from->is_FloatRegister()) {\n+          if (bt == T_DOUBLE) {\n+             strd(from->as_FloatRegister(), to_addr);\n+          } else {\n+             assert(bt == T_FLOAT, \"must be float\");\n+             strs(from->as_FloatRegister(), to_addr);\n+          }\n+        } else {\n+          str(from->as_Register(), to_addr);\n+        }\n+      }\n+    } else {\n+      Address from_addr = Address(sp, from->reg2stack() * VMRegImpl::stack_slot_size);\n+      if (to->is_reg()) {\n+        if (to->is_FloatRegister()) {\n+          if (bt == T_DOUBLE) {\n+            ldrd(to->as_FloatRegister(), from_addr);\n+          } else {\n+            assert(bt == T_FLOAT, \"must be float\");\n+            ldrs(to->as_FloatRegister(), from_addr);\n+          }\n+        } else {\n+          ldr(to->as_Register(), from_addr);\n+        }\n+      } else {\n+        int st_off = to->reg2stack() * VMRegImpl::stack_slot_size;\n+        ldr(rscratch1, from_addr);\n+        str(rscratch1, Address(sp, st_off));\n+      }\n+    }\n+  }\n+\n+  \/\/ Update register states\n+  reg_state[from->value()] = reg_writable;\n+  reg_state[to->value()] = reg_written;\n+  return true;\n+}\n+\n+\/\/ Calculate the extra stack space required for packing or unpacking inline\n+\/\/ args and adjust the stack pointer\n+int MacroAssembler::extend_stack_for_inline_args(int args_on_stack) {\n+  int sp_inc = args_on_stack * VMRegImpl::stack_slot_size;\n+  sp_inc = align_up(sp_inc, StackAlignmentInBytes);\n+  assert(sp_inc > 0, \"sanity\");\n+\n+  \/\/ Save a copy of the FP and LR here for deoptimization patching and frame walking\n+  stp(rfp, lr, Address(pre(sp, -2 * wordSize)));\n+\n+  \/\/ Adjust the stack pointer. This will be repaired on return by MacroAssembler::remove_frame\n+  if (sp_inc < (1 << 9)) {\n+    sub(sp, sp, sp_inc);   \/\/ Fits in an immediate\n+  } else {\n+    mov(rscratch1, sp_inc);\n+    sub(sp, sp, rscratch1);\n+  }\n+\n+  return sp_inc + 2 * wordSize;  \/\/ Account for the FP\/LR space\n+}\n+\n+\/\/ Read all fields from an inline type oop and store the values in registers\/stack slots\n+bool MacroAssembler::unpack_inline_helper(const GrowableArray<SigEntry>* sig, int& sig_index,\n+                                          VMReg from, int& from_index, VMRegPair* to, int to_count, int& to_index,\n+                                          RegState reg_state[]) {\n+  assert(sig->at(sig_index)._bt == T_VOID, \"should be at end delimiter\");\n+  assert(from->is_valid(), \"source must be valid\");\n+  bool progress = false;\n+#ifdef ASSERT\n+  const int start_offset = offset();\n+#endif\n+\n+  Label L_null, L_notNull;\n+  \/\/ Don't use r14 as tmp because it's used for spilling (see MacroAssembler::spill_reg_for)\n+  \/\/ TODO 8366717 We need to make sure that r14 (and potentially other long-life regs) are kept live in slowpath runtime calls in GC barriers\n+  Register tmp1 = r10;\n+  Register tmp2 = r11;\n+  Register fromReg = noreg;\n+  ScalarizedInlineArgsStream stream(sig, sig_index, to, to_count, to_index, -1);\n+  bool done = true;\n+  bool mark_done = true;\n+  VMReg toReg;\n+  BasicType bt;\n+  \/\/ Check if argument requires a null check\n+  bool null_check = false;\n+  VMReg nullCheckReg;\n+  while (stream.next(nullCheckReg, bt)) {\n+    if (sig->at(stream.sig_index())._offset == -1) {\n+      null_check = true;\n+      break;\n+    }\n+  }\n+  stream.reset(sig_index, to_index);\n+  while (stream.next(toReg, bt)) {\n+    assert(toReg->is_valid(), \"destination must be valid\");\n+    int idx = (int)toReg->value();\n+    if (reg_state[idx] == reg_readonly) {\n+      if (idx != from->value()) {\n+        mark_done = false;\n+      }\n+      done = false;\n+      continue;\n+    } else if (reg_state[idx] == reg_written) {\n+      continue;\n+    }\n+    assert(reg_state[idx] == reg_writable, \"must be writable\");\n+    reg_state[idx] = reg_written;\n+    progress = true;\n+\n+    if (fromReg == noreg) {\n+      if (from->is_reg()) {\n+        fromReg = from->as_Register();\n+      } else {\n+        int st_off = from->reg2stack() * VMRegImpl::stack_slot_size;\n+        ldr(tmp1, Address(sp, st_off));\n+        fromReg = tmp1;\n+      }\n+      if (null_check) {\n+        \/\/ Nullable inline type argument, emit null check\n+        cbz(fromReg, L_null);\n+      }\n+    }\n+    int off = sig->at(stream.sig_index())._offset;\n+    if (off == -1) {\n+      assert(null_check, \"Missing null check at\");\n+      if (toReg->is_stack()) {\n+        int st_off = toReg->reg2stack() * VMRegImpl::stack_slot_size;\n+        mov(tmp2, 1);\n+        str(tmp2, Address(sp, st_off));\n+      } else {\n+        mov(toReg->as_Register(), 1);\n+      }\n+      continue;\n+    }\n+    assert(off > 0, \"offset in object should be positive\");\n+    Address fromAddr = Address(fromReg, off);\n+    if (!toReg->is_FloatRegister()) {\n+      Register dst = toReg->is_stack() ? tmp2 : toReg->as_Register();\n+      if (is_reference_type(bt)) {\n+        load_heap_oop(dst, fromAddr, rscratch1, rscratch2);\n+      } else {\n+        bool is_signed = (bt != T_CHAR) && (bt != T_BOOLEAN);\n+        load_sized_value(dst, fromAddr, type2aelembytes(bt), is_signed);\n+      }\n+      if (toReg->is_stack()) {\n+        int st_off = toReg->reg2stack() * VMRegImpl::stack_slot_size;\n+        str(dst, Address(sp, st_off));\n+      }\n+    } else if (bt == T_DOUBLE) {\n+      ldrd(toReg->as_FloatRegister(), fromAddr);\n+    } else {\n+      assert(bt == T_FLOAT, \"must be float\");\n+      ldrs(toReg->as_FloatRegister(), fromAddr);\n+    }\n+  }\n+  if (progress && null_check) {\n+    if (done) {\n+      b(L_notNull);\n+      bind(L_null);\n+      \/\/ Set null marker to zero to signal that the argument is null.\n+      \/\/ Also set all oop fields to zero to make the GC happy.\n+      stream.reset(sig_index, to_index);\n+      while (stream.next(toReg, bt)) {\n+        if (sig->at(stream.sig_index())._offset == -1 ||\n+            bt == T_OBJECT || bt == T_ARRAY) {\n+          if (toReg->is_stack()) {\n+            int st_off = toReg->reg2stack() * VMRegImpl::stack_slot_size;\n+            str(zr, Address(sp, st_off));\n+          } else {\n+            mov(toReg->as_Register(), zr);\n+          }\n+        }\n+      }\n+      bind(L_notNull);\n+    } else {\n+      bind(L_null);\n+    }\n+  }\n+\n+  \/\/ TODO 8366717 This is probably okay but looks fishy because stream is reset in the \"Set null marker to zero\" case just above. Same on x64.\n+  sig_index = stream.sig_index();\n+  to_index = stream.regs_index();\n+\n+  if (mark_done && reg_state[from->value()] != reg_written) {\n+    \/\/ This is okay because no one else will write to that slot\n+    reg_state[from->value()] = reg_writable;\n+  }\n+  from_index--;\n+  assert(progress || (start_offset == offset()), \"should not emit code\");\n+  return done;\n+}\n+\n+\/\/ Pack fields back into an inline type oop\n+bool MacroAssembler::pack_inline_helper(const GrowableArray<SigEntry>* sig, int& sig_index, int vtarg_index,\n+                                        VMRegPair* from, int from_count, int& from_index, VMReg to,\n+                                        RegState reg_state[], Register val_array) {\n+  assert(sig->at(sig_index)._bt == T_METADATA, \"should be at delimiter\");\n+  assert(to->is_valid(), \"destination must be valid\");\n+\n+  if (reg_state[to->value()] == reg_written) {\n+    skip_unpacked_fields(sig, sig_index, from, from_count, from_index);\n+    return true; \/\/ Already written\n+  }\n+\n+  \/\/ The GC barrier expanded by store_heap_oop below may call into the\n+  \/\/ runtime so use callee-saved registers for any values that need to be\n+  \/\/ preserved. The GC barrier assembler should take care of saving the\n+  \/\/ Java argument registers.\n+  \/\/ TODO 8284443 Isn't it an issue if below code uses r14 as tmp when it contains a spilled value?\n+  \/\/ Be careful with r14 because it's used for spilling (see MacroAssembler::spill_reg_for).\n+  Register val_obj_tmp = r21;\n+  Register from_reg_tmp = r22;\n+  Register tmp1 = r14;\n+  Register tmp2 = r13;\n+  Register tmp3 = r12;\n+  Register val_obj = to->is_stack() ? val_obj_tmp : to->as_Register();\n+\n+  assert_different_registers(val_obj_tmp, from_reg_tmp, tmp1, tmp2, tmp3, val_array);\n+\n+  if (reg_state[to->value()] == reg_readonly) {\n+    if (!is_reg_in_unpacked_fields(sig, sig_index, to, from, from_count, from_index)) {\n+      skip_unpacked_fields(sig, sig_index, from, from_count, from_index);\n+      return false; \/\/ Not yet writable\n+    }\n+    val_obj = val_obj_tmp;\n+  }\n+\n+  int index = arrayOopDesc::base_offset_in_bytes(T_OBJECT) + vtarg_index * type2aelembytes(T_OBJECT);\n+  load_heap_oop(val_obj, Address(val_array, index), tmp1, tmp2);\n+\n+  ScalarizedInlineArgsStream stream(sig, sig_index, from, from_count, from_index);\n+  VMReg fromReg;\n+  BasicType bt;\n+  Label L_null;\n+  while (stream.next(fromReg, bt)) {\n+    assert(fromReg->is_valid(), \"source must be valid\");\n+    reg_state[fromReg->value()] = reg_writable;\n+\n+    int off = sig->at(stream.sig_index())._offset;\n+    if (off == -1) {\n+      \/\/ Nullable inline type argument, emit null check\n+      Label L_notNull;\n+      if (fromReg->is_stack()) {\n+        int ld_off = fromReg->reg2stack() * VMRegImpl::stack_slot_size;\n+        ldrb(tmp2, Address(sp, ld_off));\n+        cbnz(tmp2, L_notNull);\n+      } else {\n+        cbnz(fromReg->as_Register(), L_notNull);\n+      }\n+      mov(val_obj, 0);\n+      b(L_null);\n+      bind(L_notNull);\n+      continue;\n+    }\n+\n+    assert(off > 0, \"offset in object should be positive\");\n+    size_t size_in_bytes = is_java_primitive(bt) ? type2aelembytes(bt) : wordSize;\n+\n+    \/\/ Pack the scalarized field into the value object.\n+    Address dst(val_obj, off);\n+    if (!fromReg->is_FloatRegister()) {\n+      Register src;\n+      if (fromReg->is_stack()) {\n+        src = from_reg_tmp;\n+        int ld_off = fromReg->reg2stack() * VMRegImpl::stack_slot_size;\n+        load_sized_value(src, Address(sp, ld_off), size_in_bytes, \/* is_signed *\/ false);\n+      } else {\n+        src = fromReg->as_Register();\n+      }\n+      assert_different_registers(dst.base(), src, tmp1, tmp2, tmp3, val_array);\n+      if (is_reference_type(bt)) {\n+        store_heap_oop(dst, src, tmp1, tmp2, tmp3, IN_HEAP | ACCESS_WRITE | IS_DEST_UNINITIALIZED);\n+      } else {\n+        store_sized_value(dst, src, size_in_bytes);\n+      }\n+    } else if (bt == T_DOUBLE) {\n+      strd(fromReg->as_FloatRegister(), dst);\n+    } else {\n+      assert(bt == T_FLOAT, \"must be float\");\n+      strs(fromReg->as_FloatRegister(), dst);\n+    }\n+  }\n+  bind(L_null);\n+  sig_index = stream.sig_index();\n+  from_index = stream.regs_index();\n+\n+  assert(reg_state[to->value()] == reg_writable, \"must have already been read\");\n+  bool success = move_helper(val_obj->as_VMReg(), to, T_OBJECT, reg_state);\n+  assert(success, \"to register must be writeable\");\n+  return true;\n+}\n+\n+VMReg MacroAssembler::spill_reg_for(VMReg reg) {\n+  return (reg->is_FloatRegister()) ? v8->as_VMReg() : r14->as_VMReg();\n+}\n+\n@@ -6984,0 +7776,3 @@\n+  \/\/ Mask inline_type bit such that we go to the slow path if object is an inline type\n+  andr(mark, mark, ~((int) markWord::inline_type_bit_in_place));\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/macroAssembler_aarch64.cpp","additions":797,"deletions":2,"binary":false,"changes":799,"status":"modified"},{"patch":"@@ -36,0 +36,1 @@\n+#include \"utilities\/macros.hpp\"\n@@ -37,0 +38,4 @@\n+#include \"runtime\/signature.hpp\"\n+\n+\n+class ciInlineKlass;\n@@ -680,0 +685,22 @@\n+  \/\/ markWord tests, kills markWord reg\n+  void test_markword_is_inline_type(Register markword, Label& is_inline_type);\n+\n+  \/\/ inlineKlass queries, kills temp_reg\n+  void test_oop_is_not_inline_type(Register object, Register tmp, Label& not_inline_type, bool can_be_null = true);\n+\n+  void test_field_is_null_free_inline_type(Register flags, Register temp_reg, Label& is_null_free);\n+  void test_field_is_not_null_free_inline_type(Register flags, Register temp_reg, Label& not_null_free);\n+  void test_field_is_flat(Register flags, Register temp_reg, Label& is_flat);\n+  void test_field_has_null_marker(Register flags, Register temp_reg, Label& has_null_marker);\n+\n+  \/\/ Check oops for special arrays, i.e. flat arrays and\/or null-free arrays\n+  void test_oop_prototype_bit(Register oop, Register temp_reg, int32_t test_bit, bool jmp_set, Label& jmp_label);\n+  void test_flat_array_oop(Register klass, Register temp_reg, Label& is_flat_array);\n+  void test_non_flat_array_oop(Register oop, Register temp_reg, Label&is_non_flat_array);\n+  void test_null_free_array_oop(Register oop, Register temp_reg, Label& is_null_free_array);\n+  void test_non_null_free_array_oop(Register oop, Register temp_reg, Label&is_non_null_free_array);\n+\n+  \/\/ Check array klass layout helper for flat or null-free arrays...\n+  void test_flat_array_layout(Register lh, Label& is_flat_array);\n+  void test_non_flat_array_layout(Register lh, Label& is_non_flat_array);\n+\n@@ -909,0 +936,2 @@\n+  void load_metadata(Register dst, Register src);\n+\n@@ -925,0 +954,9 @@\n+  void flat_field_copy(DecoratorSet decorators, Register src, Register dst, Register inline_layout_info);\n+\n+  \/\/ inline type data payload offsets...\n+  void payload_offset(Register inline_klass, Register offset);\n+  void payload_address(Register oop, Register data, Register inline_klass);\n+  \/\/ get data payload ptr a flat value array at index, kills rcx and index\n+  void data_for_value_array_index(Register array, Register array_klass,\n+                                  Register index, Register data);\n+\n@@ -938,0 +976,2 @@\n+  void load_prototype_header(Register dst, Register src);\n+\n@@ -987,0 +1027,9 @@\n+\n+  \/\/ Object \/ value buffer allocation...\n+  \/\/ Allocate instance of klass, assumes klass initialized by caller\n+  \/\/ new_obj prefers to be rax\n+  \/\/ Kills t1 and t2, perserves klass, return allocation in new_obj (rsi on LP64)\n+  void allocate_instance(Register klass, Register new_obj,\n+                         Register t1, Register t2,\n+                         bool clear_fields, Label& alloc_failed);\n+\n@@ -997,0 +1046,5 @@\n+  \/\/ For field \"index\" within \"klass\", return inline_klass ...\n+  void get_inline_type_field_klass(Register klass, Register index, Register inline_klass);\n+  void inline_layout_info(Register holder_klass, Register index, Register layout_info);\n+\n+\n@@ -1452,0 +1506,18 @@\n+  void verified_entry(Compile* C, int sp_inc);\n+\n+  \/\/ Inline type specific methods\n+  #include \"asm\/macroAssembler_common.hpp\"\n+\n+  int store_inline_type_fields_to_buf(ciInlineKlass* vk, bool from_interpreter = true);\n+  bool move_helper(VMReg from, VMReg to, BasicType bt, RegState reg_state[]);\n+  bool unpack_inline_helper(const GrowableArray<SigEntry>* sig, int& sig_index,\n+                            VMReg from, int& from_index, VMRegPair* to, int to_count, int& to_index,\n+                            RegState reg_state[]);\n+  bool pack_inline_helper(const GrowableArray<SigEntry>* sig, int& sig_index, int vtarg_index,\n+                          VMRegPair* from, int from_count, int& from_index, VMReg to,\n+                          RegState reg_state[], Register val_array);\n+  int extend_stack_for_inline_args(int args_on_stack);\n+  void remove_frame(int initial_framesize, bool needs_stack_repair);\n+  VMReg spill_reg_for(VMReg reg);\n+  void save_stack_increment(int sp_inc, int frame_size);\n+\n@@ -1523,0 +1595,2 @@\n+  void fill_words(Register base, uint64_t cnt, Register value);\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/macroAssembler_aarch64.hpp","additions":74,"deletions":0,"binary":false,"changes":74,"status":"modified"},{"patch":"@@ -38,2 +38,24 @@\n-  int argsize = is_compiled() ? (_cb->as_nmethod()->num_stack_arg_slots() * VMRegImpl::stack_slot_size) >> LogBytesPerWord : 0;\n-  int frame_size = _cb->frame_size() + argsize;\n+  int frame_size = _cb->frame_size();\n+  if (is_compiled()) {\n+    nmethod* nm = _cb->as_nmethod_or_null();\n+    if (nm->needs_stack_repair() && nm->is_compiled_by_c2()) {\n+      frame f = to_frame();\n+      bool augmented = f.was_augmented_on_entry(frame_size);\n+      if (!augmented) {\n+        \/\/ Fix: C2 caller, so frame was not extended and thus the\n+        \/\/ size read from the frame does not include the arguments.\n+        \/\/ Ideally we have to count the arg size for the scalarized\n+        \/\/ convention. For now we include the size of the caller frame\n+        \/\/ which would at least be equal to that.\n+        RegisterMap map(nullptr,\n+                        RegisterMap::UpdateMap::skip,\n+                        RegisterMap::ProcessFrames::skip,\n+                        RegisterMap::WalkContinuation::skip);\n+        frame caller = to_frame().sender(&map);\n+        assert(caller.is_compiled_frame() && caller.cb()->as_nmethod()->is_compiled_by_c2(), \"needs stack repair but was not extended with c1\/interpreter caller\");\n+        frame_size += (caller.real_fp() - caller.sp());\n+      }\n+    } else {\n+      frame_size += _cb->as_nmethod()->num_stack_arg_slots() * VMRegImpl::stack_slot_size >> LogBytesPerWord;\n+    }\n+  }\n@@ -49,1 +71,7 @@\n-    return frame(sp(), unextended_sp(), fp(), pc(), cb(), _oopmap, true);\n+    frame f = frame(sp(), unextended_sp(), fp(), pc(), cb(), _oopmap, true);\n+    \/\/ If caller tries to get the sender of this frame and PreserveFramePointer\n+    \/\/ is set, fp() will be used which contains the old value at the time of\n+    \/\/ freeze (fp is reconstructed again during thaw). Setting sp as trusted\n+    \/\/ causes the sender code to use _unextended_sp instead (see sender_for_compiled_frame()).\n+    f.set_sp_is_trusted();\n+    return f;\n@@ -57,1 +85,1 @@\n-  return pauth_strip_pointer(*(address*)(_sp - 1));\n+  return pauth_strip_pointer(*(address*)((_callee_augmented ? _unextended_sp : _sp) - 1));\n","filename":"src\/hotspot\/cpu\/aarch64\/stackChunkFrameStream_aarch64.inline.hpp","additions":32,"deletions":4,"binary":false,"changes":36,"status":"modified"},{"patch":"@@ -176,0 +176,1 @@\n+  case Bytecodes::_fast_vputfield:\n@@ -760,4 +761,4 @@\n-    \/\/ ??? convention: move array into r3 for exception message\n-  __ mov(r3, array);\n-  __ mov(rscratch1, Interpreter::_throw_ArrayIndexOutOfBoundsException_entry);\n-  __ br(rscratch1);\n+  \/\/ ??? convention: move array into r3 for exception message\n+   __ mov(r3, array);\n+   __ mov(rscratch1, Interpreter::_throw_ArrayIndexOutOfBoundsException_entry);\n+   __ br(rscratch1);\n@@ -823,5 +824,20 @@\n-  __ add(r1, r1, arrayOopDesc::base_offset_in_bytes(T_OBJECT) >> LogBytesPerHeapOop);\n-  do_oop_load(_masm,\n-              Address(r0, r1, Address::uxtw(LogBytesPerHeapOop)),\n-              r0,\n-              IS_ARRAY);\n+  __ profile_array_type<ArrayLoadData>(r2, r0, r4);\n+  if (UseArrayFlattening) {\n+    Label is_flat_array, done;\n+\n+    __ test_flat_array_oop(r0, rscratch1 \/*temp*\/, is_flat_array);\n+    __ add(r1, r1, arrayOopDesc::base_offset_in_bytes(T_OBJECT) >> LogBytesPerHeapOop);\n+    do_oop_load(_masm, Address(r0, r1, Address::uxtw(LogBytesPerHeapOop)), r0, IS_ARRAY);\n+\n+    __ b(done);\n+    __ bind(is_flat_array);\n+    __ call_VM(r0, CAST_FROM_FN_PTR(address, InterpreterRuntime::flat_array_load), r0, r1);\n+    \/\/ Ensure the stores to copy the inline field contents are visible\n+    \/\/ before any subsequent store that publishes this reference.\n+    __ membar(Assembler::StoreStore);\n+    __ bind(done);\n+  } else {\n+    __ add(r1, r1, arrayOopDesc::base_offset_in_bytes(T_OBJECT) >> LogBytesPerHeapOop);\n+    do_oop_load(_masm, Address(r0, r1, Address::uxtw(LogBytesPerHeapOop)), r0, IS_ARRAY);\n+  }\n+  __ profile_element_type(r2, r0, r4);\n@@ -1114,1 +1130,1 @@\n-  Label is_null, ok_is_subtype, done;\n+  Label is_null, is_flat_array, ok_is_subtype, done;\n@@ -1121,2 +1137,4 @@\n-  Address element_address(r3, r4, Address::uxtw(LogBytesPerHeapOop));\n-\n+\n+  __ profile_array_type<ArrayStoreData>(r4, r3, r5);\n+  __ profile_multiple_element_types(r4, r0, r5, r6);\n+\n@@ -1125,0 +1143,2 @@\n+  Address element_address(r3, r4, Address::uxtw(LogBytesPerHeapOop));\n+  \/\/ Be careful not to clobber r4 below\n@@ -1129,0 +1149,8 @@\n+  \/\/ Move array class to r5\n+  __ load_klass(r5, r3);\n+\n+  if (UseArrayFlattening) {\n+    __ ldrw(r6, Address(r5, Klass::layout_helper_offset()));\n+    __ test_flat_array_layout(r6, is_flat_array);\n+  }\n+\n@@ -1131,4 +1159,3 @@\n-  \/\/ Move superklass into r0\n-  __ load_klass(r0, r3);\n-  __ ldr(r0, Address(r0,\n-                     ObjArrayKlass::element_klass_offset()));\n+\n+  \/\/ Move array element superklass into r0\n+  __ ldr(r0, Address(r5, ObjArrayKlass::element_klass_offset()));\n@@ -1139,1 +1166,3 @@\n-  __ gen_subtype_check(r1, ok_is_subtype);\n+\n+  \/\/ is \"r1 <: r0\" ? (value subclass <: array element superclass)\n+  __ gen_subtype_check(r1, ok_is_subtype, false);\n@@ -1157,1 +1186,16 @@\n-  __ profile_null_seen(r2);\n+  if (EnableValhalla) {\n+    Label is_null_into_value_array_npe, store_null;\n+\n+    if (UseArrayFlattening) {\n+      __ test_flat_array_oop(r3, rscratch1, is_flat_array);\n+    }\n+\n+    \/\/ No way to store null in a null-free array\n+    __ test_null_free_array_oop(r3, rscratch1, is_null_into_value_array_npe);\n+    __ b(store_null);\n+\n+    __ bind(is_null_into_value_array_npe);\n+    __ b(ExternalAddress(Interpreter::_throw_NullPointerException_entry));\n+\n+    __ bind(store_null);\n+  }\n@@ -1162,0 +1206,11 @@\n+  __ b(done);\n+\n+  if (UseArrayFlattening) {\n+     Label is_type_ok;\n+    __ bind(is_flat_array); \/\/ Store non-null value to flat\n+\n+    __ ldr(r0, at_tos());    \/\/ value\n+    __ ldr(r3, at_tos_p1()); \/\/ index\n+    __ ldr(r2, at_tos_p2()); \/\/ array\n+    __ call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::flat_array_store), r0, r2, r3);\n+  }\n@@ -1968,2 +2023,1 @@\n-void TemplateTable::if_acmp(Condition cc)\n-{\n+void TemplateTable::if_acmp(Condition cc) {\n@@ -1972,1 +2026,1 @@\n-  Label not_taken;\n+  Label taken, not_taken;\n@@ -1974,0 +2028,38 @@\n+\n+  __ profile_acmp(r2, r1, r0, r4);\n+\n+  Register is_inline_type_mask = rscratch1;\n+  __ mov(is_inline_type_mask, markWord::inline_type_pattern);\n+\n+  if (EnableValhalla) {\n+    __ cmp(r1, r0);\n+    __ br(Assembler::EQ, (cc == equal) ? taken : not_taken);\n+\n+    \/\/ might be substitutable, test if either r0 or r1 is null\n+    __ andr(r2, r0, r1);\n+    __ cbz(r2, (cc == equal) ? not_taken : taken);\n+\n+    \/\/ and both are values ?\n+    __ ldr(r2, Address(r1, oopDesc::mark_offset_in_bytes()));\n+    __ andr(r2, r2, is_inline_type_mask);\n+    __ ldr(r4, Address(r0, oopDesc::mark_offset_in_bytes()));\n+    __ andr(r4, r4, is_inline_type_mask);\n+    __ andr(r2, r2, r4);\n+    __ cmp(r2,  is_inline_type_mask);\n+    __ br(Assembler::NE, (cc == equal) ? not_taken : taken);\n+\n+    \/\/ same value klass ?\n+    __ load_metadata(r2, r1);\n+    __ load_metadata(r4, r0);\n+    __ cmp(r2, r4);\n+    __ br(Assembler::NE, (cc == equal) ? not_taken : taken);\n+\n+    \/\/ Know both are the same type, let's test for substitutability...\n+    if (cc == equal) {\n+      invoke_is_substitutable(r0, r1, taken, not_taken);\n+    } else {\n+      invoke_is_substitutable(r0, r1, not_taken, taken);\n+    }\n+    __ stop(\"Not reachable\");\n+  }\n+\n@@ -1976,0 +2068,1 @@\n+  __ bind(taken);\n@@ -1978,1 +2071,10 @@\n-  __ profile_not_taken_branch(r0);\n+  __ profile_not_taken_branch(r0, true);\n+}\n+\n+void TemplateTable::invoke_is_substitutable(Register aobj, Register bobj,\n+                                            Label& is_subst, Label& not_subst) {\n+\n+  __ call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::is_substitutable), aobj, bobj);\n+  \/\/ Restored... r0 answer, jmp to outcome...\n+  __ cbz(r0, not_subst);\n+  __ b(is_subst);\n@@ -1981,0 +2083,1 @@\n+\n@@ -2214,1 +2317,2 @@\n-  if (_desc->bytecode() == Bytecodes::_return)\n+  if (_desc->bytecode() == Bytecodes::_return\n+      || _desc->bytecode() == Bytecodes::_return_register_finalizer)\n@@ -2605,1 +2709,1 @@\n-  const Register cache     = r4;\n+  const Register cache     = r2;\n@@ -2607,0 +2711,3 @@\n+  const Register klass     = r5;\n+  const Register inline_klass = r7;\n+  const Register field_index = r23;\n@@ -2615,0 +2722,5 @@\n+\n+  \/\/ Valhalla extras\n+  __ load_unsigned_short(field_index, Address(cache, in_bytes(ResolvedFieldEntry::field_index_offset())));\n+  __ ldr(klass, Address(cache, ResolvedFieldEntry::field_holder_offset()));\n+\n@@ -2673,4 +2785,33 @@\n-  do_oop_load(_masm, field, r0, IN_HEAP);\n-  __ push(atos);\n-  if (rc == may_rewrite) {\n-    patch_bytecode(Bytecodes::_fast_agetfield, bc, r1);\n+  if (!EnableValhalla) {\n+    do_oop_load(_masm, field, r0, IN_HEAP);\n+    __ push(atos);\n+    if (rc == may_rewrite) {\n+      patch_bytecode(Bytecodes::_fast_agetfield, bc, r1);\n+    }\n+    __ b(Done);\n+  } else { \/\/ Valhalla\n+    if (is_static) {\n+      __ load_heap_oop(r0, field, rscratch1, rscratch2);\n+      __ push(atos);\n+      __ b(Done);\n+    } else {\n+      Label is_flat, rewrite_inline;\n+      __ test_field_is_flat(flags, noreg \/*temp*\/, is_flat);\n+      __ load_heap_oop(r0, field, rscratch1, rscratch2);\n+      __ push(atos);\n+      if (rc == may_rewrite) {\n+        patch_bytecode(Bytecodes::_fast_agetfield, bc, r1);\n+      }\n+      __ b(Done);\n+      __ bind(is_flat);\n+        \/\/ field is flat (null-free or nullable with a null-marker)\n+        __ mov(r0, obj);\n+        __ read_flat_field(cache, field_index, off, inline_klass \/* temp *\/, r0);\n+        __ verify_oop(r0);\n+        __ push(atos);\n+      __ bind(rewrite_inline);\n+      if (rc == may_rewrite) {\n+        patch_bytecode(Bytecodes::_fast_vgetfield, bc, r1);\n+      }\n+      __ b(Done);\n+    }\n@@ -2678,1 +2819,0 @@\n-  __ b(Done);\n@@ -2839,1 +2979,1 @@\n-  const Register flags     = r0;\n+  const Register flags     = r6;\n@@ -2841,0 +2981,1 @@\n+  const Register inline_klass = r5;\n@@ -2847,2 +2988,0 @@\n-  __ mov(r5, flags);\n-\n@@ -2851,1 +2990,1 @@\n-    __ tbz(r5, ResolvedFieldEntry::is_volatile_shift, notVolatile);\n+    __ tbz(flags, ResolvedFieldEntry::is_volatile_shift, notVolatile);\n@@ -2900,9 +3039,49 @@\n-    __ pop(atos);\n-    if (!is_static) pop_and_check_object(obj);\n-    \/\/ Store into the field\n-    \/\/ Clobbers: r10, r11, r3\n-    do_oop_store(_masm, field, r0, IN_HEAP);\n-    if (rc == may_rewrite) {\n-      patch_bytecode(Bytecodes::_fast_aputfield, bc, r1, true, byte_no);\n-    }\n-    __ b(Done);\n+     if (!EnableValhalla) {\n+      __ pop(atos);\n+      if (!is_static) pop_and_check_object(obj);\n+      \/\/ Store into the field\n+      \/\/ Clobbers: r10, r11, r3\n+      do_oop_store(_masm, field, r0, IN_HEAP);\n+      if (rc == may_rewrite) {\n+        patch_bytecode(Bytecodes::_fast_aputfield, bc, r1, true, byte_no);\n+      }\n+      __ b(Done);\n+     } else { \/\/ Valhalla\n+      __ pop(atos);\n+      if (is_static) {\n+        Label is_nullable;\n+         __ test_field_is_not_null_free_inline_type(flags, noreg \/* temp *\/, is_nullable);\n+         __ null_check(r0);  \/\/ FIXME JDK-8341120\n+         __ bind(is_nullable);\n+         do_oop_store(_masm, field, r0, IN_HEAP);\n+         __ b(Done);\n+      } else {\n+        Label null_free_reference, is_flat, rewrite_inline;\n+        __ test_field_is_flat(flags, noreg \/*temp*\/, is_flat);\n+        __ test_field_is_null_free_inline_type(flags, noreg \/*temp*\/, null_free_reference);\n+        pop_and_check_object(obj);\n+        \/\/ Store into the field\n+        \/\/ Clobbers: r10, r11, r3\n+        do_oop_store(_masm, field, r0, IN_HEAP);\n+        if (rc == may_rewrite) {\n+          patch_bytecode(Bytecodes::_fast_aputfield, bc, r19, true, byte_no);\n+        }\n+        __ b(Done);\n+        \/\/ Implementation of the inline type semantic\n+        __ bind(null_free_reference);\n+        __ null_check(r0);  \/\/ FIXME JDK-8341120\n+        pop_and_check_object(obj);\n+        \/\/ Store into the field\n+        \/\/ Clobbers: r10, r11, r3\n+        do_oop_store(_masm, field, r0, IN_HEAP);\n+        __ b(rewrite_inline);\n+        __ bind(is_flat);\n+        pop_and_check_object(r7);\n+        __ write_flat_field(cache, off, r3, r6, r7);\n+        __ bind(rewrite_inline);\n+        if (rc == may_rewrite) {\n+          patch_bytecode(Bytecodes::_fast_vputfield, bc, r19, true, byte_no);\n+        }\n+        __ b(Done);\n+      }\n+     }  \/\/ Valhalla\n@@ -3013,1 +3192,1 @@\n-    __ tbz(r5, ResolvedFieldEntry::is_volatile_shift, notVolatile);\n+    __ tbz(flags, ResolvedFieldEntry::is_volatile_shift, notVolatile);\n@@ -3047,0 +3226,1 @@\n+    case Bytecodes::_fast_vputfield: \/\/fall through\n@@ -3073,0 +3253,1 @@\n+    case Bytecodes::_fast_vputfield: \/\/fall through\n@@ -3120,0 +3301,16 @@\n+  case Bytecodes::_fast_vputfield:\n+    {\n+      Label is_flat, has_null_marker, done;\n+      __ test_field_is_flat(r5, noreg \/* temp *\/, is_flat);\n+      __ null_check(r0);\n+      do_oop_store(_masm, field, r0, IN_HEAP);\n+      __ b(done);\n+      __ bind(is_flat);\n+      __ load_field_entry(r4, r5);\n+      \/\/ Re-shuffle registers because of VM calls calling convention\n+      __ mov(r19, r1);\n+      __ mov(r7, r2);\n+      __ write_flat_field(r4, r19, r6, r8, r7);\n+      __ bind(done);\n+    }\n+    break;\n@@ -3215,0 +3412,9 @@\n+  case Bytecodes::_fast_vgetfield:\n+    {\n+      Register index = r4, tmp = r7;\n+      \/\/ field is flat\n+      __ load_unsigned_short(index, Address(r2, in_bytes(ResolvedFieldEntry::field_index_offset())));\n+      __ read_flat_field(r2, index, r1, tmp \/* temp *\/, r0);\n+      __ verify_oop(r0);\n+    }\n+    break;\n@@ -3635,63 +3841,2 @@\n-  \/\/ get instance_size in InstanceKlass (scaled to a count of bytes)\n-  __ ldrw(r3,\n-          Address(r4,\n-                  Klass::layout_helper_offset()));\n-  \/\/ test to see if it is malformed in some way\n-  __ tbnz(r3, exact_log2(Klass::_lh_instance_slow_path_bit), slow_case);\n-\n-  \/\/ Allocate the instance:\n-  \/\/  If TLAB is enabled:\n-  \/\/    Try to allocate in the TLAB.\n-  \/\/    If fails, go to the slow path.\n-  \/\/    Initialize the allocation.\n-  \/\/    Exit.\n-  \/\/\n-  \/\/  Go to slow path.\n-\n-  if (UseTLAB) {\n-    __ tlab_allocate(r0, r3, 0, noreg, r1, slow_case);\n-\n-    if (ZeroTLAB) {\n-      \/\/ the fields have been already cleared\n-      __ b(initialize_header);\n-    }\n-\n-    \/\/ The object is initialized before the header.  If the object size is\n-    \/\/ zero, go directly to the header initialization.\n-    int header_size = oopDesc::header_size() * HeapWordSize;\n-    assert(is_aligned(header_size, BytesPerLong), \"oop header size must be 8-byte-aligned\");\n-    __ sub(r3, r3, header_size);\n-    __ cbz(r3, initialize_header);\n-\n-    \/\/ Initialize object fields\n-    {\n-      __ add(r2, r0, header_size);\n-      Label loop;\n-      __ bind(loop);\n-      __ str(zr, Address(__ post(r2, BytesPerLong)));\n-      __ sub(r3, r3, BytesPerLong);\n-      __ cbnz(r3, loop);\n-    }\n-\n-    \/\/ initialize object header only.\n-    __ bind(initialize_header);\n-    if (UseCompactObjectHeaders) {\n-      __ ldr(rscratch1, Address(r4, Klass::prototype_header_offset()));\n-      __ str(rscratch1, Address(r0, oopDesc::mark_offset_in_bytes()));\n-    } else {\n-      __ mov(rscratch1, (intptr_t)markWord::prototype().value());\n-      __ str(rscratch1, Address(r0, oopDesc::mark_offset_in_bytes()));\n-      __ store_klass_gap(r0, zr);  \/\/ zero klass gap for compressed oops\n-      __ store_klass(r0, r4);      \/\/ store klass last\n-    }\n-\n-    if (DTraceAllocProbes) {\n-      \/\/ Trigger dtrace event for fastpath\n-      __ push(atos); \/\/ save the return value\n-      __ call_VM_leaf(\n-           CAST_FROM_FN_PTR(address, static_cast<int (*)(oopDesc*)>(SharedRuntime::dtrace_object_alloc)), r0);\n-      __ pop(atos); \/\/ restore the return value\n-\n-    }\n-    __ b(done);\n-  }\n+  __ allocate_instance(r4, r0, r3, r1, true, slow_case);\n+  __ b(done);\n@@ -3781,0 +3926,3 @@\n+  __ b(done);\n+  __ bind(is_null);\n+\n@@ -3783,4 +3931,1 @@\n-    __ b(done);\n-    __ bind(is_null);\n-  } else {\n-    __ bind(is_null);   \/\/ same as 'done'\n+\n@@ -3906,0 +4051,4 @@\n+  Label is_inline_type;\n+  __ ldr(rscratch1, Address(r0, oopDesc::mark_offset_in_bytes()));\n+  __ test_markword_is_inline_type(rscratch1, is_inline_type);\n+\n@@ -4007,0 +4156,5 @@\n+\n+  __ bind(is_inline_type);\n+  __ call_VM(noreg, CAST_FROM_FN_PTR(address,\n+                    InterpreterRuntime::throw_identity_exception), r0);\n+  __ should_not_reach_here();\n@@ -4017,0 +4171,12 @@\n+  const int is_inline_type_mask = markWord::inline_type_pattern;\n+  Label has_identity;\n+  __ ldr(rscratch1, Address(r0, oopDesc::mark_offset_in_bytes()));\n+  __ mov(rscratch2, is_inline_type_mask);\n+  __ andr(rscratch1, rscratch1, rscratch2);\n+  __ cmp(rscratch1, rscratch2);\n+  __ br(Assembler::NE, has_identity);\n+  __ call_VM(noreg, CAST_FROM_FN_PTR(address,\n+                     InterpreterRuntime::throw_illegal_monitor_state_exception));\n+  __ should_not_reach_here();\n+  __ bind(has_identity);\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/templateTable_aarch64.cpp","additions":276,"deletions":110,"binary":false,"changes":386,"status":"modified"},{"patch":"@@ -2560,0 +2560,4 @@\n+void LIR_Assembler::emit_profile_inline_type(LIR_OpProfileInlineType* op) {\n+  Unimplemented();\n+}\n+\n","filename":"src\/hotspot\/cpu\/arm\/c1_LIRAssembler_arm.cpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -3131,0 +3131,3 @@\n+void LIR_Assembler::emit_profile_inline_type(LIR_OpProfileInlineType* op) {\n+  Unimplemented();\n+}\n","filename":"src\/hotspot\/cpu\/ppc\/c1_LIRAssembler_ppc.cpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -116,1 +116,1 @@\n-inline void ContinuationHelper::Frame::patch_pc(const frame& f, address pc) {\n+inline void ContinuationHelper::Frame::patch_pc(const frame& f, address pc, bool callee_augmented) {\n","filename":"src\/hotspot\/cpu\/ppc\/continuationHelper_ppc.inline.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -1695,1 +1695,1 @@\n-        assert(ReturnTypeEntry::static_cell_count() < TypeStackSlotEntries::per_arg_count(),\n+        assert(SingleTypeEntry::static_cell_count() < TypeStackSlotEntries::per_arg_count(),\n@@ -1736,1 +1736,1 @@\n-    profile_obj_type(ret, R28_mdx, -in_bytes(ReturnTypeEntry::size()), tmp1, tmp2);\n+    profile_obj_type(ret, R28_mdx, -in_bytes(SingleTypeEntry::size()), tmp1, tmp2);\n","filename":"src\/hotspot\/cpu\/ppc\/interp_masm_ppc_64.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -1714,1 +1714,1 @@\n-address TemplateInterpreterGenerator::generate_normal_entry(bool synchronized) {\n+address TemplateInterpreterGenerator::generate_normal_entry(bool synchronized, bool object_init) {\n@@ -1809,0 +1809,7 @@\n+  \/\/ --------------------------------------------------------------------------\n+  \/\/ Issue a StoreStore barrier on entry to Object_init if the\n+  \/\/ class has strict field fields.  Be lazy, always do it.\n+  if (object_init) {\n+    __ membar(Assembler::StoreStore);\n+  }\n+\n","filename":"src\/hotspot\/cpu\/ppc\/templateInterpreterGenerator_ppc.cpp","additions":8,"deletions":1,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -134,1 +134,1 @@\n-inline void ContinuationHelper::Frame::patch_pc(const frame& f, address pc) {\n+inline void ContinuationHelper::Frame::patch_pc(const frame& f, address pc, bool callee_augmented) {\n","filename":"src\/hotspot\/cpu\/riscv\/continuationHelper_riscv.inline.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -2111,1 +2111,2 @@\n-  if (_desc->bytecode() == Bytecodes::_return) {\n+  if (_desc->bytecode() == Bytecodes::_return\n+      || _desc->bytecode() == Bytecodes::_return_register_finalizer) {\n","filename":"src\/hotspot\/cpu\/riscv\/templateTable_riscv.cpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -3067,0 +3067,4 @@\n+void LIR_Assembler::emit_profile_inline_type(LIR_OpProfileInlineType* op) {\n+  Unimplemented();\n+}\n+\n","filename":"src\/hotspot\/cpu\/s390\/c1_LIRAssembler_s390.cpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -1653,1 +1653,1 @@\n-        assert(ReturnTypeEntry::static_cell_count() < TypeStackSlotEntries::per_arg_count(), \"can't move past ret type\");\n+        assert(SingleTypeEntry::static_cell_count() < TypeStackSlotEntries::per_arg_count(), \"can't move past ret type\");\n@@ -1702,1 +1702,1 @@\n-    Address mdo_ret_addr(mdp, -in_bytes(ReturnTypeEntry::size()));\n+    Address mdo_ret_addr(mdp, -in_bytes(SingleTypeEntry::size()));\n","filename":"src\/hotspot\/cpu\/s390\/interp_masm_s390.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -1805,0 +1805,6 @@\n+  \/\/ If object_init == true, we should insert a StoreStore barrier here to\n+  \/\/ prevent strict fields initial default values from being observable.\n+  \/\/ However, s390 is a TSO platform, so if `this` escapes, strict fields\n+  \/\/ initialized values are guaranteed to be the ones observed, so the\n+  \/\/ barrier can be elided.\n+\n","filename":"src\/hotspot\/cpu\/s390\/templateInterpreterGenerator_s390.cpp","additions":6,"deletions":0,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -34,0 +34,1 @@\n+#include \"ci\/ciInlineKlass.hpp\"\n@@ -35,0 +36,1 @@\n+#include \"ci\/ciObjArrayKlass.hpp\"\n@@ -39,0 +41,1 @@\n+#include \"oops\/oop.inline.hpp\"\n@@ -431,1 +434,1 @@\n-  __ remove_frame(initial_frame_size_in_bytes());\n+  __ remove_frame(initial_frame_size_in_bytes(), needs_stack_repair());\n@@ -474,0 +477,44 @@\n+  if (InlineTypeReturnedAsFields) {\n+  #ifndef _LP64\n+     Unimplemented();\n+  #endif\n+    \/\/ Check if we are returning an non-null inline type and load its fields into registers\n+    ciType* return_type = compilation()->method()->return_type();\n+    if (return_type->is_inlinetype()) {\n+      ciInlineKlass* vk = return_type->as_inline_klass();\n+      if (vk->can_be_returned_as_fields()) {\n+        address unpack_handler = vk->unpack_handler();\n+        assert(unpack_handler != nullptr, \"must be\");\n+        __ call(RuntimeAddress(unpack_handler));\n+      }\n+    } else if (return_type->is_instance_klass() && (!return_type->is_loaded() || StressCallingConvention)) {\n+      Label skip;\n+      Label not_null;\n+      __ testptr(rax, rax);\n+      __ jcc(Assembler::notZero, not_null);\n+      \/\/ Returned value is null, zero all return registers because they may belong to oop fields\n+      __ xorq(j_rarg1, j_rarg1);\n+      __ xorq(j_rarg2, j_rarg2);\n+      __ xorq(j_rarg3, j_rarg3);\n+      __ xorq(j_rarg4, j_rarg4);\n+      __ xorq(j_rarg5, j_rarg5);\n+      __ jmp(skip);\n+      __ bind(not_null);\n+\n+      \/\/ Check if we are returning an non-null inline type and load its fields into registers\n+      __ test_oop_is_not_inline_type(rax, rscratch1, skip, \/* can_be_null= *\/ false);\n+\n+      \/\/ Load fields from a buffered value with an inline class specific handler\n+      __ load_klass(rdi, rax, rscratch1);\n+      __ movptr(rdi, Address(rdi, InstanceKlass::adr_inlineklass_fixed_block_offset()));\n+      __ movptr(rdi, Address(rdi, InlineKlass::unpack_handler_offset()));\n+      \/\/ Unpack handler can be null if inline type is not scalarizable in returns\n+      __ testptr(rdi, rdi);\n+      __ jcc(Assembler::zero, skip);\n+      __ call(rdi);\n+\n+      __ bind(skip);\n+    }\n+    \/\/ At this point, rax points to the value object (for interpreter or C1 caller).\n+    \/\/ The fields of the object are copied into registers (for C2 caller).\n+  }\n@@ -476,1 +523,1 @@\n-  __ remove_frame(initial_frame_size_in_bytes());\n+  __ remove_frame(initial_frame_size_in_bytes(), needs_stack_repair());\n@@ -492,0 +539,4 @@\n+int LIR_Assembler::store_inline_type_fields_to_buf(ciInlineKlass* vk) {\n+  return (__ store_inline_type_fields_to_buf(vk, false));\n+}\n+\n@@ -1230,1 +1281,1 @@\n-  if (UseSlowPath ||\n+  if (UseSlowPath || op->always_slow_path() ||\n@@ -1329,24 +1380,26 @@\n-  __ testptr(obj, obj);\n-  if (op->should_profile()) {\n-    Label not_null;\n-    Register mdo  = klass_RInfo;\n-    __ mov_metadata(mdo, md->constant_encoding());\n-    __ jccb(Assembler::notEqual, not_null);\n-    \/\/ Object is null; update MDO and exit\n-    Address data_addr(mdo, md->byte_offset_of_slot(data, DataLayout::flags_offset()));\n-    int header_bits = BitData::null_seen_byte_constant();\n-    __ orb(data_addr, header_bits);\n-    __ jmp(*obj_is_null);\n-    __ bind(not_null);\n-\n-    Label update_done;\n-    Register recv = k_RInfo;\n-    __ load_klass(recv, obj, tmp_load_klass);\n-    type_profile_helper(mdo, md, data, recv, &update_done);\n-\n-    Address nonprofiled_receiver_count_addr(mdo, md->byte_offset_of_slot(data, CounterData::count_offset()));\n-    __ addptr(nonprofiled_receiver_count_addr, DataLayout::counter_increment);\n-\n-    __ bind(update_done);\n-  } else {\n-    __ jcc(Assembler::equal, *obj_is_null);\n+  if (op->need_null_check()) {\n+    __ testptr(obj, obj);\n+    if (op->should_profile()) {\n+      Label not_null;\n+      Register mdo  = klass_RInfo;\n+      __ mov_metadata(mdo, md->constant_encoding());\n+      __ jccb(Assembler::notEqual, not_null);\n+      \/\/ Object is null; update MDO and exit\n+      Address data_addr(mdo, md->byte_offset_of_slot(data, DataLayout::flags_offset()));\n+      int header_bits = BitData::null_seen_byte_constant();\n+      __ orb(data_addr, header_bits);\n+      __ jmp(*obj_is_null);\n+      __ bind(not_null);\n+\n+      Label update_done;\n+      Register recv = k_RInfo;\n+      __ load_klass(recv, obj, tmp_load_klass);\n+      type_profile_helper(mdo, md, data, recv, &update_done);\n+\n+      Address nonprofiled_receiver_count_addr(mdo, md->byte_offset_of_slot(data, CounterData::count_offset()));\n+      __ addptr(nonprofiled_receiver_count_addr, DataLayout::counter_increment);\n+\n+      __ bind(update_done);\n+    } else {\n+      __ jcc(Assembler::equal, *obj_is_null);\n+    }\n@@ -1363,0 +1416,1 @@\n+    assert(!k->is_loaded() || !k->is_obj_array_klass(), \"Use refined array for a direct pointer comparison\");\n@@ -1387,1 +1441,8 @@\n-        __ cmpptr(klass_RInfo, k_RInfo);\n+        if (k->is_loaded() && k->is_obj_array_klass()) {\n+          \/\/ For a direct pointer comparison, we need the refined array klass pointer\n+          ciKlass* k_refined = ciObjArrayKlass::make(k->as_obj_array_klass()->element_klass());\n+          __ mov_metadata(tmp_load_klass, k_refined->constant_encoding());\n+          __ cmpptr(klass_RInfo, tmp_load_klass);\n+        } else {\n+          __ cmpptr(klass_RInfo, k_RInfo);\n+        }\n@@ -1522,0 +1583,103 @@\n+void LIR_Assembler::emit_opFlattenedArrayCheck(LIR_OpFlattenedArrayCheck* op) {\n+  \/\/ We are loading\/storing from\/to an array that *may* be a flat array (the\n+  \/\/ declared type is Object[], abstract[], interface[] or VT.ref[]).\n+  \/\/ If this array is a flat array, take the slow path.\n+  __ test_flat_array_oop(op->array()->as_register(), op->tmp()->as_register(), *op->stub()->entry());\n+  if (!op->value()->is_illegal()) {\n+    \/\/ TODO 8350865 This is also used for profiling code, right? And in that case we don't care about null but just want to know if the array is flat or not.\n+    \/\/ The array is not a flat array, but it might be null-free. If we are storing\n+    \/\/ a null into a null-free array, take the slow path (which will throw NPE).\n+    Label skip;\n+    __ cmpptr(op->value()->as_register(), NULL_WORD);\n+    __ jcc(Assembler::notEqual, skip);\n+    __ test_null_free_array_oop(op->array()->as_register(), op->tmp()->as_register(), *op->stub()->entry());\n+    __ bind(skip);\n+  }\n+}\n+\n+void LIR_Assembler::emit_opNullFreeArrayCheck(LIR_OpNullFreeArrayCheck* op) {\n+  \/\/ We are storing into an array that *may* be null-free (the declared type is\n+  \/\/ Object[], abstract[], interface[] or VT.ref[]).\n+  Label test_mark_word;\n+  Register tmp = op->tmp()->as_register();\n+  __ movptr(tmp, Address(op->array()->as_register(), oopDesc::mark_offset_in_bytes()));\n+  __ testl(tmp, markWord::unlocked_value);\n+  __ jccb(Assembler::notZero, test_mark_word);\n+  __ load_prototype_header(tmp, op->array()->as_register(), rscratch1);\n+  __ bind(test_mark_word);\n+  __ testl(tmp, markWord::null_free_array_bit_in_place);\n+}\n+\n+void LIR_Assembler::emit_opSubstitutabilityCheck(LIR_OpSubstitutabilityCheck* op) {\n+  Label L_oops_equal;\n+  Label L_oops_not_equal;\n+  Label L_end;\n+\n+  Register left  = op->left()->as_register();\n+  Register right = op->right()->as_register();\n+\n+  __ cmpptr(left, right);\n+  __ jcc(Assembler::equal, L_oops_equal);\n+\n+  \/\/ (1) Null check -- if one of the operands is null, the other must not be null (because\n+  \/\/     the two references are not equal), so they are not substitutable,\n+  \/\/     FIXME: do null check only if the operand is nullable\n+  __ testptr(left, right);\n+  __ jcc(Assembler::zero, L_oops_not_equal);\n+\n+  ciKlass* left_klass = op->left_klass();\n+  ciKlass* right_klass = op->right_klass();\n+\n+  \/\/ (2) Inline type check -- if either of the operands is not a inline type,\n+  \/\/     they are not substitutable. We do this only if we are not sure that the\n+  \/\/     operands are inline type\n+  if ((left_klass == nullptr || right_klass == nullptr) ||\/\/ The klass is still unloaded, or came from a Phi node.\n+      !left_klass->is_inlinetype() || !right_klass->is_inlinetype()) {\n+    Register tmp1  = op->tmp1()->as_register();\n+    __ movptr(tmp1, (intptr_t)markWord::inline_type_pattern);\n+    __ andptr(tmp1, Address(left, oopDesc::mark_offset_in_bytes()));\n+    __ andptr(tmp1, Address(right, oopDesc::mark_offset_in_bytes()));\n+    __ cmpptr(tmp1, (intptr_t)markWord::inline_type_pattern);\n+    __ jcc(Assembler::notEqual, L_oops_not_equal);\n+  }\n+\n+  \/\/ (3) Same klass check: if the operands are of different klasses, they are not substitutable.\n+  if (left_klass != nullptr && left_klass->is_inlinetype() && left_klass == right_klass) {\n+    \/\/ No need to load klass -- the operands are statically known to be the same inline klass.\n+    __ jmp(*op->stub()->entry());\n+  } else {\n+    Register left_klass_op = op->left_klass_op()->as_register();\n+    Register right_klass_op = op->right_klass_op()->as_register();\n+\n+    if (UseCompressedClassPointers) {\n+      __ movl(left_klass_op,  Address(left,  oopDesc::klass_offset_in_bytes()));\n+      __ movl(right_klass_op, Address(right, oopDesc::klass_offset_in_bytes()));\n+      __ cmpl(left_klass_op, right_klass_op);\n+    } else {\n+      __ movptr(left_klass_op,  Address(left,  oopDesc::klass_offset_in_bytes()));\n+      __ movptr(right_klass_op, Address(right, oopDesc::klass_offset_in_bytes()));\n+      __ cmpptr(left_klass_op, right_klass_op);\n+    }\n+\n+    __ jcc(Assembler::equal, *op->stub()->entry()); \/\/ same klass -> do slow check\n+    \/\/ fall through to L_oops_not_equal\n+  }\n+\n+  __ bind(L_oops_not_equal);\n+  move(op->not_equal_result(), op->result_opr());\n+  __ jmp(L_end);\n+\n+  __ bind(L_oops_equal);\n+  move(op->equal_result(), op->result_opr());\n+  __ jmp(L_end);\n+\n+  \/\/ We've returned from the stub. RAX contains 0x0 IFF the two\n+  \/\/ operands are not substitutable. (Don't compare against 0x1 in case the\n+  \/\/ C compiler is naughty)\n+  __ bind(*op->stub()->continuation());\n+  __ cmpl(rax, 0);\n+  __ jcc(Assembler::equal, L_oops_not_equal); \/\/ (call_stub() == 0x0) -> not_equal\n+  move(op->equal_result(), op->result_opr()); \/\/ (call_stub() != 0x0) -> equal\n+  \/\/ fall-through\n+  __ bind(L_end);\n+}\n@@ -1567,0 +1731,15 @@\n+void LIR_Assembler::move(LIR_Opr src, LIR_Opr dst) {\n+  assert(dst->is_cpu_register(), \"must be\");\n+  assert(dst->type() == src->type(), \"must be\");\n+\n+  if (src->is_cpu_register()) {\n+    reg2reg(src, dst);\n+  } else if (src->is_stack()) {\n+    stack2reg(src, dst, dst->type());\n+  } else if (src->is_constant()) {\n+    const2reg(src, dst, lir_patch_none, nullptr);\n+  } else {\n+    ShouldNotReachHere();\n+  }\n+}\n+\n@@ -2177,1 +2356,1 @@\n-  add_call_info(code_offset(), op->info());\n+  add_call_info(code_offset(), op->info(), op->maybe_return_as_fields());\n@@ -2184,1 +2363,1 @@\n-  add_call_info(code_offset(), op->info());\n+  add_call_info(code_offset(), op->info(), op->maybe_return_as_fields());\n@@ -2351,0 +2530,15 @@\n+void LIR_Assembler::arraycopy_inlinetype_check(Register obj, Register tmp, CodeStub* slow_path, bool is_dest, bool null_check) {\n+  if (null_check) {\n+    __ testptr(obj, obj);\n+    __ jcc(Assembler::zero, *slow_path->entry());\n+  }\n+  if (is_dest) {\n+    __ test_null_free_array_oop(obj, tmp, *slow_path->entry());\n+    \/\/ TODO 8350865 Flat no longer implies null-free, so we need to check for flat dest. Can we do better here?\n+    __ test_flat_array_oop(obj, tmp, *slow_path->entry());\n+  } else {\n+    __ test_flat_array_oop(obj, tmp, *slow_path->entry());\n+  }\n+}\n+\n+\n@@ -2370,0 +2564,6 @@\n+  if (flags & LIR_OpArrayCopy::always_slow_path) {\n+    __ jmp(*stub->entry());\n+    __ bind(*stub->continuation());\n+    return;\n+  }\n+\n@@ -2446,0 +2646,8 @@\n+  \/\/ Handle inline type arrays\n+  if (flags & LIR_OpArrayCopy::src_inlinetype_check) {\n+    arraycopy_inlinetype_check(src, tmp, stub, false, (flags & LIR_OpArrayCopy::src_null_check));\n+  }\n+  if (flags & LIR_OpArrayCopy::dst_inlinetype_check) {\n+    arraycopy_inlinetype_check(dst, tmp, stub, true, (flags & LIR_OpArrayCopy::dst_null_check));\n+  }\n+\n@@ -2672,0 +2880,1 @@\n+\n@@ -3008,0 +3217,21 @@\n+void LIR_Assembler::emit_profile_inline_type(LIR_OpProfileInlineType* op) {\n+  Register obj = op->obj()->as_register();\n+  Register tmp = op->tmp()->as_pointer_register();\n+  Address mdo_addr = as_Address(op->mdp()->as_address_ptr());\n+  bool not_null = op->not_null();\n+  int flag = op->flag();\n+\n+  Label not_inline_type;\n+  if (!not_null) {\n+    __ testptr(obj, obj);\n+    __ jccb(Assembler::zero, not_inline_type);\n+  }\n+\n+  __ test_oop_is_not_inline_type(obj, tmp, not_inline_type);\n+\n+  __ orb(mdo_addr, flag);\n+\n+  __ bind(not_inline_type);\n+}\n+\n+\n@@ -3193,0 +3423,3 @@\n+void LIR_Assembler::check_orig_pc() {\n+  __ cmpptr(frame_map()->address_for_orig_pc_addr(), NULL_WORD);\n+}\n","filename":"src\/hotspot\/cpu\/x86\/c1_LIRAssembler_x86.cpp","additions":263,"deletions":30,"binary":false,"changes":293,"status":"modified"},{"patch":"@@ -54,0 +54,3 @@\n+  void arraycopy_inlinetype_check(Register obj, Register tmp, CodeStub* slow_path, bool is_dest, bool null_check);\n+  void move(LIR_Opr src, LIR_Opr dst);\n+\n","filename":"src\/hotspot\/cpu\/x86\/c1_LIRAssembler_x86.hpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -68,0 +68,6 @@\n+\n+  \/\/ Repair the sender sp if the frame has been extended\n+  if (sender_cb->is_nmethod()) {\n+    sender_sp = f.repair_sender_sp(sender_sp, link_addr);\n+  }\n+\n@@ -75,1 +81,1 @@\n-frame FreezeBase::new_heap_frame(frame& f, frame& caller) {\n+frame FreezeBase::new_heap_frame(frame& f, frame& caller, int size_adjust) {\n@@ -109,2 +115,2 @@\n-    sp = caller.unextended_sp() - fsize;\n-    if (caller.is_interpreted_frame()) {\n+    sp = caller.unextended_sp() - fsize - size_adjust;\n+    if (caller.is_interpreted_frame() && size_adjust == 0) {\n@@ -177,1 +183,1 @@\n-inline void FreezeBase::patch_pd(frame& hf, const frame& caller) {\n+inline void FreezeBase::patch_pd(frame& hf, const frame& caller, bool is_bottom_frame) {\n@@ -181,1 +187,2 @@\n-  } else {\n+  } else if (is_bottom_frame && caller.pc() != nullptr) {\n+    assert(caller.is_compiled_frame(), \"\");\n@@ -252,1 +259,1 @@\n-template<typename FKind> frame ThawBase::new_stack_frame(const frame& hf, frame& caller, bool bottom) {\n+template<typename FKind> frame ThawBase::new_stack_frame(const frame& hf, frame& caller, bool bottom, int size_adjust) {\n@@ -279,1 +286,1 @@\n-    intptr_t* frame_sp = caller.unextended_sp() - fsize;\n+    intptr_t* frame_sp = caller.unextended_sp() - fsize - size_adjust;\n@@ -281,7 +288,4 @@\n-      int argsize = FKind::stack_argsize(hf);\n-\n-      fsize += argsize;\n-      frame_sp   -= argsize;\n-      caller.set_sp(caller.sp() - argsize);\n-      assert(caller.sp() == frame_sp + (fsize-argsize), \"\");\n-\n+      if (size_adjust == 0) {\n+        int argsize = FKind::stack_argsize(hf);\n+        frame_sp -= argsize;\n+      }\n@@ -290,0 +294,2 @@\n+    caller.set_sp(frame_sp + fsize);\n+    assert(is_aligned(frame_sp, frame::frame_alignment), \"\");\n@@ -296,1 +302,1 @@\n-      fp = frame_sp + FKind::size(hf) - frame::sender_sp_offset;\n+      fp = frame_sp + fsize - frame::sender_sp_offset;\n@@ -310,1 +316,0 @@\n-    caller.set_sp(caller.sp() - 1);\n@@ -317,1 +322,3 @@\n-  patch_callee_link(caller, caller.fp());\n+  if (caller.is_interpreted_frame() || PreserveFramePointer) {\n+    patch_callee_link(caller, caller.fp());\n+  }\n","filename":"src\/hotspot\/cpu\/x86\/continuationFreezeThaw_x86.inline.hpp","additions":24,"deletions":17,"binary":false,"changes":41,"status":"modified"},{"patch":"@@ -125,2 +125,2 @@\n-inline void ContinuationHelper::Frame::patch_pc(const frame& f, address pc) {\n-  address* pc_addr = &(((address*) f.sp())[-1]);\n+inline void ContinuationHelper::Frame::patch_pc(const frame& f, address pc, bool callee_augmented) {\n+  address* pc_addr = &(((address*) (callee_augmented ? f.unextended_sp() : f.sp()))[-1]);\n","filename":"src\/hotspot\/cpu\/x86\/continuationHelper_x86.inline.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -148,1 +148,0 @@\n-      sender_unextended_sp = sender_sp;\n@@ -152,2 +151,2 @@\n-      saved_fp = (intptr_t*) *(sender_sp - frame::sender_sp_offset);\n-    }\n+      intptr_t** saved_fp_addr = (intptr_t**) (sender_sp - frame::sender_sp_offset);\n+      saved_fp = *saved_fp_addr;\n@@ -155,0 +154,4 @@\n+      \/\/ Repair the sender sp if this is a method with scalarized inline type args\n+      sender_sp = repair_sender_sp(sender_sp, saved_fp_addr);\n+      sender_unextended_sp = sender_sp;\n+    }\n@@ -615,0 +618,3 @@\n+      if (cb()->is_nmethod() && cb()->as_nmethod_or_null()->needs_stack_repair()) {\n+        values.describe(frame_no, fp_loc - 1, err_msg(\"fsize for #%d\", frame_no), 1);\n+      }\n@@ -638,0 +644,37 @@\n+\/\/ Check for a method with scalarized inline type arguments that needs\n+\/\/ a stack repair and return the repaired sender stack pointer.\n+intptr_t* frame::repair_sender_sp(intptr_t* sender_sp, intptr_t** saved_fp_addr) const {\n+  nmethod* nm = _cb->as_nmethod_or_null();\n+  if (nm != nullptr && nm->needs_stack_repair()) {\n+    \/\/ The stack increment resides just below the saved rbp on the stack\n+    \/\/ and does not account for the return address.\n+    intptr_t* real_frame_size_addr = (intptr_t*) (saved_fp_addr - 1);\n+    int real_frame_size = ((*real_frame_size_addr) + wordSize) \/ wordSize;\n+    assert(real_frame_size >= _cb->frame_size() && real_frame_size <= 1000000, \"invalid frame size\");\n+    sender_sp = unextended_sp() + real_frame_size;\n+  }\n+  return sender_sp;\n+}\n+\n+intptr_t* frame::repair_sender_sp(nmethod* nm, intptr_t* sp, intptr_t** saved_fp_addr) {\n+  assert(nm != nullptr && nm->needs_stack_repair(), \"\");\n+  \/\/ The stack increment resides just below the saved rbp on the stack\n+  \/\/ and does not account for the return address.\n+  intptr_t* real_frame_size_addr = (intptr_t*) (saved_fp_addr - 1);\n+  int real_frame_size = ((*real_frame_size_addr) + wordSize) \/ wordSize;\n+  assert(real_frame_size >= nm->frame_size() && real_frame_size <= 1000000, \"invalid frame size\");\n+  return sp + real_frame_size;\n+}\n+\n+bool frame::was_augmented_on_entry(int& real_size) const {\n+  assert(is_compiled_frame(), \"\");\n+  if (_cb->as_nmethod_or_null()->needs_stack_repair()) {\n+    intptr_t* real_frame_size_addr = unextended_sp() + _cb->frame_size() - sender_sp_offset - 1;\n+    log_trace(continuations)(\"real_frame_size is addr is \" INTPTR_FORMAT, p2i(real_frame_size_addr));\n+    real_size = ((*real_frame_size_addr) + wordSize) \/ wordSize;\n+    return real_size != _cb->frame_size();\n+  }\n+  real_size = _cb->frame_size();\n+  return false;\n+}\n+\n","filename":"src\/hotspot\/cpu\/x86\/frame_x86.cpp","additions":46,"deletions":3,"binary":false,"changes":49,"status":"modified"},{"patch":"@@ -142,0 +142,5 @@\n+  \/\/ Support for scalarized inline type calling convention\n+  intptr_t* repair_sender_sp(intptr_t* sender_sp, intptr_t** saved_fp_addr) const;\n+  static intptr_t* repair_sender_sp(nmethod* nm, intptr_t* sp, intptr_t** saved_fp_addr);\n+  bool was_augmented_on_entry(int& real_size) const;\n+\n","filename":"src\/hotspot\/cpu\/x86\/frame_x86.hpp","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -35,0 +35,3 @@\n+#ifdef COMPILER1\n+#include \"c1\/c1_Runtime1.hpp\"\n+#endif\n@@ -432,2 +435,3 @@\n-  \/\/ On Intel the return_address is always the word on the stack\n-  address sender_pc = (address) *(sender_sp-1);\n+#ifdef ASSERT\n+  address sender_pc_copy = (address) *(sender_sp-1);\n+#endif\n@@ -440,0 +444,16 @@\n+  \/\/ Repair the sender sp if the frame has been extended\n+  sender_sp = repair_sender_sp(sender_sp, saved_fp_addr);\n+\n+  \/\/ On Intel the return_address is always the word on the stack\n+  address sender_pc = (address) *(sender_sp-1);\n+\n+#ifdef ASSERT\n+  if (sender_pc != sender_pc_copy) {\n+    \/\/ When extending the stack in the callee method entry to make room for unpacking of value\n+    \/\/ type args, we keep a copy of the sender pc at the expected location in the callee frame.\n+    \/\/ If the sender pc is patched due to deoptimization, the copy is not consistent anymore.\n+    nmethod* nm = CodeCache::find_blob(sender_pc)->as_nmethod();\n+    assert(sender_pc == nm->deopt_handler_entry(), \"unexpected sender pc\");\n+  }\n+#endif\n+\n@@ -444,2 +464,21 @@\n-    if (!_cb->is_nmethod()) { \/\/ compiled frames do not use callee-saved registers\n-      map->set_include_argument_oops(_cb->caller_must_gc_arguments(map->thread()));\n+    bool c1_buffering = false;\n+#ifdef COMPILER1\n+    nmethod* nm = _cb->as_nmethod_or_null();\n+    if (nm != nullptr && nm->is_compiled_by_c1() && nm->method()->has_scalarized_args() &&\n+        pc() < nm->verified_inline_entry_point()) {\n+      \/\/ TODO 8284443 Can't we do that by not passing 'dont_gc_arguments' in case 'StubId::c1_buffer_inline_args_id' in 'Runtime1::generate_code_for'?\n+      \/\/ The VEP and VIEP(RO) of C1-compiled methods call buffer_inline_args_xxx\n+      \/\/ before doing any argument shuffling, so we need to scan the oops\n+      \/\/ as the caller passes them.\n+      c1_buffering = true;\n+#ifdef ASSERT\n+      NativeCall* call = nativeCall_before(pc());\n+      address dest = call->destination();\n+      assert(dest == Runtime1::entry_for(StubId::c1_buffer_inline_args_no_receiver_id) ||\n+             dest == Runtime1::entry_for(StubId::c1_buffer_inline_args_id), \"unexpected safepoint in entry point\");\n+#endif\n+    }\n+#endif\n+    if (!_cb->is_nmethod() || c1_buffering) { \/\/ compiled frames do not use callee-saved registers\n+      bool caller_args = _cb->caller_must_gc_arguments(map->thread()) || c1_buffering;\n+      map->set_include_argument_oops(caller_args);\n","filename":"src\/hotspot\/cpu\/x86\/frame_x86.inline.hpp","additions":43,"deletions":4,"binary":false,"changes":47,"status":"modified"},{"patch":"@@ -238,2 +238,18 @@\n-  \/\/ Determine and save the live input values\n-  __ push_call_clobbered_registers();\n+  if (EnableValhalla && InlineTypePassFieldsAsArgs) {\n+    \/\/ Barriers might be emitted when converting between (scalarized) calling conventions for inline\n+    \/\/ types. Save all argument registers before calling into the runtime.\n+    \/\/ TODO 8366717: use push_set() (see JDK-8283327 push\/pop_call_clobbered_registers & aarch64 )\n+    __ pusha();\n+    __ subptr(rsp, 64);\n+    __ movdbl(Address(rsp, 0),  j_farg0);\n+    __ movdbl(Address(rsp, 8),  j_farg1);\n+    __ movdbl(Address(rsp, 16), j_farg2);\n+    __ movdbl(Address(rsp, 24), j_farg3);\n+    __ movdbl(Address(rsp, 32), j_farg4);\n+    __ movdbl(Address(rsp, 40), j_farg5);\n+    __ movdbl(Address(rsp, 48), j_farg6);\n+    __ movdbl(Address(rsp, 56), j_farg7);\n+  } else {\n+    \/\/ Determine and save the live input values\n+    __ push_call_clobbered_registers();\n+  }\n@@ -266,1 +282,15 @@\n-  __ pop_call_clobbered_registers();\n+  if (EnableValhalla && InlineTypePassFieldsAsArgs) {\n+    \/\/ Restore registers\n+    __ movdbl(j_farg0, Address(rsp, 0));\n+    __ movdbl(j_farg1, Address(rsp, 8));\n+    __ movdbl(j_farg2, Address(rsp, 16));\n+    __ movdbl(j_farg3, Address(rsp, 24));\n+    __ movdbl(j_farg4, Address(rsp, 32));\n+    __ movdbl(j_farg5, Address(rsp, 40));\n+    __ movdbl(j_farg6, Address(rsp, 48));\n+    __ movdbl(j_farg7, Address(rsp, 56));\n+    __ addptr(rsp, 64);\n+    __ popa();\n+  } else {\n+    __ pop_call_clobbered_registers();\n+  }\n@@ -388,0 +418,1 @@\n+  bool dest_uninitialized = (decorators & IS_DEST_UNINITIALIZED) != 0;\n@@ -389,1 +420,1 @@\n-  bool needs_pre_barrier = as_normal;\n+  bool needs_pre_barrier = as_normal && !dest_uninitialized;\n","filename":"src\/hotspot\/cpu\/x86\/gc\/g1\/g1BarrierSetAssembler_x86.cpp","additions":35,"deletions":4,"binary":false,"changes":39,"status":"modified"},{"patch":"@@ -173,1 +173,0 @@\n-\n@@ -184,1 +183,8 @@\n-      store_check(masm, dst.base(), dst);\n+      if (tmp3 != noreg) {\n+        \/\/ Called by MacroAssembler::pack_inline_helper. We cannot corrupt the dst.base() register\n+        __ movptr(tmp3, dst.base());\n+        store_check(masm, tmp3, dst);\n+      } else {\n+        \/\/ It's OK to corrupt the dst.base() register.\n+        store_check(masm, dst.base(), dst);\n+      }\n","filename":"src\/hotspot\/cpu\/x86\/gc\/shared\/cardTableBarrierSetAssembler_x86.cpp","additions":8,"deletions":2,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2018, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2018, 2019, Oracle and\/or its affiliates. All rights reserved.\n","filename":"src\/hotspot\/cpu\/x86\/gc\/shared\/cardTableBarrierSetAssembler_x86.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -31,0 +31,1 @@\n+#include \"oops\/constMethodFlags.hpp\"\n@@ -34,0 +35,1 @@\n+#include \"oops\/inlineKlass.hpp\"\n@@ -168,1 +170,1 @@\n-        assert(ReturnTypeEntry::static_cell_count() < TypeStackSlotEntries::per_arg_count(), \"can't move past ret type\");\n+        assert(SingleTypeEntry::static_cell_count() < TypeStackSlotEntries::per_arg_count(), \"can't move past ret type\");\n@@ -213,1 +215,1 @@\n-    Address mdo_ret_addr(mdp, -in_bytes(ReturnTypeEntry::size()));\n+    Address mdo_ret_addr(mdp, -in_bytes(SingleTypeEntry::size()));\n@@ -585,1 +587,2 @@\n-                                                  Label& ok_is_subtype) {\n+                                                  Label& ok_is_subtype,\n+                                                  bool profile) {\n@@ -593,1 +596,3 @@\n-  profile_typecheck(rcx, Rsub_klass, rdi); \/\/ blows rcx, reloads rdi\n+  if (profile) {\n+    profile_typecheck(rcx, Rsub_klass, rdi); \/\/ blows rcx, reloads rdi\n+  }\n@@ -887,1 +892,1 @@\n- \/\/ get method access flags\n+  \/\/ get method access flags\n@@ -1026,4 +1031,2 @@\n-  \/\/ remove activation\n-  \/\/ get sender sp\n-  movptr(rbx,\n-         Address(rbp, frame::interpreter_frame_sender_sp_offset * wordSize));\n+    movptr(rbx,\n+               Address(rbp, frame::interpreter_frame_sender_sp_offset * wordSize));\n@@ -1054,0 +1057,52 @@\n+  \/\/ remove activation\n+  \/\/ get sender sp\n+  movptr(rbx,\n+         Address(rbp, frame::interpreter_frame_sender_sp_offset * wordSize));\n+\n+  if (state == atos && InlineTypeReturnedAsFields) {\n+    Label skip;\n+    Label not_null;\n+    testptr(rax, rax);\n+    jcc(Assembler::notZero, not_null);\n+    \/\/ Returned value is null, zero all return registers because they may belong to oop fields\n+    xorq(j_rarg1, j_rarg1);\n+    xorq(j_rarg2, j_rarg2);\n+    xorq(j_rarg3, j_rarg3);\n+    xorq(j_rarg4, j_rarg4);\n+    xorq(j_rarg5, j_rarg5);\n+    jmp(skip);\n+    bind(not_null);\n+\n+    \/\/ Check if we are returning an non-null inline type and load its fields into registers\n+    test_oop_is_not_inline_type(rax, rscratch1, skip, \/* can_be_null= *\/ false);\n+\n+#ifndef _LP64\n+    super_call_VM_leaf(StubRoutines::load_inline_type_fields_in_regs());\n+#else\n+    \/\/ Load fields from a buffered value with an inline class specific handler\n+    load_klass(rdi, rax, rscratch1);\n+    movptr(rdi, Address(rdi, InstanceKlass::adr_inlineklass_fixed_block_offset()));\n+    movptr(rdi, Address(rdi, InlineKlass::unpack_handler_offset()));\n+    \/\/ Unpack handler can be null if inline type is not scalarizable in returns\n+    testptr(rdi, rdi);\n+    jcc(Assembler::zero, skip);\n+    call(rdi);\n+#endif\n+#ifdef ASSERT\n+    \/\/ TODO 8284443 Enable\n+    if (StressCallingConvention && false) {\n+      Label skip_stress;\n+      movptr(rscratch1, Address(rbp, frame::interpreter_frame_method_offset * wordSize));\n+      movl(rscratch1, Address(rscratch1, Method::flags_offset()));\n+      testl(rcx, MethodFlags::has_scalarized_return_flag());\n+      jcc(Assembler::zero, skip_stress);\n+      load_klass(rax, rax, rscratch1);\n+      orptr(rax, 1);\n+      bind(skip_stress);\n+    }\n+#endif\n+    \/\/ call above kills the value in rbx. Reload it.\n+    movptr(rbx, Address(rbp, frame::interpreter_frame_sender_sp_offset * wordSize));\n+    bind(skip);\n+  }\n+\n@@ -1090,0 +1145,89 @@\n+void InterpreterMacroAssembler::allocate_instance(Register klass, Register new_obj,\n+                                                  Register t1, Register t2,\n+                                                  bool clear_fields, Label& alloc_failed) {\n+  MacroAssembler::allocate_instance(klass, new_obj, t1, t2, clear_fields, alloc_failed);\n+  if (DTraceAllocProbes) {\n+    \/\/ Trigger dtrace event for fastpath\n+    push(atos);\n+    call_VM_leaf(CAST_FROM_FN_PTR(address, static_cast<int (*)(oopDesc*)>(SharedRuntime::dtrace_object_alloc)), new_obj);\n+    pop(atos);\n+  }\n+}\n+\n+void InterpreterMacroAssembler::read_flat_field(Register entry, Register tmp1, Register tmp2, Register obj) {\n+  Label alloc_failed, slow_path, done;\n+  const Register alloc_temp = LP64_ONLY(rscratch1) NOT_LP64(rsi);\n+  const Register dst_temp   = LP64_ONLY(rscratch2) NOT_LP64(rdi);\n+  assert_different_registers(obj, entry, tmp1, tmp2, dst_temp, r8, r9);\n+\n+  \/\/ If the field is nullable, jump to slow path\n+  load_unsigned_byte(tmp1, Address(entry, in_bytes(ResolvedFieldEntry::flags_offset())));\n+  testl(tmp1, 1 << ResolvedFieldEntry::is_null_free_inline_type_shift);\n+  jcc(Assembler::equal, slow_path);\n+\n+  \/\/ Grap the inline field klass\n+  const Register field_klass = tmp1;\n+  load_unsigned_short(tmp2, Address(entry, in_bytes(ResolvedFieldEntry::field_index_offset())));\n+\n+  movptr(tmp1, Address(entry, ResolvedFieldEntry::field_holder_offset()));\n+  get_inline_type_field_klass(tmp1, tmp2, field_klass);\n+\n+  \/\/ allocate buffer\n+  push(obj);  \/\/ push object being read from\n+  allocate_instance(field_klass, obj, alloc_temp, dst_temp, false, alloc_failed);\n+\n+  \/\/ Have an oop instance buffer, copy into it\n+  load_unsigned_short(r9, Address(entry, in_bytes(ResolvedFieldEntry::field_index_offset())));\n+  movptr(r8, Address(entry, in_bytes(ResolvedFieldEntry::field_holder_offset())));\n+  inline_layout_info(r8, r9, r8); \/\/ holder, index, info => InlineLayoutInfo into r8\n+\n+  payload_addr(obj, dst_temp, field_klass);\n+  pop(alloc_temp);             \/\/ restore object being read from\n+  load_sized_value(tmp2, Address(entry, in_bytes(ResolvedFieldEntry::field_offset_offset())), sizeof(int), true \/*is_signed*\/);\n+  lea(tmp2, Address(alloc_temp, tmp2));\n+  \/\/ call_VM_leaf, clobbers a few regs, save restore new obj\n+  push(obj);\n+  flat_field_copy(IS_DEST_UNINITIALIZED, tmp2, dst_temp, r8);\n+  pop(obj);\n+  jmp(done);\n+\n+  bind(alloc_failed);\n+  pop(obj);\n+  bind(slow_path);\n+  call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::read_flat_field),\n+          obj, entry);\n+  get_vm_result_oop(obj);\n+  bind(done);\n+}\n+\n+void InterpreterMacroAssembler::write_flat_field(Register entry, Register tmp1, Register tmp2,\n+                                                 Register obj, Register off, Register value) {\n+  assert_different_registers(entry, tmp1, tmp2, obj, off, value);\n+\n+  Label slow_path, done;\n+\n+  load_unsigned_byte(tmp2, Address(entry, in_bytes(ResolvedFieldEntry::flags_offset())));\n+  test_field_is_not_null_free_inline_type(tmp2, tmp1, slow_path);\n+\n+  null_check(value); \/\/ FIXME JDK-8341120\n+\n+  lea(obj, Address(obj, off, Address::times_1));\n+\n+  load_klass(tmp2, value, tmp1);\n+  payload_addr(value, value, tmp2);\n+\n+  Register idx = tmp1;\n+  load_unsigned_short(idx, Address(entry, in_bytes(ResolvedFieldEntry::field_index_offset())));\n+  movptr(tmp2, Address(entry, in_bytes(ResolvedFieldEntry::field_holder_offset())));\n+\n+  Register layout_info = off;\n+  inline_layout_info(tmp2, idx, layout_info);\n+\n+  flat_field_copy(IN_HEAP, value, obj, layout_info);\n+  jmp(done);\n+\n+  bind(slow_path);\n+  call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::write_flat_field), obj, value, entry);\n+\n+  bind(done);\n+}\n@@ -1342,1 +1486,1 @@\n-void InterpreterMacroAssembler::profile_not_taken_branch(Register mdp) {\n+void InterpreterMacroAssembler::profile_not_taken_branch(Register mdp, bool acmp) {\n@@ -1354,1 +1498,1 @@\n-    update_mdp_by_constant(mdp, in_bytes(BranchData::branch_data_size()));\n+    update_mdp_by_constant(mdp, acmp ? in_bytes(ACmpData::acmp_data_size()): in_bytes(BranchData::branch_data_size()));\n@@ -1417,1 +1561,1 @@\n-    record_klass_in_profile(receiver, mdp, reg2, true);\n+    record_klass_in_profile(receiver, mdp, reg2);\n@@ -1437,4 +1581,3 @@\n-void InterpreterMacroAssembler::record_klass_in_profile_helper(\n-                                        Register receiver, Register mdp,\n-                                        Register reg2, int start_row,\n-                                        Label& done, bool is_virtual_call) {\n+void InterpreterMacroAssembler::record_klass_in_profile_helper(Register receiver, Register mdp,\n+                                                               Register reg2, int start_row,\n+                                                               Label& done) {\n@@ -1544,3 +1687,1 @@\n-void InterpreterMacroAssembler::record_klass_in_profile(Register receiver,\n-                                                        Register mdp, Register reg2,\n-                                                        bool is_virtual_call) {\n+void InterpreterMacroAssembler::record_klass_in_profile(Register receiver, Register mdp, Register reg2) {\n@@ -1550,1 +1691,1 @@\n-  record_klass_in_profile_helper(receiver, mdp, reg2, 0, done, is_virtual_call);\n+  record_klass_in_profile_helper(receiver, mdp, reg2, 0, done);\n@@ -1627,1 +1768,1 @@\n-      record_klass_in_profile(klass, mdp, reg2, false);\n+      record_klass_in_profile(klass, mdp, reg2);\n@@ -1687,0 +1828,114 @@\n+template <class ArrayData> void InterpreterMacroAssembler::profile_array_type(Register mdp,\n+                                                                              Register array,\n+                                                                              Register tmp) {\n+  if (ProfileInterpreter) {\n+    Label profile_continue;\n+\n+    \/\/ If no method data exists, go to profile_continue.\n+    test_method_data_pointer(mdp, profile_continue);\n+\n+    mov(tmp, array);\n+    profile_obj_type(tmp, Address(mdp, in_bytes(ArrayData::array_offset())));\n+\n+    Label not_flat;\n+    test_non_flat_array_oop(array, tmp, not_flat);\n+\n+    set_mdp_flag_at(mdp, ArrayData::flat_array_byte_constant());\n+\n+    bind(not_flat);\n+\n+    Label not_null_free;\n+    test_non_null_free_array_oop(array, tmp, not_null_free);\n+\n+    set_mdp_flag_at(mdp, ArrayData::null_free_array_byte_constant());\n+\n+    bind(not_null_free);\n+\n+    bind(profile_continue);\n+  }\n+}\n+\n+template void InterpreterMacroAssembler::profile_array_type<ArrayLoadData>(Register mdp,\n+                                                                           Register array,\n+                                                                           Register tmp);\n+template void InterpreterMacroAssembler::profile_array_type<ArrayStoreData>(Register mdp,\n+                                                                            Register array,\n+                                                                            Register tmp);\n+\n+\n+void InterpreterMacroAssembler::profile_multiple_element_types(Register mdp, Register element, Register tmp, const Register tmp2) {\n+  if (ProfileInterpreter) {\n+    Label profile_continue;\n+\n+    \/\/ If no method data exists, go to profile_continue.\n+    test_method_data_pointer(mdp, profile_continue);\n+\n+    Label done, update;\n+    testptr(element, element);\n+    jccb(Assembler::notZero, update);\n+    set_mdp_flag_at(mdp, BitData::null_seen_byte_constant());\n+    jmp(done);\n+\n+    bind(update);\n+    load_klass(tmp, element, rscratch1);\n+\n+    \/\/ Record the object type.\n+    record_klass_in_profile(tmp, mdp, tmp2);\n+\n+    bind(done);\n+\n+    \/\/ The method data pointer needs to be updated.\n+    update_mdp_by_constant(mdp, in_bytes(ArrayStoreData::array_store_data_size()));\n+\n+    bind(profile_continue);\n+  }\n+}\n+\n+void InterpreterMacroAssembler::profile_element_type(Register mdp,\n+                                                     Register element,\n+                                                     Register tmp) {\n+  if (ProfileInterpreter) {\n+    Label profile_continue;\n+\n+    \/\/ If no method data exists, go to profile_continue.\n+    test_method_data_pointer(mdp, profile_continue);\n+\n+    mov(tmp, element);\n+    profile_obj_type(tmp, Address(mdp, in_bytes(ArrayLoadData::element_offset())));\n+\n+    \/\/ The method data pointer needs to be updated.\n+    update_mdp_by_constant(mdp, in_bytes(ArrayLoadData::array_load_data_size()));\n+\n+    bind(profile_continue);\n+  }\n+}\n+\n+void InterpreterMacroAssembler::profile_acmp(Register mdp,\n+                                             Register left,\n+                                             Register right,\n+                                             Register tmp) {\n+  if (ProfileInterpreter) {\n+    Label profile_continue;\n+\n+    \/\/ If no method data exists, go to profile_continue.\n+    test_method_data_pointer(mdp, profile_continue);\n+\n+    mov(tmp, left);\n+    profile_obj_type(tmp, Address(mdp, in_bytes(ACmpData::left_offset())));\n+\n+    Label left_not_inline_type;\n+    test_oop_is_not_inline_type(left, tmp, left_not_inline_type);\n+    set_mdp_flag_at(mdp, ACmpData::left_inline_type_byte_constant());\n+    bind(left_not_inline_type);\n+\n+    mov(tmp, right);\n+    profile_obj_type(tmp, Address(mdp, in_bytes(ACmpData::right_offset())));\n+\n+    Label right_not_inline_type;\n+    test_oop_is_not_inline_type(right, tmp, right_not_inline_type);\n+    set_mdp_flag_at(mdp, ACmpData::right_inline_type_byte_constant());\n+    bind(right_not_inline_type);\n+\n+    bind(profile_continue);\n+  }\n+}\n","filename":"src\/hotspot\/cpu\/x86\/interp_masm_x86.cpp","additions":276,"deletions":21,"binary":false,"changes":297,"status":"modified"},{"patch":"@@ -178,1 +178,1 @@\n-  void gen_subtype_check( Register sub_klass, Label &ok_is_subtype );\n+  void gen_subtype_check(Register sub_klass, Label &ok_is_subtype, bool profile = true);\n@@ -218,0 +218,17 @@\n+  \/\/ Kills t1 and t2, preserves klass, return allocation in new_obj\n+  void allocate_instance(Register klass, Register new_obj,\n+                         Register t1, Register t2,\n+                         bool clear_fields, Label& alloc_failed);\n+\n+  \/\/ Allocate instance in \"obj\" and read in the content of the inline field\n+  \/\/ NOTES:\n+  \/\/   - input holder object via \"obj\", which must be rax,\n+  \/\/     will return new instance via the same reg\n+  \/\/   - assumes holder_klass and valueKlass field klass have both been resolved\n+  void read_flat_field(Register entry,\n+                       Register tmp1, Register tmp2,\n+                       Register obj = rax);\n+  void write_flat_field(Register entry,\n+                        Register tmp1, Register tmp2,\n+                        Register obj, Register off, Register value);\n+\n@@ -237,5 +254,2 @@\n-  void record_klass_in_profile(Register receiver, Register mdp,\n-                               Register reg2, bool is_virtual_call);\n-  void record_klass_in_profile_helper(Register receiver, Register mdp,\n-                                      Register reg2, int start_row,\n-                                      Label& done, bool is_virtual_call);\n+  void record_klass_in_profile(Register receiver, Register mdp, Register reg2);\n+  void record_klass_in_profile_helper(Register receiver, Register mdp, Register reg2, int start_row, Label &done);\n@@ -253,1 +267,1 @@\n-  void profile_not_taken_branch(Register mdp);\n+  void profile_not_taken_branch(Register mdp, bool acmp = false);\n@@ -266,0 +280,5 @@\n+  template <class ArrayData> void profile_array_type(Register mdp, Register array, Register tmp);\n+\n+  void profile_multiple_element_types(Register mdp, Register element, Register tmp, const Register tmp2);\n+  void profile_element_type(Register mdp, Register element, Register tmp);\n+  void profile_acmp(Register mdp, Register left, Register right, Register tmp);\n","filename":"src\/hotspot\/cpu\/x86\/interp_masm_x86.hpp","additions":26,"deletions":7,"binary":false,"changes":33,"status":"modified"},{"patch":"@@ -197,1 +197,5 @@\n-  const ByteSize entry_offset = for_compiler_entry ? Method::from_compiled_offset() :\n+  \/\/ The following jump might pass an inline type argument that was erased to Object as oop to a\n+  \/\/ callee that expects inline type arguments to be passed as fields. We need to call the compiled\n+  \/\/ value entry (_code->inline_entry_point() or _adapter->c2i_inline_entry()) which will take care\n+  \/\/ of translating between the calling conventions.\n+  const ByteSize entry_offset = for_compiler_entry ? Method::from_compiled_inline_offset() :\n","filename":"src\/hotspot\/cpu\/x86\/methodHandles_x86.cpp","additions":5,"deletions":1,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -37,2 +37,24 @@\n-  int argsize = is_compiled() ? (_cb->as_nmethod()->num_stack_arg_slots() * VMRegImpl::stack_slot_size) >> LogBytesPerWord : 0;\n-  int frame_size = _cb->frame_size() + argsize;\n+  int frame_size = _cb->frame_size();\n+  if (is_compiled()) {\n+    nmethod* nm = _cb->as_nmethod_or_null();\n+    if (nm->needs_stack_repair() && nm->is_compiled_by_c2()) {\n+      frame f = to_frame();\n+      bool augmented = f.was_augmented_on_entry(frame_size);\n+      if (!augmented) {\n+        \/\/ Fix: C2 caller, so frame was not extended and thus the\n+        \/\/ size read from the frame does not include the arguments.\n+        \/\/ Ideally we have to count the arg size for the scalarized\n+        \/\/ convention. For now we include the size of the caller frame\n+        \/\/ which would at least be equal to that.\n+        RegisterMap map(nullptr,\n+                        RegisterMap::UpdateMap::skip,\n+                        RegisterMap::ProcessFrames::skip,\n+                        RegisterMap::WalkContinuation::skip);\n+        frame caller = to_frame().sender(&map);\n+        assert(caller.is_compiled_frame() && caller.cb()->as_nmethod()->is_compiled_by_c2(), \"needs stack repair but was not extended with c1\/interpreter caller\");\n+        frame_size += (caller.real_fp() - caller.sp());\n+      }\n+    } else {\n+      frame_size += _cb->as_nmethod()->num_stack_arg_slots() * VMRegImpl::stack_slot_size >> LogBytesPerWord;\n+    }\n+  }\n@@ -55,1 +77,1 @@\n-  return *(address*)(_sp - 1);\n+  return *(address*)((_callee_augmented ? _unextended_sp : _sp) - 1);\n","filename":"src\/hotspot\/cpu\/x86\/stackChunkFrameStream_x86.inline.hpp","additions":25,"deletions":3,"binary":false,"changes":28,"status":"modified"},{"patch":"@@ -39,0 +39,1 @@\n+#include \"oops\/inlineKlass.hpp\"\n@@ -170,0 +171,1 @@\n+  case Bytecodes::_fast_vputfield:\n@@ -778,9 +780,28 @@\n-  \/\/ rax: index\n-  \/\/ rdx: array\n-  index_check(rdx, rax); \/\/ kills rbx\n-  do_oop_load(_masm,\n-              Address(rdx, rax,\n-                      UseCompressedOops ? Address::times_4 : Address::times_ptr,\n-                      arrayOopDesc::base_offset_in_bytes(T_OBJECT)),\n-              rax,\n-              IS_ARRAY);\n+  Register array = rdx;\n+  Register index = rax;\n+\n+  index_check(array, index); \/\/ kills rbx\n+  __ profile_array_type<ArrayLoadData>(rbx, array, rcx);\n+  if (UseArrayFlattening) {\n+    Label is_flat_array, done;\n+    __ test_flat_array_oop(array, rbx, is_flat_array);\n+    do_oop_load(_masm,\n+                Address(array, index,\n+                        UseCompressedOops ? Address::times_4 : Address::times_ptr,\n+                        arrayOopDesc::base_offset_in_bytes(T_OBJECT)),\n+                rax,\n+                IS_ARRAY);\n+    __ jmp(done);\n+    __ bind(is_flat_array);\n+    __ movptr(rcx, array);\n+    call_VM(rax, CAST_FROM_FN_PTR(address, InterpreterRuntime::flat_array_load), rcx, index);\n+    __ bind(done);\n+  } else {\n+    do_oop_load(_masm,\n+                Address(array, index,\n+                        UseCompressedOops ? Address::times_4 : Address::times_ptr,\n+                        arrayOopDesc::base_offset_in_bytes(T_OBJECT)),\n+                rax,\n+                IS_ARRAY);\n+  }\n+  __ profile_element_type(rbx, rax, rcx);\n@@ -1060,1 +1081,1 @@\n-  Label is_null, ok_is_subtype, done;\n+  Label is_null, is_flat_array, ok_is_subtype, done;\n@@ -1072,0 +1093,4 @@\n+\n+  __ profile_array_type<ArrayStoreData>(rdi, rdx, rbx);\n+  __ profile_multiple_element_types(rdi, rax, rbx, rcx);\n+\n@@ -1075,0 +1100,7 @@\n+  \/\/ Move array class to rdi\n+  __ load_klass(rdi, rdx, rscratch1);\n+  if (UseArrayFlattening) {\n+    __ movl(rbx, Address(rdi, Klass::layout_helper_offset()));\n+    __ test_flat_array_layout(rbx, is_flat_array);\n+  }\n+\n@@ -1077,3 +1109,2 @@\n-  \/\/ Move superklass into rax\n-  __ load_klass(rax, rdx, rscratch1);\n-  __ movptr(rax, Address(rax,\n+  \/\/ Move array element superklass into rax\n+  __ movptr(rax, Address(rdi,\n@@ -1084,1 +1115,2 @@\n-  __ gen_subtype_check(rbx, ok_is_subtype);\n+  \/\/ is \"rbx <: rax\" ? (value subclass <: array element superclass)\n+  __ gen_subtype_check(rbx, ok_is_subtype, false);\n@@ -1102,1 +1134,13 @@\n-  __ profile_null_seen(rbx);\n+  if (EnableValhalla) {\n+    Label write_null_to_null_free_array, store_null;\n+\n+      \/\/ Move array class to rdi\n+    __ load_klass(rdi, rdx, rscratch1);\n+    if (UseArrayFlattening) {\n+      __ movl(rbx, Address(rdi, Klass::layout_helper_offset()));\n+      __ test_flat_array_layout(rbx, is_flat_array);\n+    }\n+\n+    \/\/ No way to store null in null-free array\n+    __ test_null_free_array_oop(rdx, rbx, write_null_to_null_free_array);\n+    __ jmp(store_null);\n@@ -1104,0 +1148,5 @@\n+    __ bind(write_null_to_null_free_array);\n+    __ jump(RuntimeAddress(Interpreter::_throw_NullPointerException_entry));\n+\n+    __ bind(store_null);\n+  }\n@@ -1106,0 +1155,5 @@\n+  __ jmp(done);\n+\n+  if (UseArrayFlattening) {\n+    Label is_type_ok;\n+    __ bind(is_flat_array); \/\/ Store non-null value to flat\n@@ -1107,0 +1161,6 @@\n+    __ movptr(rax, at_tos());\n+    __ movl(rcx, at_tos_p1()); \/\/ index\n+    __ movptr(rdx, at_tos_p2()); \/\/ array\n+\n+    call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::flat_array_store), rax, rdx, rcx);\n+  }\n@@ -1894,1 +1954,1 @@\n-  Label not_taken;\n+  Label taken, not_taken;\n@@ -1896,0 +1956,36 @@\n+\n+  __ profile_acmp(rbx, rdx, rax, rcx);\n+\n+  const int is_inline_type_mask = markWord::inline_type_pattern;\n+  if (EnableValhalla) {\n+    __ cmpoop(rdx, rax);\n+    __ jcc(Assembler::equal, (cc == equal) ? taken : not_taken);\n+\n+    \/\/ might be substitutable, test if either rax or rdx is null\n+    __ testptr(rax, rax);\n+    __ jcc(Assembler::zero, (cc == equal) ? not_taken : taken);\n+    __ testptr(rdx, rdx);\n+    __ jcc(Assembler::zero, (cc == equal) ? not_taken : taken);\n+\n+    \/\/ and both are values ?\n+    __ movptr(rbx, Address(rdx, oopDesc::mark_offset_in_bytes()));\n+    __ andptr(rbx, Address(rax, oopDesc::mark_offset_in_bytes()));\n+    __ andptr(rbx, is_inline_type_mask);\n+    __ cmpptr(rbx, is_inline_type_mask);\n+    __ jcc(Assembler::notEqual, (cc == equal) ? not_taken : taken);\n+\n+    \/\/ same value klass ?\n+    __ load_metadata(rbx, rdx);\n+    __ load_metadata(rcx, rax);\n+    __ cmpptr(rbx, rcx);\n+    __ jcc(Assembler::notEqual, (cc == equal) ? not_taken : taken);\n+\n+    \/\/ Know both are the same type, let's test for substitutability...\n+    if (cc == equal) {\n+      invoke_is_substitutable(rax, rdx, taken, not_taken);\n+    } else {\n+      invoke_is_substitutable(rax, rdx, not_taken, taken);\n+    }\n+    __ stop(\"Not reachable\");\n+  }\n+\n@@ -1898,0 +1994,1 @@\n+  __ bind(taken);\n@@ -1900,1 +1997,10 @@\n-  __ profile_not_taken_branch(rax);\n+  __ profile_not_taken_branch(rax, true);\n+}\n+\n+void TemplateTable::invoke_is_substitutable(Register aobj, Register bobj,\n+                                            Label& is_subst, Label& not_subst) {\n+  __ call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::is_substitutable), aobj, bobj);\n+  \/\/ Restored...rax answer, jmp to outcome...\n+  __ testl(rax, rax);\n+  __ jcc(Assembler::zero, not_subst);\n+  __ jmp(is_subst);\n@@ -2154,1 +2260,2 @@\n-  __ remove_activation(state, rbcp);\n+\n+  __ remove_activation(state, rbcp, true, true, true);\n@@ -2526,1 +2633,1 @@\n-  const Register obj   = c_rarg3;\n+  const Register obj   = r9;\n@@ -2538,2 +2645,0 @@\n-  if (!is_static) pop_and_check_object(obj);\n-\n@@ -2542,1 +2647,1 @@\n-  Label Done, notByte, notBool, notInt, notShort, notChar, notLong, notFloat, notObj;\n+  Label Done, notByte, notBool, notInt, notShort, notChar, notLong, notFloat, notObj, notInlineType;\n@@ -2550,0 +2655,1 @@\n+  if (!is_static) pop_and_check_object(obj);\n@@ -2561,1 +2667,1 @@\n-\n+   if (!is_static) pop_and_check_object(obj);\n@@ -2576,4 +2682,35 @@\n-  do_oop_load(_masm, field, rax);\n-  __ push(atos);\n-  if (!is_static && rc == may_rewrite) {\n-    patch_bytecode(Bytecodes::_fast_agetfield, bc, rbx);\n+  if (!EnableValhalla) {\n+    if (!is_static) pop_and_check_object(obj);\n+    do_oop_load(_masm, field, rax);\n+    __ push(atos);\n+    if (!is_static && rc == may_rewrite) {\n+      patch_bytecode(Bytecodes::_fast_agetfield, bc, rbx);\n+    }\n+    __ jmp(Done);\n+  } else {\n+    if (is_static) {\n+      __ load_heap_oop(rax, field);\n+      __ push(atos);\n+      __ jmp(Done);\n+    } else {\n+      Label is_flat, rewrite_inline;\n+      __ test_field_is_flat(flags, rscratch1, is_flat);\n+      pop_and_check_object(obj);\n+      __ load_heap_oop(rax, field);\n+      __ push(atos);\n+      if (rc == may_rewrite) {\n+        patch_bytecode(Bytecodes::_fast_agetfield, bc, rbx);\n+      }\n+      __ jmp(Done);\n+      __ bind(is_flat);\n+      \/\/ field is flat (null-free or nullable with a null-marker)\n+      pop_and_check_object(rax);\n+      __ read_flat_field(rcx, rdx, rbx, rax);\n+      __ verify_oop(rax);\n+      __ push(atos);\n+      __ bind(rewrite_inline);\n+      if (rc == may_rewrite) {\n+        patch_bytecode(Bytecodes::_fast_vgetfield, bc, rbx);\n+      }\n+      __ jmp(Done);\n+    }\n@@ -2581,1 +2718,0 @@\n-  __ jmp(Done);\n@@ -2584,0 +2720,3 @@\n+\n+  if (!is_static) pop_and_check_object(obj);\n+\n@@ -2683,1 +2822,0 @@\n-\n@@ -2745,1 +2883,1 @@\n-  const Register flags = rax;\n+  const Register flags = r9;\n@@ -2758,2 +2896,3 @@\n-  __ andl(flags, (1 << ResolvedFieldEntry::is_volatile_shift));\n-  __ testl(flags, flags);\n+  __ movl(rscratch1, flags);\n+  __ andl(rscratch1, (1 << ResolvedFieldEntry::is_volatile_shift));\n+  __ testl(rscratch1, rscratch1);\n@@ -2762,1 +2901,1 @@\n-  putfield_or_static_helper(byte_no, is_static, rc, obj, off, tos_state);\n+  putfield_or_static_helper(byte_no, is_static, rc, obj, off, tos_state, flags);\n@@ -2768,1 +2907,1 @@\n-  putfield_or_static_helper(byte_no, is_static, rc, obj, off, tos_state);\n+  putfield_or_static_helper(byte_no, is_static, rc, obj, off, tos_state, flags);\n@@ -2774,1 +2913,1 @@\n-                                              Register obj, Register off, Register tos_state) {\n+                                              Register obj, Register off, Register tos_state, Register flags) {\n@@ -2780,1 +2919,1 @@\n-        notLong, notFloat, notObj;\n+        notLong, notFloat, notObj, notInlineType;\n@@ -2821,6 +2960,44 @@\n-    __ pop(atos);\n-    if (!is_static) pop_and_check_object(obj);\n-    \/\/ Store into the field\n-    do_oop_store(_masm, field, rax);\n-    if (!is_static && rc == may_rewrite) {\n-      patch_bytecode(Bytecodes::_fast_aputfield, bc, rbx, true, byte_no);\n+    if (!EnableValhalla) {\n+      __ pop(atos);\n+      if (!is_static) pop_and_check_object(obj);\n+      \/\/ Store into the field\n+      do_oop_store(_masm, field, rax);\n+      if (!is_static && rc == may_rewrite) {\n+        patch_bytecode(Bytecodes::_fast_aputfield, bc, rbx, true, byte_no);\n+      }\n+      __ jmp(Done);\n+    } else {\n+      __ pop(atos);\n+      if (is_static) {\n+        Label is_nullable;\n+        __ test_field_is_not_null_free_inline_type(flags, rscratch1, is_nullable);\n+        __ null_check(rax);  \/\/ FIXME JDK-8341120\n+        __ bind(is_nullable);\n+        do_oop_store(_masm, field, rax);\n+        __ jmp(Done);\n+      } else {\n+        Label is_flat, null_free_reference, rewrite_inline;\n+        __ test_field_is_flat(flags, rscratch1, is_flat);\n+        __ test_field_is_null_free_inline_type(flags, rscratch1, null_free_reference);\n+        pop_and_check_object(obj);\n+        \/\/ Store into the field\n+        do_oop_store(_masm, field, rax);\n+        if (rc == may_rewrite) {\n+          patch_bytecode(Bytecodes::_fast_aputfield, bc, rbx, true, byte_no);\n+        }\n+        __ jmp(Done);\n+        __ bind(null_free_reference);\n+        __ null_check(rax);  \/\/ FIXME JDK-8341120\n+        pop_and_check_object(obj);\n+        \/\/ Store into the field\n+        do_oop_store(_masm, field, rax);\n+        __ jmp(rewrite_inline);\n+        __ bind(is_flat);\n+        pop_and_check_object(rscratch2);\n+        __ write_flat_field(rcx, r8, rscratch1, rscratch2, rbx, rax);\n+        __ bind(rewrite_inline);\n+        if (rc == may_rewrite) {\n+          patch_bytecode(Bytecodes::_fast_vputfield, bc, rbx, true, byte_no);\n+        }\n+        __ jmp(Done);\n+      }\n@@ -2828,1 +3005,0 @@\n-    __ jmp(Done);\n@@ -2965,0 +3141,1 @@\n+    case Bytecodes::_fast_vputfield: \/\/fall through\n@@ -2988,0 +3165,1 @@\n+    case Bytecodes::_fast_vputfield: \/\/ fall through\n@@ -3006,2 +3184,0 @@\n-  Register cache = rcx;\n-\n@@ -3014,3 +3190,1 @@\n-  load_resolved_field_entry(noreg, cache, rax, rbx, rdx);\n-  \/\/ RBX: field offset, RAX: TOS, RDX: flags\n-  __ andl(rdx, (1 << ResolvedFieldEntry::is_volatile_shift));\n+  load_resolved_field_entry(noreg, rcx, rax, rbx, rdx);\n@@ -3018,0 +3192,1 @@\n+  \/\/ RBX: field offset, RCX: RAX: TOS, RDX: flags\n@@ -3026,1 +3201,3 @@\n-  __ testl(rdx, rdx);\n+  __ movl(rscratch2, rdx);  \/\/ saving flags for is_flat test\n+  __ andl(rscratch2, (1 << ResolvedFieldEntry::is_volatile_shift));\n+  __ testl(rscratch2, rscratch2);\n@@ -3029,1 +3206,1 @@\n-  fast_storefield_helper(field, rax);\n+  fast_storefield_helper(field, rax, rdx);\n@@ -3035,1 +3212,1 @@\n-  fast_storefield_helper(field, rax);\n+  fast_storefield_helper(field, rax, rdx);\n@@ -3040,1 +3217,3 @@\n-void TemplateTable::fast_storefield_helper(Address field, Register rax) {\n+void TemplateTable::fast_storefield_helper(Address field, Register rax, Register flags) {\n+\n+  \/\/ DANGER: 'field' argument depends on rcx and rbx\n@@ -3044,0 +3223,15 @@\n+  case Bytecodes::_fast_vputfield:\n+    {\n+      \/\/ Field is either flat (nullable or not) or non-flat and null-free\n+      Label is_flat, done;\n+      __ test_field_is_flat(flags, rscratch1, is_flat);\n+      __ null_check(rax);  \/\/ FIXME JDK-8341120\n+      do_oop_store(_masm, field, rax);\n+      __ jmp(done);\n+      __ bind(is_flat);\n+      __ load_field_entry(r8, r9);\n+      __ movptr(rscratch2, rcx);  \/\/ re-shuffle registers because of VM call calling convention\n+      __ write_flat_field(r8, rscratch1, r9, rscratch2, rbx, rax);\n+      __ bind(done);\n+    }\n+    break;\n@@ -3045,1 +3239,3 @@\n-    do_oop_store(_masm, field, rax);\n+    {\n+      do_oop_store(_masm, field, rax);\n+    }\n@@ -3101,1 +3297,1 @@\n-  __ load_sized_value(rbx, Address(rcx, in_bytes(ResolvedFieldEntry::field_offset_offset())), sizeof(int), true \/*is_signed*\/);\n+  __ load_sized_value(rdx, Address(rcx, in_bytes(ResolvedFieldEntry::field_offset_offset())), sizeof(int), true \/*is_signed*\/);\n@@ -3106,1 +3302,1 @@\n-  Address field(rax, rbx, Address::times_1);\n+  Address field(rax, rdx, Address::times_1);\n@@ -3110,0 +3306,4 @@\n+  case Bytecodes::_fast_vgetfield:\n+    __ read_flat_field(rcx, rdx, rbx, rax);\n+    __ verify_oop(rax);\n+    break;\n@@ -3534,2 +3734,0 @@\n-  Label slow_case_no_pop;\n-  Label initialize_header;\n@@ -3545,1 +3743,1 @@\n-  __ jcc(Assembler::notEqual, slow_case_no_pop);\n+  __ jcc(Assembler::notEqual, slow_case);\n@@ -3549,1 +3747,0 @@\n-  __ push(rcx);  \/\/ save the contexts of klass for initializing the header\n@@ -3556,83 +3753,2 @@\n-  \/\/ get instance_size in InstanceKlass (scaled to a count of bytes)\n-  __ movl(rdx, Address(rcx, Klass::layout_helper_offset()));\n-  \/\/ test to see if it is malformed in some way\n-  __ testl(rdx, Klass::_lh_instance_slow_path_bit);\n-  __ jcc(Assembler::notZero, slow_case);\n-\n-  \/\/ Allocate the instance:\n-  \/\/  If TLAB is enabled:\n-  \/\/    Try to allocate in the TLAB.\n-  \/\/    If fails, go to the slow path.\n-  \/\/    Initialize the allocation.\n-  \/\/    Exit.\n-  \/\/\n-  \/\/  Go to slow path.\n-\n-  if (UseTLAB) {\n-    __ tlab_allocate(rax, rdx, 0, rcx, rbx, slow_case);\n-    if (ZeroTLAB) {\n-      \/\/ the fields have been already cleared\n-      __ jmp(initialize_header);\n-    }\n-\n-    \/\/ The object is initialized before the header.  If the object size is\n-    \/\/ zero, go directly to the header initialization.\n-    if (UseCompactObjectHeaders) {\n-      assert(is_aligned(oopDesc::base_offset_in_bytes(), BytesPerLong), \"oop base offset must be 8-byte-aligned\");\n-      __ decrement(rdx, oopDesc::base_offset_in_bytes());\n-    } else {\n-      __ decrement(rdx, sizeof(oopDesc));\n-    }\n-    __ jcc(Assembler::zero, initialize_header);\n-\n-    \/\/ Initialize topmost object field, divide rdx by 8, check if odd and\n-    \/\/ test if zero.\n-    __ xorl(rcx, rcx);    \/\/ use zero reg to clear memory (shorter code)\n-    __ shrl(rdx, LogBytesPerLong); \/\/ divide by 2*oopSize and set carry flag if odd\n-\n-    \/\/ rdx must have been multiple of 8\n-#ifdef ASSERT\n-    \/\/ make sure rdx was multiple of 8\n-    Label L;\n-    \/\/ Ignore partial flag stall after shrl() since it is debug VM\n-    __ jcc(Assembler::carryClear, L);\n-    __ stop(\"object size is not multiple of 2 - adjust this code\");\n-    __ bind(L);\n-    \/\/ rdx must be > 0, no extra check needed here\n-#endif\n-\n-    \/\/ initialize remaining object fields: rdx was a multiple of 8\n-    { Label loop;\n-    __ bind(loop);\n-    int header_size_bytes = oopDesc::header_size() * HeapWordSize;\n-    assert(is_aligned(header_size_bytes, BytesPerLong), \"oop header size must be 8-byte-aligned\");\n-    __ movptr(Address(rax, rdx, Address::times_8, header_size_bytes - 1*oopSize), rcx);\n-    __ decrement(rdx);\n-    __ jcc(Assembler::notZero, loop);\n-    }\n-\n-    \/\/ initialize object header only.\n-    __ bind(initialize_header);\n-    if (UseCompactObjectHeaders) {\n-      __ pop(rcx);   \/\/ get saved klass back in the register.\n-      __ movptr(rbx, Address(rcx, Klass::prototype_header_offset()));\n-      __ movptr(Address(rax, oopDesc::mark_offset_in_bytes()), rbx);\n-    } else {\n-      __ movptr(Address(rax, oopDesc::mark_offset_in_bytes()),\n-                (intptr_t)markWord::prototype().value()); \/\/ header\n-      __ pop(rcx);   \/\/ get saved klass back in the register.\n-      __ xorl(rsi, rsi); \/\/ use zero reg to clear memory (shorter code)\n-      __ store_klass_gap(rax, rsi);  \/\/ zero klass gap for compressed oops\n-      __ store_klass(rax, rcx, rscratch1);  \/\/ klass\n-    }\n-\n-    if (DTraceAllocProbes) {\n-      \/\/ Trigger dtrace event for fastpath\n-      __ push(atos);\n-      __ call_VM_leaf(\n-           CAST_FROM_FN_PTR(address, static_cast<int (*)(oopDesc*)>(SharedRuntime::dtrace_object_alloc)), rax);\n-      __ pop(atos);\n-    }\n-\n-    __ jmp(done);\n-  }\n+  __ allocate_instance(rcx, rax, rdx, rbx, true, slow_case);\n+  __ jmp(done);\n@@ -3642,2 +3758,0 @@\n-  __ pop(rcx);   \/\/ restore stack pointer to what it was when we came in.\n-  __ bind(slow_case_no_pop);\n@@ -3685,4 +3799,4 @@\n-  __ cmpb(Address(rdx, rbx,\n-                  Address::times_1,\n-                  Array<u1>::base_offset_in_bytes()),\n-          JVM_CONSTANT_Class);\n+  __ movzbl(rdx, Address(rdx, rbx,\n+      Address::times_1,\n+      Array<u1>::base_offset_in_bytes()));\n+  __ cmpl(rdx, JVM_CONSTANT_Class);\n@@ -3718,0 +3832,3 @@\n+  __ jmp(done);\n+\n+  __ bind(is_null);\n@@ -3721,4 +3838,1 @@\n-    __ jmp(done);\n-    __ bind(is_null);\n-  } else {\n-    __ bind(is_null);   \/\/ same as 'done'\n+\n@@ -3740,4 +3854,4 @@\n-  __ cmpb(Address(rdx, rbx,\n-                  Address::times_1,\n-                  Array<u1>::base_offset_in_bytes()),\n-          JVM_CONSTANT_Class);\n+  __ movzbl(rdx, Address(rdx, rbx,\n+        Address::times_1,\n+        Array<u1>::base_offset_in_bytes()));\n+  __ cmpl(rdx, JVM_CONSTANT_Class);\n@@ -3787,1 +3901,0 @@\n-\n@@ -3847,0 +3960,4 @@\n+  Label is_inline_type;\n+  __ movptr(rbx, Address(rax, oopDesc::mark_offset_in_bytes()));\n+  __ test_markword_is_inline_type(rbx, is_inline_type);\n+\n@@ -3939,0 +4056,5 @@\n+\n+  __ bind(is_inline_type);\n+  __ call_VM(noreg, CAST_FROM_FN_PTR(address,\n+                    InterpreterRuntime::throw_identity_exception), rax);\n+  __ should_not_reach_here();\n@@ -3947,0 +4069,11 @@\n+  const int is_inline_type_mask = markWord::inline_type_pattern;\n+  Label has_identity;\n+  __ movptr(rbx, Address(rax, oopDesc::mark_offset_in_bytes()));\n+  __ andptr(rbx, is_inline_type_mask);\n+  __ cmpl(rbx, is_inline_type_mask);\n+  __ jcc(Assembler::notEqual, has_identity);\n+  __ call_VM(noreg, CAST_FROM_FN_PTR(address,\n+                     InterpreterRuntime::throw_illegal_monitor_state_exception));\n+  __ should_not_reach_here();\n+  __ bind(has_identity);\n+\n","filename":"src\/hotspot\/cpu\/x86\/templateTable_x86.cpp","additions":290,"deletions":157,"binary":false,"changes":447,"status":"modified"},{"patch":"@@ -1795,1 +1795,1 @@\n-  if (!UseFastStosb && UseUnalignedLoadStores) {\n+  if (UseUnalignedLoadStores) {\n","filename":"src\/hotspot\/cpu\/x86\/vm_version_x86.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -1652,0 +1652,4 @@\n+  if (_entry_point == nullptr) {\n+    \/\/ CallLeafNoFPInDirect\n+    return 3; \/\/ callq (register)\n+  }\n@@ -1658,0 +1662,1 @@\n+\n@@ -1882,6 +1887,1 @@\n-  int framesize = C->output()->frame_size_in_bytes();\n-  int bangsize = C->output()->bang_size_in_bytes();\n-\n-  if (C->clinit_barrier_on_entry()) {\n-    assert(VM_Version::supports_fast_class_init_checks(), \"sanity\");\n-    assert(!C->method()->holder()->is_not_initialized(), \"initialization should have been started\");\n+  __ verified_entry(C);\n@@ -1889,9 +1889,2 @@\n-    Label L_skip_barrier;\n-    Register klass = rscratch1;\n-\n-    __ mov_metadata(klass, C->method()->holder()->constant_encoding());\n-    __ clinit_barrier(klass, &L_skip_barrier \/*L_fast_path*\/);\n-\n-    __ jump(RuntimeAddress(SharedRuntime::get_handle_wrong_method_stub())); \/\/ slow path\n-\n-    __ bind(L_skip_barrier);\n+  if (ra_->C->stub_function() == nullptr) {\n+    __ entry_barrier();\n@@ -1900,1 +1893,3 @@\n-  __ verified_entry(framesize, C->output()->need_stack_bang(bangsize)?bangsize:0, false, C->stub_function() != nullptr);\n+  if (!Compile::current()->output()->in_scratch_emit_size()) {\n+    __ bind(*_verified_entry);\n+  }\n@@ -1912,5 +1907,0 @@\n-uint MachPrologNode::size(PhaseRegAlloc* ra_) const\n-{\n-  return MachNode::size(ra_); \/\/ too many variables; just compute it\n-                              \/\/ the hard way\n-}\n@@ -1965,13 +1955,3 @@\n-  int framesize = C->output()->frame_size_in_bytes();\n-  assert((framesize & (StackAlignmentInBytes-1)) == 0, \"frame size not aligned\");\n-  \/\/ Remove word for return adr already pushed\n-  \/\/ and RBP\n-  framesize -= 2*wordSize;\n-\n-  \/\/ Note that VerifyStackAtCalls' Majik cookie does not change the frame size popped here\n-\n-  if (framesize) {\n-    __ addq(rsp, framesize);\n-  }\n-\n-  __ popq(rbp);\n+  \/\/ Subtract two words to account for return address and rbp\n+  int initial_framesize = C->output()->frame_size_in_bytes() - 2*wordSize;\n+  __ remove_frame(initial_framesize, C->needs_stack_repair());\n@@ -1996,6 +1976,0 @@\n-uint MachEpilogNode::size(PhaseRegAlloc* ra_) const\n-{\n-  return MachNode::size(ra_); \/\/ too many variables; just compute it\n-                              \/\/ the hard way\n-}\n-\n@@ -2603,0 +2577,43 @@\n+\/\/=============================================================================\n+#ifndef PRODUCT\n+void MachVEPNode::format(PhaseRegAlloc* ra_, outputStream* st) const\n+{\n+  st->print_cr(\"MachVEPNode\");\n+}\n+#endif\n+\n+void MachVEPNode::emit(C2_MacroAssembler* masm, PhaseRegAlloc* ra_) const\n+{\n+  CodeBuffer* cbuf = masm->code();\n+  uint insts_size = cbuf->insts_size();\n+  if (!_verified) {\n+    __ ic_check(1);\n+  } else {\n+    \/\/ TODO 8284443 Avoid creation of temporary frame\n+    if (ra_->C->stub_function() == nullptr) {\n+      __ verified_entry(ra_->C, 0);\n+      __ entry_barrier();\n+      int initial_framesize = ra_->C->output()->frame_size_in_bytes() - 2*wordSize;\n+      __ remove_frame(initial_framesize, false);\n+    }\n+    \/\/ Unpack inline type args passed as oop and then jump to\n+    \/\/ the verified entry point (skipping the unverified entry).\n+    int sp_inc = __ unpack_inline_args(ra_->C, _receiver_only);\n+    \/\/ Emit code for verified entry and save increment for stack repair on return\n+    __ verified_entry(ra_->C, sp_inc);\n+    if (Compile::current()->output()->in_scratch_emit_size()) {\n+      Label dummy_verified_entry;\n+      __ jmp(dummy_verified_entry);\n+    } else {\n+      __ jmp(*_verified_entry);\n+    }\n+  }\n+  \/* WARNING these NOPs are critical so that verified entry point is properly\n+     4 bytes aligned for patching by NativeJump::patch_verified_entry() *\/\n+  int nops_cnt = 4 - ((cbuf->insts_size() - insts_size) & 0x3);\n+  nops_cnt &= 0x3; \/\/ Do not add nops if code is aligned.\n+  if (nops_cnt > 0) {\n+    __ nop(nops_cnt);\n+  }\n+}\n+\n@@ -2623,6 +2640,0 @@\n-uint MachUEPNode::size(PhaseRegAlloc* ra_) const\n-{\n-  return MachNode::size(ra_); \/\/ too many variables; just compute it\n-                              \/\/ the hard way\n-}\n-\n@@ -4525,0 +4536,33 @@\n+    if (tf()->returns_inline_type_as_fields() && !_method->is_method_handle_intrinsic() && _method->return_type()->is_loaded()) {\n+      \/\/ The last return value is not set by the callee but used to pass the null marker to compiled code.\n+      \/\/ Search for the corresponding projection, get the register and emit code that initialized it.\n+      uint con = (tf()->range_cc()->cnt() - 1);\n+      for (DUIterator_Fast imax, i = fast_outs(imax); i < imax; i++) {\n+        ProjNode* proj = fast_out(i)->as_Proj();\n+        if (proj->_con == con) {\n+          \/\/ Set null marker if rax is non-null (a non-null value is returned buffered or scalarized)\n+          OptoReg::Name optoReg = ra_->get_reg_first(proj);\n+          VMReg reg = OptoReg::as_VMReg(optoReg, ra_->_framesize, OptoReg::reg2stack(ra_->_matcher._new_SP));\n+          Register toReg = reg->is_reg() ? reg->as_Register() : rscratch1;\n+          __ testq(rax, rax);\n+          __ setb(Assembler::notZero, toReg);\n+          __ movzbl(toReg, toReg);\n+          if (reg->is_stack()) {\n+            int st_off = reg->reg2stack() * VMRegImpl::stack_slot_size;\n+            __ movq(Address(rsp, st_off), toReg);\n+          }\n+          break;\n+        }\n+      }\n+      if (return_value_is_used()) {\n+        \/\/ An inline type is returned as fields in multiple registers.\n+        \/\/ Rax either contains an oop if the inline type is buffered or a pointer\n+        \/\/ to the corresponding InlineKlass with the lowest bit set to 1. Zero rax\n+        \/\/ if the lowest bit is set to allow C2 to use the oop after null checking.\n+        \/\/ rax &= (rax & 1) - 1\n+        __ movptr(rscratch1, rax);\n+        __ andptr(rscratch1, 0x1);\n+        __ subptr(rscratch1, 0x1);\n+        __ andptr(rax, rscratch1);\n+      }\n+    }\n@@ -5687,0 +5731,16 @@\n+\/\/ Indirect Narrow Oop Operand\n+operand indCompressedOop(rRegN reg) %{\n+  predicate(UseCompressedOops && (CompressedOops::shift() == Address::times_8));\n+  constraint(ALLOC_IN_RC(ptr_reg));\n+  match(DecodeN reg);\n+\n+  op_cost(10);\n+  format %{\"[R12 + $reg << 3] (compressed oop addressing)\" %}\n+  interface(MEMORY_INTER) %{\n+    base(0xc); \/\/ R12\n+    index($reg);\n+    scale(0x3);\n+    disp(0x0);\n+  %}\n+%}\n+\n@@ -6163,1 +6223,1 @@\n-               indCompressedOopOffset,\n+               indCompressedOop, indCompressedOopOffset,\n@@ -8761,0 +8821,26 @@\n+instruct castI2N(rRegN dst, rRegI src)\n+%{\n+  match(Set dst (CastI2N src));\n+\n+  format %{ \"movq    $dst, $src\\t# int -> narrow ptr\" %}\n+  ins_encode %{\n+    if ($dst$$reg != $src$$reg) {\n+      __ movl($dst$$Register, $src$$Register);\n+    }\n+  %}\n+  ins_pipe(ialu_reg_reg); \/\/ XXX\n+%}\n+\n+instruct castN2X(rRegL dst, rRegN src)\n+%{\n+  match(Set dst (CastP2X src));\n+\n+  format %{ \"movq    $dst, $src\\t# ptr -> long\" %}\n+  ins_encode %{\n+    if ($dst$$reg != $src$$reg) {\n+      __ movptr($dst$$Register, $src$$Register);\n+    }\n+  %}\n+  ins_pipe(ialu_reg_reg); \/\/ XXX\n+%}\n+\n@@ -14984,0 +15070,1 @@\n+\n@@ -14986,1 +15073,1 @@\n-instruct rep_stos(rcx_RegL cnt, rdi_RegP base, regD tmp, rax_RegI zero,\n+instruct rep_stos(rcx_RegL cnt, rdi_RegP base, regD tmp, rax_RegL val,\n@@ -14989,3 +15076,120 @@\n-  predicate(!((ClearArrayNode*)n)->is_large() && (UseAVX <= 2));\n-  match(Set dummy (ClearArray cnt base));\n-  effect(USE_KILL cnt, USE_KILL base, TEMP tmp, KILL zero, KILL cr);\n+  predicate(!((ClearArrayNode*)n)->is_large() && !((ClearArrayNode*)n)->word_copy_only() && (UseAVX <= 2));\n+  match(Set dummy (ClearArray (Binary cnt base) val));\n+  effect(USE_KILL cnt, USE_KILL base, TEMP tmp, USE_KILL val, KILL cr);\n+\n+  format %{ $$template\n+    $$emit$$\"cmp     InitArrayShortSize,rcx\\n\\t\"\n+    $$emit$$\"jg      LARGE\\n\\t\"\n+    $$emit$$\"dec     rcx\\n\\t\"\n+    $$emit$$\"js      DONE\\t# Zero length\\n\\t\"\n+    $$emit$$\"mov     rax,(rdi,rcx,8)\\t# LOOP\\n\\t\"\n+    $$emit$$\"dec     rcx\\n\\t\"\n+    $$emit$$\"jge     LOOP\\n\\t\"\n+    $$emit$$\"jmp     DONE\\n\\t\"\n+    $$emit$$\"# LARGE:\\n\\t\"\n+    if (UseFastStosb) {\n+       $$emit$$\"shlq    rcx,3\\t# Convert doublewords to bytes\\n\\t\"\n+       $$emit$$\"rep     stosb\\t# Store rax to *rdi++ while rcx--\\n\\t\"\n+    } else if (UseXMMForObjInit) {\n+       $$emit$$\"movdq   $tmp, $val\\n\\t\"\n+       $$emit$$\"punpcklqdq $tmp, $tmp\\n\\t\"\n+       $$emit$$\"vinserti128_high $tmp, $tmp\\n\\t\"\n+       $$emit$$\"jmpq    L_zero_64_bytes\\n\\t\"\n+       $$emit$$\"# L_loop:\\t# 64-byte LOOP\\n\\t\"\n+       $$emit$$\"vmovdqu $tmp,(rax)\\n\\t\"\n+       $$emit$$\"vmovdqu $tmp,0x20(rax)\\n\\t\"\n+       $$emit$$\"add     0x40,rax\\n\\t\"\n+       $$emit$$\"# L_zero_64_bytes:\\n\\t\"\n+       $$emit$$\"sub     0x8,rcx\\n\\t\"\n+       $$emit$$\"jge     L_loop\\n\\t\"\n+       $$emit$$\"add     0x4,rcx\\n\\t\"\n+       $$emit$$\"jl      L_tail\\n\\t\"\n+       $$emit$$\"vmovdqu $tmp,(rax)\\n\\t\"\n+       $$emit$$\"add     0x20,rax\\n\\t\"\n+       $$emit$$\"sub     0x4,rcx\\n\\t\"\n+       $$emit$$\"# L_tail:\\t# Clearing tail bytes\\n\\t\"\n+       $$emit$$\"add     0x4,rcx\\n\\t\"\n+       $$emit$$\"jle     L_end\\n\\t\"\n+       $$emit$$\"dec     rcx\\n\\t\"\n+       $$emit$$\"# L_sloop:\\t# 8-byte short loop\\n\\t\"\n+       $$emit$$\"vmovq   xmm0,(rax)\\n\\t\"\n+       $$emit$$\"add     0x8,rax\\n\\t\"\n+       $$emit$$\"dec     rcx\\n\\t\"\n+       $$emit$$\"jge     L_sloop\\n\\t\"\n+       $$emit$$\"# L_end:\\n\\t\"\n+    } else {\n+       $$emit$$\"rep     stosq\\t# Store rax to *rdi++ while rcx--\\n\\t\"\n+    }\n+    $$emit$$\"# DONE\"\n+  %}\n+  ins_encode %{\n+    __ clear_mem($base$$Register, $cnt$$Register, $val$$Register,\n+                 $tmp$$XMMRegister, false, false);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct rep_stos_word_copy(rcx_RegL cnt, rdi_RegP base, regD tmp, rax_RegL val,\n+                            Universe dummy, rFlagsReg cr)\n+%{\n+  predicate(!((ClearArrayNode*)n)->is_large() && ((ClearArrayNode*)n)->word_copy_only() && (UseAVX <= 2));\n+  match(Set dummy (ClearArray (Binary cnt base) val));\n+  effect(USE_KILL cnt, USE_KILL base, TEMP tmp, USE_KILL val, KILL cr);\n+\n+  format %{ $$template\n+    $$emit$$\"cmp     InitArrayShortSize,rcx\\n\\t\"\n+    $$emit$$\"jg      LARGE\\n\\t\"\n+    $$emit$$\"dec     rcx\\n\\t\"\n+    $$emit$$\"js      DONE\\t# Zero length\\n\\t\"\n+    $$emit$$\"mov     rax,(rdi,rcx,8)\\t# LOOP\\n\\t\"\n+    $$emit$$\"dec     rcx\\n\\t\"\n+    $$emit$$\"jge     LOOP\\n\\t\"\n+    $$emit$$\"jmp     DONE\\n\\t\"\n+    $$emit$$\"# LARGE:\\n\\t\"\n+    if (UseXMMForObjInit) {\n+       $$emit$$\"movdq   $tmp, $val\\n\\t\"\n+       $$emit$$\"punpcklqdq $tmp, $tmp\\n\\t\"\n+       $$emit$$\"vinserti128_high $tmp, $tmp\\n\\t\"\n+       $$emit$$\"jmpq    L_zero_64_bytes\\n\\t\"\n+       $$emit$$\"# L_loop:\\t# 64-byte LOOP\\n\\t\"\n+       $$emit$$\"vmovdqu $tmp,(rax)\\n\\t\"\n+       $$emit$$\"vmovdqu $tmp,0x20(rax)\\n\\t\"\n+       $$emit$$\"add     0x40,rax\\n\\t\"\n+       $$emit$$\"# L_zero_64_bytes:\\n\\t\"\n+       $$emit$$\"sub     0x8,rcx\\n\\t\"\n+       $$emit$$\"jge     L_loop\\n\\t\"\n+       $$emit$$\"add     0x4,rcx\\n\\t\"\n+       $$emit$$\"jl      L_tail\\n\\t\"\n+       $$emit$$\"vmovdqu $tmp,(rax)\\n\\t\"\n+       $$emit$$\"add     0x20,rax\\n\\t\"\n+       $$emit$$\"sub     0x4,rcx\\n\\t\"\n+       $$emit$$\"# L_tail:\\t# Clearing tail bytes\\n\\t\"\n+       $$emit$$\"add     0x4,rcx\\n\\t\"\n+       $$emit$$\"jle     L_end\\n\\t\"\n+       $$emit$$\"dec     rcx\\n\\t\"\n+       $$emit$$\"# L_sloop:\\t# 8-byte short loop\\n\\t\"\n+       $$emit$$\"vmovq   xmm0,(rax)\\n\\t\"\n+       $$emit$$\"add     0x8,rax\\n\\t\"\n+       $$emit$$\"dec     rcx\\n\\t\"\n+       $$emit$$\"jge     L_sloop\\n\\t\"\n+       $$emit$$\"# L_end:\\n\\t\"\n+    } else {\n+       $$emit$$\"rep     stosq\\t# Store rax to *rdi++ while rcx--\\n\\t\"\n+    }\n+    $$emit$$\"# DONE\"\n+  %}\n+  ins_encode %{\n+    __ clear_mem($base$$Register, $cnt$$Register, $val$$Register,\n+                 $tmp$$XMMRegister, false, true);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ Small non-constant length ClearArray for AVX512 targets.\n+instruct rep_stos_evex(rcx_RegL cnt, rdi_RegP base, legRegD tmp, kReg ktmp, rax_RegL val,\n+                       Universe dummy, rFlagsReg cr)\n+%{\n+  predicate(!((ClearArrayNode*)n)->is_large() && !((ClearArrayNode*)n)->word_copy_only() && (UseAVX > 2));\n+  match(Set dummy (ClearArray (Binary cnt base) val));\n+  ins_cost(125);\n+  effect(USE_KILL cnt, USE_KILL base, TEMP tmp, TEMP ktmp, USE_KILL val, KILL cr);\n@@ -15039,2 +15243,2 @@\n-    __ clear_mem($base$$Register, $cnt$$Register, $zero$$Register,\n-                 $tmp$$XMMRegister, false, knoreg);\n+    __ clear_mem($base$$Register, $cnt$$Register, $val$$Register,\n+                 $tmp$$XMMRegister, false, false, $ktmp$$KRegister);\n@@ -15045,3 +15249,2 @@\n-\/\/ Small non-constant length ClearArray for AVX512 targets.\n-instruct rep_stos_evex(rcx_RegL cnt, rdi_RegP base, legRegD tmp, kReg ktmp, rax_RegI zero,\n-                       Universe dummy, rFlagsReg cr)\n+instruct rep_stos_evex_word_copy(rcx_RegL cnt, rdi_RegP base, legRegD tmp, kReg ktmp, rax_RegL val,\n+                                 Universe dummy, rFlagsReg cr)\n@@ -15049,2 +15252,2 @@\n-  predicate(!((ClearArrayNode*)n)->is_large() && (UseAVX > 2));\n-  match(Set dummy (ClearArray cnt base));\n+  predicate(!((ClearArrayNode*)n)->is_large() && ((ClearArrayNode*)n)->word_copy_only() && (UseAVX > 2));\n+  match(Set dummy (ClearArray (Binary cnt base) val));\n@@ -15052,1 +15255,1 @@\n-  effect(USE_KILL cnt, USE_KILL base, TEMP tmp, TEMP ktmp, KILL zero, KILL cr);\n+  effect(USE_KILL cnt, USE_KILL base, TEMP tmp, TEMP ktmp, USE_KILL val, KILL cr);\n@@ -15100,2 +15303,2 @@\n-    __ clear_mem($base$$Register, $cnt$$Register, $zero$$Register,\n-                 $tmp$$XMMRegister, false, $ktmp$$KRegister);\n+    __ clear_mem($base$$Register, $cnt$$Register, $val$$Register,\n+                 $tmp$$XMMRegister, false, true, $ktmp$$KRegister);\n@@ -15107,1 +15310,1 @@\n-instruct rep_stos_large(rcx_RegL cnt, rdi_RegP base, regD tmp, rax_RegI zero,\n+instruct rep_stos_large(rcx_RegL cnt, rdi_RegP base, regD tmp, rax_RegL val,\n@@ -15110,3 +15313,99 @@\n-  predicate((UseAVX <=2) && ((ClearArrayNode*)n)->is_large());\n-  match(Set dummy (ClearArray cnt base));\n-  effect(USE_KILL cnt, USE_KILL base, TEMP tmp, KILL zero, KILL cr);\n+  predicate(((ClearArrayNode*)n)->is_large() && !((ClearArrayNode*)n)->word_copy_only() && (UseAVX <= 2));\n+  match(Set dummy (ClearArray (Binary cnt base) val));\n+  effect(USE_KILL cnt, USE_KILL base, TEMP tmp, USE_KILL val, KILL cr);\n+\n+  format %{ $$template\n+    if (UseFastStosb) {\n+       $$emit$$\"shlq    rcx,3\\t# Convert doublewords to bytes\\n\\t\"\n+       $$emit$$\"rep     stosb\\t# Store rax to *rdi++ while rcx--\"\n+    } else if (UseXMMForObjInit) {\n+       $$emit$$\"movdq   $tmp, $val\\n\\t\"\n+       $$emit$$\"punpcklqdq $tmp, $tmp\\n\\t\"\n+       $$emit$$\"vinserti128_high $tmp, $tmp\\n\\t\"\n+       $$emit$$\"jmpq    L_zero_64_bytes\\n\\t\"\n+       $$emit$$\"# L_loop:\\t# 64-byte LOOP\\n\\t\"\n+       $$emit$$\"vmovdqu $tmp,(rax)\\n\\t\"\n+       $$emit$$\"vmovdqu $tmp,0x20(rax)\\n\\t\"\n+       $$emit$$\"add     0x40,rax\\n\\t\"\n+       $$emit$$\"# L_zero_64_bytes:\\n\\t\"\n+       $$emit$$\"sub     0x8,rcx\\n\\t\"\n+       $$emit$$\"jge     L_loop\\n\\t\"\n+       $$emit$$\"add     0x4,rcx\\n\\t\"\n+       $$emit$$\"jl      L_tail\\n\\t\"\n+       $$emit$$\"vmovdqu $tmp,(rax)\\n\\t\"\n+       $$emit$$\"add     0x20,rax\\n\\t\"\n+       $$emit$$\"sub     0x4,rcx\\n\\t\"\n+       $$emit$$\"# L_tail:\\t# Clearing tail bytes\\n\\t\"\n+       $$emit$$\"add     0x4,rcx\\n\\t\"\n+       $$emit$$\"jle     L_end\\n\\t\"\n+       $$emit$$\"dec     rcx\\n\\t\"\n+       $$emit$$\"# L_sloop:\\t# 8-byte short loop\\n\\t\"\n+       $$emit$$\"vmovq   xmm0,(rax)\\n\\t\"\n+       $$emit$$\"add     0x8,rax\\n\\t\"\n+       $$emit$$\"dec     rcx\\n\\t\"\n+       $$emit$$\"jge     L_sloop\\n\\t\"\n+       $$emit$$\"# L_end:\\n\\t\"\n+    } else {\n+       $$emit$$\"rep     stosq\\t# Store rax to *rdi++ while rcx--\"\n+    }\n+  %}\n+  ins_encode %{\n+    __ clear_mem($base$$Register, $cnt$$Register, $val$$Register,\n+                 $tmp$$XMMRegister, true, false);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct rep_stos_large_word_copy(rcx_RegL cnt, rdi_RegP base, regD tmp, rax_RegL val,\n+                                  Universe dummy, rFlagsReg cr)\n+%{\n+  predicate(((ClearArrayNode*)n)->is_large() && ((ClearArrayNode*)n)->word_copy_only() && (UseAVX <= 2));\n+  match(Set dummy (ClearArray (Binary cnt base) val));\n+  effect(USE_KILL cnt, USE_KILL base, TEMP tmp, USE_KILL val, KILL cr);\n+\n+  format %{ $$template\n+    if (UseXMMForObjInit) {\n+       $$emit$$\"movdq   $tmp, $val\\n\\t\"\n+       $$emit$$\"punpcklqdq $tmp, $tmp\\n\\t\"\n+       $$emit$$\"vinserti128_high $tmp, $tmp\\n\\t\"\n+       $$emit$$\"jmpq    L_zero_64_bytes\\n\\t\"\n+       $$emit$$\"# L_loop:\\t# 64-byte LOOP\\n\\t\"\n+       $$emit$$\"vmovdqu $tmp,(rax)\\n\\t\"\n+       $$emit$$\"vmovdqu $tmp,0x20(rax)\\n\\t\"\n+       $$emit$$\"add     0x40,rax\\n\\t\"\n+       $$emit$$\"# L_zero_64_bytes:\\n\\t\"\n+       $$emit$$\"sub     0x8,rcx\\n\\t\"\n+       $$emit$$\"jge     L_loop\\n\\t\"\n+       $$emit$$\"add     0x4,rcx\\n\\t\"\n+       $$emit$$\"jl      L_tail\\n\\t\"\n+       $$emit$$\"vmovdqu $tmp,(rax)\\n\\t\"\n+       $$emit$$\"add     0x20,rax\\n\\t\"\n+       $$emit$$\"sub     0x4,rcx\\n\\t\"\n+       $$emit$$\"# L_tail:\\t# Clearing tail bytes\\n\\t\"\n+       $$emit$$\"add     0x4,rcx\\n\\t\"\n+       $$emit$$\"jle     L_end\\n\\t\"\n+       $$emit$$\"dec     rcx\\n\\t\"\n+       $$emit$$\"# L_sloop:\\t# 8-byte short loop\\n\\t\"\n+       $$emit$$\"vmovq   xmm0,(rax)\\n\\t\"\n+       $$emit$$\"add     0x8,rax\\n\\t\"\n+       $$emit$$\"dec     rcx\\n\\t\"\n+       $$emit$$\"jge     L_sloop\\n\\t\"\n+       $$emit$$\"# L_end:\\n\\t\"\n+    } else {\n+       $$emit$$\"rep     stosq\\t# Store rax to *rdi++ while rcx--\"\n+    }\n+  %}\n+  ins_encode %{\n+    __ clear_mem($base$$Register, $cnt$$Register, $val$$Register,\n+                 $tmp$$XMMRegister, true, true);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ Large non-constant length ClearArray for AVX512 targets.\n+instruct rep_stos_large_evex(rcx_RegL cnt, rdi_RegP base, legRegD tmp, kReg ktmp, rax_RegL val,\n+                             Universe dummy, rFlagsReg cr)\n+%{\n+  predicate(((ClearArrayNode*)n)->is_large() && !((ClearArrayNode*)n)->word_copy_only() && (UseAVX > 2));\n+  match(Set dummy (ClearArray (Binary cnt base) val));\n+  effect(USE_KILL cnt, USE_KILL base, TEMP tmp, TEMP ktmp, USE_KILL val, KILL cr);\n@@ -15151,2 +15450,2 @@\n-    __ clear_mem($base$$Register, $cnt$$Register, $zero$$Register,\n-                 $tmp$$XMMRegister, true, knoreg);\n+    __ clear_mem($base$$Register, $cnt$$Register, $val$$Register,\n+                 $tmp$$XMMRegister, true, false, $ktmp$$KRegister);\n@@ -15157,3 +15456,2 @@\n-\/\/ Large non-constant length ClearArray for AVX512 targets.\n-instruct rep_stos_large_evex(rcx_RegL cnt, rdi_RegP base, legRegD tmp, kReg ktmp, rax_RegI zero,\n-                             Universe dummy, rFlagsReg cr)\n+instruct rep_stos_large_evex_word_copy(rcx_RegL cnt, rdi_RegP base, legRegD tmp, kReg ktmp, rax_RegL val,\n+                                       Universe dummy, rFlagsReg cr)\n@@ -15161,3 +15459,3 @@\n-  predicate((UseAVX > 2) && ((ClearArrayNode*)n)->is_large());\n-  match(Set dummy (ClearArray cnt base));\n-  effect(USE_KILL cnt, USE_KILL base, TEMP tmp, TEMP ktmp, KILL zero, KILL cr);\n+  predicate(((ClearArrayNode*)n)->is_large() && ((ClearArrayNode*)n)->word_copy_only() && (UseAVX > 2));\n+  match(Set dummy (ClearArray (Binary cnt base) val));\n+  effect(USE_KILL cnt, USE_KILL base, TEMP tmp, TEMP ktmp, USE_KILL val, KILL cr);\n@@ -15202,2 +15500,2 @@\n-    __ clear_mem($base$$Register, $cnt$$Register, $zero$$Register,\n-                 $tmp$$XMMRegister, true, $ktmp$$KRegister);\n+    __ clear_mem($base$$Register, $cnt$$Register, $val$$Register,\n+                 $tmp$$XMMRegister, true, true, $ktmp$$KRegister);\n@@ -15209,1 +15507,1 @@\n-instruct rep_stos_im(immL cnt, rRegP base, regD tmp, rRegI zero, kReg ktmp, Universe dummy, rFlagsReg cr)\n+instruct rep_stos_im(immL cnt, rRegP base, regD tmp, rax_RegL val, kReg ktmp, Universe dummy, rFlagsReg cr)\n@@ -15211,2 +15509,3 @@\n-  predicate(!((ClearArrayNode*)n)->is_large() && (MaxVectorSize >= 32) && VM_Version::supports_avx512vl());\n-  match(Set dummy (ClearArray cnt base));\n+  predicate(!((ClearArrayNode*)n)->is_large() && !((ClearArrayNode*)n)->word_copy_only() &&\n+            ((MaxVectorSize >= 32) && VM_Version::supports_avx512vl()));\n+  match(Set dummy (ClearArray (Binary cnt base) val));\n@@ -15214,1 +15513,1 @@\n-  effect(TEMP tmp, TEMP zero, TEMP ktmp, KILL cr);\n+  effect(TEMP tmp, USE_KILL val, TEMP ktmp, KILL cr);\n@@ -15217,1 +15516,1 @@\n-   __ clear_mem($base$$Register, $cnt$$constant, $zero$$Register, $tmp$$XMMRegister, $ktmp$$KRegister);\n+    __ clear_mem($base$$Register, $cnt$$constant, $val$$Register, $tmp$$XMMRegister, $ktmp$$KRegister);\n@@ -17062,0 +17361,16 @@\n+\/\/ Call runtime without safepoint\n+\/\/ entry point is null, target holds the address to call\n+instruct CallLeafNoFPInDirect(rRegP target)\n+%{\n+  predicate(n->as_Call()->entry_point() == nullptr);\n+  match(CallLeafNoFP target);\n+\n+  ins_cost(300);\n+  format %{ \"call_leaf_nofp,runtime indirect \" %}\n+  ins_encode %{\n+     __ call($target$$Register);\n+  %}\n+\n+  ins_pipe(pipe_slow);\n+%}\n+\n@@ -17065,0 +17380,1 @@\n+  predicate(n->as_Call()->entry_point() != nullptr);\n","filename":"src\/hotspot\/cpu\/x86\/x86.ad","additions":396,"deletions":80,"binary":false,"changes":476,"status":"modified"},{"patch":"@@ -81,0 +81,2 @@\n+#include \"oops\/flatArrayKlass.hpp\"\n+#include \"oops\/inlineKlass.hpp\"\n@@ -483,1 +485,1 @@\n-  soc->do_tag(objArrayOopDesc::base_offset_in_bytes());\n+  soc->do_tag(refArrayOopDesc::base_offset_in_bytes());\n@@ -1346,0 +1348,5 @@\n+  if (CDSConfig::is_valhalla_preview()) {\n+    log_info(cds)(\"Archived java heap is not yet supported with Valhalla preview\");\n+    return;\n+  }\n+\n","filename":"src\/hotspot\/share\/cds\/aotMetaspace.cpp","additions":8,"deletions":1,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -854,1 +854,1 @@\n-      k->set_prototype_header(markWord::prototype().set_narrow_klass(nk));\n+      k->set_prototype_header_klass(nk);\n@@ -857,1 +857,7 @@\n-    if (k->is_objArray_klass()) {\n+    if (k->is_flatArray_klass()) {\n+      num_obj_array_klasses ++;\n+      type = \"flat array\";\n+    } else if (k->is_refArray_klass()) {\n+        num_obj_array_klasses ++;\n+        type = \"ref array\";\n+    } else if (k->is_objArray_klass()) {\n@@ -861,1 +867,1 @@\n-      type = \"array\";\n+      type = \"obj array\";\n","filename":"src\/hotspot\/share\/cds\/archiveBuilder.cpp","additions":9,"deletions":3,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -627,0 +627,1 @@\n+      \/\/ For valhalla, the prototype header is the same as markWord::prototype();\n@@ -1342,1 +1343,5 @@\n-      assert(resolved_k == k, \"classes used by archived heap must not be replaced by JVMTI ClassFileLoadHook\");\n+      if (resolved_k->is_array_klass()) {\n+        assert(resolved_k == k || resolved_k == k->super(), \"classes used by archived heap must not be replaced by JVMTI ClassFileLoadHook\");\n+      } else {\n+        assert(resolved_k == k, \"classes used by archived heap must not be replaced by JVMTI ClassFileLoadHook\");\n+      }\n@@ -2087,0 +2092,7 @@\n+\n+    if (CDSConfig::is_valhalla_preview() && strcmp(klass_name, \"jdk\/internal\/module\/ArchivedModuleGraph\") == 0) {\n+      \/\/ FIXME -- ArchivedModuleGraph doesn't work when java.base is patched with valhalla classes.\n+      i++;\n+      continue;\n+    }\n+\n","filename":"src\/hotspot\/share\/cds\/heapShared.cpp","additions":13,"deletions":1,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -28,0 +28,1 @@\n+#include \"ci\/ciInlineKlass.hpp\"\n@@ -479,1 +480,2 @@\n-      (sym->char_at(1) == JVM_SIGNATURE_ARRAY || sym->char_at(1) == JVM_SIGNATURE_CLASS)) {\n+      (sym->char_at(1) == JVM_SIGNATURE_ARRAY ||\n+       sym->char_at(1) == JVM_SIGNATURE_CLASS )) {\n@@ -492,1 +494,1 @@\n-      return ciObjArrayKlass::make_impl(elem_klass);\n+      return ciArrayKlass::make(elem_klass);\n@@ -518,0 +520,4 @@\n+  int i = 0;\n+  while (sym->char_at(i) == JVM_SIGNATURE_ARRAY) {\n+    i++;\n+  }\n","filename":"src\/hotspot\/share\/ci\/ciEnv.cpp","additions":8,"deletions":2,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -25,0 +25,1 @@\n+#include \"ci\/ciConstant.hpp\"\n@@ -26,0 +27,1 @@\n+#include \"ci\/ciInlineKlass.hpp\"\n@@ -27,0 +29,1 @@\n+#include \"ci\/ciSymbol.hpp\"\n@@ -33,0 +36,1 @@\n+#include \"jvm_io.h\"\n@@ -38,0 +42,1 @@\n+#include \"utilities\/globalDefinitions.hpp\"\n@@ -73,1 +78,1 @@\n-    _known_to_link_with_put(nullptr), _known_to_link_with_get(nullptr) {\n+  _original_holder(nullptr), _is_flat(false), _known_to_link_with_put(nullptr), _known_to_link_with_get(nullptr) {\n@@ -105,0 +110,3 @@\n+  _is_null_free = false;\n+  _null_marker_offset = -1;\n+\n@@ -216,0 +224,55 @@\n+\/\/ Special copy constructor used to flatten inline type fields by\n+\/\/ copying the fields of the inline type to a new holder klass.\n+ciField::ciField(ciField* declared_field, ciField* subfield) {\n+  assert(subfield->holder()->is_inlinetype() || subfield->holder()->is_abstract(), \"should only be used for inline type field flattening\");\n+  assert(!subfield->is_flat(), \"subfield must not be flat\");\n+  assert(declared_field->is_flat(), \"declared field must be flat\");\n+\n+  _flags = declared_field->flags();\n+  _holder = declared_field->holder();\n+  _offset = declared_field->offset_in_bytes() + (subfield->offset_in_bytes() - declared_field->type()->as_inline_klass()->payload_offset());\n+\n+  char buffer[256];\n+  jio_snprintf(buffer, sizeof(buffer), \"%s.%s\", declared_field->name()->as_utf8(), subfield->name()->as_utf8());\n+  _name = ciSymbol::make(buffer);\n+\n+  _signature = subfield->_signature;\n+  _type = subfield->_type;\n+  _is_constant = declared_field->is_strict() && declared_field->is_final();\n+  _known_to_link_with_put = subfield->_known_to_link_with_put;\n+  _known_to_link_with_get = subfield->_known_to_link_with_get;\n+  _constant_value = ciConstant();\n+\n+  _is_flat = false;\n+  _is_null_free = false;\n+  _null_marker_offset = -1;\n+  _original_holder = (subfield->_original_holder != nullptr) ? subfield->_original_holder : subfield->_holder;\n+}\n+\n+\/\/ Constructor for the ciField of a null marker\n+ciField::ciField(ciField* declared_field) {\n+  assert(declared_field->is_flat(), \"declared field must be flat\");\n+  assert(!declared_field->is_null_free(), \"must have a null marker\");\n+\n+  _flags = declared_field->flags();\n+  _holder = declared_field->holder();\n+  _offset = declared_field->null_marker_offset();\n+\n+  char buffer[256];\n+  jio_snprintf(buffer, sizeof(buffer), \"%s.$nullMarker$\", declared_field->name()->as_utf8());\n+  _name = ciSymbol::make(buffer);\n+\n+  _signature = ciSymbols::bool_signature();\n+  _type = ciType::make(T_BOOLEAN);\n+\n+  _is_constant = declared_field->is_strict() && declared_field->is_final();\n+  _known_to_link_with_put = nullptr;\n+  _known_to_link_with_get = nullptr;\n+  _constant_value = ciConstant();\n+\n+  _is_flat = false;\n+  _is_null_free = false;\n+  _null_marker_offset = -1;\n+  _original_holder = nullptr;\n+}\n+\n@@ -233,0 +296,3 @@\n+  \/\/ Trust final fields in inline type buffers\n+  if (holder->is_inlinetype())\n+    return true;\n@@ -257,1 +323,1 @@\n-  Klass* field_holder = fd->field_holder();\n+  InstanceKlass* field_holder = fd->field_holder();\n@@ -260,0 +326,9 @@\n+  _is_flat = fd->is_flat();\n+  _is_null_free = fd->is_null_free_inline_type();\n+  if (fd->has_null_marker()) {\n+    InlineLayoutInfo* li = field_holder->inline_layout_info_adr(fd->index());\n+    _null_marker_offset = li->null_marker_offset();\n+  } else {\n+    _null_marker_offset = -1;\n+  }\n+  _original_holder = nullptr;\n@@ -342,1 +417,3 @@\n-  ciKlass* type = CURRENT_ENV->get_klass_by_name_impl(_holder, constantPoolHandle(), _signature, false);\n+  \/\/ Use original holder for fields that came in through flattening\n+  ciKlass* accessing_klass = (_original_holder != nullptr) ? _original_holder : _holder;\n+  ciKlass* type = CURRENT_ENV->get_klass_by_name_impl(accessing_klass, constantPoolHandle(), _signature, false);\n@@ -405,0 +482,12 @@\n+  \/\/ Strict statics may require tracking if their class is not fully initialized.\n+  \/\/ For now we can bail out of the compiler and let the interpreter handle it.\n+  if (is_static && result.is_strict_static_unset()) {\n+    \/\/ If we left out this logic, we would get (a) spurious <clinit>\n+    \/\/ failures for C2 code because compiled putstatic would not write\n+    \/\/ the \"unset\" bits, and (b) missed failures for too-early reads,\n+    \/\/ since the compiled getstatic would not check the \"unset\" bits.\n+    \/\/ Test C1 on <clinit> with \"-XX:TieredStopAtLevel=2 -Xcomp -Xbatch\".\n+    \/\/ Test C2 on <clinit> with \"-XX:-TieredCompilation -Xcomp -Xbatch\".\n+    return false;\n+  }\n+\n@@ -455,0 +544,3 @@\n+  tty->print(\" is_flat=%s\", bool_to_str(_is_flat));\n+  tty->print(\" is_null_free=%s\", bool_to_str(_is_null_free));\n+  tty->print(\" null_marker_offset=%d\", _null_marker_offset);\n","filename":"src\/hotspot\/share\/ci\/ciField.cpp","additions":95,"deletions":3,"binary":false,"changes":98,"status":"modified"},{"patch":"@@ -24,0 +24,1 @@\n+\n@@ -53,0 +54,1 @@\n+#include \"oops\/inlineKlass.inline.hpp\"\n@@ -87,0 +89,1 @@\n+#include \"utilities\/stringUtils.hpp\"\n@@ -159,0 +162,2 @@\n+#define CONSTANT_CLASS_DESCRIPTORS        70\n+\n@@ -197,1 +202,1 @@\n-      case JVM_CONSTANT_Class : {\n+      case JVM_CONSTANT_Class: {\n@@ -501,0 +506,3 @@\n+\n+        Symbol* const name = cp->symbol_at(class_index);\n+        const unsigned int name_len = name->utf8_length();\n@@ -710,1 +718,1 @@\n-            } else if (!Signature::is_void_method(signature)) { \/\/ must have void signature.\n+            } else if (!Signature::is_void_method(signature)) {  \/\/ must have void signature.\n@@ -730,2 +738,3 @@\n-            if (ref_kind == JVM_REF_newInvokeSpecial) {\n-              if (name != vmSymbols::object_initializer_name()) {\n+\n+            if (name != vmSymbols::object_initializer_name()) { \/\/ !<init>\n+              if (ref_kind == JVM_REF_newInvokeSpecial) {\n@@ -737,2 +746,10 @@\n-            } else {\n-              if (name == vmSymbols::object_initializer_name()) {\n+            } else { \/\/ <init>\n+              \/\/ The allowed invocation mode of <init> depends on its signature.\n+              \/\/ This test corresponds to verify_invoke_instructions in the verifier.\n+              const int signature_ref_index =\n+                cp->signature_ref_index_at(name_and_type_ref_index);\n+              const Symbol* const signature = cp->symbol_at(signature_ref_index);\n+              if (signature->is_void_method_signature()\n+                  && ref_kind == JVM_REF_newInvokeSpecial) {\n+                \/\/ OK, could be a constructor call\n+              } else {\n@@ -790,4 +807,13 @@\n-\/\/ Side-effects: populates the _local_interfaces field\n-void ClassFileParser::parse_interfaces(const ClassFileStream* const stream,\n-                                       const int itfs_len,\n-                                       ConstantPool* const cp,\n+static void check_identity_and_value_modifiers(ClassFileParser* current, const InstanceKlass* super_type, TRAPS) {\n+  assert(super_type != nullptr,\"Method doesn't support null super type\");\n+  if (super_type->access_flags().is_identity_class() && !current->access_flags().is_identity_class()\n+      && super_type->name() != vmSymbols::java_lang_Object()) {\n+      THROW_MSG(vmSymbols::java_lang_IncompatibleClassChangeError(),\n+                err_msg(\"Value type %s has an identity type as supertype\",\n+                current->class_name()->as_klass_external_name()));\n+  }\n+}\n+\n+void ClassFileParser::parse_interfaces(const ClassFileStream* stream,\n+                                       int itfs_len,\n+                                       ConstantPool* cp,\n@@ -795,0 +821,6 @@\n+                                       \/\/ FIXME: lots of these functions\n+                                       \/\/ declare their parameters as const,\n+                                       \/\/ which adds only noise to the code.\n+                                       \/\/ Remove the spurious const modifiers.\n+                                       \/\/ Many are of the form \"const int x\"\n+                                       \/\/ or \"T* const x\".\n@@ -802,0 +834,1 @@\n+\n@@ -804,3 +837,2 @@\n-    _local_interfaces = MetadataFactory::new_array<InstanceKlass*>(_loader_data, itfs_len, nullptr, CHECK);\n-\n-    int index;\n+    _local_interface_indexes = new GrowableArray<u2>(itfs_len);\n+    int index = 0;\n@@ -809,1 +841,0 @@\n-      Klass* interf;\n@@ -814,29 +845,1 @@\n-      if (cp->tag_at(interface_index).is_klass()) {\n-        interf = cp->resolved_klass_at(interface_index);\n-      } else {\n-        Symbol* const unresolved_klass  = cp->klass_name_at(interface_index);\n-\n-        \/\/ Don't need to check legal name because it's checked when parsing constant pool.\n-        \/\/ But need to make sure it's not an array type.\n-        guarantee_property(unresolved_klass->char_at(0) != JVM_SIGNATURE_ARRAY,\n-                           \"Bad interface name in class file %s\", CHECK);\n-\n-        \/\/ Call resolve on the interface class name with class circularity checking\n-        interf = SystemDictionary::resolve_super_or_fail(_class_name,\n-                                                         unresolved_klass,\n-                                                         Handle(THREAD, _loader_data->class_loader()),\n-                                                         false, CHECK);\n-      }\n-\n-      if (!interf->is_interface()) {\n-        THROW_MSG(vmSymbols::java_lang_IncompatibleClassChangeError(),\n-                  err_msg(\"class %s can not implement %s, because it is not an interface (%s)\",\n-                          _class_name->as_klass_external_name(),\n-                          interf->external_name(),\n-                          interf->class_in_module_of_loader()));\n-      }\n-\n-      if (InstanceKlass::cast(interf)->has_nonstatic_concrete_methods()) {\n-        *has_nonstatic_concrete_methods = true;\n-      }\n-      _local_interfaces->at_put(index, InstanceKlass::cast(interf));\n+      _local_interface_indexes->at_put_grow(index, interface_index);\n@@ -854,2 +857,1 @@\n-      const InstanceKlass* const k = _local_interfaces->at(index);\n-      Symbol* interface_name = k->name();\n+      Symbol* interface_name = cp->klass_name_at(_local_interface_indexes->at(index));\n@@ -942,0 +944,2 @@\n+    _jdk_internal_LooselyConsistentValue,\n+    _jdk_internal_NullRestricted,\n@@ -1368,1 +1372,1 @@\n-                                   bool is_interface,\n+                                   AccessFlags class_access_flags,\n@@ -1381,0 +1385,2 @@\n+  bool is_inline_type = !class_access_flags.is_identity_class() && !class_access_flags.is_abstract();\n+  bool is_value_class = !class_access_flags.is_identity_class() && !class_access_flags.is_interface();\n@@ -1388,1 +1394,6 @@\n-  const int total_fields = length + num_injected;\n+\n+  \/\/ two more slots are required for inline classes:\n+  \/\/ one for the static field with a reference to the pre-allocated default value\n+  \/\/ one for the field the JVM injects when detecting an empty inline class\n+  const int total_fields = length + num_injected + (is_inline_type ? 2 : 0)\n+                           + ((UseAltSubstitutabilityMethod && is_value_class) ? 1 : 0);\n@@ -1394,0 +1405,1 @@\n+  int instance_fields_count = 0;\n@@ -1399,0 +1411,7 @@\n+    jint recognized_modifiers = JVM_RECOGNIZED_FIELD_MODIFIERS;\n+    if (!supports_inline_types()) {\n+      recognized_modifiers &= ~JVM_ACC_STRICT;\n+    }\n+\n+    const jint flags = cfs->get_u2_fast() & recognized_modifiers;\n+    verify_legal_field_modifiers(flags, class_access_flags, CHECK);\n@@ -1400,2 +1419,0 @@\n-    const jint flags = cfs->get_u2_fast() & JVM_RECOGNIZED_FIELD_MODIFIERS;\n-    verify_legal_field_modifiers(flags, is_interface, CHECK);\n@@ -1418,0 +1435,1 @@\n+    if (!access_flags.is_static()) instance_fields_count++;\n@@ -1425,0 +1443,2 @@\n+    bool is_null_restricted = false;\n+\n@@ -1444,0 +1464,18 @@\n+        if (parsed_annotations.has_annotation(AnnotationCollector::_jdk_internal_NullRestricted)) {\n+          if (!Signature::has_envelope(sig)) {\n+            Exceptions::fthrow(\n+              THREAD_AND_LOCATION,\n+              vmSymbols::java_lang_ClassFormatError(),\n+              \"Illegal use of @jdk.internal.vm.annotation.NullRestricted annotation on field %s.%s with signature %s (primitive types can never be null)\",\n+              class_name()->as_C_string(), name->as_C_string(), sig->as_C_string());\n+          }\n+          const bool is_strict = (flags & JVM_ACC_STRICT) != 0;\n+          if (!is_strict) {\n+            Exceptions::fthrow(\n+              THREAD_AND_LOCATION,\n+              vmSymbols::java_lang_ClassFormatError(),\n+              \"Illegal use of @jdk.internal.vm.annotation.NullRestricted annotation on field %s.%s which doesn't have the @jdk.internal.vm.annotation.Strict annotation\",\n+              class_name()->as_C_string(), name->as_C_string());\n+          }\n+          is_null_restricted = true;\n+        }\n@@ -1466,0 +1504,4 @@\n+    if (is_null_restricted) {\n+      fieldFlags.update_null_free_inline_type(true);\n+    }\n+\n@@ -1482,0 +1524,3 @@\n+    if (access_flags.is_strict() && access_flags.is_static()) {\n+      _has_strict_static_fields = true;\n+    }\n@@ -1486,1 +1531,0 @@\n-  int index = length;\n@@ -1514,3 +1558,2 @@\n-      fi.set_index(index);\n-      _temp_field_info->append(fi);\n-      index++;\n+      int idx = _temp_field_info->append(fi);\n+      _temp_field_info->adr_at(idx)->set_index(idx);\n@@ -1520,1 +1563,33 @@\n-  assert(_temp_field_info->length() == index, \"Must be\");\n+  if (is_inline_type) {\n+    \/\/ Inject static \".null_reset\" field. This is an all-zero value with its null-channel set to zero.\n+    \/\/ IT should never be seen by user code, it is used when writing \"null\" to a nullable flat field\n+    \/\/ The all-zero value ensure that any embedded oop will be set to null, to avoid keeping dead objects\n+    \/\/ alive.\n+    FieldInfo::FieldFlags fflags2(0);\n+    fflags2.update_injected(true);\n+    AccessFlags aflags2(JVM_ACC_STATIC);\n+    FieldInfo fi2(aflags2,\n+                 (u2)vmSymbols::as_int(VM_SYMBOL_ENUM_NAME(null_reset_value_name)),\n+                 (u2)vmSymbols::as_int(VM_SYMBOL_ENUM_NAME(object_signature)),\n+                 0,\n+                 fflags2);\n+    int idx2 = _temp_field_info->append(fi2);\n+    _temp_field_info->adr_at(idx2)->set_index(idx2);\n+    _static_oop_count++;\n+  }\n+  if (!access_flags().is_identity_class() && !access_flags().is_interface()\n+      && _class_name != vmSymbols::java_lang_Object() && UseAltSubstitutabilityMethod) {\n+    \/\/ Acmp map required for abstract and concrete value classes\n+    FieldInfo::FieldFlags fflags2(0);\n+    fflags2.update_injected(true);\n+    fflags2.update_stable(true);\n+    AccessFlags aflags2(JVM_ACC_STATIC | JVM_ACC_FINAL);\n+    FieldInfo fi3(aflags2,\n+                 (u2)vmSymbols::as_int(VM_SYMBOL_ENUM_NAME(acmp_maps_name)),\n+                 (u2)vmSymbols::as_int(VM_SYMBOL_ENUM_NAME(int_array_signature)),\n+                 0,\n+                 fflags2);\n+    int idx2 = _temp_field_info->append(fi3);\n+    _temp_field_info->adr_at(idx2)->set_index(idx2);\n+    _static_oop_count++;\n+  }\n@@ -1900,0 +1975,8 @@\n+    case VM_SYMBOL_ENUM_NAME(jdk_internal_vm_annotation_LooselyConsistentValue_signature): {\n+      if (_location != _in_class)   break; \/\/ only allow for classes\n+      return _jdk_internal_LooselyConsistentValue;\n+    }\n+    case VM_SYMBOL_ENUM_NAME(jdk_internal_vm_annotation_NullRestricted_signature): {\n+      if (_location != _in_field)   break; \/\/ only allow for fields\n+      return _jdk_internal_NullRestricted;\n+    }\n@@ -2136,0 +2219,2 @@\n+                                      bool is_value_class,\n+                                      bool is_abstract_class,\n@@ -2177,1 +2262,1 @@\n-    verify_legal_method_modifiers(flags, is_interface, name, CHECK_NULL);\n+    verify_legal_method_modifiers(flags, access_flags() , name, CHECK_NULL);\n@@ -2185,0 +2270,9 @@\n+  if (EnableValhalla) {\n+    if (((flags & JVM_ACC_SYNCHRONIZED) == JVM_ACC_SYNCHRONIZED)\n+        && ((flags & JVM_ACC_STATIC) == 0 )\n+        && !_access_flags.is_identity_class()) {\n+      classfile_parse_error(\"Invalid synchronized method in non-identity class %s\", THREAD);\n+        return nullptr;\n+    }\n+  }\n+\n@@ -2719,0 +2813,2 @@\n+                                    bool is_value_class,\n+                                    bool is_abstract_type,\n@@ -2743,0 +2839,2 @@\n+                                    is_value_class,\n+                                    is_abstract_type,\n@@ -3008,0 +3106,1 @@\n+\n@@ -3016,0 +3115,1 @@\n+\n@@ -3021,0 +3121,8 @@\n+    if (!supports_inline_types()) {\n+      const bool is_module = (flags & JVM_ACC_MODULE) != 0;\n+      const bool is_interface = (flags & JVM_ACC_INTERFACE) != 0;\n+      if (!is_module && !is_interface) {\n+        flags |= JVM_ACC_IDENTITY;\n+      }\n+    }\n+\n@@ -3126,0 +3234,43 @@\n+u2 ClassFileParser::parse_classfile_loadable_descriptors_attribute(const ClassFileStream* const cfs,\n+                                                                   const u1* const loadable_descriptors_attribute_start,\n+                                                                   TRAPS) {\n+  const u1* const current_mark = cfs->current();\n+  u2 length = 0;\n+  if (loadable_descriptors_attribute_start != nullptr) {\n+    cfs->set_current(loadable_descriptors_attribute_start);\n+    cfs->guarantee_more(2, CHECK_0);  \/\/ length\n+    length = cfs->get_u2_fast();\n+  }\n+  const int size = length;\n+  Array<u2>* const loadable_descriptors = MetadataFactory::new_array<u2>(_loader_data, size, CHECK_0);\n+  _loadable_descriptors = loadable_descriptors;\n+  if (length > 0) {\n+    int index = 0;\n+    cfs->guarantee_more(2 * length, CHECK_0);\n+    for (int n = 0; n < length; n++) {\n+      const u2 descriptor_index = cfs->get_u2_fast();\n+      guarantee_property(\n+        valid_symbol_at(descriptor_index),\n+        \"LoadableDescriptors descriptor_index %u has bad constant type in class file %s\",\n+        descriptor_index, CHECK_0);\n+      Symbol* descriptor = _cp->symbol_at(descriptor_index);\n+      bool valid = legal_field_signature(descriptor, CHECK_0);\n+      if(!valid) {\n+        ResourceMark rm(THREAD);\n+        Exceptions::fthrow(THREAD_AND_LOCATION,\n+          vmSymbols::java_lang_ClassFormatError(),\n+          \"Descriptor from LoadableDescriptors attribute at index \\\"%d\\\" in class %s has illegal signature \\\"%s\\\"\",\n+          descriptor_index, _class_name->as_C_string(), descriptor->as_C_string());\n+        return 0;\n+      }\n+      loadable_descriptors->at_put(index++, descriptor_index);\n+    }\n+    assert(index == size, \"wrong size\");\n+  }\n+\n+  \/\/ Restore buffer's current position.\n+  cfs->set_current(current_mark);\n+\n+  return length;\n+}\n+\n@@ -3391,0 +3542,2 @@\n+  \/\/ Set _loadable_descriptors attribute to default sentinel\n+  _loadable_descriptors = Universe::the_empty_short_array();\n@@ -3397,0 +3550,1 @@\n+  bool parsed_loadable_descriptors_attribute = false;\n@@ -3418,0 +3572,2 @@\n+  const u1* loadable_descriptors_attribute_start = nullptr;\n+  u4  loadable_descriptors_attribute_length = 0;\n@@ -3633,0 +3789,9 @@\n+            if (EnableValhalla && tag == vmSymbols::tag_loadable_descriptors()) {\n+              if (parsed_loadable_descriptors_attribute) {\n+                classfile_parse_error(\"Multiple LoadableDescriptors attributes in class file %s\", CHECK);\n+                return;\n+              }\n+              parsed_loadable_descriptors_attribute = true;\n+              loadable_descriptors_attribute_start = cfs->current();\n+              loadable_descriptors_attribute_length = attribute_length;\n+            }\n@@ -3709,0 +3874,12 @@\n+  if (parsed_loadable_descriptors_attribute) {\n+    const u2 num_classes = parse_classfile_loadable_descriptors_attribute(\n+                            cfs,\n+                            loadable_descriptors_attribute_start,\n+                            CHECK);\n+    if (_need_verify) {\n+      guarantee_property(\n+        loadable_descriptors_attribute_length == sizeof(num_classes) + sizeof(u2) * num_classes,\n+        \"Wrong LoadableDescriptors attribute length in class file %s\", CHECK);\n+    }\n+  }\n+\n@@ -3775,0 +3952,1 @@\n+  this_klass->set_loadable_descriptors(_loadable_descriptors);\n@@ -3778,0 +3956,1 @@\n+  this_klass->set_inline_layout_info_array(_inline_layout_info_array);\n@@ -3816,2 +3995,1 @@\n-                       \"Invalid superclass index %u in class file %s\",\n-                       super_class_index,\n+                       \"Invalid superclass index 0 in class file %s\",\n@@ -4009,0 +4187,6 @@\n+bool ClassFileParser::supports_inline_types() const {\n+  \/\/ Inline types are only supported by class file version 70.65535 and later\n+  return _major_version > JAVA_26_VERSION ||\n+         (_major_version == JAVA_26_VERSION && _minor_version == JAVA_PREVIEW_MINOR_VERSION);\n+}\n+\n@@ -4052,3 +4236,4 @@\n-  } else if (max_transitive_size == local_size) {\n-    \/\/ only local interfaces added, share local interface array\n-    return local_ifs;\n+    \/\/ The three lines below are commented to work around bug JDK-8245487\n+\/\/  } else if (max_transitive_size == local_size) {\n+\/\/    \/\/ only local interfaces added, share local interface array\n+\/\/    return local_ifs;\n@@ -4075,0 +4260,1 @@\n+\n@@ -4105,0 +4291,10 @@\n+    \/\/ The JVMS says that super classes for value types must not have the ACC_IDENTITY\n+    \/\/ flag set. But, java.lang.Object must still be allowed to be a direct super class\n+    \/\/ for a value classes.  So, it is treated as a special case for now.\n+    if (!this_klass->access_flags().is_identity_class() &&\n+        super->name() != vmSymbols::java_lang_Object() &&\n+        super->is_identity_class()) {\n+      classfile_icce_error(\"value class %s cannot inherit from class %s\", super, THREAD);\n+      return;\n+    }\n+\n@@ -4297,1 +4493,1 @@\n-  const bool is_super      = (flags & JVM_ACC_SUPER)      != 0;\n+  const bool is_identity   = (flags & JVM_ACC_IDENTITY)   != 0;\n@@ -4301,0 +4497,2 @@\n+  const bool valid_value_class = is_identity || is_interface ||\n+                                 (supports_inline_types() && (!is_identity && (is_abstract || is_final)));\n@@ -4304,2 +4502,3 @@\n-      (is_interface && major_gte_1_5 && (is_super || is_enum)) ||\n-      (!is_interface && major_gte_1_5 && is_annotation)) {\n+      (is_interface && major_gte_1_5 && (is_identity || is_enum)) ||   \/\/  ACC_SUPER (now ACC_IDENTITY) was illegal for interfaces\n+      (!is_interface && major_gte_1_5 && is_annotation) ||\n+      (!valid_value_class)) {\n@@ -4307,0 +4506,4 @@\n+    const char* class_note = \"\";\n+    if (!valid_value_class) {\n+      class_note = \" (a value class must be final or else abstract)\";\n+    }\n@@ -4400,2 +4603,2 @@\n-void ClassFileParser::verify_legal_field_modifiers(jint flags,\n-                                                   bool is_interface,\n+void ClassFileParser:: verify_legal_field_modifiers(jint flags,\n+                                                   AccessFlags class_access_flags,\n@@ -4413,0 +4616,1 @@\n+  const bool is_strict    = (flags & JVM_ACC_STRICT)    != 0;\n@@ -4415,1 +4619,2 @@\n-  bool is_illegal = false;\n+  const bool is_interface = class_access_flags.is_interface();\n+  const bool is_identity_class = class_access_flags.is_identity_class();\n@@ -4417,9 +4622,30 @@\n-  if (is_interface) {\n-    if (!is_public || !is_static || !is_final || is_private ||\n-        is_protected || is_volatile || is_transient ||\n-        (major_gte_1_5 && is_enum)) {\n-      is_illegal = true;\n-    }\n-  } else { \/\/ not interface\n-    if (has_illegal_visibility(flags) || (is_final && is_volatile)) {\n-      is_illegal = true;\n+  bool is_illegal = false;\n+  const char* error_msg = \"\";\n+\n+  \/\/ There is some overlap in the checks that apply, for example interface fields\n+  \/\/ must be static, static fields can't be strict, and therefore interfaces can't\n+  \/\/ have strict fields. So we don't have to check every possible invalid combination\n+  \/\/ individually as long as all are covered. Once we have found an illegal combination\n+  \/\/ we can stop checking.\n+\n+  if (!is_illegal) {\n+    if (is_interface) {\n+      if (!is_public || !is_static || !is_final || is_private ||\n+          is_protected || is_volatile || is_transient ||\n+          (major_gte_1_5 && is_enum)) {\n+        is_illegal = true;\n+        error_msg = \"interface fields must be public, static and final, and may be synthetic\";\n+      }\n+    } else { \/\/ not interface\n+      if (has_illegal_visibility(flags)) {\n+        is_illegal = true;\n+        error_msg = \"invalid visibility flags for class field\";\n+      } else if (is_final && is_volatile) {\n+        is_illegal = true;\n+        error_msg = \"fields cannot be final and volatile\";\n+      } else if (supports_inline_types()) {\n+        if (!is_identity_class && !is_static && (!is_strict || !is_final)) {\n+          is_illegal = true;\n+          error_msg = \"value class fields must be either non-static final and strict, or static\";\n+        }\n+      }\n@@ -4435,2 +4661,2 @@\n-      \"Illegal field modifiers in class %s: 0x%X\",\n-      _class_name->as_C_string(), flags);\n+      \"Illegal field modifiers (%s) in class %s: 0x%X\",\n+      error_msg, _class_name->as_C_string(), flags);\n@@ -4442,1 +4668,1 @@\n-                                                    bool is_interface,\n+                                                    AccessFlags class_access_flags,\n@@ -4461,0 +4687,4 @@\n+  \/\/ LW401 CR required: removal of value factories support\n+  const bool is_interface    = class_access_flags.is_interface();\n+  const bool is_identity_class = class_access_flags.is_identity_class();\n+  const bool is_abstract_class = class_access_flags.is_abstract();\n@@ -4464,0 +4694,1 @@\n+  const char* class_note = \"\";\n@@ -4503,4 +4734,9 @@\n-        if (is_abstract) {\n-          if ((is_final || is_native || is_private || is_static ||\n-              (major_gte_1_5 && (is_synchronized || (!major_gte_17 && is_strict))))) {\n-            is_illegal = true;\n+        if (!is_identity_class && is_synchronized && !is_static) {\n+          is_illegal = true;\n+          class_note = \" (not an identity class)\";\n+        } else {\n+          if (is_abstract) {\n+            if ((is_final || is_native || is_private || is_static ||\n+                (major_gte_1_5 && (is_synchronized || (!major_gte_17 && is_strict))))) {\n+              is_illegal = true;\n+            }\n@@ -4519,2 +4755,3 @@\n-      \"Method %s in class %s has illegal modifiers: 0x%X\",\n-      name->as_C_string(), _class_name->as_C_string(), flags);\n+      \"Method %s in class %s%s has illegal modifiers: 0x%X\",\n+      name->as_C_string(), _class_name->as_C_string(),\n+      class_note, flags);\n@@ -4578,0 +4815,9 @@\n+bool ClassFileParser::is_class_in_loadable_descriptors_attribute(Symbol *klass) {\n+  if (_loadable_descriptors == nullptr) return false;\n+  for (int i = 0; i < _loadable_descriptors->length(); i++) {\n+        Symbol* class_name = _cp->symbol_at(_loadable_descriptors->at(i));\n+        if (class_name == klass) return true;\n+  }\n+  return false;\n+}\n+\n@@ -4679,1 +4925,2 @@\n-    case JVM_SIGNATURE_CLASS: {\n+    case JVM_SIGNATURE_CLASS:\n+    {\n@@ -4694,1 +4941,1 @@\n-        \/\/ Skip leading 'L' and ignore first appearance of ';'\n+        \/\/ Skip leading 'L' or 'Q' and ignore first appearance of ';'\n@@ -4750,0 +4997,4 @@\n+    } else if ((_major_version >= CONSTANT_CLASS_DESCRIPTORS || _class_name->starts_with(\"jdk\/internal\/reflect\/\"))\n+                   && bytes[length - 1] == ';' ) {\n+      \/\/ Support for L...; descriptors\n+      legal = verify_unqualified_name(bytes + 1, length - 2, LegalClass);\n@@ -4817,1 +5068,2 @@\n-      if (name == vmSymbols::object_initializer_name() || name == vmSymbols::class_initializer_name()) {\n+      if (name == vmSymbols::object_initializer_name() ||\n+          name == vmSymbols::class_initializer_name()) {\n@@ -4844,0 +5096,10 @@\n+bool ClassFileParser::legal_field_signature(const Symbol* signature, TRAPS) const {\n+  const char* const bytes = (const char*)signature->bytes();\n+  const unsigned int length = signature->utf8_length();\n+  const char* const p = skip_over_field_signature(bytes, false, length, CHECK_false);\n+\n+  if (p == nullptr || (p - bytes) != (int)length) {\n+    return false;\n+  }\n+  return true;\n+}\n@@ -4879,3 +5141,3 @@\n-      name->char_at(0) == JVM_SIGNATURE_SPECIAL &&\n-      sig_length > 0 &&\n-      signature->char_at(sig_length - 1) != JVM_SIGNATURE_VOID) {\n+    name->char_at(0) == JVM_SIGNATURE_SPECIAL &&\n+    sig_length > 0 &&\n+    signature->char_at(sig_length - 1) != JVM_SIGNATURE_VOID) {\n@@ -4932,2 +5194,2 @@\n-  assert(_field_info != nullptr, \"invariant\");\n-  return _field_info->_static_field_size;\n+  assert(_layout_info != nullptr, \"invariant\");\n+  return _layout_info->_static_field_size;\n@@ -4937,2 +5199,2 @@\n-  assert(_field_info != nullptr, \"invariant\");\n-  return _field_info->oop_map_blocks->_nonstatic_oop_map_count;\n+  assert(_layout_info != nullptr, \"invariant\");\n+  return _layout_info->oop_map_blocks->_nonstatic_oop_map_count;\n@@ -4942,2 +5204,2 @@\n-  assert(_field_info != nullptr, \"invariant\");\n-  return _field_info->_instance_size;\n+  assert(_layout_info != nullptr, \"invariant\");\n+  return _layout_info->_instance_size;\n@@ -5058,1 +5320,0 @@\n-\n@@ -5081,3 +5342,3 @@\n-  assert(_field_info != nullptr, \"invariant\");\n-  assert(ik->static_field_size() == _field_info->_static_field_size, \"sanity\");\n-  assert(ik->nonstatic_oop_map_count() == _field_info->oop_map_blocks->_nonstatic_oop_map_count,\n+  assert(_layout_info != nullptr, \"invariant\");\n+  assert(ik->static_field_size() == _layout_info->_static_field_size, \"sanity\");\n+  assert(ik->nonstatic_oop_map_count() == _layout_info->oop_map_blocks->_nonstatic_oop_map_count,\n@@ -5087,1 +5348,1 @@\n-  assert(ik->size_helper() == _field_info->_instance_size, \"sanity\");\n+  assert(ik->size_helper() == _layout_info->_instance_size, \"sanity\");\n@@ -5093,2 +5354,12 @@\n-  ik->set_nonstatic_field_size(_field_info->_nonstatic_field_size);\n-  ik->set_has_nonstatic_fields(_field_info->_has_nonstatic_fields);\n+  ik->set_nonstatic_field_size(_layout_info->_nonstatic_field_size);\n+  ik->set_has_nonstatic_fields(_layout_info->_has_nonstatic_fields);\n+  ik->set_has_strict_static_fields(_has_strict_static_fields);\n+\n+  if (_layout_info->_is_naturally_atomic) {\n+    ik->set_is_naturally_atomic();\n+  }\n+\n+  if (_layout_info->_must_be_atomic) {\n+    ik->set_must_be_atomic();\n+  }\n+\n@@ -5100,0 +5371,3 @@\n+  if (ik->is_inline_klass()) {\n+    InlineKlass::cast(ik)->init_fixed_block();\n+  }\n@@ -5114,0 +5388,1 @@\n+  assert(nullptr == _loadable_descriptors, \"invariant\");\n@@ -5117,0 +5392,1 @@\n+  assert(nullptr == _inline_layout_info_array, \"invariant\");\n@@ -5193,1 +5469,1 @@\n-  OopMapBlocksBuilder* oop_map_blocks = _field_info->oop_map_blocks;\n+  OopMapBlocksBuilder* oop_map_blocks = _layout_info->oop_map_blocks;\n@@ -5254,0 +5530,49 @@\n+  if (is_inline_type()) {\n+    InlineKlass* vk = InlineKlass::cast(ik);\n+    vk->set_payload_alignment(_layout_info->_payload_alignment);\n+    vk->set_payload_offset(_layout_info->_payload_offset);\n+    vk->set_payload_size_in_bytes(_layout_info->_payload_size_in_bytes);\n+    vk->set_non_atomic_size_in_bytes(_layout_info->_non_atomic_size_in_bytes);\n+    vk->set_non_atomic_alignment(_layout_info->_non_atomic_alignment);\n+    vk->set_atomic_size_in_bytes(_layout_info->_atomic_layout_size_in_bytes);\n+    vk->set_nullable_size_in_bytes(_layout_info->_nullable_layout_size_in_bytes);\n+    vk->set_null_marker_offset(_layout_info->_null_marker_offset);\n+    vk->set_null_reset_value_offset(_layout_info->_null_reset_value_offset);\n+    if (_layout_info->_is_empty_inline_klass) vk->set_is_empty_inline_type();\n+    vk->initialize_calling_convention(CHECK);\n+  }\n+\n+  if (EnableValhalla && !access_flags().is_identity_class() && !access_flags().is_interface()\n+      && _class_name != vmSymbols::java_lang_Object() && UseAltSubstitutabilityMethod) {\n+    \/\/ Both abstract and concrete value classes need a field map for acmp\n+    ik->set_acmp_maps_offset(_layout_info->_acmp_maps_offset);\n+    \/\/ Current format of acmp maps:\n+    \/\/ All maps are stored contiguously in a single int array because it might\n+    \/\/ be too early to instantiate an Object array (to be investigated)\n+    \/\/ Format is:\n+    \/\/ [number_of_nonoop_entries][offset0][size[0][offset1][size1]...[oop_offset0][oop_offset1]...\n+    \/\/                           ^               ^\n+    \/\/                           |               |\n+    \/\/                           --------------------- Pair of integer describing a segment of\n+    \/\/                                                 contiguous non-oop fields\n+    \/\/ First element is the number of segment of contiguous non-oop fields\n+    \/\/ Then, each segment of contiguous non-oop fields is described by two consecutive elements:\n+    \/\/ the offset then the size.\n+    \/\/ After the last segment of contiguous non-oop fields, oop fields are described, one element\n+    \/\/ per oop field, containing the offset of the field.\n+    int nonoop_acmp_map_size = _layout_info->_nonoop_acmp_map->length() * 2;\n+    int oop_acmp_map_size = _layout_info->_oop_acmp_map->length();\n+    typeArrayOop map = oopFactory::new_intArray(nonoop_acmp_map_size + oop_acmp_map_size + 1, CHECK);\n+    typeArrayHandle map_h(THREAD, map);\n+    map_h->int_at_put(0, _layout_info->_nonoop_acmp_map->length());\n+    for (int i = 0; i < _layout_info->_nonoop_acmp_map->length(); i++) {\n+      map_h->int_at_put(i * 2 + 1, _layout_info->_nonoop_acmp_map->at(i).first);\n+      map_h->int_at_put(i * 2 + 2, _layout_info->_nonoop_acmp_map->at(i).second);\n+    }\n+    int oop_map_start = nonoop_acmp_map_size + 1;\n+    for (int i = 0; i < _layout_info->_oop_acmp_map->length(); i++) {\n+      map_h->int_at_put(oop_map_start + i, _layout_info->_oop_acmp_map->at(i));\n+    }\n+    ik->java_mirror()->obj_field_put(ik->acmp_maps_offset(), map_h());\n+  }\n+\n@@ -5336,0 +5661,1 @@\n+  _loadable_descriptors(nullptr),\n@@ -5338,0 +5664,1 @@\n+  _local_interface_indexes(nullptr),\n@@ -5347,1 +5674,2 @@\n-  _field_info(nullptr),\n+  _layout_info(nullptr),\n+  _inline_layout_info_array(nullptr),\n@@ -5376,0 +5704,5 @@\n+  _has_strict_static_fields(false),\n+  _has_inline_type_fields(false),\n+  _is_naturally_atomic(false),\n+  _must_be_atomic(true),\n+  _has_loosely_consistent_annotation(false),\n@@ -5413,0 +5746,1 @@\n+  _loadable_descriptors = nullptr;\n@@ -5417,0 +5751,1 @@\n+  _inline_layout_info_array = nullptr;\n@@ -5436,0 +5771,4 @@\n+  if (_inline_layout_info_array != nullptr) {\n+    MetadataFactory::free_array<InlineLayoutInfo>(_loader_data, _inline_layout_info_array);\n+  }\n+\n@@ -5458,0 +5797,4 @@\n+  if (_loadable_descriptors != nullptr && _loadable_descriptors != Universe::the_empty_short_array()) {\n+    MetadataFactory::free_array<u2>(_loader_data, _loadable_descriptors);\n+  }\n+\n@@ -5557,0 +5900,9 @@\n+  \/\/ Fixing ACC_SUPER\/ACC_IDENTITY for old class files\n+  if (!supports_inline_types()) {\n+    const bool is_module = (flags & JVM_ACC_MODULE) != 0;\n+    const bool is_interface = (flags & JVM_ACC_INTERFACE) != 0;\n+    if (!is_module && !is_interface) {\n+      flags |= JVM_ACC_IDENTITY;\n+    }\n+  }\n+\n@@ -5660,2 +6012,0 @@\n-  assert(_local_interfaces != nullptr, \"invariant\");\n-\n@@ -5664,1 +6014,1 @@\n-               _access_flags.is_interface(),\n+               _access_flags,\n@@ -5674,1 +6024,3 @@\n-                _access_flags.is_interface(),\n+                is_interface(),\n+                !is_identity_class(),\n+                is_abstract_class(),\n@@ -5762,1 +6114,1 @@\n-    if (_access_flags.is_interface()) {\n+    if (is_interface()) {\n@@ -5784,0 +6136,14 @@\n+    if (_super_klass->is_interface()) {\n+      classfile_icce_error(\"class %s has interface %s as super class\", _super_klass, THREAD);\n+      return;\n+    }\n+\n+    if (_super_klass->is_final()) {\n+      classfile_icce_error(\"class %s cannot inherit from final class %s\", _super_klass, THREAD);\n+      return;\n+    }\n+\n+    if (EnableValhalla) {\n+      check_identity_and_value_modifiers(this, _super_klass, CHECK);\n+    }\n+\n@@ -5787,0 +6153,1 @@\n+  }\n@@ -5788,3 +6155,24 @@\n-    if (_super_klass->is_interface()) {\n-      classfile_icce_error(\"class %s has interface %s as super class\", _super_klass, THREAD);\n-      return;\n+  if (_parsed_annotations->has_annotation(AnnotationCollector::_jdk_internal_LooselyConsistentValue) && _access_flags.is_identity_class()) {\n+    THROW_MSG(vmSymbols::java_lang_ClassFormatError(),\n+          err_msg(\"class %s cannot have annotation jdk.internal.vm.annotation.LooselyConsistentValue, because it is not a value class\",\n+                  _class_name->as_klass_external_name()));\n+  }\n+\n+  \/\/ Determining is the class allows tearing or not (default is not)\n+  if (EnableValhalla && !_access_flags.is_identity_class()) {\n+    if (_parsed_annotations->has_annotation(ClassAnnotationCollector::_jdk_internal_LooselyConsistentValue)\n+        && (_super_klass == vmClasses::Object_klass() || !_super_klass->must_be_atomic())) {\n+      \/\/ Conditions above are not sufficient to determine atomicity requirements,\n+      \/\/ the presence of fields with atomic requirements could force the current class to have atomicy requirements too\n+      \/\/ Marking as not needing atomicity for now, can be updated when computing the fields layout\n+      \/\/ The InstanceKlass must be filled with the value from the FieldLayoutInfo returned by\n+      \/\/ the FieldLayoutBuilder, not with this _must_be_atomic field.\n+      _must_be_atomic = false;\n+    }\n+    \/\/ Apply VM options override\n+    if (*ForceNonTearable != '\\0') {\n+      \/\/ Allow a command line switch to force the same atomicity property:\n+      const char* class_name_str = _class_name->as_C_string();\n+      if (StringUtils::class_list_match(ForceNonTearable, class_name_str)) {\n+        _must_be_atomic = true;\n+      }\n@@ -5794,0 +6182,46 @@\n+  int itfs_len = _local_interface_indexes == nullptr ? 0 : _local_interface_indexes->length();\n+  _local_interfaces = MetadataFactory::new_array<InstanceKlass*>(_loader_data, itfs_len, nullptr, CHECK);\n+  if (_local_interface_indexes != nullptr) {\n+    for (int i = 0; i < _local_interface_indexes->length(); i++) {\n+      u2 interface_index = _local_interface_indexes->at(i);\n+      Klass* interf;\n+      if (cp->tag_at(interface_index).is_klass()) {\n+        interf = cp->resolved_klass_at(interface_index);\n+      } else {\n+        Symbol* const unresolved_klass  = cp->klass_name_at(interface_index);\n+\n+        \/\/ Don't need to check legal name because it's checked when parsing constant pool.\n+        \/\/ But need to make sure it's not an array type.\n+        guarantee_property(unresolved_klass->char_at(0) != JVM_SIGNATURE_ARRAY,\n+                            \"Bad interface name in class file %s\", CHECK);\n+\n+        \/\/ Call resolve on the interface class name with class circularity checking\n+        interf = SystemDictionary::resolve_super_or_fail(\n+                                                  _class_name,\n+                                                  unresolved_klass,\n+                                                  Handle(THREAD, _loader_data->class_loader()),\n+                                                  false,\n+                                                  CHECK);\n+      }\n+\n+      if (!interf->is_interface()) {\n+        THROW_MSG(vmSymbols::java_lang_IncompatibleClassChangeError(),\n+                  err_msg(\"class %s can not implement %s, because it is not an interface (%s)\",\n+                          _class_name->as_klass_external_name(),\n+                          interf->external_name(),\n+                          interf->class_in_module_of_loader()));\n+      }\n+\n+      if (EnableValhalla) {\n+        \/\/ Check modifiers and set carries_identity_modifier\/carries_value_modifier flags\n+        check_identity_and_value_modifiers(this, InstanceKlass::cast(interf), CHECK);\n+      }\n+\n+      if (InstanceKlass::cast(interf)->has_nonstatic_concrete_methods()) {\n+        _has_nonstatic_concrete_methods = true;\n+      }\n+      _local_interfaces->at_put(i, InstanceKlass::cast(interf));\n+    }\n+  }\n+  assert(_local_interfaces != nullptr, \"invariant\");\n+\n@@ -5821,1 +6255,1 @@\n-  _itable_size = _access_flags.is_interface() ? 0 :\n+  _itable_size = is_interface() ? 0 :\n@@ -5826,3 +6260,92 @@\n-  _field_info = new FieldLayoutInfo();\n-  FieldLayoutBuilder lb(class_name(), super_klass(), _cp, \/*_fields*\/ _temp_field_info,\n-                        _parsed_annotations->is_contended(), _field_info);\n+  if (EnableValhalla) {\n+    _inline_layout_info_array = MetadataFactory::new_array<InlineLayoutInfo>(_loader_data,\n+                                                   java_fields_count(),\n+                                                   CHECK);\n+    for (GrowableArrayIterator<FieldInfo> it = _temp_field_info->begin(); it != _temp_field_info->end(); ++it) {\n+      FieldInfo fieldinfo = *it;\n+      if (fieldinfo.access_flags().is_static()) continue;  \/\/ Only non-static fields are processed at load time\n+      Symbol* sig = fieldinfo.signature(cp);\n+      if (fieldinfo.field_flags().is_null_free_inline_type()) {\n+        \/\/ Pre-load classes of null-free fields that are candidate for flattening\n+        TempNewSymbol s = Signature::strip_envelope(sig);\n+        if (s == _class_name) {\n+          THROW_MSG(vmSymbols::java_lang_ClassCircularityError(),\n+                    err_msg(\"Class %s cannot have a null-free non-static field of its own type\", _class_name->as_C_string()));\n+        }\n+        log_info(class, preload)(\"Preloading of class %s during loading of class %s. \"\n+                                  \"Cause: a null-free non-static field is declared with this type\",\n+                                  s->as_C_string(), _class_name->as_C_string());\n+        InstanceKlass* klass = SystemDictionary::resolve_with_circularity_detection(_class_name, s,\n+                                                                                    Handle(THREAD,\n+                                                                                    _loader_data->class_loader()),\n+                                                                                    false, THREAD);\n+        if (HAS_PENDING_EXCEPTION) {\n+          log_info(class, preload)(\"Preloading of class %s during loading of class %s \"\n+                                      \"(cause: null-free non-static field) failed: %s\",\n+                                      s->as_C_string(), _class_name->as_C_string(),\n+                                      PENDING_EXCEPTION->klass()->name()->as_C_string());\n+          return; \/\/ Exception is still pending\n+        }\n+        assert(klass != nullptr, \"Sanity check\");\n+        InstanceKlass::check_can_be_annotated_with_NullRestricted(klass, _class_name, CHECK);\n+        InlineKlass* vk = InlineKlass::cast(klass);\n+        _inline_layout_info_array->adr_at(fieldinfo.index())->set_klass(vk);\n+        log_info(class, preload)(\"Preloading of class %s during loading of class %s \"\n+                                 \"(cause: null-free non-static field) succeeded\",\n+                                 s->as_C_string(), _class_name->as_C_string());\n+      } else if (Signature::has_envelope(sig) && PreloadClasses) {\n+        \/\/ Preloading classes for nullable fields that are listed in the LoadableDescriptors attribute\n+        \/\/ Those classes would be required later for the flattening of nullable inline type fields\n+        TempNewSymbol name = Signature::strip_envelope(sig);\n+        if (name != _class_name && is_class_in_loadable_descriptors_attribute(sig)) {\n+          log_info(class, preload)(\"Preloading of class %s during loading of class %s. \"\n+                                   \"Cause: field type in LoadableDescriptors attribute\",\n+                                   name->as_C_string(), _class_name->as_C_string());\n+          oop loader = loader_data()->class_loader();\n+          Klass* klass = SystemDictionary::resolve_super_or_fail(_class_name, name,\n+                                                                 Handle(THREAD, loader),\n+                                                                 false, THREAD);\n+          if (klass != nullptr) {\n+            if (klass->is_inline_klass()) {\n+              _inline_layout_info_array->adr_at(fieldinfo.index())->set_klass(InlineKlass::cast(klass));\n+              log_info(class, preload)(\"Preloading of class %s during loading of class %s \"\n+                                       \"(cause: field type in LoadableDescriptors attribute) succeeded\",\n+                                       name->as_C_string(), _class_name->as_C_string());\n+            } else {\n+              \/\/ Non value class are allowed by the current spec, but it could be an indication of an issue so let's log a warning\n+              log_info(class, preload)(\"Preloading of class %s during loading of class %s \"\n+                                          \"(cause: field type in LoadableDescriptors attribute) but loaded class is not a value class\",\n+                                          name->as_C_string(), _class_name->as_C_string());\n+            }\n+          } else {\n+            log_info(class, preload)(\"Preloading of class %s during loading of class %s \"\n+                                        \"(cause: field type in LoadableDescriptors attribute) failed : %s\",\n+                                        name->as_C_string(), _class_name->as_C_string(),\n+                                        PENDING_EXCEPTION->klass()->name()->as_C_string());\n+          }\n+          \/\/ Loads triggered by the LoadableDescriptors attribute are speculative, failures must not impact loading of current class\n+          if (HAS_PENDING_EXCEPTION) {\n+            CLEAR_PENDING_EXCEPTION;\n+          }\n+        } else {\n+          \/\/ Just poking the system dictionary to see if the class has already be loaded. Looking for migrated classes\n+          \/\/ used when --enable-preview when jdk isn't compiled with --enable-preview so doesn't include LoadableDescriptors.\n+          \/\/ This is temporary.\n+          oop loader = loader_data()->class_loader();\n+          InstanceKlass* klass = SystemDictionary::find_instance_klass(THREAD, name, Handle(THREAD, loader));\n+          if (klass != nullptr && klass->is_inline_klass()) {\n+            _inline_layout_info_array->adr_at(fieldinfo.index())->set_klass(InlineKlass::cast(klass));\n+            log_info(class, preload)(\"Preloading of class %s during loading of class %s \"\n+                                     \"(cause: field type not in LoadableDescriptors attribute) succeeded\",\n+                                     name->as_C_string(), _class_name->as_C_string());\n+          }\n+        }\n+      }\n+    }\n+  }\n+\n+  _layout_info = new FieldLayoutInfo();\n+  FieldLayoutBuilder lb(class_name(), loader_data(), super_klass(), _cp, \/*_fields*\/ _temp_field_info,\n+      _parsed_annotations->is_contended(), is_inline_type(),\n+      access_flags().is_abstract() && !access_flags().is_identity_class() && !access_flags().is_interface(),\n+      _must_be_atomic, _layout_info, _inline_layout_info_array);\n@@ -5830,0 +6353,1 @@\n+  _has_inline_type_fields = _layout_info->_has_inline_fields;\n@@ -5839,0 +6363,21 @@\n+\n+  \/\/ Strict static fields track initialization status from the beginning of time.\n+  \/\/ After this class runs <clinit>, they will be verified as being \"not unset\".\n+  \/\/ See Step 8 of InstanceKlass::initialize_impl.\n+  if (_has_strict_static_fields) {\n+    bool found_one = false;\n+    for (int i = 0; i < _temp_field_info->length(); i++) {\n+      FieldInfo& fi = *_temp_field_info->adr_at(i);\n+      if (fi.access_flags().is_strict() && fi.access_flags().is_static()) {\n+        found_one = true;\n+        if (fi.initializer_index() != 0) {\n+          \/\/ skip strict static fields with ConstantValue attributes\n+        } else {\n+          _fields_status->adr_at(fi.index())->update_strict_static_unset(true);\n+          _fields_status->adr_at(fi.index())->update_strict_static_unread(true);\n+        }\n+      }\n+    }\n+    assert(found_one == _has_strict_static_fields,\n+           \"correct prediction = %d\", (int)_has_strict_static_fields);\n+  }\n","filename":"src\/hotspot\/share\/classfile\/classFileParser.cpp","additions":662,"deletions":117,"binary":false,"changes":779,"status":"modified"},{"patch":"@@ -56,0 +56,2 @@\n+#include \"oops\/flatArrayKlass.hpp\"\n+#include \"oops\/inlineKlass.inline.hpp\"\n@@ -57,1 +59,1 @@\n-#include \"oops\/instanceMirrorKlass.hpp\"\n+#include \"oops\/instanceMirrorKlass.inline.hpp\"\n@@ -872,0 +874,1 @@\n+int java_lang_Class::_is_identity_offset;\n@@ -1099,0 +1102,1 @@\n+  set_is_identity(mirror(), k->is_array_klass() || k->is_identity_class());\n@@ -1107,1 +1111,9 @@\n-    if (k->is_typeArray_klass()) {\n+    if (k->is_flatArray_klass()) {\n+      Klass* element_klass = (Klass*) FlatArrayKlass::cast(k)->element_klass();\n+      assert(element_klass->is_inline_klass(), \"Must be inline type component\");\n+      if (is_scratch) {\n+        comp_mirror = Handle(THREAD, HeapShared::scratch_java_mirror(element_klass));\n+      } else {\n+        comp_mirror = Handle(THREAD, element_klass->java_mirror());\n+      }\n+    } else if (k->is_typeArray_klass()) {\n@@ -1116,0 +1128,1 @@\n+      assert(!k->is_refArray_klass() || !k->is_flatArray_klass(), \"Must not have mirror\");\n@@ -1118,0 +1131,1 @@\n+      oop comp_oop = element_klass->java_mirror();\n@@ -1121,1 +1135,1 @@\n-        comp_mirror = Handle(THREAD, element_klass->java_mirror());\n+        comp_mirror = Handle(THREAD, comp_oop);\n@@ -1150,1 +1164,0 @@\n-\n@@ -1154,0 +1167,9 @@\n+\n+    if (k->is_refined_objArray_klass()) {\n+      Klass* super_klass = k->super();\n+      assert(super_klass != nullptr, \"Must be\");\n+      Handle mirror(THREAD, super_klass->java_mirror());\n+      k->set_java_mirror(mirror);\n+      return;\n+    }\n+\n@@ -1176,0 +1198,1 @@\n+\n@@ -1199,3 +1222,3 @@\n-  if (k->class_loader() != nullptr &&\n-      k->class_loader() != SystemDictionary::java_platform_loader() &&\n-      k->class_loader() != SystemDictionary::java_system_loader()) {\n+  if ((k->class_loader() != nullptr &&\n+       k->class_loader() != SystemDictionary::java_platform_loader() &&\n+       k->class_loader() != SystemDictionary::java_system_loader())) {\n@@ -1240,1 +1263,0 @@\n-  assert(as_Klass(m) == k, \"must be\");\n@@ -1244,0 +1266,1 @@\n+    assert(as_Klass(m) == k, \"must be\");\n@@ -1253,0 +1276,4 @@\n+  } else {\n+    ObjArrayKlass* objarray_k = (ObjArrayKlass*)as_Klass(m);\n+    \/\/ Mirror should be restored for an ObjArrayKlass or one of its refined array klasses\n+    assert(objarray_k == k || objarray_k->next_refined_array_klass() == k, \"must be\");\n@@ -1390,0 +1417,4 @@\n+void java_lang_Class::set_is_identity(oop java_class, bool value) {\n+  assert(_is_identity_offset != 0, \"must be set\");\n+  java_class->bool_field_put(_is_identity_offset, value);\n+}\n@@ -1431,1 +1462,3 @@\n-  if (is_instance)  st->print(\"L\");\n+  if (is_instance)  {\n+    st->print(\"L\");\n+  }\n@@ -1490,0 +1523,1 @@\n+  assert(!klass->is_refined_objArray_klass(), \"should not be ref or flat array klass\");\n@@ -1549,1 +1583,2 @@\n-  macro(_is_primitive_offset,        k, \"primitive\",           bool_signature,         false);\n+  macro(_is_primitive_offset,        k, \"primitive\",           bool_signature,         false); \\\n+  macro(_is_identity_offset,         k, \"identity\",            bool_signature,         false);\n@@ -2874,1 +2909,1 @@\n-      if (method->name() == vmSymbols::object_initializer_name() &&\n+      if (method->is_object_constructor() &&\n@@ -3233,2 +3268,2 @@\n-  if (m->is_object_initializer()) {\n-    flags |= java_lang_invoke_MemberName::MN_IS_CONSTRUCTOR;\n+  if (m->is_object_constructor()) {\n+    flags |= java_lang_invoke_MemberName::MN_IS_OBJECT_CONSTRUCTOR;\n@@ -3625,1 +3660,1 @@\n-int java_lang_reflect_Field::_trusted_final_offset;\n+int java_lang_reflect_Field::_flags_offset;\n@@ -3635,1 +3670,1 @@\n-  macro(_trusted_final_offset,    k, vmSymbols::trusted_final_name(),    bool_signature,       false); \\\n+  macro(_flags_offset,     k, vmSymbols::flags_name(),     int_signature,    false); \\\n@@ -3700,2 +3735,2 @@\n-void java_lang_reflect_Field::set_trusted_final(oop field) {\n-  field->bool_field_put(_trusted_final_offset, true);\n+void java_lang_reflect_Field::set_flags(oop field, int value) {\n+  field->int_field_put(_flags_offset, value);\n@@ -4001,2 +4036,1 @@\n-int java_lang_boxing_object::_value_offset;\n-int java_lang_boxing_object::_long_value_offset;\n+int* java_lang_boxing_object::_offsets;\n@@ -4004,3 +4038,9 @@\n-#define BOXING_FIELDS_DO(macro) \\\n-  macro(_value_offset,      integerKlass, \"value\", int_signature, false); \\\n-  macro(_long_value_offset, longKlass, \"value\", long_signature, false);\n+#define BOXING_FIELDS_DO(macro)                                                                                                    \\\n+  macro(java_lang_boxing_object::_offsets[T_BOOLEAN - T_BOOLEAN], vmClasses::Boolean_klass(),   \"value\", bool_signature,   false); \\\n+  macro(java_lang_boxing_object::_offsets[T_CHAR - T_BOOLEAN],    vmClasses::Character_klass(), \"value\", char_signature,   false); \\\n+  macro(java_lang_boxing_object::_offsets[T_FLOAT - T_BOOLEAN],   vmClasses::Float_klass(),     \"value\", float_signature,  false); \\\n+  macro(java_lang_boxing_object::_offsets[T_DOUBLE - T_BOOLEAN],  vmClasses::Double_klass(),    \"value\", double_signature, false); \\\n+  macro(java_lang_boxing_object::_offsets[T_BYTE - T_BOOLEAN],    vmClasses::Byte_klass(),      \"value\", byte_signature,   false); \\\n+  macro(java_lang_boxing_object::_offsets[T_SHORT - T_BOOLEAN],   vmClasses::Short_klass(),     \"value\", short_signature,  false); \\\n+  macro(java_lang_boxing_object::_offsets[T_INT - T_BOOLEAN],     vmClasses::Integer_klass(),   \"value\", int_signature,    false); \\\n+  macro(java_lang_boxing_object::_offsets[T_LONG - T_BOOLEAN],    vmClasses::Long_klass(),      \"value\", long_signature,   false);\n@@ -4009,2 +4049,2 @@\n-  InstanceKlass* integerKlass = vmClasses::Integer_klass();\n-  InstanceKlass* longKlass = vmClasses::Long_klass();\n+  assert(T_LONG - T_BOOLEAN == 7, \"Sanity check\");\n+  java_lang_boxing_object::_offsets = NEW_C_HEAP_ARRAY(int, 8, mtInternal);\n@@ -4016,0 +4056,4 @@\n+  if (f->reading()) {\n+    assert(T_LONG - T_BOOLEAN == 7, \"Sanity check\");\n+    java_lang_boxing_object::_offsets = NEW_C_HEAP_ARRAY(int, 8, mtInternal);\n+  }\n@@ -4036,1 +4080,1 @@\n-      box->bool_field_put(_value_offset, value->z);\n+      box->bool_field_put(value_offset(type), value->z);\n@@ -4039,1 +4083,1 @@\n-      box->char_field_put(_value_offset, value->c);\n+      box->char_field_put(value_offset(type), value->c);\n@@ -4042,1 +4086,1 @@\n-      box->float_field_put(_value_offset, value->f);\n+      box->float_field_put(value_offset(type), value->f);\n@@ -4045,1 +4089,1 @@\n-      box->double_field_put(_long_value_offset, value->d);\n+      box->double_field_put(value_offset(type), value->d);\n@@ -4048,1 +4092,1 @@\n-      box->byte_field_put(_value_offset, value->b);\n+      box->byte_field_put(value_offset(type), value->b);\n@@ -4051,1 +4095,1 @@\n-      box->short_field_put(_value_offset, value->s);\n+      box->short_field_put(value_offset(type), value->s);\n@@ -4054,1 +4098,1 @@\n-      box->int_field_put(_value_offset, value->i);\n+      box->int_field_put(value_offset(type), value->i);\n@@ -4057,1 +4101,1 @@\n-      box->long_field_put(_long_value_offset, value->j);\n+      box->long_field_put(value_offset(type), value->j);\n@@ -4079,1 +4123,1 @@\n-    value->z = box->bool_field(_value_offset);\n+    value->z = box->bool_field(value_offset(type));\n@@ -4082,1 +4126,1 @@\n-    value->c = box->char_field(_value_offset);\n+    value->c = box->char_field(value_offset(type));\n@@ -4085,1 +4129,1 @@\n-    value->f = box->float_field(_value_offset);\n+    value->f = box->float_field(value_offset(type));\n@@ -4088,1 +4132,1 @@\n-    value->d = box->double_field(_long_value_offset);\n+    value->d = box->double_field(value_offset(type));\n@@ -4091,1 +4135,1 @@\n-    value->b = box->byte_field(_value_offset);\n+    value->b = box->byte_field(value_offset(type));\n@@ -4094,1 +4138,1 @@\n-    value->s = box->short_field(_value_offset);\n+    value->s = box->short_field(value_offset(type));\n@@ -4097,1 +4141,1 @@\n-    value->i = box->int_field(_value_offset);\n+    value->i = box->int_field(value_offset(type));\n@@ -4100,1 +4144,1 @@\n-    value->j = box->long_field(_long_value_offset);\n+    value->j = box->long_field(value_offset(type));\n@@ -4113,1 +4157,1 @@\n-    box->bool_field_put(_value_offset, value->z);\n+    box->bool_field_put(value_offset(type), value->z);\n@@ -4116,1 +4160,1 @@\n-    box->char_field_put(_value_offset, value->c);\n+    box->char_field_put(value_offset(type), value->c);\n@@ -4119,1 +4163,1 @@\n-    box->float_field_put(_value_offset, value->f);\n+    box->float_field_put(value_offset(type), value->f);\n@@ -4122,1 +4166,1 @@\n-    box->double_field_put(_long_value_offset, value->d);\n+    box->double_field_put(value_offset(type), value->d);\n@@ -4125,1 +4169,1 @@\n-    box->byte_field_put(_value_offset, value->b);\n+    box->byte_field_put(value_offset(type), value->b);\n@@ -4128,1 +4172,1 @@\n-    box->short_field_put(_value_offset, value->s);\n+    box->short_field_put(value_offset(type), value->s);\n@@ -4131,1 +4175,1 @@\n-    box->int_field_put(_value_offset, value->i);\n+    box->int_field_put(value_offset(type), value->i);\n@@ -4134,1 +4178,1 @@\n-    box->long_field_put(_long_value_offset, value->j);\n+    box->long_field_put(value_offset(type), value->j);\n@@ -4530,1 +4574,0 @@\n-\n@@ -4540,1 +4583,1 @@\n-  return (flags(mname) & (MN_IS_METHOD | MN_IS_CONSTRUCTOR)) > 0;\n+  return (flags(mname) & (MN_IS_METHOD | MN_IS_OBJECT_CONSTRUCTOR)) > 0;\n@@ -5533,16 +5576,11 @@\n-#define CHECK_OFFSET(klass_name, cpp_klass_name, field_name, field_sig) \\\n-  valid &= check_offset(klass_name, cpp_klass_name :: _##field_name ## _offset, #field_name, field_sig)\n-\n-#define CHECK_LONG_OFFSET(klass_name, cpp_klass_name, field_name, field_sig) \\\n-  valid &= check_offset(klass_name, cpp_klass_name :: _##long_ ## field_name ## _offset, #field_name, field_sig)\n-\n-  \/\/ Boxed primitive objects (java_lang_boxing_object)\n-\n-  CHECK_OFFSET(\"java\/lang\/Boolean\",   java_lang_boxing_object, value, \"Z\");\n-  CHECK_OFFSET(\"java\/lang\/Character\", java_lang_boxing_object, value, \"C\");\n-  CHECK_OFFSET(\"java\/lang\/Float\",     java_lang_boxing_object, value, \"F\");\n-  CHECK_LONG_OFFSET(\"java\/lang\/Double\", java_lang_boxing_object, value, \"D\");\n-  CHECK_OFFSET(\"java\/lang\/Byte\",      java_lang_boxing_object, value, \"B\");\n-  CHECK_OFFSET(\"java\/lang\/Short\",     java_lang_boxing_object, value, \"S\");\n-  CHECK_OFFSET(\"java\/lang\/Integer\",   java_lang_boxing_object, value, \"I\");\n-  CHECK_LONG_OFFSET(\"java\/lang\/Long\", java_lang_boxing_object, value, \"J\");\n+#define CHECK_OFFSET(klass_name, type, field_sig) \\\n+  valid &= check_offset(klass_name, java_lang_boxing_object::value_offset(type), \"value\", field_sig)\n+\n+  CHECK_OFFSET(\"java\/lang\/Boolean\",   T_BOOLEAN, \"Z\");\n+  CHECK_OFFSET(\"java\/lang\/Character\", T_CHAR,    \"C\");\n+  CHECK_OFFSET(\"java\/lang\/Float\",     T_FLOAT,   \"F\");\n+  CHECK_OFFSET(\"java\/lang\/Double\",    T_DOUBLE,  \"D\");\n+  CHECK_OFFSET(\"java\/lang\/Byte\",      T_BYTE,    \"B\");\n+  CHECK_OFFSET(\"java\/lang\/Short\",     T_SHORT,   \"S\");\n+  CHECK_OFFSET(\"java\/lang\/Integer\",   T_INT,     \"I\");\n+  CHECK_OFFSET(\"java\/lang\/Long\",      T_LONG,    \"J\");\n","filename":"src\/hotspot\/share\/classfile\/javaClasses.cpp","additions":104,"deletions":66,"binary":false,"changes":170,"status":"modified"},{"patch":"@@ -216,0 +216,1 @@\n+  static inline bool is_instance_without_asserts(oop obj);\n@@ -255,0 +256,1 @@\n+\n@@ -262,0 +264,1 @@\n+  static int _is_identity_offset;\n@@ -276,0 +279,1 @@\n+  static void set_is_identity(oop java_class, bool value);\n@@ -319,0 +323,1 @@\n+\n@@ -835,1 +840,1 @@\n-  static int _trusted_final_offset;\n+  static int _flags_offset;\n@@ -863,1 +868,1 @@\n-  static void set_trusted_final(oop field);\n+  static void set_flags(oop field, int value);\n@@ -985,2 +990,1 @@\n-  static int _value_offset;\n-  static int _long_value_offset;\n+  static int* _offsets;\n@@ -1003,1 +1007,3 @@\n-    return is_double_word_type(type) ? _long_value_offset : _value_offset;\n+    assert(type >= T_BOOLEAN && type <= T_LONG, \"BasicType out of range\");\n+    assert(_offsets != nullptr, \"Uninitialized offsets\");\n+    return _offsets[type - T_BOOLEAN];\n@@ -1365,1 +1371,1 @@\n-    MN_IS_CONSTRUCTOR        = 0x00020000, \/\/ constructor\n+    MN_IS_OBJECT_CONSTRUCTOR = 0x00020000, \/\/ constructor\n@@ -1371,0 +1377,1 @@\n+    MN_NULL_RESTRICTED_FIELD = 0x00800000, \/\/ null-restricted field\n@@ -1372,1 +1379,3 @@\n-    MN_REFERENCE_KIND_MASK   = 0x0F000000 >> MN_REFERENCE_KIND_SHIFT,\n+    MN_REFERENCE_KIND_MASK   = 0x0F000000 >> MN_REFERENCE_KIND_SHIFT, \/\/ 4 bits\n+    MN_LAYOUT_SHIFT          = 28, \/\/ field layout\n+    MN_LAYOUT_MASK           = 0x70000000 >> MN_LAYOUT_SHIFT, \/\/ 3 bits\n@@ -1876,1 +1885,0 @@\n-\n","filename":"src\/hotspot\/share\/classfile\/javaClasses.hpp","additions":16,"deletions":8,"binary":false,"changes":24,"status":"modified"},{"patch":"@@ -130,0 +130,1 @@\n+  do_klass(ValueObjectMethods_klass,                    java_lang_runtime_ValueObjectMethods                  ) \\\n@@ -177,0 +178,24 @@\n+  \/* Other valhalla migrated klasses. *\/                                                                        \\\n+  do_klass(Number_klass,                                java_lang_Number                                      ) \\\n+  do_klass(Optional_klass,                              java_util_Optional                                    ) \\\n+  do_klass(OptionalInt_klass,                           java_util_OptionalInt                                 ) \\\n+  do_klass(OptionalLong_klass,                          java_util_OptionalLong                                ) \\\n+  do_klass(OptionalDouble_klass,                        java_util_OptionalDouble                              ) \\\n+  do_klass(LocalDate_klass,                             java_time_LocalDate                                   ) \\\n+  do_klass(LocalDateTime_klass,                         java_time_LocalDateTime                               ) \\\n+  do_klass(LocalTime_klass,                             java_time_LocalTime                                   ) \\\n+  do_klass(Duration_klass,                              java_time_Duration                                    ) \\\n+  do_klass(Instant_klass,                               java_time_Instant                                     ) \\\n+  do_klass(MonthDay_klass,                              java_time_MonthDay                                    ) \\\n+  do_klass(ZonedDateTime_klass,                         java_time_ZonedDateTime                               ) \\\n+  do_klass(OffsetDateTime_klass,                        java_time_OffsetDateTime                              ) \\\n+  do_klass(OffsetTime_klass,                            java_time_OffsetTime                                  ) \\\n+  do_klass(YearMonth_klass,                             java_time_YearMonth                                   ) \\\n+  do_klass(Year_klass,                                  java_time_Year                                        ) \\\n+  do_klass(Period_klass,                                java_time_Period                                      ) \\\n+  do_klass(chrono_ChronoLocalDateImpl_klass,            java_time_chrono_ChronoLocalDateImpl                  ) \\\n+  do_klass(chrono_MinguoDate_klass,                     java_time_chrono_MinguoDate                           ) \\\n+  do_klass(chrono_HijrahDate_klass,                     java_time_chrono_HijrahDate                           ) \\\n+  do_klass(chrono_JapaneseDate_klass,                   java_time_chrono_JapaneseDate                         ) \\\n+  do_klass(chrono_ThaiBuddhistDate_klass,               java_time_chrono_ThaiBuddhistDate                     ) \\\n+                                                                                                                \\\n","filename":"src\/hotspot\/share\/classfile\/vmClassMacros.hpp","additions":25,"deletions":0,"binary":false,"changes":25,"status":"modified"},{"patch":"@@ -94,0 +94,25 @@\n+  \/* Valhalla migrated classes. *\/                                                                \\\n+  template(java_lang_Number,                          \"java\/lang\/Number\")                         \\\n+  template(java_lang_Record,                          \"java\/lang\/Record\")                         \\\n+  template(java_util_Optional,                        \"java\/util\/Optional\")                       \\\n+  template(java_util_OptionalInt,                     \"java\/util\/OptionalInt\")                    \\\n+  template(java_util_OptionalLong,                    \"java\/util\/OptionalLong\")                   \\\n+  template(java_util_OptionalDouble,                  \"java\/util\/OptionalDouble\")                 \\\n+  template(java_time_LocalDate,                       \"java\/time\/LocalDate\")                      \\\n+  template(java_time_LocalDateTime,                   \"java\/time\/LocalDateTime\")                  \\\n+  template(java_time_LocalTime,                       \"java\/time\/LocalTime\")                      \\\n+  template(java_time_Duration,                        \"java\/time\/Duration\")                       \\\n+  template(java_time_Instant,                         \"java\/time\/Instant\")                        \\\n+  template(java_time_MonthDay,                        \"java\/time\/MonthDay\")                       \\\n+  template(java_time_ZonedDateTime,                   \"java\/time\/ZonedDateTime\")                  \\\n+  template(java_time_OffsetDateTime,                  \"java\/time\/OffsetDateTime\")                 \\\n+  template(java_time_OffsetTime,                      \"java\/time\/OffsetTime\")                     \\\n+  template(java_time_YearMonth,                       \"java\/time\/YearMonth\")                      \\\n+  template(java_time_Year,                            \"java\/time\/Year\")                           \\\n+  template(java_time_Period,                          \"java\/time\/Period\")                         \\\n+  template(java_time_chrono_ChronoLocalDateImpl,      \"java\/time\/chrono\/ChronoLocalDateImpl\")     \\\n+  template(java_time_chrono_MinguoDate,               \"java\/time\/chrono\/MinguoDate\")              \\\n+  template(java_time_chrono_HijrahDate,               \"java\/time\/chrono\/HijrahDate\")              \\\n+  template(java_time_chrono_JapaneseDate,             \"java\/time\/chrono\/JapaneseDate\")            \\\n+  template(java_time_chrono_ThaiBuddhistDate,         \"java\/time\/chrono\/ThaiBuddhistDate\")        \\\n+                                                                                                  \\\n@@ -140,1 +165,0 @@\n-  template(java_lang_Record,                          \"java\/lang\/Record\")                         \\\n@@ -170,0 +194,1 @@\n+  template(tag_loadable_descriptors,                  \"LoadableDescriptors\")                      \\\n@@ -206,0 +231,1 @@\n+  template(java_lang_IdentityException,               \"java\/lang\/IdentityException\")              \\\n@@ -254,0 +280,2 @@\n+  template(jdk_internal_vm_annotation_LooselyConsistentValue_signature,      \"Ljdk\/internal\/vm\/annotation\/LooselyConsistentValue;\") \\\n+  template(jdk_internal_vm_annotation_NullRestricted_signature,              \"Ljdk\/internal\/vm\/annotation\/NullRestricted;\") \\\n@@ -283,1 +311,0 @@\n-  template(trusted_final_name,                        \"trustedFinal\")                             \\\n@@ -505,0 +532,3 @@\n+  template(null_reset_value_name,                     \".null_reset\")                              \\\n+  template(acmp_maps_name,                            \".acmp_maps\")                               \\\n+  template(empty_marker_name,                         \".empty\")                                   \\\n@@ -581,0 +611,1 @@\n+  template(class_class_signature,                     \"(Ljava\/lang\/Class;)Ljava\/lang\/Class;\")     \\\n@@ -590,0 +621,1 @@\n+  template(object_object_boolean_signature,           \"(Ljava\/lang\/Object;Ljava\/lang\/Object;)Z\")                  \\\n@@ -717,0 +749,2 @@\n+  template(primaryType_name,                           \"primaryType\")                                             \\\n+  template(secondaryType_name,                         \"secondaryType\")                                           \\\n@@ -746,0 +780,7 @@\n+  template(java_lang_runtime_ValueObjectMethods,            \"java\/lang\/runtime\/ValueObjectMethods\")               \\\n+  template(isSubstitutable_name,                            \"isSubstitutable\")                                    \\\n+  template(isSubstitutableAlt_name,                         \"isSubstitutableAlt\")                                 \\\n+  template(valueObjectHashCode_name,                        \"valueObjectHashCode\")                                \\\n+  template(jdk_internal_value_PrimitiveClass,               \"jdk\/internal\/value\/PrimitiveClass\")                  \\\n+  template(jdk_internal_value_ValueClass,                   \"jdk\/internal\/value\/ValueClass\")                      \\\n+                                                                                                                  \\\n@@ -829,0 +870,4 @@\n+  static void initialize_migrated_class_names();\n+\n+  static const int _migrated_class_names_length = 31;\n+  static Symbol* _migrated_class_names[_migrated_class_names_length];\n@@ -864,0 +909,7 @@\n+\n+  template<typename Function>\n+  static void migrated_class_names_do(Function f) {\n+     for (int i = 0; i < _migrated_class_names_length; i++) {\n+       f(_migrated_class_names[i]);\n+     }\n+  }\n","filename":"src\/hotspot\/share\/classfile\/vmSymbols.hpp","additions":54,"deletions":2,"binary":false,"changes":56,"status":"modified"},{"patch":"@@ -65,0 +65,1 @@\n+static_assert(!std::is_polymorphic<BufferedInlineTypeBlob>::value,   \"no virtual methods are allowed in code blobs\");\n@@ -93,0 +94,1 @@\n+      &BufferedInlineTypeBlob::_vpntr,\n@@ -428,1 +430,1 @@\n-    blob = new (size) BufferBlob(name, CodeBlobKind::Buffer, cb, size);\n+    blob = new (size) BufferBlob(name, CodeBlobKind::Buffer, cb, size, sizeof(BufferBlob));\n@@ -444,0 +446,4 @@\n+BufferBlob::BufferBlob(const char* name, CodeBlobKind kind, CodeBuffer* cb, int size, uint16_t header_size, int frame_complete, int frame_size, OopMapSet* oop_maps, bool caller_must_gc_arguments)\n+  : RuntimeBlob(name, kind, cb, size, header_size, frame_complete, frame_size, oop_maps, caller_must_gc_arguments)\n+{}\n+\n@@ -448,3 +454,2 @@\n-AdapterBlob::AdapterBlob(int size, CodeBuffer* cb, int entry_offset[AdapterBlob::ENTRY_COUNT]) :\n-  BufferBlob(\"I2C\/C2I adapters\", CodeBlobKind::Adapter, cb, size, sizeof(AdapterBlob)) {\n-  assert(entry_offset[I2C] == 0, \"sanity check\");\n+AdapterBlob::AdapterBlob(int size, CodeBuffer* cb, int entry_offset[AdapterBlob::ENTRY_COUNT], int frame_complete, int frame_size, OopMapSet* oop_maps, bool caller_must_gc_arguments) :\n+  BufferBlob(\"I2C\/C2I adapters\", CodeBlobKind::Adapter, cb, size, sizeof(AdapterBlob), frame_complete, frame_size, oop_maps, caller_must_gc_arguments) {\n@@ -452,0 +457,1 @@\n+  assert(entry_offset[I2C] == 0, \"sanity check\");\n@@ -461,0 +467,2 @@\n+  _c2i_inline_offset = entry_offset[C2I_Inline];\n+  _c2i_inline_ro_offset = entry_offset[C2I_Inline_RO];\n@@ -462,0 +470,1 @@\n+  _c2i_unverified_inline_offset = entry_offset[C2I_Unverified_Inline];\n@@ -466,1 +475,1 @@\n-AdapterBlob* AdapterBlob::create(CodeBuffer* cb, int entry_offset[AdapterBlob::ENTRY_COUNT]) {\n+AdapterBlob* AdapterBlob::create(CodeBuffer* cb, int entry_offset[AdapterBlob::ENTRY_COUNT], int frame_complete, int frame_size, OopMapSet* oop_maps, bool caller_must_gc_arguments) {\n@@ -475,1 +484,1 @@\n-    blob = new (size) AdapterBlob(size, cb, entry_offset);\n+    blob = new (size) AdapterBlob(size, cb, entry_offset, frame_complete, frame_size, oop_maps, caller_must_gc_arguments);\n@@ -556,0 +565,25 @@\n+\/\/----------------------------------------------------------------------------------------------------\n+\/\/ Implementation of BufferedInlineTypeBlob\n+BufferedInlineTypeBlob::BufferedInlineTypeBlob(int size, CodeBuffer* cb, int pack_fields_off, int pack_fields_jobject_off, int unpack_fields_off) :\n+  BufferBlob(\"buffered inline type\", CodeBlobKind::BufferedInlineType, cb, size, sizeof(BufferedInlineTypeBlob)),\n+  _pack_fields_off(pack_fields_off),\n+  _pack_fields_jobject_off(pack_fields_jobject_off),\n+  _unpack_fields_off(unpack_fields_off) {\n+  CodeCache::commit(this);\n+}\n+\n+BufferedInlineTypeBlob* BufferedInlineTypeBlob::create(CodeBuffer* cb, int pack_fields_off, int pack_fields_jobject_off, int unpack_fields_off) {\n+  ThreadInVMfromUnknown __tiv;  \/\/ get to VM state in case we block on CodeCache_lock\n+\n+  BufferedInlineTypeBlob* blob = nullptr;\n+  unsigned int size = CodeBlob::allocation_size(cb, sizeof(BufferedInlineTypeBlob));\n+  {\n+    MutexLocker mu(CodeCache_lock, Mutex::_no_safepoint_check_flag);\n+    blob = new (size) BufferedInlineTypeBlob(size, cb, pack_fields_off, pack_fields_jobject_off, unpack_fields_off);\n+  }\n+  \/\/ Track memory usage statistic after releasing CodeCache_lock\n+  MemoryService::track_code_cache_memory_usage();\n+\n+  return blob;\n+}\n+\n","filename":"src\/hotspot\/share\/code\/codeBlob.cpp","additions":40,"deletions":6,"binary":false,"changes":46,"status":"modified"},{"patch":"@@ -716,0 +716,11 @@\n+\n+      \/\/ If inline types are passed as fields, use the extended signature\n+      \/\/ which contains the types of all (oop) fields of the inline type.\n+      if (is_compiled_by_c2() && callee->has_scalarized_args()) {\n+        const GrowableArray<SigEntry>* sig = callee->adapter()->get_sig_cc();\n+        assert(sig != nullptr, \"sig should never be null\");\n+        TempNewSymbol tmp_sig = SigEntry::create_symbol(sig);\n+        has_receiver = false; \/\/ The extended signature contains the receiver type\n+        fr.oops_compiled_arguments_do(tmp_sig, has_receiver, has_appendix, reg_map, f);\n+        return;\n+      }\n@@ -1246,0 +1257,4 @@\n+  _inline_entry_point             = entry_point();\n+  _verified_inline_entry_point    = verified_entry_point();\n+  _verified_inline_ro_entry_point = verified_entry_point();\n+\n@@ -1285,1 +1300,1 @@\n-\n+    assert(!method->has_scalarized_args(), \"scalarized native wrappers not supported yet\");\n@@ -1743,0 +1758,4 @@\n+    _inline_entry_point             = code_begin() + offsets->value(CodeOffsets::Inline_Entry);\n+    _verified_inline_entry_point    = code_begin() + offsets->value(CodeOffsets::Verified_Inline_Entry);\n+    _verified_inline_ro_entry_point = code_begin() + offsets->value(CodeOffsets::Verified_Inline_Entry_RO);\n+\n@@ -3188,4 +3207,4 @@\n-    if (deps.type() != Dependencies::evol_method)\n-      continue;\n-    Method* method = deps.method_argument(0);\n-    if (method == dependee) return true;\n+    if (Dependencies::has_method_dep(deps.type())) {\n+      Method* method = deps.method_argument(0);\n+      if (method == dependee) return true;\n+    }\n@@ -4029,0 +4048,1 @@\n+  if (pos == inline_entry_point())                                      label = \"[Inline Entry Point]\";\n@@ -4030,0 +4050,2 @@\n+  if (pos == verified_inline_entry_point())                             label = \"[Verified Inline Entry Point]\";\n+  if (pos == verified_inline_ro_entry_point())                          label = \"[Verified Inline Entry Point (RO)]\";\n@@ -4038,0 +4060,10 @@\n+static int maybe_print_entry_label(outputStream* stream, address pos, address entry, const char* label) {\n+  if (pos == entry) {\n+    stream->bol();\n+    stream->print_cr(\"%s\", label);\n+    return 1;\n+  } else {\n+    return 0;\n+  }\n+}\n+\n@@ -4040,4 +4072,13 @@\n-    const char* label = nmethod_section_label(block_begin);\n-    if (label != nullptr) {\n-      stream->bol();\n-      stream->print_cr(\"%s\", label);\n+    int n = 0;\n+    \/\/ Multiple entry points may be at the same position. Print them all.\n+    n += maybe_print_entry_label(stream, block_begin, entry_point(),                    \"[Entry Point]\");\n+    n += maybe_print_entry_label(stream, block_begin, inline_entry_point(),             \"[Inline Entry Point]\");\n+    n += maybe_print_entry_label(stream, block_begin, verified_entry_point(),           \"[Verified Entry Point]\");\n+    n += maybe_print_entry_label(stream, block_begin, verified_inline_entry_point(),    \"[Verified Inline Entry Point]\");\n+    n += maybe_print_entry_label(stream, block_begin, verified_inline_ro_entry_point(), \"[Verified Inline Entry Point (RO)]\");\n+    if (n == 0) {\n+      const char* label = nmethod_section_label(block_begin);\n+      if (label != nullptr) {\n+        stream->bol();\n+        stream->print_cr(\"%s\", label);\n+      }\n@@ -4047,6 +4088,62 @@\n-  if (block_begin == entry_point()) {\n-    Method* m = method();\n-    if (m != nullptr) {\n-      stream->print(\"  # \");\n-      m->print_value_on(stream);\n-      stream->cr();\n+  Method* m = method();\n+  if (m == nullptr || is_osr_method()) {\n+    return;\n+  }\n+\n+  \/\/ Print the name of the method (only once)\n+  address low = MIN3(entry_point(),\n+                     verified_entry_point(),\n+                     inline_entry_point());\n+  \/\/ The verified inline entry point and verified inline RO entry point are not always\n+  \/\/ used. When they are unused. CodeOffsets::Verified_Inline_Entry(_RO) is -1. Hence,\n+  \/\/ the calculated entry point is smaller than the block they are offsetting into.\n+  if (verified_inline_entry_point() >= block_begin) {\n+    low = MIN2(low, verified_inline_entry_point());\n+  }\n+  if (verified_inline_ro_entry_point() >= block_begin) {\n+    low = MIN2(low, verified_inline_ro_entry_point());\n+  }\n+  assert(low != 0, \"sanity\");\n+  if (block_begin == low) {\n+    stream->print(\"  # \");\n+    m->print_value_on(stream);\n+    stream->cr();\n+  }\n+\n+  \/\/ Print the arguments for the 3 types of verified entry points\n+  CompiledEntrySignature ces(m);\n+  ces.compute_calling_conventions(false);\n+  const GrowableArray<SigEntry>* sig_cc;\n+  const VMRegPair* regs;\n+  if (block_begin == verified_entry_point()) {\n+    sig_cc = ces.sig_cc();\n+    regs = ces.regs_cc();\n+  } else if (block_begin == verified_inline_entry_point()) {\n+    sig_cc = ces.sig();\n+    regs = ces.regs();\n+  } else if (block_begin == verified_inline_ro_entry_point()) {\n+    sig_cc = ces.sig_cc_ro();\n+    regs = ces.regs_cc_ro();\n+  } else {\n+    return;\n+  }\n+\n+  bool has_this = !m->is_static();\n+  if (ces.has_inline_recv() && block_begin == verified_entry_point()) {\n+    \/\/ <this> argument is scalarized for verified_entry_point()\n+    has_this = false;\n+  }\n+  const char* spname = \"sp\"; \/\/ make arch-specific?\n+  int stack_slot_offset = this->frame_size() * wordSize;\n+  int tab1 = 14, tab2 = 24;\n+  int sig_index = 0;\n+  int arg_index = has_this ? -1 : 0;\n+  bool did_old_sp = false;\n+  for (ExtendedSignature sig = ExtendedSignature(sig_cc, SigEntryFilter()); !sig.at_end(); ++sig) {\n+    bool at_this = (arg_index == -1);\n+    bool at_old_sp = false;\n+    BasicType t = (*sig)._bt;\n+    if (at_this) {\n+      stream->print(\"  # this: \");\n+    } else {\n+      stream->print(\"  # parm%d: \", arg_index);\n@@ -4054,19 +4151,7 @@\n-    if (m != nullptr && !is_osr_method()) {\n-      ResourceMark rm;\n-      int sizeargs = m->size_of_parameters();\n-      BasicType* sig_bt = NEW_RESOURCE_ARRAY(BasicType, sizeargs);\n-      VMRegPair* regs   = NEW_RESOURCE_ARRAY(VMRegPair, sizeargs);\n-      {\n-        int sig_index = 0;\n-        if (!m->is_static())\n-          sig_bt[sig_index++] = T_OBJECT; \/\/ 'this'\n-        for (SignatureStream ss(m->signature()); !ss.at_return_type(); ss.next()) {\n-          BasicType t = ss.type();\n-          sig_bt[sig_index++] = t;\n-          if (type2size[t] == 2) {\n-            sig_bt[sig_index++] = T_VOID;\n-          } else {\n-            assert(type2size[t] == 1, \"size is 1 or 2\");\n-          }\n-        }\n-        assert(sig_index == sizeargs, \"\");\n+    stream->move_to(tab1);\n+    VMReg fst = regs[sig_index].first();\n+    VMReg snd = regs[sig_index].second();\n+    if (fst->is_reg()) {\n+      stream->print(\"%s\", fst->name());\n+      if (snd->is_valid())  {\n+        stream->print(\":%s\", snd->name());\n@@ -4074,54 +4159,18 @@\n-      const char* spname = \"sp\"; \/\/ make arch-specific?\n-      SharedRuntime::java_calling_convention(sig_bt, regs, sizeargs);\n-      int stack_slot_offset = this->frame_size() * wordSize;\n-      int tab1 = 14, tab2 = 24;\n-      int sig_index = 0;\n-      int arg_index = (m->is_static() ? 0 : -1);\n-      bool did_old_sp = false;\n-      for (SignatureStream ss(m->signature()); !ss.at_return_type(); ) {\n-        bool at_this = (arg_index == -1);\n-        bool at_old_sp = false;\n-        BasicType t = (at_this ? T_OBJECT : ss.type());\n-        assert(t == sig_bt[sig_index], \"sigs in sync\");\n-        if (at_this)\n-          stream->print(\"  # this: \");\n-        else\n-          stream->print(\"  # parm%d: \", arg_index);\n-        stream->move_to(tab1);\n-        VMReg fst = regs[sig_index].first();\n-        VMReg snd = regs[sig_index].second();\n-        if (fst->is_reg()) {\n-          stream->print(\"%s\", fst->name());\n-          if (snd->is_valid())  {\n-            stream->print(\":%s\", snd->name());\n-          }\n-        } else if (fst->is_stack()) {\n-          int offset = fst->reg2stack() * VMRegImpl::stack_slot_size + stack_slot_offset;\n-          if (offset == stack_slot_offset)  at_old_sp = true;\n-          stream->print(\"[%s+0x%x]\", spname, offset);\n-        } else {\n-          stream->print(\"reg%d:%d??\", (int)(intptr_t)fst, (int)(intptr_t)snd);\n-        }\n-        stream->print(\" \");\n-        stream->move_to(tab2);\n-        stream->print(\"= \");\n-        if (at_this) {\n-          m->method_holder()->print_value_on(stream);\n-        } else {\n-          bool did_name = false;\n-          if (!at_this && ss.is_reference()) {\n-            Symbol* name = ss.as_symbol();\n-            name->print_value_on(stream);\n-            did_name = true;\n-          }\n-          if (!did_name)\n-            stream->print(\"%s\", type2name(t));\n-        }\n-        if (at_old_sp) {\n-          stream->print(\"  (%s of caller)\", spname);\n-          did_old_sp = true;\n-        }\n-        stream->cr();\n-        sig_index += type2size[t];\n-        arg_index += 1;\n-        if (!at_this)  ss.next();\n+    } else if (fst->is_stack()) {\n+      int offset = fst->reg2stack() * VMRegImpl::stack_slot_size + stack_slot_offset;\n+      if (offset == stack_slot_offset)  at_old_sp = true;\n+      stream->print(\"[%s+0x%x]\", spname, offset);\n+    } else {\n+      stream->print(\"reg%d:%d??\", (int)(intptr_t)fst, (int)(intptr_t)snd);\n+    }\n+    stream->print(\" \");\n+    stream->move_to(tab2);\n+    stream->print(\"= \");\n+    if (at_this) {\n+      m->method_holder()->print_value_on(stream);\n+    } else {\n+      bool did_name = false;\n+      if (is_reference_type(t)) {\n+        Symbol* name = (*sig)._name;\n+        name->print_value_on(stream);\n+        did_name = true;\n@@ -4129,6 +4178,4 @@\n-      if (!did_old_sp) {\n-        stream->print(\"  # \");\n-        stream->move_to(tab1);\n-        stream->print(\"[%s+0x%x]\", spname, stack_slot_offset);\n-        stream->print(\"  (%s of caller)\", spname);\n-        stream->cr();\n+      if (!did_name)\n+        stream->print(\"%s\", type2name(t));\n+      if ((*sig)._null_marker) {\n+        stream->print(\" (null marker)\");\n@@ -4137,0 +4184,14 @@\n+    if (at_old_sp) {\n+      stream->print(\"  (%s of caller)\", spname);\n+      did_old_sp = true;\n+    }\n+    stream->cr();\n+    sig_index += type2size[t];\n+    arg_index += 1;\n+  }\n+  if (!did_old_sp) {\n+    stream->print(\"  # \");\n+    stream->move_to(tab1);\n+    stream->print(\"[%s+0x%x]\", spname, stack_slot_offset);\n+    stream->print(\"  (%s of caller)\", spname);\n+    stream->cr();\n@@ -4260,1 +4321,1 @@\n-      st->print(\" {reexecute=%d rethrow=%d return_oop=%d}\", sd->should_reexecute(), sd->rethrow_exception(), sd->return_oop());\n+      st->print(\" {reexecute=%d rethrow=%d return_oop=%d return_scalarized=%d}\", sd->should_reexecute(), sd->rethrow_exception(), sd->return_oop(), sd->return_scalarized());\n","filename":"src\/hotspot\/share\/code\/nmethod.cpp","additions":156,"deletions":95,"binary":false,"changes":251,"status":"modified"},{"patch":"@@ -30,0 +30,1 @@\n+#include \"compiler\/compilerDefinitions.hpp\"\n@@ -219,0 +220,4 @@\n+  \/\/ TODO: can these be uint16_t, seem rely on -1 CodeOffset, can change later...\n+  address _inline_entry_point;              \/\/ inline type entry point (unpack all inline type args) with class check\n+  address _verified_inline_entry_point;     \/\/ inline type entry point (unpack all inline type args) without class check\n+  address _verified_inline_ro_entry_point;  \/\/ inline type entry point (unpack receiver only) without class check\n@@ -697,0 +702,3 @@\n+  address inline_entry_point() const              { return _inline_entry_point; }             \/\/ inline type entry point (unpack all inline type args)\n+  address verified_inline_entry_point() const     { return _verified_inline_entry_point; }    \/\/ inline type entry point (unpack all inline type args) without class check\n+  address verified_inline_ro_entry_point() const  { return _verified_inline_ro_entry_point; } \/\/ inline type entry point (only unpack receiver) without class check\n@@ -763,0 +771,10 @@\n+  bool  needs_stack_repair() const {\n+    if (is_compiled_by_c1()) {\n+      return method()->c1_needs_stack_repair();\n+    } else if (is_compiled_by_c2()) {\n+      return method()->c2_needs_stack_repair();\n+    } else {\n+      return false;\n+    }\n+  }\n+\n@@ -1075,3 +1093,4 @@\n-  \/\/ Fast breakpoint support. Tells if this compiled method is\n-  \/\/ dependent on the given method. Returns true if this nmethod\n-  \/\/ corresponds to the given method as well.\n+  \/\/ Tells if this compiled method is dependent on the given method.\n+  \/\/ Returns true if this nmethod corresponds to the given method as well.\n+  \/\/ It is used for fast breakpoint support and updating the calling convention\n+  \/\/ in case of mismatch.\n","filename":"src\/hotspot\/share\/code\/nmethod.hpp","additions":22,"deletions":3,"binary":false,"changes":25,"status":"modified"},{"patch":"@@ -79,0 +79,1 @@\n+#include \"oops\/flatArrayKlass.inline.hpp\"\n@@ -2373,1 +2374,1 @@\n-    cast_to_oop(copy_destination())->init_mark();\n+    cast_to_oop(copy_destination())->reinit_mark();\n","filename":"src\/hotspot\/share\/gc\/parallel\/psParallelCompact.cpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -383,0 +383,1 @@\n+  assert(obj->is_refArray(), \"Must be\");\n@@ -398,1 +399,1 @@\n-  if (obj->is_objArray()) {\n+  if (obj->is_refArray()) {\n@@ -415,1 +416,1 @@\n-  array->oop_iterate_range(&mark_and_push_closure, beg_index, end_index);\n+  refArrayOop(array)->oop_iterate_range(&mark_and_push_closure, beg_index, end_index);\n","filename":"src\/hotspot\/share\/gc\/serial\/serialFullGC.cpp","additions":3,"deletions":2,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -25,0 +25,1 @@\n+#include \"ci\/ciInlineKlass.hpp\"\n@@ -42,0 +43,15 @@\n+  \/\/ Is this a flat, atomic access that might require gc barriers on oop fields?\n+  ciInlineKlass* vk = access.vk();\n+  if (vk != nullptr && vk->has_object_fields()) {\n+    \/\/ Add pre-barriers for oop fields\n+    for (int i = 0; i < vk->nof_nonstatic_fields(); i++) {\n+      ciField* field = vk->nonstatic_field_at(i);\n+      if (!field->type()->is_primitive_type()) {\n+        int off = access.offset().opr().as_jint() + field->offset_in_bytes() - vk->payload_offset();\n+        LIRAccess inner_access(access.gen(), decorators, access.base(), LIR_OprFact::intConst(off), field->type()->basic_type(), access.patch_emit_info(), access.access_emit_info());\n+        pre_barrier(inner_access, resolve_address(inner_access, false),\n+                    LIR_OprFact::illegalOpr \/* pre_val *\/, inner_access.patch_emit_info());\n+      }\n+    }\n+  }\n+\n@@ -54,0 +70,25 @@\n+\n+  if (vk != nullptr && vk->has_object_fields()) {\n+    \/\/ Add post-barriers for oop fields\n+    for (int i = 0; i < vk->nof_nonstatic_fields(); i++) {\n+      ciField* field = vk->nonstatic_field_at(i);\n+      if (!field->type()->is_primitive_type()) {\n+        int inner_off = field->offset_in_bytes() - vk->payload_offset();\n+        int off = access.offset().opr().as_jint() + inner_off;\n+        LIRAccess inner_access(access.gen(), decorators, access.base(), LIR_OprFact::intConst(off), field->type()->basic_type(), access.patch_emit_info(), access.access_emit_info());\n+\n+        \/\/ Shift long value to extract the narrow oop field value and zero-extend\n+        LIR_Opr field_val = access.gen()->new_register(T_LONG);\n+        access.gen()->lir()->unsigned_shift_right(value,\n+                                                  LIR_OprFact::intConst(inner_off << LogBitsPerByte),\n+                                                  field_val, LIR_Opr::illegalOpr());\n+        LIR_Opr mask = access.gen()->load_immediate((julong) max_juint, T_LONG);\n+        access.gen()->lir()->logical_and(field_val, mask, field_val);\n+        LIR_Opr oop_val = access.gen()->new_register(T_OBJECT);\n+        access.gen()->lir()->move(field_val, oop_val);\n+\n+        assert(!is_array && !on_anonymous, \"not suppported\");\n+        post_barrier(inner_access, access.base().opr(), oop_val);\n+      }\n+    }\n+  }\n","filename":"src\/hotspot\/share\/gc\/shared\/c1\/cardTableBarrierSetC1.cpp","additions":41,"deletions":0,"binary":false,"changes":41,"status":"modified"},{"patch":"@@ -49,1 +49,6 @@\n-  if (!access.is_oop() || tightly_coupled_alloc || (!in_heap && !anonymous)) {\n+  const InlineTypeNode* vt = nullptr;\n+  if (access.is_parse_access() && static_cast<C2ParseAccess&>(access).vt() != nullptr) {\n+    vt = static_cast<C2ParseAccess&>(access).vt();\n+  }\n+\n+  if (vt == nullptr && (!access.is_oop() || tightly_coupled_alloc || (!in_heap && !anonymous))) {\n@@ -57,1 +62,18 @@\n-  post_barrier(parse_access.kit(), access.base(), adr, val.node(), use_precise);\n+  \/\/ TODO 8350865\n+  \/\/ - We actually only need the post barrier once for non-arrays (same for C1, right)?\n+  \/\/ - Value is only needed to determine if we are storing null. Maybe we can go with a simple boolean?\n+  GraphKit* kit = parse_access.kit();\n+  if (vt != nullptr) {\n+    for (uint i = 0; i < vt->field_count(); ++i) {\n+      ciType* type = vt->field_type(i);\n+      if (!type->is_primitive_type()) {\n+        ciInlineKlass* vk = vt->bottom_type()->inline_klass();\n+        int field_offset = vt->field_offset(i) - vk->payload_offset();\n+        Node* value = vt->field_value(i);\n+        Node* field_adr = kit->basic_plus_adr(access.base(), adr, field_offset);\n+        post_barrier(kit, access.base(), field_adr, value, use_precise);\n+      }\n+    }\n+  } else {\n+    post_barrier(kit, access.base(), adr, val.node(), use_precise);\n+  }\n@@ -206,1 +228,1 @@\n-void CardTableBarrierSetC2::eliminate_gc_barrier(PhaseMacroExpand* macro, Node* node) const {\n+void CardTableBarrierSetC2::eliminate_gc_barrier(PhaseIterGVN* igvn, Node* node) const {\n@@ -208,10 +230,16 @@\n-  Node *shift = node->unique_out();\n-  Node *addp = shift->unique_out();\n-  for (DUIterator_Last jmin, j = addp->last_outs(jmin); j >= jmin; --j) {\n-    Node *mem = addp->last_out(j);\n-    if (UseCondCardMark && mem->is_Load()) {\n-      assert(mem->Opcode() == Op_LoadB, \"unexpected code shape\");\n-      \/\/ The load is checking if the card has been written so\n-      \/\/ replace it with zero to fold the test.\n-      macro->replace_node(mem, macro->intcon(0));\n-      continue;\n+  for (DUIterator_Last imin, i = node->last_outs(imin); i >= imin; --i) {\n+    Node* shift = node->last_out(i);\n+    for (DUIterator_Last jmin, j = shift->last_outs(jmin); j >= jmin; --j) {\n+      Node* addp = shift->last_out(j);\n+      for (DUIterator_Last kmin, k = addp->last_outs(kmin); k >= kmin; --k) {\n+        Node* mem = addp->last_out(k);\n+        if (UseCondCardMark && mem->is_Load()) {\n+          assert(mem->Opcode() == Op_LoadB, \"unexpected code shape\");\n+          \/\/ The load is checking if the card has been written so\n+          \/\/ replace it with zero to fold the test.\n+          igvn->replace_node(mem, igvn->intcon(0));\n+          continue;\n+        }\n+        assert(mem->is_Store(), \"store required\");\n+        igvn->replace_node(mem, mem->in(MemNode::Memory));\n+      }\n@@ -219,2 +247,0 @@\n-    assert(mem->is_Store(), \"store required\");\n-    macro->replace_node(mem, mem->in(MemNode::Memory));\n@@ -225,1 +251,1 @@\n-  bool is_oop = is_reference_type(type);\n+  bool is_oop = type == T_OBJECT || type == T_ARRAY;\n","filename":"src\/hotspot\/share\/gc\/shared\/c2\/cardTableBarrierSetC2.cpp","additions":42,"deletions":16,"binary":false,"changes":58,"status":"modified"},{"patch":"@@ -49,1 +49,1 @@\n-  virtual void eliminate_gc_barrier(PhaseMacroExpand* macro, Node* node) const;\n+  virtual void eliminate_gc_barrier(PhaseIterGVN* igvn, Node* node) const;\n","filename":"src\/hotspot\/share\/gc\/shared\/c2\/cardTableBarrierSetC2.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -107,1 +107,1 @@\n-    static bool oop_arraycopy_in_heap(arrayOop src_obj, size_t src_offset_in_bytes, T* src_raw,\n+    static void oop_arraycopy_in_heap(arrayOop src_obj, size_t src_offset_in_bytes, T* src_raw,\n@@ -110,0 +110,5 @@\n+  private:\n+    \/\/ Failing checkcast or check null during copy, still needs barrier\n+    template <typename T>\n+    static inline void oop_arraycopy_partial_barrier(BarrierSetT *bs, T* dst_raw, T* p);\n+  public:\n@@ -124,0 +129,2 @@\n+\n+    static void value_copy_in_heap(void* src, void* dst, InlineKlass* md, LayoutKind lk);\n","filename":"src\/hotspot\/share\/gc\/shared\/cardTableBarrierSet.hpp","additions":8,"deletions":1,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -33,0 +33,1 @@\n+#include \"oops\/inlineKlass.inline.hpp\"\n@@ -102,1 +103,12 @@\n-inline bool CardTableBarrierSet::AccessBarrier<decorators, BarrierSetT>::\n+inline void CardTableBarrierSet::AccessBarrier<decorators, BarrierSetT>::\n+oop_arraycopy_partial_barrier(BarrierSetT *bs, T* dst_raw, T* p) {\n+  const size_t pd = pointer_delta(p, dst_raw, (size_t)heapOopSize);\n+  \/\/ pointer delta is scaled to number of elements (length field in\n+  \/\/ objArrayOop) which we assume is 32 bit.\n+  assert(pd == (size_t)(int)pd, \"length field overflow\");\n+  bs->write_ref_array((HeapWord*)dst_raw, pd);\n+}\n+\n+template <DecoratorSet decorators, typename BarrierSetT>\n+template <typename T>\n+inline void CardTableBarrierSet::AccessBarrier<decorators, BarrierSetT>::\n@@ -111,1 +123,2 @@\n-  if (!HasDecorator<decorators, ARRAYCOPY_CHECKCAST>::value) {\n+  if ((!HasDecorator<decorators, ARRAYCOPY_CHECKCAST>::value) &&\n+      (!HasDecorator<decorators, ARRAYCOPY_NOTNULL>::value)) {\n@@ -124,11 +137,11 @@\n-      if (oopDesc::is_instanceof_or_null(CompressedOops::decode(element), bound)) {\n-        bs->template write_ref_field_pre<decorators>(p);\n-        *p = element;\n-      } else {\n-        \/\/ We must do a barrier to cover the partial copy.\n-        const size_t pd = pointer_delta(p, dst_raw, (size_t)heapOopSize);\n-        \/\/ pointer delta is scaled to number of elements (length field in\n-        \/\/ objArrayOop) which we assume is 32 bit.\n-        assert(pd == (size_t)(int)pd, \"length field overflow\");\n-        bs->write_ref_array((HeapWord*)dst_raw, pd);\n-        return false;\n+      \/\/ Apply any required checks\n+      if (HasDecorator<decorators, ARRAYCOPY_NOTNULL>::value && CompressedOops::is_null(element)) {\n+        oop_arraycopy_partial_barrier(bs, dst_raw, p);\n+        throw_array_null_pointer_store_exception(src_obj, dst_obj, JavaThread::current());\n+        return;\n+      }\n+      if (HasDecorator<decorators, ARRAYCOPY_CHECKCAST>::value &&\n+          (!oopDesc::is_instanceof_or_null(CompressedOops::decode(element), bound))) {\n+        oop_arraycopy_partial_barrier(bs, dst_raw, p);\n+        throw_array_store_exception(src_obj, dst_obj, JavaThread::current());\n+        return;\n@@ -136,0 +149,3 @@\n+      \/\/ write\n+      bs->template write_ref_field_pre<decorators>(p);\n+      *p = element;\n@@ -139,1 +155,0 @@\n-  return true;\n@@ -150,0 +165,38 @@\n+template <DecoratorSet decorators, typename BarrierSetT>\n+inline void CardTableBarrierSet::AccessBarrier<decorators, BarrierSetT>::\n+value_copy_in_heap(void* src, void* dst, InlineKlass* md, LayoutKind lk) {\n+  if (!md->contains_oops()) {\n+    \/\/ If we do not have oops in the flat array, we can just do a raw copy.\n+    Raw::value_copy(src, dst, md, lk);\n+  } else {\n+    BarrierSetT* bs = barrier_set_cast<BarrierSetT>(BarrierSet::barrier_set());\n+    \/\/ src\/dst aren't oops, need offset to adjust oop map offset\n+    const address dst_oop_addr_offset = ((address) dst) - md->payload_offset();\n+    typedef typename ValueOopType<decorators>::type OopType;\n+\n+    \/\/ Pre-barriers...\n+    OopMapBlock* map = md->start_of_nonstatic_oop_maps();\n+    OopMapBlock* const end = map + md->nonstatic_oop_map_count();\n+    bool is_uninitialized = HasDecorator<decorators, IS_DEST_UNINITIALIZED>::value;\n+    while (map != end) {\n+      address doop_address = dst_oop_addr_offset + map->offset();\n+      \/\/ The pre-barrier only impacts G1, which will emit a barrier if the destination is\n+      \/\/ initialized. Note that we should not emit a barrier if the destination is uninitialized,\n+      \/\/ as doing so will fill the SATB queue with garbage data.\n+      bs->write_ref_array_pre((OopType*) doop_address, map->count(), is_uninitialized);\n+      map++;\n+    }\n+\n+    Raw::value_copy(src, dst, md, lk);\n+\n+    \/\/ Post-barriers...\n+    map = md->start_of_nonstatic_oop_maps();\n+    while (map != end) {\n+      address doop_address = dst_oop_addr_offset + map->offset();\n+      \/\/ The post-barrier needs to be called for initialized and uninitialized destinations.\n+      bs->write_ref_array((HeapWord*) doop_address, map->count());\n+      map++;\n+    }\n+  }\n+}\n+\n","filename":"src\/hotspot\/share\/gc\/shared\/cardTableBarrierSet.inline.hpp","additions":67,"deletions":14,"binary":false,"changes":81,"status":"modified"},{"patch":"@@ -295,0 +295,1 @@\n+  oop obj_buffer_allocate(Klass* klass, size_t size, TRAPS); \/\/ doesn't clear memory\n","filename":"src\/hotspot\/share\/gc\/shared\/collectedHeap.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -27,0 +27,1 @@\n+#include \"classfile\/systemDictionary.hpp\"\n@@ -47,0 +48,3 @@\n+#include \"oops\/flatArrayKlass.hpp\"\n+#include \"oops\/flatArrayOop.inline.hpp\"\n+#include \"oops\/inlineKlass.inline.hpp\"\n@@ -78,0 +82,1 @@\n+#include \"utilities\/globalDefinitions.hpp\"\n@@ -228,0 +233,32 @@\n+JRT_ENTRY(void, InterpreterRuntime::read_flat_field(JavaThread* current, oopDesc* obj, ResolvedFieldEntry* entry))\n+  assert(oopDesc::is_oop(obj), \"Sanity check\");\n+  Handle obj_h(THREAD, obj);\n+\n+  InstanceKlass* holder = InstanceKlass::cast(entry->field_holder());\n+  assert(entry->field_holder()->field_is_flat(entry->field_index()), \"Sanity check\");\n+\n+  InlineLayoutInfo* layout_info = holder->inline_layout_info_adr(entry->field_index());\n+  InlineKlass* field_vklass = layout_info->klass();\n+\n+#ifdef ASSERT\n+  fieldDescriptor fd;\n+  bool found = holder->find_field_from_offset(entry->field_offset(), false, &fd);\n+  assert(found, \"Field not found\");\n+  assert(fd.is_flat(), \"Field must be flat\");\n+#endif \/\/ ASSERT\n+\n+  oop res = field_vklass->read_payload_from_addr(obj_h(), entry->field_offset(), layout_info->kind(), CHECK);\n+  current->set_vm_result_oop(res);\n+JRT_END\n+\n+JRT_ENTRY(void, InterpreterRuntime::write_flat_field(JavaThread* current, oopDesc* obj, oopDesc* value, ResolvedFieldEntry* entry))\n+  assert(oopDesc::is_oop(obj), \"Sanity check\");\n+  Handle obj_h(THREAD, obj);\n+  assert(value == nullptr || oopDesc::is_oop(value), \"Sanity check\");\n+  Handle val_h(THREAD, value);\n+\n+  InstanceKlass* holder = entry->field_holder();\n+  InlineLayoutInfo* li = holder->inline_layout_info_adr(entry->field_index());\n+  InlineKlass* vk = li->klass();\n+  vk->write_value_to_addr(val_h(), ((char*)(oopDesc*)obj_h()) + entry->field_offset(), li->kind(), true, CHECK);\n+JRT_END\n@@ -237,1 +274,1 @@\n-  objArrayOop obj = oopFactory::new_objArray(klass, size, CHECK);\n+  arrayOop obj = oopFactory::new_objArray(klass, size, CHECK);\n@@ -241,0 +278,12 @@\n+JRT_ENTRY(void, InterpreterRuntime::flat_array_load(JavaThread* current, arrayOopDesc* array, int index))\n+  assert(array->is_flatArray(), \"Must be\");\n+  flatArrayOop farray = (flatArrayOop)array;\n+  oop res = farray->obj_at(index, CHECK);\n+  current->set_vm_result_oop(res);\n+JRT_END\n+\n+JRT_ENTRY(void, InterpreterRuntime::flat_array_store(JavaThread* current, oopDesc* val, arrayOopDesc* array, int index))\n+  assert(array->is_flatArray(), \"Must be\");\n+  flatArrayOop farray = (flatArrayOop)array;\n+  farray->obj_at_put(index, val, CHECK);\n+JRT_END\n@@ -246,2 +295,2 @@\n-  int          i = last_frame.get_index_u2(Bytecodes::_multianewarray);\n-  Klass* klass   = constants->klass_at(i, CHECK);\n+  int i = last_frame.get_index_u2(Bytecodes::_multianewarray);\n+  Klass* klass = constants->klass_at(i, CHECK);\n@@ -276,0 +325,24 @@\n+JRT_ENTRY(jboolean, InterpreterRuntime::is_substitutable(JavaThread* current, oopDesc* aobj, oopDesc* bobj))\n+  assert(oopDesc::is_oop(aobj) && oopDesc::is_oop(bobj), \"must be valid oops\");\n+\n+  Handle ha(THREAD, aobj);\n+  Handle hb(THREAD, bobj);\n+  JavaValue result(T_BOOLEAN);\n+  JavaCallArguments args;\n+  args.push_oop(ha);\n+  args.push_oop(hb);\n+  methodHandle method(current, UseAltSubstitutabilityMethod ?  Universe::is_substitutableAlt_method() : Universe::is_substitutable_method());\n+  method->method_holder()->initialize(CHECK_false); \/\/ Ensure class ValueObjectMethods is initialized\n+  JavaCalls::call(&result, method, &args, THREAD);\n+  if (HAS_PENDING_EXCEPTION) {\n+    \/\/ Something really bad happened because isSubstitutable() should not throw exceptions\n+    \/\/ If it is an error, just let it propagate\n+    \/\/ If it is an exception, wrap it into an InternalError\n+    if (!PENDING_EXCEPTION->is_a(vmClasses::Error_klass())) {\n+      Handle e(THREAD, PENDING_EXCEPTION);\n+      CLEAR_PENDING_EXCEPTION;\n+      THROW_MSG_CAUSE_(vmSymbols::java_lang_InternalError(), \"Internal error in substitutability test\", e, false);\n+    }\n+  }\n+  return result.get_jboolean();\n+JRT_END\n@@ -623,0 +696,4 @@\n+JRT_ENTRY(void, InterpreterRuntime::throw_InstantiationError(JavaThread* current))\n+  THROW(vmSymbols::java_lang_InstantiationError());\n+JRT_END\n+\n@@ -702,0 +779,1 @@\n+  bool strict_static_final = info.is_strict() && info.is_static() && info.is_final();\n@@ -706,1 +784,14 @@\n-  if (!uninitialized_static || VM_Version::supports_fast_class_init_checks()) {\n+  if (uninitialized_static && (info.is_strict_static_unset() || strict_static_final)) {\n+    \/\/ During <clinit>, closely track the state of strict statics.\n+    \/\/ 1. if we are reading an uninitialized strict static, throw\n+    \/\/ 2. if we are writing one, clear the \"unset\" flag\n+    \/\/\n+    \/\/ Note: If we were handling an attempted write of a null to a\n+    \/\/ null-restricted strict static, we would NOT clear the \"unset\"\n+    \/\/ flag.\n+    assert(klass->is_being_initialized(), \"else should have thrown\");\n+    assert(klass->is_reentrant_initialization(THREAD),\n+      \"<clinit> must be running in current thread\");\n+    klass->notify_strict_static_access(info.index(), is_put, CHECK);\n+    assert(!info.is_strict_static_unset(), \"after initialization, no unset flags\");\n+  } else if (!uninitialized_static || VM_Version::supports_fast_class_init_checks()) {\n@@ -714,1 +805,4 @@\n-  entry->set_flags(info.access_flags().is_final(), info.access_flags().is_volatile());\n+  entry->set_flags(info.access_flags().is_final(), info.access_flags().is_volatile(),\n+                   info.is_flat(), info.is_null_free_inline_type(),\n+                   info.has_null_marker());\n+\n@@ -761,1 +855,0 @@\n-\n@@ -766,1 +859,0 @@\n-\n@@ -781,0 +873,15 @@\n+JRT_ENTRY(void, InterpreterRuntime::throw_identity_exception(JavaThread* current, oopDesc* obj))\n+  Klass* klass = cast_to_oop(obj)->klass();\n+  ResourceMark rm(THREAD);\n+  const char* desc = \"Cannot synchronize on an instance of value class \";\n+  const char* className = klass->external_name();\n+  size_t msglen = strlen(desc) + strlen(className) + 1;\n+  char* message = NEW_RESOURCE_ARRAY(char, msglen);\n+  if (nullptr == message) {\n+    \/\/ Out of memory: can't create detailed error message\n+    THROW_MSG(vmSymbols::java_lang_IdentityException(), className);\n+  } else {\n+    jio_snprintf(message, msglen, \"%s%s\", desc, className);\n+    THROW_MSG(vmSymbols::java_lang_IdentityException(), message);\n+  }\n+JRT_END\n@@ -1194,0 +1301,1 @@\n+  assert(entry->is_valid(), \"Invalid ResolvedFieldEntry\");\n@@ -1201,0 +1309,1 @@\n+  bool is_flat = entry->is_flat();\n@@ -1209,1 +1318,1 @@\n-  jfieldID fid = jfieldIDWorkaround::to_jfieldID(field_holder, entry->field_offset(), is_static);\n+  jfieldID fid = jfieldIDWorkaround::to_jfieldID(field_holder, entry->field_offset(), is_static, is_flat);\n@@ -1217,0 +1326,1 @@\n+  assert(entry->is_valid(), \"Invalid ResolvedFieldEntry\");\n@@ -1238,0 +1348,1 @@\n+\n@@ -1239,0 +1350,1 @@\n+  bool is_flat = entry->is_flat();\n@@ -1241,1 +1353,1 @@\n-  jfieldID fid = jfieldIDWorkaround::to_jfieldID(ik, entry->field_offset(), is_static);\n+  jfieldID fid = jfieldIDWorkaround::to_jfieldID(ik, entry->field_offset(), is_static, is_flat);\n","filename":"src\/hotspot\/share\/interpreter\/interpreterRuntime.cpp","additions":121,"deletions":9,"binary":false,"changes":130,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -61,1 +61,1 @@\n-  static void    anewarray     (JavaThread* current, ConstantPool* pool, int index, jint size);\n+  static void    anewarray     (JavaThread* threcurrentad, ConstantPool* pool, int index, jint size);\n@@ -64,0 +64,8 @@\n+  static void    write_heap_copy (JavaThread* current, oopDesc* value, int offset, oopDesc* rcv);\n+  static void    read_flat_field(JavaThread* current, oopDesc* object, ResolvedFieldEntry* entry);\n+  static void    write_flat_field(JavaThread* current, oopDesc* object, oopDesc* value, ResolvedFieldEntry* entry);\n+\n+  static void flat_array_load(JavaThread* current, arrayOopDesc* array, int index);\n+  static void flat_array_store(JavaThread* current, oopDesc* val, arrayOopDesc* array, int index);\n+\n+  static jboolean is_substitutable(JavaThread* current, oopDesc* aobj, oopDesc* bobj);\n@@ -75,0 +83,1 @@\n+  static void    throw_InstantiationError(JavaThread* current);\n@@ -124,0 +133,1 @@\n+  static void    throw_identity_exception(JavaThread* current, oopDesc* obj);\n","filename":"src\/hotspot\/share\/interpreter\/interpreterRuntime.hpp","additions":12,"deletions":2,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -1004,1 +1004,2 @@\n-  bool is_put    = (byte == Bytecodes::_putfield  || byte == Bytecodes::_putstatic || byte == Bytecodes::_nofast_putfield);\n+  bool is_put    = (byte == Bytecodes::_putfield  || byte == Bytecodes::_putstatic ||\n+                    byte == Bytecodes::_nofast_putfield);\n@@ -1047,2 +1048,2 @@\n-                 is_static ? \"static\" : \"non-static\", resolved_klass->external_name(), fd.name()->as_C_string(),\n-                current_klass->external_name());\n+                  is_static ? \"static\" : \"non-static\", resolved_klass->external_name(), fd.name()->as_C_string(),\n+                  current_klass->external_name());\n@@ -1057,1 +1058,1 @@\n-                                                   !m->is_static_initializer());\n+                                                   !m->is_class_initializer());\n@@ -1060,1 +1061,1 @@\n-                                                     !m->is_object_initializer());\n+                                                     !m->is_object_constructor());\n@@ -1197,0 +1198,2 @@\n+  \/\/ Since this method is never inherited from a super, any appearance here under\n+  \/\/ the wrong class would be an error.\n@@ -1263,1 +1266,1 @@\n-      \/\/ check if the method is not <init>\n+      \/\/ check if the method is not <init>, which is never inherited\n@@ -1727,2 +1730,2 @@\n-                             const methodHandle& attached_method,\n-                             Bytecodes::Code byte, TRAPS) {\n+                                  const methodHandle& attached_method,\n+                                  Bytecodes::Code byte, bool check_null_and_abstract, TRAPS) {\n@@ -1733,0 +1736,1 @@\n+  Klass* recv_klass = recv.is_null() ? defc : recv->klass();\n@@ -1735,2 +1739,2 @@\n-      resolve_virtual_call(result, recv, recv->klass(), link_info,\n-                           \/*check_null_and_abstract=*\/true, CHECK);\n+      resolve_virtual_call(result, recv, recv_klass, link_info,\n+                           check_null_and_abstract, CHECK);\n@@ -1739,2 +1743,2 @@\n-      resolve_interface_call(result, recv, recv->klass(), link_info,\n-                             \/*check_null_and_abstract=*\/true, CHECK);\n+      resolve_interface_call(result, recv, recv_klass, link_info,\n+                             check_null_and_abstract, CHECK);\n","filename":"src\/hotspot\/share\/interpreter\/linkResolver.cpp","additions":16,"deletions":12,"binary":false,"changes":28,"status":"modified"},{"patch":"@@ -371,1 +371,1 @@\n-                             Bytecodes::Code byte, TRAPS);\n+                             Bytecodes::Code byte, bool check_null_and_abstract, TRAPS);\n","filename":"src\/hotspot\/share\/interpreter\/linkResolver.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -1630,1 +1630,1 @@\n-              Deoptimization::reassign_fields(vf->frame_pointer(), &reg_map, objects, realloc_failures, false);\n+              Deoptimization::reassign_fields(vf->frame_pointer(), &reg_map, objects, realloc_failures, false, CHECK_NULL);\n@@ -1881,1 +1881,1 @@\n-  Deoptimization::reassign_fields(fstAfterDeopt.current(), fstAfterDeopt.register_map(), objects, realloc_failures, false);\n+  Deoptimization::reassign_fields(fstAfterDeopt.current(), fstAfterDeopt.register_map(), objects, realloc_failures, false, THREAD);\n@@ -2209,1 +2209,1 @@\n-    if (m->is_object_initializer()) {\n+    if (m->is_object_constructor()) {\n@@ -2236,1 +2236,1 @@\n-    if (!m->is_object_initializer() && !m->is_static_initializer() && !m->is_overpass()) {\n+    if (!(m->is_object_constructor() || m->is_class_initializer()) && !m->is_overpass()) {\n@@ -2976,1 +2976,5 @@\n-  if (m->is_object_initializer()) {\n+  if (m->is_class_initializer()) {\n+      JVMCI_THROW_MSG_NULL(IllegalArgumentException,\n+          \"Cannot create java.lang.reflect.Method for class initializer\");\n+  }\n+  else if (m->is_object_constructor()) {\n@@ -2978,3 +2982,0 @@\n-  } else if (m->is_static_initializer()) {\n-    JVMCI_THROW_MSG_NULL(IllegalArgumentException,\n-        \"Cannot create java.lang.reflect.Method for class initializer\");\n","filename":"src\/hotspot\/share\/jvmci\/jvmciCompilerToVM.cpp","additions":9,"deletions":8,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -70,0 +70,1 @@\n+#include \"oops\/refArrayKlass.hpp\"\n@@ -117,0 +118,3 @@\n+static LatestMethodCache _is_substitutable_cache;           \/\/ ValueObjectMethods.isSubstitutable()\n+static LatestMethodCache _value_object_hash_code_cache;     \/\/ ValueObjectMethods.valueObjectHashCode()\n+static LatestMethodCache _is_substitutable_alt_cache;       \/\/ ValueObjectMethods.isSubstitutableAlt()\n@@ -467,0 +471,1 @@\n+\n@@ -514,2 +519,6 @@\n-    Klass* oak = vmClasses::Object_klass()->array_klass(CHECK);\n-    _objectArrayKlass = ObjArrayKlass::cast(oak);\n+    ArrayKlass* oak = vmClasses::Object_klass()->array_klass(CHECK);\n+    oak->append_to_sibling_list();\n+\n+    \/\/ Create a RefArrayKlass (which is the default) and initialize.\n+    ObjArrayKlass* rak = ObjArrayKlass::cast(oak)->klass_with_properties(ArrayKlass::ArrayProperties::DEFAULT, THREAD);\n+    _objectArrayKlass = rak;\n@@ -517,7 +526,0 @@\n-  \/\/ OLD\n-  \/\/ Add the class to the class hierarchy manually to make sure that\n-  \/\/ its vtable is initialized after core bootstrapping is completed.\n-  \/\/ ---\n-  \/\/ New\n-  \/\/ Have already been initialized.\n-  _objectArrayKlass->append_to_sibling_list();\n@@ -660,0 +662,3 @@\n+\n+  \/\/ This isn't added to the subclass list, so need to reinitialize vtables directly.\n+  Universe::objectArrayKlass()->vtable().initialize_vtable();\n@@ -905,1 +910,0 @@\n-\n@@ -1075,0 +1079,3 @@\n+Method* Universe::is_substitutable_method()       { return _is_substitutable_cache.get_method(); }\n+Method* Universe::value_object_hash_code_method() { return _value_object_hash_code_cache.get_method(); }\n+Method* Universe::is_substitutableAlt_method()    { return _is_substitutable_alt_cache.get_method(); }\n@@ -1104,0 +1111,15 @@\n+\n+  \/\/ Set up substitutability testing\n+  ResourceMark rm(current);\n+  _is_substitutable_cache.init(current,\n+                          vmClasses::ValueObjectMethods_klass(),\n+                          vmSymbols::isSubstitutable_name()->as_C_string(),\n+                          vmSymbols::object_object_boolean_signature(), true);\n+  _value_object_hash_code_cache.init(current,\n+                          vmClasses::ValueObjectMethods_klass(),\n+                          vmSymbols::valueObjectHashCode_name()->as_C_string(),\n+                          vmSymbols::object_int_signature(), true);\n+  _is_substitutable_alt_cache.init(current,\n+                          vmClasses::ValueObjectMethods_klass(),\n+                          vmSymbols::isSubstitutableAlt_name()->as_C_string(),\n+                          vmSymbols::object_object_boolean_signature(), true);\n","filename":"src\/hotspot\/share\/memory\/universe.cpp","additions":32,"deletions":10,"binary":false,"changes":42,"status":"modified"},{"patch":"@@ -119,0 +119,1 @@\n+\n@@ -259,0 +260,4 @@\n+  static Method*      is_substitutable_method();\n+  static Method*      value_object_hash_code_method();\n+  static Method*      is_substitutableAlt_method();\n+\n","filename":"src\/hotspot\/share\/memory\/universe.hpp","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -128,1 +128,1 @@\n-    static bool access_barrier(arrayOop src_obj, size_t src_offset_in_bytes, T* src_raw,\n+    static void access_barrier(arrayOop src_obj, size_t src_offset_in_bytes, T* src_raw,\n@@ -134,1 +134,0 @@\n-      return true;\n@@ -138,1 +137,1 @@\n-    static bool oop_access_barrier(arrayOop src_obj, size_t src_offset_in_bytes, T* src_raw,\n+    static void oop_access_barrier(arrayOop src_obj, size_t src_offset_in_bytes, T* src_raw,\n@@ -142,3 +141,3 @@\n-      return GCBarrierType::oop_arraycopy_in_heap(src_obj, src_offset_in_bytes, reinterpret_cast<OopType*>(src_raw),\n-                                                  dst_obj, dst_offset_in_bytes, reinterpret_cast<OopType*>(dst_raw),\n-                                                  length);\n+      GCBarrierType::oop_arraycopy_in_heap(src_obj, src_offset_in_bytes, reinterpret_cast<OopType*>(src_raw),\n+                                           dst_obj, dst_offset_in_bytes, reinterpret_cast<OopType*>(dst_raw),\n+                                           length);\n@@ -203,0 +202,7 @@\n+  template <class GCBarrierType, DecoratorSet decorators>\n+  struct PostRuntimeDispatch<GCBarrierType, BARRIER_VALUE_COPY, decorators>: public AllStatic {\n+    static void access_barrier(void* src, void* dst, InlineKlass* md, LayoutKind lk) {\n+      GCBarrierType::value_copy_in_heap(src, dst, md, lk);\n+    }\n+  };\n+\n@@ -334,1 +340,1 @@\n-  bool RuntimeDispatch<decorators, T, BARRIER_ARRAYCOPY>::arraycopy_init(arrayOop src_obj, size_t src_offset_in_bytes, T* src_raw,\n+  void RuntimeDispatch<decorators, T, BARRIER_ARRAYCOPY>::arraycopy_init(arrayOop src_obj, size_t src_offset_in_bytes, T* src_raw,\n@@ -339,3 +345,3 @@\n-    return function(src_obj, src_offset_in_bytes, src_raw,\n-                    dst_obj, dst_offset_in_bytes, dst_raw,\n-                    length);\n+    function(src_obj, src_offset_in_bytes, src_raw,\n+             dst_obj, dst_offset_in_bytes, dst_raw,\n+             length);\n@@ -350,0 +356,7 @@\n+\n+  template <DecoratorSet decorators, typename T>\n+  void RuntimeDispatch<decorators, T, BARRIER_VALUE_COPY>::value_copy_init(void* src, void* dst, InlineKlass* md, LayoutKind lk) {\n+    func_t function = BarrierResolver<decorators, func_t, BARRIER_VALUE_COPY>::resolve_barrier();\n+    _value_copy_func = function;\n+    function(src, dst, md,lk);\n+  }\n","filename":"src\/hotspot\/share\/oops\/access.inline.hpp","additions":23,"deletions":10,"binary":false,"changes":33,"status":"modified"},{"patch":"@@ -33,0 +33,1 @@\n+#include \"oops\/inlineKlass.hpp\"\n@@ -47,0 +48,8 @@\n+\/\/ This meta-function returns either oop or narrowOop depending on whether\n+\/\/ a back-end needs to consider compressed oops types or not.\n+template <DecoratorSet decorators>\n+struct ValueOopType: AllStatic {\n+  static const bool needs_oop_compress = HasDecorator<decorators, INTERNAL_RT_USE_COMPRESSED_OOPS>::value;\n+  using type = std::conditional_t<needs_oop_compress, narrowOop, oop>;\n+};\n+\n@@ -58,1 +67,2 @@\n-    BARRIER_CLONE\n+    BARRIER_CLONE,\n+    BARRIER_VALUE_COPY\n@@ -95,1 +105,1 @@\n-    typedef bool (*arraycopy_func_t)(arrayOop src_obj, size_t src_offset_in_bytes, T* src_raw,\n+    typedef void (*arraycopy_func_t)(arrayOop src_obj, size_t src_offset_in_bytes, T* src_raw,\n@@ -99,0 +109,1 @@\n+    typedef void (*value_copy_func_t)(void* src, void* dst, InlineKlass* md, LayoutKind lk);\n@@ -103,1 +114,1 @@\n-    typedef bool (*arraycopy_func_t)(arrayOop src_obj, size_t src_offset_in_bytes, void* src,\n+    typedef void (*arraycopy_func_t)(arrayOop src_obj, size_t src_offset_in_bytes, void* src,\n@@ -125,0 +136,1 @@\n+  ACCESS_GENERATE_ACCESS_FUNCTION(BARRIER_VALUE_COPY, value_copy_func_t);\n@@ -150,0 +162,2 @@\n+\n+  void value_copy_internal(void* src, void* dst, size_t length);\n@@ -289,1 +303,1 @@\n-  static bool arraycopy(arrayOop src_obj, size_t src_offset_in_bytes, T* src_raw,\n+  static void arraycopy(arrayOop src_obj, size_t src_offset_in_bytes, T* src_raw,\n@@ -334,1 +348,1 @@\n-  static bool oop_arraycopy(arrayOop src_obj, size_t src_offset_in_bytes, T* src_raw,\n+  static void oop_arraycopy(arrayOop src_obj, size_t src_offset_in_bytes, T* src_raw,\n@@ -339,0 +353,2 @@\n+  static void value_copy(void* src, void* dst, InlineKlass* md, LayoutKind lk);\n+\n@@ -508,1 +524,1 @@\n-    static bool arraycopy_init(arrayOop src_obj, size_t src_offset_in_bytes, T* src_raw,\n+    static void arraycopy_init(arrayOop src_obj, size_t src_offset_in_bytes, T* src_raw,\n@@ -512,1 +528,1 @@\n-    static inline bool arraycopy(arrayOop src_obj, size_t src_offset_in_bytes, T* src_raw,\n+    static inline void arraycopy(arrayOop src_obj, size_t src_offset_in_bytes, T* src_raw,\n@@ -535,0 +551,12 @@\n+  template <DecoratorSet decorators, typename T>\n+  struct RuntimeDispatch<decorators, T, BARRIER_VALUE_COPY>: AllStatic {\n+    typedef typename AccessFunction<decorators, T, BARRIER_VALUE_COPY>::type func_t;\n+    static func_t _value_copy_func;\n+\n+    static void value_copy_init(void* src, void* dst, InlineKlass* md, LayoutKind lk);\n+\n+    static inline void value_copy(void* src, void* dst, InlineKlass* md, LayoutKind lk) {\n+      _value_copy_func(src, dst, md, lk);\n+    }\n+  };\n+\n@@ -576,0 +604,4 @@\n+  template <DecoratorSet decorators, typename T>\n+  typename AccessFunction<decorators, T, BARRIER_VALUE_COPY>::type\n+  RuntimeDispatch<decorators, T, BARRIER_VALUE_COPY>::_value_copy_func = &value_copy_init;\n+\n@@ -823,1 +855,1 @@\n-      HasDecorator<decorators, AS_RAW>::value && CanHardwireRaw<decorators>::value, bool>::type\n+      HasDecorator<decorators, AS_RAW>::value && CanHardwireRaw<decorators>::value, void>::type\n@@ -829,3 +861,3 @@\n-        return Raw::oop_arraycopy(src_obj, src_offset_in_bytes, src_raw,\n-                                  dst_obj, dst_offset_in_bytes, dst_raw,\n-                                  length);\n+        Raw::oop_arraycopy(src_obj, src_offset_in_bytes, src_raw,\n+                           dst_obj, dst_offset_in_bytes, dst_raw,\n+                           length);\n@@ -833,3 +865,3 @@\n-        return Raw::arraycopy(src_obj, src_offset_in_bytes, src_raw,\n-                              dst_obj, dst_offset_in_bytes, dst_raw,\n-                              length);\n+        Raw::arraycopy(src_obj, src_offset_in_bytes, src_raw,\n+                       dst_obj, dst_offset_in_bytes, dst_raw,\n+                       length);\n@@ -841,1 +873,1 @@\n-      HasDecorator<decorators, AS_RAW>::value && !CanHardwireRaw<decorators>::value, bool>::type\n+      HasDecorator<decorators, AS_RAW>::value && !CanHardwireRaw<decorators>::value, void>::type\n@@ -847,3 +879,3 @@\n-        return PreRuntimeDispatch::arraycopy<expanded_decorators>(src_obj, src_offset_in_bytes, src_raw,\n-                                                                  dst_obj, dst_offset_in_bytes, dst_raw,\n-                                                                  length);\n+        PreRuntimeDispatch::arraycopy<expanded_decorators>(src_obj, src_offset_in_bytes, src_raw,\n+                                                           dst_obj, dst_offset_in_bytes, dst_raw,\n+                                                           length);\n@@ -852,3 +884,3 @@\n-        return PreRuntimeDispatch::arraycopy<expanded_decorators>(src_obj, src_offset_in_bytes, src_raw,\n-                                                                  dst_obj, dst_offset_in_bytes, dst_raw,\n-                                                                  length);\n+        PreRuntimeDispatch::arraycopy<expanded_decorators>(src_obj, src_offset_in_bytes, src_raw,\n+                                                           dst_obj, dst_offset_in_bytes, dst_raw,\n+                                                           length);\n@@ -860,1 +892,1 @@\n-      !HasDecorator<decorators, AS_RAW>::value, bool>::type\n+      !HasDecorator<decorators, AS_RAW>::value, void>::type\n@@ -866,3 +898,3 @@\n-        return PreRuntimeDispatch::arraycopy<expanded_decorators>(src_obj, src_offset_in_bytes, src_raw,\n-                                                                  dst_obj, dst_offset_in_bytes, dst_raw,\n-                                                                  length);\n+        PreRuntimeDispatch::arraycopy<expanded_decorators>(src_obj, src_offset_in_bytes, src_raw,\n+                                                           dst_obj, dst_offset_in_bytes, dst_raw,\n+                                                           length);\n@@ -870,3 +902,3 @@\n-        return RuntimeDispatch<decorators, T, BARRIER_ARRAYCOPY>::arraycopy(src_obj, src_offset_in_bytes, src_raw,\n-                                                                            dst_obj, dst_offset_in_bytes, dst_raw,\n-                                                                            length);\n+        RuntimeDispatch<decorators, T, BARRIER_ARRAYCOPY>::arraycopy(src_obj, src_offset_in_bytes, src_raw,\n+                                                                     dst_obj, dst_offset_in_bytes, dst_raw,\n+                                                                     length);\n@@ -890,0 +922,16 @@\n+\n+    template <DecoratorSet decorators>\n+    inline static typename EnableIf<\n+      HasDecorator<decorators, AS_RAW>::value>::type\n+    value_copy(void* src, void* dst, InlineKlass* md, LayoutKind lk) {\n+      typedef RawAccessBarrier<decorators & RAW_DECORATOR_MASK> Raw;\n+      Raw::value_copy(src, dst, md, lk);\n+    }\n+\n+    template <DecoratorSet decorators>\n+    inline static typename EnableIf<\n+      !HasDecorator<decorators, AS_RAW>::value>::type\n+      value_copy(void* src, void* dst, InlineKlass* md, LayoutKind lk) {\n+      const DecoratorSet expanded_decorators = decorators;\n+      RuntimeDispatch<expanded_decorators, void*, BARRIER_VALUE_COPY>::value_copy(src, dst, md, lk);\n+    }\n@@ -1004,1 +1052,1 @@\n-  inline bool arraycopy_reduce_types(arrayOop src_obj, size_t src_offset_in_bytes, T* src_raw,\n+  inline void arraycopy_reduce_types(arrayOop src_obj, size_t src_offset_in_bytes, T* src_raw,\n@@ -1007,3 +1055,3 @@\n-    return PreRuntimeDispatch::arraycopy<decorators>(src_obj, src_offset_in_bytes, src_raw,\n-                                                     dst_obj, dst_offset_in_bytes, dst_raw,\n-                                                     length);\n+    PreRuntimeDispatch::arraycopy<decorators>(src_obj, src_offset_in_bytes, src_raw,\n+                                              dst_obj, dst_offset_in_bytes, dst_raw,\n+                                              length);\n@@ -1013,1 +1061,1 @@\n-  inline bool arraycopy_reduce_types(arrayOop src_obj, size_t src_offset_in_bytes, HeapWord* src_raw,\n+  inline void arraycopy_reduce_types(arrayOop src_obj, size_t src_offset_in_bytes, HeapWord* src_raw,\n@@ -1017,3 +1065,3 @@\n-    return PreRuntimeDispatch::arraycopy<expanded_decorators>(src_obj, src_offset_in_bytes, src_raw,\n-                                                              dst_obj, dst_offset_in_bytes, dst_raw,\n-                                                              length);\n+    PreRuntimeDispatch::arraycopy<expanded_decorators>(src_obj, src_offset_in_bytes, src_raw,\n+                                                       dst_obj, dst_offset_in_bytes, dst_raw,\n+                                                       length);\n@@ -1023,1 +1071,1 @@\n-  inline bool arraycopy_reduce_types(arrayOop src_obj, size_t src_offset_in_bytes, narrowOop* src_raw,\n+  inline void arraycopy_reduce_types(arrayOop src_obj, size_t src_offset_in_bytes, narrowOop* src_raw,\n@@ -1028,3 +1076,3 @@\n-    return PreRuntimeDispatch::arraycopy<expanded_decorators>(src_obj, src_offset_in_bytes, src_raw,\n-                                                              dst_obj, dst_offset_in_bytes, dst_raw,\n-                                                              length);\n+    PreRuntimeDispatch::arraycopy<expanded_decorators>(src_obj, src_offset_in_bytes, src_raw,\n+                                                       dst_obj, dst_offset_in_bytes, dst_raw,\n+                                                       length);\n@@ -1163,1 +1211,1 @@\n-  inline bool arraycopy(arrayOop src_obj, size_t src_offset_in_bytes, const T* src_raw,\n+  inline void arraycopy(arrayOop src_obj, size_t src_offset_in_bytes, const T* src_raw,\n@@ -1171,3 +1219,3 @@\n-    return arraycopy_reduce_types<expanded_decorators>(src_obj, src_offset_in_bytes, const_cast<DecayedT*>(src_raw),\n-                                                       dst_obj, dst_offset_in_bytes, const_cast<DecayedT*>(dst_raw),\n-                                                       length);\n+    arraycopy_reduce_types<expanded_decorators>(src_obj, src_offset_in_bytes, const_cast<DecayedT*>(src_raw),\n+                                                dst_obj, dst_offset_in_bytes, const_cast<DecayedT*>(dst_raw),\n+                                                length);\n@@ -1182,0 +1230,6 @@\n+  template <DecoratorSet decorators>\n+  inline void value_copy(void* src, void* dst, InlineKlass* md, LayoutKind lk) {\n+    const DecoratorSet expanded_decorators = DecoratorFixup<decorators>::value;\n+    PreRuntimeDispatch::value_copy<expanded_decorators>(src, dst, md, lk);\n+  }\n+\n","filename":"src\/hotspot\/share\/oops\/accessBackend.hpp","additions":98,"deletions":44,"binary":false,"changes":142,"status":"modified"},{"patch":"@@ -34,0 +34,1 @@\n+#include \"oops\/inlineKlass.hpp\"\n@@ -126,1 +127,1 @@\n-inline bool RawAccessBarrier<decorators>::oop_arraycopy(arrayOop src_obj, size_t src_offset_in_bytes, T* src_raw,\n+inline void RawAccessBarrier<decorators>::oop_arraycopy(arrayOop src_obj, size_t src_offset_in_bytes, T* src_raw,\n@@ -129,3 +130,3 @@\n-  return arraycopy(src_obj, src_offset_in_bytes, src_raw,\n-                   dst_obj, dst_offset_in_bytes, dst_raw,\n-                   length);\n+  arraycopy(src_obj, src_offset_in_bytes, src_raw,\n+            dst_obj, dst_offset_in_bytes, dst_raw,\n+            length);\n@@ -304,1 +305,1 @@\n-inline bool RawAccessBarrier<decorators>::arraycopy(arrayOop src_obj, size_t src_offset_in_bytes, T* src_raw,\n+inline void RawAccessBarrier<decorators>::arraycopy(arrayOop src_obj, size_t src_offset_in_bytes, T* src_raw,\n@@ -310,1 +311,0 @@\n-  return true;\n@@ -333,0 +333,5 @@\n+template <DecoratorSet decorators>\n+inline void RawAccessBarrier<decorators>::value_copy(void* src, void* dst, InlineKlass* md, LayoutKind lk) {\n+  assert(is_aligned(src, md->layout_alignment(lk)) && is_aligned(dst, md->layout_alignment(lk)), \"Unaligned value_copy\");\n+  AccessInternal::value_copy_internal(src, dst, static_cast<size_t>(md->layout_size_in_bytes(lk)));\n+}\n","filename":"src\/hotspot\/share\/oops\/accessBackend.inline.hpp","additions":11,"deletions":6,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -194,0 +194,2 @@\n+\/\/ * ARRAYCOPY_NOTNULL: This property means that the source array may contain null elements\n+\/\/   but the destination does not allow null elements (i.e. throw NPE)\n@@ -200,5 +202,6 @@\n-const DecoratorSet ARRAYCOPY_DISJOINT             = UCONST64(1) << 24;\n-const DecoratorSet ARRAYCOPY_ARRAYOF              = UCONST64(1) << 25;\n-const DecoratorSet ARRAYCOPY_ATOMIC               = UCONST64(1) << 26;\n-const DecoratorSet ARRAYCOPY_ALIGNED              = UCONST64(1) << 27;\n-const DecoratorSet ARRAYCOPY_DECORATOR_MASK       = ARRAYCOPY_CHECKCAST | ARRAYCOPY_DISJOINT |\n+const DecoratorSet ARRAYCOPY_NOTNULL              = UCONST64(1) << 24;\n+const DecoratorSet ARRAYCOPY_DISJOINT             = UCONST64(1) << 25;\n+const DecoratorSet ARRAYCOPY_ARRAYOF              = UCONST64(1) << 26;\n+const DecoratorSet ARRAYCOPY_ATOMIC               = UCONST64(1) << 27;\n+const DecoratorSet ARRAYCOPY_ALIGNED              = UCONST64(1) << 28;\n+const DecoratorSet ARRAYCOPY_DECORATOR_MASK       = ARRAYCOPY_CHECKCAST | ARRAYCOPY_NOTNULL |\n@@ -212,2 +215,2 @@\n-const DecoratorSet ACCESS_READ                    = UCONST64(1) << 28;\n-const DecoratorSet ACCESS_WRITE                   = UCONST64(1) << 29;\n+const DecoratorSet ACCESS_READ                    = UCONST64(1) << 29;\n+const DecoratorSet ACCESS_WRITE                   = UCONST64(1) << 30;\n@@ -216,1 +219,1 @@\n-const DecoratorSet DECORATOR_LAST = UCONST64(1) << 29;\n+const DecoratorSet DECORATOR_LAST = UCONST64(1) << 30;\n","filename":"src\/hotspot\/share\/oops\/accessDecorators.hpp","additions":11,"deletions":8,"binary":false,"changes":19,"status":"modified"},{"patch":"@@ -66,0 +66,1 @@\n+#include \"oops\/inlineKlass.hpp\"\n@@ -72,0 +73,1 @@\n+#include \"oops\/markWord.hpp\"\n@@ -75,0 +77,1 @@\n+#include \"oops\/refArrayKlass.hpp\"\n@@ -152,0 +155,5 @@\n+void InlineLayoutInfo::metaspace_pointers_do(MetaspaceClosure* it) {\n+  log_trace(cds)(\"Iter(InlineFieldInfo): %p\", this);\n+  it->push(&_klass);\n+}\n+\n@@ -173,0 +181,13 @@\n+bool InstanceKlass::field_is_null_free_inline_type(int index) const {\n+  return field(index).field_flags().is_null_free_inline_type();\n+}\n+\n+bool InstanceKlass::is_class_in_loadable_descriptors_attribute(Symbol* name) const {\n+  if (_loadable_descriptors == nullptr) return false;\n+  for (int i = 0; i < _loadable_descriptors->length(); i++) {\n+        Symbol* class_name = _constants->symbol_at(_loadable_descriptors->at(i));\n+        if (class_name == name) return true;\n+  }\n+  return false;\n+}\n+\n@@ -462,1 +483,2 @@\n-                                       parser.is_interface());\n+                                       parser.is_interface(),\n+                                       parser.is_inline_type());\n@@ -484,0 +506,3 @@\n+  } else if (parser.is_inline_type()) {\n+    \/\/ inline type\n+    ik = new (loader_data, size, THREAD) InlineKlass(parser);\n@@ -500,0 +525,6 @@\n+#ifdef ASSERT\n+  ik->bounds_check((address) ik->start_of_vtable(), false, size);\n+  ik->bounds_check((address) ik->start_of_itable(), false, size);\n+  ik->bounds_check((address) ik->end_of_itable(), true, size);\n+  ik->bounds_check((address) ik->end_of_nonstatic_oop_maps(), true, size);\n+#endif \/\/ASSERT\n@@ -503,0 +534,23 @@\n+#ifndef PRODUCT\n+bool InstanceKlass::bounds_check(address addr, bool edge_ok, intptr_t size_in_bytes) const {\n+  const char* bad = nullptr;\n+  address end = nullptr;\n+  if (addr < (address)this) {\n+    bad = \"before\";\n+  } else if (addr == (address)this) {\n+    if (edge_ok)  return true;\n+    bad = \"just before\";\n+  } else if (addr == (end = (address)this + sizeof(intptr_t) * (size_in_bytes < 0 ? size() : size_in_bytes))) {\n+    if (edge_ok)  return true;\n+    bad = \"just after\";\n+  } else if (addr > end) {\n+    bad = \"after\";\n+  } else {\n+    return true;\n+  }\n+  tty->print_cr(\"%s object bounds: \" INTPTR_FORMAT \" [\" INTPTR_FORMAT \"..\" INTPTR_FORMAT \"]\",\n+      bad, (intptr_t)addr, (intptr_t)this, (intptr_t)end);\n+  Verbose = WizardMode = true; this->print(); \/\/@@\n+  return false;\n+}\n+#endif \/\/PRODUCT\n@@ -530,2 +584,2 @@\n-InstanceKlass::InstanceKlass(const ClassFileParser& parser, KlassKind kind, ReferenceType reference_type) :\n-  Klass(kind),\n+InstanceKlass::InstanceKlass(const ClassFileParser& parser, KlassKind kind, markWord prototype_header, ReferenceType reference_type) :\n+  Klass(kind, prototype_header),\n@@ -542,1 +596,5 @@\n-  _init_thread(nullptr)\n+  _acmp_maps_offset(0),\n+  _init_thread(nullptr),\n+  _inline_layout_info_array(nullptr),\n+  _loadable_descriptors(nullptr),\n+  _adr_inlineklass_fixed_block(nullptr)\n@@ -549,0 +607,3 @@\n+  if (parser.has_inline_fields()) {\n+    set_has_inline_type_fields();\n+  }\n@@ -693,0 +754,5 @@\n+  if (inline_layout_info_array() != nullptr) {\n+    MetadataFactory::free_array<InlineLayoutInfo>(loader_data, inline_layout_info_array());\n+  }\n+  set_inline_layout_info_array(nullptr);\n+\n@@ -727,0 +793,7 @@\n+  if (loadable_descriptors() != nullptr &&\n+      loadable_descriptors() != Universe::the_empty_short_array() &&\n+      !loadable_descriptors()->in_aot_cache()) {\n+    MetadataFactory::free_array<jushort>(loader_data, loadable_descriptors());\n+  }\n+  set_loadable_descriptors(nullptr);\n+\n@@ -921,0 +994,38 @@\n+static void load_classes_from_loadable_descriptors_attribute(InstanceKlass *ik, TRAPS) {\n+  ResourceMark rm(THREAD);\n+  if (ik->loadable_descriptors() != nullptr && PreloadClasses) {\n+    HandleMark hm(THREAD);\n+    for (int i = 0; i < ik->loadable_descriptors()->length(); i++) {\n+      Symbol* sig = ik->constants()->symbol_at(ik->loadable_descriptors()->at(i));\n+      if (!Signature::has_envelope(sig)) continue;\n+      TempNewSymbol class_name = Signature::strip_envelope(sig);\n+      if (class_name == ik->name()) continue;\n+      log_info(class, preload)(\"Preloading of class %s during linking of class %s \"\n+                               \"because of the class is listed in the LoadableDescriptors attribute\",\n+                               sig->as_C_string(), ik->name()->as_C_string());\n+      oop loader = ik->class_loader();\n+      Klass* klass = SystemDictionary::resolve_or_null(class_name,\n+                                                        Handle(THREAD, loader), THREAD);\n+      if (HAS_PENDING_EXCEPTION) {\n+        CLEAR_PENDING_EXCEPTION;\n+      }\n+      if (klass != nullptr) {\n+        log_info(class, preload)(\"Preloading of class %s during linking of class %s \"\n+                                 \"(cause: LoadableDescriptors attribute) succeeded\",\n+                                 class_name->as_C_string(), ik->name()->as_C_string());\n+        if (!klass->is_inline_klass()) {\n+          \/\/ Non value class are allowed by the current spec, but it could be an indication\n+          \/\/ of an issue so let's log a warning\n+          log_info(class, preload)(\"Preloading of class %s during linking of class %s \"\n+                                      \"(cause: LoadableDescriptors attribute) but loaded class is not a value class\",\n+                                      class_name->as_C_string(), ik->name()->as_C_string());\n+        }\n+      } else {\n+        log_info(class, preload)(\"Preloading of class %s during linking of class %s \"\n+                                    \"(cause: LoadableDescriptors attribute) failed\",\n+                                    class_name->as_C_string(), ik->name()->as_C_string());\n+      }\n+    }\n+  }\n+}\n+\n@@ -991,0 +1102,7 @@\n+  if (EnableValhalla) {\n+    \/\/ Aggressively preloading all classes from the LoadableDescriptors attribute\n+    \/\/ so inline classes can be scalarized in the calling conventions computed below\n+    load_classes_from_loadable_descriptors_attribute(this, THREAD);\n+    assert(!HAS_PENDING_EXCEPTION, \"Shouldn't have pending exceptions from call above\");\n+  }\n+\n@@ -1314,0 +1432,21 @@\n+  \/\/ Pre-allocating an all-zero value to be used to reset nullable flat storages\n+  if (is_inline_klass()) {\n+      InlineKlass* vk = InlineKlass::cast(this);\n+      if (vk->has_nullable_atomic_layout()) {\n+        oop val = vk->allocate_instance(THREAD);\n+        if (HAS_PENDING_EXCEPTION) {\n+            Handle e(THREAD, PENDING_EXCEPTION);\n+            CLEAR_PENDING_EXCEPTION;\n+            {\n+                EXCEPTION_MARK;\n+                add_initialization_error(THREAD, e);\n+                \/\/ Locks object, set state, and notify all waiting threads\n+                set_initialization_state_and_notify(initialization_error, THREAD);\n+                CLEAR_PENDING_EXCEPTION;\n+            }\n+            THROW_OOP(e());\n+        }\n+        vk->set_null_reset_value(val);\n+      }\n+  }\n+\n@@ -1346,1 +1485,0 @@\n-\n@@ -1367,0 +1505,30 @@\n+\n+    if (has_strict_static_fields() && !HAS_PENDING_EXCEPTION) {\n+      \/\/ Step 9 also verifies that strict static fields have been initialized.\n+      \/\/ Status bits were set in ClassFileParser::post_process_parsed_stream.\n+      \/\/ After <clinit>, bits must all be clear, or else we must throw an error.\n+      \/\/ This is an extremely fast check, so we won't bother with a timer.\n+      assert(fields_status() != nullptr, \"\");\n+      Symbol* bad_strict_static = nullptr;\n+      for (int index = 0; index < fields_status()->length(); index++) {\n+        \/\/ Very fast loop over single byte array looking for a set bit.\n+        if (fields_status()->adr_at(index)->is_strict_static_unset()) {\n+          \/\/ This strict static field has not been set by the class initializer.\n+          \/\/ Note that in the common no-error case, we read no field metadata.\n+          \/\/ We only unpack it when we need to report an error.\n+          FieldInfo fi = field(index);\n+          bad_strict_static = fi.name(constants());\n+          if (debug_logging_enabled) {\n+            ResourceMark rm(jt);\n+            const char* msg = format_strict_static_message(bad_strict_static);\n+            log_debug(class, init)(\"%s\", msg);\n+          } else {\n+            \/\/ If we are not logging, do not bother to look for a second offense.\n+            break;\n+          }\n+        }\n+      }\n+      if (bad_strict_static != nullptr) {\n+        throw_strict_static_exception(bad_strict_static, \"is unset after initialization of\", THREAD);\n+      }\n+    }\n@@ -1420,0 +1588,68 @@\n+void InstanceKlass::notify_strict_static_access(int field_index, bool is_writing, TRAPS) {\n+  guarantee(field_index >= 0 && field_index < fields_status()->length(), \"valid field index\");\n+  DEBUG_ONLY(FieldInfo debugfi = field(field_index));\n+  assert(debugfi.access_flags().is_strict(), \"\");\n+  assert(debugfi.access_flags().is_static(), \"\");\n+  FieldStatus& fs = *fields_status()->adr_at(field_index);\n+  LogTarget(Trace, class, init) lt;\n+  if (lt.is_enabled()) {\n+    ResourceMark rm(THREAD);\n+    LogStream ls(lt);\n+    FieldInfo fi = field(field_index);\n+    ls.print(\"notify %s %s %s%s \",\n+             external_name(), is_writing? \"Write\" : \"Read\",\n+             fs.is_strict_static_unset() ? \"Unset\" : \"(set)\",\n+             fs.is_strict_static_unread() ? \"+Unread\" : \"\");\n+    fi.print(&ls, constants());\n+  }\n+  if (fs.is_strict_static_unset()) {\n+    assert(fs.is_strict_static_unread(), \"ClassFileParser resp.\");\n+    \/\/ If it is not set, there are only two reasonable things we can do here:\n+    \/\/ - mark it set if this is putstatic\n+    \/\/ - throw an error (Read-Before-Write) if this is getstatic\n+\n+    \/\/ The unset state is (or should be) transient, and observable only in one\n+    \/\/ thread during the execution of <clinit>.  Something is wrong here as this\n+    \/\/ should not be possible\n+    guarantee(is_reentrant_initialization(THREAD), \"unscoped access to strict static\");\n+    if (is_writing) {\n+      \/\/ clear the \"unset\" bit, since the field is actually going to be written\n+      fs.update_strict_static_unset(false);\n+    } else {\n+      \/\/ throw an IllegalStateException, since we are reading before writing\n+      \/\/ see also InstanceKlass::initialize_impl, Step 8 (at end)\n+      Symbol* bad_strict_static = field(field_index).name(constants());\n+      throw_strict_static_exception(bad_strict_static, \"is unset before first read in\", CHECK);\n+    }\n+  } else {\n+    \/\/ Ensure no write after read for final strict statics\n+    FieldInfo fi = field(field_index);\n+    bool is_final = fi.access_flags().is_final();\n+    if (is_final) {\n+      \/\/ no final write after read, so observing a constant freezes it, as if <clinit> ended early\n+      \/\/ (maybe we could trust the constant a little earlier, before <clinit> ends)\n+      if (is_writing && !fs.is_strict_static_unread()) {\n+        Symbol* bad_strict_static = fi.name(constants());\n+        throw_strict_static_exception(bad_strict_static, \"is set after read (as final) in\", CHECK);\n+      } else if (!is_writing && fs.is_strict_static_unread()) {\n+        fs.update_strict_static_unread(false);\n+      }\n+    }\n+  }\n+}\n+\n+void InstanceKlass::throw_strict_static_exception(Symbol* field_name, const char* when, TRAPS) {\n+  ResourceMark rm(THREAD);\n+  const char* msg = format_strict_static_message(field_name, when);\n+  THROW_MSG(vmSymbols::java_lang_IllegalStateException(), msg);\n+}\n+\n+const char* InstanceKlass::format_strict_static_message(Symbol* field_name, const char* when) {\n+  stringStream ss;\n+  ss.print(\"Strict static \\\"%s\\\" %s %s\",\n+           field_name->as_C_string(),\n+           when == nullptr ? \"is unset in\" : when,\n+           external_name());\n+  return ss.as_string();\n+}\n+\n@@ -1670,1 +1906,1 @@\n-  ObjArrayKlass* ak = array_klasses();\n+  ArrayKlass* ak = array_klasses();\n@@ -1677,2 +1913,2 @@\n-  ObjArrayKlass* oak = array_klasses_acquire();\n-  if (oak == nullptr) {\n+  ArrayKlass* ak = array_klasses_acquire();\n+  if (ak == nullptr) {\n@@ -1681,1 +1917,1 @@\n-    return oak->array_klass_or_null(n);\n+    return ak->array_klass_or_null(n);\n@@ -1698,1 +1934,1 @@\n-  if (clinit != nullptr && clinit->has_valid_initializer_flags()) {\n+  if (clinit != nullptr && clinit->is_class_initializer()) {\n@@ -1807,4 +2043,0 @@\n-bool InstanceKlass::contains_field_offset(int offset) {\n-  fieldDescriptor fd;\n-  return find_field_from_offset(offset, false, &fd);\n-}\n@@ -1891,0 +2123,9 @@\n+bool InstanceKlass::contains_field_offset(int offset) {\n+  if (this->is_inline_klass()) {\n+    InlineKlass* vk = InlineKlass::cast(this);\n+    return offset >= vk->payload_offset() && offset < (vk->payload_offset() + vk->payload_size_in_bytes());\n+  } else {\n+    fieldDescriptor fd;\n+    return find_field_from_offset(offset, false, &fd);\n+  }\n+}\n@@ -2274,0 +2515,3 @@\n+    if (name == vmSymbols::object_initializer_name()) {\n+      break;  \/\/ <init> is never inherited\n+    }\n@@ -2686,0 +2930,1 @@\n+  it->push(&_loadable_descriptors);\n@@ -2687,0 +2932,1 @@\n+  it->push(&_inline_layout_info_array, MetaspaceClosure::_writable);\n@@ -2734,1 +2980,1 @@\n-  \/\/ These are not allocated from metaspace. They are safe to set to null.\n+  \/\/ These are not allocated from metaspace. They are safe to set to nullptr.\n@@ -2825,0 +3071,4 @@\n+  if (is_inline_klass()) {\n+    InlineKlass::cast(this)->initialize_calling_convention(CHECK);\n+  }\n+\n@@ -2858,1 +3108,1 @@\n-    assert(this == array_klasses()->bottom_klass(), \"sanity\");\n+    assert(this == ObjArrayKlass::cast(array_klasses())->bottom_klass(), \"sanity\");\n@@ -2861,0 +3111,6 @@\n+    if (class_loader_data() == nullptr) {\n+      ResourceMark rm(THREAD);\n+      log_debug(cds)(\"  loader_data %s \", loader_data == nullptr ? \"nullptr\" : \"non null\");\n+      log_debug(cds)(\"  this %s array_klasses %s \", this->name()->as_C_string(), array_klasses()->name()->as_C_string());\n+    }\n+    assert(!array_klasses()->is_refined_objArray_klass(), \"must be non-refined objarrayklass\");\n@@ -3016,0 +3272,4 @@\n+bool InstanceKlass::supports_inline_types() const {\n+  return major_version() >= Verifier::VALUE_TYPES_MAJOR_VERSION && minor_version() == Verifier::JAVA_PREVIEW_MINOR_VERSION;\n+}\n+\n@@ -3048,0 +3308,2 @@\n+  return signature_name_of_carrier(JVM_SIGNATURE_CLASS);\n+}\n@@ -3049,0 +3311,1 @@\n+const char* InstanceKlass::signature_name_of_carrier(char c) const {\n@@ -3055,1 +3318,1 @@\n-  \/\/ Add L as type indicator\n+  \/\/ Add L or Q as type indicator\n@@ -3057,1 +3320,1 @@\n-  dest[dest_index++] = JVM_SIGNATURE_CLASS;\n+  dest[dest_index++] = c;\n@@ -3338,0 +3601,19 @@\n+void InstanceKlass::check_can_be_annotated_with_NullRestricted(InstanceKlass* type, Symbol* container_klass_name, TRAPS) {\n+  assert(type->is_instance_klass(), \"Sanity check\");\n+  if (type->is_identity_class()) {\n+    ResourceMark rm(THREAD);\n+    THROW_MSG(vmSymbols::java_lang_IncompatibleClassChangeError(),\n+              err_msg(\"Class %s expects class %s to be a value class, but it is an identity class\",\n+              container_klass_name->as_C_string(),\n+              type->external_name()));\n+  }\n+\n+  if (type->is_abstract()) {\n+    ResourceMark rm(THREAD);\n+    THROW_MSG(vmSymbols::java_lang_IncompatibleClassChangeError(),\n+              err_msg(\"Class %s expects class %s to be concrete value type, but it is an abstract class\",\n+              container_klass_name->as_C_string(),\n+              type->external_name()));\n+  }\n+}\n+\n@@ -3404,2 +3686,1 @@\n-  \/\/ Remember to strip ACC_SUPER bit\n-  return (access & (~JVM_ACC_SUPER));\n+  return access;\n@@ -3659,1 +3940,4 @@\n-static void print_vtable(intptr_t* start, int len, outputStream* st) {\n+static void print_vtable(address self, intptr_t* start, int len, outputStream* st) {\n+  ResourceMark rm;\n+  int* forward_refs = NEW_RESOURCE_ARRAY(int, len);\n+  for (int i = 0; i < len; i++)  forward_refs[i] = 0;\n@@ -3663,0 +3947,5 @@\n+    if (forward_refs[i] != 0) {\n+      int from = forward_refs[i];\n+      int off = (int) start[from];\n+      st->print(\" (offset %d <= [%d])\", off, from);\n+    }\n@@ -3666,0 +3955,6 @@\n+    } else if (self != nullptr && e > 0 && e < 0x10000) {\n+      address location = self + e;\n+      int index = (int)((intptr_t*)location - start);\n+      st->print(\" (offset %d => [%d])\", (int)e, index);\n+      if (index >= 0 && index < len)\n+        forward_refs[index] = i;\n@@ -3672,1 +3967,22 @@\n-  return print_vtable(reinterpret_cast<intptr_t*>(start), len, st);\n+  return print_vtable(nullptr, reinterpret_cast<intptr_t*>(start), len, st);\n+}\n+\n+template<typename T>\n+ static void print_array_on(outputStream* st, Array<T>* array) {\n+   if (array == nullptr) { st->print_cr(\"nullptr\"); return; }\n+   array->print_value_on(st); st->cr();\n+   if (Verbose || WizardMode) {\n+     for (int i = 0; i < array->length(); i++) {\n+       st->print(\"%d : \", i); array->at(i)->print_value_on(st); st->cr();\n+     }\n+   }\n+ }\n+\n+static void print_array_on(outputStream* st, Array<int>* array) {\n+  if (array == nullptr) { st->print_cr(\"nullptr\"); return; }\n+  array->print_value_on(st); st->cr();\n+  if (Verbose || WizardMode) {\n+    for (int i = 0; i < array->length(); i++) {\n+      st->print(\"%d : %d\", i, array->at(i)); st->cr();\n+    }\n+  }\n@@ -3713,8 +4029,2 @@\n-  st->print(BULLET\"methods:           \"); methods()->print_value_on(st);               st->cr();\n-  if (Verbose || WizardMode) {\n-    Array<Method*>* method_array = methods();\n-    for (int i = 0; i < method_array->length(); i++) {\n-      st->print(\"%d : \", i); method_array->at(i)->print_value(); st->cr();\n-    }\n-  }\n-  st->print(BULLET\"method ordering:   \"); method_ordering()->print_value_on(st);      st->cr();\n+  st->print(BULLET\"methods:           \"); print_array_on(st, methods());\n+  st->print(BULLET\"method ordering:   \"); print_array_on(st, method_ordering());\n@@ -3722,7 +4032,1 @@\n-    st->print(BULLET\"default_methods:   \"); default_methods()->print_value_on(st);    st->cr();\n-    if (Verbose) {\n-      Array<Method*>* method_array = default_methods();\n-      for (int i = 0; i < method_array->length(); i++) {\n-        st->print(\"%d : \", i); method_array->at(i)->print_value(); st->cr();\n-      }\n-    }\n+    st->print(BULLET\"default_methods:   \"); print_array_on(st, default_methods());\n@@ -3788,0 +4092,1 @@\n+  st->print(BULLET\"loadable descriptors:     \"); loadable_descriptors()->print_value_on(st); st->cr();\n@@ -3798,1 +4103,1 @@\n-  if (itable_length() > 0 && (Verbose || WizardMode))  print_vtable(start_of_itable(), itable_length(), st);\n+  if (itable_length() > 0 && (Verbose || WizardMode))  print_vtable(nullptr, start_of_itable(), itable_length(), st);\n@@ -3830,0 +4135,1 @@\n+  for (int i = 0; i < _indent; i++) _st->print(\"  \");\n@@ -3832,1 +4138,1 @@\n-     fd->print_on(_st);\n+     fd->print_on(_st, _base_offset);\n@@ -3835,2 +4141,2 @@\n-     fd->print_on_for(_st, _obj);\n-     _st->cr();\n+     fd->print_on_for(_st, _obj, _indent, _base_offset);\n+     if (!fd->field_flags().is_flat()) _st->cr();\n@@ -3841,1 +4147,1 @@\n-void InstanceKlass::oop_print_on(oop obj, outputStream* st) {\n+void InstanceKlass::oop_print_on(oop obj, outputStream* st, int indent, int base_offset) {\n@@ -3857,1 +4163,1 @@\n-  FieldPrinter print_field(st, obj);\n+  FieldPrinter print_field(st, obj, indent, base_offset);\n","filename":"src\/hotspot\/share\/oops\/instanceKlass.cpp","additions":349,"deletions":43,"binary":false,"changes":392,"status":"modified"},{"patch":"@@ -28,0 +28,1 @@\n+#include \"code\/vmreg.hpp\"\n@@ -35,0 +36,1 @@\n+#include \"oops\/refArrayKlass.hpp\"\n@@ -59,0 +61,1 @@\n+\/\/    [EMBEDDED InlineKlassFixedBlock] only if is an InlineKlass instance\n@@ -74,0 +77,1 @@\n+class BufferedInlineTypeBlob;\n@@ -89,0 +93,2 @@\n+   int _indent;\n+   int _base_offset;\n@@ -90,1 +96,2 @@\n-   FieldPrinter(outputStream* st, oop obj = nullptr) : _obj(obj), _st(st) {}\n+   FieldPrinter(outputStream* st, oop obj = nullptr, int indent = 0, int base_offset = 0) :\n+                 _obj(obj), _st(st), _indent(indent), _base_offset(base_offset) {}\n@@ -134,0 +141,54 @@\n+class SigEntry;\n+\n+class InlineKlassFixedBlock {\n+  Array<SigEntry>** _extended_sig;\n+  Array<VMRegPair>** _return_regs;\n+  address* _pack_handler;\n+  address* _pack_handler_jobject;\n+  address* _unpack_handler;\n+  int* _null_reset_value_offset;\n+  int _payload_offset;          \/\/ offset of the begining of the payload in a heap buffered instance\n+  int _payload_size_in_bytes;   \/\/ size of payload layout\n+  int _payload_alignment;       \/\/ alignment required for payload\n+  int _non_atomic_size_in_bytes; \/\/ size of null-free non-atomic flat layout\n+  int _non_atomic_alignment;    \/\/ alignment requirement for null-free non-atomic layout\n+  int _atomic_size_in_bytes;    \/\/ size and alignment requirement for a null-free atomic layout, -1 if no atomic flat layout is possible\n+  int _nullable_size_in_bytes;  \/\/ size and alignment requirement for a nullable layout (always atomic), -1 if no nullable flat layout is possible\n+  int _null_marker_offset;      \/\/ expressed as an offset from the beginning of the object for a heap buffered value\n+                                \/\/ payload_offset must be subtracted to get the offset from the beginning of the payload\n+\n+  friend class InlineKlass;\n+};\n+\n+class InlineLayoutInfo : public MetaspaceObj {\n+  InlineKlass* _klass;\n+  LayoutKind _kind;\n+  int _null_marker_offset; \/\/ null marker offset for this field, relative to the beginning of the current container\n+\n+ public:\n+  InlineLayoutInfo(): _klass(nullptr), _kind(LayoutKind::UNKNOWN), _null_marker_offset(-1)  {}\n+  InlineLayoutInfo(InlineKlass* ik, LayoutKind kind, int size, int nm_offset):\n+    _klass(ik), _kind(kind), _null_marker_offset(nm_offset) {}\n+\n+  InlineKlass* klass() const { return _klass; }\n+  void set_klass(InlineKlass* k) { _klass = k; }\n+\n+  LayoutKind kind() const {\n+    assert(_kind != LayoutKind::UNKNOWN, \"Not set\");\n+    return _kind;\n+  }\n+  void set_kind(LayoutKind lk) { _kind = lk; }\n+\n+  int null_marker_offset() const {\n+    assert(_null_marker_offset != -1, \"Not set\");\n+    return _null_marker_offset;\n+  }\n+  void set_null_marker_offset(int o) { _null_marker_offset = o; }\n+\n+  void metaspace_pointers_do(MetaspaceClosure* it);\n+  MetaspaceObj::Type type() const { return InlineLayoutInfoType; }\n+\n+  static ByteSize klass_offset() { return in_ByteSize(offset_of(InlineLayoutInfo, _klass)); }\n+  static ByteSize null_marker_offset_offset() { return in_ByteSize(offset_of(InlineLayoutInfo, _null_marker_offset)); }\n+};\n+\n@@ -139,0 +200,1 @@\n+  friend class TemplateTable;\n@@ -144,1 +206,1 @@\n-  InstanceKlass(const ClassFileParser& parser, KlassKind kind = Kind, ReferenceType reference_type = REF_NONE);\n+  InstanceKlass(const ClassFileParser& parser, KlassKind kind = Kind, markWord prototype = markWord::prototype(), ReferenceType reference_type = REF_NONE);\n@@ -233,0 +295,3 @@\n+  int             _acmp_maps_offset;        \/\/ offset to injected static field storing acmp_maps for values classes\n+                                            \/\/ unfortunately, abstract values need one too so it cannot be stored in\n+                                            \/\/ the InlineKlassFixedBlock that only exist for InlineKlass.\n@@ -280,0 +345,4 @@\n+  Array<InlineLayoutInfo>* _inline_layout_info_array;\n+  Array<u2>* _loadable_descriptors;\n+  const InlineKlassFixedBlock* _adr_inlineklass_fixed_block;\n+\n@@ -328,0 +397,15 @@\n+  bool has_inline_type_fields() const { return _misc_flags.has_inline_type_fields(); }\n+  void set_has_inline_type_fields()   { _misc_flags.set_has_inline_type_fields(true); }\n+\n+  bool is_naturally_atomic() const  { return _misc_flags.is_naturally_atomic(); }\n+  void set_is_naturally_atomic()    { _misc_flags.set_is_naturally_atomic(true); }\n+\n+  \/\/ Query if this class has atomicity requirements (default is yes)\n+  \/\/ This bit can occur anywhere, but is only significant\n+  \/\/ for inline classes *and* their super types.\n+  \/\/ It inherits from supers.\n+  \/\/ Its value depends on the ForceNonTearable VM option, the LooselyConsistentValue annotation\n+  \/\/ and the presence of flat fields with atomicity requirements\n+  bool must_be_atomic() const { return _misc_flags.must_be_atomic(); }\n+  void set_must_be_atomic()   { _misc_flags.set_must_be_atomic(true); }\n+\n@@ -392,0 +476,6 @@\n+  bool field_is_flat(int index) const { return field_flags(index).is_flat(); }\n+  bool field_has_null_marker(int index) const { return field_flags(index).has_null_marker(); }\n+  bool field_is_null_free_inline_type(int index) const;\n+  bool is_class_in_loadable_descriptors_attribute(Symbol* name) const;\n+\n+  int null_marker_offset(int index) const { return inline_layout_info(index).null_marker_offset(); }\n@@ -406,0 +496,3 @@\n+  Array<u2>* loadable_descriptors() const { return _loadable_descriptors; }\n+  void set_loadable_descriptors(Array<u2>* c) { _loadable_descriptors = c; }\n+\n@@ -501,0 +594,3 @@\n+  \/\/ Check if this klass can be null-free\n+  static void check_can_be_annotated_with_NullRestricted(InstanceKlass* type, Symbol* container_klass_name, TRAPS);\n+\n@@ -536,0 +632,3 @@\n+  static ByteSize kind_offset() { return in_ByteSize(offset_of(InstanceKlass, _kind)); }\n+  static ByteSize misc_flags_offset() { return in_ByteSize(offset_of(InstanceKlass, _misc_flags)); }\n+\n@@ -552,0 +651,6 @@\n+  int acmp_maps_offset() const {\n+    assert(_acmp_maps_offset != 0, \"Not initialized\");\n+    return _acmp_maps_offset;\n+  }\n+  void set_acmp_maps_offset(int offset) { _acmp_maps_offset = offset; }\n+\n@@ -664,0 +769,2 @@\n+  bool supports_inline_types() const;\n+\n@@ -770,0 +877,7 @@\n+  \/\/ runtime support for strict statics\n+  bool has_strict_static_fields() const     { return _misc_flags.has_strict_static_fields(); }\n+  void set_has_strict_static_fields(bool b) { _misc_flags.set_has_strict_static_fields(b); }\n+  void notify_strict_static_access(int field_index, bool is_writing, TRAPS);\n+  const char* format_strict_static_message(Symbol* field_name, const char* doing_what = nullptr);\n+  void throw_strict_static_exception(Symbol* field_name, const char* when, TRAPS);\n+\n@@ -863,0 +977,3 @@\n+  static ByteSize inline_layout_info_array_offset() { return in_ByteSize(offset_of(InstanceKlass, _inline_layout_info_array)); }\n+  static ByteSize adr_inlineklass_fixed_block_offset() { return in_ByteSize(offset_of(InstanceKlass, _adr_inlineklass_fixed_block)); }\n+\n@@ -926,1 +1043,2 @@\n-                  bool is_interface) {\n+                  bool is_interface,\n+                  bool is_inline_type) {\n@@ -931,1 +1049,2 @@\n-           (is_interface ? (int)sizeof(Klass*)\/wordSize : 0));\n+           (is_interface ? (int)sizeof(Klass*)\/wordSize : 0) +\n+           (is_inline_type ? (int)sizeof(InlineKlassFixedBlock) : 0));\n@@ -937,1 +1056,2 @@\n-                                               is_interface());\n+                                               is_interface(),\n+                                               is_inline_klass());\n@@ -944,0 +1064,1 @@\n+  bool bounds_check(address addr, bool edge_ok = false, intptr_t size_in_bytes = -1) const PRODUCT_RETURN0;\n@@ -950,0 +1071,18 @@\n+  void set_inline_layout_info_array(Array<InlineLayoutInfo>* array) { _inline_layout_info_array = array; }\n+  Array<InlineLayoutInfo>* inline_layout_info_array() const { return _inline_layout_info_array; }\n+  void set_inline_layout_info(int index, InlineLayoutInfo *info) {\n+    assert(_inline_layout_info_array != nullptr ,\"Array not created\");\n+    _inline_layout_info_array->at_put(index, *info);\n+  }\n+  InlineLayoutInfo inline_layout_info(int index) const {\n+    assert(_inline_layout_info_array != nullptr ,\"Array not created\");\n+    return _inline_layout_info_array->at(index);\n+  }\n+  InlineLayoutInfo* inline_layout_info_adr(int index) {\n+    assert(_inline_layout_info_array != nullptr ,\"Array not created\");\n+    return _inline_layout_info_array->adr_at(index);\n+  }\n+\n+  inline InlineKlass* get_inline_type_field_klass(int idx) const ;\n+  inline InlineKlass* get_inline_type_field_klass_or_null(int idx) const;\n+\n@@ -951,1 +1090,1 @@\n-  int size_helper() const {\n+  virtual int size_helper() const {\n@@ -1002,0 +1141,1 @@\n+  const char* signature_name_of_carrier(char c) const;\n@@ -1127,1 +1267,1 @@\n-  void restore_unshareable_info(ClassLoaderData* loader_data, Handle protection_domain, PackageEntry* pkg_entry, TRAPS);\n+  virtual void restore_unshareable_info(ClassLoaderData* loader_data, Handle protection_domain, PackageEntry* pkg_entry, TRAPS);\n@@ -1154,1 +1294,2 @@\n-  void oop_print_on      (oop obj, outputStream* st);\n+  void oop_print_on      (oop obj, outputStream* st) { oop_print_on(obj, st, 0, 0); }\n+  void oop_print_on      (oop obj, outputStream* st, int indent = 0, int base_offset = 0);\n","filename":"src\/hotspot\/share\/oops\/instanceKlass.hpp","additions":149,"deletions":8,"binary":false,"changes":157,"status":"modified"},{"patch":"@@ -281,14 +281,0 @@\n-static markWord make_prototype(const Klass* kls) {\n-  markWord prototype = markWord::prototype();\n-#ifdef _LP64\n-  if (UseCompactObjectHeaders) {\n-    \/\/ With compact object headers, the narrow Klass ID is part of the mark word.\n-    \/\/ We therefore seed the mark word with the narrow Klass ID.\n-    precond(CompressedKlassPointers::is_encodable(kls));\n-    const narrowKlass nk = CompressedKlassPointers::encode(const_cast<Klass*>(kls));\n-    prototype = prototype.set_narrow_klass(nk);\n-  }\n-#endif\n-  return prototype;\n-}\n-\n@@ -307,2 +293,1 @@\n-Klass::Klass(KlassKind kind) : _kind(kind),\n-                               _prototype_header(make_prototype(this)),\n+Klass::Klass(KlassKind kind, markWord prototype_header) : _kind(kind),\n@@ -310,0 +295,1 @@\n+  set_prototype_header(make_prototype_header(this, prototype_header));\n@@ -322,2 +308,2 @@\n-  int  tag   =  isobj ? _lh_array_tag_obj_value : _lh_array_tag_type_value;\n-  int lh = array_layout_helper(tag, hsize, etype, exact_log2(esize));\n+  int  tag   =  isobj ? _lh_array_tag_ref_value : _lh_array_tag_type_value;\n+  int lh = array_layout_helper(tag, false, hsize, etype, exact_log2(esize));\n@@ -327,1 +313,1 @@\n-  assert(layout_helper_is_objArray(lh) == isobj, \"correct kind\");\n+  assert(layout_helper_is_refArray(lh) == isobj, \"correct kind\");\n@@ -1038,4 +1024,2 @@\n-     if (UseCompactObjectHeaders) {\n-       st->print(BULLET\"prototype_header: \" INTPTR_FORMAT, _prototype_header.value());\n-       st->cr();\n-     }\n+     st->print(BULLET\"prototype_header: \" INTPTR_FORMAT, _prototype_header.value());\n+     st->cr();\n","filename":"src\/hotspot\/share\/oops\/klass.cpp","additions":7,"deletions":23,"binary":false,"changes":30,"status":"modified"},{"patch":"@@ -69,12 +69,16 @@\n-  enum KlassKind : u2 {\n-    InstanceKlassKind,\n-    InstanceRefKlassKind,\n-    InstanceMirrorKlassKind,\n-    InstanceClassLoaderKlassKind,\n-    InstanceStackChunkKlassKind,\n-    TypeArrayKlassKind,\n-    ObjArrayKlassKind,\n-    UnknownKlassKind\n-  };\n-\n-  static const uint KLASS_KIND_COUNT = ObjArrayKlassKind + 1;\n+   enum KlassKind : u2\n+   {\n+     InstanceKlassKind,\n+     InlineKlassKind,\n+     InstanceRefKlassKind,\n+     InstanceMirrorKlassKind,\n+     InstanceClassLoaderKlassKind,\n+     InstanceStackChunkKlassKind,\n+     TypeArrayKlassKind,\n+     ObjArrayKlassKind,\n+     RefArrayKlassKind,\n+     FlatArrayKlassKind,\n+     UnknownKlassKind\n+   };\n+\n+   static const uint KLASS_KIND_COUNT = FlatArrayKlassKind + 1;\n@@ -102,1 +106,1 @@\n-  \/\/    tag is 0x80 if the elements are oops, 0xC0 if non-oops\n+  \/\/    tag is 0x80 if the elements are oops, 0xC0 if non-oops, 0xA0 if value types\n@@ -207,1 +211,1 @@\n-  Klass(KlassKind kind);\n+  Klass(KlassKind kind, markWord prototype_header = markWord::prototype());\n@@ -475,1 +479,1 @@\n-  static const int _lh_array_tag_bits          = 2;\n+  static const int _lh_array_tag_bits          = 4;\n@@ -477,2 +481,9 @@\n-  static const int _lh_array_tag_obj_value     = ~0x01;   \/\/ 0x80000000 >> 30\n-  static const unsigned int _lh_array_tag_type_value = 0Xffffffff; \/\/ ~0x00,  \/\/ 0xC0000000 >> 30\n+  static const unsigned int _lh_array_tag_type_value = 0Xfffffffc;\n+  static const unsigned int _lh_array_tag_flat_value = 0Xfffffffa;\n+  static const unsigned int _lh_array_tag_ref_value  = 0Xfffffff8;\n+\n+  \/\/ null-free array flag bit under the array tag bits, shift one more to get array tag value\n+  static const int _lh_null_free_shift = _lh_array_tag_shift - 1;\n+  static const int _lh_null_free_mask  = 1;\n+\n+  static const jint _lh_array_tag_flat_value_bit_inplace = (jint) (1 << (_lh_array_tag_shift + 1));\n@@ -496,2 +507,1 @@\n-    \/\/ _lh_array_tag_type_value == (lh >> _lh_array_tag_shift);\n-    return (juint)lh >= (juint)(_lh_array_tag_type_value << _lh_array_tag_shift);\n+    return (juint) _lh_array_tag_type_value == (juint)(lh >> _lh_array_tag_shift);\n@@ -499,3 +509,14 @@\n-  static bool layout_helper_is_objArray(jint lh) {\n-    \/\/ _lh_array_tag_obj_value == (lh >> _lh_array_tag_shift);\n-    return (jint)lh < (jint)(_lh_array_tag_type_value << _lh_array_tag_shift);\n+  static bool layout_helper_is_refArray(jint lh) {\n+    return (juint)_lh_array_tag_ref_value == (juint)(lh >> _lh_array_tag_shift);\n+  }\n+  static bool layout_helper_is_flatArray(jint lh) {\n+    return (juint)_lh_array_tag_flat_value == (juint)(lh >> _lh_array_tag_shift);\n+  }\n+  static bool layout_helper_is_null_free(jint lh) {\n+    assert(layout_helper_is_flatArray(lh) || layout_helper_is_refArray(lh), \"must be array of inline types\");\n+    return ((lh >> _lh_null_free_shift) & _lh_null_free_mask);\n+  }\n+  static jint layout_helper_set_null_free(jint lh) {\n+    lh |= (_lh_null_free_mask << _lh_null_free_shift);\n+    assert(layout_helper_is_null_free(lh), \"Bad encoding\");\n+    return lh;\n@@ -512,1 +533,1 @@\n-    assert(btvalue >= T_BOOLEAN && btvalue <= T_OBJECT, \"sanity\");\n+    assert((btvalue >= T_BOOLEAN && btvalue <= T_OBJECT) || btvalue == T_FLAT_ELEMENT, \"sanity\");\n@@ -533,1 +554,1 @@\n-    assert(l2esz <= LogBytesPerLong,\n+    assert(layout_helper_element_type(lh) == T_FLAT_ELEMENT || l2esz <= LogBytesPerLong,\n@@ -537,1 +558,1 @@\n-  static jint array_layout_helper(jint tag, int hsize, BasicType etype, int log2_esize) {\n+  static jint array_layout_helper(jint tag, bool null_free, int hsize, BasicType etype, int log2_esize) {\n@@ -539,0 +560,1 @@\n+      |    ((null_free ? 1 : 0) <<  _lh_null_free_shift)\n@@ -682,0 +704,1 @@\n+  virtual bool is_refArray_klass_slow()     const { return false; }\n@@ -683,0 +706,1 @@\n+  virtual bool is_flatArray_klass_slow()    const { return false; }\n@@ -684,0 +708,2 @@\n+  \/\/ current implementation uses this method even in non debug builds\n+  virtual bool is_inline_klass_slow()       const { return false; }\n@@ -699,2 +725,1 @@\n-  \/\/ Other is anything that is not one of the more specialized kinds of InstanceKlass.\n-  bool is_other_instance_klass()        const { return _kind == InstanceKlassKind; }\n+  bool is_inline_klass()                const { return assert_same_query(_kind == InlineKlassKind, is_inline_klass_slow()); }\n@@ -706,1 +731,3 @@\n-  bool is_objArray_klass()              const { return assert_same_query( _kind == ObjArrayKlassKind,  is_objArray_klass_slow()); }\n+  bool is_flatArray_klass()             const { return assert_same_query( _kind == FlatArrayKlassKind, is_flatArray_klass_slow()); }\n+  bool is_objArray_klass()              const { return assert_same_query( _kind == ObjArrayKlassKind || _kind == RefArrayKlassKind || _kind == FlatArrayKlassKind,  is_objArray_klass_slow()); }\n+  bool is_refArray_klass()              const { return assert_same_query( _kind == RefArrayKlassKind, is_refArray_klass_slow()); }\n@@ -708,0 +735,1 @@\n+  bool is_refined_objArray_klass()      const { return is_refArray_klass() || is_flatArray_klass(); }\n@@ -710,0 +738,2 @@\n+  inline bool is_null_free_array_klass() const { return !is_typeArray_klass() && layout_helper_is_null_free(layout_helper()); }\n+\n@@ -718,1 +748,1 @@\n-  bool is_super() const                 { return _access_flags.is_super(); }\n+  bool is_identity_class() const        { assert(is_instance_klass(), \"only for instanceKlass\"); return _access_flags.is_identity_class(); }\n@@ -735,0 +765,1 @@\n+  static inline markWord make_prototype_header(const Klass* kls, markWord prototype = markWord::prototype());\n@@ -738,0 +769,1 @@\n+  inline void set_prototype_header_klass(narrowKlass klass);\n","filename":"src\/hotspot\/share\/oops\/klass.hpp","additions":61,"deletions":29,"binary":false,"changes":90,"status":"modified"},{"patch":"@@ -31,0 +31,1 @@\n+#include \"oops\/layoutKind.hpp\"\n@@ -33,0 +34,1 @@\n+#include \"utilities\/vmEnums.hpp\"\n@@ -44,1 +46,1 @@\n-\/\/  unused:22 hash:31 -->| unused_gap:4  age:4  self-fwd:1  lock:2 (normal object)\n+\/\/  unused:22 hash:31 -->| valhalla:4  age:4  self-fwd:1  lock:2 (normal object)\n@@ -48,1 +50,1 @@\n-\/\/  klass:22  hash:31 -->| unused_gap:4  age:4  self-fwd:1  lock:2 (normal object)\n+\/\/  klass:22  hash:31 -->| valhalla:4  age:4  self-fwd:1  lock:2 (normal object)\n@@ -67,0 +69,1 @@\n+\/\/\n@@ -70,0 +73,18 @@\n+\/\/\n+\/\/  VALHALLA EXTENSIONS:\n+\/\/\n+\/\/  N.B.: 32 bit mode is not supported, this section assumes 64 bit systems.\n+\/\/\n+\/\/  Project Valhalla uses markWord bits to denote the following oops (listed least to most significant):\n+\/\/  * inline types: have alternative bytecode behavior, e.g. can not be locked\n+\/\/  * flat arrays: load\/decode of klass layout helper is expensive for aaload\n+\/\/  * \"null free\" arrays: load\/decode of klass layout helper again for aaload\n+\/\/  * inline type: \"larval state\": mutable state, but only during object init, observable\n+\/\/      by only by a single thread (generally do not mutate markWord)\n+\/\/\n+\/\/  Inline types cannot be locked, monitored or inflating.\n+\/\/\n+\/\/  Note the position of 'self-fwd' is not by accident. When forwarding an\n+\/\/  object to a new heap position, HeapWord alignment guarantees the lower\n+\/\/  bits, including 'self-fwd' are 0. \"is_self_forwarded()\" will be correctly\n+\/\/  set to false. Otherwise encode_pointer_as_mark() may have 'self-fwd' set.\n@@ -108,2 +129,1 @@\n-  \/\/ Constants\n-  static const int age_bits                       = 4;\n+  \/\/ Constants, in least significant bit order\n@@ -112,1 +132,8 @@\n-  static const int max_hash_bits                  = BitsPerWord - age_bits - lock_bits - self_fwd_bits;\n+  \/\/ instance state\n+  static const int age_bits                       = 4;\n+  \/\/ EnableValhalla: static prototype header bits (fast path instead of klass layout_helper)\n+  static const int inline_type_bits               = 1;\n+  static const int null_free_array_bits           = LP64_ONLY(1) NOT_LP64(0);\n+  static const int flat_array_bits                = LP64_ONLY(1) NOT_LP64(0);\n+  static const int larval_bits                    = 1;\n+  static const int max_hash_bits                  = BitsPerWord - age_bits - lock_bits - inline_type_bits - larval_bits - flat_array_bits - null_free_array_bits - self_fwd_bits;\n@@ -114,1 +141,0 @@\n-  static const int unused_gap_bits                = LP64_ONLY(4) NOT_LP64(0); \/\/ Reserved for Valhalla.\n@@ -119,1 +145,5 @@\n-  static const int hash_shift                     = age_shift + age_bits + unused_gap_bits;\n+  static const int inline_type_shift              = age_shift + age_bits;\n+  static const int null_free_array_shift          = inline_type_shift + inline_type_bits;\n+  static const int flat_array_shift               = null_free_array_shift + null_free_array_bits;\n+  static const int larval_shift                   = flat_array_shift + flat_array_bits;\n+  static const int hash_shift                     = larval_shift + larval_bits;\n@@ -125,0 +155,8 @@\n+  static const uintptr_t inline_type_bit_in_place = right_n_bits(inline_type_bits) << inline_type_shift;\n+  static const uintptr_t inline_type_mask_in_place = inline_type_bit_in_place + lock_mask;\n+  static const uintptr_t null_free_array_mask     = right_n_bits(null_free_array_bits);\n+  static const uintptr_t null_free_array_mask_in_place = (null_free_array_mask << null_free_array_shift) | lock_mask_in_place;\n+  static const uintptr_t null_free_array_bit_in_place  = (right_n_bits(null_free_array_bits) << null_free_array_shift);\n+  static const uintptr_t flat_array_mask          = right_n_bits(flat_array_bits);\n+  static const uintptr_t flat_array_mask_in_place = (flat_array_mask << flat_array_shift) | null_free_array_mask_in_place | lock_mask_in_place;\n+  static const uintptr_t flat_array_bit_in_place  = right_n_bits(flat_array_bits) << flat_array_shift;\n@@ -127,0 +165,5 @@\n+\n+  static const uintptr_t larval_mask              = right_n_bits(larval_bits);\n+  static const uintptr_t larval_mask_in_place     = (larval_mask << larval_shift) | inline_type_mask_in_place;\n+  static const uintptr_t larval_bit_in_place      = right_n_bits(larval_bits) << larval_shift;\n+\n@@ -149,0 +192,7 @@\n+  static const uintptr_t inline_type_pattern      = inline_type_bit_in_place | unlocked_value;\n+  static const uintptr_t null_free_array_pattern  = null_free_array_bit_in_place | unlocked_value;\n+  static const uintptr_t null_free_flat_array_pattern = flat_array_bit_in_place | null_free_array_pattern;\n+  static const uintptr_t nullable_flat_array_pattern = flat_array_bit_in_place | unlocked_value;\n+\n+  static const uintptr_t larval_pattern           = larval_bit_in_place | inline_type_pattern;\n+\n@@ -158,0 +208,4 @@\n+  bool is_inline_type() const {\n+    return (mask_bits(value(), inline_type_mask_in_place) == inline_type_pattern);\n+  }\n+\n@@ -168,0 +222,7 @@\n+\n+  \/\/ is unlocked and not an inline type (which cannot be involved in locking, displacement or inflation)\n+  \/\/ i.e. test both lock bits and the inline type bit together\n+  bool is_neutral()  const {  \/\/ Not locked, or marked - a \"clean\" neutral state\n+    return (mask_bits(value(), inline_type_mask_in_place) == unlocked_value);\n+  }\n+\n@@ -172,3 +233,0 @@\n-  bool is_neutral()  const {  \/\/ Not locked, or marked - a \"clean\" neutral state\n-    return (mask_bits(value(), lock_mask_in_place) == unlocked_value);\n-  }\n@@ -191,1 +249,2 @@\n-    return (!is_unlocked() || !has_no_hash());\n+    return (!is_unlocked() || !has_no_hash() ||\n+      (EnableValhalla && (is_larval_state() || is_inline_type() || is_flat_array() || is_null_free_array())));\n@@ -272,0 +331,28 @@\n+  \/\/ private buffered value operations\n+  markWord enter_larval_state() const {\n+    return markWord(value() | larval_bit_in_place);\n+  }\n+  markWord exit_larval_state() const {\n+    return markWord(value() & ~larval_bit_in_place);\n+  }\n+  bool is_larval_state() const {\n+    return (mask_bits(value(), larval_mask_in_place) == larval_pattern);\n+  }\n+\n+  bool is_flat_array() const {\n+#ifdef _LP64 \/\/ 64 bit encodings only\n+    return (mask_bits(value(), flat_array_mask_in_place) == null_free_flat_array_pattern)\n+           || (mask_bits(value(), flat_array_mask_in_place) == nullable_flat_array_pattern);\n+#else\n+    return false;\n+#endif\n+  }\n+\n+  bool is_null_free_array() const {\n+#ifdef _LP64 \/\/ 64 bit encodings only\n+    return (mask_bits(value(), null_free_array_mask_in_place) == null_free_array_pattern);\n+#else\n+    return false;\n+#endif\n+  }\n+\n@@ -283,0 +370,12 @@\n+  static markWord inline_type_prototype() {\n+    return markWord(inline_type_pattern);\n+  }\n+\n+#ifdef _LP64 \/\/ 64 bit encodings only\n+  static markWord flat_array_prototype(LayoutKind lk);\n+\n+  static markWord null_free_array_prototype() {\n+    return markWord(null_free_array_pattern);\n+  }\n+#endif\n+\n@@ -289,2 +388,3 @@\n-  \/\/ Recover address of oop from encoded form used in mark\n-  inline void* decode_pointer() const { return (void*)clear_lock_bits().value(); }\n+  inline void* decode_pointer() const {\n+    return (void*) (clear_lock_bits().value());\n+  }\n","filename":"src\/hotspot\/share\/oops\/markWord.hpp","additions":113,"deletions":13,"binary":false,"changes":126,"status":"modified"},{"patch":"@@ -61,0 +61,2 @@\n+  inline static objArrayOop cast(oop o);\n+\n@@ -66,0 +68,1 @@\n+  oop obj_at(int index, TRAPS) const;\n@@ -68,14 +71,1 @@\n-\n-  oop replace_if_null(int index, oop exchange_value);\n-\n-  \/\/ Sizing\n-  size_t object_size()        { return object_size(length()); }\n-\n-  static size_t object_size(int length) {\n-    \/\/ This returns the object size in HeapWords.\n-    size_t asz = (size_t)length * heapOopSize;\n-    size_t size_words = heap_word_size(base_offset_in_bytes() + asz);\n-    size_t osz = align_object_size(size_words);\n-    assert(osz < max_jint, \"no overflow\");\n-    return osz;\n-  }\n+  void obj_at_put(int index, oop value, TRAPS);\n","filename":"src\/hotspot\/share\/oops\/objArrayOop.hpp","additions":4,"deletions":14,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -46,0 +46,10 @@\n+\/\/\n+\/\/ oopDesc::_mark - the \"oop mark word\" encoding to be found separately in markWord.hpp\n+\/\/\n+\/\/ oopDesc::_metadata - encodes the object's klass pointer, as a raw pointer in \"_klass\"\n+\/\/                      or compressed pointer in \"_compressed_klass\"\n+\/\/\n+\/\/ The overall size of the _metadata field is dependent on \"UseCompressedClassPointers\",\n+\/\/ hence the terms \"narrow\" (32 bits) vs \"wide\" (64 bits).\n+\/\/\n+\n@@ -87,0 +97,1 @@\n+  inline void reinit_mark(); \/\/ special for parallelGC\n@@ -122,6 +133,11 @@\n-  inline bool is_instance()    const;\n-  inline bool is_instanceRef() const;\n-  inline bool is_stackChunk()  const;\n-  inline bool is_array()       const;\n-  inline bool is_objArray()    const;\n-  inline bool is_typeArray()   const;\n+  inline bool is_instance()         const;\n+  inline bool is_inline_type()      const;\n+  inline bool is_instanceRef()      const;\n+  inline bool is_stackChunk()       const;\n+  inline bool is_array()            const;\n+  inline bool is_objArray()         const;\n+  inline bool is_typeArray()        const;\n+  inline bool is_flatArray()        const;\n+  inline bool is_refArray()         const;\n+  inline bool is_null_free_array()  const;\n+  inline bool is_refined_objArray() const;\n@@ -130,6 +146,9 @@\n-  bool is_instance_noinline()    const;\n-  bool is_instanceRef_noinline() const;\n-  bool is_stackChunk_noinline()  const;\n-  bool is_array_noinline()       const;\n-  bool is_objArray_noinline()    const;\n-  bool is_typeArray_noinline()   const;\n+  bool is_instance_noinline()         const;\n+  bool is_instanceRef_noinline()      const;\n+  bool is_stackChunk_noinline()       const;\n+  bool is_array_noinline()            const;\n+  bool is_objArray_noinline()         const;\n+  bool is_refArray_noinline()         const;\n+  bool is_typeArray_noinline()        const;\n+  bool is_flatArray_noinline()        const;\n+  bool is_null_free_array_noinline()  const;\n","filename":"src\/hotspot\/share\/oops\/oop.hpp","additions":31,"deletions":12,"binary":false,"changes":43,"status":"modified"},{"patch":"@@ -49,0 +49,2 @@\n+typedef class     flatArrayOopDesc*           flatArrayOop;\n+typedef class     refArrayOopDesc*            refArrayOop;\n@@ -157,0 +159,2 @@\n+DEF_OOP(flatArray);\n+DEF_OOP(refArray);\n@@ -186,0 +190,1 @@\n+class     InlineKlass;\n@@ -193,0 +198,1 @@\n+class     FlatArrayKlass;\n","filename":"src\/hotspot\/share\/oops\/oopsHierarchy.hpp","additions":6,"deletions":0,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -118,1 +118,0 @@\n- private:\n","filename":"src\/hotspot\/share\/oops\/typeArrayOop.hpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -1879,1 +1879,1 @@\n-          derived->bottom_type()->make_ptr()->is_ptr()->_offset == 0, \"sanity\");\n+         derived->bottom_type()->make_ptr()->is_ptr()->offset() == 0, \"sanity\");\n@@ -1882,1 +1882,1 @@\n-  if( tj == nullptr || tj->_offset == 0 ) {\n+  if (tj == nullptr || tj->offset() == 0) {\n@@ -2048,1 +2048,1 @@\n-                  derived->bottom_type()->make_ptr()->is_ptr()->_offset == 0, \"sanity\");\n+                 derived->bottom_type()->make_ptr()->is_ptr()->offset() == 0, \"sanity\");\n@@ -2050,1 +2050,1 @@\n-          if( tj && tj->_offset != 0 && tj->isa_oop_ptr() ) {\n+          if (tj && tj->offset() != 0 && tj->isa_oop_ptr()) {\n@@ -2340,1 +2340,1 @@\n-  const TypeTuple *domain = C->tf()->domain();\n+  const TypeTuple *domain = C->tf()->domain_cc();\n@@ -2549,1 +2549,1 @@\n-                  if (is_derived && check->bottom_type()->is_ptr()->_offset != 0) {\n+                  if (is_derived && check->bottom_type()->is_ptr()->offset() != 0) {\n@@ -2553,1 +2553,1 @@\n-                    assert(check->bottom_type()->is_ptr()->_offset == 0, \"Bad base pointer\");\n+                    assert(check->bottom_type()->is_ptr()->offset() == 0, \"Bad base pointer\");\n@@ -2562,1 +2562,1 @@\n-                } else if (check->bottom_type()->is_ptr()->_offset == 0) {\n+                } else if (check->bottom_type()->is_ptr()->offset() == 0) {\n","filename":"src\/hotspot\/share\/opto\/chaitin.cpp","additions":8,"deletions":8,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -75,0 +75,1 @@\n+macro(CastI2N)\n@@ -190,0 +191,1 @@\n+macro(FlatArrayCheck)\n@@ -361,0 +363,1 @@\n+macro(StoreLSpecial)\n@@ -387,0 +390,3 @@\n+macro(InlineType)\n+macro(LoadFlat)\n+macro(StoreFlat)\n","filename":"src\/hotspot\/share\/opto\/classes.hpp","additions":6,"deletions":0,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -48,0 +48,1 @@\n+#include \"utilities\/growableArray.hpp\"\n@@ -56,0 +57,1 @@\n+class CallNode;\n@@ -99,0 +101,1 @@\n+class InlineTypeNode;\n@@ -344,0 +347,1 @@\n+  bool                  _has_circular_inline_type; \/\/ True if method loads an inline type with a circular, non-flat field\n@@ -370,0 +374,3 @@\n+  bool                  _has_flat_accesses;     \/\/ Any known flat array accesses?\n+  bool                  _flat_accesses_share_alias; \/\/ Initially all flat array share a single slice\n+  bool                  _scalarize_in_safepoints; \/\/ Scalarize inline types in safepoint debug info\n@@ -388,0 +395,2 @@\n+  GrowableArray<Node*>  _inline_type_nodes;     \/\/ List of InlineType nodes\n+  GrowableArray<Node*>  _flat_access_nodes;     \/\/ List of LoadFlat and StoreFlat nodes\n@@ -621,0 +630,2 @@\n+  bool              has_circular_inline_type() const { return _has_circular_inline_type; }\n+  void          set_has_circular_inline_type(bool z) { _has_circular_inline_type = z; }\n@@ -653,0 +664,10 @@\n+  void          set_flat_accesses()              { _has_flat_accesses = true; }\n+  bool          flat_accesses_share_alias() const { return _flat_accesses_share_alias; }\n+  void          set_flat_accesses_share_alias(bool z) { _flat_accesses_share_alias = z; }\n+  bool          scalarize_in_safepoints() const { return _scalarize_in_safepoints; }\n+  void          set_scalarize_in_safepoints(bool z) { _scalarize_in_safepoints = z; }\n+\n+  \/\/ Support for scalarized inline type calling convention\n+  bool              has_scalarized_args() const  { return _method != nullptr && _method->has_scalarized_args(); }\n+  bool              needs_stack_repair()  const  { return _method != nullptr && _method->get_Method()->c2_needs_stack_repair(); }\n+\n@@ -784,0 +805,18 @@\n+  \/\/ Keep track of inline type nodes for later processing\n+  void add_inline_type(Node* n);\n+  void remove_inline_type(Node* n);\n+  void process_inline_types(PhaseIterGVN &igvn, bool remove = false);\n+\n+  void add_flat_access(Node* n);\n+  void remove_flat_access(Node* n);\n+  void process_flat_accesses(PhaseIterGVN& igvn);\n+\n+  template <class F>\n+  void for_each_flat_access(F consumer) {\n+    for (int i = _flat_access_nodes.length() - 1; i >= 0; i--) {\n+      consumer(_flat_access_nodes.at(i));\n+    }\n+  }\n+\n+  void adjust_flat_array_access_aliases(PhaseIterGVN& igvn);\n+\n@@ -966,1 +1005,1 @@\n-  AliasType*        alias_type(const TypePtr* adr_type, ciField* field = nullptr) { return find_alias_type(adr_type, false, field); }\n+  AliasType*        alias_type(const TypePtr* adr_type, ciField* field = nullptr, bool uncached = false) { return find_alias_type(adr_type, false, field, uncached); }\n@@ -970,1 +1009,1 @@\n-  int               get_alias_index(const TypePtr* at)  { return alias_type(at)->index(); }\n+  int               get_alias_index(const TypePtr* at, bool uncached = false) { return alias_type(at, nullptr, uncached)->index(); }\n@@ -1213,1 +1252,1 @@\n-  AliasType* find_alias_type(const TypePtr* adr_type, bool no_create, ciField* field);\n+  AliasType* find_alias_type(const TypePtr* adr_type, bool no_create, ciField* field, bool uncached = false);\n@@ -1303,1 +1342,3 @@\n-  \/\/ Auxiliary methods for randomized fuzzing\/stressing\n+  Node* optimize_acmp(PhaseGVN* phase, Node* a, Node* b);\n+\n+  \/\/ Auxiliary method for randomized fuzzing\/stressing\n","filename":"src\/hotspot\/share\/opto\/compile.hpp","additions":45,"deletions":4,"binary":false,"changes":49,"status":"modified"},{"patch":"@@ -31,0 +31,1 @@\n+#include \"memory\/metaspace.hpp\"\n@@ -39,0 +40,1 @@\n+#include \"opto\/inlinetypenode.hpp\"\n@@ -166,0 +168,10 @@\n+    if ((n->Opcode() == Op_LoadX || n->Opcode() == Op_StoreX) &&\n+        !n->in(MemNode::Address)->is_AddP() &&\n+        _igvn->type(n->in(MemNode::Address))->isa_oopptr()) {\n+      \/\/ Load\/Store at mark work address is at offset 0 so has no AddP which confuses EA\n+      Node* addp = new AddPNode(n->in(MemNode::Address), n->in(MemNode::Address), _igvn->MakeConX(0));\n+      _igvn->register_new_node_with_optimizer(addp);\n+      _igvn->replace_input_of(n, MemNode::Address, addp);\n+      ideal_nodes.push(addp);\n+      _nodes.at_put_grow(addp->_idx, nullptr, nullptr);\n+    }\n@@ -416,1 +428,9 @@\n-  \/\/ 6. Reduce allocation merges used as debug information. This is done after\n+  \/\/ 6. Expand flat accesses if the object does not escape. This adds nodes to\n+  \/\/ the graph, so it has to be after split_unique_types. This expands atomic\n+  \/\/ mismatched accesses (though encapsulated in LoadFlats and StoreFlats) into\n+  \/\/ non-mismatched accesses, so it is better before reduce allocation merges.\n+  if (has_non_escaping_obj) {\n+    optimize_flat_accesses(sfn_worklist);\n+  }\n+\n+  \/\/ 7. Reduce allocation merges used as debug information. This is done after\n@@ -1255,1 +1275,9 @@\n-      SafePointScalarObjectNode* sobj = mexp.create_scalarized_object_description(alloc, sfpt);\n+      Unique_Node_List value_worklist;\n+#ifdef ASSERT\n+      const Type* res_type = alloc->result_cast()->bottom_type();\n+      if (res_type->is_inlinetypeptr() && !Compile::current()->has_circular_inline_type()) {\n+        PhiNode* phi = ophi->as_Phi();\n+        assert(!ophi->as_Phi()->can_push_inline_types_down(_igvn), \"missed earlier scalarization opportunity\");\n+      }\n+#endif\n+      SafePointScalarObjectNode* sobj = mexp.create_scalarized_object_description(alloc, sfpt, &value_worklist);\n@@ -1257,0 +1285,1 @@\n+        _compile->record_failure(C2Compiler::retry_no_reduce_allocation_merges());\n@@ -1267,0 +1296,9 @@\n+\n+      \/\/ Scalarize inline types that were added to the safepoint.\n+      \/\/ Don't allow linking a constant oop (if available) for flat array elements\n+      \/\/ because Deoptimization::reassign_flat_array_elements needs field values.\n+      const bool allow_oop = !merge_t->is_flat();\n+      for (uint j = 0; j < value_worklist.size(); ++j) {\n+        InlineTypeNode* vt = value_worklist.at(j)->as_InlineType();\n+        vt->make_scalar_in_safepoints(_igvn, allow_oop);\n+      }\n@@ -1464,1 +1502,1 @@\n-      const TypeTuple* d = call->tf()->domain();\n+      const TypeTuple* d = call->tf()->domain_sig();\n@@ -1538,0 +1576,11 @@\n+      } else if (n->as_Call()->tf()->returns_inline_type_as_fields()) {\n+        bool returns_oop = false;\n+        for (DUIterator_Fast imax, i = n->fast_outs(imax); i < imax && !returns_oop; i++) {\n+          ProjNode* pn = n->fast_out(i)->as_Proj();\n+          if (pn->_con >= TypeFunc::Parms && pn->bottom_type()->isa_ptr()) {\n+            returns_oop = true;\n+          }\n+        }\n+        if (returns_oop) {\n+          add_call_node(n->as_Call());\n+        }\n@@ -1565,1 +1614,2 @@\n-    case Op_CastX2P: {\n+    case Op_CastX2P:\n+    case Op_CastI2N: {\n@@ -1569,0 +1619,1 @@\n+    case Op_InlineType:\n@@ -1638,0 +1689,8 @@\n+    case Op_LoadFlat:\n+      \/\/ Treat LoadFlat similar to an unknown call that receives nothing and produces its results\n+      map_ideal_node(n, phantom_obj);\n+      break;\n+    case Op_StoreFlat:\n+      \/\/ Treat StoreFlat similar to a call that escapes the stored flattened fields\n+      delayed_worklist->push(n);\n+      break;\n@@ -1640,2 +1699,7 @@\n-      if (n->as_Proj()->_con == TypeFunc::Parms && n->in(0)->is_Call() &&\n-          n->in(0)->as_Call()->returns_pointer()) {\n+      if (n->as_Proj()->_con >= TypeFunc::Parms && n->in(0)->is_Call() &&\n+          (n->in(0)->as_Call()->returns_pointer() || n->bottom_type()->isa_ptr())) {\n+        assert((n->as_Proj()->_con == TypeFunc::Parms && n->in(0)->as_Call()->returns_pointer()) ||\n+               n->in(0)->as_Call()->tf()->returns_inline_type_as_fields(), \"what kind of oop return is it?\");\n+        add_local_var_and_edge(n, PointsToNode::NoEscape, n->in(0), delayed_worklist);\n+      } else if (n->as_Proj()->_con >= TypeFunc::Parms && n->in(0)->is_LoadFlat() && igvn->type(n)->isa_ptr()) {\n+        \/\/ Treat LoadFlat outputs similar to a call return value\n@@ -1727,1 +1791,1 @@\n-  assert(n->is_Store() || n->is_LoadStore() ||\n+  assert(n->is_Store() || n->is_LoadStore() || n->is_StoreFlat() ||\n@@ -1743,0 +1807,1 @@\n+    case Op_InlineType:\n@@ -1795,0 +1860,16 @@\n+    case Op_StoreFlat: {\n+      \/\/ StoreFlat globally escapes its stored flattened fields\n+      InlineTypeNode* value = n->as_StoreFlat()->value();\n+      ciInlineKlass* vk = _igvn->type(value)->inline_klass();\n+      for (int i = 0; i < vk->nof_nonstatic_fields(); i++) {\n+        ciField* field = vk->nonstatic_field_at(i);\n+        if (field->type()->is_primitive_type()) {\n+          continue;\n+        }\n+\n+        Node* field_value = value->field_value_by_offset(field->offset_in_bytes(), true);\n+        PointsToNode* field_value_ptn = ptnode_adr(field_value->_idx);\n+        set_escape_state(field_value_ptn, PointsToNode::GlobalEscape NOT_PRODUCT(COMMA \"store into a flat field\"));\n+      }\n+      break;\n+    }\n@@ -1796,4 +1877,9 @@\n-      \/\/ we are only interested in the oop result projection from a call\n-      assert(n->as_Proj()->_con == TypeFunc::Parms && n->in(0)->is_Call() &&\n-             n->in(0)->as_Call()->returns_pointer(), \"Unexpected node type\");\n-      add_local_var_and_edge(n, PointsToNode::NoEscape, n->in(0), nullptr);\n+      if (n->in(0)->is_Call()) {\n+        \/\/ we are only interested in the oop result projection from a call\n+        assert((n->as_Proj()->_con == TypeFunc::Parms && n->in(0)->as_Call()->returns_pointer()) ||\n+              n->in(0)->as_Call()->tf()->returns_inline_type_as_fields(), \"what kind of oop return is it?\");\n+        add_local_var_and_edge(n, PointsToNode::NoEscape, n->in(0), nullptr);\n+      } else if (n->in(0)->is_LoadFlat()) {\n+        \/\/ Treat LoadFlat outputs similar to a call return value\n+        add_local_var_and_edge(n, PointsToNode::NoEscape, n->in(0), nullptr);\n+      }\n@@ -1974,1 +2060,1 @@\n-  assert(call->returns_pointer(), \"only for call which returns pointer\");\n+  assert(call->returns_pointer() || call->tf()->returns_inline_type_as_fields(), \"only for call which returns pointer\");\n@@ -2050,1 +2136,3 @@\n-      assert(strncmp(name, \"C2 Runtime multianewarray\", 25) == 0, \"TODO: add failed case check\");\n+      assert(strncmp(name, \"C2 Runtime multianewarray\", 25) == 0 ||\n+             strncmp(name, \"C2 Runtime load_unknown_inline\", 30) == 0 ||\n+             strncmp(name, \"store_inline_type_fields_to_buf\", 31) == 0, \"TODO: add failed case check\");\n@@ -2081,1 +2169,1 @@\n-        const TypeTuple* d = call->tf()->domain();\n+        const TypeTuple* d = call->tf()->domain_cc();\n@@ -2129,1 +2217,1 @@\n-      const TypeTuple * d = call->tf()->domain();\n+      const TypeTuple * d = call->tf()->domain_sig();\n@@ -2160,1 +2248,4 @@\n-                               (aat->isa_aryptr() && (aat->isa_aryptr()->elem() == Type::BOTTOM || aat->isa_aryptr()->elem()->make_oopptr() != nullptr)));\n+                               (aat->isa_aryptr() && (aat->isa_aryptr()->elem() == Type::BOTTOM || aat->isa_aryptr()->elem()->make_oopptr() != nullptr)) ||\n+                               (aat->isa_aryptr() && aat->isa_aryptr()->elem() != nullptr &&\n+                                                               aat->isa_aryptr()->is_flat() &&\n+                                                               aat->isa_aryptr()->elem()->inline_klass()->contains_oops()));\n@@ -2224,0 +2315,4 @@\n+                  strcmp(call->as_CallLeaf()->_name, \"vectorizedMismatch\") == 0 ||\n+                  strcmp(call->as_CallLeaf()->_name, \"load_unknown_inline\") == 0 ||\n+                  strcmp(call->as_CallLeaf()->_name, \"store_unknown_inline\") == 0 ||\n+                  strcmp(call->as_CallLeaf()->_name, \"store_inline_type_fields_to_buf\") == 0 ||\n@@ -2286,1 +2381,1 @@\n-        const TypeTuple* d = call->tf()->domain();\n+        const TypeTuple* d = call->tf()->domain_cc();\n@@ -2330,1 +2425,1 @@\n-      const TypeTuple* d = call->tf()->domain();\n+      const TypeTuple* d = call->tf()->domain_cc();\n@@ -2743,0 +2838,1 @@\n+  PointsToNode* init_val = phantom_obj;\n@@ -2748,1 +2844,7 @@\n-    return 0;\n+    if (alloc->as_Allocate()->in(AllocateNode::InitValue) != nullptr) {\n+      \/\/ Null-free inline type arrays are initialized with an init value instead of null\n+      init_val = ptnode_adr(alloc->as_Allocate()->in(AllocateNode::InitValue)->_idx);\n+      assert(init_val != nullptr, \"init value should be registered\");\n+    } else {\n+      return 0;\n+    }\n@@ -2750,1 +2852,2 @@\n-  assert(pta->arraycopy_dst() || alloc->as_CallStaticJava(), \"sanity\");\n+  \/\/ Non-escaped allocation returned from Java or runtime call has unknown values in fields.\n+  assert(pta->arraycopy_dst() || alloc->is_CallStaticJava() || init_val != phantom_obj, \"sanity\");\n@@ -2752,1 +2855,1 @@\n-  if (!pta->arraycopy_dst() && alloc->as_CallStaticJava()->method() == nullptr) {\n+  if (alloc->is_CallStaticJava() && alloc->as_CallStaticJava()->method() == nullptr) {\n@@ -2754,1 +2857,3 @@\n-    assert(strncmp(name, \"C2 Runtime multianewarray\", 25) == 0, \"sanity\");\n+    assert(strncmp(name, \"C2 Runtime multianewarray\", 25) == 0 ||\n+           strncmp(name, \"C2 Runtime load_unknown_inline\", 30) == 0 ||\n+           strncmp(name, \"store_inline_type_fields_to_buf\", 31) == 0, \"sanity\");\n@@ -2762,1 +2867,1 @@\n-      if (add_edge(field, phantom_obj)) {\n+      if (add_edge(field, init_val)) {\n@@ -2777,1 +2882,1 @@\n-  if (!alloc->is_Allocate()) {\n+  if (!alloc->is_Allocate() || alloc->as_Allocate()->in(AllocateNode::InitValue) != nullptr) {\n@@ -2863,1 +2968,1 @@\n-                tty->print_cr(\"----------missed referernce to object-----------\");\n+                tty->print_cr(\"----------missed reference to object------------\");\n@@ -2865,1 +2970,1 @@\n-                tty->print_cr(\"----------object referernced by init store -----\");\n+                tty->print_cr(\"----------object referenced by init store-------\");\n@@ -3229,1 +3334,2 @@\n-          if (can_eliminate_lock(alock)) {\n+          const Type* obj_type = igvn->type(alock->obj_node());\n+          if (can_eliminate_lock(alock) && !obj_type->is_inlinetypeptr()) {\n@@ -3271,5 +3377,10 @@\n-      MemBarNode* mb = MemBarNode::make(C, Op_MemBarCPUOrder, Compile::AliasIdxBot);\n-      mb->init_req(TypeFunc::Memory,  storestore->in(TypeFunc::Memory));\n-      mb->init_req(TypeFunc::Control, storestore->in(TypeFunc::Control));\n-      igvn->register_new_node_with_optimizer(mb);\n-      igvn->replace_node(storestore, mb);\n+      if (alloc->in(AllocateNode::InlineType) != nullptr) {\n+        \/\/ Non-escaping inline type buffer allocations don't require a membar\n+        storestore->as_MemBar()->remove(_igvn);\n+      } else {\n+        MemBarNode* mb = MemBarNode::make(C, Op_MemBarCPUOrder, Compile::AliasIdxBot);\n+        mb->init_req(TypeFunc::Memory,  storestore->in(TypeFunc::Memory));\n+        mb->init_req(TypeFunc::Control, storestore->in(TypeFunc::Control));\n+        igvn->register_new_node_with_optimizer(mb);\n+        igvn->replace_node(storestore, mb);\n+      }\n@@ -3280,0 +3391,25 @@\n+\/\/ Atomic flat accesses on non-escaping objects can be optimized to non-atomic accesses\n+void ConnectionGraph::optimize_flat_accesses(GrowableArray<SafePointNode*>& sfn_worklist) {\n+  PhaseIterGVN& igvn = *_igvn;\n+  bool delay = igvn.delay_transform();\n+  igvn.set_delay_transform(true);\n+  igvn.C->for_each_flat_access([&](Node* n) {\n+    Node* base = n->is_LoadFlat() ? n->as_LoadFlat()->base() : n->as_StoreFlat()->base();\n+    if (!not_global_escape(base)) {\n+      return;\n+    }\n+\n+    bool expanded;\n+    if (n->is_LoadFlat()) {\n+      expanded = n->as_LoadFlat()->expand_non_atomic(igvn);\n+    } else {\n+      expanded = n->as_StoreFlat()->expand_non_atomic(igvn);\n+    }\n+    if (expanded) {\n+      sfn_worklist.remove(n->as_SafePoint());\n+      igvn.C->remove_flat_access(n);\n+    }\n+  });\n+  igvn.set_delay_transform(delay);\n+}\n+\n@@ -3439,0 +3575,1 @@\n+  int field_offset = adr_type->isa_aryptr() ? adr_type->isa_aryptr()->field_offset().get() : Type::OffsetBot;\n@@ -3440,1 +3577,1 @@\n-  if (offset == Type::OffsetBot) {\n+  if (offset == Type::OffsetBot && field_offset == Type::OffsetBot) {\n@@ -3452,1 +3589,1 @@\n-      ciField* field = _compile->alias_type(adr_type->isa_instptr())->field();\n+      ciField* field = _compile->alias_type(adr_type->is_ptr())->field();\n@@ -3471,2 +3608,14 @@\n-        const Type* elemtype = adr_type->isa_aryptr()->elem();\n-        bt = elemtype->array_element_basic_type();\n+        const Type* elemtype = adr_type->is_aryptr()->elem();\n+        if (adr_type->is_aryptr()->is_flat() && field_offset != Type::OffsetBot) {\n+          ciInlineKlass* vk = elemtype->inline_klass();\n+          field_offset += vk->payload_offset();\n+          ciField* field = vk->get_field_by_offset(field_offset, false);\n+          if (field != nullptr) {\n+            bt = field->layout_type();\n+          } else {\n+            assert(field_offset == vk->payload_offset() + vk->null_marker_offset_in_payload(), \"no field or null marker of %s at offset %d\", vk->name()->as_utf8(), field_offset);\n+            bt = T_BOOLEAN;\n+          }\n+        } else {\n+          bt = elemtype->array_element_basic_type();\n+        }\n@@ -3669,3 +3818,1 @@\n-  const TypePtr *t_ptr = adr_type->isa_ptr();\n-  assert(t_ptr != nullptr, \"must be a pointer type\");\n-  return t_ptr->offset();\n+  return adr_type->is_ptr()->flat_offset();\n@@ -3825,1 +3972,8 @@\n-    t = base_t->add_offset(offs)->is_oopptr();\n+    if (base_t->isa_aryptr() != nullptr) {\n+      \/\/ In the case of a flat inline type array, each field has its\n+      \/\/ own slice so we need to extract the field being accessed from\n+      \/\/ the address computation\n+      t = base_t->isa_aryptr()->add_field_offset_and_offset(offs)->is_oopptr();\n+    } else {\n+      t = base_t->add_offset(offs)->is_oopptr();\n+    }\n@@ -3827,1 +3981,1 @@\n-  int inst_id =  base_t->instance_id();\n+  int inst_id = base_t->instance_id();\n@@ -3841,1 +3995,1 @@\n-  \/\/ It could happened when CHA type is different from MDO type on a dead path\n+  \/\/ It could happen when CHA type is different from MDO type on a dead path\n@@ -3851,1 +4005,12 @@\n-  const TypeOopPtr *tinst = base_t->add_offset(t->offset())->is_oopptr();\n+  const TypePtr* tinst = base_t->add_offset(t->offset());\n+  if (tinst->isa_aryptr() && t->isa_aryptr()) {\n+    \/\/ In the case of a flat inline type array, each field has its\n+    \/\/ own slice so we need to keep track of the field being accessed.\n+    tinst = tinst->is_aryptr()->with_field_offset(t->is_aryptr()->field_offset().get());\n+    \/\/ Keep array properties (not flat\/null-free)\n+    tinst = tinst->is_aryptr()->update_properties(t->is_aryptr());\n+    if (tinst == nullptr) {\n+      return false; \/\/ Skip dead path with inconsistent properties\n+    }\n+  }\n+\n@@ -4155,0 +4320,1 @@\n+#if 0  \/\/ TODO: Fix 8372259\n@@ -4161,0 +4327,3 @@\n+#else\n+        }\n+#endif\n@@ -4578,0 +4747,7 @@\n+          if (tn_t->isa_aryptr()) {\n+            \/\/ Keep array properties (not flat\/null-free)\n+            tinst = tinst->is_aryptr()->update_properties(tn_t->is_aryptr());\n+            if (tinst == nullptr) {\n+              continue; \/\/ Skip dead path with inconsistent properties\n+            }\n+          }\n@@ -4603,1 +4779,1 @@\n-      if(use->is_Mem() && use->in(MemNode::Address) == n) {\n+      if (use->is_Mem() && use->in(MemNode::Address) == n) {\n@@ -4639,0 +4815,3 @@\n+      } else if (use->Opcode() == Op_Return) {\n+        \/\/ Allocation is referenced by field of returned inline type\n+        assert(_compile->tf()->returns_inline_type_as_fields(), \"EA: unexpected reference by ReturnNode\");\n@@ -4652,1 +4831,1 @@\n-              op == Op_SubTypeCheck ||\n+              op == Op_SubTypeCheck || op == Op_InlineType || op == Op_FlatArrayCheck ||\n@@ -4758,0 +4937,3 @@\n+    } else if (n->is_CallLeaf() && n->as_CallLeaf()->_name != nullptr &&\n+               strcmp(n->as_CallLeaf()->_name, \"store_unknown_inline\") == 0) {\n+      n = n->as_CallLeaf()->proj_out(TypeFunc::Memory);\n@@ -4809,1 +4991,1 @@\n-      } else if(use->is_Mem()) {\n+      } else if (use->is_Mem()) {\n@@ -4818,0 +5000,4 @@\n+      } else if (use->is_CallLeaf() && use->as_CallLeaf()->_name != nullptr &&\n+                 strcmp(use->as_CallLeaf()->_name, \"store_unknown_inline\") == 0) {\n+        \/\/ store_unknown_inline overwrites destination array\n+        memnode_worklist.append_if_missing(use);\n@@ -4827,1 +5013,1 @@\n-              op == Op_StrEquals || op == Op_StrIndexOf || op == Op_StrIndexOfChar)) {\n+              op == Op_StrEquals || op == Op_StrIndexOf || op == Op_StrIndexOfChar || op == Op_FlatArrayCheck)) {\n@@ -4928,1 +5114,1 @@\n-  \/\/ chains as is done in split_memory_phi() since they  will\n+  \/\/ chains as is done in split_memory_phi() since they will\n","filename":"src\/hotspot\/share\/opto\/escape.cpp","additions":234,"deletions":48,"binary":false,"changes":282,"status":"modified"},{"patch":"@@ -484,0 +484,2 @@\n+  \/\/ Expand flat accesses to accesses to each component if the object does not escape\n+  void optimize_flat_accesses(GrowableArray<SafePointNode*>& sfn_worklist);\n","filename":"src\/hotspot\/share\/opto\/escape.hpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -26,0 +26,2 @@\n+#include \"ci\/ciFlatArrayKlass.hpp\"\n+#include \"ci\/ciInlineKlass.hpp\"\n@@ -34,0 +36,1 @@\n+#include \"oops\/flatArrayKlass.hpp\"\n@@ -39,0 +42,1 @@\n+#include \"opto\/inlinetypenode.hpp\"\n@@ -42,0 +46,1 @@\n+#include \"opto\/narrowptrnode.hpp\"\n@@ -49,0 +54,1 @@\n+#include \"runtime\/stubRoutines.hpp\"\n@@ -55,1 +61,1 @@\n-GraphKit::GraphKit(JVMState* jvms)\n+GraphKit::GraphKit(JVMState* jvms, PhaseGVN* gvn)\n@@ -58,1 +64,1 @@\n-    _gvn(*C->initial_gvn()),\n+    _gvn((gvn != nullptr) ? *gvn : *C->initial_gvn()),\n@@ -61,0 +67,1 @@\n+  assert(gvn == nullptr || !gvn->is_IterGVN() || gvn->is_IterGVN()->delay_transform(), \"delay transform should be enabled\");\n@@ -64,0 +71,7 @@\n+#ifdef ASSERT\n+  if (_gvn.is_IterGVN() != nullptr) {\n+    assert(_gvn.is_IterGVN()->delay_transform(), \"Transformation must be delayed if IterGVN is used\");\n+    \/\/ Save the initial size of _for_igvn worklist for verification (see ~GraphKit)\n+    _worklist_size = _gvn.C->igvn_worklist()->size();\n+  }\n+#endif\n@@ -348,1 +362,2 @@\n-  assert(ex_jvms->sp() == phi_map->_jvms->sp(), \"matching stack sizes\");\n+  \/\/ TODO 8325632 Re-enable\n+  \/\/ assert(ex_jvms->sp() == phi_map->_jvms->sp(), \"matching stack sizes\");\n@@ -876,1 +891,1 @@\n-           (is_anewarray && code == Bytecodes::_multianewarray);\n+           (is_anewarray && (code == Bytecodes::_multianewarray));\n@@ -944,1 +959,2 @@\n-    assert(out_jvms->sp() >= (uint)inputs, \"not enough operands for reexecution\");\n+    \/\/ TODO 8371125\n+    \/\/ assert(out_jvms->sp() >= (uint)inputs, \"not enough operands for reexecution\");\n@@ -967,0 +983,2 @@\n+\n+  JVMState* callee_jvms = nullptr;\n@@ -992,2 +1010,3 @@\n-      for (j = 0; j < l; j++)\n-        call->set_req(p++, in_map->in(k+j));\n+      for (j = 0; j < l; j++) {\n+        call->set_req(p++, in_map->in(k + j));\n+      }\n@@ -1003,2 +1022,3 @@\n-      for (j = 0; j < l; j++)\n-        call->set_req(p++, in_map->in(k+j));\n+      for (j = 0; j < l; j++) {\n+        call->set_req(p++, in_map->in(k + j));\n+      }\n@@ -1043,0 +1063,1 @@\n+    callee_jvms = out_jvms;\n@@ -1218,1 +1239,1 @@\n-  return _gvn.transform(LoadKlassNode::make(_gvn, immutable_memory(), k_adr, TypeInstPtr::KLASS));\n+  return _gvn.transform(LoadKlassNode::make(_gvn, immutable_memory(), k_adr, TypeInstPtr::KLASS, TypeInstKlassPtr::OBJECT));\n@@ -1267,1 +1288,2 @@\n-                                  bool speculative) {\n+                                  bool speculative,\n+                                  bool null_marker_check) {\n@@ -1272,0 +1294,23 @@\n+  if (value->is_InlineType()) {\n+    \/\/ Null checking a scalarized but nullable inline type. Check the null marker\n+    \/\/ input instead of the oop input to avoid keeping buffer allocations alive.\n+    InlineTypeNode* vtptr = value->as_InlineType();\n+    while (vtptr->get_oop()->is_InlineType()) {\n+      vtptr = vtptr->get_oop()->as_InlineType();\n+    }\n+    null_check_common(vtptr->get_null_marker(), T_INT, assert_null, null_control, speculative, true);\n+    if (stopped()) {\n+      return top();\n+    }\n+    if (assert_null) {\n+      \/\/ TODO 8284443 Scalarize here (this currently leads to compilation bailouts)\n+      \/\/ vtptr = InlineTypeNode::make_null(_gvn, vtptr->type()->inline_klass());\n+      \/\/ replace_in_map(value, vtptr);\n+      \/\/ return vtptr;\n+      replace_in_map(value, null());\n+      return null();\n+    }\n+    bool do_replace_in_map = (null_control == nullptr || (*null_control) == top());\n+    return cast_not_null(value, do_replace_in_map);\n+  }\n+\n@@ -1375,1 +1420,1 @@\n-  } else if (type == T_OBJECT) {\n+  } else if (type == T_OBJECT || null_marker_check) {\n@@ -1449,1 +1494,0 @@\n-\n@@ -1453,0 +1497,9 @@\n+  if (obj->is_InlineType()) {\n+    Node* vt = obj->isa_InlineType()->clone_if_required(&gvn(), map(), do_replace_in_map);\n+    vt->as_InlineType()->set_null_marker(_gvn);\n+    vt = _gvn.transform(vt);\n+    if (do_replace_in_map) {\n+      replace_in_map(obj, vt);\n+    }\n+    return vt;\n+  }\n@@ -1469,0 +1522,11 @@\n+Node* GraphKit::cast_to_non_larval(Node* obj) {\n+  const Type* obj_type = gvn().type(obj);\n+  if (obj->is_InlineType() || !obj_type->is_inlinetypeptr()) {\n+    return obj;\n+  }\n+\n+  Node* new_obj = InlineTypeNode::make_from_oop(this, obj, obj_type->inline_klass());\n+  replace_in_map(obj, new_obj);\n+  return new_obj;\n+}\n+\n@@ -1581,0 +1645,1 @@\n+\n@@ -1634,1 +1699,3 @@\n-                                DecoratorSet decorators) {\n+                                DecoratorSet decorators,\n+                                bool safe_for_replace,\n+                                const InlineTypeNode* vt) {\n@@ -1647,0 +1714,7 @@\n+  if (val->is_InlineType()) {\n+    \/\/ Store to non-flat field. Buffer the inline type and make sure\n+    \/\/ the store is re-executed if the allocation triggers deoptimization.\n+    PreserveReexecuteState preexecs(this);\n+    jvms()->set_should_reexecute(true);\n+    val = val->as_InlineType()->buffer(this, safe_for_replace);\n+  }\n@@ -1650,1 +1724,1 @@\n-  C2ParseAccess access(this, decorators | C2_WRITE_ACCESS, bt, obj, addr);\n+  C2ParseAccess access(this, decorators | C2_WRITE_ACCESS, bt, obj, addr, nullptr, vt);\n@@ -1663,1 +1737,2 @@\n-                               DecoratorSet decorators) {\n+                               DecoratorSet decorators,\n+                               Node* ctl) {\n@@ -1669,1 +1744,1 @@\n-  C2ParseAccess access(this, decorators | C2_READ_ACCESS, bt, obj, addr);\n+  C2ParseAccess access(this, decorators | C2_READ_ACCESS, bt, obj, addr, ctl);\n@@ -1774,2 +1849,15 @@\n-  uint shift  = exact_log2(type2aelembytes(elembt));\n-  uint header = arrayOopDesc::base_offset_in_bytes(elembt);\n+  const TypeAryPtr* arytype = _gvn.type(ary)->is_aryptr();\n+  uint shift;\n+  uint header;\n+  if (arytype->is_flat() && arytype->klass_is_exact()) {\n+    \/\/ We can only determine the flat array layout statically if the klass is exact. Otherwise, we could have different\n+    \/\/ value classes at runtime with a potentially different layout. The caller needs to fall back to call\n+    \/\/ load\/store_unknown_inline_Type() at runtime. We could return a sentinel node for the non-exact case but that\n+    \/\/ might mess with other GVN transformations in between. Thus, we just continue in the else branch normally, even\n+    \/\/ though we don't need the address node in this case and throw it away again.\n+    shift = arytype->flat_log_elem_size();\n+    header = arrayOopDesc::base_offset_in_bytes(T_FLAT_ELEMENT);\n+  } else {\n+    shift = exact_log2(type2aelembytes(elembt));\n+    header = arrayOopDesc::base_offset_in_bytes(elembt);\n+  }\n@@ -1791,0 +1879,28 @@\n+Node* GraphKit::cast_to_flat_array(Node* array, ciInlineKlass* vk, bool is_null_free, bool is_not_null_free, bool is_atomic) {\n+  assert(vk->maybe_flat_in_array(), \"element of type %s cannot be flat in array\", vk->name()->as_utf8());\n+  if (!vk->has_nullable_atomic_layout()) {\n+    \/\/ Element does not have a nullable flat layout, cannot be nullable\n+    is_null_free = true;\n+  }\n+  if (!vk->has_atomic_layout() && !vk->has_non_atomic_layout()) {\n+    \/\/ Element does not have a null-free flat layout, cannot be null-free\n+    is_not_null_free = true;\n+  }\n+  if (is_null_free) {\n+    \/\/ TODO 8350865 Impossible type\n+    is_not_null_free = false;\n+  }\n+\n+  bool is_exact = is_null_free || is_not_null_free;\n+  ciArrayKlass* array_klass = ciArrayKlass::make(vk, is_null_free, is_atomic, true);\n+  assert(array_klass->is_elem_null_free() == is_null_free, \"inconsistency\");\n+  assert(array_klass->is_elem_atomic() == is_atomic, \"inconsistency\");\n+  const TypeAryPtr* arytype = TypeOopPtr::make_from_klass(array_klass)->isa_aryptr();\n+  arytype = arytype->cast_to_exactness(is_exact);\n+  arytype = arytype->cast_to_not_null_free(is_not_null_free);\n+  assert(arytype->is_null_free() == is_null_free, \"inconsistency\");\n+  assert(arytype->is_not_null_free() == is_not_null_free, \"inconsistency\");\n+  assert(arytype->is_atomic() == is_atomic, \"inconsistency\");\n+  return _gvn.transform(new CastPPNode(control(), array, arytype, ConstraintCastNode::StrongDependency));\n+}\n+\n@@ -1806,6 +1922,42 @@\n-void GraphKit::set_arguments_for_java_call(CallJavaNode* call) {\n-  \/\/ Add the call arguments:\n-  uint nargs = call->method()->arg_size();\n-  for (uint i = 0; i < nargs; i++) {\n-    Node* arg = argument(i);\n-    call->init_req(i + TypeFunc::Parms, arg);\n+void GraphKit::set_arguments_for_java_call(CallJavaNode* call, bool is_late_inline) {\n+  PreserveReexecuteState preexecs(this);\n+  if (EnableValhalla) {\n+    \/\/ Make sure the call is \"re-executed\", if buffering of inline type arguments triggers deoptimization.\n+    \/\/ At this point, the call hasn't been executed yet, so we will only ever execute the call once.\n+    jvms()->set_should_reexecute(true);\n+    int arg_size = method()->get_declared_signature_at_bci(bci())->arg_size_for_bc(java_bc());\n+    inc_sp(arg_size);\n+  }\n+  \/\/ Add the call arguments\n+  const TypeTuple* domain = call->tf()->domain_sig();\n+  uint nargs = domain->cnt();\n+  int arg_num = 0;\n+  for (uint i = TypeFunc::Parms, idx = TypeFunc::Parms; i < nargs; i++) {\n+    Node* arg = argument(i-TypeFunc::Parms);\n+    const Type* t = domain->field_at(i);\n+    \/\/ TODO 8284443 A static call to a mismatched method should still be scalarized\n+    if (t->is_inlinetypeptr() && !call->method()->get_Method()->mismatch() && call->method()->is_scalarized_arg(arg_num)) {\n+      \/\/ We don't pass inline type arguments by reference but instead pass each field of the inline type\n+      if (!arg->is_InlineType()) {\n+        assert(_gvn.type(arg)->is_zero_type() && !t->inline_klass()->is_null_free(), \"Unexpected argument type\");\n+        arg = InlineTypeNode::make_from_oop(this, arg, t->inline_klass());\n+      }\n+      InlineTypeNode* vt = arg->as_InlineType();\n+      vt->pass_fields(this, call, idx, true, !t->maybe_null());\n+      \/\/ If an inline type argument is passed as fields, attach the Method* to the call site\n+      \/\/ to be able to access the extended signature later via attached_method_before_pc().\n+      \/\/ For example, see CompiledMethod::preserve_callee_argument_oops().\n+      call->set_override_symbolic_info(true);\n+      \/\/ Register an calling convention dependency on the callee method to make sure that this method is deoptimized and\n+      \/\/ re-compiled with a non-scalarized calling convention if the callee method is later marked as mismatched.\n+      C->dependencies()->assert_mismatch_calling_convention(call->method());\n+      arg_num++;\n+      continue;\n+    } else if (arg->is_InlineType()) {\n+      \/\/ Pass inline type argument via oop to callee\n+      arg = arg->as_InlineType()->buffer(this, true);\n+    }\n+    if (t != Type::HALF) {\n+      arg_num++;\n+    }\n+    call->init_req(idx++, arg);\n@@ -1849,7 +2001,0 @@\n-  \/\/ Capture the return value, if any.\n-  Node* ret;\n-  if (call->method() == nullptr ||\n-      call->method()->return_type()->basic_type() == T_VOID)\n-        ret = top();\n-  else  ret = _gvn.transform(new ProjNode(call, TypeFunc::Parms));\n-\n@@ -1868,0 +2013,72 @@\n+\n+  \/\/ Capture the return value, if any.\n+  Node* ret;\n+  if (call->method() == nullptr || call->method()->return_type()->basic_type() == T_VOID) {\n+    ret = top();\n+  } else if (call->tf()->returns_inline_type_as_fields()) {\n+    \/\/ Return of multiple values (inline type fields): we create a\n+    \/\/ InlineType node, each field is a projection from the call.\n+    ciInlineKlass* vk = call->method()->return_type()->as_inline_klass();\n+    uint base_input = TypeFunc::Parms;\n+    ret = InlineTypeNode::make_from_multi(this, call, vk, base_input, false, false);\n+  } else {\n+    ret = _gvn.transform(new ProjNode(call, TypeFunc::Parms));\n+    ciType* t = call->method()->return_type();\n+    if (!t->is_loaded() && InlineTypeReturnedAsFields) {\n+      \/\/ The return type is unloaded but the callee might later be C2 compiled and then return\n+      \/\/ in scalarized form when the return type is loaded. Handle this similar to what we do in\n+      \/\/ PhaseMacroExpand::expand_mh_intrinsic_return by calling into the runtime to buffer.\n+      \/\/ It's a bit unfortunate because we will deopt anyway but the interpreter needs an oop.\n+      IdealKit ideal(this);\n+      IdealVariable res(ideal);\n+      ideal.declarations_done();\n+      ideal.if_then(ret, BoolTest::eq, ideal.makecon(TypePtr::NULL_PTR)); {\n+        \/\/ Return value is null\n+        ideal.set(res, makecon(TypePtr::NULL_PTR));\n+      } ideal.else_(); {\n+        \/\/ Return value is non-null\n+        sync_kit(ideal);\n+\n+        \/\/ Change return type of call to scalarized return\n+        const TypeFunc* tf = call->_tf;\n+        const TypeTuple* domain = OptoRuntime::store_inline_type_fields_Type()->domain_cc();\n+        const TypeFunc* new_tf = TypeFunc::make(tf->domain_sig(), tf->domain_cc(), tf->range_sig(), domain);\n+        call->_tf = new_tf;\n+        _gvn.set_type(call, call->Value(&_gvn));\n+        _gvn.set_type(ret, ret->Value(&_gvn));\n+\n+        Node* store_to_buf_call = make_runtime_call(RC_NO_LEAF | RC_NO_IO,\n+                                                    OptoRuntime::store_inline_type_fields_Type(),\n+                                                    StubRoutines::store_inline_type_fields_to_buf(),\n+                                                    nullptr, TypePtr::BOTTOM, ret);\n+\n+        \/\/ We don't know how many values are returned. This assumes the\n+        \/\/ worst case, that all available registers are used.\n+        for (uint i = TypeFunc::Parms+1; i < domain->cnt(); i++) {\n+          if (domain->field_at(i) == Type::HALF) {\n+            store_to_buf_call->init_req(i, top());\n+            continue;\n+          }\n+          Node* proj =_gvn.transform(new ProjNode(call, i));\n+          store_to_buf_call->init_req(i, proj);\n+        }\n+        make_slow_call_ex(store_to_buf_call, env()->Throwable_klass(), false);\n+\n+        Node* buf = _gvn.transform(new ProjNode(store_to_buf_call, TypeFunc::Parms));\n+        const Type* buf_type = TypeOopPtr::make_from_klass(t->as_klass())->join_speculative(TypePtr::NOTNULL);\n+        buf = _gvn.transform(new CheckCastPPNode(control(), buf, buf_type));\n+\n+        ideal.set(res, buf);\n+        ideal.sync_kit(this);\n+      } ideal.end_if();\n+      sync_kit(ideal);\n+      ret = _gvn.transform(ideal.value(res));\n+    }\n+    if (t->is_klass()) {\n+      const Type* type = TypeOopPtr::make_from_klass(t->as_klass());\n+      if (type->is_inlinetypeptr()) {\n+        ret = InlineTypeNode::make_from_oop(this, ret, type->inline_klass());\n+      }\n+    }\n+  }\n+\n@@ -1969,2 +2186,1 @@\n-  CallProjections callprojs;\n-  call->extract_projections(&callprojs, true, do_asserts);\n+  CallProjections* callprojs = call->extract_projections(true, do_asserts);\n@@ -1979,2 +2195,2 @@\n-  if (callprojs.fallthrough_catchproj != nullptr) {\n-    C->gvn_replace_by(callprojs.fallthrough_catchproj, final_ctl);\n+  if (callprojs->fallthrough_catchproj != nullptr) {\n+    C->gvn_replace_by(callprojs->fallthrough_catchproj, final_ctl);\n@@ -1982,1 +2198,1 @@\n-  if (callprojs.fallthrough_memproj != nullptr) {\n+  if (callprojs->fallthrough_memproj != nullptr) {\n@@ -1987,1 +2203,1 @@\n-    C->gvn_replace_by(callprojs.fallthrough_memproj,   final_mem);\n+    C->gvn_replace_by(callprojs->fallthrough_memproj,   final_mem);\n@@ -1990,2 +2206,2 @@\n-  if (callprojs.fallthrough_ioproj != nullptr) {\n-    C->gvn_replace_by(callprojs.fallthrough_ioproj,    final_io);\n+  if (callprojs->fallthrough_ioproj != nullptr) {\n+    C->gvn_replace_by(callprojs->fallthrough_ioproj,    final_io);\n@@ -1995,2 +2211,6 @@\n-  if (callprojs.resproj != nullptr && result != nullptr) {\n-    C->gvn_replace_by(callprojs.resproj, result);\n+  if (callprojs->resproj[0] != nullptr && result != nullptr) {\n+    \/\/ If the inlined code is dead, the result projections for an inline type returned as\n+    \/\/ fields have not been replaced. They will go away once the call is replaced by TOP below.\n+    assert(callprojs->nb_resproj == 1 || (call->tf()->returns_inline_type_as_fields() && stopped()),\n+           \"unexpected number of results\");\n+    C->gvn_replace_by(callprojs->resproj[0], result);\n@@ -2001,2 +2221,2 @@\n-    if (callprojs.catchall_catchproj != nullptr) {\n-      C->gvn_replace_by(callprojs.catchall_catchproj, C->top());\n+    if (callprojs->catchall_catchproj != nullptr) {\n+      C->gvn_replace_by(callprojs->catchall_catchproj, C->top());\n@@ -2004,2 +2224,2 @@\n-    if (callprojs.catchall_memproj != nullptr) {\n-      C->gvn_replace_by(callprojs.catchall_memproj,   C->top());\n+    if (callprojs->catchall_memproj != nullptr) {\n+      C->gvn_replace_by(callprojs->catchall_memproj,   C->top());\n@@ -2007,2 +2227,2 @@\n-    if (callprojs.catchall_ioproj != nullptr) {\n-      C->gvn_replace_by(callprojs.catchall_ioproj,    C->top());\n+    if (callprojs->catchall_ioproj != nullptr) {\n+      C->gvn_replace_by(callprojs->catchall_ioproj,    C->top());\n@@ -2011,2 +2231,2 @@\n-    if (callprojs.exobj != nullptr) {\n-      C->gvn_replace_by(callprojs.exobj, C->top());\n+    if (callprojs->exobj != nullptr) {\n+      C->gvn_replace_by(callprojs->exobj, C->top());\n@@ -2023,2 +2243,2 @@\n-    if (callprojs.catchall_catchproj != nullptr) {\n-      C->gvn_replace_by(callprojs.catchall_catchproj, ekit.control());\n+    if (callprojs->catchall_catchproj != nullptr) {\n+      C->gvn_replace_by(callprojs->catchall_catchproj, ekit.control());\n@@ -2027,1 +2247,1 @@\n-    if (callprojs.catchall_memproj != nullptr) {\n+    if (callprojs->catchall_memproj != nullptr) {\n@@ -2029,1 +2249,1 @@\n-      C->gvn_replace_by(callprojs.catchall_memproj,   ex_mem);\n+      C->gvn_replace_by(callprojs->catchall_memproj,   ex_mem);\n@@ -2032,2 +2252,2 @@\n-    if (callprojs.catchall_ioproj != nullptr) {\n-      C->gvn_replace_by(callprojs.catchall_ioproj,    ekit.i_o());\n+    if (callprojs->catchall_ioproj != nullptr) {\n+      C->gvn_replace_by(callprojs->catchall_ioproj,    ekit.i_o());\n@@ -2037,2 +2257,2 @@\n-    if (callprojs.exobj != nullptr) {\n-      C->gvn_replace_by(callprojs.exobj, ex_oop);\n+    if (callprojs->exobj != nullptr) {\n+      C->gvn_replace_by(callprojs->exobj, ex_oop);\n@@ -2052,1 +2272,1 @@\n-  if (callprojs.fallthrough_catchproj != nullptr && !final_ctl->is_top() && do_replaced_nodes) {\n+  if (callprojs->fallthrough_catchproj != nullptr && !final_ctl->is_top() && do_replaced_nodes) {\n@@ -2252,1 +2472,1 @@\n-    const TypePtr* ptr = (ptr_kind == ProfileMaybeNull && current_type->speculative_maybe_null()) ? TypePtr::BOTTOM : TypePtr::NOTNULL;\n+    const TypePtr* ptr = (ptr_kind != ProfileNeverNull && current_type->speculative_maybe_null()) ? TypePtr::BOTTOM : TypePtr::NOTNULL;\n@@ -2275,1 +2495,1 @@\n-    const TypeOopPtr* spec_type = TypeOopPtr::make(TypePtr::BotPTR, Type::OffsetBot, TypeOopPtr::InstanceBot, speculative);\n+    const TypeOopPtr* spec_type = TypeOopPtr::make(TypePtr::BotPTR, Type::Offset::bottom, TypeOopPtr::InstanceBot, speculative);\n@@ -2309,2 +2529,9 @@\n-      if (!data->as_BitData()->null_seen()) {\n-        ptr_kind = ProfileNeverNull;\n+      if (java_bc() == Bytecodes::_aastore) {\n+        ciKlass* array_type = nullptr;\n+        ciKlass* element_type = nullptr;\n+        ProfilePtrKind element_ptr = ProfileMaybeNull;\n+        bool flat_array = true;\n+        bool null_free_array = true;\n+        method()->array_access_profiled_type(bci(), array_type, element_type, element_ptr, flat_array, null_free_array);\n+        exact_kls = element_type;\n+        ptr_kind = element_ptr;\n@@ -2312,8 +2539,12 @@\n-        if (TypeProfileCasts) {\n-          assert(data->is_ReceiverTypeData(), \"bad profile data type\");\n-          ciReceiverTypeData* call = (ciReceiverTypeData*)data->as_ReceiverTypeData();\n-          uint i = 0;\n-          for (; i < call->row_limit(); i++) {\n-            ciKlass* receiver = call->receiver(i);\n-            if (receiver != nullptr) {\n-              break;\n+        if (!data->as_BitData()->null_seen()) {\n+          ptr_kind = ProfileNeverNull;\n+        } else {\n+          if (TypeProfileCasts) {\n+            assert(data->is_ReceiverTypeData(), \"bad profile data type\");\n+            ciReceiverTypeData* call = (ciReceiverTypeData*)data->as_ReceiverTypeData();\n+            uint i = 0;\n+            for (; i < call->row_limit(); i++) {\n+              ciKlass* receiver = call->receiver(i);\n+              if (receiver != nullptr) {\n+                break;\n+              }\n@@ -2321,0 +2552,1 @@\n+            ptr_kind = (i == call->row_limit()) ? ProfileAlwaysNull : ProfileMaybeNull;\n@@ -2322,1 +2554,0 @@\n-          ptr_kind = (i == call->row_limit()) ? ProfileAlwaysNull : ProfileMaybeNull;\n@@ -2342,1 +2573,1 @@\n-  int             nargs = tf->domain()->cnt() - TypeFunc::Parms;\n+  int             nargs = tf->domain_sig()->cnt() - TypeFunc::Parms;\n@@ -2345,1 +2576,1 @@\n-    const Type *targ = tf->domain()->field_at(j + TypeFunc::Parms);\n+    const Type *targ = tf->domain_sig()->field_at(j + TypeFunc::Parms);\n@@ -2505,1 +2736,1 @@\n-    uint num_bits = call_type->range()->field_at(TypeFunc::Parms)->is_vect()->length_in_bytes() * BitsPerByte;\n+    uint num_bits = call_type->range_sig()->field_at(TypeFunc::Parms)->is_vect()->length_in_bytes() * BitsPerByte;\n@@ -2539,1 +2770,1 @@\n-  assert(call->in(call->req()-1) != nullptr, \"must initialize all parms\");\n+  assert(call->in(call->req()-1) != nullptr || (call->req()-1) > (TypeFunc::Parms+7), \"must initialize all parms\");\n@@ -2587,0 +2818,1 @@\n+\n@@ -2683,0 +2915,8 @@\n+  const TypeKlassPtr* klass_ptr_type = gvn.type(superklass)->is_klassptr();\n+  \/\/ For a direct pointer comparison, we need the refined array klass pointer\n+  Node* vm_superklass = superklass;\n+  if (klass_ptr_type->isa_aryklassptr() && klass_ptr_type->klass_is_exact()) {\n+    assert(!klass_ptr_type->is_aryklassptr()->is_refined_type(), \"Unexpected refined array klass pointer\");\n+    vm_superklass = gvn.makecon(klass_ptr_type->is_aryklassptr()->cast_to_refined_array_klass_ptr());\n+  }\n+\n@@ -2720,1 +2960,1 @@\n-        IfNode* iff = gen_subtype_check_compare(*ctrl, subklass, superklass, BoolTest::eq, PROB_STATIC_FREQUENT, gvn, T_ADDRESS);\n+        IfNode* iff = gen_subtype_check_compare(*ctrl, subklass, vm_superklass, BoolTest::eq, PROB_STATIC_FREQUENT, gvn, T_ADDRESS);\n@@ -2798,0 +3038,4 @@\n+        if (klass_t->isa_aryklassptr()) {\n+          \/\/ For a direct pointer comparison, we need the refined array klass pointer\n+          klass_t = klass_t->is_aryklassptr()->cast_to_refined_array_klass_ptr();\n+        }\n@@ -2851,1 +3095,1 @@\n-  IfNode *iff3 = gen_subtype_check_compare(*ctrl, subklass, superklass, BoolTest::eq, PROB_LIKELY(0.36f), gvn, T_ADDRESS);\n+  IfNode *iff3 = gen_subtype_check_compare(*ctrl, subklass, vm_superklass, BoolTest::eq, PROB_LIKELY(0.36f), gvn, T_ADDRESS);\n@@ -2889,0 +3133,5 @@\n+  const Type* sub_t = _gvn.type(obj_or_subklass);\n+  if (sub_t->make_oopptr() != nullptr && sub_t->make_oopptr()->is_inlinetypeptr()) {\n+    sub_t = TypeKlassPtr::make(sub_t->inline_klass());\n+    obj_or_subklass = makecon(sub_t);\n+  }\n@@ -2894,1 +3143,1 @@\n-    if (!_gvn.type(obj_or_subklass)->isa_klassptr()) {\n+    if (!sub_t->isa_klassptr()) {\n@@ -2912,2 +3161,1 @@\n-                                    float prob,\n-                                    Node* *casted_receiver) {\n+                                    float prob, Node* *casted_receiver) {\n@@ -2915,1 +3163,12 @@\n-\n+  Node* fail = top();\n+  const Type* rec_t = _gvn.type(receiver);\n+  if (rec_t->is_inlinetypeptr()) {\n+    if (klass->equals(rec_t->inline_klass())) {\n+      (*casted_receiver) = receiver; \/\/ Always passes\n+    } else {\n+      (*casted_receiver) = top();    \/\/ Always fails\n+      fail = control();\n+      set_control(top());\n+    }\n+    return fail;\n+  }\n@@ -2917,0 +3176,4 @@\n+  if (tklass->isa_aryklassptr()) {\n+    \/\/ For a direct pointer comparison, we need the refined array klass pointer\n+    tklass = tklass->is_aryklassptr()->cast_to_refined_array_klass_ptr();\n+  }\n@@ -2918,6 +3181,1 @@\n-  Node* want_klass = makecon(tklass);\n-  Node* cmp = _gvn.transform(new CmpPNode(recv_klass, want_klass));\n-  Node* bol = _gvn.transform(new BoolNode(cmp, BoolTest::eq));\n-  IfNode* iff = create_and_xform_if(control(), bol, prob, COUNT_UNKNOWN);\n-  set_control( _gvn.transform(new IfTrueNode (iff)));\n-  Node* fail = _gvn.transform(new IfFalseNode(iff));\n+  fail = type_check(recv_klass, tklass, prob);\n@@ -2927,2 +3185,2 @@\n-    const TypeOopPtr* recvx_type = tklass->as_instance_type();\n-    assert(recvx_type->klass_is_exact(), \"\");\n+    const TypeOopPtr* recv_xtype = tklass->as_instance_type();\n+    assert(recv_xtype->klass_is_exact(), \"\");\n@@ -2930,1 +3188,1 @@\n-    if (!receiver_type->higher_equal(recvx_type)) { \/\/ ignore redundant casts\n+    if (!receiver_type->higher_equal(recv_xtype)) { \/\/ ignore redundant casts\n@@ -2933,2 +3191,7 @@\n-      Node* cast = new CheckCastPPNode(control(), receiver, recvx_type);\n-      (*casted_receiver) = _gvn.transform(cast);\n+      Node* cast = new CheckCastPPNode(control(), receiver, recv_xtype);\n+      Node* res = _gvn.transform(cast);\n+      if (recv_xtype->is_inlinetypeptr()) {\n+        assert(!gvn().type(res)->maybe_null(), \"receiver should never be null\");\n+        res = InlineTypeNode::make_from_oop(this, res, recv_xtype->inline_klass());\n+      }\n+      (*casted_receiver) = res;\n@@ -2943,0 +3206,11 @@\n+Node* GraphKit::type_check(Node* recv_klass, const TypeKlassPtr* tklass,\n+                           float prob) {\n+  Node* want_klass = makecon(tklass);\n+  Node* cmp = _gvn.transform(new CmpPNode(recv_klass, want_klass));\n+  Node* bol = _gvn.transform(new BoolNode(cmp, BoolTest::eq));\n+  IfNode* iff = create_and_xform_if(control(), bol, prob, COUNT_UNKNOWN);\n+  set_control(_gvn.transform(new IfTrueNode (iff)));\n+  Node* fail = _gvn.transform(new IfFalseNode(iff));\n+  return fail;\n+}\n+\n@@ -2955,3 +3229,6 @@\n-    if (!receiver_type->higher_equal(recv_type)) { \/\/ ignore redundant casts\n-      Node* cast = new CheckCastPPNode(control(), receiver, recv_type);\n-      (*casted_receiver) = _gvn.transform(cast);\n+    if (receiver_type != nullptr && !receiver_type->higher_equal(recv_type)) { \/\/ ignore redundant casts\n+      Node* cast = _gvn.transform(new CheckCastPPNode(control(), receiver, recv_type));\n+      if (recv_type->is_inlinetypeptr()) {\n+        cast = InlineTypeNode::make_from_oop(this, cast, recv_type->inline_klass());\n+      }\n+      (*casted_receiver) = cast;\n@@ -3066,1 +3343,14 @@\n-  ciKlass* exact_kls = spec_klass == nullptr ? profile_has_unique_klass() : spec_klass;\n+  ciKlass* exact_kls = spec_klass;\n+  if (exact_kls == nullptr) {\n+    if (java_bc() == Bytecodes::_aastore) {\n+      ciKlass* array_type = nullptr;\n+      ciKlass* element_type = nullptr;\n+      ProfilePtrKind element_ptr = ProfileMaybeNull;\n+      bool flat_array = true;\n+      bool null_free_array = true;\n+      method()->array_access_profiled_type(bci(), array_type, element_type, element_ptr, flat_array, null_free_array);\n+      exact_kls = element_type;\n+    } else {\n+      exact_kls = profile_has_unique_klass();\n+    }\n+  }\n@@ -3196,1 +3486,1 @@\n-    if (subk->is_loaded()) {\n+    if (subk != nullptr && subk->is_loaded()) {\n@@ -3252,2 +3542,1 @@\n-Node* GraphKit::gen_checkcast(Node *obj, Node* superklass,\n-                              Node* *failure_control) {\n+Node* GraphKit::gen_checkcast(Node* obj, Node* superklass, Node* *failure_control, bool null_free, bool maybe_larval) {\n@@ -3256,0 +3545,16 @@\n+  const Type* obj_type = _gvn.type(obj);\n+  if (obj_type->is_inlinetypeptr() && !obj_type->maybe_null() && klass_ptr_type->klass_is_exact() && obj_type->inline_klass() == klass_ptr_type->exact_klass(true)) {\n+    \/\/ Special case: larval inline objects must not be scalarized. They are also generally not\n+    \/\/ allowed to participate in most operations except as the first operand of putfield, or as an\n+    \/\/ argument to a constructor invocation with it being a receiver, Unsafe::putXXX with it being\n+    \/\/ the first argument, or Unsafe::finishPrivateBuffer. This allows us to aggressively scalarize\n+    \/\/ value objects in all other places. This special case comes from the limitation of the Java\n+    \/\/ language, Unsafe::makePrivateBuffer returns an Object that is checkcast-ed to the concrete\n+    \/\/ value type. We must do this first because C->static_subtype_check may do nothing when\n+    \/\/ StressReflectiveCode is set.\n+    return obj;\n+  }\n+\n+  \/\/ Else it must be a non-larval object\n+  obj = cast_to_non_larval(obj);\n+\n@@ -3258,0 +3563,2 @@\n+  bool safe_for_replace = (failure_control == nullptr);\n+  assert(!null_free || toop->can_be_inline_type(), \"must be an inline type pointer\");\n@@ -3266,3 +3573,10 @@\n-    const TypeOopPtr* objtp = _gvn.type(obj)->isa_oopptr();\n-    if (objtp != nullptr) {\n-      switch (C->static_subtype_check(improved_klass_ptr_type, objtp->as_klass_type())) {\n+    const TypeKlassPtr* kptr = nullptr;\n+    if (obj_type->isa_oop_ptr()) {\n+      kptr = obj_type->is_oopptr()->as_klass_type();\n+    } else if (obj->is_InlineType()) {\n+      ciInlineKlass* vk = obj_type->inline_klass();\n+      kptr = TypeInstKlassPtr::make(TypePtr::NotNull, vk, Type::Offset(0));\n+    }\n+\n+    if (kptr != nullptr) {\n+      switch (C->static_subtype_check(improved_klass_ptr_type, kptr)) {\n@@ -3273,1 +3587,7 @@\n-        return record_profiled_receiver_for_speculation(obj);\n+        obj = record_profiled_receiver_for_speculation(obj);\n+        if (null_free) {\n+          assert(safe_for_replace, \"must be\");\n+          obj = null_check(obj);\n+        }\n+        assert(stopped() || !toop->is_inlinetypeptr() || obj->is_InlineType(), \"should have been scalarized\");\n+        return obj;\n@@ -3275,0 +3595,4 @@\n+        if (null_free) {\n+          assert(safe_for_replace, \"must be\");\n+          obj = null_check(obj);\n+        }\n@@ -3276,2 +3600,1 @@\n-        \/\/ A non-null value will always produce an exception.\n-        if (!objtp->maybe_null()) {\n+        if (obj_type->isa_oopptr() != nullptr && !obj_type->is_oopptr()->maybe_null()) {\n@@ -3294,1 +3617,0 @@\n-  bool safe_for_replace = false;\n@@ -3299,2 +3621,3 @@\n-    data = method()->method_data()->bci_to_data(bci());\n-    safe_for_replace = true;\n+    if (method()->method_data()->is_mature()) {\n+      data = method()->method_data()->bci_to_data(bci());\n+    }\n@@ -3307,0 +3630,3 @@\n+  _gvn.set_type(region, Type::CONTROL);\n+  _gvn.set_type(phi, toop);\n+\n@@ -3314,0 +3640,7 @@\n+  if (obj->is_InlineType()) {\n+    \/\/ Re-execute if buffering during triggers deoptimization\n+    PreserveReexecuteState preexecs(this);\n+    jvms()->set_should_reexecute(true);\n+    obj = obj->as_InlineType()->buffer(this, safe_for_replace);\n+  }\n+\n@@ -3316,1 +3649,7 @@\n-  Node* not_null_obj = null_check_oop(obj, &null_ctl, never_see_null, safe_for_replace, speculative_not_null);\n+  Node* not_null_obj = nullptr;\n+  if (null_free) {\n+    assert(safe_for_replace, \"must be\");\n+    not_null_obj = null_check(obj);\n+  } else {\n+    not_null_obj = null_check_oop(obj, &null_ctl, never_see_null, safe_for_replace, speculative_not_null);\n+  }\n@@ -3321,0 +3660,3 @@\n+    if (toop->is_inlinetypeptr()) {\n+      return InlineTypeNode::make_null(_gvn, toop->inline_klass());\n+    }\n@@ -3358,0 +3700,3 @@\n+      \/\/ Only improve the super class for constants which allows subsequent sub type checks to possibly be commoned up.\n+      \/\/ The other non-constant cases cannot be improved with a cast node here since they could be folded to top.\n+      \/\/ Additionally, the benefit would only be minor in non-constant cases.\n@@ -3361,1 +3706,0 @@\n-\n@@ -3399,1 +3743,165 @@\n-  return record_profiled_receiver_for_speculation(res);\n+  bool not_inline = !toop->can_be_inline_type();\n+  bool not_flat_in_array = !UseArrayFlattening || not_inline || (toop->is_inlinetypeptr() && !toop->inline_klass()->maybe_flat_in_array());\n+  if (EnableValhalla && (not_inline || not_flat_in_array)) {\n+    \/\/ Check if obj has been loaded from an array\n+    obj = obj->isa_DecodeN() ? obj->in(1) : obj;\n+    Node* array = nullptr;\n+    if (obj->isa_Load()) {\n+      Node* address = obj->in(MemNode::Address);\n+      if (address->isa_AddP()) {\n+        array = address->as_AddP()->in(AddPNode::Base);\n+      }\n+    } else if (obj->is_Phi()) {\n+      Node* region = obj->in(0);\n+      \/\/ TODO make this more robust (see JDK-8231346)\n+      if (region->req() == 3 && region->in(2) != nullptr && region->in(2)->in(0) != nullptr) {\n+        IfNode* iff = region->in(2)->in(0)->isa_If();\n+        if (iff != nullptr) {\n+          iff->is_flat_array_check(&_gvn, &array);\n+        }\n+      }\n+    }\n+    if (array != nullptr) {\n+      const TypeAryPtr* ary_t = _gvn.type(array)->isa_aryptr();\n+      if (ary_t != nullptr) {\n+        if (!ary_t->is_not_null_free() && !ary_t->is_null_free() && not_inline) {\n+          \/\/ Casting array element to a non-inline-type, mark array as not null-free.\n+          Node* cast = _gvn.transform(new CheckCastPPNode(control(), array, ary_t->cast_to_not_null_free()));\n+          replace_in_map(array, cast);\n+          array = cast;\n+        }\n+        if (!ary_t->is_not_flat() && !ary_t->is_flat() && not_flat_in_array) {\n+          \/\/ Casting array element to a non-flat-in-array type, mark array as not flat.\n+          Node* cast = _gvn.transform(new CheckCastPPNode(control(), array, ary_t->cast_to_not_flat()));\n+          replace_in_map(array, cast);\n+          array = cast;\n+        }\n+      }\n+    }\n+  }\n+\n+  if (!stopped() && !res->is_InlineType()) {\n+    res = record_profiled_receiver_for_speculation(res);\n+    if (toop->is_inlinetypeptr() && !maybe_larval) {\n+      Node* vt = InlineTypeNode::make_from_oop(this, res, toop->inline_klass());\n+      res = vt;\n+      if (safe_for_replace) {\n+        replace_in_map(obj, vt);\n+        replace_in_map(not_null_obj, vt);\n+        replace_in_map(res, vt);\n+      }\n+    }\n+  }\n+  return res;\n+}\n+\n+Node* GraphKit::mark_word_test(Node* obj, uintptr_t mask_val, bool eq, bool check_lock) {\n+  \/\/ Load markword\n+  Node* mark_adr = basic_plus_adr(obj, oopDesc::mark_offset_in_bytes());\n+  Node* mark = make_load(nullptr, mark_adr, TypeX_X, TypeX_X->basic_type(), MemNode::unordered);\n+  if (check_lock && !UseCompactObjectHeaders) {\n+    \/\/ COH: Locking does not override the markword with a tagged pointer. We can directly read from the markword.\n+    \/\/ Check if obj is locked\n+    Node* locked_bit = MakeConX(markWord::unlocked_value);\n+    locked_bit = _gvn.transform(new AndXNode(locked_bit, mark));\n+    Node* cmp = _gvn.transform(new CmpXNode(locked_bit, MakeConX(0)));\n+    Node* is_unlocked = _gvn.transform(new BoolNode(cmp, BoolTest::ne));\n+    IfNode* iff = new IfNode(control(), is_unlocked, PROB_MAX, COUNT_UNKNOWN);\n+    _gvn.transform(iff);\n+    Node* locked_region = new RegionNode(3);\n+    Node* mark_phi = new PhiNode(locked_region, TypeX_X);\n+\n+    \/\/ Unlocked: Use bits from mark word\n+    locked_region->init_req(1, _gvn.transform(new IfTrueNode(iff)));\n+    mark_phi->init_req(1, mark);\n+\n+    \/\/ Locked: Load prototype header from klass\n+    set_control(_gvn.transform(new IfFalseNode(iff)));\n+    \/\/ Make loads control dependent to make sure they are only executed if array is locked\n+    Node* klass_adr = basic_plus_adr(obj, oopDesc::klass_offset_in_bytes());\n+    Node* klass = _gvn.transform(LoadKlassNode::make(_gvn, C->immutable_memory(), klass_adr, TypeInstPtr::KLASS, TypeInstKlassPtr::OBJECT));\n+    Node* proto_adr = basic_plus_adr(klass, in_bytes(Klass::prototype_header_offset()));\n+    Node* proto = _gvn.transform(LoadNode::make(_gvn, control(), C->immutable_memory(), proto_adr, proto_adr->bottom_type()->is_ptr(), TypeX_X, TypeX_X->basic_type(), MemNode::unordered));\n+\n+    locked_region->init_req(2, control());\n+    mark_phi->init_req(2, proto);\n+    set_control(_gvn.transform(locked_region));\n+    record_for_igvn(locked_region);\n+\n+    mark = mark_phi;\n+  }\n+\n+  \/\/ Now check if mark word bits are set\n+  Node* mask = MakeConX(mask_val);\n+  Node* masked = _gvn.transform(new AndXNode(_gvn.transform(mark), mask));\n+  record_for_igvn(masked); \/\/ Give it a chance to be optimized out by IGVN\n+  Node* cmp = _gvn.transform(new CmpXNode(masked, mask));\n+  return _gvn.transform(new BoolNode(cmp, eq ? BoolTest::eq : BoolTest::ne));\n+}\n+\n+Node* GraphKit::inline_type_test(Node* obj, bool is_inline) {\n+  return mark_word_test(obj, markWord::inline_type_pattern, is_inline, \/* check_lock = *\/ false);\n+}\n+\n+Node* GraphKit::flat_array_test(Node* array_or_klass, bool flat) {\n+  \/\/ We can't use immutable memory here because the mark word is mutable.\n+  \/\/ PhaseIdealLoop::move_flat_array_check_out_of_loop will make sure the\n+  \/\/ check is moved out of loops (mainly to enable loop unswitching).\n+  Node* cmp = _gvn.transform(new FlatArrayCheckNode(C, memory(Compile::AliasIdxRaw), array_or_klass));\n+  record_for_igvn(cmp); \/\/ Give it a chance to be optimized out by IGVN\n+  return _gvn.transform(new BoolNode(cmp, flat ? BoolTest::eq : BoolTest::ne));\n+}\n+\n+Node* GraphKit::null_free_array_test(Node* array, bool null_free) {\n+  return mark_word_test(array, markWord::null_free_array_bit_in_place, null_free);\n+}\n+\n+Node* GraphKit::null_free_atomic_array_test(Node* array, ciInlineKlass* vk) {\n+  assert(vk->has_atomic_layout() || vk->has_non_atomic_layout(), \"Can't be null-free and flat\");\n+\n+  \/\/ TODO 8350865 Add a stress flag to always access atomic if layout exists?\n+  if (!vk->has_non_atomic_layout()) {\n+    return intcon(1); \/\/ Always atomic\n+  } else if (!vk->has_atomic_layout()) {\n+    return intcon(0); \/\/ Never atomic\n+  }\n+\n+  Node* array_klass = load_object_klass(array);\n+  int layout_kind_offset = in_bytes(FlatArrayKlass::layout_kind_offset());\n+  Node* layout_kind_addr = basic_plus_adr(array_klass, array_klass, layout_kind_offset);\n+  Node* layout_kind = make_load(nullptr, layout_kind_addr, TypeInt::INT, T_INT, MemNode::unordered);\n+  Node* cmp = _gvn.transform(new CmpINode(layout_kind, intcon((int)LayoutKind::ATOMIC_FLAT)));\n+  return _gvn.transform(new BoolNode(cmp, BoolTest::eq));\n+}\n+\n+\/\/ Deoptimize if 'ary' is a null-free inline type array and 'val' is null\n+Node* GraphKit::inline_array_null_guard(Node* ary, Node* val, int nargs, bool safe_for_replace) {\n+  RegionNode* region = new RegionNode(3);\n+  Node* null_ctl = top();\n+  null_check_oop(val, &null_ctl);\n+  if (null_ctl != top()) {\n+    PreserveJVMState pjvms(this);\n+    set_control(null_ctl);\n+    {\n+      \/\/ Deoptimize if null-free array\n+      BuildCutout unless(this, null_free_array_test(ary, \/* null_free = *\/ false), PROB_MAX);\n+      inc_sp(nargs);\n+      uncommon_trap(Deoptimization::Reason_null_check,\n+                    Deoptimization::Action_none);\n+    }\n+    region->init_req(1, control());\n+  }\n+  region->init_req(2, control());\n+  set_control(_gvn.transform(region));\n+  record_for_igvn(region);\n+  if (_gvn.type(val) == TypePtr::NULL_PTR) {\n+    \/\/ Since we were just successfully storing null, the array can't be null free.\n+    const TypeAryPtr* ary_t = _gvn.type(ary)->is_aryptr();\n+    ary_t = ary_t->cast_to_not_null_free();\n+    Node* cast = _gvn.transform(new CheckCastPPNode(control(), ary, ary_t));\n+    if (safe_for_replace) {\n+      replace_in_map(ary, cast);\n+    }\n+    ary = cast;\n+  }\n+  return ary;\n@@ -3531,0 +4039,1 @@\n+  assert(!obj->is_InlineType(), \"should not unlock on inline type\");\n@@ -3571,1 +4080,8 @@\n-    if (xklass || (klass_t->isa_aryklassptr() && klass_t->is_aryklassptr()->elem() != Type::BOTTOM)) {\n+    bool can_be_flat = false;\n+    const TypeAryPtr* ary_type = klass_t->as_instance_type()->isa_aryptr();\n+    if (UseArrayFlattening && !xklass && ary_type != nullptr && !ary_type->is_null_free()) {\n+      \/\/ Don't constant fold if the runtime type might be a flat array but the static type is not.\n+      const TypeOopPtr* elem = ary_type->elem()->make_oopptr();\n+      can_be_flat = ary_type->can_be_inline_array() && (!elem->is_inlinetypeptr() || elem->inline_klass()->maybe_flat_in_array());\n+    }\n+    if (!can_be_flat && (xklass || (klass_t->isa_aryklassptr() && klass_t->is_aryklassptr()->elem() != Type::BOTTOM))) {\n@@ -3573,2 +4089,4 @@\n-      if (klass_t->isa_aryklassptr()) {\n-        BasicType elem = klass_t->as_instance_type()->isa_aryptr()->elem()->array_element_basic_type();\n+      if (klass_t->is_flat()) {\n+        lhelper = ary_type->flat_layout_helper();\n+      } else if (klass_t->isa_aryklassptr()) {\n+        BasicType elem = ary_type->elem()->array_element_basic_type();\n@@ -3603,1 +4121,3 @@\n-  kit.set_memory(init_out_raw, alias_idx);\n+  if (init_out_raw != nullptr) {\n+    kit.set_memory(init_out_raw, alias_idx);\n+  }\n@@ -3642,0 +4162,1 @@\n+    _gvn.set_type(minit_in, Type::MEMORY);\n@@ -3652,3 +4173,28 @@\n-      const TypePtr* telemref = oop_type->add_offset(Type::OffsetBot);\n-      int            elemidx  = C->get_alias_index(telemref);\n-      hook_memory_on_init(*this, elemidx, minit_in, _gvn.transform(new NarrowMemProjNode(init, C->get_adr_type(elemidx))));\n+      const TypeAryPtr* arytype = oop_type->is_aryptr();\n+      if (arytype->is_flat()) {\n+        \/\/ Initially all flat array accesses share a single slice\n+        \/\/ but that changes after parsing. Prepare the memory graph so\n+        \/\/ it can optimize flat array accesses properly once they\n+        \/\/ don't share a single slice.\n+        assert(C->flat_accesses_share_alias(), \"should be set at parse time\");\n+        C->set_flat_accesses_share_alias(false);\n+        ciInlineKlass* vk = arytype->elem()->inline_klass();\n+        for (int i = 0, len = vk->nof_nonstatic_fields(); i < len; i++) {\n+          ciField* field = vk->nonstatic_field_at(i);\n+          if (field->offset_in_bytes() >= TrackedInitializationLimit * HeapWordSize)\n+            continue;  \/\/ do not bother to track really large numbers of fields\n+          int off_in_vt = field->offset_in_bytes() - vk->payload_offset();\n+          const TypePtr* adr_type = arytype->with_field_offset(off_in_vt)->add_offset(Type::OffsetBot);\n+          int fieldidx = C->get_alias_index(adr_type, true);\n+          \/\/ Pass nullptr for init_out. Having per flat array element field memory edges as uses of the Initialize node\n+          \/\/ can result in per flat array field Phis to be created which confuses the logic of\n+          \/\/ Compile::adjust_flat_array_access_aliases().\n+          hook_memory_on_init(*this, fieldidx, minit_in, nullptr);\n+        }\n+        C->set_flat_accesses_share_alias(true);\n+        hook_memory_on_init(*this, C->get_alias_index(TypeAryPtr::INLINES), minit_in, minit_out);\n+      } else {\n+        const TypePtr* telemref = oop_type->add_offset(Type::OffsetBot);\n+        int            elemidx  = C->get_alias_index(telemref);\n+        hook_memory_on_init(*this, elemidx, minit_in, _gvn.transform(new NarrowMemProjNode(init, C->get_adr_type(elemidx))));\n+      }\n@@ -3656,0 +4202,1 @@\n+      set_memory(minit_out, C->get_alias_index(oop_type)); \/\/ mark word\n@@ -3706,1 +4253,2 @@\n-                             bool deoptimize_on_exception) {\n+                             bool deoptimize_on_exception,\n+                             InlineTypeNode* inline_type_node) {\n@@ -3713,1 +4261,1 @@\n-  int   layout_is_con = (layout_val == nullptr);\n+  bool  layout_is_con = (layout_val == nullptr);\n@@ -3764,1 +4312,1 @@\n-  \/\/ since GC and deoptimization can happened.\n+  \/\/ since GC and deoptimization can happen.\n@@ -3771,1 +4319,1 @@\n-                                         initial_slow_test);\n+                                         initial_slow_test, inline_type_node);\n@@ -3777,1 +4325,1 @@\n-\/\/ helper for both newarray and anewarray\n+\/\/ helper for newarray and anewarray\n@@ -3787,1 +4335,2 @@\n-                          bool deoptimize_on_exception) {\n+                          bool deoptimize_on_exception,\n+                          Node* init_val) {\n@@ -3790,1 +4339,1 @@\n-  int   layout_is_con = (layout_val == nullptr);\n+  bool  layout_is_con = (layout_val == nullptr);\n@@ -3820,3 +4369,1 @@\n-    assert(fast_size_limit == 0 || count_leading_zeros(fast_size_limit) > static_cast<unsigned>(LogBytesPerLong - log2_esize),\n-           \"fast_size_limit (%d) overflow when shifted left by %d\", fast_size_limit, LogBytesPerLong - log2_esize);\n-    fast_size_limit <<= (LogBytesPerLong - log2_esize);\n+    fast_size_limit <<= MAX2(LogBytesPerLong - log2_esize, 0);\n@@ -3839,0 +4386,1 @@\n+    bool is_flat_array = Klass::layout_helper_is_flatArray(layout_con);\n@@ -3841,1 +4389,1 @@\n-    assert((hsize & right_n_bits(eshift)) == 0, \"hsize is pre-rounded\");\n+    assert(is_flat_array || (hsize & right_n_bits(eshift)) == 0, \"hsize is pre-rounded\");\n@@ -3930,1 +4478,1 @@\n-  \/\/ since GC and deoptimization can happened.\n+  \/\/ since GC and deoptimization can happen.\n@@ -3939,1 +4487,21 @@\n-  const TypeOopPtr* ary_type = _gvn.type(klass_node)->is_klassptr()->as_instance_type();\n+  const TypeKlassPtr* ary_klass = _gvn.type(klass_node)->isa_klassptr();\n+  const TypeOopPtr* ary_type = ary_klass->as_instance_type();\n+\n+  Node* raw_init_value = nullptr;\n+  if (init_val != nullptr) {\n+    \/\/ TODO 8350865 Fast non-zero init not implemented yet for flat, null-free arrays\n+    if (ary_type->is_flat()) {\n+      initial_slow_test = intcon(1);\n+    }\n+\n+    if (UseCompressedOops) {\n+      \/\/ With compressed oops, the 64-bit init value is built from two 32-bit compressed oops\n+      init_val = _gvn.transform(new EncodePNode(init_val, init_val->bottom_type()->make_narrowoop()));\n+      Node* lower = _gvn.transform(new CastP2XNode(control(), init_val));\n+      Node* upper = _gvn.transform(new LShiftLNode(lower, intcon(32)));\n+      raw_init_value = _gvn.transform(new OrLNode(lower, upper));\n+    } else {\n+      raw_init_value = _gvn.transform(new CastP2XNode(control(), init_val));\n+    }\n+  }\n+\n@@ -3954,2 +4522,2 @@\n-                            length, valid_length_test);\n-\n+                            length, valid_length_test,\n+                            init_val, raw_init_value);\n@@ -4090,0 +4658,1 @@\n+  reset_memory();\n@@ -4110,1 +4679,1 @@\n-                                                     false, nullptr, 0);\n+                                                     false, nullptr, Type::Offset(0));\n@@ -4113,2 +4682,2 @@\n-                                                  TypeAry::make(TypeInt::BYTE, TypeInt::POS),\n-                                                  ciTypeArrayKlass::make(T_BYTE), true, 0);\n+                                                  TypeAry::make(TypeInt::BYTE, TypeInt::POS, false, false, true, true),\n+                                                  ciTypeArrayKlass::make(T_BYTE), true, Type::Offset(0));\n@@ -4127,1 +4696,1 @@\n-                                                     false, nullptr, 0);\n+                                                     false, nullptr, Type::Offset(0));\n@@ -4139,1 +4708,1 @@\n-                                                     false, nullptr, 0);\n+                                                     false, nullptr, Type::Offset(0));\n@@ -4149,1 +4718,1 @@\n-                                                     false, nullptr, 0);\n+                                                     false, nullptr, Type::Offset(0));\n@@ -4262,1 +4831,7 @@\n-    return makecon(con_type);\n+    Node* con = makecon(con_type);\n+    if (field->type()->is_inlinetype()) {\n+      con = InlineTypeNode::make_from_oop(this, con, field->type()->as_inline_klass());\n+    } else if (con_type->is_inlinetypeptr()) {\n+      con = InlineTypeNode::make_from_oop(this, con, con_type->inline_klass());\n+    }\n+    return con;\n@@ -4267,0 +4842,9 @@\n+\/\/---------------------------load_mirror_from_klass----------------------------\n+\/\/ Given a klass oop, load its java mirror (a java.lang.Class oop).\n+Node* GraphKit::load_mirror_from_klass(Node* klass) {\n+  Node* p = basic_plus_adr(klass, in_bytes(Klass::java_mirror_offset()));\n+  Node* load = make_load(nullptr, p, TypeRawPtr::NOTNULL, T_ADDRESS, MemNode::unordered);\n+  \/\/ mirror = ((OopHandle)mirror)->resolve();\n+  return access_load(load, TypeInstPtr::MIRROR, T_OBJECT, IN_NATIVE);\n+}\n+\n@@ -4268,1 +4852,1 @@\n-  const TypeOopPtr* obj_type = obj->bottom_type()->isa_oopptr();\n+  const Type* obj_type = obj->bottom_type();\n@@ -4270,1 +4854,1 @@\n-  if (obj_type != nullptr && sig_type->is_loaded() && !obj_type->higher_equal(sig_type)) {\n+  if (obj_type->isa_oopptr() && sig_type->is_loaded() && !obj_type->higher_equal(sig_type)) {\n@@ -4273,1 +4857,4 @@\n-    return casted_obj;\n+    obj = casted_obj;\n+  }\n+  if (sig_type->is_inlinetypeptr()) {\n+    obj = InlineTypeNode::make_from_oop(this, obj, sig_type->inline_klass());\n","filename":"src\/hotspot\/share\/opto\/graphKit.cpp","additions":731,"deletions":144,"binary":false,"changes":875,"status":"modified"},{"patch":"@@ -26,0 +26,3 @@\n+#include \"ci\/ciArrayKlass.hpp\"\n+#include \"ci\/ciFlatArrayKlass.hpp\"\n+#include \"ci\/ciInstanceKlass.hpp\"\n@@ -32,0 +35,1 @@\n+#include \"gc\/shared\/c2\/barrierSetC2.hpp\"\n@@ -34,0 +38,1 @@\n+#include \"oops\/accessDecorators.hpp\"\n@@ -35,0 +40,1 @@\n+#include \"oops\/layoutKind.hpp\"\n@@ -43,0 +49,1 @@\n+#include \"opto\/graphKit.hpp\"\n@@ -44,0 +51,1 @@\n+#include \"opto\/inlinetypenode.hpp\"\n@@ -49,0 +57,1 @@\n+#include \"opto\/opcodes.hpp\"\n@@ -53,0 +62,1 @@\n+#include \"opto\/type.hpp\"\n@@ -61,0 +71,1 @@\n+#include \"utilities\/globalDefinitions.hpp\"\n@@ -321,0 +332,2 @@\n+  case vmIntrinsics::_makePrivateBuffer:        return inline_unsafe_make_private_buffer();\n+  case vmIntrinsics::_finishPrivateBuffer:      return inline_unsafe_finish_private_buffer();\n@@ -330,0 +343,1 @@\n+  case vmIntrinsics::_getValue:                 return inline_unsafe_access(!is_store, T_OBJECT,   Relaxed, false, true);\n@@ -340,0 +354,1 @@\n+  case vmIntrinsics::_putValue:                 return inline_unsafe_access( is_store, T_OBJECT,   Relaxed, false, true);\n@@ -411,0 +426,3 @@\n+  case vmIntrinsics::_getFlatValue:             return inline_unsafe_flat_access(!is_store, Relaxed);\n+  case vmIntrinsics::_putFlatValue:             return inline_unsafe_flat_access( is_store, Relaxed);\n+\n@@ -470,0 +488,4 @@\n+  case vmIntrinsics::_arrayInstanceBaseOffset:  return inline_arrayInstanceBaseOffset();\n+  case vmIntrinsics::_arrayInstanceIndexScale:  return inline_arrayInstanceIndexScale();\n+  case vmIntrinsics::_arrayLayout:              return inline_arrayLayout();\n+\n@@ -518,0 +540,6 @@\n+  case vmIntrinsics::_newNullRestrictedNonAtomicArray: return inline_newArray(\/* null_free *\/ true, \/* atomic *\/ false);\n+  case vmIntrinsics::_newNullRestrictedAtomicArray: return inline_newArray(\/* null_free *\/ true, \/* atomic *\/ true);\n+  case vmIntrinsics::_newNullableAtomicArray:     return inline_newArray(\/* null_free *\/ false, \/* atomic *\/ true);\n+  case vmIntrinsics::_isFlatArray:              return inline_getArrayProperties(IsFlat);\n+  case vmIntrinsics::_isNullRestrictedArray:    return inline_getArrayProperties(IsNullRestricted);\n+  case vmIntrinsics::_isAtomicArray:            return inline_getArrayProperties(IsAtomic);\n@@ -2335,0 +2363,1 @@\n+  bool null_free = false;\n@@ -2340,0 +2369,1 @@\n+      null_free = alias_type->field()->is_null_free();\n@@ -2346,1 +2376,1 @@\n-    if (adr_type->offset() >= objArrayOopDesc::base_offset_in_bytes()) {\n+    if (adr_type->offset() >= refArrayOopDesc::base_offset_in_bytes()) {\n@@ -2348,0 +2378,1 @@\n+      null_free = adr_type->is_aryptr()->is_null_free();\n@@ -2360,0 +2391,3 @@\n+    if (null_free) {\n+      result = result->join_speculative(TypePtr::NOTNULL)->is_oopptr();\n+    }\n@@ -2431,1 +2465,1 @@\n-bool LibraryCallKit::inline_unsafe_access(bool is_store, const BasicType type, const AccessKind kind, const bool unaligned) {\n+bool LibraryCallKit::inline_unsafe_access(bool is_store, const BasicType type, const AccessKind kind, const bool unaligned, const bool is_flat) {\n@@ -2456,1 +2490,1 @@\n-      assert(sig->count() == 2, \"oop getter has 2 arguments\");\n+      assert(sig->count() == 2 || (is_flat && sig->count() == 3), \"oop getter has 2 or 3 arguments\");\n@@ -2462,1 +2496,1 @@\n-      assert(sig->count() == 3, \"oop putter has 3 arguments\");\n+      assert(sig->count() == 3 || (is_flat && sig->count() == 4), \"oop putter has 3 arguments\");\n@@ -2488,0 +2522,49 @@\n+\n+  ciInlineKlass* inline_klass = nullptr;\n+  if (is_flat) {\n+    const TypeInstPtr* cls = _gvn.type(argument(4))->isa_instptr();\n+    if (cls == nullptr || cls->const_oop() == nullptr) {\n+      return false;\n+    }\n+    ciType* mirror_type = cls->const_oop()->as_instance()->java_mirror_type();\n+    if (!mirror_type->is_inlinetype()) {\n+      return false;\n+    }\n+    inline_klass = mirror_type->as_inline_klass();\n+  }\n+\n+  if (base->is_InlineType()) {\n+    assert(!is_store, \"InlineTypeNodes are non-larval value objects\");\n+    InlineTypeNode* vt = base->as_InlineType();\n+    if (offset->is_Con()) {\n+      long off = find_long_con(offset, 0);\n+      ciInlineKlass* vk = vt->type()->inline_klass();\n+      if ((long)(int)off != off || !vk->contains_field_offset(off)) {\n+        return false;\n+      }\n+\n+      ciField* field = vk->get_non_flat_field_by_offset(off);\n+      if (field != nullptr) {\n+        BasicType bt = type2field[field->type()->basic_type()];\n+        if (bt == T_ARRAY || bt == T_NARROWOOP) {\n+          bt = T_OBJECT;\n+        }\n+        if (bt == type && (!field->is_flat() || field->type() == inline_klass)) {\n+          Node* value = vt->field_value_by_offset(off, false);\n+          if (value->is_InlineType()) {\n+            value = value->as_InlineType()->adjust_scalarization_depth(this);\n+          }\n+          set_result(value);\n+          return true;\n+        }\n+      }\n+    }\n+    {\n+      \/\/ Re-execute the unsafe access if allocation triggers deoptimization.\n+      PreserveReexecuteState preexecs(this);\n+      jvms()->set_should_reexecute(true);\n+      vt = vt->buffer(this);\n+    }\n+    base = vt->get_oop();\n+  }\n+\n@@ -2498,1 +2581,1 @@\n-    if (type != T_OBJECT) {\n+    if (type != T_OBJECT && (inline_klass == nullptr || !inline_klass->has_object_fields())) {\n@@ -2514,1 +2597,1 @@\n-  Node* val = is_store ? argument(4) : nullptr;\n+  Node* val = is_store ? argument(4 + (is_flat ? 1 : 0)) : nullptr;\n@@ -2531,1 +2614,29 @@\n-  BasicType bt = alias_type->basic_type();\n+  BasicType bt = T_ILLEGAL;\n+  ciField* field = nullptr;\n+  if (adr_type->isa_instptr()) {\n+    const TypeInstPtr* instptr = adr_type->is_instptr();\n+    ciInstanceKlass* k = instptr->instance_klass();\n+    int off = instptr->offset();\n+    if (instptr->const_oop() != nullptr &&\n+        k == ciEnv::current()->Class_klass() &&\n+        instptr->offset() >= (k->size_helper() * wordSize)) {\n+      k = instptr->const_oop()->as_instance()->java_lang_Class_klass()->as_instance_klass();\n+      field = k->get_field_by_offset(off, true);\n+    } else {\n+      field = k->get_non_flat_field_by_offset(off);\n+    }\n+    if (field != nullptr) {\n+      bt = type2field[field->type()->basic_type()];\n+    }\n+    if (bt != alias_type->basic_type()) {\n+      \/\/ Type mismatch. Is it an access to a nested flat field?\n+      field = k->get_field_by_offset(off, false);\n+      if (field != nullptr) {\n+        bt = type2field[field->type()->basic_type()];\n+      }\n+    }\n+    assert(bt == alias_type->basic_type() || is_flat, \"should match\");\n+  } else {\n+    bt = alias_type->basic_type();\n+  }\n+\n@@ -2552,0 +2663,21 @@\n+  if (is_flat) {\n+    if (adr_type->isa_instptr()) {\n+      if (field == nullptr || field->type() != inline_klass) {\n+        mismatched = true;\n+      }\n+    } else if (adr_type->isa_aryptr()) {\n+      const Type* elem = adr_type->is_aryptr()->elem();\n+      if (!adr_type->is_flat() || elem->inline_klass() != inline_klass) {\n+        mismatched = true;\n+      }\n+    } else {\n+      mismatched = true;\n+    }\n+    if (is_store) {\n+      const Type* val_t = _gvn.type(val);\n+      if (!val_t->is_inlinetypeptr() || val_t->inline_klass() != inline_klass) {\n+        return false;\n+      }\n+    }\n+  }\n+\n@@ -2565,4 +2697,6 @@\n-  if (!is_store && type == T_OBJECT) {\n-    const TypeOopPtr* tjp = sharpen_unsafe_type(alias_type, adr_type);\n-    if (tjp != nullptr) {\n-      value_type = tjp;\n+  if (!is_store) {\n+    if (type == T_OBJECT && !is_flat) {\n+      const TypeOopPtr* tjp = sharpen_unsafe_type(alias_type, adr_type);\n+      if (tjp != nullptr) {\n+        value_type = tjp;\n+      }\n@@ -2584,2 +2718,2 @@\n-    ciField* field = alias_type->field();\n-    if (heap_base_oop != top() && field != nullptr && field->is_constant() && !mismatched) {\n+\n+    if (heap_base_oop != top() && field != nullptr && field->is_constant() && !field->is_flat() && !mismatched) {\n@@ -2591,1 +2725,10 @@\n-      p = access_load_at(heap_base_oop, adr, adr_type, value_type, type, decorators);\n+      if (is_flat) {\n+        p = InlineTypeNode::make_from_flat(this, inline_klass, base, adr, adr_type, false, false, true);\n+      } else {\n+        p = access_load_at(heap_base_oop, adr, adr_type, value_type, type, decorators);\n+        const TypeOopPtr* ptr = value_type->make_oopptr();\n+        if (ptr != nullptr && ptr->is_inlinetypeptr()) {\n+          \/\/ Load a non-flattened inline type from memory\n+          p = InlineTypeNode::make_from_oop(this, p, ptr->inline_klass());\n+        }\n+      }\n@@ -2629,1 +2772,5 @@\n-    access_store_at(heap_base_oop, adr, adr_type, val, value_type, type, decorators);\n+    if (is_flat) {\n+      val->as_InlineType()->store_flat(this, base, adr, false, false, true, decorators);\n+    } else {\n+      access_store_at(heap_base_oop, adr, adr_type, val, value_type, type, decorators);\n+    }\n@@ -2635,0 +2782,235 @@\n+bool LibraryCallKit::inline_unsafe_flat_access(bool is_store, AccessKind kind) {\n+#ifdef ASSERT\n+  {\n+    ResourceMark rm;\n+    \/\/ Check the signatures.\n+    ciSignature* sig = callee()->signature();\n+    assert(sig->type_at(0)->basic_type() == T_OBJECT, \"base should be object, but is %s\", type2name(sig->type_at(0)->basic_type()));\n+    assert(sig->type_at(1)->basic_type() == T_LONG, \"offset should be long, but is %s\", type2name(sig->type_at(1)->basic_type()));\n+    assert(sig->type_at(2)->basic_type() == T_INT, \"layout kind should be int, but is %s\", type2name(sig->type_at(3)->basic_type()));\n+    assert(sig->type_at(3)->basic_type() == T_OBJECT, \"value klass should be object, but is %s\", type2name(sig->type_at(4)->basic_type()));\n+    if (is_store) {\n+      assert(sig->return_type()->basic_type() == T_VOID, \"putter must not return a value, but returns %s\", type2name(sig->return_type()->basic_type()));\n+      assert(sig->count() == 5, \"flat putter should have 5 arguments, but has %d\", sig->count());\n+      assert(sig->type_at(4)->basic_type() == T_OBJECT, \"put value should be object, but is %s\", type2name(sig->type_at(5)->basic_type()));\n+    } else {\n+      assert(sig->return_type()->basic_type() == T_OBJECT, \"getter must return an object, but returns %s\", type2name(sig->return_type()->basic_type()));\n+      assert(sig->count() == 4, \"flat getter should have 4 arguments, but has %d\", sig->count());\n+    }\n+ }\n+#endif \/\/ ASSERT\n+\n+  assert(kind == Relaxed, \"Only plain accesses for now\");\n+  if (callee()->is_static()) {\n+    \/\/ caller must have the capability!\n+    return false;\n+  }\n+  C->set_has_unsafe_access(true);\n+\n+  const TypeInstPtr* value_klass_node = _gvn.type(argument(5))->isa_instptr();\n+  if (value_klass_node == nullptr || value_klass_node->const_oop() == nullptr) {\n+    \/\/ parameter valueType is not a constant\n+    return false;\n+  }\n+  ciType* mirror_type = value_klass_node->const_oop()->as_instance()->java_mirror_type();\n+  if (!mirror_type->is_inlinetype()) {\n+    \/\/ Dead code\n+    return false;\n+  }\n+  ciInlineKlass* value_klass = mirror_type->as_inline_klass();\n+\n+  const TypeInt* layout_type = _gvn.type(argument(4))->isa_int();\n+  if (layout_type == nullptr || !layout_type->is_con()) {\n+    \/\/ parameter layoutKind is not a constant\n+    return false;\n+  }\n+  assert(layout_type->get_con() >= static_cast<int>(LayoutKind::REFERENCE) &&\n+         layout_type->get_con() <= static_cast<int>(LayoutKind::UNKNOWN),\n+         \"invalid layoutKind %d\", layout_type->get_con());\n+  LayoutKind layout = static_cast<LayoutKind>(layout_type->get_con());\n+  assert(layout == LayoutKind::REFERENCE || layout == LayoutKind::NON_ATOMIC_FLAT ||\n+         layout == LayoutKind::ATOMIC_FLAT || layout == LayoutKind::NULLABLE_ATOMIC_FLAT,\n+         \"unexpected layoutKind %d\", layout_type->get_con());\n+\n+  null_check(argument(0));\n+  if (stopped()) {\n+    return true;\n+  }\n+\n+  Node* base = must_be_not_null(argument(1), true);\n+  Node* offset = argument(2);\n+  const Type* base_type = _gvn.type(base);\n+\n+  Node* ptr;\n+  bool immutable_memory = false;\n+  DecoratorSet decorators = C2_UNSAFE_ACCESS | IN_HEAP | MO_UNORDERED;\n+  if (base_type->isa_instptr()) {\n+    const TypeLong* offset_type = _gvn.type(offset)->isa_long();\n+    if (offset_type == nullptr || !offset_type->is_con()) {\n+      \/\/ Offset into a non-array should be a constant\n+      decorators |= C2_MISMATCHED;\n+    } else {\n+      int offset_con = checked_cast<int>(offset_type->get_con());\n+      ciInstanceKlass* base_klass = base_type->is_instptr()->instance_klass();\n+      ciField* field = base_klass->get_non_flat_field_by_offset(offset_con);\n+      if (field == nullptr) {\n+        assert(!base_klass->is_final(), \"non-existence field at offset %d of class %s\", offset_con, base_klass->name()->as_utf8());\n+        decorators |= C2_MISMATCHED;\n+      } else {\n+        assert(field->type() == value_klass, \"field at offset %d of %s is of type %s, but valueType is %s\",\n+               offset_con, base_klass->name()->as_utf8(), field->type()->name(), value_klass->name()->as_utf8());\n+        immutable_memory = field->is_strict() && field->is_final();\n+\n+        if (base->is_InlineType()) {\n+          assert(!is_store, \"Cannot store into a non-larval value object\");\n+          set_result(base->as_InlineType()->field_value_by_offset(offset_con, false));\n+          return true;\n+        }\n+      }\n+    }\n+\n+    if (base->is_InlineType()) {\n+      assert(!is_store, \"Cannot store into a non-larval value object\");\n+      base = base->as_InlineType()->buffer(this, true);\n+    }\n+    ptr = basic_plus_adr(base, ConvL2X(offset));\n+  } else if (base_type->isa_aryptr()) {\n+    decorators |= IS_ARRAY;\n+    if (layout == LayoutKind::REFERENCE) {\n+      if (!base_type->is_aryptr()->is_not_flat()) {\n+        const TypeAryPtr* array_type = base_type->is_aryptr()->cast_to_not_flat();\n+        Node* new_base = _gvn.transform(new CastPPNode(control(), base, array_type, ConstraintCastNode::StrongDependency));\n+        replace_in_map(base, new_base);\n+        base = new_base;\n+      }\n+      ptr = basic_plus_adr(base, ConvL2X(offset));\n+    } else {\n+      if (UseArrayFlattening) {\n+        \/\/ Flat array must have an exact type\n+        bool is_null_free = layout != LayoutKind::NULLABLE_ATOMIC_FLAT;\n+        bool is_atomic = layout != LayoutKind::NON_ATOMIC_FLAT;\n+        Node* new_base = cast_to_flat_array(base, value_klass, is_null_free, !is_null_free, is_atomic);\n+        replace_in_map(base, new_base);\n+        base = new_base;\n+        ptr = basic_plus_adr(base, ConvL2X(offset));\n+        const TypeAryPtr* ptr_type = _gvn.type(ptr)->is_aryptr();\n+        if (ptr_type->field_offset().get() != 0) {\n+          ptr = _gvn.transform(new CastPPNode(control(), ptr, ptr_type->with_field_offset(0), ConstraintCastNode::StrongDependency));\n+        }\n+      } else {\n+        uncommon_trap(Deoptimization::Reason_intrinsic,\n+                      Deoptimization::Action_none);\n+        return true;\n+      }\n+    }\n+  } else {\n+    decorators |= C2_MISMATCHED;\n+    ptr = basic_plus_adr(base, ConvL2X(offset));\n+  }\n+\n+  if (is_store) {\n+    Node* value = argument(6);\n+    const Type* value_type = _gvn.type(value);\n+    if (!value_type->is_inlinetypeptr()) {\n+      value_type = Type::get_const_type(value_klass)->filter_speculative(value_type);\n+      Node* new_value = _gvn.transform(new CastPPNode(control(), value, value_type, ConstraintCastNode::StrongDependency));\n+      new_value = InlineTypeNode::make_from_oop(this, new_value, value_klass);\n+      replace_in_map(value, new_value);\n+      value = new_value;\n+    }\n+\n+    assert(value_type->inline_klass() == value_klass, \"value is of type %s while valueType is %s\", value_type->inline_klass()->name()->as_utf8(), value_klass->name()->as_utf8());\n+    if (layout == LayoutKind::REFERENCE) {\n+      const TypePtr* ptr_type = (decorators & C2_MISMATCHED) != 0 ? TypeRawPtr::BOTTOM : _gvn.type(ptr)->is_ptr();\n+      access_store_at(base, ptr, ptr_type, value, value_type, T_OBJECT, decorators);\n+    } else {\n+      bool atomic = layout != LayoutKind::NON_ATOMIC_FLAT;\n+      bool null_free = layout != LayoutKind::NULLABLE_ATOMIC_FLAT;\n+      value->as_InlineType()->store_flat(this, base, ptr, atomic, immutable_memory, null_free, decorators);\n+    }\n+\n+    return true;\n+  } else {\n+    decorators |= (C2_CONTROL_DEPENDENT_LOAD | C2_UNKNOWN_CONTROL_LOAD);\n+    InlineTypeNode* result;\n+    if (layout == LayoutKind::REFERENCE) {\n+      const TypePtr* ptr_type = (decorators & C2_MISMATCHED) != 0 ? TypeRawPtr::BOTTOM : _gvn.type(ptr)->is_ptr();\n+      Node* oop = access_load_at(base, ptr, ptr_type, Type::get_const_type(value_klass), T_OBJECT, decorators);\n+      result = InlineTypeNode::make_from_oop(this, oop, value_klass);\n+    } else {\n+      bool atomic = layout != LayoutKind::NON_ATOMIC_FLAT;\n+      bool null_free = layout != LayoutKind::NULLABLE_ATOMIC_FLAT;\n+      result = InlineTypeNode::make_from_flat(this, value_klass, base, ptr, atomic, immutable_memory, null_free, decorators);\n+    }\n+\n+    set_result(result);\n+    return true;\n+  }\n+}\n+\n+bool LibraryCallKit::inline_unsafe_make_private_buffer() {\n+  Node* receiver = argument(0);\n+  Node* value = argument(1);\n+\n+  const Type* type = gvn().type(value);\n+  if (!type->is_inlinetypeptr()) {\n+    C->record_method_not_compilable(\"value passed to Unsafe::makePrivateBuffer is not of a constant value type\");\n+    return false;\n+  }\n+\n+  null_check(receiver);\n+  if (stopped()) {\n+    return true;\n+  }\n+\n+  value = null_check(value);\n+  if (stopped()) {\n+    return true;\n+  }\n+\n+  ciInlineKlass* vk = type->inline_klass();\n+  Node* klass = makecon(TypeKlassPtr::make(vk));\n+  Node* obj = new_instance(klass);\n+  AllocateNode::Ideal_allocation(obj)->_larval = true;\n+\n+  assert(value->is_InlineType(), \"must be an InlineTypeNode\");\n+  Node* payload_ptr = basic_plus_adr(obj, vk->payload_offset());\n+  value->as_InlineType()->store_flat(this, obj, payload_ptr, false, true, true, IN_HEAP | MO_UNORDERED);\n+\n+  set_result(obj);\n+  return true;\n+}\n+\n+bool LibraryCallKit::inline_unsafe_finish_private_buffer() {\n+  Node* receiver = argument(0);\n+  Node* buffer = argument(1);\n+\n+  const Type* type = gvn().type(buffer);\n+  if (!type->is_inlinetypeptr()) {\n+    C->record_method_not_compilable(\"value passed to Unsafe::finishPrivateBuffer is not of a constant value type\");\n+    return false;\n+  }\n+\n+  AllocateNode* alloc = AllocateNode::Ideal_allocation(buffer);\n+  if (alloc == nullptr) {\n+    C->record_method_not_compilable(\"value passed to Unsafe::finishPrivateBuffer must be allocated by Unsafe::makePrivateBuffer\");\n+    return false;\n+  }\n+\n+  null_check(receiver);\n+  if (stopped()) {\n+    return true;\n+  }\n+\n+  \/\/ Unset the larval bit in the object header\n+  Node* old_header = make_load(control(), buffer, TypeX_X, TypeX_X->basic_type(), MemNode::unordered, LoadNode::Pinned);\n+  Node* new_header = gvn().transform(new AndXNode(old_header, MakeConX(~markWord::larval_bit_in_place)));\n+  access_store_at(buffer, buffer, type->is_ptr(), new_header, TypeX_X, TypeX_X->basic_type(), MO_UNORDERED | IN_HEAP);\n+\n+  \/\/ We must ensure that the buffer is properly published\n+  insert_mem_bar(Op_MemBarStoreStore, alloc->proj_out(AllocateNode::RawAddress));\n+  assert(!type->maybe_null(), \"result of an allocation should not be null\");\n+  set_result(InlineTypeNode::make_from_oop(this, buffer, type->inline_klass()));\n+  return true;\n+}\n+\n@@ -2837,0 +3219,13 @@\n+    if (oldval != nullptr && oldval->is_InlineType()) {\n+      \/\/ Re-execute the unsafe access if allocation triggers deoptimization.\n+      PreserveReexecuteState preexecs(this);\n+      jvms()->set_should_reexecute(true);\n+      oldval = oldval->as_InlineType()->buffer(this)->get_oop();\n+    }\n+    if (newval != nullptr && newval->is_InlineType()) {\n+      \/\/ Re-execute the unsafe access if allocation triggers deoptimization.\n+      PreserveReexecuteState preexecs(this);\n+      jvms()->set_should_reexecute(true);\n+      newval = newval->as_InlineType()->buffer(this)->get_oop();\n+    }\n+\n@@ -2905,0 +3300,72 @@\n+\/\/ private native int arrayInstanceBaseOffset0(Object[] array);\n+bool LibraryCallKit::inline_arrayInstanceBaseOffset() {\n+  Node* array = argument(1);\n+  Node* klass_node = load_object_klass(array);\n+\n+  jint  layout_con = Klass::_lh_neutral_value;\n+  Node* layout_val = get_layout_helper(klass_node, layout_con);\n+  int   layout_is_con = (layout_val == nullptr);\n+\n+  Node* header_size = nullptr;\n+  if (layout_is_con) {\n+    int hsize = Klass::layout_helper_header_size(layout_con);\n+    header_size = intcon(hsize);\n+  } else {\n+    Node* hss = intcon(Klass::_lh_header_size_shift);\n+    Node* hsm = intcon(Klass::_lh_header_size_mask);\n+    header_size = _gvn.transform(new URShiftINode(layout_val, hss));\n+    header_size = _gvn.transform(new AndINode(header_size, hsm));\n+  }\n+  set_result(header_size);\n+  return true;\n+}\n+\n+\/\/ private native int arrayInstanceIndexScale0(Object[] array);\n+bool LibraryCallKit::inline_arrayInstanceIndexScale() {\n+  Node* array = argument(1);\n+  Node* klass_node = load_object_klass(array);\n+\n+  jint  layout_con = Klass::_lh_neutral_value;\n+  Node* layout_val = get_layout_helper(klass_node, layout_con);\n+  int   layout_is_con = (layout_val == nullptr);\n+\n+  Node* element_size = nullptr;\n+  if (layout_is_con) {\n+    int log_element_size  = Klass::layout_helper_log2_element_size(layout_con);\n+    int elem_size = 1 << log_element_size;\n+    element_size = intcon(elem_size);\n+  } else {\n+    Node* ess = intcon(Klass::_lh_log2_element_size_shift);\n+    Node* esm = intcon(Klass::_lh_log2_element_size_mask);\n+    Node* log_element_size = _gvn.transform(new URShiftINode(layout_val, ess));\n+    log_element_size = _gvn.transform(new AndINode(log_element_size, esm));\n+    element_size = _gvn.transform(new LShiftINode(intcon(1), log_element_size));\n+  }\n+  set_result(element_size);\n+  return true;\n+}\n+\n+\/\/ private native int arrayLayout0(Object[] array);\n+bool LibraryCallKit::inline_arrayLayout() {\n+  RegionNode* region = new RegionNode(2);\n+  Node* phi = new PhiNode(region, TypeInt::POS);\n+\n+  Node* array = argument(1);\n+  Node* klass_node = load_object_klass(array);\n+  generate_refArray_guard(klass_node, region);\n+  if (region->req() == 3) {\n+    phi->add_req(intcon((jint)LayoutKind::REFERENCE));\n+  }\n+\n+  int layout_kind_offset = in_bytes(FlatArrayKlass::layout_kind_offset());\n+  Node* layout_kind_addr = basic_plus_adr(klass_node, klass_node, layout_kind_offset);\n+  Node* layout_kind = make_load(nullptr, layout_kind_addr, TypeInt::POS, T_INT, MemNode::unordered);\n+\n+  region->init_req(1, control());\n+  phi->init_req(1, layout_kind);\n+\n+  set_control(_gvn.transform(region));\n+  set_result(_gvn.transform(phi));\n+  return true;\n+}\n+\n@@ -3023,2 +3490,7 @@\n-\n-  Node* obj = new_instance(kls, test);\n+  Node* obj = nullptr;\n+  const TypeInstKlassPtr* tkls = _gvn.type(kls)->isa_instklassptr();\n+  if (tkls != nullptr && tkls->instance_klass()->is_inlinetype()) {\n+    obj = InlineTypeNode::make_all_zero(_gvn, tkls->instance_klass()->as_inline_klass())->buffer(this);\n+  } else {\n+    obj = new_instance(kls, test);\n+  }\n@@ -3805,1 +4277,1 @@\n-  const TypeAry* arr0 = TypeAry::make(etype, TypeInt::POS);\n+  const TypeAry* arr0 = TypeAry::make(etype, TypeInt::POS, \/* stable= *\/ false, \/* flat= *\/ false, \/* not_flat= *\/ true, \/* not_null_free= *\/ true);\n@@ -3810,1 +4282,1 @@\n-  const Type* objects_type = TypeAryPtr::make(TypePtr::BotPTR, arr0, objects_klass, xk, 0);\n+  const Type* objects_type = TypeAryPtr::make(TypePtr::BotPTR, arr0, objects_klass, xk, TypeAryPtr::Offset(0));\n@@ -3934,9 +4406,0 @@\n-\/\/---------------------------load_mirror_from_klass----------------------------\n-\/\/ Given a klass oop, load its java mirror (a java.lang.Class oop).\n-Node* LibraryCallKit::load_mirror_from_klass(Node* klass) {\n-  Node* p = basic_plus_adr(klass, in_bytes(Klass::java_mirror_offset()));\n-  Node* load = make_load(nullptr, p, TypeRawPtr::NOTNULL, T_ADDRESS, MemNode::unordered);\n-  \/\/ mirror = ((OopHandle)mirror)->resolve();\n-  return access_load(load, TypeInstPtr::MIRROR, T_OBJECT, IN_NATIVE);\n-}\n-\n@@ -3986,0 +4449,1 @@\n+\n@@ -4107,10 +4571,12 @@\n-    p = basic_plus_adr(kls, in_bytes(Klass::super_offset()));\n-    kls = _gvn.transform(LoadKlassNode::make(_gvn, immutable_memory(), p, TypeRawPtr::BOTTOM, TypeInstKlassPtr::OBJECT_OR_NULL));\n-    null_ctl = top();\n-    kls = null_check_oop(kls, &null_ctl);\n-    if (null_ctl != top()) {\n-      \/\/ If the guard is taken, Object.superClass is null (both klass and mirror).\n-      region->add_req(null_ctl);\n-      phi   ->add_req(null());\n-    }\n-      query_value = load_mirror_from_klass(kls);\n+      p = basic_plus_adr(kls, in_bytes(Klass::super_offset()));\n+      kls = _gvn.transform(LoadKlassNode::make(_gvn, immutable_memory(), p, TypeRawPtr::BOTTOM, TypeInstKlassPtr::OBJECT_OR_NULL));\n+      null_ctl = top();\n+      kls = null_check_oop(kls, &null_ctl);\n+      if (null_ctl != top()) {\n+        \/\/ If the guard is taken, Object.superClass is null (both klass and mirror).\n+        region->add_req(null_ctl);\n+        phi   ->add_req(null());\n+      }\n+      if (!stopped()) {\n+        query_value = load_mirror_from_klass(kls);\n+      }\n@@ -4135,0 +4601,1 @@\n+\n@@ -4157,1 +4624,2 @@\n-      int static_res = C->static_subtype_check(TypeKlassPtr::make(tm->as_klass(), Type::trust_interfaces), tp->as_klass_type());\n+      const TypeKlassPtr* tklass = TypeKlassPtr::make(tm->as_klass(), Type::trust_interfaces);\n+      int static_res = C->static_subtype_check(tklass, tp->as_klass_type());\n@@ -4186,2 +4654,2 @@\n-  \/\/ Not-subtype or the mirror's klass ptr is null (in case it is a primitive).\n-  enum { _bad_type_path = 1, _prim_path = 2, PATH_LIMIT };\n+  \/\/ Not-subtype or the mirror's klass ptr is nullptr (in case it is a primitive).\n+  enum { _bad_type_path = 1, _prim_path = 2, _npe_path = 3, PATH_LIMIT };\n@@ -4197,0 +4665,2 @@\n+  Node* io = i_o();\n+  Node* mem = merged_memory();\n@@ -4198,0 +4668,1 @@\n+\n@@ -4204,1 +4675,2 @@\n-      region->in(_bad_type_path) != top()) {\n+      region->in(_bad_type_path) != top() ||\n+      region->in(_npe_path) != top()) {\n@@ -4208,0 +4680,3 @@\n+    \/\/ Set IO and memory because gen_checkcast may override them when buffering inline types\n+    set_i_o(io);\n+    set_all_memory(mem);\n@@ -4241,0 +4716,1 @@\n+  RegionNode* prim_region = new RegionNode(2);\n@@ -4243,0 +4719,1 @@\n+  record_for_igvn(prim_region);\n@@ -4267,2 +4744,5 @@\n-    int prim_path = (which_arg == 0 ? _prim_0_path : _prim_1_path);\n-    region->init_req(prim_path, null_ctl);\n+    if (which_arg == 0) {\n+      prim_region->init_req(1, null_ctl);\n+    } else {\n+      region->init_req(_prim_1_path, null_ctl);\n+    }\n@@ -4278,1 +4758,0 @@\n-    \/\/ now we have a successful reference subtype check\n@@ -4285,1 +4764,2 @@\n-  set_control(region->in(_prim_0_path)); \/\/ go back to first null check\n+  \/\/ This path is also used if superc is a value mirror.\n+  set_control(_gvn.transform(prim_region));\n@@ -4290,1 +4770,1 @@\n-    generate_guard(bol_eq, region, PROB_FAIR);\n+    generate_fair_guard(bol_eq, region);\n@@ -4321,2 +4801,1 @@\n-Node* LibraryCallKit::generate_array_guard_common(Node* kls, RegionNode* region,\n-                                                  bool obj_array, bool not_array, Node** obj) {\n+Node* LibraryCallKit::generate_array_guard_common(Node* kls, RegionNode* region, ArrayKind kind, Node** obj) {\n@@ -4328,9 +4807,0 @@\n-  \/\/ If obj_array\/non_array==false\/false:\n-  \/\/ Branch around if the given klass is in fact an array (either obj or prim).\n-  \/\/ If obj_array\/non_array==false\/true:\n-  \/\/ Branch around if the given klass is not an array klass of any kind.\n-  \/\/ If obj_array\/non_array==true\/true:\n-  \/\/ Branch around if the kls is not an oop array (kls is int[], String, etc.)\n-  \/\/ If obj_array\/non_array==true\/false:\n-  \/\/ Branch around if the kls is an oop array (Object[] or subtype)\n-  \/\/\n@@ -4341,4 +4811,11 @@\n-    bool query = (obj_array\n-                  ? Klass::layout_helper_is_objArray(layout_con)\n-                  : Klass::layout_helper_is_array(layout_con));\n-    if (query == not_array) {\n+    bool query = 0;\n+    switch(kind) {\n+      case RefArray:       query = Klass::layout_helper_is_refArray(layout_con); break;\n+      case NonRefArray:    query = !Klass::layout_helper_is_refArray(layout_con); break;\n+      case TypeArray:      query = Klass::layout_helper_is_typeArray(layout_con); break;\n+      case AnyArray:       query = Klass::layout_helper_is_array(layout_con); break;\n+      case NonArray:       query = !Klass::layout_helper_is_array(layout_con); break;\n+      default:\n+        ShouldNotReachHere();\n+    }\n+    if (!query) {\n@@ -4354,0 +4831,21 @@\n+  unsigned int value = 0;\n+  BoolTest::mask btest = BoolTest::illegal;\n+  switch(kind) {\n+    case RefArray:\n+    case NonRefArray: {\n+      value = Klass::_lh_array_tag_ref_value;\n+      layout_val = _gvn.transform(new RShiftINode(layout_val, intcon(Klass::_lh_array_tag_shift)));\n+      btest = (kind == RefArray) ? BoolTest::eq : BoolTest::ne;\n+      break;\n+    }\n+    case TypeArray: {\n+      value = Klass::_lh_array_tag_type_value;\n+      layout_val = _gvn.transform(new RShiftINode(layout_val, intcon(Klass::_lh_array_tag_shift)));\n+      btest = BoolTest::eq;\n+      break;\n+    }\n+    case AnyArray:    value = Klass::_lh_neutral_value; btest = BoolTest::lt; break;\n+    case NonArray:    value = Klass::_lh_neutral_value; btest = BoolTest::gt; break;\n+    default:\n+      ShouldNotReachHere();\n+  }\n@@ -4355,4 +4853,1 @@\n-  jint  nval = (obj_array\n-                ? (jint)(Klass::_lh_array_tag_type_value\n-                   <<    Klass::_lh_array_tag_shift)\n-                : Klass::_lh_neutral_value);\n+  jint nval = (jint)value;\n@@ -4360,3 +4855,0 @@\n-  BoolTest::mask btest = BoolTest::lt;  \/\/ correct for testing is_[obj]array\n-  \/\/ invert the test if we are looking for a non-array\n-  if (not_array)  btest = BoolTest(btest).negate();\n@@ -4365,1 +4857,1 @@\n-  Node* is_array_ctrl = not_array ? control() : ctrl;\n+  Node* is_array_ctrl = kind == NonArray ? control() : ctrl;\n@@ -4374,0 +4866,131 @@\n+\/\/ public static native Object[] ValueClass::newNullRestrictedAtomicArray(Class<?> componentType, int length, Object initVal);\n+\/\/ public static native Object[] ValueClass::newNullRestrictedNonAtomicArray(Class<?> componentType, int length, Object initVal);\n+\/\/ public static native Object[] ValueClass::newNullableAtomicArray(Class<?> componentType, int length);\n+bool LibraryCallKit::inline_newArray(bool null_free, bool atomic) {\n+  assert(null_free || atomic, \"nullable implies atomic\");\n+  Node* componentType = argument(0);\n+  Node* length = argument(1);\n+  Node* init_val = null_free ? argument(2) : nullptr;\n+\n+  const TypeInstPtr* tp = _gvn.type(componentType)->isa_instptr();\n+  if (tp != nullptr) {\n+    ciInstanceKlass* ik = tp->instance_klass();\n+    if (ik == C->env()->Class_klass()) {\n+      ciType* t = tp->java_mirror_type();\n+      if (t != nullptr && t->is_inlinetype()) {\n+\n+        ciArrayKlass* array_klass = ciArrayKlass::make(t, null_free, atomic, true);\n+        assert(array_klass->is_elem_null_free() == null_free, \"inconsistency\");\n+        assert(array_klass->is_elem_atomic() == atomic, \"inconsistency\");\n+\n+        \/\/ TOOD 8350865 ZGC needs card marks on initializing oop stores\n+        if (UseZGC && null_free && !array_klass->is_flat_array_klass()) {\n+          return false;\n+        }\n+\n+        if (array_klass->is_loaded() && array_klass->element_klass()->as_inline_klass()->is_initialized()) {\n+          const TypeAryKlassPtr* array_klass_type = TypeAryKlassPtr::make(array_klass, Type::trust_interfaces, true);\n+          if (null_free) {\n+            if (init_val->is_InlineType()) {\n+              if (array_klass_type->is_flat() && init_val->as_InlineType()->is_all_zero(&gvn(), \/* flat *\/ true)) {\n+                \/\/ Zeroing is enough because the init value is the all-zero value\n+                init_val = nullptr;\n+              } else {\n+                init_val = init_val->as_InlineType()->buffer(this);\n+              }\n+            }\n+            \/\/ TODO 8350865 Should we add a check of the init_val type (maybe in debug only + halt)?\n+          }\n+          Node* obj = new_array(makecon(array_klass_type), length, 0, nullptr, false, init_val);\n+          const TypeAryPtr* arytype = gvn().type(obj)->is_aryptr();\n+          assert(arytype->is_null_free() == null_free, \"inconsistency\");\n+          assert(arytype->is_not_null_free() == !null_free, \"inconsistency\");\n+          assert(arytype->is_atomic() == atomic, \"inconsistency\");\n+          set_result(obj);\n+          return true;\n+        }\n+      }\n+    }\n+  }\n+  return false;\n+}\n+\n+\/\/ public static native boolean ValueClass::isFlatArray(Object array);\n+\/\/ public static native boolean ValueClass::isNullRestrictedArray(Object array);\n+\/\/ public static native boolean ValueClass::isAtomicArray(Object array);\n+bool LibraryCallKit::inline_getArrayProperties(ArrayPropertiesCheck check) {\n+  Node* array = argument(0);\n+\n+  Node* bol;\n+  switch(check) {\n+    case IsFlat:\n+      \/\/ TODO 8350865 Use the object version here instead of loading the klass\n+      \/\/ The problem is that PhaseMacroExpand::expand_flatarraycheck_node can only handle some IR shapes and will fail, for example, if the bol is directly wired to a ReturnNode\n+      bol = flat_array_test(load_object_klass(array));\n+      break;\n+    case IsNullRestricted:\n+      bol = null_free_array_test(array);\n+      break;\n+    case IsAtomic:\n+      \/\/ TODO 8350865 Implement this. It's a bit more complicated, see conditions in JVM_IsAtomicArray\n+      \/\/ Enable TestIntrinsics::test87\/88 once this is implemented\n+      \/\/ bol = null_free_atomic_array_test\n+      return false;\n+    default:\n+      ShouldNotReachHere();\n+  }\n+\n+  Node* res = gvn().transform(new CMoveINode(bol, intcon(0), intcon(1), TypeInt::BOOL));\n+  set_result(res);\n+  return true;\n+}\n+\n+\/\/ Load the default refined array klass from an ObjArrayKlass. This relies on the first entry in the\n+\/\/ '_next_refined_array_klass' linked list being the default (see ObjArrayKlass::klass_with_properties).\n+Node* LibraryCallKit::load_default_refined_array_klass(Node* klass_node, bool type_array_guard) {\n+  RegionNode* region = new RegionNode(2);\n+  Node* phi = new PhiNode(region, TypeInstKlassPtr::OBJECT_OR_NULL);\n+\n+  if (type_array_guard) {\n+    generate_typeArray_guard(klass_node, region);\n+    if (region->req() == 3) {\n+      phi->add_req(klass_node);\n+    }\n+  }\n+  Node* adr_refined_klass = basic_plus_adr(klass_node, in_bytes(ObjArrayKlass::next_refined_array_klass_offset()));\n+  Node* refined_klass = _gvn.transform(LoadKlassNode::make(_gvn, immutable_memory(), adr_refined_klass, TypeRawPtr::BOTTOM, TypeInstKlassPtr::OBJECT_OR_NULL));\n+\n+  \/\/ Can be null if not initialized yet, just deopt\n+  Node* null_ctl = top();\n+  refined_klass = null_check_oop(refined_klass, &null_ctl, \/* never_see_null= *\/ true);\n+\n+  region->init_req(1, control());\n+  phi->init_req(1, refined_klass);\n+\n+  set_control(_gvn.transform(region));\n+  return _gvn.transform(phi);\n+}\n+\n+\/\/ Load the non-refined array klass from an ObjArrayKlass.\n+Node* LibraryCallKit::load_non_refined_array_klass(Node* klass_node) {\n+  const TypeAryKlassPtr* ary_klass_ptr = _gvn.type(klass_node)->isa_aryklassptr();\n+  if (ary_klass_ptr != nullptr && ary_klass_ptr->klass_is_exact()) {\n+    return _gvn.makecon(ary_klass_ptr->cast_to_refined_array_klass_ptr(false));\n+  }\n+\n+  RegionNode* region = new RegionNode(2);\n+  Node* phi = new PhiNode(region, TypeInstKlassPtr::OBJECT);\n+\n+  generate_typeArray_guard(klass_node, region);\n+  if (region->req() == 3) {\n+    phi->add_req(klass_node);\n+  }\n+  Node* super_adr = basic_plus_adr(klass_node, in_bytes(Klass::super_offset()));\n+  Node* super_klass = _gvn.transform(LoadKlassNode::make(_gvn, immutable_memory(), super_adr, TypeRawPtr::BOTTOM, TypeInstKlassPtr::OBJECT));\n+\n+  region->init_req(1, control());\n+  phi->init_req(1, super_klass);\n+\n+  set_control(_gvn.transform(region));\n+  return _gvn.transform(phi);\n+}\n@@ -4376,1 +4999,1 @@\n-\/\/ private static native Object java.lang.reflect.newArray(Class<?> componentType, int length);\n+\/\/ private static native Object java.lang.reflect.Array.newArray(Class<?> componentType, int length);\n@@ -4434,0 +5057,3 @@\n+\n+    klass_node = load_default_refined_array_klass(klass_node);\n+\n@@ -4522,1 +5148,16 @@\n-    Node* not_objArray = generate_non_objArray_guard(klass_node, bailout);\n+    \/\/ Inline type array may have object field that would require a\n+    \/\/ write barrier. Conservatively, go to slow path.\n+    \/\/ TODO 8251971: Optimize for the case when flat src\/dst are later found\n+    \/\/ to not contain oops (i.e., move this check to the macro expansion phase).\n+    BarrierSetC2* bs = BarrierSet::barrier_set()->barrier_set_c2();\n+    const TypeAryPtr* orig_t = _gvn.type(original)->isa_aryptr();\n+    const TypeKlassPtr* tklass = _gvn.type(klass_node)->is_klassptr();\n+    bool exclude_flat = UseArrayFlattening && bs->array_copy_requires_gc_barriers(true, T_OBJECT, false, false, BarrierSetC2::Parsing) &&\n+                        \/\/ Can src array be flat and contain oops?\n+                        (orig_t == nullptr || (!orig_t->is_not_flat() && (!orig_t->is_flat() || orig_t->elem()->inline_klass()->contains_oops()))) &&\n+                        \/\/ Can dest array be flat and contain oops?\n+                        tklass->can_be_inline_array() && (!tklass->is_flat() || tklass->is_aryklassptr()->elem()->is_instklassptr()->instance_klass()->as_inline_klass()->contains_oops());\n+    Node* not_objArray = exclude_flat ? generate_non_refArray_guard(klass_node, bailout) : generate_typeArray_guard(klass_node, bailout);\n+\n+    Node* refined_klass_node = load_default_refined_array_klass(klass_node, \/* type_array_guard= *\/ false);\n+\n@@ -4526,3 +5167,3 @@\n-      const Type* akls = TypeKlassPtr::make(TypePtr::NotNull, ak, 0\/*offset*\/);\n-      Node* cast = new CastPPNode(control(), klass_node, akls);\n-      klass_node = _gvn.transform(cast);\n+      const Type* akls = TypeKlassPtr::make(TypePtr::NotNull, ak, Type::Offset(0));\n+      Node* cast = new CastPPNode(control(), refined_klass_node, akls);\n+      refined_klass_node = _gvn.transform(cast);\n@@ -4546,0 +5187,39 @@\n+    \/\/ Handle inline type arrays\n+    bool can_validate = !too_many_traps(Deoptimization::Reason_class_check);\n+    if (!stopped()) {\n+      \/\/ TODO 8251971\n+      if (!orig_t->is_null_free()) {\n+        \/\/ Not statically known to be null free, add a check\n+        generate_fair_guard(null_free_array_test(original), bailout);\n+      }\n+      orig_t = _gvn.type(original)->isa_aryptr();\n+      if (orig_t != nullptr && orig_t->is_flat()) {\n+        \/\/ Src is flat, check that dest is flat as well\n+        if (exclude_flat) {\n+          \/\/ Dest can't be flat, bail out\n+          bailout->add_req(control());\n+          set_control(top());\n+        } else {\n+          generate_fair_guard(flat_array_test(refined_klass_node, \/* flat = *\/ false), bailout);\n+        }\n+        \/\/ TODO 8350865 This is not correct anymore. Write tests and fix logic similar to arraycopy.\n+      } else if (UseArrayFlattening && (orig_t == nullptr || !orig_t->is_not_flat()) &&\n+                 \/\/ If dest is flat, src must be flat as well (guaranteed by src <: dest check if validated).\n+                 ((!tklass->is_flat() && tklass->can_be_inline_array()) || !can_validate)) {\n+        \/\/ Src might be flat and dest might not be flat. Go to the slow path if src is flat.\n+        \/\/ TODO 8251971: Optimize for the case when src\/dest are later found to be both flat.\n+        generate_fair_guard(flat_array_test(load_object_klass(original)), bailout);\n+        if (orig_t != nullptr) {\n+          orig_t = orig_t->cast_to_not_flat();\n+          original = _gvn.transform(new CheckCastPPNode(control(), original, orig_t));\n+        }\n+      }\n+      if (!can_validate) {\n+        \/\/ No validation. The subtype check emitted at macro expansion time will not go to the slow\n+        \/\/ path but call checkcast_arraycopy which can not handle flat\/null-free inline type arrays.\n+        \/\/ TODO 8251971: Optimize for the case when src\/dest are later found to be both flat\/null-free.\n+        generate_fair_guard(flat_array_test(refined_klass_node), bailout);\n+        generate_fair_guard(null_free_array_test(original), bailout);\n+      }\n+    }\n+\n@@ -4591,1 +5271,1 @@\n-      if (!too_many_traps(Deoptimization::Reason_class_check)) {\n+      if (can_validate) {\n@@ -4605,1 +5285,1 @@\n-        newcopy = new_array(klass_node, length, 0);  \/\/ no arguments to push\n+        newcopy = new_array(refined_klass_node, length, 0);  \/\/ no arguments to push\n@@ -4677,1 +5357,1 @@\n-    const TypeTuple* range = tf->range();\n+    const TypeTuple* range = tf->range_cc();\n@@ -4681,1 +5361,1 @@\n-    tf = TypeFunc::make(tf->domain(), new_range);\n+    tf = TypeFunc::make(tf->domain_cc(), new_range);\n@@ -4738,1 +5418,8 @@\n-  Node* obj = nullptr;\n+  Node* obj = argument(0);\n+\n+  \/\/ Don't intrinsify hashcode on inline types for now.\n+  \/\/ The \"is locked\" runtime check also subsumes the inline type check (as inline types cannot be locked) and goes to the slow path.\n+  if (gvn().type(obj)->is_inlinetypeptr()) {\n+    return false;\n+  }\n+\n@@ -4748,1 +5435,0 @@\n-    obj = argument(0);\n@@ -4789,0 +5475,2 @@\n+    \/\/ We cannot use the inline type mask as this may check bits that are overriden\n+    \/\/ by an object monitor's pointer when inflating locking.\n@@ -4856,1 +5544,10 @@\n-  Node* obj = null_check_receiver();\n+  Node* obj = argument(0);\n+  if (obj->is_InlineType()) {\n+    const Type* t = _gvn.type(obj);\n+    if (t->maybe_null()) {\n+      null_check(obj);\n+    }\n+    set_result(makecon(TypeInstPtr::make(t->inline_klass()->java_mirror())));\n+    return true;\n+  }\n+  obj = null_check_receiver();\n@@ -5278,1 +5975,2 @@\n-    Node* obj = null_check_receiver();\n+    Node* obj = argument(0);\n+    obj = null_check_receiver();\n@@ -5282,0 +5980,6 @@\n+    if (obj_type->is_inlinetypeptr()) {\n+      \/\/ If the object to clone is an inline type, we can simply return it (i.e. a nop) since inline types have\n+      \/\/ no identity.\n+      set_result(obj);\n+      return true;\n+    }\n@@ -5288,1 +5992,2 @@\n-        obj_type->speculative_type()->is_instance_klass()) {\n+        obj_type->speculative_type()->is_instance_klass() &&\n+        !obj_type->speculative_type()->is_inlinetype()) {\n@@ -5318,0 +6023,5 @@\n+    \/\/ We only go to the fast case code if we pass a number of guards.\n+    \/\/ The paths which do not pass are accumulated in the slow_region.\n+    RegionNode* slow_region = new RegionNode(1);\n+    record_for_igvn(slow_region);\n+\n@@ -5324,3 +6034,0 @@\n-      Node* obj_length = load_array_length(array_obj);\n-      Node* array_size = nullptr; \/\/ Size of the array without object alignment padding.\n-      Node* alloc_obj = new_array(obj_klass, obj_length, 0, &array_size, \/*deoptimize_on_exception=*\/true);\n@@ -5329,20 +6036,7 @@\n-      if (bs->array_copy_requires_gc_barriers(true, T_OBJECT, true, false, BarrierSetC2::Parsing)) {\n-        \/\/ If it is an oop array, it requires very special treatment,\n-        \/\/ because gc barriers are required when accessing the array.\n-        Node* is_obja = generate_objArray_guard(obj_klass, (RegionNode*)nullptr);\n-        if (is_obja != nullptr) {\n-          PreserveJVMState pjvms2(this);\n-          set_control(is_obja);\n-          \/\/ Generate a direct call to the right arraycopy function(s).\n-          \/\/ Clones are always tightly coupled.\n-          ArrayCopyNode* ac = ArrayCopyNode::make(this, true, array_obj, intcon(0), alloc_obj, intcon(0), obj_length, true, false);\n-          ac->set_clone_oop_array();\n-          Node* n = _gvn.transform(ac);\n-          assert(n == ac, \"cannot disappear\");\n-          ac->connect_outputs(this, \/*deoptimize_on_exception=*\/true);\n-\n-          result_reg->init_req(_objArray_path, control());\n-          result_val->init_req(_objArray_path, alloc_obj);\n-          result_i_o ->set_req(_objArray_path, i_o());\n-          result_mem ->set_req(_objArray_path, reset_memory());\n-        }\n+      const TypeAryPtr* ary_ptr = obj_type->isa_aryptr();\n+      if (UseArrayFlattening && bs->array_copy_requires_gc_barriers(true, T_OBJECT, true, false, BarrierSetC2::Expansion) &&\n+          obj_type->can_be_inline_array() &&\n+          (ary_ptr == nullptr || (!ary_ptr->is_not_flat() && (!ary_ptr->is_flat() || ary_ptr->elem()->inline_klass()->contains_oops())))) {\n+        \/\/ Flat inline type array may have object field that would require a\n+        \/\/ write barrier. Conservatively, go to slow path.\n+        generate_fair_guard(flat_array_test(obj_klass), slow_region);\n@@ -5350,7 +6044,0 @@\n-      \/\/ Otherwise, there are no barriers to worry about.\n-      \/\/ (We can dispense with card marks if we know the allocation\n-      \/\/  comes out of eden (TLAB)...  In fact, ReduceInitialCardMarks\n-      \/\/  causes the non-eden paths to take compensating steps to\n-      \/\/  simulate a fresh allocation, so that no further\n-      \/\/  card marks are required in compiled code to initialize\n-      \/\/  the object.)\n@@ -5359,7 +6046,43 @@\n-        copy_to_clone(array_obj, alloc_obj, array_size, true);\n-\n-        \/\/ Present the results of the copy.\n-        result_reg->init_req(_array_path, control());\n-        result_val->init_req(_array_path, alloc_obj);\n-        result_i_o ->set_req(_array_path, i_o());\n-        result_mem ->set_req(_array_path, reset_memory());\n+        Node* obj_length = load_array_length(array_obj);\n+        Node* array_size = nullptr; \/\/ Size of the array without object alignment padding.\n+        Node* alloc_obj = new_array(obj_klass, obj_length, 0, &array_size, \/*deoptimize_on_exception=*\/true);\n+\n+        BarrierSetC2* bs = BarrierSet::barrier_set()->barrier_set_c2();\n+        if (bs->array_copy_requires_gc_barriers(true, T_OBJECT, true, false, BarrierSetC2::Parsing)) {\n+          \/\/ If it is an oop array, it requires very special treatment,\n+          \/\/ because gc barriers are required when accessing the array.\n+          Node* is_obja = generate_refArray_guard(obj_klass, (RegionNode*)nullptr);\n+          if (is_obja != nullptr) {\n+            PreserveJVMState pjvms2(this);\n+            set_control(is_obja);\n+            \/\/ Generate a direct call to the right arraycopy function(s).\n+            \/\/ Clones are always tightly coupled.\n+            ArrayCopyNode* ac = ArrayCopyNode::make(this, true, array_obj, intcon(0), alloc_obj, intcon(0), obj_length, true, false);\n+            ac->set_clone_oop_array();\n+            Node* n = _gvn.transform(ac);\n+            assert(n == ac, \"cannot disappear\");\n+            ac->connect_outputs(this, \/*deoptimize_on_exception=*\/true);\n+\n+            result_reg->init_req(_objArray_path, control());\n+            result_val->init_req(_objArray_path, alloc_obj);\n+            result_i_o ->set_req(_objArray_path, i_o());\n+            result_mem ->set_req(_objArray_path, reset_memory());\n+          }\n+        }\n+        \/\/ Otherwise, there are no barriers to worry about.\n+        \/\/ (We can dispense with card marks if we know the allocation\n+        \/\/  comes out of eden (TLAB)...  In fact, ReduceInitialCardMarks\n+        \/\/  causes the non-eden paths to take compensating steps to\n+        \/\/  simulate a fresh allocation, so that no further\n+        \/\/  card marks are required in compiled code to initialize\n+        \/\/  the object.)\n+\n+        if (!stopped()) {\n+          copy_to_clone(obj, alloc_obj, array_size, true);\n+\n+          \/\/ Present the results of the copy.\n+          result_reg->init_req(_array_path, control());\n+          result_val->init_req(_array_path, alloc_obj);\n+          result_i_o ->set_req(_array_path, i_o());\n+          result_mem ->set_req(_array_path, reset_memory());\n+        }\n@@ -5369,4 +6092,0 @@\n-    \/\/ We only go to the instance fast case code if we pass a number of guards.\n-    \/\/ The paths which do not pass are accumulated in the slow_region.\n-    RegionNode* slow_region = new RegionNode(1);\n-    record_for_igvn(slow_region);\n@@ -5504,0 +6223,12 @@\n+  int adjustment = 1;\n+  const TypeAryKlassPtr* ary_klass_ptr = alloc->in(AllocateNode::KlassNode)->bottom_type()->is_aryklassptr();\n+  if (ary_klass_ptr->is_null_free()) {\n+    \/\/ A null-free, tightly coupled array allocation can only come from LibraryCallKit::inline_newArray which\n+    \/\/ also requires the componentType and initVal on stack for re-execution.\n+    \/\/ Re-create and push the componentType.\n+    ciArrayKlass* klass = ary_klass_ptr->exact_klass()->as_array_klass();\n+    ciInstance* instance = klass->component_mirror_instance();\n+    const TypeInstPtr* t_instance = TypeInstPtr::make(instance);\n+    sfpt->ins_req(old_jvms->stkoff() + old_jvms->sp(), makecon(t_instance));\n+    adjustment++;\n+  }\n@@ -5505,5 +6236,16 @@\n-  sfpt->ins_req(old_jvms->stkoff() + old_jvms->sp(), alloc->in(AllocateNode::ALength));\n-  old_jvms->set_sp(old_jvms->sp()+1);\n-  old_jvms->set_monoff(old_jvms->monoff()+1);\n-  old_jvms->set_scloff(old_jvms->scloff()+1);\n-  old_jvms->set_endoff(old_jvms->endoff()+1);\n+  sfpt->ins_req(old_jvms->stkoff() + old_jvms->sp() + adjustment - 1, alloc->in(AllocateNode::ALength));\n+  if (ary_klass_ptr->is_null_free()) {\n+    \/\/ Re-create and push the initVal.\n+    Node* init_val = alloc->in(AllocateNode::InitValue);\n+    if (init_val == nullptr) {\n+      init_val = InlineTypeNode::make_all_zero(_gvn, ary_klass_ptr->elem()->is_instklassptr()->instance_klass()->as_inline_klass());\n+    } else if (UseCompressedOops) {\n+      init_val = _gvn.transform(new DecodeNNode(init_val, init_val->bottom_type()->make_ptr()));\n+    }\n+    sfpt->ins_req(old_jvms->stkoff() + old_jvms->sp() + adjustment, init_val);\n+    adjustment++;\n+  }\n+  old_jvms->set_sp(old_jvms->sp() + adjustment);\n+  old_jvms->set_monoff(old_jvms->monoff() + adjustment);\n+  old_jvms->set_scloff(old_jvms->scloff() + adjustment);\n+  old_jvms->set_endoff(old_jvms->endoff() + adjustment);\n@@ -5542,2 +6284,1 @@\n-    CallProjections callprojs;\n-    alloc->extract_projections(&callprojs, true);\n+    CallProjections* callprojs = alloc->extract_projections(true);\n@@ -5546,1 +6287,1 @@\n-    C->gvn_replace_by(callprojs.fallthrough_ioproj, alloc->in(TypeFunc::I_O));\n+    C->gvn_replace_by(callprojs->fallthrough_ioproj, alloc->in(TypeFunc::I_O));\n@@ -5588,1 +6329,1 @@\n-    set_i_o(callprojs.fallthrough_ioproj);\n+    set_i_o(callprojs->fallthrough_ioproj);\n@@ -5938,1 +6679,1 @@\n-    if (src_elem == dest_elem && src_elem == T_OBJECT) {\n+    if (src_elem == dest_elem && top_src->is_flat() == top_dest->is_flat() && src_elem == T_OBJECT) {\n@@ -5965,0 +6706,2 @@\n+          src_type = _gvn.type(src);\n+          top_src = src_type->isa_aryptr();\n@@ -5968,0 +6711,2 @@\n+          dest_type = _gvn.type(dest);\n+          top_dest = dest_type->isa_aryptr();\n@@ -5983,2 +6728,1 @@\n-      can_emit_guards &&\n-      !src->is_top() && !dest->is_top()) {\n+      can_emit_guards && !src->is_top() && !dest->is_top()) {\n@@ -6025,0 +6769,1 @@\n+    Node* refined_dest_klass = dest_klass;\n@@ -6026,0 +6771,1 @@\n+      dest_klass = load_non_refined_array_klass(refined_dest_klass);\n@@ -6027,8 +6773,1 @@\n-\n-      if (not_subtype_ctrl != top()) {\n-        PreserveJVMState pjvms(this);\n-        set_control(not_subtype_ctrl);\n-        uncommon_trap(Deoptimization::Reason_intrinsic,\n-                      Deoptimization::Action_make_not_entrant);\n-        assert(stopped(), \"Should be stopped\");\n-      }\n+      slow_region->add_req(not_subtype_ctrl);\n@@ -6036,0 +6775,21 @@\n+\n+    \/\/ TODO 8350865 Improve this. What about atomicity? Make sure this is always folded for type arrays.\n+    \/\/ If destination is null-restricted, source must be null-restricted as well: src_null_restricted || !dst_null_restricted\n+    Node* src_klass = load_object_klass(src);\n+    Node* adr_prop_src = basic_plus_adr(src_klass, in_bytes(ArrayKlass::properties_offset()));\n+    Node* prop_src = _gvn.transform(LoadNode::make(_gvn, control(), immutable_memory(), adr_prop_src, TypeRawPtr::BOTTOM, TypeInt::INT, T_INT, MemNode::unordered));\n+    Node* adr_prop_dest = basic_plus_adr(refined_dest_klass, in_bytes(ArrayKlass::properties_offset()));\n+    Node* prop_dest = _gvn.transform(LoadNode::make(_gvn, control(), immutable_memory(), adr_prop_dest, TypeRawPtr::BOTTOM, TypeInt::INT, T_INT, MemNode::unordered));\n+\n+    prop_dest = _gvn.transform(new XorINode(prop_dest, intcon(ArrayKlass::ArrayProperties::NULL_RESTRICTED)));\n+    prop_src = _gvn.transform(new OrINode(prop_dest, prop_src));\n+    prop_src = _gvn.transform(new AndINode(prop_src, intcon(ArrayKlass::ArrayProperties::NULL_RESTRICTED)));\n+\n+    Node* chk = _gvn.transform(new CmpINode(prop_src, intcon(ArrayKlass::ArrayProperties::NULL_RESTRICTED)));\n+    Node* tst = _gvn.transform(new BoolNode(chk, BoolTest::ne));\n+    generate_fair_guard(tst, slow_region);\n+\n+    \/\/ TODO 8350865 This is too strong\n+    generate_fair_guard(flat_array_test(src), slow_region);\n+    generate_fair_guard(flat_array_test(dest), slow_region);\n+\n@@ -6044,2 +6804,2 @@\n-    const TypeKlassPtr* dest_klass_t = _gvn.type(dest_klass)->is_klassptr();\n-    const Type *toop = dest_klass_t->cast_to_exactness(false)->as_instance_type();\n+    const TypeKlassPtr* dest_klass_t = _gvn.type(refined_dest_klass)->is_klassptr();\n+    const Type* toop = dest_klass_t->cast_to_exactness(false)->as_instance_type();\n@@ -6054,0 +6814,3 @@\n+  Node* dest_klass = load_object_klass(dest);\n+  dest_klass = load_non_refined_array_klass(dest_klass);\n+\n@@ -6058,1 +6821,1 @@\n-                                          load_object_klass(src), load_object_klass(dest),\n+                                          load_object_klass(src), dest_klass,\n","filename":"src\/hotspot\/share\/opto\/library_call.cpp","additions":911,"deletions":148,"binary":false,"changes":1059,"status":"modified"},{"patch":"@@ -46,0 +46,1 @@\n+class UnswitchCandidate;\n@@ -88,1 +89,1 @@\n-       };\n+         FlatArrays            = 1<<18};\n@@ -111,0 +112,1 @@\n+  bool is_flat_arrays() const { return _loop_flags & FlatArrays; }\n@@ -124,0 +126,1 @@\n+  void mark_flat_arrays() { _loop_flags |= FlatArrays; }\n@@ -756,0 +759,1 @@\n+  bool no_unswitch_candidate() const;\n@@ -1560,1 +1564,2 @@\n-  IfNode* find_unswitch_candidate(const IdealLoopTree* loop) const;\n+  IfNode* find_unswitch_candidates(const IdealLoopTree* loop, Node_List& flat_array_checks) const;\n+  IfNode* find_unswitch_candidate_from_idoms(const IdealLoopTree* loop) const;\n@@ -1567,1 +1572,1 @@\n-                                   const UnswitchedLoopSelector& unswitched_loop_selector);\n+                                   const UnswitchCandidate& unswitch_candidate, const IfNode* loop_selector);\n@@ -1575,0 +1580,1 @@\n+                                            const UnswitchCandidate& unswitch_candidate,\n@@ -1772,0 +1778,1 @@\n+  void move_flat_array_check_out_of_loop(Node* n);\n@@ -1773,0 +1780,1 @@\n+  bool flat_array_element_type_check(Node *n);\n@@ -1964,0 +1972,2 @@\n+  void collect_flat_array_checks(const IdealLoopTree* loop, Node_List& flat_array_checks) const;\n+\n","filename":"src\/hotspot\/share\/opto\/loopnode.hpp","additions":13,"deletions":3,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -34,0 +34,1 @@\n+#include \"opto\/inlinetypenode.hpp\"\n@@ -66,0 +67,6 @@\n+  \/\/ Inline types should not be split through Phis because they cannot be merged\n+  \/\/ through Phi nodes but each value input needs to be merged individually.\n+  if (n->is_InlineType()) {\n+    return nullptr;\n+  }\n+\n@@ -795,0 +802,4 @@\n+      if (inp->isa_InlineType()) {\n+        \/\/ TODO 8302217 This prevents PhiNode::push_inline_types_through\n+        return nullptr;\n+      }\n@@ -1124,0 +1135,48 @@\n+\/\/ We can't use immutable memory for the flat array check because we are loading the mark word which is\n+\/\/ mutable. Although the bits we are interested in are immutable (we check for markWord::unlocked_value),\n+\/\/ we need to use raw memory to not break anti dependency analysis. Below code will attempt to still move\n+\/\/ flat array checks out of loops, mainly to enable loop unswitching.\n+void PhaseIdealLoop::move_flat_array_check_out_of_loop(Node* n) {\n+  \/\/ Skip checks for more than one array\n+  if (n->req() > 3) {\n+    return;\n+  }\n+  Node* mem = n->in(FlatArrayCheckNode::Memory);\n+  Node* array = n->in(FlatArrayCheckNode::ArrayOrKlass)->uncast();\n+  IdealLoopTree* check_loop = get_loop(get_ctrl(n));\n+  IdealLoopTree* ary_loop = get_loop(get_ctrl(array));\n+\n+  \/\/ Check if array is loop invariant\n+  if (!check_loop->is_member(ary_loop)) {\n+    \/\/ Walk up memory graph from the check until we leave the loop\n+    VectorSet wq;\n+    wq.set(mem->_idx);\n+    while (check_loop->is_member(get_loop(ctrl_or_self(mem)))) {\n+      if (mem->is_Phi()) {\n+        mem = mem->in(1);\n+      } else if (mem->is_MergeMem()) {\n+        mem = mem->as_MergeMem()->memory_at(Compile::AliasIdxRaw);\n+      } else if (mem->is_Proj()) {\n+        mem = mem->in(0);\n+      } else if (mem->is_MemBar() || mem->is_SafePoint()) {\n+        mem = mem->in(TypeFunc::Memory);\n+      } else if (mem->is_Store() || mem->is_LoadStore() || mem->is_ClearArray()) {\n+        mem = mem->in(MemNode::Memory);\n+      } else {\n+#ifdef ASSERT\n+        mem->dump();\n+#endif\n+        ShouldNotReachHere();\n+      }\n+      if (wq.test_set(mem->_idx)) {\n+        return;\n+      }\n+    }\n+    \/\/ Replace memory input and re-compute ctrl to move the check out of the loop\n+    _igvn.replace_input_of(n, 1, mem);\n+    set_ctrl_and_loop(n, get_early_ctrl(n));\n+    Node* bol = n->unique_out();\n+    set_ctrl_and_loop(bol, get_early_ctrl(bol));\n+  }\n+}\n+\n@@ -1136,0 +1195,6 @@\n+\n+  if (n->isa_FlatArrayCheck()) {\n+    move_flat_array_check_out_of_loop(n);\n+    return n;\n+  }\n+\n@@ -1408,0 +1473,98 @@\n+bool PhaseIdealLoop::flat_array_element_type_check(Node *n) {\n+  \/\/ If the CmpP is a subtype check for a value that has just been\n+  \/\/ loaded from an array, the subtype check guarantees the value\n+  \/\/ can't be stored in a flat array and the load of the value\n+  \/\/ happens with a flat array check then: push the type check\n+  \/\/ through the phi of the flat array check. This needs special\n+  \/\/ logic because the subtype check's input is not a phi but a\n+  \/\/ LoadKlass that must first be cloned through the phi.\n+  if (n->Opcode() != Op_CmpP) {\n+    return false;\n+  }\n+\n+  Node* klassptr = n->in(1);\n+  Node* klasscon = n->in(2);\n+\n+  if (klassptr->is_DecodeNarrowPtr()) {\n+    klassptr = klassptr->in(1);\n+  }\n+\n+  if (klassptr->Opcode() != Op_LoadKlass && klassptr->Opcode() != Op_LoadNKlass) {\n+    return false;\n+  }\n+\n+  if (!klasscon->is_Con()) {\n+    return false;\n+  }\n+\n+  Node* addr = klassptr->in(MemNode::Address);\n+\n+  if (!addr->is_AddP()) {\n+    return false;\n+  }\n+\n+  intptr_t offset;\n+  Node* obj = AddPNode::Ideal_base_and_offset(addr, &_igvn, offset);\n+\n+  if (obj == nullptr) {\n+    return false;\n+  }\n+\n+  assert(obj != nullptr && addr->in(AddPNode::Base) == addr->in(AddPNode::Address), \"malformed AddP?\");\n+  if (obj->Opcode() == Op_CastPP) {\n+    obj = obj->in(1);\n+  }\n+\n+  if (!obj->is_Phi()) {\n+    return false;\n+  }\n+\n+  Node* region = obj->in(0);\n+\n+  Node* phi = PhiNode::make_blank(region, n->in(1));\n+  for (uint i = 1; i < region->req(); i++) {\n+    Node* in = obj->in(i);\n+    Node* ctrl = region->in(i);\n+    if (addr->in(AddPNode::Base) != obj) {\n+      Node* cast = addr->in(AddPNode::Base);\n+      assert(cast->Opcode() == Op_CastPP && cast->in(0) != nullptr, \"inconsistent subgraph\");\n+      Node* cast_clone = cast->clone();\n+      cast_clone->set_req(0, ctrl);\n+      cast_clone->set_req(1, in);\n+      register_new_node(cast_clone, ctrl);\n+      const Type* tcast = cast_clone->Value(&_igvn);\n+      _igvn.set_type(cast_clone, tcast);\n+      cast_clone->as_Type()->set_type(tcast);\n+      in = cast_clone;\n+    }\n+    Node* addr_clone = addr->clone();\n+    addr_clone->set_req(AddPNode::Base, in);\n+    addr_clone->set_req(AddPNode::Address, in);\n+    register_new_node(addr_clone, ctrl);\n+    _igvn.set_type(addr_clone, addr_clone->Value(&_igvn));\n+    Node* klassptr_clone = klassptr->clone();\n+    klassptr_clone->set_req(2, addr_clone);\n+    register_new_node(klassptr_clone, ctrl);\n+    _igvn.set_type(klassptr_clone, klassptr_clone->Value(&_igvn));\n+    if (klassptr != n->in(1)) {\n+      Node* decode = n->in(1);\n+      assert(decode->is_DecodeNarrowPtr(), \"inconsistent subgraph\");\n+      Node* decode_clone = decode->clone();\n+      decode_clone->set_req(1, klassptr_clone);\n+      register_new_node(decode_clone, ctrl);\n+      _igvn.set_type(decode_clone, decode_clone->Value(&_igvn));\n+      klassptr_clone = decode_clone;\n+    }\n+    phi->set_req(i, klassptr_clone);\n+  }\n+  register_new_node(phi, region);\n+  Node* orig = n->in(1);\n+  _igvn.replace_input_of(n, 1, phi);\n+  split_if_with_blocks_post(n);\n+  if (n->outcnt() != 0) {\n+    _igvn.replace_input_of(n, 1, orig);\n+    _igvn.remove_dead_node(phi);\n+  }\n+  return true;\n+}\n+\n@@ -1414,0 +1577,4 @@\n+  if (flat_array_element_type_check(n)) {\n+    return;\n+  }\n+\n@@ -1557,0 +1724,5 @@\n+\n+  \/\/ Remove multiple allocations of the same inline type\n+  if (n->is_InlineType()) {\n+    n->as_InlineType()->remove_redundant_allocations(this);\n+  }\n@@ -2065,1 +2237,9 @@\n-  Node *sample_cmp = sample_bool->in(1);\n+  Node* sample_cmp = sample_bool->in(1);\n+  const Type* t = Type::TOP;\n+  const TypePtr* at = nullptr;\n+  if (sample_cmp->is_FlatArrayCheck()) {\n+    \/\/ Left input of a FlatArrayCheckNode is memory, set the (adr) type of the phi accordingly\n+    assert(sample_cmp->in(1)->bottom_type() == Type::MEMORY, \"unexpected input type\");\n+    t = Type::MEMORY;\n+    at = TypeRawPtr::BOTTOM;\n+  }\n@@ -2068,1 +2248,1 @@\n-  PhiNode *phi1 = new PhiNode(phi->in(0), Type::TOP);\n+  PhiNode *phi1 = new PhiNode(phi->in(0), t, at);\n","filename":"src\/hotspot\/share\/opto\/loopopts.cpp","additions":182,"deletions":2,"binary":false,"changes":184,"status":"modified"},{"patch":"@@ -25,0 +25,2 @@\n+#include \"ci\/ciFlatArrayKlass.hpp\"\n+#include \"ci\/ciInstanceKlass.hpp\"\n@@ -38,0 +40,1 @@\n+#include \"opto\/inlinetypenode.hpp\"\n@@ -46,0 +49,1 @@\n+#include \"opto\/opcodes.hpp\"\n@@ -55,0 +59,2 @@\n+#include \"runtime\/stubRoutines.hpp\"\n+#include \"utilities\/globalDefinitions.hpp\"\n@@ -84,11 +90,0 @@\n-void PhaseMacroExpand::migrate_outs(Node *old, Node *target) {\n-  assert(old != nullptr, \"sanity\");\n-  for (DUIterator_Fast imax, i = old->fast_outs(imax); i < imax; i++) {\n-    Node* use = old->fast_out(i);\n-    _igvn.rehash_node_delayed(use);\n-    imax -= replace_input(use, old, target);\n-    \/\/ back up iterator\n-    --i;\n-  }\n-  assert(old->outcnt() == 0, \"all uses must be deleted\");\n-}\n@@ -147,1 +142,1 @@\n-  bs->eliminate_gc_barrier(this, p2x);\n+  bs->eliminate_gc_barrier(&_igvn, p2x);\n@@ -190,0 +185,2 @@\n+      } else if (in->is_LoadFlat() || in->is_StoreFlat()) {\n+        mem = in->in(TypeFunc::Memory);\n@@ -202,1 +199,1 @@\n-        int adr_offset = atype->offset();\n+        int adr_offset = atype->flat_offset();\n@@ -247,1 +244,1 @@\n-   } else if (mem->Opcode() == Op_StrInflatedCopy) {\n+    } else if (mem->Opcode() == Op_StrInflatedCopy) {\n@@ -292,1 +289,5 @@\n-      const TypePtr* adr_type = nullptr;\n+      Node* base = ac->in(ArrayCopyNode::Src);\n+      const TypeAryPtr* adr_type = _igvn.type(base)->is_aryptr();\n+      if (adr_type->is_flat()) {\n+        shift = adr_type->flat_log_elem_size();\n+      }\n@@ -295,2 +296,2 @@\n-        Node* base = ac->in(ArrayCopyNode::Src);\n-        adr_type = _igvn.type(base)->is_ptr()->add_offset(off);\n+        adr_type = _igvn.type(adr)->is_aryptr();\n+        assert(adr_type == _igvn.type(base)->is_aryptr()->add_field_offset_and_offset(off), \"incorrect address type\");\n@@ -300,1 +301,1 @@\n-          return value_from_mem(ac->in(TypeFunc::Memory), ctl, ft, ftype, adr_type->isa_oopptr(), alloc);\n+          return value_from_mem(ac->in(TypeFunc::Memory), ctl, ft, ftype, adr_type, alloc);\n@@ -303,0 +304,5 @@\n+        if (ac->in(ArrayCopyNode::Src) == ac->in(ArrayCopyNode::Dest)) {\n+          \/\/ Non constant offset in the array: we can't statically\n+          \/\/ determine the value\n+          return nullptr;\n+        }\n@@ -310,7 +316,5 @@\n-        Node* base = ac->in(ArrayCopyNode::Src);\n-        adr_type = _igvn.type(base)->is_ptr()->add_offset(Type::OffsetBot);\n-        if (ac->in(ArrayCopyNode::Src) == ac->in(ArrayCopyNode::Dest)) {\n-          \/\/ Non constant offset in the array: we can't statically\n-          \/\/ determine the value\n-          return nullptr;\n-        }\n+        \/\/ In the case of a flat inline type array, each field has its\n+        \/\/ own slice so we need to extract the field being accessed from\n+        \/\/ the address computation\n+        adr_type = adr_type->add_field_offset_and_offset(offset)->add_offset(Type::OffsetBot)->is_aryptr();\n+        adr = _igvn.transform(new CastPPNode(ctl, adr, adr_type));\n@@ -327,0 +331,1 @@\n+      assert(res->isa_DecodeN(), \"should be narrow oop\");\n@@ -342,1 +347,1 @@\n-  int offset = adr_t->offset();\n+  int offset = adr_t->flat_offset();\n@@ -378,1 +383,1 @@\n-    } else  {\n+    } else {\n@@ -381,2 +386,14 @@\n-        \/\/ hit a sentinel, return appropriate 0 value\n-        values.at_put(j, _igvn.zerocon(ft));\n+        \/\/ hit a sentinel, return appropriate value\n+        Node* init_value = alloc->in(AllocateNode::InitValue);\n+        if (init_value != nullptr) {\n+          if (val == start_mem) {\n+            \/\/ TODO 8350865 Scalar replacement does not work well for flat arrays.\n+            \/\/ Somehow we ended up with root mem and therefore walked past the alloc. Fix this. Triggered by TestGenerated::test15\n+            \/\/ Don't we need field_value_by_offset?\n+            return nullptr;\n+          }\n+          values.at_put(j, init_value);\n+        } else {\n+          assert(alloc->in(AllocateNode::RawInitValue) == nullptr, \"init value may not be null\");\n+          values.at_put(j, _igvn.zerocon(ft));\n+        }\n@@ -401,2 +418,10 @@\n-      } else if(val->is_Proj() && val->in(0) == alloc) {\n-        values.at_put(j, _igvn.zerocon(ft));\n+      } else if (val->is_Proj() && val->in(0) == alloc) {\n+        Node* init_value = alloc->in(AllocateNode::InitValue);\n+        if (init_value != nullptr) {\n+          \/\/ TODO 8350865 Scalar replacement does not work well for flat arrays.\n+          \/\/ Is this correct for non-all-zero init values? Don't we need field_value_by_offset?\n+          values.at_put(j, init_value);\n+        } else {\n+          assert(alloc->in(AllocateNode::RawInitValue) == nullptr, \"init value may not be null\");\n+          values.at_put(j, _igvn.zerocon(ft));\n+        }\n@@ -450,1 +475,1 @@\n-  int offset = adr_t->offset();\n+  int offset = adr_t->flat_offset();\n@@ -452,1 +477,0 @@\n-  Node *alloc_ctrl = alloc->in(TypeFunc::Control);\n@@ -469,1 +493,1 @@\n-        done = true; \/\/ Something go wrong.\n+        done = true; \/\/ Something went wrong.\n@@ -479,1 +503,1 @@\n-             atype->is_known_instance_field() && atype->offset() == offset &&\n+             atype->is_known_instance_field() && atype->flat_offset() == offset &&\n@@ -516,1 +540,17 @@\n-      \/\/ hit a sentinel, return appropriate 0 value\n+      \/\/ hit a sentinel, return appropriate value\n+      Node* init_value = alloc->in(AllocateNode::InitValue);\n+      if (init_value != nullptr) {\n+        if (adr_t->is_flat()) {\n+          if (init_value->is_EncodeP()) {\n+            init_value = init_value->in(1);\n+          }\n+          assert(adr_t->is_aryptr()->field_offset().get() != Type::OffsetBot, \"Unknown offset\");\n+          offset = adr_t->is_aryptr()->field_offset().get() + init_value->bottom_type()->inline_klass()->payload_offset();\n+          init_value = init_value->as_InlineType()->field_value_by_offset(offset, true);\n+          if (ft == T_NARROWOOP) {\n+            init_value = transform_later(new EncodePNode(init_value, init_value->bottom_type()->make_ptr()));\n+          }\n+        }\n+        return init_value;\n+      }\n+      assert(alloc->in(AllocateNode::RawInitValue) == nullptr, \"init value may not be null\");\n@@ -548,1 +588,1 @@\n-  \/\/ Something go wrong.\n+  \/\/ Something went wrong.\n@@ -552,0 +592,69 @@\n+\/\/ Search the last value stored into the inline type's fields (for flat arrays).\n+Node* PhaseMacroExpand::inline_type_from_mem(ciInlineKlass* vk, const TypeAryPtr* elem_adr_type, int elem_idx, int offset_in_element, bool null_free, AllocateNode* alloc, SafePointNode* sfpt) {\n+  auto report_failure = [&](int field_offset_in_element) {\n+#ifndef PRODUCT\n+    if (PrintEliminateAllocations) {\n+      ciInlineKlass* elem_klass = elem_adr_type->elem()->inline_klass();\n+      int offset = field_offset_in_element + elem_klass->payload_offset();\n+      ciField* flattened_field = elem_klass->get_field_by_offset(offset, false);\n+      assert(flattened_field != nullptr, \"must have a field of type %s at offset %d\", elem_klass->name()->as_utf8(), offset);\n+      tty->print(\"=== At SafePoint node %d can't find value of field [%s] of array element [%d]\", sfpt->_idx, flattened_field->name()->as_utf8(), elem_idx);\n+      tty->print(\", which prevents elimination of: \");\n+      alloc->dump();\n+    }\n+#endif \/\/ PRODUCT\n+  };\n+\n+  \/\/ Create a new InlineTypeNode and retrieve the field values from memory\n+  InlineTypeNode* vt = InlineTypeNode::make_uninitialized(_igvn, vk, false);\n+  transform_later(vt);\n+  if (null_free) {\n+    vt->set_null_marker(_igvn);\n+  } else {\n+    int nm_offset_in_element = offset_in_element + vk->null_marker_offset_in_payload();\n+    const TypeAryPtr* nm_adr_type = elem_adr_type->with_field_offset(nm_offset_in_element);\n+    Node* nm_value = value_from_mem(sfpt->memory(), sfpt->control(), T_BOOLEAN, TypeInt::BOOL, nm_adr_type, alloc);\n+    if (nm_value != nullptr) {\n+      vt->set_null_marker(_igvn, nm_value);\n+    } else {\n+      report_failure(nm_offset_in_element);\n+      return nullptr;\n+    }\n+  }\n+\n+  for (int i = 0; i < vk->nof_declared_nonstatic_fields(); ++i) {\n+    ciType* field_type = vt->field_type(i);\n+    int field_offset_in_element = offset_in_element + vt->field_offset(i) - vk->payload_offset();\n+    Node* field_value = nullptr;\n+    if (vt->field_is_flat(i)) {\n+      field_value = inline_type_from_mem(field_type->as_inline_klass(), elem_adr_type, elem_idx, field_offset_in_element, vt->field_is_null_free(i), alloc, sfpt);\n+    } else {\n+      const Type* ft = Type::get_const_type(field_type);\n+      BasicType bt = type2field[field_type->basic_type()];\n+      if (UseCompressedOops && !is_java_primitive(bt)) {\n+        ft = ft->make_narrowoop();\n+        bt = T_NARROWOOP;\n+      }\n+      \/\/ Each inline type field has its own memory slice\n+      const TypeAryPtr* field_adr_type = elem_adr_type->with_field_offset(field_offset_in_element);\n+      field_value = value_from_mem(sfpt->memory(), sfpt->control(), bt, ft, field_adr_type, alloc);\n+      if (field_value == nullptr) {\n+        report_failure(field_offset_in_element);\n+      } else if (ft->isa_narrowoop()) {\n+        assert(UseCompressedOops, \"unexpected narrow oop\");\n+        if (field_value->is_EncodeP()) {\n+          field_value = field_value->in(1);\n+        } else if (!field_value->is_InlineType()) {\n+          field_value = transform_later(new DecodeNNode(field_value, field_value->get_ptr_type()));\n+        }\n+      }\n+    }\n+    if (field_value != nullptr) {\n+      vt->set_field_value(i, field_value);\n+    } else {\n+      return nullptr;\n+    }\n+  }\n+  return vt;\n+}\n+\n@@ -561,0 +670,1 @@\n+  Unique_Node_List worklist;\n@@ -569,0 +679,1 @@\n+    worklist.push(res);\n@@ -585,1 +696,1 @@\n-  if (can_eliminate && res != nullptr) {\n+  while (can_eliminate && worklist.size() > 0) {\n@@ -587,2 +698,2 @@\n-    for (DUIterator_Fast jmax, j = res->fast_outs(jmax);\n-                               j < jmax && can_eliminate; j++) {\n+    res = worklist.pop();\n+    for (DUIterator_Fast jmax, j = res->fast_outs(jmax); j < jmax && can_eliminate; j++) {\n@@ -603,1 +714,1 @@\n-          if (n->is_Mem() && n->as_Mem()->is_mismatched_access()) {\n+          if ((n->is_Mem() && n->as_Mem()->is_mismatched_access()) || n->is_LoadFlat() || n->is_StoreFlat()) {\n@@ -639,0 +750,1 @@\n+          assert(!res->is_Phi() || !res->as_Phi()->can_be_inline_type(), \"Inline type allocations should not have safepoint uses\");\n@@ -641,0 +753,23 @@\n+      } else if (use->is_InlineType() && use->as_InlineType()->get_oop() == res) {\n+        \/\/ Look at uses\n+        for (DUIterator_Fast kmax, k = use->fast_outs(kmax); k < kmax; k++) {\n+          Node* u = use->fast_out(k);\n+          if (u->is_InlineType()) {\n+            \/\/ Use in flat field can be eliminated\n+            InlineTypeNode* vt = u->as_InlineType();\n+            for (uint i = 0; i < vt->field_count(); ++i) {\n+              if (vt->field_value(i) == use && !vt->field_is_flat(i)) {\n+                can_eliminate = false; \/\/ Use in non-flat field\n+                break;\n+              }\n+            }\n+          } else {\n+            \/\/ Add other uses to the worklist to process individually\n+            worklist.push(use);\n+          }\n+        }\n+      } else if (use->Opcode() == Op_StoreX && use->in(MemNode::Address) == res) {\n+        \/\/ Store to mark word of inline type larval buffer\n+        assert(res_type->is_inlinetypeptr(), \"Unexpected store to mark word\");\n+      } else if (res_type->is_inlinetypeptr() && (use->Opcode() == Op_MemBarRelease || use->Opcode() == Op_MemBarStoreStore)) {\n+        \/\/ Inline type buffer allocations are followed by a membar\n@@ -663,0 +798,3 @@\n+      } else {\n+        assert(use->Opcode() == Op_CastP2X, \"should be\");\n+        assert(!use->has_out_with(Op_OrL), \"should have been removed because oop is never null\");\n@@ -675,1 +813,1 @@\n-    } else if (alloc->_is_scalar_replaceable) {\n+    } else {\n@@ -780,1 +918,148 @@\n-SafePointScalarObjectNode* PhaseMacroExpand::create_scalarized_object_description(AllocateNode *alloc, SafePointNode* sfpt) {\n+void PhaseMacroExpand::process_field_value_at_safepoint(const Type* field_type, Node* field_val, SafePointNode* sfpt, Unique_Node_List* value_worklist) {\n+  if (UseCompressedOops && field_type->isa_narrowoop()) {\n+    \/\/ Enable \"DecodeN(EncodeP(Allocate)) --> Allocate\" transformation\n+    \/\/ to be able scalar replace the allocation.\n+    if (field_val->is_EncodeP()) {\n+      field_val = field_val->in(1);\n+    } else if (!field_val->is_InlineType()) {\n+      field_val = transform_later(new DecodeNNode(field_val, field_val->get_ptr_type()));\n+    }\n+  }\n+\n+  \/\/ Keep track of inline types to scalarize them later\n+  if (field_val->is_InlineType()) {\n+    value_worklist->push(field_val);\n+  } else if (field_val->is_Phi()) {\n+    PhiNode* phi = field_val->as_Phi();\n+    \/\/ Eagerly replace inline type phis now since we could be removing an inline type allocation where we must\n+    \/\/ scalarize all its fields in safepoints.\n+    field_val = phi->try_push_inline_types_down(&_igvn, true);\n+    if (field_val->is_InlineType()) {\n+      value_worklist->push(field_val);\n+    }\n+  }\n+  DEBUG_ONLY(verify_type_compatability(field_val->bottom_type(), field_type);)\n+  sfpt->add_req(field_val);\n+}\n+\n+bool PhaseMacroExpand::add_array_elems_to_safepoint(AllocateNode* alloc, const TypeAryPtr* array_type, SafePointNode* sfpt, Unique_Node_List* value_worklist) {\n+  const Type* elem_type = array_type->elem();\n+  BasicType basic_elem_type = elem_type->array_element_basic_type();\n+\n+  intptr_t elem_size;\n+  uint header_size;\n+  if (array_type->is_flat()) {\n+    elem_size = array_type->flat_elem_size();\n+    header_size = arrayOopDesc::base_offset_in_bytes(T_FLAT_ELEMENT);\n+  } else {\n+    elem_size = type2aelembytes(basic_elem_type);\n+    header_size = arrayOopDesc::base_offset_in_bytes(basic_elem_type);\n+  }\n+\n+  int n_elems = alloc->in(AllocateNode::ALength)->get_int();\n+  for (int elem_idx = 0; elem_idx < n_elems; elem_idx++) {\n+    intptr_t elem_offset = header_size + elem_idx * elem_size;\n+    const TypeAryPtr* elem_adr_type = array_type->with_offset(elem_offset);\n+    Node* elem_val;\n+    if (array_type->is_flat()) {\n+      ciInlineKlass* elem_klass = elem_type->inline_klass();\n+      assert(elem_klass->maybe_flat_in_array(), \"must be flat in array\");\n+      elem_val = inline_type_from_mem(elem_klass, elem_adr_type, elem_idx, 0, array_type->is_null_free(), alloc, sfpt);\n+    } else {\n+      elem_val = value_from_mem(sfpt->memory(), sfpt->control(), basic_elem_type, elem_type, elem_adr_type, alloc);\n+#ifndef PRODUCT\n+      if (PrintEliminateAllocations && elem_val == nullptr) {\n+        tty->print(\"=== At SafePoint node %d can't find value of array element [%d]\", sfpt->_idx, elem_idx);\n+        tty->print(\", which prevents elimination of: \");\n+        alloc->dump();\n+      }\n+#endif \/\/ PRODUCT\n+    }\n+    if (elem_val == nullptr) {\n+      return false;\n+    }\n+\n+    process_field_value_at_safepoint(elem_type, elem_val, sfpt, value_worklist);\n+  }\n+\n+  return true;\n+}\n+\n+\/\/ Recursively adds all flattened fields of a type 'iklass' inside 'base' to 'sfpt'.\n+\/\/ 'offset_minus_header' refers to the offset of the payload of 'iklass' inside 'base' minus the\n+\/\/ payload offset of 'iklass'. If 'base' is of type 'iklass' then 'offset_minus_header' == 0.\n+bool PhaseMacroExpand::add_inst_fields_to_safepoint(ciInstanceKlass* iklass, AllocateNode* alloc, Node* base, int offset_minus_header, SafePointNode* sfpt, Unique_Node_List* value_worklist) {\n+  const TypeInstPtr* base_type = _igvn.type(base)->is_instptr();\n+  auto report_failure = [&](int offset) {\n+#ifndef PRODUCT\n+    if (PrintEliminateAllocations) {\n+      ciInstanceKlass* base_klass = base_type->instance_klass();\n+      ciField* flattened_field = base_klass->get_field_by_offset(offset, false);\n+      assert(flattened_field != nullptr, \"must have a field of type %s at offset %d\", base_klass->name()->as_utf8(), offset);\n+      tty->print(\"=== At SafePoint node %d can't find value of field: \", sfpt->_idx);\n+      flattened_field->print();\n+      int field_idx = C->alias_type(flattened_field)->index();\n+      tty->print(\" (alias_idx=%d)\", field_idx);\n+      tty->print(\", which prevents elimination of: \");\n+      base->dump();\n+    }\n+#endif \/\/ PRODUCT\n+  };\n+\n+  for (int i = 0; i < iklass->nof_declared_nonstatic_fields(); i++) {\n+    ciField* field = iklass->declared_nonstatic_field_at(i);\n+    if (field->is_flat()) {\n+      ciInlineKlass* fvk = field->type()->as_inline_klass();\n+      int field_offset_minus_header = offset_minus_header + field->offset_in_bytes() - fvk->payload_offset();\n+      bool success = add_inst_fields_to_safepoint(fvk, alloc, base, field_offset_minus_header, sfpt, value_worklist);\n+      if (!success) {\n+        return false;\n+      }\n+\n+      \/\/ The null marker of a field is added right after we scalarize that field\n+      if (!field->is_null_free()) {\n+        int nm_offset = offset_minus_header + field->null_marker_offset();\n+        Node* null_marker = value_from_mem(sfpt->memory(), sfpt->control(), T_BOOLEAN, TypeInt::BOOL, base_type->with_offset(nm_offset), alloc);\n+        if (null_marker == nullptr) {\n+          report_failure(nm_offset);\n+          return false;\n+        }\n+        process_field_value_at_safepoint(TypeInt::BOOL, null_marker, sfpt, value_worklist);\n+      }\n+\n+      continue;\n+    }\n+\n+    int offset = offset_minus_header + field->offset_in_bytes();\n+    ciType* elem_type = field->type();\n+    BasicType basic_elem_type = field->layout_type();\n+\n+    const Type* field_type;\n+    if (is_reference_type(basic_elem_type)) {\n+      if (!elem_type->is_loaded()) {\n+        field_type = TypeInstPtr::BOTTOM;\n+      } else {\n+        field_type = TypeOopPtr::make_from_klass(elem_type->as_klass());\n+      }\n+      if (UseCompressedOops) {\n+        field_type = field_type->make_narrowoop();\n+        basic_elem_type = T_NARROWOOP;\n+      }\n+    } else {\n+      field_type = Type::get_const_basic_type(basic_elem_type);\n+    }\n+\n+    const TypeInstPtr* field_addr_type = base_type->add_offset(offset)->isa_instptr();\n+    Node* field_val = value_from_mem(sfpt->memory(), sfpt->control(), basic_elem_type, field_type, field_addr_type, alloc);\n+    if (field_val == nullptr) {\n+      report_failure(offset);\n+      return false;\n+    }\n+    process_field_value_at_safepoint(field_type, field_val, sfpt, value_worklist);\n+  }\n+\n+  return true;\n+}\n+\n+SafePointScalarObjectNode* PhaseMacroExpand::create_scalarized_object_description(AllocateNode* alloc, SafePointNode* sfpt,\n+                                                                                  Unique_Node_List* value_worklist) {\n@@ -785,2 +1070,0 @@\n-  BasicType basic_elem_type  = T_ILLEGAL;\n-  const Type* field_type     = nullptr;\n@@ -789,2 +1072,0 @@\n-  int array_base             = 0;\n-  int element_size           = 0;\n@@ -796,0 +1077,1 @@\n+  uint before_sfpt_req = sfpt->req();\n@@ -808,4 +1090,10 @@\n-      basic_elem_type = res_type->is_aryptr()->elem()->array_element_basic_type();\n-      array_base = arrayOopDesc::base_offset_in_bytes(basic_elem_type);\n-      element_size = type2aelembytes(basic_elem_type);\n-      field_type = res_type->is_aryptr()->elem();\n+    }\n+\n+    if (res->bottom_type()->is_inlinetypeptr()) {\n+      \/\/ Nullable inline types have a null marker field which is added to the safepoint when scalarizing them (see\n+      \/\/ InlineTypeNode::make_scalar_in_safepoint()). When having circular inline types, we stop scalarizing at depth 1\n+      \/\/ to avoid an endless recursion. Therefore, we do not have a SafePointScalarObjectNode node here, yet.\n+      \/\/ We are about to create a SafePointScalarObjectNode as if this is a normal object. Add an additional int input\n+      \/\/ with value 1 which sets the null marker to true to indicate that the object is always non-null. This input is checked\n+      \/\/ later in PhaseOutput::filLocArray() for inline types.\n+      sfpt->add_req(_igvn.intcon(1));\n@@ -819,64 +1107,4 @@\n-  \/\/ Scan object's fields adding an input to the safepoint for each field.\n-  for (int j = 0; j < nfields; j++) {\n-    intptr_t offset;\n-    ciField* field = nullptr;\n-    if (iklass != nullptr) {\n-      field = iklass->nonstatic_field_at(j);\n-      offset = field->offset_in_bytes();\n-      ciType* elem_type = field->type();\n-      basic_elem_type = field->layout_type();\n-\n-      \/\/ The next code is taken from Parse::do_get_xxx().\n-      if (is_reference_type(basic_elem_type)) {\n-        if (!elem_type->is_loaded()) {\n-          field_type = TypeInstPtr::BOTTOM;\n-        } else if (field != nullptr && field->is_static_constant()) {\n-          ciObject* con = field->constant_value().as_object();\n-          \/\/ Do not \"join\" in the previous type; it doesn't add value,\n-          \/\/ and may yield a vacuous result if the field is of interface type.\n-          field_type = TypeOopPtr::make_from_constant(con)->isa_oopptr();\n-          assert(field_type != nullptr, \"field singleton type must be consistent\");\n-        } else {\n-          field_type = TypeOopPtr::make_from_klass(elem_type->as_klass());\n-        }\n-        if (UseCompressedOops) {\n-          field_type = field_type->make_narrowoop();\n-          basic_elem_type = T_NARROWOOP;\n-        }\n-      } else {\n-        field_type = Type::get_const_basic_type(basic_elem_type);\n-      }\n-    } else {\n-      offset = array_base + j * (intptr_t)element_size;\n-    }\n-\n-    const TypeOopPtr *field_addr_type = res_type->add_offset(offset)->isa_oopptr();\n-\n-    Node *field_val = value_from_mem(sfpt->memory(), sfpt->control(), basic_elem_type, field_type, field_addr_type, alloc);\n-\n-    \/\/ We weren't able to find a value for this field,\n-    \/\/ give up on eliminating this allocation.\n-    if (field_val == nullptr) {\n-      uint last = sfpt->req() - 1;\n-      for (int k = 0;  k < j; k++) {\n-        sfpt->del_req(last--);\n-      }\n-      _igvn._worklist.push(sfpt);\n-\n-#ifndef PRODUCT\n-      if (PrintEliminateAllocations) {\n-        if (field != nullptr) {\n-          tty->print(\"=== At SafePoint node %d can't find value of field: \", sfpt->_idx);\n-          field->print();\n-          int field_idx = C->get_alias_index(field_addr_type);\n-          tty->print(\" (alias_idx=%d)\", field_idx);\n-        } else { \/\/ Array's element\n-          tty->print(\"=== At SafePoint node %d can't find value of array element [%d]\", sfpt->_idx, j);\n-        }\n-        tty->print(\", which prevents elimination of: \");\n-        if (res == nullptr)\n-          alloc->dump();\n-        else\n-          res->dump();\n-      }\n-#endif\n+  if (res == nullptr) {\n+    sfpt->jvms()->set_endoff(sfpt->req());\n+    return sobj;\n+  }\n@@ -884,2 +1112,6 @@\n-      return nullptr;\n-    }\n+  bool success;\n+  if (iklass == nullptr) {\n+    success = add_array_elems_to_safepoint(alloc, res_type->is_aryptr(), sfpt, value_worklist);\n+  } else {\n+    success = add_inst_fields_to_safepoint(iklass, alloc, res, 0, sfpt, value_worklist);\n+  }\n@@ -887,8 +1119,4 @@\n-    if (UseCompressedOops && field_type->isa_narrowoop()) {\n-      \/\/ Enable \"DecodeN(EncodeP(Allocate)) --> Allocate\" transformation\n-      \/\/ to be able scalar replace the allocation.\n-      if (field_val->is_EncodeP()) {\n-        field_val = field_val->in(1);\n-      } else {\n-        field_val = transform_later(new DecodeNNode(field_val, field_val->get_ptr_type()));\n-      }\n+  \/\/ We weren't able to find a value for this field, remove all the fields added to the safepoint\n+  if (!success) {\n+    for (uint i = sfpt->req() - 1; i >= before_sfpt_req; i--) {\n+      sfpt->del_req(i);\n@@ -896,2 +1124,2 @@\n-    DEBUG_ONLY(verify_type_compatability(field_val->bottom_type(), field_type);)\n-    sfpt->add_req(field_val);\n+    _igvn._worklist.push(sfpt);\n+    return nullptr;\n@@ -901,1 +1129,0 @@\n-\n@@ -910,0 +1137,4 @@\n+  const TypeOopPtr* res_type = nullptr;\n+  if (res != nullptr) { \/\/ Could be null when there are no users\n+    res_type = _igvn.type(res)->isa_oopptr();\n+  }\n@@ -912,0 +1143,1 @@\n+  Unique_Node_List value_worklist;\n@@ -914,1 +1146,1 @@\n-    SafePointScalarObjectNode* sobj = create_scalarized_object_description(alloc, sfpt);\n+    SafePointScalarObjectNode* sobj = create_scalarized_object_description(alloc, sfpt, &value_worklist);\n@@ -930,1 +1162,8 @@\n-\n+  \/\/ Scalarize inline types that were added to the safepoint.\n+  \/\/ Don't allow linking a constant oop (if available) for flat array elements\n+  \/\/ because Deoptimization::reassign_flat_array_elements needs field values.\n+  bool allow_oop = (res_type != nullptr) && !res_type->is_flat();\n+  for (uint i = 0; i < value_worklist.size(); ++i) {\n+    InlineTypeNode* vt = value_worklist.at(i)->as_InlineType();\n+    vt->make_scalar_in_safepoints(&_igvn, allow_oop);\n+  }\n@@ -946,1 +1185,2 @@\n-void PhaseMacroExpand::process_users_of_allocation(CallNode *alloc) {\n+void PhaseMacroExpand::process_users_of_allocation(CallNode *alloc, bool inline_alloc) {\n+  Unique_Node_List worklist;\n@@ -949,0 +1189,4 @@\n+    worklist.push(res);\n+  }\n+  while (worklist.size() > 0) {\n+    res = worklist.pop();\n@@ -958,10 +1202,7 @@\n-#ifdef ASSERT\n-            \/\/ Verify that there is no dependent MemBarVolatile nodes,\n-            \/\/ they should be removed during IGVN, see MemBarNode::Ideal().\n-            for (DUIterator_Fast pmax, p = n->fast_outs(pmax);\n-                                       p < pmax; p++) {\n-              Node* mb = n->fast_out(p);\n-              assert(mb->is_Initialize() || !mb->is_MemBar() ||\n-                     mb->req() <= MemBarNode::Precedent ||\n-                     mb->in(MemBarNode::Precedent) != n,\n-                     \"MemBarVolatile should be eliminated for non-escaping object\");\n+            for (DUIterator_Fast pmax, p = n->fast_outs(pmax); p < pmax; p++) {\n+              MemBarNode* mb = n->fast_out(p)->isa_MemBar();\n+              if (mb != nullptr && mb->req() <= MemBarNode::Precedent && mb->in(MemBarNode::Precedent) == n) {\n+                \/\/ MemBarVolatiles should have been removed by MemBarNode::Ideal() for non-inline allocations\n+                assert(inline_alloc, \"MemBarVolatile should be eliminated for non-escaping object\");\n+                mb->remove(&_igvn);\n+              }\n@@ -969,1 +1210,0 @@\n-#endif\n@@ -993,2 +1233,1 @@\n-          CallProjections callprojs;\n-          ac->extract_projections(&callprojs, true);\n+          CallProjections* callprojs = ac->extract_projections(true);\n@@ -996,3 +1235,3 @@\n-          _igvn.replace_node(callprojs.fallthrough_ioproj, ac->in(TypeFunc::I_O));\n-          _igvn.replace_node(callprojs.fallthrough_memproj, ac->in(TypeFunc::Memory));\n-          _igvn.replace_node(callprojs.fallthrough_catchproj, ac->in(TypeFunc::Control));\n+          _igvn.replace_node(callprojs->fallthrough_ioproj, ac->in(TypeFunc::I_O));\n+          _igvn.replace_node(callprojs->fallthrough_memproj, ac->in(TypeFunc::Memory));\n+          _igvn.replace_node(callprojs->fallthrough_catchproj, ac->in(TypeFunc::Control));\n@@ -1015,0 +1254,24 @@\n+      } else if (use->is_InlineType()) {\n+        assert(use->as_InlineType()->get_oop() == res, \"unexpected inline type ptr use\");\n+        \/\/ Cut off oop input and remove known instance id from type\n+        _igvn.rehash_node_delayed(use);\n+        use->as_InlineType()->set_oop(_igvn, _igvn.zerocon(T_OBJECT));\n+        use->as_InlineType()->set_is_buffered(_igvn, false);\n+        const TypeOopPtr* toop = _igvn.type(use)->is_oopptr()->cast_to_instance_id(TypeOopPtr::InstanceBot);\n+        _igvn.set_type(use, toop);\n+        use->as_InlineType()->set_type(toop);\n+        \/\/ Process users\n+        for (DUIterator_Fast kmax, k = use->fast_outs(kmax); k < kmax; k++) {\n+          Node* u = use->fast_out(k);\n+          if (!u->is_InlineType() && !u->is_StoreFlat()) {\n+            worklist.push(u);\n+          }\n+        }\n+      } else if (use->Opcode() == Op_StoreX && use->in(MemNode::Address) == res) {\n+        \/\/ Store to mark word of inline type larval buffer\n+        assert(inline_alloc, \"Unexpected store to mark word\");\n+        _igvn.replace_node(use, use->in(MemNode::Memory));\n+      } else if (use->Opcode() == Op_MemBarRelease || use->Opcode() == Op_MemBarStoreStore) {\n+        \/\/ Inline type buffer allocations are followed by a membar\n+        assert(inline_alloc, \"Unexpected MemBarRelease\");\n+        use->as_MemBar()->remove(&_igvn);\n@@ -1027,1 +1290,1 @@\n-  if (_callprojs.resproj != nullptr && _callprojs.resproj->outcnt() != 0) {\n+  if (_callprojs->resproj[0] != nullptr && _callprojs->resproj[0]->outcnt() != 0) {\n@@ -1031,2 +1294,2 @@\n-    for (DUIterator_Fast jmax, j = _callprojs.resproj->fast_outs(jmax);  j < jmax; j++) {\n-      Node* use = _callprojs.resproj->fast_out(j);\n+    for (DUIterator_Fast jmax, j = _callprojs->resproj[0]->fast_outs(jmax);  j < jmax; j++) {\n+      Node* use = _callprojs->resproj[0]->fast_out(j);\n@@ -1039,3 +1302,3 @@\n-    for (DUIterator_Last jmin, j = _callprojs.resproj->last_outs(jmin); j >= jmin; ) {\n-      Node* use = _callprojs.resproj->last_out(j);\n-      uint oc1 = _callprojs.resproj->outcnt();\n+    for (DUIterator_Last jmin, j = _callprojs->resproj[0]->last_outs(jmin); j >= jmin; ) {\n+      Node* use = _callprojs->resproj[0]->last_out(j);\n+      uint oc1 = _callprojs->resproj[0]->outcnt();\n@@ -1051,1 +1314,1 @@\n-          assert(tmp == nullptr || tmp == _callprojs.fallthrough_catchproj, \"allocation control projection\");\n+          assert(tmp == nullptr || tmp == _callprojs->fallthrough_catchproj, \"allocation control projection\");\n@@ -1058,1 +1321,1 @@\n-            assert(mem->as_MergeMem()->memory_at(Compile::AliasIdxRaw) == _callprojs.fallthrough_memproj, \"allocation memory projection\");\n+            assert(mem->as_MergeMem()->memory_at(Compile::AliasIdxRaw) == _callprojs->fallthrough_memproj, \"allocation memory projection\");\n@@ -1060,1 +1323,1 @@\n-            assert(mem == _callprojs.fallthrough_memproj, \"allocation memory projection\");\n+            assert(mem == _callprojs->fallthrough_memproj, \"allocation memory projection\");\n@@ -1065,0 +1328,4 @@\n+      } else if (use->Opcode() == Op_MemBarStoreStore) {\n+        \/\/ Inline type buffer allocations are followed by a membar\n+        assert(inline_alloc, \"Unexpected MemBarStoreStore\");\n+        use->as_MemBar()->remove(&_igvn);\n@@ -1069,1 +1336,1 @@\n-      j -= (oc1 - _callprojs.resproj->outcnt());\n+      j -= (oc1 - _callprojs->resproj[0]->outcnt());\n@@ -1072,2 +1339,2 @@\n-  if (_callprojs.fallthrough_catchproj != nullptr) {\n-    _igvn.replace_node(_callprojs.fallthrough_catchproj, alloc->in(TypeFunc::Control));\n+  if (_callprojs->fallthrough_catchproj != nullptr) {\n+    _igvn.replace_node(_callprojs->fallthrough_catchproj, alloc->in(TypeFunc::Control));\n@@ -1075,2 +1342,2 @@\n-  if (_callprojs.fallthrough_memproj != nullptr) {\n-    _igvn.replace_node(_callprojs.fallthrough_memproj, alloc->in(TypeFunc::Memory));\n+  if (_callprojs->fallthrough_memproj != nullptr) {\n+    _igvn.replace_node(_callprojs->fallthrough_memproj, alloc->in(TypeFunc::Memory));\n@@ -1078,2 +1345,2 @@\n-  if (_callprojs.catchall_memproj != nullptr) {\n-    _igvn.replace_node(_callprojs.catchall_memproj, C->top());\n+  if (_callprojs->catchall_memproj != nullptr) {\n+    _igvn.replace_node(_callprojs->catchall_memproj, C->top());\n@@ -1081,2 +1348,2 @@\n-  if (_callprojs.fallthrough_ioproj != nullptr) {\n-    _igvn.replace_node(_callprojs.fallthrough_ioproj, alloc->in(TypeFunc::I_O));\n+  if (_callprojs->fallthrough_ioproj != nullptr) {\n+    _igvn.replace_node(_callprojs->fallthrough_ioproj, alloc->in(TypeFunc::I_O));\n@@ -1084,2 +1351,2 @@\n-  if (_callprojs.catchall_ioproj != nullptr) {\n-    _igvn.replace_node(_callprojs.catchall_ioproj, C->top());\n+  if (_callprojs->catchall_ioproj != nullptr) {\n+    _igvn.replace_node(_callprojs->catchall_ioproj, C->top());\n@@ -1087,2 +1354,2 @@\n-  if (_callprojs.catchall_catchproj != nullptr) {\n-    _igvn.replace_node(_callprojs.catchall_catchproj, C->top());\n+  if (_callprojs->catchall_catchproj != nullptr) {\n+    _igvn.replace_node(_callprojs->catchall_catchproj, C->top());\n@@ -1098,1 +1365,1 @@\n-  if (!EliminateAllocations || !alloc->_is_non_escaping) {\n+  if (!EliminateAllocations) {\n@@ -1103,1 +1370,8 @@\n-  Node* res = alloc->result_cast();\n+\n+  \/\/ Attempt to eliminate inline type buffer allocations\n+  \/\/ regardless of usage and escape\/replaceable status.\n+  bool inline_alloc = tklass->isa_instklassptr() &&\n+                      tklass->is_instklassptr()->instance_klass()->is_inlinetype();\n+  if (!alloc->_is_non_escaping && !inline_alloc) {\n+    return false;\n+  }\n@@ -1106,1 +1380,2 @@\n-  bool boxing_alloc = C->eliminate_boxing() &&\n+  Node* res = alloc->result_cast();\n+  bool boxing_alloc = (res == nullptr) && C->eliminate_boxing() &&\n@@ -1109,1 +1384,1 @@\n-  if (!alloc->_is_scalar_replaceable && (!boxing_alloc || (res != nullptr))) {\n+  if (!alloc->_is_scalar_replaceable && !boxing_alloc && !inline_alloc) {\n@@ -1113,1 +1388,1 @@\n-  alloc->extract_projections(&_callprojs, false \/*separate_io_proj*\/, false \/*do_asserts*\/);\n+  _callprojs = alloc->extract_projections(false \/*separate_io_proj*\/, false \/*do_asserts*\/);\n@@ -1121,1 +1396,1 @@\n-    assert(res == nullptr, \"sanity\");\n+    assert(res == nullptr || inline_alloc, \"sanity\");\n@@ -1146,1 +1421,1 @@\n-  process_users_of_allocation(alloc);\n+  process_users_of_allocation(alloc, inline_alloc);\n@@ -1168,1 +1443,1 @@\n-  boxing->extract_projections(&_callprojs, false \/*separate_io_proj*\/, false \/*do_asserts*\/);\n+  _callprojs = boxing->extract_projections(false \/*separate_io_proj*\/, false \/*do_asserts*\/);\n@@ -1170,1 +1445,1 @@\n-  const TypeTuple* r = boxing->tf()->range();\n+  const TypeTuple* r = boxing->tf()->range_sig();\n@@ -1274,0 +1549,1 @@\n+            Node* init_val, \/\/ value to initialize the array with\n@@ -1356,1 +1632,1 @@\n-    Node *toobig_true = new IfTrueNode( toobig_iff );\n+    Node* toobig_true = new IfTrueNode(toobig_iff);\n@@ -1359,1 +1635,1 @@\n-    toobig_false = new IfFalseNode( toobig_iff );\n+    toobig_false = new IfFalseNode(toobig_iff);\n@@ -1398,0 +1674,1 @@\n+\n@@ -1455,0 +1732,6 @@\n+    if (init_val != nullptr) {\n+      call->init_req(TypeFunc::Parms+2, init_val);\n+    }\n+  } else {\n+    \/\/ Let the runtime know if this is a larval allocation\n+    call->init_req(TypeFunc::Parms+1, _igvn.intcon(alloc->_larval));\n@@ -1486,1 +1769,1 @@\n-  call->extract_projections(&_callprojs, false \/*separate_io_proj*\/, false \/*do_asserts*\/);\n+  _callprojs = call->extract_projections(false \/*separate_io_proj*\/, false \/*do_asserts*\/);\n@@ -1492,2 +1775,2 @@\n-  if (expand_fast_path && _callprojs.fallthrough_memproj != nullptr) {\n-    migrate_outs(_callprojs.fallthrough_memproj, result_phi_rawmem);\n+  if (expand_fast_path && _callprojs->fallthrough_memproj != nullptr) {\n+    _igvn.replace_in_uses(_callprojs->fallthrough_memproj, result_phi_rawmem);\n@@ -1497,4 +1780,4 @@\n-  if (_callprojs.catchall_memproj != nullptr ) {\n-    if (_callprojs.fallthrough_memproj == nullptr) {\n-      _callprojs.fallthrough_memproj = new ProjNode(call, TypeFunc::Memory);\n-      transform_later(_callprojs.fallthrough_memproj);\n+  if (_callprojs->catchall_memproj != nullptr) {\n+    if (_callprojs->fallthrough_memproj == nullptr) {\n+      _callprojs->fallthrough_memproj = new ProjNode(call, TypeFunc::Memory);\n+      transform_later(_callprojs->fallthrough_memproj);\n@@ -1502,2 +1785,2 @@\n-    migrate_outs(_callprojs.catchall_memproj, _callprojs.fallthrough_memproj);\n-    _igvn.remove_dead_node(_callprojs.catchall_memproj);\n+    _igvn.replace_in_uses(_callprojs->catchall_memproj, _callprojs->fallthrough_memproj);\n+    _igvn.remove_dead_node(_callprojs->catchall_memproj);\n@@ -1511,2 +1794,2 @@\n-  if (_callprojs.fallthrough_ioproj != nullptr) {\n-    migrate_outs(_callprojs.fallthrough_ioproj, result_phi_i_o);\n+  if (_callprojs->fallthrough_ioproj != nullptr) {\n+    _igvn.replace_in_uses(_callprojs->fallthrough_ioproj, result_phi_i_o);\n@@ -1516,4 +1799,4 @@\n-  if (_callprojs.catchall_ioproj != nullptr ) {\n-    if (_callprojs.fallthrough_ioproj == nullptr) {\n-      _callprojs.fallthrough_ioproj = new ProjNode(call, TypeFunc::I_O);\n-      transform_later(_callprojs.fallthrough_ioproj);\n+  if (_callprojs->catchall_ioproj != nullptr) {\n+    if (_callprojs->fallthrough_ioproj == nullptr) {\n+      _callprojs->fallthrough_ioproj = new ProjNode(call, TypeFunc::I_O);\n+      transform_later(_callprojs->fallthrough_ioproj);\n@@ -1521,2 +1804,2 @@\n-    migrate_outs(_callprojs.catchall_ioproj, _callprojs.fallthrough_ioproj);\n-    _igvn.remove_dead_node(_callprojs.catchall_ioproj);\n+    _igvn.replace_in_uses(_callprojs->catchall_ioproj, _callprojs->fallthrough_ioproj);\n+    _igvn.remove_dead_node(_callprojs->catchall_ioproj);\n@@ -1541,2 +1824,2 @@\n-  if (_callprojs.fallthrough_catchproj != nullptr) {\n-    ctrl = _callprojs.fallthrough_catchproj->clone();\n+  if (_callprojs->fallthrough_catchproj != nullptr) {\n+    ctrl = _callprojs->fallthrough_catchproj->clone();\n@@ -1544,1 +1827,1 @@\n-    _igvn.replace_node(_callprojs.fallthrough_catchproj, result_region);\n+    _igvn.replace_node(_callprojs->fallthrough_catchproj, result_region);\n@@ -1549,1 +1832,1 @@\n-  if (_callprojs.resproj == nullptr) {\n+  if (_callprojs->resproj[0] == nullptr) {\n@@ -1553,1 +1836,1 @@\n-    slow_result = _callprojs.resproj->clone();\n+    slow_result = _callprojs->resproj[0]->clone();\n@@ -1555,1 +1838,1 @@\n-    _igvn.replace_node(_callprojs.resproj, result_phi_rawoop);\n+    _igvn.replace_node(_callprojs->resproj[0], result_phi_rawoop);\n@@ -1565,1 +1848,1 @@\n-  result_phi_rawmem->init_req(slow_result_path, _callprojs.fallthrough_memproj);\n+  result_phi_rawmem->init_req(slow_result_path, _callprojs->fallthrough_memproj);\n@@ -1577,4 +1860,4 @@\n-  alloc->extract_projections(&_callprojs, false \/*separate_io_proj*\/, false \/*do_asserts*\/);\n-  if (_callprojs.resproj != nullptr) {\n-    for (DUIterator_Fast imax, i = _callprojs.resproj->fast_outs(imax); i < imax; i++) {\n-      Node* use = _callprojs.resproj->fast_out(i);\n+  _callprojs = alloc->extract_projections(false \/*separate_io_proj*\/, false \/*do_asserts*\/);\n+  if (_callprojs->resproj[0] != nullptr) {\n+    for (DUIterator_Fast imax, i = _callprojs->resproj[0]->fast_outs(imax); i < imax; i++) {\n+      Node* use = _callprojs->resproj[0]->fast_out(i);\n@@ -1585,2 +1868,2 @@\n-    assert(_callprojs.resproj->outcnt() == 0, \"all uses must be deleted\");\n-    _igvn.remove_dead_node(_callprojs.resproj);\n+    assert(_callprojs->resproj[0]->outcnt() == 0, \"all uses must be deleted\");\n+    _igvn.remove_dead_node(_callprojs->resproj[0]);\n@@ -1588,3 +1871,3 @@\n-  if (_callprojs.fallthrough_catchproj != nullptr) {\n-    migrate_outs(_callprojs.fallthrough_catchproj, ctrl);\n-    _igvn.remove_dead_node(_callprojs.fallthrough_catchproj);\n+  if (_callprojs->fallthrough_catchproj != nullptr) {\n+    _igvn.replace_in_uses(_callprojs->fallthrough_catchproj, ctrl);\n+    _igvn.remove_dead_node(_callprojs->fallthrough_catchproj);\n@@ -1592,3 +1875,3 @@\n-  if (_callprojs.catchall_catchproj != nullptr) {\n-    _igvn.rehash_node_delayed(_callprojs.catchall_catchproj);\n-    _callprojs.catchall_catchproj->set_req(0, top());\n+  if (_callprojs->catchall_catchproj != nullptr) {\n+    _igvn.rehash_node_delayed(_callprojs->catchall_catchproj);\n+    _callprojs->catchall_catchproj->set_req(0, top());\n@@ -1596,2 +1879,2 @@\n-  if (_callprojs.fallthrough_proj != nullptr) {\n-    Node* catchnode = _callprojs.fallthrough_proj->unique_ctrl_out();\n+  if (_callprojs->fallthrough_proj != nullptr) {\n+    Node* catchnode = _callprojs->fallthrough_proj->unique_ctrl_out();\n@@ -1599,1 +1882,1 @@\n-    _igvn.remove_dead_node(_callprojs.fallthrough_proj);\n+    _igvn.remove_dead_node(_callprojs->fallthrough_proj);\n@@ -1601,3 +1884,3 @@\n-  if (_callprojs.fallthrough_memproj != nullptr) {\n-    migrate_outs(_callprojs.fallthrough_memproj, mem);\n-    _igvn.remove_dead_node(_callprojs.fallthrough_memproj);\n+  if (_callprojs->fallthrough_memproj != nullptr) {\n+    _igvn.replace_in_uses(_callprojs->fallthrough_memproj, mem);\n+    _igvn.remove_dead_node(_callprojs->fallthrough_memproj);\n@@ -1605,3 +1888,3 @@\n-  if (_callprojs.fallthrough_ioproj != nullptr) {\n-    migrate_outs(_callprojs.fallthrough_ioproj, i_o);\n-    _igvn.remove_dead_node(_callprojs.fallthrough_ioproj);\n+  if (_callprojs->fallthrough_ioproj != nullptr) {\n+    _igvn.replace_in_uses(_callprojs->fallthrough_ioproj, i_o);\n+    _igvn.remove_dead_node(_callprojs->fallthrough_ioproj);\n@@ -1609,3 +1892,3 @@\n-  if (_callprojs.catchall_memproj != nullptr) {\n-    _igvn.rehash_node_delayed(_callprojs.catchall_memproj);\n-    _callprojs.catchall_memproj->set_req(0, top());\n+  if (_callprojs->catchall_memproj != nullptr) {\n+    _igvn.rehash_node_delayed(_callprojs->catchall_memproj);\n+    _callprojs->catchall_memproj->set_req(0, top());\n@@ -1613,3 +1896,3 @@\n-  if (_callprojs.catchall_ioproj != nullptr) {\n-    _igvn.rehash_node_delayed(_callprojs.catchall_ioproj);\n-    _callprojs.catchall_ioproj->set_req(0, top());\n+  if (_callprojs->catchall_ioproj != nullptr) {\n+    _igvn.rehash_node_delayed(_callprojs->catchall_ioproj);\n+    _callprojs->catchall_ioproj->set_req(0, top());\n@@ -1749,5 +2032,4 @@\n-Node*\n-PhaseMacroExpand::initialize_object(AllocateNode* alloc,\n-                                    Node* control, Node* rawmem, Node* object,\n-                                    Node* klass_node, Node* length,\n-                                    Node* size_in_bytes) {\n+Node* PhaseMacroExpand::initialize_object(AllocateNode* alloc,\n+                                          Node* control, Node* rawmem, Node* object,\n+                                          Node* klass_node, Node* length,\n+                                          Node* size_in_bytes) {\n@@ -1756,1 +2038,1 @@\n-  Node* mark_node = alloc->make_ideal_mark(&_igvn, object, control, rawmem);\n+  Node* mark_node = alloc->make_ideal_mark(&_igvn, control, rawmem);\n@@ -1794,0 +2076,2 @@\n+                                            alloc->in(AllocateNode::InitValue),\n+                                            alloc->in(AllocateNode::RawInitValue),\n@@ -1968,1 +2252,1 @@\n-  expand_allocate_common(alloc, nullptr,\n+  expand_allocate_common(alloc, nullptr, nullptr,\n@@ -1978,0 +2262,1 @@\n+  Node* init_value = alloc->in(AllocateNode::InitValue);\n@@ -1979,0 +2264,3 @@\n+  assert(!ary_klass_t || !ary_klass_t->klass_is_exact() || !ary_klass_t->exact_klass()->is_obj_array_klass() ||\n+         ary_klass_t->is_refined_type(), \"Must be a refined array klass\");\n+  const TypeFunc* slow_call_type;\n@@ -1985,0 +2273,1 @@\n+    slow_call_type = OptoRuntime::new_array_nozero_Type();\n@@ -1987,0 +2276,7 @@\n+    slow_call_type = OptoRuntime::new_array_Type();\n+\n+    if (init_value == nullptr) {\n+      init_value = _igvn.zerocon(T_OBJECT);\n+    } else if (UseCompressedOops) {\n+      init_value = transform_later(new DecodeNNode(init_value, init_value->bottom_type()->make_ptr()));\n+    }\n@@ -1988,2 +2284,2 @@\n-  expand_allocate_common(alloc, length,\n-                         OptoRuntime::new_array_Type(),\n+  expand_allocate_common(alloc, length, init_value,\n+                         slow_call_type,\n@@ -2201,1 +2497,1 @@\n-  alock->extract_projections(&_callprojs, false \/*separate_io_proj*\/, false \/*do_asserts*\/);\n+  _callprojs = alock->extract_projections(false \/*separate_io_proj*\/, false \/*do_asserts*\/);\n@@ -2205,2 +2501,2 @@\n-         _callprojs.fallthrough_proj != nullptr &&\n-         _callprojs.fallthrough_memproj != nullptr,\n+         _callprojs->fallthrough_proj != nullptr &&\n+         _callprojs->fallthrough_memproj != nullptr,\n@@ -2209,2 +2505,2 @@\n-  Node* fallthroughproj = _callprojs.fallthrough_proj;\n-  Node* memproj_fallthrough = _callprojs.fallthrough_memproj;\n+  Node* fallthroughproj = _callprojs->fallthrough_proj;\n+  Node* memproj_fallthrough = _callprojs->fallthrough_memproj;\n@@ -2281,1 +2577,1 @@\n-  call->extract_projections(&_callprojs, false \/*separate_io_proj*\/, false \/*do_asserts*\/);\n+  _callprojs = call->extract_projections(false \/*separate_io_proj*\/, false \/*do_asserts*\/);\n@@ -2287,2 +2583,2 @@\n-  assert(_callprojs.fallthrough_ioproj == nullptr && _callprojs.catchall_ioproj == nullptr &&\n-         _callprojs.catchall_memproj == nullptr && _callprojs.catchall_catchproj == nullptr, \"Unexpected projection from Lock\");\n+  assert(_callprojs->fallthrough_ioproj == nullptr && _callprojs->catchall_ioproj == nullptr &&\n+         _callprojs->catchall_memproj == nullptr && _callprojs->catchall_catchproj == nullptr, \"Unexpected projection from Lock\");\n@@ -2293,1 +2589,1 @@\n-  Node *slow_ctrl = _callprojs.fallthrough_proj->clone();\n+  Node *slow_ctrl = _callprojs->fallthrough_proj->clone();\n@@ -2295,2 +2591,2 @@\n-  _igvn.hash_delete(_callprojs.fallthrough_proj);\n-  _callprojs.fallthrough_proj->disconnect_inputs(C);\n+  _igvn.hash_delete(_callprojs->fallthrough_proj);\n+  _callprojs->fallthrough_proj->disconnect_inputs(C);\n@@ -2300,1 +2596,1 @@\n-  _igvn.replace_node(_callprojs.fallthrough_proj, region);\n+  _igvn.replace_node(_callprojs->fallthrough_proj, region);\n@@ -2308,1 +2604,1 @@\n-  _igvn.replace_node(_callprojs.fallthrough_memproj, mem_phi);\n+  _igvn.replace_node(_callprojs->fallthrough_memproj, mem_phi);\n@@ -2341,3 +2637,3 @@\n-  call->extract_projections(&_callprojs, false \/*separate_io_proj*\/, false \/*do_asserts*\/);\n-  assert(_callprojs.fallthrough_ioproj == nullptr && _callprojs.catchall_ioproj == nullptr &&\n-         _callprojs.catchall_memproj == nullptr && _callprojs.catchall_catchproj == nullptr, \"Unexpected projection from Lock\");\n+  _callprojs = call->extract_projections(false \/*separate_io_proj*\/, false \/*do_asserts*\/);\n+  assert(_callprojs->fallthrough_ioproj == nullptr && _callprojs->catchall_ioproj == nullptr &&\n+         _callprojs->catchall_memproj == nullptr && _callprojs->catchall_catchproj == nullptr, \"Unexpected projection from Lock\");\n@@ -2349,1 +2645,1 @@\n-  Node *slow_ctrl = _callprojs.fallthrough_proj->clone();\n+  Node *slow_ctrl = _callprojs->fallthrough_proj->clone();\n@@ -2351,2 +2647,2 @@\n-  _igvn.hash_delete(_callprojs.fallthrough_proj);\n-  _callprojs.fallthrough_proj->disconnect_inputs(C);\n+  _igvn.hash_delete(_callprojs->fallthrough_proj);\n+  _callprojs->fallthrough_proj->disconnect_inputs(C);\n@@ -2356,1 +2652,1 @@\n-  _igvn.replace_node(_callprojs.fallthrough_proj, region);\n+  _igvn.replace_node(_callprojs->fallthrough_proj, region);\n@@ -2363,1 +2659,1 @@\n-  _igvn.replace_node(_callprojs.fallthrough_memproj, mem_phi);\n+  _igvn.replace_node(_callprojs->fallthrough_memproj, mem_phi);\n@@ -2366,0 +2662,222 @@\n+\/\/ An inline type might be returned from the call but we don't know its\n+\/\/ type. Either we get a buffered inline type (and nothing needs to be done)\n+\/\/ or one of the values being returned is the klass of the inline type\n+\/\/ and we need to allocate an inline type instance of that type and\n+\/\/ initialize it with other values being returned. In that case, we\n+\/\/ first try a fast path allocation and initialize the value with the\n+\/\/ inline klass's pack handler or we fall back to a runtime call.\n+void PhaseMacroExpand::expand_mh_intrinsic_return(CallStaticJavaNode* call) {\n+  assert(call->method()->is_method_handle_intrinsic(), \"must be a method handle intrinsic call\");\n+  Node* ret = call->proj_out_or_null(TypeFunc::Parms);\n+  if (ret == nullptr) {\n+    return;\n+  }\n+  const TypeFunc* tf = call->_tf;\n+  const TypeTuple* domain = OptoRuntime::store_inline_type_fields_Type()->domain_cc();\n+  const TypeFunc* new_tf = TypeFunc::make(tf->domain_sig(), tf->domain_cc(), tf->range_sig(), domain);\n+  call->_tf = new_tf;\n+  \/\/ Make sure the change of type is applied before projections are processed by igvn\n+  _igvn.set_type(call, call->Value(&_igvn));\n+  _igvn.set_type(ret, ret->Value(&_igvn));\n+\n+  \/\/ Before any new projection is added:\n+  CallProjections* projs = call->extract_projections(true, true);\n+\n+  \/\/ Create temporary hook nodes that will be replaced below.\n+  \/\/ Add an input to prevent hook nodes from being dead.\n+  Node* ctl = new Node(call);\n+  Node* mem = new Node(ctl);\n+  Node* io = new Node(ctl);\n+  Node* ex_ctl = new Node(ctl);\n+  Node* ex_mem = new Node(ctl);\n+  Node* ex_io = new Node(ctl);\n+  Node* res = new Node(ctl);\n+\n+  \/\/ Allocate a new buffered inline type only if a new one is not returned\n+  Node* cast = transform_later(new CastP2XNode(ctl, res));\n+  Node* mask = MakeConX(0x1);\n+  Node* masked = transform_later(new AndXNode(cast, mask));\n+  Node* cmp = transform_later(new CmpXNode(masked, mask));\n+  Node* bol = transform_later(new BoolNode(cmp, BoolTest::eq));\n+  IfNode* allocation_iff = new IfNode(ctl, bol, PROB_MAX, COUNT_UNKNOWN);\n+  transform_later(allocation_iff);\n+  Node* allocation_ctl = transform_later(new IfTrueNode(allocation_iff));\n+  Node* no_allocation_ctl = transform_later(new IfFalseNode(allocation_iff));\n+  Node* no_allocation_res = transform_later(new CheckCastPPNode(no_allocation_ctl, res, TypeInstPtr::BOTTOM));\n+\n+  \/\/ Try to allocate a new buffered inline instance either from TLAB or eden space\n+  Node* needgc_ctrl = nullptr; \/\/ needgc means slowcase, i.e. allocation failed\n+  CallLeafNoFPNode* handler_call;\n+  const bool alloc_in_place = UseTLAB;\n+  if (alloc_in_place) {\n+    Node* fast_oop_ctrl = nullptr;\n+    Node* fast_oop_rawmem = nullptr;\n+    Node* mask2 = MakeConX(-2);\n+    Node* masked2 = transform_later(new AndXNode(cast, mask2));\n+    Node* rawklassptr = transform_later(new CastX2PNode(masked2));\n+    Node* klass_node = transform_later(new CheckCastPPNode(allocation_ctl, rawklassptr, TypeInstKlassPtr::OBJECT_OR_NULL));\n+    Node* layout_val = make_load(nullptr, mem, klass_node, in_bytes(Klass::layout_helper_offset()), TypeInt::INT, T_INT);\n+    Node* size_in_bytes = ConvI2X(layout_val);\n+    BarrierSetC2* bs = BarrierSet::barrier_set()->barrier_set_c2();\n+    Node* fast_oop = bs->obj_allocate(this, mem, allocation_ctl, size_in_bytes, io, needgc_ctrl,\n+                                      fast_oop_ctrl, fast_oop_rawmem,\n+                                      AllocateInstancePrefetchLines);\n+    \/\/ Allocation succeed, initialize buffered inline instance header firstly,\n+    \/\/ and then initialize its fields with an inline class specific handler\n+    Node* mark_word_node;\n+    if (UseCompactObjectHeaders) {\n+      \/\/ COH: We need to load the prototype from the klass at runtime since it encodes the klass pointer already.\n+      mark_word_node = make_load(fast_oop_ctrl, fast_oop_rawmem, klass_node, in_bytes(Klass::prototype_header_offset()), TypeRawPtr::BOTTOM, T_ADDRESS);\n+    } else {\n+      \/\/ Otherwise, use the static prototype.\n+      mark_word_node = makecon(TypeRawPtr::make((address)markWord::inline_type_prototype().value()));\n+    }\n+\n+    fast_oop_rawmem = make_store(fast_oop_ctrl, fast_oop_rawmem, fast_oop, oopDesc::mark_offset_in_bytes(), mark_word_node, T_ADDRESS);\n+    if (!UseCompactObjectHeaders) {\n+      \/\/ COH: Everything is encoded in the mark word, so nothing left to do.\n+      fast_oop_rawmem = make_store(fast_oop_ctrl, fast_oop_rawmem, fast_oop, oopDesc::klass_offset_in_bytes(), klass_node, T_METADATA);\n+      if (UseCompressedClassPointers) {\n+        fast_oop_rawmem = make_store(fast_oop_ctrl, fast_oop_rawmem, fast_oop, oopDesc::klass_gap_offset_in_bytes(), intcon(0), T_INT);\n+      }\n+    }\n+    Node* fixed_block  = make_load(fast_oop_ctrl, fast_oop_rawmem, klass_node, in_bytes(InstanceKlass::adr_inlineklass_fixed_block_offset()), TypeRawPtr::BOTTOM, T_ADDRESS);\n+    Node* pack_handler = make_load(fast_oop_ctrl, fast_oop_rawmem, fixed_block, in_bytes(InlineKlass::pack_handler_offset()), TypeRawPtr::BOTTOM, T_ADDRESS);\n+    handler_call = new CallLeafNoFPNode(OptoRuntime::pack_inline_type_Type(),\n+                                        nullptr,\n+                                        \"pack handler\",\n+                                        TypeRawPtr::BOTTOM);\n+    handler_call->init_req(TypeFunc::Control, fast_oop_ctrl);\n+    handler_call->init_req(TypeFunc::Memory, fast_oop_rawmem);\n+    handler_call->init_req(TypeFunc::I_O, top());\n+    handler_call->init_req(TypeFunc::FramePtr, call->in(TypeFunc::FramePtr));\n+    handler_call->init_req(TypeFunc::ReturnAdr, top());\n+    handler_call->init_req(TypeFunc::Parms, pack_handler);\n+    handler_call->init_req(TypeFunc::Parms+1, fast_oop);\n+  } else {\n+    needgc_ctrl = allocation_ctl;\n+  }\n+\n+  \/\/ Allocation failed, fall back to a runtime call\n+  CallStaticJavaNode* slow_call = new CallStaticJavaNode(OptoRuntime::store_inline_type_fields_Type(),\n+                                                         StubRoutines::store_inline_type_fields_to_buf(),\n+                                                         \"store_inline_type_fields\",\n+                                                         TypePtr::BOTTOM);\n+  slow_call->init_req(TypeFunc::Control, needgc_ctrl);\n+  slow_call->init_req(TypeFunc::Memory, mem);\n+  slow_call->init_req(TypeFunc::I_O, io);\n+  slow_call->init_req(TypeFunc::FramePtr, call->in(TypeFunc::FramePtr));\n+  slow_call->init_req(TypeFunc::ReturnAdr, call->in(TypeFunc::ReturnAdr));\n+  slow_call->init_req(TypeFunc::Parms, res);\n+\n+  Node* slow_ctl = transform_later(new ProjNode(slow_call, TypeFunc::Control));\n+  Node* slow_mem = transform_later(new ProjNode(slow_call, TypeFunc::Memory));\n+  Node* slow_io = transform_later(new ProjNode(slow_call, TypeFunc::I_O));\n+  Node* slow_res = transform_later(new ProjNode(slow_call, TypeFunc::Parms));\n+  Node* slow_catc = transform_later(new CatchNode(slow_ctl, slow_io, 2));\n+  Node* slow_norm = transform_later(new CatchProjNode(slow_catc, CatchProjNode::fall_through_index, CatchProjNode::no_handler_bci));\n+  Node* slow_excp = transform_later(new CatchProjNode(slow_catc, CatchProjNode::catch_all_index,    CatchProjNode::no_handler_bci));\n+\n+  Node* ex_r = new RegionNode(3);\n+  Node* ex_mem_phi = new PhiNode(ex_r, Type::MEMORY, TypePtr::BOTTOM);\n+  Node* ex_io_phi = new PhiNode(ex_r, Type::ABIO);\n+  ex_r->init_req(1, slow_excp);\n+  ex_mem_phi->init_req(1, slow_mem);\n+  ex_io_phi->init_req(1, slow_io);\n+  ex_r->init_req(2, ex_ctl);\n+  ex_mem_phi->init_req(2, ex_mem);\n+  ex_io_phi->init_req(2, ex_io);\n+  transform_later(ex_r);\n+  transform_later(ex_mem_phi);\n+  transform_later(ex_io_phi);\n+\n+  \/\/ We don't know how many values are returned. This assumes the\n+  \/\/ worst case, that all available registers are used.\n+  for (uint i = TypeFunc::Parms+1; i < domain->cnt(); i++) {\n+    if (domain->field_at(i) == Type::HALF) {\n+      slow_call->init_req(i, top());\n+      if (alloc_in_place) {\n+        handler_call->init_req(i+1, top());\n+      }\n+      continue;\n+    }\n+    Node* proj = transform_later(new ProjNode(call, i));\n+    slow_call->init_req(i, proj);\n+    if (alloc_in_place) {\n+      handler_call->init_req(i+1, proj);\n+    }\n+  }\n+  \/\/ We can safepoint at that new call\n+  slow_call->copy_call_debug_info(&_igvn, call);\n+  transform_later(slow_call);\n+  if (alloc_in_place) {\n+    transform_later(handler_call);\n+  }\n+\n+  Node* fast_ctl = nullptr;\n+  Node* fast_res = nullptr;\n+  MergeMemNode* fast_mem = nullptr;\n+  if (alloc_in_place) {\n+    fast_ctl = transform_later(new ProjNode(handler_call, TypeFunc::Control));\n+    Node* rawmem = transform_later(new ProjNode(handler_call, TypeFunc::Memory));\n+    fast_res = transform_later(new ProjNode(handler_call, TypeFunc::Parms));\n+    fast_mem = MergeMemNode::make(mem);\n+    fast_mem->set_memory_at(Compile::AliasIdxRaw, rawmem);\n+    transform_later(fast_mem);\n+  }\n+\n+  Node* r = new RegionNode(alloc_in_place ? 4 : 3);\n+  Node* mem_phi = new PhiNode(r, Type::MEMORY, TypePtr::BOTTOM);\n+  Node* io_phi = new PhiNode(r, Type::ABIO);\n+  Node* res_phi = new PhiNode(r, TypeInstPtr::BOTTOM);\n+  r->init_req(1, no_allocation_ctl);\n+  mem_phi->init_req(1, mem);\n+  io_phi->init_req(1, io);\n+  res_phi->init_req(1, no_allocation_res);\n+  r->init_req(2, slow_norm);\n+  mem_phi->init_req(2, slow_mem);\n+  io_phi->init_req(2, slow_io);\n+  res_phi->init_req(2, slow_res);\n+  if (alloc_in_place) {\n+    r->init_req(3, fast_ctl);\n+    mem_phi->init_req(3, fast_mem);\n+    io_phi->init_req(3, io);\n+    res_phi->init_req(3, fast_res);\n+  }\n+  transform_later(r);\n+  transform_later(mem_phi);\n+  transform_later(io_phi);\n+  transform_later(res_phi);\n+\n+  \/\/ Do not let stores that initialize this buffer be reordered with a subsequent\n+  \/\/ store that would make this buffer accessible by other threads.\n+  MemBarNode* mb = MemBarNode::make(C, Op_MemBarStoreStore, Compile::AliasIdxBot);\n+  transform_later(mb);\n+  mb->init_req(TypeFunc::Memory, mem_phi);\n+  mb->init_req(TypeFunc::Control, r);\n+  r = new ProjNode(mb, TypeFunc::Control);\n+  transform_later(r);\n+  mem_phi = new ProjNode(mb, TypeFunc::Memory);\n+  transform_later(mem_phi);\n+\n+  assert(projs->nb_resproj == 1, \"unexpected number of results\");\n+  _igvn.replace_in_uses(projs->fallthrough_catchproj, r);\n+  _igvn.replace_in_uses(projs->fallthrough_memproj, mem_phi);\n+  _igvn.replace_in_uses(projs->fallthrough_ioproj, io_phi);\n+  _igvn.replace_in_uses(projs->resproj[0], res_phi);\n+  _igvn.replace_in_uses(projs->catchall_catchproj, ex_r);\n+  _igvn.replace_in_uses(projs->catchall_memproj, ex_mem_phi);\n+  _igvn.replace_in_uses(projs->catchall_ioproj, ex_io_phi);\n+  \/\/ The CatchNode should not use the ex_io_phi. Re-connect it to the catchall_ioproj.\n+  Node* cn = projs->fallthrough_catchproj->in(0);\n+  _igvn.replace_input_of(cn, 1, projs->catchall_ioproj);\n+\n+  _igvn.replace_node(ctl, projs->fallthrough_catchproj);\n+  _igvn.replace_node(mem, projs->fallthrough_memproj);\n+  _igvn.replace_node(io, projs->fallthrough_ioproj);\n+  _igvn.replace_node(res, projs->resproj[0]);\n+  _igvn.replace_node(ex_ctl, projs->catchall_catchproj);\n+  _igvn.replace_node(ex_mem, projs->catchall_memproj);\n+  _igvn.replace_node(ex_io, projs->catchall_ioproj);\n+ }\n+\n@@ -2391,1 +2909,1 @@\n-      subklass = _igvn.transform(LoadKlassNode::make(_igvn, C->immutable_memory(), k_adr, TypeInstPtr::KLASS));\n+      subklass = _igvn.transform(LoadKlassNode::make(_igvn, C->immutable_memory(), k_adr, TypeInstPtr::KLASS, TypeInstKlassPtr::OBJECT));\n@@ -2403,0 +2921,113 @@\n+\/\/ FlatArrayCheckNode (array1 array2 ...) is expanded into:\n+\/\/\n+\/\/ long mark = array1.mark | array2.mark | ...;\n+\/\/ long locked_bit = markWord::unlocked_value & array1.mark & array2.mark & ...;\n+\/\/ if (locked_bit == 0) {\n+\/\/   \/\/ One array is locked, load prototype header from the klass\n+\/\/   mark = array1.klass.proto | array2.klass.proto | ...\n+\/\/ }\n+\/\/ if ((mark & markWord::flat_array_bit_in_place) == 0) {\n+\/\/    ...\n+\/\/ }\n+void PhaseMacroExpand::expand_flatarraycheck_node(FlatArrayCheckNode* check) {\n+  bool array_inputs = _igvn.type(check->in(FlatArrayCheckNode::ArrayOrKlass))->isa_oopptr() != nullptr;\n+  if (array_inputs) {\n+    Node* mark = MakeConX(0);\n+    Node* locked_bit = MakeConX(markWord::unlocked_value);\n+    Node* mem = check->in(FlatArrayCheckNode::Memory);\n+    for (uint i = FlatArrayCheckNode::ArrayOrKlass; i < check->req(); ++i) {\n+      Node* ary = check->in(i);\n+      const TypeOopPtr* t = _igvn.type(ary)->isa_oopptr();\n+      assert(t != nullptr, \"Mixing array and klass inputs\");\n+      assert(!t->is_flat() && !t->is_not_flat(), \"Should have been optimized out\");\n+      Node* mark_adr = basic_plus_adr(ary, oopDesc::mark_offset_in_bytes());\n+      Node* mark_load = _igvn.transform(LoadNode::make(_igvn, nullptr, mem, mark_adr, mark_adr->bottom_type()->is_ptr(), TypeX_X, TypeX_X->basic_type(), MemNode::unordered));\n+      mark = _igvn.transform(new OrXNode(mark, mark_load));\n+      locked_bit = _igvn.transform(new AndXNode(locked_bit, mark_load));\n+    }\n+    assert(!mark->is_Con(), \"Should have been optimized out\");\n+    Node* cmp = _igvn.transform(new CmpXNode(locked_bit, MakeConX(0)));\n+    Node* is_unlocked = _igvn.transform(new BoolNode(cmp, BoolTest::ne));\n+\n+    \/\/ BoolNode might be shared, replace each if user\n+    Node* old_bol = check->unique_out();\n+    assert(old_bol->is_Bool() && old_bol->as_Bool()->_test._test == BoolTest::ne, \"unexpected condition\");\n+    for (DUIterator_Last imin, i = old_bol->last_outs(imin); i >= imin; --i) {\n+      IfNode* old_iff = old_bol->last_out(i)->as_If();\n+      Node* ctrl = old_iff->in(0);\n+      RegionNode* region = new RegionNode(3);\n+      Node* mark_phi = new PhiNode(region, TypeX_X);\n+\n+      \/\/ Check if array is unlocked\n+      IfNode* iff = _igvn.transform(new IfNode(ctrl, is_unlocked, PROB_MAX, COUNT_UNKNOWN))->as_If();\n+\n+      \/\/ Unlocked: Use bits from mark word\n+      region->init_req(1, _igvn.transform(new IfTrueNode(iff)));\n+      mark_phi->init_req(1, mark);\n+\n+      \/\/ Locked: Load prototype header from klass\n+      ctrl = _igvn.transform(new IfFalseNode(iff));\n+      Node* proto = MakeConX(0);\n+      for (uint i = FlatArrayCheckNode::ArrayOrKlass; i < check->req(); ++i) {\n+        Node* ary = check->in(i);\n+        \/\/ Make loads control dependent to make sure they are only executed if array is locked\n+        Node* klass_adr = basic_plus_adr(ary, oopDesc::klass_offset_in_bytes());\n+        Node* klass = _igvn.transform(LoadKlassNode::make(_igvn, C->immutable_memory(), klass_adr, TypeInstPtr::KLASS, TypeInstKlassPtr::OBJECT));\n+        Node* proto_adr = basic_plus_adr(klass, in_bytes(Klass::prototype_header_offset()));\n+        Node* proto_load = _igvn.transform(LoadNode::make(_igvn, ctrl, C->immutable_memory(), proto_adr, proto_adr->bottom_type()->is_ptr(), TypeX_X, TypeX_X->basic_type(), MemNode::unordered));\n+        proto = _igvn.transform(new OrXNode(proto, proto_load));\n+      }\n+      region->init_req(2, ctrl);\n+      mark_phi->init_req(2, proto);\n+\n+      \/\/ Check if flat array bits are set\n+      Node* mask = MakeConX(markWord::flat_array_bit_in_place);\n+      Node* masked = _igvn.transform(new AndXNode(_igvn.transform(mark_phi), mask));\n+      cmp = _igvn.transform(new CmpXNode(masked, MakeConX(0)));\n+      Node* is_not_flat = _igvn.transform(new BoolNode(cmp, BoolTest::eq));\n+\n+      ctrl = _igvn.transform(region);\n+      iff = _igvn.transform(new IfNode(ctrl, is_not_flat, PROB_MAX, COUNT_UNKNOWN))->as_If();\n+      _igvn.replace_node(old_iff, iff);\n+    }\n+    _igvn.replace_node(check, C->top());\n+  } else {\n+    \/\/ Fall back to layout helper check\n+    Node* lhs = intcon(0);\n+    for (uint i = FlatArrayCheckNode::ArrayOrKlass; i < check->req(); ++i) {\n+      Node* array_or_klass = check->in(i);\n+      Node* klass = nullptr;\n+      const TypePtr* t = _igvn.type(array_or_klass)->is_ptr();\n+      assert(!t->is_flat() && !t->is_not_flat(), \"Should have been optimized out\");\n+      if (t->isa_oopptr() != nullptr) {\n+        Node* klass_adr = basic_plus_adr(array_or_klass, oopDesc::klass_offset_in_bytes());\n+        klass = transform_later(LoadKlassNode::make(_igvn, C->immutable_memory(), klass_adr, TypeInstPtr::KLASS, TypeInstKlassPtr::OBJECT));\n+      } else {\n+        assert(t->isa_klassptr(), \"Unexpected input type\");\n+        klass = array_or_klass;\n+      }\n+      Node* lh_addr = basic_plus_adr(klass, in_bytes(Klass::layout_helper_offset()));\n+      Node* lh_val = _igvn.transform(LoadNode::make(_igvn, nullptr, C->immutable_memory(), lh_addr, lh_addr->bottom_type()->is_ptr(), TypeInt::INT, T_INT, MemNode::unordered));\n+      lhs = _igvn.transform(new OrINode(lhs, lh_val));\n+    }\n+    Node* masked = transform_later(new AndINode(lhs, intcon(Klass::_lh_array_tag_flat_value_bit_inplace)));\n+    Node* cmp = transform_later(new CmpINode(masked, intcon(0)));\n+    Node* bol = transform_later(new BoolNode(cmp, BoolTest::eq));\n+    Node* m2b = transform_later(new Conv2BNode(masked));\n+    \/\/ The matcher expects the input to If\/CMove nodes to be produced by a Bool(CmpI..)\n+    \/\/ pattern, but the input to other potential users (e.g. Phi) to be some\n+    \/\/ other pattern (e.g. a Conv2B node, possibly idealized as a CMoveI).\n+    Node* old_bol = check->unique_out();\n+    for (DUIterator_Last imin, i = old_bol->last_outs(imin); i >= imin; --i) {\n+      Node* user = old_bol->last_out(i);\n+      for (uint j = 0; j < user->req(); j++) {\n+        Node* n = user->in(j);\n+        if (n == old_bol) {\n+          _igvn.replace_input_of(user, j, (user->is_If() || user->is_CMove()) ? bol : m2b);\n+        }\n+      }\n+    }\n+    _igvn.replace_node(check, C->top());\n+  }\n+}\n+\n@@ -2415,2 +3046,2 @@\n-void PhaseMacroExpand::eliminate_macro_nodes() {\n-  if (C->macro_count() == 0)\n+void PhaseMacroExpand::eliminate_macro_nodes(bool eliminate_locks) {\n+  if (C->macro_count() == 0) {\n@@ -2418,0 +3049,1 @@\n+  }\n@@ -2424,7 +3056,5 @@\n-  \/\/ Before elimination may re-mark (change to Nested or NonEscObj)\n-  \/\/ all associated (same box and obj) lock and unlock nodes.\n-  int cnt = C->macro_count();\n-  for (int i=0; i < cnt; i++) {\n-    Node *n = C->macro_node(i);\n-    if (n->is_AbstractLock()) { \/\/ Lock and Unlock nodes\n-      mark_eliminated_locking_nodes(n->as_AbstractLock());\n+  int iteration = 0;\n+  while (C->macro_count() > 0) {\n+    if (iteration++ > 100) {\n+      assert(false, \"Too slow convergence of macro elimination\");\n+      break;\n@@ -2432,23 +3062,11 @@\n-  }\n-  \/\/ Re-marking may break consistency of Coarsened locks.\n-  if (!C->coarsened_locks_consistent()) {\n-    return; \/\/ recompile without Coarsened locks if broken\n-  } else {\n-    \/\/ After coarsened locks are eliminated locking regions\n-    \/\/ become unbalanced. We should not execute any more\n-    \/\/ locks elimination optimizations on them.\n-    C->mark_unbalanced_boxes();\n-  }\n-  \/\/ First, attempt to eliminate locks\n-  bool progress = true;\n-  while (progress) {\n-    progress = false;\n-    for (int i = C->macro_count(); i > 0; i = MIN2(i - 1, C->macro_count())) { \/\/ more than 1 element can be eliminated at once\n-      Node* n = C->macro_node(i - 1);\n-      bool success = false;\n-      DEBUG_ONLY(int old_macro_count = C->macro_count();)\n-      if (n->is_AbstractLock()) {\n-        success = eliminate_locking_node(n->as_AbstractLock());\n-#ifndef PRODUCT\n-        if (success && PrintOptoStatistics) {\n-          AtomicAccess::inc(&PhaseMacroExpand::_monitor_objects_removed_counter);\n+    \/\/ Postpone lock elimination to after EA when most allocations are eliminated\n+    \/\/ because they might block lock elimination if their escape state isn't\n+    \/\/ determined yet and we only got one chance at eliminating the lock.\n+    if (eliminate_locks) {\n+      \/\/ Before elimination may re-mark (change to Nested or NonEscObj)\n+      \/\/ all associated (same box and obj) lock and unlock nodes.\n+      int cnt = C->macro_count();\n+      for (int i=0; i < cnt; i++) {\n+        Node *n = C->macro_node(i);\n+        if (n->is_AbstractLock()) { \/\/ Lock and Unlock nodes\n+          mark_eliminated_locking_nodes(n->as_AbstractLock());\n@@ -2457,5 +3075,8 @@\n-#endif\n-      assert(success == (C->macro_count() < old_macro_count), \"elimination reduces macro count\");\n-      progress = progress || success;\n-      if (success) {\n-        C->print_method(PHASE_AFTER_MACRO_ELIMINATION_STEP, 5, n);\n+      \/\/ Re-marking may break consistency of Coarsened locks.\n+      if (!C->coarsened_locks_consistent()) {\n+        return; \/\/ recompile without Coarsened locks if broken\n+      } else {\n+        \/\/ After coarsened locks are eliminated locking regions\n+        \/\/ become unbalanced. We should not execute any more\n+        \/\/ locks elimination optimizations on them.\n+        C->mark_unbalanced_boxes();\n@@ -2465,5 +3086,2 @@\n-  }\n-  \/\/ Next, attempt to eliminate allocations\n-  progress = true;\n-  while (progress) {\n-    progress = false;\n+\n+    bool progress = false;\n@@ -2484,2 +3102,5 @@\n-      case Node::Class_CallStaticJava:\n-        success = eliminate_boxing_node(n->as_CallStaticJava());\n+      case Node::Class_CallStaticJava: {\n+        CallStaticJavaNode* call = n->as_CallStaticJava();\n+        if (!call->method()->is_method_handle_intrinsic()) {\n+          success = eliminate_boxing_node(n->as_CallStaticJava());\n+        }\n@@ -2487,0 +3108,1 @@\n+      }\n@@ -2489,1 +3111,8 @@\n-        assert(!n->as_AbstractLock()->is_eliminated(), \"sanity\");\n+        if (eliminate_locks) {\n+          success = eliminate_locking_node(n->as_AbstractLock());\n+#ifndef PRODUCT\n+          if (success && PrintOptoStatistics) {\n+            AtomicAccess::inc(&PhaseMacroExpand::_monitor_objects_removed_counter);\n+          }\n+#endif\n+        }\n@@ -2499,0 +3128,2 @@\n+      case Node::Class_FlatArrayCheck:\n+        break;\n@@ -2516,0 +3147,16 @@\n+\n+    \/\/ Ensure the graph after PhaseMacroExpand::eliminate_macro_nodes is canonical (no igvn\n+    \/\/ transformation is pending). If an allocation is used only in safepoints, elimination of\n+    \/\/ other macro nodes can remove all these safepoints, allowing the allocation to be removed.\n+    \/\/ Hence after igvn we retry removing macro nodes if some progress that has been made in this\n+    \/\/ iteration.\n+    _igvn.set_delay_transform(false);\n+    _igvn.optimize();\n+    if (C->failing()) {\n+      return;\n+    }\n+    _igvn.set_delay_transform(true);\n+\n+    if (!progress) {\n+      break;\n+    }\n@@ -2544,4 +3191,7 @@\n-        \/\/ Remove it from macro list and put on IGVN worklist to optimize.\n-        C->remove_macro_node(n);\n-        _igvn._worklist.push(n);\n-        success = true;\n+        CallStaticJavaNode* call = n->as_CallStaticJava();\n+        if (!call->method()->is_method_handle_intrinsic()) {\n+          \/\/ Remove it from macro list and put on IGVN worklist to optimize.\n+          C->remove_macro_node(n);\n+          _igvn._worklist.push(n);\n+          success = true;\n+        }\n@@ -2656,0 +3306,7 @@\n+    case Node::Class_CallStaticJava:\n+      expand_mh_intrinsic_return(n->as_CallStaticJava());\n+      C->remove_macro_node(n);\n+      break;\n+    case Node::Class_FlatArrayCheck:\n+      expand_flatarraycheck_node(n->as_FlatArrayCheck());\n+      break;\n@@ -2668,1 +3325,1 @@\n-        for (unsigned int i = 0; i < mod_macro->tf()->domain()->cnt() - TypeFunc::Parms; i++) {\n+        for (unsigned int i = 0; i < mod_macro->tf()->domain_cc()->cnt() - TypeFunc::Parms; i++) {\n","filename":"src\/hotspot\/share\/opto\/macro.cpp","additions":969,"deletions":312,"binary":false,"changes":1281,"status":"modified"},{"patch":"@@ -169,0 +169,45 @@\n+\/\/ Array of RegMask, one per returned values (inline type instances can\n+\/\/ be returned as multiple return values, one per field)\n+RegMask* Matcher::return_values_mask(const TypeFunc* tf) {\n+  const TypeTuple* range = tf->range_cc();\n+  uint cnt = range->cnt() - TypeFunc::Parms;\n+  if (cnt == 0) {\n+    return nullptr;\n+  }\n+  RegMask* mask = NEW_RESOURCE_ARRAY(RegMask, cnt);\n+  BasicType* sig_bt = NEW_RESOURCE_ARRAY(BasicType, cnt);\n+  VMRegPair* vm_parm_regs = NEW_RESOURCE_ARRAY(VMRegPair, cnt);\n+  for (uint i = 0; i < cnt; i++) {\n+    sig_bt[i] = range->field_at(i+TypeFunc::Parms)->basic_type();\n+    new (mask + i) RegMask();\n+  }\n+\n+  int regs = SharedRuntime::java_return_convention(sig_bt, vm_parm_regs, cnt);\n+  if (regs <= 0) {\n+    \/\/ We ran out of registers to store the null marker for a nullable inline type return.\n+    \/\/ Since it is only set in the 'call_epilog', we can simply put it on the stack.\n+    assert(tf->returns_inline_type_as_fields(), \"should have been tested during graph construction\");\n+    \/\/ TODO 8284443 Can we teach the register allocator to reserve a stack slot instead?\n+    \/\/ mask[--cnt] = STACK_ONLY_mask does not work (test with -XX:+StressGCM)\n+    int slot = C->fixed_slots() - 2;\n+    if (C->needs_stack_repair()) {\n+      slot -= 2; \/\/ Account for stack increment value\n+    }\n+    mask[--cnt].clear();\n+    mask[cnt].insert(OptoReg::stack2reg(slot));\n+  }\n+  for (uint i = 0; i < cnt; i++) {\n+    mask[i].clear();\n+\n+    OptoReg::Name reg1 = OptoReg::as_OptoReg(vm_parm_regs[i].first());\n+    if (OptoReg::is_valid(reg1)) {\n+      mask[i].insert(reg1);\n+    }\n+    OptoReg::Name reg2 = OptoReg::as_OptoReg(vm_parm_regs[i].second());\n+    if (OptoReg::is_valid(reg2)) {\n+      mask[i].insert(reg2);\n+    }\n+  }\n+\n+  return mask;\n+}\n@@ -189,15 +234,3 @@\n-  \/\/ Map a Java-signature return type into return register-value\n-  \/\/ machine registers for 0, 1 and 2 returned values.\n-  const TypeTuple *range = C->tf()->range();\n-  if( range->cnt() > TypeFunc::Parms ) { \/\/ If not a void function\n-    \/\/ Get ideal-register return type\n-    uint ireg = range->field_at(TypeFunc::Parms)->ideal_reg();\n-    \/\/ Get machine return register\n-    uint sop = C->start()->Opcode();\n-    OptoRegPair regs = return_value(ireg);\n-\n-    \/\/ And mask for same\n-    _return_value_mask.assignFrom(RegMask(regs.first()));\n-    if( OptoReg::is_valid(regs.second()) )\n-      _return_value_mask.insert(regs.second());\n-  }\n+  \/\/ Map Java-signature return types into return register-value\n+  \/\/ machine registers.\n+  _return_values_mask = return_values_mask(C->tf());\n@@ -211,1 +244,1 @@\n-  const TypeTuple *domain = C->tf()->domain();\n+  const TypeTuple *domain = C->tf()->domain_cc();\n@@ -483,0 +516,1 @@\n+\n@@ -703,1 +737,1 @@\n-  uint ret_edge_cnt = TypeFunc::Parms + ((C->tf()->range()->cnt() == TypeFunc::Parms) ? 0 : 1);\n+  uint ret_edge_cnt = C->tf()->range_cc()->cnt();\n@@ -705,4 +739,2 @@\n-  \/\/ Returns have 0 or 1 returned values depending on call signature.\n-  \/\/ Return register is specified by return_value in the AD file.\n-  if (ret_edge_cnt > TypeFunc::Parms) {\n-    ret_rms[TypeFunc::Parms + 0].assignFrom(_return_value_mask);\n+  for (i = TypeFunc::Parms; i < ret_edge_cnt; i++) {\n+    ret_rms[i].assignFrom(_return_values_mask[i-TypeFunc::Parms]);\n@@ -781,1 +813,1 @@\n-  int proj_cnt = C->tf()->domain()->cnt();\n+  int proj_cnt = C->tf()->domain_cc()->cnt();\n@@ -1061,1 +1093,5 @@\n-                m = n->in(0)->as_Multi()->match( n->as_Proj(), this );\n+                RegMask* mask = nullptr;\n+                if (n->in(0)->is_Call() && n->in(0)->as_Call()->tf()->returns_inline_type_as_fields()) {\n+                  mask = return_values_mask(n->in(0)->as_Call()->tf());\n+                }\n+                m = n->in(0)->as_Multi()->match(n->as_Proj(), this, mask);\n@@ -1197,1 +1233,1 @@\n-    domain = call->tf()->domain();\n+    domain = call->tf()->domain_cc();\n@@ -1273,1 +1309,4 @@\n-  int argcnt = cnt - TypeFunc::Parms;\n+  \/\/ Null entry point is a special cast where the target of the call\n+  \/\/ is in a register.\n+  int adj = (call != nullptr && call->entry_point() == nullptr) ? 1 : 0;\n+  int argcnt = cnt - TypeFunc::Parms - adj;\n@@ -1279,1 +1318,1 @@\n-      sig_bt[i] = domain->field_at(i+TypeFunc::Parms)->basic_type();\n+      sig_bt[i] = domain->field_at(i+TypeFunc::Parms+adj)->basic_type();\n@@ -1320,1 +1359,1 @@\n-      RegMask *rm = &mcall->_in_rms[i+TypeFunc::Parms];\n+      RegMask *rm = &mcall->_in_rms[i+TypeFunc::Parms+adj];\n@@ -1338,2 +1377,3 @@\n-      if (OptoReg::is_valid(reg1))\n-        rm->insert(reg1);\n+      if (OptoReg::is_valid(reg1)) {\n+        rm->insert( reg1 );\n+      }\n@@ -1342,2 +1382,3 @@\n-      if (OptoReg::is_valid(reg2))\n-        rm->insert(reg2);\n+      if (OptoReg::is_valid(reg2)) {\n+        rm->insert( reg2 );\n+      }\n@@ -1359,2 +1400,2 @@\n-    uint r_cnt = mcall->tf()->range()->cnt();\n-    MachProjNode* proj = new MachProjNode(mcall, r_cnt + 10000, RegMask::EMPTY, MachProjNode::fat_proj);\n+    uint r_cnt = mcall->tf()->range_sig()->cnt();\n+    MachProjNode *proj = new MachProjNode( mcall, r_cnt+10000, RegMask::EMPTY, MachProjNode::fat_proj );\n@@ -1377,1 +1418,1 @@\n-         (mcall->jvms()->debug_start() + mcall->_jvmadj == mcall->tf()->domain()->cnt()), \"\");\n+         (mcall->jvms()->debug_start() + mcall->_jvmadj == mcall->tf()->domain_cc()->cnt()), \"\");\n@@ -2066,1 +2107,1 @@\n-      for (int i = n->req() - 1; i >= 0; --i) { \/\/ For my children\n+      for (int i = n->len() - 1; i >= 0; --i) { \/\/ For my children\n@@ -2377,0 +2418,7 @@\n+    case Op_ClearArray: {\n+      Node* pair = new BinaryNode(n->in(2), n->in(3));\n+      n->set_req(2, pair);\n+      n->set_req(3, n->in(4));\n+      n->del_req(4);\n+      break;\n+    }\n@@ -2426,0 +2474,8 @@\n+    case Op_StoreLSpecial: {\n+      if (n->req() > (MemNode::ValueIn + 1) && n->in(MemNode::ValueIn + 1) != nullptr) {\n+        Node* pair = new BinaryNode(n->in(MemNode::ValueIn), n->in(MemNode::ValueIn + 1));\n+        n->set_req(MemNode::ValueIn, pair);\n+        n->del_req(MemNode::ValueIn + 1);\n+      }\n+      break;\n+    }\n","filename":"src\/hotspot\/share\/opto\/matcher.cpp","additions":91,"deletions":35,"binary":false,"changes":126,"status":"modified"},{"patch":"@@ -26,0 +26,1 @@\n+#include \"ci\/ciFlatArrayKlass.hpp\"\n@@ -27,0 +28,1 @@\n+#include \"classfile\/systemDictionary.hpp\"\n@@ -33,0 +35,1 @@\n+#include \"oops\/flatArrayKlass.hpp\"\n@@ -40,0 +43,1 @@\n+#include \"opto\/inlinetypenode.hpp\"\n@@ -55,0 +59,1 @@\n+#include \"utilities\/globalDefinitions.hpp\"\n@@ -143,8 +148,105 @@\n-Node *MemNode::optimize_simple_memory_chain(Node *mchain, const TypeOopPtr *t_oop, Node *load, PhaseGVN *phase) {\n-  assert((t_oop != nullptr), \"sanity\");\n-  bool is_instance = t_oop->is_known_instance_field();\n-  bool is_boxed_value_load = t_oop->is_ptr_to_boxed_value() &&\n-                             (load != nullptr) && load->is_Load() &&\n-                             (phase->is_IterGVN() != nullptr);\n-  if (!(is_instance || is_boxed_value_load))\n-    return mchain;  \/\/ don't try to optimize non-instance types\n+\/\/ Find the memory output corresponding to the fall-through path of a call\n+static Node* find_call_fallthrough_mem_output(CallNode* call) {\n+  ResourceMark rm;\n+  CallProjections* projs = call->extract_projections(false, false);\n+  Node* res = projs->fallthrough_memproj;\n+  assert(res != nullptr, \"must have a fallthrough mem output\");\n+  return res;\n+}\n+\n+\/\/ Try to find a better memory input for a load from a strict final field\n+static Node* try_optimize_strict_final_load_memory(PhaseGVN* phase, Node* adr, ProjNode*& base_local) {\n+  intptr_t offset = 0;\n+  Node* base = AddPNode::Ideal_base_and_offset(adr, phase, offset);\n+  if (base == nullptr) {\n+    return nullptr;\n+  }\n+\n+  Node* base_uncasted = base->uncast();\n+  if (base_uncasted->is_Proj()) {\n+    MultiNode* multi = base_uncasted->in(0)->as_Multi();\n+    if (multi->is_Allocate()) {\n+      base_local = base_uncasted->as_Proj();\n+      return nullptr;\n+    } else if (multi->is_Call()) {\n+      \/\/ The oop is returned from a call, the memory can be the fallthrough output of the call\n+      return find_call_fallthrough_mem_output(multi->as_Call());\n+    } else if (multi->is_Start()) {\n+      \/\/ The oop is a parameter\n+      if (phase->C->method()->is_object_constructor() && base_uncasted->as_Proj()->_con == TypeFunc::Parms) {\n+        \/\/ The receiver of a constructor is similar to the result of an AllocateNode\n+        base_local = base_uncasted->as_Proj();\n+        return nullptr;\n+      } else {\n+        \/\/ Use the start memory otherwise\n+        return multi->proj_out(TypeFunc::Memory);\n+      }\n+    }\n+  }\n+\n+  return nullptr;\n+}\n+\n+\/\/ Whether a call can modify a strict final field, given that the object is allocated inside the\n+\/\/ current compilation unit, or is the first parameter when the compilation root is a constructor.\n+\/\/ This is equivalent to asking whether 'call' is a constructor invocation and the class declaring\n+\/\/ the target method is a subclass of the class declaring 'field'.\n+static bool call_can_modify_local_object(ciField* field, CallNode* call) {\n+  if (!call->is_CallJava()) {\n+    return false;\n+  }\n+\n+  ciMethod* target = call->as_CallJava()->method();\n+  if (target == nullptr || !target->is_object_constructor()) {\n+    return false;\n+  }\n+\n+  \/\/ If 'field' is declared in a class that is a subclass of the one declaring the constructor,\n+  \/\/ then the field is set inside the constructor, else the field must be set before the\n+  \/\/ constructor invocation. E.g. A field Super.x will be set during the execution of Sub::<init>,\n+  \/\/ while a field Sub.y must be set before Super::<init> is invoked.\n+  \/\/ We can try to be more heroic and decide if the receiver of the constructor invocation is the\n+  \/\/ object from which we are loading from. This, however, may be problematic as deciding if 2\n+  \/\/ nodes are definitely different may not be trivial, especially if the graph is not canonical.\n+  \/\/ As a result, it is made more conservative for now.\n+  assert(call->req() > TypeFunc::Parms, \"constructor must have at least 1 argument\");\n+  return target->holder()->is_subclass_of(field->holder());\n+}\n+\n+Node* MemNode::optimize_simple_memory_chain(Node* mchain, const TypeOopPtr* t_oop, Node* load, PhaseGVN* phase) {\n+  assert(t_oop != nullptr, \"sanity\");\n+  bool is_known_instance = t_oop->is_known_instance_field();\n+  bool is_strict_final_load = false;\n+\n+  \/\/ After macro expansion, an allocation may become a call, changing the memory input to the\n+  \/\/ memory output of that call would be illegal. As a result, disallow this transformation after\n+  \/\/ macro expansion.\n+  if (phase->is_IterGVN() && phase->C->allow_macro_nodes() && load != nullptr && load->is_Load() && !load->as_Load()->is_mismatched_access()) {\n+    is_strict_final_load = t_oop->is_ptr_to_strict_final_field();\n+#ifdef ASSERT\n+    if ((t_oop->is_inlinetypeptr() && t_oop->inline_klass()->contains_field_offset(t_oop->offset())) || t_oop->is_ptr_to_boxed_value()) {\n+      assert(is_strict_final_load, \"sanity check for basic cases\");\n+    }\n+#endif \/\/ ASSERT\n+  }\n+\n+  if (!is_known_instance && !is_strict_final_load) {\n+    return mchain;\n+  }\n+\n+  Node* result = mchain;\n+  ProjNode* base_local = nullptr;\n+\n+  ciField* field = nullptr;\n+  if (is_strict_final_load) {\n+    field = phase->C->alias_type(t_oop)->field();\n+    assert(field != nullptr, \"must point to a field\");\n+\n+    Node* adr = load->in(MemNode::Address);\n+    assert(phase->type(adr) == t_oop, \"inconsistent type\");\n+    Node* tmp = try_optimize_strict_final_load_memory(phase, adr, base_local);\n+    if (tmp != nullptr) {\n+      result = tmp;\n+    }\n+  }\n+\n@@ -152,3 +254,2 @@\n-  Node *start_mem = phase->C->start()->proj_out_or_null(TypeFunc::Memory);\n-  Node *prev = nullptr;\n-  Node *result = mchain;\n+  Node* start_mem = phase->C->start()->proj_out_or_null(TypeFunc::Memory);\n+  Node* prev = nullptr;\n@@ -157,2 +258,5 @@\n-    if (result == start_mem)\n-      break;  \/\/ hit one of our sentinels\n+    if (result == start_mem) {\n+      \/\/ start_mem is the earliest memory possible\n+      break;\n+    }\n+\n@@ -161,1 +265,1 @@\n-      Node *proj_in = result->in(0);\n+      Node* proj_in = result->in(0);\n@@ -163,1 +267,2 @@\n-        break;  \/\/ hit one of our sentinels\n+        \/\/ This is the allocation that creates the object from which we are loading from\n+        break;\n@@ -166,2 +271,4 @@\n-        CallNode *call = proj_in->as_Call();\n-        if (!call->may_modify(t_oop, phase)) { \/\/ returns false for instances\n+        CallNode* call = proj_in->as_Call();\n+        if (!call->may_modify(t_oop, phase)) {\n+          result = call->in(TypeFunc::Memory);\n+        } else if (is_strict_final_load && base_local != nullptr && !call_can_modify_local_object(field, call)) {\n@@ -177,1 +284,1 @@\n-        if (is_instance) {\n+        if (is_known_instance) {\n@@ -179,1 +286,1 @@\n-        } else if (is_boxed_value_load) {\n+        } else if (is_strict_final_load) {\n@@ -183,1 +290,5 @@\n-            result = proj_in->in(TypeFunc::Memory); \/\/ not related allocation\n+            \/\/ Allocation of another type, must be another object\n+            result = proj_in->in(TypeFunc::Memory);\n+          } else if (base_local != nullptr && (base_local->is_Parm() || base_local->in(0) != alloc)) {\n+            \/\/ Allocation of another object\n+            result = proj_in->in(TypeFunc::Memory);\n@@ -192,0 +303,5 @@\n+      } else if (proj_in->is_LoadFlat() || proj_in->is_StoreFlat()) {\n+        if (is_strict_final_load) {\n+          \/\/ LoadFlat and StoreFlat cannot happen to strict final fields\n+          result = proj_in->in(TypeFunc::Memory);\n+        }\n@@ -195,1 +311,1 @@\n-        assert(false, \"unexpected projection\");\n+        assert(false, \"unexpected projection of %s\", proj_in->Name());\n@@ -198,1 +314,1 @@\n-      if (!is_instance || !ClearArrayNode::step_through(&result, instance_id, phase)) {\n+      if (!is_known_instance || !ClearArrayNode::step_through(&result, instance_id, phase)) {\n@@ -236,0 +352,2 @@\n+                     ->cast_to_not_flat(t_oop->is_aryptr()->is_not_flat())\n+                     ->cast_to_not_null_free(t_oop->is_aryptr()->is_not_null_free())\n@@ -262,1 +380,1 @@\n-               tp->isa_aryptr() &&        tp->offset() == Type::OffsetBot &&\n+        tp->isa_aryptr() &&        tp->offset() == Type::OffsetBot &&\n@@ -974,0 +1092,1 @@\n+  case T_ARRAY:\n@@ -987,1 +1106,1 @@\n-    ShouldNotReachHere();\n+    assert(false, \"unexpected basic type %s\", type2name(bt));\n@@ -1022,1 +1141,1 @@\n-    return (eliminate_boxing && non_volatile) || is_stable_ary;\n+    return (eliminate_boxing && non_volatile) || is_stable_ary || tp->is_inlinetypeptr();\n@@ -1078,2 +1197,1 @@\n-      uint header = arrayOopDesc::base_offset_in_bytes(ary_elem);\n-      uint shift  = exact_log2(type2aelembytes(ary_elem));\n+      uint shift  = ary_t->is_flat() ? ary_t->flat_log_elem_size() : exact_log2(type2aelembytes(ary_elem));\n@@ -1103,0 +1221,10 @@\n+static Node* see_through_inline_type(PhaseValues* phase, const MemNode* load, Node* base, int offset) {\n+  if (!load->is_mismatched_access() && base != nullptr && base->is_InlineType() && offset > oopDesc::klass_offset_in_bytes()) {\n+    InlineTypeNode* vt = base->as_InlineType();\n+    Node* value = vt->field_value_by_offset(offset, true);\n+    assert(value != nullptr, \"must see some value\");\n+    return value;\n+  }\n+\n+  return nullptr;\n+}\n@@ -1115,0 +1243,9 @@\n+  \/\/ Try to see through an InlineTypeNode\n+  \/\/ LoadN is special because the input is not compressed\n+  if (Opcode() != Op_LoadN) {\n+    Node* value = see_through_inline_type(phase, this, ld_base, ld_off);\n+    if (value != nullptr) {\n+      return value;\n+    }\n+  }\n+\n@@ -1198,1 +1335,1 @@\n-        const TypeVect* out_vt = as_LoadVector()->vect_type();\n+        const TypeVect* out_vt = is_Load() ? as_LoadVector()->vect_type() : as_StoreVector()->vect_type();\n@@ -1216,0 +1353,7 @@\n+      Node* init_value = ld_alloc->in(AllocateNode::InitValue);\n+      if (init_value != nullptr) {\n+        \/\/ TODO 8350865 Scalar replacement does not work well for flat arrays.\n+        \/\/ Is this correct for non-all-zero init values? Don't we need field_value_by_offset?\n+        return init_value;\n+      }\n+      assert(ld_alloc->in(AllocateNode::RawInitValue) == nullptr, \"init value may not be null\");\n@@ -1268,1 +1412,1 @@\n-    \/\/ Only instances and boxed values.\n+    \/\/ Only known instances and immutable fields\n@@ -1270,1 +1414,1 @@\n-        (t_oop->is_ptr_to_boxed_value() ||\n+        (t_oop->is_ptr_to_strict_final_field() ||\n@@ -1319,2 +1463,2 @@\n-         addr_t->is_ptr_to_boxed_value()) {\n-      \/\/ Use _idx of address base (could be Phi node) for boxed values.\n+         addr_t->is_ptr_to_strict_final_field()) {\n+      \/\/ Use _idx of address base (could be Phi node) for immutable fields in unknown instances\n@@ -1876,0 +2020,1 @@\n+        && !(phase->type(address)->is_inlinetypeptr() && is_mismatched_access())\n@@ -1972,1 +2117,8 @@\n-  return progress ? this : nullptr;\n+  if (progress) {\n+    return this;\n+  }\n+\n+  if (!can_reshape) {\n+    phase->record_for_igvn(this);\n+  }\n+  return nullptr;\n@@ -2072,0 +2224,1 @@\n+        && !ary->is_flat()\n@@ -2107,0 +2260,2 @@\n+            \/\/ Default value load\n+            tp->is_instptr()->instance_klass() == ciEnv::current()->Class_klass() ||\n@@ -2112,1 +2267,3 @@\n-    \/\/ Optimize loads from constant fields.\n+    BasicType bt = value_basic_type();\n+\n+    \/\/ Optimize loads from constant fields.\n@@ -2116,1 +2273,1 @@\n-      const Type* con_type = Type::make_constant_from_field(const_oop->as_instance(), off, is_unsigned(), value_basic_type());\n+      const Type* con_type = Type::make_constant_from_field(const_oop->as_instance(), off, is_unsigned(), bt);\n@@ -2163,6 +2320,16 @@\n-      if (UseCompactObjectHeaders) {\n-        if (tkls->offset() == in_bytes(Klass::prototype_header_offset())) {\n-          \/\/ The field is Klass::_prototype_header. Return its (constant) value.\n-          assert(this->Opcode() == Op_LoadX, \"must load a proper type from _prototype_header\");\n-          return TypeX::make(klass->prototype_header());\n-        }\n+      if (tkls->offset() == in_bytes(ObjArrayKlass::next_refined_array_klass_offset()) && klass->is_obj_array_klass()) {\n+        \/\/ Fold loads from LibraryCallKit::load_default_refined_array_klass\n+        return tkls->is_aryklassptr()->cast_to_refined_array_klass_ptr();\n+      }\n+      if (klass->is_array_klass() && tkls->offset() == in_bytes(ObjArrayKlass::properties_offset())) {\n+        assert(klass->is_type_array_klass() || tkls->is_aryklassptr()->is_refined_type(), \"Must be a refined array klass pointer\");\n+        return TypeInt::make(klass->as_array_klass()->properties());\n+      }\n+      if (klass->is_flat_array_klass() && tkls->offset() == in_bytes(FlatArrayKlass::layout_kind_offset())) {\n+        assert(Opcode() == Op_LoadI, \"must load an int from _layout_kind\");\n+        return TypeInt::make(static_cast<jint>(klass->as_flat_array_klass()->layout_kind()));\n+      }\n+      if (UseCompactObjectHeaders && tkls->offset() == in_bytes(Klass::prototype_header_offset())) {\n+        \/\/ The field is Klass::_prototype_header. Return its (constant) value.\n+        assert(this->Opcode() == Op_LoadX, \"must load a proper type from _prototype_header\");\n+        return TypeX::make(klass->prototype_header());\n@@ -2239,0 +2406,12 @@\n+      \/\/ TODO 8350865 Scalar replacement does not work well for flat arrays.\n+      \/\/ Escape Analysis assumes that arrays are always zeroed during allocation which is not true for null-free arrays\n+      \/\/ ConnectionGraph::split_unique_types will re-wire the memory of loads from such arrays around the allocation\n+      \/\/ TestArrays::test6 and test152 and TestBasicFunctionality::test20 are affected by this.\n+      if (tp->isa_aryptr() && tp->is_aryptr()->is_flat() && tp->is_aryptr()->is_null_free()) {\n+        intptr_t offset = 0;\n+        Node* base = AddPNode::Ideal_base_and_offset(adr, phase, offset);\n+        AllocateNode* alloc = AllocateNode::Ideal_allocation(base);\n+        if (alloc != nullptr && alloc->is_AllocateArray() && alloc->in(AllocateNode::InitValue) != nullptr) {\n+          return _type;\n+        }\n+      }\n@@ -2242,1 +2421,0 @@\n-\n@@ -2246,1 +2424,10 @@\n-      return TypeX::make(markWord::prototype().value());\n+      if (EnableValhalla) {\n+        \/\/ The mark word may contain property bits (inline, flat, null-free)\n+        Node* klass_node = alloc->in(AllocateNode::KlassNode);\n+        const TypeKlassPtr* tkls = phase->type(klass_node)->isa_klassptr();\n+        if (tkls != nullptr && tkls->is_loaded() && tkls->klass_is_exact()) {\n+          return TypeX::make(tkls->exact_klass()->prototype_header());\n+        }\n+      } else {\n+        return TypeX::make(markWord::prototype().value());\n+      }\n@@ -2395,0 +2582,13 @@\n+Node* LoadNNode::Ideal(PhaseGVN* phase, bool can_reshape) {\n+  \/\/ Loading from an InlineType, find the input and make an EncodeP\n+  Node* addr = in(Address);\n+  intptr_t offset;\n+  Node* base = AddPNode::Ideal_base_and_offset(addr, phase, offset);\n+  Node* value = see_through_inline_type(phase, this, base, offset);\n+  if (value != nullptr) {\n+    return new EncodePNode(value, type());\n+  }\n+\n+  return LoadNode::Ideal(phase, can_reshape);\n+}\n+\n@@ -2468,1 +2668,1 @@\n-  const TypeAryPtr *tary = tp->isa_aryptr();\n+  const TypeAryPtr* tary = tp->isa_aryptr();\n@@ -2471,1 +2671,3 @@\n-    return tary->as_klass_type(true);\n+    const TypeAryKlassPtr* res = tary->as_klass_type(true)->is_aryklassptr();\n+    \/\/ The klass of an array object must be a refined array klass\n+    return res->cast_to_refined_array_klass_ptr();\n@@ -2489,0 +2691,7 @@\n+    if (tkls->isa_aryklassptr() != nullptr && tkls->klass_is_exact() &&\n+        !tkls->exact_klass()->is_type_array_klass() &&\n+        tkls->offset() == in_bytes(Klass::super_offset())) {\n+      \/\/ We are loading the super klass of a refined array klass, return the non-refined klass pointer\n+      assert(tkls->is_aryklassptr()->is_refined_type(), \"Must be a refined array klass pointer\");\n+      return tkls->is_aryklassptr()->cast_to_refined_array_klass_ptr(false);\n+    }\n@@ -2551,0 +2760,4 @@\n+  \/\/\n+  \/\/ This optimization does not apply to arrays because if k is not a\n+  \/\/ constant, it was obtained via load_klass which returns the VM type\n+  \/\/ and '.java_mirror.as_klass' should return the Java type instead.\n@@ -2560,3 +2773,2 @@\n-            && (tkls->isa_instklassptr() || tkls->isa_aryklassptr())\n-            && adr2->is_AddP()\n-           ) {\n+            && ((tkls->isa_instklassptr() && !tkls->is_instklassptr()->might_be_an_array()))\n+            && adr2->is_AddP()) {\n@@ -2707,0 +2919,1 @@\n+  case T_ARRAY:\n@@ -2722,1 +2935,1 @@\n-    ShouldNotReachHere();\n+    assert(false, \"unexpected basic type %s\", type2name(bt));\n@@ -3384,2 +3597,2 @@\n-  \/\/ unsafe if I have intervening uses.\n-  {\n+  \/\/ unsafe if I have intervening uses...\n+  if (phase->C->get_adr_type(phase->C->get_alias_index(adr_type())) != TypeAryPtr::INLINES) {\n@@ -3405,0 +3618,2 @@\n+             (Opcode() == Op_StoreL && st->Opcode() == Op_StoreN) ||\n+             (st->adr_type()->isa_aryptr() && st->adr_type()->is_aryptr()->is_flat()) || \/\/ TODO 8343835\n@@ -3542,2 +3757,1 @@\n-  if (result == this &&\n-      ReduceFieldZeroing && phase->type(val)->is_zero_type()) {\n+  if (result == this && ReduceFieldZeroing) {\n@@ -3545,1 +3759,2 @@\n-    if (mem->is_Proj() && mem->in(0)->is_Allocate()) {\n+    if (mem->is_Proj() && mem->in(0)->is_Allocate() &&\n+        (phase->type(val)->is_zero_type() || mem->in(0)->in(AllocateNode::InitValue) == val)) {\n@@ -3549,1 +3764,1 @@\n-    if (result == this) {\n+    if (result == this && phase->type(val)->is_zero_type()) {\n@@ -4053,1 +4268,1 @@\n-    return new ClearArrayNode(in(0), in(1), in(2), in(3), true);\n+    return new ClearArrayNode(in(0), in(1), in(2), in(3), in(4), true);\n@@ -4071,1 +4286,1 @@\n-  Node *zero = phase->makecon(TypeLong::ZERO);\n+  Node *val = in(4);\n@@ -4073,1 +4288,1 @@\n-  mem = new StoreLNode(in(0),mem,adr,atp,zero,MemNode::unordered,false);\n+  mem = new StoreLNode(in(0), mem, adr, atp, val, MemNode::unordered, false);\n@@ -4078,1 +4293,1 @@\n-    mem = new StoreLNode(in(0),mem,adr,atp,zero,MemNode::unordered,false);\n+    mem = new StoreLNode(in(0), mem, adr, atp, val, MemNode::unordered, false);\n@@ -4112,0 +4327,2 @@\n+                                   Node* val,\n+                                   Node* raw_val,\n@@ -4122,1 +4339,7 @@\n-    mem = StoreNode::make(*phase, ctl, mem, adr, atp, phase->zerocon(T_INT), T_INT, MemNode::unordered);\n+    if (val != nullptr) {\n+      assert(phase->type(val)->isa_narrowoop(), \"should be narrow oop\");\n+      mem = new StoreNNode(ctl, mem, adr, atp, val, MemNode::unordered);\n+    } else {\n+      assert(raw_val == nullptr, \"val may not be null\");\n+      mem = StoreNode::make(*phase, ctl, mem, adr, atp, phase->zerocon(T_INT), T_INT, MemNode::unordered);\n+    }\n@@ -4129,1 +4352,1 @@\n-  return clear_memory(ctl, mem, dest, phase->MakeConX(offset), end_offset, phase);\n+  return clear_memory(ctl, mem, dest, raw_val, phase->MakeConX(offset), end_offset, phase);\n@@ -4133,0 +4356,1 @@\n+                                   Node* raw_val,\n@@ -4155,1 +4379,4 @@\n-  mem = new ClearArrayNode(ctl, mem, zsize, adr, false);\n+  if (raw_val == nullptr) {\n+    raw_val = phase->MakeConX(0);\n+  }\n+  mem = new ClearArrayNode(ctl, mem, zsize, adr, raw_val, false);\n@@ -4160,0 +4387,2 @@\n+                                   Node* val,\n+                                   Node* raw_val,\n@@ -4174,1 +4403,1 @@\n-    mem = clear_memory(ctl, mem, dest,\n+    mem = clear_memory(ctl, mem, dest, val, raw_val,\n@@ -4181,1 +4410,7 @@\n-    mem = StoreNode::make(*phase, ctl, mem, adr, atp, phase->zerocon(T_INT), T_INT, MemNode::unordered);\n+    if (val != nullptr) {\n+      assert(phase->type(val)->isa_narrowoop(), \"should be narrow oop\");\n+      mem = new StoreNNode(ctl, mem, adr, atp, val, MemNode::unordered);\n+    } else {\n+      assert(raw_val == nullptr, \"val may not be null\");\n+      mem = StoreNode::make(*phase, ctl, mem, adr, atp, phase->zerocon(T_INT), T_INT, MemNode::unordered);\n+    }\n@@ -4328,1 +4563,1 @@\n-Node *MemBarNode::match( const ProjNode *proj, const Matcher *m ) {\n+Node *MemBarNode::match(const ProjNode *proj, const Matcher *m, const RegMask* mask) {\n@@ -4615,1 +4850,3 @@\n-  if (init == nullptr || init->is_complete())  return false;\n+  if (init == nullptr || init->is_complete()) {\n+    return false;\n+  }\n@@ -4799,0 +5036,6 @@\n+                if (base->is_Phi()) {\n+                  \/\/ In rare case, base may be a PhiNode and it may read\n+                  \/\/ the same memory slice between InitializeNode and store.\n+                  failed = true;\n+                  break;\n+                }\n@@ -5385,0 +5628,2 @@\n+                                              allocation()->in(AllocateNode::InitValue),\n+                                              allocation()->in(AllocateNode::RawInitValue),\n@@ -5444,0 +5689,2 @@\n+                                            allocation()->in(AllocateNode::InitValue),\n+                                            allocation()->in(AllocateNode::RawInitValue),\n","filename":"src\/hotspot\/share\/opto\/memnode.cpp","additions":313,"deletions":66,"binary":false,"changes":379,"status":"modified"},{"patch":"@@ -129,0 +129,4 @@\n+#ifdef ASSERT\n+  void set_adr_type(const TypePtr* adr_type) { _adr_type = adr_type; }\n+#endif\n+\n@@ -520,0 +524,1 @@\n+  virtual Node* Ideal(PhaseGVN* phase, bool can_reshape);\n@@ -569,1 +574,0 @@\n-\n@@ -723,0 +727,19 @@\n+\/\/ Special StoreL for flat stores that emits GC barriers for field at 'oop_off' in the backend\n+class StoreLSpecialNode : public StoreNode {\n+\n+public:\n+  StoreLSpecialNode(Node* c, Node* mem, Node* adr, const TypePtr* at, Node* val, Node* oop_off, MemOrd mo)\n+    : StoreNode(c, mem, adr, at, val, mo) {\n+    set_mismatched_access();\n+    if (oop_off != nullptr) {\n+      add_req(oop_off);\n+    }\n+  }\n+  virtual int Opcode() const;\n+  virtual BasicType value_basic_type() const { return T_LONG; }\n+\n+  virtual uint match_edge(uint idx) const { return idx == MemNode::Address ||\n+                                                   idx == MemNode::ValueIn ||\n+                                                   idx == MemNode::ValueIn + 1; }\n+};\n+\n@@ -1078,0 +1101,1 @@\n+  bool _word_copy_only;\n@@ -1079,2 +1103,3 @@\n-  ClearArrayNode( Node *ctrl, Node *arymem, Node *word_cnt, Node *base, bool is_large)\n-    : Node(ctrl,arymem,word_cnt,base), _is_large(is_large) {\n+  ClearArrayNode( Node *ctrl, Node *arymem, Node *word_cnt, Node *base, Node* val, bool is_large)\n+    : Node(ctrl, arymem, word_cnt, base, val), _is_large(is_large),\n+      _word_copy_only(val->bottom_type()->isa_long() && (!val->bottom_type()->is_long()->is_con() || val->bottom_type()->is_long()->get_con() != 0)) {\n@@ -1092,0 +1117,1 @@\n+  bool word_copy_only() const { return _word_copy_only; }\n@@ -1103,0 +1129,2 @@\n+                            Node* val,\n+                            Node* raw_val,\n@@ -1107,0 +1135,2 @@\n+                            Node* val,\n+                            Node* raw_val,\n@@ -1111,0 +1141,1 @@\n+                            Node* raw_val,\n@@ -1162,1 +1193,1 @@\n-  virtual Node *match( const ProjNode *proj, const Matcher *m );\n+  virtual Node *match(const ProjNode *proj, const Matcher *m, const RegMask* mask);\n","filename":"src\/hotspot\/share\/opto\/memnode.hpp","additions":35,"deletions":4,"binary":false,"changes":39,"status":"modified"},{"patch":"@@ -201,0 +201,12 @@\n+  \/\/ Code pattern on return from a call that returns an __Value.  Can\n+  \/\/ be optimized away if the return value turns out to be an oop.\n+  if (op == Op_AndX &&\n+      in(1) != nullptr &&\n+      in(1)->Opcode() == Op_CastP2X &&\n+      in(1)->in(1) != nullptr &&\n+      phase->type(in(1)->in(1))->isa_oopptr() &&\n+      t2->isa_intptr_t()->_lo >= 0 &&\n+      t2->isa_intptr_t()->_hi <= MinObjAlignmentInBytesMask) {\n+    return add_id();\n+  }\n+\n@@ -932,0 +944,41 @@\n+  \/\/ Search for GraphKit::mark_word_test patterns and fold the test if the result is statically known\n+  Node* load1 = in(1);\n+  Node* load2 = nullptr;\n+  if (load1->is_Phi() && phase->type(load1)->isa_long()) {\n+    load1 = in(1)->in(1);\n+    load2 = in(1)->in(2);\n+  }\n+  if (load1 != nullptr && load1->is_Load() && phase->type(load1)->isa_long() &&\n+      (load2 == nullptr || (load2->is_Load() && phase->type(load2)->isa_long()))) {\n+    const TypePtr* adr_t1 = phase->type(load1->in(MemNode::Address))->isa_ptr();\n+    const TypePtr* adr_t2 = (load2 != nullptr) ? phase->type(load2->in(MemNode::Address))->isa_ptr() : nullptr;\n+    if (adr_t1 != nullptr && adr_t1->offset() == oopDesc::mark_offset_in_bytes() &&\n+        (load2 == nullptr || (adr_t2 != nullptr && adr_t2->offset() == in_bytes(Klass::prototype_header_offset())))) {\n+      if (mask == markWord::inline_type_pattern) {\n+        if (adr_t1->is_inlinetypeptr()) {\n+          set_req_X(1, in(2), phase);\n+          return this;\n+        } else if (!adr_t1->can_be_inline_type()) {\n+          set_req_X(1, phase->longcon(0), phase);\n+          return this;\n+        }\n+      } else if (mask == markWord::null_free_array_bit_in_place) {\n+        if (adr_t1->is_null_free()) {\n+          set_req_X(1, in(2), phase);\n+          return this;\n+        } else if (adr_t1->is_not_null_free()) {\n+          set_req_X(1, phase->longcon(0), phase);\n+          return this;\n+        }\n+      } else if (mask == markWord::flat_array_bit_in_place) {\n+        if (adr_t1->is_flat()) {\n+          set_req_X(1, in(2), phase);\n+          return this;\n+        } else if (adr_t1->is_not_flat()) {\n+          set_req_X(1, phase->longcon(0), phase);\n+          return this;\n+        }\n+      }\n+    }\n+  }\n+\n","filename":"src\/hotspot\/share\/opto\/mulnode.cpp","additions":53,"deletions":0,"binary":false,"changes":53,"status":"modified"},{"patch":"@@ -42,1 +42,1 @@\n-Node *MultiNode::match( const ProjNode *proj, const Matcher *m ) { return proj->clone(); }\n+Node *MultiNode::match(const ProjNode *proj, const Matcher *m, const RegMask* mask) { return proj->clone(); }\n","filename":"src\/hotspot\/share\/opto\/multnode.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -47,1 +47,1 @@\n-  virtual Node *match( const ProjNode *proj, const Matcher *m );\n+  virtual Node *match(const ProjNode *proj, const Matcher *m, const RegMask* mask);\n","filename":"src\/hotspot\/share\/opto\/multnode.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -36,0 +36,1 @@\n+#include \"opto\/inlinetypenode.hpp\"\n@@ -574,0 +575,6 @@\n+  if (n->is_InlineType()) {\n+    C->add_inline_type(n);\n+  }\n+  if (n->is_LoadFlat() || n->is_StoreFlat()) {\n+    C->add_flat_access(n);\n+  }\n@@ -634,0 +641,3 @@\n+  if (is_InlineType()) {\n+    compile->remove_inline_type(this);\n+  }\n","filename":"src\/hotspot\/share\/opto\/node.cpp","additions":10,"deletions":0,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -90,0 +90,1 @@\n+class FlatArrayCheckNode;\n@@ -122,0 +123,1 @@\n+class MachPrologNode;\n@@ -128,0 +130,1 @@\n+class MachVEPNode;\n@@ -186,0 +189,3 @@\n+class InlineTypeNode;\n+class LoadFlatNode;\n+class StoreFlatNode;\n@@ -692,0 +698,2 @@\n+        DEFINE_CLASS_ID(LoadFlat,  SafePoint, 1)\n+        DEFINE_CLASS_ID(StoreFlat, SafePoint, 2)\n@@ -708,0 +716,1 @@\n+        DEFINE_CLASS_ID(Blackhole,        MemBar, 2)\n@@ -729,0 +738,2 @@\n+      DEFINE_CLASS_ID(MachProlog,       Mach, 8)\n+      DEFINE_CLASS_ID(MachVEP,          Mach, 9)\n@@ -761,1 +772,2 @@\n-      DEFINE_CLASS_ID(Con, Type, 8)\n+      DEFINE_CLASS_ID(InlineType, Type, 8)\n+      DEFINE_CLASS_ID(Con, Type, 9)\n@@ -763,2 +775,2 @@\n-      DEFINE_CLASS_ID(SafePointScalarMerge, Type, 9)\n-      DEFINE_CLASS_ID(Convert, Type, 10)\n+      DEFINE_CLASS_ID(SafePointScalarMerge, Type, 10)\n+      DEFINE_CLASS_ID(Convert, Type, 11)\n@@ -803,3 +815,4 @@\n-        DEFINE_CLASS_ID(FastLock,   Cmp, 0)\n-        DEFINE_CLASS_ID(FastUnlock, Cmp, 1)\n-        DEFINE_CLASS_ID(SubTypeCheck,Cmp, 2)\n+        DEFINE_CLASS_ID(FastLock,       Cmp, 0)\n+        DEFINE_CLASS_ID(FastUnlock,     Cmp, 1)\n+        DEFINE_CLASS_ID(SubTypeCheck,   Cmp, 2)\n+        DEFINE_CLASS_ID(FlatArrayCheck, Cmp, 3)\n@@ -914,0 +927,1 @@\n+  DEFINE_CLASS_QUERY(Blackhole)\n@@ -947,0 +961,1 @@\n+  DEFINE_CLASS_QUERY(FlatArrayCheck)\n@@ -979,0 +994,1 @@\n+  DEFINE_CLASS_QUERY(MachProlog)\n@@ -985,0 +1001,1 @@\n+  DEFINE_CLASS_QUERY(MachVEP)\n@@ -1023,0 +1040,3 @@\n+  DEFINE_CLASS_QUERY(InlineType)\n+  DEFINE_CLASS_QUERY(LoadFlat)\n+  DEFINE_CLASS_QUERY(StoreFlat)\n","filename":"src\/hotspot\/share\/opto\/node.hpp","additions":26,"deletions":6,"binary":false,"changes":32,"status":"modified"},{"patch":"@@ -36,0 +36,1 @@\n+#include \"gc\/shared\/gc_globals.hpp\"\n@@ -232,1 +233,9 @@\n-    _orig_pc_slot = C->fixed_slots() - (sizeof(address) \/ VMRegImpl::stack_slot_size);\n+    int fixed_slots = C->fixed_slots();\n+    if (C->needs_stack_repair()) {\n+      fixed_slots -= 2;\n+    }\n+    \/\/ TODO 8284443 Only reserve extra slot if needed\n+    if (InlineTypeReturnedAsFields) {\n+      fixed_slots -= 2;\n+    }\n+    _orig_pc_slot = fixed_slots - (sizeof(address) \/ VMRegImpl::stack_slot_size);\n@@ -273,1 +282,2 @@\n-  MachPrologNode *prolog = new MachPrologNode();\n+  Label verified_entry;\n+  MachPrologNode* prolog = new MachPrologNode(&verified_entry);\n@@ -279,3 +289,2 @@\n-\n-  if( C->is_osr_compilation() ) {\n-    if( PoisonOSREntry ) {\n+  if (C->is_osr_compilation()) {\n+    if (PoisonOSREntry) {\n@@ -286,3 +295,14 @@\n-    if( C->method() && !C->method()->flags().is_static() ) {\n-      \/\/ Insert unvalidated entry point\n-      C->cfg()->insert( broot, 0, new MachUEPNode() );\n+    if (C->method()) {\n+      if (C->method()->has_scalarized_args()) {\n+        \/\/ Add entry point to unpack all inline type arguments\n+        C->cfg()->insert(broot, 0, new MachVEPNode(&verified_entry, \/* verified *\/ true, \/* receiver_only *\/ false));\n+        if (!C->method()->is_static()) {\n+          \/\/ Add verified\/unverified entry points to only unpack inline type receiver at interface calls\n+          C->cfg()->insert(broot, 0, new MachVEPNode(&verified_entry, \/* verified *\/ false, \/* receiver_only *\/ false));\n+          C->cfg()->insert(broot, 0, new MachVEPNode(&verified_entry, \/* verified *\/ true,  \/* receiver_only *\/ true));\n+          C->cfg()->insert(broot, 0, new MachVEPNode(&verified_entry, \/* verified *\/ false, \/* receiver_only *\/ true));\n+        }\n+      } else if (!C->method()->is_static()) {\n+        \/\/ Insert unvalidated entry point\n+        C->cfg()->insert(broot, 0, new MachUEPNode());\n+      }\n@@ -290,1 +310,0 @@\n-\n@@ -330,0 +349,25 @@\n+  if (!C->is_osr_compilation() && C->has_scalarized_args()) {\n+    \/\/ Compute the offsets of the entry points required by the inline type calling convention\n+    if (!C->method()->is_static()) {\n+      \/\/ We have entries at the beginning of the method, implemented by the first 4 nodes.\n+      \/\/ Entry                     (unverified) @ offset 0\n+      \/\/ Verified_Inline_Entry_RO\n+      \/\/ Inline_Entry              (unverified)\n+      \/\/ Verified_Inline_Entry\n+      uint offset = 0;\n+      _code_offsets.set_value(CodeOffsets::Entry, offset);\n+\n+      offset += ((MachVEPNode*)broot->get_node(0))->size(C->regalloc());\n+      _code_offsets.set_value(CodeOffsets::Verified_Inline_Entry_RO, offset);\n+\n+      offset += ((MachVEPNode*)broot->get_node(1))->size(C->regalloc());\n+      _code_offsets.set_value(CodeOffsets::Inline_Entry, offset);\n+\n+      offset += ((MachVEPNode*)broot->get_node(2))->size(C->regalloc());\n+      _code_offsets.set_value(CodeOffsets::Verified_Inline_Entry, offset);\n+    } else {\n+      _code_offsets.set_value(CodeOffsets::Entry, -1); \/\/ will be patched later\n+      _code_offsets.set_value(CodeOffsets::Verified_Inline_Entry, 0);\n+    }\n+  }\n+\n@@ -491,1 +535,3 @@\n-          mcall->method_set((intptr_t)mcall->entry_point());\n+          if (mcall->entry_point() != nullptr) {\n+            mcall->method_set((intptr_t)mcall->entry_point());\n+          }\n@@ -746,0 +792,29 @@\n+      uint first_ind = spobj->first_index(sfpt->jvms());\n+      \/\/ Nullable, scalarized inline types have a null_marker input\n+      \/\/ that needs to be checked before using the field values.\n+      ScopeValue* properties = nullptr;\n+      if (cik->is_inlinetype()) {\n+        Node* null_marker_node = sfpt->in(first_ind++);\n+        assert(null_marker_node != nullptr, \"null_marker node not found\");\n+        if (!null_marker_node->is_top()) {\n+          const TypeInt* null_marker_type = null_marker_node->bottom_type()->is_int();\n+          if (null_marker_node->is_Con()) {\n+            properties = new ConstantIntValue(null_marker_type->get_con());\n+          } else {\n+            OptoReg::Name null_marker_reg = C->regalloc()->get_reg_first(null_marker_node);\n+            properties = new_loc_value(C->regalloc(), null_marker_reg, Location::normal);\n+          }\n+        }\n+      }\n+      if (cik->is_array_klass() && !cik->is_type_array_klass()) {\n+        jint props = ArrayKlass::ArrayProperties::DEFAULT;\n+        if (cik->as_array_klass()->element_klass()->is_inlinetype()) {\n+          if (cik->as_array_klass()->is_elem_null_free()) {\n+            props |= ArrayKlass::ArrayProperties::NULL_RESTRICTED;\n+          }\n+          if (!cik->as_array_klass()->is_elem_atomic()) {\n+            props |= ArrayKlass::ArrayProperties::NON_ATOMIC;\n+          }\n+        }\n+        properties = new ConstantIntValue(props);\n+      }\n@@ -747,1 +822,1 @@\n-                           new ConstantOopWriteValue(cik->java_mirror()->constant_encoding()));\n+                           new ConstantOopWriteValue(cik->java_mirror()->constant_encoding()), true, properties);\n@@ -750,1 +825,0 @@\n-      uint first_ind = spobj->first_index(sfpt->jvms());\n@@ -996,0 +1070,1 @@\n+  bool return_scalarized = false;\n@@ -1011,1 +1086,1 @@\n-    if (mcall->returns_pointer()) {\n+    if (mcall->returns_pointer() || mcall->returns_scalarized()) {\n@@ -1014,0 +1089,3 @@\n+    if (mcall->returns_scalarized()) {\n+      return_scalarized = true;\n+    }\n@@ -1082,0 +1160,14 @@\n+          assert(!cik->is_inlinetype(), \"Synchronization on value object?\");\n+          ScopeValue* properties = nullptr;\n+          if (cik->is_array_klass() && !cik->is_type_array_klass()) {\n+            jint props = ArrayKlass::ArrayProperties::DEFAULT;\n+            if (cik->as_array_klass()->element_klass()->is_inlinetype()) {\n+              if (cik->as_array_klass()->is_elem_null_free()) {\n+                props |= ArrayKlass::ArrayProperties::NULL_RESTRICTED;\n+              }\n+              if (!cik->as_array_klass()->is_elem_atomic()) {\n+                props |= ArrayKlass::ArrayProperties::NON_ATOMIC;\n+              }\n+            }\n+            properties = new ConstantIntValue(props);\n+          }\n@@ -1083,1 +1175,1 @@\n-                                            new ConstantOopWriteValue(cik->java_mirror()->constant_encoding()));\n+                                            new ConstantOopWriteValue(cik->java_mirror()->constant_encoding()), true, properties);\n@@ -1190,0 +1282,1 @@\n+      return_scalarized,\n@@ -1543,2 +1636,4 @@\n-          \/\/ This destination address is NOT PC-relative\n-          mcall->method_set((intptr_t)mcall->entry_point());\n+          if (mcall->entry_point() != nullptr) {\n+            \/\/ This destination address is NOT PC-relative\n+            mcall->method_set((intptr_t)mcall->entry_point());\n+          }\n@@ -1682,1 +1777,0 @@\n-\n@@ -2937,0 +3031,13 @@\n+\n+      \/\/ Do not allow a CheckCastPP node whose input is a raw pointer to\n+      \/\/ float past a safepoint.  This can occur when a buffered inline\n+      \/\/ type is allocated in a loop and the CheckCastPP from that\n+      \/\/ allocation is reused outside the loop.  If the use inside the\n+      \/\/ loop is scalarized the CheckCastPP will no longer be connected\n+      \/\/ to the loop safepoint.  See JDK-8264340.\n+      if (m->is_Mach() && m->as_Mach()->ideal_Opcode() == Op_CheckCastPP) {\n+        Node *def = m->in(1);\n+        if (def != nullptr && def->bottom_type()->base() == Type::RawPtr) {\n+          last_safept_node->add_prec(m);\n+        }\n+      }\n@@ -3085,0 +3192,19 @@\n+    if (C->has_scalarized_args()) {\n+      \/\/ Inline type entry points (MachVEPNodes) require lots of space for GC barriers and oop verification\n+      \/\/ when loading object fields from the buffered argument. Increase scratch buffer size accordingly.\n+      ciMethod* method = C->method();\n+      int barrier_size = UseZGC ? 200 : (7 DEBUG_ONLY(+ 37));\n+      int arg_num = 0;\n+      if (!method->is_static()) {\n+        if (method->is_scalarized_arg(arg_num)) {\n+          size += method->holder()->as_inline_klass()->oop_count() * barrier_size;\n+        }\n+        arg_num++;\n+      }\n+      for (ciSignatureStream str(method->signature()); !str.at_return_type(); str.next()) {\n+        if (method->is_scalarized_arg(arg_num)) {\n+          size += str.type()->as_inline_klass()->oop_count() * barrier_size;\n+        }\n+        arg_num++;\n+      }\n+    }\n@@ -3155,1 +3281,2 @@\n-  if (is_branch) \/\/ Restore label.\n+  \/\/ Restore label.\n+  if (is_branch) {\n@@ -3157,0 +3284,1 @@\n+  }\n@@ -3197,6 +3325,9 @@\n-      if (!target->is_static()) {\n-        \/\/ The UEP of an nmethod ensures that the VEP is padded. However, the padding of the UEP is placed\n-        \/\/ before the inline cache check, so we don't have to execute any nop instructions when dispatching\n-        \/\/ through the UEP, yet we can ensure that the VEP is aligned appropriately.\n-        _code_offsets.set_value(CodeOffsets::Entry, _first_block_size - MacroAssembler::ic_check_size());\n-      }\n+      if (_code_offsets.value(CodeOffsets::Verified_Inline_Entry) == -1) {\n+        _code_offsets.set_value(CodeOffsets::Verified_Inline_Entry, _first_block_size);\n+      }\n+      if (_code_offsets.value(CodeOffsets::Verified_Inline_Entry_RO) == -1) {\n+        _code_offsets.set_value(CodeOffsets::Verified_Inline_Entry_RO, _first_block_size);\n+      }\n+      if (_code_offsets.value(CodeOffsets::Entry) == -1) {\n+        _code_offsets.set_value(CodeOffsets::Entry, _first_block_size);\n+      }\n@@ -3208,14 +3339,14 @@\n-                                     entry_bci,\n-                                     &_code_offsets,\n-                                     _orig_pc_slot_offset_in_bytes,\n-                                     code_buffer(),\n-                                     frame_size_in_words(),\n-                                     oop_map_set(),\n-                                     &_handler_table,\n-                                     inc_table(),\n-                                     compiler,\n-                                     has_unsafe_access,\n-                                     SharedRuntime::is_wide_vector(C->max_vector_size()),\n-                                     C->has_monitors(),\n-                                     C->has_scoped_access(),\n-                                     0);\n+                              entry_bci,\n+                              &_code_offsets,\n+                              _orig_pc_slot_offset_in_bytes,\n+                              code_buffer(),\n+                              frame_size_in_words(),\n+                              _oop_map_set,\n+                              &_handler_table,\n+                              inc_table(),\n+                              compiler,\n+                              has_unsafe_access,\n+                              SharedRuntime::is_wide_vector(C->max_vector_size()),\n+                              C->has_monitors(),\n+                              C->has_scoped_access(),\n+                              0);\n","filename":"src\/hotspot\/share\/opto\/output.cpp","additions":169,"deletions":38,"binary":false,"changes":207,"status":"modified"},{"patch":"@@ -1184,1 +1184,1 @@\n-  n->dump_bfs(1, nullptr, \"\", &ss);\n+  n->dump_bfs(3, nullptr, \"\", &ss);\n@@ -2077,6 +2077,0 @@\n-  if (_delay_transform) {\n-    \/\/ Register the node but don't optimize for now\n-    register_new_node_with_optimizer(n);\n-    return n;\n-  }\n-\n@@ -2089,0 +2083,6 @@\n+  if (_delay_transform) {\n+    \/\/ Add the node to the worklist but don't optimize for now\n+    _worklist.push(n);\n+    return n;\n+  }\n+\n@@ -2361,0 +2361,13 @@\n+void PhaseIterGVN::replace_in_uses(Node* n, Node* m) {\n+  assert(n != nullptr, \"sanity\");\n+  for (DUIterator_Fast imax, i = n->fast_outs(imax); i < imax; i++) {\n+    Node* u = n->fast_out(i);\n+    if (u != n) {\n+      rehash_node_delayed(u);\n+      int nb = u->replace_edge(n, m);\n+      --i, imax -= nb;\n+    }\n+  }\n+  assert(n->outcnt() == 0, \"all uses must be deleted\");\n+}\n+\n@@ -2416,0 +2429,10 @@\n+  \/\/ AndLNode::Ideal folds GraphKit::mark_word_test patterns. Give it a chance to run.\n+  if (n->is_Load() && use->is_Phi()) {\n+    for (DUIterator_Fast imax, i = use->fast_outs(imax); i < imax; i++) {\n+      Node* u = use->fast_out(i);\n+      if (u->Opcode() == Op_AndL) {\n+        worklist.push(u);\n+      }\n+    }\n+  }\n+\n@@ -2513,0 +2536,9 @@\n+  \/\/ Inline type nodes can have other inline types as users. If an input gets\n+  \/\/ updated, make sure that inline type users get a chance for optimization.\n+  if (use->is_InlineType()) {\n+    for (DUIterator_Fast i2max, i2 = use->fast_outs(i2max); i2 < i2max; i2++) {\n+      Node* u = use->fast_out(i2);\n+      if (u->is_InlineType())\n+        worklist.push(u);\n+    }\n+  }\n@@ -2625,0 +2657,8 @@\n+  if (use_op == Op_CastP2X) {\n+    for (DUIterator_Fast i2max, i2 = use->fast_outs(i2max); i2 < i2max; i2++) {\n+      Node* u = use->fast_out(i2);\n+      if (u->Opcode() == Op_AndX) {\n+        worklist.push(u);\n+      }\n+    }\n+  }\n@@ -2643,0 +2683,10 @@\n+  \/\/ Give CallStaticJavaNode::remove_useless_allocation a chance to run\n+  if (use->is_Region()) {\n+    Node* c = use;\n+    do {\n+      c = c->unique_ctrl_out_or_null();\n+    } while (c != nullptr && c->is_Region());\n+    if (c != nullptr && c->is_CallStaticJava() && c->as_CallStaticJava()->uncommon_trap_request() != 0) {\n+      worklist.push(c);\n+    }\n+  }\n@@ -2762,1 +2812,1 @@\n-    n->dump(1);\n+    n->dump(3);\n@@ -2884,0 +2934,1 @@\n+  push_cast(worklist, use);\n@@ -2995,0 +3046,12 @@\n+  }\n+}\n+\n+void PhaseCCP::push_cast(Unique_Node_List& worklist, const Node* use) {\n+  uint use_op = use->Opcode();\n+  if (use_op == Op_CastP2X) {\n+    for (DUIterator_Fast i2max, i2 = use->fast_outs(i2max); i2 < i2max; i2++) {\n+      Node* u = use->fast_out(i2);\n+      if (u->Opcode() == Op_AndX) {\n+        worklist.push(u);\n+      }\n+    }\n","filename":"src\/hotspot\/share\/opto\/phaseX.cpp","additions":71,"deletions":8,"binary":false,"changes":79,"status":"modified"},{"patch":"@@ -344,4 +344,3 @@\n-  CallProjections projs;\n-  call->extract_projections(&projs, false);\n-  if (projs.fallthrough_catchproj != nullptr) {\n-    C->gvn_replace_by(projs.fallthrough_catchproj, call->in(TypeFunc::Control));\n+  CallProjections* projs = call->extract_projections(false);\n+  if (projs->fallthrough_catchproj != nullptr) {\n+    C->gvn_replace_by(projs->fallthrough_catchproj, call->in(TypeFunc::Control));\n@@ -349,2 +348,2 @@\n-  if (projs.fallthrough_memproj != nullptr) {\n-    C->gvn_replace_by(projs.fallthrough_memproj, call->in(TypeFunc::Memory));\n+  if (projs->fallthrough_memproj != nullptr) {\n+    C->gvn_replace_by(projs->fallthrough_memproj, call->in(TypeFunc::Memory));\n@@ -352,2 +351,2 @@\n-  if (projs.catchall_memproj != nullptr) {\n-    C->gvn_replace_by(projs.catchall_memproj, C->top());\n+  if (projs->catchall_memproj != nullptr) {\n+    C->gvn_replace_by(projs->catchall_memproj, C->top());\n@@ -355,2 +354,2 @@\n-  if (projs.fallthrough_ioproj != nullptr) {\n-    C->gvn_replace_by(projs.fallthrough_ioproj, call->in(TypeFunc::I_O));\n+  if (projs->fallthrough_ioproj != nullptr) {\n+    C->gvn_replace_by(projs->fallthrough_ioproj, call->in(TypeFunc::I_O));\n@@ -358,2 +357,2 @@\n-  if (projs.catchall_ioproj != nullptr) {\n-    C->gvn_replace_by(projs.catchall_ioproj, C->top());\n+  if (projs->catchall_ioproj != nullptr) {\n+    C->gvn_replace_by(projs->catchall_ioproj, C->top());\n@@ -361,1 +360,1 @@\n-  if (projs.catchall_catchproj != nullptr) {\n+  if (projs->catchall_catchproj != nullptr) {\n@@ -364,1 +363,1 @@\n-    for (SimpleDUIterator i(projs.catchall_catchproj); i.has_next(); i.next()) {\n+    for (SimpleDUIterator i(projs->catchall_catchproj); i.has_next(); i.next()) {\n@@ -371,1 +370,1 @@\n-    C->gvn_replace_by(projs.catchall_catchproj, C->top());\n+    C->gvn_replace_by(projs->catchall_catchproj, C->top());\n@@ -373,2 +372,3 @@\n-  if (projs.resproj != nullptr) {\n-    C->gvn_replace_by(projs.resproj, C->top());\n+  if (projs->resproj[0] != nullptr) {\n+    assert(projs->nb_resproj == 1, \"unexpected number of results\");\n+    C->gvn_replace_by(projs->resproj[0], C->top());\n@@ -1155,0 +1155,3 @@\n+        if (opc == Op_CheckCastPP) {\n+          worklist.push(use);\n+        }\n","filename":"src\/hotspot\/share\/opto\/stringopts.cpp","additions":20,"deletions":17,"binary":false,"changes":37,"status":"modified"},{"patch":"@@ -32,0 +32,1 @@\n+#include \"opto\/inlinetypenode.hpp\"\n@@ -923,1 +924,8 @@\n-Node *CmpLNode::Ideal( PhaseGVN *phase, bool can_reshape ) {\n+\/\/------------------------------Ideal------------------------------------------\n+Node* CmpLNode::Ideal(PhaseGVN* phase, bool can_reshape) {\n+  Node* a = nullptr;\n+  Node* b = nullptr;\n+  if (is_double_null_check(phase, a, b) && (phase->type(a)->is_zero_type() || phase->type(b)->is_zero_type())) {\n+    \/\/ Degraded to a simple null check, use old acmp\n+    return new CmpPNode(a, b);\n+  }\n@@ -934,0 +942,25 @@\n+\/\/ Match double null check emitted by Compile::optimize_acmp()\n+bool CmpLNode::is_double_null_check(PhaseGVN* phase, Node*& a, Node*& b) const {\n+  if (in(1)->Opcode() == Op_OrL &&\n+      in(1)->in(1)->Opcode() == Op_CastP2X &&\n+      in(1)->in(2)->Opcode() == Op_CastP2X &&\n+      in(2)->bottom_type()->is_zero_type()) {\n+    assert(EnableValhalla, \"unexpected double null check\");\n+    a = in(1)->in(1)->in(1);\n+    b = in(1)->in(2)->in(1);\n+    return true;\n+  }\n+  return false;\n+}\n+\n+\/\/------------------------------Value------------------------------------------\n+const Type* CmpLNode::Value(PhaseGVN* phase) const {\n+  Node* a = nullptr;\n+  Node* b = nullptr;\n+  if (is_double_null_check(phase, a, b) && (!phase->type(a)->maybe_null() || !phase->type(b)->maybe_null())) {\n+    \/\/ One operand is never nullptr, emit constant false\n+    return TypeInt::CC_GT;\n+  }\n+  return SubNode::Value(phase);\n+}\n+\n@@ -1070,1 +1103,16 @@\n-\n+    if (!unrelated_classes) {\n+      \/\/ Handle inline type arrays\n+      if ((r0->flat_in_array() && r1->not_flat_in_array()) ||\n+          (r1->flat_in_array() && r0->not_flat_in_array())) {\n+        \/\/ One type is in flat arrays but the other type is not. Must be unrelated.\n+        unrelated_classes = true;\n+      } else if ((r0->is_not_flat() && r1->is_flat()) ||\n+                 (r1->is_not_flat() && r0->is_flat())) {\n+        \/\/ One type is a non-flat array and the other type is a flat array. Must be unrelated.\n+        unrelated_classes = true;\n+      } else if ((r0->is_not_null_free() && r1->is_null_free()) ||\n+                 (r1->is_not_null_free() && r0->is_null_free())) {\n+        \/\/ One type is a nullable array and the other type is a null-free array. Must be unrelated.\n+        unrelated_classes = true;\n+      }\n+    }\n@@ -1097,1 +1145,1 @@\n-static inline Node* isa_java_mirror_load(PhaseGVN* phase, Node* n) {\n+static inline Node* isa_java_mirror_load(PhaseGVN* phase, Node* n, bool& might_be_an_array) {\n@@ -1119,0 +1167,1 @@\n+  might_be_an_array |= tkp->isa_aryklassptr() || tkp->is_instklassptr()->might_be_an_array();\n@@ -1124,1 +1173,1 @@\n-static inline Node* isa_const_java_mirror(PhaseGVN* phase, Node* n) {\n+static inline Node* isa_const_java_mirror(PhaseGVN* phase, Node* n, bool& might_be_an_array) {\n@@ -1144,2 +1193,11 @@\n-  assert(mirror_type->is_klass(), \"mirror_type should represent a Klass*\");\n-  return phase->makecon(TypeKlassPtr::make(mirror_type->as_klass(), Type::trust_interfaces));\n+  ciKlass* mirror_klass = mirror_type->as_klass();\n+\n+  if (mirror_klass->is_array_klass()) {\n+    if (!mirror_klass->can_be_inline_array_klass()) {\n+      \/\/ Special case for non-value arrays: They only have one (default) refined class, use it\n+      return phase->makecon(TypeAryKlassPtr::make(mirror_klass, Type::trust_interfaces, true));\n+    }\n+    might_be_an_array |= true;\n+  }\n+\n+  return phase->makecon(TypeKlassPtr::make(mirror_klass, Type::trust_interfaces));\n@@ -1155,1 +1213,8 @@\n-Node *CmpPNode::Ideal( PhaseGVN *phase, bool can_reshape ) {\n+Node* CmpPNode::Ideal(PhaseGVN *phase, bool can_reshape) {\n+  \/\/ TODO 8284443 in(1) could be cast?\n+  if (in(1)->is_InlineType() && phase->type(in(2))->is_zero_type()) {\n+    \/\/ Null checking a scalarized but nullable inline type. Check the null marker\n+    \/\/ input instead of the oop input to avoid keeping buffer allocations alive.\n+    return new CmpINode(in(1)->as_InlineType()->get_null_marker(), phase->intcon(0));\n+  }\n+\n@@ -1169,3 +1234,10 @@\n-    Node* k1 = isa_java_mirror_load(phase, in(1));\n-    Node* k2 = isa_java_mirror_load(phase, in(2));\n-    Node* conk2 = isa_const_java_mirror(phase, in(2));\n+    bool might_be_an_array1 = false;\n+    bool might_be_an_array2 = false;\n+    Node* k1 = isa_java_mirror_load(phase, in(1), might_be_an_array1);\n+    Node* k2 = isa_java_mirror_load(phase, in(2), might_be_an_array2);\n+    Node* conk2 = isa_const_java_mirror(phase, in(2), might_be_an_array2);\n+    if (might_be_an_array1 && might_be_an_array2) {\n+      \/\/ Don't optimize if both sides might be an array because arrays with\n+      \/\/ the same Java mirror can have different refined array klasses.\n+      k1 = k2 = nullptr;\n+    }\n@@ -1250,0 +1322,14 @@\n+  \/\/ Do not fold the subtype check to an array klass pointer comparison for\n+  \/\/ value class arrays because they can have multiple refined array klasses.\n+  superklass = t2->exact_klass();\n+  assert(!superklass->is_flat_array_klass(), \"Unexpected flat array klass\");\n+  if (superklass->is_obj_array_klass()) {\n+    if (!superklass->as_array_klass()->is_elem_null_free() &&\n+         superklass->as_array_klass()->element_klass()->is_inlinetype()) {\n+      return nullptr;\n+    } else {\n+      \/\/ Special case for non-value arrays: They only have one (default) refined class, use it\n+      set_req_X(2, phase->makecon(t2->is_aryklassptr()->cast_to_refined_array_klass_ptr()), phase);\n+    }\n+  }\n+\n@@ -1370,0 +1456,37 @@\n+\/\/=============================================================================\n+\/\/------------------------------Value------------------------------------------\n+const Type* FlatArrayCheckNode::Value(PhaseGVN* phase) const {\n+  bool all_not_flat = true;\n+  for (uint i = ArrayOrKlass; i < req(); ++i) {\n+    const Type* t = phase->type(in(i));\n+    if (t == Type::TOP) {\n+      return Type::TOP;\n+    }\n+    if (t->is_ptr()->is_flat()) {\n+      \/\/ One of the input arrays is flat, check always passes\n+      return TypeInt::CC_EQ;\n+    } else if (!t->is_ptr()->is_not_flat()) {\n+      \/\/ One of the input arrays might be flat\n+      all_not_flat = false;\n+    }\n+  }\n+  if (all_not_flat) {\n+    \/\/ None of the input arrays can be flat, check always fails\n+    return TypeInt::CC_GT;\n+  }\n+  return TypeInt::CC;\n+}\n+\n+\/\/------------------------------Ideal------------------------------------------\n+Node* FlatArrayCheckNode::Ideal(PhaseGVN* phase, bool can_reshape) {\n+  bool changed = false;\n+  \/\/ Remove inputs that are known to be non-flat\n+  for (uint i = ArrayOrKlass; i < req(); ++i) {\n+    const Type* t = phase->type(in(i));\n+    if (t->isa_ptr() && t->is_ptr()->is_not_flat()) {\n+      del_req(i--);\n+      changed = true;\n+    }\n+  }\n+  return changed ? this : nullptr;\n+}\n","filename":"src\/hotspot\/share\/opto\/subnode.cpp","additions":133,"deletions":10,"binary":false,"changes":143,"status":"modified"},{"patch":"@@ -220,1 +220,2 @@\n-  virtual Node *Ideal(PhaseGVN *phase, bool can_reshape);\n+  virtual Node* Ideal(PhaseGVN* phase, bool can_reshape);\n+  virtual const Type* Value(PhaseGVN* phase) const;\n@@ -222,0 +223,1 @@\n+  bool is_double_null_check(PhaseGVN* phase, Node*& a, Node*& b) const;\n@@ -312,0 +314,20 @@\n+\/\/--------------------------FlatArrayCheckNode---------------------------------\n+\/\/ Returns true if one of the input array objects or array klass ptrs (there\n+\/\/ can be multiple) is flat.\n+class FlatArrayCheckNode : public CmpNode {\n+public:\n+  enum {\n+    Control,\n+    Memory,\n+    ArrayOrKlass\n+  };\n+  FlatArrayCheckNode(Compile* C, Node* mem, Node* array_or_klass) : CmpNode(mem, array_or_klass) {\n+    init_class_id(Class_FlatArrayCheck);\n+    init_flags(Flag_is_macro);\n+    C->add_macro_node(this);\n+  }\n+  virtual int Opcode() const;\n+  virtual const Type* sub(const Type*, const Type*) const { ShouldNotReachHere(); return nullptr; }\n+  const Type* Value(PhaseGVN* phase) const;\n+  virtual Node* Ideal(PhaseGVN* phase, bool can_reshape);\n+};\n","filename":"src\/hotspot\/share\/opto\/subnode.hpp","additions":23,"deletions":1,"binary":false,"changes":24,"status":"modified"},{"patch":"@@ -132,13 +132,16 @@\n-  IS_METHOD            = java_lang_invoke_MemberName::MN_IS_METHOD,\n-  IS_CONSTRUCTOR       = java_lang_invoke_MemberName::MN_IS_CONSTRUCTOR,\n-  IS_FIELD             = java_lang_invoke_MemberName::MN_IS_FIELD,\n-  IS_TYPE              = java_lang_invoke_MemberName::MN_IS_TYPE,\n-  CALLER_SENSITIVE     = java_lang_invoke_MemberName::MN_CALLER_SENSITIVE,\n-  TRUSTED_FINAL        = java_lang_invoke_MemberName::MN_TRUSTED_FINAL,\n-  HIDDEN_MEMBER        = java_lang_invoke_MemberName::MN_HIDDEN_MEMBER,\n-  REFERENCE_KIND_SHIFT = java_lang_invoke_MemberName::MN_REFERENCE_KIND_SHIFT,\n-  REFERENCE_KIND_MASK  = java_lang_invoke_MemberName::MN_REFERENCE_KIND_MASK,\n-  LM_UNCONDITIONAL     = java_lang_invoke_MemberName::MN_UNCONDITIONAL_MODE,\n-  LM_MODULE            = java_lang_invoke_MemberName::MN_MODULE_MODE,\n-  LM_TRUSTED           = java_lang_invoke_MemberName::MN_TRUSTED_MODE,\n-  ALL_KINDS      = IS_METHOD | IS_CONSTRUCTOR | IS_FIELD | IS_TYPE\n+  IS_METHOD             = java_lang_invoke_MemberName::MN_IS_METHOD,\n+  IS_OBJECT_CONSTRUCTOR = java_lang_invoke_MemberName::MN_IS_OBJECT_CONSTRUCTOR,\n+  IS_FIELD              = java_lang_invoke_MemberName::MN_IS_FIELD,\n+  IS_TYPE               = java_lang_invoke_MemberName::MN_IS_TYPE,\n+  CALLER_SENSITIVE      = java_lang_invoke_MemberName::MN_CALLER_SENSITIVE,\n+  TRUSTED_FINAL         = java_lang_invoke_MemberName::MN_TRUSTED_FINAL,\n+  HIDDEN_MEMBER         = java_lang_invoke_MemberName::MN_HIDDEN_MEMBER,\n+  NULL_RESTRICTED       = java_lang_invoke_MemberName::MN_NULL_RESTRICTED_FIELD,\n+  REFERENCE_KIND_SHIFT  = java_lang_invoke_MemberName::MN_REFERENCE_KIND_SHIFT,\n+  REFERENCE_KIND_MASK   = java_lang_invoke_MemberName::MN_REFERENCE_KIND_MASK,\n+  LAYOUT_SHIFT          = java_lang_invoke_MemberName::MN_LAYOUT_SHIFT,\n+  LAYOUT_MASK           = java_lang_invoke_MemberName::MN_LAYOUT_MASK,\n+  LM_UNCONDITIONAL      = java_lang_invoke_MemberName::MN_UNCONDITIONAL_MODE,\n+  LM_MODULE             = java_lang_invoke_MemberName::MN_MODULE_MODE,\n+  LM_TRUSTED            = java_lang_invoke_MemberName::MN_TRUSTED_MODE,\n+  ALL_KINDS      = IS_METHOD | IS_OBJECT_CONSTRUCTOR | IS_FIELD | IS_TYPE\n@@ -155,1 +158,1 @@\n-    flags |= IS_CONSTRUCTOR;\n+    flags |= IS_OBJECT_CONSTRUCTOR;\n@@ -174,1 +177,1 @@\n-    case IS_CONSTRUCTOR:\n+    case IS_OBJECT_CONSTRUCTOR:\n@@ -315,1 +318,1 @@\n-      assert(!m->is_static_initializer(), \"Cannot be static initializer\");\n+      assert(!m->is_class_initializer(), \"Cannot be static initializer\");\n@@ -317,2 +320,2 @@\n-    } else if (m->is_object_initializer()) {\n-      flags |= IS_CONSTRUCTOR | (JVM_REF_invokeSpecial << REFERENCE_KIND_SHIFT);\n+    } else if (m->is_object_constructor()) {\n+      flags |= IS_OBJECT_CONSTRUCTOR | (JVM_REF_invokeSpecial << REFERENCE_KIND_SHIFT);\n@@ -357,0 +360,6 @@\n+  if (fd.is_flat()) {\n+    int layout_kind = (int)fd.layout_kind();\n+    assert((layout_kind & LAYOUT_MASK) == layout_kind, \"Layout information loss\");\n+    flags |= layout_kind << LAYOUT_SHIFT;\n+  }\n+  if (fd.is_null_free_inline_type()) flags |= NULL_RESTRICTED;\n@@ -807,1 +816,1 @@\n-  case IS_CONSTRUCTOR:\n+  case IS_OBJECT_CONSTRUCTOR:\n@@ -813,1 +822,1 @@\n-        if (name == vmSymbols::object_initializer_name()) {\n+        if (name == vmSymbols::object_initializer_name() && type->is_void_method_signature()) {\n@@ -876,1 +885,1 @@\n-  case IS_CONSTRUCTOR:\n+  case IS_OBJECT_CONSTRUCTOR:\n@@ -989,1 +998,1 @@\n-    template(java_lang_invoke_MemberName,MN_IS_CONSTRUCTOR) \\\n+    template(java_lang_invoke_MemberName,MN_IS_OBJECT_CONSTRUCTOR) \\\n@@ -997,0 +1006,2 @@\n+    template(java_lang_invoke_MemberName,MN_LAYOUT_SHIFT) \\\n+    template(java_lang_invoke_MemberName,MN_LAYOUT_MASK) \\\n@@ -1132,1 +1143,1 @@\n-               (flags & ALL_KINDS) == IS_CONSTRUCTOR) {\n+               (flags & ALL_KINDS) == IS_OBJECT_CONSTRUCTOR) {\n","filename":"src\/hotspot\/share\/prims\/methodHandles.cpp","additions":34,"deletions":23,"binary":false,"changes":57,"status":"modified"},{"patch":"@@ -79,0 +79,2 @@\n+#include <string.h>\n+\n@@ -365,0 +367,12 @@\n+bool Arguments::patching_migrated_classes(const char* property, const char* value) {\n+  if (strncmp(property, MODULE_PROPERTY_PREFIX, MODULE_PROPERTY_PREFIX_LEN) == 0) {\n+    const char* property_suffix = property + MODULE_PROPERTY_PREFIX_LEN;\n+    if (matches_property_suffix(property_suffix, PATCH, PATCH_LEN)) {\n+      if (strcmp(value, \"java.base-valueclasses.jar\")) {\n+        return true;\n+      }\n+    }\n+  }\n+  return false;\n+}\n+\n@@ -1807,1 +1821,0 @@\n-static unsigned int patch_mod_count = 0;\n@@ -1814,1 +1827,1 @@\n-  if (!CDSConfig::check_vm_args_consistency(patch_mod_javabase, mode_flag_cmd_line)) {\n+  if (!CDSConfig::check_vm_args_consistency(mode_flag_cmd_line)) {\n@@ -1981,0 +1994,4 @@\n+  if (UseAltSubstitutabilityMethod) {\n+    no_shared_spaces(\"Alternate substitutability method doesn't work with CDS yet\");\n+  }\n+\n@@ -2063,1 +2080,1 @@\n-      add_patch_mod_prefix(module_name, module_equal + 1);\n+      add_patch_mod_prefix(module_name, module_equal + 1, false \/* no append *\/, false \/* no cds *\/);\n@@ -2065,3 +2082,0 @@\n-      if (!create_numbered_module_property(\"jdk.module.patch\", patch_mod_tail, patch_mod_count++)) {\n-        return JNI_ENOMEM;\n-      }\n@@ -2075,0 +2089,82 @@\n+\/\/ Temporary system property to disable preview patching and enable the new preview mode\n+\/\/ feature for testing\/development. Once the preview mode feature is finished, the value\n+\/\/ will be always 'true' and this code, and all related dead-code can be removed.\n+#define DISABLE_PREVIEW_PATCHING_DEFAULT false\n+\n+bool Arguments::disable_preview_patching() {\n+  const char* prop = get_property(\"DISABLE_PREVIEW_PATCHING\");\n+  return (prop != nullptr)\n+      ? strncmp(prop, \"true\", strlen(\"true\")) == 0\n+      : DISABLE_PREVIEW_PATCHING_DEFAULT;\n+}\n+\n+\/\/ VALUECLASS_STR must match string used in the build\n+#define VALUECLASS_STR \"valueclasses\"\n+#define VALUECLASS_JAR \"-\" VALUECLASS_STR \".jar\"\n+\n+\/\/ Finalize --patch-module args and --enable-preview related to value class module patches.\n+\/\/ Create all numbered properties passing module patches.\n+int Arguments::finalize_patch_module() {\n+  \/\/ If --enable-preview and EnableValhalla is true, modules may have preview mode resources.\n+  bool enable_valhalla_preview = enable_preview() && EnableValhalla;\n+  \/\/ Whether to use module patching, or the new preview mode feature for preview resources.\n+  bool disable_patching = disable_preview_patching();\n+\n+  \/\/ This must be called, even with 'false', to enable resource lookup from JImage.\n+  ClassLoader::init_jimage(disable_patching && enable_valhalla_preview);\n+\n+  \/\/ For each <module>-valueclasses.jar in <JAVA_HOME>\/lib\/valueclasses\/\n+  \/\/ appends the equivalent of --patch-module <module>=<JAVA_HOME>\/lib\/valueclasses\/<module>-valueclasses.jar\n+  if (!disable_patching && enable_valhalla_preview) {\n+    char * valueclasses_dir = AllocateHeap(JVM_MAXPATHLEN, mtArguments);\n+    const char * fileSep = os::file_separator();\n+\n+    jio_snprintf(valueclasses_dir, JVM_MAXPATHLEN, \"%s%slib%s\" VALUECLASS_STR \"%s\",\n+                 Arguments::get_java_home(), fileSep, fileSep, fileSep);\n+    DIR* dir = os::opendir(valueclasses_dir);\n+    if (dir != nullptr) {\n+      char * module_name = AllocateHeap(JVM_MAXPATHLEN, mtArguments);\n+      char * path = AllocateHeap(JVM_MAXPATHLEN, mtArguments);\n+\n+      for (dirent * entry = os::readdir(dir); entry != nullptr; entry = os::readdir(dir)) {\n+        \/\/ Test if file ends-with \"-valueclasses.jar\"\n+        int len = (int)strlen(entry->d_name) - (sizeof(VALUECLASS_JAR) - 1);\n+        if (len <= 0 || strcmp(&entry->d_name[len], VALUECLASS_JAR) != 0) {\n+          continue;         \/\/ too short or not the expected suffix\n+        }\n+\n+        strcpy(module_name, entry->d_name);\n+        module_name[len] = '\\0';     \/\/ truncate to just module-name\n+\n+        jio_snprintf(path, JVM_MAXPATHLEN, \"%s%s\", valueclasses_dir, &entry->d_name);\n+        add_patch_mod_prefix(module_name, path, true \/* append *\/, true \/* cds OK*\/);\n+        log_info(class)(\"--enable-preview appending value classes for module %s: %s\", module_name, entry->d_name);\n+      }\n+      FreeHeap(module_name);\n+      FreeHeap(path);\n+      os::closedir(dir);\n+    }\n+    FreeHeap(valueclasses_dir);\n+  }\n+\n+  \/\/ Create numbered properties for each module that has been patched either\n+  \/\/ by --patch-module (or --enable-preview if disable_patching is false).\n+  \/\/ Format is \"jdk.module.patch.<n>=<module_name>=<path>\"\n+  if (_patch_mod_prefix != nullptr) {\n+    char * prop_value = AllocateHeap(JVM_MAXPATHLEN + JVM_MAXPATHLEN + 1, mtArguments);\n+    unsigned int patch_mod_count = 0;\n+\n+    for (GrowableArrayIterator<ModulePatchPath *> it = _patch_mod_prefix->begin();\n+            it != _patch_mod_prefix->end(); ++it) {\n+      jio_snprintf(prop_value, JVM_MAXPATHLEN + JVM_MAXPATHLEN + 1, \"%s=%s\",\n+                   (*it)->module_name(), (*it)->path_string());\n+      if (!create_numbered_module_property(\"jdk.module.patch\", prop_value, patch_mod_count++)) {\n+        FreeHeap(prop_value);\n+        return JNI_ENOMEM;\n+      }\n+    }\n+    FreeHeap(prop_value);\n+  }\n+  return JNI_OK;\n+}\n+\n@@ -2343,0 +2439,4 @@\n+      \/\/ --enable-preview enables Valhalla, EnableValhalla VM option will eventually be removed before integration\n+      if (FLAG_SET_CMDLINE(EnableValhalla, true) != JVMFlag::SUCCESS) {\n+        return JNI_EINVAL;\n+      }\n@@ -2846,10 +2946,5 @@\n-void Arguments::add_patch_mod_prefix(const char* module_name, const char* path) {\n-  \/\/ For java.base check for duplicate --patch-module options being specified on the command line.\n-  \/\/ This check is only required for java.base, all other duplicate module specifications\n-  \/\/ will be checked during module system initialization.  The module system initialization\n-  \/\/ will throw an ExceptionInInitializerError if this situation occurs.\n-  if (strcmp(module_name, JAVA_BASE_NAME) == 0) {\n-    if (patch_mod_javabase) {\n-      vm_exit_during_initialization(\"Cannot specify \" JAVA_BASE_NAME \" more than once to --patch-module\");\n-    } else {\n-      patch_mod_javabase = true;\n+void Arguments::add_patch_mod_prefix(const char* module_name, const char* path, bool allow_append, bool allow_cds) {\n+  if (!allow_cds) {\n+    CDSConfig::set_module_patching_disables_cds();\n+    if (strcmp(module_name, JAVA_BASE_NAME) == 0) {\n+      CDSConfig::set_java_base_module_patching_disables_cds();\n@@ -2864,1 +2959,18 @@\n-  _patch_mod_prefix->push(new ModulePatchPath(module_name, path));\n+  \/\/ Scan patches for matching module\n+  int i = _patch_mod_prefix->find_if([&](ModulePatchPath* patch) {\n+    return (strcmp(module_name, patch->module_name()) == 0);\n+  });\n+  if (i == -1) {\n+    _patch_mod_prefix->push(new ModulePatchPath(module_name, path));\n+  } else {\n+    if (allow_append) {\n+      \/\/ append path to existing module entry\n+      _patch_mod_prefix->at(i)->append_path(path);\n+    } else {\n+      if (strcmp(module_name, JAVA_BASE_NAME) == 0) {\n+        vm_exit_during_initialization(\"Cannot specify \" JAVA_BASE_NAME \" more than once to --patch-module\");\n+      } else {\n+        vm_exit_during_initialization(\"Cannot specify a module more than once to --patch-module\", module_name);\n+      }\n+    }\n+  }\n@@ -2977,1 +3089,2 @@\n-  if (!check_vm_args_consistency()) {\n+  \/\/ finalize --module-patch and related --enable-preview\n+  if (finalize_patch_module() != JNI_OK) {\n@@ -2981,0 +3094,3 @@\n+  if (!check_vm_args_consistency()) {\n+    return JNI_ERR;\n+  }\n@@ -3866,0 +3982,12 @@\n+  if (!EnableValhalla || (is_interpreter_only() && !CDSConfig::is_dumping_archive() && !UseSharedSpaces)) {\n+    \/\/ Disable calling convention optimizations if inline types are not supported.\n+    \/\/ Also these aren't useful in -Xint. However, don't disable them when dumping or using\n+    \/\/ the CDS archive, as the values must match between dumptime and runtime.\n+    FLAG_SET_DEFAULT(InlineTypePassFieldsAsArgs, false);\n+    FLAG_SET_DEFAULT(InlineTypeReturnedAsFields, false);\n+  }\n+  if (!UseNonAtomicValueFlattening && !UseNullableValueFlattening && !UseAtomicValueFlattening) {\n+    \/\/ Flattening is disabled\n+    FLAG_SET_DEFAULT(UseArrayFlattening, false);\n+    FLAG_SET_DEFAULT(UseFieldFlattening, false);\n+  }\n","filename":"src\/hotspot\/share\/runtime\/arguments.cpp","additions":146,"deletions":18,"binary":false,"changes":164,"status":"modified"},{"patch":"@@ -471,1 +471,1 @@\n-  template<typename FKind> frame new_heap_frame(frame& f, frame& caller);\n+  template<typename FKind> frame new_heap_frame(frame& f, frame& caller, int size_adjust = 0);\n@@ -473,1 +473,1 @@\n-  inline void patch_pd(frame& callee, const frame& caller);\n+  inline void patch_pd(frame& callee, const frame& caller, bool is_bottom_frame);\n@@ -1183,1 +1183,1 @@\n-  patch_pd(hf, caller);\n+  patch_pd(hf, caller, is_bottom_frame);\n@@ -1280,2 +1280,24 @@\n-  const int argsize = ContinuationHelper::CompiledFrame::stack_argsize(f) + frame::metadata_words_at_top;\n-  const int fsize = pointer_delta_as_int(stack_frame_bottom + argsize, stack_frame_top);\n+  int argsize = ContinuationHelper::CompiledFrame::stack_argsize(f) + frame::metadata_words_at_top;\n+  int fsize = pointer_delta_as_int(stack_frame_bottom + argsize, stack_frame_top);\n+\n+  int real_frame_size = 0;\n+  bool augmented = f.was_augmented_on_entry(real_frame_size);\n+  if (augmented) {\n+    \/\/ The args reside inside the frame so clear argsize. If the caller is compiled,\n+    \/\/ this will cause the stack arguments passed by the caller to be freezed when\n+    \/\/ freezing the caller frame itself. If the caller is interpreted this will have\n+    \/\/ the effect of discarding the arg area created in the i2c stub.\n+    argsize = 0;\n+    fsize = real_frame_size - (callee_interpreted ? 0 : callee_argsize);\n+#ifdef ASSERT\n+    nmethod* nm = f.cb()->as_nmethod();\n+    Method* method = nm->method();\n+    address return_pc = ContinuationHelper::CompiledFrame::return_pc(f);\n+    CodeBlob* caller_cb = CodeCache::find_blob_fast(return_pc);\n+    assert(nm->is_compiled_by_c2() || (caller_cb->is_nmethod() && caller_cb->as_nmethod()->is_compiled_by_c2()), \"caller or callee should be c2 compiled\");\n+    assert((!caller_cb->is_nmethod() && nm->is_compiled_by_c2()) ||\n+           (nm->compiler_type() != caller_cb->as_nmethod()->compiler_type()) ||\n+           (nm->is_compiled_by_c2() && !method->is_static() && method->method_holder()->is_inline_klass()),\n+           \"frame should not be extended\");\n+#endif\n+  }\n@@ -1283,1 +1305,1 @@\n-  log_develop_trace(continuations)(\"recurse_freeze_compiled_frame %s _size: %d fsize: %d argsize: %d\",\n+  log_develop_trace(continuations)(\"recurse_freeze_compiled_frame %s _size: %d fsize: %d argsize: %d augmented: %d\",\n@@ -1286,1 +1308,1 @@\n-                             _freeze_size, fsize, argsize);\n+                             _freeze_size, fsize, argsize, augmented);\n@@ -1297,0 +1319,1 @@\n+  assert(!is_bottom_frame || !augmented, \"thaw extended frame without caller?\");\n@@ -1300,1 +1323,1 @@\n-  frame hf = new_heap_frame<ContinuationHelper::CompiledFrame>(f, caller);\n+  frame hf = new_heap_frame<ContinuationHelper::CompiledFrame>(f, caller, augmented ? real_frame_size - f.cb()->as_nmethod()->frame_size() : 0);\n@@ -2080,0 +2103,1 @@\n+  int remove_scalarized_frames(StackChunkFrameStream<ChunkFrames::CompiledOnly>& scfs, int &argsize);\n@@ -2105,1 +2129,1 @@\n-  inline void patch(frame& f, const frame& caller, bool bottom);\n+  inline void patch(frame& f, const frame& caller, bool bottom, bool augmented = false);\n@@ -2115,1 +2139,1 @@\n-  template<typename FKind> frame new_stack_frame(const frame& hf, frame& caller, bool bottom);\n+  template<typename FKind> frame new_stack_frame(const frame& hf, frame& caller, bool bottom, int size_adjust = 0);\n@@ -2192,0 +2216,14 @@\n+int ThawBase::remove_scalarized_frames(StackChunkFrameStream<ChunkFrames::CompiledOnly>& f, int &argsize) {\n+  intptr_t* top = f.sp();\n+\n+  while (f.cb()->as_nmethod()->needs_stack_repair()) {\n+    f.next(SmallRegisterMap::instance_no_args(), false \/* stop *\/);\n+  }\n+  assert(!f.is_done(), \"\");\n+  assert(f.is_compiled(), \"\");\n+\n+  intptr_t* bottom = f.sp() + f.cb()->frame_size();\n+  argsize = f.stack_argsize();\n+  return bottom - top;\n+}\n+\n@@ -2213,3 +2251,0 @@\n-    frame_size += f.cb()->frame_size();\n-    argsize = f.stack_argsize();\n-\n@@ -2222,0 +2257,9 @@\n+\n+    if (f.cb()->as_nmethod()->needs_stack_repair()) {\n+      frame_size += remove_scalarized_frames(f, argsize);\n+    } else {\n+      frame_size += f.cb()->frame_size();\n+      argsize = f.stack_argsize();\n+    }\n+  } else if (f.cb()->as_nmethod()->needs_stack_repair()) {\n+    frame_size = remove_scalarized_frames(f, argsize);\n@@ -2521,0 +2565,1 @@\n+  CodeBlob* cb = _stream.cb();\n@@ -2525,3 +2570,8 @@\n-  \/\/ we never leave a compiled caller of an interpreted frame as the top frame in the chunk\n-  \/\/ as it makes detecting that situation and adjusting unextended_sp tricky\n-  if (num_frames == 1 && !_stream.is_done() && FKind::interpreted && _stream.is_compiled()) {\n+  \/\/ We never leave a compiled caller of an interpreted frame as the top frame in the chunk\n+  \/\/ as it makes detecting that situation and adjusting unextended_sp tricky. We also always\n+  \/\/ thaw the caller of a frame that needs_stack_repair, as it would otherwise complicate things:\n+  \/\/ - Regardless of whether the frame was extended or not, we would need to copy the right arg\n+  \/\/   size if its greater than the one given by the normal method signature (non-scalarized).\n+  \/\/ - If the frame was indeed extended, leaving its caller as the top frame would complicate walking\n+  \/\/   the chunk (we need unextended_sp, but we only have sp).\n+  if (num_frames == 1 && !_stream.is_done() && ((FKind::interpreted && _stream.is_compiled()) || (FKind::compiled && cb->as_nmethod_or_null()->needs_stack_repair()))) {\n@@ -2587,1 +2637,1 @@\n-inline void ThawBase::patch(frame& f, const frame& caller, bool bottom) {\n+inline void ThawBase::patch(frame& f, const frame& caller, bool bottom, bool augmented) {\n@@ -2592,1 +2642,1 @@\n-  } else {\n+  } else if (caller.is_compiled_frame()){\n@@ -2595,1 +2645,1 @@\n-    ContinuationHelper::Frame::patch_pc(caller, caller.raw_pc());\n+    ContinuationHelper::Frame::patch_pc(caller, caller.raw_pc(), augmented \/*callee_augmented*\/);\n@@ -2826,0 +2876,10 @@\n+  int fsize = 0;\n+  int added_argsize = 0;\n+  bool augmented = hf.was_augmented_on_entry(fsize);\n+  if (!augmented) {\n+    added_argsize = (is_bottom_frame || caller.is_interpreted_frame()) ? hf.compiled_frame_stack_argsize() : 0;\n+    fsize += added_argsize;\n+  }\n+  assert(!is_bottom_frame || !augmented, \"\");\n+\n+\n@@ -2829,1 +2889,3 @@\n-  frame f = new_stack_frame<ContinuationHelper::CompiledFrame>(hf, caller, is_bottom_frame);\n+  frame f = new_stack_frame<ContinuationHelper::CompiledFrame>(hf, caller, is_bottom_frame, augmented ? fsize - hf.cb()->frame_size() : 0);\n+  assert(f.cb()->frame_size() == (int)(caller.sp() - f.sp()), \"\");\n+\n@@ -2832,5 +2894,0 @@\n-\n-  const int added_argsize = (is_bottom_frame || caller.is_interpreted_frame()) ? hf.compiled_frame_stack_argsize() : 0;\n-  int fsize = ContinuationHelper::CompiledFrame::size(hf) + added_argsize;\n-  assert(fsize <= (int)(caller.unextended_sp() - f.unextended_sp()), \"\");\n-\n@@ -2849,1 +2906,1 @@\n-  patch(f, caller, is_bottom_frame);\n+  patch(f, caller, is_bottom_frame, augmented);\n","filename":"src\/hotspot\/share\/runtime\/continuationFreezeThaw.cpp","additions":83,"deletions":26,"binary":false,"changes":109,"status":"modified"},{"patch":"@@ -53,0 +53,3 @@\n+#include \"oops\/flatArrayKlass.hpp\"\n+#include \"oops\/flatArrayOop.hpp\"\n+#include \"oops\/inlineKlass.inline.hpp\"\n@@ -301,0 +304,18 @@\n+\n+static Klass* get_refined_array_klass(Klass* k, frame* fr, RegisterMap* map, ObjectValue* sv, TRAPS) {\n+  \/\/ If it's an array, get the properties\n+  if (k->is_array_klass() && !k->is_typeArray_klass()) {\n+    assert(!k->is_refArray_klass() && !k->is_flatArray_klass(), \"Unexpected refined klass\");\n+    nmethod* nm = fr->cb()->as_nmethod_or_null();\n+    if (nm->is_compiled_by_c2()) {\n+      assert(sv->has_properties(), \"Property information is missing\");\n+      ArrayKlass::ArrayProperties props = static_cast<ArrayKlass::ArrayProperties>(StackValue::create_stack_value(fr, map, sv->properties())->get_jint());\n+      k = ObjArrayKlass::cast(k)->klass_with_properties(props, THREAD);\n+    } else {\n+      \/\/ TODO Graal needs to be fixed. Just go with the default properties for now\n+      k = ObjArrayKlass::cast(k)->klass_with_properties(ArrayKlass::ArrayProperties::DEFAULT, THREAD);\n+    }\n+  }\n+  return k;\n+}\n+\n@@ -302,2 +323,2 @@\n-static void print_objects(JavaThread* deoptee_thread,\n-                          GrowableArray<ScopeValue*>* objects, bool realloc_failures) {\n+static void print_objects(JavaThread* deoptee_thread, frame* deoptee, RegisterMap* map,\n+                          GrowableArray<ScopeValue*>* objects, bool realloc_failures, TRAPS) {\n@@ -319,0 +340,1 @@\n+    k = get_refined_array_klass(k, deoptee, map, sv, THREAD);\n@@ -352,2 +374,13 @@\n-  bool save_oop_result = chunk->at(0)->scope()->return_oop() && !thread->popframe_forcing_deopt_reexecution() && (exec_mode == Deoptimization::Unpack_deopt);\n-  Handle return_value;\n+  ScopeDesc* scope = chunk->at(0)->scope();\n+  bool save_oop_result = scope->return_oop() && !thread->popframe_forcing_deopt_reexecution() && (exec_mode == Deoptimization::Unpack_deopt);\n+  \/\/ In case of the return of multiple values, we must take care\n+  \/\/ of all oop return values.\n+  GrowableArray<Handle> return_oops;\n+  InlineKlass* vk = nullptr;\n+  if (save_oop_result && scope->return_scalarized()) {\n+    vk = InlineKlass::returned_inline_klass(map);\n+    if (vk != nullptr) {\n+      vk->save_oop_fields(map, return_oops);\n+      save_oop_result = false;\n+    }\n+  }\n@@ -359,1 +392,1 @@\n-    return_value = Handle(thread, result);\n+    return_oops.push(Handle(thread, result));\n@@ -366,1 +399,1 @@\n-  if (objects != nullptr) {\n+  if (objects != nullptr || vk != nullptr) {\n@@ -371,1 +404,9 @@\n-      realloc_failures = Deoptimization::realloc_objects(thread, &deoptee, &map, objects, CHECK_AND_CLEAR_(true));\n+      if (vk != nullptr) {\n+        realloc_failures = Deoptimization::realloc_inline_type_result(vk, map, return_oops, CHECK_AND_CLEAR_(true));\n+      }\n+      if (objects != nullptr) {\n+        realloc_failures = realloc_failures || Deoptimization::realloc_objects(thread, &deoptee, &map, objects, CHECK_AND_CLEAR_(true));\n+        guarantee(compiled_method != nullptr, \"deopt must be associated with an nmethod\");\n+        bool is_jvmci = compiled_method->is_compiled_by_jvmci();\n+        Deoptimization::reassign_fields(&deoptee, &map, objects, realloc_failures, is_jvmci, CHECK_AND_CLEAR_(true));\n+      }\n@@ -376,1 +417,9 @@\n-      realloc_failures = Deoptimization::realloc_objects(thread, &deoptee, &map, objects, THREAD);\n+      if (vk != nullptr) {\n+        realloc_failures = Deoptimization::realloc_inline_type_result(vk, map, return_oops, THREAD);\n+      }\n+      if (objects != nullptr) {\n+        realloc_failures = realloc_failures || Deoptimization::realloc_objects(thread, &deoptee, &map, objects, THREAD);\n+        guarantee(compiled_method != nullptr, \"deopt must be associated with an nmethod\");\n+        bool is_jvmci = compiled_method->is_compiled_by_jvmci();\n+        Deoptimization::reassign_fields(&deoptee, &map, objects, realloc_failures, is_jvmci, THREAD);\n+      }\n@@ -379,5 +428,2 @@\n-    guarantee(compiled_method != nullptr, \"deopt must be associated with an nmethod\");\n-    bool is_jvmci = compiled_method->is_compiled_by_jvmci();\n-    Deoptimization::reassign_fields(&deoptee, &map, objects, realloc_failures, is_jvmci);\n-    if (TraceDeoptimization) {\n-      print_objects(deoptee_thread, objects, realloc_failures);\n+    if (TraceDeoptimization && objects != nullptr) {\n+      print_objects(deoptee_thread, &deoptee, &map, objects, realloc_failures, thread);\n@@ -386,1 +432,1 @@\n-  if (save_oop_result) {\n+  if (save_oop_result || vk != nullptr) {\n@@ -388,1 +434,2 @@\n-    deoptee.set_saved_oop_result(&map, return_value());\n+    assert(return_oops.length() == 1, \"no inline type\");\n+    deoptee.set_saved_oop_result(&map, return_oops.pop()());\n@@ -722,1 +769,1 @@\n-  \/\/ If the sender is deoptimized the we must retrieve the address of the handler\n+  \/\/ If the sender is deoptimized we must retrieve the address of the handler\n@@ -1239,2 +1286,10 @@\n-\n-    oop obj = nullptr;\n+    k = get_refined_array_klass(k, fr, reg_map, sv, THREAD);\n+\n+    \/\/ Check if the object may be null and has an additional null_marker input that needs\n+    \/\/ to be checked before using the field values. Skip re-allocation if it is null.\n+    if (k->is_inline_klass() && sv->has_properties()) {\n+      jint null_marker = StackValue::create_stack_value(fr, reg_map, sv->properties())->get_jint();\n+      if (null_marker == 0) {\n+        continue;\n+      }\n+    }\n@@ -1243,0 +1298,1 @@\n+    oop obj = nullptr;\n@@ -1270,0 +1326,4 @@\n+    } else if (k->is_flatArray_klass()) {\n+      FlatArrayKlass* ak = FlatArrayKlass::cast(k);\n+      \/\/ Inline type array must be zeroed because not all memory is reassigned\n+      obj = ak->allocate_instance(sv->field_size(), ak->properties(), THREAD);\n@@ -1276,2 +1336,2 @@\n-    } else if (k->is_objArray_klass()) {\n-      ObjArrayKlass* ak = ObjArrayKlass::cast(k);\n+    } else if (k->is_refArray_klass()) {\n+      RefArrayKlass* ak = RefArrayKlass::cast(k);\n@@ -1279,1 +1339,1 @@\n-      obj = ak->allocate_instance(sv->field_size(), THREAD);\n+      obj = ak->allocate_instance(sv->field_size(), ak->properties(), THREAD);\n@@ -1301,0 +1361,15 @@\n+\/\/ We're deoptimizing at the return of a call, inline type fields are\n+\/\/ in registers. When we go back to the interpreter, it will expect a\n+\/\/ reference to an inline type instance. Allocate and initialize it from\n+\/\/ the register values here.\n+bool Deoptimization::realloc_inline_type_result(InlineKlass* vk, const RegisterMap& map, GrowableArray<Handle>& return_oops, TRAPS) {\n+  oop new_vt = vk->realloc_result(map, return_oops, THREAD);\n+  if (new_vt == nullptr) {\n+    CLEAR_PENDING_EXCEPTION;\n+    THROW_OOP_(Universe::out_of_memory_error_realloc_objects(), true);\n+  }\n+  return_oops.clear();\n+  return_oops.push(Handle(THREAD, new_vt));\n+  return false;\n+}\n+\n@@ -1472,0 +1547,3 @@\n+  InstanceKlass* _klass;\n+  bool _is_flat;\n+  bool _is_null_free;\n@@ -1473,4 +1551,1 @@\n-  ReassignedField() {\n-    _offset = 0;\n-    _type = T_ILLEGAL;\n-  }\n+  ReassignedField() : _offset(0), _type(T_ILLEGAL), _klass(nullptr), _is_flat(false), _is_null_free(false) { }\n@@ -1490,0 +1565,6 @@\n+      if (fs.is_flat()) {\n+        field._is_flat = true;\n+        field._is_null_free = fs.is_null_free_inline_type();\n+        \/\/ Resolve klass of flat inline type field\n+        field._klass = InlineKlass::cast(klass->get_inline_type_field_klass(fs.index()));\n+      }\n@@ -1496,2 +1577,3 @@\n-\/\/ Restore fields of an eliminated instance object employing the same field order used by the compiler.\n-static int reassign_fields_by_klass(InstanceKlass* klass, frame* fr, RegisterMap* reg_map, ObjectValue* sv, int svIndex, oop obj, bool is_jvmci) {\n+\/\/ Restore fields of an eliminated instance object employing the same field order used by the\n+\/\/ compiler when it scalarizes an object at safepoints.\n+static int reassign_fields_by_klass(InstanceKlass* klass, frame* fr, RegisterMap* reg_map, ObjectValue* sv, int svIndex, oop obj, bool is_jvmci, int base_offset, TRAPS) {\n@@ -1500,0 +1582,19 @@\n+    BasicType type = fields->at(i)._type;\n+    int offset = base_offset + fields->at(i)._offset;\n+    \/\/ Check for flat inline type field before accessing the ScopeValue because it might not have any fields\n+    if (fields->at(i)._is_flat) {\n+      \/\/ Recursively re-assign flat inline type fields\n+      InstanceKlass* vk = fields->at(i)._klass;\n+      assert(vk != nullptr, \"must be resolved\");\n+      offset -= InlineKlass::cast(vk)->payload_offset(); \/\/ Adjust offset to omit oop header\n+      svIndex = reassign_fields_by_klass(vk, fr, reg_map, sv, svIndex, obj, is_jvmci, offset, CHECK_0);\n+      if (!fields->at(i)._is_null_free) {\n+        ScopeValue* scope_field = sv->field_at(svIndex);\n+        StackValue* value = StackValue::create_stack_value(fr, reg_map, scope_field);\n+        int nm_offset = offset + InlineKlass::cast(vk)->null_marker_offset();\n+        obj->bool_field_put(nm_offset, value->get_jint() & 1);\n+        svIndex++;\n+      }\n+      continue; \/\/ Continue because we don't need to increment svIndex\n+    }\n+\n@@ -1502,3 +1603,2 @@\n-    int offset = fields->at(i)._offset;\n-    BasicType type = fields->at(i)._type;\n-      case T_OBJECT: case T_ARRAY:\n+      case T_OBJECT:\n+      case T_ARRAY:\n@@ -1576,0 +1676,1 @@\n+\n@@ -1579,0 +1680,23 @@\n+\/\/ restore fields of an eliminated inline type array\n+void Deoptimization::reassign_flat_array_elements(frame* fr, RegisterMap* reg_map, ObjectValue* sv, flatArrayOop obj, FlatArrayKlass* vak, bool is_jvmci, TRAPS) {\n+  InlineKlass* vk = vak->element_klass();\n+  assert(vk->maybe_flat_in_array(), \"should only be used for flat inline type arrays\");\n+  \/\/ Adjust offset to omit oop header\n+  int base_offset = arrayOopDesc::base_offset_in_bytes(T_FLAT_ELEMENT) - vk->payload_offset();\n+  \/\/ Initialize all elements of the flat inline type array\n+  for (int i = 0; i < sv->field_size(); i++) {\n+    ObjectValue* val = sv->field_at(i)->as_ObjectValue();\n+    int offset = base_offset + (i << Klass::layout_helper_log2_element_size(vak->layout_helper()));\n+    reassign_fields_by_klass(vk, fr, reg_map, val, 0, (oop)obj, is_jvmci, offset, CHECK);\n+    if (!obj->is_null_free_array()) {\n+      jboolean null_marker_value;\n+      if (val->has_properties()) {\n+        null_marker_value = StackValue::create_stack_value(fr, reg_map, val->properties())->get_jint() & 1;\n+      } else {\n+        null_marker_value = 1;\n+      }\n+      obj->bool_field_put(offset + vk->null_marker_offset(), null_marker_value);\n+    }\n+  }\n+}\n+\n@@ -1580,1 +1704,1 @@\n-void Deoptimization::reassign_fields(frame* fr, RegisterMap* reg_map, GrowableArray<ScopeValue*>* objects, bool realloc_failures, bool is_jvmci) {\n+void Deoptimization::reassign_fields(frame* fr, RegisterMap* reg_map, GrowableArray<ScopeValue*>* objects, bool realloc_failures, bool is_jvmci, TRAPS) {\n@@ -1585,0 +1709,2 @@\n+    k = get_refined_array_klass(k, fr, reg_map, sv, THREAD);\n+\n@@ -1586,1 +1712,1 @@\n-    assert(obj.not_null() || realloc_failures, \"reallocation was missed\");\n+    assert(obj.not_null() || realloc_failures || sv->has_properties(), \"reallocation was missed\");\n@@ -1624,1 +1750,4 @@\n-      reassign_fields_by_klass(ik, fr, reg_map, sv, 0, obj(), is_jvmci);\n+      reassign_fields_by_klass(ik, fr, reg_map, sv, 0, obj(), is_jvmci, 0, CHECK);\n+    } else if (k->is_flatArray_klass()) {\n+      FlatArrayKlass* vak = FlatArrayKlass::cast(k);\n+      reassign_flat_array_elements(fr, reg_map, sv, (flatArrayOop) obj(), vak, is_jvmci, CHECK);\n@@ -1628,1 +1757,1 @@\n-    } else if (k->is_objArray_klass()) {\n+    } else if (k->is_refArray_klass()) {\n@@ -1806,1 +1935,1 @@\n-  \/\/ Deoptimize only if the frame comes from compile code.\n+  \/\/ Deoptimize only if the frame comes from compiled code.\n","filename":"src\/hotspot\/share\/runtime\/deoptimization.cpp","additions":164,"deletions":35,"binary":false,"changes":199,"status":"modified"},{"patch":"@@ -38,0 +38,1 @@\n+#include \"oops\/inlineKlass.hpp\"\n@@ -62,0 +63,3 @@\n+#ifdef COMPILER1\n+#include \"c1\/c1_Runtime1.hpp\"\n+#endif\n@@ -364,0 +368,19 @@\n+\n+#ifdef COMPILER1\n+  if (nm->is_compiled_by_c1() && nm->method()->has_scalarized_args() &&\n+      pc() < nm->verified_inline_entry_point()) {\n+    \/\/ The VEP and VIEP(RO) of C1-compiled methods call into the runtime to buffer scalarized value\n+    \/\/ type args. We can't deoptimize at that point because the buffers have not yet been initialized.\n+    \/\/ Also, if the method is synchronized, we first need to acquire the lock.\n+    \/\/ Don't patch the return pc to delay deoptimization until we enter the method body (the check\n+    \/\/ added in LIRGenerator::do_Base will detect the pending deoptimization by checking the original_pc).\n+#if defined ASSERT && !defined AARCH64   \/\/ Stub call site does not look like NativeCall on AArch64\n+    NativeCall* call = nativeCall_before(this->pc());\n+    address dest = call->destination();\n+    assert(dest == Runtime1::entry_for(StubId::c1_buffer_inline_args_no_receiver_id) ||\n+           dest == Runtime1::entry_for(StubId::c1_buffer_inline_args_id), \"unexpected safepoint in entry point\");\n+#endif\n+    return;\n+  }\n+#endif\n+\n@@ -762,1 +785,1 @@\n-                          OopClosure* f) {\n+                          OopClosure* f, BufferedValueClosure* bvt_f) {\n@@ -774,1 +797,3 @@\n-      _f->do_oop(addr);\n+      if (_f != nullptr) {\n+        _f->do_oop(addr);\n+      }\n@@ -786,1 +811,3 @@\n-        _f->do_oop(addr);\n+        if (_f != nullptr) {\n+          _f->do_oop(addr);\n+        }\n@@ -948,1 +975,1 @@\n-  InterpreterFrameClosure blk(this, max_locals, m->max_stack(), f);\n+  InterpreterFrameClosure blk(this, max_locals, m->max_stack(), f, nullptr);\n@@ -960,0 +987,18 @@\n+void frame::buffered_values_interpreted_do(BufferedValueClosure* f) {\n+  assert(is_interpreted_frame(), \"Not an interpreted frame\");\n+  Thread *thread = Thread::current();\n+  methodHandle m (thread, interpreter_frame_method());\n+  jint      bci = interpreter_frame_bci();\n+\n+  assert(m->is_method(), \"checking frame value\");\n+  assert(!m->is_native() && bci >= 0 && bci < m->code_size(),\n+         \"invalid bci value\");\n+\n+  InterpreterFrameClosure blk(this, m->max_locals(), m->max_stack(), nullptr, f);\n+\n+  \/\/ process locals & expression stack\n+  InterpreterOopMap mask;\n+  m->mask_for(bci, &mask);\n+  mask.iterate_oop(&blk);\n+}\n+\n@@ -1015,0 +1060,1 @@\n+    assert(_offset < _arg_size, \"out of bounds\");\n@@ -1041,5 +1087,1 @@\n-    _arg_size  = ArgumentSizeComputer(signature).size() + (has_receiver ? 1 : 0) + (has_appendix ? 1 : 0);\n-\n-    int arg_size;\n-    _regs = SharedRuntime::find_callee_arguments(signature, has_receiver, has_appendix, &arg_size);\n-    assert(arg_size == _arg_size, \"wrong arg size\");\n+    _regs = SharedRuntime::find_callee_arguments(signature, has_receiver, has_appendix, &_arg_size);\n@@ -1425,2 +1467,2 @@\n-                    FormatBuffer<1024>(\"#%d nmethod \" INTPTR_FORMAT \" for method J %s%s\", frame_no,\n-                                       p2i(nm),\n+                    FormatBuffer<1024>(\"#%d nmethod (%s %d) \" INTPTR_FORMAT \" for method J %s%s\", frame_no,\n+                                       nm->is_compiled_by_c1() ? \"c1\" : \"c2\", nm->frame_size(), p2i(nm),\n@@ -1436,0 +1478,5 @@\n+      CompiledEntrySignature ces(m);\n+      ces.compute_calling_conventions(false);\n+      const GrowableArray<SigEntry>* sig_cc = nm->is_compiled_by_c2() ? ces.sig_cc() : ces.sig();\n+      const VMRegPair* regs = nm->is_compiled_by_c2() ? ces.regs_cc() : ces.regs();\n+\n@@ -1437,21 +1484,0 @@\n-      int sizeargs = m->size_of_parameters();\n-\n-      BasicType* sig_bt = NEW_RESOURCE_ARRAY(BasicType, sizeargs);\n-      VMRegPair* regs   = NEW_RESOURCE_ARRAY(VMRegPair, sizeargs);\n-      {\n-        int sig_index = 0;\n-        if (!m->is_static()) {\n-          sig_bt[sig_index++] = T_OBJECT; \/\/ 'this'\n-        }\n-        for (SignatureStream ss(m->signature()); !ss.at_return_type(); ss.next()) {\n-          BasicType t = ss.type();\n-          assert(type2size[t] == 1 || type2size[t] == 2, \"size is 1 or 2\");\n-          sig_bt[sig_index++] = t;\n-          if (type2size[t] == 2) {\n-            sig_bt[sig_index++] = T_VOID;\n-          }\n-        }\n-        assert(sig_index == sizeargs, \"\");\n-      }\n-      int stack_arg_slots = SharedRuntime::java_calling_convention(sig_bt, regs, sizeargs);\n-      assert(stack_arg_slots ==  nm->as_nmethod()->num_stack_arg_slots(false \/* rounded *\/) || nm->is_osr_method(), \"\");\n@@ -1461,1 +1487,1 @@\n-      for (SignatureStream ss(m->signature()); !ss.at_return_type(); ) {\n+      for (ExtendedSignature sig = ExtendedSignature(sig_cc, SigEntryFilter()); !sig.at_end(); ++sig) {\n@@ -1463,3 +1489,1 @@\n-        bool at_old_sp = false;\n-        BasicType t = (at_this ? T_OBJECT : ss.type());\n-        assert(t == sig_bt[sig_index], \"sigs in sync\");\n+        BasicType t = (*sig)._bt;\n@@ -1479,3 +1503,0 @@\n-        if (!at_this) {\n-          ss.next();\n-        }\n","filename":"src\/hotspot\/share\/runtime\/frame.cpp","additions":60,"deletions":39,"binary":false,"changes":99,"status":"modified"},{"patch":"@@ -461,0 +461,1 @@\n+  void buffered_values_interpreted_do(BufferedValueClosure* f);\n","filename":"src\/hotspot\/share\/runtime\/frame.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -33,0 +33,1 @@\n+#include \"oops\/inlineKlass.hpp\"\n@@ -144,4 +145,4 @@\n-    case T_BOOLEAN: \/\/ fall through\n-    case T_CHAR   : \/\/ fall through\n-    case T_SHORT  : \/\/ fall through\n-    case T_INT    : \/\/ fall through\n+    case T_BOOLEAN  : \/\/ fall through\n+    case T_CHAR     : \/\/ fall through\n+    case T_SHORT    : \/\/ fall through\n+    case T_INT      : \/\/ fall through\n@@ -149,2 +150,3 @@\n-    case T_OBJECT : \/\/ fall through\n-    case T_ARRAY  : \/\/ fall through\n+    case T_OBJECT   : \/\/ fall through\n+    case T_ARRAY    : \/\/ fall through\n+    case T_FLAT_ELEMENT: \/\/ fall through\n@@ -152,5 +154,5 @@\n-    case T_BYTE   : \/\/ fall through\n-    case T_VOID   : return T_INT;\n-    case T_LONG   : return T_LONG;\n-    case T_FLOAT  : return T_FLOAT;\n-    case T_DOUBLE : return T_DOUBLE;\n+    case T_BYTE     : \/\/ fall through\n+    case T_VOID     : return T_INT;\n+    case T_LONG     : return T_LONG;\n+    case T_FLOAT    : return T_FLOAT;\n+    case T_DOUBLE   : return T_DOUBLE;\n@@ -158,2 +160,2 @@\n-    case T_ARRAY  : \/\/ fall through\n-    case T_OBJECT:  return T_OBJECT;\n+    case T_ARRAY    : \/\/ fall through\n+    case T_OBJECT   : return T_OBJECT;\n@@ -380,0 +382,12 @@\n+  jobject value_buffer = nullptr;\n+  if (InlineTypeReturnedAsFields && (result->get_type() == T_OBJECT)) {\n+    \/\/ Pre allocate a buffered inline type in case the result is returned\n+    \/\/ flattened by compiled code\n+    InlineKlass* vk = method->returns_inline_type();\n+    if (vk != nullptr && vk->can_be_returned_as_fields()) {\n+      oop instance = vk->allocate_instance(CHECK);\n+      value_buffer = JNIHandles::make_local(thread, instance);\n+      result->set_jobject(value_buffer);\n+    }\n+  }\n+\n@@ -444,0 +458,1 @@\n+    JNIHandles::destroy_local(value_buffer);\n@@ -592,1 +607,1 @@\n-  if (is_reference_type(return_type)) return_type = T_OBJECT;\n+  if (return_type == T_ARRAY) return_type = T_OBJECT;\n","filename":"src\/hotspot\/share\/runtime\/javaCalls.cpp","additions":29,"deletions":14,"binary":false,"changes":43,"status":"modified"},{"patch":"@@ -50,0 +50,1 @@\n+#include \"oops\/inlineKlass.hpp\"\n","filename":"src\/hotspot\/share\/runtime\/javaThread.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -151,0 +151,1 @@\n+  oop           _return_buffered_value; \/\/ buffered value being returned\n@@ -816,0 +817,3 @@\n+  oop return_buffered_value() const              { return _return_buffered_value; }\n+  void set_return_buffered_value(oop val)        { _return_buffered_value = val; }\n+\n@@ -878,0 +882,1 @@\n+  static ByteSize return_buffered_value_offset() { return byte_offset_of(JavaThread, _return_buffered_value); }\n","filename":"src\/hotspot\/share\/runtime\/javaThread.hpp","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -48,0 +48,1 @@\n+#include \"memory\/oopFactory.hpp\"\n@@ -51,0 +52,3 @@\n+#include \"oops\/access.hpp\"\n+#include \"oops\/fieldStreams.inline.hpp\"\n+#include \"oops\/inlineKlass.inline.hpp\"\n@@ -54,0 +58,1 @@\n+#include \"oops\/objArrayOop.inline.hpp\"\n@@ -73,0 +78,1 @@\n+#include \"runtime\/signature.hpp\"\n@@ -1254,0 +1260,16 @@\n+  \/\/ Substitutability test implementation piggy backs on static call resolution\n+  Bytecodes::Code code = caller->java_code_at(bci);\n+  if (code == Bytecodes::_if_acmpeq || code == Bytecodes::_if_acmpne) {\n+    bc = Bytecodes::_invokestatic;\n+    methodHandle attached_method(THREAD, extract_attached_method(vfst));\n+    assert(attached_method.not_null(), \"must have attached method\");\n+    vmClasses::ValueObjectMethods_klass()->initialize(CHECK_NH);\n+    LinkResolver::resolve_invoke(callinfo, receiver, attached_method, bc, false, CHECK_NH);\n+#ifdef ASSERT\n+    Symbol* subst_method_name = UseAltSubstitutabilityMethod ? vmSymbols::isSubstitutableAlt_name() : vmSymbols::isSubstitutable_name();\n+    Method* is_subst = vmClasses::ValueObjectMethods_klass()->find_method(subst_method_name, vmSymbols::object_object_boolean_signature());\n+    assert(callinfo.selected_method() == is_subst, \"must be isSubstitutable method\");\n+#endif\n+    return receiver;\n+  }\n+\n@@ -1289,0 +1311,6 @@\n+    } else {\n+      assert(attached_method->has_scalarized_args(), \"invalid use of attached method\");\n+      if (!attached_method->method_holder()->is_inline_klass()) {\n+        \/\/ Ignore the attached method in this case to not confuse below code\n+        attached_method = methodHandle(current, nullptr);\n+      }\n@@ -1297,0 +1325,1 @@\n+  bool check_null_and_abstract = true;\n@@ -1310,2 +1339,3 @@\n-    if (attached_method.is_null()) {\n-      Method* callee = bytecode.static_target(CHECK_NH);\n+    Method* callee = attached_method();\n+    if (callee == nullptr) {\n+      callee = bytecode.static_target(CHECK_NH);\n@@ -1316,7 +1346,17 @@\n-\n-    \/\/ Retrieve from a compiled argument list\n-    receiver = Handle(current, callerFrame.retrieve_receiver(&reg_map2));\n-    assert(oopDesc::is_oop_or_null(receiver()), \"\");\n-\n-    if (receiver.is_null()) {\n-      THROW_(vmSymbols::java_lang_NullPointerException(), nullHandle);\n+    bool caller_is_c1 = callerFrame.is_compiled_frame() && callerFrame.cb()->as_nmethod()->is_compiled_by_c1();\n+    if (!caller_is_c1 && callee->is_scalarized_arg(0)) {\n+      \/\/ If the receiver is an inline type that is passed as fields, no oop is available\n+      \/\/ Resolve the call without receiver null checking.\n+      assert(!callee->mismatch(), \"calls with inline type receivers should never mismatch\");\n+      assert(attached_method.not_null() && !attached_method->is_abstract(), \"must have non-abstract attached method\");\n+      if (bc == Bytecodes::_invokeinterface) {\n+        bc = Bytecodes::_invokevirtual; \/\/ C2 optimistically replaces interface calls by virtual calls\n+      }\n+      check_null_and_abstract = false;\n+    } else {\n+      \/\/ Retrieve from a compiled argument list\n+      receiver = Handle(current, callerFrame.retrieve_receiver(&reg_map2));\n+      assert(oopDesc::is_oop_or_null(receiver()), \"\");\n+      if (receiver.is_null()) {\n+        THROW_(vmSymbols::java_lang_NullPointerException(), nullHandle);\n+      }\n@@ -1329,1 +1369,1 @@\n-    LinkResolver::resolve_invoke(callinfo, receiver, attached_method, bc, CHECK_NH);\n+    LinkResolver::resolve_invoke(callinfo, receiver, attached_method, bc, check_null_and_abstract, CHECK_NH);\n@@ -1338,1 +1378,1 @@\n-  if (has_receiver) {\n+  if (has_receiver && check_null_and_abstract) {\n@@ -1366,1 +1406,1 @@\n-methodHandle SharedRuntime::find_callee_method(TRAPS) {\n+methodHandle SharedRuntime::find_callee_method(bool& caller_does_not_scalarize, TRAPS) {\n@@ -1392,0 +1432,4 @@\n+    \/\/ Calls via mismatching methods are always non-scalarized\n+    if (callinfo.resolved_method()->mismatch()) {\n+      caller_does_not_scalarize = true;\n+    }\n@@ -1399,1 +1443,1 @@\n-methodHandle SharedRuntime::resolve_helper(bool is_virtual, bool is_optimized, TRAPS) {\n+methodHandle SharedRuntime::resolve_helper(bool is_virtual, bool is_optimized, bool& caller_does_not_scalarize, TRAPS) {\n@@ -1422,0 +1466,4 @@\n+  \/\/ Calls via mismatching methods are always non-scalarized\n+  if (caller_nm->is_compiled_by_c1() || call_info.resolved_method()->mismatch()) {\n+    caller_does_not_scalarize = true;\n+  }\n@@ -1440,1 +1488,1 @@\n-    tty->print(\"resolving %s%s (%s) call to\",\n+    tty->print(\"resolving %s%s (%s) %s call to\",\n@@ -1442,1 +1490,1 @@\n-               Bytecodes::name(invoke_code));\n+               Bytecodes::name(invoke_code), (caller_does_not_scalarize) ? \"non-scalar\" : \"\");\n@@ -1481,1 +1529,1 @@\n-    inline_cache->update(&call_info, receiver->klass());\n+    inline_cache->update(&call_info, receiver->klass(), caller_does_not_scalarize);\n@@ -1485,1 +1533,1 @@\n-    callsite->set(callee_method);\n+    callsite->set(callee_method, caller_does_not_scalarize);\n@@ -1505,0 +1553,1 @@\n+  bool caller_does_not_scalarize = false;\n@@ -1506,1 +1555,1 @@\n-    callee_method = SharedRuntime::handle_ic_miss_helper(CHECK_NULL);\n+    callee_method = SharedRuntime::handle_ic_miss_helper(caller_does_not_scalarize, CHECK_NULL);\n@@ -1511,1 +1560,1 @@\n-  return get_resolved_entry(current, callee_method);\n+  return get_resolved_entry(current, callee_method, false, false, caller_does_not_scalarize);\n@@ -1552,1 +1601,5 @@\n-      return callee->get_c2i_entry();\n+      if (caller_frame.is_interpreted_frame()) {\n+        return callee->get_c2i_inline_entry();\n+      } else {\n+        return callee->get_c2i_entry();\n+      }\n@@ -1558,0 +1611,3 @@\n+  bool is_static_call = false;\n+  bool is_optimized = false;\n+  bool caller_does_not_scalarize = false;\n@@ -1560,1 +1616,1 @@\n-    callee_method = SharedRuntime::reresolve_call_site(CHECK_NULL);\n+    callee_method = SharedRuntime::reresolve_call_site(is_optimized, caller_does_not_scalarize, CHECK_NULL);\n@@ -1564,1 +1620,1 @@\n-  return get_resolved_entry(current, callee_method);\n+  return get_resolved_entry(current, callee_method, callee_method->is_static(), is_optimized, caller_does_not_scalarize);\n@@ -1603,1 +1659,2 @@\n-address SharedRuntime::get_resolved_entry(JavaThread* current, methodHandle callee_method) {\n+address SharedRuntime::get_resolved_entry(JavaThread* current, methodHandle callee_method,\n+                                          bool is_static_call, bool is_optimized, bool caller_does_not_scalarize) {\n@@ -1609,2 +1666,11 @@\n-  assert(callee_method->verified_code_entry() != nullptr, \" Jump to zero!\");\n-  return callee_method->verified_code_entry();\n+\n+  if (caller_does_not_scalarize) {\n+    assert(callee_method->verified_inline_code_entry() != nullptr, \"Jump to zero!\");\n+    return callee_method->verified_inline_code_entry();\n+  } else if (is_static_call || is_optimized) {\n+    assert(callee_method->verified_code_entry() != nullptr, \"Jump to zero!\");\n+    return callee_method->verified_code_entry();\n+  } else {\n+    assert(callee_method->verified_inline_ro_code_entry() != nullptr, \"Jump to zero!\");\n+    return callee_method->verified_inline_ro_code_entry();\n+  }\n@@ -1616,0 +1682,1 @@\n+  bool caller_does_not_scalarize = false;\n@@ -1618,1 +1685,1 @@\n-    callee_method = SharedRuntime::resolve_helper(false, false, CHECK_NULL);\n+    callee_method = SharedRuntime::resolve_helper(false, false, caller_does_not_scalarize, CHECK_NULL);\n@@ -1622,1 +1689,1 @@\n-  return get_resolved_entry(current, callee_method);\n+  return get_resolved_entry(current, callee_method, true, false, caller_does_not_scalarize);\n@@ -1628,0 +1695,1 @@\n+  bool caller_does_not_scalarize = false;\n@@ -1629,1 +1697,1 @@\n-    callee_method = SharedRuntime::resolve_helper(true, false, CHECK_NULL);\n+    callee_method = SharedRuntime::resolve_helper(true, false, caller_does_not_scalarize, CHECK_NULL);\n@@ -1633,1 +1701,1 @@\n-  return get_resolved_entry(current, callee_method);\n+  return get_resolved_entry(current, callee_method, false, false, caller_does_not_scalarize);\n@@ -1641,0 +1709,1 @@\n+  bool caller_does_not_scalarize = false;\n@@ -1642,1 +1711,1 @@\n-    callee_method = SharedRuntime::resolve_helper(true, true, CHECK_NULL);\n+    callee_method = SharedRuntime::resolve_helper(true, true, caller_does_not_scalarize, CHECK_NULL);\n@@ -1646,1 +1715,1 @@\n-  return get_resolved_entry(current, callee_method);\n+  return get_resolved_entry(current, callee_method, false, true, caller_does_not_scalarize);\n@@ -1649,1 +1718,3 @@\n-methodHandle SharedRuntime::handle_ic_miss_helper(TRAPS) {\n+\n+\n+methodHandle SharedRuntime::handle_ic_miss_helper(bool& caller_does_not_scalarize, TRAPS) {\n@@ -1667,1 +1738,1 @@\n-    tty->print(\"IC miss (%s) call to\", Bytecodes::name(bc));\n+    tty->print(\"IC miss (%s) %s call to\", Bytecodes::name(bc), (caller_does_not_scalarize) ? \"non-scalar\" : \"\");\n@@ -1699,0 +1770,4 @@\n+  \/\/ Calls via mismatching methods are always non-scalarized\n+  if (caller_nm->is_compiled_by_c1() || call_info.resolved_method()->mismatch()) {\n+    caller_does_not_scalarize = true;\n+  }\n@@ -1702,1 +1777,1 @@\n-  inline_cache->update(&call_info, receiver()->klass());\n+  inline_cache->update(&call_info, receiver()->klass(), caller_does_not_scalarize);\n@@ -1713,1 +1788,1 @@\n-methodHandle SharedRuntime::reresolve_call_site(TRAPS) {\n+methodHandle SharedRuntime::reresolve_call_site(bool& is_optimized, bool& caller_does_not_scalarize, TRAPS) {\n@@ -1723,8 +1798,21 @@\n-\n-  \/\/ Do nothing if the frame isn't a live compiled frame.\n-  \/\/ nmethod could be deoptimized by the time we get here\n-  \/\/ so no update to the caller is needed.\n-\n-  if ((caller.is_compiled_frame() && !caller.is_deoptimized_frame()) ||\n-      (caller.is_native_frame() && caller.cb()->as_nmethod()->method()->is_continuation_enter_intrinsic())) {\n-\n+  if (caller.is_compiled_frame()) {\n+    caller_does_not_scalarize = caller.cb()->as_nmethod()->is_compiled_by_c1();\n+  }\n+  assert(!caller.is_interpreted_frame(), \"must be compiled\");\n+\n+  \/\/ If the frame isn't a live compiled frame (i.e. deoptimized by the time we get here), no IC clearing must be done\n+  \/\/ for the caller. However, when the caller is C2 compiled and the callee a C1 or C2 compiled method, then we still\n+  \/\/ need to figure out whether it was an optimized virtual call with an inline type receiver. Otherwise, we end up\n+  \/\/ using the wrong method entry point and accidentally skip the buffering of the receiver.\n+  methodHandle callee_method = find_callee_method(caller_does_not_scalarize, CHECK_(methodHandle()));\n+  const bool caller_is_compiled_and_not_deoptimized = caller.is_compiled_frame() && !caller.is_deoptimized_frame();\n+  const bool caller_is_continuation_enter_intrinsic =\n+    caller.is_native_frame() && caller.cb()->as_nmethod()->method()->is_continuation_enter_intrinsic();\n+  const bool do_IC_clearing = caller_is_compiled_and_not_deoptimized || caller_is_continuation_enter_intrinsic;\n+\n+  const bool callee_compiled_with_scalarized_receiver = callee_method->has_compiled_code() &&\n+                                                        !callee_method()->is_static() &&\n+                                                        callee_method()->is_scalarized_arg(0);\n+  const bool compute_is_optimized = !caller_does_not_scalarize && callee_compiled_with_scalarized_receiver;\n+\n+  if (do_IC_clearing || compute_is_optimized) {\n@@ -1763,0 +1851,1 @@\n+        is_optimized = false;\n@@ -1765,0 +1854,1 @@\n+            assert(callee_method->is_static(), \"must be\");\n@@ -1766,2 +1856,5 @@\n-            CompiledDirectCall* cdc = CompiledDirectCall::at(call_addr);\n-            cdc->set_to_clean();\n+            is_optimized = (iter.type() == relocInfo::opt_virtual_call_type);\n+            if (do_IC_clearing) {\n+              CompiledDirectCall* cdc = CompiledDirectCall::at(call_addr);\n+              cdc->set_to_clean();\n+            }\n@@ -1770,4 +1863,5 @@\n-\n-            \/\/ compiled, dispatched call (which used to call an interpreted method)\n-            CompiledIC* inline_cache = CompiledIC_at(caller_nm, call_addr);\n-            inline_cache->set_to_clean();\n+            if (do_IC_clearing) {\n+              \/\/ compiled, dispatched call (which used to call an interpreted method)\n+              CompiledIC* inline_cache = CompiledIC_at(caller_nm, call_addr);\n+              inline_cache->set_to_clean();\n+            }\n@@ -1784,3 +1878,0 @@\n-  methodHandle callee_method = find_callee_method(CHECK_(methodHandle()));\n-\n-\n@@ -1792,1 +1883,1 @@\n-    tty->print(\"handle_wrong_method reresolving call to\");\n+    tty->print(\"handle_wrong_method reresolving %s call to\", (caller_does_not_scalarize) ? \"non-scalar\" : \"\");\n@@ -1998,0 +2089,15 @@\n+char* SharedRuntime::generate_identity_exception_message(JavaThread* current, Klass* klass) {\n+  assert(klass->is_inline_klass(), \"Must be a concrete value class\");\n+  const char* desc = \"Cannot synchronize on an instance of value class \";\n+  const char* className = klass->external_name();\n+  size_t msglen = strlen(desc) + strlen(className) + 1;\n+  char* message = NEW_RESOURCE_ARRAY(char, msglen);\n+  if (nullptr == message) {\n+    \/\/ Out of memory: can't create detailed error message\n+    message = const_cast<char*>(klass->external_name());\n+  } else {\n+    jio_snprintf(message, msglen, \"%s%s\", desc, className);\n+  }\n+  return message;\n+}\n+\n@@ -2231,5 +2337,30 @@\n- private:\n-  enum {\n-    _basic_type_bits = 4,\n-    _basic_type_mask = right_n_bits(_basic_type_bits),\n-    _basic_types_per_int = BitsPerInt \/ _basic_type_bits,\n+public:\n+  class Element {\n+  private:\n+    \/\/ The highest byte is the type of the argument. The remaining bytes contain the offset of the\n+    \/\/ field if it is flattened in the calling convention, -1 otherwise.\n+    juint _payload;\n+\n+    static constexpr int offset_bit_width = 24;\n+    static constexpr juint offset_bit_mask = (1 << offset_bit_width) - 1;\n+  public:\n+    Element(BasicType bt, int offset) : _payload((static_cast<juint>(bt) << offset_bit_width) | (juint(offset) & offset_bit_mask)) {\n+      assert(offset >= -1 && offset < jint(offset_bit_mask), \"invalid offset %d\", offset);\n+    }\n+\n+    BasicType bt() const {\n+      return static_cast<BasicType>(_payload >> offset_bit_width);\n+    }\n+\n+    int offset() const {\n+      juint res = _payload & offset_bit_mask;\n+      return res == offset_bit_mask ? -1 : res;\n+    }\n+\n+    juint hash() const {\n+      return _payload;\n+    }\n+\n+    bool operator!=(const Element& other) const {\n+      return _payload != other._payload;\n+    }\n@@ -2237,3 +2368,3 @@\n-  \/\/ TO DO:  Consider integrating this with a more global scheme for compressing signatures.\n-  \/\/ For now, 4 bits per components (plus T_VOID gaps after double\/long) is not excessive.\n-  int _length;\n+private:\n+  const bool _has_ro_adapter;\n+  const int _length;\n@@ -2243,2 +2374,8 @@\n-  int* data_pointer() {\n-    return (int*)((address)this + data_offset());\n+  Element* data_pointer() {\n+    return reinterpret_cast<Element*>(reinterpret_cast<address>(this) + data_offset());\n+  }\n+\n+  const Element& element_at(int index) {\n+    assert(index < length(), \"index %d out of bounds for length %d\", index, length());\n+    Element* data = data_pointer();\n+    return data[index];\n@@ -2248,6 +2385,5 @@\n-  AdapterFingerPrint(int total_args_passed, BasicType* sig_bt, int len) {\n-    int* data = data_pointer();\n-    \/\/ Pack the BasicTypes with 8 per int\n-    assert(len == length(total_args_passed), \"sanity\");\n-    _length = len;\n-    int sig_index = 0;\n+  AdapterFingerPrint(const GrowableArray<SigEntry>* sig, bool has_ro_adapter)\n+    : _has_ro_adapter(has_ro_adapter), _length(total_args_passed_in_sig(sig)) {\n+    Element* data = data_pointer();\n+    BasicType prev_bt = T_ILLEGAL;\n+    int vt_count = 0;\n@@ -2255,5 +2391,15 @@\n-      int value = 0;\n-      for (int byte = 0; sig_index < total_args_passed && byte < _basic_types_per_int; byte++) {\n-        int bt = adapter_encoding(sig_bt[sig_index++]);\n-        assert((bt & _basic_type_mask) == bt, \"must fit in 4 bits\");\n-        value = (value << _basic_type_bits) | bt;\n+      const SigEntry& sig_entry = sig->at(index);\n+      BasicType bt = sig_entry._bt;\n+      if (bt == T_METADATA) {\n+        \/\/ Found start of inline type in signature\n+        assert(InlineTypePassFieldsAsArgs, \"unexpected start of inline type\");\n+        vt_count++;\n+      } else if (bt == T_VOID && prev_bt != T_LONG && prev_bt != T_DOUBLE) {\n+        \/\/ Found end of inline type in signature\n+        assert(InlineTypePassFieldsAsArgs, \"unexpected end of inline type\");\n+        vt_count--;\n+        assert(vt_count >= 0, \"invalid vt_count\");\n+      } else if (vt_count == 0) {\n+        \/\/ Widen fields that are not part of a scalarized inline type argument\n+        assert(sig_entry._offset == -1, \"invalid offset for argument that is not a flattened field %d\", sig_entry._offset);\n+        bt = adapter_encoding(bt);\n@@ -2261,1 +2407,3 @@\n-      data[index] = value;\n+\n+      ::new(&data[index]) Element(bt, sig_entry._offset);\n+      prev_bt = bt;\n@@ -2263,0 +2411,1 @@\n+    assert(vt_count == 0, \"invalid vt_count\");\n@@ -2270,2 +2419,2 @@\n-  static int length(int total_args) {\n-    return (total_args + (_basic_types_per_int-1)) \/ _basic_types_per_int;\n+  static int total_args_passed_in_sig(const GrowableArray<SigEntry>* sig) {\n+    return (sig != nullptr) ? sig->length() : 0;\n@@ -2275,1 +2424,1 @@\n-    return (int)heap_word_size(sizeof(AdapterFingerPrint) + (len * sizeof(int)));\n+    return (int)heap_word_size(sizeof(AdapterFingerPrint) + (len * sizeof(Element)));\n@@ -2281,1 +2430,1 @@\n-  static int adapter_encoding(BasicType in) {\n+  static BasicType adapter_encoding(BasicType in) {\n@@ -2287,1 +2436,1 @@\n-        \/\/ There are all promoted to T_INT in the calling convention\n+        \/\/ They are all promoted to T_INT in the calling convention\n@@ -2320,0 +2469,1 @@\n+public:\n@@ -2323,10 +2473,1 @@\n-      unsigned val = (unsigned)value(i);\n-      \/\/ args are packed so that first\/lower arguments are in the highest\n-      \/\/ bits of each int value, so iterate from highest to the lowest\n-      for (int j = 32 - _basic_type_bits; j >= 0; j -= _basic_type_bits) {\n-        unsigned v = (val >> j) & _basic_type_mask;\n-        if (v == 0) {\n-          continue;\n-        }\n-        function(v);\n-      }\n+      function(element_at(i));\n@@ -2336,3 +2477,2 @@\n- public:\n-  static AdapterFingerPrint* allocate(int total_args_passed, BasicType* sig_bt) {\n-    int len = length(total_args_passed);\n+  static AdapterFingerPrint* allocate(const GrowableArray<SigEntry>* sig, bool has_ro_adapter = false) {\n+    int len = total_args_passed_in_sig(sig);\n@@ -2340,1 +2480,1 @@\n-    AdapterFingerPrint* afp = new (size_in_bytes) AdapterFingerPrint(total_args_passed, sig_bt, len);\n+    AdapterFingerPrint* afp = new (size_in_bytes) AdapterFingerPrint(sig, has_ro_adapter);\n@@ -2349,3 +2489,2 @@\n-  int value(int index) {\n-    int* data = data_pointer();\n-    return data[index];\n+  bool has_ro_adapter() const {\n+    return _has_ro_adapter;\n@@ -2354,1 +2493,1 @@\n-  int length() {\n+  int length() const {\n@@ -2361,1 +2500,1 @@\n-      int v = value(i);\n+      const Element& v = element_at(i);\n@@ -2363,1 +2502,1 @@\n-      hash = ((hash << 8) ^ v ^ (hash >> 5)) + 3;\n+      hash = ((hash << 8) ^ v.hash() ^ (hash >> 5)) + 3;\n@@ -2370,1 +2509,6 @@\n-    st.print(\"0x\");\n+    st.print(\"{\");\n+    if (_has_ro_adapter) {\n+      st.print(\"has_ro_adapter\");\n+    } else {\n+      st.print(\"no_ro_adapter\");\n+    }\n@@ -2372,1 +2516,3 @@\n-      st.print(\"%x\", value(i));\n+      st.print(\", \");\n+      const Element& elem = element_at(i);\n+      st.print(\"{%s, %d}\", type2name(elem.bt()), elem.offset());\n@@ -2374,0 +2520,1 @@\n+    st.print(\"}\");\n@@ -2380,1 +2527,1 @@\n-    iterate_args([&] (int arg) {\n+    iterate_args([&] (const Element& arg) {\n@@ -2383,1 +2530,1 @@\n-        if (arg == T_VOID) {\n+        if (arg.bt() == T_VOID) {\n@@ -2389,7 +2536,4 @@\n-      switch (arg) {\n-        case T_INT:    st.print(\"I\");    break;\n-        case T_LONG:   long_prev = true; break;\n-        case T_FLOAT:  st.print(\"F\");    break;\n-        case T_DOUBLE: st.print(\"D\");    break;\n-        case T_VOID:   break;\n-        default: ShouldNotReachHere();\n+      if (arg.bt() == T_LONG) {\n+        long_prev = true;\n+      } else if (arg.bt() != T_VOID) {\n+        st.print(\"%c\", type2char(arg.bt()));\n@@ -2404,52 +2548,3 @@\n-  BasicType* as_basic_type(int& nargs) {\n-    nargs = 0;\n-    GrowableArray<BasicType> btarray;\n-    bool long_prev = false;\n-\n-    iterate_args([&] (int arg) {\n-      if (long_prev) {\n-        long_prev = false;\n-        if (arg == T_VOID) {\n-          btarray.append(T_LONG);\n-        } else {\n-          btarray.append(T_OBJECT); \/\/ it could be T_ARRAY; it shouldn't matter\n-        }\n-      }\n-      switch (arg) {\n-        case T_INT: \/\/ fallthrough\n-        case T_FLOAT: \/\/ fallthrough\n-        case T_DOUBLE:\n-        case T_VOID:\n-          btarray.append((BasicType)arg);\n-          break;\n-        case T_LONG:\n-          long_prev = true;\n-          break;\n-        default: ShouldNotReachHere();\n-      }\n-    });\n-\n-    if (long_prev) {\n-      btarray.append(T_OBJECT);\n-    }\n-\n-    nargs = btarray.length();\n-    BasicType* sig_bt = NEW_RESOURCE_ARRAY(BasicType, nargs);\n-    int index = 0;\n-    GrowableArrayIterator<BasicType> iter = btarray.begin();\n-    while (iter != btarray.end()) {\n-      sig_bt[index++] = *iter;\n-      ++iter;\n-    }\n-    assert(index == btarray.length(), \"sanity check\");\n-#ifdef ASSERT\n-    {\n-      AdapterFingerPrint* compare_fp = AdapterFingerPrint::allocate(nargs, sig_bt);\n-      assert(this->equals(compare_fp), \"sanity check\");\n-      AdapterFingerPrint::deallocate(compare_fp);\n-    }\n-#endif\n-    return sig_bt;\n-  }\n-\n-    if (other->_length != _length) {\n+    if (other->_has_ro_adapter != _has_ro_adapter) {\n+      return false;\n+    } else if (other->_length != _length) {\n@@ -2460,1 +2555,1 @@\n-        if (value(i) != other->value(i)) {\n+        if (element_at(i) != other->element_at(i)) {\n@@ -2503,1 +2598,1 @@\n-AdapterHandlerEntry* AdapterHandlerLibrary::lookup(int total_args_passed, BasicType* sig_bt) {\n+AdapterHandlerEntry* AdapterHandlerLibrary::lookup(const GrowableArray<SigEntry>* sig, bool has_ro_adapter) {\n@@ -2506,1 +2601,1 @@\n-  AdapterFingerPrint* fp = AdapterFingerPrint::allocate(total_args_passed, sig_bt);\n+  AdapterFingerPrint* fp = AdapterFingerPrint::allocate(sig, has_ro_adapter);\n@@ -2563,1 +2658,1 @@\n-static const int AdapterHandlerLibrary_size = 16*K;\n+static const int AdapterHandlerLibrary_size = 48*K;\n@@ -2611,1 +2706,3 @@\n-    _no_arg_handler = create_adapter(0, nullptr);\n+    CompiledEntrySignature no_args;\n+    no_args.compute_calling_conventions();\n+    _no_arg_handler = create_adapter(no_args, true);\n@@ -2613,2 +2710,4 @@\n-    BasicType obj_args[] = { T_OBJECT };\n-    _obj_arg_handler = create_adapter(1, obj_args);\n+    CompiledEntrySignature obj_args;\n+    SigEntry::add_entry(obj_args.sig(), T_OBJECT);\n+    obj_args.compute_calling_conventions();\n+    _obj_arg_handler = create_adapter(obj_args, true);\n@@ -2616,2 +2715,4 @@\n-    BasicType int_args[] = { T_INT };\n-    _int_arg_handler = create_adapter(1, int_args);\n+    CompiledEntrySignature int_args;\n+    SigEntry::add_entry(int_args.sig(), T_INT);\n+    int_args.compute_calling_conventions();\n+    _int_arg_handler = create_adapter(int_args, true);\n@@ -2619,2 +2720,5 @@\n-    BasicType obj_int_args[] = { T_OBJECT, T_INT };\n-    _obj_int_arg_handler = create_adapter(2, obj_int_args);\n+    CompiledEntrySignature obj_int_args;\n+    SigEntry::add_entry(obj_int_args.sig(), T_OBJECT);\n+    SigEntry::add_entry(obj_int_args.sig(), T_INT);\n+    obj_int_args.compute_calling_conventions();\n+    _obj_int_arg_handler = create_adapter(obj_int_args, true);\n@@ -2622,2 +2726,5 @@\n-    BasicType obj_obj_args[] = { T_OBJECT, T_OBJECT };\n-    _obj_obj_arg_handler = create_adapter(2, obj_obj_args);\n+    CompiledEntrySignature obj_obj_args;\n+    SigEntry::add_entry(obj_obj_args.sig(), T_OBJECT);\n+    SigEntry::add_entry(obj_obj_args.sig(), T_OBJECT);\n+    obj_obj_args.compute_calling_conventions();\n+    _obj_obj_arg_handler = create_adapter(obj_obj_args, true);\n@@ -2657,0 +2764,3 @@\n+      if (InlineTypePassFieldsAsArgs && method->method_holder()->is_inline_klass()) {\n+        return nullptr;\n+      }\n@@ -2660,1 +2770,10 @@\n-      case JVM_SIGNATURE_CLASS:\n+      case JVM_SIGNATURE_CLASS: {\n+        if (InlineTypePassFieldsAsArgs) {\n+          SignatureStream ss(method->signature());\n+          InlineKlass* vk = ss.as_inline_klass(method->method_holder());\n+          if (vk != nullptr) {\n+            return nullptr;\n+          }\n+        }\n+        return _obj_arg_handler;\n+      }\n@@ -2671,1 +2790,1 @@\n-             !method->is_static()) {\n+             !method->is_static() && (!InlineTypePassFieldsAsArgs || !method->method_holder()->is_inline_klass())) {\n@@ -2673,1 +2792,10 @@\n-      case JVM_SIGNATURE_CLASS:\n+      case JVM_SIGNATURE_CLASS: {\n+        if (InlineTypePassFieldsAsArgs) {\n+          SignatureStream ss(method->signature());\n+          InlineKlass* vk = ss.as_inline_klass(method->method_holder());\n+          if (vk != nullptr) {\n+            return nullptr;\n+          }\n+        }\n+        return _obj_obj_arg_handler;\n+      }\n@@ -2687,5 +2815,9 @@\n-class AdapterSignatureIterator : public SignatureIterator {\n- private:\n-  BasicType stack_sig_bt[16];\n-  BasicType* sig_bt;\n-  int index;\n+CompiledEntrySignature::CompiledEntrySignature(Method* method) :\n+  _method(method), _num_inline_args(0), _has_inline_recv(false),\n+  _regs(nullptr), _regs_cc(nullptr), _regs_cc_ro(nullptr),\n+  _args_on_stack(0), _args_on_stack_cc(0), _args_on_stack_cc_ro(0),\n+  _c1_needs_stack_repair(false), _c2_needs_stack_repair(false), _supers(nullptr) {\n+  _sig = new GrowableArray<SigEntry>((method != nullptr) ? method->size_of_parameters() : 1);\n+  _sig_cc = new GrowableArray<SigEntry>((method != nullptr) ? method->size_of_parameters() : 1);\n+  _sig_cc_ro = new GrowableArray<SigEntry>((method != nullptr) ? method->size_of_parameters() : 1);\n+}\n@@ -2693,11 +2825,24 @@\n- public:\n-  AdapterSignatureIterator(Symbol* signature,\n-                           fingerprint_t fingerprint,\n-                           bool is_static,\n-                           int total_args_passed) :\n-    SignatureIterator(signature, fingerprint),\n-    index(0)\n-  {\n-    sig_bt = (total_args_passed <= 16) ? stack_sig_bt : NEW_RESOURCE_ARRAY(BasicType, total_args_passed);\n-    if (!is_static) { \/\/ Pass in receiver first\n-      sig_bt[index++] = T_OBJECT;\n+\/\/ See if we can save space by sharing the same entry for VIEP and VIEP(RO),\n+\/\/ or the same entry for VEP and VIEP(RO).\n+CodeOffsets::Entries CompiledEntrySignature::c1_inline_ro_entry_type() const {\n+  if (!has_scalarized_args()) {\n+    \/\/ VEP\/VIEP\/VIEP(RO) all share the same entry. There's no packing.\n+    return CodeOffsets::Verified_Entry;\n+  }\n+  if (_method->is_static()) {\n+    \/\/ Static methods don't need VIEP(RO)\n+    return CodeOffsets::Verified_Entry;\n+  }\n+\n+  if (has_inline_recv()) {\n+    if (num_inline_args() == 1) {\n+      \/\/ Share same entry for VIEP and VIEP(RO).\n+      \/\/ This is quite common: we have an instance method in an InlineKlass that has\n+      \/\/ no inline type args other than <this>.\n+      return CodeOffsets::Verified_Inline_Entry;\n+    } else {\n+      assert(num_inline_args() > 1, \"must be\");\n+      \/\/ No sharing:\n+      \/\/   VIEP(RO) -- <this> is passed as object\n+      \/\/   VEP      -- <this> is passed as fields\n+      return CodeOffsets::Verified_Inline_Entry_RO;\n@@ -2705,1 +2850,0 @@\n-    do_parameters_on(this);\n@@ -2708,2 +2852,9 @@\n-  BasicType* basic_types() {\n-    return sig_bt;\n+  \/\/ Either a static method, or <this> is not an inline type\n+  if (args_on_stack_cc() != args_on_stack_cc_ro()) {\n+    \/\/ No sharing:\n+    \/\/ Some arguments are passed on the stack, and we have inserted reserved entries\n+    \/\/ into the VEP, but we never insert reserved entries into the VIEP(RO).\n+    return CodeOffsets::Verified_Inline_Entry_RO;\n+  } else {\n+    \/\/ Share same entry for VEP and VIEP(RO).\n+    return CodeOffsets::Verified_Entry;\n@@ -2711,0 +2862,1 @@\n+}\n@@ -2712,3 +2864,39 @@\n-#ifdef ASSERT\n-  int slots() {\n-    return index;\n+\/\/ Returns all super methods (transitive) in classes and interfaces that are overridden by the current method.\n+GrowableArray<Method*>* CompiledEntrySignature::get_supers() {\n+  if (_supers != nullptr) {\n+    return _supers;\n+  }\n+  _supers = new GrowableArray<Method*>();\n+  \/\/ Skip private, static, and <init> methods\n+  if (_method->is_private() || _method->is_static() || _method->is_object_constructor()) {\n+    return _supers;\n+  }\n+  Symbol* name = _method->name();\n+  Symbol* signature = _method->signature();\n+  const Klass* holder = _method->method_holder()->super();\n+  Symbol* holder_name = holder->name();\n+  ThreadInVMfromUnknown tiv;\n+  JavaThread* current = JavaThread::current();\n+  HandleMark hm(current);\n+  Handle loader(current, _method->method_holder()->class_loader());\n+\n+  \/\/ Walk up the class hierarchy and search for super methods\n+  while (holder != nullptr) {\n+    Method* super_method = holder->lookup_method(name, signature);\n+    if (super_method == nullptr) {\n+      break;\n+    }\n+    if (!super_method->is_static() && !super_method->is_private() &&\n+        (!super_method->is_package_private() ||\n+         super_method->method_holder()->is_same_class_package(loader(), holder_name))) {\n+      _supers->push(super_method);\n+    }\n+    holder = super_method->method_holder()->super();\n+  }\n+  \/\/ Search interfaces for super methods\n+  Array<InstanceKlass*>* interfaces = _method->method_holder()->transitive_interfaces();\n+  for (int i = 0; i < interfaces->length(); ++i) {\n+    Method* m = interfaces->at(i)->lookup_method(name, signature);\n+    if (m != nullptr && !m->is_static() && m->is_public()) {\n+      _supers->push(m);\n+    }\n@@ -2716,0 +2904,50 @@\n+  return _supers;\n+}\n+\n+\/\/ Iterate over arguments and compute scalarized and non-scalarized signatures\n+void CompiledEntrySignature::compute_calling_conventions(bool init) {\n+  bool has_scalarized = false;\n+  if (_method != nullptr) {\n+    InstanceKlass* holder = _method->method_holder();\n+    int arg_num = 0;\n+    if (!_method->is_static()) {\n+      \/\/ We shouldn't scalarize 'this' in a value class constructor\n+      if (holder->is_inline_klass() && InlineKlass::cast(holder)->can_be_passed_as_fields() && !_method->is_object_constructor() &&\n+          (init || _method->is_scalarized_arg(arg_num))) {\n+        _sig_cc->appendAll(InlineKlass::cast(holder)->extended_sig());\n+        has_scalarized = true;\n+        _has_inline_recv = true;\n+        _num_inline_args++;\n+      } else {\n+        SigEntry::add_entry(_sig_cc, T_OBJECT, holder->name());\n+      }\n+      SigEntry::add_entry(_sig, T_OBJECT, holder->name());\n+      SigEntry::add_entry(_sig_cc_ro, T_OBJECT, holder->name());\n+      arg_num++;\n+    }\n+    for (SignatureStream ss(_method->signature()); !ss.at_return_type(); ss.next()) {\n+      BasicType bt = ss.type();\n+      if (bt == T_OBJECT) {\n+        InlineKlass* vk = ss.as_inline_klass(holder);\n+        if (vk != nullptr && vk->can_be_passed_as_fields() && (init || _method->is_scalarized_arg(arg_num))) {\n+          \/\/ Check for a calling convention mismatch with super method(s)\n+          bool scalar_super = false;\n+          bool non_scalar_super = false;\n+          GrowableArray<Method*>* supers = get_supers();\n+          for (int i = 0; i < supers->length(); ++i) {\n+            Method* super_method = supers->at(i);\n+            if (super_method->is_scalarized_arg(arg_num)) {\n+              scalar_super = true;\n+            } else {\n+              non_scalar_super = true;\n+            }\n+          }\n+#ifdef ASSERT\n+          \/\/ Randomly enable below code paths for stress testing\n+          bool stress = init && StressCallingConvention;\n+          if (stress && (os::random() & 1) == 1) {\n+            non_scalar_super = true;\n+            if ((os::random() & 1) == 1) {\n+              scalar_super = true;\n+            }\n+          }\n@@ -2717,0 +2955,50 @@\n+          if (non_scalar_super) {\n+            \/\/ Found a super method with a non-scalarized argument. Fall back to the non-scalarized calling convention.\n+            if (scalar_super) {\n+              \/\/ Found non-scalar *and* scalar super methods. We can't handle both.\n+              \/\/ Mark the scalar method as mismatch and re-compile call sites to use non-scalarized calling convention.\n+              for (int i = 0; i < supers->length(); ++i) {\n+                Method* super_method = supers->at(i);\n+                if (super_method->is_scalarized_arg(arg_num) DEBUG_ONLY(|| (stress && (os::random() & 1) == 1))) {\n+                  super_method->set_mismatch();\n+                  MutexLocker ml(Compile_lock, Mutex::_safepoint_check_flag);\n+                  JavaThread* thread = JavaThread::current();\n+                  HandleMark hm(thread);\n+                  methodHandle mh(thread, super_method);\n+                  DeoptimizationScope deopt_scope;\n+                  CodeCache::mark_for_deoptimization(&deopt_scope, mh());\n+                  deopt_scope.deoptimize_marked();\n+                }\n+              }\n+            }\n+            \/\/ Fall back to non-scalarized calling convention\n+            SigEntry::add_entry(_sig_cc, T_OBJECT, ss.as_symbol());\n+            SigEntry::add_entry(_sig_cc_ro, T_OBJECT, ss.as_symbol());\n+          } else {\n+            _num_inline_args++;\n+            has_scalarized = true;\n+            int last = _sig_cc->length();\n+            int last_ro = _sig_cc_ro->length();\n+            _sig_cc->appendAll(vk->extended_sig());\n+            _sig_cc_ro->appendAll(vk->extended_sig());\n+            if (bt == T_OBJECT) {\n+              \/\/ Nullable inline type argument, insert InlineTypeNode::NullMarker field right after T_METADATA delimiter\n+              _sig_cc->insert_before(last+1, SigEntry(T_BOOLEAN, -1, nullptr, true));\n+              _sig_cc_ro->insert_before(last_ro+1, SigEntry(T_BOOLEAN, -1, nullptr, true));\n+            }\n+          }\n+        } else {\n+          SigEntry::add_entry(_sig_cc, T_OBJECT, ss.as_symbol());\n+          SigEntry::add_entry(_sig_cc_ro, T_OBJECT, ss.as_symbol());\n+        }\n+        bt = T_OBJECT;\n+      } else {\n+        SigEntry::add_entry(_sig_cc, ss.type(), ss.as_symbol());\n+        SigEntry::add_entry(_sig_cc_ro, ss.type(), ss.as_symbol());\n+      }\n+      SigEntry::add_entry(_sig, bt, ss.as_symbol());\n+      if (bt != T_VOID) {\n+        arg_num++;\n+      }\n+    }\n+  }\n@@ -2718,1 +3006,3 @@\n- private:\n+  \/\/ Compute the non-scalarized calling convention\n+  _regs = NEW_RESOURCE_ARRAY(VMRegPair, _sig->length());\n+  _args_on_stack = SharedRuntime::java_calling_convention(_sig, _regs);\n@@ -2720,5 +3010,16 @@\n-  friend class SignatureIterator;  \/\/ so do_parameters_on can call do_type\n-  void do_type(BasicType type) {\n-    sig_bt[index++] = type;\n-    if (type == T_LONG || type == T_DOUBLE) {\n-      sig_bt[index++] = T_VOID; \/\/ Longs & doubles take 2 Java slots\n+  \/\/ Compute the scalarized calling conventions if there are scalarized inline types in the signature\n+  if (has_scalarized && !_method->is_native()) {\n+    _regs_cc = NEW_RESOURCE_ARRAY(VMRegPair, _sig_cc->length());\n+    _args_on_stack_cc = SharedRuntime::java_calling_convention(_sig_cc, _regs_cc);\n+\n+    _regs_cc_ro = NEW_RESOURCE_ARRAY(VMRegPair, _sig_cc_ro->length());\n+    _args_on_stack_cc_ro = SharedRuntime::java_calling_convention(_sig_cc_ro, _regs_cc_ro);\n+\n+    _c1_needs_stack_repair = (_args_on_stack_cc < _args_on_stack) || (_args_on_stack_cc_ro < _args_on_stack);\n+    _c2_needs_stack_repair = (_args_on_stack_cc > _args_on_stack) || (_args_on_stack_cc > _args_on_stack_cc_ro);\n+\n+    \/\/ Upper bound on stack arguments to avoid hitting the argument limit and\n+    \/\/ bailing out of compilation (\"unsupported incoming calling sequence\").\n+    \/\/ TODO we need a reasonable limit (flag?) here\n+    if (MAX2(_args_on_stack_cc, _args_on_stack_cc_ro) <= 60) {\n+      return; \/\/ Success\n@@ -2727,1 +3028,130 @@\n-};\n+  \/\/ No scalarized args\n+  _sig_cc = _sig;\n+  _regs_cc = _regs;\n+  _args_on_stack_cc = _args_on_stack;\n+\n+  _sig_cc_ro = _sig;\n+  _regs_cc_ro = _regs;\n+  _args_on_stack_cc_ro = _args_on_stack;\n+}\n+\n+void CompiledEntrySignature::initialize_from_fingerprint(AdapterFingerPrint* fingerprint) {\n+  _has_inline_recv = fingerprint->has_ro_adapter();\n+\n+  int value_object_count = 0;\n+  BasicType prev_bt = T_ILLEGAL;\n+  bool has_scalarized_arguments = false;\n+  bool long_prev = false;\n+  int long_prev_offset = -1;\n+\n+  fingerprint->iterate_args([&] (const AdapterFingerPrint::Element& arg) {\n+    BasicType bt = arg.bt();\n+    int offset = arg.offset();\n+\n+    if (long_prev) {\n+      long_prev = false;\n+      BasicType bt_to_add;\n+      if (bt == T_VOID) {\n+        bt_to_add = T_LONG;\n+      } else {\n+        bt_to_add = T_OBJECT;\n+      }\n+      if (value_object_count == 0) {\n+        SigEntry::add_entry(_sig, bt_to_add);\n+      }\n+      SigEntry::add_entry(_sig_cc, bt_to_add, nullptr, long_prev_offset);\n+      SigEntry::add_entry(_sig_cc_ro, bt_to_add, nullptr, long_prev_offset);\n+    }\n+\n+    switch (bt) {\n+      case T_VOID:\n+        if (prev_bt != T_LONG && prev_bt != T_DOUBLE) {\n+          assert(InlineTypePassFieldsAsArgs, \"unexpected end of inline type\");\n+          value_object_count--;\n+          SigEntry::add_entry(_sig_cc, T_VOID, nullptr, offset);\n+          SigEntry::add_entry(_sig_cc_ro, T_VOID, nullptr, offset);\n+          assert(value_object_count >= 0, \"invalid value object count\");\n+        } else {\n+          \/\/ Nothing to add for _sig: We already added an addition T_VOID in add_entry() when adding T_LONG or T_DOUBLE.\n+        }\n+        break;\n+      case T_INT:\n+      case T_FLOAT:\n+      case T_DOUBLE:\n+        if (value_object_count == 0) {\n+          SigEntry::add_entry(_sig, bt);\n+        }\n+        SigEntry::add_entry(_sig_cc, bt, nullptr, offset);\n+        SigEntry::add_entry(_sig_cc_ro, bt, nullptr, offset);\n+        break;\n+      case T_LONG:\n+        long_prev = true;\n+        long_prev_offset = offset;\n+        break;\n+      case T_BOOLEAN:\n+      case T_CHAR:\n+      case T_BYTE:\n+      case T_SHORT:\n+      case T_OBJECT:\n+      case T_ARRAY:\n+        assert(value_object_count > 0, \"must be value object field\");\n+        SigEntry::add_entry(_sig_cc, bt, nullptr, offset);\n+        SigEntry::add_entry(_sig_cc_ro, bt, nullptr, offset);\n+        break;\n+      case T_METADATA:\n+        assert(InlineTypePassFieldsAsArgs, \"unexpected start of inline type\");\n+        if (value_object_count == 0) {\n+          SigEntry::add_entry(_sig, T_OBJECT);\n+        }\n+        SigEntry::add_entry(_sig_cc, T_METADATA, nullptr, offset);\n+        SigEntry::add_entry(_sig_cc_ro, T_METADATA, nullptr, offset);\n+        value_object_count++;\n+        has_scalarized_arguments = true;\n+        break;\n+      default: {\n+        fatal(\"Unexpected BasicType: %s\", basictype_to_str(bt));\n+      }\n+    }\n+    prev_bt = bt;\n+  });\n+\n+  if (long_prev) {\n+    \/\/ If previous bt was T_LONG and we reached the end of the signature, we know that it must be a T_OBJECT.\n+    SigEntry::add_entry(_sig, T_OBJECT);\n+    SigEntry::add_entry(_sig_cc, T_OBJECT);\n+    SigEntry::add_entry(_sig_cc_ro, T_OBJECT);\n+  }\n+  assert(value_object_count == 0, \"invalid value object count\");\n+\n+  _regs = NEW_RESOURCE_ARRAY(VMRegPair, _sig->length());\n+  _args_on_stack = SharedRuntime::java_calling_convention(_sig, _regs);\n+\n+  \/\/ Compute the scalarized calling conventions if there are scalarized inline types in the signature\n+  if (has_scalarized_arguments) {\n+    _regs_cc = NEW_RESOURCE_ARRAY(VMRegPair, _sig_cc->length());\n+    _args_on_stack_cc = SharedRuntime::java_calling_convention(_sig_cc, _regs_cc);\n+\n+    _regs_cc_ro = NEW_RESOURCE_ARRAY(VMRegPair, _sig_cc_ro->length());\n+    _args_on_stack_cc_ro = SharedRuntime::java_calling_convention(_sig_cc_ro, _regs_cc_ro);\n+\n+    _c1_needs_stack_repair = (_args_on_stack_cc < _args_on_stack) || (_args_on_stack_cc_ro < _args_on_stack);\n+    _c2_needs_stack_repair = (_args_on_stack_cc > _args_on_stack) || (_args_on_stack_cc > _args_on_stack_cc_ro);\n+  } else {\n+    \/\/ No scalarized args\n+    _sig_cc = _sig;\n+    _regs_cc = _regs;\n+    _args_on_stack_cc = _args_on_stack;\n+\n+    _sig_cc_ro = _sig;\n+    _regs_cc_ro = _regs;\n+    _args_on_stack_cc_ro = _args_on_stack;\n+  }\n+\n+#ifdef ASSERT\n+  {\n+    AdapterFingerPrint* compare_fp = AdapterFingerPrint::allocate(_sig_cc, _has_inline_recv);\n+    assert(fingerprint->equals(compare_fp), \"%s - %s\", fingerprint->as_string(), compare_fp->as_string());\n+    AdapterFingerPrint::deallocate(compare_fp);\n+  }\n+#endif\n+}\n@@ -2735,1 +3165,1 @@\n-void AdapterHandlerLibrary::verify_adapter_sharing(int total_args_passed, BasicType* sig_bt, AdapterHandlerEntry* cached_entry) {\n+void AdapterHandlerLibrary::verify_adapter_sharing(CompiledEntrySignature& ces, AdapterHandlerEntry* cached_entry) {\n@@ -2738,1 +3168,1 @@\n-  AdapterHandlerEntry* comparison_entry = create_adapter(total_args_passed, sig_bt, true);\n+  AdapterHandlerEntry* comparison_entry = create_adapter(ces, false, true);\n@@ -2763,2 +3193,13 @@\n-  \/\/ Fill in the signature array, for the calling-convention call.\n-  int total_args_passed = method->size_of_parameters(); \/\/ All args on stack\n+  CompiledEntrySignature ces(method());\n+  ces.compute_calling_conventions();\n+  if (ces.has_scalarized_args()) {\n+    if (!method->has_scalarized_args()) {\n+      method->set_has_scalarized_args();\n+    }\n+    if (ces.c1_needs_stack_repair()) {\n+      method->set_c1_needs_stack_repair();\n+    }\n+    if (ces.c2_needs_stack_repair() && !method->c2_needs_stack_repair()) {\n+      method->set_c2_needs_stack_repair();\n+    }\n+  }\n@@ -2766,4 +3207,0 @@\n-  AdapterSignatureIterator si(method->signature(), method->constMethod()->fingerprint(),\n-                              method->is_static(), total_args_passed);\n-  assert(si.slots() == total_args_passed, \"\");\n-  BasicType* sig_bt = si.basic_types();\n@@ -2774,1 +3211,1 @@\n-    entry = lookup(total_args_passed, sig_bt);\n+    entry = lookup(ces.sig_cc(), ces.has_inline_recv());\n@@ -2782,1 +3219,1 @@\n-        verify_adapter_sharing(total_args_passed, sig_bt, entry);\n+        verify_adapter_sharing(ces, entry);\n@@ -2786,1 +3223,1 @@\n-      entry = create_adapter(total_args_passed, sig_bt);\n+      entry = create_adapter(ces, \/* allocate_code_blob *\/ true);\n@@ -2841,0 +3278,2 @@\n+  entry_offset[AdapterBlob::C2I_Inline] = entry_address[AdapterBlob::C2I_Inline] - entry_address[AdapterBlob::I2C];\n+  entry_offset[AdapterBlob::C2I_Inline_RO] = entry_address[AdapterBlob::C2I_Inline_RO] - entry_address[AdapterBlob::I2C];\n@@ -2842,0 +3281,1 @@\n+  entry_offset[AdapterBlob::C2I_Unverified_Inline] = entry_address[AdapterBlob::C2I_Unverified_Inline] - entry_address[AdapterBlob::I2C];\n@@ -2850,2 +3290,2 @@\n-                                                  int total_args_passed,\n-                                                  BasicType* sig_bt,\n+                                                  CompiledEntrySignature& ces,\n+                                                  bool allocate_code_blob,\n@@ -2858,0 +3298,1 @@\n+  AdapterBlob* adapter_blob = nullptr;\n@@ -2864,2 +3305,1 @@\n-  VMRegPair stack_regs[16];\n-  VMRegPair* regs = (total_args_passed <= 16) ? stack_regs : NEW_RESOURCE_ARRAY(VMRegPair, total_args_passed);\n+  address entry_address[AdapterBlob::ENTRY_COUNT];\n@@ -2868,7 +3308,17 @@\n-  int comp_args_on_stack = SharedRuntime::java_calling_convention(sig_bt, regs, total_args_passed);\n-  address entry_address[AdapterBlob::ENTRY_COUNT];\n-                                         total_args_passed,\n-                                         comp_args_on_stack,\n-                                         sig_bt,\n-                                         regs,\n-                                         entry_address);\n+                                         ces.args_on_stack(),\n+                                         ces.sig(),\n+                                         ces.regs(),\n+                                         ces.sig_cc(),\n+                                         ces.regs_cc(),\n+                                         ces.sig_cc_ro(),\n+                                         ces.regs_cc_ro(),\n+                                         entry_address,\n+                                         adapter_blob,\n+                                         allocate_code_blob);\n+\n+  if (ces.has_scalarized_args()) {\n+    \/\/ Save a C heap allocated version of the scalarized signature and store it in the adapter\n+    GrowableArray<SigEntry>* heap_sig = new (mtInternal) GrowableArray<SigEntry>(ces.sig_cc()->length(), mtInternal);\n+    heap_sig->appendAll(ces.sig_cc());\n+    handler->set_sig_cc(heap_sig);\n+  }\n@@ -2888,1 +3338,0 @@\n-  AdapterBlob* adapter_blob = AdapterBlob::create(&buffer, entry_offset);\n@@ -2915,2 +3364,2 @@\n-AdapterHandlerEntry* AdapterHandlerLibrary::create_adapter(int total_args_passed,\n-                                                           BasicType* sig_bt,\n+AdapterHandlerEntry* AdapterHandlerLibrary::create_adapter(CompiledEntrySignature& ces,\n+                                                           bool allocate_code_blob,\n@@ -2918,1 +3367,6 @@\n-  AdapterFingerPrint* fp = AdapterFingerPrint::allocate(total_args_passed, sig_bt);\n+  AdapterFingerPrint* fp = AdapterFingerPrint::allocate(ces.sig_cc(), ces.has_inline_recv());\n+#ifdef ASSERT\n+  \/\/ Verify that we can successfully restore the compiled entry signature object.\n+  CompiledEntrySignature ces_verify;\n+  ces_verify.initialize_from_fingerprint(fp);\n+#endif\n@@ -2920,1 +3374,1 @@\n-  if (!generate_adapter_code(handler, total_args_passed, sig_bt, is_transient)) {\n+  if (!generate_adapter_code(handler, ces, allocate_code_blob, is_transient)) {\n@@ -3023,3 +3477,3 @@\n-    int nargs;\n-    BasicType* bt = _fingerprint->as_basic_type(nargs);\n-    if (!AdapterHandlerLibrary::generate_adapter_code(this, nargs, bt, \/* is_transient *\/ false)) {\n+    CompiledEntrySignature ces;\n+    ces.initialize_from_fingerprint(_fingerprint);\n+    if (!AdapterHandlerLibrary::generate_adapter_code(this, ces, true, false)) {\n@@ -3063,13 +3517,26 @@\n-  _no_arg_handler = lookup(0, nullptr);\n-\n-  BasicType obj_args[] = { T_OBJECT };\n-  _obj_arg_handler = lookup(1, obj_args);\n-\n-  BasicType int_args[] = { T_INT };\n-  _int_arg_handler = lookup(1, int_args);\n-\n-  BasicType obj_int_args[] = { T_OBJECT, T_INT };\n-  _obj_int_arg_handler = lookup(2, obj_int_args);\n-\n-  BasicType obj_obj_args[] = { T_OBJECT, T_OBJECT };\n-  _obj_obj_arg_handler = lookup(2, obj_obj_args);\n+  ResourceMark rm;\n+  CompiledEntrySignature no_args;\n+  no_args.compute_calling_conventions();\n+  _no_arg_handler = lookup(no_args.sig_cc(), no_args.has_inline_recv());\n+\n+  CompiledEntrySignature obj_args;\n+  SigEntry::add_entry(obj_args.sig(), T_OBJECT);\n+  obj_args.compute_calling_conventions();\n+  _obj_arg_handler = lookup(obj_args.sig_cc(), obj_args.has_inline_recv());\n+\n+  CompiledEntrySignature int_args;\n+  SigEntry::add_entry(int_args.sig(), T_INT);\n+  int_args.compute_calling_conventions();\n+  _int_arg_handler = lookup(int_args.sig_cc(), int_args.has_inline_recv());\n+\n+  CompiledEntrySignature obj_int_args;\n+  SigEntry::add_entry(obj_int_args.sig(), T_OBJECT);\n+  SigEntry::add_entry(obj_int_args.sig(), T_INT);\n+  obj_int_args.compute_calling_conventions();\n+  _obj_int_arg_handler = lookup(obj_int_args.sig_cc(), obj_int_args.has_inline_recv());\n+\n+  CompiledEntrySignature obj_obj_args;\n+  SigEntry::add_entry(obj_obj_args.sig(), T_OBJECT);\n+  SigEntry::add_entry(obj_obj_args.sig(), T_OBJECT);\n+  obj_obj_args.compute_calling_conventions();\n+  _obj_obj_arg_handler = lookup(obj_obj_args.sig_cc(), obj_obj_args.has_inline_recv());\n@@ -3104,0 +3571,3 @@\n+  if (_sig_cc != nullptr) {\n+    delete _sig_cc;\n+  }\n@@ -3191,0 +3661,1 @@\n+      BasicType stack_sig_bt[16];\n@@ -3192,0 +3663,1 @@\n+      BasicType* sig_bt = (total_args_passed <= 16) ? stack_sig_bt : NEW_RESOURCE_ARRAY(BasicType, total_args_passed);\n@@ -3194,5 +3666,13 @@\n-      AdapterSignatureIterator si(method->signature(), method->constMethod()->fingerprint(),\n-                              method->is_static(), total_args_passed);\n-      BasicType* sig_bt = si.basic_types();\n-      assert(si.slots() == total_args_passed, \"\");\n-      BasicType ret_type = si.return_type();\n+      int i = 0;\n+      if (!method->is_static()) {  \/\/ Pass in receiver first\n+        sig_bt[i++] = T_OBJECT;\n+      }\n+      SignatureStream ss(method->signature());\n+      for (; !ss.at_return_type(); ss.next()) {\n+        sig_bt[i++] = ss.type();  \/\/ Collect remaining bits of signature\n+        if (ss.type() == T_LONG || ss.type() == T_DOUBLE) {\n+          sig_bt[i++] = T_VOID;   \/\/ Longs & doubles take 2 Java slots\n+        }\n+      }\n+      assert(i == total_args_passed, \"\");\n+      BasicType ret_type = ss.type();\n@@ -3466,1 +3946,4 @@\n-    st->print(\" c2iUV: \" INTPTR_FORMAT, p2i(get_c2i_unverified_entry()));\n+    st->print(\" c2iVE: \" INTPTR_FORMAT, p2i(get_c2i_inline_entry()));\n+    st->print(\" c2iVROE: \" INTPTR_FORMAT, p2i(get_c2i_inline_ro_entry()));\n+    st->print(\" c2iUE: \" INTPTR_FORMAT, p2i(get_c2i_unverified_entry()));\n+    st->print(\" c2iUVE: \" INTPTR_FORMAT, p2i(get_c2i_unverified_inline_entry()));\n@@ -3553,0 +4036,195 @@\n+\n+\/\/ We are at a compiled code to interpreter call. We need backing\n+\/\/ buffers for all inline type arguments. Allocate an object array to\n+\/\/ hold them (convenient because once we're done with it we don't have\n+\/\/ to worry about freeing it).\n+oop SharedRuntime::allocate_inline_types_impl(JavaThread* current, methodHandle callee, bool allocate_receiver, TRAPS) {\n+  assert(InlineTypePassFieldsAsArgs, \"no reason to call this\");\n+  ResourceMark rm;\n+\n+  int nb_slots = 0;\n+  InstanceKlass* holder = callee->method_holder();\n+  allocate_receiver &= !callee->is_static() && holder->is_inline_klass() && callee->is_scalarized_arg(0);\n+  if (allocate_receiver) {\n+    nb_slots++;\n+  }\n+  int arg_num = callee->is_static() ? 0 : 1;\n+  for (SignatureStream ss(callee->signature()); !ss.at_return_type(); ss.next()) {\n+    BasicType bt = ss.type();\n+    if (bt == T_OBJECT && callee->is_scalarized_arg(arg_num)) {\n+      nb_slots++;\n+    }\n+    if (bt != T_VOID) {\n+      arg_num++;\n+    }\n+  }\n+  objArrayOop array_oop = oopFactory::new_objectArray(nb_slots, CHECK_NULL);\n+  objArrayHandle array(THREAD, array_oop);\n+  arg_num = callee->is_static() ? 0 : 1;\n+  int i = 0;\n+  if (allocate_receiver) {\n+    InlineKlass* vk = InlineKlass::cast(holder);\n+    oop res = vk->allocate_instance(CHECK_NULL);\n+    array->obj_at_put(i++, res);\n+  }\n+  for (SignatureStream ss(callee->signature()); !ss.at_return_type(); ss.next()) {\n+    BasicType bt = ss.type();\n+    if (bt == T_OBJECT && callee->is_scalarized_arg(arg_num)) {\n+      InlineKlass* vk = ss.as_inline_klass(holder);\n+      assert(vk != nullptr, \"Unexpected klass\");\n+      oop res = vk->allocate_instance(CHECK_NULL);\n+      array->obj_at_put(i++, res);\n+    }\n+    if (bt != T_VOID) {\n+      arg_num++;\n+    }\n+  }\n+  return array();\n+}\n+\n+JRT_ENTRY(void, SharedRuntime::allocate_inline_types(JavaThread* current, Method* callee_method, bool allocate_receiver))\n+  methodHandle callee(current, callee_method);\n+  oop array = SharedRuntime::allocate_inline_types_impl(current, callee, allocate_receiver, CHECK);\n+  current->set_vm_result_oop(array);\n+  current->set_vm_result_metadata(callee()); \/\/ TODO: required to keep callee live?\n+JRT_END\n+\n+\/\/ We're returning from an interpreted method: load each field into a\n+\/\/ register following the calling convention\n+JRT_LEAF(void, SharedRuntime::load_inline_type_fields_in_regs(JavaThread* current, oopDesc* res))\n+{\n+  assert(res->klass()->is_inline_klass(), \"only inline types here\");\n+  ResourceMark rm;\n+  RegisterMap reg_map(current,\n+                      RegisterMap::UpdateMap::include,\n+                      RegisterMap::ProcessFrames::include,\n+                      RegisterMap::WalkContinuation::skip);\n+  frame stubFrame = current->last_frame();\n+  frame callerFrame = stubFrame.sender(&reg_map);\n+  assert(callerFrame.is_interpreted_frame(), \"should be coming from interpreter\");\n+\n+  InlineKlass* vk = InlineKlass::cast(res->klass());\n+\n+  const Array<SigEntry>* sig_vk = vk->extended_sig();\n+  const Array<VMRegPair>* regs = vk->return_regs();\n+\n+  if (regs == nullptr) {\n+    \/\/ The fields of the inline klass don't fit in registers, bail out\n+    return;\n+  }\n+\n+  int j = 1;\n+  for (int i = 0; i < sig_vk->length(); i++) {\n+    BasicType bt = sig_vk->at(i)._bt;\n+    if (bt == T_METADATA) {\n+      continue;\n+    }\n+    if (bt == T_VOID) {\n+      if (sig_vk->at(i-1)._bt == T_LONG ||\n+          sig_vk->at(i-1)._bt == T_DOUBLE) {\n+        j++;\n+      }\n+      continue;\n+    }\n+    int off = sig_vk->at(i)._offset;\n+    assert(off > 0, \"offset in object should be positive\");\n+    VMRegPair pair = regs->at(j);\n+    address loc = reg_map.location(pair.first(), nullptr);\n+    switch(bt) {\n+    case T_BOOLEAN:\n+      *(jboolean*)loc = res->bool_field(off);\n+      break;\n+    case T_CHAR:\n+      *(jchar*)loc = res->char_field(off);\n+      break;\n+    case T_BYTE:\n+      *(jbyte*)loc = res->byte_field(off);\n+      break;\n+    case T_SHORT:\n+      *(jshort*)loc = res->short_field(off);\n+      break;\n+    case T_INT: {\n+      *(jint*)loc = res->int_field(off);\n+      break;\n+    }\n+    case T_LONG:\n+#ifdef _LP64\n+      *(intptr_t*)loc = res->long_field(off);\n+#else\n+      Unimplemented();\n+#endif\n+      break;\n+    case T_OBJECT:\n+    case T_ARRAY: {\n+      *(oop*)loc = res->obj_field(off);\n+      break;\n+    }\n+    case T_FLOAT:\n+      *(jfloat*)loc = res->float_field(off);\n+      break;\n+    case T_DOUBLE:\n+      *(jdouble*)loc = res->double_field(off);\n+      break;\n+    default:\n+      ShouldNotReachHere();\n+    }\n+    j++;\n+  }\n+  assert(j == regs->length(), \"missed a field?\");\n+\n+#ifdef ASSERT\n+  VMRegPair pair = regs->at(0);\n+  address loc = reg_map.location(pair.first(), nullptr);\n+  assert(*(oopDesc**)loc == res, \"overwritten object\");\n+#endif\n+\n+  current->set_vm_result_oop(res);\n+}\n+JRT_END\n+\n+\/\/ We've returned to an interpreted method, the interpreter needs a\n+\/\/ reference to an inline type instance. Allocate it and initialize it\n+\/\/ from field's values in registers.\n+JRT_BLOCK_ENTRY(void, SharedRuntime::store_inline_type_fields_to_buf(JavaThread* current, intptr_t res))\n+{\n+  ResourceMark rm;\n+  RegisterMap reg_map(current,\n+                      RegisterMap::UpdateMap::include,\n+                      RegisterMap::ProcessFrames::include,\n+                      RegisterMap::WalkContinuation::skip);\n+  frame stubFrame = current->last_frame();\n+  frame callerFrame = stubFrame.sender(&reg_map);\n+\n+#ifdef ASSERT\n+  InlineKlass* verif_vk = InlineKlass::returned_inline_klass(reg_map);\n+#endif\n+\n+  if (!is_set_nth_bit(res, 0)) {\n+    \/\/ We're not returning with inline type fields in registers (the\n+    \/\/ calling convention didn't allow it for this inline klass)\n+    assert(!Metaspace::contains((void*)res), \"should be oop or pointer in buffer area\");\n+    current->set_vm_result_oop((oopDesc*)res);\n+    assert(verif_vk == nullptr, \"broken calling convention\");\n+    return;\n+  }\n+\n+  clear_nth_bit(res, 0);\n+  InlineKlass* vk = (InlineKlass*)res;\n+  assert(verif_vk == vk, \"broken calling convention\");\n+  assert(Metaspace::contains((void*)res), \"should be klass\");\n+\n+  \/\/ Allocate handles for every oop field so they are safe in case of\n+  \/\/ a safepoint when allocating\n+  GrowableArray<Handle> handles;\n+  vk->save_oop_fields(reg_map, handles);\n+\n+  \/\/ It's unsafe to safepoint until we are here\n+  JRT_BLOCK;\n+  {\n+    JavaThread* THREAD = current;\n+    oop vt = vk->realloc_result(reg_map, handles, CHECK);\n+    current->set_vm_result_oop(vt);\n+  }\n+  JRT_BLOCK_END;\n+}\n+JRT_END\n","filename":"src\/hotspot\/share\/runtime\/sharedRuntime.cpp","additions":935,"deletions":257,"binary":false,"changes":1192,"status":"modified"},{"patch":"@@ -51,0 +51,1 @@\n+  bool _callee_augmented;\n@@ -72,1 +73,1 @@\n-  inline intptr_t* unextended_sp() const { return frame_kind == ChunkFrames::Mixed ? _unextended_sp : _sp; }\n+  inline intptr_t* unextended_sp() const { return _unextended_sp; }\n","filename":"src\/hotspot\/share\/runtime\/stackChunkFrameStream.hpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -58,0 +58,1 @@\n+  _callee_augmented = false;\n@@ -64,0 +65,2 @@\n+  } else {\n+    _unextended_sp = _sp;\n@@ -65,1 +68,0 @@\n-  DEBUG_ONLY(else _unextended_sp = nullptr;)\n@@ -89,0 +91,2 @@\n+  } else {\n+    _unextended_sp = _sp;\n@@ -90,1 +94,0 @@\n-  DEBUG_ONLY(else _unextended_sp = nullptr;)\n@@ -93,0 +96,1 @@\n+  _callee_augmented = false;\n@@ -220,0 +224,1 @@\n+  _callee_augmented = false;\n@@ -228,1 +233,8 @@\n-      _unextended_sp = is_interpreted() ? unextended_sp_for_interpreter_frame() : _sp;\n+      if (is_interpreted()) {\n+        _unextended_sp = unextended_sp_for_interpreter_frame();\n+      } else if (cb()->is_nmethod() && cb()->as_nmethod()->needs_stack_repair()) {\n+        _unextended_sp = frame::repair_sender_sp(cb()->as_nmethod(), _unextended_sp, (intptr_t**)(_sp - frame::sender_sp_offset));\n+        _callee_augmented = _unextended_sp != _sp;\n+      } else {\n+        _unextended_sp = _sp;\n+      }\n@@ -232,1 +244,7 @@\n-    _sp += cb()->frame_size();\n+    _sp = _unextended_sp + cb()->frame_size();\n+    if (cb()->is_nmethod() && cb()->as_nmethod()->needs_stack_repair()) {\n+      _unextended_sp = frame::repair_sender_sp(cb()->as_nmethod(), _unextended_sp, (intptr_t**)(_sp - frame::sender_sp_offset));\n+      _callee_augmented = _unextended_sp != _sp;\n+    } else {\n+      _unextended_sp = _sp;\n+    }\n","filename":"src\/hotspot\/share\/runtime\/stackChunkFrameStream.inline.hpp","additions":22,"deletions":4,"binary":false,"changes":26,"status":"modified"},{"patch":"@@ -40,7 +40,0 @@\n-template StackValue* StackValue::create_stack_value(const frame* fr, const RegisterMap* reg_map, ScopeValue* sv);\n-template StackValue* StackValue::create_stack_value(const frame* fr, const SmallRegisterMapNoArgs* reg_map, ScopeValue* sv);\n-\n-template<typename RegisterMapT>\n-StackValue* StackValue::create_stack_value(const frame* fr, const RegisterMapT* reg_map, ScopeValue* sv) {\n-  return create_stack_value(sv, stack_value_address(fr, reg_map, sv), reg_map);\n-}\n@@ -147,0 +140,4 @@\n+\n+template StackValue* StackValue::create_stack_value(const frame* fr, const RegisterMap* reg_map, ScopeValue* sv);\n+template StackValue* StackValue::create_stack_value(const frame* fr, const SmallRegisterMapNoArgs* reg_map, ScopeValue* sv);\n+\n@@ -148,1 +145,2 @@\n-StackValue* StackValue::create_stack_value(ScopeValue* sv, address value_addr, const RegisterMapT* reg_map) {\n+StackValue* StackValue::create_stack_value(const frame* fr, const RegisterMapT* reg_map, ScopeValue* sv) {\n+  address value_addr = stack_value_address(fr, reg_map, sv);\n@@ -249,1 +247,10 @@\n-    return new StackValue(hdl, hdl.is_null() && ov->is_scalar_replaced() ? 1 : 0);\n+    bool scalar_replaced = hdl.is_null() && ov->is_scalar_replaced();\n+    if (ov->has_properties()) {\n+      Klass* k = java_lang_Class::as_Klass(ov->klass()->as_ConstantOopReadValue()->value()());\n+      if (!k->is_array_klass()) {\n+        \/\/ Don't treat inline type as scalar replaced if it is null\n+        jint null_marker = StackValue::create_stack_value(fr, reg_map, ov->properties())->get_jint();\n+        scalar_replaced &= (null_marker != 0);\n+      }\n+    }\n+    return new StackValue(hdl, scalar_replaced ? 1 : 0);\n","filename":"src\/hotspot\/share\/runtime\/stackValue.cpp","additions":16,"deletions":9,"binary":false,"changes":25,"status":"modified"},{"patch":"@@ -315,0 +315,16 @@\n+\/\/ These checks are required for wait, notify and exit to avoid inflating the monitor to\n+\/\/ find out this inline type object cannot be locked.\n+#define CHECK_THROW_NOSYNC_IMSE(obj)  \\\n+  if ((obj)->mark().is_inline_type()) {  \\\n+    JavaThread* THREAD = current;           \\\n+    ResourceMark rm(THREAD);                \\\n+    THROW_MSG(vmSymbols::java_lang_IllegalMonitorStateException(), obj->klass()->external_name()); \\\n+  }\n+\n+#define CHECK_THROW_NOSYNC_IMSE_0(obj)  \\\n+  if ((obj)->mark().is_inline_type()) {  \\\n+    JavaThread* THREAD = current;             \\\n+    ResourceMark rm(THREAD);                  \\\n+    THROW_MSG_0(vmSymbols::java_lang_IllegalMonitorStateException(), obj->klass()->external_name()); \\\n+  }\n+\n@@ -341,0 +357,1 @@\n+  assert(!obj->klass()->is_inline_klass(), \"monitor op on inline type\");\n@@ -432,0 +449,1 @@\n+  assert(!obj->klass()->is_inline_klass(), \"JITed code should never have locked an instance of a value class\");\n@@ -439,0 +457,1 @@\n+  JavaThread* THREAD = current;\n@@ -447,0 +466,10 @@\n+  if (obj->klass()->is_inline_klass()) {\n+    ResourceMark rm(THREAD);\n+    const char* desc = \"Cannot synchronize on an instance of value class \";\n+    const char* className = obj->klass()->external_name();\n+    size_t msglen = strlen(desc) + strlen(className) + 1;\n+    char* message = NEW_RESOURCE_ARRAY(char, msglen);\n+    assert(message != nullptr, \"NEW_RESOURCE_ARRAY should have called vm_exit_out_of_memory and not return nullptr\");\n+    THROW_MSG(vmSymbols::java_lang_IdentityException(), className);\n+  }\n+\n@@ -464,0 +493,1 @@\n+  CHECK_THROW_NOSYNC_IMSE(obj);\n@@ -524,0 +554,1 @@\n+  CHECK_THROW_NOSYNC_IMSE_0(obj);\n@@ -553,0 +584,1 @@\n+  CHECK_THROW_NOSYNC_IMSE(obj);\n@@ -566,0 +598,1 @@\n+  CHECK_THROW_NOSYNC_IMSE(obj);\n@@ -672,0 +705,3 @@\n+  \/\/ VM should be calling bootstrap method.\n+  assert(!obj->klass()->is_inline_klass(), \"FastHashCode should not be called for inline classes\");\n+\n@@ -773,0 +809,3 @@\n+  if (h_obj->mark().is_inline_type()) {\n+    return false;\n+  }\n","filename":"src\/hotspot\/share\/runtime\/synchronizer.cpp","additions":39,"deletions":0,"binary":false,"changes":39,"status":"modified"},{"patch":"@@ -417,0 +417,1 @@\n+  initialize_class(vmSymbols::java_lang_IdentityException(), CHECK);\n","filename":"src\/hotspot\/share\/runtime\/threads.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -61,0 +61,2 @@\n+#include \"oops\/flatArrayKlass.hpp\"\n+#include \"oops\/inlineKlass.hpp\"\n@@ -942,1 +944,3 @@\n-           declare_type(ObjArrayKlass, ArrayKlass)                        \\\n+           declare_type(ObjArrayKlass, ArrayKlass)                        \\\n+             declare_type(FlatArrayKlass, ArrayKlass)                     \\\n+             declare_type(RefArrayKlass, ArrayKlass)                      \\\n@@ -945,0 +949,1 @@\n+        declare_type(InlineKlass, InstanceKlass)                          \\\n@@ -1417,1 +1422,1 @@\n-  declare_constant(Klass::_lh_array_tag_obj_value)                        \\\n+  declare_constant(Klass::_lh_array_tag_ref_value)                        \\\n","filename":"src\/hotspot\/share\/runtime\/vmStructs.cpp","additions":7,"deletions":2,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -267,0 +267,2 @@\n+#define THREAD_AND_LOCATION_DECL                 TRAPS, const char* file, int line\n+#define THREAD_AND_LOCATION_ARGS                 THREAD, file, line\n","filename":"src\/hotspot\/share\/utilities\/exceptions.hpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -613,0 +613,9 @@\n+\/\/----------------------------------------------------------------------------------------------------\n+\/\/ Prototyping\n+\/\/ \"Code Missing Here\" macro, un-define when integrating back from prototyping stage and break\n+\/\/ compilation on purpose (i.e. \"forget me not\")\n+#define PROTOTYPE\n+#ifdef PROTOTYPE\n+#define CMH(m)\n+#endif\n+\n@@ -691,5 +700,6 @@\n-  T_ADDRESS     = 15,\n-  T_NARROWOOP   = 16,\n-  T_METADATA    = 17,\n-  T_NARROWKLASS = 18,\n-  T_CONFLICT    = 19, \/\/ for stack value type with conflicting contents\n+  T_FLAT_ELEMENT = 15, \/\/ Not a true BasicType, only used in layout helpers of flat arrays\n+  T_ADDRESS     = 16,\n+  T_NARROWOOP   = 17,\n+  T_METADATA    = 18,\n+  T_NARROWKLASS = 19,\n+  T_CONFLICT    = 20, \/\/ for stack value type with conflicting contents\n@@ -739,0 +749,1 @@\n+  assert(t != T_FLAT_ELEMENT, \"\");  \/\/ Strong assert to detect misuses of T_FLAT_ELEMENT\n@@ -813,1 +824,2 @@\n-  T_VOID_size        = 0\n+  T_VOID_size        = 0,\n+  T_FLAT_ELEMENT_size = 0\n@@ -849,1 +861,2 @@\n-  T_VOID_aelem_bytes        = 0\n+  T_VOID_aelem_bytes        = 0,\n+  T_FLAT_ELEMENT_aelem_bytes = 0\n@@ -939,1 +952,1 @@\n-  vtos = 9,             \/\/ tos not cached\n+  vtos = 9,             \/\/ tos not cached,\n@@ -956,1 +969,1 @@\n-    case T_ARRAY  : \/\/ fall through\n+    case T_ARRAY  :   \/\/ fall through\n","filename":"src\/hotspot\/share\/utilities\/globalDefinitions.hpp","additions":22,"deletions":9,"binary":false,"changes":31,"status":"modified"},{"patch":"@@ -67,0 +67,3 @@\n+        @JEP(number=401, title=\"Value Classes and Objects\", status = \"Preview\")\n+        VALUE_OBJECTS,\n+\n@@ -84,1 +87,0 @@\n-\n","filename":"src\/java.base\/share\/classes\/jdk\/internal\/javac\/PreviewFeature.java","additions":3,"deletions":1,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -36,0 +36,1 @@\n+import java.util.function.BiFunction;\n@@ -123,1 +124,6 @@\n-        noWarnings = new Warner(null);\n+        noWarnings = new Warner(null) {\n+            @Override\n+            public String toString() {\n+                return \"NO_WARNINGS\";\n+            }\n+        };\n@@ -2157,0 +2163,4 @@\n+        return makeArrayType(t, 1);\n+    }\n+\n+    public ArrayType makeArrayType(Type t, int dimensions) {\n@@ -2160,1 +2170,5 @@\n-        return new ArrayType(t, syms.arrayClass);\n+        ArrayType result = new ArrayType(t, syms.arrayClass);\n+        for (int i = 1; i < dimensions; i++) {\n+            result = new ArrayType(result, syms.arrayClass);\n+        }\n+        return result;\n@@ -3952,1 +3966,1 @@\n-                                 class1.tsym);\n+                                 class1.tsym, List.nil());\n@@ -4935,0 +4949,1 @@\n+        private boolean encodeTypeSig;\n@@ -4936,1 +4951,1 @@\n-        public UniqueType(Type type, Types types) {\n+        public UniqueType(Type type, Types types, boolean encodeTypeSig) {\n@@ -4939,0 +4954,5 @@\n+            this.encodeTypeSig = encodeTypeSig;\n+        }\n+\n+        public UniqueType(Type type, Types types) {\n+            this(type, types, true);\n@@ -4950,0 +4970,4 @@\n+        public boolean encodeTypeSig() {\n+            return encodeTypeSig;\n+        }\n+\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/code\/Types.java","additions":28,"deletions":4,"binary":false,"changes":32,"status":"modified"},{"patch":"@@ -35,1 +35,0 @@\n-import java.util.function.ToIntFunction;\n@@ -44,11 +43,0 @@\n-import static com.sun.tools.javac.jvm.ClassFile.CONSTANT_Class;\n-import static com.sun.tools.javac.jvm.ClassFile.CONSTANT_Double;\n-import static com.sun.tools.javac.jvm.ClassFile.CONSTANT_Fieldref;\n-import static com.sun.tools.javac.jvm.ClassFile.CONSTANT_Float;\n-import static com.sun.tools.javac.jvm.ClassFile.CONSTANT_Integer;\n-import static com.sun.tools.javac.jvm.ClassFile.CONSTANT_InterfaceMethodref;\n-import static com.sun.tools.javac.jvm.ClassFile.CONSTANT_Long;\n-import static com.sun.tools.javac.jvm.ClassFile.CONSTANT_MethodHandle;\n-import static com.sun.tools.javac.jvm.ClassFile.CONSTANT_MethodType;\n-import static com.sun.tools.javac.jvm.ClassFile.CONSTANT_Methodref;\n-import static com.sun.tools.javac.jvm.ClassFile.CONSTANT_String;\n@@ -58,0 +46,3 @@\n+import java.util.Map;\n+import java.util.HashMap;\n+import java.util.Set;\n@@ -202,0 +193,8 @@\n+    private Map<Integer, Set<VarSymbol>> cpToUnsetFieldsMap = new HashMap<>();\n+\n+    public Set<VarSymbol> initialUnsetFields;\n+\n+    public Set<VarSymbol> currentUnsetFields;\n+\n+    boolean generateEarlyLarvalFrame;\n+\n@@ -214,1 +213,2 @@\n-                PoolWriter poolWriter) {\n+                PoolWriter poolWriter,\n+                boolean generateEarlyLarvalFrame) {\n@@ -236,0 +236,1 @@\n+        this.generateEarlyLarvalFrame = generateEarlyLarvalFrame;\n@@ -1086,1 +1087,0 @@\n-\n@@ -1233,0 +1233,1 @@\n+        Set<VarSymbol> unsetFields;\n@@ -1328,0 +1329,1 @@\n+        boolean hasUninitalizedThis = false;\n@@ -1333,1 +1335,1 @@\n-                if (!(vtype instanceof UninitializedType))\n+                if (!(vtype instanceof UninitializedType)) {\n@@ -1335,0 +1337,3 @@\n+                } else if (vtype.hasTag(TypeTag.UNINITIALIZED_THIS)) {\n+                    hasUninitalizedThis = true;\n+                }\n@@ -1360,0 +1365,4 @@\n+        Set<VarSymbol> unsetFieldsAtPC = cpToUnsetFieldsMap.get(pc);\n+        boolean encloseWithEarlyLarvalFrame = unsetFieldsAtPC != null && generateEarlyLarvalFrame && hasUninitalizedThis\n+                && !lastFrame.unsetFields.equals(unsetFieldsAtPC);\n+\n@@ -1367,2 +1376,9 @@\n-        stackMapTableBuffer[stackMapBufferSize++] =\n-                StackMapTableFrame.getInstance(frame, lastFrame.pc, lastFrame.locals, types);\n+\n+        StackMapTableFrame tableFrame = StackMapTableFrame.getInstance(frame, lastFrame, types, pc);\n+        if (encloseWithEarlyLarvalFrame) {\n+            tableFrame = new StackMapTableFrame.EarlyLarvalFrame(tableFrame, unsetFieldsAtPC);\n+            frame.unsetFields = unsetFieldsAtPC;\n+        } else {\n+            frame.unsetFields = lastFrame.unsetFields;\n+        }\n+        stackMapTableBuffer[stackMapBufferSize++] = tableFrame;\n@@ -1374,0 +1390,4 @@\n+    public void addUnsetFieldsAtPC(int pc, Set<VarSymbol> unsetFields) {\n+        cpToUnsetFieldsMap.put(pc, unsetFields);\n+    }\n+\n@@ -1395,0 +1415,1 @@\n+        frame.unsetFields = initialUnsetFields;\n@@ -1473,0 +1494,3 @@\n+            if (currentUnsetFields != null) {\n+                addUnsetFieldsAtPC(result.pc, currentUnsetFields);\n+            }\n@@ -1484,0 +1508,1 @@\n+        int originalTarget = target;\n@@ -1510,1 +1535,1 @@\n-                if (fatcode)\n+                if (fatcode) {\n@@ -1512,0 +1537,4 @@\n+                    if (cpToUnsetFieldsMap.get(chain.pc) != null) {\n+                        addUnsetFieldsAtPC(originalTarget, cpToUnsetFieldsMap.get(chain.pc));\n+                    }\n+                }\n@@ -1515,1 +1544,1 @@\n-                else\n+                else {\n@@ -1517,0 +1546,4 @@\n+                    if (cpToUnsetFieldsMap.get(chain.pc) != null) {\n+                        addUnsetFieldsAtPC(originalTarget, cpToUnsetFieldsMap.get(chain.pc));\n+                    }\n+                }\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/jvm\/Code.java","additions":52,"deletions":19,"binary":false,"changes":71,"status":"modified"},{"patch":"@@ -40,0 +40,1 @@\n+import com.sun.tools.javac.code.FlagsEnum;\n@@ -43,0 +44,1 @@\n+import com.sun.tools.javac.parser.Tokens.Comment.CommentStyle;\n@@ -55,0 +57,1 @@\n+import static com.sun.tools.javac.code.Flags.asFlagSet;\n@@ -63,0 +66,1 @@\n+import static com.sun.tools.javac.parser.Tokens.TokenKind.SYNCHRONIZED;\n@@ -201,0 +205,2 @@\n+        this.allowValueClasses = (!preview.isPreview(Feature.VALUE_CLASSES) || preview.isEnabled()) &&\n+                Feature.VALUE_CLASSES.allowedInSource(source);\n@@ -225,0 +231,2 @@\n+        this.allowValueClasses = (!preview.isPreview(Feature.VALUE_CLASSES) || preview.isEnabled()) &&\n+                Feature.VALUE_CLASSES.allowedInSource(source);\n@@ -263,0 +271,4 @@\n+    \/** Switch: are value classes allowed in this source level?\n+     *\/\n+    boolean allowValueClasses;\n+\n@@ -1690,2 +1702,2 @@\n-                        if (!isMode(TYPE) && isUnboundMemberRef()) {\n-                            \/\/this is an unbound method reference whose qualifier\n+                        if (!isMode(TYPE) && isParameterizedTypePrefix()) {\n+                            \/\/this is either an unbound method reference whose qualifier\n@@ -1936,1 +1948,1 @@\n-    boolean isUnboundMemberRef() {\n+    boolean isParameterizedTypePrefix() {\n@@ -3032,0 +3044,5 @@\n+        if ((isValueModifier()) && allowValueClasses) {\n+            checkSourceLevel(Feature.VALUE_CLASSES);\n+            dc = token.docComment();\n+            return List.of(classOrRecordOrInterfaceOrEnumDeclaration(modifiersOpt(), dc));\n+        }\n@@ -3637,0 +3654,5 @@\n+                if (isValueModifier()) {\n+                    checkSourceLevel(Feature.VALUE_CLASSES);\n+                    flag = Flags.VALUE_CLASS;\n+                    break;\n+                }\n@@ -3900,0 +3922,7 @@\n+        if (name == names.value) {\n+            if (allowValueClasses) {\n+                return Source.JDK23;\n+            } else if (shouldWarn) {\n+                log.warning(pos, Warnings.RestrictedTypeNotAllowedPreview(name, Source.JDK23));\n+            }\n+        }\n@@ -5021,0 +5050,26 @@\n+    protected boolean isValueModifier() {\n+        if (token.kind == IDENTIFIER && token.name() == names.value) {\n+            boolean isValueModifier = false;\n+            Token next = S.token(1);\n+            switch (next.kind) {\n+                case PRIVATE: case PROTECTED: case PUBLIC: case STATIC: case TRANSIENT:\n+                case FINAL: case ABSTRACT: case NATIVE: case VOLATILE: case SYNCHRONIZED:\n+                case STRICTFP: case MONKEYS_AT: case DEFAULT: case BYTE: case SHORT:\n+                case CHAR: case INT: case LONG: case FLOAT: case DOUBLE: case BOOLEAN: case VOID:\n+                case CLASS: case INTERFACE: case ENUM:\n+                    isValueModifier = true;\n+                    break;\n+                case IDENTIFIER: \/\/ value record R || value value || new value Comparable() {} ??\n+                    if (next.name() == names.record || next.name() == names.value\n+                            || (mode & EXPR) != 0)\n+                        isValueModifier = true;\n+                    break;\n+            }\n+            if (isValueModifier) {\n+                checkSourceLevel(Feature.VALUE_CLASSES);\n+                return true;\n+            }\n+        }\n+        return false;\n+    }\n+\n@@ -5048,1 +5103,3 @@\n-                case IDENTIFIER -> isNonSealedIdentifier(next, currentIsNonSealed ? 3 : 1) || next.name() == names.sealed;\n+                case IDENTIFIER -> isNonSealedIdentifier(next, currentIsNonSealed ? 3 : 1) ||\n+                        next.name() == names.sealed ||\n+                        allowValueClasses && next.name() == names.value;\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/parser\/JavacParser.java","additions":61,"deletions":4,"binary":false,"changes":65,"status":"modified"},{"patch":"@@ -28,2 +28,0 @@\n-\n-\n@@ -34,1 +32,0 @@\n-import com.sun.tools.javac.comp.AttrContext;\n@@ -185,0 +182,15 @@\n+    \/** Is this tree `super`, or `Ident.super`?\n+     *\/\n+    public static boolean isSuperOrSelectorDotSuper(JCTree tree) {\n+        switch (tree.getTag()) {\n+            case PARENS:\n+                return isSuperOrSelectorDotSuper(skipParens(tree));\n+            case IDENT:\n+                return ((JCIdent)tree).name == ((JCIdent)tree).name.table.names._super;\n+            case SELECT:\n+                return ((JCFieldAccess)tree).name == ((JCFieldAccess)tree).name.table.names._super;\n+            default:\n+                return false;\n+        }\n+    }\n+\n@@ -193,0 +205,1 @@\n+        Symbol.ClassSymbol currentClassSym = (Symbol.ClassSymbol) types.erasure(currentClass).tsym;\n@@ -199,1 +212,4 @@\n-                return ident.name == names._this || ident.name == names._super;\n+                return ident.name == names._this && tree.type.tsym == currentClass.tsym ||\n+                       ident.name == names._super &&\n+                               (tree.type.tsym == currentClass.tsym ||\n+                                currentClassSym.isSubClass(tree.type.tsym, types));\n@@ -206,2 +222,1 @@\n-                Symbol.ClassSymbol currentClassSym = (Symbol.ClassSymbol)((Type.ClassType)types.erasure(currentClass)).tsym;\n-                Symbol.ClassSymbol selectedClassSym = (Symbol.ClassSymbol)((Type.ClassType)selectedType).tsym;\n+                Symbol.ClassSymbol selectedClassSym = (Symbol.ClassSymbol)(selectedType).tsym;\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/tree\/TreeInfo.java","additions":21,"deletions":6,"binary":false,"changes":27,"status":"modified"},{"patch":"@@ -43,1 +43,2 @@\n-static void assert_test_pattern(Handle object, const char* pattern) {\n+template<typename Printable>\n+static void assert_test_pattern(Printable object, const char* pattern) {\n@@ -49,0 +50,5 @@\n+template<typename Printable>\n+static void assert_mark_word_print_pattern(Printable object, const char* pattern) {\n+  assert_test_pattern(object, pattern);\n+}\n+\n@@ -88,1 +94,1 @@\n-    assert_test_pattern(h_obj, \"locked\");\n+    assert_mark_word_print_pattern(h_obj, \"locked\");\n@@ -90,1 +96,1 @@\n-  assert_test_pattern(h_obj, \"is_unlocked no_hash\");\n+  assert_mark_word_print_pattern(h_obj, \"is_unlocked no_hash\");\n@@ -94,1 +100,1 @@\n-  assert_test_pattern(h_obj, \"is_unlocked hash=0x\");\n+  assert_mark_word_print_pattern(h_obj, \"is_unlocked hash=0x\");\n@@ -110,0 +116,138 @@\n+\n+static void assert_unlocked_state(markWord mark) {\n+  EXPECT_FALSE(mark.has_displaced_mark_helper());\n+  EXPECT_FALSE(mark.is_fast_locked());\n+  EXPECT_FALSE(mark.has_monitor());\n+  EXPECT_FALSE(mark.is_being_inflated());\n+  EXPECT_FALSE(mark.is_locked());\n+  EXPECT_TRUE(mark.is_unlocked());\n+}\n+\n+static void assert_copy_set_hash(markWord mark) {\n+  const intptr_t hash = 4711;\n+  EXPECT_TRUE(mark.has_no_hash());\n+  markWord copy = mark.copy_set_hash(hash);\n+  EXPECT_EQ(hash, copy.hash());\n+  EXPECT_FALSE(copy.has_no_hash());\n+}\n+\n+static void assert_type(markWord mark) {\n+  EXPECT_FALSE(mark.is_flat_array());\n+  EXPECT_FALSE(mark.is_inline_type());\n+  EXPECT_FALSE(mark.is_larval_state());\n+  EXPECT_FALSE(mark.is_null_free_array());\n+}\n+\n+TEST_VM(markWord, prototype) {\n+  markWord mark = markWord::prototype();\n+  assert_unlocked_state(mark);\n+  EXPECT_TRUE(mark.is_neutral());\n+\n+  assert_type(mark);\n+\n+  EXPECT_TRUE(mark.has_no_hash());\n+  EXPECT_FALSE(mark.is_marked());\n+\n+  assert_copy_set_hash(mark);\n+  assert_type(mark);\n+}\n+\n+static void assert_inline_type(markWord mark) {\n+  EXPECT_FALSE(mark.is_flat_array());\n+  EXPECT_TRUE(mark.is_inline_type());\n+  EXPECT_FALSE(mark.is_null_free_array());\n+}\n+\n+TEST_VM(markWord, inline_type_prototype) {\n+  markWord mark = markWord::inline_type_prototype();\n+  assert_unlocked_state(mark);\n+  EXPECT_FALSE(mark.is_neutral());\n+  assert_test_pattern(&mark, \" inline_type\");\n+\n+  assert_inline_type(mark);\n+  EXPECT_FALSE(mark.is_larval_state());\n+\n+  EXPECT_TRUE(mark.has_no_hash());\n+  EXPECT_FALSE(mark.is_marked());\n+\n+  markWord larval = mark.enter_larval_state();\n+  EXPECT_TRUE(larval.is_larval_state());\n+  assert_inline_type(larval);\n+  assert_test_pattern(&larval, \" inline_type=larval\");\n+\n+  mark = larval.exit_larval_state();\n+  EXPECT_FALSE(mark.is_larval_state());\n+  assert_inline_type(mark);\n+\n+  EXPECT_TRUE(mark.has_no_hash());\n+  EXPECT_FALSE(mark.is_marked());\n+}\n+\n+#if _LP64\n+\n+static void assert_flat_array_type(markWord mark) {\n+  EXPECT_TRUE(mark.is_flat_array());\n+  EXPECT_FALSE(mark.is_inline_type());\n+  EXPECT_FALSE(mark.is_larval_state());\n+}\n+\n+TEST_VM(markWord, null_free_flat_array_prototype) {\n+  markWord mark = markWord::flat_array_prototype(LayoutKind::NON_ATOMIC_FLAT);\n+  assert_unlocked_state(mark);\n+  EXPECT_TRUE(mark.is_neutral());\n+\n+  assert_flat_array_type(mark);\n+  EXPECT_TRUE(mark.is_null_free_array());\n+\n+  EXPECT_TRUE(mark.has_no_hash());\n+  EXPECT_FALSE(mark.is_marked());\n+\n+  assert_copy_set_hash(mark);\n+  assert_flat_array_type(mark);\n+  EXPECT_TRUE(mark.is_null_free_array());\n+\n+  assert_test_pattern(&mark, \" flat_null_free_array\");\n+}\n+\n+TEST_VM(markWord, nullable_flat_array_prototype) {\n+  markWord mark = markWord::flat_array_prototype(LayoutKind::NULLABLE_ATOMIC_FLAT);\n+  assert_unlocked_state(mark);\n+  EXPECT_TRUE(mark.is_neutral());\n+\n+  assert_flat_array_type(mark);\n+  EXPECT_FALSE(mark.is_null_free_array());\n+\n+  EXPECT_TRUE(mark.has_no_hash());\n+  EXPECT_FALSE(mark.is_marked());\n+\n+  assert_copy_set_hash(mark);\n+  assert_flat_array_type(mark);\n+  EXPECT_FALSE(mark.is_null_free_array());\n+\n+  assert_test_pattern(&mark, \" flat_array\");\n+}\n+\n+static void assert_null_free_array_type(markWord mark) {\n+  EXPECT_FALSE(mark.is_flat_array());\n+  EXPECT_FALSE(mark.is_inline_type());\n+  EXPECT_FALSE(mark.is_larval_state());\n+  EXPECT_TRUE(mark.is_null_free_array());\n+}\n+\n+TEST_VM(markWord, null_free_array_prototype) {\n+  markWord mark = markWord::null_free_array_prototype();\n+  assert_unlocked_state(mark);\n+  EXPECT_TRUE(mark.is_neutral());\n+\n+  assert_null_free_array_type(mark);\n+\n+  EXPECT_TRUE(mark.has_no_hash());\n+  EXPECT_FALSE(mark.is_marked());\n+\n+  assert_copy_set_hash(mark);\n+  assert_null_free_array_type(mark);\n+\n+  assert_test_pattern(&mark, \" null_free_array\");\n+}\n+#endif \/\/ _LP64\n+\n","filename":"test\/hotspot\/gtest\/oops\/test_markWord.cpp","additions":148,"deletions":4,"binary":false,"changes":152,"status":"modified"},{"patch":"@@ -82,0 +82,3 @@\n+compiler\/jvmci\/jdk.vm.ci.hotspot.test\/src\/jdk\/vm\/ci\/hotspot\/test\/MemoryAccessProviderTest.java 8350208 generic-all\n+compiler\/jvmci\/jdk.vm.ci.hotspot.test\/src\/jdk\/vm\/ci\/hotspot\/test\/TestHotSpotResolvedJavaField.java 8350208 generic-all\n+\n@@ -84,0 +87,21 @@\n+# Valhalla\n+compiler\/whitebox\/DeoptimizeRelocatedNMethod.java 8370571 generic-all\n+compiler\/whitebox\/RelocateNMethodMultiplePaths.java#C1 8370571 generic-all\n+compiler\/whitebox\/RelocateNMethodMultiplePaths.java#C2 8370571 generic-all\n+compiler\/whitebox\/RelocateNMethodMultiplePaths.java#G1C1 8370571 generic-all\n+compiler\/whitebox\/RelocateNMethodMultiplePaths.java#G1C2 8370571 generic-all\n+compiler\/whitebox\/RelocateNMethodMultiplePaths.java#ParallelC1 8370571 generic-all\n+compiler\/whitebox\/RelocateNMethodMultiplePaths.java#ParallelC2 8370571 generic-all\n+compiler\/whitebox\/RelocateNMethodMultiplePaths.java#SerialC1 8370571 generic-all\n+compiler\/whitebox\/RelocateNMethodMultiplePaths.java#SerialC2 8370571 generic-all\n+compiler\/whitebox\/RelocateNMethodMultiplePaths.java#ZGCC1 8370571 generic-all\n+compiler\/whitebox\/RelocateNMethodMultiplePaths.java#ZGCC2 8370571 generic-all\n+compiler\/whitebox\/StressNMethodRelocation.java 8370571 generic-all\n+compiler\/valhalla\/inlinetypes\/TestNullableArrays.java#id0 8367553 generic-aarch64\n+compiler\/valhalla\/inlinetypes\/TestNullableArrays.java#id1 8367553 generic-aarch64\n+compiler\/valhalla\/inlinetypes\/TestNullableArrays.java#id2 8367553 generic-aarch64\n+compiler\/valhalla\/inlinetypes\/TestNullableArrays.java#id3 8367553 generic-aarch64\n+compiler\/valhalla\/inlinetypes\/TestNullableArrays.java#id4 8367553 generic-aarch64\n+compiler\/valhalla\/inlinetypes\/TestNullableArrays.java#id5 8367553 generic-aarch64\n+compiler\/valhalla\/inlinetypes\/TestNullableArrays.java#id6 8367553 generic-aarch64\n+\n@@ -105,0 +129,1 @@\n+runtime\/cds\/appcds\/redefineClass\/RedefineRunningMethods_Shared.java  8304168 generic-all\n@@ -125,0 +150,15 @@\n+\n+# Valhalla\n+runtime\/valhalla\/inlinetypes\/verifier\/StrictInstanceFieldsTest.java CODETOOLS-7904031 generic-all\n+runtime\/valhalla\/inlinetypes\/verifier\/StrictStaticFieldsTest.java CODETOOLS-7904031 generic-all\n+\n+runtime\/cds\/TestDefaultArchiveLoading.java#coops_nocoh            8366774           generic-all\n+runtime\/cds\/TestDefaultArchiveLoading.java#nocoops_nocoh          8366774           generic-all\n+\n+# Valhalla + AOT\n+runtime\/cds\/appcds\/aotCache\/HelloAOTCache.java                                  8369043 generic-aarch64\n+runtime\/cds\/appcds\/aotCode\/AOTCodeFlags.java                                    8369043 generic-aarch64\n+runtime\/cds\/appcds\/methodHandles\/MethodHandlesGeneralTest.java#aot              8367408 generic-all\n+runtime\/cds\/appcds\/resolvedConstants\/ResolvedConstants.java#aot                 8371456 generic-all\n+runtime\/cds\/appcds\/resolvedConstants\/ResolvedConstants.java#static              8371456 generic-all\n+\n@@ -151,0 +191,57 @@\n+\n+# Valhalla TODO:\n+serviceability\/sa\/ClhsdbCDSCore.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbCDSJstackPrintAll.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbFindPC.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbInspect.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbLongConstant.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbJdis.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbJstack.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbPrintAll.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbPrintAs.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbPrintStatics.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbSource.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbSymbol.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbWhere.java 8190936 generic-all\n+serviceability\/sa\/JhsdbThreadInfoTest.java 8190936 generic-all\n+serviceability\/sa\/TestClassDump.java 8190936 generic-all\n+serviceability\/sa\/TestClhsdbJstackLock.java 8190936 generic-all\n+serviceability\/sa\/TestCpoolForInvokeDynamic.java 8190936 generic-all\n+serviceability\/sa\/TestHeapDumpForInvokeDynamic.java 8190936 generic-all\n+serviceability\/sa\/TestHeapDumpForLargeArray.java 8190936 generic-all\n+serviceability\/sa\/TestIntConstant.java 8190936 generic-all\n+serviceability\/sa\/TestJhsdbJstackLock.java 8190936 generic-all\n+serviceability\/sa\/TestJmapCore.java 8190936 generic-all\n+serviceability\/sa\/TestJmapCoreMetaspace.java 8190936 generic-all\n+serviceability\/sa\/TestPrintMdo.java 8190936 generic-all\n+serviceability\/sa\/jmap-hprof\/JMapHProfLargeHeapTest.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbDumpclass.java 8190936 generic-all\n+\n+# Array Changes TODO\n+serviceability\/sa\/CDSJMapClstats.java 8365722 generic-all\n+serviceability\/sa\/ClhsdbClasses.java 8365722 generic-all\n+serviceability\/sa\/sadebugd\/DisableRegistryTest.java 8365722 generic-all\n+serviceability\/sa\/ClhsdbDumpheap.java 8365722 generic-all\n+serviceability\/sa\/sadebugd\/ClhsdbTestConnectArgument.java 8365722 generic-all\n+serviceability\/sa\/sadebugd\/DebugdConnectTest.java 8365722 generic-all\n+serviceability\/sa\/ClhsdbJhisto.java 8365722 generic-all\n+serviceability\/sa\/ClhsdbJstack.java#id1 8365722 generic-all\n+serviceability\/sa\/ClhsdbJstackWithConcurrentLock.java 8365722 generic-all\n+serviceability\/sa\/TestJhsdbJstackWithVirtualThread.java 8365722 generic-all\n+serviceability\/sa\/ClhsdbJstackXcompStress.java 8365722 generic-all\n+serviceability\/sa\/ClhsdbPstack.java#process 8365722 generic-all\n+serviceability\/sa\/ClhsdbPstack.java#core 8365722 generic-all\n+serviceability\/sa\/ClhsdbScanOops.java#id0 8365722 generic-all\n+serviceability\/sa\/ClhsdbScanOops.java#id1 8365722 generic-all\n+serviceability\/sa\/ClhsdbScanOops.java#serial 8365722 generic-all\n+serviceability\/sa\/ClhsdbScanOops.java#parallel 8365722 generic-all\n+serviceability\/sa\/DeadlockDetectionTest.java 8365722 generic-all\n+serviceability\/sa\/ClhsdbJstack.java#id0 8365722 generic-all\n+serviceability\/sa\/TestInstanceKlassSize.java 8365722 generic-all\n+serviceability\/sa\/TestSysProps.java 8365722 generic-all\n+serviceability\/sa\/sadebugd\/ClhsdbAttachToDebugServer.java 8365722 generic-all\n+resourcehogs\/serviceability\/sa\/TestHeapDumpForLargeArray.java 8365722 generic-all\n+\n+resourcehogs\/serviceability\/sa\/ClhsdbRegionDetailsScanOopsForG1.java 8190936 generic-all\n+vmTestbase\/nsk\/jvmti\/scenarios\/events\/EM04\/em04t001\/TestDescription.java 8367590 generic-all\n+\n@@ -190,0 +287,2 @@\n+vmTestbase\/vm\/mlvm\/hiddenloader\/stress\/byteMutation\/Test.java 8317172 generic-all\n+\n","filename":"test\/hotspot\/jtreg\/ProblemList.txt","additions":99,"deletions":0,"binary":false,"changes":99,"status":"modified"},{"patch":"@@ -31,0 +31,1 @@\n+import compiler.valhalla.inlinetypes.InlineTypeIRNode;\n@@ -90,1 +91,1 @@\n-    private static final String PREFIX = \"_#\";\n+    public static final String PREFIX = \"_#\";\n@@ -153,0 +154,6 @@\n+    \/\/ Valhalla: Make sure that all Valhalla specific IR nodes are also properly initialized. Doing it here also\n+    \/\/           ensures that the Flag VM is able to pick up the correct compile phases.\n+    static {\n+        InlineTypeIRNode.forceStaticInitialization();\n+    }\n+\n@@ -384,2 +391,6 @@\n-        String regex = START + \"Allocate\\\\b\" + MID + \"allocationKlass:.*\\\\b\" + IS_REPLACED + \"\\\\s.*\" + END;\n-        macroNodes(ALLOC_OF, regex);\n+        allocateOfNodes(ALLOC_OF, IS_REPLACED);\n+    }\n+\n+    public static void allocateOfNodes(String irNodePlaceholder, String allocatee) {\n+        String regex = START + \"Allocate\\\\b\" + MID + \"allocationKlass:.*\\\\b\" + allocatee + \"\\\\s.*\" + END;\n+        macroNodes(irNodePlaceholder, regex);\n@@ -396,0 +407,4 @@\n+        allocateArrayOfNodes(ALLOC_ARRAY_OF, IS_REPLACED);\n+    }\n+\n+    public static void allocateArrayOfNodes(String irNodePlaceholder, String allocatee) {\n@@ -414,1 +429,1 @@\n-        String name_part = \"\\\\[+.(\" + partial_name_prefix + \")?\" + IS_REPLACED + \";\";\n+        String name_part = \"\\\\[+.(\" + partial_name_prefix + \")?\" + allocatee + \";\";\n@@ -416,1 +431,1 @@\n-        macroNodes(ALLOC_ARRAY_OF, regex);\n+        macroNodes(irNodePlaceholder, regex);\n@@ -481,1 +496,1 @@\n-        callOfNodes(CALL_OF, \"Call.*\");\n+        callOfNodes(CALL_OF, \"Call.*\", IS_REPLACED + \" \" );\n@@ -486,1 +501,6 @@\n-        callOfNodes(CALL_OF_METHOD, \"Call.*Java\");\n+        callOfNodes(CALL_OF_METHOD, \"Call.*Java\", IS_REPLACED + \" \");\n+    }\n+\n+    public static final String STATIC_CALL = PREFIX + \"STATIC_CALL\" + POSTFIX;\n+    static {\n+        beforeMatchingNameRegex(STATIC_CALL, \"CallStaticJava\");\n@@ -491,1 +511,19 @@\n-        callOfNodes(STATIC_CALL_OF_METHOD, \"CallStaticJava\");\n+        staticCallOfMethodNodes(STATIC_CALL_OF_METHOD, IS_REPLACED + \" \");\n+    }\n+\n+    public static void staticCallOfMethodNodes(String irNodePlaceholder, String calleeRegex) {\n+        callOfNodes(irNodePlaceholder, \"CallStaticJava\", calleeRegex);\n+    }\n+\n+    public static final String CALL_LEAF_NO_FP = PREFIX + \"CALL_LEAF_NO_FP\" + POSTFIX;\n+    static {\n+        beforeMatchingNameRegex(CALL_LEAF_NO_FP, \"CallLeafNoFP\");\n+    }\n+\n+    public static final String CALL_LEAF_NO_FP_OF_METHOD = COMPOSITE_PREFIX + \"CALL_LEAF_NO_FP_OF_METHOD\" + POSTFIX;\n+    static {\n+        callLeafNoFpOfMethodNodes(CALL_LEAF_NO_FP_OF_METHOD, IS_REPLACED);\n+    }\n+\n+    public static void callLeafNoFpOfMethodNodes(String irNodePlaceholder, String calleeRegex) {\n+        callOfNodes(irNodePlaceholder, \"CallLeafNoFP\", calleeRegex);\n@@ -588,0 +626,5 @@\n+    public static final String CMP_N = PREFIX + \"CMP_N\" + POSTFIX;\n+    static {\n+        beforeMatchingNameRegex(CMP_N, \"CmpN\");\n+    }\n+\n@@ -742,1 +785,1 @@\n-        callOfNodes(DYNAMIC_CALL_OF_METHOD, \"CallDynamicJava\");\n+        callOfNodes(DYNAMIC_CALL_OF_METHOD, \"CallDynamicJava\", IS_REPLACED);\n@@ -878,0 +921,5 @@\n+    public static final String INLINE_TYPE = PREFIX + \"INLINE_TYPE\" + POSTFIX;\n+    static {\n+        beforeMatchingNameRegex(INLINE_TYPE, \"InlineType\");\n+    }\n+\n@@ -924,1 +972,5 @@\n-        loadOfNodes(LOAD_OF_CLASS, \"Load(B|UB|S|US|I|L|F|D|P|N)\");\n+        anyLoadOfNodes(LOAD_OF_CLASS, IS_REPLACED);\n+    }\n+\n+    public static void anyLoadOfNodes(String irNodePlaceholder, String fieldHolder) {\n+        loadOfNodes(irNodePlaceholder, \"Load(B|UB|S|US|I|L|F|D|P|N)\", fieldHolder);\n@@ -934,1 +986,1 @@\n-        loadOfNodes(LOAD_B_OF_CLASS, \"LoadB\");\n+        loadOfNodes(LOAD_B_OF_CLASS, \"LoadB\", IS_REPLACED);\n@@ -944,1 +996,1 @@\n-        loadOfNodes(LOAD_D_OF_CLASS, \"LoadD\");\n+        loadOfNodes(LOAD_D_OF_CLASS, \"LoadD\", IS_REPLACED);\n@@ -954,1 +1006,1 @@\n-        loadOfNodes(LOAD_F_OF_CLASS, \"LoadF\");\n+        loadOfNodes(LOAD_F_OF_CLASS, \"LoadF\", IS_REPLACED);\n@@ -964,1 +1016,1 @@\n-        loadOfNodes(LOAD_I_OF_CLASS, \"LoadI\");\n+        loadOfNodes(LOAD_I_OF_CLASS, \"LoadI\", IS_REPLACED);\n@@ -989,1 +1041,1 @@\n-        loadOfNodes(LOAD_L_OF_CLASS, \"LoadL\");\n+        loadOfNodes(LOAD_L_OF_CLASS, \"LoadL\", IS_REPLACED);\n@@ -999,1 +1051,1 @@\n-        loadOfNodes(LOAD_N_OF_CLASS, \"LoadN\");\n+        loadOfNodes(LOAD_N_OF_CLASS, \"LoadN\", IS_REPLACED);\n@@ -1015,1 +1067,1 @@\n-        loadOfNodes(LOAD_P_OF_CLASS, \"LoadP\");\n+        loadOfNodes(LOAD_P_OF_CLASS, \"LoadP\", IS_REPLACED);\n@@ -1025,1 +1077,1 @@\n-        loadOfNodes(LOAD_S_OF_CLASS, \"LoadS\");\n+        loadOfNodes(LOAD_S_OF_CLASS, \"LoadS\", IS_REPLACED);\n@@ -1035,1 +1087,1 @@\n-        loadOfNodes(LOAD_UB_OF_CLASS, \"LoadUB\");\n+        loadOfNodes(LOAD_UB_OF_CLASS, \"LoadUB\", IS_REPLACED);\n@@ -1045,1 +1097,1 @@\n-        loadOfNodes(LOAD_US_OF_CLASS, \"LoadUS\");\n+        loadOfNodes(LOAD_US_OF_CLASS, \"LoadUS\", IS_REPLACED);\n@@ -1959,1 +2011,1 @@\n-        storeOfNodes(STORE_B_OF_CLASS, \"StoreB\");\n+        storeOfNodes(STORE_B_OF_CLASS, \"StoreB\", IS_REPLACED);\n@@ -1969,1 +2021,1 @@\n-        storeOfNodes(STORE_C_OF_CLASS, \"StoreC\");\n+        storeOfNodes(STORE_C_OF_CLASS, \"StoreC\", IS_REPLACED);\n@@ -1979,1 +2031,1 @@\n-        storeOfNodes(STORE_D_OF_CLASS, \"StoreD\");\n+        storeOfNodes(STORE_D_OF_CLASS, \"StoreD\", IS_REPLACED);\n@@ -1989,1 +2041,1 @@\n-        storeOfNodes(STORE_F_OF_CLASS, \"StoreF\");\n+        storeOfNodes(STORE_F_OF_CLASS, \"StoreF\", IS_REPLACED);\n@@ -1999,1 +2051,1 @@\n-        storeOfNodes(STORE_I_OF_CLASS, \"StoreI\");\n+        storeOfNodes(STORE_I_OF_CLASS, \"StoreI\", IS_REPLACED);\n@@ -2009,1 +2061,1 @@\n-        storeOfNodes(STORE_L_OF_CLASS, \"StoreL\");\n+        storeOfNodes(STORE_L_OF_CLASS, \"StoreL\", IS_REPLACED);\n@@ -2019,1 +2071,1 @@\n-        storeOfNodes(STORE_N_OF_CLASS, \"StoreN\");\n+        storeOfNodes(STORE_N_OF_CLASS, \"StoreN\", IS_REPLACED);\n@@ -2024,1 +2076,5 @@\n-        storeOfNodes(STORE_OF_CLASS, \"Store(B|C|S|I|L|F|D|P|N)\");\n+        anyStoreOfNodes(STORE_OF_CLASS, IS_REPLACED);\n+    }\n+\n+    public static void anyStoreOfNodes(String irNodePlaceholder, String fieldHolder) {\n+        storeOfNodes(irNodePlaceholder, \"Store(B|C|S|I|L|F|D|P|N)\", fieldHolder);\n@@ -2040,1 +2096,1 @@\n-        storeOfNodes(STORE_P_OF_CLASS, \"StoreP\");\n+        storeOfNodes(STORE_P_OF_CLASS, \"StoreP\", IS_REPLACED);\n@@ -2130,1 +2186,2 @@\n-        beforeMatchingNameRegex(SUBTYPE_CHECK, \"SubTypeCheck\");\n+        String regex = START + \"SubTypeCheck\" + MID + END;\n+        macroNodes(SUBTYPE_CHECK, regex);\n@@ -3082,1 +3139,1 @@\n-    private static void beforeMatching(String irNodePlaceholder, String regex) {\n+    public static void beforeMatching(String irNodePlaceholder, String regex) {\n@@ -3118,2 +3175,2 @@\n-    private static void callOfNodes(String irNodePlaceholder, String callRegex) {\n-        String regex = START + callRegex + MID + IS_REPLACED + \" \" +  END;\n+    private static void callOfNodes(String irNodePlaceholder, String callRegex, String calleeRegex) {\n+        String regex = START + callRegex + MID + calleeRegex + END;\n@@ -3127,1 +3184,1 @@\n-    private static void optoOnly(String irNodePlaceholder, String regex) {\n+    public static void optoOnly(String irNodePlaceholder, String regex) {\n@@ -3212,2 +3269,2 @@\n-    private static void loadOfNodes(String irNodePlaceholder, String irNodeRegex) {\n-        String regex = START + irNodeRegex + MID + LOAD_STORE_PREFIX + IS_REPLACED + LOAD_STORE_SUFFIX + END;\n+    private static void loadOfNodes(String irNodePlaceholder, String irNodeRegex, String loadee) {\n+        String regex = START + irNodeRegex + MID + LOAD_STORE_PREFIX + loadee + LOAD_STORE_SUFFIX + END;\n@@ -3217,2 +3274,2 @@\n-    private static void storeOfNodes(String irNodePlaceholder, String irNodeRegex) {\n-        String regex = START + irNodeRegex + MID + LOAD_STORE_PREFIX + IS_REPLACED + LOAD_STORE_SUFFIX + END;\n+    private static void storeOfNodes(String irNodePlaceholder, String irNodeRegex, String storee) {\n+        String regex = START + irNodeRegex + MID + LOAD_STORE_PREFIX + storee + LOAD_STORE_SUFFIX + END;\n","filename":"test\/hotspot\/jtreg\/compiler\/lib\/ir_framework\/IRNode.java","additions":95,"deletions":38,"binary":false,"changes":133,"status":"modified"},{"patch":"@@ -37,0 +37,3 @@\n+# Valhalla\n+java\/lang\/Thread\/virtual\/stress\/PingPong.java 8342977 generic-all\n+java\/lang\/Thread\/virtual\/stress\/Skynet.java 8342977 generic-all\n","filename":"test\/jdk\/ProblemList-Virtual.txt","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -532,0 +532,5 @@\n+java\/lang\/ModuleLayer\/LayerControllerTest.java                  8337048 generic-all\n+java\/lang\/ModuleLayer\/BasicLayerTest.java                       8337048 generic-all\n+\n+java\/lang\/Thread\/virtual\/stress\/Skynet.java#default             8342977 generic-all\n+\n@@ -549,0 +554,1 @@\n+com\/sun\/management\/HotSpotDiagnosticMXBean\/DumpThreadsWithEliminatedLock.java 8360599 generic-all\n@@ -570,0 +576,2 @@\n+java\/net\/CookieHandler\/B6644726.java                            8365811 generic-all\n+\n@@ -720,0 +728,4 @@\n+com\/sun\/jdi\/cds\/CDSBreakpointTest.java                          8304168 generic-all\n+com\/sun\/jdi\/cds\/CDSDeleteAllBkptsTest.java                      8304168 generic-all\n+com\/sun\/jdi\/cds\/CDSFieldWatchpoints.java                        8304168 generic-all\n+\n@@ -728,0 +740,1 @@\n+java\/util\/logging\/LoggingDeadlock2.java       8368801 generic-all\n@@ -814,0 +827,1 @@\n+\n@@ -819,0 +833,13 @@\n+\n+############################################################################\n+\n+# valhalla\n+java\/lang\/invoke\/VarHandles\/VarHandleTestMethodHandleAccessValue.java 8367346 generic-all\n+\n+jdk\/classfile\/AccessFlagsTest.java 8366270 generic-all\n+jdk\/jfr\/event\/runtime\/TestClassLoaderStatsEvent.java 8366820 generic-all\n+\n+sun\/tools\/jhsdb\/BasicLauncherTest.java 8366806 generic-all\n+sun\/tools\/jhsdb\/HeapDumpTest.java 8366806 generic-all\n+sun\/tools\/jhsdb\/HeapDumpTestWithActiveProcess.java 8366806 generic-all\n+sun\/tools\/jhsdb\/JShellHeapDumpTest.java 8366806 generic-all\n","filename":"test\/jdk\/ProblemList.txt","additions":27,"deletions":0,"binary":false,"changes":27,"status":"modified"},{"patch":"@@ -29,1 +29,1 @@\n- *          4802647 7123424 8024709 8193128 8327858 8368178\n+ *          4802647 7123424 8024709 8193128 8327858 8346307 8368178\n@@ -35,0 +35,1 @@\n+ * @run main MOAT --enable-preview\n","filename":"test\/jdk\/java\/util\/Collection\/MOAT.java","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"}]}