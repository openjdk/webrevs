{"files":[{"patch":"@@ -4668,3 +4668,2 @@\n-  void dilithium_load16zetas(int o0, Register zetas) {\n-    __ ldpq(as_FloatRegister(o0), as_FloatRegister(o0 + 1), __ post (zetas, 32));\n-    __ ldpq(as_FloatRegister(o0 + 2), as_FloatRegister(o0 + 3), __ post (zetas, 32));\n+  \/\/ Helpers to schedule parallel operation bundles across vector\n+  \/\/ register sequences of size 2, 4 or 8.\n@@ -4672,0 +4671,162 @@\n+  \/\/ Implement various primitive computations across vector sequences\n+\n+  template<int N>\n+  void vs_addv(const VSeq<N>& v, Assembler::SIMD_Arrangement T,\n+               const VSeq<N>& v1, const VSeq<N>& v2) {\n+    for (int i = 0; i < N; i++) {\n+      __ addv(v[i], T, v1[i], v2[i]);\n+    }\n+  }\n+\n+  template<int N>\n+  void vs_subv(const VSeq<N>& v, Assembler::SIMD_Arrangement T,\n+               const VSeq<N>& v1, const VSeq<N>& v2) {\n+    for (int i = 0; i < N; i++) {\n+      __ subv(v[i], T, v1[i], v2[i]);\n+    }\n+  }\n+\n+  template<int N>\n+  void vs_mulv(const VSeq<N>& v, Assembler::SIMD_Arrangement T,\n+               const VSeq<N>& v1, const VSeq<N>& v2) {\n+    for (int i = 0; i < N; i++) {\n+      __ mulv(v[i], T, v1[i], v2[i]);\n+    }\n+  }\n+\n+  template<int N>\n+  void vs_negr(const VSeq<N>& v, Assembler::SIMD_Arrangement T, const VSeq<N>& v1) {\n+    for (int i = 0; i < N; i++) {\n+      __ negr(v[i], T, v1[i]);\n+    }\n+  }\n+\n+  template<int N>\n+  void vs_sshr(const VSeq<N>& v, Assembler::SIMD_Arrangement T,\n+               const VSeq<N>& v1, int shift) {\n+    for (int i = 0; i < N; i++) {\n+      __ sshr(v[i], T, v1[i], shift);\n+    }\n+  }\n+\n+  template<int N>\n+  void vs_andr(const VSeq<N>& v, const VSeq<N>& v1, const VSeq<N>& v2) {\n+    for (int i = 0; i < N; i++) {\n+      __ andr(v[i], __ T16B, v1[i], v2[i]);\n+    }\n+  }\n+\n+  template<int N>\n+  void vs_orr(const VSeq<N>& v, const VSeq<N>& v1, const VSeq<N>& v2) {\n+    for (int i = 0; i < N; i++) {\n+      __ orr(v[i], __ T16B, v1[i], v2[i]);\n+    }\n+  }\n+\n+  template<int N>\n+    void vs_notr(const VSeq<N>& v, const VSeq<N>& v1) {\n+    for (int i = 0; i < N; i++) {\n+      __ notr(v[i], __ T16B, v1[i]);\n+    }\n+  }\n+\n+  \/\/ load N\/2 successive pairs of quadword values from memory in order\n+  \/\/ into N successive vector registers of the sequence via the\n+  \/\/ address supplied in base.\n+  template<int N>\n+  void vs_ldpq(const VSeq<N>& v, Register base) {\n+    for (int i = 0; i < N; i += 2) {\n+      __ ldpq(v[i], v[i+1], Address(base, 32 * i));\n+    }\n+  }\n+\n+  \/\/ load N\/2 successive pairs of quadword values from memory in order\n+  \/\/ into N vector registers of the sequence via the address supplied\n+  \/\/ in base using post-increment addressing\n+  template<int N>\n+  void vs_ldpq_post(const VSeq<N>& v, Register base) {\n+    for (int i = 0; i < N; i += 2) {\n+      __ ldpq(v[i], v[i+1], __ post(base, 32));\n+    }\n+  }\n+\n+  \/\/ store N successive vector registers of the sequence into N\/2\n+  \/\/ successive pairs of quadword memory locations via the address\n+  \/\/ supplied in base using post-increment addressing\n+  template<int N>\n+  void vs_stpq_post(const VSeq<N>& v, Register base) {\n+    for (int i = 0; i < N; i += 2) {\n+      __ stpq(v[i], v[i+1], __ post(base, 32));\n+    }\n+  }\n+\n+  \/\/ load N\/2 pairs of quadword values from memory into N vector\n+  \/\/ registers via the address supplied in base with each pair indexed\n+  \/\/ using the the start offset plus the corresponding entry in the\n+  \/\/ offsets array\n+  template<int N>\n+  void vs_ldpq_indexed(const VSeq<N>& v, Register base, int start, int (&offsets)[N\/2]) {\n+    for (int i = 0; i < N\/2; i++) {\n+      __ ldpq(v[2*i], v[2*i+1], Address(base, start + offsets[i]));\n+    }\n+  }\n+\n+  \/\/ store N vector registers into N\/2 pairs of quadword memory\n+  \/\/ locations via the address supplied in base with each pair indexed\n+  \/\/ using the the start offset plus the corresponding entry in the\n+  \/\/ offsets array\n+  template<int N>\n+  void vs_stpq_indexed(const VSeq<N>& v, Register base, int start, int offsets[N\/2]) {\n+    for (int i = 0; i < N\/2; i++) {\n+      __ stpq(v[2*i], v[2*i+1], Address(base, start + offsets[i]));\n+    }\n+  }\n+\n+  \/\/ load N single quadword values from memory into N vector registers\n+  \/\/ via the address supplied in base with each value indexed using\n+  \/\/ the the start offset plus the corresponding entry in the offsets\n+  \/\/ array\n+  template<int N>\n+  void vs_ldr_indexed(const VSeq<N>& v, Assembler::SIMD_RegVariant T, Register base,\n+                      int start, int (&offsets)[N]) {\n+    for (int i = 0; i < N; i++) {\n+      __ ldr(v[i], T, Address(base, start + offsets[i]));\n+    }\n+  }\n+\n+  \/\/ store N vector registers into N single quadword memory locations\n+  \/\/ via the address supplied in base with each value indexed using\n+  \/\/ the the start offset plus the corresponding entry in the offsets\n+  \/\/ array\n+  template<int N>\n+  void vs_str_indexed(const VSeq<N>& v, Assembler::SIMD_RegVariant T, Register base,\n+                      int start, int (&offsets)[N]) {\n+    for (int i = 0; i < N; i++) {\n+      __ str(v[i], T, Address(base, start + offsets[i]));\n+    }\n+  }\n+\n+  \/\/ load N\/2 pairs of quadword values from memory de-interleaved into\n+  \/\/ N vector registers 2 at a time via the address supplied in base\n+  \/\/ with each pair indexed using the the start offset plus the\n+  \/\/ corresponding entry in the offsets array\n+  template<int N>\n+  void vs_ld2_indexed(const VSeq<N>& v, Assembler::SIMD_Arrangement T, Register base,\n+                      Register tmp, int start, int (&offsets)[N\/2]) {\n+    for (int i = 0; i < N\/2; i++) {\n+      __ add(tmp, base, start + offsets[i]);\n+      __ ld2(v[2*i], v[2*i+1], T, tmp);\n+    }\n+  }\n+\n+  \/\/ store N vector registers 2 at a time interleaved into N\/2 pairs\n+  \/\/ of quadword memory locations via the address supplied in base\n+  \/\/ with each pair indexed using the the start offset plus the\n+  \/\/ corresponding entry in the offsets array\n+  template<int N>\n+  void vs_st2_indexed(const VSeq<N>& v, Assembler::SIMD_Arrangement T, Register base,\n+                      Register tmp, int start, int (&offsets)[N\/2]) {\n+    for (int i = 0; i < N\/2; i++) {\n+      __ add(tmp, base, start + offsets[i]);\n+      __ st2(v[2*i], v[2*i+1], T, tmp);\n+    }\n@@ -4674,3 +4835,48 @@\n-  void dilithium_load32zetas(Register zetas) {\n-    dilithium_load16zetas(16, zetas);\n-    dilithium_load16zetas(20, zetas);\n+  \/\/ Helper routines for various flavours of dilithium montgomery\n+  \/\/ multiply\n+\n+  \/\/ Perform 16 32-bit Montgomery multiplications in parallel\n+  \/\/ See the montMul() method of the sun.security.provider.ML_DSA class.\n+  \/\/\n+  \/\/ Computes 4x4S results\n+  \/\/    a = b * c * 2^-32 mod MONT_Q\n+  \/\/ Inputs:  vb, vc - 4x4S vector register sequences\n+  \/\/          vq - 2x4S constants <MONT_Q, MONT_Q_INV_MOD_R>\n+  \/\/ Temps:   vtmp - 4x4S vector sequence trashed after call\n+  \/\/ Outputs: va - 4x4S vector register sequences\n+  \/\/ vb, vc, vtmp and vq must all be disjoint\n+  \/\/ va must be disjoint from all other inputs\/temps or must equal vc\n+  \/\/ n.b. MONT_R_BITS is 32, so the right shift by it is implicit.\n+  void dilithium_montmul16(const VSeq<4>& va, const VSeq<4>& vb, const VSeq<4>& vc,\n+                    const VSeq<4>& vtmp, const VSeq<2>& vq) {\n+    assert(vs_disjoint(vb, vc), \"vb and vc overlap\");\n+    assert(vs_disjoint(vb, vq), \"vb and vq overlap\");\n+    assert(vs_disjoint(vb, vtmp), \"vb and vtmp overlap\");\n+\n+    assert(vs_disjoint(vc, vq), \"vc and vq overlap\");\n+    assert(vs_disjoint(vc, vtmp), \"vc and vtmp overlap\");\n+\n+    assert(vs_disjoint(vq, vtmp), \"vq and vtmp overlap\");\n+\n+    assert(vs_disjoint(va, vc) || vs_same(va, vc), \"va and vc neither disjoint nor equal\");\n+    assert(vs_disjoint(va, vb), \"va and vb overlap\");\n+    assert(vs_disjoint(va, vq), \"va and vq overlap\");\n+    assert(vs_disjoint(va, vtmp), \"va and vtmp overlap\");\n+\n+    \/\/ schedule 4 streams of instructions across the vector sequences\n+    for (int i = 0; i < 4; i++) {\n+      __ sqdmulh(vtmp[i], __ T4S, vb[i], vc[i]); \/\/ aHigh = hi32(2 * b * c)\n+      __ mulv(va[i], __ T4S, vb[i], vc[i]);    \/\/ aLow = lo32(b * c)\n+    }\n+\n+    for (int i = 0; i < 4; i++) {\n+      __ mulv(va[i], __ T4S, va[i], vq[0]);     \/\/ m = aLow * qinv\n+    }\n+\n+    for (int i = 0; i < 4; i++) {\n+      __ sqdmulh(va[i], __ T4S, va[i], vq[1]);  \/\/ n = hi32(2 * m * q)\n+    }\n+\n+    for (int i = 0; i < 4; i++) {\n+      __ shsubv(va[i], __ T4S, vtmp[i], va[i]);   \/\/ a = (aHigh - n) \/ 2\n+    }\n@@ -4679,1 +4885,1 @@\n-  \/\/ 2x16 32-bit Montgomery multiplications in parallel\n+  \/\/ Perform 2x16 32-bit Montgomery multiplications in parallel\n@@ -4681,63 +4887,41 @@\n-  \/\/ Here MONT_R_BITS is 32, so the right shift by it is implicit.\n-  \/\/ The constants qInv = MONT_Q_INV_MOD_R and q = MONT_Q are loaded in\n-  \/\/ (all 32-bit chunks of) vector registers v30 and v31, resp.\n-  \/\/ The inputs are b[i]s in v0-v7 and c[i]s v16-v23 and\n-  \/\/ the results are a[i]s in v16-v23, four 32-bit values in each register\n-  \/\/ and we do a_i = b_i * c_i * 2^-32 mod MONT_Q for all\n-  void dilithium_montmul32(bool by_constant) {\n-    FloatRegister vr0 = by_constant ? v29 : v0;\n-    FloatRegister vr1 = by_constant ? v29 : v1;\n-    FloatRegister vr2 = by_constant ? v29 : v2;\n-    FloatRegister vr3 = by_constant ? v29 : v3;\n-    FloatRegister vr4 = by_constant ? v29 : v4;\n-    FloatRegister vr5 = by_constant ? v29 : v5;\n-    FloatRegister vr6 = by_constant ? v29 : v6;\n-    FloatRegister vr7 = by_constant ? v29 : v7;\n-\n-    __ sqdmulh(v24, __ T4S, vr0, v16); \/\/ aHigh = hi32(2 * b * c)\n-    __ mulv(v16, __ T4S, vr0, v16);    \/\/ aLow = lo32(b * c)\n-    __ sqdmulh(v25, __ T4S, vr1, v17);\n-    __ mulv(v17, __ T4S, vr1, v17);\n-    __ sqdmulh(v26, __ T4S, vr2, v18);\n-    __ mulv(v18, __ T4S, vr2, v18);\n-    __ sqdmulh(v27, __ T4S, vr3, v19);\n-    __ mulv(v19, __ T4S, vr3, v19);\n-\n-    __ mulv(v16, __ T4S, v16, v30);     \/\/ m = aLow * qinv\n-    __ mulv(v17, __ T4S, v17, v30);\n-    __ mulv(v18, __ T4S, v18, v30);\n-    __ mulv(v19, __ T4S, v19, v30);\n-\n-    __ sqdmulh(v16, __ T4S, v16, v31);  \/\/ n = hi32(2 * m * q)\n-    __ sqdmulh(v17, __ T4S, v17, v31);\n-    __ sqdmulh(v18, __ T4S, v18, v31);\n-    __ sqdmulh(v19, __ T4S, v19, v31);\n-\n-    __ shsubv(v16, __ T4S, v24, v16);   \/\/ a = (aHigh - n) \/ 2\n-    __ shsubv(v17, __ T4S, v25, v17);\n-    __ shsubv(v18, __ T4S, v26, v18);\n-    __ shsubv(v19, __ T4S, v27, v19);\n-\n-    __ sqdmulh(v24, __ T4S, vr4, v20);\n-    __ mulv(v20, __ T4S, vr4, v20);\n-    __ sqdmulh(v25, __ T4S, vr5, v21);\n-    __ mulv(v21, __ T4S, vr5, v21);\n-    __ sqdmulh(v26, __ T4S, vr6, v22);\n-    __ mulv(v22, __ T4S, vr6, v22);\n-    __ sqdmulh(v27, __ T4S, vr7, v23);\n-    __ mulv(v23, __ T4S, vr7, v23);\n-\n-    __ mulv(v20, __ T4S, v20, v30);\n-    __ mulv(v21, __ T4S, v21, v30);\n-    __ mulv(v22, __ T4S, v22, v30);\n-    __ mulv(v23, __ T4S, v23, v30);\n-\n-    __ sqdmulh(v20, __ T4S, v20, v31);\n-    __ sqdmulh(v21, __ T4S, v21, v31);\n-    __ sqdmulh(v22, __ T4S, v22, v31);\n-    __ sqdmulh(v23, __ T4S, v23, v31);\n-\n-    __ shsubv(v20, __ T4S, v24, v20);\n-    __ shsubv(v21, __ T4S, v25, v21);\n-    __ shsubv(v22, __ T4S, v26, v22);\n-    __ shsubv(v23, __ T4S, v27, v23);\n+  \/\/\n+  \/\/ Computes 8x4S results\n+  \/\/    a = b * c * 2^-32 mod MONT_Q\n+  \/\/ Inputs:  vb, vc - 8x4S vector register sequences\n+  \/\/          vq - 2x4S constants <MONT_Q, MONT_Q_INV_MOD_R>\n+  \/\/ Temps:   vtmp - 4x4S vector sequence trashed after call\n+  \/\/ Outputs: va - 8x4S vector register sequences\n+  \/\/ vb, vc, vtmp and vq must all be disjoint\n+  \/\/ va must be disjoint from all other inputs\/temps or must equal vc\n+  \/\/ n.b. MONT_R_BITS is 32, so the right shift by it is implicit.\n+  void vs_montmul32(const VSeq<8>& va, const VSeq<8>& vb, const VSeq<8>& vc,\n+                    const VSeq<4>& vtmp, const VSeq<2>& vq) {\n+    \/\/ vb, vc, vtmp and vq must be disjoint. va must either be\n+    \/\/ disjoint from all other registers or equal vc\n+\n+    assert(vs_disjoint(vb, vc), \"vb and vc overlap\");\n+    assert(vs_disjoint(vb, vq), \"vb and vq overlap\");\n+    assert(vs_disjoint(vb, vtmp), \"vb and vtmp overlap\");\n+\n+    assert(vs_disjoint(vc, vq), \"vc and vq overlap\");\n+    assert(vs_disjoint(vc, vtmp), \"vc and vtmp overlap\");\n+\n+    assert(vs_disjoint(vq, vtmp), \"vq and vtmp overlap\");\n+\n+    assert(vs_disjoint(va, vc) || vs_same(va, vc), \"va and vc neither disjoint nor equal\");\n+    assert(vs_disjoint(va, vb), \"va and vb overlap\");\n+    assert(vs_disjoint(va, vq), \"va and vq overlap\");\n+    assert(vs_disjoint(va, vtmp), \"va and vtmp overlap\");\n+\n+    \/\/ we need to multiply the front and back halves of each sequence\n+    \/\/ 4x4S at a time because\n+    \/\/\n+    \/\/ 1) we are currently only able to get 4-way instruction\n+    \/\/ parallelism at best\n+    \/\/\n+    \/\/ 2) we need registers for the constants in vq and temporary\n+    \/\/ scratch registers to hold intermediate results so vtmp can only\n+    \/\/ be a VSeq<4> which means we only have 4 scratch slots\n+\n+    dilithium_montmul16(vs_front(va), vs_front(vb), vs_front(vc), vtmp, vq);\n+    dilithium_montmul16(vs_back(va), vs_back(vb), vs_back(vc), vtmp, vq);\n@@ -4746,20 +4930,10 @@\n- \/\/ Do the addition and subtraction done in the ntt algorithm.\n- \/\/ See sun.security.provider.ML_DSA.implDilithiumAlmostNttJava()\n-  void dilithium_add_sub32() {\n-    __ addv(v24, __ T4S, v0, v16); \/\/ coeffs[j] = coeffs[j] + tmp;\n-    __ addv(v25, __ T4S, v1, v17);\n-    __ addv(v26, __ T4S, v2, v18);\n-    __ addv(v27, __ T4S, v3, v19);\n-    __ addv(v28, __ T4S, v4, v20);\n-    __ addv(v29, __ T4S, v5, v21);\n-    __ addv(v30, __ T4S, v6, v22);\n-    __ addv(v31, __ T4S, v7, v23);\n-\n-    __ subv(v0, __ T4S, v0, v16);  \/\/ coeffs[j + l] = coeffs[j] - tmp;\n-    __ subv(v1, __ T4S, v1, v17);\n-    __ subv(v2, __ T4S, v2, v18);\n-    __ subv(v3, __ T4S, v3, v19);\n-    __ subv(v4, __ T4S, v4, v20);\n-    __ subv(v5, __ T4S, v5, v21);\n-    __ subv(v6, __ T4S, v6, v22);\n-    __ subv(v7, __ T4S, v7, v23);\n+  \/\/ perform combined montmul then add\/sub on 4x4S vectors\n+\n+  void dilithium_montmul16_sub_add(const VSeq<4>& va0, const VSeq<4>& va1, const VSeq<4>& vc,\n+                                   const VSeq<4>& vtmp, const VSeq<2>& vq) {\n+    \/\/ compute a = montmul(a1, c)\n+    dilithium_montmul16(vc, va1, vc, vtmp, vq);\n+    \/\/ ouptut a1 = a0 - a\n+    vs_subv(va1, __ T4S, va0, vc);\n+    \/\/    and a0 = a0 + a\n+    vs_addv(va0, __ T4S, va0, vc);\n@@ -4768,38 +4942,10 @@\n-  \/\/ Do the same computation that\n-  \/\/ dilithium_montmul32() and dilithium_add_sub32() does,\n-  \/\/ except for only 4x4 32-bit vector elements and with\n-  \/\/ different register usage.\n-  void dilithium_montmul_sub_add16() {\n-    __ sqdmulh(v24, __ T4S, v1, v16);\n-    __ mulv(v16, __ T4S, v1, v16);\n-    __ sqdmulh(v25, __ T4S, v3, v17);\n-    __ mulv(v17, __ T4S, v3, v17);\n-    __ sqdmulh(v26, __ T4S, v5, v18);\n-    __ mulv(v18, __ T4S, v5, v18);\n-    __ sqdmulh(v27, __ T4S, v7, v19);\n-    __ mulv(v19, __ T4S, v7, v19);\n-\n-    __ mulv(v16, __ T4S, v16, v30);\n-    __ mulv(v17, __ T4S, v17, v30);\n-    __ mulv(v18, __ T4S, v18, v30);\n-    __ mulv(v19, __ T4S, v19, v30);\n-\n-    __ sqdmulh(v16, __ T4S, v16, v31);\n-    __ sqdmulh(v17, __ T4S, v17, v31);\n-    __ sqdmulh(v18, __ T4S, v18, v31);\n-    __ sqdmulh(v19, __ T4S, v19, v31);\n-\n-    __ shsubv(v16, __ T4S, v24, v16);\n-    __ shsubv(v17, __ T4S, v25, v17);\n-    __ shsubv(v18, __ T4S, v26, v18);\n-    __ shsubv(v19, __ T4S, v27, v19);\n-\n-    __ subv(v1, __ T4S, v0, v16);\n-    __ subv(v3, __ T4S, v2, v17);\n-    __ subv(v5, __ T4S, v4, v18);\n-    __ subv(v7, __ T4S, v6, v19);\n-\n-    __ addv(v0, __ T4S, v0, v16);\n-    __ addv(v2, __ T4S, v2, v17);\n-    __ addv(v4, __ T4S, v4, v18);\n-    __ addv(v6, __ T4S, v6, v19);\n+  \/\/ perform combined add\/sub then montul on 4x4S vectors\n+\n+  void dilithium_sub_add_montmul16(const VSeq<4>& va0, const VSeq<4>& va1, const VSeq<4>& vb,\n+                                   const VSeq<4>& vtmp1, const VSeq<4>& vtmp2, const VSeq<2>& vq) {\n+    \/\/ compute c = a0 - a1\n+    vs_subv(vtmp1, __ T4S, va0, va1);\n+    \/\/ output a0 = a0 + a1\n+    vs_addv(va0, __ T4S, va0, va1);\n+    \/\/ output a1 = b montmul c\n+    dilithium_montmul16(va1, vtmp1, vb, vtmp2, vq);\n@@ -4823,3 +4969,5 @@\n-    int incr1 = 32;\n-    int incr2 = 64;\n-    int incr3 = 96;\n+    \/\/ don't use callee save registers v8 - v15\n+    VSeq<8> vs1(0), vs2(16), vs3(24);  \/\/ 3 sets of 8x4s inputs\/outputs\n+    VSeq<4> vtmp = vs_front(vs3);         \/\/ n.b. tmp registers overlap vs3\n+    VSeq<2> vq(30);                    \/\/ n.b. constants overlap vs3\n+    int offsets[4] = { 0, 32, 64, 96 };\n@@ -4831,3 +4979,3 @@\n-        incr1 = 32;\n-        incr2 = 128;\n-        incr3 = 160;\n+        offsets[1] = 32;\n+        offsets[2] = 128;\n+        offsets[3] = 160;\n@@ -4835,3 +4983,3 @@\n-        incr1 = 64;\n-        incr2 = 128;\n-        incr3 = 192;\n+        offsets[1] = 64;\n+        offsets[2] = 128;\n+        offsets[3] = 192;\n@@ -4840,0 +4988,4 @@\n+      \/\/ for levels 1 - 4 we simply load 2 x 4 adjacent values at a\n+      \/\/ time at 4 different offsets and multiply them in order by the\n+      \/\/ next set of input values. So we employ indexed load and store\n+      \/\/ pair instructions with arrangement 4S\n@@ -4841,20 +4993,17 @@\n-        __ ldpq(v30, v31, Address(dilithiumConsts, 0)); \/\/ qInv, q\n-        __ ldpq(v0, v1, Address(coeffs, c2Start));\n-        __ ldpq(v2, v3, Address(coeffs, c2Start + incr1));\n-        __ ldpq(v4, v5, Address(coeffs, c2Start + incr2));\n-        __ ldpq(v6, v7, Address(coeffs, c2Start + incr3));\n-        dilithium_load32zetas(zetas);\n-        dilithium_montmul32(false);\n-        __ ldpq(v0, v1, Address(coeffs, c1Start));\n-        __ ldpq(v2, v3, Address(coeffs, c1Start + incr1));\n-        __ ldpq(v4, v5, Address(coeffs, c1Start + incr2));\n-        __ ldpq(v6, v7, Address(coeffs, c1Start + incr3));\n-        dilithium_add_sub32();\n-        __ stpq(v24, v25, Address(coeffs, c1Start));\n-        __ stpq(v26, v27, Address(coeffs, c1Start + incr1));\n-        __ stpq(v28, v29, Address(coeffs, c1Start + incr2));\n-        __ stpq(v30, v31, Address(coeffs, c1Start + incr3));\n-        __ stpq(v0, v1, Address(coeffs, c2Start));\n-        __ stpq(v2, v3, Address(coeffs, c2Start + incr1));\n-        __ stpq(v4, v5, Address(coeffs, c2Start + incr2));\n-        __ stpq(v6, v7, Address(coeffs, c2Start + incr3));\n+        \/\/ reload q and qinv\n+        vs_ldpq(vq, dilithiumConsts); \/\/ qInv, q\n+        \/\/ load 8x4S coefficients via second start pos == c2\n+        vs_ldpq_indexed(vs1, coeffs, c2Start, offsets);\n+        \/\/ load next 8x4S inputs == b\n+        vs_ldpq_post(vs2, zetas);\n+        \/\/ compute a == c2 * b mod MONT_Q\n+        vs_montmul32(vs2, vs1, vs2, vtmp, vq);\n+        \/\/ load 8x4s coefficients via first start pos == c1\n+        vs_ldpq_indexed(vs1, coeffs, c1Start, offsets);\n+        \/\/ compute a1 =  c1 + a\n+        vs_addv(vs3, __ T4S, vs1, vs2);\n+        \/\/ compute a2 =  c1 - a\n+        vs_subv(vs1, __ T4S, vs1, vs2);\n+        \/\/ output a1 and a2\n+        vs_stpq_indexed(vs3, coeffs, c1Start, offsets);\n+        vs_stpq_indexed(vs1, coeffs, c2Start, offsets);\n@@ -4901,1 +5050,7 @@\n-\n+    \/\/ don't use callee save registers v8 - v15\n+    VSeq<8> vs1(0), vs2(16), vs3(24);  \/\/ 3 sets of 8x4s inputs\/outputs\n+    VSeq<4> vtmp = vs_front(vs3);         \/\/ n.b. tmp registers overlap vs3\n+    VSeq<2> vq(30);                    \/\/ n.b. constants overlap vs3\n+    int offsets[4] = {0, 32, 64, 96};\n+    int offsets1[8] = {16, 48, 80, 112, 144, 176, 208, 240 };\n+    int offsets2[8] = { 0, 32, 64, 96, 128, 160, 192, 224 };\n@@ -4911,0 +5066,7 @@\n+\n+    \/\/ at level 5 the coefficients we need to combine with the zetas\n+    \/\/ are grouped in memory in blocks of size 4. So, for both sets of\n+    \/\/ coefficients we load 4 adjacent values at 8 different offsets\n+    \/\/ using an indexed ldr with register variant Q and multiply them\n+    \/\/ in sequence order by the next set of inputs. Likewise we store\n+    \/\/ the resuls using an indexed str with register variant Q.\n@@ -4912,36 +5074,16 @@\n-      __ ldpq(v30, v31, Address(dilithiumConsts, 0));  \/\/ qInv, q\n-      __ ldr(v0, __ Q, Address(coeffs, i + 16));\n-      __ ldr(v1, __ Q, Address(coeffs, i + 48));\n-      __ ldr(v2, __ Q, Address(coeffs, i + 80));\n-      __ ldr(v3, __ Q, Address(coeffs, i + 112));\n-      __ ldr(v4, __ Q, Address(coeffs, i + 144));\n-      __ ldr(v5, __ Q, Address(coeffs, i + 176));\n-      __ ldr(v6, __ Q, Address(coeffs, i + 208));\n-      __ ldr(v7, __ Q, Address(coeffs, i + 240));\n-      dilithium_load32zetas(zetas);\n-      dilithium_montmul32(false);\n-      __ ldr(v0, __ Q, Address(coeffs, i));\n-      __ ldr(v1, __ Q, Address(coeffs, i + 32));\n-      __ ldr(v2, __ Q, Address(coeffs, i + 64));\n-      __ ldr(v3, __ Q, Address(coeffs, i + 96));\n-      __ ldr(v4, __ Q, Address(coeffs, i + 128));\n-      __ ldr(v5, __ Q, Address(coeffs, i + 160));\n-      __ ldr(v6, __ Q, Address(coeffs, i + 192));\n-      __ ldr(v7, __ Q, Address(coeffs, i + 224));\n-      dilithium_add_sub32();\n-      __ str(v24, __ Q, Address(coeffs, i));\n-      __ str(v25, __ Q, Address(coeffs, i + 32));\n-      __ str(v26, __ Q, Address(coeffs, i + 64));\n-      __ str(v27, __ Q, Address(coeffs, i + 96));\n-      __ str(v28, __ Q, Address(coeffs, i + 128));\n-      __ str(v29, __ Q, Address(coeffs, i + 160));\n-      __ str(v30, __ Q, Address(coeffs, i + 192));\n-      __ str(v31, __ Q, Address(coeffs, i + 224));\n-      __ str(v0, __ Q, Address(coeffs, i + 16));\n-      __ str(v1, __ Q, Address(coeffs, i + 48));\n-      __ str(v2, __ Q, Address(coeffs, i + 80));\n-      __ str(v3, __ Q, Address(coeffs, i + 112));\n-      __ str(v4, __ Q, Address(coeffs, i + 144));\n-      __ str(v5, __ Q, Address(coeffs, i + 176));\n-      __ str(v6, __ Q, Address(coeffs, i + 208));\n-      __ str(v7, __ Q, Address(coeffs, i + 240));\n+      \/\/ reload constants q, qinv each iteration as they get clobbered later\n+      vs_ldpq(vq, dilithiumConsts); \/\/ qInv, q\n+      \/\/ load 32 (8x4S) coefficients via first offsets = c1\n+      vs_ldr_indexed(vs1, __ Q, coeffs, i, offsets1);\n+      \/\/ load next 32 (8x4S) inputs = b\n+      vs_ldpq_post(vs2, zetas);\n+      \/\/ a = b montul c1\n+      vs_montmul32(vs2, vs1, vs2, vtmp, vq);\n+      \/\/ load 32 (8x4S) coefficients via second offsets = c2\n+      vs_ldr_indexed(vs1, __ Q, coeffs, i, offsets2);\n+      \/\/ add\/sub with result of multiply\n+      vs_addv(vs3, __ T4S, vs1, vs2);     \/\/ a1 = a - c2\n+      vs_subv(vs1, __ T4S, vs1, vs2);     \/\/ a0 = a + c1\n+      \/\/ write back new coefficients using same offsets\n+      vs_str_indexed(vs3, __ Q, coeffs, i, offsets2);\n+      vs_str_indexed(vs1, __ Q, coeffs, i, offsets1);\n@@ -4951,0 +5093,13 @@\n+    \/\/ at level 6 the coefficients we need to combine with the zetas\n+    \/\/ are grouped in memory in pairs, the first two being montmul\n+    \/\/ inputs and the second add\/sub inputs. We can still implement\n+    \/\/ the montmul+sub+add using 4-way parallelism but only if we\n+    \/\/ combine the coefficients with the zetas 16 at a time. We load 8\n+    \/\/ adjacent values at 4 different offsets using an ld2 load with\n+    \/\/ arrangement 2D. That interleaves the lower and upper halves of\n+    \/\/ each pair of quadwords into successive vector registers. We\n+    \/\/ then need to montmul the 4 even elements of the coefficients\n+    \/\/ register sequence by the zetas in order and then add\/sub the 4\n+    \/\/ odd elements of the coefficients register sequence. We use an\n+    \/\/ equivalent st2 operation to store the results back into memory\n+    \/\/ de-interleaved.\n@@ -4952,19 +5107,11 @@\n-      __ ldpq(v30, v31, Address(dilithiumConsts, 0));  \/\/ qInv, q\n-      __ add(tmpAddr, coeffs, i);\n-      __ ld2(v0, v1, __ T2D, tmpAddr);\n-      __ add(tmpAddr, coeffs, i + 32);\n-      __ ld2(v2, v3, __ T2D, tmpAddr);\n-      __ add(tmpAddr, coeffs, i + 64);\n-      __ ld2(v4, v5, __ T2D, tmpAddr);\n-      __ add(tmpAddr, coeffs, i + 96);\n-      __ ld2(v6, v7, __ T2D, tmpAddr);\n-      dilithium_load16zetas(16, zetas);\n-      dilithium_montmul_sub_add16();\n-      __ add(tmpAddr, coeffs, i);\n-      __ st2(v0, v1, __ T2D, tmpAddr);\n-      __ add(tmpAddr, coeffs, i + 32);\n-      __ st2(v2, v3, __ T2D, tmpAddr);\n-      __ add(tmpAddr, coeffs, i + 64);\n-      __ st2(v4, v5, __ T2D, tmpAddr);\n-      __ add(tmpAddr, coeffs, i + 96);\n-      __ st2(v6, v7, __ T2D, tmpAddr);\n+      \/\/ reload constants q, qinv each iteration as they get clobbered later\n+      vs_ldpq(vq, dilithiumConsts); \/\/ qInv, q\n+      \/\/ load interleaved 16 (4x2D) coefficients via offsets\n+      vs_ld2_indexed(vs1, __ T2D, coeffs, tmpAddr, i, offsets);\n+      \/\/ load next 16 (4x4S) inputs\n+      vs_ldpq_post(vs_front(vs2), zetas);\n+      \/\/ mont multiply odd elements of vs1 by vs2 and add\/sub into odds\/evens\n+      dilithium_montmul16_sub_add(vs_even(vs1), vs_odd(vs1),\n+                                  vs_front(vs2), vtmp, vq);\n+      \/\/ store interleaved 16 (4x2D) coefficients via offsets\n+      vs_st2_indexed(vs1, __ T2D, coeffs, tmpAddr, i, offsets);\n@@ -4974,0 +5121,13 @@\n+    \/\/ at level 7 the coefficients we need to combine with the zetas\n+    \/\/ occur singly with montmul inputs alterating with add\/sub\n+    \/\/ inputs. Once again we can use 4-way parallelism to combine 16\n+    \/\/ zetas at a time. However, we have to load 8 adjacent values at\n+    \/\/ 4 different offsets using an ld2 load with arrangement 4S. That\n+    \/\/ interleaves the the odd words of each pair into one\n+    \/\/ coefficients vector register and the even words of the pair\n+    \/\/ into the next register. We then need to montmul the 4 even\n+    \/\/ elements of the coefficients register sequence by the zetas in\n+    \/\/ order and then add\/sub the 4 odd elements of the coefficients\n+    \/\/ register sequence. We use an equivalent st2 operation to store\n+    \/\/ the results back into memory de-interleaved.\n+\n@@ -4975,19 +5135,11 @@\n-      __ ldpq(v30, v31, Address(dilithiumConsts, 0));  \/\/ qInv, q\n-      __ add(tmpAddr, coeffs, i);\n-      __ ld2(v0, v1, __ T4S, tmpAddr);\n-      __ add(tmpAddr, coeffs, i + 32);\n-      __ ld2(v2, v3, __ T4S, tmpAddr);\n-      __ add(tmpAddr, coeffs, i + 64);\n-      __ ld2(v4, v5, __ T4S, tmpAddr);\n-      __ add(tmpAddr, coeffs, i + 96);\n-      __ ld2(v6, v7, __ T4S, tmpAddr);\n-      dilithium_load16zetas(16, zetas);\n-      dilithium_montmul_sub_add16();\n-      __ add(tmpAddr, coeffs, i);\n-      __ st2(v0, v1, __ T4S, tmpAddr);\n-      __ add(tmpAddr, coeffs, i + 32);\n-      __ st2(v2, v3, __ T4S, tmpAddr);\n-      __ add(tmpAddr, coeffs, i + 64);\n-      __ st2(v4, v5, __ T4S, tmpAddr);\n-      __ add(tmpAddr, coeffs, i + 96);\n-      __ st2(v6, v7, __ T4S, tmpAddr);\n+      \/\/ reload constants q, qinv each iteration as they get clobbered later\n+      vs_ldpq(vq, dilithiumConsts); \/\/ qInv, q\n+      \/\/ load interleaved 16 (4x4S) coefficients via offsets\n+      vs_ld2_indexed(vs1, __ T4S, coeffs, tmpAddr, i, offsets);\n+      \/\/ load next 16 (4x4S) inputs\n+      vs_ldpq_post(vs_front(vs2), zetas);\n+      \/\/ mont multiply odd elements of vs1 by vs2 and add\/sub into odds\/evens\n+      dilithium_montmul16_sub_add(vs_even(vs1), vs_odd(vs1),\n+                                  vs_front(vs2), vtmp, vq);\n+      \/\/ store interleaved 16 (4x4S) coefficients via offsets\n+      vs_st2_indexed(vs1, __ T4S, coeffs, tmpAddr, i, offsets);\n@@ -5000,46 +5152,0 @@\n-\n-  }\n-\n-  \/\/ Do the computations that can be found in the body of the loop in\n-  \/\/ sun.security.provider.ML_DSA.implDilithiumAlmostInverseNttJava()\n-  \/\/ for 16 coefficients in parallel:\n-  \/\/ tmp = coeffs[j];\n-  \/\/ coeffs[j] = (tmp + coeffs[j + l]);\n-  \/\/ coeffs[j + l] = montMul(tmp - coeffs[j + l], -MONT_ZETAS_FOR_NTT[m]);\n-  \/\/ coefss[j]s are loaded in v0, v2, v4 and v6,\n-  \/\/ coeffs[j + l]s in v1, v3, v5 and v7,\n-  \/\/ the corresponding zetas in v16, v17, v18 and v19.\n-  void dilithium_sub_add_montmul16() {\n-    __ subv(v20, __ T4S, v0, v1);\n-    __ subv(v21, __ T4S, v2, v3);\n-    __ subv(v22, __ T4S, v4, v5);\n-    __ subv(v23, __ T4S, v6, v7);\n-\n-    __ addv(v0, __ T4S, v0, v1);\n-    __ addv(v2, __ T4S, v2, v3);\n-    __ addv(v4, __ T4S, v4, v5);\n-    __ addv(v6, __ T4S, v6, v7);\n-\n-    __ sqdmulh(v24, __ T4S, v20, v16); \/\/ aHigh = hi32(2 * b * c)\n-    __ mulv(v1, __ T4S, v20, v16);     \/\/ aLow = lo32(b * c)\n-    __ sqdmulh(v25, __ T4S, v21, v17);\n-    __ mulv(v3, __ T4S, v21, v17);\n-    __ sqdmulh(v26, __ T4S, v22, v18);\n-    __ mulv(v5, __ T4S, v22, v18);\n-    __ sqdmulh(v27, __ T4S, v23, v19);\n-    __ mulv(v7, __ T4S, v23, v19);\n-\n-    __ mulv(v1, __ T4S, v1, v30);      \/\/ m = (aLow * q)\n-    __ mulv(v3, __ T4S, v3, v30);\n-    __ mulv(v5, __ T4S, v5, v30);\n-    __ mulv(v7, __ T4S, v7, v30);\n-\n-    __ sqdmulh(v1, __ T4S, v1, v31);  \/\/ n = hi32(2 * m * q)\n-    __ sqdmulh(v3, __ T4S, v3, v31);\n-    __ sqdmulh(v5, __ T4S, v5, v31);\n-    __ sqdmulh(v7, __ T4S, v7, v31);\n-\n-    __ shsubv(v1, __ T4S, v24, v1);  \/\/ a = (aHigh  - n) \/ 2\n-    __ shsubv(v3, __ T4S, v25, v3);\n-    __ shsubv(v5, __ T4S, v26, v5);\n-    __ shsubv(v7, __ T4S, v27, v7);\n@@ -5052,5 +5158,5 @@\n-  \/\/ We collect the coefficients that correspond to the 'j's into v0-v7\n-  \/\/ the coefficiets that correspond to the 'j+l's into v16-v23 then\n-  \/\/ do the additions into v24-v31 and the subtractions into v0-v7 then\n-  \/\/ save the result of the additions, load the zetas into v16-v23\n-  \/\/ do the (Montgomery) multiplications by zeta in parallel into v16-v23\n+  \/\/ We collect the coefficients that correspond to the 'j's into vs1\n+  \/\/ the coefficiets that correspond to the 'j+l's into vs2 then\n+  \/\/ do the additions into vs3 and the subtractions into vs1 then\n+  \/\/ save the result of the additions, load the zetas into vs2\n+  \/\/ do the (Montgomery) multiplications by zeta in parallel into vs2\n@@ -5063,3 +5169,6 @@\n-    int incr1;\n-    int incr2;\n-    int incr3;\n+    int offsets[4];\n+    VSeq<8> vs1(0), vs2(16), vs3(24);  \/\/ 3 sets of 8x4s inputs\/outputs\n+    VSeq<4> vtmp = vs_front(vs3);      \/\/ n.b. tmp registers overlap vs3\n+    VSeq<2> vq(30);                    \/\/ n.b. constants overlap vs3\n+\n+    offsets[0] = 0;\n@@ -5071,3 +5180,3 @@\n-        incr1 = 64;\n-        incr2 = 128;\n-        incr3 = 192;\n+        offsets[1] = 64;\n+        offsets[2] = 128;\n+        offsets[3] = 192;\n@@ -5075,3 +5184,3 @@\n-        incr1 = 32;\n-        incr2 = 128;\n-        incr3 = 160;\n+        offsets[1] = 32;\n+        offsets[2] = 128;\n+        offsets[3] = 160;\n@@ -5079,3 +5188,3 @@\n-        incr1 = 32;\n-        incr2 = 64;\n-        incr3 = 96;\n+        offsets[1] = 32;\n+        offsets[2] = 64;\n+        offsets[3] = 96;\n@@ -5084,0 +5193,4 @@\n+      \/\/ for levels 3 - 7 we simply load 2 x 4 adjacent values at a\n+      \/\/ time at 4 different offsets and multiply them in order by the\n+      \/\/ next set of input values. So we employ indexed load and store\n+      \/\/ pair instructions with arrangement 4S\n@@ -5085,20 +5198,18 @@\n-        __ ldpq(v0, v1, Address(coeffs, c1Start));\n-        __ ldpq(v2, v3, Address(coeffs, c1Start + incr1));\n-        __ ldpq(v4, v5, Address(coeffs, c1Start + incr2));\n-        __ ldpq(v6, v7, Address(coeffs, c1Start + incr3));\n-        __ ldpq(v16, v17, Address(coeffs, c2Start));\n-        __ ldpq(v18, v19, Address(coeffs, c2Start + incr1));\n-        __ ldpq(v20, v21, Address(coeffs, c2Start + incr2));\n-        __ ldpq(v22, v23, Address(coeffs, c2Start + incr3));\n-        dilithium_add_sub32();\n-        __ stpq(v24, v25, Address(coeffs, c1Start));\n-        __ stpq(v26, v27, Address(coeffs, c1Start + incr1));\n-        __ stpq(v28, v29, Address(coeffs, c1Start + incr2));\n-        __ stpq(v30, v31, Address(coeffs, c1Start + incr3));\n-        __ ldpq(v30, v31, Address(dilithiumConsts, 0));   \/\/ qInv, q\n-        dilithium_load32zetas(zetas);\n-        dilithium_montmul32(false);\n-        __ stpq(v16, v17, Address(coeffs, c2Start));\n-        __ stpq(v18, v19, Address(coeffs, c2Start + incr1));\n-        __ stpq(v20, v21, Address(coeffs, c2Start + incr2));\n-        __ stpq(v22, v23, Address(coeffs, c2Start + incr3));\n+        \/\/ load v1 32 (8x4S) coefficients relative to first start index\n+        vs_ldpq_indexed(vs1, coeffs, c1Start, offsets);\n+        \/\/ load v2 32 (8x4S) coefficients relative to second start index\n+        vs_ldpq_indexed(vs2, coeffs, c2Start, offsets);\n+        \/\/ a0 = v1 + v2 -- n.b. clobbers vqs\n+        vs_addv(vs3, __ T4S, vs1, vs2);\n+        \/\/ a1 = v1 - v2\n+        vs_subv(vs1, __ T4S, vs1, vs2);\n+        \/\/ save a1 relative to first start index\n+        vs_stpq_indexed(vs3, coeffs, c1Start, offsets);\n+        \/\/ load constants q, qinv each iteration as they get clobbered above\n+        vs_ldpq(vq, dilithiumConsts); \/\/ qInv, q\n+        \/\/ load b next 32 (8x4S) inputs\n+        vs_ldpq_post(vs2, zetas);\n+        \/\/ a = a1 montmul b\n+        vs_montmul32(vs2, vs1, vs2, vtmp, vq);\n+        \/\/ save a relative to second start index\n+        vs_stpq_indexed(vs2, coeffs, c2Start, offsets);\n@@ -5145,0 +5256,6 @@\n+    VSeq<8> vs1(0), vs2(16), vs3(24);  \/\/ 3 sets of 8x4s inputs\/outputs\n+    VSeq<4> vtmp = vs_front(vs3);     \/\/ n.b. tmp registers overlap vs3\n+    VSeq<2> vq(30);                    \/\/ n.b. constants overlap vs3\n+    int offsets[4] = { 0, 32, 64, 96 };\n+    int offsets1[8] = { 0, 32, 64, 96, 128, 160, 192, 224 };\n+    int offsets2[8] = { 16, 48, 80, 112, 144, 176, 208, 240 };\n@@ -5151,0 +5268,6 @@\n+\n+    \/\/ level 0\n+    \/\/ At level 0 we need to interleave adjacent quartets of\n+    \/\/ coefficients before we multiply and add\/sub by the next 16\n+    \/\/ zetas just as we did for level 7 in the multiply code. So we\n+    \/\/ load and store the values using an ld2\/st2 with arrangement 4S\n@@ -5152,19 +5275,14 @@\n-      __ ldpq(v30, v31, Address(dilithiumConsts, 0));  \/\/ qInv, q\n-      __ add(tmpAddr, coeffs, i);\n-      __ ld2(v0, v1, __ T4S, tmpAddr);\n-      __ add(tmpAddr, coeffs, i + 32);\n-      __ ld2(v2, v3, __ T4S, tmpAddr);\n-      __ add(tmpAddr, coeffs, i + 64);\n-      __ ld2(v4, v5, __ T4S, tmpAddr);\n-      __ add(tmpAddr, coeffs, i + 96);\n-      __ ld2(v6, v7, __ T4S, tmpAddr);\n-      dilithium_load16zetas(16, zetas);\n-      dilithium_sub_add_montmul16();\n-      __ add(tmpAddr, coeffs, i);\n-      __ st2(v0, v1, __ T4S, tmpAddr);\n-      __ add(tmpAddr, coeffs, i + 32);\n-      __ st2(v2, v3, __ T4S, tmpAddr);\n-      __ add(tmpAddr, coeffs, i + 64);\n-      __ st2(v4, v5, __ T4S, tmpAddr);\n-      __ add(tmpAddr, coeffs, i + 96);\n-      __ st2(v6, v7, __ T4S, tmpAddr);\n+      \/\/ load constants q, qinv\n+      \/\/ n.b. this can be moved out of the loop as they do not get\n+      \/\/ clobbered by first two loops\n+      vs_ldpq(vq, dilithiumConsts); \/\/ qInv, q\n+      \/\/ a0\/a1 load interleaved 32 (8x4S) coefficients\n+      vs_ld2_indexed(vs1, __ T4S, coeffs, tmpAddr, i, offsets);\n+      \/\/ b load next 32 (8x4S) inputs\n+      vs_ldpq_post(vs_front(vs2), zetas);\n+      \/\/ compute in parallel (a0, a1) = (a0 + a1, (a0 - a1) montmul b)\n+      \/\/ n.b. second half of vs2 provides temporary register storage\n+      dilithium_sub_add_montmul16(vs_even(vs1), vs_odd(vs1),\n+                                  vs_front(vs2), vs_back(vs2), vtmp, vq);\n+      \/\/ a0\/a1 store interleaved 32 (8x4S) coefficients\n+      vs_st2_indexed(vs1, __ T4S, coeffs, tmpAddr, i, offsets);\n@@ -5174,0 +5292,4 @@\n+    \/\/ At level 1 we need to interleave pairs of adjacent pairs of\n+    \/\/ coefficients before we multiply by the next 16 zetas just as we\n+    \/\/ did for level 6 in the multiply code. So we load and store the\n+    \/\/ values an ld2\/st2 with arrangement 2D\n@@ -5175,18 +5297,10 @@\n-      __ add(tmpAddr, coeffs, i);\n-      __ ld2(v0, v1, __ T2D, tmpAddr);\n-      __ add(tmpAddr, coeffs, i + 32);\n-      __ ld2(v2, v3, __ T2D, tmpAddr);\n-      __ add(tmpAddr, coeffs, i + 64);\n-      __ ld2(v4, v5, __ T2D, tmpAddr);\n-      __ add(tmpAddr, coeffs, i + 96);\n-      __ ld2(v6, v7, __ T2D, tmpAddr);\n-      dilithium_load16zetas(16, zetas);\n-      dilithium_sub_add_montmul16();\n-      __ add(tmpAddr, coeffs, i);\n-      __ st2(v0, v1, __ T2D, tmpAddr);\n-      __ add(tmpAddr, coeffs, i + 32);\n-      __ st2(v2, v3, __ T2D, tmpAddr);\n-      __ add(tmpAddr, coeffs, i + 64);\n-      __ st2(v4, v5, __ T2D, tmpAddr);\n-      __ add(tmpAddr, coeffs, i + 96);\n-      __ st2(v6, v7, __ T2D, tmpAddr);\n+      \/\/ a0\/a1 load interleaved 32 (8x2D) coefficients\n+      vs_ld2_indexed(vs1, __ T2D, coeffs, tmpAddr, i, offsets);\n+      \/\/ b load next 16 (4x4S) inputs\n+      vs_ldpq_post(vs_front(vs2), zetas);\n+      \/\/ compute in parallel (a0, a1) = (a0 + a1, (a0 - a1) montmul b)\n+      \/\/ n.b. second half of vs2 provides temporary register storage\n+      dilithium_sub_add_montmul16(vs_even(vs1), vs_odd(vs1),\n+                                  vs_front(vs2), vs_back(vs2), vtmp, vq);\n+      \/\/ a0\/a1 store interleaved 32 (8x2D) coefficients\n+      vs_st2_indexed(vs1, __ T2D, coeffs, tmpAddr, i, offsets);\n@@ -5195,1 +5309,6 @@\n-    \/\/level 2\n+    \/\/ level 2\n+    \/\/ At level 2 coefficients come in blocks of 4. So, we load 4\n+    \/\/ adjacent coefficients at 8 distinct offsets for both the first\n+    \/\/ and second coefficient sequences, using an ldr with register\n+    \/\/ variant Q then combine them with next set of 32 zetas. Likewise\n+    \/\/ we store the results using an str with register variant Q.\n@@ -5197,36 +5316,18 @@\n-      __ ldr(v0, __ Q, Address(coeffs, i));\n-      __ ldr(v1, __ Q, Address(coeffs, i + 32));\n-      __ ldr(v2, __ Q, Address(coeffs, i + 64));\n-      __ ldr(v3, __ Q, Address(coeffs, i + 96));\n-      __ ldr(v4, __ Q, Address(coeffs, i + 128));\n-      __ ldr(v5, __ Q, Address(coeffs, i + 160));\n-      __ ldr(v6, __ Q, Address(coeffs, i + 192));\n-      __ ldr(v7, __ Q, Address(coeffs, i + 224));\n-      __ ldr(v16, __ Q, Address(coeffs, i + 16));\n-      __ ldr(v17, __ Q, Address(coeffs, i + 48));\n-      __ ldr(v18, __ Q, Address(coeffs, i + 80));\n-      __ ldr(v19, __ Q, Address(coeffs, i + 112));\n-      __ ldr(v20, __ Q, Address(coeffs, i + 144));\n-      __ ldr(v21, __ Q, Address(coeffs, i + 176));\n-      __ ldr(v22, __ Q, Address(coeffs, i + 208));\n-      __ ldr(v23, __ Q, Address(coeffs, i + 240));\n-      dilithium_add_sub32();\n-      __ str(v24, __ Q, Address(coeffs, i));\n-      __ str(v25, __ Q, Address(coeffs, i + 32));\n-      __ str(v26, __ Q, Address(coeffs, i + 64));\n-      __ str(v27, __ Q, Address(coeffs, i + 96));\n-      __ str(v28, __ Q, Address(coeffs, i + 128));\n-      __ str(v29, __ Q, Address(coeffs, i + 160));\n-      __ str(v30, __ Q, Address(coeffs, i + 192));\n-      __ str(v31, __ Q, Address(coeffs, i + 224));\n-      dilithium_load32zetas(zetas);\n-      __ ldpq(v30, v31, Address(dilithiumConsts, 0));  \/\/ qInv, q\n-      dilithium_montmul32(false);\n-      __ str(v16, __ Q, Address(coeffs, i + 16));\n-      __ str(v17, __ Q, Address(coeffs, i + 48));\n-      __ str(v18, __ Q, Address(coeffs, i + 80));\n-      __ str(v19, __ Q, Address(coeffs, i + 112));\n-      __ str(v20, __ Q, Address(coeffs, i + 144));\n-      __ str(v21, __ Q, Address(coeffs, i + 176));\n-      __ str(v22, __ Q, Address(coeffs, i + 208));\n-      __ str(v23, __ Q, Address(coeffs, i + 240));\n+      \/\/ c0 load 32 (8x4S) coefficients via first offsets\n+      vs_ldr_indexed(vs1, __ Q, coeffs, i, offsets1);\n+      \/\/ c1 load 32 (8x4S) coefficients via second offsets\n+      vs_ldr_indexed(vs2, __ Q,coeffs, i, offsets2);\n+      \/\/ a0 = c0 + c1  n.b. clobbers vq which overlaps vs3\n+      vs_addv(vs3, __ T4S, vs1, vs2);\n+      \/\/ c = c0 - c1\n+      vs_subv(vs1, __ T4S, vs1, vs2);\n+      \/\/ store a0 32 (8x4S) coefficients via first offsets\n+      vs_str_indexed(vs3, __ Q, coeffs, i, offsets1);\n+      \/\/ b load 32 (8x4S) next inputs\n+      vs_ldpq_post(vs2, zetas);\n+      \/\/ reload constants q, qinv -- they were clobbered earlier\n+      vs_ldpq(vq, dilithiumConsts); \/\/ qInv, q\n+      \/\/ compute a1 = b montmul c\n+      vs_montmul32(vs2, vs1, vs2, vtmp, vq);\n+      \/\/ store a1 32 (8x4S) coefficients via second offsets\n+      vs_str_indexed(vs2, __ Q, coeffs, i, offsets2);\n@@ -5257,1 +5358,1 @@\n-    __ align(CodeEntryAlignment);\n+        __ align(CodeEntryAlignment);\n@@ -5272,0 +5373,5 @@\n+    VSeq<8> vs1(0), vs2(16), vs3(24);  \/\/ 3 sets of 8x4s inputs\/outputs\n+    VSeq<4> vtmp = vs_front(vs3);         \/\/ n.b. tmp registers overlap vs3\n+    VSeq<2> vq(30);                    \/\/ n.b. constants overlap vs3\n+    VSeq<8> vrsquare(29, 0);           \/\/ for montmul by constant RSQUARE\n+\n@@ -5274,1 +5380,3 @@\n-    __ ldpq(v30, v31, Address(dilithiumConsts, 0));   \/\/ qInv, q\n+    \/\/ load constants q, qinv\n+    vs_ldpq(vq, dilithiumConsts); \/\/ qInv, q\n+    \/\/ load constant rSquare into v29\n@@ -5282,14 +5390,10 @@\n-    __ ldpq(v0, v1, __ post(poly1, 32));\n-    __ ldpq(v2, v3, __ post(poly1, 32));\n-    __ ldpq(v4, v5, __ post(poly1, 32));\n-    __ ldpq(v6, v7, __ post(poly1, 32));\n-    __ ldpq(v16, v17, __ post(poly2, 32));\n-    __ ldpq(v18, v19, __ post(poly2, 32));\n-    __ ldpq(v20, v21, __ post(poly2, 32));\n-    __ ldpq(v22, v23, __ post(poly2, 32));\n-    dilithium_montmul32(false);\n-    dilithium_montmul32(true);\n-    __ stpq(v16, v17, __ post(result, 32));\n-    __ stpq(v18, v19, __ post(result, 32));\n-    __ stpq(v20, v21, __ post(result, 32));\n-    __ stpq(v22, v23, __ post(result, 32));\n+    \/\/ b load 32 (8x4S) next inputs from poly1\n+    vs_ldpq_post(vs1, poly1);\n+    \/\/ c load 32 (8x4S) next inputs from poly2\n+    vs_ldpq_post(vs2, poly2);\n+    \/\/ compute a = b montmul c\n+    vs_montmul32(vs2, vs1, vs2, vtmp, vq);\n+    \/\/ compute a = rsquare montmul a\n+    vs_montmul32(vs2, vrsquare, vs2, vtmp, vq);\n+    \/\/ save a 32 (8x4S) results\n+    vs_stpq_post(vs2, result);\n@@ -5333,0 +5437,6 @@\n+    VSeq<8> vs1(0), vs2(16), vs3(24);  \/\/ 3 sets of 8x4s inputs\/outputs\n+    VSeq<4> vtmp = vs_front(vs3);         \/\/ n.b. tmp registers overlap vs3\n+    VSeq<2> vq(30);                    \/\/ n.b. constants overlap vs3\n+    VSeq<8> vconst(29, 0);             \/\/ for montmul by constant\n+\n+    \/\/ results track inputs\n@@ -5336,2 +5446,4 @@\n-    __ ldpq(v30, v31, Address(dilithiumConsts, 0));   \/\/ qInv, q\n-    __ dup(v29, __ T4S, constant);\n+    \/\/ load constants q, qinv -- they do not get clobbered by first two loops\n+    vs_ldpq(vq, dilithiumConsts); \/\/ qInv, q\n+    \/\/ copy caller supplied constant across vconst\n+    __ dup(vconst[0], __ T4S, constant);\n@@ -5343,9 +5455,6 @@\n-    __ ldpq(v16, v17, __ post(coeffs, 32));\n-    __ ldpq(v18, v19, __ post(coeffs, 32));\n-    __ ldpq(v20, v21, __ post(coeffs, 32));\n-    __ ldpq(v22, v23, __ post(coeffs, 32));\n-    dilithium_montmul32(true);\n-    __ stpq(v16, v17, __ post(result, 32));\n-    __ stpq(v18, v19, __ post(result, 32));\n-    __ stpq(v20, v21, __ post(result, 32));\n-    __ stpq(v22, v23, __ post(result, 32));\n+    \/\/ load next 32 inputs\n+    vs_ldpq_post(vs2, coeffs);\n+    \/\/ mont mul by constant\n+    vs_montmul32(vs2, vconst, vs2, vtmp, vq);\n+    \/\/ write next 32 results\n+    vs_stpq_post(vs2, result);\n@@ -5362,0 +5471,1 @@\n+\n@@ -5380,2 +5490,0 @@\n-    __ enter();\n-\n@@ -5394,0 +5502,12 @@\n+    VSeq<4> vs1(0), vs2(4), vs3(8); \/\/ 6 independent sets of 4x4s values\n+    VSeq<4> vs4(12), vs5(16), vtmp(20);\n+    VSeq<4> one(25, 0);            \/\/ 7 constants for cross-multiplying\n+    VSeq<4> qminus1(26, 0);\n+    VSeq<4> g2(27, 0);\n+    VSeq<4> twog2(28, 0);\n+    VSeq<4> mult(29, 0);\n+    VSeq<4> q(30, 0);\n+    VSeq<4> qadd(31, 0);\n+\n+    __ enter();\n+\n@@ -5402,1 +5522,1 @@\n-\n+    \/\/ populate constant registers\n@@ -5405,7 +5525,7 @@\n-    __ dup(v25, __ T4S, tmp); \/\/ 1\n-    __ ldr(v30, __ Q, Address(dilithiumConsts, 16)); \/\/ q\n-    __ ldr(v31, __ Q, Address(dilithiumConsts, 64)); \/\/ addend for mod q reduce\n-    __ dup(v28, __ T4S, twoGamma2); \/\/ 2 * gamma2\n-    __ dup(v29, __ T4S, multiplier); \/\/ multiplier for mod 2 * gamma reduce\n-    __ subv(v26, __ T4S, v30, v25); \/\/ q - 1\n-    __ sshr(v27, __ T4S, v28, 1); \/\/ gamma2\n+    __ dup(one[0], __ T4S, tmp); \/\/ 1\n+    __ ldr(q[0], __ Q, Address(dilithiumConsts, 16)); \/\/ q\n+    __ ldr(qadd[0], __ Q, Address(dilithiumConsts, 64)); \/\/ addend for mod q reduce\n+    __ dup(twog2[0], __ T4S, twoGamma2); \/\/ 2 * gamma2\n+    __ dup(mult[0], __ T4S, multiplier); \/\/ multiplier for mod 2 * gamma reduce\n+    __ subv(qminus1[0], __ T4S, v30, v25); \/\/ q - 1\n+    __ sshr(g2[0], __ T4S, v28, 1); \/\/ gamma2\n@@ -5418,8 +5538,2 @@\n-    __ ld4(v0, v1, v2, v3, __ T4S, __ post(input, 64));\n-\n-    \/\/ rplus in v0\n-    \/\/  rplus = rplus - ((rplus + 5373807) >> 23) * dilithium_q;\n-    __ addv(v4, __ T4S, v0, v31);\n-    __ addv(v5, __ T4S, v1, v31);\n-    __ addv(v6, __ T4S, v2, v31);\n-    __ addv(v7, __ T4S, v3, v31);\n+    \/\/ load next 4x4S inputs interleaved: rplus --> vs1\n+    __ ld4(vs1[0], vs1[1], vs1[2], vs1[3], __ T4S, __ post(input, 64));\n@@ -5427,4 +5541,5 @@\n-    __ sshr(v4, __ T4S, v4, 23);\n-    __ sshr(v5, __ T4S, v5, 23);\n-    __ sshr(v6, __ T4S, v6, 23);\n-    __ sshr(v7, __ T4S, v7, 23);\n+    \/\/  rplus = rplus - ((rplus + qadd) >> 23) * q\n+    vs_addv(vtmp, __ T4S, vs1, qadd);\n+    vs_sshr(vtmp, __ T4S, vtmp, 23);\n+    vs_mulv(vtmp, __ T4S, vtmp, q);\n+    vs_subv(vs1, __ T4S, vs1, vtmp);\n@@ -5432,32 +5547,3 @@\n-    __ mulv(v4, __ T4S, v4, v30);\n-    __ mulv(v5, __ T4S, v5, v30);\n-    __ mulv(v6, __ T4S, v6, v30);\n-    __ mulv(v7, __ T4S, v7, v30);\n-\n-    __ subv(v0, __ T4S, v0, v4);\n-    __ subv(v1, __ T4S, v1, v5);\n-    __ subv(v2, __ T4S, v2, v6);\n-    __ subv(v3, __ T4S, v3, v7);\n-\n-    \/\/ rplus in v0\n-    __ sshr(v4, __ T4S, v0, 31);\n-    __ sshr(v5, __ T4S, v1, 31);\n-    __ sshr(v6, __ T4S, v2, 31);\n-    __ sshr(v7, __ T4S, v3, 31);\n-\n-    __ andr(v4, __ T16B, v4, v30);\n-    __ andr(v5, __ T16B, v5, v30);\n-    __ andr(v6, __ T16B, v6, v30);\n-    __ andr(v7, __ T16B, v7, v30);\n-\n-    __ addv(v0, __ T4S, v0, v4);\n-    __ addv(v1, __ T4S, v1, v5);\n-    __ addv(v2, __ T4S, v2, v6);\n-    __ addv(v3, __ T4S, v3, v7);\n-\n-    \/\/ rplus in v0\n-    \/\/ int quotient = (rplus * multiplier) >> 22;\n-    __ mulv(v4, __ T4S, v0, v29);\n-    __ mulv(v5, __ T4S, v1, v29);\n-    __ mulv(v6, __ T4S, v2, v29);\n-    __ mulv(v7, __ T4S, v3, v29);\n+    vs_sshr(vtmp, __ T4S, vs1, 31);\n+    vs_andr(vtmp, vtmp, q);\n+    vs_addv(vs1, __ T4S, vs1, vtmp);\n@@ -5466,4 +5552,4 @@\n-    __ sshr(v4, __ T4S, v4, 22);\n-    __ sshr(v5, __ T4S, v5, 22);\n-    __ sshr(v6, __ T4S, v6, 22);\n-    __ sshr(v7, __ T4S, v7, 22);\n+    \/\/ quotient --> vs2\n+    \/\/ int quotient = (rplus * multiplier) >> 22;\n+    vs_mulv(vtmp, __ T4S, vs1, mult);\n+    vs_sshr(vs2, __ T4S, vtmp, 22);\n@@ -5471,1 +5557,1 @@\n-    \/\/ quotient in v4\n+    \/\/ r0 --> vs3\n@@ -5473,9 +5559,2 @@\n-    __ mulv(v8, __ T4S, v4, v28);\n-    __ mulv(v9, __ T4S, v5, v28);\n-    __ mulv(v10, __ T4S, v6, v28);\n-    __ mulv(v11, __ T4S, v7, v28);\n-\n-    __ subv(v8, __ T4S, v0, v8);\n-    __ subv(v9, __ T4S, v1, v9);\n-    __ subv(v10, __ T4S, v2, v10);\n-    __ subv(v11, __ T4S, v3, v11);\n+    vs_mulv(vtmp, __ T4S, vs2, twog2);\n+    vs_subv(vs3, __ T4S, vs1, vtmp);\n@@ -5483,1 +5562,1 @@\n-    \/\/ r0 in v8\n+    \/\/ mask --> vs4\n@@ -5485,9 +5564,2 @@\n-    __ subv(v12, __ T4S, v28, v8);\n-    __ subv(v13, __ T4S, v28, v9);\n-    __ subv(v14, __ T4S, v28, v10);\n-    __ subv(v15, __ T4S, v28, v11);\n-\n-    __ sshr(v12, __ T4S, v12, 22);\n-    __ sshr(v13, __ T4S, v13, 22);\n-    __ sshr(v14, __ T4S, v14, 22);\n-    __ sshr(v15, __ T4S, v15, 22);\n+    vs_subv(vtmp, __ T4S, twog2, vs3);\n+    vs_sshr(vs4, __ T4S, vtmp, 22);\n@@ -5495,5 +5567,2 @@\n-    \/\/ mask in v12\n-    __ andr(v16, __ T16B, v12, v28);\n-    __ andr(v17, __ T16B, v13, v28);\n-    __ andr(v18, __ T16B, v14, v28);\n-    __ andr(v19, __ T16B, v15, v28);\n+    vs_andr(vtmp, vs4, twog2);\n+    vs_subv(vs3, __ T4S, vs3, vtmp);\n@@ -5502,15 +5571,2 @@\n-    __ subv(v8, __ T4S, v8, v16);\n-    __ subv(v9, __ T4S, v9, v17);\n-    __ subv(v10, __ T4S, v10, v18);\n-    __ subv(v11, __ T4S, v11, v19);\n-\n-    \/\/ r0 in v8\n-    __ andr(v16, __ T16B, v12, v25);\n-    __ andr(v17, __ T16B, v13, v25);\n-    __ andr(v18, __ T16B, v14, v25);\n-    __ andr(v19, __ T16B, v15, v25);\n-\n-    __ addv(v4, __ T4S, v4, v16);\n-    __ addv(v5, __ T4S, v5, v17);\n-    __ addv(v6, __ T4S, v6, v18);\n-    __ addv(v7, __ T4S, v7, v19);\n+    vs_andr(vtmp, vs4, one);\n+    vs_addv(vs2, __ T4S, vs2, vtmp);\n@@ -5520,9 +5576,2 @@\n-    __ subv(v12, __ T4S, v27, v8);\n-    __ subv(v13, __ T4S, v27, v9);\n-    __ subv(v14, __ T4S, v27, v10);\n-    __ subv(v15, __ T4S, v27, v11);\n-\n-    __ sshr(v12, __ T4S, v12, 31);\n-    __ sshr(v13, __ T4S, v13, 31);\n-    __ sshr(v14, __ T4S, v14, 31);\n-    __ sshr(v15, __ T4S, v15, 31);\n+    vs_subv(vtmp, __ T4S, g2, vs3);\n+    vs_sshr(vs4, __ T4S, vtmp, 31);\n@@ -5531,9 +5580,2 @@\n-    __ andr(v16, __ T16B, v12, v28);\n-    __ andr(v17, __ T16B, v13, v28);\n-    __ andr(v18, __ T16B, v14, v28);\n-    __ andr(v19, __ T16B, v15, v28);\n-\n-    __ subv(v8, __ T4S, v8, v16);\n-    __ subv(v9, __ T4S, v9, v17);\n-    __ subv(v10, __ T4S, v10, v18);\n-    __ subv(v11, __ T4S, v11, v19);\n+    vs_andr(vtmp, vs4, twog2);\n+    vs_subv(vs3, __ T4S, vs3, vtmp);\n@@ -5542,9 +5584,2 @@\n-    __ andr(v16, __ T16B, v12, v25);\n-    __ andr(v17, __ T16B, v13, v25);\n-    __ andr(v18, __ T16B, v14, v25);\n-    __ andr(v19, __ T16B, v15, v25);\n-\n-    __ addv(v4, __ T4S, v4, v16);\n-    __ addv(v5, __ T4S, v5, v17);\n-    __ addv(v6, __ T4S, v6, v18);\n-    __ addv(v7, __ T4S, v7, v19);\n+    vs_andr(vtmp, vs4, one);\n+    vs_addv(vs2, __ T4S, vs2, vtmp);\n@@ -5552,0 +5587,1 @@\n+    \/\/ r1 --> vs5\n@@ -5553,9 +5589,2 @@\n-    __ subv(v16, __ T4S, v0, v8);\n-    __ subv(v17, __ T4S, v1, v9);\n-    __ subv(v18, __ T4S, v2, v10);\n-    __ subv(v19, __ T4S, v3, v11);\n-\n-    __ subv(v16, __ T4S, v16, v26);\n-    __ subv(v17, __ T4S, v17, v26);\n-    __ subv(v18, __ T4S, v18, v26);\n-    __ subv(v19, __ T4S, v19, v26);\n+    vs_subv(vtmp, __ T4S, vs1, vs3);\n+    vs_subv(vs5, __ T4S, vtmp, qminus1);\n@@ -5563,1 +5592,1 @@\n-    \/\/ r1 in v16\n+    \/\/ r1 --> vs1 (overwriting rplus)\n@@ -5565,21 +5594,3 @@\n-    __ negr(v20, __ T4S, v16);\n-    __ negr(v21, __ T4S, v17);\n-    __ negr(v22, __ T4S, v18);\n-    __ negr(v23, __ T4S, v19);\n-\n-    __ orr(v16, __ T16B, v16, v20);\n-    __ orr(v17, __ T16B, v17, v21);\n-    __ orr(v18, __ T16B, v18, v22);\n-    __ orr(v19, __ T16B, v19, v23);\n-\n-    __ sshr(v0, __ T4S, v16, 31);\n-    __ sshr(v1, __ T4S, v17, 31);\n-    __ sshr(v2, __ T4S, v18, 31);\n-    __ sshr(v3, __ T4S, v19, 31);\n-\n-    \/\/ r1 in v0\n-    \/\/ r0 += ~r1;\n-    __ notr(v20, __ T16B, v0);\n-    __ notr(v21, __ T16B, v1);\n-    __ notr(v22, __ T16B, v2);\n-    __ notr(v23, __ T16B, v3);\n+    vs_negr(vtmp, __ T4S, vs5);\n+    vs_orr(vtmp, vs5, vtmp);\n+    vs_sshr(vs1, __ T4S, vtmp, 31);\n@@ -5587,4 +5598,3 @@\n-    __ addv(v8, __ T4S, v8, v20);\n-    __ addv(v9, __ T4S, v9, v21);\n-    __ addv(v10, __ T4S, v10, v22);\n-    __ addv(v11, __ T4S, v11, v23);\n+    \/\/ r0 += ~r1;\n+    vs_notr(vtmp, vs1);\n+    vs_addv(vs3, __ T4S, vs3, vtmp);\n@@ -5592,5 +5602,1 @@\n-    \/\/ r0 in v8\n-    __ andr(v0, __ T16B, v4, v0);\n-    __ andr(v1, __ T16B, v5, v1);\n-    __ andr(v2, __ T16B, v6, v2);\n-    __ andr(v3, __ T16B, v7, v3);\n+    vs_andr(vs1, vs2, vs1);\n@@ -5599,1 +5605,1 @@\n-    \/\/ r1 in v0\n+    \/\/ store results inteleaved\n@@ -5602,2 +5608,2 @@\n-    __ st4(v8, v9, v10, v11, __ T4S, __ post(lowPart, 64));\n-    __ st4(v0, v1, v2, v3, __ T4S, __ post(highPart, 64));\n+    __ st4(vs3[0], vs3[1], vs3[2], vs3[3], __ T4S, __ post(lowPart, 64));\n+    __ st4(vs1[0], vs1[1], vs1[2], vs1[3], __ T4S, __ post(highPart, 64));\n@@ -5621,0 +5627,1 @@\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/stubGenerator_aarch64.cpp","additions":591,"deletions":584,"binary":false,"changes":1175,"status":"modified"},{"patch":"@@ -1082,1 +1082,1 @@\n-  assert(UseCRC32Intrinsics, \"need AVX and LCMUL instructions support\");\n+  assert(UseCRC32Intrinsics, \"need AVX and CLMUL instructions support\");\n@@ -1159,1 +1159,66 @@\n-  Unimplemented();\n+  assert(UseCRC32CIntrinsics, \"need AVX and CLMUL instructions support\");\n+  LIR_Opr result = rlock_result(x);\n+\n+  switch (x->id()) {\n+    case vmIntrinsics::_updateBytesCRC32C:\n+    case vmIntrinsics::_updateDirectByteBufferCRC32C: {\n+      bool is_updateBytes = (x->id() == vmIntrinsics::_updateBytesCRC32C);\n+\n+      LIRItem crc(x->argument_at(0), this);\n+      LIRItem buf(x->argument_at(1), this);\n+      LIRItem off(x->argument_at(2), this);\n+      LIRItem end(x->argument_at(3), this);\n+      buf.load_item();\n+      off.load_nonconstant();\n+      end.load_nonconstant();\n+\n+      \/\/ len = end - off\n+      LIR_Opr len  = end.result();\n+      LIR_Opr tmpA = new_register(T_INT);\n+      LIR_Opr tmpB = new_register(T_INT);\n+      __ move(end.result(), tmpA);\n+      __ move(off.result(), tmpB);\n+      __ sub(tmpA, tmpB, tmpA);\n+      len = tmpA;\n+\n+      LIR_Opr index = off.result();\n+      int offset = is_updateBytes ? arrayOopDesc::base_offset_in_bytes(T_BYTE) : 0;\n+      if (off.result()->is_constant()) {\n+        index = LIR_OprFact::illegalOpr;\n+        offset += off.result()->as_jint();\n+      }\n+      LIR_Opr base_op = buf.result();\n+      LIR_Address* a = nullptr;\n+\n+      if (index->is_valid()) {\n+        LIR_Opr tmp = new_register(T_LONG);\n+        __ convert(Bytecodes::_i2l, index, tmp);\n+        index = tmp;\n+        a = new LIR_Address(base_op, index, offset, T_BYTE);\n+      } else {\n+        a = new LIR_Address(base_op, offset, T_BYTE);\n+      }\n+\n+      BasicTypeList signature(3);\n+      signature.append(T_INT);\n+      signature.append(T_ADDRESS);\n+      signature.append(T_INT);\n+      CallingConvention* cc = frame_map()->c_calling_convention(&signature);\n+      const LIR_Opr result_reg = result_register_for(x->type());\n+\n+      LIR_Opr arg1 = cc->at(0),\n+              arg2 = cc->at(1),\n+              arg3 = cc->at(2);\n+\n+      crc.load_item_force(arg1);\n+      __ leal(LIR_OprFact::address(a), arg2);\n+      __ move(len, arg3);\n+\n+      __ call_runtime_leaf(StubRoutines::updateBytesCRC32C(), getThreadTemp(), result_reg, cc->args());\n+      __ move(result_reg, result);\n+      break;\n+    }\n+    default: {\n+      ShouldNotReachHere();\n+    }\n+  }\n","filename":"src\/hotspot\/cpu\/x86\/c1_LIRGenerator_x86.cpp","additions":67,"deletions":2,"binary":false,"changes":69,"status":"modified"},{"patch":"@@ -6887,1 +6887,1 @@\n-    if (vlen_enc == Assembler::AVX_512bit) {\n+    if (VM_Version::supports_avx512vl() || vlen_enc == Assembler::AVX_512bit) {\n@@ -6903,1 +6903,1 @@\n-    if (vlen_enc == Assembler::AVX_512bit) {\n+    if (VM_Version::supports_avx512vl() || vlen_enc == Assembler::AVX_512bit) {\n","filename":"src\/hotspot\/cpu\/x86\/c2_MacroAssembler_x86.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -590,1 +590,1 @@\n-  return is_dumping_static_archive();\n+  return is_dumping_classic_static_archive() || is_dumping_final_static_archive();\n","filename":"src\/hotspot\/share\/cds\/cdsConfig.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -221,23 +221,2 @@\n-\/\/ If p is not aligned, move it up to the next address that's aligned with alignment.\n-\/\/ If this is not possible (because p is too high), return nullptr. Example:\n-\/\/     p = 0xffffffffffff0000, alignment= 0x10000    => return nullptr.\n-static char* align_up_or_null(char* p, size_t alignment) {\n-  assert(p != nullptr, \"sanity\");\n-  if (is_aligned(p, alignment)) {\n-    return p;\n-  }\n-\n-  char* down = align_down(p, alignment);\n-  if (max_uintx - uintx(down) < uintx(alignment)) {\n-    \/\/ Run out of address space to align up.\n-    return nullptr;\n-  }\n-\n-  char* aligned = align_up(p, alignment);\n-  assert(aligned >= p, \"sanity\");\n-  assert(aligned != nullptr, \"sanity\");\n-  return aligned;\n-}\n-\n-  \/\/ Caller should have checked if align_up_or_null( returns nullptr (comparing specified_base\n-  \/\/ with nullptr is UB).\n+  \/\/ Caller should have checked that aligned_base was successfully aligned and is not nullptr.\n+  \/\/ Comparing specified_base with nullptr is UB.\n@@ -268,1 +247,3 @@\n-  char* aligned_base = align_up_or_null(specified_base, alignment);\n+  char* aligned_base = can_align_up(specified_base, alignment)\n+                           ? align_up(specified_base, alignment)\n+                           : nullptr;\n@@ -828,0 +809,3 @@\n+      \/\/ We are in the JVM that runs the training run. Continue execution,\n+      \/\/ so that it can finish all clean-up and return the correct exit\n+      \/\/ code to the OS.\n@@ -829,1 +813,0 @@\n-      vm_exit(0);\n@@ -866,24 +849,3 @@\n-  \/\/ Construct the path to the class list (in jre\/lib)\n-  \/\/ Walk up two directories from the location of the VM and\n-  \/\/ optionally tack on \"lib\" (depending on platform)\n-  os::jvm_path(default_classlist, (jint)(buf_size));\n-  for (int i = 0; i < 3; i++) {\n-    char *end = strrchr(default_classlist, *os::file_separator());\n-    if (end != nullptr) *end = '\\0';\n-  }\n-  size_t classlist_path_len = strlen(default_classlist);\n-  if (classlist_path_len >= 3) {\n-    if (strcmp(default_classlist + classlist_path_len - 3, \"lib\") != 0) {\n-      if (classlist_path_len < buf_size - 4) {\n-        jio_snprintf(default_classlist + classlist_path_len,\n-                     buf_size - classlist_path_len,\n-                     \"%slib\", os::file_separator());\n-        classlist_path_len += 4;\n-      }\n-    }\n-  }\n-  if (classlist_path_len < buf_size - 10) {\n-    jio_snprintf(default_classlist + classlist_path_len,\n-                 buf_size - classlist_path_len,\n-                 \"%sclasslist\", os::file_separator());\n-  }\n+  const char* filesep = os::file_separator();\n+  jio_snprintf(default_classlist, buf_size, \"%s%slib%sclasslist\",\n+               Arguments::get_java_home(), filesep, filesep);\n@@ -896,1 +858,1 @@\n-  get_default_classlist(default_classlist, sizeof(default_classlist));\n+  get_default_classlist(default_classlist, JVM_MAXPATHLEN);\n@@ -951,1 +913,1 @@\n-    get_default_classlist(default_classlist, sizeof(default_classlist));\n+    get_default_classlist(default_classlist, JVM_MAXPATHLEN);\n","filename":"src\/hotspot\/share\/cds\/metaspaceShared.cpp","additions":13,"deletions":51,"binary":false,"changes":64,"status":"modified"},{"patch":"@@ -192,0 +192,2 @@\n+  do_intrinsic(_maxL,                     java_lang_Math,         max_name,           long2_long_signature,      F_S)   \\\n+  do_intrinsic(_minL,                     java_lang_Math,         min_name,           long2_long_signature,      F_S)   \\\n","filename":"src\/hotspot\/share\/classfile\/vmIntrinsics.hpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -289,5 +289,0 @@\n-  \/\/ These Ideal node counts are extracted from the pre-matching Ideal graph\n-  \/\/ generated when compiling the following method with early barrier expansion:\n-  \/\/   static void write(MyObject obj1, Object o) {\n-  \/\/     obj1.o1 = o;\n-  \/\/   }\n@@ -297,1 +292,6 @@\n-    nodes += 50;\n+    \/\/ Only consider the fast path for the barrier that is\n+    \/\/ actually inlined into the main code stream.\n+    \/\/ The slow path is laid out separately and does not\n+    \/\/ directly affect performance.\n+    \/\/ It has a cost of 6 (AddP, LoadB, Cmp, Bool, If, IfProj).\n+    nodes += 6;\n","filename":"src\/hotspot\/share\/gc\/g1\/c2\/g1BarrierSetC2.cpp","additions":6,"deletions":6,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -637,0 +637,2 @@\n+  case vmIntrinsics::_maxL:\n+  case vmIntrinsics::_minL:\n","filename":"src\/hotspot\/share\/opto\/c2compiler.cpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -31,0 +31,1 @@\n+#include \"opto\/predicates_enums.hpp\"\n@@ -61,4 +62,1 @@\n-\n-\/\/ The success projection of a Parse Predicate is always an IfTrueNode and the uncommon projection an IfFalseNode\n-typedef IfTrueNode ParsePredicateSuccessProj;\n-typedef IfFalseNode ParsePredicateUncommonProj;\n+enum class PredicateState;\n@@ -503,1 +501,5 @@\n-  bool _useless; \/\/ If the associated loop dies, this parse predicate becomes useless and can be cleaned up by Value().\n+\n+  \/\/ When a Parse Predicate loses its connection to a loop head, it will be marked useless by\n+  \/\/ EliminateUselessPredicates and cleaned up by Value(). It can also become useless when cloning it to both loops\n+  \/\/ during Loop Multiversioning - we no longer use the old version.\n+  PredicateState _predicate_state;\n@@ -514,1 +516,7 @@\n-    return _useless;\n+    return _predicate_state == PredicateState::Useless;\n+  }\n+\n+  void mark_useless(PhaseIterGVN& igvn);\n+\n+  void mark_maybe_useful() {\n+    _predicate_state = PredicateState::MaybeUseful;\n@@ -517,2 +525,2 @@\n-  void mark_useless() {\n-    _useless = true;\n+  bool is_useful() const {\n+    return _predicate_state == PredicateState::Useful;\n@@ -522,1 +530,1 @@\n-    _useless = false;\n+    _predicate_state = PredicateState::Useful;\n","filename":"src\/hotspot\/share\/opto\/cfgnode.hpp","additions":17,"deletions":9,"binary":false,"changes":26,"status":"modified"},{"patch":"@@ -72,0 +72,1 @@\n+#include \"opto\/opaquenode.hpp\"\n@@ -399,1 +400,1 @@\n-    remove_template_assertion_predicate_opaq(dead);\n+    remove_template_assertion_predicate_opaque(dead->as_OpaqueTemplateAssertionPredicate());\n@@ -464,1 +465,2 @@\n-  remove_useless_nodes(_template_assertion_predicate_opaqs, useful); \/\/ remove useless Assertion Predicate opaque nodes\n+  \/\/ Remove useless Template Assertion Predicate opaque nodes\n+  remove_useless_nodes(_template_assertion_predicate_opaques, useful);\n@@ -671,1 +673,1 @@\n-      _template_assertion_predicate_opaqs(comp_arena(), 8, 0, nullptr),\n+      _template_assertion_predicate_opaques(comp_arena(), 8, 0, nullptr),\n@@ -1905,2 +1907,1 @@\n-    parse_predicate->mark_useless();\n-    igvn._worklist.push(parse_predicate);\n+    parse_predicate->mark_useless(igvn);\n","filename":"src\/hotspot\/share\/opto\/compile.cpp","additions":6,"deletions":5,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -75,0 +75,1 @@\n+class OpaqueTemplateAssertionPredicateNode;\n@@ -380,2 +381,3 @@\n-  \/\/ List of OpaqueTemplateAssertionPredicateNode nodes for Template Assertion Predicates.\n-  GrowableArray<Node*>  _template_assertion_predicate_opaqs;\n+  \/\/ List of OpaqueTemplateAssertionPredicateNode nodes for Template Assertion Predicates which can be seen as list\n+  \/\/ of Template Assertion Predicates themselves.\n+  GrowableArray<OpaqueTemplateAssertionPredicateNode*>  _template_assertion_predicate_opaques;\n@@ -705,0 +707,8 @@\n+  const GrowableArray<ParsePredicateNode*>& parse_predicates() const {\n+    return _parse_predicates;\n+  }\n+\n+  const GrowableArray<OpaqueTemplateAssertionPredicateNode*>& template_assertion_predicate_opaques() const {\n+    return _template_assertion_predicate_opaques;\n+  }\n+\n@@ -707,1 +717,1 @@\n-  int           template_assertion_predicate_count() const { return _template_assertion_predicate_opaqs.length(); }\n+  int           template_assertion_predicate_count() const { return _template_assertion_predicate_opaques.length(); }\n@@ -712,5 +722,0 @@\n-  ParsePredicateNode* parse_predicate(int idx) const { return _parse_predicates.at(idx); }\n-\n-  Node* template_assertion_predicate_opaq_node(int idx) const {\n-    return _template_assertion_predicate_opaqs.at(idx);\n-  }\n@@ -752,2 +757,2 @@\n-  void add_template_assertion_predicate_opaq(Node* n) {\n-    assert(!_template_assertion_predicate_opaqs.contains(n),\n+  void add_template_assertion_predicate_opaque(OpaqueTemplateAssertionPredicateNode* n) {\n+    assert(!_template_assertion_predicate_opaques.contains(n),\n@@ -755,1 +760,1 @@\n-    _template_assertion_predicate_opaqs.append(n);\n+    _template_assertion_predicate_opaques.append(n);\n@@ -758,1 +763,1 @@\n-  void remove_template_assertion_predicate_opaq(Node* n) {\n+  void remove_template_assertion_predicate_opaque(OpaqueTemplateAssertionPredicateNode* n) {\n@@ -760,1 +765,1 @@\n-      _template_assertion_predicate_opaqs.remove_if_existing(n);\n+      _template_assertion_predicate_opaques.remove_if_existing(n);\n","filename":"src\/hotspot\/share\/opto\/compile.hpp","additions":18,"deletions":13,"binary":false,"changes":31,"status":"modified"},{"patch":"@@ -34,1 +34,1 @@\n-#include \"opto\/predicates.hpp\"\n+#include \"opto\/predicates_enums.hpp\"\n@@ -2189,1 +2189,1 @@\n-      _useless(false) {\n+      _predicate_state(PredicateState::Useful) {\n@@ -2206,0 +2206,5 @@\n+void ParsePredicateNode::mark_useless(PhaseIterGVN& igvn) {\n+  _predicate_state = PredicateState::Useless;\n+  igvn._worklist.push(this);\n+}\n+\n@@ -2215,0 +2220,2 @@\n+  assert(_predicate_state != PredicateState::MaybeUseful, \"should only be MaybeUseful when eliminating useless \"\n+                                                          \"predicates during loop opts\");\n@@ -2218,1 +2225,1 @@\n-  if (_useless || phase->C->post_loop_opts_phase()) {\n+  if (_predicate_state == PredicateState::Useless || phase->C->post_loop_opts_phase()) {\n@@ -2220,2 +2227,1 @@\n-  } else {\n-    return bottom_type();\n+  return bottom_type();\n@@ -2244,1 +2250,1 @@\n-  if (_useless) {\n+  if (_predicate_state == PredicateState::Useless) {\n","filename":"src\/hotspot\/share\/opto\/ifnode.cpp","additions":12,"deletions":6,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -703,3 +703,2 @@\n-    return inline_min_max(intrinsic_id());\n-\n-  case vmIntrinsics::_maxF:\n+  case vmIntrinsics::_minL:\n+  case vmIntrinsics::_maxL:\n@@ -707,1 +706,1 @@\n-  case vmIntrinsics::_maxD:\n+  case vmIntrinsics::_maxF:\n@@ -709,1 +708,1 @@\n-  case vmIntrinsics::_maxF_strict:\n+  case vmIntrinsics::_maxD:\n@@ -711,1 +710,1 @@\n-  case vmIntrinsics::_maxD_strict:\n+  case vmIntrinsics::_maxF_strict:\n@@ -713,1 +712,2 @@\n-      return inline_fp_min_max(intrinsic_id());\n+  case vmIntrinsics::_maxD_strict:\n+    return inline_min_max(intrinsic_id());\n@@ -1954,1 +1954,72 @@\n-  set_result(generate_min_max(id, argument(0), argument(1)));\n+  Node* a = nullptr;\n+  Node* b = nullptr;\n+  Node* n = nullptr;\n+  switch (id) {\n+    case vmIntrinsics::_min:\n+    case vmIntrinsics::_max:\n+    case vmIntrinsics::_minF:\n+    case vmIntrinsics::_maxF:\n+    case vmIntrinsics::_minF_strict:\n+    case vmIntrinsics::_maxF_strict:\n+    case vmIntrinsics::_min_strict:\n+    case vmIntrinsics::_max_strict:\n+      assert(callee()->signature()->size() == 2, \"minF\/maxF has 2 parameters of size 1 each.\");\n+      a = argument(0);\n+      b = argument(1);\n+      break;\n+    case vmIntrinsics::_minD:\n+    case vmIntrinsics::_maxD:\n+    case vmIntrinsics::_minD_strict:\n+    case vmIntrinsics::_maxD_strict:\n+      assert(callee()->signature()->size() == 4, \"minD\/maxD has 2 parameters of size 2 each.\");\n+      a = round_double_node(argument(0));\n+      b = round_double_node(argument(2));\n+      break;\n+    case vmIntrinsics::_minL:\n+    case vmIntrinsics::_maxL:\n+      assert(callee()->signature()->size() == 4, \"minL\/maxL has 2 parameters of size 2 each.\");\n+      a = argument(0);\n+      b = argument(2);\n+      break;\n+    default:\n+      fatal_unexpected_iid(id);\n+      break;\n+  }\n+\n+  switch (id) {\n+    case vmIntrinsics::_min:\n+    case vmIntrinsics::_min_strict:\n+      n = new MinINode(a, b);\n+      break;\n+    case vmIntrinsics::_max:\n+    case vmIntrinsics::_max_strict:\n+      n = new MaxINode(a, b);\n+      break;\n+    case vmIntrinsics::_minF:\n+    case vmIntrinsics::_minF_strict:\n+      n = new MinFNode(a, b);\n+      break;\n+    case vmIntrinsics::_maxF:\n+    case vmIntrinsics::_maxF_strict:\n+      n = new MaxFNode(a, b);\n+      break;\n+    case vmIntrinsics::_minD:\n+    case vmIntrinsics::_minD_strict:\n+      n = new MinDNode(a, b);\n+      break;\n+    case vmIntrinsics::_maxD:\n+    case vmIntrinsics::_maxD_strict:\n+      n = new MaxDNode(a, b);\n+      break;\n+    case vmIntrinsics::_minL:\n+      n = new MinLNode(_gvn.C, a, b);\n+      break;\n+    case vmIntrinsics::_maxL:\n+      n = new MaxLNode(_gvn.C, a, b);\n+      break;\n+    default:\n+      fatal_unexpected_iid(id);\n+      break;\n+  }\n+\n+  set_result(_gvn.transform(n));\n@@ -2033,19 +2104,0 @@\n-Node*\n-LibraryCallKit::generate_min_max(vmIntrinsics::ID id, Node* x0, Node* y0) {\n-  Node* result_val = nullptr;\n-  switch (id) {\n-  case vmIntrinsics::_min:\n-  case vmIntrinsics::_min_strict:\n-    result_val = _gvn.transform(new MinINode(x0, y0));\n-    break;\n-  case vmIntrinsics::_max:\n-  case vmIntrinsics::_max_strict:\n-    result_val = _gvn.transform(new MaxINode(x0, y0));\n-    break;\n-  default:\n-    fatal_unexpected_iid(id);\n-    break;\n-  }\n-  return result_val;\n-}\n-\n@@ -4821,1 +4873,1 @@\n-      Node* moved = generate_min_max(vmIntrinsics::_min, orig_tail, length);\n+      Node* moved = _gvn.transform(new MinINode(orig_tail, length));\n@@ -7031,1 +7083,1 @@\n-  assert(UseCRC32Intrinsics, \"need AVX and LCMUL instructions support\");\n+  assert(UseCRC32Intrinsics, \"need AVX and CLMUL instructions support\");\n@@ -7066,1 +7118,1 @@\n-  assert(UseCRC32Intrinsics, \"need AVX and LCMUL instructions support\");\n+  assert(UseCRC32Intrinsics, \"need AVX and CLMUL instructions support\");\n@@ -7110,1 +7162,1 @@\n-  assert(UseCRC32Intrinsics, \"need AVX and LCMUL instructions support\");\n+  assert(UseCRC32Intrinsics, \"need AVX and CLMUL instructions support\");\n@@ -8944,85 +8996,0 @@\n-\/\/------------------------------inline_fp_min_max------------------------------\n-bool LibraryCallKit::inline_fp_min_max(vmIntrinsics::ID id) {\n-\/* DISABLED BECAUSE METHOD DATA ISN'T COLLECTED PER CALL-SITE, SEE JDK-8015416.\n-\n-  \/\/ The intrinsic should be used only when the API branches aren't predictable,\n-  \/\/ the last one performing the most important comparison. The following heuristic\n-  \/\/ uses the branch statistics to eventually bail out if necessary.\n-\n-  ciMethodData *md = callee()->method_data();\n-\n-  if ( md != nullptr && md->is_mature() && md->invocation_count() > 0 ) {\n-    ciCallProfile cp = caller()->call_profile_at_bci(bci());\n-\n-    if ( ((double)cp.count()) \/ ((double)md->invocation_count()) < 0.8 ) {\n-      \/\/ Bail out if the call-site didn't contribute enough to the statistics.\n-      return false;\n-    }\n-\n-    uint taken = 0, not_taken = 0;\n-\n-    for (ciProfileData *p = md->first_data(); md->is_valid(p); p = md->next_data(p)) {\n-      if (p->is_BranchData()) {\n-        taken = ((ciBranchData*)p)->taken();\n-        not_taken = ((ciBranchData*)p)->not_taken();\n-      }\n-    }\n-\n-    double balance = (((double)taken) - ((double)not_taken)) \/ ((double)md->invocation_count());\n-    balance = balance < 0 ? -balance : balance;\n-    if ( balance > 0.2 ) {\n-      \/\/ Bail out if the most important branch is predictable enough.\n-      return false;\n-    }\n-  }\n-*\/\n-\n-  Node *a = nullptr;\n-  Node *b = nullptr;\n-  Node *n = nullptr;\n-  switch (id) {\n-  case vmIntrinsics::_maxF:\n-  case vmIntrinsics::_minF:\n-  case vmIntrinsics::_maxF_strict:\n-  case vmIntrinsics::_minF_strict:\n-    assert(callee()->signature()->size() == 2, \"minF\/maxF has 2 parameters of size 1 each.\");\n-    a = argument(0);\n-    b = argument(1);\n-    break;\n-  case vmIntrinsics::_maxD:\n-  case vmIntrinsics::_minD:\n-  case vmIntrinsics::_maxD_strict:\n-  case vmIntrinsics::_minD_strict:\n-    assert(callee()->signature()->size() == 4, \"minD\/maxD has 2 parameters of size 2 each.\");\n-    a = round_double_node(argument(0));\n-    b = round_double_node(argument(2));\n-    break;\n-  default:\n-    fatal_unexpected_iid(id);\n-    break;\n-  }\n-  switch (id) {\n-  case vmIntrinsics::_maxF:\n-  case vmIntrinsics::_maxF_strict:\n-    n = new MaxFNode(a, b);\n-    break;\n-  case vmIntrinsics::_minF:\n-  case vmIntrinsics::_minF_strict:\n-    n = new MinFNode(a, b);\n-    break;\n-  case vmIntrinsics::_maxD:\n-  case vmIntrinsics::_maxD_strict:\n-    n = new MaxDNode(a, b);\n-    break;\n-  case vmIntrinsics::_minD:\n-  case vmIntrinsics::_minD_strict:\n-    n = new MinDNode(a, b);\n-    break;\n-  default:\n-    fatal_unexpected_iid(id);\n-    break;\n-  }\n-  set_result(_gvn.transform(n));\n-  return true;\n-}\n-\n","filename":"src\/hotspot\/share\/opto\/library_call.cpp","additions":83,"deletions":116,"binary":false,"changes":199,"status":"modified"},{"patch":"@@ -246,1 +246,0 @@\n-  Node* generate_min_max(vmIntrinsics::ID id, Node* x, Node* y);\n@@ -382,1 +381,0 @@\n-  bool inline_fp_min_max(vmIntrinsics::ID id);\n","filename":"src\/hotspot\/share\/opto\/library_call.hpp","additions":0,"deletions":2,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -4403,1 +4403,1 @@\n-void PhaseIdealLoop::eliminate_useless_predicates() {\n+void PhaseIdealLoop::eliminate_useless_predicates() const {\n@@ -4408,105 +4408,2 @@\n-  eliminate_useless_parse_predicates();\n-  eliminate_useless_template_assertion_predicates();\n-}\n-\n-\/\/ Eliminate all Parse Predicates that do not belong to a loop anymore by marking them useless. These will be removed\n-\/\/ during the next round of IGVN.\n-void PhaseIdealLoop::eliminate_useless_parse_predicates() {\n-  mark_all_parse_predicates_useless();\n-  if (C->has_loops()) {\n-    mark_loop_associated_parse_predicates_useful();\n-  }\n-  add_useless_parse_predicates_to_igvn_worklist();\n-}\n-\n-void PhaseIdealLoop::mark_all_parse_predicates_useless() const {\n-  for (int i = 0; i < C->parse_predicate_count(); i++) {\n-    C->parse_predicate(i)->mark_useless();\n-  }\n-}\n-\n-void PhaseIdealLoop::mark_loop_associated_parse_predicates_useful() {\n-  for (LoopTreeIterator iterator(_ltree_root); !iterator.done(); iterator.next()) {\n-    IdealLoopTree* loop = iterator.current();\n-    if (loop->can_apply_loop_predication()) {\n-      mark_useful_parse_predicates_for_loop(loop);\n-    }\n-  }\n-}\n-\n-\/\/ This visitor marks all visited Parse Predicates useful.\n-class ParsePredicateUsefulMarker : public PredicateVisitor {\n- public:\n-  using PredicateVisitor::visit;\n-\n-  void visit(const ParsePredicate& parse_predicate) override {\n-    parse_predicate.head()->mark_useful();\n-  }\n-};\n-\n-void PhaseIdealLoop::mark_useful_parse_predicates_for_loop(IdealLoopTree* loop) {\n-  Node* entry = loop->_head->as_Loop()->skip_strip_mined()->in(LoopNode::EntryControl);\n-  const PredicateIterator predicate_iterator(entry);\n-  ParsePredicateUsefulMarker useful_marker;\n-  predicate_iterator.for_each(useful_marker);\n-}\n-\n-void PhaseIdealLoop::add_useless_parse_predicates_to_igvn_worklist() {\n-  for (int i = 0; i < C->parse_predicate_count(); i++) {\n-    ParsePredicateNode* parse_predicate_node = C->parse_predicate(i);\n-    if (parse_predicate_node->is_useless()) {\n-      _igvn._worklist.push(parse_predicate_node);\n-    }\n-  }\n-}\n-\n-\n-\/\/ Eliminate all Template Assertion Predicates that do not belong to their originally associated loop anymore by\n-\/\/ replacing the OpaqueTemplateAssertionPredicate node of the If node with true. These nodes will be removed during the\n-\/\/ next round of IGVN.\n-void PhaseIdealLoop::eliminate_useless_template_assertion_predicates() {\n-  Unique_Node_List useful_predicates;\n-  if (C->has_loops()) {\n-    collect_useful_template_assertion_predicates(useful_predicates);\n-  }\n-  eliminate_useless_template_assertion_predicates(useful_predicates);\n-}\n-\n-void PhaseIdealLoop::collect_useful_template_assertion_predicates(Unique_Node_List& useful_predicates) {\n-  for (LoopTreeIterator iterator(_ltree_root); !iterator.done(); iterator.next()) {\n-    IdealLoopTree* loop = iterator.current();\n-    if (loop->can_apply_loop_predication()) {\n-      collect_useful_template_assertion_predicates_for_loop(loop, useful_predicates);\n-    }\n-  }\n-}\n-\n-void PhaseIdealLoop::collect_useful_template_assertion_predicates_for_loop(IdealLoopTree* loop,\n-                                                                           Unique_Node_List &useful_predicates) {\n-  Node* entry = loop->_head->as_Loop()->skip_strip_mined()->in(LoopNode::EntryControl);\n-  const Predicates predicates(entry);\n-  if (UseProfiledLoopPredicate) {\n-    const PredicateBlock* profiled_loop_predicate_block = predicates.profiled_loop_predicate_block();\n-    if (profiled_loop_predicate_block->has_parse_predicate()) {\n-      ParsePredicateSuccessProj* parse_predicate_proj = profiled_loop_predicate_block->parse_predicate_success_proj();\n-      get_opaque_template_assertion_predicate_nodes(parse_predicate_proj, useful_predicates);\n-    }\n-  }\n-\n-  if (UseLoopPredicate) {\n-    const PredicateBlock* loop_predicate_block = predicates.loop_predicate_block();\n-    if (loop_predicate_block->has_parse_predicate()) {\n-      ParsePredicateSuccessProj* parse_predicate_proj = loop_predicate_block->parse_predicate_success_proj();\n-      get_opaque_template_assertion_predicate_nodes(parse_predicate_proj, useful_predicates);\n-    }\n-  }\n-}\n-\n-void PhaseIdealLoop::eliminate_useless_template_assertion_predicates(Unique_Node_List& useful_predicates) const {\n-  for (int i = C->template_assertion_predicate_count(); i > 0; i--) {\n-    OpaqueTemplateAssertionPredicateNode* opaque_node =\n-        C->template_assertion_predicate_opaq_node(i - 1)->as_OpaqueTemplateAssertionPredicate();\n-    if (!useful_predicates.member(opaque_node)) { \/\/ not in the useful list\n-      opaque_node->mark_useless(_igvn);\n-    }\n-  }\n+  EliminateUselessPredicates eliminate_useless_predicates(_igvn, _ltree_root);\n+  eliminate_useless_predicates.eliminate();\n","filename":"src\/hotspot\/share\/opto\/loopnode.cpp","additions":3,"deletions":106,"binary":false,"changes":109,"status":"modified"},{"patch":"@@ -1449,12 +1449,1 @@\n-  void eliminate_useless_predicates();\n-\n-  void eliminate_useless_parse_predicates();\n-  void mark_all_parse_predicates_useless() const;\n-  void mark_loop_associated_parse_predicates_useful();\n-  static void mark_useful_parse_predicates_for_loop(IdealLoopTree* loop);\n-  void add_useless_parse_predicates_to_igvn_worklist();\n-\n-  void eliminate_useless_template_assertion_predicates();\n-  void collect_useful_template_assertion_predicates(Unique_Node_List& useful_predicates);\n-  static void collect_useful_template_assertion_predicates_for_loop(IdealLoopTree* loop, Unique_Node_List& useful_predicates);\n-  void eliminate_useless_template_assertion_predicates(Unique_Node_List& useful_predicates) const;\n+  void eliminate_useless_predicates() const;\n","filename":"src\/hotspot\/share\/opto\/loopnode.hpp","additions":1,"deletions":12,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -860,1 +860,0 @@\n-    if (PrintOpto && VerifyLoopOptimizations) { tty->print_cr(\"CMOV\"); }\n@@ -868,6 +867,0 @@\n-#ifndef PRODUCT\n-          if (PrintOpto && VerifyLoopOptimizations) {\n-            tty->print(\"  speculate: \");\n-            m->dump();\n-          }\n-#endif\n@@ -1661,1 +1654,1 @@\n-    if ((PrintOpto && VerifyLoopOptimizations) || TraceLoopOpts) {\n+    if (TraceLoopOpts) {\n","filename":"src\/hotspot\/share\/opto\/loopopts.cpp","additions":1,"deletions":8,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -520,0 +520,3 @@\n+  if (n->is_OpaqueTemplateAssertionPredicate()) {\n+    C->add_template_assertion_predicate_opaque(n->as_OpaqueTemplateAssertionPredicate());\n+  }\n@@ -619,1 +622,1 @@\n-    compile->remove_template_assertion_predicate_opaq(this);\n+    compile->remove_template_assertion_predicate_opaque(as_OpaqueTemplateAssertionPredicate());\n@@ -682,3 +685,3 @@\n-\/\/------------------------------grow-------------------------------------------\n-\/\/ Grow the input array, making space for more edges\n-void Node::grow(uint len) {\n+\/\/ Resize input or output array to grow it to the next larger power-of-2 bigger\n+\/\/ than len.\n+void Node::resize_array(Node**& array, node_idx_t& max_size, uint len, bool needs_clearing) {\n@@ -686,9 +689,10 @@\n-  uint new_max = _max;\n-  if( new_max == 0 ) {\n-    _max = 4;\n-    _in = (Node**)arena->Amalloc(4*sizeof(Node*));\n-    Node** to = _in;\n-    to[0] = nullptr;\n-    to[1] = nullptr;\n-    to[2] = nullptr;\n-    to[3] = nullptr;\n+  uint new_max = max_size;\n+  if (new_max == 0) {\n+    max_size = 4;\n+    array = (Node**)arena->Amalloc(4 * sizeof(Node*));\n+    if (needs_clearing) {\n+      array[0] = nullptr;\n+      array[1] = nullptr;\n+      array[2] = nullptr;\n+      array[3] = nullptr;\n+    }\n@@ -698,6 +702,6 @@\n-  \/\/ Trimming to limit allows a uint8 to handle up to 255 edges.\n-  \/\/ Previously I was using only powers-of-2 which peaked at 128 edges.\n-  \/\/if( new_max >= limit ) new_max = limit-1;\n-  _in = (Node**)arena->Arealloc(_in, _max*sizeof(Node*), new_max*sizeof(Node*));\n-  Copy::zero_to_bytes(&_in[_max], (new_max-_max)*sizeof(Node*)); \/\/ null all new space\n-  _max = new_max;               \/\/ Record new max length\n+  assert(needs_clearing || (array != nullptr && array != NO_OUT_ARRAY), \"out must have sensible value\");\n+  array = (Node**)arena->Arealloc(array, max_size * sizeof(Node*), new_max * sizeof(Node*));\n+  if (needs_clearing) {\n+    Copy::zero_to_bytes(&array[max_size], (new_max - max_size) * sizeof(Node*)); \/\/ null all new space\n+  }\n+  max_size = new_max;               \/\/ Record new max length\n@@ -706,1 +710,7 @@\n-  assert(_max == new_max && _max > len, \"int width of _max is too small\");\n+  assert(max_size > len, \"int width of _max or _outmax is too small\");\n+}\n+\n+\/\/------------------------------grow-------------------------------------------\n+\/\/ Grow the input array, making space for more edges\n+void Node::grow(uint len) {\n+  resize_array(_in, _max, len, true);\n@@ -711,1 +721,1 @@\n-void Node::out_grow( uint len ) {\n+void Node::out_grow(uint len) {\n@@ -713,18 +723,1 @@\n-  Arena* arena = Compile::current()->node_arena();\n-  uint new_max = _outmax;\n-  if( new_max == 0 ) {\n-    _outmax = 4;\n-    _out = (Node **)arena->Amalloc(4*sizeof(Node*));\n-    return;\n-  }\n-  new_max = next_power_of_2(len);\n-  \/\/ Trimming to limit allows a uint8 to handle up to 255 edges.\n-  \/\/ Previously I was using only powers-of-2 which peaked at 128 edges.\n-  \/\/if( new_max >= limit ) new_max = limit-1;\n-  assert(_out != nullptr && _out != NO_OUT_ARRAY, \"out must have sensible value\");\n-  _out = (Node**)arena->Arealloc(_out,_outmax*sizeof(Node*),new_max*sizeof(Node*));\n-  \/\/Copy::zero_to_bytes(&_out[_outmax], (new_max-_outmax)*sizeof(Node*)); \/\/ null all new space\n-  _outmax = new_max;               \/\/ Record new max length\n-  \/\/ This assertion makes sure that Node::_max is wide enough to\n-  \/\/ represent the numerical value of new_max.\n-  assert(_outmax == new_max && _outmax > len, \"int width of _outmax is too small\");\n+  resize_array(_out, _outmax, len, false);\n","filename":"src\/hotspot\/share\/opto\/node.cpp","additions":32,"deletions":39,"binary":false,"changes":71,"status":"modified"},{"patch":"@@ -342,0 +342,3 @@\n+  \/\/ Resize input or output array to grow it to the next larger power-of-2\n+  \/\/ bigger than len.\n+  void resize_array(Node**& array, node_idx_t& max_size, uint len, bool needs_clearing);\n@@ -343,1 +346,1 @@\n- public:\n+public:\n","filename":"src\/hotspot\/share\/opto\/node.hpp","additions":4,"deletions":1,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -139,8 +139,0 @@\n-  \/\/ Found some other Node; must clone it up\n-#ifndef PRODUCT\n-  if( PrintOpto && VerifyLoopOptimizations ) {\n-    tty->print(\"Cloning up: \");\n-    n->dump();\n-  }\n-#endif\n-\n@@ -312,6 +304,0 @@\n-#ifndef PRODUCT\n-      if( PrintOpto && VerifyLoopOptimizations ) {\n-        tty->print(\"Cloning down: \");\n-        n->dump();\n-      }\n-#endif\n@@ -347,6 +333,0 @@\n-#ifndef PRODUCT\n-            if( PrintOpto && VerifyLoopOptimizations ) {\n-              tty->print(\"Cloning down: \");\n-              bol->dump();\n-            }\n-#endif\n","filename":"src\/hotspot\/share\/opto\/split_if.cpp","additions":0,"deletions":20,"binary":false,"changes":20,"status":"modified"},{"patch":"@@ -1457,1 +1457,1 @@\n-          range(0, max_uintx)                                               \\\n+          range(0, max_uintx \/ 2 + 1)                                       \\\n","filename":"src\/hotspot\/share\/runtime\/globals.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2025, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2024, Oracle and\/or its affiliates. All rights reserved.\n","filename":"src\/hotspot\/share\/runtime\/signature.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -32,2 +32,0 @@\n-#include \"sanitizers\/ub.hpp\"\n-\n@@ -343,1 +341,1 @@\n-  ATTRIBUTE_NO_UBSAN\n+\n@@ -346,2 +344,4 @@\n-    _accumulator |= ((fingerprint_t)type << _shift_count);\n-    _shift_count += fp_parameter_feature_size;\n+    if (_param_size <= fp_max_size_of_parameters) {\n+      _accumulator |= ((fingerprint_t)type << _shift_count);\n+      _shift_count += fp_parameter_feature_size;\n+    }\n","filename":"src\/hotspot\/share\/runtime\/signature.hpp","additions":6,"deletions":6,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -2200,5 +2200,0 @@\n-            @Override\n-            public void exit(int statusCode) {\n-                Shutdown.exit(statusCode);\n-            }\n-\n","filename":"src\/java.base\/share\/classes\/java\/lang\/System.java","additions":0,"deletions":5,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -481,6 +481,0 @@\n-    \/**\n-     * Direct access to Shutdown.exit to avoid security manager checks\n-     * @param statusCode the status code\n-     *\/\n-    void exit(int statusCode);\n-\n","filename":"src\/java.base\/share\/classes\/jdk\/internal\/access\/JavaLangAccess.java","additions":0,"deletions":6,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -82,0 +82,3 @@\n+compiler\/ciReplay\/TestInliningProtectionDomain.java 8349191 generic-all\n+compiler\/ciReplay\/TestIncrementalInlining.java 8349191 generic-all\n+\n@@ -103,1 +106,0 @@\n-gc\/TestAllocHumongousFragment.java#generational 8351464 generic-all\n","filename":"test\/hotspot\/jtreg\/ProblemList.txt","additions":3,"deletions":1,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -760,1 +760,0 @@\n-jdk\/incubator\/vector\/Long256VectorTests.java                    8350840 generic-x64\n@@ -770,0 +769,3 @@\n+jdk\/jfr\/api\/consumer\/streaming\/TestJVMCrash.java                8344671 macosx-all\n+jdk\/jfr\/api\/consumer\/streaming\/TestJVMExit.java                 8344671 macosx-all\n+jdk\/jfr\/api\/consumer\/streaming\/TestOutOfProcessMigration.java   8344671 macosx-all\n","filename":"test\/jdk\/ProblemList.txt","additions":3,"deletions":1,"binary":false,"changes":4,"status":"modified"}]}