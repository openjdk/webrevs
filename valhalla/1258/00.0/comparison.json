{"files":[{"patch":"@@ -2,1 +2,1 @@\n-# Copyright (c) 2011, 2020, Oracle and\/or its affiliates. All rights reserved.\n+# Copyright (c) 2011, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -33,0 +33,2 @@\n+include MakeIO.gmk\n+\n@@ -198,1 +200,1 @@\n-    $1_JAVAC_CMD := $$(JAVAC)\n+    $1_JAVAC_CMD := $$(JAVAC) -J$$(JAVA_FLAGS_TMPDIR)\n@@ -212,1 +214,1 @@\n-    $1_JAVAC_CMD := $$(BUILD_JAVAC)\n+    $1_JAVAC_CMD := $$(BUILD_JAVAC) -J$$(JAVA_FLAGS_TMPDIR)\n","filename":"make\/common\/JavaCompilation.gmk","additions":5,"deletions":3,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -3,1 +3,1 @@\n-\/\/ Copyright (c) 2014, 2021, Red Hat, Inc. All rights reserved.\n+\/\/ Copyright (c) 2014, 2024, Red Hat, Inc. All rights reserved.\n@@ -697,0 +697,5 @@\n+\/\/ Class for all non_special pointer registers (excluding rfp)\n+reg_class no_special_no_rfp_ptr_reg %{\n+  return _NO_SPECIAL_NO_RFP_PTR_REG_mask;\n+%}\n+\n@@ -1128,0 +1133,1 @@\n+extern RegMask _NO_SPECIAL_NO_RFP_PTR_REG_mask;\n@@ -1151,2 +1157,2 @@\n-  static int emit_exception_handler(CodeBuffer &cbuf);\n-  static int emit_deopt_handler(CodeBuffer& cbuf);\n+  static int emit_exception_handler(C2_MacroAssembler *masm);\n+  static int emit_deopt_handler(C2_MacroAssembler* masm);\n@@ -1216,0 +1222,1 @@\n+  RegMask _NO_SPECIAL_NO_RFP_PTR_REG_mask;\n@@ -1252,0 +1259,3 @@\n+\n+    _NO_SPECIAL_NO_RFP_PTR_REG_mask = _NO_SPECIAL_PTR_REG_mask;\n+    _NO_SPECIAL_NO_RFP_PTR_REG_mask.Remove(OptoReg::as_OptoReg(r29->as_VMReg()));\n@@ -1605,1 +1615,1 @@\n-#define __ _masm.\n+#define __ masm->\n@@ -1663,2 +1673,1 @@\n-void MachBreakpointNode::emit(CodeBuffer &cbuf, PhaseRegAlloc *ra_) const {\n-  C2_MacroAssembler _masm(&cbuf);\n+void MachBreakpointNode::emit(C2_MacroAssembler *masm, PhaseRegAlloc *ra_) const {\n@@ -1680,2 +1689,1 @@\n-  void MachNopNode::emit(CodeBuffer &cbuf, PhaseRegAlloc*) const {\n-    C2_MacroAssembler _masm(&cbuf);\n+  void MachNopNode::emit(C2_MacroAssembler *masm, PhaseRegAlloc*) const {\n@@ -1703,1 +1711,1 @@\n-void MachConstantBaseNode::emit(CodeBuffer& cbuf, PhaseRegAlloc* ra_) const {\n+void MachConstantBaseNode::emit(C2_MacroAssembler* masm, PhaseRegAlloc* ra_) const {\n@@ -1757,1 +1765,1 @@\n-void MachPrologNode::emit(CodeBuffer &cbuf, PhaseRegAlloc *ra_) const {\n+void MachPrologNode::emit(C2_MacroAssembler *masm, PhaseRegAlloc *ra_) const {\n@@ -1759,1 +1767,0 @@\n-  C2_MacroAssembler _masm(&cbuf);\n@@ -1779,1 +1786,1 @@\n-  C->output()->set_frame_complete(cbuf.insts_size());\n+  C->output()->set_frame_complete(__ offset());\n@@ -1827,1 +1834,1 @@\n-void MachEpilogNode::emit(CodeBuffer &cbuf, PhaseRegAlloc *ra_) const {\n+void MachEpilogNode::emit(C2_MacroAssembler *masm, PhaseRegAlloc *ra_) const {\n@@ -1829,1 +1836,0 @@\n-  C2_MacroAssembler _masm(&cbuf);\n@@ -1862,4 +1868,0 @@\n-\/\/ Figure out which register class each belongs in: rc_int, rc_float or\n-\/\/ rc_stack.\n-enum RC { rc_bad, rc_int, rc_float, rc_predicate, rc_stack };\n-\n@@ -1896,1 +1898,1 @@\n-uint MachSpillCopyNode::implementation(CodeBuffer *cbuf, PhaseRegAlloc *ra_, bool do_size, outputStream *st) const {\n+uint MachSpillCopyNode::implementation(C2_MacroAssembler *masm, PhaseRegAlloc *ra_, bool do_size, outputStream *st) const {\n@@ -1929,2 +1931,1 @@\n-    if (ireg == Op_VecA && cbuf) {\n-      C2_MacroAssembler _masm(cbuf);\n+    if (ireg == Op_VecA && masm) {\n@@ -1949,1 +1950,1 @@\n-    } else if (cbuf) {\n+    } else if (masm) {\n@@ -1951,1 +1952,0 @@\n-      C2_MacroAssembler _masm(cbuf);\n@@ -1978,2 +1978,1 @@\n-  } else if (cbuf) {\n-    C2_MacroAssembler _masm(cbuf);\n+  } else if (masm) {\n@@ -1987,1 +1986,0 @@\n-            C2_MacroAssembler _masm(cbuf);\n@@ -2115,2 +2113,2 @@\n-void MachSpillCopyNode::emit(CodeBuffer &cbuf, PhaseRegAlloc *ra_) const {\n-  implementation(&cbuf, ra_, false, nullptr);\n+void MachSpillCopyNode::emit(C2_MacroAssembler *masm, PhaseRegAlloc *ra_) const {\n+  implementation(masm, ra_, false, nullptr);\n@@ -2134,3 +2132,1 @@\n-void BoxLockNode::emit(CodeBuffer &cbuf, PhaseRegAlloc *ra_) const {\n-  C2_MacroAssembler _masm(&cbuf);\n-\n+void BoxLockNode::emit(C2_MacroAssembler *masm, PhaseRegAlloc *ra_) const {\n@@ -2169,1 +2165,1 @@\n-void MachVEPNode::emit(CodeBuffer& cbuf, PhaseRegAlloc* ra_) const\n+void MachVEPNode::emit(C2_MacroAssembler *masm, PhaseRegAlloc* ra_) const\n@@ -2171,2 +2167,0 @@\n-  C2_MacroAssembler _masm(&cbuf);\n-\n@@ -2219,1 +2213,1 @@\n-void MachUEPNode::emit(CodeBuffer& cbuf, PhaseRegAlloc* ra_) const\n+void MachUEPNode::emit(C2_MacroAssembler* masm, PhaseRegAlloc* ra_) const\n@@ -2221,2 +2215,0 @@\n-  \/\/ This is the unverified entry point.\n-  C2_MacroAssembler _masm(&cbuf);\n@@ -2231,1 +2223,1 @@\n-int HandlerImpl::emit_exception_handler(CodeBuffer& cbuf)\n+int HandlerImpl::emit_exception_handler(C2_MacroAssembler* masm)\n@@ -2237,1 +2229,0 @@\n-  C2_MacroAssembler _masm(&cbuf);\n@@ -2251,1 +2242,1 @@\n-int HandlerImpl::emit_deopt_handler(CodeBuffer& cbuf)\n+int HandlerImpl::emit_deopt_handler(C2_MacroAssembler* masm)\n@@ -2255,1 +2246,0 @@\n-  C2_MacroAssembler _masm(&cbuf);\n@@ -2691,1 +2681,0 @@\n-  C2_MacroAssembler _masm(&cbuf);                                       \\\n@@ -2736,1 +2725,1 @@\n-  static void loadStore(C2_MacroAssembler masm, mem_insn insn,\n+  static void loadStore(C2_MacroAssembler* masm, mem_insn insn,\n@@ -2746,1 +2735,1 @@\n-      addr = masm.legitimize_address(addr, size_in_memory, rscratch1);\n+      addr = __ legitimize_address(addr, size_in_memory, rscratch1);\n@@ -2748,1 +2737,1 @@\n-    (masm.*insn)(reg, addr);\n+    (masm->*insn)(reg, addr);\n@@ -2751,1 +2740,1 @@\n-  static void loadStore(C2_MacroAssembler masm, mem_float_insn insn,\n+  static void loadStore(C2_MacroAssembler* masm, mem_float_insn insn,\n@@ -2774,2 +2763,2 @@\n-      addr = masm.legitimize_address(addr, size_in_memory, rscratch1);\n-      (masm.*insn)(reg, addr);\n+      addr = __ legitimize_address(addr, size_in_memory, rscratch1);\n+      (masm->*insn)(reg, addr);\n@@ -2778,1 +2767,1 @@\n-      (masm.*insn)(reg, Address(base, as_Register(index), scale));\n+      (masm->*insn)(reg, Address(base, as_Register(index), scale));\n@@ -2782,1 +2771,1 @@\n-  static void loadStore(C2_MacroAssembler masm, mem_vector_insn insn,\n+  static void loadStore(C2_MacroAssembler* masm, mem_vector_insn insn,\n@@ -2787,1 +2776,1 @@\n-      (masm.*insn)(reg, T, Address(base, disp));\n+      (masm->*insn)(reg, T, Address(base, disp));\n@@ -2790,1 +2779,1 @@\n-      (masm.*insn)(reg, T, Address(base, as_Register(index), Address::lsl(size)));\n+      (masm->*insn)(reg, T, Address(base, as_Register(index), Address::lsl(size)));\n@@ -2835,1 +2824,0 @@\n-    C2_MacroAssembler _masm(&cbuf);\n@@ -2845,1 +2833,1 @@\n-    loadStore(C2_MacroAssembler(&cbuf), &MacroAssembler::ldrsbw, dst_reg, $mem->opcode(),\n+    loadStore(masm, &MacroAssembler::ldrsbw, dst_reg, $mem->opcode(),\n@@ -2853,1 +2841,1 @@\n-    loadStore(C2_MacroAssembler(&cbuf), &MacroAssembler::ldrsb, dst_reg, $mem->opcode(),\n+    loadStore(masm, &MacroAssembler::ldrsb, dst_reg, $mem->opcode(),\n@@ -2861,1 +2849,1 @@\n-    loadStore(C2_MacroAssembler(&cbuf), &MacroAssembler::ldrb, dst_reg, $mem->opcode(),\n+    loadStore(masm, &MacroAssembler::ldrb, dst_reg, $mem->opcode(),\n@@ -2869,1 +2857,1 @@\n-    loadStore(C2_MacroAssembler(&cbuf), &MacroAssembler::ldrb, dst_reg, $mem->opcode(),\n+    loadStore(masm, &MacroAssembler::ldrb, dst_reg, $mem->opcode(),\n@@ -2877,1 +2865,1 @@\n-    loadStore(C2_MacroAssembler(&cbuf), &MacroAssembler::ldrshw, dst_reg, $mem->opcode(),\n+    loadStore(masm, &MacroAssembler::ldrshw, dst_reg, $mem->opcode(),\n@@ -2885,1 +2873,1 @@\n-    loadStore(C2_MacroAssembler(&cbuf), &MacroAssembler::ldrsh, dst_reg, $mem->opcode(),\n+    loadStore(masm, &MacroAssembler::ldrsh, dst_reg, $mem->opcode(),\n@@ -2893,1 +2881,1 @@\n-    loadStore(C2_MacroAssembler(&cbuf), &MacroAssembler::ldrh, dst_reg, $mem->opcode(),\n+    loadStore(masm, &MacroAssembler::ldrh, dst_reg, $mem->opcode(),\n@@ -2901,1 +2889,1 @@\n-    loadStore(C2_MacroAssembler(&cbuf), &MacroAssembler::ldrh, dst_reg, $mem->opcode(),\n+    loadStore(masm, &MacroAssembler::ldrh, dst_reg, $mem->opcode(),\n@@ -2909,1 +2897,1 @@\n-    loadStore(C2_MacroAssembler(&cbuf), &MacroAssembler::ldrw, dst_reg, $mem->opcode(),\n+    loadStore(masm, &MacroAssembler::ldrw, dst_reg, $mem->opcode(),\n@@ -2917,1 +2905,1 @@\n-    loadStore(C2_MacroAssembler(&cbuf), &MacroAssembler::ldrw, dst_reg, $mem->opcode(),\n+    loadStore(masm, &MacroAssembler::ldrw, dst_reg, $mem->opcode(),\n@@ -2925,1 +2913,1 @@\n-    loadStore(C2_MacroAssembler(&cbuf), &MacroAssembler::ldrsw, dst_reg, $mem->opcode(),\n+    loadStore(masm, &MacroAssembler::ldrsw, dst_reg, $mem->opcode(),\n@@ -2933,1 +2921,1 @@\n-    loadStore(C2_MacroAssembler(&cbuf), &MacroAssembler::ldr, dst_reg, $mem->opcode(),\n+    loadStore(masm, &MacroAssembler::ldr, dst_reg, $mem->opcode(),\n@@ -2941,1 +2929,1 @@\n-    loadStore(C2_MacroAssembler(&cbuf), &MacroAssembler::ldrs, dst_reg, $mem->opcode(),\n+    loadStore(masm, &MacroAssembler::ldrs, dst_reg, $mem->opcode(),\n@@ -2949,1 +2937,1 @@\n-    loadStore(C2_MacroAssembler(&cbuf), &MacroAssembler::ldrd, dst_reg, $mem->opcode(),\n+    loadStore(masm, &MacroAssembler::ldrd, dst_reg, $mem->opcode(),\n@@ -2957,1 +2945,1 @@\n-    loadStore(C2_MacroAssembler(&cbuf), &MacroAssembler::strb, src_reg, $mem->opcode(),\n+    loadStore(masm, &MacroAssembler::strb, src_reg, $mem->opcode(),\n@@ -2964,2 +2952,1 @@\n-    C2_MacroAssembler _masm(&cbuf);\n-    loadStore(_masm, &MacroAssembler::strb, zr, $mem->opcode(),\n+    loadStore(masm, &MacroAssembler::strb, zr, $mem->opcode(),\n@@ -2973,1 +2960,1 @@\n-    loadStore(C2_MacroAssembler(&cbuf), &MacroAssembler::strh, src_reg, $mem->opcode(),\n+    loadStore(masm, &MacroAssembler::strh, src_reg, $mem->opcode(),\n@@ -2980,2 +2967,1 @@\n-    C2_MacroAssembler _masm(&cbuf);\n-    loadStore(_masm, &MacroAssembler::strh, zr, $mem->opcode(),\n+    loadStore(masm, &MacroAssembler::strh, zr, $mem->opcode(),\n@@ -2989,1 +2975,1 @@\n-    loadStore(C2_MacroAssembler(&cbuf), &MacroAssembler::strw, src_reg, $mem->opcode(),\n+    loadStore(masm, &MacroAssembler::strw, src_reg, $mem->opcode(),\n@@ -2996,2 +2982,1 @@\n-    C2_MacroAssembler _masm(&cbuf);\n-    loadStore(_masm, &MacroAssembler::strw, zr, $mem->opcode(),\n+    loadStore(masm, &MacroAssembler::strw, zr, $mem->opcode(),\n@@ -3008,1 +2993,0 @@\n-      C2_MacroAssembler _masm(&cbuf);\n@@ -3013,1 +2997,1 @@\n-    loadStore(C2_MacroAssembler(&cbuf), &MacroAssembler::str, src_reg, $mem->opcode(),\n+    loadStore(masm, &MacroAssembler::str, src_reg, $mem->opcode(),\n@@ -3020,2 +3004,1 @@\n-    C2_MacroAssembler _masm(&cbuf);\n-    loadStore(_masm, &MacroAssembler::str, zr, $mem->opcode(),\n+    loadStore(masm, &MacroAssembler::str, zr, $mem->opcode(),\n@@ -3029,1 +3012,1 @@\n-    loadStore(C2_MacroAssembler(&cbuf), &MacroAssembler::strs, src_reg, $mem->opcode(),\n+    loadStore(masm, &MacroAssembler::strs, src_reg, $mem->opcode(),\n@@ -3037,1 +3020,1 @@\n-    loadStore(C2_MacroAssembler(&cbuf), &MacroAssembler::strd, src_reg, $mem->opcode(),\n+    loadStore(masm, &MacroAssembler::strd, src_reg, $mem->opcode(),\n@@ -3044,2 +3027,1 @@\n-      C2_MacroAssembler _masm(&cbuf);\n-      loadStore(_masm, &MacroAssembler::strb, zr, $mem->opcode(),\n+      loadStore(masm, &MacroAssembler::strb, zr, $mem->opcode(),\n@@ -3055,1 +3037,1 @@\n-    loadStore(C2_MacroAssembler(&cbuf), &MacroAssembler::ldr, dst_reg, MacroAssembler::H,\n+    loadStore(masm, &MacroAssembler::ldr, dst_reg, MacroAssembler::H,\n@@ -3061,1 +3043,1 @@\n-    loadStore(C2_MacroAssembler(&cbuf), &MacroAssembler::ldr, dst_reg, MacroAssembler::S,\n+    loadStore(masm, &MacroAssembler::ldr, dst_reg, MacroAssembler::S,\n@@ -3067,1 +3049,1 @@\n-    loadStore(C2_MacroAssembler(&cbuf), &MacroAssembler::ldr, dst_reg, MacroAssembler::D,\n+    loadStore(masm, &MacroAssembler::ldr, dst_reg, MacroAssembler::D,\n@@ -3073,1 +3055,1 @@\n-    loadStore(C2_MacroAssembler(&cbuf), &MacroAssembler::ldr, dst_reg, MacroAssembler::Q,\n+    loadStore(masm, &MacroAssembler::ldr, dst_reg, MacroAssembler::Q,\n@@ -3079,1 +3061,1 @@\n-    loadStore(C2_MacroAssembler(&cbuf), &MacroAssembler::str, src_reg, MacroAssembler::H,\n+    loadStore(masm, &MacroAssembler::str, src_reg, MacroAssembler::H,\n@@ -3085,1 +3067,1 @@\n-    loadStore(C2_MacroAssembler(&cbuf), &MacroAssembler::str, src_reg, MacroAssembler::S,\n+    loadStore(masm, &MacroAssembler::str, src_reg, MacroAssembler::S,\n@@ -3091,1 +3073,1 @@\n-    loadStore(C2_MacroAssembler(&cbuf), &MacroAssembler::str, src_reg, MacroAssembler::D,\n+    loadStore(masm, &MacroAssembler::str, src_reg, MacroAssembler::D,\n@@ -3097,1 +3079,1 @@\n-    loadStore(C2_MacroAssembler(&cbuf), &MacroAssembler::str, src_reg, MacroAssembler::Q,\n+    loadStore(masm, &MacroAssembler::str, src_reg, MacroAssembler::Q,\n@@ -3213,1 +3195,0 @@\n-      C2_MacroAssembler _masm(&cbuf);\n@@ -3229,1 +3210,0 @@\n-      C2_MacroAssembler _masm(&cbuf);\n@@ -3239,1 +3219,0 @@\n-      C2_MacroAssembler _masm(&cbuf);\n@@ -3250,1 +3229,0 @@\n-    C2_MacroAssembler _masm(&cbuf);\n@@ -3279,1 +3257,0 @@\n-    C2_MacroAssembler _masm(&cbuf);\n@@ -3309,1 +3286,0 @@\n-    C2_MacroAssembler _masm(&cbuf);\n@@ -3317,1 +3293,0 @@\n-    C2_MacroAssembler _masm(&cbuf);\n@@ -3325,1 +3300,0 @@\n-    C2_MacroAssembler _masm(&cbuf);\n@@ -3333,1 +3307,0 @@\n-    C2_MacroAssembler _masm(&cbuf);\n@@ -3346,1 +3319,0 @@\n-    C2_MacroAssembler _masm(&cbuf);\n@@ -3354,1 +3326,0 @@\n-    C2_MacroAssembler _masm(&cbuf);\n@@ -3362,1 +3333,0 @@\n-    C2_MacroAssembler _masm(&cbuf);\n@@ -3370,1 +3340,0 @@\n-    C2_MacroAssembler _masm(&cbuf);\n@@ -3379,1 +3348,0 @@\n-    C2_MacroAssembler _masm(&cbuf);\n@@ -3387,1 +3355,0 @@\n-    C2_MacroAssembler _masm(&cbuf);\n@@ -3408,1 +3375,0 @@\n-    C2_MacroAssembler _masm(&cbuf);\n@@ -3419,1 +3385,0 @@\n-    C2_MacroAssembler _masm(&cbuf);\n@@ -3430,1 +3395,0 @@\n-    C2_MacroAssembler _masm(&cbuf);\n@@ -3456,1 +3420,0 @@\n-    C2_MacroAssembler _masm(&cbuf);\n@@ -3462,1 +3425,0 @@\n-    C2_MacroAssembler _masm(&cbuf);\n@@ -3468,1 +3430,0 @@\n-    C2_MacroAssembler _masm(&cbuf);\n@@ -3473,1 +3434,0 @@\n-    C2_MacroAssembler _masm(&cbuf);\n@@ -3486,1 +3446,0 @@\n-    C2_MacroAssembler _masm(&cbuf);\n@@ -3492,1 +3451,0 @@\n-    C2_MacroAssembler _masm(&cbuf);\n@@ -3507,1 +3465,0 @@\n-    C2_MacroAssembler _masm(&cbuf);\n@@ -3521,1 +3478,0 @@\n-    C2_MacroAssembler _masm(&cbuf);\n@@ -3535,1 +3491,0 @@\n-    C2_MacroAssembler _masm(&cbuf);\n@@ -3543,1 +3498,0 @@\n-    C2_MacroAssembler _masm(&cbuf);\n@@ -3551,1 +3505,0 @@\n-    C2_MacroAssembler _masm(&cbuf);\n@@ -3559,1 +3512,0 @@\n-    C2_MacroAssembler _masm(&cbuf);\n@@ -3569,1 +3521,0 @@\n-    C2_MacroAssembler _masm(&cbuf);\n@@ -3576,1 +3527,0 @@\n-    C2_MacroAssembler _masm(&cbuf);\n@@ -3587,1 +3537,0 @@\n-    C2_MacroAssembler _masm(&cbuf);\n@@ -3595,1 +3544,0 @@\n-    C2_MacroAssembler _masm(&cbuf);\n@@ -3602,1 +3550,0 @@\n-    C2_MacroAssembler _masm(&cbuf);\n@@ -3617,1 +3564,0 @@\n-    C2_MacroAssembler _masm(&cbuf);\n@@ -3625,1 +3571,0 @@\n-    C2_MacroAssembler _masm(&cbuf);\n@@ -3632,1 +3577,0 @@\n-    C2_MacroAssembler _masm(&cbuf);\n@@ -3639,1 +3583,0 @@\n-    C2_MacroAssembler _masm(&cbuf);\n@@ -3645,1 +3588,0 @@\n-    C2_MacroAssembler _masm(&cbuf);\n@@ -3651,1 +3593,0 @@\n-    C2_MacroAssembler _masm(&cbuf);\n@@ -3657,1 +3598,0 @@\n-    C2_MacroAssembler _masm(&cbuf);\n@@ -3663,1 +3603,0 @@\n-    C2_MacroAssembler _masm(&cbuf);\n@@ -3676,1 +3615,0 @@\n-     C2_MacroAssembler _masm(&cbuf);\n@@ -3687,2 +3625,0 @@\n-    C2_MacroAssembler _masm(&cbuf);\n-\n@@ -3704,1 +3640,1 @@\n-      int method_index = resolved_method_index(cbuf);\n+      int method_index = resolved_method_index(masm);\n@@ -3715,1 +3651,1 @@\n-        cbuf.shared_stub_to_interp_for(_method, call - cbuf.insts_begin());\n+        __ code()->shared_stub_to_interp_for(_method, call - __ begin());\n@@ -3718,1 +3654,1 @@\n-        address stub = CompiledDirectCall::emit_to_interp_stub(cbuf, call);\n+        address stub = CompiledDirectCall::emit_to_interp_stub(masm, call);\n@@ -3735,2 +3671,1 @@\n-    C2_MacroAssembler _masm(&cbuf);\n-    int method_index = resolved_method_index(cbuf);\n+    int method_index = resolved_method_index(masm);\n@@ -3749,1 +3684,0 @@\n-    C2_MacroAssembler _masm(&cbuf);\n@@ -3788,2 +3722,0 @@\n-    C2_MacroAssembler _masm(&cbuf);\n-\n@@ -3820,1 +3752,0 @@\n-    C2_MacroAssembler _masm(&cbuf);\n@@ -3825,1 +3756,0 @@\n-    C2_MacroAssembler _masm(&cbuf);\n@@ -3835,1 +3765,0 @@\n-    C2_MacroAssembler _masm(&cbuf);\n@@ -3841,1 +3770,0 @@\n-    C2_MacroAssembler _masm(&cbuf);\n@@ -4610,1 +4538,1 @@\n-\/\/ Null Pointer Immediate\n+\/\/ nullptr Pointer Immediate\n@@ -4718,1 +4646,1 @@\n-\/\/ Narrow Null Pointer Immediate\n+\/\/ Narrow nullptr Pointer Immediate\n@@ -4815,0 +4743,12 @@\n+\/\/ This operand is not allowed to use rfp even if\n+\/\/ rfp is not used to hold the frame pointer.\n+operand iRegPNoSpNoRfp()\n+%{\n+  constraint(ALLOC_IN_RC(no_special_no_rfp_ptr_reg));\n+  match(RegP);\n+  match(iRegPNoSp);\n+  op_cost(0);\n+  format %{ %}\n+  interface(REG_INTER);\n+%}\n+\n@@ -6813,1 +6753,1 @@\n-  format %{ \"mov  $dst, $con\\t# null pointer\" %}\n+  format %{ \"mov  $dst, $con\\t# nullptr ptr\" %}\n@@ -6827,1 +6767,1 @@\n-  format %{ \"mov  $dst, $con\\t# null pointer\" %}\n+  format %{ \"mov  $dst, $con\\t# nullptr ptr\" %}\n@@ -6869,1 +6809,1 @@\n-  format %{ \"mov  $dst, $con\\t# compressed null pointer\" %}\n+  format %{ \"mov  $dst, $con\\t# compressed nullptr ptr\" %}\n@@ -7780,1 +7720,1 @@\n-    loadStore(C2_MacroAssembler(&cbuf), &MacroAssembler::ldrs, tmp_reg, $mem->opcode(),\n+    loadStore(masm, &MacroAssembler::ldrs, tmp_reg, $mem->opcode(),\n@@ -7821,1 +7761,1 @@\n-    loadStore(C2_MacroAssembler(&cbuf), &MacroAssembler::ldrd, tmp_reg, $mem->opcode(),\n+    loadStore(masm, &MacroAssembler::ldrd, tmp_reg, $mem->opcode(),\n@@ -16403,1 +16343,3 @@\n-instruct TailCalljmpInd(iRegPNoSp jump_target, inline_cache_RegP method_ptr)\n+\/\/ Don't use rfp for 'jump_target' because a MachEpilogNode has already been\n+\/\/ emitted just above the TailCall which has reset rfp to the caller state.\n+instruct TailCalljmpInd(iRegPNoSpNoRfp jump_target, inline_cache_RegP method_ptr)\n@@ -16416,1 +16358,1 @@\n-instruct TailjmpInd(iRegPNoSp jump_target, iRegP_R0 ex_oop)\n+instruct TailjmpInd(iRegPNoSpNoRfp jump_target, iRegP_R0 ex_oop)\n@@ -16513,0 +16455,33 @@\n+instruct partialSubtypeCheckConstSuper(iRegP_R4 sub, iRegP_R0 super_reg, immP super_con, vRegD_V0 vtemp, iRegP_R5 result,\n+                                       iRegP_R1 tempR1, iRegP_R2 tempR2, iRegP_R3 tempR3,\n+                                       rFlagsReg cr)\n+%{\n+  match(Set result (PartialSubtypeCheck sub (Binary super_reg super_con)));\n+  predicate(UseSecondarySupersTable);\n+  effect(KILL cr, TEMP tempR1, TEMP tempR2, TEMP tempR3, TEMP vtemp);\n+\n+  ins_cost(700);  \/\/ smaller than the next version\n+  format %{ \"partialSubtypeCheck $result, $sub, $super_reg, $super_con\" %}\n+\n+  ins_encode %{\n+    bool success = false;\n+    u1 super_klass_slot = ((Klass*)$super_con$$constant)->hash_slot();\n+    if (InlineSecondarySupersTest) {\n+      success = __ lookup_secondary_supers_table($sub$$Register, $super_reg$$Register,\n+                                                 $tempR1$$Register, $tempR2$$Register, $tempR3$$Register,\n+                                                 $vtemp$$FloatRegister,\n+                                                 $result$$Register,\n+                                                 super_klass_slot);\n+    } else {\n+      address call = __ trampoline_call(RuntimeAddress(StubRoutines::lookup_secondary_supers_table_stub(super_klass_slot)));\n+      success = (call != nullptr);\n+    }\n+    if (!success) {\n+      ciEnv::current()->record_failure(\"CodeCache is full\");\n+      return;\n+    }\n+  %}\n+\n+  ins_pipe(pipe_class_memory);\n+%}\n+\n@@ -17106,1 +17081,1 @@\n-    loadStore(C2_MacroAssembler(&cbuf), &MacroAssembler::ldrs, $tsrc$$FloatRegister, $mem->opcode(),\n+    loadStore(masm, &MacroAssembler::ldrs, $tsrc$$FloatRegister, $mem->opcode(),\n@@ -17143,1 +17118,1 @@\n-    loadStore(C2_MacroAssembler(&cbuf), &MacroAssembler::ldrd, $tsrc$$FloatRegister, $mem->opcode(),\n+    loadStore(masm, &MacroAssembler::ldrd, $tsrc$$FloatRegister, $mem->opcode(),\n@@ -17180,1 +17155,1 @@\n-    loadStore(C2_MacroAssembler(&cbuf), &MacroAssembler::ldrs, $tsrc$$FloatRegister, $mem->opcode(),\n+    loadStore(masm, &MacroAssembler::ldrs, $tsrc$$FloatRegister, $mem->opcode(),\n@@ -17218,1 +17193,1 @@\n-    loadStore(C2_MacroAssembler(&cbuf), &MacroAssembler::ldrd, $tsrc$$FloatRegister, $mem->opcode(),\n+    loadStore(masm, &MacroAssembler::ldrd, $tsrc$$FloatRegister, $mem->opcode(),\n","filename":"src\/hotspot\/cpu\/aarch64\/aarch64.ad","additions":138,"deletions":163,"binary":false,"changes":301,"status":"modified"},{"patch":"@@ -97,1 +97,1 @@\n-  static void loadStoreA_predicated(C2_MacroAssembler masm, bool is_store, FloatRegister reg,\n+  static void loadStoreA_predicated(C2_MacroAssembler* masm, bool is_store, FloatRegister reg,\n@@ -122,1 +122,1 @@\n-      (masm.*insn)(reg, Assembler::elemType_to_regVariant(vector_elem_bt), pg, Address(base, imm4));\n+      (masm->*insn)(reg, Assembler::elemType_to_regVariant(vector_elem_bt), pg, Address(base, imm4));\n@@ -172,2 +172,0 @@\n-      case Op_LoadVectorGather:\n-      case Op_LoadVectorGatherMasked:\n@@ -183,0 +181,6 @@\n+      case Op_LoadVectorGather:\n+      case Op_LoadVectorGatherMasked:\n+        if (UseSVE == 0 || is_subword_type(bt)) {\n+          return false;\n+        }\n+        break;\n@@ -442,1 +446,1 @@\n-    loadStoreA_predicated(C2_MacroAssembler(&cbuf), \/* is_store *\/ false,\n+    loadStoreA_predicated(masm, \/* is_store *\/ false,\n@@ -459,1 +463,1 @@\n-    loadStoreA_predicated(C2_MacroAssembler(&cbuf), \/* is_store *\/ true,\n+    loadStoreA_predicated(masm, \/* is_store *\/ true,\n@@ -474,1 +478,1 @@\n-    loadStoreA_predicated(C2_MacroAssembler(&cbuf), \/* is_store *\/ false, $dst$$FloatRegister,\n+    loadStoreA_predicated(masm, \/* is_store *\/ false, $dst$$FloatRegister,\n@@ -487,1 +491,1 @@\n-    loadStoreA_predicated(C2_MacroAssembler(&cbuf), \/* is_store *\/ true, $src$$FloatRegister,\n+    loadStoreA_predicated(masm, \/* is_store *\/ true, $src$$FloatRegister,\n@@ -5209,1 +5213,1 @@\n-    loadStoreA_predicated(C2_MacroAssembler(&cbuf), false, $tmp$$FloatRegister,\n+    loadStoreA_predicated(masm, false, $tmp$$FloatRegister,\n@@ -5230,1 +5234,1 @@\n-    loadStoreA_predicated(C2_MacroAssembler(&cbuf), false, $tmp$$FloatRegister,\n+    loadStoreA_predicated(masm, false, $tmp$$FloatRegister,\n@@ -5257,1 +5261,1 @@\n-    loadStoreA_predicated(C2_MacroAssembler(&cbuf), false, $tmp$$FloatRegister,\n+    loadStoreA_predicated(masm, false, $tmp$$FloatRegister,\n@@ -5285,1 +5289,1 @@\n-    loadStoreA_predicated(C2_MacroAssembler(&cbuf), false, $tmp$$FloatRegister,\n+    loadStoreA_predicated(masm, false, $tmp$$FloatRegister,\n@@ -5310,1 +5314,1 @@\n-    loadStoreA_predicated(C2_MacroAssembler(&cbuf), true, $tmp$$FloatRegister,\n+    loadStoreA_predicated(masm, true, $tmp$$FloatRegister,\n@@ -5332,1 +5336,1 @@\n-    loadStoreA_predicated(C2_MacroAssembler(&cbuf), true, $tmp$$FloatRegister,\n+    loadStoreA_predicated(masm, true, $tmp$$FloatRegister,\n@@ -5358,1 +5362,1 @@\n-    loadStoreA_predicated(C2_MacroAssembler(&cbuf), true, $tmp$$FloatRegister,\n+    loadStoreA_predicated(masm, true, $tmp$$FloatRegister,\n@@ -5385,1 +5389,1 @@\n-    loadStoreA_predicated(C2_MacroAssembler(&cbuf), true, $tmp$$FloatRegister,\n+    loadStoreA_predicated(masm, true, $tmp$$FloatRegister,\n","filename":"src\/hotspot\/cpu\/aarch64\/aarch64_vector.ad","additions":20,"deletions":16,"binary":false,"changes":36,"status":"modified"},{"patch":"@@ -87,1 +87,1 @@\n-  static void loadStoreA_predicated(C2_MacroAssembler masm, bool is_store, FloatRegister reg,\n+  static void loadStoreA_predicated(C2_MacroAssembler* masm, bool is_store, FloatRegister reg,\n@@ -112,1 +112,1 @@\n-      (masm.*insn)(reg, Assembler::elemType_to_regVariant(vector_elem_bt), pg, Address(base, imm4));\n+      (masm->*insn)(reg, Assembler::elemType_to_regVariant(vector_elem_bt), pg, Address(base, imm4));\n@@ -162,2 +162,0 @@\n-      case Op_LoadVectorGather:\n-      case Op_LoadVectorGatherMasked:\n@@ -173,0 +171,6 @@\n+      case Op_LoadVectorGather:\n+      case Op_LoadVectorGatherMasked:\n+        if (UseSVE == 0 || is_subword_type(bt)) {\n+          return false;\n+        }\n+        break;\n@@ -381,1 +385,1 @@\n-    loadStoreA_predicated(C2_MacroAssembler(&cbuf), \/* is_store *\/ false,\n+    loadStoreA_predicated(masm, \/* is_store *\/ false,\n@@ -398,1 +402,1 @@\n-    loadStoreA_predicated(C2_MacroAssembler(&cbuf), \/* is_store *\/ true,\n+    loadStoreA_predicated(masm, \/* is_store *\/ true,\n@@ -413,1 +417,1 @@\n-    loadStoreA_predicated(C2_MacroAssembler(&cbuf), \/* is_store *\/ false, $dst$$FloatRegister,\n+    loadStoreA_predicated(masm, \/* is_store *\/ false, $dst$$FloatRegister,\n@@ -426,1 +430,1 @@\n-    loadStoreA_predicated(C2_MacroAssembler(&cbuf), \/* is_store *\/ true, $src$$FloatRegister,\n+    loadStoreA_predicated(masm, \/* is_store *\/ true, $src$$FloatRegister,\n@@ -3408,1 +3412,1 @@\n-    loadStoreA_predicated(C2_MacroAssembler(&cbuf), false, $tmp$$FloatRegister,\n+    loadStoreA_predicated(masm, false, $tmp$$FloatRegister,\n@@ -3429,1 +3433,1 @@\n-    loadStoreA_predicated(C2_MacroAssembler(&cbuf), false, $tmp$$FloatRegister,\n+    loadStoreA_predicated(masm, false, $tmp$$FloatRegister,\n@@ -3456,1 +3460,1 @@\n-    loadStoreA_predicated(C2_MacroAssembler(&cbuf), false, $tmp$$FloatRegister,\n+    loadStoreA_predicated(masm, false, $tmp$$FloatRegister,\n@@ -3484,1 +3488,1 @@\n-    loadStoreA_predicated(C2_MacroAssembler(&cbuf), false, $tmp$$FloatRegister,\n+    loadStoreA_predicated(masm, false, $tmp$$FloatRegister,\n@@ -3509,1 +3513,1 @@\n-    loadStoreA_predicated(C2_MacroAssembler(&cbuf), true, $tmp$$FloatRegister,\n+    loadStoreA_predicated(masm, true, $tmp$$FloatRegister,\n@@ -3531,1 +3535,1 @@\n-    loadStoreA_predicated(C2_MacroAssembler(&cbuf), true, $tmp$$FloatRegister,\n+    loadStoreA_predicated(masm, true, $tmp$$FloatRegister,\n@@ -3557,1 +3561,1 @@\n-    loadStoreA_predicated(C2_MacroAssembler(&cbuf), true, $tmp$$FloatRegister,\n+    loadStoreA_predicated(masm, true, $tmp$$FloatRegister,\n@@ -3584,1 +3588,1 @@\n-    loadStoreA_predicated(C2_MacroAssembler(&cbuf), true, $tmp$$FloatRegister,\n+    loadStoreA_predicated(masm, true, $tmp$$FloatRegister,\n","filename":"src\/hotspot\/cpu\/aarch64\/aarch64_vector_ad.m4","additions":20,"deletions":16,"binary":false,"changes":36,"status":"modified"},{"patch":"@@ -2616,0 +2616,1 @@\n+  \/\/ Advanced SIMD across lanes\n","filename":"src\/hotspot\/cpu\/aarch64\/assembler_aarch64.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -178,2 +178,4 @@\n-      LIR_Opr tmp = new_pointer_register();\n-      __ shift_left(index, shift, tmp);\n+      \/\/ Use long register to avoid overflow when shifting large index values left.\n+      LIR_Opr tmp = new_register(T_LONG);\n+      __ convert(Bytecodes::_i2l, index, tmp);\n+      __ shift_left(tmp, shift, tmp);\n@@ -892,1 +894,7 @@\n-  CodeEmitInfo* info = state_for(x, x->state());\n+  CodeEmitInfo* info = nullptr;\n+  if (x->state_before() != nullptr && x->state_before()->force_reexecute()) {\n+    info = state_for(x, x->state_before());\n+    info->set_force_reexecute();\n+  } else {\n+    info = state_for(x, x->state());\n+  }\n@@ -925,0 +933,3 @@\n+  if (x->check_flag(Instruction::OmitChecksFlag)) {\n+    flags = 0;\n+  }\n@@ -1147,1 +1158,7 @@\n-  CodeEmitInfo* info = state_for(x, x->state());\n+  CodeEmitInfo* info = nullptr;\n+  if (x->state_before() != nullptr && x->state_before()->force_reexecute()) {\n+    info = state_for(x, x->state_before());\n+    info->set_force_reexecute();\n+  } else {\n+    info = state_for(x, x->state());\n+  }\n@@ -1164,1 +1181,1 @@\n-  __ allocate_array(reg, len, tmp1, tmp2, tmp3, tmp4, elem_type, klass_reg, slow_path, false);\n+  __ allocate_array(reg, len, tmp1, tmp2, tmp3, tmp4, elem_type, klass_reg, slow_path, x->zero_array());\n@@ -1199,1 +1216,1 @@\n-  __ allocate_array(reg, len, tmp1, tmp2, tmp3, tmp4, T_OBJECT, klass_reg, slow_path, is_null_free);\n+  __ allocate_array(reg, len, tmp1, tmp2, tmp3, tmp4, T_OBJECT, klass_reg, slow_path, true, x->is_null_free());\n","filename":"src\/hotspot\/cpu\/aarch64\/c1_LIRGenerator_aarch64.cpp","additions":23,"deletions":6,"binary":false,"changes":29,"status":"modified"},{"patch":"@@ -176,0 +176,2 @@\n+  constexpr static bool supports_secondary_supers_table() { return true; }\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/vm_version_aarch64.hpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -79,1 +79,2 @@\n-  0,  0,  0    \/\/ EVEX_NTUP\n+  1,   1,  1,  \/\/ EVEX_NOSCALE(0)\n+  0,  0,  0    \/\/ EVEX_ETUP\n@@ -297,5 +298,1 @@\n-  int enc = r->encoding();\n-  if (enc >= 8) {\n-    enc -= 8;\n-  }\n-  return enc;\n+  return r->encoding() & 7;\n@@ -441,0 +438,3 @@\n+    case EVEX_NOSCALE:\n+      break;\n+\n@@ -528,0 +528,3 @@\n+    case EVEX_NOSCALE:\n+      break;\n+\n@@ -549,0 +552,18 @@\n+bool Assembler::needs_rex2(Register reg1, Register reg2, Register reg3) {\n+  bool rex2 = (reg1->is_valid() && reg1->encoding() >= 16) ||\n+              (reg2->is_valid() && reg2->encoding() >= 16) ||\n+              (reg3->is_valid() && reg3->encoding() >= 16);\n+  assert(!rex2 || UseAPX, \"extended gpr use requires UseAPX\");\n+  return rex2;\n+}\n+\n+bool Assembler::needs_eevex(Register reg1, Register reg2, Register reg3) {\n+  return needs_rex2(reg1, reg2, reg3);\n+}\n+\n+bool Assembler::needs_eevex(int enc1, int enc2, int enc3) {\n+  bool eevex = enc1 >= 16 || enc2 >= 16 || enc3 >=16;\n+  assert(!eevex || UseAPX, \"extended gpr use requires UseAPX\");\n+  return eevex;\n+}\n+\n@@ -619,2 +640,1 @@\n-      if (disp == 0 && no_relocation &&\n-          base_enc != rbp->encoding() LP64_ONLY(&& base_enc != r13->encoding())) {\n+      if (disp == 0 && no_relocation && ((base_enc & 0x7) != 5)) {\n@@ -622,0 +642,1 @@\n+        \/\/ !(rbp | r13 | r21 | r29)\n@@ -638,1 +659,2 @@\n-    } else if (base_enc == rsp->encoding() LP64_ONLY(|| base_enc == r12->encoding())) {\n+    } else if ((base_enc & 0x7) == 4) {\n+      \/\/ rsp | r12 | r20 | r28\n@@ -660,3 +682,3 @@\n-      assert(base_enc != rsp->encoding() LP64_ONLY(&& base_enc != r12->encoding()), \"illegal addressing mode\");\n-      if (disp == 0 && no_relocation &&\n-          base_enc != rbp->encoding() LP64_ONLY(&& base_enc != r13->encoding())) {\n+      \/\/ !(rsp | r12 | r20 | r28) were handled above\n+      assert(((base_enc & 0x7) != 4), \"illegal addressing mode\");\n+      if (disp == 0 && no_relocation &&  ((base_enc & 0x7) != 5)) {\n@@ -664,0 +686,1 @@\n+        \/\/ !(rbp | r13 | r21 | r29)\n@@ -820,0 +843,7 @@\n+  case REX2:\n+    NOT_LP64(assert(false, \"64bit prefixes\"));\n+    if ((0xFF & *ip++) & REXBIT_W) {\n+      is_64bit = true;\n+    }\n+    goto again_after_prefix;\n+\n@@ -869,0 +899,8 @@\n+\n+    case REX2:\n+      NOT_LP64(assert(false, \"64bit prefix found\"));\n+      if ((0xFF & *ip++) & REXBIT_W) {\n+        is_64bit = true;\n+      }\n+      goto again_after_size_prefix2;\n+\n@@ -937,0 +975,1 @@\n+    case 0xD6: \/\/ movq\n@@ -1157,0 +1196,1 @@\n+    case REX2:\n@@ -1283,0 +1323,27 @@\n+void Assembler::emit_opcode_prefix_and_encoding(int byte1, int byte2, int ocp_and_encoding, int byte3) {\n+  int opcode_prefix = (ocp_and_encoding & 0xFF00) >> 8;\n+  if (opcode_prefix != 0) {\n+    emit_int32(opcode_prefix, (unsigned char)byte1, byte2 | (ocp_and_encoding & 0xFF), byte3);\n+  } else {\n+    emit_int24((unsigned char)byte1, byte2 | (ocp_and_encoding & 0xFF), byte3);\n+  }\n+}\n+\n+void Assembler::emit_opcode_prefix_and_encoding(int byte1, int byte2, int ocp_and_encoding) {\n+  int opcode_prefix = (ocp_and_encoding & 0xFF00) >> 8;\n+  if (opcode_prefix != 0) {\n+    emit_int24(opcode_prefix, (unsigned char)byte1, byte2 | (ocp_and_encoding & 0xFF));\n+  } else {\n+    emit_int16((unsigned char)byte1, byte2 | (ocp_and_encoding & 0xFF));\n+  }\n+}\n+\n+void Assembler::emit_opcode_prefix_and_encoding(int byte1, int ocp_and_encoding) {\n+  int opcode_prefix = (ocp_and_encoding & 0xFF00) >> 8;\n+  if (opcode_prefix != 0) {\n+    emit_int16(opcode_prefix, (unsigned char)byte1 | (ocp_and_encoding & 0xFF));\n+  } else {\n+    emit_int8((unsigned char)byte1 | (ocp_and_encoding & 0xFF));\n+  }\n+}\n+\n@@ -1605,2 +1672,2 @@\n-  InstructionAttr attributes(AVX_128bit, \/* rex_w *\/ false, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n-  int encode = vex_prefix_and_encode(dst->encoding(), src1->encoding(), src2->encoding(), VEX_SIMD_NONE, VEX_OPCODE_0F_38, &attributes);\n+  InstructionAttr attributes(AVX_128bit, \/* rex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  int encode = vex_prefix_and_encode(dst->encoding(), src1->encoding(), src2->encoding(), VEX_SIMD_NONE, VEX_OPCODE_0F_38, &attributes, true);\n@@ -1613,1 +1680,2 @@\n-  InstructionAttr attributes(AVX_128bit, \/* rex_w *\/ false, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  InstructionAttr attributes(AVX_128bit, \/* rex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_NOSCALE, \/* input_size_in_bits *\/ EVEX_32bit);\n@@ -1620,4 +1688,2 @@\n-  int encode = prefix_and_encode(dst->encoding(), src->encoding());\n-  emit_int24(0x0F,\n-             (unsigned char)0xBC,\n-             0xC0 | encode);\n+  int encode = prefix_and_encode(dst->encoding(), src->encoding(), true \/* is_map1 *\/);\n+  emit_opcode_prefix_and_encoding((unsigned char)0xBC, 0xC0, encode);\n@@ -1627,4 +1693,2 @@\n-  int encode = prefix_and_encode(dst->encoding(), src->encoding());\n-  emit_int24(0x0F,\n-             (unsigned char)0xBD,\n-             0xC0 | encode);\n+  int encode = prefix_and_encode(dst->encoding(), src->encoding(), true \/* is_map1 *\/);\n+  emit_opcode_prefix_and_encoding((unsigned char)0xBD, 0xC0, encode);\n@@ -1634,2 +1698,2 @@\n-  int encode = prefix_and_encode(reg->encoding());\n-  emit_int16(0x0F, (0xC8 | encode));\n+  int encode = prefix_and_encode(reg->encoding(), false, true \/* is_map1 *\/);\n+  emit_opcode_prefix_and_encoding((unsigned char)0xC8, encode);\n@@ -1640,2 +1704,2 @@\n-  InstructionAttr attributes(AVX_128bit, \/* vex_w *\/ false, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n-  int encode = vex_prefix_and_encode(rbx->encoding(), dst->encoding(), src->encoding(), VEX_SIMD_NONE, VEX_OPCODE_0F_38, &attributes);\n+  InstructionAttr attributes(AVX_128bit, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  int encode = vex_prefix_and_encode(rbx->encoding(), dst->encoding(), src->encoding(), VEX_SIMD_NONE, VEX_OPCODE_0F_38, &attributes, true);\n@@ -1648,1 +1712,2 @@\n-  InstructionAttr attributes(AVX_128bit, \/* vex_w *\/ false, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  InstructionAttr attributes(AVX_128bit, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_NOSCALE, \/* input_size_in_bits *\/ EVEX_32bit);\n@@ -1656,2 +1721,2 @@\n-  InstructionAttr attributes(AVX_128bit, \/* vex_w *\/ false, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n-  int encode = vex_prefix_and_encode(rdx->encoding(), dst->encoding(), src->encoding(), VEX_SIMD_NONE, VEX_OPCODE_0F_38, &attributes);\n+  InstructionAttr attributes(AVX_128bit, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  int encode = vex_prefix_and_encode(rdx->encoding(), dst->encoding(), src->encoding(), VEX_SIMD_NONE, VEX_OPCODE_0F_38, &attributes, true);\n@@ -1665,1 +1730,2 @@\n-  InstructionAttr attributes(AVX_128bit, \/* vex_w *\/ false, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  InstructionAttr attributes(AVX_128bit, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_NOSCALE, \/* input_size_in_bits *\/ EVEX_32bit);\n@@ -1673,2 +1739,2 @@\n-  InstructionAttr attributes(AVX_128bit, \/* vex_w *\/ false, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n-  int encode = vex_prefix_and_encode(rcx->encoding(), dst->encoding(), src->encoding(), VEX_SIMD_NONE, VEX_OPCODE_0F_38, &attributes);\n+  InstructionAttr attributes(AVX_128bit, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  int encode = vex_prefix_and_encode(rcx->encoding(), dst->encoding(), src->encoding(), VEX_SIMD_NONE, VEX_OPCODE_0F_38, &attributes, true);\n@@ -1681,1 +1747,2 @@\n-  InstructionAttr attributes(AVX_128bit, \/* vex_w *\/ false, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  InstructionAttr attributes(AVX_128bit, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_NOSCALE, \/* input_size_in_bits *\/ EVEX_32bit);\n@@ -1745,4 +1812,2 @@\n-  int encode = prefix_and_encode(dst->encoding(), src->encoding());\n-  emit_int24(0x0F,\n-             0x40 | cc,\n-             0xC0 | encode);\n+  int encode = prefix_and_encode(dst->encoding(), src->encoding(), true \/* is_map1 *\/);\n+  emit_opcode_prefix_and_encoding(0x40 | cc, 0xC0, encode);\n@@ -1751,1 +1816,0 @@\n-\n@@ -1755,2 +1819,2 @@\n-  prefix(src, dst);\n-  emit_int16(0x0F, (0x40 | cc));\n+  prefix(src, dst, false, true \/* is_map1 *\/);\n+  emit_int8((0x40 | cc));\n@@ -1760,0 +1824,16 @@\n+void Assembler::cmpb(Address dst, Register reg) {\n+  assert(reg->has_byte_register(), \"must have byte register\");\n+  InstructionMark im(this);\n+  prefix(dst, reg, true);\n+  emit_int8((unsigned char)0x38);\n+  emit_operand(reg, dst, 0);\n+}\n+\n+void Assembler::cmpb(Register reg, Address dst) {\n+  assert(reg->has_byte_register(), \"must have byte register\");\n+  InstructionMark im(this);\n+  prefix(dst, reg, true);\n+  emit_int8((unsigned char)0x3a);\n+  emit_operand(reg, dst, 0);\n+}\n+\n@@ -1791,0 +1871,7 @@\n+void Assembler::cmpl(Address dst,  Register reg) {\n+  InstructionMark im(this);\n+  prefix(dst, reg);\n+  emit_int8(0x39);\n+  emit_operand(reg, dst, 0);\n+}\n+\n@@ -1806,0 +1893,8 @@\n+void Assembler::cmpw(Address dst, Register reg) {\n+  InstructionMark im(this);\n+  emit_int8(0x66);\n+  prefix(dst, reg);\n+  emit_int8((unsigned char)0x39);\n+  emit_operand(reg, dst, 0);\n+}\n+\n@@ -1811,2 +1906,2 @@\n-  prefix(adr, reg);\n-  emit_int16(0x0F, (unsigned char)0xB1);\n+  prefix(adr, reg, false, true \/* is_map1 *\/);\n+  emit_int8((unsigned char)0xB1);\n@@ -1819,2 +1914,2 @@\n-  prefix(adr, reg);\n-  emit_int16(0x0F, (unsigned char)0xB1);\n+  prefix(adr, reg, false, true \/* is_map1 *\/);\n+  emit_int8((unsigned char)0xB1);\n@@ -1829,2 +1924,2 @@\n-  prefix(adr, reg, true);\n-  emit_int16(0x0F, (unsigned char)0xB0);\n+  prefix(adr, reg, true, true \/* is_map1 *\/);\n+  emit_int8((unsigned char)0xB0);\n@@ -1888,2 +1983,7 @@\n-  int8_t w = 0x01;\n-  Prefix p = Prefix_EMPTY;\n+  if (needs_eevex(crc, v)) {\n+    InstructionAttr attributes(AVX_128bit, \/* rex_w *\/ sizeInBytes == 8, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+    int encode = vex_prefix_and_encode(crc->encoding(), 0, v->encoding(), sizeInBytes == 2 ? VEX_SIMD_66 : VEX_SIMD_NONE, VEX_OPCODE_0F_3C, &attributes, true);\n+    emit_int16(sizeInBytes == 1 ? (unsigned char)0xF0 : (unsigned char)0xF1, (0xC0 | encode));\n+  } else {\n+    int8_t w = 0x01;\n+    Prefix p = Prefix_EMPTY;\n@@ -1891,27 +1991,33 @@\n-  emit_int8((unsigned char)0xF2);\n-  switch (sizeInBytes) {\n-  case 1:\n-    w = 0;\n-    break;\n-  case 2:\n-  case 4:\n-    break;\n-  LP64_ONLY(case 8:)\n-    \/\/ This instruction is not valid in 32 bits\n-    \/\/ Note:\n-    \/\/ http:\/\/www.intel.com\/content\/dam\/www\/public\/us\/en\/documents\/manuals\/64-ia-32-architectures-software-developer-instruction-set-reference-manual-325383.pdf\n-    \/\/\n-    \/\/ Page B - 72   Vol. 2C says\n-    \/\/ qwreg2 to qwreg            1111 0010 : 0100 1R0B : 0000 1111 : 0011 1000 : 1111 0000 : 11 qwreg1 qwreg2\n-    \/\/ mem64 to qwreg             1111 0010 : 0100 1R0B : 0000 1111 : 0011 1000 : 1111 0000 : mod qwreg r \/ m\n-    \/\/                                                                            F0!!!\n-    \/\/ while 3 - 208 Vol. 2A\n-    \/\/ F2 REX.W 0F 38 F1 \/ r       CRC32 r64, r \/ m64             RM         Valid      N.E.Accumulate CRC32 on r \/ m64.\n-    \/\/\n-    \/\/ the 0 on a last bit is reserved for a different flavor of this instruction :\n-    \/\/ F2 REX.W 0F 38 F0 \/ r       CRC32 r64, r \/ m8              RM         Valid      N.E.Accumulate CRC32 on r \/ m8.\n-    p = REX_W;\n-    break;\n-  default:\n-    assert(0, \"Unsupported value for a sizeInBytes argument\");\n-    break;\n+    emit_int8((unsigned char)0xF2);\n+    switch (sizeInBytes) {\n+    case 1:\n+      w = 0;\n+      break;\n+    case 2:\n+    case 4:\n+      break;\n+    LP64_ONLY(case 8:)\n+      \/\/ This instruction is not valid in 32 bits\n+      \/\/ Note:\n+      \/\/ http:\/\/www.intel.com\/content\/dam\/www\/public\/us\/en\/documents\/manuals\/64-ia-32-architectures-software-developer-instruction-set-reference-manual-325383.pdf\n+      \/\/\n+      \/\/ Page B - 72   Vol. 2C says\n+      \/\/ qwreg2 to qwreg            1111 0010 : 0100 1R0B : 0000 1111 : 0011 1000 : 1111 0000 : 11 qwreg1 qwreg2\n+      \/\/ mem64 to qwreg             1111 0010 : 0100 1R0B : 0000 1111 : 0011 1000 : 1111 0000 : mod qwreg r \/ m\n+      \/\/                                                                            F0!!!\n+      \/\/ while 3 - 208 Vol. 2A\n+      \/\/ F2 REX.W 0F 38 F1 \/ r       CRC32 r64, r \/ m64             RM         Valid      N.E.Accumulate CRC32 on r \/ m64.\n+      \/\/\n+      \/\/ the 0 on a last bit is reserved for a different flavor of this instruction :\n+      \/\/ F2 REX.W 0F 38 F0 \/ r       CRC32 r64, r \/ m8              RM         Valid      N.E.Accumulate CRC32 on r \/ m8.\n+      p = REX_W;\n+      break;\n+    default:\n+      assert(0, \"Unsupported value for a sizeInBytes argument\");\n+      break;\n+    }\n+    LP64_ONLY(prefix(crc, v, p);)\n+    emit_int32(0x0F,\n+               0x38,\n+               0xF0 | w,\n+               0xC0 | ((crc->encoding() & 0x7) << 3) | (v->encoding() & 7));\n@@ -1919,5 +2025,0 @@\n-  LP64_ONLY(prefix(crc, v, p);)\n-  emit_int32(0x0F,\n-             0x38,\n-             0xF0 | w,\n-             0xC0 | ((crc->encoding() & 0x7) << 3) | (v->encoding() & 7));\n@@ -1929,2 +2030,9 @@\n-  int8_t w = 0x01;\n-  Prefix p = Prefix_EMPTY;\n+  if (needs_eevex(crc, adr.base(), adr.index())) {\n+    InstructionAttr attributes(AVX_128bit, \/* vex_w *\/ sizeInBytes == 8, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+    attributes.set_address_attributes(\/* tuple_type *\/ EVEX_NOSCALE, \/* input_size_in_bits *\/ EVEX_64bit);\n+    vex_prefix(adr, 0, crc->encoding(), sizeInBytes == 2 ? VEX_SIMD_66 : VEX_SIMD_NONE, VEX_OPCODE_0F_3C, &attributes);\n+    emit_int8(sizeInBytes == 1 ? (unsigned char)0xF0 : (unsigned char)0xF1);\n+    emit_operand(crc, adr, 0);\n+  } else {\n+    int8_t w = 0x01;\n+    Prefix p = Prefix_EMPTY;\n@@ -1932,15 +2040,19 @@\n-  emit_int8((uint8_t)0xF2);\n-  switch (sizeInBytes) {\n-  case 1:\n-    w = 0;\n-    break;\n-  case 2:\n-  case 4:\n-    break;\n-  LP64_ONLY(case 8:)\n-    \/\/ This instruction is not valid in 32 bits\n-    p = REX_W;\n-    break;\n-  default:\n-    assert(0, \"Unsupported value for a sizeInBytes argument\");\n-    break;\n+    emit_int8((uint8_t)0xF2);\n+    switch (sizeInBytes) {\n+    case 1:\n+      w = 0;\n+      break;\n+    case 2:\n+    case 4:\n+      break;\n+    LP64_ONLY(case 8:)\n+      \/\/ This instruction is not valid in 32 bits\n+      p = REX_W;\n+      break;\n+    default:\n+      assert(0, \"Unsupported value for a sizeInBytes argument\");\n+      break;\n+    }\n+    LP64_ONLY(prefix(crc, adr, p);)\n+    emit_int24(0x0F, 0x38, (0xF0 | w));\n+    emit_operand(crc, adr, 0);\n@@ -1948,3 +2060,0 @@\n-  LP64_ONLY(prefix(crc, adr, p);)\n-  emit_int24(0x0F, 0x38, (0xF0 | w));\n-  emit_operand(crc, adr, 0);\n@@ -2034,1 +2143,1 @@\n-  int encode = simd_prefix_and_encode(dst, dst, src, VEX_SIMD_F2, VEX_OPCODE_0F, &attributes);\n+  int encode = simd_prefix_and_encode(dst, src, src, VEX_SIMD_F2, VEX_OPCODE_0F, &attributes);\n@@ -2052,1 +2161,1 @@\n-  int encode = simd_prefix_and_encode(dst, dst, as_XMMRegister(src->encoding()), VEX_SIMD_F2, VEX_OPCODE_0F, &attributes);\n+  int encode = simd_prefix_and_encode(dst, dst, as_XMMRegister(src->encoding()), VEX_SIMD_F2, VEX_OPCODE_0F, &attributes, true);\n@@ -2069,1 +2178,1 @@\n-  int encode = simd_prefix_and_encode(dst, dst, as_XMMRegister(src->encoding()), VEX_SIMD_F3, VEX_OPCODE_0F, &attributes);\n+  int encode = simd_prefix_and_encode(dst, dst, as_XMMRegister(src->encoding()), VEX_SIMD_F3, VEX_OPCODE_0F, &attributes, true);\n@@ -2086,1 +2195,1 @@\n-  int encode = simd_prefix_and_encode(dst, dst, as_XMMRegister(src->encoding()), VEX_SIMD_F3, VEX_OPCODE_0F, &attributes);\n+  int encode = simd_prefix_and_encode(dst, dst, as_XMMRegister(src->encoding()), VEX_SIMD_F3, VEX_OPCODE_0F, &attributes, true);\n@@ -2093,1 +2202,1 @@\n-  int encode = simd_prefix_and_encode(dst, dst, src, VEX_SIMD_F3, VEX_OPCODE_0F, &attributes);\n+  int encode = simd_prefix_and_encode(dst, src, src, VEX_SIMD_F3, VEX_OPCODE_0F, &attributes);\n@@ -2389,4 +2498,2 @@\n-  int encode = prefix_and_encode(dst->encoding(), src->encoding());\n-  emit_int24(0x0F,\n-             (unsigned char)0xAF,\n-             (0xC0 | encode));\n+  int encode = prefix_and_encode(dst->encoding(), src->encoding(), true \/* is_map1 *\/);\n+  emit_opcode_prefix_and_encoding((unsigned char)0xAF, 0xC0, encode);\n@@ -2421,2 +2528,2 @@\n-  prefix(src, dst);\n-  emit_int16(0x0F, (unsigned char)0xAF);\n+  prefix(src, dst, false, true \/* is_map1 *\/);\n+  emit_int8((unsigned char)0xAF);\n@@ -2558,1 +2665,4 @@\n-  if (UseAVX > 0 ) {\n+  \/\/ This instruction should be SSE encoded with the REX2 prefix when an\n+  \/\/ extended GPR is present. To be consistent when UseAPX is enabled, use\n+  \/\/ this encoding even when an extended GPR is not used.\n+  if (UseAVX > 0 && !UseAPX ) {\n@@ -2567,2 +2677,2 @@\n-    prefix(src);\n-    emit_int16(0x0F, (unsigned char)0xAE);\n+    prefix(src, true \/* is_map1 *\/);\n+    emit_int8((unsigned char)0xAE);\n@@ -2595,2 +2705,2 @@\n-  int encode = prefix_and_encode(dst->encoding(), src->encoding());\n-  emit_int24(0x0F, (unsigned char)0xBD, (0xC0 | encode));\n+  int encode = prefix_and_encode(dst->encoding(), src->encoding(), true \/* is_map1 *\/);\n+  emit_opcode_prefix_and_encoding((unsigned char)0xBD, 0xC0, encode);\n@@ -2603,2 +2713,2 @@\n-  prefix(src, dst);\n-  emit_int16(0x0F, (unsigned char)0xBD);\n+  prefix(src, dst, false, true \/* is_map1 *\/);\n+  emit_int8((unsigned char)0xBD);\n@@ -2696,2 +2806,2 @@\n-  InstructionAttr attributes(AVX_128bit, \/* rex_w *\/ false, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n-  int encode = vex_prefix_and_encode(dst->encoding(), 0, src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  InstructionAttr attributes(AVX_128bit, \/* rex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  int encode = vex_prefix_and_encode(dst->encoding(), 0, src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes, true);\n@@ -2703,1 +2813,1 @@\n-  InstructionAttr attributes(AVX_128bit, \/* rex_w *\/ false, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  InstructionAttr attributes(AVX_128bit, \/* rex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n@@ -2710,2 +2820,2 @@\n-  InstructionAttr attributes(AVX_128bit, \/* rex_w *\/ false, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n-  int encode = vex_prefix_and_encode(dst->encoding(), 0, src->encoding(), VEX_SIMD_NONE, VEX_OPCODE_0F, &attributes);\n+  InstructionAttr attributes(AVX_128bit, \/* rex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  int encode = vex_prefix_and_encode(dst->encoding(), 0, src->encoding(), VEX_SIMD_NONE, VEX_OPCODE_0F, &attributes, true);\n@@ -2717,1 +2827,1 @@\n-  InstructionAttr attributes(AVX_128bit, \/* rex_w *\/ false, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  InstructionAttr attributes(AVX_128bit, \/* rex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n@@ -2725,1 +2835,2 @@\n-  InstructionAttr attributes(AVX_128bit, \/* vex_w *\/ false, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  InstructionAttr attributes(AVX_128bit, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_NOSCALE, \/* input_size_in_bits *\/ EVEX_32bit);\n@@ -2734,1 +2845,2 @@\n-  InstructionAttr attributes(AVX_128bit, \/* vex_w *\/ false, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  InstructionAttr attributes(AVX_128bit, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_NOSCALE, \/* input_size_in_bits *\/ EVEX_32bit);\n@@ -2749,2 +2861,2 @@\n-  InstructionAttr attributes(AVX_128bit, \/* rex_w *\/ false, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n-  int encode = vex_prefix_and_encode(dst->encoding(), 0, src->encoding(), VEX_SIMD_F2, VEX_OPCODE_0F, &attributes);\n+  InstructionAttr attributes(AVX_128bit, \/* rex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  int encode = vex_prefix_and_encode(dst->encoding(), 0, src->encoding(), VEX_SIMD_F2, VEX_OPCODE_0F, &attributes, true);\n@@ -2756,1 +2868,1 @@\n-  InstructionAttr attributes(AVX_128bit, \/* rex_w *\/ false, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  InstructionAttr attributes(AVX_128bit, \/* rex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n@@ -2771,1 +2883,2 @@\n-  InstructionAttr attributes(AVX_128bit, \/* vex_w *\/ true, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  InstructionAttr attributes(AVX_128bit, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_NOSCALE, \/* input_size_in_bits *\/ EVEX_32bit);\n@@ -2780,1 +2893,2 @@\n-  InstructionAttr attributes(AVX_128bit, \/* vex_w *\/ true, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  InstructionAttr attributes(AVX_128bit, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_NOSCALE, \/* input_size_in_bits *\/ EVEX_32bit);\n@@ -2788,2 +2902,2 @@\n-  InstructionAttr attributes(AVX_128bit, \/* rex_w *\/ true, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n-  int encode = vex_prefix_and_encode(dst->encoding(), 0, src->encoding(), VEX_SIMD_F2, VEX_OPCODE_0F, &attributes);\n+  InstructionAttr attributes(AVX_128bit, \/* rex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  int encode = vex_prefix_and_encode(dst->encoding(), 0, src->encoding(), VEX_SIMD_F2, VEX_OPCODE_0F, &attributes, true);\n@@ -2795,1 +2909,1 @@\n-  InstructionAttr attributes(AVX_128bit, \/* rex_w *\/ true, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  InstructionAttr attributes(AVX_128bit, \/* rex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n@@ -3076,1 +3190,1 @@\n-  int encode = simd_prefix_and_encode(dst, xnoreg, as_XMMRegister(src->encoding()), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  int encode = simd_prefix_and_encode(dst, xnoreg, as_XMMRegister(src->encoding()), VEX_SIMD_66, VEX_OPCODE_0F, &attributes, true);\n@@ -3084,1 +3198,1 @@\n-  int encode = simd_prefix_and_encode(src, xnoreg, as_XMMRegister(dst->encoding()), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  int encode = simd_prefix_and_encode(src, xnoreg, as_XMMRegister(dst->encoding()), VEX_SIMD_66, VEX_OPCODE_0F, &attributes, true);\n@@ -3569,1 +3683,1 @@\n-  int encode = simd_prefix_and_encode(src, xnoreg, as_XMMRegister(dst->encoding()), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  int encode = simd_prefix_and_encode(src, xnoreg, as_XMMRegister(dst->encoding()), VEX_SIMD_66, VEX_OPCODE_0F, &attributes, true);\n@@ -3576,1 +3690,1 @@\n-  int encode = simd_prefix_and_encode(dst, xnoreg, as_XMMRegister(src->encoding()), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  int encode = simd_prefix_and_encode(dst, xnoreg, as_XMMRegister(src->encoding()), VEX_SIMD_66, VEX_OPCODE_0F, &attributes, true);\n@@ -3582,2 +3696,2 @@\n-  prefix(src, dst);\n-  emit_int16(0x0F, (unsigned char)0xBE);\n+  prefix(src, dst, false, true \/* is_map1 *\/);\n+  emit_int8((unsigned char)0xBE);\n@@ -3589,2 +3703,2 @@\n-  int encode = prefix_and_encode(dst->encoding(), false, src->encoding(), true);\n-  emit_int24(0x0F, (unsigned char)0xBE, (0xC0 | encode));\n+  int encode = prefix_and_encode(dst->encoding(), false, src->encoding(), true, true \/* is_map1 *\/);\n+  emit_opcode_prefix_and_encoding((unsigned char)0xBE, 0xC0, encode);\n@@ -3662,2 +3776,2 @@\n-  prefix(src, dst);\n-  emit_int16(0x0F, (unsigned char)0xBF);\n+  prefix(src, dst, false, true \/* is_map1 *\/);\n+  emit_int8((unsigned char)0xBF);\n@@ -3668,2 +3782,2 @@\n-  int encode = prefix_and_encode(dst->encoding(), src->encoding());\n-  emit_int24(0x0F, (unsigned char)0xBF, (0xC0 | encode));\n+  int encode = prefix_and_encode(dst->encoding(), src->encoding(), true \/* is_map1 *\/);\n+  emit_opcode_prefix_and_encoding((unsigned char)0xBF, 0xC0, encode);\n@@ -3740,2 +3854,2 @@\n-  prefix(src, dst);\n-  emit_int16(0x0F, (unsigned char)0xB6);\n+  prefix(src, dst, false, true \/* is_map1 *\/);\n+  emit_int8((unsigned char)0xB6);\n@@ -3747,2 +3861,2 @@\n-  int encode = prefix_and_encode(dst->encoding(), false, src->encoding(), true);\n-  emit_int24(0x0F, (unsigned char)0xB6, 0xC0 | encode);\n+  int encode = prefix_and_encode(dst->encoding(), false, src->encoding(), true, true \/* is_map1 *\/);\n+  emit_opcode_prefix_and_encoding((unsigned char)0xB6, 0xC0, encode);\n@@ -3753,2 +3867,2 @@\n-  prefix(src, dst);\n-  emit_int16(0x0F, (unsigned char)0xB7);\n+  prefix(src, dst, false, true \/* is_map1 *\/);\n+  emit_int8((unsigned char)0xB7);\n@@ -3759,2 +3873,2 @@\n-  int encode = prefix_and_encode(dst->encoding(), src->encoding());\n-  emit_int24(0x0F, (unsigned char)0xB7, 0xC0 | encode);\n+  int encode = prefix_and_encode(dst->encoding(), src->encoding(), true \/* is_map1 *\/);\n+  emit_opcode_prefix_and_encoding((unsigned char)0xB7, 0xC0, encode);\n@@ -3823,1 +3937,1 @@\n-void Assembler::nop(int i) {\n+void Assembler::nop(uint i) {\n@@ -4279,0 +4393,1 @@\n+  InstructionMark im(this);\n@@ -4402,0 +4517,1 @@\n+  assert(!needs_eevex(src.base(), src.index()), \"does not support extended gprs\");\n@@ -4442,0 +4558,10 @@\n+void Assembler::vpcmpeqb(XMMRegister dst, XMMRegister src1, Address src2, int vector_len) {\n+  assert(vector_len == AVX_128bit ? VM_Version::supports_avx() : VM_Version::supports_avx2(), \"\");\n+  assert(vector_len <= AVX_256bit, \"evex encoding is different - has k register as dest\");\n+  InstructionMark im(this);\n+  InstructionAttr attributes(vector_len, \/* rex_w *\/ false, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  vex_prefix(src2, src1->encoding(), dst->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int8(0x74);\n+  emit_operand(dst, src2, 0);\n+}\n+\n@@ -4548,0 +4674,11 @@\n+\/\/ In this context, the dst vector contains the components that are equal, non equal components are zeroed in dst\n+void Assembler::vpcmpeqw(XMMRegister dst, XMMRegister nds, Address src, int vector_len) {\n+  assert(vector_len == AVX_128bit ? VM_Version::supports_avx() : VM_Version::supports_avx2(), \"\");\n+  assert(vector_len <= AVX_256bit, \"evex encoding is different - has k register as dest\");\n+  InstructionMark im(this);\n+  InstructionAttr attributes(vector_len, \/* rex_w *\/ false, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  vex_prefix(src, nds->encoding(), dst->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int8(0x75);\n+  emit_operand(dst, src, 0);\n+}\n+\n@@ -4708,1 +4845,1 @@\n-  int encode = simd_prefix_and_encode(src, xnoreg, as_XMMRegister(dst->encoding()), VEX_SIMD_66, VEX_OPCODE_0F_3A, &attributes);\n+  int encode = simd_prefix_and_encode(src, xnoreg, as_XMMRegister(dst->encoding()), VEX_SIMD_66, VEX_OPCODE_0F_3A, &attributes, true);\n@@ -4714,0 +4851,1 @@\n+  InstructionMark im(this);\n@@ -4725,1 +4863,1 @@\n-  int encode = simd_prefix_and_encode(src, xnoreg, as_XMMRegister(dst->encoding()), VEX_SIMD_66, VEX_OPCODE_0F_3A, &attributes);\n+  int encode = simd_prefix_and_encode(src, xnoreg, as_XMMRegister(dst->encoding()), VEX_SIMD_66, VEX_OPCODE_0F_3A, &attributes, true);\n@@ -4731,0 +4869,1 @@\n+  InstructionMark im(this);\n@@ -4748,0 +4887,1 @@\n+  InstructionMark im(this);\n@@ -4759,1 +4899,1 @@\n-  int encode = simd_prefix_and_encode(src, xnoreg, as_XMMRegister(dst->encoding()), VEX_SIMD_66, VEX_OPCODE_0F_3A, &attributes);\n+  int encode = simd_prefix_and_encode(src, xnoreg, as_XMMRegister(dst->encoding()), VEX_SIMD_66, VEX_OPCODE_0F_3A, &attributes, true);\n@@ -4765,0 +4905,1 @@\n+  InstructionMark im(this);\n@@ -4776,1 +4917,1 @@\n-  int encode = simd_prefix_and_encode(dst, dst, as_XMMRegister(src->encoding()), VEX_SIMD_66, VEX_OPCODE_0F_3A, &attributes);\n+  int encode = simd_prefix_and_encode(dst, dst, as_XMMRegister(src->encoding()), VEX_SIMD_66, VEX_OPCODE_0F_3A, &attributes, true);\n@@ -4782,0 +4923,1 @@\n+  InstructionMark im(this);\n@@ -4793,1 +4935,1 @@\n-  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_3A, &attributes);\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_3A, &attributes, true);\n@@ -4800,1 +4942,1 @@\n-  int encode = simd_prefix_and_encode(dst, dst, as_XMMRegister(src->encoding()), VEX_SIMD_66, VEX_OPCODE_0F_3A, &attributes);\n+  int encode = simd_prefix_and_encode(dst, dst, as_XMMRegister(src->encoding()), VEX_SIMD_66, VEX_OPCODE_0F_3A, &attributes, true);\n@@ -4806,0 +4948,1 @@\n+  InstructionMark im(this);\n@@ -4817,1 +4960,1 @@\n-  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_3A, &attributes);\n+  int encode = vex_prefix_and_encode(dst->encoding(),  nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_3A, &attributes, true);\n@@ -4824,1 +4967,1 @@\n-  int encode = simd_prefix_and_encode(dst, dst, as_XMMRegister(src->encoding()), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  int encode = simd_prefix_and_encode(dst, dst, as_XMMRegister(src->encoding()), VEX_SIMD_66, VEX_OPCODE_0F, &attributes, true);\n@@ -4830,0 +4973,1 @@\n+  InstructionMark im(this);\n@@ -4841,1 +4985,1 @@\n-  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes, true);\n@@ -4847,0 +4991,1 @@\n+  InstructionMark im(this);\n@@ -4858,1 +5003,1 @@\n-  int encode = simd_prefix_and_encode(dst, dst, as_XMMRegister(src->encoding()), VEX_SIMD_66, VEX_OPCODE_0F_3A, &attributes);\n+  int encode = simd_prefix_and_encode(dst, dst, as_XMMRegister(src->encoding()), VEX_SIMD_66, VEX_OPCODE_0F_3A, &attributes, true);\n@@ -4865,1 +5010,1 @@\n-  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_3A, &attributes);\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_3A, &attributes, true);\n@@ -5268,2 +5413,2 @@\n-  prefix(src, dst);\n-  emit_int16(0x0F, (unsigned char)0xB8);\n+  prefix(src, dst, false, true \/* is_map1 *\/);\n+  emit_int8((unsigned char)0xB8);\n@@ -5276,2 +5421,2 @@\n-  int encode = prefix_and_encode(dst->encoding(), src->encoding());\n-  emit_int24(0x0F, (unsigned char)0xB8, (0xC0 | encode));\n+  int encode = prefix_and_encode(dst->encoding(), src->encoding(), true \/* is_map1 *\/);\n+  emit_opcode_prefix_and_encoding((unsigned char)0xB8, 0xC0, encode);\n@@ -5349,2 +5494,2 @@\n-  prefix(src);\n-  emit_int16(0x0F, 0x18);\n+  prefix(src, true \/* is_map1 *\/);\n+  emit_int8(0x18);\n@@ -5357,2 +5502,2 @@\n-  prefix(src);\n-  emit_int16(0x0F, 0x0D);\n+  prefix(src, true \/* is_map1 *\/);\n+  emit_int8(0x0D);\n@@ -5365,2 +5510,2 @@\n-  prefix(src);\n-  emit_int16(0x0F, 0x18);\n+  prefix(src, true \/* is_map1 *\/);\n+  emit_int8(0x18);\n@@ -5373,2 +5518,2 @@\n-  prefix(src);\n-  emit_int16(0x0F, 0x18);\n+  prefix(src, true \/* is_map1 *\/);\n+  emit_int8(0x18);\n@@ -5381,2 +5526,2 @@\n-  prefix(src);\n-  emit_int16(0x0F, 0x18);\n+  prefix(src, true \/* is_map1 *\/);\n+  emit_int8(0x18);\n@@ -5389,2 +5534,2 @@\n-  prefix(src);\n-  emit_int16(0x0F, 0x0D);\n+  prefix(src, true \/* is_map1 *\/);\n+  emit_int8(0x0D);\n@@ -5398,0 +5543,6 @@\n+void Assembler::prefix16(int prefix) {\n+  assert(UseAPX, \"APX features not enabled\");\n+  emit_int8((prefix & 0xff00) >> 8);\n+  emit_int8(prefix & 0xff);\n+}\n+\n@@ -5605,0 +5756,1 @@\n+  assert(!needs_eevex(src.base(), src.index()), \"does not support extended gprs\");\n@@ -5622,0 +5774,1 @@\n+  assert(!needs_eevex(src.base(), src.index()), \"does not support extended gprs\");\n@@ -6035,2 +6188,2 @@\n-  int encode = prefix_and_encode(dst->encoding(), true);\n-  emit_int24(0x0F, (unsigned char)0x90 | cc, (0xC0 | encode));\n+  int encode = prefix_and_encode(dst->encoding(), true, true \/* is_map1 *\/);\n+  emit_opcode_prefix_and_encoding((unsigned char)0x90 | cc, 0xC0, encode);\n@@ -6169,2 +6322,2 @@\n-  int encode = prefix_and_encode(src->encoding(), dst->encoding());\n-  emit_int24(0x0F, (unsigned char)0xA5, (0xC0 | encode));\n+  int encode = prefix_and_encode(src->encoding(), dst->encoding(), true \/* is_map1 *\/);\n+  emit_opcode_prefix_and_encoding((unsigned char)0xA5, 0xC0, encode);\n@@ -6174,2 +6327,2 @@\n-  int encode = prefix_and_encode(src->encoding(), dst->encoding());\n-  emit_int32(0x0F, (unsigned char)0xA4, (0xC0 | encode), imm8);\n+  int encode = prefix_and_encode(src->encoding(), dst->encoding(), true \/* is_map1 *\/);\n+  emit_opcode_prefix_and_encoding((unsigned char)0xA4, 0xC0, encode, imm8);\n@@ -6179,2 +6332,2 @@\n-  int encode = prefix_and_encode(src->encoding(), dst->encoding());\n-  emit_int24(0x0F, (unsigned char)0xAD, (0xC0 | encode));\n+  int encode = prefix_and_encode(src->encoding(), dst->encoding(), true \/* is_map1 *\/);\n+  emit_opcode_prefix_and_encoding((unsigned char)0xAD, 0xC0, encode);\n@@ -6184,2 +6337,2 @@\n-  int encode = prefix_and_encode(src->encoding(), dst->encoding());\n-  emit_int32(0x0F, (unsigned char)0xAC, (0xC0 | encode), imm8);\n+  int encode = prefix_and_encode(src->encoding(), dst->encoding(), true \/* is_map1 *\/);\n+  emit_opcode_prefix_and_encoding((unsigned char)0xAC, 0xC0, encode, imm8);\n@@ -6190,2 +6343,2 @@\n-  int encode = prefixq_and_encode(src->encoding(), dst->encoding());\n-  emit_int32(0x0F, (unsigned char)0xA4, (0xC0 | encode), imm8);\n+  int encode = prefixq_and_encode(src->encoding(), dst->encoding(), true \/* is_map1 *\/);\n+  emit_opcode_prefix_and_encoding((unsigned char)0xA4, 0xC0, encode, imm8);\n@@ -6195,2 +6348,2 @@\n-  int encode = prefixq_and_encode(src->encoding(), dst->encoding());\n-  emit_int32(0x0F, (unsigned char)0xAC, (0xC0 | encode), imm8);\n+  int encode = prefixq_and_encode(src->encoding(), dst->encoding(), true \/* is_map1 *\/);\n+  emit_opcode_prefix_and_encoding((unsigned char)0xAC, 0xC0, encode, imm8);\n@@ -6262,2 +6415,5 @@\n-void Assembler::stmxcsr( Address dst) {\n-  if (UseAVX > 0 ) {\n+void Assembler::stmxcsr(Address dst) {\n+  \/\/ This instruction should be SSE encoded with the REX2 prefix when an\n+  \/\/ extended GPR is present. To be consistent when UseAPX is enabled, use\n+  \/\/ this encoding even when an extended GPR is not used.\n+  if (UseAVX > 0 && !UseAPX  ) {\n@@ -6273,2 +6429,2 @@\n-    prefix(dst);\n-    emit_int16(0x0F, (unsigned char)0xAE);\n+    prefix(dst, true \/* is_map1 *\/);\n+    emit_int8((unsigned char)0xAE);\n@@ -6414,4 +6570,2 @@\n-  int encode = prefix_and_encode(dst->encoding(), src->encoding());\n-  emit_int24(0x0F,\n-             (unsigned char)0xBC,\n-             0xC0 | encode);\n+  int encode = prefix_and_encode(dst->encoding(), src->encoding(), true \/* is_map1 *\/);\n+  emit_opcode_prefix_and_encoding((unsigned char)0xBC, 0xC0, encode);\n@@ -6424,2 +6578,2 @@\n-  prefix(src, dst);\n-  emit_int16(0x0F, (unsigned char)0xBC);\n+  prefix(src, dst, false, true \/* is_map1 *\/);\n+  emit_int8((unsigned char)0xBC);\n@@ -6432,2 +6586,2 @@\n-  int encode = prefixq_and_encode(dst->encoding(), src->encoding());\n-  emit_int24(0x0F, (unsigned char)0xBC, (0xC0 | encode));\n+  int encode = prefixq_and_encode(dst->encoding(), src->encoding(), true \/* is_map1 *\/);\n+  emit_opcode_prefix_and_encoding((unsigned char)0xBC, 0xC0, encode);\n@@ -6440,2 +6594,2 @@\n-  prefixq(src, dst);\n-  emit_int16(0x0F, (unsigned char)0xBC);\n+  prefixq(src, dst, true \/* is_map1 *\/);\n+  emit_int8((unsigned char)0xBC);\n@@ -6487,2 +6641,2 @@\n-  prefix(dst, src, true);\n-  emit_int16(0x0F, (unsigned char)0xC0);\n+  prefix(dst, src, true, true \/* is_map1 *\/);\n+  emit_int8((unsigned char)0xC0);\n@@ -6495,2 +6649,2 @@\n-  prefix(dst, src);\n-  emit_int16(0x0F, (unsigned char)0xC1);\n+  prefix(dst, src, false, true \/* is_map1 *\/);\n+  emit_int8((unsigned char)0xC1);\n@@ -6502,2 +6656,2 @@\n-  prefix(dst, src);\n-  emit_int16(0x0F, (unsigned char)0xC1);\n+  prefix(dst, src, false, true \/* is_map1 *\/);\n+  emit_int8((unsigned char)0xC1);\n@@ -6607,0 +6761,8 @@\n+void Assembler::xorw(Register dst, Address src) {\n+  InstructionMark im(this);\n+  emit_int8(0x66);\n+  prefix(src, dst);\n+  emit_int8(0x33);\n+  emit_operand(dst, src, 0);\n+}\n+\n@@ -7873,0 +8035,8 @@\n+void Assembler::vpminub(XMMRegister dst, XMMRegister nds, XMMRegister src, int vector_len) {\n+  assert(vector_len == AVX_128bit ? VM_Version::supports_avx() :\n+        (vector_len == AVX_256bit ? VM_Version::supports_avx2() : VM_Version::supports_avx512bw()), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ _legacy_mode_bw, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int16(0xDA, (0xC0 | encode));\n+}\n+\n@@ -9273,1 +9443,1 @@\n-  int encode = simd_prefix_and_encode(src, xnoreg, as_XMMRegister(dst->encoding()), VEX_SIMD_66, VEX_OPCODE_0F_3A, &attributes);\n+  int encode = simd_prefix_and_encode(src, xnoreg, as_XMMRegister(dst->encoding()), VEX_SIMD_66, VEX_OPCODE_0F_3A, &attributes, true);\n@@ -11043,1 +11213,1 @@\n-  int encode = vex_prefix_and_encode(dst->encoding(), 0, src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  int encode = vex_prefix_and_encode(dst->encoding(), 0, src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes, true);\n@@ -11052,1 +11222,1 @@\n-  int encode = vex_prefix_and_encode(dst->encoding(), 0, src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  int encode = vex_prefix_and_encode(dst->encoding(), 0, src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes , true);\n@@ -11061,1 +11231,1 @@\n-  int encode = vex_prefix_and_encode(dst->encoding(), 0, src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  int encode = vex_prefix_and_encode(dst->encoding(), 0, src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes, true);\n@@ -11070,1 +11240,1 @@\n-  int encode = vex_prefix_and_encode(dst->encoding(), 0, src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  int encode = vex_prefix_and_encode(dst->encoding(), 0, src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes, true);\n@@ -11830,1 +12000,2 @@\n-void Assembler::evex_prefix(bool vex_r, bool vex_b, bool vex_x, bool evex_r, bool evex_v, int nds_enc, VexSimdPrefix pre, VexOpcode opc){\n+void Assembler::evex_prefix(bool vex_r, bool vex_b, bool vex_x, bool evex_r, bool eevex_b, bool evex_v,\n+                       bool eevex_x, int nds_enc, VexSimdPrefix pre, VexOpcode opc) {\n@@ -11839,1 +12010,1 @@\n-  \/\/ P0: byte 2, initialized to RXBR`00mm\n+  \/\/ P0: byte 2, initialized to RXBR'0mmm\n@@ -11843,0 +12014,1 @@\n+  byte2 |= eevex_b ? EEVEX_B : 0;\n@@ -11844,1 +12016,1 @@\n-  \/\/ of form {0F, 0F_38, 0F_3A, MAP5}\n+  \/\/ of form {0F, 0F_38, 0F_3A, 0F_3C, MAP5, MAP6}\n@@ -11849,2 +12021,1 @@\n-  \/\/ p[10] is always 1\n-  byte3 |= EVEX_F;\n+  byte3 |= (eevex_x ? 0 : EEVEX_X);\n@@ -11877,0 +12048,4 @@\n+  if (adr.base_needs_rex2() || adr.index_needs_rex2()) {\n+    assert(UseAPX, \"APX features not enabled\");\n+  }\n+  bool is_extended = adr.base_needs_rex2() || adr.index_needs_rex2() || nds_enc >= 16 || xreg_enc >= 16;\n@@ -11886,1 +12061,0 @@\n-\n@@ -11892,1 +12066,1 @@\n-      if ((attributes->get_vector_len() != AVX_512bit) && (nds_enc < 16) && (xreg_enc < 16)) {\n+      if ((attributes->get_vector_len() != AVX_512bit) && !is_extended) {\n@@ -11903,1 +12077,1 @@\n-    assert(((nds_enc < 16 && xreg_enc < 16) || (!attributes->is_legacy_mode())),\"XMM register should be 0-15\");\n+    assert((!is_extended || (!attributes->is_legacy_mode())),\"XMM register should be 0-15\");\n@@ -11917,0 +12091,2 @@\n+    bool eevex_x = adr.index_needs_rex2();\n+    bool eevex_b = adr.base_needs_rex2();\n@@ -11918,1 +12094,1 @@\n-    evex_prefix(vex_r, vex_b, vex_x, evex_r, evex_v, nds_enc, pre, opc);\n+    evex_prefix(vex_r, vex_b, vex_x, evex_r, eevex_b, evex_v, eevex_x, nds_enc, pre, opc);\n@@ -11927,1 +12103,5 @@\n-int Assembler::vex_prefix_and_encode(int dst_enc, int nds_enc, int src_enc, VexSimdPrefix pre, VexOpcode opc, InstructionAttr *attributes) {\n+int Assembler::vex_prefix_and_encode(int dst_enc, int nds_enc, int src_enc, VexSimdPrefix pre, VexOpcode opc, InstructionAttr *attributes, bool src_is_gpr) {\n+  if (src_is_gpr && src_enc >= 16) {\n+    assert(UseAPX, \"APX features not enabled\");\n+  }\n+  bool is_extended = dst_enc >= 16 || nds_enc >= 16 || src_enc >=16;\n@@ -11939,1 +12119,1 @@\n-          (dst_enc < 16) && (nds_enc < 16) && (src_enc < 16)) {\n+           !is_extended) {\n@@ -11956,1 +12136,1 @@\n-    assert(((dst_enc < 16 && nds_enc < 16 && src_enc < 16) || (!attributes->is_legacy_mode())),\"XMM register should be 0-15\");\n+    assert(((!is_extended) || (!attributes->is_legacy_mode())),\"XMM register should be 0-15\");\n@@ -11964,0 +12144,1 @@\n+    bool evex_b = (src_enc >= 16) && src_is_gpr;\n@@ -11965,1 +12146,1 @@\n-    vex_x = (src_enc >= 16);\n+    vex_x = (src_enc >= 16) && !src_is_gpr;\n@@ -11967,1 +12148,1 @@\n-    evex_prefix(vex_r, vex_b, vex_x, evex_r, evex_v, nds_enc, pre, opc);\n+    evex_prefix(vex_r, vex_b, vex_x, evex_r, evex_b, evex_v, false \/*eevex_x*\/, nds_enc, pre, opc);\n@@ -11979,1 +12160,0 @@\n-\n@@ -11993,1 +12173,1 @@\n-                                      VexOpcode opc, InstructionAttr *attributes) {\n+                                      VexOpcode opc, InstructionAttr *attributes, bool src_is_gpr) {\n@@ -11998,1 +12178,1 @@\n-    return vex_prefix_and_encode(dst_enc, nds_enc, src_enc, pre, opc, attributes);\n+    return vex_prefix_and_encode(dst_enc, nds_enc, src_enc, pre, opc, attributes, src_is_gpr);\n@@ -12493,2 +12673,9 @@\n-  InstructionAttr attributes(AVX_128bit, \/* vex_w *\/ true, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n-  int encode = vex_prefix_and_encode(dst->encoding(), src2->encoding(), src1->encoding(), VEX_SIMD_NONE, VEX_OPCODE_0F_38, &attributes);\n+  InstructionAttr attributes(AVX_128bit, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  int encode = vex_prefix_and_encode(dst->encoding(), src2->encoding(), src1->encoding(), VEX_SIMD_NONE, VEX_OPCODE_0F_38, &attributes, true);\n+  emit_int16((unsigned char)0xF5, (0xC0 | encode));\n+}\n+\n+void Assembler::bzhil(Register dst, Register src1, Register src2) {\n+  assert(VM_Version::supports_bmi2(), \"bit manipulation instructions not supported\");\n+  InstructionAttr attributes(AVX_128bit, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  int encode = vex_prefix_and_encode(dst->encoding(), src2->encoding(), src1->encoding(), VEX_SIMD_NONE, VEX_OPCODE_0F_38, &attributes, true);\n@@ -12500,2 +12687,2 @@\n-  InstructionAttr attributes(AVX_128bit, \/* vex_w *\/ false, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n-  int encode = vex_prefix_and_encode(dst->encoding(), src1->encoding(), src2->encoding(), VEX_SIMD_F3, VEX_OPCODE_0F_38, &attributes);\n+  InstructionAttr attributes(AVX_128bit, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  int encode = vex_prefix_and_encode(dst->encoding(), src1->encoding(), src2->encoding(), VEX_SIMD_F3, VEX_OPCODE_0F_38, &attributes, true);\n@@ -12507,2 +12694,2 @@\n-  InstructionAttr attributes(AVX_128bit, \/* vex_w *\/ false, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n-  int encode = vex_prefix_and_encode(dst->encoding(), src1->encoding(), src2->encoding(), VEX_SIMD_F2, VEX_OPCODE_0F_38, &attributes);\n+  InstructionAttr attributes(AVX_128bit, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  int encode = vex_prefix_and_encode(dst->encoding(), src1->encoding(), src2->encoding(), VEX_SIMD_F2, VEX_OPCODE_0F_38, &attributes, true);\n@@ -12514,2 +12701,2 @@\n-  InstructionAttr attributes(AVX_128bit, \/* vex_w *\/ true, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n-  int encode = vex_prefix_and_encode(dst->encoding(), src1->encoding(), src2->encoding(), VEX_SIMD_F3, VEX_OPCODE_0F_38, &attributes);\n+  InstructionAttr attributes(AVX_128bit, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  int encode = vex_prefix_and_encode(dst->encoding(), src1->encoding(), src2->encoding(), VEX_SIMD_F3, VEX_OPCODE_0F_38, &attributes, true);\n@@ -12521,2 +12708,2 @@\n-  InstructionAttr attributes(AVX_128bit, \/* vex_w *\/ true, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n-  int encode = vex_prefix_and_encode(dst->encoding(), src1->encoding(), src2->encoding(), VEX_SIMD_F2, VEX_OPCODE_0F_38, &attributes);\n+  InstructionAttr attributes(AVX_128bit, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  int encode = vex_prefix_and_encode(dst->encoding(), src1->encoding(), src2->encoding(), VEX_SIMD_F2, VEX_OPCODE_0F_38, &attributes, true);\n@@ -12529,1 +12716,2 @@\n-  InstructionAttr attributes(AVX_128bit, \/* vex_w *\/ false, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  InstructionAttr attributes(AVX_128bit, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_NOSCALE, \/* input_size_in_bits *\/ EVEX_32bit);\n@@ -12538,1 +12726,2 @@\n-  InstructionAttr attributes(AVX_128bit, \/* vex_w *\/ false, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  InstructionAttr attributes(AVX_128bit, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_NOSCALE, \/* input_size_in_bits *\/ EVEX_32bit);\n@@ -12547,1 +12736,2 @@\n-  InstructionAttr attributes(AVX_128bit, \/* vex_w *\/ true, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  InstructionAttr attributes(AVX_128bit, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_NOSCALE, \/* input_size_in_bits *\/ EVEX_64bit);\n@@ -12556,1 +12746,2 @@\n-  InstructionAttr attributes(AVX_128bit, \/* vex_w *\/ true, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  InstructionAttr attributes(AVX_128bit, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_NOSCALE, \/* input_size_in_bits *\/ EVEX_64bit);\n@@ -12564,2 +12755,2 @@\n-  InstructionAttr attributes(AVX_128bit, \/* vex_w *\/ false, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n-  int encode = vex_prefix_and_encode(dst->encoding(), src2->encoding(), src1->encoding(), VEX_SIMD_F3, VEX_OPCODE_0F_38, &attributes);\n+  InstructionAttr attributes(AVX_128bit, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  int encode = vex_prefix_and_encode(dst->encoding(), src2->encoding(), src1->encoding(), VEX_SIMD_F3, VEX_OPCODE_0F_38, &attributes, true);\n@@ -12572,1 +12763,2 @@\n-  InstructionAttr attributes(AVX_128bit, \/* vex_w *\/ false, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  InstructionAttr attributes(AVX_128bit, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_NOSCALE, \/* input_size_in_bits *\/ EVEX_32bit);\n@@ -12580,2 +12772,2 @@\n-  InstructionAttr attributes(AVX_128bit, \/* vex_w *\/ true, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n-  int encode = vex_prefix_and_encode(dst->encoding(), src2->encoding(), src1->encoding(), VEX_SIMD_F3, VEX_OPCODE_0F_38, &attributes);\n+  InstructionAttr attributes(AVX_128bit, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  int encode = vex_prefix_and_encode(dst->encoding(), src2->encoding(), src1->encoding(), VEX_SIMD_F3, VEX_OPCODE_0F_38, &attributes, true);\n@@ -12588,1 +12780,2 @@\n-  InstructionAttr attributes(AVX_128bit, \/* vex_w *\/ true, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  InstructionAttr attributes(AVX_128bit, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_NOSCALE, \/* input_size_in_bits *\/ EVEX_64bit);\n@@ -12596,2 +12789,2 @@\n-  InstructionAttr attributes(AVX_128bit, \/* vex_w *\/ false, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n-  int encode = vex_prefix_and_encode(dst->encoding(), src2->encoding(), src1->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  InstructionAttr attributes(AVX_128bit, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  int encode = vex_prefix_and_encode(dst->encoding(), src2->encoding(), src1->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes, true);\n@@ -12604,1 +12797,2 @@\n-  InstructionAttr attributes(AVX_128bit, \/* vex_w *\/ false, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  InstructionAttr attributes(AVX_128bit, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_NOSCALE, \/* input_size_in_bits *\/ EVEX_32bit);\n@@ -12612,2 +12806,2 @@\n-  InstructionAttr attributes(AVX_128bit, \/* vex_w *\/ true, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n-  int encode = vex_prefix_and_encode(dst->encoding(), src2->encoding(), src1->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  InstructionAttr attributes(AVX_128bit, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  int encode = vex_prefix_and_encode(dst->encoding(), src2->encoding(), src1->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes, true);\n@@ -12620,1 +12814,2 @@\n-  InstructionAttr attributes(AVX_128bit, \/* vex_w *\/ true, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  InstructionAttr attributes(AVX_128bit, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_NOSCALE, \/* input_size_in_bits *\/ EVEX_64bit);\n@@ -12628,2 +12823,2 @@\n-  InstructionAttr attributes(AVX_128bit, \/* vex_w *\/ false, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n-  int encode = vex_prefix_and_encode(dst->encoding(), src2->encoding(), src1->encoding(), VEX_SIMD_F2, VEX_OPCODE_0F_38, &attributes);\n+  InstructionAttr attributes(AVX_128bit, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  int encode = vex_prefix_and_encode(dst->encoding(), src2->encoding(), src1->encoding(), VEX_SIMD_F2, VEX_OPCODE_0F_38, &attributes, true);\n@@ -12636,1 +12831,2 @@\n-  InstructionAttr attributes(AVX_128bit, \/* vex_w *\/ false, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  InstructionAttr attributes(AVX_128bit, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_NOSCALE, \/* input_size_in_bits *\/ EVEX_32bit);\n@@ -12644,2 +12840,2 @@\n-  InstructionAttr attributes(AVX_128bit, \/* vex_w *\/ true, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n-  int encode = vex_prefix_and_encode(dst->encoding(), src2->encoding(), src1->encoding(), VEX_SIMD_F2, VEX_OPCODE_0F_38, &attributes);\n+  InstructionAttr attributes(AVX_128bit, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  int encode = vex_prefix_and_encode(dst->encoding(), src2->encoding(), src1->encoding(), VEX_SIMD_F2, VEX_OPCODE_0F_38, &attributes, true);\n@@ -12652,1 +12848,2 @@\n-  InstructionAttr attributes(AVX_128bit, \/* vex_w *\/ true, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  InstructionAttr attributes(AVX_128bit, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_NOSCALE, \/* input_size_in_bits *\/ EVEX_64bit);\n@@ -12956,0 +13153,29 @@\n+int Assembler::get_base_prefix_bits(int enc) {\n+  int bits = 0;\n+  if (enc & 16) bits |= REX2BIT_B4;\n+  if (enc & 8) bits |= REXBIT_B;\n+  return bits;\n+}\n+\n+int Assembler::get_index_prefix_bits(int enc) {\n+  int bits = 0;\n+  if (enc & 16) bits |= REX2BIT_X4;\n+  if (enc & 8) bits |= REXBIT_X;\n+  return bits;\n+}\n+\n+int Assembler::get_base_prefix_bits(Register base) {\n+  return base->is_valid() ? get_base_prefix_bits(base->encoding()) : 0;\n+}\n+\n+int Assembler::get_index_prefix_bits(Register index) {\n+  return index->is_valid() ? get_index_prefix_bits(index->encoding()) : 0;\n+}\n+\n+int Assembler::get_reg_prefix_bits(int enc) {\n+  int bits = 0;\n+  if (enc & 16) bits |= REX2BIT_R4;\n+  if (enc & 8) bits |= REXBIT_R;\n+  return bits;\n+}\n+\n@@ -12957,1 +13183,3 @@\n-  if (reg->encoding() >= 8) {\n+  if (reg->encoding() >= 16) {\n+    prefix16(WREX2 | get_base_prefix_bits(reg->encoding()));\n+  } else if (reg->encoding() >= 8) {\n@@ -12963,0 +13191,4 @@\n+  if ((p & WREX2) || src->encoding() >= 16 || dst->encoding() >= 16) {\n+    prefix_rex2(dst, src);\n+    return;\n+  }\n@@ -12975,0 +13207,7 @@\n+void Assembler::prefix_rex2(Register dst, Register src) {\n+  int bits = 0;\n+  bits |= get_base_prefix_bits(src->encoding());\n+  bits |= get_reg_prefix_bits(dst->encoding());\n+  prefix16(WREX2 | bits);\n+}\n+\n@@ -12976,0 +13215,3 @@\n+  if (adr.base_needs_rex2() || adr.index_needs_rex2() || dst->encoding() >= 16) {\n+    prefix_rex2(dst, adr);\n+  }\n@@ -12996,1 +13238,13 @@\n-void Assembler::prefix(Address adr) {\n+void Assembler::prefix_rex2(Register dst, Address adr) {\n+  assert(!adr.index_needs_rex2(), \"prefix(Register dst, Address adr) does not support handling of an X\");\n+  int bits = 0;\n+  bits |= get_base_prefix_bits(adr.base());\n+  bits |= get_reg_prefix_bits(dst->encoding());\n+  prefix16(WREX2 | bits);\n+}\n+\n+void Assembler::prefix(Address adr, bool is_map1) {\n+  if (adr.base_needs_rex2() || adr.index_needs_rex2()) {\n+    prefix_rex2(adr, is_map1);\n+    return;\n+  }\n@@ -13008,0 +13262,8 @@\n+  if (is_map1) emit_int8(0x0F);\n+}\n+\n+void Assembler::prefix_rex2(Address adr, bool is_map1) {\n+  int bits = is_map1 ? REX2BIT_M0 : 0;\n+  bits |= get_base_prefix_bits(adr.base());\n+  bits |= get_index_prefix_bits(adr.index());\n+  prefix16(WREX2 | bits);\n@@ -13010,1 +13272,5 @@\n-void Assembler::prefix(Address adr, Register reg, bool byteinst) {\n+void Assembler::prefix(Address adr, Register reg, bool byteinst, bool is_map1) {\n+  if (reg->encoding() >= 16 || adr.base_needs_rex2() || adr.index_needs_rex2()) {\n+    prefix_rex2(adr, reg, byteinst, is_map1);\n+    return;\n+  }\n@@ -13040,0 +13306,9 @@\n+  if (is_map1) emit_int8(0x0F);\n+}\n+\n+void Assembler::prefix_rex2(Address adr, Register reg, bool byteinst, bool is_map1) {\n+  int bits = is_map1 ? REX2BIT_M0 : 0;\n+  bits |= get_base_prefix_bits(adr.base());\n+  bits |= get_index_prefix_bits(adr.index());\n+  bits |= get_reg_prefix_bits(reg->encoding());\n+  prefix16(WREX2 | bits);\n@@ -13043,0 +13318,4 @@\n+  if (reg->encoding() >= 16 || adr.base_needs_rex2() || adr.index_needs_rex2()) {\n+    prefixq_rex2(adr, reg);\n+    return;\n+  }\n@@ -13072,1 +13351,12 @@\n-int Assembler::prefix_and_encode(int reg_enc, bool byteinst) {\n+void Assembler::prefix_rex2(Address adr, XMMRegister src) {\n+  int bits = 0;\n+  bits |= get_base_prefix_bits(adr.base());\n+  bits |= get_index_prefix_bits(adr.index());\n+  bits |= get_reg_prefix_bits(src->encoding());\n+  prefix16(WREX2 | bits);\n+}\n+\n+int Assembler::prefix_and_encode(int reg_enc, bool byteinst, bool is_map1) {\n+  if (reg_enc >= 16) {\n+    return prefix_and_encode_rex2(reg_enc, is_map1);\n+  }\n@@ -13079,1 +13369,7 @@\n-  return reg_enc;\n+  int opc_prefix = is_map1 ? 0x0F00 : 0;\n+  return opc_prefix | reg_enc;\n+}\n+\n+int Assembler::prefix_and_encode_rex2(int reg_enc, bool is_map1) {\n+  prefix16(WREX2 | (is_map1 ? REX2BIT_M0 : 0) | get_base_prefix_bits(reg_enc));\n+  return reg_enc & 0x7;\n@@ -13082,1 +13378,4 @@\n-int Assembler::prefix_and_encode(int dst_enc, bool dst_is_byte, int src_enc, bool src_is_byte) {\n+int Assembler::prefix_and_encode(int dst_enc, bool dst_is_byte, int src_enc, bool src_is_byte, bool is_map1) {\n+  if (src_enc >= 16 || dst_enc >= 16) {\n+    return prefix_and_encode_rex2(dst_enc, src_enc, is_map1 ? REX2BIT_M0 : 0);\n+  }\n@@ -13099,0 +13398,11 @@\n+  int opcode_prefix = is_map1 ? 0x0F00 : 0;\n+  return opcode_prefix | (dst_enc << 3 | src_enc);\n+}\n+\n+int Assembler::prefix_and_encode_rex2(int dst_enc, int src_enc, int init_bits) {\n+  int bits = init_bits;\n+  bits |= get_reg_prefix_bits(dst_enc);\n+  bits |= get_base_prefix_bits(src_enc);\n+  dst_enc &= 0x7;\n+  src_enc &= 0x7;\n+  prefix16(WREX2 | bits);\n@@ -13102,1 +13412,8 @@\n-int8_t Assembler::get_prefixq(Address adr) {\n+bool Assembler::prefix_is_rex2(int prefix) {\n+  return (prefix & 0xFF00) == WREX2;\n+}\n+\n+int Assembler::get_prefixq(Address adr, bool is_map1) {\n+  if (adr.base_needs_rex2() || adr.index_needs_rex2()) {\n+    return get_prefixq_rex2(adr, is_map1);\n+  }\n@@ -13105,1 +13422,10 @@\n-  return prfx;\n+  return is_map1 ? (((int16_t)prfx) << 8) | 0x0F : (int16_t)prfx;\n+}\n+\n+int Assembler::get_prefixq_rex2(Address adr, bool is_map1) {\n+  assert(UseAPX, \"APX features not enabled\");\n+  int bits = REXBIT_W;\n+  if (is_map1) bits |= REX2BIT_M0;\n+  bits |= get_base_prefix_bits(adr.base());\n+  bits |= get_index_prefix_bits(adr.index());\n+  return WREX2 | bits;\n@@ -13108,1 +13434,4 @@\n-int8_t Assembler::get_prefixq(Address adr, Register src) {\n+int Assembler::get_prefixq(Address adr, Register src, bool is_map1) {\n+  if (adr.base_needs_rex2() || adr.index_needs_rex2() || src->encoding() >= 16) {\n+    return get_prefixq_rex2(adr, src, is_map1);\n+  }\n@@ -13144,1 +13473,11 @@\n-  return prfx;\n+  return is_map1 ? (((int16_t)prfx) << 8) | 0x0F : (int16_t)prfx;\n+}\n+\n+int Assembler::get_prefixq_rex2(Address adr, Register src, bool is_map1) {\n+  assert(UseAPX, \"APX features not enabled\");\n+  int bits = REXBIT_W;\n+  if (is_map1) bits |= REX2BIT_M0;\n+  bits |= get_base_prefix_bits(adr.base());\n+  bits |= get_index_prefix_bits(adr.index());\n+  bits |= get_reg_prefix_bits(src->encoding());\n+  return WREX2 | bits;\n@@ -13148,1 +13487,5 @@\n-  emit_int8(get_prefixq(adr));\n+  if (adr.base_needs_rex2() || adr.index_needs_rex2()) {\n+    prefix16(get_prefixq_rex2(adr));\n+  } else {\n+    emit_int8(get_prefixq(adr));\n+  }\n@@ -13151,2 +13494,7 @@\n-void Assembler::prefixq(Address adr, Register src) {\n-  emit_int8(get_prefixq(adr, src));\n+void Assembler::prefixq(Address adr, Register src, bool is_map1) {\n+  if (adr.base_needs_rex2() || adr.index_needs_rex2() || src->encoding() >= 16) {\n+    prefix16(get_prefixq_rex2(adr, src, is_map1));\n+  } else {\n+    emit_int8(get_prefixq(adr, src));\n+    if (is_map1) emit_int8(0x0F);\n+  }\n@@ -13155,0 +13503,1 @@\n+\n@@ -13156,0 +13505,4 @@\n+  if (src->encoding() >= 16 || adr.base_needs_rex2() || adr.index_needs_rex2()) {\n+    prefixq_rex2(adr, src);\n+    return;\n+  }\n@@ -13187,1 +13540,12 @@\n-int Assembler::prefixq_and_encode(int reg_enc) {\n+void Assembler::prefixq_rex2(Address adr, XMMRegister src) {\n+  int bits = REXBIT_W;\n+  bits |= get_base_prefix_bits(adr.base());\n+  bits |= get_index_prefix_bits(adr.index());\n+  bits |= get_reg_prefix_bits(src->encoding());\n+  prefix16(WREX2 | bits);\n+}\n+\n+int Assembler::prefixq_and_encode(int reg_enc, bool is_map1) {\n+  if (reg_enc >= 16) {\n+    return prefixq_and_encode_rex2(reg_enc, is_map1);\n+  }\n@@ -13194,1 +13558,8 @@\n-  return reg_enc;\n+  int opcode_prefix = is_map1 ? 0x0F00 : 0;\n+  return opcode_prefix | reg_enc;\n+}\n+\n+\n+int Assembler::prefixq_and_encode_rex2(int reg_enc, bool is_map1) {\n+  prefix16(WREX2 | REXBIT_W | (is_map1 ? REX2BIT_M0: 0) | get_base_prefix_bits(reg_enc));\n+  return reg_enc & 0x7;\n@@ -13197,1 +13568,4 @@\n-int Assembler::prefixq_and_encode(int dst_enc, int src_enc) {\n+int Assembler::prefixq_and_encode(int dst_enc, int src_enc, bool is_map1) {\n+  if (dst_enc >= 16 || src_enc >= 16) {\n+    return prefixq_and_encode_rex2(dst_enc, src_enc, is_map1);\n+  }\n@@ -13214,1 +13588,16 @@\n-  return dst_enc << 3 | src_enc;\n+  int opcode_prefix = is_map1 ? 0x0F00 : 0;\n+  return opcode_prefix | (dst_enc << 3 | src_enc);\n+}\n+\n+int Assembler::prefixq_and_encode_rex2(int dst_enc, int src_enc, bool is_map1) {\n+  int init_bits = REXBIT_W | (is_map1 ? REX2BIT_M0 : 0);\n+  return prefix_and_encode_rex2(dst_enc, src_enc, init_bits);\n+}\n+\n+void Assembler::emit_prefix_and_int8(int prefix, int b1) {\n+  if ((prefix & 0xFF00) == 0) {\n+    emit_int16(prefix, b1);\n+  } else {\n+    assert((prefix & 0xFF00) != WREX2 || UseAPX, \"APX features not enabled\");\n+    emit_int24((prefix & 0xFF00) >> 8, prefix & 0x00FF, b1);\n+  }\n@@ -13224,1 +13613,1 @@\n-  emit_int16(get_prefixq(src, dst), 0x13);\n+  emit_prefix_and_int8(get_prefixq(src, dst), 0x13);\n@@ -13241,1 +13630,1 @@\n-  emit_int16(get_prefixq(dst, src), 0x01);\n+  emit_prefix_and_int8(get_prefixq(dst, src), 0x01);\n@@ -13252,1 +13641,1 @@\n-  emit_int16(get_prefixq(src, dst), 0x03);\n+  emit_prefix_and_int8(get_prefixq(src, dst), 0x03);\n@@ -13263,6 +13652,12 @@\n-  emit_int8(0x66);\n-  int encode = prefixq_and_encode(dst->encoding(), src->encoding());\n-  emit_int32(0x0F,\n-             0x38,\n-             (unsigned char)0xF6,\n-             (0xC0 | encode));\n+  if (needs_rex2(dst, src)) {\n+    InstructionAttr attributes(AVX_128bit, \/* rex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+    int encode = vex_prefix_and_encode(dst->encoding(), 0, src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_3C, &attributes, true);\n+    emit_int16((unsigned char)0x66, (0xC0 | encode));\n+  } else {\n+    emit_int8(0x66);\n+    int encode = prefixq_and_encode(dst->encoding(), src->encoding());\n+    emit_int32(0x0F,\n+               0x38,\n+               (unsigned char)0xF6,\n+               (0xC0 | encode));\n+  }\n@@ -13273,6 +13668,12 @@\n-  emit_int8((unsigned char)0xF3);\n-  int encode = prefixq_and_encode(dst->encoding(), src->encoding());\n-  emit_int32(0x0F,\n-             0x38,\n-             (unsigned char)0xF6,\n-             (0xC0 | encode));\n+  if (needs_rex2(dst, src)) {\n+    InstructionAttr attributes(AVX_128bit, \/* rex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+    int encode = vex_prefix_and_encode(dst->encoding(), 0, src->encoding(), VEX_SIMD_F3, VEX_OPCODE_0F_3C, &attributes, true);\n+    emit_int16((unsigned char)0x66, (0xC0 | encode));\n+  } else {\n+    emit_int8((unsigned char)0xF3);\n+    int encode = prefixq_and_encode(dst->encoding(), src->encoding());\n+    emit_int32(0x0F,\n+               0x38,\n+               (unsigned char)0xF6,\n+               (0xC0 | encode));\n+  }\n@@ -13280,1 +13681,0 @@\n-\n@@ -13294,1 +13694,1 @@\n-  emit_int16(get_prefixq(src, dst), 0x23);\n+  emit_prefix_and_int8(get_prefixq(src, dst), 0x23);\n@@ -13305,1 +13705,1 @@\n-  emit_int16(get_prefixq(dst, src), 0x21);\n+  emit_prefix_and_int8(get_prefixq(dst, src), 0x21);\n@@ -13311,2 +13711,2 @@\n-  InstructionAttr attributes(AVX_128bit, \/* vex_w *\/ true, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n-  int encode = vex_prefix_and_encode(dst->encoding(), src1->encoding(), src2->encoding(), VEX_SIMD_NONE, VEX_OPCODE_0F_38, &attributes);\n+  InstructionAttr attributes(AVX_128bit, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  int encode = vex_prefix_and_encode(dst->encoding(), src1->encoding(), src2->encoding(), VEX_SIMD_NONE, VEX_OPCODE_0F_38, &attributes, true);\n@@ -13319,1 +13719,2 @@\n-  InstructionAttr attributes(AVX_128bit, \/* vex_w *\/ true, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  InstructionAttr attributes(AVX_128bit, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_NOSCALE, \/* input_size_in_bits *\/ EVEX_64bit);\n@@ -13326,2 +13727,2 @@\n-  int encode = prefixq_and_encode(dst->encoding(), src->encoding());\n-  emit_int24(0x0F, (unsigned char)0xBC, (0xC0 | encode));\n+  int encode = prefixq_and_encode(dst->encoding(), src->encoding(), true \/* is_map1 *\/);\n+  emit_opcode_prefix_and_encoding((unsigned char)0xBC, 0xC0, encode);\n@@ -13331,2 +13732,2 @@\n-  int encode = prefixq_and_encode(dst->encoding(), src->encoding());\n-  emit_int24(0x0F, (unsigned char)0xBD, (0xC0 | encode));\n+  int encode = prefixq_and_encode(dst->encoding(), src->encoding(), true \/* is_map1 *\/);\n+  emit_opcode_prefix_and_encoding((unsigned char)0xBD, 0xC0, encode);\n@@ -13336,2 +13737,2 @@\n-  int encode = prefixq_and_encode(reg->encoding());\n-  emit_int16(0x0F, (0xC8 | encode));\n+  int encode = prefixq_and_encode(reg->encoding(), true \/* is_map1 *\/);\n+  emit_opcode_prefix_and_encoding((unsigned char)0xC8, encode);\n@@ -13342,2 +13743,2 @@\n-  InstructionAttr attributes(AVX_128bit, \/* vex_w *\/ true, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n-  int encode = vex_prefix_and_encode(rbx->encoding(), dst->encoding(), src->encoding(), VEX_SIMD_NONE, VEX_OPCODE_0F_38, &attributes);\n+  InstructionAttr attributes(AVX_128bit, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  int encode = vex_prefix_and_encode(rbx->encoding(), dst->encoding(), src->encoding(), VEX_SIMD_NONE, VEX_OPCODE_0F_38, &attributes, true);\n@@ -13350,1 +13751,2 @@\n-  InstructionAttr attributes(AVX_128bit, \/* vex_w *\/ true, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  InstructionAttr attributes(AVX_128bit, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_NOSCALE, \/* input_size_in_bits *\/ EVEX_64bit);\n@@ -13358,2 +13760,2 @@\n-  InstructionAttr attributes(AVX_128bit, \/* vex_w *\/ true, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n-  int encode = vex_prefix_and_encode(rdx->encoding(), dst->encoding(), src->encoding(), VEX_SIMD_NONE, VEX_OPCODE_0F_38, &attributes);\n+  InstructionAttr attributes(AVX_128bit, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  int encode = vex_prefix_and_encode(rdx->encoding(),  dst->encoding(), src->encoding(), VEX_SIMD_NONE, VEX_OPCODE_0F_38, &attributes, true);\n@@ -13366,1 +13768,2 @@\n-  InstructionAttr attributes(AVX_128bit, \/* vex_w *\/ true, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  InstructionAttr attributes(AVX_128bit, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_NOSCALE, \/* input_size_in_bits *\/ EVEX_64bit);\n@@ -13374,2 +13777,2 @@\n-  InstructionAttr attributes(AVX_128bit, \/* vex_w *\/ true, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n-  int encode = vex_prefix_and_encode(rcx->encoding(), dst->encoding(), src->encoding(), VEX_SIMD_NONE, VEX_OPCODE_0F_38, &attributes);\n+  InstructionAttr attributes(AVX_128bit, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  int encode = vex_prefix_and_encode(rcx->encoding(), dst->encoding(), src->encoding(), VEX_SIMD_NONE, VEX_OPCODE_0F_38, &attributes, true);\n@@ -13382,1 +13785,2 @@\n-  InstructionAttr attributes(AVX_128bit, \/* vex_w *\/ true, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  InstructionAttr attributes(AVX_128bit, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_NOSCALE, \/* input_size_in_bits *\/ EVEX_64bit);\n@@ -13392,0 +13796,4 @@\n+void Assembler::cdqe() {\n+  emit_int16(REX_W, (unsigned char)0x98);\n+}\n+\n@@ -13394,2 +13802,2 @@\n-  prefix(adr);\n-  emit_int16(0x0F, (unsigned char)0xAE);\n+  prefix(adr, true \/* is_map1 *\/);\n+  emit_int8((unsigned char)0xAE);\n@@ -13407,1 +13815,1 @@\n-  prefix(adr);\n+  prefix(adr, true \/* is_map1 *\/);\n@@ -13409,1 +13817,1 @@\n-  emit_int16(0x0F, (unsigned char)0xAE);\n+  emit_int8((unsigned char)0xAE);\n@@ -13422,1 +13830,1 @@\n-  prefix(adr);\n+  prefix(adr, true \/* is_map1 *\/);\n@@ -13424,1 +13832,1 @@\n-  emit_int16(0x0F, (unsigned char)0xAE);\n+  emit_int8((unsigned char)0xAE);\n@@ -13430,2 +13838,2 @@\n-  int encode = prefixq_and_encode(dst->encoding(), src->encoding());\n-  emit_int24(0x0F, (0x40 | cc), (0xC0 | encode));\n+  int encode = prefixq_and_encode(dst->encoding(), src->encoding(), true \/* is_map1 *\/);\n+  emit_opcode_prefix_and_encoding((0x40 | cc), 0xC0, encode);\n@@ -13436,1 +13844,2 @@\n-  emit_int24(get_prefixq(src, dst), 0x0F, (0x40 | cc));\n+  int prefix = get_prefixq(src, dst, true \/* is_map1 *\/);\n+  emit_prefix_and_int8(prefix, (0x40 | cc));\n@@ -13453,1 +13862,1 @@\n-  emit_int16(get_prefixq(dst, src), 0x39);\n+  emit_prefix_and_int8(get_prefixq(dst, src), 0x39);\n@@ -13464,1 +13873,1 @@\n-  emit_int16(get_prefixq(src, dst), 0x3B);\n+  emit_prefix_and_int8(get_prefixq(src, dst), 0x3B);\n@@ -13470,1 +13879,2 @@\n-  emit_int24(get_prefixq(adr, reg), 0x0F, (unsigned char)0xB1);\n+  int prefix = get_prefixq(adr, reg, true \/* is_map1 *\/);\n+  emit_prefix_and_int8(prefix, (unsigned char)0xB1);\n@@ -13477,1 +13887,1 @@\n-  int encode = simd_prefix_and_encode(dst, dst, as_XMMRegister(src->encoding()), VEX_SIMD_F2, VEX_OPCODE_0F, &attributes);\n+  int encode = simd_prefix_and_encode(dst, dst, as_XMMRegister(src->encoding()), VEX_SIMD_F2, VEX_OPCODE_0F, &attributes, true);\n@@ -13506,1 +13916,3 @@\n-  emit_int32((unsigned char)0xF2, REX_W, 0x0F, 0x2C);\n+  emit_int8((unsigned char)0xF2);\n+  prefixq(src, dst, true \/* is_map1 *\/);\n+  emit_int8((unsigned char)0x2C);\n@@ -13548,1 +13960,1 @@\n-  emit_int16(get_prefixq(dst), (unsigned char)0xFF);\n+  emit_prefix_and_int8(get_prefixq(dst), (unsigned char)0xFF);\n@@ -13552,0 +13964,1 @@\n+\/\/ can't use REX2\n@@ -13553,0 +13966,1 @@\n+  InstructionMark im(this);\n@@ -13557,0 +13971,1 @@\n+\/\/ can't use REX2\n@@ -13558,0 +13973,1 @@\n+  InstructionMark im(this);\n@@ -13562,0 +13978,1 @@\n+\/\/ can't use REX2\n@@ -13563,0 +13980,1 @@\n+  InstructionMark im(this);\n@@ -13567,0 +13985,1 @@\n+\/\/ cant use REX2\n@@ -13568,0 +13987,1 @@\n+  InstructionMark im(this);\n@@ -13583,2 +14003,2 @@\n-  int encode = prefixq_and_encode(dst->encoding(), src->encoding());\n-  emit_int24(0x0F, (unsigned char)0xAF, (0xC0 | encode));\n+  int encode = prefixq_and_encode(dst->encoding(), src->encoding(), true \/* is_map1 *\/);\n+  emit_opcode_prefix_and_encoding((unsigned char)0xAF, 0xC0, encode);\n@@ -13618,1 +14038,2 @@\n-  emit_int24(get_prefixq(src, dst), 0x0F, (unsigned char)0xAF);\n+  int prefix = get_prefixq(src, dst, true \/* is_map1 *\/);\n+  emit_prefix_and_int8(prefix, (unsigned char)0xAF);\n@@ -13639,1 +14060,1 @@\n-  emit_int16(get_prefixq(dst), (unsigned char)0xFF);\n+  emit_prefix_and_int8(get_prefixq(dst), (unsigned char)0xFF);\n@@ -13649,1 +14070,1 @@\n-  emit_int16(get_prefixq(src, dst), (unsigned char)0x8D);\n+  emit_prefix_and_int8(get_prefixq(src, dst), (unsigned char)0x8D);\n@@ -13707,2 +14128,2 @@\n-  int encode = prefixq_and_encode(dst->encoding(), src->encoding());\n-  emit_int24(0x0F, (unsigned char)0xBD, (0xC0 | encode));\n+  int encode = prefixq_and_encode(dst->encoding(), src->encoding(), true \/* is_map1 *\/);\n+  emit_opcode_prefix_and_encoding((unsigned char)0xBD, 0xC0, encode);\n@@ -13715,2 +14136,2 @@\n-  prefixq(src, dst);\n-  emit_int16(0x0F, (unsigned char)0xBD);\n+  prefixq(src, dst, true \/* is_map1 *\/);\n+  emit_int8((unsigned char)0xBD);\n@@ -13724,1 +14145,1 @@\n-  int encode = simd_prefix_and_encode(dst, xnoreg, as_XMMRegister(src->encoding()), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  int encode = simd_prefix_and_encode(dst, xnoreg, as_XMMRegister(src->encoding()), VEX_SIMD_66, VEX_OPCODE_0F, &attributes, true);\n@@ -13733,1 +14154,1 @@\n-  int encode = simd_prefix_and_encode(src, xnoreg, as_XMMRegister(dst->encoding()), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  int encode = simd_prefix_and_encode(src, xnoreg, as_XMMRegister(dst->encoding()), VEX_SIMD_66, VEX_OPCODE_0F, &attributes, true);\n@@ -13746,1 +14167,1 @@\n-  emit_int16(get_prefixq(src, dst), (unsigned char)0x8B);\n+  emit_prefix_and_int8(get_prefixq(src, dst), (unsigned char)0x8B);\n@@ -13752,1 +14173,1 @@\n-  emit_int16(get_prefixq(dst, src), (unsigned char)0x89);\n+  emit_prefix_and_int8(get_prefixq(dst, src), (unsigned char)0x89);\n@@ -13758,1 +14179,1 @@\n-  emit_int16(get_prefixq(dst), (unsigned char)0xC7);\n+  emit_prefix_and_int8(get_prefixq(dst), (unsigned char)0xC7);\n@@ -13771,3 +14192,2 @@\n-  emit_int24(get_prefixq(src, dst),\n-             0x0F,\n-             (unsigned char)0xBE);\n+  int prefix = get_prefixq(src, dst, true \/* is_map1 *\/);\n+  emit_prefix_and_int8(prefix, (unsigned char)0xBE);\n@@ -13778,2 +14198,2 @@\n-  int encode = prefixq_and_encode(dst->encoding(), src->encoding());\n-  emit_int24(0x0F, (unsigned char)0xBE, (0xC0 | encode));\n+  int encode = prefixq_and_encode(dst->encoding(), src->encoding(), true \/* is_map1 *\/);\n+  emit_opcode_prefix_and_encoding((unsigned char)0xBE, 0xC0, encode);\n@@ -13785,1 +14205,1 @@\n-  emit_int16(get_prefixq(dst), (unsigned char)0xC7);\n+  emit_prefix_and_int8(get_prefixq(dst), (unsigned char)0xC7);\n@@ -13792,1 +14212,1 @@\n-  emit_int16(get_prefixq(src, dst), 0x63);\n+  emit_prefix_and_int8(get_prefixq(src, dst), 0x63);\n@@ -13803,3 +14223,2 @@\n-  emit_int24(get_prefixq(src, dst),\n-             0x0F,\n-             (unsigned char)0xBF);\n+  int prefix = get_prefixq(src, dst, true \/* is_map1 *\/);\n+  emit_prefix_and_int8(prefix, (unsigned char)0xBF);\n@@ -13810,2 +14229,2 @@\n-  int encode = prefixq_and_encode(dst->encoding(), src->encoding());\n-  emit_int24(0x0F, (unsigned char)0xBF, (0xC0 | encode));\n+  int encode = prefixq_and_encode(dst->encoding(), src->encoding(), true \/* is_map1 *\/);\n+  emit_opcode_prefix_and_encoding((unsigned char)0xBF, 0xC0, encode);\n@@ -13816,3 +14235,2 @@\n-  emit_int24(get_prefixq(src, dst),\n-             0x0F,\n-             (unsigned char)0xB6);\n+  int prefix = get_prefixq(src, dst, true \/* is_map1 *\/);\n+  emit_prefix_and_int8(prefix, (unsigned char)0xB6);\n@@ -13823,2 +14241,2 @@\n-  int encode = prefixq_and_encode(dst->encoding(), src->encoding());\n-  emit_int24(0x0F, (unsigned char)0xB6, (0xC0 | encode));\n+  int encode = prefixq_and_encode(dst->encoding(), src->encoding(), true \/* is_map1 *\/);\n+  emit_opcode_prefix_and_encoding((unsigned char)0xB6, 0xC0, encode);\n@@ -13829,3 +14247,2 @@\n-  emit_int24(get_prefixq(src, dst),\n-             0x0F,\n-             (unsigned char)0xB7);\n+  int prefix = get_prefixq(src, dst, true \/* is_map1 *\/);\n+  emit_prefix_and_int8(prefix, (unsigned char)0xB7);\n@@ -13836,2 +14253,2 @@\n-  int encode = prefixq_and_encode(dst->encoding(), src->encoding());\n-  emit_int24(0x0F, (unsigned char)0xB7, (0xC0 | encode));\n+  int encode = prefixq_and_encode(dst->encoding(), src->encoding(), true \/* is_map1 *\/);\n+  emit_opcode_prefix_and_encoding((unsigned char)0xB7, 0xC0, encode);\n@@ -13842,1 +14259,1 @@\n-  emit_int16(get_prefixq(src), (unsigned char)0xF7);\n+  emit_prefix_and_int8(get_prefixq(src), (unsigned char)0xF7);\n@@ -13853,2 +14270,2 @@\n-  InstructionAttr attributes(AVX_128bit, \/* vex_w *\/ true, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n-  int encode = vex_prefix_and_encode(dst1->encoding(), dst2->encoding(), src->encoding(), VEX_SIMD_F2, VEX_OPCODE_0F_38, &attributes);\n+  InstructionAttr attributes(AVX_128bit, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  int encode = vex_prefix_and_encode(dst1->encoding(), dst2->encoding(), src->encoding(), VEX_SIMD_F2, VEX_OPCODE_0F_38, &attributes, true);\n@@ -13865,1 +14282,1 @@\n-  emit_int16(get_prefixq(dst), (unsigned char)0xF7);\n+  emit_prefix_and_int8(get_prefixq(dst), (unsigned char)0xF7);\n@@ -13874,0 +14291,12 @@\n+void Assembler::btq(Register dst, Register src) {\n+  int encode = prefixq_and_encode(src->encoding(), dst->encoding(), true \/* is_map1 *\/);\n+  emit_opcode_prefix_and_encoding((unsigned char)0xA3, 0xC0, encode);\n+}\n+\n+void Assembler::btq(Register src, int imm8) {\n+  assert(isByte(imm8), \"not a byte\");\n+  int encode = prefixq_and_encode(src->encoding(), true \/* is_map1 *\/);\n+  emit_opcode_prefix_and_encoding((unsigned char)0xBA, 0xE0, encode);\n+  emit_int8(imm8);\n+}\n+\n@@ -13877,3 +14306,2 @@\n-  emit_int24(get_prefixq(dst),\n-             0x0F,\n-             (unsigned char)0xBA);\n+  int prefix = get_prefixq(dst, true \/* is_map1 *\/);\n+  emit_prefix_and_int8(prefix, (unsigned char)0xBA);\n@@ -13887,3 +14315,2 @@\n-  emit_int24(get_prefixq(dst),\n-             0x0F,\n-             (unsigned char)0xBA);\n+  int prefix = get_prefixq(dst, true \/* is_map1 *\/);\n+  emit_prefix_and_int8(prefix, (unsigned char)0xBA);\n@@ -13902,1 +14329,1 @@\n-  emit_int16(get_prefixq(dst, src), (unsigned char)0x09);\n+  emit_prefix_and_int8(get_prefixq(dst, src), (unsigned char)0x09);\n@@ -13918,1 +14345,1 @@\n-  emit_int16(get_prefixq(src, dst), 0x0B);\n+  emit_prefix_and_int8(get_prefixq(src, dst), 0x0B);\n@@ -13930,4 +14357,2 @@\n-  emit_int32((unsigned char)0xF3,\n-             get_prefixq(src, dst),\n-             0x0F,\n-             (unsigned char)0xB8);\n+  emit_int8((unsigned char)0xF3);\n+  emit_prefix_and_int8(get_prefixq(src, dst, true \/* is_map1 *\/), (unsigned char) 0xB8);\n@@ -13940,2 +14365,2 @@\n-  int encode = prefixq_and_encode(dst->encoding(), src->encoding());\n-  emit_int24(0x0F, (unsigned char)0xB8, (0xC0 | encode));\n+  int encode = prefixq_and_encode(dst->encoding(), src->encoding(), true \/* is_map1 *\/);\n+  emit_opcode_prefix_and_encoding((unsigned char)0xB8, 0xC0, encode);\n@@ -13946,1 +14371,1 @@\n-  emit_int16(get_prefixq(dst), (unsigned char)0x8F);\n+  emit_prefix_and_int8(get_prefixq(dst), (unsigned char)0x8F);\n@@ -13951,1 +14376,2 @@\n-  emit_int8((unsigned char)0x58 | dst->encoding());\n+  int encode = prefix_and_encode(dst->encoding());\n+  emit_int8((unsigned char)0x58 | encode);\n@@ -14091,1 +14517,1 @@\n-  emit_int16(get_prefixq(src), (unsigned char)0xFF);\n+  emit_prefix_and_int8(get_prefixq(src), (unsigned char)0xFF);\n@@ -14117,2 +14543,2 @@\n-  InstructionAttr attributes(AVX_128bit, \/* vex_w *\/ false, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n-  int encode = vex_prefix_and_encode(dst->encoding(), 0, src->encoding(), VEX_SIMD_F2, VEX_OPCODE_0F_3A, &attributes);\n+  InstructionAttr attributes(AVX_128bit, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  int encode = vex_prefix_and_encode(dst->encoding(), 0, src->encoding(), VEX_SIMD_F2, VEX_OPCODE_0F_3A, &attributes, true);\n@@ -14125,1 +14551,2 @@\n-  InstructionAttr attributes(AVX_128bit, \/* vex_w *\/ false, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  InstructionAttr attributes(AVX_128bit, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_NOSCALE, \/* input_size_in_bits *\/ EVEX_32bit);\n@@ -14134,2 +14561,2 @@\n-  InstructionAttr attributes(AVX_128bit, \/* vex_w *\/ true, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n-  int encode = vex_prefix_and_encode(dst->encoding(), 0, src->encoding(), VEX_SIMD_F2, VEX_OPCODE_0F_3A, &attributes);\n+  InstructionAttr attributes(AVX_128bit, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  int encode = vex_prefix_and_encode(dst->encoding(), 0,  src->encoding(), VEX_SIMD_F2, VEX_OPCODE_0F_3A, &attributes, true);\n@@ -14142,1 +14569,2 @@\n-  InstructionAttr attributes(AVX_128bit, \/* vex_w *\/ true, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  InstructionAttr attributes(AVX_128bit, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_NOSCALE, \/* input_size_in_bits *\/ EVEX_64bit);\n@@ -14154,1 +14582,1 @@\n-    emit_int16(get_prefixq(dst), (unsigned char)0xD1);\n+    emit_prefix_and_int8(get_prefixq(dst), (unsigned char)0xD1);\n@@ -14158,1 +14586,1 @@\n-    emit_int16(get_prefixq(dst), (unsigned char)0xC1);\n+    emit_prefix_and_int8(get_prefixq(dst), (unsigned char)0xC1);\n@@ -14166,1 +14594,1 @@\n-  emit_int16(get_prefixq(dst), (unsigned char)0xD3);\n+  emit_prefix_and_int8(get_prefixq(dst), (unsigned char)0xD3);\n@@ -14189,1 +14617,1 @@\n-    emit_int16(get_prefixq(dst), (unsigned char)0xD1);\n+    emit_prefix_and_int8(get_prefixq(dst), (unsigned char)0xD1);\n@@ -14193,1 +14621,1 @@\n-    emit_int16(get_prefixq(dst), (unsigned char)0xC1);\n+    emit_prefix_and_int8(get_prefixq(dst), (unsigned char)0xC1);\n@@ -14201,1 +14629,1 @@\n-  emit_int16(get_prefixq(dst), (unsigned char)0xD3);\n+  emit_prefix_and_int8(get_prefixq(dst), (unsigned char)0xD3);\n@@ -14234,1 +14662,1 @@\n-  emit_int16(get_prefixq(src, dst), 0x1B);\n+  emit_prefix_and_int8(get_prefixq(src, dst), 0x1B);\n@@ -14276,1 +14704,1 @@\n-  emit_int16(get_prefixq(dst), (unsigned char)0xD3);\n+  emit_prefix_and_int8(get_prefixq(dst), (unsigned char)0xD3);\n@@ -14284,1 +14712,1 @@\n-    emit_int16(get_prefixq(dst), (unsigned char)0xD1);\n+    emit_prefix_and_int8(get_prefixq(dst), (unsigned char)0xD1);\n@@ -14288,1 +14716,1 @@\n-    emit_int16(get_prefixq(dst), (unsigned char)0xC1);\n+    emit_prefix_and_int8(get_prefixq(dst), (unsigned char)0xC1);\n@@ -14302,1 +14730,1 @@\n-  emit_int16(get_prefixq(dst, src), 0x29);\n+  emit_prefix_and_int8(get_prefixq(dst, src), 0x29);\n@@ -14319,1 +14747,1 @@\n-  emit_int16(get_prefixq(src, dst), 0x2B);\n+  emit_prefix_and_int8(get_prefixq(src, dst), 0x2B);\n@@ -14330,1 +14758,1 @@\n-  emit_int16(get_prefixq(dst), (unsigned char)0xF7);\n+  emit_prefix_and_int8(get_prefixq(dst), (unsigned char)0xF7);\n@@ -14358,1 +14786,1 @@\n-  emit_int16(get_prefixq(src, dst), (unsigned char)0x85);\n+  emit_prefix_and_int8(get_prefixq(src, dst), (unsigned char)0x85);\n@@ -14364,1 +14792,2 @@\n-  emit_int24(get_prefixq(dst, src), 0x0F, (unsigned char)0xC1);\n+  int prefix = get_prefixq(dst, src, true \/* is_map1 *\/);\n+  emit_prefix_and_int8(prefix, (unsigned char)0xC1);\n@@ -14370,1 +14799,1 @@\n-  emit_int16(get_prefixq(src, dst), (unsigned char)0x87);\n+  emit_prefix_and_int8(get_prefixq(src, dst), (unsigned char)0x87);\n@@ -14386,1 +14815,1 @@\n-  emit_int16(get_prefixq(src, dst), 0x33);\n+  emit_prefix_and_int8(get_prefixq(src, dst), 0x33);\n@@ -14403,1 +14832,1 @@\n-  emit_int16(get_prefixq(dst, src), 0x31);\n+  emit_prefix_and_int8(get_prefixq(dst, src), 0x31);\n","filename":"src\/hotspot\/cpu\/x86\/assembler_x86.cpp","additions":849,"deletions":420,"binary":false,"changes":1269,"status":"modified"},{"patch":"@@ -311,1 +311,5 @@\n-    return _base->is_valid() && _base->encoding() >= 8;\n+    return _base->is_valid() && ((_base->encoding() & 8) == 8);\n+  }\n+\n+  bool base_needs_rex2() const {\n+    return _base->is_valid() && _base->encoding() >= 16;\n@@ -315,1 +319,5 @@\n-    return _index->is_valid() &&_index->encoding() >= 8;\n+    return _index->is_valid() && ((_index->encoding() & 8) == 8);\n+  }\n+\n+  bool index_needs_rex2() const {\n+    return _index->is_valid() &&_index->encoding() >= 16;\n@@ -322,0 +330,4 @@\n+  bool xmmindex_needs_rex2() const {\n+    return _xmmindex->is_valid() && _xmmindex->encoding() >= 16;\n+  }\n+\n@@ -511,0 +523,3 @@\n+    REX2       = 0xd5,\n+    WREX2      = REX2 << 8,\n+\n@@ -517,0 +532,11 @@\n+  enum PrefixBits {\n+    REXBIT_B  = 0x01,\n+    REXBIT_X  = 0x02,\n+    REXBIT_R  = 0x04,\n+    REXBIT_W  = 0x08,\n+    REX2BIT_B4 = 0x10,\n+    REX2BIT_X4 = 0x20,\n+    REX2BIT_R4 = 0x40,\n+    REX2BIT_M0 = 0x80\n+  };\n+\n@@ -528,0 +554,1 @@\n+    EVEX_B  = 0x20,\n@@ -532,0 +559,7 @@\n+  enum ExtEvexPrefix {\n+    EEVEX_R = 0x10,\n+    EEVEX_B = 0x08,\n+    EEVEX_X = 0x04,\n+    EEVEX_V = 0x08\n+  };\n+\n@@ -543,1 +577,1 @@\n-    VEX_SIMD_F2   = 0x3\n+    VEX_SIMD_F2   = 0x3,\n@@ -551,0 +585,1 @@\n+    VEX_OPCODE_0F_3C = 0x4,\n@@ -577,1 +612,2 @@\n-    EVEX_ETUP = 23\n+    EVEX_NOSCALE = 23,\n+    EVEX_ETUP = 24\n@@ -691,0 +727,6 @@\n+  int get_base_prefix_bits(int enc);\n+  int get_index_prefix_bits(int enc);\n+  int get_base_prefix_bits(Register base);\n+  int get_index_prefix_bits(Register index);\n+  int get_reg_prefix_bits(int enc);\n+\n@@ -694,0 +736,1 @@\n+  void prefix_rex2(Register dst, Register src);\n@@ -695,3 +738,10 @@\n-\n-  void prefix(Address adr);\n-  void prefix(Address adr, Register reg,  bool byteinst = false);\n+  void prefix_rex2(Register dst, Address adr);\n+\n+  \/\/ The is_map1 bool indicates an x86 map1 instruction which, when\n+  \/\/ legacy encoded, uses a 0x0F opcode prefix.  By specification, the\n+  \/\/ opcode prefix is omitted when using rex2 encoding in support\n+  \/\/ of APX extended GPRs.\n+  void prefix(Address adr, bool is_map1 = false);\n+  void prefix_rex2(Address adr, bool is_map1 = false);\n+  void prefix(Address adr, Register reg,  bool byteinst = false, bool is_map1 = false);\n+  void prefix_rex2(Address adr, Register reg,  bool byteinst = false, bool is_map1 = false);\n@@ -699,0 +749,1 @@\n+  void prefix_rex2(Address adr, XMMRegister reg);\n@@ -700,3 +751,4 @@\n-  int prefix_and_encode(int reg_enc, bool byteinst = false);\n-  int prefix_and_encode(int dst_enc, int src_enc) {\n-    return prefix_and_encode(dst_enc, false, src_enc, false);\n+  int prefix_and_encode(int reg_enc, bool byteinst = false, bool is_map1 = false);\n+  int prefix_and_encode_rex2(int reg_enc, bool is_map1 = false);\n+  int prefix_and_encode(int dst_enc, int src_enc, bool is_map1 = false) {\n+    return prefix_and_encode(dst_enc, false, src_enc, false, is_map1);\n@@ -704,1 +756,1 @@\n-  int prefix_and_encode(int dst_enc, bool dst_is_byte, int src_enc, bool src_is_byte);\n+  int prefix_and_encode(int dst_enc, bool dst_is_byte, int src_enc, bool src_is_byte, bool is_map1 = false);\n@@ -706,0 +758,1 @@\n+  int prefix_and_encode_rex2(int dst_enc, int src_enc, int init_bits = 0);\n@@ -709,2 +762,4 @@\n-  int8_t get_prefixq(Address adr);\n-  int8_t get_prefixq(Address adr, Register reg);\n+  int get_prefixq(Address adr, bool is_map1 = false);\n+  int get_prefixq_rex2(Address adr, bool is_map1 = false);\n+  int get_prefixq(Address adr, Register reg, bool is_map1 = false);\n+  int get_prefixq_rex2(Address adr, Register reg, bool ismap1 = false);\n@@ -713,1 +768,1 @@\n-  void prefixq(Address adr, Register reg);\n+  void prefixq(Address adr, Register reg, bool is_map1 = false);\n@@ -715,0 +770,3 @@\n+  void prefixq_rex2(Address adr, XMMRegister src);\n+\n+  bool prefix_is_rex2(int prefix);\n@@ -716,2 +774,9 @@\n-  int prefixq_and_encode(int reg_enc);\n-  int prefixq_and_encode(int dst_enc, int src_enc);\n+  int prefixq_and_encode(int reg_enc, bool is_map1 = false);\n+  int prefixq_and_encode_rex2(int reg_enc, bool is_map1 = false);\n+  int prefixq_and_encode(int dst_enc, int src_enc, bool is_map1 = false);\n+  int prefixq_and_encode_rex2(int dst_enc, int src_enc, bool is_map1 = false);\n+\n+  bool needs_rex2(Register reg1, Register reg2 = noreg, Register reg3 = noreg);\n+\n+  bool needs_eevex(Register reg1, Register reg2 = noreg, Register reg3 = noreg);\n+  bool needs_eevex(int enc1, int enc2 = -1, int enc3 = -1);\n@@ -726,2 +791,2 @@\n-  void evex_prefix(bool vex_r, bool vex_b, bool vex_x, bool evex_r, bool evex_v,\n-                   int nds_enc, VexSimdPrefix pre, VexOpcode opc);\n+  void evex_prefix(bool vex_r, bool vex_b, bool vex_x, bool evex_v, bool evex_r, bool evex_b,\n+                       bool eevex_x, int nds_enc, VexSimdPrefix pre, VexOpcode opc);\n@@ -729,2 +794,1 @@\n-  void vex_prefix(Address adr, int nds_enc, int xreg_enc,\n-                  VexSimdPrefix pre, VexOpcode opc,\n+  void vex_prefix(Address adr, int nds_enc, int xreg_enc, VexSimdPrefix pre, VexOpcode opc,\n@@ -735,1 +799,1 @@\n-                             InstructionAttr *attributes);\n+                             InstructionAttr *attributes, bool src_is_gpr = false);\n@@ -741,1 +805,1 @@\n-                             VexOpcode opc, InstructionAttr *attributes);\n+                             VexOpcode opc, InstructionAttr *attributes, bool src_is_gpr = false);\n@@ -826,0 +890,4 @@\n+  void emit_prefix_and_int8(int prefix, int b1);\n+  void emit_opcode_prefix_and_encoding(int byte1, int ocp_and_encoding);\n+  void emit_opcode_prefix_and_encoding(int byte1, int byte2, int ocp_and_encoding);\n+  void emit_opcode_prefix_and_encoding(int byte1, int byte2, int ocp_and_encoding, int byte3);\n@@ -912,0 +980,2 @@\n+  void prefix16(int p);\n+\n@@ -1099,0 +1169,1 @@\n+  void cdqe();\n@@ -1114,0 +1185,2 @@\n+  void cmpb(Address dst, Register reg);\n+  void cmpb(Register reg, Address dst);\n@@ -1120,0 +1193,1 @@\n+  void cmpl(Address dst,  Register reg);\n@@ -1128,0 +1202,1 @@\n+  void cmpw(Address dst, Register reg);\n@@ -1733,1 +1808,1 @@\n-  void nop(int i = 1);\n+  void nop(uint i = 1);\n@@ -1742,0 +1817,1 @@\n+  void btq(Register src, int imm8);\n@@ -1743,0 +1819,2 @@\n+  void btq(Register dst, Register src);\n+\n@@ -1807,0 +1885,1 @@\n+  void vpcmpeqb(XMMRegister dst, XMMRegister src1, Address src2, int vector_len);\n@@ -1821,0 +1900,1 @@\n+  void vpcmpeqw(XMMRegister dst, XMMRegister nds, Address src, int vector_len);\n@@ -2260,0 +2340,1 @@\n+  void xorw(Register dst, Address src);\n@@ -2311,0 +2392,1 @@\n+  void bzhil(Register dst, Register src1, Register src2);\n@@ -2613,0 +2695,1 @@\n+  void vpminub(XMMRegister dst, XMMRegister src1, XMMRegister src2, int vector_len);\n","filename":"src\/hotspot\/cpu\/x86\/assembler_x86.hpp","additions":106,"deletions":23,"binary":false,"changes":129,"status":"modified"},{"patch":"@@ -1032,1 +1032,7 @@\n-  CodeEmitInfo* info = state_for(x, x->state());\n+  CodeEmitInfo* info = nullptr;\n+  if (x->state_before() != nullptr && x->state_before()->force_reexecute()) {\n+    info = state_for(x, x->state_before());\n+    info->set_force_reexecute();\n+  } else {\n+    info = state_for(x, x->state());\n+  }\n@@ -1044,0 +1050,7 @@\n+  int flags;\n+  ciArrayKlass* expected_type;\n+  arraycopy_helper(x, &flags, &expected_type);\n+  if (x->check_flag(Instruction::OmitChecksFlag)) {\n+    flags = 0;\n+  }\n+\n@@ -1051,0 +1064,5 @@\n+\n+  if (expected_type != nullptr && flags == 0) {\n+    FrameMap* f = Compilation::current()->frame_map();\n+    f->update_reserved_argument_area_size(3 * BytesPerWord);\n+  }\n@@ -1072,4 +1090,0 @@\n-  int flags;\n-  ciArrayKlass* expected_type;\n-  arraycopy_helper(x, &flags, &expected_type);\n-\n@@ -1338,1 +1352,7 @@\n-  CodeEmitInfo* info = state_for(x, x->state());\n+  CodeEmitInfo* info = nullptr;\n+  if (x->state_before() != nullptr && x->state_before()->force_reexecute()) {\n+    info = state_for(x, x->state_before());\n+    info->set_force_reexecute();\n+  } else {\n+    info = state_for(x, x->state());\n+  }\n@@ -1355,1 +1375,1 @@\n-  __ allocate_array(reg, len, tmp1, tmp2, tmp3, tmp4, elem_type, klass_reg, slow_path, false);\n+  __ allocate_array(reg, len, tmp1, tmp2, tmp3, tmp4, elem_type, klass_reg, slow_path, x->zero_array());\n@@ -1390,1 +1410,1 @@\n-  __ allocate_array(reg, len, tmp1, tmp2, tmp3, tmp4, T_OBJECT, klass_reg, slow_path, is_null_free);\n+  __ allocate_array(reg, len, tmp1, tmp2, tmp3, tmp4, T_OBJECT, klass_reg, slow_path, true, x->is_null_free());\n","filename":"src\/hotspot\/cpu\/x86\/c1_LIRGenerator_x86.cpp","additions":28,"deletions":8,"binary":false,"changes":36,"status":"modified"},{"patch":"@@ -1054,2 +1054,3 @@\n-  jccb(Assembler::zero, zf_correct);\n-  stop(\"Fast Lock ZF != 1\");\n+  Label zf_bad_zero;\n+  jcc(Assembler::zero, zf_correct);\n+  jmp(zf_bad_zero);\n@@ -1061,1 +1062,1 @@\n-  jccb(Assembler::notZero, zf_correct);\n+  jcc(Assembler::notZero, zf_correct);\n@@ -1063,0 +1064,2 @@\n+  bind(zf_bad_zero);\n+  stop(\"Fast Lock ZF != 1\");\n@@ -1193,1 +1196,1 @@\n-  jccb(Assembler::zero, zf_correct);\n+  jcc(Assembler::zero, zf_correct);\n@@ -1825,0 +1828,124 @@\n+#ifdef _LP64\n+void C2_MacroAssembler::vgather8b_masked_offset(BasicType elem_bt,\n+                                                XMMRegister dst, Register base,\n+                                                Register idx_base,\n+                                                Register offset, Register mask,\n+                                                Register mask_idx, Register rtmp,\n+                                                int vlen_enc) {\n+  vpxor(dst, dst, dst, vlen_enc);\n+  if (elem_bt == T_SHORT) {\n+    for (int i = 0; i < 4; i++) {\n+      \/\/ dst[i] = mask[i] ? src[offset + idx_base[i]] : 0\n+      Label skip_load;\n+      btq(mask, mask_idx);\n+      jccb(Assembler::carryClear, skip_load);\n+      movl(rtmp, Address(idx_base, i * 4));\n+      if (offset != noreg) {\n+        addl(rtmp, offset);\n+      }\n+      pinsrw(dst, Address(base, rtmp, Address::times_2), i);\n+      bind(skip_load);\n+      incq(mask_idx);\n+    }\n+  } else {\n+    assert(elem_bt == T_BYTE, \"\");\n+    for (int i = 0; i < 8; i++) {\n+      \/\/ dst[i] = mask[i] ? src[offset + idx_base[i]] : 0\n+      Label skip_load;\n+      btq(mask, mask_idx);\n+      jccb(Assembler::carryClear, skip_load);\n+      movl(rtmp, Address(idx_base, i * 4));\n+      if (offset != noreg) {\n+        addl(rtmp, offset);\n+      }\n+      pinsrb(dst, Address(base, rtmp), i);\n+      bind(skip_load);\n+      incq(mask_idx);\n+    }\n+  }\n+}\n+#endif \/\/ _LP64\n+\n+void C2_MacroAssembler::vgather8b_offset(BasicType elem_bt, XMMRegister dst,\n+                                         Register base, Register idx_base,\n+                                         Register offset, Register rtmp,\n+                                         int vlen_enc) {\n+  vpxor(dst, dst, dst, vlen_enc);\n+  if (elem_bt == T_SHORT) {\n+    for (int i = 0; i < 4; i++) {\n+      \/\/ dst[i] = src[offset + idx_base[i]]\n+      movl(rtmp, Address(idx_base, i * 4));\n+      if (offset != noreg) {\n+        addl(rtmp, offset);\n+      }\n+      pinsrw(dst, Address(base, rtmp, Address::times_2), i);\n+    }\n+  } else {\n+    assert(elem_bt == T_BYTE, \"\");\n+    for (int i = 0; i < 8; i++) {\n+      \/\/ dst[i] = src[offset + idx_base[i]]\n+      movl(rtmp, Address(idx_base, i * 4));\n+      if (offset != noreg) {\n+        addl(rtmp, offset);\n+      }\n+      pinsrb(dst, Address(base, rtmp), i);\n+    }\n+  }\n+}\n+\n+\/*\n+ * Gather using hybrid algorithm, first partially unroll scalar loop\n+ * to accumulate values from gather indices into a quad-word(64bit) slice.\n+ * A slice may hold 8 bytes or 4 short values. This is followed by a vector\n+ * permutation to place the slice into appropriate vector lane\n+ * locations in destination vector. Following pseudo code describes the\n+ * algorithm in detail:\n+ *\n+ * DST_VEC = ZERO_VEC\n+ * PERM_INDEX = {0, 1, 2, 3, 4, 5, 6, 7, 8..}\n+ * TWO_VEC    = {2, 2, 2, 2, 2, 2, 2, 2, 2..}\n+ * FOREACH_ITER:\n+ *     TMP_VEC_64 = PICK_SUB_WORDS_FROM_GATHER_INDICES\n+ *     TEMP_PERM_VEC = PERMUTE TMP_VEC_64 PERM_INDEX\n+ *     DST_VEC = DST_VEC OR TEMP_PERM_VEC\n+ *     PERM_INDEX = PERM_INDEX - TWO_VEC\n+ *\n+ * With each iteration, doubleword permute indices (0,1) corresponding\n+ * to gathered quadword gets right shifted by two lane positions.\n+ *\n+ *\/\n+void C2_MacroAssembler::vgather_subword(BasicType elem_ty, XMMRegister dst,\n+                                        Register base, Register idx_base,\n+                                        Register offset, Register mask,\n+                                        XMMRegister xtmp1, XMMRegister xtmp2,\n+                                        XMMRegister temp_dst, Register rtmp,\n+                                        Register mask_idx, Register length,\n+                                        int vector_len, int vlen_enc) {\n+  Label GATHER8_LOOP;\n+  assert(is_subword_type(elem_ty), \"\");\n+  movl(length, vector_len);\n+  vpxor(xtmp1, xtmp1, xtmp1, vlen_enc); \/\/ xtmp1 = {0, ...}\n+  vpxor(dst, dst, dst, vlen_enc); \/\/ dst = {0, ...}\n+  vallones(xtmp2, vlen_enc);\n+  vpsubd(xtmp2, xtmp1, xtmp2, vlen_enc);\n+  vpslld(xtmp2, xtmp2, 1, vlen_enc); \/\/ xtmp2 = {2, 2, ...}\n+  load_iota_indices(xtmp1, vector_len * type2aelembytes(elem_ty), T_INT); \/\/ xtmp1 = {0, 1, 2, ...}\n+\n+  bind(GATHER8_LOOP);\n+    \/\/ TMP_VEC_64(temp_dst) = PICK_SUB_WORDS_FROM_GATHER_INDICES\n+    if (mask == noreg) {\n+      vgather8b_offset(elem_ty, temp_dst, base, idx_base, offset, rtmp, vlen_enc);\n+    } else {\n+      LP64_ONLY(vgather8b_masked_offset(elem_ty, temp_dst, base, idx_base, offset, mask, mask_idx, rtmp, vlen_enc));\n+    }\n+    \/\/ TEMP_PERM_VEC(temp_dst) = PERMUTE TMP_VEC_64(temp_dst) PERM_INDEX(xtmp1)\n+    vpermd(temp_dst, xtmp1, temp_dst, vlen_enc == Assembler::AVX_512bit ? vlen_enc : Assembler::AVX_256bit);\n+    \/\/ PERM_INDEX(xtmp1) = PERM_INDEX(xtmp1) - TWO_VEC(xtmp2)\n+    vpsubd(xtmp1, xtmp1, xtmp2, vlen_enc);\n+    \/\/ DST_VEC = DST_VEC OR TEMP_PERM_VEC\n+    vpor(dst, dst, temp_dst, vlen_enc);\n+    addptr(idx_base,  32 >> (type2aelembytes(elem_ty) - 1));\n+    subl(length, 8 >> (type2aelembytes(elem_ty) - 1));\n+    jcc(Assembler::notEqual, GATHER8_LOOP);\n+}\n+\n@@ -5536,1 +5663,0 @@\n-#ifdef _LP64\n@@ -5570,1 +5696,0 @@\n-#endif\n@@ -6533,2 +6658,2 @@\n-    case Op_MaxHF: eminsh(dst, src1, src2); break;\n-    case Op_MinHF: emaxsh(dst, src1, src2); break;\n+    case Op_MaxHF: emaxsh(dst, src1, src2); break;\n+    case Op_MinHF: eminsh(dst, src1, src2); break;\n@@ -6545,2 +6670,2 @@\n-    case Op_MaxVHF: evminph(dst, src1, src2, vlen_enc); break;\n-    case Op_MinVHF: evmaxph(dst, src1, src2, vlen_enc); break;\n+    case Op_MaxVHF: evmaxph(dst, src1, src2, vlen_enc); break;\n+    case Op_MinVHF: evminph(dst, src1, src2, vlen_enc); break;\n@@ -6557,2 +6682,2 @@\n-    case Op_MaxVHF: evminph(dst, src1, src2, vlen_enc); break;\n-    case Op_MinVHF: evmaxph(dst, src1, src2, vlen_enc); break;\n+    case Op_MaxVHF: evmaxph(dst, src1, src2, vlen_enc); break;\n+    case Op_MinVHF: evminph(dst, src1, src2, vlen_enc); break;\n","filename":"src\/hotspot\/cpu\/x86\/c2_MacroAssembler_x86.cpp","additions":137,"deletions":12,"binary":false,"changes":149,"status":"modified"},{"patch":"@@ -510,0 +510,11 @@\n+  void vgather_subword(BasicType elem_ty, XMMRegister dst,  Register base, Register idx_base, Register offset,\n+                       Register mask, XMMRegister xtmp1, XMMRegister xtmp2, XMMRegister xtmp3, Register rtmp,\n+                       Register midx, Register length, int vector_len, int vlen_enc);\n+\n+#ifdef _LP64\n+  void vgather8b_masked_offset(BasicType elem_bt, XMMRegister dst, Register base, Register idx_base,\n+                               Register offset, Register mask, Register midx, Register rtmp, int vlen_enc);\n+#endif\n+  void vgather8b_offset(BasicType elem_bt, XMMRegister dst, Register base, Register idx_base,\n+                              Register offset, Register rtmp, int vlen_enc);\n+\n","filename":"src\/hotspot\/cpu\/x86\/c2_MacroAssembler_x86.hpp","additions":11,"deletions":0,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -1007,0 +1007,8 @@\n+  \/\/ APX support not enabled yet\n+  if (UseAPX) {\n+    if (!FLAG_IS_DEFAULT(UseAPX)) {\n+        warning(\"APX is not supported on this CPU.\");\n+    }\n+    FLAG_SET_DEFAULT(UseAPX, false);\n+  }\n+\n@@ -1371,0 +1379,12 @@\n+#ifdef _LP64\n+  if (supports_avx512ifma() && supports_avx512vlbw()) {\n+    if (FLAG_IS_DEFAULT(UseIntPolyIntrinsics)) {\n+      FLAG_SET_DEFAULT(UseIntPolyIntrinsics, true);\n+    }\n+  } else\n+#endif\n+  if (UseIntPolyIntrinsics) {\n+    warning(\"Intrinsics for Polynomial crypto functions not available on this CPU.\");\n+    FLAG_SET_DEFAULT(UseIntPolyIntrinsics, false);\n+  }\n+\n@@ -2956,0 +2976,2 @@\n+    if (sef_cpuid7_ecx.bits.gfni != 0)\n+        result |= CPU_GFNI;\n@@ -2981,2 +3003,0 @@\n-      if (sef_cpuid7_ecx.bits.gfni != 0)\n-        result |= CPU_GFNI;\n","filename":"src\/hotspot\/cpu\/x86\/vm_version_x86.cpp","additions":22,"deletions":2,"binary":false,"changes":24,"status":"modified"},{"patch":"@@ -795,0 +795,5 @@\n+  \/\/ x86_64 supports secondary supers table\n+  constexpr static bool supports_secondary_supers_table() {\n+    return LP64_ONLY(true) NOT_LP64(false); \/\/ not implemented on x86_32\n+  }\n+\n","filename":"src\/hotspot\/cpu\/x86\/vm_version_x86.hpp","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -1190,2 +1190,2 @@\n-  static int emit_exception_handler(CodeBuffer &cbuf);\n-  static int emit_deopt_handler(CodeBuffer& cbuf);\n+  static int emit_exception_handler(C2_MacroAssembler *masm);\n+  static int emit_deopt_handler(C2_MacroAssembler* masm);\n@@ -1309,1 +1309,1 @@\n-int HandlerImpl::emit_exception_handler(CodeBuffer& cbuf) {\n+int HandlerImpl::emit_exception_handler(C2_MacroAssembler* masm) {\n@@ -1313,1 +1313,0 @@\n-  C2_MacroAssembler _masm(&cbuf);\n@@ -1327,1 +1326,1 @@\n-int HandlerImpl::emit_deopt_handler(CodeBuffer& cbuf) {\n+int HandlerImpl::emit_deopt_handler(C2_MacroAssembler* masm) {\n@@ -1331,1 +1330,0 @@\n-  C2_MacroAssembler _masm(&cbuf);\n@@ -1588,0 +1586,1 @@\n+    case Op_LoadVectorGatherMasked:\n@@ -1779,1 +1778,0 @@\n-    case Op_ClearArray:\n@@ -1938,0 +1936,11 @@\n+      if (!is_subword_type(bt) && size_in_bits < 512 && !VM_Version::supports_avx512vl()) {\n+        return false;\n+      }\n+      if (is_subword_type(bt) &&\n+         (!is_LP64                                                ||\n+         (size_in_bits > 256 && !VM_Version::supports_avx512bw()) ||\n+         (size_in_bits < 64)                                      ||\n+         (bt == T_SHORT && !VM_Version::supports_bmi2()))) {\n+        return false;\n+      }\n+      break;\n@@ -1947,1 +1956,4 @@\n-      if (size_in_bits == 64 ) {\n+      if (!is_subword_type(bt) && size_in_bits == 64) {\n+        return false;\n+      }\n+      if (is_subword_type(bt) && size_in_bits < 64) {\n@@ -2553,1 +2565,1 @@\n-static void vec_mov_helper(CodeBuffer *cbuf, int src_lo, int dst_lo,\n+static void vec_mov_helper(C2_MacroAssembler *masm, int src_lo, int dst_lo,\n@@ -2559,2 +2571,1 @@\n-  if (cbuf) {\n-    C2_MacroAssembler _masm(cbuf);\n+  if (masm) {\n@@ -2611,1 +2622,1 @@\n-void vec_spill_helper(CodeBuffer *cbuf, bool is_load,\n+void vec_spill_helper(C2_MacroAssembler *masm, bool is_load,\n@@ -2613,2 +2624,1 @@\n-  if (cbuf) {\n-    C2_MacroAssembler _masm(cbuf);\n+  if (masm) {\n@@ -2772,2 +2782,1 @@\n-  void MachNopNode::emit(CodeBuffer &cbuf, PhaseRegAlloc*) const {\n-    C2_MacroAssembler _masm(&cbuf);\n+  void MachNopNode::emit(C2_MacroAssembler *masm, PhaseRegAlloc*) const {\n@@ -2787,2 +2796,1 @@\n-  void MachBreakpointNode::emit(CodeBuffer &cbuf, PhaseRegAlloc* ra_) const {\n-    C2_MacroAssembler _masm(&cbuf);\n+  void MachBreakpointNode::emit(C2_MacroAssembler *masm, PhaseRegAlloc* ra_) const {\n@@ -2801,1 +2809,0 @@\n-    C2_MacroAssembler _masm(&cbuf);\n@@ -2813,1 +2820,0 @@\n-      C2_MacroAssembler _masm(&cbuf);\n@@ -4095,1 +4101,1 @@\n-\/\/ Gather INT, LONG, FLOAT, DOUBLE\n+\/\/ Gather BYTE, SHORT, INT, LONG, FLOAT, DOUBLE\n@@ -4098,1 +4104,2 @@\n-  predicate(!VM_Version::supports_avx512vl() && Matcher::vector_length_in_bytes(n) <= 32);\n+  predicate(!VM_Version::supports_avx512vl() && !is_subword_type(Matcher::vector_element_basic_type(n)) &&\n+            Matcher::vector_length_in_bytes(n) <= 32);\n@@ -4115,1 +4122,2 @@\n-  predicate(VM_Version::supports_avx512vl() || Matcher::vector_length_in_bytes(n) == 64);\n+  predicate((VM_Version::supports_avx512vl() || Matcher::vector_length_in_bytes(n) == 64) &&\n+            !is_subword_type(Matcher::vector_element_basic_type(n)));\n@@ -4130,1 +4138,2 @@\n-  predicate(VM_Version::supports_avx512vl() || Matcher::vector_length_in_bytes(n) == 64);\n+  predicate((VM_Version::supports_avx512vl() || Matcher::vector_length_in_bytes(n) == 64) &&\n+            !is_subword_type(Matcher::vector_element_basic_type(n)));\n@@ -4148,0 +4157,232 @@\n+\n+instruct vgather_subwordLE8B(vec dst, memory mem, rRegP idx_base, immI_0 offset, rRegP tmp, rRegI rtmp) %{\n+  predicate(is_subword_type(Matcher::vector_element_basic_type(n)) && Matcher::vector_length_in_bytes(n) <= 8);\n+  match(Set dst (LoadVectorGather mem (Binary idx_base offset)));\n+  effect(TEMP tmp, TEMP rtmp);\n+  format %{ \"vector_gatherLE8 $dst, $mem, $idx_base\\t! using $tmp and $rtmp as TEMP\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    BasicType elem_bt = Matcher::vector_element_basic_type(this);\n+    __ lea($tmp$$Register, $mem$$Address);\n+    __ vgather8b_offset(elem_bt, $dst$$XMMRegister, $tmp$$Register, $idx_base$$Register, noreg, $rtmp$$Register, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vgather_subwordGT8B(vec dst, memory mem, rRegP idx_base, immI_0 offset, rRegP tmp, rRegP idx_base_temp,\n+                             vec xtmp1, vec xtmp2, vec xtmp3, rRegI rtmp, rRegI length, rFlagsReg cr) %{\n+  predicate(is_subword_type(Matcher::vector_element_basic_type(n)) && Matcher::vector_length_in_bytes(n) > 8);\n+  match(Set dst (LoadVectorGather mem (Binary idx_base offset)));\n+  effect(TEMP_DEF dst, TEMP tmp, TEMP idx_base_temp, TEMP xtmp1, TEMP xtmp2, TEMP xtmp3, TEMP rtmp, TEMP length, KILL cr);\n+  format %{ \"vector_gatherGT8 $dst, $mem, $idx_base\\t! using $tmp, $idx_base_temp, $xtmp1, $xtmp2, $xtmp3, $rtmp and $length as TEMP\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    int vector_len = Matcher::vector_length(this);\n+    BasicType elem_bt = Matcher::vector_element_basic_type(this);\n+    __ lea($tmp$$Register, $mem$$Address);\n+    __ movptr($idx_base_temp$$Register, $idx_base$$Register);\n+    __ vgather_subword(elem_bt, $dst$$XMMRegister, $tmp$$Register, $idx_base_temp$$Register, noreg, noreg, $xtmp1$$XMMRegister,\n+                       $xtmp2$$XMMRegister, $xtmp3$$XMMRegister, $rtmp$$Register, noreg, $length$$Register, vector_len, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vgather_subwordLE8B_off(vec dst, memory mem, rRegP idx_base, rRegI offset, rRegP tmp, rRegI rtmp, rFlagsReg cr) %{\n+  predicate(is_subword_type(Matcher::vector_element_basic_type(n)) && Matcher::vector_length_in_bytes(n) <= 8);\n+  match(Set dst (LoadVectorGather mem (Binary idx_base offset)));\n+  effect(TEMP tmp, TEMP rtmp, KILL cr);\n+  format %{ \"vector_gatherLE8_off $dst, $mem, $idx_base, $offset\\t! using $tmp and $rtmp as TEMP\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    BasicType elem_bt = Matcher::vector_element_basic_type(this);\n+    __ lea($tmp$$Register, $mem$$Address);\n+    __ vgather8b_offset(elem_bt, $dst$$XMMRegister, $tmp$$Register, $idx_base$$Register, $offset$$Register, $rtmp$$Register, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+\n+instruct vgather_subwordGT8B_off(vec dst, memory mem, rRegP idx_base, rRegI offset, rRegP tmp, rRegP idx_base_temp,\n+                                 vec xtmp1, vec xtmp2, vec xtmp3, rRegI rtmp, rRegI length, rFlagsReg cr) %{\n+  predicate(is_subword_type(Matcher::vector_element_basic_type(n)) && Matcher::vector_length_in_bytes(n) > 8);\n+  match(Set dst (LoadVectorGather mem (Binary idx_base offset)));\n+  effect(TEMP_DEF dst, TEMP tmp, TEMP idx_base_temp, TEMP xtmp1, TEMP xtmp2, TEMP xtmp3, TEMP rtmp, TEMP length, KILL cr);\n+  format %{ \"vector_gatherGT8_off $dst, $mem, $idx_base, $offset\\t! using $tmp, $idx_base_temp, $xtmp1, $xtmp2, $xtmp3, $rtmp and $length as TEMP\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    int vector_len = Matcher::vector_length(this);\n+    BasicType elem_bt = Matcher::vector_element_basic_type(this);\n+    __ lea($tmp$$Register, $mem$$Address);\n+    __ movptr($idx_base_temp$$Register, $idx_base$$Register);\n+    __ vgather_subword(elem_bt, $dst$$XMMRegister, $tmp$$Register, $idx_base_temp$$Register, $offset$$Register, noreg, $xtmp1$$XMMRegister,\n+                       $xtmp2$$XMMRegister, $xtmp3$$XMMRegister, $rtmp$$Register, noreg, $length$$Register, vector_len, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+\n+#ifdef _LP64\n+instruct vgather_masked_subwordLE8B_avx3(vec dst, memory mem, rRegP idx_base, immI_0 offset, kReg mask, rRegL mask_idx, rRegP tmp, rRegI rtmp, rRegL rtmp2, rFlagsReg cr) %{\n+  predicate(VM_Version::supports_avx512bw() && is_subword_type(Matcher::vector_element_basic_type(n)) && Matcher::vector_length_in_bytes(n) <= 8);\n+  match(Set dst (LoadVectorGatherMasked mem (Binary idx_base (Binary mask offset))));\n+  effect(TEMP mask_idx, TEMP tmp, TEMP rtmp, TEMP rtmp2, KILL cr);\n+  format %{ \"vector_masked_gatherLE8 $dst, $mem, $idx_base, $mask\\t! using $mask_idx, $tmp, $rtmp and $rtmp2 as TEMP\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    BasicType elem_bt = Matcher::vector_element_basic_type(this);\n+    __ xorq($mask_idx$$Register, $mask_idx$$Register);\n+    __ lea($tmp$$Register, $mem$$Address);\n+    __ kmovql($rtmp2$$Register, $mask$$KRegister);\n+    __ vgather8b_masked_offset(elem_bt, $dst$$XMMRegister, $tmp$$Register, $idx_base$$Register, noreg, $rtmp2$$Register, $mask_idx$$Register, $rtmp$$Register, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vgather_masked_subwordGT8B_avx3(vec dst, memory mem, rRegP idx_base, immI_0 offset, kReg mask, rRegP tmp, rRegP idx_base_temp,\n+                                         vec xtmp1, vec xtmp2, vec xtmp3, rRegI rtmp, rRegL rtmp2, rRegL mask_idx, rRegI length, rFlagsReg cr) %{\n+  predicate(VM_Version::supports_avx512bw() && is_subword_type(Matcher::vector_element_basic_type(n)) && Matcher::vector_length_in_bytes(n) > 8);\n+  match(Set dst (LoadVectorGatherMasked mem (Binary idx_base (Binary mask offset))));\n+  effect(TEMP_DEF dst, TEMP tmp, TEMP idx_base_temp, TEMP xtmp1, TEMP xtmp2, TEMP xtmp3, TEMP rtmp, TEMP rtmp2, TEMP mask_idx, TEMP length, KILL cr);\n+  format %{ \"vector_gatherGT8_masked $dst, $mem, $idx_base, $mask\\t! using $tmp, $idx_base_temp, $xtmp1, $xtmp2, $xtmp3, $rtmp, $rtmp2, $mask_idx and $length as TEMP\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    int vector_len = Matcher::vector_length(this);\n+    BasicType elem_bt = Matcher::vector_element_basic_type(this);\n+    __ xorq($mask_idx$$Register, $mask_idx$$Register);\n+    __ lea($tmp$$Register, $mem$$Address);\n+    __ movptr($idx_base_temp$$Register, $idx_base$$Register);\n+    __ kmovql($rtmp2$$Register, $mask$$KRegister);\n+    __ vgather_subword(elem_bt, $dst$$XMMRegister, $tmp$$Register, $idx_base_temp$$Register, noreg, $rtmp2$$Register, $xtmp1$$XMMRegister,\n+                       $xtmp2$$XMMRegister, $xtmp3$$XMMRegister, $rtmp$$Register, $mask_idx$$Register, $length$$Register, vector_len, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vgather_masked_subwordLE8B_off_avx3(vec dst, memory mem, rRegP idx_base, rRegI offset, kReg mask, rRegL mask_idx, rRegP tmp, rRegI rtmp, rRegL rtmp2, rFlagsReg cr) %{\n+  predicate(VM_Version::supports_avx512bw() && is_subword_type(Matcher::vector_element_basic_type(n)) && Matcher::vector_length_in_bytes(n) <= 8);\n+  match(Set dst (LoadVectorGatherMasked mem (Binary idx_base (Binary mask offset))));\n+  effect(TEMP mask_idx, TEMP tmp, TEMP rtmp, TEMP rtmp2, KILL cr);\n+  format %{ \"vector_masked_gatherLE8_off $dst, $mem, $idx_base, $offset, $mask\\t! using $mask_idx, $tmp, $rtmp and $rtmp2 as TEMP\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    BasicType elem_bt = Matcher::vector_element_basic_type(this);\n+    __ xorq($mask_idx$$Register, $mask_idx$$Register);\n+    __ lea($tmp$$Register, $mem$$Address);\n+    __ kmovql($rtmp2$$Register, $mask$$KRegister);\n+    __ vgather8b_masked_offset(elem_bt, $dst$$XMMRegister, $tmp$$Register, $idx_base$$Register, $offset$$Register,\n+                                $rtmp2$$Register, $mask_idx$$Register, $rtmp$$Register, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vgather_masked_subwordGT8B_off_avx3(vec dst, memory mem, rRegP idx_base, rRegI offset, kReg mask, rRegP tmp, rRegP idx_base_temp,\n+                                             vec xtmp1, vec xtmp2, vec xtmp3, rRegI rtmp, rRegL rtmp2, rRegL mask_idx, rRegI length, rFlagsReg cr) %{\n+  predicate(VM_Version::supports_avx512bw() && is_subword_type(Matcher::vector_element_basic_type(n)) && Matcher::vector_length_in_bytes(n) > 8);\n+  match(Set dst (LoadVectorGatherMasked mem (Binary idx_base (Binary mask offset))));\n+  effect(TEMP_DEF dst, TEMP tmp, TEMP idx_base_temp, TEMP xtmp1, TEMP xtmp2, TEMP xtmp3, TEMP rtmp, TEMP rtmp2, TEMP mask_idx, TEMP length, KILL cr);\n+  format %{ \"vector_gatherGT8_masked_off $dst, $mem, $idx_base, $offset, $mask\\t! using $tmp, $idx_base_temp, $xtmp1, $xtmp2, $xtmp3, $rtmp, $rtmp2, $mask_idx and $length as TEMP\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    int vector_len = Matcher::vector_length(this);\n+    BasicType elem_bt = Matcher::vector_element_basic_type(this);\n+    __ xorq($mask_idx$$Register, $mask_idx$$Register);\n+    __ lea($tmp$$Register, $mem$$Address);\n+    __ movptr($idx_base_temp$$Register, $idx_base$$Register);\n+    __ kmovql($rtmp2$$Register, $mask$$KRegister);\n+    __ vgather_subword(elem_bt, $dst$$XMMRegister, $tmp$$Register, $idx_base_temp$$Register, $offset$$Register, $rtmp2$$Register, $xtmp1$$XMMRegister,\n+                       $xtmp2$$XMMRegister, $xtmp3$$XMMRegister, $rtmp$$Register, $mask_idx$$Register, $length$$Register, vector_len, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vgather_masked_subwordLE8B_avx2(vec dst, memory mem, rRegP idx_base, immI_0 offset, vec mask, rRegI mask_idx, rRegP tmp, rRegI rtmp, rRegI rtmp2, rFlagsReg cr) %{\n+  predicate(!VM_Version::supports_avx512vlbw() && is_subword_type(Matcher::vector_element_basic_type(n)) && Matcher::vector_length_in_bytes(n) <= 8);\n+  match(Set dst (LoadVectorGatherMasked mem (Binary idx_base (Binary mask offset))));\n+  effect(TEMP mask_idx, TEMP tmp, TEMP rtmp, TEMP rtmp2, KILL cr);\n+  format %{ \"vector_masked_gatherLE8 $dst, $mem, $idx_base, $mask\\t! using $mask_idx, $tmp, $rtmp and $rtmp2 as TEMP\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    BasicType elem_bt = Matcher::vector_element_basic_type(this);\n+    __ lea($tmp$$Register, $mem$$Address);\n+    __ vpmovmskb($rtmp2$$Register, $mask$$XMMRegister, vlen_enc);\n+    if (elem_bt == T_SHORT) {\n+      __ movl($mask_idx$$Register, 0x55555555);\n+      __ pextl($rtmp2$$Register, $rtmp2$$Register, $mask_idx$$Register);\n+    }\n+    __ xorl($mask_idx$$Register, $mask_idx$$Register);\n+    __ vgather8b_masked_offset(elem_bt, $dst$$XMMRegister, $tmp$$Register, $idx_base$$Register, noreg, $rtmp2$$Register, $mask_idx$$Register, $rtmp$$Register, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vgather_masked_subwordGT8B_avx2(vec dst, memory mem, rRegP idx_base, immI_0 offset, vec mask, rRegP tmp, rRegP idx_base_temp,\n+                                         vec xtmp1, vec xtmp2, vec xtmp3, rRegI rtmp, rRegI rtmp2, rRegI mask_idx, rRegI length, rFlagsReg cr) %{\n+  predicate(!VM_Version::supports_avx512vlbw() && is_subword_type(Matcher::vector_element_basic_type(n)) && Matcher::vector_length_in_bytes(n) > 8);\n+  match(Set dst (LoadVectorGatherMasked mem (Binary idx_base (Binary mask offset))));\n+  effect(TEMP_DEF dst, TEMP tmp, TEMP idx_base_temp, TEMP xtmp1, TEMP xtmp2, TEMP xtmp3, TEMP rtmp, TEMP rtmp2, TEMP mask_idx, TEMP length, KILL cr);\n+  format %{ \"vector_gatherGT8_masked $dst, $mem, $idx_base, $mask\\t! using $tmp, $idx_base_temp, $xtmp1, $xtmp2, $xtmp3, $rtmp, $rtmp2, $mask_idx and $length as TEMP\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    int vector_len = Matcher::vector_length(this);\n+    BasicType elem_bt = Matcher::vector_element_basic_type(this);\n+    __ lea($tmp$$Register, $mem$$Address);\n+    __ movptr($idx_base_temp$$Register, $idx_base$$Register);\n+    __ vpmovmskb($rtmp2$$Register, $mask$$XMMRegister, vlen_enc);\n+    if (elem_bt == T_SHORT) {\n+      __ movl($mask_idx$$Register, 0x55555555);\n+      __ pextl($rtmp2$$Register, $rtmp2$$Register, $mask_idx$$Register);\n+    }\n+    __ xorl($mask_idx$$Register, $mask_idx$$Register);\n+    __ vgather_subword(elem_bt, $dst$$XMMRegister, $tmp$$Register, $idx_base_temp$$Register, noreg, $rtmp2$$Register, $xtmp1$$XMMRegister,\n+                       $xtmp2$$XMMRegister, $xtmp3$$XMMRegister, $rtmp$$Register, $mask_idx$$Register, $length$$Register, vector_len, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vgather_masked_subwordLE8B_off_avx2(vec dst, memory mem, rRegP idx_base, rRegI offset, vec mask, rRegI mask_idx, rRegP tmp, rRegI rtmp, rRegI rtmp2, rFlagsReg cr) %{\n+  predicate(!VM_Version::supports_avx512vlbw() && is_subword_type(Matcher::vector_element_basic_type(n)) && Matcher::vector_length_in_bytes(n) <= 8);\n+  match(Set dst (LoadVectorGatherMasked mem (Binary idx_base (Binary mask offset))));\n+  effect(TEMP mask_idx, TEMP tmp, TEMP rtmp, TEMP rtmp2, KILL cr);\n+  format %{ \"vector_masked_gatherLE8_off $dst, $mem, $idx_base, $offset, $mask\\t! using $mask_idx, $tmp, $rtmp and $rtmp2 as TEMP\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    BasicType elem_bt = Matcher::vector_element_basic_type(this);\n+    __ lea($tmp$$Register, $mem$$Address);\n+    __ vpmovmskb($rtmp2$$Register, $mask$$XMMRegister, vlen_enc);\n+    if (elem_bt == T_SHORT) {\n+      __ movl($mask_idx$$Register, 0x55555555);\n+      __ pextl($rtmp2$$Register, $rtmp2$$Register, $mask_idx$$Register);\n+    }\n+    __ xorl($mask_idx$$Register, $mask_idx$$Register);\n+    __ vgather8b_masked_offset(elem_bt, $dst$$XMMRegister, $tmp$$Register, $idx_base$$Register, $offset$$Register,\n+                                $rtmp2$$Register, $mask_idx$$Register, $rtmp$$Register, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vgather_masked_subwordGT8B_off_avx2(vec dst, memory mem, rRegP idx_base, rRegI offset, vec mask, rRegP tmp, rRegP idx_base_temp,\n+                                             vec xtmp1, vec xtmp2, vec xtmp3, rRegI rtmp, rRegI rtmp2, rRegI mask_idx, rRegI length, rFlagsReg cr) %{\n+  predicate(!VM_Version::supports_avx512vlbw() && is_subword_type(Matcher::vector_element_basic_type(n)) && Matcher::vector_length_in_bytes(n) > 8);\n+  match(Set dst (LoadVectorGatherMasked mem (Binary idx_base (Binary mask offset))));\n+  effect(TEMP_DEF dst, TEMP tmp, TEMP idx_base_temp, TEMP xtmp1, TEMP xtmp2, TEMP xtmp3, TEMP rtmp, TEMP rtmp2, TEMP mask_idx, TEMP length, KILL cr);\n+  format %{ \"vector_gatherGT8_masked_off $dst, $mem, $idx_base, $offset, $mask\\t! using $tmp, $idx_base_temp, $xtmp1, $xtmp2, $xtmp3, $rtmp, $rtmp2, $mask_idx and $length as TEMP\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    int vector_len = Matcher::vector_length(this);\n+    BasicType elem_bt = Matcher::vector_element_basic_type(this);\n+    __ xorl($mask_idx$$Register, $mask_idx$$Register);\n+    __ lea($tmp$$Register, $mem$$Address);\n+    __ movptr($idx_base_temp$$Register, $idx_base$$Register);\n+    __ vpmovmskb($rtmp2$$Register, $mask$$XMMRegister, vlen_enc);\n+    if (elem_bt == T_SHORT) {\n+      __ movl($mask_idx$$Register, 0x55555555);\n+      __ pextl($rtmp2$$Register, $rtmp2$$Register, $mask_idx$$Register);\n+    }\n+    __ xorl($mask_idx$$Register, $mask_idx$$Register);\n+    __ vgather_subword(elem_bt, $dst$$XMMRegister, $tmp$$Register, $idx_base_temp$$Register, $offset$$Register, $rtmp2$$Register, $xtmp1$$XMMRegister,\n+                       $xtmp2$$XMMRegister, $xtmp3$$XMMRegister, $rtmp$$Register, $mask_idx$$Register, $length$$Register, vector_len, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+#endif\n+\n@@ -7056,0 +7297,1 @@\n+  predicate(VM_Version::supports_avx512vl() || Matcher::vector_element_basic_type(n) != T_DOUBLE);\n@@ -7059,2 +7301,0 @@\n-    assert(UseAVX > 0, \"required\");\n-\n@@ -7068,0 +7308,11 @@\n+instruct vcastBtoD(legVec dst, legVec src) %{\n+  predicate(!VM_Version::supports_avx512vl() && Matcher::vector_element_basic_type(n) == T_DOUBLE);\n+  match(Set dst (VectorCastB2X src));\n+  format %{ \"vector_cast_b2x $dst,$src\\t!\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    __ vconvert_b2x(T_DOUBLE, $dst$$XMMRegister, $src$$XMMRegister, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n@@ -10070,1 +10321,1 @@\n-    assert(0 == Type::cmp(mask1->bottom_type(), mask2->bottom_type()), \"\");\n+    assert(Type::equals(mask1->bottom_type(), mask2->bottom_type()), \"Mask types must be equal\");\n","filename":"src\/hotspot\/cpu\/x86\/x86.ad","additions":278,"deletions":27,"binary":false,"changes":305,"status":"modified"},{"patch":"@@ -218,2 +218,3 @@\n-  typedef ResourceHashtable<oop, CachedOopInfo,\n-      36137, \/\/ prime number\n+  static const int INITIAL_TABLE_SIZE = 15889; \/\/ prime number\n+  static const int MAX_TABLE_SIZE     = 1000000;\n+  typedef ResizeableResourceHashtable<oop, CachedOopInfo,\n@@ -280,2 +281,1 @@\n-  typedef ResourceHashtable<oop, bool,\n-      15889, \/\/ prime number\n+  typedef ResizeableResourceHashtable<oop, bool,\n@@ -303,1 +303,1 @@\n-    _seen_objects_table = new (mtClass)SeenObjectsTable();\n+    _seen_objects_table = new (mtClass)SeenObjectsTable(INITIAL_TABLE_SIZE, MAX_TABLE_SIZE);\n@@ -358,1 +358,1 @@\n-      new (mtClass)ArchivedObjectCache();\n+      new (mtClass)ArchivedObjectCache(INITIAL_TABLE_SIZE, MAX_TABLE_SIZE);\n@@ -383,3 +383,2 @@\n-  static bool has_oop_pointers(oop obj);\n-  static bool has_native_pointers(oop obj);\n-  static void set_has_native_pointers(oop obj);\n+  static void get_pointer_info(oop src_obj, bool& has_oop_pointers, bool& has_native_pointers);\n+  static void set_has_native_pointers(oop src_obj);\n@@ -440,2 +439,1 @@\n-  public ResourceHashtable<oop, bool,\n-                           15889, \/\/ prime number\n+  public ResizeableResourceHashtable<oop, bool,\n@@ -445,1 +443,8 @@\n-{};\n+{\n+public:\n+  DumpedInternedStrings(unsigned size, unsigned max_size) :\n+    ResizeableResourceHashtable<oop, bool,\n+                                AnyObj::C_HEAP,\n+                                mtClassShared,\n+                                HeapShared::string_oop_hash>(size, max_size) {}\n+};\n","filename":"src\/hotspot\/share\/cds\/heapShared.hpp","additions":17,"deletions":12,"binary":false,"changes":29,"status":"modified"},{"patch":"@@ -2937,5 +2937,0 @@\n-  if (name == vmSymbols::object_initializer_name() &&\n-      signature == vmSymbols::void_method_signature() &&\n-      m->is_vanilla_constructor()) {\n-    _has_vanilla_constructor = true;\n-  }\n@@ -4355,23 +4350,0 @@\n-  \/\/ Check if this klass has a vanilla default constructor\n-  if (super == nullptr) {\n-    \/\/ java.lang.Object has empty default constructor\n-    ik->set_has_vanilla_constructor();\n-  } else {\n-    if (super->has_vanilla_constructor() &&\n-        _has_vanilla_constructor) {\n-      ik->set_has_vanilla_constructor();\n-    }\n-#ifdef ASSERT\n-    bool v = false;\n-    if (super->has_vanilla_constructor()) {\n-      const Method* const constructor =\n-        ik->find_method(vmSymbols::object_initializer_name(),\n-                       vmSymbols::void_method_signature());\n-      if (constructor != nullptr && constructor->is_vanilla_constructor()) {\n-        v = true;\n-      }\n-    }\n-    assert(v == ik->has_vanilla_constructor(), \"inconsistent has_vanilla_constructor\");\n-#endif\n-  }\n-\n@@ -4381,2 +4353,1 @@\n-  if ((!RegisterFinalizersAtInit && ik->has_finalizer())\n-      || ik->is_abstract() || ik->is_interface()\n+  if (ik->is_abstract() || ik->is_interface()\n@@ -5695,1 +5666,1 @@\n-  \/\/ Fill in has_finalizer, has_vanilla_constructor, and layout_helper\n+  \/\/ Fill in has_finalizer and layout_helper\n@@ -5922,1 +5893,0 @@\n-  _has_vanilla_constructor(false),\n","filename":"src\/hotspot\/share\/classfile\/classFileParser.cpp","additions":2,"deletions":32,"binary":false,"changes":34,"status":"modified"},{"patch":"@@ -223,1 +223,0 @@\n-  bool _has_vanilla_constructor;\n","filename":"src\/hotspot\/share\/classfile\/classFileParser.hpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -557,1 +557,12 @@\n-   \/* support for java.util.Base64.Encoder*\/                                                                            \\\n+  \/* support for sun.security.util.math.intpoly.MontgomeryIntegerPolynomialP256 *\/                                      \\\n+  do_class(sun_security_util_math_intpoly_MontgomeryIntegerPolynomialP256, \"sun\/security\/util\/math\/intpoly\/MontgomeryIntegerPolynomialP256\")  \\\n+  do_intrinsic(_intpoly_montgomeryMult_P256, sun_security_util_math_intpoly_MontgomeryIntegerPolynomialP256, intPolyMult_name, intPolyMult_signature, F_R) \\\n+  do_name(intPolyMult_name, \"mult\")                                                                                     \\\n+  do_signature(intPolyMult_signature, \"([J[J[J)I\")                                                                      \\\n+                                                                                                                        \\\n+  do_class(sun_security_util_math_intpoly_IntegerPolynomial, \"sun\/security\/util\/math\/intpoly\/IntegerPolynomial\")        \\\n+  do_intrinsic(_intpoly_assign, sun_security_util_math_intpoly_IntegerPolynomial, intPolyAssign_name, intPolyAssign_signature, F_S) \\\n+   do_name(intPolyAssign_name, \"conditionalAssign\")                                                                     \\\n+   do_signature(intPolyAssign_signature, \"(I[J[J)V\")                                                                    \\\n+                                                                                                                        \\\n+  \/* support for java.util.Base64.Encoder*\/                                                                             \\\n@@ -653,0 +664,3 @@\n+  do_intrinsic(_setMemory,                jdk_internal_misc_Unsafe,     setMemory_name,  setMemory_signature,          F_RN)     \\\n+   do_name(     setMemory_name,                                         \"setMemory0\")                                            \\\n+   do_signature(setMemory_signature,                                    \"(Ljava\/lang\/Object;JJB)V\")                              \\\n","filename":"src\/hotspot\/share\/classfile\/vmIntrinsics.hpp","additions":15,"deletions":1,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -61,1 +61,0 @@\n-  template(java_lang_Package,                         \"java\/lang\/Package\")                        \\\n@@ -123,1 +122,0 @@\n-  template(java_lang_CharSequence,                    \"java\/lang\/CharSequence\")                   \\\n@@ -136,5 +134,0 @@\n-  template(java_io_OutputStream,                      \"java\/io\/OutputStream\")                     \\\n-  template(java_io_Reader,                            \"java\/io\/Reader\")                           \\\n-  template(java_io_BufferedReader,                    \"java\/io\/BufferedReader\")                   \\\n-  template(java_io_File,                              \"java\/io\/File\")                             \\\n-  template(java_io_FileInputStream,                   \"java\/io\/FileInputStream\")                  \\\n@@ -145,5 +138,0 @@\n-  template(java_util_Objects,                         \"java\/util\/Objects\")                        \\\n-  template(java_util_Vector,                          \"java\/util\/Vector\")                         \\\n-  template(java_util_AbstractList,                    \"java\/util\/AbstractList\")                   \\\n-  template(java_util_Hashtable,                       \"java\/util\/Hashtable\")                      \\\n-  template(java_lang_Compiler,                        \"java\/lang\/Compiler\")                       \\\n@@ -155,2 +143,0 @@\n-  template(getBootClassPathEntryForClass_name,        \"getBootClassPathEntryForClass\")            \\\n-  template(sun_net_www_ParseUtil,                     \"sun\/net\/www\/ParseUtil\")                    \\\n@@ -234,1 +220,0 @@\n-  template(java_lang_NoSuchFieldException,            \"java\/lang\/NoSuchFieldException\")           \\\n@@ -244,1 +229,0 @@\n-  template(java_security_PrivilegedActionException,   \"java\/security\/PrivilegedActionException\")  \\\n@@ -294,1 +278,0 @@\n-  template(checkedExceptions_name,                    \"checkedExceptions\")                        \\\n@@ -373,4 +356,0 @@\n-  template(setTargetNormal_name,                      \"setTargetNormal\")                          \\\n-  template(setTargetVolatile_name,                    \"setTargetVolatile\")                        \\\n-  template(setTarget_signature,                       \"(Ljava\/lang\/invoke\/MethodHandle;)V\")       \\\n-  template(DEFAULT_CONTEXT_name,                      \"DEFAULT_CONTEXT\")                          \\\n@@ -421,1 +400,0 @@\n-  template(reference_lock_name,                       \"lock\")                                     \\\n@@ -436,2 +414,0 @@\n-  template(getStacks_name,                            \"getStacks\")                                \\\n-  template(onPinned_name,                             \"onPinned0\")                                \\\n@@ -442,1 +418,1 @@\n-  template(argsize_name,                              \"argsize\")                                  \\\n+  template(bottom_name,                               \"bottom\")                                   \\\n@@ -445,1 +421,0 @@\n-  template(numOops_name,                              \"numOops\")                                  \\\n@@ -451,1 +426,0 @@\n-  template(numInterpretedFrames_name,                 \"numInterpretedFrames\")                     \\\n@@ -457,2 +431,0 @@\n-  template(refStack_name,                             \"refStack\")                                 \\\n-  template(refSP_name,                                \"refSP\")                                    \\\n@@ -464,1 +436,0 @@\n-  template(deadChild_name,                            \"deadChild\")                                \\\n@@ -480,1 +451,0 @@\n-  template(newInstance0_name,                         \"newInstance0\")                             \\\n@@ -502,1 +472,0 @@\n-  template(vmcount_name,                              \"vmcount\")                                  \\\n@@ -572,1 +541,0 @@\n-  template(bool_bool_void_signature,                  \"(ZZ)V\")                                    \\\n@@ -601,2 +569,0 @@\n-  template(int_array_signature,                       \"[I\")                                       \\\n-  template(long_array_signature,                      \"[J\")                                       \\\n@@ -607,1 +573,0 @@\n-  template(vthread_signature,                         \"Ljava\/lang\/VirtualThread;\")                \\\n@@ -614,2 +579,0 @@\n-  template(string_int_signature,                      \"(Ljava\/lang\/String;)I\")                    \\\n-  template(string_byte_array_signature,               \"(Ljava\/lang\/String;)[B\")                   \\\n@@ -626,4 +589,0 @@\n-  template(throwable_string_void_signature,           \"(Ljava\/lang\/Throwable;Ljava\/lang\/String;)V\")               \\\n-  template(string_array_void_signature,               \"([Ljava\/lang\/String;)V\")                                   \\\n-  template(string_array_string_array_void_signature,  \"([Ljava\/lang\/String;[Ljava\/lang\/String;)V\")                \\\n-  template(thread_throwable_void_signature,           \"(Ljava\/lang\/Thread;Ljava\/lang\/Throwable;)V\")               \\\n@@ -638,2 +597,1 @@\n-  template(object_object_boolean_signature,           \"(Ljava\/lang\/Object;Ljava\/lang\/Object;)Z\") \\\n-  template(string_string_string_signature,            \"(Ljava\/lang\/String;Ljava\/lang\/String;)Ljava\/lang\/String;\") \\\n+  template(object_object_boolean_signature,           \"(Ljava\/lang\/Object;Ljava\/lang\/Object;)Z\")                  \\\n@@ -643,2 +601,0 @@\n-  template(char_array_void_signature,                 \"([C)V\")                                                    \\\n-  template(int_int_void_signature,                    \"(II)V\")                                                    \\\n@@ -653,1 +609,0 @@\n-  template(void_module_signature,                     \"()Ljava\/lang\/Module;\")                                     \\\n@@ -656,1 +611,0 @@\n-  template(exception_void_signature,                  \"(Ljava\/lang\/Exception;)V\")                                 \\\n@@ -662,1 +616,0 @@\n-  template(thread_array_signature,                    \"[Ljava\/lang\/Thread;\")                                      \\\n@@ -674,1 +627,0 @@\n-  template(weakreference_array_signature,             \"[Ljava\/lang\/ref\/WeakReference;\")                           \\\n@@ -704,1 +656,0 @@\n-  template(java_lang_management_ThreadState,           \"java\/lang\/management\/ThreadState\")                        \\\n@@ -741,1 +692,0 @@\n-  template(gcInfoBuilder_name,                         \"gcInfoBuilder\")                                           \\\n@@ -748,4 +698,0 @@\n-  template(addThreadDumpForMonitors_name,              \"addThreadDumpForMonitors\")                                \\\n-  template(addThreadDumpForSynchronizers_name,         \"addThreadDumpForSynchronizers\")                           \\\n-  template(addThreadDumpForMonitors_signature,         \"(Ljava\/lang\/management\/ThreadInfo;[Ljava\/lang\/Object;[I)V\") \\\n-  template(addThreadDumpForSynchronizers_signature,    \"(Ljava\/lang\/management\/ThreadInfo;[Ljava\/lang\/Object;)V\")   \\\n@@ -807,1 +753,0 @@\n-  template(url_void_signature,                              \"(Ljava\/net\/URL;)V\")                                  \\\n","filename":"src\/hotspot\/share\/classfile\/vmSymbols.hpp","additions":2,"deletions":57,"binary":false,"changes":59,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2022, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2022, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -33,1 +33,1 @@\n-#include \"gc\/g1\/g1_globals.hpp\"\n+#include \"gc\/shared\/gc_globals.hpp\"\n@@ -108,1 +108,1 @@\n-    bool should_rebuild_or_scrub(HeapRegion* hr) const {\n+    bool should_rebuild_or_scrub(G1HeapRegion* hr) const {\n@@ -116,1 +116,1 @@\n-    bool scan_large_object(HeapRegion* hr, const oop obj, MemRegion scan_range) {\n+    bool scan_large_object(G1HeapRegion* hr, const oop obj, MemRegion scan_range) {\n@@ -145,1 +145,1 @@\n-    size_t scan_object(HeapRegion* hr, HeapWord* current) {\n+    size_t scan_object(G1HeapRegion* hr, HeapWord* current) {\n@@ -171,1 +171,1 @@\n-    HeapWord* scrub_to_next_live(HeapRegion* hr, HeapWord* scrub_start, HeapWord* limit) {\n+    HeapWord* scrub_to_next_live(G1HeapRegion* hr, HeapWord* scrub_start, HeapWord* limit) {\n@@ -183,1 +183,1 @@\n-    bool scan_and_scrub_to_pb(HeapRegion* hr, HeapWord* start, HeapWord* const limit) {\n+    bool scan_and_scrub_to_pb(G1HeapRegion* hr, HeapWord* start, HeapWord* const limit) {\n@@ -211,1 +211,1 @@\n-    bool scan_from_pb_to_tars(HeapRegion* hr, HeapWord* start, HeapWord* const limit) {\n+    bool scan_from_pb_to_tars(G1HeapRegion* hr, HeapWord* start, HeapWord* const limit) {\n@@ -232,1 +232,1 @@\n-    bool scan_and_scrub_region(HeapRegion* hr, HeapWord* const pb) {\n+    bool scan_and_scrub_region(G1HeapRegion* hr, HeapWord* const pb) {\n@@ -262,1 +262,1 @@\n-    bool scan_humongous_region(HeapRegion* hr, HeapWord* const pb) {\n+    bool scan_humongous_region(G1HeapRegion* hr, HeapWord* const pb) {\n@@ -301,1 +301,1 @@\n-    bool do_heap_region(HeapRegion* hr) {\n+    bool do_heap_region(G1HeapRegion* hr) {\n","filename":"src\/hotspot\/share\/gc\/g1\/g1ConcurrentRebuildAndScrub.cpp","additions":11,"deletions":11,"binary":false,"changes":22,"status":"modified"},{"patch":"@@ -231,14 +231,0 @@\n-  \/\/ At this point the class may not be fully initialized\n-  \/\/ because of recursive initialization. If it is fully\n-  \/\/ initialized & has_finalized is not set, we rewrite\n-  \/\/ it into its fast version (Note: no locking is needed\n-  \/\/ here since this is an atomic byte write and can be\n-  \/\/ done more than once).\n-  \/\/\n-  \/\/ Note: In case of classes with has_finalized we don't\n-  \/\/       rewrite since that saves us an extra check in\n-  \/\/       the fast version which then would call the\n-  \/\/       slow version anyway (and do a call back into\n-  \/\/       Java).\n-  \/\/       If we have a breakpoint, then we don't rewrite\n-  \/\/       because the _breakpoint bytecode would be lost.\n@@ -1062,5 +1048,1 @@\n-    if (JvmtiExport::can_hotswap_or_post_breakpoint() && info.resolved_method()->is_old()) {\n-      resolved_method = methodHandle(current, info.resolved_method()->get_new_method());\n-    } else {\n-      resolved_method = methodHandle(current, info.resolved_method());\n-    }\n+    resolved_method = methodHandle(current, info.resolved_method());\n@@ -1069,0 +1051,3 @@\n+  \/\/ Don't allow safepoints until the method is cached.\n+  NoSafepointVerifier nsv;\n+\n@@ -1158,1 +1143,1 @@\n-  pool->cache()->set_dynamic_call(info, pool->decode_invokedynamic_index(index));\n+  pool->cache()->set_dynamic_call(info, index);\n","filename":"src\/hotspot\/share\/interpreter\/interpreterRuntime.cpp","additions":5,"deletions":20,"binary":false,"changes":25,"status":"modified"},{"patch":"@@ -246,0 +246,3 @@\n+\/\/\n+\/\/ Value classes could also have fields in abstract super value classes.\n+\/\/ Use a HierarchicalFieldStream to get them as well.\n@@ -249,1 +252,1 @@\n-  for (JavaFieldStream fs(this); !fs.done(); fs.next()) {\n+  for (HierarchicalFieldStream<JavaFieldStream> fs(this); !fs.done(); fs.next()) {\n","filename":"src\/hotspot\/share\/oops\/inlineKlass.cpp","additions":4,"deletions":1,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -262,3 +262,3 @@\n-  JNIid*          _jni_ids;              \/\/ First JNI identifier for static fields in this class\n-  jmethodID*      volatile _methods_jmethod_ids;  \/\/ jmethodIDs corresponding to method_idnum, or null if none\n-  nmethodBucket*  volatile _dep_context;          \/\/ packed DependencyContext structure\n+  JNIid*          _jni_ids;                  \/\/ First JNI identifier for static fields in this class\n+  jmethodID* volatile _methods_jmethod_ids;  \/\/ jmethodIDs corresponding to method_idnum, or null if none\n+  nmethodBucket*  volatile _dep_context;     \/\/ packed DependencyContext structure\n@@ -833,2 +833,0 @@\n-  bool has_vanilla_constructor() const  { return _misc_flags.has_vanilla_constructor(); }\n-  void set_has_vanilla_constructor()    { _misc_flags.set_has_vanilla_constructor(true); }\n@@ -861,6 +859,0 @@\n-  jmethodID get_jmethod_id_fetch_or_update(size_t idnum,\n-                     jmethodID new_id, jmethodID* new_jmeths,\n-                     jmethodID* to_dealloc_id_p,\n-                     jmethodID** to_dealloc_jmeths_p);\n-  static void get_jmethod_id_length_value(jmethodID* cache, size_t idnum,\n-                size_t *length_p, jmethodID* id_p);\n@@ -869,0 +861,1 @@\n+  void update_methods_jmethod_cache();\n@@ -1050,1 +1043,0 @@\n-  \/\/  - the class has a finalizer (if !RegisterFinalizersAtInit)\n@@ -1160,5 +1152,0 @@\n-  \/\/ The RedefineClasses() API can cause new method idnums to be needed\n-  \/\/ which will cause the caches to grow. Safety requires different\n-  \/\/ cache management logic if the caches can grow instead of just\n-  \/\/ going from null to non-null.\n-  bool idnum_can_increment() const      { return has_been_redefined(); }\n@@ -1169,0 +1156,1 @@\n+  jmethodID update_jmethod_id(jmethodID* jmeths, Method* method, int idnum);\n@@ -1170,1 +1158,0 @@\n-  \/\/ Lock during initialization\n","filename":"src\/hotspot\/share\/oops\/instanceKlass.hpp","additions":6,"deletions":19,"binary":false,"changes":25,"status":"modified"},{"patch":"@@ -56,9 +56,8 @@\n-    flag(has_vanilla_constructor            , 1 << 13) \/* True if klass has a vanilla default constructor *\/ \\\n-    flag(has_final_method                   , 1 << 14) \/* True if klass has final method *\/ \\\n-    flag(has_inline_type_fields             , 1 << 15) \/* has inline fields and related embedded section is not empty *\/ \\\n-    flag(is_empty_inline_type               , 1 << 16) \/* empty inline type (*) *\/ \\\n-    flag(is_naturally_atomic                , 1 << 17) \/* loaded\/stored in one instruction *\/ \\\n-    flag(must_be_atomic                     , 1 << 18) \/* doesn't allow tearing *\/ \\\n-    flag(has_loosely_consistent_annotation  , 1 << 19) \/* the class has the LooselyConsistentValue annotation WARNING: it doesn't automatically mean that the class allows tearing *\/ \\\n-    flag(is_implicitly_constructible        , 1 << 20) \/* the class has the ImplicitlyConstrutible annotation *\/ \\\n-    flag(has_null_restricted_array          , 1 << 21) \/* the class has the NullRestrictedArray annotation *\/\n+    flag(has_final_method                   , 1 << 13) \/* True if klass has final method *\/ \\\n+    flag(has_inline_type_fields             , 1 << 14) \/* has inline fields and related embedded section is not empty *\/ \\\n+    flag(is_empty_inline_type               , 1 << 15) \/* empty inline type (*) *\/ \\\n+    flag(is_naturally_atomic                , 1 << 16) \/* loaded\/stored in one instruction *\/ \\\n+    flag(must_be_atomic                     , 1 << 17) \/* doesn't allow tearing *\/ \\\n+    flag(has_loosely_consistent_annotation  , 1 << 18) \/* the class has the LooselyConsistentValue annotation WARNING: it doesn't automatically mean that the class allows tearing *\/ \\\n+    flag(is_implicitly_constructible        , 1 << 19) \/* the class has the ImplicitlyConstrutible annotation *\/ \\\n+    flag(has_null_restricted_array          , 1 << 20) \/* the class has the NullRestrictedArray annotation *\/\n","filename":"src\/hotspot\/share\/oops\/instanceKlassFlags.hpp","additions":8,"deletions":9,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -192,1 +192,1 @@\n-  int unpack_offsets(Node* elements[], int length);\n+  int unpack_offsets(Node* elements[], int length) const;\n@@ -280,0 +280,1 @@\n+  virtual Node* Identity(PhaseGVN* phase);\n","filename":"src\/hotspot\/share\/opto\/addnode.hpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -261,0 +261,3 @@\n+  case vmIntrinsics::_setMemory:\n+    if (StubRoutines::unsafe_setmemory() == nullptr) return false;\n+    break;\n@@ -823,0 +826,2 @@\n+  case vmIntrinsics::_intpoly_montgomeryMult_P256:\n+  case vmIntrinsics::_intpoly_assign:\n","filename":"src\/hotspot\/share\/opto\/c2compiler.cpp","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -283,0 +283,1 @@\n+macro(OpaqueInitializedAssertionPredicate)\n","filename":"src\/hotspot\/share\/opto\/classes.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -36,0 +36,1 @@\n+#include \"compiler\/compilerOracle.hpp\"\n@@ -639,0 +640,1 @@\n+                  _allow_macro_nodes(true),\n@@ -858,2 +860,0 @@\n-  \/\/ If any phase is randomized for stress testing, seed random number\n-  \/\/ generation and log the seed for repeatability.\n@@ -862,9 +862,1 @@\n-    if (FLAG_IS_DEFAULT(StressSeed) || (FLAG_IS_ERGO(StressSeed) && directive->RepeatCompilationOption)) {\n-      _stress_seed = static_cast<uint>(Ticks::now().nanoseconds());\n-      FLAG_SET_ERGO(StressSeed, _stress_seed);\n-    } else {\n-      _stress_seed = StressSeed;\n-    }\n-    if (_log != nullptr) {\n-      _log->elem(\"stress_test seed='%u'\", _stress_seed);\n-    }\n+    initialize_stress_seed(directive);\n@@ -940,0 +932,1 @@\n+    _allow_macro_nodes(true),\n@@ -957,0 +950,1 @@\n+    _for_post_loop_igvn(comp_arena(), 8, 0, nullptr),\n@@ -1006,0 +1000,5 @@\n+\n+  if (StressLCM || StressGCM) {\n+    initialize_stress_seed(directive);\n+  }\n+\n@@ -1113,1 +1112,1 @@\n-    if (method_has_option(CompileCommand::NoRTMLockEliding) || ((rtm_state & NoRTM) != 0)) {\n+    if (method_has_option(CompileCommandEnum::NoRTMLockEliding) || ((rtm_state & NoRTM) != 0)) {\n@@ -1116,1 +1115,1 @@\n-    } else if (method_has_option(CompileCommand::UseRTMLockEliding) || ((rtm_state & UseRTM) != 0) || !UseRTMDeopt) {\n+    } else if (method_has_option(CompileCommandEnum::UseRTMLockEliding) || ((rtm_state & UseRTM) != 0) || !UseRTMDeopt) {\n@@ -5556,10 +5555,1 @@\n-#ifdef ASSERT\n-          if (!in_hash) {\n-            tty->print_cr(\"current graph:\");\n-            n->dump_bfs(MaxNodeLimit, nullptr, \"S$\");\n-            tty->cr();\n-            tty->print_cr(\"erroneous node:\");\n-            n->dump();\n-            assert(false, \"node should be in igvn hash table\");\n-          }\n-#endif\n+          assert(in_hash || n->hash() == Node::NO_HASH, \"node should be in igvn hash table\");\n@@ -5636,0 +5626,12 @@\n+void Compile::initialize_stress_seed(const DirectiveSet* directive) {\n+  if (FLAG_IS_DEFAULT(StressSeed) || (FLAG_IS_ERGO(StressSeed) && directive->RepeatCompilationOption)) {\n+    _stress_seed = static_cast<uint>(Ticks::now().nanoseconds());\n+    FLAG_SET_ERGO(StressSeed, _stress_seed);\n+  } else {\n+    _stress_seed = StressSeed;\n+  }\n+  if (_log != nullptr) {\n+    _log->elem(\"stress_test seed='%u'\", _stress_seed);\n+  }\n+}\n+\n","filename":"src\/hotspot\/share\/opto\/compile.cpp","additions":25,"deletions":23,"binary":false,"changes":48,"status":"modified"},{"patch":"@@ -147,1 +147,1 @@\n-  InlineTypeNode* buffer(GraphKit* kit, bool safe_for_replace = true);\n+  InlineTypeNode* buffer(GraphKit* kit, bool safe_for_replace = true, bool must_init = true);\n@@ -175,0 +175,2 @@\n+\n+  NOT_PRODUCT(void dump_spec(outputStream* st) const;)\n","filename":"src\/hotspot\/share\/opto\/inlinetypenode.hpp","additions":3,"deletions":1,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -516,0 +516,1 @@\n+  case vmIntrinsics::_setMemory:                return inline_unsafe_setMemory();\n@@ -656,1 +657,4 @@\n-\n+  case vmIntrinsics::_intpoly_montgomeryMult_P256:\n+    return inline_intpoly_montgomeryMult_P256();\n+  case vmIntrinsics::_intpoly_assign:\n+    return inline_intpoly_assign();\n@@ -5344,0 +5348,51 @@\n+\/\/ unsafe_setmemory(void *base, ulong offset, size_t length, char fill_value);\n+\/\/ Fill 'length' bytes starting from 'base[offset]' with 'fill_value'\n+bool LibraryCallKit::inline_unsafe_setMemory() {\n+  if (callee()->is_static())  return false;  \/\/ caller must have the capability!\n+  null_check_receiver();  \/\/ null-check receiver\n+  if (stopped())  return true;\n+\n+  C->set_has_unsafe_access(true);  \/\/ Mark eventual nmethod as \"unsafe\".\n+\n+  Node* dst_base =         argument(1);  \/\/ type: oop\n+  Node* dst_off  = ConvL2X(argument(2)); \/\/ type: long\n+  Node* size     = ConvL2X(argument(4)); \/\/ type: long\n+  Node* byte     =         argument(6);  \/\/ type: byte\n+\n+  assert(Unsafe_field_offset_to_byte_offset(11) == 11,\n+         \"fieldOffset must be byte-scaled\");\n+\n+  Node* dst_addr = make_unsafe_address(dst_base, dst_off);\n+\n+  Node* thread = _gvn.transform(new ThreadLocalNode());\n+  Node* doing_unsafe_access_addr = basic_plus_adr(top(), thread, in_bytes(JavaThread::doing_unsafe_access_offset()));\n+  BasicType doing_unsafe_access_bt = T_BYTE;\n+  assert((sizeof(bool) * CHAR_BIT) == 8, \"not implemented\");\n+\n+  \/\/ update volatile field\n+  store_to_memory(control(), doing_unsafe_access_addr, intcon(1), doing_unsafe_access_bt, Compile::AliasIdxRaw, MemNode::unordered);\n+\n+  int flags = RC_LEAF | RC_NO_FP;\n+\n+  const TypePtr* dst_type = TypePtr::BOTTOM;\n+\n+  \/\/ Adjust memory effects of the runtime call based on input values.\n+  if (!has_wide_mem(_gvn, dst_addr, dst_base)) {\n+    dst_type = _gvn.type(dst_addr)->is_ptr(); \/\/ narrow out memory\n+\n+    flags |= RC_NARROW_MEM; \/\/ narrow in memory\n+  }\n+\n+  \/\/ Call it.  Note that the length argument is not scaled.\n+  make_runtime_call(flags,\n+                    OptoRuntime::make_setmemory_Type(),\n+                    StubRoutines::unsafe_setmemory(),\n+                    \"unsafe_setmemory\",\n+                    dst_type,\n+                    dst_addr, size XTOP, byte);\n+\n+  store_to_memory(control(), doing_unsafe_access_addr, intcon(0), doing_unsafe_access_bt, Compile::AliasIdxRaw, MemNode::unordered);\n+\n+  return true;\n+}\n+\n@@ -6390,57 +6445,4 @@\n-  \/\/ Set the original stack and the reexecute bit for the interpreter to reexecute\n-  \/\/ the bytecode that invokes BigInteger.multiplyToLen() if deoptimization happens\n-  \/\/ on the return from z array allocation in runtime.\n-  { PreserveReexecuteState preexecs(this);\n-    jvms()->set_should_reexecute(true);\n-\n-    Node* x_start = array_element_address(x, intcon(0), x_elem);\n-    Node* y_start = array_element_address(y, intcon(0), y_elem);\n-    \/\/ 'x_start' points to x array + scaled xlen\n-    \/\/ 'y_start' points to y array + scaled ylen\n-\n-    \/\/ Allocate the result array\n-    Node* zlen = _gvn.transform(new AddINode(xlen, ylen));\n-    ciKlass* klass = ciTypeArrayKlass::make(T_INT);\n-    Node* klass_node = makecon(TypeKlassPtr::make(klass));\n-\n-    IdealKit ideal(this);\n-\n-#define __ ideal.\n-     Node* one = __ ConI(1);\n-     Node* zero = __ ConI(0);\n-     IdealVariable need_alloc(ideal), z_alloc(ideal);  __ declarations_done();\n-     __ set(need_alloc, zero);\n-     __ set(z_alloc, z);\n-     __ if_then(z, BoolTest::eq, null()); {\n-       __ increment (need_alloc, one);\n-     } __ else_(); {\n-       \/\/ Update graphKit memory and control from IdealKit.\n-       sync_kit(ideal);\n-       Node* cast = new CastPPNode(control(), z, TypePtr::NOTNULL);\n-       _gvn.set_type(cast, cast->bottom_type());\n-       C->record_for_igvn(cast);\n-\n-       Node* zlen_arg = load_array_length(cast);\n-       \/\/ Update IdealKit memory and control from graphKit.\n-       __ sync_kit(this);\n-       __ if_then(zlen_arg, BoolTest::lt, zlen); {\n-         __ increment (need_alloc, one);\n-       } __ end_if();\n-     } __ end_if();\n-\n-     __ if_then(__ value(need_alloc), BoolTest::ne, zero); {\n-       \/\/ Update graphKit memory and control from IdealKit.\n-       sync_kit(ideal);\n-       Node * narr = new_array(klass_node, zlen, 1);\n-       \/\/ Update IdealKit memory and control from graphKit.\n-       __ sync_kit(this);\n-       __ set(z_alloc, narr);\n-     } __ end_if();\n-\n-     sync_kit(ideal);\n-     z = __ value(z_alloc);\n-     \/\/ Can't use TypeAryPtr::INTS which uses Bottom offset.\n-     _gvn.set_type(z, TypeOopPtr::make_from_klass(klass));\n-     \/\/ Final sync IdealKit and GraphKit.\n-     final_sync(ideal);\n-#undef __\n+  Node* x_start = array_element_address(x, intcon(0), x_elem);\n+  Node* y_start = array_element_address(y, intcon(0), y_elem);\n+  \/\/ 'x_start' points to x array + scaled xlen\n+  \/\/ 'y_start' points to y array + scaled ylen\n@@ -6448,1 +6450,1 @@\n-    Node* z_start = array_element_address(z, intcon(0), T_INT);\n+  Node* z_start = array_element_address(z, intcon(0), T_INT);\n@@ -6450,5 +6452,4 @@\n-    Node* call = make_runtime_call(RC_LEAF|RC_NO_FP,\n-                                   OptoRuntime::multiplyToLen_Type(),\n-                                   stubAddr, stubName, TypePtr::BOTTOM,\n-                                   x_start, xlen, y_start, ylen, z_start, zlen);\n-  } \/\/ original reexecute is set back here\n+  Node* call = make_runtime_call(RC_LEAF|RC_NO_FP,\n+                                 OptoRuntime::multiplyToLen_Type(),\n+                                 stubAddr, stubName, TypePtr::BOTTOM,\n+                                 x_start, xlen, y_start, ylen, z_start);\n@@ -7983,0 +7984,63 @@\n+bool LibraryCallKit::inline_intpoly_montgomeryMult_P256() {\n+  address stubAddr;\n+  const char *stubName;\n+  assert(UseIntPolyIntrinsics, \"need intpoly intrinsics support\");\n+  assert(callee()->signature()->size() == 3, \"intpoly_montgomeryMult_P256 has %d parameters\", callee()->signature()->size());\n+  stubAddr = StubRoutines::intpoly_montgomeryMult_P256();\n+  stubName = \"intpoly_montgomeryMult_P256\";\n+\n+  if (!stubAddr) return false;\n+  null_check_receiver();  \/\/ null-check receiver\n+  if (stopped())  return true;\n+\n+  Node* a = argument(1);\n+  Node* b = argument(2);\n+  Node* r = argument(3);\n+\n+  a = must_be_not_null(a, true);\n+  b = must_be_not_null(b, true);\n+  r = must_be_not_null(r, true);\n+\n+  Node* a_start = array_element_address(a, intcon(0), T_LONG);\n+  assert(a_start, \"a array is NULL\");\n+  Node* b_start = array_element_address(b, intcon(0), T_LONG);\n+  assert(b_start, \"b array is NULL\");\n+  Node* r_start = array_element_address(r, intcon(0), T_LONG);\n+  assert(r_start, \"r array is NULL\");\n+\n+  Node* call = make_runtime_call(RC_LEAF | RC_NO_FP,\n+                                 OptoRuntime::intpoly_montgomeryMult_P256_Type(),\n+                                 stubAddr, stubName, TypePtr::BOTTOM,\n+                                 a_start, b_start, r_start);\n+  Node* result = _gvn.transform(new ProjNode(call, TypeFunc::Parms));\n+  set_result(result);\n+  return true;\n+}\n+\n+bool LibraryCallKit::inline_intpoly_assign() {\n+  assert(UseIntPolyIntrinsics, \"need intpoly intrinsics support\");\n+  assert(callee()->signature()->size() == 3, \"intpoly_assign has %d parameters\", callee()->signature()->size());\n+  const char *stubName = \"intpoly_assign\";\n+  address stubAddr = StubRoutines::intpoly_assign();\n+  if (!stubAddr) return false;\n+\n+  Node* set = argument(0);\n+  Node* a = argument(1);\n+  Node* b = argument(2);\n+  Node* arr_length = load_array_length(a);\n+\n+  a = must_be_not_null(a, true);\n+  b = must_be_not_null(b, true);\n+\n+  Node* a_start = array_element_address(a, intcon(0), T_LONG);\n+  assert(a_start, \"a array is NULL\");\n+  Node* b_start = array_element_address(b, intcon(0), T_LONG);\n+  assert(b_start, \"b array is NULL\");\n+\n+  Node* call = make_runtime_call(RC_LEAF | RC_NO_FP,\n+                                 OptoRuntime::intpoly_assign_Type(),\n+                                 stubAddr, stubName, TypePtr::BOTTOM,\n+                                 set, a_start, b_start, arr_length);\n+  return true;\n+}\n+\n","filename":"src\/hotspot\/share\/opto\/library_call.cpp","additions":128,"deletions":64,"binary":false,"changes":192,"status":"modified"},{"patch":"@@ -258,0 +258,1 @@\n+  bool inline_unsafe_setMemory();\n@@ -335,0 +336,2 @@\n+  bool inline_intpoly_montgomeryMult_P256();\n+  bool inline_intpoly_assign();\n","filename":"src\/hotspot\/share\/opto\/library_call.hpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -2355,1 +2355,2 @@\n-  if (ABS(cl->stride_con()) == 1 ||\n+  if (cl->stride_con() == 1 ||\n+      cl->stride_con() == -1 ||\n@@ -2965,1 +2966,4 @@\n-  jlong scaled_iters_long = ((jlong)LoopStripMiningIter) * ABS(stride);\n+  \/\/ For a min int stride, LoopStripMiningIter * stride overflows the int range for all values of LoopStripMiningIter\n+  \/\/ except 0 or 1. Those values are handled early on in this method and causes the method to return. So for a min int\n+  \/\/ stride, the method is guaranteed to return at the next check below.\n+  jlong scaled_iters_long = ((jlong)LoopStripMiningIter) * ABS((jlong)stride);\n@@ -2967,1 +2971,6 @@\n-  int short_scaled_iters = LoopStripMiningIterShortLoop* ABS(stride);\n+  if ((jlong)scaled_iters != scaled_iters_long) {\n+    \/\/ Remove outer loop and safepoint (too few iterations)\n+    remove_outer_loop_and_safepoint(igvn);\n+    return;\n+  }\n+  jlong short_scaled_iters = LoopStripMiningIterShortLoop * ABS(stride);\n@@ -2971,2 +2980,2 @@\n-  if ((jlong)scaled_iters != scaled_iters_long || iter_estimate <= short_scaled_iters) {\n-    \/\/ Remove outer loop and safepoint (too few iterations)\n+  if (iter_estimate <= short_scaled_iters) {\n+    \/\/ Remove outer loop and safepoint: loop executes less than LoopStripMiningIterShortLoop\n@@ -4314,1 +4323,1 @@\n-  Node* entry = loop->_head->in(LoopNode::EntryControl);\n+  Node* entry = loop->_head->as_Loop()->skip_strip_mined()->in(LoopNode::EntryControl);\n@@ -4353,1 +4362,1 @@\n-  Node* entry = loop->_head->in(LoopNode::EntryControl);\n+  Node* entry = loop->_head->as_Loop()->skip_strip_mined()->in(LoopNode::EntryControl);\n@@ -4374,4 +4383,3 @@\n-    Node* opaque4 = C->template_assertion_predicate_opaq_node(i - 1);\n-    assert(opaque4->Opcode() == Op_Opaque4, \"must be\");\n-    if (!useful_predicates.member(opaque4)) { \/\/ not in the useful list\n-      _igvn.replace_node(opaque4, opaque4->in(2));\n+    Opaque4Node* opaque4_node = C->template_assertion_predicate_opaq_node(i - 1)->as_Opaque4();\n+    if (!useful_predicates.member(opaque4_node)) { \/\/ not in the useful list\n+      _igvn.replace_node(opaque4_node, opaque4_node->in(2));\n","filename":"src\/hotspot\/share\/opto\/loopnode.cpp","additions":19,"deletions":11,"binary":false,"changes":30,"status":"modified"},{"patch":"@@ -1934,5 +1934,1 @@\n-  if (leaf->is_LoadStore()) {\n-    mach->set_barrier_data(leaf->as_LoadStore()->barrier_data());\n-  } else if (leaf->is_Mem()) {\n-    mach->set_barrier_data(leaf->as_Mem()->barrier_data());\n-  }\n+  mach->set_barrier_data(MemNode::barrier_data(leaf));\n@@ -2533,0 +2529,7 @@\n+    case Op_LoadVectorGather:\n+      if (is_subword_type(n->bottom_type()->is_vect()->element_basic_type())) {\n+        Node* pair = new BinaryNode(n->in(MemNode::ValueIn), n->in(MemNode::ValueIn+1));\n+        n->set_req(MemNode::ValueIn, pair);\n+        n->del_req(MemNode::ValueIn+1);\n+      }\n+      break;\n@@ -2534,0 +2537,8 @@\n+      if (is_subword_type(n->bottom_type()->is_vect()->element_basic_type())) {\n+        Node* pair2 = new BinaryNode(n->in(MemNode::ValueIn + 1), n->in(MemNode::ValueIn + 2));\n+        Node* pair1 = new BinaryNode(n->in(MemNode::ValueIn), pair2);\n+        n->set_req(MemNode::ValueIn, pair1);\n+        n->del_req(MemNode::ValueIn+2);\n+        n->del_req(MemNode::ValueIn+1);\n+        break;\n+      } \/\/ fall-through\n@@ -2555,0 +2566,8 @@\n+    case Op_PartialSubtypeCheck: {\n+      if (UseSecondarySupersTable && n->in(2)->is_Con()) {\n+        \/\/ PartialSubtypeCheck uses both constant and register operands for superclass input.\n+        n->set_req(2, new BinaryNode(n->in(2), n->in(2)));\n+        break;\n+      }\n+      break;\n+    }\n","filename":"src\/hotspot\/share\/opto\/matcher.cpp","additions":24,"deletions":5,"binary":false,"changes":29,"status":"modified"},{"patch":"@@ -36,0 +36,1 @@\n+#include \"opto\/rootnode.hpp\"\n@@ -47,1 +48,0 @@\n-  _align_to_ref(nullptr),                                   \/\/ memory reference to align vectors to\n@@ -53,0 +53,2 @@\n+  _mem_ref_for_main_loop_alignment(nullptr),\n+  _aw_for_main_loop_alignment(0),\n@@ -518,5 +520,0 @@\n-  \/\/ Take the first mem_ref as the reference to align to. The pre-loop trip count is\n-  \/\/ modified to align this reference to a vector-aligned address. If strict alignment\n-  \/\/ is required, we may change the reference later (see filter_packs_for_alignment()).\n-  MemNode* align_to_mem_ref = nullptr;\n-\n@@ -529,5 +526,0 @@\n-    if (align_to_mem_ref == nullptr) {\n-      align_to_mem_ref = mem_ref;\n-      set_align_to_ref(align_to_mem_ref);\n-    }\n-\n@@ -575,3 +567,0 @@\n-  assert(_pairset.is_empty() || align_to_mem_ref != nullptr,\n-         \"pairset empty or we find the alignment reference\");\n-\n@@ -1725,1 +1714,5 @@\n-    set_align_to_ref(current->as_constrained()->mem_ref());\n+    MemNode const* mem = current->as_constrained()->mem_ref();\n+    Node_List* pack = get_pack(mem);\n+    assert(pack != nullptr, \"memop of final solution must still be packed\");\n+    _mem_ref_for_main_loop_alignment = mem;\n+    _aw_for_main_loop_alignment = pack->size() * mem->memory_size();\n@@ -2556,2 +2549,1 @@\n-        igvn().register_new_node_with_optimizer(mask);\n-        phase()->set_ctrl(mask, phase()->get_ctrl(p->at(0)));\n+        phase()->register_new_node_with_ctrl_of(mask, p->at(0));\n@@ -2631,2 +2623,1 @@\n-        igvn().register_new_node_with_optimizer(longval);\n-        phase()->set_ctrl(longval, phase()->get_ctrl(first));\n+        phase()->register_new_node_with_ctrl_of(longval, first);\n@@ -2679,2 +2670,1 @@\n-      igvn().register_new_node_with_optimizer(vn);\n-      phase()->set_ctrl(vn, phase()->get_ctrl(first));\n+      phase()->register_new_node_with_ctrl_of(vn, first);\n@@ -2749,2 +2739,1 @@\n-    igvn().register_new_node_with_optimizer(vn);\n-    phase()->set_ctrl(vn, phase()->get_ctrl(opd));\n+    phase()->register_new_node_with_ctrl_of(vn, opd);\n@@ -2770,2 +2759,2 @@\n-          cnt = ConNode::make(TypeInt::make(shift & mask));\n-          igvn().register_new_node_with_optimizer(cnt);\n+          cnt = igvn().intcon(shift & mask);\n+          phase()->set_ctrl(cnt, phase()->C->root());\n@@ -2775,2 +2764,1 @@\n-          cnt = ConNode::make(TypeInt::make(mask));\n-          igvn().register_new_node_with_optimizer(cnt);\n+          cnt = igvn().intcon(mask);\n@@ -2778,2 +2766,1 @@\n-          igvn().register_new_node_with_optimizer(cnt);\n-          phase()->set_ctrl(cnt, phase()->get_ctrl(opd));\n+          phase()->register_new_node_with_ctrl_of(cnt, opd);\n@@ -2788,2 +2775,1 @@\n-      igvn().register_new_node_with_optimizer(cnt);\n-      phase()->set_ctrl(cnt, phase()->get_ctrl(opd));\n+      phase()->register_new_node_with_ctrl_of(cnt, opd);\n@@ -2807,2 +2793,1 @@\n-         igvn().register_new_node_with_optimizer(conv);\n-         phase()->set_ctrl(conv, phase()->get_ctrl(opd));\n+         phase()->register_new_node_with_ctrl_of(conv, opd);\n@@ -2816,2 +2801,1 @@\n-    igvn().register_new_node_with_optimizer(vn);\n-    phase()->set_ctrl(vn, phase()->get_ctrl(opd));\n+    phase()->register_new_node_with_ctrl_of(vn, opd);\n@@ -2846,2 +2830,1 @@\n-  igvn().register_new_node_with_optimizer(pk);\n-  phase()->set_ctrl(pk, phase()->get_ctrl(opd));\n+  phase()->register_new_node_with_ctrl_of(pk, opd);\n@@ -3116,0 +3099,22 @@\n+\n+      \/\/ If a Load depends on the same memory state as a Store, we must make sure that\n+      \/\/ the Load is ordered before the Store.\n+      \/\/\n+      \/\/      mem\n+      \/\/       |\n+      \/\/    +--+--+\n+      \/\/    |     |\n+      \/\/    |    Load (n)\n+      \/\/    |\n+      \/\/   Store (mem_use)\n+      \/\/\n+      if (n->is_Load()) {\n+        Node* mem = n->in(MemNode::Memory);\n+        for (DUIterator_Fast imax, i = mem->fast_outs(imax); i < imax; i++) {\n+          Node* mem_use = mem->fast_out(i);\n+          if (mem_use->is_Store() && _vloop.in_bb(mem_use) && !visited.test(bb_idx(mem_use))) {\n+            stack.push(mem_use); \/\/ Ordering edge: Load (n) -> Store (mem_use)\n+          }\n+        }\n+      }\n+\n@@ -3121,1 +3126,1 @@\n-          stack.push(use);\n+          stack.push(use); \/\/ Ordering edge: n -> use\n@@ -3124,0 +3129,1 @@\n+\n@@ -3403,0 +3409,26 @@\n+\/\/ Find the memop pack with the maximum vector width, unless they were already\n+\/\/ determined by SuperWord::filter_packs_for_alignment().\n+void SuperWord::determine_mem_ref_and_aw_for_main_loop_alignment() {\n+  if (_mem_ref_for_main_loop_alignment != nullptr) {\n+    assert(vectors_should_be_aligned(), \"mem_ref only set if filtered for alignment\");\n+    return;\n+  }\n+\n+  MemNode const* mem_ref = nullptr;\n+  int max_aw = 0;\n+  for (int i = 0; i < _packset.length(); i++) {\n+    Node_List* pack = _packset.at(i);\n+    MemNode* first = pack->at(0)->isa_Mem();\n+    if (first == nullptr) { continue; }\n+\n+    int vw = first->memory_size() * pack->size();\n+    if (vw > max_aw) {\n+      max_aw = vw;\n+      mem_ref = first;\n+    }\n+  }\n+  assert(mem_ref != nullptr && max_aw > 0, \"found mem_ref and aw\");\n+  _mem_ref_for_main_loop_alignment = mem_ref;\n+  _aw_for_main_loop_alignment = max_aw;\n+}\n+\n@@ -3413,2 +3445,3 @@\n-\/\/ the address of \"align_to_ref\" to the maximal possible vector width. We adjust the pre-loop\n-\/\/ iteration count by adjusting the pre-loop limit.\n+\/\/ the address of \"_mem_ref_for_main_loop_alignment\" to \"_aw_for_main_loop_alignment\", which is a\n+\/\/ sufficiently large alignment width. We adjust the pre-loop iteration count by adjusting the\n+\/\/ pre-loop limit.\n@@ -3416,2 +3449,4 @@\n-  const MemNode* align_to_ref = _align_to_ref;\n-  assert(align_to_ref != nullptr, \"align_to_ref must be set\");\n+  determine_mem_ref_and_aw_for_main_loop_alignment();\n+  const MemNode* align_to_ref = _mem_ref_for_main_loop_alignment;\n+  const int aw                = _aw_for_main_loop_alignment;\n+  assert(align_to_ref != nullptr && aw > 0, \"must have alignment reference and aw\");\n@@ -3562,4 +3597,1 @@\n-\n-  \/\/ We chose an aw that is the maximal possible vector width for the type of\n-  \/\/ align_to_ref.\n-  const int aw       = vector_width_in_bytes(align_to_ref);\n+  \/\/\n@@ -3637,1 +3669,1 @@\n-      igvn().register_new_node_with_optimizer(invar);\n+      phase()->register_new_node(invar, pre_ctrl);\n@@ -3645,2 +3677,1 @@\n-    igvn().register_new_node_with_optimizer(xboi);\n-    phase()->set_ctrl(xboi, pre_ctrl);\n+    phase()->register_new_node(xboi, pre_ctrl);\n@@ -3656,1 +3687,1 @@\n-    igvn().register_new_node_with_optimizer(xbase);\n+    phase()->register_new_node(xbase, pre_ctrl);\n@@ -3660,1 +3691,1 @@\n-    igvn().register_new_node_with_optimizer(xbase);\n+    phase()->register_new_node(xbase, pre_ctrl);\n@@ -3668,2 +3699,1 @@\n-    igvn().register_new_node_with_optimizer(xboi);\n-    phase()->set_ctrl(xboi, pre_ctrl);\n+    phase()->register_new_node(xboi, pre_ctrl);\n@@ -3678,2 +3708,1 @@\n-  igvn().register_new_node_with_optimizer(XBOI);\n-  phase()->set_ctrl(XBOI, pre_ctrl);\n+  phase()->register_new_node(XBOI, pre_ctrl);\n@@ -3693,2 +3722,1 @@\n-  igvn().register_new_node_with_optimizer(XBOI_OP_old_limit);\n-  phase()->set_ctrl(XBOI_OP_old_limit, pre_ctrl);\n+  phase()->register_new_node(XBOI_OP_old_limit, pre_ctrl);\n@@ -3705,2 +3733,1 @@\n-  igvn().register_new_node_with_optimizer(adjust_pre_iter);\n-  phase()->set_ctrl(adjust_pre_iter, pre_ctrl);\n+  phase()->register_new_node(adjust_pre_iter, pre_ctrl);\n@@ -3719,2 +3746,1 @@\n-  igvn().register_new_node_with_optimizer(new_limit);\n-  phase()->set_ctrl(new_limit, pre_ctrl);\n+  phase()->register_new_node(new_limit, pre_ctrl);\n@@ -3728,2 +3754,1 @@\n-  igvn().register_new_node_with_optimizer(constrained_limit);\n-  phase()->set_ctrl(constrained_limit, pre_ctrl);\n+  phase()->register_new_node(constrained_limit, pre_ctrl);\n","filename":"src\/hotspot\/share\/opto\/superword.cpp","additions":88,"deletions":63,"binary":false,"changes":151,"status":"modified"},{"patch":"@@ -443,5 +443,1 @@\n-  switch (n->Opcode()) {\n-  case Op_MulAddS2I:\n-    return true;\n-  }\n-  return false;\n+  return n->Opcode() == Op_MulAddS2I;\n@@ -455,4 +451,1 @@\n-  if (n->Opcode() == Op_MulAddS2I) {\n-    return true;\n-  }\n-  return false;\n+  return n->Opcode() == Op_MulAddS2I;\n@@ -462,4 +455,1 @@\n-  if (n->Opcode() == Op_RoundDoubleMode) {\n-    return true;\n-  }\n-  return false;\n+  return n->Opcode() == Op_RoundDoubleMode;\n@@ -632,4 +622,1 @@\n-  if (is_rotate_opcode(n->Opcode())) {\n-    return true;\n-  }\n-  return false;\n+  return is_rotate_opcode(n->Opcode());\n@@ -1727,1 +1714,1 @@\n-    if (Type::cmp(bottom_type(), n->in(1)->bottom_type()) == 0 &&\n+    if (Type::equals(bottom_type(), n->in(1)->bottom_type()) &&\n@@ -1739,1 +1726,1 @@\n-Node* VectorInsertNode::make(Node* vec, Node* new_val, int position) {\n+Node* VectorInsertNode::make(Node* vec, Node* new_val, int position, PhaseGVN& gvn) {\n@@ -1741,1 +1728,1 @@\n-  ConINode* pos = ConINode::make(position);\n+  ConINode* pos = gvn.intcon(position);\n@@ -1748,1 +1735,1 @@\n-    if (Type::cmp(bottom_type(), n->in(VectorBoxNode::Value)->bottom_type()) == 0) {\n+    if (Type::equals(bottom_type(), n->in(VectorBoxNode::Value)->bottom_type())) {\n@@ -1785,1 +1772,1 @@\n-    if (Type::cmp(bottom_type(), n->in(VectorBoxNode::Value)->bottom_type()) == 0) {\n+    if (Type::equals(bottom_type(), n->in(VectorBoxNode::Value)->bottom_type())) {\n","filename":"src\/hotspot\/share\/opto\/vectornode.cpp","additions":9,"deletions":22,"binary":false,"changes":31,"status":"modified"},{"patch":"@@ -976,1 +976,1 @@\n-  LoadVectorGatherNode(Node* c, Node* mem, Node* adr, const TypePtr* at, const TypeVect* vt, Node* indices)\n+  LoadVectorGatherNode(Node* c, Node* mem, Node* adr, const TypePtr* at, const TypeVect* vt, Node* indices, Node* offset = nullptr)\n@@ -979,2 +979,7 @@\n-    assert(indices->bottom_type()->is_vect(), \"indices must be in vector\");\n-    assert(req() == MemNode::ValueIn + 1, \"match_edge expects that last input is in MemNode::ValueIn\");\n+    DEBUG_ONLY(bool is_subword = is_subword_type(vt->element_basic_type()));\n+    assert(is_subword || indices->bottom_type()->is_vect(), \"indices must be in vector\");\n+    assert(is_subword || !offset, \"\");\n+    assert(req() == MemNode::ValueIn + 1, \"match_edge expects that index input is in MemNode::ValueIn\");\n+    if (offset) {\n+      add_req(offset);\n+    }\n@@ -985,1 +990,10 @@\n-  virtual uint match_edge(uint idx) const { return idx == MemNode::Address || idx == MemNode::ValueIn; }\n+  virtual uint match_edge(uint idx) const {\n+     return idx == MemNode::Address ||\n+            idx == MemNode::ValueIn ||\n+            ((is_subword_type(vect_type()->element_basic_type())) &&\n+              idx == MemNode::ValueIn + 1);\n+  }\n+  virtual int store_Opcode() const {\n+    \/\/ Ensure it is different from any store opcode to avoid folding when indices are used\n+    return -1;\n+  }\n@@ -1018,0 +1032,2 @@\n+  virtual Node* mask() const { return nullptr; }\n+  virtual Node* indices() const { return nullptr; }\n@@ -1033,0 +1049,1 @@\n+   enum { Indices = 4 };\n@@ -1044,0 +1061,1 @@\n+   virtual Node* indices() const { return in(Indices); }\n@@ -1050,0 +1068,1 @@\n+  enum { Mask = 4 };\n@@ -1063,0 +1082,1 @@\n+  virtual Node* mask() const { return in(Mask); }\n@@ -1083,0 +1103,4 @@\n+  virtual int store_Opcode() const {\n+    \/\/ Ensure it is different from any store opcode to avoid folding when a mask is used\n+    return -1;\n+  }\n@@ -1089,1 +1113,1 @@\n-  LoadVectorGatherMaskedNode(Node* c, Node* mem, Node* adr, const TypePtr* at, const TypeVect* vt, Node* indices, Node* mask)\n+  LoadVectorGatherMaskedNode(Node* c, Node* mem, Node* adr, const TypePtr* at, const TypeVect* vt, Node* indices, Node* mask, Node* offset = nullptr)\n@@ -1092,2 +1116,0 @@\n-    assert(indices->bottom_type()->is_vect(), \"indices must be in vector\");\n-    assert(mask->bottom_type()->isa_vectmask(), \"sanity\");\n@@ -1097,0 +1119,3 @@\n+    if (is_subword_type(vt->element_basic_type())) {\n+      add_req(offset);\n+    }\n@@ -1102,1 +1127,7 @@\n-                                                   idx == MemNode::ValueIn + 1; }\n+                                                   idx == MemNode::ValueIn + 1 ||\n+                                                   (is_subword_type(vect_type()->is_vect()->element_basic_type()) &&\n+                                                   idx == MemNode::ValueIn + 2); }\n+  virtual int store_Opcode() const {\n+    \/\/ Ensure it is different from any store opcode to avoid folding when indices and mask are used\n+    return -1;\n+  }\n@@ -1109,0 +1140,3 @@\n+   enum { Indices = 4,\n+          Mask\n+   };\n@@ -1123,0 +1157,2 @@\n+   virtual Node* mask() const { return in(Mask); }\n+   virtual Node* indices() const { return in(Indices); }\n@@ -1633,1 +1669,1 @@\n-    return VectorNode::cmp(n) && !Type::cmp(_src_vt,((VectorReinterpretNode&)n)._src_vt);\n+    return VectorNode::cmp(n) && Type::equals(_src_vt, ((VectorReinterpretNode&) n)._src_vt);\n@@ -1771,1 +1807,1 @@\n-   assert(Type::cmp(vt, vsrc->bottom_type()) == 0, \"input and output must be same type\");\n+   assert(Type::equals(vt, vsrc->bottom_type()), \"input and output must be same type\");\n@@ -1776,1 +1812,1 @@\n-  static Node* make(Node* vec, Node* new_val, int position);\n+  static Node* make(Node* vec, Node* new_val, int position, PhaseGVN& gvn);\n","filename":"src\/hotspot\/share\/opto\/vectornode.hpp","additions":47,"deletions":11,"binary":false,"changes":58,"status":"modified"},{"patch":"@@ -312,1 +312,1 @@\n-  volatile_nonstatic_field(Method,             _code,                                         CompiledMethod*)                       \\\n+  volatile_nonstatic_field(Method,             _code,                                         nmethod*)                              \\\n@@ -513,1 +513,1 @@\n-  nonstatic_field(HeapBlock::Header,           _length,                                       size_t)                                \\\n+  nonstatic_field(HeapBlock::Header,           _length,                                       uint32_t)                              \\\n@@ -554,11 +554,11 @@\n-  nonstatic_field(CodeBlob,                 _name,                                   const char*)                                    \\\n-  nonstatic_field(CodeBlob,                 _size,                                   int)                                            \\\n-  nonstatic_field(CodeBlob,                 _header_size,                            int)                                            \\\n-  nonstatic_field(CodeBlob,                 _frame_complete_offset,                  int)                                            \\\n-  nonstatic_field(CodeBlob,                 _data_offset,                            int)                                            \\\n-  nonstatic_field(CodeBlob,                 _frame_size,                             int)                                            \\\n-  nonstatic_field(CodeBlob,                 _oop_maps,                               ImmutableOopMapSet*)                            \\\n-  nonstatic_field(CodeBlob,                 _code_begin,                             address)                                        \\\n-  nonstatic_field(CodeBlob,                 _code_end,                               address)                                        \\\n-  nonstatic_field(CodeBlob,                 _content_begin,                          address)                                        \\\n-  nonstatic_field(CodeBlob,                 _data_end,                               address)                                        \\\n+  nonstatic_field(CodeBlob,                    _name,                                         const char*)                           \\\n+  nonstatic_field(CodeBlob,                    _size,                                         int)                                   \\\n+  nonstatic_field(CodeBlob,                    _header_size,                                  u2)                                    \\\n+  nonstatic_field(CodeBlob,                    _relocation_size,                              int)                                   \\\n+  nonstatic_field(CodeBlob,                    _content_offset,                               int)                                   \\\n+  nonstatic_field(CodeBlob,                    _code_offset,                                  int)                                   \\\n+  nonstatic_field(CodeBlob,                    _frame_complete_offset,                        int16_t)                               \\\n+  nonstatic_field(CodeBlob,                    _data_offset,                                  int)                                   \\\n+  nonstatic_field(CodeBlob,                    _frame_size,                                   int)                                   \\\n+  nonstatic_field(CodeBlob,                    _oop_maps,                                     ImmutableOopMapSet*)                   \\\n+  nonstatic_field(CodeBlob,                    _caller_must_gc_arguments,                     bool)                                  \\\n@@ -568,12 +568,0 @@\n-  nonstatic_field(RuntimeStub,                 _caller_must_gc_arguments,                     bool)                                  \\\n-                                                                                                                                     \\\n-  \/********************************************************\/                                                                         \\\n-  \/* CompiledMethod (NOTE: incomplete, but only a little) *\/                                                                         \\\n-  \/********************************************************\/                                                                         \\\n-                                                                                                                                     \\\n-  nonstatic_field(CompiledMethod,                     _method,                                       Method*)                        \\\n-  volatile_nonstatic_field(CompiledMethod,            _exception_cache,                              ExceptionCache*)                \\\n-  nonstatic_field(CompiledMethod,                     _scopes_data_begin,                            address)                        \\\n-  nonstatic_field(CompiledMethod,                     _deopt_handler_begin,                          address)                        \\\n-  nonstatic_field(CompiledMethod,                     _deopt_mh_handler_begin,                       address)                        \\\n-                                                                                                                                     \\\n@@ -584,0 +572,1 @@\n+  nonstatic_field(nmethod,                     _method,                                       Method*)                               \\\n@@ -588,0 +577,2 @@\n+  nonstatic_field(nmethod,                     _deopt_handler_offset,                         int)                                   \\\n+  nonstatic_field(nmethod,                     _deopt_mh_handler_offset,                      int)                                   \\\n@@ -590,10 +581,7 @@\n-  nonstatic_field(nmethod,                     _consts_offset,                                int)                                   \\\n-  nonstatic_field(nmethod,                     _oops_offset,                                  int)                                   \\\n-  nonstatic_field(nmethod,                     _metadata_offset,                              int)                                   \\\n-  nonstatic_field(nmethod,                     _scopes_pcs_offset,                            int)                                   \\\n-  nonstatic_field(nmethod,                     _dependencies_offset,                          int)                                   \\\n-  nonstatic_field(nmethod,                     _handler_table_offset,                         int)                                   \\\n-  nonstatic_field(nmethod,                     _nul_chk_table_offset,                         int)                                   \\\n-  nonstatic_field(nmethod,                     _nmethod_end_offset,                           int)                                   \\\n-  nonstatic_field(nmethod,                     _entry_point,                                  address)                               \\\n-  nonstatic_field(nmethod,                     _verified_entry_point,                         address)                               \\\n+  nonstatic_field(nmethod,                     _metadata_offset,                              u2)                                    \\\n+  nonstatic_field(nmethod,                     _scopes_pcs_offset,                            int)                                    \\\n+  nonstatic_field(nmethod,                     _scopes_data_offset,                           int)                                   \\\n+  nonstatic_field(nmethod,                     _handler_table_offset,                         u2)                                    \\\n+  nonstatic_field(nmethod,                     _nul_chk_table_offset,                         u2)                                    \\\n+  nonstatic_field(nmethod,                     _entry_offset,                                 u2)                                    \\\n+  nonstatic_field(nmethod,                     _verified_entry_offset,                        u2)                                    \\\n@@ -601,0 +589,2 @@\n+  nonstatic_field(nmethod,                     _immutable_data,                               address)                               \\\n+  nonstatic_field(nmethod,                     _immutable_data_size,                          int)                                   \\\n@@ -603,0 +593,1 @@\n+  volatile_nonstatic_field(nmethod,            _exception_cache,                              ExceptionCache*)                       \\\n@@ -1144,0 +1135,1 @@\n+  declare_integer_type(int16_t)                                           \\\n@@ -1318,2 +1310,1 @@\n-  declare_type(CompiledMethod,           CodeBlob)                        \\\n-  declare_type(nmethod,                  CompiledMethod)                  \\\n+  declare_type(nmethod,                  CodeBlob)                        \\\n","filename":"src\/hotspot\/share\/runtime\/vmStructs.cpp","additions":28,"deletions":37,"binary":false,"changes":65,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1996, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1996, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -5790,12 +5790,2 @@\n-        long q1, r_tmp;\n-        if (v1 == 1) {\n-            q1 = tmp;\n-            r_tmp = 0;\n-        } else if (tmp >= 0) {\n-            q1 = tmp \/ v1;\n-            r_tmp = tmp - q1 * v1;\n-        } else {\n-            long[] rq = divRemNegativeLong(tmp, v1);\n-            q1 = rq[1];\n-            r_tmp = rq[0];\n-        }\n+        long q1 = Long.divideUnsigned(tmp, v1);\n+        long r_tmp = Long.remainderUnsigned(tmp, v1);\n@@ -5812,12 +5802,2 @@\n-        long q0;\n-        if (v1 == 1) {\n-            q0 = tmp;\n-            r_tmp = 0;\n-        } else if (tmp >= 0) {\n-            q0 = tmp \/ v1;\n-            r_tmp = tmp - q0 * v1;\n-        } else {\n-            long[] rq = divRemNegativeLong(tmp, v1);\n-            q0 = rq[1];\n-            r_tmp = rq[0];\n-        }\n+        long q0 = Long.divideUnsigned(tmp, v1);\n+        r_tmp = Long.remainderUnsigned(tmp, v1);\n@@ -5903,31 +5883,0 @@\n-    \/**\n-     * Calculate the quotient and remainder of dividing a negative long by\n-     * another long.\n-     *\n-     * @param n the numerator; must be negative\n-     * @param d the denominator; must not be unity\n-     * @return a two-element {@code long} array with the remainder and quotient in\n-     *         the initial and final elements, respectively\n-     *\/\n-    private static long[] divRemNegativeLong(long n, long d) {\n-        assert n < 0 : \"Non-negative numerator \" + n;\n-        assert d != 1 : \"Unity denominator\";\n-\n-        \/\/ Approximate the quotient and remainder\n-        long q = (n >>> 1) \/ (d >>> 1);\n-        long r = n - q * d;\n-\n-        \/\/ Correct the approximation\n-        while (r < 0) {\n-            r += d;\n-            q--;\n-        }\n-        while (r >= d) {\n-            r -= d;\n-            q++;\n-        }\n-\n-        \/\/ n - q*d == r && 0 <= r < d, hence we're done.\n-        return new long[] {r, q};\n-    }\n-\n","filename":"src\/java.base\/share\/classes\/java\/math\/BigDecimal.java","additions":5,"deletions":56,"binary":false,"changes":61,"status":"modified"},{"patch":"@@ -1834,0 +1834,4 @@\n+\n+        if (z == null || z.length < (xlen + ylen))\n+            z = new int[xlen + ylen];\n+\n@@ -1842,3 +1846,0 @@\n-        if (z == null || z.length < (xlen+ ylen))\n-             z = new int[xlen+ylen];\n-\n@@ -4870,1 +4871,1 @@\n-    \/**\n+    \/*\n","filename":"src\/java.base\/share\/classes\/java\/math\/BigInteger.java","additions":5,"deletions":4,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -195,1 +195,0 @@\n-        UseNeon,\n","filename":"src\/jdk.internal.vm.ci\/share\/classes\/jdk\/vm\/ci\/aarch64\/AArch64.java","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -150,2 +150,0 @@\n-    @IR(counts = {IRNode.ABS_HF, \"> 0\", IRNode.REINTERPRET_S2HF, \"> 0\", IRNode.REINTERPRET_HF2S, \"> 0\"},\n-        applyIfCPUFeature = {\"avx512_fp16\", \"true\"})\n@@ -163,2 +161,0 @@\n-    @IR(counts = {IRNode.NEG_HF, \"> 0\", IRNode.REINTERPRET_S2HF, \"> 0\", IRNode.REINTERPRET_HF2S, \"> 0\"},\n-        applyIfCPUFeature = {\"avx512_fp16\", \"true\"})\n","filename":"test\/hotspot\/jtreg\/compiler\/intrinsics\/float16\/TestFP16ScalarOps.java","additions":0,"deletions":4,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -803,0 +803,5 @@\n+    public static final String LOAD_VECTOR_MASKED = PREFIX + \"LOAD_VECTOR_MASKED\" + POSTFIX;\n+    static {\n+        beforeMatchingNameRegex(LOAD_VECTOR_MASKED, \"LoadVectorMasked\");\n+    }\n+\n@@ -867,1 +872,6 @@\n-        beforeMatchingNameRegex(MAX, \"Max(I|L)\");\n+        beforeMatchingNameRegex(MAX, \"Max(I|L|F|D)\");\n+    }\n+\n+    public static final String MAX_D = PREFIX + \"MAX_D\" + POSTFIX;\n+    static {\n+        beforeMatchingNameRegex(MAX_D, \"MaxD\");\n@@ -880,0 +890,5 @@\n+    public static final String MAX_F = PREFIX + \"MAX_F\" + POSTFIX;\n+    static {\n+        beforeMatchingNameRegex(MAX_F, \"MaxF\");\n+    }\n+\n@@ -930,0 +945,10 @@\n+    public static final String MEMBAR_ACQUIRE = PREFIX + \"MEMBAR_ACQUIRE\" + POSTFIX;\n+    static {\n+        beforeMatchingNameRegex(MEMBAR_ACQUIRE, \"MemBarAcquire\");\n+    }\n+\n+    public static final String MEMBAR_RELEASE = PREFIX + \"MEMBAR_RELEASE\" + POSTFIX;\n+    static {\n+        beforeMatchingNameRegex(MEMBAR_RELEASE, \"MemBarRelease\");\n+    }\n+\n@@ -935,0 +960,5 @@\n+    public static final String MEMBAR_VOLATILE = PREFIX + \"MEMBAR_VOLATILE\" + POSTFIX;\n+    static {\n+        beforeMatchingNameRegex(MEMBAR_VOLATILE, \"MemBarVolatile\");\n+    }\n+\n@@ -940,0 +970,5 @@\n+    public static final String MIN_D = PREFIX + \"MIN_D\" + POSTFIX;\n+    static {\n+        beforeMatchingNameRegex(MIN_D, \"MinD\");\n+    }\n+\n@@ -950,0 +985,5 @@\n+    public static final String MIN_F = PREFIX + \"MIN_F\" + POSTFIX;\n+    static {\n+        beforeMatchingNameRegex(MIN_F, \"MinF\");\n+    }\n+\n@@ -1212,1 +1252,6 @@\n-    public static final String COUNTTRAILINGZEROS_VL = VECTOR_PREFIX + \"COUNTTRAILINGZEROS_VL\" + POSTFIX;\n+    public static final String COUNT_TRAILING_ZEROS_VL = VECTOR_PREFIX + \"COUNT_TRAILING_ZEROS_VL\" + POSTFIX;\n+    static {\n+        vectorNode(COUNT_TRAILING_ZEROS_VL, \"CountTrailingZerosV\", TYPE_LONG);\n+    }\n+\n+    public static final String COUNT_TRAILING_ZEROS_VI = VECTOR_PREFIX + \"COUNT_TRAILING_ZEROS_VI\" + POSTFIX;\n@@ -1214,1 +1259,1 @@\n-        vectorNode(COUNTTRAILINGZEROS_VL, \"CountTrailingZerosV\", TYPE_LONG);\n+        vectorNode(COUNT_TRAILING_ZEROS_VI, \"CountTrailingZerosV\", TYPE_INT);\n@@ -1217,1 +1262,1 @@\n-    public static final String COUNTLEADINGZEROS_VL = VECTOR_PREFIX + \"COUNTLEADINGZEROS_VL\" + POSTFIX;\n+    public static final String COUNT_LEADING_ZEROS_VL = VECTOR_PREFIX + \"COUNT_LEADING_ZEROS_VL\" + POSTFIX;\n@@ -1219,1 +1264,6 @@\n-        vectorNode(COUNTLEADINGZEROS_VL, \"CountLeadingZerosV\", TYPE_LONG);\n+        vectorNode(COUNT_LEADING_ZEROS_VL, \"CountLeadingZerosV\", TYPE_LONG);\n+    }\n+\n+    public static final String COUNT_LEADING_ZEROS_VI = VECTOR_PREFIX + \"COUNT_LEADING_ZEROS_VI\" + POSTFIX;\n+    static {\n+        vectorNode(COUNT_LEADING_ZEROS_VI, \"CountLeadingZerosV\", TYPE_INT);\n@@ -1547,0 +1597,5 @@\n+    public static final String STORE_VECTOR_MASKED = PREFIX + \"STORE_VECTOR_MASKED\" + POSTFIX;\n+    static {\n+        beforeMatchingNameRegex(STORE_VECTOR_MASKED, \"StoreVectorMasked\");\n+    }\n+\n@@ -2209,0 +2264,6 @@\n+    public static final String Z_COMPARE_AND_SWAP_P_WITH_BARRIER_FLAG = COMPOSITE_PREFIX + \"Z_COMPARE_AND_SWAP_P_WITH_BARRIER_FLAG\" + POSTFIX;\n+    static {\n+        String regex = START + \"zCompareAndSwapP\" + MID + \"barrier\\\\(\\\\s*\" + IS_REPLACED + \"\\\\s*\\\\)\" + END;\n+        machOnly(Z_COMPARE_AND_SWAP_P_WITH_BARRIER_FLAG, regex);\n+    }\n+\n","filename":"test\/hotspot\/jtreg\/compiler\/lib\/ir_framework\/IRNode.java","additions":66,"deletions":5,"binary":false,"changes":71,"status":"modified"},{"patch":"@@ -108,1 +108,4 @@\n-        \"asimdhp\"\n+        \"asimdhp\",\n+        \/\/ Riscv64\n+        \"v\",\n+        \"zvbb\"\n@@ -162,1 +165,1 @@\n-        } else if (irAnno.applyIfPlatform().length != 0 && !hasAllRequiredPlatform(irAnno.applyIfPlatform())) {\n+        } else if (irAnno.applyIfPlatform().length != 0 && !hasAllRequiredPlatform(irAnno.applyIfPlatform(), \"applyIfPlatform\")) {\n@@ -165,1 +168,1 @@\n-        } else if (irAnno.applyIfPlatformAnd().length != 0 && !hasAllRequiredPlatform(irAnno.applyIfPlatformAnd())) {\n+        } else if (irAnno.applyIfPlatformAnd().length != 0 && !hasAllRequiredPlatform(irAnno.applyIfPlatformAnd(), \"applyIfPlatformAnd\")) {\n@@ -168,1 +171,1 @@\n-        } else if (irAnno.applyIfPlatformOr().length != 0 && !hasAnyRequiredPlatform(irAnno.applyIfPlatformOr())) {\n+        } else if (irAnno.applyIfPlatformOr().length != 0 && !hasAnyRequiredPlatform(irAnno.applyIfPlatformOr(), \"applyIfPlatformOr\")) {\n@@ -171,1 +174,1 @@\n-        } else if (irAnno.applyIfCPUFeature().length != 0 && !hasAllRequiredCPUFeature(irAnno.applyIfCPUFeature())) {\n+        } else if (irAnno.applyIfCPUFeature().length != 0 && !hasAllRequiredCPUFeature(irAnno.applyIfCPUFeature(), \"applyIfCPUFeature\")) {\n@@ -174,1 +177,1 @@\n-        } else if (irAnno.applyIfCPUFeatureAnd().length != 0 && !hasAllRequiredCPUFeature(irAnno.applyIfCPUFeatureAnd())) {\n+        } else if (irAnno.applyIfCPUFeatureAnd().length != 0 && !hasAllRequiredCPUFeature(irAnno.applyIfCPUFeatureAnd(), \"applyIfCPUFeatureAnd\")) {\n@@ -177,1 +180,1 @@\n-        } else if (irAnno.applyIfCPUFeatureOr().length != 0 && !hasAnyRequiredCPUFeature(irAnno.applyIfCPUFeatureOr())) {\n+        } else if (irAnno.applyIfCPUFeatureOr().length != 0 && !hasAnyRequiredCPUFeature(irAnno.applyIfCPUFeatureOr(), \"applyIfCPUFeatureOr\")) {\n@@ -289,1 +292,1 @@\n-    private boolean hasAllRequiredPlatform(String[] andRules) {\n+    private boolean hasAllRequiredPlatform(String[] andRules, String ruleType) {\n@@ -294,0 +297,1 @@\n+            TestFormat.check(i < andRules.length, \"Missing value for platform \" + platform + \" in \" + ruleType + failAt());\n@@ -300,1 +304,1 @@\n-    private boolean hasAnyRequiredPlatform(String[] orRules) {\n+    private boolean hasAnyRequiredPlatform(String[] orRules, String ruleType) {\n@@ -305,0 +309,1 @@\n+            TestFormat.check(i < orRules.length, \"Missing value for platform \" + platform + \" in \" + ruleType + failAt());\n@@ -365,1 +370,1 @@\n-    private boolean hasAllRequiredCPUFeature(String[] andRules) {\n+    private boolean hasAllRequiredCPUFeature(String[] andRules, String ruleType) {\n@@ -370,0 +375,1 @@\n+            TestFormat.check(i < andRules.length, \"Missing value for cpu feature \" + feature + \" in \" + ruleType + failAt());\n@@ -376,1 +382,1 @@\n-    private boolean hasAnyRequiredCPUFeature(String[] orRules) {\n+    private boolean hasAnyRequiredCPUFeature(String[] orRules, String ruleType) {\n@@ -381,0 +387,1 @@\n+            TestFormat.check(i < orRules.length, \"Missing value for cpu feature \" + feature + \" in \" + ruleType + failAt());\n","filename":"test\/hotspot\/jtreg\/compiler\/lib\/ir_framework\/test\/IREncodingPrinter.java","additions":18,"deletions":11,"binary":false,"changes":29,"status":"modified"},{"patch":"@@ -41,1 +41,3 @@\n-    private Float16[] input;\n+    private Float16[] input1;\n+    private Float16[] input2;\n+    private Float16[] input3;\n@@ -51,1 +53,3 @@\n-        input  = new Float16[LEN];\n+        input1 = new Float16[LEN];\n+        input2 = new Float16[LEN];\n+        input3 = new Float16[LEN];\n@@ -55,1 +59,3 @@\n-            input[i] = shortBitsToFloat16(Float.floatToFloat16(rng.nextFloat()));\n+            input1[i] = shortBitsToFloat16(Float.floatToFloat16(rng.nextFloat()));\n+            input2[i] = shortBitsToFloat16(Float.floatToFloat16(rng.nextFloat()));\n+            input3[i] = shortBitsToFloat16(Float.floatToFloat16(rng.nextFloat()));\n@@ -67,1 +73,1 @@\n-            output[i] = Float16.add(input[i], input[i]);\n+            output[i] = Float16.add(input1[i], input2[i]);\n@@ -74,1 +80,1 @@\n-            Float16 expected = Float16.add(input[i], input[i]);\n+            Float16 expected = Float16.add(input1[i], input2[i]);\n@@ -89,1 +95,1 @@\n-            output[i] = Float16.subtract(input[i], input[i]);\n+            output[i] = Float16.subtract(input1[i], input2[i]);\n@@ -96,1 +102,1 @@\n-            Float16 expected = Float16.subtract(input[i], input[i]);\n+            Float16 expected = Float16.subtract(input1[i], input2[i]);\n@@ -111,1 +117,1 @@\n-            output[i] = Float16.multiply(input[i], input[i]);\n+            output[i] = Float16.multiply(input1[i], input2[i]);\n@@ -118,1 +124,1 @@\n-            Float16 expected = Float16.multiply(input[i], input[i]);\n+            Float16 expected = Float16.multiply(input1[i], input2[i]);\n@@ -133,1 +139,1 @@\n-            output[i] = Float16.divide(input[i], input[i]);\n+            output[i] = Float16.divide(input1[i], input2[i]);\n@@ -140,1 +146,1 @@\n-            Float16 expected = Float16.divide(input[i], input[i]);\n+            Float16 expected = Float16.divide(input1[i], input2[i]);\n@@ -155,1 +161,1 @@\n-            output[i] = Float16.min(input[i], input[i]);\n+            output[i] = Float16.min(input1[i], input2[i]);\n@@ -162,1 +168,1 @@\n-            Float16 expected = Float16.min(input[i], input[i]);\n+            Float16 expected = Float16.min(input1[i], input2[i]);\n@@ -177,1 +183,1 @@\n-            output[i] = Float16.max(input[i], input[i]);\n+            output[i] = Float16.max(input1[i], input2[i]);\n@@ -184,1 +190,1 @@\n-            Float16 expected = Float16.max(input[i], input[i]);\n+            Float16 expected = Float16.max(input1[i], input2[i]);\n@@ -194,1 +200,1 @@\n-        applyIfCPUFeatureOr = {\"avx512_fp16\", \"true\", \"sve\", \"true\"})\n+        applyIfCPUFeatureOr = {\"sve\", \"true\"})\n@@ -199,1 +205,1 @@\n-            output[i] = Float16.abs(input[i]);\n+            output[i] = Float16.abs(input1[i]);\n@@ -206,1 +212,1 @@\n-            Float16 expected = Float16.abs(input[i]);\n+            Float16 expected = Float16.abs(input1[i]);\n@@ -216,1 +222,1 @@\n-        applyIfCPUFeatureOr = {\"avx512_fp16\", \"true\", \"sve\", \"true\"})\n+        applyIfCPUFeatureOr = {\"sve\", \"true\"})\n@@ -221,1 +227,1 @@\n-            output[i] = Float16.negate(input[i]);\n+            output[i] = Float16.negate(input1[i]);\n@@ -228,1 +234,1 @@\n-            Float16 expected = Float16.negate(input[i]);\n+            Float16 expected = Float16.negate(input1[i]);\n@@ -243,1 +249,1 @@\n-            output[i] = Float16.sqrt(input[i]);\n+            output[i] = Float16.sqrt(input1[i]);\n@@ -250,1 +256,1 @@\n-            Float16 expected = Float16.sqrt(input[i]);\n+            Float16 expected = Float16.sqrt(input1[i]);\n@@ -265,1 +271,1 @@\n-            output[i] = Float16.fma(input[i], input[i], input[i]);\n+            output[i] = Float16.fma(input1[i], input2[i], input3[i]);\n@@ -272,1 +278,1 @@\n-            Float16 expected = Float16.fma(input[i], input[i], input[i]);\n+            Float16 expected = Float16.fma(input1[i], input2[i], input3[i]);\n","filename":"test\/hotspot\/jtreg\/compiler\/vectorization\/TestFloat16VectorOps.java","additions":31,"deletions":25,"binary":false,"changes":56,"status":"modified"}]}