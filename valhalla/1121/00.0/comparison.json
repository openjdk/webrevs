{"files":[{"patch":"@@ -14,1 +14,1 @@\n-files=.*\\.cpp|.*\\.hpp|.*\\.c|.*\\.h|.*\\.java|.*\\.cc|.*\\.hh|.*\\.m|.*\\.mm|.*\\.md|.*\\.gmk|.*\\.m4|.*\\.ac|Makefile\n+files=.*\\.cpp|.*\\.hpp|.*\\.c|.*\\.h|.*\\.java|.*\\.cc|.*\\.hh|.*\\.m|.*\\.mm|.*\\.md|.*\\.properties|.*\\.gmk|.*\\.m4|.*\\.ac|Makefile\n","filename":".jcheck\/conf","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2015, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2015, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -444,1 +444,1 @@\n-            dependencies: [\"devkit\", \"gtest\", \"pandoc\"],\n+            dependencies: [\"devkit\", \"gtest\", \"graphviz\", \"pandoc\"],\n@@ -456,1 +456,1 @@\n-            dependencies: [\"devkit\", \"gtest\", \"pandoc\"],\n+            dependencies: [\"devkit\", \"gtest\", \"graphviz\", \"pandoc\"],\n@@ -489,1 +489,1 @@\n-            dependencies: [\"devkit\", \"gtest\", \"build_devkit\", \"pandoc\"],\n+            dependencies: [\"devkit\", \"gtest\", \"build_devkit\", \"graphviz\", \"pandoc\"],\n@@ -1184,6 +1184,0 @@\n-        cups: {\n-            organization: common.organization,\n-            ext: \"tar.gz\",\n-            revision: \"1.0118+1.0\"\n-        },\n-\n@@ -1240,1 +1234,1 @@\n-            revision: \"2.38.0-1+1.1\",\n+            revision: \"9.0.0+1.0\",\n","filename":"make\/conf\/jib-profiles.js","additions":5,"deletions":11,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -2212,4 +2212,3 @@\n-    st->print_cr(\"\\tldrw rscratch1, j_rarg0 + oopDesc::klass_offset_in_bytes()]\\t# compressed klass\");\n-    if (CompressedKlassPointers::shift() != 0) {\n-      st->print_cr(\"\\tdecode_klass_not_null rscratch1, rscratch1\");\n-    }\n+    st->print_cr(\"\\tldrw rscratch1, [j_rarg0 + oopDesc::klass_offset_in_bytes()]\\t# compressed klass\");\n+    st->print_cr(\"\\tldrw r10, [rscratch2 + CompiledICData::speculated_klass_offset()]\\t# compressed klass\");\n+    st->print_cr(\"\\tcmpw rscratch1, r10\");\n@@ -2217,1 +2216,3 @@\n-   st->print_cr(\"\\tldr rscratch1, j_rarg0 + oopDesc::klass_offset_in_bytes()]\\t# compressed klass\");\n+    st->print_cr(\"\\tldr rscratch1, [j_rarg0 + oopDesc::klass_offset_in_bytes()]\\t# compressed klass\");\n+    st->print_cr(\"\\tldr r10, [rscratch2 + CompiledICData::speculated_klass_offset()]\\t# compressed klass\");\n+    st->print_cr(\"\\tcmp rscratch1, r10\");\n@@ -2219,1 +2220,0 @@\n-  st->print_cr(\"\\tcmp r0, rscratch1\\t # Inline cache check\");\n@@ -2228,10 +2228,1 @@\n-  Label skip;\n-\n-  \/\/ UseCompressedClassPointers logic are inside cmp_klass\n-  __ cmp_klass(j_rarg0, rscratch2, rscratch1);\n-\n-  \/\/ TODO\n-  \/\/ can we avoid this skip and still use a reloc?\n-  __ br(Assembler::EQ, skip);\n-  __ far_jump(RuntimeAddress(SharedRuntime::get_ic_miss_stub()));\n-  __ bind(skip);\n+  __ ic_check(InteriorEntryAlignment);\n@@ -2586,1 +2577,1 @@\n-bool is_valid_sve_arith_imm_pattern(Node* n, Node* m) {\n+static bool is_valid_sve_arith_imm_pattern(Node* n, Node* m) {\n@@ -2627,1 +2618,1 @@\n-bool is_vector_bitwise_not_pattern(Node* n, Node* m) {\n+static bool is_vector_bitwise_not_pattern(Node* n, Node* m) {\n@@ -3719,1 +3710,1 @@\n-        address stub = CompiledStaticCall::emit_to_interp_stub(cbuf, call);\n+        address stub = CompiledDirectCall::emit_to_interp_stub(cbuf, call);\n@@ -8324,1 +8315,1 @@\n-            \"dmb ishld\" %}\n+            \"dmb ish\" %}\n@@ -8378,1 +8369,1 @@\n-            \"dmb ishst\\n\\tdmb ishld\" %}\n+            \"dmb ish\" %}\n@@ -8382,2 +8373,1 @@\n-    __ membar(Assembler::StoreStore);\n-    __ membar(Assembler::LoadStore);\n+    __ membar(Assembler::LoadStore|Assembler::StoreStore);\n","filename":"src\/hotspot\/cpu\/aarch64\/aarch64.ad","additions":13,"deletions":23,"binary":false,"changes":36,"status":"modified"},{"patch":"@@ -58,1 +58,0 @@\n-const Register IC_Klass    = rscratch2;   \/\/ where the IC klass is cached\n@@ -298,21 +297,1 @@\n-  Register receiver = FrameMap::receiver_opr->as_register();\n-  Register ic_klass = IC_Klass;\n-  int start_offset = __ offset();\n-  __ inline_cache_check(receiver, ic_klass);\n-\n-  \/\/ if icache check fails, then jump to runtime routine\n-  \/\/ Note: RECEIVER must still contain the receiver!\n-  Label dont;\n-  __ br(Assembler::EQ, dont);\n-  __ far_jump(RuntimeAddress(SharedRuntime::get_ic_miss_stub()));\n-\n-  \/\/ We align the verified entry point unless the method body\n-  \/\/ (including its inline cache check) will fit in a single 64-byte\n-  \/\/ icache line.\n-  if (! method()->is_accessor() || __ offset() - start_offset > 4 * 4) {\n-    \/\/ force alignment after the cache check.\n-    __ align(CodeEntryAlignment);\n-  }\n-\n-  __ bind(dont);\n-  return start_offset;\n+  return __ ic_check(CodeEntryAlignment);\n@@ -2203,1 +2182,1 @@\n-  assert(__ offset() - start + CompiledStaticCall::to_trampoline_stub_size()\n+  assert(__ offset() - start + CompiledDirectCall::to_trampoline_stub_size()\n","filename":"src\/hotspot\/cpu\/aarch64\/c1_LIRAssembler_aarch64.cpp","additions":2,"deletions":23,"binary":false,"changes":25,"status":"modified"},{"patch":"@@ -74,2 +74,2 @@\n-    \/\/ call stub: CompiledStaticCall::to_interp_stub_size() +\n-    \/\/            CompiledStaticCall::to_trampoline_stub_size()\n+    \/\/ call stub: CompiledDirectCall::to_interp_stub_size() +\n+    \/\/            CompiledDirectCall::to_trampoline_stub_size()\n","filename":"src\/hotspot\/cpu\/aarch64\/c1_LIRAssembler_aarch64.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -326,10 +326,0 @@\n-\n-void C1_MacroAssembler::inline_cache_check(Register receiver, Register iCache) {\n-  verify_oop(receiver);\n-  \/\/ explicit null check not needed since load from [klass_offset] causes a trap\n-  \/\/ check against inline cache\n-  assert(!MacroAssembler::needs_explicit_null_check(oopDesc::klass_offset_in_bytes()), \"must add explicit null check\");\n-\n-  cmp_klass(receiver, iCache, rscratch1);\n-}\n-\n","filename":"src\/hotspot\/cpu\/aarch64\/c1_MacroAssembler_aarch64.cpp","additions":0,"deletions":10,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -41,1 +41,0 @@\n-#include \"oops\/compiledICHolder.hpp\"\n","filename":"src\/hotspot\/cpu\/aarch64\/c1_Runtime1_aarch64.cpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -684,1 +684,1 @@\n-void internal_pf(uintptr_t sp, uintptr_t fp, uintptr_t pc, uintptr_t bcx) {\n+static void internal_pf(uintptr_t sp, uintptr_t fp, uintptr_t pc, uintptr_t bcx) {\n","filename":"src\/hotspot\/cpu\/aarch64\/frame_aarch64.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -133,2 +133,0 @@\n-  product(bool, AlwaysMergeDMB, false, DIAGNOSTIC,                      \\\n-          \"Always merge DMB instructions in code emission\")             \\\n","filename":"src\/hotspot\/cpu\/aarch64\/globals_aarch64.hpp","additions":0,"deletions":2,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -33,0 +33,1 @@\n+#include \"code\/compiledIC.hpp\"\n@@ -972,1 +973,1 @@\n-  \/\/ CompiledDirectStaticCall::set_to_interpreted knows the\n+  \/\/ CompiledDirectCall::set_to_interpreted knows the\n@@ -1002,1 +1003,1 @@\n-  movptr(rscratch2, (uintptr_t)Universe::non_oop_word());\n+  movptr(rscratch2, (intptr_t)Universe::non_oop_word());\n@@ -1006,0 +1007,41 @@\n+int MacroAssembler::ic_check_size() {\n+  if (target_needs_far_branch(CAST_FROM_FN_PTR(address, SharedRuntime::get_ic_miss_stub()))) {\n+    return NativeInstruction::instruction_size * 7;\n+  } else {\n+    return NativeInstruction::instruction_size * 5;\n+  }\n+}\n+\n+int MacroAssembler::ic_check(int end_alignment) {\n+  Register receiver = j_rarg0;\n+  Register data = rscratch2;\n+  Register tmp1 = rscratch1;\n+  Register tmp2 = r10;\n+\n+  \/\/ The UEP of a code blob ensures that the VEP is padded. However, the padding of the UEP is placed\n+  \/\/ before the inline cache check, so we don't have to execute any nop instructions when dispatching\n+  \/\/ through the UEP, yet we can ensure that the VEP is aligned appropriately. That's why we align\n+  \/\/ before the inline cache check here, and not after\n+  align(end_alignment, offset() + ic_check_size());\n+\n+  int uep_offset = offset();\n+\n+  if (UseCompressedClassPointers) {\n+    ldrw(tmp1, Address(receiver, oopDesc::klass_offset_in_bytes()));\n+    ldrw(tmp2, Address(data, CompiledICData::speculated_klass_offset()));\n+    cmpw(tmp1, tmp2);\n+  } else {\n+    ldr(tmp1, Address(receiver, oopDesc::klass_offset_in_bytes()));\n+    ldr(tmp2, Address(data, CompiledICData::speculated_klass_offset()));\n+    cmp(tmp1, tmp2);\n+  }\n+\n+  Label dont;\n+  br(Assembler::EQ, dont);\n+  far_jump(RuntimeAddress(SharedRuntime::get_ic_miss_stub()));\n+  bind(dont);\n+  assert((offset() % end_alignment) == 0, \"Misaligned verified entry point\");\n+\n+  return uep_offset;\n+}\n+\n@@ -1107,1 +1149,8 @@\n-  while (offset() % modulus != 0) nop();\n+  align(modulus, offset());\n+}\n+\n+\/\/ Ensure that the code at target bytes offset from the current offset() is aligned\n+\/\/ according to modulus.\n+void MacroAssembler::align(int modulus, int target) {\n+  int delta = target - offset();\n+  while ((offset() + delta) % modulus != 0) nop();\n@@ -1239,1 +1288,1 @@\n-\/\/ - check recv_klass (actual object class) is a subtype of resolved_klass from CompiledICHolder\n+\/\/ - check recv_klass (actual object class) is a subtype of resolved_klass from CompiledICData\n@@ -2225,12 +2274,7 @@\n-    \/\/ Don't promote DMB ST|DMB LD to DMB (a full barrier) because\n-    \/\/ doing so would introduce a StoreLoad which the caller did not\n-    \/\/ intend\n-    if (AlwaysMergeDMB || bar->get_kind() == order_constraint\n-        || bar->get_kind() == AnyAny\n-        || order_constraint == AnyAny) {\n-      \/\/ We are merging two memory barrier instructions.  On AArch64 we\n-      \/\/ can do this simply by ORing them together.\n-      bar->set_kind(bar->get_kind() | order_constraint);\n-      BLOCK_COMMENT(\"merged membar\");\n-      return;\n-    }\n+    \/\/ We are merging two memory barrier instructions.  On AArch64 we\n+    \/\/ can do this simply by ORing them together.\n+    bar->set_kind(bar->get_kind() | order_constraint);\n+    BLOCK_COMMENT(\"merged membar\");\n+  } else {\n+    code()->set_last_insn(pc());\n+    dmb(Assembler::barrier(order_constraint));\n@@ -2238,2 +2282,0 @@\n-  code()->set_last_insn(pc());\n-  dmb(Assembler::barrier(order_constraint));\n","filename":"src\/hotspot\/cpu\/aarch64\/macroAssembler_aarch64.cpp","additions":60,"deletions":18,"binary":false,"changes":78,"status":"modified"},{"patch":"@@ -758,0 +758,1 @@\n+  void align(int modulus, int target);\n@@ -1310,0 +1311,2 @@\n+  static int ic_check_size();\n+  int ic_check(int end_alignment);\n","filename":"src\/hotspot\/cpu\/aarch64\/macroAssembler_aarch64.hpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -34,1 +34,0 @@\n-#include \"code\/icBuffer.hpp\"\n@@ -43,1 +42,0 @@\n-#include \"oops\/compiledICHolder.hpp\"\n@@ -989,0 +987,3 @@\n+  Register data = rscratch2;\n+  __ ic_check(1 \/* end_alignment *\/);\n+  __ ldr(rmethod, Address(data, CompiledICData::speculated_method_offset()));\n@@ -990,33 +991,6 @@\n-  Label ok;\n-\n-  Register holder = rscratch2;\n-  Register receiver = j_rarg0;\n-  Register tmp = r10;  \/\/ A call-clobbered register not used for arg passing\n-\n-  \/\/ -------------------------------------------------------------------------\n-  \/\/ Generate a C2I adapter.  On entry we know rmethod holds the Method* during calls\n-  \/\/ to the interpreter.  The args start out packed in the compiled layout.  They\n-  \/\/ need to be unpacked into the interpreter layout.  This will almost always\n-  \/\/ require some stack space.  We grow the current (compiled) stack, then repack\n-  \/\/ the args.  We  finally end in a jump to the generic interpreter entry point.\n-  \/\/ On exit from the interpreter, the interpreter will restore our SP (lest the\n-  \/\/ compiled code, which relies solely on SP and not FP, get sick).\n-\n-  {\n-    __ block_comment(\"c2i_unverified_entry {\");\n-    __ load_klass(rscratch1, receiver);\n-    __ ldr(tmp, Address(holder, CompiledICHolder::holder_klass_offset()));\n-    __ cmp(rscratch1, tmp);\n-    __ ldr(rmethod, Address(holder, CompiledICHolder::holder_metadata_offset()));\n-    __ br(Assembler::EQ, ok);\n-    __ far_jump(RuntimeAddress(SharedRuntime::get_ic_miss_stub()));\n-\n-    __ bind(ok);\n-    \/\/ Method might have been compiled since the call site was patched to\n-    \/\/ interpreted; if that is the case treat it as a miss so we can get\n-    \/\/ the call site corrected.\n-    __ ldr(rscratch1, Address(rmethod, in_bytes(Method::code_offset())));\n-    __ cbz(rscratch1, skip_fixup);\n-    __ far_jump(RuntimeAddress(SharedRuntime::get_ic_miss_stub()));\n-    __ block_comment(\"} c2i_unverified_entry\");\n-  }\n+  \/\/ Method might have been compiled since the call site was patched to\n+  \/\/ interpreted; if that is the case treat it as a miss so we can get\n+  \/\/ the call site corrected.\n+  __ ldr(rscratch1, Address(rmethod, in_bytes(Method::code_offset())));\n+  __ cbz(rscratch1, skip_fixup);\n+  __ far_jump(RuntimeAddress(SharedRuntime::get_ic_miss_stub()));\n@@ -1025,1 +999,0 @@\n-\n@@ -1042,0 +1015,9 @@\n+  \/\/ -------------------------------------------------------------------------\n+  \/\/ Generate a C2I adapter.  On entry we know rmethod holds the Method* during calls\n+  \/\/ to the interpreter.  The args start out packed in the compiled layout.  They\n+  \/\/ need to be unpacked into the interpreter layout.  This will almost always\n+  \/\/ require some stack space.  We grow the current (compiled) stack, then repack\n+  \/\/ the args.  We  finally end in a jump to the generic interpreter entry point.\n+  \/\/ On exit from the interpreter, the interpreter will restore our SP (lest the\n+  \/\/ compiled code, which relies solely on SP and not FP, get sick).\n+\n@@ -1405,1 +1387,1 @@\n-    address stub = CompiledStaticCall::emit_to_interp_stub(*cbuf, tr_call);\n+    address stub = CompiledDirectCall::emit_to_interp_stub(*cbuf, tr_call);\n@@ -1470,1 +1452,1 @@\n-  address stub = CompiledStaticCall::emit_to_interp_stub(*cbuf, tr_call);\n+  address stub = CompiledDirectCall::emit_to_interp_stub(*cbuf, tr_call);\n@@ -1678,0 +1660,1 @@\n+    if (nm == nullptr) return nm;\n@@ -1825,2 +1808,0 @@\n-\n-  const Register ic_reg = rscratch2;\n@@ -1829,1 +1810,0 @@\n-  Label hit;\n@@ -1832,1 +1812,1 @@\n-  assert_different_registers(ic_reg, receiver, rscratch1);\n+  assert_different_registers(receiver, rscratch1);\n@@ -1834,4 +1814,1 @@\n-  __ cmp_klass(receiver, ic_reg, rscratch1);\n-  __ br(Assembler::EQ, hit);\n-\n-  __ far_jump(RuntimeAddress(SharedRuntime::get_ic_miss_stub()));\n+  __ ic_check(8 \/* end_alignment *\/);\n@@ -1840,4 +1817,0 @@\n-  __ align(8);\n-\n-  __ bind(hit);\n-\n","filename":"src\/hotspot\/cpu\/aarch64\/sharedRuntime_aarch64.cpp","additions":23,"deletions":50,"binary":false,"changes":73,"status":"modified"},{"patch":"@@ -29,0 +29,1 @@\n+#include \"code\/compiledIC.hpp\"\n@@ -32,1 +33,0 @@\n-#include \"oops\/compiledICHolder.hpp\"\n@@ -180,1 +180,1 @@\n-  \/\/  rscratch2: CompiledICHolder\n+  \/\/  rscratch2: CompiledICData\n@@ -186,1 +186,1 @@\n-  const Register holder_klass_reg   = r16; \/\/ declaring interface klass (DECC)\n+  const Register holder_klass_reg   = r16; \/\/ declaring interface klass (DEFC)\n@@ -190,1 +190,1 @@\n-  const Register icholder_reg       = rscratch2;\n+  const Register icdata_reg         = rscratch2;\n@@ -194,2 +194,2 @@\n-  __ ldr(resolved_klass_reg, Address(icholder_reg, CompiledICHolder::holder_klass_offset()));\n-  __ ldr(holder_klass_reg,   Address(icholder_reg, CompiledICHolder::holder_metadata_offset()));\n+  __ ldr(resolved_klass_reg, Address(icdata_reg, CompiledICData::itable_refc_klass_offset()));\n+  __ ldr(holder_klass_reg,   Address(icdata_reg, CompiledICData::itable_defc_klass_offset()));\n","filename":"src\/hotspot\/cpu\/aarch64\/vtableStubs_aarch64.cpp","additions":6,"deletions":6,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -164,4 +164,1 @@\n-  Register receiver = LIR_Assembler::receiverOpr()->as_register();\n-  int offset = __ offset();\n-  __ inline_cache_check(receiver, Ricklass);\n-  return offset;\n+  return __ ic_check(CodeEntryAlignment);\n@@ -1953,1 +1950,1 @@\n-  \/\/ (See CompiledStaticCall::set_to_interpreted())\n+  \/\/ (See CompiledDirectCall::set_to_interpreted())\n","filename":"src\/hotspot\/cpu\/arm\/c1_LIRAssembler_arm.cpp","additions":2,"deletions":5,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -80,3 +80,1 @@\n-  int offset = __ offset();\n-  __ inline_cache_check(R3_ARG1, R19_inline_cache_reg);\n-  return offset;\n+  return __ ic_check(CodeEntryAlignment);\n","filename":"src\/hotspot\/cpu\/ppc\/c1_LIRAssembler_ppc.cpp","additions":1,"deletions":3,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -79,4 +79,1 @@\n-  Register receiver = receiverOpr()->as_register();\n-  int offset = __ offset();\n-  __ inline_cache_check(receiver, Z_inline_cache);\n-  return offset;\n+  return __ ic_check(CodeEntryAlignment);\n","filename":"src\/hotspot\/cpu\/s390\/c1_LIRAssembler_s390.cpp","additions":1,"deletions":4,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -77,1 +77,0 @@\n-const Register IC_Klass    = rax;   \/\/ where the IC klass is cached\n@@ -341,17 +340,1 @@\n-  Register receiver = FrameMap::receiver_opr->as_register();\n-  Register ic_klass = IC_Klass;\n-  const int ic_cmp_size = LP64_ONLY(10) NOT_LP64(9);\n-  const bool do_post_padding = VerifyOops || UseCompressedClassPointers;\n-  if (!do_post_padding) {\n-    \/\/ insert some nops so that the verified entry point is aligned on CodeEntryAlignment\n-    __ align(CodeEntryAlignment, __ offset() + ic_cmp_size);\n-  }\n-  int offset = __ offset();\n-  __ inline_cache_check(receiver, IC_Klass);\n-  assert(__ offset() % CodeEntryAlignment == 0 || do_post_padding, \"alignment must be correct\");\n-  if (do_post_padding) {\n-    \/\/ force alignment after the cache check.\n-    \/\/ It's been verified to be aligned if !VerifyOops\n-    __ align(CodeEntryAlignment);\n-  }\n-  return offset;\n+  return __ ic_check(CodeEntryAlignment);\n","filename":"src\/hotspot\/cpu\/x86\/c1_LIRAssembler_x86.cpp","additions":1,"deletions":18,"binary":false,"changes":19,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2005, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2005, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -1235,0 +1235,1 @@\n+#ifndef _LP64\n@@ -1237,1 +1238,1 @@\n-LIR_Opr fixed_register_for(BasicType type) {\n+static LIR_Opr fixed_register_for(BasicType type) {\n@@ -1246,0 +1247,1 @@\n+#endif\n","filename":"src\/hotspot\/cpu\/x86\/c1_LIRGenerator_x86.cpp","additions":4,"deletions":2,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1999, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1999, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -28,0 +28,1 @@\n+#include \"code\/compiledIC.hpp\"\n@@ -38,0 +39,1 @@\n+#include \"runtime\/globals.hpp\"\n@@ -42,0 +44,1 @@\n+#include \"utilities\/globalDefinitions.hpp\"\n@@ -64,3 +67,0 @@\n-  \/\/ Load object header\n-  movptr(hdr, Address(obj, hdr_offset));\n-\n@@ -77,0 +77,2 @@\n+    \/\/ Load object header\n+    movptr(hdr, Address(obj, hdr_offset));\n@@ -142,3 +144,8 @@\n-    movptr(disp_hdr, Address(obj, hdr_offset));\n-    andptr(disp_hdr, ~(int32_t)markWord::lock_mask_in_place);\n-    lightweight_unlock(obj, disp_hdr, hdr, slow_case);\n+#ifdef _LP64\n+    lightweight_unlock(obj, disp_hdr, r15_thread, hdr, slow_case);\n+#else\n+    \/\/ This relies on the implementation of lightweight_unlock being able to handle\n+    \/\/ that the reg_rax and thread Register parameters may alias each other.\n+    get_thread(disp_hdr);\n+    lightweight_unlock(obj, disp_hdr, disp_hdr, hdr, slow_case);\n+#endif\n@@ -311,23 +318,0 @@\n-\n-\n-void C1_MacroAssembler::inline_cache_check(Register receiver, Register iCache) {\n-  verify_oop(receiver);\n-  \/\/ explicit null check not needed since load from [klass_offset] causes a trap\n-  \/\/ check against inline cache\n-  assert(!MacroAssembler::needs_explicit_null_check(oopDesc::klass_offset_in_bytes()), \"must add explicit null check\");\n-  int start_offset = offset();\n-\n-  if (UseCompressedClassPointers) {\n-    load_klass(rscratch1, receiver, rscratch2);\n-    cmpptr(rscratch1, iCache);\n-  } else {\n-    cmpptr(iCache, Address(receiver, oopDesc::klass_offset_in_bytes()));\n-  }\n-  \/\/ if icache check fails, then jump to runtime routine\n-  \/\/ Note: RECEIVER must still contain the receiver!\n-  jump_cc(Assembler::notEqual,\n-          RuntimeAddress(SharedRuntime::get_ic_miss_stub()));\n-  const int ic_cmp_size = LP64_ONLY(10) NOT_LP64(9);\n-  assert(UseCompressedClassPointers || offset() - start_offset == ic_cmp_size, \"check alignment in emit_method_entry\");\n-}\n-\n","filename":"src\/hotspot\/cpu\/x86\/c1_MacroAssembler_x86.cpp","additions":14,"deletions":30,"binary":false,"changes":44,"status":"modified"},{"patch":"@@ -41,1 +41,0 @@\n-#include \"oops\/compiledICHolder.hpp\"\n","filename":"src\/hotspot\/cpu\/x86\/c1_Runtime1_x86.cpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -36,0 +36,1 @@\n+#include \"runtime\/globals.hpp\"\n@@ -39,0 +40,3 @@\n+#include \"utilities\/globalDefinitions.hpp\"\n+#include \"utilities\/powerOfTwo.hpp\"\n+#include \"utilities\/sizes.hpp\"\n@@ -582,0 +586,1 @@\n+  assert(LockingMode != LM_LIGHTWEIGHT, \"lightweight locking should use fast_lock_lightweight\");\n@@ -633,1 +638,2 @@\n-  } else if (LockingMode == LM_LEGACY) {\n+  } else {\n+    assert(LockingMode == LM_LEGACY, \"must be\");\n@@ -652,4 +658,0 @@\n-  } else {\n-    assert(LockingMode == LM_LIGHTWEIGHT, \"\");\n-    lightweight_lock(objReg, tmpReg, thread, scrReg, NO_COUNT);\n-    jmp(COUNT);\n@@ -786,0 +788,1 @@\n+  assert(LockingMode != LM_LIGHTWEIGHT, \"lightweight locking should use fast_unlock_lightweight\");\n@@ -816,17 +819,0 @@\n-  if (LockingMode == LM_LIGHTWEIGHT) {\n-    \/\/ If the owner is ANONYMOUS, we need to fix it -  in an outline stub.\n-    testb(Address(tmpReg, OM_OFFSET_NO_MONITOR_VALUE_TAG(owner)), (int32_t) ObjectMonitor::ANONYMOUS_OWNER);\n-#ifdef _LP64\n-    if (!Compile::current()->output()->in_scratch_emit_size()) {\n-      C2HandleAnonOMOwnerStub* stub = new (Compile::current()->comp_arena()) C2HandleAnonOMOwnerStub(tmpReg, boxReg);\n-      Compile::current()->output()->add_stub(stub);\n-      jcc(Assembler::notEqual, stub->entry());\n-      bind(stub->continuation());\n-    } else\n-#endif\n-    {\n-      \/\/ We can't easily implement this optimization on 32 bit because we don't have a thread register.\n-      \/\/ Call the slow-path instead.\n-      jcc(Assembler::notEqual, NO_COUNT);\n-    }\n-  }\n@@ -954,1 +940,1 @@\n-  if (LockingMode != LM_MONITOR) {\n+  if (LockingMode == LM_LEGACY) {\n@@ -956,9 +942,3 @@\n-    if (LockingMode == LM_LIGHTWEIGHT) {\n-      mov(boxReg, tmpReg);\n-      lightweight_unlock(objReg, boxReg, tmpReg, NO_COUNT);\n-      jmp(COUNT);\n-    } else if (LockingMode == LM_LEGACY) {\n-      movptr(tmpReg, Address (boxReg, 0));      \/\/ re-fetch\n-      lock();\n-      cmpxchgptr(tmpReg, Address(objReg, oopDesc::mark_offset_in_bytes())); \/\/ Uses RAX which is box\n-    }\n+    movptr(tmpReg, Address (boxReg, 0));      \/\/ re-fetch\n+    lock();\n+    cmpxchgptr(tmpReg, Address(objReg, oopDesc::mark_offset_in_bytes())); \/\/ Uses RAX which is box\n@@ -967,0 +947,1 @@\n+\n@@ -987,0 +968,241 @@\n+void C2_MacroAssembler::fast_lock_lightweight(Register obj, Register box, Register rax_reg,\n+                                              Register t, Register thread) {\n+  assert(LockingMode == LM_LIGHTWEIGHT, \"must be\");\n+  assert(rax_reg == rax, \"Used for CAS\");\n+  assert_different_registers(obj, box, rax_reg, t, thread);\n+\n+  \/\/ Handle inflated monitor.\n+  Label inflated;\n+  \/\/ Finish fast lock successfully. ZF value is irrelevant.\n+  Label locked;\n+  \/\/ Finish fast lock unsuccessfully. MUST jump with ZF == 0\n+  Label slow_path;\n+\n+  if (DiagnoseSyncOnValueBasedClasses != 0) {\n+    load_klass(rax_reg, obj, t);\n+    movl(rax_reg, Address(rax_reg, Klass::access_flags_offset()));\n+    testl(rax_reg, JVM_ACC_IS_VALUE_BASED_CLASS);\n+    jcc(Assembler::notZero, slow_path);\n+  }\n+\n+  const Register mark = t;\n+\n+  { \/\/ Lightweight Lock\n+\n+    Label push;\n+\n+    const Register top = box;\n+\n+    \/\/ Load the mark.\n+    movptr(mark, Address(obj, oopDesc::mark_offset_in_bytes()));\n+\n+    \/\/ Prefetch top.\n+    movl(top, Address(thread, JavaThread::lock_stack_top_offset()));\n+\n+    \/\/ Check for monitor (0b10).\n+    testptr(mark, markWord::monitor_value);\n+    jcc(Assembler::notZero, inflated);\n+\n+    \/\/ Check if lock-stack is full.\n+    cmpl(top, LockStack::end_offset() - 1);\n+    jcc(Assembler::greater, slow_path);\n+\n+    \/\/ Check if recursive.\n+    cmpptr(obj, Address(thread, top, Address::times_1, -oopSize));\n+    jccb(Assembler::equal, push);\n+\n+    \/\/ Try to lock. Transition lock bits 0b01 => 0b00\n+    movptr(rax_reg, mark);\n+    orptr(rax_reg, markWord::unlocked_value);\n+    andptr(mark, ~(int32_t)markWord::unlocked_value);\n+    lock(); cmpxchgptr(mark, Address(obj, oopDesc::mark_offset_in_bytes()));\n+    jcc(Assembler::notEqual, slow_path);\n+\n+    bind(push);\n+    \/\/ After successful lock, push object on lock-stack.\n+    movptr(Address(thread, top), obj);\n+    addl(Address(thread, JavaThread::lock_stack_top_offset()), oopSize);\n+    jmpb(locked);\n+  }\n+\n+  { \/\/ Handle inflated monitor.\n+    bind(inflated);\n+\n+    const Register tagged_monitor = mark;\n+\n+    \/\/ CAS owner (null => current thread).\n+    xorptr(rax_reg, rax_reg);\n+    lock(); cmpxchgptr(thread, Address(tagged_monitor, OM_OFFSET_NO_MONITOR_VALUE_TAG(owner)));\n+    jccb(Assembler::equal, locked);\n+\n+    \/\/ Check if recursive.\n+    cmpptr(thread, rax_reg);\n+    jccb(Assembler::notEqual, slow_path);\n+\n+    \/\/ Recursive.\n+    increment(Address(tagged_monitor, OM_OFFSET_NO_MONITOR_VALUE_TAG(recursions)));\n+  }\n+\n+  bind(locked);\n+  increment(Address(thread, JavaThread::held_monitor_count_offset()));\n+  \/\/ Set ZF = 1\n+  xorl(rax_reg, rax_reg);\n+\n+#ifdef ASSERT\n+  \/\/ Check that locked label is reached with ZF set.\n+  Label zf_correct;\n+  jccb(Assembler::zero, zf_correct);\n+  stop(\"Fast Lock ZF != 1\");\n+#endif\n+\n+  bind(slow_path);\n+#ifdef ASSERT\n+  \/\/ Check that slow_path label is reached with ZF not set.\n+  jccb(Assembler::notZero, zf_correct);\n+  stop(\"Fast Lock ZF != 0\");\n+  bind(zf_correct);\n+#endif\n+  \/\/ C2 uses the value of ZF to determine the continuation.\n+}\n+\n+void C2_MacroAssembler::fast_unlock_lightweight(Register obj, Register reg_rax, Register t, Register thread) {\n+  assert(LockingMode == LM_LIGHTWEIGHT, \"must be\");\n+  assert(reg_rax == rax, \"Used for CAS\");\n+  assert_different_registers(obj, reg_rax, t);\n+\n+  \/\/ Handle inflated monitor.\n+  Label inflated, inflated_check_lock_stack;\n+  \/\/ Finish fast unlock successfully.  MUST jump with ZF == 1\n+  Label unlocked;\n+\n+  \/\/ Assume success.\n+  decrement(Address(thread, JavaThread::held_monitor_count_offset()));\n+\n+  const Register mark = t;\n+  const Register top = reg_rax;\n+\n+  Label dummy;\n+  C2FastUnlockLightweightStub* stub = nullptr;\n+\n+  if (!Compile::current()->output()->in_scratch_emit_size()) {\n+    stub = new (Compile::current()->comp_arena()) C2FastUnlockLightweightStub(obj, mark, reg_rax, thread);\n+    Compile::current()->output()->add_stub(stub);\n+  }\n+\n+  Label& push_and_slow_path = stub == nullptr ? dummy : stub->push_and_slow_path();\n+  Label& check_successor = stub == nullptr ? dummy : stub->check_successor();\n+\n+  { \/\/ Lightweight Unlock\n+\n+    \/\/ Load top.\n+    movl(top, Address(thread, JavaThread::lock_stack_top_offset()));\n+\n+    \/\/ Prefetch mark.\n+    movptr(mark, Address(obj, oopDesc::mark_offset_in_bytes()));\n+\n+    \/\/ Check if obj is top of lock-stack.\n+    cmpptr(obj, Address(thread, top, Address::times_1, -oopSize));\n+    \/\/ Top of lock stack was not obj. Must be monitor.\n+    jcc(Assembler::notEqual, inflated_check_lock_stack);\n+\n+    \/\/ Pop lock-stack.\n+    DEBUG_ONLY(movptr(Address(thread, top, Address::times_1, -oopSize), 0);)\n+    subl(Address(thread, JavaThread::lock_stack_top_offset()), oopSize);\n+\n+    \/\/ Check if recursive.\n+    cmpptr(obj, Address(thread, top, Address::times_1, -2 * oopSize));\n+    jcc(Assembler::equal, unlocked);\n+\n+    \/\/ We elide the monitor check, let the CAS fail instead.\n+\n+    \/\/ Try to unlock. Transition lock bits 0b00 => 0b01\n+    movptr(reg_rax, mark);\n+    andptr(reg_rax, ~(int32_t)markWord::lock_mask);\n+    orptr(mark, markWord::unlocked_value);\n+    lock(); cmpxchgptr(mark, Address(obj, oopDesc::mark_offset_in_bytes()));\n+    jcc(Assembler::notEqual, push_and_slow_path);\n+    jmp(unlocked);\n+  }\n+\n+\n+  { \/\/ Handle inflated monitor.\n+    bind(inflated_check_lock_stack);\n+#ifdef ASSERT\n+    Label check_done;\n+    subl(top, oopSize);\n+    cmpl(top, in_bytes(JavaThread::lock_stack_base_offset()));\n+    jcc(Assembler::below, check_done);\n+    cmpptr(obj, Address(thread, top));\n+    jccb(Assembler::notEqual, inflated_check_lock_stack);\n+    stop(\"Fast Unlock lock on stack\");\n+    bind(check_done);\n+    testptr(mark, markWord::monitor_value);\n+    jccb(Assembler::notZero, inflated);\n+    stop(\"Fast Unlock not monitor\");\n+#endif\n+\n+    bind(inflated);\n+\n+    \/\/ mark contains the tagged ObjectMonitor*.\n+    const Register monitor = mark;\n+\n+#ifndef _LP64\n+    \/\/ Check if recursive.\n+    xorptr(reg_rax, reg_rax);\n+    orptr(reg_rax, Address(monitor, OM_OFFSET_NO_MONITOR_VALUE_TAG(recursions)));\n+    jcc(Assembler::notZero, check_successor);\n+\n+    \/\/ Check if the entry lists are empty.\n+    movptr(reg_rax, Address(monitor, OM_OFFSET_NO_MONITOR_VALUE_TAG(EntryList)));\n+    orptr(reg_rax, Address(monitor, OM_OFFSET_NO_MONITOR_VALUE_TAG(cxq)));\n+    jcc(Assembler::notZero, check_successor);\n+\n+    \/\/ Release lock.\n+    movptr(Address(monitor, OM_OFFSET_NO_MONITOR_VALUE_TAG(owner)), NULL_WORD);\n+#else \/\/ _LP64\n+    Label recursive;\n+\n+    \/\/ Check if recursive.\n+    cmpptr(Address(monitor, OM_OFFSET_NO_MONITOR_VALUE_TAG(recursions)), 0);\n+    jccb(Assembler::notEqual, recursive);\n+\n+    \/\/ Check if the entry lists are empty.\n+    movptr(reg_rax, Address(monitor, OM_OFFSET_NO_MONITOR_VALUE_TAG(cxq)));\n+    orptr(reg_rax, Address(monitor, OM_OFFSET_NO_MONITOR_VALUE_TAG(EntryList)));\n+    jcc(Assembler::notZero, check_successor);\n+\n+    \/\/ Release lock.\n+    movptr(Address(monitor, OM_OFFSET_NO_MONITOR_VALUE_TAG(owner)), NULL_WORD);\n+    jmpb(unlocked);\n+\n+    \/\/ Recursive unlock.\n+    bind(recursive);\n+    decrement(Address(monitor, OM_OFFSET_NO_MONITOR_VALUE_TAG(recursions)));\n+    xorl(t, t);\n+#endif\n+  }\n+\n+  bind(unlocked);\n+  if (stub != nullptr) {\n+    bind(stub->unlocked_continuation());\n+  }\n+\n+#ifdef ASSERT\n+  \/\/ Check that unlocked label is reached with ZF set.\n+  Label zf_correct;\n+  jccb(Assembler::zero, zf_correct);\n+  stop(\"Fast Unlock ZF != 1\");\n+#endif\n+\n+  if (stub != nullptr) {\n+    bind(stub->slow_path_continuation());\n+  }\n+#ifdef ASSERT\n+  \/\/ Check that stub->continuation() label is reached with ZF not set.\n+  jccb(Assembler::notZero, zf_correct);\n+  stop(\"Fast Unlock ZF != 0\");\n+  bind(zf_correct);\n+#endif\n+  \/\/ C2 uses the value of ZF to determine the continuation.\n+}\n+\n","filename":"src\/hotspot\/cpu\/x86\/c2_MacroAssembler_x86.cpp","additions":254,"deletions":32,"binary":false,"changes":286,"status":"modified"},{"patch":"@@ -47,0 +47,4 @@\n+  void fast_lock_lightweight(Register obj, Register box, Register rax_reg,\n+                             Register t, Register thread);\n+  void fast_unlock_lightweight(Register obj, Register reg_rax, Register t, Register thread);\n+\n","filename":"src\/hotspot\/cpu\/x86\/c2_MacroAssembler_x86.hpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -1344,2 +1344,0 @@\n-      \/\/ Load object header, prepare for CAS from unlocked to locked.\n-      movptr(swap_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n@@ -1467,1 +1465,1 @@\n-      const Register thread = r15_thread;\n+      lightweight_unlock(obj_reg, swap_reg, r15_thread, header_reg, slow_case);\n@@ -1469,2 +1467,4 @@\n-      const Register thread = header_reg;\n-      get_thread(thread);\n+      \/\/ This relies on the implementation of lightweight_unlock being able to handle\n+      \/\/ that the reg_rax and thread Register parameters may alias each other.\n+      get_thread(swap_reg);\n+      lightweight_unlock(obj_reg, swap_reg, swap_reg, header_reg, slow_case);\n@@ -1472,9 +1472,0 @@\n-      \/\/ Handle unstructured locking.\n-      Register tmp = swap_reg;\n-      movl(tmp, Address(thread, JavaThread::lock_stack_top_offset()));\n-      cmpptr(obj_reg, Address(thread, tmp, Address::times_1,  -oopSize));\n-      jcc(Assembler::notEqual, slow_case);\n-      \/\/ Try to swing header from locked to unlocked.\n-      movptr(swap_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n-      andptr(swap_reg, ~(int32_t)markWord::lock_mask_in_place);\n-      lightweight_unlock(obj_reg, swap_reg, header_reg, slow_case);\n","filename":"src\/hotspot\/cpu\/x86\/interp_masm_x86.cpp","additions":6,"deletions":15,"binary":false,"changes":21,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -28,0 +28,1 @@\n+#include \"code\/compiledIC.hpp\"\n@@ -1351,1 +1352,1 @@\n-  mov64(rax, (intptr_t)Universe::non_oop_word());\n+  mov64(rax, (int64_t)Universe::non_oop_word());\n@@ -1358,0 +1359,32 @@\n+int MacroAssembler::ic_check_size() {\n+  return LP64_ONLY(14) NOT_LP64(12);\n+}\n+\n+int MacroAssembler::ic_check(int end_alignment) {\n+  Register receiver = LP64_ONLY(j_rarg0) NOT_LP64(rcx);\n+  Register data = rax;\n+  Register temp = LP64_ONLY(rscratch1) NOT_LP64(rbx);\n+\n+  \/\/ The UEP of a code blob ensures that the VEP is padded. However, the padding of the UEP is placed\n+  \/\/ before the inline cache check, so we don't have to execute any nop instructions when dispatching\n+  \/\/ through the UEP, yet we can ensure that the VEP is aligned appropriately. That's why we align\n+  \/\/ before the inline cache check here, and not after\n+  align(end_alignment, offset() + ic_check_size());\n+\n+  int uep_offset = offset();\n+\n+  if (UseCompressedClassPointers) {\n+    movl(temp, Address(receiver, oopDesc::klass_offset_in_bytes()));\n+    cmpl(temp, Address(data, CompiledICData::speculated_klass_offset()));\n+  } else {\n+    movptr(temp, Address(receiver, oopDesc::klass_offset_in_bytes()));\n+    cmpptr(temp, Address(data, CompiledICData::speculated_klass_offset()));\n+  }\n+\n+  \/\/ if inline cache check fails, then jump to runtime routine\n+  jump_cc(Assembler::notEqual, RuntimeAddress(SharedRuntime::get_ic_miss_stub()));\n+  assert((offset() % end_alignment) == 0, \"Misaligned verified entry point\");\n+\n+  return uep_offset;\n+}\n+\n@@ -4340,2 +4373,3 @@\n-int register_section_sizes(RegSet gp_registers, XMMRegSet xmm_registers, bool save_fpu,\n-                           int& gp_area_size, int& fp_area_size, int& xmm_area_size) {\n+static int register_section_sizes(RegSet gp_registers, XMMRegSet xmm_registers,\n+                                  bool save_fpu, int& gp_area_size,\n+                                  int& fp_area_size, int& xmm_area_size) {\n@@ -4657,1 +4691,1 @@\n-\/\/ - check recv_klass (actual object class) is a subtype of resolved_klass from CompiledICHolder\n+\/\/ - check recv_klass (actual object class) is a subtype of resolved_klass from CompiledICData\n@@ -10639,2 +10673,0 @@\n-\/\/ Branches to slow upon failure to lock the object, with ZF cleared.\n-\/\/ Falls through upon success with unspecified ZF.\n@@ -10643,1 +10675,1 @@\n-\/\/ hdr: the (pre-loaded) header of the object, must be rax\n+\/\/ reg_rax: rax\n@@ -10646,17 +10678,30 @@\n-void MacroAssembler::lightweight_lock(Register obj, Register hdr, Register thread, Register tmp, Label& slow) {\n-  assert(hdr == rax, \"header must be in rax for cmpxchg\");\n-  assert_different_registers(obj, hdr, thread, tmp);\n-\n-  \/\/ First we need to check if the lock-stack has room for pushing the object reference.\n-  \/\/ Note: we subtract 1 from the end-offset so that we can do a 'greater' comparison, instead\n-  \/\/ of 'greaterEqual' below, which readily clears the ZF. This makes C2 code a little simpler and\n-  \/\/ avoids one branch.\n-  cmpl(Address(thread, JavaThread::lock_stack_top_offset()), LockStack::end_offset() - 1);\n-  jcc(Assembler::greater, slow);\n-\n-  \/\/ Now we attempt to take the fast-lock.\n-  \/\/ Clear lock_mask bits (locked state).\n-  andptr(hdr, ~(int32_t)markWord::lock_mask_in_place);\n-  movptr(tmp, hdr);\n-  \/\/ Set unlocked_value bit.\n-  orptr(hdr, markWord::unlocked_value);\n+void MacroAssembler::lightweight_lock(Register obj, Register reg_rax, Register thread, Register tmp, Label& slow) {\n+  assert(reg_rax == rax, \"\");\n+  assert_different_registers(obj, reg_rax, thread, tmp);\n+\n+  Label push;\n+  const Register top = tmp;\n+\n+  \/\/ Preload the markWord. It is important that this is the first\n+  \/\/ instruction emitted as it is part of C1's null check semantics.\n+  movptr(reg_rax, Address(obj, oopDesc::mark_offset_in_bytes()));\n+\n+  \/\/ Load top.\n+  movl(top, Address(thread, JavaThread::lock_stack_top_offset()));\n+\n+  \/\/ Check if the lock-stack is full.\n+  cmpl(top, LockStack::end_offset());\n+  jcc(Assembler::greaterEqual, slow);\n+\n+  \/\/ Check for recursion.\n+  cmpptr(obj, Address(thread, top, Address::times_1, -oopSize));\n+  jcc(Assembler::equal, push);\n+\n+  \/\/ Check header for monitor (0b10).\n+  testptr(reg_rax, markWord::monitor_value);\n+  jcc(Assembler::notZero, slow);\n+\n+  \/\/ Try to lock. Transition lock bits 0b01 => 0b00\n+  movptr(tmp, reg_rax);\n+  andptr(tmp, ~(int32_t)markWord::unlocked_value);\n+  orptr(reg_rax, markWord::unlocked_value);\n@@ -10665,1 +10710,1 @@\n-    andptr(hdr, ~((int) markWord::inline_type_bit_in_place));\n+    andptr(reg_rax, ~((int) markWord::inline_type_bit_in_place));\n@@ -10667,2 +10712,1 @@\n-  lock();\n-  cmpxchgptr(tmp, Address(obj, oopDesc::mark_offset_in_bytes()));\n+  lock(); cmpxchgptr(tmp, Address(obj, oopDesc::mark_offset_in_bytes()));\n@@ -10671,5 +10715,8 @@\n-  \/\/ If successful, push object to lock-stack.\n-  movl(tmp, Address(thread, JavaThread::lock_stack_top_offset()));\n-  movptr(Address(thread, tmp), obj);\n-  incrementl(tmp, oopSize);\n-  movl(Address(thread, JavaThread::lock_stack_top_offset()), tmp);\n+  \/\/ Restore top, CAS clobbers register.\n+  movl(top, Address(thread, JavaThread::lock_stack_top_offset()));\n+\n+  bind(push);\n+  \/\/ After successful lock, push object on lock-stack.\n+  movptr(Address(thread, top), obj);\n+  incrementl(top, oopSize);\n+  movl(Address(thread, JavaThread::lock_stack_top_offset()), top);\n@@ -10679,2 +10726,0 @@\n-\/\/ Branches to slow upon failure, with ZF cleared.\n-\/\/ Falls through upon success, with unspecified ZF.\n@@ -10683,1 +10728,2 @@\n-\/\/ hdr: the (pre-loaded) header of the object, must be rax\n+\/\/ reg_rax: rax\n+\/\/ thread: the thread\n@@ -10685,9 +10731,14 @@\n-void MacroAssembler::lightweight_unlock(Register obj, Register hdr, Register tmp, Label& slow) {\n-  assert(hdr == rax, \"header must be in rax for cmpxchg\");\n-  assert_different_registers(obj, hdr, tmp);\n-\n-  \/\/ Mark-word must be lock_mask now, try to swing it back to unlocked_value.\n-  movptr(tmp, hdr); \/\/ The expected old value\n-  orptr(tmp, markWord::unlocked_value);\n-  lock();\n-  cmpxchgptr(tmp, Address(obj, oopDesc::mark_offset_in_bytes()));\n+\/\/\n+\/\/ x86_32 Note: reg_rax and thread may alias each other due to limited register\n+\/\/              availiability.\n+void MacroAssembler::lightweight_unlock(Register obj, Register reg_rax, Register thread, Register tmp, Label& slow) {\n+  assert(reg_rax == rax, \"\");\n+  assert_different_registers(obj, reg_rax, tmp);\n+  LP64_ONLY(assert_different_registers(obj, reg_rax, thread, tmp);)\n+\n+  Label unlocked, push_and_slow;\n+  const Register top = tmp;\n+\n+  \/\/ Check if obj is top of lock-stack.\n+  movl(top, Address(thread, JavaThread::lock_stack_top_offset()));\n+  cmpptr(obj, Address(thread, top, Address::times_1, -oopSize));\n@@ -10695,7 +10746,3 @@\n-  \/\/ Pop the lock object from the lock-stack.\n-#ifdef _LP64\n-  const Register thread = r15_thread;\n-#else\n-  const Register thread = rax;\n-  get_thread(thread);\n-#endif\n+\n+  \/\/ Pop lock-stack.\n+  DEBUG_ONLY(movptr(Address(thread, top, Address::times_1, -oopSize), 0);)\n@@ -10703,0 +10750,31 @@\n+\n+  \/\/ Check if recursive.\n+  cmpptr(obj, Address(thread, top, Address::times_1, -2 * oopSize));\n+  jcc(Assembler::equal, unlocked);\n+\n+  \/\/ Not recursive. Check header for monitor (0b10).\n+  movptr(reg_rax, Address(obj, oopDesc::mark_offset_in_bytes()));\n+  testptr(reg_rax, markWord::monitor_value);\n+  jcc(Assembler::notZero, push_and_slow);\n+\n+#ifdef ASSERT\n+  \/\/ Check header not unlocked (0b01).\n+  Label not_unlocked;\n+  testptr(reg_rax, markWord::unlocked_value);\n+  jcc(Assembler::zero, not_unlocked);\n+  stop(\"lightweight_unlock already unlocked\");\n+  bind(not_unlocked);\n+#endif\n+\n+  \/\/ Try to unlock. Transition lock bits 0b00 => 0b01\n+  movptr(tmp, reg_rax);\n+  orptr(tmp, markWord::unlocked_value);\n+  lock(); cmpxchgptr(tmp, Address(obj, oopDesc::mark_offset_in_bytes()));\n+  jcc(Assembler::equal, unlocked);\n+\n+  bind(push_and_slow);\n+  \/\/ Restore lock-stack and handle the unlock in runtime.\n+  if (thread == reg_rax) {\n+    \/\/ On x86_32 we may lose the thread.\n+    get_thread(thread);\n+  }\n@@ -10704,2 +10782,2 @@\n-  movl(tmp, Address(thread, JavaThread::lock_stack_top_offset()));\n-  movptr(Address(thread, tmp), 0);\n+  movl(top, Address(thread, JavaThread::lock_stack_top_offset()));\n+  movptr(Address(thread, top), obj);\n@@ -10707,0 +10785,4 @@\n+  addl(Address(thread, JavaThread::lock_stack_top_offset()), oopSize);\n+  jmp(slow);\n+\n+  bind(unlocked);\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.cpp","additions":136,"deletions":54,"binary":false,"changes":190,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -957,0 +957,2 @@\n+  static int ic_check_size();\n+  int ic_check(int end_alignment);\n@@ -2107,2 +2109,2 @@\n-  void lightweight_lock(Register obj, Register hdr, Register thread, Register tmp, Label& slow);\n-  void lightweight_unlock(Register obj, Register hdr, Register tmp, Label& slow);\n+  void lightweight_lock(Register obj, Register reg_rax, Register thread, Register tmp, Label& slow);\n+  void lightweight_unlock(Register obj, Register reg_rax, Register thread, Register tmp, Label& slow);\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.hpp","additions":5,"deletions":3,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2003, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2003, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -28,0 +28,1 @@\n+#include \"code\/compiledIC.hpp\"\n@@ -29,1 +30,0 @@\n-#include \"code\/icBuffer.hpp\"\n@@ -39,1 +39,0 @@\n-#include \"oops\/compiledICHolder.hpp\"\n@@ -958,1 +957,1 @@\n-  Register holder = rax;\n+  Register data = rax;\n@@ -963,6 +962,2 @@\n-\n-    Label missed;\n-    __ movptr(temp, Address(receiver, oopDesc::klass_offset_in_bytes()));\n-    __ cmpptr(temp, Address(holder, CompiledICHolder::holder_klass_offset()));\n-    __ movptr(rbx, Address(holder, CompiledICHolder::holder_metadata_offset()));\n-    __ jcc(Assembler::notEqual, missed);\n+    __ ic_check(1 \/* end_alignment *\/);\n+    __ movptr(rbx, Address(data, CompiledICData::speculated_method_offset()));\n@@ -974,3 +969,0 @@\n-\n-    __ bind(missed);\n-    __ jump(RuntimeAddress(SharedRuntime::get_ic_miss_stub()));\n@@ -1467,2 +1459,0 @@\n-  const Register ic_reg = rax;\n-  Label hit;\n@@ -1473,10 +1463,1 @@\n-  __ cmpptr(ic_reg, Address(receiver, oopDesc::klass_offset_in_bytes()));\n-  __ jcc(Assembler::equal, hit);\n-\n-  __ jump(RuntimeAddress(SharedRuntime::get_ic_miss_stub()));\n-\n-  \/\/ and the first 5 bytes must be in the same cache line\n-  \/\/ if we align at 8 then we will be sure 5 bytes are in the same line\n-  __ align(8);\n-\n-  __ bind(hit);\n+  __ ic_check(8 \/* end_alignment *\/);\n@@ -1731,2 +1712,0 @@\n-      \/\/ Load object header\n-      __ movptr(swap_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n@@ -1890,3 +1869,1 @@\n-      __ movptr(swap_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n-      __ andptr(swap_reg, ~(int32_t)markWord::lock_mask_in_place);\n-      __ lightweight_unlock(obj_reg, swap_reg, lock_reg, slow_path_unlock);\n+      __ lightweight_unlock(obj_reg, swap_reg, thread, lock_reg, slow_path_unlock);\n","filename":"src\/hotspot\/cpu\/x86\/sharedRuntime_x86_32.cpp","additions":7,"deletions":30,"binary":false,"changes":37,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2003, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2003, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -34,1 +34,0 @@\n-#include \"code\/icBuffer.hpp\"\n@@ -46,1 +45,0 @@\n-#include \"oops\/compiledICHolder.hpp\"\n@@ -1239,11 +1237,3 @@\n-  Label ok;\n-\n-  Register holder = rax;\n-  Register receiver = j_rarg0;\n-  Register temp = rbx;\n-\n-  __ load_klass(temp, receiver, rscratch1);\n-  __ cmpptr(temp, Address(holder, CompiledICHolder::holder_klass_offset()));\n-  __ movptr(rbx, Address(holder, CompiledICHolder::holder_metadata_offset()));\n-  __ jcc(Assembler::equal, ok);\n-  __ jump(RuntimeAddress(SharedRuntime::get_ic_miss_stub()));\n+  Register data = rax;\n+  __ ic_check(1 \/* end_alignment *\/);\n+  __ movptr(rbx, Address(data, CompiledICData::speculated_method_offset()));\n@@ -1251,1 +1241,0 @@\n-  __ bind(ok);\n@@ -1321,1 +1310,0 @@\n-\n@@ -1727,1 +1715,1 @@\n-    address stub = CompiledStaticCall::emit_to_interp_stub(*cbuf, __ pc());\n+    address stub = CompiledDirectCall::emit_to_interp_stub(*cbuf, __ pc());\n@@ -1764,1 +1752,1 @@\n-  address stub = CompiledStaticCall::emit_to_interp_stub(*cbuf, __ pc());\n+  address stub = CompiledDirectCall::emit_to_interp_stub(*cbuf, __ pc());\n@@ -2021,0 +2009,1 @@\n+    if (nm == nullptr) return nm;\n@@ -2159,2 +2148,0 @@\n-\n-  const Register ic_reg = rax;\n@@ -2163,1 +2150,0 @@\n-  Label hit;\n@@ -2166,1 +2152,1 @@\n-  assert_different_registers(ic_reg, receiver, rscratch1, rscratch2);\n+  assert_different_registers(receiver, rscratch1, rscratch2);\n@@ -2168,10 +2154,1 @@\n-  __ load_klass(rscratch1, receiver, rscratch2);\n-  __ cmpq(ic_reg, rscratch1);\n-  __ jcc(Assembler::equal, hit);\n-\n-  __ jump(RuntimeAddress(SharedRuntime::get_ic_miss_stub()));\n-\n-  \/\/ Verified entry point must be aligned\n-  __ align(8);\n-\n-  __ bind(hit);\n+  __ ic_check(8 \/* end_alignment *\/);\n@@ -2470,2 +2447,0 @@\n-      \/\/ Load object header\n-      __ movptr(swap_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n@@ -2614,3 +2589,1 @@\n-      __ movptr(swap_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n-      __ andptr(swap_reg, ~(int32_t)markWord::lock_mask_in_place);\n-      __ lightweight_unlock(obj_reg, swap_reg, lock_reg, slow_path_unlock);\n+      __ lightweight_unlock(obj_reg, swap_reg, r15_thread, lock_reg, slow_path_unlock);\n","filename":"src\/hotspot\/cpu\/x86\/sharedRuntime_x86_64.cpp","additions":10,"deletions":37,"binary":false,"changes":47,"status":"modified"},{"patch":"@@ -27,0 +27,1 @@\n+#include \"code\/compiledIC.hpp\"\n@@ -30,1 +31,0 @@\n-#include \"oops\/compiledICHolder.hpp\"\n@@ -173,1 +173,1 @@\n-  \/\/  rax: CompiledICHolder\n+  \/\/  rax: CompiledICData\n@@ -179,1 +179,1 @@\n-  const Register holder_klass_reg   = rax; \/\/ declaring interface klass (DECC)\n+  const Register holder_klass_reg   = rax; \/\/ declaring interface klass (DEFC)\n@@ -184,1 +184,1 @@\n-  const Register icholder_reg       = rax;\n+  const Register icdata_reg         = rax;\n@@ -186,2 +186,2 @@\n-  __ movptr(resolved_klass_reg, Address(icholder_reg, CompiledICHolder::holder_klass_offset()));\n-  __ movptr(holder_klass_reg,   Address(icholder_reg, CompiledICHolder::holder_metadata_offset()));\n+  __ movptr(resolved_klass_reg, Address(icdata_reg, CompiledICData::itable_refc_klass_offset()));\n+  __ movptr(holder_klass_reg,   Address(icdata_reg, CompiledICData::itable_defc_klass_offset()));\n","filename":"src\/hotspot\/cpu\/x86\/vtableStubs_x86_64.cpp","additions":6,"deletions":6,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -1361,1 +1361,1 @@\n-Assembler::Width widthForType(BasicType bt) {\n+static Assembler::Width widthForType(BasicType bt) {\n","filename":"src\/hotspot\/cpu\/x86\/x86.ad","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -507,1 +507,1 @@\n-void emit_cmpfp3(MacroAssembler& _masm, Register dst) {\n+static void emit_cmpfp3(MacroAssembler& _masm, Register dst) {\n@@ -1383,14 +1383,1 @@\n-#ifdef ASSERT\n-  uint insts_size = cbuf.insts_size();\n-#endif\n-  masm.cmpptr(rax, Address(rcx, oopDesc::klass_offset_in_bytes()));\n-  masm.jump_cc(Assembler::notEqual,\n-               RuntimeAddress(SharedRuntime::get_ic_miss_stub()));\n-  \/* WARNING these NOPs are critical so that verified entry point is properly\n-     aligned for patching by NativeJump::patch_verified_entry() *\/\n-  int nops_cnt = 2;\n-  if( !OptoBreakpoint ) \/\/ Leave space for int3\n-     nops_cnt += 1;\n-  masm.nop(nops_cnt);\n-\n-  assert(cbuf.insts_size() - insts_size == size(ra_), \"checking code size of inline cache node\");\n+  masm.ic_check(CodeEntryAlignment);\n@@ -1400,1 +1387,2 @@\n-  return OptoBreakpoint ? 11 : 12;\n+  return MachNode::size(ra_); \/\/ too many variables; just compute it\n+                              \/\/ the hard way\n@@ -1842,1 +1830,1 @@\n-        address stub = CompiledStaticCall::emit_to_interp_stub(cbuf, mark);\n+        address stub = CompiledDirectCall::emit_to_interp_stub(cbuf, mark);\n@@ -13776,1 +13764,1 @@\n-  predicate(!Compile::current()->use_rtm());\n+  predicate(LockingMode != LM_LIGHTWEIGHT && !Compile::current()->use_rtm());\n@@ -13790,0 +13778,1 @@\n+  predicate(LockingMode != LM_LIGHTWEIGHT);\n@@ -13800,0 +13789,26 @@\n+instruct cmpFastLockLightweight(eFlagsReg cr, eRegP object, eBXRegP box, eAXRegI eax_reg, eRegP tmp, eRegP thread) %{\n+  predicate(LockingMode == LM_LIGHTWEIGHT);\n+  match(Set cr (FastLock object box));\n+  effect(TEMP eax_reg, TEMP tmp, USE_KILL box, TEMP thread);\n+  ins_cost(300);\n+  format %{ \"FASTLOCK $object,$box\\t! kills $box,$eax_reg,$tmp\" %}\n+  ins_encode %{\n+    __ get_thread($thread$$Register);\n+    __ fast_lock_lightweight($object$$Register, $box$$Register, $eax_reg$$Register, $tmp$$Register, $thread$$Register);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct cmpFastUnlockLightweight(eFlagsReg cr, eRegP object, eAXRegP eax_reg, eRegP tmp, eRegP thread) %{\n+  predicate(LockingMode == LM_LIGHTWEIGHT);\n+  match(Set cr (FastUnlock object eax_reg));\n+  effect(TEMP tmp, USE_KILL eax_reg, TEMP thread);\n+  ins_cost(300);\n+  format %{ \"FASTUNLOCK $object,$eax_reg\\t! kills $eax_reg,$tmp\" %}\n+  ins_encode %{\n+    __ get_thread($thread$$Register);\n+    __ fast_unlock_lightweight($object$$Register, $eax_reg$$Register, $tmp$$Register, $thread$$Register);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n","filename":"src\/hotspot\/cpu\/x86\/x86_32.ad","additions":33,"deletions":18,"binary":false,"changes":51,"status":"modified"},{"patch":"@@ -527,1 +527,1 @@\n-void emit_cmpfp_fixup(MacroAssembler& _masm) {\n+static void emit_cmpfp_fixup(MacroAssembler& _masm) {\n@@ -547,1 +547,1 @@\n-void emit_cmpfp3(MacroAssembler& _masm, Register dst) {\n+static void emit_cmpfp3(MacroAssembler& _masm, Register dst) {\n@@ -566,4 +566,4 @@\n-void emit_fp_min_max(MacroAssembler& _masm, XMMRegister dst,\n-                     XMMRegister a, XMMRegister b,\n-                     XMMRegister xmmt, Register rt,\n-                     bool min, bool single) {\n+static void emit_fp_min_max(MacroAssembler& _masm, XMMRegister dst,\n+                            XMMRegister a, XMMRegister b,\n+                            XMMRegister xmmt, Register rt,\n+                            bool min, bool single) {\n@@ -1497,2 +1497,1 @@\n-    st->print_cr(\"\\tdecode_klass_not_null rscratch1, rscratch1\");\n-    st->print_cr(\"\\tcmpq    rax, rscratch1\\t # Inline cache check\");\n+    st->print_cr(\"\\tcmpl    rscratch1, [rax + CompiledICData::speculated_klass_offset()]\\t # Inline cache check\");\n@@ -1500,2 +1499,2 @@\n-    st->print_cr(\"\\tcmpq    rax, [j_rarg0 + oopDesc::klass_offset_in_bytes()]\\t\"\n-                 \"# Inline cache check\");\n+    st->print_cr(\"movq    rscratch1, [j_rarg0 + oopDesc::klass_offset_in_bytes()]\\t# compressed klass\");\n+    st->print_cr(\"\\tcmpq    rscratch1, [rax + CompiledICData::speculated_klass_offset()]\\t # Inline cache check\");\n@@ -1504,1 +1503,0 @@\n-  st->print_cr(\"\\tnop\\t# nops to align entry point\");\n@@ -1511,20 +1509,1 @@\n-  uint insts_size = cbuf.insts_size();\n-  if (UseCompressedClassPointers) {\n-    masm.load_klass(rscratch1, j_rarg0, rscratch2);\n-    masm.cmpptr(rax, rscratch1);\n-  } else {\n-    masm.cmpptr(rax, Address(j_rarg0, oopDesc::klass_offset_in_bytes()));\n-  }\n-\n-  masm.jump_cc(Assembler::notEqual, RuntimeAddress(SharedRuntime::get_ic_miss_stub()));\n-\n-  \/* WARNING these NOPs are critical so that verified entry point is properly\n-     4 bytes aligned for patching by NativeJump::patch_verified_entry() *\/\n-  int nops_cnt = 4 - ((cbuf.insts_size() - insts_size) & 0x3);\n-  if (OptoBreakpoint) {\n-    \/\/ Leave space for int3\n-    nops_cnt -= 1;\n-  }\n-  nops_cnt &= 0x3; \/\/ Do not add nops if code is aligned.\n-  if (nops_cnt > 0)\n-    masm.nop(nops_cnt);\n+  masm.ic_check(InteriorEntryAlignment);\n@@ -1858,1 +1837,1 @@\n-        address stub = CompiledStaticCall::emit_to_interp_stub(cbuf, mark);\n+        address stub = CompiledDirectCall::emit_to_interp_stub(cbuf, mark);\n@@ -12663,1 +12642,1 @@\n-  predicate(!Compile::current()->use_rtm());\n+  predicate(LockingMode != LM_LIGHTWEIGHT && !Compile::current()->use_rtm());\n@@ -12676,0 +12655,1 @@\n+  predicate(LockingMode != LM_LIGHTWEIGHT);\n@@ -12686,0 +12666,24 @@\n+instruct cmpFastLockLightweight(rFlagsReg cr, rRegP object, rbx_RegP box, rax_RegI rax_reg, rRegP tmp) %{\n+  predicate(LockingMode == LM_LIGHTWEIGHT);\n+  match(Set cr (FastLock object box));\n+  effect(TEMP rax_reg, TEMP tmp, USE_KILL box);\n+  ins_cost(300);\n+  format %{ \"fastlock $object,$box\\t! kills $box,$rax_reg,$tmp\" %}\n+  ins_encode %{\n+    __ fast_lock_lightweight($object$$Register, $box$$Register, $rax_reg$$Register, $tmp$$Register, r15_thread);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct cmpFastUnlockLightweight(rFlagsReg cr, rRegP object, rax_RegP rax_reg, rRegP tmp) %{\n+  predicate(LockingMode == LM_LIGHTWEIGHT);\n+  match(Set cr (FastUnlock object rax_reg));\n+  effect(TEMP tmp, USE_KILL rax_reg);\n+  ins_cost(300);\n+  format %{ \"fastunlock $object,$rax_reg\\t! kills $rax_reg,$tmp\" %}\n+  ins_encode %{\n+    __ fast_unlock_lightweight($object$$Register, $rax_reg$$Register, $tmp$$Register, r15_thread);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n","filename":"src\/hotspot\/cpu\/x86\/x86_64.ad","additions":37,"deletions":33,"binary":false,"changes":70,"status":"modified"},{"patch":"@@ -29,1 +29,0 @@\n-#include \"code\/icBuffer.hpp\"\n@@ -32,1 +31,0 @@\n-#include \"oops\/compiledICHolder.hpp\"\n","filename":"src\/hotspot\/cpu\/zero\/sharedRuntime_zero.cpp","additions":0,"deletions":2,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -220,1 +220,0 @@\n-  AD.addInclude(AD._CPP_file, \"oops\/compiledICHolder.hpp\");\n","filename":"src\/hotspot\/share\/adlc\/main.cpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -219,1 +219,1 @@\n-    assert(allocates2(pc),     \"relocation addr must be in this section\");\n+    assert(allocates2(pc),     \"relocation addr \" INTPTR_FORMAT \" must be in this section from \" INTPTR_FORMAT \" to \" INTPTR_FORMAT, p2i(pc), p2i(_start), p2i(_limit));\n","filename":"src\/hotspot\/share\/asm\/codeBuffer.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -528,1 +528,1 @@\n-int compare_depth_first(BlockBegin** a, BlockBegin** b) {\n+static int compare_depth_first(BlockBegin** a, BlockBegin** b) {\n","filename":"src\/hotspot\/share\/c1\/c1_GraphBuilder.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2000, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2000, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -979,2 +979,0 @@\n-          stringStream st;\n-          st.print(\"bad oop %s at %d\", r->as_Register()->name(), _masm->offset());\n","filename":"src\/hotspot\/share\/c1\/c1_LIRAssembler.cpp","additions":1,"deletions":3,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -1449,2 +1449,2 @@\n-#ifndef PRODUCT\n-int interval_cmp(Interval* const& l, Interval* const& r) {\n+#ifdef ASSERT\n+static int interval_cmp(Interval* const& l, Interval* const& r) {\n@@ -1454,1 +1454,1 @@\n-bool find_interval(Interval* interval, IntervalArray* intervals) {\n+static bool find_interval(Interval* interval, IntervalArray* intervals) {\n@@ -2306,1 +2306,1 @@\n-void assert_equal(Location l1, Location l2) {\n+static void assert_equal(Location l1, Location l2) {\n@@ -2310,1 +2310,1 @@\n-void assert_equal(ScopeValue* v1, ScopeValue* v2) {\n+static void assert_equal(ScopeValue* v1, ScopeValue* v2) {\n@@ -2331,1 +2331,1 @@\n-void assert_equal(MonitorValue* m1, MonitorValue* m2) {\n+static void assert_equal(MonitorValue* m1, MonitorValue* m2) {\n@@ -2336,1 +2336,1 @@\n-void assert_equal(IRScopeDebugInfo* d1, IRScopeDebugInfo* d2) {\n+static void assert_equal(IRScopeDebugInfo* d1, IRScopeDebugInfo* d2) {\n@@ -2378,1 +2378,1 @@\n-void check_stack_depth(CodeEmitInfo* info, int stack_end) {\n+static void check_stack_depth(CodeEmitInfo* info, int stack_end) {\n","filename":"src\/hotspot\/share\/c1\/c1_LinearScan.cpp","additions":8,"deletions":8,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -44,1 +44,0 @@\n-  void inline_cache_check(Register receiver, Register iCache);\n","filename":"src\/hotspot\/share\/c1\/c1_MacroAssembler.hpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1999, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1999, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -341,1 +341,1 @@\n-void disconnect_from_graph(BlockBegin* block) {\n+static void disconnect_from_graph(BlockBegin* block) {\n@@ -722,0 +722,2 @@\n+  void handle_Constant        (Constant* x);\n+  void handle_IfOp            (IfOp* x);\n@@ -736,1 +738,1 @@\n-void NullCheckVisitor::do_Constant       (Constant*        x) { \/* FIXME: handle object constants *\/ }\n+void NullCheckVisitor::do_Constant       (Constant*        x) { nce()->handle_Constant(x); }\n@@ -747,1 +749,1 @@\n-void NullCheckVisitor::do_IfOp           (IfOp*            x) {}\n+void NullCheckVisitor::do_IfOp           (IfOp*            x) { nce()->handle_IfOp(x); }\n@@ -891,1 +893,3 @@\n-    if (instr->is_pinned() || instr->can_trap() || (instr->as_NullCheck() != nullptr)) {\n+    if (instr->is_pinned() || instr->can_trap() || (instr->as_NullCheck() != nullptr)\n+        || (instr->as_Constant() != nullptr && instr->as_Constant()->type()->is_object())\n+        || (instr->as_IfOp() != nullptr)) {\n@@ -1212,0 +1216,22 @@\n+void NullCheckEliminator::handle_Constant(Constant *x) {\n+  ObjectType* ot = x->type()->as_ObjectType();\n+  if (ot != nullptr && ot->is_loaded()) {\n+    ObjectConstant* oc = ot->as_ObjectConstant();\n+    if (oc == nullptr || !oc->value()->is_null_object()) {\n+      set_put(x);\n+      if (PrintNullCheckElimination) {\n+        tty->print_cr(\"Constant %d is non-null\", x->id());\n+      }\n+    }\n+  }\n+}\n+\n+void NullCheckEliminator::handle_IfOp(IfOp *x) {\n+  if (x->type()->is_object() && set_contains(x->tval()) && set_contains(x->fval())) {\n+    set_put(x);\n+    if (PrintNullCheckElimination) {\n+      tty->print_cr(\"IfOp %d is non-null\", x->id());\n+    }\n+  }\n+}\n+\n","filename":"src\/hotspot\/share\/c1\/c1_Optimizer.cpp","additions":31,"deletions":5,"binary":false,"changes":36,"status":"modified"},{"patch":"@@ -1739,3 +1739,3 @@\n-char* map_memory(int fd, const char* file_name, size_t file_offset,\n-                 char *addr, size_t bytes, bool read_only,\n-                 bool allow_exec, MEMFLAGS flags = mtNone) {\n+static char* map_memory(int fd, const char* file_name, size_t file_offset,\n+                        char *addr, size_t bytes, bool read_only,\n+                        bool allow_exec, MEMFLAGS flags = mtNone) {\n","filename":"src\/hotspot\/share\/cds\/filemap.cpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -137,1 +137,2 @@\n-bool string_starts_with(const char* str, const char* str_to_find) {\n+#if INCLUDE_CDS\n+static bool string_starts_with(const char* str, const char* str_to_find) {\n@@ -145,0 +146,1 @@\n+#endif\n@@ -1012,2 +1014,2 @@\n-ClassPathEntry* find_first_module_cpe(ModuleEntry* mod_entry,\n-                                      const GrowableArray<ModuleClassPathList*>* const module_list) {\n+static ClassPathEntry* find_first_module_cpe(ModuleEntry* mod_entry,\n+                                             const GrowableArray<ModuleClassPathList*>* const module_list) {\n@@ -1373,1 +1375,1 @@\n-char* lookup_vm_resource(JImageFile *jimage, const char *jimage_version, const char *path) {\n+static char* lookup_vm_resource(JImageFile *jimage, const char *jimage_version, const char *path) {\n","filename":"src\/hotspot\/share\/classfile\/classLoader.cpp","additions":6,"deletions":4,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2003, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2003, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -203,1 +203,1 @@\n-PlaceholderEntry* add_entry(Symbol* class_name, ClassLoaderData* loader_data,\n+static PlaceholderEntry* add_entry(Symbol* class_name, ClassLoaderData* loader_data,\n@@ -218,1 +218,1 @@\n-void remove_entry(Symbol* class_name, ClassLoaderData* loader_data) {\n+static void remove_entry(Symbol* class_name, ClassLoaderData* loader_data) {\n","filename":"src\/hotspot\/share\/classfile\/placeholders.cpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -218,1 +218,1 @@\n-bool is_parallelCapable(Handle class_loader) {\n+static bool is_parallelCapable(Handle class_loader) {\n@@ -224,1 +224,1 @@\n-bool is_parallelDefine(Handle class_loader) {\n+static bool is_parallelDefine(Handle class_loader) {\n@@ -286,1 +286,1 @@\n-void verify_dictionary_entry(Symbol* class_name, InstanceKlass* k) {\n+static void verify_dictionary_entry(Symbol* class_name, InstanceKlass* k) {\n","filename":"src\/hotspot\/share\/classfile\/systemDictionary.cpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -28,1 +28,0 @@\n-#include \"code\/icBuffer.hpp\"\n@@ -681,5 +680,0 @@\n-    \/\/ the InlineCacheBuffer is using stubs generated into a buffer blob\n-    if (InlineCacheBuffer::contains(addr)) {\n-      st->print_cr(INTPTR_FORMAT \" is pointing into InlineCacheBuffer\", p2i(addr));\n-      return;\n-    }\n","filename":"src\/hotspot\/share\/code\/codeBlob.cpp","additions":0,"deletions":6,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -29,1 +29,0 @@\n-#include \"code\/icBuffer.hpp\"\n@@ -32,4 +31,0 @@\n-#include \"interpreter\/interpreter.hpp\"\n-#include \"interpreter\/linkResolver.hpp\"\n-#include \"memory\/metadataFactory.hpp\"\n-#include \"memory\/oopFactory.hpp\"\n@@ -38,0 +33,1 @@\n+#include \"oops\/compressedKlass.hpp\"\n@@ -40,2 +36,1 @@\n-#include \"oops\/oop.inline.hpp\"\n-#include \"oops\/symbol.hpp\"\n+#include \"runtime\/atomic.hpp\"\n@@ -44,2 +39,1 @@\n-#include \"runtime\/icache.hpp\"\n-#include \"runtime\/safepoint.hpp\"\n+#include \"runtime\/interfaceSupport.inline.hpp\"\n@@ -47,2 +41,0 @@\n-#include \"runtime\/stubRoutines.hpp\"\n-#include \"utilities\/events.hpp\"\n@@ -78,3 +70,6 @@\n-\/\/-----------------------------------------------------------------------------\n-\/\/ Low-level access to an inline cache. Private, since they might not be\n-\/\/ MT-safe to use.\n+CompiledICData::CompiledICData()\n+  : _speculated_method(),\n+    _speculated_klass(),\n+    _itable_defc_klass(),\n+    _itable_refc_klass(),\n+    _is_initialized() {}\n@@ -82,10 +77,5 @@\n-void* CompiledIC::cached_value() const {\n-  assert(CompiledICLocker::is_safe(_method), \"mt unsafe call\");\n-  assert (!is_optimized(), \"an optimized virtual call does not have a cached metadata\");\n-\n-  if (!is_in_transition_state()) {\n-    void* data = get_data();\n-    \/\/ If we let the metadata value here be initialized to zero...\n-    assert(data != nullptr || Universe::non_oop_word() == nullptr,\n-           \"no raw nulls in CompiledIC metadatas, because of patching races\");\n-    return (data == (void*)Universe::non_oop_word()) ? nullptr : data;\n+\/\/ Inline cache callsite info is initialized once the first time it is resolved\n+void CompiledICData::initialize(CallInfo* call_info, Klass* receiver_klass) {\n+  _speculated_method = call_info->selected_method();\n+  if (UseCompressedClassPointers) {\n+    _speculated_klass = (uintptr_t)CompressedKlassPointers::encode_not_null(receiver_klass);\n@@ -93,1 +83,1 @@\n-    return InlineCacheBuffer::cached_value_for((CompiledIC *)this);\n+    _speculated_klass = (uintptr_t)receiver_klass;\n@@ -95,0 +85,5 @@\n+  if (call_info->call_kind() == CallInfo::itable_call) {\n+    _itable_defc_klass = call_info->resolved_method()->method_holder();\n+    _itable_refc_klass = call_info->resolved_klass();\n+  }\n+  _is_initialized = true;\n@@ -97,0 +92,3 @@\n+bool CompiledICData::is_speculated_klass_unloaded() const {\n+  return is_initialized() && _speculated_klass == 0;\n+}\n@@ -98,18 +96,3 @@\n-void CompiledIC::internal_set_ic_destination(address entry_point, bool is_icstub, void* cache, bool is_icholder) {\n-  assert(entry_point != nullptr, \"must set legal entry point\");\n-  assert(CompiledICLocker::is_safe(_method), \"mt unsafe call\");\n-  assert (!is_optimized() || cache == nullptr, \"an optimized virtual call does not have a cached metadata\");\n-  assert (cache == nullptr || cache != (Metadata*)badOopVal, \"invalid metadata\");\n-\n-  assert(!is_icholder || is_icholder_entry(entry_point), \"must be\");\n-\n-  \/\/ Don't use ic_destination for this test since that forwards\n-  \/\/ through ICBuffer instead of returning the actual current state of\n-  \/\/ the CompiledIC.\n-  if (is_icholder_entry(_call->destination())) {\n-    \/\/ When patching for the ICStub case the cached value isn't\n-    \/\/ overwritten until the ICStub copied into the CompiledIC during\n-    \/\/ the next safepoint.  Make sure that the CompiledICHolder* is\n-    \/\/ marked for release at this point since it won't be identifiable\n-    \/\/ once the entry point is overwritten.\n-    InlineCacheBuffer::queue_for_release((CompiledICHolder*)get_data());\n+void CompiledICData::clean_metadata() {\n+  if (!is_initialized() || is_speculated_klass_unloaded()) {\n+    return;\n@@ -118,11 +101,8 @@\n-  if (TraceCompiledIC) {\n-    tty->print(\"  \");\n-    print_compiled_ic();\n-    tty->print(\" changing destination to \" INTPTR_FORMAT, p2i(entry_point));\n-    if (!is_optimized()) {\n-      tty->print(\" changing cached %s to \" INTPTR_FORMAT, is_icholder ? \"icholder\" : \"metadata\", p2i((address)cache));\n-    }\n-    if (is_icstub) {\n-      tty->print(\" (icstub)\");\n-    }\n-    tty->cr();\n+  \/\/ GC cleaning doesn't need to change the state of the inline cache,\n+  \/\/ only nuke stale speculated metadata if it gets unloaded. If the\n+  \/\/ inline cache is monomorphic, the unverified entries will miss, and\n+  \/\/ subsequent miss handlers will upgrade the callsite to megamorphic,\n+  \/\/ which makes sense as it obviously is megamorphic then.\n+  if (!speculated_klass()->is_loader_alive()) {\n+    Atomic::store(&_speculated_klass, (uintptr_t)0);\n+    Atomic::store(&_speculated_method, (Method*)nullptr);\n@@ -131,7 +111,3 @@\n-#ifdef ASSERT\n-  {\n-    CodeBlob* cb = CodeCache::find_blob(_call->instruction_address());\n-    assert(cb != nullptr && cb->is_compiled(), \"must be compiled\");\n-  }\n-#endif\n-  _call->set_destination_mt_safe(entry_point);\n+  assert(_speculated_method == nullptr || _speculated_method->method_holder()->is_loader_alive(),\n+         \"Speculated method is not unloaded despite class being unloaded\");\n+}\n@@ -139,5 +115,2 @@\n-  if (is_optimized() || is_icstub) {\n-    \/\/ Optimized call sites don't have a cache value and ICStub call\n-    \/\/ sites only change the entry point.  Changing the value in that\n-    \/\/ case could lead to MT safety issues.\n-    assert(cache == nullptr, \"must be null\");\n+void CompiledICData::metadata_do(MetadataClosure* cl) {\n+  if (!is_initialized()) {\n@@ -147,8 +120,10 @@\n-  if (cache == nullptr)  cache = Universe::non_oop_word();\n-\n-  set_data((intptr_t)cache);\n-}\n-\n-\n-void CompiledIC::set_ic_destination(ICStub* stub) {\n-  internal_set_ic_destination(stub->code_begin(), true, nullptr, false);\n+  if (!is_speculated_klass_unloaded()) {\n+    cl->do_metadata(_speculated_method);\n+    cl->do_metadata(speculated_klass());\n+  }\n+  if (_itable_refc_klass != nullptr) {\n+    cl->do_metadata(_itable_refc_klass);\n+  }\n+  if (_itable_defc_klass != nullptr) {\n+    cl->do_metadata(_itable_defc_klass);\n+  }\n@@ -157,0 +132,4 @@\n+Klass* CompiledICData::speculated_klass() const {\n+  if (is_speculated_klass_unloaded()) {\n+    return nullptr;\n+  }\n@@ -158,5 +137,2 @@\n-\n-address CompiledIC::ic_destination() const {\n-  assert(CompiledICLocker::is_safe(_method), \"mt unsafe call\");\n-  if (!is_in_transition_state()) {\n-    return _call->destination();\n+  if (UseCompressedClassPointers) {\n+    return CompressedKlassPointers::decode_not_null((narrowKlass)_speculated_klass);\n@@ -164,1 +140,1 @@\n-    return InlineCacheBuffer::ic_destination_for((CompiledIC *)this);\n+    return (Klass*)_speculated_klass;\n@@ -168,0 +144,2 @@\n+\/\/-----------------------------------------------------------------------------\n+\/\/ High-level access to an inline cache. Guaranteed to be MT-safe.\n@@ -169,3 +147,2 @@\n-bool CompiledIC::is_in_transition_state() const {\n-  assert(CompiledICLocker::is_safe(_method), \"mt unsafe call\");\n-  return InlineCacheBuffer::contains(_call->destination());;\n+CompiledICData* CompiledIC::data() const {\n+  return _data;\n@@ -174,0 +151,8 @@\n+CompiledICData* data_from_reloc_iter(RelocIterator* iter) {\n+  assert(iter->type() == relocInfo::virtual_call_type, \"wrong reloc. info\");\n+\n+  virtual_call_Relocation* r = iter->virtual_call_reloc();\n+  NativeMovConstReg* value = nativeMovConstReg_at(r->cached_value());\n+\n+  return (CompiledICData*)value->data();\n+}\n@@ -175,1 +160,7 @@\n-bool CompiledIC::is_icholder_call() const {\n+CompiledIC::CompiledIC(RelocIterator* iter)\n+  : _method(iter->code()),\n+    _data(data_from_reloc_iter(iter)),\n+    _call(nativeCall_at(iter->addr()))\n+{\n+  assert(_method != nullptr, \"must pass compiled method\");\n+  assert(_method->contains(iter->addr()), \"must be in compiled method\");\n@@ -177,1 +168,0 @@\n-  return !_is_optimized && is_icholder_entry(ic_destination());\n@@ -180,5 +170,3 @@\n-\/\/ Returns native address of 'call' instruction in inline-cache. Used by\n-\/\/ the InlineCacheBuffer when it needs to find the stub.\n-address CompiledIC::stub_address() const {\n-  assert(is_in_transition_state(), \"should only be called when we are in a transition state\");\n-  return _call->destination();\n+CompiledIC* CompiledIC_before(CompiledMethod* nm, address return_addr) {\n+  address call_site = nativeCall_before(return_addr)->instruction_address();\n+  return CompiledIC_at(nm, call_site);\n@@ -187,6 +175,4 @@\n-\/\/ Clears the IC stub if the compiled IC is in transition state\n-void CompiledIC::clear_ic_stub() {\n-  if (is_in_transition_state()) {\n-    ICStub* stub = ICStub::from_destination_address(stub_address());\n-    stub->clear();\n-  }\n+CompiledIC* CompiledIC_at(CompiledMethod* nm, address call_site) {\n+  RelocIterator iter(nm, call_site, call_site + 1);\n+  iter.next();\n+  return CompiledIC_at(&iter);\n@@ -195,2 +181,5 @@\n-\/\/-----------------------------------------------------------------------------\n-\/\/ High-level access to an inline cache. Guaranteed to be MT-safe.\n+CompiledIC* CompiledIC_at(Relocation* call_reloc) {\n+  address call_site = call_reloc->addr();\n+  CompiledMethod* cm = CodeCache::find_blob(call_reloc->addr())->as_compiled_method();\n+  return CompiledIC_at(cm, call_site);\n+}\n@@ -198,2 +187,5 @@\n-void CompiledIC::initialize_from_iter(RelocIterator* iter) {\n-  assert(iter->addr() == _call->instruction_address(), \"must find ic_call\");\n+CompiledIC* CompiledIC_at(RelocIterator* reloc_iter) {\n+  CompiledIC* c_ic = new CompiledIC(reloc_iter);\n+  c_ic->verify();\n+  return c_ic;\n+}\n@@ -201,8 +193,3 @@\n-  if (iter->type() == relocInfo::virtual_call_type) {\n-    virtual_call_Relocation* r = iter->virtual_call_reloc();\n-    _is_optimized = false;\n-    _value = _call->get_load_instruction(r);\n-  } else {\n-    assert(iter->type() == relocInfo::opt_virtual_call_type, \"must be a virtual call\");\n-    _is_optimized = true;\n-    _value = nullptr;\n+void CompiledIC::ensure_initialized(CallInfo* call_info, Klass* receiver_klass) {\n+  if (!_data->is_initialized()) {\n+    _data->initialize(call_info, receiver_klass);\n@@ -212,17 +199,3 @@\n-CompiledIC::CompiledIC(CompiledMethod* cm, NativeCall* call)\n-  : _method(cm)\n-{\n-  _call = _method->call_wrapper_at((address) call);\n-  address ic_call = _call->instruction_address();\n-\n-  assert(ic_call != nullptr, \"ic_call address must be set\");\n-  assert(cm != nullptr, \"must pass compiled method\");\n-  assert(cm->contains(ic_call), \"must be in compiled method\");\n-\n-  \/\/ Search for the ic_call at the given address.\n-  RelocIterator iter(cm, ic_call, ic_call+1);\n-  bool ret = iter.next();\n-  assert(ret == true, \"relocInfo must exist at this address\");\n-  assert(iter.addr() == ic_call, \"must find ic_call\");\n-\n-  initialize_from_iter(&iter);\n+void CompiledIC::set_to_clean() {\n+  log_debug(inlinecache)(\"IC@\" INTPTR_FORMAT \": set to clean\", p2i(_call->instruction_address()));\n+  _call->set_destination_mt_safe(SharedRuntime::get_resolve_virtual_call_stub());\n@@ -231,5 +204,12 @@\n-CompiledIC::CompiledIC(RelocIterator* iter)\n-  : _method(iter->code())\n-{\n-  _call = _method->call_wrapper_at(iter->addr());\n-  address ic_call = _call->instruction_address();\n+void CompiledIC::set_to_monomorphic(bool caller_is_c1) {\n+  assert(data()->is_initialized(), \"must be initialized\");\n+  Method* method = data()->speculated_method();\n+  CompiledMethod* code = method->code();\n+  address entry;\n+  bool to_compiled = code != nullptr && code->is_in_use() && !code->is_unloading();\n+\n+  if (to_compiled) {\n+    entry = caller_is_c1 ? code->inline_entry_point() : code->entry_point();\n+  } else {\n+    entry = caller_is_c1 ? method->get_c2i_unverified_inline_entry() : method->get_c2i_unverified_entry();\n+  }\n@@ -237,4 +217,4 @@\n-  CompiledMethod* nm = iter->code();\n-  assert(ic_call != nullptr, \"ic_call address must be set\");\n-  assert(nm != nullptr, \"must pass compiled method\");\n-  assert(nm->contains(ic_call), \"must be in compiled method\");\n+  log_trace(inlinecache)(\"IC@\" INTPTR_FORMAT \": monomorphic to %s: %s\",\n+                         p2i(_call->instruction_address()),\n+                         to_compiled ? \"compiled\" : \"interpreter\",\n+                         method->print_value_string());\n@@ -242,1 +222,1 @@\n-  initialize_from_iter(iter);\n+  _call->set_destination_mt_safe(entry);\n@@ -245,10 +225,2 @@\n-\/\/ This function may fail for two reasons: either due to running out of vtable\n-\/\/ stubs, or due to running out of IC stubs in an attempted transition to a\n-\/\/ transitional state. The needs_ic_stub_refill value will be set if the failure\n-\/\/ was due to running out of IC stubs, in which case the caller will refill IC\n-\/\/ stubs and retry.\n-bool CompiledIC::set_to_megamorphic(CallInfo* call_info, Bytecodes::Code bytecode,\n-                                    bool& needs_ic_stub_refill, bool caller_is_c1, TRAPS) {\n-  assert(CompiledICLocker::is_safe(_method), \"mt unsafe call\");\n-  assert(!is_optimized(), \"cannot set an optimized virtual call to megamorphic\");\n-  assert(is_call_to_compiled() || is_call_to_interpreted(), \"going directly to megamorphic?\");\n+void CompiledIC::set_to_megamorphic(CallInfo* call_info, bool caller_is_c1) {\n+  assert(data()->is_initialized(), \"must be initialized\");\n@@ -257,2 +229,6 @@\n-  if (call_info->call_kind() == CallInfo::itable_call) {\n-    assert(bytecode == Bytecodes::_invokeinterface, \"\");\n+  if (call_info->call_kind() == CallInfo::direct_call) {\n+    \/\/ C1 sometimes compiles a callsite before the target method is loaded, resulting in\n+    \/\/ dynamically bound callsites that should really be statically bound. However, the\n+    \/\/ target method might not have a vtable or itable. We just wait for better code to arrive\n+    return;\n+  } else if (call_info->call_kind() == CallInfo::itable_call) {\n@@ -262,1 +238,1 @@\n-      return false;\n+      return;\n@@ -270,12 +246,1 @@\n-    CompiledICHolder* holder = new CompiledICHolder(call_info->resolved_method()->method_holder(),\n-                                                    call_info->resolved_klass(), false);\n-    holder->claim();\n-    if (!InlineCacheBuffer::create_transition_stub(this, holder, entry)) {\n-      delete holder;\n-      needs_ic_stub_refill = true;\n-      return false;\n-    }\n-    \/\/ LSan appears unable to follow malloc-based memory consistently when embedded as an immediate\n-    \/\/ in generated machine code. So we have to ignore it.\n-    LSAN_IGNORE_OBJECT(holder);\n-    assert(call_info->call_kind() == CallInfo::vtable_call, \"either itable or vtable\");\n+    assert(call_info->call_kind() == CallInfo::vtable_call, \"what else?\");\n@@ -288,5 +253,1 @@\n-      return false;\n-    }\n-    if (!InlineCacheBuffer::create_transition_stub(this, nullptr, entry)) {\n-      needs_ic_stub_refill = true;\n-      return false;\n+      return;\n@@ -296,6 +257,2 @@\n-  {\n-    ResourceMark rm;\n-    assert(call_info->selected_method() != nullptr, \"Unexpected null selected method\");\n-    log_trace(inlinecache)(\"IC@\" INTPTR_FORMAT \": to megamorphic %s entry: \" INTPTR_FORMAT,\n-                   p2i(instruction_address()), call_info->selected_method()->print_value_string(), p2i(entry));\n-  }\n+  log_trace(inlinecache)(\"IC@\" INTPTR_FORMAT \": to megamorphic %s entry: \" INTPTR_FORMAT,\n+                         p2i(_call->instruction_address()), call_info->selected_method()->print_value_string(), p2i(entry));\n@@ -303,8 +260,2 @@\n-  \/\/ We can't check this anymore. With lazy deopt we could have already\n-  \/\/ cleaned this IC entry before we even return. This is possible if\n-  \/\/ we ran out of space in the inline cache buffer trying to do the\n-  \/\/ set_next and we safepointed to free up space. This is a benign\n-  \/\/ race because the IC entry was complete when we safepointed so\n-  \/\/ cleaning it immediately is harmless.\n-  \/\/ assert(is_megamorphic(), \"sanity check\");\n-  return true;\n+  _call->set_destination_mt_safe(entry);\n+  assert(is_megamorphic(), \"sanity check\");\n@@ -313,0 +264,3 @@\n+void CompiledIC::update(CallInfo* call_info, Klass* receiver_klass, bool caller_is_c1) {\n+  \/\/ If this is the first time we fix the inline cache, we ensure it's initialized\n+  ensure_initialized(call_info, receiver_klass);\n@@ -314,30 +268,4 @@\n-\/\/ true if destination is megamorphic stub\n-bool CompiledIC::is_megamorphic() const {\n-  assert(CompiledICLocker::is_safe(_method), \"mt unsafe call\");\n-  assert(!is_optimized(), \"an optimized call cannot be megamorphic\");\n-\n-  \/\/ Cannot rely on cached_value. It is either an interface or a method.\n-  return VtableStubs::entry_point(ic_destination()) != nullptr;\n-}\n-\n-bool CompiledIC::is_call_to_compiled() const {\n-  assert(CompiledICLocker::is_safe(_method), \"mt unsafe call\");\n-\n-  CodeBlob* cb = CodeCache::find_blob(ic_destination());\n-  bool is_monomorphic = (cb != nullptr && cb->is_compiled());\n-  \/\/ Check that the cached_value is a klass for non-optimized monomorphic calls\n-  \/\/ This assertion is invalid for compiler1: a call that does not look optimized (no static stub) can be used\n-  \/\/ for calling directly to vep without using the inline cache (i.e., cached_value == nullptr).\n-  \/\/ For JVMCI this occurs because CHA is only used to improve inlining so call sites which could be optimized\n-  \/\/ virtuals because there are no currently loaded subclasses of a type are left as virtual call sites.\n-#ifdef ASSERT\n-  CodeBlob* caller = CodeCache::find_blob(instruction_address());\n-  bool is_c1_or_jvmci_method = caller->is_compiled_by_c1() || caller->is_compiled_by_jvmci();\n-  assert( is_c1_or_jvmci_method ||\n-         !is_monomorphic ||\n-         is_optimized() ||\n-         (cached_metadata() != nullptr && cached_metadata()->is_klass()), \"sanity check\");\n-#endif \/\/ ASSERT\n-  return is_monomorphic;\n-}\n-\n+  if (is_megamorphic()) {\n+    \/\/ Terminal state for the inline cache\n+    return;\n+  }\n@@ -345,9 +273,4 @@\n-bool CompiledIC::is_call_to_interpreted() const {\n-  assert(CompiledICLocker::is_safe(_method), \"mt unsafe call\");\n-  \/\/ Call to interpreter if destination is either calling to a stub (if it\n-  \/\/ is optimized), or calling to an I2C blob\n-  bool is_call_to_interpreted = false;\n-  if (!is_optimized()) {\n-    CodeBlob* cb = CodeCache::find_blob(ic_destination());\n-    is_call_to_interpreted = (cb != nullptr && cb->is_adapter_blob());\n-    assert(!is_call_to_interpreted || (is_icholder_call() && cached_icholder() != nullptr), \"sanity check\");\n+  if (is_speculated_klass(receiver_klass)) {\n+    \/\/ If the speculated class matches the receiver klass, we can speculate that will\n+    \/\/ continue to be the case with a monomorphic inline cache\n+    set_to_monomorphic(caller_is_c1);\n@@ -355,8 +278,3 @@\n-    \/\/ Check if we are calling into our own codeblob (i.e., to a stub)\n-    address dest = ic_destination();\n-#ifdef ASSERT\n-    {\n-      _call->verify_resolve_call(dest);\n-    }\n-#endif \/* ASSERT *\/\n-    is_call_to_interpreted = _call->is_call_to_interpreted(dest);\n+    \/\/ If the dynamic type speculation fails, we try to transform to a megamorphic state\n+    \/\/ for the inline cache using stubs to dispatch in tables\n+    set_to_megamorphic(call_info, caller_is_c1);\n@@ -364,1 +282,0 @@\n-  return is_call_to_interpreted;\n@@ -367,11 +284,3 @@\n-bool CompiledIC::set_to_clean(bool in_use) {\n-  assert(CompiledICLocker::is_safe(_method), \"mt unsafe call\");\n-  if (TraceInlineCacheClearing) {\n-    tty->print_cr(\"IC@\" INTPTR_FORMAT \": set to clean\", p2i(instruction_address()));\n-    print();\n-  }\n-  log_trace(inlinecache)(\"IC@\" INTPTR_FORMAT \": set to clean\", p2i(instruction_address()));\n-\n-  address entry = _call->get_resolve_call_stub(is_optimized());\n-\n-  bool safe_transition = _call->is_safe_for_patching() || !in_use || is_optimized() || SafepointSynchronize::is_at_safepoint();\n+bool CompiledIC::is_clean() const {\n+  return destination() == SharedRuntime::get_resolve_virtual_call_stub();\n+}\n@@ -379,22 +288,2 @@\n-  if (safe_transition) {\n-    \/\/ Kill any leftover stub we might have too\n-    clear_ic_stub();\n-    if (is_optimized()) {\n-      set_ic_destination(entry);\n-    } else {\n-      set_ic_destination_and_value(entry, (void*)nullptr);\n-    }\n-  } else {\n-    \/\/ Unsafe transition - create stub.\n-    if (!InlineCacheBuffer::create_transition_stub(this, nullptr, entry)) {\n-      return false;\n-    }\n-  }\n-  \/\/ We can't check this anymore. With lazy deopt we could have already\n-  \/\/ cleaned this IC entry before we even return. This is possible if\n-  \/\/ we ran out of space in the inline cache buffer trying to do the\n-  \/\/ set_next and we safepointed to free up space. This is a benign\n-  \/\/ race because the IC entry was complete when we safepointed so\n-  \/\/ cleaning it immediately is harmless.\n-  \/\/ assert(is_clean(), \"sanity check\");\n-  return true;\n+bool CompiledIC::is_monomorphic() const {\n+  return !is_clean() && !is_megamorphic();\n@@ -403,7 +292,2 @@\n-bool CompiledIC::is_clean() const {\n-  assert(CompiledICLocker::is_safe(_method), \"mt unsafe call\");\n-  bool is_clean = false;\n-  address dest = ic_destination();\n-  is_clean = dest == _call->get_resolve_call_stub(is_optimized());\n-  assert(!is_clean || is_optimized() || cached_value() == nullptr, \"sanity check\");\n-  return is_clean;\n+bool CompiledIC::is_megamorphic() const {\n+  return VtableStubs::entry_point(destination()) != nullptr;\n@@ -412,72 +296,3 @@\n-bool CompiledIC::set_to_monomorphic(CompiledICInfo& info) {\n-  assert(CompiledICLocker::is_safe(_method), \"mt unsafe call\");\n-  \/\/ Updating a cache to the wrong entry can cause bugs that are very hard\n-  \/\/ to track down - if cache entry gets invalid - we just clean it. In\n-  \/\/ this way it is always the same code path that is responsible for\n-  \/\/ updating and resolving an inline cache\n-  \/\/\n-  \/\/ The above is no longer true. SharedRuntime::fixup_callers_callsite will change optimized\n-  \/\/ callsites. In addition ic_miss code will update a site to monomorphic if it determines\n-  \/\/ that an monomorphic call to the interpreter can now be monomorphic to compiled code.\n-  \/\/\n-  \/\/ In both of these cases the only thing being modified is the jump\/call target and these\n-  \/\/ transitions are mt_safe\n-\n-  Thread *thread = Thread::current();\n-  if (info.to_interpreter()) {\n-    \/\/ Call to interpreter\n-    if (info.is_optimized() && is_optimized()) {\n-      assert(is_clean(), \"unsafe IC path\");\n-      \/\/ the call analysis (callee structure) specifies that the call is optimized\n-      \/\/ (either because of CHA or the static target is final)\n-      \/\/ At code generation time, this call has been emitted as static call\n-      \/\/ Call via stub\n-      assert(info.cached_metadata() != nullptr && info.cached_metadata()->is_method(), \"sanity check\");\n-      methodHandle method (thread, (Method*)info.cached_metadata());\n-      _call->set_to_interpreted(method, info);\n-\n-      {\n-        ResourceMark rm(thread);\n-        log_trace(inlinecache)(\"IC@\" INTPTR_FORMAT \": monomorphic to interpreter: %s\",\n-           p2i(instruction_address()),\n-           method->print_value_string());\n-      }\n-    } else {\n-      \/\/ Call via method-klass-holder\n-      CompiledICHolder* holder = info.claim_cached_icholder();\n-      if (!InlineCacheBuffer::create_transition_stub(this, holder, info.entry())) {\n-        delete holder;\n-        return false;\n-      }\n-      \/\/ LSan appears unable to follow malloc-based memory consistently when embedded as an\n-      \/\/ immediate in generated machine code. So we have to ignore it.\n-      LSAN_IGNORE_OBJECT(holder);\n-      {\n-         ResourceMark rm(thread);\n-         log_trace(inlinecache)(\"IC@\" INTPTR_FORMAT \": monomorphic to interpreter via icholder \", p2i(instruction_address()));\n-      }\n-    }\n-  } else {\n-    \/\/ Call to compiled code\n-    bool static_bound = info.is_optimized() || (info.cached_metadata() == nullptr);\n-#ifdef ASSERT\n-    CodeBlob* cb = CodeCache::find_blob(info.entry());\n-    assert (cb != nullptr && cb->is_compiled(), \"must be compiled!\");\n-#endif \/* ASSERT *\/\n-\n-    \/\/ This is MT safe if we come from a clean-cache and go through a\n-    \/\/ non-verified entry point\n-    bool safe = SafepointSynchronize::is_at_safepoint() ||\n-                (!is_in_transition_state() && (info.is_optimized() || static_bound || is_clean()));\n-\n-    if (!safe) {\n-      if (!InlineCacheBuffer::create_transition_stub(this, info.cached_metadata(), info.entry())) {\n-        return false;\n-      }\n-    } else {\n-      if (is_optimized()) {\n-        set_ic_destination(info.entry());\n-      } else {\n-        set_ic_destination_and_value(info.entry(), info.cached_metadata());\n-      }\n-    }\n+bool CompiledIC::is_speculated_klass(Klass* receiver_klass) {\n+  return data()->speculated_klass() == receiver_klass;\n+}\n@@ -485,78 +300,3 @@\n-    {\n-      ResourceMark rm(thread);\n-      assert(info.cached_metadata() == nullptr || info.cached_metadata()->is_klass(), \"must be\");\n-      log_trace(inlinecache)(\"IC@\" INTPTR_FORMAT \": monomorphic to compiled (rcvr klass = %s) %s\",\n-        p2i(instruction_address()),\n-        (info.cached_metadata() != nullptr) ? ((Klass*)info.cached_metadata())->print_value_string() : \"nullptr\",\n-        (safe) ? \"\" : \" via stub\");\n-    }\n-  }\n-  \/\/ We can't check this anymore. With lazy deopt we could have already\n-  \/\/ cleaned this IC entry before we even return. This is possible if\n-  \/\/ we ran out of space in the inline cache buffer trying to do the\n-  \/\/ set_next and we safepointed to free up space. This is a benign\n-  \/\/ race because the IC entry was complete when we safepointed so\n-  \/\/ cleaning it immediately is harmless.\n-  \/\/ assert(is_call_to_compiled() || is_call_to_interpreted(), \"sanity check\");\n-  return true;\n-}\n-\n-\n-\/\/ is_optimized: Compiler has generated an optimized call (i.e. fixed, no inline cache)\n-\/\/ static_bound: The call can be static bound. If it isn't also optimized, the property\n-\/\/ wasn't provable at time of compilation. An optimized call will have any necessary\n-\/\/ null check, while a static_bound won't. A static_bound (but not optimized) must\n-\/\/ therefore use the unverified entry point.\n-void CompiledIC::compute_monomorphic_entry(const methodHandle& method,\n-                                           Klass* receiver_klass,\n-                                           bool is_optimized,\n-                                           bool static_bound,\n-                                           bool caller_is_nmethod,\n-                                           bool caller_is_c1,\n-                                           CompiledICInfo& info,\n-                                           TRAPS) {\n-  CompiledMethod* method_code = method->code();\n-\n-  address entry = nullptr;\n-  if (method_code != nullptr && method_code->is_in_use() && !method_code->is_unloading()) {\n-    assert(method_code->is_compiled(), \"must be compiled\");\n-    \/\/ Call to compiled code\n-    \/\/\n-    \/\/ Note: the following problem exists with Compiler1:\n-    \/\/   - at compile time we may or may not know if the destination is final\n-    \/\/   - if we know that the destination is final (is_optimized), we will emit\n-    \/\/     an optimized virtual call (no inline cache), and need a Method* to make\n-    \/\/     a call to the interpreter\n-    \/\/   - if we don't know if the destination is final, we emit a standard\n-    \/\/     virtual call, and use CompiledICHolder to call interpreted code\n-    \/\/     (no static call stub has been generated)\n-    \/\/   - In the case that we here notice the call is static bound we\n-    \/\/     convert the call into what looks to be an optimized virtual call,\n-    \/\/     but we must use the unverified entry point (since there will be no\n-    \/\/     null check on a call when the target isn't loaded).\n-    \/\/     This causes problems when verifying the IC because\n-    \/\/     it looks vanilla but is optimized. Code in is_call_to_interpreted\n-    \/\/     is aware of this and weakens its asserts.\n-    if (is_optimized) {\n-      entry      = caller_is_c1 ? method_code->verified_inline_entry_point() : method_code->verified_entry_point();\n-    } else {\n-      entry      = caller_is_c1 ? method_code->inline_entry_point() : method_code->entry_point();\n-    }\n-  }\n-  if (entry != nullptr) {\n-    \/\/ Call to near compiled code.\n-    info.set_compiled_entry(entry, is_optimized ? nullptr : receiver_klass, is_optimized);\n-  } else {\n-    if (is_optimized) {\n-      \/\/ Use stub entry\n-      address entry = caller_is_c1 ? method()->get_c2i_inline_entry() : method()->get_c2i_entry();\n-      info.set_interpreter_entry(entry, method());\n-    } else {\n-      \/\/ Use icholder entry\n-      assert(method_code == nullptr || method_code->is_compiled(), \"must be compiled\");\n-      CompiledICHolder* holder = new CompiledICHolder(method(), receiver_klass);\n-      entry = (caller_is_c1)? method()->get_c2i_unverified_inline_entry() : method()->get_c2i_unverified_entry();\n-      info.set_icholder_entry(entry, holder);\n-    }\n-  }\n-  assert(info.is_optimized() == is_optimized, \"must agree\");\n+\/\/ GC support\n+void CompiledIC::clean_metadata() {\n+  data()->clean_metadata();\n@@ -565,0 +305,3 @@\n+void CompiledIC::metadata_do(MetadataClosure* cl) {\n+  data()->metadata_do(cl);\n+}\n@@ -566,11 +309,5 @@\n-bool CompiledIC::is_icholder_entry(address entry) {\n-  CodeBlob* cb = CodeCache::find_blob(entry);\n-  if (cb == nullptr) {\n-    return false;\n-  }\n-  if (cb->is_adapter_blob()) {\n-    return true;\n-  } else if (cb->is_vtable_blob()) {\n-    return VtableStubs::is_icholder_entry(entry);\n-  }\n-  return false;\n+#ifndef PRODUCT\n+void CompiledIC::print() {\n+  tty->print(\"Inline cache at \" INTPTR_FORMAT \", calling \" INTPTR_FORMAT \" cached_value \" INTPTR_FORMAT,\n+             p2i(instruction_address()), p2i(destination()), p2i(data()));\n+  tty->cr();\n@@ -579,4 +316,2 @@\n-bool CompiledIC::is_icholder_call_site(virtual_call_Relocation* call_site, const CompiledMethod* cm) {\n-  \/\/ This call site might have become stale so inspect it carefully.\n-  address dest = cm->call_wrapper_at(call_site->addr())->destination();\n-  return is_icholder_entry(dest);\n+void CompiledIC::verify() {\n+  _call->verify();\n@@ -584,0 +319,1 @@\n+#endif\n@@ -587,1 +323,1 @@\n-bool CompiledStaticCall::set_to_clean(bool in_use) {\n+void CompiledDirectCall::set_to_clean() {\n@@ -591,1 +327,14 @@\n-  set_destination_mt_safe(resolve_call_stub());\n+  RelocIterator iter((nmethod*)nullptr, instruction_address(), instruction_address() + 1);\n+  while (iter.next()) {\n+    switch(iter.type()) {\n+    case relocInfo::static_call_type:\n+      _call->set_destination_mt_safe(SharedRuntime::get_resolve_static_call_stub());\n+      break;\n+    case relocInfo::opt_virtual_call_type:\n+      _call->set_destination_mt_safe(SharedRuntime::get_resolve_opt_virtual_call_stub());\n+      break;\n+    default:\n+      ShouldNotReachHere();\n+    }\n+  }\n+  assert(is_clean(), \"should be clean after cleaning\");\n@@ -593,4 +342,1 @@\n-  \/\/ Do not reset stub here:  It is too expensive to call find_stub.\n-  \/\/ Instead, rely on caller (nmethod::clear_inline_caches) to clear\n-  \/\/ both the call and its stub.\n-  return true;\n+  log_debug(inlinecache)(\"DC@\" INTPTR_FORMAT \": set to clean\", p2i(_call->instruction_address()));\n@@ -599,3 +345,3 @@\n-bool CompiledStaticCall::is_clean() const {\n-  return destination() == resolve_call_stub();\n-}\n+void CompiledDirectCall::set(const methodHandle& callee_method, bool caller_is_c1) {\n+  CompiledMethod* code = callee_method->code();\n+  CompiledMethod* caller = CodeCache::find_compiled(instruction_address());\n@@ -603,3 +349,2 @@\n-bool CompiledStaticCall::is_call_to_compiled() const {\n-  return CodeCache::contains(destination());\n-}\n+  bool to_interp_cont_enter = caller->method()->is_continuation_enter_intrinsic() &&\n+                              ContinuationEntry::is_interpreted_call(instruction_address());\n@@ -607,6 +352,1 @@\n-bool CompiledDirectStaticCall::is_call_to_interpreted() const {\n-  \/\/ It is a call to interpreted, if it calls to a stub. Hence, the destination\n-  \/\/ must be in the stub part of the nmethod that contains the call\n-  CompiledMethod* cm = CodeCache::find_compiled(instruction_address());\n-  return cm->stub_contains(destination());\n-}\n+  bool to_compiled = !to_interp_cont_enter && code != nullptr && code->is_in_use() && !code->is_unloading();\n@@ -614,7 +354,9 @@\n-void CompiledStaticCall::set_to_compiled(address entry) {\n-  {\n-    ResourceMark rm;\n-    log_trace(inlinecache)(\"%s@\" INTPTR_FORMAT \": set_to_compiled \" INTPTR_FORMAT,\n-        name(),\n-        p2i(instruction_address()),\n-        p2i(entry));\n+  if (to_compiled) {\n+    _call->set_destination_mt_safe(caller_is_c1 ? code->verified_inline_entry_point() : code->verified_entry_point());\n+    assert(is_call_to_compiled(), \"should be compiled after set to compiled\");\n+  } else {\n+    \/\/ Patch call site to C2I adapter if code is deoptimized or unloaded.\n+    \/\/ We also need to patch the static call stub to set the rmethod register\n+    \/\/ to the callee_method so the c2i adapter knows how to build the frame\n+    set_to_interpreted(callee_method, caller_is_c1 ? callee_method->get_c2i_inline_entry() : callee_method->get_c2i_entry());\n+    assert(is_call_to_interpreted(), \"should be interpreted after set to interpreted\");\n@@ -622,3 +364,6 @@\n-  \/\/ Call to compiled code\n-  assert(CodeCache::contains(entry), \"wrong entry point\");\n-  set_destination_mt_safe(entry);\n+\n+  log_trace(inlinecache)(\"DC@\" INTPTR_FORMAT \": set to %s: %s: \" INTPTR_FORMAT,\n+                         p2i(_call->instruction_address()),\n+                         to_compiled ? \"compiled\" : \"interpreter\",\n+                         callee_method->print_value_string(),\n+                         p2i(_call->destination()));\n@@ -627,14 +372,3 @@\n-void CompiledStaticCall::set(const StaticCallInfo& info) {\n-  assert(CompiledICLocker::is_safe(instruction_address()), \"mt unsafe call\");\n-  \/\/ Updating a cache to the wrong entry can cause bugs that are very hard\n-  \/\/ to track down - if cache entry gets invalid - we just clean it. In\n-  \/\/ this way it is always the same code path that is responsible for\n-  \/\/ updating and resolving an inline cache\n-  assert(is_clean(), \"do not update a call entry - use clean\");\n-\n-  if (info._to_interpreter) {\n-    \/\/ Call to interpreted code\n-    set_to_interpreted(info.callee(), info.entry());\n-  } else {\n-    set_to_compiled(info.entry());\n-  }\n+bool CompiledDirectCall::is_clean() const {\n+  return destination() == SharedRuntime::get_resolve_static_call_stub() ||\n+         destination() == SharedRuntime::get_resolve_opt_virtual_call_stub();\n@@ -643,27 +377,5 @@\n-\/\/ Compute settings for a CompiledStaticCall. Since we might have to set\n-\/\/ the stub when calling to the interpreter, we need to return arguments.\n-void CompiledStaticCall::compute_entry(const methodHandle& m, CompiledMethod* caller_nm, StaticCallInfo& info) {\n-  assert(!m->mismatch(), \"Mismatch for static call\");\n-  bool caller_is_nmethod = caller_nm->is_nmethod();\n-  CompiledMethod* m_code = m->code();\n-  info._callee = m;\n-  if (m_code != nullptr && m_code->is_in_use() && !m_code->is_unloading()) {\n-    info._to_interpreter = false;\n-    if (caller_nm->is_compiled_by_c1()) {\n-      info._entry = m_code->verified_inline_entry_point();\n-    } else {\n-      info._entry = m_code->verified_entry_point();\n-    }\n-  } else {\n-    \/\/ Callee is interpreted code.  In any case entering the interpreter\n-    \/\/ puts a converter-frame on the stack to save arguments.\n-    assert(!m->is_method_handle_intrinsic(), \"Compiled code should never call interpreter MH intrinsics\");\n-    info._to_interpreter = true;\n-    if (caller_nm->is_compiled_by_c1()) {\n-      \/\/ C1 -> interp: values passed as oops\n-      info._entry = m()->get_c2i_inline_entry();\n-    } else {\n-      \/\/ C2 -> interp: values passed as fields\n-      info._entry = m()->get_c2i_entry();\n-    }\n-  }\n+bool CompiledDirectCall::is_call_to_interpreted() const {\n+  \/\/ It is a call to interpreted, if it calls to a stub. Hence, the destination\n+  \/\/ must be in the stub part of the nmethod that contains the call\n+  CompiledMethod* cm = CodeCache::find_compiled(instruction_address());\n+  return cm->stub_contains(destination());\n@@ -672,5 +384,4 @@\n-void CompiledStaticCall::compute_entry_for_continuation_entry(const methodHandle& m, StaticCallInfo& info) {\n-  if (ContinuationEntry::is_interpreted_call(instruction_address())) {\n-    info._to_interpreter = true;\n-    info._entry = m()->get_c2i_entry();\n-  }\n+bool CompiledDirectCall::is_call_to_compiled() const {\n+  CompiledMethod* caller = CodeCache::find_compiled(instruction_address());\n+  CodeBlob* dest_cb = CodeCache::find_blob(destination());\n+  return !caller->stub_contains(destination()) && dest_cb->is_compiled();\n@@ -679,1 +390,1 @@\n-address CompiledDirectStaticCall::find_stub_for(address instruction) {\n+address CompiledDirectCall::find_stub_for(address instruction) {\n@@ -691,2 +402,0 @@\n-        case relocInfo::poll_type:\n-        case relocInfo::poll_return_type: \/\/ A safepoint can't overlap a call.\n@@ -701,2 +410,2 @@\n-address CompiledDirectStaticCall::find_stub() {\n-  return CompiledDirectStaticCall::find_stub_for(instruction_address());\n+address CompiledDirectCall::find_stub() {\n+  return find_stub_for(instruction_address());\n@@ -705,25 +414,2 @@\n-address CompiledDirectStaticCall::resolve_call_stub() const {\n-  return SharedRuntime::get_resolve_static_call_stub();\n-}\n-\n-\/\/-----------------------------------------------------------------------------\n-\/\/ Non-product mode code\n-\n-void CompiledIC::verify() {\n-  _call->verify();\n-  assert(is_clean() || is_call_to_compiled() || is_call_to_interpreted()\n-          || is_optimized() || is_megamorphic(), \"sanity check\");\n-}\n-\n-void CompiledIC::print() {\n-  print_compiled_ic();\n-  tty->cr();\n-}\n-\n-void CompiledIC::print_compiled_ic() {\n-  tty->print(\"Inline cache at \" INTPTR_FORMAT \", calling %s \" INTPTR_FORMAT \" cached_value \" INTPTR_FORMAT,\n-             p2i(instruction_address()), is_call_to_interpreted() ? \"interpreted \" : \"\", p2i(ic_destination()), p2i(is_optimized() ? nullptr : cached_value()));\n-}\n-\n-void CompiledDirectStaticCall::print() {\n-  tty->print(\"static call at \" INTPTR_FORMAT \" -> \", p2i(instruction_address()));\n+void CompiledDirectCall::print() {\n+  tty->print(\"direct call at \" INTPTR_FORMAT \" to \" INTPTR_FORMAT \" -> \", p2i(instruction_address()), p2i(destination()));\n@@ -741,3 +427,4 @@\n-void CompiledDirectStaticCall::verify_mt_safe(const methodHandle& callee, address entry,\n-                                              NativeMovConstReg* method_holder,\n-                                              NativeJump*        jump) {\n+void CompiledDirectCall::verify_mt_safe(const methodHandle& callee, address entry,\n+                                        NativeMovConstReg* method_holder,\n+                                        NativeJump* jump) {\n+  _call->verify();\n@@ -761,1 +448,1 @@\n-#endif \/\/ !PRODUCT\n+#endif\n","filename":"src\/hotspot\/share\/code\/compiledIC.cpp","additions":227,"deletions":540,"binary":false,"changes":767,"status":"modified"},{"patch":"@@ -30,1 +30,0 @@\n-#include \"oops\/compiledICHolder.hpp\"\n@@ -36,25 +35,4 @@\n-\/\/ In order to make patching of the inline cache MT-safe, we only allow the following\n-\/\/ transitions (when not at a safepoint):\n-\/\/\n-\/\/\n-\/\/         [1] --<--  Clean -->---  [1]\n-\/\/            \/       (null)      \\\n-\/\/           \/                     \\      \/-<-\\\n-\/\/          \/          [2]          \\    \/     \\\n-\/\/      Interpreted  ---------> Monomorphic     | [3]\n-\/\/  (CompiledICHolder*)            (Klass*)     |\n-\/\/          \\                        \/   \\     \/\n-\/\/       [4] \\                      \/ [4] \\->-\/\n-\/\/            \\->-  Megamorphic -<-\/\n-\/\/              (CompiledICHolder*)\n-\/\/\n-\/\/ The text in parentheses () refers to the value of the inline cache receiver (mov instruction)\n-\/\/\n-\/\/ The numbers in square brackets refer to the kind of transition:\n-\/\/ [1]: Initial fixup. Receiver it found from debug information\n-\/\/ [2]: Compilation of a method\n-\/\/ [3]: Recompilation of a method (note: only entry is changed. The Klass* must stay the same)\n-\/\/ [4]: Inline cache miss. We go directly to megamorphic call.\n-\/\/\n-\/\/ The class automatically inserts transition stubs (using the InlineCacheBuffer) when an MT-unsafe\n-\/\/ transition is made to a stub.\n+\/\/ It's safe to transition from any state to any state. Typically an inline cache starts\n+\/\/ in the clean state, meaning it will resolve the call when called. Then it typically\n+\/\/ transitions to monomorphic, assuming the first dynamic receiver will be the only one\n+\/\/ observed. If that speculation fails, we transition to megamorphic.\n@@ -65,1 +43,0 @@\n-class ICStub;\n@@ -80,8 +57,17 @@\n-class CompiledICInfo : public StackObj {\n- private:\n-  address _entry;              \/\/ entry point for call\n-  void*   _cached_value;         \/\/ Value of cached_value (either in stub or inline cache)\n-  bool    _is_icholder;          \/\/ Is the cached value a CompiledICHolder*\n-  bool    _is_optimized;       \/\/ it is an optimized virtual call (i.e., can be statically bound)\n-  bool    _to_interpreter;     \/\/ Call it to interpreter\n-  bool    _release_icholder;\n+\/\/ A CompiledICData is a helper object for the inline cache implementation.\n+\/\/ It comprises:\n+\/\/ (1) The first receiver klass and its selected method\n+\/\/ (2) Itable call metadata\n+\n+class CompiledICData : public CHeapObj<mtCode> {\n+  friend class VMStructs;\n+  friend class JVMCIVMStructs;\n+\n+  Method*   volatile _speculated_method;\n+  uintptr_t volatile _speculated_klass;\n+  Klass*             _itable_defc_klass;\n+  Klass*             _itable_refc_klass;\n+  bool               _is_initialized;\n+\n+  bool is_speculated_klass_unloaded() const;\n+\n@@ -89,21 +75,2 @@\n-  address entry() const        { return _entry; }\n-  Metadata*    cached_metadata() const         { assert(!_is_icholder, \"\"); return (Metadata*)_cached_value; }\n-  CompiledICHolder*    claim_cached_icholder() {\n-    assert(_is_icholder, \"\");\n-    assert(_cached_value != nullptr, \"must be non-null\");\n-    _release_icholder = false;\n-    CompiledICHolder* icholder = (CompiledICHolder*)_cached_value;\n-    icholder->claim();\n-    return icholder;\n-  }\n-  bool    is_optimized() const { return _is_optimized; }\n-  bool  to_interpreter() const { return _to_interpreter; }\n-\n-  void set_compiled_entry(address entry, Klass* klass, bool is_optimized) {\n-    _entry      = entry;\n-    _cached_value = (void*)klass;\n-    _to_interpreter = false;\n-    _is_icholder = false;\n-    _is_optimized = is_optimized;\n-    _release_icholder = false;\n-  }\n+  \/\/ Constructor\n+  CompiledICData();\n@@ -111,8 +78,5 @@\n-  void set_interpreter_entry(address entry, Method* method) {\n-    _entry      = entry;\n-    _cached_value = (void*)method;\n-    _to_interpreter = true;\n-    _is_icholder = false;\n-    _is_optimized = true;\n-    _release_icholder = false;\n-  }\n+  \/\/ accessors\n+  Klass*    speculated_klass()  const;\n+  Method*   speculated_method() const { return _speculated_method; }\n+  Klass*    itable_defc_klass() const { return _itable_defc_klass; }\n+  Klass*    itable_refc_klass() const { return _itable_refc_klass; }\n@@ -120,8 +84,2 @@\n-  void set_icholder_entry(address entry, CompiledICHolder* icholder) {\n-    _entry      = entry;\n-    _cached_value = (void*)icholder;\n-    _to_interpreter = true;\n-    _is_icholder = true;\n-    _is_optimized = false;\n-    _release_icholder = true;\n-  }\n+  static ByteSize speculated_method_offset() { return byte_offset_of(CompiledICData, _speculated_method); }\n+  static ByteSize speculated_klass_offset()  { return byte_offset_of(CompiledICData, _speculated_klass); }\n@@ -129,14 +87,2 @@\n-  CompiledICInfo(): _entry(nullptr), _cached_value(nullptr), _is_icholder(false),\n-                    _is_optimized(false), _to_interpreter(false), _release_icholder(false) {\n-  }\n-  ~CompiledICInfo() {\n-    \/\/ In rare cases the info is computed but not used, so release any\n-    \/\/ CompiledICHolder* that was created\n-    if (_release_icholder) {\n-      assert(_is_icholder, \"must be\");\n-      CompiledICHolder* icholder = (CompiledICHolder*)_cached_value;\n-      icholder->claim();\n-      delete icholder;\n-    }\n-  }\n-};\n+  static ByteSize itable_defc_klass_offset() { return byte_offset_of(CompiledICData, _itable_defc_klass); }\n+  static ByteSize itable_refc_klass_offset() { return byte_offset_of(CompiledICData, _itable_refc_klass); }\n@@ -144,19 +90,7 @@\n-class NativeCallWrapper: public ResourceObj {\n-public:\n-  virtual address destination() const = 0;\n-  virtual address instruction_address() const = 0;\n-  virtual address next_instruction_address() const = 0;\n-  virtual address return_address() const = 0;\n-  virtual address get_resolve_call_stub(bool is_optimized) const = 0;\n-  virtual void set_destination_mt_safe(address dest) = 0;\n-  virtual void set_to_interpreted(const methodHandle& method, CompiledICInfo& info) = 0;\n-  virtual void verify() const = 0;\n-  virtual void verify_resolve_call(address dest) const = 0;\n-\n-  virtual bool is_call_to_interpreted(address dest) const = 0;\n-  virtual bool is_safe_for_patching() const = 0;\n-\n-  virtual NativeInstruction* get_load_instruction(virtual_call_Relocation* r) const = 0;\n-\n-  virtual void *get_data(NativeInstruction* instruction) const = 0;\n-  virtual void set_data(NativeInstruction* instruction, intptr_t data) = 0;\n+  void initialize(CallInfo* call_info, Klass* receiver_klass);\n+\n+  bool is_initialized()       const { return _is_initialized; }\n+\n+  \/\/ GC Support\n+  void clean_metadata();\n+  void metadata_do(MetadataClosure* cl);\n@@ -166,7 +100,1 @@\n-  friend class InlineCacheBuffer;\n-  friend class ICStub;\n-\n- private:\n-  NativeCallWrapper* _call;\n-  NativeInstruction* _value;    \/\/ patchable value cell for this IC\n-  bool          _is_optimized;  \/\/ an optimized virtual call (i.e., no compiled IC)\n+private:\n@@ -174,0 +102,2 @@\n+  CompiledICData* _data;\n+  NativeCall* _call;\n@@ -175,1 +105,0 @@\n-  CompiledIC(CompiledMethod* cm, NativeCall* ic_call);\n@@ -178,1 +107,3 @@\n-  void initialize_from_iter(RelocIterator* iter);\n+  \/\/ CompiledICData wrappers\n+  void ensure_initialized(CallInfo* call_info, Klass* receiver_klass);\n+  bool is_speculated_klass(Klass* receiver_klass);\n@@ -180,1 +111,3 @@\n-  static bool is_icholder_entry(address entry);\n+  \/\/ Inline cache states\n+  void set_to_monomorphic(bool caller_is_c1);\n+  void set_to_megamorphic(CallInfo* call_info, bool caller_is_c1);\n@@ -182,26 +115,1 @@\n-  \/\/ low-level inline-cache manipulation. Cannot be accessed directly, since it might not be MT-safe\n-  \/\/ to change an inline-cache. These changes the underlying inline-cache directly. They *newer* make\n-  \/\/ changes to a transition stub.\n-  void internal_set_ic_destination(address entry_point, bool is_icstub, void* cache, bool is_icholder);\n-  void set_ic_destination(ICStub* stub);\n-  void set_ic_destination(address entry_point) {\n-    assert(_is_optimized, \"use set_ic_destination_and_value instead\");\n-    internal_set_ic_destination(entry_point, false, nullptr, false);\n-  }\n-  \/\/ This only for use by ICStubs where the type of the value isn't known\n-  void set_ic_destination_and_value(address entry_point, void* value) {\n-    internal_set_ic_destination(entry_point, false, value, is_icholder_entry(entry_point));\n-  }\n-  void set_ic_destination_and_value(address entry_point, Metadata* value) {\n-    internal_set_ic_destination(entry_point, false, value, false);\n-  }\n-  void set_ic_destination_and_value(address entry_point, CompiledICHolder* value) {\n-    internal_set_ic_destination(entry_point, false, value, true);\n-  }\n-\n-  \/\/ Reads the location of the transition stub. This will fail with an assertion, if no transition stub is\n-  \/\/ associated with the inline cache.\n-  address stub_address() const;\n-  bool is_in_transition_state() const;  \/\/ Use InlineCacheBuffer\n-\n- public:\n+public:\n@@ -214,25 +122,1 @@\n-  static bool is_icholder_call_site(virtual_call_Relocation* call_site, const CompiledMethod* cm);\n-\n-  \/\/ Return the cached_metadata\/destination associated with this inline cache. If the cache currently points\n-  \/\/ to a transition stub, it will read the values from the transition stub.\n-  void* cached_value() const;\n-  CompiledICHolder* cached_icholder() const {\n-    assert(is_icholder_call(), \"must be\");\n-    return (CompiledICHolder*) cached_value();\n-  }\n-  Metadata* cached_metadata() const {\n-    assert(!is_icholder_call(), \"must be\");\n-    return (Metadata*) cached_value();\n-  }\n-\n-  void* get_data() const {\n-    return _call->get_data(_value);\n-  }\n-\n-  void set_data(intptr_t data) {\n-    _call->set_data(_value, data);\n-  }\n-\n-  address ic_destination() const;\n-\n-  bool is_optimized() const   { return _is_optimized; }\n+  CompiledICData* data() const;\n@@ -241,1 +125,2 @@\n-  bool is_clean() const;\n+  bool is_clean()       const;\n+  bool is_monomorphic() const;\n@@ -243,5 +128,1 @@\n-  bool is_call_to_compiled() const;\n-  bool is_call_to_interpreted() const;\n-\n-  bool is_icholder_call() const;\n-  address end_of_call() const { return  _call->return_address(); }\n+  address end_of_call() const { return _call->return_address(); }\n@@ -250,1 +131,1 @@\n-  \/\/ MT-safe patching of inline caches. Note: Only safe to call is_xxx when holding the CompiledIC_ock\n+  \/\/ MT-safe patching of inline caches. Note: Only safe to call is_xxx when holding the CompiledICLocker\n@@ -252,17 +133,6 @@\n-  \/\/\n-  \/\/ Note: We do not provide any direct access to the stub code, to prevent parts of the code\n-  \/\/ to manipulate the inline cache in MT-unsafe ways.\n-  \/\/\n-  \/\/ They all takes a TRAP argument, since they can cause a GC if the inline-cache buffer is full.\n-  \/\/\n-  bool set_to_clean(bool in_use = true);\n-  bool set_to_monomorphic(CompiledICInfo& info);\n-  void clear_ic_stub();\n-\n-  \/\/ Returns true if successful and false otherwise. The call can fail if memory\n-  \/\/ allocation in the code cache fails, or ic stub refill is required.\n-  bool set_to_megamorphic(CallInfo* call_info, Bytecodes::Code bytecode, bool& needs_ic_stub_refill, bool caller_is_c1, TRAPS);\n-\n-  static void compute_monomorphic_entry(const methodHandle& method, Klass* receiver_klass,\n-                                        bool is_optimized, bool static_bound, bool caller_is_nmethod,\n-                                        bool caller_is_c1, CompiledICInfo& info, TRAPS);\n+  void set_to_clean();\n+  void update(CallInfo* call_info, Klass* receiver_klass, bool caller_is_c1);\n+\n+  \/\/ GC support\n+  void clean_metadata();\n+  void metadata_do(MetadataClosure* cl);\n@@ -272,0 +142,1 @@\n+  address destination() const         { return _call->destination(); }\n@@ -275,1 +146,0 @@\n-  void print_compiled_ic() PRODUCT_RETURN;\n@@ -279,27 +149,4 @@\n-inline CompiledIC* CompiledIC_before(CompiledMethod* nm, address return_addr) {\n-  CompiledIC* c_ic = new CompiledIC(nm, nativeCall_before(return_addr));\n-  c_ic->verify();\n-  return c_ic;\n-}\n-\n-inline CompiledIC* CompiledIC_at(CompiledMethod* nm, address call_site) {\n-  CompiledIC* c_ic = new CompiledIC(nm, nativeCall_at(call_site));\n-  c_ic->verify();\n-  return c_ic;\n-}\n-\n-inline CompiledIC* CompiledIC_at(Relocation* call_site) {\n-  assert(call_site->type() == relocInfo::virtual_call_type ||\n-         call_site->type() == relocInfo::opt_virtual_call_type, \"wrong reloc. info\");\n-  CompiledIC* c_ic = new CompiledIC(call_site->code(), nativeCall_at(call_site->addr()));\n-  c_ic->verify();\n-  return c_ic;\n-}\n-\n-inline CompiledIC* CompiledIC_at(RelocIterator* reloc_iter) {\n-  assert(reloc_iter->type() == relocInfo::virtual_call_type ||\n-      reloc_iter->type() == relocInfo::opt_virtual_call_type, \"wrong reloc. info\");\n-  CompiledIC* c_ic = new CompiledIC(reloc_iter);\n-  c_ic->verify();\n-  return c_ic;\n-}\n+CompiledIC* CompiledIC_before(CompiledMethod* nm, address return_addr);\n+CompiledIC* CompiledIC_at(CompiledMethod* nm, address call_site);\n+CompiledIC* CompiledIC_at(Relocation* call_site);\n+CompiledIC* CompiledIC_at(RelocIterator* reloc_iter);\n@@ -308,3 +155,1 @@\n-\/\/ The CompiledStaticCall represents a call to a static method in the compiled\n-\/\/\n-\/\/ Transition diagram of a static call site is somewhat simpler than for an inlined cache:\n+\/\/ The CompiledDirectCall represents a call to a method in the compiled code\n@@ -324,57 +169,1 @@\n-class StaticCallInfo {\n- private:\n-  address      _entry;          \/\/ Entrypoint\n-  methodHandle _callee;         \/\/ Callee (used when calling interpreter)\n-  bool         _to_interpreter; \/\/ call to interpreted method (otherwise compiled)\n-\n-  friend class CompiledStaticCall;\n-  friend class CompiledDirectStaticCall;\n-  friend class CompiledPltStaticCall;\n- public:\n-  address      entry() const    { return _entry;  }\n-  methodHandle callee() const   { return _callee; }\n-};\n-\n-class CompiledStaticCall : public ResourceObj {\n- public:\n-  \/\/ Code\n-\n-  \/\/ Returns null if CodeBuffer::expand fails\n-  static address emit_to_interp_stub(CodeBuffer &cbuf, address mark = nullptr);\n-  static int to_interp_stub_size();\n-  static int to_trampoline_stub_size();\n-  static int reloc_to_interp_stub();\n-\n-  \/\/ Compute entry point given a method\n-  static void compute_entry(const methodHandle& m, CompiledMethod* caller_nm, StaticCallInfo& info);\n-  void compute_entry_for_continuation_entry(const methodHandle& m, StaticCallInfo& info);\n-\n-public:\n-  \/\/ Clean static call (will force resolving on next use)\n-  virtual address destination() const = 0;\n-\n-  \/\/ Clean static call (will force resolving on next use)\n-  bool set_to_clean(bool in_use = true);\n-\n-  \/\/ Set state. The entry must be the same, as computed by compute_entry.\n-  \/\/ Computation and setting is split up, since the actions are separate during\n-  \/\/ a OptoRuntime::resolve_xxx.\n-  void set(const StaticCallInfo& info);\n-\n-  \/\/ State\n-  bool is_clean() const;\n-  bool is_call_to_compiled() const;\n-  virtual bool is_call_to_interpreted() const = 0;\n-\n-  virtual address instruction_address() const = 0;\n-  virtual address end_of_call() const = 0;\n-protected:\n-  virtual address resolve_call_stub() const = 0;\n-  virtual void set_destination_mt_safe(address dest) = 0;\n-  virtual void set_to_interpreted(const methodHandle& callee, address entry) = 0;\n-  virtual const char* name() const = 0;\n-\n-  void set_to_compiled(address entry);\n-};\n-\n-class CompiledDirectStaticCall : public CompiledStaticCall {\n+class CompiledDirectCall : public ResourceObj {\n@@ -395,1 +184,1 @@\n-  CompiledDirectStaticCall(NativeCall* call) : _call(call) {}\n+  CompiledDirectCall(NativeCall* call) : _call(call) {}\n@@ -398,2 +187,8 @@\n-  static inline CompiledDirectStaticCall* before(address return_addr) {\n-    CompiledDirectStaticCall* st = new CompiledDirectStaticCall(nativeCall_before(return_addr));\n+  \/\/ Returns null if CodeBuffer::expand fails\n+  static address emit_to_interp_stub(CodeBuffer &cbuf, address mark = nullptr);\n+  static int to_interp_stub_size();\n+  static int to_trampoline_stub_size();\n+  static int reloc_to_interp_stub();\n+\n+  static inline CompiledDirectCall* before(address return_addr) {\n+    CompiledDirectCall* st = new CompiledDirectCall(nativeCall_before(return_addr));\n@@ -404,2 +199,2 @@\n-  static inline CompiledDirectStaticCall* at(address native_call) {\n-    CompiledDirectStaticCall* st = new CompiledDirectStaticCall(nativeCall_at(native_call));\n+  static inline CompiledDirectCall* at(address native_call) {\n+    CompiledDirectCall* st = new CompiledDirectCall(nativeCall_at(native_call));\n@@ -410,1 +205,1 @@\n-  static inline CompiledDirectStaticCall* at(Relocation* call_site) {\n+  static inline CompiledDirectCall* at(Relocation* call_site) {\n@@ -418,0 +213,5 @@\n+  \/\/ Clean static call (will force resolving on next use)\n+  void set_to_clean();\n+\n+  void set(const methodHandle& callee_method, bool caller_is_c1);\n+\n@@ -419,1 +219,3 @@\n-  virtual bool is_call_to_interpreted() const;\n+  bool is_clean() const;\n+  bool is_call_to_interpreted() const;\n+  bool is_call_to_compiled() const;\n@@ -429,4 +231,0 @@\n-\n- protected:\n-  virtual address resolve_call_stub() const;\n-  virtual const char* name() const { return \"CompiledDirectStaticCall\"; }\n","filename":"src\/hotspot\/share\/code\/compiledIC.hpp","additions":87,"deletions":289,"binary":false,"changes":376,"status":"modified"},{"patch":"@@ -31,1 +31,0 @@\n-#include \"code\/icBuffer.hpp\"\n@@ -39,1 +38,0 @@\n-#include \"oops\/compiledICHolder.inline.hpp\"\n@@ -338,22 +336,0 @@\n-int CompiledMethod::verify_icholder_relocations() {\n-  ResourceMark rm;\n-  int count = 0;\n-\n-  RelocIterator iter(this);\n-  while(iter.next()) {\n-    if (iter.type() == relocInfo::virtual_call_type) {\n-      if (CompiledIC::is_icholder_call_site(iter.virtual_call_reloc(), this)) {\n-        CompiledIC *ic = CompiledIC_at(&iter);\n-        if (TraceCompiledIC) {\n-          tty->print(\"noticed icholder \" INTPTR_FORMAT \" \", p2i(ic->cached_icholder()));\n-          ic->print();\n-        }\n-        assert(ic->cached_icholder() != nullptr, \"must be non-nullptr\");\n-        count++;\n-      }\n-    }\n-  }\n-\n-  return count;\n-}\n-\n@@ -445,14 +421,0 @@\n-\/\/ Clear IC callsites, releasing ICStubs of all compiled ICs\n-\/\/ as well as any associated CompiledICHolders.\n-void CompiledMethod::clear_ic_callsites() {\n-  assert(CompiledICLocker::is_safe(this), \"mt unsafe call\");\n-  ResourceMark rm;\n-  RelocIterator iter(this);\n-  while(iter.next()) {\n-    if (iter.type() == relocInfo::virtual_call_type) {\n-      CompiledIC* ic = CompiledIC_at(&iter);\n-      ic->set_to_clean(false);\n-    }\n-  }\n-}\n-\n@@ -480,36 +442,2 @@\n-bool CompiledMethod::clean_ic_if_metadata_is_dead(CompiledIC *ic) {\n-  if (ic->is_clean()) {\n-    return true;\n-  }\n-  if (ic->is_icholder_call()) {\n-    \/\/ The only exception is compiledICHolder metadata which may\n-    \/\/ yet be marked below. (We check this further below).\n-    CompiledICHolder* cichk_metdata = ic->cached_icholder();\n-\n-    if (cichk_metdata->is_loader_alive()) {\n-      return true;\n-    }\n-  } else {\n-    Metadata* ic_metdata = ic->cached_metadata();\n-    if (ic_metdata != nullptr) {\n-      if (ic_metdata->is_klass()) {\n-        if (((Klass*)ic_metdata)->is_loader_alive()) {\n-          return true;\n-        }\n-      } else if (ic_metdata->is_method()) {\n-        Method* method = (Method*)ic_metdata;\n-        assert(!method->is_old(), \"old method should have been cleaned\");\n-        if (method->method_holder()->is_loader_alive()) {\n-          return true;\n-        }\n-      } else {\n-        ShouldNotReachHere();\n-      }\n-    } else {\n-      \/\/ This inline cache is a megamorphic vtable call. Those ICs never hold\n-      \/\/ any Metadata and should therefore never be cleaned by this function.\n-      return true;\n-    }\n-  }\n-\n-  return ic->set_to_clean();\n+static void clean_ic_if_metadata_is_dead(CompiledIC *ic) {\n+  ic->clean_metadata();\n@@ -519,2 +447,2 @@\n-template <class CompiledICorStaticCall>\n-static bool clean_if_nmethod_is_unloaded(CompiledICorStaticCall *ic, address addr, CompiledMethod* from,\n+template <typename CallsiteT>\n+static void clean_if_nmethod_is_unloaded(CallsiteT* callsite, CompiledMethod* from,\n@@ -522,10 +450,7 @@\n-  CodeBlob *cb = CodeCache::find_blob(addr);\n-  CompiledMethod* nm = (cb != nullptr) ? cb->as_compiled_method_or_null() : nullptr;\n-  if (nm != nullptr) {\n-    \/\/ Clean inline caches pointing to bad nmethods\n-    if (clean_all || !nm->is_in_use() || nm->is_unloading() || (nm->method()->code() != nm)) {\n-      if (!ic->set_to_clean(!from->is_unloading())) {\n-        return false;\n-      }\n-      assert(ic->is_clean(), \"nmethod \" PTR_FORMAT \"not clean %s\", p2i(from), from->method()->name_and_sig_as_C_string());\n-    }\n+  CodeBlob* cb = CodeCache::find_blob(callsite->destination());\n+  if (!cb->is_compiled()) {\n+    return;\n+  }\n+  CompiledMethod* cm = cb->as_compiled_method();\n+  if (clean_all || !cm->is_in_use() || cm->is_unloading() || cm->method()->code() != cm) {\n+    callsite->set_to_clean();\n@@ -533,11 +458,0 @@\n-  return true;\n-}\n-\n-static bool clean_if_nmethod_is_unloaded(CompiledIC *ic, CompiledMethod* from,\n-                                         bool clean_all) {\n-  return clean_if_nmethod_is_unloaded(ic, ic->ic_destination(), from, clean_all);\n-}\n-\n-static bool clean_if_nmethod_is_unloaded(CompiledStaticCall *csc, CompiledMethod* from,\n-                                         bool clean_all) {\n-  return clean_if_nmethod_is_unloaded(csc, csc->destination(), from, clean_all);\n@@ -553,1 +467,1 @@\n-bool CompiledMethod::unload_nmethod_caches(bool unloading_occurred) {\n+void CompiledMethod::unload_nmethod_caches(bool unloading_occurred) {\n@@ -561,3 +475,1 @@\n-  if (!cleanup_inline_caches_impl(unloading_occurred, false)) {\n-    return false;\n-  }\n+  cleanup_inline_caches_impl(unloading_occurred, false);\n@@ -570,1 +482,0 @@\n-  return true;\n@@ -592,2 +503,1 @@\n-  guarantee(cleanup_inline_caches_impl(false \/* unloading_occurred *\/, true \/* clean_all *\/),\n-            \"Inline cache cleaning in a safepoint can't fail\");\n+  cleanup_inline_caches_impl(false \/* unloading_occurred *\/, true \/* clean_all *\/);\n@@ -601,1 +511,1 @@\n-bool CompiledMethod::cleanup_inline_caches_impl(bool unloading_occurred, bool clean_all) {\n+void CompiledMethod::cleanup_inline_caches_impl(bool unloading_occurred, bool clean_all) {\n@@ -616,3 +526,1 @@\n-        if (!clean_ic_if_metadata_is_dead(CompiledIC_at(&iter))) {\n-          return false;\n-        }\n+        clean_ic_if_metadata_is_dead(CompiledIC_at(&iter));\n@@ -621,3 +529,1 @@\n-      if (!clean_if_nmethod_is_unloaded(CompiledIC_at(&iter), this, clean_all)) {\n-        return false;\n-      }\n+      clean_if_nmethod_is_unloaded(CompiledIC_at(&iter), this, clean_all);\n@@ -627,8 +533,1 @@\n-      if (!clean_if_nmethod_is_unloaded(CompiledIC_at(&iter), this, clean_all)) {\n-        return false;\n-      }\n-      break;\n-\n-      if (!clean_if_nmethod_is_unloaded(compiledStaticCall_at(iter.reloc()), this, clean_all)) {\n-        return false;\n-      }\n+      clean_if_nmethod_is_unloaded(CompiledDirectCall::at(iter.reloc()), this, clean_all);\n@@ -686,2 +585,0 @@\n-\n-  return true;\n","filename":"src\/hotspot\/share\/code\/compiledMethod.cpp","additions":18,"deletions":121,"binary":false,"changes":139,"status":"modified"},{"patch":"@@ -38,1 +38,1 @@\n-class CompiledStaticCall;\n+class CompiledDirectCall;\n@@ -380,1 +380,1 @@\n-  bool cleanup_inline_caches_impl(bool unloading_occurred, bool clean_all);\n+  void cleanup_inline_caches_impl(bool unloading_occurred, bool clean_all);\n@@ -389,1 +389,0 @@\n-  void clear_ic_callsites();\n@@ -394,2 +393,0 @@\n-  \/\/ Verify and count cached icholder relocations.\n-  int  verify_icholder_relocations();\n@@ -405,2 +402,0 @@\n-  virtual NativeCallWrapper* call_wrapper_at(address call) const = 0;\n-  virtual NativeCallWrapper* call_wrapper_before(address return_pc) const = 0;\n@@ -409,4 +404,0 @@\n-  virtual CompiledStaticCall* compiledStaticCall_at(Relocation* call_site) const = 0;\n-  virtual CompiledStaticCall* compiledStaticCall_at(address addr) const = 0;\n-  virtual CompiledStaticCall* compiledStaticCall_before(address addr) const = 0;\n-\n@@ -422,3 +413,0 @@\n- private:\n-  bool static clean_ic_if_metadata_is_dead(CompiledIC *ic);\n-\n@@ -431,1 +419,1 @@\n-  bool unload_nmethod_caches(bool class_unloading_occurred);\n+  void unload_nmethod_caches(bool class_unloading_occurred);\n","filename":"src\/hotspot\/share\/code\/compiledMethod.hpp","additions":3,"deletions":15,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -643,0 +643,1 @@\n+  _compiled_ic_data(nullptr),\n@@ -706,0 +707,2 @@\n+    finalize_relocations();\n+\n@@ -710,2 +713,0 @@\n-\n-    finalize_relocations();\n@@ -793,0 +794,1 @@\n+  _compiled_ic_data(nullptr),\n@@ -899,0 +901,2 @@\n+    finalize_relocations();\n+\n@@ -904,2 +908,0 @@\n-    finalize_relocations();\n-\n@@ -1157,0 +1159,2 @@\n+  GrowableArray<NativeMovConstReg*> virtual_call_data;\n+\n@@ -1161,1 +1165,5 @@\n-    if (iter.type() == relocInfo::post_call_nop_type) {\n+    if (iter.type() == relocInfo::virtual_call_type) {\n+      virtual_call_Relocation* r = iter.virtual_call_reloc();\n+      NativeMovConstReg* value = nativeMovConstReg_at(r->cached_value());\n+      virtual_call_data.append(value);\n+    } else if (iter.type() == relocInfo::post_call_nop_type) {\n@@ -1167,0 +1175,11 @@\n+\n+  if (virtual_call_data.length() > 0) {\n+    \/\/ We allocate a block of CompiledICData per nmethod so the GC can purge this faster.\n+    _compiled_ic_data = new CompiledICData[virtual_call_data.length()];\n+    CompiledICData* next_data = _compiled_ic_data;\n+\n+    for (NativeMovConstReg* value : virtual_call_data) {\n+      value->set_data((intptr_t)next_data);\n+      next_data++;\n+    }\n+  }\n@@ -1192,2 +1211,1 @@\n-      case relocInfo::virtual_call_type:\n-      case relocInfo::opt_virtual_call_type: {\n+      case relocInfo::virtual_call_type: {\n@@ -1203,2 +1221,3 @@\n-      case relocInfo::static_call_type: {\n-        CompiledStaticCall *csc = compiledStaticCall_at(iter.reloc());\n+      case relocInfo::static_call_type:\n+      case relocInfo::opt_virtual_call_type: {\n+        CompiledDirectCall *csc = CompiledDirectCall::at(iter.reloc());\n@@ -1231,2 +1250,1 @@\n-      case relocInfo::virtual_call_type:\n-      case relocInfo::opt_virtual_call_type: {\n+      case relocInfo::virtual_call_type: {\n@@ -1234,1 +1252,1 @@\n-        CodeBlob *cb = CodeCache::find_blob(ic->ic_destination());\n+        CodeBlob *cb = CodeCache::find_blob(ic->destination());\n@@ -1237,1 +1255,1 @@\n-        if( nm != nullptr ) {\n+        if (nm != nullptr) {\n@@ -1239,1 +1257,1 @@\n-          if (!nm->is_in_use() || (nm->method()->code() != nm)) {\n+          if (!nm->is_in_use() || nm->is_unloading()) {\n@@ -1245,3 +1263,4 @@\n-      case relocInfo::static_call_type: {\n-        CompiledStaticCall *csc = compiledStaticCall_at(iter.reloc());\n-        CodeBlob *cb = CodeCache::find_blob(csc->destination());\n+      case relocInfo::static_call_type:\n+      case relocInfo::opt_virtual_call_type: {\n+        CompiledDirectCall *cdc = CompiledDirectCall::at(iter.reloc());\n+        CodeBlob *cb = CodeCache::find_blob(cdc->destination());\n@@ -1250,1 +1269,1 @@\n-        if( nm != nullptr ) {\n+        if (nm != nullptr) {\n@@ -1252,2 +1271,2 @@\n-          if (!nm->is_in_use() || (nm->method()->code() != nm)) {\n-            assert(csc->is_clean(), \"IC should be clean\");\n+          if (!nm->is_in_use() || nm->is_unloading() || nm->method()->code() != nm) {\n+            assert(cdc->is_clean(), \"IC should be clean\");\n@@ -1417,3 +1436,1 @@\n-    \/\/ Already unlinked. It can be invoked twice because concurrent code cache\n-    \/\/ unloading might need to restart when inline cache cleaning fails due to\n-    \/\/ running out of ICStubs, which can only be refilled at safepoints\n+    \/\/ Already unlinked.\n@@ -1430,1 +1447,0 @@\n-  clear_ic_callsites();\n@@ -1475,0 +1491,2 @@\n+  delete[] _compiled_ic_data;\n+\n@@ -1478,1 +1496,0 @@\n-\n@@ -1616,10 +1633,1 @@\n-        if (ic->is_icholder_call()) {\n-          CompiledICHolder* cichk = ic->cached_icholder();\n-          f->do_metadata(cichk->holder_metadata());\n-          f->do_metadata(cichk->holder_klass());\n-        } else {\n-          Metadata* ic_oop = ic->cached_metadata();\n-          if (ic_oop != nullptr) {\n-            f->do_metadata(ic_oop);\n-          }\n-        }\n+        ic->metadata_do(f);\n@@ -1762,2 +1770,1 @@\n-    guarantee(unload_nmethod_caches(unloading_occurred),\n-              \"Should not need transition stubs\");\n+    unload_nmethod_caches(unloading_occurred);\n@@ -2296,1 +2303,1 @@\n-void nmethod::verify_interrupt_point(address call_site) {\n+void nmethod::verify_interrupt_point(address call_site, bool is_inline_cache) {\n@@ -2301,1 +2308,5 @@\n-      CompiledIC_at(this, call_site);\n+      if (is_inline_cache) {\n+        CompiledIC_at(this, call_site);\n+      } else {\n+        CompiledDirectCall::at(call_site);\n+      }\n@@ -2304,1 +2315,5 @@\n-      CompiledIC_at(this, call_site);\n+      if (is_inline_cache) {\n+        CompiledIC_at(this, call_site);\n+      } else {\n+        CompiledDirectCall::at(call_site);\n+      }\n@@ -2328,1 +2343,1 @@\n-        verify_interrupt_point(iter.addr());\n+        verify_interrupt_point(iter.addr(), true \/* is_inline_cache *\/);\n@@ -2332,1 +2347,1 @@\n-        verify_interrupt_point(iter.addr());\n+        verify_interrupt_point(iter.addr(), false \/* is_inline_cache *\/);\n@@ -2336,1 +2351,1 @@\n-        \/\/verify_interrupt_point(iter.addr());\n+        verify_interrupt_point(iter.addr(), false \/* is_inline_cache *\/);\n@@ -3280,69 +3295,0 @@\n-class DirectNativeCallWrapper: public NativeCallWrapper {\n-private:\n-  NativeCall* _call;\n-\n-public:\n-  DirectNativeCallWrapper(NativeCall* call) : _call(call) {}\n-\n-  virtual address destination() const { return _call->destination(); }\n-  virtual address instruction_address() const { return _call->instruction_address(); }\n-  virtual address next_instruction_address() const { return _call->next_instruction_address(); }\n-  virtual address return_address() const { return _call->return_address(); }\n-\n-  virtual address get_resolve_call_stub(bool is_optimized) const {\n-    if (is_optimized) {\n-      return SharedRuntime::get_resolve_opt_virtual_call_stub();\n-    }\n-    return SharedRuntime::get_resolve_virtual_call_stub();\n-  }\n-\n-  virtual void set_destination_mt_safe(address dest) {\n-    _call->set_destination_mt_safe(dest);\n-  }\n-\n-  virtual void set_to_interpreted(const methodHandle& method, CompiledICInfo& info) {\n-    CompiledDirectStaticCall* csc = CompiledDirectStaticCall::at(instruction_address());\n-    {\n-      csc->set_to_interpreted(method, info.entry());\n-    }\n-  }\n-\n-  virtual void verify() const {\n-    \/\/ make sure code pattern is actually a call imm32 instruction\n-    _call->verify();\n-    _call->verify_alignment();\n-  }\n-\n-  virtual void verify_resolve_call(address dest) const {\n-    CodeBlob* db = CodeCache::find_blob(dest);\n-    assert(db != nullptr && !db->is_adapter_blob(), \"must use stub!\");\n-  }\n-\n-  virtual bool is_call_to_interpreted(address dest) const {\n-    CodeBlob* cb = CodeCache::find_blob(_call->instruction_address());\n-    return cb->contains(dest);\n-  }\n-\n-  virtual bool is_safe_for_patching() const { return false; }\n-\n-  virtual NativeInstruction* get_load_instruction(virtual_call_Relocation* r) const {\n-    return nativeMovConstReg_at(r->cached_value());\n-  }\n-\n-  virtual void *get_data(NativeInstruction* instruction) const {\n-    return (void*)((NativeMovConstReg*) instruction)->data();\n-  }\n-\n-  virtual void set_data(NativeInstruction* instruction, intptr_t data) {\n-    ((NativeMovConstReg*) instruction)->set_data(data);\n-  }\n-};\n-\n-NativeCallWrapper* nmethod::call_wrapper_at(address call) const {\n-  return new DirectNativeCallWrapper((NativeCall*) call);\n-}\n-\n-NativeCallWrapper* nmethod::call_wrapper_before(address return_pc) const {\n-  return new DirectNativeCallWrapper(nativeCall_before(return_pc));\n-}\n-\n@@ -3357,12 +3303,0 @@\n-CompiledStaticCall* nmethod::compiledStaticCall_at(Relocation* call_site) const {\n-  return CompiledDirectStaticCall::at(call_site);\n-}\n-\n-CompiledStaticCall* nmethod::compiledStaticCall_at(address call_site) const {\n-  return CompiledDirectStaticCall::at(call_site);\n-}\n-\n-CompiledStaticCall* nmethod::compiledStaticCall_before(address return_addr) const {\n-  return CompiledDirectStaticCall::before(return_addr);\n-}\n-\n@@ -3382,2 +3316,1 @@\n-    case relocInfo::virtual_call_type:\n-    case relocInfo::opt_virtual_call_type: {\n+    case relocInfo::virtual_call_type: {\n@@ -3389,2 +3322,3 @@\n-      st->print_cr(\"Static call at \" INTPTR_FORMAT, p2i(iter.reloc()->addr()));\n-      CompiledDirectStaticCall::at(iter.reloc())->print();\n+    case relocInfo::opt_virtual_call_type:\n+      st->print_cr(\"Direct call at \" INTPTR_FORMAT, p2i(iter.reloc()->addr()));\n+      CompiledDirectCall::at(iter.reloc())->print();\n","filename":"src\/hotspot\/share\/code\/nmethod.cpp","additions":62,"deletions":128,"binary":false,"changes":190,"status":"modified"},{"patch":"@@ -31,0 +31,1 @@\n+class CompiledICData;\n@@ -203,0 +204,1 @@\n+  CompiledICData* _compiled_ic_data;\n@@ -614,1 +616,1 @@\n-  void verify_interrupt_point(address interrupt_point);\n+  void verify_interrupt_point(address interrupt_point, bool is_inline_cache);\n@@ -709,2 +711,0 @@\n-  NativeCallWrapper* call_wrapper_at(address call) const;\n-  NativeCallWrapper* call_wrapper_before(address return_pc) const;\n@@ -713,4 +713,0 @@\n-  virtual CompiledStaticCall* compiledStaticCall_at(Relocation* call_site) const;\n-  virtual CompiledStaticCall* compiledStaticCall_at(address addr) const;\n-  virtual CompiledStaticCall* compiledStaticCall_before(address addr) const;\n-\n","filename":"src\/hotspot\/share\/code\/nmethod.hpp","additions":3,"deletions":7,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -290,7 +290,0 @@\n-bool VtableStubs::is_icholder_entry(address pc) {\n-  assert(contains(pc), \"must contain all vtable blobs\");\n-  VtableStub* stub = (VtableStub*)(pc - VtableStub::entry_offset());\n-  \/\/ itable stubs use CompiledICHolder.\n-  return stub->is_itable_stub();\n-}\n-\n","filename":"src\/hotspot\/share\/code\/vtableStubs.cpp","additions":0,"deletions":7,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -110,1 +110,0 @@\n-  static bool        is_icholder_entry(address pc);                  \/\/ is the blob containing pc (which must be a vtable blob) an icholder?\n","filename":"src\/hotspot\/share\/code\/vtableStubs.hpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1999, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1999, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -585,1 +585,1 @@\n-void register_jfr_phasetype_serializer(CompilerType compiler_type) {\n+static void register_jfr_phasetype_serializer(CompilerType compiler_type) {\n","filename":"src\/hotspot\/share\/compiler\/compileBroker.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2016, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2016, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -249,1 +249,1 @@\n-void skip_leading_spaces(char*& line, int* total_bytes_read ) {\n+static void skip_leading_spaces(char*& line, int* total_bytes_read ) {\n","filename":"src\/hotspot\/share\/compiler\/methodMatcher.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2005, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2005, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -250,1 +250,1 @@\n-void\n+static void\n@@ -316,1 +316,1 @@\n-void\n+static void\n@@ -397,1 +397,1 @@\n-void\n+static void\n","filename":"src\/hotspot\/share\/gc\/parallel\/psParallelCompact.cpp","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -1089,7 +1089,0 @@\n-  if (resolved_method->is_continuation_native_intrinsic()\n-      && resolved_method->from_interpreted_entry() == nullptr) { \/\/ does a load_acquire\n-    methodHandle mh(THREAD, resolved_method);\n-    \/\/ Generate a compiled form of the enterSpecial intrinsic.\n-    AdapterHandlerLibrary::create_native_wrapper(mh);\n-  }\n-\n","filename":"src\/hotspot\/share\/interpreter\/linkResolver.cpp","additions":0,"deletions":7,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -899,2 +899,2 @@\n-  int size = static_call_stubs * CompiledStaticCall::to_interp_stub_size();\n-  size += trampoline_stubs * CompiledStaticCall::to_trampoline_stub_size();\n+  int size = static_call_stubs * CompiledDirectCall::to_interp_stub_size();\n+  size += trampoline_stubs * CompiledDirectCall::to_trampoline_stub_size();\n@@ -1247,1 +1247,1 @@\n-      if (CompiledStaticCall::emit_to_interp_stub(buffer, _instructions->start() + pc_offset) == nullptr) {\n+      if (CompiledDirectCall::emit_to_interp_stub(buffer, _instructions->start() + pc_offset) == nullptr) {\n","filename":"src\/hotspot\/share\/jvmci\/jvmciCodeInstaller.cpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -1383,1 +1383,1 @@\n-GrowableArray<Method*>* init_resolved_methods(jobjectArray methods, JVMCIEnv* JVMCIENV) {\n+static GrowableArray<Method*>* init_resolved_methods(jobjectArray methods, JVMCIEnv* JVMCIENV) {\n@@ -1402,1 +1402,1 @@\n-bool matches(jobjectArray methods, Method* method, GrowableArray<Method*>** resolved_methods_ref, Handle* matched_jvmci_method_ref, Thread* THREAD, JVMCIEnv* JVMCIENV) {\n+static bool matches(jobjectArray methods, Method* method, GrowableArray<Method*>** resolved_methods_ref, Handle* matched_jvmci_method_ref, Thread* THREAD, JVMCIEnv* JVMCIENV) {\n@@ -1423,1 +1423,1 @@\n-methodHandle resolve_interface_call(Klass* spec_klass, Symbol* name, Symbol* signature, JavaCallArguments* args, TRAPS) {\n+static methodHandle resolve_interface_call(Klass* spec_klass, Symbol* name, Symbol* signature, JavaCallArguments* args, TRAPS) {\n@@ -1438,1 +1438,1 @@\n-void resync_vframestream_to_compiled_frame(vframeStream& vfst, intptr_t* stack_pointer, int vframe_id, JavaThread* thread, TRAPS) {\n+static void resync_vframestream_to_compiled_frame(vframeStream& vfst, intptr_t* stack_pointer, int vframe_id, JavaThread* thread, TRAPS) {\n@@ -1461,1 +1461,1 @@\n-GrowableArray<ScopeValue*>* get_unallocated_objects_or_null(GrowableArray<ScopeValue*>* scope_objects) {\n+static GrowableArray<ScopeValue*>* get_unallocated_objects_or_null(GrowableArray<ScopeValue*>* scope_objects) {\n","filename":"src\/hotspot\/share\/jvmci\/jvmciCompilerToVM.cpp","additions":5,"deletions":5,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -27,0 +27,1 @@\n+#include \"code\/compiledIC.hpp\"\n@@ -147,0 +148,5 @@\n+  volatile_nonstatic_field(CompiledICData,     _speculated_method,                     Method*)                                      \\\n+  volatile_nonstatic_field(CompiledICData,     _speculated_klass,                      uintptr_t)                                    \\\n+  nonstatic_field(CompiledICData,              _itable_defc_klass,                     Klass*)                                       \\\n+  nonstatic_field(CompiledICData,              _itable_refc_klass,                     Klass*)                                       \\\n+                                                                                                                                     \\\n@@ -433,0 +439,1 @@\n+  declare_toplevel_type(CompiledICData)                                   \\\n","filename":"src\/hotspot\/share\/jvmci\/vmStructs_jvmci.cpp","additions":7,"deletions":0,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -243,10 +243,0 @@\n-\/\/ SpaceClosure is used for iterating over spaces\n-\n-class Space;\n-\n-class SpaceClosure : public StackObj {\n- public:\n-  \/\/ Called for each space\n-  virtual void do_space(Space* s) = 0;\n-};\n-\n","filename":"src\/hotspot\/share\/memory\/iterator.hpp","additions":0,"deletions":10,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -307,1 +307,1 @@\n-void initialize_basic_type_klass(Klass* k, TRAPS) {\n+static void initialize_basic_type_klass(Klass* k, TRAPS) {\n@@ -935,5 +935,5 @@\n-void initialize_known_method(LatestMethodCache* method_cache,\n-                             InstanceKlass* ik,\n-                             const char* method,\n-                             Symbol* signature,\n-                             bool is_static, TRAPS)\n+static void initialize_known_method(LatestMethodCache* method_cache,\n+                                    InstanceKlass* ik,\n+                                    const char* method,\n+                                    Symbol* signature,\n+                                    bool is_static, TRAPS)\n","filename":"src\/hotspot\/share\/memory\/universe.cpp","additions":7,"deletions":7,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1998, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1998, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -457,1 +457,1 @@\n-void log_adjust(const char* entry_type, Method* old_method, Method* new_method, bool* trace_name_printed) {\n+static void log_adjust(const char* entry_type, Method* old_method, Method* new_method, bool* trace_name_printed) {\n","filename":"src\/hotspot\/share\/oops\/cpCache.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -1458,1 +1458,1 @@\n-void visit_all_interfaces(Array<InstanceKlass*>* transitive_intf, InterfaceVisiterClosure *blk) {\n+static void visit_all_interfaces(Array<InstanceKlass*>* transitive_intf, InterfaceVisiterClosure *blk) {\n","filename":"src\/hotspot\/share\/oops\/klassVtable.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -76,0 +76,1 @@\n+#include \"runtime\/threads.hpp\"\n@@ -1294,1 +1295,0 @@\n-    \/\/ the entry points to this method will be set in set_code, called when first resolving this method\n@@ -1298,0 +1298,8 @@\n+    if (Continuations::enabled()) {\n+      assert(!Threads::is_vm_complete(), \"should only be called during vm init\");\n+      AdapterHandlerLibrary::create_native_wrapper(h_method);\n+      if (!h_method->has_compiled_code()) {\n+        THROW_MSG(vmSymbols::java_lang_OutOfMemoryError(), \"Initial size of CodeCache is too small\");\n+      }\n+      assert(_from_interpreted_entry == get_i2c_entry(), \"invariant\");\n+    }\n","filename":"src\/hotspot\/share\/oops\/method.cpp","additions":9,"deletions":1,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -184,3 +184,0 @@\n-\/\/      class CHeapObj\n-class   CompiledICHolder;\n-\n","filename":"src\/hotspot\/share\/oops\/oopsHierarchy.hpp","additions":0,"deletions":3,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -779,1 +779,1 @@\n-Node* rotate_shift(PhaseGVN* phase, Node* lshift, Node* rshift, int mask) {\n+static Node* rotate_shift(PhaseGVN* phase, Node* lshift, Node* rshift, int mask) {\n@@ -1095,1 +1095,1 @@\n-Node* build_min_max_int(Node* a, Node* b, bool is_max) {\n+static Node* build_min_max_int(Node* a, Node* b, bool is_max) {\n@@ -1327,1 +1327,1 @@\n-Node* fold_subI_no_underflow_pattern(Node* n, PhaseGVN* phase) {\n+static Node* fold_subI_no_underflow_pattern(Node* n, PhaseGVN* phase) {\n","filename":"src\/hotspot\/share\/opto\/addnode.cpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1998, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1998, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -54,1 +54,1 @@\n-void print_trace_type_profile(outputStream* out, int depth, ciKlass* prof_klass, int site_count, int receiver_count) {\n+static void print_trace_type_profile(outputStream* out, int depth, ciKlass* prof_klass, int site_count, int receiver_count) {\n@@ -61,2 +61,2 @@\n-void trace_type_profile(Compile* C, ciMethod* method, int depth, int bci, ciMethod* prof_method,\n-                        ciKlass* prof_klass, int site_count, int receiver_count) {\n+static void trace_type_profile(Compile* C, ciMethod* method, int depth, int bci, ciMethod* prof_method,\n+                               ciKlass* prof_klass, int site_count, int receiver_count) {\n","filename":"src\/hotspot\/share\/opto\/doCall.cpp","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -46,0 +46,1 @@\n+class VSharedData;\n@@ -237,3 +238,0 @@\n-  \/\/ Cached CountedLoopEndNode of pre loop for main loops\n-  CountedLoopEndNode* _pre_loop_end;\n-\n@@ -244,1 +242,1 @@\n-      _slp_maximum_unroll_factor(0), _pre_loop_end(nullptr) {\n+      _slp_maximum_unroll_factor(0) {\n@@ -336,3 +334,0 @@\n-  CountedLoopNode* pre_loop_head() const;\n-  CountedLoopEndNode* pre_loop_end();\n-  void set_pre_loop_end(CountedLoopEndNode* pre_loop_end);\n@@ -1109,0 +1104,1 @@\n+    _loop_or_ctrl(igvn.C->comp_arena()),\n@@ -1123,0 +1119,1 @@\n+    _loop_or_ctrl(igvn.C->comp_arena()),\n@@ -1443,0 +1440,8 @@\n+  \/\/ AutoVectorize the loop: replace scalar ops with vector ops.\n+  enum AutoVectorizeStatus {\n+    Impossible,      \/\/ This loop has the wrong shape to even try vectorization.\n+    Success,         \/\/ We just successfully vectorized the loop.\n+    TriedAndFailed,  \/\/ We tried to vectorize, but failed.\n+  };\n+  AutoVectorizeStatus auto_vectorize(IdealLoopTree* lpt, VSharedData &vshared);\n+\n","filename":"src\/hotspot\/share\/opto\/loopnode.hpp","additions":12,"deletions":7,"binary":false,"changes":19,"status":"modified"},{"patch":"@@ -45,0 +45,1 @@\n+#include \"opto\/superword.hpp\"\n@@ -4392,0 +4393,30 @@\n+\/\/ AutoVectorize the loop: replace scalar ops with vector ops.\n+PhaseIdealLoop::AutoVectorizeStatus\n+PhaseIdealLoop::auto_vectorize(IdealLoopTree* lpt, VSharedData &vshared) {\n+  \/\/ Counted loop only\n+  if (!lpt->is_counted()) {\n+    return AutoVectorizeStatus::Impossible;\n+  }\n+\n+  \/\/ Main-loop only\n+  CountedLoopNode* cl = lpt->_head->as_CountedLoop();\n+  if (!cl->is_main_loop()) {\n+    return AutoVectorizeStatus::Impossible;\n+  }\n+\n+  VLoop vloop(lpt, false);\n+  if (!vloop.check_preconditions()) {\n+    return AutoVectorizeStatus::TriedAndFailed;\n+  }\n+\n+  \/\/ Ensure the shared data is cleared before each use\n+  vshared.clear();\n+\n+  SuperWord sw(vloop, vshared);\n+  if (!sw.transform_loop()) {\n+    return AutoVectorizeStatus::TriedAndFailed;\n+  }\n+\n+  return AutoVectorizeStatus::Success;\n+}\n+\n","filename":"src\/hotspot\/share\/opto\/loopopts.cpp","additions":31,"deletions":0,"binary":false,"changes":31,"status":"modified"},{"patch":"@@ -161,1 +161,2 @@\n-  Node *alloc_mem = alloc->in(TypeFunc::Memory);\n+  Node *alloc_mem = alloc->as_Allocate()->proj_out_or_null(TypeFunc::Memory, \/*io_use:*\/false);\n+  assert(alloc_mem != nullptr, \"Allocation without a memory projection.\");\n@@ -373,1 +374,2 @@\n-  Node *alloc_mem = alloc->in(TypeFunc::Memory);\n+  Node *alloc_mem = alloc->proj_out_or_null(TypeFunc::Memory, \/*io_use:*\/false);\n+  assert(alloc_mem != nullptr, \"Allocation without a memory projection.\");\n@@ -469,1 +471,2 @@\n-  Node *alloc_mem = alloc->in(TypeFunc::Memory);\n+  Node *alloc_mem = alloc->proj_out_or_null(TypeFunc::Memory, \/*io_use:*\/false);\n+  assert(alloc_mem != nullptr, \"Allocation without a memory projection.\");\n","filename":"src\/hotspot\/share\/opto\/macro.cpp","additions":6,"deletions":3,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -1642,1 +1642,1 @@\n-Node* find_node_by_idx(Node* start, uint idx, bool traverse_output, bool only_ctrl) {\n+static Node* find_node_by_idx(Node* start, uint idx, bool traverse_output, bool only_ctrl) {\n@@ -1658,1 +1658,1 @@\n-int node_idx_cmp(const Node** n1, const Node** n2) {\n+static int node_idx_cmp(const Node** n1, const Node** n2) {\n@@ -1662,1 +1662,1 @@\n-void find_nodes_by_name(Node* start, const char* name) {\n+static void find_nodes_by_name(Node* start, const char* name) {\n@@ -1677,1 +1677,1 @@\n-void find_nodes_by_dump(Node* start, const char* pattern) {\n+static void find_nodes_by_dump(Node* start, const char* pattern) {\n","filename":"src\/hotspot\/share\/opto\/node.cpp","additions":5,"deletions":5,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -556,2 +556,2 @@\n-            stub_size  += CompiledStaticCall::to_interp_stub_size();\n-            reloc_size += CompiledStaticCall::reloc_to_interp_stub();\n+            stub_size  += CompiledDirectCall::to_interp_stub_size();\n+            reloc_size += CompiledDirectCall::reloc_to_interp_stub();\n@@ -3517,0 +3517,7 @@\n+      if (!target->is_static()) {\n+        \/\/ The UEP of an nmethod ensures that the VEP is padded. However, the padding of the UEP is placed\n+        \/\/ before the inline cache check, so we don't have to execute any nop instructions when dispatching\n+        \/\/ through the UEP, yet we can ensure that the VEP is aligned appropriately.\n+        \/\/ TODO 8325106 Check this\n+        \/\/ _code_offsets.set_value(CodeOffsets::Entry, _first_block_size - MacroAssembler::ic_check_size());\n+      }\n","filename":"src\/hotspot\/share\/opto\/output.cpp","additions":9,"deletions":2,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -31,1 +31,0 @@\n-#include \"code\/icBuffer.hpp\"\n","filename":"src\/hotspot\/share\/opto\/runtime.cpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2003, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2003, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -51,0 +51,1 @@\n+#include \"oops\/method.hpp\"\n@@ -67,0 +68,1 @@\n+#include \"utilities\/macros.hpp\"\n@@ -1191,0 +1193,1 @@\n+      JFR_ONLY(k_new_method->copy_trace_flags(*k_old_method->trace_flags_addr());)\n@@ -1843,0 +1846,6 @@\n+  \/\/ ensure merged constant pool size does not overflow u2\n+  if (merge_cp_length > 0xFFFF) {\n+    log_warning(redefine, class, constantpool)(\"Merged constant pool overflow: %d entries\", merge_cp_length);\n+    return JVMTI_ERROR_INTERNAL;\n+  }\n+\n","filename":"src\/hotspot\/share\/prims\/jvmtiRedefineClasses.cpp","additions":10,"deletions":1,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -91,0 +91,1 @@\n+#include \"runtime\/lockStack.hpp\"\n@@ -1853,0 +1854,8 @@\n+WB_ENTRY(jint, WB_getLockStackCapacity(JNIEnv* env))\n+  return (jint) LockStack::CAPACITY;\n+WB_END\n+\n+WB_ENTRY(jboolean, WB_supportsRecursiveLightweightLocking(JNIEnv* env))\n+  return (jboolean) VM_Version::supports_recursive_lightweight_locking();\n+WB_END\n+\n@@ -2922,0 +2931,2 @@\n+  {CC\"getLockStackCapacity\", CC\"()I\",                 (void*)&WB_getLockStackCapacity },\n+  {CC\"supportsRecursiveLightweightLocking\", CC\"()Z\",  (void*)&WB_supportsRecursiveLightweightLocking },\n","filename":"src\/hotspot\/share\/prims\/whitebox.cpp","additions":11,"deletions":0,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -1368,1 +1368,1 @@\n-void set_object_alignment() {\n+static void set_object_alignment() {\n@@ -2024,0 +2024,1 @@\n+#if !INCLUDE_JVMTI\n@@ -2028,1 +2029,1 @@\n-bool valid_jdwp_agent(char *name, bool is_path) {\n+static bool valid_jdwp_agent(char *name, bool is_path) {\n@@ -2068,0 +2069,1 @@\n+#endif\n","filename":"src\/hotspot\/share\/runtime\/arguments.cpp","additions":4,"deletions":2,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -1509,1 +1509,1 @@\n-int compare(ReassignedField* left, ReassignedField* right) {\n+static int compare(ReassignedField* left, ReassignedField* right) {\n","filename":"src\/hotspot\/share\/runtime\/deoptimization.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -904,3 +904,0 @@\n-  develop(bool, TraceInlineCacheClearing, false,                            \\\n-          \"Trace clearing of inline caches in nmethods\")                    \\\n-                                                                            \\\n@@ -925,6 +922,0 @@\n-  develop(bool, TraceICBuffer, false,                                       \\\n-          \"Trace usage of IC buffer\")                                       \\\n-                                                                            \\\n-  develop(bool, TraceCompiledIC, false,                                     \\\n-          \"Trace changes of compiled IC\")                                   \\\n-                                                                            \\\n","filename":"src\/hotspot\/share\/runtime\/globals.hpp","additions":0,"deletions":9,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -28,1 +28,0 @@\n-#include \"code\/icBuffer.hpp\"\n@@ -86,1 +85,0 @@\n-void InlineCacheBuffer_init();\n@@ -166,1 +164,0 @@\n-  InlineCacheBuffer_init();\n","filename":"src\/hotspot\/share\/runtime\/init.cpp","additions":0,"deletions":3,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -267,1 +267,1 @@\n-bool jvmci_counters_include(JavaThread* thread) {\n+static bool jvmci_counters_include(JavaThread* thread) {\n@@ -286,1 +286,1 @@\n-jlong* resize_counters_array(jlong* old_counters, int current_size, int new_size) {\n+static jlong* resize_counters_array(jlong* old_counters, int current_size, int new_size) {\n@@ -1460,1 +1460,1 @@\n-const char* _get_thread_state_name(JavaThreadState _thread_state) {\n+static const char* _get_thread_state_name(JavaThreadState _thread_state) {\n","filename":"src\/hotspot\/share\/runtime\/javaThread.cpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -30,1 +30,0 @@\n-#include \"code\/icBuffer.hpp\"\n@@ -517,1 +516,0 @@\n-  if (!InlineCacheBuffer::is_empty()) return true;\n@@ -562,4 +560,0 @@\n-    if (InlineCacheBuffer::needs_update_inline_caches()) {\n-      workers++;\n-    }\n-\n@@ -603,5 +597,0 @@\n-    if (_subtasks.try_claim_task(SafepointSynchronize::SAFEPOINT_CLEANUP_UPDATE_INLINE_CACHES)) {\n-      Tracer t(\"updating inline caches\");\n-      InlineCacheBuffer::update_inline_caches();\n-    }\n-\n@@ -637,2 +626,0 @@\n-  assert(InlineCacheBuffer::is_empty(), \"should have cleaned up ICBuffer\");\n-\n","filename":"src\/hotspot\/share\/runtime\/safepoint.cpp","additions":0,"deletions":13,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -32,1 +32,0 @@\n-#include \"code\/icBuffer.hpp\"\n@@ -53,1 +52,0 @@\n-#include \"oops\/compiledICHolder.inline.hpp\"\n@@ -1331,124 +1329,0 @@\n-  methodHandle callee_method;\n-  callee_method = resolve_sub_helper(is_virtual, is_optimized, caller_is_c1, THREAD);\n-  if (JvmtiExport::can_hotswap_or_post_breakpoint()) {\n-    int retry_count = 0;\n-    while (!HAS_PENDING_EXCEPTION && callee_method->is_old() &&\n-           callee_method->method_holder() != vmClasses::Object_klass()) {\n-      \/\/ If has a pending exception then there is no need to re-try to\n-      \/\/ resolve this method.\n-      \/\/ If the method has been redefined, we need to try again.\n-      \/\/ Hack: we have no way to update the vtables of arrays, so don't\n-      \/\/ require that java.lang.Object has been updated.\n-\n-      \/\/ It is very unlikely that method is redefined more than 100 times\n-      \/\/ in the middle of resolve. If it is looping here more than 100 times\n-      \/\/ means then there could be a bug here.\n-      guarantee((retry_count++ < 100),\n-                \"Could not resolve to latest version of redefined method\");\n-      \/\/ method is redefined in the middle of resolve so re-try.\n-      callee_method = resolve_sub_helper(is_virtual, is_optimized, caller_is_c1, THREAD);\n-    }\n-  }\n-  return callee_method;\n-}\n-\n-\/\/ This fails if resolution required refilling of IC stubs\n-bool SharedRuntime::resolve_sub_helper_internal(methodHandle callee_method, const frame& caller_frame,\n-                                                CompiledMethod* caller_nm, bool is_virtual, bool is_optimized, bool& caller_is_c1,\n-                                                Handle receiver, CallInfo& call_info, Bytecodes::Code invoke_code, TRAPS) {\n-  StaticCallInfo static_call_info;\n-  CompiledICInfo virtual_call_info;\n-\n-  \/\/ Make sure the callee nmethod does not get deoptimized and removed before\n-  \/\/ we are done patching the code.\n-  CompiledMethod* callee = callee_method->code();\n-\n-  if (callee != nullptr) {\n-    assert(callee->is_compiled(), \"must be nmethod for patching\");\n-  }\n-\n-  if (callee != nullptr && !callee->is_in_use()) {\n-    \/\/ Patch call site to C2I adapter if callee nmethod is deoptimized or unloaded.\n-    callee = nullptr;\n-  }\n-#ifdef ASSERT\n-  address dest_entry_point = callee == nullptr ? 0 : callee->entry_point(); \/\/ used below\n-#endif\n-\n-  bool is_nmethod = caller_nm->is_nmethod();\n-\n-  if (is_virtual) {\n-    Klass* receiver_klass = nullptr;\n-    if (!caller_is_c1 && callee_method->is_scalarized_arg(0)) {\n-      \/\/ If the receiver is an inline type that is passed as fields, no oop is available\n-      receiver_klass = callee_method->method_holder();\n-    } else {\n-      assert(receiver.not_null() || invoke_code == Bytecodes::_invokehandle, \"sanity check\");\n-      receiver_klass = invoke_code == Bytecodes::_invokehandle ? nullptr : receiver->klass();\n-    }\n-    bool static_bound = call_info.resolved_method()->can_be_statically_bound();\n-    CompiledIC::compute_monomorphic_entry(callee_method, receiver_klass,\n-                     is_optimized, static_bound, is_nmethod, caller_is_c1, virtual_call_info,\n-                     CHECK_false);\n-  } else {\n-    \/\/ static call\n-    CompiledStaticCall::compute_entry(callee_method, caller_nm, static_call_info);\n-  }\n-\n-  JFR_ONLY(bool patched_caller = false;)\n-  \/\/ grab lock, check for deoptimization and potentially patch caller\n-  {\n-    CompiledICLocker ml(caller_nm);\n-\n-    \/\/ Lock blocks for safepoint during which both nmethods can change state.\n-\n-    \/\/ Now that we are ready to patch if the Method* was redefined then\n-    \/\/ don't update call site and let the caller retry.\n-    \/\/ Don't update call site if callee nmethod was unloaded or deoptimized.\n-    \/\/ Don't update call site if callee nmethod was replaced by an other nmethod\n-    \/\/ which may happen when multiply alive nmethod (tiered compilation)\n-    \/\/ will be supported.\n-    if (!callee_method->is_old() &&\n-        (callee == nullptr || (callee->is_in_use() && callee_method->code() == callee))) {\n-      NoSafepointVerifier nsv;\n-#ifdef ASSERT\n-      \/\/ We must not try to patch to jump to an already unloaded method.\n-      if (dest_entry_point != 0) {\n-        CodeBlob* cb = CodeCache::find_blob(dest_entry_point);\n-        assert((cb != nullptr) && cb->is_compiled() && (((CompiledMethod*)cb) == callee),\n-               \"should not call unloaded nmethod\");\n-      }\n-#endif\n-      if (is_virtual) {\n-        CompiledIC* inline_cache = CompiledIC_before(caller_nm, caller_frame.pc());\n-        if (inline_cache->is_clean()) {\n-          if (!inline_cache->set_to_monomorphic(virtual_call_info)) {\n-            return false;\n-          }\n-          JFR_ONLY(patched_caller = true;)\n-        }\n-      } else {\n-        if (VM_Version::supports_fast_class_init_checks() &&\n-            invoke_code == Bytecodes::_invokestatic &&\n-            callee_method->needs_clinit_barrier() &&\n-            callee != nullptr && callee->is_compiled_by_jvmci()) {\n-          return true; \/\/ skip patching for JVMCI\n-        }\n-        CompiledStaticCall* ssc = caller_nm->compiledStaticCall_before(caller_frame.pc());\n-        if (is_nmethod && caller_nm->method()->is_continuation_enter_intrinsic()) {\n-          ssc->compute_entry_for_continuation_entry(callee_method, static_call_info);\n-        }\n-        if (ssc->is_clean()) {\n-          ssc->set(static_call_info);\n-          JFR_ONLY(patched_caller = true;)\n-        }\n-      }\n-    }\n-  } \/\/ unlock CompiledICLocker\n-  JFR_ONLY(if (patched_caller) Jfr::on_backpatching(callee_method(), THREAD);)\n-  return true;\n-}\n-\n-\/\/ Resolves a call.  The compilers generate code for calls that go here\n-\/\/ and are patched with the real destination of the call.\n-methodHandle SharedRuntime::resolve_sub_helper(bool is_virtual, bool is_optimized, bool& caller_is_c1, TRAPS) {\n@@ -1465,1 +1339,1 @@\n-  CompiledMethod* caller_nm = caller_cb->as_compiled_method_or_null();\n+  CompiledMethod* caller_nm = caller_cb->as_compiled_method();\n@@ -1473,0 +1347,3 @@\n+\n+  NoSafepointVerifier nsv;\n+\n@@ -1496,1 +1373,1 @@\n-    tty->print(\"resolving %s%s (%s) call to\",\n+    tty->print(\"resolving %s%s (%s) call%s to\",\n@@ -1498,1 +1375,1 @@\n-               Bytecodes::name(invoke_code));\n+               Bytecodes::name(invoke_code), (caller_is_c1) ? \" from C1\" : \"\");\n@@ -1521,0 +1398,1 @@\n+\n@@ -1525,24 +1403,16 @@\n-  \/\/ TODO detune for now but we might need it again\n-\/\/  assert(!callee_method->is_compiled_lambda_form() ||\n-\/\/         caller_nm->is_method_handle_return(caller_frame.pc()), \"must be MH call site\");\n-\n-  \/\/ Compute entry points. This might require generation of C2I converter\n-  \/\/ frames, so we cannot be holding any locks here. Furthermore, the\n-  \/\/ computation of the entry points is independent of patching the call.  We\n-  \/\/ always return the entry-point, but we only patch the stub if the call has\n-  \/\/ not been deoptimized.  Return values: For a virtual call this is an\n-  \/\/ (cached_oop, destination address) pair. For a static call\/optimized\n-  \/\/ virtual this is just a destination address.\n-\n-  \/\/ Patching IC caches may fail if we run out if transition stubs.\n-  \/\/ We refill the ic stubs then and try again.\n-  for (;;) {\n-    ICRefillVerifier ic_refill_verifier;\n-    bool successful = resolve_sub_helper_internal(callee_method, caller_frame, caller_nm,\n-                                                  is_virtual, is_optimized, caller_is_c1, receiver,\n-                                                  call_info, invoke_code, CHECK_(methodHandle()));\n-    if (successful) {\n-      return callee_method;\n-    } else {\n-      InlineCacheBuffer::refill_ic_stubs();\n-    }\n+\n+  \/\/ Compute entry points. The computation of the entry points is independent of\n+  \/\/ patching the call.\n+\n+  \/\/ Make sure the callee nmethod does not get deoptimized and removed before\n+  \/\/ we are done patching the code.\n+\n+\n+  CompiledICLocker ml(caller_nm);\n+  if (is_virtual && !is_optimized) {\n+    CompiledIC* inline_cache = CompiledIC_before(caller_nm, caller_frame.pc());\n+    inline_cache->update(&call_info, receiver->klass(), caller_is_c1);\n+  } else {\n+    \/\/ Callsite is a direct call - set it to the destination method\n+    CompiledDirectCall* callsite = CompiledDirectCall::before(caller_frame.pc());\n+    callsite->set(callee_method, caller_is_c1);\n@@ -1551,0 +1421,1 @@\n+  return callee_method;\n@@ -1553,1 +1424,0 @@\n-\n@@ -1747,46 +1617,0 @@\n-\/\/ The handle_ic_miss_helper_internal function returns false if it failed due\n-\/\/ to either running out of vtable stubs or ic stubs due to IC transitions\n-\/\/ to transitional states. The needs_ic_stub_refill value will be set if\n-\/\/ the failure was due to running out of IC stubs, in which case handle_ic_miss_helper\n-\/\/ refills the IC stubs and tries again.\n-bool SharedRuntime::handle_ic_miss_helper_internal(Handle receiver, CompiledMethod* caller_nm,\n-                                                   const frame& caller_frame, methodHandle callee_method,\n-                                                   Bytecodes::Code bc, CallInfo& call_info,\n-                                                   bool& needs_ic_stub_refill, bool& is_optimized, bool caller_is_c1, TRAPS) {\n-  CompiledICLocker ml(caller_nm);\n-  CompiledIC* inline_cache = CompiledIC_before(caller_nm, caller_frame.pc());\n-  bool should_be_mono = false;\n-  if (inline_cache->is_optimized()) {\n-    if (TraceCallFixup) {\n-      ResourceMark rm(THREAD);\n-      tty->print(\"OPTIMIZED IC miss (%s) call to\", Bytecodes::name(bc));\n-      callee_method->print_short_name(tty);\n-      tty->print_cr(\" code: \" INTPTR_FORMAT, p2i(callee_method->code()));\n-    }\n-    is_optimized = true;\n-    should_be_mono = true;\n-  } else if (inline_cache->is_icholder_call()) {\n-    CompiledICHolder* ic_oop = inline_cache->cached_icholder();\n-    if (ic_oop != nullptr) {\n-      if (!ic_oop->is_loader_alive()) {\n-        \/\/ Deferred IC cleaning due to concurrent class unloading\n-        if (!inline_cache->set_to_clean()) {\n-          needs_ic_stub_refill = true;\n-          return false;\n-        }\n-      } else if (receiver()->klass() == ic_oop->holder_klass()) {\n-        \/\/ This isn't a real miss. We must have seen that compiled code\n-        \/\/ is now available and we want the call site converted to a\n-        \/\/ monomorphic compiled call site.\n-        \/\/ We can't assert for callee_method->code() != nullptr because it\n-        \/\/ could have been deoptimized in the meantime\n-        if (TraceCallFixup) {\n-          ResourceMark rm(THREAD);\n-          tty->print(\"FALSE IC miss (%s) converting to compiled call to\", Bytecodes::name(bc));\n-          callee_method->print_short_name(tty);\n-          tty->print_cr(\" code: \" INTPTR_FORMAT, p2i(callee_method->code()));\n-        }\n-        should_be_mono = true;\n-      }\n-    }\n-  }\n@@ -1794,34 +1618,0 @@\n-  if (should_be_mono) {\n-    \/\/ We have a path that was monomorphic but was going interpreted\n-    \/\/ and now we have (or had) a compiled entry. We correct the IC\n-    \/\/ by using a new icBuffer.\n-    CompiledICInfo info;\n-    Klass* receiver_klass = receiver()->klass();\n-    inline_cache->compute_monomorphic_entry(callee_method,\n-                                            receiver_klass,\n-                                            inline_cache->is_optimized(),\n-                                            false, caller_nm->is_nmethod(),\n-                                            caller_is_c1,\n-                                            info, CHECK_false);\n-    if (!inline_cache->set_to_monomorphic(info)) {\n-      needs_ic_stub_refill = true;\n-      return false;\n-    }\n-  } else if (!inline_cache->is_megamorphic() && !inline_cache->is_clean()) {\n-    \/\/ Potential change to megamorphic\n-\n-    bool successful = inline_cache->set_to_megamorphic(&call_info, bc, needs_ic_stub_refill, caller_is_c1, CHECK_false);\n-    if (needs_ic_stub_refill) {\n-      return false;\n-    }\n-    if (!successful) {\n-      if (!inline_cache->set_to_clean()) {\n-        needs_ic_stub_refill = true;\n-        return false;\n-      }\n-    }\n-  } else {\n-    \/\/ Either clean or megamorphic\n-  }\n-  return true;\n-}\n@@ -1838,28 +1628,0 @@\n-  \/\/ Compiler1 can produce virtual call sites that can actually be statically bound\n-  \/\/ If we fell thru to below we would think that the site was going megamorphic\n-  \/\/ when in fact the site can never miss. Worse because we'd think it was megamorphic\n-  \/\/ we'd try and do a vtable dispatch however methods that can be statically bound\n-  \/\/ don't have vtable entries (vtable_index < 0) and we'd blow up. So we force a\n-  \/\/ reresolution of the  call site (as if we did a handle_wrong_method and not an\n-  \/\/ plain ic_miss) and the site will be converted to an optimized virtual call site\n-  \/\/ never to miss again. I don't believe C2 will produce code like this but if it\n-  \/\/ did this would still be the correct thing to do for it too, hence no ifdef.\n-  \/\/\n-  if (call_info.resolved_method()->can_be_statically_bound()) {\n-    bool is_static_call = false;\n-    methodHandle callee_method = SharedRuntime::reresolve_call_site(is_static_call, is_optimized, caller_is_c1, CHECK_(methodHandle()));\n-    assert(!is_static_call, \"IC miss at static call?\");\n-    if (TraceCallFixup) {\n-      RegisterMap reg_map(current,\n-                          RegisterMap::UpdateMap::skip,\n-                          RegisterMap::ProcessFrames::include,\n-                          RegisterMap::WalkContinuation::skip);\n-      frame caller_frame = current->last_frame().sender(&reg_map);\n-      ResourceMark rm(current);\n-      tty->print(\"converting IC miss to reresolve (%s) call to\", Bytecodes::name(bc));\n-      callee_method->print_short_name(tty);\n-      tty->print_cr(\" from pc: \" INTPTR_FORMAT, p2i(caller_frame.pc()));\n-      tty->print_cr(\" code: \" INTPTR_FORMAT, p2i(callee_method->code()));\n-    }\n-    return callee_method;\n-  }\n@@ -1875,1 +1637,1 @@\n-    tty->print(\"IC miss (%s) call to\", Bytecodes::name(bc));\n+    tty->print(\"IC miss (%s) call%s to\", Bytecodes::name(bc), (caller_is_c1) ? \" from C1\" : \"\");\n@@ -1900,3 +1662,0 @@\n-  \/\/ Transitioning IC caches may require transition stubs. If we run out\n-  \/\/ of transition stubs, we have to drop locks and perform a safepoint\n-  \/\/ that refills them.\n@@ -1915,27 +1674,4 @@\n-  for (;;) {\n-    ICRefillVerifier ic_refill_verifier;\n-    bool needs_ic_stub_refill = false;\n-    bool successful = handle_ic_miss_helper_internal(receiver, caller_nm, caller_frame, callee_method,\n-                                                     bc, call_info, needs_ic_stub_refill, is_optimized, caller_is_c1, CHECK_(methodHandle()));\n-    if (successful || !needs_ic_stub_refill) {\n-      return callee_method;\n-    } else {\n-      InlineCacheBuffer::refill_ic_stubs();\n-    }\n-  }\n-}\n-\n-static bool clear_ic_at_addr(CompiledMethod* caller_nm, address call_addr, bool is_static_call) {\n-  if (is_static_call) {\n-    CompiledStaticCall* ssc = caller_nm->compiledStaticCall_at(call_addr);\n-    if (!ssc->is_clean()) {\n-      return ssc->set_to_clean();\n-    }\n-  } else {\n-    \/\/ compiled, dispatched call (which used to call an interpreted method)\n-    CompiledIC* inline_cache = CompiledIC_at(caller_nm, call_addr);\n-    if (!inline_cache->is_clean()) {\n-      return inline_cache->set_to_clean();\n-    }\n-  }\n-  return true;\n+  CompiledIC* inline_cache = CompiledIC_before(caller_nm, caller_frame.pc());\n+  inline_cache->update(&call_info, receiver()->klass(), caller_is_c1);\n+\n+  return callee_method;\n@@ -1974,1 +1710,0 @@\n-    \/\/ Check for static or virtual call\n@@ -1995,8 +1730,2 @@\n-    address call_addr = nullptr;\n-    {\n-      \/\/ Get call instruction under lock because another thread may be\n-      \/\/ busy patching it.\n-      CompiledICLocker ml(caller_nm);\n-      \/\/ Location of call instruction\n-      call_addr = caller_nm->call_instruction_address(pc);\n-    }\n+    CompiledICLocker ml(caller_nm);\n+    address call_addr = caller_nm->call_instruction_address(pc);\n@@ -2004,2 +1733,0 @@\n-    \/\/ Check relocations for the matching call to 1) avoid false positives,\n-    \/\/ and 2) determine the type.\n@@ -2017,3 +1744,1 @@\n-\n-          case relocInfo::virtual_call_type:\n-          case relocInfo::opt_virtual_call_type:\n+          case relocInfo::opt_virtual_call_type: {\n@@ -2021,15 +1746,8 @@\n-            \/\/ Cleaning the inline cache will force a new resolve. This is more robust\n-            \/\/ than directly setting it to the new destination, since resolving of calls\n-            \/\/ is always done through the same code path. (experience shows that it\n-            \/\/ leads to very hard to track down bugs, if an inline cache gets updated\n-            \/\/ to a wrong method). It should not be performance critical, since the\n-            \/\/ resolve is only done once.\n-            guarantee(iter.addr() == call_addr, \"must find call\");\n-            for (;;) {\n-              ICRefillVerifier ic_refill_verifier;\n-              if (!clear_ic_at_addr(caller_nm, call_addr, is_static_call)) {\n-                InlineCacheBuffer::refill_ic_stubs();\n-              } else {\n-                break;\n-              }\n-            }\n+            CompiledDirectCall* cdc = CompiledDirectCall::at(call_addr);\n+            cdc->set_to_clean();\n+            break;\n+          }\n+          case relocInfo::virtual_call_type: {\n+            \/\/ compiled, dispatched call (which used to call an interpreted method)\n+            CompiledIC* inline_cache = CompiledIC_at(caller_nm, call_addr);\n+            inline_cache->set_to_clean();\n@@ -2037,0 +1755,1 @@\n+          }\n@@ -2051,1 +1770,1 @@\n-    tty->print(\"handle_wrong_method reresolving call to\");\n+    tty->print(\"handle_wrong_method reresolving call%s to\", (caller_is_c1) ? \" from C1\" : \"\");\n@@ -2099,31 +1818,0 @@\n-bool SharedRuntime::should_fixup_call_destination(address destination, address entry_point, address caller_pc, Method* moop, CodeBlob* cb) {\n-  if (destination != entry_point) {\n-    CodeBlob* callee = CodeCache::find_blob(destination);\n-    \/\/ callee == cb seems weird. It means calling interpreter thru stub.\n-    if (callee != nullptr && (callee == cb || callee->is_adapter_blob())) {\n-      \/\/ static call or optimized virtual\n-      if (TraceCallFixup) {\n-        tty->print(\"fixup callsite           at \" INTPTR_FORMAT \" to compiled code for\", p2i(caller_pc));\n-        moop->print_short_name(tty);\n-        tty->print_cr(\" to \" INTPTR_FORMAT, p2i(entry_point));\n-      }\n-      return true;\n-    } else {\n-      if (TraceCallFixup) {\n-        tty->print(\"failed to fixup callsite at \" INTPTR_FORMAT \" to compiled code for\", p2i(caller_pc));\n-        moop->print_short_name(tty);\n-        tty->print_cr(\" to \" INTPTR_FORMAT, p2i(entry_point));\n-      }\n-      \/\/ assert is too strong could also be resolve destinations.\n-      \/\/ assert(InlineCacheBuffer::contains(destination) || VtableStubs::contains(destination), \"must be\");\n-    }\n-  } else {\n-    if (TraceCallFixup) {\n-      tty->print(\"already patched callsite at \" INTPTR_FORMAT \" to compiled code for\", p2i(caller_pc));\n-      moop->print_short_name(tty);\n-      tty->print_cr(\" to \" INTPTR_FORMAT, p2i(entry_point));\n-    }\n-  }\n-  return false;\n-}\n-\n@@ -2137,2 +1825,0 @@\n-  Method* moop(method);\n-\n@@ -2154,1 +1840,1 @@\n-  CompiledMethod* callee = moop->code();\n+  CompiledMethod* callee = method->code();\n@@ -2163,1 +1849,1 @@\n-  if (cb == nullptr || !cb->is_compiled() || callee->is_unloading()) {\n+  if (cb == nullptr || !cb->is_compiled() || !callee->is_in_use() || callee->is_unloading()) {\n@@ -2168,2 +1854,1 @@\n-  CompiledMethod* nm = cb->as_compiled_method_or_null();\n-  assert(nm, \"must be\");\n+  CompiledMethod* caller = cb->as_compiled_method();\n@@ -2174,47 +1859,15 @@\n-  \/\/ There is a benign race here. We could be attempting to patch to a compiled\n-  \/\/ entry point at the same time the callee is being deoptimized. If that is\n-  \/\/ the case then entry_point may in fact point to a c2i and we'd patch the\n-  \/\/ call site with the same old data. clear_code will set code() to null\n-  \/\/ at the end of it. If we happen to see that null then we can skip trying\n-  \/\/ to patch. If we hit the window where the callee has a c2i in the\n-  \/\/ from_compiled_entry and the null isn't present yet then we lose the race\n-  \/\/ and patch the code with the same old data. Asi es la vida.\n-\n-  if (moop->code() == nullptr) return;\n-\n-  if (nm->is_in_use()) {\n-    \/\/ Expect to find a native call there (unless it was no-inline cache vtable dispatch)\n-    CompiledICLocker ic_locker(nm);\n-    if (NativeCall::is_call_before(return_pc)) {\n-      ResourceMark mark;\n-      NativeCallWrapper* call = nm->call_wrapper_before(return_pc);\n-      \/\/\n-      \/\/ bug 6281185. We might get here after resolving a call site to a vanilla\n-      \/\/ virtual call. Because the resolvee uses the verified entry it may then\n-      \/\/ see compiled code and attempt to patch the site by calling us. This would\n-      \/\/ then incorrectly convert the call site to optimized and its downhill from\n-      \/\/ there. If you're lucky you'll get the assert in the bugid, if not you've\n-      \/\/ just made a call site that could be megamorphic into a monomorphic site\n-      \/\/ for the rest of its life! Just another racing bug in the life of\n-      \/\/ fixup_callers_callsite ...\n-      \/\/\n-      RelocIterator iter(nm, call->instruction_address(), call->next_instruction_address());\n-      iter.next();\n-      assert(iter.has_current(), \"must have a reloc at java call site\");\n-      relocInfo::relocType typ = iter.reloc()->type();\n-      if (typ != relocInfo::static_call_type &&\n-           typ != relocInfo::opt_virtual_call_type &&\n-           typ != relocInfo::static_stub_type) {\n-        return;\n-      }\n-      if (nm->method()->is_continuation_enter_intrinsic()) {\n-        if (ContinuationEntry::is_interpreted_call(call->instruction_address())) {\n-          return;\n-        }\n-      }\n-      address destination = call->destination();\n-      address entry_point = cb->is_compiled_by_c1() ? callee->verified_inline_entry_point() : callee->verified_entry_point();\n-      if (should_fixup_call_destination(destination, entry_point, caller_pc, moop, cb)) {\n-        call->set_destination_mt_safe(entry_point);\n-      }\n-    }\n+  if (!caller->is_in_use() || !NativeCall::is_call_before(return_pc)) {\n+    return;\n+  }\n+\n+  \/\/ Expect to find a native call there (unless it was no-inline cache vtable dispatch)\n+  CompiledICLocker ic_locker(caller);\n+  ResourceMark rm;\n+\n+  \/\/ If we got here through a static call or opt_virtual call, then we know where the\n+  \/\/ call address would be; let's peek at it\n+  address callsite_addr = (address)nativeCall_before(return_pc);\n+  RelocIterator iter(caller, callsite_addr, callsite_addr + 1);\n+  if (!iter.next()) {\n+    \/\/ No reloc entry found; not a static or optimized virtual call\n+    return;\n@@ -2222,0 +1875,9 @@\n+\n+  relocInfo::relocType type = iter.reloc()->type();\n+  if (type != relocInfo::static_call_type &&\n+      type != relocInfo::opt_virtual_call_type) {\n+    return;\n+  }\n+\n+  CompiledDirectCall* callsite = CompiledDirectCall::before(return_pc);\n+  callsite->set_to_clean();\n@@ -3787,2 +3449,2 @@\n-      }\n-    }\n+          }\n+        }\n","filename":"src\/hotspot\/share\/runtime\/sharedRuntime.cpp","additions":71,"deletions":409,"binary":false,"changes":480,"status":"modified"},{"patch":"@@ -52,5 +52,0 @@\n-  static bool resolve_sub_helper_internal(methodHandle callee_method, const frame& caller_frame,\n-                                          CompiledMethod* caller_nm, bool is_virtual, bool is_optimized, bool& caller_is_c1,\n-                                          Handle receiver, CallInfo& call_info, Bytecodes::Code invoke_code, TRAPS);\n-  static methodHandle resolve_sub_helper(bool is_virtual, bool is_optimized, bool& caller_is_c1, TRAPS);\n-\n","filename":"src\/hotspot\/share\/runtime\/sharedRuntime.hpp","additions":0,"deletions":5,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -34,0 +34,1 @@\n+#include \"runtime\/threadWXSetters.inline.hpp\"\n@@ -459,0 +460,1 @@\n+    MACOS_AARCH64_ONLY(ThreadWXEnable wx(WXExec, Thread::current());) \/\/ About to call into code cache\n@@ -464,0 +466,1 @@\n+    MACOS_AARCH64_ONLY(ThreadWXEnable wx(WXExec, Thread::current());) \/\/ About to call into code cache\n","filename":"src\/hotspot\/share\/runtime\/stubRoutines.hpp","additions":4,"deletions":1,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -254,1 +254,1 @@\n-int dtrace_waited_probe(ObjectMonitor* monitor, Handle obj, JavaThread* thr) {\n+static int dtrace_waited_probe(ObjectMonitor* monitor, Handle obj, JavaThread* thr) {\n@@ -412,0 +412,13 @@\n+  if (LockingMode == LM_LIGHTWEIGHT) {\n+    LockStack& lock_stack = current->lock_stack();\n+    if (lock_stack.is_full()) {\n+      \/\/ Always go into runtime if the lock stack is full.\n+      return false;\n+    }\n+    if (lock_stack.try_recursive_enter(obj)) {\n+      \/\/ Recursive lock successful.\n+      current->inc_held_monitor_count();\n+      return true;\n+    }\n+  }\n+\n@@ -581,14 +594,40 @@\n-      if (lock_stack.can_push()) {\n-        markWord mark = obj()->mark_acquire();\n-        while (mark.is_neutral()) {\n-          \/\/ Retry until a lock state change has been observed.  cas_set_mark() may collide with non lock bits modifications.\n-          \/\/ Try to swing into 'fast-locked' state.\n-          assert(!lock_stack.contains(obj()), \"thread must not already hold the lock\");\n-          const markWord locked_mark = mark.set_fast_locked();\n-          const markWord old_mark = obj()->cas_set_mark(locked_mark, mark);\n-          if (old_mark == mark) {\n-            \/\/ Successfully fast-locked, push object to lock-stack and return.\n-            lock_stack.push(obj());\n-            return true;\n-          }\n-          mark = old_mark;\n+      if (lock_stack.is_full()) {\n+        \/\/ We unconditionally make room on the lock stack by inflating\n+        \/\/ the least recently locked object on the lock stack.\n+\n+        \/\/ About the choice to inflate least recently locked object.\n+        \/\/ First we must chose to inflate a lock, either some lock on\n+        \/\/ the lock-stack or the lock that is currently being entered\n+        \/\/ (which may or may not be on the lock-stack).\n+        \/\/ Second the best lock to inflate is a lock which is entered\n+        \/\/ in a control flow where there are only a very few locks being\n+        \/\/ used, as the costly part of inflated locking is inflation,\n+        \/\/ not locking. But this property is entirely program dependent.\n+        \/\/ Third inflating the lock currently being entered on when it\n+        \/\/ is not present on the lock-stack will result in a still full\n+        \/\/ lock-stack. This creates a scenario where every deeper nested\n+        \/\/ monitorenter must call into the runtime.\n+        \/\/ The rational here is as follows:\n+        \/\/ Because we cannot (currently) figure out the second, and want\n+        \/\/ to avoid the third, we inflate a lock on the lock-stack.\n+        \/\/ The least recently locked lock is chosen as it is the lock\n+        \/\/ with the longest critical section.\n+\n+        log_info(monitorinflation)(\"LockStack capacity exceeded, inflating.\");\n+        ObjectMonitor* monitor = inflate_for(locking_thread, lock_stack.bottom(), inflate_cause_vm_internal);\n+        assert(monitor->owner() == Thread::current(), \"must be owner=\" PTR_FORMAT \" current=\" PTR_FORMAT \" mark=\" PTR_FORMAT,\n+               p2i(monitor->owner()), p2i(Thread::current()), monitor->object()->mark_acquire().value());\n+        assert(!lock_stack.is_full(), \"must have made room here\");\n+      }\n+\n+      markWord mark = obj()->mark_acquire();\n+      while (mark.is_neutral()) {\n+        \/\/ Retry until a lock state change has been observed. cas_set_mark() may collide with non lock bits modifications.\n+        \/\/ Try to swing into 'fast-locked' state.\n+        assert(!lock_stack.contains(obj()), \"thread must not already hold the lock\");\n+        const markWord locked_mark = mark.set_fast_locked();\n+        const markWord old_mark = obj()->cas_set_mark(locked_mark, mark);\n+        if (old_mark == mark) {\n+          \/\/ Successfully fast-locked, push object to lock-stack and return.\n+          lock_stack.push(obj());\n+          return true;\n@@ -596,0 +635,6 @@\n+        mark = old_mark;\n+      }\n+\n+      if (mark.is_fast_locked() && lock_stack.try_recursive_enter(obj())) {\n+        \/\/ Recursive lock successful.\n+        return true;\n@@ -643,7 +688,21 @@\n-      while (mark.is_fast_locked()) {\n-        \/\/ Retry until a lock state change has been observed.  cas_set_mark() may collide with non lock bits modifications.\n-        const markWord unlocked_mark = mark.set_unlocked();\n-        const markWord old_mark = object->cas_set_mark(unlocked_mark, mark);\n-        if (old_mark == mark) {\n-          current->lock_stack().remove(object);\n-          return;\n+      LockStack& lock_stack = current->lock_stack();\n+      if (mark.is_fast_locked() && lock_stack.try_recursive_exit(object)) {\n+        \/\/ Recursively unlocked.\n+        return;\n+      }\n+\n+      if (mark.is_fast_locked() && lock_stack.is_recursive(object)) {\n+        \/\/ This lock is recursive but is not at the top of the lock stack so we're\n+        \/\/ doing an unbalanced exit. We have to fall thru to inflation below and\n+        \/\/ let ObjectMonitor::exit() do the unlock.\n+      } else {\n+        while (mark.is_fast_locked()) {\n+          \/\/ Retry until a lock state change has been observed. cas_set_mark() may collide with non lock bits modifications.\n+          const markWord unlocked_mark = mark.set_unlocked();\n+          const markWord old_mark = object->cas_set_mark(unlocked_mark, mark);\n+          if (old_mark == mark) {\n+            size_t recursions = lock_stack.remove(object) - 1;\n+            assert(recursions == 0, \"must not be recursive here\");\n+            return;\n+          }\n+          mark = old_mark;\n@@ -651,1 +710,0 @@\n-        mark = old_mark;\n@@ -1415,1 +1473,2 @@\n-        inflating_thread->lock_stack().remove(object);\n+        size_t removed = inflating_thread->lock_stack().remove(object);\n+        inf->set_recursions(removed - 1);\n@@ -1461,1 +1520,2 @@\n-          inflating_thread->lock_stack().remove(object);\n+          size_t removed = inflating_thread->lock_stack().remove(object);\n+          monitor->set_recursions(removed - 1);\n@@ -2024,3 +2084,1 @@\n-    \/\/ This should not happen, but if it does, it is not fatal.\n-    out->print_cr(\"WARNING: monitor=\" INTPTR_FORMAT \": in-use monitor is \"\n-                  \"deflated.\", p2i(n));\n+    \/\/ This could happen when monitor deflation blocks for a safepoint.\n@@ -2029,0 +2087,1 @@\n+\n","filename":"src\/hotspot\/share\/runtime\/synchronizer.cpp","additions":87,"deletions":28,"binary":false,"changes":115,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -753,0 +753,7 @@\n+  if (Continuations::enabled()) {\n+    \/\/ Initialize Continuation class now so that failure to create enterSpecial\/doYield\n+    \/\/ special nmethods due to limited CodeCache size can be treated as a fatal error at\n+    \/\/ startup with the proper message that CodeCache size is too small.\n+    initialize_class(vmSymbols::jdk_internal_vm_Continuation(), CHECK_JNI_ERR);\n+  }\n+\n@@ -1115,1 +1122,1 @@\n-void assert_thread_claimed(const char* kind, Thread* t, uintx expected) {\n+static void assert_thread_claimed(const char* kind, Thread* t, uintx expected) {\n","filename":"src\/hotspot\/share\/runtime\/threads.cpp","additions":9,"deletions":2,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -43,0 +43,1 @@\n+#include \"code\/compiledIC.hpp\"\n@@ -66,1 +67,0 @@\n-#include \"oops\/compiledICHolder.hpp\"\n@@ -216,2 +216,0 @@\n-  nonstatic_field(CompiledICHolder,            _holder_metadata,                              Metadata*)                             \\\n-  nonstatic_field(CompiledICHolder,            _holder_klass,                                 Klass*)                                \\\n@@ -1167,1 +1165,0 @@\n-  declare_toplevel_type(CompiledICHolder)                                 \\\n","filename":"src\/hotspot\/share\/runtime\/vmStructs.cpp","additions":1,"deletions":4,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -63,1 +63,1 @@\n-BasicType char2type(int ch) {\n+static BasicType char2type(int ch) {\n","filename":"src\/hotspot\/share\/utilities\/globalDefinitions.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -4520,42 +4520,12 @@\n-     * Access of bytes at an index may be aligned or misaligned for {@code T},\n-     * with respect to the underlying memory address, {@code A} say, associated\n-     * with the array and index.\n-     * If access is misaligned then access for anything other than the\n-     * {@code get} and {@code set} access modes will result in an\n-     * {@code IllegalStateException}.  In such cases atomic access is only\n-     * guaranteed with respect to the largest power of two that divides the GCD\n-     * of {@code A} and the size (in bytes) of {@code T}.\n-     * If access is aligned then following access modes are supported and are\n-     * guaranteed to support atomic access:\n-     * <ul>\n-     * <li>read write access modes for all {@code T}, with the exception of\n-     *     access modes {@code get} and {@code set} for {@code long} and\n-     *     {@code double} on 32-bit platforms.\n-     * <li>atomic update access modes for {@code int}, {@code long},\n-     *     {@code float} or {@code double}.\n-     *     (Future major platform releases of the JDK may support additional\n-     *     types for certain currently unsupported access modes.)\n-     * <li>numeric atomic update access modes for {@code int} and {@code long}.\n-     *     (Future major platform releases of the JDK may support additional\n-     *     numeric types for certain currently unsupported access modes.)\n-     * <li>bitwise atomic update access modes for {@code int} and {@code long}.\n-     *     (Future major platform releases of the JDK may support additional\n-     *     numeric types for certain currently unsupported access modes.)\n-     * <\/ul>\n-     * <p>\n-     * Misaligned access, and therefore atomicity guarantees, may be determined\n-     * for {@code byte[]} arrays without operating on a specific array.  Given\n-     * an {@code index}, {@code T} and its corresponding boxed type,\n-     * {@code T_BOX}, misalignment may be determined as follows:\n-     * <pre>{@code\n-     * int sizeOfT = T_BOX.BYTES;  \/\/ size in bytes of T\n-     * int misalignedAtZeroIndex = ByteBuffer.wrap(new byte[0]).\n-     *     alignmentOffset(0, sizeOfT);\n-     * int misalignedAtIndex = (misalignedAtZeroIndex + index) % sizeOfT;\n-     * boolean isMisaligned = misalignedAtIndex != 0;\n-     * }<\/pre>\n-     * <p>\n-     * If the variable type is {@code float} or {@code double} then atomic\n-     * update access modes compare values using their bitwise representation\n-     * (see {@link Float#floatToRawIntBits} and\n-     * {@link Double#doubleToRawLongBits}, respectively).\n+     * Only plain {@linkplain VarHandle.AccessMode#GET get} and {@linkplain VarHandle.AccessMode#SET set}\n+     * access modes are supported by the returned var handle. For all other access modes, an\n+     * {@link UnsupportedOperationException} will be thrown.\n+     *\n+     * @apiNote if access modes other than plain access are required, clients should\n+     * consider using off-heap memory through\n+     * {@linkplain java.nio.ByteBuffer#allocateDirect(int) direct byte buffers} or\n+     * off-heap {@linkplain java.lang.foreign.MemorySegment memory segments},\n+     * or memory segments backed by a\n+     * {@linkplain java.lang.foreign.MemorySegment#ofArray(long[]) {@code long[]}},\n+     * for which stronger alignment guarantees can be made.\n+     *\n@@ -4607,1 +4577,7 @@\n-     * Access of bytes at an index may be aligned or misaligned for {@code T},\n+     * For heap byte buffers, access is always unaligned. As a result, only the plain\n+     * {@linkplain VarHandle.AccessMode#GET get}\n+     * and {@linkplain VarHandle.AccessMode#SET set} access modes are supported by the\n+     * returned var handle. For all other access modes, an {@link IllegalStateException}\n+     * will be thrown.\n+     * <p>\n+     * For direct buffers only, access of bytes at an index may be aligned or misaligned for {@code T},\n","filename":"src\/java.base\/share\/classes\/java\/lang\/invoke\/MethodHandles.java","additions":19,"deletions":43,"binary":false,"changes":62,"status":"modified"},{"patch":"@@ -1480,0 +1480,5 @@\n+\n+            if (swtch instanceof JCSwitchExpression) {\n+                 \/\/ Emit line position for the end of a switch expression\n+                 code.statBegin(TreeInfo.endPos(swtch));\n+            }\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/jvm\/Gen.java","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1999, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1999, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -784,1 +784,1 @@\n-                log.error(DiagnosticFlag.SYNTAX, token.pos, Errors.IntNumberTooLarge(strval(prefix)));\n+                reportIntegralLiteralError(prefix, pos);\n@@ -793,1 +793,1 @@\n-                log.error(DiagnosticFlag.SYNTAX, token.pos, Errors.IntNumberTooLarge(strval(prefix)));\n+                reportIntegralLiteralError(prefix, pos);\n@@ -876,0 +876,22 @@\n+        void reportIntegralLiteralError(Name prefix, int pos) {\n+            int radix = token.radix();\n+            if (radix == 2 || radix == 8) {\n+                \/\/attempt to produce more user-friendly error message for\n+                \/\/binary and octal literals with wrong digits:\n+                String value = strval(prefix);\n+                char[] cs = value.toCharArray();\n+                for (int i = 0; i < cs.length; i++) {\n+                    char c = cs[i];\n+                    int d = Character.digit(c, radix);\n+                    if (d == (-1)) {\n+                        Error err = radix == 2 ? Errors.IllegalDigitInBinaryLiteral\n+                                               : Errors.IllegalDigitInOctalLiteral;\n+                        log.error(DiagnosticFlag.SYNTAX,\n+                                  token.pos + i,\n+                                  err);\n+                        return ;\n+                    }\n+                }\n+            }\n+            log.error(DiagnosticFlag.SYNTAX, token.pos, Errors.IntNumberTooLarge(strval(prefix)));\n+        }\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/parser\/JavacParser.java","additions":25,"deletions":3,"binary":false,"changes":28,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n-# Copyright (c) 1999, 2023, Oracle and\/or its affiliates. All rights reserved.\n+# Copyright (c) 1999, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -743,0 +743,6 @@\n+compiler.err.illegal.digit.in.binary.literal=\\\n+    illegal digit in a binary literal\n+\n+compiler.err.illegal.digit.in.octal.literal=\\\n+    illegal digit in an octal literal\n+\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/resources\/compiler.properties","additions":7,"deletions":1,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -79,3 +79,0 @@\n-compiler\/intrinsics\/float16\/TestConstFloat16ToFloat.java 8325264 macosx-aarch64\n-compiler\/intrinsics\/float16\/Binary16Conversion.java 8325264 macosx-aarch64\n-\n@@ -90,0 +87,1 @@\n+gc\/parallel\/TestAlwaysPreTouchBehavior.java 8325218 linux-all\n","filename":"test\/hotspot\/jtreg\/ProblemList.txt","additions":1,"deletions":3,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n-# Copyright (c) 2013, 2023, Oracle and\/or its affiliates. All rights reserved.\n+# Copyright (c) 2013, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -153,0 +153,1 @@\n+  gtest\/LockStackGtests.java \\\n","filename":"test\/hotspot\/jtreg\/TEST.groups","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -44,1 +44,1 @@\n-    @Arguments({Argument.MAX}) \/\/ the argument needs to be big enough to fall out of cache.\n+    @Arguments(values = {Argument.MAX}) \/\/ the argument needs to be big enough to fall out of cache.\n","filename":"test\/hotspot\/jtreg\/compiler\/c2\/irTests\/TestOptimizeUnstableIf.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -111,0 +111,1 @@\n+    private final HashMap<String, Method> setupMethodMap = new HashMap<>();\n@@ -229,0 +230,2 @@\n+            TestFormat.checkNoThrow(getAnnotation(m, Setup.class) == null,\n+                                    \"Cannot use @Setup annotation in \" + clazzType + \" \" + c + \" at \" + m);\n@@ -263,0 +266,5 @@\n+\n+        \/\/ Collect the @Setup methods so we can reference them\n+        \/\/ from the test methods\n+        collectSetupMethods();\n+\n@@ -502,0 +510,29 @@\n+\n+    \/**\n+     *  Collect all @Setup annotated methods and add them to setupMethodMap, for convenience to reference later from\n+     *  tests with @Arguments(setup = \"setupMethodName\").\n+     *\/\n+    private void collectSetupMethods() {\n+        for (Method m : testClass.getDeclaredMethods()) {\n+            Setup setupAnnotation = getAnnotation(m, Setup.class);\n+            if (setupAnnotation != null) {\n+                addSetupMethod(m);\n+            }\n+        }\n+    }\n+\n+    private void addSetupMethod(Method m) {\n+        TestFormat.checkNoThrow(getAnnotation(m, Test.class) == null,\n+                                \"@Setup method cannot have @Test annotation: \" + m);\n+        TestFormat.checkNoThrow(getAnnotation(m, Check.class) == null,\n+                                \"@Setup method cannot have @Check annotation: \" + m);\n+        TestFormat.checkNoThrow(getAnnotation(m, Arguments.class) == null,\n+                                \"@Setup method cannot have @Arguments annotation: \" + m);\n+        TestFormat.checkNoThrow(getAnnotation(m, Run.class) == null,\n+                                \"@Setup method cannot have @Run annotation: \" + m);\n+        Method mOverloaded = setupMethodMap.put(m.getName(), m);\n+        TestFormat.checkNoThrow(mOverloaded == null,\n+                                \"@Setup method cannot be overloaded: \" + mOverloaded + \" with \" + m);\n+        m.setAccessible(true);\n+    }\n+\n@@ -547,1 +584,2 @@\n-        DeclaredTest test = new DeclaredTest(m, ArgumentValue.getArguments(m), compLevel, warmupIterations);\n+        ArgumentsProvider argumentsProvider = ArgumentsProviderBuilder.build(m, setupMethodMap);\n+        DeclaredTest test = new DeclaredTest(m, argumentsProvider, compLevel, warmupIterations);\n@@ -736,1 +774,2 @@\n-        TestFormat.check(!test.hasArguments(),\n+        Arguments argumentsAnno = getAnnotation(testMethod, Arguments.class);\n+        TestFormat.check(argumentsAnno == null,\n","filename":"test\/hotspot\/jtreg\/compiler\/lib\/ir_framework\/test\/TestVM.java","additions":41,"deletions":2,"binary":false,"changes":43,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n-# Copyright (c) 2022, 2023, Oracle and\/or its affiliates. All rights reserved.\n+# Copyright (c) 2022, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -78,0 +78,1 @@\n+javax\/management\/ImplementationVersion\/ImplVersionTest.java 0000000 generic-all\n@@ -79,0 +80,1 @@\n+javax\/management\/remote\/mandatory\/version\/ImplVersionTest.java 0000000 generic-all\n","filename":"test\/jdk\/ProblemList-Virtual.txt","additions":3,"deletions":1,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -162,1 +162,3 @@\n-        VarHandleSource(VarHandle vh, MemoryMode... modes) {\n+        final boolean supportsAtomicAccess;\n+\n+        VarHandleSource(VarHandle vh, boolean supportsAtomicAccess, MemoryMode... modes) {\n@@ -164,0 +166,1 @@\n+            this.supportsAtomicAccess = supportsAtomicAccess;\n","filename":"test\/jdk\/java\/lang\/invoke\/VarHandles\/VarHandleBaseByteArrayTest.java","additions":4,"deletions":1,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2012, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2012, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -122,0 +122,4 @@\n+  public native int getLockStackCapacity();\n+\n+  public native boolean supportsRecursiveLightweightLocking();\n+\n","filename":"test\/lib\/jdk\/test\/whitebox\/WhiteBox.java","additions":5,"deletions":1,"binary":false,"changes":6,"status":"modified"}]}