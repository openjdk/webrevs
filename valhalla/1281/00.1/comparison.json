{"files":[{"patch":"@@ -2,1 +2,1 @@\n-# Copyright (c) 2011, 2023, Oracle and\/or its affiliates. All rights reserved.\n+# Copyright (c) 2011, 2024, Oracle and\/or its affiliates. All rights reserved.\n","filename":"make\/Main.gmk","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -99,1 +99,1 @@\n-        serial preview dangling-doc-comments, \\\n+        serial preview unchecked deprecation dangling-doc-comments, \\\n","filename":"make\/test\/BuildMicrobenchmark.gmk","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -55,1 +55,0 @@\n-    JAVAC_FLAGS := --enable-preview, \\\n@@ -63,1 +62,1 @@\n-    EXCLUDES := jdk\/test\/lib\/containers jdk\/test\/lib\/security, \\\n+    EXCLUDES := jdk\/test\/lib\/containers jdk\/test\/lib\/security org, \\\n@@ -67,1 +66,1 @@\n-    DISABLED_WARNINGS := try deprecation rawtypes unchecked serial cast removal preview restricted dangling-doc-comments, \\\n+    DISABLED_WARNINGS := try deprecation rawtypes unchecked serial cast removal preview restricted varargs dangling-doc-comments, \\\n@@ -73,0 +72,6 @@\n+        --add-exports jdk.compiler\/com.sun.tools.javac.api=ALL-UNNAMED \\\n+        --add-exports jdk.compiler\/com.sun.tools.javac.code=ALL-UNNAMED \\\n+        --add-exports jdk.compiler\/com.sun.tools.javac.comp=ALL-UNNAMED \\\n+        --add-exports jdk.compiler\/com.sun.tools.javac.main=ALL-UNNAMED \\\n+        --add-exports jdk.compiler\/com.sun.tools.javac.tree=ALL-UNNAMED \\\n+        --add-exports jdk.compiler\/com.sun.tools.javac.util=ALL-UNNAMED \\\n","filename":"make\/test\/BuildTestLib.gmk","additions":8,"deletions":3,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -3207,0 +3207,3 @@\n+void LIR_Assembler::emit_profile_inline_type(LIR_OpProfileInlineType* op) {\n+  Unimplemented();\n+}\n","filename":"src\/hotspot\/cpu\/ppc\/c1_LIRAssembler_ppc.cpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -33,0 +33,2 @@\n+#include \"ci\/ciFlatArrayKlass.hpp\"\n+#include \"ci\/ciInlineKlass.hpp\"\n@@ -1066,1 +1068,9 @@\n-  ValueStack* state_before = copy_state_indexed_access();\n+  ValueStack* state_before = nullptr;\n+  int array_idx = state()->stack_size() - 2;\n+  if (type == T_OBJECT && state()->stack_at(array_idx)->maybe_flat_array()) {\n+    \/\/ Save the entire state and re-execute on deopt when accessing flat arrays\n+    state_before = copy_state_before();\n+    state_before->set_should_reexecute(true);\n+  } else {\n+    state_before = copy_state_indexed_access();\n+  }\n@@ -1078,1 +1088,61 @@\n-  push(as_ValueType(type), append(new LoadIndexed(array, index, length, type, state_before)));\n+\n+  bool need_membar = false;\n+  LoadIndexed* load_indexed = nullptr;\n+  Instruction* result = nullptr;\n+  if (array->is_loaded_flat_array()) {\n+    ciType* array_type = array->declared_type();\n+    ciInlineKlass* elem_klass = array_type->as_flat_array_klass()->element_klass()->as_inline_klass();\n+\n+    bool can_delay_access = false;\n+    ciBytecodeStream s(method());\n+    s.force_bci(bci());\n+    s.next();\n+    if (s.cur_bc() == Bytecodes::_getfield) {\n+      bool will_link;\n+      ciField* next_field = s.get_field(will_link);\n+      bool next_needs_patching = !next_field->holder()->is_initialized() ||\n+                                 !next_field->will_link(method(), Bytecodes::_getfield) ||\n+                                 PatchALot;\n+      can_delay_access = C1UseDelayedFlattenedFieldReads && !next_needs_patching;\n+    }\n+    if (can_delay_access) {\n+      \/\/ potentially optimizable array access, storing information for delayed decision\n+      LoadIndexed* li = new LoadIndexed(array, index, length, type, state_before);\n+      DelayedLoadIndexed* dli = new DelayedLoadIndexed(li, state_before);\n+      li->set_delayed(dli);\n+      set_pending_load_indexed(dli);\n+      return; \/\/ Nothing else to do for now\n+    } else {\n+      if (elem_klass->is_empty()) {\n+        \/\/ No need to create a new instance, the default instance will be used instead\n+        load_indexed = new LoadIndexed(array, index, length, type, state_before);\n+        apush(append(load_indexed));\n+      } else {\n+        NewInstance* new_instance = new NewInstance(elem_klass, state_before, false, true);\n+        _memory->new_instance(new_instance);\n+        apush(append_split(new_instance));\n+        load_indexed = new LoadIndexed(array, index, length, type, state_before);\n+        load_indexed->set_vt(new_instance);\n+        \/\/ The LoadIndexed node will initialise this instance by copying from\n+        \/\/ the flat field.  Ensure these stores are visible before any\n+        \/\/ subsequent store that publishes this reference.\n+        need_membar = true;\n+      }\n+    }\n+  } else {\n+    load_indexed = new LoadIndexed(array, index, length, type, state_before);\n+    if (profile_array_accesses() && is_reference_type(type)) {\n+      compilation()->set_would_profile(true);\n+      load_indexed->set_should_profile(true);\n+      load_indexed->set_profiled_method(method());\n+      load_indexed->set_profiled_bci(bci());\n+    }\n+  }\n+  result = append(load_indexed);\n+  if (need_membar) {\n+    append(new MemBar(lir_membar_storestore));\n+  }\n+  assert(!load_indexed->should_profile() || load_indexed == result, \"should not be optimized out\");\n+  if (!array->is_loaded_flat_array()) {\n+    push(as_ValueType(type), result);\n+  }\n@@ -1084,1 +1154,9 @@\n-  ValueStack* state_before = copy_state_indexed_access();\n+  ValueStack* state_before = nullptr;\n+  int array_idx = state()->stack_size() - 3;\n+  if (type == T_OBJECT && state()->stack_at(array_idx)->maybe_flat_array()) {\n+    \/\/ Save the entire state and re-execute on deopt when accessing flat arrays\n+    state_before = copy_state_before();\n+    state_before->set_should_reexecute(true);\n+  } else {\n+    state_before = copy_state_indexed_access();\n+  }\n@@ -1109,5 +1187,2 @@\n-  StoreIndexed* result = new StoreIndexed(array, index, length, type, value, state_before, check_boolean);\n-  append(result);\n-  _memory->store_value(value);\n-  if (type == T_OBJECT && is_profiling()) {\n-    \/\/ Note that we'd collect profile data in this method if we wanted it.\n+  StoreIndexed* store_indexed = new StoreIndexed(array, index, length, type, value, state_before, check_boolean);\n+  if (profile_array_accesses() && is_reference_type(type) && !array->is_loaded_flat_array()) {\n@@ -1116,6 +1191,3 @@\n-\n-    if (profile_checkcasts()) {\n-      result->set_profiled_method(method());\n-      result->set_profiled_bci(bci());\n-      result->set_should_profile(true);\n-    }\n+    store_indexed->set_should_profile(true);\n+    store_indexed->set_profiled_method(method());\n+    store_indexed->set_profiled_bci(bci());\n@@ -1123,0 +1195,3 @@\n+  Instruction* result = append(store_indexed);\n+  assert(!store_indexed->should_profile() || store_indexed == result, \"should not be optimized out\");\n+  _memory->store_value(value);\n@@ -1125,1 +1200,0 @@\n-\n@@ -1129,1 +1203,1 @@\n-      { state()->raw_pop();\n+      { Value w = state()->raw_pop();\n@@ -1133,2 +1207,2 @@\n-      { state()->raw_pop();\n-        state()->raw_pop();\n+      { Value w1 = state()->raw_pop();\n+        Value w2 = state()->raw_pop();\n@@ -1313,0 +1387,27 @@\n+\n+  bool subst_check = false;\n+  if (EnableValhalla && (stream()->cur_bc() == Bytecodes::_if_acmpeq || stream()->cur_bc() == Bytecodes::_if_acmpne)) {\n+    ValueType* left_vt = x->type();\n+    ValueType* right_vt = y->type();\n+    if (left_vt->is_object()) {\n+      assert(right_vt->is_object(), \"must be\");\n+      ciKlass* left_klass = x->as_loaded_klass_or_null();\n+      ciKlass* right_klass = y->as_loaded_klass_or_null();\n+\n+      if (left_klass == nullptr || right_klass == nullptr) {\n+        \/\/ The klass is still unloaded, or came from a Phi node. Go slow case;\n+        subst_check = true;\n+      } else if (left_klass->can_be_inline_klass() || right_klass->can_be_inline_klass()) {\n+        \/\/ Either operand may be a value object, but we're not sure. Go slow case;\n+        subst_check = true;\n+      } else {\n+        \/\/ No need to do substitutability check\n+      }\n+    }\n+  }\n+  if ((stream()->cur_bc() == Bytecodes::_if_acmpeq || stream()->cur_bc() == Bytecodes::_if_acmpne) &&\n+      is_profiling() && profile_branches()) {\n+    compilation()->set_would_profile(true);\n+    append(new ProfileACmpTypes(method(), bci(), x, y));\n+  }\n+\n@@ -1315,1 +1416,1 @@\n-  Instruction *i = append(new If(x, cond, false, y, tsux, fsux, (is_bb || compilation()->is_optimistic()) ? state_before : nullptr, is_bb));\n+  Instruction *i = append(new If(x, cond, false, y, tsux, fsux, (is_bb || compilation()->is_optimistic() || subst_check) ? state_before : nullptr, is_bb, subst_check));\n@@ -1565,1 +1666,1 @@\n-  if (method()->name() == ciSymbols::object_initializer_name() &&\n+  if (method()->is_object_constructor() &&\n@@ -1716,0 +1817,13 @@\n+void GraphBuilder::copy_inline_content(ciInlineKlass* vk, Value src, int src_off, Value dest, int dest_off, ValueStack* state_before, ciField* enclosing_field) {\n+  for (int i = 0; i < vk->nof_nonstatic_fields(); i++) {\n+    ciField* inner_field = vk->nonstatic_field_at(i);\n+    assert(!inner_field->is_flat(), \"the iteration over nested fields is handled by the loop itself\");\n+    int off = inner_field->offset_in_bytes() - vk->first_field_offset();\n+    LoadField* load = new LoadField(src, src_off + off, inner_field, false, state_before, false);\n+    Value replacement = append(load);\n+    StoreField* store = new StoreField(dest, dest_off + off, inner_field, replacement, false, state_before, false);\n+    store->set_enclosing_field(enclosing_field);\n+    append(store);\n+  }\n+}\n+\n@@ -1722,0 +1836,1 @@\n+\n@@ -1725,1 +1840,1 @@\n-                              PatchALot;\n+                              (!field->is_flat() && PatchALot);\n@@ -1757,1 +1872,1 @@\n-  const int offset = !needs_patching ? field->offset_in_bytes() : -1;\n+  int offset = !needs_patching ? field->offset_in_bytes() : -1;\n@@ -1767,0 +1882,4 @@\n+      } else if (field->is_null_free() && field->type()->as_instance_klass()->is_initialized() &&\n+                 field->type()->as_inline_klass()->is_empty()) {\n+        \/\/ Loading from a field of an empty inline type. Just return the default instance.\n+        constant = new Constant(new InstanceConstant(field->type()->as_inline_klass()->default_instance()));\n@@ -1774,2 +1893,3 @@\n-        push(type, append(new LoadField(append(obj), offset, field, true,\n-                                        state_before, needs_patching)));\n+        LoadField* load_field = new LoadField(append(obj), offset, field, true,\n+                                        state_before, needs_patching);\n+        push(type, append(load_field));\n@@ -1784,1 +1904,1 @@\n-      if (field->type()->basic_type() == T_BOOLEAN) {\n+      if (field_type == T_BOOLEAN) {\n@@ -1788,0 +1908,7 @@\n+      if (field->is_null_free()) {\n+        null_check(val);\n+      }\n+      if (field->is_null_free() && field->type()->is_loaded() && field->type()->as_inline_klass()->is_empty()) {\n+        \/\/ Storing to a field of an empty inline type. Ignore.\n+        break;\n+      }\n@@ -1794,14 +1921,31 @@\n-      obj = apop();\n-      ObjectType* obj_type = obj->type()->as_ObjectType();\n-      if (field->is_constant() && obj_type->is_constant() && !PatchALot) {\n-        ciObject* const_oop = obj_type->constant_value();\n-        if (!const_oop->is_null_object() && const_oop->is_loaded()) {\n-          ciConstant field_value = field->constant_value_of(const_oop);\n-          if (field_value.is_valid()) {\n-            constant = make_constant(field_value, field);\n-            \/\/ For CallSite objects add a dependency for invalidation of the optimization.\n-            if (field->is_call_site_target()) {\n-              ciCallSite* call_site = const_oop->as_call_site();\n-              if (!call_site->is_fully_initialized_constant_call_site()) {\n-                ciMethodHandle* target = field_value.as_object()->as_method_handle();\n-                dependency_recorder()->assert_call_site_target_value(call_site, target);\n+      if (state_before == nullptr && field->is_flat()) {\n+        \/\/ Save the entire state and re-execute on deopt when accessing flat fields\n+        assert(Interpreter::bytecode_should_reexecute(code), \"should reexecute\");\n+        state_before = copy_state_before();\n+      }\n+      if (!has_pending_field_access() && !has_pending_load_indexed()) {\n+        obj = apop();\n+        ObjectType* obj_type = obj->type()->as_ObjectType();\n+        if (field->is_null_free() && field->type()->as_instance_klass()->is_initialized()\n+            && field->type()->as_inline_klass()->is_empty()) {\n+          \/\/ Loading from a field of an empty inline type. Just return the default instance.\n+          null_check(obj);\n+          constant = new Constant(new InstanceConstant(field->type()->as_inline_klass()->default_instance()));\n+        } else if (field->is_constant() && !field->is_flat() && obj_type->is_constant() && !PatchALot) {\n+          ciObject* const_oop = obj_type->constant_value();\n+          if (!const_oop->is_null_object() && const_oop->is_loaded()) {\n+            ciConstant field_value = field->constant_value_of(const_oop);\n+            if (field_value.is_valid()) {\n+              if (field->is_null_free() && field_value.is_null_or_zero()) {\n+                \/\/ Non-flat inline type field. Replace null by the default value.\n+                constant = new Constant(new InstanceConstant(field->type()->as_inline_klass()->default_instance()));\n+              } else {\n+                constant = make_constant(field_value, field);\n+              }\n+              \/\/ For CallSite objects add a dependency for invalidation of the optimization.\n+              if (field->is_call_site_target()) {\n+                ciCallSite* call_site = const_oop->as_call_site();\n+                if (!call_site->is_fully_initialized_constant_call_site()) {\n+                  ciMethodHandle* target = field_value.as_object()->as_method_handle();\n+                  dependency_recorder()->assert_call_site_target_value(call_site, target);\n+                }\n@@ -1819,19 +1963,15 @@\n-        LoadField* load = new LoadField(obj, offset, field, false, state_before, needs_patching);\n-        Value replacement = !needs_patching ? _memory->load(load) : load;\n-        if (replacement != load) {\n-          assert(replacement->is_linked() || !replacement->can_be_linked(), \"should already by linked\");\n-          \/\/ Writing an (integer) value to a boolean, byte, char or short field includes an implicit narrowing\n-          \/\/ conversion. Emit an explicit conversion here to get the correct field value after the write.\n-          BasicType bt = field->type()->basic_type();\n-          switch (bt) {\n-          case T_BOOLEAN:\n-          case T_BYTE:\n-            replacement = append(new Convert(Bytecodes::_i2b, replacement, as_ValueType(bt)));\n-            break;\n-          case T_CHAR:\n-            replacement = append(new Convert(Bytecodes::_i2c, replacement, as_ValueType(bt)));\n-            break;\n-          case T_SHORT:\n-            replacement = append(new Convert(Bytecodes::_i2s, replacement, as_ValueType(bt)));\n-            break;\n-          default:\n+        if (!field->is_flat()) {\n+          if (has_pending_field_access()) {\n+            assert(!needs_patching, \"Can't patch delayed field access\");\n+            obj = pending_field_access()->obj();\n+            offset += pending_field_access()->offset() - field->holder()->as_inline_klass()->first_field_offset();\n+            field = pending_field_access()->holder()->get_field_by_offset(offset, false);\n+            assert(field != nullptr, \"field not found\");\n+            set_pending_field_access(nullptr);\n+          } else if (has_pending_load_indexed()) {\n+            assert(!needs_patching, \"Can't patch delayed field access\");\n+            pending_load_indexed()->update(field, offset - field->holder()->as_inline_klass()->first_field_offset());\n+            LoadIndexed* li = pending_load_indexed()->load_instr();\n+            li->set_type(type);\n+            push(type, append(li));\n+            set_pending_load_indexed(nullptr);\n@@ -1840,3 +1980,94 @@\n-          push(type, replacement);\n-        } else {\n-          push(type, append(load));\n+          LoadField* load = new LoadField(obj, offset, field, false, state_before, needs_patching);\n+          Value replacement = !needs_patching ? _memory->load(load) : load;\n+          if (replacement != load) {\n+            assert(replacement->is_linked() || !replacement->can_be_linked(), \"should already by linked\");\n+            \/\/ Writing an (integer) value to a boolean, byte, char or short field includes an implicit narrowing\n+            \/\/ conversion. Emit an explicit conversion here to get the correct field value after the write.\n+            switch (field_type) {\n+            case T_BOOLEAN:\n+            case T_BYTE:\n+              replacement = append(new Convert(Bytecodes::_i2b, replacement, type));\n+              break;\n+            case T_CHAR:\n+              replacement = append(new Convert(Bytecodes::_i2c, replacement, type));\n+              break;\n+            case T_SHORT:\n+              replacement = append(new Convert(Bytecodes::_i2s, replacement, type));\n+              break;\n+            default:\n+              break;\n+            }\n+            push(type, replacement);\n+          } else {\n+            push(type, append(load));\n+          }\n+        } else {  \/\/ field is flat\n+          \/\/ Look at the next bytecode to check if we can delay the field access\n+          bool can_delay_access = false;\n+          ciBytecodeStream s(method());\n+          s.force_bci(bci());\n+          s.next();\n+          if (s.cur_bc() == Bytecodes::_getfield && !needs_patching) {\n+            ciField* next_field = s.get_field(will_link);\n+            bool next_needs_patching = !next_field->holder()->is_loaded() ||\n+                                       !next_field->will_link(method(), Bytecodes::_getfield) ||\n+                                       PatchALot;\n+            can_delay_access = C1UseDelayedFlattenedFieldReads && !next_needs_patching;\n+          }\n+          if (can_delay_access) {\n+            if (has_pending_load_indexed()) {\n+              pending_load_indexed()->update(field, offset - field->holder()->as_inline_klass()->first_field_offset());\n+            } else if (has_pending_field_access()) {\n+              pending_field_access()->inc_offset(offset - field->holder()->as_inline_klass()->first_field_offset());\n+            } else {\n+              null_check(obj);\n+              DelayedFieldAccess* dfa = new DelayedFieldAccess(obj, field->holder(), field->offset_in_bytes(), state_before);\n+              set_pending_field_access(dfa);\n+            }\n+          } else {\n+            ciInlineKlass* inline_klass = field->type()->as_inline_klass();\n+            scope()->set_wrote_final();\n+            scope()->set_wrote_fields();\n+            bool need_membar = false;\n+            if (inline_klass->is_initialized() && inline_klass->is_empty()) {\n+              apush(append(new Constant(new InstanceConstant(inline_klass->default_instance()))));\n+              if (has_pending_field_access()) {\n+                set_pending_field_access(nullptr);\n+              } else if (has_pending_load_indexed()) {\n+                set_pending_load_indexed(nullptr);\n+              }\n+            } else if (has_pending_load_indexed()) {\n+              assert(!needs_patching, \"Can't patch delayed field access\");\n+              pending_load_indexed()->update(field, offset - field->holder()->as_inline_klass()->first_field_offset());\n+              NewInstance* vt = new NewInstance(inline_klass, pending_load_indexed()->state_before(), false, true);\n+              _memory->new_instance(vt);\n+              pending_load_indexed()->load_instr()->set_vt(vt);\n+              apush(append_split(vt));\n+              append(pending_load_indexed()->load_instr());\n+              set_pending_load_indexed(nullptr);\n+              need_membar = true;\n+            } else {\n+              if (has_pending_field_access()) {\n+                state_before = pending_field_access()->state_before();\n+              }\n+              NewInstance* new_instance = new NewInstance(inline_klass, state_before, false, true);\n+              _memory->new_instance(new_instance);\n+              apush(append_split(new_instance));\n+              assert(!needs_patching, \"Can't patch flat inline type field access\");\n+              if (has_pending_field_access()) {\n+                copy_inline_content(inline_klass, pending_field_access()->obj(),\n+                                    pending_field_access()->offset() + field->offset_in_bytes() - field->holder()->as_inline_klass()->first_field_offset(),\n+                                    new_instance, inline_klass->first_field_offset(), state_before);\n+                set_pending_field_access(nullptr);\n+              } else {\n+                copy_inline_content(inline_klass, obj, field->offset_in_bytes(), new_instance, inline_klass->first_field_offset(), state_before);\n+              }\n+              need_membar = true;\n+            }\n+            if (need_membar) {\n+              \/\/ If we allocated a new instance ensure the stores to copy the\n+              \/\/ field contents are visible before any subsequent store that\n+              \/\/ publishes this reference.\n+              append(new MemBar(lir_membar_storestore));\n+            }\n+          }\n@@ -1853,1 +2084,1 @@\n-      if (field->type()->basic_type() == T_BOOLEAN) {\n+      if (field_type == T_BOOLEAN) {\n@@ -1857,4 +2088,17 @@\n-      StoreField* store = new StoreField(obj, offset, field, val, false, state_before, needs_patching);\n-      if (!needs_patching) store = _memory->store(store);\n-      if (store != nullptr) {\n-        append(store);\n+      if (field->is_null_free() && field->type()->is_loaded() && field->type()->as_inline_klass()->is_empty()) {\n+        \/\/ Storing to a field of an empty inline type. Ignore.\n+        null_check(obj);\n+        null_check(val);\n+      } else if (!field->is_flat()) {\n+        if (field->is_null_free()) {\n+          null_check(val);\n+        }\n+        StoreField* store = new StoreField(obj, offset, field, val, false, state_before, needs_patching);\n+        if (!needs_patching) store = _memory->store(store);\n+        if (store != nullptr) {\n+          append(store);\n+        }\n+      } else {\n+        assert(!needs_patching, \"Can't patch flat inline type field access\");\n+        ciInlineKlass* inline_klass = field->type()->as_inline_klass();\n+        copy_inline_content(inline_klass, val, inline_klass->first_field_offset(), obj, offset, state_before, field);\n@@ -1870,1 +2114,0 @@\n-\n@@ -1987,1 +2230,1 @@\n-    } else if (bc_raw == Bytecodes::_invokespecial && !target->is_object_initializer() && calling_klass->is_interface()) {\n+    } else if (bc_raw == Bytecodes::_invokespecial && !target->is_object_constructor() && calling_klass->is_interface()) {\n@@ -2243,3 +2486,9 @@\n-  NewInstance* new_instance = new NewInstance(klass->as_instance_klass(), state_before, stream()->is_unresolved_klass());\n-  _memory->new_instance(new_instance);\n-  apush(append_split(new_instance));\n+  if (!stream()->is_unresolved_klass() && klass->is_inlinetype() &&\n+      klass->as_inline_klass()->is_initialized() && klass->as_inline_klass()->is_empty()) {\n+    ciInlineKlass* vk = klass->as_inline_klass();\n+    apush(append(new Constant(new InstanceConstant(vk->default_instance()))));\n+  } else {\n+    NewInstance* new_instance = new NewInstance(klass->as_instance_klass(), state_before, stream()->is_unresolved_klass(), false);\n+    _memory->new_instance(new_instance);\n+    apush(append_split(new_instance));\n+  }\n@@ -2248,1 +2497,0 @@\n-\n@@ -2321,0 +2569,19 @@\n+  bool maybe_inlinetype = false;\n+  if (bci == InvocationEntryBci) {\n+    \/\/ Called by GraphBuilder::inline_sync_entry.\n+#ifdef ASSERT\n+    ciType* obj_type = x->declared_type();\n+    assert(obj_type == nullptr || !obj_type->is_inlinetype(), \"inline types cannot have synchronized methods\");\n+#endif\n+  } else {\n+    \/\/ We are compiling a monitorenter bytecode\n+    if (EnableValhalla) {\n+      ciType* obj_type = x->declared_type();\n+      if (obj_type == nullptr || obj_type->as_klass()->can_be_inline_klass()) {\n+        \/\/ If we're (possibly) locking on an inline type, check for markWord::always_locked_pattern\n+        \/\/ and throw IMSE. (obj_type is null for Phi nodes, so let's just be conservative).\n+        maybe_inlinetype = true;\n+      }\n+    }\n+  }\n+\n@@ -2323,1 +2590,1 @@\n-  append_with_bci(new MonitorEnter(x, state()->lock(x), state_before), bci);\n+  append_with_bci(new MonitorEnter(x, state()->lock(x), state_before, maybe_inlinetype), bci);\n@@ -2471,0 +2738,1 @@\n+    if (value->is_null_free()) return;\n@@ -2496,1 +2764,3 @@\n-    assert(cur_bci == SynchronizationEntryBCI || cur_bci == cur_scope_data->stream()->cur_bci(), \"invalid bci\");\n+    assert(cur_bci == SynchronizationEntryBCI || cur_bci == cur_scope_data->stream()->cur_bci()\n+           || has_pending_field_access() || has_pending_load_indexed(), \"invalid bci\");\n+\n@@ -3278,1 +3548,2 @@\n-    state->store_local(idx, new Local(method()->holder(), objectType, idx, true));\n+    state->store_local(idx, new Local(method()->holder(), objectType, idx,\n+             \/*receiver*\/ true, \/*null_free*\/ method()->holder()->is_flat_array_klass()));\n@@ -3290,1 +3561,1 @@\n-    state->store_local(idx, new Local(type, vt, idx, false));\n+    state->store_local(idx, new Local(type, vt, idx, false, false));\n@@ -3310,0 +3581,2 @@\n+  , _pending_field_access(nullptr)\n+  , _pending_load_indexed(nullptr)\n","filename":"src\/hotspot\/share\/c1\/c1_GraphBuilder.cpp","additions":349,"deletions":76,"binary":false,"changes":425,"status":"modified"},{"patch":"@@ -54,0 +54,1 @@\n+#include \"runtime\/sharedRuntime.hpp\"\n@@ -664,0 +665,54 @@\n+bool ciMethod::array_access_profiled_type(int bci, ciKlass*& array_type, ciKlass*& element_type, ProfilePtrKind& element_ptr, bool &flat_array, bool &null_free_array) {\n+  if (method_data() != nullptr && method_data()->is_mature()) {\n+    ciProfileData* data = method_data()->bci_to_data(bci);\n+    if (data != nullptr) {\n+      if (data->is_ArrayLoadData()) {\n+        ciArrayLoadData* array_access = (ciArrayLoadData*) data->as_ArrayLoadData();\n+        array_type = array_access->array()->valid_type();\n+        element_type = array_access->element()->valid_type();\n+        element_ptr = array_access->element()->ptr_kind();\n+        flat_array = array_access->flat_array();\n+        null_free_array = array_access->null_free_array();\n+        return true;\n+      } else if (data->is_ArrayStoreData()) {\n+        ciArrayStoreData* array_access = (ciArrayStoreData*) data->as_ArrayStoreData();\n+        array_type = array_access->array()->valid_type();\n+        flat_array = array_access->flat_array();\n+        null_free_array = array_access->null_free_array();\n+        ciCallProfile call_profile = call_profile_at_bci(bci);\n+        if (call_profile.morphism() == 1) {\n+          element_type = call_profile.receiver(0);\n+        } else {\n+          element_type = nullptr;\n+        }\n+        if (!array_access->null_seen()) {\n+          element_ptr = ProfileNeverNull;\n+        } else if (call_profile.count() == 0) {\n+          element_ptr = ProfileAlwaysNull;\n+        } else {\n+          element_ptr = ProfileMaybeNull;\n+        }\n+        return true;\n+      }\n+    }\n+  }\n+  return false;\n+}\n+\n+bool ciMethod::acmp_profiled_type(int bci, ciKlass*& left_type, ciKlass*& right_type, ProfilePtrKind& left_ptr, ProfilePtrKind& right_ptr, bool &left_inline_type, bool &right_inline_type) {\n+  if (method_data() != nullptr && method_data()->is_mature()) {\n+    ciProfileData* data = method_data()->bci_to_data(bci);\n+    if (data != nullptr && data->is_ACmpData()) {\n+      ciACmpData* acmp = (ciACmpData*)data->as_ACmpData();\n+      left_type = acmp->left()->valid_type();\n+      right_type = acmp->right()->valid_type();\n+      left_ptr = acmp->left()->ptr_kind();\n+      right_ptr = acmp->right()->ptr_kind();\n+      left_inline_type = acmp->left_inline_type();\n+      right_inline_type = acmp->right_inline_type();\n+      return true;\n+    }\n+  }\n+  return false;\n+}\n+\n@@ -973,1 +1028,1 @@\n-\/\/ ciMethod::is_object_initializer\n+\/\/ ciMethod::is_object_constructor\n@@ -975,2 +1030,5 @@\n-bool ciMethod::is_object_initializer() const {\n-   return name() == ciSymbols::object_initializer_name();\n+bool ciMethod::is_object_constructor() const {\n+   return (name() == ciSymbols::object_initializer_name()\n+           && signature()->return_type()->is_void());\n+   \/\/ Note:  We can't test is_static, because that would\n+   \/\/ require the method to be loaded.  Sometimes it isn't.\n@@ -1252,1 +1310,0 @@\n-bool ciMethod::is_initializer () const {         FETCH_FLAG_FROM_VM(is_initializer); }\n@@ -1499,0 +1556,18 @@\n+\n+bool ciMethod::is_scalarized_arg(int idx) const {\n+  VM_ENTRY_MARK;\n+  return get_Method()->is_scalarized_arg(idx);\n+}\n+\n+bool ciMethod::has_scalarized_args() const {\n+  VM_ENTRY_MARK;\n+  return get_Method()->has_scalarized_args();\n+}\n+\n+const GrowableArray<SigEntry>* ciMethod::get_sig_cc() const {\n+  VM_ENTRY_MARK;\n+  if (get_Method()->adapter() == nullptr) {\n+    return nullptr;\n+  }\n+  return get_Method()->adapter()->get_sig_cc();\n+}\n","filename":"src\/hotspot\/share\/ci\/ciMethod.cpp","additions":79,"deletions":4,"binary":false,"changes":83,"status":"modified"},{"patch":"@@ -49,0 +49,1 @@\n+  ProfileUnknownNull,\n@@ -201,1 +202,1 @@\n-  bool is_static_initializer()  const { return get_Method()->is_static_initializer();  }\n+  bool is_class_initializer()   const { return get_Method()->is_class_initializer();   }\n@@ -272,1 +273,4 @@\n-\n+  bool          array_access_profiled_type(int bci, ciKlass*& array_type, ciKlass*& element_type, ProfilePtrKind& element_ptr, bool &flat_array, bool &null_free);\n+  bool          acmp_profiled_type(int bci, ciKlass*& left_type, ciKlass*& right_type,\n+                                   ProfilePtrKind& left_ptr, ProfilePtrKind& right_ptr,\n+                                   bool &left_inline_type, bool &right_inline_type);\n@@ -344,0 +348,1 @@\n+  bool has_vararg     () const                   { return flags().has_vararg(); }\n@@ -355,1 +360,0 @@\n-  bool is_initializer () const;\n@@ -361,0 +365,1 @@\n+  bool is_object_constructor() const;\n@@ -362,1 +367,0 @@\n-  bool is_object_initializer() const;\n@@ -388,0 +392,5 @@\n+\n+  \/\/ Support for the inline type calling convention\n+  bool is_scalarized_arg(int idx) const;\n+  bool has_scalarized_args() const;\n+  const GrowableArray<SigEntry>* get_sig_cc() const;\n","filename":"src\/hotspot\/share\/ci\/ciMethod.hpp","additions":13,"deletions":4,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -59,0 +59,1 @@\n+#include \"oops\/fieldStreams.inline.hpp\"\n@@ -70,0 +71,1 @@\n+#include \"oops\/inlineKlass.inline.hpp\"\n@@ -78,0 +80,1 @@\n+#include \"runtime\/os.hpp\"\n@@ -353,1 +356,1 @@\n-      \/\/ Ignore wrapping L and ;.\n+      \/\/ Ignore wrapping L and ; (and Q and ; for value types).\n@@ -458,0 +461,1 @@\n+    \/\/ Otherwise, a LinkageError will be thrown later.\n@@ -957,2 +961,1 @@\n-  assert(!ModuleEntryTable::javabase_moduleEntry()->is_patched(),\n-         \"Cannot use sharing if java.base is patched\");\n+  assert(!CDSConfig::module_patching_disables_cds(), \"Cannot use CDS\");\n@@ -1034,1 +1037,1 @@\n-        assert(!mod_entry->is_patched(), \"cannot load archived classes for patched module\");\n+        assert(!CDSConfig::module_patching_disables_cds(), \"Cannot use CDS\");\n@@ -1160,0 +1163,32 @@\n+  if (ik->has_inline_type_fields()) {\n+    for (AllFieldStream fs(ik); !fs.done(); fs.next()) {\n+      if (fs.access_flags().is_static()) continue;\n+      Symbol* sig = fs.signature();\n+      if (fs.is_null_free_inline_type()) {\n+        \/\/ Pre-load inline class\n+        TempNewSymbol name = Signature::strip_envelope(sig);\n+        Klass* real_k = SystemDictionary::resolve_with_circularity_detection_or_fail(ik->name(), name,\n+          class_loader, protection_domain, false, CHECK_NULL);\n+        Klass* k = ik->get_inline_type_field_klass_or_null(fs.index());\n+        if (real_k != k) {\n+          \/\/ oops, the app has substituted a different version of k!\n+          return nullptr;\n+        }\n+      } else if (Signature::has_envelope(sig)) {\n+        TempNewSymbol name = Signature::strip_envelope(sig);\n+        if (name != ik->name() && ik->is_class_in_loadable_descriptors_attribute(name)) {\n+          Klass* real_k = SystemDictionary::resolve_with_circularity_detection_or_fail(ik->name(), name,\n+            class_loader, protection_domain, false, THREAD);\n+          if (HAS_PENDING_EXCEPTION) {\n+            CLEAR_PENDING_EXCEPTION;\n+          }\n+          Klass* k = ik->get_inline_type_field_klass_or_null(fs.index());\n+          if (real_k != k) {\n+            \/\/ oops, the app has substituted a different version of k!\n+            return nullptr;\n+          }\n+        }\n+      }\n+    }\n+  }\n+\n@@ -1195,0 +1230,1 @@\n+\n","filename":"src\/hotspot\/share\/classfile\/systemDictionary.cpp","additions":40,"deletions":4,"binary":false,"changes":44,"status":"modified"},{"patch":"@@ -134,0 +134,1 @@\n+  do_klass(ValueObjectMethods_klass,                    java_lang_runtime_ValueObjectMethods                  ) \\\n","filename":"src\/hotspot\/share\/classfile\/vmClassMacros.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -265,0 +265,1 @@\n+  case vmIntrinsics::_newNullRestrictedArray:\n@@ -328,0 +329,2 @@\n+  case vmIntrinsics::_makePrivateBuffer:\n+  case vmIntrinsics::_finishPrivateBuffer:\n@@ -337,0 +340,1 @@\n+  case vmIntrinsics::_getValue:\n@@ -346,0 +350,1 @@\n+  case vmIntrinsics::_putValue:\n","filename":"src\/hotspot\/share\/classfile\/vmIntrinsics.cpp","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -332,0 +332,3 @@\n+  do_intrinsic(_newNullRestrictedArray,   jdk_internal_value_ValueClass, newNullRestrictedArray_name, newNullRestrictedArray_signature, F_SN) \\\n+   do_signature(newNullRestrictedArray_signature,                 \"(Ljava\/lang\/Class;I)[Ljava\/lang\/Object;\")            \\\n+   do_name(     newNullRestrictedArray_name,                      \"newNullRestrictedArray\")                             \\\n@@ -636,0 +639,2 @@\n+  do_intrinsic(_isFlatArray,              jdk_internal_misc_Unsafe,     isFlatArray_name, class_boolean_signature, F_RN)         \\\n+   do_name(     isFlatArray_name,                                       \"isFlatArray\")                                           \\\n@@ -687,0 +692,2 @@\n+  do_signature(getValue_signature,        \"(Ljava\/lang\/Object;JLjava\/lang\/Class;)Ljava\/lang\/Object;\")                   \\\n+  do_signature(putValue_signature,        \"(Ljava\/lang\/Object;JLjava\/lang\/Class;Ljava\/lang\/Object;)V\")                  \\\n@@ -697,0 +704,3 @@\n+  do_name(getValue_name,\"getValue\")             do_name(putValue_name,\"putValue\")                                       \\\n+  do_name(makePrivateBuffer_name,\"makePrivateBuffer\")                                                                   \\\n+  do_name(finishPrivateBuffer_name,\"finishPrivateBuffer\")                                                               \\\n@@ -707,0 +717,1 @@\n+  do_intrinsic(_getValue,           jdk_internal_misc_Unsafe,     getValue_name, getValue_signature,             F_RN)  \\\n@@ -716,0 +727,4 @@\n+  do_intrinsic(_putValue,           jdk_internal_misc_Unsafe,     putValue_name, putValue_signature,             F_RN)  \\\n+                                                                                                                        \\\n+  do_intrinsic(_makePrivateBuffer,  jdk_internal_misc_Unsafe,     makePrivateBuffer_name, object_object_signature, F_RN)   \\\n+  do_intrinsic(_finishPrivateBuffer,  jdk_internal_misc_Unsafe,   finishPrivateBuffer_name, object_object_signature, F_RN) \\\n","filename":"src\/hotspot\/share\/classfile\/vmIntrinsics.hpp","additions":15,"deletions":0,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -173,0 +173,1 @@\n+  template(tag_loadable_descriptors,                  \"LoadableDescriptors\")                      \\\n@@ -209,0 +210,1 @@\n+  template(java_lang_IdentityException,               \"java\/lang\/IdentityException\")              \\\n@@ -256,0 +258,3 @@\n+  template(jdk_internal_vm_annotation_ImplicitlyConstructible_signature,     \"Ljdk\/internal\/vm\/annotation\/ImplicitlyConstructible;\") \\\n+  template(jdk_internal_vm_annotation_LooselyConsistentValue_signature,      \"Ljdk\/internal\/vm\/annotation\/LooselyConsistentValue;\") \\\n+  template(jdk_internal_vm_annotation_NullRestricted_signature,              \"Ljdk\/internal\/vm\/annotation\/NullRestricted;\") \\\n@@ -287,1 +292,0 @@\n-  template(trusted_final_name,                        \"trustedFinal\")                             \\\n@@ -511,0 +515,2 @@\n+  template(default_value_name,                        \".default\")                                 \\\n+  template(empty_marker_name,                         \".empty\")                                   \\\n@@ -582,0 +588,1 @@\n+  template(class_class_signature,                     \"(Ljava\/lang\/Class;)Ljava\/lang\/Class;\")     \\\n@@ -591,0 +598,1 @@\n+  template(object_object_boolean_signature,           \"(Ljava\/lang\/Object;Ljava\/lang\/Object;)Z\")                  \\\n@@ -721,0 +729,2 @@\n+  template(primaryType_name,                           \"primaryType\")                                             \\\n+  template(secondaryType_name,                         \"secondaryType\")                                           \\\n@@ -746,0 +756,6 @@\n+  template(java_lang_runtime_ValueObjectMethods,            \"java\/lang\/runtime\/ValueObjectMethods\")               \\\n+  template(isSubstitutable_name,                            \"isSubstitutable\")                                    \\\n+  template(valueObjectHashCode_name,                        \"valueObjectHashCode\")                                \\\n+  template(jdk_internal_value_PrimitiveClass,               \"jdk\/internal\/value\/PrimitiveClass\")                  \\\n+  template(jdk_internal_value_ValueClass,                   \"jdk\/internal\/value\/ValueClass\")                      \\\n+                                                                                                                  \\\n","filename":"src\/hotspot\/share\/classfile\/vmSymbols.hpp","additions":17,"deletions":1,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -1602,1 +1602,1 @@\n-              Deoptimization::reassign_fields(vf->frame_pointer(), &reg_map, objects, realloc_failures, false);\n+              Deoptimization::reassign_fields(vf->frame_pointer(), &reg_map, objects, realloc_failures, false, CHECK_NULL);\n@@ -1853,1 +1853,1 @@\n-  Deoptimization::reassign_fields(fstAfterDeopt.current(), fstAfterDeopt.register_map(), objects, realloc_failures, false);\n+  Deoptimization::reassign_fields(fstAfterDeopt.current(), fstAfterDeopt.register_map(), objects, realloc_failures, false, THREAD);\n@@ -2181,1 +2181,1 @@\n-    if (m->is_initializer() && !m->is_static()) {\n+    if (m->is_object_constructor()) {\n@@ -2208,1 +2208,1 @@\n-    if (!m->is_initializer() && !m->is_overpass()) {\n+    if (!(m->is_object_constructor() || m->is_class_initializer()) && !m->is_overpass()) {\n@@ -2924,2 +2924,1 @@\n-  if (m->is_initializer()) {\n-    if (m->is_static_initializer()) {\n+  if (m->is_class_initializer()) {\n@@ -2928,1 +2927,2 @@\n-    }\n+  }\n+  else if (m->is_object_constructor()) {\n","filename":"src\/hotspot\/share\/jvmci\/jvmciCompilerToVM.cpp","additions":7,"deletions":7,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -210,1 +210,1 @@\n-  nonstatic_field(InstanceKlass,               _misc_flags._flags,                            u2)                                    \\\n+  nonstatic_field(InstanceKlass,               _misc_flags._flags,                            u4)                                    \\\n@@ -683,0 +683,3 @@\n+  declare_constant(DataLayout::array_store_data_tag)                      \\\n+  declare_constant(DataLayout::array_load_data_tag)                       \\\n+  declare_constant(DataLayout::acmp_data_tag)                             \\\n","filename":"src\/hotspot\/share\/jvmci\/vmStructs_jvmci.cpp","additions":4,"deletions":1,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -142,1 +142,1 @@\n-    if (!is_static)\n+    if (!is_static) {\n@@ -144,0 +144,1 @@\n+    }\n@@ -1605,0 +1606,1 @@\n+    case Bytecodes::_invokeinterface:\n@@ -1606,4 +1608,3 @@\n-    case Bytecodes::_invokespecial:     do_method(false, false, itr->get_index_u2(), itr->bci(), itr->code()); break;\n-    case Bytecodes::_invokestatic:      do_method(true,  false, itr->get_index_u2(), itr->bci(), itr->code()); break;\n-    case Bytecodes::_invokedynamic:     do_method(true,  false, itr->get_index_u4(), itr->bci(), itr->code()); break;\n-    case Bytecodes::_invokeinterface:   do_method(false, true,  itr->get_index_u2(), itr->bci(), itr->code()); break;\n+    case Bytecodes::_invokespecial:     do_method(false, itr->get_index_u2(), itr->bci(), itr->code()); break;\n+    case Bytecodes::_invokestatic:      do_method(true , itr->get_index_u2(), itr->bci(), itr->code()); break;\n+    case Bytecodes::_invokedynamic:     do_method(true , itr->get_index_u4(), itr->bci(), itr->code()); break;\n@@ -1629,0 +1630,1 @@\n+\n@@ -1954,1 +1956,3 @@\n-  if (!is_static) in[i++] = CellTypeState::ref;\n+  if (!is_static) {\n+    in[i++] = CellTypeState::ref;\n+  }\n@@ -1960,1 +1964,1 @@\n-void GenerateOopMap::do_method(int is_static, int is_interface, int idx, int bci, Bytecodes::Code bc) {\n+void GenerateOopMap::do_method(int is_static, int idx, int bci, Bytecodes::Code bc) {\n","filename":"src\/hotspot\/share\/oops\/generateOopMap.cpp","additions":11,"deletions":7,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -63,0 +63,1 @@\n+\/\/\n@@ -66,0 +67,64 @@\n+\/\/\n+\/\/\n+\/\/\n+\/\/  Valhalla\n+\/\/\n+\/\/  <CMH: merge this doc into the text above>\n+\/\/\n+\/\/  Project Valhalla has mark word encoding requirements for the following oops:\n+\/\/\n+\/\/  * inline types: have alternative bytecode behavior, e.g. can not be locked\n+\/\/    - \"larval state\": mutable state, but only during object init, observable\n+\/\/      by only by a single thread (generally do not mutate markWord)\n+\/\/\n+\/\/  * flat arrays: load\/decode of klass layout helper is expensive for aaload\n+\/\/\n+\/\/  * \"null free\" arrays: load\/decode of klass layout helper again for aaload\n+\/\/\n+\/\/  EnableValhalla\n+\/\/\n+\/\/  Formerly known as \"biased lock bit\", \"unused_gap\" is free to use: using this\n+\/\/  bit to indicate inline type, combined with \"unlocked\" lock bits, means we\n+\/\/  will not interfere with lock encodings (displaced, inflating, and monitor),\n+\/\/  since inline types can't be locked.\n+\/\/\n+\/\/  Further state encoding\n+\/\/\n+\/\/  32 bit plaforms currently have no further room for encoding. No room for\n+\/\/  \"denormalized layout helper bits\", these fast mark word tests can only be made on\n+\/\/  64 bit platforms. 32-bit platforms need to load the klass->_layout_helper. This\n+\/\/  said, the larval state bit is still required for operation, stealing from the hash\n+\/\/  code is simplest mechanism.\n+\/\/\n+\/\/  Valhalla specific encodings\n+\/\/\n+\/\/  Revised Bit-format of an object header (most significant first, big endian layout below):\n+\/\/\n+\/\/  32 bits:\n+\/\/  --------\n+\/\/  hash:24 ------------>| larval:1 age:4 inline_type:1 lock:2\n+\/\/\n+\/\/  64 bits:\n+\/\/  --------\n+\/\/  unused:1 | <-- hash:31 -->| unused:22 larval:1 age:4 flat_array:1 null_free_array:1 inline_type:1 lock:2\n+\/\/\n+\/\/  The \"fast\" static type bits (flat_array, null_free_array, and inline_type)\n+\/\/  are placed lowest next to lock bits to more easily decode forwarding pointers.\n+\/\/  G1 for example, implicitly clears age bits (\"G1FullGCCompactionPoint::forward()\")\n+\/\/  using \"oopDesc->forwardee()\", so it necessary for \"markWord::decode_pointer()\"\n+\/\/  to return a non-nullptr for this case, but not confuse the static type bits for\n+\/\/  a pointer.\n+\/\/\n+\/\/  Static types bits are recorded in the \"klass->prototype_header()\", displaced\n+\/\/  mark should simply use the prototype header as \"slow path\", rather chasing\n+\/\/  monitor or stack lock races.\n+\/\/\n+\/\/  Lock patterns (note inline types can't be locked\/monitor\/inflating)...\n+\/\/\n+\/\/  [ptr            | 000]  locked             ptr points to real header on stack\n+\/\/  [header         | ?01]  unlocked           regular object header\n+\/\/  [ptr            | 010]  monitor            inflated lock (header is wapped out)\n+\/\/  [ptr            | ?11]  marked             used to mark an object\n+\/\/  [0 ............ | 000]  inflating          inflation in progress\n+\/\/\n+\/\/\n@@ -104,2 +169,1 @@\n-  \/\/ Constants\n-  static const int age_bits                       = 4;\n+  \/\/ Constants, in least significant bit order\n@@ -107,2 +171,9 @@\n-  static const int first_unused_gap_bits          = 1;\n-  static const int max_hash_bits                  = BitsPerWord - age_bits - lock_bits - first_unused_gap_bits;\n+  static const int first_unused_gap_bits          = 1; \/\/ When !EnableValhalla\n+  \/\/ EnableValhalla: static prototype header bits (fast path instead of klass layout_helper)\n+  static const int inline_type_bits               = 1;\n+  static const int null_free_array_bits           = LP64_ONLY(1) NOT_LP64(0);\n+  static const int flat_array_bits                = LP64_ONLY(1) NOT_LP64(0);\n+  \/\/ instance state\n+  static const int age_bits                       = 4;\n+  static const int larval_bits                    = 1;\n+  static const int max_hash_bits                  = BitsPerWord - age_bits - lock_bits - inline_type_bits - larval_bits - flat_array_bits - null_free_array_bits;\n@@ -110,1 +181,1 @@\n-  static const int second_unused_gap_bits         = LP64_ONLY(1) NOT_LP64(0);\n+  static const int second_unused_gap_bits         = LP64_ONLY(1) NOT_LP64(0); \/\/ !EnableValhalla: unused\n@@ -113,2 +184,7 @@\n-  static const int age_shift                      = lock_bits + first_unused_gap_bits;\n-  static const int hash_shift                     = age_shift + age_bits + second_unused_gap_bits;\n+  static const int inline_type_shift              = lock_bits;\n+  static const int null_free_array_shift          = inline_type_shift + inline_type_bits;\n+  static const int flat_array_shift               = null_free_array_shift + null_free_array_bits;\n+  static const int age_shift                      = flat_array_shift + flat_array_bits;\n+  static const int unused_gap_shift               = age_shift + age_bits; \/\/ !EnableValhalla: unused\n+  static const int larval_shift                   = age_shift + age_bits;\n+  static const int hash_shift                     = LP64_ONLY(32) NOT_LP64(larval_shift + larval_bits);\n@@ -118,0 +194,10 @@\n+  static const uintptr_t inline_type_mask         = right_n_bits(lock_bits + inline_type_bits);\n+  static const uintptr_t inline_type_mask_in_place = inline_type_mask << lock_shift;\n+  static const uintptr_t inline_type_bit_in_place = 1 << inline_type_shift;\n+  static const uintptr_t null_free_array_mask     = right_n_bits(null_free_array_bits);\n+  static const uintptr_t null_free_array_mask_in_place = (null_free_array_mask << null_free_array_shift) | lock_mask_in_place;\n+  static const uintptr_t null_free_array_bit_in_place  = (1 << null_free_array_shift);\n+  static const uintptr_t flat_array_mask          = right_n_bits(flat_array_bits);\n+  static const uintptr_t flat_array_mask_in_place = (flat_array_mask << flat_array_shift) | null_free_array_mask_in_place | lock_mask_in_place;\n+  static const uintptr_t flat_array_bit_in_place  = (1 << flat_array_shift);\n+\n@@ -120,0 +206,5 @@\n+\n+  static const uintptr_t larval_mask              = right_n_bits(larval_bits);\n+  static const uintptr_t larval_mask_in_place     = (larval_mask << larval_shift) | inline_type_mask_in_place;\n+  static const uintptr_t larval_bit_in_place      = (1 << larval_shift);\n+\n@@ -128,0 +219,10 @@\n+  static const uintptr_t inline_type_pattern      = inline_type_bit_in_place | unlocked_value;\n+  static const uintptr_t null_free_array_pattern  = null_free_array_bit_in_place | unlocked_value;\n+  static const uintptr_t flat_array_pattern       = flat_array_bit_in_place | null_free_array_pattern;\n+  \/\/ Has static klass prototype, used for decode\/encode pointer\n+  static const uintptr_t static_prototype_mask    = LP64_ONLY(right_n_bits(inline_type_bits + flat_array_bits + null_free_array_bits)) NOT_LP64(right_n_bits(inline_type_bits));\n+  static const uintptr_t static_prototype_mask_in_place = static_prototype_mask << lock_bits;\n+  static const uintptr_t static_prototype_value_max = (1 << age_shift) - 1;\n+\n+  static const uintptr_t larval_pattern           = larval_bit_in_place | inline_type_pattern;\n+\n@@ -137,0 +238,4 @@\n+  bool is_inline_type() const {\n+    return (mask_bits(value(), inline_type_mask_in_place) == inline_type_pattern);\n+  }\n+\n@@ -147,0 +252,7 @@\n+\n+  \/\/ is unlocked and not an inline type (which cannot be involved in locking, displacement or inflation)\n+  \/\/ i.e. test both lock bits and the inline type bit together\n+  bool is_neutral()  const {  \/\/ Not locked, or marked - a \"clean\" neutral state\n+    return (mask_bits(value(), inline_type_mask_in_place) == unlocked_value);\n+  }\n+\n@@ -150,3 +262,0 @@\n-  bool is_neutral()  const {  \/\/ Not locked, or marked - a \"clean\" neutral state\n-    return (mask_bits(value(), lock_mask_in_place) == unlocked_value);\n-  }\n@@ -169,1 +278,1 @@\n-    return (!is_unlocked() || !has_no_hash());\n+    return (!is_unlocked() || !has_no_hash() || (EnableValhalla && is_larval_state()));\n@@ -263,0 +372,30 @@\n+  \/\/ private buffered value operations\n+  markWord enter_larval_state() const {\n+    return markWord(value() | larval_bit_in_place);\n+  }\n+  markWord exit_larval_state() const {\n+    return markWord(value() & ~larval_bit_in_place);\n+  }\n+  bool is_larval_state() const {\n+    return (mask_bits(value(), larval_mask_in_place) == larval_pattern);\n+  }\n+\n+#ifdef _LP64 \/\/ 64 bit encodings only\n+  bool is_flat_array() const {\n+    return (mask_bits(value(), flat_array_mask_in_place) == flat_array_pattern);\n+  }\n+\n+  bool is_null_free_array() const {\n+    return (mask_bits(value(), null_free_array_mask_in_place) == null_free_array_pattern);\n+  }\n+#else\n+  bool is_flat_array() const {\n+    fatal(\"Should not ask this for mark word, ask oopDesc\");\n+    return false;\n+  }\n+\n+  bool is_null_free_array() const {\n+    fatal(\"Should not ask this for mark word, ask oopDesc\");\n+    return false;\n+  }\n+#endif\n@@ -268,0 +407,14 @@\n+  static markWord inline_type_prototype() {\n+    return markWord(inline_type_pattern);\n+  }\n+\n+#ifdef _LP64 \/\/ 64 bit encodings only\n+  static markWord flat_array_prototype() {\n+    return markWord(flat_array_pattern);\n+  }\n+\n+  static markWord null_free_array_prototype() {\n+    return markWord(null_free_array_pattern);\n+  }\n+#endif\n+\n@@ -275,1 +428,4 @@\n-  inline void* decode_pointer() const { return (void*)clear_lock_bits().value(); }\n+  inline void* decode_pointer() const {\n+    return (EnableValhalla && _value < static_prototype_value_max) ? nullptr :\n+      (void*) (clear_lock_bits().value());\n+  }\n","filename":"src\/hotspot\/share\/oops\/markWord.hpp","additions":168,"deletions":12,"binary":false,"changes":180,"status":"modified"},{"patch":"@@ -127,1 +127,2 @@\n-  bool eliminate_boxing = EliminateAutoBox;\n+  \/\/ TODO 8328675 Re-enable\n+  bool eliminate_boxing = false; \/\/ EliminateAutoBox;\n@@ -645,0 +646,2 @@\n+  case vmIntrinsics::_makePrivateBuffer:\n+  case vmIntrinsics::_finishPrivateBuffer:\n@@ -654,0 +657,1 @@\n+  case vmIntrinsics::_getValue:\n@@ -663,0 +667,1 @@\n+  case vmIntrinsics::_putValue:\n@@ -745,0 +750,1 @@\n+  case vmIntrinsics::_isFlatArray:\n@@ -746,0 +752,1 @@\n+  case vmIntrinsics::_newNullRestrictedArray:\n","filename":"src\/hotspot\/share\/opto\/c2compiler.cpp","additions":8,"deletions":1,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -27,0 +27,1 @@\n+#include \"ci\/ciFlatArrayKlass.hpp\"\n@@ -327,0 +328,2 @@\n+  case vmIntrinsics::_makePrivateBuffer:        return inline_unsafe_make_private_buffer();\n+  case vmIntrinsics::_finishPrivateBuffer:      return inline_unsafe_finish_private_buffer();\n@@ -336,0 +339,1 @@\n+  case vmIntrinsics::_getValue:                 return inline_unsafe_access(!is_store, T_OBJECT,   Relaxed, false, true);\n@@ -346,0 +350,1 @@\n+  case vmIntrinsics::_putValue:                 return inline_unsafe_access( is_store, T_OBJECT,   Relaxed, false, true);\n@@ -513,0 +518,1 @@\n+  case vmIntrinsics::_isFlatArray:              return inline_unsafe_isFlatArray();\n@@ -525,0 +531,1 @@\n+  case vmIntrinsics::_newNullRestrictedArray:   return inline_newNullRestrictedArray();\n@@ -2245,0 +2252,1 @@\n+  bool null_free = false;\n@@ -2250,0 +2258,1 @@\n+      null_free = alias_type->field()->is_null_free();\n@@ -2258,0 +2267,1 @@\n+      null_free = adr_type->is_aryptr()->is_null_free();\n@@ -2270,0 +2280,3 @@\n+    if (null_free) {\n+      result = result->join_speculative(TypePtr::NOTNULL)->is_oopptr();\n+    }\n@@ -2300,1 +2313,1 @@\n-bool LibraryCallKit::inline_unsafe_access(bool is_store, const BasicType type, const AccessKind kind, const bool unaligned) {\n+bool LibraryCallKit::inline_unsafe_access(bool is_store, const BasicType type, const AccessKind kind, const bool unaligned, const bool is_flat) {\n@@ -2325,1 +2338,1 @@\n-      assert(sig->count() == 2, \"oop getter has 2 arguments\");\n+      assert(sig->count() == 2 || (is_flat && sig->count() == 3), \"oop getter has 2 or 3 arguments\");\n@@ -2331,1 +2344,1 @@\n-      assert(sig->count() == 3, \"oop putter has 3 arguments\");\n+      assert(sig->count() == 3 || (is_flat && sig->count() == 4), \"oop putter has 3 arguments\");\n@@ -2357,0 +2370,55 @@\n+\n+  ciInlineKlass* inline_klass = nullptr;\n+  if (is_flat) {\n+    const TypeInstPtr* cls = _gvn.type(argument(4))->isa_instptr();\n+    if (cls == nullptr || cls->const_oop() == nullptr) {\n+      return false;\n+    }\n+    ciType* mirror_type = cls->const_oop()->as_instance()->java_mirror_type();\n+    if (!mirror_type->is_inlinetype()) {\n+      return false;\n+    }\n+    inline_klass = mirror_type->as_inline_klass();\n+  }\n+\n+  if (base->is_InlineType()) {\n+    InlineTypeNode* vt = base->as_InlineType();\n+    if (is_store) {\n+      if (!vt->is_allocated(&_gvn)) {\n+        return false;\n+      }\n+      base = vt->get_oop();\n+    } else {\n+      if (offset->is_Con()) {\n+        long off = find_long_con(offset, 0);\n+        ciInlineKlass* vk = vt->type()->inline_klass();\n+        if ((long)(int)off != off || !vk->contains_field_offset(off)) {\n+          return false;\n+        }\n+\n+        ciField* field = vk->get_non_flat_field_by_offset(off);\n+        if (field != nullptr) {\n+          BasicType bt = type2field[field->type()->basic_type()];\n+          if (bt == T_ARRAY || bt == T_NARROWOOP) {\n+            bt = T_OBJECT;\n+          }\n+          if (bt == type && (!field->is_flat() || field->type() == inline_klass)) {\n+            Node* value = vt->field_value_by_offset(off, false);\n+            if (value->is_InlineType()) {\n+              value = value->as_InlineType()->adjust_scalarization_depth(this);\n+            }\n+            set_result(value);\n+            return true;\n+          }\n+        }\n+      }\n+      {\n+        \/\/ Re-execute the unsafe access if allocation triggers deoptimization.\n+        PreserveReexecuteState preexecs(this);\n+        jvms()->set_should_reexecute(true);\n+        vt = vt->buffer(this);\n+      }\n+      base = vt->get_oop();\n+    }\n+  }\n+\n@@ -2367,1 +2435,1 @@\n-    if (type != T_OBJECT) {\n+    if (type != T_OBJECT && (inline_klass == nullptr || !inline_klass->has_object_fields())) {\n@@ -2385,1 +2453,1 @@\n-  Node* val = is_store ? argument(4) : nullptr;\n+  Node* val = is_store ? argument(4 + (is_flat ? 1 : 0)) : nullptr;\n@@ -2406,1 +2474,22 @@\n-  BasicType bt = alias_type->basic_type();\n+  BasicType bt = T_ILLEGAL;\n+  ciField* field = nullptr;\n+  if (adr_type->isa_instptr()) {\n+    const TypeInstPtr* instptr = adr_type->is_instptr();\n+    ciInstanceKlass* k = instptr->instance_klass();\n+    int off = instptr->offset();\n+    if (instptr->const_oop() != nullptr &&\n+        k == ciEnv::current()->Class_klass() &&\n+        instptr->offset() >= (k->size_helper() * wordSize)) {\n+      k = instptr->const_oop()->as_instance()->java_lang_Class_klass()->as_instance_klass();\n+      field = k->get_field_by_offset(off, true);\n+    } else {\n+      field = k->get_non_flat_field_by_offset(off);\n+    }\n+    if (field != nullptr) {\n+      bt = type2field[field->type()->basic_type()];\n+    }\n+    assert(bt == alias_type->basic_type() || is_flat, \"should match\");\n+  } else {\n+    bt = alias_type->basic_type();\n+  }\n+\n@@ -2429,0 +2518,23 @@\n+  if (is_flat) {\n+    if (adr_type->isa_instptr()) {\n+      if (field == nullptr || field->type() != inline_klass) {\n+        mismatched = true;\n+      }\n+    } else if (adr_type->isa_aryptr()) {\n+      const Type* elem = adr_type->is_aryptr()->elem();\n+      if (!adr_type->is_flat() || elem->inline_klass() != inline_klass) {\n+        mismatched = true;\n+      }\n+    } else {\n+      mismatched = true;\n+    }\n+    if (is_store) {\n+      const Type* val_t = _gvn.type(val);\n+      if (!val_t->is_inlinetypeptr() || val_t->inline_klass() != inline_klass) {\n+        set_map(old_map);\n+        set_sp(old_sp);\n+        return false;\n+      }\n+    }\n+  }\n+\n@@ -2430,1 +2542,1 @@\n-  assert(!mismatched || alias_type->adr_type()->is_oopptr(), \"off-heap access can't be mismatched\");\n+  assert(!mismatched || is_flat || alias_type->adr_type()->is_oopptr(), \"off-heap access can't be mismatched\");\n@@ -2442,4 +2554,6 @@\n-  if (!is_store && type == T_OBJECT) {\n-    const TypeOopPtr* tjp = sharpen_unsafe_type(alias_type, adr_type);\n-    if (tjp != nullptr) {\n-      value_type = tjp;\n+  if (!is_store) {\n+    if (type == T_OBJECT && !is_flat) {\n+      const TypeOopPtr* tjp = sharpen_unsafe_type(alias_type, adr_type);\n+      if (tjp != nullptr) {\n+        value_type = tjp;\n+      }\n@@ -2461,2 +2575,2 @@\n-    ciField* field = alias_type->field();\n-    if (heap_base_oop != top() && field != nullptr && field->is_constant() && !mismatched) {\n+\n+    if (heap_base_oop != top() && field != nullptr && field->is_constant() && !field->is_flat() && !mismatched) {\n@@ -2468,1 +2582,16 @@\n-      p = access_load_at(heap_base_oop, adr, adr_type, value_type, type, decorators);\n+      if (is_flat) {\n+        if (adr_type->isa_instptr() && !mismatched) {\n+          ciInstanceKlass* holder = adr_type->is_instptr()->instance_klass();\n+          int offset = adr_type->is_instptr()->offset();\n+          p = InlineTypeNode::make_from_flat(this, inline_klass, base, base, holder, offset, decorators);\n+        } else {\n+          p = InlineTypeNode::make_from_flat(this, inline_klass, base, adr, nullptr, 0, decorators);\n+        }\n+      } else {\n+        p = access_load_at(heap_base_oop, adr, adr_type, value_type, type, decorators);\n+        const TypeOopPtr* ptr = value_type->make_oopptr();\n+        if (ptr != nullptr && ptr->is_inlinetypeptr()) {\n+          \/\/ Load a non-flattened inline type from memory\n+          p = InlineTypeNode::make_from_oop(this, p, ptr->inline_klass(), !ptr->maybe_null());\n+        }\n+      }\n@@ -2506,1 +2635,17 @@\n-    access_store_at(heap_base_oop, adr, adr_type, val, value_type, type, decorators);\n+    if (is_flat) {\n+      if (adr_type->isa_instptr() && !mismatched) {\n+        ciInstanceKlass* holder = adr_type->is_instptr()->instance_klass();\n+        int offset = adr_type->is_instptr()->offset();\n+        val->as_InlineType()->store_flat(this, base, base, holder, offset, decorators);\n+      } else {\n+        val->as_InlineType()->store_flat(this, base, adr, nullptr, 0, decorators);\n+      }\n+    } else {\n+      access_store_at(heap_base_oop, adr, adr_type, val, value_type, type, decorators);\n+    }\n+  }\n+\n+  if (argument(1)->is_InlineType() && is_store) {\n+    InlineTypeNode* value = InlineTypeNode::make_from_oop(this, base, _gvn.type(argument(1))->inline_klass());\n+    value = value->make_larval(this, false);\n+    replace_in_map(argument(1), value);\n@@ -2512,0 +2657,40 @@\n+bool LibraryCallKit::inline_unsafe_make_private_buffer() {\n+  Node* receiver = argument(0);\n+  Node* value = argument(1);\n+  if (!value->is_InlineType()) {\n+    return false;\n+  }\n+\n+  receiver = null_check(receiver);\n+  if (stopped()) {\n+    return true;\n+  }\n+\n+  set_result(value->as_InlineType()->make_larval(this, true));\n+  return true;\n+}\n+\n+bool LibraryCallKit::inline_unsafe_finish_private_buffer() {\n+  Node* receiver = argument(0);\n+  Node* buffer = argument(1);\n+  if (!buffer->is_InlineType()) {\n+    return false;\n+  }\n+  InlineTypeNode* vt = buffer->as_InlineType();\n+  if (!vt->is_allocated(&_gvn)) {\n+    return false;\n+  }\n+  \/\/ TODO 8239003 Why is this needed?\n+  if (AllocateNode::Ideal_allocation(vt->get_oop()) == nullptr) {\n+    return false;\n+  }\n+\n+  receiver = null_check(receiver);\n+  if (stopped()) {\n+    return true;\n+  }\n+\n+  set_result(vt->finish_larval(this));\n+  return true;\n+}\n+\n@@ -2717,0 +2902,13 @@\n+    if (oldval != nullptr && oldval->is_InlineType()) {\n+      \/\/ Re-execute the unsafe access if allocation triggers deoptimization.\n+      PreserveReexecuteState preexecs(this);\n+      jvms()->set_should_reexecute(true);\n+      oldval = oldval->as_InlineType()->buffer(this)->get_oop();\n+    }\n+    if (newval != nullptr && newval->is_InlineType()) {\n+      \/\/ Re-execute the unsafe access if allocation triggers deoptimization.\n+      PreserveReexecuteState preexecs(this);\n+      jvms()->set_should_reexecute(true);\n+      newval = newval->as_InlineType()->buffer(this)->get_oop();\n+    }\n+\n@@ -2903,2 +3101,7 @@\n-\n-  Node* obj = new_instance(kls, test);\n+  Node* obj = nullptr;\n+  const TypeInstKlassPtr* tkls = _gvn.type(kls)->isa_instklassptr();\n+  if (tkls != nullptr && tkls->instance_klass()->is_inlinetype()) {\n+    obj = InlineTypeNode::make_default(_gvn, tkls->instance_klass()->as_inline_klass())->buffer(this);\n+  } else {\n+    obj = new_instance(kls, test);\n+  }\n@@ -3698,1 +3901,1 @@\n-  const TypeAry* arr0 = TypeAry::make(etype, TypeInt::POS);\n+  const TypeAry* arr0 = TypeAry::make(etype, TypeInt::POS, \/* stable= *\/ false, \/* flat= *\/ false, \/* not_flat= *\/ true, \/* not_null_free= *\/ true);\n@@ -3703,1 +3906,1 @@\n-  const Type* objects_type = TypeAryPtr::make(TypePtr::BotPTR, arr0, objects_klass, xk, 0);\n+  const Type* objects_type = TypeAryPtr::make(TypePtr::BotPTR, arr0, objects_klass, xk, TypeAryPtr::Offset(0));\n@@ -3826,9 +4029,0 @@\n-\/\/---------------------------load_mirror_from_klass----------------------------\n-\/\/ Given a klass oop, load its java mirror (a java.lang.Class oop).\n-Node* LibraryCallKit::load_mirror_from_klass(Node* klass) {\n-  Node* p = basic_plus_adr(klass, in_bytes(Klass::java_mirror_offset()));\n-  Node* load = make_load(nullptr, p, TypeRawPtr::NOTNULL, T_ADDRESS, MemNode::unordered);\n-  \/\/ mirror = ((OopHandle)mirror)->resolve();\n-  return access_load(load, TypeInstPtr::MIRROR, T_OBJECT, IN_NATIVE);\n-}\n-\n@@ -3877,0 +4071,1 @@\n+\n@@ -4070,0 +4265,1 @@\n+\n@@ -4085,1 +4281,2 @@\n-  ciType* tm = mirror_con->java_mirror_type();\n+  bool is_null_free_array = false;\n+  ciType* tm = mirror_con->java_mirror_type(&is_null_free_array);\n@@ -4092,1 +4289,5 @@\n-      int static_res = C->static_subtype_check(TypeKlassPtr::make(tm->as_klass(), Type::trust_interfaces), tp->as_klass_type());\n+      const TypeKlassPtr* tklass = TypeKlassPtr::make(tm->as_klass(), Type::trust_interfaces);\n+      if (is_null_free_array) {\n+        tklass = tklass->is_aryklassptr()->cast_to_null_free();\n+      }\n+      int static_res = C->static_subtype_check(tklass, tp->as_klass_type());\n@@ -4121,2 +4322,2 @@\n-  \/\/ Not-subtype or the mirror's klass ptr is null (in case it is a primitive).\n-  enum { _bad_type_path = 1, _prim_path = 2, PATH_LIMIT };\n+  \/\/ Not-subtype or the mirror's klass ptr is nullptr (in case it is a primitive).\n+  enum { _bad_type_path = 1, _prim_path = 2, _npe_path = 3, PATH_LIMIT };\n@@ -4132,0 +4333,2 @@\n+  Node* io = i_o();\n+  Node* mem = merged_memory();\n@@ -4133,0 +4336,1 @@\n+\n@@ -4139,1 +4343,2 @@\n-      region->in(_bad_type_path) != top()) {\n+      region->in(_bad_type_path) != top() ||\n+      region->in(_npe_path) != top()) {\n@@ -4143,0 +4348,3 @@\n+    \/\/ Set IO and memory because gen_checkcast may override them when buffering inline types\n+    set_i_o(io);\n+    set_all_memory(mem);\n@@ -4176,0 +4384,1 @@\n+  RegionNode* prim_region = new RegionNode(2);\n@@ -4178,0 +4387,1 @@\n+  record_for_igvn(prim_region);\n@@ -4202,2 +4412,5 @@\n-    int prim_path = (which_arg == 0 ? _prim_0_path : _prim_1_path);\n-    region->init_req(prim_path, null_ctl);\n+    if (which_arg == 0) {\n+      prim_region->init_req(1, null_ctl);\n+    } else {\n+      region->init_req(_prim_1_path, null_ctl);\n+    }\n@@ -4213,1 +4426,0 @@\n-    \/\/ now we have a successful reference subtype check\n@@ -4220,1 +4432,2 @@\n-  set_control(region->in(_prim_0_path)); \/\/ go back to first null check\n+  \/\/ This path is also used if superc is a value mirror.\n+  set_control(_gvn.transform(prim_region));\n@@ -4225,1 +4438,1 @@\n-    generate_guard(bol_eq, region, PROB_FAIR);\n+    generate_fair_guard(bol_eq, region);\n@@ -4256,2 +4469,1 @@\n-Node* LibraryCallKit::generate_array_guard_common(Node* kls, RegionNode* region,\n-                                                  bool obj_array, bool not_array) {\n+Node* LibraryCallKit::generate_array_guard_common(Node* kls, RegionNode* region, ArrayKind kind) {\n@@ -4263,9 +4475,0 @@\n-  \/\/ If obj_array\/non_array==false\/false:\n-  \/\/ Branch around if the given klass is in fact an array (either obj or prim).\n-  \/\/ If obj_array\/non_array==false\/true:\n-  \/\/ Branch around if the given klass is not an array klass of any kind.\n-  \/\/ If obj_array\/non_array==true\/true:\n-  \/\/ Branch around if the kls is not an oop array (kls is int[], String, etc.)\n-  \/\/ If obj_array\/non_array==true\/false:\n-  \/\/ Branch around if the kls is an oop array (Object[] or subtype)\n-  \/\/\n@@ -4276,4 +4479,11 @@\n-    bool query = (obj_array\n-                  ? Klass::layout_helper_is_objArray(layout_con)\n-                  : Klass::layout_helper_is_array(layout_con));\n-    if (query == not_array) {\n+    bool query = 0;\n+    switch(kind) {\n+      case ObjectArray:    query = Klass::layout_helper_is_objArray(layout_con); break;\n+      case NonObjectArray: query = !Klass::layout_helper_is_objArray(layout_con); break;\n+      case TypeArray:      query = Klass::layout_helper_is_typeArray(layout_con); break;\n+      case AnyArray:       query = Klass::layout_helper_is_array(layout_con); break;\n+      case NonArray:       query = !Klass::layout_helper_is_array(layout_con); break;\n+      default:\n+        ShouldNotReachHere();\n+    }\n+    if (!query) {\n@@ -4289,0 +4499,21 @@\n+  unsigned int value = 0;\n+  BoolTest::mask btest = BoolTest::illegal;\n+  switch(kind) {\n+    case ObjectArray:\n+    case NonObjectArray: {\n+      value = Klass::_lh_array_tag_obj_value;\n+      layout_val = _gvn.transform(new RShiftINode(layout_val, intcon(Klass::_lh_array_tag_shift)));\n+      btest = (kind == ObjectArray) ? BoolTest::eq : BoolTest::ne;\n+      break;\n+    }\n+    case TypeArray: {\n+      value = Klass::_lh_array_tag_type_value;\n+      layout_val = _gvn.transform(new RShiftINode(layout_val, intcon(Klass::_lh_array_tag_shift)));\n+      btest = BoolTest::eq;\n+      break;\n+    }\n+    case AnyArray:    value = Klass::_lh_neutral_value; btest = BoolTest::lt; break;\n+    case NonArray:    value = Klass::_lh_neutral_value; btest = BoolTest::gt; break;\n+    default:\n+      ShouldNotReachHere();\n+  }\n@@ -4290,4 +4521,1 @@\n-  jint  nval = (obj_array\n-                ? (jint)(Klass::_lh_array_tag_type_value\n-                   <<    Klass::_lh_array_tag_shift)\n-                : Klass::_lh_neutral_value);\n+  jint nval = (jint)value;\n@@ -4295,3 +4523,0 @@\n-  BoolTest::mask btest = BoolTest::lt;  \/\/ correct for testing is_[obj]array\n-  \/\/ invert the test if we are looking for a non-array\n-  if (not_array)  btest = BoolTest(btest).negate();\n@@ -4302,0 +4527,26 @@\n+\/\/-----------------------inline_newNullRestrictedArray--------------------------\n+\/\/ public static native Object[] newNullRestrictedArray(Class<?> componentType, int length);\n+bool LibraryCallKit::inline_newNullRestrictedArray() {\n+  Node* componentType = argument(0);\n+  Node* length = argument(1);\n+\n+  const TypeInstPtr* tp = _gvn.type(componentType)->isa_instptr();\n+  if (tp != nullptr) {\n+    ciInstanceKlass* ik = tp->instance_klass();\n+    if (ik == C->env()->Class_klass()) {\n+      ciType* t = tp->java_mirror_type();\n+      if (t != nullptr && t->is_inlinetype()) {\n+        ciArrayKlass* array_klass = ciArrayKlass::make(t, true);\n+        if (array_klass->is_loaded() && array_klass->element_klass()->as_inline_klass()->is_initialized()) {\n+          const TypeAryKlassPtr* array_klass_type = TypeKlassPtr::make(array_klass, Type::trust_interfaces)->is_aryklassptr();\n+          array_klass_type = array_klass_type->cast_to_null_free();\n+          Node* obj = new_array(makecon(array_klass_type), length, 0, nullptr, false);  \/\/ no arguments to push\n+          set_result(obj);\n+          assert(gvn().type(obj)->is_aryptr()->is_null_free(), \"must be null-free\");\n+          return true;\n+        }\n+      }\n+    }\n+  }\n+  return false;\n+}\n@@ -4304,1 +4555,1 @@\n-\/\/ private static native Object java.lang.reflect.newArray(Class<?> componentType, int length);\n+\/\/ private static native Object java.lang.reflect.Array.newArray(Class<?> componentType, int length);\n@@ -4450,1 +4701,13 @@\n-    Node* not_objArray = generate_non_objArray_guard(klass_node, bailout);\n+    \/\/ Inline type array may have object field that would require a\n+    \/\/ write barrier. Conservatively, go to slow path.\n+    \/\/ TODO 8251971: Optimize for the case when flat src\/dst are later found\n+    \/\/ to not contain oops (i.e., move this check to the macro expansion phase).\n+    BarrierSetC2* bs = BarrierSet::barrier_set()->barrier_set_c2();\n+    const TypeAryPtr* orig_t = _gvn.type(original)->isa_aryptr();\n+    const TypeKlassPtr* tklass = _gvn.type(klass_node)->is_klassptr();\n+    bool exclude_flat = UseFlatArray && bs->array_copy_requires_gc_barriers(true, T_OBJECT, false, false, BarrierSetC2::Parsing) &&\n+                        \/\/ Can src array be flat and contain oops?\n+                        (orig_t == nullptr || (!orig_t->is_not_flat() && (!orig_t->is_flat() || orig_t->elem()->inline_klass()->contains_oops()))) &&\n+                        \/\/ Can dest array be flat and contain oops?\n+                        tklass->can_be_inline_array() && (!tklass->is_flat() || tklass->is_aryklassptr()->elem()->is_instklassptr()->instance_klass()->as_inline_klass()->contains_oops());\n+    Node* not_objArray = exclude_flat ? generate_non_objArray_guard(klass_node, bailout) : generate_typeArray_guard(klass_node, bailout);\n@@ -4454,1 +4717,1 @@\n-      const Type* akls = TypeKlassPtr::make(TypePtr::NotNull, ak, 0\/*offset*\/);\n+      const Type* akls = TypeKlassPtr::make(TypePtr::NotNull, ak, Type::Offset(0));\n@@ -4474,0 +4737,38 @@\n+    \/\/ Handle inline type arrays\n+    bool can_validate = !too_many_traps(Deoptimization::Reason_class_check);\n+    if (!stopped()) {\n+      \/\/ TODO JDK-8329224\n+      if (!orig_t->is_null_free()) {\n+        \/\/ Not statically known to be null free, add a check\n+        generate_fair_guard(null_free_array_test(original), bailout);\n+      }\n+      orig_t = _gvn.type(original)->isa_aryptr();\n+      if (orig_t != nullptr && orig_t->is_flat()) {\n+        \/\/ Src is flat, check that dest is flat as well\n+        if (exclude_flat) {\n+          \/\/ Dest can't be flat, bail out\n+          bailout->add_req(control());\n+          set_control(top());\n+        } else {\n+          generate_fair_guard(flat_array_test(klass_node, \/* flat = *\/ false), bailout);\n+        }\n+      } else if (UseFlatArray && (orig_t == nullptr || !orig_t->is_not_flat()) &&\n+                 \/\/ If dest is flat, src must be flat as well (guaranteed by src <: dest check if validated).\n+                 ((!tklass->is_flat() && tklass->can_be_inline_array()) || !can_validate)) {\n+        \/\/ Src might be flat and dest might not be flat. Go to the slow path if src is flat.\n+        \/\/ TODO 8251971: Optimize for the case when src\/dest are later found to be both flat.\n+        generate_fair_guard(flat_array_test(load_object_klass(original)), bailout);\n+        if (orig_t != nullptr) {\n+          orig_t = orig_t->cast_to_not_flat();\n+          original = _gvn.transform(new CheckCastPPNode(control(), original, orig_t));\n+        }\n+      }\n+      if (!can_validate) {\n+        \/\/ No validation. The subtype check emitted at macro expansion time will not go to the slow\n+        \/\/ path but call checkcast_arraycopy which can not handle flat\/null-free inline type arrays.\n+        \/\/ TODO 8251971: Optimize for the case when src\/dest are later found to be both flat\/null-free.\n+        generate_fair_guard(flat_array_test(klass_node), bailout);\n+        generate_fair_guard(null_free_array_test(original), bailout);\n+      }\n+    }\n+\n@@ -4519,1 +4820,1 @@\n-      if (!too_many_traps(Deoptimization::Reason_class_check)) {\n+      if (can_validate) {\n@@ -4605,1 +4906,1 @@\n-    const TypeTuple* range = tf->range();\n+    const TypeTuple* range = tf->range_cc();\n@@ -4609,1 +4910,1 @@\n-    tf = TypeFunc::make(tf->domain(), new_range);\n+    tf = TypeFunc::make(tf->domain_cc(), new_range);\n@@ -4666,1 +4967,8 @@\n-  Node* obj = nullptr;\n+  Node* obj = argument(0);\n+\n+  \/\/ Don't intrinsify hashcode on inline types for now.\n+  \/\/ The \"is locked\" runtime check below also serves as inline type check and goes to the slow path.\n+  if (gvn().type(obj)->is_inlinetypeptr()) {\n+    return false;\n+  }\n+\n@@ -4676,1 +4984,0 @@\n-    obj = argument(0);\n@@ -4717,1 +5024,2 @@\n-    Node *lock_mask      = _gvn.MakeConX(markWord::lock_mask_in_place);\n+  \/\/ This also serves as guard against inline types\n+    Node *lock_mask      = _gvn.MakeConX(markWord::inline_type_mask_in_place);\n@@ -4792,1 +5100,10 @@\n-  Node* obj = null_check_receiver();\n+  Node* obj = argument(0);\n+  if (obj->is_InlineType()) {\n+    const Type* t = _gvn.type(obj);\n+    if (t->maybe_null()) {\n+      null_check(obj);\n+    }\n+    set_result(makecon(TypeInstPtr::make(t->inline_klass()->java_mirror())));\n+    return true;\n+  }\n+  obj = null_check_receiver();\n@@ -5144,0 +5461,14 @@\n+\/\/----------------------inline_unsafe_isFlatArray------------------------\n+\/\/ public native boolean Unsafe.isFlatArray(Class<?> arrayClass);\n+\/\/ This intrinsic exploits assumptions made by the native implementation\n+\/\/ (arrayClass is neither null nor primitive) to avoid unnecessary null checks.\n+bool LibraryCallKit::inline_unsafe_isFlatArray() {\n+  Node* cls = argument(1);\n+  Node* p = basic_plus_adr(cls, java_lang_Class::klass_offset());\n+  Node* kls = _gvn.transform(LoadKlassNode::make(_gvn, nullptr, immutable_memory(), p,\n+                                                 TypeRawPtr::BOTTOM, TypeInstKlassPtr::OBJECT));\n+  Node* result = flat_array_test(kls);\n+  set_result(result);\n+  return true;\n+}\n+\n@@ -5214,1 +5545,2 @@\n-    Node* obj = null_check_receiver();\n+    Node* obj = argument(0);\n+    obj = null_check_receiver();\n@@ -5218,0 +5550,6 @@\n+    if (obj_type->is_inlinetypeptr()) {\n+      \/\/ If the object to clone is an inline type, we can simply return it (i.e. a nop) since inline types have\n+      \/\/ no identity.\n+      set_result(obj);\n+      return true;\n+    }\n@@ -5224,1 +5562,2 @@\n-        obj_type->speculative_type()->is_instance_klass()) {\n+        obj_type->speculative_type()->is_instance_klass() &&\n+        !obj_type->speculative_type()->is_inlinetype()) {\n@@ -5254,0 +5593,5 @@\n+    \/\/ We only go to the fast case code if we pass a number of guards.\n+    \/\/ The paths which do not pass are accumulated in the slow_region.\n+    RegionNode* slow_region = new RegionNode(1);\n+    record_for_igvn(slow_region);\n+\n@@ -5259,3 +5603,0 @@\n-      Node* obj_length = load_array_length(obj);\n-      Node* array_size = nullptr; \/\/ Size of the array without object alignment padding.\n-      Node* alloc_obj = new_array(obj_klass, obj_length, 0, &array_size, \/*deoptimize_on_exception=*\/true);\n@@ -5264,20 +5605,7 @@\n-      if (bs->array_copy_requires_gc_barriers(true, T_OBJECT, true, false, BarrierSetC2::Parsing)) {\n-        \/\/ If it is an oop array, it requires very special treatment,\n-        \/\/ because gc barriers are required when accessing the array.\n-        Node* is_obja = generate_objArray_guard(obj_klass, (RegionNode*)nullptr);\n-        if (is_obja != nullptr) {\n-          PreserveJVMState pjvms2(this);\n-          set_control(is_obja);\n-          \/\/ Generate a direct call to the right arraycopy function(s).\n-          \/\/ Clones are always tightly coupled.\n-          ArrayCopyNode* ac = ArrayCopyNode::make(this, true, obj, intcon(0), alloc_obj, intcon(0), obj_length, true, false);\n-          ac->set_clone_oop_array();\n-          Node* n = _gvn.transform(ac);\n-          assert(n == ac, \"cannot disappear\");\n-          ac->connect_outputs(this, \/*deoptimize_on_exception=*\/true);\n-\n-          result_reg->init_req(_objArray_path, control());\n-          result_val->init_req(_objArray_path, alloc_obj);\n-          result_i_o ->set_req(_objArray_path, i_o());\n-          result_mem ->set_req(_objArray_path, reset_memory());\n-        }\n+      const TypeAryPtr* ary_ptr = obj_type->isa_aryptr();\n+      if (UseFlatArray && bs->array_copy_requires_gc_barriers(true, T_OBJECT, true, false, BarrierSetC2::Expansion) &&\n+          obj_type->can_be_inline_array() &&\n+          (ary_ptr == nullptr || (!ary_ptr->is_not_flat() && (!ary_ptr->is_flat() || ary_ptr->elem()->inline_klass()->contains_oops())))) {\n+        \/\/ Flat inline type array may have object field that would require a\n+        \/\/ write barrier. Conservatively, go to slow path.\n+        generate_fair_guard(flat_array_test(obj_klass), slow_region);\n@@ -5285,7 +5613,0 @@\n-      \/\/ Otherwise, there are no barriers to worry about.\n-      \/\/ (We can dispense with card marks if we know the allocation\n-      \/\/  comes out of eden (TLAB)...  In fact, ReduceInitialCardMarks\n-      \/\/  causes the non-eden paths to take compensating steps to\n-      \/\/  simulate a fresh allocation, so that no further\n-      \/\/  card marks are required in compiled code to initialize\n-      \/\/  the object.)\n@@ -5294,7 +5615,43 @@\n-        copy_to_clone(obj, alloc_obj, array_size, true);\n-\n-        \/\/ Present the results of the copy.\n-        result_reg->init_req(_array_path, control());\n-        result_val->init_req(_array_path, alloc_obj);\n-        result_i_o ->set_req(_array_path, i_o());\n-        result_mem ->set_req(_array_path, reset_memory());\n+        Node* obj_length = load_array_length(obj);\n+        Node* array_size = nullptr; \/\/ Size of the array without object alignment padding.\n+        Node* alloc_obj = new_array(obj_klass, obj_length, 0, &array_size, \/*deoptimize_on_exception=*\/true);\n+\n+        BarrierSetC2* bs = BarrierSet::barrier_set()->barrier_set_c2();\n+        if (bs->array_copy_requires_gc_barriers(true, T_OBJECT, true, false, BarrierSetC2::Parsing)) {\n+          \/\/ If it is an oop array, it requires very special treatment,\n+          \/\/ because gc barriers are required when accessing the array.\n+          Node* is_obja = generate_objArray_guard(obj_klass, (RegionNode*)nullptr);\n+          if (is_obja != nullptr) {\n+            PreserveJVMState pjvms2(this);\n+            set_control(is_obja);\n+            \/\/ Generate a direct call to the right arraycopy function(s).\n+            \/\/ Clones are always tightly coupled.\n+            ArrayCopyNode* ac = ArrayCopyNode::make(this, true, obj, intcon(0), alloc_obj, intcon(0), obj_length, true, false);\n+            ac->set_clone_oop_array();\n+            Node* n = _gvn.transform(ac);\n+            assert(n == ac, \"cannot disappear\");\n+            ac->connect_outputs(this, \/*deoptimize_on_exception=*\/true);\n+\n+            result_reg->init_req(_objArray_path, control());\n+            result_val->init_req(_objArray_path, alloc_obj);\n+            result_i_o ->set_req(_objArray_path, i_o());\n+            result_mem ->set_req(_objArray_path, reset_memory());\n+          }\n+        }\n+        \/\/ Otherwise, there are no barriers to worry about.\n+        \/\/ (We can dispense with card marks if we know the allocation\n+        \/\/  comes out of eden (TLAB)...  In fact, ReduceInitialCardMarks\n+        \/\/  causes the non-eden paths to take compensating steps to\n+        \/\/  simulate a fresh allocation, so that no further\n+        \/\/  card marks are required in compiled code to initialize\n+        \/\/  the object.)\n+\n+        if (!stopped()) {\n+          copy_to_clone(obj, alloc_obj, array_size, true);\n+\n+          \/\/ Present the results of the copy.\n+          result_reg->init_req(_array_path, control());\n+          result_val->init_req(_array_path, alloc_obj);\n+          result_i_o ->set_req(_array_path, i_o());\n+          result_mem ->set_req(_array_path, reset_memory());\n+        }\n@@ -5304,4 +5661,0 @@\n-    \/\/ We only go to the instance fast case code if we pass a number of guards.\n-    \/\/ The paths which do not pass are accumulated in the slow_region.\n-    RegionNode* slow_region = new RegionNode(1);\n-    record_for_igvn(slow_region);\n@@ -5439,0 +5792,12 @@\n+  int adjustment = 1;\n+  const TypeAryKlassPtr* ary_klass_ptr = alloc->in(AllocateNode::KlassNode)->bottom_type()->is_aryklassptr();\n+  if (ary_klass_ptr->is_null_free()) {\n+    \/\/ A null-free, tightly coupled array allocation can only come from LibraryCallKit::inline_newNullRestrictedArray\n+    \/\/ which requires both the component type and the array length on stack for re-execution. Re-create and push\n+    \/\/ the component type.\n+    ciArrayKlass* klass = ary_klass_ptr->exact_klass()->as_array_klass();\n+    ciInstance* instance = klass->component_mirror_instance();\n+    const TypeInstPtr* t_instance = TypeInstPtr::make(instance);\n+    sfpt->ins_req(old_jvms->stkoff() + old_jvms->sp(), makecon(t_instance));\n+    adjustment++;\n+  }\n@@ -5440,5 +5805,5 @@\n-  sfpt->ins_req(old_jvms->stkoff() + old_jvms->sp(), alloc->in(AllocateNode::ALength));\n-  old_jvms->set_sp(old_jvms->sp()+1);\n-  old_jvms->set_monoff(old_jvms->monoff()+1);\n-  old_jvms->set_scloff(old_jvms->scloff()+1);\n-  old_jvms->set_endoff(old_jvms->endoff()+1);\n+  sfpt->ins_req(old_jvms->stkoff() + old_jvms->sp() + adjustment - 1, alloc->in(AllocateNode::ALength));\n+  old_jvms->set_sp(old_jvms->sp() + adjustment);\n+  old_jvms->set_monoff(old_jvms->monoff() + adjustment);\n+  old_jvms->set_scloff(old_jvms->scloff() + adjustment);\n+  old_jvms->set_endoff(old_jvms->endoff() + adjustment);\n@@ -5477,2 +5842,1 @@\n-    CallProjections callprojs;\n-    alloc->extract_projections(&callprojs, true);\n+    CallProjections* callprojs = alloc->extract_projections(true);\n@@ -5481,1 +5845,1 @@\n-    C->gvn_replace_by(callprojs.fallthrough_ioproj, alloc->in(TypeFunc::I_O));\n+    C->gvn_replace_by(callprojs->fallthrough_ioproj, alloc->in(TypeFunc::I_O));\n@@ -5523,1 +5887,1 @@\n-    set_i_o(callprojs.fallthrough_ioproj);\n+    set_i_o(callprojs->fallthrough_ioproj);\n@@ -5861,1 +6225,1 @@\n-    if (src_elem == dest_elem && src_elem == T_OBJECT) {\n+    if (src_elem == dest_elem && top_src->is_flat() == top_dest->is_flat() && src_elem == T_OBJECT) {\n@@ -5888,0 +6252,2 @@\n+          src_type = _gvn.type(src);\n+          top_src = src_type->isa_aryptr();\n@@ -5891,0 +6257,2 @@\n+          dest_type = _gvn.type(dest);\n+          top_dest = dest_type->isa_aryptr();\n@@ -5906,2 +6274,1 @@\n-      can_emit_guards &&\n-      !src->is_top() && !dest->is_top()) {\n+      can_emit_guards && !src->is_top() && !dest->is_top()) {\n@@ -5950,0 +6317,2 @@\n+      slow_region->add_req(not_subtype_ctrl);\n+    }\n@@ -5951,6 +6320,28 @@\n-      if (not_subtype_ctrl != top()) {\n-        PreserveJVMState pjvms(this);\n-        set_control(not_subtype_ctrl);\n-        uncommon_trap(Deoptimization::Reason_intrinsic,\n-                      Deoptimization::Action_make_not_entrant);\n-        assert(stopped(), \"Should be stopped\");\n+    const TypeKlassPtr* dest_klass_t = _gvn.type(dest_klass)->is_klassptr();\n+    const Type* toop = dest_klass_t->cast_to_exactness(false)->as_instance_type();\n+    src = _gvn.transform(new CheckCastPPNode(control(), src, toop));\n+    src_type = _gvn.type(src);\n+    top_src  = src_type->isa_aryptr();\n+\n+    \/\/ Handle flat inline type arrays (null-free arrays are handled by the subtype check above)\n+    if (!stopped() && UseFlatArray) {\n+      \/\/ If dest is flat, src must be flat as well (guaranteed by src <: dest check). Handle flat src here.\n+      assert(top_dest == nullptr || !top_dest->is_flat() || top_src->is_flat(), \"src array must be flat\");\n+      if (top_src != nullptr && top_src->is_flat()) {\n+        \/\/ Src is flat, check that dest is flat as well\n+        if (top_dest != nullptr && !top_dest->is_flat()) {\n+          generate_fair_guard(flat_array_test(dest_klass, \/* flat = *\/ false), slow_region);\n+          \/\/ Since dest is flat and src <: dest, dest must have the same type as src.\n+          top_dest = top_src->cast_to_exactness(false);\n+          assert(top_dest->is_flat(), \"dest must be flat\");\n+          dest = _gvn.transform(new CheckCastPPNode(control(), dest, top_dest));\n+        }\n+      } else if (top_src == nullptr || !top_src->is_not_flat()) {\n+        \/\/ Src might be flat and dest might not be flat. Go to the slow path if src is flat.\n+        \/\/ TODO 8251971: Optimize for the case when src\/dest are later found to be both flat.\n+        assert(top_dest == nullptr || !top_dest->is_flat(), \"dest array must not be flat\");\n+        generate_fair_guard(flat_array_test(src), slow_region);\n+        if (top_src != nullptr) {\n+          top_src = top_src->cast_to_not_flat();\n+          src = _gvn.transform(new CheckCastPPNode(control(), src, top_src));\n+        }\n@@ -5959,0 +6350,1 @@\n+\n@@ -5966,4 +6358,0 @@\n-\n-    const TypeKlassPtr* dest_klass_t = _gvn.type(dest_klass)->is_klassptr();\n-    const Type *toop = dest_klass_t->cast_to_exactness(false)->as_instance_type();\n-    src = _gvn.transform(new CheckCastPPNode(control(), src, toop));\n","filename":"src\/hotspot\/share\/opto\/library_call.cpp","additions":523,"deletions":135,"binary":false,"changes":658,"status":"modified"},{"patch":"@@ -31,0 +31,1 @@\n+#include \"opto\/inlinetypenode.hpp\"\n@@ -108,3 +109,11 @@\n-    if (!stopped() && result() != nullptr) {\n-      BasicType bt = result()->bottom_type()->basic_type();\n-      push_node(bt, result());\n+    Node* res = result();\n+    if (!stopped() && res != nullptr) {\n+      BasicType bt = res->bottom_type()->basic_type();\n+      if (C->inlining_incrementally() && res->is_InlineType()) {\n+        \/\/ The caller expects an oop when incrementally inlining an intrinsic that returns an\n+        \/\/ inline type. Make sure the call is re-executed if the allocation triggers a deoptimization.\n+        PreserveReexecuteState preexecs(this);\n+        jvms()->set_should_reexecute(true);\n+        res = res->as_InlineType()->buffer(this);\n+      }\n+      push_node(bt, res);\n@@ -141,1 +150,0 @@\n-  Node* load_mirror_from_klass(Node* klass);\n@@ -163,0 +171,9 @@\n+\n+  enum ArrayKind {\n+    AnyArray,\n+    NonArray,\n+    ObjectArray,\n+    NonObjectArray,\n+    TypeArray\n+  };\n+\n@@ -164,0 +181,1 @@\n+\n@@ -165,1 +183,1 @@\n-    return generate_array_guard_common(kls, region, false, false);\n+    return generate_array_guard_common(kls, region, AnyArray);\n@@ -168,1 +186,1 @@\n-    return generate_array_guard_common(kls, region, false, true);\n+    return generate_array_guard_common(kls, region, NonArray);\n@@ -171,1 +189,1 @@\n-    return generate_array_guard_common(kls, region, true, false);\n+    return generate_array_guard_common(kls, region, ObjectArray);\n@@ -174,1 +192,4 @@\n-    return generate_array_guard_common(kls, region, true, true);\n+    return generate_array_guard_common(kls, region, NonObjectArray);\n+  }\n+  Node* generate_typeArray_guard(Node* kls, RegionNode* region) {\n+    return generate_array_guard_common(kls, region, TypeArray);\n@@ -176,2 +197,1 @@\n-  Node* generate_array_guard_common(Node* kls, RegionNode* region,\n-                                    bool obj_array, bool not_array);\n+  Node* generate_array_guard_common(Node* kls, RegionNode* region, ArrayKind kind);\n@@ -227,1 +247,1 @@\n-  bool inline_unsafe_access(bool is_store, BasicType type, AccessKind kind, bool is_unaligned);\n+  bool inline_unsafe_access(bool is_store, BasicType type, AccessKind kind, bool is_unaligned, bool is_flat = false);\n@@ -231,0 +251,1 @@\n+  bool inline_newNullRestrictedArray();\n@@ -234,0 +255,3 @@\n+  bool inline_unsafe_isFlatArray();\n+  bool inline_unsafe_make_private_buffer();\n+  bool inline_unsafe_finish_private_buffer();\n@@ -260,0 +284,1 @@\n+  bool inline_primitive_Class_conversion(vmIntrinsics::ID id);\n","filename":"src\/hotspot\/share\/opto\/library_call.hpp","additions":36,"deletions":11,"binary":false,"changes":47,"status":"modified"},{"patch":"@@ -36,0 +36,1 @@\n+#include \"opto\/inlinetypenode.hpp\"\n@@ -67,0 +68,6 @@\n+  \/\/ Inline types should not be split through Phis because they cannot be merged\n+  \/\/ through Phi nodes but each value input needs to be merged individually.\n+  if (n->is_InlineType()) {\n+    return nullptr;\n+  }\n+\n@@ -763,0 +770,4 @@\n+      if (inp->isa_InlineType()) {\n+        \/\/ TODO 8302217 This prevents PhiNode::push_inline_types_through\n+        return nullptr;\n+      }\n@@ -1092,0 +1103,48 @@\n+\/\/ We can't use immutable memory for the flat array check because we are loading the mark word which is\n+\/\/ mutable. Although the bits we are interested in are immutable (we check for markWord::unlocked_value),\n+\/\/ we need to use raw memory to not break anti dependency analysis. Below code will attempt to still move\n+\/\/ flat array checks out of loops, mainly to enable loop unswitching.\n+void PhaseIdealLoop::move_flat_array_check_out_of_loop(Node* n) {\n+  \/\/ Skip checks for more than one array\n+  if (n->req() > 3) {\n+    return;\n+  }\n+  Node* mem = n->in(FlatArrayCheckNode::Memory);\n+  Node* array = n->in(FlatArrayCheckNode::ArrayOrKlass)->uncast();\n+  IdealLoopTree* check_loop = get_loop(get_ctrl(n));\n+  IdealLoopTree* ary_loop = get_loop(get_ctrl(array));\n+\n+  \/\/ Check if array is loop invariant\n+  if (!check_loop->is_member(ary_loop)) {\n+    \/\/ Walk up memory graph from the check until we leave the loop\n+    VectorSet wq;\n+    wq.set(mem->_idx);\n+    while (check_loop->is_member(get_loop(ctrl_or_self(mem)))) {\n+      if (mem->is_Phi()) {\n+        mem = mem->in(1);\n+      } else if (mem->is_MergeMem()) {\n+        mem = mem->as_MergeMem()->memory_at(Compile::AliasIdxRaw);\n+      } else if (mem->is_Proj()) {\n+        mem = mem->in(0);\n+      } else if (mem->is_MemBar() || mem->is_SafePoint()) {\n+        mem = mem->in(TypeFunc::Memory);\n+      } else if (mem->is_Store() || mem->is_LoadStore() || mem->is_ClearArray()) {\n+        mem = mem->in(MemNode::Memory);\n+      } else {\n+#ifdef ASSERT\n+        mem->dump();\n+#endif\n+        ShouldNotReachHere();\n+      }\n+      if (wq.test_set(mem->_idx)) {\n+        return;\n+      }\n+    }\n+    \/\/ Replace memory input and re-compute ctrl to move the check out of the loop\n+    _igvn.replace_input_of(n, 1, mem);\n+    set_ctrl_and_loop(n, get_early_ctrl(n));\n+    Node* bol = n->unique_out();\n+    set_ctrl_and_loop(bol, get_early_ctrl(bol));\n+  }\n+}\n+\n@@ -1104,0 +1163,6 @@\n+\n+  if (n->isa_FlatArrayCheck()) {\n+    move_flat_array_check_out_of_loop(n);\n+    return n;\n+  }\n+\n@@ -1379,0 +1444,98 @@\n+bool PhaseIdealLoop::flat_array_element_type_check(Node *n) {\n+  \/\/ If the CmpP is a subtype check for a value that has just been\n+  \/\/ loaded from an array, the subtype check guarantees the value\n+  \/\/ can't be stored in a flat array and the load of the value\n+  \/\/ happens with a flat array check then: push the type check\n+  \/\/ through the phi of the flat array check. This needs special\n+  \/\/ logic because the subtype check's input is not a phi but a\n+  \/\/ LoadKlass that must first be cloned through the phi.\n+  if (n->Opcode() != Op_CmpP) {\n+    return false;\n+  }\n+\n+  Node* klassptr = n->in(1);\n+  Node* klasscon = n->in(2);\n+\n+  if (klassptr->is_DecodeNarrowPtr()) {\n+    klassptr = klassptr->in(1);\n+  }\n+\n+  if (klassptr->Opcode() != Op_LoadKlass && klassptr->Opcode() != Op_LoadNKlass) {\n+    return false;\n+  }\n+\n+  if (!klasscon->is_Con()) {\n+    return false;\n+  }\n+\n+  Node* addr = klassptr->in(MemNode::Address);\n+\n+  if (!addr->is_AddP()) {\n+    return false;\n+  }\n+\n+  intptr_t offset;\n+  Node* obj = AddPNode::Ideal_base_and_offset(addr, &_igvn, offset);\n+\n+  if (obj == nullptr) {\n+    return false;\n+  }\n+\n+  assert(obj != nullptr && addr->in(AddPNode::Base) == addr->in(AddPNode::Address), \"malformed AddP?\");\n+  if (obj->Opcode() == Op_CastPP) {\n+    obj = obj->in(1);\n+  }\n+\n+  if (!obj->is_Phi()) {\n+    return false;\n+  }\n+\n+  Node* region = obj->in(0);\n+\n+  Node* phi = PhiNode::make_blank(region, n->in(1));\n+  for (uint i = 1; i < region->req(); i++) {\n+    Node* in = obj->in(i);\n+    Node* ctrl = region->in(i);\n+    if (addr->in(AddPNode::Base) != obj) {\n+      Node* cast = addr->in(AddPNode::Base);\n+      assert(cast->Opcode() == Op_CastPP && cast->in(0) != nullptr, \"inconsistent subgraph\");\n+      Node* cast_clone = cast->clone();\n+      cast_clone->set_req(0, ctrl);\n+      cast_clone->set_req(1, in);\n+      register_new_node(cast_clone, ctrl);\n+      const Type* tcast = cast_clone->Value(&_igvn);\n+      _igvn.set_type(cast_clone, tcast);\n+      cast_clone->as_Type()->set_type(tcast);\n+      in = cast_clone;\n+    }\n+    Node* addr_clone = addr->clone();\n+    addr_clone->set_req(AddPNode::Base, in);\n+    addr_clone->set_req(AddPNode::Address, in);\n+    register_new_node(addr_clone, ctrl);\n+    _igvn.set_type(addr_clone, addr_clone->Value(&_igvn));\n+    Node* klassptr_clone = klassptr->clone();\n+    klassptr_clone->set_req(2, addr_clone);\n+    register_new_node(klassptr_clone, ctrl);\n+    _igvn.set_type(klassptr_clone, klassptr_clone->Value(&_igvn));\n+    if (klassptr != n->in(1)) {\n+      Node* decode = n->in(1);\n+      assert(decode->is_DecodeNarrowPtr(), \"inconsistent subgraph\");\n+      Node* decode_clone = decode->clone();\n+      decode_clone->set_req(1, klassptr_clone);\n+      register_new_node(decode_clone, ctrl);\n+      _igvn.set_type(decode_clone, decode_clone->Value(&_igvn));\n+      klassptr_clone = decode_clone;\n+    }\n+    phi->set_req(i, klassptr_clone);\n+  }\n+  register_new_node(phi, region);\n+  Node* orig = n->in(1);\n+  _igvn.replace_input_of(n, 1, phi);\n+  split_if_with_blocks_post(n);\n+  if (n->outcnt() != 0) {\n+    _igvn.replace_input_of(n, 1, orig);\n+    _igvn.remove_dead_node(phi);\n+  }\n+  return true;\n+}\n+\n@@ -1385,0 +1548,4 @@\n+  if (flat_array_element_type_check(n)) {\n+    return;\n+  }\n+\n@@ -1532,0 +1699,5 @@\n+\n+  \/\/ Remove multiple allocations of the same inline type\n+  if (n->is_InlineType()) {\n+    n->as_InlineType()->remove_redundant_allocations(this);\n+  }\n@@ -2014,1 +2186,9 @@\n-  Node *sample_cmp = sample_bool->in(1);\n+  Node* sample_cmp = sample_bool->in(1);\n+  const Type* t = Type::TOP;\n+  const TypePtr* at = nullptr;\n+  if (sample_cmp->is_FlatArrayCheck()) {\n+    \/\/ Left input of a FlatArrayCheckNode is memory, set the (adr) type of the phi accordingly\n+    assert(sample_cmp->in(1)->bottom_type() == Type::MEMORY, \"unexpected input type\");\n+    t = Type::MEMORY;\n+    at = TypeRawPtr::BOTTOM;\n+  }\n@@ -2017,1 +2197,1 @@\n-  PhiNode *phi1 = new PhiNode(phi->in(0), Type::TOP);\n+  PhiNode *phi1 = new PhiNode(phi->in(0), t, at);\n","filename":"src\/hotspot\/share\/opto\/loopopts.cpp","additions":182,"deletions":2,"binary":false,"changes":184,"status":"modified"},{"patch":"@@ -27,0 +27,1 @@\n+#include \"ci\/ciFlatArrayKlass.hpp\"\n@@ -28,0 +29,1 @@\n+#include \"classfile\/systemDictionary.hpp\"\n@@ -42,0 +44,1 @@\n+#include \"opto\/inlinetypenode.hpp\"\n@@ -235,0 +238,2 @@\n+                     ->cast_to_not_flat(t_oop->is_aryptr()->is_not_flat())\n+                     ->cast_to_not_null_free(t_oop->is_aryptr()->is_not_null_free())\n@@ -261,1 +266,1 @@\n-               tp->isa_aryptr() &&        tp->offset() == Type::OffsetBot &&\n+        tp->isa_aryptr() &&        tp->offset() == Type::OffsetBot &&\n@@ -1014,1 +1019,1 @@\n-    return (eliminate_boxing && non_volatile) || is_stable_ary;\n+    return (eliminate_boxing && non_volatile) || is_stable_ary || tp->is_inlinetypeptr();\n@@ -1071,1 +1076,1 @@\n-      uint shift  = exact_log2(type2aelembytes(ary_elem));\n+      uint shift  = ary_t->is_flat() ? ary_t->flat_log_elem_size() : exact_log2(type2aelembytes(ary_elem));\n@@ -1190,1 +1195,1 @@\n-        const TypeVect* out_vt = as_LoadVector()->vect_type();\n+        const TypeVect* out_vt = is_Load() ? as_LoadVector()->vect_type() : as_StoreVector()->vect_type();\n@@ -1208,0 +1213,5 @@\n+      Node* default_value = ld_alloc->in(AllocateNode::DefaultValue);\n+      if (default_value != nullptr) {\n+        return default_value;\n+      }\n+      assert(ld_alloc->in(AllocateNode::RawDefaultValue) == nullptr, \"default value may not be null\");\n@@ -1275,0 +1285,17 @@\n+  \/\/ Loading from an InlineType? The InlineType has the values of\n+  \/\/ all fields as input. Look for the field with matching offset.\n+  Node* addr = in(Address);\n+  intptr_t offset;\n+  Node* base = AddPNode::Ideal_base_and_offset(addr, phase, offset);\n+  if (base != nullptr && base->is_InlineType() && offset > oopDesc::klass_offset_in_bytes()) {\n+    Node* value = base->as_InlineType()->field_value_by_offset((int)offset, true);\n+    if (value != nullptr) {\n+      if (Opcode() == Op_LoadN) {\n+        \/\/ Encode oop value if we are loading a narrow oop\n+        assert(!phase->type(value)->isa_narrowoop(), \"should already be decoded\");\n+        value = phase->transform(new EncodePNode(value, bottom_type()));\n+      }\n+      return value;\n+    }\n+  }\n+\n@@ -2053,0 +2080,1 @@\n+        && !ary->is_flat()\n@@ -2088,0 +2116,2 @@\n+            \/\/ Default value load\n+            tp->is_instptr()->instance_klass() == ciEnv::current()->Class_klass() ||\n@@ -2093,1 +2123,3 @@\n-    \/\/ Optimize loads from constant fields.\n+    BasicType bt = memory_type();\n+\n+    \/\/ Optimize loads from constant fields.\n@@ -2097,1 +2129,1 @@\n-      const Type* con_type = Type::make_constant_from_field(const_oop->as_instance(), off, is_unsigned(), memory_type());\n+      const Type* con_type = Type::make_constant_from_field(const_oop->as_instance(), off, is_unsigned(), bt);\n@@ -2232,1 +2264,0 @@\n-\n@@ -2235,1 +2266,10 @@\n-    return TypeX::make(markWord::prototype().value());\n+    if (EnableValhalla) {\n+      \/\/ The mark word may contain property bits (inline, flat, null-free)\n+      Node* klass_node = alloc->in(AllocateNode::KlassNode);\n+      const TypeKlassPtr* tkls = phase->type(klass_node)->isa_klassptr();\n+      if (tkls != nullptr && tkls->is_loaded() && tkls->klass_is_exact()) {\n+        return TypeX::make(tkls->exact_klass()->prototype_header().value());\n+      }\n+    } else {\n+      return TypeX::make(markWord::prototype().value());\n+    }\n@@ -2386,1 +2426,2 @@\n-Node* LoadKlassNode::make(PhaseGVN& gvn, Node* ctl, Node* mem, Node* adr, const TypePtr* at, const TypeKlassPtr* tk) {\n+Node* LoadKlassNode::make(PhaseGVN& gvn, Node* ctl, Node* mem, Node* adr, const TypePtr* at,\n+                          const TypeKlassPtr* tk) {\n@@ -2433,1 +2474,2 @@\n-      ciType* t = tinst->java_mirror_type();\n+      bool is_null_free_array = false;\n+      ciType* t = tinst->java_mirror_type(&is_null_free_array);\n@@ -2443,1 +2485,5 @@\n-          return TypeKlassPtr::make(ciArrayKlass::make(t), Type::trust_interfaces);\n+          const TypeKlassPtr* tklass = TypeKlassPtr::make(ciArrayKlass::make(t), Type::trust_interfaces);\n+          if (is_null_free_array) {\n+            tklass = tklass->is_aryklassptr()->cast_to_null_free();\n+          }\n+          return tklass;\n@@ -2450,1 +2496,5 @@\n-        return TypeKlassPtr::make(t->as_klass(), Type::trust_interfaces);\n+        const TypeKlassPtr* tklass = TypeKlassPtr::make(t->as_klass(), Type::trust_interfaces);\n+        if (is_null_free_array) {\n+          tklass = tklass->is_aryklassptr()->cast_to_null_free();\n+        }\n+        return tklass;\n@@ -2462,1 +2512,1 @@\n-  const TypeAryPtr *tary = tp->isa_aryptr();\n+  const TypeAryPtr* tary = tp->isa_aryptr();\n@@ -3462,1 +3512,1 @@\n-  {\n+  if (phase->C->get_adr_type(phase->C->get_alias_index(adr_type())) != TypeAryPtr::INLINES) {\n@@ -3482,0 +3532,1 @@\n+             (Opcode() == Op_StoreL && st->Opcode() == Op_StoreN) ||\n@@ -3608,2 +3659,1 @@\n-  if (result == this &&\n-      ReduceFieldZeroing && phase->type(val)->is_zero_type()) {\n+  if (result == this && ReduceFieldZeroing) {\n@@ -3611,1 +3661,2 @@\n-    if (mem->is_Proj() && mem->in(0)->is_Allocate()) {\n+    if (mem->is_Proj() && mem->in(0)->is_Allocate() &&\n+        (phase->type(val)->is_zero_type() || mem->in(0)->in(AllocateNode::DefaultValue) == val)) {\n@@ -3615,1 +3666,1 @@\n-    if (result == this) {\n+    if (result == this && phase->type(val)->is_zero_type()) {\n@@ -3800,3 +3851,7 @@\n-    Node* mem = my_store->as_MergeMem()->memory_at(oop_alias_idx());\n-    set_req_X(MemNode::OopStore, mem, phase);\n-    return this;\n+    if (oop_alias_idx() != phase->C->get_alias_index(TypeAryPtr::INLINES) ||\n+        phase->C->flat_accesses_share_alias()) {\n+      \/\/ The alias that was recorded is no longer accurate enough.\n+      Node* mem = my_store->as_MergeMem()->memory_at(oop_alias_idx());\n+      set_req_X(MemNode::OopStore, mem, phase);\n+      return this;\n+    }\n@@ -3973,1 +4028,1 @@\n-    return new ClearArrayNode(in(0), in(1), in(2), in(3), true);\n+    return new ClearArrayNode(in(0), in(1), in(2), in(3), in(4), true);\n@@ -3991,1 +4046,1 @@\n-  Node *zero = phase->makecon(TypeLong::ZERO);\n+  Node *val = in(4);\n@@ -3993,1 +4048,1 @@\n-  mem = new StoreLNode(in(0),mem,adr,atp,zero,MemNode::unordered,false);\n+  mem = new StoreLNode(in(0), mem, adr, atp, val, MemNode::unordered, false);\n@@ -3998,1 +4053,1 @@\n-    mem = new StoreLNode(in(0),mem,adr,atp,zero,MemNode::unordered,false);\n+    mem = new StoreLNode(in(0), mem, adr, atp, val, MemNode::unordered, false);\n@@ -4032,0 +4087,2 @@\n+                                   Node* val,\n+                                   Node* raw_val,\n@@ -4042,1 +4099,7 @@\n-    mem = StoreNode::make(*phase, ctl, mem, adr, atp, phase->zerocon(T_INT), T_INT, MemNode::unordered);\n+    if (val != nullptr) {\n+      assert(phase->type(val)->isa_narrowoop(), \"should be narrow oop\");\n+      mem = new StoreNNode(ctl, mem, adr, atp, val, MemNode::unordered);\n+    } else {\n+      assert(raw_val == nullptr, \"val may not be null\");\n+      mem = StoreNode::make(*phase, ctl, mem, adr, atp, phase->zerocon(T_INT), T_INT, MemNode::unordered);\n+    }\n@@ -4049,1 +4112,1 @@\n-  return clear_memory(ctl, mem, dest, phase->MakeConX(offset), end_offset, phase);\n+  return clear_memory(ctl, mem, dest, raw_val, phase->MakeConX(offset), end_offset, phase);\n@@ -4053,0 +4116,1 @@\n+                                   Node* raw_val,\n@@ -4075,1 +4139,4 @@\n-  mem = new ClearArrayNode(ctl, mem, zsize, adr, false);\n+  if (raw_val == nullptr) {\n+    raw_val = phase->MakeConX(0);\n+  }\n+  mem = new ClearArrayNode(ctl, mem, zsize, adr, raw_val, false);\n@@ -4080,0 +4147,2 @@\n+                                   Node* val,\n+                                   Node* raw_val,\n@@ -4094,1 +4163,1 @@\n-    mem = clear_memory(ctl, mem, dest,\n+    mem = clear_memory(ctl, mem, dest, val, raw_val,\n@@ -4101,1 +4170,7 @@\n-    mem = StoreNode::make(*phase, ctl, mem, adr, atp, phase->zerocon(T_INT), T_INT, MemNode::unordered);\n+    if (val != nullptr) {\n+      assert(phase->type(val)->isa_narrowoop(), \"should be narrow oop\");\n+      mem = new StoreNNode(ctl, mem, adr, atp, val, MemNode::unordered);\n+    } else {\n+      assert(raw_val == nullptr, \"val may not be null\");\n+      mem = StoreNode::make(*phase, ctl, mem, adr, atp, phase->zerocon(T_INT), T_INT, MemNode::unordered);\n+    }\n@@ -4247,1 +4322,1 @@\n-Node *MemBarNode::match( const ProjNode *proj, const Matcher *m ) {\n+Node *MemBarNode::match(const ProjNode *proj, const Matcher *m, const RegMask* mask) {\n@@ -4534,1 +4609,3 @@\n-  if (init == nullptr || init->is_complete())  return false;\n+  if (init == nullptr || init->is_complete()) {\n+    return false;\n+  }\n@@ -4713,0 +4790,6 @@\n+                if (base->is_Phi()) {\n+                  \/\/ In rare case, base may be a PhiNode and it may read\n+                  \/\/ the same memory slice between InitializeNode and store.\n+                  failed = true;\n+                  break;\n+                }\n@@ -5299,0 +5382,2 @@\n+                                              allocation()->in(AllocateNode::DefaultValue),\n+                                              allocation()->in(AllocateNode::RawDefaultValue),\n@@ -5358,0 +5443,2 @@\n+                                            allocation()->in(AllocateNode::DefaultValue),\n+                                            allocation()->in(AllocateNode::RawDefaultValue),\n","filename":"src\/hotspot\/share\/opto\/memnode.cpp","additions":119,"deletions":32,"binary":false,"changes":151,"status":"modified"},{"patch":"@@ -134,0 +134,4 @@\n+#ifdef ASSERT\n+  void set_adr_type(const TypePtr* adr_type) { _adr_type = adr_type; }\n+#endif\n+\n@@ -563,1 +567,0 @@\n-\n@@ -1102,0 +1105,1 @@\n+  bool _word_copy_only;\n@@ -1103,2 +1107,3 @@\n-  ClearArrayNode( Node *ctrl, Node *arymem, Node *word_cnt, Node *base, bool is_large)\n-    : Node(ctrl,arymem,word_cnt,base), _is_large(is_large) {\n+  ClearArrayNode( Node *ctrl, Node *arymem, Node *word_cnt, Node *base, Node* val, bool is_large)\n+    : Node(ctrl, arymem, word_cnt, base, val), _is_large(is_large),\n+      _word_copy_only(val->bottom_type()->isa_long() && (!val->bottom_type()->is_long()->is_con() || val->bottom_type()->is_long()->get_con() != 0)) {\n@@ -1116,0 +1121,1 @@\n+  bool word_copy_only() const { return _word_copy_only; }\n@@ -1122,0 +1128,2 @@\n+                            Node* val,\n+                            Node* raw_val,\n@@ -1126,0 +1134,2 @@\n+                            Node* val,\n+                            Node* raw_val,\n@@ -1130,0 +1140,1 @@\n+                            Node* raw_val,\n@@ -1181,1 +1192,1 @@\n-  virtual Node *match( const ProjNode *proj, const Matcher *m );\n+  virtual Node *match(const ProjNode *proj, const Matcher *m, const RegMask* mask);\n","filename":"src\/hotspot\/share\/opto\/memnode.hpp","additions":15,"deletions":4,"binary":false,"changes":19,"status":"modified"},{"patch":"@@ -37,0 +37,1 @@\n+#include \"opto\/inlinetypenode.hpp\"\n@@ -563,0 +564,3 @@\n+  if (n->is_InlineType()) {\n+    C->add_inline_type(n);\n+  }\n@@ -623,0 +627,3 @@\n+  if (is_InlineType()) {\n+    compile->remove_inline_type(this);\n+  }\n","filename":"src\/hotspot\/share\/opto\/node.cpp","additions":7,"deletions":0,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -88,0 +88,1 @@\n+class FlatArrayCheckNode;\n@@ -120,0 +121,1 @@\n+class MachPrologNode;\n@@ -126,0 +128,1 @@\n+class MachVEPNode;\n@@ -179,0 +182,1 @@\n+class InlineTypeNode;\n@@ -693,0 +697,1 @@\n+        DEFINE_CLASS_ID(Blackhole,        MemBar, 2)\n@@ -714,0 +719,2 @@\n+      DEFINE_CLASS_ID(MachProlog,       Mach, 8)\n+      DEFINE_CLASS_ID(MachVEP,          Mach, 9)\n@@ -743,1 +750,2 @@\n-      DEFINE_CLASS_ID(Con, Type, 8)\n+      DEFINE_CLASS_ID(InlineType, Type, 8)\n+      DEFINE_CLASS_ID(Con, Type, 9)\n@@ -745,2 +753,2 @@\n-      DEFINE_CLASS_ID(SafePointScalarMerge, Type, 9)\n-      DEFINE_CLASS_ID(Convert, Type, 10)\n+      DEFINE_CLASS_ID(SafePointScalarMerge, Type, 10)\n+      DEFINE_CLASS_ID(Convert, Type, 11)\n@@ -784,3 +792,4 @@\n-        DEFINE_CLASS_ID(FastLock,   Cmp, 0)\n-        DEFINE_CLASS_ID(FastUnlock, Cmp, 1)\n-        DEFINE_CLASS_ID(SubTypeCheck,Cmp, 2)\n+        DEFINE_CLASS_ID(FastLock,       Cmp, 0)\n+        DEFINE_CLASS_ID(FastUnlock,     Cmp, 1)\n+        DEFINE_CLASS_ID(SubTypeCheck,   Cmp, 2)\n+        DEFINE_CLASS_ID(FlatArrayCheck, Cmp, 3)\n@@ -892,0 +901,1 @@\n+  DEFINE_CLASS_QUERY(Blackhole)\n@@ -923,0 +933,1 @@\n+  DEFINE_CLASS_QUERY(FlatArrayCheck)\n@@ -955,0 +966,1 @@\n+  DEFINE_CLASS_QUERY(MachProlog)\n@@ -961,0 +973,1 @@\n+  DEFINE_CLASS_QUERY(MachVEP)\n@@ -995,0 +1008,1 @@\n+  DEFINE_CLASS_QUERY(InlineType)\n","filename":"src\/hotspot\/share\/opto\/node.hpp","additions":20,"deletions":6,"binary":false,"changes":26,"status":"modified"},{"patch":"@@ -33,0 +33,1 @@\n+#include \"opto\/inlinetypenode.hpp\"\n@@ -895,1 +896,8 @@\n-Node *CmpLNode::Ideal( PhaseGVN *phase, bool can_reshape ) {\n+\/\/------------------------------Ideal------------------------------------------\n+Node* CmpLNode::Ideal(PhaseGVN* phase, bool can_reshape) {\n+  Node* a = nullptr;\n+  Node* b = nullptr;\n+  if (is_double_null_check(phase, a, b) && (phase->type(a)->is_zero_type() || phase->type(b)->is_zero_type())) {\n+    \/\/ Degraded to a simple null check, use old acmp\n+    return new CmpPNode(a, b);\n+  }\n@@ -906,0 +914,25 @@\n+\/\/ Match double null check emitted by Compile::optimize_acmp()\n+bool CmpLNode::is_double_null_check(PhaseGVN* phase, Node*& a, Node*& b) const {\n+  if (in(1)->Opcode() == Op_OrL &&\n+      in(1)->in(1)->Opcode() == Op_CastP2X &&\n+      in(1)->in(2)->Opcode() == Op_CastP2X &&\n+      in(2)->bottom_type()->is_zero_type()) {\n+    assert(EnableValhalla, \"unexpected double null check\");\n+    a = in(1)->in(1)->in(1);\n+    b = in(1)->in(2)->in(1);\n+    return true;\n+  }\n+  return false;\n+}\n+\n+\/\/------------------------------Value------------------------------------------\n+const Type* CmpLNode::Value(PhaseGVN* phase) const {\n+  Node* a = nullptr;\n+  Node* b = nullptr;\n+  if (is_double_null_check(phase, a, b) && (!phase->type(a)->maybe_null() || !phase->type(b)->maybe_null())) {\n+    \/\/ One operand is never nullptr, emit constant false\n+    return TypeInt::CC_GT;\n+  }\n+  return SubNode::Value(phase);\n+}\n+\n@@ -1031,1 +1064,16 @@\n-\n+    if (!unrelated_classes) {\n+      \/\/ Handle inline type arrays\n+      if ((r0->flat_in_array() && r1->not_flat_in_array()) ||\n+          (r1->flat_in_array() && r0->not_flat_in_array())) {\n+        \/\/ One type is in flat arrays but the other type is not. Must be unrelated.\n+        unrelated_classes = true;\n+      } else if ((r0->is_not_flat() && r1->is_flat()) ||\n+                 (r1->is_not_flat() && r0->is_flat())) {\n+        \/\/ One type is a non-flat array and the other type is a flat array. Must be unrelated.\n+        unrelated_classes = true;\n+      } else if ((r0->is_not_null_free() && r1->is_null_free()) ||\n+                 (r1->is_not_null_free() && r0->is_null_free())) {\n+        \/\/ One type is a nullable array and the other type is a null-free array. Must be unrelated.\n+        unrelated_classes = true;\n+      }\n+    }\n@@ -1116,1 +1164,8 @@\n-Node *CmpPNode::Ideal( PhaseGVN *phase, bool can_reshape ) {\n+Node* CmpPNode::Ideal(PhaseGVN *phase, bool can_reshape) {\n+  \/\/ TODO 8284443 in(1) could be cast?\n+  if (in(1)->is_InlineType() && phase->type(in(2))->is_zero_type()) {\n+    \/\/ Null checking a scalarized but nullable inline type. Check the IsInit\n+    \/\/ input instead of the oop input to avoid keeping buffer allocations alive.\n+    return new CmpINode(in(1)->as_InlineType()->get_is_init(), phase->intcon(0));\n+  }\n+\n@@ -1188,0 +1243,7 @@\n+  \/\/ Do not fold the subtype check to an array klass pointer comparison for null-able inline type arrays\n+  \/\/ because null-free [LMyValue <: null-able [LMyValue but the klasses are different. Perform a full test.\n+  if (superklass->is_obj_array_klass() && !superklass->as_array_klass()->is_elem_null_free() &&\n+      superklass->as_array_klass()->element_klass()->is_inlinetype()) {\n+    return nullptr;\n+  }\n+\n@@ -1331,0 +1393,37 @@\n+\/\/=============================================================================\n+\/\/------------------------------Value------------------------------------------\n+const Type* FlatArrayCheckNode::Value(PhaseGVN* phase) const {\n+  bool all_not_flat = true;\n+  for (uint i = ArrayOrKlass; i < req(); ++i) {\n+    const Type* t = phase->type(in(i));\n+    if (t == Type::TOP) {\n+      return Type::TOP;\n+    }\n+    if (t->is_ptr()->is_flat()) {\n+      \/\/ One of the input arrays is flat, check always passes\n+      return TypeInt::CC_EQ;\n+    } else if (!t->is_ptr()->is_not_flat()) {\n+      \/\/ One of the input arrays might be flat\n+      all_not_flat = false;\n+    }\n+  }\n+  if (all_not_flat) {\n+    \/\/ None of the input arrays can be flat, check always fails\n+    return TypeInt::CC_GT;\n+  }\n+  return TypeInt::CC;\n+}\n+\n+\/\/------------------------------Ideal------------------------------------------\n+Node* FlatArrayCheckNode::Ideal(PhaseGVN* phase, bool can_reshape) {\n+  bool changed = false;\n+  \/\/ Remove inputs that are known to be non-flat\n+  for (uint i = ArrayOrKlass; i < req(); ++i) {\n+    const Type* t = phase->type(in(i));\n+    if (t->isa_ptr() && t->is_ptr()->is_not_flat()) {\n+      del_req(i--);\n+      changed = true;\n+    }\n+  }\n+  return changed ? this : nullptr;\n+}\n","filename":"src\/hotspot\/share\/opto\/subnode.cpp","additions":102,"deletions":3,"binary":false,"changes":105,"status":"modified"},{"patch":"@@ -208,1 +208,2 @@\n-  virtual Node *Ideal(PhaseGVN *phase, bool can_reshape);\n+  virtual Node* Ideal(PhaseGVN* phase, bool can_reshape);\n+  virtual const Type* Value(PhaseGVN* phase) const;\n@@ -210,0 +211,1 @@\n+  bool is_double_null_check(PhaseGVN* phase, Node*& a, Node*& b) const;\n@@ -300,0 +302,20 @@\n+\/\/--------------------------FlatArrayCheckNode---------------------------------\n+\/\/ Returns true if one of the input array objects or array klass ptrs (there\n+\/\/ can be multiple) is flat.\n+class FlatArrayCheckNode : public CmpNode {\n+public:\n+  enum {\n+    Control,\n+    Memory,\n+    ArrayOrKlass\n+  };\n+  FlatArrayCheckNode(Compile* C, Node* mem, Node* array_or_klass) : CmpNode(mem, array_or_klass) {\n+    init_class_id(Class_FlatArrayCheck);\n+    init_flags(Flag_is_macro);\n+    C->add_macro_node(this);\n+  }\n+  virtual int Opcode() const;\n+  virtual const Type* sub(const Type*, const Type*) const { ShouldNotReachHere(); return nullptr; }\n+  const Type* Value(PhaseGVN* phase) const;\n+  virtual Node* Ideal(PhaseGVN* phase, bool can_reshape);\n+};\n","filename":"src\/hotspot\/share\/opto\/subnode.hpp","additions":23,"deletions":1,"binary":false,"changes":24,"status":"modified"},{"patch":"@@ -56,0 +56,1 @@\n+#include \"memory\/iterator.inline.hpp\"\n@@ -65,0 +66,1 @@\n+#include \"oops\/compressedOops.inline.hpp\"\n@@ -72,0 +74,1 @@\n+#include \"oops\/objArrayOop.inline.hpp\"\n@@ -1905,0 +1908,85 @@\n+WB_ENTRY(jobjectArray, WB_getObjectsViaKlassOopMaps(JNIEnv* env, jobject wb, jobject thing))\n+  oop aoop = JNIHandles::resolve(thing);\n+  if (!aoop->is_instance()) {\n+    return nullptr;\n+  }\n+  instanceHandle ih(THREAD, (instanceOop) aoop);\n+  InstanceKlass* klass = InstanceKlass::cast(ih->klass());\n+  if (klass->nonstatic_oop_map_count() == 0) {\n+    return nullptr;\n+  }\n+  const OopMapBlock* map = klass->start_of_nonstatic_oop_maps();\n+  const OopMapBlock* const end = map + klass->nonstatic_oop_map_count();\n+  int oop_count = 0;\n+  while (map < end) {\n+    oop_count += map->count();\n+    map++;\n+  }\n+\n+  objArrayHandle result_array =\n+      oopFactory::new_objArray_handle(vmClasses::Object_klass(), oop_count, CHECK_NULL);\n+  map = klass->start_of_nonstatic_oop_maps();\n+  int index = 0;\n+  while (map < end) {\n+    int offset = map->offset();\n+    for (unsigned int j = 0; j < map->count(); j++) {\n+      result_array->obj_at_put(index++, ih->obj_field(offset));\n+      offset += heapOopSize;\n+    }\n+    map++;\n+  }\n+  return (jobjectArray)JNIHandles::make_local(THREAD, result_array());\n+WB_END\n+\n+class CollectOops : public BasicOopIterateClosure {\n+ public:\n+  GrowableArray<Handle>* array;\n+\n+  jobjectArray create_jni_result(JNIEnv* env, TRAPS) {\n+    objArrayHandle result_array =\n+        oopFactory::new_objArray_handle(vmClasses::Object_klass(), array->length(), CHECK_NULL);\n+    for (int i = 0 ; i < array->length(); i++) {\n+      result_array->obj_at_put(i, array->at(i)());\n+    }\n+    return (jobjectArray)JNIHandles::make_local(THREAD, result_array());\n+  }\n+\n+  void add_oop(oop o) {\n+    Handle oh = Handle(Thread::current(), o);\n+    \/\/ Value might be oop, but JLS can't see as Object, just iterate through it...\n+    if (oh != nullptr && oh->is_inline_type()) {\n+      oh->oop_iterate(this);\n+    } else {\n+      array->append(oh);\n+    }\n+  }\n+\n+  void do_oop(oop* o) { add_oop(HeapAccess<>::oop_load(o)); }\n+  void do_oop(narrowOop* v) { add_oop(HeapAccess<>::oop_load(v)); }\n+};\n+\n+WB_ENTRY(jobjectArray, WB_getObjectsViaOopIterator(JNIEnv* env, jobject wb, jobject thing))\n+  ResourceMark rm(thread);\n+  Handle objh(thread, JNIHandles::resolve(thing));\n+  GrowableArray<Handle>* array = new GrowableArray<Handle>(128);\n+  CollectOops collectOops;\n+  collectOops.array = array;\n+  objh->oop_iterate(&collectOops);\n+  return collectOops.create_jni_result(env, THREAD);\n+WB_END\n+\n+WB_ENTRY(jobjectArray, WB_getObjectsViaFrameOopIterator(JNIEnv* env, jobject wb, jint depth))\n+  ResourceMark rm(THREAD);\n+  GrowableArray<Handle>* array = new GrowableArray<Handle>(128);\n+  CollectOops collectOops;\n+  collectOops.array = array;\n+  StackFrameStream sfs(thread, false \/* update *\/, true \/* process_frames *\/);\n+  while (depth > 0) { \/\/ Skip the native WB API frame\n+    sfs.next();\n+    frame* f = sfs.current();\n+    f->oops_do(&collectOops, nullptr, sfs.register_map());\n+    depth--;\n+  }\n+  return collectOops.create_jni_result(env, THREAD);\n+WB_END\n+\n@@ -2871,0 +2959,6 @@\n+  {CC\"getObjectsViaKlassOopMaps0\",\n+      CC\"(Ljava\/lang\/Object;)[Ljava\/lang\/Object;\",    (void*)&WB_getObjectsViaKlassOopMaps},\n+  {CC\"getObjectsViaOopIterator0\",\n+          CC\"(Ljava\/lang\/Object;)[Ljava\/lang\/Object;\",(void*)&WB_getObjectsViaOopIterator},\n+  {CC\"getObjectsViaFrameOopIterator\",\n+      CC\"(I)[Ljava\/lang\/Object;\",                     (void*)&WB_getObjectsViaFrameOopIterator},\n","filename":"src\/hotspot\/share\/prims\/whitebox.cpp","additions":94,"deletions":0,"binary":false,"changes":94,"status":"modified"},{"patch":"@@ -78,0 +78,1 @@\n+#include <string.h>\n@@ -1773,1 +1774,0 @@\n-unsigned int patch_mod_count = 0;\n@@ -1820,0 +1820,10 @@\n+  if (AMD64_ONLY(false &&) AARCH64_ONLY(false &&) !FLAG_IS_DEFAULT(InlineTypePassFieldsAsArgs)) {\n+    FLAG_SET_CMDLINE(InlineTypePassFieldsAsArgs, false);\n+    warning(\"InlineTypePassFieldsAsArgs is not supported on this platform\");\n+  }\n+\n+  if (AMD64_ONLY(false &&) AARCH64_ONLY(false &&) !FLAG_IS_DEFAULT(InlineTypeReturnedAsFields)) {\n+    FLAG_SET_CMDLINE(InlineTypeReturnedAsFields, false);\n+    warning(\"InlineTypeReturnedAsFields is not supported on this platform\");\n+  }\n+\n@@ -1950,2 +1960,0 @@\n-  bool patch_mod_javabase = false;\n-\n@@ -1965,1 +1973,1 @@\n-  jint result = parse_each_vm_init_arg(vm_options_args, &patch_mod_javabase, JVMFlagOrigin::JIMAGE_RESOURCE);\n+  jint result = parse_each_vm_init_arg(vm_options_args, JVMFlagOrigin::JIMAGE_RESOURCE);\n@@ -1972,1 +1980,1 @@\n-  result = parse_each_vm_init_arg(java_tool_options_args, &patch_mod_javabase, JVMFlagOrigin::ENVIRON_VAR);\n+  result = parse_each_vm_init_arg(java_tool_options_args, JVMFlagOrigin::ENVIRON_VAR);\n@@ -1978,1 +1986,1 @@\n-  result = parse_each_vm_init_arg(cmd_line_args, &patch_mod_javabase, JVMFlagOrigin::COMMAND_LINE);\n+  result = parse_each_vm_init_arg(cmd_line_args, JVMFlagOrigin::COMMAND_LINE);\n@@ -1985,1 +1993,1 @@\n-  result = parse_each_vm_init_arg(java_options_args, &patch_mod_javabase, JVMFlagOrigin::ENVIRON_VAR);\n+  result = parse_each_vm_init_arg(java_options_args, JVMFlagOrigin::ENVIRON_VAR);\n@@ -2006,1 +2014,1 @@\n-  result = finalize_vm_init_args(patch_mod_javabase);\n+  result = finalize_vm_init_args();\n@@ -2061,1 +2069,1 @@\n-int Arguments::process_patch_mod_option(const char* patch_mod_tail, bool* patch_mod_javabase) {\n+int Arguments::process_patch_mod_option(const char* patch_mod_tail) {\n@@ -2077,1 +2085,1 @@\n-      add_patch_mod_prefix(module_name, module_equal + 1, patch_mod_javabase);\n+      add_patch_mod_prefix(module_name, module_equal + 1, false \/* no append *\/, false \/* no cds *\/);\n@@ -2079,3 +2087,0 @@\n-      if (!create_numbered_module_property(\"jdk.module.patch\", patch_mod_tail, patch_mod_count++)) {\n-        return JNI_ENOMEM;\n-      }\n@@ -2089,0 +2094,64 @@\n+\/\/ VALUECLASS_STR must match string used in the build\n+#define VALUECLASS_STR \"valueclasses\"\n+#define VALUECLASS_JAR \"-\" VALUECLASS_STR \".jar\"\n+\n+\/\/ Finalize --patch-module args and --enable-preview related to value class module patches.\n+\/\/ Create all numbered properties passing module patches.\n+int Arguments::finalize_patch_module() {\n+  \/\/ If --enable-preview and EnableValhalla is true, each module may have value classes that\n+  \/\/ are to be patched into the module.\n+  \/\/ For each <module>-valueclasses.jar in <JAVA_HOME>\/lib\/valueclasses\/\n+  \/\/ appends the equivalent of --patch-module <module>=<JAVA_HOME>\/lib\/valueclasses\/<module>-valueclasses.jar\n+  if (enable_preview() && EnableValhalla) {\n+    char * valueclasses_dir = AllocateHeap(JVM_MAXPATHLEN, mtArguments);\n+    const char * fileSep = os::file_separator();\n+\n+    jio_snprintf(valueclasses_dir, JVM_MAXPATHLEN, \"%s%slib%s\" VALUECLASS_STR \"%s\",\n+                 Arguments::get_java_home(), fileSep, fileSep, fileSep);\n+    DIR* dir = os::opendir(valueclasses_dir);\n+    if (dir != nullptr) {\n+      char * module_name = AllocateHeap(JVM_MAXPATHLEN, mtArguments);\n+      char * path = AllocateHeap(JVM_MAXPATHLEN, mtArguments);\n+\n+      for (dirent * entry = os::readdir(dir); entry != nullptr; entry = os::readdir(dir)) {\n+        \/\/ Test if file ends-with \"-valueclasses.jar\"\n+        int len = (int)strlen(entry->d_name) - (sizeof(VALUECLASS_JAR) - 1);\n+        if (len <= 0 || strcmp(&entry->d_name[len], VALUECLASS_JAR) != 0) {\n+          continue;         \/\/ too short or not the expected suffix\n+        }\n+\n+        strcpy(module_name, entry->d_name);\n+        module_name[len] = '\\0';     \/\/ truncate to just module-name\n+\n+        jio_snprintf(path, JVM_MAXPATHLEN, \"%s%s\", valueclasses_dir, &entry->d_name);\n+        add_patch_mod_prefix(module_name, path, true \/* append *\/, true \/* cds OK*\/);\n+        log_info(class)(\"--enable-preview appending value classes for module %s: %s\", module_name, entry->d_name);\n+      }\n+      FreeHeap(module_name);\n+      FreeHeap(path);\n+      os::closedir(dir);\n+    }\n+    FreeHeap(valueclasses_dir);\n+  }\n+\n+  \/\/ Create numbered properties for each module that has been patched either\n+  \/\/ by --patch-module or --enable-preview\n+  \/\/ Format is \"jdk.module.patch.<n>=<module_name>=<path>\"\n+  if (_patch_mod_prefix != nullptr) {\n+    char * prop_value = AllocateHeap(JVM_MAXPATHLEN + JVM_MAXPATHLEN + 1, mtArguments);\n+    unsigned int patch_mod_count = 0;\n+\n+    for (GrowableArrayIterator<ModulePatchPath *> it = _patch_mod_prefix->begin();\n+            it != _patch_mod_prefix->end(); ++it) {\n+      jio_snprintf(prop_value, JVM_MAXPATHLEN + JVM_MAXPATHLEN + 1, \"%s=%s\",\n+                   (*it)->module_name(), (*it)->path_string());\n+      if (!create_numbered_module_property(\"jdk.module.patch\", prop_value, patch_mod_count++)) {\n+        FreeHeap(prop_value);\n+        return JNI_ENOMEM;\n+      }\n+    }\n+    FreeHeap(prop_value);\n+  }\n+  return JNI_OK;\n+}\n+\n@@ -2143,1 +2212,1 @@\n-jint Arguments::parse_each_vm_init_arg(const JavaVMInitArgs* args, bool* patch_mod_javabase, JVMFlagOrigin origin) {\n+jint Arguments::parse_each_vm_init_arg(const JavaVMInitArgs* args, JVMFlagOrigin origin) {\n@@ -2270,1 +2339,1 @@\n-      int res = process_patch_mod_option(tail, patch_mod_javabase);\n+      int res = process_patch_mod_option(tail);\n@@ -2340,0 +2409,4 @@\n+      \/\/ --enable-preview enables Valhalla, EnableValhalla VM option will eventually be removed before integration\n+      if (FLAG_SET_CMDLINE(EnableValhalla, true) != JVMFlag::SUCCESS) {\n+        return JNI_EINVAL;\n+      }\n@@ -2815,10 +2888,5 @@\n-void Arguments::add_patch_mod_prefix(const char* module_name, const char* path, bool* patch_mod_javabase) {\n-  \/\/ For java.base check for duplicate --patch-module options being specified on the command line.\n-  \/\/ This check is only required for java.base, all other duplicate module specifications\n-  \/\/ will be checked during module system initialization.  The module system initialization\n-  \/\/ will throw an ExceptionInInitializerError if this situation occurs.\n-  if (strcmp(module_name, JAVA_BASE_NAME) == 0) {\n-    if (*patch_mod_javabase) {\n-      vm_exit_during_initialization(\"Cannot specify \" JAVA_BASE_NAME \" more than once to --patch-module\");\n-    } else {\n-      *patch_mod_javabase = true;\n+void Arguments::add_patch_mod_prefix(const char* module_name, const char* path, bool allow_append, bool allow_cds) {\n+  if (!allow_cds) {\n+    CDSConfig::set_module_patching_disables_cds();\n+    if (strcmp(module_name, JAVA_BASE_NAME) == 0) {\n+      CDSConfig::set_java_base_module_patching_disables_cds();\n@@ -2833,1 +2901,18 @@\n-  _patch_mod_prefix->push(new ModulePatchPath(module_name, path));\n+  \/\/ Scan patches for matching module\n+  int i = _patch_mod_prefix->find_if([&](ModulePatchPath* patch) {\n+    return (strcmp(module_name, patch->module_name()) == 0);\n+  });\n+  if (i == -1) {\n+    _patch_mod_prefix->push(new ModulePatchPath(module_name, path));\n+  } else {\n+    if (allow_append) {\n+      \/\/ append path to existing module entry\n+      _patch_mod_prefix->at(i)->append_path(path);\n+    } else {\n+      if (strcmp(module_name, JAVA_BASE_NAME) == 0) {\n+        vm_exit_during_initialization(\"Cannot specify \" JAVA_BASE_NAME \" more than once to --patch-module\");\n+      } else {\n+        vm_exit_during_initialization(\"Cannot specify a module more than once to --patch-module\", module_name);\n+      }\n+    }\n+  }\n@@ -2876,1 +2961,1 @@\n-jint Arguments::finalize_vm_init_args(bool patch_mod_javabase) {\n+jint Arguments::finalize_vm_init_args() {\n@@ -2950,1 +3035,6 @@\n-  if (!CDSConfig::check_vm_args_consistency(patch_mod_javabase, mode_flag_cmd_line)) {\n+  \/\/ finalize --module-patch and related --enable-preview\n+  if (finalize_patch_module() != JNI_OK) {\n+    return JNI_ERR;\n+  }\n+\n+  if (!CDSConfig::check_vm_args_consistency(mode_flag_cmd_line)) {\n@@ -3720,0 +3810,7 @@\n+  if (!EnableValhalla || (is_interpreter_only() && !CDSConfig::is_dumping_archive() && !UseSharedSpaces)) {\n+    \/\/ Disable calling convention optimizations if inline types are not supported.\n+    \/\/ Also these aren't useful in -Xint. However, don't disable them when dumping or using\n+    \/\/ the CDS archive, as the values must match between dumptime and runtime.\n+    InlineTypePassFieldsAsArgs = false;\n+    InlineTypeReturnedAsFields = false;\n+  }\n","filename":"src\/hotspot\/share\/runtime\/arguments.cpp","additions":125,"deletions":28,"binary":false,"changes":153,"status":"modified"},{"patch":"@@ -43,0 +43,1 @@\n+#include \"oops\/inlineKlass.hpp\"\n@@ -63,0 +64,3 @@\n+#ifdef COMPILER1\n+#include \"c1\/c1_Runtime1.hpp\"\n+#endif\n@@ -362,0 +366,19 @@\n+\n+#ifdef COMPILER1\n+  if (nm->is_compiled_by_c1() && nm->method()->has_scalarized_args() &&\n+      pc() < nm->verified_inline_entry_point()) {\n+    \/\/ The VEP and VIEP(RO) of C1-compiled methods call into the runtime to buffer scalarized value\n+    \/\/ type args. We can't deoptimize at that point because the buffers have not yet been initialized.\n+    \/\/ Also, if the method is synchronized, we first need to acquire the lock.\n+    \/\/ Don't patch the return pc to delay deoptimization until we enter the method body (the check\n+    \/\/ added in LIRGenerator::do_Base will detect the pending deoptimization by checking the original_pc).\n+#if defined ASSERT && !defined AARCH64   \/\/ Stub call site does not look like NativeCall on AArch64\n+    NativeCall* call = nativeCall_before(this->pc());\n+    address dest = call->destination();\n+    assert(dest == Runtime1::entry_for(Runtime1::buffer_inline_args_no_receiver_id) ||\n+           dest == Runtime1::entry_for(Runtime1::buffer_inline_args_id), \"unexpected safepoint in entry point\");\n+#endif\n+    return;\n+  }\n+#endif\n+\n@@ -748,1 +771,1 @@\n-                          OopClosure* f) {\n+                          OopClosure* f, BufferedValueClosure* bvt_f) {\n@@ -760,1 +783,3 @@\n-      _f->do_oop(addr);\n+      if (_f != nullptr) {\n+        _f->do_oop(addr);\n+      }\n@@ -772,1 +797,3 @@\n-        _f->do_oop(addr);\n+        if (_f != nullptr) {\n+          _f->do_oop(addr);\n+        }\n@@ -947,1 +974,1 @@\n-  InterpreterFrameClosure blk(this, max_locals, m->max_stack(), f);\n+  InterpreterFrameClosure blk(this, max_locals, m->max_stack(), f, nullptr);\n@@ -959,0 +986,17 @@\n+void frame::buffered_values_interpreted_do(BufferedValueClosure* f) {\n+  assert(is_interpreted_frame(), \"Not an interpreted frame\");\n+  Thread *thread = Thread::current();\n+  methodHandle m (thread, interpreter_frame_method());\n+  jint      bci = interpreter_frame_bci();\n+\n+  assert(m->is_method(), \"checking frame value\");\n+  assert(!m->is_native() && bci >= 0 && bci < m->code_size(),\n+         \"invalid bci value\");\n+\n+  InterpreterFrameClosure blk(this, m->max_locals(), m->max_stack(), nullptr, f);\n+\n+  \/\/ process locals & expression stack\n+  InterpreterOopMap mask;\n+  m->mask_for(bci, &mask);\n+  mask.iterate_oop(&blk);\n+}\n@@ -1011,0 +1055,1 @@\n+    assert(_offset < _arg_size, \"out of bounds\");\n@@ -1037,5 +1082,1 @@\n-    _arg_size  = ArgumentSizeComputer(signature).size() + (has_receiver ? 1 : 0) + (has_appendix ? 1 : 0);\n-\n-    int arg_size;\n-    _regs = SharedRuntime::find_callee_arguments(signature, has_receiver, has_appendix, &arg_size);\n-    assert(arg_size == _arg_size, \"wrong arg size\");\n+    _regs = SharedRuntime::find_callee_arguments(signature, has_receiver, has_appendix, &_arg_size);\n","filename":"src\/hotspot\/share\/runtime\/frame.cpp","additions":50,"deletions":9,"binary":false,"changes":59,"status":"modified"},{"patch":"@@ -457,0 +457,1 @@\n+  void buffered_values_interpreted_do(BufferedValueClosure* f);\n","filename":"src\/hotspot\/share\/runtime\/frame.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -51,0 +51,1 @@\n+#include \"oops\/inlineKlass.hpp\"\n","filename":"src\/hotspot\/share\/runtime\/javaThread.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -43,0 +43,1 @@\n+import java.lang.reflect.ClassFileFormatVersion;\n@@ -2763,0 +2764,4 @@\n+            public int classFileFormatVersion(Class<?> clazz) {\n+                return clazz.getClassFileVersion();\n+            }\n+\n","filename":"src\/java.base\/share\/classes\/java\/lang\/System.java","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -469,0 +469,1 @@\n+ *     | LoadableDescriptorsAttribute?(List<Utf8Entry> loadableDescriptors)\n","filename":"src\/java.base\/share\/classes\/java\/lang\/classfile\/package-info.java","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -52,1 +52,0 @@\n-import jdk.internal.constant.ConstantUtils;\n@@ -55,0 +54,1 @@\n+import jdk.internal.constant.ConstantUtils;\n@@ -707,1 +707,1 @@\n-                    clb.withFlags(AccessFlag.FINAL, AccessFlag.SUPER, AccessFlag.SYNTHETIC)\n+                    clb.withFlags(AccessFlag.FINAL, (PreviewFeatures.isEnabled())  ? AccessFlag.IDENTITY : AccessFlag.SUPER, AccessFlag.SYNTHETIC)\n","filename":"src\/java.base\/share\/classes\/java\/lang\/runtime\/SwitchBootstraps.java","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -136,0 +136,1 @@\n+@jdk.internal.MigratedValueClass\n","filename":"src\/java.base\/share\/classes\/java\/time\/LocalDateTime.java","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -129,0 +129,1 @@\n+@jdk.internal.MigratedValueClass\n","filename":"src\/java.base\/share\/classes\/java\/time\/OffsetDateTime.java","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -119,0 +119,1 @@\n+@jdk.internal.MigratedValueClass\n","filename":"src\/java.base\/share\/classes\/java\/time\/OffsetTime.java","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -165,0 +165,1 @@\n+@jdk.internal.MigratedValueClass\n","filename":"src\/java.base\/share\/classes\/java\/time\/ZonedDateTime.java","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -35,0 +35,1 @@\n+import java.lang.reflect.ClassFileFormatVersion;\n@@ -589,0 +590,6 @@\n+\n+    \/**\n+     * Returns the class file format version of the class.\n+     *\/\n+    int classFileFormatVersion(Class<?> klass);\n+\n","filename":"src\/java.base\/share\/classes\/jdk\/internal\/access\/JavaLangAccess.java","additions":7,"deletions":0,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -942,0 +942,17 @@\n+    public static final class BoundLoadableDescriptorsAttribute extends BoundAttribute<LoadableDescriptorsAttribute>\n+            implements LoadableDescriptorsAttribute {\n+        private List<Utf8Entry> loadableDescriptors = null;\n+\n+        public BoundLoadableDescriptorsAttribute(ClassReader cf, AttributeMapper<LoadableDescriptorsAttribute> mapper, int pos) {\n+            super(cf, mapper, pos);\n+        }\n+\n+        @Override\n+        public List<Utf8Entry> loadableDescriptors() {\n+            if (loadableDescriptors == null) {\n+                loadableDescriptors = readEntryList(payloadStart, Utf8Entry.class);\n+            }\n+            return loadableDescriptors;\n+        }\n+    }\n+\n@@ -1021,0 +1038,2 @@\n+           case 0x5f348b64 ->\n+                name.equalsString(NAME_LOADABLE_DESCRIPTORS) ? loadableDescriptors() : null;\n","filename":"src\/java.base\/share\/classes\/jdk\/internal\/classfile\/impl\/BoundAttribute.java","additions":19,"deletions":0,"binary":false,"changes":19,"status":"modified"},{"patch":"@@ -139,1 +139,0 @@\n-\n@@ -273,0 +272,2 @@\n+    exports jdk.internal.value to  \/\/ Needed by Unsafe\n+        jdk.unsupported;\n","filename":"src\/java.base\/share\/classes\/module-info.java","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -234,0 +234,15 @@\n+    public boolean isValueClass() {\n+        return false;\n+    }\n+\n+    public boolean isIdentityClass() {\n+        return false;\n+    }\n+\n+    \/\/ Does this type need to be preloaded in the context of the referring class ??\n+    public boolean requiresLoadableDescriptors(Symbol referringClass) {\n+        if (this.tsym == referringClass)\n+            return false; \/\/ pointless\n+        return this.isValueClass() && this.isFinal();\n+    }\n+\n@@ -1177,0 +1192,10 @@\n+        @Override\n+        public boolean isValueClass() {\n+            return tsym != null && tsym.isValueClass();\n+        }\n+\n+        @Override\n+        public boolean isIdentityClass() {\n+            return tsym != null && tsym.isIdentityClass();\n+        }\n+\n@@ -2336,2 +2361,1 @@\n-            super(noType, List.nil(), null);\n-            this.tsym = tsym;\n+            super(noType, List.nil(), tsym, List.nil());\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/code\/Type.java","additions":26,"deletions":2,"binary":false,"changes":28,"status":"modified"},{"patch":"@@ -327,0 +327,10 @@\n+\n+        if (!env.info.ctorPrologue &&\n+                v.owner.isValueClass() &&\n+                v.owner.kind == TYP &&\n+                v.owner == env.enclClass.sym &&\n+                (v.flags() & STATIC) == 0 &&\n+                (base == null ||\n+                        TreeInfo.isExplicitThisReference(types, (ClassType)env.enclClass.type, base))) {\n+            log.error(pos, Errors.CantRefAfterCtorCalled(v));\n+        }\n@@ -1203,1 +1213,5 @@\n-                        tree.body.stats = tree.body.stats.prepend(supCall);\n+                        if (owner.isValueClass()) {\n+                            tree.body.stats = tree.body.stats.append(supCall);\n+                        } else {\n+                            tree.body.stats = tree.body.stats.prepend(supCall);\n+                        }\n@@ -1317,4 +1331,13 @@\n-                    attribExpr(tree.init, initEnv, v.type);\n-                    if (tree.isImplicitlyTyped()) {\n-                        \/\/fixup local variable type\n-                        v.type = chk.checkLocalVarType(tree, tree.init.type, tree.name);\n+                    boolean previousCtorPrologue = initEnv.info.ctorPrologue;\n+                    try {\n+                        if (v.owner.kind == TYP && v.owner.isValueClass() && !v.isStatic()) {\n+                            \/\/ strict instance initializer in a value class\n+                            initEnv.info.ctorPrologue = true;\n+                        }\n+                        attribExpr(tree.init, initEnv, v.type);\n+                        if (tree.isImplicitlyTyped()) {\n+                            \/\/fixup local variable type\n+                            v.type = chk.checkLocalVarType(tree, tree.init.type, tree.name);\n+                        }\n+                    } finally {\n+                        initEnv.info.ctorPrologue = previousCtorPrologue;\n@@ -1429,1 +1452,5 @@\n-            if ((tree.flags & STATIC) != 0) localEnv.info.staticLevel++;\n+            if ((tree.flags & STATIC) != 0) {\n+                localEnv.info.staticLevel++;\n+            } else {\n+                localEnv.info.instanceInitializerBlock = true;\n+            }\n@@ -1938,2 +1965,4 @@\n-        chk.checkRefType(tree.pos(), attribExpr(tree.lock, env));\n-        if (env.info.lint.isEnabled(LintCategory.SYNCHRONIZATION) && isValueBased(tree.lock.type)) {\n+        boolean identityType = chk.checkIdentityType(tree.pos(), attribExpr(tree.lock, env));\n+        if (env.info.lint.isEnabled(LintCategory.SYNCHRONIZATION) &&\n+                identityType &&\n+                isValueBased(tree.lock.type)) {\n@@ -1950,1 +1979,0 @@\n-\n@@ -4405,0 +4433,1 @@\n+        Assert.check(site == tree.selected.type);\n@@ -5500,1 +5529,1 @@\n-                } else {\n+                } else if ((c.flags_field & Flags.COMPOUND) == 0) {\n@@ -5538,0 +5567,5 @@\n+                if (c.isValueClass()) {\n+                    Assert.check(env.tree.hasTag(CLASSDEF));\n+                    chk.checkConstraintsOfValueClass((JCClassDecl) env.tree, c);\n+                }\n+\n@@ -5680,1 +5714,1 @@\n-            chk.checkSerialStructure(tree, c);\n+            chk.checkSerialStructure(env, tree, c);\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/comp\/Attr.java","additions":45,"deletions":11,"binary":false,"changes":56,"status":"modified"},{"patch":"@@ -40,0 +40,1 @@\n+import com.sun.tools.javac.code.Flags.Flag;\n@@ -43,0 +44,1 @@\n+import com.sun.tools.javac.parser.Tokens.Comment.CommentStyle;\n@@ -54,0 +56,1 @@\n+import static com.sun.tools.javac.code.Flags.asFlagSet;\n@@ -62,0 +65,1 @@\n+import static com.sun.tools.javac.parser.Tokens.TokenKind.SYNCHRONIZED;\n@@ -202,0 +206,2 @@\n+        this.allowValueClasses = (!preview.isPreview(Feature.VALUE_CLASSES) || preview.isEnabled()) &&\n+                Feature.VALUE_CLASSES.allowedInSource(source);\n@@ -226,0 +232,2 @@\n+        this.allowValueClasses = (!preview.isPreview(Feature.VALUE_CLASSES) || preview.isEnabled()) &&\n+                Feature.VALUE_CLASSES.allowedInSource(source);\n@@ -263,0 +271,4 @@\n+    \/** Switch: are value classes allowed in this source level?\n+     *\/\n+    boolean allowValueClasses;\n+\n@@ -1644,2 +1656,2 @@\n-                        if (!isMode(TYPE) && isUnboundMemberRef()) {\n-                            \/\/this is an unbound method reference whose qualifier\n+                        if (!isMode(TYPE) && isParameterizedTypePrefix()) {\n+                            \/\/this is either an unbound method reference whose qualifier\n@@ -1887,1 +1899,1 @@\n-    boolean isUnboundMemberRef() {\n+    boolean isParameterizedTypePrefix() {\n@@ -2984,0 +2996,5 @@\n+        if ((isValueModifier()) && allowValueClasses) {\n+            checkSourceLevel(Feature.VALUE_CLASSES);\n+            dc = token.docComment();\n+            return List.of(classOrRecordOrInterfaceOrEnumDeclaration(modifiersOpt(), dc));\n+        }\n@@ -3589,0 +3606,5 @@\n+                if (isValueModifier()) {\n+                    checkSourceLevel(Feature.VALUE_CLASSES);\n+                    flag = Flags.VALUE_CLASS;\n+                    break;\n+                }\n@@ -3857,0 +3879,7 @@\n+        if (name == names.value) {\n+            if (allowValueClasses) {\n+                return Source.JDK23;\n+            } else if (shouldWarn) {\n+                log.warning(pos, Warnings.RestrictedTypeNotAllowedPreview(name, Source.JDK23));\n+            }\n+        }\n@@ -4965,0 +4994,26 @@\n+    protected boolean isValueModifier() {\n+        if (token.kind == IDENTIFIER && token.name() == names.value) {\n+            boolean isValueModifier = false;\n+            Token next = S.token(1);\n+            switch (next.kind) {\n+                case PRIVATE: case PROTECTED: case PUBLIC: case STATIC: case TRANSIENT:\n+                case FINAL: case ABSTRACT: case NATIVE: case VOLATILE: case SYNCHRONIZED:\n+                case STRICTFP: case MONKEYS_AT: case DEFAULT: case BYTE: case SHORT:\n+                case CHAR: case INT: case LONG: case FLOAT: case DOUBLE: case BOOLEAN: case VOID:\n+                case CLASS: case INTERFACE: case ENUM:\n+                    isValueModifier = true;\n+                    break;\n+                case IDENTIFIER: \/\/ value record R || value value || new value Comparable() {} ??\n+                    if (next.name() == names.record || next.name() == names.value\n+                            || (mode & EXPR) != 0)\n+                        isValueModifier = true;\n+                    break;\n+            }\n+            if (isValueModifier) {\n+                checkSourceLevel(Feature.VALUE_CLASSES);\n+                return true;\n+            }\n+        }\n+        return false;\n+    }\n+\n@@ -4992,1 +5047,3 @@\n-                case IDENTIFIER -> isNonSealedIdentifier(next, currentIsNonSealed ? 3 : 1) || next.name() == names.sealed;\n+                case IDENTIFIER -> isNonSealedIdentifier(next, currentIsNonSealed ? 3 : 1) ||\n+                        next.name() == names.sealed ||\n+                        allowValueClasses && next.name() == names.value;\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/parser\/JavacParser.java","additions":61,"deletions":4,"binary":false,"changes":65,"status":"modified"},{"patch":"@@ -279,1 +279,1 @@\n-        String optoRegex = \"(.*precise .*\\\\R((.*(?i:mov|mv|xorl|nop|spill).*|\\\\s*)\\\\R)*.*(?i:call,static).*wrapper for: _new_instance_Java\" + END;\n+        String optoRegex = \"(.*precise .*\\\\R((.*(?i:mov|mv|xorl|nop|spill|pushq|popq).*|\\\\s*)\\\\R)*.*(?i:call,static).*wrapper for: _new_instance_Java\" + END;\n@@ -285,1 +285,1 @@\n-        String regex = \"(.*precise .*\" + IS_REPLACED + \":.*\\\\R((.*(?i:mov|mv|xorl|nop|spill).*|\\\\s*)\\\\R)*.*(?i:call,static).*wrapper for: _new_instance_Java\" + END;\n+        String regex = \"(.*precise .*\" + IS_REPLACED + \":.*\\\\R((.*(?i:mov|mv|xorl|nop|spill|pushq|popq).*|\\\\s*)\\\\R)*.*(?i:call,static).*wrapper for: _new_instance_Java\" + END;\n@@ -291,1 +291,1 @@\n-        String optoRegex = \"(.*precise \\\\[.*\\\\R((.*(?i:mov|mv|xor|nop|spill).*|\\\\s*|.*(LGHI|LI).*)\\\\R)*.*(?i:call,static).*wrapper for: _new_array_Java\" + END;\n+        String optoRegex = \"(.*precise \\\\[.*\\\\R((.*(?i:mov|mv|xor|nop|spill|pushq|popq).*|\\\\s*|.*(LGHI|LI).*)\\\\R)*.*(?i:call,static).*wrapper for: _new_array_Java\" + END;\n@@ -297,1 +297,1 @@\n-        String regex = \"(.*precise \\\\[.*\" + IS_REPLACED + \":.*\\\\R((.*(?i:mov|mv|xorl|nop|spill).*|\\\\s*|.*(LGHI|LI).*)\\\\R)*.*(?i:call,static).*wrapper for: _new_array_Java\" + END;\n+        String regex = \"(.*precise \\\\[.*\" + IS_REPLACED + \":.*\\\\R((.*(?i:mov|mv|xorl|nop|spill|pushq|popq).*|\\\\s*|.*(LGHI|LI).*)\\\\R)*.*(?i:call,static).*wrapper for: _new_array_Java\" + END;\n@@ -574,0 +574,5 @@\n+    public static final String INLINE_TYPE = PREFIX + \"INLINE_TYPE\" + POSTFIX;\n+    static {\n+        beforeMatchingNameRegex(INLINE_TYPE, \"InlineType\");\n+    }\n+\n@@ -2244,1 +2249,1 @@\n-    private static void beforeMatching(String irNodePlaceholder, String regex) {\n+    public static void beforeMatching(String irNodePlaceholder, String regex) {\n@@ -2290,1 +2295,1 @@\n-    private static void optoOnly(String irNodePlaceholder, String regex) {\n+    public static void optoOnly(String irNodePlaceholder, String regex) {\n","filename":"test\/hotspot\/jtreg\/compiler\/lib\/ir_framework\/IRNode.java","additions":11,"deletions":6,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -499,0 +499,3 @@\n+java\/lang\/ModuleLayer\/LayerControllerTest.java                  8337048 generic-all\n+java\/lang\/ModuleLayer\/BasicLayerTest.java                       8337048 generic-all\n+\n@@ -712,0 +715,4 @@\n+com\/sun\/jdi\/cds\/CDSBreakpointTest.java                          8304168 generic-all\n+com\/sun\/jdi\/cds\/CDSDeleteAllBkptsTest.java                      8304168 generic-all\n+com\/sun\/jdi\/cds\/CDSFieldWatchpoints.java                        8304168 generic-all\n+\n@@ -754,0 +761,6 @@\n+jdk\/classfile\/SwapTest.java                                     8308778 generic-all\n+jdk\/classfile\/LowAdaptTest.java                                 8308778 generic-all\n+jdk\/classfile\/BuilderBlockTest.java                             8308778 generic-all\n+jdk\/classfile\/BuilderTryCatchTest.java                          8308778 generic-all\n+jdk\/classfile\/PrimitiveClassConstantTest.java                   8310649 generic-all\n+\n@@ -790,0 +803,3 @@\n+\n+# valhalla\n+jdk\/jfr\/event\/runtime\/TestSyncOnValueBasedClassEvent.java 8328777 generic-all\n","filename":"test\/jdk\/ProblemList.txt","additions":16,"deletions":0,"binary":false,"changes":16,"status":"modified"}]}