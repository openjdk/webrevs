{"files":[{"patch":"@@ -30,0 +30,1 @@\n+#include \"classfile\/symbolTable.hpp\"\n@@ -361,0 +362,79 @@\n+\n+const uint SharedRuntime::java_return_convention_max_int = Argument::n_int_register_parameters_j;\n+const uint SharedRuntime::java_return_convention_max_float = Argument::n_float_register_parameters_j;\n+\n+int SharedRuntime::java_return_convention(const BasicType *sig_bt, VMRegPair *regs, int total_args_passed) {\n+\n+  \/\/ Create the mapping between argument positions and registers.\n+\n+  static const Register INT_ArgReg[java_return_convention_max_int] = {\n+    r0 \/* j_rarg7 *\/, j_rarg6, j_rarg5, j_rarg4, j_rarg3, j_rarg2, j_rarg1, j_rarg0\n+  };\n+\n+  static const FloatRegister FP_ArgReg[java_return_convention_max_float] = {\n+    j_farg0, j_farg1, j_farg2, j_farg3, j_farg4, j_farg5, j_farg6, j_farg7\n+  };\n+\n+  uint int_args = 0;\n+  uint fp_args = 0;\n+\n+  for (int i = 0; i < total_args_passed; i++) {\n+    switch (sig_bt[i]) {\n+    case T_BOOLEAN:\n+    case T_CHAR:\n+    case T_BYTE:\n+    case T_SHORT:\n+    case T_INT:\n+      if (int_args < SharedRuntime::java_return_convention_max_int) {\n+        regs[i].set1(INT_ArgReg[int_args]->as_VMReg());\n+        int_args ++;\n+      } else {\n+        return -1;\n+      }\n+      break;\n+    case T_VOID:\n+      \/\/ halves of T_LONG or T_DOUBLE\n+      assert(i != 0 && (sig_bt[i - 1] == T_LONG || sig_bt[i - 1] == T_DOUBLE), \"expecting half\");\n+      regs[i].set_bad();\n+      break;\n+    case T_LONG:\n+      assert((i + 1) < total_args_passed && sig_bt[i + 1] == T_VOID, \"expecting half\");\n+      \/\/ fall through\n+    case T_OBJECT:\n+    case T_ARRAY:\n+    case T_ADDRESS:\n+      \/\/ Should T_METADATA be added to java_calling_convention as well ?\n+    case T_METADATA:\n+      if (int_args < SharedRuntime::java_return_convention_max_int) {\n+        regs[i].set2(INT_ArgReg[int_args]->as_VMReg());\n+        int_args ++;\n+      } else {\n+        return -1;\n+      }\n+      break;\n+    case T_FLOAT:\n+      if (fp_args < SharedRuntime::java_return_convention_max_float) {\n+        regs[i].set1(FP_ArgReg[fp_args]->as_VMReg());\n+        fp_args ++;\n+      } else {\n+        return -1;\n+      }\n+      break;\n+    case T_DOUBLE:\n+      assert((i + 1) < total_args_passed && sig_bt[i + 1] == T_VOID, \"expecting half\");\n+      if (fp_args < SharedRuntime::java_return_convention_max_float) {\n+        regs[i].set2(FP_ArgReg[fp_args]->as_VMReg());\n+        fp_args ++;\n+      } else {\n+        return -1;\n+      }\n+      break;\n+    default:\n+      ShouldNotReachHere();\n+      break;\n+    }\n+  }\n+\n+  return int_args + fp_args;\n+}\n+\n@@ -395,0 +475,108 @@\n+\/\/ For each inline type argument, sig includes the list of fields of\n+\/\/ the inline type. This utility function computes the number of\n+\/\/ arguments for the call if inline types are passed by reference (the\n+\/\/ calling convention the interpreter expects).\n+static int compute_total_args_passed_int(const GrowableArray<SigEntry>* sig_extended) {\n+  int total_args_passed = 0;\n+  if (InlineTypePassFieldsAsArgs) {\n+     for (int i = 0; i < sig_extended->length(); i++) {\n+       BasicType bt = sig_extended->at(i)._bt;\n+       if (bt == T_METADATA) {\n+         \/\/ In sig_extended, an inline type argument starts with:\n+         \/\/ T_METADATA, followed by the types of the fields of the\n+         \/\/ inline type and T_VOID to mark the end of the value\n+         \/\/ type. Inline types are flattened so, for instance, in the\n+         \/\/ case of an inline type with an int field and an inline type\n+         \/\/ field that itself has 2 fields, an int and a long:\n+         \/\/ T_METADATA T_INT T_METADATA T_INT T_LONG T_VOID (second\n+         \/\/ slot for the T_LONG) T_VOID (inner inline type) T_VOID\n+         \/\/ (outer inline type)\n+         total_args_passed++;\n+         int vt = 1;\n+         do {\n+           i++;\n+           BasicType bt = sig_extended->at(i)._bt;\n+           BasicType prev_bt = sig_extended->at(i-1)._bt;\n+           if (bt == T_METADATA) {\n+             vt++;\n+           } else if (bt == T_VOID &&\n+                      prev_bt != T_LONG &&\n+                      prev_bt != T_DOUBLE) {\n+             vt--;\n+           }\n+         } while (vt != 0);\n+       } else {\n+         total_args_passed++;\n+       }\n+     }\n+  } else {\n+    total_args_passed = sig_extended->length();\n+  }\n+\n+  return total_args_passed;\n+}\n+\n+\n+static void gen_c2i_adapter_helper(MacroAssembler* masm,\n+                                   BasicType bt,\n+                                   BasicType prev_bt,\n+                                   size_t size_in_bytes,\n+                                   const VMRegPair& reg_pair,\n+                                   const Address& to,\n+                                   Register tmp1,\n+                                   Register tmp2,\n+                                   Register tmp3,\n+                                   int extraspace,\n+                                   bool is_oop) {\n+  if (bt == T_VOID) {\n+    assert(prev_bt == T_LONG || prev_bt == T_DOUBLE, \"missing half\");\n+    return;\n+  }\n+\n+  \/\/ Say 4 args:\n+  \/\/ i   st_off\n+  \/\/ 0   32 T_LONG\n+  \/\/ 1   24 T_VOID\n+  \/\/ 2   16 T_OBJECT\n+  \/\/ 3    8 T_BOOL\n+  \/\/ -    0 return address\n+  \/\/\n+  \/\/ However to make thing extra confusing. Because we can fit a Java long\/double in\n+  \/\/ a single slot on a 64 bt vm and it would be silly to break them up, the interpreter\n+  \/\/ leaves one slot empty and only stores to a single slot. In this case the\n+  \/\/ slot that is occupied is the T_VOID slot. See I said it was confusing.\n+\n+  bool wide = (size_in_bytes == wordSize);\n+  VMReg r_1 = reg_pair.first();\n+  VMReg r_2 = reg_pair.second();\n+  assert(r_2->is_valid() == wide, \"invalid size\");\n+  if (!r_1->is_valid()) {\n+    assert(!r_2->is_valid(), \"\");\n+    return;\n+  }\n+\n+  if (!r_1->is_FloatRegister()) {\n+    Register val = r25;\n+    if (r_1->is_stack()) {\n+      \/\/ memory to memory use r25 (scratch registers is used by store_heap_oop)\n+      int ld_off = r_1->reg2stack() * VMRegImpl::stack_slot_size + extraspace;\n+      __ load_sized_value(val, Address(sp, ld_off), size_in_bytes, \/* is_signed *\/ false);\n+    } else {\n+      val = r_1->as_Register();\n+    }\n+    assert_different_registers(to.base(), val, tmp1, tmp2, tmp3);\n+    if (is_oop) {\n+      __ store_heap_oop(to, val, tmp1, tmp2, tmp3, IN_HEAP | ACCESS_WRITE | IS_DEST_UNINITIALIZED);\n+    } else {\n+      __ store_sized_value(to, val, size_in_bytes);\n+    }\n+  } else {\n+    if (wide) {\n+      __ strd(r_1->as_FloatRegister(), to);\n+    } else {\n+      \/\/ only a float use just part of the slot\n+      __ strs(r_1->as_FloatRegister(), to);\n+    }\n+  }\n+}\n+\n@@ -396,3 +584,1 @@\n-                            int total_args_passed,\n-                            int comp_args_on_stack,\n-                            const BasicType *sig_bt,\n+                            const GrowableArray<SigEntry>* sig_extended,\n@@ -400,1 +586,28 @@\n-                            Label& skip_fixup) {\n+                            bool requires_clinit_barrier,\n+                            address& c2i_no_clinit_check_entry,\n+                            Label& skip_fixup,\n+                            address start,\n+                            OopMapSet* oop_maps,\n+                            int& frame_complete,\n+                            int& frame_size_in_words,\n+                            bool alloc_inline_receiver) {\n+  if (requires_clinit_barrier && VM_Version::supports_fast_class_init_checks()) {\n+    Label L_skip_barrier;\n+\n+    { \/\/ Bypass the barrier for non-static methods\n+      __ ldrw(rscratch1, Address(rmethod, Method::access_flags_offset()));\n+      __ andsw(zr, rscratch1, JVM_ACC_STATIC);\n+      __ br(Assembler::EQ, L_skip_barrier); \/\/ non-static\n+    }\n+\n+    __ load_method_holder(rscratch2, rmethod);\n+    __ clinit_barrier(rscratch2, rscratch1, &L_skip_barrier);\n+    __ far_jump(RuntimeAddress(SharedRuntime::get_handle_wrong_method_stub()));\n+\n+    __ bind(L_skip_barrier);\n+    c2i_no_clinit_check_entry = __ pc();\n+  }\n+\n+  BarrierSetAssembler* bs = BarrierSet::barrier_set()->barrier_set_assembler();\n+  bs->c2i_entry_barrier(masm);\n+\n@@ -410,1 +623,22 @@\n-  int words_pushed = 0;\n+  \/\/ Name some registers to be used in the following code. We can use\n+  \/\/ anything except r0-r7 which are arguments in the Java calling\n+  \/\/ convention, rmethod (r12), and r13 which holds the outgoing sender\n+  \/\/ SP for the interpreter.\n+  Register buf_array = r10;   \/\/ Array of buffered inline types\n+  Register buf_oop = r11;     \/\/ Buffered inline type oop\n+  Register tmp1 = r15;\n+  Register tmp2 = r16;\n+  Register tmp3 = r17;\n+\n+  if (InlineTypePassFieldsAsArgs) {\n+    \/\/ Is there an inline type argument?\n+    bool has_inline_argument = false;\n+    for (int i = 0; i < sig_extended->length() && !has_inline_argument; i++) {\n+      has_inline_argument = (sig_extended->at(i)._bt == T_METADATA);\n+    }\n+    if (has_inline_argument) {\n+      \/\/ There is at least an inline type argument: we're coming from\n+      \/\/ compiled code so we have no buffers to back the inline types\n+      \/\/ Allocate the buffers here with a runtime call.\n+      RegisterSaver reg_save(false \/* save_vectors *\/);\n+      OopMap* map = reg_save.save_live_registers(masm, 0, &frame_size_in_words);\n@@ -412,2 +646,2 @@\n-  \/\/ Since all args are passed on the stack, total_args_passed *\n-  \/\/ Interpreter::stackElementSize is the space we need.\n+      frame_complete = __ offset();\n+      address the_pc = __ pc();\n@@ -415,1 +649,2 @@\n-  int extraspace = total_args_passed * Interpreter::stackElementSize;\n+      Label retaddr;\n+      __ set_last_Java_frame(sp, noreg, retaddr, rscratch1);\n@@ -417,1 +652,3 @@\n-  __ mov(r19_sender_sp, sp);\n+      __ mov(c_rarg0, rthread);\n+      __ mov(c_rarg1, rmethod);\n+      __ mov(c_rarg2, (int64_t)alloc_inline_receiver);\n@@ -419,2 +656,3 @@\n-  \/\/ stack is aligned, keep it that way\n-  extraspace = align_up(extraspace, 2*wordSize);\n+      __ lea(rscratch1, RuntimeAddress(CAST_FROM_FN_PTR(address, SharedRuntime::allocate_inline_types)));\n+      __ blr(rscratch1);\n+      __ bind(retaddr);\n@@ -422,2 +660,2 @@\n-  if (extraspace)\n-    __ sub(sp, sp, extraspace);\n+      oop_maps->add_gc_map(__ pc() - start, map);\n+      __ reset_last_Java_frame(false);\n@@ -425,6 +663,1 @@\n-  \/\/ Now write the args into the outgoing interpreter space\n-  for (int i = 0; i < total_args_passed; i++) {\n-    if (sig_bt[i] == T_VOID) {\n-      assert(i > 0 && (sig_bt[i-1] == T_LONG || sig_bt[i-1] == T_DOUBLE), \"missing half\");\n-      continue;\n-    }\n+      reg_save.restore_live_registers(masm);\n@@ -432,16 +665,3 @@\n-    \/\/ offset to start parameters\n-    int st_off   = (total_args_passed - i - 1) * Interpreter::stackElementSize;\n-    int next_off = st_off - Interpreter::stackElementSize;\n-\n-    \/\/ Say 4 args:\n-    \/\/ i   st_off\n-    \/\/ 0   32 T_LONG\n-    \/\/ 1   24 T_VOID\n-    \/\/ 2   16 T_OBJECT\n-    \/\/ 3    8 T_BOOL\n-    \/\/ -    0 return address\n-    \/\/\n-    \/\/ However to make thing extra confusing. Because we can fit a Java long\/double in\n-    \/\/ a single slot on a 64 bt vm and it would be silly to break them up, the interpreter\n-    \/\/ leaves one slot empty and only stores to a single slot. In this case the\n-    \/\/ slot that is occupied is the T_VOID slot. See I said it was confusing.\n+      Label no_exception;\n+      __ ldr(rscratch1, Address(rthread, Thread::pending_exception_offset()));\n+      __ cbz(rscratch1, no_exception);\n@@ -449,5 +669,9 @@\n-    VMReg r_1 = regs[i].first();\n-    VMReg r_2 = regs[i].second();\n-    if (!r_1->is_valid()) {\n-      assert(!r_2->is_valid(), \"\");\n-      continue;\n+      __ str(zr, Address(rthread, JavaThread::vm_result_offset()));\n+      __ ldr(r0, Address(rthread, Thread::pending_exception_offset()));\n+      __ b(RuntimeAddress(StubRoutines::forward_exception_entry()));\n+\n+      __ bind(no_exception);\n+\n+      \/\/ We get an array of objects from the runtime call\n+      __ get_vm_result(buf_array, rthread);\n+      __ get_vm_result_2(rmethod, rthread); \/\/ TODO: required to keep the callee Method live?\n@@ -455,9 +679,1 @@\n-    if (r_1->is_stack()) {\n-      \/\/ memory to memory use rscratch1\n-      int ld_off = (r_1->reg2stack() * VMRegImpl::stack_slot_size\n-                    + extraspace\n-                    + words_pushed * wordSize);\n-      if (!r_2->is_valid()) {\n-        \/\/ sign extend??\n-        __ ldrw(rscratch1, Address(sp, ld_off));\n-        __ str(rscratch1, Address(sp, st_off));\n+  }\n@@ -465,1 +681,5 @@\n-      } else {\n+  \/\/ Since all args are passed on the stack, total_args_passed *\n+  \/\/ Interpreter::stackElementSize is the space we need.\n+\n+  int total_args_passed = compute_total_args_passed_int(sig_extended);\n+  int extraspace = total_args_passed * Interpreter::stackElementSize;\n@@ -467,1 +687,2 @@\n-        __ ldr(rscratch1, Address(sp, ld_off));\n+  \/\/ stack is aligned, keep it that way\n+  extraspace = align_up(extraspace, StackAlignmentInBytes);\n@@ -469,6 +690,31 @@\n-        \/\/ Two VMREgs|OptoRegs can be T_OBJECT, T_ADDRESS, T_DOUBLE, T_LONG\n-        \/\/ T_DOUBLE and T_LONG use two slots in the interpreter\n-        if ( sig_bt[i] == T_LONG || sig_bt[i] == T_DOUBLE) {\n-          \/\/ ld_off == LSW, ld_off+wordSize == MSW\n-          \/\/ st_off == MSW, next_off == LSW\n-          __ str(rscratch1, Address(sp, next_off));\n+  \/\/ set senderSP value\n+  __ mov(r19_sender_sp, sp);\n+\n+  __ sub(sp, sp, extraspace);\n+\n+  \/\/ Now write the args into the outgoing interpreter space\n+\n+  \/\/ next_arg_comp is the next argument from the compiler point of\n+  \/\/ view (inline type fields are passed in registers\/on the stack). In\n+  \/\/ sig_extended, an inline type argument starts with: T_METADATA,\n+  \/\/ followed by the types of the fields of the inline type and T_VOID\n+  \/\/ to mark the end of the inline type. ignored counts the number of\n+  \/\/ T_METADATA\/T_VOID. next_vt_arg is the next inline type argument:\n+  \/\/ used to get the buffer for that argument from the pool of buffers\n+  \/\/ we allocated above and want to pass to the\n+  \/\/ interpreter. next_arg_int is the next argument from the\n+  \/\/ interpreter point of view (inline types are passed by reference).\n+  for (int next_arg_comp = 0, ignored = 0, next_vt_arg = 0, next_arg_int = 0;\n+       next_arg_comp < sig_extended->length(); next_arg_comp++) {\n+    assert(ignored <= next_arg_comp, \"shouldn't skip over more slots than there are arguments\");\n+    assert(next_arg_int <= total_args_passed, \"more arguments for the interpreter than expected?\");\n+    BasicType bt = sig_extended->at(next_arg_comp)._bt;\n+    int st_off = (total_args_passed - next_arg_int - 1) * Interpreter::stackElementSize;\n+    if (!InlineTypePassFieldsAsArgs || bt != T_METADATA) {\n+      int next_off = st_off - Interpreter::stackElementSize;\n+      const int offset = (bt == T_LONG || bt == T_DOUBLE) ? next_off : st_off;\n+      const VMRegPair reg_pair = regs[next_arg_comp-ignored];\n+      size_t size_in_bytes = reg_pair.second()->is_valid() ? 8 : 4;\n+      gen_c2i_adapter_helper(masm, bt, next_arg_comp > 0 ? sig_extended->at(next_arg_comp-1)._bt : T_ILLEGAL,\n+                             size_in_bytes, reg_pair, Address(sp, offset), tmp1, tmp2, tmp3, extraspace, false);\n+      next_arg_int++;\n@@ -476,7 +722,4 @@\n-          \/\/ Overwrite the unused slot with known junk\n-          __ mov(rscratch1, (uint64_t)0xdeadffffdeadaaaaull);\n-          __ str(rscratch1, Address(sp, st_off));\n-#endif \/* ASSERT *\/\n-        } else {\n-          __ str(rscratch1, Address(sp, st_off));\n-        }\n+      if (bt == T_LONG || bt == T_DOUBLE) {\n+        \/\/ Overwrite the unused slot with known junk\n+        __ mov(rscratch1, CONST64(0xdeadffffdeadaaaa));\n+        __ str(rscratch1, Address(sp, st_off));\n@@ -484,16 +727,24 @@\n-    } else if (r_1->is_Register()) {\n-      Register r = r_1->as_Register();\n-      if (!r_2->is_valid()) {\n-        \/\/ must be only an int (or less ) so move only 32bits to slot\n-        \/\/ why not sign extend??\n-        __ str(r, Address(sp, st_off));\n-      } else {\n-        \/\/ Two VMREgs|OptoRegs can be T_OBJECT, T_ADDRESS, T_DOUBLE, T_LONG\n-        \/\/ T_DOUBLE and T_LONG use two slots in the interpreter\n-        if ( sig_bt[i] == T_LONG || sig_bt[i] == T_DOUBLE) {\n-          \/\/ jlong\/double in gpr\n-#ifdef ASSERT\n-          \/\/ Overwrite the unused slot with known junk\n-          __ mov(rscratch1, (uint64_t)0xdeadffffdeadaaabull);\n-          __ str(rscratch1, Address(sp, st_off));\n-          __ str(r, Address(sp, next_off));\n+    } else {\n+      ignored++;\n+      \/\/ get the buffer from the just allocated pool of buffers\n+      int index = arrayOopDesc::base_offset_in_bytes(T_OBJECT) + next_vt_arg * type2aelembytes(T_OBJECT);\n+      __ load_heap_oop(buf_oop, Address(buf_array, index), tmp1, tmp2);\n+      next_vt_arg++; next_arg_int++;\n+      int vt = 1;\n+      \/\/ write fields we get from compiled code in registers\/stack\n+      \/\/ slots to the buffer: we know we are done with that inline type\n+      \/\/ argument when we hit the T_VOID that acts as an end of inline\n+      \/\/ type delimiter for this inline type. Inline types are flattened\n+      \/\/ so we might encounter embedded inline types. Each entry in\n+      \/\/ sig_extended contains a field offset in the buffer.\n+      Label L_null;\n+      do {\n+        next_arg_comp++;\n+        BasicType bt = sig_extended->at(next_arg_comp)._bt;\n+        BasicType prev_bt = sig_extended->at(next_arg_comp - 1)._bt;\n+        if (bt == T_METADATA) {\n+          vt++;\n+          ignored++;\n+        } else if (bt == T_VOID && prev_bt != T_LONG && prev_bt != T_DOUBLE) {\n+          vt--;\n+          ignored++;\n@@ -502,1 +753,22 @@\n-          __ str(r, Address(sp, st_off));\n+          int off = sig_extended->at(next_arg_comp)._offset;\n+          if (off == -1) {\n+            \/\/ Nullable inline type argument, emit null check\n+            VMReg reg = regs[next_arg_comp-ignored].first();\n+            Label L_notNull;\n+            if (reg->is_stack()) {\n+              int ld_off = reg->reg2stack() * VMRegImpl::stack_slot_size + extraspace;\n+              __ ldrb(tmp1, Address(sp, ld_off));\n+              __ cbnz(tmp1, L_notNull);\n+            } else {\n+              __ cbnz(reg->as_Register(), L_notNull);\n+            }\n+            __ str(zr, Address(sp, st_off));\n+            __ b(L_null);\n+            __ bind(L_notNull);\n+            continue;\n+          }\n+          assert(off > 0, \"offset in object should be positive\");\n+          size_t size_in_bytes = is_java_primitive(bt) ? type2aelembytes(bt) : wordSize;\n+          bool is_oop = is_reference_type(bt);\n+          gen_c2i_adapter_helper(masm, bt, next_arg_comp > 0 ? sig_extended->at(next_arg_comp-1)._bt : T_ILLEGAL,\n+                                 size_in_bytes, regs[next_arg_comp-ignored], Address(buf_oop, off), tmp1, tmp2, tmp3, extraspace, is_oop);\n@@ -504,14 +776,4 @@\n-      }\n-    } else {\n-      assert(r_1->is_FloatRegister(), \"\");\n-      if (!r_2->is_valid()) {\n-        \/\/ only a float use just part of the slot\n-        __ strs(r_1->as_FloatRegister(), Address(sp, st_off));\n-      } else {\n-#ifdef ASSERT\n-        \/\/ Overwrite the unused slot with known junk\n-        __ mov(rscratch1, (uint64_t)0xdeadffffdeadaaacull);\n-        __ str(rscratch1, Address(sp, st_off));\n-#endif \/* ASSERT *\/\n-        __ strd(r_1->as_FloatRegister(), Address(sp, next_off));\n-      }\n+      } while (vt != 0);\n+      \/\/ pass the buffer to the interpreter\n+      __ str(buf_oop, Address(sp, st_off));\n+      __ bind(L_null);\n@@ -527,0 +789,1 @@\n+void SharedRuntime::gen_i2c_adapter(MacroAssembler *masm, int comp_args_on_stack, const GrowableArray<SigEntry>* sig, const VMRegPair *regs) {\n@@ -528,5 +791,0 @@\n-void SharedRuntime::gen_i2c_adapter(MacroAssembler *masm,\n-                                    int total_args_passed,\n-                                    int comp_args_on_stack,\n-                                    const BasicType *sig_bt,\n-                                    const VMRegPair *regs) {\n@@ -595,1 +853,1 @@\n-  int comp_words_on_stack = align_up(comp_args_on_stack*VMRegImpl::stack_slot_size, wordSize)>>LogBytesPerWord;\n+  int comp_words_on_stack = 0;\n@@ -597,2 +855,3 @@\n-    __ sub(rscratch1, sp, comp_words_on_stack * wordSize);\n-    __ andr(sp, rscratch1, -16);\n+     comp_words_on_stack = align_up(comp_args_on_stack * VMRegImpl::stack_slot_size, wordSize) >> LogBytesPerWord;\n+     __ sub(rscratch1, sp, comp_words_on_stack * wordSize);\n+     __ andr(sp, rscratch1, -16);\n@@ -603,1 +862,1 @@\n-  __ ldr(rscratch1, Address(rmethod, in_bytes(Method::from_compiled_offset())));\n+  __ ldr(rscratch1, Address(rmethod, in_bytes(Method::from_compiled_inline_offset())));\n@@ -617,0 +876,2 @@\n+  int total_args_passed = sig->length();\n+\n@@ -619,2 +880,3 @@\n-    if (sig_bt[i] == T_VOID) {\n-      assert(i > 0 && (sig_bt[i-1] == T_LONG || sig_bt[i-1] == T_DOUBLE), \"missing half\");\n+    BasicType bt = sig->at(i)._bt;\n+    if (bt == T_VOID) {\n+      assert(i > 0 && (sig->at(i - 1)._bt == T_LONG || sig->at(i - 1)._bt == T_DOUBLE), \"missing half\");\n@@ -625,0 +887,1 @@\n+    assert(!regs[i].second()->is_valid() || regs[i].first()->next() == regs[i].second(), \"scrambled load targets?\");\n@@ -626,3 +889,1 @@\n-    assert(!regs[i].second()->is_valid() || regs[i].first()->next() == regs[i].second(),\n-            \"scrambled load targets?\");\n-    int ld_off = (total_args_passed - i - 1)*Interpreter::stackElementSize;\n+    int ld_off = (total_args_passed - i - 1) * Interpreter::stackElementSize;\n@@ -643,1 +904,1 @@\n-      int st_off = regs[i].first()->reg2stack()*VMRegImpl::stack_slot_size;\n+      int st_off = regs[i].first()->reg2stack() * VMRegImpl::stack_slot_size;\n@@ -660,2 +921,1 @@\n-        const int offset = (sig_bt[i]==T_LONG||sig_bt[i]==T_DOUBLE)?\n-                           next_off : ld_off;\n+        const int offset = (bt == T_LONG || bt == T_DOUBLE) ? next_off : ld_off;\n@@ -664,14 +924,28 @@\n-        __ str(rscratch2, Address(sp, st_off));\n-      }\n-    } else if (r_1->is_Register()) {  \/\/ Register argument\n-      Register r = r_1->as_Register();\n-      if (r_2->is_valid()) {\n-        \/\/\n-        \/\/ We are using two VMRegs. This can be either T_OBJECT,\n-        \/\/ T_ADDRESS, T_LONG, or T_DOUBLE the interpreter allocates\n-        \/\/ two slots but only uses one for thr T_LONG or T_DOUBLE case\n-        \/\/ So we must adjust where to pick up the data to match the\n-        \/\/ interpreter.\n-\n-        const int offset = (sig_bt[i]==T_LONG||sig_bt[i]==T_DOUBLE)?\n-                           next_off : ld_off;\n+         __ str(rscratch2, Address(sp, st_off));\n+       }\n+     } else if (r_1->is_Register()) {  \/\/ Register argument\n+       Register r = r_1->as_Register();\n+       if (r_2->is_valid()) {\n+         \/\/\n+         \/\/ We are using two VMRegs. This can be either T_OBJECT,\n+         \/\/ T_ADDRESS, T_LONG, or T_DOUBLE the interpreter allocates\n+         \/\/ two slots but only uses one for thr T_LONG or T_DOUBLE case\n+         \/\/ So we must adjust where to pick up the data to match the\n+         \/\/ interpreter.\n+\n+        const int offset = (bt == T_LONG || bt == T_DOUBLE) ? next_off : ld_off;\n+\n+         \/\/ this can be a misaligned move\n+         __ ldr(r, Address(esp, offset));\n+       } else {\n+         \/\/ sign extend and use a full word?\n+         __ ldrw(r, Address(esp, ld_off));\n+       }\n+     } else {\n+       if (!r_2->is_valid()) {\n+         __ ldrs(r_1->as_FloatRegister(), Address(esp, ld_off));\n+       } else {\n+         __ ldrd(r_1->as_FloatRegister(), Address(esp, next_off));\n+       }\n+     }\n+   }\n@@ -679,14 +953,0 @@\n-        \/\/ this can be a misaligned move\n-        __ ldr(r, Address(esp, offset));\n-      } else {\n-        \/\/ sign extend and use a full word?\n-        __ ldrw(r, Address(esp, ld_off));\n-      }\n-    } else {\n-      if (!r_2->is_valid()) {\n-        __ ldrs(r_1->as_FloatRegister(), Address(esp, ld_off));\n-      } else {\n-        __ ldrd(r_1->as_FloatRegister(), Address(esp, next_off));\n-      }\n-    }\n-  }\n@@ -709,1 +969,0 @@\n-\n@@ -713,8 +972,4 @@\n-\/\/ ---------------------------------------------------------------\n-AdapterHandlerEntry* SharedRuntime::generate_i2c2i_adapters(MacroAssembler *masm,\n-                                                            int total_args_passed,\n-                                                            int comp_args_on_stack,\n-                                                            const BasicType *sig_bt,\n-                                                            const VMRegPair *regs,\n-                                                            AdapterFingerPrint* fingerprint) {\n-  address i2c_entry = __ pc();\n+static void gen_inline_cache_check(MacroAssembler *masm, Label& skip_fixup) {\n+  Register data = rscratch2;\n+  __ ic_check(1 \/* end_alignment *\/);\n+  __ ldr(rmethod, Address(data, CompiledICData::speculated_method_offset()));\n@@ -722,1 +977,7 @@\n-  gen_i2c_adapter(masm, total_args_passed, comp_args_on_stack, sig_bt, regs);\n+  \/\/ Method might have been compiled since the call site was patched to\n+  \/\/ interpreted; if that is the case treat it as a miss so we can get\n+  \/\/ the call site corrected.\n+  __ ldr(rscratch1, Address(rmethod, in_bytes(Method::code_offset())));\n+  __ cbz(rscratch1, skip_fixup);\n+  __ far_jump(RuntimeAddress(SharedRuntime::get_ic_miss_stub()));\n+}\n@@ -724,2 +985,12 @@\n-  address c2i_unverified_entry = __ pc();\n-  Label skip_fixup;\n+\/\/ ---------------------------------------------------------------\n+AdapterHandlerEntry* SharedRuntime::generate_i2c2i_adapters(MacroAssembler* masm,\n+                                                            int comp_args_on_stack,\n+                                                            const GrowableArray<SigEntry>* sig,\n+                                                            const VMRegPair* regs,\n+                                                            const GrowableArray<SigEntry>* sig_cc,\n+                                                            const VMRegPair* regs_cc,\n+                                                            const GrowableArray<SigEntry>* sig_cc_ro,\n+                                                            const VMRegPair* regs_cc_ro,\n+                                                            AdapterFingerPrint* fingerprint,\n+                                                            AdapterBlob*& new_adapter,\n+                                                            bool allocate_code_blob) {\n@@ -727,3 +998,2 @@\n-  Register data = rscratch2;\n-  Register receiver = j_rarg0;\n-  Register tmp = r10;  \/\/ A call-clobbered register not used for arg passing\n+  address i2c_entry = __ pc();\n+  gen_i2c_adapter(masm, comp_args_on_stack, sig, regs);\n@@ -740,7 +1010,3 @@\n-  {\n-    __ block_comment(\"c2i_unverified_entry {\");\n-    \/\/ Method might have been compiled since the call site was patched to\n-    \/\/ interpreted; if that is the case treat it as a miss so we can get\n-    \/\/ the call site corrected.\n-    __ ic_check(1 \/* end_alignment *\/);\n-    __ ldr(rmethod, Address(data, CompiledICData::speculated_method_offset()));\n+  address c2i_unverified_entry        = __ pc();\n+  address c2i_unverified_inline_entry = __ pc();\n+  Label skip_fixup;\n@@ -748,5 +1014,1 @@\n-    __ ldr(rscratch1, Address(rmethod, in_bytes(Method::code_offset())));\n-    __ cbz(rscratch1, skip_fixup);\n-    __ far_jump(RuntimeAddress(SharedRuntime::get_ic_miss_stub()));\n-    __ block_comment(\"} c2i_unverified_entry\");\n-  }\n+  gen_inline_cache_check(masm, skip_fixup);\n@@ -754,1 +1016,3 @@\n-  address c2i_entry = __ pc();\n+  OopMapSet* oop_maps = new OopMapSet();\n+  int frame_complete = CodeOffsets::frame_never_safe;\n+  int frame_size_in_words = 0;\n@@ -756,1 +1020,1 @@\n-  \/\/ Class initialization barrier for static methods\n+  \/\/ Scalarized c2i adapter with non-scalarized receiver (i.e., don't pack receiver)\n@@ -758,12 +1022,7 @@\n-  if (VM_Version::supports_fast_class_init_checks()) {\n-    Label L_skip_barrier;\n-\n-    { \/\/ Bypass the barrier for non-static methods\n-      __ ldrw(rscratch1, Address(rmethod, Method::access_flags_offset()));\n-      __ andsw(zr, rscratch1, JVM_ACC_STATIC);\n-      __ br(Assembler::EQ, L_skip_barrier); \/\/ non-static\n-    }\n-\n-    __ load_method_holder(rscratch2, rmethod);\n-    __ clinit_barrier(rscratch2, rscratch1, &L_skip_barrier);\n-    __ far_jump(RuntimeAddress(SharedRuntime::get_handle_wrong_method_stub()));\n+  address c2i_inline_ro_entry = __ pc();\n+  if (regs_cc != regs_cc_ro) {\n+    \/\/ No class init barrier needed because method is guaranteed to be non-static\n+    gen_c2i_adapter(masm, sig_cc_ro, regs_cc_ro, \/* requires_clinit_barrier = *\/ false, c2i_no_clinit_check_entry,\n+                    skip_fixup, i2c_entry, oop_maps, frame_complete, frame_size_in_words, \/* alloc_inline_receiver = *\/ false);\n+    skip_fixup.reset();\n+  }\n@@ -771,2 +1030,15 @@\n-    __ bind(L_skip_barrier);\n-    c2i_no_clinit_check_entry = __ pc();\n+  \/\/ Scalarized c2i adapter\n+  address c2i_entry        = __ pc();\n+  address c2i_inline_entry = __ pc();\n+  gen_c2i_adapter(masm, sig_cc, regs_cc, \/* requires_clinit_barrier = *\/ true, c2i_no_clinit_check_entry,\n+                  skip_fixup, i2c_entry, oop_maps, frame_complete, frame_size_in_words, \/* alloc_inline_receiver = *\/ true);\n+\n+  \/\/ Non-scalarized c2i adapter\n+  if (regs != regs_cc) {\n+    c2i_unverified_inline_entry = __ pc();\n+    Label inline_entry_skip_fixup;\n+    gen_inline_cache_check(masm, inline_entry_skip_fixup);\n+\n+    c2i_inline_entry = __ pc();\n+    gen_c2i_adapter(masm, sig, regs, \/* requires_clinit_barrier = *\/ true, c2i_no_clinit_check_entry,\n+                    inline_entry_skip_fixup, i2c_entry, oop_maps, frame_complete, frame_size_in_words, \/* alloc_inline_receiver = *\/ false);\n@@ -775,3 +1047,6 @@\n-  BarrierSetAssembler* bs = BarrierSet::barrier_set()->barrier_set_assembler();\n-  bs->c2i_entry_barrier(masm);\n-  gen_c2i_adapter(masm, total_args_passed, comp_args_on_stack, sig_bt, regs, skip_fixup);\n+  \/\/ The c2i adapter might safepoint and trigger a GC. The caller must make sure that\n+  \/\/ the GC knows about the location of oop argument locations passed to the c2i adapter.\n+  if (allocate_code_blob) {\n+    bool caller_must_gc_arguments = (regs != regs_cc);\n+    new_adapter = AdapterBlob::create(masm->code(), frame_complete, frame_size_in_words, oop_maps, caller_must_gc_arguments);\n+  }\n@@ -780,1 +1055,1 @@\n-  return AdapterHandlerLibrary::new_entry(fingerprint, i2c_entry, c2i_entry, c2i_unverified_entry, c2i_no_clinit_check_entry);\n+  return AdapterHandlerLibrary::new_entry(fingerprint, i2c_entry, c2i_entry, c2i_inline_entry, c2i_inline_ro_entry, c2i_unverified_entry, c2i_unverified_inline_entry, c2i_no_clinit_check_entry);\n@@ -1793,0 +2068,4 @@\n+      if (EnableValhalla) {\n+        \/\/ Mask inline_type bit such that we go to the slow path if object is an inline type\n+        __ andr(swap_reg, swap_reg, ~((int) markWord::inline_type_bit_in_place));\n+      }\n@@ -2780,0 +3059,113 @@\n+BufferedInlineTypeBlob* SharedRuntime::generate_buffered_inline_type_adapter(const InlineKlass* vk) {\n+  BufferBlob* buf = BufferBlob::create(\"inline types pack\/unpack\", 16 * K);\n+  CodeBuffer buffer(buf);\n+  short buffer_locs[20];\n+  buffer.insts()->initialize_shared_locs((relocInfo*)buffer_locs,\n+                                         sizeof(buffer_locs)\/sizeof(relocInfo));\n+\n+  MacroAssembler _masm(&buffer);\n+  MacroAssembler* masm = &_masm;\n+\n+  const Array<SigEntry>* sig_vk = vk->extended_sig();\n+  const Array<VMRegPair>* regs = vk->return_regs();\n+\n+  int pack_fields_jobject_off = __ offset();\n+  \/\/ Resolve pre-allocated buffer from JNI handle.\n+  \/\/ We cannot do this in generate_call_stub() because it requires GC code to be initialized.\n+  Register Rresult = r14;  \/\/ See StubGenerator::generate_call_stub().\n+  __ ldr(r0, Address(Rresult));\n+  __ resolve_jobject(r0 \/* value *\/,\n+                     rthread \/* thread *\/,\n+                     r12 \/* tmp *\/);\n+  __ str(r0, Address(Rresult));\n+\n+  int pack_fields_off = __ offset();\n+\n+  int j = 1;\n+  for (int i = 0; i < sig_vk->length(); i++) {\n+    BasicType bt = sig_vk->at(i)._bt;\n+    if (bt == T_METADATA) {\n+      continue;\n+    }\n+    if (bt == T_VOID) {\n+      if (sig_vk->at(i-1)._bt == T_LONG ||\n+          sig_vk->at(i-1)._bt == T_DOUBLE) {\n+        j++;\n+      }\n+      continue;\n+    }\n+    int off = sig_vk->at(i)._offset;\n+    VMRegPair pair = regs->at(j);\n+    VMReg r_1 = pair.first();\n+    VMReg r_2 = pair.second();\n+    Address to(r0, off);\n+    if (bt == T_FLOAT) {\n+      __ strs(r_1->as_FloatRegister(), to);\n+    } else if (bt == T_DOUBLE) {\n+      __ strd(r_1->as_FloatRegister(), to);\n+    } else {\n+      Register val = r_1->as_Register();\n+      assert_different_registers(to.base(), val, r15, r16, r17);\n+      if (is_reference_type(bt)) {\n+        __ store_heap_oop(to, val, r15, r16, r17, IN_HEAP | ACCESS_WRITE | IS_DEST_UNINITIALIZED);\n+      } else {\n+        __ store_sized_value(to, r_1->as_Register(), type2aelembytes(bt));\n+      }\n+    }\n+    j++;\n+  }\n+  assert(j == regs->length(), \"missed a field?\");\n+\n+  __ ret(lr);\n+\n+  int unpack_fields_off = __ offset();\n+\n+  Label skip;\n+  __ cbz(r0, skip);\n+\n+  j = 1;\n+  for (int i = 0; i < sig_vk->length(); i++) {\n+    BasicType bt = sig_vk->at(i)._bt;\n+    if (bt == T_METADATA) {\n+      continue;\n+    }\n+    if (bt == T_VOID) {\n+      if (sig_vk->at(i-1)._bt == T_LONG ||\n+          sig_vk->at(i-1)._bt == T_DOUBLE) {\n+        j++;\n+      }\n+      continue;\n+    }\n+    int off = sig_vk->at(i)._offset;\n+    assert(off > 0, \"offset in object should be positive\");\n+    VMRegPair pair = regs->at(j);\n+    VMReg r_1 = pair.first();\n+    VMReg r_2 = pair.second();\n+    Address from(r0, off);\n+    if (bt == T_FLOAT) {\n+      __ ldrs(r_1->as_FloatRegister(), from);\n+    } else if (bt == T_DOUBLE) {\n+      __ ldrd(r_1->as_FloatRegister(), from);\n+    } else if (bt == T_OBJECT || bt == T_ARRAY) {\n+      assert_different_registers(r0, r_1->as_Register());\n+      __ load_heap_oop(r_1->as_Register(), from, rscratch1, rscratch2);\n+    } else {\n+      assert(is_java_primitive(bt), \"unexpected basic type\");\n+      assert_different_registers(r0, r_1->as_Register());\n+\n+      size_t size_in_bytes = type2aelembytes(bt);\n+      __ load_sized_value(r_1->as_Register(), from, size_in_bytes, bt != T_CHAR && bt != T_BOOLEAN);\n+    }\n+    j++;\n+  }\n+  assert(j == regs->length(), \"missed a field?\");\n+\n+  __ bind(skip);\n+\n+  __ ret(lr);\n+\n+  __ flush();\n+\n+  return BufferedInlineTypeBlob::create(&buffer, pack_fields_off, pack_fields_jobject_off, unpack_fields_off);\n+}\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/sharedRuntime_aarch64.cpp","additions":579,"deletions":187,"binary":false,"changes":766,"status":"modified"},{"patch":"@@ -3207,0 +3207,3 @@\n+void LIR_Assembler::emit_profile_inline_type(LIR_OpProfileInlineType* op) {\n+  Unimplemented();\n+}\n","filename":"src\/hotspot\/cpu\/ppc\/c1_LIRAssembler_ppc.cpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -1832,1 +1832,1 @@\n-        assert(ReturnTypeEntry::static_cell_count() < TypeStackSlotEntries::per_arg_count(),\n+        assert(SingleTypeEntry::static_cell_count() < TypeStackSlotEntries::per_arg_count(),\n@@ -1873,1 +1873,1 @@\n-    profile_obj_type(ret, R28_mdx, -in_bytes(ReturnTypeEntry::size()), tmp1, tmp2);\n+    profile_obj_type(ret, R28_mdx, -in_bytes(SingleTypeEntry::size()), tmp1, tmp2);\n","filename":"src\/hotspot\/cpu\/ppc\/interp_masm_ppc_64.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -540,0 +540,9 @@\n+const uint SharedRuntime::java_return_convention_max_int = 1;\n+const uint SharedRuntime::java_return_convention_max_float = 1;\n+int SharedRuntime::java_return_convention(const BasicType *sig_bt,\n+                                          VMRegPair *regs,\n+                                          int total_args_passed) {\n+  Unimplemented();\n+  return 0;\n+}\n+\n@@ -601,3 +610,1 @@\n-                            int total_args_passed,\n-                            int comp_args_on_stack,\n-                            const BasicType *sig_bt,\n+                            const GrowableArray<SigEntry>& sig_extended,\n@@ -605,1 +612,5 @@\n-                            Label& skip_fixup) {\n+                            Label& skip_fixup,\n+                            address start,\n+                            OopMapSet*& oop_maps,\n+                            int& frame_complete,\n+                            int& frame_size_in_words) {\n@@ -627,1 +638,1 @@\n-  int extraspace = total_args_passed * Interpreter::stackElementSize;\n+  int extraspace = sig_extended.length() * Interpreter::stackElementSize;\n@@ -638,3 +649,3 @@\n-  for (int i = 0; i < total_args_passed; i++) {\n-    if (sig_bt[i] == T_VOID) {\n-      assert(i > 0 && (sig_bt[i-1] == T_LONG || sig_bt[i-1] == T_DOUBLE), \"missing half\");\n+  for (int i = 0; i < sig_extended.length(); i++) {\n+    if (sig_extended.at(i)._bt == T_VOID) {\n+      assert(i > 0 && (sig_extended.at(i-1)._bt == T_LONG || sig_extended.at(i-1)._bt == T_DOUBLE), \"missing half\");\n@@ -645,1 +656,1 @@\n-    int st_off = ((total_args_passed - 1) - i) * Interpreter::stackElementSize;\n+    int st_off = ((sig_extended.length() - 1) - i) * Interpreter::stackElementSize;\n@@ -691,1 +702,1 @@\n-        assert(sig_bt[i] == T_DOUBLE || sig_bt[i] == T_LONG, \"wrong type\");\n+        assert(sig_extended.at(i)._bt == T_DOUBLE || sig_extended.at(i)._bt == T_LONG, \"wrong type\");\n@@ -724,2 +735,1 @@\n-                                    int total_args_passed,\n-                                    const BasicType *sig_bt,\n+                                    const GrowableArray<SigEntry>& sig_extended,\n@@ -728,0 +738,1 @@\n+\n@@ -821,2 +832,2 @@\n-  for (int i = 0; i < total_args_passed; i++) {\n-    if (sig_bt[i] == T_VOID) {\n+  for (int i = 0; i < sig_extended.length(); i++) {\n+    if (sig_extended.at(i)._bt == T_VOID) {\n@@ -825,1 +836,1 @@\n-      assert(i > 0 && (sig_bt[i-1] == T_LONG || sig_bt[i-1] == T_DOUBLE), \"missing half\");\n+      assert(i > 0 && (sig_extended.at(i-1)._bt == T_LONG || sig_extended.at(i-1)._bt == T_DOUBLE), \"missing half\");\n@@ -834,1 +845,1 @@\n-    int ld_off = (total_args_passed - i) * Interpreter::stackElementSize;\n+    int ld_off = (sig_extended.length() - i) * Interpreter::stackElementSize;\n@@ -932,2 +943,1 @@\n-                                                            int total_args_passed,\n-                                                            const BasicType *sig_bt,\n+                                                            const GrowableArray<SigEntry>& sig_extended,\n@@ -936,1 +946,2 @@\n-                                                            AdapterFingerPrint* fingerprint) {\n+                                                            AdapterFingerPrint* fingerprint,\n+                                                            AdapterBlob*& new_adapter) {\n@@ -939,1 +950,1 @@\n-  gen_i2c_adapter(masm, total_args_passed, comp_args_on_stack, sig_bt, regs);\n+  gen_i2c_adapter(masm, comp_args_on_stack, sig_extended, regs);\n@@ -972,1 +983,4 @@\n-  gen_c2i_adapter(masm, total_args_passed, comp_args_on_stack, sig_bt, regs, skip_fixup);\n+  OopMapSet* oop_maps = nullptr;\n+  int frame_complete = CodeOffsets::frame_never_safe;\n+  int frame_size_in_words = 0;\n+  gen_c2i_adapter(masm, sig_extended, regs, skip_fixup, i2c_entry, oop_maps, frame_complete, frame_size_in_words);\n@@ -974,0 +988,1 @@\n+  new_adapter = AdapterBlob::create(masm->code(), frame_complete, frame_size_in_words, oop_maps);\n@@ -2647,0 +2662,5 @@\n+BufferedInlineTypeBlob* SharedRuntime::generate_buffered_inline_type_adapter(const InlineKlass* vk) {\n+  Unimplemented();\n+  return nullptr;\n+}\n+\n","filename":"src\/hotspot\/cpu\/x86\/sharedRuntime_x86_32.cpp","additions":41,"deletions":21,"binary":false,"changes":62,"status":"modified"},{"patch":"@@ -31,0 +31,1 @@\n+#include \"classfile\/symbolTable.hpp\"\n@@ -637,0 +638,81 @@\n+\/\/ Same as java_calling_convention() but for multiple return\n+\/\/ values. There's no way to store them on the stack so if we don't\n+\/\/ have enough registers, multiple values can't be returned.\n+const uint SharedRuntime::java_return_convention_max_int = Argument::n_int_register_parameters_j+1;\n+const uint SharedRuntime::java_return_convention_max_float = Argument::n_float_register_parameters_j;\n+int SharedRuntime::java_return_convention(const BasicType *sig_bt,\n+                                          VMRegPair *regs,\n+                                          int total_args_passed) {\n+  \/\/ Create the mapping between argument positions and\n+  \/\/ registers.\n+  static const Register INT_ArgReg[java_return_convention_max_int] = {\n+    rax, j_rarg5, j_rarg4, j_rarg3, j_rarg2, j_rarg1, j_rarg0\n+  };\n+  static const XMMRegister FP_ArgReg[java_return_convention_max_float] = {\n+    j_farg0, j_farg1, j_farg2, j_farg3,\n+    j_farg4, j_farg5, j_farg6, j_farg7\n+  };\n+\n+\n+  uint int_args = 0;\n+  uint fp_args = 0;\n+\n+  for (int i = 0; i < total_args_passed; i++) {\n+    switch (sig_bt[i]) {\n+    case T_BOOLEAN:\n+    case T_CHAR:\n+    case T_BYTE:\n+    case T_SHORT:\n+    case T_INT:\n+      if (int_args < Argument::n_int_register_parameters_j+1) {\n+        regs[i].set1(INT_ArgReg[int_args]->as_VMReg());\n+        int_args++;\n+      } else {\n+        return -1;\n+      }\n+      break;\n+    case T_VOID:\n+      \/\/ halves of T_LONG or T_DOUBLE\n+      assert(i != 0 && (sig_bt[i - 1] == T_LONG || sig_bt[i - 1] == T_DOUBLE), \"expecting half\");\n+      regs[i].set_bad();\n+      break;\n+    case T_LONG:\n+      assert(sig_bt[i + 1] == T_VOID, \"expecting half\");\n+      \/\/ fall through\n+    case T_OBJECT:\n+    case T_ARRAY:\n+    case T_ADDRESS:\n+    case T_METADATA:\n+      if (int_args < Argument::n_int_register_parameters_j+1) {\n+        regs[i].set2(INT_ArgReg[int_args]->as_VMReg());\n+        int_args++;\n+      } else {\n+        return -1;\n+      }\n+      break;\n+    case T_FLOAT:\n+      if (fp_args < Argument::n_float_register_parameters_j) {\n+        regs[i].set1(FP_ArgReg[fp_args]->as_VMReg());\n+        fp_args++;\n+      } else {\n+        return -1;\n+      }\n+      break;\n+    case T_DOUBLE:\n+      assert(sig_bt[i + 1] == T_VOID, \"expecting half\");\n+      if (fp_args < Argument::n_float_register_parameters_j) {\n+        regs[i].set2(FP_ArgReg[fp_args]->as_VMReg());\n+        fp_args++;\n+      } else {\n+        return -1;\n+      }\n+      break;\n+    default:\n+      ShouldNotReachHere();\n+      break;\n+    }\n+  }\n+\n+  return int_args + fp_args;\n+}\n+\n@@ -679,0 +761,105 @@\n+\/\/ For each inline type argument, sig includes the list of fields of\n+\/\/ the inline type. This utility function computes the number of\n+\/\/ arguments for the call if inline types are passed by reference (the\n+\/\/ calling convention the interpreter expects).\n+static int compute_total_args_passed_int(const GrowableArray<SigEntry>* sig_extended) {\n+  int total_args_passed = 0;\n+  if (InlineTypePassFieldsAsArgs) {\n+    for (int i = 0; i < sig_extended->length(); i++) {\n+      BasicType bt = sig_extended->at(i)._bt;\n+      if (bt == T_METADATA) {\n+        \/\/ In sig_extended, an inline type argument starts with:\n+        \/\/ T_METADATA, followed by the types of the fields of the\n+        \/\/ inline type and T_VOID to mark the end of the value\n+        \/\/ type. Inline types are flattened so, for instance, in the\n+        \/\/ case of an inline type with an int field and an inline type\n+        \/\/ field that itself has 2 fields, an int and a long:\n+        \/\/ T_METADATA T_INT T_METADATA T_INT T_LONG T_VOID (second\n+        \/\/ slot for the T_LONG) T_VOID (inner inline type) T_VOID\n+        \/\/ (outer inline type)\n+        total_args_passed++;\n+        int vt = 1;\n+        do {\n+          i++;\n+          BasicType bt = sig_extended->at(i)._bt;\n+          BasicType prev_bt = sig_extended->at(i-1)._bt;\n+          if (bt == T_METADATA) {\n+            vt++;\n+          } else if (bt == T_VOID &&\n+                     prev_bt != T_LONG &&\n+                     prev_bt != T_DOUBLE) {\n+            vt--;\n+          }\n+        } while (vt != 0);\n+      } else {\n+        total_args_passed++;\n+      }\n+    }\n+  } else {\n+    total_args_passed = sig_extended->length();\n+  }\n+  return total_args_passed;\n+}\n+\n+\n+static void gen_c2i_adapter_helper(MacroAssembler* masm,\n+                                   BasicType bt,\n+                                   BasicType prev_bt,\n+                                   size_t size_in_bytes,\n+                                   const VMRegPair& reg_pair,\n+                                   const Address& to,\n+                                   int extraspace,\n+                                   bool is_oop) {\n+  if (bt == T_VOID) {\n+    assert(prev_bt == T_LONG || prev_bt == T_DOUBLE, \"missing half\");\n+    return;\n+  }\n+\n+  \/\/ Say 4 args:\n+  \/\/ i   st_off\n+  \/\/ 0   32 T_LONG\n+  \/\/ 1   24 T_VOID\n+  \/\/ 2   16 T_OBJECT\n+  \/\/ 3    8 T_BOOL\n+  \/\/ -    0 return address\n+  \/\/\n+  \/\/ However to make thing extra confusing. Because we can fit a long\/double in\n+  \/\/ a single slot on a 64 bt vm and it would be silly to break them up, the interpreter\n+  \/\/ leaves one slot empty and only stores to a single slot. In this case the\n+  \/\/ slot that is occupied is the T_VOID slot. See I said it was confusing.\n+\n+  bool wide = (size_in_bytes == wordSize);\n+  VMReg r_1 = reg_pair.first();\n+  VMReg r_2 = reg_pair.second();\n+  assert(r_2->is_valid() == wide, \"invalid size\");\n+  if (!r_1->is_valid()) {\n+    assert(!r_2->is_valid(), \"must be invalid\");\n+    return;\n+  }\n+\n+  if (!r_1->is_XMMRegister()) {\n+    Register val = rax;\n+    if (r_1->is_stack()) {\n+      int ld_off = r_1->reg2stack() * VMRegImpl::stack_slot_size + extraspace;\n+      __ load_sized_value(val, Address(rsp, ld_off), size_in_bytes, \/* is_signed *\/ false);\n+    } else {\n+      val = r_1->as_Register();\n+    }\n+    assert_different_registers(to.base(), val, rscratch1);\n+    if (is_oop) {\n+      __ push(r13);\n+      __ push(rbx);\n+      __ store_heap_oop(to, val, rscratch1, r13, rbx, IN_HEAP | ACCESS_WRITE | IS_DEST_UNINITIALIZED);\n+      __ pop(rbx);\n+      __ pop(r13);\n+    } else {\n+      __ store_sized_value(to, val, size_in_bytes);\n+    }\n+  } else {\n+    if (wide) {\n+      __ movdbl(to, r_1->as_XMMRegister());\n+    } else {\n+      __ movflt(to, r_1->as_XMMRegister());\n+    }\n+  }\n+}\n@@ -681,3 +868,1 @@\n-                            int total_args_passed,\n-                            int comp_args_on_stack,\n-                            const BasicType *sig_bt,\n+                            const GrowableArray<SigEntry>* sig_extended,\n@@ -685,1 +870,32 @@\n-                            Label& skip_fixup) {\n+                            bool requires_clinit_barrier,\n+                            address& c2i_no_clinit_check_entry,\n+                            Label& skip_fixup,\n+                            address start,\n+                            OopMapSet* oop_maps,\n+                            int& frame_complete,\n+                            int& frame_size_in_words,\n+                            bool alloc_inline_receiver) {\n+  if (requires_clinit_barrier && VM_Version::supports_fast_class_init_checks()) {\n+    Label L_skip_barrier;\n+    Register method = rbx;\n+\n+    { \/\/ Bypass the barrier for non-static methods\n+      Register flags = rscratch1;\n+      __ movl(flags, Address(method, Method::access_flags_offset()));\n+      __ testl(flags, JVM_ACC_STATIC);\n+      __ jcc(Assembler::zero, L_skip_barrier); \/\/ non-static\n+    }\n+\n+    Register klass = rscratch1;\n+    __ load_method_holder(klass, method);\n+    __ clinit_barrier(klass, r15_thread, &L_skip_barrier \/*L_fast_path*\/);\n+\n+    __ jump(RuntimeAddress(SharedRuntime::get_handle_wrong_method_stub())); \/\/ slow path\n+\n+    __ bind(L_skip_barrier);\n+    c2i_no_clinit_check_entry = __ pc();\n+  }\n+\n+  BarrierSetAssembler* bs = BarrierSet::barrier_set()->barrier_set_assembler();\n+  bs->c2i_entry_barrier(masm);\n+\n@@ -695,0 +911,42 @@\n+  if (InlineTypePassFieldsAsArgs) {\n+    \/\/ Is there an inline type argument?\n+    bool has_inline_argument = false;\n+    for (int i = 0; i < sig_extended->length() && !has_inline_argument; i++) {\n+      has_inline_argument = (sig_extended->at(i)._bt == T_METADATA);\n+    }\n+    if (has_inline_argument) {\n+      \/\/ There is at least an inline type argument: we're coming from\n+      \/\/ compiled code so we have no buffers to back the inline types.\n+      \/\/ Allocate the buffers here with a runtime call.\n+      OopMap* map = RegisterSaver::save_live_registers(masm, 0, &frame_size_in_words, \/*save_vectors*\/ false);\n+\n+      frame_complete = __ offset();\n+\n+      __ set_last_Java_frame(noreg, noreg, nullptr, rscratch1);\n+\n+      __ mov(c_rarg0, r15_thread);\n+      __ mov(c_rarg1, rbx);\n+      __ mov64(c_rarg2, (int64_t)alloc_inline_receiver);\n+      __ call(RuntimeAddress(CAST_FROM_FN_PTR(address, SharedRuntime::allocate_inline_types)));\n+\n+      oop_maps->add_gc_map((int)(__ pc() - start), map);\n+      __ reset_last_Java_frame(false);\n+\n+      RegisterSaver::restore_live_registers(masm);\n+\n+      Label no_exception;\n+      __ cmpptr(Address(r15_thread, Thread::pending_exception_offset()), NULL_WORD);\n+      __ jcc(Assembler::equal, no_exception);\n+\n+      __ movptr(Address(r15_thread, JavaThread::vm_result_offset()), NULL_WORD);\n+      __ movptr(rax, Address(r15_thread, Thread::pending_exception_offset()));\n+      __ jump(RuntimeAddress(StubRoutines::forward_exception_entry()));\n+\n+      __ bind(no_exception);\n+\n+      \/\/ We get an array of objects from the runtime call\n+      __ get_vm_result(rscratch2, r15_thread); \/\/ Use rscratch2 (r11) as temporary because rscratch1 (r10) is trashed by movptr()\n+      __ get_vm_result_2(rbx, r15_thread); \/\/ TODO: required to keep the callee Method live?\n+    }\n+  }\n+\n@@ -697,1 +955,1 @@\n-\n+  int total_args_passed = compute_total_args_passed_int(sig_extended);\n@@ -732,46 +990,24 @@\n-  for (int i = 0; i < total_args_passed; i++) {\n-    if (sig_bt[i] == T_VOID) {\n-      assert(i > 0 && (sig_bt[i-1] == T_LONG || sig_bt[i-1] == T_DOUBLE), \"missing half\");\n-      continue;\n-    }\n-\n-    \/\/ offset to start parameters\n-    int st_off   = (total_args_passed - i) * Interpreter::stackElementSize;\n-    int next_off = st_off - Interpreter::stackElementSize;\n-\n-    \/\/ Say 4 args:\n-    \/\/ i   st_off\n-    \/\/ 0   32 T_LONG\n-    \/\/ 1   24 T_VOID\n-    \/\/ 2   16 T_OBJECT\n-    \/\/ 3    8 T_BOOL\n-    \/\/ -    0 return address\n-    \/\/\n-    \/\/ However to make thing extra confusing. Because we can fit a long\/double in\n-    \/\/ a single slot on a 64 bt vm and it would be silly to break them up, the interpreter\n-    \/\/ leaves one slot empty and only stores to a single slot. In this case the\n-    \/\/ slot that is occupied is the T_VOID slot. See I said it was confusing.\n-\n-    VMReg r_1 = regs[i].first();\n-    VMReg r_2 = regs[i].second();\n-    if (!r_1->is_valid()) {\n-      assert(!r_2->is_valid(), \"\");\n-      continue;\n-    }\n-    if (r_1->is_stack()) {\n-      \/\/ memory to memory use rax\n-      int ld_off = r_1->reg2stack() * VMRegImpl::stack_slot_size + extraspace;\n-      if (!r_2->is_valid()) {\n-        \/\/ sign extend??\n-        __ movl(rax, Address(rsp, ld_off));\n-        __ movptr(Address(rsp, st_off), rax);\n-\n-      } else {\n-        __ movq(rax, Address(rsp, ld_off));\n-\n-        \/\/ Two VMREgs|OptoRegs can be T_OBJECT, T_ADDRESS, T_DOUBLE, T_LONG\n-        \/\/ T_DOUBLE and T_LONG use two slots in the interpreter\n-        if ( sig_bt[i] == T_LONG || sig_bt[i] == T_DOUBLE) {\n-          \/\/ ld_off == LSW, ld_off+wordSize == MSW\n-          \/\/ st_off == MSW, next_off == LSW\n-          __ movq(Address(rsp, next_off), rax);\n+  \/\/ next_arg_comp is the next argument from the compiler point of\n+  \/\/ view (inline type fields are passed in registers\/on the stack). In\n+  \/\/ sig_extended, an inline type argument starts with: T_METADATA,\n+  \/\/ followed by the types of the fields of the inline type and T_VOID\n+  \/\/ to mark the end of the inline type. ignored counts the number of\n+  \/\/ T_METADATA\/T_VOID. next_vt_arg is the next inline type argument:\n+  \/\/ used to get the buffer for that argument from the pool of buffers\n+  \/\/ we allocated above and want to pass to the\n+  \/\/ interpreter. next_arg_int is the next argument from the\n+  \/\/ interpreter point of view (inline types are passed by reference).\n+  for (int next_arg_comp = 0, ignored = 0, next_vt_arg = 0, next_arg_int = 0;\n+       next_arg_comp < sig_extended->length(); next_arg_comp++) {\n+    assert(ignored <= next_arg_comp, \"shouldn't skip over more slots than there are arguments\");\n+    assert(next_arg_int <= total_args_passed, \"more arguments for the interpreter than expected?\");\n+    BasicType bt = sig_extended->at(next_arg_comp)._bt;\n+    int st_off = (total_args_passed - next_arg_int) * Interpreter::stackElementSize;\n+    if (!InlineTypePassFieldsAsArgs || bt != T_METADATA) {\n+      int next_off = st_off - Interpreter::stackElementSize;\n+      const int offset = (bt == T_LONG || bt == T_DOUBLE) ? next_off : st_off;\n+      const VMRegPair reg_pair = regs[next_arg_comp-ignored];\n+      size_t size_in_bytes = reg_pair.second()->is_valid() ? 8 : 4;\n+      gen_c2i_adapter_helper(masm, bt, next_arg_comp > 0 ? sig_extended->at(next_arg_comp-1)._bt : T_ILLEGAL,\n+                             size_in_bytes, reg_pair, Address(rsp, offset), extraspace, false);\n+      next_arg_int++;\n@@ -780,7 +1016,4 @@\n-          \/\/ Overwrite the unused slot with known junk\n-          __ mov64(rax, CONST64(0xdeadffffdeadaaaa));\n-          __ movptr(Address(rsp, st_off), rax);\n-#endif \/* ASSERT *\/\n-        } else {\n-          __ movq(Address(rsp, st_off), rax);\n-        }\n+      if (bt == T_LONG || bt == T_DOUBLE) {\n+        \/\/ Overwrite the unused slot with known junk\n+        __ mov64(rax, CONST64(0xdeadffffdeadaaaa));\n+        __ movptr(Address(rsp, st_off), rax);\n@@ -788,16 +1021,26 @@\n-    } else if (r_1->is_Register()) {\n-      Register r = r_1->as_Register();\n-      if (!r_2->is_valid()) {\n-        \/\/ must be only an int (or less ) so move only 32bits to slot\n-        \/\/ why not sign extend??\n-        __ movl(Address(rsp, st_off), r);\n-      } else {\n-        \/\/ Two VMREgs|OptoRegs can be T_OBJECT, T_ADDRESS, T_DOUBLE, T_LONG\n-        \/\/ T_DOUBLE and T_LONG use two slots in the interpreter\n-        if ( sig_bt[i] == T_LONG || sig_bt[i] == T_DOUBLE) {\n-          \/\/ long\/double in gpr\n-#ifdef ASSERT\n-          \/\/ Overwrite the unused slot with known junk\n-          __ mov64(rax, CONST64(0xdeadffffdeadaaab));\n-          __ movptr(Address(rsp, st_off), rax);\n-          __ movq(Address(rsp, next_off), r);\n+    } else {\n+      ignored++;\n+      \/\/ get the buffer from the just allocated pool of buffers\n+      int index = arrayOopDesc::base_offset_in_bytes(T_OBJECT) + next_vt_arg * type2aelembytes(T_OBJECT);\n+      __ load_heap_oop(r14, Address(rscratch2, index));\n+      next_vt_arg++; next_arg_int++;\n+      int vt = 1;\n+      \/\/ write fields we get from compiled code in registers\/stack\n+      \/\/ slots to the buffer: we know we are done with that inline type\n+      \/\/ argument when we hit the T_VOID that acts as an end of inline\n+      \/\/ type delimiter for this inline type. Inline types are flattened\n+      \/\/ so we might encounter embedded inline types. Each entry in\n+      \/\/ sig_extended contains a field offset in the buffer.\n+      Label L_null;\n+      do {\n+        next_arg_comp++;\n+        BasicType bt = sig_extended->at(next_arg_comp)._bt;\n+        BasicType prev_bt = sig_extended->at(next_arg_comp-1)._bt;\n+        if (bt == T_METADATA) {\n+          vt++;\n+          ignored++;\n+        } else if (bt == T_VOID &&\n+                   prev_bt != T_LONG &&\n+                   prev_bt != T_DOUBLE) {\n+          vt--;\n+          ignored++;\n@@ -806,1 +1049,22 @@\n-          __ movptr(Address(rsp, st_off), r);\n+          int off = sig_extended->at(next_arg_comp)._offset;\n+          if (off == -1) {\n+            \/\/ Nullable inline type argument, emit null check\n+            VMReg reg = regs[next_arg_comp-ignored].first();\n+            Label L_notNull;\n+            if (reg->is_stack()) {\n+              int ld_off = reg->reg2stack() * VMRegImpl::stack_slot_size + extraspace;\n+              __ testb(Address(rsp, ld_off), 1);\n+            } else {\n+              __ testb(reg->as_Register(), 1);\n+            }\n+            __ jcc(Assembler::notZero, L_notNull);\n+            __ movptr(Address(rsp, st_off), 0);\n+            __ jmp(L_null);\n+            __ bind(L_notNull);\n+            continue;\n+          }\n+          assert(off > 0, \"offset in object should be positive\");\n+          size_t size_in_bytes = is_java_primitive(bt) ? type2aelembytes(bt) : wordSize;\n+          bool is_oop = is_reference_type(bt);\n+          gen_c2i_adapter_helper(masm, bt, next_arg_comp > 0 ? sig_extended->at(next_arg_comp-1)._bt : T_ILLEGAL,\n+                                 size_in_bytes, regs[next_arg_comp-ignored], Address(r14, off), extraspace, is_oop);\n@@ -808,14 +1072,4 @@\n-      }\n-    } else {\n-      assert(r_1->is_XMMRegister(), \"\");\n-      if (!r_2->is_valid()) {\n-        \/\/ only a float use just part of the slot\n-        __ movflt(Address(rsp, st_off), r_1->as_XMMRegister());\n-      } else {\n-#ifdef ASSERT\n-        \/\/ Overwrite the unused slot with known junk\n-        __ mov64(rax, CONST64(0xdeadffffdeadaaac));\n-        __ movptr(Address(rsp, st_off), rax);\n-#endif \/* ASSERT *\/\n-        __ movdbl(Address(rsp, next_off), r_1->as_XMMRegister());\n-      }\n+      } while (vt != 0);\n+      \/\/ pass the buffer to the interpreter\n+      __ movptr(Address(rsp, st_off), r14);\n+      __ bind(L_null);\n@@ -844,2 +1098,1 @@\n-                                    int total_args_passed,\n-                                    const BasicType *sig_bt,\n+                                    const GrowableArray<SigEntry>* sig,\n@@ -937,1 +1190,1 @@\n-  __ movptr(r11, Address(rbx, in_bytes(Method::from_compiled_offset())));\n+  __ movptr(r11, Address(rbx, in_bytes(Method::from_compiled_inline_offset())));\n@@ -951,0 +1204,2 @@\n+  int total_args_passed = sig->length();\n+\n@@ -954,1 +1209,2 @@\n-    if (sig_bt[i] == T_VOID) {\n+    BasicType bt = sig->at(i)._bt;\n+    if (bt == T_VOID) {\n@@ -957,1 +1213,2 @@\n-      assert(i > 0 && (sig_bt[i-1] == T_LONG || sig_bt[i-1] == T_DOUBLE), \"missing half\");\n+      BasicType prev_bt = (i > 0) ? sig->at(i-1)._bt : T_ILLEGAL;\n+      assert(i > 0 && (prev_bt == T_LONG || prev_bt == T_DOUBLE), \"missing half\");\n@@ -999,1 +1256,1 @@\n-        const int offset = (sig_bt[i]==T_LONG||sig_bt[i]==T_DOUBLE)?\n+        const int offset = (bt==T_LONG||bt==T_DOUBLE)?\n@@ -1014,1 +1271,1 @@\n-        const int offset = (sig_bt[i]==T_LONG||sig_bt[i]==T_DOUBLE)?\n+        const int offset = (bt==T_LONG||bt==T_DOUBLE)?\n@@ -1047,1 +1304,1 @@\n-  \/\/ only needed because eof c2 resolve stubs return Method* as a result in\n+  \/\/ only needed because of c2 resolve stubs return Method* as a result in\n@@ -1053,0 +1310,13 @@\n+static void gen_inline_cache_check(MacroAssembler *masm, Label& skip_fixup) {\n+  Register data = rax;\n+  __ ic_check(1 \/* end_alignment *\/);\n+  __ movptr(rbx, Address(data, CompiledICData::speculated_method_offset()));\n+\n+  \/\/ Method might have been compiled since the call site was patched to\n+  \/\/ interpreted if that is the case treat it as a miss so we can get\n+  \/\/ the call site corrected.\n+  __ cmpptr(Address(rbx, in_bytes(Method::code_offset())), NULL_WORD);\n+  __ jcc(Assembler::equal, skip_fixup);\n+  __ jump(RuntimeAddress(SharedRuntime::get_ic_miss_stub()));\n+}\n+\n@@ -1054,2 +1324,1 @@\n-AdapterHandlerEntry* SharedRuntime::generate_i2c2i_adapters(MacroAssembler *masm,\n-                                                            int total_args_passed,\n+AdapterHandlerEntry* SharedRuntime::generate_i2c2i_adapters(MacroAssembler* masm,\n@@ -1057,3 +1326,9 @@\n-                                                            const BasicType *sig_bt,\n-                                                            const VMRegPair *regs,\n-                                                            AdapterFingerPrint* fingerprint) {\n+                                                            const GrowableArray<SigEntry>* sig,\n+                                                            const VMRegPair* regs,\n+                                                            const GrowableArray<SigEntry>* sig_cc,\n+                                                            const VMRegPair* regs_cc,\n+                                                            const GrowableArray<SigEntry>* sig_cc_ro,\n+                                                            const VMRegPair* regs_cc_ro,\n+                                                            AdapterFingerPrint* fingerprint,\n+                                                            AdapterBlob*& new_adapter,\n+                                                            bool allocate_code_blob) {\n@@ -1061,2 +1336,1 @@\n-\n-  gen_i2c_adapter(masm, total_args_passed, comp_args_on_stack, sig_bt, regs);\n+  gen_i2c_adapter(masm, comp_args_on_stack, sig, regs);\n@@ -1073,1 +1347,2 @@\n-  address c2i_unverified_entry = __ pc();\n+  address c2i_unverified_entry        = __ pc();\n+  address c2i_unverified_inline_entry = __ pc();\n@@ -1076,14 +1351,1 @@\n-  Register data = rax;\n-  Register receiver = j_rarg0;\n-  Register temp = rbx;\n-\n-  {\n-    __ ic_check(1 \/* end_alignment *\/);\n-    __ movptr(rbx, Address(data, CompiledICData::speculated_method_offset()));\n-    \/\/ Method might have been compiled since the call site was patched to\n-    \/\/ interpreted if that is the case treat it as a miss so we can get\n-    \/\/ the call site corrected.\n-    __ cmpptr(Address(rbx, in_bytes(Method::code_offset())), NULL_WORD);\n-    __ jcc(Assembler::equal, skip_fixup);\n-    __ jump(RuntimeAddress(SharedRuntime::get_ic_miss_stub()));\n-  }\n+  gen_inline_cache_check(masm, skip_fixup);\n@@ -1091,1 +1353,3 @@\n-  address c2i_entry = __ pc();\n+  OopMapSet* oop_maps = new OopMapSet();\n+  int frame_complete = CodeOffsets::frame_never_safe;\n+  int frame_size_in_words = 0;\n@@ -1093,1 +1357,1 @@\n-  \/\/ Class initialization barrier for static methods\n+  \/\/ Scalarized c2i adapter with non-scalarized receiver (i.e., don't pack receiver)\n@@ -1095,10 +1359,7 @@\n-  if (VM_Version::supports_fast_class_init_checks()) {\n-    Label L_skip_barrier;\n-    Register method = rbx;\n-\n-    { \/\/ Bypass the barrier for non-static methods\n-      Register flags = rscratch1;\n-      __ movl(flags, Address(method, Method::access_flags_offset()));\n-      __ testl(flags, JVM_ACC_STATIC);\n-      __ jcc(Assembler::zero, L_skip_barrier); \/\/ non-static\n-    }\n+  address c2i_inline_ro_entry = __ pc();\n+  if (regs_cc != regs_cc_ro) {\n+    \/\/ No class init barrier needed because method is guaranteed to be non-static\n+    gen_c2i_adapter(masm, sig_cc_ro, regs_cc_ro, \/* requires_clinit_barrier = *\/ false, c2i_no_clinit_check_entry,\n+                    skip_fixup, i2c_entry, oop_maps, frame_complete, frame_size_in_words, \/* alloc_inline_receiver = *\/ false);\n+    skip_fixup.reset();\n+  }\n@@ -1106,3 +1367,5 @@\n-    Register klass = rscratch1;\n-    __ load_method_holder(klass, method);\n-    __ clinit_barrier(klass, r15_thread, &L_skip_barrier \/*L_fast_path*\/);\n+  \/\/ Scalarized c2i adapter\n+  address c2i_entry        = __ pc();\n+  address c2i_inline_entry = __ pc();\n+  gen_c2i_adapter(masm, sig_cc, regs_cc, \/* requires_clinit_barrier = *\/ true, c2i_no_clinit_check_entry,\n+                  skip_fixup, i2c_entry, oop_maps, frame_complete, frame_size_in_words, \/* alloc_inline_receiver = *\/ true);\n@@ -1110,1 +1373,5 @@\n-    __ jump(RuntimeAddress(SharedRuntime::get_handle_wrong_method_stub())); \/\/ slow path\n+  \/\/ Non-scalarized c2i adapter\n+  if (regs != regs_cc) {\n+    c2i_unverified_inline_entry = __ pc();\n+    Label inline_entry_skip_fixup;\n+    gen_inline_cache_check(masm, inline_entry_skip_fixup);\n@@ -1112,2 +1379,3 @@\n-    __ bind(L_skip_barrier);\n-    c2i_no_clinit_check_entry = __ pc();\n+    c2i_inline_entry = __ pc();\n+    gen_c2i_adapter(masm, sig, regs, \/* requires_clinit_barrier = *\/ true, c2i_no_clinit_check_entry,\n+                    inline_entry_skip_fixup, i2c_entry, oop_maps, frame_complete, frame_size_in_words, \/* alloc_inline_receiver = *\/ false);\n@@ -1116,4 +1384,6 @@\n-  BarrierSetAssembler* bs = BarrierSet::barrier_set()->barrier_set_assembler();\n-  bs->c2i_entry_barrier(masm);\n-\n-  gen_c2i_adapter(masm, total_args_passed, comp_args_on_stack, sig_bt, regs, skip_fixup);\n+  \/\/ The c2i adapters might safepoint and trigger a GC. The caller must make sure that\n+  \/\/ the GC knows about the location of oop argument locations passed to the c2i adapter.\n+  if (allocate_code_blob) {\n+    bool caller_must_gc_arguments = (regs != regs_cc);\n+    new_adapter = AdapterBlob::create(masm->code(), frame_complete, frame_size_in_words, oop_maps, caller_must_gc_arguments);\n+  }\n@@ -1121,1 +1391,1 @@\n-  return AdapterHandlerLibrary::new_entry(fingerprint, i2c_entry, c2i_entry, c2i_unverified_entry, c2i_no_clinit_check_entry);\n+  return AdapterHandlerLibrary::new_entry(fingerprint, i2c_entry, c2i_entry, c2i_inline_entry, c2i_inline_ro_entry, c2i_unverified_entry, c2i_unverified_inline_entry, c2i_no_clinit_check_entry);\n@@ -2248,0 +2518,4 @@\n+      if (EnableValhalla) {\n+        \/\/ Mask inline_type bit such that we go to the slow path if object is an inline type\n+        __ andptr(swap_reg, ~((int) markWord::inline_type_bit_in_place));\n+      }\n@@ -3589,0 +3863,111 @@\n+BufferedInlineTypeBlob* SharedRuntime::generate_buffered_inline_type_adapter(const InlineKlass* vk) {\n+  BufferBlob* buf = BufferBlob::create(\"inline types pack\/unpack\", 16 * K);\n+  CodeBuffer buffer(buf);\n+  short buffer_locs[20];\n+  buffer.insts()->initialize_shared_locs((relocInfo*)buffer_locs,\n+                                         sizeof(buffer_locs)\/sizeof(relocInfo));\n+\n+  MacroAssembler* masm = new MacroAssembler(&buffer);\n+\n+  const Array<SigEntry>* sig_vk = vk->extended_sig();\n+  const Array<VMRegPair>* regs = vk->return_regs();\n+\n+  int pack_fields_jobject_off = __ offset();\n+  \/\/ Resolve pre-allocated buffer from JNI handle.\n+  \/\/ We cannot do this in generate_call_stub() because it requires GC code to be initialized.\n+  __ movptr(rax, Address(r13, 0));\n+  __ resolve_jobject(rax \/* value *\/,\n+                     r15_thread \/* thread *\/,\n+                     r12 \/* tmp *\/);\n+  __ movptr(Address(r13, 0), rax);\n+\n+  int pack_fields_off = __ offset();\n+\n+  int j = 1;\n+  for (int i = 0; i < sig_vk->length(); i++) {\n+    BasicType bt = sig_vk->at(i)._bt;\n+    if (bt == T_METADATA) {\n+      continue;\n+    }\n+    if (bt == T_VOID) {\n+      if (sig_vk->at(i-1)._bt == T_LONG ||\n+          sig_vk->at(i-1)._bt == T_DOUBLE) {\n+        j++;\n+      }\n+      continue;\n+    }\n+    int off = sig_vk->at(i)._offset;\n+    assert(off > 0, \"offset in object should be positive\");\n+    VMRegPair pair = regs->at(j);\n+    VMReg r_1 = pair.first();\n+    VMReg r_2 = pair.second();\n+    Address to(rax, off);\n+    if (bt == T_FLOAT) {\n+      __ movflt(to, r_1->as_XMMRegister());\n+    } else if (bt == T_DOUBLE) {\n+      __ movdbl(to, r_1->as_XMMRegister());\n+    } else {\n+      Register val = r_1->as_Register();\n+      assert_different_registers(to.base(), val, r14, r13, rbx, rscratch1);\n+      if (is_reference_type(bt)) {\n+        __ store_heap_oop(to, val, r14, r13, rbx, IN_HEAP | ACCESS_WRITE | IS_DEST_UNINITIALIZED);\n+      } else {\n+        __ store_sized_value(to, r_1->as_Register(), type2aelembytes(bt));\n+      }\n+    }\n+    j++;\n+  }\n+  assert(j == regs->length(), \"missed a field?\");\n+\n+  __ ret(0);\n+\n+  int unpack_fields_off = __ offset();\n+\n+  Label skip;\n+  __ testptr(rax, rax);\n+  __ jcc(Assembler::zero, skip);\n+\n+  j = 1;\n+  for (int i = 0; i < sig_vk->length(); i++) {\n+    BasicType bt = sig_vk->at(i)._bt;\n+    if (bt == T_METADATA) {\n+      continue;\n+    }\n+    if (bt == T_VOID) {\n+      if (sig_vk->at(i-1)._bt == T_LONG ||\n+          sig_vk->at(i-1)._bt == T_DOUBLE) {\n+        j++;\n+      }\n+      continue;\n+    }\n+    int off = sig_vk->at(i)._offset;\n+    assert(off > 0, \"offset in object should be positive\");\n+    VMRegPair pair = regs->at(j);\n+    VMReg r_1 = pair.first();\n+    VMReg r_2 = pair.second();\n+    Address from(rax, off);\n+    if (bt == T_FLOAT) {\n+      __ movflt(r_1->as_XMMRegister(), from);\n+    } else if (bt == T_DOUBLE) {\n+      __ movdbl(r_1->as_XMMRegister(), from);\n+    } else if (bt == T_OBJECT || bt == T_ARRAY) {\n+      assert_different_registers(rax, r_1->as_Register());\n+      __ load_heap_oop(r_1->as_Register(), from);\n+    } else {\n+      assert(is_java_primitive(bt), \"unexpected basic type\");\n+      assert_different_registers(rax, r_1->as_Register());\n+      size_t size_in_bytes = type2aelembytes(bt);\n+      __ load_sized_value(r_1->as_Register(), from, size_in_bytes, bt != T_CHAR && bt != T_BOOLEAN);\n+    }\n+    j++;\n+  }\n+  assert(j == regs->length(), \"missed a field?\");\n+\n+  __ bind(skip);\n+  __ ret(0);\n+\n+  __ flush();\n+\n+  return BufferedInlineTypeBlob::create(&buffer, pack_fields_off, pack_fields_jobject_off, unpack_fields_off);\n+}\n+\n@@ -3681,1 +4066,0 @@\n-\n","filename":"src\/hotspot\/cpu\/x86\/sharedRuntime_x86_64.cpp","additions":527,"deletions":143,"binary":false,"changes":670,"status":"modified"},{"patch":"@@ -54,7 +54,26 @@\n-AdapterHandlerEntry* SharedRuntime::generate_i2c2i_adapters(\n-                        MacroAssembler *masm,\n-                        int total_args_passed,\n-                        int comp_args_on_stack,\n-                        const BasicType *sig_bt,\n-                        const VMRegPair *regs,\n-                        AdapterFingerPrint *fingerprint) {\n+int SharedRuntime::java_return_convention(const BasicType *sig_bt,\n+                                           VMRegPair *regs,\n+                                           int total_args_passed) {\n+  Unimplemented();\n+  return 0;\n+}\n+\n+BufferedInlineTypeBlob* SharedRuntime::generate_buffered_inline_type_adapter(const InlineKlass* vk) {\n+  Unimplemented();\n+  return NULL;\n+}\n+\n+AdapterHandlerEntry* SharedRuntime::generate_i2c2i_adapters(MacroAssembler* masm,\n+                                                            int comp_args_on_stack,\n+                                                            const GrowableArray <SigEntry>* sig,\n+                                                            const VMRegPair* regs,\n+                                                            const GrowableArray <SigEntry>* sig_cc,\n+                                                            const VMRegPair* regs_cc,\n+                                                            const GrowableArray <SigEntry>* sig_cc_ro,\n+                                                            const VMRegPair* regs_cc_ro,\n+                                                            AdapterFingerPrint* fingerprint,\n+                                                            AdapterBlob*& new_adapter,\n+                                                            bool allocate_code_blob) {\n+  if (allocate_code_blob) {\n+    new_adapter = AdapterBlob::create(masm->code(), 0, 0, NULL);\n+  }\n@@ -65,0 +84,4 @@\n+    CAST_FROM_FN_PTR(address,zero_null_code_stub),\n+    CAST_FROM_FN_PTR(address,zero_null_code_stub),\n+    CAST_FROM_FN_PTR(address,zero_null_code_stub),\n+    CAST_FROM_FN_PTR(address,zero_null_code_stub),\n","filename":"src\/hotspot\/cpu\/zero\/sharedRuntime_zero.cpp","additions":30,"deletions":7,"binary":false,"changes":37,"status":"modified"},{"patch":"@@ -24,0 +24,2 @@\n+\n+#include \"oops\/inlineKlass.hpp\"\n@@ -54,0 +56,1 @@\n+#include \"oops\/inlineKlass.inline.hpp\"\n@@ -88,0 +91,1 @@\n+#include \"utilities\/stringUtils.hpp\"\n@@ -154,0 +158,2 @@\n+#define CONSTANT_CLASS_DESCRIPTORS        68\n+\n@@ -194,1 +200,1 @@\n-      case JVM_CONSTANT_Class : {\n+      case JVM_CONSTANT_Class: {\n@@ -515,0 +521,3 @@\n+\n+        Symbol* const name = cp->symbol_at(class_index);\n+        const unsigned int name_len = name->utf8_length();\n@@ -724,1 +733,1 @@\n-            } else if (!Signature::is_void_method(signature)) { \/\/ must have void signature.\n+            } else if (!Signature::is_void_method(signature)) {  \/\/ must have void signature.\n@@ -744,2 +753,3 @@\n-            if (ref_kind == JVM_REF_newInvokeSpecial) {\n-              if (name != vmSymbols::object_initializer_name()) {\n+\n+            if (name != vmSymbols::object_initializer_name()) { \/\/ !<init>\n+              if (ref_kind == JVM_REF_newInvokeSpecial) {\n@@ -751,2 +761,10 @@\n-            } else {\n-              if (name == vmSymbols::object_initializer_name()) {\n+            } else { \/\/ <init>\n+              \/\/ The allowed invocation mode of <init> depends on its signature.\n+              \/\/ This test corresponds to verify_invoke_instructions in the verifier.\n+              const int signature_ref_index =\n+                cp->signature_ref_index_at(name_and_type_ref_index);\n+              const Symbol* const signature = cp->symbol_at(signature_ref_index);\n+              if (signature->is_void_method_signature()\n+                  && ref_kind == JVM_REF_newInvokeSpecial) {\n+                \/\/ OK, could be a constructor call\n+              } else {\n@@ -804,4 +822,13 @@\n-\/\/ Side-effects: populates the _local_interfaces field\n-void ClassFileParser::parse_interfaces(const ClassFileStream* const stream,\n-                                       const int itfs_len,\n-                                       ConstantPool* const cp,\n+static void check_identity_and_value_modifiers(ClassFileParser* current, const InstanceKlass* super_type, TRAPS) {\n+  assert(super_type != nullptr,\"Method doesn't support null super type\");\n+  if (super_type->access_flags().is_identity_class() && !current->access_flags().is_identity_class()\n+      && super_type->name() != vmSymbols::java_lang_Object()) {\n+      THROW_MSG(vmSymbols::java_lang_IncompatibleClassChangeError(),\n+                err_msg(\"Value type %s has an identity type as supertype\",\n+                current->class_name()->as_klass_external_name()));\n+  }\n+}\n+\n+void ClassFileParser::parse_interfaces(const ClassFileStream* stream,\n+                                       int itfs_len,\n+                                       ConstantPool* cp,\n@@ -809,0 +836,6 @@\n+                                       \/\/ FIXME: lots of these functions\n+                                       \/\/ declare their parameters as const,\n+                                       \/\/ which adds only noise to the code.\n+                                       \/\/ Remove the spurious const modifiers.\n+                                       \/\/ Many are of the form \"const int x\"\n+                                       \/\/ or \"T* const x\".\n@@ -816,0 +849,1 @@\n+\n@@ -818,3 +852,2 @@\n-    _local_interfaces = MetadataFactory::new_array<InstanceKlass*>(_loader_data, itfs_len, nullptr, CHECK);\n-\n-    int index;\n+    _local_interface_indexes = new GrowableArray<u2>(itfs_len);\n+    int index = 0;\n@@ -823,1 +856,0 @@\n-      Klass* interf;\n@@ -828,30 +860,1 @@\n-      if (cp->tag_at(interface_index).is_klass()) {\n-        interf = cp->resolved_klass_at(interface_index);\n-      } else {\n-        Symbol* const unresolved_klass  = cp->klass_name_at(interface_index);\n-\n-        \/\/ Don't need to check legal name because it's checked when parsing constant pool.\n-        \/\/ But need to make sure it's not an array type.\n-        guarantee_property(unresolved_klass->char_at(0) != JVM_SIGNATURE_ARRAY,\n-                           \"Bad interface name in class file %s\", CHECK);\n-\n-        \/\/ Call resolve on the interface class name with class circularity checking\n-        interf = SystemDictionary::resolve_super_or_fail(_class_name,\n-                                                         unresolved_klass,\n-                                                         Handle(THREAD, _loader_data->class_loader()),\n-                                                         _protection_domain,\n-                                                         false, CHECK);\n-      }\n-\n-      if (!interf->is_interface()) {\n-        THROW_MSG(vmSymbols::java_lang_IncompatibleClassChangeError(),\n-                  err_msg(\"class %s can not implement %s, because it is not an interface (%s)\",\n-                          _class_name->as_klass_external_name(),\n-                          interf->external_name(),\n-                          interf->class_in_module_of_loader()));\n-      }\n-\n-      if (InstanceKlass::cast(interf)->has_nonstatic_concrete_methods()) {\n-        *has_nonstatic_concrete_methods = true;\n-      }\n-      _local_interfaces->at_put(index, InstanceKlass::cast(interf));\n+      _local_interface_indexes->at_put_grow(index, interface_index);\n@@ -869,2 +872,1 @@\n-      const InstanceKlass* const k = _local_interfaces->at(index);\n-      Symbol* interface_name = k->name();\n+      Symbol* interface_name = cp->klass_name_at(_local_interface_indexes->at(index));\n@@ -956,0 +958,3 @@\n+    _jdk_internal_ImplicitlyConstructible,\n+    _jdk_internal_LooselyConsistentValue,\n+    _jdk_internal_NullRestricted,\n@@ -1358,1 +1363,1 @@\n-                                   bool is_interface,\n+                                   AccessFlags class_access_flags,\n@@ -1371,0 +1376,1 @@\n+  bool is_inline_type = !class_access_flags.is_identity_class() && !class_access_flags.is_abstract();\n@@ -1378,1 +1384,5 @@\n-  const int total_fields = length + num_injected;\n+\n+  \/\/ two more slots are required for inline classes:\n+  \/\/ one for the static field with a reference to the pre-allocated default value\n+  \/\/ one for the field the JVM injects when detecting an empty inline class\n+  const int total_fields = length + num_injected + (is_inline_type ? 2 : 0);\n@@ -1384,0 +1394,1 @@\n+  int instance_fields_count = 0;\n@@ -1389,0 +1400,7 @@\n+    jint recognized_modifiers = JVM_RECOGNIZED_FIELD_MODIFIERS;\n+    if (!supports_inline_types()) {\n+      recognized_modifiers &= ~JVM_ACC_STRICT;\n+    }\n+\n+    const jint flags = cfs->get_u2_fast() & recognized_modifiers;\n+    verify_legal_field_modifiers(flags, class_access_flags, CHECK);\n@@ -1390,2 +1408,0 @@\n-    const jint flags = cfs->get_u2_fast() & JVM_RECOGNIZED_FIELD_MODIFIERS;\n-    verify_legal_field_modifiers(flags, is_interface, CHECK);\n@@ -1408,0 +1424,1 @@\n+    if (!access_flags.is_static()) instance_fields_count++;\n@@ -1415,0 +1432,2 @@\n+    bool is_null_restricted = false;\n+\n@@ -1434,0 +1453,10 @@\n+        if (parsed_annotations.has_annotation(AnnotationCollector::_jdk_internal_NullRestricted)) {\n+          if (!Signature::has_envelope(sig)) {\n+            Exceptions::fthrow(\n+              THREAD_AND_LOCATION,\n+              vmSymbols::java_lang_ClassFormatError(),\n+              \"Illegal use of @jdk.internal.vm.annotation.NullRestricted annotation on field %s with signature %s (primitive types can never be null)\",\n+              name->as_C_string(), sig->as_C_string());\n+          }\n+          is_null_restricted = true;\n+        }\n@@ -1456,0 +1485,4 @@\n+    if (is_null_restricted) {\n+      fieldFlags.update_null_free_inline_type(true);\n+    }\n+\n@@ -1510,0 +1543,38 @@\n+  if (is_inline_type) {\n+    \/\/ Inject static \".default\" field\n+    FieldInfo::FieldFlags fflags(0);\n+    fflags.update_injected(true);\n+    AccessFlags aflags(JVM_ACC_STATIC);\n+    FieldInfo fi(aflags,\n+                 (u2)vmSymbols::as_int(VM_SYMBOL_ENUM_NAME(default_value_name)),\n+                 (u2)vmSymbols::as_int(VM_SYMBOL_ENUM_NAME(object_signature)),\n+                 0,\n+                 fflags);\n+    fi.set_index(index);\n+    _temp_field_info->append(fi);\n+    _static_oop_count++;\n+    index++;\n+  }\n+\n+  if (is_inline_type && instance_fields_count == 0) {\n+    \/\/ Inject \".empty\" dummy field\n+    _is_empty_inline_type = true;\n+\n+    FieldInfo::FieldFlags fflags(0);\n+    fflags.update_injected(true);\n+    AccessFlags aflags;\n+    FieldInfo fi(aflags,\n+                 (u2)vmSymbols::as_int(VM_SYMBOL_ENUM_NAME(empty_marker_name)),\n+                 (u2)vmSymbols::as_int(VM_SYMBOL_ENUM_NAME(byte_signature)),\n+                 0,\n+                 fflags);\n+    fi.set_index(index);\n+    _temp_field_info->append(fi);\n+\n+    index++;\n+  }\n+\n+  if (instance_fields_count > 0) {\n+    _has_nonstatic_fields = true;\n+  }\n+\n@@ -1884,0 +1955,12 @@\n+    case VM_SYMBOL_ENUM_NAME(jdk_internal_vm_annotation_ImplicitlyConstructible_signature): {\n+      if (_location != _in_class)   break; \/\/ only allow for classes\n+      return _jdk_internal_ImplicitlyConstructible;\n+    }\n+    case VM_SYMBOL_ENUM_NAME(jdk_internal_vm_annotation_LooselyConsistentValue_signature): {\n+      if (_location != _in_class)   break; \/\/ only allow for classes\n+      return _jdk_internal_LooselyConsistentValue;\n+    }\n+    case VM_SYMBOL_ENUM_NAME(jdk_internal_vm_annotation_NullRestricted_signature): {\n+      if (_location != _in_field)   break; \/\/ only allow for fields\n+      return _jdk_internal_NullRestricted;\n+    }\n@@ -2105,0 +2188,2 @@\n+                                      bool is_value_class,\n+                                      bool is_abstract_class,\n@@ -2146,1 +2231,1 @@\n-    verify_legal_method_modifiers(flags, is_interface, name, CHECK_NULL);\n+    verify_legal_method_modifiers(flags, access_flags() , name, CHECK_NULL);\n@@ -2154,0 +2239,9 @@\n+  if (EnableValhalla) {\n+    if (((flags & JVM_ACC_SYNCHRONIZED) == JVM_ACC_SYNCHRONIZED)\n+        && ((flags & JVM_ACC_STATIC) == 0 )\n+        && !_access_flags.is_identity_class()) {\n+      classfile_parse_error(\"Invalid synchronized method in non-identity class %s\", THREAD);\n+        return nullptr;\n+    }\n+  }\n+\n@@ -2681,0 +2775,2 @@\n+                                    bool is_value_class,\n+                                    bool is_abstract_type,\n@@ -2705,0 +2801,2 @@\n+                                    is_value_class,\n+                                    is_abstract_type,\n@@ -2970,2 +3068,2 @@\n-    \/\/ Access flags\n-    jint flags;\n+\n+    jint recognized_modifiers = RECOGNIZED_INNER_CLASS_MODIFIERS;\n@@ -2974,3 +3072,1 @@\n-      flags = cfs->get_u2_fast() & (RECOGNIZED_INNER_CLASS_MODIFIERS | JVM_ACC_MODULE);\n-    } else {\n-      flags = cfs->get_u2_fast() & RECOGNIZED_INNER_CLASS_MODIFIERS;\n+      recognized_modifiers |= JVM_ACC_MODULE;\n@@ -2978,0 +3074,4 @@\n+\n+    \/\/ Access flags\n+    jint flags = cfs->get_u2_fast() & recognized_modifiers;\n+\n@@ -2982,1 +3082,11 @@\n-    verify_legal_class_modifiers(flags, CHECK_0);\n+\n+    if (!supports_inline_types()) {\n+      const bool is_module = (flags & JVM_ACC_MODULE) != 0;\n+      const bool is_interface = (flags & JVM_ACC_INTERFACE) != 0;\n+      if (!is_module && !is_interface) {\n+        flags |= JVM_ACC_IDENTITY;\n+      }\n+    }\n+\n+    const char* name = inner_name_index == 0 ? \"unnamed\" : cp->symbol_at(inner_name_index)->as_utf8();\n+    verify_legal_class_modifiers(flags, name, false, CHECK_0);\n@@ -3086,0 +3196,43 @@\n+u2 ClassFileParser::parse_classfile_loadable_descriptors_attribute(const ClassFileStream* const cfs,\n+                                                                   const u1* const loadable_descriptors_attribute_start,\n+                                                                   TRAPS) {\n+  const u1* const current_mark = cfs->current();\n+  u2 length = 0;\n+  if (loadable_descriptors_attribute_start != nullptr) {\n+    cfs->set_current(loadable_descriptors_attribute_start);\n+    cfs->guarantee_more(2, CHECK_0);  \/\/ length\n+    length = cfs->get_u2_fast();\n+  }\n+  const int size = length;\n+  Array<u2>* const loadable_descriptors = MetadataFactory::new_array<u2>(_loader_data, size, CHECK_0);\n+  _loadable_descriptors = loadable_descriptors;\n+  if (length > 0) {\n+    int index = 0;\n+    cfs->guarantee_more(2 * length, CHECK_0);\n+    for (int n = 0; n < length; n++) {\n+      const u2 descriptor_index = cfs->get_u2_fast();\n+      check_property(\n+        valid_symbol_at(descriptor_index),\n+        \"LoadableDescriptors descriptor_index %u has bad constant type in class file %s\",\n+        descriptor_index, CHECK_0);\n+      Symbol* descriptor = _cp->symbol_at(descriptor_index);\n+      bool valid = legal_field_signature(descriptor, CHECK_0);\n+      if(!valid) {\n+        ResourceMark rm(THREAD);\n+        Exceptions::fthrow(THREAD_AND_LOCATION,\n+          vmSymbols::java_lang_ClassFormatError(),\n+          \"Descriptor from LoadableDescriptors attribute at index \\\"%d\\\" in class %s has illegal signature \\\"%s\\\"\",\n+          descriptor_index, _class_name->as_C_string(), descriptor->as_C_string());\n+        return 0;\n+      }\n+      loadable_descriptors->at_put(index++, descriptor_index);\n+    }\n+    assert(index == size, \"wrong size\");\n+  }\n+\n+  \/\/ Restore buffer's current position.\n+  cfs->set_current(current_mark);\n+\n+  return length;\n+}\n+\n@@ -3351,0 +3504,2 @@\n+  \/\/ Set _loadable_descriptors attribute to default sentinel\n+  _loadable_descriptors = Universe::the_empty_short_array();\n@@ -3357,0 +3512,1 @@\n+  bool parsed_loadable_descriptors_attribute = false;\n@@ -3378,0 +3534,2 @@\n+  const u1* loadable_descriptors_attribute_start = nullptr;\n+  u4  loadable_descriptors_attribute_length = 0;\n@@ -3593,0 +3751,9 @@\n+            if (EnableValhalla && tag == vmSymbols::tag_loadable_descriptors()) {\n+              if (parsed_loadable_descriptors_attribute) {\n+                classfile_parse_error(\"Multiple LoadableDescriptors attributes in class file %s\", CHECK);\n+                return;\n+              }\n+              parsed_loadable_descriptors_attribute = true;\n+              loadable_descriptors_attribute_start = cfs->current();\n+              loadable_descriptors_attribute_length = attribute_length;\n+            }\n@@ -3669,0 +3836,12 @@\n+  if (parsed_loadable_descriptors_attribute) {\n+    const u2 num_classes = parse_classfile_loadable_descriptors_attribute(\n+                            cfs,\n+                            loadable_descriptors_attribute_start,\n+                            CHECK);\n+    if (_need_verify) {\n+      guarantee_property(\n+        loadable_descriptors_attribute_length == sizeof(num_classes) + sizeof(u2) * num_classes,\n+        \"Wrong LoadableDescriptors attribute length in class file %s\", CHECK);\n+    }\n+  }\n+\n@@ -3734,0 +3913,1 @@\n+  this_klass->set_loadable_descriptors(_loadable_descriptors);\n@@ -3737,0 +3917,2 @@\n+  this_klass->set_inline_type_field_klasses_array(_inline_type_field_klasses);\n+  this_klass->set_null_marker_offsets_array(_null_marker_offsets);\n@@ -3773,2 +3955,1 @@\n-                   \"Invalid superclass index %u in class file %s\",\n-                   super_class_index,\n+                   \"Invalid superclass index 0 in class file %s\",\n@@ -3783,1 +3964,0 @@\n-    bool is_array = false;\n@@ -3786,4 +3966,0 @@\n-      if (need_verify)\n-        is_array = super_klass->is_array_klass();\n-    } else if (need_verify) {\n-      is_array = (cp->klass_name_at(super_class_index)->char_at(0) == JVM_SIGNATURE_ARRAY);\n@@ -3792,0 +3968,1 @@\n+      bool is_array = (cp->klass_name_at(super_class_index)->char_at(0) == JVM_SIGNATURE_ARRAY);\n@@ -3965,0 +4142,6 @@\n+bool ClassFileParser::supports_inline_types() const {\n+  \/\/ Inline types are only supported by class file version 68.65535 and later\n+  return _major_version > JAVA_24_VERSION ||\n+         (_major_version == JAVA_24_VERSION && _minor_version == JAVA_PREVIEW_MINOR_VERSION);\n+}\n+\n@@ -4008,3 +4191,4 @@\n-  } else if (max_transitive_size == local_size) {\n-    \/\/ only local interfaces added, share local interface array\n-    return local_ifs;\n+    \/\/ The three lines below are commented to work around bug JDK-8245487\n+\/\/  } else if (max_transitive_size == local_size) {\n+\/\/    \/\/ only local interfaces added, share local interface array\n+\/\/    return local_ifs;\n@@ -4031,0 +4215,1 @@\n+\n@@ -4059,0 +4244,10 @@\n+    \/\/ The JVMS says that super classes for value types must not have the ACC_IDENTITY\n+    \/\/ flag set. But, java.lang.Object must still be allowed to be a direct super class\n+    \/\/ for a value classes.  So, it is treated as a special case for now.\n+    if (!this_klass->access_flags().is_identity_class() &&\n+        super_ik->name() != vmSymbols::java_lang_Object() &&\n+        super_ik->is_identity_class()) {\n+      classfile_icce_error(\"value class %s cannot inherit from class %s\", super_ik, THREAD);\n+      return;\n+    }\n+\n@@ -4244,1 +4439,1 @@\n-void ClassFileParser::verify_legal_class_modifiers(jint flags, TRAPS) const {\n+void ClassFileParser::verify_legal_class_modifiers(jint flags, const char* name, bool is_Object, TRAPS) const {\n@@ -4246,0 +4441,1 @@\n+  const bool is_inner_class = name != nullptr;\n@@ -4262,1 +4458,1 @@\n-  const bool is_super      = (flags & JVM_ACC_SUPER)      != 0;\n+  const bool is_identity   = (flags & JVM_ACC_IDENTITY)   != 0;\n@@ -4266,0 +4462,2 @@\n+  const bool valid_value_class = is_identity || is_interface ||\n+                                 (supports_inline_types() && (!is_identity && (is_abstract || is_final)));\n@@ -4269,2 +4467,3 @@\n-      (is_interface && major_gte_1_5 && (is_super || is_enum)) ||\n-      (!is_interface && major_gte_1_5 && is_annotation)) {\n+      (is_interface && major_gte_1_5 && (is_identity || is_enum)) ||   \/\/  ACC_SUPER (now ACC_IDENTITY) was illegal for interfaces\n+      (!is_interface && major_gte_1_5 && is_annotation) ||\n+      (!valid_value_class)) {\n@@ -4272,7 +4471,21 @@\n-    Exceptions::fthrow(\n-      THREAD_AND_LOCATION,\n-      vmSymbols::java_lang_ClassFormatError(),\n-      \"Illegal class modifiers in class %s: 0x%X\",\n-      _class_name->as_C_string(), flags\n-    );\n-    return;\n+    const char* class_note = \"\";\n+    if (!valid_value_class) {\n+      class_note = \" (a value class must be final or else abstract)\";\n+    }\n+    if (name == nullptr) { \/\/ Not an inner class\n+      Exceptions::fthrow(\n+        THREAD_AND_LOCATION,\n+        vmSymbols::java_lang_ClassFormatError(),\n+        \"Illegal class modifiers in class %s%s: 0x%X\",\n+        _class_name->as_C_string(), class_note, flags\n+      );\n+      return;\n+    } else {\n+      Exceptions::fthrow(\n+        THREAD_AND_LOCATION,\n+        vmSymbols::java_lang_ClassFormatError(),\n+        \"Illegal class modifiers in declaration of inner class %s%s of class %s: 0x%X\",\n+        name, class_note, _class_name->as_C_string(), flags\n+      );\n+      return;\n+    }\n@@ -4344,2 +4557,2 @@\n-void ClassFileParser::verify_legal_field_modifiers(jint flags,\n-                                                   bool is_interface,\n+void ClassFileParser:: verify_legal_field_modifiers(jint flags,\n+                                                   AccessFlags class_access_flags,\n@@ -4357,0 +4570,1 @@\n+  const bool is_strict    = (flags & JVM_ACC_STRICT)    != 0;\n@@ -4359,0 +4573,3 @@\n+  const bool is_interface = class_access_flags.is_interface();\n+  const bool is_identity_class = class_access_flags.is_identity_class();\n+\n@@ -4360,0 +4577,1 @@\n+  const char* error_msg = \"\";\n@@ -4361,4 +4579,8 @@\n-  if (is_interface) {\n-    if (!is_public || !is_static || !is_final || is_private ||\n-        is_protected || is_volatile || is_transient ||\n-        (major_gte_1_5 && is_enum)) {\n+  \/\/ There is some overlap in the checks that apply, for example interface fields\n+  \/\/ must be static, static fields can't be strict, and therefore interfaces can't\n+  \/\/ have strict fields. So we don't have to check every possible invalid combination\n+  \/\/ individually as long as all are covered. Once we have found an illegal combination\n+  \/\/ we can stop checking.\n+\n+  if (supports_inline_types()) {\n+    if (is_strict && is_static) {\n@@ -4366,0 +4588,1 @@\n+      error_msg = \"field cannot be strict and static\";\n@@ -4367,2 +4590,1 @@\n-  } else { \/\/ not interface\n-    if (has_illegal_visibility(flags) || (is_final && is_volatile)) {\n+    else if (is_strict && !is_final) {\n@@ -4370,0 +4592,25 @@\n+      error_msg = \"strict field must be final\";\n+    }\n+  }\n+\n+  if (!is_illegal) {\n+    if (is_interface) {\n+      if (!is_public || !is_static || !is_final || is_private ||\n+          is_protected || is_volatile || is_transient ||\n+          (major_gte_1_5 && is_enum)) {\n+        is_illegal = true;\n+        error_msg = \"interface fields must be public, static and final, and may be synthetic\";\n+      }\n+    } else { \/\/ not interface\n+      if (has_illegal_visibility(flags)) {\n+        is_illegal = true;\n+        error_msg = \"invalid visibility flags for class field\";\n+      } else if (is_final && is_volatile) {\n+        is_illegal = true;\n+        error_msg = \"fields cannot be final and volatile\";\n+      } else if (supports_inline_types()) {\n+        if (!is_identity_class && !is_static && !is_strict) {\n+          is_illegal = true;\n+          error_msg = \"value class fields must be either strict or static\";\n+        }\n+      }\n@@ -4378,2 +4625,2 @@\n-      \"Illegal field modifiers in class %s: 0x%X\",\n-      _class_name->as_C_string(), flags);\n+      \"Illegal field modifiers (%s) in class %s: 0x%X\",\n+      error_msg, _class_name->as_C_string(), flags);\n@@ -4385,1 +4632,1 @@\n-                                                    bool is_interface,\n+                                                    AccessFlags class_access_flags,\n@@ -4404,0 +4651,4 @@\n+  \/\/ LW401 CR required: removal of value factories support\n+  const bool is_interface    = class_access_flags.is_interface();\n+  const bool is_identity_class = class_access_flags.is_identity_class();\n+  const bool is_abstract_class = class_access_flags.is_abstract();\n@@ -4407,0 +4658,1 @@\n+  const char* class_note = \"\";\n@@ -4446,4 +4698,9 @@\n-        if (is_abstract) {\n-          if ((is_final || is_native || is_private || is_static ||\n-              (major_gte_1_5 && (is_synchronized || (!major_gte_17 && is_strict))))) {\n-            is_illegal = true;\n+        if (!is_identity_class && is_synchronized && !is_static) {\n+          is_illegal = true;\n+          class_note = \" (not an identity class)\";\n+        } else {\n+          if (is_abstract) {\n+            if ((is_final || is_native || is_private || is_static ||\n+                (major_gte_1_5 && (is_synchronized || (!major_gte_17 && is_strict))))) {\n+              is_illegal = true;\n+            }\n@@ -4461,2 +4718,3 @@\n-      \"Method %s in class %s has illegal modifiers: 0x%X\",\n-      name->as_C_string(), _class_name->as_C_string(), flags);\n+      \"Method %s in class %s%s has illegal modifiers: 0x%X\",\n+      name->as_C_string(), _class_name->as_C_string(),\n+      class_note, flags);\n@@ -4520,0 +4778,9 @@\n+bool ClassFileParser::is_class_in_loadable_descriptors_attribute(Symbol *klass) {\n+  if (_loadable_descriptors == nullptr) return false;\n+  for (int i = 0; i < _loadable_descriptors->length(); i++) {\n+        Symbol* class_name = _cp->symbol_at(_loadable_descriptors->at(i));\n+        if (class_name == klass) return true;\n+  }\n+  return false;\n+}\n+\n@@ -4621,1 +4888,2 @@\n-    case JVM_SIGNATURE_CLASS: {\n+    case JVM_SIGNATURE_CLASS:\n+    {\n@@ -4632,1 +4900,1 @@\n-        \/\/ Skip leading 'L' and ignore first appearance of ';'\n+        \/\/ Skip leading 'L' or 'Q' and ignore first appearance of ';'\n@@ -4688,0 +4956,4 @@\n+    } else if ((_major_version >= CONSTANT_CLASS_DESCRIPTORS || _class_name->starts_with(\"jdk\/internal\/reflect\/\"))\n+                   && bytes[length - 1] == ';' ) {\n+      \/\/ Support for L...; descriptors\n+      legal = verify_unqualified_name(bytes + 1, length - 2, LegalClass);\n@@ -4753,1 +5025,2 @@\n-      if (name == vmSymbols::object_initializer_name() || name == vmSymbols::class_initializer_name()) {\n+      if (name == vmSymbols::object_initializer_name() ||\n+          name == vmSymbols::class_initializer_name()) {\n@@ -4779,0 +5052,10 @@\n+bool ClassFileParser::legal_field_signature(const Symbol* signature, TRAPS) const {\n+  const char* const bytes = (const char*)signature->bytes();\n+  const unsigned int length = signature->utf8_length();\n+  const char* const p = skip_over_field_signature(bytes, false, length, CHECK_false);\n+\n+  if (p == nullptr || (p - bytes) != (int)length) {\n+    return false;\n+  }\n+  return true;\n+}\n@@ -4814,3 +5097,3 @@\n-      name->char_at(0) == JVM_SIGNATURE_SPECIAL &&\n-      sig_length > 0 &&\n-      signature->char_at(sig_length - 1) != JVM_SIGNATURE_VOID) {\n+    name->char_at(0) == JVM_SIGNATURE_SPECIAL &&\n+    sig_length > 0 &&\n+    signature->char_at(sig_length - 1) != JVM_SIGNATURE_VOID) {\n@@ -4993,1 +5276,0 @@\n-\n@@ -5029,0 +5311,4 @@\n+  if (_field_info->_is_naturally_atomic && ik->is_inline_klass()) {\n+    ik->set_is_naturally_atomic();\n+  }\n+\n@@ -5034,0 +5320,3 @@\n+  if (ik->is_inline_klass()) {\n+    InlineKlass::cast(ik)->init_fixed_block();\n+  }\n@@ -5047,0 +5336,1 @@\n+  assert(nullptr == _loadable_descriptors, \"invariant\");\n@@ -5050,0 +5340,1 @@\n+  assert(nullptr == _inline_type_field_klasses, \"invariant\");\n@@ -5085,0 +5376,6 @@\n+  if (_must_be_atomic) {\n+    ik->set_must_be_atomic();\n+  }\n+  if (_is_implicitly_constructible) {\n+    ik->set_is_implicitly_constructible();\n+  }\n@@ -5189,0 +5486,28 @@\n+  bool all_fields_empty = true;\n+  for (AllFieldStream fs(ik); !fs.done(); fs.next()) {\n+    if (!fs.access_flags().is_static()) {\n+      if (fs.field_descriptor().is_null_free_inline_type()) {\n+        Klass* k = ik->inline_type_field_klasses_array()->at(fs.index());\n+        assert(k->is_inline_klass(), \"must be\");\n+        if (!InlineKlass::cast(k)->is_empty_inline_type()) { all_fields_empty = false; }\n+      } else {\n+        all_fields_empty = false;\n+      }\n+    } else if (is_inline_type() && (fs.name() == vmSymbols::default_value_name())) {\n+      InlineKlass::cast(ik)->set_default_value_offset(ik->field_offset(fs.index()));\n+    }\n+  }\n+\n+  if (_is_empty_inline_type || (is_inline_type() && all_fields_empty)) {\n+    ik->set_is_empty_inline_type();\n+  }\n+\n+  if (is_inline_type()) {\n+    InlineKlass* vk = InlineKlass::cast(ik);\n+    vk->set_alignment(_alignment);\n+    vk->set_first_field_offset(_first_field_offset);\n+    vk->set_payload_size_in_bytes(_payload_size_in_bytes);\n+    vk->set_internal_null_marker_offset(_internal_null_marker_offset);\n+    InlineKlass::cast(ik)->initialize_calling_convention(CHECK);\n+  }\n+\n@@ -5281,0 +5606,1 @@\n+  _loadable_descriptors(nullptr),\n@@ -5283,0 +5609,1 @@\n+  _local_interface_indexes(nullptr),\n@@ -5292,0 +5619,2 @@\n+  _inline_type_field_klasses(nullptr),\n+  _null_marker_offsets(nullptr),\n@@ -5321,0 +5650,8 @@\n+  _has_inline_type_fields(false),\n+  _has_nonstatic_fields(false),\n+  _is_empty_inline_type(false),\n+  _is_naturally_atomic(false),\n+  _must_be_atomic(true),\n+  _is_implicitly_constructible(false),\n+  _has_loosely_consistent_annotation(false),\n+  _has_implicitly_constructible_annotation(false),\n@@ -5370,0 +5707,1 @@\n+  _loadable_descriptors = nullptr;\n@@ -5374,0 +5712,2 @@\n+  _inline_type_field_klasses = nullptr;\n+  _null_marker_offsets = nullptr;\n@@ -5392,0 +5732,8 @@\n+  if (_inline_type_field_klasses != nullptr) {\n+     MetadataFactory::free_array<InlineKlass*>(_loader_data, _inline_type_field_klasses);\n+  }\n+\n+  if (_null_marker_offsets != nullptr) {\n+     MetadataFactory::free_array<int>(_loader_data, _null_marker_offsets);\n+  }\n+\n@@ -5414,0 +5762,4 @@\n+  if (_loadable_descriptors != nullptr && _loadable_descriptors != Universe::the_empty_short_array()) {\n+    MetadataFactory::free_array<u2>(_loader_data, _loadable_descriptors);\n+  }\n+\n@@ -5498,2 +5850,1 @@\n-  \/\/ Access flags\n-  jint flags;\n+  jint recognized_modifiers = JVM_RECOGNIZED_CLASS_MODIFIERS;\n@@ -5502,3 +5853,1 @@\n-    flags = stream->get_u2_fast() & (JVM_RECOGNIZED_CLASS_MODIFIERS | JVM_ACC_MODULE);\n-  } else {\n-    flags = stream->get_u2_fast() & JVM_RECOGNIZED_CLASS_MODIFIERS;\n+    recognized_modifiers |= JVM_ACC_MODULE;\n@@ -5507,0 +5856,3 @@\n+  \/\/ Access flags\n+  jint flags = stream->get_u2_fast() & recognized_modifiers;\n+\n@@ -5512,8 +5864,7 @@\n-  verify_legal_class_modifiers(flags, CHECK);\n-\n-  short bad_constant = class_bad_constant_seen();\n-  if (bad_constant != 0) {\n-    \/\/ Do not throw CFE until after the access_flags are checked because if\n-    \/\/ ACC_MODULE is set in the access flags, then NCDFE must be thrown, not CFE.\n-    classfile_parse_error(\"Unknown constant tag %u in class file %s\", bad_constant, THREAD);\n-    return;\n+  \/\/ Fixing ACC_SUPER\/ACC_IDENTITY for old class files\n+  if (!supports_inline_types()) {\n+    const bool is_module = (flags & JVM_ACC_MODULE) != 0;\n+    const bool is_interface = (flags & JVM_ACC_INTERFACE) != 0;\n+    if (!is_module && !is_interface) {\n+      flags |= JVM_ACC_IDENTITY;\n+    }\n@@ -5522,1 +5873,0 @@\n-  _access_flags.set_flags(flags);\n@@ -5535,0 +5885,14 @@\n+  bool is_java_lang_Object = class_name_in_cp == vmSymbols::java_lang_Object();\n+\n+  verify_legal_class_modifiers(flags, nullptr, is_java_lang_Object, CHECK);\n+\n+  _access_flags.set_flags(flags);\n+\n+  short bad_constant = class_bad_constant_seen();\n+  if (bad_constant != 0) {\n+    \/\/ Do not throw CFE until after the access_flags are checked because if\n+    \/\/ ACC_MODULE is set in the access flags, then NCDFE must be thrown, not CFE.\n+    classfile_parse_error(\"Unknown constant tag %u in class file %s\", bad_constant, THREAD);\n+    return;\n+  }\n+\n@@ -5614,3 +5978,1 @@\n-  assert(_local_interfaces != nullptr, \"invariant\");\n-\n-               _access_flags.is_interface(),\n+               _access_flags,\n@@ -5628,1 +5990,3 @@\n-                _access_flags.is_interface(),\n+                is_interface(),\n+                !is_identity_class(),\n+                is_abstract_class(),\n@@ -5708,2 +6072,2 @@\n-                   \"java.lang.Object cannot implement an interface in class file %s\",\n-                   CHECK);\n+        \"java.lang.Object cannot implement an interface in class file %s\",\n+        CHECK);\n@@ -5714,1 +6078,1 @@\n-    if (_access_flags.is_interface()) {\n+    if (is_interface()) {\n@@ -5726,1 +6090,1 @@\n-                       SystemDictionary::resolve_super_or_fail(_class_name,\n+                       SystemDictionary::resolve_with_circularity_detection_or_fail(_class_name,\n@@ -5736,0 +6100,14 @@\n+    if (_super_klass->is_interface()) {\n+      classfile_icce_error(\"class %s has interface %s as super class\", _super_klass, THREAD);\n+      return;\n+    }\n+\n+    if (_super_klass->is_final()) {\n+      classfile_icce_error(\"class %s cannot inherit from final class %s\", _super_klass, THREAD);\n+      return;\n+    }\n+\n+    if (EnableValhalla) {\n+      check_identity_and_value_modifiers(this, _super_klass, CHECK);\n+    }\n+\n@@ -5739,0 +6117,1 @@\n+  }\n@@ -5740,3 +6119,29 @@\n-    if (_super_klass->is_interface()) {\n-      classfile_icce_error(\"class %s has interface %s as super class\", _super_klass, THREAD);\n-      return;\n+  if (_parsed_annotations->has_annotation(AnnotationCollector::_jdk_internal_LooselyConsistentValue) && _access_flags.is_identity_class()) {\n+    THROW_MSG(vmSymbols::java_lang_ClassFormatError(),\n+          err_msg(\"class %s cannot have annotation jdk.internal.vm.annotation.LooselyConsistentValue, because it is not a value class\",\n+                  _class_name->as_klass_external_name()));\n+  }\n+  if (_parsed_annotations->has_annotation(AnnotationCollector::_jdk_internal_ImplicitlyConstructible) && _access_flags.is_identity_class()) {\n+    THROW_MSG(vmSymbols::java_lang_ClassFormatError(),\n+          err_msg(\"class %s cannot have annotation jdk.internal.vm.annotation.ImplicitlyConstructible, because it is not a value class\",\n+                  _class_name->as_klass_external_name()));\n+  }\n+\n+  \/\/ Determining is the class allows tearing or not (default is not)\n+  \/\/ Test might need extensions when field inheritance is added for value classes\n+  if (EnableValhalla && !_access_flags.is_identity_class()) {\n+    if (_parsed_annotations->has_annotation(ClassAnnotationCollector::_jdk_internal_LooselyConsistentValue)\n+        && (_super_klass == vmClasses::Object_klass() || !_super_klass->must_be_atomic())) {\n+      _must_be_atomic = false;\n+    }\n+    if (_parsed_annotations->has_annotation(ClassAnnotationCollector::_jdk_internal_ImplicitlyConstructible)\n+        && (_super_klass == vmClasses::Object_klass() || _super_klass->is_implicitly_constructible())) {\n+      _is_implicitly_constructible = true;\n+    }\n+    \/\/ Apply VM options override\n+    if (*ForceNonTearable != '\\0') {\n+      \/\/ Allow a command line switch to force the same atomicity property:\n+      const char* class_name_str = _class_name->as_C_string();\n+      if (StringUtils::class_list_match(ForceNonTearable, class_name_str)) {\n+        _must_be_atomic = true;\n+      }\n@@ -5746,0 +6151,47 @@\n+  int itfs_len = _local_interface_indexes == nullptr ? 0 : _local_interface_indexes->length();\n+  _local_interfaces = MetadataFactory::new_array<InstanceKlass*>(_loader_data, itfs_len, nullptr, CHECK);\n+  if (_local_interface_indexes != nullptr) {\n+    for (int i = 0; i < _local_interface_indexes->length(); i++) {\n+      u2 interface_index = _local_interface_indexes->at(i);\n+      Klass* interf;\n+      if (cp->tag_at(interface_index).is_klass()) {\n+        interf = cp->resolved_klass_at(interface_index);\n+      } else {\n+        Symbol* const unresolved_klass  = cp->klass_name_at(interface_index);\n+\n+        \/\/ Don't need to check legal name because it's checked when parsing constant pool.\n+        \/\/ But need to make sure it's not an array type.\n+        guarantee_property(unresolved_klass->char_at(0) != JVM_SIGNATURE_ARRAY,\n+                            \"Bad interface name in class file %s\", CHECK);\n+\n+        \/\/ Call resolve on the interface class name with class circularity checking\n+        interf = SystemDictionary::resolve_with_circularity_detection_or_fail(\n+                                                  _class_name,\n+                                                  unresolved_klass,\n+                                                  Handle(THREAD, _loader_data->class_loader()),\n+                                                  _protection_domain,\n+                                                  false,\n+                                                  CHECK);\n+      }\n+\n+      if (!interf->is_interface()) {\n+        THROW_MSG(vmSymbols::java_lang_IncompatibleClassChangeError(),\n+                  err_msg(\"class %s can not implement %s, because it is not an interface (%s)\",\n+                          _class_name->as_klass_external_name(),\n+                          interf->external_name(),\n+                          interf->class_in_module_of_loader()));\n+      }\n+\n+      if (EnableValhalla) {\n+        \/\/ Check modifiers and set carries_identity_modifier\/carries_value_modifier flags\n+        check_identity_and_value_modifiers(this, InstanceKlass::cast(interf), CHECK);\n+      }\n+\n+      if (InstanceKlass::cast(interf)->has_nonstatic_concrete_methods()) {\n+        _has_nonstatic_concrete_methods = true;\n+      }\n+      _local_interfaces->at_put(i, InstanceKlass::cast(interf));\n+    }\n+  }\n+  assert(_local_interfaces != nullptr, \"invariant\");\n+\n@@ -5773,1 +6225,1 @@\n-  _itable_size = _access_flags.is_interface() ? 0 :\n+  _itable_size = is_interface() ? 0 :\n@@ -5778,0 +6230,76 @@\n+  if (EnableValhalla) {\n+    _inline_type_field_klasses = MetadataFactory::new_array<InlineKlass*>(_loader_data,\n+                                                   java_fields_count(),\n+                                                   nullptr,\n+                                                   CHECK);\n+    for (GrowableArrayIterator<FieldInfo> it = _temp_field_info->begin(); it != _temp_field_info->end(); ++it) {\n+      FieldInfo fieldinfo = *it;\n+      if (fieldinfo.access_flags().is_static()) continue;  \/\/ Only non-static fields are processed at load time\n+      Symbol* sig = fieldinfo.signature(cp);\n+      if (fieldinfo.field_flags().is_null_free_inline_type()) {\n+        \/\/ Pre-load classes of null-free fields that are candidate for flattening\n+        TempNewSymbol s = Signature::strip_envelope(sig);\n+        if (s == _class_name) {\n+          THROW_MSG(vmSymbols::java_lang_ClassCircularityError(), err_msg(\"Class %s cannot have a null-free non-static field of its own type\", _class_name->as_C_string()));\n+        }\n+        log_info(class, preload)(\"Preloading class %s during loading of class %s. Cause: a null-free non-static field is declared with this type\", s->as_C_string(), _class_name->as_C_string());\n+        Klass* klass = SystemDictionary::resolve_with_circularity_detection_or_fail(_class_name, s, Handle(THREAD, _loader_data->class_loader()), _protection_domain, false, THREAD);\n+        if (HAS_PENDING_EXCEPTION) {\n+          log_warning(class, preload)(\"Preloading of class %s during loading of class %s (cause: null-free non-static field) failed: %s\",\n+                                      s->as_C_string(), _class_name->as_C_string(), PENDING_EXCEPTION->klass()->name()->as_C_string());\n+          return; \/\/ Exception is still pending\n+        }\n+        assert(klass != nullptr, \"Sanity check\");\n+        if (klass->access_flags().is_identity_class()) {\n+          assert(klass->is_instance_klass(), \"Sanity check\");\n+          ResourceMark rm(THREAD);\n+          THROW_MSG(vmSymbols::java_lang_IncompatibleClassChangeError(),\n+                    err_msg(\"Class %s expects class %s to be a value class, but it is an identity class\",\n+                    _class_name->as_C_string(),\n+                    InstanceKlass::cast(klass)->external_name()));\n+        }\n+        if (klass->is_abstract()) {\n+          assert(klass->is_instance_klass(), \"Sanity check\");\n+          ResourceMark rm(THREAD);\n+          THROW_MSG(vmSymbols::java_lang_IncompatibleClassChangeError(),\n+                    err_msg(\"Class %s expects class %s to be concrete value type, but it is an abstract class\",\n+                    _class_name->as_C_string(),\n+                    InstanceKlass::cast(klass)->external_name()));\n+        }\n+        InlineKlass* vk = InlineKlass::cast(klass);\n+        if (!vk->is_implicitly_constructible()) {\n+          THROW_MSG(vmSymbols::java_lang_IncompatibleClassChangeError(),\n+                    err_msg(\"class %s is not implicitly constructible and it is used in a null restricted non-static field (not supported)\",\n+                    klass->name()->as_C_string()));\n+        }\n+        _inline_type_field_klasses->at_put(fieldinfo.index(), vk);\n+        log_info(class, preload)(\"Preloading of class %s during loading of class %s (cause: null-free non-static field) succeeded\", s->as_C_string(), _class_name->as_C_string());\n+      } else if (Signature::has_envelope(sig)) {\n+        \/\/ Preloading classes for nullable fields that are listed in the LoadableDescriptors attribute\n+        \/\/ Those classes would be required later for the flattening of nullable inline type fields\n+        TempNewSymbol name = Signature::strip_envelope(sig);\n+        if (name != _class_name && is_class_in_loadable_descriptors_attribute(sig)) {\n+          log_info(class, preload)(\"Preloading class %s during loading of class %s. Cause: field type in LoadableDescriptors attribute\", name->as_C_string(), _class_name->as_C_string());\n+          oop loader = loader_data()->class_loader();\n+          Klass* klass = SystemDictionary::resolve_with_circularity_detection_or_fail(_class_name, name, Handle(THREAD, loader), _protection_domain, false, THREAD);\n+          if (klass != nullptr) {\n+            if (klass->is_inline_klass()) {\n+              _inline_type_field_klasses->at_put(fieldinfo.index(), InlineKlass::cast(klass));\n+              log_info(class, preload)(\"Preloading of class %s during loading of class %s (cause: field type in LoadableDescriptors attribute) succeeded\", name->as_C_string(), _class_name->as_C_string());\n+            } else {\n+              \/\/ Non value class are allowed by the current spec, but it could be an indication of an issue so let's log a warning\n+              log_warning(class, preload)(\"Preloading class %s during loading of class %s (cause: field type in LoadableDescriptors attribute) but loaded class is not a value class\", name->as_C_string(), _class_name->as_C_string());\n+            }\n+            } else {\n+            log_warning(class, preload)(\"Preloading of class %s during loading of class %s (cause: field type in LoadableDescriptors attribute) failed : %s\",\n+                                          name->as_C_string(), _class_name->as_C_string(), PENDING_EXCEPTION->klass()->name()->as_C_string());\n+          }\n+          \/\/ Loads triggered by the LoadableDescriptors attribute are speculative, failures must not impact loading of current class\n+          if (HAS_PENDING_EXCEPTION) {\n+            CLEAR_PENDING_EXCEPTION;\n+          }\n+        }\n+      }\n+    }\n+  }\n+\n@@ -5779,2 +6307,4 @@\n-  FieldLayoutBuilder lb(class_name(), super_klass(), _cp, \/*_fields*\/ _temp_field_info,\n-                        _parsed_annotations->is_contended(), _field_info);\n+  FieldLayoutBuilder lb(class_name(), loader_data(), super_klass(), _cp, \/*_fields*\/ _temp_field_info,\n+      _parsed_annotations->is_contended(), is_inline_type(),\n+      access_flags().is_abstract() && !access_flags().is_identity_class() && !access_flags().is_interface(),\n+      _field_info, _inline_type_field_klasses);\n@@ -5782,0 +6312,7 @@\n+  if (is_inline_type()) {\n+    _alignment = lb.get_alignment();\n+    _first_field_offset = lb.get_first_field_offset();\n+    _payload_size_in_bytes = lb.get_payload_size_in_byte();\n+    _internal_null_marker_offset = lb.get_internal_null_marker_offset();\n+  }\n+  _has_inline_type_fields = _field_info->_has_inline_fields;\n@@ -5787,0 +6324,1 @@\n+\n@@ -5790,0 +6328,11 @@\n+  if (_field_info->_has_null_marker_offsets) {\n+    int idx = 0;\n+    _null_marker_offsets = MetadataFactory::new_array<int>(_loader_data, _temp_field_info->length(), 0, CHECK);\n+    for (GrowableArrayIterator<FieldInfo> it = _temp_field_info->begin(); it != _temp_field_info->end(); ++it, ++idx) {\n+      FieldInfo fieldinfo = *it;\n+      if (fieldinfo.field_flags().has_null_marker()) {\n+        assert(fieldinfo.null_marker_offset() != 0, \"Invalid value\");\n+        _null_marker_offsets->at_put(idx, fieldinfo.null_marker_offset());\n+      }\n+    }\n+  }\n","filename":"src\/hotspot\/share\/classfile\/classFileParser.cpp","additions":679,"deletions":130,"binary":false,"changes":809,"status":"modified"},{"patch":"@@ -77,1 +77,4 @@\n-  bool  _has_nonstatic_fields;\n+  bool _has_nonstatic_fields;\n+  bool _is_naturally_atomic;\n+  bool _has_inline_fields;\n+  bool _has_null_marker_offsets;\n@@ -132,0 +135,1 @@\n+  Array<u2>* _loadable_descriptors;\n@@ -134,0 +138,1 @@\n+  GrowableArray<u2>* _local_interface_indexes;\n@@ -144,0 +149,2 @@\n+  Array<InlineKlass*>* _inline_type_field_klasses;\n+  Array<int>* _null_marker_offsets;\n@@ -158,0 +165,5 @@\n+  int _alignment;\n+  int _first_field_offset;\n+  int _payload_size_in_bytes;\n+  int _internal_null_marker_offset;\n+\n@@ -196,0 +208,10 @@\n+  bool _has_inline_type_fields;\n+  bool _has_null_marker_offsets;\n+  bool _has_nonstatic_fields;\n+  bool _is_empty_inline_type;\n+  bool _is_naturally_atomic;\n+  bool _must_be_atomic;\n+  bool _is_implicitly_constructible;\n+  bool _has_loosely_consistent_annotation;\n+  bool _has_implicitly_constructible_annotation;\n+\n@@ -261,1 +283,1 @@\n-                    bool is_interface,\n+                    AccessFlags class_access_flags,\n@@ -270,0 +292,2 @@\n+                       bool is_value_class,\n+                       bool is_abstract_class,\n@@ -276,0 +300,2 @@\n+                     bool is_value_class,\n+                     bool is_abstract_class,\n@@ -330,0 +356,4 @@\n+  u2 parse_classfile_loadable_descriptors_attribute(const ClassFileStream* const cfs,\n+                                                    const u1* const loadable_descriptors_attribute_start,\n+                                                    TRAPS);\n+\n@@ -456,0 +486,2 @@\n+  bool legal_field_signature(const Symbol* signature, TRAPS) const;\n+\n@@ -468,2 +500,4 @@\n-  void verify_legal_class_modifiers(jint flags, TRAPS) const;\n-  void verify_legal_field_modifiers(jint flags, bool is_interface, TRAPS) const;\n+  void verify_legal_class_modifiers(jint flags, const char* name, bool is_Object, TRAPS) const;\n+  void verify_legal_field_modifiers(jint flags,\n+                                    AccessFlags class_access_flags,\n+                                    TRAPS) const;\n@@ -471,1 +505,1 @@\n-                                     bool is_interface,\n+                                     AccessFlags class_access_flags,\n@@ -523,0 +557,3 @@\n+  \/\/ Check if the class file supports inline types\n+  bool supports_inline_types() const;\n+\n@@ -550,0 +587,7 @@\n+  \/\/ Being an inline type means being a concrete value class\n+  bool is_inline_type() const { return !_access_flags.is_identity_class() && !_access_flags.is_interface() && !_access_flags.is_abstract(); }\n+  bool is_abstract_class() const { return _access_flags.is_abstract(); }\n+  bool is_identity_class() const { return _access_flags.is_identity_class(); }\n+  bool has_inline_fields() const { return _has_inline_type_fields; }\n+\n+  u2 java_fields_count() const { return _java_fields_count; }\n@@ -564,0 +608,2 @@\n+  bool is_class_in_loadable_descriptors_attribute(Symbol *klass);\n+\n","filename":"src\/hotspot\/share\/classfile\/classFileParser.hpp","additions":51,"deletions":5,"binary":false,"changes":56,"status":"modified"},{"patch":"@@ -61,0 +61,1 @@\n+\/\/    BufferedInlineTypeBlob   : used for pack\/unpack handlers\n@@ -84,0 +85,1 @@\n+  BufferedInlineType,\n@@ -164,0 +166,1 @@\n+  bool is_buffered_inline_type_blob() const   { return _kind == CodeBlobKind::BufferedInlineType; }\n@@ -305,0 +308,1 @@\n+  friend class BufferedInlineTypeBlob;\n@@ -311,1 +315,2 @@\n-  BufferBlob(const char* name, CodeBlobKind kind, CodeBuffer* cb, int size);\n+  BufferBlob(const char* name, CodeBlobKind kind, CodeBuffer* cb, int size, int header_size);\n+  BufferBlob(const char* name, CodeBlobKind kind, CodeBuffer* cb, int size, int frame_complete, int frame_size, OopMapSet* oop_maps, bool caller_must_gc_arguments = false);\n@@ -335,1 +340,1 @@\n-  AdapterBlob(int size, CodeBuffer* cb);\n+  AdapterBlob(int size, CodeBuffer* cb, int frame_complete, int frame_size, OopMapSet* oop_maps, bool caller_must_gc_arguments = false);\n@@ -339,1 +344,7 @@\n-  static AdapterBlob* create(CodeBuffer* cb);\n+  static AdapterBlob* create(CodeBuffer* cb,\n+                             int frame_complete,\n+                             int frame_size,\n+                             OopMapSet* oop_maps,\n+                             bool caller_must_gc_arguments = false);\n+\n+  bool caller_must_gc_arguments(JavaThread* thread) const { return true; }\n@@ -366,0 +377,19 @@\n+\/\/----------------------------------------------------------------------------------------------------\n+\/\/ BufferedInlineTypeBlob : used for pack\/unpack handlers\n+\n+class BufferedInlineTypeBlob: public BufferBlob {\n+private:\n+  const int _pack_fields_off;\n+  const int _pack_fields_jobject_off;\n+  const int _unpack_fields_off;\n+\n+  BufferedInlineTypeBlob(int size, CodeBuffer* cb, int pack_fields_off, int pack_fields_jobject_off, int unpack_fields_off);\n+\n+public:\n+  \/\/ Creation\n+  static BufferedInlineTypeBlob* create(CodeBuffer* cb, int pack_fields_off, int pack_fields_jobject_off, int unpack_fields_off);\n+\n+  address pack_fields() const { return code_begin() + _pack_fields_off; }\n+  address pack_fields_jobject() const { return code_begin() + _pack_fields_jobject_off; }\n+  address unpack_fields() const { return code_begin() + _unpack_fields_off; }\n+};\n","filename":"src\/hotspot\/share\/code\/codeBlob.hpp","additions":33,"deletions":3,"binary":false,"changes":36,"status":"modified"},{"patch":"@@ -1114,0 +1114,1 @@\n+    case Bytecodes::_checkcast:\n@@ -1174,1 +1175,7 @@\n-    os->print(\"\\\" is null\");\n+    address code_base = _method->constMethod()->code_base();\n+    Bytecodes::Code code = Bytecodes::java_code_at(_method, code_base + bci);\n+    if (code == Bytecodes::_aastore) {\n+      os->print(\"\\\" is null or is a null-free array and there's an attempt to store null in it\");\n+    } else {\n+      os->print(\"\\\" is null\");\n+    }\n@@ -1436,0 +1443,5 @@\n+    case Bytecodes::_checkcast: {\n+        int cp_index = Bytes::get_Java_u2(code_base + pos);\n+        ConstantPool* cp = _method->constants();\n+        os->print(\"Cannot cast to null-free type \\\"%s\\\"\", cp->klass_at_noresolve(cp_index)->as_C_string());\n+      } break;\n","filename":"src\/hotspot\/share\/interpreter\/bytecodeUtils.cpp","additions":13,"deletions":1,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -30,0 +30,1 @@\n+#include \"classfile\/symbolTable.hpp\"\n@@ -41,0 +42,1 @@\n+#include \"oops\/objArrayKlass.hpp\"\n@@ -112,0 +114,19 @@\n+Symbol* ArrayKlass::create_element_klass_array_name(Klass* element_klass, TRAPS) {\n+  ResourceMark rm(THREAD);\n+  Symbol* name = nullptr;\n+  char *name_str = element_klass->name()->as_C_string();\n+  int len = element_klass->name()->utf8_length();\n+  char *new_str = NEW_RESOURCE_ARRAY(char, len + 4);\n+  int idx = 0;\n+  new_str[idx++] = JVM_SIGNATURE_ARRAY;\n+  if (element_klass->is_instance_klass()) { \/\/ it could be an array or simple type\n+    new_str[idx++] = JVM_SIGNATURE_CLASS;\n+  }\n+  memcpy(&new_str[idx], name_str, len * sizeof(char));\n+  idx += len;\n+  if (element_klass->is_instance_klass()) {\n+    new_str[idx++] = JVM_SIGNATURE_ENDCLASS;\n+  }\n+  new_str[idx++] = '\\0';\n+  return SymbolTable::new_symbol(new_str);\n+}\n@@ -143,1 +164,1 @@\n-          ObjArrayKlass::allocate_objArray_klass(class_loader_data(), dim + 1, this, CHECK_NULL);\n+      ObjArrayKlass::allocate_objArray_klass(class_loader_data(), dim + 1, this, false, CHECK_NULL);\n@@ -201,0 +222,4 @@\n+oop ArrayKlass::component_mirror() const {\n+  return java_lang_Class::component_mirror(java_mirror());\n+}\n+\n@@ -202,1 +227,4 @@\n-  return JVM_ACC_ABSTRACT | JVM_ACC_FINAL | JVM_ACC_PUBLIC;\n+  int identity_flag = (Arguments::enable_preview()) ? JVM_ACC_IDENTITY : 0;\n+\n+  return JVM_ACC_ABSTRACT | JVM_ACC_FINAL | JVM_ACC_PUBLIC\n+                    | identity_flag;\n","filename":"src\/hotspot\/share\/oops\/arrayKlass.cpp","additions":30,"deletions":2,"binary":false,"changes":32,"status":"modified"},{"patch":"@@ -46,0 +46,7 @@\n+  Klass* _element_klass;            \/\/ The klass of the elements of this array type\n+                                    \/\/ The element type must be registered for both object arrays\n+                                    \/\/ (incl. object arrays with value type elements) and value type\n+                                    \/\/ arrays containing flat value types. However, the element\n+                                    \/\/ type must not be registered for arrays of primitive types.\n+                                    \/\/ TODO: Update the class hierarchy so that element klass appears\n+                                    \/\/ only in array that contain non-primitive types.\n@@ -52,0 +59,3 @@\n+  \/\/ Create array_name for element klass\n+  static Symbol* create_element_klass_array_name(Klass* element_klass, TRAPS);\n+\n@@ -55,0 +65,11 @@\n+  \/\/ Instance variables\n+  virtual Klass* element_klass() const      { return _element_klass; }\n+  virtual void set_element_klass(Klass* k)  { _element_klass = k; }\n+\n+  \/\/ Compiler\/Interpreter offset\n+  static ByteSize element_klass_offset() { return in_ByteSize(offset_of(ArrayKlass, _element_klass)); }\n+\n+  \/\/ Are loads and stores to this concrete array type atomic?\n+  \/\/ Note that Object[] is naturally atomic, but its subtypes may not be.\n+  virtual bool element_access_is_atomic() { return true; }\n+\n@@ -113,0 +134,2 @@\n+  oop component_mirror() const;\n+\n","filename":"src\/hotspot\/share\/oops\/arrayKlass.hpp","additions":23,"deletions":0,"binary":false,"changes":23,"status":"modified"},{"patch":"@@ -75,0 +75,1 @@\n+#include \"oops\/inlineKlass.hpp\"\n@@ -172,0 +173,13 @@\n+bool InstanceKlass::field_is_null_free_inline_type(int index) const {\n+  return field(index).field_flags().is_null_free_inline_type();\n+}\n+\n+bool InstanceKlass::is_class_in_loadable_descriptors_attribute(Symbol* name) const {\n+  if (_loadable_descriptors == nullptr) return false;\n+  for (int i = 0; i < _loadable_descriptors->length(); i++) {\n+        Symbol* class_name = _constants->klass_at_noresolve(_loadable_descriptors->at(i));\n+        if (class_name == name) return true;\n+  }\n+  return false;\n+}\n+\n@@ -449,1 +463,2 @@\n-                                       parser.is_interface());\n+                                       parser.is_interface(),\n+                                       parser.is_inline_type());\n@@ -472,0 +487,3 @@\n+  } else if (parser.is_inline_type()) {\n+    \/\/ inline type\n+    ik = new (loader_data, size, use_class_space, THREAD) InlineKlass(parser);\n@@ -483,0 +501,6 @@\n+#ifdef ASSERT\n+  ik->bounds_check((address) ik->start_of_vtable(), false, size);\n+  ik->bounds_check((address) ik->start_of_itable(), false, size);\n+  ik->bounds_check((address) ik->end_of_itable(), true, size);\n+  ik->bounds_check((address) ik->end_of_nonstatic_oop_maps(), true, size);\n+#endif \/\/ASSERT\n@@ -486,0 +510,23 @@\n+#ifndef PRODUCT\n+bool InstanceKlass::bounds_check(address addr, bool edge_ok, intptr_t size_in_bytes) const {\n+  const char* bad = nullptr;\n+  address end = nullptr;\n+  if (addr < (address)this) {\n+    bad = \"before\";\n+  } else if (addr == (address)this) {\n+    if (edge_ok)  return true;\n+    bad = \"just before\";\n+  } else if (addr == (end = (address)this + sizeof(intptr_t) * (size_in_bytes < 0 ? size() : size_in_bytes))) {\n+    if (edge_ok)  return true;\n+    bad = \"just after\";\n+  } else if (addr > end) {\n+    bad = \"after\";\n+  } else {\n+    return true;\n+  }\n+  tty->print_cr(\"%s object bounds: \" INTPTR_FORMAT \" [\" INTPTR_FORMAT \"..\" INTPTR_FORMAT \"]\",\n+      bad, (intptr_t)addr, (intptr_t)this, (intptr_t)end);\n+  Verbose = WizardMode = true; this->print(); \/\/@@\n+  return false;\n+}\n+#endif \/\/PRODUCT\n@@ -525,1 +572,5 @@\n-  _init_thread(nullptr)\n+  _init_thread(nullptr),\n+  _inline_type_field_klasses(nullptr),\n+  _null_marker_offsets(nullptr),\n+  _loadable_descriptors(nullptr),\n+  _adr_inlineklass_fixed_block(nullptr)\n@@ -532,0 +583,3 @@\n+  if (parser.has_inline_fields()) {\n+    set_has_inline_type_fields();\n+  }\n@@ -671,0 +725,10 @@\n+  if (inline_type_field_klasses_array() != nullptr) {\n+    MetadataFactory::free_array<InlineKlass*>(loader_data, inline_type_field_klasses_array());\n+    set_inline_type_field_klasses_array(nullptr);\n+  }\n+\n+  if (null_marker_offsets_array() != nullptr) {\n+    MetadataFactory::free_array<int>(loader_data, null_marker_offsets_array());\n+    set_null_marker_offsets_array(nullptr);\n+  }\n+\n@@ -705,0 +769,7 @@\n+  if (loadable_descriptors() != nullptr &&\n+      loadable_descriptors() != Universe::the_empty_short_array() &&\n+      !loadable_descriptors()->is_shared()) {\n+    MetadataFactory::free_array<jushort>(loader_data, loadable_descriptors());\n+  }\n+  set_loadable_descriptors(nullptr);\n+\n@@ -863,0 +934,102 @@\n+\n+  \/\/ If a class declares a method that uses an inline class as an argument\n+  \/\/ type or return inline type, this inline class must be loaded during the\n+  \/\/ linking of this class because size and properties of the inline class\n+  \/\/ must be known in order to be able to perform inline type optimizations.\n+  \/\/ The implementation below is an approximation of this rule, the code\n+  \/\/ iterates over all methods of the current class (including overridden\n+  \/\/ methods), not only the methods declared by this class. This\n+  \/\/ approximation makes the code simpler, and doesn't change the semantic\n+  \/\/ because classes declaring methods overridden by the current class are\n+  \/\/ linked (and have performed their own pre-loading) before the linking\n+  \/\/ of the current class.\n+\n+\n+  \/\/ Note:\n+  \/\/ Inline class types are loaded during\n+  \/\/ the loading phase (see ClassFileParser::post_process_parsed_stream()).\n+  \/\/ Inline class types used as element types for array creation\n+  \/\/ are not pre-loaded. Their loading is triggered by either anewarray\n+  \/\/ or multianewarray bytecodes.\n+\n+  \/\/ Could it be possible to do the following processing only if the\n+  \/\/ class uses inline types?\n+  if (EnableValhalla) {\n+    ResourceMark rm(THREAD);\n+    for (AllFieldStream fs(this); !fs.done(); fs.next()) {\n+      if (fs.is_null_free_inline_type() && fs.access_flags().is_static()) {\n+        Symbol* sig = fs.signature();\n+        TempNewSymbol s = Signature::strip_envelope(sig);\n+        if (s != name()) {\n+          log_info(class, preload)(\"Preloading class %s during linking of class %s. Cause: a null-free static field is declared with this type\", s->as_C_string(), name()->as_C_string());\n+          Klass* klass = SystemDictionary::resolve_or_fail(s,\n+                                                          Handle(THREAD, class_loader()), Handle(THREAD, protection_domain()), true,\n+                                                          CHECK_false);\n+          if (HAS_PENDING_EXCEPTION) {\n+            log_warning(class, preload)(\"Preloading of class %s during linking of class %s (cause: null-free static field) failed: %s\",\n+                                      s->as_C_string(), name()->as_C_string(), PENDING_EXCEPTION->klass()->name()->as_C_string());\n+            return false; \/\/ Exception is still pending\n+          }\n+          log_info(class, preload)(\"Preloading of class %s during linking of class %s (cause: null-free static field) succeeded\",\n+                                   s->as_C_string(), name()->as_C_string());\n+          assert(klass != nullptr, \"Sanity check\");\n+          if (!klass->is_inline_klass()) {\n+            THROW_MSG_(vmSymbols::java_lang_IncompatibleClassChangeError(),\n+                       err_msg(\"class %s expects class %s to be a value class but it is an identity class\",\n+                       name()->as_C_string(), klass->external_name()), false);\n+          }\n+          if (klass->is_abstract()) {\n+            THROW_MSG_(vmSymbols::java_lang_IncompatibleClassChangeError(),\n+                      err_msg(\"Class %s expects class %s to be concrete value class, but it is an abstract class\",\n+                      name()->as_C_string(),\n+                      InstanceKlass::cast(klass)->external_name()), false);\n+          }\n+          InstanceKlass* ik = InstanceKlass::cast(klass);\n+          if (!ik->is_implicitly_constructible()) {\n+             THROW_MSG_(vmSymbols::java_lang_IncompatibleClassChangeError(),\n+                        err_msg(\"class %s is not implicitly constructible and it is used in a null restricted static field (not supported)\",\n+                        klass->external_name()), false);\n+          }\n+          \/\/ the inline_type_field_klasses_array might have been loaded with CDS, so update only if not already set and check consistency\n+          if (inline_type_field_klasses_array()->at(fs.index()) == nullptr) {\n+            set_inline_type_field_klass(fs.index(), InlineKlass::cast(ik));\n+          }\n+          assert(get_inline_type_field_klass(fs.index()) == ik, \"Must match\");\n+        } else {\n+          if (inline_type_field_klasses_array()->at(fs.index()) == nullptr) {\n+            set_inline_type_field_klass(fs.index(), InlineKlass::cast(this));\n+          }\n+          assert(get_inline_type_field_klass(fs.index()) == this, \"Must match\");\n+        }\n+      }\n+    }\n+\n+    \/\/ Aggressively preloading all classes from the LoadableDescriptors attribute\n+    if (loadable_descriptors() != nullptr) {\n+      HandleMark hm(THREAD);\n+      for (int i = 0; i < loadable_descriptors()->length(); i++) {\n+        Symbol* sig = constants()->symbol_at(loadable_descriptors()->at(i));\n+        if (!Signature::has_envelope(sig)) continue;\n+        TempNewSymbol class_name = Signature::strip_envelope(sig);\n+        if (class_name == name()) continue;\n+        log_info(class, preload)(\"Preloading class %s during linking of class %s because of the class is listed in the LoadableDescriptors attribute\", sig->as_C_string(), name()->as_C_string());\n+        oop loader = class_loader();\n+        oop protection_domain = this->protection_domain();\n+        Klass* klass = SystemDictionary::resolve_or_null(class_name,\n+                                                         Handle(THREAD, loader), Handle(THREAD, protection_domain), THREAD);\n+        if (HAS_PENDING_EXCEPTION) {\n+          CLEAR_PENDING_EXCEPTION;\n+        }\n+        if (klass != nullptr) {\n+          log_info(class, preload)(\"Preloading of class %s during linking of class %s (cause: LoadableDescriptors attribute) succeeded\", class_name->as_C_string(), name()->as_C_string());\n+          if (!klass->is_inline_klass()) {\n+            \/\/ Non value class are allowed by the current spec, but it could be an indication of an issue so let's log a warning\n+              log_warning(class, preload)(\"Preloading class %s during linking of class %s (cause: LoadableDescriptors attribute) but loaded class is not a value class\", class_name->as_C_string(), name()->as_C_string());\n+          }\n+        } else {\n+          log_warning(class, preload)(\"Preloading of class %s during linking of class %s (cause: LoadableDescriptors attribute) failed\", class_name->as_C_string(), name()->as_C_string());\n+        }\n+      }\n+    }\n+  }\n+\n@@ -1167,0 +1340,19 @@\n+  \/\/ Pre-allocating an instance of the default value\n+  if (is_inline_klass()) {\n+      InlineKlass* vk = InlineKlass::cast(this);\n+      oop val = vk->allocate_instance(THREAD);\n+      if (HAS_PENDING_EXCEPTION) {\n+          Handle e(THREAD, PENDING_EXCEPTION);\n+          CLEAR_PENDING_EXCEPTION;\n+          {\n+              EXCEPTION_MARK;\n+              add_initialization_error(THREAD, e);\n+              \/\/ Locks object, set state, and notify all waiting threads\n+              set_initialization_state_and_notify(initialization_error, THREAD);\n+              CLEAR_PENDING_EXCEPTION;\n+          }\n+          THROW_OOP(e());\n+      }\n+      vk->set_default_value(val);\n+  }\n+\n@@ -1199,1 +1391,33 @@\n-\n+  \/\/ Initialize classes of inline fields\n+  if (EnableValhalla) {\n+    for (AllFieldStream fs(this); !fs.done(); fs.next()) {\n+      if (fs.is_null_free_inline_type()) {\n+\n+        \/\/ inline type field klass array entries must have alreadyt been filed at load time or link time\n+        Klass* klass = get_inline_type_field_klass(fs.index());\n+\n+        InstanceKlass::cast(klass)->initialize(THREAD);\n+        if (fs.access_flags().is_static()) {\n+          if (java_mirror()->obj_field(fs.offset()) == nullptr) {\n+            java_mirror()->obj_field_put(fs.offset(), InlineKlass::cast(klass)->default_value());\n+          }\n+        }\n+\n+        if (HAS_PENDING_EXCEPTION) {\n+          Handle e(THREAD, PENDING_EXCEPTION);\n+          CLEAR_PENDING_EXCEPTION;\n+          {\n+            EXCEPTION_MARK;\n+            add_initialization_error(THREAD, e);\n+            \/\/ Locks object, set state, and notify all waiting threads\n+            set_initialization_state_and_notify(initialization_error, THREAD);\n+            CLEAR_PENDING_EXCEPTION;\n+          }\n+          THROW_OOP(e());\n+        }\n+      }\n+    }\n+  }\n+\n+\n+  \/\/ Step 9\n@@ -1222,1 +1446,1 @@\n-  \/\/ Step 9\n+  \/\/ Step 10\n@@ -1228,1 +1452,1 @@\n-    \/\/ Step 10 and 11\n+    \/\/ Step 11 and 12\n@@ -1533,1 +1757,1 @@\n-      ObjArrayKlass* k = ObjArrayKlass::allocate_objArray_klass(class_loader_data(), 1, this, CHECK_NULL);\n+      ObjArrayKlass* k = ObjArrayKlass::allocate_objArray_klass(class_loader_data(), 1, this, false, CHECK_NULL);\n@@ -1540,1 +1764,1 @@\n-  ObjArrayKlass* ak = array_klasses();\n+  ArrayKlass* ak = array_klasses();\n@@ -1547,2 +1771,2 @@\n-  ObjArrayKlass* oak = array_klasses_acquire();\n-  if (oak == nullptr) {\n+  ArrayKlass* ak = array_klasses_acquire();\n+  if (ak == nullptr) {\n@@ -1551,1 +1775,1 @@\n-    return oak->array_klass_or_null(n);\n+    return ak->array_klass_or_null(n);\n@@ -1568,1 +1792,1 @@\n-  if (clinit != nullptr && clinit->has_valid_initializer_flags()) {\n+  if (clinit != nullptr && clinit->is_class_initializer()) {\n@@ -1632,4 +1856,0 @@\n-bool InstanceKlass::contains_field_offset(int offset) {\n-  fieldDescriptor fd;\n-  return find_field_from_offset(offset, false, &fd);\n-}\n@@ -1717,0 +1937,9 @@\n+bool InstanceKlass::contains_field_offset(int offset) {\n+  if (this->is_inline_klass()) {\n+    InlineKlass* vk = InlineKlass::cast(this);\n+    return offset >= vk->first_field_offset() && offset < (vk->first_field_offset() + vk->get_payload_size_in_bytes());\n+  } else {\n+    fieldDescriptor fd;\n+    return find_field_from_offset(offset, false, &fd);\n+  }\n+}\n@@ -2108,0 +2337,3 @@\n+    if (name == vmSymbols::object_initializer_name()) {\n+      break;  \/\/ <init> is never inherited\n+    }\n@@ -2504,0 +2736,1 @@\n+  it->push(&_loadable_descriptors);\n@@ -2505,0 +2738,3 @@\n+\n+  it->push(&_inline_type_field_klasses, MetaspaceClosure::_writable);\n+  it->push(&_null_marker_offsets);\n@@ -2550,1 +2786,1 @@\n-  \/\/ These are not allocated from metaspace. They are safe to set to null.\n+  \/\/ These are not allocated from metaspace. They are safe to set to nullptr.\n@@ -2634,0 +2870,4 @@\n+  if (is_inline_klass()) {\n+    InlineKlass::cast(this)->initialize_calling_convention(CHECK);\n+  }\n+\n@@ -2662,1 +2902,1 @@\n-    assert(this == array_klasses()->bottom_klass(), \"sanity\");\n+    assert(this == ObjArrayKlass::cast(array_klasses())->bottom_klass(), \"sanity\");\n@@ -2855,0 +3095,2 @@\n+  return signature_name_of_carrier(JVM_SIGNATURE_CLASS);\n+}\n@@ -2856,0 +3098,1 @@\n+const char* InstanceKlass::signature_name_of_carrier(char c) const {\n@@ -2862,1 +3105,1 @@\n-  \/\/ Add L as type indicator\n+  \/\/ Add L or Q as type indicator\n@@ -2864,1 +3107,1 @@\n-  dest[dest_index++] = JVM_SIGNATURE_CLASS;\n+  dest[dest_index++] = c;\n@@ -3206,2 +3449,1 @@\n-  \/\/ Remember to strip ACC_SUPER bit\n-  return (access & (~JVM_ACC_SUPER)) & JVM_ACC_WRITTEN_FLAGS;\n+  return (access & JVM_ACC_WRITTEN_FLAGS);\n@@ -3461,1 +3703,4 @@\n-static void print_vtable(intptr_t* start, int len, outputStream* st) {\n+static void print_vtable(address self, intptr_t* start, int len, outputStream* st) {\n+  ResourceMark rm;\n+  int* forward_refs = NEW_RESOURCE_ARRAY(int, len);\n+  for (int i = 0; i < len; i++)  forward_refs[i] = 0;\n@@ -3465,0 +3710,5 @@\n+    if (forward_refs[i] != 0) {\n+      int from = forward_refs[i];\n+      int off = (int) start[from];\n+      st->print(\" (offset %d <= [%d])\", off, from);\n+    }\n@@ -3468,0 +3718,6 @@\n+    } else if (self != nullptr && e > 0 && e < 0x10000) {\n+      address location = self + e;\n+      int index = (int)((intptr_t*)location - start);\n+      st->print(\" (offset %d => [%d])\", (int)e, index);\n+      if (index >= 0 && index < len)\n+        forward_refs[index] = i;\n@@ -3474,1 +3730,22 @@\n-  return print_vtable(reinterpret_cast<intptr_t*>(start), len, st);\n+  return print_vtable(nullptr, reinterpret_cast<intptr_t*>(start), len, st);\n+}\n+\n+template<typename T>\n+ static void print_array_on(outputStream* st, Array<T>* array) {\n+   if (array == nullptr) { st->print_cr(\"nullptr\"); return; }\n+   array->print_value_on(st); st->cr();\n+   if (Verbose || WizardMode) {\n+     for (int i = 0; i < array->length(); i++) {\n+       st->print(\"%d : \", i); array->at(i)->print_value_on(st); st->cr();\n+     }\n+   }\n+ }\n+\n+static void print_array_on(outputStream* st, Array<int>* array) {\n+  if (array == nullptr) { st->print_cr(\"nullptr\"); return; }\n+  array->print_value_on(st); st->cr();\n+  if (Verbose || WizardMode) {\n+    for (int i = 0; i < array->length(); i++) {\n+      st->print(\"%d : %d\", i, array->at(i)); st->cr();\n+    }\n+  }\n@@ -3515,8 +3792,2 @@\n-  st->print(BULLET\"methods:           \"); methods()->print_value_on(st);               st->cr();\n-  if (Verbose || WizardMode) {\n-    Array<Method*>* method_array = methods();\n-    for (int i = 0; i < method_array->length(); i++) {\n-      st->print(\"%d : \", i); method_array->at(i)->print_value(); st->cr();\n-    }\n-  }\n-  st->print(BULLET\"method ordering:   \"); method_ordering()->print_value_on(st);      st->cr();\n+  st->print(BULLET\"methods:           \"); print_array_on(st, methods());\n+  st->print(BULLET\"method ordering:   \"); print_array_on(st, method_ordering());\n@@ -3524,7 +3795,1 @@\n-    st->print(BULLET\"default_methods:   \"); default_methods()->print_value_on(st);    st->cr();\n-    if (Verbose) {\n-      Array<Method*>* method_array = default_methods();\n-      for (int i = 0; i < method_array->length(); i++) {\n-        st->print(\"%d : \", i); method_array->at(i)->print_value(); st->cr();\n-      }\n-    }\n+    st->print(BULLET\"default_methods:   \"); print_array_on(st, default_methods());\n@@ -3590,0 +3855,1 @@\n+  st->print(BULLET\"loadable descriptors:     \"); loadable_descriptors()->print_value_on(st); st->cr();\n@@ -3600,1 +3866,1 @@\n-  if (itable_length() > 0 && (Verbose || WizardMode))  print_vtable(start_of_itable(), itable_length(), st);\n+  if (itable_length() > 0 && (Verbose || WizardMode))  print_vtable(nullptr, start_of_itable(), itable_length(), st);\n","filename":"src\/hotspot\/share\/oops\/instanceKlass.cpp","additions":305,"deletions":39,"binary":false,"changes":344,"status":"modified"},{"patch":"@@ -28,0 +28,1 @@\n+#include \"code\/vmreg.hpp\"\n@@ -60,0 +61,1 @@\n+\/\/    [EMBEDDED InlineKlassFixedBlock] only if is an InlineKlass instance\n@@ -76,0 +78,1 @@\n+class BufferedInlineTypeBlob;\n@@ -136,0 +139,18 @@\n+class SigEntry;\n+\n+class InlineKlassFixedBlock {\n+  Array<SigEntry>** _extended_sig;\n+  Array<VMRegPair>** _return_regs;\n+  address* _pack_handler;\n+  address* _pack_handler_jobject;\n+  address* _unpack_handler;\n+  int* _default_value_offset;\n+  ArrayKlass** _null_free_inline_array_klasses;\n+  int _alignment;\n+  int _first_field_offset;\n+  int _payload_size_in_bytes;\n+  int _internal_null_marker_offset; \/\/ -1 if none\n+\n+  friend class InlineKlass;\n+};\n+\n@@ -141,0 +162,1 @@\n+  friend class TemplateTable;\n@@ -176,1 +198,1 @@\n-  ObjArrayKlass* volatile _array_klasses;\n+  ArrayKlass* volatile _array_klasses;\n@@ -283,0 +305,5 @@\n+  Array<InlineKlass*>* _inline_type_field_klasses; \/\/ For \"inline class\" fields, null if none present\n+  Array<int>* _null_marker_offsets; \/\/ for flat fields with a null marker\n+  Array<u2>* _loadable_descriptors;\n+  const InlineKlassFixedBlock* _adr_inlineklass_fixed_block;\n+\n@@ -336,0 +363,24 @@\n+  bool has_inline_type_fields() const { return _misc_flags.has_inline_type_fields(); }\n+  void set_has_inline_type_fields()   { _misc_flags.set_has_inline_type_fields(true); }\n+\n+  bool is_empty_inline_type() const   { return _misc_flags.is_empty_inline_type(); }\n+  void set_is_empty_inline_type()     { _misc_flags.set_is_empty_inline_type(true); }\n+\n+  \/\/ Note:  The naturally_atomic property only applies to\n+  \/\/ inline classes; it is never true on identity classes.\n+  \/\/ The bit is placed on instanceKlass for convenience.\n+\n+  \/\/ Query if h\/w provides atomic load\/store for instances.\n+  bool is_naturally_atomic() const  { return _misc_flags.is_naturally_atomic(); }\n+  void set_is_naturally_atomic()    { _misc_flags.set_is_naturally_atomic(true); }\n+\n+  \/\/ Query if this class is mentioned in the JVM option ForceNonTearable.\n+  \/\/ This bit can occur anywhere, but is only significant\n+  \/\/ for inline classes *and* their super types.\n+  \/\/ It inherits from supers.\n+  bool must_be_atomic() const { return _misc_flags.must_be_atomic(); }\n+  void set_must_be_atomic()   { _misc_flags.set_must_be_atomic(true); }\n+\n+  bool is_implicitly_constructible() const { return _misc_flags.is_implicitly_constructible(); }\n+  void set_is_implicitly_constructible()   { _misc_flags.set_is_implicitly_constructible(true); }\n+\n@@ -351,4 +402,4 @@\n-  ObjArrayKlass* array_klasses() const     { return _array_klasses; }\n-  inline ObjArrayKlass* array_klasses_acquire() const; \/\/ load with acquire semantics\n-  inline void release_set_array_klasses(ObjArrayKlass* k); \/\/ store with release semantics\n-  void set_array_klasses(ObjArrayKlass* k) { _array_klasses = k; }\n+  ArrayKlass* array_klasses() const     { return _array_klasses; }\n+  inline ArrayKlass* array_klasses_acquire() const; \/\/ load with acquire semantics\n+  inline void release_set_array_klasses(ArrayKlass* k); \/\/ store with release semantics\n+  void set_array_klasses(ArrayKlass* k) { _array_klasses = k; }\n@@ -394,2 +445,2 @@\n-  int     field_offset      (int index) const { return field(index).offset(); }\n-  int     field_access_flags(int index) const { return field(index).access_flags().as_int(); }\n+  int field_offset      (int index) const { return field(index).offset(); }\n+  int field_access_flags(int index) const { return field(index).access_flags().as_int(); }\n@@ -400,0 +451,4 @@\n+  bool field_is_flat(int index) const { return field_flags(index).is_flat(); }\n+  bool field_has_null_marker(int index) const { return field_flags(index).has_null_marker(); }\n+  bool field_is_null_free_inline_type(int index) const;\n+  bool is_class_in_loadable_descriptors_attribute(Symbol* name) const;\n@@ -411,0 +466,3 @@\n+  Array<u2>* loadable_descriptors() const { return _loadable_descriptors; }\n+  void set_loadable_descriptors(Array<u2>* c) { _loadable_descriptors = c; }\n+\n@@ -532,0 +590,3 @@\n+  static ByteSize kind_offset() { return in_ByteSize(offset_of(InstanceKlass, _kind)); }\n+  static ByteSize misc_flags_offset() { return in_ByteSize(offset_of(InstanceKlass, _misc_flags)); }\n+\n@@ -858,0 +919,4 @@\n+  static ByteSize inline_type_field_klasses_offset() { return in_ByteSize(offset_of(InstanceKlass, _inline_type_field_klasses)); }\n+  static ByteSize null_marker_array_offset() { return in_ByteSize(offset_of(InstanceKlass, _null_marker_offsets)); }\n+  static ByteSize adr_inlineklass_fixed_block_offset() { return in_ByteSize(offset_of(InstanceKlass, _adr_inlineklass_fixed_block)); }\n+\n@@ -915,1 +980,2 @@\n-                  bool is_interface) {\n+                  bool is_interface,\n+                  bool is_inline_type) {\n@@ -920,1 +986,2 @@\n-           (is_interface ? (int)sizeof(Klass*)\/wordSize : 0));\n+           (is_interface ? (int)sizeof(Klass*)\/wordSize : 0) +\n+           (is_inline_type ? (int)sizeof(InlineKlassFixedBlock) : 0));\n@@ -926,1 +993,2 @@\n-                                               is_interface());\n+                                               is_interface(),\n+                                               is_inline_klass());\n@@ -933,0 +1001,1 @@\n+  bool bounds_check(address addr, bool edge_ok = false, intptr_t size_in_bytes = -1) const PRODUCT_RETURN0;\n@@ -939,0 +1008,11 @@\n+  Array<InlineKlass*>* inline_type_field_klasses_array() const { return _inline_type_field_klasses; }\n+  void set_inline_type_field_klasses_array(Array<InlineKlass*>* array) { _inline_type_field_klasses = array; }\n+\n+  Array<int>* null_marker_offsets_array() const { return _null_marker_offsets; }\n+  void set_null_marker_offsets_array(Array<int>* array) { _null_marker_offsets = array; }\n+\n+  inline InlineKlass* get_inline_type_field_klass(int idx) const;\n+  inline InlineKlass* get_inline_type_field_klass_or_null(int idx) const;\n+  inline void set_inline_type_field_klass(int idx, InlineKlass* k);\n+  inline void reset_inline_type_field_klass(int idx);\n+\n@@ -940,1 +1020,1 @@\n-  int size_helper() const {\n+  virtual int size_helper() const {\n@@ -991,0 +1071,1 @@\n+  const char* signature_name_of_carrier(char c) const;\n@@ -1116,1 +1197,1 @@\n-  void restore_unshareable_info(ClassLoaderData* loader_data, Handle protection_domain, PackageEntry* pkg_entry, TRAPS);\n+  virtual void restore_unshareable_info(ClassLoaderData* loader_data, Handle protection_domain, PackageEntry* pkg_entry, TRAPS);\n","filename":"src\/hotspot\/share\/oops\/instanceKlass.hpp","additions":93,"deletions":12,"binary":false,"changes":105,"status":"modified"},{"patch":"@@ -259,1 +259,2 @@\n-                           _shared_class_path_index(-1) {\n+                               _prototype_header(markWord::prototype()),\n+                               _shared_class_path_index(-1) {\n@@ -273,1 +274,1 @@\n-  int lh = array_layout_helper(tag, hsize, etype, exact_log2(esize));\n+  int lh = array_layout_helper(tag, false, hsize, etype, exact_log2(esize));\n@@ -969,0 +970,2 @@\n+     st->print(BULLET\"prototype_header: \" INTPTR_FORMAT, _prototype_header.value());\n+     st->cr();\n","filename":"src\/hotspot\/share\/oops\/klass.cpp","additions":5,"deletions":2,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -74,0 +74,1 @@\n+    InlineKlassKind,\n@@ -79,0 +80,1 @@\n+    FlatArrayKlassKind,\n@@ -105,1 +107,1 @@\n-  \/\/    tag is 0x80 if the elements are oops, 0xC0 if non-oops\n+  \/\/    tag is 0x80 if the elements are oops, 0xC0 if non-oops, 0xA0 if value types\n@@ -174,0 +176,1 @@\n+  markWord _prototype_header;  \/\/ inline type and inline array mark patterns\n@@ -445,1 +448,1 @@\n-  static const int _lh_array_tag_bits          = 2;\n+  static const int _lh_array_tag_bits          = 3;\n@@ -447,2 +450,9 @@\n-  static const int _lh_array_tag_obj_value     = ~0x01;   \/\/ 0x80000000 >> 30\n-  static const unsigned int _lh_array_tag_type_value = 0Xffffffff; \/\/ ~0x00,  \/\/ 0xC0000000 >> 30\n+  static const unsigned int _lh_array_tag_type_value = 0Xfffffffc;\n+  static const unsigned int _lh_array_tag_vt_value   = 0Xfffffffd;\n+  static const unsigned int _lh_array_tag_obj_value  = 0Xfffffffe;\n+\n+  \/\/ null-free array flag bit under the array tag bits, shift one more to get array tag value\n+  static const int _lh_null_free_shift = _lh_array_tag_shift - 1;\n+  static const int _lh_null_free_mask  = 1;\n+\n+  static const jint _lh_array_tag_flat_value_bit_inplace = (jint) (1 << _lh_array_tag_shift);\n@@ -466,2 +476,1 @@\n-    \/\/ _lh_array_tag_type_value == (lh >> _lh_array_tag_shift);\n-    return (juint)lh >= (juint)(_lh_array_tag_type_value << _lh_array_tag_shift);\n+    return (juint) _lh_array_tag_type_value == (juint)(lh >> _lh_array_tag_shift);\n@@ -470,2 +479,13 @@\n-    \/\/ _lh_array_tag_obj_value == (lh >> _lh_array_tag_shift);\n-    return (jint)lh < (jint)(_lh_array_tag_type_value << _lh_array_tag_shift);\n+    return (juint)_lh_array_tag_obj_value == (juint)(lh >> _lh_array_tag_shift);\n+  }\n+  static bool layout_helper_is_flatArray(jint lh) {\n+    return (juint)_lh_array_tag_vt_value == (juint)(lh >> _lh_array_tag_shift);\n+  }\n+  static bool layout_helper_is_null_free(jint lh) {\n+    assert(layout_helper_is_flatArray(lh) || layout_helper_is_objArray(lh), \"must be array of inline types\");\n+    return ((lh >> _lh_null_free_shift) & _lh_null_free_mask);\n+  }\n+  static jint layout_helper_set_null_free(jint lh) {\n+    lh |= (_lh_null_free_mask << _lh_null_free_shift);\n+    assert(layout_helper_is_null_free(lh), \"Bad encoding\");\n+    return lh;\n@@ -482,1 +502,1 @@\n-    assert(btvalue >= T_BOOLEAN && btvalue <= T_OBJECT, \"sanity\");\n+    assert((btvalue >= T_BOOLEAN && btvalue <= T_OBJECT) || btvalue == T_PRIMITIVE_OBJECT, \"sanity\");\n@@ -503,1 +523,1 @@\n-    assert(l2esz <= LogBytesPerLong,\n+    assert(layout_helper_element_type(lh) == T_PRIMITIVE_OBJECT || l2esz <= LogBytesPerLong,\n@@ -507,1 +527,1 @@\n-  static jint array_layout_helper(jint tag, int hsize, BasicType etype, int log2_esize) {\n+  static jint array_layout_helper(jint tag, bool null_free, int hsize, BasicType etype, int log2_esize) {\n@@ -509,0 +529,1 @@\n+      |    ((null_free ? 1 : 0) <<  _lh_null_free_shift)\n@@ -661,0 +682,1 @@\n+  virtual bool is_flatArray_klass_slow()    const { return false; }\n@@ -662,0 +684,2 @@\n+  \/\/ current implementation uses this method even in non debug builds\n+  virtual bool is_inline_klass_slow()       const { return false; }\n@@ -677,2 +701,1 @@\n-  \/\/ Other is anything that is not one of the more specialized kinds of InstanceKlass.\n-  bool is_other_instance_klass()        const { return _kind == InstanceKlassKind; }\n+  bool is_inline_klass()                const { return assert_same_query(_kind == InlineKlassKind, is_inline_klass_slow()); }\n@@ -684,0 +707,1 @@\n+  bool is_flatArray_klass()             const { return assert_same_query( _kind == FlatArrayKlassKind, is_flatArray_klass_slow()); }\n@@ -688,0 +712,2 @@\n+  inline bool is_null_free_array_klass()      const { return layout_helper_is_null_free(layout_helper()); }\n+\n@@ -696,1 +722,1 @@\n-  bool is_super() const                 { return _access_flags.is_super(); }\n+  bool is_identity_class() const        { return _access_flags.is_identity_class(); }\n@@ -713,0 +739,11 @@\n+  \/\/ inline types and inline type array patterns\n+  markWord prototype_header() const {\n+    return _prototype_header;\n+  }\n+  static inline markWord default_prototype_header(Klass* k) {\n+    return (k == nullptr) ? markWord::prototype() : k->prototype_header();\n+  }\n+\n+  inline void set_prototype_header(markWord header);\n+  static ByteSize prototype_header_offset() { return in_ByteSize(offset_of(Klass, _prototype_header)); }\n+\n","filename":"src\/hotspot\/share\/oops\/klass.hpp","additions":51,"deletions":14,"binary":false,"changes":65,"status":"modified"},{"patch":"@@ -60,0 +60,1 @@\n+#include \"opto\/inlinetypenode.hpp\"\n@@ -405,0 +406,3 @@\n+  if (dead->is_InlineType()) {\n+    remove_inline_type(dead);\n+  }\n@@ -446,0 +450,3 @@\n+    if (n->outcnt() == 0) {\n+      worklist.push(n);\n+    }\n@@ -453,0 +460,6 @@\n+  remove_useless_nodes(_inline_type_nodes,  useful); \/\/ remove useless inline type nodes\n+#ifdef ASSERT\n+  if (_modified_nodes != nullptr) {\n+    _modified_nodes->remove_useless_nodes(useful.member_set());\n+  }\n+#endif\n@@ -632,0 +645,1 @@\n+                  _has_circular_inline_type(false),\n@@ -651,0 +665,1 @@\n+                  _inline_type_nodes (comp_arena(), 8, 0, nullptr),\n@@ -752,4 +767,2 @@\n-      const TypeTuple *domain = StartOSRNode::osr_domain();\n-      const TypeTuple *range = TypeTuple::make_range(method()->signature());\n-      init_tf(TypeFunc::make(domain, range));\n-      StartNode* s = new StartOSRNode(root(), domain);\n+      init_tf(TypeFunc::make(method(), \/* is_osr_compilation = *\/ true));\n+      StartNode* s = new StartOSRNode(root(), tf()->domain_sig());\n@@ -762,1 +775,1 @@\n-      StartNode* s = new StartNode(root(), tf()->domain());\n+      StartNode* s = new StartNode(root(), tf()->domain_cc());\n@@ -878,0 +891,10 @@\n+  if (needs_stack_repair()) {\n+    \/\/ One extra slot for the special stack increment value\n+    next_slot += 2;\n+  }\n+  \/\/ TODO 8284443 Only reserve extra slot if needed\n+  if (InlineTypeReturnedAsFields) {\n+    \/\/ One extra slot to hold the IsInit information for a nullable\n+    \/\/ inline type return if we run out of registers.\n+    next_slot += 2;\n+  }\n@@ -912,0 +935,1 @@\n+    _has_circular_inline_type(false),\n@@ -1057,0 +1081,4 @@\n+  _has_flat_accesses = false;\n+  _flat_accesses_share_alias = true;\n+  _scalarize_in_safepoints = false;\n+\n@@ -1328,1 +1356,2 @@\n-    assert(InlineUnsafeOps || StressReflectiveCode, \"indeterminate pointers come only from unsafe ops\");\n+    bool default_value_load = EnableValhalla && tj->is_instptr()->instance_klass() == ciEnv::current()->Class_klass();\n+    assert(InlineUnsafeOps || StressReflectiveCode || default_value_load, \"indeterminate pointers come only from unsafe ops\");\n@@ -1341,0 +1370,9 @@\n+  if (ta && ta->is_not_flat()) {\n+    \/\/ Erase not flat property for alias analysis.\n+    tj = ta = ta->cast_to_not_flat(false);\n+  }\n+  if (ta && ta->is_not_null_free()) {\n+    \/\/ Erase not null free property for alias analysis.\n+    tj = ta = ta->cast_to_not_null_free(false);\n+  }\n+\n@@ -1354,0 +1392,2 @@\n+    \/\/ For flat inline type array, each field has its own slice so\n+    \/\/ we must include the field offset.\n@@ -1394,1 +1434,1 @@\n-      tj = ta = TypeAryPtr::make(ptr,ta->const_oop(),tary,nullptr,false,offset);\n+      tj = ta = TypeAryPtr::make(ptr,ta->const_oop(),tary,nullptr,false,Type::Offset(offset), ta->field_offset());\n@@ -1398,1 +1438,6 @@\n-      tj = ta = TypeAryPtr::make(ptr,ta->const_oop(),tary,nullptr,false,offset);\n+      tj = ta = TypeAryPtr::make(ptr,ta->const_oop(),tary,nullptr,false,Type::Offset(offset), ta->field_offset());\n+    }\n+    \/\/ Initially all flattened array accesses share a single slice\n+    if (ta->is_flat() && ta->elem() != TypeInstPtr::BOTTOM && _flat_accesses_share_alias) {\n+      const TypeAry* tary = TypeAry::make(TypeInstPtr::BOTTOM, ta->size(), \/* stable= *\/ false, \/* flat= *\/ true);\n+      tj = ta = TypeAryPtr::make(ptr,ta->const_oop(),tary,nullptr,false,Type::Offset(offset), Type::Offset(Type::OffsetBot));\n@@ -1405,1 +1450,1 @@\n-      tj = ta = TypeAryPtr::make(ptr,ta->const_oop(),tary,aklass,false,offset);\n+      tj = ta = TypeAryPtr::make(ptr,ta->const_oop(),tary,aklass,false,Type::Offset(offset), ta->field_offset());\n@@ -1455,1 +1500,1 @@\n-        tj = to = TypeInstPtr::make(TypePtr::BotPTR, env()->Object_klass(), false, nullptr, offset);\n+        tj = to = TypeInstPtr::make(TypePtr::BotPTR, env()->Object_klass(), false, nullptr, Type::Offset(offset));\n@@ -1470,1 +1515,1 @@\n-          tj = to = TypeInstPtr::make(to->ptr(), canonical_holder, true, nullptr, offset, to->instance_id());\n+          tj = to = TypeInstPtr::make(to->ptr(), canonical_holder, true, nullptr, Type::Offset(offset), to->instance_id());\n@@ -1472,1 +1517,1 @@\n-          tj = to = TypeInstPtr::make(to->ptr(), canonical_holder, false, nullptr, offset);\n+          tj = to = TypeInstPtr::make(to->ptr(), canonical_holder, false, nullptr, Type::Offset(offset));\n@@ -1488,1 +1533,1 @@\n-                                       offset);\n+                                       Type::Offset(offset));\n@@ -1494,1 +1539,1 @@\n-        tj = tk = TypeInstKlassPtr::make(TypePtr::NotNull, env()->Object_klass(), offset);\n+        tj = tk = TypeInstKlassPtr::make(TypePtr::NotNull, env()->Object_klass(), Type::Offset(offset));\n@@ -1496,1 +1541,1 @@\n-        tj = tk = TypeAryKlassPtr::make(TypePtr::NotNull, tk->is_aryklassptr()->elem(), k, offset);\n+        tj = tk = TypeAryKlassPtr::make(TypePtr::NotNull, tk->is_aryklassptr()->elem(), k, Type::Offset(offset), tk->is_not_flat(), tk->is_not_null_free(), tk->is_null_free());\n@@ -1499,1 +1544,0 @@\n-\n@@ -1629,1 +1673,1 @@\n-Compile::AliasType* Compile::find_alias_type(const TypePtr* adr_type, bool no_create, ciField* original_field) {\n+Compile::AliasType* Compile::find_alias_type(const TypePtr* adr_type, bool no_create, ciField* original_field, bool uncached) {\n@@ -1634,3 +1678,6 @@\n-  AliasCacheEntry* ace = probe_alias_cache(adr_type);\n-  if (ace->_adr_type == adr_type) {\n-    return alias_type(ace->_index);\n+  AliasCacheEntry* ace = nullptr;\n+  if (!uncached) {\n+    ace = probe_alias_cache(adr_type);\n+    if (ace->_adr_type == adr_type) {\n+      return alias_type(ace->_index);\n+    }\n@@ -1686,0 +1733,1 @@\n+    ciField* field = nullptr;\n@@ -1692,0 +1740,1 @@\n+      const Type* elemtype = flat->is_aryptr()->elem();\n@@ -1693,1 +1742,8 @@\n-        alias_type(idx)->set_element(flat->is_aryptr()->elem());\n+        alias_type(idx)->set_element(elemtype);\n+      }\n+      int field_offset = flat->is_aryptr()->field_offset().get();\n+      if (flat->is_flat() &&\n+          field_offset != Type::OffsetBot) {\n+        ciInlineKlass* vk = elemtype->inline_klass();\n+        field_offset += vk->first_field_offset();\n+        field = vk->get_field_by_offset(field_offset, false);\n@@ -1707,0 +1763,2 @@\n+      if (flat->offset() == in_bytes(Klass::layout_helper_offset()))\n+        alias_type(idx)->set_rewritable(false);\n@@ -1717,1 +1775,0 @@\n-      ciField* field;\n@@ -1724,0 +1781,4 @@\n+      } else if (tinst->is_inlinetypeptr()) {\n+        \/\/ Inline type field\n+        ciInlineKlass* vk = tinst->inline_klass();\n+        field = vk->get_field_by_offset(tinst->offset(), false);\n@@ -1728,7 +1789,14 @@\n-      assert(field == nullptr ||\n-             original_field == nullptr ||\n-             (field->holder() == original_field->holder() &&\n-              field->offset_in_bytes() == original_field->offset_in_bytes() &&\n-              field->is_static() == original_field->is_static()), \"wrong field?\");\n-      \/\/ Set field() and is_rewritable() attributes.\n-      if (field != nullptr)  alias_type(idx)->set_field(field);\n+    }\n+    assert(field == nullptr ||\n+           original_field == nullptr ||\n+           (field->holder() == original_field->holder() &&\n+            field->offset_in_bytes() == original_field->offset_in_bytes() &&\n+            field->is_static() == original_field->is_static()), \"wrong field?\");\n+    \/\/ Set field() and is_rewritable() attributes.\n+    if (field != nullptr) {\n+      alias_type(idx)->set_field(field);\n+      if (flat->isa_aryptr()) {\n+        \/\/ Fields of flat arrays are rewritable although they are declared final\n+        assert(flat->is_flat(), \"must be a flat array\");\n+        alias_type(idx)->set_rewritable(true);\n+      }\n@@ -1739,3 +1807,4 @@\n-  ace->_adr_type = adr_type;\n-  ace->_index    = idx;\n-  assert(alias_type(adr_type) == alias_type(idx),  \"type must be installed\");\n+  if (!uncached) {\n+    ace->_adr_type = adr_type;\n+    ace->_index    = idx;\n+    assert(alias_type(adr_type) == alias_type(idx),  \"type must be installed\");\n@@ -1743,6 +1812,7 @@\n-  \/\/ Might as well try to fill the cache for the flattened version, too.\n-  AliasCacheEntry* face = probe_alias_cache(flat);\n-  if (face->_adr_type == nullptr) {\n-    face->_adr_type = flat;\n-    face->_index    = idx;\n-    assert(alias_type(flat) == alias_type(idx), \"flat type must work too\");\n+    \/\/ Might as well try to fill the cache for the flattened version, too.\n+    AliasCacheEntry* face = probe_alias_cache(flat);\n+    if (face->_adr_type == nullptr) {\n+      face->_adr_type = flat;\n+      face->_index    = idx;\n+      assert(alias_type(flat) == alias_type(idx), \"flat type must work too\");\n+    }\n@@ -1868,0 +1938,414 @@\n+void Compile::add_inline_type(Node* n) {\n+  assert(n->is_InlineType(), \"unexpected node\");\n+  _inline_type_nodes.push(n);\n+}\n+\n+void Compile::remove_inline_type(Node* n) {\n+  assert(n->is_InlineType(), \"unexpected node\");\n+  if (_inline_type_nodes.contains(n)) {\n+    _inline_type_nodes.remove(n);\n+  }\n+}\n+\n+\/\/ Does the return value keep otherwise useless inline type allocations alive?\n+static bool return_val_keeps_allocations_alive(Node* ret_val) {\n+  ResourceMark rm;\n+  Unique_Node_List wq;\n+  wq.push(ret_val);\n+  bool some_allocations = false;\n+  for (uint i = 0; i < wq.size(); i++) {\n+    Node* n = wq.at(i);\n+    if (n->outcnt() > 1) {\n+      \/\/ Some other use for the allocation\n+      return false;\n+    } else if (n->is_InlineType()) {\n+      wq.push(n->in(1));\n+    } else if (n->is_Phi()) {\n+      for (uint j = 1; j < n->req(); j++) {\n+        wq.push(n->in(j));\n+      }\n+    } else if (n->is_CheckCastPP() &&\n+               n->in(1)->is_Proj() &&\n+               n->in(1)->in(0)->is_Allocate()) {\n+      some_allocations = true;\n+    } else if (n->is_CheckCastPP()) {\n+      wq.push(n->in(1));\n+    }\n+  }\n+  return some_allocations;\n+}\n+\n+void Compile::process_inline_types(PhaseIterGVN &igvn, bool remove) {\n+  \/\/ Make sure that the return value does not keep an otherwise unused allocation alive\n+  if (tf()->returns_inline_type_as_fields()) {\n+    Node* ret = nullptr;\n+    for (uint i = 1; i < root()->req(); i++) {\n+      Node* in = root()->in(i);\n+      if (in->Opcode() == Op_Return) {\n+        assert(ret == nullptr, \"only one return\");\n+        ret = in;\n+      }\n+    }\n+    if (ret != nullptr) {\n+      Node* ret_val = ret->in(TypeFunc::Parms);\n+      if (igvn.type(ret_val)->isa_oopptr() &&\n+          return_val_keeps_allocations_alive(ret_val)) {\n+        igvn.replace_input_of(ret, TypeFunc::Parms, InlineTypeNode::tagged_klass(igvn.type(ret_val)->inline_klass(), igvn));\n+        assert(ret_val->outcnt() == 0, \"should be dead now\");\n+        igvn.remove_dead_node(ret_val);\n+      }\n+    }\n+  }\n+  if (_inline_type_nodes.length() == 0) {\n+    return;\n+  }\n+  \/\/ Scalarize inline types in safepoint debug info.\n+  \/\/ Delay this until all inlining is over to avoid getting inconsistent debug info.\n+  set_scalarize_in_safepoints(true);\n+  for (int i = _inline_type_nodes.length()-1; i >= 0; i--) {\n+    InlineTypeNode* vt = _inline_type_nodes.at(i)->as_InlineType();\n+    vt->make_scalar_in_safepoints(&igvn);\n+    igvn.record_for_igvn(vt);\n+  }\n+  if (remove) {\n+    \/\/ Remove inline type nodes by replacing them with their oop input\n+    while (_inline_type_nodes.length() > 0) {\n+      InlineTypeNode* vt = _inline_type_nodes.pop()->as_InlineType();\n+      if (vt->outcnt() == 0) {\n+        igvn.remove_dead_node(vt);\n+        continue;\n+      }\n+      for (DUIterator i = vt->outs(); vt->has_out(i); i++) {\n+        DEBUG_ONLY(bool must_be_buffered = false);\n+        Node* u = vt->out(i);\n+        \/\/ Check if any users are blackholes. If so, rewrite them to use either the\n+        \/\/ allocated buffer, or individual components, instead of the inline type node\n+        \/\/ that goes away.\n+        if (u->is_Blackhole()) {\n+          BlackholeNode* bh = u->as_Blackhole();\n+\n+          \/\/ Unlink the old input\n+          int idx = bh->find_edge(vt);\n+          assert(idx != -1, \"The edge should be there\");\n+          bh->del_req(idx);\n+          --i;\n+\n+          if (vt->is_allocated(&igvn)) {\n+            \/\/ Already has the allocated instance, blackhole that\n+            bh->add_req(vt->get_oop());\n+          } else {\n+            \/\/ Not allocated yet, blackhole the components\n+            for (uint c = 0; c < vt->field_count(); c++) {\n+              bh->add_req(vt->field_value(c));\n+            }\n+          }\n+\n+          \/\/ Node modified, record for IGVN\n+          igvn.record_for_igvn(bh);\n+        }\n+#ifdef ASSERT\n+        \/\/ Verify that inline type is buffered when replacing by oop\n+        else if (u->is_InlineType()) {\n+          \/\/ InlineType uses don't need buffering because they are about to be replaced as well\n+        } else if (u->is_Phi()) {\n+          \/\/ TODO 8302217 Remove this once InlineTypeNodes are reliably pushed through\n+        } else {\n+          must_be_buffered = true;\n+        }\n+        if (must_be_buffered && !vt->is_allocated(&igvn)) {\n+          vt->dump(0);\n+          u->dump(0);\n+          assert(false, \"Should have been buffered\");\n+        }\n+#endif\n+      }\n+      igvn.replace_node(vt, vt->get_oop());\n+    }\n+  }\n+  igvn.optimize();\n+}\n+\n+void Compile::adjust_flat_array_access_aliases(PhaseIterGVN& igvn) {\n+  if (!_has_flat_accesses) {\n+    return;\n+  }\n+  \/\/ Initially, all flat array accesses share the same slice to\n+  \/\/ keep dependencies with Object[] array accesses (that could be\n+  \/\/ to a flat array) correct. We're done with parsing so we\n+  \/\/ now know all flat array accesses in this compile\n+  \/\/ unit. Let's move flat array accesses to their own slice,\n+  \/\/ one per element field. This should help memory access\n+  \/\/ optimizations.\n+  ResourceMark rm;\n+  Unique_Node_List wq;\n+  wq.push(root());\n+\n+  Node_List mergememnodes;\n+  Node_List memnodes;\n+\n+  \/\/ Alias index currently shared by all flat memory accesses\n+  int index = get_alias_index(TypeAryPtr::INLINES);\n+\n+  \/\/ Find MergeMem nodes and flat array accesses\n+  for (uint i = 0; i < wq.size(); i++) {\n+    Node* n = wq.at(i);\n+    if (n->is_Mem()) {\n+      const TypePtr* adr_type = nullptr;\n+      if (n->Opcode() == Op_StoreCM) {\n+        adr_type = get_adr_type(get_alias_index(n->in(MemNode::OopStore)->adr_type()));\n+      } else {\n+        adr_type = get_adr_type(get_alias_index(n->adr_type()));\n+      }\n+      if (adr_type == TypeAryPtr::INLINES) {\n+        memnodes.push(n);\n+      }\n+    } else if (n->is_MergeMem()) {\n+      MergeMemNode* mm = n->as_MergeMem();\n+      if (mm->memory_at(index) != mm->base_memory()) {\n+        mergememnodes.push(n);\n+      }\n+    }\n+    for (uint j = 0; j < n->req(); j++) {\n+      Node* m = n->in(j);\n+      if (m != nullptr) {\n+        wq.push(m);\n+      }\n+    }\n+  }\n+\n+  if (memnodes.size() > 0) {\n+    _flat_accesses_share_alias = false;\n+\n+    \/\/ We are going to change the slice for the flat array\n+    \/\/ accesses so we need to clear the cache entries that refer to\n+    \/\/ them.\n+    for (uint i = 0; i < AliasCacheSize; i++) {\n+      AliasCacheEntry* ace = &_alias_cache[i];\n+      if (ace->_adr_type != nullptr &&\n+          ace->_adr_type->is_flat()) {\n+        ace->_adr_type = nullptr;\n+        ace->_index = (i != 0) ? 0 : AliasIdxTop; \/\/ Make sure the nullptr adr_type resolves to AliasIdxTop\n+      }\n+    }\n+\n+    \/\/ Find what aliases we are going to add\n+    int start_alias = num_alias_types()-1;\n+    int stop_alias = 0;\n+\n+    for (uint i = 0; i < memnodes.size(); i++) {\n+      Node* m = memnodes.at(i);\n+      const TypePtr* adr_type = nullptr;\n+      if (m->Opcode() == Op_StoreCM) {\n+        adr_type = m->in(MemNode::OopStore)->adr_type();\n+        if (adr_type != TypeAryPtr::INLINES) {\n+          \/\/ store was optimized out and we lost track of the adr_type\n+          Node* clone = new StoreCMNode(m->in(MemNode::Control), m->in(MemNode::Memory), m->in(MemNode::Address),\n+                                        m->adr_type(), m->in(MemNode::ValueIn), m->in(MemNode::OopStore),\n+                                        get_alias_index(adr_type));\n+          igvn.register_new_node_with_optimizer(clone);\n+          igvn.replace_node(m, clone);\n+        }\n+      } else {\n+        adr_type = m->adr_type();\n+#ifdef ASSERT\n+        m->as_Mem()->set_adr_type(adr_type);\n+#endif\n+      }\n+      int idx = get_alias_index(adr_type);\n+      start_alias = MIN2(start_alias, idx);\n+      stop_alias = MAX2(stop_alias, idx);\n+    }\n+\n+    assert(stop_alias >= start_alias, \"should have expanded aliases\");\n+\n+    Node_Stack stack(0);\n+#ifdef ASSERT\n+    VectorSet seen(Thread::current()->resource_area());\n+#endif\n+    \/\/ Now let's fix the memory graph so each flat array access\n+    \/\/ is moved to the right slice. Start from the MergeMem nodes.\n+    uint last = unique();\n+    for (uint i = 0; i < mergememnodes.size(); i++) {\n+      MergeMemNode* current = mergememnodes.at(i)->as_MergeMem();\n+      Node* n = current->memory_at(index);\n+      MergeMemNode* mm = nullptr;\n+      do {\n+        \/\/ Follow memory edges through memory accesses, phis and\n+        \/\/ narrow membars and push nodes on the stack. Once we hit\n+        \/\/ bottom memory, we pop element off the stack one at a\n+        \/\/ time, in reverse order, and move them to the right slice\n+        \/\/ by changing their memory edges.\n+        if ((n->is_Phi() && n->adr_type() != TypePtr::BOTTOM) || n->is_Mem() || n->adr_type() == TypeAryPtr::INLINES) {\n+          assert(!seen.test_set(n->_idx), \"\");\n+          \/\/ Uses (a load for instance) will need to be moved to the\n+          \/\/ right slice as well and will get a new memory state\n+          \/\/ that we don't know yet. The use could also be the\n+          \/\/ backedge of a loop. We put a place holder node between\n+          \/\/ the memory node and its uses. We replace that place\n+          \/\/ holder with the correct memory state once we know it,\n+          \/\/ i.e. when nodes are popped off the stack. Using the\n+          \/\/ place holder make the logic work in the presence of\n+          \/\/ loops.\n+          if (n->outcnt() > 1) {\n+            Node* place_holder = nullptr;\n+            assert(!n->has_out_with(Op_Node), \"\");\n+            for (DUIterator k = n->outs(); n->has_out(k); k++) {\n+              Node* u = n->out(k);\n+              if (u != current && u->_idx < last) {\n+                bool success = false;\n+                for (uint l = 0; l < u->req(); l++) {\n+                  if (!stack.is_empty() && u == stack.node() && l == stack.index()) {\n+                    continue;\n+                  }\n+                  Node* in = u->in(l);\n+                  if (in == n) {\n+                    if (place_holder == nullptr) {\n+                      place_holder = new Node(1);\n+                      place_holder->init_req(0, n);\n+                    }\n+                    igvn.replace_input_of(u, l, place_holder);\n+                    success = true;\n+                  }\n+                }\n+                if (success) {\n+                  --k;\n+                }\n+              }\n+            }\n+          }\n+          if (n->is_Phi()) {\n+            stack.push(n, 1);\n+            n = n->in(1);\n+          } else if (n->is_Mem()) {\n+            stack.push(n, n->req());\n+            n = n->in(MemNode::Memory);\n+          } else {\n+            assert(n->is_Proj() && n->in(0)->Opcode() == Op_MemBarCPUOrder, \"\");\n+            stack.push(n, n->req());\n+            n = n->in(0)->in(TypeFunc::Memory);\n+          }\n+        } else {\n+          assert(n->adr_type() == TypePtr::BOTTOM || (n->Opcode() == Op_Node && n->_idx >= last) || (n->is_Proj() && n->in(0)->is_Initialize()), \"\");\n+          \/\/ Build a new MergeMem node to carry the new memory state\n+          \/\/ as we build it. IGVN should fold extraneous MergeMem\n+          \/\/ nodes.\n+          mm = MergeMemNode::make(n);\n+          igvn.register_new_node_with_optimizer(mm);\n+          while (stack.size() > 0) {\n+            Node* m = stack.node();\n+            uint idx = stack.index();\n+            if (m->is_Mem()) {\n+              \/\/ Move memory node to its new slice\n+              const TypePtr* adr_type = m->adr_type();\n+              int alias = get_alias_index(adr_type);\n+              Node* prev = mm->memory_at(alias);\n+              igvn.replace_input_of(m, MemNode::Memory, prev);\n+              mm->set_memory_at(alias, m);\n+            } else if (m->is_Phi()) {\n+              \/\/ We need as many new phis as there are new aliases\n+              igvn.replace_input_of(m, idx, mm);\n+              if (idx == m->req()-1) {\n+                Node* r = m->in(0);\n+                for (uint j = (uint)start_alias; j <= (uint)stop_alias; j++) {\n+                  const TypePtr* adr_type = get_adr_type(j);\n+                  if (!adr_type->isa_aryptr() || !adr_type->is_flat() || j == (uint)index) {\n+                    continue;\n+                  }\n+                  Node* phi = new PhiNode(r, Type::MEMORY, get_adr_type(j));\n+                  igvn.register_new_node_with_optimizer(phi);\n+                  for (uint k = 1; k < m->req(); k++) {\n+                    phi->init_req(k, m->in(k)->as_MergeMem()->memory_at(j));\n+                  }\n+                  mm->set_memory_at(j, phi);\n+                }\n+                Node* base_phi = new PhiNode(r, Type::MEMORY, TypePtr::BOTTOM);\n+                igvn.register_new_node_with_optimizer(base_phi);\n+                for (uint k = 1; k < m->req(); k++) {\n+                  base_phi->init_req(k, m->in(k)->as_MergeMem()->base_memory());\n+                }\n+                mm->set_base_memory(base_phi);\n+              }\n+            } else {\n+              \/\/ This is a MemBarCPUOrder node from\n+              \/\/ Parse::array_load()\/Parse::array_store(), in the\n+              \/\/ branch that handles flat arrays hidden under\n+              \/\/ an Object[] array. We also need one new membar per\n+              \/\/ new alias to keep the unknown access that the\n+              \/\/ membars protect properly ordered with accesses to\n+              \/\/ known flat array.\n+              assert(m->is_Proj(), \"projection expected\");\n+              Node* ctrl = m->in(0)->in(TypeFunc::Control);\n+              igvn.replace_input_of(m->in(0), TypeFunc::Control, top());\n+              for (uint j = (uint)start_alias; j <= (uint)stop_alias; j++) {\n+                const TypePtr* adr_type = get_adr_type(j);\n+                if (!adr_type->isa_aryptr() || !adr_type->is_flat() || j == (uint)index) {\n+                  continue;\n+                }\n+                MemBarNode* mb = new MemBarCPUOrderNode(this, j, nullptr);\n+                igvn.register_new_node_with_optimizer(mb);\n+                Node* mem = mm->memory_at(j);\n+                mb->init_req(TypeFunc::Control, ctrl);\n+                mb->init_req(TypeFunc::Memory, mem);\n+                ctrl = new ProjNode(mb, TypeFunc::Control);\n+                igvn.register_new_node_with_optimizer(ctrl);\n+                mem = new ProjNode(mb, TypeFunc::Memory);\n+                igvn.register_new_node_with_optimizer(mem);\n+                mm->set_memory_at(j, mem);\n+              }\n+              igvn.replace_node(m->in(0)->as_Multi()->proj_out(TypeFunc::Control), ctrl);\n+            }\n+            if (idx < m->req()-1) {\n+              idx += 1;\n+              stack.set_index(idx);\n+              n = m->in(idx);\n+              break;\n+            }\n+            \/\/ Take care of place holder nodes\n+            if (m->has_out_with(Op_Node)) {\n+              Node* place_holder = m->find_out_with(Op_Node);\n+              if (place_holder != nullptr) {\n+                Node* mm_clone = mm->clone();\n+                igvn.register_new_node_with_optimizer(mm_clone);\n+                Node* hook = new Node(1);\n+                hook->init_req(0, mm);\n+                igvn.replace_node(place_holder, mm_clone);\n+                hook->destruct(&igvn);\n+              }\n+              assert(!m->has_out_with(Op_Node), \"place holder should be gone now\");\n+            }\n+            stack.pop();\n+          }\n+        }\n+      } while(stack.size() > 0);\n+      \/\/ Fix the memory state at the MergeMem we started from\n+      igvn.rehash_node_delayed(current);\n+      for (uint j = (uint)start_alias; j <= (uint)stop_alias; j++) {\n+        const TypePtr* adr_type = get_adr_type(j);\n+        if (!adr_type->isa_aryptr() || !adr_type->is_flat()) {\n+          continue;\n+        }\n+        current->set_memory_at(j, mm);\n+      }\n+      current->set_memory_at(index, current->base_memory());\n+    }\n+    igvn.optimize();\n+  }\n+  print_method(PHASE_SPLIT_INLINES_ARRAY, 2);\n+#ifdef ASSERT\n+  if (!_flat_accesses_share_alias) {\n+    wq.clear();\n+    wq.push(root());\n+    for (uint i = 0; i < wq.size(); i++) {\n+      Node* n = wq.at(i);\n+      assert(n->adr_type() != TypeAryPtr::INLINES, \"should have been removed from the graph\");\n+      for (uint j = 0; j < n->req(); j++) {\n+        Node* m = n->in(j);\n+        if (m != nullptr) {\n+          wq.push(m);\n+        }\n+      }\n+    }\n+  }\n+#endif\n+}\n+\n@@ -1943,1 +2427,1 @@\n-        if (!live_locals.at(i) && !local->is_top() && local != lhs && local!= rhs) {\n+        if (!live_locals.at(i) && !local->is_top() && local != lhs && local != rhs) {\n@@ -1958,1 +2442,1 @@\n-    \/\/ keep the mondified trap for late query\n+    \/\/ keep the modified trap for late query\n@@ -2153,1 +2637,4 @@\n-  assert(_modified_nodes == nullptr, \"not allowed\");\n+#ifdef ASSERT\n+  Unique_Node_List* modified_nodes = _modified_nodes;\n+  _modified_nodes = nullptr;\n+#endif\n@@ -2166,0 +2653,1 @@\n+  DEBUG_ONLY( _modified_nodes = modified_nodes; )\n@@ -2312,0 +2800,5 @@\n+  \/\/ Process inline type nodes now that all inlining is over\n+  process_inline_types(igvn);\n+\n+  adjust_flat_array_access_aliases(igvn);\n+\n@@ -2435,0 +2928,8 @@\n+  assert(_late_inlines.length() == 0 || IncrementalInlineMH || IncrementalInlineVirtual, \"not empty\");\n+\n+  if (_late_inlines.length() > 0) {\n+    \/\/ More opportunities to optimize virtual and MH calls.\n+    \/\/ Though it's maybe too late to perform inlining, strength-reducing them to direct calls is still an option.\n+    process_late_inline_calls_no_inline(igvn);\n+  }\n+\n@@ -2446,0 +2947,4 @@\n+  \/\/ Process inline type nodes again and remove them. From here\n+  \/\/ on we don't need to keep track of field values anymore.\n+  process_inline_types(igvn, \/* remove= *\/ true);\n+\n@@ -2462,0 +2967,1 @@\n+  DEBUG_ONLY( _late_inlines.clear(); )\n@@ -2464,9 +2970,0 @@\n-\n-  assert(_late_inlines.length() == 0 || IncrementalInlineMH || IncrementalInlineVirtual, \"not empty\");\n-\n-  if (_late_inlines.length() > 0) {\n-    \/\/ More opportunities to optimize virtual and MH calls.\n-    \/\/ Though it's maybe too late to perform inlining, strength-reducing them to direct calls is still an option.\n-    process_late_inline_calls_no_inline(igvn);\n-    if (failing())  return;\n-  }\n@@ -3100,0 +3597,1 @@\n+\n@@ -3251,1 +3749,16 @@\n-      n->add_prec(prec);\n+      if (prec->is_MergeMem()) {\n+        MergeMemNode* mm = prec->as_MergeMem();\n+        Node* base = mm->base_memory();\n+        for (int i = AliasIdxRaw + 1; i < num_alias_types(); i++) {\n+          const TypePtr* adr_type = get_adr_type(i);\n+          if (adr_type->is_flat()) {\n+            Node* m = mm->memory_at(i);\n+            n->add_prec(m);\n+          }\n+        }\n+        if (mm->outcnt() == 0) {\n+          mm->disconnect_inputs(this);\n+        }\n+      } else {\n+        n->add_prec(prec);\n+      }\n@@ -3872,0 +4385,7 @@\n+#ifdef ASSERT\n+  case Op_InlineType: {\n+    n->dump(-1);\n+    assert(false, \"inline type node was not removed\");\n+    break;\n+  }\n+#endif\n@@ -4251,2 +4771,2 @@\n-      if (accessing_method->is_static_initializer() ||\n-          accessing_method->is_object_initializer() ||\n+      if (accessing_method->is_class_initializer() ||\n+          accessing_method->is_object_constructor() ||\n@@ -4260,1 +4780,1 @@\n-      if (accessing_method->is_static_initializer()) {\n+      if (accessing_method->is_class_initializer()) {\n@@ -4316,0 +4836,1 @@\n+               (n->is_Allocate() && i >= AllocateNode::InlineType) ||\n@@ -4318,1 +4839,1 @@\n-              \"only region, phi, arraycopy, unlock or membar nodes have null data edges\");\n+              \"only region, phi, arraycopy, allocate, unlock or membar nodes have null data edges\");\n@@ -4448,0 +4969,7 @@\n+\n+    \/\/ Do not fold the subtype check to an array klass pointer comparison for null-able inline type arrays\n+    \/\/ because null-free [LMyValue <: null-able [LMyValue but the klasses are different. Perform a full test.\n+    if (!superk->is_aryklassptr()->is_null_free() && superk->is_aryklassptr()->elem()->isa_instklassptr() &&\n+        superk->is_aryklassptr()->elem()->is_instklassptr()->instance_klass()->is_inlinetype()) {\n+      return SSC_full_test;\n+    }\n@@ -5047,0 +5575,21 @@\n+Node* Compile::optimize_acmp(PhaseGVN* phase, Node* a, Node* b) {\n+  const TypeInstPtr* ta = phase->type(a)->isa_instptr();\n+  const TypeInstPtr* tb = phase->type(b)->isa_instptr();\n+  if (!EnableValhalla || ta == nullptr || tb == nullptr ||\n+      ta->is_zero_type() || tb->is_zero_type() ||\n+      !ta->can_be_inline_type() || !tb->can_be_inline_type()) {\n+    \/\/ Use old acmp if one operand is null or not an inline type\n+    return new CmpPNode(a, b);\n+  } else if (ta->is_inlinetypeptr() || tb->is_inlinetypeptr()) {\n+    \/\/ We know that one operand is an inline type. Therefore,\n+    \/\/ new acmp will only return true if both operands are nullptr.\n+    \/\/ Check if both operands are null by or'ing the oops.\n+    a = phase->transform(new CastP2XNode(nullptr, a));\n+    b = phase->transform(new CastP2XNode(nullptr, b));\n+    a = phase->transform(new OrXNode(a, b));\n+    return new CmpXNode(a, phase->MakeConX(0));\n+  }\n+  \/\/ Use new acmp\n+  return nullptr;\n+}\n+\n","filename":"src\/hotspot\/share\/opto\/compile.cpp","additions":604,"deletions":55,"binary":false,"changes":659,"status":"modified"},{"patch":"@@ -55,0 +55,1 @@\n+class CallNode;\n@@ -97,0 +98,1 @@\n+class InlineTypeNode;\n@@ -333,0 +335,1 @@\n+  bool                  _has_circular_inline_type; \/\/ True if method loads an inline type with a circular, non-flat field\n@@ -360,0 +363,3 @@\n+  bool                  _has_flat_accesses;     \/\/ Any known flat array accesses?\n+  bool                  _flat_accesses_share_alias; \/\/ Initially all flat array share a single slice\n+  bool                  _scalarize_in_safepoints; \/\/ Scalarize inline types in safepoint debug info\n@@ -376,0 +382,1 @@\n+  GrowableArray<Node*>  _inline_type_nodes;     \/\/ List of InlineType nodes\n@@ -642,0 +649,2 @@\n+  bool              has_circular_inline_type() const { return _has_circular_inline_type; }\n+  void          set_has_circular_inline_type(bool z) { _has_circular_inline_type = z; }\n@@ -674,0 +683,10 @@\n+  void          set_flat_accesses()              { _has_flat_accesses = true; }\n+  bool          flat_accesses_share_alias() const { return _flat_accesses_share_alias; }\n+  void          set_flat_accesses_share_alias(bool z) { _flat_accesses_share_alias = z; }\n+  bool          scalarize_in_safepoints() const { return _scalarize_in_safepoints; }\n+  void          set_scalarize_in_safepoints(bool z) { _scalarize_in_safepoints = z; }\n+\n+  \/\/ Support for scalarized inline type calling convention\n+  bool              has_scalarized_args() const  { return _method != nullptr && _method->has_scalarized_args(); }\n+  bool              needs_stack_repair()  const  { return _method != nullptr && _method->get_Method()->c2_needs_stack_repair(); }\n+\n@@ -796,0 +815,7 @@\n+  \/\/ Keep track of inline type nodes for later processing\n+  void add_inline_type(Node* n);\n+  void remove_inline_type(Node* n);\n+  void process_inline_types(PhaseIterGVN &igvn, bool remove = false);\n+\n+  void adjust_flat_array_access_aliases(PhaseIterGVN& igvn);\n+\n@@ -953,1 +979,1 @@\n-  AliasType*        alias_type(const TypePtr* adr_type, ciField* field = nullptr) { return find_alias_type(adr_type, false, field); }\n+  AliasType*        alias_type(const TypePtr* adr_type, ciField* field = nullptr, bool uncached = false) { return find_alias_type(adr_type, false, field, uncached); }\n@@ -957,1 +983,1 @@\n-  int               get_alias_index(const TypePtr* at)  { return alias_type(at)->index(); }\n+  int               get_alias_index(const TypePtr* at, bool uncached = false) { return alias_type(at, nullptr, uncached)->index(); }\n@@ -1199,1 +1225,1 @@\n-  AliasType* find_alias_type(const TypePtr* adr_type, bool no_create, ciField* field);\n+  AliasType* find_alias_type(const TypePtr* adr_type, bool no_create, ciField* field, bool uncached = false);\n@@ -1275,1 +1301,3 @@\n-  \/\/ Auxiliary methods for randomized fuzzing\/stressing\n+  Node* optimize_acmp(PhaseGVN* phase, Node* a, Node* b);\n+\n+  \/\/ Auxiliary method for randomized fuzzing\/stressing\n","filename":"src\/hotspot\/share\/opto\/compile.hpp","additions":32,"deletions":4,"binary":false,"changes":36,"status":"modified"},{"patch":"@@ -51,2 +51,2 @@\n-  const TypeTuple *jdomain = C->tf()->domain();\n-  const TypeTuple *jrange  = C->tf()->range();\n+  const TypeTuple *jdomain = C->tf()->domain_sig();\n+  const TypeTuple *jrange  = C->tf()->range_sig();\n@@ -277,1 +277,1 @@\n-    if (C->tf()->range()->cnt() > TypeFunc::Parms)\n+    if (C->tf()->range_sig()->cnt() > TypeFunc::Parms)\n","filename":"src\/hotspot\/share\/opto\/generateOptoStub.cpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -46,0 +46,1 @@\n+class UnswitchCandidate;\n@@ -82,1 +83,2 @@\n-         LoopNestLongOuterLoop = 1<<16 };\n+         LoopNestLongOuterLoop = 1<<16,\n+         FlatArrays            = 1<<17};\n@@ -105,0 +107,1 @@\n+  bool is_flat_arrays() const { return _loop_flags & FlatArrays; }\n@@ -118,0 +121,1 @@\n+  void mark_flat_arrays() { _loop_flags |= FlatArrays; }\n@@ -693,0 +697,1 @@\n+  bool no_unswitch_candidate() const;\n@@ -1435,1 +1440,2 @@\n-  IfNode* find_unswitch_candidate(const IdealLoopTree* loop) const;\n+  IfNode* find_unswitch_candidates(const IdealLoopTree* loop, Node_List& flat_array_checks) const;\n+  IfNode* find_unswitch_candidate_from_idoms(const IdealLoopTree* loop) const;\n@@ -1442,1 +1448,1 @@\n-                                   const UnswitchedLoopSelector& unswitched_loop_selector);\n+                                   const UnswitchCandidate& unswitch_candidate, const IfNode* loop_selector);\n@@ -1450,0 +1456,1 @@\n+                                            const UnswitchCandidate& unswitch_candidate,\n@@ -1584,0 +1591,1 @@\n+  void move_flat_array_check_out_of_loop(Node* n);\n@@ -1585,0 +1593,1 @@\n+  bool flat_array_element_type_check(Node *n);\n@@ -1794,0 +1803,2 @@\n+  void collect_flat_array_checks(const IdealLoopTree* loop, Node_List& flat_array_checks) const;\n+\n","filename":"src\/hotspot\/share\/opto\/loopnode.hpp","additions":14,"deletions":3,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -200,0 +200,12 @@\n+  \/\/ Code pattern on return from a call that returns an __Value.  Can\n+  \/\/ be optimized away if the return value turns out to be an oop.\n+  if (op == Op_AndX &&\n+      in(1) != nullptr &&\n+      in(1)->Opcode() == Op_CastP2X &&\n+      in(1)->in(1) != nullptr &&\n+      phase->type(in(1)->in(1))->isa_oopptr() &&\n+      t2->isa_intptr_t()->_lo >= 0 &&\n+      t2->isa_intptr_t()->_hi <= MinObjAlignmentInBytesMask) {\n+    return add_id();\n+  }\n+\n@@ -902,0 +914,41 @@\n+  \/\/ Search for GraphKit::mark_word_test patterns and fold the test if the result is statically known\n+  Node* load1 = in(1);\n+  Node* load2 = nullptr;\n+  if (load1->is_Phi() && phase->type(load1)->isa_long()) {\n+    load1 = in(1)->in(1);\n+    load2 = in(1)->in(2);\n+  }\n+  if (load1 != nullptr && load1->is_Load() && phase->type(load1)->isa_long() &&\n+      (load2 == nullptr || (load2->is_Load() && phase->type(load2)->isa_long()))) {\n+    const TypePtr* adr_t1 = phase->type(load1->in(MemNode::Address))->isa_ptr();\n+    const TypePtr* adr_t2 = (load2 != nullptr) ? phase->type(load2->in(MemNode::Address))->isa_ptr() : nullptr;\n+    if (adr_t1 != nullptr && adr_t1->offset() == oopDesc::mark_offset_in_bytes() &&\n+        (load2 == nullptr || (adr_t2 != nullptr && adr_t2->offset() == in_bytes(Klass::prototype_header_offset())))) {\n+      if (mask == markWord::inline_type_pattern) {\n+        if (adr_t1->is_inlinetypeptr()) {\n+          set_req_X(1, in(2), phase);\n+          return this;\n+        } else if (!adr_t1->can_be_inline_type()) {\n+          set_req_X(1, phase->longcon(0), phase);\n+          return this;\n+        }\n+      } else if (mask == markWord::null_free_array_bit_in_place) {\n+        if (adr_t1->is_null_free()) {\n+          set_req_X(1, in(2), phase);\n+          return this;\n+        } else if (adr_t1->is_not_null_free()) {\n+          set_req_X(1, phase->longcon(0), phase);\n+          return this;\n+        }\n+      } else if (mask == markWord::flat_array_bit_in_place) {\n+        if (adr_t1->is_flat()) {\n+          set_req_X(1, in(2), phase);\n+          return this;\n+        } else if (adr_t1->is_not_flat()) {\n+          set_req_X(1, phase->longcon(0), phase);\n+          return this;\n+        }\n+      }\n+    }\n+  }\n+\n","filename":"src\/hotspot\/share\/opto\/mulnode.cpp","additions":53,"deletions":0,"binary":false,"changes":53,"status":"modified"},{"patch":"@@ -33,0 +33,1 @@\n+#include \"opto\/convertnode.hpp\"\n@@ -34,0 +35,1 @@\n+#include \"opto\/inlinetypenode.hpp\"\n@@ -105,4 +107,10 @@\n-Node *Parse::fetch_interpreter_state(int index,\n-                                     BasicType bt,\n-                                     Node *local_addrs,\n-                                     Node *local_addrs_base) {\n+Node* Parse::fetch_interpreter_state(int index,\n+                                     const Type* type,\n+                                     Node* local_addrs,\n+                                     Node* local_addrs_base) {\n+  BasicType bt = type->basic_type();\n+  if (type == TypePtr::NULL_PTR) {\n+    \/\/ Ptr types are mixed together with T_ADDRESS but nullptr is\n+    \/\/ really for T_OBJECT types so correct it.\n+    bt = T_OBJECT;\n+  }\n@@ -150,1 +158,0 @@\n-\n@@ -174,0 +181,6 @@\n+    if (tp->is_inlinetypeptr() && !tp->maybe_null()) {\n+      \/\/ Check inline types for null here to prevent checkcast from adding an\n+      \/\/ exception state before the bytecode entry (use 'bad_type_ctrl' instead).\n+      l = null_check_oop(l, &bad_type_ctrl);\n+      bad_type_exit->control()->add_req(bad_type_ctrl);\n+    }\n@@ -190,1 +203,0 @@\n-\n@@ -245,1 +257,1 @@\n-    Node *lock_object = fetch_interpreter_state(index*2, T_OBJECT, monitors_addr, osr_buf);\n+    Node* lock_object = fetch_interpreter_state(index*2, Type::get_const_basic_type(T_OBJECT), monitors_addr, osr_buf);\n@@ -247,2 +259,1 @@\n-    Node *displaced_hdr = fetch_interpreter_state((index*2) + 1, T_ADDRESS, monitors_addr, osr_buf);\n-\n+    Node* displaced_hdr = fetch_interpreter_state((index*2) + 1, Type::get_const_basic_type(T_ADDRESS), monitors_addr, osr_buf);\n@@ -316,7 +327,1 @@\n-    BasicType bt = type->basic_type();\n-    if (type == TypePtr::NULL_PTR) {\n-      \/\/ Ptr types are mixed together with T_ADDRESS but null is\n-      \/\/ really for T_OBJECT types so correct it.\n-      bt = T_OBJECT;\n-    }\n-    Node *value = fetch_interpreter_state(index, bt, locals_addr, osr_buf);\n+    Node* value = fetch_interpreter_state(index, type, locals_addr, osr_buf);\n@@ -522,1 +527,3 @@\n-    assert(false, \"type flow analysis failed during parsing\");\n+    \/\/ TODO Adding a trap due to an unloaded return type in ciTypeFlow::StateVector::do_invoke\n+    \/\/ can lead to this. Re-enable once 8284443 is fixed.\n+    \/\/assert(false, \"type flow analysis failed during parsing\");\n@@ -613,0 +620,22 @@\n+  \/\/ Handle inline type arguments\n+  int arg_size = method()->arg_size();\n+  for (int i = 0; i < arg_size; i++) {\n+    Node* parm = local(i);\n+    const Type* t = _gvn.type(parm);\n+    if (t->is_inlinetypeptr()) {\n+      \/\/ Create InlineTypeNode from the oop and replace the parameter\n+      bool is_larval = (i == 0) && method()->is_object_constructor() && !method()->holder()->is_java_lang_Object();\n+      Node* vt = InlineTypeNode::make_from_oop(this, parm, t->inline_klass(), !t->maybe_null(), is_larval);\n+      replace_in_map(parm, vt);\n+    } else if (UseTypeSpeculation && (i == (arg_size - 1)) && !is_osr_parse() && method()->has_vararg() &&\n+               t->isa_aryptr() != nullptr && !t->is_aryptr()->is_null_free() && !t->is_aryptr()->is_not_null_free()) {\n+      \/\/ Speculate on varargs Object array being not null-free (and therefore also not flat)\n+      const TypePtr* spec_type = t->speculative();\n+      spec_type = (spec_type != nullptr && spec_type->isa_aryptr() != nullptr) ? spec_type : t->is_aryptr();\n+      spec_type = spec_type->remove_speculative()->is_aryptr()->cast_to_not_null_free();\n+      spec_type = TypeOopPtr::make(TypePtr::BotPTR, Type::Offset::bottom, TypeOopPtr::InstanceBot, spec_type);\n+      Node* cast = _gvn.transform(new CheckCastPPNode(control(), parm, t->join_speculative(spec_type)));\n+      replace_in_map(parm, cast);\n+    }\n+  }\n+\n@@ -798,2 +827,2 @@\n-  if (tf()->range()->cnt() > TypeFunc::Parms) {\n-    const Type* ret_type = tf()->range()->field_at(TypeFunc::Parms);\n+  if (tf()->range_sig()->cnt() > TypeFunc::Parms) {\n+    const Type* ret_type = tf()->range_sig()->field_at(TypeFunc::Parms);\n@@ -821,1 +850,1 @@\n-    assert((int)(tf()->range()->cnt() - TypeFunc::Parms) == ret_size, \"good tf range\");\n+    assert((int)(tf()->range_sig()->cnt() - TypeFunc::Parms) == ret_size, \"good tf range\");\n@@ -828,1 +857,0 @@\n-\n@@ -833,2 +861,2 @@\n-  int        arg_size = tf->domain()->cnt();\n-  int        max_size = MAX2(arg_size, (int)tf->range()->cnt());\n+  int        arg_size = tf->domain_sig()->cnt();\n+  int        max_size = MAX2(arg_size, (int)tf->range_cc()->cnt());\n@@ -837,0 +865,1 @@\n+  jvms->set_map(map);\n@@ -848,3 +877,20 @@\n-  uint i;\n-  for (i = 0; i < (uint)arg_size; i++) {\n-    Node* parm = initial_gvn()->transform(new ParmNode(start, i));\n+  PhaseGVN& gvn = *initial_gvn();\n+  uint i = 0;\n+  int arg_num = 0;\n+  for (uint j = 0; i < (uint)arg_size; i++) {\n+    const Type* t = tf->domain_sig()->field_at(i);\n+    Node* parm = nullptr;\n+    if (t->is_inlinetypeptr() && method()->is_scalarized_arg(arg_num)) {\n+      \/\/ Inline type arguments are not passed by reference: we get an argument per\n+      \/\/ field of the inline type. Build InlineTypeNodes from the inline type arguments.\n+      GraphKit kit(jvms, &gvn);\n+      kit.set_control(map->control());\n+      Node* old_mem = map->memory();\n+      \/\/ Use immutable memory for inline type loads and restore it below\n+      kit.set_all_memory(C->immutable_memory());\n+      parm = InlineTypeNode::make_from_multi(&kit, start, t->inline_klass(), j, \/* in= *\/ true, \/* null_free= *\/ !t->maybe_null());\n+      map->set_control(kit.control());\n+      map->set_memory(old_mem);\n+    } else {\n+      parm = gvn.transform(new ParmNode(start, j++));\n+    }\n@@ -854,0 +900,3 @@\n+    if (i >= TypeFunc::Parms && t != Type::HALF) {\n+      arg_num++;\n+    }\n@@ -860,1 +909,0 @@\n-  jvms->set_map(map);\n@@ -887,1 +935,1 @@\n-  int ret_size = tf()->range()->cnt() - TypeFunc::Parms;\n+  int ret_size = tf()->range_sig()->cnt() - TypeFunc::Parms;\n@@ -891,2 +939,24 @@\n-    ret->add_req(kit.argument(0));\n-    \/\/ Note:  The second dummy edge is not needed by a ReturnNode.\n+    Node* res = kit.argument(0);\n+    if (tf()->returns_inline_type_as_fields()) {\n+      \/\/ Multiple return values (inline type fields): add as many edges\n+      \/\/ to the Return node as returned values.\n+      InlineTypeNode* vt = res->as_InlineType();\n+      ret->add_req_batch(nullptr, tf()->range_cc()->cnt() - TypeFunc::Parms);\n+      if (vt->is_allocated(&kit.gvn()) && !StressCallingConvention) {\n+        ret->init_req(TypeFunc::Parms, vt);\n+      } else {\n+        \/\/ Return the tagged klass pointer to signal scalarization to the caller\n+        Node* tagged_klass = vt->tagged_klass(kit.gvn());\n+        \/\/ Return null if the inline type is null (IsInit field is not set)\n+        Node* conv   = kit.gvn().transform(new ConvI2LNode(vt->get_is_init()));\n+        Node* shl    = kit.gvn().transform(new LShiftLNode(conv, kit.intcon(63)));\n+        Node* shr    = kit.gvn().transform(new RShiftLNode(shl, kit.intcon(63)));\n+        tagged_klass = kit.gvn().transform(new AndLNode(tagged_klass, shr));\n+        ret->init_req(TypeFunc::Parms, tagged_klass);\n+      }\n+      uint idx = TypeFunc::Parms + 1;\n+      vt->pass_fields(&kit, ret, idx, false, false);\n+    } else {\n+      ret->add_req(res);\n+      \/\/ Note:  The second dummy edge is not needed by a ReturnNode.\n+    }\n@@ -1016,1 +1086,1 @@\n-  if (method()->is_object_initializer() &&\n+  if ((method()->is_object_constructor() || method()->is_class_initializer()) &&\n@@ -1044,2 +1114,2 @@\n-  if (tf()->range()->cnt() > TypeFunc::Parms) {\n-    const Type* ret_type = tf()->range()->field_at(TypeFunc::Parms);\n+  if (tf()->range_sig()->cnt() > TypeFunc::Parms) {\n+    const Type* ret_type = tf()->range_sig()->field_at(TypeFunc::Parms);\n@@ -1139,1 +1209,2 @@\n-    kit.null_check_receiver_before_call(method());\n+    Node* receiver = kit.argument(0);\n+    Node* null_free = kit.null_check_receiver_before_call(method());\n@@ -1141,0 +1212,5 @@\n+    if (receiver->is_InlineType() && receiver->as_InlineType()->is_larval()) {\n+      \/\/ Replace the larval inline type receiver in the exit map as well to make sure that\n+      \/\/ we can find and update it in Parse::do_call when we are done with the initialization.\n+      _exits.map()->replace_edge(receiver, null_free);\n+    }\n@@ -1177,1 +1253,1 @@\n-  uint arg_size = tf()->domain()->cnt();\n+  uint arg_size = tf()->domain_sig()->cnt();\n@@ -1251,0 +1327,1 @@\n+      assert(!_gvn.type(lock_obj)->make_oopptr()->can_be_inline_type(), \"can't be an inline type\");\n@@ -1686,0 +1763,36 @@\n+  \/\/ Check for merge conflicts involving inline types\n+  JVMState* old_jvms = map()->jvms();\n+  int old_bci = bci();\n+  JVMState* tmp_jvms = old_jvms->clone_shallow(C);\n+  tmp_jvms->set_should_reexecute(true);\n+  tmp_jvms->bind_map(map());\n+  \/\/ Execution needs to restart a the next bytecode (entry of next\n+  \/\/ block)\n+  if (target->is_merged() ||\n+      pnum > PhiNode::Input ||\n+      target->is_handler() ||\n+      target->is_loop_head()) {\n+    set_parse_bci(target->start());\n+    for (uint j = TypeFunc::Parms; j < map()->req(); j++) {\n+      Node* n = map()->in(j);                 \/\/ Incoming change to target state.\n+      const Type* t = nullptr;\n+      if (tmp_jvms->is_loc(j)) {\n+        t = target->local_type_at(j - tmp_jvms->locoff());\n+      } else if (tmp_jvms->is_stk(j) && j < (uint)sp() + tmp_jvms->stkoff()) {\n+        t = target->stack_type_at(j - tmp_jvms->stkoff());\n+      }\n+      if (t != nullptr && t != Type::BOTTOM) {\n+        if (n->is_InlineType() && !t->is_inlinetypeptr()) {\n+          \/\/ Allocate inline type in src block to be able to merge it with oop in target block\n+          map()->set_req(j, n->as_InlineType()->buffer(this));\n+        } else if (!n->is_InlineType() && t->is_inlinetypeptr()) {\n+          \/\/ Scalarize null in src block to be able to merge it with inline type in target block\n+          assert(gvn().type(n)->is_zero_type(), \"Should have been scalarized\");\n+          map()->set_req(j, InlineTypeNode::make_null(gvn(), t->inline_klass()));\n+        }\n+      }\n+    }\n+  }\n+  old_jvms->bind_map(map());\n+  set_parse_bci(old_bci);\n+\n@@ -1740,0 +1853,1 @@\n+\n@@ -1775,0 +1889,1 @@\n+    bool last_merge = (pnum == PhiNode::Input);\n@@ -1779,1 +1894,1 @@\n-      if (m->is_Phi() && m->as_Phi()->region() == r)\n+      if (m->is_Phi() && m->as_Phi()->region() == r) {\n@@ -1781,1 +1896,3 @@\n-      else\n+      } else if (m->is_InlineType() && m->as_InlineType()->has_phi_inputs(r)) {\n+        phi = m->as_InlineType()->get_oop()->as_Phi();\n+      } else {\n@@ -1783,0 +1900,1 @@\n+      }\n@@ -1829,1 +1947,24 @@\n-      if (phi != nullptr) {\n+      \/\/ Merging two inline types?\n+      if (phi != nullptr && phi->bottom_type()->is_inlinetypeptr()) {\n+        \/\/ Reload current state because it may have been updated by ensure_phi\n+        m = map()->in(j);\n+        InlineTypeNode* vtm = m->as_InlineType(); \/\/ Current inline type\n+        InlineTypeNode* vtn = n->as_InlineType(); \/\/ Incoming inline type\n+        assert(vtm->get_oop() == phi, \"Inline type should have Phi input\");\n+        if (TraceOptoParse) {\n+#ifdef ASSERT\n+          tty->print_cr(\"\\nMerging inline types\");\n+          tty->print_cr(\"Current:\");\n+          vtm->dump(2);\n+          tty->print_cr(\"Incoming:\");\n+          vtn->dump(2);\n+          tty->cr();\n+#endif\n+        }\n+        \/\/ Do the merge\n+        vtm->merge_with(&_gvn, vtn, pnum, last_merge);\n+        if (last_merge) {\n+          map()->set_req(j, _gvn.transform(vtm));\n+          record_for_igvn(vtm);\n+        }\n+      } else if (phi != nullptr) {\n@@ -1833,1 +1974,1 @@\n-        if (pnum == PhiNode::Input) {\n+        if (last_merge) {\n@@ -1849,2 +1990,1 @@\n-    if (pnum == PhiNode::Input &&\n-        !r->in(0)) {         \/\/ The occasional useless Region\n+    if (last_merge && !r->in(0)) {         \/\/ The occasional useless Region\n@@ -2002,0 +2142,2 @@\n+      } else if (n->is_InlineType() && n->as_InlineType()->has_phi_inputs(r)) {\n+        n->as_InlineType()->add_new_path(r);\n@@ -2024,0 +2166,4 @@\n+  InlineTypeNode* vt = o->isa_InlineType();\n+  if (vt != nullptr && vt->has_phi_inputs(region)) {\n+    return vt->get_oop()->as_Phi();\n+  }\n@@ -2043,2 +2189,2 @@\n-  \/\/ is mixing ints and oops or some such.  Forcing it to top\n-  \/\/ makes it go dead.\n+  \/\/ is already dead or is mixing ints and oops or some such.\n+  \/\/ Forcing it to top makes it go dead.\n@@ -2057,5 +2203,14 @@\n-  PhiNode* phi = PhiNode::make(region, o, t);\n-  gvn().set_type(phi, t);\n-  if (C->do_escape_analysis()) record_for_igvn(phi);\n-  map->set_req(idx, phi);\n-  return phi;\n+  if (vt != nullptr && t->is_inlinetypeptr()) {\n+    \/\/ Inline types are merged by merging their field values.\n+    \/\/ Create a cloned InlineTypeNode with phi inputs that\n+    \/\/ represents the merged inline type and update the map.\n+    vt = vt->clone_with_phis(&_gvn, region);\n+    map->set_req(idx, vt);\n+    return vt->get_oop()->as_Phi();\n+  } else {\n+    PhiNode* phi = PhiNode::make(region, o, t);\n+    gvn().set_type(phi, t);\n+    if (C->do_escape_analysis()) record_for_igvn(phi);\n+    map->set_req(idx, phi);\n+    return phi;\n+  }\n@@ -2191,0 +2346,35 @@\n+  \/\/ frame pointer is always same, already captured\n+  if (value != nullptr) {\n+    Node* phi = _exits.argument(0);\n+    const Type* return_type = phi->bottom_type();\n+    const TypeInstPtr* tr = return_type->isa_instptr();\n+    assert(!value->is_InlineType() || !value->as_InlineType()->is_larval(), \"returning a larval\");\n+    if ((tf()->returns_inline_type_as_fields() || (_caller->has_method() && !Compile::current()->inlining_incrementally())) &&\n+        return_type->is_inlinetypeptr()) {\n+      \/\/ Inline type is returned as fields, make sure it is scalarized\n+      if (!value->is_InlineType()) {\n+        value = InlineTypeNode::make_from_oop(this, value, return_type->inline_klass(), false);\n+      }\n+      if (!_caller->has_method() || Compile::current()->inlining_incrementally()) {\n+        \/\/ Returning from root or an incrementally inlined method. Make sure all non-flat\n+        \/\/ fields are buffered and re-execute if allocation triggers deoptimization.\n+        PreserveReexecuteState preexecs(this);\n+        assert(tf()->returns_inline_type_as_fields(), \"must be returned as fields\");\n+        jvms()->set_should_reexecute(true);\n+        inc_sp(1);\n+        value = value->as_InlineType()->allocate_fields(this);\n+      }\n+    } else if (value->is_InlineType()) {\n+      \/\/ Inline type is returned as oop, make sure it is buffered and re-execute\n+      \/\/ if allocation triggers deoptimization.\n+      PreserveReexecuteState preexecs(this);\n+      jvms()->set_should_reexecute(true);\n+      inc_sp(1);\n+      value = value->as_InlineType()->buffer(this);\n+    }\n+    \/\/ ...else\n+    \/\/ If returning oops to an interface-return, there is a silent free\n+    \/\/ cast from oop to interface allowed by the Verifier. Make it explicit here.\n+    phi->add_req(value);\n+  }\n+\n@@ -2199,0 +2389,1 @@\n+\n@@ -2216,9 +2407,0 @@\n-  \/\/ frame pointer is always same, already captured\n-  if (value != nullptr) {\n-    \/\/ If returning oops to an interface-return, there is a silent free\n-    \/\/ cast from oop to interface allowed by the Verifier.  Make it explicit\n-    \/\/ here.\n-    Node* phi = _exits.argument(0);\n-    phi->add_req(value);\n-  }\n-\n","filename":"src\/hotspot\/share\/opto\/parse1.cpp","additions":239,"deletions":57,"binary":false,"changes":296,"status":"modified"},{"patch":"@@ -56,0 +56,1 @@\n+#include \"memory\/iterator.inline.hpp\"\n@@ -65,0 +66,1 @@\n+#include \"oops\/compressedOops.inline.hpp\"\n@@ -72,0 +74,1 @@\n+#include \"oops\/objArrayOop.inline.hpp\"\n@@ -1905,0 +1908,85 @@\n+WB_ENTRY(jobjectArray, WB_getObjectsViaKlassOopMaps(JNIEnv* env, jobject wb, jobject thing))\n+  oop aoop = JNIHandles::resolve(thing);\n+  if (!aoop->is_instance()) {\n+    return nullptr;\n+  }\n+  instanceHandle ih(THREAD, (instanceOop) aoop);\n+  InstanceKlass* klass = InstanceKlass::cast(ih->klass());\n+  if (klass->nonstatic_oop_map_count() == 0) {\n+    return nullptr;\n+  }\n+  const OopMapBlock* map = klass->start_of_nonstatic_oop_maps();\n+  const OopMapBlock* const end = map + klass->nonstatic_oop_map_count();\n+  int oop_count = 0;\n+  while (map < end) {\n+    oop_count += map->count();\n+    map++;\n+  }\n+\n+  objArrayHandle result_array =\n+      oopFactory::new_objArray_handle(vmClasses::Object_klass(), oop_count, CHECK_NULL);\n+  map = klass->start_of_nonstatic_oop_maps();\n+  int index = 0;\n+  while (map < end) {\n+    int offset = map->offset();\n+    for (unsigned int j = 0; j < map->count(); j++) {\n+      result_array->obj_at_put(index++, ih->obj_field(offset));\n+      offset += heapOopSize;\n+    }\n+    map++;\n+  }\n+  return (jobjectArray)JNIHandles::make_local(THREAD, result_array());\n+WB_END\n+\n+class CollectOops : public BasicOopIterateClosure {\n+ public:\n+  GrowableArray<Handle>* array;\n+\n+  jobjectArray create_jni_result(JNIEnv* env, TRAPS) {\n+    objArrayHandle result_array =\n+        oopFactory::new_objArray_handle(vmClasses::Object_klass(), array->length(), CHECK_NULL);\n+    for (int i = 0 ; i < array->length(); i++) {\n+      result_array->obj_at_put(i, array->at(i)());\n+    }\n+    return (jobjectArray)JNIHandles::make_local(THREAD, result_array());\n+  }\n+\n+  void add_oop(oop o) {\n+    Handle oh = Handle(Thread::current(), o);\n+    \/\/ Value might be oop, but JLS can't see as Object, just iterate through it...\n+    if (oh != nullptr && oh->is_inline_type()) {\n+      oh->oop_iterate(this);\n+    } else {\n+      array->append(oh);\n+    }\n+  }\n+\n+  void do_oop(oop* o) { add_oop(HeapAccess<>::oop_load(o)); }\n+  void do_oop(narrowOop* v) { add_oop(HeapAccess<>::oop_load(v)); }\n+};\n+\n+WB_ENTRY(jobjectArray, WB_getObjectsViaOopIterator(JNIEnv* env, jobject wb, jobject thing))\n+  ResourceMark rm(thread);\n+  Handle objh(thread, JNIHandles::resolve(thing));\n+  GrowableArray<Handle>* array = new GrowableArray<Handle>(128);\n+  CollectOops collectOops;\n+  collectOops.array = array;\n+  objh->oop_iterate(&collectOops);\n+  return collectOops.create_jni_result(env, THREAD);\n+WB_END\n+\n+WB_ENTRY(jobjectArray, WB_getObjectsViaFrameOopIterator(JNIEnv* env, jobject wb, jint depth))\n+  ResourceMark rm(THREAD);\n+  GrowableArray<Handle>* array = new GrowableArray<Handle>(128);\n+  CollectOops collectOops;\n+  collectOops.array = array;\n+  StackFrameStream sfs(thread, false \/* update *\/, true \/* process_frames *\/);\n+  while (depth > 0) { \/\/ Skip the native WB API frame\n+    sfs.next();\n+    frame* f = sfs.current();\n+    f->oops_do(&collectOops, nullptr, sfs.register_map());\n+    depth--;\n+  }\n+  return collectOops.create_jni_result(env, THREAD);\n+WB_END\n+\n@@ -2878,0 +2966,6 @@\n+  {CC\"getObjectsViaKlassOopMaps0\",\n+      CC\"(Ljava\/lang\/Object;)[Ljava\/lang\/Object;\",    (void*)&WB_getObjectsViaKlassOopMaps},\n+  {CC\"getObjectsViaOopIterator0\",\n+          CC\"(Ljava\/lang\/Object;)[Ljava\/lang\/Object;\",(void*)&WB_getObjectsViaOopIterator},\n+  {CC\"getObjectsViaFrameOopIterator\",\n+      CC\"(I)[Ljava\/lang\/Object;\",                     (void*)&WB_getObjectsViaFrameOopIterator},\n","filename":"src\/hotspot\/share\/prims\/whitebox.cpp","additions":94,"deletions":0,"binary":false,"changes":94,"status":"modified"},{"patch":"@@ -47,0 +47,1 @@\n+#include \"memory\/oopFactory.hpp\"\n@@ -49,0 +50,2 @@\n+#include \"oops\/access.hpp\"\n+#include \"oops\/fieldStreams.inline.hpp\"\n@@ -53,0 +56,1 @@\n+#include \"oops\/objArrayOop.inline.hpp\"\n@@ -54,0 +58,1 @@\n+#include \"oops\/inlineKlass.inline.hpp\"\n@@ -1190,0 +1195,15 @@\n+  \/\/ Substitutability test implementation piggy backs on static call resolution\n+  Bytecodes::Code code = caller->java_code_at(bci);\n+  if (code == Bytecodes::_if_acmpeq || code == Bytecodes::_if_acmpne) {\n+    bc = Bytecodes::_invokestatic;\n+    methodHandle attached_method(THREAD, extract_attached_method(vfst));\n+    assert(attached_method.not_null(), \"must have attached method\");\n+    vmClasses::ValueObjectMethods_klass()->initialize(CHECK_NH);\n+    LinkResolver::resolve_invoke(callinfo, receiver, attached_method, bc, false, CHECK_NH);\n+#ifdef ASSERT\n+    Method* is_subst = vmClasses::ValueObjectMethods_klass()->find_method(vmSymbols::isSubstitutable_name(), vmSymbols::object_object_boolean_signature());\n+    assert(callinfo.selected_method() == is_subst, \"must be isSubstitutable method\");\n+#endif\n+    return receiver;\n+  }\n+\n@@ -1225,0 +1245,6 @@\n+    } else {\n+      assert(attached_method->has_scalarized_args(), \"invalid use of attached method\");\n+      if (!attached_method->method_holder()->is_inline_klass()) {\n+        \/\/ Ignore the attached method in this case to not confuse below code\n+        attached_method = methodHandle(current, nullptr);\n+      }\n@@ -1233,0 +1259,1 @@\n+  bool check_null_and_abstract = true;\n@@ -1246,2 +1273,3 @@\n-    if (attached_method.is_null()) {\n-      Method* callee = bytecode.static_target(CHECK_NH);\n+    Method* callee = attached_method();\n+    if (callee == nullptr) {\n+      callee = bytecode.static_target(CHECK_NH);\n@@ -1252,7 +1280,17 @@\n-\n-    \/\/ Retrieve from a compiled argument list\n-    receiver = Handle(current, callerFrame.retrieve_receiver(&reg_map2));\n-    assert(oopDesc::is_oop_or_null(receiver()), \"\");\n-\n-    if (receiver.is_null()) {\n-      THROW_(vmSymbols::java_lang_NullPointerException(), nullHandle);\n+    bool caller_is_c1 = callerFrame.is_compiled_frame() && callerFrame.cb()->as_nmethod()->is_compiled_by_c1();\n+    if (!caller_is_c1 && callee->is_scalarized_arg(0)) {\n+      \/\/ If the receiver is an inline type that is passed as fields, no oop is available\n+      \/\/ Resolve the call without receiver null checking.\n+      assert(!callee->mismatch(), \"calls with inline type receivers should never mismatch\");\n+      assert(attached_method.not_null() && !attached_method->is_abstract(), \"must have non-abstract attached method\");\n+      if (bc == Bytecodes::_invokeinterface) {\n+        bc = Bytecodes::_invokevirtual; \/\/ C2 optimistically replaces interface calls by virtual calls\n+      }\n+      check_null_and_abstract = false;\n+    } else {\n+      \/\/ Retrieve from a compiled argument list\n+      receiver = Handle(current, callerFrame.retrieve_receiver(&reg_map2));\n+      assert(oopDesc::is_oop_or_null(receiver()), \"\");\n+      if (receiver.is_null()) {\n+        THROW_(vmSymbols::java_lang_NullPointerException(), nullHandle);\n+      }\n@@ -1265,1 +1303,1 @@\n-    LinkResolver::resolve_invoke(callinfo, receiver, attached_method, bc, CHECK_NH);\n+    LinkResolver::resolve_invoke(callinfo, receiver, attached_method, bc, check_null_and_abstract, CHECK_NH);\n@@ -1274,1 +1312,1 @@\n-  if (has_receiver) {\n+  if (has_receiver && check_null_and_abstract) {\n@@ -1302,1 +1340,1 @@\n-methodHandle SharedRuntime::find_callee_method(TRAPS) {\n+methodHandle SharedRuntime::find_callee_method(bool is_optimized, bool& caller_is_c1, TRAPS) {\n@@ -1328,0 +1366,4 @@\n+    \/\/ Calls via mismatching methods are always non-scalarized\n+    if (callinfo.resolved_method()->mismatch() && !is_optimized) {\n+      caller_is_c1 = true;\n+    }\n@@ -1335,1 +1377,1 @@\n-methodHandle SharedRuntime::resolve_helper(bool is_virtual, bool is_optimized, TRAPS) {\n+methodHandle SharedRuntime::resolve_helper(bool is_virtual, bool is_optimized, bool& caller_is_c1, TRAPS) {\n@@ -1358,0 +1400,4 @@\n+  \/\/ Calls via mismatching methods are always non-scalarized\n+  if (caller_nm->is_compiled_by_c1() || (call_info.resolved_method()->mismatch() && !is_optimized)) {\n+    caller_is_c1 = true;\n+  }\n@@ -1376,1 +1422,1 @@\n-    tty->print(\"resolving %s%s (%s) call to\",\n+    tty->print(\"resolving %s%s (%s) call%s to\",\n@@ -1378,1 +1424,1 @@\n-               Bytecodes::name(invoke_code));\n+               Bytecodes::name(invoke_code), (caller_is_c1) ? \" from C1\" : \"\");\n@@ -1417,1 +1463,1 @@\n-    inline_cache->update(&call_info, receiver->klass());\n+    inline_cache->update(&call_info, receiver->klass(), caller_is_c1);\n@@ -1421,1 +1467,1 @@\n-    callsite->set(callee_method);\n+    callsite->set(callee_method, caller_is_c1);\n@@ -1441,0 +1487,2 @@\n+  bool is_optimized = false;\n+  bool caller_is_c1 = false;\n@@ -1442,1 +1490,1 @@\n-    callee_method = SharedRuntime::handle_ic_miss_helper(CHECK_NULL);\n+    callee_method = SharedRuntime::handle_ic_miss_helper(is_optimized, caller_is_c1, CHECK_NULL);\n@@ -1447,1 +1495,1 @@\n-  return get_resolved_entry(current, callee_method);\n+  return get_resolved_entry(current, callee_method, false, is_optimized, caller_is_c1);\n@@ -1488,1 +1536,5 @@\n-      return callee->get_c2i_entry();\n+      if (caller_frame.is_interpreted_frame()) {\n+        return callee->get_c2i_inline_entry();\n+      } else {\n+        return callee->get_c2i_entry();\n+      }\n@@ -1494,0 +1546,3 @@\n+  bool is_static_call = false;\n+  bool is_optimized = false;\n+  bool caller_is_c1 = false;\n@@ -1496,1 +1551,1 @@\n-    callee_method = SharedRuntime::reresolve_call_site(CHECK_NULL);\n+    callee_method = SharedRuntime::reresolve_call_site(is_static_call, is_optimized, caller_is_c1, CHECK_NULL);\n@@ -1500,1 +1555,1 @@\n-  return get_resolved_entry(current, callee_method);\n+  return get_resolved_entry(current, callee_method, is_static_call, is_optimized, caller_is_c1);\n@@ -1539,1 +1594,2 @@\n-address SharedRuntime::get_resolved_entry(JavaThread* current, methodHandle callee_method) {\n+address SharedRuntime::get_resolved_entry(JavaThread* current, methodHandle callee_method,\n+                                          bool is_static_call, bool is_optimized, bool caller_is_c1) {\n@@ -1545,2 +1601,11 @@\n-  assert(callee_method->verified_code_entry() != nullptr, \" Jump to zero!\");\n-  return callee_method->verified_code_entry();\n+\n+  if (caller_is_c1) {\n+    assert(callee_method->verified_inline_code_entry() != nullptr, \"Jump to zero!\");\n+    return callee_method->verified_inline_code_entry();\n+  } else if (is_static_call || is_optimized) {\n+    assert(callee_method->verified_code_entry() != nullptr, \"Jump to zero!\");\n+    return callee_method->verified_code_entry();\n+  } else {\n+    assert(callee_method->verified_inline_ro_code_entry() != nullptr, \"Jump to zero!\");\n+    return callee_method->verified_inline_ro_code_entry();\n+  }\n@@ -1552,0 +1617,1 @@\n+  bool caller_is_c1 = false;\n@@ -1554,1 +1620,1 @@\n-    callee_method = SharedRuntime::resolve_helper(false, false, CHECK_NULL);\n+    callee_method = SharedRuntime::resolve_helper(false, false, caller_is_c1, CHECK_NULL);\n@@ -1558,1 +1624,1 @@\n-  return get_resolved_entry(current, callee_method);\n+  return get_resolved_entry(current, callee_method, true, false, caller_is_c1);\n@@ -1564,0 +1630,1 @@\n+  bool caller_is_c1 = false;\n@@ -1565,1 +1632,1 @@\n-    callee_method = SharedRuntime::resolve_helper(true, false, CHECK_NULL);\n+    callee_method = SharedRuntime::resolve_helper(true, false, caller_is_c1, CHECK_NULL);\n@@ -1569,1 +1636,1 @@\n-  return get_resolved_entry(current, callee_method);\n+  return get_resolved_entry(current, callee_method, false, false, caller_is_c1);\n@@ -1577,0 +1644,1 @@\n+  bool caller_is_c1 = false;\n@@ -1578,1 +1646,1 @@\n-    callee_method = SharedRuntime::resolve_helper(true, true, CHECK_NULL);\n+    callee_method = SharedRuntime::resolve_helper(true, true, caller_is_c1, CHECK_NULL);\n@@ -1582,1 +1650,1 @@\n-  return get_resolved_entry(current, callee_method);\n+  return get_resolved_entry(current, callee_method, false, true, caller_is_c1);\n@@ -1585,1 +1653,3 @@\n-methodHandle SharedRuntime::handle_ic_miss_helper(TRAPS) {\n+\n+\n+methodHandle SharedRuntime::handle_ic_miss_helper(bool& is_optimized, bool& caller_is_c1, TRAPS) {\n@@ -1603,1 +1673,1 @@\n-    tty->print(\"IC miss (%s) call to\", Bytecodes::name(bc));\n+    tty->print(\"IC miss (%s) call%s to\", Bytecodes::name(bc), (caller_is_c1) ? \" from C1\" : \"\");\n@@ -1635,0 +1705,4 @@\n+  \/\/ Calls via mismatching methods are always non-scalarized\n+  if (caller_nm->is_compiled_by_c1() || call_info.resolved_method()->mismatch()) {\n+    caller_is_c1 = true;\n+  }\n@@ -1638,1 +1712,1 @@\n-  inline_cache->update(&call_info, receiver()->klass());\n+  inline_cache->update(&call_info, receiver()->klass(), caller_is_c1);\n@@ -1649,1 +1723,1 @@\n-methodHandle SharedRuntime::reresolve_call_site(TRAPS) {\n+methodHandle SharedRuntime::reresolve_call_site(bool& is_static_call, bool& is_optimized, bool& caller_is_c1, TRAPS) {\n@@ -1659,0 +1733,3 @@\n+  if (caller.is_compiled_frame()) {\n+    caller_is_c1 = caller.cb()->as_nmethod()->is_compiled_by_c1();\n+  }\n@@ -1699,0 +1776,2 @@\n+        is_static_call = false;\n+        is_optimized = false;\n@@ -1701,0 +1780,1 @@\n+            is_static_call = true;\n@@ -1702,0 +1782,1 @@\n+            is_optimized = (iter.type() == relocInfo::opt_virtual_call_type);\n@@ -1706,1 +1787,0 @@\n-\n@@ -1720,2 +1800,1 @@\n-  methodHandle callee_method = find_callee_method(CHECK_(methodHandle()));\n-\n+  methodHandle callee_method = find_callee_method(is_optimized, caller_is_c1, CHECK_(methodHandle()));\n@@ -1728,1 +1807,1 @@\n-    tty->print(\"handle_wrong_method reresolving call to\");\n+    tty->print(\"handle_wrong_method reresolving call%s to\", (caller_is_c1) ? \" from C1\" : \"\");\n@@ -1934,0 +2013,15 @@\n+char* SharedRuntime::generate_identity_exception_message(JavaThread* current, Klass* klass) {\n+  assert(klass->is_inline_klass(), \"Must be a concrete value class\");\n+  const char* desc = \"Cannot synchronize on an instance of value class \";\n+  const char* className = klass->external_name();\n+  size_t msglen = strlen(desc) + strlen(className) + 1;\n+  char* message = NEW_RESOURCE_ARRAY(char, msglen);\n+  if (nullptr == message) {\n+    \/\/ Out of memory: can't create detailed error message\n+    message = const_cast<char*>(klass->external_name());\n+  } else {\n+    jio_snprintf(message, msglen, \"%s%s\", desc, className);\n+  }\n+  return message;\n+}\n+\n@@ -2164,1 +2258,1 @@\n-    _basic_type_bits = 4,\n+    _basic_type_bits = 5,\n@@ -2182,1 +2276,1 @@\n-  static int adapter_encoding(BasicType in) {\n+  static BasicType adapter_encoding(BasicType in) {\n@@ -2188,1 +2282,1 @@\n-        \/\/ There are all promoted to T_INT in the calling convention\n+        \/\/ They are all promoted to T_INT in the calling convention\n@@ -2215,1 +2309,1 @@\n-  AdapterFingerPrint(int total_args_passed, BasicType* sig_bt) {\n+  AdapterFingerPrint(const GrowableArray<SigEntry>* sig, bool has_ro_adapter = false) {\n@@ -2218,0 +2312,1 @@\n+    int total_args_passed = (sig != nullptr) ? sig->length() : 0;\n@@ -2235,0 +2330,2 @@\n+    BasicType prev_bt = T_ILLEGAL;\n+    int vt_count = 0;\n@@ -2237,4 +2334,27 @@\n-      for (int byte = 0; sig_index < total_args_passed && byte < _basic_types_per_int; byte++) {\n-        int bt = adapter_encoding(sig_bt[sig_index++]);\n-        assert((bt & _basic_type_mask) == bt, \"must fit in 4 bits\");\n-        value = (value << _basic_type_bits) | bt;\n+      for (int byte = 0; byte < _basic_types_per_int; byte++) {\n+        BasicType bt = T_ILLEGAL;\n+        if (sig_index < total_args_passed) {\n+          bt = sig->at(sig_index++)._bt;\n+          if (bt == T_METADATA) {\n+            \/\/ Found start of inline type in signature\n+            assert(InlineTypePassFieldsAsArgs, \"unexpected start of inline type\");\n+            if (sig_index == 1 && has_ro_adapter) {\n+              \/\/ With a ro_adapter, replace receiver inline type delimiter by T_VOID to prevent matching\n+              \/\/ with other adapters that have the same inline type as first argument and no receiver.\n+              bt = T_VOID;\n+            }\n+            vt_count++;\n+          } else if (bt == T_VOID && prev_bt != T_LONG && prev_bt != T_DOUBLE) {\n+            \/\/ Found end of inline type in signature\n+            assert(InlineTypePassFieldsAsArgs, \"unexpected end of inline type\");\n+            vt_count--;\n+            assert(vt_count >= 0, \"invalid vt_count\");\n+          } else if (vt_count == 0) {\n+            \/\/ Widen fields that are not part of a scalarized inline type argument\n+            bt = adapter_encoding(bt);\n+          }\n+          prev_bt = bt;\n+        }\n+        int bt_val = (bt == T_ILLEGAL) ? 0 : bt;\n+        assert((bt_val & _basic_type_mask) == bt_val, \"must fit in 4 bits\");\n+        value = (value << _basic_type_bits) | bt_val;\n@@ -2244,0 +2364,1 @@\n+    assert(vt_count == 0, \"invalid vt_count\");\n@@ -2308,8 +2429,4 @@\n-        }\n-        switch (v) {\n-          case T_INT:    st.print(\"I\");    break;\n-          case T_LONG:   long_prev = true; break;\n-          case T_FLOAT:  st.print(\"F\");    break;\n-          case T_DOUBLE: st.print(\"D\");    break;\n-          case T_VOID:   break;\n-          default: ShouldNotReachHere();\n+        } else if (v == T_LONG) {\n+          long_prev = true;\n+        } else if (v != T_VOID){\n+          st.print(\"%c\", type2char((BasicType)v));\n@@ -2363,1 +2480,1 @@\n-static AdapterHandlerEntry* lookup(int total_args_passed, BasicType* sig_bt) {\n+static AdapterHandlerEntry* lookup(const GrowableArray<SigEntry>* sig, bool has_ro_adapter = false) {\n@@ -2366,1 +2483,1 @@\n-  AdapterFingerPrint fp(total_args_passed, sig_bt);\n+  AdapterFingerPrint fp(sig, has_ro_adapter);\n@@ -2400,1 +2517,1 @@\n-const int AdapterHandlerLibrary_size = 16*K;\n+const int AdapterHandlerLibrary_size = 48*K;\n@@ -2443,1 +2560,1 @@\n-    _abstract_method_handler = AdapterHandlerLibrary::new_entry(new AdapterFingerPrint(0, nullptr),\n+    _abstract_method_handler = AdapterHandlerLibrary::new_entry(new AdapterFingerPrint(nullptr),\n@@ -2445,0 +2562,1 @@\n+                                                                wrong_method_abstract, wrong_method_abstract, wrong_method_abstract,\n@@ -2446,4 +2564,8 @@\n-\n-    _no_arg_handler = create_adapter(no_arg_blob, 0, nullptr, true);\n-    BasicType obj_args[] = { T_OBJECT };\n-    _obj_arg_handler = create_adapter(obj_arg_blob, 1, obj_args, true);\n+    CompiledEntrySignature no_args;\n+    no_args.compute_calling_conventions();\n+    _no_arg_handler = create_adapter(no_arg_blob, no_args, true);\n+\n+    CompiledEntrySignature obj_args;\n+    SigEntry::add_entry(obj_args.sig(), T_OBJECT, nullptr);\n+    obj_args.compute_calling_conventions();\n+    _obj_arg_handler = create_adapter(obj_arg_blob, obj_args, true);\n@@ -2453,2 +2575,4 @@\n-    BasicType int_args[] = { T_INT };\n-    _int_arg_handler = create_adapter(int_arg_blob, 1, int_args, true);\n+    CompiledEntrySignature int_args;\n+    SigEntry::add_entry(int_args.sig(), T_INT, nullptr);\n+    int_args.compute_calling_conventions();\n+    _int_arg_handler = create_adapter(int_arg_blob, int_args, true);\n@@ -2456,2 +2580,5 @@\n-    BasicType obj_int_args[] = { T_OBJECT, T_INT };\n-    _obj_int_arg_handler = create_adapter(obj_int_arg_blob, 2, obj_int_args, true);\n+    CompiledEntrySignature obj_int_args;\n+    SigEntry::add_entry(obj_int_args.sig(), T_OBJECT, nullptr);\n+    SigEntry::add_entry(obj_int_args.sig(), T_INT, nullptr);\n+    obj_int_args.compute_calling_conventions();\n+    _obj_int_arg_handler = create_adapter(obj_int_arg_blob, obj_int_args, true);\n@@ -2459,2 +2586,5 @@\n-    BasicType obj_obj_args[] = { T_OBJECT, T_OBJECT };\n-    _obj_obj_arg_handler = create_adapter(obj_obj_arg_blob, 2, obj_obj_args, true);\n+    CompiledEntrySignature obj_obj_args;\n+    SigEntry::add_entry(obj_obj_args.sig(), T_OBJECT, nullptr);\n+    SigEntry::add_entry(obj_obj_args.sig(), T_OBJECT, nullptr);\n+    obj_obj_args.compute_calling_conventions();\n+    _obj_obj_arg_handler = create_adapter(obj_obj_arg_blob, obj_obj_args, true);\n@@ -2468,0 +2598,1 @@\n+  return;\n@@ -2480,0 +2611,2 @@\n+                                                      address c2i_inline_entry,\n+                                                      address c2i_inline_ro_entry,\n@@ -2481,0 +2614,1 @@\n+                                                      address c2i_unverified_inline_entry,\n@@ -2482,3 +2616,2 @@\n-  \/\/ Insert an entry into the table\n-  return new AdapterHandlerEntry(fingerprint, i2c_entry, c2i_entry, c2i_unverified_entry,\n-                                 c2i_no_clinit_check_entry);\n+  return new AdapterHandlerEntry(fingerprint, i2c_entry, c2i_entry, c2i_inline_entry, c2i_inline_ro_entry, c2i_unverified_entry,\n+                              c2i_unverified_inline_entry, c2i_no_clinit_check_entry);\n@@ -2489,1 +2622,1 @@\n-    return _abstract_method_handler;\n+    return nullptr;\n@@ -2496,0 +2629,3 @@\n+      if (InlineTypePassFieldsAsArgs && method->method_holder()->is_inline_klass()) {\n+        return nullptr;\n+      }\n@@ -2499,1 +2635,10 @@\n-      case JVM_SIGNATURE_CLASS:\n+      case JVM_SIGNATURE_CLASS: {\n+        if (InlineTypePassFieldsAsArgs) {\n+          SignatureStream ss(method->signature());\n+          InlineKlass* vk = ss.as_inline_klass(method->method_holder());\n+          if (vk != nullptr) {\n+            return nullptr;\n+          }\n+        }\n+        return _obj_arg_handler;\n+      }\n@@ -2510,1 +2655,1 @@\n-             !method->is_static()) {\n+             !method->is_static() && (!InlineTypePassFieldsAsArgs || !method->method_holder()->is_inline_klass())) {\n@@ -2512,1 +2657,10 @@\n-      case JVM_SIGNATURE_CLASS:\n+      case JVM_SIGNATURE_CLASS: {\n+        if (InlineTypePassFieldsAsArgs) {\n+          SignatureStream ss(method->signature());\n+          InlineKlass* vk = ss.as_inline_klass(method->method_holder());\n+          if (vk != nullptr) {\n+            return nullptr;\n+          }\n+        }\n+        return _obj_obj_arg_handler;\n+      }\n@@ -2526,5 +2680,9 @@\n-class AdapterSignatureIterator : public SignatureIterator {\n- private:\n-  BasicType stack_sig_bt[16];\n-  BasicType* sig_bt;\n-  int index;\n+CompiledEntrySignature::CompiledEntrySignature(Method* method) :\n+  _method(method), _num_inline_args(0), _has_inline_recv(false),\n+  _regs(nullptr), _regs_cc(nullptr), _regs_cc_ro(nullptr),\n+  _args_on_stack(0), _args_on_stack_cc(0), _args_on_stack_cc_ro(0),\n+  _c1_needs_stack_repair(false), _c2_needs_stack_repair(false), _supers(nullptr) {\n+  _sig = new GrowableArray<SigEntry>((method != nullptr) ? method->size_of_parameters() : 1);\n+  _sig_cc = new GrowableArray<SigEntry>((method != nullptr) ? method->size_of_parameters() : 1);\n+  _sig_cc_ro = new GrowableArray<SigEntry>((method != nullptr) ? method->size_of_parameters() : 1);\n+}\n@@ -2532,11 +2690,24 @@\n- public:\n-  AdapterSignatureIterator(Symbol* signature,\n-                           fingerprint_t fingerprint,\n-                           bool is_static,\n-                           int total_args_passed) :\n-    SignatureIterator(signature, fingerprint),\n-    index(0)\n-  {\n-    sig_bt = (total_args_passed <= 16) ? stack_sig_bt : NEW_RESOURCE_ARRAY(BasicType, total_args_passed);\n-    if (!is_static) { \/\/ Pass in receiver first\n-      sig_bt[index++] = T_OBJECT;\n+\/\/ See if we can save space by sharing the same entry for VIEP and VIEP(RO),\n+\/\/ or the same entry for VEP and VIEP(RO).\n+CodeOffsets::Entries CompiledEntrySignature::c1_inline_ro_entry_type() const {\n+  if (!has_scalarized_args()) {\n+    \/\/ VEP\/VIEP\/VIEP(RO) all share the same entry. There's no packing.\n+    return CodeOffsets::Verified_Entry;\n+  }\n+  if (_method->is_static()) {\n+    \/\/ Static methods don't need VIEP(RO)\n+    return CodeOffsets::Verified_Entry;\n+  }\n+\n+  if (has_inline_recv()) {\n+    if (num_inline_args() == 1) {\n+      \/\/ Share same entry for VIEP and VIEP(RO).\n+      \/\/ This is quite common: we have an instance method in an InlineKlass that has\n+      \/\/ no inline type args other than <this>.\n+      return CodeOffsets::Verified_Inline_Entry;\n+    } else {\n+      assert(num_inline_args() > 1, \"must be\");\n+      \/\/ No sharing:\n+      \/\/   VIEP(RO) -- <this> is passed as object\n+      \/\/   VEP      -- <this> is passed as fields\n+      return CodeOffsets::Verified_Inline_Entry_RO;\n@@ -2544,1 +2715,0 @@\n-    do_parameters_on(this);\n@@ -2547,2 +2717,9 @@\n-  BasicType* basic_types() {\n-    return sig_bt;\n+  \/\/ Either a static method, or <this> is not an inline type\n+  if (args_on_stack_cc() != args_on_stack_cc_ro()) {\n+    \/\/ No sharing:\n+    \/\/ Some arguments are passed on the stack, and we have inserted reserved entries\n+    \/\/ into the VEP, but we never insert reserved entries into the VIEP(RO).\n+    return CodeOffsets::Verified_Inline_Entry_RO;\n+  } else {\n+    \/\/ Share same entry for VEP and VIEP(RO).\n+    return CodeOffsets::Verified_Entry;\n@@ -2550,0 +2727,1 @@\n+}\n@@ -2551,3 +2729,39 @@\n-#ifdef ASSERT\n-  int slots() {\n-    return index;\n+\/\/ Returns all super methods (transitive) in classes and interfaces that are overridden by the current method.\n+GrowableArray<Method*>* CompiledEntrySignature::get_supers() {\n+  if (_supers != nullptr) {\n+    return _supers;\n+  }\n+  _supers = new GrowableArray<Method*>();\n+  \/\/ Skip private, static, and <init> methods\n+  if (_method->is_private() || _method->is_static() || _method->is_object_constructor()) {\n+    return _supers;\n+  }\n+  Symbol* name = _method->name();\n+  Symbol* signature = _method->signature();\n+  const Klass* holder = _method->method_holder()->super();\n+  Symbol* holder_name = holder->name();\n+  ThreadInVMfromUnknown tiv;\n+  JavaThread* current = JavaThread::current();\n+  HandleMark hm(current);\n+  Handle loader(current, _method->method_holder()->class_loader());\n+\n+  \/\/ Walk up the class hierarchy and search for super methods\n+  while (holder != nullptr) {\n+    Method* super_method = holder->lookup_method(name, signature);\n+    if (super_method == nullptr) {\n+      break;\n+    }\n+    if (!super_method->is_static() && !super_method->is_private() &&\n+        (!super_method->is_package_private() ||\n+         super_method->method_holder()->is_same_class_package(loader(), holder_name))) {\n+      _supers->push(super_method);\n+    }\n+    holder = super_method->method_holder()->super();\n+  }\n+  \/\/ Search interfaces for super methods\n+  Array<InstanceKlass*>* interfaces = _method->method_holder()->transitive_interfaces();\n+  for (int i = 0; i < interfaces->length(); ++i) {\n+    Method* m = interfaces->at(i)->lookup_method(name, signature);\n+    if (m != nullptr && !m->is_static() && m->is_public()) {\n+      _supers->push(m);\n+    }\n@@ -2555,0 +2769,50 @@\n+  return _supers;\n+}\n+\n+\/\/ Iterate over arguments and compute scalarized and non-scalarized signatures\n+void CompiledEntrySignature::compute_calling_conventions(bool init) {\n+  bool has_scalarized = false;\n+  if (_method != nullptr) {\n+    InstanceKlass* holder = _method->method_holder();\n+    int arg_num = 0;\n+    if (!_method->is_static()) {\n+      \/\/ We shouldn't scalarize 'this' in a value class constructor\n+      if (holder->is_inline_klass() && InlineKlass::cast(holder)->can_be_passed_as_fields() && !_method->is_object_constructor() &&\n+          (init || _method->is_scalarized_arg(arg_num))) {\n+        _sig_cc->appendAll(InlineKlass::cast(holder)->extended_sig());\n+        has_scalarized = true;\n+        _has_inline_recv = true;\n+        _num_inline_args++;\n+      } else {\n+        SigEntry::add_entry(_sig_cc, T_OBJECT, holder->name());\n+      }\n+      SigEntry::add_entry(_sig, T_OBJECT, holder->name());\n+      SigEntry::add_entry(_sig_cc_ro, T_OBJECT, holder->name());\n+      arg_num++;\n+    }\n+    for (SignatureStream ss(_method->signature()); !ss.at_return_type(); ss.next()) {\n+      BasicType bt = ss.type();\n+      if (bt == T_OBJECT) {\n+        InlineKlass* vk = ss.as_inline_klass(holder);\n+        if (vk != nullptr && vk->can_be_passed_as_fields() && (init || _method->is_scalarized_arg(arg_num))) {\n+          \/\/ Check for a calling convention mismatch with super method(s)\n+          bool scalar_super = false;\n+          bool non_scalar_super = false;\n+          GrowableArray<Method*>* supers = get_supers();\n+          for (int i = 0; i < supers->length(); ++i) {\n+            Method* super_method = supers->at(i);\n+            if (super_method->is_scalarized_arg(arg_num)) {\n+              scalar_super = true;\n+            } else {\n+              non_scalar_super = true;\n+            }\n+          }\n+#ifdef ASSERT\n+          \/\/ Randomly enable below code paths for stress testing\n+          bool stress = init && StressCallingConvention;\n+          if (stress && (os::random() & 1) == 1) {\n+            non_scalar_super = true;\n+            if ((os::random() & 1) == 1) {\n+              scalar_super = true;\n+            }\n+          }\n@@ -2556,0 +2820,50 @@\n+          if (non_scalar_super) {\n+            \/\/ Found a super method with a non-scalarized argument. Fall back to the non-scalarized calling convention.\n+            if (scalar_super) {\n+              \/\/ Found non-scalar *and* scalar super methods. We can't handle both.\n+              \/\/ Mark the scalar method as mismatch and re-compile call sites to use non-scalarized calling convention.\n+              for (int i = 0; i < supers->length(); ++i) {\n+                Method* super_method = supers->at(i);\n+                if (super_method->is_scalarized_arg(arg_num) debug_only(|| (stress && (os::random() & 1) == 1))) {\n+                  super_method->set_mismatch();\n+                  MutexLocker ml(Compile_lock, Mutex::_safepoint_check_flag);\n+                  JavaThread* thread = JavaThread::current();\n+                  HandleMark hm(thread);\n+                  methodHandle mh(thread, super_method);\n+                  DeoptimizationScope deopt_scope;\n+                  CodeCache::mark_for_deoptimization(&deopt_scope, mh());\n+                  deopt_scope.deoptimize_marked();\n+                }\n+              }\n+            }\n+            \/\/ Fall back to non-scalarized calling convention\n+            SigEntry::add_entry(_sig_cc, T_OBJECT, ss.as_symbol());\n+            SigEntry::add_entry(_sig_cc_ro, T_OBJECT, ss.as_symbol());\n+          } else {\n+            _num_inline_args++;\n+            has_scalarized = true;\n+            int last = _sig_cc->length();\n+            int last_ro = _sig_cc_ro->length();\n+            _sig_cc->appendAll(vk->extended_sig());\n+            _sig_cc_ro->appendAll(vk->extended_sig());\n+            if (bt == T_OBJECT) {\n+              \/\/ Nullable inline type argument, insert InlineTypeNode::IsInit field right after T_METADATA delimiter\n+              _sig_cc->insert_before(last+1, SigEntry(T_BOOLEAN, -1, nullptr));\n+              _sig_cc_ro->insert_before(last_ro+1, SigEntry(T_BOOLEAN, -1, nullptr));\n+            }\n+          }\n+        } else {\n+          SigEntry::add_entry(_sig_cc, T_OBJECT, ss.as_symbol());\n+          SigEntry::add_entry(_sig_cc_ro, T_OBJECT, ss.as_symbol());\n+        }\n+        bt = T_OBJECT;\n+      } else {\n+        SigEntry::add_entry(_sig_cc, ss.type(), ss.as_symbol());\n+        SigEntry::add_entry(_sig_cc_ro, ss.type(), ss.as_symbol());\n+      }\n+      SigEntry::add_entry(_sig, bt, ss.as_symbol());\n+      if (bt != T_VOID) {\n+        arg_num++;\n+      }\n+    }\n+  }\n@@ -2557,1 +2871,11 @@\n- private:\n+  \/\/ Compute the non-scalarized calling convention\n+  _regs = NEW_RESOURCE_ARRAY(VMRegPair, _sig->length());\n+  _args_on_stack = SharedRuntime::java_calling_convention(_sig, _regs);\n+\n+  \/\/ Compute the scalarized calling conventions if there are scalarized inline types in the signature\n+  if (has_scalarized && !_method->is_native()) {\n+    _regs_cc = NEW_RESOURCE_ARRAY(VMRegPair, _sig_cc->length());\n+    _args_on_stack_cc = SharedRuntime::java_calling_convention(_sig_cc, _regs_cc);\n+\n+    _regs_cc_ro = NEW_RESOURCE_ARRAY(VMRegPair, _sig_cc_ro->length());\n+    _args_on_stack_cc_ro = SharedRuntime::java_calling_convention(_sig_cc_ro, _regs_cc_ro);\n@@ -2559,5 +2883,8 @@\n-  friend class SignatureIterator;  \/\/ so do_parameters_on can call do_type\n-  void do_type(BasicType type) {\n-    sig_bt[index++] = type;\n-    if (type == T_LONG || type == T_DOUBLE) {\n-      sig_bt[index++] = T_VOID; \/\/ Longs & doubles take 2 Java slots\n+    _c1_needs_stack_repair = (_args_on_stack_cc < _args_on_stack) || (_args_on_stack_cc_ro < _args_on_stack);\n+    _c2_needs_stack_repair = (_args_on_stack_cc > _args_on_stack) || (_args_on_stack_cc > _args_on_stack_cc_ro);\n+\n+    \/\/ Upper bound on stack arguments to avoid hitting the argument limit and\n+    \/\/ bailing out of compilation (\"unsupported incoming calling sequence\").\n+    \/\/ TODO we need a reasonable limit (flag?) here\n+    if (MAX2(_args_on_stack_cc, _args_on_stack_cc_ro) <= 60) {\n+      return; \/\/ Success\n@@ -2566,1 +2893,10 @@\n-};\n+\n+  \/\/ No scalarized args\n+  _sig_cc = _sig;\n+  _regs_cc = _regs;\n+  _args_on_stack_cc = _args_on_stack;\n+\n+  _sig_cc_ro = _sig;\n+  _regs_cc_ro = _regs;\n+  _args_on_stack_cc_ro = _args_on_stack;\n+}\n@@ -2583,2 +2919,15 @@\n-  \/\/ Fill in the signature array, for the calling-convention call.\n-  int total_args_passed = method->size_of_parameters(); \/\/ All args on stack\n+  CompiledEntrySignature ces(method());\n+  ces.compute_calling_conventions();\n+  if (ces.has_scalarized_args()) {\n+    if (!method->has_scalarized_args()) {\n+      method->set_has_scalarized_args();\n+    }\n+    if (ces.c1_needs_stack_repair()) {\n+      method->set_c1_needs_stack_repair();\n+    }\n+    if (ces.c2_needs_stack_repair() && !method->c2_needs_stack_repair()) {\n+      method->set_c2_needs_stack_repair();\n+    }\n+  } else if (method->is_abstract()) {\n+    return _abstract_method_handler;\n+  }\n@@ -2586,4 +2935,0 @@\n-  AdapterSignatureIterator si(method->signature(), method->constMethod()->fingerprint(),\n-                              method->is_static(), total_args_passed);\n-  assert(si.slots() == total_args_passed, \"\");\n-  BasicType* sig_bt = si.basic_types();\n@@ -2593,0 +2938,13 @@\n+    if (ces.has_scalarized_args() && method->is_abstract()) {\n+      \/\/ Save a C heap allocated version of the signature for abstract methods with scalarized inline type arguments\n+      address wrong_method_abstract = SharedRuntime::get_handle_wrong_method_abstract_stub();\n+      entry = AdapterHandlerLibrary::new_entry(new AdapterFingerPrint(nullptr),\n+                                               SharedRuntime::throw_AbstractMethodError_entry(),\n+                                               wrong_method_abstract, wrong_method_abstract, wrong_method_abstract,\n+                                               wrong_method_abstract, wrong_method_abstract);\n+      GrowableArray<SigEntry>* heap_sig = new (mtInternal) GrowableArray<SigEntry>(ces.sig_cc_ro()->length(), mtInternal);\n+      heap_sig->appendAll(ces.sig_cc_ro());\n+      entry->set_sig_cc(heap_sig);\n+      return entry;\n+    }\n+\n@@ -2594,1 +2952,1 @@\n-    entry = lookup(total_args_passed, sig_bt);\n+    entry = lookup(ces.sig_cc(), ces.has_inline_recv());\n@@ -2600,1 +2958,1 @@\n-        AdapterHandlerEntry* comparison_entry = create_adapter(comparison_blob, total_args_passed, sig_bt, false);\n+        AdapterHandlerEntry* comparison_entry = create_adapter(comparison_blob, ces, false);\n@@ -2610,1 +2968,1 @@\n-    entry = create_adapter(new_adapter, total_args_passed, sig_bt, \/* allocate_code_blob *\/ true);\n+    entry = create_adapter(new_adapter, ces, \/* allocate_code_blob *\/ true);\n@@ -2621,2 +2979,1 @@\n-                                                           int total_args_passed,\n-                                                           BasicType* sig_bt,\n+                                                           CompiledEntrySignature& ces,\n@@ -2634,5 +2991,0 @@\n-  VMRegPair stack_regs[16];\n-  VMRegPair* regs = (total_args_passed <= 16) ? stack_regs : NEW_RESOURCE_ARRAY(VMRegPair, total_args_passed);\n-\n-  \/\/ Get a description of the compiled java calling convention and the largest used (VMReg) stack slot usage\n-  int comp_args_on_stack = SharedRuntime::java_calling_convention(sig_bt, regs, total_args_passed);\n@@ -2646,1 +2998,1 @@\n-  AdapterFingerPrint* fingerprint = new AdapterFingerPrint(total_args_passed, sig_bt);\n+  AdapterFingerPrint* fingerprint = new AdapterFingerPrint(ces.sig_cc(), ces.has_inline_recv());\n@@ -2649,5 +3001,17 @@\n-                                                total_args_passed,\n-                                                comp_args_on_stack,\n-                                                sig_bt,\n-                                                regs,\n-                                                fingerprint);\n+                                                ces.args_on_stack(),\n+                                                ces.sig(),\n+                                                ces.regs(),\n+                                                ces.sig_cc(),\n+                                                ces.regs_cc(),\n+                                                ces.sig_cc_ro(),\n+                                                ces.regs_cc_ro(),\n+                                                fingerprint,\n+                                                new_adapter,\n+                                                allocate_code_blob);\n+\n+  if (ces.has_scalarized_args()) {\n+    \/\/ Save a C heap allocated version of the scalarized signature and store it in the adapter\n+    GrowableArray<SigEntry>* heap_sig = new (mtInternal) GrowableArray<SigEntry>(ces.sig_cc()->length(), mtInternal);\n+    heap_sig->appendAll(ces.sig_cc());\n+    entry->set_sig_cc(heap_sig);\n+  }\n@@ -2664,1 +3028,0 @@\n-  new_adapter = AdapterBlob::create(&buffer);\n@@ -2706,0 +3069,2 @@\n+  assert(base <= _c2i_inline_entry || _c2i_inline_entry == nullptr, \"\");\n+  assert(base <= _c2i_inline_ro_entry || _c2i_inline_ro_entry == nullptr, \"\");\n@@ -2707,0 +3072,1 @@\n+  assert(base <= _c2i_unverified_inline_entry || _c2i_unverified_inline_entry == nullptr, \"\");\n@@ -2719,0 +3085,4 @@\n+  if (_c2i_inline_entry != nullptr)\n+    _c2i_inline_entry += delta;\n+  if (_c2i_inline_ro_entry != nullptr)\n+    _c2i_inline_ro_entry += delta;\n@@ -2721,0 +3091,2 @@\n+  if (_c2i_unverified_inline_entry != nullptr)\n+    _c2i_unverified_inline_entry += delta;\n@@ -2729,0 +3101,3 @@\n+  if (_sig_cc != nullptr) {\n+    delete _sig_cc;\n+  }\n@@ -2815,0 +3190,1 @@\n+      BasicType stack_sig_bt[16];\n@@ -2816,0 +3192,1 @@\n+      BasicType* sig_bt = (total_args_passed <= 16) ? stack_sig_bt : NEW_RESOURCE_ARRAY(BasicType, total_args_passed);\n@@ -2818,5 +3195,13 @@\n-      AdapterSignatureIterator si(method->signature(), method->constMethod()->fingerprint(),\n-                              method->is_static(), total_args_passed);\n-      BasicType* sig_bt = si.basic_types();\n-      assert(si.slots() == total_args_passed, \"\");\n-      BasicType ret_type = si.return_type();\n+      int i = 0;\n+      if (!method->is_static()) {  \/\/ Pass in receiver first\n+        sig_bt[i++] = T_OBJECT;\n+      }\n+      SignatureStream ss(method->signature());\n+      for (; !ss.at_return_type(); ss.next()) {\n+        sig_bt[i++] = ss.type();  \/\/ Collect remaining bits of signature\n+        if (ss.type() == T_LONG || ss.type() == T_DOUBLE) {\n+          sig_bt[i++] = T_VOID;   \/\/ Longs & doubles take 2 Java slots\n+        }\n+      }\n+      assert(i == total_args_passed, \"\");\n+      BasicType ret_type = ss.type();\n@@ -3069,0 +3454,6 @@\n+  if (get_c2i_entry() != nullptr) {\n+    st->print(\" c2iVE: \" INTPTR_FORMAT, p2i(get_c2i_inline_entry()));\n+  }\n+  if (get_c2i_entry() != nullptr) {\n+    st->print(\" c2iVROE: \" INTPTR_FORMAT, p2i(get_c2i_inline_ro_entry()));\n+  }\n@@ -3070,1 +3461,4 @@\n-    st->print(\" c2iUV: \" INTPTR_FORMAT, p2i(get_c2i_unverified_entry()));\n+    st->print(\" c2iUE: \" INTPTR_FORMAT, p2i(get_c2i_unverified_entry()));\n+  }\n+  if (get_c2i_unverified_entry() != nullptr) {\n+    st->print(\" c2iUVE: \" INTPTR_FORMAT, p2i(get_c2i_unverified_inline_entry()));\n@@ -3159,0 +3553,195 @@\n+\n+\/\/ We are at a compiled code to interpreter call. We need backing\n+\/\/ buffers for all inline type arguments. Allocate an object array to\n+\/\/ hold them (convenient because once we're done with it we don't have\n+\/\/ to worry about freeing it).\n+oop SharedRuntime::allocate_inline_types_impl(JavaThread* current, methodHandle callee, bool allocate_receiver, TRAPS) {\n+  assert(InlineTypePassFieldsAsArgs, \"no reason to call this\");\n+  ResourceMark rm;\n+\n+  int nb_slots = 0;\n+  InstanceKlass* holder = callee->method_holder();\n+  allocate_receiver &= !callee->is_static() && holder->is_inline_klass() && callee->is_scalarized_arg(0);\n+  if (allocate_receiver) {\n+    nb_slots++;\n+  }\n+  int arg_num = callee->is_static() ? 0 : 1;\n+  for (SignatureStream ss(callee->signature()); !ss.at_return_type(); ss.next()) {\n+    BasicType bt = ss.type();\n+    if (bt == T_OBJECT && callee->is_scalarized_arg(arg_num)) {\n+      nb_slots++;\n+    }\n+    if (bt != T_VOID) {\n+      arg_num++;\n+    }\n+  }\n+  objArrayOop array_oop = oopFactory::new_objectArray(nb_slots, CHECK_NULL);\n+  objArrayHandle array(THREAD, array_oop);\n+  arg_num = callee->is_static() ? 0 : 1;\n+  int i = 0;\n+  if (allocate_receiver) {\n+    InlineKlass* vk = InlineKlass::cast(holder);\n+    oop res = vk->allocate_instance(CHECK_NULL);\n+    array->obj_at_put(i++, res);\n+  }\n+  for (SignatureStream ss(callee->signature()); !ss.at_return_type(); ss.next()) {\n+    BasicType bt = ss.type();\n+    if (bt == T_OBJECT && callee->is_scalarized_arg(arg_num)) {\n+      InlineKlass* vk = ss.as_inline_klass(holder);\n+      assert(vk != nullptr, \"Unexpected klass\");\n+      oop res = vk->allocate_instance(CHECK_NULL);\n+      array->obj_at_put(i++, res);\n+    }\n+    if (bt != T_VOID) {\n+      arg_num++;\n+    }\n+  }\n+  return array();\n+}\n+\n+JRT_ENTRY(void, SharedRuntime::allocate_inline_types(JavaThread* current, Method* callee_method, bool allocate_receiver))\n+  methodHandle callee(current, callee_method);\n+  oop array = SharedRuntime::allocate_inline_types_impl(current, callee, allocate_receiver, CHECK);\n+  current->set_vm_result(array);\n+  current->set_vm_result_2(callee()); \/\/ TODO: required to keep callee live?\n+JRT_END\n+\n+\/\/ We're returning from an interpreted method: load each field into a\n+\/\/ register following the calling convention\n+JRT_LEAF(void, SharedRuntime::load_inline_type_fields_in_regs(JavaThread* current, oopDesc* res))\n+{\n+  assert(res->klass()->is_inline_klass(), \"only inline types here\");\n+  ResourceMark rm;\n+  RegisterMap reg_map(current,\n+                      RegisterMap::UpdateMap::include,\n+                      RegisterMap::ProcessFrames::include,\n+                      RegisterMap::WalkContinuation::skip);\n+  frame stubFrame = current->last_frame();\n+  frame callerFrame = stubFrame.sender(&reg_map);\n+  assert(callerFrame.is_interpreted_frame(), \"should be coming from interpreter\");\n+\n+  InlineKlass* vk = InlineKlass::cast(res->klass());\n+\n+  const Array<SigEntry>* sig_vk = vk->extended_sig();\n+  const Array<VMRegPair>* regs = vk->return_regs();\n+\n+  if (regs == nullptr) {\n+    \/\/ The fields of the inline klass don't fit in registers, bail out\n+    return;\n+  }\n+\n+  int j = 1;\n+  for (int i = 0; i < sig_vk->length(); i++) {\n+    BasicType bt = sig_vk->at(i)._bt;\n+    if (bt == T_METADATA) {\n+      continue;\n+    }\n+    if (bt == T_VOID) {\n+      if (sig_vk->at(i-1)._bt == T_LONG ||\n+          sig_vk->at(i-1)._bt == T_DOUBLE) {\n+        j++;\n+      }\n+      continue;\n+    }\n+    int off = sig_vk->at(i)._offset;\n+    assert(off > 0, \"offset in object should be positive\");\n+    VMRegPair pair = regs->at(j);\n+    address loc = reg_map.location(pair.first(), nullptr);\n+    switch(bt) {\n+    case T_BOOLEAN:\n+      *(jboolean*)loc = res->bool_field(off);\n+      break;\n+    case T_CHAR:\n+      *(jchar*)loc = res->char_field(off);\n+      break;\n+    case T_BYTE:\n+      *(jbyte*)loc = res->byte_field(off);\n+      break;\n+    case T_SHORT:\n+      *(jshort*)loc = res->short_field(off);\n+      break;\n+    case T_INT: {\n+      *(jint*)loc = res->int_field(off);\n+      break;\n+    }\n+    case T_LONG:\n+#ifdef _LP64\n+      *(intptr_t*)loc = res->long_field(off);\n+#else\n+      Unimplemented();\n+#endif\n+      break;\n+    case T_OBJECT:\n+    case T_ARRAY: {\n+      *(oop*)loc = res->obj_field(off);\n+      break;\n+    }\n+    case T_FLOAT:\n+      *(jfloat*)loc = res->float_field(off);\n+      break;\n+    case T_DOUBLE:\n+      *(jdouble*)loc = res->double_field(off);\n+      break;\n+    default:\n+      ShouldNotReachHere();\n+    }\n+    j++;\n+  }\n+  assert(j == regs->length(), \"missed a field?\");\n+\n+#ifdef ASSERT\n+  VMRegPair pair = regs->at(0);\n+  address loc = reg_map.location(pair.first(), nullptr);\n+  assert(*(oopDesc**)loc == res, \"overwritten object\");\n+#endif\n+\n+  current->set_vm_result(res);\n+}\n+JRT_END\n+\n+\/\/ We've returned to an interpreted method, the interpreter needs a\n+\/\/ reference to an inline type instance. Allocate it and initialize it\n+\/\/ from field's values in registers.\n+JRT_BLOCK_ENTRY(void, SharedRuntime::store_inline_type_fields_to_buf(JavaThread* current, intptr_t res))\n+{\n+  ResourceMark rm;\n+  RegisterMap reg_map(current,\n+                      RegisterMap::UpdateMap::include,\n+                      RegisterMap::ProcessFrames::include,\n+                      RegisterMap::WalkContinuation::skip);\n+  frame stubFrame = current->last_frame();\n+  frame callerFrame = stubFrame.sender(&reg_map);\n+\n+#ifdef ASSERT\n+  InlineKlass* verif_vk = InlineKlass::returned_inline_klass(reg_map);\n+#endif\n+\n+  if (!is_set_nth_bit(res, 0)) {\n+    \/\/ We're not returning with inline type fields in registers (the\n+    \/\/ calling convention didn't allow it for this inline klass)\n+    assert(!Metaspace::contains((void*)res), \"should be oop or pointer in buffer area\");\n+    current->set_vm_result((oopDesc*)res);\n+    assert(verif_vk == nullptr, \"broken calling convention\");\n+    return;\n+  }\n+\n+  clear_nth_bit(res, 0);\n+  InlineKlass* vk = (InlineKlass*)res;\n+  assert(verif_vk == vk, \"broken calling convention\");\n+  assert(Metaspace::contains((void*)res), \"should be klass\");\n+\n+  \/\/ Allocate handles for every oop field so they are safe in case of\n+  \/\/ a safepoint when allocating\n+  GrowableArray<Handle> handles;\n+  vk->save_oop_fields(reg_map, handles);\n+\n+  \/\/ It's unsafe to safepoint until we are here\n+  JRT_BLOCK;\n+  {\n+    JavaThread* THREAD = current;\n+    oop vt = vk->realloc_result(reg_map, handles, CHECK);\n+    current->set_vm_result(vt);\n+  }\n+  JRT_BLOCK_END;\n+}\n+JRT_END\n","filename":"src\/hotspot\/share\/runtime\/sharedRuntime.cpp","additions":723,"deletions":134,"binary":false,"changes":857,"status":"modified"},{"patch":"@@ -28,0 +28,1 @@\n+#include \"asm\/codeBuffer.hpp\"\n@@ -33,0 +34,1 @@\n+#include \"runtime\/signature.hpp\"\n@@ -39,0 +41,1 @@\n+class SigEntry;\n@@ -369,0 +372,2 @@\n+  static char* generate_identity_exception_message(JavaThread* thr, Klass* klass);\n+\n@@ -371,1 +376,1 @@\n-  static methodHandle resolve_helper(bool is_virtual, bool is_optimized, TRAPS);\n+  static methodHandle resolve_helper(bool is_virtual, bool is_optimized, bool& caller_is_c1, TRAPS);\n@@ -379,1 +384,1 @@\n-                                             bool& needs_ic_stub_refill, TRAPS);\n+                                             bool& needs_ic_stub_refill, bool& is_optimized, bool caller_is_c1, TRAPS);\n@@ -385,1 +390,1 @@\n-  static methodHandle reresolve_call_site(TRAPS);\n+  static methodHandle reresolve_call_site(bool& is_static_call, bool& is_optimized, bool& caller_is_c1, TRAPS);\n@@ -389,1 +394,1 @@\n-  static methodHandle handle_ic_miss_helper(TRAPS);\n+  static methodHandle handle_ic_miss_helper(bool& is_optimized, bool& caller_is_c1, TRAPS);\n@@ -392,1 +397,1 @@\n-  static methodHandle find_callee_method(TRAPS);\n+  static methodHandle find_callee_method(bool is_optimized, bool& caller_is_c1, TRAPS);\n@@ -423,0 +428,8 @@\n+  static int java_calling_convention(const GrowableArray<SigEntry>* sig, VMRegPair* regs) {\n+    BasicType* sig_bt = NEW_RESOURCE_ARRAY(BasicType, sig->length());\n+    int total_args_passed = SigEntry::fill_sig_bt(sig, sig_bt);\n+    return java_calling_convention(sig_bt, regs, total_args_passed);\n+  }\n+  static int java_return_convention(const BasicType* sig_bt, VMRegPair* regs, int total_args_passed);\n+  static const uint java_return_convention_max_int;\n+  static const uint java_return_convention_max_float;\n@@ -469,6 +482,11 @@\n-  static AdapterHandlerEntry* generate_i2c2i_adapters(MacroAssembler *_masm,\n-                                                      int total_args_passed,\n-                                                      int max_arg,\n-                                                      const BasicType *sig_bt,\n-                                                      const VMRegPair *regs,\n-                                                      AdapterFingerPrint* fingerprint);\n+  static AdapterHandlerEntry* generate_i2c2i_adapters(MacroAssembler *masm,\n+                                                      int comp_args_on_stack,\n+                                                      const GrowableArray<SigEntry>* sig,\n+                                                      const VMRegPair* regs,\n+                                                      const GrowableArray<SigEntry>* sig_cc,\n+                                                      const VMRegPair* regs_cc,\n+                                                      const GrowableArray<SigEntry>* sig_cc_ro,\n+                                                      const VMRegPair* regs_cc_ro,\n+                                                      AdapterFingerPrint* fingerprint,\n+                                                      AdapterBlob*& new_adapter,\n+                                                      bool allocate_code_blob);\n@@ -477,2 +495,1 @@\n-                              int total_args_passed,\n-                              const BasicType *sig_bt,\n+                              const GrowableArray<SigEntry>* sig,\n@@ -548,1 +565,2 @@\n-  static address get_resolved_entry        (JavaThread* current, methodHandle callee_method);\n+  static address get_resolved_entry        (JavaThread* current, methodHandle callee_method,\n+                                            bool is_static_call, bool is_optimized, bool caller_is_c1);\n@@ -553,0 +571,3 @@\n+  static void load_inline_type_fields_in_regs(JavaThread* current, oopDesc* res);\n+  static void store_inline_type_fields_to_buf(JavaThread* current, intptr_t res);\n+\n@@ -563,0 +584,2 @@\n+  static void allocate_inline_types(JavaThread* current, Method* callee, bool allocate_receiver);\n+  static oop allocate_inline_types_impl(JavaThread* current, methodHandle callee, bool allocate_receiver, TRAPS);\n@@ -566,0 +589,1 @@\n+  static BufferedInlineTypeBlob* generate_buffered_inline_type_adapter(const InlineKlass* vk);\n@@ -674,0 +698,2 @@\n+  address _c2i_inline_entry;\n+  address _c2i_inline_ro_entry;\n@@ -675,0 +701,1 @@\n+  address _c2i_unverified_inline_entry;\n@@ -677,0 +704,3 @@\n+  \/\/ Support for scalarized inline type calling convention\n+  const GrowableArray<SigEntry>* _sig_cc;\n+\n@@ -685,1 +715,2 @@\n-                      address c2i_unverified_entry,\n+                      address c2i_inline_entry, address c2i_inline_ro_entry,\n+                      address c2i_unverified_entry, address c2i_unverified_inline_entry,\n@@ -690,0 +721,2 @@\n+    _c2i_inline_entry(c2i_inline_entry),\n+    _c2i_inline_ro_entry(c2i_inline_ro_entry),\n@@ -691,1 +724,3 @@\n-    _c2i_no_clinit_check_entry(c2i_no_clinit_check_entry)\n+    _c2i_unverified_inline_entry(c2i_unverified_inline_entry),\n+    _c2i_no_clinit_check_entry(c2i_no_clinit_check_entry),\n+    _sig_cc(nullptr)\n@@ -700,4 +735,7 @@\n-  address get_i2c_entry()                  const { return _i2c_entry; }\n-  address get_c2i_entry()                  const { return _c2i_entry; }\n-  address get_c2i_unverified_entry()       const { return _c2i_unverified_entry; }\n-  address get_c2i_no_clinit_check_entry()  const { return _c2i_no_clinit_check_entry; }\n+  address get_i2c_entry()                   const { return _i2c_entry; }\n+  address get_c2i_entry()                   const { return _c2i_entry; }\n+  address get_c2i_inline_entry()            const { return _c2i_inline_entry; }\n+  address get_c2i_inline_ro_entry()         const { return _c2i_inline_ro_entry; }\n+  address get_c2i_unverified_entry()        const { return _c2i_unverified_entry; }\n+  address get_c2i_unverified_inline_entry() const { return _c2i_unverified_inline_entry; }\n+  address get_c2i_no_clinit_check_entry()   const { return _c2i_no_clinit_check_entry; }\n@@ -708,0 +746,4 @@\n+  \/\/ Support for scalarized inline type calling convention\n+  void set_sig_cc(const GrowableArray<SigEntry>* sig)  { _sig_cc = sig; }\n+  const GrowableArray<SigEntry>* get_sig_cc()    const { return _sig_cc; }\n+\n@@ -720,0 +762,2 @@\n+class CompiledEntrySignature;\n+\n@@ -734,2 +778,1 @@\n-                                             int total_args_passed,\n-                                             BasicType* sig_bt,\n+                                             CompiledEntrySignature& ces,\n@@ -741,4 +784,2 @@\n-                                        address i2c_entry,\n-                                        address c2i_entry,\n-                                        address c2i_unverified_entry,\n-                                        address c2i_no_clinit_check_entry = nullptr);\n+                                        address i2c_entry, address c2i_entry, address c2i_inline_entry, address c2i_inline_ro_entry,\n+                                        address c2i_unverified_entry, address c2i_unverified_inline_entry, address c2i_no_clinit_check_entry = nullptr);\n@@ -757,0 +798,59 @@\n+\/\/ Utility class for computing the calling convention of the 3 types\n+\/\/ of compiled method entries:\n+\/\/     Method::_from_compiled_entry               - sig_cc\n+\/\/     Method::_from_compiled_inline_ro_entry     - sig_cc_ro\n+\/\/     Method::_from_compiled_inline_entry        - sig\n+class CompiledEntrySignature : public StackObj {\n+  Method* _method;\n+  int  _num_inline_args;\n+  bool _has_inline_recv;\n+  GrowableArray<SigEntry>* _sig;\n+  GrowableArray<SigEntry>* _sig_cc;\n+  GrowableArray<SigEntry>* _sig_cc_ro;\n+  VMRegPair* _regs;\n+  VMRegPair* _regs_cc;\n+  VMRegPair* _regs_cc_ro;\n+\n+  int _args_on_stack;\n+  int _args_on_stack_cc;\n+  int _args_on_stack_cc_ro;\n+\n+  bool _c1_needs_stack_repair;\n+  bool _c2_needs_stack_repair;\n+\n+  GrowableArray<Method*>* _supers;\n+\n+public:\n+  Method* method()                     const { return _method; }\n+\n+  \/\/ Used by Method::_from_compiled_inline_entry\n+  GrowableArray<SigEntry>* sig()       const { return _sig; }\n+\n+  \/\/ Used by Method::_from_compiled_entry\n+  GrowableArray<SigEntry>* sig_cc()    const { return _sig_cc; }\n+\n+  \/\/ Used by Method::_from_compiled_inline_ro_entry\n+  GrowableArray<SigEntry>* sig_cc_ro() const { return _sig_cc_ro; }\n+\n+  VMRegPair* regs()                    const { return _regs; }\n+  VMRegPair* regs_cc()                 const { return _regs_cc; }\n+  VMRegPair* regs_cc_ro()              const { return _regs_cc_ro; }\n+\n+  int args_on_stack()                  const { return _args_on_stack; }\n+  int args_on_stack_cc()               const { return _args_on_stack_cc; }\n+  int args_on_stack_cc_ro()            const { return _args_on_stack_cc_ro; }\n+\n+  int  num_inline_args()               const { return _num_inline_args; }\n+  bool has_inline_recv()               const { return _has_inline_recv; }\n+\n+  bool has_scalarized_args()           const { return _sig != _sig_cc; }\n+  bool c1_needs_stack_repair()         const { return _c1_needs_stack_repair; }\n+  bool c2_needs_stack_repair()         const { return _c2_needs_stack_repair; }\n+  CodeOffsets::Entries c1_inline_ro_entry_type() const;\n+\n+  GrowableArray<Method*>* get_supers();\n+\n+  CompiledEntrySignature(Method* method = nullptr);\n+  void compute_calling_conventions(bool init = true);\n+};\n+\n","filename":"src\/hotspot\/share\/runtime\/sharedRuntime.hpp","additions":126,"deletions":26,"binary":false,"changes":152,"status":"modified"},{"patch":"@@ -71,0 +71,2 @@\n+#include \"oops\/flatArrayKlass.hpp\"\n+#include \"oops\/inlineKlass.hpp\"\n@@ -233,1 +235,1 @@\n-  volatile_nonstatic_field(InstanceKlass,      _array_klasses,                                ObjArrayKlass*)                        \\\n+  volatile_nonstatic_field(InstanceKlass,      _array_klasses,                                ArrayKlass*)                        \\\n@@ -1166,0 +1168,1 @@\n+           declare_type(FlatArrayKlass, ArrayKlass)                       \\\n@@ -1169,0 +1172,1 @@\n+        declare_type(InlineKlass, InstanceKlass)                          \\\n@@ -1548,0 +1552,1 @@\n+  declare_c2_type(MachVEPNode, MachIdealNode)                             \\\n","filename":"src\/hotspot\/share\/runtime\/vmStructs.cpp","additions":6,"deletions":1,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -42,0 +42,1 @@\n+import java.lang.reflect.ClassFileFormatVersion;\n@@ -227,3 +228,3 @@\n-    private static final int ANNOTATION= 0x00002000;\n-    private static final int ENUM      = 0x00004000;\n-    private static final int SYNTHETIC = 0x00001000;\n+    private static final int ANNOTATION = 0x00002000;\n+    private static final int ENUM       = 0x00004000;\n+    private static final int SYNTHETIC  = 0x00001000;\n@@ -316,0 +317,2 @@\n+                \/\/ Modifier.toString() below mis-interprets SYNCHRONIZED, STRICT, and VOLATILE bits\n+                modifiers &= ~(Modifier.SYNCHRONIZED | Modifier.STRICT | Modifier.VOLATILE);\n@@ -329,0 +332,3 @@\n+                if (isValue()) {\n+                    sb.append(\"value \");\n+                }\n@@ -678,0 +684,33 @@\n+    \/**\n+     * {@return {@code true} if this {@code Class} object represents an identity\n+     * class or interface; otherwise {@code false}}\n+     *\n+     * If this {@code Class} object represents an array type, then this method\n+     * returns {@code true}.\n+     * If this {@code Class} object represents a primitive type, or {@code void},\n+     * then this method returns {@code false}.\n+     *\n+     * @since Valhalla\n+     *\/\n+    @PreviewFeature(feature = PreviewFeature.Feature.VALUE_OBJECTS, reflective=true)\n+    public native boolean isIdentity();\n+\n+    \/**\n+     * {@return {@code true} if this {@code Class} object represents a value\n+     * class; otherwise {@code false}}\n+     *\n+     * If this {@code Class} object represents an array type, an interface,\n+     * a primitive type, or {@code void}, then this method returns {@code false}.\n+     *\n+     * @since Valhalla\n+     *\/\n+    @PreviewFeature(feature = PreviewFeature.Feature.VALUE_OBJECTS, reflective=true)\n+    public boolean isValue() {\n+        if (!PreviewFeatures.isEnabled()) {\n+            return false;\n+        }\n+         if (isPrimitive() || isArray() || isInterface())\n+             return false;\n+        return ((getModifiers() & Modifier.IDENTITY) == 0);\n+    }\n+\n@@ -1444,0 +1483,1 @@\n+     * <li> its {@code identity} modifier is always true\n@@ -1469,1 +1509,1 @@\n-    \/**\n+   \/**\n@@ -1472,0 +1512,1 @@\n+     * The {@code AccessFlags} may depend on the class file format version of the class.\n@@ -1480,0 +1521,1 @@\n+    * <li> its {@code identity} modifier is always true\n@@ -1501,4 +1543,11 @@\n-        return AccessFlag.maskToAccessFlags((location == AccessFlag.Location.CLASS) ?\n-                                            getClassAccessFlagsRaw() :\n-                                            getModifiers(),\n-                                            location);\n+        int accessFlags = (location == AccessFlag.Location.CLASS) ?\n+                getClassAccessFlagsRaw() : getModifiers();\n+        if (isArray() && PreviewFeatures.isEnabled()) {\n+            accessFlags |= Modifier.IDENTITY;\n+        }\n+        var cffv = ClassFileFormatVersion.fromMajor(getClassFileVersion() & 0xffff);\n+        if (cffv.compareTo(ClassFileFormatVersion.latest()) >= 0) {\n+            \/\/ Ignore unspecified (0x0800) access flag for current version\n+            accessFlags &= ~0x0800;\n+        }\n+        return AccessFlag.maskToAccessFlags(accessFlags, location, cffv);\n@@ -1507,1 +1556,1 @@\n-    \/**\n+   \/**\n@@ -1515,0 +1564,1 @@\n+\n@@ -4441,1 +4491,1 @@\n-         return TypeAnnotationParser.buildAnnotatedInterfaces(getRawTypeAnnotations(), getConstantPool(), this);\n+        return TypeAnnotationParser.buildAnnotatedInterfaces(getRawTypeAnnotations(), getConstantPool(), this);\n@@ -4839,1 +4889,2 @@\n-    private int getClassFileVersion() {\n+    \/* package-private *\/\n+    int getClassFileVersion() {\n","filename":"src\/java.base\/share\/classes\/java\/lang\/Class.java","additions":62,"deletions":11,"binary":false,"changes":73,"status":"modified"},{"patch":"@@ -36,0 +36,1 @@\n+import jdk.internal.value.DeserializeConstructor;\n@@ -360,0 +361,1 @@\n+@jdk.internal.MigratedValueClass\n@@ -951,0 +953,1 @@\n+    @DeserializeConstructor\n","filename":"src\/java.base\/share\/classes\/java\/lang\/Double.java","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -90,0 +90,2 @@\n+@jdk.internal.MigratedValueClass\n+@jdk.internal.ValueBased\n","filename":"src\/java.base\/share\/classes\/java\/lang\/Record.java","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -43,0 +43,1 @@\n+import java.lang.reflect.ClassFileFormatVersion;\n@@ -2784,0 +2785,4 @@\n+            public int classFileFormatVersion(Class<?> clazz) {\n+                return clazz.getClassFileVersion();\n+            }\n+\n","filename":"src\/java.base\/share\/classes\/java\/lang\/System.java","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -28,0 +28,1 @@\n+import jdk.internal.misc.PreviewFeatures;\n@@ -41,0 +42,1 @@\n+\n@@ -42,0 +44,1 @@\n+import java.lang.constant.ConstantDescs;\n@@ -44,0 +47,2 @@\n+import java.lang.reflect.AccessFlag;\n+import java.lang.reflect.ClassFileFormatVersion;\n@@ -45,0 +50,2 @@\n+import java.util.ArrayList;\n+import java.util.HashSet;\n@@ -52,0 +59,1 @@\n+import java.lang.classfile.attribute.LoadableDescriptorsAttribute;\n@@ -55,0 +63,1 @@\n+import java.lang.classfile.constantpool.Utf8Entry;\n@@ -310,1 +319,2 @@\n-                clb.withFlags(ACC_SUPER | ACC_FINAL | ACC_SYNTHETIC)\n+                clb.withVersion(ClassFileFormatVersion.latest().major(), (PreviewFeatures.isEnabled() ? 0xFFFF0000 : 0))\n+                   .withFlags(ACC_SUPER | ACC_FINAL | ACC_SYNTHETIC)\n@@ -312,0 +322,6 @@\n+\n+                \/\/ generate LoadableDescriptors attribute if it references any value class\n+                if (PreviewFeatures.isEnabled()) {\n+                    generateLoadableDescriptors(clb);\n+                }\n+\n@@ -545,0 +561,67 @@\n+    \/*\n+     * LoadableDescriptors attribute builder\n+     *\/\n+    static class LoadableDescriptorsAttributeBuilder {\n+        private final Set<String> loadableDescriptors = new HashSet<>();\n+        LoadableDescriptorsAttributeBuilder(Class<?> targetClass) {\n+            if (requiresLoadableDescriptors(targetClass)) {\n+                loadableDescriptors.add(targetClass.descriptorString());\n+            }\n+        }\n+\n+        \/*\n+         * Add the value types referenced in the given MethodType.\n+         *\/\n+        LoadableDescriptorsAttributeBuilder add(MethodType mt) {\n+            \/\/ parameter types\n+            for (Class<?> paramType : mt.ptypes()) {\n+                if (requiresLoadableDescriptors(paramType)) {\n+                    loadableDescriptors.add(paramType.descriptorString());\n+                }\n+            }\n+            \/\/ return type\n+            if (requiresLoadableDescriptors(mt.returnType())) {\n+                loadableDescriptors.add(mt.returnType().descriptorString());\n+            }\n+            return this;\n+        }\n+\n+        LoadableDescriptorsAttributeBuilder add(MethodType... mtypes) {\n+            for (MethodType mt : mtypes) {\n+                add(mt);\n+            }\n+            return this;\n+        }\n+\n+        boolean requiresLoadableDescriptors(Class<?> cls) {\n+            return cls.isValue() && cls.accessFlags().contains(AccessFlag.FINAL);\n+        }\n+\n+        boolean isEmpty() {\n+            return loadableDescriptors.isEmpty();\n+        }\n+\n+        void build(ClassBuilder clb) {\n+            if (!isEmpty()) {\n+                List<Utf8Entry> lds = new ArrayList<Utf8Entry>(loadableDescriptors.size());\n+                for (String ld : loadableDescriptors) {\n+                    lds.add(clb.constantPool().utf8Entry(ld));\n+                }\n+                clb.with(LoadableDescriptorsAttribute.of(lds));\n+            }\n+        }\n+    }\n+\n+    \/**\n+     * Generate LoadableDescriptors attribute if it references any value class\n+     *\/\n+    private void generateLoadableDescriptors(ClassBuilder clb) {\n+        LoadableDescriptorsAttributeBuilder builder = new LoadableDescriptorsAttributeBuilder(targetClass);\n+        builder.add(factoryType)\n+               .add(interfaceMethodType)\n+               .add(implMethodType)\n+               .add(dynamicMethodType)\n+               .add(altMethods)\n+          .build(clb);\n+    }\n+\n","filename":"src\/java.base\/share\/classes\/java\/lang\/invoke\/InnerClassLambdaMetafactory.java","additions":84,"deletions":1,"binary":false,"changes":85,"status":"modified"},{"patch":"@@ -270,0 +270,4 @@\n+        GET_VALUE(\"getValue\"),\n+        PUT_VALUE(\"putValue\"),\n+        GET_VALUE_VOLATILE(\"getValueVolatile\"),\n+        PUT_VALUE_VOLATILE(\"putValueVolatile\"),\n","filename":"src\/java.base\/share\/classes\/java\/lang\/invoke\/LambdaForm.java","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2794,0 +2794,2 @@\n+         *\n+         *\n@@ -2809,0 +2811,3 @@\n+            if (type.returnType() != void.class) {\n+                throw new NoSuchMethodException(\"Constructors must have void return type: \" + refc.getName());\n+            }\n@@ -4077,1 +4082,0 @@\n-\n","filename":"src\/java.base\/share\/classes\/java\/lang\/invoke\/MethodHandles.java","additions":5,"deletions":1,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -28,0 +28,3 @@\n+import jdk.internal.javac.PreviewFeature;\n+import jdk.internal.misc.PreviewFeatures;\n+\n@@ -188,0 +191,20 @@\n+     * If preview feature is enabled,\n+     * the {@code 0x0020} access flag bit is {@linkplain #IDENTITY IDENTITY access flag}.\n+     *\/\n+    SUPER(0x0000_0020, false,\n+            PreviewFeatures.isEnabled() ? Location.EMPTY_SET : Location.SET_CLASS,\n+            new Function<ClassFileFormatVersion, Set<Location>>() {\n+            @Override\n+            public Set<Location> apply(ClassFileFormatVersion cffv) {\n+                return (cffv.compareTo(ClassFileFormatVersion.latest()) >= 0) &&\n+                        PreviewFeatures.isEnabled() ? Location.EMPTY_SET : Location.SET_CLASS;\n+            }\n+        }),\n+\n+    \/**\n+     * The access flag {@code ACC_IDENTITY}, corresponding to the\n+     * modifier {@link Modifier#IDENTITY identity}, with a mask\n+     * value of <code>{@value \"0x%04x\" Modifier#IDENTITY}<\/code>.\n+     * @jvms 4.1 -B. Class access and property modifiers\n+     *\n+     * @since Valhalla\n@@ -189,1 +212,11 @@\n-    SUPER(0x0000_0020, false, Location.SET_CLASS, null),\n+    @PreviewFeature(feature = PreviewFeature.Feature.VALUE_OBJECTS, reflective=true)\n+    IDENTITY(Modifier.IDENTITY, false,\n+            PreviewFeatures.isEnabled() ? Location.SET_CLASS_INNER_CLASS : Location.EMPTY_SET,\n+            new Function<ClassFileFormatVersion, Set<Location>>() {\n+                @Override\n+                public Set<Location> apply(ClassFileFormatVersion cffv) {\n+                    return (cffv.compareTo(ClassFileFormatVersion.latest()) >= 0\n+                            && PreviewFeatures.isEnabled())\n+                            ? Location.SET_CLASS_INNER_CLASS : Location.EMPTY_SET;\n+                }\n+            }),\n@@ -196,8 +229,8 @@\n-        OPEN(0x0000_0020, false, Location.SET_MODULE,\n-             new Function<ClassFileFormatVersion, Set<Location>>() {\n-                 @Override\n-                 public Set<Location> apply(ClassFileFormatVersion cffv) {\n-                     return (cffv.compareTo(ClassFileFormatVersion.RELEASE_9) >= 0 ) ?\n-                         Location.SET_MODULE:\n-                         Location.EMPTY_SET;}\n-             }),\n+    OPEN(0x0000_0020, false, Location.SET_MODULE,\n+         new Function<ClassFileFormatVersion, Set<Location>>() {\n+             @Override\n+             public Set<Location> apply(ClassFileFormatVersion cffv) {\n+                 return (cffv.compareTo(ClassFileFormatVersion.RELEASE_9) >= 0 ) ?\n+                     Location.SET_MODULE:\n+                     Location.EMPTY_SET;}\n+         }),\n@@ -240,1 +273,1 @@\n-   \/**\n+    \/**\n@@ -338,0 +371,24 @@\n+    \/**\n+     * The access flag {@code ACC_STRICT}, with a mask\n+     * value of <code>{@value \"0x%04x\" Modifier#STRICT}<\/code>.\n+     * @jvms 4.5 Fields\n+     *\n+     * @since Valhalla\n+     *\/\n+    @PreviewFeature(feature = PreviewFeature.Feature.VALUE_OBJECTS, reflective=true)\n+    STRICT_FIELD(Modifier.STRICT, false,\n+            PreviewFeatures.isEnabled() ? Location.SET_FIELD : Location.EMPTY_SET,\n+            new Function<ClassFileFormatVersion, Set<Location>>() {\n+                @Override\n+                public Set<Location> apply(ClassFileFormatVersion cffv) {\n+                    return (cffv.compareTo(ClassFileFormatVersion.latest()) >= 0\n+                            && PreviewFeatures.isEnabled())\n+                            ? Location.SET_FIELD : Location.EMPTY_SET;\n+                }\n+            }) {\n+        @Override\n+        public String toString() {\n+            return \"STRICT\";\n+        }\n+    },\n+\n@@ -489,1 +546,1 @@\n-     * positions not support for the location in question\n+     * positions not supported for the location in question\n@@ -495,1 +552,1 @@\n-            if ((mask &  accessMask) != 0) {\n+            if ((mask & accessMask) != 0) {\n@@ -498,0 +555,3 @@\n+                if (mask == 0) {\n+                    break;      \/\/ no more mask bits\n+                }\n@@ -508,0 +568,36 @@\n+    \/**\n+     * {@return an unmodifiable set of access flags for the given mask value\n+     * appropriate for the location in question}\n+     *\n+     * @param mask bit mask of access flags\n+     * @param location context to interpret mask value\n+     * @param cffv the class file format version\n+     * @throws IllegalArgumentException if the mask contains bit\n+     * positions not supported for the location in question\n+     *\/\n+    public static Set<AccessFlag> maskToAccessFlags(int mask, Location location,\n+                                                    ClassFileFormatVersion cffv) {\n+        Set<AccessFlag> result = java.util.EnumSet.noneOf(AccessFlag.class);\n+        for (var accessFlag : AccessFlag.values()) {\n+            int accessMask = accessFlag.mask();\n+            if ((mask & accessMask) != 0) {\n+                var locations = accessFlag.locations(cffv);\n+                if (locations.contains(location)) {\n+                    result.add(accessFlag);\n+                    mask = mask & ~accessMask;\n+                    if (mask == 0) {\n+                        break;      \/\/ no more mask bits\n+                    }\n+                }\n+            }\n+        }\n+        if (mask != 0) {\n+            throw new IllegalArgumentException(\"Unmatched bit position 0x\" +\n+                                               Integer.toHexString(mask) +\n+                                               \" for location \" + location +\n+                                               \" for class file format version \" + cffv);\n+        }\n+        return Collections.unmodifiableSet(result);\n+    }\n+\n+\n@@ -626,1 +722,1 @@\n-                                Set.of(PUBLIC, FINAL, SUPER,\n+                                Set.of(PUBLIC, FINAL, (PreviewFeatures.isEnabled() ? IDENTITY : SUPER),\n@@ -631,3 +727,8 @@\n-                                Set.of(PUBLIC, PRIVATE, PROTECTED,\n-                                       STATIC, FINAL, VOLATILE,\n-                                       TRANSIENT, SYNTHETIC, ENUM)),\n+                                PreviewFeatures.isEnabled() ?\n+                                        \/\/ STRICT_FIELD should be included only if preview is enabled\n+                                        Set.of(PUBLIC, PRIVATE, PROTECTED,\n+                                            STATIC, FINAL, VOLATILE,\n+                                            TRANSIENT, SYNTHETIC, ENUM, STRICT_FIELD) :\n+                                        Set.of(PUBLIC, PRIVATE, PROTECTED,\n+                                                STATIC, FINAL, VOLATILE,\n+                                                TRANSIENT, SYNTHETIC, ENUM)),\n@@ -640,3 +741,3 @@\n-                                Set.of(PUBLIC, PRIVATE, PROTECTED,\n-                                       STATIC, FINAL, INTERFACE, ABSTRACT,\n-                                       SYNTHETIC, ANNOTATION, ENUM)),\n+                                          Set.of(PUBLIC, PRIVATE, PROTECTED, (PreviewFeatures.isEnabled() ? IDENTITY : SUPER),\n+                                                  STATIC, FINAL, INTERFACE, ABSTRACT,\n+                                                  SYNTHETIC, ANNOTATION, ENUM)),\n","filename":"src\/java.base\/share\/classes\/java\/lang\/reflect\/AccessFlag.java","additions":120,"deletions":19,"binary":false,"changes":139,"status":"modified"},{"patch":"@@ -67,1 +67,3 @@\n-    private static final HashMap<Class<?>, MethodHandle> primitiveEquals = new HashMap<>();\n+    \/* package-private *\/\n+    static final HashMap<Class<?>, MethodHandle> primitiveEquals = new HashMap<>();\n+\n@@ -394,1 +396,1 @@\n-        if (type instanceof MethodType mt)\n+        if (type instanceof MethodType mt) {\n@@ -396,1 +398,4 @@\n-        else {\n+            if (mt.parameterType(0) != recordClass) {\n+                throw new IllegalArgumentException(\"Bad method type: \" + mt);\n+            }\n+        } else {\n@@ -402,0 +407,5 @@\n+        for (MethodHandle getter : getterList) {\n+            if (getter.type().parameterType(0) != recordClass) {\n+                throw new IllegalArgumentException(\"Bad receiver type: \" + getter);\n+            }\n+        }\n","filename":"src\/java.base\/share\/classes\/java\/lang\/runtime\/ObjectMethods.java","additions":13,"deletions":3,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -36,0 +36,1 @@\n+import java.lang.reflect.ClassFileFormatVersion;\n@@ -621,0 +622,6 @@\n+\n+    \/**\n+     * Returns the class file format version of the class.\n+     *\/\n+    int classFileFormatVersion(Class<?> klass);\n+\n","filename":"src\/java.base\/share\/classes\/jdk\/internal\/access\/JavaLangAccess.java","additions":7,"deletions":0,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -313,0 +313,18 @@\n+    public static final class LoadableDescriptorsMapper extends AbstractAttributeMapper<LoadableDescriptorsAttribute> {\n+        public static final LoadableDescriptorsMapper INSTANCE = new LoadableDescriptorsMapper();\n+\n+        private LoadableDescriptorsMapper() {\n+            super(NAME_LOADABLE_DESCRIPTORS, AttributeStability.CP_REFS);\n+        }\n+\n+        @Override\n+        public LoadableDescriptorsAttribute readAttribute(AttributedElement e, ClassReader cf, int p) {\n+            return new BoundAttribute.BoundLoadableDescriptorsAttribute(cf, this, p);\n+        }\n+\n+        @Override\n+        protected void writeBody(BufWriter buf, LoadableDescriptorsAttribute attr) {\n+            Util.writeListIndices(buf, attr.loadableDescriptors());\n+        }\n+    }\n+\n","filename":"src\/java.base\/share\/classes\/jdk\/internal\/classfile\/impl\/AbstractAttributeMapper.java","additions":18,"deletions":0,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -52,0 +52,1 @@\n+import java.lang.classfile.attribute.LoadableDescriptorsAttribute;\n@@ -431,0 +432,16 @@\n+    public static final class UnboundLoadableDescriptorsAttribute\n+            extends UnboundAttribute<LoadableDescriptorsAttribute>\n+            implements LoadableDescriptorsAttribute {\n+        private final List<Utf8Entry> loadableDescriptors;\n+\n+        public UnboundLoadableDescriptorsAttribute(List<Utf8Entry> loadableDescriptors) {\n+            super(Attributes.loadableDescriptors());\n+            this.loadableDescriptors = List.copyOf(loadableDescriptors);\n+        }\n+\n+        @Override\n+        public List<Utf8Entry> loadableDescriptors() {\n+            return loadableDescriptors;\n+        }\n+    }\n+\n","filename":"src\/java.base\/share\/classes\/jdk\/internal\/classfile\/impl\/UnboundAttribute.java","additions":17,"deletions":0,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -139,1 +139,0 @@\n-\n@@ -274,0 +273,2 @@\n+    exports jdk.internal.value to  \/\/ Needed by Unsafe\n+        jdk.unsupported;\n","filename":"src\/java.base\/share\/classes\/module-info.java","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -65,0 +65,1 @@\n+import com.sun.tools.javac.resources.CompilerProperties;\n@@ -68,0 +69,1 @@\n+import com.sun.tools.javac.tree.JCTree;\n@@ -79,0 +81,1 @@\n+import static com.sun.tools.javac.code.Scope.LookupKind.NON_RECURSIVE;\n@@ -114,0 +117,4 @@\n+    \/** Switch: allow value classes.\n+     *\/\n+    boolean allowValueClasses;\n+\n@@ -294,0 +301,2 @@\n+        allowValueClasses = (!preview.isPreview(Feature.VALUE_CLASSES) || preview.isEnabled()) &&\n+                Feature.VALUE_CLASSES.allowedInSource(source);\n@@ -588,3 +597,5 @@\n-                    return (outer == Type.noType) ?\n-                            t.erasure(types) :\n-                        new ClassType(outer, List.nil(), t);\n+                    if (outer == Type.noType) {\n+                        ClassType et = (ClassType) t.erasure(types);\n+                        return new ClassType(et.getEnclosingType(), List.nil(), et.tsym, et.getMetadata());\n+                    }\n+                    return new ClassType(outer, List.nil(), t, List.nil());\n@@ -612,1 +623,1 @@\n-                outer = new ClassType(outer, actuals, t) {\n+                outer = new ClassType(outer, actuals, t, List.nil()) {\n@@ -696,1 +707,1 @@\n-                    outer = new ClassType(outer, List.nil(), t);\n+                    outer = new ClassType(outer, List.nil(), t, List.nil());\n@@ -1557,0 +1568,7 @@\n+            } else if (proxy.type.tsym.flatName() == syms.migratedValueClassInternalType.tsym.flatName()) {\n+                Assert.check(sym.kind == TYP);\n+                sym.flags_field |= MIGRATED_VALUE_CLASS;\n+                if (needsValueFlag(sym, sym.flags_field)) {\n+                    sym.flags_field |= VALUE_CLASS;\n+                    sym.flags_field &= ~IDENTITY_TYPE;\n+                }\n@@ -1573,0 +1591,6 @@\n+                }  else if (proxy.type.tsym == syms.migratedValueClassType.tsym && sym.kind == TYP) {\n+                    sym.flags_field |= MIGRATED_VALUE_CLASS;\n+                    if (needsValueFlag(sym, sym.flags_field)) {\n+                        sym.flags_field |= VALUE_CLASS;\n+                        sym.flags_field &= ~IDENTITY_TYPE;\n+                    }\n@@ -3061,1 +3085,1 @@\n-        long flags = adjustClassFlags(f);\n+        long flags = adjustClassFlags(c, f);\n@@ -3153,1 +3177,1 @@\n-            long flags = adjustClassFlags(nextChar());\n+            long flags = adjustClassFlags(c, nextChar());\n@@ -3295,0 +3319,5 @@\n+        boolean previewClassFile = minorVersion == ClassFile.PREVIEW_MINOR_VERSION;\n+        if (allowValueClasses && previewClassFile && (flags & ACC_STRICT) != 0) {\n+            flags &= ~ACC_STRICT;\n+            flags |= STRICT;\n+        }\n@@ -3310,1 +3339,1 @@\n-    long adjustClassFlags(long flags) {\n+    long adjustClassFlags(ClassSymbol c, long flags) {\n@@ -3315,1 +3344,23 @@\n-        return flags & ~ACC_SUPER; \/\/ SUPER and SYNCHRONIZED bits overloaded\n+        if (((flags & ACC_IDENTITY) != 0 && !isMigratedValueClass(flags)) || (majorVersion < V67.major && (flags & INTERFACE) == 0)) {\n+            flags |= IDENTITY_TYPE;\n+        } else if (needsValueFlag(c, flags)) {\n+            flags |= VALUE_CLASS;\n+            flags &= ~IDENTITY_TYPE;\n+        }\n+        flags &= ~ACC_IDENTITY; \/\/ ACC_IDENTITY and SYNCHRONIZED bits overloaded\n+        return flags;\n+    }\n+\n+    private boolean needsValueFlag(Symbol c, long flags) {\n+        boolean previewClassFile = minorVersion == ClassFile.PREVIEW_MINOR_VERSION;\n+        if (allowValueClasses) {\n+            if (previewClassFile && majorVersion >= V67.major && (flags & INTERFACE) == 0 ||\n+                    majorVersion >= V67.major && isMigratedValueClass(flags)) {\n+                return true;\n+            }\n+        }\n+        return false;\n+    }\n+\n+    private boolean isMigratedValueClass(long flags) {\n+        return allowValueClasses && ((flags & MIGRATED_VALUE_CLASS) != 0);\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/jvm\/ClassReader.java","additions":60,"deletions":9,"binary":false,"changes":69,"status":"modified"},{"patch":"@@ -779,1 +779,1 @@\n-    improperly formed type, some parameters are missing\n+    improperly formed type, some parameters are missing or misplaced\n@@ -2080,0 +2080,6 @@\n+compiler.warn.serializable.value.class.without.write.replace.1=\\\n+    serializable value class does not declare, or inherits, a writeReplace method\n+\n+compiler.warn.serializable.value.class.without.write.replace.2=\\\n+    serializable class does not declare, or inherits, a writeReplace method\n+\n@@ -2745,0 +2751,3 @@\n+compiler.misc.type.req.identity=\\\n+    a type with identity\n+\n@@ -3774,0 +3783,3 @@\n+compiler.misc.bad.access.flags=\\\n+    bad access flags combination: {0}\n+\n@@ -4107,0 +4119,19 @@\n+compiler.misc.feature.value.classes=\\\n+    value classes\n+\n+# 0: type, 1: type\n+compiler.err.value.type.has.identity.super.type=\\\n+    The identity type {1} cannot be a supertype of the value type {0}\n+\n+# 0: symbol, 1: type\n+compiler.err.concrete.supertype.for.value.class=\\\n+    The concrete class {1} is not allowed to be a super class of the value class {0} either directly or indirectly\n+\n+# 0: symbol, 1: symbol, 2: type\n+compiler.err.super.class.method.cannot.be.synchronized=\\\n+    The method {0} in the super class {2} of the value class {1} is synchronized. This is disallowed\n+\n+# 0: symbol or name\n+compiler.err.cant.ref.after.ctor.called=\\\n+    cannot assign to {0} after supertype constructor has been called\n+\n@@ -4131,0 +4162,3 @@\n+\n+compiler.warn.value.finalize=\\\n+    value classes should not have finalize methods, they are not invoked\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/resources\/compiler.properties","additions":35,"deletions":1,"binary":false,"changes":36,"status":"modified"},{"patch":"@@ -119,0 +119,28 @@\n+\n+# Valhalla...\n+compiler\/valhalla\/inlinetypes\/TestArrayCopyWithOops.java      8341844   generic-all\n+compiler\/valhalla\/inlinetypes\/TestCallingConvention.java      8341844   generic-all\n+compiler\/valhalla\/inlinetypes\/TestNullableInlineTypes.java    8341844   generic-all\n+runtime\/valhalla\/inlinetypes\/InlineTypesTest.java             8341844   generic-all\n+\n+compiler\/valhalla\/inlinetypes\/TestStressReturnBuffering.java  8341846   generic-all\n+\n+compiler\/valhalla\/inlinetypes\/TestUnloadedInlineTypeArray.java   8341848 generic-all\n+compiler\/valhalla\/inlinetypes\/TestArrays.java                 8341848   generic-all\n+compiler\/valhalla\/inlinetypes\/TestBasicFunctionality.java     8341848   generic-all\n+compiler\/valhalla\/inlinetypes\/TestLWorld.java                 8341848   generic-all\n+\n+compiler\/valhalla\/inlinetypes\/TestIntrinsics.java             8341849   generic-all\n+\n+compiler\/valhalla\/inlinetypes\/TestNullableArrays.java         8341850   generic-all\n+\n+compiler\/valhalla\/inlinetypes\/TestBufferTearing.java          8341947   generic-aarch64\n+compiler\/valhalla\/inlinetypes\/TestC1.java                     8341947   generic-aarch64\n+compiler\/valhalla\/inlinetypes\/TestGetfieldChains.java         8341947   generic-aarch64\n+compiler\/valhalla\/inlinetypes\/TestUnloadedInlineTypeField.java 8341947   generic-aarch64\n+runtime\/valhalla\/inlinetypes\/CircularityTest.java             8341947   generic-aarch64\n+runtime\/valhalla\/inlinetypes\/EmptyInlineTest.java             8341947   generic-aarch64\n+runtime\/valhalla\/inlinetypes\/FlattenableSemanticTest.java     8341947   generic-aarch64\n+runtime\/valhalla\/inlinetypes\/QuickeningTest.java              8341947   generic-aarch64\n+runtime\/valhalla\/inlinetypes\/UninitializedInlineFieldsTest.java 8341947   generic-aarch64\n+runtime\/valhalla\/inlinetypes\/ValueTearing.java                 8341947   generic-aarch64\n","filename":"test\/hotspot\/jtreg\/ProblemList-generational-zgc.txt","additions":28,"deletions":0,"binary":false,"changes":28,"status":"modified"},{"patch":"@@ -81,0 +81,7 @@\n+compiler\/valhalla\/inlinetypes\/TestMethodHandles.java 8341949 generic-all\n+compiler\/valhalla\/inlinetypes\/TestCallingConvention.java   8342064 generic-all\n+compiler\/valhalla\/inlinetypes\/TestIntrinsics.java          8342064 generic-all\n+compiler\/valhalla\/inlinetypes\/TestNullableInlineTypes.java 8342064 generic-all\n+compiler\/valhalla\/inlinetypes\/TestNullableArrays.java      8342064 generic-all\n+\n+compiler\/c2\/irTests\/scalarReplacement\/ScalarReplacementWithGCBarrierTests.java  8342488 generic-all\n@@ -107,0 +114,1 @@\n+runtime\/cds\/appcds\/redefineClass\/RedefineRunningMethods_Shared.java  8304168 generic-all\n@@ -127,0 +135,5 @@\n+\n+# Valhalla\n+runtime\/AccModule\/ConstModule.java 8294051 generic-all\n+runtime\/valhalla\/inlinetypes\/InlineOops.java#ZGen 8313607 linux-aarch64,macosx-aarch64\n+\n@@ -152,0 +165,31 @@\n+# Valhalla TODO:\n+serviceability\/jvmti\/valhalla\/HeapDump\/HeapDump.java 8317416 generic-all\n+\n+serviceability\/sa\/ClhsdbCDSCore.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbCDSJstackPrintAll.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbFindPC.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbInspect.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbLongConstant.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbJdis.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbJstack.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbPrintAll.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbPrintAs.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbPrintStatics.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbSource.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbSymbol.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbWhere.java 8190936 generic-all\n+serviceability\/sa\/JhsdbThreadInfoTest.java 8190936 generic-all\n+serviceability\/sa\/TestClassDump.java 8190936 generic-all\n+serviceability\/sa\/TestClhsdbJstackLock.java 8190936 generic-all\n+serviceability\/sa\/TestCpoolForInvokeDynamic.java 8190936 generic-all\n+serviceability\/sa\/TestHeapDumpForInvokeDynamic.java 8190936 generic-all\n+serviceability\/sa\/TestHeapDumpForLargeArray.java 8190936 generic-all\n+serviceability\/sa\/TestIntConstant.java 8190936 generic-all\n+serviceability\/sa\/TestJhsdbJstackLock.java 8190936 generic-all\n+serviceability\/sa\/TestJmapCore.java 8190936 generic-all\n+serviceability\/sa\/TestJmapCoreMetaspace.java 8190936 generic-all\n+serviceability\/sa\/TestPrintMdo.java 8190936 generic-all\n+serviceability\/sa\/jmap-hprof\/JMapHProfLargeHeapTest.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbDumpclass.java 8190936 generic-all\n+\n+\n@@ -190,0 +234,2 @@\n+vmTestbase\/vm\/mlvm\/hiddenloader\/stress\/byteMutation\/Test.java 8317172 generic-all\n+\n","filename":"test\/hotspot\/jtreg\/ProblemList.txt","additions":46,"deletions":0,"binary":false,"changes":46,"status":"modified"},{"patch":"@@ -129,1 +129,9 @@\n-    jni\/nullCaller\n+    jni\/nullCaller \\\n+    valhalla\n+\n+# valhalla lworld tests\n+jdk_valhalla = \\\n+    java\/lang\/invoke \\\n+    valhalla \\\n+    java\/lang\/instrument\/valhalla\n+\n","filename":"test\/jdk\/TEST.groups","additions":9,"deletions":1,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -388,0 +388,1 @@\n+        vmOptFinalFlag(map, \"TieredCompilation\");\n","filename":"test\/jtreg-ext\/requires\/VMProps.java","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -70,1 +70,1 @@\n-\n+tools\/javap\/UndefinedAccessFlagTest.java                                        8342036    generic-all\n","filename":"test\/langtools\/ProblemList.txt","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -62,0 +62,1 @@\n+compiler.misc.bad.access.flags                          # bad class file\n@@ -214,0 +215,1 @@\n+\n@@ -221,0 +223,2 @@\n+# Value Objects\n+compiler.misc.feature.value.classes\n","filename":"test\/langtools\/tools\/javac\/diags\/examples.not-yet.txt","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -160,0 +160,14 @@\n+  private native Object[] getObjectsViaKlassOopMaps0(Object thing);\n+  public Object[] getObjectsViaKlassOopMaps(Object thing) {\n+    Objects.requireNonNull(thing);\n+    return getObjectsViaKlassOopMaps0(thing);\n+  }\n+\n+  private native Object[] getObjectsViaOopIterator0(Object thing);\n+  public Object[] getObjectsViaOopIterator(Object thing) {\n+    Objects.requireNonNull(thing);\n+    return getObjectsViaOopIterator0(thing);\n+  }\n+\n+  public native Object[] getObjectsViaFrameOopIterator(int depth);\n+\n","filename":"test\/lib\/jdk\/test\/whitebox\/WhiteBox.java","additions":14,"deletions":0,"binary":false,"changes":14,"status":"modified"}]}