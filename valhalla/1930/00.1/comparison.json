{"files":[{"patch":"@@ -58,4 +58,4 @@\n-          - arm\n-          - s390x\n-          - ppc64le\n-          - riscv64\n+          # - arm\n+          # - s390x\n+          # - ppc64le\n+          # - riscv64\n@@ -69,25 +69,25 @@\n-          - target-cpu: arm\n-            gnu-arch: arm\n-            debian-arch: armhf\n-            debian-repository: https:\/\/httpredir.debian.org\/debian\/\n-            debian-version: trixie\n-            tolerate-sysroot-errors: false\n-            gnu-abi: eabihf\n-          - target-cpu: s390x\n-            gnu-arch: s390x\n-            debian-arch: s390x\n-            debian-repository: https:\/\/httpredir.debian.org\/debian\/\n-            debian-version: trixie\n-            tolerate-sysroot-errors: false\n-          - target-cpu: ppc64le\n-            gnu-arch: powerpc64le\n-            debian-arch: ppc64el\n-            debian-repository: https:\/\/httpredir.debian.org\/debian\/\n-            debian-version: trixie\n-            tolerate-sysroot-errors: false\n-          - target-cpu: riscv64\n-            gnu-arch: riscv64\n-            debian-arch: riscv64\n-            debian-repository: https:\/\/httpredir.debian.org\/debian\/\n-            debian-version: trixie\n-            tolerate-sysroot-errors: false\n+          # - target-cpu: arm\n+          #   gnu-arch: arm\n+          #   debian-arch: armhf\n+          #   debian-repository: https:\/\/httpredir.debian.org\/debian\/\n+          #   debian-version: trixie\n+          #   tolerate-sysroot-errors: false\n+          #   gnu-abi: eabihf\n+          # - target-cpu: s390x\n+          #   gnu-arch: s390x\n+          #   debian-arch: s390x\n+          #   debian-repository: https:\/\/httpredir.debian.org\/debian\/\n+          #   debian-version: trixie\n+          #   tolerate-sysroot-errors: false\n+          # - target-cpu: ppc64le\n+          #   gnu-arch: powerpc64le\n+          #   debian-arch: ppc64el\n+          #   debian-repository: https:\/\/httpredir.debian.org\/debian\/\n+          #   debian-version: trixie\n+          #   tolerate-sysroot-errors: false\n+          # - target-cpu: riscv64\n+          #   gnu-arch: riscv64\n+          #   debian-arch: riscv64\n+          #   debian-repository: https:\/\/httpredir.debian.org\/debian\/\n+          #   debian-version: trixie\n+          #   tolerate-sysroot-errors: false\n","filename":".github\/workflows\/build-cross-compile.yml","additions":29,"deletions":29,"binary":false,"changes":58,"status":"modified"},{"patch":"@@ -146,0 +146,1 @@\n+          --disable-jvm-feature-shenandoahgc\n","filename":".github\/workflows\/build-linux.yml","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -113,0 +113,1 @@\n+          --disable-jvm-feature-shenandoahgc\n","filename":".github\/workflows\/build-macos.yml","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2025, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2026, Oracle and\/or its affiliates. All rights reserved.\n@@ -29,0 +29,1 @@\n+#include \"ci\/ciInlineKlass.hpp\"\n@@ -50,0 +51,2 @@\n+#include \"oops\/resolvedFieldEntry.hpp\"\n+#include \"runtime\/arguments.hpp\"\n@@ -51,0 +54,1 @@\n+#include \"runtime\/globals.hpp\"\n@@ -56,0 +60,1 @@\n+#include \"runtime\/signature_cc.hpp\"\n@@ -59,0 +64,1 @@\n+#include \"vmreg_aarch64.inline.hpp\"\n@@ -2031,1 +2037,5 @@\n-  if (!VerifyOops) return;\n+  if (!VerifyOops || VerifyAdapterSharing) {\n+    \/\/ Below address of the code string confuses VerifyAdapterSharing\n+    \/\/ because it may differ between otherwise equivalent adapters.\n+    return;\n+  }\n@@ -2064,1 +2074,5 @@\n-  if (!VerifyOops) return;\n+  if (!VerifyOops || VerifyAdapterSharing) {\n+    \/\/ Below address of the code string confuses VerifyAdapterSharing\n+    \/\/ because it may differ between otherwise equivalent adapters.\n+    return;\n+  }\n@@ -2162,0 +2176,4 @@\n+void MacroAssembler::super_call_VM_leaf(address entry_point) {\n+  MacroAssembler::call_VM_leaf_base(entry_point, 1);\n+}\n+\n@@ -2207,0 +2225,88 @@\n+void MacroAssembler::test_markword_is_inline_type(Register markword, Label& is_inline_type) {\n+  assert_different_registers(markword, rscratch2);\n+  mov(rscratch2, markWord::inline_type_mask_in_place);\n+  andr(markword, markword, rscratch2);\n+  mov(rscratch2, markWord::inline_type_pattern);\n+  cmp(markword, rscratch2);\n+  br(Assembler::EQ, is_inline_type);\n+}\n+\n+void MacroAssembler::test_oop_is_not_inline_type(Register object, Register tmp, Label& not_inline_type, bool can_be_null) {\n+  assert_different_registers(tmp, rscratch1);\n+  if (can_be_null) {\n+    cbz(object, not_inline_type);\n+  }\n+  const int is_inline_type_mask = markWord::inline_type_pattern;\n+  ldr(tmp, Address(object, oopDesc::mark_offset_in_bytes()));\n+  mov(rscratch1, is_inline_type_mask);\n+  andr(tmp, tmp, rscratch1);\n+  cmp(tmp, rscratch1);\n+  br(Assembler::NE, not_inline_type);\n+}\n+\n+void MacroAssembler::test_field_is_null_free_inline_type(Register flags, Register temp_reg, Label& is_null_free_inline_type) {\n+  assert(temp_reg == noreg, \"not needed\"); \/\/ keep signature uniform with x86\n+  tbnz(flags, ResolvedFieldEntry::is_null_free_inline_type_shift, is_null_free_inline_type);\n+}\n+\n+void MacroAssembler::test_field_is_not_null_free_inline_type(Register flags, Register temp_reg, Label& not_null_free_inline_type) {\n+  assert(temp_reg == noreg, \"not needed\"); \/\/ keep signature uniform with x86\n+  tbz(flags, ResolvedFieldEntry::is_null_free_inline_type_shift, not_null_free_inline_type);\n+}\n+\n+void MacroAssembler::test_field_is_flat(Register flags, Register temp_reg, Label& is_flat) {\n+  assert(temp_reg == noreg, \"not needed\"); \/\/ keep signature uniform with x86\n+  tbnz(flags, ResolvedFieldEntry::is_flat_shift, is_flat);\n+}\n+\n+void MacroAssembler::test_field_has_null_marker(Register flags, Register temp_reg, Label& has_null_marker) {\n+  assert(temp_reg == noreg, \"not needed\"); \/\/ keep signature uniform with x86\n+  tbnz(flags, ResolvedFieldEntry::has_null_marker_shift, has_null_marker);\n+}\n+\n+void MacroAssembler::test_oop_prototype_bit(Register oop, Register temp_reg, int32_t test_bit, bool jmp_set, Label& jmp_label) {\n+  Label test_mark_word;\n+  \/\/ load mark word\n+  ldr(temp_reg, Address(oop, oopDesc::mark_offset_in_bytes()));\n+  \/\/ check displaced\n+  tst(temp_reg, markWord::unlocked_value);\n+  br(Assembler::NE, test_mark_word);\n+  \/\/ slow path use klass prototype\n+  load_prototype_header(temp_reg, oop);\n+\n+  bind(test_mark_word);\n+  andr(temp_reg, temp_reg, test_bit);\n+  if (jmp_set) {\n+    cbnz(temp_reg, jmp_label);\n+  } else {\n+    cbz(temp_reg, jmp_label);\n+  }\n+}\n+\n+void MacroAssembler::test_flat_array_oop(Register oop, Register temp_reg, Label& is_flat_array) {\n+  test_oop_prototype_bit(oop, temp_reg, markWord::flat_array_bit_in_place, true, is_flat_array);\n+}\n+\n+void MacroAssembler::test_non_flat_array_oop(Register oop, Register temp_reg,\n+                                                  Label&is_non_flat_array) {\n+  test_oop_prototype_bit(oop, temp_reg, markWord::flat_array_bit_in_place, false, is_non_flat_array);\n+}\n+\n+void MacroAssembler::test_null_free_array_oop(Register oop, Register temp_reg, Label& is_null_free_array) {\n+  test_oop_prototype_bit(oop, temp_reg, markWord::null_free_array_bit_in_place, true, is_null_free_array);\n+}\n+\n+void MacroAssembler::test_non_null_free_array_oop(Register oop, Register temp_reg, Label&is_non_null_free_array) {\n+  test_oop_prototype_bit(oop, temp_reg, markWord::null_free_array_bit_in_place, false, is_non_null_free_array);\n+}\n+\n+void MacroAssembler::test_flat_array_layout(Register lh, Label& is_flat_array) {\n+  tst(lh, Klass::_lh_array_tag_flat_value_bit_inplace);\n+  br(Assembler::NE, is_flat_array);\n+}\n+\n+void MacroAssembler::test_non_flat_array_layout(Register lh, Label& is_non_flat_array) {\n+  tst(lh, Klass::_lh_array_tag_flat_value_bit_inplace);\n+  br(Assembler::EQ, is_non_flat_array);\n+}\n+\n@@ -4907,0 +5013,10 @@\n+void MacroAssembler::load_metadata(Register dst, Register src) {\n+  if (UseCompactObjectHeaders) {\n+    load_narrow_klass_compact(dst, src);\n+  } else if (UseCompressedClassPointers) {\n+    ldrw(dst, Address(src, oopDesc::klass_offset_in_bytes()));\n+  } else {\n+    ldr(dst, Address(src, oopDesc::klass_offset_in_bytes()));\n+  }\n+}\n+\n@@ -5017,0 +5133,5 @@\n+void MacroAssembler::load_prototype_header(Register dst, Register src) {\n+  load_klass(dst, src);\n+  ldr(dst, Address(dst, Klass::prototype_header_offset()));\n+}\n+\n@@ -5417,0 +5538,40 @@\n+void MacroAssembler::flat_field_copy(DecoratorSet decorators, Register src, Register dst,\n+                                     Register inline_layout_info) {\n+  BarrierSetAssembler* bs = BarrierSet::barrier_set()->barrier_set_assembler();\n+  bs->flat_field_copy(this, decorators, src, dst, inline_layout_info);\n+}\n+\n+void MacroAssembler::payload_offset(Register inline_klass, Register offset) {\n+  ldr(offset, Address(inline_klass, InlineKlass::adr_members_offset()));\n+  ldrw(offset, Address(offset, InlineKlass::payload_offset_offset()));\n+}\n+\n+void MacroAssembler::payload_address(Register oop, Register data, Register inline_klass) {\n+  \/\/ ((address) (void*) o) + vk->payload_offset();\n+  Register offset = (data == oop) ? rscratch1 : data;\n+  payload_offset(inline_klass, offset);\n+  if (data == oop) {\n+    add(data, data, offset);\n+  } else {\n+    lea(data, Address(oop, offset));\n+  }\n+}\n+\n+void MacroAssembler::data_for_value_array_index(Register array, Register array_klass,\n+                                                Register index, Register data) {\n+  assert_different_registers(array, array_klass, index);\n+  assert_different_registers(rscratch1, array, index);\n+\n+  \/\/ array->base() + (index << Klass::layout_helper_log2_element_size(lh));\n+  ldrw(rscratch1, Address(array_klass, Klass::layout_helper_offset()));\n+\n+  \/\/ Klass::layout_helper_log2_element_size(lh)\n+  \/\/ (lh >> _lh_log2_element_size_shift) & _lh_log2_element_size_mask;\n+  lsr(rscratch1, rscratch1, Klass::_lh_log2_element_size_shift);\n+  andr(rscratch1, rscratch1, Klass::_lh_log2_element_size_mask);\n+  lslv(index, index, rscratch1);\n+\n+  add(data, array, index);\n+  add(data, data, arrayOopDesc::base_offset_in_bytes(T_FLAT_ELEMENT));\n+}\n+\n@@ -5492,0 +5653,102 @@\n+\/\/ Object \/ value buffer allocation...\n+void MacroAssembler::allocate_instance(Register klass, Register new_obj,\n+                                       Register t1, Register t2,\n+                                       bool clear_fields, Label& alloc_failed)\n+{\n+  Label done, initialize_header, initialize_object, slow_case, slow_case_no_pop;\n+  Register layout_size = t1;\n+  assert(new_obj == r0, \"needs to be r0\");\n+  assert_different_registers(klass, new_obj, t1, t2);\n+\n+  \/\/ get instance_size in InstanceKlass (scaled to a count of bytes)\n+  ldrw(layout_size, Address(klass, Klass::layout_helper_offset()));\n+  \/\/ test to see if it is malformed in some way\n+  tst(layout_size, Klass::_lh_instance_slow_path_bit);\n+  br(Assembler::NE, slow_case_no_pop);\n+\n+  \/\/ Allocate the instance:\n+  \/\/  If TLAB is enabled:\n+  \/\/    Try to allocate in the TLAB.\n+  \/\/    If fails, go to the slow path.\n+  \/\/    Initialize the allocation.\n+  \/\/    Exit.\n+  \/\/\n+  \/\/  Go to slow path.\n+\n+  if (UseTLAB) {\n+    push(klass);\n+    tlab_allocate(new_obj, layout_size, 0, klass, t2, slow_case);\n+    if (ZeroTLAB || (!clear_fields)) {\n+      \/\/ the fields have been already cleared\n+      b(initialize_header);\n+    } else {\n+      \/\/ initialize both the header and fields\n+      b(initialize_object);\n+    }\n+\n+    if (clear_fields) {\n+      \/\/ The object is initialized before the header.  If the object size is\n+      \/\/ zero, go directly to the header initialization.\n+      bind(initialize_object);\n+      int header_size = oopDesc::header_size() * HeapWordSize;\n+      assert(is_aligned(header_size, BytesPerLong), \"oop header size must be 8-byte-aligned\");\n+      subs(layout_size, layout_size, header_size);\n+      br(Assembler::EQ, initialize_header);\n+\n+      \/\/ Initialize topmost object field, divide size by 8, check if odd and\n+      \/\/ test if zero.\n+\n+  #ifdef ASSERT\n+      \/\/ make sure instance_size was multiple of 8\n+      Label L;\n+      tst(layout_size, 7);\n+      br(Assembler::EQ, L);\n+      stop(\"object size is not multiple of 8 - adjust this code\");\n+      bind(L);\n+      \/\/ must be > 0, no extra check needed here\n+  #endif\n+\n+      lsr(layout_size, layout_size, LogBytesPerLong);\n+\n+      \/\/ initialize remaining object fields: instance_size was a multiple of 8\n+      {\n+        Label loop;\n+        Register base = t2;\n+\n+        bind(loop);\n+        add(rscratch1, new_obj, layout_size, Assembler::LSL, LogBytesPerLong);\n+        str(zr, Address(rscratch1, header_size - 1*oopSize));\n+        subs(layout_size, layout_size, 1);\n+        br(Assembler::NE, loop);\n+      }\n+    } \/\/ clear_fields\n+\n+    \/\/ initialize object header only.\n+    bind(initialize_header);\n+    pop(klass);\n+    Register mark_word = t2;\n+    if (UseCompactObjectHeaders || Arguments::is_valhalla_enabled()) {\n+      ldr(mark_word, Address(klass, Klass::prototype_header_offset()));\n+      str(mark_word, Address(new_obj, oopDesc::mark_offset_in_bytes()));\n+    } else {\n+      mov(mark_word, (intptr_t)markWord::prototype().value());\n+      str(mark_word, Address(new_obj, oopDesc::mark_offset_in_bytes()));\n+    }\n+    if (!UseCompactObjectHeaders) {\n+      store_klass_gap(new_obj, zr);  \/\/ zero klass gap for compressed oops\n+      mov(t2, klass);                \/\/ preserve klass\n+      store_klass(new_obj, t2);      \/\/ src klass reg is potentially compressed\n+    }\n+    b(done);\n+  }\n+\n+  if (UseTLAB) {\n+    bind(slow_case);\n+    pop(klass);\n+  }\n+  bind(slow_case_no_pop);\n+  b(alloc_failed);\n+\n+  bind(done);\n+}\n+\n@@ -5531,0 +5794,20 @@\n+void MacroAssembler::get_inline_type_field_klass(Register holder_klass, Register index, Register inline_klass) {\n+  inline_layout_info(holder_klass, index, inline_klass);\n+  ldr(inline_klass, Address(inline_klass, InlineLayoutInfo::klass_offset()));\n+}\n+\n+void MacroAssembler::inline_layout_info(Register holder_klass, Register index, Register layout_info) {\n+  assert_different_registers(holder_klass, index, layout_info);\n+  InlineLayoutInfo array[2];\n+  int size = (char*)&array[1] - (char*)&array[0]; \/\/ computing size of array elements\n+  if (is_power_of_2(size)) {\n+    lsl(index, index, log2i_exact(size)); \/\/ Scale index by power of 2\n+  } else {\n+    mov(layout_info, size);\n+    mul(index, index, layout_info); \/\/ Scale the index to be the entry index * array_element_size\n+  }\n+  ldr(layout_info, Address(holder_klass, InstanceKlass::inline_layout_info_array_offset()));\n+  add(layout_info, layout_info, Array<InlineLayoutInfo>::base_offset_in_bytes());\n+  lea(layout_info, Address(layout_info, index));\n+}\n+\n@@ -5617,0 +5900,1 @@\n+#ifdef ASSERT\n@@ -5618,0 +5902,5 @@\n+  build_frame(framesize, false);\n+}\n+#endif\n+\n+void MacroAssembler::build_frame(int framesize DEBUG_ONLY(COMMA bool zap_rfp_lr_spills)) {\n@@ -5623,1 +5912,6 @@\n-    stp(rfp, lr, Address(sp, framesize - 2 * wordSize));\n+    if (DEBUG_ONLY(zap_rfp_lr_spills ||) false) {\n+      mov_immediate64(rscratch1, ((uint64_t)badRegWordVal) << 32 | (uint64_t)badRegWordVal);\n+      stp(rscratch1, rscratch1, Address(sp, framesize - 2 * wordSize));\n+    } else {\n+      stp(rfp, lr, Address(sp, framesize - 2 * wordSize));\n+    }\n@@ -5626,1 +5920,6 @@\n-    stp(rfp, lr, Address(pre(sp, -2 * wordSize)));\n+    if (DEBUG_ONLY(zap_rfp_lr_spills ||) false) {\n+      mov_immediate64(rscratch1, ((uint64_t)badRegWordVal) << 32 | (uint64_t)badRegWordVal);\n+      stp(rscratch1, rscratch1, Address(pre(sp, -2 * wordSize)));\n+    } else {\n+      stp(rfp, lr, Address(pre(sp, -2 * wordSize)));\n+    }\n@@ -5656,0 +5955,82 @@\n+void MacroAssembler::remove_frame(int initial_framesize, bool needs_stack_repair) {\n+  if (needs_stack_repair) {\n+    \/\/ The method has a scalarized entry point (where fields of value object arguments\n+    \/\/ are passed through registers and stack), and a non-scalarized entry point (where\n+    \/\/ value object arguments are given as oops). The non-scalarized entry point will\n+    \/\/ first load each field of value object arguments and store them in registers and on\n+    \/\/ the stack in a way compatible with the scalarized entry point. To do so, some extra\n+    \/\/ stack space might be reserved (if argument registers are not enough). On leaving the\n+    \/\/ method, this space must be freed.\n+    \/\/\n+    \/\/ In case we used the non-scalarized entry point the stack looks like this:\n+    \/\/\n+    \/\/ | Arguments from caller     |\n+    \/\/ |---------------------------|  <-- caller's SP\n+    \/\/ | Saved LR #1               |\n+    \/\/ | Saved FP #1               |\n+    \/\/ |---------------------------|\n+    \/\/ | Extension space for       |\n+    \/\/ |   inline arg (un)packing  |\n+    \/\/ |---------------------------|  <-- start of this method's frame\n+    \/\/ | Saved LR #2               |\n+    \/\/ | Saved FP #2               |\n+    \/\/ |---------------------------|  <-- FP\n+    \/\/ | sp_inc                    |\n+    \/\/ | method locals             |\n+    \/\/ |---------------------------|  <-- SP\n+    \/\/\n+    \/\/ There are two copies of FP and LR on the stack. They will be identical at\n+    \/\/ first, but that can change.\n+    \/\/ If the caller has been deoptimized, LR #1 will be patched to point at the\n+    \/\/ deopt blob, and LR #2 will still point into the old method.\n+    \/\/ If the saved FP (x29) was not used as the frame pointer, but to store an\n+    \/\/ oop, the GC will be aware only of FP #1 as the spilled location of x29 and\n+    \/\/ will fix only this one. Overall, FP\/LR #2 are not reliable and are simply\n+    \/\/ needed to add space between the extension space and the locals, as there\n+    \/\/ would be between the real arguments and the locals if we don't need to\n+    \/\/ do unpacking (from the scalarized entry point).\n+    \/\/\n+    \/\/ When restoring, one must then load FP #1 into x29, and LR #1 into x30,\n+    \/\/ while keeping in mind that from the scalarized entry point, there will be\n+    \/\/ only one copy of each. Indeed, in the case we used the scalarized calling\n+    \/\/ convention, the stack looks like this:\n+    \/\/\n+    \/\/ | Arguments from caller     |\n+    \/\/ |---------------------------|  <-- caller's SP \/ start of this method's frame\n+    \/\/ | Saved LR                  |\n+    \/\/ | Saved FP                  |\n+    \/\/ |---------------------------|  <-- FP\n+    \/\/ | sp_inc                    |\n+    \/\/ | method locals             |\n+    \/\/ |---------------------------|  <-- SP\n+    \/\/\n+    \/\/ The sp_inc stack slot holds the total size of the frame including the\n+    \/\/ extension space minus two words for the saved FP and LR. That is how to\n+    \/\/ find FP\/LR #1. This size is expressed in bytes. Be careful when using it\n+    \/\/ from C++ in pointer arithmetic; you might need to divide it by wordSize.\n+    \/\/\n+    \/\/ One can find sp_inc since the start the method's frame is SP + initial_framesize.\n+\n+    int sp_inc_offset = initial_framesize - 3 * wordSize;  \/\/ Immediately below saved LR and FP\n+\n+    ldr(rscratch1, Address(sp, sp_inc_offset));\n+    add(sp, sp, rscratch1);\n+    ldp(rfp, lr, Address(post(sp, 2 * wordSize)));\n+  } else {\n+    remove_frame(initial_framesize);\n+  }\n+}\n+\n+void MacroAssembler::save_stack_increment(int sp_inc, int frame_size) {\n+  int real_frame_size = frame_size + sp_inc;\n+  assert(sp_inc == 0 || sp_inc > 2*wordSize, \"invalid sp_inc value\");\n+  assert(real_frame_size >= 2*wordSize, \"frame size must include FP\/LR space\");\n+  assert((real_frame_size & (StackAlignmentInBytes-1)) == 0, \"frame size not aligned\");\n+\n+  int sp_inc_offset = frame_size - 3 * wordSize;  \/\/ Immediately below saved LR and FP\n+\n+  \/\/ Subtract two words for the saved FP and LR as these will be popped\n+  \/\/ separately. See remove_frame above.\n+  mov(rscratch1, real_frame_size - 2*wordSize);\n+  str(rscratch1, Address(sp, sp_inc_offset));\n+}\n@@ -6569,0 +6950,457 @@\n+#ifdef COMPILER2\n+\/\/ C2 compiled method's prolog code\n+\/\/ Moved here from aarch64.ad to support Valhalla code belows\n+void MacroAssembler::verified_entry(Compile* C, int sp_inc) {\n+  if (C->clinit_barrier_on_entry()) {\n+    assert(!C->method()->holder()->is_not_initialized(), \"initialization should have been started\");\n+\n+    Label L_skip_barrier;\n+\n+    mov_metadata(rscratch2, C->method()->holder()->constant_encoding());\n+    clinit_barrier(rscratch2, rscratch1, &L_skip_barrier);\n+    far_jump(RuntimeAddress(SharedRuntime::get_handle_wrong_method_stub()));\n+    bind(L_skip_barrier);\n+  }\n+\n+  if (C->max_vector_size() > 0) {\n+    reinitialize_ptrue();\n+  }\n+\n+  int bangsize = C->output()->bang_size_in_bytes();\n+  if (C->output()->need_stack_bang(bangsize))\n+    generate_stack_overflow_check(bangsize);\n+\n+  \/\/ n.b. frame size includes space for return pc and rfp\n+  const long framesize = C->output()->frame_size_in_bytes();\n+  build_frame(framesize DEBUG_ONLY(COMMA sp_inc != 0));\n+\n+  if (C->needs_stack_repair()) {\n+    save_stack_increment(sp_inc, framesize);\n+  }\n+\n+  if (VerifyStackAtCalls) {\n+    Unimplemented();\n+  }\n+}\n+#endif \/\/ COMPILER2\n+\n+int MacroAssembler::store_inline_type_fields_to_buf(ciInlineKlass* vk, bool from_interpreter) {\n+  assert(InlineTypeReturnedAsFields, \"Inline types should never be returned as fields\");\n+  \/\/ An inline type might be returned. If fields are in registers we\n+  \/\/ need to allocate an inline type instance and initialize it with\n+  \/\/ the value of the fields.\n+  Label skip;\n+  \/\/ We only need a new buffered inline type if a new one is not returned\n+  tbz(r0, 0, skip);\n+  int call_offset = -1;\n+\n+  \/\/ Be careful not to clobber r1-7 which hold returned fields\n+  \/\/ Also do not use callee-saved registers as these may be live in the interpreter\n+  Register tmp1 = r13, tmp2 = r14, klass = r15, r0_preserved = r12;\n+\n+  \/\/ The following code is similar to allocate_instance but has some slight differences,\n+  \/\/ e.g. object size is always not zero, sometimes it's constant; storing klass ptr after\n+  \/\/ allocating is not necessary if vk != nullptr, etc. allocate_instance is not aware of these.\n+  Label slow_case;\n+  \/\/ 1. Try to allocate a new buffered inline instance either from TLAB or eden space\n+  mov(r0_preserved, r0); \/\/ save r0 for slow_case since *_allocate may corrupt it when allocation failed\n+\n+  if (vk != nullptr) {\n+    \/\/ Called from C1, where the return type is statically known.\n+    movptr(klass, (intptr_t)vk->get_InlineKlass());\n+    jint lh = vk->layout_helper();\n+    assert(lh != Klass::_lh_neutral_value, \"inline class in return type must have been resolved\");\n+    if (UseTLAB && !Klass::layout_helper_needs_slow_path(lh)) {\n+      tlab_allocate(r0, noreg, lh, tmp1, tmp2, slow_case);\n+    } else {\n+      b(slow_case);\n+    }\n+  } else {\n+    \/\/ Call from interpreter. R0 contains ((the InlineKlass* of the return type) | 0x01)\n+    andr(klass, r0, -2);\n+    if (UseTLAB) {\n+      ldrw(tmp2, Address(klass, Klass::layout_helper_offset()));\n+      tst(tmp2, Klass::_lh_instance_slow_path_bit);\n+      br(Assembler::NE, slow_case);\n+      tlab_allocate(r0, tmp2, 0, tmp1, tmp2, slow_case);\n+    } else {\n+      b(slow_case);\n+    }\n+  }\n+  if (UseTLAB) {\n+    \/\/ 2. Initialize buffered inline instance header\n+    Register buffer_obj = r0;\n+    if (UseCompactObjectHeaders) {\n+      ldr(rscratch1, Address(klass, Klass::prototype_header_offset()));\n+      str(rscratch1, Address(buffer_obj, oopDesc::mark_offset_in_bytes()));\n+    } else {\n+      mov(rscratch1, (intptr_t)markWord::inline_type_prototype().value());\n+      str(rscratch1, Address(buffer_obj, oopDesc::mark_offset_in_bytes()));\n+      store_klass_gap(buffer_obj, zr);\n+      if (vk == nullptr) {\n+        \/\/ store_klass corrupts klass, so save it for later use (interpreter case only).\n+        mov(tmp1, klass);\n+      }\n+      store_klass(buffer_obj, klass);\n+      klass = tmp1;\n+    }\n+    \/\/ 3. Initialize its fields with an inline class specific handler\n+    if (vk != nullptr) {\n+      far_call(RuntimeAddress(vk->pack_handler())); \/\/ no need for call info as this will not safepoint.\n+    } else {\n+      ldr(tmp1, Address(klass, InlineKlass::adr_members_offset()));\n+      ldr(tmp1, Address(tmp1, InlineKlass::pack_handler_offset()));\n+      blr(tmp1);\n+    }\n+\n+    membar(Assembler::StoreStore);\n+    b(skip);\n+  } else {\n+    \/\/ Must have already branched to slow_case above.\n+    DEBUG_ONLY(should_not_reach_here());\n+  }\n+  bind(slow_case);\n+  \/\/ We failed to allocate a new inline type, fall back to a runtime\n+  \/\/ call. Some oop field may be live in some registers but we can't\n+  \/\/ tell. That runtime call will take care of preserving them\n+  \/\/ across a GC if there's one.\n+  mov(r0, r0_preserved);\n+\n+  if (from_interpreter) {\n+    super_call_VM_leaf(StubRoutines::store_inline_type_fields_to_buf());\n+  } else {\n+    far_call(RuntimeAddress(StubRoutines::store_inline_type_fields_to_buf()));\n+    call_offset = offset();\n+  }\n+  membar(Assembler::StoreStore);\n+\n+  bind(skip);\n+  return call_offset;\n+}\n+\n+\/\/ Move a value between registers\/stack slots and update the reg_state\n+bool MacroAssembler::move_helper(VMReg from, VMReg to, BasicType bt, RegState reg_state[]) {\n+  assert(from->is_valid() && to->is_valid(), \"source and destination must be valid\");\n+  if (reg_state[to->value()] == reg_written) {\n+    return true; \/\/ Already written\n+  }\n+\n+  if (from != to && bt != T_VOID) {\n+    if (reg_state[to->value()] == reg_readonly) {\n+      return false; \/\/ Not yet writable\n+    }\n+    if (from->is_reg()) {\n+      if (to->is_reg()) {\n+        if (from->is_Register() && to->is_Register()) {\n+          mov(to->as_Register(), from->as_Register());\n+        } else if (from->is_FloatRegister() && to->is_FloatRegister()) {\n+          fmovd(to->as_FloatRegister(), from->as_FloatRegister());\n+        } else {\n+          ShouldNotReachHere();\n+        }\n+      } else {\n+        int st_off = to->reg2stack() * VMRegImpl::stack_slot_size;\n+        Address to_addr = Address(sp, st_off);\n+        if (from->is_FloatRegister()) {\n+          if (bt == T_DOUBLE) {\n+             strd(from->as_FloatRegister(), to_addr);\n+          } else {\n+             assert(bt == T_FLOAT, \"must be float\");\n+             strs(from->as_FloatRegister(), to_addr);\n+          }\n+        } else {\n+          str(from->as_Register(), to_addr);\n+        }\n+      }\n+    } else {\n+      Address from_addr = Address(sp, from->reg2stack() * VMRegImpl::stack_slot_size);\n+      if (to->is_reg()) {\n+        if (to->is_FloatRegister()) {\n+          if (bt == T_DOUBLE) {\n+            ldrd(to->as_FloatRegister(), from_addr);\n+          } else {\n+            assert(bt == T_FLOAT, \"must be float\");\n+            ldrs(to->as_FloatRegister(), from_addr);\n+          }\n+        } else {\n+          ldr(to->as_Register(), from_addr);\n+        }\n+      } else {\n+        int st_off = to->reg2stack() * VMRegImpl::stack_slot_size;\n+        ldr(rscratch1, from_addr);\n+        str(rscratch1, Address(sp, st_off));\n+      }\n+    }\n+  }\n+\n+  \/\/ Update register states\n+  reg_state[from->value()] = reg_writable;\n+  reg_state[to->value()] = reg_written;\n+  return true;\n+}\n+\n+\/\/ Calculate the extra stack space required for packing or unpacking inline\n+\/\/ args and adjust the stack pointer\n+int MacroAssembler::extend_stack_for_inline_args(int args_on_stack) {\n+  int sp_inc = args_on_stack * VMRegImpl::stack_slot_size;\n+  sp_inc = align_up(sp_inc, StackAlignmentInBytes);\n+  assert(sp_inc > 0, \"sanity\");\n+\n+  \/\/ Save a copy of the FP and LR here for deoptimization patching and frame walking\n+  stp(rfp, lr, Address(pre(sp, -2 * wordSize)));\n+\n+  \/\/ Adjust the stack pointer. This will be repaired on return by MacroAssembler::remove_frame\n+  if (sp_inc < (1 << 9)) {\n+    sub(sp, sp, sp_inc);   \/\/ Fits in an immediate\n+  } else {\n+    mov(rscratch1, sp_inc);\n+    sub(sp, sp, rscratch1);\n+  }\n+\n+  return sp_inc + 2 * wordSize;  \/\/ Account for the FP\/LR space\n+}\n+\n+\/\/ Read all fields from an inline type oop and store the values in registers\/stack slots\n+bool MacroAssembler::unpack_inline_helper(const GrowableArray<SigEntry>* sig, int& sig_index,\n+                                          VMReg from, int& from_index, VMRegPair* to, int to_count, int& to_index,\n+                                          RegState reg_state[]) {\n+  assert(sig->at(sig_index)._bt == T_VOID, \"should be at end delimiter\");\n+  assert(from->is_valid(), \"source must be valid\");\n+  bool progress = false;\n+#ifdef ASSERT\n+  const int start_offset = offset();\n+#endif\n+\n+  Label L_null, L_notNull;\n+  \/\/ Don't use r14 as tmp because it's used for spilling (see MacroAssembler::spill_reg_for)\n+  Register tmp1 = r10;\n+  Register tmp2 = r11;\n+\n+#ifndef ASSERT\n+  RegSet clobbered_gp_regs = MacroAssembler::call_clobbered_gp_registers();\n+  assert(clobbered_gp_regs.contains(tmp1), \"tmp1 must be saved explicitly if it's not a clobber\");\n+  assert(clobbered_gp_regs.contains(tmp2), \"tmp2 must be saved explicitly if it's not a clobber\");\n+  assert(clobbered_gp_regs.contains(r14), \"r14 must be saved explicitly if it's not a clobber\");\n+#endif\n+\n+  Register fromReg = noreg;\n+  ScalarizedInlineArgsStream stream(sig, sig_index, to, to_count, to_index, true);\n+  bool done = true;\n+  bool mark_done = true;\n+  VMReg toReg;\n+  BasicType bt;\n+  \/\/ Check if argument requires a null check\n+  bool null_check = false;\n+  VMReg nullCheckReg;\n+  while (stream.next(nullCheckReg, bt)) {\n+    if (sig->at(stream.sig_index())._offset == -1) {\n+      null_check = true;\n+      break;\n+    }\n+  }\n+  stream.reset(sig_index, to_index);\n+  while (stream.next(toReg, bt)) {\n+    assert(toReg->is_valid(), \"destination must be valid\");\n+    int idx = (int)toReg->value();\n+    if (reg_state[idx] == reg_readonly) {\n+      if (idx != from->value()) {\n+        mark_done = false;\n+      }\n+      done = false;\n+      continue;\n+    } else if (reg_state[idx] == reg_written) {\n+      continue;\n+    }\n+    assert(reg_state[idx] == reg_writable, \"must be writable\");\n+    reg_state[idx] = reg_written;\n+    progress = true;\n+\n+    if (fromReg == noreg) {\n+      if (from->is_reg()) {\n+        fromReg = from->as_Register();\n+      } else {\n+        int st_off = from->reg2stack() * VMRegImpl::stack_slot_size;\n+        ldr(tmp1, Address(sp, st_off));\n+        fromReg = tmp1;\n+      }\n+      if (null_check) {\n+        \/\/ Nullable inline type argument, emit null check\n+        cbz(fromReg, L_null);\n+      }\n+    }\n+    int off = sig->at(stream.sig_index())._offset;\n+    if (off == -1) {\n+      assert(null_check, \"Missing null check at\");\n+      if (toReg->is_stack()) {\n+        int st_off = toReg->reg2stack() * VMRegImpl::stack_slot_size;\n+        mov(tmp2, 1);\n+        str(tmp2, Address(sp, st_off));\n+      } else {\n+        mov(toReg->as_Register(), 1);\n+      }\n+      continue;\n+    }\n+    assert(off > 0, \"offset in object should be positive\");\n+    Address fromAddr = Address(fromReg, off);\n+    if (!toReg->is_FloatRegister()) {\n+      Register dst = toReg->is_stack() ? tmp2 : toReg->as_Register();\n+      if (is_reference_type(bt)) {\n+        load_heap_oop(dst, fromAddr, rscratch1, rscratch2);\n+      } else {\n+        bool is_signed = (bt != T_CHAR) && (bt != T_BOOLEAN);\n+        load_sized_value(dst, fromAddr, type2aelembytes(bt), is_signed);\n+      }\n+      if (toReg->is_stack()) {\n+        int st_off = toReg->reg2stack() * VMRegImpl::stack_slot_size;\n+        str(dst, Address(sp, st_off));\n+      }\n+    } else if (bt == T_DOUBLE) {\n+      ldrd(toReg->as_FloatRegister(), fromAddr);\n+    } else {\n+      assert(bt == T_FLOAT, \"must be float\");\n+      ldrs(toReg->as_FloatRegister(), fromAddr);\n+    }\n+  }\n+  if (progress && null_check) {\n+    if (done) {\n+      b(L_notNull);\n+      bind(L_null);\n+      \/\/ Set null marker to zero to signal that the argument is null.\n+      \/\/ Also set all oop fields to zero to make the GC happy.\n+      stream.reset(sig_index, to_index);\n+      while (stream.next(toReg, bt)) {\n+        if (sig->at(stream.sig_index())._offset == -1 ||\n+            bt == T_OBJECT || bt == T_ARRAY) {\n+          if (toReg->is_stack()) {\n+            int st_off = toReg->reg2stack() * VMRegImpl::stack_slot_size;\n+            str(zr, Address(sp, st_off));\n+          } else {\n+            mov(toReg->as_Register(), zr);\n+          }\n+        }\n+      }\n+      bind(L_notNull);\n+    } else {\n+      bind(L_null);\n+    }\n+  }\n+\n+  sig_index = stream.sig_index();\n+  to_index = stream.regs_index();\n+\n+  if (mark_done && reg_state[from->value()] != reg_written) {\n+    \/\/ This is okay because no one else will write to that slot\n+    reg_state[from->value()] = reg_writable;\n+  }\n+  from_index--;\n+  assert(progress || (start_offset == offset()), \"should not emit code\");\n+  return done;\n+}\n+\n+\/\/ Pack fields back into an inline type oop\n+bool MacroAssembler::pack_inline_helper(const GrowableArray<SigEntry>* sig, int& sig_index, int vtarg_index,\n+                                        VMRegPair* from, int from_count, int& from_index, VMReg to,\n+                                        RegState reg_state[], Register val_array) {\n+  assert(sig->at(sig_index)._bt == T_METADATA, \"should be at delimiter\");\n+  assert(to->is_valid(), \"destination must be valid\");\n+\n+  if (reg_state[to->value()] == reg_written) {\n+    skip_unpacked_fields(sig, sig_index, from, from_count, from_index);\n+    return true; \/\/ Already written\n+  }\n+\n+  \/\/ The GC barrier expanded by store_heap_oop below may call into the\n+  \/\/ runtime so use callee-saved registers for any values that need to be\n+  \/\/ preserved. The GC barrier assembler should take care of saving the\n+  \/\/ Java argument registers.\n+  \/\/ TODO 8284443 Isn't it an issue if below code uses r14 as tmp when it contains a spilled value?\n+  \/\/ Be careful with r14 because it's used for spilling (see MacroAssembler::spill_reg_for).\n+  Register val_obj_tmp = r21;\n+  Register from_reg_tmp = r22;\n+  Register tmp1 = r14;\n+  Register tmp2 = r13;\n+  Register tmp3 = r12;\n+  Register val_obj = to->is_stack() ? val_obj_tmp : to->as_Register();\n+\n+  assert_different_registers(val_obj_tmp, from_reg_tmp, tmp1, tmp2, tmp3, val_array);\n+\n+  if (reg_state[to->value()] == reg_readonly) {\n+    if (!is_reg_in_unpacked_fields(sig, sig_index, to, from, from_count, from_index)) {\n+      skip_unpacked_fields(sig, sig_index, from, from_count, from_index);\n+      return false; \/\/ Not yet writable\n+    }\n+    val_obj = val_obj_tmp;\n+  }\n+\n+  int index = arrayOopDesc::base_offset_in_bytes(T_OBJECT) + vtarg_index * type2aelembytes(T_OBJECT);\n+  load_heap_oop(val_obj, Address(val_array, index), tmp1, tmp2);\n+\n+  ScalarizedInlineArgsStream stream(sig, sig_index, from, from_count, from_index);\n+  VMReg fromReg;\n+  BasicType bt;\n+  Label L_null;\n+  while (stream.next(fromReg, bt)) {\n+    assert(fromReg->is_valid(), \"source must be valid\");\n+    reg_state[fromReg->value()] = reg_writable;\n+\n+    int off = sig->at(stream.sig_index())._offset;\n+    if (off == -1) {\n+      \/\/ Nullable inline type argument, emit null check\n+      Label L_notNull;\n+      if (fromReg->is_stack()) {\n+        int ld_off = fromReg->reg2stack() * VMRegImpl::stack_slot_size;\n+        ldrb(tmp2, Address(sp, ld_off));\n+        cbnz(tmp2, L_notNull);\n+      } else {\n+        cbnz(fromReg->as_Register(), L_notNull);\n+      }\n+      mov(val_obj, 0);\n+      b(L_null);\n+      bind(L_notNull);\n+      continue;\n+    }\n+\n+    assert(off > 0, \"offset in object should be positive\");\n+    size_t size_in_bytes = is_java_primitive(bt) ? type2aelembytes(bt) : wordSize;\n+\n+    \/\/ Pack the scalarized field into the value object.\n+    Address dst(val_obj, off);\n+    if (!fromReg->is_FloatRegister()) {\n+      Register src;\n+      if (fromReg->is_stack()) {\n+        src = from_reg_tmp;\n+        int ld_off = fromReg->reg2stack() * VMRegImpl::stack_slot_size;\n+        load_sized_value(src, Address(sp, ld_off), size_in_bytes, \/* is_signed *\/ false);\n+      } else {\n+        src = fromReg->as_Register();\n+      }\n+      assert_different_registers(dst.base(), src, tmp1, tmp2, tmp3, val_array);\n+      if (is_reference_type(bt)) {\n+        \/\/ store_heap_oop transitively calls oop_store_at which corrupts to.base(). We need to keep val_obj valid.\n+        mov(tmp3, val_obj);\n+        Address dst_with_tmp3(tmp3, off);\n+        store_heap_oop(dst_with_tmp3, src, tmp1, tmp2, tmp3, IN_HEAP | ACCESS_WRITE | IS_DEST_UNINITIALIZED);\n+      } else {\n+        store_sized_value(dst, src, size_in_bytes);\n+      }\n+    } else if (bt == T_DOUBLE) {\n+      strd(fromReg->as_FloatRegister(), dst);\n+    } else {\n+      assert(bt == T_FLOAT, \"must be float\");\n+      strs(fromReg->as_FloatRegister(), dst);\n+    }\n+  }\n+  bind(L_null);\n+  sig_index = stream.sig_index();\n+  from_index = stream.regs_index();\n+\n+  assert(reg_state[to->value()] == reg_writable, \"must have already been read\");\n+  bool success = move_helper(val_obj->as_VMReg(), to, T_OBJECT, reg_state);\n+  assert(success, \"to register must be writeable\");\n+  return true;\n+}\n+\n+VMReg MacroAssembler::spill_reg_for(VMReg reg) {\n+  return (reg->is_FloatRegister()) ? v8->as_VMReg() : r14->as_VMReg();\n+}\n+\n@@ -6979,0 +7817,3 @@\n+  \/\/ Mask inline_type bit such that we go to the slow path if object is an inline type\n+  andr(mark, mark, ~((int) markWord::inline_type_bit_in_place));\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/macroAssembler_aarch64.cpp","additions":846,"deletions":5,"binary":false,"changes":851,"status":"modified"},{"patch":"@@ -31,0 +31,1 @@\n+#include \"ci\/ciInlineKlass.hpp\"\n@@ -46,0 +47,1 @@\n+#include \"oops\/resolvedFieldEntry.hpp\"\n@@ -47,0 +49,1 @@\n+#include \"runtime\/arguments.hpp\"\n@@ -56,0 +59,1 @@\n+#include \"runtime\/signature_cc.hpp\"\n@@ -59,0 +63,4 @@\n+#include \"vmreg_x86.inline.hpp\"\n+#ifdef COMPILER2\n+#include \"opto\/output.hpp\"\n+#endif\n@@ -1306,0 +1314,4 @@\n+void MacroAssembler::super_call_VM_leaf(address entry_point) {\n+  MacroAssembler::call_VM_leaf_base(entry_point, 1);\n+}\n+\n@@ -2359,0 +2371,107 @@\n+void MacroAssembler::test_markword_is_inline_type(Register markword, Label& is_inline_type) {\n+  andptr(markword, markWord::inline_type_mask_in_place);\n+  cmpptr(markword, markWord::inline_type_pattern);\n+  jcc(Assembler::equal, is_inline_type);\n+}\n+\n+void MacroAssembler::test_oop_is_not_inline_type(Register object, Register tmp, Label& not_inline_type, bool can_be_null) {\n+  if (can_be_null) {\n+    testptr(object, object);\n+    jcc(Assembler::zero, not_inline_type);\n+  }\n+  const int is_inline_type_mask = markWord::inline_type_pattern;\n+  movptr(tmp, Address(object, oopDesc::mark_offset_in_bytes()));\n+  andptr(tmp, is_inline_type_mask);\n+  cmpptr(tmp, is_inline_type_mask);\n+  jcc(Assembler::notEqual, not_inline_type);\n+}\n+\n+void MacroAssembler::test_field_is_null_free_inline_type(Register flags, Register temp_reg, Label& is_null_free_inline_type) {\n+  movl(temp_reg, flags);\n+  testl(temp_reg, 1 << ResolvedFieldEntry::is_null_free_inline_type_shift);\n+  jcc(Assembler::notEqual, is_null_free_inline_type);\n+}\n+\n+void MacroAssembler::test_field_is_not_null_free_inline_type(Register flags, Register temp_reg, Label& not_null_free_inline_type) {\n+  movl(temp_reg, flags);\n+  testl(temp_reg, 1 << ResolvedFieldEntry::is_null_free_inline_type_shift);\n+  jcc(Assembler::equal, not_null_free_inline_type);\n+}\n+\n+void MacroAssembler::test_field_is_flat(Register flags, Register temp_reg, Label& is_flat) {\n+  movl(temp_reg, flags);\n+  testl(temp_reg, 1 << ResolvedFieldEntry::is_flat_shift);\n+  jcc(Assembler::notEqual, is_flat);\n+}\n+\n+void MacroAssembler::test_field_has_null_marker(Register flags, Register temp_reg, Label& has_null_marker) {\n+  movl(temp_reg, flags);\n+  testl(temp_reg, 1 << ResolvedFieldEntry::has_null_marker_shift);\n+  jcc(Assembler::notEqual, has_null_marker);\n+}\n+\n+void MacroAssembler::test_oop_prototype_bit(Register oop, Register temp_reg, int32_t test_bit, bool jmp_set, Label& jmp_label) {\n+  Label test_mark_word;\n+  \/\/ load mark word\n+  movptr(temp_reg, Address(oop, oopDesc::mark_offset_in_bytes()));\n+  \/\/ check displaced\n+  testl(temp_reg, markWord::unlocked_value);\n+  jccb(Assembler::notZero, test_mark_word);\n+  \/\/ slow path use klass prototype\n+  push(rscratch1);\n+  load_prototype_header(temp_reg, oop, rscratch1);\n+  pop(rscratch1);\n+\n+  bind(test_mark_word);\n+  testl(temp_reg, test_bit);\n+  jcc((jmp_set) ? Assembler::notZero : Assembler::zero, jmp_label);\n+}\n+\n+void MacroAssembler::test_flat_array_oop(Register oop, Register temp_reg,\n+                                         Label& is_flat_array) {\n+#ifdef _LP64\n+  test_oop_prototype_bit(oop, temp_reg, markWord::flat_array_bit_in_place, true, is_flat_array);\n+#else\n+  load_klass(temp_reg, oop, noreg);\n+  movl(temp_reg, Address(temp_reg, Klass::layout_helper_offset()));\n+  test_flat_array_layout(temp_reg, is_flat_array);\n+#endif\n+}\n+\n+void MacroAssembler::test_non_flat_array_oop(Register oop, Register temp_reg,\n+                                             Label& is_non_flat_array) {\n+#ifdef _LP64\n+  test_oop_prototype_bit(oop, temp_reg, markWord::flat_array_bit_in_place, false, is_non_flat_array);\n+#else\n+  load_klass(temp_reg, oop, noreg);\n+  movl(temp_reg, Address(temp_reg, Klass::layout_helper_offset()));\n+  test_non_flat_array_layout(temp_reg, is_non_flat_array);\n+#endif\n+}\n+\n+void MacroAssembler::test_null_free_array_oop(Register oop, Register temp_reg, Label&is_null_free_array) {\n+#ifdef _LP64\n+  test_oop_prototype_bit(oop, temp_reg, markWord::null_free_array_bit_in_place, true, is_null_free_array);\n+#else\n+  Unimplemented();\n+#endif\n+}\n+\n+void MacroAssembler::test_non_null_free_array_oop(Register oop, Register temp_reg, Label&is_non_null_free_array) {\n+#ifdef _LP64\n+  test_oop_prototype_bit(oop, temp_reg, markWord::null_free_array_bit_in_place, false, is_non_null_free_array);\n+#else\n+  Unimplemented();\n+#endif\n+}\n+\n+void MacroAssembler::test_flat_array_layout(Register lh, Label& is_flat_array) {\n+  testl(lh, Klass::_lh_array_tag_flat_value_bit_inplace);\n+  jcc(Assembler::notZero, is_flat_array);\n+}\n+\n+void MacroAssembler::test_non_flat_array_layout(Register lh, Label& is_non_flat_array) {\n+  testl(lh, Klass::_lh_array_tag_flat_value_bit_inplace);\n+  jcc(Assembler::zero, is_non_flat_array);\n+}\n+\n@@ -3441,0 +3560,118 @@\n+\/\/ Object \/ value buffer allocation...\n+\/\/\n+\/\/ Kills klass and rsi on LP64\n+void MacroAssembler::allocate_instance(Register klass, Register new_obj,\n+                                       Register t1, Register t2,\n+                                       bool clear_fields, Label& alloc_failed)\n+{\n+  Label done, initialize_header, initialize_object, slow_case, slow_case_no_pop;\n+  Register layout_size = t1;\n+  assert(new_obj == rax, \"needs to be rax\");\n+  assert_different_registers(klass, new_obj, t1, t2);\n+\n+  \/\/ get instance_size in InstanceKlass (scaled to a count of bytes)\n+  movl(layout_size, Address(klass, Klass::layout_helper_offset()));\n+  \/\/ test to see if it is malformed in some way\n+  testl(layout_size, Klass::_lh_instance_slow_path_bit);\n+  jcc(Assembler::notZero, slow_case_no_pop);\n+\n+  \/\/ Allocate the instance:\n+  \/\/  If TLAB is enabled:\n+  \/\/    Try to allocate in the TLAB.\n+  \/\/    If fails, go to the slow path.\n+  \/\/  Else If inline contiguous allocations are enabled:\n+  \/\/    Try to allocate in eden.\n+  \/\/    If fails due to heap end, go to slow path.\n+  \/\/\n+  \/\/  If TLAB is enabled OR inline contiguous is enabled:\n+  \/\/    Initialize the allocation.\n+  \/\/    Exit.\n+  \/\/\n+  \/\/  Go to slow path.\n+\n+  push(klass);\n+  if (UseTLAB) {\n+    tlab_allocate(new_obj, layout_size, 0, klass, t2, slow_case);\n+    if (ZeroTLAB || (!clear_fields)) {\n+      \/\/ the fields have been already cleared\n+      jmp(initialize_header);\n+    } else {\n+      \/\/ initialize both the header and fields\n+      jmp(initialize_object);\n+    }\n+  } else {\n+    jmp(slow_case);\n+  }\n+\n+  \/\/ If UseTLAB is true, the object is created above and there is an initialize need.\n+  \/\/ Otherwise, skip and go to the slow path.\n+  if (UseTLAB) {\n+    if (clear_fields) {\n+      \/\/ The object is initialized before the header.  If the object size is\n+      \/\/ zero, go directly to the header initialization.\n+      bind(initialize_object);\n+      if (UseCompactObjectHeaders) {\n+        assert(is_aligned(oopDesc::base_offset_in_bytes(), BytesPerLong), \"oop base offset must be 8-byte-aligned\");\n+        decrement(layout_size, oopDesc::base_offset_in_bytes());\n+      } else {\n+        decrement(layout_size, sizeof(oopDesc));\n+      }\n+      jcc(Assembler::zero, initialize_header);\n+\n+      \/\/ Initialize topmost object field, divide size by 8, check if odd and\n+      \/\/ test if zero.\n+      Register zero = klass;\n+      xorl(zero, zero);    \/\/ use zero reg to clear memory (shorter code)\n+      shrl(layout_size, LogBytesPerLong); \/\/ divide by 2*oopSize and set carry flag if odd\n+\n+  #ifdef ASSERT\n+      \/\/ make sure instance_size was multiple of 8\n+      Label L;\n+      \/\/ Ignore partial flag stall after shrl() since it is debug VM\n+      jcc(Assembler::carryClear, L);\n+      stop(\"object size is not multiple of 2 - adjust this code\");\n+      bind(L);\n+      \/\/ must be > 0, no extra check needed here\n+  #endif\n+\n+      \/\/ initialize remaining object fields: instance_size was a multiple of 8\n+      {\n+        Label loop;\n+        bind(loop);\n+        int header_size_bytes = oopDesc::header_size() * HeapWordSize;\n+        assert(is_aligned(header_size_bytes, BytesPerLong), \"oop header size must be 8-byte-aligned\");\n+        movptr(Address(new_obj, layout_size, Address::times_8, header_size_bytes - 1*oopSize), zero);\n+        decrement(layout_size);\n+        jcc(Assembler::notZero, loop);\n+      }\n+    } \/\/ clear_fields\n+\n+    \/\/ initialize object header only.\n+    bind(initialize_header);\n+    if (UseCompactObjectHeaders || Arguments::is_valhalla_enabled()) {\n+      pop(klass);\n+      Register mark_word = t2;\n+      movptr(mark_word, Address(klass, Klass::prototype_header_offset()));\n+      movptr(Address(new_obj, oopDesc::mark_offset_in_bytes ()), mark_word);\n+    } else {\n+     movptr(Address(new_obj, oopDesc::mark_offset_in_bytes()),\n+            (intptr_t)markWord::prototype().value()); \/\/ header\n+     pop(klass);   \/\/ get saved klass back in the register.\n+    }\n+    if (!UseCompactObjectHeaders) {\n+      xorl(rsi, rsi);                 \/\/ use zero reg to clear memory (shorter code)\n+      store_klass_gap(new_obj, rsi);  \/\/ zero klass gap for compressed oops\n+      movptr(t2, klass);         \/\/ preserve klass\n+      store_klass(new_obj, t2, rscratch1);  \/\/ src klass reg is potentially compressed\n+    }\n+    jmp(done);\n+  }\n+\n+  bind(slow_case);\n+  pop(klass);\n+  bind(slow_case_no_pop);\n+  jmp(alloc_failed);\n+\n+  bind(done);\n+}\n+\n@@ -3644,0 +3881,27 @@\n+void MacroAssembler::get_inline_type_field_klass(Register holder_klass, Register index, Register inline_klass) {\n+  inline_layout_info(holder_klass, index, inline_klass);\n+  movptr(inline_klass, Address(inline_klass, InlineLayoutInfo::klass_offset()));\n+}\n+\n+void MacroAssembler::inline_layout_info(Register holder_klass, Register index, Register layout_info) {\n+  movptr(layout_info, Address(holder_klass, InstanceKlass::inline_layout_info_array_offset()));\n+#ifdef ASSERT\n+  {\n+    Label done;\n+    cmpptr(layout_info, 0);\n+    jcc(Assembler::notEqual, done);\n+    stop(\"inline_layout_info_array is null\");\n+    bind(done);\n+  }\n+#endif\n+\n+  InlineLayoutInfo array[2];\n+  int size = (char*)&array[1] - (char*)&array[0]; \/\/ computing size of array elements\n+  if (is_power_of_2(size)) {\n+    shll(index, log2i_exact(size)); \/\/ Scale index by power of 2\n+  } else {\n+    imull(index, index, size); \/\/ Scale the index to be the entry index * array_element_size\n+  }\n+  lea(layout_info, Address(layout_info, index, Address::times_1, Array<InlineLayoutInfo>::base_offset_in_bytes()));\n+}\n+\n@@ -4694,1 +4958,5 @@\n-  if (!VerifyOops) return;\n+  if (!VerifyOops || VerifyAdapterSharing) {\n+    \/\/ Below address of the code string confuses VerifyAdapterSharing\n+    \/\/ because it may differ between otherwise equivalent adapters.\n+    return;\n+  }\n@@ -4950,1 +5218,5 @@\n-  if (!VerifyOops) return;\n+  if (!VerifyOops || VerifyAdapterSharing) {\n+    \/\/ Below address of the code string confuses VerifyAdapterSharing\n+    \/\/ because it may differ between otherwise equivalent adapters.\n+    return;\n+  }\n@@ -5344,0 +5616,10 @@\n+void MacroAssembler::load_metadata(Register dst, Register src) {\n+  if (UseCompactObjectHeaders) {\n+    load_narrow_klass_compact(dst, src);\n+  } else if (UseCompressedClassPointers) {\n+    movl(dst, Address(src, oopDesc::klass_offset_in_bytes()));\n+  } else {\n+    movptr(dst, Address(src, oopDesc::klass_offset_in_bytes()));\n+  }\n+}\n+\n@@ -5365,0 +5647,5 @@\n+void MacroAssembler::load_prototype_header(Register dst, Register src, Register tmp) {\n+  load_klass(dst, src, tmp);\n+  movptr(dst, Address(dst, Klass::prototype_header_offset()));\n+}\n+\n@@ -5430,0 +5717,40 @@\n+void MacroAssembler::flat_field_copy(DecoratorSet decorators, Register src, Register dst,\n+                                     Register inline_layout_info) {\n+  BarrierSetAssembler* bs = BarrierSet::barrier_set()->barrier_set_assembler();\n+  bs->flat_field_copy(this, decorators, src, dst, inline_layout_info);\n+}\n+\n+void MacroAssembler::payload_offset(Register inline_klass, Register offset) {\n+  movptr(offset, Address(inline_klass, InlineKlass::adr_members_offset()));\n+  movl(offset, Address(offset, InlineKlass::payload_offset_offset()));\n+}\n+\n+void MacroAssembler::payload_addr(Register oop, Register data, Register inline_klass) {\n+  \/\/ ((address) (void*) o) + vk->payload_offset();\n+  Register offset = (data == oop) ? rscratch1 : data;\n+  payload_offset(inline_klass, offset);\n+  if (data == oop) {\n+    addptr(data, offset);\n+  } else {\n+    lea(data, Address(oop, offset));\n+  }\n+}\n+\n+void MacroAssembler::data_for_value_array_index(Register array, Register array_klass,\n+                                                Register index, Register data) {\n+  assert(index != rcx, \"index needs to shift by rcx\");\n+  assert_different_registers(array, array_klass, index);\n+  assert_different_registers(rcx, array, index);\n+\n+  \/\/ array->base() + (index << Klass::layout_helper_log2_element_size(lh));\n+  movl(rcx, Address(array_klass, Klass::layout_helper_offset()));\n+\n+  \/\/ Klass::layout_helper_log2_element_size(lh)\n+  \/\/ (lh >> _lh_log2_element_size_shift) & _lh_log2_element_size_mask;\n+  shrl(rcx, Klass::_lh_log2_element_size_shift);\n+  andl(rcx, Klass::_lh_log2_element_size_mask);\n+  shlptr(index); \/\/ index << rcx\n+\n+  lea(data, Address(array, index, Address::times_1, arrayOopDesc::base_offset_in_bytes(T_FLAT_ELEMENT)));\n+}\n+\n@@ -5786,0 +6113,476 @@\n+int MacroAssembler::store_inline_type_fields_to_buf(ciInlineKlass* vk, bool from_interpreter) {\n+  assert(InlineTypeReturnedAsFields, \"Inline types should never be returned as fields\");\n+  \/\/ An inline type might be returned. If fields are in registers we\n+  \/\/ need to allocate an inline type instance and initialize it with\n+  \/\/ the value of the fields.\n+  Label skip;\n+  \/\/ We only need a new buffered inline type if a new one is not returned\n+  testptr(rax, 1);\n+  jcc(Assembler::zero, skip);\n+  int call_offset = -1;\n+\n+#ifdef _LP64\n+  \/\/ The following code is similar to allocate_instance but has some slight differences,\n+  \/\/ e.g. object size is always not zero, sometimes it's constant; storing klass ptr after\n+  \/\/ allocating is not necessary if vk != nullptr, etc. allocate_instance is not aware of these.\n+  Label slow_case;\n+  \/\/ 1. Try to allocate a new buffered inline instance either from TLAB or eden space\n+  mov(rscratch1, rax); \/\/ save rax for slow_case since *_allocate may corrupt it when allocation failed\n+  if (vk != nullptr) {\n+    \/\/ Called from C1, where the return type is statically known.\n+    movptr(rbx, (intptr_t)vk->get_InlineKlass());\n+    jint lh = vk->layout_helper();\n+    assert(lh != Klass::_lh_neutral_value, \"inline class in return type must have been resolved\");\n+    if (UseTLAB && !Klass::layout_helper_needs_slow_path(lh)) {\n+      tlab_allocate(rax, noreg, lh, r13, r14, slow_case);\n+    } else {\n+      jmp(slow_case);\n+    }\n+  } else {\n+    \/\/ Call from interpreter. RAX contains ((the InlineKlass* of the return type) | 0x01)\n+    mov(rbx, rax);\n+    andptr(rbx, -2);\n+    if (UseTLAB) {\n+      movl(r14, Address(rbx, Klass::layout_helper_offset()));\n+      testl(r14, Klass::_lh_instance_slow_path_bit);\n+      jcc(Assembler::notZero, slow_case);\n+      tlab_allocate(rax, r14, 0, r13, r14, slow_case);\n+    } else {\n+      jmp(slow_case);\n+    }\n+  }\n+  if (UseTLAB) {\n+    \/\/ 2. Initialize buffered inline instance header\n+    Register buffer_obj = rax;\n+    Register klass = rbx;\n+    if (UseCompactObjectHeaders) {\n+      Register mark_word = r13;\n+      movptr(mark_word, Address(klass, Klass::prototype_header_offset()));\n+      movptr(Address(buffer_obj, oopDesc::mark_offset_in_bytes()), mark_word);\n+    } else {\n+      movptr(Address(buffer_obj, oopDesc::mark_offset_in_bytes()), (intptr_t)markWord::inline_type_prototype().value());\n+      xorl(r13, r13);\n+      store_klass_gap(buffer_obj, r13);\n+      if (vk == nullptr) {\n+        \/\/ store_klass corrupts rbx(klass), so save it in r13 for later use (interpreter case only).\n+        mov(r13, klass);\n+      }\n+      store_klass(buffer_obj, klass, rscratch1);\n+      klass = r13;\n+    }\n+    \/\/ 3. Initialize its fields with an inline class specific handler\n+    if (vk != nullptr) {\n+      call(RuntimeAddress(vk->pack_handler())); \/\/ no need for call info as this will not safepoint.\n+    } else {\n+      movptr(rbx, Address(klass, InlineKlass::adr_members_offset()));\n+      movptr(rbx, Address(rbx, InlineKlass::pack_handler_offset()));\n+      call(rbx);\n+    }\n+    jmp(skip);\n+  }\n+  bind(slow_case);\n+  \/\/ We failed to allocate a new inline type, fall back to a runtime\n+  \/\/ call. Some oop field may be live in some registers but we can't\n+  \/\/ tell. That runtime call will take care of preserving them\n+  \/\/ across a GC if there's one.\n+  mov(rax, rscratch1);\n+#endif\n+\n+  if (from_interpreter) {\n+    super_call_VM_leaf(StubRoutines::store_inline_type_fields_to_buf());\n+  } else {\n+    call(RuntimeAddress(StubRoutines::store_inline_type_fields_to_buf()));\n+    call_offset = offset();\n+  }\n+\n+  bind(skip);\n+  return call_offset;\n+}\n+\n+\/\/ Move a value between registers\/stack slots and update the reg_state\n+bool MacroAssembler::move_helper(VMReg from, VMReg to, BasicType bt, RegState reg_state[]) {\n+  assert(from->is_valid() && to->is_valid(), \"source and destination must be valid\");\n+  if (reg_state[to->value()] == reg_written) {\n+    return true; \/\/ Already written\n+  }\n+  if (from != to && bt != T_VOID) {\n+    if (reg_state[to->value()] == reg_readonly) {\n+      return false; \/\/ Not yet writable\n+    }\n+    if (from->is_reg()) {\n+      if (to->is_reg()) {\n+        if (from->is_XMMRegister()) {\n+          if (bt == T_DOUBLE) {\n+            movdbl(to->as_XMMRegister(), from->as_XMMRegister());\n+          } else {\n+            assert(bt == T_FLOAT, \"must be float\");\n+            movflt(to->as_XMMRegister(), from->as_XMMRegister());\n+          }\n+        } else {\n+          movq(to->as_Register(), from->as_Register());\n+        }\n+      } else {\n+        int st_off = to->reg2stack() * VMRegImpl::stack_slot_size + wordSize;\n+        Address to_addr = Address(rsp, st_off);\n+        if (from->is_XMMRegister()) {\n+          if (bt == T_DOUBLE) {\n+            movdbl(to_addr, from->as_XMMRegister());\n+          } else {\n+            assert(bt == T_FLOAT, \"must be float\");\n+            movflt(to_addr, from->as_XMMRegister());\n+          }\n+        } else {\n+          movq(to_addr, from->as_Register());\n+        }\n+      }\n+    } else {\n+      Address from_addr = Address(rsp, from->reg2stack() * VMRegImpl::stack_slot_size + wordSize);\n+      if (to->is_reg()) {\n+        if (to->is_XMMRegister()) {\n+          if (bt == T_DOUBLE) {\n+            movdbl(to->as_XMMRegister(), from_addr);\n+          } else {\n+            assert(bt == T_FLOAT, \"must be float\");\n+            movflt(to->as_XMMRegister(), from_addr);\n+          }\n+        } else {\n+          movq(to->as_Register(), from_addr);\n+        }\n+      } else {\n+        int st_off = to->reg2stack() * VMRegImpl::stack_slot_size + wordSize;\n+        movq(r13, from_addr);\n+        movq(Address(rsp, st_off), r13);\n+      }\n+    }\n+  }\n+  \/\/ Update register states\n+  reg_state[from->value()] = reg_writable;\n+  reg_state[to->value()] = reg_written;\n+  return true;\n+}\n+\n+\/\/ Calculate the extra stack space required for packing or unpacking inline\n+\/\/ args and adjust the stack pointer.\n+\/\/\n+\/\/ This extra stack space take into account the copy #2 of the return address,\n+\/\/ but NOT the saved RBP or the normal size of the frame (see MacroAssembler::remove_frame\n+\/\/ for notations).\n+int MacroAssembler::extend_stack_for_inline_args(int args_on_stack) {\n+  \/\/ Two additional slots to account for return address\n+  int sp_inc = (args_on_stack + 2) * VMRegImpl::stack_slot_size;\n+  sp_inc = align_up(sp_inc, StackAlignmentInBytes);\n+  \/\/ Save the return address, adjust the stack (make sure it is properly\n+  \/\/ 16-byte aligned) and copy the return address to the new top of the stack.\n+  \/\/ The stack will be repaired on return (see MacroAssembler::remove_frame).\n+  assert(sp_inc > 0, \"sanity\");\n+  pop(r13);\n+  subptr(rsp, sp_inc);\n+#ifdef ASSERT\n+  movl(Address(rsp, -VMRegImpl::stack_slot_size), badRegWordVal);\n+  movl(Address(rsp, -2 * VMRegImpl::stack_slot_size), badRegWordVal);\n+  subptr(rsp, 2 * VMRegImpl::stack_slot_size);\n+#else\n+  push(r13);\n+#endif\n+  return sp_inc;\n+}\n+\n+\/\/ Read all fields from an inline type buffer and store the field values in registers\/stack slots.\n+bool MacroAssembler::unpack_inline_helper(const GrowableArray<SigEntry>* sig, int& sig_index,\n+                                          VMReg from, int& from_index, VMRegPair* to, int to_count, int& to_index,\n+                                          RegState reg_state[]) {\n+  assert(sig->at(sig_index)._bt == T_VOID, \"should be at end delimiter\");\n+  assert(from->is_valid(), \"source must be valid\");\n+  bool progress = false;\n+#ifdef ASSERT\n+  const int start_offset = offset();\n+#endif\n+\n+  Label L_null, L_notNull;\n+  \/\/ Don't use r14 as tmp because it's used for spilling (see MacroAssembler::spill_reg_for)\n+  Register tmp1 = r10;\n+  Register tmp2 = r13;\n+  Register fromReg = noreg;\n+  ScalarizedInlineArgsStream stream(sig, sig_index, to, to_count, to_index, true);\n+  bool done = true;\n+  bool mark_done = true;\n+  VMReg toReg;\n+  BasicType bt;\n+  \/\/ Check if argument requires a null check\n+  bool null_check = false;\n+  VMReg nullCheckReg;\n+  while (stream.next(nullCheckReg, bt)) {\n+    if (sig->at(stream.sig_index())._offset == -1) {\n+      null_check = true;\n+      break;\n+    }\n+  }\n+  stream.reset(sig_index, to_index);\n+  while (stream.next(toReg, bt)) {\n+    assert(toReg->is_valid(), \"destination must be valid\");\n+    int idx = (int)toReg->value();\n+    if (reg_state[idx] == reg_readonly) {\n+      if (idx != from->value()) {\n+        mark_done = false;\n+      }\n+      done = false;\n+      continue;\n+    } else if (reg_state[idx] == reg_written) {\n+      continue;\n+    }\n+    assert(reg_state[idx] == reg_writable, \"must be writable\");\n+    reg_state[idx] = reg_written;\n+    progress = true;\n+\n+    if (fromReg == noreg) {\n+      if (from->is_reg()) {\n+        fromReg = from->as_Register();\n+      } else {\n+        int st_off = from->reg2stack() * VMRegImpl::stack_slot_size + wordSize;\n+        movq(tmp1, Address(rsp, st_off));\n+        fromReg = tmp1;\n+      }\n+      if (null_check) {\n+        \/\/ Nullable inline type argument, emit null check\n+        testptr(fromReg, fromReg);\n+        jcc(Assembler::zero, L_null);\n+      }\n+    }\n+    int off = sig->at(stream.sig_index())._offset;\n+    if (off == -1) {\n+      assert(null_check, \"Missing null check at\");\n+      if (toReg->is_stack()) {\n+        int st_off = toReg->reg2stack() * VMRegImpl::stack_slot_size + wordSize;\n+        movq(Address(rsp, st_off), 1);\n+      } else {\n+        movq(toReg->as_Register(), 1);\n+      }\n+      continue;\n+    }\n+    assert(off > 0, \"offset in object should be positive\");\n+    Address fromAddr = Address(fromReg, off);\n+    if (!toReg->is_XMMRegister()) {\n+      Register dst = toReg->is_stack() ? tmp2 : toReg->as_Register();\n+      if (is_reference_type(bt)) {\n+        load_heap_oop(dst, fromAddr);\n+      } else {\n+        bool is_signed = (bt != T_CHAR) && (bt != T_BOOLEAN);\n+        load_sized_value(dst, fromAddr, type2aelembytes(bt), is_signed);\n+      }\n+      if (toReg->is_stack()) {\n+        int st_off = toReg->reg2stack() * VMRegImpl::stack_slot_size + wordSize;\n+        movq(Address(rsp, st_off), dst);\n+      }\n+    } else if (bt == T_DOUBLE) {\n+      movdbl(toReg->as_XMMRegister(), fromAddr);\n+    } else {\n+      assert(bt == T_FLOAT, \"must be float\");\n+      movflt(toReg->as_XMMRegister(), fromAddr);\n+    }\n+  }\n+  if (progress && null_check) {\n+    if (done) {\n+      jmp(L_notNull);\n+      bind(L_null);\n+      \/\/ Set null marker to zero to signal that the argument is null.\n+      \/\/ Also set all oop fields to zero to make the GC happy.\n+      stream.reset(sig_index, to_index);\n+      while (stream.next(toReg, bt)) {\n+        if (sig->at(stream.sig_index())._offset == -1 ||\n+            bt == T_OBJECT || bt == T_ARRAY) {\n+          if (toReg->is_stack()) {\n+            int st_off = toReg->reg2stack() * VMRegImpl::stack_slot_size + wordSize;\n+            movq(Address(rsp, st_off), 0);\n+          } else {\n+            xorq(toReg->as_Register(), toReg->as_Register());\n+          }\n+        }\n+      }\n+      bind(L_notNull);\n+    } else {\n+      bind(L_null);\n+    }\n+  }\n+\n+  sig_index = stream.sig_index();\n+  to_index = stream.regs_index();\n+\n+  if (mark_done && reg_state[from->value()] != reg_written) {\n+    \/\/ This is okay because no one else will write to that slot\n+    reg_state[from->value()] = reg_writable;\n+  }\n+  from_index--;\n+  assert(progress || (start_offset == offset()), \"should not emit code\");\n+  return done;\n+}\n+\n+bool MacroAssembler::pack_inline_helper(const GrowableArray<SigEntry>* sig, int& sig_index, int vtarg_index,\n+                                        VMRegPair* from, int from_count, int& from_index, VMReg to,\n+                                        RegState reg_state[], Register val_array) {\n+  assert(sig->at(sig_index)._bt == T_METADATA, \"should be at delimiter\");\n+  assert(to->is_valid(), \"destination must be valid\");\n+\n+  if (reg_state[to->value()] == reg_written) {\n+    skip_unpacked_fields(sig, sig_index, from, from_count, from_index);\n+    return true; \/\/ Already written\n+  }\n+\n+  \/\/ TODO 8284443 Isn't it an issue if below code uses r14 as tmp when it contains a spilled value?\n+  \/\/ Be careful with r14 because it's used for spilling (see MacroAssembler::spill_reg_for).\n+  Register val_obj_tmp = r11;\n+  Register from_reg_tmp = r14;\n+  Register tmp1 = r10;\n+  Register tmp2 = r13;\n+  Register tmp3 = rbx;\n+  Register val_obj = to->is_stack() ? val_obj_tmp : to->as_Register();\n+\n+  assert_different_registers(val_obj_tmp, from_reg_tmp, tmp1, tmp2, tmp3, val_array);\n+\n+  if (reg_state[to->value()] == reg_readonly) {\n+    if (!is_reg_in_unpacked_fields(sig, sig_index, to, from, from_count, from_index)) {\n+      skip_unpacked_fields(sig, sig_index, from, from_count, from_index);\n+      return false; \/\/ Not yet writable\n+    }\n+    val_obj = val_obj_tmp;\n+  }\n+\n+  int index = arrayOopDesc::base_offset_in_bytes(T_OBJECT) + vtarg_index * type2aelembytes(T_OBJECT);\n+  load_heap_oop(val_obj, Address(val_array, index));\n+\n+  ScalarizedInlineArgsStream stream(sig, sig_index, from, from_count, from_index);\n+  VMReg fromReg;\n+  BasicType bt;\n+  Label L_null;\n+  while (stream.next(fromReg, bt)) {\n+    assert(fromReg->is_valid(), \"source must be valid\");\n+    reg_state[fromReg->value()] = reg_writable;\n+\n+    int off = sig->at(stream.sig_index())._offset;\n+    if (off == -1) {\n+      \/\/ Nullable inline type argument, emit null check\n+      Label L_notNull;\n+      if (fromReg->is_stack()) {\n+        int ld_off = fromReg->reg2stack() * VMRegImpl::stack_slot_size + wordSize;\n+        testb(Address(rsp, ld_off), 1);\n+      } else {\n+        testb(fromReg->as_Register(), 1);\n+      }\n+      jcc(Assembler::notZero, L_notNull);\n+      movptr(val_obj, 0);\n+      jmp(L_null);\n+      bind(L_notNull);\n+      continue;\n+    }\n+\n+    assert(off > 0, \"offset in object should be positive\");\n+    size_t size_in_bytes = is_java_primitive(bt) ? type2aelembytes(bt) : wordSize;\n+\n+    \/\/ Pack the scalarized field into the value object.\n+    Address dst(val_obj, off);\n+    if (!fromReg->is_XMMRegister()) {\n+      Register src;\n+      if (fromReg->is_stack()) {\n+        src = from_reg_tmp;\n+        int ld_off = fromReg->reg2stack() * VMRegImpl::stack_slot_size + wordSize;\n+        load_sized_value(src, Address(rsp, ld_off), size_in_bytes, \/* is_signed *\/ false);\n+      } else {\n+        src = fromReg->as_Register();\n+      }\n+      assert_different_registers(dst.base(), src, tmp1, tmp2, tmp3, val_array);\n+      if (is_reference_type(bt)) {\n+        \/\/ store_heap_oop transitively calls oop_store_at which corrupts to.base(). We need to keep val_obj valid.\n+        mov(tmp3, val_obj);\n+        Address dst_with_tmp3(tmp3, off);\n+        store_heap_oop(dst_with_tmp3, src, tmp1, tmp2, tmp3, IN_HEAP | ACCESS_WRITE | IS_DEST_UNINITIALIZED);\n+      } else {\n+        store_sized_value(dst, src, size_in_bytes);\n+      }\n+    } else if (bt == T_DOUBLE) {\n+      movdbl(dst, fromReg->as_XMMRegister());\n+    } else {\n+      assert(bt == T_FLOAT, \"must be float\");\n+      movflt(dst, fromReg->as_XMMRegister());\n+    }\n+  }\n+  bind(L_null);\n+  sig_index = stream.sig_index();\n+  from_index = stream.regs_index();\n+\n+  assert(reg_state[to->value()] == reg_writable, \"must have already been read\");\n+  bool success = move_helper(val_obj->as_VMReg(), to, T_OBJECT, reg_state);\n+  assert(success, \"to register must be writeable\");\n+  return true;\n+}\n+\n+VMReg MacroAssembler::spill_reg_for(VMReg reg) {\n+  return reg->is_XMMRegister() ? xmm8->as_VMReg() : r14->as_VMReg();\n+}\n+\n+void MacroAssembler::remove_frame(int initial_framesize, bool needs_stack_repair) {\n+  assert((initial_framesize & (StackAlignmentInBytes-1)) == 0, \"frame size not aligned\");\n+  if (needs_stack_repair) {\n+    \/\/ The method has a scalarized entry point (where fields of value object arguments\n+    \/\/ are passed through registers and stack), and a non-scalarized entry point (where\n+    \/\/ value object arguments are given as oops). The non-scalarized entry point will\n+    \/\/ first load each field of value object arguments and store them in registers and on\n+    \/\/ the stack in a way compatible with the scalarized entry point. To do so, some extra\n+    \/\/ stack space might be reserved (if argument registers are not enough). On leaving the\n+    \/\/ method, this space must be freed.\n+    \/\/\n+    \/\/ In case we used the non-scalarized entry point the stack looks like this:\n+    \/\/\n+    \/\/ | Arguments from caller     |\n+    \/\/ |---------------------------|  <-- caller's SP\n+    \/\/ | Return address #1         |\n+    \/\/ |---------------------------|\n+    \/\/ | Extension space for       |\n+    \/\/ |   inline arg (un)packing  |\n+    \/\/ |---------------------------|\n+    \/\/ | Return address #2         |\n+    \/\/ | Saved RBP                 |\n+    \/\/ |---------------------------|  <-- start of this method's frame\n+    \/\/ | sp_inc                    |\n+    \/\/ | method locals             |\n+    \/\/ |---------------------------|  <-- SP\n+    \/\/\n+    \/\/ There is two copies of the return address on the stack. They will be identical at\n+    \/\/ first, but that can change.\n+    \/\/ If the caller has been deoptimized, the copy #1 will be patched to point at the\n+    \/\/ deopt blob, and the copy #2 will still point into the old method. In short\n+    \/\/ the copy #2 is not reliable and should not be used. It is mostly needed to\n+    \/\/ add space between the extension space and the locals, as there would be between\n+    \/\/ the real arguments and the locals if we don't need to do unpacking (from the\n+    \/\/ scalarized entry point).\n+    \/\/\n+    \/\/ When leaving, one must use the copy #1 of the return address, while keeping in mind\n+    \/\/ that from the scalarized entry point, there will be only one copy. Indeed, in the\n+    \/\/ case we used the scalarized calling convention, the stack looks like this:\n+    \/\/\n+    \/\/ | Arguments from caller     |\n+    \/\/ |---------------------------|  <-- caller's SP\n+    \/\/ | Return address            |\n+    \/\/ | Saved RBP                 |\n+    \/\/ |---------------------------|  <-- start of this method's frame\n+    \/\/ | sp_inc                    |\n+    \/\/ | method locals             |\n+    \/\/ |---------------------------|  <-- SP\n+    \/\/\n+    \/\/ The sp_inc stack slot holds the total size of the frame, including the extension\n+    \/\/ space the possible copy #2 of the return address and the saved RBP (but never the\n+    \/\/ copy #1 of the return address). That is how to find the copy #1 of the return address.\n+    \/\/ This size is expressed in bytes. Be careful when using it from C++ in pointer arithmetic;\n+    \/\/ you might need to divide it by wordSize.\n+    \/\/\n+    \/\/ One can find sp_inc since the start the method's frame is SP + initial_framesize.\n+\n+    movq(rbp, Address(rsp, initial_framesize));\n+    \/\/ The stack increment resides just below the saved rbp\n+    addq(rsp, Address(rsp, initial_framesize - wordSize));\n+  } else {\n+    if (initial_framesize > 0) {\n+      addq(rsp, initial_framesize);\n+    }\n+    pop(rbp);\n+  }\n+}\n+\n@@ -5789,1 +6592,1 @@\n-void MacroAssembler::xmm_clear_mem(Register base, Register cnt, Register rtmp, XMMRegister xtmp, KRegister mask) {\n+void MacroAssembler::xmm_clear_mem(Register base, Register cnt, Register val, XMMRegister xtmp, KRegister mask) {\n@@ -5795,1 +6598,1 @@\n-    vpxor(xtmp, xtmp, xtmp, AVX_512bit);\n+    evpbroadcastq(xtmp, val, AVX_512bit);\n@@ -5797,1 +6600,3 @@\n-    vpxor(xtmp, xtmp, xtmp, AVX_256bit);\n+    movdq(xtmp, val);\n+    punpcklqdq(xtmp, xtmp);\n+    vinserti128_high(xtmp, xtmp);\n@@ -5799,1 +6604,2 @@\n-    pxor(xtmp, xtmp);\n+    movdq(xtmp, val);\n+    punpcklqdq(xtmp, xtmp);\n@@ -5822,1 +6628,1 @@\n-    fill64_masked(3, base, 0, xtmp, mask, cnt, rtmp, true);\n+    fill64_masked(3, base, 0, xtmp, mask, cnt, val, true);\n@@ -5841,1 +6647,1 @@\n-    fill32_masked(3, base, 0, xtmp, mask, cnt, rtmp);\n+    fill32_masked(3, base, 0, xtmp, mask, cnt, val);\n@@ -5943,2 +6749,2 @@\n-void MacroAssembler::clear_mem(Register base, Register cnt, Register tmp, XMMRegister xtmp,\n-                               bool is_large, KRegister mask) {\n+void MacroAssembler::clear_mem(Register base, Register cnt, Register val, XMMRegister xtmp,\n+                               bool is_large, bool word_copy_only, KRegister mask) {\n@@ -5949,1 +6755,1 @@\n-  assert(tmp==rax,   \"tmp register must be eax for rep stos\");\n+  assert(val==rax,   \"val register must be eax for rep stos\");\n@@ -5955,3 +6761,0 @@\n-  if (!is_large || !UseXMMForObjInit) {\n-    xorptr(tmp, tmp);\n-  }\n@@ -5969,1 +6772,1 @@\n-    movptr(Address(base, cnt, Address::times_ptr), tmp);\n+    movptr(Address(base, cnt, Address::times_ptr), val);\n@@ -5978,1 +6781,1 @@\n-  if (UseFastStosb) {\n+  if (UseFastStosb && !word_copy_only) {\n@@ -5982,1 +6785,1 @@\n-    xmm_clear_mem(base, cnt, tmp, xtmp, mask);\n+    xmm_clear_mem(base, cnt, val, xtmp, mask);\n@@ -9888,0 +10691,3 @@\n+  \/\/ Mask inline_type bit such that we go to the slow path if object is an inline type\n+  andptr(reg_rax, ~((int) markWord::inline_type_bit_in_place));\n+\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.cpp","additions":823,"deletions":17,"binary":false,"changes":840,"status":"modified"},{"patch":"@@ -1652,0 +1652,4 @@\n+  if (_entry_point == nullptr) {\n+    \/\/ CallLeafNoFPInDirect\n+    return 3; \/\/ callq (register)\n+  }\n@@ -1658,0 +1662,1 @@\n+\n@@ -1883,6 +1888,1 @@\n-  int framesize = C->output()->frame_size_in_bytes();\n-  int bangsize = C->output()->bang_size_in_bytes();\n-\n-  if (C->clinit_barrier_on_entry()) {\n-    assert(VM_Version::supports_fast_class_init_checks(), \"sanity\");\n-    assert(!C->method()->holder()->is_not_initialized(), \"initialization should have been started\");\n+  __ verified_entry(C);\n@@ -1890,9 +1890,2 @@\n-    Label L_skip_barrier;\n-    Register klass = rscratch1;\n-\n-    __ mov_metadata(klass, C->method()->holder()->constant_encoding());\n-    __ clinit_barrier(klass, &L_skip_barrier \/*L_fast_path*\/);\n-\n-    __ jump(RuntimeAddress(SharedRuntime::get_handle_wrong_method_stub())); \/\/ slow path\n-\n-    __ bind(L_skip_barrier);\n+  if (ra_->C->stub_function() == nullptr) {\n+    __ entry_barrier();\n@@ -1901,1 +1894,3 @@\n-  __ verified_entry(framesize, C->output()->need_stack_bang(bangsize)?bangsize:0, false, C->stub_function() != nullptr);\n+  if (!Compile::current()->output()->in_scratch_emit_size()) {\n+    __ bind(*_verified_entry);\n+  }\n@@ -1913,5 +1908,0 @@\n-uint MachPrologNode::size(PhaseRegAlloc* ra_) const\n-{\n-  return MachNode::size(ra_); \/\/ too many variables; just compute it\n-                              \/\/ the hard way\n-}\n@@ -1965,13 +1955,3 @@\n-  int framesize = C->output()->frame_size_in_bytes();\n-  assert((framesize & (StackAlignmentInBytes-1)) == 0, \"frame size not aligned\");\n-  \/\/ Remove word for return adr already pushed\n-  \/\/ and RBP\n-  framesize -= 2*wordSize;\n-\n-  \/\/ Note that VerifyStackAtCalls' Majik cookie does not change the frame size popped here\n-\n-  if (framesize) {\n-    __ addq(rsp, framesize);\n-  }\n-\n-  __ popq(rbp);\n+  \/\/ Subtract two words to account for return address and rbp\n+  int initial_framesize = C->output()->frame_size_in_bytes() - 2*wordSize;\n+  __ remove_frame(initial_framesize, C->needs_stack_repair());\n@@ -1996,6 +1976,0 @@\n-uint MachEpilogNode::size(PhaseRegAlloc* ra_) const\n-{\n-  return MachNode::size(ra_); \/\/ too many variables; just compute it\n-                              \/\/ the hard way\n-}\n-\n@@ -2603,0 +2577,43 @@\n+\/\/=============================================================================\n+#ifndef PRODUCT\n+void MachVEPNode::format(PhaseRegAlloc* ra_, outputStream* st) const\n+{\n+  st->print_cr(\"MachVEPNode\");\n+}\n+#endif\n+\n+void MachVEPNode::emit(C2_MacroAssembler* masm, PhaseRegAlloc* ra_) const\n+{\n+  CodeBuffer* cbuf = masm->code();\n+  uint insts_size = cbuf->insts_size();\n+  if (!_verified) {\n+    __ ic_check(1);\n+  } else {\n+    \/\/ TODO 8284443 Avoid creation of temporary frame\n+    if (ra_->C->stub_function() == nullptr) {\n+      __ verified_entry(ra_->C, 0);\n+      __ entry_barrier();\n+      int initial_framesize = ra_->C->output()->frame_size_in_bytes() - 2*wordSize;\n+      __ remove_frame(initial_framesize, false);\n+    }\n+    \/\/ Unpack inline type args passed as oop and then jump to\n+    \/\/ the verified entry point (skipping the unverified entry).\n+    int sp_inc = __ unpack_inline_args(ra_->C, _receiver_only);\n+    \/\/ Emit code for verified entry and save increment for stack repair on return\n+    __ verified_entry(ra_->C, sp_inc);\n+    if (Compile::current()->output()->in_scratch_emit_size()) {\n+      Label dummy_verified_entry;\n+      __ jmp(dummy_verified_entry);\n+    } else {\n+      __ jmp(*_verified_entry);\n+    }\n+  }\n+  \/* WARNING these NOPs are critical so that verified entry point is properly\n+     4 bytes aligned for patching by NativeJump::patch_verified_entry() *\/\n+  int nops_cnt = 4 - ((cbuf->insts_size() - insts_size) & 0x3);\n+  nops_cnt &= 0x3; \/\/ Do not add nops if code is aligned.\n+  if (nops_cnt > 0) {\n+    __ nop(nops_cnt);\n+  }\n+}\n+\n@@ -2623,6 +2640,0 @@\n-uint MachUEPNode::size(PhaseRegAlloc* ra_) const\n-{\n-  return MachNode::size(ra_); \/\/ too many variables; just compute it\n-                              \/\/ the hard way\n-}\n-\n@@ -4604,0 +4615,33 @@\n+    if (tf()->returns_inline_type_as_fields() && !_method->is_method_handle_intrinsic() && _method->return_type()->is_loaded()) {\n+      \/\/ The last return value is not set by the callee but used to pass the null marker to compiled code.\n+      \/\/ Search for the corresponding projection, get the register and emit code that initialized it.\n+      uint con = (tf()->range_cc()->cnt() - 1);\n+      for (DUIterator_Fast imax, i = fast_outs(imax); i < imax; i++) {\n+        ProjNode* proj = fast_out(i)->as_Proj();\n+        if (proj->_con == con) {\n+          \/\/ Set null marker if rax is non-null (a non-null value is returned buffered or scalarized)\n+          OptoReg::Name optoReg = ra_->get_reg_first(proj);\n+          VMReg reg = OptoReg::as_VMReg(optoReg, ra_->_framesize, OptoReg::reg2stack(ra_->_matcher._new_SP));\n+          Register toReg = reg->is_reg() ? reg->as_Register() : rscratch1;\n+          __ testq(rax, rax);\n+          __ setb(Assembler::notZero, toReg);\n+          __ movzbl(toReg, toReg);\n+          if (reg->is_stack()) {\n+            int st_off = reg->reg2stack() * VMRegImpl::stack_slot_size;\n+            __ movq(Address(rsp, st_off), toReg);\n+          }\n+          break;\n+        }\n+      }\n+      if (return_value_is_used()) {\n+        \/\/ An inline type is returned as fields in multiple registers.\n+        \/\/ Rax either contains an oop if the inline type is buffered or a pointer\n+        \/\/ to the corresponding InlineKlass with the lowest bit set to 1. Zero rax\n+        \/\/ if the lowest bit is set to allow C2 to use the oop after null checking.\n+        \/\/ rax &= (rax & 1) - 1\n+        __ movptr(rscratch1, rax);\n+        __ andptr(rscratch1, 0x1);\n+        __ subptr(rscratch1, 0x1);\n+        __ andptr(rax, rscratch1);\n+      }\n+    }\n@@ -5766,0 +5810,16 @@\n+\/\/ Indirect Narrow Oop Operand\n+operand indCompressedOop(rRegN reg) %{\n+  predicate(UseCompressedOops && (CompressedOops::shift() == Address::times_8));\n+  constraint(ALLOC_IN_RC(ptr_reg));\n+  match(DecodeN reg);\n+\n+  op_cost(10);\n+  format %{\"[R12 + $reg << 3] (compressed oop addressing)\" %}\n+  interface(MEMORY_INTER) %{\n+    base(0xc); \/\/ R12\n+    index($reg);\n+    scale(0x3);\n+    disp(0x0);\n+  %}\n+%}\n+\n@@ -6242,1 +6302,1 @@\n-               indCompressedOopOffset,\n+               indCompressedOop, indCompressedOopOffset,\n@@ -8840,0 +8900,26 @@\n+instruct castI2N(rRegN dst, rRegI src)\n+%{\n+  match(Set dst (CastI2N src));\n+\n+  format %{ \"movq    $dst, $src\\t# int -> narrow ptr\" %}\n+  ins_encode %{\n+    if ($dst$$reg != $src$$reg) {\n+      __ movl($dst$$Register, $src$$Register);\n+    }\n+  %}\n+  ins_pipe(ialu_reg_reg); \/\/ XXX\n+%}\n+\n+instruct castN2X(rRegL dst, rRegN src)\n+%{\n+  match(Set dst (CastP2X src));\n+\n+  format %{ \"movq    $dst, $src\\t# ptr -> long\" %}\n+  ins_encode %{\n+    if ($dst$$reg != $src$$reg) {\n+      __ movptr($dst$$Register, $src$$Register);\n+    }\n+  %}\n+  ins_pipe(ialu_reg_reg); \/\/ XXX\n+%}\n+\n@@ -15086,0 +15172,1 @@\n+\n@@ -15088,1 +15175,1 @@\n-instruct rep_stos(rcx_RegL cnt, rdi_RegP base, regD tmp, rax_RegI zero,\n+instruct rep_stos(rcx_RegL cnt, rdi_RegP base, regD tmp, rax_RegL val,\n@@ -15091,3 +15178,120 @@\n-  predicate(!((ClearArrayNode*)n)->is_large() && (UseAVX <= 2));\n-  match(Set dummy (ClearArray cnt base));\n-  effect(USE_KILL cnt, USE_KILL base, TEMP tmp, KILL zero, KILL cr);\n+  predicate(!((ClearArrayNode*)n)->is_large() && !((ClearArrayNode*)n)->word_copy_only() && (UseAVX <= 2));\n+  match(Set dummy (ClearArray (Binary cnt base) val));\n+  effect(USE_KILL cnt, USE_KILL base, TEMP tmp, USE_KILL val, KILL cr);\n+\n+  format %{ $$template\n+    $$emit$$\"cmp     InitArrayShortSize,rcx\\n\\t\"\n+    $$emit$$\"jg      LARGE\\n\\t\"\n+    $$emit$$\"dec     rcx\\n\\t\"\n+    $$emit$$\"js      DONE\\t# Zero length\\n\\t\"\n+    $$emit$$\"mov     rax,(rdi,rcx,8)\\t# LOOP\\n\\t\"\n+    $$emit$$\"dec     rcx\\n\\t\"\n+    $$emit$$\"jge     LOOP\\n\\t\"\n+    $$emit$$\"jmp     DONE\\n\\t\"\n+    $$emit$$\"# LARGE:\\n\\t\"\n+    if (UseFastStosb) {\n+       $$emit$$\"shlq    rcx,3\\t# Convert doublewords to bytes\\n\\t\"\n+       $$emit$$\"rep     stosb\\t# Store rax to *rdi++ while rcx--\\n\\t\"\n+    } else if (UseXMMForObjInit) {\n+       $$emit$$\"movdq   $tmp, $val\\n\\t\"\n+       $$emit$$\"punpcklqdq $tmp, $tmp\\n\\t\"\n+       $$emit$$\"vinserti128_high $tmp, $tmp\\n\\t\"\n+       $$emit$$\"jmpq    L_zero_64_bytes\\n\\t\"\n+       $$emit$$\"# L_loop:\\t# 64-byte LOOP\\n\\t\"\n+       $$emit$$\"vmovdqu $tmp,(rax)\\n\\t\"\n+       $$emit$$\"vmovdqu $tmp,0x20(rax)\\n\\t\"\n+       $$emit$$\"add     0x40,rax\\n\\t\"\n+       $$emit$$\"# L_zero_64_bytes:\\n\\t\"\n+       $$emit$$\"sub     0x8,rcx\\n\\t\"\n+       $$emit$$\"jge     L_loop\\n\\t\"\n+       $$emit$$\"add     0x4,rcx\\n\\t\"\n+       $$emit$$\"jl      L_tail\\n\\t\"\n+       $$emit$$\"vmovdqu $tmp,(rax)\\n\\t\"\n+       $$emit$$\"add     0x20,rax\\n\\t\"\n+       $$emit$$\"sub     0x4,rcx\\n\\t\"\n+       $$emit$$\"# L_tail:\\t# Clearing tail bytes\\n\\t\"\n+       $$emit$$\"add     0x4,rcx\\n\\t\"\n+       $$emit$$\"jle     L_end\\n\\t\"\n+       $$emit$$\"dec     rcx\\n\\t\"\n+       $$emit$$\"# L_sloop:\\t# 8-byte short loop\\n\\t\"\n+       $$emit$$\"vmovq   xmm0,(rax)\\n\\t\"\n+       $$emit$$\"add     0x8,rax\\n\\t\"\n+       $$emit$$\"dec     rcx\\n\\t\"\n+       $$emit$$\"jge     L_sloop\\n\\t\"\n+       $$emit$$\"# L_end:\\n\\t\"\n+    } else {\n+       $$emit$$\"rep     stosq\\t# Store rax to *rdi++ while rcx--\\n\\t\"\n+    }\n+    $$emit$$\"# DONE\"\n+  %}\n+  ins_encode %{\n+    __ clear_mem($base$$Register, $cnt$$Register, $val$$Register,\n+                 $tmp$$XMMRegister, false, false);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct rep_stos_word_copy(rcx_RegL cnt, rdi_RegP base, regD tmp, rax_RegL val,\n+                            Universe dummy, rFlagsReg cr)\n+%{\n+  predicate(!((ClearArrayNode*)n)->is_large() && ((ClearArrayNode*)n)->word_copy_only() && (UseAVX <= 2));\n+  match(Set dummy (ClearArray (Binary cnt base) val));\n+  effect(USE_KILL cnt, USE_KILL base, TEMP tmp, USE_KILL val, KILL cr);\n+\n+  format %{ $$template\n+    $$emit$$\"cmp     InitArrayShortSize,rcx\\n\\t\"\n+    $$emit$$\"jg      LARGE\\n\\t\"\n+    $$emit$$\"dec     rcx\\n\\t\"\n+    $$emit$$\"js      DONE\\t# Zero length\\n\\t\"\n+    $$emit$$\"mov     rax,(rdi,rcx,8)\\t# LOOP\\n\\t\"\n+    $$emit$$\"dec     rcx\\n\\t\"\n+    $$emit$$\"jge     LOOP\\n\\t\"\n+    $$emit$$\"jmp     DONE\\n\\t\"\n+    $$emit$$\"# LARGE:\\n\\t\"\n+    if (UseXMMForObjInit) {\n+       $$emit$$\"movdq   $tmp, $val\\n\\t\"\n+       $$emit$$\"punpcklqdq $tmp, $tmp\\n\\t\"\n+       $$emit$$\"vinserti128_high $tmp, $tmp\\n\\t\"\n+       $$emit$$\"jmpq    L_zero_64_bytes\\n\\t\"\n+       $$emit$$\"# L_loop:\\t# 64-byte LOOP\\n\\t\"\n+       $$emit$$\"vmovdqu $tmp,(rax)\\n\\t\"\n+       $$emit$$\"vmovdqu $tmp,0x20(rax)\\n\\t\"\n+       $$emit$$\"add     0x40,rax\\n\\t\"\n+       $$emit$$\"# L_zero_64_bytes:\\n\\t\"\n+       $$emit$$\"sub     0x8,rcx\\n\\t\"\n+       $$emit$$\"jge     L_loop\\n\\t\"\n+       $$emit$$\"add     0x4,rcx\\n\\t\"\n+       $$emit$$\"jl      L_tail\\n\\t\"\n+       $$emit$$\"vmovdqu $tmp,(rax)\\n\\t\"\n+       $$emit$$\"add     0x20,rax\\n\\t\"\n+       $$emit$$\"sub     0x4,rcx\\n\\t\"\n+       $$emit$$\"# L_tail:\\t# Clearing tail bytes\\n\\t\"\n+       $$emit$$\"add     0x4,rcx\\n\\t\"\n+       $$emit$$\"jle     L_end\\n\\t\"\n+       $$emit$$\"dec     rcx\\n\\t\"\n+       $$emit$$\"# L_sloop:\\t# 8-byte short loop\\n\\t\"\n+       $$emit$$\"vmovq   xmm0,(rax)\\n\\t\"\n+       $$emit$$\"add     0x8,rax\\n\\t\"\n+       $$emit$$\"dec     rcx\\n\\t\"\n+       $$emit$$\"jge     L_sloop\\n\\t\"\n+       $$emit$$\"# L_end:\\n\\t\"\n+    } else {\n+       $$emit$$\"rep     stosq\\t# Store rax to *rdi++ while rcx--\\n\\t\"\n+    }\n+    $$emit$$\"# DONE\"\n+  %}\n+  ins_encode %{\n+    __ clear_mem($base$$Register, $cnt$$Register, $val$$Register,\n+                 $tmp$$XMMRegister, false, true);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ Small non-constant length ClearArray for AVX512 targets.\n+instruct rep_stos_evex(rcx_RegL cnt, rdi_RegP base, legRegD tmp, kReg ktmp, rax_RegL val,\n+                       Universe dummy, rFlagsReg cr)\n+%{\n+  predicate(!((ClearArrayNode*)n)->is_large() && !((ClearArrayNode*)n)->word_copy_only() && (UseAVX > 2));\n+  match(Set dummy (ClearArray (Binary cnt base) val));\n+  ins_cost(125);\n+  effect(USE_KILL cnt, USE_KILL base, TEMP tmp, TEMP ktmp, USE_KILL val, KILL cr);\n@@ -15141,2 +15345,2 @@\n-    __ clear_mem($base$$Register, $cnt$$Register, $zero$$Register,\n-                 $tmp$$XMMRegister, false, knoreg);\n+    __ clear_mem($base$$Register, $cnt$$Register, $val$$Register,\n+                 $tmp$$XMMRegister, false, false, $ktmp$$KRegister);\n@@ -15147,3 +15351,2 @@\n-\/\/ Small non-constant length ClearArray for AVX512 targets.\n-instruct rep_stos_evex(rcx_RegL cnt, rdi_RegP base, legRegD tmp, kReg ktmp, rax_RegI zero,\n-                       Universe dummy, rFlagsReg cr)\n+instruct rep_stos_evex_word_copy(rcx_RegL cnt, rdi_RegP base, legRegD tmp, kReg ktmp, rax_RegL val,\n+                                 Universe dummy, rFlagsReg cr)\n@@ -15151,2 +15354,2 @@\n-  predicate(!((ClearArrayNode*)n)->is_large() && (UseAVX > 2));\n-  match(Set dummy (ClearArray cnt base));\n+  predicate(!((ClearArrayNode*)n)->is_large() && ((ClearArrayNode*)n)->word_copy_only() && (UseAVX > 2));\n+  match(Set dummy (ClearArray (Binary cnt base) val));\n@@ -15154,1 +15357,1 @@\n-  effect(USE_KILL cnt, USE_KILL base, TEMP tmp, TEMP ktmp, KILL zero, KILL cr);\n+  effect(USE_KILL cnt, USE_KILL base, TEMP tmp, TEMP ktmp, USE_KILL val, KILL cr);\n@@ -15202,2 +15405,2 @@\n-    __ clear_mem($base$$Register, $cnt$$Register, $zero$$Register,\n-                 $tmp$$XMMRegister, false, $ktmp$$KRegister);\n+    __ clear_mem($base$$Register, $cnt$$Register, $val$$Register,\n+                 $tmp$$XMMRegister, false, true, $ktmp$$KRegister);\n@@ -15209,1 +15412,1 @@\n-instruct rep_stos_large(rcx_RegL cnt, rdi_RegP base, regD tmp, rax_RegI zero,\n+instruct rep_stos_large(rcx_RegL cnt, rdi_RegP base, regD tmp, rax_RegL val,\n@@ -15212,3 +15415,99 @@\n-  predicate((UseAVX <=2) && ((ClearArrayNode*)n)->is_large());\n-  match(Set dummy (ClearArray cnt base));\n-  effect(USE_KILL cnt, USE_KILL base, TEMP tmp, KILL zero, KILL cr);\n+  predicate(((ClearArrayNode*)n)->is_large() && !((ClearArrayNode*)n)->word_copy_only() && (UseAVX <= 2));\n+  match(Set dummy (ClearArray (Binary cnt base) val));\n+  effect(USE_KILL cnt, USE_KILL base, TEMP tmp, USE_KILL val, KILL cr);\n+\n+  format %{ $$template\n+    if (UseFastStosb) {\n+       $$emit$$\"shlq    rcx,3\\t# Convert doublewords to bytes\\n\\t\"\n+       $$emit$$\"rep     stosb\\t# Store rax to *rdi++ while rcx--\"\n+    } else if (UseXMMForObjInit) {\n+       $$emit$$\"movdq   $tmp, $val\\n\\t\"\n+       $$emit$$\"punpcklqdq $tmp, $tmp\\n\\t\"\n+       $$emit$$\"vinserti128_high $tmp, $tmp\\n\\t\"\n+       $$emit$$\"jmpq    L_zero_64_bytes\\n\\t\"\n+       $$emit$$\"# L_loop:\\t# 64-byte LOOP\\n\\t\"\n+       $$emit$$\"vmovdqu $tmp,(rax)\\n\\t\"\n+       $$emit$$\"vmovdqu $tmp,0x20(rax)\\n\\t\"\n+       $$emit$$\"add     0x40,rax\\n\\t\"\n+       $$emit$$\"# L_zero_64_bytes:\\n\\t\"\n+       $$emit$$\"sub     0x8,rcx\\n\\t\"\n+       $$emit$$\"jge     L_loop\\n\\t\"\n+       $$emit$$\"add     0x4,rcx\\n\\t\"\n+       $$emit$$\"jl      L_tail\\n\\t\"\n+       $$emit$$\"vmovdqu $tmp,(rax)\\n\\t\"\n+       $$emit$$\"add     0x20,rax\\n\\t\"\n+       $$emit$$\"sub     0x4,rcx\\n\\t\"\n+       $$emit$$\"# L_tail:\\t# Clearing tail bytes\\n\\t\"\n+       $$emit$$\"add     0x4,rcx\\n\\t\"\n+       $$emit$$\"jle     L_end\\n\\t\"\n+       $$emit$$\"dec     rcx\\n\\t\"\n+       $$emit$$\"# L_sloop:\\t# 8-byte short loop\\n\\t\"\n+       $$emit$$\"vmovq   xmm0,(rax)\\n\\t\"\n+       $$emit$$\"add     0x8,rax\\n\\t\"\n+       $$emit$$\"dec     rcx\\n\\t\"\n+       $$emit$$\"jge     L_sloop\\n\\t\"\n+       $$emit$$\"# L_end:\\n\\t\"\n+    } else {\n+       $$emit$$\"rep     stosq\\t# Store rax to *rdi++ while rcx--\"\n+    }\n+  %}\n+  ins_encode %{\n+    __ clear_mem($base$$Register, $cnt$$Register, $val$$Register,\n+                 $tmp$$XMMRegister, true, false);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct rep_stos_large_word_copy(rcx_RegL cnt, rdi_RegP base, regD tmp, rax_RegL val,\n+                                  Universe dummy, rFlagsReg cr)\n+%{\n+  predicate(((ClearArrayNode*)n)->is_large() && ((ClearArrayNode*)n)->word_copy_only() && (UseAVX <= 2));\n+  match(Set dummy (ClearArray (Binary cnt base) val));\n+  effect(USE_KILL cnt, USE_KILL base, TEMP tmp, USE_KILL val, KILL cr);\n+\n+  format %{ $$template\n+    if (UseXMMForObjInit) {\n+       $$emit$$\"movdq   $tmp, $val\\n\\t\"\n+       $$emit$$\"punpcklqdq $tmp, $tmp\\n\\t\"\n+       $$emit$$\"vinserti128_high $tmp, $tmp\\n\\t\"\n+       $$emit$$\"jmpq    L_zero_64_bytes\\n\\t\"\n+       $$emit$$\"# L_loop:\\t# 64-byte LOOP\\n\\t\"\n+       $$emit$$\"vmovdqu $tmp,(rax)\\n\\t\"\n+       $$emit$$\"vmovdqu $tmp,0x20(rax)\\n\\t\"\n+       $$emit$$\"add     0x40,rax\\n\\t\"\n+       $$emit$$\"# L_zero_64_bytes:\\n\\t\"\n+       $$emit$$\"sub     0x8,rcx\\n\\t\"\n+       $$emit$$\"jge     L_loop\\n\\t\"\n+       $$emit$$\"add     0x4,rcx\\n\\t\"\n+       $$emit$$\"jl      L_tail\\n\\t\"\n+       $$emit$$\"vmovdqu $tmp,(rax)\\n\\t\"\n+       $$emit$$\"add     0x20,rax\\n\\t\"\n+       $$emit$$\"sub     0x4,rcx\\n\\t\"\n+       $$emit$$\"# L_tail:\\t# Clearing tail bytes\\n\\t\"\n+       $$emit$$\"add     0x4,rcx\\n\\t\"\n+       $$emit$$\"jle     L_end\\n\\t\"\n+       $$emit$$\"dec     rcx\\n\\t\"\n+       $$emit$$\"# L_sloop:\\t# 8-byte short loop\\n\\t\"\n+       $$emit$$\"vmovq   xmm0,(rax)\\n\\t\"\n+       $$emit$$\"add     0x8,rax\\n\\t\"\n+       $$emit$$\"dec     rcx\\n\\t\"\n+       $$emit$$\"jge     L_sloop\\n\\t\"\n+       $$emit$$\"# L_end:\\n\\t\"\n+    } else {\n+       $$emit$$\"rep     stosq\\t# Store rax to *rdi++ while rcx--\"\n+    }\n+  %}\n+  ins_encode %{\n+    __ clear_mem($base$$Register, $cnt$$Register, $val$$Register,\n+                 $tmp$$XMMRegister, true, true);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ Large non-constant length ClearArray for AVX512 targets.\n+instruct rep_stos_large_evex(rcx_RegL cnt, rdi_RegP base, legRegD tmp, kReg ktmp, rax_RegL val,\n+                             Universe dummy, rFlagsReg cr)\n+%{\n+  predicate(((ClearArrayNode*)n)->is_large() && !((ClearArrayNode*)n)->word_copy_only() && (UseAVX > 2));\n+  match(Set dummy (ClearArray (Binary cnt base) val));\n+  effect(USE_KILL cnt, USE_KILL base, TEMP tmp, TEMP ktmp, USE_KILL val, KILL cr);\n@@ -15253,2 +15552,2 @@\n-    __ clear_mem($base$$Register, $cnt$$Register, $zero$$Register,\n-                 $tmp$$XMMRegister, true, knoreg);\n+    __ clear_mem($base$$Register, $cnt$$Register, $val$$Register,\n+                 $tmp$$XMMRegister, true, false, $ktmp$$KRegister);\n@@ -15259,3 +15558,2 @@\n-\/\/ Large non-constant length ClearArray for AVX512 targets.\n-instruct rep_stos_large_evex(rcx_RegL cnt, rdi_RegP base, legRegD tmp, kReg ktmp, rax_RegI zero,\n-                             Universe dummy, rFlagsReg cr)\n+instruct rep_stos_large_evex_word_copy(rcx_RegL cnt, rdi_RegP base, legRegD tmp, kReg ktmp, rax_RegL val,\n+                                       Universe dummy, rFlagsReg cr)\n@@ -15263,3 +15561,3 @@\n-  predicate((UseAVX > 2) && ((ClearArrayNode*)n)->is_large());\n-  match(Set dummy (ClearArray cnt base));\n-  effect(USE_KILL cnt, USE_KILL base, TEMP tmp, TEMP ktmp, KILL zero, KILL cr);\n+  predicate(((ClearArrayNode*)n)->is_large() && ((ClearArrayNode*)n)->word_copy_only() && (UseAVX > 2));\n+  match(Set dummy (ClearArray (Binary cnt base) val));\n+  effect(USE_KILL cnt, USE_KILL base, TEMP tmp, TEMP ktmp, USE_KILL val, KILL cr);\n@@ -15304,2 +15602,2 @@\n-    __ clear_mem($base$$Register, $cnt$$Register, $zero$$Register,\n-                 $tmp$$XMMRegister, true, $ktmp$$KRegister);\n+    __ clear_mem($base$$Register, $cnt$$Register, $val$$Register,\n+                 $tmp$$XMMRegister, true, true, $ktmp$$KRegister);\n@@ -15311,1 +15609,1 @@\n-instruct rep_stos_im(immL cnt, rRegP base, regD tmp, rRegI zero, kReg ktmp, Universe dummy, rFlagsReg cr)\n+instruct rep_stos_im(immL cnt, rRegP base, regD tmp, rax_RegL val, kReg ktmp, Universe dummy, rFlagsReg cr)\n@@ -15313,2 +15611,3 @@\n-  predicate(!((ClearArrayNode*)n)->is_large() && (MaxVectorSize >= 32) && VM_Version::supports_avx512vl());\n-  match(Set dummy (ClearArray cnt base));\n+  predicate(!((ClearArrayNode*)n)->is_large() && !((ClearArrayNode*)n)->word_copy_only() &&\n+            ((MaxVectorSize >= 32) && VM_Version::supports_avx512vl()));\n+  match(Set dummy (ClearArray (Binary cnt base) val));\n@@ -15316,1 +15615,1 @@\n-  effect(TEMP tmp, TEMP zero, TEMP ktmp, KILL cr);\n+  effect(TEMP tmp, USE_KILL val, TEMP ktmp, KILL cr);\n@@ -15319,1 +15618,1 @@\n-   __ clear_mem($base$$Register, $cnt$$constant, $zero$$Register, $tmp$$XMMRegister, $ktmp$$KRegister);\n+    __ clear_mem($base$$Register, $cnt$$constant, $val$$Register, $tmp$$XMMRegister, $ktmp$$KRegister);\n@@ -17166,0 +17465,16 @@\n+\/\/ Call runtime without safepoint\n+\/\/ entry point is null, target holds the address to call\n+instruct CallLeafNoFPInDirect(rRegP target)\n+%{\n+  predicate(n->as_Call()->entry_point() == nullptr);\n+  match(CallLeafNoFP target);\n+\n+  ins_cost(300);\n+  format %{ \"call_leaf_nofp,runtime indirect \" %}\n+  ins_encode %{\n+     __ call($target$$Register);\n+  %}\n+\n+  ins_pipe(pipe_slow);\n+%}\n+\n@@ -17169,0 +17484,1 @@\n+  predicate(n->as_Call()->entry_point() != nullptr);\n","filename":"src\/hotspot\/cpu\/x86\/x86.ad","additions":396,"deletions":80,"binary":false,"changes":476,"status":"modified"},{"patch":"@@ -46,0 +46,1 @@\n+#include \"runtime\/arguments.hpp\"\n@@ -297,1 +298,1 @@\n-  memset(mem, 0, objArrayOopDesc::object_size(element_count));\n+  memset(mem, 0, refArrayOopDesc::object_size(element_count));\n@@ -303,0 +304,1 @@\n+    assert(!Arguments::is_valhalla_enabled() || Universe::objectArrayKlass()->prototype_header() == markWord::prototype(), \"should be the same\");\n@@ -333,1 +335,1 @@\n-  assert(objArrayOopDesc::object_size(max_elem_count)*HeapWordSize == MIN_GC_REGION_ALIGNMENT,\n+  assert(refArrayOopDesc::object_size(max_elem_count)*HeapWordSize == MIN_GC_REGION_ALIGNMENT,\n@@ -448,1 +450,1 @@\n-  size_t byte_size = objArrayOopDesc::object_size(length) * HeapWordSize;\n+  size_t byte_size = refArrayOopDesc::object_size(length) * HeapWordSize;\n@@ -477,0 +479,1 @@\n+    assert(!Arguments::is_valhalla_enabled() || Universe::objectArrayKlass()->prototype_header() == markWord::prototype(), \"should be the same\");\n@@ -731,1 +734,2 @@\n-    fake_oop->set_mark(markWord::prototype().set_narrow_klass(nk));\n+    markWord prototype_header = src_klass->prototype_header().set_narrow_klass(nk);\n+    fake_oop->set_mark(prototype_header);\n@@ -741,1 +745,1 @@\n-  if (!src_obj->fast_no_hash_check()) {\n+  if (!src_obj->fast_no_hash_check() && (!(Arguments::is_valhalla_enabled() && src_obj->mark().is_inline_type()))) {\n@@ -744,1 +748,3 @@\n-      fake_oop->set_mark(markWord::prototype().set_narrow_klass(nk).copy_set_hash(src_hash));\n+      fake_oop->set_mark(fake_oop->mark().copy_set_hash(src_hash));\n+    } else if (Arguments::is_valhalla_enabled()) {\n+      fake_oop->set_mark(src_klass->prototype_header().copy_set_hash(src_hash));\n","filename":"src\/hotspot\/share\/cds\/aotMappedHeapWriter.cpp","additions":12,"deletions":6,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -80,0 +80,2 @@\n+#include \"oops\/flatArrayKlass.hpp\"\n+#include \"oops\/inlineKlass.hpp\"\n@@ -506,1 +508,1 @@\n-  soc->do_tag(objArrayOopDesc::base_offset_in_bytes());\n+  soc->do_tag(refArrayOopDesc::base_offset_in_bytes());\n@@ -585,1 +587,8 @@\n-        case atos: new_code = Bytecodes::_fast_agetfield; break;\n+        case atos: {\n+          if (rfe->is_flat()) {\n+            new_code = Bytecodes::_fast_vgetfield;\n+          } else {\n+            new_code = Bytecodes::_fast_agetfield;\n+          }\n+          break;\n+        }\n@@ -610,1 +619,8 @@\n-        case atos: new_code = Bytecodes::_fast_aputfield; break;\n+        case atos: {\n+          if (rfe->is_flat() || rfe->is_null_free_inline_type()) {\n+            new_code = Bytecodes::_fast_vputfield;\n+          } else {\n+            new_code = Bytecodes::_fast_aputfield;\n+          }\n+          break;\n+        }\n","filename":"src\/hotspot\/share\/cds\/aotMetaspace.cpp","additions":19,"deletions":3,"binary":false,"changes":22,"status":"modified"},{"patch":"@@ -853,1 +853,1 @@\n-      k->set_prototype_header(markWord::prototype().set_narrow_klass(nk));\n+      k->set_prototype_header_klass(nk);\n@@ -856,1 +856,7 @@\n-    if (k->is_objArray_klass()) {\n+    if (k->is_flatArray_klass()) {\n+      num_obj_array_klasses ++;\n+      type = \"flat array\";\n+    } else if (k->is_refArray_klass()) {\n+        num_obj_array_klasses ++;\n+        type = \"ref array\";\n+    } else if (k->is_objArray_klass()) {\n@@ -860,1 +866,1 @@\n-      type = \"array\";\n+      type = \"obj array\";\n","filename":"src\/hotspot\/share\/cds\/archiveBuilder.cpp","additions":9,"deletions":3,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -99,1 +99,1 @@\n-\/\/ JimageFile pointer, or null if exploded JDK build.\n+\/\/ JImageFile pointer, or null if exploded JDK build.\n@@ -102,0 +102,9 @@\n+\/\/ PreviewMode status to control preview behaviour. JImage_file is unusable\n+\/\/ for normal lookup until (Preview_mode != PREVIEW_MODE_UNINITIALIZED).\n+enum PreviewMode {\n+  PREVIEW_MODE_UNINITIALIZED = 0,\n+  PREVIEW_MODE_DEFAULT = 1,\n+  PREVIEW_MODE_ENABLE_PREVIEW = 2\n+};\n+static PreviewMode                     Preview_mode           = PREVIEW_MODE_UNINITIALIZED;\n+\n@@ -156,1 +165,1 @@\n-ClassPathEntry* ClassLoader::_jrt_entry = nullptr;\n+ClassPathImageEntry* ClassLoader::_jrt_entry = nullptr;\n@@ -173,9 +182,0 @@\n-static const char* get_jimage_version_string() {\n-  static char version_string[10] = \"\";\n-  if (version_string[0] == '\\0') {\n-    jio_snprintf(version_string, sizeof(version_string), \"%d.%d\",\n-                 VM_Version::vm_major_version(), VM_Version::vm_minor_version());\n-  }\n-  return (const char*)version_string;\n-}\n-\n@@ -236,0 +236,59 @@\n+\/\/ --------------------------------\n+\/\/ The following jimage_xxx static functions encapsulate all JImage_file and Preview_mode access.\n+\/\/ This is done to make it easy to reason about the JImage file state (exists vs initialized etc.).\n+\n+\/\/ Opens the named JImage file and sets the JImage file reference.\n+\/\/ Returns true if opening the JImage file was successful (see also jimage_exists()).\n+static bool jimage_open(const char* modules_path) {\n+  \/\/ Currently 'error' is not set to anything useful, so ignore it here.\n+  jint error;\n+  JImage_file = (*JImageOpen)(modules_path, &error);\n+  return JImage_file != nullptr;\n+}\n+\n+\/\/ Closes and clears the JImage file reference (this will only be called during shutdown).\n+static void jimage_close() {\n+  if (JImage_file != nullptr) {\n+    (*JImageClose)(JImage_file);\n+    JImage_file = nullptr;\n+  }\n+}\n+\n+\/\/ Returns whether a JImage file was opened (but NOT whether it was initialized yet).\n+static bool jimage_exists() {\n+  return JImage_file != nullptr;\n+}\n+\n+\/\/ Returns the JImage file reference (which may or may not be initialized).\n+static JImageFile* jimage_non_null() {\n+  assert(jimage_exists(), \"should have been opened by ClassLoader::lookup_vm_options \"\n+                          \"and remained throughout normal JVM lifetime\");\n+  return JImage_file;\n+}\n+\n+\/\/ Returns true if jimage_init() has been called. Once the JImage file is initialized,\n+\/\/ jimage_is_preview_enabled() can be called to correctly determine the access mode.\n+static bool jimage_is_initialized() {\n+  return jimage_exists() && Preview_mode != PREVIEW_MODE_UNINITIALIZED;\n+}\n+\n+\/\/ Returns the access mode for an initialized JImage file (reflects --enable-preview).\n+static bool is_preview_enabled() {\n+  return Preview_mode == PREVIEW_MODE_ENABLE_PREVIEW;\n+}\n+\n+\/\/ Looks up the location of a named JImage resource. This \"raw\" lookup function allows\n+\/\/ the preview mode to be manually specified, so must not be accessible outside this\n+\/\/ class. ClassPathImageEntry manages all calls for resources after startup is complete.\n+static JImageLocationRef jimage_find_resource(const char* module_name,\n+                                              const char* file_name,\n+                                              bool is_preview,\n+                                              jlong *size) {\n+  return ((*JImageFindResource)(jimage_non_null(),\n+                                module_name,\n+                                file_name,\n+                                is_preview,\n+                                size));\n+}\n+\/\/ --------------------------------\n+\n@@ -374,15 +433,1 @@\n-JImageFile* ClassPathImageEntry::jimage() const {\n-  return JImage_file;\n-}\n-\n-JImageFile* ClassPathImageEntry::jimage_non_null() const {\n-  assert(ClassLoader::has_jrt_entry(), \"must be\");\n-  assert(jimage() != nullptr, \"should have been opened by ClassLoader::lookup_vm_options \"\n-                           \"and remained throughout normal JVM lifetime\");\n-  return jimage();\n-}\n-\n-  if (jimage() != nullptr) {\n-    (*JImageClose)(jimage());\n-    JImage_file = nullptr;\n-  }\n+  jimage_close();\n@@ -392,1 +437,1 @@\n-ClassPathImageEntry::ClassPathImageEntry(JImageFile* jimage, const char* name) :\n+ClassPathImageEntry::ClassPathImageEntry(const char* name) :\n@@ -394,1 +439,1 @@\n-  guarantee(jimage != nullptr, \"jimage file is null\");\n+  guarantee(jimage_is_initialized(), \"jimage is not initialized\");\n@@ -396,0 +441,1 @@\n+\n@@ -414,0 +460,2 @@\n+  const bool is_preview = is_preview_enabled();\n+\n@@ -422,1 +470,1 @@\n-      location = (*JImageFindResource)(jimage_non_null(), JAVA_BASE_NAME, get_jimage_version_string(), name, &size);\n+      location = jimage_find_resource(JAVA_BASE_NAME, name, is_preview, &size);\n@@ -433,1 +481,1 @@\n-          location = (*JImageFindResource)(jimage_non_null(), module_name, get_jimage_version_string(), name, &size);\n+          location = jimage_find_resource(module_name, name, is_preview, &size);\n@@ -446,1 +494,1 @@\n-    assert(this == (ClassPathImageEntry*)ClassLoader::get_jrt_entry(), \"must be\");\n+    assert(this == ClassLoader::get_jrt_entry(), \"must be\");\n@@ -456,7 +504,0 @@\n-JImageLocationRef ClassLoader::jimage_find_resource(JImageFile* jf,\n-                                                    const char* module_name,\n-                                                    const char* file_name,\n-                                                    jlong &size) {\n-  return ((*JImageFindResource)(jf, module_name, get_jimage_version_string(), file_name, &size));\n-}\n-\n@@ -465,1 +506,1 @@\n-  assert(this == (ClassPathImageEntry*)ClassLoader::get_jrt_entry(), \"must be used for jrt entry\");\n+  assert(this == ClassLoader::get_jrt_entry(), \"must be used for jrt entry\");\n@@ -620,1 +661,1 @@\n-        if (JImage_file != nullptr) {\n+        if (jimage_exists()) {\n@@ -625,1 +666,3 @@\n-          _jrt_entry = new ClassPathImageEntry(JImage_file, canonical_path);\n+          \/\/ Hand over lifecycle control of the JImage file to the _jrt_entry singleton\n+          \/\/ (see ClassPathImageEntry::close_jimage). The image must be initialized by now.\n+          _jrt_entry = new ClassPathImageEntry(canonical_path);\n@@ -627,1 +670,0 @@\n-          assert(_jrt_entry->jimage() != nullptr, \"No java runtime image\");\n@@ -647,1 +689,1 @@\n-  \/\/ 10 represents the length of \"modules\" + 2 file separators + \\0\n+  \/\/ 10 represents the length of \"modules\" (7) + 2 file separators + \\0\n@@ -654,0 +696,10 @@\n+\/\/ Gets a preview path for a given class path as a resource.\n+static const char* get_preview_path(const char* path) {\n+  const char file_sep = os::file_separator()[0];\n+  \/\/ 18 represents the length of \"META-INF\" (8) + \"preview\" (7) + 2 file separators + \\0\n+  size_t len = strlen(path) + 18;\n+  char *preview_path = NEW_RESOURCE_ARRAY(char, len);\n+  jio_snprintf(preview_path, len, \"%s%cMETA-INF%cpreview\", path, file_sep, file_sep);\n+  return preview_path;\n+}\n+\n@@ -669,1 +721,0 @@\n-\n@@ -676,0 +727,15 @@\n+      log_info(class, load)(\"path: %s\", path);\n+\n+      \/\/ If we are in preview mode, attempt to add a preview entry *before* the\n+      \/\/ new class path entry if a preview path exists.\n+      if (is_preview_enabled()) {\n+        const char* preview_path = get_preview_path(path);\n+        if (os::stat(preview_path, &st) == 0) {\n+          ClassPathEntry* preview_entry = create_class_path_entry(current, preview_path, &st);\n+          if (preview_entry != nullptr) {\n+            module_cpl->add_to_list(preview_entry);\n+            log_info(class, load)(\"preview path: %s\", preview_path);\n+          }\n+        }\n+      }\n+\n@@ -681,1 +747,0 @@\n-      log_info(class, load)(\"path: %s\", path);\n@@ -1046,0 +1111,1 @@\n+  bool is_patched = false;\n@@ -1065,2 +1131,16 @@\n-    assert(!CDSConfig::is_dumping_archive(), \"CDS doesn't support --patch-module during dumping\");\n-    stream = search_module_entries(THREAD, _patch_mod_entries, pkg_entry, file_name);\n+    \/\/ At CDS dump time, the --patch-module entries are ignored. That means a\n+    \/\/ class is still loaded from the runtime image even if it might\n+    \/\/ appear in the _patch_mod_entries. The runtime shared class visibility\n+    \/\/ check will determine if a shared class is visible based on the runtime\n+    \/\/ environment, including the runtime --patch-module setting.\n+    if (!Arguments::is_valhalla_enabled()) {\n+      \/\/ Dynamic dumping requires UseSharedSpaces to be enabled. Since --patch-module\n+      \/\/ is not supported with UseSharedSpaces, we can never come here during dynamic dumping.\n+      assert(!CDSConfig::is_dumping_archive(), \"CDS doesn't support --patch-module during dumping\");\n+    }\n+    if (Arguments::is_valhalla_enabled() || !CDSConfig::is_dumping_static_archive()) {\n+      stream = search_module_entries(THREAD, _patch_mod_entries, pkg_entry, file_name);\n+      if (stream != nullptr) {\n+        is_patched = true;\n+      }\n+    }\n@@ -1115,0 +1195,3 @@\n+  if (is_patched) {\n+    result->set_shared_classpath_index(0);\n+  }\n@@ -1191,0 +1274,4 @@\n+  if (ik->shared_classpath_index() == 0 && ik->defined_by_boot_loader()) {\n+    return;\n+  }\n+\n@@ -1398,11 +1485,0 @@\n-static char* lookup_vm_resource(JImageFile *jimage, const char *jimage_version, const char *path) {\n-  jlong size;\n-  JImageLocationRef location = (*JImageFindResource)(jimage, \"java.base\", jimage_version, path, &size);\n-  if (location == 0)\n-    return nullptr;\n-  char *val = NEW_C_HEAP_ARRAY(char, size+1, mtClass);\n-  (*JImageGetResource)(jimage, location, val, size);\n-  val[size] = '\\0';\n-  return val;\n-}\n-\n@@ -1411,1 +1487,0 @@\n-  jint error;\n@@ -1419,3 +1494,13 @@\n-  JImage_file =(*JImageOpen)(modules_path, &error);\n-  if (JImage_file == nullptr) {\n-    return nullptr;\n+  if (jimage_open(modules_path)) {\n+    \/\/ Special case where we lookup the options string *before* set_preview_mode() is called.\n+    \/\/ Since VM arguments have not been parsed, and the ClassPathImageEntry singleton\n+    \/\/ has not been created yet, we access the JImage file directly in non-preview mode.\n+    jlong size;\n+    JImageLocationRef location =\n+            jimage_find_resource(JAVA_BASE_NAME, \"jdk\/internal\/vm\/options\", \/* is_preview *\/ false, &size);\n+    if (location != 0) {\n+      char *options = NEW_C_HEAP_ARRAY(char, size+1, mtClass);\n+      (*JImageGetResource)(jimage_non_null(), location, options, size);\n+      options[size] = '\\0';\n+      return options;\n+    }\n@@ -1423,0 +1508,2 @@\n+  return nullptr;\n+}\n@@ -1424,3 +1511,4 @@\n-  const char *jimage_version = get_jimage_version_string();\n-  char *options = lookup_vm_resource(JImage_file, jimage_version, \"jdk\/internal\/vm\/options\");\n-  return options;\n+\/\/ Finishes initializing the JImageFile (if present) by setting the access mode.\n+void ClassLoader::set_preview_mode(bool enable_preview) {\n+  assert(Preview_mode == PREVIEW_MODE_UNINITIALIZED, \"set_preview_mode must not be called twice\");\n+  Preview_mode = enable_preview ? PREVIEW_MODE_ENABLE_PREVIEW : PREVIEW_MODE_DEFAULT;\n@@ -1431,1 +1519,1 @@\n-  if (JImage_file == nullptr) {\n+  if (!jimage_exists()) {\n@@ -1438,0 +1526,1 @@\n+  \/\/ We don't expect preview mode (i.e. --enable-preview) to affect module visibility.\n@@ -1439,2 +1528,1 @@\n-  const char *jimage_version = get_jimage_version_string();\n-  return (*JImageFindResource)(JImage_file, module_name, jimage_version, \"module-info.class\", &size) != 0;\n+  return jimage_find_resource(module_name, \"module-info.class\", \/* is_preview *\/ false, &size) != 0;\n","filename":"src\/hotspot\/share\/classfile\/classLoader.cpp","additions":156,"deletions":68,"binary":false,"changes":224,"status":"modified"},{"patch":"@@ -55,0 +55,2 @@\n+#include \"oops\/flatArrayKlass.hpp\"\n+#include \"oops\/inlineKlass.inline.hpp\"\n@@ -56,1 +58,1 @@\n-#include \"oops\/instanceMirrorKlass.hpp\"\n+#include \"oops\/instanceMirrorKlass.inline.hpp\"\n@@ -871,0 +873,1 @@\n+int java_lang_Class::_is_identity_offset;\n@@ -1094,0 +1097,1 @@\n+  set_is_identity(mirror(), k->is_array_klass() || k->is_identity_class());\n@@ -1104,1 +1108,9 @@\n-    if (k->is_typeArray_klass()) {\n+    if (k->is_flatArray_klass()) {\n+      Klass* element_klass = (Klass*) FlatArrayKlass::cast(k)->element_klass();\n+      assert(element_klass->is_inline_klass(), \"Must be inline type component\");\n+      if (is_scratch) {\n+        comp_mirror = Handle(THREAD, HeapShared::scratch_java_mirror(element_klass));\n+      } else {\n+        comp_mirror = Handle(THREAD, element_klass->java_mirror());\n+      }\n+    } else if (k->is_typeArray_klass()) {\n@@ -1113,0 +1125,1 @@\n+      assert(!k->is_refArray_klass() || !k->is_flatArray_klass(), \"Must not have mirror\");\n@@ -1115,0 +1128,1 @@\n+      oop comp_oop = element_klass->java_mirror();\n@@ -1118,1 +1132,1 @@\n-        comp_mirror = Handle(THREAD, element_klass->java_mirror());\n+        comp_mirror = Handle(THREAD, comp_oop);\n@@ -1149,1 +1163,0 @@\n-\n@@ -1153,0 +1166,9 @@\n+\n+    if (k->is_refined_objArray_klass()) {\n+      Klass* super_klass = k->super();\n+      assert(super_klass != nullptr, \"Must be\");\n+      Handle mirror(THREAD, super_klass->java_mirror());\n+      k->set_java_mirror(mirror);\n+      return;\n+    }\n+\n@@ -1175,0 +1197,1 @@\n+\n@@ -1198,3 +1221,3 @@\n-  if (k->class_loader() != nullptr &&\n-      k->class_loader() != SystemDictionary::java_platform_loader() &&\n-      k->class_loader() != SystemDictionary::java_system_loader()) {\n+  if ((k->class_loader() != nullptr &&\n+       k->class_loader() != SystemDictionary::java_platform_loader() &&\n+       k->class_loader() != SystemDictionary::java_system_loader())) {\n@@ -1239,1 +1262,0 @@\n-  assert(as_Klass(m) == k, \"must be\");\n@@ -1243,0 +1265,1 @@\n+    assert(as_Klass(m) == k, \"must be\");\n@@ -1249,0 +1272,4 @@\n+  } else {\n+    ObjArrayKlass* objarray_k = (ObjArrayKlass*)as_Klass(m);\n+    \/\/ Mirror should be restored for an ObjArrayKlass or one of its refined array klasses\n+    assert(objarray_k == k || objarray_k->next_refined_array_klass() == k, \"must be\");\n@@ -1381,0 +1408,4 @@\n+void java_lang_Class::set_is_identity(oop java_class, bool value) {\n+  assert(_is_identity_offset != 0, \"must be set\");\n+  java_class->bool_field_put(_is_identity_offset, value);\n+}\n@@ -1422,1 +1453,3 @@\n-  if (is_instance)  st->print(\"L\");\n+  if (is_instance)  {\n+    st->print(\"L\");\n+  }\n@@ -1480,0 +1513,1 @@\n+  assert(!klass->is_refined_objArray_klass(), \"should not be ref or flat array klass\");\n@@ -1539,1 +1573,2 @@\n-  macro(_is_primitive_offset,        k, \"primitive\",           bool_signature,         false);\n+  macro(_is_primitive_offset,        k, \"primitive\",           bool_signature,         false); \\\n+  macro(_is_identity_offset,         k, \"identity\",            bool_signature,         false);\n@@ -2859,1 +2894,1 @@\n-      if (method->name() == vmSymbols::object_initializer_name() &&\n+      if (method->is_object_constructor() &&\n@@ -3218,2 +3253,2 @@\n-  if (m->is_object_initializer()) {\n-    flags |= java_lang_invoke_MemberName::MN_IS_CONSTRUCTOR;\n+  if (m->is_object_constructor()) {\n+    flags |= java_lang_invoke_MemberName::MN_IS_OBJECT_CONSTRUCTOR;\n@@ -3610,1 +3645,1 @@\n-int java_lang_reflect_Field::_trusted_final_offset;\n+int java_lang_reflect_Field::_flags_offset;\n@@ -3620,1 +3655,1 @@\n-  macro(_trusted_final_offset,    k, vmSymbols::trusted_final_name(),    bool_signature,       false); \\\n+  macro(_flags_offset,     k, vmSymbols::flags_name(),     int_signature,    false); \\\n@@ -3685,2 +3720,2 @@\n-void java_lang_reflect_Field::set_trusted_final(oop field) {\n-  field->bool_field_put(_trusted_final_offset, true);\n+void java_lang_reflect_Field::set_flags(oop field, int value) {\n+  field->int_field_put(_flags_offset, value);\n@@ -3986,2 +4021,1 @@\n-int java_lang_boxing_object::_value_offset;\n-int java_lang_boxing_object::_long_value_offset;\n+int* java_lang_boxing_object::_offsets;\n@@ -3989,3 +4023,9 @@\n-#define BOXING_FIELDS_DO(macro) \\\n-  macro(_value_offset,      integerKlass, \"value\", int_signature, false); \\\n-  macro(_long_value_offset, longKlass, \"value\", long_signature, false);\n+#define BOXING_FIELDS_DO(macro)                                                                                                    \\\n+  macro(java_lang_boxing_object::_offsets[T_BOOLEAN - T_BOOLEAN], vmClasses::Boolean_klass(),   \"value\", bool_signature,   false); \\\n+  macro(java_lang_boxing_object::_offsets[T_CHAR - T_BOOLEAN],    vmClasses::Character_klass(), \"value\", char_signature,   false); \\\n+  macro(java_lang_boxing_object::_offsets[T_FLOAT - T_BOOLEAN],   vmClasses::Float_klass(),     \"value\", float_signature,  false); \\\n+  macro(java_lang_boxing_object::_offsets[T_DOUBLE - T_BOOLEAN],  vmClasses::Double_klass(),    \"value\", double_signature, false); \\\n+  macro(java_lang_boxing_object::_offsets[T_BYTE - T_BOOLEAN],    vmClasses::Byte_klass(),      \"value\", byte_signature,   false); \\\n+  macro(java_lang_boxing_object::_offsets[T_SHORT - T_BOOLEAN],   vmClasses::Short_klass(),     \"value\", short_signature,  false); \\\n+  macro(java_lang_boxing_object::_offsets[T_INT - T_BOOLEAN],     vmClasses::Integer_klass(),   \"value\", int_signature,    false); \\\n+  macro(java_lang_boxing_object::_offsets[T_LONG - T_BOOLEAN],    vmClasses::Long_klass(),      \"value\", long_signature,   false);\n@@ -3994,2 +4034,2 @@\n-  InstanceKlass* integerKlass = vmClasses::Integer_klass();\n-  InstanceKlass* longKlass = vmClasses::Long_klass();\n+  assert(T_LONG - T_BOOLEAN == 7, \"Sanity check\");\n+  java_lang_boxing_object::_offsets = NEW_C_HEAP_ARRAY(int, 8, mtInternal);\n@@ -4001,0 +4041,4 @@\n+  if (f->reading()) {\n+    assert(T_LONG - T_BOOLEAN == 7, \"Sanity check\");\n+    java_lang_boxing_object::_offsets = NEW_C_HEAP_ARRAY(int, 8, mtInternal);\n+  }\n@@ -4021,1 +4065,1 @@\n-      box->bool_field_put(_value_offset, value->z);\n+      box->bool_field_put(value_offset(type), value->z);\n@@ -4024,1 +4068,1 @@\n-      box->char_field_put(_value_offset, value->c);\n+      box->char_field_put(value_offset(type), value->c);\n@@ -4027,1 +4071,1 @@\n-      box->float_field_put(_value_offset, value->f);\n+      box->float_field_put(value_offset(type), value->f);\n@@ -4030,1 +4074,1 @@\n-      box->double_field_put(_long_value_offset, value->d);\n+      box->double_field_put(value_offset(type), value->d);\n@@ -4033,1 +4077,1 @@\n-      box->byte_field_put(_value_offset, value->b);\n+      box->byte_field_put(value_offset(type), value->b);\n@@ -4036,1 +4080,1 @@\n-      box->short_field_put(_value_offset, value->s);\n+      box->short_field_put(value_offset(type), value->s);\n@@ -4039,1 +4083,1 @@\n-      box->int_field_put(_value_offset, value->i);\n+      box->int_field_put(value_offset(type), value->i);\n@@ -4042,1 +4086,1 @@\n-      box->long_field_put(_long_value_offset, value->j);\n+      box->long_field_put(value_offset(type), value->j);\n@@ -4064,1 +4108,1 @@\n-    value->z = box->bool_field(_value_offset);\n+    value->z = box->bool_field(value_offset(type));\n@@ -4067,1 +4111,1 @@\n-    value->c = box->char_field(_value_offset);\n+    value->c = box->char_field(value_offset(type));\n@@ -4070,1 +4114,1 @@\n-    value->f = box->float_field(_value_offset);\n+    value->f = box->float_field(value_offset(type));\n@@ -4073,1 +4117,1 @@\n-    value->d = box->double_field(_long_value_offset);\n+    value->d = box->double_field(value_offset(type));\n@@ -4076,1 +4120,1 @@\n-    value->b = box->byte_field(_value_offset);\n+    value->b = box->byte_field(value_offset(type));\n@@ -4079,1 +4123,1 @@\n-    value->s = box->short_field(_value_offset);\n+    value->s = box->short_field(value_offset(type));\n@@ -4082,1 +4126,1 @@\n-    value->i = box->int_field(_value_offset);\n+    value->i = box->int_field(value_offset(type));\n@@ -4085,1 +4129,1 @@\n-    value->j = box->long_field(_long_value_offset);\n+    value->j = box->long_field(value_offset(type));\n@@ -4098,1 +4142,1 @@\n-    box->bool_field_put(_value_offset, value->z);\n+    box->bool_field_put(value_offset(type), value->z);\n@@ -4101,1 +4145,1 @@\n-    box->char_field_put(_value_offset, value->c);\n+    box->char_field_put(value_offset(type), value->c);\n@@ -4104,1 +4148,1 @@\n-    box->float_field_put(_value_offset, value->f);\n+    box->float_field_put(value_offset(type), value->f);\n@@ -4107,1 +4151,1 @@\n-    box->double_field_put(_long_value_offset, value->d);\n+    box->double_field_put(value_offset(type), value->d);\n@@ -4110,1 +4154,1 @@\n-    box->byte_field_put(_value_offset, value->b);\n+    box->byte_field_put(value_offset(type), value->b);\n@@ -4113,1 +4157,1 @@\n-    box->short_field_put(_value_offset, value->s);\n+    box->short_field_put(value_offset(type), value->s);\n@@ -4116,1 +4160,1 @@\n-    box->int_field_put(_value_offset, value->i);\n+    box->int_field_put(value_offset(type), value->i);\n@@ -4119,1 +4163,1 @@\n-    box->long_field_put(_long_value_offset, value->j);\n+    box->long_field_put(value_offset(type), value->j);\n@@ -4507,1 +4551,0 @@\n-\n@@ -4517,1 +4560,1 @@\n-  return (flags(mname) & (MN_IS_METHOD | MN_IS_CONSTRUCTOR)) > 0;\n+  return (flags(mname) & (MN_IS_METHOD | MN_IS_OBJECT_CONSTRUCTOR)) > 0;\n@@ -5510,16 +5553,11 @@\n-#define CHECK_OFFSET(klass_name, cpp_klass_name, field_name, field_sig) \\\n-  valid &= check_offset(klass_name, cpp_klass_name :: _##field_name ## _offset, #field_name, field_sig)\n-\n-#define CHECK_LONG_OFFSET(klass_name, cpp_klass_name, field_name, field_sig) \\\n-  valid &= check_offset(klass_name, cpp_klass_name :: _##long_ ## field_name ## _offset, #field_name, field_sig)\n-\n-  \/\/ Boxed primitive objects (java_lang_boxing_object)\n-\n-  CHECK_OFFSET(\"java\/lang\/Boolean\",   java_lang_boxing_object, value, \"Z\");\n-  CHECK_OFFSET(\"java\/lang\/Character\", java_lang_boxing_object, value, \"C\");\n-  CHECK_OFFSET(\"java\/lang\/Float\",     java_lang_boxing_object, value, \"F\");\n-  CHECK_LONG_OFFSET(\"java\/lang\/Double\", java_lang_boxing_object, value, \"D\");\n-  CHECK_OFFSET(\"java\/lang\/Byte\",      java_lang_boxing_object, value, \"B\");\n-  CHECK_OFFSET(\"java\/lang\/Short\",     java_lang_boxing_object, value, \"S\");\n-  CHECK_OFFSET(\"java\/lang\/Integer\",   java_lang_boxing_object, value, \"I\");\n-  CHECK_LONG_OFFSET(\"java\/lang\/Long\", java_lang_boxing_object, value, \"J\");\n+#define CHECK_OFFSET(klass_name, type, field_sig) \\\n+  valid &= check_offset(klass_name, java_lang_boxing_object::value_offset(type), \"value\", field_sig)\n+\n+  CHECK_OFFSET(\"java\/lang\/Boolean\",   T_BOOLEAN, \"Z\");\n+  CHECK_OFFSET(\"java\/lang\/Character\", T_CHAR,    \"C\");\n+  CHECK_OFFSET(\"java\/lang\/Float\",     T_FLOAT,   \"F\");\n+  CHECK_OFFSET(\"java\/lang\/Double\",    T_DOUBLE,  \"D\");\n+  CHECK_OFFSET(\"java\/lang\/Byte\",      T_BYTE,    \"B\");\n+  CHECK_OFFSET(\"java\/lang\/Short\",     T_SHORT,   \"S\");\n+  CHECK_OFFSET(\"java\/lang\/Integer\",   T_INT,     \"I\");\n+  CHECK_OFFSET(\"java\/lang\/Long\",      T_LONG,    \"J\");\n","filename":"src\/hotspot\/share\/classfile\/javaClasses.cpp","additions":104,"deletions":66,"binary":false,"changes":170,"status":"modified"},{"patch":"@@ -216,0 +216,1 @@\n+  static inline bool is_instance_without_asserts(oop obj);\n@@ -255,0 +256,1 @@\n+\n@@ -262,0 +264,1 @@\n+  static int _is_identity_offset;\n@@ -276,0 +279,1 @@\n+  static void set_is_identity(oop java_class, bool value);\n@@ -323,0 +327,1 @@\n+\n@@ -837,1 +842,1 @@\n-  static int _trusted_final_offset;\n+  static int _flags_offset;\n@@ -865,1 +870,1 @@\n-  static void set_trusted_final(oop field);\n+  static void set_flags(oop field, int value);\n@@ -987,2 +992,1 @@\n-  static int _value_offset;\n-  static int _long_value_offset;\n+  static int* _offsets;\n@@ -1005,1 +1009,3 @@\n-    return is_double_word_type(type) ? _long_value_offset : _value_offset;\n+    assert(type >= T_BOOLEAN && type <= T_LONG, \"BasicType out of range\");\n+    assert(_offsets != nullptr, \"Uninitialized offsets\");\n+    return _offsets[type - T_BOOLEAN];\n@@ -1353,1 +1359,1 @@\n-    MN_IS_CONSTRUCTOR        = 0x00020000, \/\/ constructor\n+    MN_IS_OBJECT_CONSTRUCTOR = 0x00020000, \/\/ constructor\n@@ -1359,0 +1365,1 @@\n+    MN_NULL_RESTRICTED_FIELD = 0x00800000, \/\/ null-restricted field\n@@ -1360,1 +1367,3 @@\n-    MN_REFERENCE_KIND_MASK   = 0x0F000000 >> MN_REFERENCE_KIND_SHIFT,\n+    MN_REFERENCE_KIND_MASK   = 0x0F000000 >> MN_REFERENCE_KIND_SHIFT, \/\/ 4 bits\n+    MN_LAYOUT_SHIFT          = 28, \/\/ field layout\n+    MN_LAYOUT_MASK           = 0x70000000 >> MN_LAYOUT_SHIFT, \/\/ 3 bits\n@@ -1864,1 +1873,0 @@\n-\n","filename":"src\/hotspot\/share\/classfile\/javaClasses.hpp","additions":16,"deletions":8,"binary":false,"changes":24,"status":"modified"},{"patch":"@@ -328,0 +328,14 @@\n+  do_intrinsic(_newNullRestrictedAtomicArray, jdk_internal_value_ValueClass, newNullRestrictedAtomicArray_name, newArray_signature3, F_SN) \\\n+   do_name(     newNullRestrictedAtomicArray_name,                \"newNullRestrictedAtomicArray\")                       \\\n+  do_intrinsic(_newNullRestrictedNonAtomicArray, jdk_internal_value_ValueClass, newNullRestrictedNonAtomicArray_name, newArray_signature3, F_SN) \\\n+   do_name(     newNullRestrictedNonAtomicArray_name,             \"newNullRestrictedNonAtomicArray\")                    \\\n+  do_intrinsic(_newNullableAtomicArray, jdk_internal_value_ValueClass, newNullableAtomicArray_name, newArray_signature2, F_SN) \\\n+   do_name(     newNullableAtomicArray_name,                      \"newNullableAtomicArray\")                             \\\n+   do_signature(newArray_signature2,                              \"(Ljava\/lang\/Class;I)[Ljava\/lang\/Object;\")            \\\n+   do_signature(newArray_signature3,                              \"(Ljava\/lang\/Class;ILjava\/lang\/Object;)[Ljava\/lang\/Object;\") \\\n+  do_intrinsic(_isFlatArray, jdk_internal_value_ValueClass, isFlatArray_name, object_boolean_signature, F_SN)           \\\n+   do_name(     isFlatArray_name,                                 \"isFlatArray\")                                        \\\n+  do_intrinsic(_isNullRestrictedArray, jdk_internal_value_ValueClass, isNullRestrictedArray_name, object_boolean_signature, F_SN) \\\n+   do_name(     isNullRestrictedArray_name,                       \"isNullRestrictedArray\")                              \\\n+  do_intrinsic(_isAtomicArray, jdk_internal_value_ValueClass, isAtomicArray_name, object_boolean_signature, F_SN)       \\\n+   do_name(     isAtomicArray_name,                               \"isAtomicArray\")                                      \\\n@@ -695,0 +709,10 @@\n+  do_intrinsic(_arrayInstanceBaseOffset,  jdk_internal_misc_Unsafe,     arrayInstanceBaseOffset_name, arrayProperties_signature, F_RN) \\\n+   do_name(     arrayInstanceBaseOffset_name,                           \"arrayInstanceBaseOffset0\")                              \\\n+   do_signature(arrayProperties_signature,                              \"([Ljava\/lang\/Object;)I\")                                \\\n+  do_intrinsic(_arrayInstanceIndexScale,  jdk_internal_misc_Unsafe,     arrayInstanceIndexScale_name, arrayProperties_signature, F_RN) \\\n+   do_name(     arrayInstanceIndexScale_name,                           \"arrayInstanceIndexScale0\")                              \\\n+  do_intrinsic(_arrayLayout,              jdk_internal_misc_Unsafe,     arrayLayout_name, arrayProperties_signature, F_RN)       \\\n+   do_name(     arrayLayout_name,                                       \"arrayLayout0\")                                          \\\n+  do_intrinsic(_getFieldMap,              jdk_internal_misc_Unsafe,     getFieldMap_name, getFieldMap_signature, F_RN)           \\\n+   do_name(     getFieldMap_name,                                       \"getFieldMap0\")                                          \\\n+   do_signature(getFieldMap_signature,                                  \"(Ljava\/lang\/Class;)[I\")                                 \\\n@@ -731,0 +755,2 @@\n+  do_signature(getFlatValue_signature,    \"(Ljava\/lang\/Object;JILjava\/lang\/Class;)Ljava\/lang\/Object;\")                  \\\n+  do_signature(putFlatValue_signature,    \"(Ljava\/lang\/Object;JILjava\/lang\/Class;Ljava\/lang\/Object;)V\")                 \\\n@@ -741,0 +767,4 @@\n+  do_name(getValue_name,\"getValue\")             do_name(putValue_name,\"putValue\")                                       \\\n+  do_name(getFlatValue_name,\"getFlatValue\")     do_name(putFlatValue_name,\"putFlatValue\")                               \\\n+  do_name(makePrivateBuffer_name,\"makePrivateBuffer\")                                                                   \\\n+  do_name(finishPrivateBuffer_name,\"finishPrivateBuffer\")                                                               \\\n@@ -751,0 +781,1 @@\n+  do_intrinsic(_getFlatValue,       jdk_internal_misc_Unsafe,     getFlatValue_name, getFlatValue_signature,     F_RN)  \\\n@@ -760,0 +791,4 @@\n+  do_intrinsic(_putFlatValue,       jdk_internal_misc_Unsafe,     putFlatValue_name, putFlatValue_signature,     F_RN)  \\\n+                                                                                                                        \\\n+  do_intrinsic(_makePrivateBuffer,  jdk_internal_misc_Unsafe,     makePrivateBuffer_name, object_object_signature, F_RN)   \\\n+  do_intrinsic(_finishPrivateBuffer,  jdk_internal_misc_Unsafe,   finishPrivateBuffer_name, object_object_signature, F_RN) \\\n","filename":"src\/hotspot\/share\/classfile\/vmIntrinsics.hpp","additions":35,"deletions":0,"binary":false,"changes":35,"status":"modified"},{"patch":"@@ -1279,0 +1279,1 @@\n+  SET_ADDRESS(_extrs, SharedRuntime::allocate_inline_types);\n@@ -1328,0 +1329,8 @@\n+    SET_ADDRESS(_extrs, Runtime1::new_null_free_array);\n+    SET_ADDRESS(_extrs, Runtime1::load_flat_array);\n+    SET_ADDRESS(_extrs, Runtime1::store_flat_array);\n+    SET_ADDRESS(_extrs, Runtime1::substitutability_check);\n+    SET_ADDRESS(_extrs, Runtime1::buffer_inline_args);\n+    SET_ADDRESS(_extrs, Runtime1::buffer_inline_args_no_receiver);\n+    SET_ADDRESS(_extrs, Runtime1::throw_identity_exception);\n+    SET_ADDRESS(_extrs, Runtime1::throw_illegal_monitor_state_exception);\n@@ -1355,0 +1364,2 @@\n+    SET_ADDRESS(_extrs, OptoRuntime::load_unknown_inline_C);\n+    SET_ADDRESS(_extrs, OptoRuntime::store_unknown_inline_C);\n@@ -1386,0 +1397,4 @@\n+  if (UseCompressedOops) {\n+    SET_ADDRESS(_extrs, CompressedOops::base_addr());\n+  }\n+\n","filename":"src\/hotspot\/share\/code\/aotCodeCache.cpp","additions":15,"deletions":0,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -869,0 +869,6 @@\n+  if (tp->isa_aryptr()) {\n+    \/\/ In the case of a flat inline type array, each field has its\n+    \/\/ own slice so we need to extract the field being accessed from\n+    \/\/ the address computation\n+    return tp->is_aryptr()->add_field_offset_and_offset(txoffset);\n+  }\n@@ -889,0 +895,6 @@\n+  if (p1->isa_aryptr()) {\n+    \/\/ In the case of a flat inline type array, each field has its\n+    \/\/ own slice so we need to extract the field being accessed from\n+    \/\/ the address computation\n+    return p1->is_aryptr()->add_field_offset_and_offset(p2offset);\n+  }\n","filename":"src\/hotspot\/share\/opto\/addnode.cpp","additions":12,"deletions":0,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -847,0 +847,6 @@\n+  product(bool, UseArrayLoadStoreProfile, true,                             \\\n+          \"Take advantage of profiling at array load\/store\")                \\\n+                                                                            \\\n+  product(bool, UseACmpProfile, true,                                       \\\n+          \"Take advantage of profiling at acmp\")                            \\\n+                                                                            \\\n","filename":"src\/hotspot\/share\/opto\/c2_globals.hpp","additions":6,"deletions":0,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -32,0 +32,1 @@\n+#include \"oops\/accessDecorators.hpp\"\n@@ -37,0 +38,1 @@\n+#include \"opto\/inlinetypenode.hpp\"\n@@ -121,1 +123,1 @@\n-  \/\/ paths to facilitate late inlinig.\n+  \/\/ paths to facilitate late inlining.\n@@ -130,0 +132,1 @@\n+      _call_node(nullptr),\n@@ -132,0 +135,8 @@\n+    if (InlineTypeReturnedAsFields && method->is_method_handle_intrinsic()) {\n+      \/\/ If that call has not been optimized by the time optimizations are over,\n+      \/\/ we'll need to add a call to create an inline type instance from the klass\n+      \/\/ returned by the call (see PhaseMacroExpand::expand_mh_intrinsic_return).\n+      \/\/ Separating memory and I\/O projections for exceptions is required to\n+      \/\/ perform that graph transformation.\n+      _separate_io_proj = true;\n+    }\n@@ -145,0 +156,1 @@\n+  PhaseGVN& gvn = kit.gvn();\n@@ -173,1 +185,4 @@\n-  kit.set_arguments_for_java_call(call);\n+  kit.set_arguments_for_java_call(call, is_late_inline());\n+  if (kit.stopped()) {\n+    return kit.transfer_exceptions_into_jvms();\n+  }\n@@ -214,1 +229,0 @@\n-\n@@ -272,0 +286,3 @@\n+  if (kit.stopped()) {\n+    return kit.transfer_exceptions_into_jvms();\n+  }\n@@ -356,0 +373,4 @@\n+  virtual CallGenerator* inline_cg() {\n+    return _inline_cg;\n+  }\n+\n@@ -417,0 +438,8 @@\n+    \/\/ AlwaysIncrementalInline causes for_method_handle_inline() to\n+    \/\/ return a LateInlineCallGenerator. Extract the\n+    \/\/ InlineCallGenerator from it.\n+    if (AlwaysIncrementalInline && cg->is_late_inline() && !cg->is_virtual_late_inline()) {\n+      cg = cg->inline_cg();\n+      assert(cg != nullptr, \"inline call generator expected\");\n+    }\n+\n@@ -577,3 +606,3 @@\n-  const TypeTuple *r = call->tf()->domain();\n-  for (int i1 = 0; i1 < method()->arg_size(); i1++) {\n-    if (call->in(TypeFunc::Parms + i1)->is_top() && r->field_at(TypeFunc::Parms + i1) != Type::HALF) {\n+  const TypeTuple* r = call->tf()->domain_cc();\n+  for (uint i1 = TypeFunc::Parms; i1 < r->cnt(); i1++) {\n+    if (call->in(i1)->is_top() && r->field_at(i1) != Type::HALF) {\n@@ -597,1 +626,0 @@\n-  CallProjections callprojs;\n@@ -601,9 +629,8 @@\n-  call->extract_projections(&callprojs, true, do_asserts);\n-  if ((callprojs.fallthrough_catchproj == call->in(0)) ||\n-      (callprojs.catchall_catchproj    == call->in(0)) ||\n-      (callprojs.fallthrough_memproj   == call->in(TypeFunc::Memory)) ||\n-      (callprojs.catchall_memproj      == call->in(TypeFunc::Memory)) ||\n-      (callprojs.fallthrough_ioproj    == call->in(TypeFunc::I_O)) ||\n-      (callprojs.catchall_ioproj       == call->in(TypeFunc::I_O)) ||\n-      (callprojs.resproj != nullptr && call->find_edge(callprojs.resproj) != -1) ||\n-      (callprojs.exobj   != nullptr && call->find_edge(callprojs.exobj) != -1)) {\n+  CallProjections* callprojs = call->extract_projections(true, do_asserts);\n+  if ((callprojs->fallthrough_catchproj == call->in(0)) ||\n+      (callprojs->catchall_catchproj    == call->in(0)) ||\n+      (callprojs->fallthrough_memproj   == call->in(TypeFunc::Memory)) ||\n+      (callprojs->catchall_memproj      == call->in(TypeFunc::Memory)) ||\n+      (callprojs->fallthrough_ioproj    == call->in(TypeFunc::I_O)) ||\n+      (callprojs->catchall_ioproj       == call->in(TypeFunc::I_O)) ||\n+      (callprojs->exobj != nullptr && call->find_edge(callprojs->exobj) != -1)) {\n@@ -619,3 +646,12 @@\n-  \/\/ The call is marked as pure (no important side effects), but result isn't used.\n-  \/\/ It's safe to remove the call.\n-  bool result_not_used = (callprojs.resproj == nullptr || callprojs.resproj->outcnt() == 0);\n+\n+  bool result_not_used = true;\n+  for (uint i = 0; i < callprojs->nb_resproj; i++) {\n+    if (callprojs->resproj[i] != nullptr) {\n+      if (callprojs->resproj[i]->outcnt() != 0) {\n+        result_not_used = false;\n+      }\n+      if (call->find_edge(callprojs->resproj[i]) != -1) {\n+        return;\n+      }\n+    }\n+  }\n@@ -624,0 +660,2 @@\n+    \/\/ The call is marked as pure (no important side effects), but result isn't used.\n+    \/\/ It's safe to remove the call.\n@@ -636,0 +674,1 @@\n+    PhaseGVN& gvn = *C->initial_gvn();\n@@ -639,1 +678,1 @@\n-      C->initial_gvn()->set_type_bottom(mem);\n+      gvn.set_type_bottom(mem);\n@@ -643,4 +682,2 @@\n-    uint nargs = method()->arg_size();\n-    Node* top = C->top();\n-    for (uint i1 = 0; i1 < nargs; i1++) {\n-      map->set_req(TypeFunc::Parms + i1, top);\n+    for (uint i1 = TypeFunc::Parms; i1 < r->cnt(); i1++) {\n+      map->set_req(i1, C->top());\n@@ -654,0 +691,6 @@\n+    const TypeTuple* domain_sig = call->_tf->domain_sig();\n+    uint nargs = method()->arg_size();\n+    assert(domain_sig->cnt() - TypeFunc::Parms == nargs, \"inconsistent signature\");\n+\n+    uint j = TypeFunc::Parms;\n+    int arg_num = 0;\n@@ -655,1 +698,14 @@\n-      map->set_argument(jvms, i1, call->in(TypeFunc::Parms + i1));\n+      const Type* t = domain_sig->field_at(TypeFunc::Parms + i1);\n+      if (t->is_inlinetypeptr() && !method()->get_Method()->mismatch() && method()->is_scalarized_arg(arg_num)) {\n+        \/\/ Inline type arguments are not passed by reference: we get an argument per\n+        \/\/ field of the inline type. Build InlineTypeNodes from the inline type arguments.\n+        GraphKit arg_kit(jvms, &gvn);\n+        Node* vt = InlineTypeNode::make_from_multi(&arg_kit, call, t->inline_klass(), j, \/* in= *\/ true, \/* null_free= *\/ !t->maybe_null());\n+        map->set_control(arg_kit.control());\n+        map->set_argument(jvms, i1, vt);\n+      } else {\n+        map->set_argument(jvms, i1, call->in(j++));\n+      }\n+      if (t != Type::HALF) {\n+        arg_num++;\n+      }\n@@ -666,0 +722,20 @@\n+    \/\/ Check if we are late inlining a method handle call that returns an inline type as fields.\n+    Node* buffer_oop = nullptr;\n+    ciMethod* inline_method = inline_cg()->method();\n+    ciType* return_type = inline_method->return_type();\n+    if (!call->tf()->returns_inline_type_as_fields() &&\n+        return_type->is_inlinetype() && return_type->as_inline_klass()->can_be_returned_as_fields()) {\n+      \/\/ Allocate a buffer for the inline type returned as fields because the caller expects an oop return.\n+      \/\/ Do this before the method handle call in case the buffer allocation triggers deoptimization and\n+      \/\/ we need to \"re-execute\" the call in the interpreter (to make sure the call is only executed once).\n+      GraphKit arg_kit(jvms, &gvn);\n+      {\n+        PreserveReexecuteState preexecs(&arg_kit);\n+        arg_kit.jvms()->set_should_reexecute(true);\n+        arg_kit.inc_sp(nargs);\n+        Node* klass_node = arg_kit.makecon(TypeKlassPtr::make(return_type->as_inline_klass()));\n+        buffer_oop = arg_kit.new_instance(klass_node, nullptr, nullptr, \/* deoptimize_on_exception *\/ true);\n+      }\n+      jvms = arg_kit.transfer_exceptions_into_jvms();\n+    }\n+\n@@ -706,2 +782,2 @@\n-      C->set_has_loops(C->has_loops() || inline_cg()->method()->has_loops());\n-      C->env()->notice_inlined_method(inline_cg()->method());\n+      C->set_has_loops(C->has_loops() || inline_method->has_loops());\n+      C->env()->notice_inlined_method(inline_method);\n@@ -711,0 +787,54 @@\n+\n+    \/\/ Handle inline type returns\n+    InlineTypeNode* vt = result->isa_InlineType();\n+    if (vt != nullptr) {\n+      if (call->tf()->returns_inline_type_as_fields()) {\n+        vt->replace_call_results(&kit, call, C);\n+      } else {\n+        \/\/ Result might still be allocated (for example, if it has been stored to a non-flat field)\n+        if (!vt->is_allocated(&kit.gvn())) {\n+          assert(buffer_oop != nullptr, \"should have allocated a buffer\");\n+          RegionNode* region = new RegionNode(3);\n+\n+          \/\/ Check if result is null\n+          Node* null_ctl = kit.top();\n+          kit.null_check_common(vt->get_null_marker(), T_INT, false, &null_ctl);\n+          region->init_req(1, null_ctl);\n+          PhiNode* oop = PhiNode::make(region, kit.gvn().zerocon(T_OBJECT), TypeInstPtr::make(TypePtr::BotPTR, vt->type()->inline_klass()));\n+          Node* init_mem = kit.reset_memory();\n+          PhiNode* mem = PhiNode::make(region, init_mem, Type::MEMORY, TypePtr::BOTTOM);\n+\n+          \/\/ Not null, initialize the buffer\n+          kit.set_all_memory(init_mem);\n+\n+          Node* payload_ptr = kit.basic_plus_adr(buffer_oop, kit.gvn().type(vt)->inline_klass()->payload_offset());\n+          vt->store_flat(&kit, buffer_oop, payload_ptr, false, true, true, IN_HEAP | MO_UNORDERED);\n+          \/\/ Do not let stores that initialize this buffer be reordered with a subsequent\n+          \/\/ store that would make this buffer accessible by other threads.\n+          AllocateNode* alloc = AllocateNode::Ideal_allocation(buffer_oop);\n+          assert(alloc != nullptr, \"must have an allocation node\");\n+          kit.insert_mem_bar(Op_MemBarStoreStore, alloc->proj_out_or_null(AllocateNode::RawAddress));\n+          region->init_req(2, kit.control());\n+          oop->init_req(2, buffer_oop);\n+          mem->init_req(2, kit.merged_memory());\n+\n+          \/\/ Update oop input to buffer\n+          kit.gvn().hash_delete(vt);\n+          vt->set_oop(kit.gvn(), kit.gvn().transform(oop));\n+          vt->set_is_buffered(kit.gvn());\n+          vt = kit.gvn().transform(vt)->as_InlineType();\n+\n+          kit.set_control(kit.gvn().transform(region));\n+          kit.set_all_memory(kit.gvn().transform(mem));\n+          kit.record_for_igvn(region);\n+          kit.record_for_igvn(oop);\n+          kit.record_for_igvn(mem);\n+        }\n+        result = vt;\n+      }\n+      DEBUG_ONLY(buffer_oop = nullptr);\n+    } else {\n+      assert(result->is_top() || !call->tf()->returns_inline_type_as_fields() || !call->as_CallJava()->method()->return_type()->is_loaded(), \"Unexpected return value\");\n+    }\n+    assert(buffer_oop == nullptr, \"unused buffer allocation\");\n+\n@@ -939,0 +1069,23 @@\n+  \/\/ Allocate inline types if they are merged with objects (similar to Parse::merge_common())\n+  uint tos = kit.jvms()->stkoff() + kit.sp();\n+  uint limit = slow_map->req();\n+  for (uint i = TypeFunc::Parms; i < limit; i++) {\n+    Node* m = kit.map()->in(i);\n+    Node* n = slow_map->in(i);\n+    const Type* t = gvn.type(m)->meet_speculative(gvn.type(n));\n+    \/\/ TODO 8284443 still needed?\n+    if (m->is_InlineType() && !t->is_inlinetypeptr()) {\n+      \/\/ Allocate inline type in fast path\n+      m = m->as_InlineType()->buffer(&kit);\n+      kit.map()->set_req(i, m);\n+    }\n+    if (n->is_InlineType() && !t->is_inlinetypeptr()) {\n+      \/\/ Allocate inline type in slow path\n+      PreserveJVMState pjvms(&kit);\n+      kit.set_map(slow_map);\n+      n = n->as_InlineType()->buffer(&kit);\n+      kit.map()->set_req(i, n);\n+      slow_map = kit.stop();\n+    }\n+  }\n+\n@@ -962,2 +1115,0 @@\n-  uint tos = kit.jvms()->stkoff() + kit.sp();\n-  uint limit = slow_map->req();\n@@ -1000,2 +1151,2 @@\n-  if (IncrementalInlineMH && call_site_count > 0 &&\n-      (should_delay || input_not_const || !C->inlining_incrementally() || C->over_inlining_cutoff())) {\n+  if (IncrementalInlineMH && (AlwaysIncrementalInline ||\n+                            (call_site_count > 0 && (should_delay || input_not_const || !C->inlining_incrementally() || C->over_inlining_cutoff())))) {\n@@ -1009,0 +1160,1 @@\n+\n@@ -1056,0 +1208,1 @@\n+      int nargs = callee->arg_size();\n@@ -1057,1 +1210,1 @@\n-      Node* member_name = kit.argument(callee->arg_size() - 1);\n+      Node* member_name = kit.argument(nargs - 1);\n@@ -1127,1 +1280,2 @@\n-                                              speculative_receiver_type);\n+                                              speculative_receiver_type,\n+                                              true);\n@@ -1199,1 +1353,1 @@\n-    Node* receiver = kit.null_check_receiver_before_call(method());\n+    kit.null_check_receiver_before_call(method());\n","filename":"src\/hotspot\/share\/opto\/callGenerator.cpp","additions":188,"deletions":34,"binary":false,"changes":222,"status":"modified"},{"patch":"@@ -26,0 +26,2 @@\n+#include \"ci\/ciFlatArrayKlass.hpp\"\n+#include \"ci\/ciSymbols.hpp\"\n@@ -37,0 +39,1 @@\n+#include \"opto\/inlinetypenode.hpp\"\n@@ -40,0 +43,1 @@\n+#include \"opto\/movenode.hpp\"\n@@ -45,0 +49,1 @@\n+#include \"runtime\/arguments.hpp\"\n@@ -46,0 +51,1 @@\n+#include \"runtime\/stubRoutines.hpp\"\n@@ -80,1 +86,1 @@\n-Node *StartNode::match( const ProjNode *proj, const Matcher *match ) {\n+Node *StartNode::match(const ProjNode *proj, const Matcher *match, const RegMask* mask) {\n@@ -104,11 +110,0 @@\n-\/\/------------------------------StartOSRNode----------------------------------\n-\/\/ The method start node for an on stack replacement adapter\n-\n-\/\/------------------------------osr_domain-----------------------------\n-const TypeTuple *StartOSRNode::osr_domain() {\n-  const Type **fields = TypeTuple::fields(2);\n-  fields[TypeFunc::Parms+0] = TypeRawPtr::BOTTOM;  \/\/ address of osr buffer\n-\n-  return TypeTuple::make(TypeFunc::Parms+1, fields);\n-}\n-\n@@ -502,0 +497,2 @@\n+      } else {\n+        assert(false, \"unexpected type %s\", cik->name()->as_utf8());\n@@ -507,0 +504,7 @@\n+        if (iklass != nullptr && iklass->is_inlinetype()) {\n+          Node* null_marker = mcall->in(first_ind++);\n+          if (!null_marker->is_top()) {\n+            st->print(\" [null marker\");\n+            format_helper(regalloc, st, null_marker, \":\", -1, nullptr);\n+          }\n+        }\n@@ -508,1 +512,0 @@\n-        ciField* cifield;\n@@ -511,2 +514,1 @@\n-          cifield = iklass->nonstatic_field_at(0);\n-          cifield->print_name_on(st);\n+          iklass->nonstatic_field_at(0)->print_name_on(st);\n@@ -521,2 +523,1 @@\n-            cifield = iklass->nonstatic_field_at(j);\n-            cifield->print_name_on(st);\n+            iklass->nonstatic_field_at(j)->print_name_on(st);\n@@ -757,1 +758,1 @@\n-const Type *CallNode::bottom_type() const { return tf()->range(); }\n+const Type *CallNode::bottom_type() const { return tf()->range_cc(); }\n@@ -762,1 +763,1 @@\n-  return tf()->range();\n+  return tf()->range_cc();\n@@ -767,0 +768,7 @@\n+  if (_entry_point == StubRoutines::store_inline_type_fields_to_buf()) {\n+    \/\/ The call to that stub is a special case: its inputs are\n+    \/\/ multiple values returned from a call and so it should follow\n+    \/\/ the return convention.\n+    SharedRuntime::java_return_convention(sig_bt, parm_regs, argcnt);\n+    return;\n+  }\n@@ -775,27 +783,26 @@\n-Node *CallNode::match( const ProjNode *proj, const Matcher *match ) {\n-  switch (proj->_con) {\n-  case TypeFunc::Control:\n-  case TypeFunc::I_O:\n-  case TypeFunc::Memory:\n-    return new MachProjNode(this,proj->_con,RegMask::EMPTY,MachProjNode::unmatched_proj);\n-\n-  case TypeFunc::Parms+1:       \/\/ For LONG & DOUBLE returns\n-    assert(tf()->range()->field_at(TypeFunc::Parms+1) == Type::HALF, \"\");\n-    \/\/ 2nd half of doubles and longs\n-    return new MachProjNode(this,proj->_con, RegMask::EMPTY, (uint)OptoReg::Bad);\n-\n-  case TypeFunc::Parms: {       \/\/ Normal returns\n-    uint ideal_reg = tf()->range()->field_at(TypeFunc::Parms)->ideal_reg();\n-    OptoRegPair regs = Opcode() == Op_CallLeafVector\n-      ? match->vector_return_value(ideal_reg)      \/\/ Calls into assembly vector routine\n-      : is_CallRuntime()\n-        ? match->c_return_value(ideal_reg)  \/\/ Calls into C runtime\n-        : match->  return_value(ideal_reg); \/\/ Calls into compiled Java code\n-    RegMask rm = RegMask(regs.first());\n-\n-    if (Opcode() == Op_CallLeafVector) {\n-      \/\/ If the return is in vector, compute appropriate regmask taking into account the whole range\n-      if(ideal_reg >= Op_VecA && ideal_reg <= Op_VecZ) {\n-        if(OptoReg::is_valid(regs.second())) {\n-          for (OptoReg::Name r = regs.first(); r <= regs.second(); r = OptoReg::add(r, 1)) {\n-            rm.insert(r);\n+Node *CallNode::match(const ProjNode *proj, const Matcher *match, const RegMask* mask) {\n+  uint con = proj->_con;\n+  const TypeTuple* range_cc = tf()->range_cc();\n+  if (con >= TypeFunc::Parms) {\n+    if (tf()->returns_inline_type_as_fields()) {\n+      \/\/ The call returns multiple values (inline type fields): we\n+      \/\/ create one projection per returned value.\n+      assert(con <= TypeFunc::Parms+1 || InlineTypeReturnedAsFields, \"only for multi value return\");\n+      uint ideal_reg = range_cc->field_at(con)->ideal_reg();\n+      return new MachProjNode(this, con, mask[con-TypeFunc::Parms], ideal_reg);\n+    } else {\n+      if (con == TypeFunc::Parms) {\n+        uint ideal_reg = range_cc->field_at(TypeFunc::Parms)->ideal_reg();\n+        OptoRegPair regs = Opcode() == Op_CallLeafVector\n+          ? match->vector_return_value(ideal_reg)      \/\/ Calls into assembly vector routine\n+          : match->c_return_value(ideal_reg);\n+        RegMask rm = RegMask(regs.first());\n+\n+        if (Opcode() == Op_CallLeafVector) {\n+          \/\/ If the return is in vector, compute appropriate regmask taking into account the whole range\n+          if(ideal_reg >= Op_VecA && ideal_reg <= Op_VecZ) {\n+            if(OptoReg::is_valid(regs.second())) {\n+              for (OptoReg::Name r = regs.first(); r <= regs.second(); r = OptoReg::add(r, 1)) {\n+                rm.insert(r);\n+              }\n+            }\n@@ -804,0 +811,9 @@\n+\n+        if (OptoReg::is_valid(regs.second())) {\n+          rm.insert(regs.second());\n+        }\n+        return new MachProjNode(this,con,rm,ideal_reg);\n+      } else {\n+        assert(con == TypeFunc::Parms+1, \"only one return value\");\n+        assert(range_cc->field_at(TypeFunc::Parms+1) == Type::HALF, \"\");\n+        return new MachProjNode(this,con, RegMask::EMPTY, (uint)OptoReg::Bad);\n@@ -806,4 +822,0 @@\n-\n-    if( OptoReg::is_valid(regs.second()) )\n-      rm.insert(regs.second());\n-    return new MachProjNode(this,proj->_con,rm,ideal_reg);\n@@ -812,0 +824,6 @@\n+  switch (con) {\n+  case TypeFunc::Control:\n+  case TypeFunc::I_O:\n+  case TypeFunc::Memory:\n+    return new MachProjNode(this,proj->_con,RegMask::EMPTY,MachProjNode::unmatched_proj);\n+\n@@ -832,1 +850,1 @@\n-    const TypeTuple* args = _tf->domain();\n+    const TypeTuple* args = _tf->domain_sig();\n@@ -881,1 +899,1 @@\n-      const TypeTuple* d = tf()->domain();\n+      const TypeTuple* d = tf()->domain_cc();\n@@ -896,2 +914,2 @@\n-bool CallNode::has_non_debug_use(Node *n) {\n-  const TypeTuple * d = tf()->domain();\n+bool CallNode::has_non_debug_use(Node* n) {\n+  const TypeTuple* d = tf()->domain_cc();\n@@ -899,2 +917,1 @@\n-    Node *arg = in(i);\n-    if (arg == n) {\n+    if (in(i) == n) {\n@@ -907,0 +924,11 @@\n+bool CallNode::has_debug_use(Node* n) {\n+  if (jvms() != nullptr) {\n+    for (uint i = jvms()->debug_start(); i < jvms()->debug_end(); i++) {\n+      if (in(i) == n) {\n+        return true;\n+      }\n+    }\n+  }\n+  return false;\n+}\n+\n@@ -938,10 +966,15 @@\n-void CallNode::extract_projections(CallProjections* projs, bool separate_io_proj, bool do_asserts) const {\n-  projs->fallthrough_proj      = nullptr;\n-  projs->fallthrough_catchproj = nullptr;\n-  projs->fallthrough_ioproj    = nullptr;\n-  projs->catchall_ioproj       = nullptr;\n-  projs->catchall_catchproj    = nullptr;\n-  projs->fallthrough_memproj   = nullptr;\n-  projs->catchall_memproj      = nullptr;\n-  projs->resproj               = nullptr;\n-  projs->exobj                 = nullptr;\n+CallProjections* CallNode::extract_projections(bool separate_io_proj, bool do_asserts) const {\n+  uint max_res = TypeFunc::Parms-1;\n+  for (DUIterator_Fast imax, i = fast_outs(imax); i < imax; i++) {\n+    ProjNode *pn = fast_out(i)->as_Proj();\n+    max_res = MAX2(max_res, pn->_con);\n+  }\n+\n+  assert(max_res < _tf->range_cc()->cnt(), \"result out of bounds\");\n+\n+  uint projs_size = sizeof(CallProjections);\n+  if (max_res > TypeFunc::Parms) {\n+    projs_size += (max_res-TypeFunc::Parms)*sizeof(Node*);\n+  }\n+  char* projs_storage = resource_allocate_bytes(projs_size);\n+  CallProjections* projs = new(projs_storage)CallProjections(max_res - TypeFunc::Parms + 1);\n@@ -993,1 +1026,1 @@\n-      projs->resproj = pn;\n+      projs->resproj[0] = pn;\n@@ -996,1 +1029,3 @@\n-      assert(false, \"unexpected projection from allocation node.\");\n+      assert(pn->_con <= max_res, \"unexpected projection from allocation node.\");\n+      projs->resproj[pn->_con-TypeFunc::Parms] = pn;\n+      break;\n@@ -1003,1 +1038,1 @@\n-  assert(projs->fallthrough_proj      != nullptr, \"must be found\");\n+  assert(!do_asserts || projs->fallthrough_proj      != nullptr, \"must be found\");\n@@ -1013,0 +1048,1 @@\n+  return projs;\n@@ -1053,2 +1089,2 @@\n-  uint old_dbg_start = sfpt->is_Call() ? sfpt->as_Call()->tf()->domain()->cnt() : (uint)TypeFunc::Parms+1;\n-  uint new_dbg_start = tf()->domain()->cnt();\n+  uint old_dbg_start = sfpt->is_Call() ? sfpt->as_Call()->tf()->domain_sig()->cnt() : (uint)TypeFunc::Parms+1;\n+  uint new_dbg_start = tf()->domain_sig()->cnt();\n@@ -1095,0 +1131,4 @@\n+  Bytecodes::Code bc = jvms()->method()->java_code_at_bci(jvms()->bci());\n+  if (Arguments::is_valhalla_enabled() && (bc == Bytecodes::_if_acmpeq || bc == Bytecodes::_if_acmpne)) {\n+    return true;\n+  }\n@@ -1137,0 +1177,65 @@\n+  if (can_reshape && uncommon_trap_request() != 0) {\n+    PhaseIterGVN* igvn = phase->is_IterGVN();\n+    if (remove_unknown_flat_array_load(igvn, control(), memory(), in(TypeFunc::Parms))) {\n+      if (!control()->is_Region()) {\n+        igvn->replace_input_of(this, 0, phase->C->top());\n+      }\n+      return this;\n+    }\n+  }\n+\n+  \/\/ Try to replace the runtime call to the substitutability test emitted by acmp if (at least) one operand is a known type\n+  if (can_reshape && !control()->is_top() && method() != nullptr && method()->holder() == phase->C->env()->ValueObjectMethods_klass() &&\n+      (method()->name() == ciSymbols::isSubstitutableAlt_name() || method()->name() == ciSymbols::isSubstitutable_name())) {\n+    Node* left = in(TypeFunc::Parms);\n+    Node* right = in(TypeFunc::Parms + 1);\n+    if (!left->is_top() && !right->is_top() && (left->is_InlineType() || right->is_InlineType())) {\n+      if (!left->is_InlineType()) {\n+        swap(left, right);\n+      }\n+      InlineTypeNode* vt = left->as_InlineType();\n+\n+      \/\/ Check if the field layout can be optimized\n+      if (vt->can_emit_substitutability_check(right)) {\n+        PhaseIterGVN* igvn = phase->is_IterGVN();\n+\n+        Node* ctrl = control();\n+        RegionNode* region = new RegionNode(1);\n+        Node* phi = new PhiNode(region, TypeInt::POS);\n+\n+        Node* base = right;\n+        Node* ptr = right;\n+        if (!base->is_InlineType()) {\n+          \/\/ Parse time checks guarantee that both operands are non-null and have the same type\n+          base = igvn->register_new_node_with_optimizer(new CheckCastPPNode(ctrl, base, vt->bottom_type()));\n+          ptr = base;\n+        }\n+        \/\/ Emit IR for field-wise comparison\n+        vt->check_substitutability(igvn, region, phi, &ctrl, in(MemNode::Memory), base, ptr);\n+\n+        \/\/ Equals\n+        region->add_req(ctrl);\n+        phi->add_req(igvn->intcon(1));\n+\n+        ctrl = igvn->register_new_node_with_optimizer(region);\n+        Node* res = igvn->register_new_node_with_optimizer(phi);\n+\n+        \/\/ Kill exception projections and return a tuple that will replace the call\n+        CallProjections* projs = extract_projections(false \/*separate_io_proj*\/);\n+        if (projs->fallthrough_catchproj != nullptr) {\n+          igvn->replace_node(projs->fallthrough_catchproj, ctrl);\n+        }\n+        if (projs->catchall_memproj != nullptr) {\n+          igvn->replace_node(projs->catchall_memproj, igvn->C->top());\n+        }\n+        if (projs->catchall_ioproj != nullptr) {\n+          igvn->replace_node(projs->catchall_ioproj, igvn->C->top());\n+        }\n+        if (projs->catchall_catchproj != nullptr) {\n+          igvn->replace_node(projs->catchall_catchproj, igvn->C->top());\n+        }\n+        return TupleNode::make(tf()->range_cc(), ctrl, i_o(), memory(), frameptr(), returnadr(), res);\n+      }\n+    }\n+  }\n+\n@@ -1197,0 +1302,127 @@\n+\/\/ Split if can cause the flat array branch of an array load with unknown type (see\n+\/\/ Parse::array_load) to end in an uncommon trap. In that case, the call to\n+\/\/ 'load_unknown_inline' is useless. Replace it with an uncommon trap with the same JVMState.\n+bool CallStaticJavaNode::remove_unknown_flat_array_load(PhaseIterGVN* igvn, Node* ctl, Node* mem, Node* unc_arg) {\n+  if (ctl == nullptr || ctl->is_top() || mem == nullptr || mem->is_top() || !mem->is_MergeMem()) {\n+    return false;\n+  }\n+  if (ctl->is_Region()) {\n+    bool res = false;\n+    for (uint i = 1; i < ctl->req(); i++) {\n+      MergeMemNode* mm = mem->clone()->as_MergeMem();\n+      for (MergeMemStream mms(mm); mms.next_non_empty(); ) {\n+        Node* m = mms.memory();\n+        if (m->is_Phi() && m->in(0) == ctl) {\n+          mms.set_memory(m->in(i));\n+        }\n+      }\n+      if (remove_unknown_flat_array_load(igvn, ctl->in(i), mm, unc_arg)) {\n+        res = true;\n+        if (!ctl->in(i)->is_Region()) {\n+          igvn->replace_input_of(ctl, i, igvn->C->top());\n+        }\n+      }\n+      igvn->remove_dead_node(mm);\n+    }\n+    return res;\n+  }\n+  \/\/ Verify the control flow is ok\n+  Node* call = ctl;\n+  MemBarNode* membar = nullptr;\n+  for (;;) {\n+    if (call == nullptr || call->is_top()) {\n+      return false;\n+    }\n+    if (call->is_Proj() || call->is_Catch() || call->is_MemBar()) {\n+      call = call->in(0);\n+    } else if (call->Opcode() == Op_CallStaticJava && !call->in(0)->is_top() &&\n+               call->as_Call()->entry_point() == OptoRuntime::load_unknown_inline_Java()) {\n+      \/\/ If there is no explicit flat array accesses in the compilation unit, there would be no\n+      \/\/ membar here\n+      if (call->in(0)->is_Proj() && call->in(0)->in(0)->is_MemBar()) {\n+        membar = call->in(0)->in(0)->as_MemBar();\n+      }\n+      break;\n+    } else {\n+      return false;\n+    }\n+  }\n+\n+  JVMState* jvms = call->jvms();\n+  if (igvn->C->too_many_traps(jvms->method(), jvms->bci(), Deoptimization::trap_request_reason(uncommon_trap_request()))) {\n+    return false;\n+  }\n+\n+  Node* call_mem = call->in(TypeFunc::Memory);\n+  if (call_mem == nullptr || call_mem->is_top()) {\n+    return false;\n+  }\n+  if (!call_mem->is_MergeMem()) {\n+    call_mem = MergeMemNode::make(call_mem);\n+    igvn->register_new_node_with_optimizer(call_mem);\n+  }\n+\n+  \/\/ Verify that there's no unexpected side effect\n+  for (MergeMemStream mms2(mem->as_MergeMem(), call_mem->as_MergeMem()); mms2.next_non_empty2(); ) {\n+    Node* m1 = mms2.is_empty() ? mms2.base_memory() : mms2.memory();\n+    Node* m2 = mms2.memory2();\n+\n+    for (uint i = 0; i < 100; i++) {\n+      if (m1 == m2) {\n+        break;\n+      } else if (m1->is_Proj()) {\n+        m1 = m1->in(0);\n+      } else if (m1->is_MemBar()) {\n+        m1 = m1->in(TypeFunc::Memory);\n+      } else if (m1->Opcode() == Op_CallStaticJava &&\n+                 m1->as_Call()->entry_point() == OptoRuntime::load_unknown_inline_Java()) {\n+        if (m1 != call) {\n+          return false;\n+        }\n+        break;\n+      } else if (m1->is_MergeMem()) {\n+        MergeMemNode* mm = m1->as_MergeMem();\n+        int idx = mms2.alias_idx();\n+        if (idx == Compile::AliasIdxBot) {\n+          m1 = mm->base_memory();\n+        } else {\n+          m1 = mm->memory_at(idx);\n+        }\n+      } else {\n+        return false;\n+      }\n+    }\n+  }\n+  if (call_mem->outcnt() == 0) {\n+    igvn->remove_dead_node(call_mem);\n+  }\n+\n+  \/\/ Remove membar preceding the call\n+  if (membar != nullptr) {\n+    membar->remove(igvn);\n+  }\n+\n+  address call_addr = OptoRuntime::uncommon_trap_blob()->entry_point();\n+  CallNode* unc = new CallStaticJavaNode(OptoRuntime::uncommon_trap_Type(), call_addr, \"uncommon_trap\", nullptr);\n+  unc->init_req(TypeFunc::Control, call->in(0));\n+  unc->init_req(TypeFunc::I_O, call->in(TypeFunc::I_O));\n+  unc->init_req(TypeFunc::Memory, call->in(TypeFunc::Memory));\n+  unc->init_req(TypeFunc::FramePtr,  call->in(TypeFunc::FramePtr));\n+  unc->init_req(TypeFunc::ReturnAdr, call->in(TypeFunc::ReturnAdr));\n+  unc->init_req(TypeFunc::Parms+0, unc_arg);\n+  unc->set_cnt(PROB_UNLIKELY_MAG(4));\n+  unc->copy_call_debug_info(igvn, call->as_CallStaticJava());\n+\n+  \/\/ Replace the call with an uncommon trap\n+  igvn->replace_input_of(call, 0, igvn->C->top());\n+\n+  igvn->register_new_node_with_optimizer(unc);\n+\n+  Node* ctrl = igvn->transform(new ProjNode(unc, TypeFunc::Control));\n+  Node* halt = igvn->transform(new HaltNode(ctrl, call->in(TypeFunc::FramePtr), \"uncommon trap returned which should never happen\"));\n+  igvn->add_input_to(igvn->C->root(), halt);\n+\n+  return true;\n+}\n+\n+\n@@ -1312,0 +1544,7 @@\n+  if (_entry_point == nullptr) {\n+    \/\/ The call to that stub is a special case: its inputs are\n+    \/\/ multiple values returned from a call and so it should follow\n+    \/\/ the return convention.\n+    SharedRuntime::java_return_convention(sig_bt, parm_regs, argcnt);\n+    return;\n+  }\n@@ -1317,1 +1556,1 @@\n-  assert(tf()->range()->field_at(TypeFunc::Parms)->is_vect()->length_in_bytes() * BitsPerByte == _num_bits,\n+  assert(tf()->range_sig()->field_at(TypeFunc::Parms)->is_vect()->length_in_bytes() * BitsPerByte == _num_bits,\n@@ -1319,1 +1558,1 @@\n-  const TypeTuple* d = tf()->domain();\n+  const TypeTuple* d = tf()->domain_sig();\n@@ -1355,1 +1594,1 @@\n-      tf()->range(),\n+      tf()->range_cc(),\n@@ -1363,1 +1602,1 @@\n-  for (uint i = TypeFunc::Parms; i < tf()->range()->cnt(); i++) {\n+  for (uint i = TypeFunc::Parms; i < tf()->range_cc()->cnt(); i++) {\n@@ -1394,0 +1633,6 @@\n+uint CallLeafNoFPNode::match_edge(uint idx) const {\n+  \/\/ Null entry point is a special case for which the target is in a\n+  \/\/ register. Need to match that edge.\n+  return entry_point() == nullptr && idx == TypeFunc::Parms;\n+}\n+\n@@ -1444,1 +1689,14 @@\n-  return remove_dead_region(phase, can_reshape) ? this : nullptr;\n+  if (remove_dead_region(phase, can_reshape)) {\n+    return this;\n+  }\n+  \/\/ Scalarize inline types in safepoint debug info.\n+  \/\/ Delay this until all inlining is over to avoid getting inconsistent debug info.\n+  if (phase->C->scalarize_in_safepoints() && can_reshape && jvms() != nullptr) {\n+    for (uint i = jvms()->debug_start(); i < jvms()->debug_end(); i++) {\n+      Node* n = in(i)->uncast();\n+      if (n->is_InlineType()) {\n+        n->as_InlineType()->make_scalar_in_safepoints(phase->is_IterGVN());\n+      }\n+    }\n+  }\n+  return nullptr;\n@@ -1598,1 +1856,1 @@\n-  if (!alloc->is_Allocate() && !(alloc->Opcode() == Op_VectorBox)) {\n+  if (alloc != nullptr && !alloc->is_Allocate() && !(alloc->Opcode() == Op_VectorBox)) {\n@@ -1703,1 +1961,3 @@\n-                           Node *size, Node *klass_node, Node *initial_test)\n+                           Node *size, Node *klass_node,\n+                           Node* initial_test,\n+                           InlineTypeNode* inline_type_node)\n@@ -1711,0 +1971,1 @@\n+  _larval = false;\n@@ -1723,0 +1984,3 @@\n+  init_req( InlineType     , inline_type_node);\n+  \/\/ DefaultValue defaults to nullptr\n+  \/\/ RawDefaultValue defaults to nullptr\n@@ -1728,1 +1992,2 @@\n-  assert(initializer != nullptr && initializer->is_object_initializer(),\n+  assert(initializer != nullptr &&\n+         (initializer->is_object_constructor() || initializer->is_class_initializer()),\n@@ -1740,1 +2005,2 @@\n-Node *AllocateNode::make_ideal_mark(PhaseGVN *phase, Node* obj, Node* control, Node* mem) {\n+\n+Node* AllocateNode::make_ideal_mark(PhaseGVN* phase, Node* control, Node* mem) {\n@@ -1742,1 +2008,1 @@\n-  if (UseCompactObjectHeaders) {\n+  if (UseCompactObjectHeaders || Arguments::is_valhalla_enabled()) {\n@@ -1746,0 +2012,6 @@\n+    if (Arguments::is_valhalla_enabled()) {\n+      mark_node = phase->transform(mark_node);\n+      \/\/ Avoid returning a constant (old node) here because this method is used by LoadNode::Ideal\n+      mark_node = new OrXNode(mark_node, phase->MakeConX(_larval ? markWord::larval_bit_in_place : 0));\n+    }\n+    return mark_node;\n@@ -1747,2 +2019,1 @@\n-    \/\/ For now only enable fast locking for non-array types\n-    mark_node = phase->MakeConX(markWord::prototype().value());\n+    return phase->MakeConX(markWord::prototype().value());\n@@ -1750,1 +2021,0 @@\n-  return mark_node;\n@@ -2146,1 +2416,2 @@\n-  if (can_reshape && EliminateLocks && !is_non_esc_obj()) {\n+  const Type* obj_type = phase->type(obj_node());\n+  if (can_reshape && EliminateLocks && !is_non_esc_obj() && !obj_type->is_inlinetypeptr()) {\n@@ -2347,1 +2618,2 @@\n-  if (can_reshape && EliminateLocks && !is_non_esc_obj()) {\n+  const Type* obj_type = phase->type(obj_node());\n+  if (can_reshape && EliminateLocks && !is_non_esc_obj() && !obj_type->is_inlinetypeptr()) {\n@@ -2427,1 +2699,2 @@\n-    dest_t = dest_t->add_offset(Type::OffsetBot)->is_oopptr();\n+    dest_t = dest_t->is_aryptr()->with_field_offset(Type::OffsetBot)->add_offset(Type::OffsetBot)->is_oopptr();\n+    t_oop = t_oop->is_aryptr()->with_field_offset(Type::OffsetBot);\n","filename":"src\/hotspot\/share\/opto\/callnode.cpp","additions":360,"deletions":87,"binary":false,"changes":447,"status":"modified"},{"patch":"@@ -79,1 +79,1 @@\n-  virtual Node *match( const ProjNode *proj, const Matcher *m );\n+  virtual Node *match(const ProjNode *proj, const Matcher *m, const RegMask* mask);\n@@ -93,1 +93,0 @@\n-  static  const TypeTuple *osr_domain();\n@@ -669,1 +668,1 @@\n-class CallProjections : public StackObj {\n+class CallProjections {\n@@ -678,1 +677,19 @@\n-  Node* resproj;\n+  uint nb_resproj;\n+  Node* resproj[1]; \/\/ at least one projection\n+\n+  CallProjections(uint nbres) {\n+    fallthrough_proj      = nullptr;\n+    fallthrough_catchproj = nullptr;\n+    fallthrough_memproj   = nullptr;\n+    fallthrough_ioproj    = nullptr;\n+    catchall_catchproj    = nullptr;\n+    catchall_memproj      = nullptr;\n+    catchall_ioproj       = nullptr;\n+    exobj                 = nullptr;\n+    nb_resproj            = nbres;\n+    resproj[0]            = nullptr;\n+    for (uint i = 1; i < nb_resproj; i++) {\n+      resproj[i]          = nullptr;\n+    }\n+  }\n+\n@@ -700,1 +717,1 @@\n-    : SafePointNode(tf->domain()->cnt(), jvms, adr_type),\n+    : SafePointNode(tf->domain_cc()->cnt(), jvms, adr_type),\n@@ -727,1 +744,1 @@\n-  virtual Node*       match(const ProjNode* proj, const Matcher* m);\n+  virtual Node*       match(const ProjNode* proj, const Matcher* m, const RegMask* mask);\n@@ -742,0 +759,1 @@\n+  bool                has_debug_use(Node* n);\n@@ -748,2 +766,3 @@\n-    const TypeTuple* r = tf()->range();\n-    return (r->cnt() > TypeFunc::Parms &&\n+    const TypeTuple* r = tf()->range_sig();\n+    return (!tf()->returns_inline_type_as_fields() &&\n+            r->cnt() > TypeFunc::Parms &&\n@@ -756,1 +775,1 @@\n-  void extract_projections(CallProjections* projs, bool separate_io_proj, bool do_asserts = true) const;\n+  CallProjections* extract_projections(bool separate_io_proj, bool do_asserts = true) const;\n@@ -823,0 +842,3 @@\n+\n+  bool remove_unknown_flat_array_load(PhaseIterGVN* igvn, Node* ctl, Node* mem, Node* unc_arg);\n+\n@@ -831,0 +853,11 @@\n+    const TypeTuple *r = tf->range_sig();\n+    if (InlineTypeReturnedAsFields &&\n+        method != nullptr &&\n+        method->is_method_handle_intrinsic() &&\n+        r->cnt() > TypeFunc::Parms &&\n+        r->field_at(TypeFunc::Parms)->isa_oopptr() &&\n+        r->field_at(TypeFunc::Parms)->is_oopptr()->can_be_inline_type()) {\n+      \/\/ Make sure this call is processed by PhaseMacroExpand::expand_mh_intrinsic_return\n+      init_flags(Flag_is_macro);\n+      C->add_macro_node(this);\n+    }\n@@ -967,0 +1000,1 @@\n+  virtual uint match_edge(uint idx) const;\n@@ -1009,0 +1043,3 @@\n+    InlineType,                       \/\/ InlineTypeNode if this is an inline type allocation\n+    InitValue,                        \/\/ Init value for null-free inline type arrays\n+    RawInitValue,                     \/\/ Same as above but as raw machine word\n@@ -1019,0 +1056,3 @@\n+    fields[InlineType] = Type::BOTTOM;\n+    fields[InitValue] = TypeInstPtr::NOTNULL;\n+    fields[RawInitValue] = TypeX_X;\n@@ -1036,0 +1076,1 @@\n+  bool _larval;\n@@ -1039,1 +1080,2 @@\n-               Node *size, Node *klass_node, Node *initial_test);\n+               Node *size, Node *klass_node, Node *initial_test,\n+               InlineTypeNode* inline_type_node = nullptr);\n@@ -1104,1 +1146,1 @@\n-  Node* make_ideal_mark(PhaseGVN *phase, Node* obj, Node* control, Node* mem);\n+  Node* make_ideal_mark(PhaseGVN* phase, Node* control, Node* mem);\n@@ -1116,1 +1158,2 @@\n-                    Node* initial_test, Node* count_val, Node* valid_length_test)\n+                    Node* initial_test, Node* count_val, Node* valid_length_test,\n+                    Node* init_value, Node* raw_init_value)\n@@ -1121,1 +1164,1 @@\n-    set_req(AllocateNode::ALength,        count_val);\n+    set_req(AllocateNode::ALength, count_val);\n@@ -1123,0 +1166,2 @@\n+    init_req(AllocateNode::InitValue, init_value);\n+    init_req(AllocateNode::RawInitValue, raw_init_value);\n@@ -1124,0 +1169,1 @@\n+  virtual uint size_of() const { return sizeof(*this); }\n","filename":"src\/hotspot\/share\/opto\/callnode.hpp","additions":59,"deletions":13,"binary":false,"changes":72,"status":"modified"},{"patch":"@@ -1621,1 +1621,1 @@\n-      tf()->range(),\n+      tf()->range_cc(),\n@@ -1684,1 +1684,1 @@\n-Node *DivModINode::match( const ProjNode *proj, const Matcher *match ) {\n+Node *DivModINode::match(const ProjNode *proj, const Matcher *match, const RegMask* mask) {\n@@ -1699,1 +1699,1 @@\n-Node *DivModLNode::match( const ProjNode *proj, const Matcher *match ) {\n+Node *DivModLNode::match(const ProjNode *proj, const Matcher *match, const RegMask* mask) {\n@@ -1737,1 +1737,1 @@\n-Node* UDivModINode::match( const ProjNode *proj, const Matcher *match ) {\n+Node* UDivModINode::match(const ProjNode* proj, const Matcher* match, const RegMask* mask) {\n@@ -1752,1 +1752,1 @@\n-Node* UDivModLNode::match( const ProjNode *proj, const Matcher *match ) {\n+Node* UDivModLNode::match( const ProjNode* proj, const Matcher* match, const RegMask* mask) {\n","filename":"src\/hotspot\/share\/opto\/divnode.cpp","additions":5,"deletions":5,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -31,0 +31,1 @@\n+#include \"memory\/metaspace.hpp\"\n@@ -39,0 +40,1 @@\n+#include \"opto\/inlinetypenode.hpp\"\n@@ -170,0 +172,10 @@\n+    if ((n->Opcode() == Op_LoadX || n->Opcode() == Op_StoreX) &&\n+        !n->in(MemNode::Address)->is_AddP() &&\n+        _igvn->type(n->in(MemNode::Address))->isa_oopptr()) {\n+      \/\/ Load\/Store at mark work address is at offset 0 so has no AddP which confuses EA\n+      Node* addp = new AddPNode(n->in(MemNode::Address), n->in(MemNode::Address), _igvn->MakeConX(0));\n+      _igvn->register_new_node_with_optimizer(addp);\n+      _igvn->replace_input_of(n, MemNode::Address, addp);\n+      ideal_nodes.push(addp);\n+      _nodes.at_put_grow(addp->_idx, nullptr, nullptr);\n+    }\n@@ -427,0 +439,8 @@\n+  \/\/ 6. Expand flat accesses if the object does not escape. This adds nodes to\n+  \/\/ the graph, so it has to be after split_unique_types. This expands atomic\n+  \/\/ mismatched accesses (though encapsulated in LoadFlats and StoreFlats) into\n+  \/\/ non-mismatched accesses, so it is better before reduce allocation merges.\n+  if (has_non_escaping_obj) {\n+    optimize_flat_accesses(sfn_worklist);\n+  }\n+\n@@ -429,1 +449,1 @@\n-  \/\/ 6. Reduce allocation merges used as debug information. This is done after\n+  \/\/ 7. Reduce allocation merges used as debug information. This is done after\n@@ -1303,1 +1323,9 @@\n-      SafePointScalarObjectNode* sobj = mexp.create_scalarized_object_description(alloc, sfpt);\n+      Unique_Node_List value_worklist;\n+#ifdef ASSERT\n+      const Type* res_type = alloc->result_cast()->bottom_type();\n+      if (res_type->is_inlinetypeptr() && !Compile::current()->has_circular_inline_type()) {\n+        PhiNode* phi = ophi->as_Phi();\n+        assert(!ophi->as_Phi()->can_push_inline_types_down(_igvn), \"missed earlier scalarization opportunity\");\n+      }\n+#endif\n+      SafePointScalarObjectNode* sobj = mexp.create_scalarized_object_description(alloc, sfpt, &value_worklist);\n@@ -1305,0 +1333,1 @@\n+        _compile->record_failure(C2Compiler::retry_no_reduce_allocation_merges());\n@@ -1315,0 +1344,9 @@\n+\n+      \/\/ Scalarize inline types that were added to the safepoint.\n+      \/\/ Don't allow linking a constant oop (if available) for flat array elements\n+      \/\/ because Deoptimization::reassign_flat_array_elements needs field values.\n+      const bool allow_oop = !merge_t->is_flat();\n+      for (uint j = 0; j < value_worklist.size(); ++j) {\n+        InlineTypeNode* vt = value_worklist.at(j)->as_InlineType();\n+        vt->make_scalar_in_safepoints(_igvn, allow_oop);\n+      }\n@@ -1517,1 +1555,1 @@\n-      const TypeTuple* d = call->tf()->domain();\n+      const TypeTuple* d = call->tf()->domain_sig();\n@@ -1591,0 +1629,11 @@\n+      } else if (n->as_Call()->tf()->returns_inline_type_as_fields()) {\n+        bool returns_oop = false;\n+        for (DUIterator_Fast imax, i = n->fast_outs(imax); i < imax && !returns_oop; i++) {\n+          ProjNode* pn = n->fast_out(i)->as_Proj();\n+          if (pn->_con >= TypeFunc::Parms && pn->bottom_type()->isa_ptr()) {\n+            returns_oop = true;\n+          }\n+        }\n+        if (returns_oop) {\n+          add_call_node(n->as_Call());\n+        }\n@@ -1618,1 +1667,2 @@\n-    case Op_CastX2P: {\n+    case Op_CastX2P:\n+    case Op_CastI2N: {\n@@ -1622,0 +1672,1 @@\n+    case Op_InlineType:\n@@ -1691,0 +1742,8 @@\n+    case Op_LoadFlat:\n+      \/\/ Treat LoadFlat similar to an unknown call that receives nothing and produces its results\n+      map_ideal_node(n, phantom_obj);\n+      break;\n+    case Op_StoreFlat:\n+      \/\/ Treat StoreFlat similar to a call that escapes the stored flattened fields\n+      delayed_worklist->push(n);\n+      break;\n@@ -1693,2 +1752,7 @@\n-      if (n->as_Proj()->_con == TypeFunc::Parms && n->in(0)->is_Call() &&\n-          n->in(0)->as_Call()->returns_pointer()) {\n+      if (n->as_Proj()->_con >= TypeFunc::Parms && n->in(0)->is_Call() &&\n+          (n->in(0)->as_Call()->returns_pointer() || n->bottom_type()->isa_ptr())) {\n+        assert((n->as_Proj()->_con == TypeFunc::Parms && n->in(0)->as_Call()->returns_pointer()) ||\n+               n->in(0)->as_Call()->tf()->returns_inline_type_as_fields(), \"what kind of oop return is it?\");\n+        add_local_var_and_edge(n, PointsToNode::NoEscape, n->in(0), delayed_worklist);\n+      } else if (n->as_Proj()->_con >= TypeFunc::Parms && n->in(0)->is_LoadFlat() && igvn->type(n)->isa_ptr()) {\n+        \/\/ Treat LoadFlat outputs similar to a call return value\n@@ -1780,1 +1844,1 @@\n-  assert(n->is_Store() || n->is_LoadStore() ||\n+  assert(n->is_Store() || n->is_LoadStore() || n->is_StoreFlat() ||\n@@ -1796,0 +1860,1 @@\n+    case Op_InlineType:\n@@ -1848,0 +1913,16 @@\n+    case Op_StoreFlat: {\n+      \/\/ StoreFlat globally escapes its stored flattened fields\n+      InlineTypeNode* value = n->as_StoreFlat()->value();\n+      ciInlineKlass* vk = _igvn->type(value)->inline_klass();\n+      for (int i = 0; i < vk->nof_nonstatic_fields(); i++) {\n+        ciField* field = vk->nonstatic_field_at(i);\n+        if (field->type()->is_primitive_type()) {\n+          continue;\n+        }\n+\n+        Node* field_value = value->field_value_by_offset(field->offset_in_bytes(), true);\n+        PointsToNode* field_value_ptn = ptnode_adr(field_value->_idx);\n+        set_escape_state(field_value_ptn, PointsToNode::GlobalEscape NOT_PRODUCT(COMMA \"store into a flat field\"));\n+      }\n+      break;\n+    }\n@@ -1849,4 +1930,9 @@\n-      \/\/ we are only interested in the oop result projection from a call\n-      assert(n->as_Proj()->_con == TypeFunc::Parms && n->in(0)->is_Call() &&\n-             n->in(0)->as_Call()->returns_pointer(), \"Unexpected node type\");\n-      add_local_var_and_edge(n, PointsToNode::NoEscape, n->in(0), nullptr);\n+      if (n->in(0)->is_Call()) {\n+        \/\/ we are only interested in the oop result projection from a call\n+        assert((n->as_Proj()->_con == TypeFunc::Parms && n->in(0)->as_Call()->returns_pointer()) ||\n+              n->in(0)->as_Call()->tf()->returns_inline_type_as_fields(), \"what kind of oop return is it?\");\n+        add_local_var_and_edge(n, PointsToNode::NoEscape, n->in(0), nullptr);\n+      } else if (n->in(0)->is_LoadFlat()) {\n+        \/\/ Treat LoadFlat outputs similar to a call return value\n+        add_local_var_and_edge(n, PointsToNode::NoEscape, n->in(0), nullptr);\n+      }\n@@ -2027,1 +2113,1 @@\n-  assert(call->returns_pointer(), \"only for call which returns pointer\");\n+  assert(call->returns_pointer() || call->tf()->returns_inline_type_as_fields(), \"only for call which returns pointer\");\n@@ -2102,1 +2188,4 @@\n-      assert(call->as_CallStaticJava()->is_call_to_multianewarray_stub(), \"TODO: add failed case check\");\n+      const char* name = call->as_CallStaticJava()->_name;\n+      assert(call->as_CallStaticJava()->is_call_to_multianewarray_stub() ||\n+             strncmp(name, \"load_unknown_inline\", 19) == 0 ||\n+             strncmp(name, \"store_inline_type_fields_to_buf\", 31) == 0, \"TODO: add failed case check\");\n@@ -2133,1 +2222,1 @@\n-        const TypeTuple* d = call->tf()->domain();\n+        const TypeTuple* d = call->tf()->domain_cc();\n@@ -2181,1 +2270,1 @@\n-      const TypeTuple * d = call->tf()->domain();\n+      const TypeTuple * d = call->tf()->domain_sig();\n@@ -2212,1 +2301,4 @@\n-                               (aat->isa_aryptr() && (aat->isa_aryptr()->elem() == Type::BOTTOM || aat->isa_aryptr()->elem()->make_oopptr() != nullptr)));\n+                               (aat->isa_aryptr() && (aat->isa_aryptr()->elem() == Type::BOTTOM || aat->isa_aryptr()->elem()->make_oopptr() != nullptr)) ||\n+                               (aat->isa_aryptr() && aat->isa_aryptr()->elem() != nullptr &&\n+                                                               aat->isa_aryptr()->is_flat() &&\n+                                                               aat->isa_aryptr()->elem()->inline_klass()->contains_oops()));\n@@ -2276,0 +2368,4 @@\n+                  strcmp(call->as_CallLeaf()->_name, \"vectorizedMismatch\") == 0 ||\n+                  strcmp(call->as_CallLeaf()->_name, \"load_unknown_inline\") == 0 ||\n+                  strcmp(call->as_CallLeaf()->_name, \"store_unknown_inline\") == 0 ||\n+                  strcmp(call->as_CallLeaf()->_name, \"store_inline_type_fields_to_buf\") == 0 ||\n@@ -2338,1 +2434,1 @@\n-        const TypeTuple* d = call->tf()->domain();\n+        const TypeTuple* d = call->tf()->domain_cc();\n@@ -2382,1 +2478,1 @@\n-      const TypeTuple* d = call->tf()->domain();\n+      const TypeTuple* d = call->tf()->domain_cc();\n@@ -2800,0 +2896,1 @@\n+  PointsToNode* init_val = phantom_obj;\n@@ -2805,1 +2902,7 @@\n-    return 0;\n+    if (alloc->as_Allocate()->in(AllocateNode::InitValue) != nullptr) {\n+      \/\/ Null-free inline type arrays are initialized with an init value instead of null\n+      init_val = ptnode_adr(alloc->as_Allocate()->in(AllocateNode::InitValue)->_idx);\n+      assert(init_val != nullptr, \"init value should be registered\");\n+    } else {\n+      return 0;\n+    }\n@@ -2807,1 +2910,2 @@\n-  assert(pta->arraycopy_dst() || alloc->as_CallStaticJava(), \"sanity\");\n+  \/\/ Non-escaped allocation returned from Java or runtime call has unknown values in fields.\n+  assert(pta->arraycopy_dst() || alloc->is_CallStaticJava() || init_val != phantom_obj, \"sanity\");\n@@ -2809,2 +2913,5 @@\n-  if (!pta->arraycopy_dst() && alloc->as_CallStaticJava()->method() == nullptr) {\n-    assert(alloc->as_CallStaticJava()->is_call_to_multianewarray_stub(), \"sanity\");\n+  if (alloc->is_CallStaticJava() && alloc->as_CallStaticJava()->method() == nullptr) {\n+    const char* name = alloc->as_CallStaticJava()->_name;\n+    assert(alloc->as_CallStaticJava()->is_call_to_multianewarray_stub() ||\n+           strncmp(name, \"load_unknown_inline\", 19) == 0 ||\n+           strncmp(name, \"store_inline_type_fields_to_buf\", 31) == 0, \"sanity\");\n@@ -2818,1 +2925,1 @@\n-      if (add_edge(field, phantom_obj)) {\n+      if (add_edge(field, init_val)) {\n@@ -2833,1 +2940,1 @@\n-  if (!alloc->is_Allocate()) {\n+  if (!alloc->is_Allocate() || alloc->as_Allocate()->in(AllocateNode::InitValue) != nullptr) {\n@@ -2919,1 +3026,1 @@\n-                tty->print_cr(\"----------missed referernce to object-----------\");\n+                tty->print_cr(\"----------missed reference to object------------\");\n@@ -2921,1 +3028,1 @@\n-                tty->print_cr(\"----------object referernced by init store -----\");\n+                tty->print_cr(\"----------object referenced by init store-------\");\n@@ -3286,1 +3393,2 @@\n-          if (can_eliminate_lock(alock)) {\n+          const Type* obj_type = igvn->type(alock->obj_node());\n+          if (can_eliminate_lock(alock) && !obj_type->is_inlinetypeptr()) {\n@@ -3328,5 +3436,10 @@\n-      MemBarNode* mb = MemBarNode::make(C, Op_MemBarCPUOrder, Compile::AliasIdxBot);\n-      mb->init_req(TypeFunc::Memory,  storestore->in(TypeFunc::Memory));\n-      mb->init_req(TypeFunc::Control, storestore->in(TypeFunc::Control));\n-      igvn->register_new_node_with_optimizer(mb);\n-      igvn->replace_node(storestore, mb);\n+      if (alloc->in(AllocateNode::InlineType) != nullptr) {\n+        \/\/ Non-escaping inline type buffer allocations don't require a membar\n+        storestore->as_MemBar()->remove(_igvn);\n+      } else {\n+        MemBarNode* mb = MemBarNode::make(C, Op_MemBarCPUOrder, Compile::AliasIdxBot);\n+        mb->init_req(TypeFunc::Memory,  storestore->in(TypeFunc::Memory));\n+        mb->init_req(TypeFunc::Control, storestore->in(TypeFunc::Control));\n+        igvn->register_new_node_with_optimizer(mb);\n+        igvn->replace_node(storestore, mb);\n+      }\n@@ -3337,0 +3450,25 @@\n+\/\/ Atomic flat accesses on non-escaping objects can be optimized to non-atomic accesses\n+void ConnectionGraph::optimize_flat_accesses(GrowableArray<SafePointNode*>& sfn_worklist) {\n+  PhaseIterGVN& igvn = *_igvn;\n+  bool delay = igvn.delay_transform();\n+  igvn.set_delay_transform(true);\n+  igvn.C->for_each_flat_access([&](Node* n) {\n+    Node* base = n->is_LoadFlat() ? n->as_LoadFlat()->base() : n->as_StoreFlat()->base();\n+    if (!not_global_escape(base)) {\n+      return;\n+    }\n+\n+    bool expanded;\n+    if (n->is_LoadFlat()) {\n+      expanded = n->as_LoadFlat()->expand_non_atomic(igvn);\n+    } else {\n+      expanded = n->as_StoreFlat()->expand_non_atomic(igvn);\n+    }\n+    if (expanded) {\n+      sfn_worklist.remove(n->as_SafePoint());\n+      igvn.C->remove_flat_access(n);\n+    }\n+  });\n+  igvn.set_delay_transform(delay);\n+}\n+\n@@ -3496,0 +3634,1 @@\n+  int field_offset = adr_type->isa_aryptr() ? adr_type->isa_aryptr()->field_offset().get() : Type::OffsetBot;\n@@ -3497,1 +3636,1 @@\n-  if (offset == Type::OffsetBot) {\n+  if (offset == Type::OffsetBot && field_offset == Type::OffsetBot) {\n@@ -3509,1 +3648,1 @@\n-      ciField* field = _compile->alias_type(adr_type->isa_instptr())->field();\n+      ciField* field = _compile->alias_type(adr_type->is_ptr())->field();\n@@ -3528,2 +3667,14 @@\n-        const Type* elemtype = adr_type->isa_aryptr()->elem();\n-        bt = elemtype->array_element_basic_type();\n+        const Type* elemtype = adr_type->is_aryptr()->elem();\n+        if (adr_type->is_aryptr()->is_flat() && field_offset != Type::OffsetBot) {\n+          ciInlineKlass* vk = elemtype->inline_klass();\n+          field_offset += vk->payload_offset();\n+          ciField* field = vk->get_field_by_offset(field_offset, false);\n+          if (field != nullptr) {\n+            bt = field->layout_type();\n+          } else {\n+            assert(field_offset == vk->payload_offset() + vk->null_marker_offset_in_payload(), \"no field or null marker of %s at offset %d\", vk->name()->as_utf8(), field_offset);\n+            bt = T_BOOLEAN;\n+          }\n+        } else {\n+          bt = elemtype->array_element_basic_type();\n+        }\n@@ -3726,3 +3877,1 @@\n-  const TypePtr *t_ptr = adr_type->isa_ptr();\n-  assert(t_ptr != nullptr, \"must be a pointer type\");\n-  return t_ptr->offset();\n+  return adr_type->is_ptr()->flat_offset();\n@@ -3882,1 +4031,8 @@\n-    t = base_t->add_offset(offs)->is_oopptr();\n+    if (base_t->isa_aryptr() != nullptr) {\n+      \/\/ In the case of a flat inline type array, each field has its\n+      \/\/ own slice so we need to extract the field being accessed from\n+      \/\/ the address computation\n+      t = base_t->isa_aryptr()->add_field_offset_and_offset(offs)->is_oopptr();\n+    } else {\n+      t = base_t->add_offset(offs)->is_oopptr();\n+    }\n@@ -3884,1 +4040,1 @@\n-  int inst_id =  base_t->instance_id();\n+  int inst_id = base_t->instance_id();\n@@ -3898,1 +4054,1 @@\n-  \/\/ It could happened when CHA type is different from MDO type on a dead path\n+  \/\/ It could happen when CHA type is different from MDO type on a dead path\n@@ -3908,1 +4064,12 @@\n-  const TypeOopPtr *tinst = base_t->add_offset(t->offset())->is_oopptr();\n+  const TypePtr* tinst = base_t->add_offset(t->offset());\n+  if (tinst->isa_aryptr() && t->isa_aryptr()) {\n+    \/\/ In the case of a flat inline type array, each field has its\n+    \/\/ own slice so we need to keep track of the field being accessed.\n+    tinst = tinst->is_aryptr()->with_field_offset(t->is_aryptr()->field_offset().get());\n+    \/\/ Keep array properties (not flat\/null-free)\n+    tinst = tinst->is_aryptr()->update_properties(t->is_aryptr());\n+    if (tinst == nullptr) {\n+      return false; \/\/ Skip dead path with inconsistent properties\n+    }\n+  }\n+\n@@ -4518,1 +4685,6 @@\n-          const TypePtr* new_adr_type = tinst->add_offset(adr_type->offset());\n+          const TypePtr* new_adr_type = tinst->with_offset(adr_type->offset());\n+          if (adr_type->isa_aryptr()) {\n+            \/\/ In the case of a flat inline type array, each field has its own slice so we need a\n+            \/\/ NarrowMemProj for each field of the flat array elements\n+            new_adr_type = new_adr_type->is_aryptr()->with_field_offset(adr_type->is_aryptr()->field_offset().get());\n+          }\n@@ -4635,0 +4807,7 @@\n+          if (tn_t->isa_aryptr()) {\n+            \/\/ Keep array properties (not flat\/null-free)\n+            tinst = tinst->is_aryptr()->update_properties(tn_t->is_aryptr());\n+            if (tinst == nullptr) {\n+              continue; \/\/ Skip dead path with inconsistent properties\n+            }\n+          }\n@@ -4660,1 +4839,1 @@\n-      if(use->is_Mem() && use->in(MemNode::Address) == n) {\n+      if (use->is_Mem() && use->in(MemNode::Address) == n) {\n@@ -4696,0 +4875,3 @@\n+      } else if (use->Opcode() == Op_Return) {\n+        \/\/ Allocation is referenced by field of returned inline type\n+        assert(_compile->tf()->returns_inline_type_as_fields(), \"EA: unexpected reference by ReturnNode\");\n@@ -4709,1 +4891,1 @@\n-              op == Op_SubTypeCheck ||\n+              op == Op_SubTypeCheck || op == Op_InlineType || op == Op_FlatArrayCheck ||\n@@ -4817,0 +4999,3 @@\n+    } else if (n->is_CallLeaf() && n->as_CallLeaf()->_name != nullptr &&\n+               strcmp(n->as_CallLeaf()->_name, \"store_unknown_inline\") == 0) {\n+      n = n->as_CallLeaf()->proj_out(TypeFunc::Memory);\n@@ -4868,1 +5053,1 @@\n-      } else if(use->is_Mem()) {\n+      } else if (use->is_Mem()) {\n@@ -4877,0 +5062,4 @@\n+      } else if (use->is_CallLeaf() && use->as_CallLeaf()->_name != nullptr &&\n+                 strcmp(use->as_CallLeaf()->_name, \"store_unknown_inline\") == 0) {\n+        \/\/ store_unknown_inline overwrites destination array\n+        memnode_worklist.append_if_missing(use);\n@@ -4886,1 +5075,1 @@\n-              op == Op_StrEquals || op == Op_StrIndexOf || op == Op_StrIndexOfChar)) {\n+              op == Op_StrEquals || op == Op_StrIndexOf || op == Op_StrIndexOfChar || op == Op_FlatArrayCheck)) {\n@@ -4989,1 +5178,1 @@\n-  \/\/ chains as is done in split_memory_phi() since they  will\n+  \/\/ chains as is done in split_memory_phi() since they will\n","filename":"src\/hotspot\/share\/opto\/escape.cpp","additions":238,"deletions":49,"binary":false,"changes":287,"status":"modified"},{"patch":"@@ -26,0 +26,3 @@\n+#include \"ci\/ciArrayKlass.hpp\"\n+#include \"ci\/ciFlatArrayKlass.hpp\"\n+#include \"ci\/ciInstanceKlass.hpp\"\n@@ -32,0 +35,1 @@\n+#include \"gc\/shared\/c2\/barrierSetC2.hpp\"\n@@ -34,0 +38,1 @@\n+#include \"oops\/accessDecorators.hpp\"\n@@ -35,0 +40,1 @@\n+#include \"oops\/layoutKind.hpp\"\n@@ -43,0 +49,1 @@\n+#include \"opto\/graphKit.hpp\"\n@@ -44,0 +51,1 @@\n+#include \"opto\/inlinetypenode.hpp\"\n@@ -49,0 +57,1 @@\n+#include \"opto\/opcodes.hpp\"\n@@ -53,0 +62,1 @@\n+#include \"opto\/type.hpp\"\n@@ -57,0 +67,1 @@\n+#include \"runtime\/globals.hpp\"\n@@ -62,0 +73,1 @@\n+#include \"utilities\/globalDefinitions.hpp\"\n@@ -322,0 +334,2 @@\n+  case vmIntrinsics::_makePrivateBuffer:        return inline_unsafe_make_private_buffer();\n+  case vmIntrinsics::_finishPrivateBuffer:      return inline_unsafe_finish_private_buffer();\n@@ -412,0 +426,3 @@\n+  case vmIntrinsics::_getFlatValue:             return inline_unsafe_flat_access(!is_store, Relaxed);\n+  case vmIntrinsics::_putFlatValue:             return inline_unsafe_flat_access( is_store, Relaxed);\n+\n@@ -471,0 +488,5 @@\n+  case vmIntrinsics::_arrayInstanceBaseOffset:  return inline_arrayInstanceBaseOffset();\n+  case vmIntrinsics::_arrayInstanceIndexScale:  return inline_arrayInstanceIndexScale();\n+  case vmIntrinsics::_arrayLayout:              return inline_arrayLayout();\n+  case vmIntrinsics::_getFieldMap:              return inline_getFieldMap();\n+\n@@ -519,0 +541,6 @@\n+  case vmIntrinsics::_newNullRestrictedNonAtomicArray: return inline_newArray(\/* null_free *\/ true, \/* atomic *\/ false);\n+  case vmIntrinsics::_newNullRestrictedAtomicArray: return inline_newArray(\/* null_free *\/ true, \/* atomic *\/ true);\n+  case vmIntrinsics::_newNullableAtomicArray:     return inline_newArray(\/* null_free *\/ false, \/* atomic *\/ true);\n+  case vmIntrinsics::_isFlatArray:              return inline_getArrayProperties(IsFlat);\n+  case vmIntrinsics::_isNullRestrictedArray:    return inline_getArrayProperties(IsNullRestricted);\n+  case vmIntrinsics::_isAtomicArray:            return inline_getArrayProperties(IsAtomic);\n@@ -2323,0 +2351,1 @@\n+  bool null_free = false;\n@@ -2328,0 +2357,1 @@\n+      null_free = alias_type->field()->is_null_free();\n@@ -2334,1 +2364,1 @@\n-    if (adr_type->offset() >= objArrayOopDesc::base_offset_in_bytes()) {\n+    if (adr_type->offset() >= refArrayOopDesc::base_offset_in_bytes()) {\n@@ -2336,0 +2366,1 @@\n+      null_free = adr_type->is_aryptr()->is_null_free();\n@@ -2348,0 +2379,3 @@\n+    if (null_free) {\n+      result = result->join_speculative(TypePtr::NOTNULL)->is_oopptr();\n+    }\n@@ -2476,0 +2510,36 @@\n+\n+  if (base->is_InlineType()) {\n+    assert(!is_store, \"InlineTypeNodes are non-larval value objects\");\n+    InlineTypeNode* vt = base->as_InlineType();\n+    if (offset->is_Con()) {\n+      long off = find_long_con(offset, 0);\n+      ciInlineKlass* vk = vt->type()->inline_klass();\n+      if ((long)(int)off != off || !vk->contains_field_offset(off)) {\n+        return false;\n+      }\n+\n+      ciField* field = vk->get_non_flat_field_by_offset(off);\n+      if (field != nullptr) {\n+        BasicType bt = type2field[field->type()->basic_type()];\n+        if (bt == T_ARRAY || bt == T_NARROWOOP) {\n+          bt = T_OBJECT;\n+        }\n+        if (bt == type && !field->is_flat()) {\n+          Node* value = vt->field_value_by_offset(off, false);\n+          if (value->is_InlineType()) {\n+            value = value->as_InlineType()->adjust_scalarization_depth(this);\n+          }\n+          set_result(value);\n+          return true;\n+        }\n+      }\n+    }\n+    {\n+      \/\/ Re-execute the unsafe access if allocation triggers deoptimization.\n+      PreserveReexecuteState preexecs(this);\n+      jvms()->set_should_reexecute(true);\n+      vt = vt->buffer(this);\n+    }\n+    base = vt->get_oop();\n+  }\n+\n@@ -2519,1 +2589,29 @@\n-  BasicType bt = alias_type->basic_type();\n+  BasicType bt = T_ILLEGAL;\n+  ciField* field = nullptr;\n+  if (adr_type->isa_instptr()) {\n+    const TypeInstPtr* instptr = adr_type->is_instptr();\n+    ciInstanceKlass* k = instptr->instance_klass();\n+    int off = instptr->offset();\n+    if (instptr->const_oop() != nullptr &&\n+        k == ciEnv::current()->Class_klass() &&\n+        instptr->offset() >= (k->size_helper() * wordSize)) {\n+      k = instptr->const_oop()->as_instance()->java_lang_Class_klass()->as_instance_klass();\n+      field = k->get_field_by_offset(off, true);\n+    } else {\n+      field = k->get_non_flat_field_by_offset(off);\n+    }\n+    if (field != nullptr) {\n+      bt = type2field[field->type()->basic_type()];\n+    }\n+    if (bt != alias_type->basic_type()) {\n+      \/\/ Type mismatch. Is it an access to a nested flat field?\n+      field = k->get_field_by_offset(off, false);\n+      if (field != nullptr) {\n+        bt = type2field[field->type()->basic_type()];\n+      }\n+    }\n+    assert(bt == alias_type->basic_type(), \"should match\");\n+  } else {\n+    bt = alias_type->basic_type();\n+  }\n+\n@@ -2553,4 +2651,6 @@\n-  if (!is_store && type == T_OBJECT) {\n-    const TypeOopPtr* tjp = sharpen_unsafe_type(alias_type, adr_type);\n-    if (tjp != nullptr) {\n-      value_type = tjp;\n+  if (!is_store) {\n+    if (type == T_OBJECT) {\n+      const TypeOopPtr* tjp = sharpen_unsafe_type(alias_type, adr_type);\n+      if (tjp != nullptr) {\n+        value_type = tjp;\n+      }\n@@ -2572,2 +2672,2 @@\n-    ciField* field = alias_type->field();\n-    if (heap_base_oop != top() && field != nullptr && field->is_constant() && !mismatched) {\n+\n+    if (heap_base_oop != top() && field != nullptr && field->is_constant() && !field->is_flat() && !mismatched) {\n@@ -2580,0 +2680,5 @@\n+      const TypeOopPtr* ptr = value_type->make_oopptr();\n+      if (ptr != nullptr && ptr->is_inlinetypeptr()) {\n+        \/\/ Load a non-flattened inline type from memory\n+        p = InlineTypeNode::make_from_oop(this, p, ptr->inline_klass());\n+      }\n@@ -2623,0 +2728,235 @@\n+bool LibraryCallKit::inline_unsafe_flat_access(bool is_store, AccessKind kind) {\n+#ifdef ASSERT\n+  {\n+    ResourceMark rm;\n+    \/\/ Check the signatures.\n+    ciSignature* sig = callee()->signature();\n+    assert(sig->type_at(0)->basic_type() == T_OBJECT, \"base should be object, but is %s\", type2name(sig->type_at(0)->basic_type()));\n+    assert(sig->type_at(1)->basic_type() == T_LONG, \"offset should be long, but is %s\", type2name(sig->type_at(1)->basic_type()));\n+    assert(sig->type_at(2)->basic_type() == T_INT, \"layout kind should be int, but is %s\", type2name(sig->type_at(3)->basic_type()));\n+    assert(sig->type_at(3)->basic_type() == T_OBJECT, \"value klass should be object, but is %s\", type2name(sig->type_at(4)->basic_type()));\n+    if (is_store) {\n+      assert(sig->return_type()->basic_type() == T_VOID, \"putter must not return a value, but returns %s\", type2name(sig->return_type()->basic_type()));\n+      assert(sig->count() == 5, \"flat putter should have 5 arguments, but has %d\", sig->count());\n+      assert(sig->type_at(4)->basic_type() == T_OBJECT, \"put value should be object, but is %s\", type2name(sig->type_at(5)->basic_type()));\n+    } else {\n+      assert(sig->return_type()->basic_type() == T_OBJECT, \"getter must return an object, but returns %s\", type2name(sig->return_type()->basic_type()));\n+      assert(sig->count() == 4, \"flat getter should have 4 arguments, but has %d\", sig->count());\n+    }\n+ }\n+#endif \/\/ ASSERT\n+\n+  assert(kind == Relaxed, \"Only plain accesses for now\");\n+  if (callee()->is_static()) {\n+    \/\/ caller must have the capability!\n+    return false;\n+  }\n+  C->set_has_unsafe_access(true);\n+\n+  const TypeInstPtr* value_klass_node = _gvn.type(argument(5))->isa_instptr();\n+  if (value_klass_node == nullptr || value_klass_node->const_oop() == nullptr) {\n+    \/\/ parameter valueType is not a constant\n+    return false;\n+  }\n+  ciType* mirror_type = value_klass_node->const_oop()->as_instance()->java_mirror_type();\n+  if (!mirror_type->is_inlinetype()) {\n+    \/\/ Dead code\n+    return false;\n+  }\n+  ciInlineKlass* value_klass = mirror_type->as_inline_klass();\n+\n+  const TypeInt* layout_type = _gvn.type(argument(4))->isa_int();\n+  if (layout_type == nullptr || !layout_type->is_con()) {\n+    \/\/ parameter layoutKind is not a constant\n+    return false;\n+  }\n+  assert(layout_type->get_con() >= static_cast<int>(LayoutKind::REFERENCE) &&\n+         layout_type->get_con() <= static_cast<int>(LayoutKind::UNKNOWN),\n+         \"invalid layoutKind %d\", layout_type->get_con());\n+  LayoutKind layout = static_cast<LayoutKind>(layout_type->get_con());\n+  assert(layout == LayoutKind::REFERENCE || layout == LayoutKind::NULL_FREE_NON_ATOMIC_FLAT ||\n+         layout == LayoutKind::NULL_FREE_ATOMIC_FLAT || layout == LayoutKind::NULLABLE_ATOMIC_FLAT,\n+         \"unexpected layoutKind %d\", layout_type->get_con());\n+\n+  null_check(argument(0));\n+  if (stopped()) {\n+    return true;\n+  }\n+\n+  Node* base = must_be_not_null(argument(1), true);\n+  Node* offset = argument(2);\n+  const Type* base_type = _gvn.type(base);\n+\n+  Node* ptr;\n+  bool immutable_memory = false;\n+  DecoratorSet decorators = C2_UNSAFE_ACCESS | IN_HEAP | MO_UNORDERED;\n+  if (base_type->isa_instptr()) {\n+    const TypeLong* offset_type = _gvn.type(offset)->isa_long();\n+    if (offset_type == nullptr || !offset_type->is_con()) {\n+      \/\/ Offset into a non-array should be a constant\n+      decorators |= C2_MISMATCHED;\n+    } else {\n+      int offset_con = checked_cast<int>(offset_type->get_con());\n+      ciInstanceKlass* base_klass = base_type->is_instptr()->instance_klass();\n+      ciField* field = base_klass->get_non_flat_field_by_offset(offset_con);\n+      if (field == nullptr) {\n+        assert(!base_klass->is_final(), \"non-existence field at offset %d of class %s\", offset_con, base_klass->name()->as_utf8());\n+        decorators |= C2_MISMATCHED;\n+      } else {\n+        assert(field->type() == value_klass, \"field at offset %d of %s is of type %s, but valueType is %s\",\n+               offset_con, base_klass->name()->as_utf8(), field->type()->name(), value_klass->name()->as_utf8());\n+        immutable_memory = field->is_strict() && field->is_final();\n+\n+        if (base->is_InlineType()) {\n+          assert(!is_store, \"Cannot store into a non-larval value object\");\n+          set_result(base->as_InlineType()->field_value_by_offset(offset_con, false));\n+          return true;\n+        }\n+      }\n+    }\n+\n+    if (base->is_InlineType()) {\n+      assert(!is_store, \"Cannot store into a non-larval value object\");\n+      base = base->as_InlineType()->buffer(this, true);\n+    }\n+    ptr = basic_plus_adr(base, ConvL2X(offset));\n+  } else if (base_type->isa_aryptr()) {\n+    decorators |= IS_ARRAY;\n+    if (layout == LayoutKind::REFERENCE) {\n+      if (!base_type->is_aryptr()->is_not_flat()) {\n+        const TypeAryPtr* array_type = base_type->is_aryptr()->cast_to_not_flat();\n+        Node* new_base = _gvn.transform(new CastPPNode(control(), base, array_type, ConstraintCastNode::DependencyType::NonFloatingNarrowing));\n+        replace_in_map(base, new_base);\n+        base = new_base;\n+      }\n+      ptr = basic_plus_adr(base, ConvL2X(offset));\n+    } else {\n+      if (UseArrayFlattening) {\n+        \/\/ Flat array must have an exact type\n+        bool is_null_free = !LayoutKindHelper::is_nullable_flat(layout);\n+        bool is_atomic = LayoutKindHelper::is_atomic_flat(layout);\n+        Node* new_base = cast_to_flat_array_exact(base, value_klass, is_null_free, is_atomic);\n+        replace_in_map(base, new_base);\n+        base = new_base;\n+        ptr = basic_plus_adr(base, ConvL2X(offset));\n+        const TypeAryPtr* ptr_type = _gvn.type(ptr)->is_aryptr();\n+        if (ptr_type->field_offset().get() != 0) {\n+          ptr = _gvn.transform(new CastPPNode(control(), ptr, ptr_type->with_field_offset(0), ConstraintCastNode::DependencyType::NonFloatingNarrowing));\n+        }\n+      } else {\n+        uncommon_trap(Deoptimization::Reason_intrinsic,\n+                      Deoptimization::Action_none);\n+        return true;\n+      }\n+    }\n+  } else {\n+    decorators |= C2_MISMATCHED;\n+    ptr = basic_plus_adr(base, ConvL2X(offset));\n+  }\n+\n+  if (is_store) {\n+    Node* value = argument(6);\n+    const Type* value_type = _gvn.type(value);\n+    if (!value_type->is_inlinetypeptr()) {\n+      value_type = Type::get_const_type(value_klass)->filter_speculative(value_type);\n+      Node* new_value = _gvn.transform(new CastPPNode(control(), value, value_type, ConstraintCastNode::DependencyType::NonFloatingNarrowing));\n+      new_value = InlineTypeNode::make_from_oop(this, new_value, value_klass);\n+      replace_in_map(value, new_value);\n+      value = new_value;\n+    }\n+\n+    assert(value_type->inline_klass() == value_klass, \"value is of type %s while valueType is %s\", value_type->inline_klass()->name()->as_utf8(), value_klass->name()->as_utf8());\n+    if (layout == LayoutKind::REFERENCE) {\n+      const TypePtr* ptr_type = (decorators & C2_MISMATCHED) != 0 ? TypeRawPtr::BOTTOM : _gvn.type(ptr)->is_ptr();\n+      access_store_at(base, ptr, ptr_type, value, value_type, T_OBJECT, decorators);\n+    } else {\n+      bool atomic = LayoutKindHelper::is_atomic_flat(layout);\n+      bool null_free = !LayoutKindHelper::is_nullable_flat(layout);\n+      value->as_InlineType()->store_flat(this, base, ptr, atomic, immutable_memory, null_free, decorators);\n+    }\n+\n+    return true;\n+  } else {\n+    decorators |= (C2_CONTROL_DEPENDENT_LOAD | C2_UNKNOWN_CONTROL_LOAD);\n+    InlineTypeNode* result;\n+    if (layout == LayoutKind::REFERENCE) {\n+      const TypePtr* ptr_type = (decorators & C2_MISMATCHED) != 0 ? TypeRawPtr::BOTTOM : _gvn.type(ptr)->is_ptr();\n+      Node* oop = access_load_at(base, ptr, ptr_type, Type::get_const_type(value_klass), T_OBJECT, decorators);\n+      result = InlineTypeNode::make_from_oop(this, oop, value_klass);\n+    } else {\n+      bool atomic = LayoutKindHelper::is_atomic_flat(layout);\n+      bool null_free = !LayoutKindHelper::is_nullable_flat(layout);\n+      result = InlineTypeNode::make_from_flat(this, value_klass, base, ptr, atomic, immutable_memory, null_free, decorators);\n+    }\n+\n+    set_result(result);\n+    return true;\n+  }\n+}\n+\n+bool LibraryCallKit::inline_unsafe_make_private_buffer() {\n+  Node* receiver = argument(0);\n+  Node* value = argument(1);\n+\n+  const Type* type = gvn().type(value);\n+  if (!type->is_inlinetypeptr()) {\n+    C->record_method_not_compilable(\"value passed to Unsafe::makePrivateBuffer is not of a constant value type\");\n+    return false;\n+  }\n+\n+  null_check(receiver);\n+  if (stopped()) {\n+    return true;\n+  }\n+\n+  value = null_check(value);\n+  if (stopped()) {\n+    return true;\n+  }\n+\n+  ciInlineKlass* vk = type->inline_klass();\n+  Node* klass = makecon(TypeKlassPtr::make(vk));\n+  Node* obj = new_instance(klass);\n+  AllocateNode::Ideal_allocation(obj)->_larval = true;\n+\n+  assert(value->is_InlineType(), \"must be an InlineTypeNode\");\n+  Node* payload_ptr = basic_plus_adr(obj, vk->payload_offset());\n+  value->as_InlineType()->store_flat(this, obj, payload_ptr, false, true, true, IN_HEAP | MO_UNORDERED);\n+\n+  set_result(obj);\n+  return true;\n+}\n+\n+bool LibraryCallKit::inline_unsafe_finish_private_buffer() {\n+  Node* receiver = argument(0);\n+  Node* buffer = argument(1);\n+\n+  const Type* type = gvn().type(buffer);\n+  if (!type->is_inlinetypeptr()) {\n+    C->record_method_not_compilable(\"value passed to Unsafe::finishPrivateBuffer is not of a constant value type\");\n+    return false;\n+  }\n+\n+  AllocateNode* alloc = AllocateNode::Ideal_allocation(buffer);\n+  if (alloc == nullptr) {\n+    C->record_method_not_compilable(\"value passed to Unsafe::finishPrivateBuffer must be allocated by Unsafe::makePrivateBuffer\");\n+    return false;\n+  }\n+\n+  null_check(receiver);\n+  if (stopped()) {\n+    return true;\n+  }\n+\n+  \/\/ Unset the larval bit in the object header\n+  Node* old_header = make_load(control(), buffer, TypeX_X, TypeX_X->basic_type(), MemNode::unordered, LoadNode::Pinned);\n+  Node* new_header = gvn().transform(new AndXNode(old_header, MakeConX(~markWord::larval_bit_in_place)));\n+  access_store_at(buffer, buffer, type->is_ptr(), new_header, TypeX_X, TypeX_X->basic_type(), MO_UNORDERED | IN_HEAP);\n+\n+  \/\/ We must ensure that the buffer is properly published\n+  insert_mem_bar(Op_MemBarStoreStore, alloc->proj_out(AllocateNode::RawAddress));\n+  assert(!type->maybe_null(), \"result of an allocation should not be null\");\n+  set_result(InlineTypeNode::make_from_oop(this, buffer, type->inline_klass()));\n+  return true;\n+}\n+\n@@ -2825,0 +3165,13 @@\n+    if (oldval != nullptr && oldval->is_InlineType()) {\n+      \/\/ Re-execute the unsafe access if allocation triggers deoptimization.\n+      PreserveReexecuteState preexecs(this);\n+      jvms()->set_should_reexecute(true);\n+      oldval = oldval->as_InlineType()->buffer(this)->get_oop();\n+    }\n+    if (newval != nullptr && newval->is_InlineType()) {\n+      \/\/ Re-execute the unsafe access if allocation triggers deoptimization.\n+      PreserveReexecuteState preexecs(this);\n+      jvms()->set_should_reexecute(true);\n+      newval = newval->as_InlineType()->buffer(this)->get_oop();\n+    }\n+\n@@ -2893,0 +3246,98 @@\n+\/\/ private native int arrayInstanceBaseOffset0(Object[] array);\n+bool LibraryCallKit::inline_arrayInstanceBaseOffset() {\n+  Node* array = argument(1);\n+  Node* klass_node = load_object_klass(array);\n+\n+  jint  layout_con = Klass::_lh_neutral_value;\n+  Node* layout_val = get_layout_helper(klass_node, layout_con);\n+  int   layout_is_con = (layout_val == nullptr);\n+\n+  Node* header_size = nullptr;\n+  if (layout_is_con) {\n+    int hsize = Klass::layout_helper_header_size(layout_con);\n+    header_size = intcon(hsize);\n+  } else {\n+    Node* hss = intcon(Klass::_lh_header_size_shift);\n+    Node* hsm = intcon(Klass::_lh_header_size_mask);\n+    header_size = _gvn.transform(new URShiftINode(layout_val, hss));\n+    header_size = _gvn.transform(new AndINode(header_size, hsm));\n+  }\n+  set_result(header_size);\n+  return true;\n+}\n+\n+\/\/ private native int arrayInstanceIndexScale0(Object[] array);\n+bool LibraryCallKit::inline_arrayInstanceIndexScale() {\n+  Node* array = argument(1);\n+  Node* klass_node = load_object_klass(array);\n+\n+  jint  layout_con = Klass::_lh_neutral_value;\n+  Node* layout_val = get_layout_helper(klass_node, layout_con);\n+  int   layout_is_con = (layout_val == nullptr);\n+\n+  Node* element_size = nullptr;\n+  if (layout_is_con) {\n+    int log_element_size  = Klass::layout_helper_log2_element_size(layout_con);\n+    int elem_size = 1 << log_element_size;\n+    element_size = intcon(elem_size);\n+  } else {\n+    Node* ess = intcon(Klass::_lh_log2_element_size_shift);\n+    Node* esm = intcon(Klass::_lh_log2_element_size_mask);\n+    Node* log_element_size = _gvn.transform(new URShiftINode(layout_val, ess));\n+    log_element_size = _gvn.transform(new AndINode(log_element_size, esm));\n+    element_size = _gvn.transform(new LShiftINode(intcon(1), log_element_size));\n+  }\n+  set_result(element_size);\n+  return true;\n+}\n+\n+\/\/ private native int arrayLayout0(Object[] array);\n+bool LibraryCallKit::inline_arrayLayout() {\n+  RegionNode* region = new RegionNode(2);\n+  Node* phi = new PhiNode(region, TypeInt::POS);\n+\n+  Node* array = argument(1);\n+  Node* klass_node = load_object_klass(array);\n+  generate_refArray_guard(klass_node, region);\n+  if (region->req() == 3) {\n+    phi->add_req(intcon((jint)LayoutKind::REFERENCE));\n+  }\n+\n+  int layout_kind_offset = in_bytes(FlatArrayKlass::layout_kind_offset());\n+  Node* layout_kind_addr = basic_plus_adr(klass_node, layout_kind_offset);\n+  Node* layout_kind = make_load(nullptr, layout_kind_addr, TypeInt::POS, T_INT, MemNode::unordered);\n+\n+  region->init_req(1, control());\n+  phi->init_req(1, layout_kind);\n+\n+  set_control(_gvn.transform(region));\n+  set_result(_gvn.transform(phi));\n+  return true;\n+}\n+\n+\/\/ private native int[] getFieldMap0(Class <?> c);\n+\/\/   int offset = c._klass._acmp_maps_offset;\n+\/\/   return (int[])c.obj_field(offset);\n+bool LibraryCallKit::inline_getFieldMap() {\n+  if (!UseAltSubstitutabilityMethod) {\n+    return false;\n+  }\n+\n+  Node* mirror = argument(1);\n+  Node* klass = load_klass_from_mirror(mirror, false, nullptr, 0);\n+\n+  int field_map_offset_offset = in_bytes(InstanceKlass::acmp_maps_offset_offset());\n+  Node* field_map_offset_addr = basic_plus_adr(klass, field_map_offset_offset);\n+  Node* field_map_offset = make_load(nullptr, field_map_offset_addr, TypeInt::INT, T_INT, MemNode::unordered);\n+  field_map_offset = _gvn.transform(ConvI2L(field_map_offset));\n+\n+  Node* map_addr = basic_plus_adr(mirror, field_map_offset);\n+  const TypeAryPtr* val_type = TypeAryPtr::INTS->cast_to_ptr_type(TypePtr::NotNull)->with_offset(0);\n+  \/\/ TODO 8350865 Remove this\n+  val_type = val_type->cast_to_not_flat(true)->cast_to_not_null_free(true);\n+  Node* map = access_load_at(mirror, map_addr, TypeAryPtr::INTS, val_type, T_ARRAY, IN_HEAP | MO_UNORDERED);\n+\n+  set_result(map);\n+  return true;\n+}\n+\n@@ -3011,2 +3462,7 @@\n-\n-  Node* obj = new_instance(kls, test);\n+  Node* obj = nullptr;\n+  const TypeInstKlassPtr* tkls = _gvn.type(kls)->isa_instklassptr();\n+  if (tkls != nullptr && tkls->instance_klass()->is_inlinetype()) {\n+    obj = InlineTypeNode::make_all_zero(_gvn, tkls->instance_klass()->as_inline_klass())->buffer(this);\n+  } else {\n+    obj = new_instance(kls, test);\n+  }\n@@ -3827,1 +4283,1 @@\n-  const TypeAry* arr0 = TypeAry::make(etype, TypeInt::POS);\n+  const TypeAry* arr0 = TypeAry::make(etype, TypeInt::POS, \/* stable= *\/ false, \/* flat= *\/ false, \/* not_flat= *\/ true, \/* not_null_free= *\/ true, true);\n@@ -3832,1 +4288,1 @@\n-  const Type* objects_type = TypeAryPtr::make(TypePtr::BotPTR, arr0, objects_klass, xk, 0);\n+  const Type* objects_type = TypeAryPtr::make(TypePtr::BotPTR, arr0, objects_klass, xk, TypeAryPtr::Offset(0));\n@@ -4008,0 +4464,1 @@\n+\n@@ -4133,10 +4590,12 @@\n-    p = basic_plus_adr(kls, in_bytes(Klass::super_offset()));\n-    kls = _gvn.transform(LoadKlassNode::make(_gvn, immutable_memory(), p, TypeRawPtr::BOTTOM, TypeInstKlassPtr::OBJECT_OR_NULL));\n-    null_ctl = top();\n-    kls = null_check_oop(kls, &null_ctl);\n-    if (null_ctl != top()) {\n-      \/\/ If the guard is taken, Object.superClass is null (both klass and mirror).\n-      region->add_req(null_ctl);\n-      phi   ->add_req(null());\n-    }\n-      query_value = load_mirror_from_klass(kls);\n+      p = basic_plus_adr(kls, in_bytes(Klass::super_offset()));\n+      kls = _gvn.transform(LoadKlassNode::make(_gvn, immutable_memory(), p, TypeRawPtr::BOTTOM, TypeInstKlassPtr::OBJECT_OR_NULL));\n+      null_ctl = top();\n+      kls = null_check_oop(kls, &null_ctl);\n+      if (null_ctl != top()) {\n+        \/\/ If the guard is taken, Object.superClass is null (both klass and mirror).\n+        region->add_req(null_ctl);\n+        phi   ->add_req(null());\n+      }\n+      if (!stopped()) {\n+        query_value = load_mirror_from_klass(kls);\n+      }\n@@ -4161,0 +4620,1 @@\n+\n@@ -4183,1 +4643,2 @@\n-      int static_res = C->static_subtype_check(TypeKlassPtr::make(tm->as_klass(), Type::trust_interfaces), tp->as_klass_type());\n+      const TypeKlassPtr* tklass = TypeKlassPtr::make(tm->as_klass(), Type::trust_interfaces);\n+      int static_res = C->static_subtype_check(tklass, tp->as_klass_type());\n@@ -4212,2 +4673,2 @@\n-  \/\/ Not-subtype or the mirror's klass ptr is null (in case it is a primitive).\n-  enum { _bad_type_path = 1, _prim_path = 2, PATH_LIMIT };\n+  \/\/ Not-subtype or the mirror's klass ptr is nullptr (in case it is a primitive).\n+  enum { _bad_type_path = 1, _prim_path = 2, _npe_path = 3, PATH_LIMIT };\n@@ -4223,0 +4684,2 @@\n+  Node* io = i_o();\n+  Node* mem = merged_memory();\n@@ -4224,0 +4687,1 @@\n+\n@@ -4230,1 +4694,2 @@\n-      region->in(_bad_type_path) != top()) {\n+      region->in(_bad_type_path) != top() ||\n+      region->in(_npe_path) != top()) {\n@@ -4234,0 +4699,3 @@\n+    \/\/ Set IO and memory because gen_checkcast may override them when buffering inline types\n+    set_i_o(io);\n+    set_all_memory(mem);\n@@ -4267,0 +4735,1 @@\n+  RegionNode* prim_region = new RegionNode(2);\n@@ -4269,0 +4738,1 @@\n+  record_for_igvn(prim_region);\n@@ -4293,2 +4763,5 @@\n-    int prim_path = (which_arg == 0 ? _prim_0_path : _prim_1_path);\n-    region->init_req(prim_path, null_ctl);\n+    if (which_arg == 0) {\n+      prim_region->init_req(1, null_ctl);\n+    } else {\n+      region->init_req(_prim_1_path, null_ctl);\n+    }\n@@ -4304,1 +4777,0 @@\n-    \/\/ now we have a successful reference subtype check\n@@ -4311,1 +4783,2 @@\n-  set_control(region->in(_prim_0_path)); \/\/ go back to first null check\n+  \/\/ This path is also used if superc is a value mirror.\n+  set_control(_gvn.transform(prim_region));\n@@ -4316,1 +4789,1 @@\n-    generate_guard(bol_eq, region, PROB_FAIR);\n+    generate_fair_guard(bol_eq, region);\n@@ -4347,2 +4820,1 @@\n-Node* LibraryCallKit::generate_array_guard_common(Node* kls, RegionNode* region,\n-                                                  bool obj_array, bool not_array, Node** obj) {\n+Node* LibraryCallKit::generate_array_guard_common(Node* kls, RegionNode* region, ArrayKind kind, Node** obj) {\n@@ -4354,9 +4826,0 @@\n-  \/\/ If obj_array\/non_array==false\/false:\n-  \/\/ Branch around if the given klass is in fact an array (either obj or prim).\n-  \/\/ If obj_array\/non_array==false\/true:\n-  \/\/ Branch around if the given klass is not an array klass of any kind.\n-  \/\/ If obj_array\/non_array==true\/true:\n-  \/\/ Branch around if the kls is not an oop array (kls is int[], String, etc.)\n-  \/\/ If obj_array\/non_array==true\/false:\n-  \/\/ Branch around if the kls is an oop array (Object[] or subtype)\n-  \/\/\n@@ -4367,4 +4830,11 @@\n-    bool query = (obj_array\n-                  ? Klass::layout_helper_is_objArray(layout_con)\n-                  : Klass::layout_helper_is_array(layout_con));\n-    if (query == not_array) {\n+    bool query = 0;\n+    switch(kind) {\n+      case RefArray:       query = Klass::layout_helper_is_refArray(layout_con); break;\n+      case NonRefArray:    query = !Klass::layout_helper_is_refArray(layout_con); break;\n+      case TypeArray:      query = Klass::layout_helper_is_typeArray(layout_con); break;\n+      case AnyArray:       query = Klass::layout_helper_is_array(layout_con); break;\n+      case NonArray:       query = !Klass::layout_helper_is_array(layout_con); break;\n+      default:\n+        ShouldNotReachHere();\n+    }\n+    if (!query) {\n@@ -4380,0 +4850,21 @@\n+  unsigned int value = 0;\n+  BoolTest::mask btest = BoolTest::illegal;\n+  switch(kind) {\n+    case RefArray:\n+    case NonRefArray: {\n+      value = Klass::_lh_array_tag_ref_value;\n+      layout_val = _gvn.transform(new RShiftINode(layout_val, intcon(Klass::_lh_array_tag_shift)));\n+      btest = (kind == RefArray) ? BoolTest::eq : BoolTest::ne;\n+      break;\n+    }\n+    case TypeArray: {\n+      value = Klass::_lh_array_tag_type_value;\n+      layout_val = _gvn.transform(new RShiftINode(layout_val, intcon(Klass::_lh_array_tag_shift)));\n+      btest = BoolTest::eq;\n+      break;\n+    }\n+    case AnyArray:    value = Klass::_lh_neutral_value; btest = BoolTest::lt; break;\n+    case NonArray:    value = Klass::_lh_neutral_value; btest = BoolTest::gt; break;\n+    default:\n+      ShouldNotReachHere();\n+  }\n@@ -4381,4 +4872,1 @@\n-  jint  nval = (obj_array\n-                ? (jint)(Klass::_lh_array_tag_type_value\n-                   <<    Klass::_lh_array_tag_shift)\n-                : Klass::_lh_neutral_value);\n+  jint nval = (jint)value;\n@@ -4386,3 +4874,0 @@\n-  BoolTest::mask btest = BoolTest::lt;  \/\/ correct for testing is_[obj]array\n-  \/\/ invert the test if we are looking for a non-array\n-  if (not_array)  btest = BoolTest(btest).negate();\n@@ -4391,1 +4876,1 @@\n-  Node* is_array_ctrl = not_array ? control() : ctrl;\n+  Node* is_array_ctrl = kind == NonArray ? control() : ctrl;\n@@ -4400,0 +4885,132 @@\n+\/\/ public static native Object[] ValueClass::newNullRestrictedAtomicArray(Class<?> componentType, int length, Object initVal);\n+\/\/ public static native Object[] ValueClass::newNullRestrictedNonAtomicArray(Class<?> componentType, int length, Object initVal);\n+\/\/ public static native Object[] ValueClass::newNullableAtomicArray(Class<?> componentType, int length);\n+bool LibraryCallKit::inline_newArray(bool null_free, bool atomic) {\n+  assert(null_free || atomic, \"nullable implies atomic\");\n+  Node* componentType = argument(0);\n+  Node* length = argument(1);\n+  Node* init_val = null_free ? argument(2) : nullptr;\n+\n+  const TypeInstPtr* tp = _gvn.type(componentType)->isa_instptr();\n+  if (tp != nullptr) {\n+    ciInstanceKlass* ik = tp->instance_klass();\n+    if (ik == C->env()->Class_klass()) {\n+      ciType* t = tp->java_mirror_type();\n+      if (t != nullptr && t->is_inlinetype()) {\n+\n+        ciArrayKlass* array_klass = ciArrayKlass::make(t, null_free, atomic, true);\n+        assert(array_klass->is_elem_null_free() == null_free, \"inconsistency\");\n+\n+        \/\/ TOOD 8350865 ZGC needs card marks on initializing oop stores\n+        if (UseZGC && null_free && !array_klass->is_flat_array_klass()) {\n+          return false;\n+        }\n+\n+        if (array_klass->is_loaded() && array_klass->element_klass()->as_inline_klass()->is_initialized()) {\n+          const TypeAryKlassPtr* array_klass_type = TypeAryKlassPtr::make(array_klass, Type::trust_interfaces);\n+          if (null_free) {\n+            if (init_val->is_InlineType()) {\n+              if (array_klass_type->is_flat() && init_val->as_InlineType()->is_all_zero(&gvn(), \/* flat *\/ true)) {\n+                \/\/ Zeroing is enough because the init value is the all-zero value\n+                init_val = nullptr;\n+              } else {\n+                init_val = init_val->as_InlineType()->buffer(this);\n+              }\n+            }\n+            \/\/ TODO 8350865 Should we add a check of the init_val type (maybe in debug only + halt)?\n+            \/\/ If we insert a checkcast here, we can be sure that init_val is an InlineTypeNode, so\n+            \/\/ when we folded a field load from an allocation (e.g. during escape analysis), we can\n+            \/\/ remove the check init_val->is_InlineType().\n+          }\n+          Node* obj = new_array(makecon(array_klass_type), length, 0, nullptr, false, init_val);\n+          const TypeAryPtr* arytype = gvn().type(obj)->is_aryptr();\n+          assert(arytype->is_null_free() == null_free, \"inconsistency\");\n+          assert(arytype->is_not_null_free() == !null_free, \"inconsistency\");\n+          set_result(obj);\n+          return true;\n+        }\n+      }\n+    }\n+  }\n+  return false;\n+}\n+\n+\/\/ public static native boolean ValueClass::isFlatArray(Object array);\n+\/\/ public static native boolean ValueClass::isNullRestrictedArray(Object array);\n+\/\/ public static native boolean ValueClass::isAtomicArray(Object array);\n+bool LibraryCallKit::inline_getArrayProperties(ArrayPropertiesCheck check) {\n+  Node* array = argument(0);\n+\n+  Node* bol;\n+  switch(check) {\n+    case IsFlat:\n+      \/\/ TODO 8350865 Use the object version here instead of loading the klass\n+      \/\/ The problem is that PhaseMacroExpand::expand_flatarraycheck_node can only handle some IR shapes and will fail, for example, if the bol is directly wired to a ReturnNode\n+      bol = flat_array_test(load_object_klass(array));\n+      break;\n+    case IsNullRestricted:\n+      bol = null_free_array_test(array);\n+      break;\n+    case IsAtomic:\n+      \/\/ TODO 8350865 Implement this. It's a bit more complicated, see conditions in JVM_IsAtomicArray\n+      \/\/ Enable TestIntrinsics::test87\/88 once this is implemented\n+      \/\/ bol = null_free_atomic_array_test\n+      return false;\n+    default:\n+      ShouldNotReachHere();\n+  }\n+\n+  Node* res = gvn().transform(new CMoveINode(bol, intcon(0), intcon(1), TypeInt::BOOL));\n+  set_result(res);\n+  return true;\n+}\n+\n+\/\/ Load the default refined array klass from an ObjArrayKlass. This relies on the first entry in the\n+\/\/ '_next_refined_array_klass' linked list being the default (see ObjArrayKlass::klass_with_properties).\n+Node* LibraryCallKit::load_default_refined_array_klass(Node* klass_node, bool type_array_guard) {\n+  RegionNode* region = new RegionNode(2);\n+  Node* phi = new PhiNode(region, TypeInstKlassPtr::OBJECT_OR_NULL);\n+\n+  if (type_array_guard) {\n+    generate_typeArray_guard(klass_node, region);\n+    if (region->req() == 3) {\n+      phi->add_req(klass_node);\n+    }\n+  }\n+  Node* adr_refined_klass = basic_plus_adr(klass_node, in_bytes(ObjArrayKlass::next_refined_array_klass_offset()));\n+  Node* refined_klass = _gvn.transform(LoadKlassNode::make(_gvn, immutable_memory(), adr_refined_klass, TypeRawPtr::BOTTOM, TypeInstKlassPtr::OBJECT_OR_NULL));\n+\n+  \/\/ Can be null if not initialized yet, just deopt\n+  Node* null_ctl = top();\n+  refined_klass = null_check_oop(refined_klass, &null_ctl, \/* never_see_null= *\/ true);\n+\n+  region->init_req(1, control());\n+  phi->init_req(1, refined_klass);\n+\n+  set_control(_gvn.transform(region));\n+  return _gvn.transform(phi);\n+}\n+\n+\/\/ Load the non-refined array klass from an ObjArrayKlass.\n+Node* LibraryCallKit::load_non_refined_array_klass(Node* klass_node) {\n+  const TypeAryKlassPtr* ary_klass_ptr = _gvn.type(klass_node)->isa_aryklassptr();\n+  if (ary_klass_ptr != nullptr && ary_klass_ptr->klass_is_exact()) {\n+    return _gvn.makecon(ary_klass_ptr->cast_to_refined_array_klass_ptr(false));\n+  }\n+\n+  RegionNode* region = new RegionNode(2);\n+  Node* phi = new PhiNode(region, TypeInstKlassPtr::OBJECT);\n+\n+  generate_typeArray_guard(klass_node, region);\n+  if (region->req() == 3) {\n+    phi->add_req(klass_node);\n+  }\n+  Node* super_adr = basic_plus_adr(klass_node, in_bytes(Klass::super_offset()));\n+  Node* super_klass = _gvn.transform(LoadKlassNode::make(_gvn, immutable_memory(), super_adr, TypeRawPtr::BOTTOM, TypeInstKlassPtr::OBJECT));\n+\n+  region->init_req(1, control());\n+  phi->init_req(1, super_klass);\n+\n+  set_control(_gvn.transform(region));\n+  return _gvn.transform(phi);\n+}\n@@ -4402,1 +5019,1 @@\n-\/\/ private static native Object java.lang.reflect.newArray(Class<?> componentType, int length);\n+\/\/ private static native Object java.lang.reflect.Array.newArray(Class<?> componentType, int length);\n@@ -4460,0 +5077,3 @@\n+\n+    klass_node = load_default_refined_array_klass(klass_node);\n+\n@@ -4548,1 +5168,16 @@\n-    Node* not_objArray = generate_non_objArray_guard(klass_node, bailout);\n+    \/\/ Inline type array may have object field that would require a\n+    \/\/ write barrier. Conservatively, go to slow path.\n+    \/\/ TODO 8251971: Optimize for the case when flat src\/dst are later found\n+    \/\/ to not contain oops (i.e., move this check to the macro expansion phase).\n+    BarrierSetC2* bs = BarrierSet::barrier_set()->barrier_set_c2();\n+    const TypeAryPtr* orig_t = _gvn.type(original)->isa_aryptr();\n+    const TypeKlassPtr* tklass = _gvn.type(klass_node)->is_klassptr();\n+    bool exclude_flat = UseArrayFlattening && bs->array_copy_requires_gc_barriers(true, T_OBJECT, false, false, BarrierSetC2::Parsing) &&\n+                        \/\/ Can src array be flat and contain oops?\n+                        (orig_t == nullptr || (!orig_t->is_not_flat() && (!orig_t->is_flat() || orig_t->elem()->inline_klass()->contains_oops()))) &&\n+                        \/\/ Can dest array be flat and contain oops?\n+                        tklass->can_be_inline_array() && (!tklass->is_flat() || tklass->is_aryklassptr()->elem()->is_instklassptr()->instance_klass()->as_inline_klass()->contains_oops());\n+    Node* not_objArray = exclude_flat ? generate_non_refArray_guard(klass_node, bailout) : generate_typeArray_guard(klass_node, bailout);\n+\n+    Node* refined_klass_node = load_default_refined_array_klass(klass_node, \/* type_array_guard= *\/ false);\n+\n@@ -4552,3 +5187,5 @@\n-      const Type* akls = TypeKlassPtr::make(TypePtr::NotNull, ak, 0\/*offset*\/);\n-      Node* cast = new CastPPNode(control(), klass_node, akls);\n-      klass_node = _gvn.transform(cast);\n+      bool not_flat = !UseArrayFlattening;\n+      bool not_null_free = !Arguments::is_valhalla_enabled();\n+      const Type* akls = TypeAryKlassPtr::make(TypePtr::NotNull, ak, Type::Offset(0), Type::trust_interfaces, not_flat, not_null_free, false, false, not_flat, true);\n+      Node* cast = new CastPPNode(control(), refined_klass_node, akls);\n+      refined_klass_node = _gvn.transform(cast);\n@@ -4572,0 +5209,39 @@\n+    \/\/ Handle inline type arrays\n+    bool can_validate = !too_many_traps(Deoptimization::Reason_class_check);\n+    if (!stopped()) {\n+      \/\/ TODO 8251971\n+      if (!orig_t->is_null_free()) {\n+        \/\/ Not statically known to be null free, add a check\n+        generate_fair_guard(null_free_array_test(original), bailout);\n+      }\n+      orig_t = _gvn.type(original)->isa_aryptr();\n+      if (orig_t != nullptr && orig_t->is_flat()) {\n+        \/\/ Src is flat, check that dest is flat as well\n+        if (exclude_flat) {\n+          \/\/ Dest can't be flat, bail out\n+          bailout->add_req(control());\n+          set_control(top());\n+        } else {\n+          generate_fair_guard(flat_array_test(refined_klass_node, \/* flat = *\/ false), bailout);\n+        }\n+        \/\/ TODO 8350865 This is not correct anymore. Write tests and fix logic similar to arraycopy.\n+      } else if (UseArrayFlattening && (orig_t == nullptr || !orig_t->is_not_flat()) &&\n+                 \/\/ If dest is flat, src must be flat as well (guaranteed by src <: dest check if validated).\n+                 ((!tklass->is_flat() && tklass->can_be_inline_array()) || !can_validate)) {\n+        \/\/ Src might be flat and dest might not be flat. Go to the slow path if src is flat.\n+        \/\/ TODO 8251971: Optimize for the case when src\/dest are later found to be both flat.\n+        generate_fair_guard(flat_array_test(load_object_klass(original)), bailout);\n+        if (orig_t != nullptr) {\n+          orig_t = orig_t->cast_to_not_flat();\n+          original = _gvn.transform(new CheckCastPPNode(control(), original, orig_t));\n+        }\n+      }\n+      if (!can_validate) {\n+        \/\/ No validation. The subtype check emitted at macro expansion time will not go to the slow\n+        \/\/ path but call checkcast_arraycopy which can not handle flat\/null-free inline type arrays.\n+        \/\/ TODO 8251971: Optimize for the case when src\/dest are later found to be both flat\/null-free.\n+        generate_fair_guard(flat_array_test(refined_klass_node), bailout);\n+        generate_fair_guard(null_free_array_test(original), bailout);\n+      }\n+    }\n+\n@@ -4617,1 +5293,1 @@\n-      if (!too_many_traps(Deoptimization::Reason_class_check)) {\n+      if (can_validate) {\n@@ -4631,1 +5307,1 @@\n-        newcopy = new_array(klass_node, length, 0);  \/\/ no arguments to push\n+        newcopy = new_array(refined_klass_node, length, 0);  \/\/ no arguments to push\n@@ -4703,1 +5379,1 @@\n-    const TypeTuple* range = tf->range();\n+    const TypeTuple* range = tf->range_cc();\n@@ -4707,1 +5383,1 @@\n-    tf = TypeFunc::make(tf->domain(), new_range);\n+    tf = TypeFunc::make(tf->domain_cc(), new_range);\n@@ -4764,1 +5440,8 @@\n-  Node* obj = nullptr;\n+  Node* obj = argument(0);\n+\n+  \/\/ Don't intrinsify hashcode on inline types for now.\n+  \/\/ The \"is locked\" runtime check also subsumes the inline type check (as inline types cannot be locked) and goes to the slow path.\n+  if (gvn().type(obj)->is_inlinetypeptr()) {\n+    return false;\n+  }\n+\n@@ -4774,1 +5457,0 @@\n-    obj = argument(0);\n@@ -4815,0 +5497,2 @@\n+    \/\/ We cannot use the inline type mask as this may check bits that are overriden\n+    \/\/ by an object monitor's pointer when inflating locking.\n@@ -4882,1 +5566,10 @@\n-  Node* obj = null_check_receiver();\n+  Node* obj = argument(0);\n+  if (obj->is_InlineType()) {\n+    const Type* t = _gvn.type(obj);\n+    if (t->maybe_null()) {\n+      null_check(obj);\n+    }\n+    set_result(makecon(TypeInstPtr::make(t->inline_klass()->java_mirror())));\n+    return true;\n+  }\n+  obj = null_check_receiver();\n@@ -5304,1 +5997,2 @@\n-    Node* obj = null_check_receiver();\n+    Node* obj = argument(0);\n+    obj = null_check_receiver();\n@@ -5308,0 +6002,6 @@\n+    if (obj_type->is_inlinetypeptr()) {\n+      \/\/ If the object to clone is an inline type, we can simply return it (i.e. a nop) since inline types have\n+      \/\/ no identity.\n+      set_result(obj);\n+      return true;\n+    }\n@@ -5314,1 +6014,2 @@\n-        obj_type->speculative_type()->is_instance_klass()) {\n+        obj_type->speculative_type()->is_instance_klass() &&\n+        !obj_type->speculative_type()->is_inlinetype()) {\n@@ -5344,0 +6045,5 @@\n+    \/\/ We only go to the fast case code if we pass a number of guards.\n+    \/\/ The paths which do not pass are accumulated in the slow_region.\n+    RegionNode* slow_region = new RegionNode(1);\n+    record_for_igvn(slow_region);\n+\n@@ -5350,3 +6056,0 @@\n-      Node* obj_length = load_array_length(array_obj);\n-      Node* array_size = nullptr; \/\/ Size of the array without object alignment padding.\n-      Node* alloc_obj = new_array(obj_klass, obj_length, 0, &array_size, \/*deoptimize_on_exception=*\/true);\n@@ -5355,20 +6058,7 @@\n-      if (bs->array_copy_requires_gc_barriers(true, T_OBJECT, true, false, BarrierSetC2::Parsing)) {\n-        \/\/ If it is an oop array, it requires very special treatment,\n-        \/\/ because gc barriers are required when accessing the array.\n-        Node* is_obja = generate_objArray_guard(obj_klass, (RegionNode*)nullptr);\n-        if (is_obja != nullptr) {\n-          PreserveJVMState pjvms2(this);\n-          set_control(is_obja);\n-          \/\/ Generate a direct call to the right arraycopy function(s).\n-          \/\/ Clones are always tightly coupled.\n-          ArrayCopyNode* ac = ArrayCopyNode::make(this, true, array_obj, intcon(0), alloc_obj, intcon(0), obj_length, true, false);\n-          ac->set_clone_oop_array();\n-          Node* n = _gvn.transform(ac);\n-          assert(n == ac, \"cannot disappear\");\n-          ac->connect_outputs(this, \/*deoptimize_on_exception=*\/true);\n-\n-          result_reg->init_req(_objArray_path, control());\n-          result_val->init_req(_objArray_path, alloc_obj);\n-          result_i_o ->set_req(_objArray_path, i_o());\n-          result_mem ->set_req(_objArray_path, reset_memory());\n-        }\n+      const TypeAryPtr* ary_ptr = obj_type->isa_aryptr();\n+      if (UseArrayFlattening && bs->array_copy_requires_gc_barriers(true, T_OBJECT, true, false, BarrierSetC2::Expansion) &&\n+          obj_type->can_be_inline_array() &&\n+          (ary_ptr == nullptr || (!ary_ptr->is_not_flat() && (!ary_ptr->is_flat() || ary_ptr->elem()->inline_klass()->contains_oops())))) {\n+        \/\/ Flat inline type array may have object field that would require a\n+        \/\/ write barrier. Conservatively, go to slow path.\n+        generate_fair_guard(flat_array_test(obj_klass), slow_region);\n@@ -5376,7 +6066,0 @@\n-      \/\/ Otherwise, there are no barriers to worry about.\n-      \/\/ (We can dispense with card marks if we know the allocation\n-      \/\/  comes out of eden (TLAB)...  In fact, ReduceInitialCardMarks\n-      \/\/  causes the non-eden paths to take compensating steps to\n-      \/\/  simulate a fresh allocation, so that no further\n-      \/\/  card marks are required in compiled code to initialize\n-      \/\/  the object.)\n@@ -5385,7 +6068,43 @@\n-        copy_to_clone(array_obj, alloc_obj, array_size, true);\n-\n-        \/\/ Present the results of the copy.\n-        result_reg->init_req(_array_path, control());\n-        result_val->init_req(_array_path, alloc_obj);\n-        result_i_o ->set_req(_array_path, i_o());\n-        result_mem ->set_req(_array_path, reset_memory());\n+        Node* obj_length = load_array_length(array_obj);\n+        Node* array_size = nullptr; \/\/ Size of the array without object alignment padding.\n+        Node* alloc_obj = new_array(obj_klass, obj_length, 0, &array_size, \/*deoptimize_on_exception=*\/true);\n+\n+        BarrierSetC2* bs = BarrierSet::barrier_set()->barrier_set_c2();\n+        if (bs->array_copy_requires_gc_barriers(true, T_OBJECT, true, false, BarrierSetC2::Parsing)) {\n+          \/\/ If it is an oop array, it requires very special treatment,\n+          \/\/ because gc barriers are required when accessing the array.\n+          Node* is_obja = generate_refArray_guard(obj_klass, (RegionNode*)nullptr);\n+          if (is_obja != nullptr) {\n+            PreserveJVMState pjvms2(this);\n+            set_control(is_obja);\n+            \/\/ Generate a direct call to the right arraycopy function(s).\n+            \/\/ Clones are always tightly coupled.\n+            ArrayCopyNode* ac = ArrayCopyNode::make(this, true, array_obj, intcon(0), alloc_obj, intcon(0), obj_length, true, false);\n+            ac->set_clone_oop_array();\n+            Node* n = _gvn.transform(ac);\n+            assert(n == ac, \"cannot disappear\");\n+            ac->connect_outputs(this, \/*deoptimize_on_exception=*\/true);\n+\n+            result_reg->init_req(_objArray_path, control());\n+            result_val->init_req(_objArray_path, alloc_obj);\n+            result_i_o ->set_req(_objArray_path, i_o());\n+            result_mem ->set_req(_objArray_path, reset_memory());\n+          }\n+        }\n+        \/\/ Otherwise, there are no barriers to worry about.\n+        \/\/ (We can dispense with card marks if we know the allocation\n+        \/\/  comes out of eden (TLAB)...  In fact, ReduceInitialCardMarks\n+        \/\/  causes the non-eden paths to take compensating steps to\n+        \/\/  simulate a fresh allocation, so that no further\n+        \/\/  card marks are required in compiled code to initialize\n+        \/\/  the object.)\n+\n+        if (!stopped()) {\n+          copy_to_clone(obj, alloc_obj, array_size, true);\n+\n+          \/\/ Present the results of the copy.\n+          result_reg->init_req(_array_path, control());\n+          result_val->init_req(_array_path, alloc_obj);\n+          result_i_o ->set_req(_array_path, i_o());\n+          result_mem ->set_req(_array_path, reset_memory());\n+        }\n@@ -5395,4 +6114,0 @@\n-    \/\/ We only go to the instance fast case code if we pass a number of guards.\n-    \/\/ The paths which do not pass are accumulated in the slow_region.\n-    RegionNode* slow_region = new RegionNode(1);\n-    record_for_igvn(slow_region);\n@@ -5530,0 +6245,12 @@\n+  int adjustment = 1;\n+  const TypeAryKlassPtr* ary_klass_ptr = alloc->in(AllocateNode::KlassNode)->bottom_type()->is_aryklassptr();\n+  if (ary_klass_ptr->is_null_free()) {\n+    \/\/ A null-free, tightly coupled array allocation can only come from LibraryCallKit::inline_newArray which\n+    \/\/ also requires the componentType and initVal on stack for re-execution.\n+    \/\/ Re-create and push the componentType.\n+    ciArrayKlass* klass = ary_klass_ptr->exact_klass()->as_array_klass();\n+    ciInstance* instance = klass->component_mirror_instance();\n+    const TypeInstPtr* t_instance = TypeInstPtr::make(instance);\n+    sfpt->ins_req(old_jvms->stkoff() + old_jvms->sp(), makecon(t_instance));\n+    adjustment++;\n+  }\n@@ -5531,5 +6258,16 @@\n-  sfpt->ins_req(old_jvms->stkoff() + old_jvms->sp(), alloc->in(AllocateNode::ALength));\n-  old_jvms->set_sp(old_jvms->sp()+1);\n-  old_jvms->set_monoff(old_jvms->monoff()+1);\n-  old_jvms->set_scloff(old_jvms->scloff()+1);\n-  old_jvms->set_endoff(old_jvms->endoff()+1);\n+  sfpt->ins_req(old_jvms->stkoff() + old_jvms->sp() + adjustment - 1, alloc->in(AllocateNode::ALength));\n+  if (ary_klass_ptr->is_null_free()) {\n+    \/\/ Re-create and push the initVal.\n+    Node* init_val = alloc->in(AllocateNode::InitValue);\n+    if (init_val == nullptr) {\n+      init_val = InlineTypeNode::make_all_zero(_gvn, ary_klass_ptr->elem()->is_instklassptr()->instance_klass()->as_inline_klass());\n+    } else if (UseCompressedOops) {\n+      init_val = _gvn.transform(new DecodeNNode(init_val, init_val->bottom_type()->make_ptr()));\n+    }\n+    sfpt->ins_req(old_jvms->stkoff() + old_jvms->sp() + adjustment, init_val);\n+    adjustment++;\n+  }\n+  old_jvms->set_sp(old_jvms->sp() + adjustment);\n+  old_jvms->set_monoff(old_jvms->monoff() + adjustment);\n+  old_jvms->set_scloff(old_jvms->scloff() + adjustment);\n+  old_jvms->set_endoff(old_jvms->endoff() + adjustment);\n@@ -5568,2 +6306,1 @@\n-    CallProjections callprojs;\n-    alloc->extract_projections(&callprojs, true);\n+    CallProjections* callprojs = alloc->extract_projections(true);\n@@ -5572,1 +6309,1 @@\n-    C->gvn_replace_by(callprojs.fallthrough_ioproj, alloc->in(TypeFunc::I_O));\n+    C->gvn_replace_by(callprojs->fallthrough_ioproj, alloc->in(TypeFunc::I_O));\n@@ -5614,1 +6351,1 @@\n-    set_i_o(callprojs.fallthrough_ioproj);\n+    set_i_o(callprojs->fallthrough_ioproj);\n@@ -5964,1 +6701,1 @@\n-    if (src_elem == dest_elem && src_elem == T_OBJECT) {\n+    if (src_elem == dest_elem && top_src->is_flat() == top_dest->is_flat() && src_elem == T_OBJECT) {\n@@ -5991,0 +6728,2 @@\n+          src_type = _gvn.type(src);\n+          top_src = src_type->isa_aryptr();\n@@ -5994,0 +6733,2 @@\n+          dest_type = _gvn.type(dest);\n+          top_dest = dest_type->isa_aryptr();\n@@ -6009,2 +6750,1 @@\n-      can_emit_guards &&\n-      !src->is_top() && !dest->is_top()) {\n+      can_emit_guards && !src->is_top() && !dest->is_top()) {\n@@ -6051,0 +6791,1 @@\n+    Node* refined_dest_klass = dest_klass;\n@@ -6052,0 +6793,1 @@\n+      dest_klass = load_non_refined_array_klass(refined_dest_klass);\n@@ -6053,8 +6795,1 @@\n-\n-      if (not_subtype_ctrl != top()) {\n-        PreserveJVMState pjvms(this);\n-        set_control(not_subtype_ctrl);\n-        uncommon_trap(Deoptimization::Reason_intrinsic,\n-                      Deoptimization::Action_make_not_entrant);\n-        assert(stopped(), \"Should be stopped\");\n-      }\n+      slow_region->add_req(not_subtype_ctrl);\n@@ -6062,0 +6797,21 @@\n+\n+    \/\/ TODO 8350865 Improve this. What about atomicity? Make sure this is always folded for type arrays.\n+    \/\/ If destination is null-restricted, source must be null-restricted as well: src_null_restricted || !dst_null_restricted\n+    Node* src_klass = load_object_klass(src);\n+    Node* adr_prop_src = basic_plus_adr(src_klass, in_bytes(ArrayKlass::properties_offset()));\n+    Node* prop_src = _gvn.transform(LoadNode::make(_gvn, control(), immutable_memory(), adr_prop_src, TypeRawPtr::BOTTOM, TypeInt::INT, T_INT, MemNode::unordered));\n+    Node* adr_prop_dest = basic_plus_adr(refined_dest_klass, in_bytes(ArrayKlass::properties_offset()));\n+    Node* prop_dest = _gvn.transform(LoadNode::make(_gvn, control(), immutable_memory(), adr_prop_dest, TypeRawPtr::BOTTOM, TypeInt::INT, T_INT, MemNode::unordered));\n+\n+    prop_dest = _gvn.transform(new XorINode(prop_dest, intcon(ArrayKlass::ArrayProperties::NULL_RESTRICTED)));\n+    prop_src = _gvn.transform(new OrINode(prop_dest, prop_src));\n+    prop_src = _gvn.transform(new AndINode(prop_src, intcon(ArrayKlass::ArrayProperties::NULL_RESTRICTED)));\n+\n+    Node* chk = _gvn.transform(new CmpINode(prop_src, intcon(ArrayKlass::ArrayProperties::NULL_RESTRICTED)));\n+    Node* tst = _gvn.transform(new BoolNode(chk, BoolTest::ne));\n+    generate_fair_guard(tst, slow_region);\n+\n+    \/\/ TODO 8350865 This is too strong\n+    generate_fair_guard(flat_array_test(src), slow_region);\n+    generate_fair_guard(flat_array_test(dest), slow_region);\n+\n@@ -6070,2 +6826,10 @@\n-    const TypeKlassPtr* dest_klass_t = _gvn.type(dest_klass)->is_klassptr();\n-    const Type *toop = dest_klass_t->cast_to_exactness(false)->as_instance_type();\n+    const TypeKlassPtr* dest_klass_t = _gvn.type(dest_klass)->isa_klassptr();\n+    if (dest_klass_t == nullptr) {\n+      \/\/ refined_dest_klass may not be an array, which leads to dest_klass being top. This means we\n+      \/\/ are in a dead path.\n+      uncommon_trap(Deoptimization::Reason_intrinsic,\n+                    Deoptimization::Action_make_not_entrant);\n+      return true;\n+    }\n+\n+    const Type* toop = dest_klass_t->cast_to_exactness(false)->as_instance_type();\n@@ -6080,0 +6844,3 @@\n+  Node* dest_klass = load_object_klass(dest);\n+  dest_klass = load_non_refined_array_klass(dest_klass);\n+\n@@ -6084,1 +6851,1 @@\n-                                          load_object_klass(src), load_object_klass(dest),\n+                                          load_object_klass(src), dest_klass,\n","filename":"src\/hotspot\/share\/opto\/library_call.cpp","additions":899,"deletions":132,"binary":false,"changes":1031,"status":"modified"},{"patch":"@@ -34,0 +34,1 @@\n+#include \"opto\/inlinetypenode.hpp\"\n@@ -111,2 +112,3 @@\n-    if (!stopped() && result() != nullptr) {\n-      if (result()->is_top()) {\n+    Node* res = result();\n+    if (!stopped() && res != nullptr) {\n+      if (res->is_top()) {\n@@ -116,2 +118,9 @@\n-      BasicType bt = result()->bottom_type()->basic_type();\n-      push_node(bt, result());\n+      BasicType bt = res->bottom_type()->basic_type();\n+      if (C->inlining_incrementally() && res->is_InlineType()) {\n+        \/\/ The caller expects an oop when incrementally inlining an intrinsic that returns an\n+        \/\/ inline type. Make sure the call is re-executed if the allocation triggers a deoptimization.\n+        PreserveReexecuteState preexecs(this);\n+        jvms()->set_should_reexecute(true);\n+        res = res->as_InlineType()->buffer(this);\n+      }\n+      push_node(bt, res);\n@@ -189,0 +198,3 @@\n+  Node* load_default_refined_array_klass(Node* klass_node, bool type_array_guard = true);\n+  Node* load_non_refined_array_klass(Node* klass_node);\n+\n@@ -195,0 +207,9 @@\n+\n+  enum ArrayKind {\n+    AnyArray,\n+    NonArray,\n+    RefArray,\n+    NonRefArray,\n+    TypeArray\n+  };\n+\n@@ -196,0 +217,1 @@\n+\n@@ -197,1 +219,1 @@\n-    return generate_array_guard_common(kls, region, false, false, obj);\n+    return generate_array_guard_common(kls, region, AnyArray, obj);\n@@ -200,1 +222,4 @@\n-    return generate_array_guard_common(kls, region, false, true, obj);\n+    return generate_array_guard_common(kls, region, NonArray, obj);\n+  }\n+  Node* generate_refArray_guard(Node* kls, RegionNode* region, Node** obj = nullptr) {\n+    return generate_array_guard_common(kls, region, RefArray, obj);\n@@ -202,2 +227,2 @@\n-  Node* generate_objArray_guard(Node* kls, RegionNode* region, Node** obj = nullptr) {\n-    return generate_array_guard_common(kls, region, true, false, obj);\n+  Node* generate_non_refArray_guard(Node* kls, RegionNode* region, Node** obj = nullptr) {\n+    return generate_array_guard_common(kls, region, NonRefArray, obj);\n@@ -205,2 +230,2 @@\n-  Node* generate_non_objArray_guard(Node* kls, RegionNode* region, Node** obj = nullptr) {\n-    return generate_array_guard_common(kls, region, true, true, obj);\n+  Node* generate_typeArray_guard(Node* kls, RegionNode* region, Node** obj = nullptr) {\n+    return generate_array_guard_common(kls, region, TypeArray, obj);\n@@ -208,2 +233,1 @@\n-  Node* generate_array_guard_common(Node* kls, RegionNode* region,\n-                                    bool obj_array, bool not_array, Node** obj = nullptr);\n+  Node* generate_array_guard_common(Node* kls, RegionNode* region, ArrayKind kind, Node** obj = nullptr);\n@@ -258,0 +282,1 @@\n+  bool inline_unsafe_flat_access(bool is_store, AccessKind kind);\n@@ -261,0 +286,3 @@\n+  bool inline_newArray(bool null_free, bool atomic);\n+  typedef enum { IsFlat, IsNullRestricted, IsAtomic } ArrayPropertiesCheck;\n+  bool inline_getArrayProperties(ArrayPropertiesCheck check);\n@@ -264,0 +292,2 @@\n+  bool inline_unsafe_make_private_buffer();\n+  bool inline_unsafe_finish_private_buffer();\n@@ -292,0 +322,1 @@\n+  bool inline_primitive_Class_conversion(vmIntrinsics::ID id);\n@@ -321,0 +352,4 @@\n+  bool inline_arrayInstanceBaseOffset();\n+  bool inline_arrayInstanceIndexScale();\n+  bool inline_arrayLayout();\n+  bool inline_getFieldMap();\n","filename":"src\/hotspot\/share\/opto\/library_call.hpp","additions":47,"deletions":12,"binary":false,"changes":59,"status":"modified"},{"patch":"@@ -25,0 +25,3 @@\n+#include \"ci\/ciFlatArrayKlass.hpp\"\n+#include \"ci\/ciInlineKlass.hpp\"\n+#include \"ci\/ciInstanceKlass.hpp\"\n@@ -38,0 +41,1 @@\n+#include \"opto\/inlinetypenode.hpp\"\n@@ -46,0 +50,1 @@\n+#include \"opto\/opcodes.hpp\"\n@@ -55,0 +60,2 @@\n+#include \"runtime\/stubRoutines.hpp\"\n+#include \"utilities\/globalDefinitions.hpp\"\n@@ -84,11 +91,0 @@\n-void PhaseMacroExpand::migrate_outs(Node *old, Node *target) {\n-  assert(old != nullptr, \"sanity\");\n-  for (DUIterator_Fast imax, i = old->fast_outs(imax); i < imax; i++) {\n-    Node* use = old->fast_out(i);\n-    _igvn.rehash_node_delayed(use);\n-    imax -= replace_input(use, old, target);\n-    \/\/ back up iterator\n-    --i;\n-  }\n-  assert(old->outcnt() == 0, \"all uses must be deleted\");\n-}\n@@ -147,1 +143,1 @@\n-  bs->eliminate_gc_barrier(this, p2x);\n+  bs->eliminate_gc_barrier(&_igvn, p2x);\n@@ -190,0 +186,2 @@\n+      } else if (in->is_LoadFlat() || in->is_StoreFlat()) {\n+        mem = in->in(TypeFunc::Memory);\n@@ -202,1 +200,1 @@\n-        int adr_offset = atype->offset();\n+        int adr_offset = atype->flat_offset();\n@@ -247,1 +245,1 @@\n-   } else if (mem->Opcode() == Op_StrInflatedCopy) {\n+    } else if (mem->Opcode() == Op_StrInflatedCopy) {\n@@ -292,1 +290,5 @@\n-      const TypePtr* adr_type = nullptr;\n+      Node* base = ac->in(ArrayCopyNode::Src);\n+      const TypeAryPtr* adr_type = _igvn.type(base)->is_aryptr();\n+      if (adr_type->is_flat()) {\n+        shift = adr_type->flat_log_elem_size();\n+      }\n@@ -295,2 +297,2 @@\n-        Node* base = ac->in(ArrayCopyNode::Src);\n-        adr_type = _igvn.type(base)->is_ptr()->add_offset(off);\n+        adr_type = _igvn.type(adr)->is_aryptr();\n+        assert(adr_type == _igvn.type(base)->is_aryptr()->add_field_offset_and_offset(off), \"incorrect address type\");\n@@ -300,1 +302,1 @@\n-          return value_from_mem(ac->in(TypeFunc::Memory), ctl, ft, ftype, adr_type->isa_oopptr(), alloc);\n+          return value_from_mem(ac->in(TypeFunc::Memory), ctl, ft, ftype, adr_type, alloc);\n@@ -303,0 +305,5 @@\n+        if (ac->in(ArrayCopyNode::Src) == ac->in(ArrayCopyNode::Dest)) {\n+          \/\/ Non constant offset in the array: we can't statically\n+          \/\/ determine the value\n+          return nullptr;\n+        }\n@@ -310,7 +317,5 @@\n-        Node* base = ac->in(ArrayCopyNode::Src);\n-        adr_type = _igvn.type(base)->is_ptr()->add_offset(Type::OffsetBot);\n-        if (ac->in(ArrayCopyNode::Src) == ac->in(ArrayCopyNode::Dest)) {\n-          \/\/ Non constant offset in the array: we can't statically\n-          \/\/ determine the value\n-          return nullptr;\n-        }\n+        \/\/ In the case of a flat inline type array, each field has its\n+        \/\/ own slice so we need to extract the field being accessed from\n+        \/\/ the address computation\n+        adr_type = adr_type->add_field_offset_and_offset(offset)->add_offset(Type::OffsetBot)->is_aryptr();\n+        adr = _igvn.transform(new CastPPNode(ctl, adr, adr_type));\n@@ -327,0 +332,1 @@\n+      assert(res->isa_DecodeN(), \"should be narrow oop\");\n@@ -342,1 +348,1 @@\n-  int offset = adr_t->offset();\n+  int offset = adr_t->flat_offset();\n@@ -378,1 +384,1 @@\n-    } else  {\n+    } else {\n@@ -381,3 +387,8 @@\n-        \/\/ hit a sentinel, return appropriate 0 value\n-        values.at_put(j, _igvn.zerocon(ft));\n-        continue;\n+        \/\/ hit a sentinel, return appropriate value\n+        Node* init_value = value_from_alloc(ft, adr_t, alloc);\n+        if (init_value == nullptr) {\n+          return nullptr;\n+        } else {\n+          values.at_put(j, init_value);\n+          continue;\n+        }\n@@ -401,2 +412,7 @@\n-      } else if(val->is_Proj() && val->in(0) == alloc) {\n-        values.at_put(j, _igvn.zerocon(ft));\n+      } else if (val->is_Proj() && val->in(0) == alloc) {\n+        Node* init_value = value_from_alloc(ft, adr_t, alloc);\n+        if (init_value == nullptr) {\n+          return nullptr;\n+        } else {\n+          values.at_put(j, init_value);\n+        }\n@@ -443,0 +459,56 @@\n+\/\/ Extract the initial value of a field in an allocation\n+Node* PhaseMacroExpand::value_from_alloc(BasicType ft, const TypeOopPtr* adr_t, AllocateNode* alloc) {\n+  Node* init_value = alloc->in(AllocateNode::InitValue);\n+  if (init_value == nullptr) {\n+    assert(alloc->in(AllocateNode::RawInitValue) == nullptr, \"conflicting InitValue and RawInitValue\");\n+    return _igvn.zerocon(ft);\n+  }\n+\n+  const TypeAryPtr* ary_t = adr_t->isa_aryptr();\n+  assert(ary_t != nullptr, \"must be a pointer into an array\");\n+\n+  \/\/ If this is not a flat array, then it must be an oop array with elements being init_value\n+  if (ary_t->is_not_flat()) {\n+#ifdef ASSERT\n+    BasicType init_bt = init_value->bottom_type()->basic_type();\n+    assert(ft == init_bt ||\n+           (!is_java_primitive(ft) && !is_java_primitive(init_bt) && type2aelembytes(ft, true) == type2aelembytes(init_bt, true)) ||\n+           (is_subword_type(ft) && init_bt == T_INT),\n+           \"invalid init_value of type %s for field of type %s\", type2name(init_bt), type2name(ft));\n+#endif \/\/ ASSERT\n+    return init_value;\n+  }\n+\n+  assert(ary_t->klass_is_exact() && ary_t->is_flat(), \"must be an exact flat array\");\n+  assert(ary_t->field_offset().get() != Type::OffsetBot, \"unknown offset\");\n+  if (init_value->is_EncodeP()) {\n+    init_value = init_value->in(1);\n+  }\n+  \/\/ Cannot look through init_value if it is an oop\n+  if (!init_value->is_InlineType()) {\n+    return nullptr;\n+  }\n+\n+  ciInlineKlass* vk = init_value->bottom_type()->inline_klass();\n+  if (ary_t->field_offset().get() == vk->null_marker_offset_in_payload()) {\n+    init_value = init_value->as_InlineType()->get_null_marker();\n+  } else {\n+    init_value = init_value->as_InlineType()->field_value_by_offset(ary_t->field_offset().get() + vk->payload_offset(), true);\n+  }\n+\n+  if (ft == T_NARROWOOP) {\n+    assert(init_value->bottom_type()->isa_ptr(), \"must be a pointer\");\n+    init_value = transform_later(new EncodePNode(init_value, init_value->bottom_type()->make_narrowoop()));\n+  }\n+\n+#ifdef ASSERT\n+  BasicType init_bt = init_value->bottom_type()->basic_type();\n+  assert(ft == init_bt ||\n+         (!is_java_primitive(ft) && !is_java_primitive(init_bt) && type2aelembytes(ft, true) == type2aelembytes(init_bt, true)) ||\n+         (is_subword_type(ft) && init_bt == T_INT),\n+         \"invalid init_value of type %s for field of type %s\", type2name(init_bt), type2name(ft));\n+#endif \/\/ ASSERT\n+\n+  return init_value;\n+}\n+\n@@ -450,1 +522,1 @@\n-  int offset = adr_t->offset();\n+  int offset = adr_t->flat_offset();\n@@ -452,1 +524,0 @@\n-  Node *alloc_ctrl = alloc->in(TypeFunc::Control);\n@@ -469,1 +540,1 @@\n-        done = true; \/\/ Something go wrong.\n+        done = true; \/\/ Something went wrong.\n@@ -479,1 +550,1 @@\n-             atype->is_known_instance_field() && atype->offset() == offset &&\n+             atype->is_known_instance_field() && atype->flat_offset() == offset &&\n@@ -516,2 +587,2 @@\n-      \/\/ hit a sentinel, return appropriate 0 value\n-      return _igvn.zerocon(ft);\n+      \/\/ hit a sentinel, return appropriate value\n+      return value_from_alloc(ft, adr_t, alloc);\n@@ -548,1 +619,1 @@\n-  \/\/ Something go wrong.\n+  \/\/ Something went wrong.\n@@ -552,0 +623,69 @@\n+\/\/ Search the last value stored into the inline type's fields (for flat arrays).\n+Node* PhaseMacroExpand::inline_type_from_mem(ciInlineKlass* vk, const TypeAryPtr* elem_adr_type, int elem_idx, int offset_in_element, bool null_free, AllocateNode* alloc, SafePointNode* sfpt) {\n+  auto report_failure = [&](int field_offset_in_element) {\n+#ifndef PRODUCT\n+    if (PrintEliminateAllocations) {\n+      ciInlineKlass* elem_klass = elem_adr_type->elem()->inline_klass();\n+      int offset = field_offset_in_element + elem_klass->payload_offset();\n+      ciField* flattened_field = elem_klass->get_field_by_offset(offset, false);\n+      assert(flattened_field != nullptr, \"must have a field of type %s at offset %d\", elem_klass->name()->as_utf8(), offset);\n+      tty->print(\"=== At SafePoint node %d can't find value of field [%s] of array element [%d]\", sfpt->_idx, flattened_field->name()->as_utf8(), elem_idx);\n+      tty->print(\", which prevents elimination of: \");\n+      alloc->dump();\n+    }\n+#endif \/\/ PRODUCT\n+  };\n+\n+  \/\/ Create a new InlineTypeNode and retrieve the field values from memory\n+  InlineTypeNode* vt = InlineTypeNode::make_uninitialized(_igvn, vk, false);\n+  transform_later(vt);\n+  if (null_free) {\n+    vt->set_null_marker(_igvn);\n+  } else {\n+    int nm_offset_in_element = offset_in_element + vk->null_marker_offset_in_payload();\n+    const TypeAryPtr* nm_adr_type = elem_adr_type->with_field_offset(nm_offset_in_element);\n+    Node* nm_value = value_from_mem(sfpt->memory(), sfpt->control(), T_BOOLEAN, TypeInt::BOOL, nm_adr_type, alloc);\n+    if (nm_value != nullptr) {\n+      vt->set_null_marker(_igvn, nm_value);\n+    } else {\n+      report_failure(nm_offset_in_element);\n+      return nullptr;\n+    }\n+  }\n+\n+  for (int i = 0; i < vk->nof_declared_nonstatic_fields(); ++i) {\n+    ciType* field_type = vt->field_type(i);\n+    int field_offset_in_element = offset_in_element + vt->field_offset(i) - vk->payload_offset();\n+    Node* field_value = nullptr;\n+    if (vt->field_is_flat(i)) {\n+      field_value = inline_type_from_mem(field_type->as_inline_klass(), elem_adr_type, elem_idx, field_offset_in_element, vt->field_is_null_free(i), alloc, sfpt);\n+    } else {\n+      const Type* ft = Type::get_const_type(field_type);\n+      BasicType bt = type2field[field_type->basic_type()];\n+      if (UseCompressedOops && !is_java_primitive(bt)) {\n+        ft = ft->make_narrowoop();\n+        bt = T_NARROWOOP;\n+      }\n+      \/\/ Each inline type field has its own memory slice\n+      const TypeAryPtr* field_adr_type = elem_adr_type->with_field_offset(field_offset_in_element);\n+      field_value = value_from_mem(sfpt->memory(), sfpt->control(), bt, ft, field_adr_type, alloc);\n+      if (field_value == nullptr) {\n+        report_failure(field_offset_in_element);\n+      } else if (ft->isa_narrowoop()) {\n+        assert(UseCompressedOops, \"unexpected narrow oop\");\n+        if (field_value->is_EncodeP()) {\n+          field_value = field_value->in(1);\n+        } else if (!field_value->is_InlineType()) {\n+          field_value = transform_later(new DecodeNNode(field_value, field_value->get_ptr_type()));\n+        }\n+      }\n+    }\n+    if (field_value != nullptr) {\n+      vt->set_field_value(i, field_value);\n+    } else {\n+      return nullptr;\n+    }\n+  }\n+  return vt;\n+}\n+\n@@ -561,0 +701,1 @@\n+  Unique_Node_List worklist;\n@@ -569,0 +710,1 @@\n+    worklist.push(res);\n@@ -585,1 +727,1 @@\n-  if (can_eliminate && res != nullptr) {\n+  while (can_eliminate && worklist.size() > 0) {\n@@ -587,2 +729,2 @@\n-    for (DUIterator_Fast jmax, j = res->fast_outs(jmax);\n-                               j < jmax && can_eliminate; j++) {\n+    res = worklist.pop();\n+    for (DUIterator_Fast jmax, j = res->fast_outs(jmax); j < jmax && can_eliminate; j++) {\n@@ -603,1 +745,1 @@\n-          if (n->is_Mem() && n->as_Mem()->is_mismatched_access()) {\n+          if ((n->is_Mem() && n->as_Mem()->is_mismatched_access()) || n->is_LoadFlat() || n->is_StoreFlat()) {\n@@ -639,0 +781,1 @@\n+          assert(!res->is_Phi() || !res->as_Phi()->can_be_inline_type(), \"Inline type allocations should not have safepoint uses\");\n@@ -641,0 +784,23 @@\n+      } else if (use->is_InlineType() && use->as_InlineType()->get_oop() == res) {\n+        \/\/ Look at uses\n+        for (DUIterator_Fast kmax, k = use->fast_outs(kmax); k < kmax; k++) {\n+          Node* u = use->fast_out(k);\n+          if (u->is_InlineType()) {\n+            \/\/ Use in flat field can be eliminated\n+            InlineTypeNode* vt = u->as_InlineType();\n+            for (uint i = 0; i < vt->field_count(); ++i) {\n+              if (vt->field_value(i) == use && !vt->field_is_flat(i)) {\n+                can_eliminate = false; \/\/ Use in non-flat field\n+                break;\n+              }\n+            }\n+          } else {\n+            \/\/ Add other uses to the worklist to process individually\n+            worklist.push(use);\n+          }\n+        }\n+      } else if (use->Opcode() == Op_StoreX && use->in(MemNode::Address) == res) {\n+        \/\/ Store to mark word of inline type larval buffer\n+        assert(res_type->is_inlinetypeptr(), \"Unexpected store to mark word\");\n+      } else if (res_type->is_inlinetypeptr() && (use->Opcode() == Op_MemBarRelease || use->Opcode() == Op_MemBarStoreStore)) {\n+        \/\/ Inline type buffer allocations are followed by a membar\n@@ -663,0 +829,3 @@\n+      } else {\n+        assert(use->Opcode() == Op_CastP2X, \"should be\");\n+        assert(!use->has_out_with(Op_OrL), \"should have been removed because oop is never null\");\n@@ -675,1 +844,1 @@\n-    } else if (alloc->_is_scalar_replaceable) {\n+    } else {\n@@ -780,1 +949,148 @@\n-SafePointScalarObjectNode* PhaseMacroExpand::create_scalarized_object_description(AllocateNode *alloc, SafePointNode* sfpt) {\n+void PhaseMacroExpand::process_field_value_at_safepoint(const Type* field_type, Node* field_val, SafePointNode* sfpt, Unique_Node_List* value_worklist) {\n+  if (UseCompressedOops && field_type->isa_narrowoop()) {\n+    \/\/ Enable \"DecodeN(EncodeP(Allocate)) --> Allocate\" transformation\n+    \/\/ to be able scalar replace the allocation.\n+    if (field_val->is_EncodeP()) {\n+      field_val = field_val->in(1);\n+    } else if (!field_val->is_InlineType()) {\n+      field_val = transform_later(new DecodeNNode(field_val, field_val->get_ptr_type()));\n+    }\n+  }\n+\n+  \/\/ Keep track of inline types to scalarize them later\n+  if (field_val->is_InlineType()) {\n+    value_worklist->push(field_val);\n+  } else if (field_val->is_Phi()) {\n+    PhiNode* phi = field_val->as_Phi();\n+    \/\/ Eagerly replace inline type phis now since we could be removing an inline type allocation where we must\n+    \/\/ scalarize all its fields in safepoints.\n+    field_val = phi->try_push_inline_types_down(&_igvn, true);\n+    if (field_val->is_InlineType()) {\n+      value_worklist->push(field_val);\n+    }\n+  }\n+  DEBUG_ONLY(verify_type_compatability(field_val->bottom_type(), field_type);)\n+  sfpt->add_req(field_val);\n+}\n+\n+bool PhaseMacroExpand::add_array_elems_to_safepoint(AllocateNode* alloc, const TypeAryPtr* array_type, SafePointNode* sfpt, Unique_Node_List* value_worklist) {\n+  const Type* elem_type = array_type->elem();\n+  BasicType basic_elem_type = elem_type->array_element_basic_type();\n+\n+  intptr_t elem_size;\n+  uint header_size;\n+  if (array_type->is_flat()) {\n+    elem_size = array_type->flat_elem_size();\n+    header_size = arrayOopDesc::base_offset_in_bytes(T_FLAT_ELEMENT);\n+  } else {\n+    elem_size = type2aelembytes(basic_elem_type);\n+    header_size = arrayOopDesc::base_offset_in_bytes(basic_elem_type);\n+  }\n+\n+  int n_elems = alloc->in(AllocateNode::ALength)->get_int();\n+  for (int elem_idx = 0; elem_idx < n_elems; elem_idx++) {\n+    intptr_t elem_offset = header_size + elem_idx * elem_size;\n+    const TypeAryPtr* elem_adr_type = array_type->with_offset(elem_offset);\n+    Node* elem_val;\n+    if (array_type->is_flat()) {\n+      ciInlineKlass* elem_klass = elem_type->inline_klass();\n+      assert(elem_klass->maybe_flat_in_array(), \"must be flat in array\");\n+      elem_val = inline_type_from_mem(elem_klass, elem_adr_type, elem_idx, 0, array_type->is_null_free(), alloc, sfpt);\n+    } else {\n+      elem_val = value_from_mem(sfpt->memory(), sfpt->control(), basic_elem_type, elem_type, elem_adr_type, alloc);\n+#ifndef PRODUCT\n+      if (PrintEliminateAllocations && elem_val == nullptr) {\n+        tty->print(\"=== At SafePoint node %d can't find value of array element [%d]\", sfpt->_idx, elem_idx);\n+        tty->print(\", which prevents elimination of: \");\n+        alloc->dump();\n+      }\n+#endif \/\/ PRODUCT\n+    }\n+    if (elem_val == nullptr) {\n+      return false;\n+    }\n+\n+    process_field_value_at_safepoint(elem_type, elem_val, sfpt, value_worklist);\n+  }\n+\n+  return true;\n+}\n+\n+\/\/ Recursively adds all flattened fields of a type 'iklass' inside 'base' to 'sfpt'.\n+\/\/ 'offset_minus_header' refers to the offset of the payload of 'iklass' inside 'base' minus the\n+\/\/ payload offset of 'iklass'. If 'base' is of type 'iklass' then 'offset_minus_header' == 0.\n+bool PhaseMacroExpand::add_inst_fields_to_safepoint(ciInstanceKlass* iklass, AllocateNode* alloc, Node* base, int offset_minus_header, SafePointNode* sfpt, Unique_Node_List* value_worklist) {\n+  const TypeInstPtr* base_type = _igvn.type(base)->is_instptr();\n+  auto report_failure = [&](int offset) {\n+#ifndef PRODUCT\n+    if (PrintEliminateAllocations) {\n+      ciInstanceKlass* base_klass = base_type->instance_klass();\n+      ciField* flattened_field = base_klass->get_field_by_offset(offset, false);\n+      assert(flattened_field != nullptr, \"must have a field of type %s at offset %d\", base_klass->name()->as_utf8(), offset);\n+      tty->print(\"=== At SafePoint node %d can't find value of field: \", sfpt->_idx);\n+      flattened_field->print();\n+      int field_idx = C->alias_type(flattened_field)->index();\n+      tty->print(\" (alias_idx=%d)\", field_idx);\n+      tty->print(\", which prevents elimination of: \");\n+      base->dump();\n+    }\n+#endif \/\/ PRODUCT\n+  };\n+\n+  for (int i = 0; i < iklass->nof_declared_nonstatic_fields(); i++) {\n+    ciField* field = iklass->declared_nonstatic_field_at(i);\n+    if (field->is_flat()) {\n+      ciInlineKlass* fvk = field->type()->as_inline_klass();\n+      int field_offset_minus_header = offset_minus_header + field->offset_in_bytes() - fvk->payload_offset();\n+      bool success = add_inst_fields_to_safepoint(fvk, alloc, base, field_offset_minus_header, sfpt, value_worklist);\n+      if (!success) {\n+        return false;\n+      }\n+\n+      \/\/ The null marker of a field is added right after we scalarize that field\n+      if (!field->is_null_free()) {\n+        int nm_offset = offset_minus_header + field->null_marker_offset();\n+        Node* null_marker = value_from_mem(sfpt->memory(), sfpt->control(), T_BOOLEAN, TypeInt::BOOL, base_type->with_offset(nm_offset), alloc);\n+        if (null_marker == nullptr) {\n+          report_failure(nm_offset);\n+          return false;\n+        }\n+        process_field_value_at_safepoint(TypeInt::BOOL, null_marker, sfpt, value_worklist);\n+      }\n+\n+      continue;\n+    }\n+\n+    int offset = offset_minus_header + field->offset_in_bytes();\n+    ciType* elem_type = field->type();\n+    BasicType basic_elem_type = field->layout_type();\n+\n+    const Type* field_type;\n+    if (is_reference_type(basic_elem_type)) {\n+      if (!elem_type->is_loaded()) {\n+        field_type = TypeInstPtr::BOTTOM;\n+      } else {\n+        field_type = TypeOopPtr::make_from_klass(elem_type->as_klass());\n+      }\n+      if (UseCompressedOops) {\n+        field_type = field_type->make_narrowoop();\n+        basic_elem_type = T_NARROWOOP;\n+      }\n+    } else {\n+      field_type = Type::get_const_basic_type(basic_elem_type);\n+    }\n+\n+    const TypeInstPtr* field_addr_type = base_type->add_offset(offset)->isa_instptr();\n+    Node* field_val = value_from_mem(sfpt->memory(), sfpt->control(), basic_elem_type, field_type, field_addr_type, alloc);\n+    if (field_val == nullptr) {\n+      report_failure(offset);\n+      return false;\n+    }\n+    process_field_value_at_safepoint(field_type, field_val, sfpt, value_worklist);\n+  }\n+\n+  return true;\n+}\n+\n+SafePointScalarObjectNode* PhaseMacroExpand::create_scalarized_object_description(AllocateNode* alloc, SafePointNode* sfpt,\n+                                                                                  Unique_Node_List* value_worklist) {\n@@ -785,2 +1101,0 @@\n-  BasicType basic_elem_type  = T_ILLEGAL;\n-  const Type* field_type     = nullptr;\n@@ -789,2 +1103,0 @@\n-  int array_base             = 0;\n-  int element_size           = 0;\n@@ -796,0 +1108,1 @@\n+  uint before_sfpt_req = sfpt->req();\n@@ -808,4 +1121,10 @@\n-      basic_elem_type = res_type->is_aryptr()->elem()->array_element_basic_type();\n-      array_base = arrayOopDesc::base_offset_in_bytes(basic_elem_type);\n-      element_size = type2aelembytes(basic_elem_type);\n-      field_type = res_type->is_aryptr()->elem();\n+    }\n+\n+    if (res->bottom_type()->is_inlinetypeptr()) {\n+      \/\/ Nullable inline types have a null marker field which is added to the safepoint when scalarizing them (see\n+      \/\/ InlineTypeNode::make_scalar_in_safepoint()). When having circular inline types, we stop scalarizing at depth 1\n+      \/\/ to avoid an endless recursion. Therefore, we do not have a SafePointScalarObjectNode node here, yet.\n+      \/\/ We are about to create a SafePointScalarObjectNode as if this is a normal object. Add an additional int input\n+      \/\/ with value 1 which sets the null marker to true to indicate that the object is always non-null. This input is checked\n+      \/\/ later in PhaseOutput::filLocArray() for inline types.\n+      sfpt->add_req(_igvn.intcon(1));\n@@ -819,64 +1138,4 @@\n-  \/\/ Scan object's fields adding an input to the safepoint for each field.\n-  for (int j = 0; j < nfields; j++) {\n-    intptr_t offset;\n-    ciField* field = nullptr;\n-    if (iklass != nullptr) {\n-      field = iklass->nonstatic_field_at(j);\n-      offset = field->offset_in_bytes();\n-      ciType* elem_type = field->type();\n-      basic_elem_type = field->layout_type();\n-\n-      \/\/ The next code is taken from Parse::do_get_xxx().\n-      if (is_reference_type(basic_elem_type)) {\n-        if (!elem_type->is_loaded()) {\n-          field_type = TypeInstPtr::BOTTOM;\n-        } else if (field != nullptr && field->is_static_constant()) {\n-          ciObject* con = field->constant_value().as_object();\n-          \/\/ Do not \"join\" in the previous type; it doesn't add value,\n-          \/\/ and may yield a vacuous result if the field is of interface type.\n-          field_type = TypeOopPtr::make_from_constant(con)->isa_oopptr();\n-          assert(field_type != nullptr, \"field singleton type must be consistent\");\n-        } else {\n-          field_type = TypeOopPtr::make_from_klass(elem_type->as_klass());\n-        }\n-        if (UseCompressedOops) {\n-          field_type = field_type->make_narrowoop();\n-          basic_elem_type = T_NARROWOOP;\n-        }\n-      } else {\n-        field_type = Type::get_const_basic_type(basic_elem_type);\n-      }\n-    } else {\n-      offset = array_base + j * (intptr_t)element_size;\n-    }\n-\n-    const TypeOopPtr *field_addr_type = res_type->add_offset(offset)->isa_oopptr();\n-\n-    Node *field_val = value_from_mem(sfpt->memory(), sfpt->control(), basic_elem_type, field_type, field_addr_type, alloc);\n-\n-    \/\/ We weren't able to find a value for this field,\n-    \/\/ give up on eliminating this allocation.\n-    if (field_val == nullptr) {\n-      uint last = sfpt->req() - 1;\n-      for (int k = 0;  k < j; k++) {\n-        sfpt->del_req(last--);\n-      }\n-      _igvn._worklist.push(sfpt);\n-\n-#ifndef PRODUCT\n-      if (PrintEliminateAllocations) {\n-        if (field != nullptr) {\n-          tty->print(\"=== At SafePoint node %d can't find value of field: \", sfpt->_idx);\n-          field->print();\n-          int field_idx = C->get_alias_index(field_addr_type);\n-          tty->print(\" (alias_idx=%d)\", field_idx);\n-        } else { \/\/ Array's element\n-          tty->print(\"=== At SafePoint node %d can't find value of array element [%d]\", sfpt->_idx, j);\n-        }\n-        tty->print(\", which prevents elimination of: \");\n-        if (res == nullptr)\n-          alloc->dump();\n-        else\n-          res->dump();\n-      }\n-#endif\n+  if (res == nullptr) {\n+    sfpt->jvms()->set_endoff(sfpt->req());\n+    return sobj;\n+  }\n@@ -884,2 +1143,6 @@\n-      return nullptr;\n-    }\n+  bool success;\n+  if (iklass == nullptr) {\n+    success = add_array_elems_to_safepoint(alloc, res_type->is_aryptr(), sfpt, value_worklist);\n+  } else {\n+    success = add_inst_fields_to_safepoint(iklass, alloc, res, 0, sfpt, value_worklist);\n+  }\n@@ -887,8 +1150,4 @@\n-    if (UseCompressedOops && field_type->isa_narrowoop()) {\n-      \/\/ Enable \"DecodeN(EncodeP(Allocate)) --> Allocate\" transformation\n-      \/\/ to be able scalar replace the allocation.\n-      if (field_val->is_EncodeP()) {\n-        field_val = field_val->in(1);\n-      } else {\n-        field_val = transform_later(new DecodeNNode(field_val, field_val->get_ptr_type()));\n-      }\n+  \/\/ We weren't able to find a value for this field, remove all the fields added to the safepoint\n+  if (!success) {\n+    for (uint i = sfpt->req() - 1; i >= before_sfpt_req; i--) {\n+      sfpt->del_req(i);\n@@ -896,2 +1155,2 @@\n-    DEBUG_ONLY(verify_type_compatability(field_val->bottom_type(), field_type);)\n-    sfpt->add_req(field_val);\n+    _igvn._worklist.push(sfpt);\n+    return nullptr;\n@@ -901,1 +1160,0 @@\n-\n@@ -910,0 +1168,4 @@\n+  const TypeOopPtr* res_type = nullptr;\n+  if (res != nullptr) { \/\/ Could be null when there are no users\n+    res_type = _igvn.type(res)->isa_oopptr();\n+  }\n@@ -912,0 +1174,1 @@\n+  Unique_Node_List value_worklist;\n@@ -914,1 +1177,1 @@\n-    SafePointScalarObjectNode* sobj = create_scalarized_object_description(alloc, sfpt);\n+    SafePointScalarObjectNode* sobj = create_scalarized_object_description(alloc, sfpt, &value_worklist);\n@@ -930,1 +1193,8 @@\n-\n+  \/\/ Scalarize inline types that were added to the safepoint.\n+  \/\/ Don't allow linking a constant oop (if available) for flat array elements\n+  \/\/ because Deoptimization::reassign_flat_array_elements needs field values.\n+  bool allow_oop = (res_type != nullptr) && !res_type->is_flat();\n+  for (uint i = 0; i < value_worklist.size(); ++i) {\n+    InlineTypeNode* vt = value_worklist.at(i)->as_InlineType();\n+    vt->make_scalar_in_safepoints(&_igvn, allow_oop);\n+  }\n@@ -946,1 +1216,2 @@\n-void PhaseMacroExpand::process_users_of_allocation(CallNode *alloc) {\n+void PhaseMacroExpand::process_users_of_allocation(CallNode *alloc, bool inline_alloc) {\n+  Unique_Node_List worklist;\n@@ -949,0 +1220,4 @@\n+    worklist.push(res);\n+  }\n+  while (worklist.size() > 0) {\n+    res = worklist.pop();\n@@ -958,10 +1233,7 @@\n-#ifdef ASSERT\n-            \/\/ Verify that there is no dependent MemBarVolatile nodes,\n-            \/\/ they should be removed during IGVN, see MemBarNode::Ideal().\n-            for (DUIterator_Fast pmax, p = n->fast_outs(pmax);\n-                                       p < pmax; p++) {\n-              Node* mb = n->fast_out(p);\n-              assert(mb->is_Initialize() || !mb->is_MemBar() ||\n-                     mb->req() <= MemBarNode::Precedent ||\n-                     mb->in(MemBarNode::Precedent) != n,\n-                     \"MemBarVolatile should be eliminated for non-escaping object\");\n+            for (DUIterator_Fast pmax, p = n->fast_outs(pmax); p < pmax; p++) {\n+              MemBarNode* mb = n->fast_out(p)->isa_MemBar();\n+              if (mb != nullptr && mb->req() <= MemBarNode::Precedent && mb->in(MemBarNode::Precedent) == n) {\n+                \/\/ MemBarVolatiles should have been removed by MemBarNode::Ideal() for non-inline allocations\n+                assert(inline_alloc, \"MemBarVolatile should be eliminated for non-escaping object\");\n+                mb->remove(&_igvn);\n+              }\n@@ -969,1 +1241,0 @@\n-#endif\n@@ -993,2 +1264,1 @@\n-          CallProjections callprojs;\n-          ac->extract_projections(&callprojs, true);\n+          CallProjections* callprojs = ac->extract_projections(true);\n@@ -996,3 +1266,3 @@\n-          _igvn.replace_node(callprojs.fallthrough_ioproj, ac->in(TypeFunc::I_O));\n-          _igvn.replace_node(callprojs.fallthrough_memproj, ac->in(TypeFunc::Memory));\n-          _igvn.replace_node(callprojs.fallthrough_catchproj, ac->in(TypeFunc::Control));\n+          _igvn.replace_node(callprojs->fallthrough_ioproj, ac->in(TypeFunc::I_O));\n+          _igvn.replace_node(callprojs->fallthrough_memproj, ac->in(TypeFunc::Memory));\n+          _igvn.replace_node(callprojs->fallthrough_catchproj, ac->in(TypeFunc::Control));\n@@ -1015,0 +1285,24 @@\n+      } else if (use->is_InlineType()) {\n+        assert(use->as_InlineType()->get_oop() == res, \"unexpected inline type ptr use\");\n+        \/\/ Cut off oop input and remove known instance id from type\n+        _igvn.rehash_node_delayed(use);\n+        use->as_InlineType()->set_oop(_igvn, _igvn.zerocon(T_OBJECT));\n+        use->as_InlineType()->set_is_buffered(_igvn, false);\n+        const TypeOopPtr* toop = _igvn.type(use)->is_oopptr()->cast_to_instance_id(TypeOopPtr::InstanceBot);\n+        _igvn.set_type(use, toop);\n+        use->as_InlineType()->set_type(toop);\n+        \/\/ Process users\n+        for (DUIterator_Fast kmax, k = use->fast_outs(kmax); k < kmax; k++) {\n+          Node* u = use->fast_out(k);\n+          if (!u->is_InlineType() && !u->is_StoreFlat()) {\n+            worklist.push(u);\n+          }\n+        }\n+      } else if (use->Opcode() == Op_StoreX && use->in(MemNode::Address) == res) {\n+        \/\/ Store to mark word of inline type larval buffer\n+        assert(inline_alloc, \"Unexpected store to mark word\");\n+        _igvn.replace_node(use, use->in(MemNode::Memory));\n+      } else if (use->Opcode() == Op_MemBarRelease || use->Opcode() == Op_MemBarStoreStore) {\n+        \/\/ Inline type buffer allocations are followed by a membar\n+        assert(inline_alloc, \"Unexpected MemBarRelease\");\n+        use->as_MemBar()->remove(&_igvn);\n@@ -1027,1 +1321,1 @@\n-  if (_callprojs.resproj != nullptr && _callprojs.resproj->outcnt() != 0) {\n+  if (_callprojs->resproj[0] != nullptr && _callprojs->resproj[0]->outcnt() != 0) {\n@@ -1031,2 +1325,2 @@\n-    for (DUIterator_Fast jmax, j = _callprojs.resproj->fast_outs(jmax);  j < jmax; j++) {\n-      Node* use = _callprojs.resproj->fast_out(j);\n+    for (DUIterator_Fast jmax, j = _callprojs->resproj[0]->fast_outs(jmax);  j < jmax; j++) {\n+      Node* use = _callprojs->resproj[0]->fast_out(j);\n@@ -1039,3 +1333,3 @@\n-    for (DUIterator_Last jmin, j = _callprojs.resproj->last_outs(jmin); j >= jmin; ) {\n-      Node* use = _callprojs.resproj->last_out(j);\n-      uint oc1 = _callprojs.resproj->outcnt();\n+    for (DUIterator_Last jmin, j = _callprojs->resproj[0]->last_outs(jmin); j >= jmin; ) {\n+      Node* use = _callprojs->resproj[0]->last_out(j);\n+      uint oc1 = _callprojs->resproj[0]->outcnt();\n@@ -1051,1 +1345,1 @@\n-          assert(tmp == nullptr || tmp == _callprojs.fallthrough_catchproj, \"allocation control projection\");\n+          assert(tmp == nullptr || tmp == _callprojs->fallthrough_catchproj, \"allocation control projection\");\n@@ -1058,1 +1352,1 @@\n-            assert(mem->as_MergeMem()->memory_at(Compile::AliasIdxRaw) == _callprojs.fallthrough_memproj, \"allocation memory projection\");\n+            assert(mem->as_MergeMem()->memory_at(Compile::AliasIdxRaw) == _callprojs->fallthrough_memproj, \"allocation memory projection\");\n@@ -1060,1 +1354,1 @@\n-            assert(mem == _callprojs.fallthrough_memproj, \"allocation memory projection\");\n+            assert(mem == _callprojs->fallthrough_memproj, \"allocation memory projection\");\n@@ -1066,0 +1360,4 @@\n+      } else if (use->Opcode() == Op_MemBarStoreStore) {\n+        \/\/ Inline type buffer allocations are followed by a membar\n+        assert(inline_alloc, \"Unexpected MemBarStoreStore\");\n+        use->as_MemBar()->remove(&_igvn);\n@@ -1069,1 +1367,1 @@\n-      j -= (oc1 - _callprojs.resproj->outcnt());\n+      j -= (oc1 - _callprojs->resproj[0]->outcnt());\n@@ -1072,2 +1370,2 @@\n-  if (_callprojs.fallthrough_catchproj != nullptr) {\n-    _igvn.replace_node(_callprojs.fallthrough_catchproj, alloc->in(TypeFunc::Control));\n+  if (_callprojs->fallthrough_catchproj != nullptr) {\n+    _igvn.replace_node(_callprojs->fallthrough_catchproj, alloc->in(TypeFunc::Control));\n@@ -1075,2 +1373,2 @@\n-  if (_callprojs.fallthrough_memproj != nullptr) {\n-    _igvn.replace_node(_callprojs.fallthrough_memproj, alloc->in(TypeFunc::Memory));\n+  if (_callprojs->fallthrough_memproj != nullptr) {\n+    _igvn.replace_node(_callprojs->fallthrough_memproj, alloc->in(TypeFunc::Memory));\n@@ -1078,2 +1376,2 @@\n-  if (_callprojs.catchall_memproj != nullptr) {\n-    _igvn.replace_node(_callprojs.catchall_memproj, C->top());\n+  if (_callprojs->catchall_memproj != nullptr) {\n+    _igvn.replace_node(_callprojs->catchall_memproj, C->top());\n@@ -1081,2 +1379,2 @@\n-  if (_callprojs.fallthrough_ioproj != nullptr) {\n-    _igvn.replace_node(_callprojs.fallthrough_ioproj, alloc->in(TypeFunc::I_O));\n+  if (_callprojs->fallthrough_ioproj != nullptr) {\n+    _igvn.replace_node(_callprojs->fallthrough_ioproj, alloc->in(TypeFunc::I_O));\n@@ -1084,2 +1382,2 @@\n-  if (_callprojs.catchall_ioproj != nullptr) {\n-    _igvn.replace_node(_callprojs.catchall_ioproj, C->top());\n+  if (_callprojs->catchall_ioproj != nullptr) {\n+    _igvn.replace_node(_callprojs->catchall_ioproj, C->top());\n@@ -1087,2 +1385,2 @@\n-  if (_callprojs.catchall_catchproj != nullptr) {\n-    _igvn.replace_node(_callprojs.catchall_catchproj, C->top());\n+  if (_callprojs->catchall_catchproj != nullptr) {\n+    _igvn.replace_node(_callprojs->catchall_catchproj, C->top());\n@@ -1098,1 +1396,1 @@\n-  if (!EliminateAllocations || !alloc->_is_non_escaping) {\n+  if (!EliminateAllocations) {\n@@ -1103,1 +1401,8 @@\n-  Node* res = alloc->result_cast();\n+\n+  \/\/ Attempt to eliminate inline type buffer allocations\n+  \/\/ regardless of usage and escape\/replaceable status.\n+  bool inline_alloc = tklass->isa_instklassptr() &&\n+                      tklass->is_instklassptr()->instance_klass()->is_inlinetype();\n+  if (!alloc->_is_non_escaping && !inline_alloc) {\n+    return false;\n+  }\n@@ -1106,1 +1411,2 @@\n-  bool boxing_alloc = C->eliminate_boxing() &&\n+  Node* res = alloc->result_cast();\n+  bool boxing_alloc = (res == nullptr) && C->eliminate_boxing() &&\n@@ -1109,1 +1415,1 @@\n-  if (!alloc->_is_scalar_replaceable && (!boxing_alloc || (res != nullptr))) {\n+  if (!alloc->_is_scalar_replaceable && !boxing_alloc && !inline_alloc) {\n@@ -1113,1 +1419,1 @@\n-  alloc->extract_projections(&_callprojs, false \/*separate_io_proj*\/, false \/*do_asserts*\/);\n+  _callprojs = alloc->extract_projections(false \/*separate_io_proj*\/, false \/*do_asserts*\/);\n@@ -1121,1 +1427,1 @@\n-    assert(res == nullptr, \"sanity\");\n+    assert(res == nullptr || inline_alloc, \"sanity\");\n@@ -1146,1 +1452,1 @@\n-  process_users_of_allocation(alloc);\n+  process_users_of_allocation(alloc, inline_alloc);\n@@ -1168,1 +1474,1 @@\n-  boxing->extract_projections(&_callprojs, false \/*separate_io_proj*\/, false \/*do_asserts*\/);\n+  _callprojs = boxing->extract_projections(false \/*separate_io_proj*\/, false \/*do_asserts*\/);\n@@ -1170,1 +1476,1 @@\n-  const TypeTuple* r = boxing->tf()->range();\n+  const TypeTuple* r = boxing->tf()->range_sig();\n@@ -1274,0 +1580,1 @@\n+            Node* init_val, \/\/ value to initialize the array with\n@@ -1356,1 +1663,1 @@\n-    Node *toobig_true = new IfTrueNode( toobig_iff );\n+    Node* toobig_true = new IfTrueNode(toobig_iff);\n@@ -1359,1 +1666,1 @@\n-    toobig_false = new IfFalseNode( toobig_iff );\n+    toobig_false = new IfFalseNode(toobig_iff);\n@@ -1398,0 +1705,1 @@\n+\n@@ -1455,0 +1763,6 @@\n+    if (init_val != nullptr) {\n+      call->init_req(TypeFunc::Parms+2, init_val);\n+    }\n+  } else {\n+    \/\/ Let the runtime know if this is a larval allocation\n+    call->init_req(TypeFunc::Parms+1, _igvn.intcon(alloc->_larval));\n@@ -1486,1 +1800,1 @@\n-  call->extract_projections(&_callprojs, false \/*separate_io_proj*\/, false \/*do_asserts*\/);\n+  _callprojs = call->extract_projections(false \/*separate_io_proj*\/, false \/*do_asserts*\/);\n@@ -1492,2 +1806,2 @@\n-  if (expand_fast_path && _callprojs.fallthrough_memproj != nullptr) {\n-    migrate_outs(_callprojs.fallthrough_memproj, result_phi_rawmem);\n+  if (expand_fast_path && _callprojs->fallthrough_memproj != nullptr) {\n+    _igvn.replace_in_uses(_callprojs->fallthrough_memproj, result_phi_rawmem);\n@@ -1497,4 +1811,4 @@\n-  if (_callprojs.catchall_memproj != nullptr ) {\n-    if (_callprojs.fallthrough_memproj == nullptr) {\n-      _callprojs.fallthrough_memproj = new ProjNode(call, TypeFunc::Memory);\n-      transform_later(_callprojs.fallthrough_memproj);\n+  if (_callprojs->catchall_memproj != nullptr) {\n+    if (_callprojs->fallthrough_memproj == nullptr) {\n+      _callprojs->fallthrough_memproj = new ProjNode(call, TypeFunc::Memory);\n+      transform_later(_callprojs->fallthrough_memproj);\n@@ -1502,2 +1816,2 @@\n-    migrate_outs(_callprojs.catchall_memproj, _callprojs.fallthrough_memproj);\n-    _igvn.remove_dead_node(_callprojs.catchall_memproj);\n+    _igvn.replace_in_uses(_callprojs->catchall_memproj, _callprojs->fallthrough_memproj);\n+    _igvn.remove_dead_node(_callprojs->catchall_memproj);\n@@ -1511,2 +1825,2 @@\n-  if (_callprojs.fallthrough_ioproj != nullptr) {\n-    migrate_outs(_callprojs.fallthrough_ioproj, result_phi_i_o);\n+  if (_callprojs->fallthrough_ioproj != nullptr) {\n+    _igvn.replace_in_uses(_callprojs->fallthrough_ioproj, result_phi_i_o);\n@@ -1516,4 +1830,4 @@\n-  if (_callprojs.catchall_ioproj != nullptr ) {\n-    if (_callprojs.fallthrough_ioproj == nullptr) {\n-      _callprojs.fallthrough_ioproj = new ProjNode(call, TypeFunc::I_O);\n-      transform_later(_callprojs.fallthrough_ioproj);\n+  if (_callprojs->catchall_ioproj != nullptr) {\n+    if (_callprojs->fallthrough_ioproj == nullptr) {\n+      _callprojs->fallthrough_ioproj = new ProjNode(call, TypeFunc::I_O);\n+      transform_later(_callprojs->fallthrough_ioproj);\n@@ -1521,2 +1835,2 @@\n-    migrate_outs(_callprojs.catchall_ioproj, _callprojs.fallthrough_ioproj);\n-    _igvn.remove_dead_node(_callprojs.catchall_ioproj);\n+    _igvn.replace_in_uses(_callprojs->catchall_ioproj, _callprojs->fallthrough_ioproj);\n+    _igvn.remove_dead_node(_callprojs->catchall_ioproj);\n@@ -1541,2 +1855,2 @@\n-  if (_callprojs.fallthrough_catchproj != nullptr) {\n-    ctrl = _callprojs.fallthrough_catchproj->clone();\n+  if (_callprojs->fallthrough_catchproj != nullptr) {\n+    ctrl = _callprojs->fallthrough_catchproj->clone();\n@@ -1544,1 +1858,1 @@\n-    _igvn.replace_node(_callprojs.fallthrough_catchproj, result_region);\n+    _igvn.replace_node(_callprojs->fallthrough_catchproj, result_region);\n@@ -1549,1 +1863,1 @@\n-  if (_callprojs.resproj == nullptr) {\n+  if (_callprojs->resproj[0] == nullptr) {\n@@ -1553,1 +1867,1 @@\n-    slow_result = _callprojs.resproj->clone();\n+    slow_result = _callprojs->resproj[0]->clone();\n@@ -1555,1 +1869,1 @@\n-    _igvn.replace_node(_callprojs.resproj, result_phi_rawoop);\n+    _igvn.replace_node(_callprojs->resproj[0], result_phi_rawoop);\n@@ -1565,1 +1879,1 @@\n-  result_phi_rawmem->init_req(slow_result_path, _callprojs.fallthrough_memproj);\n+  result_phi_rawmem->init_req(slow_result_path, _callprojs->fallthrough_memproj);\n@@ -1577,4 +1891,4 @@\n-  alloc->extract_projections(&_callprojs, false \/*separate_io_proj*\/, false \/*do_asserts*\/);\n-  if (_callprojs.resproj != nullptr) {\n-    for (DUIterator_Fast imax, i = _callprojs.resproj->fast_outs(imax); i < imax; i++) {\n-      Node* use = _callprojs.resproj->fast_out(i);\n+  _callprojs = alloc->extract_projections(false \/*separate_io_proj*\/, false \/*do_asserts*\/);\n+  if (_callprojs->resproj[0] != nullptr) {\n+    for (DUIterator_Fast imax, i = _callprojs->resproj[0]->fast_outs(imax); i < imax; i++) {\n+      Node* use = _callprojs->resproj[0]->fast_out(i);\n@@ -1585,2 +1899,2 @@\n-    assert(_callprojs.resproj->outcnt() == 0, \"all uses must be deleted\");\n-    _igvn.remove_dead_node(_callprojs.resproj);\n+    assert(_callprojs->resproj[0]->outcnt() == 0, \"all uses must be deleted\");\n+    _igvn.remove_dead_node(_callprojs->resproj[0]);\n@@ -1588,3 +1902,3 @@\n-  if (_callprojs.fallthrough_catchproj != nullptr) {\n-    migrate_outs(_callprojs.fallthrough_catchproj, ctrl);\n-    _igvn.remove_dead_node(_callprojs.fallthrough_catchproj);\n+  if (_callprojs->fallthrough_catchproj != nullptr) {\n+    _igvn.replace_in_uses(_callprojs->fallthrough_catchproj, ctrl);\n+    _igvn.remove_dead_node(_callprojs->fallthrough_catchproj);\n@@ -1592,3 +1906,3 @@\n-  if (_callprojs.catchall_catchproj != nullptr) {\n-    _igvn.rehash_node_delayed(_callprojs.catchall_catchproj);\n-    _callprojs.catchall_catchproj->set_req(0, top());\n+  if (_callprojs->catchall_catchproj != nullptr) {\n+    _igvn.rehash_node_delayed(_callprojs->catchall_catchproj);\n+    _callprojs->catchall_catchproj->set_req(0, top());\n@@ -1596,2 +1910,2 @@\n-  if (_callprojs.fallthrough_proj != nullptr) {\n-    Node* catchnode = _callprojs.fallthrough_proj->unique_ctrl_out();\n+  if (_callprojs->fallthrough_proj != nullptr) {\n+    Node* catchnode = _callprojs->fallthrough_proj->unique_ctrl_out();\n@@ -1599,1 +1913,1 @@\n-    _igvn.remove_dead_node(_callprojs.fallthrough_proj);\n+    _igvn.remove_dead_node(_callprojs->fallthrough_proj);\n@@ -1601,3 +1915,3 @@\n-  if (_callprojs.fallthrough_memproj != nullptr) {\n-    migrate_outs(_callprojs.fallthrough_memproj, mem);\n-    _igvn.remove_dead_node(_callprojs.fallthrough_memproj);\n+  if (_callprojs->fallthrough_memproj != nullptr) {\n+    _igvn.replace_in_uses(_callprojs->fallthrough_memproj, mem);\n+    _igvn.remove_dead_node(_callprojs->fallthrough_memproj);\n@@ -1605,3 +1919,3 @@\n-  if (_callprojs.fallthrough_ioproj != nullptr) {\n-    migrate_outs(_callprojs.fallthrough_ioproj, i_o);\n-    _igvn.remove_dead_node(_callprojs.fallthrough_ioproj);\n+  if (_callprojs->fallthrough_ioproj != nullptr) {\n+    _igvn.replace_in_uses(_callprojs->fallthrough_ioproj, i_o);\n+    _igvn.remove_dead_node(_callprojs->fallthrough_ioproj);\n@@ -1609,3 +1923,3 @@\n-  if (_callprojs.catchall_memproj != nullptr) {\n-    _igvn.rehash_node_delayed(_callprojs.catchall_memproj);\n-    _callprojs.catchall_memproj->set_req(0, top());\n+  if (_callprojs->catchall_memproj != nullptr) {\n+    _igvn.rehash_node_delayed(_callprojs->catchall_memproj);\n+    _callprojs->catchall_memproj->set_req(0, top());\n@@ -1613,3 +1927,3 @@\n-  if (_callprojs.catchall_ioproj != nullptr) {\n-    _igvn.rehash_node_delayed(_callprojs.catchall_ioproj);\n-    _callprojs.catchall_ioproj->set_req(0, top());\n+  if (_callprojs->catchall_ioproj != nullptr) {\n+    _igvn.rehash_node_delayed(_callprojs->catchall_ioproj);\n+    _callprojs->catchall_ioproj->set_req(0, top());\n@@ -1749,5 +2063,4 @@\n-Node*\n-PhaseMacroExpand::initialize_object(AllocateNode* alloc,\n-                                    Node* control, Node* rawmem, Node* object,\n-                                    Node* klass_node, Node* length,\n-                                    Node* size_in_bytes) {\n+Node* PhaseMacroExpand::initialize_object(AllocateNode* alloc,\n+                                          Node* control, Node* rawmem, Node* object,\n+                                          Node* klass_node, Node* length,\n+                                          Node* size_in_bytes) {\n@@ -1756,1 +2069,1 @@\n-  Node* mark_node = alloc->make_ideal_mark(&_igvn, object, control, rawmem);\n+  Node* mark_node = alloc->make_ideal_mark(&_igvn, control, rawmem);\n@@ -1794,0 +2107,2 @@\n+                                            alloc->in(AllocateNode::InitValue),\n+                                            alloc->in(AllocateNode::RawInitValue),\n@@ -1968,1 +2283,1 @@\n-  expand_allocate_common(alloc, nullptr,\n+  expand_allocate_common(alloc, nullptr, nullptr,\n@@ -1978,0 +2293,1 @@\n+  Node* init_value = alloc->in(AllocateNode::InitValue);\n@@ -1979,0 +2295,3 @@\n+  assert(!ary_klass_t || !ary_klass_t->klass_is_exact() || !ary_klass_t->exact_klass()->is_obj_array_klass() ||\n+         ary_klass_t->is_refined_type(), \"Must be a refined array klass\");\n+  const TypeFunc* slow_call_type;\n@@ -1985,0 +2304,1 @@\n+    slow_call_type = OptoRuntime::new_array_nozero_Type();\n@@ -1987,0 +2307,7 @@\n+    slow_call_type = OptoRuntime::new_array_Type();\n+\n+    if (init_value == nullptr) {\n+      init_value = _igvn.zerocon(T_OBJECT);\n+    } else if (UseCompressedOops) {\n+      init_value = transform_later(new DecodeNNode(init_value, init_value->bottom_type()->make_ptr()));\n+    }\n@@ -1988,2 +2315,2 @@\n-  expand_allocate_common(alloc, length,\n-                         OptoRuntime::new_array_Type(),\n+  expand_allocate_common(alloc, length, init_value,\n+                         slow_call_type,\n@@ -2201,1 +2528,1 @@\n-  alock->extract_projections(&_callprojs, false \/*separate_io_proj*\/, false \/*do_asserts*\/);\n+  _callprojs = alock->extract_projections(false \/*separate_io_proj*\/, false \/*do_asserts*\/);\n@@ -2205,2 +2532,2 @@\n-         _callprojs.fallthrough_proj != nullptr &&\n-         _callprojs.fallthrough_memproj != nullptr,\n+         _callprojs->fallthrough_proj != nullptr &&\n+         _callprojs->fallthrough_memproj != nullptr,\n@@ -2209,2 +2536,2 @@\n-  Node* fallthroughproj = _callprojs.fallthrough_proj;\n-  Node* memproj_fallthrough = _callprojs.fallthrough_memproj;\n+  Node* fallthroughproj = _callprojs->fallthrough_proj;\n+  Node* memproj_fallthrough = _callprojs->fallthrough_memproj;\n@@ -2281,1 +2608,1 @@\n-  call->extract_projections(&_callprojs, false \/*separate_io_proj*\/, false \/*do_asserts*\/);\n+  _callprojs = call->extract_projections(false \/*separate_io_proj*\/, false \/*do_asserts*\/);\n@@ -2287,2 +2614,2 @@\n-  assert(_callprojs.fallthrough_ioproj == nullptr && _callprojs.catchall_ioproj == nullptr &&\n-         _callprojs.catchall_memproj == nullptr && _callprojs.catchall_catchproj == nullptr, \"Unexpected projection from Lock\");\n+  assert(_callprojs->fallthrough_ioproj == nullptr && _callprojs->catchall_ioproj == nullptr &&\n+         _callprojs->catchall_memproj == nullptr && _callprojs->catchall_catchproj == nullptr, \"Unexpected projection from Lock\");\n@@ -2293,1 +2620,1 @@\n-  Node *slow_ctrl = _callprojs.fallthrough_proj->clone();\n+  Node *slow_ctrl = _callprojs->fallthrough_proj->clone();\n@@ -2295,2 +2622,2 @@\n-  _igvn.hash_delete(_callprojs.fallthrough_proj);\n-  _callprojs.fallthrough_proj->disconnect_inputs(C);\n+  _igvn.hash_delete(_callprojs->fallthrough_proj);\n+  _callprojs->fallthrough_proj->disconnect_inputs(C);\n@@ -2300,1 +2627,1 @@\n-  _igvn.replace_node(_callprojs.fallthrough_proj, region);\n+  _igvn.replace_node(_callprojs->fallthrough_proj, region);\n@@ -2308,1 +2635,1 @@\n-  _igvn.replace_node(_callprojs.fallthrough_memproj, mem_phi);\n+  _igvn.replace_node(_callprojs->fallthrough_memproj, mem_phi);\n@@ -2341,3 +2668,3 @@\n-  call->extract_projections(&_callprojs, false \/*separate_io_proj*\/, false \/*do_asserts*\/);\n-  assert(_callprojs.fallthrough_ioproj == nullptr && _callprojs.catchall_ioproj == nullptr &&\n-         _callprojs.catchall_memproj == nullptr && _callprojs.catchall_catchproj == nullptr, \"Unexpected projection from Lock\");\n+  _callprojs = call->extract_projections(false \/*separate_io_proj*\/, false \/*do_asserts*\/);\n+  assert(_callprojs->fallthrough_ioproj == nullptr && _callprojs->catchall_ioproj == nullptr &&\n+         _callprojs->catchall_memproj == nullptr && _callprojs->catchall_catchproj == nullptr, \"Unexpected projection from Lock\");\n@@ -2349,1 +2676,1 @@\n-  Node *slow_ctrl = _callprojs.fallthrough_proj->clone();\n+  Node *slow_ctrl = _callprojs->fallthrough_proj->clone();\n@@ -2351,2 +2678,2 @@\n-  _igvn.hash_delete(_callprojs.fallthrough_proj);\n-  _callprojs.fallthrough_proj->disconnect_inputs(C);\n+  _igvn.hash_delete(_callprojs->fallthrough_proj);\n+  _callprojs->fallthrough_proj->disconnect_inputs(C);\n@@ -2356,1 +2683,1 @@\n-  _igvn.replace_node(_callprojs.fallthrough_proj, region);\n+  _igvn.replace_node(_callprojs->fallthrough_proj, region);\n@@ -2363,1 +2690,1 @@\n-  _igvn.replace_node(_callprojs.fallthrough_memproj, mem_phi);\n+  _igvn.replace_node(_callprojs->fallthrough_memproj, mem_phi);\n@@ -2366,0 +2693,222 @@\n+\/\/ An inline type might be returned from the call but we don't know its\n+\/\/ type. Either we get a buffered inline type (and nothing needs to be done)\n+\/\/ or one of the values being returned is the klass of the inline type\n+\/\/ and we need to allocate an inline type instance of that type and\n+\/\/ initialize it with other values being returned. In that case, we\n+\/\/ first try a fast path allocation and initialize the value with the\n+\/\/ inline klass's pack handler or we fall back to a runtime call.\n+void PhaseMacroExpand::expand_mh_intrinsic_return(CallStaticJavaNode* call) {\n+  assert(call->method()->is_method_handle_intrinsic(), \"must be a method handle intrinsic call\");\n+  Node* ret = call->proj_out_or_null(TypeFunc::Parms);\n+  if (ret == nullptr) {\n+    return;\n+  }\n+  const TypeFunc* tf = call->_tf;\n+  const TypeTuple* domain = OptoRuntime::store_inline_type_fields_Type()->domain_cc();\n+  const TypeFunc* new_tf = TypeFunc::make(tf->domain_sig(), tf->domain_cc(), tf->range_sig(), domain);\n+  call->_tf = new_tf;\n+  \/\/ Make sure the change of type is applied before projections are processed by igvn\n+  _igvn.set_type(call, call->Value(&_igvn));\n+  _igvn.set_type(ret, ret->Value(&_igvn));\n+\n+  \/\/ Before any new projection is added:\n+  CallProjections* projs = call->extract_projections(true, true);\n+\n+  \/\/ Create temporary hook nodes that will be replaced below.\n+  \/\/ Add an input to prevent hook nodes from being dead.\n+  Node* ctl = new Node(call);\n+  Node* mem = new Node(ctl);\n+  Node* io = new Node(ctl);\n+  Node* ex_ctl = new Node(ctl);\n+  Node* ex_mem = new Node(ctl);\n+  Node* ex_io = new Node(ctl);\n+  Node* res = new Node(ctl);\n+\n+  \/\/ Allocate a new buffered inline type only if a new one is not returned\n+  Node* cast = transform_later(new CastP2XNode(ctl, res));\n+  Node* mask = MakeConX(0x1);\n+  Node* masked = transform_later(new AndXNode(cast, mask));\n+  Node* cmp = transform_later(new CmpXNode(masked, mask));\n+  Node* bol = transform_later(new BoolNode(cmp, BoolTest::eq));\n+  IfNode* allocation_iff = new IfNode(ctl, bol, PROB_MAX, COUNT_UNKNOWN);\n+  transform_later(allocation_iff);\n+  Node* allocation_ctl = transform_later(new IfTrueNode(allocation_iff));\n+  Node* no_allocation_ctl = transform_later(new IfFalseNode(allocation_iff));\n+  Node* no_allocation_res = transform_later(new CheckCastPPNode(no_allocation_ctl, res, TypeInstPtr::BOTTOM));\n+\n+  \/\/ Try to allocate a new buffered inline instance either from TLAB or eden space\n+  Node* needgc_ctrl = nullptr; \/\/ needgc means slowcase, i.e. allocation failed\n+  CallLeafNoFPNode* handler_call;\n+  const bool alloc_in_place = UseTLAB;\n+  if (alloc_in_place) {\n+    Node* fast_oop_ctrl = nullptr;\n+    Node* fast_oop_rawmem = nullptr;\n+    Node* mask2 = MakeConX(-2);\n+    Node* masked2 = transform_later(new AndXNode(cast, mask2));\n+    Node* rawklassptr = transform_later(new CastX2PNode(masked2));\n+    Node* klass_node = transform_later(new CheckCastPPNode(allocation_ctl, rawklassptr, TypeInstKlassPtr::OBJECT_OR_NULL));\n+    Node* layout_val = make_load(nullptr, mem, klass_node, in_bytes(Klass::layout_helper_offset()), TypeInt::INT, T_INT);\n+    Node* size_in_bytes = ConvI2X(layout_val);\n+    BarrierSetC2* bs = BarrierSet::barrier_set()->barrier_set_c2();\n+    Node* fast_oop = bs->obj_allocate(this, mem, allocation_ctl, size_in_bytes, io, needgc_ctrl,\n+                                      fast_oop_ctrl, fast_oop_rawmem,\n+                                      AllocateInstancePrefetchLines);\n+    \/\/ Allocation succeed, initialize buffered inline instance header firstly,\n+    \/\/ and then initialize its fields with an inline class specific handler\n+    Node* mark_word_node;\n+    if (UseCompactObjectHeaders) {\n+      \/\/ COH: We need to load the prototype from the klass at runtime since it encodes the klass pointer already.\n+      mark_word_node = make_load(fast_oop_ctrl, fast_oop_rawmem, klass_node, in_bytes(Klass::prototype_header_offset()), TypeRawPtr::BOTTOM, T_ADDRESS);\n+    } else {\n+      \/\/ Otherwise, use the static prototype.\n+      mark_word_node = makecon(TypeRawPtr::make((address)markWord::inline_type_prototype().value()));\n+    }\n+\n+    fast_oop_rawmem = make_store(fast_oop_ctrl, fast_oop_rawmem, fast_oop, oopDesc::mark_offset_in_bytes(), mark_word_node, T_ADDRESS);\n+    if (!UseCompactObjectHeaders) {\n+      \/\/ COH: Everything is encoded in the mark word, so nothing left to do.\n+      fast_oop_rawmem = make_store(fast_oop_ctrl, fast_oop_rawmem, fast_oop, oopDesc::klass_offset_in_bytes(), klass_node, T_METADATA);\n+      if (UseCompressedClassPointers) {\n+        fast_oop_rawmem = make_store(fast_oop_ctrl, fast_oop_rawmem, fast_oop, oopDesc::klass_gap_offset_in_bytes(), intcon(0), T_INT);\n+      }\n+    }\n+    Node* members  = make_load(fast_oop_ctrl, fast_oop_rawmem, klass_node, in_bytes(InlineKlass::adr_members_offset()), TypeRawPtr::BOTTOM, T_ADDRESS);\n+    Node* pack_handler = make_load(fast_oop_ctrl, fast_oop_rawmem, members, in_bytes(InlineKlass::pack_handler_offset()), TypeRawPtr::BOTTOM, T_ADDRESS);\n+    handler_call = new CallLeafNoFPNode(OptoRuntime::pack_inline_type_Type(),\n+                                        nullptr,\n+                                        \"pack handler\",\n+                                        TypeRawPtr::BOTTOM);\n+    handler_call->init_req(TypeFunc::Control, fast_oop_ctrl);\n+    handler_call->init_req(TypeFunc::Memory, fast_oop_rawmem);\n+    handler_call->init_req(TypeFunc::I_O, top());\n+    handler_call->init_req(TypeFunc::FramePtr, call->in(TypeFunc::FramePtr));\n+    handler_call->init_req(TypeFunc::ReturnAdr, top());\n+    handler_call->init_req(TypeFunc::Parms, pack_handler);\n+    handler_call->init_req(TypeFunc::Parms+1, fast_oop);\n+  } else {\n+    needgc_ctrl = allocation_ctl;\n+  }\n+\n+  \/\/ Allocation failed, fall back to a runtime call\n+  CallStaticJavaNode* slow_call = new CallStaticJavaNode(OptoRuntime::store_inline_type_fields_Type(),\n+                                                         StubRoutines::store_inline_type_fields_to_buf(),\n+                                                         \"store_inline_type_fields\",\n+                                                         TypePtr::BOTTOM);\n+  slow_call->init_req(TypeFunc::Control, needgc_ctrl);\n+  slow_call->init_req(TypeFunc::Memory, mem);\n+  slow_call->init_req(TypeFunc::I_O, io);\n+  slow_call->init_req(TypeFunc::FramePtr, call->in(TypeFunc::FramePtr));\n+  slow_call->init_req(TypeFunc::ReturnAdr, call->in(TypeFunc::ReturnAdr));\n+  slow_call->init_req(TypeFunc::Parms, res);\n+\n+  Node* slow_ctl = transform_later(new ProjNode(slow_call, TypeFunc::Control));\n+  Node* slow_mem = transform_later(new ProjNode(slow_call, TypeFunc::Memory));\n+  Node* slow_io = transform_later(new ProjNode(slow_call, TypeFunc::I_O));\n+  Node* slow_res = transform_later(new ProjNode(slow_call, TypeFunc::Parms));\n+  Node* slow_catc = transform_later(new CatchNode(slow_ctl, slow_io, 2));\n+  Node* slow_norm = transform_later(new CatchProjNode(slow_catc, CatchProjNode::fall_through_index, CatchProjNode::no_handler_bci));\n+  Node* slow_excp = transform_later(new CatchProjNode(slow_catc, CatchProjNode::catch_all_index,    CatchProjNode::no_handler_bci));\n+\n+  Node* ex_r = new RegionNode(3);\n+  Node* ex_mem_phi = new PhiNode(ex_r, Type::MEMORY, TypePtr::BOTTOM);\n+  Node* ex_io_phi = new PhiNode(ex_r, Type::ABIO);\n+  ex_r->init_req(1, slow_excp);\n+  ex_mem_phi->init_req(1, slow_mem);\n+  ex_io_phi->init_req(1, slow_io);\n+  ex_r->init_req(2, ex_ctl);\n+  ex_mem_phi->init_req(2, ex_mem);\n+  ex_io_phi->init_req(2, ex_io);\n+  transform_later(ex_r);\n+  transform_later(ex_mem_phi);\n+  transform_later(ex_io_phi);\n+\n+  \/\/ We don't know how many values are returned. This assumes the\n+  \/\/ worst case, that all available registers are used.\n+  for (uint i = TypeFunc::Parms+1; i < domain->cnt(); i++) {\n+    if (domain->field_at(i) == Type::HALF) {\n+      slow_call->init_req(i, top());\n+      if (alloc_in_place) {\n+        handler_call->init_req(i+1, top());\n+      }\n+      continue;\n+    }\n+    Node* proj = transform_later(new ProjNode(call, i));\n+    slow_call->init_req(i, proj);\n+    if (alloc_in_place) {\n+      handler_call->init_req(i+1, proj);\n+    }\n+  }\n+  \/\/ We can safepoint at that new call\n+  slow_call->copy_call_debug_info(&_igvn, call);\n+  transform_later(slow_call);\n+  if (alloc_in_place) {\n+    transform_later(handler_call);\n+  }\n+\n+  Node* fast_ctl = nullptr;\n+  Node* fast_res = nullptr;\n+  MergeMemNode* fast_mem = nullptr;\n+  if (alloc_in_place) {\n+    fast_ctl = transform_later(new ProjNode(handler_call, TypeFunc::Control));\n+    Node* rawmem = transform_later(new ProjNode(handler_call, TypeFunc::Memory));\n+    fast_res = transform_later(new ProjNode(handler_call, TypeFunc::Parms));\n+    fast_mem = MergeMemNode::make(mem);\n+    fast_mem->set_memory_at(Compile::AliasIdxRaw, rawmem);\n+    transform_later(fast_mem);\n+  }\n+\n+  Node* r = new RegionNode(alloc_in_place ? 4 : 3);\n+  Node* mem_phi = new PhiNode(r, Type::MEMORY, TypePtr::BOTTOM);\n+  Node* io_phi = new PhiNode(r, Type::ABIO);\n+  Node* res_phi = new PhiNode(r, TypeInstPtr::BOTTOM);\n+  r->init_req(1, no_allocation_ctl);\n+  mem_phi->init_req(1, mem);\n+  io_phi->init_req(1, io);\n+  res_phi->init_req(1, no_allocation_res);\n+  r->init_req(2, slow_norm);\n+  mem_phi->init_req(2, slow_mem);\n+  io_phi->init_req(2, slow_io);\n+  res_phi->init_req(2, slow_res);\n+  if (alloc_in_place) {\n+    r->init_req(3, fast_ctl);\n+    mem_phi->init_req(3, fast_mem);\n+    io_phi->init_req(3, io);\n+    res_phi->init_req(3, fast_res);\n+  }\n+  transform_later(r);\n+  transform_later(mem_phi);\n+  transform_later(io_phi);\n+  transform_later(res_phi);\n+\n+  \/\/ Do not let stores that initialize this buffer be reordered with a subsequent\n+  \/\/ store that would make this buffer accessible by other threads.\n+  MemBarNode* mb = MemBarNode::make(C, Op_MemBarStoreStore, Compile::AliasIdxBot);\n+  transform_later(mb);\n+  mb->init_req(TypeFunc::Memory, mem_phi);\n+  mb->init_req(TypeFunc::Control, r);\n+  r = new ProjNode(mb, TypeFunc::Control);\n+  transform_later(r);\n+  mem_phi = new ProjNode(mb, TypeFunc::Memory);\n+  transform_later(mem_phi);\n+\n+  assert(projs->nb_resproj == 1, \"unexpected number of results\");\n+  _igvn.replace_in_uses(projs->fallthrough_catchproj, r);\n+  _igvn.replace_in_uses(projs->fallthrough_memproj, mem_phi);\n+  _igvn.replace_in_uses(projs->fallthrough_ioproj, io_phi);\n+  _igvn.replace_in_uses(projs->resproj[0], res_phi);\n+  _igvn.replace_in_uses(projs->catchall_catchproj, ex_r);\n+  _igvn.replace_in_uses(projs->catchall_memproj, ex_mem_phi);\n+  _igvn.replace_in_uses(projs->catchall_ioproj, ex_io_phi);\n+  \/\/ The CatchNode should not use the ex_io_phi. Re-connect it to the catchall_ioproj.\n+  Node* cn = projs->fallthrough_catchproj->in(0);\n+  _igvn.replace_input_of(cn, 1, projs->catchall_ioproj);\n+\n+  _igvn.replace_node(ctl, projs->fallthrough_catchproj);\n+  _igvn.replace_node(mem, projs->fallthrough_memproj);\n+  _igvn.replace_node(io, projs->fallthrough_ioproj);\n+  _igvn.replace_node(res, projs->resproj[0]);\n+  _igvn.replace_node(ex_ctl, projs->catchall_catchproj);\n+  _igvn.replace_node(ex_mem, projs->catchall_memproj);\n+  _igvn.replace_node(ex_io, projs->catchall_ioproj);\n+ }\n+\n@@ -2391,1 +2940,1 @@\n-      subklass = _igvn.transform(LoadKlassNode::make(_igvn, C->immutable_memory(), k_adr, TypeInstPtr::KLASS));\n+      subklass = _igvn.transform(LoadKlassNode::make(_igvn, C->immutable_memory(), k_adr, TypeInstPtr::KLASS, TypeInstKlassPtr::OBJECT));\n@@ -2403,0 +2952,113 @@\n+\/\/ FlatArrayCheckNode (array1 array2 ...) is expanded into:\n+\/\/\n+\/\/ long mark = array1.mark | array2.mark | ...;\n+\/\/ long locked_bit = markWord::unlocked_value & array1.mark & array2.mark & ...;\n+\/\/ if (locked_bit == 0) {\n+\/\/   \/\/ One array is locked, load prototype header from the klass\n+\/\/   mark = array1.klass.proto | array2.klass.proto | ...\n+\/\/ }\n+\/\/ if ((mark & markWord::flat_array_bit_in_place) == 0) {\n+\/\/    ...\n+\/\/ }\n+void PhaseMacroExpand::expand_flatarraycheck_node(FlatArrayCheckNode* check) {\n+  bool array_inputs = _igvn.type(check->in(FlatArrayCheckNode::ArrayOrKlass))->isa_oopptr() != nullptr;\n+  if (array_inputs) {\n+    Node* mark = MakeConX(0);\n+    Node* locked_bit = MakeConX(markWord::unlocked_value);\n+    Node* mem = check->in(FlatArrayCheckNode::Memory);\n+    for (uint i = FlatArrayCheckNode::ArrayOrKlass; i < check->req(); ++i) {\n+      Node* ary = check->in(i);\n+      const TypeOopPtr* t = _igvn.type(ary)->isa_oopptr();\n+      assert(t != nullptr, \"Mixing array and klass inputs\");\n+      assert(!t->is_flat() && !t->is_not_flat(), \"Should have been optimized out\");\n+      Node* mark_adr = basic_plus_adr(ary, oopDesc::mark_offset_in_bytes());\n+      Node* mark_load = _igvn.transform(LoadNode::make(_igvn, nullptr, mem, mark_adr, mark_adr->bottom_type()->is_ptr(), TypeX_X, TypeX_X->basic_type(), MemNode::unordered));\n+      mark = _igvn.transform(new OrXNode(mark, mark_load));\n+      locked_bit = _igvn.transform(new AndXNode(locked_bit, mark_load));\n+    }\n+    assert(!mark->is_Con(), \"Should have been optimized out\");\n+    Node* cmp = _igvn.transform(new CmpXNode(locked_bit, MakeConX(0)));\n+    Node* is_unlocked = _igvn.transform(new BoolNode(cmp, BoolTest::ne));\n+\n+    \/\/ BoolNode might be shared, replace each if user\n+    Node* old_bol = check->unique_out();\n+    assert(old_bol->is_Bool() && old_bol->as_Bool()->_test._test == BoolTest::ne, \"unexpected condition\");\n+    for (DUIterator_Last imin, i = old_bol->last_outs(imin); i >= imin; --i) {\n+      IfNode* old_iff = old_bol->last_out(i)->as_If();\n+      Node* ctrl = old_iff->in(0);\n+      RegionNode* region = new RegionNode(3);\n+      Node* mark_phi = new PhiNode(region, TypeX_X);\n+\n+      \/\/ Check if array is unlocked\n+      IfNode* iff = _igvn.transform(new IfNode(ctrl, is_unlocked, PROB_MAX, COUNT_UNKNOWN))->as_If();\n+\n+      \/\/ Unlocked: Use bits from mark word\n+      region->init_req(1, _igvn.transform(new IfTrueNode(iff)));\n+      mark_phi->init_req(1, mark);\n+\n+      \/\/ Locked: Load prototype header from klass\n+      ctrl = _igvn.transform(new IfFalseNode(iff));\n+      Node* proto = MakeConX(0);\n+      for (uint i = FlatArrayCheckNode::ArrayOrKlass; i < check->req(); ++i) {\n+        Node* ary = check->in(i);\n+        \/\/ Make loads control dependent to make sure they are only executed if array is locked\n+        Node* klass_adr = basic_plus_adr(ary, oopDesc::klass_offset_in_bytes());\n+        Node* klass = _igvn.transform(LoadKlassNode::make(_igvn, C->immutable_memory(), klass_adr, TypeInstPtr::KLASS, TypeInstKlassPtr::OBJECT));\n+        Node* proto_adr = basic_plus_adr(klass, in_bytes(Klass::prototype_header_offset()));\n+        Node* proto_load = _igvn.transform(LoadNode::make(_igvn, ctrl, C->immutable_memory(), proto_adr, proto_adr->bottom_type()->is_ptr(), TypeX_X, TypeX_X->basic_type(), MemNode::unordered));\n+        proto = _igvn.transform(new OrXNode(proto, proto_load));\n+      }\n+      region->init_req(2, ctrl);\n+      mark_phi->init_req(2, proto);\n+\n+      \/\/ Check if flat array bits are set\n+      Node* mask = MakeConX(markWord::flat_array_bit_in_place);\n+      Node* masked = _igvn.transform(new AndXNode(_igvn.transform(mark_phi), mask));\n+      cmp = _igvn.transform(new CmpXNode(masked, MakeConX(0)));\n+      Node* is_not_flat = _igvn.transform(new BoolNode(cmp, BoolTest::eq));\n+\n+      ctrl = _igvn.transform(region);\n+      iff = _igvn.transform(new IfNode(ctrl, is_not_flat, PROB_MAX, COUNT_UNKNOWN))->as_If();\n+      _igvn.replace_node(old_iff, iff);\n+    }\n+    _igvn.replace_node(check, C->top());\n+  } else {\n+    \/\/ Fall back to layout helper check\n+    Node* lhs = intcon(0);\n+    for (uint i = FlatArrayCheckNode::ArrayOrKlass; i < check->req(); ++i) {\n+      Node* array_or_klass = check->in(i);\n+      Node* klass = nullptr;\n+      const TypePtr* t = _igvn.type(array_or_klass)->is_ptr();\n+      assert(!t->is_flat() && !t->is_not_flat(), \"Should have been optimized out\");\n+      if (t->isa_oopptr() != nullptr) {\n+        Node* klass_adr = basic_plus_adr(array_or_klass, oopDesc::klass_offset_in_bytes());\n+        klass = transform_later(LoadKlassNode::make(_igvn, C->immutable_memory(), klass_adr, TypeInstPtr::KLASS, TypeInstKlassPtr::OBJECT));\n+      } else {\n+        assert(t->isa_klassptr(), \"Unexpected input type\");\n+        klass = array_or_klass;\n+      }\n+      Node* lh_addr = basic_plus_adr(klass, in_bytes(Klass::layout_helper_offset()));\n+      Node* lh_val = _igvn.transform(LoadNode::make(_igvn, nullptr, C->immutable_memory(), lh_addr, lh_addr->bottom_type()->is_ptr(), TypeInt::INT, T_INT, MemNode::unordered));\n+      lhs = _igvn.transform(new OrINode(lhs, lh_val));\n+    }\n+    Node* masked = transform_later(new AndINode(lhs, intcon(Klass::_lh_array_tag_flat_value_bit_inplace)));\n+    Node* cmp = transform_later(new CmpINode(masked, intcon(0)));\n+    Node* bol = transform_later(new BoolNode(cmp, BoolTest::eq));\n+    Node* m2b = transform_later(new Conv2BNode(masked));\n+    \/\/ The matcher expects the input to If\/CMove nodes to be produced by a Bool(CmpI..)\n+    \/\/ pattern, but the input to other potential users (e.g. Phi) to be some\n+    \/\/ other pattern (e.g. a Conv2B node, possibly idealized as a CMoveI).\n+    Node* old_bol = check->unique_out();\n+    for (DUIterator_Last imin, i = old_bol->last_outs(imin); i >= imin; --i) {\n+      Node* user = old_bol->last_out(i);\n+      for (uint j = 0; j < user->req(); j++) {\n+        Node* n = user->in(j);\n+        if (n == old_bol) {\n+          _igvn.replace_input_of(user, j, (user->is_If() || user->is_CMove()) ? bol : m2b);\n+        }\n+      }\n+    }\n+    _igvn.replace_node(check, C->top());\n+  }\n+}\n+\n@@ -2415,2 +3077,2 @@\n-void PhaseMacroExpand::eliminate_macro_nodes() {\n-  if (C->macro_count() == 0)\n+void PhaseMacroExpand::eliminate_macro_nodes(bool eliminate_locks) {\n+  if (C->macro_count() == 0) {\n@@ -2418,0 +3080,1 @@\n+  }\n@@ -2424,7 +3087,5 @@\n-  \/\/ Before elimination may re-mark (change to Nested or NonEscObj)\n-  \/\/ all associated (same box and obj) lock and unlock nodes.\n-  int cnt = C->macro_count();\n-  for (int i=0; i < cnt; i++) {\n-    Node *n = C->macro_node(i);\n-    if (n->is_AbstractLock()) { \/\/ Lock and Unlock nodes\n-      mark_eliminated_locking_nodes(n->as_AbstractLock());\n+  int iteration = 0;\n+  while (C->macro_count() > 0) {\n+    if (iteration++ > 100) {\n+      assert(false, \"Too slow convergence of macro elimination\");\n+      break;\n@@ -2432,23 +3093,11 @@\n-  }\n-  \/\/ Re-marking may break consistency of Coarsened locks.\n-  if (!C->coarsened_locks_consistent()) {\n-    return; \/\/ recompile without Coarsened locks if broken\n-  } else {\n-    \/\/ After coarsened locks are eliminated locking regions\n-    \/\/ become unbalanced. We should not execute any more\n-    \/\/ locks elimination optimizations on them.\n-    C->mark_unbalanced_boxes();\n-  }\n-  \/\/ First, attempt to eliminate locks\n-  bool progress = true;\n-  while (progress) {\n-    progress = false;\n-    for (int i = C->macro_count(); i > 0; i = MIN2(i - 1, C->macro_count())) { \/\/ more than 1 element can be eliminated at once\n-      Node* n = C->macro_node(i - 1);\n-      bool success = false;\n-      DEBUG_ONLY(int old_macro_count = C->macro_count();)\n-      if (n->is_AbstractLock()) {\n-        success = eliminate_locking_node(n->as_AbstractLock());\n-#ifndef PRODUCT\n-        if (success && PrintOptoStatistics) {\n-          AtomicAccess::inc(&PhaseMacroExpand::_monitor_objects_removed_counter);\n+    \/\/ Postpone lock elimination to after EA when most allocations are eliminated\n+    \/\/ because they might block lock elimination if their escape state isn't\n+    \/\/ determined yet and we only got one chance at eliminating the lock.\n+    if (eliminate_locks) {\n+      \/\/ Before elimination may re-mark (change to Nested or NonEscObj)\n+      \/\/ all associated (same box and obj) lock and unlock nodes.\n+      int cnt = C->macro_count();\n+      for (int i=0; i < cnt; i++) {\n+        Node *n = C->macro_node(i);\n+        if (n->is_AbstractLock()) { \/\/ Lock and Unlock nodes\n+          mark_eliminated_locking_nodes(n->as_AbstractLock());\n@@ -2457,5 +3106,8 @@\n-#endif\n-      assert(success == (C->macro_count() < old_macro_count), \"elimination reduces macro count\");\n-      progress = progress || success;\n-      if (success) {\n-        C->print_method(PHASE_AFTER_MACRO_ELIMINATION_STEP, 5, n);\n+      \/\/ Re-marking may break consistency of Coarsened locks.\n+      if (!C->coarsened_locks_consistent()) {\n+        return; \/\/ recompile without Coarsened locks if broken\n+      } else {\n+        \/\/ After coarsened locks are eliminated locking regions\n+        \/\/ become unbalanced. We should not execute any more\n+        \/\/ locks elimination optimizations on them.\n+        C->mark_unbalanced_boxes();\n@@ -2465,5 +3117,2 @@\n-  }\n-  \/\/ Next, attempt to eliminate allocations\n-  progress = true;\n-  while (progress) {\n-    progress = false;\n+\n+    bool progress = false;\n@@ -2484,2 +3133,5 @@\n-      case Node::Class_CallStaticJava:\n-        success = eliminate_boxing_node(n->as_CallStaticJava());\n+      case Node::Class_CallStaticJava: {\n+        CallStaticJavaNode* call = n->as_CallStaticJava();\n+        if (!call->method()->is_method_handle_intrinsic()) {\n+          success = eliminate_boxing_node(n->as_CallStaticJava());\n+        }\n@@ -2487,0 +3139,1 @@\n+      }\n@@ -2489,1 +3142,8 @@\n-        assert(!n->as_AbstractLock()->is_eliminated(), \"sanity\");\n+        if (eliminate_locks) {\n+          success = eliminate_locking_node(n->as_AbstractLock());\n+#ifndef PRODUCT\n+          if (success && PrintOptoStatistics) {\n+            AtomicAccess::inc(&PhaseMacroExpand::_monitor_objects_removed_counter);\n+          }\n+#endif\n+        }\n@@ -2499,0 +3159,2 @@\n+      case Node::Class_FlatArrayCheck:\n+        break;\n@@ -2516,0 +3178,16 @@\n+\n+    \/\/ Ensure the graph after PhaseMacroExpand::eliminate_macro_nodes is canonical (no igvn\n+    \/\/ transformation is pending). If an allocation is used only in safepoints, elimination of\n+    \/\/ other macro nodes can remove all these safepoints, allowing the allocation to be removed.\n+    \/\/ Hence after igvn we retry removing macro nodes if some progress that has been made in this\n+    \/\/ iteration.\n+    _igvn.set_delay_transform(false);\n+    _igvn.optimize();\n+    if (C->failing()) {\n+      return;\n+    }\n+    _igvn.set_delay_transform(true);\n+\n+    if (!progress) {\n+      break;\n+    }\n@@ -2544,4 +3222,7 @@\n-        \/\/ Remove it from macro list and put on IGVN worklist to optimize.\n-        C->remove_macro_node(n);\n-        _igvn._worklist.push(n);\n-        success = true;\n+        CallStaticJavaNode* call = n->as_CallStaticJava();\n+        if (!call->method()->is_method_handle_intrinsic()) {\n+          \/\/ Remove it from macro list and put on IGVN worklist to optimize.\n+          C->remove_macro_node(n);\n+          _igvn._worklist.push(n);\n+          success = true;\n+        }\n@@ -2656,0 +3337,7 @@\n+    case Node::Class_CallStaticJava:\n+      expand_mh_intrinsic_return(n->as_CallStaticJava());\n+      C->remove_macro_node(n);\n+      break;\n+    case Node::Class_FlatArrayCheck:\n+      expand_flatarraycheck_node(n->as_FlatArrayCheck());\n+      break;\n@@ -2667,1 +3355,1 @@\n-        for (unsigned int i = 0; i < mod_macro->tf()->domain()->cnt() - TypeFunc::Parms; i++) {\n+        for (unsigned int i = 0; i < mod_macro->tf()->domain_cc()->cnt() - TypeFunc::Parms; i++) {\n","filename":"src\/hotspot\/share\/opto\/macro.cpp","additions":1002,"deletions":314,"binary":false,"changes":1316,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2025, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2026, Oracle and\/or its affiliates. All rights reserved.\n@@ -26,0 +26,2 @@\n+#include \"ci\/ciFlatArrayKlass.hpp\"\n+#include \"ci\/ciInlineKlass.hpp\"\n@@ -27,0 +29,1 @@\n+#include \"classfile\/systemDictionary.hpp\"\n@@ -33,0 +36,1 @@\n+#include \"oops\/flatArrayKlass.hpp\"\n@@ -36,0 +40,1 @@\n+#include \"opto\/callnode.hpp\"\n@@ -40,0 +45,1 @@\n+#include \"opto\/inlinetypenode.hpp\"\n@@ -53,0 +59,1 @@\n+#include \"runtime\/arguments.hpp\"\n@@ -55,0 +62,1 @@\n+#include \"utilities\/globalDefinitions.hpp\"\n@@ -143,8 +151,105 @@\n-Node *MemNode::optimize_simple_memory_chain(Node *mchain, const TypeOopPtr *t_oop, Node *load, PhaseGVN *phase) {\n-  assert((t_oop != nullptr), \"sanity\");\n-  bool is_instance = t_oop->is_known_instance_field();\n-  bool is_boxed_value_load = t_oop->is_ptr_to_boxed_value() &&\n-                             (load != nullptr) && load->is_Load() &&\n-                             (phase->is_IterGVN() != nullptr);\n-  if (!(is_instance || is_boxed_value_load))\n-    return mchain;  \/\/ don't try to optimize non-instance types\n+\/\/ Find the memory output corresponding to the fall-through path of a call\n+static Node* find_call_fallthrough_mem_output(CallNode* call) {\n+  ResourceMark rm;\n+  CallProjections* projs = call->extract_projections(false, false);\n+  Node* res = projs->fallthrough_memproj;\n+  assert(res != nullptr, \"must have a fallthrough mem output\");\n+  return res;\n+}\n+\n+\/\/ Try to find a better memory input for a load from a strict final field\n+static Node* try_optimize_strict_final_load_memory(PhaseGVN* phase, Node* adr, ProjNode*& base_local) {\n+  intptr_t offset = 0;\n+  Node* base = AddPNode::Ideal_base_and_offset(adr, phase, offset);\n+  if (base == nullptr) {\n+    return nullptr;\n+  }\n+\n+  Node* base_uncasted = base->uncast();\n+  if (base_uncasted->is_Proj()) {\n+    MultiNode* multi = base_uncasted->in(0)->as_Multi();\n+    if (multi->is_Allocate()) {\n+      base_local = base_uncasted->as_Proj();\n+      return nullptr;\n+    } else if (multi->is_Call()) {\n+      \/\/ The oop is returned from a call, the memory can be the fallthrough output of the call\n+      return find_call_fallthrough_mem_output(multi->as_Call());\n+    } else if (multi->is_Start()) {\n+      \/\/ The oop is a parameter\n+      if (phase->C->method()->is_object_constructor() && base_uncasted->as_Proj()->_con == TypeFunc::Parms) {\n+        \/\/ The receiver of a constructor is similar to the result of an AllocateNode\n+        base_local = base_uncasted->as_Proj();\n+        return nullptr;\n+      } else {\n+        \/\/ Use the start memory otherwise\n+        return multi->proj_out(TypeFunc::Memory);\n+      }\n+    }\n+  }\n+\n+  return nullptr;\n+}\n+\n+\/\/ Whether a call can modify a strict final field, given that the object is allocated inside the\n+\/\/ current compilation unit, or is the first parameter when the compilation root is a constructor.\n+\/\/ This is equivalent to asking whether 'call' is a constructor invocation and the class declaring\n+\/\/ the target method is a subclass of the class declaring 'field'.\n+static bool call_can_modify_local_object(ciField* field, CallNode* call) {\n+  if (!call->is_CallJava()) {\n+    return false;\n+  }\n+\n+  ciMethod* target = call->as_CallJava()->method();\n+  if (target == nullptr || !target->is_object_constructor()) {\n+    return false;\n+  }\n+\n+  \/\/ If 'field' is declared in a class that is a subclass of the one declaring the constructor,\n+  \/\/ then the field is set inside the constructor, else the field must be set before the\n+  \/\/ constructor invocation. E.g. A field Super.x will be set during the execution of Sub::<init>,\n+  \/\/ while a field Sub.y must be set before Super::<init> is invoked.\n+  \/\/ We can try to be more heroic and decide if the receiver of the constructor invocation is the\n+  \/\/ object from which we are loading from. This, however, may be problematic as deciding if 2\n+  \/\/ nodes are definitely different may not be trivial, especially if the graph is not canonical.\n+  \/\/ As a result, it is made more conservative for now.\n+  assert(call->req() > TypeFunc::Parms, \"constructor must have at least 1 argument\");\n+  return target->holder()->is_subclass_of(field->holder());\n+}\n+\n+Node* MemNode::optimize_simple_memory_chain(Node* mchain, const TypeOopPtr* t_oop, Node* load, PhaseGVN* phase) {\n+  assert(t_oop != nullptr, \"sanity\");\n+  bool is_known_instance = t_oop->is_known_instance_field();\n+  bool is_strict_final_load = false;\n+\n+  \/\/ After macro expansion, an allocation may become a call, changing the memory input to the\n+  \/\/ memory output of that call would be illegal. As a result, disallow this transformation after\n+  \/\/ macro expansion.\n+  if (phase->is_IterGVN() && phase->C->allow_macro_nodes() && load != nullptr && load->is_Load() && !load->as_Load()->is_mismatched_access()) {\n+    is_strict_final_load = t_oop->is_ptr_to_strict_final_field();\n+#ifdef ASSERT\n+    if ((t_oop->is_inlinetypeptr() && t_oop->inline_klass()->contains_field_offset(t_oop->offset())) || t_oop->is_ptr_to_boxed_value()) {\n+      assert(is_strict_final_load, \"sanity check for basic cases\");\n+    }\n+#endif \/\/ ASSERT\n+  }\n+\n+  if (!is_known_instance && !is_strict_final_load) {\n+    return mchain;\n+  }\n+\n+  Node* result = mchain;\n+  ProjNode* base_local = nullptr;\n+\n+  ciField* field = nullptr;\n+  if (is_strict_final_load) {\n+    field = phase->C->alias_type(t_oop)->field();\n+    assert(field != nullptr, \"must point to a field\");\n+\n+    Node* adr = load->in(MemNode::Address);\n+    assert(phase->type(adr) == t_oop, \"inconsistent type\");\n+    Node* tmp = try_optimize_strict_final_load_memory(phase, adr, base_local);\n+    if (tmp != nullptr) {\n+      result = tmp;\n+    }\n+  }\n+\n@@ -152,3 +257,2 @@\n-  Node *start_mem = phase->C->start()->proj_out_or_null(TypeFunc::Memory);\n-  Node *prev = nullptr;\n-  Node *result = mchain;\n+  Node* start_mem = phase->C->start()->proj_out_or_null(TypeFunc::Memory);\n+  Node* prev = nullptr;\n@@ -157,2 +261,5 @@\n-    if (result == start_mem)\n-      break;  \/\/ hit one of our sentinels\n+    if (result == start_mem) {\n+      \/\/ start_mem is the earliest memory possible\n+      break;\n+    }\n+\n@@ -161,1 +268,1 @@\n-      Node *proj_in = result->in(0);\n+      Node* proj_in = result->in(0);\n@@ -163,1 +270,2 @@\n-        break;  \/\/ hit one of our sentinels\n+        \/\/ This is the allocation that creates the object from which we are loading from\n+        break;\n@@ -166,2 +274,4 @@\n-        CallNode *call = proj_in->as_Call();\n-        if (!call->may_modify(t_oop, phase)) { \/\/ returns false for instances\n+        CallNode* call = proj_in->as_Call();\n+        if (!call->may_modify(t_oop, phase)) {\n+          result = call->in(TypeFunc::Memory);\n+        } else if (is_strict_final_load && base_local != nullptr && !call_can_modify_local_object(field, call)) {\n@@ -170,0 +280,3 @@\n+      } else if (proj_in->Opcode() == Op_Tuple) {\n+        \/\/ The call will be folded, skip over it.\n+        break;\n@@ -177,1 +290,1 @@\n-        if (is_instance) {\n+        if (is_known_instance) {\n@@ -179,1 +292,1 @@\n-        } else if (is_boxed_value_load) {\n+        } else if (is_strict_final_load) {\n@@ -183,1 +296,5 @@\n-            result = proj_in->in(TypeFunc::Memory); \/\/ not related allocation\n+            \/\/ Allocation of another type, must be another object\n+            result = proj_in->in(TypeFunc::Memory);\n+          } else if (base_local != nullptr && (base_local->is_Parm() || base_local->in(0) != alloc)) {\n+            \/\/ Allocation of another object\n+            result = proj_in->in(TypeFunc::Memory);\n@@ -192,0 +309,5 @@\n+      } else if (proj_in->is_LoadFlat() || proj_in->is_StoreFlat()) {\n+        if (is_strict_final_load) {\n+          \/\/ LoadFlat and StoreFlat cannot happen to strict final fields\n+          result = proj_in->in(TypeFunc::Memory);\n+        }\n@@ -195,1 +317,1 @@\n-        assert(false, \"unexpected projection\");\n+        assert(false, \"unexpected projection of %s\", proj_in->Name());\n@@ -198,1 +320,1 @@\n-      if (!is_instance || !ClearArrayNode::step_through(&result, instance_id, phase)) {\n+      if (!is_known_instance || !ClearArrayNode::step_through(&result, instance_id, phase)) {\n@@ -236,0 +358,2 @@\n+                     ->cast_to_not_flat(t_oop->is_aryptr()->is_not_flat())\n+                     ->cast_to_not_null_free(t_oop->is_aryptr()->is_not_null_free())\n@@ -262,1 +386,1 @@\n-               tp->isa_aryptr() &&        tp->offset() == Type::OffsetBot &&\n+        tp->isa_aryptr() &&        tp->offset() == Type::OffsetBot &&\n@@ -974,0 +1098,1 @@\n+  case T_ARRAY:\n@@ -987,1 +1112,1 @@\n-    ShouldNotReachHere();\n+    assert(false, \"unexpected basic type %s\", type2name(bt));\n@@ -1022,1 +1147,1 @@\n-    return (eliminate_boxing && non_volatile) || is_stable_ary;\n+    return (eliminate_boxing && non_volatile) || is_stable_ary || tp->is_inlinetypeptr();\n@@ -1078,2 +1203,1 @@\n-      uint header = arrayOopDesc::base_offset_in_bytes(ary_elem);\n-      uint shift  = exact_log2(type2aelembytes(ary_elem));\n+      uint shift  = ary_t->is_flat() ? ary_t->flat_log_elem_size() : exact_log2(type2aelembytes(ary_elem));\n@@ -1103,0 +1227,10 @@\n+static Node* see_through_inline_type(PhaseValues* phase, const MemNode* load, Node* base, int offset) {\n+  if (!load->is_mismatched_access() && base != nullptr && base->is_InlineType() && offset > oopDesc::klass_offset_in_bytes()) {\n+    InlineTypeNode* vt = base->as_InlineType();\n+    Node* value = vt->field_value_by_offset(offset, true);\n+    assert(value != nullptr, \"must see some value\");\n+    return value;\n+  }\n+\n+  return nullptr;\n+}\n@@ -1111,0 +1245,1 @@\n+\/\/ This method may find an unencoded node instead of the corresponding encoded one.\n@@ -1115,0 +1250,9 @@\n+  \/\/ Try to see through an InlineTypeNode\n+  \/\/ LoadN is special because the input is not compressed\n+  if (Opcode() != Op_LoadN) {\n+    Node* value = see_through_inline_type(phase, this, ld_base, ld_off);\n+    if (value != nullptr) {\n+      return value;\n+    }\n+  }\n+\n@@ -1198,1 +1342,1 @@\n-        const TypeVect* out_vt = as_LoadVector()->vect_type();\n+        const TypeVect* out_vt = is_Load() ? as_LoadVector()->vect_type() : as_StoreVector()->vect_type();\n@@ -1216,0 +1360,29 @@\n+      Node* init_value = ld_alloc->in(AllocateNode::InitValue);\n+      if (init_value != nullptr) {\n+        const TypeAryPtr* ld_adr_type = phase->type(ld_adr)->isa_aryptr();\n+        if (ld_adr_type == nullptr) {\n+          return nullptr;\n+        }\n+\n+        \/\/ We know that this is not a flat array, the load should return the whole oop\n+        if (ld_adr_type->is_not_flat()) {\n+          return init_value;\n+        }\n+\n+        \/\/ If this is a flat array, try to see through init_value\n+        if (init_value->is_EncodeP()) {\n+          init_value = init_value->in(1);\n+        }\n+        if (!init_value->is_InlineType() || ld_adr_type->field_offset() == Type::Offset::bottom) {\n+          return nullptr;\n+        }\n+\n+        ciInlineKlass* vk = phase->type(init_value)->inline_klass();\n+        int field_offset_in_payload = ld_adr_type->field_offset().get();\n+        if (field_offset_in_payload == vk->null_marker_offset_in_payload()) {\n+          return init_value->as_InlineType()->get_null_marker();\n+        } else {\n+          return init_value->as_InlineType()->field_value_by_offset(field_offset_in_payload + vk->payload_offset(), true);\n+        }\n+      }\n+      assert(ld_alloc->in(AllocateNode::RawInitValue) == nullptr, \"init value may not be null\");\n@@ -1268,1 +1441,1 @@\n-    \/\/ Only instances and boxed values.\n+    \/\/ Only known instances and immutable fields\n@@ -1270,1 +1443,1 @@\n-        (t_oop->is_ptr_to_boxed_value() ||\n+        (t_oop->is_ptr_to_strict_final_field() ||\n@@ -1298,0 +1471,4 @@\n+\n+    if (phase->type(value)->isa_ptr() && phase->type(this)->isa_narrowoop()) {\n+      return this;\n+    }\n@@ -1319,2 +1496,2 @@\n-         addr_t->is_ptr_to_boxed_value()) {\n-      \/\/ Use _idx of address base (could be Phi node) for boxed values.\n+         addr_t->is_ptr_to_strict_final_field()) {\n+      \/\/ Use _idx of address base (could be Phi node) for immutable fields in unknown instances\n@@ -1876,0 +2053,2 @@\n+        \/\/ TODO 8350865 Can we re-enable this?\n+        && !(phase->type(address)->is_inlinetypeptr() && is_mismatched_access())\n@@ -1972,1 +2151,8 @@\n-  return progress ? this : nullptr;\n+  if (progress) {\n+    return this;\n+  }\n+\n+  if (!can_reshape) {\n+    phase->record_for_igvn(this);\n+  }\n+  return nullptr;\n@@ -2020,2 +2206,6 @@\n-    assert(value->bottom_type()->higher_equal(_type), \"sanity\");\n-    return value->bottom_type();\n+    if (phase->type(value)->isa_ptr() && _type->isa_narrowoop()) {\n+      return phase->type(value)->make_narrowoop();\n+    } else {\n+      assert(value->bottom_type()->higher_equal(_type), \"sanity\");\n+      return phase->type(value);\n+    }\n@@ -2023,1 +2213,0 @@\n-\n@@ -2044,1 +2233,1 @@\n-        const Type* con_type = Type::make_constant_from_array_element(aobj->as_array(), off,\n+        const Type* con_type = Type::make_constant_from_array_element(aobj->as_array(), off, ary->field_offset().get(),\n@@ -2070,0 +2259,1 @@\n+        && !ary->is_flat()\n@@ -2105,0 +2295,2 @@\n+            \/\/ Default value load\n+            tp->is_instptr()->instance_klass() == ciEnv::current()->Class_klass() ||\n@@ -2110,1 +2302,17 @@\n-    \/\/ Optimize loads from constant fields.\n+    BasicType bt = value_basic_type();\n+\n+    \/\/ Fold loads of the field map\n+    if (UseAltSubstitutabilityMethod && tinst != nullptr) {\n+      ciInstanceKlass* ik = tinst->instance_klass();\n+      int offset = tinst->offset();\n+      if (ik == phase->C->env()->Class_klass()) {\n+        ciType* t = tinst->java_mirror_type();\n+        if (t != nullptr && t->is_inlinetype() && offset == t->as_inline_klass()->field_map_offset()) {\n+          ciConstant map = t->as_inline_klass()->get_field_map();\n+          bool is_narrow_oop = (bt == T_NARROWOOP);\n+          return Type::make_from_constant(map, true, 1, is_narrow_oop);\n+        }\n+      }\n+    }\n+\n+    \/\/ Optimize loads from constant fields.\n@@ -2114,1 +2322,1 @@\n-      const Type* con_type = Type::make_constant_from_field(const_oop->as_instance(), off, is_unsigned(), value_basic_type());\n+      const Type* con_type = Type::make_constant_from_field(const_oop->as_instance(), off, is_unsigned(), bt);\n@@ -2161,6 +2369,19 @@\n-      if (UseCompactObjectHeaders) {\n-        if (tkls->offset() == in_bytes(Klass::prototype_header_offset())) {\n-          \/\/ The field is Klass::_prototype_header. Return its (constant) value.\n-          assert(this->Opcode() == Op_LoadX, \"must load a proper type from _prototype_header\");\n-          return TypeX::make(klass->prototype_header());\n-        }\n+      if (klass->is_inlinetype() && tkls->offset() == in_bytes(InstanceKlass::acmp_maps_offset_offset())) {\n+        return TypeInt::make(klass->as_inline_klass()->field_map_offset());\n+      }\n+      if (klass->is_obj_array_klass() && tkls->offset() == in_bytes(ObjArrayKlass::next_refined_array_klass_offset())) {\n+        \/\/ Fold loads from LibraryCallKit::load_default_refined_array_klass\n+        return tkls->is_aryklassptr()->cast_to_refined_array_klass_ptr();\n+      }\n+      if (klass->is_array_klass() && tkls->offset() == in_bytes(ObjArrayKlass::properties_offset())) {\n+        assert(klass->is_type_array_klass() || tkls->is_aryklassptr()->is_refined_type(), \"Must be a refined array klass pointer\");\n+        return TypeInt::make(klass->as_array_klass()->properties());\n+      }\n+      if (klass->is_flat_array_klass() && tkls->offset() == in_bytes(FlatArrayKlass::layout_kind_offset())) {\n+        assert(Opcode() == Op_LoadI, \"must load an int from _layout_kind\");\n+        return TypeInt::make(static_cast<jint>(klass->as_flat_array_klass()->layout_kind()));\n+      }\n+      if (UseCompactObjectHeaders && tkls->offset() == in_bytes(Klass::prototype_header_offset())) {\n+        \/\/ The field is Klass::_prototype_header. Return its (constant) value.\n+        assert(this->Opcode() == Op_LoadX, \"must load a proper type from _prototype_header\");\n+        return TypeX::make(klass->prototype_header());\n@@ -2240,0 +2461,12 @@\n+      \/\/ TODO 8350865 Scalar replacement does not work well for flat arrays.\n+      \/\/ Escape Analysis assumes that arrays are always zeroed during allocation which is not true for null-free arrays\n+      \/\/ ConnectionGraph::split_unique_types will re-wire the memory of loads from such arrays around the allocation\n+      \/\/ TestArrays::test6 and test152 and TestBasicFunctionality::test20 are affected by this.\n+      if (tp->isa_aryptr() && tp->is_aryptr()->is_flat() && tp->is_aryptr()->is_null_free()) {\n+        intptr_t offset = 0;\n+        Node* base = AddPNode::Ideal_base_and_offset(adr, phase, offset);\n+        AllocateNode* alloc = AllocateNode::Ideal_allocation(base);\n+        if (alloc != nullptr && alloc->is_AllocateArray() && alloc->in(AllocateNode::InitValue) != nullptr) {\n+          return _type;\n+        }\n+      }\n@@ -2243,1 +2476,0 @@\n-\n@@ -2247,1 +2479,10 @@\n-      return TypeX::make(markWord::prototype().value());\n+      if (Arguments::is_valhalla_enabled()) {\n+        \/\/ The mark word may contain property bits (inline, flat, null-free)\n+        Node* klass_node = alloc->in(AllocateNode::KlassNode);\n+        const TypeKlassPtr* tkls = phase->type(klass_node)->isa_klassptr();\n+        if (tkls != nullptr && tkls->is_loaded() && tkls->klass_is_exact()) {\n+          return TypeX::make(tkls->exact_klass()->prototype_header());\n+        }\n+      } else {\n+        return TypeX::make(markWord::prototype().value());\n+      }\n@@ -2396,0 +2637,20 @@\n+Node* LoadNNode::Ideal(PhaseGVN* phase, bool can_reshape) {\n+  \/\/ Loading from an InlineType, find the input and make an EncodeP\n+  Node* addr = in(Address);\n+  intptr_t offset;\n+  Node* base = AddPNode::Ideal_base_and_offset(addr, phase, offset);\n+  Node* value = see_through_inline_type(phase, this, base, offset);\n+  if (value != nullptr) {\n+    return new EncodePNode(value, type());\n+  }\n+\n+  \/\/ Can see the corresponding value, may need to add an EncodeP\n+  value = can_see_stored_value(in(Memory), phase);\n+  if (value != nullptr && phase->type(value)->isa_ptr() && type()->isa_narrowoop()) {\n+    return new EncodePNode(value, type());\n+  }\n+\n+  \/\/ Identity call will handle the case where EncodeP is unnecessary\n+  return LoadNode::Ideal(phase, can_reshape);\n+}\n+\n@@ -2469,1 +2730,1 @@\n-  const TypeAryPtr *tary = tp->isa_aryptr();\n+  const TypeAryPtr* tary = tp->isa_aryptr();\n@@ -2472,1 +2733,1 @@\n-    return tary->as_klass_type(true);\n+    return tary->as_klass_type(true)->is_aryklassptr();\n@@ -2490,0 +2751,7 @@\n+    if (tkls->isa_aryklassptr() != nullptr && tkls->klass_is_exact() &&\n+        !tkls->exact_klass()->is_type_array_klass() &&\n+        tkls->offset() == in_bytes(Klass::super_offset())) {\n+      \/\/ We are loading the super klass of a refined array klass, return the non-refined klass pointer\n+      assert(tkls->is_aryklassptr()->is_refined_type(), \"Must be a refined array klass pointer\");\n+      return tkls->is_aryklassptr()->with_offset(0)->cast_to_non_refined();\n+    }\n@@ -2552,0 +2820,4 @@\n+  \/\/\n+  \/\/ This optimization does not apply to arrays because if k is not a\n+  \/\/ constant, it was obtained via load_klass which returns the VM type\n+  \/\/ and '.java_mirror.as_klass' should return the Java type instead.\n@@ -2561,3 +2833,2 @@\n-            && (tkls->isa_instklassptr() || tkls->isa_aryklassptr())\n-            && adr2->is_AddP()\n-           ) {\n+            && ((tkls->isa_instklassptr() && !tkls->is_instklassptr()->might_be_an_array()))\n+            && adr2->is_AddP()) {\n@@ -2708,0 +2979,1 @@\n+  case T_ARRAY:\n@@ -2723,1 +2995,1 @@\n-    ShouldNotReachHere();\n+    assert(false, \"unexpected basic type %s\", type2name(bt));\n@@ -3385,2 +3657,2 @@\n-  \/\/ unsafe if I have intervening uses.\n-  {\n+  \/\/ unsafe if I have intervening uses...\n+  if (phase->C->get_adr_type(phase->C->get_alias_index(adr_type())) != TypeAryPtr::INLINES) {\n@@ -3406,0 +3678,2 @@\n+             (Opcode() == Op_StoreL && st->Opcode() == Op_StoreN) ||\n+             (st->adr_type()->isa_aryptr() && st->adr_type()->is_aryptr()->is_flat()) || \/\/ TODO 8343835\n@@ -3543,2 +3817,1 @@\n-  if (result == this &&\n-      ReduceFieldZeroing && phase->type(val)->is_zero_type()) {\n+  if (result == this && ReduceFieldZeroing) {\n@@ -3546,1 +3819,2 @@\n-    if (mem->is_Proj() && mem->in(0)->is_Allocate()) {\n+    if (mem->is_Proj() && mem->in(0)->is_Allocate() &&\n+        (phase->type(val)->is_zero_type() || mem->in(0)->in(AllocateNode::InitValue) == val)) {\n@@ -3550,1 +3824,1 @@\n-    if (result == this) {\n+    if (result == this && phase->type(val)->is_zero_type()) {\n@@ -4059,1 +4333,1 @@\n-    return new ClearArrayNode(in(0), in(1), in(2), in(3), true);\n+    return new ClearArrayNode(in(0), in(1), in(2), in(3), in(4), true);\n@@ -4077,1 +4351,1 @@\n-  Node *zero = phase->makecon(TypeLong::ZERO);\n+  Node *val = in(4);\n@@ -4079,1 +4353,1 @@\n-  mem = new StoreLNode(in(0),mem,adr,atp,zero,MemNode::unordered,false);\n+  mem = new StoreLNode(in(0), mem, adr, atp, val, MemNode::unordered, false);\n@@ -4084,1 +4358,1 @@\n-    mem = new StoreLNode(in(0),mem,adr,atp,zero,MemNode::unordered,false);\n+    mem = new StoreLNode(in(0), mem, adr, atp, val, MemNode::unordered, false);\n@@ -4118,0 +4392,2 @@\n+                                   Node* val,\n+                                   Node* raw_val,\n@@ -4128,1 +4404,7 @@\n-    mem = StoreNode::make(*phase, ctl, mem, adr, atp, phase->zerocon(T_INT), T_INT, MemNode::unordered);\n+    if (val != nullptr) {\n+      assert(phase->type(val)->isa_narrowoop(), \"should be narrow oop\");\n+      mem = new StoreNNode(ctl, mem, adr, atp, val, MemNode::unordered);\n+    } else {\n+      assert(raw_val == nullptr, \"val may not be null\");\n+      mem = StoreNode::make(*phase, ctl, mem, adr, atp, phase->zerocon(T_INT), T_INT, MemNode::unordered);\n+    }\n@@ -4135,1 +4417,1 @@\n-  return clear_memory(ctl, mem, dest, phase->MakeConX(offset), end_offset, phase);\n+  return clear_memory(ctl, mem, dest, raw_val, phase->MakeConX(offset), end_offset, phase);\n@@ -4139,0 +4421,1 @@\n+                                   Node* raw_val,\n@@ -4161,1 +4444,4 @@\n-  mem = new ClearArrayNode(ctl, mem, zsize, adr, false);\n+  if (raw_val == nullptr) {\n+    raw_val = phase->MakeConX(0);\n+  }\n+  mem = new ClearArrayNode(ctl, mem, zsize, adr, raw_val, false);\n@@ -4166,0 +4452,2 @@\n+                                   Node* val,\n+                                   Node* raw_val,\n@@ -4180,1 +4468,1 @@\n-    mem = clear_memory(ctl, mem, dest,\n+    mem = clear_memory(ctl, mem, dest, val, raw_val,\n@@ -4187,1 +4475,7 @@\n-    mem = StoreNode::make(*phase, ctl, mem, adr, atp, phase->zerocon(T_INT), T_INT, MemNode::unordered);\n+    if (val != nullptr) {\n+      assert(phase->type(val)->isa_narrowoop(), \"should be narrow oop\");\n+      mem = new StoreNNode(ctl, mem, adr, atp, val, MemNode::unordered);\n+    } else {\n+      assert(raw_val == nullptr, \"val may not be null\");\n+      mem = StoreNode::make(*phase, ctl, mem, adr, atp, phase->zerocon(T_INT), T_INT, MemNode::unordered);\n+    }\n@@ -4334,1 +4628,1 @@\n-Node *MemBarNode::match( const ProjNode *proj, const Matcher *m ) {\n+Node *MemBarNode::match(const ProjNode *proj, const Matcher *m, const RegMask* mask) {\n@@ -4621,1 +4915,3 @@\n-  if (init == nullptr || init->is_complete())  return false;\n+  if (init == nullptr || init->is_complete()) {\n+    return false;\n+  }\n@@ -4805,0 +5101,6 @@\n+                if (base->is_Phi()) {\n+                  \/\/ In rare case, base may be a PhiNode and it may read\n+                  \/\/ the same memory slice between InitializeNode and store.\n+                  failed = true;\n+                  break;\n+                }\n@@ -5391,0 +5693,2 @@\n+                                              allocation()->in(AllocateNode::InitValue),\n+                                              allocation()->in(AllocateNode::RawInitValue),\n@@ -5450,0 +5754,2 @@\n+                                            allocation()->in(AllocateNode::InitValue),\n+                                            allocation()->in(AllocateNode::RawInitValue),\n","filename":"src\/hotspot\/share\/opto\/memnode.cpp","additions":377,"deletions":71,"binary":false,"changes":448,"status":"modified"},{"patch":"@@ -129,0 +129,4 @@\n+#ifdef ASSERT\n+  void set_adr_type(const TypePtr* adr_type) { _adr_type = adr_type; }\n+#endif\n+\n@@ -520,0 +524,1 @@\n+  virtual Node* Ideal(PhaseGVN* phase, bool can_reshape);\n@@ -569,1 +574,0 @@\n-\n@@ -723,0 +727,19 @@\n+\/\/ Special StoreL for flat stores that emits GC barriers for field at 'oop_off' in the backend\n+class StoreLSpecialNode : public StoreNode {\n+\n+public:\n+  StoreLSpecialNode(Node* c, Node* mem, Node* adr, const TypePtr* at, Node* val, Node* oop_off, MemOrd mo)\n+    : StoreNode(c, mem, adr, at, val, mo) {\n+    set_mismatched_access();\n+    if (oop_off != nullptr) {\n+      add_req(oop_off);\n+    }\n+  }\n+  virtual int Opcode() const;\n+  virtual BasicType value_basic_type() const { return T_LONG; }\n+\n+  virtual uint match_edge(uint idx) const { return idx == MemNode::Address ||\n+                                                   idx == MemNode::ValueIn ||\n+                                                   idx == MemNode::ValueIn + 1; }\n+};\n+\n@@ -1075,0 +1098,1 @@\n+  bool _word_copy_only;\n@@ -1076,2 +1100,3 @@\n-  ClearArrayNode( Node *ctrl, Node *arymem, Node *word_cnt, Node *base, bool is_large)\n-    : Node(ctrl,arymem,word_cnt,base), _is_large(is_large) {\n+  ClearArrayNode( Node *ctrl, Node *arymem, Node *word_cnt, Node *base, Node* val, bool is_large)\n+    : Node(ctrl, arymem, word_cnt, base, val), _is_large(is_large),\n+      _word_copy_only(val->bottom_type()->isa_long() && (!val->bottom_type()->is_long()->is_con() || val->bottom_type()->is_long()->get_con() != 0)) {\n@@ -1089,0 +1114,1 @@\n+  bool word_copy_only() const { return _word_copy_only; }\n@@ -1100,0 +1126,2 @@\n+                            Node* val,\n+                            Node* raw_val,\n@@ -1104,0 +1132,2 @@\n+                            Node* val,\n+                            Node* raw_val,\n@@ -1108,0 +1138,1 @@\n+                            Node* raw_val,\n@@ -1159,1 +1190,1 @@\n-  virtual Node *match( const ProjNode *proj, const Matcher *m );\n+  virtual Node *match(const ProjNode *proj, const Matcher *m, const RegMask* mask);\n","filename":"src\/hotspot\/share\/opto\/memnode.hpp","additions":35,"deletions":4,"binary":false,"changes":39,"status":"modified"},{"patch":"@@ -90,0 +90,1 @@\n+class FlatArrayCheckNode;\n@@ -122,0 +123,1 @@\n+class MachPrologNode;\n@@ -128,0 +130,1 @@\n+class MachVEPNode;\n@@ -187,0 +190,3 @@\n+class InlineTypeNode;\n+class LoadFlatNode;\n+class StoreFlatNode;\n@@ -693,0 +699,2 @@\n+        DEFINE_CLASS_ID(LoadFlat,  SafePoint, 1)\n+        DEFINE_CLASS_ID(StoreFlat, SafePoint, 2)\n@@ -709,0 +717,1 @@\n+        DEFINE_CLASS_ID(Blackhole,        MemBar, 2)\n@@ -730,0 +739,2 @@\n+      DEFINE_CLASS_ID(MachProlog,       Mach, 8)\n+      DEFINE_CLASS_ID(MachVEP,          Mach, 9)\n@@ -762,1 +773,2 @@\n-      DEFINE_CLASS_ID(Con, Type, 8)\n+      DEFINE_CLASS_ID(InlineType, Type, 8)\n+      DEFINE_CLASS_ID(Con, Type, 9)\n@@ -764,2 +776,2 @@\n-      DEFINE_CLASS_ID(SafePointScalarMerge, Type, 9)\n-      DEFINE_CLASS_ID(Convert, Type, 10)\n+      DEFINE_CLASS_ID(SafePointScalarMerge, Type, 10)\n+      DEFINE_CLASS_ID(Convert, Type, 11)\n@@ -804,3 +816,4 @@\n-        DEFINE_CLASS_ID(FastLock,   Cmp, 0)\n-        DEFINE_CLASS_ID(FastUnlock, Cmp, 1)\n-        DEFINE_CLASS_ID(SubTypeCheck,Cmp, 2)\n+        DEFINE_CLASS_ID(FastLock,       Cmp, 0)\n+        DEFINE_CLASS_ID(FastUnlock,     Cmp, 1)\n+        DEFINE_CLASS_ID(SubTypeCheck,   Cmp, 2)\n+        DEFINE_CLASS_ID(FlatArrayCheck, Cmp, 3)\n@@ -916,0 +929,1 @@\n+  DEFINE_CLASS_QUERY(Blackhole)\n@@ -949,0 +963,1 @@\n+  DEFINE_CLASS_QUERY(FlatArrayCheck)\n@@ -981,0 +996,1 @@\n+  DEFINE_CLASS_QUERY(MachProlog)\n@@ -987,0 +1003,1 @@\n+  DEFINE_CLASS_QUERY(MachVEP)\n@@ -1026,0 +1043,3 @@\n+  DEFINE_CLASS_QUERY(InlineType)\n+  DEFINE_CLASS_QUERY(LoadFlat)\n+  DEFINE_CLASS_QUERY(StoreFlat)\n","filename":"src\/hotspot\/share\/opto\/node.hpp","additions":26,"deletions":6,"binary":false,"changes":32,"status":"modified"},{"patch":"@@ -1042,1 +1042,1 @@\n-  const int max_live_nodes_increase_per_iteration = NodeLimitFudgeFactor * 3;\n+  const int max_live_nodes_increase_per_iteration = NodeLimitFudgeFactor * 5;\n@@ -1196,1 +1196,1 @@\n-  n->dump_bfs(1, nullptr, \"\", &ss);\n+  n->dump_bfs(3, nullptr, \"\", &ss);\n@@ -2104,6 +2104,0 @@\n-  if (_delay_transform) {\n-    \/\/ Register the node but don't optimize for now\n-    register_new_node_with_optimizer(n);\n-    return n;\n-  }\n-\n@@ -2116,0 +2110,6 @@\n+  if (_delay_transform) {\n+    \/\/ Add the node to the worklist but don't optimize for now\n+    _worklist.push(n);\n+    return n;\n+  }\n+\n@@ -2388,0 +2388,13 @@\n+void PhaseIterGVN::replace_in_uses(Node* n, Node* m) {\n+  assert(n != nullptr, \"sanity\");\n+  for (DUIterator_Fast imax, i = n->fast_outs(imax); i < imax; i++) {\n+    Node* u = n->fast_out(i);\n+    if (u != n) {\n+      rehash_node_delayed(u);\n+      int nb = u->replace_edge(n, m);\n+      --i, imax -= nb;\n+    }\n+  }\n+  assert(n->outcnt() == 0, \"all uses must be deleted\");\n+}\n+\n@@ -2443,0 +2456,10 @@\n+  \/\/ AndLNode::Ideal folds GraphKit::mark_word_test patterns. Give it a chance to run.\n+  if (n->is_Load() && use->is_Phi()) {\n+    for (DUIterator_Fast imax, i = use->fast_outs(imax); i < imax; i++) {\n+      Node* u = use->fast_out(i);\n+      if (u->Opcode() == Op_AndL) {\n+        worklist.push(u);\n+      }\n+    }\n+  }\n+\n@@ -2540,0 +2563,9 @@\n+  \/\/ Inline type nodes can have other inline types as users. If an input gets\n+  \/\/ updated, make sure that inline type users get a chance for optimization.\n+  if (use->is_InlineType()) {\n+    for (DUIterator_Fast i2max, i2 = use->fast_outs(i2max); i2 < i2max; i2++) {\n+      Node* u = use->fast_out(i2);\n+      if (u->is_InlineType())\n+        worklist.push(u);\n+    }\n+  }\n@@ -2675,0 +2707,18 @@\n+  if (use_op == Op_CastP2X) {\n+    for (DUIterator_Fast i2max, i2 = use->fast_outs(i2max); i2 < i2max; i2++) {\n+      Node* u = use->fast_out(i2);\n+      \/\/ TODO 8350865 Still needed? Yes, I think this is from PhaseMacroExpand::expand_mh_intrinsic_return\n+      if (u->Opcode() == Op_AndX) {\n+        worklist.push(u);\n+      }\n+      \/\/ Search for CmpL(OrL(CastP2X(..), CastP2X(..)), 0L)\n+      if (u->Opcode() == Op_OrL) {\n+        for (DUIterator_Fast i3max, i3 = u->fast_outs(i3max); i3 < i3max; i3++) {\n+          Node* cmp = u->fast_out(i3);\n+          if (cmp->Opcode() == Op_CmpL) {\n+            worklist.push(cmp);\n+          }\n+        }\n+      }\n+    }\n+  }\n@@ -2693,0 +2743,10 @@\n+  \/\/ Give CallStaticJavaNode::remove_useless_allocation a chance to run\n+  if (use->is_Region()) {\n+    Node* c = use;\n+    do {\n+      c = c->unique_ctrl_out_or_null();\n+    } while (c != nullptr && c->is_Region());\n+    if (c != nullptr && c->is_CallStaticJava() && c->as_CallStaticJava()->uncommon_trap_request() != 0) {\n+      worklist.push(c);\n+    }\n+  }\n@@ -2812,1 +2872,1 @@\n-    n->dump(1);\n+    n->dump(3);\n@@ -2976,0 +3036,1 @@\n+  push_cast(worklist, use);\n@@ -3087,0 +3148,13 @@\n+  }\n+}\n+\n+\/\/ TODO 8350865 Still needed? Yes, I think this is from PhaseMacroExpand::expand_mh_intrinsic_return\n+void PhaseCCP::push_cast(Unique_Node_List& worklist, const Node* use) {\n+  uint use_op = use->Opcode();\n+  if (use_op == Op_CastP2X) {\n+    for (DUIterator_Fast i2max, i2 = use->fast_outs(i2max); i2 < i2max; i2++) {\n+      Node* u = use->fast_out(i2);\n+      if (u->Opcode() == Op_AndX) {\n+        worklist.push(u);\n+      }\n+    }\n","filename":"src\/hotspot\/share\/opto\/phaseX.cpp","additions":83,"deletions":9,"binary":false,"changes":92,"status":"modified"},{"patch":"@@ -78,0 +78,2 @@\n+#include <string.h>\n+\n@@ -1803,1 +1805,0 @@\n-static unsigned int patch_mod_count = 0;\n@@ -1811,1 +1812,1 @@\n-  if (!CDSConfig::check_vm_args_consistency(patch_mod_javabase, mode_flag_cmd_line)) {\n+  if (!CDSConfig::check_vm_args_consistency(mode_flag_cmd_line)) {\n@@ -1978,0 +1979,4 @@\n+  if (UseAltSubstitutabilityMethod) {\n+    no_shared_spaces(\"Alternate substitutability method doesn't work with CDS yet\");\n+  }\n+\n@@ -2060,1 +2065,1 @@\n-      add_patch_mod_prefix(module_name, module_equal + 1);\n+      add_patch_mod_prefix(module_name, module_equal + 1, false \/* no append *\/, false \/* no cds *\/);\n@@ -2062,3 +2067,0 @@\n-      if (!create_numbered_module_property(\"jdk.module.patch\", patch_mod_tail, patch_mod_count++)) {\n-        return JNI_ENOMEM;\n-      }\n@@ -2072,0 +2074,23 @@\n+\/\/ Finalize --patch-module args and --enable-preview related to value class module patches.\n+\/\/ Create all numbered properties passing module patches.\n+int Arguments::finalize_patch_module() {\n+  \/\/ Create numbered properties for each module that has been patched by --patch-module.\n+  \/\/ Format is \"jdk.module.patch.<n>=<module_name>=<path>\"\n+  if (_patch_mod_prefix != nullptr) {\n+    char * prop_value = AllocateHeap(JVM_MAXPATHLEN + JVM_MAXPATHLEN + 1, mtArguments);\n+    unsigned int patch_mod_count = 0;\n+\n+    for (GrowableArrayIterator<ModulePatchPath *> it = _patch_mod_prefix->begin();\n+            it != _patch_mod_prefix->end(); ++it) {\n+      jio_snprintf(prop_value, JVM_MAXPATHLEN + JVM_MAXPATHLEN + 1, \"%s=%s\",\n+                   (*it)->module_name(), (*it)->path_string());\n+      if (!create_numbered_module_property(\"jdk.module.patch\", prop_value, patch_mod_count++)) {\n+        FreeHeap(prop_value);\n+        return JNI_ENOMEM;\n+      }\n+    }\n+    FreeHeap(prop_value);\n+  }\n+  return JNI_OK;\n+}\n+\n@@ -2863,10 +2888,5 @@\n-void Arguments::add_patch_mod_prefix(const char* module_name, const char* path) {\n-  \/\/ For java.base check for duplicate --patch-module options being specified on the command line.\n-  \/\/ This check is only required for java.base, all other duplicate module specifications\n-  \/\/ will be checked during module system initialization.  The module system initialization\n-  \/\/ will throw an ExceptionInInitializerError if this situation occurs.\n-  if (strcmp(module_name, JAVA_BASE_NAME) == 0) {\n-    if (patch_mod_javabase) {\n-      vm_exit_during_initialization(\"Cannot specify \" JAVA_BASE_NAME \" more than once to --patch-module\");\n-    } else {\n-      patch_mod_javabase = true;\n+void Arguments::add_patch_mod_prefix(const char* module_name, const char* path, bool allow_append, bool allow_cds) {\n+  if (!allow_cds) {\n+    CDSConfig::set_module_patching_disables_cds();\n+    if (strcmp(module_name, JAVA_BASE_NAME) == 0) {\n+      CDSConfig::set_java_base_module_patching_disables_cds();\n@@ -2881,1 +2901,18 @@\n-  _patch_mod_prefix->push(new ModulePatchPath(module_name, path));\n+  \/\/ Scan patches for matching module\n+  int i = _patch_mod_prefix->find_if([&](ModulePatchPath* patch) {\n+    return (strcmp(module_name, patch->module_name()) == 0);\n+  });\n+  if (i == -1) {\n+    _patch_mod_prefix->push(new ModulePatchPath(module_name, path));\n+  } else {\n+    if (allow_append) {\n+      \/\/ append path to existing module entry\n+      _patch_mod_prefix->at(i)->append_path(path);\n+    } else {\n+      if (strcmp(module_name, JAVA_BASE_NAME) == 0) {\n+        vm_exit_during_initialization(\"Cannot specify \" JAVA_BASE_NAME \" more than once to --patch-module\");\n+      } else {\n+        vm_exit_during_initialization(\"Cannot specify a module more than once to --patch-module\", module_name);\n+      }\n+    }\n+  }\n@@ -2994,1 +3031,4 @@\n-  if (!check_vm_args_consistency()) {\n+  ClassLoader::set_preview_mode(is_valhalla_enabled());\n+\n+  \/\/ finalize --module-patch.\n+  if (finalize_patch_module() != JNI_OK) {\n@@ -2998,0 +3038,3 @@\n+  if (!check_vm_args_consistency()) {\n+    return JNI_ERR;\n+  }\n@@ -3883,0 +3926,49 @@\n+  if (!is_valhalla_enabled()) {\n+#define WARN_IF_NOT_DEFAULT_FLAG(flag)                                                                       \\\n+    if (!FLAG_IS_DEFAULT(flag)) {                                                                            \\\n+      warning(\"Valhalla-specific flag \\\"%s\\\" has no effect when --enable-preview is not specified.\", #flag); \\\n+    }\n+\n+#define DISABLE_FLAG_AND_WARN_IF_NOT_DEFAULT(flag)  \\\n+    WARN_IF_NOT_DEFAULT_FLAG(flag)                  \\\n+    FLAG_SET_DEFAULT(flag, false);\n+\n+    DISABLE_FLAG_AND_WARN_IF_NOT_DEFAULT(InlineTypePassFieldsAsArgs);\n+    DISABLE_FLAG_AND_WARN_IF_NOT_DEFAULT(InlineTypeReturnedAsFields);\n+    DISABLE_FLAG_AND_WARN_IF_NOT_DEFAULT(UseArrayFlattening);\n+    DISABLE_FLAG_AND_WARN_IF_NOT_DEFAULT(UseFieldFlattening);\n+    DISABLE_FLAG_AND_WARN_IF_NOT_DEFAULT(UseNonAtomicValueFlattening);\n+    DISABLE_FLAG_AND_WARN_IF_NOT_DEFAULT(UseNullableValueFlattening);\n+    DISABLE_FLAG_AND_WARN_IF_NOT_DEFAULT(UseAtomicValueFlattening);\n+    DISABLE_FLAG_AND_WARN_IF_NOT_DEFAULT(PrintInlineLayout);\n+    DISABLE_FLAG_AND_WARN_IF_NOT_DEFAULT(PrintFlatArrayLayout);\n+    WARN_IF_NOT_DEFAULT_FLAG(FlatArrayElementMaxOops);\n+    WARN_IF_NOT_DEFAULT_FLAG(UseAltSubstitutabilityMethod);\n+#ifdef ASSERT\n+    DISABLE_FLAG_AND_WARN_IF_NOT_DEFAULT(StressCallingConvention);\n+    DISABLE_FLAG_AND_WARN_IF_NOT_DEFAULT(PreloadClasses);\n+    WARN_IF_NOT_DEFAULT_FLAG(PrintInlineKlassFields);\n+#endif\n+#ifdef COMPILER1\n+    DEBUG_ONLY(DISABLE_FLAG_AND_WARN_IF_NOT_DEFAULT(C1UseDelayedFlattenedFieldReads);)\n+#endif\n+#ifdef COMPILER2\n+    DISABLE_FLAG_AND_WARN_IF_NOT_DEFAULT(UseArrayLoadStoreProfile);\n+    DISABLE_FLAG_AND_WARN_IF_NOT_DEFAULT(UseACmpProfile);\n+#endif\n+#undef DISABLE_FLAG_AND_WARN_IF_NOT_DEFAULT\n+#undef WARN_IF_NOT_DEFAULT_FLAG\n+  } else {\n+    if (is_interpreter_only() && !CDSConfig::is_dumping_archive() && !UseSharedSpaces) {\n+      \/\/ Disable calling convention optimizations if inline types are not supported.\n+      \/\/ Also these aren't useful in -Xint. However, don't disable them when dumping or using\n+      \/\/ the CDS archive, as the values must match between dumptime and runtime.\n+      FLAG_SET_DEFAULT(InlineTypePassFieldsAsArgs, false);\n+      FLAG_SET_DEFAULT(InlineTypeReturnedAsFields, false);\n+    }\n+    if (!UseNonAtomicValueFlattening && !UseNullableValueFlattening && !UseAtomicValueFlattening) {\n+      \/\/ Flattening is disabled\n+      FLAG_SET_DEFAULT(UseArrayFlattening, false);\n+      FLAG_SET_DEFAULT(UseFieldFlattening, false);\n+    }\n+  }\n","filename":"src\/hotspot\/share\/runtime\/arguments.cpp","additions":110,"deletions":18,"binary":false,"changes":128,"status":"modified"},{"patch":"@@ -138,0 +138,1 @@\n+  do_blob(new_null_free_array)                                         \\\n@@ -139,0 +140,5 @@\n+  do_blob(load_flat_array)                                             \\\n+  do_blob(store_flat_array)                                            \\\n+  do_blob(substitutability_check)                                      \\\n+  do_blob(buffer_inline_args)                                          \\\n+  do_blob(buffer_inline_args_no_receiver)                              \\\n@@ -145,0 +151,2 @@\n+  do_blob(throw_illegal_monitor_state_exception)                       \\\n+  do_blob(throw_identity_exception)                                    \\\n@@ -232,0 +240,2 @@\n+  do_stub(load_unknown_inline, 0, true, false)                         \\\n+  do_stub(store_unknown_inline, 0, true, false)                        \\\n","filename":"src\/hotspot\/share\/runtime\/stubDeclarations.hpp","additions":10,"deletions":0,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -613,0 +613,9 @@\n+\/\/----------------------------------------------------------------------------------------------------\n+\/\/ Prototyping\n+\/\/ \"Code Missing Here\" macro, un-define when integrating back from prototyping stage and break\n+\/\/ compilation on purpose (i.e. \"forget me not\")\n+#define PROTOTYPE\n+#ifdef PROTOTYPE\n+#define CMH(m)\n+#endif\n+\n@@ -691,5 +700,6 @@\n-  T_ADDRESS     = 15,\n-  T_NARROWOOP   = 16,\n-  T_METADATA    = 17,\n-  T_NARROWKLASS = 18,\n-  T_CONFLICT    = 19, \/\/ for stack value type with conflicting contents\n+  T_FLAT_ELEMENT = 15, \/\/ Not a true BasicType, only used in layout helpers of flat arrays\n+  T_ADDRESS     = 16,\n+  T_NARROWOOP   = 17,\n+  T_METADATA    = 18,\n+  T_NARROWKLASS = 19,\n+  T_CONFLICT    = 20, \/\/ for stack value type with conflicting contents\n@@ -739,0 +749,1 @@\n+  assert(t != T_FLAT_ELEMENT, \"\");  \/\/ Strong assert to detect misuses of T_FLAT_ELEMENT\n@@ -813,1 +824,2 @@\n-  T_VOID_size        = 0\n+  T_VOID_size        = 0,\n+  T_FLAT_ELEMENT_size = 0\n@@ -849,1 +861,2 @@\n-  T_VOID_aelem_bytes        = 0\n+  T_VOID_aelem_bytes        = 0,\n+  T_FLAT_ELEMENT_aelem_bytes = 0\n@@ -939,1 +952,1 @@\n-  vtos = 9,             \/\/ tos not cached\n+  vtos = 9,             \/\/ tos not cached,\n@@ -956,1 +969,1 @@\n-    case T_ARRAY  : \/\/ fall through\n+    case T_ARRAY  :   \/\/ fall through\n@@ -1034,0 +1047,1 @@\n+const juint    badRegWordVal      = 0xDEADDA7A;             \/\/ value used to zap registers\n","filename":"src\/hotspot\/share\/utilities\/globalDefinitions.hpp","additions":23,"deletions":9,"binary":false,"changes":32,"status":"modified"},{"patch":"@@ -29,0 +29,1 @@\n+import jdk.internal.misc.PreviewFeatures;\n@@ -31,0 +32,1 @@\n+import jdk.internal.value.DeserializeConstructor;\n@@ -59,4 +61,12 @@\n- * class; programmers should treat instances that are\n- * {@linkplain #equals(Object) equal} as interchangeable and should not\n- * use instances for synchronization, or unpredictable behavior may\n- * occur. For example, in a future release, synchronization may fail.\n+ * class; programmers should treat instances that are {@linkplain #equals(Object) equal}\n+ * as interchangeable and should not use instances for synchronization, mutexes, or\n+ * with {@linkplain java.lang.ref.Reference object references}.\n+ *\n+ * <div class=\"preview-block\">\n+ *      <div class=\"preview-comment\">\n+ *          When preview features are enabled, {@code Integer} is a {@linkplain Class#isValue value class}.\n+ *          Use of value class instances for synchronization, mutexes, or with\n+ *          {@linkplain java.lang.ref.Reference object references} result in\n+ *          {@link IdentityException}.\n+ *      <\/div>\n+ * <\/div>\n@@ -73,0 +83,1 @@\n+@jdk.internal.MigratedValueClass\n@@ -974,8 +985,20 @@\n-     * {@code int} value.  If a new {@code Integer} instance is not\n-     * required, this method should generally be used in preference to\n-     * the constructor {@link #Integer(int)}, as this method is likely\n-     * to yield significantly better space and time performance by\n-     * caching frequently requested values.\n-     *\n-     * This method will always cache values in the range -128 to 127,\n-     * inclusive, and may cache other values outside of this range.\n+     * {@code int} value.\n+     * <div class=\"preview-block\">\n+     *      <div class=\"preview-comment\">\n+     *          <p>\n+     *              - When preview features are NOT enabled, {@code Integer} is an identity class.\n+     *              If a new {@code Integer} instance is not\n+     *              required, this method should generally be used in preference to\n+     *              the constructor {@link #Integer(int)}, as this method is likely\n+     *              to yield significantly better space and time performance by\n+     *              caching frequently requested values.\n+     *              This method will always cache values in the range -128 to 127,\n+     *              inclusive, and may cache other values outside of this range.\n+     *          <\/p>\n+     *          <p>\n+     *              - When preview features are enabled, {@code Integer} is a {@linkplain Class#isValue value class}.\n+     *              The {@code valueOf} behavior is the same as invoking the constructor,\n+     *              whether cached or not.\n+     *          <\/p>\n+     *      <\/div>\n+     * <\/div>\n@@ -988,0 +1011,1 @@\n+    @DeserializeConstructor\n@@ -989,2 +1013,4 @@\n-        if (i >= IntegerCache.low && i <= IntegerCache.high)\n-            return IntegerCache.cache[i + (-IntegerCache.low)];\n+        if (!PreviewFeatures.isEnabled()) {\n+            if (i >= IntegerCache.low && i <= IntegerCache.high)\n+                return IntegerCache.cache[i + (-IntegerCache.low)];\n+        }\n","filename":"src\/java.base\/share\/classes\/java\/lang\/Integer.java","additions":40,"deletions":14,"binary":false,"changes":54,"status":"modified"},{"patch":"@@ -37,0 +37,2 @@\n+import jdk.internal.misc.PreviewFeatures;\n+import jdk.internal.value.DeserializeConstructor;\n@@ -58,4 +60,13 @@\n- * class; programmers should treat instances that are\n- * {@linkplain #equals(Object) equal} as interchangeable and should not\n- * use instances for synchronization, or unpredictable behavior may\n- * occur. For example, in a future release, synchronization may fail.\n+ * class; programmers should treat instances that are {@linkplain #equals(Object) equal}\n+ * as interchangeable and should not use instances for synchronization, mutexes, or\n+ * with {@linkplain java.lang.ref.Reference object references}.\n+ *\n+ * <div class=\"preview-block\">\n+ *      <div class=\"preview-comment\">\n+ *          When preview features are enabled, {@code Long} is a {@linkplain Class#isValue value class}.\n+ *          Use of value class instances for synchronization, mutexes, or with\n+ *          {@linkplain java.lang.ref.Reference object references} result in\n+ *          {@link IdentityException}.\n+ *      <\/div>\n+ * <\/div>\n+ *\n@@ -72,0 +83,1 @@\n+@jdk.internal.MigratedValueClass\n@@ -944,8 +956,19 @@\n-     * If a new {@code Long} instance is not required, this method\n-     * should generally be used in preference to the constructor\n-     * {@link #Long(long)}, as this method is likely to yield\n-     * significantly better space and time performance by caching\n-     * frequently requested values.\n-     *\n-     * This method will always cache values in the range -128 to 127,\n-     * inclusive, and may cache other values outside of this range.\n+     * <div class=\"preview-block\">\n+     *      <div class=\"preview-comment\">\n+     *          <p>\n+     *              - When preview features are NOT enabled, {@code Long} is an identity class.\n+     *              If a new {@code Long} instance is not required, this method\n+     *              should generally be used in preference to the constructor\n+     *              {@link #Long(long)}, as this method is likely to yield\n+     *              significantly better space and time performance by caching\n+     *              frequently requested values.\n+     *              This method will always cache values in the range -128 to 127,\n+     *              inclusive, and may cache other values outside of this range.\n+     *          <\/p>\n+     *          <p>\n+     *              - When preview features are enabled, {@code Long} is a {@linkplain Class#isValue value class}.\n+     *              The {@code valueOf} behavior is the same as invoking the constructor,\n+     *              whether cached or not.\n+     *          <\/p>\n+     *      <\/div>\n+     * <\/div>\n@@ -958,0 +981,1 @@\n+    @DeserializeConstructor\n@@ -959,3 +983,5 @@\n-        final int offset = 128;\n-        if (l >= -128 && l <= 127) { \/\/ will cache\n-            return LongCache.cache[(int)l + offset];\n+        if (!PreviewFeatures.isEnabled()) {\n+            if (l >= -128 && l <= 127) { \/\/ will cache\n+                final int offset = 128;\n+                return LongCache.cache[(int) l + offset];\n+            }\n","filename":"src\/java.base\/share\/classes\/java\/lang\/Long.java","additions":41,"deletions":15,"binary":false,"changes":56,"status":"modified"},{"patch":"@@ -42,0 +42,1 @@\n+import java.lang.reflect.ClassFileFormatVersion;\n@@ -491,0 +492,15 @@\n+     * <div class=\"preview-block\">\n+     *      <div class=\"preview-comment\">\n+     *          The \"identity hash code\" of a {@linkplain Class#isValue() value object}\n+     *          is computed by combining the identity hash codes of the value object's fields recursively.\n+     *      <\/div>\n+     * <\/div>\n+     * @apiNote\n+     * <div class=\"preview-block\">\n+     *      <div class=\"preview-comment\">\n+     *          Note that, like ==, this hash code exposes information about a value object's\n+     *          private fields that might otherwise be hidden by an identity object.\n+     *          Developers should be cautious about storing sensitive secrets in value object fields.\n+     *      <\/div>\n+     * <\/div>\n+     *\n@@ -2329,0 +2345,4 @@\n+            public int classFileFormatVersion(Class<?> clazz) {\n+                return clazz.getClassFileVersion();\n+            }\n+\n","filename":"src\/java.base\/share\/classes\/java\/lang\/System.java","additions":20,"deletions":0,"binary":false,"changes":20,"status":"modified"},{"patch":"@@ -36,0 +36,1 @@\n+import java.lang.reflect.ClassFileFormatVersion;\n@@ -628,0 +629,6 @@\n+\n+    \/**\n+     * Returns the class file format version of the class.\n+     *\/\n+    int classFileFormatVersion(Class<?> klass);\n+\n","filename":"src\/java.base\/share\/classes\/jdk\/internal\/access\/JavaLangAccess.java","additions":7,"deletions":0,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -262,0 +262,6 @@\n+        \/**\n+         * Warn about code in identity classes that wouldn't be allowed in early\n+         * construction due to a this dependency.\n+         *\/\n+        INITIALIZATION(\"initialization\"),\n+\n@@ -277,0 +283,5 @@\n+        \/**\n+         * Warn about issues related to migration of JDK classes.\n+         *\/\n+        MIGRATION(\"migration\"),\n+\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/code\/Lint.java","additions":11,"deletions":0,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -100,0 +100,1 @@\n+    private final Preview preview;\n@@ -119,0 +120,1 @@\n+        preview = Preview.instance(context);\n@@ -378,0 +380,16 @@\n+            if (!c.type.isErroneous()\n+                    && toAnnotate.kind == TYP\n+                    && types.isSameType(c.type, syms.migratedValueClassType)) {\n+                toAnnotate.flags_field |= Flags.MIGRATED_VALUE_CLASS;\n+            }\n+\n+            if (!c.type.isErroneous()\n+                    && toAnnotate.kind == VAR\n+                    && toAnnotate.owner.kind == TYP\n+                    && types.isSameType(c.type, syms.strictType)) {\n+                preview.checkSourceLevel(pos.get(c), Feature.VALUE_CLASSES);\n+                toAnnotate.flags_field |= Flags.STRICT;\n+                \/\/ temporary hack to indicate that a class has at least one strict field\n+                toAnnotate.owner.flags_field |= Flags.HAS_STRICT;\n+            }\n+\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/comp\/Annotate.java","additions":18,"deletions":0,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n-# Copyright (c) 2016, 2025, Oracle and\/or its affiliates. All rights reserved.\n+# Copyright (c) 2016, 2026, Oracle and\/or its affiliates. All rights reserved.\n@@ -80,0 +80,23 @@\n+# Valhalla\n+compiler\/whitebox\/DeoptimizeRelocatedNMethod.java 8370571 generic-all\n+compiler\/whitebox\/RelocateNMethodMultiplePaths.java#C1 8370571 generic-all\n+compiler\/whitebox\/RelocateNMethodMultiplePaths.java#C2 8370571 generic-all\n+compiler\/whitebox\/RelocateNMethodMultiplePaths.java#G1C1 8370571 generic-all\n+compiler\/whitebox\/RelocateNMethodMultiplePaths.java#G1C2 8370571 generic-all\n+compiler\/whitebox\/RelocateNMethodMultiplePaths.java#ParallelC1 8370571 generic-all\n+compiler\/whitebox\/RelocateNMethodMultiplePaths.java#ParallelC2 8370571 generic-all\n+compiler\/whitebox\/RelocateNMethodMultiplePaths.java#SerialC1 8370571 generic-all\n+compiler\/whitebox\/RelocateNMethodMultiplePaths.java#SerialC2 8370571 generic-all\n+compiler\/whitebox\/RelocateNMethodMultiplePaths.java#ZGCC1 8370571 generic-all\n+compiler\/whitebox\/RelocateNMethodMultiplePaths.java#ZGCC2 8370571 generic-all\n+compiler\/whitebox\/StressNMethodRelocation.java 8370571 generic-all\n+compiler\/valhalla\/inlinetypes\/TestTrivialMethods.java 8373692 generic-all\n+compiler\/c2\/cmove\/TestScalarConditionalMoveCmpObj.java    8374122 generic-all\n+compiler\/codegen\/TestRedundantLea.java#GetAndSet          8361089 generic-all\n+compiler\/codegen\/TestRedundantLea.java#StringEquals       8361089 generic-all\n+compiler\/codegen\/TestRedundantLea.java#StringInflate      8361089 generic-all\n+compiler\/codegen\/TestRedundantLea.java#RegexFind          8361089 generic-all\n+compiler\/codegen\/TestRedundantLea.java#StoreNSerial       8361089 generic-all\n+compiler\/codegen\/TestRedundantLea.java#StoreNParallel     8361089 generic-all\n+compiler\/codegen\/TestRedundantLea.java#Spill              8361089 generic-all\n+\n@@ -101,0 +124,1 @@\n+runtime\/cds\/appcds\/redefineClass\/RedefineRunningMethods_Shared.java  8304168 generic-all\n@@ -120,0 +144,5 @@\n+\n+# Valhalla\n+runtime\/valhalla\/inlinetypes\/verifier\/StrictInstanceFieldsTest.java CODETOOLS-7904031 generic-all\n+runtime\/valhalla\/inlinetypes\/verifier\/StrictStaticFieldsTest.java CODETOOLS-7904031 generic-all\n+\n@@ -145,0 +174,57 @@\n+\n+# Valhalla TODO:\n+serviceability\/sa\/ClhsdbCDSCore.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbCDSJstackPrintAll.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbFindPC.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbInspect.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbLongConstant.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbJdis.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbJstack.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbPrintAll.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbPrintAs.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbPrintStatics.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbSource.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbSymbol.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbWhere.java 8190936 generic-all\n+serviceability\/sa\/JhsdbThreadInfoTest.java 8190936 generic-all\n+serviceability\/sa\/TestClassDump.java 8190936 generic-all\n+serviceability\/sa\/TestClhsdbJstackLock.java 8190936 generic-all\n+serviceability\/sa\/TestCpoolForInvokeDynamic.java 8190936 generic-all\n+serviceability\/sa\/TestHeapDumpForInvokeDynamic.java 8190936 generic-all\n+serviceability\/sa\/TestHeapDumpForLargeArray.java 8190936 generic-all\n+serviceability\/sa\/TestIntConstant.java 8190936 generic-all\n+serviceability\/sa\/TestJhsdbJstackLock.java 8190936 generic-all\n+serviceability\/sa\/TestJmapCore.java 8190936 generic-all\n+serviceability\/sa\/TestJmapCoreMetaspace.java 8190936 generic-all\n+serviceability\/sa\/TestPrintMdo.java 8190936 generic-all\n+serviceability\/sa\/jmap-hprof\/JMapHProfLargeHeapTest.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbDumpclass.java 8190936 generic-all\n+\n+# Array Changes TODO\n+serviceability\/sa\/CDSJMapClstats.java 8365722 generic-all\n+serviceability\/sa\/ClhsdbClasses.java 8365722 generic-all\n+serviceability\/sa\/sadebugd\/DisableRegistryTest.java 8365722 generic-all\n+serviceability\/sa\/ClhsdbDumpheap.java 8365722 generic-all\n+serviceability\/sa\/sadebugd\/ClhsdbTestConnectArgument.java 8365722 generic-all\n+serviceability\/sa\/sadebugd\/DebugdConnectTest.java 8365722 generic-all\n+serviceability\/sa\/ClhsdbJhisto.java 8365722 generic-all\n+serviceability\/sa\/ClhsdbJstack.java#id1 8365722 generic-all\n+serviceability\/sa\/ClhsdbJstackWithConcurrentLock.java 8365722 generic-all\n+serviceability\/sa\/TestJhsdbJstackWithVirtualThread.java 8365722 generic-all\n+serviceability\/sa\/ClhsdbJstackXcompStress.java 8365722 generic-all\n+serviceability\/sa\/ClhsdbPstack.java#process 8365722 generic-all\n+serviceability\/sa\/ClhsdbPstack.java#core 8365722 generic-all\n+serviceability\/sa\/ClhsdbScanOops.java#id0 8365722 generic-all\n+serviceability\/sa\/ClhsdbScanOops.java#id1 8365722 generic-all\n+serviceability\/sa\/ClhsdbScanOops.java#serial 8365722 generic-all\n+serviceability\/sa\/ClhsdbScanOops.java#parallel 8365722 generic-all\n+serviceability\/sa\/DeadlockDetectionTest.java 8365722 generic-all\n+serviceability\/sa\/ClhsdbJstack.java#id0 8365722 generic-all\n+serviceability\/sa\/TestInstanceKlassSize.java 8365722 generic-all\n+serviceability\/sa\/TestSysProps.java 8365722 generic-all\n+serviceability\/sa\/sadebugd\/ClhsdbAttachToDebugServer.java 8365722 generic-all\n+resourcehogs\/serviceability\/sa\/TestHeapDumpForLargeArray.java 8365722 generic-all\n+\n+resourcehogs\/serviceability\/sa\/ClhsdbRegionDetailsScanOopsForG1.java 8190936 generic-all\n+vmTestbase\/nsk\/jvmti\/scenarios\/events\/EM04\/em04t001\/TestDescription.java 8367590 generic-all\n+\n@@ -184,0 +270,2 @@\n+vmTestbase\/vm\/mlvm\/hiddenloader\/stress\/byteMutation\/Test.java 8317172 generic-all\n+\n@@ -185,0 +273,4 @@\n+\n+vmTestbase\/nsk\/jdb\/exclude\/exclude001\/exclude001.java 8375062 windows-x64\n+vmTestbase\/nsk\/jdi\/BScenarios\/multithrd\/tc04x001\/TestDescription.java 8375076 windows-x64\n+vmTestbase\/nsk\/jdi\/Scenarios\/invokeMethod\/popframes001\/TestDescription.java 8375076 windows-x64\n","filename":"test\/hotspot\/jtreg\/ProblemList.txt","additions":93,"deletions":1,"binary":false,"changes":94,"status":"modified"},{"patch":"@@ -0,0 +1,37 @@\n+\/*\n+ * Copyright (c) 2022, 2025, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+package compiler.valhalla.inlinetypes;\n+\n+public class InlineTypeRegexes {\n+    public static final String MYVALUE_KLASS = \"compiler\/valhalla\/inlinetypes\/.*MyValue\\\\w*\";\n+    public static final String ANY_KLASS = \"compiler\/valhalla\/inlinetypes\/[\\\\w\/]*\";\n+    public static final String STORE_INLINE_TYPE_FIELDS = \"store_inline_type_fields\";\n+    public static final String JDK_INTERNAL_MISC_UNSAFE = \"# Static  jdk.internal.misc.Unsafe::\";\n+    public static final String LOAD_UNKNOWN_INLINE = \"load_unknown_inline_blob \\\\(C2 runtime\\\\)\";\n+    public static final String STORE_UNKNOWN_INLINE = \"store_unknown_inline_blob \\\\(C2 runtime\\\\)\";\n+    public static final String INLINE_ARRAY_NULL_GUARD = \"null_check' action='none'\";\n+    public static final String JLONG_DISJOINT_ARRAYCOPY = \"jlong_disjoint_arraycopy\";\n+    public static final String CHECKCAST_ARRAYCOPY = \"checkcast_arraycopy\";\n+    public static final String JAVA_LANG_OBJECT_CLONE = \"java.lang.Object::clone\";\n+}\n","filename":"test\/hotspot\/jtreg\/compiler\/valhalla\/inlinetypes\/InlineTypeRegexes.java","additions":37,"deletions":0,"binary":false,"changes":37,"status":"added"},{"patch":"@@ -507,0 +507,5 @@\n+java\/lang\/ModuleLayer\/LayerControllerTest.java                  8337048 generic-all\n+java\/lang\/ModuleLayer\/BasicLayerTest.java                       8337048 generic-all\n+\n+java\/lang\/Thread\/virtual\/stress\/Skynet.java#default             8342977 generic-all\n+\n@@ -524,0 +529,1 @@\n+com\/sun\/management\/HotSpotDiagnosticMXBean\/DumpThreadsWithEliminatedLock.java 8360599 generic-all\n@@ -545,0 +551,2 @@\n+java\/net\/CookieHandler\/B6644726.java                            8365811 generic-all\n+\n@@ -682,0 +690,4 @@\n+com\/sun\/jdi\/cds\/CDSBreakpointTest.java                          8304168 generic-all\n+com\/sun\/jdi\/cds\/CDSDeleteAllBkptsTest.java                      8304168 generic-all\n+com\/sun\/jdi\/cds\/CDSFieldWatchpoints.java                        8304168 generic-all\n+\n@@ -690,0 +702,1 @@\n+java\/util\/Collection\/MOAT.java 8375086 generic-all\n@@ -775,0 +788,1 @@\n+\n@@ -780,0 +794,19 @@\n+\n+############################################################################\n+\n+# valhalla\n+java\/lang\/invoke\/VarHandles\/VarHandleTestMethodHandleAccessValue.java 8367346 generic-all\n+\n+jdk\/classfile\/AccessFlagsTest.java 8366270 generic-all\n+jdk\/jfr\/event\/runtime\/TestClassLoaderStatsEvent.java 8366820 generic-all\n+\n+jdk\/internal\/vm\/Continuation\/Fuzz.java#default 8370177 generic-aarch64,generic-x64\n+jdk\/internal\/vm\/Continuation\/Fuzz.java#preserve-fp 8370177 generic-aarch64,generic-x64\n+\n+sun\/tools\/jhsdb\/BasicLauncherTest.java 8366806 generic-all\n+sun\/tools\/jhsdb\/HeapDumpTest.java 8366806 generic-all\n+sun\/tools\/jhsdb\/HeapDumpTestWithActiveProcess.java 8366806 generic-all\n+sun\/tools\/jhsdb\/JShellHeapDumpTest.java 8366806 generic-all\n+sun\/tools\/jhsdb\/JStackStressTest.java 8375470 macosx-aarch64\n+\n+tools\/sincechecker\/modules\/java.base\/JavaBaseCheckSince.java 8375574 generic-all\n","filename":"test\/jdk\/ProblemList.txt","additions":33,"deletions":0,"binary":false,"changes":33,"status":"modified"},{"patch":"@@ -26,1 +26,1 @@\n- * @bug 8246774\n+ * @bug 8246774 8326879\n@@ -29,0 +29,1 @@\n+ * @run testng\/othervm --enable-preview RecordClassTest\n","filename":"test\/jdk\/java\/io\/Serializable\/records\/RecordClassTest.java","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -70,0 +70,1 @@\n+    @SuppressWarnings(\"initialization\")\n","filename":"test\/lib\/jdk\/test\/lib\/cds\/CDSAppTester.java","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -63,0 +63,1 @@\n+    @SuppressWarnings(\"initialization\")\n","filename":"test\/lib\/jdk\/test\/lib\/cds\/SimpleCDSAppTester.java","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"}]}