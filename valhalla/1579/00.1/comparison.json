{"files":[{"patch":"@@ -1657,0 +1657,3 @@\n+  } else if (_entry_point == nullptr) {\n+    \/\/ See CallLeafNoFPIndirect\n+    return 1 * NativeInstruction::instruction_size;\n@@ -1765,12 +1768,1 @@\n-  \/\/ n.b. frame size includes space for return pc and rfp\n-  const int framesize = C->output()->frame_size_in_bytes();\n-\n-  if (C->clinit_barrier_on_entry()) {\n-    assert(!C->method()->holder()->is_not_initialized(), \"initialization should have been started\");\n-\n-    Label L_skip_barrier;\n-    __ mov_metadata(rscratch2, C->method()->holder()->constant_encoding());\n-    __ clinit_barrier(rscratch2, rscratch1, &L_skip_barrier);\n-    __ far_jump(RuntimeAddress(SharedRuntime::get_handle_wrong_method_stub()));\n-    __ bind(L_skip_barrier);\n-  }\n+  __ verified_entry(C, 0);\n@@ -1779,2 +1771,2 @@\n-  if (C->max_vector_size() > 0) {\n-    __ reinitialize_ptrue();\n+  if (C->stub_function() == nullptr) {\n+    __ entry_barrier();\n@@ -1783,25 +1775,2 @@\n-  int bangsize = C->output()->bang_size_in_bytes();\n-  if (C->output()->need_stack_bang(bangsize))\n-    __ generate_stack_overflow_check(bangsize);\n-\n-  __ build_frame(framesize);\n-\n-  if (C->stub_function() == nullptr) {\n-    BarrierSetAssembler* bs = BarrierSet::barrier_set()->barrier_set_assembler();\n-    \/\/ Dummy labels for just measuring the code size\n-    Label dummy_slow_path;\n-    Label dummy_continuation;\n-    Label dummy_guard;\n-    Label* slow_path = &dummy_slow_path;\n-    Label* continuation = &dummy_continuation;\n-    Label* guard = &dummy_guard;\n-    if (!Compile::current()->output()->in_scratch_emit_size()) {\n-      \/\/ Use real labels from actual stub when not emitting code for the purpose of measuring its size\n-      C2EntryBarrierStub* stub = new (Compile::current()->comp_arena()) C2EntryBarrierStub();\n-      Compile::current()->output()->add_stub(stub);\n-      slow_path = &stub->entry();\n-      continuation = &stub->continuation();\n-      guard = &stub->guard();\n-    }\n-    \/\/ In the C2 code, we move the non-hot part of nmethod entry barriers out-of-line to a stub.\n-    bs->nmethod_entry_barrier(masm, slow_path, continuation, guard);\n+  if (!Compile::current()->output()->in_scratch_emit_size()) {\n+    __ bind(*_verified_entry);\n@@ -1824,6 +1793,0 @@\n-uint MachPrologNode::size(PhaseRegAlloc* ra_) const\n-{\n-  return MachNode::size(ra_); \/\/ too many variables; just compute it\n-                              \/\/ the hard way\n-}\n-\n@@ -1872,1 +1835,1 @@\n-  __ remove_frame(framesize);\n+  __ remove_frame(framesize, C->needs_stack_repair());\n@@ -1891,5 +1854,0 @@\n-uint MachEpilogNode::size(PhaseRegAlloc *ra_) const {\n-  \/\/ Variable size. Determine dynamically.\n-  return MachNode::size(ra_);\n-}\n-\n@@ -2191,1 +2149,12 @@\n-\/\/=============================================================================\n+\/\/\/=============================================================================\n+#ifndef PRODUCT\n+void MachVEPNode::format(PhaseRegAlloc* ra_, outputStream* st) const\n+{\n+  st->print_cr(\"# MachVEPNode\");\n+  if (!_verified) {\n+    st->print_cr(\"\\t load_class\");\n+  } else {\n+    st->print_cr(\"\\t unpack_inline_arg\");\n+  }\n+}\n+#endif\n@@ -2193,0 +2162,27 @@\n+void MachVEPNode::emit(C2_MacroAssembler *masm, PhaseRegAlloc* ra_) const\n+{\n+  if (!_verified) {\n+    __ ic_check(1);\n+  } else {\n+    \/\/ TODO 8284443 Avoid creation of temporary frame\n+    if (ra_->C->stub_function() == nullptr) {\n+      __ verified_entry(ra_->C, 0);\n+      __ entry_barrier();\n+      int framesize = ra_->C->output()->frame_slots() << LogBytesPerInt;\n+      __ remove_frame(framesize, false);\n+    }\n+    \/\/ Unpack inline type args passed as oop and then jump to\n+    \/\/ the verified entry point (skipping the unverified entry).\n+    int sp_inc = __ unpack_inline_args(ra_->C, _receiver_only);\n+    \/\/ Emit code for verified entry and save increment for stack repair on return\n+    __ verified_entry(ra_->C, sp_inc);\n+    if (Compile::current()->output()->in_scratch_emit_size()) {\n+      Label dummy_verified_entry;\n+      __ b(dummy_verified_entry);\n+    } else {\n+      __ b(*_verified_entry);\n+    }\n+  }\n+}\n+\n+\/\/=============================================================================\n@@ -2215,5 +2211,0 @@\n-uint MachUEPNode::size(PhaseRegAlloc* ra_) const\n-{\n-  return MachNode::size(ra_);\n-}\n-\n@@ -3712,0 +3703,31 @@\n+    if (tf()->returns_inline_type_as_fields() && !_method->is_method_handle_intrinsic() && _method->return_type()->is_loaded()) {\n+      \/\/ The last return value is not set by the callee but used to pass the null marker to compiled code.\n+      \/\/ Search for the corresponding projection, get the register and emit code that initialized it.\n+      uint con = (tf()->range_cc()->cnt() - 1);\n+      for (DUIterator_Fast imax, i = fast_outs(imax); i < imax; i++) {\n+        ProjNode* proj = fast_out(i)->as_Proj();\n+        if (proj->_con == con) {\n+          \/\/ Set null marker if r0 is non-null (a non-null value is returned buffered or scalarized)\n+          OptoReg::Name optoReg = ra_->get_reg_first(proj);\n+          VMReg reg = OptoReg::as_VMReg(optoReg, ra_->_framesize, OptoReg::reg2stack(ra_->_matcher._new_SP));\n+          Register toReg = reg->is_reg() ? reg->as_Register() : rscratch1;\n+          __ cmp(r0, zr);\n+          __ cset(toReg, Assembler::NE);\n+          if (reg->is_stack()) {\n+            int st_off = reg->reg2stack() * VMRegImpl::stack_slot_size;\n+            __ str(toReg, Address(sp, st_off));\n+          }\n+          break;\n+        }\n+      }\n+      if (return_value_is_used()) {\n+        \/\/ An inline type is returned as fields in multiple registers.\n+        \/\/ R0 either contains an oop if the inline type is buffered or a pointer\n+        \/\/ to the corresponding InlineKlass with the lowest bit set to 1. Zero r0\n+        \/\/ if the lowest bit is set to allow C2 to use the oop after null checking.\n+        \/\/ r0 &= (r0 & 1) - 1\n+        __ andr(rscratch1, r0, 0x1);\n+        __ sub(rscratch1, rscratch1, 0x1);\n+        __ andr(r0, r0, rscratch1);\n+      }\n+    }\n@@ -6821,1 +6843,1 @@\n-    \"mov  $dst, $con\\t# ptr\\n\\t\"\n+    \"mov  $dst, $con\\t# ptr\"\n@@ -8030,0 +8052,30 @@\n+instruct castI2N(iRegNNoSp dst, iRegI src) %{\n+  match(Set dst (CastI2N src));\n+\n+  ins_cost(INSN_COST);\n+  format %{ \"mov $dst, $src\\t# int -> narrow ptr\" %}\n+\n+  ins_encode %{\n+    if ($dst$$reg != $src$$reg) {\n+      __ mov(as_Register($dst$$reg), as_Register($src$$reg));\n+    }\n+  %}\n+\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct castN2X(iRegLNoSp dst, iRegN src) %{\n+  match(Set dst (CastP2X src));\n+\n+  ins_cost(INSN_COST);\n+  format %{ \"mov $dst, $src\\t# ptr -> long\" %}\n+\n+  ins_encode %{\n+    if ($dst$$reg != $src$$reg) {\n+      __ mov(as_Register($dst$$reg), as_Register($src$$reg));\n+    }\n+  %}\n+\n+  ins_pipe(ialu_reg);\n+%}\n+\n@@ -14983,1 +15035,1 @@\n-instruct clearArray_reg_reg(iRegL_R11 cnt, iRegP_R10 base, Universe dummy, rFlagsReg cr)\n+instruct clearArray_reg_reg_immL0(iRegL_R11 cnt, iRegP_R10 base, immL0 zero, Universe dummy, rFlagsReg cr)\n@@ -14985,1 +15037,1 @@\n-  match(Set dummy (ClearArray cnt base));\n+  match(Set dummy (ClearArray (Binary cnt base) zero));\n@@ -15002,0 +15054,16 @@\n+instruct clearArray_reg_reg(iRegL_R11 cnt, iRegP_R10 base, iRegL val, Universe dummy, rFlagsReg cr)\n+%{\n+  predicate(((ClearArrayNode*)n)->word_copy_only());\n+  match(Set dummy (ClearArray (Binary cnt base) val));\n+  effect(USE_KILL cnt, USE_KILL base, KILL cr);\n+\n+  ins_cost(4 * INSN_COST);\n+  format %{ \"ClearArray $cnt, $base, $val\" %}\n+\n+  ins_encode %{\n+    __ fill_words($base$$Register, $cnt$$Register, $val$$Register);\n+  %}\n+\n+  ins_pipe(pipe_class_memory);\n+%}\n+\n@@ -15005,1 +15073,2 @@\n-            < (uint64_t)(BlockZeroingLowLimit >> LogBytesPerWord));\n+            < (uint64_t)(BlockZeroingLowLimit >> LogBytesPerWord)\n+            && !((ClearArrayNode*)n)->word_copy_only());\n@@ -16356,0 +16425,18 @@\n+\/\/ entry point is null, target holds the address to call\n+instruct CallLeafNoFPIndirect(iRegP target)\n+%{\n+  predicate(n->as_Call()->entry_point() == nullptr);\n+\n+  match(CallLeafNoFP target);\n+\n+  ins_cost(CALL_COST);\n+\n+  format %{ \"CALL, runtime leaf nofp indirect $target\" %}\n+\n+  ins_encode %{\n+    __ blr($target$$Register);\n+  %}\n+\n+  ins_pipe(pipe_class_call);\n+%}\n+\n@@ -16358,0 +16445,2 @@\n+  predicate(n->as_Call()->entry_point() != nullptr);\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/aarch64.ad","additions":150,"deletions":61,"binary":false,"changes":211,"status":"modified"},{"patch":"@@ -35,0 +35,1 @@\n+#include \"oops\/constMethodFlags.hpp\"\n@@ -38,0 +39,1 @@\n+#include \"oops\/inlineKlass.hpp\"\n@@ -211,0 +213,52 @@\n+void InterpreterMacroAssembler::allocate_instance(Register klass, Register new_obj,\n+                                                  Register t1, Register t2,\n+                                                  bool clear_fields, Label& alloc_failed) {\n+  MacroAssembler::allocate_instance(klass, new_obj, t1, t2, clear_fields, alloc_failed);\n+  if (DTraceAllocProbes) {\n+    \/\/ Trigger dtrace event for fastpath\n+    push(atos);\n+    call_VM_leaf(CAST_FROM_FN_PTR(address, static_cast<int (*)(oopDesc*)>(SharedRuntime::dtrace_object_alloc)), new_obj);\n+    pop(atos);\n+  }\n+}\n+\n+void InterpreterMacroAssembler::read_flat_field(Register entry,\n+                                                Register field_index, Register field_offset,\n+                                                Register temp, Register obj) {\n+  Label alloc_failed, done;\n+  const Register src = field_offset;\n+  const Register alloc_temp = r10;\n+  const Register dst_temp   = field_index;\n+  const Register layout_info = temp;\n+  assert_different_registers(obj, entry, field_index, field_offset, temp, alloc_temp);\n+\n+  \/\/ Grab the inline field klass\n+  ldr(rscratch1, Address(entry, in_bytes(ResolvedFieldEntry::field_holder_offset())));\n+  inline_layout_info(rscratch1, field_index, layout_info);\n+\n+  const Register field_klass = dst_temp;\n+  ldr(field_klass, Address(layout_info, in_bytes(InlineLayoutInfo::klass_offset())));\n+\n+  \/\/ allocate buffer\n+  push(obj); \/\/ save holder\n+  allocate_instance(field_klass, obj, alloc_temp, rscratch2, false, alloc_failed);\n+\n+  \/\/ Have an oop instance buffer, copy into it\n+  payload_address(obj, dst_temp, field_klass);  \/\/ danger, uses rscratch1\n+  pop(alloc_temp);             \/\/ restore holder\n+  lea(src, Address(alloc_temp, field_offset));\n+  \/\/ call_VM_leaf, clobbers a few regs, save restore new obj\n+  push(obj);\n+  flat_field_copy(IS_DEST_UNINITIALIZED, src, dst_temp, layout_info);\n+  pop(obj);\n+  b(done);\n+\n+  bind(alloc_failed);\n+  pop(obj);\n+  call_VM(obj, CAST_FROM_FN_PTR(address, InterpreterRuntime::read_flat_field),\n+          obj, entry);\n+\n+  bind(done);\n+  membar(Assembler::StoreStore);\n+}\n+\n@@ -245,1 +299,2 @@\n-                                                  Label& ok_is_subtype) {\n+                                                  Label& ok_is_subtype,\n+                                                  bool profile) {\n@@ -251,1 +306,3 @@\n-  profile_typecheck(r2, Rsub_klass, r5); \/\/ blows r2, reloads r5\n+  if (profile) {\n+    profile_typecheck(r2, Rsub_klass, r5); \/\/ blows r2, reloads r5\n+  }\n@@ -627,0 +684,1 @@\n+\n@@ -654,0 +712,44 @@\n+  if (state == atos && InlineTypeReturnedAsFields) {\n+    Label skip;\n+    Label not_null;\n+    cbnz(r0, not_null);\n+    \/\/ Returned value is null, zero all return registers because they may belong to oop fields\n+    mov(j_rarg1, zr);\n+    mov(j_rarg2, zr);\n+    mov(j_rarg3, zr);\n+    mov(j_rarg4, zr);\n+    mov(j_rarg5, zr);\n+    mov(j_rarg6, zr);\n+    mov(j_rarg7, zr);\n+    b(skip);\n+    bind(not_null);\n+\n+    \/\/ Check if we are returning an non-null inline type and load its fields into registers\n+    test_oop_is_not_inline_type(r0, rscratch2, skip, \/* can_be_null= *\/ false);\n+\n+    \/\/ Load fields from a buffered value with an inline class specific handler\n+    load_klass(rscratch1 \/*dst*\/, r0 \/*src*\/);\n+    ldr(rscratch1, Address(rscratch1, InstanceKlass::adr_inlineklass_fixed_block_offset()));\n+    ldr(rscratch1, Address(rscratch1, InlineKlass::unpack_handler_offset()));\n+    \/\/ Unpack handler can be null if inline type is not scalarizable in returns\n+    cbz(rscratch1, skip);\n+\n+    blr(rscratch1);\n+#ifdef ASSERT\n+    \/\/ TODO 8284443 Enable\n+    if (StressCallingConvention && false) {\n+      Label skip_stress;\n+      ldr(rscratch1, Address(rfp, frame::interpreter_frame_method_offset * wordSize));\n+      ldrw(rscratch1, Address(rscratch1, Method::flags_offset()));\n+      tstw(rscratch1, MethodFlags::has_scalarized_return_flag());\n+      br(Assembler::EQ, skip_stress);\n+      load_klass(r0, r0);\n+      orr(r0, r0, 1);\n+      bind(skip_stress);\n+    }\n+#endif\n+    bind(skip);\n+    \/\/ Check above kills sender esp in rscratch2. Reload it.\n+    ldr(rscratch2, Address(rfp, frame::interpreter_frame_sender_sp_offset * wordSize));\n+  }\n+\n@@ -732,0 +834,4 @@\n+      if (EnableValhalla) {\n+        \/\/ Mask inline_type bit such that we go to the slow path if object is an inline type\n+        andr(swap_reg, swap_reg, ~((int) markWord::inline_type_bit_in_place));\n+      }\n@@ -1036,1 +1142,1 @@\n-void InterpreterMacroAssembler::profile_not_taken_branch(Register mdp) {\n+void InterpreterMacroAssembler::profile_not_taken_branch(Register mdp, bool acmp) {\n@@ -1048,1 +1154,1 @@\n-    update_mdp_by_constant(mdp, in_bytes(BranchData::branch_data_size()));\n+    update_mdp_by_constant(mdp, acmp ? in_bytes(ACmpData::acmp_data_size()) : in_bytes(BranchData::branch_data_size()));\n@@ -1371,0 +1477,114 @@\n+template <class ArrayData> void InterpreterMacroAssembler::profile_array_type(Register mdp,\n+                                                                              Register array,\n+                                                                              Register tmp) {\n+  if (ProfileInterpreter) {\n+    Label profile_continue;\n+\n+    \/\/ If no method data exists, go to profile_continue.\n+    test_method_data_pointer(mdp, profile_continue);\n+\n+    mov(tmp, array);\n+    profile_obj_type(tmp, Address(mdp, in_bytes(ArrayData::array_offset())));\n+\n+    Label not_flat;\n+    test_non_flat_array_oop(array, tmp, not_flat);\n+\n+    set_mdp_flag_at(mdp, ArrayData::flat_array_byte_constant());\n+\n+    bind(not_flat);\n+\n+    Label not_null_free;\n+    test_non_null_free_array_oop(array, tmp, not_null_free);\n+\n+    set_mdp_flag_at(mdp, ArrayData::null_free_array_byte_constant());\n+\n+    bind(not_null_free);\n+\n+    bind(profile_continue);\n+  }\n+}\n+\n+template void InterpreterMacroAssembler::profile_array_type<ArrayLoadData>(Register mdp,\n+                                                                           Register array,\n+                                                                           Register tmp);\n+template void InterpreterMacroAssembler::profile_array_type<ArrayStoreData>(Register mdp,\n+                                                                            Register array,\n+                                                                            Register tmp);\n+\n+void InterpreterMacroAssembler::profile_multiple_element_types(Register mdp, Register element, Register tmp, const Register tmp2) {\n+  if (ProfileInterpreter) {\n+    Label profile_continue;\n+\n+    \/\/ If no method data exists, go to profile_continue.\n+    test_method_data_pointer(mdp, profile_continue);\n+\n+    Label done, update;\n+    cbnz(element, update);\n+    set_mdp_flag_at(mdp, BitData::null_seen_byte_constant());\n+    b(done);\n+\n+    bind(update);\n+    load_klass(tmp, element);\n+\n+    \/\/ Record the object type.\n+    record_klass_in_profile(tmp, mdp, tmp2);\n+\n+    bind(done);\n+\n+    \/\/ The method data pointer needs to be updated.\n+    update_mdp_by_constant(mdp, in_bytes(ArrayStoreData::array_store_data_size()));\n+\n+    bind(profile_continue);\n+  }\n+}\n+\n+\n+void InterpreterMacroAssembler::profile_element_type(Register mdp,\n+                                                     Register element,\n+                                                     Register tmp) {\n+  if (ProfileInterpreter) {\n+    Label profile_continue;\n+\n+    \/\/ If no method data exists, go to profile_continue.\n+    test_method_data_pointer(mdp, profile_continue);\n+\n+    mov(tmp, element);\n+    profile_obj_type(tmp, Address(mdp, in_bytes(ArrayLoadData::element_offset())));\n+\n+    \/\/ The method data pointer needs to be updated.\n+    update_mdp_by_constant(mdp, in_bytes(ArrayLoadData::array_load_data_size()));\n+\n+    bind(profile_continue);\n+  }\n+}\n+\n+void InterpreterMacroAssembler::profile_acmp(Register mdp,\n+                                             Register left,\n+                                             Register right,\n+                                             Register tmp) {\n+  if (ProfileInterpreter) {\n+    Label profile_continue;\n+\n+    \/\/ If no method data exists, go to profile_continue.\n+    test_method_data_pointer(mdp, profile_continue);\n+\n+    mov(tmp, left);\n+    profile_obj_type(tmp, Address(mdp, in_bytes(ACmpData::left_offset())));\n+\n+    Label left_not_inline_type;\n+    test_oop_is_not_inline_type(left, tmp, left_not_inline_type);\n+    set_mdp_flag_at(mdp, ACmpData::left_inline_type_byte_constant());\n+    bind(left_not_inline_type);\n+\n+    mov(tmp, right);\n+    profile_obj_type(tmp, Address(mdp, in_bytes(ACmpData::right_offset())));\n+\n+    Label right_not_inline_type;\n+    test_oop_is_not_inline_type(right, tmp, right_not_inline_type);\n+    set_mdp_flag_at(mdp, ACmpData::right_inline_type_byte_constant());\n+    bind(right_not_inline_type);\n+\n+    bind(profile_continue);\n+  }\n+}\n+\n@@ -1669,1 +1889,1 @@\n-        assert(ReturnTypeEntry::static_cell_count() < TypeStackSlotEntries::per_arg_count(), \"can't move past ret type\");\n+        assert(SingleTypeEntry::static_cell_count() < TypeStackSlotEntries::per_arg_count(), \"can't move past ret type\");\n@@ -1715,1 +1935,1 @@\n-    Address mdo_ret_addr(mdp, -in_bytes(ReturnTypeEntry::size()));\n+    Address mdo_ret_addr(mdp, -in_bytes(SingleTypeEntry::size()));\n","filename":"src\/hotspot\/cpu\/aarch64\/interp_masm_aarch64.cpp","additions":226,"deletions":6,"binary":false,"changes":232,"status":"modified"},{"patch":"@@ -147,0 +147,22 @@\n+  \/\/ Kills t1 and t2, perserves klass, return allocation in new_obj\n+  void allocate_instance(Register klass, Register new_obj,\n+                         Register t1, Register t2,\n+                         bool clear_fields, Label& alloc_failed);\n+\n+  \/\/ Allocate instance in \"obj\" and read in the content of the inline field\n+  \/\/ NOTES:\n+  \/\/   - input holder object via \"obj\", which must be r0,\n+  \/\/     will return new instance via the same reg\n+  \/\/   - assumes holder_klass and valueKlass field klass have both been resolved\n+  void read_flat_field(Register entry,\n+                       Register field_index, Register field_offset,\n+                       Register temp, Register obj = r0);\n+\n+  \/\/ Allocate value buffer in \"obj\" and read in flat element at the given index\n+  \/\/ NOTES:\n+  \/\/   - Return via \"obj\" must be r0\n+  \/\/   - kills all given regs\n+  void read_flat_element(Register array, Register index,\n+                         Register t1, Register t2,\n+                         Register obj = r0);\n+\n@@ -191,1 +213,1 @@\n-  void gen_subtype_check( Register sub_klass, Label &ok_is_subtype );\n+  void gen_subtype_check( Register sub_klass, Label &ok_is_subtype, bool profile = true);\n@@ -280,1 +302,1 @@\n-  void profile_not_taken_branch(Register mdp);\n+  void profile_not_taken_branch(Register mdp, bool acmp = false);\n@@ -293,0 +315,4 @@\n+  template <class ArrayData> void profile_array_type(Register mdp, Register array, Register tmp);\n+  void profile_multiple_element_types(Register mdp, Register element, Register tmp, Register tmp2);\n+  void profile_element_type(Register mdp, Register element, Register tmp);\n+  void profile_acmp(Register mdp, Register left, Register right, Register tmp);\n","filename":"src\/hotspot\/cpu\/aarch64\/interp_masm_aarch64.hpp","additions":28,"deletions":2,"binary":false,"changes":30,"status":"modified"},{"patch":"@@ -175,0 +175,1 @@\n+  case Bytecodes::_fast_vputfield:\n@@ -756,4 +757,4 @@\n-    \/\/ ??? convention: move array into r3 for exception message\n-  __ mov(r3, array);\n-  __ mov(rscratch1, Interpreter::_throw_ArrayIndexOutOfBoundsException_entry);\n-  __ br(rscratch1);\n+  \/\/ ??? convention: move array into r3 for exception message\n+   __ mov(r3, array);\n+   __ mov(rscratch1, Interpreter::_throw_ArrayIndexOutOfBoundsException_entry);\n+   __ br(rscratch1);\n@@ -819,5 +820,20 @@\n-  __ add(r1, r1, arrayOopDesc::base_offset_in_bytes(T_OBJECT) >> LogBytesPerHeapOop);\n-  do_oop_load(_masm,\n-              Address(r0, r1, Address::uxtw(LogBytesPerHeapOop)),\n-              r0,\n-              IS_ARRAY);\n+  __ profile_array_type<ArrayLoadData>(r2, r0, r4);\n+  if (UseArrayFlattening) {\n+    Label is_flat_array, done;\n+\n+    __ test_flat_array_oop(r0, rscratch1 \/*temp*\/, is_flat_array);\n+    __ add(r1, r1, arrayOopDesc::base_offset_in_bytes(T_OBJECT) >> LogBytesPerHeapOop);\n+    do_oop_load(_masm, Address(r0, r1, Address::uxtw(LogBytesPerHeapOop)), r0, IS_ARRAY);\n+\n+    __ b(done);\n+    __ bind(is_flat_array);\n+    __ call_VM(r0, CAST_FROM_FN_PTR(address, InterpreterRuntime::flat_array_load), r0, r1);\n+    \/\/ Ensure the stores to copy the inline field contents are visible\n+    \/\/ before any subsequent store that publishes this reference.\n+    __ membar(Assembler::StoreStore);\n+    __ bind(done);\n+  } else {\n+    __ add(r1, r1, arrayOopDesc::base_offset_in_bytes(T_OBJECT) >> LogBytesPerHeapOop);\n+    do_oop_load(_masm, Address(r0, r1, Address::uxtw(LogBytesPerHeapOop)), r0, IS_ARRAY);\n+  }\n+  __ profile_element_type(r2, r0, r4);\n@@ -1110,1 +1126,1 @@\n-  Label is_null, ok_is_subtype, done;\n+  Label is_null, is_flat_array, ok_is_subtype, done;\n@@ -1117,2 +1133,4 @@\n-  Address element_address(r3, r4, Address::uxtw(LogBytesPerHeapOop));\n-\n+\n+  __ profile_array_type<ArrayStoreData>(r4, r3, r5);\n+  __ profile_multiple_element_types(r4, r0, r5, r6);\n+\n@@ -1121,0 +1139,2 @@\n+  Address element_address(r3, r4, Address::uxtw(LogBytesPerHeapOop));\n+  \/\/ Be careful not to clobber r4 below\n@@ -1125,0 +1145,8 @@\n+  \/\/ Move array class to r5\n+  __ load_klass(r5, r3);\n+\n+  if (UseArrayFlattening) {\n+    __ ldrw(r6, Address(r5, Klass::layout_helper_offset()));\n+    __ test_flat_array_layout(r6, is_flat_array);\n+  }\n+\n@@ -1127,4 +1155,3 @@\n-  \/\/ Move superklass into r0\n-  __ load_klass(r0, r3);\n-  __ ldr(r0, Address(r0,\n-                     ObjArrayKlass::element_klass_offset()));\n+\n+  \/\/ Move array element superklass into r0\n+  __ ldr(r0, Address(r5, ObjArrayKlass::element_klass_offset()));\n@@ -1135,1 +1162,3 @@\n-  __ gen_subtype_check(r1, ok_is_subtype);\n+\n+  \/\/ is \"r1 <: r0\" ? (value subclass <: array element superclass)\n+  __ gen_subtype_check(r1, ok_is_subtype, false);\n@@ -1153,1 +1182,16 @@\n-  __ profile_null_seen(r2);\n+  if (EnableValhalla) {\n+    Label is_null_into_value_array_npe, store_null;\n+\n+    if (UseArrayFlattening) {\n+      __ test_flat_array_oop(r3, rscratch1, is_flat_array);\n+    }\n+\n+    \/\/ No way to store null in a null-free array\n+    __ test_null_free_array_oop(r3, rscratch1, is_null_into_value_array_npe);\n+    __ b(store_null);\n+\n+    __ bind(is_null_into_value_array_npe);\n+    __ b(ExternalAddress(Interpreter::_throw_NullPointerException_entry));\n+\n+    __ bind(store_null);\n+  }\n@@ -1158,0 +1202,11 @@\n+  __ b(done);\n+\n+  if (UseArrayFlattening) {\n+     Label is_type_ok;\n+    __ bind(is_flat_array); \/\/ Store non-null value to flat\n+\n+    __ ldr(r0, at_tos());    \/\/ value\n+    __ ldr(r3, at_tos_p1()); \/\/ index\n+    __ ldr(r2, at_tos_p2()); \/\/ array\n+    __ call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::flat_array_store), r0, r2, r3);\n+  }\n@@ -1964,2 +2019,1 @@\n-void TemplateTable::if_acmp(Condition cc)\n-{\n+void TemplateTable::if_acmp(Condition cc) {\n@@ -1968,1 +2022,1 @@\n-  Label not_taken;\n+  Label taken, not_taken;\n@@ -1970,0 +2024,38 @@\n+\n+  __ profile_acmp(r2, r1, r0, r4);\n+\n+  Register is_inline_type_mask = rscratch1;\n+  __ mov(is_inline_type_mask, markWord::inline_type_pattern);\n+\n+  if (EnableValhalla) {\n+    __ cmp(r1, r0);\n+    __ br(Assembler::EQ, (cc == equal) ? taken : not_taken);\n+\n+    \/\/ might be substitutable, test if either r0 or r1 is null\n+    __ andr(r2, r0, r1);\n+    __ cbz(r2, (cc == equal) ? not_taken : taken);\n+\n+    \/\/ and both are values ?\n+    __ ldr(r2, Address(r1, oopDesc::mark_offset_in_bytes()));\n+    __ andr(r2, r2, is_inline_type_mask);\n+    __ ldr(r4, Address(r0, oopDesc::mark_offset_in_bytes()));\n+    __ andr(r4, r4, is_inline_type_mask);\n+    __ andr(r2, r2, r4);\n+    __ cmp(r2,  is_inline_type_mask);\n+    __ br(Assembler::NE, (cc == equal) ? not_taken : taken);\n+\n+    \/\/ same value klass ?\n+    __ load_metadata(r2, r1);\n+    __ load_metadata(r4, r0);\n+    __ cmp(r2, r4);\n+    __ br(Assembler::NE, (cc == equal) ? not_taken : taken);\n+\n+    \/\/ Know both are the same type, let's test for substitutability...\n+    if (cc == equal) {\n+      invoke_is_substitutable(r0, r1, taken, not_taken);\n+    } else {\n+      invoke_is_substitutable(r0, r1, not_taken, taken);\n+    }\n+    __ stop(\"Not reachable\");\n+  }\n+\n@@ -1972,0 +2064,1 @@\n+  __ bind(taken);\n@@ -1974,1 +2067,1 @@\n-  __ profile_not_taken_branch(r0);\n+  __ profile_not_taken_branch(r0, true);\n@@ -1977,0 +2070,10 @@\n+void TemplateTable::invoke_is_substitutable(Register aobj, Register bobj,\n+                                            Label& is_subst, Label& not_subst) {\n+\n+  __ call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::is_substitutable), aobj, bobj);\n+  \/\/ Restored... r0 answer, jmp to outcome...\n+  __ cbz(r0, not_subst);\n+  __ b(is_subst);\n+}\n+\n+\n@@ -2586,1 +2689,1 @@\n-  const Register cache     = r4;\n+  const Register cache     = r2;\n@@ -2588,0 +2691,3 @@\n+  const Register klass     = r5;\n+  const Register inline_klass = r7;\n+  const Register field_index = r23;\n@@ -2596,0 +2702,5 @@\n+\n+  \/\/ Valhalla extras\n+  __ load_unsigned_short(field_index, Address(cache, in_bytes(ResolvedFieldEntry::field_index_offset())));\n+  __ ldr(klass, Address(cache, ResolvedFieldEntry::field_holder_offset()));\n+\n@@ -2654,4 +2765,61 @@\n-  do_oop_load(_masm, field, r0, IN_HEAP);\n-  __ push(atos);\n-  if (rc == may_rewrite) {\n-    patch_bytecode(Bytecodes::_fast_agetfield, bc, r1);\n+  if (!EnableValhalla) {\n+    do_oop_load(_masm, field, r0, IN_HEAP);\n+    __ push(atos);\n+    if (rc == may_rewrite) {\n+      patch_bytecode(Bytecodes::_fast_agetfield, bc, r1);\n+    }\n+    __ b(Done);\n+  } else { \/\/ Valhalla\n+    if (is_static) {\n+      __ load_heap_oop(r0, field, rscratch1, rscratch2);\n+      Label is_null_free_inline_type, uninitialized;\n+      \/\/ Issue below if the static field has not been initialized yet\n+      __ test_field_is_null_free_inline_type(flags, noreg \/*temp*\/, is_null_free_inline_type);\n+        \/\/ field is not a null free inline type\n+        __ push(atos);\n+        __ b(Done);\n+      \/\/ field is a null free inline type, must not return null even if uninitialized\n+      __ bind(is_null_free_inline_type);\n+        __ cbz(r0, uninitialized);\n+          __ push(atos);\n+          __ b(Done);\n+        __ bind(uninitialized);\n+          __ b(ExternalAddress(Interpreter::_throw_NPE_UninitializedField_entry));\n+    } else {\n+      Label is_flat, nonnull, is_inline_type, has_null_marker, rewrite_inline;\n+      __ test_field_is_null_free_inline_type(flags, noreg \/*temp*\/, is_inline_type);\n+      __ test_field_has_null_marker(flags, noreg \/*temp*\/, has_null_marker);\n+        \/\/ Non-inline field case\n+        __ load_heap_oop(r0, field, rscratch1, rscratch2);\n+        __ push(atos);\n+        if (rc == may_rewrite) {\n+          patch_bytecode(Bytecodes::_fast_agetfield, bc, r1);\n+        }\n+        __ b(Done);\n+      __ bind(is_inline_type);\n+        __ test_field_is_flat(flags, noreg \/* temp *\/, is_flat);\n+         \/\/ field is not flat\n+          __ load_heap_oop(r0, field, rscratch1, rscratch2);\n+          __ cbnz(r0, nonnull);\n+            __ b(ExternalAddress(Interpreter::_throw_NPE_UninitializedField_entry));\n+          __ bind(nonnull);\n+          __ verify_oop(r0);\n+          __ push(atos);\n+          __ b(rewrite_inline);\n+        __ bind(is_flat);\n+        \/\/ field is flat\n+          __ mov(r0, obj);\n+          __ read_flat_field(cache, field_index, off, inline_klass \/* temp *\/, r0);\n+          __ verify_oop(r0);\n+          __ push(atos);\n+          __ b(rewrite_inline);\n+        __ bind(has_null_marker);\n+          call_VM(r0, CAST_FROM_FN_PTR(address, InterpreterRuntime::read_nullable_flat_field), obj, cache);\n+          __ verify_oop(r0);\n+          __ push(atos);\n+      __ bind(rewrite_inline);\n+      if (rc == may_rewrite) {\n+        patch_bytecode(Bytecodes::_fast_vgetfield, bc, r1);\n+      }\n+      __ b(Done);\n+    }\n@@ -2659,1 +2827,0 @@\n-  __ b(Done);\n@@ -2820,1 +2987,1 @@\n-  const Register flags     = r0;\n+  const Register flags     = r6;\n@@ -2822,0 +2989,1 @@\n+  const Register inline_klass = r5;\n@@ -2828,2 +2996,0 @@\n-  __ mov(r5, flags);\n-\n@@ -2832,1 +2998,1 @@\n-    __ tbz(r5, ResolvedFieldEntry::is_volatile_shift, notVolatile);\n+    __ tbz(flags, ResolvedFieldEntry::is_volatile_shift, notVolatile);\n@@ -2881,9 +3047,66 @@\n-    __ pop(atos);\n-    if (!is_static) pop_and_check_object(obj);\n-    \/\/ Store into the field\n-    \/\/ Clobbers: r10, r11, r3\n-    do_oop_store(_masm, field, r0, IN_HEAP);\n-    if (rc == may_rewrite) {\n-      patch_bytecode(Bytecodes::_fast_aputfield, bc, r1, true, byte_no);\n-    }\n-    __ b(Done);\n+     if (!EnableValhalla) {\n+      __ pop(atos);\n+      if (!is_static) pop_and_check_object(obj);\n+      \/\/ Store into the field\n+      \/\/ Clobbers: r10, r11, r3\n+      do_oop_store(_masm, field, r0, IN_HEAP);\n+      if (rc == may_rewrite) {\n+        patch_bytecode(Bytecodes::_fast_aputfield, bc, r1, true, byte_no);\n+      }\n+      __ b(Done);\n+     } else { \/\/ Valhalla\n+      __ pop(atos);\n+      if (is_static) {\n+        Label is_inline_type;\n+         __ test_field_is_not_null_free_inline_type(flags, noreg \/* temp *\/, is_inline_type);\n+         __ null_check(r0);\n+         __ bind(is_inline_type);\n+         do_oop_store(_masm, field, r0, IN_HEAP);\n+         __ b(Done);\n+      } else {\n+        Label is_inline_type, is_flat, has_null_marker, rewrite_not_inline, rewrite_inline;\n+        __ test_field_is_null_free_inline_type(flags, noreg \/*temp*\/, is_inline_type);\n+        __ test_field_has_null_marker(flags, noreg \/*temp*\/, has_null_marker);\n+        \/\/ Not an inline type\n+        pop_and_check_object(obj);\n+        \/\/ Store into the field\n+        \/\/ Clobbers: r10, r11, r3\n+        do_oop_store(_masm, field, r0, IN_HEAP);\n+        __ bind(rewrite_not_inline);\n+        if (rc == may_rewrite) {\n+          patch_bytecode(Bytecodes::_fast_aputfield, bc, r19, true, byte_no);\n+        }\n+        __ b(Done);\n+        \/\/ Implementation of the inline type semantic\n+        __ bind(is_inline_type);\n+        __ null_check(r0);\n+        __ test_field_is_flat(flags, noreg \/*temp*\/, is_flat);\n+        \/\/ field is not flat\n+        pop_and_check_object(obj);\n+        \/\/ Store into the field\n+        \/\/ Clobbers: r10, r11, r3\n+        do_oop_store(_masm, field, r0, IN_HEAP);\n+        __ b(rewrite_inline);\n+        __ bind(is_flat);\n+        __ load_field_entry(cache, index); \/\/ reload field entry (cache) because it was erased by tos_state\n+        __ load_unsigned_short(index, Address(cache, in_bytes(ResolvedFieldEntry::field_index_offset())));\n+        __ ldr(r2, Address(cache, in_bytes(ResolvedFieldEntry::field_holder_offset())));\n+        __ inline_layout_info(r2, index, r6);\n+        pop_and_check_object(obj);\n+        __ load_klass(inline_klass, r0);\n+        __ payload_address(r0, r0, inline_klass);\n+        __ add(obj, obj, off);\n+        \/\/ because we use InlineLayoutInfo, we need special value access code specialized for fields (arrays will need a different API)\n+        __ flat_field_copy(IN_HEAP, r0, obj, r6);\n+        __ b(rewrite_inline);\n+        __ bind(has_null_marker);\n+        assert_different_registers(r0, cache, r19);\n+        pop_and_check_object(r19);\n+        __ call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::write_nullable_flat_field), r19, r0, cache);\n+        __ bind(rewrite_inline);\n+        if (rc == may_rewrite) {\n+          patch_bytecode(Bytecodes::_fast_vputfield, bc, r19, true, byte_no);\n+        }\n+        __ b(Done);\n+      }\n+     }  \/\/ Valhalla\n@@ -2994,1 +3217,1 @@\n-    __ tbz(r5, ResolvedFieldEntry::is_volatile_shift, notVolatile);\n+    __ tbz(flags, ResolvedFieldEntry::is_volatile_shift, notVolatile);\n@@ -3028,0 +3251,1 @@\n+    case Bytecodes::_fast_vputfield: \/\/fall through\n@@ -3054,0 +3278,1 @@\n+    case Bytecodes::_fast_vputfield: \/\/fall through\n@@ -3100,0 +3325,27 @@\n+  case Bytecodes::_fast_vputfield:\n+   {\n+      Label is_flat, has_null_marker, done;\n+      __ test_field_has_null_marker(r5, noreg \/* temp *\/, has_null_marker);\n+      __ null_check(r0);\n+      __ test_field_is_flat(r5, noreg \/* temp *\/, is_flat);\n+      \/\/ field is not flat\n+      do_oop_store(_masm, field, r0, IN_HEAP);\n+      __ b(done);\n+      __ bind(is_flat);\n+      \/\/ field is flat\n+      __ load_field_entry(r4, r5);\n+      __ load_unsigned_short(r5, Address(r4, in_bytes(ResolvedFieldEntry::field_index_offset())));\n+      __ ldr(r4, Address(r4, in_bytes(ResolvedFieldEntry::field_holder_offset())));\n+      __ inline_layout_info(r4, r5, r6);\n+      __ load_klass(r4, r0);\n+      __ payload_address(r0, r0, r4);\n+      __ lea(rscratch1, field);\n+      __ flat_field_copy(IN_HEAP, r0, rscratch1, r6);\n+      __ b(done);\n+      __ bind(has_null_marker);\n+      __ load_field_entry(r4, r1);\n+      __ mov(r1, r2);\n+      __ call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::write_nullable_flat_field), r1, r0, r4);\n+      __ bind(done);\n+    }\n+    break;\n@@ -3193,0 +3445,25 @@\n+  case Bytecodes::_fast_vgetfield:\n+    {\n+      Register index = r4, klass = r5, inline_klass = r6, tmp = r7;\n+      Label is_flat, has_null_marker, nonnull, Done;\n+      __ test_field_has_null_marker(r3, noreg \/*temp*\/, has_null_marker);\n+      __ test_field_is_flat(r3, noreg \/* temp *\/, is_flat);\n+        \/\/ field is not flat\n+        __ load_heap_oop(r0, field, rscratch1, rscratch2);\n+        __ cbnz(r0, nonnull);\n+          __ b(ExternalAddress(Interpreter::_throw_NPE_UninitializedField_entry));\n+        __ bind(nonnull);\n+        __ verify_oop(r0);\n+        __ b(Done);\n+      __ bind(is_flat);\n+      \/\/ field is flat\n+        __ load_unsigned_short(index, Address(r2, in_bytes(ResolvedFieldEntry::field_index_offset())));\n+        __ read_flat_field(r2, index, r1, tmp \/* temp *\/, r0);\n+        __ verify_oop(r0);\n+        __ b(Done);\n+      __ bind(has_null_marker);\n+        call_VM(r0, CAST_FROM_FN_PTR(address, InterpreterRuntime::read_nullable_flat_field), r0, r2);\n+        __ verify_oop(r0);\n+      __ bind(Done);\n+    }\n+    break;\n@@ -3611,63 +3888,2 @@\n-  \/\/ get instance_size in InstanceKlass (scaled to a count of bytes)\n-  __ ldrw(r3,\n-          Address(r4,\n-                  Klass::layout_helper_offset()));\n-  \/\/ test to see if it is malformed in some way\n-  __ tbnz(r3, exact_log2(Klass::_lh_instance_slow_path_bit), slow_case);\n-\n-  \/\/ Allocate the instance:\n-  \/\/  If TLAB is enabled:\n-  \/\/    Try to allocate in the TLAB.\n-  \/\/    If fails, go to the slow path.\n-  \/\/    Initialize the allocation.\n-  \/\/    Exit.\n-  \/\/\n-  \/\/  Go to slow path.\n-\n-  if (UseTLAB) {\n-    __ tlab_allocate(r0, r3, 0, noreg, r1, slow_case);\n-\n-    if (ZeroTLAB) {\n-      \/\/ the fields have been already cleared\n-      __ b(initialize_header);\n-    }\n-\n-    \/\/ The object is initialized before the header.  If the object size is\n-    \/\/ zero, go directly to the header initialization.\n-    int header_size = oopDesc::header_size() * HeapWordSize;\n-    assert(is_aligned(header_size, BytesPerLong), \"oop header size must be 8-byte-aligned\");\n-    __ sub(r3, r3, header_size);\n-    __ cbz(r3, initialize_header);\n-\n-    \/\/ Initialize object fields\n-    {\n-      __ add(r2, r0, header_size);\n-      Label loop;\n-      __ bind(loop);\n-      __ str(zr, Address(__ post(r2, BytesPerLong)));\n-      __ sub(r3, r3, BytesPerLong);\n-      __ cbnz(r3, loop);\n-    }\n-\n-    \/\/ initialize object header only.\n-    __ bind(initialize_header);\n-    if (UseCompactObjectHeaders) {\n-      __ ldr(rscratch1, Address(r4, Klass::prototype_header_offset()));\n-      __ str(rscratch1, Address(r0, oopDesc::mark_offset_in_bytes()));\n-    } else {\n-      __ mov(rscratch1, (intptr_t)markWord::prototype().value());\n-      __ str(rscratch1, Address(r0, oopDesc::mark_offset_in_bytes()));\n-      __ store_klass_gap(r0, zr);  \/\/ zero klass gap for compressed oops\n-      __ store_klass(r0, r4);      \/\/ store klass last\n-    }\n-\n-    if (DTraceAllocProbes) {\n-      \/\/ Trigger dtrace event for fastpath\n-      __ push(atos); \/\/ save the return value\n-      __ call_VM_leaf(\n-           CAST_FROM_FN_PTR(address, static_cast<int (*)(oopDesc*)>(SharedRuntime::dtrace_object_alloc)), r0);\n-      __ pop(atos); \/\/ restore the return value\n-\n-    }\n-    __ b(done);\n-  }\n+  __ allocate_instance(r4, r0, r3, r1, true, slow_case);\n+  __ b(done);\n@@ -3757,0 +3973,3 @@\n+  __ b(done);\n+  __ bind(is_null);\n+\n@@ -3759,4 +3978,1 @@\n-    __ b(done);\n-    __ bind(is_null);\n-  } else {\n-    __ bind(is_null);   \/\/ same as 'done'\n+\n@@ -3882,0 +4098,4 @@\n+  Label is_inline_type;\n+  __ ldr(rscratch1, Address(r0, oopDesc::mark_offset_in_bytes()));\n+  __ test_markword_is_inline_type(rscratch1, is_inline_type);\n+\n@@ -3983,0 +4203,5 @@\n+\n+  __ bind(is_inline_type);\n+  __ call_VM(noreg, CAST_FROM_FN_PTR(address,\n+                    InterpreterRuntime::throw_identity_exception), r0);\n+  __ should_not_reach_here();\n@@ -3993,0 +4218,12 @@\n+  const int is_inline_type_mask = markWord::inline_type_pattern;\n+  Label has_identity;\n+  __ ldr(rscratch1, Address(r0, oopDesc::mark_offset_in_bytes()));\n+  __ mov(rscratch2, is_inline_type_mask);\n+  __ andr(rscratch1, rscratch1, rscratch2);\n+  __ cmp(rscratch1, rscratch2);\n+  __ br(Assembler::NE, has_identity);\n+  __ call_VM(noreg, CAST_FROM_FN_PTR(address,\n+                     InterpreterRuntime::throw_illegal_monitor_state_exception));\n+  __ should_not_reach_here();\n+  __ bind(has_identity);\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/templateTable_aarch64.cpp","additions":346,"deletions":109,"binary":false,"changes":455,"status":"modified"},{"patch":"@@ -37,0 +37,1 @@\n+#include \"runtime\/frame.inline.hpp\"\n@@ -73,0 +74,4 @@\n+    if (EnableValhalla) {\n+      \/\/ Mask inline_type bit such that we go to the slow path if object is an inline type\n+      andptr(hdr, ~((int) markWord::inline_type_bit_in_place));\n+    }\n@@ -160,1 +165,1 @@\n-  if (UseCompactObjectHeaders) {\n+  if (UseCompactObjectHeaders || EnableValhalla) {\n@@ -163,1 +168,1 @@\n-  } else if (UseCompressedClassPointers) { \/\/ Take care not to kill klass\n+  } else {\n@@ -165,0 +170,2 @@\n+  }\n+  if (UseCompressedClassPointers) { \/\/ Take care not to kill klass\n@@ -168,2 +175,1 @@\n-  } else {\n-    movptr(Address(obj, oopDesc::mark_offset_in_bytes()), checked_cast<int32_t>(markWord::prototype().value()));\n+  } else if (!UseCompactObjectHeaders) {\n@@ -300,2 +306,20 @@\n-void C1_MacroAssembler::build_frame(int frame_size_in_bytes, int bang_size_in_bytes) {\n-  assert(bang_size_in_bytes >= frame_size_in_bytes, \"stack bang size incorrect\");\n+void C1_MacroAssembler::build_frame_helper(int frame_size_in_bytes, int sp_offset_for_orig_pc, int sp_inc, bool reset_orig_pc, bool needs_stack_repair) {\n+  push(rbp);\n+  if (PreserveFramePointer) {\n+    mov(rbp, rsp);\n+  }\n+  decrement(rsp, frame_size_in_bytes);\n+\n+  if (needs_stack_repair) {\n+    \/\/ Save stack increment (also account for fixed framesize and rbp)\n+    assert((sp_inc & (StackAlignmentInBytes-1)) == 0, \"stack increment not aligned\");\n+    int real_frame_size = sp_inc + frame_size_in_bytes + wordSize;\n+    movptr(Address(rsp, frame_size_in_bytes - wordSize), real_frame_size);\n+  }\n+  if (reset_orig_pc) {\n+    \/\/ Zero orig_pc to detect deoptimization during buffering in the entry points\n+    movptr(Address(rsp, sp_offset_for_orig_pc), 0);\n+  }\n+}\n+\n+void C1_MacroAssembler::build_frame(int frame_size_in_bytes, int bang_size_in_bytes, int sp_offset_for_orig_pc, bool needs_stack_repair, bool has_scalarized_args, Label* verified_inline_entry_label) {\n@@ -307,0 +331,1 @@\n+  assert(bang_size_in_bytes >= frame_size_in_bytes, \"stack bang size incorrect\");\n@@ -309,5 +334,1 @@\n-  push(rbp);\n-  if (PreserveFramePointer) {\n-    mov(rbp, rsp);\n-  }\n-  decrement(rsp, frame_size_in_bytes); \/\/ does not emit code for frame_size == 0\n+  build_frame_helper(frame_size_in_bytes, sp_offset_for_orig_pc, 0, has_scalarized_args, needs_stack_repair);\n@@ -318,5 +339,4 @@\n-}\n-\n-void C1_MacroAssembler::remove_frame(int frame_size_in_bytes) {\n-  increment(rsp, frame_size_in_bytes);  \/\/ Does not emit code for frame_size == 0\n-  pop(rbp);\n+  if (verified_inline_entry_label != nullptr) {\n+    \/\/ Jump here from the scalarized entry points that already created the frame.\n+    bind(*verified_inline_entry_label);\n+  }\n@@ -326,1 +346,0 @@\n-\n@@ -332,0 +351,58 @@\n+int C1_MacroAssembler::scalarized_entry(const CompiledEntrySignature* ces, int frame_size_in_bytes, int bang_size_in_bytes, int sp_offset_for_orig_pc, Label& verified_inline_entry_label, bool is_inline_ro_entry) {\n+  assert(InlineTypePassFieldsAsArgs, \"sanity\");\n+  \/\/ Make sure there is enough stack space for this method's activation.\n+  assert(bang_size_in_bytes >= frame_size_in_bytes, \"stack bang size incorrect\");\n+  generate_stack_overflow_check(bang_size_in_bytes);\n+\n+  GrowableArray<SigEntry>* sig    = ces->sig();\n+  GrowableArray<SigEntry>* sig_cc = is_inline_ro_entry ? ces->sig_cc_ro() : ces->sig_cc();\n+  VMRegPair* regs      = ces->regs();\n+  VMRegPair* regs_cc   = is_inline_ro_entry ? ces->regs_cc_ro() : ces->regs_cc();\n+  int args_on_stack    = ces->args_on_stack();\n+  int args_on_stack_cc = is_inline_ro_entry ? ces->args_on_stack_cc_ro() : ces->args_on_stack_cc();\n+\n+  assert(sig->length() <= sig_cc->length(), \"Zero-sized inline class not allowed!\");\n+  BasicType* sig_bt = NEW_RESOURCE_ARRAY(BasicType, sig_cc->length());\n+  int args_passed = sig->length();\n+  int args_passed_cc = SigEntry::fill_sig_bt(sig_cc, sig_bt);\n+\n+  \/\/ Create a temp frame so we can call into the runtime. It must be properly set up to accommodate GC.\n+  build_frame_helper(frame_size_in_bytes, sp_offset_for_orig_pc, 0, true, ces->c1_needs_stack_repair());\n+\n+  \/\/ The runtime call might safepoint, make sure nmethod entry barrier is executed\n+  BarrierSetAssembler* bs = BarrierSet::barrier_set()->barrier_set_assembler();\n+  \/\/ C1 code is not hot enough to micro optimize the nmethod entry barrier with an out-of-line stub\n+  bs->nmethod_entry_barrier(this, nullptr \/* slow_path *\/, nullptr \/* continuation *\/);\n+\n+  \/\/ FIXME -- call runtime only if we cannot in-line allocate all the incoming inline type args.\n+  movptr(rbx, (intptr_t)(ces->method()));\n+  if (is_inline_ro_entry) {\n+    call(RuntimeAddress(Runtime1::entry_for(C1StubId::buffer_inline_args_no_receiver_id)));\n+  } else {\n+    call(RuntimeAddress(Runtime1::entry_for(C1StubId::buffer_inline_args_id)));\n+  }\n+  int rt_call_offset = offset();\n+\n+  \/\/ Remove the temp frame\n+  addptr(rsp, frame_size_in_bytes);\n+  pop(rbp);\n+\n+  \/\/ Check if we need to extend the stack for packing\n+  int sp_inc = 0;\n+  if (args_on_stack > args_on_stack_cc) {\n+    sp_inc = extend_stack_for_inline_args(args_on_stack);\n+  }\n+\n+  shuffle_inline_args(true, is_inline_ro_entry, sig_cc,\n+                      args_passed_cc, args_on_stack_cc, regs_cc, \/\/ from\n+                      args_passed, args_on_stack, regs,          \/\/ to\n+                      sp_inc, rax);\n+\n+  \/\/ Create the real frame. Below jump will then skip over the stack banging and frame\n+  \/\/ setup code in the verified_inline_entry (which has a different real_frame_size).\n+  build_frame_helper(frame_size_in_bytes, sp_offset_for_orig_pc, sp_inc, false, ces->c1_needs_stack_repair());\n+\n+  jmp(verified_inline_entry_label);\n+  return rt_call_offset;\n+}\n+\n","filename":"src\/hotspot\/cpu\/x86\/c1_MacroAssembler_x86.cpp","additions":94,"deletions":17,"binary":false,"changes":111,"status":"modified"},{"patch":"@@ -52,1 +52,21 @@\n-void C2_MacroAssembler::verified_entry(int framesize, int stack_bang_size, bool fp_mode_24b, bool is_stub) {\n+void C2_MacroAssembler::verified_entry(Compile* C, int sp_inc) {\n+  if (C->clinit_barrier_on_entry()) {\n+    assert(VM_Version::supports_fast_class_init_checks(), \"sanity\");\n+    assert(!C->method()->holder()->is_not_initialized(), \"initialization should have been started\");\n+\n+    Label L_skip_barrier;\n+    Register klass = rscratch1;\n+\n+    mov_metadata(klass, C->method()->holder()->constant_encoding());\n+    clinit_barrier(klass, &L_skip_barrier \/*L_fast_path*\/);\n+\n+    jump(RuntimeAddress(SharedRuntime::get_handle_wrong_method_stub())); \/\/ slow path\n+\n+    bind(L_skip_barrier);\n+  }\n+\n+  int framesize = C->output()->frame_size_in_bytes();\n+  int bangsize = C->output()->bang_size_in_bytes();\n+  bool fp_mode_24b = false;\n+  int stack_bang_size = C->output()->need_stack_bang(bangsize) ? bangsize : 0;\n+\n@@ -97,0 +117,6 @@\n+  if (C->needs_stack_repair()) {\n+    \/\/ Save stack increment just below the saved rbp (also account for fixed framesize and rbp)\n+    assert((sp_inc & (StackAlignmentInBytes-1)) == 0, \"stack increment not aligned\");\n+    movptr(Address(rsp, framesize - wordSize), sp_inc + framesize + wordSize);\n+  }\n+\n@@ -115,0 +141,1 @@\n+}\n@@ -116,15 +143,13 @@\n-  if (!is_stub) {\n-    BarrierSetAssembler* bs = BarrierSet::barrier_set()->barrier_set_assembler();\n-    \/\/ We put the non-hot code of the nmethod entry barrier out-of-line in a stub.\n-    Label dummy_slow_path;\n-    Label dummy_continuation;\n-    Label* slow_path = &dummy_slow_path;\n-    Label* continuation = &dummy_continuation;\n-    if (!Compile::current()->output()->in_scratch_emit_size()) {\n-      \/\/ Use real labels from actual stub when not emitting code for the purpose of measuring its size\n-      C2EntryBarrierStub* stub = new (Compile::current()->comp_arena()) C2EntryBarrierStub();\n-      Compile::current()->output()->add_stub(stub);\n-      slow_path = &stub->entry();\n-      continuation = &stub->continuation();\n-    }\n-    bs->nmethod_entry_barrier(this, slow_path, continuation);\n+void C2_MacroAssembler::entry_barrier() {\n+  BarrierSetAssembler* bs = BarrierSet::barrier_set()->barrier_set_assembler();\n+  \/\/ We put the non-hot code of the nmethod entry barrier out-of-line in a stub.\n+  Label dummy_slow_path;\n+  Label dummy_continuation;\n+  Label* slow_path = &dummy_slow_path;\n+  Label* continuation = &dummy_continuation;\n+  if (!Compile::current()->output()->in_scratch_emit_size()) {\n+    \/\/ Use real labels from actual stub when not emitting code for the purpose of measuring its size\n+    C2EntryBarrierStub* stub = new (Compile::current()->comp_arena()) C2EntryBarrierStub();\n+    Compile::current()->output()->add_stub(stub);\n+    slow_path = &stub->entry();\n+    continuation = &stub->continuation();\n@@ -132,0 +157,1 @@\n+  bs->nmethod_entry_barrier(this, slow_path, continuation);\n@@ -269,0 +295,4 @@\n+    if (EnableValhalla) {\n+      \/\/ Mask inline_type bit such that we go to the slow path if object is an inline type\n+      andptr(tmpReg, ~((int) markWord::inline_type_bit_in_place));\n+    }\n","filename":"src\/hotspot\/cpu\/x86\/c2_MacroAssembler_x86.cpp","additions":46,"deletions":16,"binary":false,"changes":62,"status":"modified"},{"patch":"@@ -31,0 +31,1 @@\n+#include \"ci\/ciInlineKlass.hpp\"\n@@ -46,0 +47,1 @@\n+#include \"oops\/resolvedFieldEntry.hpp\"\n@@ -56,0 +58,1 @@\n+#include \"runtime\/signature_cc.hpp\"\n@@ -59,0 +62,4 @@\n+#include \"vmreg_x86.inline.hpp\"\n+#ifdef COMPILER2\n+#include \"opto\/output.hpp\"\n+#endif\n@@ -1290,0 +1297,4 @@\n+void MacroAssembler::super_call_VM_leaf(address entry_point) {\n+  MacroAssembler::call_VM_leaf_base(entry_point, 1);\n+}\n+\n@@ -2343,0 +2354,107 @@\n+void MacroAssembler::test_markword_is_inline_type(Register markword, Label& is_inline_type) {\n+  andptr(markword, markWord::inline_type_mask_in_place);\n+  cmpptr(markword, markWord::inline_type_pattern);\n+  jcc(Assembler::equal, is_inline_type);\n+}\n+\n+void MacroAssembler::test_oop_is_not_inline_type(Register object, Register tmp, Label& not_inline_type, bool can_be_null) {\n+  if (can_be_null) {\n+    testptr(object, object);\n+    jcc(Assembler::zero, not_inline_type);\n+  }\n+  const int is_inline_type_mask = markWord::inline_type_pattern;\n+  movptr(tmp, Address(object, oopDesc::mark_offset_in_bytes()));\n+  andptr(tmp, is_inline_type_mask);\n+  cmpptr(tmp, is_inline_type_mask);\n+  jcc(Assembler::notEqual, not_inline_type);\n+}\n+\n+void MacroAssembler::test_field_is_null_free_inline_type(Register flags, Register temp_reg, Label& is_null_free_inline_type) {\n+  movl(temp_reg, flags);\n+  testl(temp_reg, 1 << ResolvedFieldEntry::is_null_free_inline_type_shift);\n+  jcc(Assembler::notEqual, is_null_free_inline_type);\n+}\n+\n+void MacroAssembler::test_field_is_not_null_free_inline_type(Register flags, Register temp_reg, Label& not_null_free_inline_type) {\n+  movl(temp_reg, flags);\n+  testl(temp_reg, 1 << ResolvedFieldEntry::is_null_free_inline_type_shift);\n+  jcc(Assembler::equal, not_null_free_inline_type);\n+}\n+\n+void MacroAssembler::test_field_is_flat(Register flags, Register temp_reg, Label& is_flat) {\n+  movl(temp_reg, flags);\n+  testl(temp_reg, 1 << ResolvedFieldEntry::is_flat_shift);\n+  jcc(Assembler::notEqual, is_flat);\n+}\n+\n+void MacroAssembler::test_field_has_null_marker(Register flags, Register temp_reg, Label& has_null_marker) {\n+  movl(temp_reg, flags);\n+  testl(temp_reg, 1 << ResolvedFieldEntry::has_null_marker_shift);\n+  jcc(Assembler::notEqual, has_null_marker);\n+}\n+\n+void MacroAssembler::test_oop_prototype_bit(Register oop, Register temp_reg, int32_t test_bit, bool jmp_set, Label& jmp_label) {\n+  Label test_mark_word;\n+  \/\/ load mark word\n+  movptr(temp_reg, Address(oop, oopDesc::mark_offset_in_bytes()));\n+  \/\/ check displaced\n+  testl(temp_reg, markWord::unlocked_value);\n+  jccb(Assembler::notZero, test_mark_word);\n+  \/\/ slow path use klass prototype\n+  push(rscratch1);\n+  load_prototype_header(temp_reg, oop, rscratch1);\n+  pop(rscratch1);\n+\n+  bind(test_mark_word);\n+  testl(temp_reg, test_bit);\n+  jcc((jmp_set) ? Assembler::notZero : Assembler::zero, jmp_label);\n+}\n+\n+void MacroAssembler::test_flat_array_oop(Register oop, Register temp_reg,\n+                                         Label& is_flat_array) {\n+#ifdef _LP64\n+  test_oop_prototype_bit(oop, temp_reg, markWord::flat_array_bit_in_place, true, is_flat_array);\n+#else\n+  load_klass(temp_reg, oop, noreg);\n+  movl(temp_reg, Address(temp_reg, Klass::layout_helper_offset()));\n+  test_flat_array_layout(temp_reg, is_flat_array);\n+#endif\n+}\n+\n+void MacroAssembler::test_non_flat_array_oop(Register oop, Register temp_reg,\n+                                             Label& is_non_flat_array) {\n+#ifdef _LP64\n+  test_oop_prototype_bit(oop, temp_reg, markWord::flat_array_bit_in_place, false, is_non_flat_array);\n+#else\n+  load_klass(temp_reg, oop, noreg);\n+  movl(temp_reg, Address(temp_reg, Klass::layout_helper_offset()));\n+  test_non_flat_array_layout(temp_reg, is_non_flat_array);\n+#endif\n+}\n+\n+void MacroAssembler::test_null_free_array_oop(Register oop, Register temp_reg, Label&is_null_free_array) {\n+#ifdef _LP64\n+  test_oop_prototype_bit(oop, temp_reg, markWord::null_free_array_bit_in_place, true, is_null_free_array);\n+#else\n+  Unimplemented();\n+#endif\n+}\n+\n+void MacroAssembler::test_non_null_free_array_oop(Register oop, Register temp_reg, Label&is_non_null_free_array) {\n+#ifdef _LP64\n+  test_oop_prototype_bit(oop, temp_reg, markWord::null_free_array_bit_in_place, false, is_non_null_free_array);\n+#else\n+  Unimplemented();\n+#endif\n+}\n+\n+void MacroAssembler::test_flat_array_layout(Register lh, Label& is_flat_array) {\n+  testl(lh, Klass::_lh_array_tag_flat_value_bit_inplace);\n+  jcc(Assembler::notZero, is_flat_array);\n+}\n+\n+void MacroAssembler::test_non_flat_array_layout(Register lh, Label& is_non_flat_array) {\n+  testl(lh, Klass::_lh_array_tag_flat_value_bit_inplace);\n+  jcc(Assembler::zero, is_non_flat_array);\n+}\n+\n@@ -3431,0 +3549,118 @@\n+\/\/ Object \/ value buffer allocation...\n+\/\/\n+\/\/ Kills klass and rsi on LP64\n+void MacroAssembler::allocate_instance(Register klass, Register new_obj,\n+                                       Register t1, Register t2,\n+                                       bool clear_fields, Label& alloc_failed)\n+{\n+  Label done, initialize_header, initialize_object, slow_case, slow_case_no_pop;\n+  Register layout_size = t1;\n+  assert(new_obj == rax, \"needs to be rax\");\n+  assert_different_registers(klass, new_obj, t1, t2);\n+\n+  \/\/ get instance_size in InstanceKlass (scaled to a count of bytes)\n+  movl(layout_size, Address(klass, Klass::layout_helper_offset()));\n+  \/\/ test to see if it is malformed in some way\n+  testl(layout_size, Klass::_lh_instance_slow_path_bit);\n+  jcc(Assembler::notZero, slow_case_no_pop);\n+\n+  \/\/ Allocate the instance:\n+  \/\/  If TLAB is enabled:\n+  \/\/    Try to allocate in the TLAB.\n+  \/\/    If fails, go to the slow path.\n+  \/\/  Else If inline contiguous allocations are enabled:\n+  \/\/    Try to allocate in eden.\n+  \/\/    If fails due to heap end, go to slow path.\n+  \/\/\n+  \/\/  If TLAB is enabled OR inline contiguous is enabled:\n+  \/\/    Initialize the allocation.\n+  \/\/    Exit.\n+  \/\/\n+  \/\/  Go to slow path.\n+\n+  push(klass);\n+  if (UseTLAB) {\n+    tlab_allocate(new_obj, layout_size, 0, klass, t2, slow_case);\n+    if (ZeroTLAB || (!clear_fields)) {\n+      \/\/ the fields have been already cleared\n+      jmp(initialize_header);\n+    } else {\n+      \/\/ initialize both the header and fields\n+      jmp(initialize_object);\n+    }\n+  } else {\n+    jmp(slow_case);\n+  }\n+\n+  \/\/ If UseTLAB is true, the object is created above and there is an initialize need.\n+  \/\/ Otherwise, skip and go to the slow path.\n+  if (UseTLAB) {\n+    if (clear_fields) {\n+      \/\/ The object is initialized before the header.  If the object size is\n+      \/\/ zero, go directly to the header initialization.\n+      bind(initialize_object);\n+      if (UseCompactObjectHeaders) {\n+        assert(is_aligned(oopDesc::base_offset_in_bytes(), BytesPerLong), \"oop base offset must be 8-byte-aligned\");\n+        decrement(layout_size, oopDesc::base_offset_in_bytes());\n+      } else {\n+        decrement(layout_size, sizeof(oopDesc));\n+      }\n+      jcc(Assembler::zero, initialize_header);\n+\n+      \/\/ Initialize topmost object field, divide size by 8, check if odd and\n+      \/\/ test if zero.\n+      Register zero = klass;\n+      xorl(zero, zero);    \/\/ use zero reg to clear memory (shorter code)\n+      shrl(layout_size, LogBytesPerLong); \/\/ divide by 2*oopSize and set carry flag if odd\n+\n+  #ifdef ASSERT\n+      \/\/ make sure instance_size was multiple of 8\n+      Label L;\n+      \/\/ Ignore partial flag stall after shrl() since it is debug VM\n+      jcc(Assembler::carryClear, L);\n+      stop(\"object size is not multiple of 2 - adjust this code\");\n+      bind(L);\n+      \/\/ must be > 0, no extra check needed here\n+  #endif\n+\n+      \/\/ initialize remaining object fields: instance_size was a multiple of 8\n+      {\n+        Label loop;\n+        bind(loop);\n+        int header_size_bytes = oopDesc::header_size() * HeapWordSize;\n+        assert(is_aligned(header_size_bytes, BytesPerLong), \"oop header size must be 8-byte-aligned\");\n+        movptr(Address(new_obj, layout_size, Address::times_8, header_size_bytes - 1*oopSize), zero);\n+        decrement(layout_size);\n+        jcc(Assembler::notZero, loop);\n+      }\n+    } \/\/ clear_fields\n+\n+    \/\/ initialize object header only.\n+    bind(initialize_header);\n+    if (UseCompactObjectHeaders || EnableValhalla) {\n+      pop(klass);\n+      Register mark_word = t2;\n+      movptr(mark_word, Address(klass, Klass::prototype_header_offset()));\n+      movptr(Address(new_obj, oopDesc::mark_offset_in_bytes ()), mark_word);\n+    } else {\n+     movptr(Address(new_obj, oopDesc::mark_offset_in_bytes()),\n+            (intptr_t)markWord::prototype().value()); \/\/ header\n+     pop(klass);   \/\/ get saved klass back in the register.\n+    }\n+    if (!UseCompactObjectHeaders) {\n+      xorl(rsi, rsi);                 \/\/ use zero reg to clear memory (shorter code)\n+      store_klass_gap(new_obj, rsi);  \/\/ zero klass gap for compressed oops\n+      movptr(t2, klass);         \/\/ preserve klass\n+      store_klass(new_obj, t2, rscratch1);  \/\/ src klass reg is potentially compressed\n+    }\n+    jmp(done);\n+  }\n+\n+  bind(slow_case);\n+  pop(klass);\n+  bind(slow_case_no_pop);\n+  jmp(alloc_failed);\n+\n+  bind(done);\n+}\n+\n@@ -3634,0 +3870,27 @@\n+void MacroAssembler::get_inline_type_field_klass(Register holder_klass, Register index, Register inline_klass) {\n+  inline_layout_info(holder_klass, index, inline_klass);\n+  movptr(inline_klass, Address(inline_klass, InlineLayoutInfo::klass_offset()));\n+}\n+\n+void MacroAssembler::inline_layout_info(Register holder_klass, Register index, Register layout_info) {\n+  movptr(layout_info, Address(holder_klass, InstanceKlass::inline_layout_info_array_offset()));\n+#ifdef ASSERT\n+  {\n+    Label done;\n+    cmpptr(layout_info, 0);\n+    jcc(Assembler::notEqual, done);\n+    stop(\"inline_layout_info_array is null\");\n+    bind(done);\n+  }\n+#endif\n+\n+  InlineLayoutInfo array[2];\n+  int size = (char*)&array[1] - (char*)&array[0]; \/\/ computing size of array elements\n+  if (is_power_of_2(size)) {\n+    shll(index, log2i_exact(size)); \/\/ Scale index by power of 2\n+  } else {\n+    imull(index, index, size); \/\/ Scale the index to be the entry index * array_element_size\n+  }\n+  lea(layout_info, Address(layout_info, index, Address::times_1, Array<InlineLayoutInfo>::base_offset_in_bytes()));\n+}\n+\n@@ -4684,1 +4947,5 @@\n-  if (!VerifyOops) return;\n+  if (!VerifyOops || VerifyAdapterSharing) {\n+    \/\/ Below address of the code string confuses VerifyAdapterSharing\n+    \/\/ because it may differ between otherwise equivalent adapters.\n+    return;\n+  }\n@@ -4743,1 +5010,5 @@\n-  if (!VerifyOops) return;\n+  if (!VerifyOops || VerifyAdapterSharing) {\n+    \/\/ Below address of the code string confuses VerifyAdapterSharing\n+    \/\/ because it may differ between otherwise equivalent adapters.\n+    return;\n+  }\n@@ -5137,0 +5408,10 @@\n+void MacroAssembler::load_metadata(Register dst, Register src) {\n+  if (UseCompactObjectHeaders) {\n+    load_narrow_klass_compact(dst, src);\n+  } else if (UseCompressedClassPointers) {\n+    movl(dst, Address(src, oopDesc::klass_offset_in_bytes()));\n+  } else {\n+    movptr(dst, Address(src, oopDesc::klass_offset_in_bytes()));\n+  }\n+}\n+\n@@ -5158,0 +5439,5 @@\n+void MacroAssembler::load_prototype_header(Register dst, Register src, Register tmp) {\n+  load_klass(dst, src, tmp);\n+  movptr(dst, Address(dst, Klass::prototype_header_offset()));\n+}\n+\n@@ -5223,0 +5509,40 @@\n+void MacroAssembler::flat_field_copy(DecoratorSet decorators, Register src, Register dst,\n+                                     Register inline_layout_info) {\n+  BarrierSetAssembler* bs = BarrierSet::barrier_set()->barrier_set_assembler();\n+  bs->flat_field_copy(this, decorators, src, dst, inline_layout_info);\n+}\n+\n+void MacroAssembler::payload_offset(Register inline_klass, Register offset) {\n+  movptr(offset, Address(inline_klass, InstanceKlass::adr_inlineklass_fixed_block_offset()));\n+  movl(offset, Address(offset, InlineKlass::payload_offset_offset()));\n+}\n+\n+void MacroAssembler::payload_addr(Register oop, Register data, Register inline_klass) {\n+  \/\/ ((address) (void*) o) + vk->payload_offset();\n+  Register offset = (data == oop) ? rscratch1 : data;\n+  payload_offset(inline_klass, offset);\n+  if (data == oop) {\n+    addptr(data, offset);\n+  } else {\n+    lea(data, Address(oop, offset));\n+  }\n+}\n+\n+void MacroAssembler::data_for_value_array_index(Register array, Register array_klass,\n+                                                Register index, Register data) {\n+  assert(index != rcx, \"index needs to shift by rcx\");\n+  assert_different_registers(array, array_klass, index);\n+  assert_different_registers(rcx, array, index);\n+\n+  \/\/ array->base() + (index << Klass::layout_helper_log2_element_size(lh));\n+  movl(rcx, Address(array_klass, Klass::layout_helper_offset()));\n+\n+  \/\/ Klass::layout_helper_log2_element_size(lh)\n+  \/\/ (lh >> _lh_log2_element_size_shift) & _lh_log2_element_size_mask;\n+  shrl(rcx, Klass::_lh_log2_element_size_shift);\n+  andl(rcx, Klass::_lh_log2_element_size_mask);\n+  shlptr(index); \/\/ index << rcx\n+\n+  lea(data, Address(array, index, Address::times_1, arrayOopDesc::base_offset_in_bytes(T_FLAT_ELEMENT)));\n+}\n+\n@@ -5582,1 +5908,1 @@\n-void MacroAssembler::xmm_clear_mem(Register base, Register cnt, Register rtmp, XMMRegister xtmp, KRegister mask) {\n+void MacroAssembler::xmm_clear_mem(Register base, Register cnt, Register val, XMMRegister xtmp, KRegister mask) {\n@@ -5588,1 +5914,1 @@\n-    vpxor(xtmp, xtmp, xtmp, AVX_512bit);\n+    evpbroadcastq(xtmp, val, AVX_512bit);\n@@ -5590,1 +5916,3 @@\n-    vpxor(xtmp, xtmp, xtmp, AVX_256bit);\n+    movdq(xtmp, val);\n+    punpcklqdq(xtmp, xtmp);\n+    vinserti128_high(xtmp, xtmp);\n@@ -5592,1 +5920,2 @@\n-    pxor(xtmp, xtmp);\n+    movdq(xtmp, val);\n+    punpcklqdq(xtmp, xtmp);\n@@ -5615,1 +5944,1 @@\n-    fill64_masked(3, base, 0, xtmp, mask, cnt, rtmp, true);\n+    fill64_masked(3, base, 0, xtmp, mask, cnt, val, true);\n@@ -5634,1 +5963,1 @@\n-    fill32_masked(3, base, 0, xtmp, mask, cnt, rtmp);\n+    fill32_masked(3, base, 0, xtmp, mask, cnt, val);\n@@ -5647,0 +5976,408 @@\n+int MacroAssembler::store_inline_type_fields_to_buf(ciInlineKlass* vk, bool from_interpreter) {\n+  assert(InlineTypeReturnedAsFields, \"Inline types should never be returned as fields\");\n+  \/\/ An inline type might be returned. If fields are in registers we\n+  \/\/ need to allocate an inline type instance and initialize it with\n+  \/\/ the value of the fields.\n+  Label skip;\n+  \/\/ We only need a new buffered inline type if a new one is not returned\n+  testptr(rax, 1);\n+  jcc(Assembler::zero, skip);\n+  int call_offset = -1;\n+\n+#ifdef _LP64\n+  \/\/ The following code is similar to allocate_instance but has some slight differences,\n+  \/\/ e.g. object size is always not zero, sometimes it's constant; storing klass ptr after\n+  \/\/ allocating is not necessary if vk != nullptr, etc. allocate_instance is not aware of these.\n+  Label slow_case;\n+  \/\/ 1. Try to allocate a new buffered inline instance either from TLAB or eden space\n+  mov(rscratch1, rax); \/\/ save rax for slow_case since *_allocate may corrupt it when allocation failed\n+  if (vk != nullptr) {\n+    \/\/ Called from C1, where the return type is statically known.\n+    movptr(rbx, (intptr_t)vk->get_InlineKlass());\n+    jint lh = vk->layout_helper();\n+    assert(lh != Klass::_lh_neutral_value, \"inline class in return type must have been resolved\");\n+    if (UseTLAB && !Klass::layout_helper_needs_slow_path(lh)) {\n+      tlab_allocate(rax, noreg, lh, r13, r14, slow_case);\n+    } else {\n+      jmp(slow_case);\n+    }\n+  } else {\n+    \/\/ Call from interpreter. RAX contains ((the InlineKlass* of the return type) | 0x01)\n+    mov(rbx, rax);\n+    andptr(rbx, -2);\n+    if (UseTLAB) {\n+      movl(r14, Address(rbx, Klass::layout_helper_offset()));\n+      testl(r14, Klass::_lh_instance_slow_path_bit);\n+      jcc(Assembler::notZero, slow_case);\n+      tlab_allocate(rax, r14, 0, r13, r14, slow_case);\n+    } else {\n+      jmp(slow_case);\n+    }\n+  }\n+  if (UseTLAB) {\n+    \/\/ 2. Initialize buffered inline instance header\n+    Register buffer_obj = rax;\n+    if (UseCompactObjectHeaders) {\n+      Register mark_word = r13;\n+      movptr(mark_word, Address(rbx, Klass::prototype_header_offset()));\n+      movptr(Address(buffer_obj, oopDesc::mark_offset_in_bytes ()), mark_word);\n+    } else {\n+      movptr(Address(buffer_obj, oopDesc::mark_offset_in_bytes()), (intptr_t)markWord::inline_type_prototype().value());\n+      xorl(r13, r13);\n+      store_klass_gap(buffer_obj, r13);\n+      if (vk == nullptr) {\n+        \/\/ store_klass corrupts rbx(klass), so save it in r13 for later use (interpreter case only).\n+        mov(r13, rbx);\n+      }\n+      store_klass(buffer_obj, rbx, rscratch1);\n+    }\n+    \/\/ 3. Initialize its fields with an inline class specific handler\n+    if (vk != nullptr) {\n+      call(RuntimeAddress(vk->pack_handler())); \/\/ no need for call info as this will not safepoint.\n+    } else {\n+      movptr(rbx, Address(r13, InstanceKlass::adr_inlineklass_fixed_block_offset()));\n+      movptr(rbx, Address(rbx, InlineKlass::pack_handler_offset()));\n+      call(rbx);\n+    }\n+    jmp(skip);\n+  }\n+  bind(slow_case);\n+  \/\/ We failed to allocate a new inline type, fall back to a runtime\n+  \/\/ call. Some oop field may be live in some registers but we can't\n+  \/\/ tell. That runtime call will take care of preserving them\n+  \/\/ across a GC if there's one.\n+  mov(rax, rscratch1);\n+#endif\n+\n+  if (from_interpreter) {\n+    super_call_VM_leaf(StubRoutines::store_inline_type_fields_to_buf());\n+  } else {\n+    call(RuntimeAddress(StubRoutines::store_inline_type_fields_to_buf()));\n+    call_offset = offset();\n+  }\n+\n+  bind(skip);\n+  return call_offset;\n+}\n+\n+\/\/ Move a value between registers\/stack slots and update the reg_state\n+bool MacroAssembler::move_helper(VMReg from, VMReg to, BasicType bt, RegState reg_state[]) {\n+  assert(from->is_valid() && to->is_valid(), \"source and destination must be valid\");\n+  if (reg_state[to->value()] == reg_written) {\n+    return true; \/\/ Already written\n+  }\n+  if (from != to && bt != T_VOID) {\n+    if (reg_state[to->value()] == reg_readonly) {\n+      return false; \/\/ Not yet writable\n+    }\n+    if (from->is_reg()) {\n+      if (to->is_reg()) {\n+        if (from->is_XMMRegister()) {\n+          if (bt == T_DOUBLE) {\n+            movdbl(to->as_XMMRegister(), from->as_XMMRegister());\n+          } else {\n+            assert(bt == T_FLOAT, \"must be float\");\n+            movflt(to->as_XMMRegister(), from->as_XMMRegister());\n+          }\n+        } else {\n+          movq(to->as_Register(), from->as_Register());\n+        }\n+      } else {\n+        int st_off = to->reg2stack() * VMRegImpl::stack_slot_size + wordSize;\n+        Address to_addr = Address(rsp, st_off);\n+        if (from->is_XMMRegister()) {\n+          if (bt == T_DOUBLE) {\n+            movdbl(to_addr, from->as_XMMRegister());\n+          } else {\n+            assert(bt == T_FLOAT, \"must be float\");\n+            movflt(to_addr, from->as_XMMRegister());\n+          }\n+        } else {\n+          movq(to_addr, from->as_Register());\n+        }\n+      }\n+    } else {\n+      Address from_addr = Address(rsp, from->reg2stack() * VMRegImpl::stack_slot_size + wordSize);\n+      if (to->is_reg()) {\n+        if (to->is_XMMRegister()) {\n+          if (bt == T_DOUBLE) {\n+            movdbl(to->as_XMMRegister(), from_addr);\n+          } else {\n+            assert(bt == T_FLOAT, \"must be float\");\n+            movflt(to->as_XMMRegister(), from_addr);\n+          }\n+        } else {\n+          movq(to->as_Register(), from_addr);\n+        }\n+      } else {\n+        int st_off = to->reg2stack() * VMRegImpl::stack_slot_size + wordSize;\n+        movq(r13, from_addr);\n+        movq(Address(rsp, st_off), r13);\n+      }\n+    }\n+  }\n+  \/\/ Update register states\n+  reg_state[from->value()] = reg_writable;\n+  reg_state[to->value()] = reg_written;\n+  return true;\n+}\n+\n+\/\/ Calculate the extra stack space required for packing or unpacking inline\n+\/\/ args and adjust the stack pointer\n+int MacroAssembler::extend_stack_for_inline_args(int args_on_stack) {\n+  \/\/ Two additional slots to account for return address\n+  int sp_inc = (args_on_stack + 2) * VMRegImpl::stack_slot_size;\n+  sp_inc = align_up(sp_inc, StackAlignmentInBytes);\n+  \/\/ Save the return address, adjust the stack (make sure it is properly\n+  \/\/ 16-byte aligned) and copy the return address to the new top of the stack.\n+  \/\/ The stack will be repaired on return (see MacroAssembler::remove_frame).\n+  assert(sp_inc > 0, \"sanity\");\n+  pop(r13);\n+  subptr(rsp, sp_inc);\n+  push(r13);\n+  return sp_inc;\n+}\n+\n+\/\/ Read all fields from an inline type buffer and store the field values in registers\/stack slots.\n+bool MacroAssembler::unpack_inline_helper(const GrowableArray<SigEntry>* sig, int& sig_index,\n+                                          VMReg from, int& from_index, VMRegPair* to, int to_count, int& to_index,\n+                                          RegState reg_state[]) {\n+  assert(sig->at(sig_index)._bt == T_VOID, \"should be at end delimiter\");\n+  assert(from->is_valid(), \"source must be valid\");\n+  bool progress = false;\n+#ifdef ASSERT\n+  const int start_offset = offset();\n+#endif\n+\n+  Label L_null, L_notNull;\n+  \/\/ Don't use r14 as tmp because it's used for spilling (see MacroAssembler::spill_reg_for)\n+  Register tmp1 = r10;\n+  Register tmp2 = r13;\n+  Register fromReg = noreg;\n+  ScalarizedInlineArgsStream stream(sig, sig_index, to, to_count, to_index, -1);\n+  bool done = true;\n+  bool mark_done = true;\n+  VMReg toReg;\n+  BasicType bt;\n+  \/\/ Check if argument requires a null check\n+  bool null_check = false;\n+  VMReg nullCheckReg;\n+  while (stream.next(nullCheckReg, bt)) {\n+    if (sig->at(stream.sig_index())._offset == -1) {\n+      null_check = true;\n+      break;\n+    }\n+  }\n+  stream.reset(sig_index, to_index);\n+  while (stream.next(toReg, bt)) {\n+    assert(toReg->is_valid(), \"destination must be valid\");\n+    int idx = (int)toReg->value();\n+    if (reg_state[idx] == reg_readonly) {\n+      if (idx != from->value()) {\n+        mark_done = false;\n+      }\n+      done = false;\n+      continue;\n+    } else if (reg_state[idx] == reg_written) {\n+      continue;\n+    }\n+    assert(reg_state[idx] == reg_writable, \"must be writable\");\n+    reg_state[idx] = reg_written;\n+    progress = true;\n+\n+    if (fromReg == noreg) {\n+      if (from->is_reg()) {\n+        fromReg = from->as_Register();\n+      } else {\n+        int st_off = from->reg2stack() * VMRegImpl::stack_slot_size + wordSize;\n+        movq(tmp1, Address(rsp, st_off));\n+        fromReg = tmp1;\n+      }\n+      if (null_check) {\n+        \/\/ Nullable inline type argument, emit null check\n+        testptr(fromReg, fromReg);\n+        jcc(Assembler::zero, L_null);\n+      }\n+    }\n+    int off = sig->at(stream.sig_index())._offset;\n+    if (off == -1) {\n+      assert(null_check, \"Missing null check at\");\n+      if (toReg->is_stack()) {\n+        int st_off = toReg->reg2stack() * VMRegImpl::stack_slot_size + wordSize;\n+        movq(Address(rsp, st_off), 1);\n+      } else {\n+        movq(toReg->as_Register(), 1);\n+      }\n+      continue;\n+    }\n+    assert(off > 0, \"offset in object should be positive\");\n+    Address fromAddr = Address(fromReg, off);\n+    if (!toReg->is_XMMRegister()) {\n+      Register dst = toReg->is_stack() ? tmp2 : toReg->as_Register();\n+      if (is_reference_type(bt)) {\n+        load_heap_oop(dst, fromAddr);\n+      } else {\n+        bool is_signed = (bt != T_CHAR) && (bt != T_BOOLEAN);\n+        load_sized_value(dst, fromAddr, type2aelembytes(bt), is_signed);\n+      }\n+      if (toReg->is_stack()) {\n+        int st_off = toReg->reg2stack() * VMRegImpl::stack_slot_size + wordSize;\n+        movq(Address(rsp, st_off), dst);\n+      }\n+    } else if (bt == T_DOUBLE) {\n+      movdbl(toReg->as_XMMRegister(), fromAddr);\n+    } else {\n+      assert(bt == T_FLOAT, \"must be float\");\n+      movflt(toReg->as_XMMRegister(), fromAddr);\n+    }\n+  }\n+  if (progress && null_check) {\n+    if (done) {\n+      jmp(L_notNull);\n+      bind(L_null);\n+      \/\/ Set null marker to zero to signal that the argument is null.\n+      \/\/ Also set all oop fields to zero to make the GC happy.\n+      stream.reset(sig_index, to_index);\n+      while (stream.next(toReg, bt)) {\n+        if (sig->at(stream.sig_index())._offset == -1 ||\n+            bt == T_OBJECT || bt == T_ARRAY) {\n+          if (toReg->is_stack()) {\n+            int st_off = toReg->reg2stack() * VMRegImpl::stack_slot_size + wordSize;\n+            movq(Address(rsp, st_off), 0);\n+          } else {\n+            xorq(toReg->as_Register(), toReg->as_Register());\n+          }\n+        }\n+      }\n+      bind(L_notNull);\n+    } else {\n+      bind(L_null);\n+    }\n+  }\n+\n+  sig_index = stream.sig_index();\n+  to_index = stream.regs_index();\n+\n+  if (mark_done && reg_state[from->value()] != reg_written) {\n+    \/\/ This is okay because no one else will write to that slot\n+    reg_state[from->value()] = reg_writable;\n+  }\n+  from_index--;\n+  assert(progress || (start_offset == offset()), \"should not emit code\");\n+  return done;\n+}\n+\n+bool MacroAssembler::pack_inline_helper(const GrowableArray<SigEntry>* sig, int& sig_index, int vtarg_index,\n+                                        VMRegPair* from, int from_count, int& from_index, VMReg to,\n+                                        RegState reg_state[], Register val_array) {\n+  assert(sig->at(sig_index)._bt == T_METADATA, \"should be at delimiter\");\n+  assert(to->is_valid(), \"destination must be valid\");\n+\n+  if (reg_state[to->value()] == reg_written) {\n+    skip_unpacked_fields(sig, sig_index, from, from_count, from_index);\n+    return true; \/\/ Already written\n+  }\n+\n+  \/\/ TODO 8284443 Isn't it an issue if below code uses r14 as tmp when it contains a spilled value?\n+  \/\/ Be careful with r14 because it's used for spilling (see MacroAssembler::spill_reg_for).\n+  Register val_obj_tmp = r11;\n+  Register from_reg_tmp = r14;\n+  Register tmp1 = r10;\n+  Register tmp2 = r13;\n+  Register tmp3 = rbx;\n+  Register val_obj = to->is_stack() ? val_obj_tmp : to->as_Register();\n+\n+  assert_different_registers(val_obj_tmp, from_reg_tmp, tmp1, tmp2, tmp3, val_array);\n+\n+  if (reg_state[to->value()] == reg_readonly) {\n+    if (!is_reg_in_unpacked_fields(sig, sig_index, to, from, from_count, from_index)) {\n+      skip_unpacked_fields(sig, sig_index, from, from_count, from_index);\n+      return false; \/\/ Not yet writable\n+    }\n+    val_obj = val_obj_tmp;\n+  }\n+\n+  int index = arrayOopDesc::base_offset_in_bytes(T_OBJECT) + vtarg_index * type2aelembytes(T_OBJECT);\n+  load_heap_oop(val_obj, Address(val_array, index));\n+\n+  ScalarizedInlineArgsStream stream(sig, sig_index, from, from_count, from_index);\n+  VMReg fromReg;\n+  BasicType bt;\n+  Label L_null;\n+  while (stream.next(fromReg, bt)) {\n+    assert(fromReg->is_valid(), \"source must be valid\");\n+    reg_state[fromReg->value()] = reg_writable;\n+\n+    int off = sig->at(stream.sig_index())._offset;\n+    if (off == -1) {\n+      \/\/ Nullable inline type argument, emit null check\n+      Label L_notNull;\n+      if (fromReg->is_stack()) {\n+        int ld_off = fromReg->reg2stack() * VMRegImpl::stack_slot_size + wordSize;\n+        testb(Address(rsp, ld_off), 1);\n+      } else {\n+        testb(fromReg->as_Register(), 1);\n+      }\n+      jcc(Assembler::notZero, L_notNull);\n+      movptr(val_obj, 0);\n+      jmp(L_null);\n+      bind(L_notNull);\n+      continue;\n+    }\n+\n+    assert(off > 0, \"offset in object should be positive\");\n+    size_t size_in_bytes = is_java_primitive(bt) ? type2aelembytes(bt) : wordSize;\n+\n+    \/\/ Pack the scalarized field into the value object.\n+    Address dst(val_obj, off);\n+    if (!fromReg->is_XMMRegister()) {\n+      Register src;\n+      if (fromReg->is_stack()) {\n+        src = from_reg_tmp;\n+        int ld_off = fromReg->reg2stack() * VMRegImpl::stack_slot_size + wordSize;\n+        load_sized_value(src, Address(rsp, ld_off), size_in_bytes, \/* is_signed *\/ false);\n+      } else {\n+        src = fromReg->as_Register();\n+      }\n+      assert_different_registers(dst.base(), src, tmp1, tmp2, tmp3, val_array);\n+      if (is_reference_type(bt)) {\n+        store_heap_oop(dst, src, tmp1, tmp2, tmp3, IN_HEAP | ACCESS_WRITE | IS_DEST_UNINITIALIZED);\n+      } else {\n+        store_sized_value(dst, src, size_in_bytes);\n+      }\n+    } else if (bt == T_DOUBLE) {\n+      movdbl(dst, fromReg->as_XMMRegister());\n+    } else {\n+      assert(bt == T_FLOAT, \"must be float\");\n+      movflt(dst, fromReg->as_XMMRegister());\n+    }\n+  }\n+  bind(L_null);\n+  sig_index = stream.sig_index();\n+  from_index = stream.regs_index();\n+\n+  assert(reg_state[to->value()] == reg_writable, \"must have already been read\");\n+  bool success = move_helper(val_obj->as_VMReg(), to, T_OBJECT, reg_state);\n+  assert(success, \"to register must be writeable\");\n+  return true;\n+}\n+\n+VMReg MacroAssembler::spill_reg_for(VMReg reg) {\n+  return reg->is_XMMRegister() ? xmm8->as_VMReg() : r14->as_VMReg();\n+}\n+\n+void MacroAssembler::remove_frame(int initial_framesize, bool needs_stack_repair) {\n+  assert((initial_framesize & (StackAlignmentInBytes-1)) == 0, \"frame size not aligned\");\n+  if (needs_stack_repair) {\n+    \/\/ TODO 8284443 Add a comment drawing the frame like in Aarch64's version of MacroAssembler::remove_frame\n+    movq(rbp, Address(rsp, initial_framesize));\n+    \/\/ The stack increment resides just below the saved rbp\n+    addq(rsp, Address(rsp, initial_framesize - wordSize));\n+  } else {\n+    if (initial_framesize > 0) {\n+      addq(rsp, initial_framesize);\n+    }\n+    pop(rbp);\n+  }\n+}\n+\n@@ -5736,2 +6473,2 @@\n-void MacroAssembler::clear_mem(Register base, Register cnt, Register tmp, XMMRegister xtmp,\n-                               bool is_large, KRegister mask) {\n+void MacroAssembler::clear_mem(Register base, Register cnt, Register val, XMMRegister xtmp,\n+                               bool is_large, bool word_copy_only, KRegister mask) {\n@@ -5742,1 +6479,1 @@\n-  assert(tmp==rax,   \"tmp register must be eax for rep stos\");\n+  assert(val==rax,   \"val register must be eax for rep stos\");\n@@ -5748,3 +6485,0 @@\n-  if (!is_large || !UseXMMForObjInit) {\n-    xorptr(tmp, tmp);\n-  }\n@@ -5762,1 +6496,1 @@\n-    movptr(Address(base, cnt, Address::times_ptr), tmp);\n+    movptr(Address(base, cnt, Address::times_ptr), val);\n@@ -5771,1 +6505,1 @@\n-  if (UseFastStosb) {\n+  if (UseFastStosb && !word_copy_only) {\n@@ -5775,1 +6509,1 @@\n-    xmm_clear_mem(base, cnt, tmp, xtmp, mask);\n+    xmm_clear_mem(base, cnt, val, xtmp, mask);\n@@ -9632,0 +10366,4 @@\n+  if (EnableValhalla) {\n+    \/\/ Mask inline_type bit such that we go to the slow path if object is an inline type\n+    andptr(reg_rax, ~((int) markWord::inline_type_bit_in_place));\n+  }\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.cpp","additions":755,"deletions":17,"binary":false,"changes":772,"status":"modified"},{"patch":"@@ -33,0 +33,1 @@\n+#include \"runtime\/signature.hpp\"\n@@ -36,0 +37,2 @@\n+class ciInlineKlass;\n+\n@@ -97,0 +100,22 @@\n+  \/\/ markWord tests, kills markWord reg\n+  void test_markword_is_inline_type(Register markword, Label& is_inline_type);\n+\n+  \/\/ inlineKlass queries, kills temp_reg\n+  void test_oop_is_not_inline_type(Register object, Register tmp, Label& not_inline_type, bool can_be_null = true);\n+\n+  void test_field_is_null_free_inline_type(Register flags, Register temp_reg, Label& is_null_free);\n+  void test_field_is_not_null_free_inline_type(Register flags, Register temp_reg, Label& not_null_free);\n+  void test_field_is_flat(Register flags, Register temp_reg, Label& is_flat);\n+  void test_field_has_null_marker(Register flags, Register temp_reg, Label& has_null_marker);\n+\n+  \/\/ Check oops for special arrays, i.e. flat arrays and\/or null-free arrays\n+  void test_oop_prototype_bit(Register oop, Register temp_reg, int32_t test_bit, bool jmp_set, Label& jmp_label);\n+  void test_flat_array_oop(Register oop, Register temp_reg, Label& is_flat_array);\n+  void test_non_flat_array_oop(Register oop, Register temp_reg, Label& is_non_flat_array);\n+  void test_null_free_array_oop(Register oop, Register temp_reg, Label& is_null_free_array);\n+  void test_non_null_free_array_oop(Register oop, Register temp_reg, Label& is_non_null_free_array);\n+\n+  \/\/ Check array klass layout helper for flat or null-free arrays...\n+  void test_flat_array_layout(Register lh, Label& is_flat_array);\n+  void test_non_flat_array_layout(Register lh, Label& is_non_flat_array);\n+\n@@ -350,0 +375,3 @@\n+\n+  \/\/ Load oopDesc._metadata without decode (useful for direct Klass* compare from oops)\n+  void load_metadata(Register dst, Register src);\n@@ -367,0 +395,9 @@\n+  void flat_field_copy(DecoratorSet decorators, Register src, Register dst, Register inline_layout_info);\n+\n+  \/\/ inline type data payload offsets...\n+  void payload_offset(Register inline_klass, Register offset);\n+  void payload_addr(Register oop, Register data, Register inline_klass);\n+  \/\/ get data payload ptr a flat value array at index, kills rcx and index\n+  void data_for_value_array_index(Register array, Register array_klass,\n+                                  Register index, Register data);\n+\n@@ -376,0 +413,2 @@\n+  void load_prototype_header(Register dst, Register src, Register tmp);\n+\n@@ -513,0 +552,9 @@\n+\n+  \/\/ Object \/ value buffer allocation...\n+  \/\/ Allocate instance of klass, assumes klass initialized by caller\n+  \/\/ new_obj prefers to be rax\n+  \/\/ Kills t1 and t2, perserves klass, return allocation in new_obj (rsi on LP64)\n+  void allocate_instance(Register klass, Register new_obj,\n+                         Register t1, Register t2,\n+                         bool clear_fields, Label& alloc_failed);\n+\n@@ -523,0 +571,5 @@\n+  \/\/ For field \"index\" within \"klass\", return inline_klass ...\n+  void get_inline_type_field_klass(Register klass, Register index, Register inline_klass);\n+\n+  void inline_layout_info(Register klass, Register index, Register layout_info);\n+\n@@ -769,0 +822,1 @@\n+  void andptr(Register dst, Address src) { andq(dst, src); }\n@@ -1904,0 +1958,15 @@\n+  \/\/ Inline type specific methods\n+  #include \"asm\/macroAssembler_common.hpp\"\n+\n+  int store_inline_type_fields_to_buf(ciInlineKlass* vk, bool from_interpreter = true);\n+  bool move_helper(VMReg from, VMReg to, BasicType bt, RegState reg_state[]);\n+  bool unpack_inline_helper(const GrowableArray<SigEntry>* sig, int& sig_index,\n+                            VMReg from, int& from_index, VMRegPair* to, int to_count, int& to_index,\n+                            RegState reg_state[]);\n+  bool pack_inline_helper(const GrowableArray<SigEntry>* sig, int& sig_index, int vtarg_index,\n+                          VMRegPair* from, int from_count, int& from_index, VMReg to,\n+                          RegState reg_state[], Register val_array);\n+  int extend_stack_for_inline_args(int args_on_stack);\n+  void remove_frame(int initial_framesize, bool needs_stack_repair);\n+  VMReg spill_reg_for(VMReg reg);\n+\n@@ -1906,1 +1975,1 @@\n-  void clear_mem(Register base, Register cnt, Register rtmp, XMMRegister xtmp, bool is_large, KRegister mask=knoreg);\n+  void clear_mem(Register base, Register cnt, Register val, XMMRegister xtmp, bool is_large, bool word_copy_only, KRegister mask=knoreg);\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.hpp","additions":70,"deletions":1,"binary":false,"changes":71,"status":"modified"},{"patch":"@@ -618,0 +618,4 @@\n+  if (_entry_point == nullptr) {\n+    \/\/ CallLeafNoFPInDirect\n+    return 3; \/\/ callq (register)\n+  }\n@@ -624,0 +628,1 @@\n+\n@@ -849,14 +854,1 @@\n-  int framesize = C->output()->frame_size_in_bytes();\n-  int bangsize = C->output()->bang_size_in_bytes();\n-\n-  if (C->clinit_barrier_on_entry()) {\n-    assert(VM_Version::supports_fast_class_init_checks(), \"sanity\");\n-    assert(!C->method()->holder()->is_not_initialized(), \"initialization should have been started\");\n-\n-    Label L_skip_barrier;\n-    Register klass = rscratch1;\n-\n-    __ mov_metadata(klass, C->method()->holder()->constant_encoding());\n-    __ clinit_barrier(klass, &L_skip_barrier \/*L_fast_path*\/);\n-\n-    __ jump(RuntimeAddress(SharedRuntime::get_handle_wrong_method_stub())); \/\/ slow path\n+  __ verified_entry(C);\n@@ -864,1 +856,2 @@\n-    __ bind(L_skip_barrier);\n+  if (ra_->C->stub_function() == nullptr) {\n+    __ entry_barrier();\n@@ -867,1 +860,3 @@\n-  __ verified_entry(framesize, C->output()->need_stack_bang(bangsize)?bangsize:0, false, C->stub_function() != nullptr);\n+  if (!Compile::current()->output()->in_scratch_emit_size()) {\n+    __ bind(*_verified_entry);\n+  }\n@@ -879,6 +874,0 @@\n-uint MachPrologNode::size(PhaseRegAlloc* ra_) const\n-{\n-  return MachNode::size(ra_); \/\/ too many variables; just compute it\n-                              \/\/ the hard way\n-}\n-\n@@ -931,13 +920,3 @@\n-  int framesize = C->output()->frame_size_in_bytes();\n-  assert((framesize & (StackAlignmentInBytes-1)) == 0, \"frame size not aligned\");\n-  \/\/ Remove word for return adr already pushed\n-  \/\/ and RBP\n-  framesize -= 2*wordSize;\n-\n-  \/\/ Note that VerifyStackAtCalls' Majik cookie does not change the frame size popped here\n-\n-  if (framesize) {\n-    __ addq(rsp, framesize);\n-  }\n-\n-  __ popq(rbp);\n+  \/\/ Subtract two words to account for return address and rbp\n+  int initial_framesize = C->output()->frame_size_in_bytes() - 2*wordSize;\n+  __ remove_frame(initial_framesize, C->needs_stack_repair());\n@@ -962,6 +941,0 @@\n-uint MachEpilogNode::size(PhaseRegAlloc* ra_) const\n-{\n-  return MachNode::size(ra_); \/\/ too many variables; just compute it\n-                              \/\/ the hard way\n-}\n-\n@@ -1569,0 +1542,43 @@\n+\/\/=============================================================================\n+#ifndef PRODUCT\n+void MachVEPNode::format(PhaseRegAlloc* ra_, outputStream* st) const\n+{\n+  st->print_cr(\"MachVEPNode\");\n+}\n+#endif\n+\n+void MachVEPNode::emit(C2_MacroAssembler* masm, PhaseRegAlloc* ra_) const\n+{\n+  CodeBuffer* cbuf = masm->code();\n+  uint insts_size = cbuf->insts_size();\n+  if (!_verified) {\n+    __ ic_check(1);\n+  } else {\n+    \/\/ TODO 8284443 Avoid creation of temporary frame\n+    if (ra_->C->stub_function() == nullptr) {\n+      __ verified_entry(ra_->C, 0);\n+      __ entry_barrier();\n+      int initial_framesize = ra_->C->output()->frame_size_in_bytes() - 2*wordSize;\n+      __ remove_frame(initial_framesize, false);\n+    }\n+    \/\/ Unpack inline type args passed as oop and then jump to\n+    \/\/ the verified entry point (skipping the unverified entry).\n+    int sp_inc = __ unpack_inline_args(ra_->C, _receiver_only);\n+    \/\/ Emit code for verified entry and save increment for stack repair on return\n+    __ verified_entry(ra_->C, sp_inc);\n+    if (Compile::current()->output()->in_scratch_emit_size()) {\n+      Label dummy_verified_entry;\n+      __ jmp(dummy_verified_entry);\n+    } else {\n+      __ jmp(*_verified_entry);\n+    }\n+  }\n+  \/* WARNING these NOPs are critical so that verified entry point is properly\n+     4 bytes aligned for patching by NativeJump::patch_verified_entry() *\/\n+  int nops_cnt = 4 - ((cbuf->insts_size() - insts_size) & 0x3);\n+  nops_cnt &= 0x3; \/\/ Do not add nops if code is aligned.\n+  if (nops_cnt > 0) {\n+    __ nop(nops_cnt);\n+  }\n+}\n+\n@@ -1589,7 +1605,0 @@\n-uint MachUEPNode::size(PhaseRegAlloc* ra_) const\n-{\n-  return MachNode::size(ra_); \/\/ too many variables; just compute it\n-                              \/\/ the hard way\n-}\n-\n-\n@@ -3070,0 +3079,16 @@\n+\/\/ Indirect Narrow Oop Operand\n+operand indCompressedOop(rRegN reg) %{\n+  predicate(UseCompressedOops && (CompressedOops::shift() == Address::times_8));\n+  constraint(ALLOC_IN_RC(ptr_reg));\n+  match(DecodeN reg);\n+\n+  op_cost(10);\n+  format %{\"[R12 + $reg << 3] (compressed oop addressing)\" %}\n+  interface(MEMORY_INTER) %{\n+    base(0xc); \/\/ R12\n+    index($reg);\n+    scale(0x3);\n+    disp(0x0);\n+  %}\n+%}\n+\n@@ -3416,1 +3441,1 @@\n-               indCompressedOopOffset,\n+               indCompressedOop, indCompressedOopOffset,\n@@ -5957,0 +5982,26 @@\n+instruct castI2N(rRegN dst, rRegI src)\n+%{\n+  match(Set dst (CastI2N src));\n+\n+  format %{ \"movq    $dst, $src\\t# int -> narrow ptr\" %}\n+  ins_encode %{\n+    if ($dst$$reg != $src$$reg) {\n+      __ movl($dst$$Register, $src$$Register);\n+    }\n+  %}\n+  ins_pipe(ialu_reg_reg); \/\/ XXX\n+%}\n+\n+instruct castN2X(rRegL dst, rRegN src)\n+%{\n+  match(Set dst (CastP2X src));\n+\n+  format %{ \"movq    $dst, $src\\t# ptr -> long\" %}\n+  ins_encode %{\n+    if ($dst$$reg != $src$$reg) {\n+      __ movptr($dst$$Register, $src$$Register);\n+    }\n+  %}\n+  ins_pipe(ialu_reg_reg); \/\/ XXX\n+%}\n+\n@@ -12088,0 +12139,1 @@\n+\n@@ -12090,1 +12142,1 @@\n-instruct rep_stos(rcx_RegL cnt, rdi_RegP base, regD tmp, rax_RegI zero,\n+instruct rep_stos(rcx_RegL cnt, rdi_RegP base, regD tmp, rax_RegL val,\n@@ -12093,3 +12145,120 @@\n-  predicate(!((ClearArrayNode*)n)->is_large() && (UseAVX <= 2));\n-  match(Set dummy (ClearArray cnt base));\n-  effect(USE_KILL cnt, USE_KILL base, TEMP tmp, KILL zero, KILL cr);\n+  predicate(!((ClearArrayNode*)n)->is_large() && !((ClearArrayNode*)n)->word_copy_only() && (UseAVX <= 2));\n+  match(Set dummy (ClearArray (Binary cnt base) val));\n+  effect(USE_KILL cnt, USE_KILL base, TEMP tmp, USE_KILL val, KILL cr);\n+\n+  format %{ $$template\n+    $$emit$$\"cmp     InitArrayShortSize,rcx\\n\\t\"\n+    $$emit$$\"jg      LARGE\\n\\t\"\n+    $$emit$$\"dec     rcx\\n\\t\"\n+    $$emit$$\"js      DONE\\t# Zero length\\n\\t\"\n+    $$emit$$\"mov     rax,(rdi,rcx,8)\\t# LOOP\\n\\t\"\n+    $$emit$$\"dec     rcx\\n\\t\"\n+    $$emit$$\"jge     LOOP\\n\\t\"\n+    $$emit$$\"jmp     DONE\\n\\t\"\n+    $$emit$$\"# LARGE:\\n\\t\"\n+    if (UseFastStosb) {\n+       $$emit$$\"shlq    rcx,3\\t# Convert doublewords to bytes\\n\\t\"\n+       $$emit$$\"rep     stosb\\t# Store rax to *rdi++ while rcx--\\n\\t\"\n+    } else if (UseXMMForObjInit) {\n+       $$emit$$\"movdq   $tmp, $val\\n\\t\"\n+       $$emit$$\"punpcklqdq $tmp, $tmp\\n\\t\"\n+       $$emit$$\"vinserti128_high $tmp, $tmp\\n\\t\"\n+       $$emit$$\"jmpq    L_zero_64_bytes\\n\\t\"\n+       $$emit$$\"# L_loop:\\t# 64-byte LOOP\\n\\t\"\n+       $$emit$$\"vmovdqu $tmp,(rax)\\n\\t\"\n+       $$emit$$\"vmovdqu $tmp,0x20(rax)\\n\\t\"\n+       $$emit$$\"add     0x40,rax\\n\\t\"\n+       $$emit$$\"# L_zero_64_bytes:\\n\\t\"\n+       $$emit$$\"sub     0x8,rcx\\n\\t\"\n+       $$emit$$\"jge     L_loop\\n\\t\"\n+       $$emit$$\"add     0x4,rcx\\n\\t\"\n+       $$emit$$\"jl      L_tail\\n\\t\"\n+       $$emit$$\"vmovdqu $tmp,(rax)\\n\\t\"\n+       $$emit$$\"add     0x20,rax\\n\\t\"\n+       $$emit$$\"sub     0x4,rcx\\n\\t\"\n+       $$emit$$\"# L_tail:\\t# Clearing tail bytes\\n\\t\"\n+       $$emit$$\"add     0x4,rcx\\n\\t\"\n+       $$emit$$\"jle     L_end\\n\\t\"\n+       $$emit$$\"dec     rcx\\n\\t\"\n+       $$emit$$\"# L_sloop:\\t# 8-byte short loop\\n\\t\"\n+       $$emit$$\"vmovq   xmm0,(rax)\\n\\t\"\n+       $$emit$$\"add     0x8,rax\\n\\t\"\n+       $$emit$$\"dec     rcx\\n\\t\"\n+       $$emit$$\"jge     L_sloop\\n\\t\"\n+       $$emit$$\"# L_end:\\n\\t\"\n+    } else {\n+       $$emit$$\"rep     stosq\\t# Store rax to *rdi++ while rcx--\\n\\t\"\n+    }\n+    $$emit$$\"# DONE\"\n+  %}\n+  ins_encode %{\n+    __ clear_mem($base$$Register, $cnt$$Register, $val$$Register,\n+                 $tmp$$XMMRegister, false, false);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct rep_stos_word_copy(rcx_RegL cnt, rdi_RegP base, regD tmp, rax_RegL val,\n+                            Universe dummy, rFlagsReg cr)\n+%{\n+  predicate(!((ClearArrayNode*)n)->is_large() && ((ClearArrayNode*)n)->word_copy_only() && (UseAVX <= 2));\n+  match(Set dummy (ClearArray (Binary cnt base) val));\n+  effect(USE_KILL cnt, USE_KILL base, TEMP tmp, USE_KILL val, KILL cr);\n+\n+  format %{ $$template\n+    $$emit$$\"cmp     InitArrayShortSize,rcx\\n\\t\"\n+    $$emit$$\"jg      LARGE\\n\\t\"\n+    $$emit$$\"dec     rcx\\n\\t\"\n+    $$emit$$\"js      DONE\\t# Zero length\\n\\t\"\n+    $$emit$$\"mov     rax,(rdi,rcx,8)\\t# LOOP\\n\\t\"\n+    $$emit$$\"dec     rcx\\n\\t\"\n+    $$emit$$\"jge     LOOP\\n\\t\"\n+    $$emit$$\"jmp     DONE\\n\\t\"\n+    $$emit$$\"# LARGE:\\n\\t\"\n+    if (UseXMMForObjInit) {\n+       $$emit$$\"movdq   $tmp, $val\\n\\t\"\n+       $$emit$$\"punpcklqdq $tmp, $tmp\\n\\t\"\n+       $$emit$$\"vinserti128_high $tmp, $tmp\\n\\t\"\n+       $$emit$$\"jmpq    L_zero_64_bytes\\n\\t\"\n+       $$emit$$\"# L_loop:\\t# 64-byte LOOP\\n\\t\"\n+       $$emit$$\"vmovdqu $tmp,(rax)\\n\\t\"\n+       $$emit$$\"vmovdqu $tmp,0x20(rax)\\n\\t\"\n+       $$emit$$\"add     0x40,rax\\n\\t\"\n+       $$emit$$\"# L_zero_64_bytes:\\n\\t\"\n+       $$emit$$\"sub     0x8,rcx\\n\\t\"\n+       $$emit$$\"jge     L_loop\\n\\t\"\n+       $$emit$$\"add     0x4,rcx\\n\\t\"\n+       $$emit$$\"jl      L_tail\\n\\t\"\n+       $$emit$$\"vmovdqu $tmp,(rax)\\n\\t\"\n+       $$emit$$\"add     0x20,rax\\n\\t\"\n+       $$emit$$\"sub     0x4,rcx\\n\\t\"\n+       $$emit$$\"# L_tail:\\t# Clearing tail bytes\\n\\t\"\n+       $$emit$$\"add     0x4,rcx\\n\\t\"\n+       $$emit$$\"jle     L_end\\n\\t\"\n+       $$emit$$\"dec     rcx\\n\\t\"\n+       $$emit$$\"# L_sloop:\\t# 8-byte short loop\\n\\t\"\n+       $$emit$$\"vmovq   xmm0,(rax)\\n\\t\"\n+       $$emit$$\"add     0x8,rax\\n\\t\"\n+       $$emit$$\"dec     rcx\\n\\t\"\n+       $$emit$$\"jge     L_sloop\\n\\t\"\n+       $$emit$$\"# L_end:\\n\\t\"\n+    } else {\n+       $$emit$$\"rep     stosq\\t# Store rax to *rdi++ while rcx--\\n\\t\"\n+    }\n+    $$emit$$\"# DONE\"\n+  %}\n+  ins_encode %{\n+    __ clear_mem($base$$Register, $cnt$$Register, $val$$Register,\n+                 $tmp$$XMMRegister, false, true);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ Small non-constant length ClearArray for AVX512 targets.\n+instruct rep_stos_evex(rcx_RegL cnt, rdi_RegP base, legRegD tmp, kReg ktmp, rax_RegL val,\n+                       Universe dummy, rFlagsReg cr)\n+%{\n+  predicate(!((ClearArrayNode*)n)->is_large() && !((ClearArrayNode*)n)->word_copy_only() && (UseAVX > 2));\n+  match(Set dummy (ClearArray (Binary cnt base) val));\n+  ins_cost(125);\n+  effect(USE_KILL cnt, USE_KILL base, TEMP tmp, TEMP ktmp, USE_KILL val, KILL cr);\n@@ -12143,2 +12312,2 @@\n-    __ clear_mem($base$$Register, $cnt$$Register, $zero$$Register,\n-                 $tmp$$XMMRegister, false, knoreg);\n+    __ clear_mem($base$$Register, $cnt$$Register, $val$$Register,\n+                 $tmp$$XMMRegister, false, false, $ktmp$$KRegister);\n@@ -12149,3 +12318,2 @@\n-\/\/ Small non-constant length ClearArray for AVX512 targets.\n-instruct rep_stos_evex(rcx_RegL cnt, rdi_RegP base, legRegD tmp, kReg ktmp, rax_RegI zero,\n-                       Universe dummy, rFlagsReg cr)\n+instruct rep_stos_evex_word_copy(rcx_RegL cnt, rdi_RegP base, legRegD tmp, kReg ktmp, rax_RegL val,\n+                                 Universe dummy, rFlagsReg cr)\n@@ -12153,2 +12321,2 @@\n-  predicate(!((ClearArrayNode*)n)->is_large() && (UseAVX > 2));\n-  match(Set dummy (ClearArray cnt base));\n+  predicate(!((ClearArrayNode*)n)->is_large() && ((ClearArrayNode*)n)->word_copy_only() && (UseAVX > 2));\n+  match(Set dummy (ClearArray (Binary cnt base) val));\n@@ -12156,1 +12324,1 @@\n-  effect(USE_KILL cnt, USE_KILL base, TEMP tmp, TEMP ktmp, KILL zero, KILL cr);\n+  effect(USE_KILL cnt, USE_KILL base, TEMP tmp, TEMP ktmp, USE_KILL val, KILL cr);\n@@ -12204,2 +12372,2 @@\n-    __ clear_mem($base$$Register, $cnt$$Register, $zero$$Register,\n-                 $tmp$$XMMRegister, false, $ktmp$$KRegister);\n+    __ clear_mem($base$$Register, $cnt$$Register, $val$$Register,\n+                 $tmp$$XMMRegister, false, true, $ktmp$$KRegister);\n@@ -12211,1 +12379,1 @@\n-instruct rep_stos_large(rcx_RegL cnt, rdi_RegP base, regD tmp, rax_RegI zero,\n+instruct rep_stos_large(rcx_RegL cnt, rdi_RegP base, regD tmp, rax_RegL val,\n@@ -12214,3 +12382,99 @@\n-  predicate((UseAVX <=2) && ((ClearArrayNode*)n)->is_large());\n-  match(Set dummy (ClearArray cnt base));\n-  effect(USE_KILL cnt, USE_KILL base, TEMP tmp, KILL zero, KILL cr);\n+  predicate(((ClearArrayNode*)n)->is_large() && !((ClearArrayNode*)n)->word_copy_only() && (UseAVX <= 2));\n+  match(Set dummy (ClearArray (Binary cnt base) val));\n+  effect(USE_KILL cnt, USE_KILL base, TEMP tmp, USE_KILL val, KILL cr);\n+\n+  format %{ $$template\n+    if (UseFastStosb) {\n+       $$emit$$\"shlq    rcx,3\\t# Convert doublewords to bytes\\n\\t\"\n+       $$emit$$\"rep     stosb\\t# Store rax to *rdi++ while rcx--\"\n+    } else if (UseXMMForObjInit) {\n+       $$emit$$\"movdq   $tmp, $val\\n\\t\"\n+       $$emit$$\"punpcklqdq $tmp, $tmp\\n\\t\"\n+       $$emit$$\"vinserti128_high $tmp, $tmp\\n\\t\"\n+       $$emit$$\"jmpq    L_zero_64_bytes\\n\\t\"\n+       $$emit$$\"# L_loop:\\t# 64-byte LOOP\\n\\t\"\n+       $$emit$$\"vmovdqu $tmp,(rax)\\n\\t\"\n+       $$emit$$\"vmovdqu $tmp,0x20(rax)\\n\\t\"\n+       $$emit$$\"add     0x40,rax\\n\\t\"\n+       $$emit$$\"# L_zero_64_bytes:\\n\\t\"\n+       $$emit$$\"sub     0x8,rcx\\n\\t\"\n+       $$emit$$\"jge     L_loop\\n\\t\"\n+       $$emit$$\"add     0x4,rcx\\n\\t\"\n+       $$emit$$\"jl      L_tail\\n\\t\"\n+       $$emit$$\"vmovdqu $tmp,(rax)\\n\\t\"\n+       $$emit$$\"add     0x20,rax\\n\\t\"\n+       $$emit$$\"sub     0x4,rcx\\n\\t\"\n+       $$emit$$\"# L_tail:\\t# Clearing tail bytes\\n\\t\"\n+       $$emit$$\"add     0x4,rcx\\n\\t\"\n+       $$emit$$\"jle     L_end\\n\\t\"\n+       $$emit$$\"dec     rcx\\n\\t\"\n+       $$emit$$\"# L_sloop:\\t# 8-byte short loop\\n\\t\"\n+       $$emit$$\"vmovq   xmm0,(rax)\\n\\t\"\n+       $$emit$$\"add     0x8,rax\\n\\t\"\n+       $$emit$$\"dec     rcx\\n\\t\"\n+       $$emit$$\"jge     L_sloop\\n\\t\"\n+       $$emit$$\"# L_end:\\n\\t\"\n+    } else {\n+       $$emit$$\"rep     stosq\\t# Store rax to *rdi++ while rcx--\"\n+    }\n+  %}\n+  ins_encode %{\n+    __ clear_mem($base$$Register, $cnt$$Register, $val$$Register,\n+                 $tmp$$XMMRegister, true, false);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct rep_stos_large_word_copy(rcx_RegL cnt, rdi_RegP base, regD tmp, rax_RegL val,\n+                                  Universe dummy, rFlagsReg cr)\n+%{\n+  predicate(((ClearArrayNode*)n)->is_large() && ((ClearArrayNode*)n)->word_copy_only() && (UseAVX <= 2));\n+  match(Set dummy (ClearArray (Binary cnt base) val));\n+  effect(USE_KILL cnt, USE_KILL base, TEMP tmp, USE_KILL val, KILL cr);\n+\n+  format %{ $$template\n+    if (UseXMMForObjInit) {\n+       $$emit$$\"movdq   $tmp, $val\\n\\t\"\n+       $$emit$$\"punpcklqdq $tmp, $tmp\\n\\t\"\n+       $$emit$$\"vinserti128_high $tmp, $tmp\\n\\t\"\n+       $$emit$$\"jmpq    L_zero_64_bytes\\n\\t\"\n+       $$emit$$\"# L_loop:\\t# 64-byte LOOP\\n\\t\"\n+       $$emit$$\"vmovdqu $tmp,(rax)\\n\\t\"\n+       $$emit$$\"vmovdqu $tmp,0x20(rax)\\n\\t\"\n+       $$emit$$\"add     0x40,rax\\n\\t\"\n+       $$emit$$\"# L_zero_64_bytes:\\n\\t\"\n+       $$emit$$\"sub     0x8,rcx\\n\\t\"\n+       $$emit$$\"jge     L_loop\\n\\t\"\n+       $$emit$$\"add     0x4,rcx\\n\\t\"\n+       $$emit$$\"jl      L_tail\\n\\t\"\n+       $$emit$$\"vmovdqu $tmp,(rax)\\n\\t\"\n+       $$emit$$\"add     0x20,rax\\n\\t\"\n+       $$emit$$\"sub     0x4,rcx\\n\\t\"\n+       $$emit$$\"# L_tail:\\t# Clearing tail bytes\\n\\t\"\n+       $$emit$$\"add     0x4,rcx\\n\\t\"\n+       $$emit$$\"jle     L_end\\n\\t\"\n+       $$emit$$\"dec     rcx\\n\\t\"\n+       $$emit$$\"# L_sloop:\\t# 8-byte short loop\\n\\t\"\n+       $$emit$$\"vmovq   xmm0,(rax)\\n\\t\"\n+       $$emit$$\"add     0x8,rax\\n\\t\"\n+       $$emit$$\"dec     rcx\\n\\t\"\n+       $$emit$$\"jge     L_sloop\\n\\t\"\n+       $$emit$$\"# L_end:\\n\\t\"\n+    } else {\n+       $$emit$$\"rep     stosq\\t# Store rax to *rdi++ while rcx--\"\n+    }\n+  %}\n+  ins_encode %{\n+    __ clear_mem($base$$Register, $cnt$$Register, $val$$Register,\n+                 $tmp$$XMMRegister, true, true);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ Large non-constant length ClearArray for AVX512 targets.\n+instruct rep_stos_large_evex(rcx_RegL cnt, rdi_RegP base, legRegD tmp, kReg ktmp, rax_RegL val,\n+                             Universe dummy, rFlagsReg cr)\n+%{\n+  predicate(((ClearArrayNode*)n)->is_large() && !((ClearArrayNode*)n)->word_copy_only() && (UseAVX > 2));\n+  match(Set dummy (ClearArray (Binary cnt base) val));\n+  effect(USE_KILL cnt, USE_KILL base, TEMP tmp, TEMP ktmp, USE_KILL val, KILL cr);\n@@ -12255,2 +12519,2 @@\n-    __ clear_mem($base$$Register, $cnt$$Register, $zero$$Register,\n-                 $tmp$$XMMRegister, true, knoreg);\n+    __ clear_mem($base$$Register, $cnt$$Register, $val$$Register,\n+                 $tmp$$XMMRegister, true, false, $ktmp$$KRegister);\n@@ -12261,3 +12525,2 @@\n-\/\/ Large non-constant length ClearArray for AVX512 targets.\n-instruct rep_stos_large_evex(rcx_RegL cnt, rdi_RegP base, legRegD tmp, kReg ktmp, rax_RegI zero,\n-                             Universe dummy, rFlagsReg cr)\n+instruct rep_stos_large_evex_word_copy(rcx_RegL cnt, rdi_RegP base, legRegD tmp, kReg ktmp, rax_RegL val,\n+                                       Universe dummy, rFlagsReg cr)\n@@ -12265,3 +12528,3 @@\n-  predicate((UseAVX > 2) && ((ClearArrayNode*)n)->is_large());\n-  match(Set dummy (ClearArray cnt base));\n-  effect(USE_KILL cnt, USE_KILL base, TEMP tmp, TEMP ktmp, KILL zero, KILL cr);\n+  predicate(((ClearArrayNode*)n)->is_large() && ((ClearArrayNode*)n)->word_copy_only() && (UseAVX > 2));\n+  match(Set dummy (ClearArray (Binary cnt base) val));\n+  effect(USE_KILL cnt, USE_KILL base, TEMP tmp, TEMP ktmp, USE_KILL val, KILL cr);\n@@ -12306,2 +12569,2 @@\n-    __ clear_mem($base$$Register, $cnt$$Register, $zero$$Register,\n-                 $tmp$$XMMRegister, true, $ktmp$$KRegister);\n+    __ clear_mem($base$$Register, $cnt$$Register, $val$$Register,\n+                 $tmp$$XMMRegister, true, true, $ktmp$$KRegister);\n@@ -12313,1 +12576,1 @@\n-instruct rep_stos_im(immL cnt, rRegP base, regD tmp, rRegI zero, kReg ktmp, Universe dummy, rFlagsReg cr)\n+instruct rep_stos_im(immL cnt, rRegP base, regD tmp, rax_RegL val, kReg ktmp, Universe dummy, rFlagsReg cr)\n@@ -12315,2 +12578,3 @@\n-  predicate(!((ClearArrayNode*)n)->is_large() && (MaxVectorSize >= 32) && VM_Version::supports_avx512vl());\n-  match(Set dummy (ClearArray cnt base));\n+  predicate(!((ClearArrayNode*)n)->is_large() && !((ClearArrayNode*)n)->word_copy_only() &&\n+            ((MaxVectorSize >= 32) && VM_Version::supports_avx512vl()));\n+  match(Set dummy (ClearArray (Binary cnt base) val));\n@@ -12318,1 +12582,1 @@\n-  effect(TEMP tmp, TEMP zero, TEMP ktmp, KILL cr);\n+  effect(TEMP tmp, USE_KILL val, TEMP ktmp, KILL cr);\n@@ -12321,1 +12585,1 @@\n-   __ clear_mem($base$$Register, $cnt$$constant, $zero$$Register, $tmp$$XMMRegister, $ktmp$$KRegister);\n+    __ clear_mem($base$$Register, $cnt$$constant, $val$$Register, $tmp$$XMMRegister, $ktmp$$KRegister);\n@@ -14194,0 +14458,15 @@\n+\/\/ entry point is null, target holds the address to call\n+instruct CallLeafNoFPInDirect(rRegP target)\n+%{\n+  predicate(n->as_Call()->entry_point() == nullptr);\n+  match(CallLeafNoFP target);\n+\n+  ins_cost(300);\n+  format %{ \"call_leaf_nofp,runtime indirect \" %}\n+  ins_encode %{\n+     __ call($target$$Register);\n+  %}\n+\n+  ins_pipe(pipe_slow);\n+%}\n+\n@@ -14196,0 +14475,1 @@\n+  predicate(n->as_Call()->entry_point() != nullptr);\n","filename":"src\/hotspot\/cpu\/x86\/x86_64.ad","additions":362,"deletions":82,"binary":false,"changes":444,"status":"modified"},{"patch":"@@ -589,0 +589,1 @@\n+, _compiled_entry_signature(method->get_Method())\n@@ -606,1 +607,0 @@\n-\n@@ -609,0 +609,6 @@\n+  {\n+    ResetNoHandleMark rnhm; \/\/ Huh? Required when doing class lookup of the Q-types\n+    \/\/ TODO 8284443 Should only be computed once\n+    _compiled_entry_signature.compute_calling_conventions(false);\n+  }\n+\n","filename":"src\/hotspot\/share\/c1\/c1_Compilation.cpp","additions":7,"deletions":1,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -35,0 +35,1 @@\n+#include \"runtime\/sharedRuntime.hpp\"\n@@ -98,0 +99,1 @@\n+  CompiledEntrySignature _compiled_entry_signature;\n@@ -260,0 +262,4 @@\n+  bool profile_array_accesses() {\n+    return env()->comp_level() == CompLevel_full_profile &&\n+      C1UpdateMethodData;\n+  }\n@@ -285,0 +291,7 @@\n+\n+  const CompiledEntrySignature* compiled_entry_signature() const {\n+    return &_compiled_entry_signature;\n+  }\n+  bool needs_stack_repair() const {\n+    return compiled_entry_signature()->c1_needs_stack_repair();\n+  }\n","filename":"src\/hotspot\/share\/c1\/c1_Compilation.hpp","additions":13,"deletions":0,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -32,0 +32,2 @@\n+#include \"ci\/ciFlatArrayKlass.hpp\"\n+#include \"ci\/ciInlineKlass.hpp\"\n@@ -1050,1 +1052,9 @@\n-  ValueStack* state_before = copy_state_indexed_access();\n+  ValueStack* state_before = nullptr;\n+  int array_idx = state()->stack_size() - 2;\n+  if (type == T_OBJECT && state()->stack_at(array_idx)->maybe_flat_array()) {\n+    \/\/ Save the entire state and re-execute on deopt when accessing flat arrays\n+    state_before = copy_state_before();\n+    state_before->set_should_reexecute(true);\n+  } else {\n+    state_before = copy_state_indexed_access();\n+  }\n@@ -1062,1 +1072,56 @@\n-  push(as_ValueType(type), append(new LoadIndexed(array, index, length, type, state_before)));\n+\n+  bool need_membar = false;\n+  LoadIndexed* load_indexed = nullptr;\n+  Instruction* result = nullptr;\n+  if (array->is_loaded_flat_array()) {\n+    \/\/ TODO 8350865 This is currently dead code. Can we use set_null_free on the result here if the array is null-free?\n+    ciType* array_type = array->declared_type();\n+    ciInlineKlass* elem_klass = array_type->as_flat_array_klass()->element_klass()->as_inline_klass();\n+\n+    bool can_delay_access = false;\n+    ciBytecodeStream s(method());\n+    s.force_bci(bci());\n+    s.next();\n+    if (s.cur_bc() == Bytecodes::_getfield) {\n+      bool will_link;\n+      ciField* next_field = s.get_field(will_link);\n+      bool next_needs_patching = !next_field->holder()->is_initialized() ||\n+                                 !next_field->will_link(method(), Bytecodes::_getfield) ||\n+                                 PatchALot;\n+      can_delay_access = C1UseDelayedFlattenedFieldReads && !next_needs_patching;\n+    }\n+    if (can_delay_access) {\n+      \/\/ potentially optimizable array access, storing information for delayed decision\n+      LoadIndexed* li = new LoadIndexed(array, index, length, type, state_before);\n+      DelayedLoadIndexed* dli = new DelayedLoadIndexed(li, state_before);\n+      li->set_delayed(dli);\n+      set_pending_load_indexed(dli);\n+      return; \/\/ Nothing else to do for now\n+    } else {\n+      NewInstance* new_instance = new NewInstance(elem_klass, state_before, false, true);\n+      _memory->new_instance(new_instance);\n+      apush(append_split(new_instance));\n+      load_indexed = new LoadIndexed(array, index, length, type, state_before);\n+      load_indexed->set_vt(new_instance);\n+      \/\/ The LoadIndexed node will initialise this instance by copying from\n+      \/\/ the flat field.  Ensure these stores are visible before any\n+      \/\/ subsequent store that publishes this reference.\n+      need_membar = true;\n+    }\n+  } else {\n+    load_indexed = new LoadIndexed(array, index, length, type, state_before);\n+    if (profile_array_accesses() && is_reference_type(type)) {\n+      compilation()->set_would_profile(true);\n+      load_indexed->set_should_profile(true);\n+      load_indexed->set_profiled_method(method());\n+      load_indexed->set_profiled_bci(bci());\n+    }\n+  }\n+  result = append(load_indexed);\n+  if (need_membar) {\n+    append(new MemBar(lir_membar_storestore));\n+  }\n+  assert(!load_indexed->should_profile() || load_indexed == result, \"should not be optimized out\");\n+  if (!array->is_loaded_flat_array()) {\n+    push(as_ValueType(type), result);\n+  }\n@@ -1068,1 +1133,9 @@\n-  ValueStack* state_before = copy_state_indexed_access();\n+  ValueStack* state_before = nullptr;\n+  int array_idx = state()->stack_size() - 3;\n+  if (type == T_OBJECT && state()->stack_at(array_idx)->maybe_flat_array()) {\n+    \/\/ Save the entire state and re-execute on deopt when accessing flat arrays\n+    state_before = copy_state_before();\n+    state_before->set_should_reexecute(true);\n+  } else {\n+    state_before = copy_state_indexed_access();\n+  }\n@@ -1093,5 +1166,2 @@\n-  StoreIndexed* result = new StoreIndexed(array, index, length, type, value, state_before, check_boolean);\n-  append(result);\n-  _memory->store_value(value);\n-  if (type == T_OBJECT && is_profiling()) {\n-    \/\/ Note that we'd collect profile data in this method if we wanted it.\n+  StoreIndexed* store_indexed = new StoreIndexed(array, index, length, type, value, state_before, check_boolean);\n+  if (profile_array_accesses() && is_reference_type(type) && !array->is_loaded_flat_array()) {\n@@ -1100,6 +1170,3 @@\n-\n-    if (profile_checkcasts()) {\n-      result->set_profiled_method(method());\n-      result->set_profiled_bci(bci());\n-      result->set_should_profile(true);\n-    }\n+    store_indexed->set_should_profile(true);\n+    store_indexed->set_profiled_method(method());\n+    store_indexed->set_profiled_bci(bci());\n@@ -1107,0 +1174,3 @@\n+  Instruction* result = append(store_indexed);\n+  assert(!store_indexed->should_profile() || store_indexed == result, \"should not be optimized out\");\n+  _memory->store_value(value);\n@@ -1109,1 +1179,0 @@\n-\n@@ -1113,1 +1182,1 @@\n-      { state()->raw_pop();\n+      { Value w = state()->raw_pop();\n@@ -1117,2 +1186,2 @@\n-      { state()->raw_pop();\n-        state()->raw_pop();\n+      { Value w1 = state()->raw_pop();\n+        Value w2 = state()->raw_pop();\n@@ -1294,0 +1363,27 @@\n+\n+  bool subst_check = false;\n+  if (EnableValhalla && (stream()->cur_bc() == Bytecodes::_if_acmpeq || stream()->cur_bc() == Bytecodes::_if_acmpne)) {\n+    ValueType* left_vt = x->type();\n+    ValueType* right_vt = y->type();\n+    if (left_vt->is_object()) {\n+      assert(right_vt->is_object(), \"must be\");\n+      ciKlass* left_klass = x->as_loaded_klass_or_null();\n+      ciKlass* right_klass = y->as_loaded_klass_or_null();\n+\n+      if (left_klass == nullptr || right_klass == nullptr) {\n+        \/\/ The klass is still unloaded, or came from a Phi node. Go slow case;\n+        subst_check = true;\n+      } else if (left_klass->can_be_inline_klass() || right_klass->can_be_inline_klass()) {\n+        \/\/ Either operand may be a value object, but we're not sure. Go slow case;\n+        subst_check = true;\n+      } else {\n+        \/\/ No need to do substitutability check\n+      }\n+    }\n+  }\n+  if ((stream()->cur_bc() == Bytecodes::_if_acmpeq || stream()->cur_bc() == Bytecodes::_if_acmpne) &&\n+      is_profiling() && profile_branches()) {\n+    compilation()->set_would_profile(true);\n+    append(new ProfileACmpTypes(method(), bci(), x, y));\n+  }\n+\n@@ -1296,1 +1392,1 @@\n-  Instruction *i = append(new If(x, cond, false, y, tsux, fsux, (is_bb || compilation()->is_optimistic()) ? state_before : nullptr, is_bb));\n+  Instruction *i = append(new If(x, cond, false, y, tsux, fsux, (is_bb || compilation()->is_optimistic() || subst_check) ? state_before : nullptr, is_bb, subst_check));\n@@ -1551,1 +1647,1 @@\n-  if (method()->name() == ciSymbols::object_initializer_name() &&\n+  if (method()->is_object_constructor() &&\n@@ -1702,0 +1798,23 @@\n+void GraphBuilder::copy_inline_content(ciInlineKlass* vk, Value src, int src_off, Value dest, int dest_off, ValueStack* state_before, ciField* enclosing_field) {\n+  for (int i = 0; i < vk->nof_declared_nonstatic_fields(); i++) {\n+    ciField* field = vk->declared_nonstatic_field_at(i);\n+    int offset = field->offset_in_bytes() - vk->payload_offset();\n+    if (field->is_flat()) {\n+      copy_inline_content(field->type()->as_inline_klass(), src, src_off + offset, dest, dest_off + offset, state_before, enclosing_field);\n+      if (!field->is_null_free()) {\n+        \/\/ Nullable, copy the null marker using Unsafe because null markers are no real fields\n+        int null_marker_offset = field->null_marker_offset() - vk->payload_offset();\n+        Value offset = append(new Constant(new LongConstant(src_off + null_marker_offset)));\n+        Value nm = append(new UnsafeGet(T_BOOLEAN, src, offset, false));\n+        offset = append(new Constant(new LongConstant(dest_off + null_marker_offset)));\n+        append(new UnsafePut(T_BOOLEAN, dest, offset, nm, false));\n+      }\n+    } else {\n+      Value value = append(new LoadField(src, src_off + offset, field, false, state_before, false));\n+      StoreField* store = new StoreField(dest, dest_off + offset, field, value, false, state_before, false);\n+      store->set_enclosing_field(enclosing_field);\n+      append(store);\n+    }\n+  }\n+}\n+\n@@ -1708,0 +1827,1 @@\n+\n@@ -1711,1 +1831,1 @@\n-                              PatchALot;\n+                              (!field->is_flat() && PatchALot);\n@@ -1743,1 +1863,1 @@\n-  const int offset = !needs_patching ? field->offset_in_bytes() : -1;\n+  int offset = !needs_patching ? field->offset_in_bytes() : -1;\n@@ -1760,2 +1880,3 @@\n-        push(type, append(new LoadField(append(obj), offset, field, true,\n-                                        state_before, needs_patching)));\n+        LoadField* load_field = new LoadField(append(obj), offset, field, true,\n+                                        state_before, needs_patching);\n+        push(type, append(load_field));\n@@ -1770,1 +1891,1 @@\n-      if (field->type()->basic_type() == T_BOOLEAN) {\n+      if (field_type == T_BOOLEAN) {\n@@ -1774,0 +1895,7 @@\n+      if (field->is_null_free()) {\n+        null_check(val);\n+      }\n+      if (field->is_null_free() && field->type()->is_loaded() && field->type()->as_inline_klass()->is_empty() && (!method()->is_class_initializer() || field->is_flat())) {\n+        \/\/ Storing to a field of an empty, null-free inline type that is already initialized. Ignore.\n+        break;\n+      }\n@@ -1780,14 +1908,21 @@\n-      obj = apop();\n-      ObjectType* obj_type = obj->type()->as_ObjectType();\n-      if (field->is_constant() && obj_type->is_constant() && !PatchALot) {\n-        ciObject* const_oop = obj_type->constant_value();\n-        if (!const_oop->is_null_object() && const_oop->is_loaded()) {\n-          ciConstant field_value = field->constant_value_of(const_oop);\n-          if (field_value.is_valid()) {\n-            constant = make_constant(field_value, field);\n-            \/\/ For CallSite objects add a dependency for invalidation of the optimization.\n-            if (field->is_call_site_target()) {\n-              ciCallSite* call_site = const_oop->as_call_site();\n-              if (!call_site->is_fully_initialized_constant_call_site()) {\n-                ciMethodHandle* target = field_value.as_object()->as_method_handle();\n-                dependency_recorder()->assert_call_site_target_value(call_site, target);\n+      if (state_before == nullptr && field->is_flat()) {\n+        \/\/ Save the entire state and re-execute on deopt when accessing flat fields\n+        assert(Interpreter::bytecode_should_reexecute(code), \"should reexecute\");\n+        state_before = copy_state_before();\n+      }\n+      if (!has_pending_field_access() && !has_pending_load_indexed()) {\n+        obj = apop();\n+        ObjectType* obj_type = obj->type()->as_ObjectType();\n+        if (field->is_constant() && !field->is_flat() && obj_type->is_constant() && !PatchALot) {\n+          ciObject* const_oop = obj_type->constant_value();\n+          if (!const_oop->is_null_object() && const_oop->is_loaded()) {\n+            ciConstant field_value = field->constant_value_of(const_oop);\n+            if (field_value.is_valid()) {\n+              constant = make_constant(field_value, field);\n+              \/\/ For CallSite objects add a dependency for invalidation of the optimization.\n+              if (field->is_call_site_target()) {\n+                ciCallSite* call_site = const_oop->as_call_site();\n+                if (!call_site->is_fully_initialized_constant_call_site()) {\n+                  ciMethodHandle* target = field_value.as_object()->as_method_handle();\n+                  dependency_recorder()->assert_call_site_target_value(call_site, target);\n+                }\n@@ -1805,19 +1940,15 @@\n-        LoadField* load = new LoadField(obj, offset, field, false, state_before, needs_patching);\n-        Value replacement = !needs_patching ? _memory->load(load) : load;\n-        if (replacement != load) {\n-          assert(replacement->is_linked() || !replacement->can_be_linked(), \"should already by linked\");\n-          \/\/ Writing an (integer) value to a boolean, byte, char or short field includes an implicit narrowing\n-          \/\/ conversion. Emit an explicit conversion here to get the correct field value after the write.\n-          BasicType bt = field->type()->basic_type();\n-          switch (bt) {\n-          case T_BOOLEAN:\n-          case T_BYTE:\n-            replacement = append(new Convert(Bytecodes::_i2b, replacement, as_ValueType(bt)));\n-            break;\n-          case T_CHAR:\n-            replacement = append(new Convert(Bytecodes::_i2c, replacement, as_ValueType(bt)));\n-            break;\n-          case T_SHORT:\n-            replacement = append(new Convert(Bytecodes::_i2s, replacement, as_ValueType(bt)));\n-            break;\n-          default:\n+        if (!field->is_flat()) {\n+          if (has_pending_field_access()) {\n+            assert(!needs_patching, \"Can't patch delayed field access\");\n+            obj = pending_field_access()->obj();\n+            offset += pending_field_access()->offset() - field->holder()->as_inline_klass()->payload_offset();\n+            field = pending_field_access()->holder()->get_field_by_offset(offset, false);\n+            assert(field != nullptr, \"field not found\");\n+            set_pending_field_access(nullptr);\n+          } else if (has_pending_load_indexed()) {\n+            assert(!needs_patching, \"Can't patch delayed field access\");\n+            pending_load_indexed()->update(field, offset - field->holder()->as_inline_klass()->payload_offset());\n+            LoadIndexed* li = pending_load_indexed()->load_instr();\n+            li->set_type(type);\n+            push(type, append(li));\n+            set_pending_load_indexed(nullptr);\n@@ -1826,1 +1957,24 @@\n-          push(type, replacement);\n+          LoadField* load = new LoadField(obj, offset, field, false, state_before, needs_patching);\n+          Value replacement = !needs_patching ? _memory->load(load) : load;\n+          if (replacement != load) {\n+            assert(replacement->is_linked() || !replacement->can_be_linked(), \"should already by linked\");\n+            \/\/ Writing an (integer) value to a boolean, byte, char or short field includes an implicit narrowing\n+            \/\/ conversion. Emit an explicit conversion here to get the correct field value after the write.\n+            switch (field_type) {\n+            case T_BOOLEAN:\n+            case T_BYTE:\n+              replacement = append(new Convert(Bytecodes::_i2b, replacement, type));\n+              break;\n+            case T_CHAR:\n+              replacement = append(new Convert(Bytecodes::_i2c, replacement, type));\n+              break;\n+            case T_SHORT:\n+              replacement = append(new Convert(Bytecodes::_i2s, replacement, type));\n+              break;\n+            default:\n+              break;\n+            }\n+            push(type, replacement);\n+          } else {\n+            push(type, append(load));\n+          }\n@@ -1828,1 +1982,78 @@\n-          push(type, append(load));\n+          \/\/ Flat field\n+          assert(!needs_patching, \"Can't patch flat inline type field access\");\n+          ciInlineKlass* inline_klass = field->type()->as_inline_klass();\n+          bool is_naturally_atomic = inline_klass->nof_declared_nonstatic_fields() <= 1;\n+          bool needs_atomic_access = !field->is_null_free() || (field->is_volatile() && !is_naturally_atomic);\n+          if (needs_atomic_access) {\n+            assert(!has_pending_field_access(), \"Pending field accesses are not supported\");\n+            LoadField* load = new LoadField(obj, offset, field, false, state_before, needs_patching);\n+            push(type, append(load));\n+          } else {\n+            assert(field->is_null_free(), \"must be null-free\");\n+            \/\/ Look at the next bytecode to check if we can delay the field access\n+            bool can_delay_access = false;\n+            ciBytecodeStream s(method());\n+            s.force_bci(bci());\n+            s.next();\n+            if (s.cur_bc() == Bytecodes::_getfield && !needs_patching) {\n+              ciField* next_field = s.get_field(will_link);\n+              bool next_needs_patching = !next_field->holder()->is_loaded() ||\n+                                         !next_field->will_link(method(), Bytecodes::_getfield) ||\n+                                         PatchALot;\n+              \/\/ We can't update the offset for atomic accesses\n+              bool next_needs_atomic_access = !next_field->is_null_free() || next_field->is_volatile();\n+              can_delay_access = C1UseDelayedFlattenedFieldReads && !next_needs_patching && !next_needs_atomic_access;\n+            }\n+            if (can_delay_access) {\n+              if (has_pending_load_indexed()) {\n+                pending_load_indexed()->update(field, offset - field->holder()->as_inline_klass()->payload_offset());\n+              } else if (has_pending_field_access()) {\n+                pending_field_access()->inc_offset(offset - field->holder()->as_inline_klass()->payload_offset());\n+              } else {\n+                null_check(obj);\n+                DelayedFieldAccess* dfa = new DelayedFieldAccess(obj, field->holder(), field->offset_in_bytes(), state_before);\n+                set_pending_field_access(dfa);\n+              }\n+            } else {\n+              scope()->set_wrote_final();\n+              scope()->set_wrote_fields();\n+              bool need_membar = false;\n+              if (has_pending_load_indexed()) {\n+                assert(!needs_patching, \"Can't patch delayed field access\");\n+                pending_load_indexed()->update(field, offset - field->holder()->as_inline_klass()->payload_offset());\n+                NewInstance* vt = new NewInstance(inline_klass, pending_load_indexed()->state_before(), false, true);\n+                _memory->new_instance(vt);\n+                pending_load_indexed()->load_instr()->set_vt(vt);\n+                apush(append_split(vt));\n+                append(pending_load_indexed()->load_instr());\n+                set_pending_load_indexed(nullptr);\n+                need_membar = true;\n+              } else {\n+                if (has_pending_field_access()) {\n+                  state_before = pending_field_access()->state_before();\n+                }\n+                NewInstance* new_instance = new NewInstance(inline_klass, state_before, false, true);\n+                _memory->new_instance(new_instance);\n+                apush(append_split(new_instance));\n+                if (has_pending_field_access()) {\n+                  copy_inline_content(inline_klass, pending_field_access()->obj(),\n+                                      pending_field_access()->offset() + field->offset_in_bytes() - field->holder()->as_inline_klass()->payload_offset(),\n+                                      new_instance, inline_klass->payload_offset(), state_before);\n+                  set_pending_field_access(nullptr);\n+                } else {\n+                  if (field->type()->as_instance_klass()->is_initialized() && field->type()->as_inline_klass()->is_empty()) {\n+                    \/\/ Needs an explicit null check because below code does not perform any actual load if there are no fields\n+                    null_check(obj);\n+                  }\n+                  copy_inline_content(inline_klass, obj, field->offset_in_bytes(), new_instance, inline_klass->payload_offset(), state_before);\n+                }\n+                need_membar = true;\n+              }\n+              if (need_membar) {\n+                \/\/ If we allocated a new instance ensure the stores to copy the\n+                \/\/ field contents are visible before any subsequent store that\n+                \/\/ publishes this reference.\n+                append(new MemBar(lir_membar_storestore));\n+              }\n+            }\n+          }\n@@ -1839,1 +2070,1 @@\n-      if (field->type()->basic_type() == T_BOOLEAN) {\n+      if (field_type == T_BOOLEAN) {\n@@ -1843,4 +2074,29 @@\n-      StoreField* store = new StoreField(obj, offset, field, val, false, state_before, needs_patching);\n-      if (!needs_patching) store = _memory->store(store);\n-      if (store != nullptr) {\n-        append(store);\n+\n+      if (field->is_null_free() && field->type()->is_loaded() && field->type()->as_inline_klass()->is_empty() && (!method()->is_object_constructor() || field->is_flat())) {\n+        \/\/ Storing to a field of an empty, null-free inline type that is already initialized. Ignore.\n+        null_check(obj);\n+        null_check(val);\n+      } else if (!field->is_flat()) {\n+        if (field->is_null_free()) {\n+          null_check(val);\n+        }\n+        StoreField* store = new StoreField(obj, offset, field, val, false, state_before, needs_patching);\n+        if (!needs_patching) store = _memory->store(store);\n+        if (store != nullptr) {\n+          append(store);\n+        }\n+      } else {\n+        \/\/ Flat field\n+        assert(!needs_patching, \"Can't patch flat inline type field access\");\n+        ciInlineKlass* inline_klass = field->type()->as_inline_klass();\n+        bool is_naturally_atomic = inline_klass->nof_declared_nonstatic_fields() <= 1;\n+        bool needs_atomic_access = !field->is_null_free() || (field->is_volatile() && !is_naturally_atomic);\n+        if (needs_atomic_access) {\n+          if (field->is_null_free()) {\n+            null_check(val);\n+          }\n+          append(new StoreField(obj, offset, field, val, false, state_before, needs_patching));\n+        } else {\n+          assert(field->is_null_free(), \"must be null-free\");\n+          copy_inline_content(inline_klass, val, inline_klass->payload_offset(), obj, offset, state_before, field);\n+        }\n@@ -1856,1 +2112,0 @@\n-\n@@ -1972,1 +2227,1 @@\n-    } else if (bc_raw == Bytecodes::_invokespecial && !target->is_object_initializer() && calling_klass->is_interface()) {\n+    } else if (bc_raw == Bytecodes::_invokespecial && !target->is_object_constructor() && calling_klass->is_interface()) {\n@@ -2228,1 +2483,1 @@\n-  NewInstance* new_instance = new NewInstance(klass->as_instance_klass(), state_before, stream()->is_unresolved_klass());\n+  NewInstance* new_instance = new NewInstance(klass->as_instance_klass(), state_before, stream()->is_unresolved_klass(), false);\n@@ -2233,1 +2488,0 @@\n-\n@@ -2306,0 +2560,19 @@\n+  bool maybe_inlinetype = false;\n+  if (bci == InvocationEntryBci) {\n+    \/\/ Called by GraphBuilder::inline_sync_entry.\n+#ifdef ASSERT\n+    ciType* obj_type = x->declared_type();\n+    assert(obj_type == nullptr || !obj_type->is_inlinetype(), \"inline types cannot have synchronized methods\");\n+#endif\n+  } else {\n+    \/\/ We are compiling a monitorenter bytecode\n+    if (EnableValhalla) {\n+      ciType* obj_type = x->declared_type();\n+      if (obj_type == nullptr || obj_type->as_klass()->can_be_inline_klass()) {\n+        \/\/ If we're (possibly) locking on an inline type, check for markWord::always_locked_pattern\n+        \/\/ and throw IMSE. (obj_type is null for Phi nodes, so let's just be conservative).\n+        maybe_inlinetype = true;\n+      }\n+    }\n+  }\n+\n@@ -2308,1 +2581,1 @@\n-  append_with_bci(new MonitorEnter(x, state()->lock(x), state_before), bci);\n+  append_with_bci(new MonitorEnter(x, state()->lock(x), state_before, maybe_inlinetype), bci);\n@@ -2433,0 +2706,1 @@\n+    if (value->is_null_free()) return;\n@@ -2458,1 +2732,3 @@\n-    assert(cur_bci == SynchronizationEntryBCI || cur_bci == cur_scope_data->stream()->cur_bci(), \"invalid bci\");\n+    assert(cur_bci == SynchronizationEntryBCI || cur_bci == cur_scope_data->stream()->cur_bci()\n+           || has_pending_field_access() || has_pending_load_indexed(), \"invalid bci\");\n+\n@@ -3272,0 +3548,2 @@\n+  , _pending_field_access(nullptr)\n+  , _pending_load_indexed(nullptr)\n","filename":"src\/hotspot\/share\/c1\/c1_GraphBuilder.cpp","additions":349,"deletions":71,"binary":false,"changes":420,"status":"modified"},{"patch":"@@ -33,0 +33,2 @@\n+#include \"ci\/ciFlatArrayKlass.hpp\"\n+#include \"ci\/ciInlineKlass.hpp\"\n@@ -35,0 +37,1 @@\n+#include \"ci\/ciObjArrayKlass.hpp\"\n@@ -218,0 +221,2 @@\n+  assert(!_gen->in_conditional_code(), \"LIRItem cannot be loaded in conditional code\");\n+\n@@ -625,1 +630,2 @@\n-void LIRGenerator::monitor_enter(LIR_Opr object, LIR_Opr lock, LIR_Opr hdr, LIR_Opr scratch, int monitor_no, CodeEmitInfo* info_for_exception, CodeEmitInfo* info) {\n+void LIRGenerator::monitor_enter(LIR_Opr object, LIR_Opr lock, LIR_Opr hdr, LIR_Opr scratch, int monitor_no,\n+                                 CodeEmitInfo* info_for_exception, CodeEmitInfo* info, CodeStub* throw_ie_stub) {\n@@ -628,1 +634,1 @@\n-  CodeStub* slow_path = new MonitorEnterStub(object, lock, info);\n+  CodeStub* slow_path = new MonitorEnterStub(object, lock, info, throw_ie_stub, scratch);\n@@ -631,1 +637,1 @@\n-  __ lock_object(hdr, object, lock, scratch, slow_path, info_for_exception);\n+  __ lock_object(hdr, object, lock, scratch, slow_path, info_for_exception, throw_ie_stub);\n@@ -655,4 +661,9 @@\n-void LIRGenerator::new_instance(LIR_Opr dst, ciInstanceKlass* klass, bool is_unresolved, LIR_Opr scratch1, LIR_Opr scratch2, LIR_Opr scratch3, LIR_Opr scratch4, LIR_Opr klass_reg, CodeEmitInfo* info) {\n-  klass2reg_with_patching(klass_reg, klass, info, is_unresolved);\n-  \/\/ If klass is not loaded we do not know if the klass has finalizers:\n-  if (UseFastNewInstance && klass->is_loaded()\n+void LIRGenerator::new_instance(LIR_Opr dst, ciInstanceKlass* klass, bool is_unresolved, bool allow_inline, LIR_Opr scratch1, LIR_Opr scratch2, LIR_Opr scratch3, LIR_Opr scratch4, LIR_Opr klass_reg, CodeEmitInfo* info) {\n+  if (allow_inline) {\n+    assert(!is_unresolved && klass->is_loaded(), \"inline type klass should be resolved\");\n+    __ metadata2reg(klass->constant_encoding(), klass_reg);\n+  } else {\n+    klass2reg_with_patching(klass_reg, klass, info, is_unresolved);\n+  }\n+  \/\/ If klass is not loaded we do not know if the klass has finalizers or is an unexpected inline klass\n+  if (UseFastNewInstance && klass->is_loaded() && (allow_inline || !klass->is_inlinetype())\n@@ -673,1 +684,1 @@\n-    __ branch(lir_cond_always, slow_path);\n+    __ jump(slow_path);\n@@ -773,0 +784,10 @@\n+  if (!src->is_loaded_flat_array() && !dst->is_loaded_flat_array()) {\n+    flags &= ~LIR_OpArrayCopy::always_slow_path;\n+  }\n+  if (!src->maybe_flat_array()) {\n+    flags &= ~LIR_OpArrayCopy::src_inlinetype_check;\n+  }\n+  if (!dst->maybe_flat_array() && !dst->maybe_null_free_array()) {\n+    flags &= ~LIR_OpArrayCopy::dst_inlinetype_check;\n+  }\n+\n@@ -880,0 +901,6 @@\n+\n+  \/\/ TODO 8366668\n+  if (expected_type != nullptr && expected_type->is_obj_array_klass()) {\n+    expected_type = ciArrayKlass::make(expected_type->as_array_klass()->element_klass(), false, true, true);\n+  }\n+\n@@ -1465,1 +1492,1 @@\n-  for (int i = 0; i < _constants.length(); i++) {\n+  for (int i = 0; i < _constants.length() && !in_conditional_code(); i++) {\n@@ -1490,2 +1517,4 @@\n-  _constants.append(c);\n-  _reg_for_constants.append(result);\n+  if (!in_conditional_code()) {\n+    _constants.append(c);\n+    _reg_for_constants.append(result);\n+  }\n@@ -1495,0 +1524,6 @@\n+void LIRGenerator::set_in_conditional_code(bool v) {\n+  assert(v != _in_conditional_code, \"must change state\");\n+  _in_conditional_code = v;\n+}\n+\n+\n@@ -1512,0 +1547,8 @@\n+\/\/ Returns a int\/long value with the null marker bit set\n+static LIR_Opr null_marker_mask(BasicType bt, ciField* field) {\n+  assert(field->null_marker_offset() != -1, \"field does not have null marker\");\n+  int nm_offset = field->null_marker_offset() - field->offset_in_bytes();\n+  jlong null_marker = 1ULL << (nm_offset << LogBitsPerByte);\n+  return (bt == T_LONG) ? LIR_OprFact::longConst(null_marker) : LIR_OprFact::intConst(null_marker);\n+}\n+\n@@ -1541,0 +1584,1 @@\n+  ciField* field = x->field();\n@@ -1542,1 +1586,1 @@\n-  bool is_volatile = x->field()->is_volatile();\n+  bool is_volatile = field->is_volatile();\n@@ -1563,10 +1607,2 @@\n-  if (is_volatile || needs_patching) {\n-    \/\/ load item if field is volatile (fewer special cases for volatiles)\n-    \/\/ load item if field not initialized\n-    \/\/ load item if field not constant\n-    \/\/ because of code patching we cannot inline constants\n-    if (field_type == T_BYTE || field_type == T_BOOLEAN) {\n-      value.load_byte_item();\n-    } else  {\n-      value.load_item();\n-    }\n+  if (field->is_flat()) {\n+    value.load_item();\n@@ -1574,1 +1610,13 @@\n-    value.load_for_store(field_type);\n+    if (is_volatile || needs_patching) {\n+      \/\/ load item if field is volatile (fewer special cases for volatiles)\n+      \/\/ load item if field not initialized\n+      \/\/ load item if field not constant\n+      \/\/ because of code patching we cannot inline constants\n+      if (field_type == T_BYTE || field_type == T_BOOLEAN) {\n+        value.load_byte_item();\n+      } else  {\n+        value.load_item();\n+      }\n+    } else {\n+      value.load_for_store(field_type);\n+    }\n@@ -1603,0 +1651,43 @@\n+  if (field->is_flat()) {\n+    ciInlineKlass* vk = field->type()->as_inline_klass();\n+\n+#ifdef ASSERT\n+    bool is_naturally_atomic = vk->nof_declared_nonstatic_fields() <= 1;\n+    bool needs_atomic_access = !field->is_null_free() || (field->is_volatile() && !is_naturally_atomic);\n+    assert(needs_atomic_access, \"No atomic access required\");\n+    \/\/ ZGC does not support compressed oops, so only one oop can be in the payload which is written by a \"normal\" oop store.\n+    assert(!vk->contains_oops() || !UseZGC, \"ZGC does not support embedded oops in flat fields\");\n+#endif\n+\n+    \/\/ Zero the payload\n+    BasicType bt = vk->atomic_size_to_basic_type(field->is_null_free());\n+    LIR_Opr payload = new_register((bt == T_LONG) ? bt : T_INT);\n+    LIR_Opr zero = (bt == T_LONG) ? LIR_OprFact::longConst(0) : LIR_OprFact::intConst(0);\n+    __ move(zero, payload);\n+\n+    bool is_constant_null = value.is_constant() && value.value()->is_null_obj();\n+    if (!is_constant_null) {\n+      LabelObj* L_isNull = new LabelObj();\n+      bool needs_null_check = !value.is_constant() || value.value()->is_null_obj();\n+      if (needs_null_check) {\n+        __ cmp(lir_cond_equal, value.result(), LIR_OprFact::oopConst(nullptr));\n+        __ branch(lir_cond_equal, L_isNull->label());\n+      }\n+      \/\/ Load payload (if not empty) and set null marker (if not null-free)\n+      if (!vk->is_empty()) {\n+        access_load_at(decorators, bt, value, LIR_OprFact::intConst(vk->payload_offset()), payload);\n+      }\n+      if (!field->is_null_free()) {\n+        __ logical_or(payload, null_marker_mask(bt, field), payload);\n+      }\n+      if (needs_null_check) {\n+        __ branch_destination(L_isNull->label());\n+      }\n+    }\n+    access_store_at(decorators, bt, object, LIR_OprFact::intConst(x->offset()), payload,\n+                    \/\/ Make sure to emit an implicit null check and pass the information\n+                    \/\/ that this is a flat store that might require gc barriers for oop fields.\n+                    info != nullptr ? new CodeEmitInfo(info) : nullptr, info, vk);\n+    return;\n+  }\n+\n@@ -1607,0 +1698,155 @@\n+\/\/ FIXME -- I can't find any other way to pass an address to access_load_at().\n+class TempResolvedAddress: public Instruction {\n+ public:\n+  TempResolvedAddress(ValueType* type, LIR_Opr addr) : Instruction(type) {\n+    set_operand(addr);\n+  }\n+  virtual void input_values_do(ValueVisitor*) {}\n+  virtual void visit(InstructionVisitor* v)   {}\n+  virtual const char* name() const  { return \"TempResolvedAddress\"; }\n+};\n+\n+LIR_Opr LIRGenerator::get_and_load_element_address(LIRItem& array, LIRItem& index) {\n+  ciType* array_type = array.value()->declared_type();\n+  ciFlatArrayKlass* flat_array_klass = array_type->as_flat_array_klass();\n+  assert(flat_array_klass->is_loaded(), \"must be\");\n+\n+  int array_header_size = flat_array_klass->array_header_in_bytes();\n+  int shift = flat_array_klass->log2_element_size();\n+\n+#ifndef _LP64\n+  LIR_Opr index_op = new_register(T_INT);\n+  \/\/ FIXME -- on 32-bit, the shift below can overflow, so we need to check that\n+  \/\/ the top (shift+1) bits of index_op must be zero, or\n+  \/\/ else throw ArrayIndexOutOfBoundsException\n+  if (index.result()->is_constant()) {\n+    jint const_index = index.result()->as_jint();\n+    __ move(LIR_OprFact::intConst(const_index << shift), index_op);\n+  } else {\n+    __ shift_left(index_op, shift, index.result());\n+  }\n+#else\n+  LIR_Opr index_op = new_register(T_LONG);\n+  if (index.result()->is_constant()) {\n+    jint const_index = index.result()->as_jint();\n+    __ move(LIR_OprFact::longConst(const_index << shift), index_op);\n+  } else {\n+    __ convert(Bytecodes::_i2l, index.result(), index_op);\n+    \/\/ Need to shift manually, as LIR_Address can scale only up to 3.\n+    __ shift_left(index_op, shift, index_op);\n+  }\n+#endif\n+\n+  LIR_Opr elm_op = new_pointer_register();\n+  LIR_Address* elm_address = generate_address(array.result(), index_op, 0, array_header_size, T_ADDRESS);\n+  __ leal(LIR_OprFact::address(elm_address), elm_op);\n+  return elm_op;\n+}\n+\n+void LIRGenerator::access_sub_element(LIRItem& array, LIRItem& index, LIR_Opr& result, ciField* field, int sub_offset) {\n+  assert(field != nullptr, \"Need a subelement type specified\");\n+\n+  \/\/ Find the starting address of the source (inside the array)\n+  LIR_Opr elm_op = get_and_load_element_address(array, index);\n+\n+  BasicType subelt_type = field->type()->basic_type();\n+  TempResolvedAddress* elm_resolved_addr = new TempResolvedAddress(as_ValueType(subelt_type), elm_op);\n+  LIRItem elm_item(elm_resolved_addr, this);\n+\n+  DecoratorSet decorators = IN_HEAP;\n+  access_load_at(decorators, subelt_type,\n+                     elm_item, LIR_OprFact::intConst(sub_offset), result,\n+                     nullptr, nullptr);\n+}\n+\n+void LIRGenerator::access_flat_array(bool is_load, LIRItem& array, LIRItem& index, LIRItem& obj_item,\n+                                          ciField* field, int sub_offset) {\n+  assert(sub_offset == 0 || field != nullptr, \"Sanity check\");\n+\n+  \/\/ Find the starting address of the source (inside the array)\n+  LIR_Opr elm_op = get_and_load_element_address(array, index);\n+\n+  ciInlineKlass* elem_klass = nullptr;\n+  if (field != nullptr) {\n+    elem_klass = field->type()->as_inline_klass();\n+  } else {\n+    elem_klass = array.value()->declared_type()->as_flat_array_klass()->element_klass()->as_inline_klass();\n+  }\n+  for (int i = 0; i < elem_klass->nof_nonstatic_fields(); i++) {\n+    ciField* inner_field = elem_klass->nonstatic_field_at(i);\n+    assert(!inner_field->is_flat(), \"flat fields must have been expanded\");\n+    int obj_offset = inner_field->offset_in_bytes();\n+    int elm_offset = obj_offset - elem_klass->payload_offset() + sub_offset; \/\/ object header is not stored in array.\n+    BasicType field_type = inner_field->type()->basic_type();\n+\n+    \/\/ Types which are smaller than int are still passed in an int register.\n+    BasicType reg_type = field_type;\n+    switch (reg_type) {\n+    case T_BYTE:\n+    case T_BOOLEAN:\n+    case T_SHORT:\n+    case T_CHAR:\n+      reg_type = T_INT;\n+      break;\n+    default:\n+      break;\n+    }\n+\n+    LIR_Opr temp = new_register(reg_type);\n+    TempResolvedAddress* elm_resolved_addr = new TempResolvedAddress(as_ValueType(field_type), elm_op);\n+    LIRItem elm_item(elm_resolved_addr, this);\n+\n+    DecoratorSet decorators = IN_HEAP;\n+    if (is_load) {\n+      access_load_at(decorators, field_type,\n+                     elm_item, LIR_OprFact::intConst(elm_offset), temp,\n+                     nullptr, nullptr);\n+      access_store_at(decorators, field_type,\n+                      obj_item, LIR_OprFact::intConst(obj_offset), temp,\n+                      nullptr, nullptr);\n+    } else {\n+      access_load_at(decorators, field_type,\n+                     obj_item, LIR_OprFact::intConst(obj_offset), temp,\n+                     nullptr, nullptr);\n+      access_store_at(decorators, field_type,\n+                      elm_item, LIR_OprFact::intConst(elm_offset), temp,\n+                      nullptr, nullptr);\n+    }\n+  }\n+}\n+\n+void LIRGenerator::check_flat_array(LIR_Opr array, LIR_Opr value, CodeStub* slow_path) {\n+  LIR_Opr tmp = new_register(T_METADATA);\n+  __ check_flat_array(array, value, tmp, slow_path);\n+}\n+\n+void LIRGenerator::check_null_free_array(LIRItem& array, LIRItem& value, CodeEmitInfo* info) {\n+  LabelObj* L_end = new LabelObj();\n+  LIR_Opr tmp = new_register(T_METADATA);\n+  __ check_null_free_array(array.result(), tmp);\n+  __ branch(lir_cond_equal, L_end->label());\n+  __ null_check(value.result(), info);\n+  __ branch_destination(L_end->label());\n+}\n+\n+bool LIRGenerator::needs_flat_array_store_check(StoreIndexed* x) {\n+  if (x->elt_type() == T_OBJECT && x->array()->maybe_flat_array()) {\n+    ciType* type = x->value()->declared_type();\n+    if (type != nullptr && type->is_klass()) {\n+      ciKlass* klass = type->as_klass();\n+      if (!klass->can_be_inline_klass() || (klass->is_inlinetype() && !klass->as_inline_klass()->maybe_flat_in_array())) {\n+        \/\/ This is known to be a non-flat object. If the array is a flat array,\n+        \/\/ it will be caught by the code generated by array_store_check().\n+        return false;\n+      }\n+    }\n+    \/\/ We're not 100% sure, so let's do the flat_array_store_check.\n+    return true;\n+  }\n+  return false;\n+}\n+\n+bool LIRGenerator::needs_null_free_array_store_check(StoreIndexed* x) {\n+  return x->elt_type() == T_OBJECT && x->array()->maybe_null_free_array();\n+}\n+\n@@ -1609,0 +1855,2 @@\n+  assert(x->elt_type() != T_ARRAY, \"never used\");\n+  bool is_loaded_flat_array = x->array()->is_loaded_flat_array();\n@@ -1612,3 +1860,3 @@\n-  bool needs_store_check = obj_store && (x->value()->as_Constant() == nullptr ||\n-                                         !get_jobject_constant(x->value())->is_null_object() ||\n-                                         x->should_profile());\n+  bool needs_store_check = obj_store && !(is_loaded_flat_array && x->is_exact_flat_array_store()) &&\n+                                        (x->value()->as_Constant() == nullptr ||\n+                                         !get_jobject_constant(x->value())->is_null_object());\n@@ -1627,2 +1875,3 @@\n-\n-  if (needs_store_check || x->check_boolean()) {\n+\n+  if (needs_store_check || x->check_boolean()\n+      || is_loaded_flat_array || needs_flat_array_store_check(x) || needs_null_free_array_store_check(x)) {\n@@ -1657,0 +1906,20 @@\n+  if (x->should_profile()) {\n+    if (is_loaded_flat_array) {\n+      \/\/ No need to profile a store to a flat array of known type. This can happen if\n+      \/\/ the type only became known after optimizations (for example, after the PhiSimplifier).\n+      x->set_should_profile(false);\n+    } else {\n+      int bci = x->profiled_bci();\n+      ciMethodData* md = x->profiled_method()->method_data();\n+      assert(md != nullptr, \"Sanity\");\n+      ciProfileData* data = md->bci_to_data(bci);\n+      assert(data != nullptr && data->is_ArrayStoreData(), \"incorrect profiling entry\");\n+      ciArrayStoreData* store_data = (ciArrayStoreData*)data;\n+      profile_array_type(x, md, store_data);\n+      assert(store_data->is_ArrayStoreData(), \"incorrect profiling entry\");\n+      if (x->array()->maybe_null_free_array()) {\n+        profile_null_free_array(array, md, store_data);\n+      }\n+    }\n+  }\n+\n@@ -1662,4 +1931,27 @@\n-  DecoratorSet decorators = IN_HEAP | IS_ARRAY;\n-  if (x->check_boolean()) {\n-    decorators |= C1_MASK_BOOLEAN;\n-  }\n+  if (is_loaded_flat_array) {\n+    \/\/ TODO 8350865 This is currently dead code\n+    if (!x->value()->is_null_free()) {\n+      __ null_check(value.result(), new CodeEmitInfo(range_check_info));\n+    }\n+    \/\/ If array element is an empty inline type, no need to copy anything\n+    if (!x->array()->declared_type()->as_flat_array_klass()->element_klass()->as_inline_klass()->is_empty()) {\n+      access_flat_array(false, array, index, value);\n+    }\n+  } else {\n+    StoreFlattenedArrayStub* slow_path = nullptr;\n+\n+    if (needs_flat_array_store_check(x)) {\n+      \/\/ Check if we indeed have a flat array\n+      index.load_item();\n+      slow_path = new StoreFlattenedArrayStub(array.result(), index.result(), value.result(), state_for(x, x->state_before()));\n+      check_flat_array(array.result(), value.result(), slow_path);\n+      set_in_conditional_code(true);\n+    } else if (needs_null_free_array_store_check(x)) {\n+      CodeEmitInfo* info = new CodeEmitInfo(range_check_info);\n+      check_null_free_array(array, value, info);\n+    }\n+\n+    DecoratorSet decorators = IN_HEAP | IS_ARRAY;\n+    if (x->check_boolean()) {\n+      decorators |= C1_MASK_BOOLEAN;\n+    }\n@@ -1667,2 +1959,6 @@\n-  access_store_at(decorators, x->elt_type(), array, index.result(), value.result(),\n-                  nullptr, null_check_info);\n+    access_store_at(decorators, x->elt_type(), array, index.result(), value.result(), nullptr, null_check_info);\n+    if (slow_path != nullptr) {\n+      __ branch_destination(slow_path->continuation());\n+      set_in_conditional_code(false);\n+    }\n+  }\n@@ -1697,1 +1993,2 @@\n-                                   CodeEmitInfo* patch_info, CodeEmitInfo* store_emit_info) {\n+                                   CodeEmitInfo* patch_info, CodeEmitInfo* store_emit_info,\n+                                   ciInlineKlass* vk) {\n@@ -1699,1 +1996,1 @@\n-  LIRAccess access(this, decorators, base, offset, type, patch_info, store_emit_info);\n+  LIRAccess access(this, decorators, base, offset, type, patch_info, store_emit_info, vk);\n@@ -1750,0 +2047,1 @@\n+  ciField* field = x->field();\n@@ -1751,1 +2049,1 @@\n-  bool is_volatile = x->field()->is_volatile();\n+  bool is_volatile = field->is_volatile();\n@@ -1802,0 +2100,37 @@\n+  if (field->is_flat()) {\n+    ciInlineKlass* vk = field->type()->as_inline_klass();\n+#ifdef ASSERT\n+    bool is_naturally_atomic = vk->nof_declared_nonstatic_fields() <= 1;\n+    bool needs_atomic_access = !field->is_null_free() || (field->is_volatile() && !is_naturally_atomic);\n+    assert(needs_atomic_access, \"No atomic access required\");\n+    assert(x->state_before() != nullptr, \"Needs state before\");\n+#endif\n+\n+    \/\/ Allocate buffer (we can't easily do this conditionally on the null check below\n+    \/\/ because branches added in the LIR are opaque to the register allocator).\n+    NewInstance* buffer = new NewInstance(vk, x->state_before(), false, true);\n+    do_NewInstance(buffer);\n+    LIRItem dest(buffer, this);\n+\n+    \/\/ Copy the payload to the buffer\n+    BasicType bt = vk->atomic_size_to_basic_type(field->is_null_free());\n+    LIR_Opr payload = new_register((bt == T_LONG) ? bt : T_INT);\n+    access_load_at(decorators, bt, object, LIR_OprFact::intConst(field->offset_in_bytes()), payload,\n+                   \/\/ Make sure to emit an implicit null check\n+                   info ? new CodeEmitInfo(info) : nullptr, info);\n+    access_store_at(decorators, bt, dest, LIR_OprFact::intConst(vk->payload_offset()), payload);\n+\n+    if (field->is_null_free()) {\n+      set_result(x, buffer->operand());\n+    } else {\n+      \/\/ Check the null marker and set result to null if it's not set\n+      __ logical_and(payload, null_marker_mask(bt, field), payload);\n+      __ cmp(lir_cond_equal, payload, (bt == T_LONG) ? LIR_OprFact::longConst(0) : LIR_OprFact::intConst(0));\n+      __ cmove(lir_cond_equal, LIR_OprFact::oopConst(nullptr), buffer->operand(), rlock_result(x), T_OBJECT);\n+    }\n+\n+    \/\/ Ensure the copy is visible before any subsequent store that publishes the buffer.\n+    __ membar_storestore();\n+    return;\n+  }\n+\n@@ -1950,1 +2285,53 @@\n-  DecoratorSet decorators = IN_HEAP | IS_ARRAY;\n+  ciMethodData* md = nullptr;\n+  ciArrayLoadData* load_data = nullptr;\n+  if (x->should_profile()) {\n+    if (x->array()->is_loaded_flat_array()) {\n+      \/\/ No need to profile a load from a flat array of known type. This can happen if\n+      \/\/ the type only became known after optimizations (for example, after the PhiSimplifier).\n+      x->set_should_profile(false);\n+    } else {\n+      int bci = x->profiled_bci();\n+      md = x->profiled_method()->method_data();\n+      assert(md != nullptr, \"Sanity\");\n+      ciProfileData* data = md->bci_to_data(bci);\n+      assert(data != nullptr && data->is_ArrayLoadData(), \"incorrect profiling entry\");\n+      load_data = (ciArrayLoadData*)data;\n+      profile_array_type(x, md, load_data);\n+    }\n+  }\n+\n+  Value element;\n+  if (x->vt() != nullptr) {\n+    assert(x->array()->is_loaded_flat_array(), \"must be\");\n+    \/\/ Find the destination address (of the NewInlineTypeInstance).\n+    LIRItem obj_item(x->vt(), this);\n+\n+    access_flat_array(true, array, index, obj_item,\n+                      x->delayed() == nullptr ? 0 : x->delayed()->field(),\n+                      x->delayed() == nullptr ? 0 : x->delayed()->offset());\n+    set_no_result(x);\n+  } else if (x->delayed() != nullptr) {\n+    assert(x->array()->is_loaded_flat_array(), \"must be\");\n+    LIR_Opr result = rlock_result(x, x->delayed()->field()->type()->basic_type());\n+    access_sub_element(array, index, result, x->delayed()->field(), x->delayed()->offset());\n+  } else {\n+    LIR_Opr result = rlock_result(x, x->elt_type());\n+    LoadFlattenedArrayStub* slow_path = nullptr;\n+\n+    if (x->should_profile() && x->array()->maybe_null_free_array()) {\n+      profile_null_free_array(array, md, load_data);\n+    }\n+\n+    if (x->elt_type() == T_OBJECT && x->array()->maybe_flat_array()) {\n+      assert(x->delayed() == nullptr, \"Delayed LoadIndexed only apply to loaded_flat_arrays\");\n+      index.load_item();\n+      \/\/ if we are loading from a flat array, load it using a runtime call\n+      slow_path = new LoadFlattenedArrayStub(array.result(), index.result(), result, state_for(x, x->state_before()));\n+      check_flat_array(array.result(), LIR_OprFact::illegalOpr, slow_path);\n+      set_in_conditional_code(true);\n+    }\n+\n+    DecoratorSet decorators = IN_HEAP | IS_ARRAY;\n+    access_load_at(decorators, x->elt_type(),\n+                   array, index.result(), result,\n+                   nullptr, null_check_info);\n@@ -1952,4 +2339,11 @@\n-  LIR_Opr result = rlock_result(x, x->elt_type());\n-  access_load_at(decorators, x->elt_type(),\n-                 array, index.result(), result,\n-                 nullptr, null_check_info);\n+    if (slow_path != nullptr) {\n+      __ branch_destination(slow_path->continuation());\n+      set_in_conditional_code(false);\n+    }\n+\n+    element = x;\n+  }\n+\n+  if (x->should_profile()) {\n+    profile_element_type(element, md, load_data);\n+  }\n@@ -2430,0 +2824,10 @@\n+    \/\/ TODO 8366668\n+    if (exact_klass != nullptr && exact_klass->is_obj_array_klass()) {\n+      if (exact_klass->as_obj_array_klass()->element_klass()->is_inlinetype()) {\n+        \/\/ Could be flat, null free etc.\n+        exact_klass = nullptr;\n+      } else {\n+        exact_klass = ciObjArrayKlass::make(exact_klass->as_array_klass()->element_klass(), true);\n+      }\n+    }\n+\n@@ -2438,1 +2842,1 @@\n-  if (do_update) {\n+  if (do_update && signature_at_call_k != nullptr) {\n@@ -2467,0 +2871,11 @@\n+\n+    \/\/ TODO 8366668\n+    if (exact_klass != nullptr && exact_klass->is_obj_array_klass()) {\n+      if (exact_klass->as_obj_array_klass()->element_klass()->is_inlinetype()) {\n+        \/\/ Could be flat, null free etc.\n+        exact_klass = nullptr;\n+      } else {\n+        exact_klass = ciObjArrayKlass::make(exact_klass->as_array_klass()->element_klass(), true);\n+      }\n+    }\n+\n@@ -2523,0 +2938,40 @@\n+void LIRGenerator::profile_flags(ciMethodData* md, ciProfileData* data, int flag, LIR_Condition condition) {\n+  assert(md != nullptr && data != nullptr, \"should have been initialized\");\n+  LIR_Opr mdp = new_register(T_METADATA);\n+  __ metadata2reg(md->constant_encoding(), mdp);\n+  LIR_Address* addr = new LIR_Address(mdp, md->byte_offset_of_slot(data, DataLayout::flags_offset()), T_BYTE);\n+  LIR_Opr flags = new_register(T_INT);\n+  __ move(addr, flags);\n+  if (condition != lir_cond_always) {\n+    LIR_Opr update = new_register(T_INT);\n+    __ cmove(condition, LIR_OprFact::intConst(0), LIR_OprFact::intConst(flag), update, T_INT);\n+  } else {\n+    __ logical_or(flags, LIR_OprFact::intConst(flag), flags);\n+  }\n+  __ store(flags, addr);\n+}\n+\n+template <class ArrayData> void LIRGenerator::profile_null_free_array(LIRItem array, ciMethodData* md, ArrayData* load_store) {\n+  assert(compilation()->profile_array_accesses(), \"array access profiling is disabled\");\n+  LabelObj* L_end = new LabelObj();\n+  LIR_Opr tmp = new_register(T_METADATA);\n+  __ check_null_free_array(array.result(), tmp);\n+\n+  profile_flags(md, load_store, ArrayStoreData::null_free_array_byte_constant(), lir_cond_equal);\n+}\n+\n+template <class ArrayData> void LIRGenerator::profile_array_type(AccessIndexed* x, ciMethodData*& md, ArrayData*& load_store) {\n+  assert(compilation()->profile_array_accesses(), \"array access profiling is disabled\");\n+  LIR_Opr mdp = LIR_OprFact::illegalOpr;\n+  profile_type(md, md->byte_offset_of_slot(load_store, ArrayData::array_offset()), 0,\n+               load_store->array()->type(), x->array(), mdp, true, nullptr, nullptr);\n+}\n+\n+void LIRGenerator::profile_element_type(Value element, ciMethodData* md, ciArrayLoadData* load_data) {\n+  assert(compilation()->profile_array_accesses(), \"array access profiling is disabled\");\n+  assert(md != nullptr && load_data != nullptr, \"should have been initialized\");\n+  LIR_Opr mdp = LIR_OprFact::illegalOpr;\n+  profile_type(md, md->byte_offset_of_slot(load_data, ArrayLoadData::element_offset()), 0,\n+               load_data->element()->type(), element, mdp, false, nullptr, nullptr);\n+}\n+\n@@ -2605,0 +3060,8 @@\n+  if (method()->has_scalarized_args()) {\n+    \/\/ Check if deoptimization was triggered (i.e. orig_pc was set) while buffering scalarized inline type arguments\n+    \/\/ in the entry point (see comments in frame::deoptimize). If so, deoptimize only now that we have the right state.\n+    CodeEmitInfo* info = new CodeEmitInfo(scope()->start()->state()->copy(ValueStack::StateBefore, 0), nullptr, false);\n+    CodeStub* deopt_stub = new DeoptimizeStub(info, Deoptimization::Reason_none, Deoptimization::Action_none);\n+    __ append(new LIR_Op0(lir_check_orig_pc));\n+    __ branch(lir_cond_notEqual, deopt_stub);\n+  }\n@@ -2620,0 +3083,13 @@\n+void LIRGenerator::invoke_load_one_argument(LIRItem* param, LIR_Opr loc) {\n+  if (loc->is_register()) {\n+    param->load_item_force(loc);\n+  } else {\n+    LIR_Address* addr = loc->as_address_ptr();\n+    param->load_for_store(addr->type());\n+    if (addr->type() == T_OBJECT) {\n+      __ move_wide(param->result(), addr);\n+    } else {\n+      __ move(param->result(), addr);\n+    }\n+  }\n+}\n@@ -2627,10 +3103,1 @@\n-    if (loc->is_register()) {\n-      param->load_item_force(loc);\n-    } else {\n-      LIR_Address* addr = loc->as_address_ptr();\n-      param->load_for_store(addr->type());\n-      if (addr->type() == T_OBJECT) {\n-        __ move_wide(param->result(), addr);\n-      } else\n-        __ move(param->result(), addr);\n-    }\n+    invoke_load_one_argument(param, loc);\n@@ -2802,1 +3269,1 @@\n-  if (can_inline_as_constant(right.value())) {\n+  if (can_inline_as_constant(right.value()) && !x->substitutability_check()) {\n@@ -2805,0 +3272,1 @@\n+    \/\/ substitutability_check() needs to use right as a base register.\n@@ -2812,3 +3280,60 @@\n-  LIR_Opr reg = rlock_result(x);\n-  __ cmp(lir_cond(x->cond()), left.result(), right.result());\n-  __ cmove(lir_cond(x->cond()), t_val.result(), f_val.result(), reg, as_BasicType(x->x()->type()));\n+  if (x->substitutability_check()) {\n+    substitutability_check(x, left, right, t_val, f_val);\n+  } else {\n+    LIR_Opr reg = rlock_result(x);\n+    __ cmp(lir_cond(x->cond()), left.result(), right.result());\n+    __ cmove(lir_cond(x->cond()), t_val.result(), f_val.result(), reg, as_BasicType(x->x()->type()));\n+  }\n+}\n+\n+void LIRGenerator::substitutability_check(IfOp* x, LIRItem& left, LIRItem& right, LIRItem& t_val, LIRItem& f_val) {\n+  assert(x->cond() == If::eql || x->cond() == If::neq, \"must be\");\n+  bool is_acmpeq = (x->cond() == If::eql);\n+  LIR_Opr equal_result     = is_acmpeq ? t_val.result() : f_val.result();\n+  LIR_Opr not_equal_result = is_acmpeq ? f_val.result() : t_val.result();\n+  LIR_Opr result = rlock_result(x);\n+  CodeEmitInfo* info = state_for(x, x->state_before());\n+\n+  substitutability_check_common(x->x(), x->y(), left, right, equal_result, not_equal_result, result, info);\n+}\n+\n+void LIRGenerator::substitutability_check(If* x, LIRItem& left, LIRItem& right) {\n+  LIR_Opr equal_result     = LIR_OprFact::intConst(1);\n+  LIR_Opr not_equal_result = LIR_OprFact::intConst(0);\n+  LIR_Opr result = new_register(T_INT);\n+  CodeEmitInfo* info = state_for(x, x->state_before());\n+\n+  substitutability_check_common(x->x(), x->y(), left, right, equal_result, not_equal_result, result, info);\n+\n+  assert(x->cond() == If::eql || x->cond() == If::neq, \"must be\");\n+  __ cmp(lir_cond(x->cond()), result, equal_result);\n+}\n+\n+void LIRGenerator::substitutability_check_common(Value left_val, Value right_val, LIRItem& left, LIRItem& right,\n+                                                 LIR_Opr equal_result, LIR_Opr not_equal_result, LIR_Opr result,\n+                                                 CodeEmitInfo* info) {\n+  LIR_Opr tmp1 = LIR_OprFact::illegalOpr;\n+  LIR_Opr tmp2 = LIR_OprFact::illegalOpr;\n+  LIR_Opr left_klass_op = LIR_OprFact::illegalOpr;\n+  LIR_Opr right_klass_op = LIR_OprFact::illegalOpr;\n+\n+  ciKlass* left_klass  = left_val ->as_loaded_klass_or_null();\n+  ciKlass* right_klass = right_val->as_loaded_klass_or_null();\n+\n+  if ((left_klass == nullptr || right_klass == nullptr) ||\/\/ The klass is still unloaded, or came from a Phi node.\n+      !left_klass->is_inlinetype() || !right_klass->is_inlinetype()) {\n+    init_temps_for_substitutability_check(tmp1, tmp2);\n+  }\n+\n+  if (left_klass != nullptr && left_klass->is_inlinetype() && left_klass == right_klass) {\n+    \/\/ No need to load klass -- the operands are statically known to be the same inline klass.\n+  } else {\n+    BasicType t_klass = UseCompressedOops ? T_INT : T_METADATA;\n+    left_klass_op = new_register(t_klass);\n+    right_klass_op = new_register(t_klass);\n+  }\n+\n+  CodeStub* slow_path = new SubstitutabilityCheckStub(left.result(), right.result(), info);\n+  __ substitutability_check(result, left.result(), right.result(), equal_result, not_equal_result,\n+                            tmp1, tmp2,\n+                            left_klass, right_klass, left_klass_op, right_klass_op, info, slow_path);\n@@ -3089,1 +3614,1 @@\n-    ciReturnTypeEntry* ret = data->is_CallTypeData() ? ((ciCallTypeData*)data)->ret() : ((ciVirtualCallTypeData*)data)->ret();\n+    ciSingleTypeEntry* ret = data->is_CallTypeData() ? ((ciCallTypeData*)data)->ret() : ((ciVirtualCallTypeData*)data)->ret();\n@@ -3110,0 +3635,47 @@\n+bool LIRGenerator::profile_inline_klass(ciMethodData* md, ciProfileData* data, Value value, int flag) {\n+  ciKlass* klass = value->as_loaded_klass_or_null();\n+  if (klass != nullptr) {\n+    if (klass->is_inlinetype()) {\n+      profile_flags(md, data, flag, lir_cond_always);\n+    } else if (klass->can_be_inline_klass()) {\n+      return false;\n+    }\n+  } else {\n+    return false;\n+  }\n+  return true;\n+}\n+\n+\n+void LIRGenerator::do_ProfileACmpTypes(ProfileACmpTypes* x) {\n+  ciMethod* method = x->method();\n+  assert(method != nullptr, \"method should be set if branch is profiled\");\n+  ciMethodData* md = method->method_data_or_null();\n+  assert(md != nullptr, \"Sanity\");\n+  ciProfileData* data = md->bci_to_data(x->bci());\n+  assert(data != nullptr, \"must have profiling data\");\n+  assert(data->is_ACmpData(), \"need BranchData for two-way branches\");\n+  ciACmpData* acmp = (ciACmpData*)data;\n+  LIR_Opr mdp = LIR_OprFact::illegalOpr;\n+  profile_type(md, md->byte_offset_of_slot(acmp, ACmpData::left_offset()), 0,\n+               acmp->left()->type(), x->left(), mdp, !x->left_maybe_null(), nullptr, nullptr);\n+  int flags_offset = md->byte_offset_of_slot(data, DataLayout::flags_offset());\n+  if (!profile_inline_klass(md, acmp, x->left(), ACmpData::left_inline_type_byte_constant())) {\n+    LIR_Opr mdp = new_register(T_METADATA);\n+    __ metadata2reg(md->constant_encoding(), mdp);\n+    LIRItem value(x->left(), this);\n+    value.load_item();\n+    __ profile_inline_type(new LIR_Address(mdp, flags_offset, T_INT), value.result(), ACmpData::left_inline_type_byte_constant(), new_register(T_INT), !x->left_maybe_null());\n+  }\n+  profile_type(md, md->byte_offset_of_slot(acmp, ACmpData::left_offset()),\n+               in_bytes(ACmpData::right_offset()) - in_bytes(ACmpData::left_offset()),\n+               acmp->right()->type(), x->right(), mdp, !x->right_maybe_null(), nullptr, nullptr);\n+  if (!profile_inline_klass(md, acmp, x->right(), ACmpData::right_inline_type_byte_constant())) {\n+    LIR_Opr mdp = new_register(T_METADATA);\n+    __ metadata2reg(md->constant_encoding(), mdp);\n+    LIRItem value(x->right(), this);\n+    value.load_item();\n+    __ profile_inline_type(new LIR_Address(mdp, flags_offset, T_INT), value.result(), ACmpData::right_inline_type_byte_constant(), new_register(T_INT), !x->left_maybe_null());\n+  }\n+}\n+\n","filename":"src\/hotspot\/share\/c1\/c1_LIRGenerator.cpp","additions":630,"deletions":58,"binary":false,"changes":688,"status":"modified"},{"patch":"@@ -172,0 +172,1 @@\n+  bool          _in_conditional_code;\n@@ -198,0 +199,1 @@\n+  void set_in_conditional_code(bool v);\n@@ -217,0 +219,1 @@\n+  bool in_conditional_code() { return _in_conditional_code; }\n@@ -275,0 +278,13 @@\n+  void access_flat_array(bool is_load, LIRItem& array, LIRItem& index, LIRItem& obj_item, ciField* field = nullptr, int offset = 0);\n+  void access_sub_element(LIRItem& array, LIRItem& index, LIR_Opr& result, ciField* field, int sub_offset);\n+  LIR_Opr get_and_load_element_address(LIRItem& array, LIRItem& index);\n+  bool needs_flat_array_store_check(StoreIndexed* x);\n+  void check_flat_array(LIR_Opr array, LIR_Opr value, CodeStub* slow_path);\n+  bool needs_null_free_array_store_check(StoreIndexed* x);\n+  void check_null_free_array(LIRItem& array, LIRItem& value,  CodeEmitInfo* info);\n+  void substitutability_check(IfOp* x, LIRItem& left, LIRItem& right, LIRItem& t_val, LIRItem& f_val);\n+  void substitutability_check(If* x, LIRItem& left, LIRItem& right);\n+  void substitutability_check_common(Value left_val, Value right_val, LIRItem& left, LIRItem& right,\n+                                     LIR_Opr equal_result, LIR_Opr not_equal_result, LIR_Opr result, CodeEmitInfo* info);\n+  void init_temps_for_substitutability_check(LIR_Opr& tmp1, LIR_Opr& tmp2);\n+\n@@ -291,1 +307,1 @@\n-                       CodeEmitInfo* patch_info = nullptr, CodeEmitInfo* store_emit_info = nullptr);\n+                       CodeEmitInfo* patch_info = nullptr, CodeEmitInfo* store_emit_info = nullptr, ciInlineKlass* vk = nullptr);\n@@ -328,1 +344,1 @@\n-\n+  void invoke_load_one_argument(LIRItem* param, LIR_Opr loc);\n@@ -364,1 +380,1 @@\n-  void monitor_enter (LIR_Opr object, LIR_Opr lock, LIR_Opr hdr, LIR_Opr scratch, int monitor_no, CodeEmitInfo* info_for_exception, CodeEmitInfo* info);\n+  void monitor_enter (LIR_Opr object, LIR_Opr lock, LIR_Opr hdr, LIR_Opr scratch, int monitor_no, CodeEmitInfo* info_for_exception, CodeEmitInfo* info, CodeStub* throw_ie_stub);\n@@ -367,1 +383,1 @@\n-  void new_instance    (LIR_Opr  dst, ciInstanceKlass* klass, bool is_unresolved, LIR_Opr  scratch1, LIR_Opr  scratch2, LIR_Opr  scratch3,  LIR_Opr scratch4, LIR_Opr  klass_reg, CodeEmitInfo* info);\n+  void new_instance(LIR_Opr dst, ciInstanceKlass* klass, bool is_unresolved, bool allow_inline, LIR_Opr scratch1, LIR_Opr scratch2, LIR_Opr scratch3, LIR_Opr scratch4, LIR_Opr klass_reg, CodeEmitInfo* info);\n@@ -479,0 +495,5 @@\n+  void profile_flags(ciMethodData* md, ciProfileData* load_store, int flag, LIR_Condition condition = lir_cond_always);\n+  template <class ArrayData> void profile_null_free_array(LIRItem array, ciMethodData* md, ArrayData* load_store);\n+  template <class ArrayData> void profile_array_type(AccessIndexed* x, ciMethodData*& md, ArrayData*& load_store);\n+  void profile_element_type(Value element, ciMethodData* md, ciArrayLoadData* load_store);\n+  bool profile_inline_klass(ciMethodData* md, ciProfileData* data, Value value, int flag);\n@@ -506,0 +527,1 @@\n+    , _in_conditional_code(false)\n@@ -587,0 +609,1 @@\n+  virtual void do_ProfileACmpTypes(ProfileACmpTypes* x);\n","filename":"src\/hotspot\/share\/c1\/c1_LIRGenerator.hpp","additions":27,"deletions":4,"binary":false,"changes":31,"status":"modified"},{"patch":"@@ -53,0 +53,2 @@\n+#include \"oops\/flatArrayKlass.hpp\"\n+#include \"oops\/flatArrayOop.inline.hpp\"\n@@ -123,0 +125,1 @@\n+uint Runtime1::_new_null_free_array_slowcase_cnt = 0;\n@@ -125,0 +128,5 @@\n+uint Runtime1::_load_flat_array_slowcase_cnt = 0;\n+uint Runtime1::_store_flat_array_slowcase_cnt = 0;\n+uint Runtime1::_substitutability_check_slowcase_cnt = 0;\n+uint Runtime1::_buffer_inline_args_slowcase_cnt = 0;\n+uint Runtime1::_buffer_inline_args_no_receiver_slowcase_cnt = 0;\n@@ -134,0 +142,2 @@\n+uint Runtime1::_throw_illegal_monitor_state_exception_count = 0;\n+uint Runtime1::_throw_identity_exception_count = 0;\n@@ -376,2 +386,1 @@\n-\n-JRT_ENTRY(void, Runtime1::new_instance(JavaThread* current, Klass* klass))\n+static void allocate_instance(JavaThread* current, Klass* klass, TRAPS) {\n@@ -380,1 +389,1 @@\n-    _new_instance_slowcase_cnt++;\n+    Runtime1::_new_instance_slowcase_cnt++;\n@@ -394,0 +403,3 @@\n+JRT_ENTRY(void, Runtime1::new_instance(JavaThread* current, Klass* klass))\n+  allocate_instance(current, klass, CHECK);\n+JRT_END\n@@ -439,0 +451,24 @@\n+JRT_ENTRY(void, Runtime1::new_null_free_array(JavaThread* current, Klass* array_klass, jint length))\n+  NOT_PRODUCT(_new_null_free_array_slowcase_cnt++;)\n+  \/\/ TODO 8350865 This is dead code since 8325660 because null-free arrays can only be created via the factory methods that are not yet implemented in C1. Should probably be fixed by 8265122.\n+\n+  \/\/ Note: no handle for klass needed since they are not used\n+  \/\/       anymore after new_objArray() and no GC can happen before.\n+  \/\/       (This may have to change if this code changes!)\n+  assert(array_klass->is_klass(), \"not a class\");\n+  Handle holder(THREAD, array_klass->klass_holder()); \/\/ keep the klass alive\n+  Klass* elem_klass = ObjArrayKlass::cast(array_klass)->element_klass();\n+  assert(elem_klass->is_inline_klass(), \"must be\");\n+  InlineKlass* vk = InlineKlass::cast(elem_klass);\n+  \/\/ Logically creates elements, ensure klass init\n+  elem_klass->initialize(CHECK);\n+  arrayOop obj= oopFactory::new_objArray(elem_klass, length, ArrayKlass::ArrayProperties::NULL_RESTRICTED, CHECK);\n+  current->set_vm_result_oop(obj);\n+  \/\/ This is pretty rare but this runtime patch is stressful to deoptimization\n+  \/\/ if we deoptimize here so force a deopt to stress the path.\n+  if (DeoptimizeALot) {\n+    deopt_caller(current);\n+  }\n+JRT_END\n+\n+\n@@ -453,0 +489,96 @@\n+static void profile_flat_array(JavaThread* current, bool load, bool null_free) {\n+  ResourceMark rm(current);\n+  vframeStream vfst(current, true);\n+  assert(!vfst.at_end(), \"Java frame must exist\");\n+  \/\/ Check if array access profiling is enabled\n+  if (vfst.nm()->comp_level() != CompLevel_full_profile || !C1UpdateMethodData) {\n+    return;\n+  }\n+  int bci = vfst.bci();\n+  Method* method = vfst.method();\n+  MethodData* md = method->method_data();\n+  if (md != nullptr) {\n+    \/\/ Lock to access ProfileData, and ensure lock is not broken by a safepoint\n+    MutexLocker ml(md->extra_data_lock(), Mutex::_no_safepoint_check_flag);\n+\n+    ProfileData* data = md->bci_to_data(bci);\n+    assert(data != nullptr, \"incorrect profiling entry\");\n+    if (data->is_ArrayLoadData()) {\n+      assert(load, \"should be an array load\");\n+      ArrayLoadData* load_data = (ArrayLoadData*) data;\n+      load_data->set_flat_array();\n+      if (null_free) {\n+        load_data->set_null_free_array();\n+      }\n+    } else {\n+      assert(data->is_ArrayStoreData(), \"\");\n+      assert(!load, \"should be an array store\");\n+      ArrayStoreData* store_data = (ArrayStoreData*) data;\n+      store_data->set_flat_array();\n+      if (null_free) {\n+        store_data->set_null_free_array();\n+      }\n+    }\n+  }\n+}\n+\n+JRT_ENTRY(void, Runtime1::load_flat_array(JavaThread* current, flatArrayOopDesc* array, int index))\n+  assert(array->klass()->is_flatArray_klass(), \"should not be called\");\n+  profile_flat_array(current, true, array->is_null_free_array());\n+\n+  NOT_PRODUCT(_load_flat_array_slowcase_cnt++;)\n+  assert(array->length() > 0 && index < array->length(), \"already checked\");\n+  flatArrayHandle vah(current, array);\n+  oop obj = array->obj_at(index, CHECK);\n+  current->set_vm_result_oop(obj);\n+JRT_END\n+\n+JRT_ENTRY(void, Runtime1::store_flat_array(JavaThread* current, flatArrayOopDesc* array, int index, oopDesc* value))\n+  \/\/ TOOD 8350865 We can call here with a non-flat array because of LIR_Assembler::emit_opFlattenedArrayCheck\n+  if (array->klass()->is_flatArray_klass()) {\n+    profile_flat_array(current, false, array->is_null_free_array());\n+  }\n+\n+  NOT_PRODUCT(_store_flat_array_slowcase_cnt++;)\n+  if (value == nullptr && array->is_null_free_array()) {\n+    SharedRuntime::throw_and_post_jvmti_exception(current, vmSymbols::java_lang_NullPointerException());\n+  } else {\n+    assert(array->klass()->is_flatArray_klass(), \"should not be called\");\n+    array->obj_at_put(index, value, CHECK);\n+  }\n+JRT_END\n+\n+JRT_ENTRY(int, Runtime1::substitutability_check(JavaThread* current, oopDesc* left, oopDesc* right))\n+  NOT_PRODUCT(_substitutability_check_slowcase_cnt++;)\n+  JavaCallArguments args;\n+  args.push_oop(Handle(THREAD, left));\n+  args.push_oop(Handle(THREAD, right));\n+  JavaValue result(T_BOOLEAN);\n+  JavaCalls::call_static(&result,\n+                         vmClasses::ValueObjectMethods_klass(),\n+                         vmSymbols::isSubstitutable_name(),\n+                         vmSymbols::object_object_boolean_signature(),\n+                         &args, CHECK_0);\n+  return result.get_jboolean() ? 1 : 0;\n+JRT_END\n+\n+\n+extern \"C\" void ps();\n+\n+void Runtime1::buffer_inline_args_impl(JavaThread* current, Method* m, bool allocate_receiver) {\n+  JavaThread* THREAD = current;\n+  methodHandle method(current, m); \/\/ We are inside the verified_entry or verified_inline_ro_entry of this method.\n+  oop obj = SharedRuntime::allocate_inline_types_impl(current, method, allocate_receiver, CHECK);\n+  current->set_vm_result_oop(obj);\n+}\n+\n+JRT_ENTRY(void, Runtime1::buffer_inline_args(JavaThread* current, Method* method))\n+  NOT_PRODUCT(_buffer_inline_args_slowcase_cnt++;)\n+  buffer_inline_args_impl(current, method, true);\n+JRT_END\n+\n+JRT_ENTRY(void, Runtime1::buffer_inline_args_no_receiver(JavaThread* current, Method* method))\n+  NOT_PRODUCT(_buffer_inline_args_no_receiver_slowcase_cnt++;)\n+  buffer_inline_args_impl(current, method, false);\n+JRT_END\n+\n@@ -776,0 +908,13 @@\n+JRT_ENTRY(void, Runtime1::throw_illegal_monitor_state_exception(JavaThread* current))\n+  NOT_PRODUCT(_throw_illegal_monitor_state_exception_count++;)\n+  ResourceMark rm(current);\n+  SharedRuntime::throw_and_post_jvmti_exception(current, vmSymbols::java_lang_IllegalMonitorStateException());\n+JRT_END\n+\n+JRT_ENTRY(void, Runtime1::throw_identity_exception(JavaThread* current, oopDesc* object))\n+  NOT_PRODUCT(_throw_identity_exception_count++;)\n+  ResourceMark rm(current);\n+  char* message = SharedRuntime::generate_identity_exception_message(current, object->klass());\n+  SharedRuntime::throw_and_post_jvmti_exception(current, vmSymbols::java_lang_IdentityException(), message);\n+JRT_END\n+\n@@ -981,0 +1126,3 @@\n+  bool deoptimize_for_null_free = false;\n+  bool deoptimize_for_flat = false;\n+  bool deoptimize_for_strict_static = false;\n@@ -1024,0 +1172,13 @@\n+    \/\/ The field we are patching is null-free. Deoptimize and regenerate\n+    \/\/ the compiled code if we patch a putfield\/putstatic because it\n+    \/\/ does not contain the required null check.\n+    deoptimize_for_null_free = result.is_null_free_inline_type() && (field_access.is_putfield() || field_access.is_putstatic());\n+\n+    \/\/ The field we are patching is flat. Deoptimize and regenerate\n+    \/\/ the compiled code which can't handle the layout of the flat\n+    \/\/ field because it was unknown at compile time.\n+    deoptimize_for_flat = result.is_flat();\n+\n+    \/\/ Strict statics may require tracking if their class is not fully initialized.\n+    \/\/ For now we can bail out of the compiler and let the interpreter handle it.\n+    deoptimize_for_strict_static = result.is_strict_static_unset();\n@@ -1058,0 +1219,6 @@\n+          if (!k->is_typeArray_klass() && !k->is_refArray_klass() && !k->is_flatArray_klass()) {\n+            k = ObjArrayKlass::cast(k)->klass_with_properties(ArrayKlass::ArrayProperties::DEFAULT, THREAD);\n+          }\n+          if (k->is_flatArray_klass()) {\n+            deoptimize_for_flat = true;\n+          }\n@@ -1096,1 +1263,5 @@\n-  if (deoptimize_for_volatile || deoptimize_for_atomic) {\n+  if (deoptimize_for_volatile  ||\n+      deoptimize_for_atomic    ||\n+      deoptimize_for_null_free ||\n+      deoptimize_for_flat      ||\n+      deoptimize_for_strict_static) {\n@@ -1107,0 +1278,9 @@\n+      if (deoptimize_for_null_free) {\n+        tty->print_cr(\"Deoptimizing for patching null-free field reference\");\n+      }\n+      if (deoptimize_for_flat) {\n+        tty->print_cr(\"Deoptimizing for patching flat field or array reference\");\n+      }\n+      if (deoptimize_for_strict_static) {\n+        tty->print_cr(\"Deoptimizing for patching strict static field reference\");\n+      }\n@@ -1557,0 +1737,1 @@\n+  tty->print_cr(\" _new_null_free_array_slowcase_cnt: %u\", _new_null_free_array_slowcase_cnt);\n@@ -1559,0 +1740,6 @@\n+  tty->print_cr(\" _load_flat_array_slowcase_cnt:   %u\", _load_flat_array_slowcase_cnt);\n+  tty->print_cr(\" _store_flat_array_slowcase_cnt:  %u\", _store_flat_array_slowcase_cnt);\n+  tty->print_cr(\" _substitutability_check_slowcase_cnt: %u\", _substitutability_check_slowcase_cnt);\n+  tty->print_cr(\" _buffer_inline_args_slowcase_cnt:%u\", _buffer_inline_args_slowcase_cnt);\n+  tty->print_cr(\" _buffer_inline_args_no_receiver_slowcase_cnt:%u\", _buffer_inline_args_no_receiver_slowcase_cnt);\n+\n@@ -1569,0 +1756,2 @@\n+  tty->print_cr(\" _throw_illegal_monitor_state_exception_count:  %u:\", _throw_illegal_monitor_state_exception_count);\n+  tty->print_cr(\" _throw_identity_exception_count:               %u:\", _throw_identity_exception_count);\n","filename":"src\/hotspot\/share\/c1\/c1_Runtime1.cpp","additions":193,"deletions":4,"binary":false,"changes":197,"status":"modified"},{"patch":"@@ -303,1 +303,4 @@\n-          \"print control flow graph to a separate file during compilation\")\n+          \"print control flow graph to a separate file during compilation\") \\\n+                                                                            \\\n+  develop(bool, C1UseDelayedFlattenedFieldReads, true,                      \\\n+          \"Use delayed reads of flat fields to reduce heap buffering\")\n","filename":"src\/hotspot\/share\/c1\/c1_globals.hpp","additions":4,"deletions":1,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -369,0 +369,3 @@\n+      if (oak->is_refined_objArray_klass()) {\n+        oak = ObjArrayKlass::cast(oak->super());\n+      }\n@@ -439,8 +442,10 @@\n-\n-      if (elm->is_instance_klass()) {\n-        assert(InstanceKlass::cast(elm)->array_klasses() == nullptr, \"must be\");\n-        InstanceKlass::cast(elm)->set_array_klasses(oak);\n-      } else {\n-        assert(elm->is_array_klass(), \"sanity\");\n-        assert(ArrayKlass::cast(elm)->higher_dimension() == nullptr, \"must be\");\n-        ArrayKlass::cast(elm)->set_higher_dimension(oak);\n+      \/\/ Higher dimension may have been set when doing setup on ObjArrayKlass\n+      if (!oak->is_refined_objArray_klass()) {\n+        if (elm->is_instance_klass()) {\n+          assert(InstanceKlass::cast(elm)->array_klasses() == nullptr, \"must be\");\n+          InstanceKlass::cast(elm)->set_array_klasses(oak);\n+        } else {\n+          assert(elm->is_array_klass(), \"sanity\");\n+          assert(ArrayKlass::cast(elm)->higher_dimension() == nullptr, \"must be\");\n+          ArrayKlass::cast(elm)->set_higher_dimension(oak);\n+        }\n","filename":"src\/hotspot\/share\/cds\/dynamicArchive.cpp","additions":13,"deletions":8,"binary":false,"changes":21,"status":"modified"},{"patch":"@@ -80,0 +80,2 @@\n+#include \"oops\/flatArrayKlass.hpp\"\n+#include \"oops\/inlineKlass.hpp\"\n@@ -127,1 +129,1 @@\n-\/\/ [0] All classes are loaded in MetaspaceShared::preload_classes(). All metadata are\n+\/\/ [0] All classes are loaded in MetaspaceShared::loadable_descriptors(). All metadata are\n@@ -463,1 +465,1 @@\n-  soc->do_tag(objArrayOopDesc::base_offset_in_bytes());\n+  soc->do_tag(refArrayOopDesc::base_offset_in_bytes());\n@@ -879,1 +881,1 @@\n-void MetaspaceShared::preload_classes(TRAPS) {\n+void MetaspaceShared::loadable_descriptors(TRAPS) {\n@@ -926,1 +928,1 @@\n-    preload_classes(CHECK);\n+    loadable_descriptors(CHECK);\n@@ -1231,0 +1233,5 @@\n+  if (CDSConfig::is_valhalla_preview()) {\n+    log_info(cds)(\"Archived java heap is not yet supported with Valhalla preview\");\n+    return;\n+  }\n+\n","filename":"src\/hotspot\/share\/cds\/metaspaceShared.cpp","additions":11,"deletions":4,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -44,0 +44,1 @@\n+#include \"oops\/inlineKlass.inline.hpp\"\n@@ -514,1 +515,1 @@\n-      \/\/ array\n+      \/\/ TODO 8350865 I think we need to handle null-free\/flat arrays here\n@@ -964,0 +965,1 @@\n+\n@@ -1010,27 +1012,79 @@\n-  \/\/ staticfield <klass> <name> <signature> <value>\n-  \/\/\n-  \/\/ Initialize a class and fill in the value for a static field.\n-  \/\/ This is useful when the compile was dependent on the value of\n-  \/\/ static fields but it's impossible to properly rerun the static\n-  \/\/ initializer.\n-  void process_staticfield(TRAPS) {\n-    InstanceKlass* k = (InstanceKlass *)parse_klass(CHECK);\n-\n-    if (k == nullptr || ReplaySuppressInitializers == 0 ||\n-        (ReplaySuppressInitializers == 2 && k->class_loader() == nullptr)) {\n-      skip_remaining();\n-      return;\n-    }\n-\n-    assert(k->is_initialized(), \"must be\");\n-\n-    const char* field_name = parse_escaped_string();\n-    const char* field_signature = parse_string();\n-    fieldDescriptor fd;\n-    Symbol* name = SymbolTable::new_symbol(field_name);\n-    Symbol* sig = SymbolTable::new_symbol(field_signature);\n-    if (!k->find_local_field(name, sig, &fd) ||\n-        !fd.is_static() ||\n-        fd.has_initial_value()) {\n-      report_error(field_name);\n-      return;\n+  class InlineTypeFieldInitializer : public FieldClosure {\n+    oop _vt;\n+    CompileReplay* _replay;\n+  public:\n+    InlineTypeFieldInitializer(oop vt, CompileReplay* replay)\n+  : _vt(vt), _replay(replay) {}\n+\n+    void do_field(fieldDescriptor* fd) {\n+      BasicType bt = fd->field_type();\n+      const char* string_value = fd->is_null_free_inline_type() ? nullptr : _replay->parse_escaped_string();\n+      switch (bt) {\n+      case T_BYTE: {\n+        int value = atoi(string_value);\n+        _vt->byte_field_put(fd->offset(), value);\n+        break;\n+      }\n+      case T_BOOLEAN: {\n+        int value = atoi(string_value);\n+        _vt->bool_field_put(fd->offset(), value);\n+        break;\n+      }\n+      case T_SHORT: {\n+        int value = atoi(string_value);\n+        _vt->short_field_put(fd->offset(), value);\n+        break;\n+      }\n+      case T_CHAR: {\n+        int value = atoi(string_value);\n+        _vt->char_field_put(fd->offset(), value);\n+        break;\n+      }\n+      case T_INT: {\n+        int value = atoi(string_value);\n+        _vt->int_field_put(fd->offset(), value);\n+        break;\n+      }\n+      case T_LONG: {\n+        jlong value;\n+        if (sscanf(string_value, JLONG_FORMAT, &value) != 1) {\n+          fprintf(stderr, \"Error parsing long: %s\\n\", string_value);\n+          break;\n+        }\n+        _vt->long_field_put(fd->offset(), value);\n+        break;\n+      }\n+      case T_FLOAT: {\n+        float value = atof(string_value);\n+        _vt->float_field_put(fd->offset(), value);\n+        break;\n+      }\n+      case T_DOUBLE: {\n+        double value = atof(string_value);\n+        _vt->double_field_put(fd->offset(), value);\n+        break;\n+      }\n+      case T_ARRAY:\n+      case T_OBJECT:\n+        if (!fd->is_null_free_inline_type()) {\n+          JavaThread* THREAD = JavaThread::current();\n+          bool res = _replay->process_staticfield_reference(string_value, _vt, fd, THREAD);\n+          assert(res, \"should succeed for arrays & objects\");\n+          break;\n+        } else {\n+          InlineKlass* vk = InlineKlass::cast(fd->field_holder()->get_inline_type_field_klass(fd->index()));\n+          if (fd->is_flat()) {\n+            int field_offset = fd->offset() - vk->payload_offset();\n+            oop obj = cast_to_oop(cast_from_oop<address>(_vt) + field_offset);\n+            InlineTypeFieldInitializer init_fields(obj, _replay);\n+            vk->do_nonstatic_fields(&init_fields);\n+          } else {\n+            oop value = vk->allocate_instance(JavaThread::current());\n+            _vt->obj_field_put(fd->offset(), value);\n+          }\n+          break;\n+        }\n+      default: {\n+        fatal(\"Unhandled type: %s\", type2name(bt));\n+      }\n+      }\n@@ -1038,0 +1092,1 @@\n+  };\n@@ -1039,1 +1094,1 @@\n-    oop java_mirror = k->java_mirror();\n+  bool process_staticfield_reference(const char* field_signature, oop java_mirror, fieldDescriptor* fd, TRAPS) {\n@@ -1047,4 +1102,2 @@\n-          ArrayKlass* kelem = (ArrayKlass *)parse_klass(CHECK);\n-          if (kelem == nullptr) {\n-            return;\n-          }\n+          Klass* k = resolve_klass(field_signature, CHECK_(true));\n+          ArrayKlass* kelem = (ArrayKlass *)k;\n@@ -1060,1 +1113,1 @@\n-          value = kelem->multi_allocate(rank, dims, CHECK);\n+          value = kelem->multi_allocate(rank, dims, CHECK_(true));\n@@ -1063,1 +1116,1 @@\n-            value = oopFactory::new_byteArray(length, CHECK);\n+            value = oopFactory::new_byteArray(length, CHECK_(true));\n@@ -1065,1 +1118,1 @@\n-            value = oopFactory::new_boolArray(length, CHECK);\n+            value = oopFactory::new_boolArray(length, CHECK_(true));\n@@ -1067,1 +1120,1 @@\n-            value = oopFactory::new_charArray(length, CHECK);\n+            value = oopFactory::new_charArray(length, CHECK_(true));\n@@ -1069,1 +1122,1 @@\n-            value = oopFactory::new_shortArray(length, CHECK);\n+            value = oopFactory::new_shortArray(length, CHECK_(true));\n@@ -1071,1 +1124,1 @@\n-            value = oopFactory::new_floatArray(length, CHECK);\n+            value = oopFactory::new_floatArray(length, CHECK_(true));\n@@ -1073,1 +1126,1 @@\n-            value = oopFactory::new_doubleArray(length, CHECK);\n+            value = oopFactory::new_doubleArray(length, CHECK_(true));\n@@ -1075,1 +1128,1 @@\n-            value = oopFactory::new_intArray(length, CHECK);\n+            value = oopFactory::new_intArray(length, CHECK_(true));\n@@ -1077,1 +1130,1 @@\n-            value = oopFactory::new_longArray(length, CHECK);\n+            value = oopFactory::new_longArray(length, CHECK_(true));\n@@ -1080,1 +1133,4 @@\n-            Klass* actual_array_klass = parse_klass(CHECK);\n+            Klass* actual_array_klass = parse_klass(CHECK_(true));\n+            \/\/ TODO 8350865 I think we need to handle null-free\/flat arrays here\n+            \/\/ This handling will change the array property argument passed to the\n+            \/\/ factory below\n@@ -1082,1 +1138,1 @@\n-            value = oopFactory::new_objArray(kelem, length, CHECK);\n+            value = oopFactory::new_objArray(kelem, length, CHECK_(true));\n@@ -1087,0 +1143,14 @@\n+        java_mirror->obj_field_put(fd->offset(), value);\n+        return true;\n+      }\n+    } else if (strcmp(field_signature, \"Ljava\/lang\/String;\") == 0) {\n+      const char* string_value = parse_escaped_string();\n+      Handle value = java_lang_String::create_from_str(string_value, CHECK_(true));\n+      java_mirror->obj_field_put(fd->offset(), value());\n+      return true;\n+    } else if (field_signature[0] == JVM_SIGNATURE_CLASS) {\n+      const char* instance = parse_escaped_string();\n+      oop value = nullptr;\n+      if (instance != nullptr) {\n+        Klass* k = resolve_klass(instance, CHECK_(true));\n+        value = InstanceKlass::cast(k)->allocate_instance(CHECK_(true));\n@@ -1088,0 +1158,76 @@\n+      java_mirror->obj_field_put(fd->offset(), value);\n+      return true;\n+    }\n+    return false;\n+  }\n+\n+  \/\/ Initialize a class and fill in the value for a static field.\n+  \/\/ This is useful when the compile was dependent on the value of\n+  \/\/ static fields but it's impossible to properly rerun the static\n+  \/\/ initializer.\n+  void process_staticfield(TRAPS) {\n+    InstanceKlass* k = (InstanceKlass *)parse_klass(CHECK);\n+\n+    if (k == nullptr || ReplaySuppressInitializers == 0 ||\n+        (ReplaySuppressInitializers == 2 && k->class_loader() == nullptr)) {\n+        skip_remaining();\n+      return;\n+    }\n+\n+    assert(k->is_initialized(), \"must be\");\n+\n+    const char* field_name = parse_escaped_string();\n+    const char* field_signature = parse_string();\n+    fieldDescriptor fd;\n+    Symbol* name = SymbolTable::new_symbol(field_name);\n+    Symbol* sig = SymbolTable::new_symbol(field_signature);\n+    if (!k->find_local_field(name, sig, &fd) ||\n+        !fd.is_static() ||\n+        fd.has_initial_value()) {\n+      report_error(field_name);\n+      return;\n+    }\n+\n+    oop java_mirror = k->java_mirror();\n+    if (strcmp(field_signature, \"I\") == 0) {\n+      const char* string_value = parse_escaped_string();\n+      int value = atoi(string_value);\n+      java_mirror->int_field_put(fd.offset(), value);\n+    } else if (strcmp(field_signature, \"B\") == 0) {\n+      const char* string_value = parse_escaped_string();\n+      int value = atoi(string_value);\n+      java_mirror->byte_field_put(fd.offset(), value);\n+    } else if (strcmp(field_signature, \"C\") == 0) {\n+      const char* string_value = parse_escaped_string();\n+      int value = atoi(string_value);\n+      java_mirror->char_field_put(fd.offset(), value);\n+    } else if (strcmp(field_signature, \"S\") == 0) {\n+      const char* string_value = parse_escaped_string();\n+      int value = atoi(string_value);\n+      java_mirror->short_field_put(fd.offset(), value);\n+    } else if (strcmp(field_signature, \"Z\") == 0) {\n+      const char* string_value = parse_escaped_string();\n+      int value = atoi(string_value);\n+      java_mirror->bool_field_put(fd.offset(), value);\n+    } else if (strcmp(field_signature, \"J\") == 0) {\n+      const char* string_value = parse_escaped_string();\n+      jlong value;\n+      if (sscanf(string_value, JLONG_FORMAT, &value) != 1) {\n+        fprintf(stderr, \"Error parsing long: %s\\n\", string_value);\n+        return;\n+      }\n+      java_mirror->long_field_put(fd.offset(), value);\n+    } else if (strcmp(field_signature, \"F\") == 0) {\n+      const char* string_value = parse_escaped_string();\n+      float value = atof(string_value);\n+      java_mirror->float_field_put(fd.offset(), value);\n+    } else if (strcmp(field_signature, \"D\") == 0) {\n+      const char* string_value = parse_escaped_string();\n+      double value = atof(string_value);\n+      java_mirror->double_field_put(fd.offset(), value);\n+    } else if (fd.is_null_free_inline_type()) {\n+      Klass* kelem = resolve_klass(field_signature, CHECK);\n+      InlineKlass* vk = InlineKlass::cast(kelem);\n+      oop value = vk->allocate_instance(CHECK);\n+      InlineTypeFieldInitializer init_fields(value, this);\n+      vk->do_nonstatic_fields(&init_fields);\n@@ -1090,40 +1236,2 @@\n-      const char* string_value = parse_escaped_string();\n-      if (strcmp(field_signature, \"I\") == 0) {\n-        int value = atoi(string_value);\n-        java_mirror->int_field_put(fd.offset(), value);\n-      } else if (strcmp(field_signature, \"B\") == 0) {\n-        int value = atoi(string_value);\n-        java_mirror->byte_field_put(fd.offset(), value);\n-      } else if (strcmp(field_signature, \"C\") == 0) {\n-        int value = atoi(string_value);\n-        java_mirror->char_field_put(fd.offset(), value);\n-      } else if (strcmp(field_signature, \"S\") == 0) {\n-        int value = atoi(string_value);\n-        java_mirror->short_field_put(fd.offset(), value);\n-      } else if (strcmp(field_signature, \"Z\") == 0) {\n-        int value = atoi(string_value);\n-        java_mirror->bool_field_put(fd.offset(), value);\n-      } else if (strcmp(field_signature, \"J\") == 0) {\n-        jlong value;\n-        if (sscanf(string_value, JLONG_FORMAT, &value) != 1) {\n-          fprintf(stderr, \"Error parsing long: %s\\n\", string_value);\n-          return;\n-        }\n-        java_mirror->long_field_put(fd.offset(), value);\n-      } else if (strcmp(field_signature, \"F\") == 0) {\n-        float value = atof(string_value);\n-        java_mirror->float_field_put(fd.offset(), value);\n-      } else if (strcmp(field_signature, \"D\") == 0) {\n-        double value = atof(string_value);\n-        java_mirror->double_field_put(fd.offset(), value);\n-      } else if (strcmp(field_signature, \"Ljava\/lang\/String;\") == 0) {\n-        Handle value = java_lang_String::create_from_str(string_value, CHECK);\n-        java_mirror->obj_field_put(fd.offset(), value());\n-      } else if (field_signature[0] == JVM_SIGNATURE_CLASS) {\n-        oop value = nullptr;\n-        if (string_value != nullptr) {\n-          Klass* k = resolve_klass(string_value, CHECK);\n-          value = InstanceKlass::cast(k)->allocate_instance(CHECK);\n-        }\n-        java_mirror->obj_field_put(fd.offset(), value);\n-      } else {\n+      bool res = process_staticfield_reference(field_signature, java_mirror, &fd, CHECK);\n+      if (!res)  {\n","filename":"src\/hotspot\/share\/ci\/ciReplay.cpp","additions":192,"deletions":84,"binary":false,"changes":276,"status":"modified"},{"patch":"@@ -130,0 +130,1 @@\n+    frame->set_assert_unset_fields(stackmap_frame->assert_unset_fields());\n@@ -160,1 +161,2 @@\n-                               u2 max_locals, u2 max_stack, TRAPS) :\n+                               u2 max_locals, u2 max_stack,\n+                               StackMapFrame::AssertUnsetFieldTable* initial_strict_fields, TRAPS) :\n@@ -164,1 +166,2 @@\n-                                  _max_stack(max_stack), _first(true) {\n+                                  _max_stack(max_stack), _assert_unset_fields_buffer(initial_strict_fields),\n+                                  _first(true) {\n@@ -206,1 +209,2 @@\n-    return VerificationType::reference_type(_cp->klass_name_at(class_index));\n+    Symbol* klass_name = _cp->klass_name_at(class_index);\n+    return VerificationType::reference_type(klass_name);\n@@ -247,0 +251,57 @@\n+  if (frame_type == EARLY_LARVAL) {\n+    u2 num_unset_fields = _stream->get_u2(CHECK_NULL);\n+    StackMapFrame::AssertUnsetFieldTable* new_fields = new StackMapFrame::AssertUnsetFieldTable();\n+\n+    for (u2 i = 0; i < num_unset_fields; i++) {\n+      u2 index = _stream->get_u2(CHECK_NULL);\n+\n+      if (!_cp->is_within_bounds(index) || !_cp->tag_at(index).is_name_and_type()) {\n+        _prev_frame->verifier()->verify_error(\n+          ErrorContext::bad_strict_fields(_prev_frame->offset(), _prev_frame),\n+          \"Invalid constant pool index in early larval frame: %d\", index);\n+        return nullptr;\n+      }\n+\n+      Symbol* name = _cp->symbol_at(_cp->name_ref_index_at(index));\n+      Symbol* sig = _cp->symbol_at(_cp->signature_ref_index_at(index));\n+      NameAndSig tmp(name, sig);\n+\n+      if (!_prev_frame->assert_unset_fields()->contains(tmp)) {\n+        log_info(verification)(\"NameAndType %s%s(CP index: %d) is not found among initial strict instance fields\", name->as_C_string(), sig->as_C_string(), index);\n+        StackMapFrame::print_strict_fields(_prev_frame->assert_unset_fields());\n+        _prev_frame->verifier()->verify_error(\n+            ErrorContext::bad_strict_fields(_prev_frame->offset(), _prev_frame),\n+            \"Strict fields not a subset of initial strict instance fields: %s:%s\", name->as_C_string(), sig->as_C_string());\n+        return nullptr;\n+      } else {\n+        new_fields->put(tmp, false);\n+      }\n+    }\n+\n+    \/\/ Only modify strict instance fields the frame has uninitialized this\n+    if (_prev_frame->flag_this_uninit()) {\n+      _assert_unset_fields_buffer = _prev_frame->merge_unset_fields(new_fields);\n+    } else if (new_fields->number_of_entries() > 0) {\n+      _prev_frame->verifier()->verify_error(\n+        ErrorContext::bad_strict_fields(_prev_frame->offset(), _prev_frame),\n+        \"Cannot have uninitialized strict fields after class initialization\");\n+      return nullptr;\n+    }\n+\n+    \/\/ Continue reading frame data\n+    if (at_end()) {\n+      _prev_frame->verifier()->verify_error(\n+        ErrorContext::bad_strict_fields(_prev_frame->offset(), _prev_frame),\n+        \"Early larval frame must be followed by a base frame\");\n+      return nullptr;\n+    }\n+\n+    frame_type = _stream->get_u1(CHECK_NULL);\n+    if (frame_type == EARLY_LARVAL) {\n+      _prev_frame->verifier()->verify_error(\n+        ErrorContext::bad_strict_fields(_prev_frame->offset(), _prev_frame),\n+        \"Early larval frame must be followed by a base frame\");\n+      return nullptr;\n+    }\n+  }\n+\n@@ -262,1 +323,2 @@\n-      _max_locals, _max_stack, locals, nullptr, _verifier);\n+      _max_locals, _max_stack, locals, nullptr,\n+      _assert_unset_fields_buffer, _verifier);\n@@ -294,1 +356,2 @@\n-      _max_locals, _max_stack, locals, stack, _verifier);\n+      _max_locals, _max_stack, locals, stack,\n+      _assert_unset_fields_buffer, _verifier);\n@@ -304,1 +367,1 @@\n-  if (frame_type < SAME_LOCALS_1_STACK_ITEM_EXTENDED) {\n+  if (frame_type < EARLY_LARVAL) {\n@@ -335,1 +398,2 @@\n-      _max_locals, _max_stack, locals, stack, _verifier);\n+      _max_locals, _max_stack, locals, stack,\n+      _assert_unset_fields_buffer, _verifier);\n@@ -378,1 +442,2 @@\n-      locals, nullptr, _verifier);\n+      locals, nullptr,\n+      _assert_unset_fields_buffer, _verifier);\n@@ -413,1 +478,2 @@\n-      _max_stack, locals, nullptr, _verifier);\n+      _max_stack, locals, nullptr,\n+      _assert_unset_fields_buffer, _verifier);\n@@ -461,1 +527,2 @@\n-      _max_locals, _max_stack, locals, stack, _verifier);\n+      _max_locals, _max_stack, locals, stack,\n+      _assert_unset_fields_buffer, _verifier);\n","filename":"src\/hotspot\/share\/classfile\/stackMapTable.cpp","additions":77,"deletions":10,"binary":false,"changes":87,"status":"modified"},{"patch":"@@ -131,0 +131,3 @@\n+  \/\/ Contains assert_unset_fields generated from classfile\n+  StackMapFrame::AssertUnsetFieldTable* _assert_unset_fields_buffer;\n+\n@@ -157,1 +160,2 @@\n-    RESERVED_END = 246,\n+    RESERVED_END = 245,\n+    EARLY_LARVAL = 246,\n@@ -172,1 +176,2 @@\n-                 u2 max_locals, u2 max_stack, TRAPS);\n+                 u2 max_locals, u2 max_stack,\n+                 StackMapFrame::AssertUnsetFieldTable* initial_strict_fields, TRAPS);\n","filename":"src\/hotspot\/share\/classfile\/stackMapTable.hpp","additions":7,"deletions":2,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -266,0 +266,3 @@\n+  case vmIntrinsics::_newNullRestrictedNonAtomicArray:\n+  case vmIntrinsics::_newNullRestrictedAtomicArray:\n+  case vmIntrinsics::_newNullableAtomicArray:\n@@ -336,0 +339,2 @@\n+  case vmIntrinsics::_makePrivateBuffer:\n+  case vmIntrinsics::_finishPrivateBuffer:\n@@ -345,0 +350,2 @@\n+  case vmIntrinsics::_getValue:\n+  case vmIntrinsics::_getFlatValue:\n@@ -354,0 +361,2 @@\n+  case vmIntrinsics::_putValue:\n+  case vmIntrinsics::_putFlatValue:\n","filename":"src\/hotspot\/share\/classfile\/vmIntrinsics.cpp","additions":9,"deletions":0,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -328,0 +328,8 @@\n+  do_intrinsic(_newNullRestrictedAtomicArray, jdk_internal_value_ValueClass, newNullRestrictedAtomicArray_name, newArray_signature3, F_SN) \\\n+   do_name(     newNullRestrictedAtomicArray_name,                \"newNullRestrictedAtomicArray\")                       \\\n+  do_intrinsic(_newNullRestrictedNonAtomicArray, jdk_internal_value_ValueClass, newNullRestrictedNonAtomicArray_name, newArray_signature3, F_SN) \\\n+   do_name(     newNullRestrictedNonAtomicArray_name,             \"newNullRestrictedNonAtomicArray\")                    \\\n+  do_intrinsic(_newNullableAtomicArray, jdk_internal_value_ValueClass, newNullableAtomicArray_name, newArray_signature2, F_SN) \\\n+   do_name(     newNullableAtomicArray_name,                      \"newNullableAtomicArray\")                             \\\n+   do_signature(newArray_signature2,                              \"(Ljava\/lang\/Class;I)[Ljava\/lang\/Object;\")            \\\n+   do_signature(newArray_signature3,                              \"(Ljava\/lang\/Class;ILjava\/lang\/Object;)[Ljava\/lang\/Object;\") \\\n@@ -731,0 +739,4 @@\n+  do_signature(getValue_signature,        \"(Ljava\/lang\/Object;JLjava\/lang\/Class;)Ljava\/lang\/Object;\")                   \\\n+  do_signature(putValue_signature,        \"(Ljava\/lang\/Object;JLjava\/lang\/Class;Ljava\/lang\/Object;)V\")                  \\\n+  do_signature(getFlatValue_signature,    \"(Ljava\/lang\/Object;JILjava\/lang\/Class;)Ljava\/lang\/Object;\")                  \\\n+  do_signature(putFlatValue_signature,    \"(Ljava\/lang\/Object;JILjava\/lang\/Class;Ljava\/lang\/Object;)V\")                 \\\n@@ -741,0 +753,4 @@\n+  do_name(getValue_name,\"getValue\")             do_name(putValue_name,\"putValue\")                                       \\\n+  do_name(getFlatValue_name,\"getFlatValue\")     do_name(putFlatValue_name,\"putFlatValue\")                               \\\n+  do_name(makePrivateBuffer_name,\"makePrivateBuffer\")                                                                   \\\n+  do_name(finishPrivateBuffer_name,\"finishPrivateBuffer\")                                                               \\\n@@ -751,0 +767,2 @@\n+  do_intrinsic(_getValue,           jdk_internal_misc_Unsafe,     getValue_name, getValue_signature,             F_RN)  \\\n+  do_intrinsic(_getFlatValue,       jdk_internal_misc_Unsafe,     getFlatValue_name, getFlatValue_signature,     F_RN)  \\\n@@ -760,0 +778,5 @@\n+  do_intrinsic(_putValue,           jdk_internal_misc_Unsafe,     putValue_name, putValue_signature,             F_RN)  \\\n+  do_intrinsic(_putFlatValue,       jdk_internal_misc_Unsafe,     putFlatValue_name, putFlatValue_signature,     F_RN)  \\\n+                                                                                                                        \\\n+  do_intrinsic(_makePrivateBuffer,  jdk_internal_misc_Unsafe,     makePrivateBuffer_name, object_object_signature, F_RN)   \\\n+  do_intrinsic(_finishPrivateBuffer,  jdk_internal_misc_Unsafe,   finishPrivateBuffer_name, object_object_signature, F_RN) \\\n","filename":"src\/hotspot\/share\/classfile\/vmIntrinsics.hpp","additions":23,"deletions":0,"binary":false,"changes":23,"status":"modified"},{"patch":"@@ -94,0 +94,25 @@\n+  \/* Valhalla migrated classes. *\/                                                                \\\n+  template(java_lang_Number,                          \"java\/lang\/Number\")                         \\\n+  template(java_lang_Record,                          \"java\/lang\/Record\")                         \\\n+  template(java_util_Optional,                        \"java\/util\/Optional\")                       \\\n+  template(java_util_OptionalInt,                     \"java\/util\/OptionalInt\")                    \\\n+  template(java_util_OptionalLong,                    \"java\/util\/OptionalLong\")                   \\\n+  template(java_util_OptionalDouble,                  \"java\/util\/OptionalDouble\")                 \\\n+  template(java_time_LocalDate,                       \"java\/time\/LocalDate\")                      \\\n+  template(java_time_LocalDateTime,                   \"java\/time\/LocalDateTime\")                  \\\n+  template(java_time_LocalTime,                       \"java\/time\/LocalTime\")                      \\\n+  template(java_time_Duration,                        \"java\/time\/Duration\")                       \\\n+  template(java_time_Instant,                         \"java\/time\/Instant\")                        \\\n+  template(java_time_MonthDay,                        \"java\/time\/MonthDay\")                       \\\n+  template(java_time_ZonedDateTime,                   \"java\/time\/ZonedDateTime\")                  \\\n+  template(java_time_OffsetDateTime,                  \"java\/time\/OffsetDateTime\")                 \\\n+  template(java_time_OffsetTime,                      \"java\/time\/OffsetTime\")                     \\\n+  template(java_time_YearMonth,                       \"java\/time\/YearMonth\")                      \\\n+  template(java_time_Year,                            \"java\/time\/Year\")                           \\\n+  template(java_time_Period,                          \"java\/time\/Period\")                         \\\n+  template(java_time_chrono_ChronoLocalDateImpl,      \"java\/time\/chrono\/ChronoLocalDateImpl\")     \\\n+  template(java_time_chrono_MinguoDate,               \"java\/time\/chrono\/MinguoDate\")              \\\n+  template(java_time_chrono_HijrahDate,               \"java\/time\/chrono\/HijrahDate\")              \\\n+  template(java_time_chrono_JapaneseDate,             \"java\/time\/chrono\/JapaneseDate\")            \\\n+  template(java_time_chrono_ThaiBuddhistDate,         \"java\/time\/chrono\/ThaiBuddhistDate\")        \\\n+                                                                                                  \\\n@@ -140,1 +165,0 @@\n-  template(java_lang_Record,                          \"java\/lang\/Record\")                         \\\n@@ -170,0 +194,1 @@\n+  template(tag_loadable_descriptors,                  \"LoadableDescriptors\")                      \\\n@@ -206,0 +231,1 @@\n+  template(java_lang_IdentityException,               \"java\/lang\/IdentityException\")              \\\n@@ -253,0 +279,2 @@\n+  template(jdk_internal_vm_annotation_LooselyConsistentValue_signature,      \"Ljdk\/internal\/vm\/annotation\/LooselyConsistentValue;\") \\\n+  template(jdk_internal_vm_annotation_NullRestricted_signature,              \"Ljdk\/internal\/vm\/annotation\/NullRestricted;\") \\\n@@ -282,1 +310,0 @@\n-  template(trusted_final_name,                        \"trustedFinal\")                             \\\n@@ -504,0 +531,2 @@\n+  template(null_reset_value_name,                     \".null_reset\")                              \\\n+  template(empty_marker_name,                         \".empty\")                                   \\\n@@ -578,0 +607,1 @@\n+  template(class_class_signature,                     \"(Ljava\/lang\/Class;)Ljava\/lang\/Class;\")     \\\n@@ -587,0 +617,1 @@\n+  template(object_object_boolean_signature,           \"(Ljava\/lang\/Object;Ljava\/lang\/Object;)Z\")                  \\\n@@ -714,0 +745,2 @@\n+  template(primaryType_name,                           \"primaryType\")                                             \\\n+  template(secondaryType_name,                         \"secondaryType\")                                           \\\n@@ -741,0 +774,6 @@\n+  template(java_lang_runtime_ValueObjectMethods,            \"java\/lang\/runtime\/ValueObjectMethods\")               \\\n+  template(isSubstitutable_name,                            \"isSubstitutable\")                                    \\\n+  template(valueObjectHashCode_name,                        \"valueObjectHashCode\")                                \\\n+  template(jdk_internal_value_PrimitiveClass,               \"jdk\/internal\/value\/PrimitiveClass\")                  \\\n+  template(jdk_internal_value_ValueClass,                   \"jdk\/internal\/value\/ValueClass\")                      \\\n+                                                                                                                  \\\n@@ -824,0 +863,4 @@\n+  static void initialize_migrated_class_names();\n+\n+  static const int _migrated_class_names_length = 31;\n+  static Symbol* _migrated_class_names[_migrated_class_names_length];\n@@ -859,0 +902,7 @@\n+\n+  template<typename Function>\n+  static void migrated_class_names_do(Function f) {\n+     for (int i = 0; i < _migrated_class_names_length; i++) {\n+       f(_migrated_class_names[i]);\n+     }\n+  }\n","filename":"src\/hotspot\/share\/classfile\/vmSymbols.hpp","additions":52,"deletions":2,"binary":false,"changes":54,"status":"modified"},{"patch":"@@ -723,0 +723,11 @@\n+\n+      \/\/ If inline types are passed as fields, use the extended signature\n+      \/\/ which contains the types of all (oop) fields of the inline type.\n+      if (is_compiled_by_c2() && callee->has_scalarized_args()) {\n+        const GrowableArray<SigEntry>* sig = callee->adapter()->get_sig_cc();\n+        assert(sig != nullptr, \"sig should never be null\");\n+        TempNewSymbol tmp_sig = SigEntry::create_symbol(sig);\n+        has_receiver = false; \/\/ The extended signature contains the receiver type\n+        fr.oops_compiled_arguments_do(tmp_sig, has_receiver, has_appendix, reg_map, f);\n+        return;\n+      }\n@@ -1259,0 +1270,4 @@\n+  _inline_entry_point             = entry_point();\n+  _verified_inline_entry_point    = verified_entry_point();\n+  _verified_inline_ro_entry_point = verified_entry_point();\n+\n@@ -1298,1 +1313,1 @@\n-\n+    assert(!method->has_scalarized_args(), \"scalarized native wrappers not supported yet\");\n@@ -1502,0 +1517,4 @@\n+    _inline_entry_point             = code_begin() + offsets->value(CodeOffsets::Inline_Entry);\n+    _verified_inline_entry_point    = code_begin() + offsets->value(CodeOffsets::Verified_Inline_Entry);\n+    _verified_inline_ro_entry_point = code_begin() + offsets->value(CodeOffsets::Verified_Inline_Entry_RO);\n+\n@@ -3707,0 +3726,1 @@\n+  if (pos == inline_entry_point())                                      label = \"[Inline Entry Point]\";\n@@ -3708,0 +3728,2 @@\n+  if (pos == verified_inline_entry_point())                             label = \"[Verified Inline Entry Point]\";\n+  if (pos == verified_inline_ro_entry_point())                          label = \"[Verified Inline Entry Point (RO)]\";\n@@ -3717,0 +3739,10 @@\n+static int maybe_print_entry_label(outputStream* stream, address pos, address entry, const char* label) {\n+  if (pos == entry) {\n+    stream->bol();\n+    stream->print_cr(\"%s\", label);\n+    return 1;\n+  } else {\n+    return 0;\n+  }\n+}\n+\n@@ -3719,33 +3751,12 @@\n-    const char* label = nmethod_section_label(block_begin);\n-    if (label != nullptr) {\n-      stream->bol();\n-      stream->print_cr(\"%s\", label);\n-    }\n-  }\n-\n-  if (block_begin == entry_point()) {\n-    Method* m = method();\n-    if (m != nullptr) {\n-      stream->print(\"  # \");\n-      m->print_value_on(stream);\n-      stream->cr();\n-    }\n-    if (m != nullptr && !is_osr_method()) {\n-      ResourceMark rm;\n-      int sizeargs = m->size_of_parameters();\n-      BasicType* sig_bt = NEW_RESOURCE_ARRAY(BasicType, sizeargs);\n-      VMRegPair* regs   = NEW_RESOURCE_ARRAY(VMRegPair, sizeargs);\n-      {\n-        int sig_index = 0;\n-        if (!m->is_static())\n-          sig_bt[sig_index++] = T_OBJECT; \/\/ 'this'\n-        for (SignatureStream ss(m->signature()); !ss.at_return_type(); ss.next()) {\n-          BasicType t = ss.type();\n-          sig_bt[sig_index++] = t;\n-          if (type2size[t] == 2) {\n-            sig_bt[sig_index++] = T_VOID;\n-          } else {\n-            assert(type2size[t] == 1, \"size is 1 or 2\");\n-          }\n-        }\n-        assert(sig_index == sizeargs, \"\");\n+    int n = 0;\n+    \/\/ Multiple entry points may be at the same position. Print them all.\n+    n += maybe_print_entry_label(stream, block_begin, entry_point(),                    \"[Entry Point]\");\n+    n += maybe_print_entry_label(stream, block_begin, inline_entry_point(),             \"[Inline Entry Point]\");\n+    n += maybe_print_entry_label(stream, block_begin, verified_entry_point(),           \"[Verified Entry Point]\");\n+    n += maybe_print_entry_label(stream, block_begin, verified_inline_entry_point(),    \"[Verified Inline Entry Point]\");\n+    n += maybe_print_entry_label(stream, block_begin, verified_inline_ro_entry_point(), \"[Verified Inline Entry Point (RO)]\");\n+    if (n == 0) {\n+      const char* label = nmethod_section_label(block_begin);\n+      if (label != nullptr) {\n+        stream->bol();\n+        stream->print_cr(\"%s\", label);\n@@ -3753,54 +3764,73 @@\n-      const char* spname = \"sp\"; \/\/ make arch-specific?\n-      SharedRuntime::java_calling_convention(sig_bt, regs, sizeargs);\n-      int stack_slot_offset = this->frame_size() * wordSize;\n-      int tab1 = 14, tab2 = 24;\n-      int sig_index = 0;\n-      int arg_index = (m->is_static() ? 0 : -1);\n-      bool did_old_sp = false;\n-      for (SignatureStream ss(m->signature()); !ss.at_return_type(); ) {\n-        bool at_this = (arg_index == -1);\n-        bool at_old_sp = false;\n-        BasicType t = (at_this ? T_OBJECT : ss.type());\n-        assert(t == sig_bt[sig_index], \"sigs in sync\");\n-        if (at_this)\n-          stream->print(\"  # this: \");\n-        else\n-          stream->print(\"  # parm%d: \", arg_index);\n-        stream->move_to(tab1);\n-        VMReg fst = regs[sig_index].first();\n-        VMReg snd = regs[sig_index].second();\n-        if (fst->is_reg()) {\n-          stream->print(\"%s\", fst->name());\n-          if (snd->is_valid())  {\n-            stream->print(\":%s\", snd->name());\n-          }\n-        } else if (fst->is_stack()) {\n-          int offset = fst->reg2stack() * VMRegImpl::stack_slot_size + stack_slot_offset;\n-          if (offset == stack_slot_offset)  at_old_sp = true;\n-          stream->print(\"[%s+0x%x]\", spname, offset);\n-        } else {\n-          stream->print(\"reg%d:%d??\", (int)(intptr_t)fst, (int)(intptr_t)snd);\n-        }\n-        stream->print(\" \");\n-        stream->move_to(tab2);\n-        stream->print(\"= \");\n-        if (at_this) {\n-          m->method_holder()->print_value_on(stream);\n-        } else {\n-          bool did_name = false;\n-          if (!at_this && ss.is_reference()) {\n-            Symbol* name = ss.as_symbol();\n-            name->print_value_on(stream);\n-            did_name = true;\n-          }\n-          if (!did_name)\n-            stream->print(\"%s\", type2name(t));\n-        }\n-        if (at_old_sp) {\n-          stream->print(\"  (%s of caller)\", spname);\n-          did_old_sp = true;\n-        }\n-        stream->cr();\n-        sig_index += type2size[t];\n-        arg_index += 1;\n-        if (!at_this)  ss.next();\n+    }\n+  }\n+\n+  Method* m = method();\n+  if (m == nullptr || is_osr_method()) {\n+    return;\n+  }\n+\n+  \/\/ Print the name of the method (only once)\n+  address low = MIN3(entry_point(),\n+                     verified_entry_point(),\n+                     inline_entry_point());\n+  \/\/ The verified inline entry point and verified inline RO entry point are not always\n+  \/\/ used. When they are unused. CodeOffsets::Verified_Inline_Entry(_RO) is -1. Hence,\n+  \/\/ the calculated entry point is smaller than the block they are offsetting into.\n+  if (verified_inline_entry_point() >= block_begin) {\n+    low = MIN2(low, verified_inline_entry_point());\n+  }\n+  if (verified_inline_ro_entry_point() >= block_begin) {\n+    low = MIN2(low, verified_inline_ro_entry_point());\n+  }\n+  assert(low != 0, \"sanity\");\n+  if (block_begin == low) {\n+    stream->print(\"  # \");\n+    m->print_value_on(stream);\n+    stream->cr();\n+  }\n+\n+  \/\/ Print the arguments for the 3 types of verified entry points\n+  CompiledEntrySignature ces(m);\n+  ces.compute_calling_conventions(false);\n+  const GrowableArray<SigEntry>* sig_cc;\n+  const VMRegPair* regs;\n+  if (block_begin == verified_entry_point()) {\n+    sig_cc = ces.sig_cc();\n+    regs = ces.regs_cc();\n+  } else if (block_begin == verified_inline_entry_point()) {\n+    sig_cc = ces.sig();\n+    regs = ces.regs();\n+  } else if (block_begin == verified_inline_ro_entry_point()) {\n+    sig_cc = ces.sig_cc_ro();\n+    regs = ces.regs_cc_ro();\n+  } else {\n+    return;\n+  }\n+\n+  bool has_this = !m->is_static();\n+  if (ces.has_inline_recv() && block_begin == verified_entry_point()) {\n+    \/\/ <this> argument is scalarized for verified_entry_point()\n+    has_this = false;\n+  }\n+  const char* spname = \"sp\"; \/\/ make arch-specific?\n+  int stack_slot_offset = this->frame_size() * wordSize;\n+  int tab1 = 14, tab2 = 24;\n+  int sig_index = 0;\n+  int arg_index = has_this ? -1 : 0;\n+  bool did_old_sp = false;\n+  for (ExtendedSignature sig = ExtendedSignature(sig_cc, SigEntryFilter()); !sig.at_end(); ++sig) {\n+    bool at_this = (arg_index == -1);\n+    bool at_old_sp = false;\n+    BasicType t = (*sig)._bt;\n+    if (at_this) {\n+      stream->print(\"  # this: \");\n+    } else {\n+      stream->print(\"  # parm%d: \", arg_index);\n+    }\n+    stream->move_to(tab1);\n+    VMReg fst = regs[sig_index].first();\n+    VMReg snd = regs[sig_index].second();\n+    if (fst->is_reg()) {\n+      stream->print(\"%s\", fst->name());\n+      if (snd->is_valid())  {\n+        stream->print(\":%s\", snd->name());\n@@ -3808,6 +3838,23 @@\n-      if (!did_old_sp) {\n-        stream->print(\"  # \");\n-        stream->move_to(tab1);\n-        stream->print(\"[%s+0x%x]\", spname, stack_slot_offset);\n-        stream->print(\"  (%s of caller)\", spname);\n-        stream->cr();\n+    } else if (fst->is_stack()) {\n+      int offset = fst->reg2stack() * VMRegImpl::stack_slot_size + stack_slot_offset;\n+      if (offset == stack_slot_offset)  at_old_sp = true;\n+      stream->print(\"[%s+0x%x]\", spname, offset);\n+    } else {\n+      stream->print(\"reg%d:%d??\", (int)(intptr_t)fst, (int)(intptr_t)snd);\n+    }\n+    stream->print(\" \");\n+    stream->move_to(tab2);\n+    stream->print(\"= \");\n+    if (at_this) {\n+      m->method_holder()->print_value_on(stream);\n+    } else {\n+      bool did_name = false;\n+      if (is_reference_type(t)) {\n+        Symbol* name = (*sig)._name;\n+        name->print_value_on(stream);\n+        did_name = true;\n+      }\n+      if (!did_name)\n+        stream->print(\"%s\", type2name(t));\n+      if ((*sig)._null_marker) {\n+        stream->print(\" (null marker)\");\n@@ -3816,0 +3863,14 @@\n+    if (at_old_sp) {\n+      stream->print(\"  (%s of caller)\", spname);\n+      did_old_sp = true;\n+    }\n+    stream->cr();\n+    sig_index += type2size[t];\n+    arg_index += 1;\n+  }\n+  if (!did_old_sp) {\n+    stream->print(\"  # \");\n+    stream->move_to(tab1);\n+    stream->print(\"[%s+0x%x]\", spname, stack_slot_offset);\n+    stream->print(\"  (%s of caller)\", spname);\n+    stream->cr();\n@@ -3939,1 +4000,1 @@\n-      st->print(\" {reexecute=%d rethrow=%d return_oop=%d}\", sd->should_reexecute(), sd->rethrow_exception(), sd->return_oop());\n+      st->print(\" {reexecute=%d rethrow=%d return_oop=%d return_scalarized=%d}\", sd->should_reexecute(), sd->rethrow_exception(), sd->return_oop(), sd->return_scalarized());\n","filename":"src\/hotspot\/share\/code\/nmethod.cpp","additions":156,"deletions":95,"binary":false,"changes":251,"status":"modified"},{"patch":"@@ -30,0 +30,1 @@\n+#include \"compiler\/compilerDefinitions.hpp\"\n@@ -216,0 +217,4 @@\n+  \/\/ TODO: can these be uint16_t, seem rely on -1 CodeOffset, can change later...\n+  address _inline_entry_point;              \/\/ inline type entry point (unpack all inline type args) with class check\n+  address _verified_inline_entry_point;     \/\/ inline type entry point (unpack all inline type args) without class check\n+  address _verified_inline_ro_entry_point;  \/\/ inline type entry point (unpack receiver only) without class check\n@@ -686,0 +691,3 @@\n+  address inline_entry_point() const              { return _inline_entry_point; }             \/\/ inline type entry point (unpack all inline type args)\n+  address verified_inline_entry_point() const     { return _verified_inline_entry_point; }    \/\/ inline type entry point (unpack all inline type args) without class check\n+  address verified_inline_ro_entry_point() const  { return _verified_inline_ro_entry_point; } \/\/ inline type entry point (only unpack receiver) without class check\n@@ -755,0 +763,10 @@\n+  bool  needs_stack_repair() const {\n+    if (is_compiled_by_c1()) {\n+      return method()->c1_needs_stack_repair();\n+    } else if (is_compiled_by_c2()) {\n+      return method()->c2_needs_stack_repair();\n+    } else {\n+      return false;\n+    }\n+  }\n+\n","filename":"src\/hotspot\/share\/code\/nmethod.hpp","additions":18,"deletions":0,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -173,1 +173,1 @@\n-  return to_oop(addr)->is_objArray();\n+  return to_oop(addr)->is_refArray();\n","filename":"src\/hotspot\/share\/gc\/z\/zMark.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -177,0 +177,3 @@\n+JNIEXPORT jboolean JNICALL\n+JVM_IsValhallaEnabled(void);\n+\n@@ -561,0 +564,3 @@\n+JNIEXPORT jboolean JNICALL\n+JVM_IsIdentityClass(JNIEnv *env, jclass cls);\n+\n@@ -1097,0 +1103,21 @@\n+JNIEXPORT jarray JNICALL\n+JVM_CopyOfSpecialArray(JNIEnv *env, jarray orig, jint from, jint to);\n+\n+JNIEXPORT jarray JNICALL\n+JVM_NewNullRestrictedNonAtomicArray(JNIEnv *env, jclass elmClass, jint len, jobject initVal);\n+\n+JNIEXPORT jarray JNICALL\n+JVM_NewNullRestrictedAtomicArray(JNIEnv *env, jclass elmClass, jint len, jobject initVal);\n+\n+JNIEXPORT jarray JNICALL\n+JVM_NewNullableAtomicArray(JNIEnv *env, jclass elmClass, jint len);\n+\n+JNIEXPORT jboolean JNICALL\n+JVM_IsFlatArray(JNIEnv *env, jobject obj);\n+\n+JNIEXPORT jboolean JNICALL\n+JVM_IsNullRestrictedArray(JNIEnv *env, jobject obj);\n+\n+JNIEXPORT jboolean JNICALL\n+JVM_IsAtomicArray(JNIEnv *env, jobject obj);\n+\n","filename":"src\/hotspot\/share\/include\/jvm.h","additions":27,"deletions":0,"binary":false,"changes":27,"status":"modified"},{"patch":"@@ -174,0 +174,1 @@\n+    Interpreter::_throw_NPE_UninitializedField_entry         = generate_exception_handler(\"java\/lang\/NullPointerException\", \"Uninitialized null-restricted field\");\n","filename":"src\/hotspot\/share\/interpreter\/templateInterpreterGenerator.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -1630,1 +1630,1 @@\n-              Deoptimization::reassign_fields(vf->frame_pointer(), &reg_map, objects, realloc_failures, false);\n+              Deoptimization::reassign_fields(vf->frame_pointer(), &reg_map, objects, realloc_failures, false, CHECK_NULL);\n@@ -1881,1 +1881,1 @@\n-  Deoptimization::reassign_fields(fstAfterDeopt.current(), fstAfterDeopt.register_map(), objects, realloc_failures, false);\n+  Deoptimization::reassign_fields(fstAfterDeopt.current(), fstAfterDeopt.register_map(), objects, realloc_failures, false, THREAD);\n@@ -2209,1 +2209,1 @@\n-    if (m->is_object_initializer()) {\n+    if (m->is_object_constructor()) {\n@@ -2236,1 +2236,1 @@\n-    if (!m->is_object_initializer() && !m->is_static_initializer() && !m->is_overpass()) {\n+    if (!(m->is_object_constructor() || m->is_class_initializer()) && !m->is_overpass()) {\n@@ -2975,1 +2975,5 @@\n-  if (m->is_object_initializer()) {\n+  if (m->is_class_initializer()) {\n+      JVMCI_THROW_MSG_NULL(IllegalArgumentException,\n+          \"Cannot create java.lang.reflect.Method for class initializer\");\n+  }\n+  else if (m->is_object_constructor()) {\n@@ -2977,3 +2981,0 @@\n-  } else if (m->is_static_initializer()) {\n-    JVMCI_THROW_MSG_NULL(IllegalArgumentException,\n-        \"Cannot create java.lang.reflect.Method for class initializer\");\n","filename":"src\/hotspot\/share\/jvmci\/jvmciCompilerToVM.cpp","additions":9,"deletions":8,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -222,1 +222,1 @@\n-  nonstatic_field(InstanceKlass,               _misc_flags._flags,                            u2)                                    \\\n+  nonstatic_field(InstanceKlass,               _misc_flags._flags,                            u4)                                    \\\n@@ -738,0 +738,3 @@\n+  declare_constant(DataLayout::array_store_data_tag)                      \\\n+  declare_constant(DataLayout::array_load_data_tag)                       \\\n+  declare_constant(DataLayout::acmp_data_tag)                             \\\n@@ -821,1 +824,1 @@\n-  declare_constant(Klass::_lh_array_tag_obj_value)                        \\\n+  declare_constant(Klass::_lh_array_tag_ref_value)                        \\\n","filename":"src\/hotspot\/share\/jvmci\/vmStructs_jvmci.cpp","additions":5,"deletions":2,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -56,0 +56,1 @@\n+#include \"oops\/flatArrayKlass.hpp\"\n@@ -61,0 +62,1 @@\n+#include \"oops\/refArrayOop.hpp\"\n@@ -194,1 +196,1 @@\n-  return resolved_references()->replace_if_null(index, new_result);\n+  return refArrayOopDesc::cast(resolved_references())->replace_if_null(index, new_result);\n@@ -265,1 +267,1 @@\n-      \/\/ All of these should have been reverted back to ClassIndex before calling\n+      \/\/ All of these should have been reverted back to Unresolved before calling\n@@ -481,0 +483,1 @@\n+    assert(src_k->is_instance_klass() || src_k->is_typeArray_klass(), \"Sanity check\");\n@@ -622,0 +625,6 @@\n+void check_is_inline_type(Klass* k, TRAPS) {\n+  if (!k->is_inline_klass()) {\n+    THROW(vmSymbols::java_lang_IncompatibleClassChangeError());\n+  }\n+}\n+\n@@ -659,0 +668,1 @@\n+  bool inline_type_signature = false;\n@@ -667,0 +677,3 @@\n+  if (inline_type_signature) {\n+    name->decrement_refcount();\n+  }\n@@ -675,0 +688,16 @@\n+  if (!HAS_PENDING_EXCEPTION && inline_type_signature) {\n+    check_is_inline_type(k, THREAD);\n+  }\n+\n+  if (!HAS_PENDING_EXCEPTION) {\n+    Klass* bottom_klass = nullptr;\n+    if (k->is_objArray_klass()) {\n+      bottom_klass = ObjArrayKlass::cast(k)->bottom_klass();\n+      assert(bottom_klass != nullptr, \"Should be set\");\n+      assert(bottom_klass->is_instance_klass() || bottom_klass->is_typeArray_klass(), \"Sanity check\");\n+    } else if (k->is_flatArray_klass()) {\n+      bottom_klass = FlatArrayKlass::cast(k)->element_klass();\n+      assert(bottom_klass != nullptr, \"Should be set\");\n+    }\n+  }\n+\n","filename":"src\/hotspot\/share\/oops\/constantPool.cpp","additions":31,"deletions":2,"binary":false,"changes":33,"status":"modified"},{"patch":"@@ -306,1 +306,1 @@\n-  \/\/ For temporary use while constructing constant pool\n+  \/\/ For temporary use while constructing constant pool. Used during a retransform\/class redefinition as well.\n","filename":"src\/hotspot\/share\/oops\/constantPool.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -72,0 +72,1 @@\n+#include \"oops\/markWord.hpp\"\n@@ -76,0 +77,2 @@\n+#include \"oops\/inlineKlass.hpp\"\n+#include \"oops\/refArrayKlass.hpp\"\n@@ -152,0 +155,5 @@\n+void InlineLayoutInfo::metaspace_pointers_do(MetaspaceClosure* it) {\n+  log_trace(cds)(\"Iter(InlineFieldInfo): %p\", this);\n+  it->push(&_klass);\n+}\n+\n@@ -173,0 +181,13 @@\n+bool InstanceKlass::field_is_null_free_inline_type(int index) const {\n+  return field(index).field_flags().is_null_free_inline_type();\n+}\n+\n+bool InstanceKlass::is_class_in_loadable_descriptors_attribute(Symbol* name) const {\n+  if (_loadable_descriptors == nullptr) return false;\n+  for (int i = 0; i < _loadable_descriptors->length(); i++) {\n+        Symbol* class_name = _constants->symbol_at(_loadable_descriptors->at(i));\n+        if (class_name == name) return true;\n+  }\n+  return false;\n+}\n+\n@@ -467,1 +488,2 @@\n-                                       parser.is_interface());\n+                                       parser.is_interface(),\n+                                       parser.is_inline_type());\n@@ -490,0 +512,3 @@\n+  } else if (parser.is_inline_type()) {\n+    \/\/ inline type\n+    ik = new (loader_data, size, use_class_space, THREAD) InlineKlass(parser);\n@@ -506,0 +531,6 @@\n+#ifdef ASSERT\n+  ik->bounds_check((address) ik->start_of_vtable(), false, size);\n+  ik->bounds_check((address) ik->start_of_itable(), false, size);\n+  ik->bounds_check((address) ik->end_of_itable(), true, size);\n+  ik->bounds_check((address) ik->end_of_nonstatic_oop_maps(), true, size);\n+#endif \/\/ASSERT\n@@ -509,0 +540,23 @@\n+#ifndef PRODUCT\n+bool InstanceKlass::bounds_check(address addr, bool edge_ok, intptr_t size_in_bytes) const {\n+  const char* bad = nullptr;\n+  address end = nullptr;\n+  if (addr < (address)this) {\n+    bad = \"before\";\n+  } else if (addr == (address)this) {\n+    if (edge_ok)  return true;\n+    bad = \"just before\";\n+  } else if (addr == (end = (address)this + sizeof(intptr_t) * (size_in_bytes < 0 ? size() : size_in_bytes))) {\n+    if (edge_ok)  return true;\n+    bad = \"just after\";\n+  } else if (addr > end) {\n+    bad = \"after\";\n+  } else {\n+    return true;\n+  }\n+  tty->print_cr(\"%s object bounds: \" INTPTR_FORMAT \" [\" INTPTR_FORMAT \"..\" INTPTR_FORMAT \"]\",\n+      bad, (intptr_t)addr, (intptr_t)this, (intptr_t)end);\n+  Verbose = WizardMode = true; this->print(); \/\/@@\n+  return false;\n+}\n+#endif \/\/PRODUCT\n@@ -536,2 +590,2 @@\n-InstanceKlass::InstanceKlass(const ClassFileParser& parser, KlassKind kind, ReferenceType reference_type) :\n-  Klass(kind),\n+InstanceKlass::InstanceKlass(const ClassFileParser& parser, KlassKind kind, markWord prototype_header, ReferenceType reference_type) :\n+  Klass(kind, prototype_header),\n@@ -548,1 +602,4 @@\n-  _init_thread(nullptr)\n+  _init_thread(nullptr),\n+  _inline_layout_info_array(nullptr),\n+  _loadable_descriptors(nullptr),\n+  _adr_inlineklass_fixed_block(nullptr)\n@@ -555,0 +612,3 @@\n+  if (parser.has_inline_fields()) {\n+    set_has_inline_type_fields();\n+  }\n@@ -699,0 +759,5 @@\n+  if (inline_layout_info_array() != nullptr) {\n+    MetadataFactory::free_array<InlineLayoutInfo>(loader_data, inline_layout_info_array());\n+  }\n+  set_inline_layout_info_array(nullptr);\n+\n@@ -733,0 +798,7 @@\n+  if (loadable_descriptors() != nullptr &&\n+      loadable_descriptors() != Universe::the_empty_short_array() &&\n+      !loadable_descriptors()->is_shared()) {\n+    MetadataFactory::free_array<jushort>(loader_data, loadable_descriptors());\n+  }\n+  set_loadable_descriptors(nullptr);\n+\n@@ -972,0 +1044,101 @@\n+\n+  \/\/ If a class declares a method that uses an inline class as an argument\n+  \/\/ type or return inline type, this inline class must be loaded during the\n+  \/\/ linking of this class because size and properties of the inline class\n+  \/\/ must be known in order to be able to perform inline type optimizations.\n+  \/\/ The implementation below is an approximation of this rule, the code\n+  \/\/ iterates over all methods of the current class (including overridden\n+  \/\/ methods), not only the methods declared by this class. This\n+  \/\/ approximation makes the code simpler, and doesn't change the semantic\n+  \/\/ because classes declaring methods overridden by the current class are\n+  \/\/ linked (and have performed their own pre-loading) before the linking\n+  \/\/ of the current class.\n+\n+\n+  \/\/ Note:\n+  \/\/ Inline class types are loaded during\n+  \/\/ the loading phase (see ClassFileParser::post_process_parsed_stream()).\n+  \/\/ Inline class types used as element types for array creation\n+  \/\/ are not pre-loaded. Their loading is triggered by either anewarray\n+  \/\/ or multianewarray bytecodes.\n+\n+  \/\/ Could it be possible to do the following processing only if the\n+  \/\/ class uses inline types?\n+  if (EnableValhalla) {\n+    ResourceMark rm(THREAD);\n+    for (AllFieldStream fs(this); !fs.done(); fs.next()) {\n+      if (fs.is_null_free_inline_type() && fs.access_flags().is_static()) {\n+        assert(fs.access_flags().is_strict(), \"null-free fields must be strict\");\n+        Symbol* sig = fs.signature();\n+        TempNewSymbol s = Signature::strip_envelope(sig);\n+        if (s != name()) {\n+          log_info(class, preload)(\"Preloading of class %s during linking of class %s. Cause: a null-free static field is declared with this type\", s->as_C_string(), name()->as_C_string());\n+          Klass* klass = SystemDictionary::resolve_or_fail(s,\n+                                                          Handle(THREAD, class_loader()), true,\n+                                                          CHECK_false);\n+          if (HAS_PENDING_EXCEPTION) {\n+            log_warning(class, preload)(\"Preloading of class %s during linking of class %s (cause: null-free static field) failed: %s\",\n+                                      s->as_C_string(), name()->as_C_string(), PENDING_EXCEPTION->klass()->name()->as_C_string());\n+            return false; \/\/ Exception is still pending\n+          }\n+          log_info(class, preload)(\"Preloading of class %s during linking of class %s (cause: null-free static field) succeeded\",\n+                                   s->as_C_string(), name()->as_C_string());\n+          assert(klass != nullptr, \"Sanity check\");\n+          if (klass->is_abstract()) {\n+            THROW_MSG_(vmSymbols::java_lang_IncompatibleClassChangeError(),\n+                      err_msg(\"Class %s expects class %s to be concrete value class, but it is an abstract class\",\n+                      name()->as_C_string(),\n+                      InstanceKlass::cast(klass)->external_name()), false);\n+          }\n+          if (!klass->is_inline_klass()) {\n+            THROW_MSG_(vmSymbols::java_lang_IncompatibleClassChangeError(),\n+                       err_msg(\"class %s expects class %s to be a value class but it is an identity class\",\n+                       name()->as_C_string(), klass->external_name()), false);\n+          }\n+          InlineKlass* vk = InlineKlass::cast(klass);\n+          \/\/ the inline_type_field_klasses_array might have been loaded with CDS, so update only if not already set and check consistency\n+          InlineLayoutInfo* li = inline_layout_info_adr(fs.index());\n+          if (li->klass() == nullptr) {\n+            li->set_klass(InlineKlass::cast(vk));\n+            li->set_kind(LayoutKind::REFERENCE);\n+          }\n+          assert(get_inline_type_field_klass(fs.index()) == vk, \"Must match\");\n+        } else {\n+          InlineLayoutInfo* li = inline_layout_info_adr(fs.index());\n+          if (li->klass() == nullptr) {\n+            li->set_klass(InlineKlass::cast(this));\n+            li->set_kind(LayoutKind::REFERENCE);\n+          }\n+          assert(get_inline_type_field_klass(fs.index()) == this, \"Must match\");\n+        }\n+      }\n+    }\n+\n+    \/\/ Aggressively preloading all classes from the LoadableDescriptors attribute\n+    if (loadable_descriptors() != nullptr && PreloadClasses) {\n+      HandleMark hm(THREAD);\n+      for (int i = 0; i < loadable_descriptors()->length(); i++) {\n+        Symbol* sig = constants()->symbol_at(loadable_descriptors()->at(i));\n+        if (!Signature::has_envelope(sig)) continue;\n+        TempNewSymbol class_name = Signature::strip_envelope(sig);\n+        if (class_name == name()) continue;\n+        log_info(class, preload)(\"Preloading of class %s during linking of class %s because of the class is listed in the LoadableDescriptors attribute\", sig->as_C_string(), name()->as_C_string());\n+        oop loader = class_loader();\n+        Klass* klass = SystemDictionary::resolve_or_null(class_name,\n+                                                         Handle(THREAD, loader), THREAD);\n+        if (HAS_PENDING_EXCEPTION) {\n+          CLEAR_PENDING_EXCEPTION;\n+        }\n+        if (klass != nullptr) {\n+          log_info(class, preload)(\"Preloading of class %s during linking of class %s (cause: LoadableDescriptors attribute) succeeded\", class_name->as_C_string(), name()->as_C_string());\n+          if (!klass->is_inline_klass()) {\n+            \/\/ Non value class are allowed by the current spec, but it could be an indication of an issue so let's log a warning\n+            log_warning(class, preload)(\"Preloading of class %s during linking of class %s (cause: LoadableDescriptors attribute) but loaded class is not a value class\", class_name->as_C_string(), name()->as_C_string());\n+          }\n+        } else {\n+          log_warning(class, preload)(\"Preloading of class %s during linking of class %s (cause: LoadableDescriptors attribute) failed\", class_name->as_C_string(), name()->as_C_string());\n+        }\n+      }\n+    }\n+  }\n+\n@@ -1276,0 +1449,21 @@\n+  \/\/ Pre-allocating an all-zero value to be used to reset nullable flat storages\n+  if (is_inline_klass()) {\n+      InlineKlass* vk = InlineKlass::cast(this);\n+      if (vk->has_nullable_atomic_layout()) {\n+        oop val = vk->allocate_instance(THREAD);\n+        if (HAS_PENDING_EXCEPTION) {\n+            Handle e(THREAD, PENDING_EXCEPTION);\n+            CLEAR_PENDING_EXCEPTION;\n+            {\n+                EXCEPTION_MARK;\n+                add_initialization_error(THREAD, e);\n+                \/\/ Locks object, set state, and notify all waiting threads\n+                set_initialization_state_and_notify(initialization_error, THREAD);\n+                CLEAR_PENDING_EXCEPTION;\n+            }\n+            THROW_OOP(e());\n+        }\n+        vk->set_null_reset_value(val);\n+      }\n+  }\n+\n@@ -1308,1 +1502,0 @@\n-\n@@ -1329,0 +1522,30 @@\n+\n+    if (has_strict_static_fields() && !HAS_PENDING_EXCEPTION) {\n+      \/\/ Step 9 also verifies that strict static fields have been initialized.\n+      \/\/ Status bits were set in ClassFileParser::post_process_parsed_stream.\n+      \/\/ After <clinit>, bits must all be clear, or else we must throw an error.\n+      \/\/ This is an extremely fast check, so we won't bother with a timer.\n+      assert(fields_status() != nullptr, \"\");\n+      Symbol* bad_strict_static = nullptr;\n+      for (int index = 0; index < fields_status()->length(); index++) {\n+        \/\/ Very fast loop over single byte array looking for a set bit.\n+        if (fields_status()->adr_at(index)->is_strict_static_unset()) {\n+          \/\/ This strict static field has not been set by the class initializer.\n+          \/\/ Note that in the common no-error case, we read no field metadata.\n+          \/\/ We only unpack it when we need to report an error.\n+          FieldInfo fi = field(index);\n+          bad_strict_static = fi.name(constants());\n+          if (debug_logging_enabled) {\n+            ResourceMark rm(jt);\n+            const char* msg = format_strict_static_message(bad_strict_static);\n+            log_debug(class, init)(\"%s\", msg);\n+          } else {\n+            \/\/ If we are not logging, do not bother to look for a second offense.\n+            break;\n+          }\n+        }\n+      }\n+      if (bad_strict_static != nullptr) {\n+        throw_strict_static_exception(bad_strict_static, \"is unset after initialization of\", THREAD);\n+      }\n+    }\n@@ -1382,0 +1605,68 @@\n+void InstanceKlass::notify_strict_static_access(int field_index, bool is_writing, TRAPS) {\n+  guarantee(field_index >= 0 && field_index < fields_status()->length(), \"valid field index\");\n+  DEBUG_ONLY(FieldInfo debugfi = field(field_index));\n+  assert(debugfi.access_flags().is_strict(), \"\");\n+  assert(debugfi.access_flags().is_static(), \"\");\n+  FieldStatus& fs = *fields_status()->adr_at(field_index);\n+  LogTarget(Trace, class, init) lt;\n+  if (lt.is_enabled()) {\n+    ResourceMark rm(THREAD);\n+    LogStream ls(lt);\n+    FieldInfo fi = field(field_index);\n+    ls.print(\"notify %s %s %s%s \",\n+             external_name(), is_writing? \"Write\" : \"Read\",\n+             fs.is_strict_static_unset() ? \"Unset\" : \"(set)\",\n+             fs.is_strict_static_unread() ? \"+Unread\" : \"\");\n+    fi.print(&ls, constants());\n+  }\n+  if (fs.is_strict_static_unset()) {\n+    assert(fs.is_strict_static_unread(), \"ClassFileParser resp.\");\n+    \/\/ If it is not set, there are only two reasonable things we can do here:\n+    \/\/ - mark it set if this is putstatic\n+    \/\/ - throw an error (Read-Before-Write) if this is getstatic\n+\n+    \/\/ The unset state is (or should be) transient, and observable only in one\n+    \/\/ thread during the execution of <clinit>.  Something is wrong here as this\n+    \/\/ should not be possible\n+    guarantee(is_reentrant_initialization(THREAD), \"unscoped access to strict static\");\n+    if (is_writing) {\n+      \/\/ clear the \"unset\" bit, since the field is actually going to be written\n+      fs.update_strict_static_unset(false);\n+    } else {\n+      \/\/ throw an IllegalStateException, since we are reading before writing\n+      \/\/ see also InstanceKlass::initialize_impl, Step 8 (at end)\n+      Symbol* bad_strict_static = field(field_index).name(constants());\n+      throw_strict_static_exception(bad_strict_static, \"is unset before first read in\", CHECK);\n+    }\n+  } else {\n+    \/\/ Ensure no write after read for final strict statics\n+    FieldInfo fi = field(field_index);\n+    bool is_final = fi.access_flags().is_final();\n+    if (is_final) {\n+      \/\/ no final write after read, so observing a constant freezes it, as if <clinit> ended early\n+      \/\/ (maybe we could trust the constant a little earlier, before <clinit> ends)\n+      if (is_writing && !fs.is_strict_static_unread()) {\n+        Symbol* bad_strict_static = fi.name(constants());\n+        throw_strict_static_exception(bad_strict_static, \"is set after read (as final) in\", CHECK);\n+      } else if (!is_writing && fs.is_strict_static_unread()) {\n+        fs.update_strict_static_unread(false);\n+      }\n+    }\n+  }\n+}\n+\n+void InstanceKlass::throw_strict_static_exception(Symbol* field_name, const char* when, TRAPS) {\n+  ResourceMark rm(THREAD);\n+  const char* msg = format_strict_static_message(field_name, when);\n+  THROW_MSG(vmSymbols::java_lang_IllegalStateException(), msg);\n+}\n+\n+const char* InstanceKlass::format_strict_static_message(Symbol* field_name, const char* when) {\n+  stringStream ss;\n+  ss.print(\"Strict static \\\"%s\\\" %s %s\",\n+           field_name->as_C_string(),\n+           when == nullptr ? \"is unset in\" : when,\n+           external_name());\n+  return ss.as_string();\n+}\n+\n@@ -1565,7 +1856,3 @@\n-objArrayOop InstanceKlass::allocate_objArray(int n, int length, TRAPS) {\n-  check_array_allocation_length(length, arrayOopDesc::max_array_length(T_OBJECT), CHECK_NULL);\n-  size_t size = objArrayOopDesc::object_size(length);\n-  ArrayKlass* ak = array_klass(n, CHECK_NULL);\n-  objArrayOop o = (objArrayOop)Universe::heap()->array_allocate(ak, size, length,\n-                                                                \/* do_zero *\/ true, CHECK_NULL);\n-  return o;\n+objArrayOop InstanceKlass::allocate_objArray(int length, ArrayKlass::ArrayProperties props, TRAPS) {\n+  ArrayKlass* ak = array_klass(CHECK_NULL);\n+  return ObjArrayKlass::cast(ak)->allocate_instance(length, props, CHECK_NULL);\n@@ -1641,1 +1928,1 @@\n-  ObjArrayKlass* ak = array_klasses();\n+  ArrayKlass* ak = array_klasses();\n@@ -1648,2 +1935,2 @@\n-  ObjArrayKlass* oak = array_klasses_acquire();\n-  if (oak == nullptr) {\n+  ArrayKlass* ak = array_klasses_acquire();\n+  if (ak == nullptr) {\n@@ -1652,1 +1939,1 @@\n-    return oak->array_klass_or_null(n);\n+    return ak->array_klass_or_null(n);\n@@ -1669,1 +1956,1 @@\n-  if (clinit != nullptr && clinit->has_valid_initializer_flags()) {\n+  if (clinit != nullptr && clinit->is_class_initializer()) {\n@@ -1778,4 +2065,0 @@\n-bool InstanceKlass::contains_field_offset(int offset) {\n-  fieldDescriptor fd;\n-  return find_field_from_offset(offset, false, &fd);\n-}\n@@ -1862,0 +2145,9 @@\n+bool InstanceKlass::contains_field_offset(int offset) {\n+  if (this->is_inline_klass()) {\n+    InlineKlass* vk = InlineKlass::cast(this);\n+    return offset >= vk->payload_offset() && offset < (vk->payload_offset() + vk->payload_size_in_bytes());\n+  } else {\n+    fieldDescriptor fd;\n+    return find_field_from_offset(offset, false, &fd);\n+  }\n+}\n@@ -2245,0 +2537,3 @@\n+    if (name == vmSymbols::object_initializer_name()) {\n+      break;  \/\/ <init> is never inherited\n+    }\n@@ -2644,0 +2939,1 @@\n+  it->push(&_loadable_descriptors);\n@@ -2645,0 +2941,1 @@\n+  it->push(&_inline_layout_info_array, MetaspaceClosure::_writable);\n@@ -2692,1 +2989,1 @@\n-  \/\/ These are not allocated from metaspace. They are safe to set to null.\n+  \/\/ These are not allocated from metaspace. They are safe to set to nullptr.\n@@ -2783,0 +3080,4 @@\n+  if (is_inline_klass()) {\n+    InlineKlass::cast(this)->initialize_calling_convention(CHECK);\n+  }\n+\n@@ -2816,1 +3117,1 @@\n-    assert(this == array_klasses()->bottom_klass(), \"sanity\");\n+    assert(this == ObjArrayKlass::cast(array_klasses())->bottom_klass(), \"sanity\");\n@@ -2819,0 +3120,6 @@\n+    if (class_loader_data() == nullptr) {\n+      ResourceMark rm(THREAD);\n+      log_debug(cds)(\"  loader_data %s \", loader_data == nullptr ? \"nullptr\" : \"non null\");\n+      log_debug(cds)(\"  this %s array_klasses %s \", this->name()->as_C_string(), array_klasses()->name()->as_C_string());\n+    }\n+    assert(!array_klasses()->is_refined_objArray_klass(), \"must be non-refined objarrayklass\");\n@@ -2972,0 +3279,4 @@\n+bool InstanceKlass::supports_inline_types() const {\n+  return major_version() >= Verifier::VALUE_TYPES_MAJOR_VERSION && minor_version() == Verifier::JAVA_PREVIEW_MINOR_VERSION;\n+}\n+\n@@ -3004,0 +3315,2 @@\n+  return signature_name_of_carrier(JVM_SIGNATURE_CLASS);\n+}\n@@ -3005,0 +3318,1 @@\n+const char* InstanceKlass::signature_name_of_carrier(char c) const {\n@@ -3011,1 +3325,1 @@\n-  \/\/ Add L as type indicator\n+  \/\/ Add L or Q as type indicator\n@@ -3013,1 +3327,1 @@\n-  dest[dest_index++] = JVM_SIGNATURE_CLASS;\n+  dest[dest_index++] = c;\n@@ -3294,0 +3608,19 @@\n+void InstanceKlass::check_can_be_annotated_with_NullRestricted(InstanceKlass* type, Symbol* container_klass_name, TRAPS) {\n+  assert(type->is_instance_klass(), \"Sanity check\");\n+  if (type->is_identity_class()) {\n+    ResourceMark rm(THREAD);\n+    THROW_MSG(vmSymbols::java_lang_IncompatibleClassChangeError(),\n+              err_msg(\"Class %s expects class %s to be a value class, but it is an identity class\",\n+              container_klass_name->as_C_string(),\n+              type->external_name()));\n+  }\n+\n+  if (type->is_abstract()) {\n+    ResourceMark rm(THREAD);\n+    THROW_MSG(vmSymbols::java_lang_IncompatibleClassChangeError(),\n+              err_msg(\"Class %s expects class %s to be concrete value type, but it is an abstract class\",\n+              container_klass_name->as_C_string(),\n+              type->external_name()));\n+  }\n+}\n+\n@@ -3360,2 +3693,1 @@\n-  \/\/ Remember to strip ACC_SUPER bit\n-  return (access & (~JVM_ACC_SUPER));\n+  return access;\n@@ -3615,1 +3947,4 @@\n-static void print_vtable(intptr_t* start, int len, outputStream* st) {\n+static void print_vtable(address self, intptr_t* start, int len, outputStream* st) {\n+  ResourceMark rm;\n+  int* forward_refs = NEW_RESOURCE_ARRAY(int, len);\n+  for (int i = 0; i < len; i++)  forward_refs[i] = 0;\n@@ -3619,0 +3954,5 @@\n+    if (forward_refs[i] != 0) {\n+      int from = forward_refs[i];\n+      int off = (int) start[from];\n+      st->print(\" (offset %d <= [%d])\", off, from);\n+    }\n@@ -3622,0 +3962,6 @@\n+    } else if (self != nullptr && e > 0 && e < 0x10000) {\n+      address location = self + e;\n+      int index = (int)((intptr_t*)location - start);\n+      st->print(\" (offset %d => [%d])\", (int)e, index);\n+      if (index >= 0 && index < len)\n+        forward_refs[index] = i;\n@@ -3628,1 +3974,22 @@\n-  return print_vtable(reinterpret_cast<intptr_t*>(start), len, st);\n+  return print_vtable(nullptr, reinterpret_cast<intptr_t*>(start), len, st);\n+}\n+\n+template<typename T>\n+ static void print_array_on(outputStream* st, Array<T>* array) {\n+   if (array == nullptr) { st->print_cr(\"nullptr\"); return; }\n+   array->print_value_on(st); st->cr();\n+   if (Verbose || WizardMode) {\n+     for (int i = 0; i < array->length(); i++) {\n+       st->print(\"%d : \", i); array->at(i)->print_value_on(st); st->cr();\n+     }\n+   }\n+ }\n+\n+static void print_array_on(outputStream* st, Array<int>* array) {\n+  if (array == nullptr) { st->print_cr(\"nullptr\"); return; }\n+  array->print_value_on(st); st->cr();\n+  if (Verbose || WizardMode) {\n+    for (int i = 0; i < array->length(); i++) {\n+      st->print(\"%d : %d\", i, array->at(i)); st->cr();\n+    }\n+  }\n@@ -3669,8 +4036,2 @@\n-  st->print(BULLET\"methods:           \"); methods()->print_value_on(st);               st->cr();\n-  if (Verbose || WizardMode) {\n-    Array<Method*>* method_array = methods();\n-    for (int i = 0; i < method_array->length(); i++) {\n-      st->print(\"%d : \", i); method_array->at(i)->print_value(); st->cr();\n-    }\n-  }\n-  st->print(BULLET\"method ordering:   \"); method_ordering()->print_value_on(st);      st->cr();\n+  st->print(BULLET\"methods:           \"); print_array_on(st, methods());\n+  st->print(BULLET\"method ordering:   \"); print_array_on(st, method_ordering());\n@@ -3678,7 +4039,1 @@\n-    st->print(BULLET\"default_methods:   \"); default_methods()->print_value_on(st);    st->cr();\n-    if (Verbose) {\n-      Array<Method*>* method_array = default_methods();\n-      for (int i = 0; i < method_array->length(); i++) {\n-        st->print(\"%d : \", i); method_array->at(i)->print_value(); st->cr();\n-      }\n-    }\n+    st->print(BULLET\"default_methods:   \"); print_array_on(st, default_methods());\n@@ -3744,0 +4099,1 @@\n+  st->print(BULLET\"loadable descriptors:     \"); loadable_descriptors()->print_value_on(st); st->cr();\n@@ -3754,1 +4110,1 @@\n-  if (itable_length() > 0 && (Verbose || WizardMode))  print_vtable(start_of_itable(), itable_length(), st);\n+  if (itable_length() > 0 && (Verbose || WizardMode))  print_vtable(nullptr, start_of_itable(), itable_length(), st);\n@@ -3786,0 +4142,1 @@\n+  for (int i = 0; i < _indent; i++) _st->print(\"  \");\n@@ -3788,1 +4145,1 @@\n-     fd->print_on(_st);\n+     fd->print_on(_st, _base_offset);\n@@ -3791,2 +4148,2 @@\n-     fd->print_on_for(_st, _obj);\n-     _st->cr();\n+     fd->print_on_for(_st, _obj, _indent, _base_offset);\n+     if (!fd->field_flags().is_flat()) _st->cr();\n@@ -3797,1 +4154,1 @@\n-void InstanceKlass::oop_print_on(oop obj, outputStream* st) {\n+void InstanceKlass::oop_print_on(oop obj, outputStream* st, int indent, int base_offset) {\n@@ -3813,1 +4170,1 @@\n-  FieldPrinter print_field(st, obj);\n+  FieldPrinter print_field(st, obj, indent, base_offset);\n","filename":"src\/hotspot\/share\/oops\/instanceKlass.cpp","additions":407,"deletions":50,"binary":false,"changes":457,"status":"modified"},{"patch":"@@ -63,0 +63,1 @@\n+#include \"oops\/inlineKlass.inline.hpp\"\n@@ -124,1 +125,0 @@\n-\n@@ -163,0 +163,5 @@\n+address Method::get_c2i_inline_entry() {\n+  assert(adapter() != nullptr, \"must have\");\n+  return adapter()->get_c2i_inline_entry();\n+}\n+\n@@ -168,0 +173,5 @@\n+address Method::get_c2i_unverified_inline_entry() {\n+  assert(adapter() != nullptr, \"must have\");\n+  return adapter()->get_c2i_unverified_inline_entry();\n+}\n+\n@@ -393,1 +403,1 @@\n-  if (!method_holder()->is_rewritten()) {\n+  if (!method_holder()->is_rewritten() || CDSConfig::is_valhalla_preview()) {\n@@ -436,0 +446,2 @@\n+    _from_compiled_inline_entry = _adapter->get_c2i_inline_entry();\n+    _from_compiled_inline_ro_entry = _adapter->get_c2i_inline_ro_entry();\n@@ -725,0 +737,14 @@\n+\/\/ InlineKlass the method is declared to return. This must not\n+\/\/ safepoint as it is called with references live on the stack at\n+\/\/ locations the GC is unaware of.\n+InlineKlass* Method::returns_inline_type() const {\n+  assert(InlineTypeReturnedAsFields, \"Inline types should never be returned as fields\");\n+  if (is_native()) {\n+    return nullptr;\n+  }\n+  NoSafepointVerifier nsv;\n+  SignatureStream ss(signature());\n+  ss.skip_to_return_type();\n+  return ss.as_inline_klass(method_holder());\n+}\n+\n@@ -873,0 +899,5 @@\n+  if (has_scalarized_return()) {\n+    \/\/ Don't treat this as (trivial) getter method because the\n+    \/\/ inline type should be returned in a scalarized form.\n+    return false;\n+  }\n@@ -894,0 +925,5 @@\n+  if (has_scalarized_args()) {\n+    \/\/ Don't treat this as (trivial) setter method because the\n+    \/\/ inline type argument should be passed in a scalarized form.\n+    return false;\n+  }\n@@ -904,6 +940,2 @@\n-          Bytecodes::is_return(java_code_at(last_index)));\n-}\n-\n-bool Method::has_valid_initializer_flags() const {\n-  return (is_static() ||\n-          method_holder()->major_version() < 51);\n+          Bytecodes::is_return(java_code_at(last_index)) &&\n+          !has_scalarized_args());\n@@ -912,1 +944,1 @@\n-bool Method::is_static_initializer() const {\n+bool Method::is_class_initializer() const {\n@@ -916,2 +948,3 @@\n-  return name() == vmSymbols::class_initializer_name() &&\n-         has_valid_initializer_flags();\n+  return (name() == vmSymbols::class_initializer_name() &&\n+          (is_static() ||\n+           method_holder()->major_version() < 51));\n@@ -920,2 +953,3 @@\n-bool Method::is_object_initializer() const {\n-   return name() == vmSymbols::object_initializer_name();\n+\/\/ A method named <init>, is a classic object constructor.\n+bool Method::is_object_constructor() const {\n+  return name() == vmSymbols::object_initializer_name();\n@@ -984,1 +1018,1 @@\n-  if( constants()->tag_at(klass_index).is_unresolved_klass() ) {\n+  if( constants()->tag_at(klass_index).is_unresolved_klass()) {\n@@ -999,1 +1033,3 @@\n-    if (constants()->tag_at(klass_index).is_unresolved_klass()) return false;\n+    if (constants()->tag_at(klass_index).is_unresolved_klass()) {\n+      return false;\n+    }\n@@ -1168,0 +1204,2 @@\n+    _from_compiled_inline_entry = nullptr;\n+    _from_compiled_inline_ro_entry = nullptr;\n@@ -1170,0 +1208,2 @@\n+    _from_compiled_inline_entry = adapter()->get_c2i_inline_entry();\n+    _from_compiled_inline_ro_entry = adapter()->get_c2i_inline_ro_entry();\n@@ -1203,0 +1243,2 @@\n+  _from_compiled_inline_entry = nullptr;\n+  _from_compiled_inline_ro_entry = nullptr;\n@@ -1234,0 +1276,2 @@\n+  set_has_scalarized_args(false);\n+  set_has_scalarized_return(false);\n@@ -1270,0 +1314,3 @@\n+  if (InlineTypeReturnedAsFields && returns_inline_type() && !has_scalarized_return()) {\n+    set_has_scalarized_return();\n+  }\n@@ -1321,0 +1368,2 @@\n+  mh->_from_compiled_inline_entry = adapter->get_c2i_inline_entry();\n+  mh->_from_compiled_inline_ro_entry = adapter->get_c2i_inline_ro_entry();\n@@ -1337,0 +1386,12 @@\n+address Method::verified_inline_code_entry() {\n+  DEBUG_ONLY(NoSafepointVerifier nsv;)\n+  assert(_from_compiled_inline_entry != nullptr, \"must be set\");\n+  return _from_compiled_inline_entry;\n+}\n+\n+address Method::verified_inline_ro_code_entry() {\n+  DEBUG_ONLY(NoSafepointVerifier nsv;)\n+  assert(_from_compiled_inline_ro_entry != nullptr, \"must be set\");\n+  return _from_compiled_inline_ro_entry;\n+}\n+\n@@ -1368,0 +1429,2 @@\n+  mh->_from_compiled_inline_entry = code->verified_inline_entry_point();\n+  mh->_from_compiled_inline_ro_entry = code->verified_inline_ro_entry_point();\n@@ -1560,0 +1623,2 @@\n+    m->set_from_compiled_inline_entry(m->adapter()->get_c2i_inline_entry());\n+    m->set_from_compiled_inline_ro_entry(m->adapter()->get_c2i_inline_ro_entry());\n@@ -2326,0 +2391,25 @@\n+bool Method::is_scalarized_arg(int idx) const {\n+  if (!has_scalarized_args()) {\n+    return false;\n+  }\n+  \/\/ Search through signature and check if argument is wrapped in T_METADATA\/T_VOID\n+  int depth = 0;\n+  const GrowableArray<SigEntry>* sig = adapter()->get_sig_cc();\n+  for (int i = 0; i < sig->length(); i++) {\n+    BasicType bt = sig->at(i)._bt;\n+    if (bt == T_METADATA) {\n+      depth++;\n+    }\n+    if (idx == 0) {\n+      break; \/\/ Argument found\n+    }\n+    if (bt == T_VOID && (sig->at(i-1)._bt != T_LONG && sig->at(i-1)._bt != T_DOUBLE)) {\n+      depth--;\n+    }\n+    if (depth == 0 && bt != T_LONG && bt != T_DOUBLE) {\n+      idx--; \/\/ Advance to next argument\n+    }\n+  }\n+  return depth != 0;\n+}\n+\n@@ -2358,0 +2448,4 @@\n+#ifdef ASSERT\n+  if (valid_itable_index())\n+    st->print_cr(\" - itable index:      %d\",   itable_index());\n+#endif\n@@ -2365,1 +2459,3 @@\n-  st->print_cr(\" - compiled entry     \" PTR_FORMAT, p2i(from_compiled_entry()));\n+  st->print_cr(\" - compiled entry           \" PTR_FORMAT, p2i(from_compiled_entry()));\n+  st->print_cr(\" - compiled inline entry    \" PTR_FORMAT, p2i(from_compiled_inline_entry()));\n+  st->print_cr(\" - compiled inline ro entry \" PTR_FORMAT, p2i(from_compiled_inline_ro_entry()));\n@@ -2435,0 +2531,1 @@\n+  if (WizardMode) access_flags().print_on(st);\n","filename":"src\/hotspot\/share\/oops\/method.cpp","additions":113,"deletions":16,"binary":false,"changes":129,"status":"modified"},{"patch":"@@ -696,0 +696,6 @@\n+  if (tp->isa_aryptr()) {\n+    \/\/ In the case of a flat inline type array, each field has its\n+    \/\/ own slice so we need to extract the field being accessed from\n+    \/\/ the address computation\n+    return tp->is_aryptr()->add_field_offset_and_offset(txoffset);\n+  }\n@@ -716,0 +722,6 @@\n+  if (p1->isa_aryptr()) {\n+    \/\/ In the case of a flat inline type array, each field has its\n+    \/\/ own slice so we need to extract the field being accessed from\n+    \/\/ the address computation\n+    return p1->is_aryptr()->add_field_offset_and_offset(p2offset);\n+  }\n","filename":"src\/hotspot\/share\/opto\/addnode.cpp","additions":12,"deletions":0,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -88,1 +88,1 @@\n-  if (callee_method->is_object_initializer()) {\n+  if (callee_method->is_object_constructor()) {\n@@ -91,1 +91,1 @@\n-  if (caller_method->is_object_initializer() &&\n+  if ((caller_method->is_object_constructor() || caller_method->is_class_initializer()) &&\n","filename":"src\/hotspot\/share\/opto\/bytecodeInfo.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -26,0 +26,1 @@\n+#include \"classfile\/vmIntrinsics.hpp\"\n@@ -126,1 +127,2 @@\n-  bool eliminate_boxing = EliminateAutoBox;\n+  \/\/ TODO 8328675 Re-enable\n+  bool eliminate_boxing = false; \/\/ EliminateAutoBox;\n@@ -654,0 +656,2 @@\n+  case vmIntrinsics::_makePrivateBuffer:\n+  case vmIntrinsics::_finishPrivateBuffer:\n@@ -663,0 +667,2 @@\n+  case vmIntrinsics::_getValue:\n+  case vmIntrinsics::_getFlatValue:\n@@ -672,0 +678,2 @@\n+  case vmIntrinsics::_putValue:\n+  case vmIntrinsics::_putFlatValue:\n@@ -755,0 +763,3 @@\n+  case vmIntrinsics::_newNullRestrictedNonAtomicArray:\n+  case vmIntrinsics::_newNullRestrictedAtomicArray:\n+  case vmIntrinsics::_newNullableAtomicArray:\n","filename":"src\/hotspot\/share\/opto\/c2compiler.cpp","additions":12,"deletions":1,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -32,0 +32,1 @@\n+#include \"oops\/accessDecorators.hpp\"\n@@ -37,0 +38,1 @@\n+#include \"opto\/inlinetypenode.hpp\"\n@@ -121,1 +123,1 @@\n-  \/\/ paths to facilitate late inlinig.\n+  \/\/ paths to facilitate late inlining.\n@@ -130,0 +132,1 @@\n+      _call_node(nullptr),\n@@ -132,0 +135,8 @@\n+    if (InlineTypeReturnedAsFields && method->is_method_handle_intrinsic()) {\n+      \/\/ If that call has not been optimized by the time optimizations are over,\n+      \/\/ we'll need to add a call to create an inline type instance from the klass\n+      \/\/ returned by the call (see PhaseMacroExpand::expand_mh_intrinsic_return).\n+      \/\/ Separating memory and I\/O projections for exceptions is required to\n+      \/\/ perform that graph transformation.\n+      _separate_io_proj = true;\n+    }\n@@ -145,0 +156,1 @@\n+  PhaseGVN& gvn = kit.gvn();\n@@ -177,1 +189,4 @@\n-  kit.set_arguments_for_java_call(call);\n+  kit.set_arguments_for_java_call(call, is_late_inline());\n+  if (kit.stopped()) {\n+    return kit.transfer_exceptions_into_jvms();\n+  }\n@@ -218,1 +233,0 @@\n-\n@@ -276,0 +290,3 @@\n+  if (kit.stopped()) {\n+    return kit.transfer_exceptions_into_jvms();\n+  }\n@@ -360,0 +377,4 @@\n+  virtual CallGenerator* inline_cg() {\n+    return _inline_cg;\n+  }\n+\n@@ -421,0 +442,8 @@\n+    \/\/ AlwaysIncrementalInline causes for_method_handle_inline() to\n+    \/\/ return a LateInlineCallGenerator. Extract the\n+    \/\/ InlineCallGenerator from it.\n+    if (AlwaysIncrementalInline && cg->is_late_inline() && !cg->is_virtual_late_inline()) {\n+      cg = cg->inline_cg();\n+      assert(cg != nullptr, \"inline call generator expected\");\n+    }\n+\n@@ -577,3 +606,3 @@\n-  const TypeTuple *r = call->tf()->domain();\n-  for (int i1 = 0; i1 < method()->arg_size(); i1++) {\n-    if (call->in(TypeFunc::Parms + i1)->is_top() && r->field_at(TypeFunc::Parms + i1) != Type::HALF) {\n+  const TypeTuple* r = call->tf()->domain_cc();\n+  for (uint i1 = TypeFunc::Parms; i1 < r->cnt(); i1++) {\n+    if (call->in(i1)->is_top() && r->field_at(i1) != Type::HALF) {\n@@ -597,1 +626,0 @@\n-  CallProjections callprojs;\n@@ -601,9 +629,8 @@\n-  call->extract_projections(&callprojs, true, do_asserts);\n-  if ((callprojs.fallthrough_catchproj == call->in(0)) ||\n-      (callprojs.catchall_catchproj    == call->in(0)) ||\n-      (callprojs.fallthrough_memproj   == call->in(TypeFunc::Memory)) ||\n-      (callprojs.catchall_memproj      == call->in(TypeFunc::Memory)) ||\n-      (callprojs.fallthrough_ioproj    == call->in(TypeFunc::I_O)) ||\n-      (callprojs.catchall_ioproj       == call->in(TypeFunc::I_O)) ||\n-      (callprojs.resproj != nullptr && call->find_edge(callprojs.resproj) != -1) ||\n-      (callprojs.exobj   != nullptr && call->find_edge(callprojs.exobj) != -1)) {\n+  CallProjections* callprojs = call->extract_projections(true, do_asserts);\n+  if ((callprojs->fallthrough_catchproj == call->in(0)) ||\n+      (callprojs->catchall_catchproj    == call->in(0)) ||\n+      (callprojs->fallthrough_memproj   == call->in(TypeFunc::Memory)) ||\n+      (callprojs->catchall_memproj      == call->in(TypeFunc::Memory)) ||\n+      (callprojs->fallthrough_ioproj    == call->in(TypeFunc::I_O)) ||\n+      (callprojs->catchall_ioproj       == call->in(TypeFunc::I_O)) ||\n+      (callprojs->exobj != nullptr && call->find_edge(callprojs->exobj) != -1)) {\n@@ -619,3 +646,12 @@\n-  \/\/ The call is marked as pure (no important side effects), but result isn't used.\n-  \/\/ It's safe to remove the call.\n-  bool result_not_used = (callprojs.resproj == nullptr || callprojs.resproj->outcnt() == 0);\n+\n+  bool result_not_used = true;\n+  for (uint i = 0; i < callprojs->nb_resproj; i++) {\n+    if (callprojs->resproj[i] != nullptr) {\n+      if (callprojs->resproj[i]->outcnt() != 0) {\n+        result_not_used = false;\n+      }\n+      if (call->find_edge(callprojs->resproj[i]) != -1) {\n+        return;\n+      }\n+    }\n+  }\n@@ -624,0 +660,2 @@\n+    \/\/ The call is marked as pure (no important side effects), but result isn't used.\n+    \/\/ It's safe to remove the call.\n@@ -636,0 +674,1 @@\n+    PhaseGVN& gvn = *C->initial_gvn();\n@@ -639,1 +678,1 @@\n-      C->initial_gvn()->set_type_bottom(mem);\n+      gvn.set_type_bottom(mem);\n@@ -643,4 +682,2 @@\n-    uint nargs = method()->arg_size();\n-    Node* top = C->top();\n-    for (uint i1 = 0; i1 < nargs; i1++) {\n-      map->set_req(TypeFunc::Parms + i1, top);\n+    for (uint i1 = TypeFunc::Parms; i1 < r->cnt(); i1++) {\n+      map->set_req(i1, C->top());\n@@ -654,0 +691,6 @@\n+    const TypeTuple* domain_sig = call->_tf->domain_sig();\n+    uint nargs = method()->arg_size();\n+    assert(domain_sig->cnt() - TypeFunc::Parms == nargs, \"inconsistent signature\");\n+\n+    uint j = TypeFunc::Parms;\n+    int arg_num = 0;\n@@ -655,1 +698,14 @@\n-      map->set_argument(jvms, i1, call->in(TypeFunc::Parms + i1));\n+      const Type* t = domain_sig->field_at(TypeFunc::Parms + i1);\n+      if (t->is_inlinetypeptr() && !method()->get_Method()->mismatch() && method()->is_scalarized_arg(arg_num)) {\n+        \/\/ Inline type arguments are not passed by reference: we get an argument per\n+        \/\/ field of the inline type. Build InlineTypeNodes from the inline type arguments.\n+        GraphKit arg_kit(jvms, &gvn);\n+        Node* vt = InlineTypeNode::make_from_multi(&arg_kit, call, t->inline_klass(), j, \/* in= *\/ true, \/* null_free= *\/ !t->maybe_null());\n+        map->set_control(arg_kit.control());\n+        map->set_argument(jvms, i1, vt);\n+      } else {\n+        map->set_argument(jvms, i1, call->in(j++));\n+      }\n+      if (t != Type::HALF) {\n+        arg_num++;\n+      }\n@@ -666,0 +722,20 @@\n+    \/\/ Check if we are late inlining a method handle call that returns an inline type as fields.\n+    Node* buffer_oop = nullptr;\n+    ciMethod* inline_method = inline_cg()->method();\n+    ciType* return_type = inline_method->return_type();\n+    if (!call->tf()->returns_inline_type_as_fields() &&\n+        return_type->is_inlinetype() && return_type->as_inline_klass()->can_be_returned_as_fields()) {\n+      \/\/ Allocate a buffer for the inline type returned as fields because the caller expects an oop return.\n+      \/\/ Do this before the method handle call in case the buffer allocation triggers deoptimization and\n+      \/\/ we need to \"re-execute\" the call in the interpreter (to make sure the call is only executed once).\n+      GraphKit arg_kit(jvms, &gvn);\n+      {\n+        PreserveReexecuteState preexecs(&arg_kit);\n+        arg_kit.jvms()->set_should_reexecute(true);\n+        arg_kit.inc_sp(nargs);\n+        Node* klass_node = arg_kit.makecon(TypeKlassPtr::make(return_type->as_inline_klass()));\n+        buffer_oop = arg_kit.new_instance(klass_node, nullptr, nullptr, \/* deoptimize_on_exception *\/ true);\n+      }\n+      jvms = arg_kit.transfer_exceptions_into_jvms();\n+    }\n+\n@@ -706,2 +782,2 @@\n-      C->set_has_loops(C->has_loops() || inline_cg()->method()->has_loops());\n-      C->env()->notice_inlined_method(inline_cg()->method());\n+      C->set_has_loops(C->has_loops() || inline_method->has_loops());\n+      C->env()->notice_inlined_method(inline_method);\n@@ -711,0 +787,54 @@\n+\n+    \/\/ Handle inline type returns\n+    InlineTypeNode* vt = result->isa_InlineType();\n+    if (vt != nullptr) {\n+      if (call->tf()->returns_inline_type_as_fields()) {\n+        vt->replace_call_results(&kit, call, C);\n+      } else {\n+        \/\/ Result might still be allocated (for example, if it has been stored to a non-flat field)\n+        if (!vt->is_allocated(&kit.gvn())) {\n+          assert(buffer_oop != nullptr, \"should have allocated a buffer\");\n+          RegionNode* region = new RegionNode(3);\n+\n+          \/\/ Check if result is null\n+          Node* null_ctl = kit.top();\n+          kit.null_check_common(vt->get_null_marker(), T_INT, false, &null_ctl);\n+          region->init_req(1, null_ctl);\n+          PhiNode* oop = PhiNode::make(region, kit.gvn().zerocon(T_OBJECT), TypeInstPtr::make(TypePtr::BotPTR, vt->type()->inline_klass()));\n+          Node* init_mem = kit.reset_memory();\n+          PhiNode* mem = PhiNode::make(region, init_mem, Type::MEMORY, TypePtr::BOTTOM);\n+\n+          \/\/ Not null, initialize the buffer\n+          kit.set_all_memory(init_mem);\n+\n+          Node* payload_ptr = kit.basic_plus_adr(buffer_oop, kit.gvn().type(vt)->inline_klass()->payload_offset());\n+          vt->store_flat(&kit, buffer_oop, payload_ptr, false, true, true, IN_HEAP | MO_UNORDERED);\n+          \/\/ Do not let stores that initialize this buffer be reordered with a subsequent\n+          \/\/ store that would make this buffer accessible by other threads.\n+          AllocateNode* alloc = AllocateNode::Ideal_allocation(buffer_oop);\n+          assert(alloc != nullptr, \"must have an allocation node\");\n+          kit.insert_mem_bar(Op_MemBarStoreStore, alloc->proj_out_or_null(AllocateNode::RawAddress));\n+          region->init_req(2, kit.control());\n+          oop->init_req(2, buffer_oop);\n+          mem->init_req(2, kit.merged_memory());\n+\n+          \/\/ Update oop input to buffer\n+          kit.gvn().hash_delete(vt);\n+          vt->set_oop(kit.gvn(), kit.gvn().transform(oop));\n+          vt->set_is_buffered(kit.gvn());\n+          vt = kit.gvn().transform(vt)->as_InlineType();\n+\n+          kit.set_control(kit.gvn().transform(region));\n+          kit.set_all_memory(kit.gvn().transform(mem));\n+          kit.record_for_igvn(region);\n+          kit.record_for_igvn(oop);\n+          kit.record_for_igvn(mem);\n+        }\n+        result = vt;\n+      }\n+      DEBUG_ONLY(buffer_oop = nullptr);\n+    } else {\n+      assert(result->is_top() || !call->tf()->returns_inline_type_as_fields() || !call->as_CallJava()->method()->return_type()->is_loaded(), \"Unexpected return value\");\n+    }\n+    assert(buffer_oop == nullptr, \"unused buffer allocation\");\n+\n@@ -939,0 +1069,23 @@\n+  \/\/ Allocate inline types if they are merged with objects (similar to Parse::merge_common())\n+  uint tos = kit.jvms()->stkoff() + kit.sp();\n+  uint limit = slow_map->req();\n+  for (uint i = TypeFunc::Parms; i < limit; i++) {\n+    Node* m = kit.map()->in(i);\n+    Node* n = slow_map->in(i);\n+    const Type* t = gvn.type(m)->meet_speculative(gvn.type(n));\n+    \/\/ TODO 8284443 still needed?\n+    if (m->is_InlineType() && !t->is_inlinetypeptr()) {\n+      \/\/ Allocate inline type in fast path\n+      m = m->as_InlineType()->buffer(&kit);\n+      kit.map()->set_req(i, m);\n+    }\n+    if (n->is_InlineType() && !t->is_inlinetypeptr()) {\n+      \/\/ Allocate inline type in slow path\n+      PreserveJVMState pjvms(&kit);\n+      kit.set_map(slow_map);\n+      n = n->as_InlineType()->buffer(&kit);\n+      kit.map()->set_req(i, n);\n+      slow_map = kit.stop();\n+    }\n+  }\n+\n@@ -962,2 +1115,0 @@\n-  uint tos = kit.jvms()->stkoff() + kit.sp();\n-  uint limit = slow_map->req();\n@@ -1000,2 +1151,2 @@\n-  if (IncrementalInlineMH && call_site_count > 0 &&\n-      (should_delay || input_not_const || !C->inlining_incrementally() || C->over_inlining_cutoff())) {\n+  if (IncrementalInlineMH && (AlwaysIncrementalInline ||\n+                            (call_site_count > 0 && (should_delay || input_not_const || !C->inlining_incrementally() || C->over_inlining_cutoff())))) {\n@@ -1009,0 +1160,1 @@\n+\n@@ -1056,0 +1208,1 @@\n+      int nargs = callee->arg_size();\n@@ -1057,1 +1210,1 @@\n-      Node* member_name = kit.argument(callee->arg_size() - 1);\n+      Node* member_name = kit.argument(nargs - 1);\n@@ -1127,1 +1280,2 @@\n-                                              speculative_receiver_type);\n+                                              speculative_receiver_type,\n+                                              true);\n@@ -1199,1 +1353,1 @@\n-    Node* receiver = kit.null_check_receiver_before_call(method());\n+    kit.null_check_receiver_before_call(method());\n","filename":"src\/hotspot\/share\/opto\/callGenerator.cpp","additions":188,"deletions":34,"binary":false,"changes":222,"status":"modified"},{"patch":"@@ -25,0 +25,1 @@\n+#include \"ci\/ciFlatArrayKlass.hpp\"\n@@ -37,0 +38,1 @@\n+#include \"opto\/inlinetypenode.hpp\"\n@@ -46,0 +48,1 @@\n+#include \"runtime\/stubRoutines.hpp\"\n@@ -80,1 +83,1 @@\n-Node *StartNode::match( const ProjNode *proj, const Matcher *match ) {\n+Node *StartNode::match(const ProjNode *proj, const Matcher *match, const RegMask* mask) {\n@@ -104,11 +107,0 @@\n-\/\/------------------------------StartOSRNode----------------------------------\n-\/\/ The method start node for an on stack replacement adapter\n-\n-\/\/------------------------------osr_domain-----------------------------\n-const TypeTuple *StartOSRNode::osr_domain() {\n-  const Type **fields = TypeTuple::fields(2);\n-  fields[TypeFunc::Parms+0] = TypeRawPtr::BOTTOM;  \/\/ address of osr buffer\n-\n-  return TypeTuple::make(TypeFunc::Parms+1, fields);\n-}\n-\n@@ -500,0 +492,8 @@\n+      } else if (cik->is_flat_array_klass()) {\n+        ciKlass* cie = cik->as_flat_array_klass()->base_element_klass();\n+        cie->print_name_on(st);\n+        st->print(\"[%d]\", spobj->n_fields());\n+        int ndim = cik->as_array_klass()->dimension() - 1;\n+        while (ndim-- > 0) {\n+          st->print(\"[]\");\n+        }\n@@ -505,0 +505,7 @@\n+        if (iklass != nullptr && iklass->is_inlinetype()) {\n+          Node* null_marker = mcall->in(first_ind++);\n+          if (!null_marker->is_top()) {\n+            st->print(\" [null marker\");\n+            format_helper(regalloc, st, null_marker, \":\", -1, nullptr);\n+          }\n+        }\n@@ -506,1 +513,0 @@\n-        ciField* cifield;\n@@ -509,2 +515,1 @@\n-          cifield = iklass->nonstatic_field_at(0);\n-          cifield->print_name_on(st);\n+          iklass->nonstatic_field_at(0)->print_name_on(st);\n@@ -519,2 +524,1 @@\n-            cifield = iklass->nonstatic_field_at(j);\n-            cifield->print_name_on(st);\n+            iklass->nonstatic_field_at(j)->print_name_on(st);\n@@ -740,1 +744,1 @@\n-const Type *CallNode::bottom_type() const { return tf()->range(); }\n+const Type *CallNode::bottom_type() const { return tf()->range_cc(); }\n@@ -745,1 +749,1 @@\n-  return tf()->range();\n+  return tf()->range_cc();\n@@ -750,0 +754,7 @@\n+  if (_entry_point == StubRoutines::store_inline_type_fields_to_buf()) {\n+    \/\/ The call to that stub is a special case: its inputs are\n+    \/\/ multiple values returned from a call and so it should follow\n+    \/\/ the return convention.\n+    SharedRuntime::java_return_convention(sig_bt, parm_regs, argcnt);\n+    return;\n+  }\n@@ -758,27 +769,26 @@\n-Node *CallNode::match( const ProjNode *proj, const Matcher *match ) {\n-  switch (proj->_con) {\n-  case TypeFunc::Control:\n-  case TypeFunc::I_O:\n-  case TypeFunc::Memory:\n-    return new MachProjNode(this,proj->_con,RegMask::Empty,MachProjNode::unmatched_proj);\n-\n-  case TypeFunc::Parms+1:       \/\/ For LONG & DOUBLE returns\n-    assert(tf()->range()->field_at(TypeFunc::Parms+1) == Type::HALF, \"\");\n-    \/\/ 2nd half of doubles and longs\n-    return new MachProjNode(this,proj->_con, RegMask::Empty, (uint)OptoReg::Bad);\n-\n-  case TypeFunc::Parms: {       \/\/ Normal returns\n-    uint ideal_reg = tf()->range()->field_at(TypeFunc::Parms)->ideal_reg();\n-    OptoRegPair regs = Opcode() == Op_CallLeafVector\n-      ? match->vector_return_value(ideal_reg)      \/\/ Calls into assembly vector routine\n-      : is_CallRuntime()\n-        ? match->c_return_value(ideal_reg)  \/\/ Calls into C runtime\n-        : match->  return_value(ideal_reg); \/\/ Calls into compiled Java code\n-    RegMask rm = RegMask(regs.first());\n-\n-    if (Opcode() == Op_CallLeafVector) {\n-      \/\/ If the return is in vector, compute appropriate regmask taking into account the whole range\n-      if(ideal_reg >= Op_VecA && ideal_reg <= Op_VecZ) {\n-        if(OptoReg::is_valid(regs.second())) {\n-          for (OptoReg::Name r = regs.first(); r <= regs.second(); r = OptoReg::add(r, 1)) {\n-            rm.Insert(r);\n+Node *CallNode::match(const ProjNode *proj, const Matcher *match, const RegMask* mask) {\n+  uint con = proj->_con;\n+  const TypeTuple* range_cc = tf()->range_cc();\n+  if (con >= TypeFunc::Parms) {\n+    if (tf()->returns_inline_type_as_fields()) {\n+      \/\/ The call returns multiple values (inline type fields): we\n+      \/\/ create one projection per returned value.\n+      assert(con <= TypeFunc::Parms+1 || InlineTypeReturnedAsFields, \"only for multi value return\");\n+      uint ideal_reg = range_cc->field_at(con)->ideal_reg();\n+      return new MachProjNode(this, con, mask[con-TypeFunc::Parms], ideal_reg);\n+    } else {\n+      if (con == TypeFunc::Parms) {\n+        uint ideal_reg = range_cc->field_at(TypeFunc::Parms)->ideal_reg();\n+        OptoRegPair regs = Opcode() == Op_CallLeafVector\n+          ? match->vector_return_value(ideal_reg)      \/\/ Calls into assembly vector routine\n+          : match->c_return_value(ideal_reg);\n+        RegMask rm = RegMask(regs.first());\n+\n+        if (Opcode() == Op_CallLeafVector) {\n+          \/\/ If the return is in vector, compute appropriate regmask taking into account the whole range\n+          if(ideal_reg >= Op_VecA && ideal_reg <= Op_VecZ) {\n+            if(OptoReg::is_valid(regs.second())) {\n+              for (OptoReg::Name r = regs.first(); r <= regs.second(); r = OptoReg::add(r, 1)) {\n+                rm.Insert(r);\n+              }\n+            }\n@@ -787,0 +797,9 @@\n+\n+        if (OptoReg::is_valid(regs.second())) {\n+          rm.Insert(regs.second());\n+        }\n+        return new MachProjNode(this,con,rm,ideal_reg);\n+      } else {\n+        assert(con == TypeFunc::Parms+1, \"only one return value\");\n+        assert(range_cc->field_at(TypeFunc::Parms+1) == Type::HALF, \"\");\n+        return new MachProjNode(this,con, RegMask::Empty, (uint)OptoReg::Bad);\n@@ -789,4 +808,0 @@\n-\n-    if( OptoReg::is_valid(regs.second()) )\n-      rm.Insert( regs.second() );\n-    return new MachProjNode(this,proj->_con,rm,ideal_reg);\n@@ -795,0 +810,6 @@\n+  switch (con) {\n+  case TypeFunc::Control:\n+  case TypeFunc::I_O:\n+  case TypeFunc::Memory:\n+    return new MachProjNode(this,proj->_con,RegMask::Empty,MachProjNode::unmatched_proj);\n+\n@@ -815,1 +836,1 @@\n-    const TypeTuple* args = _tf->domain();\n+    const TypeTuple* args = _tf->domain_sig();\n@@ -864,1 +885,1 @@\n-      const TypeTuple* d = tf()->domain();\n+      const TypeTuple* d = tf()->domain_cc();\n@@ -879,2 +900,2 @@\n-bool CallNode::has_non_debug_use(Node *n) {\n-  const TypeTuple * d = tf()->domain();\n+bool CallNode::has_non_debug_use(Node* n) {\n+  const TypeTuple* d = tf()->domain_cc();\n@@ -882,2 +903,1 @@\n-    Node *arg = in(i);\n-    if (arg == n) {\n+    if (in(i) == n) {\n@@ -890,0 +910,11 @@\n+bool CallNode::has_debug_use(Node* n) {\n+  if (jvms() != nullptr) {\n+    for (uint i = jvms()->debug_start(); i < jvms()->debug_end(); i++) {\n+      if (in(i) == n) {\n+        return true;\n+      }\n+    }\n+  }\n+  return false;\n+}\n+\n@@ -921,10 +952,15 @@\n-void CallNode::extract_projections(CallProjections* projs, bool separate_io_proj, bool do_asserts) {\n-  projs->fallthrough_proj      = nullptr;\n-  projs->fallthrough_catchproj = nullptr;\n-  projs->fallthrough_ioproj    = nullptr;\n-  projs->catchall_ioproj       = nullptr;\n-  projs->catchall_catchproj    = nullptr;\n-  projs->fallthrough_memproj   = nullptr;\n-  projs->catchall_memproj      = nullptr;\n-  projs->resproj               = nullptr;\n-  projs->exobj                 = nullptr;\n+CallProjections* CallNode::extract_projections(bool separate_io_proj, bool do_asserts) {\n+  uint max_res = TypeFunc::Parms-1;\n+  for (DUIterator_Fast imax, i = fast_outs(imax); i < imax; i++) {\n+    ProjNode *pn = fast_out(i)->as_Proj();\n+    max_res = MAX2(max_res, pn->_con);\n+  }\n+\n+  assert(max_res < _tf->range_cc()->cnt(), \"result out of bounds\");\n+\n+  uint projs_size = sizeof(CallProjections);\n+  if (max_res > TypeFunc::Parms) {\n+    projs_size += (max_res-TypeFunc::Parms)*sizeof(Node*);\n+  }\n+  char* projs_storage = resource_allocate_bytes(projs_size);\n+  CallProjections* projs = new(projs_storage)CallProjections(max_res - TypeFunc::Parms + 1);\n@@ -976,1 +1012,1 @@\n-      projs->resproj = pn;\n+      projs->resproj[0] = pn;\n@@ -979,1 +1015,3 @@\n-      assert(false, \"unexpected projection from allocation node.\");\n+      assert(pn->_con <= max_res, \"unexpected projection from allocation node.\");\n+      projs->resproj[pn->_con-TypeFunc::Parms] = pn;\n+      break;\n@@ -986,1 +1024,1 @@\n-  assert(projs->fallthrough_proj      != nullptr, \"must be found\");\n+  assert(!do_asserts || projs->fallthrough_proj      != nullptr, \"must be found\");\n@@ -996,0 +1034,1 @@\n+  return projs;\n@@ -1027,2 +1066,2 @@\n-  uint old_dbg_start = sfpt->is_Call() ? sfpt->as_Call()->tf()->domain()->cnt() : (uint)TypeFunc::Parms+1;\n-  uint new_dbg_start = tf()->domain()->cnt();\n+  uint old_dbg_start = sfpt->is_Call() ? sfpt->as_Call()->tf()->domain_sig()->cnt() : (uint)TypeFunc::Parms+1;\n+  uint new_dbg_start = tf()->domain_sig()->cnt();\n@@ -1069,0 +1108,4 @@\n+  Bytecodes::Code bc = jvms()->method()->java_code_at_bci(jvms()->bci());\n+  if (EnableValhalla && (bc == Bytecodes::_if_acmpeq || bc == Bytecodes::_if_acmpne)) {\n+    return true;\n+  }\n@@ -1111,0 +1154,10 @@\n+  if (can_reshape && uncommon_trap_request() != 0) {\n+    PhaseIterGVN* igvn = phase->is_IterGVN();\n+    if (remove_unknown_flat_array_load(igvn, in(0), in(TypeFunc::Memory), in(TypeFunc::Parms))) {\n+      if (!in(0)->is_Region()) {\n+        igvn->replace_input_of(this, 0, phase->C->top());\n+      }\n+      return this;\n+    }\n+  }\n+\n@@ -1172,0 +1225,122 @@\n+\/\/ Split if can cause the flat array branch of an array load with unknown type (see\n+\/\/ Parse::array_load) to end in an uncommon trap. In that case, the call to\n+\/\/ 'load_unknown_inline' is useless. Replace it with an uncommon trap with the same JVMState.\n+bool CallStaticJavaNode::remove_unknown_flat_array_load(PhaseIterGVN* igvn, Node* ctl, Node* mem, Node* unc_arg) {\n+  if (ctl == nullptr || ctl->is_top() || mem == nullptr || mem->is_top() || !mem->is_MergeMem()) {\n+    return false;\n+  }\n+  if (ctl->is_Region()) {\n+    bool res = false;\n+    for (uint i = 1; i < ctl->req(); i++) {\n+      MergeMemNode* mm = mem->clone()->as_MergeMem();\n+      for (MergeMemStream mms(mm); mms.next_non_empty(); ) {\n+        Node* m = mms.memory();\n+        if (m->is_Phi() && m->in(0) == ctl) {\n+          mms.set_memory(m->in(i));\n+        }\n+      }\n+      if (remove_unknown_flat_array_load(igvn, ctl->in(i), mm, unc_arg)) {\n+        res = true;\n+        if (!ctl->in(i)->is_Region()) {\n+          igvn->replace_input_of(ctl, i, igvn->C->top());\n+        }\n+      }\n+      igvn->remove_dead_node(mm);\n+    }\n+    return res;\n+  }\n+  \/\/ Verify the control flow is ok\n+  Node* call = ctl;\n+  MemBarNode* membar = nullptr;\n+  for (;;) {\n+    if (call == nullptr || call->is_top()) {\n+      return false;\n+    }\n+    if (call->is_Proj() || call->is_Catch() || call->is_MemBar()) {\n+      call = call->in(0);\n+    } else if (call->Opcode() == Op_CallStaticJava && !call->in(0)->is_top() &&\n+               call->as_Call()->entry_point() == OptoRuntime::load_unknown_inline_Java()) {\n+      assert(call->in(0)->is_Proj() && call->in(0)->in(0)->is_MemBar(), \"missing membar\");\n+      membar = call->in(0)->in(0)->as_MemBar();\n+      break;\n+    } else {\n+      return false;\n+    }\n+  }\n+\n+  JVMState* jvms = call->jvms();\n+  if (igvn->C->too_many_traps(jvms->method(), jvms->bci(), Deoptimization::trap_request_reason(uncommon_trap_request()))) {\n+    return false;\n+  }\n+\n+  Node* call_mem = call->in(TypeFunc::Memory);\n+  if (call_mem == nullptr || call_mem->is_top()) {\n+    return false;\n+  }\n+  if (!call_mem->is_MergeMem()) {\n+    call_mem = MergeMemNode::make(call_mem);\n+    igvn->register_new_node_with_optimizer(call_mem);\n+  }\n+\n+  \/\/ Verify that there's no unexpected side effect\n+  for (MergeMemStream mms2(mem->as_MergeMem(), call_mem->as_MergeMem()); mms2.next_non_empty2(); ) {\n+    Node* m1 = mms2.is_empty() ? mms2.base_memory() : mms2.memory();\n+    Node* m2 = mms2.memory2();\n+\n+    for (uint i = 0; i < 100; i++) {\n+      if (m1 == m2) {\n+        break;\n+      } else if (m1->is_Proj()) {\n+        m1 = m1->in(0);\n+      } else if (m1->is_MemBar()) {\n+        m1 = m1->in(TypeFunc::Memory);\n+      } else if (m1->Opcode() == Op_CallStaticJava &&\n+                 m1->as_Call()->entry_point() == OptoRuntime::load_unknown_inline_Java()) {\n+        if (m1 != call) {\n+          return false;\n+        }\n+        break;\n+      } else if (m1->is_MergeMem()) {\n+        MergeMemNode* mm = m1->as_MergeMem();\n+        int idx = mms2.alias_idx();\n+        if (idx == Compile::AliasIdxBot) {\n+          m1 = mm->base_memory();\n+        } else {\n+          m1 = mm->memory_at(idx);\n+        }\n+      } else {\n+        return false;\n+      }\n+    }\n+  }\n+  if (call_mem->outcnt() == 0) {\n+    igvn->remove_dead_node(call_mem);\n+  }\n+\n+  \/\/ Remove membar preceding the call\n+  membar->remove(igvn);\n+\n+  address call_addr = OptoRuntime::uncommon_trap_blob()->entry_point();\n+  CallNode* unc = new CallStaticJavaNode(OptoRuntime::uncommon_trap_Type(), call_addr, \"uncommon_trap\", nullptr);\n+  unc->init_req(TypeFunc::Control, call->in(0));\n+  unc->init_req(TypeFunc::I_O, call->in(TypeFunc::I_O));\n+  unc->init_req(TypeFunc::Memory, call->in(TypeFunc::Memory));\n+  unc->init_req(TypeFunc::FramePtr,  call->in(TypeFunc::FramePtr));\n+  unc->init_req(TypeFunc::ReturnAdr, call->in(TypeFunc::ReturnAdr));\n+  unc->init_req(TypeFunc::Parms+0, unc_arg);\n+  unc->set_cnt(PROB_UNLIKELY_MAG(4));\n+  unc->copy_call_debug_info(igvn, call->as_CallStaticJava());\n+\n+  \/\/ Replace the call with an uncommon trap\n+  igvn->replace_input_of(call, 0, igvn->C->top());\n+\n+  igvn->register_new_node_with_optimizer(unc);\n+\n+  Node* ctrl = igvn->transform(new ProjNode(unc, TypeFunc::Control));\n+  Node* halt = igvn->transform(new HaltNode(ctrl, call->in(TypeFunc::FramePtr), \"uncommon trap returned which should never happen\"));\n+  igvn->add_input_to(igvn->C->root(), halt);\n+\n+  return true;\n+}\n+\n+\n@@ -1283,0 +1458,7 @@\n+  if (_entry_point == nullptr) {\n+    \/\/ The call to that stub is a special case: its inputs are\n+    \/\/ multiple values returned from a call and so it should follow\n+    \/\/ the return convention.\n+    SharedRuntime::java_return_convention(sig_bt, parm_regs, argcnt);\n+    return;\n+  }\n@@ -1288,1 +1470,1 @@\n-  assert(tf()->range()->field_at(TypeFunc::Parms)->is_vect()->length_in_bytes() * BitsPerByte == _num_bits,\n+  assert(tf()->range_sig()->field_at(TypeFunc::Parms)->is_vect()->length_in_bytes() * BitsPerByte == _num_bits,\n@@ -1290,1 +1472,1 @@\n-  const TypeTuple* d = tf()->domain();\n+  const TypeTuple* d = tf()->domain_sig();\n@@ -1314,0 +1496,6 @@\n+uint CallLeafNoFPNode::match_edge(uint idx) const {\n+  \/\/ Null entry point is a special case for which the target is in a\n+  \/\/ register. Need to match that edge.\n+  return entry_point() == nullptr && idx == TypeFunc::Parms;\n+}\n+\n@@ -1364,1 +1552,14 @@\n-  return remove_dead_region(phase, can_reshape) ? this : nullptr;\n+  if (remove_dead_region(phase, can_reshape)) {\n+    return this;\n+  }\n+  \/\/ Scalarize inline types in safepoint debug info.\n+  \/\/ Delay this until all inlining is over to avoid getting inconsistent debug info.\n+  if (phase->C->scalarize_in_safepoints() && can_reshape && jvms() != nullptr) {\n+    for (uint i = jvms()->debug_start(); i < jvms()->debug_end(); i++) {\n+      Node* n = in(i)->uncast();\n+      if (n->is_InlineType()) {\n+        n->as_InlineType()->make_scalar_in_safepoints(phase->is_IterGVN());\n+      }\n+    }\n+  }\n+  return nullptr;\n@@ -1522,1 +1723,1 @@\n-  if (!alloc->is_Allocate() && !(alloc->Opcode() == Op_VectorBox)) {\n+  if (alloc != nullptr && !alloc->is_Allocate() && !(alloc->Opcode() == Op_VectorBox)) {\n@@ -1627,1 +1828,3 @@\n-                           Node *size, Node *klass_node, Node *initial_test)\n+                           Node *size, Node *klass_node,\n+                           Node* initial_test,\n+                           InlineTypeNode* inline_type_node)\n@@ -1635,0 +1838,1 @@\n+  _larval = false;\n@@ -1647,0 +1851,3 @@\n+  init_req( InlineType     , inline_type_node);\n+  \/\/ DefaultValue defaults to nullptr\n+  \/\/ RawDefaultValue defaults to nullptr\n@@ -1652,1 +1859,2 @@\n-  assert(initializer != nullptr && initializer->is_object_initializer(),\n+  assert(initializer != nullptr &&\n+         (initializer->is_object_constructor() || initializer->is_class_initializer()),\n@@ -1664,1 +1872,2 @@\n-Node *AllocateNode::make_ideal_mark(PhaseGVN *phase, Node* obj, Node* control, Node* mem) {\n+\n+Node* AllocateNode::make_ideal_mark(PhaseGVN* phase, Node* control, Node* mem) {\n@@ -1666,1 +1875,1 @@\n-  if (UseCompactObjectHeaders) {\n+  if (UseCompactObjectHeaders || EnableValhalla) {\n@@ -1670,0 +1879,6 @@\n+    if (EnableValhalla) {\n+      mark_node = phase->transform(mark_node);\n+      \/\/ Avoid returning a constant (old node) here because this method is used by LoadNode::Ideal\n+      mark_node = new OrXNode(mark_node, phase->MakeConX(_larval ? markWord::larval_bit_in_place : 0));\n+    }\n+    return mark_node;\n@@ -1671,2 +1886,1 @@\n-    \/\/ For now only enable fast locking for non-array types\n-    mark_node = phase->MakeConX(markWord::prototype().value());\n+    return phase->MakeConX(markWord::prototype().value());\n@@ -1674,1 +1888,0 @@\n-  return mark_node;\n@@ -2070,1 +2283,2 @@\n-  if (can_reshape && EliminateLocks && !is_non_esc_obj()) {\n+  const Type* obj_type = phase->type(obj_node());\n+  if (can_reshape && EliminateLocks && !is_non_esc_obj() && !obj_type->is_inlinetypeptr()) {\n@@ -2271,1 +2485,2 @@\n-  if (can_reshape && EliminateLocks && !is_non_esc_obj()) {\n+  const Type* obj_type = phase->type(obj_node());\n+  if (can_reshape && EliminateLocks && !is_non_esc_obj() && !obj_type->is_inlinetypeptr()) {\n@@ -2351,1 +2566,2 @@\n-    dest_t = dest_t->add_offset(Type::OffsetBot)->is_oopptr();\n+    dest_t = dest_t->is_aryptr()->with_field_offset(Type::OffsetBot)->add_offset(Type::OffsetBot)->is_oopptr();\n+    t_oop = t_oop->is_aryptr()->with_field_offset(Type::OffsetBot);\n","filename":"src\/hotspot\/share\/opto\/callnode.cpp","additions":301,"deletions":85,"binary":false,"changes":386,"status":"modified"},{"patch":"@@ -30,0 +30,2 @@\n+#include \"opto\/graphKit.hpp\"\n+#include \"opto\/inlinetypenode.hpp\"\n@@ -32,0 +34,1 @@\n+#include \"opto\/rootnode.hpp\"\n@@ -99,1 +102,1 @@\n-Node* ConstraintCastNode::Ideal(PhaseGVN* phase, bool can_reshape) {\n+Node *ConstraintCastNode::Ideal(PhaseGVN *phase, bool can_reshape) {\n@@ -103,0 +106,14 @@\n+\n+  \/\/ Push cast through InlineTypeNode\n+  InlineTypeNode* vt = in(1)->isa_InlineType();\n+  if (vt != nullptr && phase->type(vt)->filter_speculative(_type) != Type::TOP) {\n+    Node* cast = clone();\n+    cast->set_req(1, vt->get_oop());\n+    vt = vt->clone()->as_InlineType();\n+    if (!_type->maybe_null()) {\n+      vt->as_InlineType()->set_null_marker(*phase);\n+    }\n+    vt->set_oop(*phase, phase->transform(cast));\n+    return vt;\n+  }\n+\n@@ -106,0 +123,1 @@\n+\n@@ -358,0 +376,10 @@\n+\/\/=============================================================================\n+\/\/------------------------------Identity---------------------------------------\n+\/\/ If input is already higher or equal to cast type, then this is an identity.\n+Node* CheckCastPPNode::Identity(PhaseGVN* phase) {\n+  if (in(1)->is_InlineType() && _type->isa_instptr() && phase->type(in(1))->inline_klass()->is_subtype_of(_type->is_instptr()->instance_klass())) {\n+    return in(1);\n+  }\n+  return ConstraintCastNode::Identity(phase);\n+}\n+\n@@ -374,0 +402,10 @@\n+    \/\/ TODO 8302672\n+    if (!StressReflectiveCode && my_type->isa_aryptr() && in_type->isa_aryptr()) {\n+      \/\/ Propagate array properties (not flat\/null-free)\n+      \/\/ Don't do this when StressReflectiveCode is enabled because it might lead to\n+      \/\/ a dying data path while the corresponding flat\/null-free check is not folded.\n+      my_type = my_type->is_aryptr()->update_properties(in_type->is_aryptr());\n+      if (my_type == nullptr) {\n+        return Type::TOP; \/\/ Inconsistent properties\n+      }\n+    }\n@@ -378,1 +416,1 @@\n-      result =  my_type->cast_to_ptr_type(my_type->join_ptr(in_ptr));\n+      result = my_type->cast_to_ptr_type(my_type->join_ptr(in_ptr));\n@@ -465,0 +503,16 @@\n+\n+  if (t->is_zero_type() || !t->maybe_null()) {\n+    for (DUIterator_Fast imax, i = fast_outs(imax); i < imax; i++) {\n+      Node* u = fast_out(i);\n+      if (u->Opcode() == Op_OrL) {\n+        for (DUIterator_Fast jmax, j = u->fast_outs(jmax); j < jmax; j++) {\n+          Node* cmp = u->fast_out(j);\n+          if (cmp->Opcode() == Op_CmpL) {\n+            \/\/ Give CmpL a chance to get optimized\n+            phase->record_for_igvn(cmp);\n+          }\n+        }\n+      }\n+    }\n+  }\n+\n","filename":"src\/hotspot\/share\/opto\/castnode.cpp","additions":56,"deletions":2,"binary":false,"changes":58,"status":"modified"},{"patch":"@@ -35,0 +35,1 @@\n+#include \"opto\/inlinetypenode.hpp\"\n@@ -523,0 +524,1 @@\n+\n@@ -969,1 +971,2 @@\n-             cmp1->is_SubTypeCheck() || cmp2->is_SubTypeCheck()) {\n+             cmp1->is_SubTypeCheck() || cmp2->is_SubTypeCheck() ||\n+             cmp1->is_FlatArrayCheck() || cmp2->is_FlatArrayCheck()) {\n@@ -1048,1 +1051,1 @@\n-\/\/ note that these functions assume that the _adr_type field is flattened\n+\/\/ note that these functions assume that the _adr_type field is flat\n@@ -1066,1 +1069,1 @@\n-  assert(t != Type::MEMORY || at == flatten_phi_adr_type(at), \"flatten at\");\n+  assert(t != Type::MEMORY || at == flatten_phi_adr_type(at) || (flatten_phi_adr_type(at) == TypeAryPtr::INLINES && Compile::current()->flat_accesses_share_alias()), \"flatten at\");\n@@ -1195,0 +1198,8 @@\n+  \/\/ Flat array element shouldn't get their own memory slice until flat_accesses_share_alias is cleared.\n+  \/\/ It could be the graph has no loads\/stores and flat_accesses_share_alias is never cleared. EA could still\n+  \/\/ creates per element Phis but that wouldn't be a problem as there are no memory accesses for that array.\n+  assert(_adr_type == nullptr || _adr_type->isa_aryptr() == nullptr ||\n+         _adr_type->is_aryptr()->is_known_instance() ||\n+         !_adr_type->is_aryptr()->is_flat() ||\n+         !Compile::current()->flat_accesses_share_alias() ||\n+         _adr_type == TypeAryPtr::INLINES, \"flat array element shouldn't get its own slice yet\");\n@@ -1413,0 +1424,1 @@\n+\n@@ -1467,0 +1479,4 @@\n+  uin = unique_input_recursive(phase);\n+  if (uin != nullptr) {\n+    return uin;\n+  }\n@@ -1566,0 +1582,33 @@\n+\/\/ Find the unique input, try to look recursively through input Phis\n+Node* PhiNode::unique_input_recursive(PhaseGVN* phase) {\n+  if (!phase->is_IterGVN()) {\n+    return nullptr;\n+  }\n+\n+  ResourceMark rm;\n+  Node* unique = nullptr;\n+  Unique_Node_List visited;\n+  visited.push(this);\n+\n+  for (uint visited_idx = 0; visited_idx < visited.size(); visited_idx++) {\n+    Node* current = visited.at(visited_idx);\n+    for (uint i = 1; i < current->req(); i++) {\n+      Node* phi_in = current->in(i);\n+      if (phi_in == nullptr) {\n+        continue;\n+      }\n+\n+      if (phi_in->is_Phi()) {\n+        visited.push(phi_in);\n+      } else {\n+        if (unique == nullptr) {\n+          unique = phi_in;\n+        } else if (unique != phi_in) {\n+          return nullptr;\n+        }\n+      }\n+    }\n+  }\n+  return unique;\n+}\n+\n@@ -2033,0 +2082,46 @@\n+\/\/ Push inline type input nodes (and null) down through the phi recursively (can handle data loops).\n+InlineTypeNode* PhiNode::push_inline_types_down(PhaseGVN* phase, bool can_reshape, ciInlineKlass* inline_klass) {\n+  assert(inline_klass != nullptr, \"must be\");\n+  InlineTypeNode* vt = InlineTypeNode::make_null(*phase, inline_klass, \/* transform = *\/ false)->clone_with_phis(phase, in(0), nullptr, !_type->maybe_null());\n+  if (can_reshape) {\n+    \/\/ Replace phi right away to be able to use the inline\n+    \/\/ type node when reaching the phi again through data loops.\n+    PhaseIterGVN* igvn = phase->is_IterGVN();\n+    for (DUIterator_Fast imax, i = fast_outs(imax); i < imax; i++) {\n+      Node* u = fast_out(i);\n+      igvn->rehash_node_delayed(u);\n+      imax -= u->replace_edge(this, vt);\n+      --i;\n+    }\n+    igvn->rehash_node_delayed(this);\n+    assert(outcnt() == 0, \"should be dead now\");\n+  }\n+  ResourceMark rm;\n+  Node_List casts;\n+  for (uint i = 1; i < req(); ++i) {\n+    Node* n = in(i);\n+    while (n->is_ConstraintCast()) {\n+      casts.push(n);\n+      n = n->in(1);\n+    }\n+    if (phase->type(n)->is_zero_type()) {\n+      n = InlineTypeNode::make_null(*phase, inline_klass);\n+    } else if (n->is_Phi()) {\n+      assert(can_reshape, \"can only handle phis during IGVN\");\n+      n = phase->transform(n->as_Phi()->push_inline_types_down(phase, can_reshape, inline_klass));\n+    }\n+    while (casts.size() != 0) {\n+      \/\/ Push the cast(s) through the InlineTypeNode\n+      \/\/ TODO 8302217 Can we avoid cloning? See InlineTypeNode::clone_if_required\n+      Node* cast = casts.pop()->clone();\n+      cast->set_req_X(1, n->as_InlineType()->get_oop(), phase);\n+      n = n->clone();\n+      n->as_InlineType()->set_oop(*phase, phase->transform(cast));\n+      n = phase->transform(n);\n+    }\n+    bool transform = !can_reshape && (i == (req()-1)); \/\/ Transform phis on last merge\n+    vt->merge_with(phase, n->as_InlineType(), i, transform);\n+  }\n+  return vt;\n+}\n+\n@@ -2433,0 +2528,2 @@\n+    \/\/ TODO revisit this with JDK-8247216\n+    bool mergemem_only = true;\n@@ -2448,0 +2545,2 @@\n+      } else {\n+        mergemem_only = false;\n@@ -2471,1 +2570,1 @@\n-    if (!split_always_terminates && adr_type() == TypePtr::BOTTOM &&\n+    if (!mergemem_only && !split_always_terminates && adr_type() == TypePtr::BOTTOM &&\n@@ -2506,1 +2605,1 @@\n-      } else if (split_always_terminates) {\n+      } else if (mergemem_only || split_always_terminates) {\n@@ -2542,0 +2641,5 @@\n+            if (igvn) {\n+              \/\/ TODO revisit this with JDK-8247216\n+              \/\/ Put 'n' on the worklist because it might be modified by MergeMemStream::iteration_setup\n+              igvn->_worklist.push(n);\n+            }\n@@ -2652,0 +2756,5 @@\n+  Node* inline_type = try_push_inline_types_down(phase, can_reshape);\n+  if (inline_type != this) {\n+    return inline_type;\n+  }\n+\n@@ -2695,0 +2804,95 @@\n+\/\/ Check recursively if inputs are either an inline type, constant null\n+\/\/ or another Phi (including self references through data loops). If so,\n+\/\/ push the inline types down through the phis to enable folding of loads.\n+Node* PhiNode::try_push_inline_types_down(PhaseGVN* phase, const bool can_reshape) {\n+  if (!can_be_inline_type()) {\n+    return this;\n+  }\n+\n+  ciInlineKlass* inline_klass;\n+  if (can_push_inline_types_down(phase, can_reshape, inline_klass)) {\n+    assert(inline_klass != nullptr, \"must be\");\n+    return push_inline_types_down(phase, can_reshape, inline_klass);\n+  }\n+  return this;\n+}\n+\n+bool PhiNode::can_push_inline_types_down(PhaseGVN* phase, const bool can_reshape, ciInlineKlass*& inline_klass) {\n+  if (req() <= 2) {\n+    \/\/ Dead phi.\n+    return false;\n+  }\n+  inline_klass = nullptr;\n+\n+  \/\/ TODO 8302217 We need to prevent endless pushing through\n+  bool only_phi = (outcnt() != 0);\n+  for (DUIterator_Fast imax, i = fast_outs(imax); i < imax; i++) {\n+    Node* n = fast_out(i);\n+    if (n->is_InlineType() && n->in(1) == this) {\n+      return false;\n+    }\n+    if (!n->is_Phi()) {\n+      only_phi = false;\n+    }\n+  }\n+  if (only_phi) {\n+    return false;\n+  }\n+\n+  ResourceMark rm;\n+  Unique_Node_List worklist;\n+  worklist.push(this);\n+  Node_List casts;\n+\n+  for (uint next = 0; next < worklist.size(); next++) {\n+    Node* phi = worklist.at(next);\n+    for (uint i = 1; i < phi->req(); i++) {\n+      Node* n = phi->in(i);\n+      if (n == nullptr) {\n+        return false;\n+      }\n+      while (n->is_ConstraintCast()) {\n+        if (n->in(0) != nullptr && n->in(0)->is_top()) {\n+          \/\/ Will die, don't optimize\n+          return false;\n+        }\n+        casts.push(n);\n+        n = n->in(1);\n+      }\n+      const Type* type = phase->type(n);\n+      if (n->is_InlineType() && (inline_klass == nullptr || inline_klass == type->inline_klass())) {\n+        inline_klass = type->inline_klass();\n+      } else if (n->is_Phi() && can_reshape && n->bottom_type()->isa_ptr()) {\n+        worklist.push(n);\n+      } else if (!type->is_zero_type()) {\n+        return false;\n+      }\n+    }\n+  }\n+  if (inline_klass == nullptr) {\n+    return false;\n+  }\n+\n+  \/\/ Check if cast nodes can be pushed through\n+  const Type* t = Type::get_const_type(inline_klass);\n+  while (casts.size() != 0 && t != nullptr) {\n+    Node* cast = casts.pop();\n+    if (t->filter(cast->bottom_type()) == Type::TOP) {\n+      return false;\n+    }\n+  }\n+\n+  return true;\n+}\n+\n+#ifdef ASSERT\n+bool PhiNode::can_push_inline_types_down(PhaseGVN* phase) {\n+  if (!can_be_inline_type()) {\n+    return false;\n+  }\n+\n+  ciInlineKlass* inline_klass;\n+  return can_push_inline_types_down(phase, true, inline_klass);\n+}\n+#endif \/\/ ASSERT\n+\n@@ -3077,0 +3281,6 @@\n+\n+  \/\/ CheckCastPPNode::Ideal() for inline types reuses the exception\n+  \/\/ paths of a call to perform an allocation: we can see a Phi here.\n+  if (in(1)->is_Phi()) {\n+    return this;\n+  }\n","filename":"src\/hotspot\/share\/opto\/cfgnode.cpp","additions":215,"deletions":5,"binary":false,"changes":220,"status":"modified"},{"patch":"@@ -34,0 +34,1 @@\n+#include \"opto\/inlinetypenode.hpp\"\n","filename":"src\/hotspot\/share\/opto\/classes.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -62,0 +62,1 @@\n+#include \"opto\/inlinetypenode.hpp\"\n@@ -69,0 +70,1 @@\n+#include \"opto\/movenode.hpp\"\n@@ -408,0 +410,3 @@\n+  if (dead->is_InlineType()) {\n+    remove_inline_type(dead);\n+  }\n@@ -455,0 +460,3 @@\n+    if (n->outcnt() == 0) {\n+      worklist.push(n);\n+    }\n@@ -463,0 +471,6 @@\n+  remove_useless_nodes(_inline_type_nodes,  useful); \/\/ remove useless inline type nodes\n+#ifdef ASSERT\n+  if (_modified_nodes != nullptr) {\n+    _modified_nodes->remove_useless_nodes(useful.member_set());\n+  }\n+#endif\n@@ -649,0 +663,1 @@\n+      _has_circular_inline_type(false),\n@@ -668,0 +683,1 @@\n+      _inline_type_nodes (comp_arena(), 8, 0, nullptr),\n@@ -773,4 +789,2 @@\n-      const TypeTuple *domain = StartOSRNode::osr_domain();\n-      const TypeTuple *range = TypeTuple::make_range(method()->signature());\n-      init_tf(TypeFunc::make(domain, range));\n-      StartNode* s = new StartOSRNode(root(), domain);\n+      init_tf(TypeFunc::make(method(), \/* is_osr_compilation = *\/ true));\n+      StartNode* s = new StartOSRNode(root(), tf()->domain_sig());\n@@ -783,1 +797,1 @@\n-      StartNode* s = new StartNode(root(), tf()->domain());\n+      StartNode* s = new StartNode(root(), tf()->domain_cc());\n@@ -884,0 +898,10 @@\n+  if (needs_stack_repair()) {\n+    \/\/ One extra slot for the special stack increment value\n+    next_slot += 2;\n+  }\n+  \/\/ TODO 8284443 Only reserve extra slot if needed\n+  if (InlineTypeReturnedAsFields) {\n+    \/\/ One extra slot to hold the null marker for a nullable\n+    \/\/ inline type return if we run out of registers.\n+    next_slot += 2;\n+  }\n@@ -921,0 +945,1 @@\n+      _has_circular_inline_type(false),\n@@ -1073,0 +1098,4 @@\n+  _has_flat_accesses = false;\n+  _flat_accesses_share_alias = true;\n+  _scalarize_in_safepoints = false;\n+\n@@ -1357,0 +1386,9 @@\n+  if (ta && ta->is_not_flat()) {\n+    \/\/ Erase not flat property for alias analysis.\n+    tj = ta = ta->cast_to_not_flat(false);\n+  }\n+  if (ta && ta->is_not_null_free()) {\n+    \/\/ Erase not null free property for alias analysis.\n+    tj = ta = ta->cast_to_not_null_free(false);\n+  }\n+\n@@ -1370,0 +1408,2 @@\n+    \/\/ For flat inline type array, each field has its own slice so\n+    \/\/ we must include the field offset.\n@@ -1410,1 +1450,1 @@\n-      tj = ta = TypeAryPtr::make(ptr,ta->const_oop(),tary,nullptr,false,offset);\n+      tj = ta = TypeAryPtr::make(ptr,ta->const_oop(),tary,nullptr,false,Type::Offset(offset), ta->field_offset());\n@@ -1414,1 +1454,6 @@\n-      tj = ta = TypeAryPtr::make(ptr,ta->const_oop(),tary,nullptr,false,offset);\n+      tj = ta = TypeAryPtr::make(ptr,ta->const_oop(),tary,nullptr,false,Type::Offset(offset), ta->field_offset());\n+    }\n+    \/\/ Initially all flattened array accesses share a single slice\n+    if (ta->is_flat() && ta->elem() != TypeInstPtr::BOTTOM && _flat_accesses_share_alias) {\n+      const TypeAry* tary = TypeAry::make(TypeInstPtr::BOTTOM, ta->size(), \/* stable= *\/ false, \/* flat= *\/ true);\n+      tj = ta = TypeAryPtr::make(ptr,ta->const_oop(),tary,nullptr,false,Type::Offset(offset), Type::Offset(Type::OffsetBot));\n@@ -1421,1 +1466,1 @@\n-      tj = ta = TypeAryPtr::make(ptr,ta->const_oop(),tary,aklass,false,offset);\n+      tj = ta = TypeAryPtr::make(ptr,ta->const_oop(),tary,aklass,false,Type::Offset(offset), ta->field_offset());\n@@ -1471,1 +1516,1 @@\n-        tj = to = TypeInstPtr::make(TypePtr::BotPTR, env()->Object_klass(), false, nullptr, offset);\n+        tj = to = TypeInstPtr::make(TypePtr::BotPTR, env()->Object_klass(), false, nullptr, Type::Offset(offset));\n@@ -1492,1 +1537,1 @@\n-        assert(tj == TypeInstPtr::make(to->ptr(), canonical_holder, is_known_inst, nullptr, offset, instance_id), \"exact type should be canonical type\");\n+        assert(tj == TypeInstPtr::make(to->ptr(), canonical_holder, is_known_inst, nullptr, Type::Offset(offset), instance_id), \"exact type should be canonical type\");\n@@ -1495,1 +1540,1 @@\n-        tj = to = TypeInstPtr::make(to->ptr(), canonical_holder, is_known_inst, nullptr, offset, instance_id);\n+        tj = to = TypeInstPtr::make(to->ptr(), canonical_holder, is_known_inst, nullptr, Type::Offset(offset), instance_id);\n@@ -1510,1 +1555,1 @@\n-                                       offset);\n+                                       Type::Offset(offset));\n@@ -1516,1 +1561,1 @@\n-        tj = tk = TypeInstKlassPtr::make(TypePtr::NotNull, env()->Object_klass(), offset);\n+        tj = tk = TypeInstKlassPtr::make(TypePtr::NotNull, env()->Object_klass(), Type::Offset(offset));\n@@ -1518,1 +1563,1 @@\n-        tj = tk = TypeAryKlassPtr::make(TypePtr::NotNull, tk->is_aryklassptr()->elem(), k, offset);\n+        tj = tk = TypeAryKlassPtr::make(TypePtr::NotNull, tk->is_aryklassptr()->elem(), k, Type::Offset(offset), tk->is_not_flat(), tk->is_not_null_free(), tk->is_flat(), tk->is_null_free(), tk->is_atomic(), tk->is_aryklassptr()->is_vm_type());\n@@ -1521,1 +1566,0 @@\n-\n@@ -1651,1 +1695,1 @@\n-Compile::AliasType* Compile::find_alias_type(const TypePtr* adr_type, bool no_create, ciField* original_field) {\n+Compile::AliasType* Compile::find_alias_type(const TypePtr* adr_type, bool no_create, ciField* original_field, bool uncached) {\n@@ -1656,3 +1700,6 @@\n-  AliasCacheEntry* ace = probe_alias_cache(adr_type);\n-  if (ace->_adr_type == adr_type) {\n-    return alias_type(ace->_index);\n+  AliasCacheEntry* ace = nullptr;\n+  if (!uncached) {\n+    ace = probe_alias_cache(adr_type);\n+    if (ace->_adr_type == adr_type) {\n+      return alias_type(ace->_index);\n+    }\n@@ -1708,0 +1755,1 @@\n+    ciField* field = nullptr;\n@@ -1714,0 +1762,1 @@\n+      const Type* elemtype = flat->is_aryptr()->elem();\n@@ -1715,1 +1764,8 @@\n-        alias_type(idx)->set_element(flat->is_aryptr()->elem());\n+        alias_type(idx)->set_element(elemtype);\n+      }\n+      int field_offset = flat->is_aryptr()->field_offset().get();\n+      if (flat->is_flat() &&\n+          field_offset != Type::OffsetBot) {\n+        ciInlineKlass* vk = elemtype->inline_klass();\n+        field_offset += vk->payload_offset();\n+        field = vk->get_field_by_offset(field_offset, false);\n@@ -1731,0 +1787,2 @@\n+      if (flat->offset() == in_bytes(Klass::layout_helper_offset()))\n+        alias_type(idx)->set_rewritable(false);\n@@ -1741,1 +1799,0 @@\n-      ciField* field;\n@@ -1748,0 +1805,4 @@\n+      } else if (tinst->is_inlinetypeptr()) {\n+        \/\/ Inline type field\n+        ciInlineKlass* vk = tinst->inline_klass();\n+        field = vk->get_field_by_offset(tinst->offset(), false);\n@@ -1752,7 +1813,14 @@\n-      assert(field == nullptr ||\n-             original_field == nullptr ||\n-             (field->holder() == original_field->holder() &&\n-              field->offset_in_bytes() == original_field->offset_in_bytes() &&\n-              field->is_static() == original_field->is_static()), \"wrong field?\");\n-      \/\/ Set field() and is_rewritable() attributes.\n-      if (field != nullptr)  alias_type(idx)->set_field(field);\n+    }\n+    assert(field == nullptr ||\n+           original_field == nullptr ||\n+           (field->holder() == original_field->holder() &&\n+            field->offset_in_bytes() == original_field->offset_in_bytes() &&\n+            field->is_static() == original_field->is_static()), \"wrong field?\");\n+    \/\/ Set field() and is_rewritable() attributes.\n+    if (field != nullptr) {\n+      alias_type(idx)->set_field(field);\n+      if (flat->isa_aryptr()) {\n+        \/\/ Fields of flat arrays are rewritable although they are declared final\n+        assert(flat->is_flat(), \"must be a flat array\");\n+        alias_type(idx)->set_rewritable(true);\n+      }\n@@ -1763,3 +1831,4 @@\n-  ace->_adr_type = adr_type;\n-  ace->_index    = idx;\n-  assert(alias_type(adr_type) == alias_type(idx),  \"type must be installed\");\n+  if (!uncached) {\n+    ace->_adr_type = adr_type;\n+    ace->_index    = idx;\n+    assert(alias_type(adr_type) == alias_type(idx),  \"type must be installed\");\n@@ -1767,6 +1836,7 @@\n-  \/\/ Might as well try to fill the cache for the flattened version, too.\n-  AliasCacheEntry* face = probe_alias_cache(flat);\n-  if (face->_adr_type == nullptr) {\n-    face->_adr_type = flat;\n-    face->_index    = idx;\n-    assert(alias_type(flat) == alias_type(idx), \"flat type must work too\");\n+    \/\/ Might as well try to fill the cache for the flattened version, too.\n+    AliasCacheEntry* face = probe_alias_cache(flat);\n+    if (face->_adr_type == nullptr) {\n+      face->_adr_type = flat;\n+      face->_index    = idx;\n+      assert(alias_type(flat) == alias_type(idx), \"flat type must work too\");\n+    }\n@@ -1894,0 +1964,398 @@\n+void Compile::add_inline_type(Node* n) {\n+  assert(n->is_InlineType(), \"unexpected node\");\n+  _inline_type_nodes.push(n);\n+}\n+\n+void Compile::remove_inline_type(Node* n) {\n+  assert(n->is_InlineType(), \"unexpected node\");\n+  if (_inline_type_nodes.contains(n)) {\n+    _inline_type_nodes.remove(n);\n+  }\n+}\n+\n+\/\/ Does the return value keep otherwise useless inline type allocations alive?\n+static bool return_val_keeps_allocations_alive(Node* ret_val) {\n+  ResourceMark rm;\n+  Unique_Node_List wq;\n+  wq.push(ret_val);\n+  bool some_allocations = false;\n+  for (uint i = 0; i < wq.size(); i++) {\n+    Node* n = wq.at(i);\n+    if (n->outcnt() > 1) {\n+      \/\/ Some other use for the allocation\n+      return false;\n+    } else if (n->is_InlineType()) {\n+      wq.push(n->in(1));\n+    } else if (n->is_Phi()) {\n+      for (uint j = 1; j < n->req(); j++) {\n+        wq.push(n->in(j));\n+      }\n+    } else if (n->is_CheckCastPP() &&\n+               n->in(1)->is_Proj() &&\n+               n->in(1)->in(0)->is_Allocate()) {\n+      some_allocations = true;\n+    } else if (n->is_CheckCastPP()) {\n+      wq.push(n->in(1));\n+    }\n+  }\n+  return some_allocations;\n+}\n+\n+void Compile::process_inline_types(PhaseIterGVN &igvn, bool remove) {\n+  \/\/ Make sure that the return value does not keep an otherwise unused allocation alive\n+  if (tf()->returns_inline_type_as_fields()) {\n+    Node* ret = nullptr;\n+    for (uint i = 1; i < root()->req(); i++) {\n+      Node* in = root()->in(i);\n+      if (in->Opcode() == Op_Return) {\n+        assert(ret == nullptr, \"only one return\");\n+        ret = in;\n+      }\n+    }\n+    if (ret != nullptr) {\n+      Node* ret_val = ret->in(TypeFunc::Parms);\n+      if (igvn.type(ret_val)->isa_oopptr() &&\n+          return_val_keeps_allocations_alive(ret_val)) {\n+        igvn.replace_input_of(ret, TypeFunc::Parms, InlineTypeNode::tagged_klass(igvn.type(ret_val)->inline_klass(), igvn));\n+        assert(ret_val->outcnt() == 0, \"should be dead now\");\n+        igvn.remove_dead_node(ret_val);\n+      }\n+    }\n+  }\n+  if (_inline_type_nodes.length() == 0) {\n+    return;\n+  }\n+  \/\/ Scalarize inline types in safepoint debug info.\n+  \/\/ Delay this until all inlining is over to avoid getting inconsistent debug info.\n+  set_scalarize_in_safepoints(true);\n+  for (int i = _inline_type_nodes.length()-1; i >= 0; i--) {\n+    InlineTypeNode* vt = _inline_type_nodes.at(i)->as_InlineType();\n+    vt->make_scalar_in_safepoints(&igvn);\n+    igvn.record_for_igvn(vt);\n+  }\n+  if (remove) {\n+    \/\/ Remove inline type nodes by replacing them with their oop input\n+    while (_inline_type_nodes.length() > 0) {\n+      InlineTypeNode* vt = _inline_type_nodes.pop()->as_InlineType();\n+      if (vt->outcnt() == 0) {\n+        igvn.remove_dead_node(vt);\n+        continue;\n+      }\n+      for (DUIterator i = vt->outs(); vt->has_out(i); i++) {\n+        DEBUG_ONLY(bool must_be_buffered = false);\n+        Node* u = vt->out(i);\n+        \/\/ Check if any users are blackholes. If so, rewrite them to use either the\n+        \/\/ allocated buffer, or individual components, instead of the inline type node\n+        \/\/ that goes away.\n+        if (u->is_Blackhole()) {\n+          BlackholeNode* bh = u->as_Blackhole();\n+\n+          \/\/ Unlink the old input\n+          int idx = bh->find_edge(vt);\n+          assert(idx != -1, \"The edge should be there\");\n+          bh->del_req(idx);\n+          --i;\n+\n+          if (vt->is_allocated(&igvn)) {\n+            \/\/ Already has the allocated instance, blackhole that\n+            bh->add_req(vt->get_oop());\n+          } else {\n+            \/\/ Not allocated yet, blackhole the components\n+            for (uint c = 0; c < vt->field_count(); c++) {\n+              bh->add_req(vt->field_value(c));\n+            }\n+          }\n+\n+          \/\/ Node modified, record for IGVN\n+          igvn.record_for_igvn(bh);\n+        }\n+#ifdef ASSERT\n+        \/\/ Verify that inline type is buffered when replacing by oop\n+        else if (u->is_InlineType()) {\n+          \/\/ InlineType uses don't need buffering because they are about to be replaced as well\n+        } else if (u->is_Phi()) {\n+          \/\/ TODO 8302217 Remove this once InlineTypeNodes are reliably pushed through\n+        } else {\n+          must_be_buffered = true;\n+        }\n+        if (must_be_buffered && !vt->is_allocated(&igvn)) {\n+          vt->dump(0);\n+          u->dump(0);\n+          assert(false, \"Should have been buffered\");\n+        }\n+#endif\n+      }\n+      igvn.replace_node(vt, vt->get_oop());\n+    }\n+  }\n+  igvn.optimize();\n+}\n+\n+void Compile::adjust_flat_array_access_aliases(PhaseIterGVN& igvn) {\n+  if (!_has_flat_accesses) {\n+    return;\n+  }\n+  \/\/ Initially, all flat array accesses share the same slice to\n+  \/\/ keep dependencies with Object[] array accesses (that could be\n+  \/\/ to a flat array) correct. We're done with parsing so we\n+  \/\/ now know all flat array accesses in this compile\n+  \/\/ unit. Let's move flat array accesses to their own slice,\n+  \/\/ one per element field. This should help memory access\n+  \/\/ optimizations.\n+  ResourceMark rm;\n+  Unique_Node_List wq;\n+  wq.push(root());\n+\n+  Node_List mergememnodes;\n+  Node_List memnodes;\n+\n+  \/\/ Alias index currently shared by all flat memory accesses\n+  int index = get_alias_index(TypeAryPtr::INLINES);\n+\n+  \/\/ Find MergeMem nodes and flat array accesses\n+  for (uint i = 0; i < wq.size(); i++) {\n+    Node* n = wq.at(i);\n+    if (n->is_Mem()) {\n+      const TypePtr* adr_type = nullptr;\n+      adr_type = get_adr_type(get_alias_index(n->adr_type()));\n+      if (adr_type == TypeAryPtr::INLINES) {\n+        memnodes.push(n);\n+      }\n+    } else if (n->is_MergeMem()) {\n+      MergeMemNode* mm = n->as_MergeMem();\n+      if (mm->memory_at(index) != mm->base_memory()) {\n+        mergememnodes.push(n);\n+      }\n+    }\n+    for (uint j = 0; j < n->req(); j++) {\n+      Node* m = n->in(j);\n+      if (m != nullptr) {\n+        wq.push(m);\n+      }\n+    }\n+  }\n+\n+  if (memnodes.size() > 0) {\n+    _flat_accesses_share_alias = false;\n+\n+    \/\/ We are going to change the slice for the flat array\n+    \/\/ accesses so we need to clear the cache entries that refer to\n+    \/\/ them.\n+    for (uint i = 0; i < AliasCacheSize; i++) {\n+      AliasCacheEntry* ace = &_alias_cache[i];\n+      if (ace->_adr_type != nullptr &&\n+          ace->_adr_type->is_flat()) {\n+        ace->_adr_type = nullptr;\n+        ace->_index = (i != 0) ? 0 : AliasIdxTop; \/\/ Make sure the nullptr adr_type resolves to AliasIdxTop\n+      }\n+    }\n+\n+    \/\/ Find what aliases we are going to add\n+    int start_alias = num_alias_types()-1;\n+    int stop_alias = 0;\n+\n+    for (uint i = 0; i < memnodes.size(); i++) {\n+      Node* m = memnodes.at(i);\n+      const TypePtr* adr_type = nullptr;\n+      adr_type = m->adr_type();\n+#ifdef ASSERT\n+      m->as_Mem()->set_adr_type(adr_type);\n+#endif\n+      int idx = get_alias_index(adr_type);\n+      start_alias = MIN2(start_alias, idx);\n+      stop_alias = MAX2(stop_alias, idx);\n+    }\n+\n+    assert(stop_alias >= start_alias, \"should have expanded aliases\");\n+\n+    Node_Stack stack(0);\n+#ifdef ASSERT\n+    VectorSet seen(Thread::current()->resource_area());\n+#endif\n+    \/\/ Now let's fix the memory graph so each flat array access\n+    \/\/ is moved to the right slice. Start from the MergeMem nodes.\n+    uint last = unique();\n+    for (uint i = 0; i < mergememnodes.size(); i++) {\n+      MergeMemNode* current = mergememnodes.at(i)->as_MergeMem();\n+      Node* n = current->memory_at(index);\n+      MergeMemNode* mm = nullptr;\n+      do {\n+        \/\/ Follow memory edges through memory accesses, phis and\n+        \/\/ narrow membars and push nodes on the stack. Once we hit\n+        \/\/ bottom memory, we pop element off the stack one at a\n+        \/\/ time, in reverse order, and move them to the right slice\n+        \/\/ by changing their memory edges.\n+        if ((n->is_Phi() && n->adr_type() != TypePtr::BOTTOM) || n->is_Mem() || n->adr_type() == TypeAryPtr::INLINES) {\n+          assert(!seen.test_set(n->_idx), \"\");\n+          \/\/ Uses (a load for instance) will need to be moved to the\n+          \/\/ right slice as well and will get a new memory state\n+          \/\/ that we don't know yet. The use could also be the\n+          \/\/ backedge of a loop. We put a place holder node between\n+          \/\/ the memory node and its uses. We replace that place\n+          \/\/ holder with the correct memory state once we know it,\n+          \/\/ i.e. when nodes are popped off the stack. Using the\n+          \/\/ place holder make the logic work in the presence of\n+          \/\/ loops.\n+          if (n->outcnt() > 1) {\n+            Node* place_holder = nullptr;\n+            assert(!n->has_out_with(Op_Node), \"\");\n+            for (DUIterator k = n->outs(); n->has_out(k); k++) {\n+              Node* u = n->out(k);\n+              if (u != current && u->_idx < last) {\n+                bool success = false;\n+                for (uint l = 0; l < u->req(); l++) {\n+                  if (!stack.is_empty() && u == stack.node() && l == stack.index()) {\n+                    continue;\n+                  }\n+                  Node* in = u->in(l);\n+                  if (in == n) {\n+                    if (place_holder == nullptr) {\n+                      place_holder = new Node(1);\n+                      place_holder->init_req(0, n);\n+                    }\n+                    igvn.replace_input_of(u, l, place_holder);\n+                    success = true;\n+                  }\n+                }\n+                if (success) {\n+                  --k;\n+                }\n+              }\n+            }\n+          }\n+          if (n->is_Phi()) {\n+            stack.push(n, 1);\n+            n = n->in(1);\n+          } else if (n->is_Mem()) {\n+            stack.push(n, n->req());\n+            n = n->in(MemNode::Memory);\n+          } else {\n+            assert(n->is_Proj() && n->in(0)->Opcode() == Op_MemBarCPUOrder, \"\");\n+            stack.push(n, n->req());\n+            n = n->in(0)->in(TypeFunc::Memory);\n+          }\n+        } else {\n+          assert(n->adr_type() == TypePtr::BOTTOM || (n->Opcode() == Op_Node && n->_idx >= last) || (n->is_Proj() && n->in(0)->is_Initialize()), \"\");\n+          \/\/ Build a new MergeMem node to carry the new memory state\n+          \/\/ as we build it. IGVN should fold extraneous MergeMem\n+          \/\/ nodes.\n+          mm = MergeMemNode::make(n);\n+          igvn.register_new_node_with_optimizer(mm);\n+          while (stack.size() > 0) {\n+            Node* m = stack.node();\n+            uint idx = stack.index();\n+            if (m->is_Mem()) {\n+              \/\/ Move memory node to its new slice\n+              const TypePtr* adr_type = m->adr_type();\n+              int alias = get_alias_index(adr_type);\n+              Node* prev = mm->memory_at(alias);\n+              igvn.replace_input_of(m, MemNode::Memory, prev);\n+              mm->set_memory_at(alias, m);\n+            } else if (m->is_Phi()) {\n+              \/\/ We need as many new phis as there are new aliases\n+              igvn.replace_input_of(m, idx, mm);\n+              if (idx == m->req()-1) {\n+                Node* r = m->in(0);\n+                for (uint j = (uint)start_alias; j <= (uint)stop_alias; j++) {\n+                  const TypePtr* adr_type = get_adr_type(j);\n+                  if (!adr_type->isa_aryptr() || !adr_type->is_flat() || j == (uint)index) {\n+                    continue;\n+                  }\n+                  Node* phi = new PhiNode(r, Type::MEMORY, get_adr_type(j));\n+                  igvn.register_new_node_with_optimizer(phi);\n+                  for (uint k = 1; k < m->req(); k++) {\n+                    phi->init_req(k, m->in(k)->as_MergeMem()->memory_at(j));\n+                  }\n+                  mm->set_memory_at(j, phi);\n+                }\n+                Node* base_phi = new PhiNode(r, Type::MEMORY, TypePtr::BOTTOM);\n+                igvn.register_new_node_with_optimizer(base_phi);\n+                for (uint k = 1; k < m->req(); k++) {\n+                  base_phi->init_req(k, m->in(k)->as_MergeMem()->base_memory());\n+                }\n+                mm->set_base_memory(base_phi);\n+              }\n+            } else {\n+              \/\/ This is a MemBarCPUOrder node from\n+              \/\/ Parse::array_load()\/Parse::array_store(), in the\n+              \/\/ branch that handles flat arrays hidden under\n+              \/\/ an Object[] array. We also need one new membar per\n+              \/\/ new alias to keep the unknown access that the\n+              \/\/ membars protect properly ordered with accesses to\n+              \/\/ known flat array.\n+              assert(m->is_Proj(), \"projection expected\");\n+              Node* ctrl = m->in(0)->in(TypeFunc::Control);\n+              igvn.replace_input_of(m->in(0), TypeFunc::Control, top());\n+              for (uint j = (uint)start_alias; j <= (uint)stop_alias; j++) {\n+                const TypePtr* adr_type = get_adr_type(j);\n+                if (!adr_type->isa_aryptr() || !adr_type->is_flat() || j == (uint)index) {\n+                  continue;\n+                }\n+                MemBarNode* mb = new MemBarCPUOrderNode(this, j, nullptr);\n+                igvn.register_new_node_with_optimizer(mb);\n+                Node* mem = mm->memory_at(j);\n+                mb->init_req(TypeFunc::Control, ctrl);\n+                mb->init_req(TypeFunc::Memory, mem);\n+                ctrl = new ProjNode(mb, TypeFunc::Control);\n+                igvn.register_new_node_with_optimizer(ctrl);\n+                mem = new ProjNode(mb, TypeFunc::Memory);\n+                igvn.register_new_node_with_optimizer(mem);\n+                mm->set_memory_at(j, mem);\n+              }\n+              igvn.replace_node(m->in(0)->as_Multi()->proj_out(TypeFunc::Control), ctrl);\n+            }\n+            if (idx < m->req()-1) {\n+              idx += 1;\n+              stack.set_index(idx);\n+              n = m->in(idx);\n+              break;\n+            }\n+            \/\/ Take care of place holder nodes\n+            if (m->has_out_with(Op_Node)) {\n+              Node* place_holder = m->find_out_with(Op_Node);\n+              if (place_holder != nullptr) {\n+                Node* mm_clone = mm->clone();\n+                igvn.register_new_node_with_optimizer(mm_clone);\n+                Node* hook = new Node(1);\n+                hook->init_req(0, mm);\n+                igvn.replace_node(place_holder, mm_clone);\n+                hook->destruct(&igvn);\n+              }\n+              assert(!m->has_out_with(Op_Node), \"place holder should be gone now\");\n+            }\n+            stack.pop();\n+          }\n+        }\n+      } while(stack.size() > 0);\n+      \/\/ Fix the memory state at the MergeMem we started from\n+      igvn.rehash_node_delayed(current);\n+      for (uint j = (uint)start_alias; j <= (uint)stop_alias; j++) {\n+        const TypePtr* adr_type = get_adr_type(j);\n+        if (!adr_type->isa_aryptr() || !adr_type->is_flat()) {\n+          continue;\n+        }\n+        current->set_memory_at(j, mm);\n+      }\n+      current->set_memory_at(index, current->base_memory());\n+    }\n+    igvn.optimize();\n+  }\n+  print_method(PHASE_SPLIT_INLINES_ARRAY, 2);\n+#ifdef ASSERT\n+  if (!_flat_accesses_share_alias) {\n+    wq.clear();\n+    wq.push(root());\n+    for (uint i = 0; i < wq.size(); i++) {\n+      Node* n = wq.at(i);\n+      assert(n->adr_type() != TypeAryPtr::INLINES, \"should have been removed from the graph\");\n+      for (uint j = 0; j < n->req(); j++) {\n+        Node* m = n->in(j);\n+        if (m != nullptr) {\n+          wq.push(m);\n+        }\n+      }\n+    }\n+  }\n+#endif\n+}\n+\n@@ -2012,1 +2480,1 @@\n-        if (!live_locals.at(i) && !local->is_top() && local != lhs && local!= rhs) {\n+        if (!live_locals.at(i) && !local->is_top() && local != lhs && local != rhs) {\n@@ -2027,1 +2495,1 @@\n-    \/\/ keep the mondified trap for late query\n+    \/\/ keep the modified trap for late query\n@@ -2233,1 +2701,4 @@\n-  assert(_modified_nodes == nullptr, \"not allowed\");\n+#ifdef ASSERT\n+  Unique_Node_List* modified_nodes = _modified_nodes;\n+  _modified_nodes = nullptr;\n+#endif\n@@ -2246,0 +2717,1 @@\n+  DEBUG_ONLY( _modified_nodes = modified_nodes; )\n@@ -2390,0 +2862,5 @@\n+  \/\/ Process inline type nodes now that all inlining is over\n+  process_inline_types(igvn);\n+\n+  adjust_flat_array_access_aliases(igvn);\n+\n@@ -2392,0 +2869,11 @@\n+  if (C->macro_count() > 0) {\n+    \/\/ Eliminate some macro nodes before EA to reduce analysis pressure\n+    PhaseMacroExpand mexp(igvn);\n+    mexp.eliminate_macro_nodes(\/* eliminate_locks= *\/ false);\n+    if (failing()) {\n+      return;\n+    }\n+    igvn.set_delay_transform(false);\n+    print_method(PHASE_ITER_GVN_AFTER_ELIMINATION, 2);\n+  }\n+\n@@ -2402,1 +2890,14 @@\n-      if (failing())  return;\n+      if (failing()) {\n+        return;\n+      }\n+      print_method(PHASE_PHASEIDEAL_BEFORE_EA, 2);\n+      if (C->macro_count() > 0) {\n+        \/\/ Eliminate some macro nodes before EA to reduce analysis pressure\n+        PhaseMacroExpand mexp(igvn);\n+        mexp.eliminate_macro_nodes(\/* eliminate_locks= *\/ false);\n+        if (failing()) {\n+          return;\n+        }\n+        igvn.set_delay_transform(false);\n+        print_method(PHASE_ITER_GVN_AFTER_ELIMINATION, 2);\n+      }\n@@ -2404,0 +2905,1 @@\n+\n@@ -2405,1 +2907,0 @@\n-    print_method(PHASE_PHASEIDEAL_BEFORE_EA, 2);\n@@ -2423,2 +2924,3 @@\n-        if (failing()) return;\n-\n+        if (failing()) {\n+          return;\n+        }\n@@ -2426,3 +2928,0 @@\n-        igvn.optimize();\n-        if (failing()) return;\n-\n@@ -2519,0 +3018,8 @@\n+  assert(_late_inlines.length() == 0 || IncrementalInlineMH || IncrementalInlineVirtual, \"not empty\");\n+\n+  if (_late_inlines.length() > 0) {\n+    \/\/ More opportunities to optimize virtual and MH calls.\n+    \/\/ Though it's maybe too late to perform inlining, strength-reducing them to direct calls is still an option.\n+    process_late_inline_calls_no_inline(igvn);\n+  }\n+\n@@ -2521,0 +3028,7 @@\n+    PhaseMacroExpand mex(igvn);\n+    \/\/ Last attempt to eliminate macro nodes.\n+    mex.eliminate_macro_nodes();\n+    if (failing()) {\n+      return;\n+    }\n+\n@@ -2522,1 +3036,0 @@\n-    PhaseMacroExpand  mex(igvn);\n@@ -2530,0 +3043,4 @@\n+  \/\/ Process inline type nodes again and remove them. From here\n+  \/\/ on we don't need to keep track of field values anymore.\n+  process_inline_types(igvn, \/* remove= *\/ true);\n+\n@@ -2546,0 +3063,1 @@\n+  DEBUG_ONLY( _late_inlines.clear(); )\n@@ -2548,9 +3066,0 @@\n-\n-  assert(_late_inlines.length() == 0 || IncrementalInlineMH || IncrementalInlineVirtual, \"not empty\");\n-\n-  if (_late_inlines.length() > 0) {\n-    \/\/ More opportunities to optimize virtual and MH calls.\n-    \/\/ Though it's maybe too late to perform inlining, strength-reducing them to direct calls is still an option.\n-    process_late_inline_calls_no_inline(igvn);\n-    if (failing())  return;\n-  }\n@@ -3329,0 +3838,1 @@\n+  case Op_StoreLSpecial:\n@@ -3872,0 +4382,5 @@\n+  case Op_InlineType: {\n+    n->dump(-1);\n+    assert(false, \"inline type node was not removed\");\n+    break;\n+  }\n@@ -4247,2 +4762,2 @@\n-      if (accessing_method->is_static_initializer() ||\n-          accessing_method->is_object_initializer() ||\n+      if (accessing_method->is_class_initializer() ||\n+          accessing_method->is_object_constructor() ||\n@@ -4256,1 +4771,1 @@\n-      if (accessing_method->is_static_initializer()) {\n+      if (accessing_method->is_class_initializer()) {\n@@ -4326,0 +4841,1 @@\n+               (n->is_Allocate() && i >= AllocateNode::InlineType) ||\n@@ -4328,1 +4844,1 @@\n-              \"only region, phi, arraycopy, unlock or membar nodes have null data edges\");\n+              \"only region, phi, arraycopy, allocate, unlock or membar nodes have null data edges\");\n@@ -4479,0 +4995,7 @@\n+\n+    \/\/ Do not fold the subtype check to an array klass pointer comparison for null-able inline type arrays\n+    \/\/ because null-free [LMyValue <: null-able [LMyValue but the klasses are different. Perform a full test.\n+    if (!superk->is_aryklassptr()->is_null_free() && superk->is_aryklassptr()->elem()->isa_instklassptr() &&\n+        superk->is_aryklassptr()->elem()->is_instklassptr()->instance_klass()->is_inlinetype()) {\n+      return SSC_full_test;\n+    }\n@@ -4962,0 +5485,21 @@\n+Node* Compile::optimize_acmp(PhaseGVN* phase, Node* a, Node* b) {\n+  const TypeInstPtr* ta = phase->type(a)->isa_instptr();\n+  const TypeInstPtr* tb = phase->type(b)->isa_instptr();\n+  if (!EnableValhalla || ta == nullptr || tb == nullptr ||\n+      ta->is_zero_type() || tb->is_zero_type() ||\n+      !ta->can_be_inline_type() || !tb->can_be_inline_type()) {\n+    \/\/ Use old acmp if one operand is null or not an inline type\n+    return new CmpPNode(a, b);\n+  } else if (ta->is_inlinetypeptr() || tb->is_inlinetypeptr()) {\n+    \/\/ We know that one operand is an inline type. Therefore,\n+    \/\/ new acmp will only return true if both operands are nullptr.\n+    \/\/ Check if both operands are null by or'ing the oops.\n+    a = phase->transform(new CastP2XNode(nullptr, a));\n+    b = phase->transform(new CastP2XNode(nullptr, b));\n+    a = phase->transform(new OrXNode(a, b));\n+    return new CmpXNode(a, phase->MakeConX(0));\n+  }\n+  \/\/ Use new acmp\n+  return nullptr;\n+}\n+\n@@ -5317,0 +5861,2 @@\n+  } else if (bt == T_FLOAT) {\n+    result = new MoveI2FNode(value);\n","filename":"src\/hotspot\/share\/opto\/compile.cpp","additions":607,"deletions":61,"binary":false,"changes":668,"status":"modified"},{"patch":"@@ -56,0 +56,1 @@\n+class CallNode;\n@@ -99,0 +100,1 @@\n+class InlineTypeNode;\n@@ -335,0 +337,1 @@\n+  bool                  _has_circular_inline_type; \/\/ True if method loads an inline type with a circular, non-flat field\n@@ -363,0 +366,3 @@\n+  bool                  _has_flat_accesses;     \/\/ Any known flat array accesses?\n+  bool                  _flat_accesses_share_alias; \/\/ Initially all flat array share a single slice\n+  bool                  _scalarize_in_safepoints; \/\/ Scalarize inline types in safepoint debug info\n@@ -381,0 +387,1 @@\n+  GrowableArray<Node*>  _inline_type_nodes;     \/\/ List of InlineType nodes\n@@ -608,0 +615,2 @@\n+  bool              has_circular_inline_type() const { return _has_circular_inline_type; }\n+  void          set_has_circular_inline_type(bool z) { _has_circular_inline_type = z; }\n@@ -640,0 +649,10 @@\n+  void          set_flat_accesses()              { _has_flat_accesses = true; }\n+  bool          flat_accesses_share_alias() const { return _flat_accesses_share_alias; }\n+  void          set_flat_accesses_share_alias(bool z) { _flat_accesses_share_alias = z; }\n+  bool          scalarize_in_safepoints() const { return _scalarize_in_safepoints; }\n+  void          set_scalarize_in_safepoints(bool z) { _scalarize_in_safepoints = z; }\n+\n+  \/\/ Support for scalarized inline type calling convention\n+  bool              has_scalarized_args() const  { return _method != nullptr && _method->has_scalarized_args(); }\n+  bool              needs_stack_repair()  const  { return _method != nullptr && _method->get_Method()->c2_needs_stack_repair(); }\n+\n@@ -775,0 +794,7 @@\n+  \/\/ Keep track of inline type nodes for later processing\n+  void add_inline_type(Node* n);\n+  void remove_inline_type(Node* n);\n+  void process_inline_types(PhaseIterGVN &igvn, bool remove = false);\n+\n+  void adjust_flat_array_access_aliases(PhaseIterGVN& igvn);\n+\n@@ -957,1 +983,1 @@\n-  AliasType*        alias_type(const TypePtr* adr_type, ciField* field = nullptr) { return find_alias_type(adr_type, false, field); }\n+  AliasType*        alias_type(const TypePtr* adr_type, ciField* field = nullptr, bool uncached = false) { return find_alias_type(adr_type, false, field, uncached); }\n@@ -961,1 +987,1 @@\n-  int               get_alias_index(const TypePtr* at)  { return alias_type(at)->index(); }\n+  int               get_alias_index(const TypePtr* at, bool uncached = false) { return alias_type(at, nullptr, uncached)->index(); }\n@@ -1202,1 +1228,1 @@\n-  AliasType* find_alias_type(const TypePtr* adr_type, bool no_create, ciField* field);\n+  AliasType* find_alias_type(const TypePtr* adr_type, bool no_create, ciField* field, bool uncached = false);\n@@ -1292,1 +1318,3 @@\n-  \/\/ Auxiliary methods for randomized fuzzing\/stressing\n+  Node* optimize_acmp(PhaseGVN* phase, Node* a, Node* b);\n+\n+  \/\/ Auxiliary method for randomized fuzzing\/stressing\n","filename":"src\/hotspot\/share\/opto\/compile.hpp","additions":32,"deletions":4,"binary":false,"changes":36,"status":"modified"},{"patch":"@@ -1626,5 +1626,4 @@\n-  CallProjections projs;\n-  extract_projections(&projs, false, false);\n-  phase->replace_node(projs.fallthrough_proj, in(TypeFunc::Control));\n-  if (projs.fallthrough_catchproj != nullptr) {\n-    phase->replace_node(projs.fallthrough_catchproj, in(TypeFunc::Control));\n+  CallProjections* projs = extract_projections(false, false);\n+  phase->replace_node(projs->fallthrough_proj, in(TypeFunc::Control));\n+  if (projs->fallthrough_catchproj != nullptr) {\n+    phase->replace_node(projs->fallthrough_catchproj, in(TypeFunc::Control));\n@@ -1632,2 +1631,2 @@\n-  if (projs.fallthrough_memproj != nullptr) {\n-    phase->replace_node(projs.fallthrough_memproj, in(TypeFunc::Memory));\n+  if (projs->fallthrough_memproj != nullptr) {\n+    phase->replace_node(projs->fallthrough_memproj, in(TypeFunc::Memory));\n@@ -1635,2 +1634,2 @@\n-  if (projs.catchall_memproj != nullptr) {\n-    phase->replace_node(projs.catchall_memproj, C->top());\n+  if (projs->catchall_memproj != nullptr) {\n+    phase->replace_node(projs->catchall_memproj, C->top());\n@@ -1638,2 +1637,2 @@\n-  if (projs.fallthrough_ioproj != nullptr) {\n-    phase->replace_node(projs.fallthrough_ioproj, in(TypeFunc::I_O));\n+  if (projs->fallthrough_ioproj != nullptr) {\n+    phase->replace_node(projs->fallthrough_ioproj, in(TypeFunc::I_O));\n@@ -1641,4 +1640,4 @@\n-  assert(projs.catchall_ioproj == nullptr, \"no exceptions from floating mod\");\n-  assert(projs.catchall_catchproj == nullptr, \"no exceptions from floating mod\");\n-  if (projs.resproj != nullptr) {\n-    phase->replace_node(projs.resproj, con_node);\n+  assert(projs->catchall_ioproj == nullptr, \"no exceptions from floating mod\");\n+  assert(projs->catchall_catchproj == nullptr, \"no exceptions from floating mod\");\n+  if (projs->resproj[0] != nullptr) {\n+    phase->replace_node(projs->resproj[0], con_node);\n@@ -1704,1 +1703,1 @@\n-Node *DivModINode::match( const ProjNode *proj, const Matcher *match ) {\n+Node *DivModINode::match(const ProjNode *proj, const Matcher *match, const RegMask* mask) {\n@@ -1719,1 +1718,1 @@\n-Node *DivModLNode::match( const ProjNode *proj, const Matcher *match ) {\n+Node *DivModLNode::match(const ProjNode *proj, const Matcher *match, const RegMask* mask) {\n@@ -1757,1 +1756,1 @@\n-Node* UDivModINode::match( const ProjNode *proj, const Matcher *match ) {\n+Node* UDivModINode::match(const ProjNode* proj, const Matcher* match, const RegMask* mask) {\n@@ -1772,1 +1771,1 @@\n-Node* UDivModLNode::match( const ProjNode *proj, const Matcher *match ) {\n+Node* UDivModLNode::match( const ProjNode* proj, const Matcher* match, const RegMask* mask) {\n","filename":"src\/hotspot\/share\/opto\/divnode.cpp","additions":18,"deletions":19,"binary":false,"changes":37,"status":"modified"},{"patch":"@@ -31,0 +31,1 @@\n+#include \"memory\/metaspace.hpp\"\n@@ -39,0 +40,1 @@\n+#include \"opto\/inlinetypenode.hpp\"\n@@ -166,0 +168,10 @@\n+    if ((n->Opcode() == Op_LoadX || n->Opcode() == Op_StoreX) &&\n+        !n->in(MemNode::Address)->is_AddP() &&\n+        _igvn->type(n->in(MemNode::Address))->isa_oopptr()) {\n+      \/\/ Load\/Store at mark work address is at offset 0 so has no AddP which confuses EA\n+      Node* addp = new AddPNode(n->in(MemNode::Address), n->in(MemNode::Address), _igvn->MakeConX(0));\n+      _igvn->register_new_node_with_optimizer(addp);\n+      _igvn->replace_input_of(n, MemNode::Address, addp);\n+      ideal_nodes.push(addp);\n+      _nodes.at_put_grow(addp->_idx, nullptr, nullptr);\n+    }\n@@ -1255,1 +1267,9 @@\n-      SafePointScalarObjectNode* sobj = mexp.create_scalarized_object_description(alloc, sfpt);\n+      Unique_Node_List value_worklist;\n+#ifdef ASSERT\n+      const Type* res_type = alloc->result_cast()->bottom_type();\n+      if (res_type->is_inlinetypeptr() && !Compile::current()->has_circular_inline_type()) {\n+        PhiNode* phi = ophi->as_Phi();\n+        assert(!ophi->as_Phi()->can_push_inline_types_down(_igvn), \"missed earlier scalarization opportunity\");\n+      }\n+#endif\n+      SafePointScalarObjectNode* sobj = mexp.create_scalarized_object_description(alloc, sfpt, &value_worklist);\n@@ -1257,0 +1277,1 @@\n+        _compile->record_failure(C2Compiler::retry_no_reduce_allocation_merges());\n@@ -1267,0 +1288,9 @@\n+\n+      \/\/ Scalarize inline types that were added to the safepoint.\n+      \/\/ Don't allow linking a constant oop (if available) for flat array elements\n+      \/\/ because Deoptimization::reassign_flat_array_elements needs field values.\n+      const bool allow_oop = !merge_t->is_flat();\n+      for (uint j = 0; j < value_worklist.size(); ++j) {\n+        InlineTypeNode* vt = value_worklist.at(j)->as_InlineType();\n+        vt->make_scalar_in_safepoints(_igvn, allow_oop);\n+      }\n@@ -1465,1 +1495,1 @@\n-      const TypeTuple* d = call->tf()->domain();\n+      const TypeTuple* d = call->tf()->domain_sig();\n@@ -1539,0 +1569,11 @@\n+      } else if (n->as_Call()->tf()->returns_inline_type_as_fields()) {\n+        bool returns_oop = false;\n+        for (DUIterator_Fast imax, i = n->fast_outs(imax); i < imax && !returns_oop; i++) {\n+          ProjNode* pn = n->fast_out(i)->as_Proj();\n+          if (pn->_con >= TypeFunc::Parms && pn->bottom_type()->isa_ptr()) {\n+            returns_oop = true;\n+          }\n+        }\n+        if (returns_oop) {\n+          add_call_node(n->as_Call());\n+        }\n@@ -1566,1 +1607,2 @@\n-    case Op_CastX2P: {\n+    case Op_CastX2P:\n+    case Op_CastI2N: {\n@@ -1570,0 +1612,1 @@\n+    case Op_InlineType:\n@@ -1641,2 +1684,4 @@\n-      if (n->as_Proj()->_con == TypeFunc::Parms && n->in(0)->is_Call() &&\n-          n->in(0)->as_Call()->returns_pointer()) {\n+      if (n->as_Proj()->_con >= TypeFunc::Parms && n->in(0)->is_Call() &&\n+          (n->in(0)->as_Call()->returns_pointer() || n->bottom_type()->isa_ptr())) {\n+        assert((n->as_Proj()->_con == TypeFunc::Parms && n->in(0)->as_Call()->returns_pointer()) ||\n+               n->in(0)->as_Call()->tf()->returns_inline_type_as_fields(), \"what kind of oop return is it?\");\n@@ -1744,0 +1789,1 @@\n+    case Op_InlineType:\n@@ -1798,2 +1844,2 @@\n-      assert(n->as_Proj()->_con == TypeFunc::Parms && n->in(0)->is_Call() &&\n-             n->in(0)->as_Call()->returns_pointer(), \"Unexpected node type\");\n+      assert((n->as_Proj()->_con == TypeFunc::Parms && n->in(0)->as_Call()->returns_pointer()) ||\n+             n->in(0)->as_Call()->tf()->returns_inline_type_as_fields(), \"what kind of oop return is it?\");\n@@ -1975,1 +2021,1 @@\n-  assert(call->returns_pointer(), \"only for call which returns pointer\");\n+  assert(call->returns_pointer() || call->tf()->returns_inline_type_as_fields(), \"only for call which returns pointer\");\n@@ -2051,1 +2097,3 @@\n-      assert(strncmp(name, \"C2 Runtime multianewarray\", 25) == 0, \"TODO: add failed case check\");\n+      assert(strncmp(name, \"C2 Runtime multianewarray\", 25) == 0 ||\n+             strncmp(name, \"C2 Runtime load_unknown_inline\", 30) == 0 ||\n+             strncmp(name, \"store_inline_type_fields_to_buf\", 31) == 0, \"TODO: add failed case check\");\n@@ -2082,1 +2130,1 @@\n-        const TypeTuple* d = call->tf()->domain();\n+        const TypeTuple* d = call->tf()->domain_cc();\n@@ -2130,1 +2178,1 @@\n-      const TypeTuple * d = call->tf()->domain();\n+      const TypeTuple * d = call->tf()->domain_sig();\n@@ -2161,1 +2209,4 @@\n-                               (aat->isa_aryptr() && (aat->isa_aryptr()->elem() == Type::BOTTOM || aat->isa_aryptr()->elem()->make_oopptr() != nullptr)));\n+                               (aat->isa_aryptr() && (aat->isa_aryptr()->elem() == Type::BOTTOM || aat->isa_aryptr()->elem()->make_oopptr() != nullptr)) ||\n+                               (aat->isa_aryptr() && aat->isa_aryptr()->elem() != nullptr &&\n+                                                               aat->isa_aryptr()->is_flat() &&\n+                                                               aat->isa_aryptr()->elem()->inline_klass()->contains_oops()));\n@@ -2225,0 +2276,4 @@\n+                  strcmp(call->as_CallLeaf()->_name, \"vectorizedMismatch\") == 0 ||\n+                  strcmp(call->as_CallLeaf()->_name, \"load_unknown_inline\") == 0 ||\n+                  strcmp(call->as_CallLeaf()->_name, \"store_unknown_inline\") == 0 ||\n+                  strcmp(call->as_CallLeaf()->_name, \"store_inline_type_fields_to_buf\") == 0 ||\n@@ -2287,1 +2342,1 @@\n-        const TypeTuple* d = call->tf()->domain();\n+        const TypeTuple* d = call->tf()->domain_cc();\n@@ -2331,1 +2386,1 @@\n-      const TypeTuple* d = call->tf()->domain();\n+      const TypeTuple* d = call->tf()->domain_cc();\n@@ -2744,0 +2799,1 @@\n+  PointsToNode* init_val = phantom_obj;\n@@ -2749,1 +2805,7 @@\n-    return 0;\n+    if (alloc->as_Allocate()->in(AllocateNode::InitValue) != nullptr) {\n+      \/\/ Null-free inline type arrays are initialized with an init value instead of null\n+      init_val = ptnode_adr(alloc->as_Allocate()->in(AllocateNode::InitValue)->_idx);\n+      assert(init_val != nullptr, \"init value should be registered\");\n+    } else {\n+      return 0;\n+    }\n@@ -2751,1 +2813,2 @@\n-  assert(pta->arraycopy_dst() || alloc->as_CallStaticJava(), \"sanity\");\n+  \/\/ Non-escaped allocation returned from Java or runtime call has unknown values in fields.\n+  assert(pta->arraycopy_dst() || alloc->is_CallStaticJava() || init_val != phantom_obj, \"sanity\");\n@@ -2753,1 +2816,1 @@\n-  if (!pta->arraycopy_dst() && alloc->as_CallStaticJava()->method() == nullptr) {\n+  if (alloc->is_CallStaticJava() && alloc->as_CallStaticJava()->method() == nullptr) {\n@@ -2755,1 +2818,3 @@\n-    assert(strncmp(name, \"C2 Runtime multianewarray\", 25) == 0, \"sanity\");\n+    assert(strncmp(name, \"C2 Runtime multianewarray\", 25) == 0 ||\n+           strncmp(name, \"C2 Runtime load_unknown_inline\", 30) == 0 ||\n+           strncmp(name, \"store_inline_type_fields_to_buf\", 31) == 0, \"sanity\");\n@@ -2763,1 +2828,1 @@\n-      if (add_edge(field, phantom_obj)) {\n+      if (add_edge(field, init_val)) {\n@@ -2778,1 +2843,1 @@\n-  if (!alloc->is_Allocate()) {\n+  if (!alloc->is_Allocate() || alloc->as_Allocate()->in(AllocateNode::InitValue) != nullptr) {\n@@ -2864,1 +2929,1 @@\n-                tty->print_cr(\"----------missed referernce to object-----------\");\n+                tty->print_cr(\"----------missed reference to object------------\");\n@@ -2866,1 +2931,1 @@\n-                tty->print_cr(\"----------object referernced by init store -----\");\n+                tty->print_cr(\"----------object referenced by init store-------\");\n@@ -3222,1 +3287,2 @@\n-          if (can_eliminate_lock(alock)) {\n+          const Type* obj_type = igvn->type(alock->obj_node());\n+          if (can_eliminate_lock(alock) && !obj_type->is_inlinetypeptr()) {\n@@ -3264,5 +3330,10 @@\n-      MemBarNode* mb = MemBarNode::make(C, Op_MemBarCPUOrder, Compile::AliasIdxBot);\n-      mb->init_req(TypeFunc::Memory,  storestore->in(TypeFunc::Memory));\n-      mb->init_req(TypeFunc::Control, storestore->in(TypeFunc::Control));\n-      igvn->register_new_node_with_optimizer(mb);\n-      igvn->replace_node(storestore, mb);\n+      if (alloc->in(AllocateNode::InlineType) != nullptr) {\n+        \/\/ Non-escaping inline type buffer allocations don't require a membar\n+        storestore->as_MemBar()->remove(_igvn);\n+      } else {\n+        MemBarNode* mb = MemBarNode::make(C, Op_MemBarCPUOrder, Compile::AliasIdxBot);\n+        mb->init_req(TypeFunc::Memory,  storestore->in(TypeFunc::Memory));\n+        mb->init_req(TypeFunc::Control, storestore->in(TypeFunc::Control));\n+        igvn->register_new_node_with_optimizer(mb);\n+        igvn->replace_node(storestore, mb);\n+      }\n@@ -3430,0 +3501,1 @@\n+  int field_offset = adr_type->isa_aryptr() ? adr_type->isa_aryptr()->field_offset().get() : Type::OffsetBot;\n@@ -3431,1 +3503,1 @@\n-  if (offset == Type::OffsetBot) {\n+  if (offset == Type::OffsetBot && field_offset == Type::OffsetBot) {\n@@ -3443,1 +3515,1 @@\n-      ciField* field = _compile->alias_type(adr_type->isa_instptr())->field();\n+      ciField* field = _compile->alias_type(adr_type->is_ptr())->field();\n@@ -3462,2 +3534,14 @@\n-        const Type* elemtype = adr_type->isa_aryptr()->elem();\n-        bt = elemtype->array_element_basic_type();\n+        const Type* elemtype = adr_type->is_aryptr()->elem();\n+        if (adr_type->is_aryptr()->is_flat() && field_offset != Type::OffsetBot) {\n+          ciInlineKlass* vk = elemtype->inline_klass();\n+          field_offset += vk->payload_offset();\n+          ciField* field = vk->get_field_by_offset(field_offset, false);\n+          if (field != nullptr) {\n+            bt = field->layout_type();\n+          } else {\n+            assert(field_offset == vk->payload_offset() + vk->null_marker_offset_in_payload(), \"no field or null marker of %s at offset %d\", vk->name()->as_utf8(), field_offset);\n+            bt = T_BOOLEAN;\n+          }\n+        } else {\n+          bt = elemtype->array_element_basic_type();\n+        }\n@@ -3660,3 +3744,1 @@\n-  const TypePtr *t_ptr = adr_type->isa_ptr();\n-  assert(t_ptr != nullptr, \"must be a pointer type\");\n-  return t_ptr->offset();\n+  return adr_type->is_ptr()->flat_offset();\n@@ -3816,1 +3898,8 @@\n-    t = base_t->add_offset(offs)->is_oopptr();\n+    if (base_t->isa_aryptr() != nullptr) {\n+      \/\/ In the case of a flat inline type array, each field has its\n+      \/\/ own slice so we need to extract the field being accessed from\n+      \/\/ the address computation\n+      t = base_t->isa_aryptr()->add_field_offset_and_offset(offs)->is_oopptr();\n+    } else {\n+      t = base_t->add_offset(offs)->is_oopptr();\n+    }\n@@ -3818,1 +3907,1 @@\n-  int inst_id =  base_t->instance_id();\n+  int inst_id = base_t->instance_id();\n@@ -3832,1 +3921,1 @@\n-  \/\/ It could happened when CHA type is different from MDO type on a dead path\n+  \/\/ It could happen when CHA type is different from MDO type on a dead path\n@@ -3842,1 +3931,12 @@\n-  const TypeOopPtr *tinst = base_t->add_offset(t->offset())->is_oopptr();\n+  const TypePtr* tinst = base_t->add_offset(t->offset());\n+  if (tinst->isa_aryptr() && t->isa_aryptr()) {\n+    \/\/ In the case of a flat inline type array, each field has its\n+    \/\/ own slice so we need to keep track of the field being accessed.\n+    tinst = tinst->is_aryptr()->with_field_offset(t->is_aryptr()->field_offset().get());\n+    \/\/ Keep array properties (not flat\/null-free)\n+    tinst = tinst->is_aryptr()->update_properties(t->is_aryptr());\n+    if (tinst == nullptr) {\n+      return false; \/\/ Skip dead path with inconsistent properties\n+    }\n+  }\n+\n@@ -4548,0 +4648,7 @@\n+          if (tn_t->isa_aryptr()) {\n+            \/\/ Keep array properties (not flat\/null-free)\n+            tinst = tinst->is_aryptr()->update_properties(tn_t->is_aryptr());\n+            if (tinst == nullptr) {\n+              continue; \/\/ Skip dead path with inconsistent properties\n+            }\n+          }\n@@ -4573,1 +4680,1 @@\n-      if(use->is_Mem() && use->in(MemNode::Address) == n) {\n+      if (use->is_Mem() && use->in(MemNode::Address) == n) {\n@@ -4609,0 +4716,3 @@\n+      } else if (use->Opcode() == Op_Return) {\n+        \/\/ Allocation is referenced by field of returned inline type\n+        assert(_compile->tf()->returns_inline_type_as_fields(), \"EA: unexpected reference by ReturnNode\");\n@@ -4622,1 +4732,1 @@\n-              op == Op_SubTypeCheck ||\n+              op == Op_SubTypeCheck || op == Op_InlineType || op == Op_FlatArrayCheck ||\n@@ -4726,0 +4836,3 @@\n+    } else if (n->is_CallLeaf() && n->as_CallLeaf()->_name != nullptr &&\n+               strcmp(n->as_CallLeaf()->_name, \"store_unknown_inline\") == 0) {\n+      n = n->as_CallLeaf()->proj_out(TypeFunc::Memory);\n@@ -4770,1 +4883,1 @@\n-      } else if(use->is_Mem()) {\n+      } else if (use->is_Mem()) {\n@@ -4779,0 +4892,4 @@\n+      } else if (use->is_CallLeaf() && use->as_CallLeaf()->_name != nullptr &&\n+                 strcmp(use->as_CallLeaf()->_name, \"store_unknown_inline\") == 0) {\n+        \/\/ store_unknown_inline overwrites destination array\n+        memnode_worklist.append_if_missing(use);\n@@ -4788,1 +4905,1 @@\n-              op == Op_StrEquals || op == Op_StrIndexOf || op == Op_StrIndexOfChar)) {\n+              op == Op_StrEquals || op == Op_StrIndexOf || op == Op_StrIndexOfChar || op == Op_FlatArrayCheck)) {\n@@ -4889,1 +5006,1 @@\n-  \/\/ chains as is done in split_memory_phi() since they  will\n+  \/\/ chains as is done in split_memory_phi() since they will\n","filename":"src\/hotspot\/share\/opto\/escape.cpp","additions":161,"deletions":44,"binary":false,"changes":205,"status":"modified"},{"patch":"@@ -236,1 +236,1 @@\n-      for (int i = node->req()-1; i >= 0; --i) {\n+      for (int i = node->len()-1; i >= 0; --i) {\n@@ -1593,0 +1593,3 @@\n+      case Op_CastI2N:\n+        early->add_inst(self);\n+        continue;\n","filename":"src\/hotspot\/share\/opto\/gcm.cpp","additions":4,"deletions":1,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -26,0 +26,2 @@\n+#include \"ci\/ciFlatArrayKlass.hpp\"\n+#include \"ci\/ciInlineKlass.hpp\"\n@@ -34,0 +36,1 @@\n+#include \"oops\/flatArrayKlass.hpp\"\n@@ -39,0 +42,1 @@\n+#include \"opto\/inlinetypenode.hpp\"\n@@ -42,0 +46,1 @@\n+#include \"opto\/narrowptrnode.hpp\"\n@@ -49,0 +54,1 @@\n+#include \"runtime\/stubRoutines.hpp\"\n@@ -55,1 +61,1 @@\n-GraphKit::GraphKit(JVMState* jvms)\n+GraphKit::GraphKit(JVMState* jvms, PhaseGVN* gvn)\n@@ -58,1 +64,1 @@\n-    _gvn(*C->initial_gvn()),\n+    _gvn((gvn != nullptr) ? *gvn : *C->initial_gvn()),\n@@ -61,0 +67,1 @@\n+  assert(gvn == nullptr || !gvn->is_IterGVN() || gvn->is_IterGVN()->delay_transform(), \"delay transform should be enabled\");\n@@ -64,0 +71,7 @@\n+#ifdef ASSERT\n+  if (_gvn.is_IterGVN() != nullptr) {\n+    assert(_gvn.is_IterGVN()->delay_transform(), \"Transformation must be delayed if IterGVN is used\");\n+    \/\/ Save the initial size of _for_igvn worklist for verification (see ~GraphKit)\n+    _worklist_size = _gvn.C->igvn_worklist()->size();\n+  }\n+#endif\n@@ -348,1 +362,2 @@\n-  assert(ex_jvms->sp() == phi_map->_jvms->sp(), \"matching stack sizes\");\n+  \/\/ TODO 8325632 Re-enable\n+  \/\/ assert(ex_jvms->sp() == phi_map->_jvms->sp(), \"matching stack sizes\");\n@@ -876,1 +891,1 @@\n-           (is_anewarray && code == Bytecodes::_multianewarray);\n+           (is_anewarray && (code == Bytecodes::_multianewarray));\n@@ -967,0 +982,2 @@\n+\n+  JVMState* callee_jvms = nullptr;\n@@ -992,2 +1009,3 @@\n-      for (j = 0; j < l; j++)\n-        call->set_req(p++, in_map->in(k+j));\n+      for (j = 0; j < l; j++) {\n+        call->set_req(p++, in_map->in(k + j));\n+      }\n@@ -1003,2 +1021,3 @@\n-      for (j = 0; j < l; j++)\n-        call->set_req(p++, in_map->in(k+j));\n+      for (j = 0; j < l; j++) {\n+        call->set_req(p++, in_map->in(k + j));\n+      }\n@@ -1043,0 +1062,1 @@\n+    callee_jvms = out_jvms;\n@@ -1218,1 +1238,1 @@\n-  return _gvn.transform(LoadKlassNode::make(_gvn, immutable_memory(), k_adr, TypeInstPtr::KLASS));\n+  return _gvn.transform(LoadKlassNode::make(_gvn, immutable_memory(), k_adr, TypeInstPtr::KLASS, TypeInstKlassPtr::OBJECT));\n@@ -1267,1 +1287,2 @@\n-                                  bool speculative) {\n+                                  bool speculative,\n+                                  bool null_marker_check) {\n@@ -1272,0 +1293,23 @@\n+  if (value->is_InlineType()) {\n+    \/\/ Null checking a scalarized but nullable inline type. Check the null marker\n+    \/\/ input instead of the oop input to avoid keeping buffer allocations alive.\n+    InlineTypeNode* vtptr = value->as_InlineType();\n+    while (vtptr->get_oop()->is_InlineType()) {\n+      vtptr = vtptr->get_oop()->as_InlineType();\n+    }\n+    null_check_common(vtptr->get_null_marker(), T_INT, assert_null, null_control, speculative, true);\n+    if (stopped()) {\n+      return top();\n+    }\n+    if (assert_null) {\n+      \/\/ TODO 8284443 Scalarize here (this currently leads to compilation bailouts)\n+      \/\/ vtptr = InlineTypeNode::make_null(_gvn, vtptr->type()->inline_klass());\n+      \/\/ replace_in_map(value, vtptr);\n+      \/\/ return vtptr;\n+      replace_in_map(value, null());\n+      return null();\n+    }\n+    bool do_replace_in_map = (null_control == nullptr || (*null_control) == top());\n+    return cast_not_null(value, do_replace_in_map);\n+  }\n+\n@@ -1375,1 +1419,1 @@\n-  } else if (type == T_OBJECT) {\n+  } else if (type == T_OBJECT || null_marker_check) {\n@@ -1449,1 +1493,0 @@\n-\n@@ -1453,0 +1496,9 @@\n+  if (obj->is_InlineType()) {\n+    Node* vt = obj->isa_InlineType()->clone_if_required(&gvn(), map(), do_replace_in_map);\n+    vt->as_InlineType()->set_null_marker(_gvn);\n+    vt = _gvn.transform(vt);\n+    if (do_replace_in_map) {\n+      replace_in_map(obj, vt);\n+    }\n+    return vt;\n+  }\n@@ -1469,0 +1521,11 @@\n+Node* GraphKit::cast_to_non_larval(Node* obj) {\n+  const Type* obj_type = gvn().type(obj);\n+  if (obj->is_InlineType() || !obj_type->is_inlinetypeptr()) {\n+    return obj;\n+  }\n+\n+  Node* new_obj = InlineTypeNode::make_from_oop(this, obj, obj_type->inline_klass());\n+  replace_in_map(obj, new_obj);\n+  return new_obj;\n+}\n+\n@@ -1581,0 +1644,1 @@\n+\n@@ -1634,1 +1698,3 @@\n-                                DecoratorSet decorators) {\n+                                DecoratorSet decorators,\n+                                bool safe_for_replace,\n+                                const InlineTypeNode* vt) {\n@@ -1647,0 +1713,7 @@\n+  if (val->is_InlineType()) {\n+    \/\/ Store to non-flat field. Buffer the inline type and make sure\n+    \/\/ the store is re-executed if the allocation triggers deoptimization.\n+    PreserveReexecuteState preexecs(this);\n+    jvms()->set_should_reexecute(true);\n+    val = val->as_InlineType()->buffer(this, safe_for_replace);\n+  }\n@@ -1650,1 +1723,1 @@\n-  C2ParseAccess access(this, decorators | C2_WRITE_ACCESS, bt, obj, addr);\n+  C2ParseAccess access(this, decorators | C2_WRITE_ACCESS, bt, obj, addr, nullptr, vt);\n@@ -1663,1 +1736,2 @@\n-                               DecoratorSet decorators) {\n+                               DecoratorSet decorators,\n+                               Node* ctl) {\n@@ -1669,1 +1743,1 @@\n-  C2ParseAccess access(this, decorators | C2_READ_ACCESS, bt, obj, addr);\n+  C2ParseAccess access(this, decorators | C2_READ_ACCESS, bt, obj, addr, ctl);\n@@ -1774,1 +1848,12 @@\n-  uint shift  = exact_log2(type2aelembytes(elembt));\n+  const TypeAryPtr* arytype = _gvn.type(ary)->is_aryptr();\n+  uint shift;\n+  if (arytype->is_flat() && arytype->klass_is_exact()) {\n+    \/\/ We can only determine the flat array layout statically if the klass is exact. Otherwise, we could have different\n+    \/\/ value classes at runtime with a potentially different layout. The caller needs to fall back to call\n+    \/\/ load\/store_unknown_inline_Type() at runtime. We could return a sentinel node for the non-exact case but that\n+    \/\/ might mess with other GVN transformations in between. Thus, we just continue in the else branch normally, even\n+    \/\/ though we don't need the address node in this case and throw it away again.\n+    shift = arytype->flat_log_elem_size();\n+  } else {\n+    shift = exact_log2(type2aelembytes(elembt));\n+  }\n@@ -1791,0 +1876,28 @@\n+Node* GraphKit::cast_to_flat_array(Node* array, ciInlineKlass* vk, bool is_null_free, bool is_not_null_free, bool is_atomic) {\n+  assert(vk->maybe_flat_in_array(), \"element of type %s cannot be flat in array\", vk->name()->as_utf8());\n+  if (!vk->has_nullable_atomic_layout()) {\n+    \/\/ Element does not have a nullable flat layout, cannot be nullable\n+    is_null_free = true;\n+  }\n+  if (!vk->has_atomic_layout() && !vk->has_non_atomic_layout()) {\n+    \/\/ Element does not have a null-free flat layout, cannot be null-free\n+    is_not_null_free = true;\n+  }\n+  if (is_null_free) {\n+    \/\/ TODO 8350865 Impossible type\n+    is_not_null_free = false;\n+  }\n+\n+  bool is_exact = is_null_free || is_not_null_free;\n+  ciArrayKlass* array_klass = ciArrayKlass::make(vk, is_null_free, is_atomic, true);\n+  assert(array_klass->is_elem_null_free() == is_null_free, \"inconsistency\");\n+  assert(array_klass->is_elem_atomic() == is_atomic, \"inconsistency\");\n+  const TypeAryPtr* arytype = TypeOopPtr::make_from_klass(array_klass)->isa_aryptr();\n+  arytype = arytype->cast_to_exactness(is_exact);\n+  arytype = arytype->cast_to_not_null_free(is_not_null_free);\n+  assert(arytype->is_null_free() == is_null_free, \"inconsistency\");\n+  assert(arytype->is_not_null_free() == is_not_null_free, \"inconsistency\");\n+  assert(arytype->is_atomic() == is_atomic, \"inconsistency\");\n+  return _gvn.transform(new CastPPNode(control(), array, arytype, ConstraintCastNode::StrongDependency));\n+}\n+\n@@ -1806,6 +1919,42 @@\n-void GraphKit::set_arguments_for_java_call(CallJavaNode* call) {\n-  \/\/ Add the call arguments:\n-  uint nargs = call->method()->arg_size();\n-  for (uint i = 0; i < nargs; i++) {\n-    Node* arg = argument(i);\n-    call->init_req(i + TypeFunc::Parms, arg);\n+void GraphKit::set_arguments_for_java_call(CallJavaNode* call, bool is_late_inline) {\n+  PreserveReexecuteState preexecs(this);\n+  if (EnableValhalla) {\n+    \/\/ Make sure the call is \"re-executed\", if buffering of inline type arguments triggers deoptimization.\n+    \/\/ At this point, the call hasn't been executed yet, so we will only ever execute the call once.\n+    jvms()->set_should_reexecute(true);\n+    int arg_size = method()->get_declared_signature_at_bci(bci())->arg_size_for_bc(java_bc());\n+    inc_sp(arg_size);\n+  }\n+  \/\/ Add the call arguments\n+  const TypeTuple* domain = call->tf()->domain_sig();\n+  uint nargs = domain->cnt();\n+  int arg_num = 0;\n+  for (uint i = TypeFunc::Parms, idx = TypeFunc::Parms; i < nargs; i++) {\n+    Node* arg = argument(i-TypeFunc::Parms);\n+    const Type* t = domain->field_at(i);\n+    \/\/ TODO 8284443 A static call to a mismatched method should still be scalarized\n+    if (t->is_inlinetypeptr() && !call->method()->get_Method()->mismatch() && call->method()->is_scalarized_arg(arg_num)) {\n+      \/\/ We don't pass inline type arguments by reference but instead pass each field of the inline type\n+      if (!arg->is_InlineType()) {\n+        assert(_gvn.type(arg)->is_zero_type() && !t->inline_klass()->is_null_free(), \"Unexpected argument type\");\n+        arg = InlineTypeNode::make_from_oop(this, arg, t->inline_klass());\n+      }\n+      InlineTypeNode* vt = arg->as_InlineType();\n+      vt->pass_fields(this, call, idx, true, !t->maybe_null());\n+      \/\/ If an inline type argument is passed as fields, attach the Method* to the call site\n+      \/\/ to be able to access the extended signature later via attached_method_before_pc().\n+      \/\/ For example, see CompiledMethod::preserve_callee_argument_oops().\n+      call->set_override_symbolic_info(true);\n+      \/\/ Register an evol dependency on the callee method to make sure that this method is deoptimized and\n+      \/\/ re-compiled with a non-scalarized calling convention if the callee method is later marked as mismatched.\n+      C->dependencies()->assert_evol_method(call->method());\n+      arg_num++;\n+      continue;\n+    } else if (arg->is_InlineType()) {\n+      \/\/ Pass inline type argument via oop to callee\n+      arg = arg->as_InlineType()->buffer(this, true);\n+    }\n+    if (t != Type::HALF) {\n+      arg_num++;\n+    }\n+    call->init_req(idx++, arg);\n@@ -1849,7 +1998,0 @@\n-  \/\/ Capture the return value, if any.\n-  Node* ret;\n-  if (call->method() == nullptr ||\n-      call->method()->return_type()->basic_type() == T_VOID)\n-        ret = top();\n-  else  ret = _gvn.transform(new ProjNode(call, TypeFunc::Parms));\n-\n@@ -1868,0 +2010,72 @@\n+\n+  \/\/ Capture the return value, if any.\n+  Node* ret;\n+  if (call->method() == nullptr || call->method()->return_type()->basic_type() == T_VOID) {\n+    ret = top();\n+  } else if (call->tf()->returns_inline_type_as_fields()) {\n+    \/\/ Return of multiple values (inline type fields): we create a\n+    \/\/ InlineType node, each field is a projection from the call.\n+    ciInlineKlass* vk = call->method()->return_type()->as_inline_klass();\n+    uint base_input = TypeFunc::Parms;\n+    ret = InlineTypeNode::make_from_multi(this, call, vk, base_input, false, false);\n+  } else {\n+    ret = _gvn.transform(new ProjNode(call, TypeFunc::Parms));\n+    ciType* t = call->method()->return_type();\n+    if (!t->is_loaded() && InlineTypeReturnedAsFields) {\n+      \/\/ The return type is unloaded but the callee might later be C2 compiled and then return\n+      \/\/ in scalarized form when the return type is loaded. Handle this similar to what we do in\n+      \/\/ PhaseMacroExpand::expand_mh_intrinsic_return by calling into the runtime to buffer.\n+      \/\/ It's a bit unfortunate because we will deopt anyway but the interpreter needs an oop.\n+      IdealKit ideal(this);\n+      IdealVariable res(ideal);\n+      ideal.declarations_done();\n+      ideal.if_then(ret, BoolTest::eq, ideal.makecon(TypePtr::NULL_PTR)); {\n+        \/\/ Return value is null\n+        ideal.set(res, ret);\n+      } ideal.else_(); {\n+        \/\/ Return value is non-null\n+        sync_kit(ideal);\n+\n+        \/\/ Change return type of call to scalarized return\n+        const TypeFunc* tf = call->_tf;\n+        const TypeTuple* domain = OptoRuntime::store_inline_type_fields_Type()->domain_cc();\n+        const TypeFunc* new_tf = TypeFunc::make(tf->domain_sig(), tf->domain_cc(), tf->range_sig(), domain);\n+        call->_tf = new_tf;\n+        _gvn.set_type(call, call->Value(&_gvn));\n+        _gvn.set_type(ret, ret->Value(&_gvn));\n+\n+        Node* store_to_buf_call = make_runtime_call(RC_NO_LEAF | RC_NO_IO,\n+                                                    OptoRuntime::store_inline_type_fields_Type(),\n+                                                    StubRoutines::store_inline_type_fields_to_buf(),\n+                                                    nullptr, TypePtr::BOTTOM, ret);\n+\n+        \/\/ We don't know how many values are returned. This assumes the\n+        \/\/ worst case, that all available registers are used.\n+        for (uint i = TypeFunc::Parms+1; i < domain->cnt(); i++) {\n+          if (domain->field_at(i) == Type::HALF) {\n+            store_to_buf_call->init_req(i, top());\n+            continue;\n+          }\n+          Node* proj =_gvn.transform(new ProjNode(call, i));\n+          store_to_buf_call->init_req(i, proj);\n+        }\n+        make_slow_call_ex(store_to_buf_call, env()->Throwable_klass(), false);\n+\n+        Node* buf = _gvn.transform(new ProjNode(store_to_buf_call, TypeFunc::Parms));\n+        const Type* buf_type = TypeOopPtr::make_from_klass(t->as_klass())->join_speculative(TypePtr::NOTNULL);\n+        buf = _gvn.transform(new CheckCastPPNode(control(), buf, buf_type));\n+\n+        ideal.set(res, buf);\n+        ideal.sync_kit(this);\n+      } ideal.end_if();\n+      sync_kit(ideal);\n+      ret = _gvn.transform(ideal.value(res));\n+    }\n+    if (t->is_klass()) {\n+      const Type* type = TypeOopPtr::make_from_klass(t->as_klass());\n+      if (type->is_inlinetypeptr()) {\n+        ret = InlineTypeNode::make_from_oop(this, ret, type->inline_klass());\n+      }\n+    }\n+  }\n+\n@@ -1958,2 +2172,1 @@\n-  CallProjections callprojs;\n-  call->extract_projections(&callprojs, true, do_asserts);\n+  CallProjections* callprojs = call->extract_projections(true, do_asserts);\n@@ -1968,2 +2181,2 @@\n-  if (callprojs.fallthrough_catchproj != nullptr) {\n-    C->gvn_replace_by(callprojs.fallthrough_catchproj, final_ctl);\n+  if (callprojs->fallthrough_catchproj != nullptr) {\n+    C->gvn_replace_by(callprojs->fallthrough_catchproj, final_ctl);\n@@ -1971,1 +2184,1 @@\n-  if (callprojs.fallthrough_memproj != nullptr) {\n+  if (callprojs->fallthrough_memproj != nullptr) {\n@@ -1976,1 +2189,1 @@\n-    C->gvn_replace_by(callprojs.fallthrough_memproj,   final_mem);\n+    C->gvn_replace_by(callprojs->fallthrough_memproj,   final_mem);\n@@ -1979,2 +2192,2 @@\n-  if (callprojs.fallthrough_ioproj != nullptr) {\n-    C->gvn_replace_by(callprojs.fallthrough_ioproj,    final_io);\n+  if (callprojs->fallthrough_ioproj != nullptr) {\n+    C->gvn_replace_by(callprojs->fallthrough_ioproj,    final_io);\n@@ -1984,2 +2197,6 @@\n-  if (callprojs.resproj != nullptr && result != nullptr) {\n-    C->gvn_replace_by(callprojs.resproj, result);\n+  if (callprojs->resproj[0] != nullptr && result != nullptr) {\n+    \/\/ If the inlined code is dead, the result projections for an inline type returned as\n+    \/\/ fields have not been replaced. They will go away once the call is replaced by TOP below.\n+    assert(callprojs->nb_resproj == 1 || (call->tf()->returns_inline_type_as_fields() && stopped()),\n+           \"unexpected number of results\");\n+    C->gvn_replace_by(callprojs->resproj[0], result);\n@@ -1990,2 +2207,2 @@\n-    if (callprojs.catchall_catchproj != nullptr) {\n-      C->gvn_replace_by(callprojs.catchall_catchproj, C->top());\n+    if (callprojs->catchall_catchproj != nullptr) {\n+      C->gvn_replace_by(callprojs->catchall_catchproj, C->top());\n@@ -1993,2 +2210,2 @@\n-    if (callprojs.catchall_memproj != nullptr) {\n-      C->gvn_replace_by(callprojs.catchall_memproj,   C->top());\n+    if (callprojs->catchall_memproj != nullptr) {\n+      C->gvn_replace_by(callprojs->catchall_memproj,   C->top());\n@@ -1996,2 +2213,2 @@\n-    if (callprojs.catchall_ioproj != nullptr) {\n-      C->gvn_replace_by(callprojs.catchall_ioproj,    C->top());\n+    if (callprojs->catchall_ioproj != nullptr) {\n+      C->gvn_replace_by(callprojs->catchall_ioproj,    C->top());\n@@ -2000,2 +2217,2 @@\n-    if (callprojs.exobj != nullptr) {\n-      C->gvn_replace_by(callprojs.exobj, C->top());\n+    if (callprojs->exobj != nullptr) {\n+      C->gvn_replace_by(callprojs->exobj, C->top());\n@@ -2012,2 +2229,2 @@\n-    if (callprojs.catchall_catchproj != nullptr) {\n-      C->gvn_replace_by(callprojs.catchall_catchproj, ekit.control());\n+    if (callprojs->catchall_catchproj != nullptr) {\n+      C->gvn_replace_by(callprojs->catchall_catchproj, ekit.control());\n@@ -2016,1 +2233,1 @@\n-    if (callprojs.catchall_memproj != nullptr) {\n+    if (callprojs->catchall_memproj != nullptr) {\n@@ -2018,1 +2235,1 @@\n-      C->gvn_replace_by(callprojs.catchall_memproj,   ex_mem);\n+      C->gvn_replace_by(callprojs->catchall_memproj,   ex_mem);\n@@ -2021,2 +2238,2 @@\n-    if (callprojs.catchall_ioproj != nullptr) {\n-      C->gvn_replace_by(callprojs.catchall_ioproj,    ekit.i_o());\n+    if (callprojs->catchall_ioproj != nullptr) {\n+      C->gvn_replace_by(callprojs->catchall_ioproj,    ekit.i_o());\n@@ -2026,2 +2243,2 @@\n-    if (callprojs.exobj != nullptr) {\n-      C->gvn_replace_by(callprojs.exobj, ex_oop);\n+    if (callprojs->exobj != nullptr) {\n+      C->gvn_replace_by(callprojs->exobj, ex_oop);\n@@ -2041,1 +2258,1 @@\n-  if (callprojs.fallthrough_catchproj != nullptr && !final_ctl->is_top() && do_replaced_nodes) {\n+  if (callprojs->fallthrough_catchproj != nullptr && !final_ctl->is_top() && do_replaced_nodes) {\n@@ -2241,1 +2458,1 @@\n-    const TypePtr* ptr = (ptr_kind == ProfileMaybeNull && current_type->speculative_maybe_null()) ? TypePtr::BOTTOM : TypePtr::NOTNULL;\n+    const TypePtr* ptr = (ptr_kind != ProfileNeverNull && current_type->speculative_maybe_null()) ? TypePtr::BOTTOM : TypePtr::NOTNULL;\n@@ -2264,1 +2481,1 @@\n-    const TypeOopPtr* spec_type = TypeOopPtr::make(TypePtr::BotPTR, Type::OffsetBot, TypeOopPtr::InstanceBot, speculative);\n+    const TypeOopPtr* spec_type = TypeOopPtr::make(TypePtr::BotPTR, Type::Offset::bottom, TypeOopPtr::InstanceBot, speculative);\n@@ -2298,2 +2515,9 @@\n-      if (!data->as_BitData()->null_seen()) {\n-        ptr_kind = ProfileNeverNull;\n+      if (java_bc() == Bytecodes::_aastore) {\n+        ciKlass* array_type = nullptr;\n+        ciKlass* element_type = nullptr;\n+        ProfilePtrKind element_ptr = ProfileMaybeNull;\n+        bool flat_array = true;\n+        bool null_free_array = true;\n+        method()->array_access_profiled_type(bci(), array_type, element_type, element_ptr, flat_array, null_free_array);\n+        exact_kls = element_type;\n+        ptr_kind = element_ptr;\n@@ -2301,7 +2525,11 @@\n-        assert(data->is_ReceiverTypeData(), \"bad profile data type\");\n-        ciReceiverTypeData* call = (ciReceiverTypeData*)data->as_ReceiverTypeData();\n-        uint i = 0;\n-        for (; i < call->row_limit(); i++) {\n-          ciKlass* receiver = call->receiver(i);\n-          if (receiver != nullptr) {\n-            break;\n+        if (!data->as_BitData()->null_seen()) {\n+          ptr_kind = ProfileNeverNull;\n+        } else {\n+          assert(data->is_ReceiverTypeData(), \"bad profile data type\");\n+          ciReceiverTypeData* call = (ciReceiverTypeData*)data->as_ReceiverTypeData();\n+          uint i = 0;\n+          for (; i < call->row_limit(); i++) {\n+            ciKlass* receiver = call->receiver(i);\n+            if (receiver != nullptr) {\n+              break;\n+            }\n@@ -2309,0 +2537,1 @@\n+          ptr_kind = (i == call->row_limit()) ? ProfileAlwaysNull : ProfileMaybeNull;\n@@ -2310,1 +2539,0 @@\n-        ptr_kind = (i == call->row_limit()) ? ProfileAlwaysNull : ProfileMaybeNull;\n@@ -2329,1 +2557,1 @@\n-  int             nargs = tf->domain()->cnt() - TypeFunc::Parms;\n+  int             nargs = tf->domain_sig()->cnt() - TypeFunc::Parms;\n@@ -2332,1 +2560,1 @@\n-    const Type *targ = tf->domain()->field_at(j + TypeFunc::Parms);\n+    const Type *targ = tf->domain_sig()->field_at(j + TypeFunc::Parms);\n@@ -2492,1 +2720,1 @@\n-    uint num_bits = call_type->range()->field_at(TypeFunc::Parms)->is_vect()->length_in_bytes() * BitsPerByte;\n+    uint num_bits = call_type->range_sig()->field_at(TypeFunc::Parms)->is_vect()->length_in_bytes() * BitsPerByte;\n@@ -2524,1 +2752,1 @@\n-  assert(call->in(call->req()-1) != nullptr, \"must initialize all parms\");\n+  assert(call->in(call->req()-1) != nullptr || (call->req()-1) > (TypeFunc::Parms+7), \"must initialize all parms\");\n@@ -2572,0 +2800,1 @@\n+\n@@ -2668,0 +2897,9 @@\n+  const TypeKlassPtr* klass_ptr_type = gvn.type(superklass)->is_klassptr();\n+  const TypeAryKlassPtr* ary_klass_t = klass_ptr_type->isa_aryklassptr();\n+  Node* vm_superklass = superklass;\n+  \/\/ TODO 8366668 Compute the VM type here for when we do a direct pointer comparison\n+  if (ary_klass_t && ary_klass_t->klass_is_exact() && ary_klass_t->exact_klass()->is_obj_array_klass()) {\n+    ary_klass_t = ary_klass_t->get_vm_type();\n+    vm_superklass = gvn.makecon(ary_klass_t);\n+  }\n+\n@@ -2705,1 +2943,1 @@\n-        IfNode* iff = gen_subtype_check_compare(*ctrl, subklass, superklass, BoolTest::eq, PROB_STATIC_FREQUENT, gvn, T_ADDRESS);\n+        IfNode* iff = gen_subtype_check_compare(*ctrl, subklass, vm_superklass, BoolTest::eq, PROB_STATIC_FREQUENT, gvn, T_ADDRESS);\n@@ -2727,1 +2965,2 @@\n-  bool might_be_cache = (chk_off_con == cacheoff_con);\n+  \/\/ TODO 8366668 Re-enable. This breaks test\/hotspot\/jtreg\/compiler\/c2\/irTests\/ProfileAtTypeCheck.java\n+  bool might_be_cache = true;\/\/(chk_off_con == cacheoff_con);\n@@ -2778,0 +3017,1 @@\n+        \/\/ TODO 8366668 Do we need adjustments here??\n@@ -2828,0 +3068,2 @@\n+  \/\/ TODO 8366668 Re-enable\n+\/*\n@@ -2832,1 +3074,1 @@\n-\n+*\/\n@@ -2836,1 +3078,1 @@\n-  IfNode *iff3 = gen_subtype_check_compare(*ctrl, subklass, superklass, BoolTest::eq, PROB_LIKELY(0.36f), gvn, T_ADDRESS);\n+  IfNode *iff3 = gen_subtype_check_compare(*ctrl, subklass, vm_superklass, BoolTest::eq, PROB_LIKELY(0.36f), gvn, T_ADDRESS);\n@@ -2874,0 +3116,5 @@\n+  const Type* sub_t = _gvn.type(obj_or_subklass);\n+  if (sub_t->make_oopptr() != nullptr && sub_t->make_oopptr()->is_inlinetypeptr()) {\n+    sub_t = TypeKlassPtr::make(sub_t->inline_klass());\n+    obj_or_subklass = makecon(sub_t);\n+  }\n@@ -2879,1 +3126,1 @@\n-    if (!_gvn.type(obj_or_subklass)->isa_klassptr()) {\n+    if (!sub_t->isa_klassptr()) {\n@@ -2897,2 +3144,1 @@\n-                                    float prob,\n-                                    Node* *casted_receiver) {\n+                                    float prob, Node* *casted_receiver) {\n@@ -2900,1 +3146,12 @@\n-\n+  Node* fail = top();\n+  const Type* rec_t = _gvn.type(receiver);\n+  if (rec_t->is_inlinetypeptr()) {\n+    if (klass->equals(rec_t->inline_klass())) {\n+      (*casted_receiver) = receiver; \/\/ Always passes\n+    } else {\n+      (*casted_receiver) = top();    \/\/ Always fails\n+      fail = control();\n+      set_control(top());\n+    }\n+    return fail;\n+  }\n@@ -2902,0 +3159,5 @@\n+  const TypeAryKlassPtr* ary_klass_t = tklass->isa_aryklassptr();\n+    \/\/ TODO 8366668 Compute the VM type\n+  if (ary_klass_t && ary_klass_t->klass_is_exact() && ary_klass_t->exact_klass()->is_obj_array_klass()) {\n+    tklass = ary_klass_t->get_vm_type();\n+  }\n@@ -2903,6 +3165,1 @@\n-  Node* want_klass = makecon(tklass);\n-  Node* cmp = _gvn.transform(new CmpPNode(recv_klass, want_klass));\n-  Node* bol = _gvn.transform(new BoolNode(cmp, BoolTest::eq));\n-  IfNode* iff = create_and_xform_if(control(), bol, prob, COUNT_UNKNOWN);\n-  set_control( _gvn.transform(new IfTrueNode (iff)));\n-  Node* fail = _gvn.transform(new IfFalseNode(iff));\n+  fail = type_check(recv_klass, tklass, prob);\n@@ -2912,2 +3169,2 @@\n-    const TypeOopPtr* recvx_type = tklass->as_instance_type();\n-    assert(recvx_type->klass_is_exact(), \"\");\n+    const TypeOopPtr* recv_xtype = tklass->as_instance_type();\n+    assert(recv_xtype->klass_is_exact(), \"\");\n@@ -2915,1 +3172,1 @@\n-    if (!receiver_type->higher_equal(recvx_type)) { \/\/ ignore redundant casts\n+    if (!receiver_type->higher_equal(recv_xtype)) { \/\/ ignore redundant casts\n@@ -2918,2 +3175,7 @@\n-      Node* cast = new CheckCastPPNode(control(), receiver, recvx_type);\n-      (*casted_receiver) = _gvn.transform(cast);\n+      Node* cast = new CheckCastPPNode(control(), receiver, recv_xtype);\n+      Node* res = _gvn.transform(cast);\n+      if (recv_xtype->is_inlinetypeptr()) {\n+        assert(!gvn().type(res)->maybe_null(), \"receiver should never be null\");\n+        res = InlineTypeNode::make_from_oop(this, res, recv_xtype->inline_klass());\n+      }\n+      (*casted_receiver) = res;\n@@ -2928,0 +3190,11 @@\n+Node* GraphKit::type_check(Node* recv_klass, const TypeKlassPtr* tklass,\n+                           float prob) {\n+  Node* want_klass = makecon(tklass);\n+  Node* cmp = _gvn.transform(new CmpPNode(recv_klass, want_klass));\n+  Node* bol = _gvn.transform(new BoolNode(cmp, BoolTest::eq));\n+  IfNode* iff = create_and_xform_if(control(), bol, prob, COUNT_UNKNOWN);\n+  set_control(_gvn.transform(new IfTrueNode (iff)));\n+  Node* fail = _gvn.transform(new IfFalseNode(iff));\n+  return fail;\n+}\n+\n@@ -2940,3 +3213,6 @@\n-    if (!receiver_type->higher_equal(recv_type)) { \/\/ ignore redundant casts\n-      Node* cast = new CheckCastPPNode(control(), receiver, recv_type);\n-      (*casted_receiver) = _gvn.transform(cast);\n+    if (receiver_type != nullptr && !receiver_type->higher_equal(recv_type)) { \/\/ ignore redundant casts\n+      Node* cast = _gvn.transform(new CheckCastPPNode(control(), receiver, recv_type));\n+      if (recv_type->is_inlinetypeptr()) {\n+        cast = InlineTypeNode::make_from_oop(this, cast, recv_type->inline_klass());\n+      }\n+      (*casted_receiver) = cast;\n@@ -3051,1 +3327,14 @@\n-  ciKlass* exact_kls = spec_klass == nullptr ? profile_has_unique_klass() : spec_klass;\n+  ciKlass* exact_kls = spec_klass;\n+  if (exact_kls == nullptr) {\n+    if (java_bc() == Bytecodes::_aastore) {\n+      ciKlass* array_type = nullptr;\n+      ciKlass* element_type = nullptr;\n+      ProfilePtrKind element_ptr = ProfileMaybeNull;\n+      bool flat_array = true;\n+      bool null_free_array = true;\n+      method()->array_access_profiled_type(bci(), array_type, element_type, element_ptr, flat_array, null_free_array);\n+      exact_kls = element_type;\n+    } else {\n+      exact_kls = profile_has_unique_klass();\n+    }\n+  }\n@@ -3181,1 +3470,1 @@\n-    if (subk->is_loaded()) {\n+    if (subk != nullptr && subk->is_loaded()) {\n@@ -3237,2 +3526,1 @@\n-Node* GraphKit::gen_checkcast(Node *obj, Node* superklass,\n-                              Node* *failure_control) {\n+Node* GraphKit::gen_checkcast(Node* obj, Node* superklass, Node* *failure_control, bool null_free, bool maybe_larval) {\n@@ -3241,0 +3529,16 @@\n+  const Type* obj_type = _gvn.type(obj);\n+  if (obj_type->is_inlinetypeptr() && !obj_type->maybe_null() && klass_ptr_type->klass_is_exact() && obj_type->inline_klass() == klass_ptr_type->exact_klass(true)) {\n+    \/\/ Special case: larval inline objects must not be scalarized. They are also generally not\n+    \/\/ allowed to participate in most operations except as the first operand of putfield, or as an\n+    \/\/ argument to a constructor invocation with it being a receiver, Unsafe::putXXX with it being\n+    \/\/ the first argument, or Unsafe::finishPrivateBuffer. This allows us to aggressively scalarize\n+    \/\/ value objects in all other places. This special case comes from the limitation of the Java\n+    \/\/ language, Unsafe::makePrivateBuffer returns an Object that is checkcast-ed to the concrete\n+    \/\/ value type. We must do this first because C->static_subtype_check may do nothing when\n+    \/\/ StressReflectiveCode is set.\n+    return obj;\n+  }\n+\n+  \/\/ Else it must be a non-larval object\n+  obj = cast_to_non_larval(obj);\n+\n@@ -3243,0 +3547,2 @@\n+  bool safe_for_replace = (failure_control == nullptr);\n+  assert(!null_free || toop->can_be_inline_type(), \"must be an inline type pointer\");\n@@ -3251,3 +3557,10 @@\n-    const TypeOopPtr* objtp = _gvn.type(obj)->isa_oopptr();\n-    if (objtp != nullptr) {\n-      switch (C->static_subtype_check(improved_klass_ptr_type, objtp->as_klass_type())) {\n+    const TypeKlassPtr* kptr = nullptr;\n+    if (obj_type->isa_oop_ptr()) {\n+      kptr = obj_type->is_oopptr()->as_klass_type();\n+    } else if (obj->is_InlineType()) {\n+      ciInlineKlass* vk = obj_type->inline_klass();\n+      kptr = TypeInstKlassPtr::make(TypePtr::NotNull, vk, Type::Offset(0));\n+    }\n+\n+    if (kptr != nullptr) {\n+      switch (C->static_subtype_check(improved_klass_ptr_type, kptr)) {\n@@ -3258,1 +3571,7 @@\n-        return record_profiled_receiver_for_speculation(obj);\n+        obj = record_profiled_receiver_for_speculation(obj);\n+        if (null_free) {\n+          assert(safe_for_replace, \"must be\");\n+          obj = null_check(obj);\n+        }\n+        assert(stopped() || !toop->is_inlinetypeptr() || obj->is_InlineType(), \"should have been scalarized\");\n+        return obj;\n@@ -3260,0 +3579,4 @@\n+        if (null_free) {\n+          assert(safe_for_replace, \"must be\");\n+          obj = null_check(obj);\n+        }\n@@ -3261,2 +3584,1 @@\n-        \/\/ A non-null value will always produce an exception.\n-        if (!objtp->maybe_null()) {\n+        if (obj_type->isa_oopptr() != nullptr && !obj_type->is_oopptr()->maybe_null()) {\n@@ -3279,1 +3601,0 @@\n-  bool safe_for_replace = false;\n@@ -3284,2 +3605,3 @@\n-    data = method()->method_data()->bci_to_data(bci());\n-    safe_for_replace = true;\n+    if (method()->method_data()->is_mature()) {\n+      data = method()->method_data()->bci_to_data(bci());\n+    }\n@@ -3292,0 +3614,3 @@\n+  _gvn.set_type(region, Type::CONTROL);\n+  _gvn.set_type(phi, toop);\n+\n@@ -3299,0 +3624,7 @@\n+  if (obj->is_InlineType()) {\n+    \/\/ Re-execute if buffering during triggers deoptimization\n+    PreserveReexecuteState preexecs(this);\n+    jvms()->set_should_reexecute(true);\n+    obj = obj->as_InlineType()->buffer(this, safe_for_replace);\n+  }\n+\n@@ -3301,1 +3633,7 @@\n-  Node* not_null_obj = null_check_oop(obj, &null_ctl, never_see_null, safe_for_replace, speculative_not_null);\n+  Node* not_null_obj = nullptr;\n+  if (null_free) {\n+    assert(safe_for_replace, \"must be\");\n+    not_null_obj = null_check(obj);\n+  } else {\n+    not_null_obj = null_check_oop(obj, &null_ctl, never_see_null, safe_for_replace, speculative_not_null);\n+  }\n@@ -3306,0 +3644,3 @@\n+    if (toop->is_inlinetypeptr()) {\n+      return InlineTypeNode::make_null(_gvn, toop->inline_klass());\n+    }\n@@ -3343,0 +3684,3 @@\n+      \/\/ Only improve the super class for constants which allows subsequent sub type checks to possibly be commoned up.\n+      \/\/ The other non-constant cases cannot be improved with a cast node here since they could be folded to top.\n+      \/\/ Additionally, the benefit would only be minor in non-constant cases.\n@@ -3346,1 +3690,0 @@\n-\n@@ -3384,1 +3727,164 @@\n-  return record_profiled_receiver_for_speculation(res);\n+  bool not_inline = !toop->can_be_inline_type();\n+  bool not_flat_in_array = !UseArrayFlattening || not_inline || (toop->is_inlinetypeptr() && !toop->inline_klass()->maybe_flat_in_array());\n+  if (EnableValhalla && (not_inline || not_flat_in_array)) {\n+    \/\/ Check if obj has been loaded from an array\n+    obj = obj->isa_DecodeN() ? obj->in(1) : obj;\n+    Node* array = nullptr;\n+    if (obj->isa_Load()) {\n+      Node* address = obj->in(MemNode::Address);\n+      if (address->isa_AddP()) {\n+        array = address->as_AddP()->in(AddPNode::Base);\n+      }\n+    } else if (obj->is_Phi()) {\n+      Node* region = obj->in(0);\n+      \/\/ TODO make this more robust (see JDK-8231346)\n+      if (region->req() == 3 && region->in(2) != nullptr && region->in(2)->in(0) != nullptr) {\n+        IfNode* iff = region->in(2)->in(0)->isa_If();\n+        if (iff != nullptr) {\n+          iff->is_flat_array_check(&_gvn, &array);\n+        }\n+      }\n+    }\n+    if (array != nullptr) {\n+      const TypeAryPtr* ary_t = _gvn.type(array)->isa_aryptr();\n+      if (ary_t != nullptr) {\n+        if (!ary_t->is_not_null_free() && !ary_t->is_null_free() && not_inline) {\n+          \/\/ Casting array element to a non-inline-type, mark array as not null-free.\n+          Node* cast = _gvn.transform(new CheckCastPPNode(control(), array, ary_t->cast_to_not_null_free()));\n+          replace_in_map(array, cast);\n+          array = cast;\n+        }\n+        if (!ary_t->is_not_flat() && !ary_t->is_flat() && not_flat_in_array) {\n+          \/\/ Casting array element to a non-flat-in-array type, mark array as not flat.\n+          Node* cast = _gvn.transform(new CheckCastPPNode(control(), array, ary_t->cast_to_not_flat()));\n+          replace_in_map(array, cast);\n+          array = cast;\n+        }\n+      }\n+    }\n+  }\n+\n+  if (!stopped() && !res->is_InlineType()) {\n+    res = record_profiled_receiver_for_speculation(res);\n+    if (toop->is_inlinetypeptr() && !maybe_larval) {\n+      Node* vt = InlineTypeNode::make_from_oop(this, res, toop->inline_klass());\n+      res = vt;\n+      if (safe_for_replace) {\n+        replace_in_map(obj, vt);\n+        replace_in_map(not_null_obj, vt);\n+        replace_in_map(res, vt);\n+      }\n+    }\n+  }\n+  return res;\n+}\n+\n+Node* GraphKit::mark_word_test(Node* obj, uintptr_t mask_val, bool eq, bool check_lock) {\n+  \/\/ Load markword\n+  Node* mark_adr = basic_plus_adr(obj, oopDesc::mark_offset_in_bytes());\n+  Node* mark = make_load(nullptr, mark_adr, TypeX_X, TypeX_X->basic_type(), MemNode::unordered);\n+  if (check_lock) {\n+    \/\/ Check if obj is locked\n+    Node* locked_bit = MakeConX(markWord::unlocked_value);\n+    locked_bit = _gvn.transform(new AndXNode(locked_bit, mark));\n+    Node* cmp = _gvn.transform(new CmpXNode(locked_bit, MakeConX(0)));\n+    Node* is_unlocked = _gvn.transform(new BoolNode(cmp, BoolTest::ne));\n+    IfNode* iff = new IfNode(control(), is_unlocked, PROB_MAX, COUNT_UNKNOWN);\n+    _gvn.transform(iff);\n+    Node* locked_region = new RegionNode(3);\n+    Node* mark_phi = new PhiNode(locked_region, TypeX_X);\n+\n+    \/\/ Unlocked: Use bits from mark word\n+    locked_region->init_req(1, _gvn.transform(new IfTrueNode(iff)));\n+    mark_phi->init_req(1, mark);\n+\n+    \/\/ Locked: Load prototype header from klass\n+    set_control(_gvn.transform(new IfFalseNode(iff)));\n+    \/\/ Make loads control dependent to make sure they are only executed if array is locked\n+    Node* klass_adr = basic_plus_adr(obj, oopDesc::klass_offset_in_bytes());\n+    Node* klass = _gvn.transform(LoadKlassNode::make(_gvn, C->immutable_memory(), klass_adr, TypeInstPtr::KLASS, TypeInstKlassPtr::OBJECT));\n+    Node* proto_adr = basic_plus_adr(klass, in_bytes(Klass::prototype_header_offset()));\n+    Node* proto = _gvn.transform(LoadNode::make(_gvn, control(), C->immutable_memory(), proto_adr, proto_adr->bottom_type()->is_ptr(), TypeX_X, TypeX_X->basic_type(), MemNode::unordered));\n+\n+    locked_region->init_req(2, control());\n+    mark_phi->init_req(2, proto);\n+    set_control(_gvn.transform(locked_region));\n+    record_for_igvn(locked_region);\n+\n+    mark = mark_phi;\n+  }\n+\n+  \/\/ Now check if mark word bits are set\n+  Node* mask = MakeConX(mask_val);\n+  Node* masked = _gvn.transform(new AndXNode(_gvn.transform(mark), mask));\n+  record_for_igvn(masked); \/\/ Give it a chance to be optimized out by IGVN\n+  Node* cmp = _gvn.transform(new CmpXNode(masked, mask));\n+  return _gvn.transform(new BoolNode(cmp, eq ? BoolTest::eq : BoolTest::ne));\n+}\n+\n+Node* GraphKit::inline_type_test(Node* obj, bool is_inline) {\n+  return mark_word_test(obj, markWord::inline_type_pattern, is_inline, \/* check_lock = *\/ false);\n+}\n+\n+Node* GraphKit::flat_array_test(Node* array_or_klass, bool flat) {\n+  \/\/ We can't use immutable memory here because the mark word is mutable.\n+  \/\/ PhaseIdealLoop::move_flat_array_check_out_of_loop will make sure the\n+  \/\/ check is moved out of loops (mainly to enable loop unswitching).\n+  Node* cmp = _gvn.transform(new FlatArrayCheckNode(C, memory(Compile::AliasIdxRaw), array_or_klass));\n+  record_for_igvn(cmp); \/\/ Give it a chance to be optimized out by IGVN\n+  return _gvn.transform(new BoolNode(cmp, flat ? BoolTest::eq : BoolTest::ne));\n+}\n+\n+Node* GraphKit::null_free_array_test(Node* array, bool null_free) {\n+  return mark_word_test(array, markWord::null_free_array_bit_in_place, null_free);\n+}\n+\n+Node* GraphKit::null_free_atomic_array_test(Node* array, ciInlineKlass* vk) {\n+  assert(vk->has_atomic_layout() || vk->has_non_atomic_layout(), \"Can't be null-free and flat\");\n+\n+  \/\/ TODO 8350865 Add a stress flag to always access atomic if layout exists?\n+  if (!vk->has_non_atomic_layout()) {\n+    return intcon(1); \/\/ Always atomic\n+  } else if (!vk->has_atomic_layout()) {\n+    return intcon(0); \/\/ Never atomic\n+  }\n+\n+  Node* array_klass = load_object_klass(array);\n+  int layout_kind_offset = in_bytes(FlatArrayKlass::layout_kind_offset());\n+  Node* layout_kind_addr = basic_plus_adr(array_klass, array_klass, layout_kind_offset);\n+  Node* layout_kind = make_load(nullptr, layout_kind_addr, TypeInt::INT, T_INT, MemNode::unordered);\n+  Node* cmp = _gvn.transform(new CmpINode(layout_kind, intcon((int)LayoutKind::ATOMIC_FLAT)));\n+  return _gvn.transform(new BoolNode(cmp, BoolTest::eq));\n+}\n+\n+\/\/ Deoptimize if 'ary' is a null-free inline type array and 'val' is null\n+Node* GraphKit::inline_array_null_guard(Node* ary, Node* val, int nargs, bool safe_for_replace) {\n+  RegionNode* region = new RegionNode(3);\n+  Node* null_ctl = top();\n+  null_check_oop(val, &null_ctl);\n+  if (null_ctl != top()) {\n+    PreserveJVMState pjvms(this);\n+    set_control(null_ctl);\n+    {\n+      \/\/ Deoptimize if null-free array\n+      BuildCutout unless(this, null_free_array_test(ary, \/* null_free = *\/ false), PROB_MAX);\n+      inc_sp(nargs);\n+      uncommon_trap(Deoptimization::Reason_null_check,\n+                    Deoptimization::Action_none);\n+    }\n+    region->init_req(1, control());\n+  }\n+  region->init_req(2, control());\n+  set_control(_gvn.transform(region));\n+  record_for_igvn(region);\n+  if (_gvn.type(val) == TypePtr::NULL_PTR) {\n+    \/\/ Since we were just successfully storing null, the array can't be null free.\n+    const TypeAryPtr* ary_t = _gvn.type(ary)->is_aryptr();\n+    ary_t = ary_t->cast_to_not_null_free();\n+    Node* cast = _gvn.transform(new CheckCastPPNode(control(), ary, ary_t));\n+    if (safe_for_replace) {\n+      replace_in_map(ary, cast);\n+    }\n+    ary = cast;\n+  }\n+  return ary;\n@@ -3452,0 +3958,1 @@\n+\n@@ -3520,0 +4027,1 @@\n+  assert(!obj->is_InlineType(), \"should not unlock on inline type\");\n@@ -3560,1 +4068,8 @@\n-    if (xklass || (klass_t->isa_aryklassptr() && klass_t->is_aryklassptr()->elem() != Type::BOTTOM)) {\n+    bool can_be_flat = false;\n+    const TypeAryPtr* ary_type = klass_t->as_instance_type()->isa_aryptr();\n+    if (UseArrayFlattening && !xklass && ary_type != nullptr && !ary_type->is_null_free()) {\n+      \/\/ Don't constant fold if the runtime type might be a flat array but the static type is not.\n+      const TypeOopPtr* elem = ary_type->elem()->make_oopptr();\n+      can_be_flat = ary_type->can_be_inline_array() && (!elem->is_inlinetypeptr() || elem->inline_klass()->maybe_flat_in_array());\n+    }\n+    if (!can_be_flat && (xklass || (klass_t->isa_aryklassptr() && klass_t->is_aryklassptr()->elem() != Type::BOTTOM))) {\n@@ -3562,2 +4077,4 @@\n-      if (klass_t->isa_aryklassptr()) {\n-        BasicType elem = klass_t->as_instance_type()->isa_aryptr()->elem()->array_element_basic_type();\n+      if (klass_t->is_flat()) {\n+        lhelper = ary_type->flat_layout_helper();\n+      } else if (klass_t->isa_aryklassptr()) {\n+        BasicType elem = ary_type->elem()->array_element_basic_type();\n@@ -3592,1 +4109,3 @@\n-  kit.set_memory(init_out_raw, alias_idx);\n+  if (init_out_raw != nullptr) {\n+    kit.set_memory(init_out_raw, alias_idx);\n+  }\n@@ -3631,0 +4150,1 @@\n+    _gvn.set_type(minit_in, Type::MEMORY);\n@@ -3638,3 +4158,28 @@\n-      const TypePtr* telemref = oop_type->add_offset(Type::OffsetBot);\n-      int            elemidx  = C->get_alias_index(telemref);\n-      hook_memory_on_init(*this, elemidx, minit_in, minit_out);\n+      const TypeAryPtr* arytype = oop_type->is_aryptr();\n+      if (arytype->is_flat()) {\n+        \/\/ Initially all flat array accesses share a single slice\n+        \/\/ but that changes after parsing. Prepare the memory graph so\n+        \/\/ it can optimize flat array accesses properly once they\n+        \/\/ don't share a single slice.\n+        assert(C->flat_accesses_share_alias(), \"should be set at parse time\");\n+        C->set_flat_accesses_share_alias(false);\n+        ciInlineKlass* vk = arytype->elem()->inline_klass();\n+        for (int i = 0, len = vk->nof_nonstatic_fields(); i < len; i++) {\n+          ciField* field = vk->nonstatic_field_at(i);\n+          if (field->offset_in_bytes() >= TrackedInitializationLimit * HeapWordSize)\n+            continue;  \/\/ do not bother to track really large numbers of fields\n+          int off_in_vt = field->offset_in_bytes() - vk->payload_offset();\n+          const TypePtr* adr_type = arytype->with_field_offset(off_in_vt)->add_offset(Type::OffsetBot);\n+          int fieldidx = C->get_alias_index(adr_type, true);\n+          \/\/ Pass nullptr for init_out. Having per flat array element field memory edges as uses of the Initialize node\n+          \/\/ can result in per flat array field Phis to be created which confuses the logic of\n+          \/\/ Compile::adjust_flat_array_access_aliases().\n+          hook_memory_on_init(*this, fieldidx, minit_in, nullptr);\n+        }\n+        C->set_flat_accesses_share_alias(true);\n+        hook_memory_on_init(*this, C->get_alias_index(TypeAryPtr::INLINES), minit_in, minit_out);\n+      } else {\n+        const TypePtr* telemref = oop_type->add_offset(Type::OffsetBot);\n+        int            elemidx  = C->get_alias_index(telemref);\n+        hook_memory_on_init(*this, elemidx, minit_in, minit_out);\n+      }\n@@ -3642,0 +4187,1 @@\n+      set_memory(minit_out, C->get_alias_index(oop_type)); \/\/ mark word\n@@ -3692,1 +4238,2 @@\n-                             bool deoptimize_on_exception) {\n+                             bool deoptimize_on_exception,\n+                             InlineTypeNode* inline_type_node) {\n@@ -3699,1 +4246,1 @@\n-  int   layout_is_con = (layout_val == nullptr);\n+  bool  layout_is_con = (layout_val == nullptr);\n@@ -3750,1 +4297,1 @@\n-  \/\/ since GC and deoptimization can happened.\n+  \/\/ since GC and deoptimization can happen.\n@@ -3757,1 +4304,1 @@\n-                                         initial_slow_test);\n+                                         initial_slow_test, inline_type_node);\n@@ -3763,1 +4310,1 @@\n-\/\/ helper for both newarray and anewarray\n+\/\/ helper for newarray and anewarray\n@@ -3773,1 +4320,2 @@\n-                          bool deoptimize_on_exception) {\n+                          bool deoptimize_on_exception,\n+                          Node* init_val) {\n@@ -3776,1 +4324,1 @@\n-  int   layout_is_con = (layout_val == nullptr);\n+  bool  layout_is_con = (layout_val == nullptr);\n@@ -3806,3 +4354,1 @@\n-    assert(fast_size_limit == 0 || count_leading_zeros(fast_size_limit) > static_cast<unsigned>(LogBytesPerLong - log2_esize),\n-           \"fast_size_limit (%d) overflow when shifted left by %d\", fast_size_limit, LogBytesPerLong - log2_esize);\n-    fast_size_limit <<= (LogBytesPerLong - log2_esize);\n+    fast_size_limit <<= MAX2(LogBytesPerLong - log2_esize, 0);\n@@ -3825,0 +4371,1 @@\n+    bool is_flat_array = Klass::layout_helper_is_flatArray(layout_con);\n@@ -3827,1 +4374,1 @@\n-    assert((hsize & right_n_bits(eshift)) == 0, \"hsize is pre-rounded\");\n+    assert(is_flat_array || (hsize & right_n_bits(eshift)) == 0, \"hsize is pre-rounded\");\n@@ -3916,1 +4463,1 @@\n-  \/\/ since GC and deoptimization can happened.\n+  \/\/ since GC and deoptimization can happen.\n@@ -3925,1 +4472,21 @@\n-  const TypeOopPtr* ary_type = _gvn.type(klass_node)->is_klassptr()->as_instance_type();\n+  const TypeKlassPtr* ary_klass = _gvn.type(klass_node)->isa_klassptr();\n+  const TypeOopPtr* ary_type = ary_klass->as_instance_type();\n+\n+  Node* raw_init_value = nullptr;\n+  if (init_val != nullptr) {\n+    \/\/ TODO 8350865 Fast non-zero init not implemented yet for flat, null-free arrays\n+    if (ary_type->is_flat()) {\n+      initial_slow_test = intcon(1);\n+    }\n+\n+    if (UseCompressedOops) {\n+      \/\/ With compressed oops, the 64-bit init value is built from two 32-bit compressed oops\n+      init_val = _gvn.transform(new EncodePNode(init_val, init_val->bottom_type()->make_narrowoop()));\n+      Node* lower = _gvn.transform(new CastP2XNode(control(), init_val));\n+      Node* upper = _gvn.transform(new LShiftLNode(lower, intcon(32)));\n+      raw_init_value = _gvn.transform(new OrLNode(lower, upper));\n+    } else {\n+      raw_init_value = _gvn.transform(new CastP2XNode(control(), init_val));\n+    }\n+  }\n+\n@@ -3940,2 +4507,2 @@\n-                            length, valid_length_test);\n-\n+                            length, valid_length_test,\n+                            init_val, raw_init_value);\n@@ -4089,1 +4656,1 @@\n-                                                     false, nullptr, 0);\n+                                                     false, nullptr, Type::Offset(0));\n@@ -4092,2 +4659,2 @@\n-                                                  TypeAry::make(TypeInt::BYTE, TypeInt::POS),\n-                                                  ciTypeArrayKlass::make(T_BYTE), true, 0);\n+                                                  TypeAry::make(TypeInt::BYTE, TypeInt::POS, false, false, true, true),\n+                                                  ciTypeArrayKlass::make(T_BYTE), true, Type::Offset(0));\n@@ -4106,1 +4673,1 @@\n-                                                     false, nullptr, 0);\n+                                                     false, nullptr, Type::Offset(0));\n@@ -4118,1 +4685,1 @@\n-                                                     false, nullptr, 0);\n+                                                     false, nullptr, Type::Offset(0));\n@@ -4128,1 +4695,1 @@\n-                                                     false, nullptr, 0);\n+                                                     false, nullptr, Type::Offset(0));\n@@ -4241,1 +4808,7 @@\n-    return makecon(con_type);\n+    Node* con = makecon(con_type);\n+    if (field->type()->is_inlinetype()) {\n+      con = InlineTypeNode::make_from_oop(this, con, field->type()->as_inline_klass());\n+    } else if (con_type->is_inlinetypeptr()) {\n+      con = InlineTypeNode::make_from_oop(this, con, con_type->inline_klass());\n+    }\n+    return con;\n@@ -4246,0 +4819,9 @@\n+\/\/---------------------------load_mirror_from_klass----------------------------\n+\/\/ Given a klass oop, load its java mirror (a java.lang.Class oop).\n+Node* GraphKit::load_mirror_from_klass(Node* klass) {\n+  Node* p = basic_plus_adr(klass, in_bytes(Klass::java_mirror_offset()));\n+  Node* load = make_load(nullptr, p, TypeRawPtr::NOTNULL, T_ADDRESS, MemNode::unordered);\n+  \/\/ mirror = ((OopHandle)mirror)->resolve();\n+  return access_load(load, TypeInstPtr::MIRROR, T_OBJECT, IN_NATIVE);\n+}\n+\n@@ -4247,1 +4829,1 @@\n-  const TypeOopPtr* obj_type = obj->bottom_type()->isa_oopptr();\n+  const Type* obj_type = obj->bottom_type();\n@@ -4249,1 +4831,1 @@\n-  if (obj_type != nullptr && sig_type->is_loaded() && !obj_type->higher_equal(sig_type)) {\n+  if (obj_type->isa_oopptr() && sig_type->is_loaded() && !obj_type->higher_equal(sig_type)) {\n@@ -4252,1 +4834,4 @@\n-    return casted_obj;\n+    obj = casted_obj;\n+  }\n+  if (sig_type->is_inlinetypeptr()) {\n+    obj = InlineTypeNode::make_from_oop(this, obj, sig_type->inline_klass());\n","filename":"src\/hotspot\/share\/opto\/graphKit.cpp","additions":728,"deletions":143,"binary":false,"changes":871,"status":"modified"},{"patch":"@@ -166,0 +166,1 @@\n+  void if_then(Node* bol, float prob = PROB_FAIR, float cnt = COUNT_UNKNOWN, bool push_new_state = true);\n","filename":"src\/hotspot\/share\/opto\/idealKit.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -1274,0 +1274,17 @@\n+\/\/ Returns true if this IfNode belongs to a flat array check\n+\/\/ and returns the corresponding array in the 'array' parameter.\n+bool IfNode::is_flat_array_check(PhaseTransform* phase, Node** array) {\n+  Node* bol = in(1);\n+  if (!bol->is_Bool()) {\n+    return false;\n+  }\n+  Node* cmp = bol->in(1);\n+  if (cmp->isa_FlatArrayCheck()) {\n+    if (array != nullptr) {\n+      *array = cmp->in(FlatArrayCheckNode::ArrayOrKlass);\n+    }\n+    return true;\n+  }\n+  return false;\n+}\n+\n","filename":"src\/hotspot\/share\/opto\/ifnode.cpp","additions":17,"deletions":0,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -0,0 +1,1652 @@\n+\/*\n+ * Copyright (c) 2017, 2025, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"ci\/ciInlineKlass.hpp\"\n+#include \"gc\/shared\/barrierSet.hpp\"\n+#include \"gc\/shared\/c2\/barrierSetC2.hpp\"\n+#include \"gc\/shared\/gc_globals.hpp\"\n+#include \"oops\/accessDecorators.hpp\"\n+#include \"opto\/addnode.hpp\"\n+#include \"opto\/castnode.hpp\"\n+#include \"opto\/convertnode.hpp\"\n+#include \"opto\/graphKit.hpp\"\n+#include \"opto\/inlinetypenode.hpp\"\n+#include \"opto\/movenode.hpp\"\n+#include \"opto\/narrowptrnode.hpp\"\n+#include \"opto\/opcodes.hpp\"\n+#include \"opto\/phaseX.hpp\"\n+#include \"opto\/rootnode.hpp\"\n+#include \"opto\/type.hpp\"\n+#include \"utilities\/globalDefinitions.hpp\"\n+\n+\/\/ Clones the inline type to handle control flow merges involving multiple inline types.\n+\/\/ The inputs are replaced by PhiNodes to represent the merged values for the given region.\n+InlineTypeNode* InlineTypeNode::clone_with_phis(PhaseGVN* gvn, Node* region, SafePointNode* map, bool is_non_null) {\n+  InlineTypeNode* vt = clone_if_required(gvn, map);\n+  const Type* t = Type::get_const_type(inline_klass());\n+  gvn->set_type(vt, t);\n+  vt->as_InlineType()->set_type(t);\n+\n+  \/\/ Create a PhiNode for merging the oop values\n+  PhiNode* oop = PhiNode::make(region, vt->get_oop(), t);\n+  gvn->set_type(oop, t);\n+  gvn->record_for_igvn(oop);\n+  vt->set_oop(*gvn, oop);\n+\n+  \/\/ Create a PhiNode for merging the is_buffered values\n+  t = Type::get_const_basic_type(T_BOOLEAN);\n+  Node* is_buffered_node = PhiNode::make(region, vt->get_is_buffered(), t);\n+  gvn->set_type(is_buffered_node, t);\n+  gvn->record_for_igvn(is_buffered_node);\n+  vt->set_req(IsBuffered, is_buffered_node);\n+\n+  \/\/ Create a PhiNode for merging the null_marker values\n+  Node* null_marker_node;\n+  if (is_non_null) {\n+    null_marker_node = gvn->intcon(1);\n+  } else {\n+    t = Type::get_const_basic_type(T_BOOLEAN);\n+    null_marker_node = PhiNode::make(region, vt->get_null_marker(), t);\n+    gvn->set_type(null_marker_node, t);\n+    gvn->record_for_igvn(null_marker_node);\n+  }\n+  vt->set_req(NullMarker, null_marker_node);\n+\n+  \/\/ Create a PhiNode each for merging the field values\n+  for (uint i = 0; i < vt->field_count(); ++i) {\n+    ciType* type = vt->field_type(i);\n+    Node*  value = vt->field_value(i);\n+    \/\/ We limit scalarization for inline types with circular fields and can therefore observe nodes\n+    \/\/ of the same type but with different scalarization depth during GVN. To avoid inconsistencies\n+    \/\/ during merging, make sure that we only create Phis for fields that are guaranteed to be scalarized.\n+    bool no_circularity = !gvn->C->has_circular_inline_type() || field_is_flat(i);\n+    if (type->is_inlinetype() && no_circularity) {\n+      \/\/ Handle inline type fields recursively\n+      value = value->as_InlineType()->clone_with_phis(gvn, region, map);\n+    } else {\n+      t = Type::get_const_type(type);\n+      value = PhiNode::make(region, value, t);\n+      gvn->set_type(value, t);\n+      gvn->record_for_igvn(value);\n+    }\n+    vt->set_field_value(i, value);\n+  }\n+  gvn->record_for_igvn(vt);\n+  return vt;\n+}\n+\n+\/\/ Checks if the inputs of the InlineTypeNode were replaced by PhiNodes\n+\/\/ for the given region (see InlineTypeNode::clone_with_phis).\n+bool InlineTypeNode::has_phi_inputs(Node* region) {\n+  \/\/ Check oop input\n+  bool result = get_oop()->is_Phi() && get_oop()->as_Phi()->region() == region;\n+#ifdef ASSERT\n+  if (result) {\n+    \/\/ Check all field value inputs for consistency\n+    for (uint i = Values; i < field_count(); ++i) {\n+      Node* n = in(i);\n+      if (n->is_InlineType()) {\n+        assert(n->as_InlineType()->has_phi_inputs(region), \"inconsistent phi inputs\");\n+      } else {\n+        assert(n->is_Phi() && n->as_Phi()->region() == region, \"inconsistent phi inputs\");\n+      }\n+    }\n+  }\n+#endif\n+  return result;\n+}\n+\n+\/\/ Merges 'this' with 'other' by updating the input PhiNodes added by 'clone_with_phis'\n+InlineTypeNode* InlineTypeNode::merge_with(PhaseGVN* gvn, const InlineTypeNode* other, int pnum, bool transform) {\n+  assert(inline_klass() == other->inline_klass(), \"Merging incompatible types\");\n+\n+  \/\/ Merge oop inputs\n+  PhiNode* phi = get_oop()->as_Phi();\n+  phi->set_req(pnum, other->get_oop());\n+  if (transform) {\n+    set_oop(*gvn, gvn->transform(phi));\n+  }\n+\n+  \/\/ Merge is_buffered inputs\n+  phi = get_is_buffered()->as_Phi();\n+  phi->set_req(pnum, other->get_is_buffered());\n+  if (transform) {\n+    set_req(IsBuffered, gvn->transform(phi));\n+  }\n+\n+  \/\/ Merge null_marker inputs\n+  Node* null_marker = get_null_marker();\n+  if (null_marker->is_Phi()) {\n+    phi = null_marker->as_Phi();\n+    phi->set_req(pnum, other->get_null_marker());\n+    if (transform) {\n+      set_req(NullMarker, gvn->transform(phi));\n+    }\n+  } else {\n+    assert(null_marker->find_int_con(0) == 1, \"only with a non null inline type\");\n+  }\n+\n+  \/\/ Merge field values\n+  for (uint i = 0; i < field_count(); ++i) {\n+    Node* val1 =        field_value(i);\n+    Node* val2 = other->field_value(i);\n+    if (val1->is_InlineType()) {\n+      if (val2->is_Phi()) {\n+        val2 = gvn->transform(val2);\n+      }\n+      val1->as_InlineType()->merge_with(gvn, val2->as_InlineType(), pnum, transform);\n+    } else {\n+      assert(val1->is_Phi(), \"must be a phi node\");\n+      val1->set_req(pnum, val2);\n+    }\n+    if (transform) {\n+      set_field_value(i, gvn->transform(val1));\n+    }\n+  }\n+  return this;\n+}\n+\n+\/\/ Adds a new merge path to an inline type node with phi inputs\n+void InlineTypeNode::add_new_path(Node* region) {\n+  assert(has_phi_inputs(region), \"must have phi inputs\");\n+\n+  PhiNode* phi = get_oop()->as_Phi();\n+  phi->add_req(nullptr);\n+  assert(phi->req() == region->req(), \"must be same size as region\");\n+\n+  phi = get_is_buffered()->as_Phi();\n+  phi->add_req(nullptr);\n+  assert(phi->req() == region->req(), \"must be same size as region\");\n+\n+  phi = get_null_marker()->as_Phi();\n+  phi->add_req(nullptr);\n+  assert(phi->req() == region->req(), \"must be same size as region\");\n+\n+  for (uint i = 0; i < field_count(); ++i) {\n+    Node* val = field_value(i);\n+    if (val->is_InlineType()) {\n+      val->as_InlineType()->add_new_path(region);\n+    } else {\n+      val->as_Phi()->add_req(nullptr);\n+      assert(val->req() == region->req(), \"must be same size as region\");\n+    }\n+  }\n+}\n+\n+Node* InlineTypeNode::field_value(uint index) const {\n+  assert(index < field_count(), \"index out of bounds\");\n+  return in(Values + index);\n+}\n+\n+\/\/ Get the value of the field at the given offset.\n+\/\/ If 'recursive' is true, flat inline type fields will be resolved recursively.\n+Node* InlineTypeNode::field_value_by_offset(int offset, bool recursive) const {\n+  \/\/ Find the declared field which contains the field we are looking for\n+  int index = inline_klass()->field_index_by_offset(offset);\n+  Node* value = field_value(index);\n+  assert(value != nullptr, \"field value not found\");\n+\n+  if (!recursive || !field_is_flat(index)) {\n+    assert(offset == field_offset(index), \"offset mismatch\");\n+    return value;\n+  }\n+\n+  \/\/ Flat inline type field\n+  InlineTypeNode* vt = value->as_InlineType();\n+  if (offset == field_null_marker_offset(index)) {\n+    return vt->get_null_marker();\n+  } else {\n+    int sub_offset = offset - field_offset(index); \/\/ Offset of the flattened field inside the declared field\n+    sub_offset += vt->inline_klass()->payload_offset(); \/\/ Add header size\n+    return vt->field_value_by_offset(sub_offset, recursive);\n+  }\n+}\n+\n+void InlineTypeNode::set_field_value(uint index, Node* value) {\n+  assert(index < field_count(), \"index out of bounds\");\n+  set_req(Values + index, value);\n+}\n+\n+void InlineTypeNode::set_field_value_by_offset(int offset, Node* value) {\n+  set_field_value(field_index(offset), value);\n+}\n+\n+int InlineTypeNode::field_offset(uint index) const {\n+  assert(index < field_count(), \"index out of bounds\");\n+  return inline_klass()->declared_nonstatic_field_at(index)->offset_in_bytes();\n+}\n+\n+uint InlineTypeNode::field_index(int offset) const {\n+  uint i = 0;\n+  for (; i < field_count() && field_offset(i) != offset; i++) { }\n+  assert(i < field_count(), \"field not found\");\n+  return i;\n+}\n+\n+ciType* InlineTypeNode::field_type(uint index) const {\n+  assert(index < field_count(), \"index out of bounds\");\n+  return inline_klass()->declared_nonstatic_field_at(index)->type();\n+}\n+\n+bool InlineTypeNode::field_is_flat(uint index) const {\n+  assert(index < field_count(), \"index out of bounds\");\n+  ciField* field = inline_klass()->declared_nonstatic_field_at(index);\n+  assert(!field->is_flat() || field->type()->is_inlinetype(), \"must be an inline type\");\n+  return field->is_flat();\n+}\n+\n+bool InlineTypeNode::field_is_null_free(uint index) const {\n+  assert(index < field_count(), \"index out of bounds\");\n+  ciField* field = inline_klass()->declared_nonstatic_field_at(index);\n+  assert(!field->is_flat() || field->type()->is_inlinetype(), \"must be an inline type\");\n+  return field->is_null_free();\n+}\n+\n+bool InlineTypeNode::field_is_volatile(uint index) const {\n+  assert(index < field_count(), \"index out of bounds\");\n+  ciField* field = inline_klass()->declared_nonstatic_field_at(index);\n+  assert(!field->is_flat() || field->type()->is_inlinetype(), \"must be an inline type\");\n+  return field->is_volatile();\n+}\n+\n+int InlineTypeNode::field_null_marker_offset(uint index) const {\n+  assert(index < field_count(), \"index out of bounds\");\n+  ciField* field = inline_klass()->declared_nonstatic_field_at(index);\n+  assert(field->is_flat(), \"must be an inline type\");\n+  return field->null_marker_offset();\n+}\n+\n+uint InlineTypeNode::add_fields_to_safepoint(Unique_Node_List& worklist, SafePointNode* sfpt) {\n+  uint cnt = 0;\n+  for (uint i = 0; i < field_count(); ++i) {\n+    Node* value = field_value(i);\n+    if (field_is_flat(i)) {\n+      InlineTypeNode* vt = value->as_InlineType();\n+      cnt += vt->add_fields_to_safepoint(worklist, sfpt);\n+      if (!field_is_null_free(i)) {\n+        \/\/ The null marker of a flat field is added right after we scalarize that field\n+        sfpt->add_req(vt->get_null_marker());\n+        cnt++;\n+      }\n+      continue;\n+    }\n+    if (value->is_InlineType()) {\n+      \/\/ Add inline type to the worklist to process later\n+      worklist.push(value);\n+    }\n+    sfpt->add_req(value);\n+    cnt++;\n+  }\n+  return cnt;\n+}\n+\n+void InlineTypeNode::make_scalar_in_safepoint(PhaseIterGVN* igvn, Unique_Node_List& worklist, SafePointNode* sfpt) {\n+  JVMState* jvms = sfpt->jvms();\n+  assert(jvms != nullptr, \"missing JVMS\");\n+  uint first_ind = (sfpt->req() - jvms->scloff());\n+\n+  \/\/ Iterate over the inline type fields in order of increasing offset and add the\n+  \/\/ field values to the safepoint. Nullable inline types have an null marker field that\n+  \/\/ needs to be checked before using the field values.\n+  sfpt->add_req(get_null_marker());\n+  uint nfields = add_fields_to_safepoint(worklist, sfpt);\n+  jvms->set_endoff(sfpt->req());\n+  \/\/ Replace safepoint edge by SafePointScalarObjectNode\n+  SafePointScalarObjectNode* sobj = new SafePointScalarObjectNode(type()->isa_instptr(),\n+                                                                  nullptr,\n+                                                                  first_ind,\n+                                                                  sfpt->jvms()->depth(),\n+                                                                  nfields);\n+  sobj->init_req(0, igvn->C->root());\n+  sobj = igvn->transform(sobj)->as_SafePointScalarObject();\n+  igvn->rehash_node_delayed(sfpt);\n+  for (uint i = jvms->debug_start(); i < jvms->debug_end(); i++) {\n+    Node* debug = sfpt->in(i);\n+    if (debug != nullptr && debug->uncast() == this) {\n+      sfpt->set_req(i, sobj);\n+    }\n+  }\n+}\n+\n+void InlineTypeNode::make_scalar_in_safepoints(PhaseIterGVN* igvn, bool allow_oop) {\n+  \/\/ If the inline type has a constant or loaded oop, use the oop instead of scalarization\n+  \/\/ in the safepoint to avoid keeping field loads live just for the debug info.\n+  Node* oop = get_oop();\n+  bool use_oop = false;\n+  if (allow_oop && is_allocated(igvn) && oop->is_Phi()) {\n+    Unique_Node_List worklist;\n+    VectorSet visited;\n+    visited.set(oop->_idx);\n+    worklist.push(oop);\n+    use_oop = true;\n+    while (worklist.size() > 0 && use_oop) {\n+      Node* n = worklist.pop();\n+      for (uint i = 1; i < n->req(); i++) {\n+        Node* in = n->in(i);\n+        if (in->is_Phi() && !visited.test_set(in->_idx)) {\n+          worklist.push(in);\n+        } else if (!(in->is_Con() || in->is_Parm())) {\n+          use_oop = false;\n+          break;\n+        }\n+      }\n+    }\n+  } else {\n+    use_oop = allow_oop && is_allocated(igvn) &&\n+              (oop->is_Con() || oop->is_Parm() || oop->is_Load() || (oop->isa_DecodeN() && oop->in(1)->is_Load()));\n+  }\n+\n+  ResourceMark rm;\n+  Unique_Node_List safepoints;\n+  Unique_Node_List vt_worklist;\n+  Unique_Node_List worklist;\n+  worklist.push(this);\n+  while (worklist.size() > 0) {\n+    Node* n = worklist.pop();\n+    for (DUIterator_Fast imax, i = n->fast_outs(imax); i < imax; i++) {\n+      Node* use = n->fast_out(i);\n+      if (use->is_SafePoint() && !use->is_CallLeaf() && (!use->is_Call() || use->as_Call()->has_debug_use(n))) {\n+        safepoints.push(use);\n+      } else if (use->is_ConstraintCast()) {\n+        worklist.push(use);\n+      }\n+    }\n+  }\n+\n+  \/\/ Process all safepoint uses and scalarize inline type\n+  while (safepoints.size() > 0) {\n+    SafePointNode* sfpt = safepoints.pop()->as_SafePoint();\n+    if (use_oop) {\n+      for (uint i = sfpt->jvms()->debug_start(); i < sfpt->jvms()->debug_end(); i++) {\n+        Node* debug = sfpt->in(i);\n+        if (debug != nullptr && debug->uncast() == this) {\n+          sfpt->set_req(i, get_oop());\n+        }\n+      }\n+      igvn->rehash_node_delayed(sfpt);\n+    } else {\n+      make_scalar_in_safepoint(igvn, vt_worklist, sfpt);\n+    }\n+  }\n+  \/\/ Now scalarize non-flat fields\n+  for (uint i = 0; i < vt_worklist.size(); ++i) {\n+    InlineTypeNode* vt = vt_worklist.at(i)->isa_InlineType();\n+    vt->make_scalar_in_safepoints(igvn);\n+  }\n+  if (outcnt() == 0) {\n+    igvn->record_for_igvn(this);\n+  }\n+}\n+\n+\/\/ We limit scalarization for inline types with circular fields and can therefore observe nodes\n+\/\/ of the same type but with different scalarization depth during GVN. This method adjusts the\n+\/\/ scalarization depth to avoid inconsistencies during merging.\n+InlineTypeNode* InlineTypeNode::adjust_scalarization_depth(GraphKit* kit) {\n+  if (!kit->C->has_circular_inline_type()) {\n+    return this;\n+  }\n+  GrowableArray<ciType*> visited;\n+  visited.push(inline_klass());\n+  return adjust_scalarization_depth_impl(kit, visited);\n+}\n+\n+InlineTypeNode* InlineTypeNode::adjust_scalarization_depth_impl(GraphKit* kit, GrowableArray<ciType*>& visited) {\n+  InlineTypeNode* val = this;\n+  for (uint i = 0; i < field_count(); ++i) {\n+    Node* value = field_value(i);\n+    Node* new_value = value;\n+    ciType* ft = field_type(i);\n+    if (value->is_InlineType()) {\n+      if (!field_is_flat(i) && visited.contains(ft)) {\n+        new_value = value->as_InlineType()->buffer(kit)->get_oop();\n+      } else {\n+        int old_len = visited.length();\n+        visited.push(ft);\n+        new_value = value->as_InlineType()->adjust_scalarization_depth_impl(kit, visited);\n+        visited.trunc_to(old_len);\n+      }\n+    } else if (ft->is_inlinetype() && !visited.contains(ft)) {\n+      int old_len = visited.length();\n+      visited.push(ft);\n+      new_value = make_from_oop_impl(kit, value, ft->as_inline_klass(), visited);\n+      visited.trunc_to(old_len);\n+    }\n+    if (value != new_value) {\n+      if (val == this) {\n+        val = clone_if_required(&kit->gvn(), kit->map());\n+      }\n+      val->set_field_value(i, new_value);\n+    }\n+  }\n+  return (val == this) ? this : kit->gvn().transform(val)->as_InlineType();\n+}\n+\n+void InlineTypeNode::load(GraphKit* kit, Node* base, Node* ptr, bool immutable_memory, bool trust_null_free_oop, DecoratorSet decorators, GrowableArray<ciType*>& visited) {\n+  \/\/ Initialize the inline type by loading its field values from\n+  \/\/ memory and adding the values as input edges to the node.\n+  ciInlineKlass* vk = inline_klass();\n+  for (uint i = 0; i < field_count(); ++i) {\n+    int field_off = field_offset(i) - vk->payload_offset();\n+    Node* field_ptr = kit->basic_plus_adr(base, ptr, field_off);\n+    Node* value = nullptr;\n+    ciType* ft = field_type(i);\n+    bool field_null_free = field_is_null_free(i);\n+    if (field_is_flat(i)) {\n+      \/\/ Recursively load the flat inline type field\n+      ciInlineKlass* fvk = ft->as_inline_klass();\n+      \/\/ Atomic if nullable or not LooselyConsistentValue\n+      bool atomic = !field_null_free || fvk->must_be_atomic();\n+\n+      int old_len = visited.length();\n+      visited.push(ft);\n+      value = make_from_flat_impl(kit, fvk, base, field_ptr, atomic, immutable_memory,\n+                                  field_null_free, trust_null_free_oop && field_null_free, decorators, visited);\n+      visited.trunc_to(old_len);\n+    } else {\n+      \/\/ Load field value from memory\n+      BasicType bt = type2field[ft->basic_type()];\n+      assert(is_java_primitive(bt) || field_ptr->bottom_type()->is_ptr_to_narrowoop() == UseCompressedOops, \"inconsistent\");\n+      const Type* val_type = Type::get_const_type(ft);\n+      if (trust_null_free_oop && field_null_free) {\n+        val_type = val_type->join_speculative(TypePtr::NOTNULL);\n+      }\n+      const TypePtr* field_ptr_type = (decorators & C2_MISMATCHED) == 0 ? kit->gvn().type(field_ptr)->is_ptr() : TypeRawPtr::BOTTOM;\n+      value = kit->access_load_at(base, field_ptr, field_ptr_type, val_type, bt, decorators);\n+      \/\/ Loading a non-flattened inline type from memory\n+      if (visited.contains(ft)) {\n+        kit->C->set_has_circular_inline_type(true);\n+      } else if (ft->is_inlinetype()) {\n+        int old_len = visited.length();\n+        visited.push(ft);\n+        value = make_from_oop_impl(kit, value, ft->as_inline_klass(), visited);\n+        visited.trunc_to(old_len);\n+      }\n+    }\n+    set_field_value(i, value);\n+  }\n+}\n+\n+\/\/ Get a field value from the payload by shifting it according to the offset\n+static Node* get_payload_value(PhaseGVN* gvn, Node* payload, BasicType bt, BasicType val_bt, int offset) {\n+  \/\/ Shift to the right position in the long value\n+  assert((offset + type2aelembytes(val_bt)) <= type2aelembytes(bt), \"Value does not fit into payload\");\n+  Node* value = nullptr;\n+  Node* shift_val = gvn->intcon(offset << LogBitsPerByte);\n+  if (bt == T_LONG) {\n+    value = gvn->transform(new URShiftLNode(payload, shift_val));\n+    value = gvn->transform(new ConvL2INode(value));\n+  } else {\n+    value = gvn->transform(new URShiftINode(payload, shift_val));\n+  }\n+\n+  if (val_bt == T_INT || val_bt == T_OBJECT || val_bt == T_ARRAY) {\n+    return value;\n+  } else {\n+    \/\/ Make sure to zero unused bits in the 32-bit value\n+    return Compile::narrow_value(val_bt, value, nullptr, gvn, true);\n+  }\n+}\n+\n+\/\/ Convert a payload value to field values\n+void InlineTypeNode::convert_from_payload(GraphKit* kit, BasicType bt, Node* payload, int holder_offset, bool null_free, bool trust_null_free_oop) {\n+  PhaseGVN* gvn = &kit->gvn();\n+  ciInlineKlass* vk = inline_klass();\n+  Node* value = nullptr;\n+  if (!null_free) {\n+    \/\/ Get the null marker\n+    value = get_payload_value(gvn, payload, bt, T_BOOLEAN, holder_offset + vk->null_marker_offset_in_payload());\n+    set_req(NullMarker, value);\n+  }\n+  \/\/ Iterate over the fields and get their values from the payload\n+  for (uint i = 0; i < field_count(); ++i) {\n+    ciType* ft = field_type(i);\n+    bool field_null_free = field_is_null_free(i);\n+    int offset = holder_offset + field_offset(i) - vk->payload_offset();\n+    if (field_is_flat(i)) {\n+      InlineTypeNode* vt = make_uninitialized(*gvn, ft->as_inline_klass(), field_null_free);\n+      vt->convert_from_payload(kit, bt, payload, offset, field_null_free, trust_null_free_oop && field_null_free);\n+      value = gvn->transform(vt);\n+    } else {\n+      value = get_payload_value(gvn, payload, bt, ft->basic_type(), offset);\n+      if (!ft->is_primitive_type()) {\n+        \/\/ Narrow oop field\n+        assert(UseCompressedOops && bt == T_LONG, \"Naturally atomic\");\n+        const Type* val_type = Type::get_const_type(ft);\n+        if (trust_null_free_oop && field_null_free) {\n+          val_type = val_type->join_speculative(TypePtr::NOTNULL);\n+        }\n+        value = gvn->transform(new CastI2NNode(kit->control(), value, val_type->make_narrowoop()));\n+        value = gvn->transform(new DecodeNNode(value, val_type->make_narrowoop()));\n+\n+        \/\/ Similar to CheckCastPP nodes with raw input, CastI2N nodes require special handling in 'PhaseCFG::schedule_late' to ensure the\n+        \/\/ register allocator does not move the CastI2N below a safepoint. This is necessary to avoid having the raw pointer span a safepoint,\n+        \/\/ making it opaque to the GC. Unlike CheckCastPPs, which need extra handling in 'Scheduling::ComputeRegisterAntidependencies' due to\n+        \/\/ scalarization, CastI2N nodes are always used by a load if scalarization happens which inherently keeps them pinned above the safepoint.\n+\n+        if (ft->is_inlinetype()) {\n+          GrowableArray<ciType*> visited;\n+          value = make_from_oop_impl(kit, value, ft->as_inline_klass(), visited);\n+        }\n+      }\n+    }\n+    set_field_value(i, value);\n+  }\n+}\n+\n+\/\/ Set a field value in the payload by shifting it according to the offset\n+static Node* set_payload_value(PhaseGVN* gvn, Node* payload, BasicType bt, Node* value, BasicType val_bt, int offset) {\n+  assert((offset + type2aelembytes(val_bt)) <= type2aelembytes(bt), \"Value does not fit into payload\");\n+\n+  \/\/ Make sure to zero unused bits in the 32-bit value\n+  if (val_bt == T_BYTE || val_bt == T_BOOLEAN) {\n+    value = gvn->transform(new AndINode(value, gvn->intcon(0xFF)));\n+  } else if (val_bt == T_CHAR || val_bt == T_SHORT) {\n+    value = gvn->transform(new AndINode(value, gvn->intcon(0xFFFF)));\n+  } else if (val_bt == T_FLOAT) {\n+    value = gvn->transform(new MoveF2INode(value));\n+  } else {\n+    assert(val_bt == T_INT, \"Unsupported type: %s\", type2name(val_bt));\n+  }\n+\n+  Node* shift_val = gvn->intcon(offset << LogBitsPerByte);\n+  if (bt == T_LONG) {\n+    \/\/ Convert to long and remove the sign bit (the backend will fold this and emit a zero extend i2l)\n+    value = gvn->transform(new ConvI2LNode(value));\n+    value = gvn->transform(new AndLNode(value, gvn->longcon(0xFFFFFFFF)));\n+\n+    Node* shift_value = gvn->transform(new LShiftLNode(value, shift_val));\n+    payload = new OrLNode(shift_value, payload);\n+  } else {\n+    Node* shift_value = gvn->transform(new LShiftINode(value, shift_val));\n+    payload = new OrINode(shift_value, payload);\n+  }\n+  return gvn->transform(payload);\n+}\n+\n+\/\/ Convert the field values to a payload value of type 'bt'\n+Node* InlineTypeNode::convert_to_payload(GraphKit* kit, BasicType bt, Node* payload, int holder_offset, bool null_free, int null_marker_offset, int& oop_off_1, int& oop_off_2) const {\n+  PhaseGVN* gvn = &kit->gvn();\n+  Node* value = nullptr;\n+  if (!null_free) {\n+    \/\/ Set the null marker\n+    value = get_null_marker();\n+    payload = set_payload_value(gvn, payload, bt, value, T_BOOLEAN, null_marker_offset);\n+  }\n+  \/\/ Iterate over the fields and add their values to the payload\n+  for (uint i = 0; i < field_count(); ++i) {\n+    value = field_value(i);\n+    int inner_offset = field_offset(i) - inline_klass()->payload_offset();\n+    int offset = holder_offset + inner_offset;\n+    if (field_is_flat(i)) {\n+      null_marker_offset = holder_offset + field_null_marker_offset(i) - inline_klass()->payload_offset();\n+      payload = value->as_InlineType()->convert_to_payload(kit, bt, payload, offset, field_is_null_free(i), null_marker_offset, oop_off_1, oop_off_2);\n+    } else {\n+      ciType* ft = field_type(i);\n+      BasicType field_bt = ft->basic_type();\n+      if (!ft->is_primitive_type()) {\n+        \/\/ Narrow oop field\n+        assert(UseCompressedOops && bt == T_LONG, \"Naturally atomic\");\n+        assert(inner_offset != -1, \"sanity\");\n+        if (oop_off_1 == -1) {\n+          oop_off_1 = inner_offset;\n+        } else {\n+          assert(oop_off_2 == -1, \"already set\");\n+          oop_off_2 = inner_offset;\n+        }\n+        const Type* val_type = Type::get_const_type(ft)->make_narrowoop();\n+        if (value->is_InlineType()) {\n+          PreserveReexecuteState preexecs(kit);\n+          kit->jvms()->set_should_reexecute(true);\n+          value = value->as_InlineType()->buffer(kit, false);\n+        }\n+        value = gvn->transform(new EncodePNode(value, val_type));\n+        value = gvn->transform(new CastP2XNode(kit->control(), value));\n+        value = gvn->transform(new ConvL2INode(value));\n+        field_bt = T_INT;\n+      }\n+      payload = set_payload_value(gvn, payload, bt, value, field_bt, offset);\n+    }\n+  }\n+  return payload;\n+}\n+\n+void InlineTypeNode::store_flat(GraphKit* kit, Node* base, Node* ptr, bool atomic, bool immutable_memory, bool null_free, DecoratorSet decorators) const {\n+  ciInlineKlass* vk = inline_klass();\n+  bool do_atomic = atomic;\n+  \/\/ With immutable memory, a non-atomic load and an atomic load are the same\n+  if (immutable_memory) {\n+    do_atomic = false;\n+  }\n+  \/\/ If there is only one flattened field, a non-atomic load and an atomic load are the same\n+  if (vk->is_naturally_atomic(null_free)) {\n+    do_atomic = false;\n+  }\n+\n+  if (!do_atomic) {\n+    if (!null_free) {\n+      int nm_offset = vk->null_marker_offset_in_payload();\n+      Node* nm_ptr = kit->basic_plus_adr(base, ptr, nm_offset);\n+      const TypePtr* nm_ptr_type = (decorators & C2_MISMATCHED) == 0 ? kit->gvn().type(nm_ptr)->is_ptr() : TypeRawPtr::BOTTOM;\n+      kit->access_store_at(base, nm_ptr, nm_ptr_type, get_null_marker(), TypeInt::BOOL, T_BOOLEAN, decorators);\n+    }\n+    store(kit, base, ptr, immutable_memory, decorators);\n+    return;\n+  }\n+\n+  \/\/ Convert to a payload value <= 64-bit and write atomically.\n+  \/\/ The payload might contain at most two oop fields that must be narrow because otherwise they would be 64-bit\n+  \/\/ in size and would then be written by a \"normal\" oop store. If the payload contains oops, its size is always\n+  \/\/ 64-bit because the next smaller (power-of-two) size would be 32-bit which could only hold one narrow oop that\n+  \/\/ would then be written by a normal narrow oop store. These properties are asserted in 'convert_to_payload'.\n+  assert(!immutable_memory, \"immutable memory does not need explicit atomic access\");\n+  BasicType store_bt = vk->atomic_size_to_basic_type(null_free);\n+  Node* payload = (store_bt == T_LONG) ? kit->longcon(0) : kit->intcon(0);\n+  int oop_off_1 = -1;\n+  int oop_off_2 = -1;\n+  payload = convert_to_payload(kit, store_bt, payload, 0, null_free, vk->null_marker_offset_in_payload(), oop_off_1, oop_off_2);\n+  if (!UseG1GC || oop_off_1 == -1) {\n+    \/\/ No oop fields or no late barrier expansion. Emit an atomic store of the payload and add GC barriers if needed.\n+    assert(oop_off_2 == -1 || !UseG1GC, \"sanity\");\n+    \/\/ ZGC does not support compressed oops, so only one oop can be in the payload which is written by a \"normal\" oop store.\n+    assert((oop_off_1 == -1 && oop_off_2 == -1) || !UseZGC, \"ZGC does not support embedded oops in flat fields\");\n+    const Type* val_type = Type::get_const_basic_type(store_bt);\n+    kit->insert_mem_bar(Op_MemBarCPUOrder);\n+    kit->access_store_at(base, ptr, TypeRawPtr::BOTTOM, payload, val_type, store_bt, decorators | C2_MISMATCHED, true, this);\n+    kit->insert_mem_bar(Op_MemBarCPUOrder);\n+  } else {\n+    \/\/ Contains oops and requires late barrier expansion. Emit a special store node that allows to emit GC barriers in the backend.\n+    assert(UseG1GC, \"Unexpected GC\");\n+    assert(store_bt == T_LONG, \"Unexpected payload type\");\n+    \/\/ If one oop, set the offset (if no offset is set, two oops are assumed by the backend)\n+    Node* oop_offset = (oop_off_2 == -1) ? kit->intcon(oop_off_1) : nullptr;\n+    kit->insert_mem_bar(Op_MemBarCPUOrder);\n+    Node* mem = kit->reset_memory();\n+    kit->set_all_memory(mem);\n+    Node* st = kit->gvn().transform(new StoreLSpecialNode(kit->control(), mem, ptr, TypeRawPtr::BOTTOM, payload, oop_offset, MemNode::unordered));\n+    kit->set_memory(st, TypeRawPtr::BOTTOM);\n+    kit->insert_mem_bar(Op_MemBarCPUOrder);\n+  }\n+}\n+\n+void InlineTypeNode::store_flat_array(GraphKit* kit, Node* base, Node* idx) const {\n+  PhaseGVN& gvn = kit->gvn();\n+  DecoratorSet decorators = IN_HEAP | IS_ARRAY | MO_UNORDERED;\n+  kit->C->set_flat_accesses();\n+  ciInlineKlass* vk = inline_klass();\n+  assert(vk->maybe_flat_in_array(), \"element type %s cannot be flat in array\", vk->name()->as_utf8());\n+\n+  RegionNode* region = new RegionNode(4);\n+  gvn.set_type(region, Type::CONTROL);\n+  kit->record_for_igvn(region);\n+\n+  Node* input_memory_state = kit->reset_memory();\n+  kit->set_all_memory(input_memory_state);\n+\n+  PhiNode* mem = PhiNode::make(region, input_memory_state, Type::MEMORY, TypePtr::BOTTOM);\n+  gvn.set_type(mem, Type::MEMORY);\n+  kit->record_for_igvn(mem);\n+\n+  PhiNode* io = PhiNode::make(region, kit->i_o(), Type::ABIO);\n+  gvn.set_type(io, Type::ABIO);\n+  kit->record_for_igvn(io);\n+\n+  Node* bol_null_free = kit->null_free_array_test(base); \/\/ Argument evaluation order is undefined in C++ and since this sets control, it needs to come first\n+  IfNode* iff_null_free = kit->create_and_map_if(kit->control(), bol_null_free, PROB_FAIR, COUNT_UNKNOWN);\n+\n+  \/\/ Nullable\n+  kit->set_control(kit->IfFalse(iff_null_free));\n+  if (!kit->stopped()) {\n+    assert(vk->has_nullable_atomic_layout(), \"element type %s does not have a nullable flat layout\", vk->name()->as_utf8());\n+    kit->set_all_memory(input_memory_state);\n+    Node* cast = kit->cast_to_flat_array(base, vk, false, true, true);\n+    Node* ptr = kit->array_element_address(cast, idx, T_FLAT_ELEMENT);\n+    store_flat(kit, cast, ptr, true, false, false, decorators);\n+\n+    region->init_req(1, kit->control());\n+    mem->set_req(1, kit->reset_memory());\n+    io->set_req(1, kit->i_o());\n+  }\n+\n+  \/\/ Null-free\n+  kit->set_control(kit->IfTrue(iff_null_free));\n+  if (!kit->stopped()) {\n+    kit->set_all_memory(input_memory_state);\n+\n+    Node* bol_atomic = kit->null_free_atomic_array_test(base, vk);\n+    IfNode* iff_atomic = kit->create_and_map_if(kit->control(), bol_atomic, PROB_FAIR, COUNT_UNKNOWN);\n+\n+    \/\/ Atomic\n+    kit->set_control(kit->IfTrue(iff_atomic));\n+    if (!kit->stopped()) {\n+      assert(vk->has_atomic_layout(), \"element type %s does not have a null-free atomic flat layout\", vk->name()->as_utf8());\n+      kit->set_all_memory(input_memory_state);\n+      Node* cast = kit->cast_to_flat_array(base, vk, true, false, true);\n+      Node* ptr = kit->array_element_address(cast, idx, T_FLAT_ELEMENT);\n+      store_flat(kit, cast, ptr, true, false, true, decorators);\n+\n+      region->init_req(2, kit->control());\n+      mem->set_req(2, kit->reset_memory());\n+      io->set_req(2, kit->i_o());\n+    }\n+\n+    \/\/ Non-atomic\n+    kit->set_control(kit->IfFalse(iff_atomic));\n+    if (!kit->stopped()) {\n+      assert(vk->has_non_atomic_layout(), \"element type %s does not have a null-free non-atomic flat layout\", vk->name()->as_utf8());\n+      kit->set_all_memory(input_memory_state);\n+      Node* cast = kit->cast_to_flat_array(base, vk, true, false, false);\n+      Node* ptr = kit->array_element_address(cast, idx, T_FLAT_ELEMENT);\n+      store_flat(kit, cast, ptr, false, false, true, decorators);\n+\n+      region->init_req(3, kit->control());\n+      mem->set_req(3, kit->reset_memory());\n+      io->set_req(3, kit->i_o());\n+    }\n+  }\n+\n+  kit->set_control(gvn.transform(region));\n+  kit->set_all_memory(gvn.transform(mem));\n+  kit->set_i_o(gvn.transform(io));\n+}\n+\n+void InlineTypeNode::store(GraphKit* kit, Node* base, Node* ptr, bool immutable_memory, DecoratorSet decorators) const {\n+  \/\/ Write field values to memory\n+  ciInlineKlass* vk = inline_klass();\n+  for (uint i = 0; i < field_count(); ++i) {\n+    int field_off = field_offset(i) - vk->payload_offset();\n+    Node* field_val = field_value(i);\n+    bool field_null_free = field_is_null_free(i);\n+    ciType* ft = field_type(i);\n+    Node* field_ptr = kit->basic_plus_adr(base, ptr, field_off);\n+    if (field_is_flat(i)) {\n+      \/\/ Recursively store the flat inline type field\n+      ciInlineKlass* fvk = ft->as_inline_klass();\n+      \/\/ Atomic if nullable or not LooselyConsistentValue\n+      bool atomic = !field_null_free || fvk->must_be_atomic();\n+\n+      field_val->as_InlineType()->store_flat(kit, base, field_ptr, atomic, immutable_memory, field_null_free, decorators);\n+    } else {\n+      \/\/ Store field value to memory\n+      BasicType bt = type2field[ft->basic_type()];\n+      const TypePtr* field_ptr_type = (decorators & C2_MISMATCHED) == 0 ? kit->gvn().type(field_ptr)->is_ptr() : TypeRawPtr::BOTTOM;\n+      const Type* val_type = Type::get_const_type(ft);\n+      kit->access_store_at(base, field_ptr, field_ptr_type, field_val, val_type, bt, decorators);\n+    }\n+  }\n+}\n+\n+InlineTypeNode* InlineTypeNode::buffer(GraphKit* kit, bool safe_for_replace) {\n+  if (kit->gvn().find_int_con(get_is_buffered(), 0) == 1) {\n+    \/\/ Already buffered\n+    return this;\n+  }\n+\n+  \/\/ Check if inline type is already buffered\n+  Node* not_buffered_ctl = kit->top();\n+  Node* not_null_oop = kit->null_check_oop(get_oop(), &not_buffered_ctl, \/* never_see_null = *\/ false, safe_for_replace);\n+  if (not_buffered_ctl->is_top()) {\n+    \/\/ Already buffered\n+    InlineTypeNode* vt = clone_if_required(&kit->gvn(), kit->map(), safe_for_replace);\n+    vt->set_is_buffered(kit->gvn());\n+    vt = kit->gvn().transform(vt)->as_InlineType();\n+    if (safe_for_replace) {\n+      kit->replace_in_map(this, vt);\n+    }\n+    return vt;\n+  }\n+  Node* buffered_ctl = kit->control();\n+  kit->set_control(not_buffered_ctl);\n+\n+  \/\/ Inline type is not buffered, check if it is null.\n+  Node* null_ctl = kit->top();\n+  kit->null_check_common(get_null_marker(), T_INT, false, &null_ctl);\n+  bool null_free = null_ctl->is_top();\n+\n+  RegionNode* region = new RegionNode(4);\n+  PhiNode* oop = PhiNode::make(region, not_null_oop, type()->join_speculative(null_free ? TypePtr::NOTNULL : TypePtr::BOTTOM));\n+\n+  \/\/ InlineType is already buffered\n+  region->init_req(1, buffered_ctl);\n+  oop->init_req(1, not_null_oop);\n+\n+  \/\/ InlineType is null\n+  region->init_req(2, null_ctl);\n+  oop->init_req(2, kit->gvn().zerocon(T_OBJECT));\n+\n+  PhiNode* io  = PhiNode::make(region, kit->i_o(), Type::ABIO);\n+  PhiNode* mem = PhiNode::make(region, kit->merged_memory(), Type::MEMORY, TypePtr::BOTTOM);\n+\n+  if (!kit->stopped()) {\n+    assert(!is_allocated(&kit->gvn()), \"already buffered\");\n+    PreserveJVMState pjvms(kit);\n+    ciInlineKlass* vk = inline_klass();\n+    \/\/ Allocate and initialize buffer, re-execute on deoptimization.\n+    kit->jvms()->set_bci(kit->bci());\n+    kit->jvms()->set_should_reexecute(true);\n+    kit->kill_dead_locals();\n+    Node* klass_node = kit->makecon(TypeKlassPtr::make(vk));\n+    Node* alloc_oop  = kit->new_instance(klass_node, nullptr, nullptr, \/* deoptimize_on_exception *\/ true, this);\n+    Node* payload_alloc_oop = kit->basic_plus_adr(alloc_oop, vk->payload_offset());\n+    store(kit, alloc_oop, payload_alloc_oop, true, IN_HEAP | MO_UNORDERED | C2_TIGHTLY_COUPLED_ALLOC);\n+\n+    \/\/ Do not let stores that initialize this buffer be reordered with a subsequent\n+    \/\/ store that would make this buffer accessible by other threads.\n+    AllocateNode* alloc = AllocateNode::Ideal_allocation(alloc_oop);\n+    assert(alloc != nullptr, \"must have an allocation node\");\n+    kit->insert_mem_bar(Op_MemBarStoreStore, alloc->proj_out_or_null(AllocateNode::RawAddress));\n+    oop->init_req(3, alloc_oop);\n+    region->init_req(3, kit->control());\n+    io    ->init_req(3, kit->i_o());\n+    mem   ->init_req(3, kit->merged_memory());\n+  }\n+\n+  \/\/ Update GraphKit\n+  kit->set_control(kit->gvn().transform(region));\n+  kit->set_i_o(kit->gvn().transform(io));\n+  kit->set_all_memory(kit->gvn().transform(mem));\n+  kit->record_for_igvn(region);\n+  kit->record_for_igvn(oop);\n+  kit->record_for_igvn(io);\n+  kit->record_for_igvn(mem);\n+\n+  \/\/ Use cloned InlineTypeNode to propagate oop from now on\n+  Node* res_oop = kit->gvn().transform(oop);\n+  InlineTypeNode* vt = clone_if_required(&kit->gvn(), kit->map(), safe_for_replace);\n+  vt->set_oop(kit->gvn(), res_oop);\n+  vt->set_is_buffered(kit->gvn());\n+  vt = kit->gvn().transform(vt)->as_InlineType();\n+  if (safe_for_replace) {\n+    kit->replace_in_map(this, vt);\n+  }\n+  \/\/ InlineTypeNode::remove_redundant_allocations piggybacks on split if.\n+  \/\/ Make sure it gets a chance to remove this allocation.\n+  kit->C->set_has_split_ifs(true);\n+  return vt;\n+}\n+\n+bool InlineTypeNode::is_allocated(PhaseGVN* phase) const {\n+  if (phase->find_int_con(get_is_buffered(), 0) == 1) {\n+    return true;\n+  }\n+  Node* oop = get_oop();\n+  const Type* oop_type = (phase != nullptr) ? phase->type(oop) : oop->bottom_type();\n+  return !oop_type->maybe_null();\n+}\n+\n+static void replace_proj(Compile* C, CallNode* call, uint& proj_idx, Node* value, BasicType bt) {\n+  ProjNode* pn = call->proj_out_or_null(proj_idx);\n+  if (pn != nullptr) {\n+    C->gvn_replace_by(pn, value);\n+    C->initial_gvn()->hash_delete(pn);\n+    pn->set_req(0, C->top());\n+  }\n+  proj_idx += type2size[bt];\n+}\n+\n+\/\/ When a call returns multiple values, it has several result\n+\/\/ projections, one per field. Replacing the result of the call by an\n+\/\/ inline type node (after late inlining) requires that for each result\n+\/\/ projection, we find the corresponding inline type field.\n+void InlineTypeNode::replace_call_results(GraphKit* kit, CallNode* call, Compile* C) {\n+  uint proj_idx = TypeFunc::Parms;\n+  \/\/ Replace oop projection\n+  replace_proj(C, call, proj_idx, get_oop(), T_OBJECT);\n+  \/\/ Replace field projections\n+  replace_field_projs(C, call, proj_idx);\n+  \/\/ Replace null_marker projection\n+  replace_proj(C, call, proj_idx, get_null_marker(), T_BOOLEAN);\n+  assert(proj_idx == call->tf()->range_cc()->cnt(), \"missed a projection\");\n+}\n+\n+void InlineTypeNode::replace_field_projs(Compile* C, CallNode* call, uint& proj_idx) {\n+  for (uint i = 0; i < field_count(); ++i) {\n+    Node* value = field_value(i);\n+    if (field_is_flat(i)) {\n+      InlineTypeNode* vt = value->as_InlineType();\n+      \/\/ Replace field projections for flat field\n+      vt->replace_field_projs(C, call, proj_idx);\n+      if (!field_is_null_free(i)) {\n+        \/\/ Replace null_marker projection for nullable field\n+        replace_proj(C, call, proj_idx, vt->get_null_marker(), T_BOOLEAN);\n+      }\n+      continue;\n+    }\n+    \/\/ Replace projection for field value\n+    replace_proj(C, call, proj_idx, value, field_type(i)->basic_type());\n+  }\n+}\n+\n+Node* InlineTypeNode::allocate_fields(GraphKit* kit) {\n+  InlineTypeNode* vt = clone_if_required(&kit->gvn(), kit->map());\n+  for (uint i = 0; i < field_count(); i++) {\n+     Node* value = field_value(i);\n+     if (field_is_flat(i)) {\n+       \/\/ Flat inline type field\n+       vt->set_field_value(i, value->as_InlineType()->allocate_fields(kit));\n+     } else if (value->is_InlineType()) {\n+       \/\/ Non-flat inline type field\n+       vt->set_field_value(i, value->as_InlineType()->buffer(kit));\n+     }\n+  }\n+  vt = kit->gvn().transform(vt)->as_InlineType();\n+  kit->replace_in_map(this, vt);\n+  return vt;\n+}\n+\n+\/\/ Replace a buffer allocation by a dominating allocation\n+static void replace_allocation(PhaseIterGVN* igvn, Node* res, Node* dom) {\n+  \/\/ Remove initializing stores and GC barriers\n+  for (DUIterator_Fast imax, i = res->fast_outs(imax); i < imax; i++) {\n+    Node* use = res->fast_out(i);\n+    if (use->is_AddP()) {\n+      for (DUIterator_Fast jmax, j = use->fast_outs(jmax); j < jmax; j++) {\n+        Node* store = use->fast_out(j)->isa_Store();\n+        if (store != nullptr) {\n+          igvn->rehash_node_delayed(store);\n+          igvn->replace_in_uses(store, store->in(MemNode::Memory));\n+        }\n+      }\n+    } else if (use->Opcode() == Op_CastP2X) {\n+      if (UseG1GC && use->find_out_with(Op_XorX)->in(1) != use) {\n+        \/\/ The G1 pre-barrier uses a CastP2X both for the pointer of the object\n+        \/\/ we store into, as well as the value we are storing. Skip if this is a\n+        \/\/ barrier for storing 'res' into another object.\n+        continue;\n+      }\n+      BarrierSetC2* bs = BarrierSet::barrier_set()->barrier_set_c2();\n+      bs->eliminate_gc_barrier(igvn, use);\n+      --i; --imax;\n+    }\n+  }\n+  igvn->replace_node(res, dom);\n+}\n+\n+Node* InlineTypeNode::Ideal(PhaseGVN* phase, bool can_reshape) {\n+  Node* oop = get_oop();\n+  Node* is_buffered = get_is_buffered();\n+\n+  if (oop->isa_InlineType() && !phase->type(oop)->maybe_null()) {\n+    InlineTypeNode* vtptr = oop->as_InlineType();\n+    set_oop(*phase, vtptr->get_oop());\n+    set_is_buffered(*phase);\n+    set_null_marker(*phase);\n+    for (uint i = Values; i < vtptr->req(); ++i) {\n+      set_req(i, vtptr->in(i));\n+    }\n+    return this;\n+  }\n+\n+  \/\/ Use base oop if fields are loaded from memory, don't do so if base is the CheckCastPP of an\n+  \/\/ allocation because the only case we load from a naked CheckCastPP is when we exit a\n+  \/\/ constructor of an inline type and we want to relinquish the larval oop there. This has a\n+  \/\/ couple of benefits:\n+  \/\/ - The allocation is likely to be elided earlier if it is not an input of an InlineTypeNode.\n+  \/\/ - The InlineTypeNode without an allocation input is more likely to be GVN-ed. This may emerge\n+  \/\/   when we try to clone a value object.\n+  \/\/ - The buffering, if needed, is delayed until it is required. This new allocation, since it is\n+  \/\/   created from an InlineTypeNode, is recognized as not having a unique identity and in the\n+  \/\/   future, we can move them around more freely such as hoisting out of loops. This is not true\n+  \/\/   for the old allocation since larval value objects do have unique identities.\n+  Node* base = is_loaded(phase);\n+  if (base != nullptr && !base->is_InlineType() && !phase->type(base)->maybe_null() && AllocateNode::Ideal_allocation(base) == nullptr) {\n+    if (oop != base || phase->type(is_buffered) != TypeInt::ONE) {\n+      set_oop(*phase, base);\n+      set_is_buffered(*phase);\n+      return this;\n+    }\n+  }\n+\n+  if (can_reshape) {\n+    PhaseIterGVN* igvn = phase->is_IterGVN();\n+    if (is_allocated(phase)) {\n+      \/\/ Search for and remove re-allocations of this inline type. Ignore scalar replaceable ones,\n+      \/\/ they will be removed anyway and changing the memory chain will confuse other optimizations.\n+      \/\/ This can happen with late inlining when we first allocate an inline type argument\n+      \/\/ but later decide to inline the call after the callee code also triggered allocation.\n+      for (DUIterator_Fast imax, i = fast_outs(imax); i < imax; i++) {\n+        AllocateNode* alloc = fast_out(i)->isa_Allocate();\n+        if (alloc != nullptr && alloc->in(AllocateNode::InlineType) == this && !alloc->_is_scalar_replaceable) {\n+          \/\/ Found a re-allocation\n+          Node* res = alloc->result_cast();\n+          if (res != nullptr && res->is_CheckCastPP()) {\n+            \/\/ Replace allocation by oop and unlink AllocateNode\n+            replace_allocation(igvn, res, oop);\n+            igvn->replace_input_of(alloc, AllocateNode::InlineType, igvn->C->top());\n+            --i; --imax;\n+          }\n+        }\n+      }\n+    }\n+  }\n+\n+  return nullptr;\n+}\n+\n+InlineTypeNode* InlineTypeNode::make_uninitialized(PhaseGVN& gvn, ciInlineKlass* vk, bool null_free) {\n+  \/\/ Create a new InlineTypeNode with uninitialized values and nullptr oop\n+  InlineTypeNode* vt = new InlineTypeNode(vk, gvn.zerocon(T_OBJECT), null_free);\n+  vt->set_is_buffered(gvn, false);\n+  vt->set_null_marker(gvn);\n+  return vt;\n+}\n+\n+InlineTypeNode* InlineTypeNode::make_all_zero(PhaseGVN& gvn, ciInlineKlass* vk) {\n+  GrowableArray<ciType*> visited;\n+  visited.push(vk);\n+  return make_all_zero_impl(gvn, vk, visited);\n+}\n+\n+InlineTypeNode* InlineTypeNode::make_all_zero_impl(PhaseGVN& gvn, ciInlineKlass* vk, GrowableArray<ciType*>& visited) {\n+  \/\/ Create a new InlineTypeNode initialized with all zero\n+  InlineTypeNode* vt = new InlineTypeNode(vk, gvn.zerocon(T_OBJECT), \/* null_free= *\/ true);\n+  vt->set_is_buffered(gvn, false);\n+  vt->set_null_marker(gvn);\n+  for (uint i = 0; i < vt->field_count(); ++i) {\n+    ciType* ft = vt->field_type(i);\n+    Node* value = gvn.zerocon(ft->basic_type());\n+    if (!vt->field_is_flat(i) && visited.contains(ft)) {\n+      gvn.C->set_has_circular_inline_type(true);\n+    } else if (ft->is_inlinetype()) {\n+      int old_len = visited.length();\n+      visited.push(ft);\n+      ciInlineKlass* vk = ft->as_inline_klass();\n+      if (vt->field_is_null_free(i)) {\n+        value = make_all_zero_impl(gvn, vk, visited);\n+      } else {\n+        value = make_null_impl(gvn, vk, visited);\n+      }\n+      visited.trunc_to(old_len);\n+    }\n+    vt->set_field_value(i, value);\n+  }\n+  vt = gvn.transform(vt)->as_InlineType();\n+  assert(vt->is_all_zero(&gvn), \"must be the all-zero inline type\");\n+  return vt;\n+}\n+\n+bool InlineTypeNode::is_all_zero(PhaseGVN* gvn, bool flat) const {\n+  const TypeInt* tinit = gvn->type(get_null_marker())->isa_int();\n+  if (tinit == nullptr || !tinit->is_con(1)) {\n+    return false; \/\/ May be null\n+  }\n+  for (uint i = 0; i < field_count(); ++i) {\n+    Node* value = field_value(i);\n+    if (field_is_null_free(i)) {\n+      \/\/ Null-free value class field must have the all-zero value. If 'flat' is set,\n+      \/\/ reject non-flat fields because they need to be initialized with an oop to a buffer.\n+      if (!value->is_InlineType() || !value->as_InlineType()->is_all_zero(gvn) || (flat && !field_is_flat(i))) {\n+        return false;\n+      }\n+      continue;\n+    } else if (value->is_InlineType()) {\n+      \/\/ Nullable value class field must be null\n+      tinit = gvn->type(value->as_InlineType()->get_null_marker())->isa_int();\n+      if (tinit != nullptr && tinit->is_con(0)) {\n+        continue;\n+      }\n+      return false;\n+    } else if (!gvn->type(value)->is_zero_type()) {\n+      return false;\n+    }\n+  }\n+  return true;\n+}\n+\n+InlineTypeNode* InlineTypeNode::make_from_oop(GraphKit* kit, Node* oop, ciInlineKlass* vk) {\n+  GrowableArray<ciType*> visited;\n+  visited.push(vk);\n+  return make_from_oop_impl(kit, oop, vk, visited);\n+}\n+\n+InlineTypeNode* InlineTypeNode::make_from_oop_impl(GraphKit* kit, Node* oop, ciInlineKlass* vk, GrowableArray<ciType*>& visited) {\n+  PhaseGVN& gvn = kit->gvn();\n+\n+  \/\/ Create and initialize an InlineTypeNode by loading all field\n+  \/\/ values from a heap-allocated version and also save the oop.\n+  InlineTypeNode* vt = nullptr;\n+\n+  if (oop->isa_InlineType()) {\n+    return oop->as_InlineType();\n+  }\n+\n+  if (gvn.type(oop)->maybe_null()) {\n+    \/\/ Add a null check because the oop may be null\n+    Node* null_ctl = kit->top();\n+    Node* not_null_oop = kit->null_check_oop(oop, &null_ctl);\n+    if (kit->stopped()) {\n+      \/\/ Constant null\n+      kit->set_control(null_ctl);\n+      vt = make_null_impl(gvn, vk, visited);\n+      kit->record_for_igvn(vt);\n+      return vt;\n+    }\n+    vt = new InlineTypeNode(vk, not_null_oop, \/* null_free= *\/ false);\n+    vt->set_is_buffered(gvn);\n+    vt->set_null_marker(gvn);\n+    Node* payload_ptr = kit->basic_plus_adr(not_null_oop, vk->payload_offset());\n+    vt->load(kit, not_null_oop, payload_ptr, true, true, IN_HEAP | MO_UNORDERED, visited);\n+\n+    if (null_ctl != kit->top()) {\n+      InlineTypeNode* null_vt = make_null_impl(gvn, vk, visited);\n+      Node* region = new RegionNode(3);\n+      region->init_req(1, kit->control());\n+      region->init_req(2, null_ctl);\n+      vt = vt->clone_with_phis(&gvn, region, kit->map());\n+      vt->merge_with(&gvn, null_vt, 2, true);\n+      vt->set_oop(gvn, oop);\n+      kit->set_control(gvn.transform(region));\n+    }\n+  } else {\n+    \/\/ Oop can never be null\n+    vt = new InlineTypeNode(vk, oop, \/* null_free= *\/ true);\n+    Node* init_ctl = kit->control();\n+    vt->set_is_buffered(gvn);\n+    vt->set_null_marker(gvn);\n+    Node* payload_ptr = kit->basic_plus_adr(oop, vk->payload_offset());\n+    vt->load(kit, oop, payload_ptr, true, true, IN_HEAP | MO_UNORDERED, visited);\n+\/\/ TODO 8284443\n+\/\/    assert(!null_free || vt->as_InlineType()->is_all_zero(&gvn) || init_ctl != kit->control() || !gvn.type(oop)->is_inlinetypeptr() || oop->is_Con() || oop->Opcode() == Op_InlineType ||\n+\/\/           AllocateNode::Ideal_allocation(oop, &gvn) != nullptr || vt->as_InlineType()->is_loaded(&gvn) == oop, \"inline type should be loaded\");\n+  }\n+  assert(vt->is_allocated(&gvn), \"inline type should be allocated\");\n+  kit->record_for_igvn(vt);\n+  return gvn.transform(vt)->as_InlineType();\n+}\n+\n+InlineTypeNode* InlineTypeNode::make_from_flat(GraphKit* kit, ciInlineKlass* vk, Node* base, Node* ptr,\n+                                               bool atomic, bool immutable_memory, bool null_free, DecoratorSet decorators) {\n+  GrowableArray<ciType*> visited;\n+  visited.push(vk);\n+  return make_from_flat_impl(kit, vk, base, ptr, atomic, immutable_memory, null_free, null_free, decorators, visited);\n+}\n+\n+\/\/ GraphKit wrapper for the 'make_from_flat' method\n+InlineTypeNode* InlineTypeNode::make_from_flat_impl(GraphKit* kit, ciInlineKlass* vk, Node* base, Node* ptr, bool atomic, bool immutable_memory,\n+                                                    bool null_free, bool trust_null_free_oop, DecoratorSet decorators, GrowableArray<ciType*>& visited) {\n+  assert(null_free || !trust_null_free_oop, \"cannot trust null-free oop when the holder object is not null-free\");\n+  PhaseGVN& gvn = kit->gvn();\n+  bool do_atomic = atomic;\n+  \/\/ With immutable memory, a non-atomic load and an atomic load are the same\n+  if (immutable_memory) {\n+    do_atomic = false;\n+  }\n+  \/\/ If there is only one flattened field, a non-atomic load and an atomic load are the same\n+  if (vk->is_naturally_atomic(null_free)) {\n+    do_atomic = false;\n+  }\n+\n+  if (!do_atomic) {\n+    InlineTypeNode* vt = make_uninitialized(kit->gvn(), vk, null_free);\n+    if (!null_free) {\n+      int nm_offset = vk->null_marker_offset_in_payload();\n+      Node* nm_ptr = kit->basic_plus_adr(base, ptr, nm_offset);\n+      const TypePtr* nm_ptr_type = (decorators & C2_MISMATCHED) == 0 ? gvn.type(nm_ptr)->is_ptr() : TypeRawPtr::BOTTOM;\n+      Node* nm_value = kit->access_load_at(base, nm_ptr, nm_ptr_type, TypeInt::BOOL, T_BOOLEAN, decorators);\n+      vt->set_req(NullMarker, nm_value);\n+    }\n+\n+    vt->load(kit, base, ptr, immutable_memory, trust_null_free_oop, decorators, visited);\n+    return gvn.transform(vt)->as_InlineType();\n+  }\n+\n+  assert(!immutable_memory, \"immutable memory does not need explicit atomic access\");\n+  InlineTypeNode* vt = make_uninitialized(kit->gvn(), vk, null_free);\n+  BasicType load_bt = vk->atomic_size_to_basic_type(null_free);\n+  decorators |= C2_MISMATCHED | C2_CONTROL_DEPENDENT_LOAD;\n+  const Type* val_type = Type::get_const_basic_type(load_bt);\n+  kit->insert_mem_bar(Op_MemBarCPUOrder);\n+  Node* payload = kit->access_load_at(base, ptr, TypeRawPtr::BOTTOM, val_type, load_bt, decorators, kit->control());\n+  kit->insert_mem_bar(Op_MemBarCPUOrder);\n+  vt->convert_from_payload(kit, load_bt, kit->gvn().transform(payload), 0, null_free, trust_null_free_oop);\n+  return gvn.transform(vt)->as_InlineType();\n+}\n+\n+InlineTypeNode* InlineTypeNode::make_from_flat_array(GraphKit* kit, ciInlineKlass* vk, Node* base, Node* idx) {\n+  assert(vk->maybe_flat_in_array(), \"element type %s cannot be flat in array\", vk->name()->as_utf8());\n+  PhaseGVN& gvn = kit->gvn();\n+  DecoratorSet decorators = IN_HEAP | IS_ARRAY | MO_UNORDERED | C2_CONTROL_DEPENDENT_LOAD;\n+  kit->C->set_flat_accesses();\n+  InlineTypeNode* vt_nullable = nullptr;\n+  InlineTypeNode* vt_null_free = nullptr;\n+  InlineTypeNode* vt_non_atomic = nullptr;\n+\n+  RegionNode* region = new RegionNode(4);\n+  gvn.set_type(region, Type::CONTROL);\n+  kit->record_for_igvn(region);\n+\n+  Node* input_memory_state = kit->reset_memory();\n+  kit->set_all_memory(input_memory_state);\n+\n+  PhiNode* mem = PhiNode::make(region, input_memory_state, Type::MEMORY, TypePtr::BOTTOM);\n+  gvn.set_type(mem, Type::MEMORY);\n+  kit->record_for_igvn(mem);\n+\n+  PhiNode* io = PhiNode::make(region, kit->i_o(), Type::ABIO);\n+  gvn.set_type(io, Type::ABIO);\n+  kit->record_for_igvn(io);\n+\n+  Node* bol_null_free = kit->null_free_array_test(base); \/\/ Argument evaluation order is undefined in C++ and since this sets control, it needs to come first\n+  IfNode* iff_null_free = kit->create_and_map_if(kit->control(), bol_null_free, PROB_FAIR, COUNT_UNKNOWN);\n+\n+  \/\/ Nullable\n+  kit->set_control(kit->IfFalse(iff_null_free));\n+  if (!kit->stopped()) {\n+    assert(vk->has_nullable_atomic_layout(), \"element type %s does not have a nullable flat layout\", vk->name()->as_utf8());\n+    kit->set_all_memory(input_memory_state);\n+    Node* cast = kit->cast_to_flat_array(base, vk, false, true, true);\n+    Node* ptr = kit->array_element_address(cast, idx, T_FLAT_ELEMENT);\n+    vt_nullable = InlineTypeNode::make_from_flat(kit, vk, cast, ptr, true, false, false, decorators);\n+\n+    region->init_req(1, kit->control());\n+    mem->set_req(1, kit->reset_memory());\n+    io->set_req(1, kit->i_o());\n+  }\n+\n+  \/\/ Null-free\n+  kit->set_control(kit->IfTrue(iff_null_free));\n+  if (!kit->stopped()) {\n+    kit->set_all_memory(input_memory_state);\n+\n+    Node* bol_atomic = kit->null_free_atomic_array_test(base, vk);\n+    IfNode* iff_atomic = kit->create_and_map_if(kit->control(), bol_atomic, PROB_FAIR, COUNT_UNKNOWN);\n+\n+    \/\/ Atomic\n+    kit->set_control(kit->IfTrue(iff_atomic));\n+    if (!kit->stopped()) {\n+      assert(vk->has_atomic_layout(), \"element type %s does not have a null-free atomic flat layout\", vk->name()->as_utf8());\n+      kit->set_all_memory(input_memory_state);\n+      Node* cast = kit->cast_to_flat_array(base, vk, true, false, true);\n+      Node* ptr = kit->array_element_address(cast, idx, T_FLAT_ELEMENT);\n+      vt_null_free = InlineTypeNode::make_from_flat(kit, vk, cast, ptr, true, false, true, decorators);\n+\n+      region->init_req(2, kit->control());\n+      mem->set_req(2, kit->reset_memory());\n+      io->set_req(2, kit->i_o());\n+    }\n+\n+    \/\/ Non-Atomic\n+    kit->set_control(kit->IfFalse(iff_atomic));\n+    if (!kit->stopped()) {\n+      assert(vk->has_non_atomic_layout(), \"element type %s does not have a null-free non-atomic flat layout\", vk->name()->as_utf8());\n+      kit->set_all_memory(input_memory_state);\n+      Node* cast = kit->cast_to_flat_array(base, vk, true, false, false);\n+      Node* ptr = kit->array_element_address(cast, idx, T_FLAT_ELEMENT);\n+      vt_non_atomic = InlineTypeNode::make_from_flat(kit, vk, cast, ptr, false, false, true, decorators);\n+\n+      region->init_req(3, kit->control());\n+      mem->set_req(3, kit->reset_memory());\n+      io->set_req(3, kit->i_o());\n+    }\n+  }\n+\n+  InlineTypeNode* vt = nullptr;\n+  if (vt_nullable == nullptr && vt_null_free == nullptr && vt_non_atomic == nullptr) {\n+    \/\/ All paths are dead\n+    vt = make_null(gvn, vk);\n+  } else if (vt_nullable == nullptr && vt_null_free == nullptr) {\n+    vt = vt_non_atomic;\n+  } else if (vt_nullable == nullptr && vt_non_atomic == nullptr) {\n+    vt = vt_null_free;\n+  } else if (vt_null_free == nullptr && vt_non_atomic == nullptr) {\n+    vt = vt_nullable;\n+  }\n+  if (vt != nullptr) {\n+    kit->set_control(kit->gvn().transform(region));\n+    kit->set_all_memory(kit->gvn().transform(mem));\n+    kit->set_i_o(kit->gvn().transform(io));\n+    return vt;\n+  }\n+\n+  InlineTypeNode* zero = InlineTypeNode::make_null(gvn, vk);\n+  vt = zero->clone_with_phis(&gvn, region);\n+  if (vt_nullable != nullptr) {\n+    vt = vt->merge_with(&gvn, vt_nullable, 1, false);\n+  }\n+  if (vt_null_free != nullptr) {\n+    vt = vt->merge_with(&gvn, vt_null_free, 2, false);\n+  }\n+  if (vt_non_atomic != nullptr) {\n+    vt = vt->merge_with(&gvn, vt_non_atomic, 3, false);\n+  }\n+\n+  kit->set_control(kit->gvn().transform(region));\n+  kit->set_all_memory(kit->gvn().transform(mem));\n+  kit->set_i_o(kit->gvn().transform(io));\n+  return gvn.transform(vt)->as_InlineType();\n+}\n+\n+InlineTypeNode* InlineTypeNode::make_from_multi(GraphKit* kit, MultiNode* multi, ciInlineKlass* vk, uint& base_input, bool in, bool null_free) {\n+  InlineTypeNode* vt = make_uninitialized(kit->gvn(), vk, null_free);\n+  if (!in) {\n+    \/\/ Keep track of the oop. The returned inline type might already be buffered.\n+    Node* oop = kit->gvn().transform(new ProjNode(multi, base_input++));\n+    vt->set_oop(kit->gvn(), oop);\n+  }\n+  GrowableArray<ciType*> visited;\n+  visited.push(vk);\n+  vt->initialize_fields(kit, multi, base_input, in, null_free, nullptr, visited);\n+  return kit->gvn().transform(vt)->as_InlineType();\n+}\n+\n+Node* InlineTypeNode::is_loaded(PhaseGVN* phase, ciInlineKlass* vk, Node* base, int holder_offset) {\n+  if (vk == nullptr) {\n+    vk = inline_klass();\n+  }\n+  for (uint i = 0; i < field_count(); ++i) {\n+    int offset = holder_offset + field_offset(i);\n+    Node* value = field_value(i);\n+    if (value->is_InlineType()) {\n+      InlineTypeNode* vt = value->as_InlineType();\n+      if (vt->type()->inline_klass()->is_empty()) {\n+        continue;\n+      } else if (field_is_flat(i) && vt->is_InlineType()) {\n+        \/\/ Check inline type field load recursively\n+        base = vt->as_InlineType()->is_loaded(phase, vk, base, offset - vt->type()->inline_klass()->payload_offset());\n+        if (base == nullptr) {\n+          return nullptr;\n+        }\n+        continue;\n+      } else {\n+        value = vt->get_oop();\n+        if (value->Opcode() == Op_CastPP) {\n+          \/\/ Skip CastPP\n+          value = value->in(1);\n+        }\n+      }\n+    }\n+    if (value->isa_DecodeN()) {\n+      \/\/ Skip DecodeN\n+      value = value->in(1);\n+    }\n+    if (value->isa_Load()) {\n+      \/\/ Check if base and offset of field load matches inline type layout\n+      intptr_t loffset = 0;\n+      Node* lbase = AddPNode::Ideal_base_and_offset(value->in(MemNode::Address), phase, loffset);\n+      if (lbase == nullptr || (lbase != base && base != nullptr) || loffset != offset) {\n+        return nullptr;\n+      } else if (base == nullptr) {\n+        \/\/ Set base and check if pointer type matches\n+        base = lbase;\n+        const TypeInstPtr* vtptr = phase->type(base)->isa_instptr();\n+        if (vtptr == nullptr || !vtptr->instance_klass()->equals(vk)) {\n+          return nullptr;\n+        }\n+      }\n+    } else {\n+      return nullptr;\n+    }\n+  }\n+  return base;\n+}\n+\n+Node* InlineTypeNode::tagged_klass(ciInlineKlass* vk, PhaseGVN& gvn) {\n+  const TypeKlassPtr* tk = TypeKlassPtr::make(vk);\n+  intptr_t bits = tk->get_con();\n+  set_nth_bit(bits, 0);\n+  return gvn.longcon((jlong)bits);\n+}\n+\n+void InlineTypeNode::pass_fields(GraphKit* kit, Node* n, uint& base_input, bool in, bool null_free) {\n+  if (!null_free && in) {\n+    n->init_req(base_input++, get_null_marker());\n+  }\n+  for (uint i = 0; i < field_count(); i++) {\n+    Node* arg = field_value(i);\n+    if (field_is_flat(i)) {\n+      \/\/ Flat inline type field\n+      arg->as_InlineType()->pass_fields(kit, n, base_input, in);\n+      if (!field_is_null_free(i)) {\n+        assert(field_null_marker_offset(i) != -1, \"inconsistency\");\n+        n->init_req(base_input++, arg->as_InlineType()->get_null_marker());\n+      }\n+    } else {\n+      if (arg->is_InlineType()) {\n+        \/\/ Non-flat inline type field\n+        InlineTypeNode* vt = arg->as_InlineType();\n+        assert(n->Opcode() != Op_Return || vt->is_allocated(&kit->gvn()), \"inline type field should be allocated on return\");\n+        arg = vt->buffer(kit);\n+      }\n+      \/\/ Initialize call\/return arguments\n+      n->init_req(base_input++, arg);\n+      if (field_type(i)->size() == 2) {\n+        n->init_req(base_input++, kit->top());\n+      }\n+    }\n+  }\n+  \/\/ The last argument is used to pass the null marker to compiled code and not required here.\n+  if (!null_free && !in) {\n+    n->init_req(base_input++, kit->top());\n+  }\n+}\n+\n+void InlineTypeNode::initialize_fields(GraphKit* kit, MultiNode* multi, uint& base_input, bool in, bool null_free, Node* null_check_region, GrowableArray<ciType*>& visited) {\n+  PhaseGVN& gvn = kit->gvn();\n+  Node* null_marker = nullptr;\n+  if (!null_free) {\n+    \/\/ Nullable inline type\n+    if (in) {\n+      \/\/ Set null marker\n+      if (multi->is_Start()) {\n+        null_marker = gvn.transform(new ParmNode(multi->as_Start(), base_input));\n+      } else {\n+        null_marker = multi->as_Call()->in(base_input);\n+      }\n+      set_req(NullMarker, null_marker);\n+      base_input++;\n+    }\n+    \/\/ Add a null check to make subsequent loads dependent on\n+    assert(null_check_region == nullptr, \"already set\");\n+    if (null_marker == nullptr) {\n+      \/\/ Will only be initialized below, use dummy node for now\n+      null_marker = new Node(1);\n+      null_marker->init_req(0, kit->control()); \/\/ Add an input to prevent dummy from being dead\n+      gvn.set_type_bottom(null_marker);\n+    }\n+    Node* null_ctrl = kit->top();\n+    kit->null_check_common(null_marker, T_INT, false, &null_ctrl);\n+    Node* non_null_ctrl = kit->control();\n+    null_check_region = new RegionNode(3);\n+    null_check_region->init_req(1, non_null_ctrl);\n+    null_check_region->init_req(2, null_ctrl);\n+    null_check_region = gvn.transform(null_check_region);\n+    kit->set_control(null_check_region);\n+  }\n+\n+  for (uint i = 0; i < field_count(); ++i) {\n+    ciType* type = field_type(i);\n+    Node* parm = nullptr;\n+    if (field_is_flat(i)) {\n+      \/\/ Flat inline type field\n+      InlineTypeNode* vt = make_uninitialized(gvn, type->as_inline_klass(), field_is_null_free(i));\n+      vt->initialize_fields(kit, multi, base_input, in, true, null_check_region, visited);\n+      if (!field_is_null_free(i)) {\n+        assert(field_null_marker_offset(i) != -1, \"inconsistency\");\n+        Node* null_marker = nullptr;\n+        if (multi->is_Start()) {\n+          null_marker = gvn.transform(new ParmNode(multi->as_Start(), base_input));\n+        } else if (in) {\n+          null_marker = multi->as_Call()->in(base_input);\n+        } else {\n+          null_marker = gvn.transform(new ProjNode(multi->as_Call(), base_input));\n+        }\n+        vt->set_req(NullMarker, null_marker);\n+        base_input++;\n+      }\n+      parm = gvn.transform(vt);\n+    } else {\n+      if (multi->is_Start()) {\n+        assert(in, \"return from start?\");\n+        parm = gvn.transform(new ParmNode(multi->as_Start(), base_input));\n+      } else if (in) {\n+        parm = multi->as_Call()->in(base_input);\n+      } else {\n+        parm = gvn.transform(new ProjNode(multi->as_Call(), base_input));\n+      }\n+      bool null_free = field_is_null_free(i);\n+      \/\/ Non-flat inline type field\n+      if (type->is_inlinetype()) {\n+        if (null_check_region != nullptr) {\n+          \/\/ We limit scalarization for inline types with circular fields and can therefore observe nodes\n+          \/\/ of the same type but with different scalarization depth during GVN. To avoid inconsistencies\n+          \/\/ during merging, make sure that we only create Phis for fields that are guaranteed to be scalarized.\n+          if (parm->is_InlineType() && kit->C->has_circular_inline_type()) {\n+            parm = parm->as_InlineType()->get_oop();\n+          }\n+          \/\/ Holder is nullable, set field to nullptr if holder is nullptr to avoid loading from uninitialized memory\n+          parm = PhiNode::make(null_check_region, parm, TypeInstPtr::make(TypePtr::BotPTR, type->as_inline_klass()));\n+          parm->set_req(2, kit->zerocon(T_OBJECT));\n+          parm = gvn.transform(parm);\n+          null_free = false;\n+        }\n+        if (visited.contains(type)) {\n+          kit->C->set_has_circular_inline_type(true);\n+        } else if (!parm->is_InlineType()) {\n+          int old_len = visited.length();\n+          visited.push(type);\n+          if (null_free) {\n+            parm = kit->cast_not_null(parm);\n+          }\n+          parm = make_from_oop_impl(kit, parm, type->as_inline_klass(), visited);\n+          visited.trunc_to(old_len);\n+        }\n+      }\n+      base_input += type->size();\n+    }\n+    assert(parm != nullptr, \"should never be null\");\n+    assert(field_value(i) == nullptr, \"already set\");\n+    set_field_value(i, parm);\n+    gvn.record_for_igvn(parm);\n+  }\n+  \/\/ The last argument is used to pass the null marker to compiled code\n+  if (!null_free && !in) {\n+    Node* cmp = null_marker->raw_out(0);\n+    null_marker = gvn.transform(new ProjNode(multi->as_Call(), base_input));\n+    set_req(NullMarker, null_marker);\n+    gvn.hash_delete(cmp);\n+    cmp->set_req(1, null_marker);\n+    gvn.hash_find_insert(cmp);\n+    gvn.record_for_igvn(cmp);\n+    base_input++;\n+  }\n+}\n+\n+\/\/ Search for multiple allocations of this inline type and try to replace them by dominating allocations.\n+\/\/ Equivalent InlineTypeNodes are merged by GVN, so we just need to search for AllocateNode users to find redundant allocations.\n+void InlineTypeNode::remove_redundant_allocations(PhaseIdealLoop* phase) {\n+  PhaseIterGVN* igvn = &phase->igvn();\n+  \/\/ Search for allocations of this inline type. Ignore scalar replaceable ones, they\n+  \/\/ will be removed anyway and changing the memory chain will confuse other optimizations.\n+  for (DUIterator_Fast imax, i = fast_outs(imax); i < imax; i++) {\n+    AllocateNode* alloc = fast_out(i)->isa_Allocate();\n+    if (alloc != nullptr && alloc->in(AllocateNode::InlineType) == this && !alloc->_is_scalar_replaceable) {\n+      Node* res = alloc->result_cast();\n+      if (res == nullptr || !res->is_CheckCastPP()) {\n+        break; \/\/ No unique CheckCastPP\n+      }\n+      \/\/ Search for a dominating allocation of the same inline type\n+      Node* res_dom = res;\n+      for (DUIterator_Fast jmax, j = fast_outs(jmax); j < jmax; j++) {\n+        AllocateNode* alloc_other = fast_out(j)->isa_Allocate();\n+        if (alloc_other != nullptr && alloc_other->in(AllocateNode::InlineType) == this && !alloc_other->_is_scalar_replaceable) {\n+          Node* res_other = alloc_other->result_cast();\n+          if (res_other != nullptr && res_other->is_CheckCastPP() && res_other != res_dom &&\n+              phase->is_dominator(res_other->in(0), res_dom->in(0))) {\n+            res_dom = res_other;\n+          }\n+        }\n+      }\n+      if (res_dom != res) {\n+        \/\/ Replace allocation by dominating one.\n+        replace_allocation(igvn, res, res_dom);\n+        \/\/ The result of the dominated allocation is now unused and will be removed\n+        \/\/ later in PhaseMacroExpand::eliminate_allocate_node to not confuse loop opts.\n+        igvn->_worklist.push(alloc);\n+      }\n+    }\n+  }\n+}\n+\n+InlineTypeNode* InlineTypeNode::make_null(PhaseGVN& gvn, ciInlineKlass* vk, bool transform) {\n+  GrowableArray<ciType*> visited;\n+  visited.push(vk);\n+  return make_null_impl(gvn, vk, visited, transform);\n+}\n+\n+InlineTypeNode* InlineTypeNode::make_null_impl(PhaseGVN& gvn, ciInlineKlass* vk, GrowableArray<ciType*>& visited, bool transform) {\n+  InlineTypeNode* vt = new InlineTypeNode(vk, gvn.zerocon(T_OBJECT), \/* null_free= *\/ false);\n+  vt->set_is_buffered(gvn);\n+  vt->set_null_marker(gvn, gvn.intcon(0));\n+  for (uint i = 0; i < vt->field_count(); i++) {\n+    ciType* ft = vt->field_type(i);\n+    Node* value = gvn.zerocon(ft->basic_type());\n+    if (!vt->field_is_flat(i) && visited.contains(ft)) {\n+      gvn.C->set_has_circular_inline_type(true);\n+    } else if (ft->is_inlinetype()) {\n+      int old_len = visited.length();\n+      visited.push(ft);\n+      value = make_null_impl(gvn, ft->as_inline_klass(), visited);\n+      visited.trunc_to(old_len);\n+    }\n+    vt->set_field_value(i, value);\n+  }\n+  return transform ? gvn.transform(vt)->as_InlineType() : vt;\n+}\n+\n+InlineTypeNode* InlineTypeNode::clone_if_required(PhaseGVN* gvn, SafePointNode* map, bool safe_for_replace) {\n+  if (!safe_for_replace || (map == nullptr && outcnt() != 0)) {\n+    return clone()->as_InlineType();\n+  }\n+  for (DUIterator_Fast imax, i = fast_outs(imax); i < imax; i++) {\n+    if (fast_out(i) != map) {\n+      return clone()->as_InlineType();\n+    }\n+  }\n+  gvn->hash_delete(this);\n+  return this;\n+}\n+\n+const Type* InlineTypeNode::Value(PhaseGVN* phase) const {\n+  Node* oop = get_oop();\n+  const Type* toop = phase->type(oop);\n+#ifdef ASSERT\n+  if (oop->is_Con() && toop->is_zero_type() && _type->isa_oopptr()->is_known_instance()) {\n+    \/\/ We are not allocated (anymore) and should therefore not have an instance id\n+    dump(1);\n+    assert(false, \"Unbuffered inline type should not have known instance id\");\n+  }\n+#endif\n+  const Type* t = toop->filter_speculative(_type);\n+  if (t->singleton()) {\n+    \/\/ Don't replace InlineType by a constant\n+    t = _type;\n+  }\n+  const Type* tinit = phase->type(in(NullMarker));\n+  if (tinit == Type::TOP) {\n+    return Type::TOP;\n+  }\n+  if (tinit->isa_int() && tinit->is_int()->is_con(1)) {\n+    t = t->join_speculative(TypePtr::NOTNULL);\n+  }\n+  return t;\n+}\n","filename":"src\/hotspot\/share\/opto\/inlinetypenode.cpp","additions":1652,"deletions":0,"binary":false,"changes":1652,"status":"added"},{"patch":"@@ -229,0 +229,1 @@\n+    case Op_StoreLSpecial:\n@@ -318,1 +319,1 @@\n-        if (offset == Type::OffsetBot || tptr->_offset == Type::OffsetBot)\n+        if (offset == Type::OffsetBot || tptr->offset() == Type::OffsetBot)\n@@ -320,1 +321,1 @@\n-        offset += tptr->_offset; \/\/ correct if base is offsetted\n+        offset += tptr->offset(); \/\/ correct if base is offsetted\n@@ -365,1 +366,5 @@\n-      Block *inb = get_block_for_node(mach->in(j));\n+      Block* inb = get_block_for_node(mach->in(j));\n+      if (mach->in(j)->is_Con() && mach->in(j)->req() == 1 && inb == get_block_for_node(mach)) {\n+        \/\/ Ignore constant loads scheduled in the same block (we can simply hoist them as well)\n+        continue;\n+      }\n@@ -438,0 +443,21 @@\n+  \/\/ Hoist constant load inputs as well.\n+  for (uint i = 1; i < best->req(); ++i) {\n+    Node* n = best->in(i);\n+    if (n->is_Con() && get_block_for_node(n) == get_block_for_node(best)) {\n+      get_block_for_node(n)->find_remove(n);\n+      block->add_inst(n);\n+      map_node_to_block(n, block);\n+      \/\/ Constant loads may kill flags (for example, when XORing a register).\n+      \/\/ Check for flag-killing projections that also need to be hoisted.\n+      for (DUIterator_Fast jmax, j = n->fast_outs(jmax); j < jmax; j++) {\n+        Node* proj = n->fast_out(j);\n+        if (proj->is_MachProj()) {\n+          get_block_for_node(proj)->find_remove(proj);\n+          block->add_inst(proj);\n+          map_node_to_block(proj, block);\n+        }\n+      }\n+    }\n+  }\n+\n+\n@@ -746,0 +772,1 @@\n+        case Op_StoreLSpecial:\n@@ -912,1 +939,1 @@\n-  uint r_cnt = mcall->tf()->range()->cnt();\n+  uint r_cnt = mcall->tf()->range_cc()->cnt();\n","filename":"src\/hotspot\/share\/opto\/lcm.cpp","additions":31,"deletions":4,"binary":false,"changes":35,"status":"modified"},{"patch":"@@ -26,0 +26,3 @@\n+#include \"ci\/ciArrayKlass.hpp\"\n+#include \"ci\/ciFlatArrayKlass.hpp\"\n+#include \"ci\/ciInstanceKlass.hpp\"\n@@ -32,0 +35,1 @@\n+#include \"gc\/shared\/c2\/barrierSetC2.hpp\"\n@@ -34,0 +38,1 @@\n+#include \"oops\/accessDecorators.hpp\"\n@@ -35,0 +40,1 @@\n+#include \"oops\/layoutKind.hpp\"\n@@ -43,0 +49,1 @@\n+#include \"opto\/graphKit.hpp\"\n@@ -44,0 +51,1 @@\n+#include \"opto\/inlinetypenode.hpp\"\n@@ -49,0 +57,1 @@\n+#include \"opto\/opcodes.hpp\"\n@@ -53,0 +62,1 @@\n+#include \"opto\/type.hpp\"\n@@ -61,0 +71,1 @@\n+#include \"utilities\/globalDefinitions.hpp\"\n@@ -319,0 +330,2 @@\n+  case vmIntrinsics::_makePrivateBuffer:        return inline_unsafe_make_private_buffer();\n+  case vmIntrinsics::_finishPrivateBuffer:      return inline_unsafe_finish_private_buffer();\n@@ -328,0 +341,1 @@\n+  case vmIntrinsics::_getValue:                 return inline_unsafe_access(!is_store, T_OBJECT,   Relaxed, false, true);\n@@ -338,0 +352,1 @@\n+  case vmIntrinsics::_putValue:                 return inline_unsafe_access( is_store, T_OBJECT,   Relaxed, false, true);\n@@ -409,0 +424,3 @@\n+  case vmIntrinsics::_getFlatValue:             return inline_unsafe_flat_access(!is_store, Relaxed);\n+  case vmIntrinsics::_putFlatValue:             return inline_unsafe_flat_access( is_store, Relaxed);\n+\n@@ -516,0 +534,3 @@\n+  case vmIntrinsics::_newNullRestrictedNonAtomicArray: return inline_newArray(\/* null_free *\/ true, \/* atomic *\/ false);\n+  case vmIntrinsics::_newNullRestrictedAtomicArray: return inline_newArray(\/* null_free *\/ true, \/* atomic *\/ true);\n+  case vmIntrinsics::_newNullableAtomicArray:     return inline_newArray(\/* null_free *\/ false, \/* atomic *\/ true);\n@@ -2321,0 +2342,1 @@\n+  bool null_free = false;\n@@ -2326,0 +2348,1 @@\n+      null_free = alias_type->field()->is_null_free();\n@@ -2332,1 +2355,1 @@\n-    if (adr_type->offset() >= objArrayOopDesc::base_offset_in_bytes()) {\n+    if (adr_type->offset() >= refArrayOopDesc::base_offset_in_bytes()) {\n@@ -2334,0 +2357,1 @@\n+      null_free = adr_type->is_aryptr()->is_null_free();\n@@ -2346,0 +2370,3 @@\n+    if (null_free) {\n+      result = result->join_speculative(TypePtr::NOTNULL)->is_oopptr();\n+    }\n@@ -2376,1 +2403,1 @@\n-bool LibraryCallKit::inline_unsafe_access(bool is_store, const BasicType type, const AccessKind kind, const bool unaligned) {\n+bool LibraryCallKit::inline_unsafe_access(bool is_store, const BasicType type, const AccessKind kind, const bool unaligned, const bool is_flat) {\n@@ -2401,1 +2428,1 @@\n-      assert(sig->count() == 2, \"oop getter has 2 arguments\");\n+      assert(sig->count() == 2 || (is_flat && sig->count() == 3), \"oop getter has 2 or 3 arguments\");\n@@ -2407,1 +2434,1 @@\n-      assert(sig->count() == 3, \"oop putter has 3 arguments\");\n+      assert(sig->count() == 3 || (is_flat && sig->count() == 4), \"oop putter has 3 arguments\");\n@@ -2433,0 +2460,49 @@\n+\n+  ciInlineKlass* inline_klass = nullptr;\n+  if (is_flat) {\n+    const TypeInstPtr* cls = _gvn.type(argument(4))->isa_instptr();\n+    if (cls == nullptr || cls->const_oop() == nullptr) {\n+      return false;\n+    }\n+    ciType* mirror_type = cls->const_oop()->as_instance()->java_mirror_type();\n+    if (!mirror_type->is_inlinetype()) {\n+      return false;\n+    }\n+    inline_klass = mirror_type->as_inline_klass();\n+  }\n+\n+  if (base->is_InlineType()) {\n+    assert(!is_store, \"InlineTypeNodes are non-larval value objects\");\n+    InlineTypeNode* vt = base->as_InlineType();\n+    if (offset->is_Con()) {\n+      long off = find_long_con(offset, 0);\n+      ciInlineKlass* vk = vt->type()->inline_klass();\n+      if ((long)(int)off != off || !vk->contains_field_offset(off)) {\n+        return false;\n+      }\n+\n+      ciField* field = vk->get_non_flat_field_by_offset(off);\n+      if (field != nullptr) {\n+        BasicType bt = type2field[field->type()->basic_type()];\n+        if (bt == T_ARRAY || bt == T_NARROWOOP) {\n+          bt = T_OBJECT;\n+        }\n+        if (bt == type && (!field->is_flat() || field->type() == inline_klass)) {\n+          Node* value = vt->field_value_by_offset(off, false);\n+          if (value->is_InlineType()) {\n+            value = value->as_InlineType()->adjust_scalarization_depth(this);\n+          }\n+          set_result(value);\n+          return true;\n+        }\n+      }\n+    }\n+    {\n+      \/\/ Re-execute the unsafe access if allocation triggers deoptimization.\n+      PreserveReexecuteState preexecs(this);\n+      jvms()->set_should_reexecute(true);\n+      vt = vt->buffer(this);\n+    }\n+    base = vt->get_oop();\n+  }\n+\n@@ -2444,1 +2520,1 @@\n-    if (type != T_OBJECT) {\n+    if (type != T_OBJECT && (inline_klass == nullptr || !inline_klass->has_object_fields())) {\n@@ -2462,1 +2538,1 @@\n-  Node* val = is_store ? argument(4) : nullptr;\n+  Node* val = is_store ? argument(4 + (is_flat ? 1 : 0)) : nullptr;\n@@ -2483,1 +2559,29 @@\n-  BasicType bt = alias_type->basic_type();\n+  BasicType bt = T_ILLEGAL;\n+  ciField* field = nullptr;\n+  if (adr_type->isa_instptr()) {\n+    const TypeInstPtr* instptr = adr_type->is_instptr();\n+    ciInstanceKlass* k = instptr->instance_klass();\n+    int off = instptr->offset();\n+    if (instptr->const_oop() != nullptr &&\n+        k == ciEnv::current()->Class_klass() &&\n+        instptr->offset() >= (k->size_helper() * wordSize)) {\n+      k = instptr->const_oop()->as_instance()->java_lang_Class_klass()->as_instance_klass();\n+      field = k->get_field_by_offset(off, true);\n+    } else {\n+      field = k->get_non_flat_field_by_offset(off);\n+    }\n+    if (field != nullptr) {\n+      bt = type2field[field->type()->basic_type()];\n+    }\n+    if (bt != alias_type->basic_type()) {\n+      \/\/ Type mismatch. Is it an access to a nested flat field?\n+      field = k->get_field_by_offset(off, false);\n+      if (field != nullptr) {\n+        bt = type2field[field->type()->basic_type()];\n+      }\n+    }\n+    assert(bt == alias_type->basic_type() || is_flat, \"should match\");\n+  } else {\n+    bt = alias_type->basic_type();\n+  }\n+\n@@ -2506,0 +2610,23 @@\n+  if (is_flat) {\n+    if (adr_type->isa_instptr()) {\n+      if (field == nullptr || field->type() != inline_klass) {\n+        mismatched = true;\n+      }\n+    } else if (adr_type->isa_aryptr()) {\n+      const Type* elem = adr_type->is_aryptr()->elem();\n+      if (!adr_type->is_flat() || elem->inline_klass() != inline_klass) {\n+        mismatched = true;\n+      }\n+    } else {\n+      mismatched = true;\n+    }\n+    if (is_store) {\n+      const Type* val_t = _gvn.type(val);\n+      if (!val_t->is_inlinetypeptr() || val_t->inline_klass() != inline_klass) {\n+        set_map(old_map);\n+        set_sp(old_sp);\n+        return false;\n+      }\n+    }\n+  }\n+\n@@ -2507,1 +2634,1 @@\n-  assert(!mismatched || alias_type->adr_type()->is_oopptr(), \"off-heap access can't be mismatched\");\n+  assert(!mismatched || is_flat || alias_type->adr_type()->is_oopptr(), \"off-heap access can't be mismatched\");\n@@ -2519,4 +2646,6 @@\n-  if (!is_store && type == T_OBJECT) {\n-    const TypeOopPtr* tjp = sharpen_unsafe_type(alias_type, adr_type);\n-    if (tjp != nullptr) {\n-      value_type = tjp;\n+  if (!is_store) {\n+    if (type == T_OBJECT && !is_flat) {\n+      const TypeOopPtr* tjp = sharpen_unsafe_type(alias_type, adr_type);\n+      if (tjp != nullptr) {\n+        value_type = tjp;\n+      }\n@@ -2538,2 +2667,2 @@\n-    ciField* field = alias_type->field();\n-    if (heap_base_oop != top() && field != nullptr && field->is_constant() && !mismatched) {\n+\n+    if (heap_base_oop != top() && field != nullptr && field->is_constant() && !field->is_flat() && !mismatched) {\n@@ -2545,1 +2674,10 @@\n-      p = access_load_at(heap_base_oop, adr, adr_type, value_type, type, decorators);\n+      if (is_flat) {\n+        p = InlineTypeNode::make_from_flat(this, inline_klass, base, adr, adr_type, false, false, true);\n+      } else {\n+        p = access_load_at(heap_base_oop, adr, adr_type, value_type, type, decorators);\n+        const TypeOopPtr* ptr = value_type->make_oopptr();\n+        if (ptr != nullptr && ptr->is_inlinetypeptr()) {\n+          \/\/ Load a non-flattened inline type from memory\n+          p = InlineTypeNode::make_from_oop(this, p, ptr->inline_klass());\n+        }\n+      }\n@@ -2583,1 +2721,5 @@\n-    access_store_at(heap_base_oop, adr, adr_type, val, value_type, type, decorators);\n+    if (is_flat) {\n+      val->as_InlineType()->store_flat(this, base, adr, false, false, true, decorators);\n+    } else {\n+      access_store_at(heap_base_oop, adr, adr_type, val, value_type, type, decorators);\n+    }\n@@ -2589,0 +2731,235 @@\n+bool LibraryCallKit::inline_unsafe_flat_access(bool is_store, AccessKind kind) {\n+#ifdef ASSERT\n+  {\n+    ResourceMark rm;\n+    \/\/ Check the signatures.\n+    ciSignature* sig = callee()->signature();\n+    assert(sig->type_at(0)->basic_type() == T_OBJECT, \"base should be object, but is %s\", type2name(sig->type_at(0)->basic_type()));\n+    assert(sig->type_at(1)->basic_type() == T_LONG, \"offset should be long, but is %s\", type2name(sig->type_at(1)->basic_type()));\n+    assert(sig->type_at(2)->basic_type() == T_INT, \"layout kind should be int, but is %s\", type2name(sig->type_at(3)->basic_type()));\n+    assert(sig->type_at(3)->basic_type() == T_OBJECT, \"value klass should be object, but is %s\", type2name(sig->type_at(4)->basic_type()));\n+    if (is_store) {\n+      assert(sig->return_type()->basic_type() == T_VOID, \"putter must not return a value, but returns %s\", type2name(sig->return_type()->basic_type()));\n+      assert(sig->count() == 5, \"flat putter should have 5 arguments, but has %d\", sig->count());\n+      assert(sig->type_at(4)->basic_type() == T_OBJECT, \"put value should be object, but is %s\", type2name(sig->type_at(5)->basic_type()));\n+    } else {\n+      assert(sig->return_type()->basic_type() == T_OBJECT, \"getter must return an object, but returns %s\", type2name(sig->return_type()->basic_type()));\n+      assert(sig->count() == 4, \"flat getter should have 4 arguments, but has %d\", sig->count());\n+    }\n+ }\n+#endif \/\/ ASSERT\n+\n+  assert(kind == Relaxed, \"Only plain accesses for now\");\n+  if (callee()->is_static()) {\n+    \/\/ caller must have the capability!\n+    return false;\n+  }\n+  C->set_has_unsafe_access(true);\n+\n+  const TypeInstPtr* value_klass_node = _gvn.type(argument(5))->isa_instptr();\n+  if (value_klass_node == nullptr || value_klass_node->const_oop() == nullptr) {\n+    \/\/ parameter valueType is not a constant\n+    return false;\n+  }\n+  ciType* mirror_type = value_klass_node->const_oop()->as_instance()->java_mirror_type();\n+  if (!mirror_type->is_inlinetype()) {\n+    \/\/ Dead code\n+    return false;\n+  }\n+  ciInlineKlass* value_klass = mirror_type->as_inline_klass();\n+\n+  const TypeInt* layout_type = _gvn.type(argument(4))->isa_int();\n+  if (layout_type == nullptr || !layout_type->is_con()) {\n+    \/\/ parameter layoutKind is not a constant\n+    return false;\n+  }\n+  assert(layout_type->get_con() >= static_cast<int>(LayoutKind::REFERENCE) &&\n+         layout_type->get_con() <= static_cast<int>(LayoutKind::UNKNOWN),\n+         \"invalid layoutKind %d\", layout_type->get_con());\n+  LayoutKind layout = static_cast<LayoutKind>(layout_type->get_con());\n+  assert(layout == LayoutKind::REFERENCE || layout == LayoutKind::NON_ATOMIC_FLAT ||\n+         layout == LayoutKind::ATOMIC_FLAT || layout == LayoutKind::NULLABLE_ATOMIC_FLAT,\n+         \"unexpected layoutKind %d\", layout_type->get_con());\n+\n+  null_check(argument(0));\n+  if (stopped()) {\n+    return true;\n+  }\n+\n+  Node* base = must_be_not_null(argument(1), true);\n+  Node* offset = argument(2);\n+  const Type* base_type = _gvn.type(base);\n+\n+  Node* ptr;\n+  bool immutable_memory = false;\n+  DecoratorSet decorators = C2_UNSAFE_ACCESS | IN_HEAP | MO_UNORDERED;\n+  if (base_type->isa_instptr()) {\n+    const TypeLong* offset_type = _gvn.type(offset)->isa_long();\n+    if (offset_type == nullptr || !offset_type->is_con()) {\n+      \/\/ Offset into a non-array should be a constant\n+      decorators |= C2_MISMATCHED;\n+    } else {\n+      int offset_con = checked_cast<int>(offset_type->get_con());\n+      ciInstanceKlass* base_klass = base_type->is_instptr()->instance_klass();\n+      ciField* field = base_klass->get_non_flat_field_by_offset(offset_con);\n+      if (field == nullptr) {\n+        assert(!base_klass->is_final(), \"non-existence field at offset %d of class %s\", offset_con, base_klass->name()->as_utf8());\n+        decorators |= C2_MISMATCHED;\n+      } else {\n+        assert(field->type() == value_klass, \"field at offset %d of %s is of type %s, but valueType is %s\",\n+               offset_con, base_klass->name()->as_utf8(), field->type()->name(), value_klass->name()->as_utf8());\n+        immutable_memory = field->is_strict() && field->is_final();\n+\n+        if (base->is_InlineType()) {\n+          assert(!is_store, \"Cannot store into a non-larval value object\");\n+          set_result(base->as_InlineType()->field_value_by_offset(offset_con, false));\n+          return true;\n+        }\n+      }\n+    }\n+\n+    if (base->is_InlineType()) {\n+      assert(!is_store, \"Cannot store into a non-larval value object\");\n+      base = base->as_InlineType()->buffer(this, true);\n+    }\n+    ptr = basic_plus_adr(base, ConvL2X(offset));\n+  } else if (base_type->isa_aryptr()) {\n+    decorators |= IS_ARRAY;\n+    if (layout == LayoutKind::REFERENCE) {\n+      if (!base_type->is_aryptr()->is_not_flat()) {\n+        const TypeAryPtr* array_type = base_type->is_aryptr()->cast_to_not_flat();\n+        Node* new_base = _gvn.transform(new CastPPNode(control(), base, array_type, ConstraintCastNode::StrongDependency));\n+        replace_in_map(base, new_base);\n+        base = new_base;\n+      }\n+      ptr = basic_plus_adr(base, ConvL2X(offset));\n+    } else {\n+      if (UseArrayFlattening) {\n+        \/\/ Flat array must have an exact type\n+        bool is_null_free = layout != LayoutKind::NULLABLE_ATOMIC_FLAT;\n+        bool is_atomic = layout != LayoutKind::NON_ATOMIC_FLAT;\n+        Node* new_base = cast_to_flat_array(base, value_klass, is_null_free, !is_null_free, is_atomic);\n+        replace_in_map(base, new_base);\n+        base = new_base;\n+        ptr = basic_plus_adr(base, ConvL2X(offset));\n+        const TypeAryPtr* ptr_type = _gvn.type(ptr)->is_aryptr();\n+        if (ptr_type->field_offset().get() != 0) {\n+          ptr = _gvn.transform(new CastPPNode(control(), ptr, ptr_type->with_field_offset(0), ConstraintCastNode::StrongDependency));\n+        }\n+      } else {\n+        uncommon_trap(Deoptimization::Reason_intrinsic,\n+                      Deoptimization::Action_none);\n+        return true;\n+      }\n+    }\n+  } else {\n+    decorators |= C2_MISMATCHED;\n+    ptr = basic_plus_adr(base, ConvL2X(offset));\n+  }\n+\n+  if (is_store) {\n+    Node* value = argument(6);\n+    const Type* value_type = _gvn.type(value);\n+    if (!value_type->is_inlinetypeptr()) {\n+      value_type = Type::get_const_type(value_klass)->filter_speculative(value_type);\n+      Node* new_value = _gvn.transform(new CastPPNode(control(), value, value_type, ConstraintCastNode::StrongDependency));\n+      new_value = InlineTypeNode::make_from_oop(this, new_value, value_klass);\n+      replace_in_map(value, new_value);\n+      value = new_value;\n+    }\n+\n+    assert(value_type->inline_klass() == value_klass, \"value is of type %s while valueType is %s\", value_type->inline_klass()->name()->as_utf8(), value_klass->name()->as_utf8());\n+    if (layout == LayoutKind::REFERENCE) {\n+      const TypePtr* ptr_type = (decorators & C2_MISMATCHED) != 0 ? TypeRawPtr::BOTTOM : _gvn.type(ptr)->is_ptr();\n+      access_store_at(base, ptr, ptr_type, value, value_type, T_OBJECT, decorators);\n+    } else {\n+      bool atomic = layout != LayoutKind::NON_ATOMIC_FLAT;\n+      bool null_free = layout != LayoutKind::NULLABLE_ATOMIC_FLAT;\n+      value->as_InlineType()->store_flat(this, base, ptr, atomic, immutable_memory, null_free, decorators);\n+    }\n+\n+    return true;\n+  } else {\n+    decorators |= (C2_CONTROL_DEPENDENT_LOAD | C2_UNKNOWN_CONTROL_LOAD);\n+    InlineTypeNode* result;\n+    if (layout == LayoutKind::REFERENCE) {\n+      const TypePtr* ptr_type = (decorators & C2_MISMATCHED) != 0 ? TypeRawPtr::BOTTOM : _gvn.type(ptr)->is_ptr();\n+      Node* oop = access_load_at(base, ptr, ptr_type, Type::get_const_type(value_klass), T_OBJECT, decorators);\n+      result = InlineTypeNode::make_from_oop(this, oop, value_klass);\n+    } else {\n+      bool atomic = layout != LayoutKind::NON_ATOMIC_FLAT;\n+      bool null_free = layout != LayoutKind::NULLABLE_ATOMIC_FLAT;\n+      result = InlineTypeNode::make_from_flat(this, value_klass, base, ptr, atomic, immutable_memory, null_free, decorators);\n+    }\n+\n+    set_result(result);\n+    return true;\n+  }\n+}\n+\n+bool LibraryCallKit::inline_unsafe_make_private_buffer() {\n+  Node* receiver = argument(0);\n+  Node* value = argument(1);\n+\n+  const Type* type = gvn().type(value);\n+  if (!type->is_inlinetypeptr()) {\n+    C->record_method_not_compilable(\"value passed to Unsafe::makePrivateBuffer is not of a constant value type\");\n+    return false;\n+  }\n+\n+  null_check(receiver);\n+  if (stopped()) {\n+    return true;\n+  }\n+\n+  value = null_check(value);\n+  if (stopped()) {\n+    return true;\n+  }\n+\n+  ciInlineKlass* vk = type->inline_klass();\n+  Node* klass = makecon(TypeKlassPtr::make(vk));\n+  Node* obj = new_instance(klass);\n+  AllocateNode::Ideal_allocation(obj)->_larval = true;\n+\n+  assert(value->is_InlineType(), \"must be an InlineTypeNode\");\n+  Node* payload_ptr = basic_plus_adr(obj, vk->payload_offset());\n+  value->as_InlineType()->store_flat(this, obj, payload_ptr, false, true, true, IN_HEAP | MO_UNORDERED);\n+\n+  set_result(obj);\n+  return true;\n+}\n+\n+bool LibraryCallKit::inline_unsafe_finish_private_buffer() {\n+  Node* receiver = argument(0);\n+  Node* buffer = argument(1);\n+\n+  const Type* type = gvn().type(buffer);\n+  if (!type->is_inlinetypeptr()) {\n+    C->record_method_not_compilable(\"value passed to Unsafe::finishPrivateBuffer is not of a constant value type\");\n+    return false;\n+  }\n+\n+  AllocateNode* alloc = AllocateNode::Ideal_allocation(buffer);\n+  if (alloc == nullptr) {\n+    C->record_method_not_compilable(\"value passed to Unsafe::finishPrivateBuffer must be allocated by Unsafe::makePrivateBuffer\");\n+    return false;\n+  }\n+\n+  null_check(receiver);\n+  if (stopped()) {\n+    return true;\n+  }\n+\n+  \/\/ Unset the larval bit in the object header\n+  Node* old_header = make_load(control(), buffer, TypeX_X, TypeX_X->basic_type(), MemNode::unordered, LoadNode::Pinned);\n+  Node* new_header = gvn().transform(new AndXNode(old_header, MakeConX(~markWord::larval_bit_in_place)));\n+  access_store_at(buffer, buffer, type->is_ptr(), new_header, TypeX_X, TypeX_X->basic_type(), MO_UNORDERED | IN_HEAP);\n+\n+  \/\/ We must ensure that the buffer is properly published\n+  insert_mem_bar(Op_MemBarStoreStore, alloc->proj_out(AllocateNode::RawAddress));\n+  assert(!type->maybe_null(), \"result of an allocation should not be null\");\n+  set_result(InlineTypeNode::make_from_oop(this, buffer, type->inline_klass()));\n+  return true;\n+}\n+\n@@ -2794,0 +3171,13 @@\n+    if (oldval != nullptr && oldval->is_InlineType()) {\n+      \/\/ Re-execute the unsafe access if allocation triggers deoptimization.\n+      PreserveReexecuteState preexecs(this);\n+      jvms()->set_should_reexecute(true);\n+      oldval = oldval->as_InlineType()->buffer(this)->get_oop();\n+    }\n+    if (newval != nullptr && newval->is_InlineType()) {\n+      \/\/ Re-execute the unsafe access if allocation triggers deoptimization.\n+      PreserveReexecuteState preexecs(this);\n+      jvms()->set_should_reexecute(true);\n+      newval = newval->as_InlineType()->buffer(this)->get_oop();\n+    }\n+\n@@ -2980,2 +3370,7 @@\n-\n-  Node* obj = new_instance(kls, test);\n+  Node* obj = nullptr;\n+  const TypeInstKlassPtr* tkls = _gvn.type(kls)->isa_instklassptr();\n+  if (tkls != nullptr && tkls->instance_klass()->is_inlinetype()) {\n+    obj = InlineTypeNode::make_all_zero(_gvn, tkls->instance_klass()->as_inline_klass())->buffer(this);\n+  } else {\n+    obj = new_instance(kls, test);\n+  }\n@@ -3762,1 +4157,1 @@\n-  const TypeAry* arr0 = TypeAry::make(etype, TypeInt::POS);\n+  const TypeAry* arr0 = TypeAry::make(etype, TypeInt::POS, \/* stable= *\/ false, \/* flat= *\/ false, \/* not_flat= *\/ true, \/* not_null_free= *\/ true);\n@@ -3767,1 +4162,1 @@\n-  const Type* objects_type = TypeAryPtr::make(TypePtr::BotPTR, arr0, objects_klass, xk, 0);\n+  const Type* objects_type = TypeAryPtr::make(TypePtr::BotPTR, arr0, objects_klass, xk, TypeAryPtr::Offset(0));\n@@ -3891,9 +4286,0 @@\n-\/\/---------------------------load_mirror_from_klass----------------------------\n-\/\/ Given a klass oop, load its java mirror (a java.lang.Class oop).\n-Node* LibraryCallKit::load_mirror_from_klass(Node* klass) {\n-  Node* p = basic_plus_adr(klass, in_bytes(Klass::java_mirror_offset()));\n-  Node* load = make_load(nullptr, p, TypeRawPtr::NOTNULL, T_ADDRESS, MemNode::unordered);\n-  \/\/ mirror = ((OopHandle)mirror)->resolve();\n-  return access_load(load, TypeInstPtr::MIRROR, T_OBJECT, IN_NATIVE);\n-}\n-\n@@ -3943,0 +4329,1 @@\n+\n@@ -4101,0 +4488,1 @@\n+\n@@ -4123,1 +4511,2 @@\n-      int static_res = C->static_subtype_check(TypeKlassPtr::make(tm->as_klass(), Type::trust_interfaces), tp->as_klass_type());\n+      const TypeKlassPtr* tklass = TypeKlassPtr::make(tm->as_klass(), Type::trust_interfaces);\n+      int static_res = C->static_subtype_check(tklass, tp->as_klass_type());\n@@ -4152,2 +4541,2 @@\n-  \/\/ Not-subtype or the mirror's klass ptr is null (in case it is a primitive).\n-  enum { _bad_type_path = 1, _prim_path = 2, PATH_LIMIT };\n+  \/\/ Not-subtype or the mirror's klass ptr is nullptr (in case it is a primitive).\n+  enum { _bad_type_path = 1, _prim_path = 2, _npe_path = 3, PATH_LIMIT };\n@@ -4163,0 +4552,2 @@\n+  Node* io = i_o();\n+  Node* mem = merged_memory();\n@@ -4164,0 +4555,1 @@\n+\n@@ -4170,1 +4562,2 @@\n-      region->in(_bad_type_path) != top()) {\n+      region->in(_bad_type_path) != top() ||\n+      region->in(_npe_path) != top()) {\n@@ -4174,0 +4567,3 @@\n+    \/\/ Set IO and memory because gen_checkcast may override them when buffering inline types\n+    set_i_o(io);\n+    set_all_memory(mem);\n@@ -4207,0 +4603,1 @@\n+  RegionNode* prim_region = new RegionNode(2);\n@@ -4209,0 +4606,1 @@\n+  record_for_igvn(prim_region);\n@@ -4233,2 +4631,5 @@\n-    int prim_path = (which_arg == 0 ? _prim_0_path : _prim_1_path);\n-    region->init_req(prim_path, null_ctl);\n+    if (which_arg == 0) {\n+      prim_region->init_req(1, null_ctl);\n+    } else {\n+      region->init_req(_prim_1_path, null_ctl);\n+    }\n@@ -4244,1 +4645,0 @@\n-    \/\/ now we have a successful reference subtype check\n@@ -4251,1 +4651,2 @@\n-  set_control(region->in(_prim_0_path)); \/\/ go back to first null check\n+  \/\/ This path is also used if superc is a value mirror.\n+  set_control(_gvn.transform(prim_region));\n@@ -4256,1 +4657,1 @@\n-    generate_guard(bol_eq, region, PROB_FAIR);\n+    generate_fair_guard(bol_eq, region);\n@@ -4287,2 +4688,1 @@\n-Node* LibraryCallKit::generate_array_guard_common(Node* kls, RegionNode* region,\n-                                                  bool obj_array, bool not_array, Node** obj) {\n+Node* LibraryCallKit::generate_array_guard_common(Node* kls, RegionNode* region, ArrayKind kind, Node** obj) {\n@@ -4294,9 +4694,0 @@\n-  \/\/ If obj_array\/non_array==false\/false:\n-  \/\/ Branch around if the given klass is in fact an array (either obj or prim).\n-  \/\/ If obj_array\/non_array==false\/true:\n-  \/\/ Branch around if the given klass is not an array klass of any kind.\n-  \/\/ If obj_array\/non_array==true\/true:\n-  \/\/ Branch around if the kls is not an oop array (kls is int[], String, etc.)\n-  \/\/ If obj_array\/non_array==true\/false:\n-  \/\/ Branch around if the kls is an oop array (Object[] or subtype)\n-  \/\/\n@@ -4307,4 +4698,11 @@\n-    bool query = (obj_array\n-                  ? Klass::layout_helper_is_objArray(layout_con)\n-                  : Klass::layout_helper_is_array(layout_con));\n-    if (query == not_array) {\n+    bool query = 0;\n+    switch(kind) {\n+      case RefArray:       query = Klass::layout_helper_is_refArray(layout_con); break;\n+      case NonRefArray:    query = !Klass::layout_helper_is_refArray(layout_con); break;\n+      case TypeArray:      query = Klass::layout_helper_is_typeArray(layout_con); break;\n+      case AnyArray:       query = Klass::layout_helper_is_array(layout_con); break;\n+      case NonArray:       query = !Klass::layout_helper_is_array(layout_con); break;\n+      default:\n+        ShouldNotReachHere();\n+    }\n+    if (!query) {\n@@ -4320,0 +4718,21 @@\n+  unsigned int value = 0;\n+  BoolTest::mask btest = BoolTest::illegal;\n+  switch(kind) {\n+    case RefArray:\n+    case NonRefArray: {\n+      value = Klass::_lh_array_tag_ref_value;\n+      layout_val = _gvn.transform(new RShiftINode(layout_val, intcon(Klass::_lh_array_tag_shift)));\n+      btest = (kind == RefArray) ? BoolTest::eq : BoolTest::ne;\n+      break;\n+    }\n+    case TypeArray: {\n+      value = Klass::_lh_array_tag_type_value;\n+      layout_val = _gvn.transform(new RShiftINode(layout_val, intcon(Klass::_lh_array_tag_shift)));\n+      btest = BoolTest::eq;\n+      break;\n+    }\n+    case AnyArray:    value = Klass::_lh_neutral_value; btest = BoolTest::lt; break;\n+    case NonArray:    value = Klass::_lh_neutral_value; btest = BoolTest::gt; break;\n+    default:\n+      ShouldNotReachHere();\n+  }\n@@ -4321,4 +4740,1 @@\n-  jint  nval = (obj_array\n-                ? (jint)(Klass::_lh_array_tag_type_value\n-                   <<    Klass::_lh_array_tag_shift)\n-                : Klass::_lh_neutral_value);\n+  jint nval = (jint)value;\n@@ -4326,3 +4742,0 @@\n-  BoolTest::mask btest = BoolTest::lt;  \/\/ correct for testing is_[obj]array\n-  \/\/ invert the test if we are looking for a non-array\n-  if (not_array)  btest = BoolTest(btest).negate();\n@@ -4331,1 +4744,1 @@\n-  Node* is_array_ctrl = not_array ? control() : ctrl;\n+  Node* is_array_ctrl = kind == NonArray ? control() : ctrl;\n@@ -4340,0 +4753,116 @@\n+\/\/ public static native Object[] newNullRestrictedAtomicArray(Class<?> componentType, int length, Object initVal);\n+\/\/ public static native Object[] newNullRestrictedNonAtomicArray(Class<?> componentType, int length, Object initVal);\n+\/\/ public static native Object[] newNullableAtomicArray(Class<?> componentType, int length);\n+bool LibraryCallKit::inline_newArray(bool null_free, bool atomic) {\n+  assert(null_free || atomic, \"nullable implies atomic\");\n+  Node* componentType = argument(0);\n+  Node* length = argument(1);\n+  Node* init_val = null_free ? argument(2) : nullptr;\n+\n+  const TypeInstPtr* tp = _gvn.type(componentType)->isa_instptr();\n+  if (tp != nullptr) {\n+    ciInstanceKlass* ik = tp->instance_klass();\n+    if (ik == C->env()->Class_klass()) {\n+      ciType* t = tp->java_mirror_type();\n+      if (t != nullptr && t->is_inlinetype()) {\n+\n+        ciArrayKlass* array_klass = ciArrayKlass::make(t, null_free, atomic, true);\n+        assert(array_klass->is_elem_null_free() == null_free, \"inconsistency\");\n+        assert(array_klass->is_elem_atomic() == atomic, \"inconsistency\");\n+\n+        \/\/ TOOD 8350865 ZGC needs card marks on initializing oop stores\n+        if (UseZGC && null_free && !array_klass->is_flat_array_klass()) {\n+          return false;\n+        }\n+\n+        if (array_klass->is_loaded() && array_klass->element_klass()->as_inline_klass()->is_initialized()) {\n+          const TypeAryKlassPtr* array_klass_type = TypeAryKlassPtr::make(array_klass, Type::trust_interfaces, true);\n+          if (null_free) {\n+            if (init_val->is_InlineType()) {\n+              if (array_klass_type->is_flat() && init_val->as_InlineType()->is_all_zero(&gvn(), \/* flat *\/ true)) {\n+                \/\/ Zeroing is enough because the init value is the all-zero value\n+                init_val = nullptr;\n+              } else {\n+                init_val = init_val->as_InlineType()->buffer(this);\n+              }\n+            }\n+            \/\/ TODO 8350865 Should we add a check of the init_val type (maybe in debug only + halt)?\n+          }\n+          Node* obj = new_array(makecon(array_klass_type), length, 0, nullptr, false, init_val);\n+          const TypeAryPtr* arytype = gvn().type(obj)->is_aryptr();\n+          assert(arytype->is_null_free() == null_free, \"inconsistency\");\n+          assert(arytype->is_not_null_free() == !null_free, \"inconsistency\");\n+          assert(arytype->is_atomic() == atomic, \"inconsistency\");\n+          set_result(obj);\n+          return true;\n+        }\n+      }\n+    }\n+  }\n+  return false;\n+}\n+\n+Node* LibraryCallKit::load_default_array_klass(Node* klass_node) {\n+  \/\/ TODO 8366668\n+  \/\/ - Fred suggested that we could just have the first entry in the refined list point to the array with ArrayKlass::ArrayProperties::DEFAULT property\n+  \/\/   For now, we just load from ObjArrayKlass::_next_refined_array_klass, which would always be the refKlass for non-values, and deopt if it's not\n+  \/\/ - Convert this to an IGVN optimization, so it's also folded after parsing\n+  \/\/ - The generate_typeArray_guard is not needed by all callers, double-check that it's folded\n+\n+  const Type* klass_t = _gvn.type(klass_node);\n+  const TypeAryKlassPtr* ary_klass_t = klass_t->isa_aryklassptr();\n+  if (ary_klass_t && ary_klass_t->klass_is_exact()) {\n+    if (ary_klass_t->exact_klass()->is_obj_array_klass()) {\n+      ary_klass_t = ary_klass_t->get_vm_type(false);\n+      return makecon(ary_klass_t);\n+    } else {\n+      return klass_node;\n+    }\n+  }\n+\n+  \/\/ Load next refined array klass if klass is an ObjArrayKlass\n+  RegionNode* refined_region = new RegionNode(2);\n+  Node* refined_phi = new PhiNode(refined_region, klass_t);\n+\n+  generate_typeArray_guard(klass_node, refined_region);\n+  if (refined_region->req() == 3) {\n+    refined_phi->add_req(klass_node);\n+  }\n+\n+  Node* adr_refined_klass = basic_plus_adr(klass_node, in_bytes(ObjArrayKlass::next_refined_array_klass_offset()));\n+  Node* refined_klass = _gvn.transform(LoadKlassNode::make(_gvn, immutable_memory(), adr_refined_klass, TypeRawPtr::BOTTOM, TypeInstKlassPtr::OBJECT_OR_NULL));\n+\n+  RegionNode* refined_region2 = new RegionNode(3);\n+  Node* refined_phi2 = new PhiNode(refined_region2, klass_t);\n+\n+  Node* null_ctl = top();\n+  Node* null_free_klass = null_check_common(refined_klass, T_OBJECT, false, &null_ctl);\n+  refined_region2->init_req(1, null_ctl);\n+  refined_phi2->init_req(1, klass_node);\n+\n+  refined_region2->init_req(2, control());\n+  refined_phi2->init_req(2, null_free_klass);\n+\n+  set_control(_gvn.transform(refined_region2));\n+  refined_klass = _gvn.transform(refined_phi2);\n+\n+  Node* adr_properties = basic_plus_adr(refined_klass, in_bytes(ObjArrayKlass::properties_offset()));\n+\n+  Node* properties = _gvn.transform(LoadNode::make(_gvn, control(), immutable_memory(), adr_properties, TypeRawPtr::BOTTOM, TypeInt::INT, T_INT, MemNode::unordered));\n+  Node* default_val = makecon(TypeInt::make(ArrayKlass::ArrayProperties::DEFAULT));\n+  Node* chk = _gvn.transform(new CmpINode(properties, default_val));\n+  Node* tst = _gvn.transform(new BoolNode(chk, BoolTest::eq));\n+\n+  { \/\/ Deoptimize if not the default property\n+    BuildCutout unless(this, tst, PROB_MAX);\n+    uncommon_trap_exact(Deoptimization::Reason_class_check, Deoptimization::Action_none);\n+  }\n+\n+  refined_region->init_req(1, control());\n+  refined_phi->init_req(1, refined_klass);\n+\n+  set_control(_gvn.transform(refined_region));\n+  klass_node = _gvn.transform(refined_phi);\n+\n+  return klass_node;\n+}\n@@ -4342,1 +4871,1 @@\n-\/\/ private static native Object java.lang.reflect.newArray(Class<?> componentType, int length);\n+\/\/ private static native Object java.lang.reflect.Array.newArray(Class<?> componentType, int length);\n@@ -4400,0 +4929,3 @@\n+\n+    klass_node = load_default_array_klass(klass_node);\n+\n@@ -4488,1 +5020,17 @@\n-    Node* not_objArray = generate_non_objArray_guard(klass_node, bailout);\n+    \/\/ Inline type array may have object field that would require a\n+    \/\/ write barrier. Conservatively, go to slow path.\n+    \/\/ TODO 8251971: Optimize for the case when flat src\/dst are later found\n+    \/\/ to not contain oops (i.e., move this check to the macro expansion phase).\n+    BarrierSetC2* bs = BarrierSet::barrier_set()->barrier_set_c2();\n+    const TypeAryPtr* orig_t = _gvn.type(original)->isa_aryptr();\n+    const TypeKlassPtr* tklass = _gvn.type(klass_node)->is_klassptr();\n+    bool exclude_flat = UseArrayFlattening && bs->array_copy_requires_gc_barriers(true, T_OBJECT, false, false, BarrierSetC2::Parsing) &&\n+                        \/\/ Can src array be flat and contain oops?\n+                        (orig_t == nullptr || (!orig_t->is_not_flat() && (!orig_t->is_flat() || orig_t->elem()->inline_klass()->contains_oops()))) &&\n+                        \/\/ Can dest array be flat and contain oops?\n+                        tklass->can_be_inline_array() && (!tklass->is_flat() || tklass->is_aryklassptr()->elem()->is_instklassptr()->instance_klass()->as_inline_klass()->contains_oops());\n+    \/\/ TODO 8366668 generate_non_refArray_guard also passed for ref arrays??\n+    Node* not_objArray = exclude_flat ? generate_non_refArray_guard(klass_node, bailout) : generate_typeArray_guard(klass_node, bailout);\n+\n+    klass_node = load_default_array_klass(klass_node);\n+\n@@ -4492,1 +5040,1 @@\n-      const Type* akls = TypeKlassPtr::make(TypePtr::NotNull, ak, 0\/*offset*\/);\n+      const Type* akls = TypeKlassPtr::make(TypePtr::NotNull, ak, Type::Offset(0));\n@@ -4512,0 +5060,39 @@\n+    \/\/ Handle inline type arrays\n+    bool can_validate = !too_many_traps(Deoptimization::Reason_class_check);\n+    if (!stopped()) {\n+      \/\/ TODO JDK-8329224\n+      if (!orig_t->is_null_free()) {\n+        \/\/ Not statically known to be null free, add a check\n+        generate_fair_guard(null_free_array_test(original), bailout);\n+      }\n+      orig_t = _gvn.type(original)->isa_aryptr();\n+      if (orig_t != nullptr && orig_t->is_flat()) {\n+        \/\/ Src is flat, check that dest is flat as well\n+        if (exclude_flat) {\n+          \/\/ Dest can't be flat, bail out\n+          bailout->add_req(control());\n+          set_control(top());\n+        } else {\n+          generate_fair_guard(flat_array_test(klass_node, \/* flat = *\/ false), bailout);\n+        }\n+        \/\/ TODO 8350865 This is not correct anymore. Write tests and fix logic similar to arraycopy.\n+      } else if (UseArrayFlattening && (orig_t == nullptr || !orig_t->is_not_flat()) &&\n+                 \/\/ If dest is flat, src must be flat as well (guaranteed by src <: dest check if validated).\n+                 ((!tklass->is_flat() && tklass->can_be_inline_array()) || !can_validate)) {\n+        \/\/ Src might be flat and dest might not be flat. Go to the slow path if src is flat.\n+        \/\/ TODO 8251971: Optimize for the case when src\/dest are later found to be both flat.\n+        generate_fair_guard(flat_array_test(load_object_klass(original)), bailout);\n+        if (orig_t != nullptr) {\n+          orig_t = orig_t->cast_to_not_flat();\n+          original = _gvn.transform(new CheckCastPPNode(control(), original, orig_t));\n+        }\n+      }\n+      if (!can_validate) {\n+        \/\/ No validation. The subtype check emitted at macro expansion time will not go to the slow\n+        \/\/ path but call checkcast_arraycopy which can not handle flat\/null-free inline type arrays.\n+        \/\/ TODO 8251971: Optimize for the case when src\/dest are later found to be both flat\/null-free.\n+        generate_fair_guard(flat_array_test(klass_node), bailout);\n+        generate_fair_guard(null_free_array_test(original), bailout);\n+      }\n+    }\n+\n@@ -4557,1 +5144,1 @@\n-      if (!too_many_traps(Deoptimization::Reason_class_check)) {\n+      if (can_validate) {\n@@ -4643,1 +5230,1 @@\n-    const TypeTuple* range = tf->range();\n+    const TypeTuple* range = tf->range_cc();\n@@ -4647,1 +5234,1 @@\n-    tf = TypeFunc::make(tf->domain(), new_range);\n+    tf = TypeFunc::make(tf->domain_cc(), new_range);\n@@ -4704,1 +5291,8 @@\n-  Node* obj = nullptr;\n+  Node* obj = argument(0);\n+\n+  \/\/ Don't intrinsify hashcode on inline types for now.\n+  \/\/ The \"is locked\" runtime check below also serves as inline type check and goes to the slow path.\n+  if (gvn().type(obj)->is_inlinetypeptr()) {\n+    return false;\n+  }\n+\n@@ -4714,1 +5308,0 @@\n-    obj = argument(0);\n@@ -4755,1 +5348,2 @@\n-    Node *lock_mask      = _gvn.MakeConX(markWord::lock_mask_in_place);\n+  \/\/ This also serves as guard against inline types\n+    Node *lock_mask      = _gvn.MakeConX(markWord::inline_type_mask_in_place);\n@@ -4830,1 +5424,10 @@\n-  Node* obj = null_check_receiver();\n+  Node* obj = argument(0);\n+  if (obj->is_InlineType()) {\n+    const Type* t = _gvn.type(obj);\n+    if (t->maybe_null()) {\n+      null_check(obj);\n+    }\n+    set_result(makecon(TypeInstPtr::make(t->inline_klass()->java_mirror())));\n+    return true;\n+  }\n+  obj = null_check_receiver();\n@@ -5252,1 +5855,2 @@\n-    Node* obj = null_check_receiver();\n+    Node* obj = argument(0);\n+    obj = null_check_receiver();\n@@ -5256,0 +5860,6 @@\n+    if (obj_type->is_inlinetypeptr()) {\n+      \/\/ If the object to clone is an inline type, we can simply return it (i.e. a nop) since inline types have\n+      \/\/ no identity.\n+      set_result(obj);\n+      return true;\n+    }\n@@ -5262,1 +5872,2 @@\n-        obj_type->speculative_type()->is_instance_klass()) {\n+        obj_type->speculative_type()->is_instance_klass() &&\n+        !obj_type->speculative_type()->is_inlinetype()) {\n@@ -5292,0 +5903,5 @@\n+    \/\/ We only go to the fast case code if we pass a number of guards.\n+    \/\/ The paths which do not pass are accumulated in the slow_region.\n+    RegionNode* slow_region = new RegionNode(1);\n+    record_for_igvn(slow_region);\n+\n@@ -5298,3 +5914,0 @@\n-      Node* obj_length = load_array_length(array_obj);\n-      Node* array_size = nullptr; \/\/ Size of the array without object alignment padding.\n-      Node* alloc_obj = new_array(obj_klass, obj_length, 0, &array_size, \/*deoptimize_on_exception=*\/true);\n@@ -5303,20 +5916,7 @@\n-      if (bs->array_copy_requires_gc_barriers(true, T_OBJECT, true, false, BarrierSetC2::Parsing)) {\n-        \/\/ If it is an oop array, it requires very special treatment,\n-        \/\/ because gc barriers are required when accessing the array.\n-        Node* is_obja = generate_objArray_guard(obj_klass, (RegionNode*)nullptr);\n-        if (is_obja != nullptr) {\n-          PreserveJVMState pjvms2(this);\n-          set_control(is_obja);\n-          \/\/ Generate a direct call to the right arraycopy function(s).\n-          \/\/ Clones are always tightly coupled.\n-          ArrayCopyNode* ac = ArrayCopyNode::make(this, true, array_obj, intcon(0), alloc_obj, intcon(0), obj_length, true, false);\n-          ac->set_clone_oop_array();\n-          Node* n = _gvn.transform(ac);\n-          assert(n == ac, \"cannot disappear\");\n-          ac->connect_outputs(this, \/*deoptimize_on_exception=*\/true);\n-\n-          result_reg->init_req(_objArray_path, control());\n-          result_val->init_req(_objArray_path, alloc_obj);\n-          result_i_o ->set_req(_objArray_path, i_o());\n-          result_mem ->set_req(_objArray_path, reset_memory());\n-        }\n+      const TypeAryPtr* ary_ptr = obj_type->isa_aryptr();\n+      if (UseArrayFlattening && bs->array_copy_requires_gc_barriers(true, T_OBJECT, true, false, BarrierSetC2::Expansion) &&\n+          obj_type->can_be_inline_array() &&\n+          (ary_ptr == nullptr || (!ary_ptr->is_not_flat() && (!ary_ptr->is_flat() || ary_ptr->elem()->inline_klass()->contains_oops())))) {\n+        \/\/ Flat inline type array may have object field that would require a\n+        \/\/ write barrier. Conservatively, go to slow path.\n+        generate_fair_guard(flat_array_test(obj_klass), slow_region);\n@@ -5324,7 +5924,0 @@\n-      \/\/ Otherwise, there are no barriers to worry about.\n-      \/\/ (We can dispense with card marks if we know the allocation\n-      \/\/  comes out of eden (TLAB)...  In fact, ReduceInitialCardMarks\n-      \/\/  causes the non-eden paths to take compensating steps to\n-      \/\/  simulate a fresh allocation, so that no further\n-      \/\/  card marks are required in compiled code to initialize\n-      \/\/  the object.)\n@@ -5333,7 +5926,43 @@\n-        copy_to_clone(array_obj, alloc_obj, array_size, true);\n-\n-        \/\/ Present the results of the copy.\n-        result_reg->init_req(_array_path, control());\n-        result_val->init_req(_array_path, alloc_obj);\n-        result_i_o ->set_req(_array_path, i_o());\n-        result_mem ->set_req(_array_path, reset_memory());\n+        Node* obj_length = load_array_length(array_obj);\n+        Node* array_size = nullptr; \/\/ Size of the array without object alignment padding.\n+        Node* alloc_obj = new_array(obj_klass, obj_length, 0, &array_size, \/*deoptimize_on_exception=*\/true);\n+\n+        BarrierSetC2* bs = BarrierSet::barrier_set()->barrier_set_c2();\n+        if (bs->array_copy_requires_gc_barriers(true, T_OBJECT, true, false, BarrierSetC2::Parsing)) {\n+          \/\/ If it is an oop array, it requires very special treatment,\n+          \/\/ because gc barriers are required when accessing the array.\n+          Node* is_obja = generate_refArray_guard(obj_klass, (RegionNode*)nullptr);\n+          if (is_obja != nullptr) {\n+            PreserveJVMState pjvms2(this);\n+            set_control(is_obja);\n+            \/\/ Generate a direct call to the right arraycopy function(s).\n+            \/\/ Clones are always tightly coupled.\n+            ArrayCopyNode* ac = ArrayCopyNode::make(this, true, array_obj, intcon(0), alloc_obj, intcon(0), obj_length, true, false);\n+            ac->set_clone_oop_array();\n+            Node* n = _gvn.transform(ac);\n+            assert(n == ac, \"cannot disappear\");\n+            ac->connect_outputs(this, \/*deoptimize_on_exception=*\/true);\n+\n+            result_reg->init_req(_objArray_path, control());\n+            result_val->init_req(_objArray_path, alloc_obj);\n+            result_i_o ->set_req(_objArray_path, i_o());\n+            result_mem ->set_req(_objArray_path, reset_memory());\n+          }\n+        }\n+        \/\/ Otherwise, there are no barriers to worry about.\n+        \/\/ (We can dispense with card marks if we know the allocation\n+        \/\/  comes out of eden (TLAB)...  In fact, ReduceInitialCardMarks\n+        \/\/  causes the non-eden paths to take compensating steps to\n+        \/\/  simulate a fresh allocation, so that no further\n+        \/\/  card marks are required in compiled code to initialize\n+        \/\/  the object.)\n+\n+        if (!stopped()) {\n+          copy_to_clone(obj, alloc_obj, array_size, true);\n+\n+          \/\/ Present the results of the copy.\n+          result_reg->init_req(_array_path, control());\n+          result_val->init_req(_array_path, alloc_obj);\n+          result_i_o ->set_req(_array_path, i_o());\n+          result_mem ->set_req(_array_path, reset_memory());\n+        }\n@@ -5343,4 +5972,0 @@\n-    \/\/ We only go to the instance fast case code if we pass a number of guards.\n-    \/\/ The paths which do not pass are accumulated in the slow_region.\n-    RegionNode* slow_region = new RegionNode(1);\n-    record_for_igvn(slow_region);\n@@ -5478,0 +6103,12 @@\n+  int adjustment = 1;\n+  const TypeAryKlassPtr* ary_klass_ptr = alloc->in(AllocateNode::KlassNode)->bottom_type()->is_aryklassptr();\n+  if (ary_klass_ptr->is_null_free()) {\n+    \/\/ A null-free, tightly coupled array allocation can only come from LibraryCallKit::inline_newArray which\n+    \/\/ also requires the componentType and initVal on stack for re-execution.\n+    \/\/ Re-create and push the componentType.\n+    ciArrayKlass* klass = ary_klass_ptr->exact_klass()->as_array_klass();\n+    ciInstance* instance = klass->component_mirror_instance();\n+    const TypeInstPtr* t_instance = TypeInstPtr::make(instance);\n+    sfpt->ins_req(old_jvms->stkoff() + old_jvms->sp(), makecon(t_instance));\n+    adjustment++;\n+  }\n@@ -5479,5 +6116,16 @@\n-  sfpt->ins_req(old_jvms->stkoff() + old_jvms->sp(), alloc->in(AllocateNode::ALength));\n-  old_jvms->set_sp(old_jvms->sp()+1);\n-  old_jvms->set_monoff(old_jvms->monoff()+1);\n-  old_jvms->set_scloff(old_jvms->scloff()+1);\n-  old_jvms->set_endoff(old_jvms->endoff()+1);\n+  sfpt->ins_req(old_jvms->stkoff() + old_jvms->sp() + adjustment - 1, alloc->in(AllocateNode::ALength));\n+  if (ary_klass_ptr->is_null_free()) {\n+    \/\/ Re-create and push the initVal.\n+    Node* init_val = alloc->in(AllocateNode::InitValue);\n+    if (init_val == nullptr) {\n+      init_val = InlineTypeNode::make_all_zero(_gvn, ary_klass_ptr->elem()->is_instklassptr()->instance_klass()->as_inline_klass());\n+    } else if (UseCompressedOops) {\n+      init_val = _gvn.transform(new DecodeNNode(init_val, init_val->bottom_type()->make_ptr()));\n+    }\n+    sfpt->ins_req(old_jvms->stkoff() + old_jvms->sp() + adjustment, init_val);\n+    adjustment++;\n+  }\n+  old_jvms->set_sp(old_jvms->sp() + adjustment);\n+  old_jvms->set_monoff(old_jvms->monoff() + adjustment);\n+  old_jvms->set_scloff(old_jvms->scloff() + adjustment);\n+  old_jvms->set_endoff(old_jvms->endoff() + adjustment);\n@@ -5516,2 +6164,1 @@\n-    CallProjections callprojs;\n-    alloc->extract_projections(&callprojs, true);\n+    CallProjections* callprojs = alloc->extract_projections(true);\n@@ -5520,1 +6167,1 @@\n-    C->gvn_replace_by(callprojs.fallthrough_ioproj, alloc->in(TypeFunc::I_O));\n+    C->gvn_replace_by(callprojs->fallthrough_ioproj, alloc->in(TypeFunc::I_O));\n@@ -5562,1 +6209,1 @@\n-    set_i_o(callprojs.fallthrough_ioproj);\n+    set_i_o(callprojs->fallthrough_ioproj);\n@@ -5900,1 +6547,1 @@\n-    if (src_elem == dest_elem && src_elem == T_OBJECT) {\n+    if (src_elem == dest_elem && top_src->is_flat() == top_dest->is_flat() && src_elem == T_OBJECT) {\n@@ -5927,0 +6574,2 @@\n+          src_type = _gvn.type(src);\n+          top_src = src_type->isa_aryptr();\n@@ -5930,0 +6579,2 @@\n+          dest_type = _gvn.type(dest);\n+          top_dest = dest_type->isa_aryptr();\n@@ -5945,2 +6596,1 @@\n-      can_emit_guards &&\n-      !src->is_top() && !dest->is_top()) {\n+      can_emit_guards && !src->is_top() && !dest->is_top()) {\n@@ -5989,0 +6639,2 @@\n+      slow_region->add_req(not_subtype_ctrl);\n+    }\n@@ -5990,6 +6642,32 @@\n-      if (not_subtype_ctrl != top()) {\n-        PreserveJVMState pjvms(this);\n-        set_control(not_subtype_ctrl);\n-        uncommon_trap(Deoptimization::Reason_intrinsic,\n-                      Deoptimization::Action_make_not_entrant);\n-        assert(stopped(), \"Should be stopped\");\n+    \/\/ TODO 8350865 Fix below logic. Also handle atomicity.\n+    generate_fair_guard(flat_array_test(src), slow_region);\n+    generate_fair_guard(flat_array_test(dest), slow_region);\n+\n+    const TypeKlassPtr* dest_klass_t = _gvn.type(dest_klass)->is_klassptr();\n+    const Type* toop = dest_klass_t->cast_to_exactness(false)->as_instance_type();\n+    src = _gvn.transform(new CheckCastPPNode(control(), src, toop));\n+    src_type = _gvn.type(src);\n+    top_src  = src_type->isa_aryptr();\n+\n+    \/\/ Handle flat inline type arrays (null-free arrays are handled by the subtype check above)\n+    if (!stopped() && UseArrayFlattening) {\n+      \/\/ If dest is flat, src must be flat as well (guaranteed by src <: dest check). Handle flat src here.\n+      assert(top_dest == nullptr || !top_dest->is_flat() || top_src->is_flat(), \"src array must be flat\");\n+      if (top_src != nullptr && top_src->is_flat()) {\n+        \/\/ Src is flat, check that dest is flat as well\n+        if (top_dest != nullptr && !top_dest->is_flat()) {\n+          generate_fair_guard(flat_array_test(dest_klass, \/* flat = *\/ false), slow_region);\n+          \/\/ Since dest is flat and src <: dest, dest must have the same type as src.\n+          top_dest = top_src->cast_to_exactness(false);\n+          assert(top_dest->is_flat(), \"dest must be flat\");\n+          dest = _gvn.transform(new CheckCastPPNode(control(), dest, top_dest));\n+        }\n+      } else if (top_src == nullptr || !top_src->is_not_flat()) {\n+        \/\/ Src might be flat and dest might not be flat. Go to the slow path if src is flat.\n+        \/\/ TODO 8251971: Optimize for the case when src\/dest are later found to be both flat.\n+        assert(top_dest == nullptr || !top_dest->is_flat(), \"dest array must not be flat\");\n+        generate_fair_guard(flat_array_test(src), slow_region);\n+        if (top_src != nullptr) {\n+          top_src = top_src->cast_to_not_flat();\n+          src = _gvn.transform(new CheckCastPPNode(control(), src, top_src));\n+        }\n@@ -5998,0 +6676,1 @@\n+\n@@ -6005,4 +6684,0 @@\n-\n-    const TypeKlassPtr* dest_klass_t = _gvn.type(dest_klass)->is_klassptr();\n-    const Type *toop = dest_klass_t->cast_to_exactness(false)->as_instance_type();\n-    src = _gvn.transform(new CheckCastPPNode(control(), src, toop));\n","filename":"src\/hotspot\/share\/opto\/library_call.cpp","additions":811,"deletions":136,"binary":false,"changes":947,"status":"modified"},{"patch":"@@ -31,0 +31,1 @@\n+#include \"opto\/inlinetypenode.hpp\"\n@@ -108,2 +109,3 @@\n-    if (!stopped() && result() != nullptr) {\n-      if (result()->is_top()) {\n+    Node* res = result();\n+    if (!stopped() && res != nullptr) {\n+      if (res->is_top()) {\n@@ -113,2 +115,9 @@\n-      BasicType bt = result()->bottom_type()->basic_type();\n-      push_node(bt, result());\n+      BasicType bt = res->bottom_type()->basic_type();\n+      if (C->inlining_incrementally() && res->is_InlineType()) {\n+        \/\/ The caller expects an oop when incrementally inlining an intrinsic that returns an\n+        \/\/ inline type. Make sure the call is re-executed if the allocation triggers a deoptimization.\n+        PreserveReexecuteState preexecs(this);\n+        jvms()->set_should_reexecute(true);\n+        res = res->as_InlineType()->buffer(this);\n+      }\n+      push_node(bt, res);\n@@ -145,1 +154,0 @@\n-  Node* load_mirror_from_klass(Node* klass);\n@@ -163,0 +171,2 @@\n+  Node* load_default_array_klass(Node* klass_node);\n+\n@@ -169,0 +179,9 @@\n+\n+  enum ArrayKind {\n+    AnyArray,\n+    NonArray,\n+    RefArray,\n+    NonRefArray,\n+    TypeArray\n+  };\n+\n@@ -170,0 +189,1 @@\n+\n@@ -171,1 +191,1 @@\n-    return generate_array_guard_common(kls, region, false, false, obj);\n+    return generate_array_guard_common(kls, region, AnyArray, obj);\n@@ -174,1 +194,4 @@\n-    return generate_array_guard_common(kls, region, false, true, obj);\n+    return generate_array_guard_common(kls, region, NonArray, obj);\n+  }\n+  Node* generate_refArray_guard(Node* kls, RegionNode* region, Node** obj = nullptr) {\n+    return generate_array_guard_common(kls, region, RefArray, obj);\n@@ -176,2 +199,2 @@\n-  Node* generate_objArray_guard(Node* kls, RegionNode* region, Node** obj = nullptr) {\n-    return generate_array_guard_common(kls, region, true, false, obj);\n+  Node* generate_non_refArray_guard(Node* kls, RegionNode* region, Node** obj = nullptr) {\n+    return generate_array_guard_common(kls, region, NonRefArray, obj);\n@@ -179,2 +202,2 @@\n-  Node* generate_non_objArray_guard(Node* kls, RegionNode* region, Node** obj = nullptr) {\n-    return generate_array_guard_common(kls, region, true, true, obj);\n+  Node* generate_typeArray_guard(Node* kls, RegionNode* region, Node** obj = nullptr) {\n+    return generate_array_guard_common(kls, region, TypeArray, obj);\n@@ -182,2 +205,1 @@\n-  Node* generate_array_guard_common(Node* kls, RegionNode* region,\n-                                    bool obj_array, bool not_array, Node** obj = nullptr);\n+  Node* generate_array_guard_common(Node* kls, RegionNode* region, ArrayKind kind, Node** obj = nullptr);\n@@ -231,1 +253,2 @@\n-  bool inline_unsafe_access(bool is_store, BasicType type, AccessKind kind, bool is_unaligned);\n+  bool inline_unsafe_access(bool is_store, BasicType type, AccessKind kind, bool is_unaligned, bool is_flat = false);\n+  bool inline_unsafe_flat_access(bool is_store, AccessKind kind);\n@@ -235,0 +258,1 @@\n+  bool inline_newArray(bool null_free, bool atomic);\n@@ -238,0 +262,2 @@\n+  bool inline_unsafe_make_private_buffer();\n+  bool inline_unsafe_finish_private_buffer();\n@@ -264,0 +290,1 @@\n+  bool inline_primitive_Class_conversion(vmIntrinsics::ID id);\n","filename":"src\/hotspot\/share\/opto\/library_call.hpp","additions":41,"deletions":14,"binary":false,"changes":55,"status":"modified"},{"patch":"@@ -46,0 +46,1 @@\n+class UnswitchCandidate;\n@@ -88,1 +89,1 @@\n-       };\n+         FlatArrays            = 1<<18};\n@@ -111,0 +112,1 @@\n+  bool is_flat_arrays() const { return _loop_flags & FlatArrays; }\n@@ -124,0 +126,1 @@\n+  void mark_flat_arrays() { _loop_flags |= FlatArrays; }\n@@ -741,0 +744,1 @@\n+  bool no_unswitch_candidate() const;\n@@ -1486,1 +1490,2 @@\n-  IfNode* find_unswitch_candidate(const IdealLoopTree* loop) const;\n+  IfNode* find_unswitch_candidates(const IdealLoopTree* loop, Node_List& flat_array_checks) const;\n+  IfNode* find_unswitch_candidate_from_idoms(const IdealLoopTree* loop) const;\n@@ -1493,1 +1498,1 @@\n-                                   const UnswitchedLoopSelector& unswitched_loop_selector);\n+                                   const UnswitchCandidate& unswitch_candidate, const IfNode* loop_selector);\n@@ -1501,0 +1506,1 @@\n+                                            const UnswitchCandidate& unswitch_candidate,\n@@ -1640,0 +1646,1 @@\n+  void move_flat_array_check_out_of_loop(Node* n);\n@@ -1641,0 +1648,1 @@\n+  bool flat_array_element_type_check(Node *n);\n@@ -1830,0 +1838,2 @@\n+  void collect_flat_array_checks(const IdealLoopTree* loop, Node_List& flat_array_checks) const;\n+\n","filename":"src\/hotspot\/share\/opto\/loopnode.hpp","additions":13,"deletions":3,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -33,0 +33,1 @@\n+#include \"opto\/inlinetypenode.hpp\"\n@@ -65,0 +66,6 @@\n+  \/\/ Inline types should not be split through Phis because they cannot be merged\n+  \/\/ through Phi nodes but each value input needs to be merged individually.\n+  if (n->is_InlineType()) {\n+    return nullptr;\n+  }\n+\n@@ -762,0 +769,4 @@\n+      if (inp->isa_InlineType()) {\n+        \/\/ TODO 8302217 This prevents PhiNode::push_inline_types_through\n+        return nullptr;\n+      }\n@@ -1091,0 +1102,48 @@\n+\/\/ We can't use immutable memory for the flat array check because we are loading the mark word which is\n+\/\/ mutable. Although the bits we are interested in are immutable (we check for markWord::unlocked_value),\n+\/\/ we need to use raw memory to not break anti dependency analysis. Below code will attempt to still move\n+\/\/ flat array checks out of loops, mainly to enable loop unswitching.\n+void PhaseIdealLoop::move_flat_array_check_out_of_loop(Node* n) {\n+  \/\/ Skip checks for more than one array\n+  if (n->req() > 3) {\n+    return;\n+  }\n+  Node* mem = n->in(FlatArrayCheckNode::Memory);\n+  Node* array = n->in(FlatArrayCheckNode::ArrayOrKlass)->uncast();\n+  IdealLoopTree* check_loop = get_loop(get_ctrl(n));\n+  IdealLoopTree* ary_loop = get_loop(get_ctrl(array));\n+\n+  \/\/ Check if array is loop invariant\n+  if (!check_loop->is_member(ary_loop)) {\n+    \/\/ Walk up memory graph from the check until we leave the loop\n+    VectorSet wq;\n+    wq.set(mem->_idx);\n+    while (check_loop->is_member(get_loop(ctrl_or_self(mem)))) {\n+      if (mem->is_Phi()) {\n+        mem = mem->in(1);\n+      } else if (mem->is_MergeMem()) {\n+        mem = mem->as_MergeMem()->memory_at(Compile::AliasIdxRaw);\n+      } else if (mem->is_Proj()) {\n+        mem = mem->in(0);\n+      } else if (mem->is_MemBar() || mem->is_SafePoint()) {\n+        mem = mem->in(TypeFunc::Memory);\n+      } else if (mem->is_Store() || mem->is_LoadStore() || mem->is_ClearArray()) {\n+        mem = mem->in(MemNode::Memory);\n+      } else {\n+#ifdef ASSERT\n+        mem->dump();\n+#endif\n+        ShouldNotReachHere();\n+      }\n+      if (wq.test_set(mem->_idx)) {\n+        return;\n+      }\n+    }\n+    \/\/ Replace memory input and re-compute ctrl to move the check out of the loop\n+    _igvn.replace_input_of(n, 1, mem);\n+    set_ctrl_and_loop(n, get_early_ctrl(n));\n+    Node* bol = n->unique_out();\n+    set_ctrl_and_loop(bol, get_early_ctrl(bol));\n+  }\n+}\n+\n@@ -1103,0 +1162,6 @@\n+\n+  if (n->isa_FlatArrayCheck()) {\n+    move_flat_array_check_out_of_loop(n);\n+    return n;\n+  }\n+\n@@ -1378,0 +1443,98 @@\n+bool PhaseIdealLoop::flat_array_element_type_check(Node *n) {\n+  \/\/ If the CmpP is a subtype check for a value that has just been\n+  \/\/ loaded from an array, the subtype check guarantees the value\n+  \/\/ can't be stored in a flat array and the load of the value\n+  \/\/ happens with a flat array check then: push the type check\n+  \/\/ through the phi of the flat array check. This needs special\n+  \/\/ logic because the subtype check's input is not a phi but a\n+  \/\/ LoadKlass that must first be cloned through the phi.\n+  if (n->Opcode() != Op_CmpP) {\n+    return false;\n+  }\n+\n+  Node* klassptr = n->in(1);\n+  Node* klasscon = n->in(2);\n+\n+  if (klassptr->is_DecodeNarrowPtr()) {\n+    klassptr = klassptr->in(1);\n+  }\n+\n+  if (klassptr->Opcode() != Op_LoadKlass && klassptr->Opcode() != Op_LoadNKlass) {\n+    return false;\n+  }\n+\n+  if (!klasscon->is_Con()) {\n+    return false;\n+  }\n+\n+  Node* addr = klassptr->in(MemNode::Address);\n+\n+  if (!addr->is_AddP()) {\n+    return false;\n+  }\n+\n+  intptr_t offset;\n+  Node* obj = AddPNode::Ideal_base_and_offset(addr, &_igvn, offset);\n+\n+  if (obj == nullptr) {\n+    return false;\n+  }\n+\n+  assert(obj != nullptr && addr->in(AddPNode::Base) == addr->in(AddPNode::Address), \"malformed AddP?\");\n+  if (obj->Opcode() == Op_CastPP) {\n+    obj = obj->in(1);\n+  }\n+\n+  if (!obj->is_Phi()) {\n+    return false;\n+  }\n+\n+  Node* region = obj->in(0);\n+\n+  Node* phi = PhiNode::make_blank(region, n->in(1));\n+  for (uint i = 1; i < region->req(); i++) {\n+    Node* in = obj->in(i);\n+    Node* ctrl = region->in(i);\n+    if (addr->in(AddPNode::Base) != obj) {\n+      Node* cast = addr->in(AddPNode::Base);\n+      assert(cast->Opcode() == Op_CastPP && cast->in(0) != nullptr, \"inconsistent subgraph\");\n+      Node* cast_clone = cast->clone();\n+      cast_clone->set_req(0, ctrl);\n+      cast_clone->set_req(1, in);\n+      register_new_node(cast_clone, ctrl);\n+      const Type* tcast = cast_clone->Value(&_igvn);\n+      _igvn.set_type(cast_clone, tcast);\n+      cast_clone->as_Type()->set_type(tcast);\n+      in = cast_clone;\n+    }\n+    Node* addr_clone = addr->clone();\n+    addr_clone->set_req(AddPNode::Base, in);\n+    addr_clone->set_req(AddPNode::Address, in);\n+    register_new_node(addr_clone, ctrl);\n+    _igvn.set_type(addr_clone, addr_clone->Value(&_igvn));\n+    Node* klassptr_clone = klassptr->clone();\n+    klassptr_clone->set_req(2, addr_clone);\n+    register_new_node(klassptr_clone, ctrl);\n+    _igvn.set_type(klassptr_clone, klassptr_clone->Value(&_igvn));\n+    if (klassptr != n->in(1)) {\n+      Node* decode = n->in(1);\n+      assert(decode->is_DecodeNarrowPtr(), \"inconsistent subgraph\");\n+      Node* decode_clone = decode->clone();\n+      decode_clone->set_req(1, klassptr_clone);\n+      register_new_node(decode_clone, ctrl);\n+      _igvn.set_type(decode_clone, decode_clone->Value(&_igvn));\n+      klassptr_clone = decode_clone;\n+    }\n+    phi->set_req(i, klassptr_clone);\n+  }\n+  register_new_node(phi, region);\n+  Node* orig = n->in(1);\n+  _igvn.replace_input_of(n, 1, phi);\n+  split_if_with_blocks_post(n);\n+  if (n->outcnt() != 0) {\n+    _igvn.replace_input_of(n, 1, orig);\n+    _igvn.remove_dead_node(phi);\n+  }\n+  return true;\n+}\n+\n@@ -1384,0 +1547,4 @@\n+  if (flat_array_element_type_check(n)) {\n+    return;\n+  }\n+\n@@ -1534,0 +1701,5 @@\n+\n+  \/\/ Remove multiple allocations of the same inline type\n+  if (n->is_InlineType()) {\n+    n->as_InlineType()->remove_redundant_allocations(this);\n+  }\n@@ -2031,1 +2203,9 @@\n-  Node *sample_cmp = sample_bool->in(1);\n+  Node* sample_cmp = sample_bool->in(1);\n+  const Type* t = Type::TOP;\n+  const TypePtr* at = nullptr;\n+  if (sample_cmp->is_FlatArrayCheck()) {\n+    \/\/ Left input of a FlatArrayCheckNode is memory, set the (adr) type of the phi accordingly\n+    assert(sample_cmp->in(1)->bottom_type() == Type::MEMORY, \"unexpected input type\");\n+    t = Type::MEMORY;\n+    at = TypeRawPtr::BOTTOM;\n+  }\n@@ -2034,1 +2214,1 @@\n-  PhiNode *phi1 = new PhiNode(phi->in(0), Type::TOP);\n+  PhiNode *phi1 = new PhiNode(phi->in(0), t, at);\n","filename":"src\/hotspot\/share\/opto\/loopopts.cpp","additions":182,"deletions":2,"binary":false,"changes":184,"status":"modified"},{"patch":"@@ -25,0 +25,1 @@\n+#include \"ci\/ciFlatArrayKlass.hpp\"\n@@ -148,1 +149,1 @@\n-inline Node* PhaseMacroExpand::generate_slow_guard(Node** ctrl, Node* test, RegionNode* region) {\n+Node* PhaseMacroExpand::generate_slow_guard(Node** ctrl, Node* test, RegionNode* region) {\n@@ -152,0 +153,4 @@\n+inline Node* PhaseMacroExpand::generate_fair_guard(Node** ctrl, Node* test, RegionNode* region) {\n+  return generate_guard(ctrl, test, region, PROB_FAIR);\n+}\n+\n@@ -292,0 +297,43 @@\n+Node* PhaseMacroExpand::mark_word_test(Node** ctrl, Node* obj, MergeMemNode* mem, uintptr_t mask_val, RegionNode* region) {\n+  \/\/ Load markword and check if obj is locked\n+  Node* mark = make_load(nullptr, mem->memory_at(Compile::AliasIdxRaw), obj, oopDesc::mark_offset_in_bytes(), TypeX_X, TypeX_X->basic_type());\n+  Node* locked_bit = MakeConX(markWord::unlocked_value);\n+  locked_bit = transform_later(new AndXNode(locked_bit, mark));\n+  Node* cmp = transform_later(new CmpXNode(locked_bit, MakeConX(0)));\n+  Node* is_unlocked = transform_later(new BoolNode(cmp, BoolTest::ne));\n+  IfNode* iff = transform_later(new IfNode(*ctrl, is_unlocked, PROB_MAX, COUNT_UNKNOWN))->as_If();\n+  Node* locked_region = transform_later(new RegionNode(3));\n+  Node* mark_phi = transform_later(new PhiNode(locked_region, TypeX_X));\n+\n+  \/\/ Unlocked: Use bits from mark word\n+  locked_region->init_req(1, transform_later(new IfTrueNode(iff)));\n+  mark_phi->init_req(1, mark);\n+\n+  \/\/ Locked: Load prototype header from klass\n+  *ctrl = transform_later(new IfFalseNode(iff));\n+  \/\/ Make loads control dependent to make sure they are only executed if array is locked\n+  Node* klass_adr = basic_plus_adr(obj, oopDesc::klass_offset_in_bytes());\n+  Node* klass = transform_later(LoadKlassNode::make(_igvn, C->immutable_memory(), klass_adr, TypeInstPtr::KLASS, TypeInstKlassPtr::OBJECT));\n+  Node* proto_adr = basic_plus_adr(klass, in_bytes(Klass::prototype_header_offset()));\n+  Node* proto = transform_later(LoadNode::make(_igvn, *ctrl, C->immutable_memory(), proto_adr, proto_adr->bottom_type()->is_ptr(), TypeX_X, TypeX_X->basic_type(), MemNode::unordered));\n+\n+  locked_region->init_req(2, *ctrl);\n+  mark_phi->init_req(2, proto);\n+  *ctrl = locked_region;\n+\n+  \/\/ Now check if mark word bits are set\n+  Node* mask = MakeConX(mask_val);\n+  Node* masked = transform_later(new AndXNode(mark_phi, mask));\n+  cmp = transform_later(new CmpXNode(masked, mask));\n+  Node* bol = transform_later(new BoolNode(cmp, BoolTest::eq));\n+  return generate_fair_guard(ctrl, bol, region);\n+}\n+\n+Node* PhaseMacroExpand::generate_flat_array_guard(Node** ctrl, Node* array, MergeMemNode* mem, RegionNode* region) {\n+  return mark_word_test(ctrl, array, mem, markWord::flat_array_bit_in_place, region);\n+}\n+\n+Node* PhaseMacroExpand::generate_null_free_array_guard(Node** ctrl, Node* array, MergeMemNode* mem, RegionNode* region) {\n+  return mark_word_test(ctrl, array, mem, markWord::null_free_array_bit_in_place, region);\n+}\n+\n@@ -386,0 +434,1 @@\n+                                           Node* dest_length,\n@@ -397,0 +446,2 @@\n+  Node* init_value = nullptr;\n+  Node* raw_init_value = nullptr;\n@@ -427,0 +478,2 @@\n+      init_value = alloc->in(AllocateNode::InitValue);\n+      raw_init_value = alloc->in(AllocateNode::RawInitValue);\n@@ -496,1 +549,0 @@\n-      Node* dest_length = alloc->in(AllocateNode::ALength);\n@@ -503,1 +555,3 @@\n-                             adr_type, dest, basic_elem_type,\n+                             adr_type, dest,\n+                             init_value, raw_init_value,\n+                             basic_elem_type,\n@@ -534,1 +588,0 @@\n-    Node* dest_length = alloc->in(AllocateNode::ALength);\n@@ -540,1 +593,3 @@\n-                           adr_type, dest, basic_elem_type,\n+                           adr_type, dest,\n+                           init_value, raw_init_value,\n+                           basic_elem_type,\n@@ -589,1 +644,3 @@\n-                             adr_type, dest, basic_elem_type,\n+                             adr_type, dest,\n+                             init_value, raw_init_value,\n+                             basic_elem_type,\n@@ -599,1 +656,3 @@\n-                             adr_type, dest, basic_elem_type,\n+                             adr_type, dest,\n+                             init_value, raw_init_value,\n+                             basic_elem_type,\n@@ -776,1 +835,3 @@\n-                           adr_type, dest, basic_elem_type,\n+                           adr_type, dest,\n+                           init_value, raw_init_value,\n+                           basic_elem_type,\n@@ -845,3 +906,3 @@\n-  _igvn.replace_node(_callprojs.fallthrough_memproj, out_mem);\n-  if (_callprojs.fallthrough_ioproj != nullptr) {\n-    _igvn.replace_node(_callprojs.fallthrough_ioproj, *io);\n+  _igvn.replace_node(_callprojs->fallthrough_memproj, out_mem);\n+  if (_callprojs->fallthrough_ioproj != nullptr) {\n+    _igvn.replace_node(_callprojs->fallthrough_ioproj, *io);\n@@ -849,1 +910,1 @@\n-  _igvn.replace_node(_callprojs.fallthrough_catchproj, *ctrl);\n+  _igvn.replace_node(_callprojs->fallthrough_catchproj, *ctrl);\n@@ -889,0 +950,2 @@\n+                                            Node* val,\n+                                            Node* raw_val,\n@@ -927,1 +990,1 @@\n-    mem = ClearArrayNode::clear_memory(ctrl, mem, dest,\n+    mem = ClearArrayNode::clear_memory(ctrl, mem, dest, val, raw_val,\n@@ -932,1 +995,1 @@\n-    mem = ClearArrayNode::clear_memory(ctrl, mem, dest,\n+    mem = ClearArrayNode::clear_memory(ctrl, mem, dest, val, raw_val,\n@@ -945,1 +1008,1 @@\n-    mem = ClearArrayNode::clear_memory(ctrl, mem, dest,\n+    mem = ClearArrayNode::clear_memory(ctrl, mem, dest, val, raw_val,\n@@ -974,1 +1037,7 @@\n-        mem = StoreNode::make(_igvn, ctrl, mem, p1, adr_type, intcon(0), T_INT, MemNode::unordered);\n+        if (val == nullptr) {\n+          assert(raw_val == nullptr, \"val may not be null\");\n+          mem = StoreNode::make(_igvn, ctrl, mem, p1, adr_type, intcon(0), T_INT, MemNode::unordered);\n+        } else {\n+          assert(_igvn.type(val)->isa_narrowoop(), \"should be narrow oop\");\n+          mem = new StoreNNode(ctrl, mem, p1, adr_type, val, MemNode::unordered);\n+        }\n@@ -979,1 +1048,1 @@\n-    mem = ClearArrayNode::clear_memory(ctrl, mem, dest,\n+    mem = ClearArrayNode::clear_memory(ctrl, mem, dest, raw_val,\n@@ -1095,2 +1164,2 @@\n-  call->extract_projections(&_callprojs, false \/*separate_io_proj*\/, false \/*do_asserts*\/);\n-  *ctrl = _callprojs.fallthrough_catchproj->clone();\n+  _callprojs = call->extract_projections(false \/*separate_io_proj*\/, false \/*do_asserts*\/);\n+  *ctrl = _callprojs->fallthrough_catchproj->clone();\n@@ -1099,1 +1168,1 @@\n-  Node* m = _callprojs.fallthrough_memproj->clone();\n+  Node* m = _callprojs->fallthrough_memproj->clone();\n@@ -1113,3 +1182,3 @@\n-  \/\/ could be null. Skip clone and update null fallthrough_ioproj.\n-  if (_callprojs.fallthrough_ioproj != nullptr) {\n-    *io = _callprojs.fallthrough_ioproj->clone();\n+  \/\/ could be nullptr. Skip clone and update nullptr fallthrough_ioproj.\n+  if (_callprojs->fallthrough_ioproj != nullptr) {\n+    *io = _callprojs->fallthrough_ioproj->clone();\n@@ -1247,0 +1316,36 @@\n+const TypePtr* PhaseMacroExpand::adjust_for_flat_array(const TypeAryPtr* top_dest, Node*& src_offset,\n+                                                       Node*& dest_offset, Node*& length, BasicType& dest_elem,\n+                                                       Node*& dest_length) {\n+#ifdef ASSERT\n+  BarrierSetC2* bs = BarrierSet::barrier_set()->barrier_set_c2();\n+  bool needs_barriers = top_dest->elem()->inline_klass()->contains_oops() &&\n+    bs->array_copy_requires_gc_barriers(dest_length != nullptr, T_OBJECT, false, false, BarrierSetC2::Optimization);\n+  assert(!needs_barriers || StressReflectiveCode, \"Flat arracopy would require GC barriers\");\n+#endif\n+  int elem_size = top_dest->flat_elem_size();\n+  if (elem_size >= 8) {\n+    if (elem_size > 8) {\n+      \/\/ treat as array of long but scale length, src offset and dest offset\n+      assert((elem_size % 8) == 0, \"not a power of 2?\");\n+      int factor = elem_size \/ 8;\n+      length = transform_later(new MulINode(length, intcon(factor)));\n+      src_offset = transform_later(new MulINode(src_offset, intcon(factor)));\n+      dest_offset = transform_later(new MulINode(dest_offset, intcon(factor)));\n+      if (dest_length != nullptr) {\n+        dest_length = transform_later(new MulINode(dest_length, intcon(factor)));\n+      }\n+      elem_size = 8;\n+    }\n+    dest_elem = T_LONG;\n+  } else if (elem_size == 4) {\n+    dest_elem = T_INT;\n+  } else if (elem_size == 2) {\n+    dest_elem = T_CHAR;\n+  } else if (elem_size == 1) {\n+    dest_elem = T_BYTE;\n+  } else {\n+    ShouldNotReachHere();\n+  }\n+  return TypeRawPtr::BOTTOM;\n+}\n+\n@@ -1264,3 +1369,16 @@\n-    Node* mem = ac->in(TypeFunc::Memory);\n-    merge_mem = MergeMemNode::make(mem);\n-    transform_later(merge_mem);\n+    const Type* src_type = _igvn.type(src);\n+    const Type* dest_type = _igvn.type(dest);\n+    const TypeAryPtr* top_src = src_type->isa_aryptr();\n+    \/\/ Note: The destination could have type Object (i.e. non-array) when directly invoking the protected method\n+    \/\/       Object::clone() with reflection on a declared Object that is an array at runtime. top_dest is then null.\n+    const TypeAryPtr* top_dest = dest_type->isa_aryptr();\n+    BasicType dest_elem = T_OBJECT;\n+    if (top_dest != nullptr && top_dest->elem() != Type::BOTTOM) {\n+      dest_elem = top_dest->elem()->array_element_basic_type();\n+    }\n+    if (is_reference_type(dest_elem, true)) dest_elem = T_OBJECT;\n+\n+    if (top_src != nullptr && top_src->is_flat()) {\n+      \/\/ If src is flat, dest is guaranteed to be flat as well\n+      top_dest = top_src;\n+    }\n@@ -1269,0 +1387,1 @@\n+    Node* dest_length = nullptr;\n@@ -1272,0 +1391,1 @@\n+      dest_length = alloc->in(AllocateNode::ALength);\n@@ -1274,3 +1394,20 @@\n-    const TypePtr* adr_type = _igvn.type(dest)->is_oopptr()->add_offset(Type::OffsetBot);\n-    if (ac->_dest_type != TypeOopPtr::BOTTOM) {\n-      adr_type = ac->_dest_type->add_offset(Type::OffsetBot)->is_ptr();\n+    Node* mem = ac->in(TypeFunc::Memory);\n+    const TypePtr* adr_type = nullptr;\n+    if (top_dest != nullptr && top_dest->is_flat()) {\n+      assert(dest_length != nullptr || StressReflectiveCode, \"must be tightly coupled\");\n+      \/\/ Copy to a flat array modifies multiple memory slices. Conservatively insert a barrier\n+      \/\/ on all slices to prevent writes into the source from floating below the arraycopy.\n+      int mem_bar_alias_idx = Compile::AliasIdxBot;\n+      if (ac->_dest_type != TypeOopPtr::BOTTOM) {\n+        mem_bar_alias_idx = C->get_alias_index(ac->_dest_type->add_offset(Type::OffsetBot)->is_ptr());\n+      }\n+      insert_mem_bar(&ctrl, &mem, Op_MemBarCPUOrder, mem_bar_alias_idx);\n+      adr_type = adjust_for_flat_array(top_dest, src_offset, dest_offset, length, dest_elem, dest_length);\n+    } else {\n+      adr_type = dest_type->is_oopptr()->add_offset(Type::OffsetBot);\n+      if (ac->_dest_type != TypeOopPtr::BOTTOM) {\n+        adr_type = ac->_dest_type->add_offset(Type::OffsetBot)->is_ptr();\n+      }\n+      if (ac->_src_type != ac->_dest_type) {\n+        adr_type = TypeRawPtr::BOTTOM;\n+      }\n@@ -1278,0 +1415,3 @@\n+    merge_mem = MergeMemNode::make(mem);\n+    transform_later(merge_mem);\n+\n@@ -1279,1 +1419,1 @@\n-                       adr_type, T_OBJECT,\n+                       adr_type, dest_elem,\n@@ -1281,0 +1421,1 @@\n+                       dest_length,\n@@ -1316,3 +1457,1 @@\n-  if (ac->is_arraycopy_validated() &&\n-      dest_elem != T_CONFLICT &&\n-      src_elem == T_CONFLICT) {\n+  if (ac->is_arraycopy_validated() && dest_elem != T_CONFLICT && src_elem == T_CONFLICT) {\n@@ -1334,6 +1473,7 @@\n-    Node* mem = generate_arraycopy(ac, nullptr, &ctrl, merge_mem, &io,\n-                                   TypeRawPtr::BOTTOM, T_CONFLICT,\n-                                   src, src_offset, dest, dest_offset, length,\n-                                   \/\/ If a  negative length guard was generated for the ArrayCopyNode,\n-                                   \/\/ the length of the array can never be negative.\n-                                   false, ac->has_negative_length_guard());\n+    generate_arraycopy(ac, nullptr, &ctrl, merge_mem, &io,\n+                       TypeRawPtr::BOTTOM, T_CONFLICT,\n+                       src, src_offset, dest, dest_offset, length,\n+                       nullptr,\n+                       \/\/ If a  negative length guard was generated for the ArrayCopyNode,\n+                       \/\/ the length of the array can never be negative.\n+                       false, ac->has_negative_length_guard());\n@@ -1347,1 +1487,8 @@\n-  if (src_elem != dest_elem || dest_elem == T_VOID) {\n+  \/\/\n+  \/\/ We have no stub to copy flat inline type arrays with oop\n+  \/\/ fields if we need to emit write barriers.\n+  \/\/\n+  BarrierSetC2* bs = BarrierSet::barrier_set()->barrier_set_c2();\n+  if (src_elem != dest_elem || top_src->is_flat() != top_dest->is_flat() || dest_elem == T_VOID ||\n+      (top_src->is_flat() && top_dest->elem()->inline_klass()->contains_oops() &&\n+       bs->array_copy_requires_gc_barriers(alloc != nullptr, T_OBJECT, false, false, BarrierSetC2::Optimization))) {\n@@ -1355,3 +1502,3 @@\n-    _igvn.replace_node(_callprojs.fallthrough_memproj, merge_mem);\n-    if (_callprojs.fallthrough_ioproj != nullptr) {\n-      _igvn.replace_node(_callprojs.fallthrough_ioproj, io);\n+    _igvn.replace_node(_callprojs->fallthrough_memproj, merge_mem);\n+    if (_callprojs->fallthrough_ioproj != nullptr) {\n+      _igvn.replace_node(_callprojs->fallthrough_ioproj, io);\n@@ -1359,1 +1506,1 @@\n-    _igvn.replace_node(_callprojs.fallthrough_catchproj, ctrl);\n+    _igvn.replace_node(_callprojs->fallthrough_catchproj, ctrl);\n@@ -1376,4 +1523,9 @@\n-  {\n-    Node* mem = ac->in(TypeFunc::Memory);\n-    merge_mem = MergeMemNode::make(mem);\n-    transform_later(merge_mem);\n+  Node* mem = ac->in(TypeFunc::Memory);\n+  if (top_dest->is_flat()) {\n+    \/\/ Copy to a flat array modifies multiple memory slices. Conservatively insert a barrier\n+    \/\/ on all slices to prevent writes into the source from floating below the arraycopy.\n+    int mem_bar_alias_idx = Compile::AliasIdxBot;\n+    if (ac->_dest_type != TypeOopPtr::BOTTOM) {\n+      mem_bar_alias_idx = C->get_alias_index(ac->_dest_type->add_offset(Type::OffsetBot)->is_ptr());\n+    }\n+    insert_mem_bar(&ctrl, &mem, Op_MemBarCPUOrder, mem_bar_alias_idx);\n@@ -1381,0 +1533,2 @@\n+  merge_mem = MergeMemNode::make(mem);\n+  transform_later(merge_mem);\n@@ -1421,0 +1575,22 @@\n+\n+    \/\/ TODO 8350865 Fix below logic. Also handle atomicity.\n+    \/\/ We need to be careful here because 'adjust_for_flat_array' will adjust offsets\/length etc. which then does not work anymore for the slow call to SharedRuntime::slow_arraycopy_C.\n+    if (!(top_src->is_flat() && top_dest->is_flat())) {\n+      generate_flat_array_guard(&ctrl, src, merge_mem, slow_region);\n+      generate_flat_array_guard(&ctrl, dest, merge_mem, slow_region);\n+    }\n+\n+    \/\/ Handle inline type arrays\n+    if (!top_src->is_flat()) {\n+      if (UseArrayFlattening && !top_src->is_not_flat()) {\n+        \/\/ Src might be flat and dest might not be flat. Go to the slow path if src is flat.\n+        generate_flat_array_guard(&ctrl, src, merge_mem, slow_region);\n+      }\n+      if (EnableValhalla) {\n+        \/\/ No validation. The subtype check emitted at macro expansion time will not go to the slow\n+        \/\/ path but call checkcast_arraycopy which can not handle flat\/null-free inline type arrays.\n+        generate_null_free_array_guard(&ctrl, dest, merge_mem, slow_region);\n+      }\n+    } else {\n+      assert(top_dest->is_flat(), \"dest array must be flat\");\n+    }\n@@ -1422,0 +1598,1 @@\n+\n@@ -1424,1 +1601,5 @@\n-  if (ac->_dest_type != TypeOopPtr::BOTTOM) {\n+  Node* dest_length = (alloc != nullptr) ? alloc->in(AllocateNode::ALength) : nullptr;\n+\n+  if (top_src->is_flat() && top_dest->is_flat()) {\n+    adr_type = adjust_for_flat_array(top_dest, src_offset, dest_offset, length, dest_elem, dest_length);\n+  } else if (ac->_dest_type != TypeOopPtr::BOTTOM) {\n@@ -1433,0 +1614,1 @@\n+                     dest_length,\n@@ -1435,1 +1617,2 @@\n-                     false, ac->has_negative_length_guard(), slow_region);\n+                     false, ac->has_negative_length_guard(),\n+                     slow_region);\n","filename":"src\/hotspot\/share\/opto\/macroArrayCopy.cpp","additions":233,"deletions":50,"binary":false,"changes":283,"status":"modified"},{"patch":"@@ -26,0 +26,1 @@\n+#include \"ci\/ciFlatArrayKlass.hpp\"\n@@ -27,0 +28,1 @@\n+#include \"classfile\/systemDictionary.hpp\"\n@@ -40,0 +42,1 @@\n+#include \"opto\/inlinetypenode.hpp\"\n@@ -143,2 +146,70 @@\n-Node *MemNode::optimize_simple_memory_chain(Node *mchain, const TypeOopPtr *t_oop, Node *load, PhaseGVN *phase) {\n-  assert((t_oop != nullptr), \"sanity\");\n+\/\/ Find the memory output corresponding to the fall-through path of a call\n+static Node* find_call_fallthrough_mem_output(CallNode* call) {\n+  ResourceMark rm;\n+  CallProjections* projs = call->extract_projections(false, false);\n+  Node* res = projs->fallthrough_memproj;\n+  assert(res != nullptr, \"must have a fallthrough mem output\");\n+  return res;\n+}\n+\n+\/\/ Try to find a better memory input for a load from a strict final field\n+static Node* try_optimize_strict_final_load_memory(PhaseGVN* phase, Node* adr, ProjNode*& base_local) {\n+  intptr_t offset = 0;\n+  Node* base = AddPNode::Ideal_base_and_offset(adr, phase, offset);\n+  if (base == nullptr) {\n+    return nullptr;\n+  }\n+\n+  Node* base_uncasted = base->uncast();\n+  if (base_uncasted->is_Proj()) {\n+    MultiNode* multi = base_uncasted->in(0)->as_Multi();\n+    if (multi->is_Allocate()) {\n+      base_local = base_uncasted->as_Proj();\n+      return nullptr;\n+    } else if (multi->is_Call()) {\n+      \/\/ The oop is returned from a call, the memory can be the fallthrough output of the call\n+      return find_call_fallthrough_mem_output(multi->as_Call());\n+    } else if (multi->is_Start()) {\n+      \/\/ The oop is a parameter\n+      if (phase->C->method()->is_object_constructor() && base_uncasted->as_Proj()->_con == TypeFunc::Parms) {\n+        \/\/ The receiver of a constructor is similar to the result of an AllocateNode\n+        base_local = base_uncasted->as_Proj();\n+        return nullptr;\n+      } else {\n+        \/\/ Use the start memory otherwise\n+        return multi->proj_out(TypeFunc::Memory);\n+      }\n+    }\n+  }\n+\n+  return nullptr;\n+}\n+\n+\/\/ Whether a call can modify a strict final field, given that the object is allocated inside the\n+\/\/ current compilation unit, or is the first parameter when the compilation root is a constructor.\n+\/\/ This is equivalent to asking whether 'call' is a constructor invocation and the class declaring\n+\/\/ the target method is a subclass of the class declaring 'field'.\n+static bool call_can_modify_local_object(ciField* field, CallNode* call) {\n+  if (!call->is_CallJava()) {\n+    return false;\n+  }\n+\n+  ciMethod* target = call->as_CallJava()->method();\n+  if (target == nullptr || !target->is_object_constructor()) {\n+    return false;\n+  }\n+\n+  \/\/ If 'field' is declared in a class that is a subclass of the one declaring the constructor,\n+  \/\/ then the field is set inside the constructor, else the field must be set before the\n+  \/\/ constructor invocation. E.g. A field Super.x will be set during the execution of Sub::<init>,\n+  \/\/ while a field Sub.y must be set before Super::<init> is invoked.\n+  \/\/ We can try to be more heroic and decide if the receiver of the constructor invocation is the\n+  \/\/ object from which we are loading from. This, however, may be problematic as deciding if 2\n+  \/\/ nodes are definitely different may not be trivial, especially if the graph is not canonical.\n+  \/\/ As a result, it is made more conservative for now.\n+  assert(call->req() > TypeFunc::Parms, \"constructor must have at least 1 argument\");\n+  return target->holder()->is_subclass_of(field->holder());\n+}\n+\n+Node* MemNode::optimize_simple_memory_chain(Node* mchain, const TypeOopPtr* t_oop, Node* load, PhaseGVN* phase) {\n+  assert(t_oop != nullptr, \"sanity\");\n@@ -146,5 +217,38 @@\n-  bool is_boxed_value_load = t_oop->is_ptr_to_boxed_value() &&\n-                             (load != nullptr) && load->is_Load() &&\n-                             (phase->is_IterGVN() != nullptr);\n-  if (!(is_instance || is_boxed_value_load))\n-    return mchain;  \/\/ don't try to optimize non-instance types\n+\n+  ciField* field = phase->C->alias_type(t_oop)->field();\n+  bool is_strict_final_load = false;\n+\n+  \/\/ After macro expansion, an allocation may become a call, changing the memory input to the\n+  \/\/ memory output of that call would be illegal. As a result, disallow this transformation after\n+  \/\/ macro expansion.\n+  if (phase->is_IterGVN() && phase->C->allow_macro_nodes() && load != nullptr && load->is_Load() && !load->as_Load()->is_mismatched_access()) {\n+    if (EnableValhalla) {\n+      if (field != nullptr && (field->holder()->is_inlinetype() || field->holder()->is_abstract_value_klass())) {\n+        is_strict_final_load = true;\n+      }\n+#ifdef ASSERT\n+      if (t_oop->is_inlinetypeptr() && t_oop->inline_klass()->contains_field_offset(t_oop->offset())) {\n+        assert(is_strict_final_load, \"sanity check for basic cases\");\n+      }\n+#endif\n+    } else {\n+      is_strict_final_load = field != nullptr && t_oop->is_ptr_to_boxed_value();\n+    }\n+  }\n+\n+  if (!is_instance && !is_strict_final_load) {\n+    return mchain;\n+  }\n+\n+  Node* result = mchain;\n+  ProjNode* base_local = nullptr;\n+\n+  if (is_strict_final_load) {\n+    Node* adr = load->in(MemNode::Address);\n+    assert(phase->type(adr) == t_oop, \"inconsistent type\");\n+    Node* tmp = try_optimize_strict_final_load_memory(phase, adr, base_local);\n+    if (tmp != nullptr) {\n+      result = tmp;\n+    }\n+  }\n+\n@@ -152,3 +256,2 @@\n-  Node *start_mem = phase->C->start()->proj_out_or_null(TypeFunc::Memory);\n-  Node *prev = nullptr;\n-  Node *result = mchain;\n+  Node* start_mem = phase->C->start()->proj_out_or_null(TypeFunc::Memory);\n+  Node* prev = nullptr;\n@@ -157,2 +260,5 @@\n-    if (result == start_mem)\n-      break;  \/\/ hit one of our sentinels\n+    if (result == start_mem) {\n+      \/\/ start_mem is the earliest memory possible\n+      break;\n+    }\n+\n@@ -161,1 +267,1 @@\n-      Node *proj_in = result->in(0);\n+      Node* proj_in = result->in(0);\n@@ -163,1 +269,2 @@\n-        break;  \/\/ hit one of our sentinels\n+        \/\/ This is the allocation that creates the object from which we are loading from\n+        break;\n@@ -166,2 +273,4 @@\n-        CallNode *call = proj_in->as_Call();\n-        if (!call->may_modify(t_oop, phase)) { \/\/ returns false for instances\n+        CallNode* call = proj_in->as_Call();\n+        if (!call->may_modify(t_oop, phase)) {\n+          result = call->in(TypeFunc::Memory);\n+        } else if (is_strict_final_load && base_local != nullptr && !call_can_modify_local_object(field, call)) {\n@@ -179,1 +288,1 @@\n-        } else if (is_boxed_value_load) {\n+        } else if (is_strict_final_load) {\n@@ -183,1 +292,5 @@\n-            result = proj_in->in(TypeFunc::Memory); \/\/ not related allocation\n+            \/\/ Allocation of another type, must be another object\n+            result = proj_in->in(TypeFunc::Memory);\n+          } else if (base_local != nullptr && (base_local->is_Parm() || base_local->in(0) != alloc)) {\n+            \/\/ Allocation of another object\n+            result = proj_in->in(TypeFunc::Memory);\n@@ -236,0 +349,2 @@\n+                     ->cast_to_not_flat(t_oop->is_aryptr()->is_not_flat())\n+                     ->cast_to_not_null_free(t_oop->is_aryptr()->is_not_null_free())\n@@ -262,1 +377,1 @@\n-               tp->isa_aryptr() &&        tp->offset() == Type::OffsetBot &&\n+        tp->isa_aryptr() &&        tp->offset() == Type::OffsetBot &&\n@@ -1022,1 +1137,1 @@\n-    return (eliminate_boxing && non_volatile) || is_stable_ary;\n+    return (eliminate_boxing && non_volatile) || is_stable_ary || tp->is_inlinetypeptr();\n@@ -1079,1 +1194,1 @@\n-      uint shift  = exact_log2(type2aelembytes(ary_elem));\n+      uint shift  = ary_t->is_flat() ? ary_t->flat_log_elem_size() : exact_log2(type2aelembytes(ary_elem));\n@@ -1103,0 +1218,10 @@\n+static Node* see_through_inline_type(PhaseValues* phase, const MemNode* load, Node* base, int offset) {\n+  if (!load->is_mismatched_access() && base != nullptr && base->is_InlineType() && offset > oopDesc::klass_offset_in_bytes()) {\n+    InlineTypeNode* vt = base->as_InlineType();\n+    Node* value = vt->field_value_by_offset(offset, true);\n+    assert(value != nullptr, \"must see some value\");\n+    return value;\n+  }\n+\n+  return nullptr;\n+}\n@@ -1115,0 +1240,9 @@\n+  \/\/ Try to see through an InlineTypeNode\n+  \/\/ LoadN is special because the input is not compressed\n+  if (Opcode() != Op_LoadN) {\n+    Node* value = see_through_inline_type(phase, this, ld_base, ld_off);\n+    if (value != nullptr) {\n+      return value;\n+    }\n+  }\n+\n@@ -1198,1 +1332,1 @@\n-        const TypeVect* out_vt = as_LoadVector()->vect_type();\n+        const TypeVect* out_vt = is_Load() ? as_LoadVector()->vect_type() : as_StoreVector()->vect_type();\n@@ -1216,0 +1350,7 @@\n+      Node* init_value = ld_alloc->in(AllocateNode::InitValue);\n+      if (init_value != nullptr) {\n+        \/\/ TODO 8350865 Scalar replacement does not work well for flat arrays.\n+        \/\/ Is this correct for non-all-zero init values? Don't we need field_value_by_offset?\n+        return init_value;\n+      }\n+      assert(ld_alloc->in(AllocateNode::RawInitValue) == nullptr, \"init value may not be null\");\n@@ -1876,0 +2017,1 @@\n+        && !(phase->type(address)->is_inlinetypeptr() && is_mismatched_access())\n@@ -1972,1 +2114,8 @@\n-  return progress ? this : nullptr;\n+  if (progress) {\n+    return this;\n+  }\n+\n+  if (!can_reshape) {\n+    phase->record_for_igvn(this);\n+  }\n+  return nullptr;\n@@ -2074,0 +2223,1 @@\n+        && !ary->is_flat()\n@@ -2109,0 +2259,2 @@\n+            \/\/ Default value load\n+            tp->is_instptr()->instance_klass() == ciEnv::current()->Class_klass() ||\n@@ -2114,1 +2266,3 @@\n-    \/\/ Optimize loads from constant fields.\n+    BasicType bt = value_basic_type();\n+\n+    \/\/ Optimize loads from constant fields.\n@@ -2118,1 +2272,1 @@\n-      const Type* con_type = Type::make_constant_from_field(const_oop->as_instance(), off, is_unsigned(), value_basic_type());\n+      const Type* con_type = Type::make_constant_from_field(const_oop->as_instance(), off, is_unsigned(), bt);\n@@ -2165,1 +2319,1 @@\n-      if (UseCompactObjectHeaders) {\n+      if (UseCompactObjectHeaders) { \/\/ TODO: Should EnableValhalla also take this path ?\n@@ -2241,0 +2395,12 @@\n+      \/\/ TODO 8350865 Scalar replacement does not work well for flat arrays.\n+      \/\/ Escape Analysis assumes that arrays are always zeroed during allocation which is not true for null-free arrays\n+      \/\/ ConnectionGraph::split_unique_types will re-wire the memory of loads from such arrays around the allocation\n+      \/\/ TestArrays::test6 and test152 and TestBasicFunctionality::test20 are affected by this.\n+      if (tp->isa_aryptr() && tp->is_aryptr()->is_flat() && tp->is_aryptr()->is_null_free()) {\n+        intptr_t offset = 0;\n+        Node* base = AddPNode::Ideal_base_and_offset(adr, phase, offset);\n+        AllocateNode* alloc = AllocateNode::Ideal_allocation(base);\n+        if (alloc != nullptr && alloc->is_AllocateArray() && alloc->in(AllocateNode::InitValue) != nullptr) {\n+          return _type;\n+        }\n+      }\n@@ -2244,1 +2410,0 @@\n-\n@@ -2248,1 +2413,10 @@\n-      return TypeX::make(markWord::prototype().value());\n+      if (EnableValhalla) {\n+        \/\/ The mark word may contain property bits (inline, flat, null-free)\n+        Node* klass_node = alloc->in(AllocateNode::KlassNode);\n+        const TypeKlassPtr* tkls = phase->type(klass_node)->isa_klassptr();\n+        if (tkls != nullptr && tkls->is_loaded() && tkls->klass_is_exact()) {\n+          return TypeX::make(tkls->exact_klass()->prototype_header());\n+        }\n+      } else {\n+        return TypeX::make(markWord::prototype().value());\n+      }\n@@ -2397,0 +2571,13 @@\n+Node* LoadNNode::Ideal(PhaseGVN* phase, bool can_reshape) {\n+  \/\/ Loading from an InlineType, find the input and make an EncodeP\n+  Node* addr = in(Address);\n+  intptr_t offset;\n+  Node* base = AddPNode::Ideal_base_and_offset(addr, phase, offset);\n+  Node* value = see_through_inline_type(phase, this, base, offset);\n+  if (value != nullptr) {\n+    return new EncodePNode(value, type());\n+  }\n+\n+  return LoadNode::Ideal(phase, can_reshape);\n+}\n+\n@@ -2470,1 +2657,1 @@\n-  const TypeAryPtr *tary = tp->isa_aryptr();\n+  const TypeAryPtr* tary = tp->isa_aryptr();\n@@ -2561,0 +2748,1 @@\n+        \/\/ TODO 8366668 Re-enable this for arrays\n@@ -2562,1 +2750,1 @@\n-            && (tkls->isa_instklassptr() || tkls->isa_aryklassptr())\n+            && ((tkls->isa_instklassptr() && !tkls->is_instklassptr()->might_be_an_array()) || (tkls->isa_aryklassptr() && false))\n@@ -3386,2 +3574,2 @@\n-  \/\/ unsafe if I have intervening uses.\n-  {\n+  \/\/ unsafe if I have intervening uses...\n+  if (phase->C->get_adr_type(phase->C->get_alias_index(adr_type())) != TypeAryPtr::INLINES) {\n@@ -3407,0 +3595,2 @@\n+             (Opcode() == Op_StoreL && st->Opcode() == Op_StoreN) ||\n+             (st->adr_type()->isa_aryptr() && st->adr_type()->is_aryptr()->is_flat()) || \/\/ TODO 8343835\n@@ -3544,2 +3734,1 @@\n-  if (result == this &&\n-      ReduceFieldZeroing && phase->type(val)->is_zero_type()) {\n+  if (result == this && ReduceFieldZeroing) {\n@@ -3547,1 +3736,2 @@\n-    if (mem->is_Proj() && mem->in(0)->is_Allocate()) {\n+    if (mem->is_Proj() && mem->in(0)->is_Allocate() &&\n+        (phase->type(val)->is_zero_type() || mem->in(0)->in(AllocateNode::InitValue) == val)) {\n@@ -3551,1 +3741,1 @@\n-    if (result == this) {\n+    if (result == this && phase->type(val)->is_zero_type()) {\n@@ -4055,1 +4245,1 @@\n-    return new ClearArrayNode(in(0), in(1), in(2), in(3), true);\n+    return new ClearArrayNode(in(0), in(1), in(2), in(3), in(4), true);\n@@ -4073,1 +4263,1 @@\n-  Node *zero = phase->makecon(TypeLong::ZERO);\n+  Node *val = in(4);\n@@ -4075,1 +4265,1 @@\n-  mem = new StoreLNode(in(0),mem,adr,atp,zero,MemNode::unordered,false);\n+  mem = new StoreLNode(in(0), mem, adr, atp, val, MemNode::unordered, false);\n@@ -4080,1 +4270,1 @@\n-    mem = new StoreLNode(in(0),mem,adr,atp,zero,MemNode::unordered,false);\n+    mem = new StoreLNode(in(0), mem, adr, atp, val, MemNode::unordered, false);\n@@ -4114,0 +4304,2 @@\n+                                   Node* val,\n+                                   Node* raw_val,\n@@ -4124,1 +4316,7 @@\n-    mem = StoreNode::make(*phase, ctl, mem, adr, atp, phase->zerocon(T_INT), T_INT, MemNode::unordered);\n+    if (val != nullptr) {\n+      assert(phase->type(val)->isa_narrowoop(), \"should be narrow oop\");\n+      mem = new StoreNNode(ctl, mem, adr, atp, val, MemNode::unordered);\n+    } else {\n+      assert(raw_val == nullptr, \"val may not be null\");\n+      mem = StoreNode::make(*phase, ctl, mem, adr, atp, phase->zerocon(T_INT), T_INT, MemNode::unordered);\n+    }\n@@ -4131,1 +4329,1 @@\n-  return clear_memory(ctl, mem, dest, phase->MakeConX(offset), end_offset, phase);\n+  return clear_memory(ctl, mem, dest, raw_val, phase->MakeConX(offset), end_offset, phase);\n@@ -4135,0 +4333,1 @@\n+                                   Node* raw_val,\n@@ -4157,1 +4356,4 @@\n-  mem = new ClearArrayNode(ctl, mem, zsize, adr, false);\n+  if (raw_val == nullptr) {\n+    raw_val = phase->MakeConX(0);\n+  }\n+  mem = new ClearArrayNode(ctl, mem, zsize, adr, raw_val, false);\n@@ -4162,0 +4364,2 @@\n+                                   Node* val,\n+                                   Node* raw_val,\n@@ -4176,1 +4380,1 @@\n-    mem = clear_memory(ctl, mem, dest,\n+    mem = clear_memory(ctl, mem, dest, val, raw_val,\n@@ -4183,1 +4387,7 @@\n-    mem = StoreNode::make(*phase, ctl, mem, adr, atp, phase->zerocon(T_INT), T_INT, MemNode::unordered);\n+    if (val != nullptr) {\n+      assert(phase->type(val)->isa_narrowoop(), \"should be narrow oop\");\n+      mem = new StoreNNode(ctl, mem, adr, atp, val, MemNode::unordered);\n+    } else {\n+      assert(raw_val == nullptr, \"val may not be null\");\n+      mem = StoreNode::make(*phase, ctl, mem, adr, atp, phase->zerocon(T_INT), T_INT, MemNode::unordered);\n+    }\n@@ -4329,1 +4539,1 @@\n-Node *MemBarNode::match( const ProjNode *proj, const Matcher *m ) {\n+Node *MemBarNode::match(const ProjNode *proj, const Matcher *m, const RegMask* mask) {\n@@ -4616,1 +4826,3 @@\n-  if (init == nullptr || init->is_complete())  return false;\n+  if (init == nullptr || init->is_complete()) {\n+    return false;\n+  }\n@@ -4800,0 +5012,6 @@\n+                if (base->is_Phi()) {\n+                  \/\/ In rare case, base may be a PhiNode and it may read\n+                  \/\/ the same memory slice between InitializeNode and store.\n+                  failed = true;\n+                  break;\n+                }\n@@ -5386,0 +5604,2 @@\n+                                              allocation()->in(AllocateNode::InitValue),\n+                                              allocation()->in(AllocateNode::RawInitValue),\n@@ -5445,0 +5665,2 @@\n+                                            allocation()->in(AllocateNode::InitValue),\n+                                            allocation()->in(AllocateNode::RawInitValue),\n","filename":"src\/hotspot\/share\/opto\/memnode.cpp","additions":269,"deletions":47,"binary":false,"changes":316,"status":"modified"},{"patch":"@@ -35,0 +35,1 @@\n+#include \"gc\/shared\/gc_globals.hpp\"\n@@ -244,1 +245,9 @@\n-    _orig_pc_slot = C->fixed_slots() - (sizeof(address) \/ VMRegImpl::stack_slot_size);\n+    int fixed_slots = C->fixed_slots();\n+    if (C->needs_stack_repair()) {\n+      fixed_slots -= 2;\n+    }\n+    \/\/ TODO 8284443 Only reserve extra slot if needed\n+    if (InlineTypeReturnedAsFields) {\n+      fixed_slots -= 2;\n+    }\n+    _orig_pc_slot = fixed_slots - (sizeof(address) \/ VMRegImpl::stack_slot_size);\n@@ -285,1 +294,2 @@\n-  MachPrologNode *prolog = new MachPrologNode();\n+  Label verified_entry;\n+  MachPrologNode* prolog = new MachPrologNode(&verified_entry);\n@@ -291,3 +301,2 @@\n-\n-  if( C->is_osr_compilation() ) {\n-    if( PoisonOSREntry ) {\n+  if (C->is_osr_compilation()) {\n+    if (PoisonOSREntry) {\n@@ -298,3 +307,14 @@\n-    if( C->method() && !C->method()->flags().is_static() ) {\n-      \/\/ Insert unvalidated entry point\n-      C->cfg()->insert( broot, 0, new MachUEPNode() );\n+    if (C->method()) {\n+      if (C->method()->has_scalarized_args()) {\n+        \/\/ Add entry point to unpack all inline type arguments\n+        C->cfg()->insert(broot, 0, new MachVEPNode(&verified_entry, \/* verified *\/ true, \/* receiver_only *\/ false));\n+        if (!C->method()->is_static()) {\n+          \/\/ Add verified\/unverified entry points to only unpack inline type receiver at interface calls\n+          C->cfg()->insert(broot, 0, new MachVEPNode(&verified_entry, \/* verified *\/ false, \/* receiver_only *\/ false));\n+          C->cfg()->insert(broot, 0, new MachVEPNode(&verified_entry, \/* verified *\/ true,  \/* receiver_only *\/ true));\n+          C->cfg()->insert(broot, 0, new MachVEPNode(&verified_entry, \/* verified *\/ false, \/* receiver_only *\/ true));\n+        }\n+      } else if (!C->method()->is_static()) {\n+        \/\/ Insert unvalidated entry point\n+        C->cfg()->insert(broot, 0, new MachUEPNode());\n+      }\n@@ -302,1 +322,0 @@\n-\n@@ -342,0 +361,25 @@\n+  if (!C->is_osr_compilation() && C->has_scalarized_args()) {\n+    \/\/ Compute the offsets of the entry points required by the inline type calling convention\n+    if (!C->method()->is_static()) {\n+      \/\/ We have entries at the beginning of the method, implemented by the first 4 nodes.\n+      \/\/ Entry                     (unverified) @ offset 0\n+      \/\/ Verified_Inline_Entry_RO\n+      \/\/ Inline_Entry              (unverified)\n+      \/\/ Verified_Inline_Entry\n+      uint offset = 0;\n+      _code_offsets.set_value(CodeOffsets::Entry, offset);\n+\n+      offset += ((MachVEPNode*)broot->get_node(0))->size(C->regalloc());\n+      _code_offsets.set_value(CodeOffsets::Verified_Inline_Entry_RO, offset);\n+\n+      offset += ((MachVEPNode*)broot->get_node(1))->size(C->regalloc());\n+      _code_offsets.set_value(CodeOffsets::Inline_Entry, offset);\n+\n+      offset += ((MachVEPNode*)broot->get_node(2))->size(C->regalloc());\n+      _code_offsets.set_value(CodeOffsets::Verified_Inline_Entry, offset);\n+    } else {\n+      _code_offsets.set_value(CodeOffsets::Entry, -1); \/\/ will be patched later\n+      _code_offsets.set_value(CodeOffsets::Verified_Inline_Entry, 0);\n+    }\n+  }\n+\n@@ -503,1 +547,3 @@\n-          mcall->method_set((intptr_t)mcall->entry_point());\n+          if (mcall->entry_point() != nullptr) {\n+            mcall->method_set((intptr_t)mcall->entry_point());\n+          }\n@@ -758,0 +804,29 @@\n+      uint first_ind = spobj->first_index(sfpt->jvms());\n+      \/\/ Nullable, scalarized inline types have a null_marker input\n+      \/\/ that needs to be checked before using the field values.\n+      ScopeValue* properties = nullptr;\n+      if (cik->is_inlinetype()) {\n+        Node* null_marker_node = sfpt->in(first_ind++);\n+        assert(null_marker_node != nullptr, \"null_marker node not found\");\n+        if (!null_marker_node->is_top()) {\n+          const TypeInt* null_marker_type = null_marker_node->bottom_type()->is_int();\n+          if (null_marker_node->is_Con()) {\n+            properties = new ConstantIntValue(null_marker_type->get_con());\n+          } else {\n+            OptoReg::Name null_marker_reg = C->regalloc()->get_reg_first(null_marker_node);\n+            properties = new_loc_value(C->regalloc(), null_marker_reg, Location::normal);\n+          }\n+        }\n+      }\n+      if (cik->is_array_klass() && !cik->is_type_array_klass()) {\n+        jint props = ArrayKlass::ArrayProperties::DEFAULT;\n+        if (cik->as_array_klass()->element_klass()->is_inlinetype()) {\n+          if (cik->as_array_klass()->is_elem_null_free()) {\n+            props |= ArrayKlass::ArrayProperties::NULL_RESTRICTED;\n+          }\n+          if (!cik->as_array_klass()->is_elem_atomic()) {\n+            props |= ArrayKlass::ArrayProperties::NON_ATOMIC;\n+          }\n+        }\n+        properties = new ConstantIntValue(props);\n+      }\n@@ -759,1 +834,1 @@\n-                           new ConstantOopWriteValue(cik->java_mirror()->constant_encoding()));\n+                           new ConstantOopWriteValue(cik->java_mirror()->constant_encoding()), true, properties);\n@@ -762,1 +837,0 @@\n-      uint first_ind = spobj->first_index(sfpt->jvms());\n@@ -1009,0 +1083,1 @@\n+  bool return_scalarized = false;\n@@ -1029,1 +1104,1 @@\n-    if (mcall->returns_pointer()) {\n+    if (mcall->returns_pointer() || mcall->returns_scalarized()) {\n@@ -1032,0 +1107,3 @@\n+    if (mcall->returns_scalarized()) {\n+      return_scalarized = true;\n+    }\n@@ -1101,0 +1179,14 @@\n+          assert(!cik->is_inlinetype(), \"Synchronization on value object?\");\n+          ScopeValue* properties = nullptr;\n+          if (cik->is_array_klass() && !cik->is_type_array_klass()) {\n+            jint props = ArrayKlass::ArrayProperties::DEFAULT;\n+            if (cik->as_array_klass()->element_klass()->is_inlinetype()) {\n+              if (cik->as_array_klass()->is_elem_null_free()) {\n+                props |= ArrayKlass::ArrayProperties::NULL_RESTRICTED;\n+              }\n+              if (!cik->as_array_klass()->is_elem_atomic()) {\n+                props |= ArrayKlass::ArrayProperties::NON_ATOMIC;\n+              }\n+            }\n+            properties = new ConstantIntValue(props);\n+          }\n@@ -1102,1 +1194,1 @@\n-                                            new ConstantOopWriteValue(cik->java_mirror()->constant_encoding()));\n+                                            new ConstantOopWriteValue(cik->java_mirror()->constant_encoding()), true, properties);\n@@ -1210,0 +1302,1 @@\n+      return_scalarized,\n@@ -1585,2 +1678,4 @@\n-          \/\/ This destination address is NOT PC-relative\n-          mcall->method_set((intptr_t)mcall->entry_point());\n+          if (mcall->entry_point() != nullptr) {\n+            \/\/ This destination address is NOT PC-relative\n+            mcall->method_set((intptr_t)mcall->entry_point());\n+          }\n@@ -1727,1 +1822,0 @@\n-\n@@ -3165,0 +3259,13 @@\n+\n+      \/\/ Do not allow a CheckCastPP node whose input is a raw pointer to\n+      \/\/ float past a safepoint.  This can occur when a buffered inline\n+      \/\/ type is allocated in a loop and the CheckCastPP from that\n+      \/\/ allocation is reused outside the loop.  If the use inside the\n+      \/\/ loop is scalarized the CheckCastPP will no longer be connected\n+      \/\/ to the loop safepoint.  See JDK-8264340.\n+      if (m->is_Mach() && m->as_Mach()->ideal_Opcode() == Op_CheckCastPP) {\n+        Node *def = m->in(1);\n+        if (def != nullptr && def->bottom_type()->base() == Type::RawPtr) {\n+          last_safept_node->add_prec(m);\n+        }\n+      }\n@@ -3323,0 +3430,19 @@\n+    if (C->has_scalarized_args()) {\n+      \/\/ Inline type entry points (MachVEPNodes) require lots of space for GC barriers and oop verification\n+      \/\/ when loading object fields from the buffered argument. Increase scratch buffer size accordingly.\n+      ciMethod* method = C->method();\n+      int barrier_size = UseZGC ? 200 : (7 DEBUG_ONLY(+ 37));\n+      int arg_num = 0;\n+      if (!method->is_static()) {\n+        if (method->is_scalarized_arg(arg_num)) {\n+          size += method->holder()->as_inline_klass()->oop_count() * barrier_size;\n+        }\n+        arg_num++;\n+      }\n+      for (ciSignatureStream str(method->signature()); !str.at_return_type(); str.next()) {\n+        if (method->is_scalarized_arg(arg_num)) {\n+          size += str.type()->as_inline_klass()->oop_count() * barrier_size;\n+        }\n+        arg_num++;\n+      }\n+    }\n@@ -3393,1 +3519,2 @@\n-  if (is_branch) \/\/ Restore label.\n+  \/\/ Restore label.\n+  if (is_branch) {\n@@ -3395,0 +3522,1 @@\n+  }\n@@ -3435,6 +3563,9 @@\n-      if (!target->is_static()) {\n-        \/\/ The UEP of an nmethod ensures that the VEP is padded. However, the padding of the UEP is placed\n-        \/\/ before the inline cache check, so we don't have to execute any nop instructions when dispatching\n-        \/\/ through the UEP, yet we can ensure that the VEP is aligned appropriately.\n-        _code_offsets.set_value(CodeOffsets::Entry, _first_block_size - MacroAssembler::ic_check_size());\n-      }\n+      if (_code_offsets.value(CodeOffsets::Verified_Inline_Entry) == -1) {\n+        _code_offsets.set_value(CodeOffsets::Verified_Inline_Entry, _first_block_size);\n+      }\n+      if (_code_offsets.value(CodeOffsets::Verified_Inline_Entry_RO) == -1) {\n+        _code_offsets.set_value(CodeOffsets::Verified_Inline_Entry_RO, _first_block_size);\n+      }\n+      if (_code_offsets.value(CodeOffsets::Entry) == -1) {\n+        _code_offsets.set_value(CodeOffsets::Entry, _first_block_size);\n+      }\n@@ -3446,14 +3577,14 @@\n-                                     entry_bci,\n-                                     &_code_offsets,\n-                                     _orig_pc_slot_offset_in_bytes,\n-                                     code_buffer(),\n-                                     frame_size_in_words(),\n-                                     oop_map_set(),\n-                                     &_handler_table,\n-                                     inc_table(),\n-                                     compiler,\n-                                     has_unsafe_access,\n-                                     SharedRuntime::is_wide_vector(C->max_vector_size()),\n-                                     C->has_monitors(),\n-                                     C->has_scoped_access(),\n-                                     0);\n+                              entry_bci,\n+                              &_code_offsets,\n+                              _orig_pc_slot_offset_in_bytes,\n+                              code_buffer(),\n+                              frame_size_in_words(),\n+                              _oop_map_set,\n+                              &_handler_table,\n+                              inc_table(),\n+                              compiler,\n+                              has_unsafe_access,\n+                              SharedRuntime::is_wide_vector(C->max_vector_size()),\n+                              C->has_monitors(),\n+                              C->has_scoped_access(),\n+                              0);\n","filename":"src\/hotspot\/share\/opto\/output.cpp","additions":169,"deletions":38,"binary":false,"changes":207,"status":"modified"},{"patch":"@@ -47,0 +47,2 @@\n+#include \"oops\/flatArrayKlass.hpp\"\n+#include \"oops\/flatArrayOop.inline.hpp\"\n@@ -201,0 +203,1 @@\n+const TypeFunc* OptoRuntime::_new_array_nozero_Type               = nullptr;\n@@ -331,1 +334,1 @@\n-JRT_BLOCK_ENTRY(void, OptoRuntime::new_instance_C(Klass* klass, JavaThread* current))\n+JRT_BLOCK_ENTRY(void, OptoRuntime::new_instance_C(Klass* klass, bool is_larval, JavaThread* current))\n@@ -351,1 +354,5 @@\n-    oop result = InstanceKlass::cast(klass)->allocate_instance(THREAD);\n+    instanceOop result = InstanceKlass::cast(klass)->allocate_instance(THREAD);\n+    if (is_larval) {\n+      \/\/ Check if this is a larval buffer allocation\n+      result->set_mark(result->mark().enter_larval_state());\n+    }\n@@ -369,1 +376,1 @@\n-JRT_BLOCK_ENTRY(void, OptoRuntime::new_array_C(Klass* array_type, int len, JavaThread* current))\n+JRT_BLOCK_ENTRY(void, OptoRuntime::new_array_C(Klass* array_type, int len, oopDesc* init_val, JavaThread* current))\n@@ -378,0 +385,1 @@\n+  Handle h_init_val(current, init_val); \/\/ keep the init_val object alive\n@@ -379,1 +387,26 @@\n-  if (array_type->is_typeArray_klass()) {\n+  if (array_type->is_flatArray_klass()) {\n+    Handle holder(current, array_type->klass_holder()); \/\/ keep the array klass alive\n+    FlatArrayKlass* fak = FlatArrayKlass::cast(array_type);\n+    InlineKlass* vk = fak->element_klass();\n+    ArrayKlass::ArrayProperties props = ArrayKlass::ArrayProperties::DEFAULT;\n+    switch(fak->layout_kind()) {\n+      case LayoutKind::ATOMIC_FLAT:\n+        props = ArrayKlass::ArrayProperties::NULL_RESTRICTED;\n+      break;\n+      case LayoutKind::NON_ATOMIC_FLAT:\n+        props = (ArrayKlass::ArrayProperties)(ArrayKlass::ArrayProperties::NULL_RESTRICTED | ArrayKlass::ArrayProperties::NON_ATOMIC);\n+      break;\n+      case LayoutKind::NULLABLE_ATOMIC_FLAT:\n+      props = ArrayKlass::ArrayProperties::NON_ATOMIC;\n+      break;\n+      default:\n+        ShouldNotReachHere();\n+    }\n+    result = oopFactory::new_flatArray(vk, len, props, fak->layout_kind(), THREAD);\n+    if (array_type->is_null_free_array_klass() && !h_init_val.is_null()) {\n+      \/\/ Null-free arrays need to be initialized\n+      for (int i = 0; i < len; i++) {\n+        vk->write_value_to_addr(h_init_val(), ((flatArrayOop)result)->value_at_addr(i, fak->layout_helper()), fak->layout_kind(), true, CHECK);\n+      }\n+    }\n+  } else if (array_type->is_typeArray_klass()) {\n@@ -385,5 +418,8 @@\n-    \/\/ Although the oopFactory likes to work with the elem_type,\n-    \/\/ the compiler prefers the array_type, since it must already have\n-    \/\/ that latter value in hand for the fast path.\n-    Klass* elem_type = ObjArrayKlass::cast(array_type)->element_klass();\n-    result = oopFactory::new_objArray(elem_type, len, THREAD);\n+    RefArrayKlass* array_klass = RefArrayKlass::cast(array_type);\n+    result = array_klass->allocate_instance(len, RefArrayKlass::cast(array_type)->properties(), THREAD);\n+    if (array_type->is_null_free_array_klass() && !h_init_val.is_null()) {\n+      \/\/ Null-free arrays need to be initialized\n+      for (int i = 0; i < len; i++) {\n+        ((objArrayOop)result)->obj_at_put(i, h_init_val());\n+      }\n+    }\n@@ -587,1 +623,1 @@\n-  const Type **fields = TypeTuple::fields(1);\n+  const Type **fields = TypeTuple::fields(2);\n@@ -589,1 +625,2 @@\n-  const TypeTuple *domain = TypeTuple::make(TypeFunc::Parms+1, fields);\n+  fields[TypeFunc::Parms+1] = TypeInt::BOOL;        \/\/ is_larval\n+  const TypeTuple *domain = TypeTuple::make(TypeFunc::Parms+2, fields);\n@@ -632,0 +669,17 @@\n+  \/\/ create input type (domain)\n+  const Type **fields = TypeTuple::fields(3);\n+  fields[TypeFunc::Parms+0] = TypeInstPtr::NOTNULL;   \/\/ element klass\n+  fields[TypeFunc::Parms+1] = TypeInt::INT;       \/\/ array size\n+  fields[TypeFunc::Parms+2] = TypeInstPtr::NOTNULL;       \/\/ init value\n+  const TypeTuple *domain = TypeTuple::make(TypeFunc::Parms+3, fields);\n+\n+  \/\/ create result type (range)\n+  fields = TypeTuple::fields(1);\n+  fields[TypeFunc::Parms+0] = TypeRawPtr::NOTNULL; \/\/ Returned oop\n+\n+  const TypeTuple *range = TypeTuple::make(TypeFunc::Parms+1, fields);\n+\n+  return TypeFunc::make(domain, range);\n+}\n+\n+static const TypeFunc* make_new_array_nozero_Type() {\n@@ -707,1 +761,1 @@\n-  return TypeFunc::make(domain,range);\n+  return TypeFunc::make(domain, range);\n@@ -2087,1 +2141,1 @@\n-  return TypeFunc::make(domain,range);\n+  return TypeFunc::make(domain, range);\n@@ -2119,1 +2173,1 @@\n-  return TypeFunc::make(domain,range);\n+  return TypeFunc::make(domain, range);\n@@ -2135,1 +2189,1 @@\n-  return TypeFunc::make(domain,range);\n+  return TypeFunc::make(domain, range);\n@@ -2226,0 +2280,1 @@\n+  _new_array_nozero_Type              = make_new_array_nozero_Type();\n@@ -2326,0 +2381,105 @@\n+\n+const TypeFunc *OptoRuntime::store_inline_type_fields_Type() {\n+  \/\/ create input type (domain)\n+  uint total = SharedRuntime::java_return_convention_max_int + SharedRuntime::java_return_convention_max_float*2;\n+  const Type **fields = TypeTuple::fields(total);\n+  \/\/ We don't know the number of returned values and their\n+  \/\/ types. Assume all registers available to the return convention\n+  \/\/ are used.\n+  fields[TypeFunc::Parms] = TypePtr::BOTTOM;\n+  uint i = 1;\n+  for (; i < SharedRuntime::java_return_convention_max_int; i++) {\n+    fields[TypeFunc::Parms+i] = TypeInt::INT;\n+  }\n+  for (; i < total; i+=2) {\n+    fields[TypeFunc::Parms+i] = Type::DOUBLE;\n+    fields[TypeFunc::Parms+i+1] = Type::HALF;\n+  }\n+  const TypeTuple* domain = TypeTuple::make(TypeFunc::Parms + total, fields);\n+\n+  \/\/ create result type (range)\n+  fields = TypeTuple::fields(1);\n+  fields[TypeFunc::Parms+0] = TypeInstPtr::BOTTOM;\n+\n+  const TypeTuple *range = TypeTuple::make(TypeFunc::Parms+1,fields);\n+\n+  return TypeFunc::make(domain, range);\n+}\n+\n+const TypeFunc *OptoRuntime::pack_inline_type_Type() {\n+  \/\/ create input type (domain)\n+  uint total = 1 + SharedRuntime::java_return_convention_max_int + SharedRuntime::java_return_convention_max_float*2;\n+  const Type **fields = TypeTuple::fields(total);\n+  \/\/ We don't know the number of returned values and their\n+  \/\/ types. Assume all registers available to the return convention\n+  \/\/ are used.\n+  fields[TypeFunc::Parms] = TypeRawPtr::BOTTOM;\n+  fields[TypeFunc::Parms+1] = TypeRawPtr::BOTTOM;\n+  uint i = 2;\n+  for (; i < SharedRuntime::java_return_convention_max_int+1; i++) {\n+    fields[TypeFunc::Parms+i] = TypeInt::INT;\n+  }\n+  for (; i < total; i+=2) {\n+    fields[TypeFunc::Parms+i] = Type::DOUBLE;\n+    fields[TypeFunc::Parms+i+1] = Type::HALF;\n+  }\n+  const TypeTuple* domain = TypeTuple::make(TypeFunc::Parms + total, fields);\n+\n+  \/\/ create result type (range)\n+  fields = TypeTuple::fields(1);\n+  fields[TypeFunc::Parms+0] = TypeInstPtr::NOTNULL;\n+\n+  const TypeTuple *range = TypeTuple::make(TypeFunc::Parms+1,fields);\n+\n+  return TypeFunc::make(domain, range);\n+}\n+\n+JRT_BLOCK_ENTRY(void, OptoRuntime::load_unknown_inline_C(flatArrayOopDesc* array, int index, JavaThread* current))\n+  JRT_BLOCK;\n+  oop buffer = array->obj_at(index, THREAD);\n+  deoptimize_caller_frame(current, HAS_PENDING_EXCEPTION);\n+  current->set_vm_result_oop(buffer);\n+  JRT_BLOCK_END;\n+JRT_END\n+\n+const TypeFunc* OptoRuntime::load_unknown_inline_Type() {\n+  \/\/ create input type (domain)\n+  const Type** fields = TypeTuple::fields(2);\n+  fields[TypeFunc::Parms] = TypeOopPtr::NOTNULL;\n+  fields[TypeFunc::Parms+1] = TypeInt::POS;\n+\n+  const TypeTuple* domain = TypeTuple::make(TypeFunc::Parms+2, fields);\n+\n+  \/\/ create result type (range)\n+  fields = TypeTuple::fields(1);\n+  fields[TypeFunc::Parms] = TypeInstPtr::BOTTOM;\n+\n+  const TypeTuple* range = TypeTuple::make(TypeFunc::Parms+1, fields);\n+\n+  return TypeFunc::make(domain, range);\n+}\n+\n+JRT_BLOCK_ENTRY(void, OptoRuntime::store_unknown_inline_C(instanceOopDesc* buffer, flatArrayOopDesc* array, int index, JavaThread* current))\n+  JRT_BLOCK;\n+  array->obj_at_put(index, buffer, THREAD);\n+  if (HAS_PENDING_EXCEPTION) {\n+      fatal(\"This entry must be changed to be a non-leaf entry because writing to a flat array can now throw an exception\");\n+  }\n+  JRT_BLOCK_END;\n+JRT_END\n+\n+const TypeFunc* OptoRuntime::store_unknown_inline_Type() {\n+  \/\/ create input type (domain)\n+  const Type** fields = TypeTuple::fields(3);\n+  fields[TypeFunc::Parms] = TypeInstPtr::NOTNULL;\n+  fields[TypeFunc::Parms+1] = TypeOopPtr::NOTNULL;\n+  fields[TypeFunc::Parms+2] = TypeInt::POS;\n+\n+  const TypeTuple* domain = TypeTuple::make(TypeFunc::Parms+3, fields);\n+\n+  \/\/ create result type (range)\n+  fields = TypeTuple::fields(0);\n+  const TypeTuple* range = TypeTuple::make(TypeFunc::Parms, fields);\n+\n+  return TypeFunc::make(domain, range);\n+}\n","filename":"src\/hotspot\/share\/opto\/runtime.cpp","additions":175,"deletions":15,"binary":false,"changes":190,"status":"modified"},{"patch":"@@ -324,4 +324,3 @@\n-  CallProjections projs;\n-  call->extract_projections(&projs, false);\n-  if (projs.fallthrough_catchproj != nullptr) {\n-    C->gvn_replace_by(projs.fallthrough_catchproj, call->in(TypeFunc::Control));\n+  CallProjections* projs = call->extract_projections(false);\n+  if (projs->fallthrough_catchproj != nullptr) {\n+    C->gvn_replace_by(projs->fallthrough_catchproj, call->in(TypeFunc::Control));\n@@ -329,2 +328,2 @@\n-  if (projs.fallthrough_memproj != nullptr) {\n-    C->gvn_replace_by(projs.fallthrough_memproj, call->in(TypeFunc::Memory));\n+  if (projs->fallthrough_memproj != nullptr) {\n+    C->gvn_replace_by(projs->fallthrough_memproj, call->in(TypeFunc::Memory));\n@@ -332,2 +331,2 @@\n-  if (projs.catchall_memproj != nullptr) {\n-    C->gvn_replace_by(projs.catchall_memproj, C->top());\n+  if (projs->catchall_memproj != nullptr) {\n+    C->gvn_replace_by(projs->catchall_memproj, C->top());\n@@ -335,2 +334,2 @@\n-  if (projs.fallthrough_ioproj != nullptr) {\n-    C->gvn_replace_by(projs.fallthrough_ioproj, call->in(TypeFunc::I_O));\n+  if (projs->fallthrough_ioproj != nullptr) {\n+    C->gvn_replace_by(projs->fallthrough_ioproj, call->in(TypeFunc::I_O));\n@@ -338,2 +337,2 @@\n-  if (projs.catchall_ioproj != nullptr) {\n-    C->gvn_replace_by(projs.catchall_ioproj, C->top());\n+  if (projs->catchall_ioproj != nullptr) {\n+    C->gvn_replace_by(projs->catchall_ioproj, C->top());\n@@ -341,1 +340,1 @@\n-  if (projs.catchall_catchproj != nullptr) {\n+  if (projs->catchall_catchproj != nullptr) {\n@@ -344,1 +343,1 @@\n-    for (SimpleDUIterator i(projs.catchall_catchproj); i.has_next(); i.next()) {\n+    for (SimpleDUIterator i(projs->catchall_catchproj); i.has_next(); i.next()) {\n@@ -351,1 +350,1 @@\n-    C->gvn_replace_by(projs.catchall_catchproj, C->top());\n+    C->gvn_replace_by(projs->catchall_catchproj, C->top());\n@@ -353,2 +352,3 @@\n-  if (projs.resproj != nullptr) {\n-    C->gvn_replace_by(projs.resproj, C->top());\n+  if (projs->resproj[0] != nullptr) {\n+    assert(projs->nb_resproj == 1, \"unexpected number of results\");\n+    C->gvn_replace_by(projs->resproj[0], C->top());\n","filename":"src\/hotspot\/share\/opto\/stringopts.cpp","additions":17,"deletions":17,"binary":false,"changes":34,"status":"modified"},{"patch":"@@ -25,0 +25,3 @@\n+#include \"ci\/ciField.hpp\"\n+#include \"ci\/ciFlatArrayKlass.hpp\"\n+#include \"ci\/ciInlineKlass.hpp\"\n@@ -48,0 +51,1 @@\n+#include \"utilities\/globalDefinitions.hpp\"\n@@ -57,0 +61,45 @@\n+const Type::Offset Type::Offset::top(Type::OffsetTop);\n+const Type::Offset Type::Offset::bottom(Type::OffsetBot);\n+\n+const Type::Offset Type::Offset::meet(const Type::Offset other) const {\n+  \/\/ Either is 'TOP' offset?  Return the other offset!\n+  if (_offset == OffsetTop) return other;\n+  if (other._offset == OffsetTop) return *this;\n+  \/\/ If either is different, return 'BOTTOM' offset\n+  if (_offset != other._offset) return bottom;\n+  return Offset(_offset);\n+}\n+\n+const Type::Offset Type::Offset::dual() const {\n+  if (_offset == OffsetTop) return bottom;\/\/ Map 'TOP' into 'BOTTOM'\n+  if (_offset == OffsetBot) return top;\/\/ Map 'BOTTOM' into 'TOP'\n+  return Offset(_offset);               \/\/ Map everything else into self\n+}\n+\n+const Type::Offset Type::Offset::add(intptr_t offset) const {\n+  \/\/ Adding to 'TOP' offset?  Return 'TOP'!\n+  if (_offset == OffsetTop || offset == OffsetTop) return top;\n+  \/\/ Adding to 'BOTTOM' offset?  Return 'BOTTOM'!\n+  if (_offset == OffsetBot || offset == OffsetBot) return bottom;\n+  \/\/ Addition overflows or \"accidentally\" equals to OffsetTop? Return 'BOTTOM'!\n+  offset += (intptr_t)_offset;\n+  if (offset != (int)offset || offset == OffsetTop) return bottom;\n+\n+  \/\/ assert( _offset >= 0 && _offset+offset >= 0, \"\" );\n+  \/\/ It is possible to construct a negative offset during PhaseCCP\n+\n+  return Offset((int)offset);        \/\/ Sum valid offsets\n+}\n+\n+void Type::Offset::dump2(outputStream *st) const {\n+  if (_offset == 0) {\n+    return;\n+  } else if (_offset == OffsetTop) {\n+    st->print(\"+top\");\n+  }\n+  else if (_offset == OffsetBot) {\n+    st->print(\"+bot\");\n+  } else if (_offset) {\n+    st->print(\"+%d\", _offset);\n+  }\n+}\n@@ -235,0 +284,3 @@\n+  case T_OBJECT:\n+    return Type::get_const_type(type->unwrap())->join_speculative(type->is_null_free() ? TypePtr::NOTNULL : TypePtr::BOTTOM);\n+\n@@ -557,3 +609,3 @@\n-  TypePtr::NULL_PTR= TypePtr::make(AnyPtr, TypePtr::Null, 0);\n-  TypePtr::NOTNULL = TypePtr::make(AnyPtr, TypePtr::NotNull, OffsetBot);\n-  TypePtr::BOTTOM  = TypePtr::make(AnyPtr, TypePtr::BotPTR, OffsetBot);\n+  TypePtr::NULL_PTR= TypePtr::make(AnyPtr, TypePtr::Null, Offset(0));\n+  TypePtr::NOTNULL = TypePtr::make(AnyPtr, TypePtr::NotNull, Offset::bottom);\n+  TypePtr::BOTTOM  = TypePtr::make(AnyPtr, TypePtr::BotPTR, Offset::bottom);\n@@ -576,1 +628,1 @@\n-                                           false, nullptr, oopDesc::mark_offset_in_bytes());\n+                                           false, nullptr, Offset(oopDesc::mark_offset_in_bytes()));\n@@ -578,2 +630,2 @@\n-                                           false, nullptr, oopDesc::klass_offset_in_bytes());\n-  TypeOopPtr::BOTTOM  = TypeOopPtr::make(TypePtr::BotPTR, OffsetBot, TypeOopPtr::InstanceBot);\n+                                           false, nullptr, Offset(oopDesc::klass_offset_in_bytes()));\n+  TypeOopPtr::BOTTOM  = TypeOopPtr::make(TypePtr::BotPTR, Offset::bottom, TypeOopPtr::InstanceBot);\n@@ -581,1 +633,1 @@\n-  TypeMetadataPtr::BOTTOM = TypeMetadataPtr::make(TypePtr::BotPTR, nullptr, OffsetBot);\n+  TypeMetadataPtr::BOTTOM = TypeMetadataPtr::make(TypePtr::BotPTR, nullptr, Offset::bottom);\n@@ -604,2 +656,2 @@\n-  TypeAryPtr::BOTTOM = TypeAryPtr::make(TypePtr::BotPTR, TypeAry::make(Type::BOTTOM, TypeInt::POS), nullptr, false, Type::OffsetBot);\n-  TypeAryPtr::RANGE   = TypeAryPtr::make( TypePtr::BotPTR, TypeAry::make(Type::BOTTOM,TypeInt::POS), nullptr \/* current->env()->Object_klass() *\/, false, arrayOopDesc::length_offset_in_bytes());\n+  TypeAryPtr::BOTTOM = TypeAryPtr::make(TypePtr::BotPTR, TypeAry::make(Type::BOTTOM, TypeInt::POS), nullptr, false, Offset::bottom);\n+  TypeAryPtr::RANGE   = TypeAryPtr::make(TypePtr::BotPTR, TypeAry::make(Type::BOTTOM,TypeInt::POS), nullptr \/* current->env()->Object_klass() *\/, false, Offset(arrayOopDesc::length_offset_in_bytes()));\n@@ -607,1 +659,1 @@\n-  TypeAryPtr::NARROWOOPS = TypeAryPtr::make(TypePtr::BotPTR, TypeAry::make(TypeNarrowOop::BOTTOM, TypeInt::POS), nullptr \/*ciArrayKlass::make(o)*\/,  false,  Type::OffsetBot);\n+  TypeAryPtr::NARROWOOPS = TypeAryPtr::make(TypePtr::BotPTR, TypeAry::make(TypeNarrowOop::BOTTOM, TypeInt::POS), nullptr \/*ciArrayKlass::make(o)*\/,  false,  Offset::bottom);\n@@ -617,1 +669,1 @@\n-    TypeAryPtr::OOPS  = TypeAryPtr::make(TypePtr::BotPTR, TypeAry::make(TypeInstPtr::BOTTOM,TypeInt::POS), nullptr \/*ciArrayKlass::make(o)*\/,  false,  Type::OffsetBot);\n+    TypeAryPtr::OOPS  = TypeAryPtr::make(TypePtr::BotPTR, TypeAry::make(TypeInstPtr::BOTTOM,TypeInt::POS), nullptr \/*ciArrayKlass::make(o)*\/,  false,  Offset::bottom);\n@@ -619,7 +671,8 @@\n-  TypeAryPtr::BYTES   = TypeAryPtr::make(TypePtr::BotPTR, TypeAry::make(TypeInt::BYTE      ,TypeInt::POS), ciTypeArrayKlass::make(T_BYTE),   true,  Type::OffsetBot);\n-  TypeAryPtr::SHORTS  = TypeAryPtr::make(TypePtr::BotPTR, TypeAry::make(TypeInt::SHORT     ,TypeInt::POS), ciTypeArrayKlass::make(T_SHORT),  true,  Type::OffsetBot);\n-  TypeAryPtr::CHARS   = TypeAryPtr::make(TypePtr::BotPTR, TypeAry::make(TypeInt::CHAR      ,TypeInt::POS), ciTypeArrayKlass::make(T_CHAR),   true,  Type::OffsetBot);\n-  TypeAryPtr::INTS    = TypeAryPtr::make(TypePtr::BotPTR, TypeAry::make(TypeInt::INT       ,TypeInt::POS), ciTypeArrayKlass::make(T_INT),    true,  Type::OffsetBot);\n-  TypeAryPtr::LONGS   = TypeAryPtr::make(TypePtr::BotPTR, TypeAry::make(TypeLong::LONG     ,TypeInt::POS), ciTypeArrayKlass::make(T_LONG),   true,  Type::OffsetBot);\n-  TypeAryPtr::FLOATS  = TypeAryPtr::make(TypePtr::BotPTR, TypeAry::make(Type::FLOAT        ,TypeInt::POS), ciTypeArrayKlass::make(T_FLOAT),  true,  Type::OffsetBot);\n-  TypeAryPtr::DOUBLES = TypeAryPtr::make(TypePtr::BotPTR, TypeAry::make(Type::DOUBLE       ,TypeInt::POS), ciTypeArrayKlass::make(T_DOUBLE), true,  Type::OffsetBot);\n+  TypeAryPtr::BYTES   = TypeAryPtr::make(TypePtr::BotPTR, TypeAry::make(TypeInt::BYTE      ,TypeInt::POS), ciTypeArrayKlass::make(T_BYTE),   true,  Offset::bottom);\n+  TypeAryPtr::SHORTS  = TypeAryPtr::make(TypePtr::BotPTR, TypeAry::make(TypeInt::SHORT     ,TypeInt::POS), ciTypeArrayKlass::make(T_SHORT),  true,  Offset::bottom);\n+  TypeAryPtr::CHARS   = TypeAryPtr::make(TypePtr::BotPTR, TypeAry::make(TypeInt::CHAR      ,TypeInt::POS), ciTypeArrayKlass::make(T_CHAR),   true,  Offset::bottom);\n+  TypeAryPtr::INTS    = TypeAryPtr::make(TypePtr::BotPTR, TypeAry::make(TypeInt::INT       ,TypeInt::POS), ciTypeArrayKlass::make(T_INT),    true,  Offset::bottom);\n+  TypeAryPtr::LONGS   = TypeAryPtr::make(TypePtr::BotPTR, TypeAry::make(TypeLong::LONG     ,TypeInt::POS), ciTypeArrayKlass::make(T_LONG),   true,  Offset::bottom);\n+  TypeAryPtr::FLOATS  = TypeAryPtr::make(TypePtr::BotPTR, TypeAry::make(Type::FLOAT        ,TypeInt::POS), ciTypeArrayKlass::make(T_FLOAT),  true,  Offset::bottom);\n+  TypeAryPtr::DOUBLES = TypeAryPtr::make(TypePtr::BotPTR, TypeAry::make(Type::DOUBLE       ,TypeInt::POS), ciTypeArrayKlass::make(T_DOUBLE), true,  Offset::bottom);\n+  TypeAryPtr::INLINES = TypeAryPtr::make(TypePtr::BotPTR, TypeAry::make(TypeInstPtr::BOTTOM,TypeInt::POS, \/* stable= *\/ false, \/* flat= *\/ true), nullptr, false, Offset::bottom);\n@@ -630,0 +683,1 @@\n+  TypeAryPtr::_array_body_type[T_FLAT_ELEMENT] = TypeAryPtr::OOPS;\n@@ -640,2 +694,2 @@\n-  TypeInstKlassPtr::OBJECT = TypeInstKlassPtr::make(TypePtr::NotNull, current->env()->Object_klass(), 0);\n-  TypeInstKlassPtr::OBJECT_OR_NULL = TypeInstKlassPtr::make(TypePtr::BotPTR, current->env()->Object_klass(), 0);\n+  TypeInstKlassPtr::OBJECT = TypeInstKlassPtr::make(TypePtr::NotNull, current->env()->Object_klass(), Offset(0));\n+  TypeInstKlassPtr::OBJECT_OR_NULL = TypeInstKlassPtr::make(TypePtr::BotPTR, current->env()->Object_klass(), Offset(0));\n@@ -680,0 +734,1 @@\n+  _const_basic_type[T_FLAT_ELEMENT] = TypeInstPtr::BOTTOM;\n@@ -696,0 +751,1 @@\n+  _zero_type[T_FLAT_ELEMENT] = TypePtr::NULL_PTR;\n@@ -970,0 +1026,3 @@\n+\n+  \/\/ Verify that:\n+  \/\/      this meet t == t meet this\n@@ -986,0 +1045,9 @@\n+  \/\/ Verify that:\n+  \/\/ 1)     mt_dual meet t_dual    == t_dual\n+  \/\/    which corresponds to\n+  \/\/       !(t meet this)  meet !t ==\n+  \/\/       (!t join !this) meet !t == !t\n+  \/\/ 2)    mt_dual meet this_dual     == this_dual\n+  \/\/    which corresponds to\n+  \/\/       !(t meet this)  meet !this ==\n+  \/\/       (!t join !this) meet !this == !this\n@@ -996,0 +1064,1 @@\n+    \/\/ 1)\n@@ -997,0 +1066,1 @@\n+    \/\/ 2)\n@@ -1034,0 +1104,3 @@\n+  \/\/ TODO 8350865 This currently triggers a verification failure, the code around \"\/\/ Even though MyValue is final\" needs adjustments\n+  if ((this_t->isa_ptr() && this_t->is_ptr()->is_not_flat()) ||\n+      (this_t->_dual->isa_ptr() && this_t->_dual->is_ptr()->is_not_flat())) return mt;\n@@ -2050,0 +2123,21 @@\n+static void collect_inline_fields(ciInlineKlass* vk, const Type** field_array, uint& pos) {\n+  for (int i = 0; i < vk->nof_declared_nonstatic_fields(); i++) {\n+    ciField* field = vk->declared_nonstatic_field_at(i);\n+    if (field->is_flat()) {\n+      collect_inline_fields(field->type()->as_inline_klass(), field_array, pos);\n+      if (!field->is_null_free()) {\n+        \/\/ Use T_INT instead of T_BOOLEAN here because the upper bits can contain garbage if the holder\n+        \/\/ is null and C2 will only zero them for T_INT assuming that T_BOOLEAN is already canonicalized.\n+        field_array[pos++] = Type::get_const_basic_type(T_INT);\n+      }\n+    } else {\n+      BasicType bt = field->type()->basic_type();\n+      const Type* ft = Type::get_const_type(field->type());\n+      field_array[pos++] = ft;\n+      if (type2size[bt] == 2) {\n+        field_array[pos++] = Type::HALF;\n+      }\n+    }\n+  }\n+}\n+\n@@ -2052,1 +2146,1 @@\n-const TypeTuple *TypeTuple::make_range(ciSignature* sig, InterfaceHandling interface_handling) {\n+const TypeTuple *TypeTuple::make_range(ciSignature* sig, InterfaceHandling interface_handling, bool ret_vt_fields) {\n@@ -2055,0 +2149,5 @@\n+  if (ret_vt_fields) {\n+    arg_cnt = return_type->as_inline_klass()->inline_arg_slots() + 1;\n+    \/\/ InlineTypeNode::NullMarker field used for null checking\n+    arg_cnt++;\n+  }\n@@ -2066,0 +2165,12 @@\n+    if (return_type->is_inlinetype() && ret_vt_fields) {\n+      uint pos = TypeFunc::Parms;\n+      field_array[pos++] = get_const_type(return_type); \/\/ Oop might be null when returning as fields\n+      collect_inline_fields(return_type->as_inline_klass(), field_array, pos);\n+      \/\/ InlineTypeNode::NullMarker field used for null checking\n+      field_array[pos++] = get_const_basic_type(T_BOOLEAN);\n+      assert(pos == (TypeFunc::Parms + arg_cnt), \"out of bounds\");\n+      break;\n+    } else {\n+      field_array[TypeFunc::Parms] = get_const_type(return_type, interface_handling)->join_speculative(TypePtr::BOTTOM);\n+    }\n+    break;\n@@ -2084,2 +2195,10 @@\n-const TypeTuple *TypeTuple::make_domain(ciInstanceKlass* recv, ciSignature* sig, InterfaceHandling interface_handling) {\n-  uint arg_cnt = sig->size();\n+const TypeTuple *TypeTuple::make_domain(ciMethod* method, InterfaceHandling interface_handling, bool vt_fields_as_args) {\n+  ciSignature* sig = method->signature();\n+  uint arg_cnt = sig->size() + (method->is_static() ? 0 : 1);\n+  if (vt_fields_as_args) {\n+    arg_cnt = 0;\n+    assert(method->get_sig_cc() != nullptr, \"Should have scalarized signature\");\n+    for (ExtendedSignature sig_cc = ExtendedSignature(method->get_sig_cc(), SigEntryFilter()); !sig_cc.at_end(); ++sig_cc) {\n+      arg_cnt += type2size[(*sig_cc)._bt];\n+    }\n+  }\n@@ -2088,8 +2207,8 @@\n-  const Type **field_array;\n-  if (recv != nullptr) {\n-    arg_cnt++;\n-    field_array = fields(arg_cnt);\n-    \/\/ Use get_const_type here because it respects UseUniqueSubclasses:\n-    field_array[pos++] = get_const_type(recv, interface_handling)->join_speculative(TypePtr::NOTNULL);\n-  } else {\n-    field_array = fields(arg_cnt);\n+  const Type** field_array = fields(arg_cnt);\n+  if (!method->is_static()) {\n+    ciInstanceKlass* recv = method->holder();\n+    if (vt_fields_as_args && recv->is_inlinetype() && recv->as_inline_klass()->can_be_passed_as_fields() && method->is_scalarized_arg(0)) {\n+      collect_inline_fields(recv->as_inline_klass(), field_array, pos);\n+    } else {\n+      field_array[pos++] = get_const_type(recv, interface_handling)->join_speculative(TypePtr::NOTNULL);\n+    }\n@@ -2101,0 +2220,1 @@\n+    BasicType bt = type->basic_type();\n@@ -2102,1 +2222,1 @@\n-    switch (type->basic_type()) {\n+    switch (bt) {\n@@ -2112,0 +2232,8 @@\n+      if (type->is_inlinetype() && vt_fields_as_args && method->is_scalarized_arg(i + (method->is_static() ? 0 : 1))) {\n+        \/\/ InlineTypeNode::NullMarker field used for null checking\n+        field_array[pos++] = get_const_basic_type(T_BOOLEAN);\n+        collect_inline_fields(type->as_inline_klass(), field_array, pos);\n+      } else {\n+        field_array[pos++] = get_const_type(type, interface_handling);\n+      }\n+      break;\n@@ -2128,0 +2256,1 @@\n+  assert(pos == TypeFunc::Parms + arg_cnt, \"wrong number of arguments\");\n@@ -2262,1 +2391,2 @@\n-const TypeAry* TypeAry::make(const Type* elem, const TypeInt* size, bool stable) {\n+const TypeAry* TypeAry::make(const Type* elem, const TypeInt* size, bool stable,\n+                             bool flat, bool not_flat, bool not_null_free, bool atomic) {\n@@ -2267,1 +2397,1 @@\n-  return (TypeAry*)(new TypeAry(elem,size,stable))->hashcons();\n+  return (TypeAry*)(new TypeAry(elem, size, stable, flat, not_flat, not_null_free, atomic))->hashcons();\n@@ -2294,1 +2424,5 @@\n-                         isize, _stable && a->_stable);\n+                         isize, _stable && a->_stable,\n+                         _flat && a->_flat,\n+                         _not_flat && a->_not_flat,\n+                         _not_null_free && a->_not_null_free,\n+                         _atomic && a->_atomic);\n@@ -2307,1 +2441,1 @@\n-  return new TypeAry(_elem->dual(), size_dual, !_stable);\n+  return new TypeAry(_elem->dual(), size_dual, !_stable, !_flat, !_not_flat, !_not_null_free, !_atomic);\n@@ -2316,1 +2450,6 @@\n-    _size == a->_size;\n+    _size == a->_size &&\n+    _flat == a->_flat &&\n+    _not_flat == a->_not_flat &&\n+    _not_null_free == a->_not_null_free &&\n+    _atomic == a->_atomic;\n+\n@@ -2322,1 +2461,2 @@\n-  return (uint)(uintptr_t)_elem + (uint)(uintptr_t)_size + (uint)(_stable ? 43 : 0);\n+  return (uint)(uintptr_t)_elem + (uint)(uintptr_t)_size + (uint)(_stable ? 43 : 0) +\n+      (uint)(_flat ? 44 : 0) + (uint)(_not_flat ? 45 : 0) + (uint)(_not_null_free ? 46 : 0) + (uint)(_atomic ? 47 : 0);\n@@ -2329,1 +2469,1 @@\n-  return make(_elem->remove_speculative(), _size, _stable);\n+  return make(_elem->remove_speculative(), _size, _stable, _flat, _not_flat, _not_null_free, _atomic);\n@@ -2336,1 +2476,1 @@\n-  return make(_elem->cleanup_speculative(), _size, _stable);\n+  return make(_elem->cleanup_speculative(), _size, _stable, _flat, _not_flat, _not_null_free, _atomic);\n@@ -2355,0 +2495,6 @@\n+  if (_flat) st->print(\"flat:\");\n+  if (Verbose) {\n+    if (_not_flat) st->print(\"not flat:\");\n+    if (_not_null_free) st->print(\"not null free:\");\n+  }\n+  if (_atomic) st->print(\"atomic:\");\n@@ -2394,2 +2540,13 @@\n-  if (tinst)\n-    return tinst->instance_klass()->is_final();\n+  if (tinst) {\n+    if (tinst->instance_klass()->is_final()) {\n+      \/\/ Even though MyValue is final, [LMyValue is only exact if the array\n+      \/\/ is (not) null-free due to null-free [LMyValue <: null-able [LMyValue.\n+      \/\/ TODO 8350865 If we know that the array can't be null-free, it's allowed to be exact, right?\n+      \/\/ If so, we should add '&& !_not_null_free'\n+      if (tinst->is_inlinetypeptr() && (tinst->ptr() != TypePtr::NotNull)) {\n+        return false;\n+      }\n+      return true;\n+    }\n+    return false;\n+  }\n@@ -2570,1 +2727,1 @@\n-const TypePtr *TypePtr::make(TYPES t, enum PTR ptr, int offset, const TypePtr* speculative, int inline_depth) {\n+const TypePtr* TypePtr::make(TYPES t, enum PTR ptr, Offset offset, const TypePtr* speculative, int inline_depth) {\n@@ -2584,1 +2741,1 @@\n-  return _offset;\n+  return offset();\n@@ -2658,7 +2815,2 @@\n-int TypePtr::meet_offset( int offset ) const {\n-  \/\/ Either is 'TOP' offset?  Return the other offset!\n-  if( _offset == OffsetTop ) return offset;\n-  if( offset == OffsetTop ) return _offset;\n-  \/\/ If either is different, return 'BOTTOM' offset\n-  if( _offset != offset ) return OffsetBot;\n-  return _offset;\n+Type::Offset TypePtr::meet_offset(int offset) const {\n+  return _offset.meet(Offset(offset));\n@@ -2668,4 +2820,2 @@\n-int TypePtr::dual_offset( ) const {\n-  if( _offset == OffsetTop ) return OffsetBot;\/\/ Map 'TOP' into 'BOTTOM'\n-  if( _offset == OffsetBot ) return OffsetTop;\/\/ Map 'BOTTOM' into 'TOP'\n-  return _offset;               \/\/ Map everything else into self\n+Type::Offset TypePtr::dual_offset() const {\n+  return _offset.dual();\n@@ -2684,13 +2834,2 @@\n-int TypePtr::xadd_offset( intptr_t offset ) const {\n-  \/\/ Adding to 'TOP' offset?  Return 'TOP'!\n-  if( _offset == OffsetTop || offset == OffsetTop ) return OffsetTop;\n-  \/\/ Adding to 'BOTTOM' offset?  Return 'BOTTOM'!\n-  if( _offset == OffsetBot || offset == OffsetBot ) return OffsetBot;\n-  \/\/ Addition overflows or \"accidentally\" equals to OffsetTop? Return 'BOTTOM'!\n-  offset += (intptr_t)_offset;\n-  if (offset != (int)offset || offset == OffsetTop) return OffsetBot;\n-\n-  \/\/ assert( _offset >= 0 && _offset+offset >= 0, \"\" );\n-  \/\/ It is possible to construct a negative offset during PhaseCCP\n-\n-  return (int)offset;        \/\/ Sum valid offsets\n+Type::Offset TypePtr::xadd_offset(intptr_t offset) const {\n+  return _offset.add(offset);\n@@ -2705,1 +2844,1 @@\n-  return make(AnyPtr, _ptr, offset, _speculative, _inline_depth);\n+  return make(AnyPtr, _ptr, Offset(offset), _speculative, _inline_depth);\n@@ -2712,1 +2851,1 @@\n-  return _ptr == a->ptr() && _offset == a->offset() && eq_speculative(a) && _inline_depth == a->_inline_depth;\n+  return _ptr == a->ptr() && _offset == a->_offset && eq_speculative(a) && _inline_depth == a->_inline_depth;\n@@ -2718,1 +2857,1 @@\n-  return (uint)_ptr + (uint)_offset + (uint)hash_speculative() + (uint)_inline_depth;\n+  return (uint)_ptr + (uint)offset() + (uint)hash_speculative() + (uint)_inline_depth;\n@@ -2984,3 +3123,1 @@\n-  if( _offset == OffsetTop ) st->print(\"+top\");\n-  else if( _offset == OffsetBot ) st->print(\"+bot\");\n-  else if( _offset ) st->print(\"+%d\", _offset);\n+  _offset.dump2(st);\n@@ -3021,1 +3158,1 @@\n-  return (_offset != OffsetBot) && !below_centerline(_ptr);\n+  return (_offset != Offset::bottom) && !below_centerline(_ptr);\n@@ -3025,1 +3162,1 @@\n-  return (_offset == OffsetTop) || above_centerline(_ptr);\n+  return (_offset == Offset::top) || above_centerline(_ptr);\n@@ -3432,1 +3569,1 @@\n-TypeOopPtr::TypeOopPtr(TYPES t, PTR ptr, ciKlass* k, const TypeInterfaces* interfaces, bool xk, ciObject* o, int offset,\n+TypeOopPtr::TypeOopPtr(TYPES t, PTR ptr, ciKlass* k, const TypeInterfaces* interfaces, bool xk, ciObject* o, Offset offset, Offset field_offset,\n@@ -3448,2 +3585,2 @@\n-      (offset > 0) && xk && (k != nullptr) && k->is_instance_klass()) {\n-    _is_ptr_to_boxed_value = k->as_instance_klass()->is_boxed_value_offset(offset);\n+      (offset.get() > 0) && xk && (k != nullptr) && k->is_instance_klass()) {\n+    _is_ptr_to_boxed_value = k->as_instance_klass()->is_boxed_value_offset(offset.get());\n@@ -3452,2 +3589,2 @@\n-  if (_offset > 0 || _offset == Type::OffsetTop || _offset == Type::OffsetBot) {\n-    if (_offset == oopDesc::klass_offset_in_bytes()) {\n+  if (this->offset() > 0 || this->offset() == Type::OffsetTop || this->offset() == Type::OffsetBot) {\n+    if (this->offset() == oopDesc::klass_offset_in_bytes()) {\n@@ -3459,3 +3596,17 @@\n-    } else if (this->isa_aryptr()) {\n-      _is_ptr_to_narrowoop = (UseCompressedOops && klass()->is_obj_array_klass() &&\n-                             _offset != arrayOopDesc::length_offset_in_bytes());\n+    } else if (UseCompressedOops && this->isa_aryptr() && this->offset() != arrayOopDesc::length_offset_in_bytes()) {\n+      if (klass()->is_obj_array_klass()) {\n+        _is_ptr_to_narrowoop = true;\n+      } else if (klass()->is_flat_array_klass() && field_offset != Offset::top && field_offset != Offset::bottom) {\n+        \/\/ Check if the field of the inline type array element contains oops\n+        ciInlineKlass* vk = klass()->as_flat_array_klass()->element_klass()->as_inline_klass();\n+        int foffset = field_offset.get() + vk->payload_offset();\n+        BasicType field_bt;\n+        ciField* field = vk->get_field_by_offset(foffset, false);\n+        if (field != nullptr) {\n+          field_bt = field->layout_type();\n+        } else {\n+          assert(field_offset.get() == vk->null_marker_offset_in_payload(), \"no field or null marker of %s at offset %d\", vk->name()->as_utf8(), foffset);\n+          field_bt = T_BOOLEAN;\n+        }\n+        _is_ptr_to_narrowoop = UseCompressedOops && ::is_reference_type(field_bt);\n+      }\n@@ -3463,1 +3614,0 @@\n-      ciInstanceKlass* ik = klass()->as_instance_klass();\n@@ -3466,1 +3616,1 @@\n-      } else if (_offset == OffsetBot || _offset == OffsetTop) {\n+      } else if (_offset == Offset::bottom || _offset == Offset::top) {\n@@ -3471,3 +3621,2 @@\n-\n-            (_offset == java_lang_Class::klass_offset() ||\n-             _offset == java_lang_Class::array_klass_offset())) {\n+            (this->offset() == java_lang_Class::klass_offset() ||\n+             this->offset() == java_lang_Class::array_klass_offset())) {\n@@ -3479,1 +3628,1 @@\n-                   _offset >= InstanceMirrorKlass::offset_of_static_fields()) {\n+                   this->offset() >= InstanceMirrorKlass::offset_of_static_fields()) {\n@@ -3484,1 +3633,1 @@\n-            field = k->get_field_by_offset(_offset, true);\n+            field = k->get_field_by_offset(this->offset(), true);\n@@ -3495,1 +3644,2 @@\n-          ciField* field = ik->get_field_by_offset(_offset, false);\n+          ciInstanceKlass* ik = klass()->as_instance_klass();\n+          ciField* field = ik->get_field_by_offset(this->offset(), false);\n@@ -3515,2 +3665,2 @@\n-const TypeOopPtr *TypeOopPtr::make(PTR ptr, int offset, int instance_id,\n-                                     const TypePtr* speculative, int inline_depth) {\n+const TypeOopPtr *TypeOopPtr::make(PTR ptr, Offset offset, int instance_id,\n+                                   const TypePtr* speculative, int inline_depth) {\n@@ -3522,1 +3672,1 @@\n-  return (TypeOopPtr*)(new TypeOopPtr(OopPtr, ptr, k, interfaces, xk, o, offset, instance_id, speculative, inline_depth))->hashcons();\n+  return (TypeOopPtr*)(new TypeOopPtr(OopPtr, ptr, k, interfaces, xk, o, offset, Offset::bottom, instance_id, speculative, inline_depth))->hashcons();\n@@ -3547,1 +3697,0 @@\n-\n@@ -3596,1 +3745,1 @@\n-    int offset = meet_offset(tp->offset());\n+    Offset offset = meet_offset(tp->offset());\n@@ -3638,1 +3787,1 @@\n-  return new TypeOopPtr(_base, dual_ptr(), klass(), _interfaces, klass_is_exact(), const_oop(), dual_offset(), dual_instance_id(), dual_speculative(), dual_inline_depth());\n+  return new TypeOopPtr(_base, dual_ptr(), klass(), _interfaces, klass_is_exact(), const_oop(), dual_offset(), Offset::bottom, dual_instance_id(), dual_speculative(), dual_inline_depth());\n@@ -3643,2 +3792,2 @@\n-const TypeOopPtr* TypeOopPtr::make_from_klass_common(ciKlass* klass, bool klass_change, bool try_for_exact, InterfaceHandling interface_handling) {\n-  if (klass->is_instance_klass()) {\n+const TypeOopPtr* TypeOopPtr::make_from_klass_common(ciKlass *klass, bool klass_change, bool try_for_exact, InterfaceHandling interface_handling) {\n+  if (klass->is_instance_klass() || klass->is_inlinetype()) {\n@@ -3671,1 +3820,1 @@\n-    return TypeInstPtr::make(TypePtr::BotPTR, klass, interfaces, klass_is_exact, nullptr, 0);\n+    return TypeInstPtr::make(TypePtr::BotPTR, klass, interfaces, klass_is_exact, nullptr, Offset(0));\n@@ -3673,5 +3822,19 @@\n-    \/\/ Element is an object array. Recursively call ourself.\n-    ciKlass* eklass = klass->as_obj_array_klass()->element_klass();\n-    const TypeOopPtr *etype = TypeOopPtr::make_from_klass_common(eklass, false, try_for_exact, interface_handling);\n-    bool xk = etype->klass_is_exact();\n-    const TypeAry* arr0 = TypeAry::make(etype, TypeInt::POS);\n+    \/\/ Element is an object or inline type array. Recursively call ourself.\n+    const TypeOopPtr* etype = TypeOopPtr::make_from_klass_common(klass->as_array_klass()->element_klass(), \/* klass_change= *\/ false, try_for_exact, interface_handling);\n+    \/\/ Determine null-free\/flat properties\n+    const bool is_null_free = klass->as_array_klass()->is_elem_null_free();\n+    if (is_null_free) {\n+      etype = etype->join_speculative(NOTNULL)->is_oopptr();\n+    }\n+    const TypeOopPtr* exact_etype = etype;\n+    if (etype->can_be_inline_type()) {\n+      \/\/ Use exact type if element can be an inline type\n+      exact_etype = TypeOopPtr::make_from_klass_common(klass->as_array_klass()->element_klass(), \/* klass_change= *\/ true, \/* try_for_exact= *\/ true, interface_handling);\n+    }\n+    bool not_inline = !exact_etype->can_be_inline_type();\n+    bool not_null_free = not_inline;\n+    bool not_flat = !UseArrayFlattening || not_inline || (exact_etype->is_inlinetypeptr() && !exact_etype->inline_klass()->maybe_flat_in_array());\n+    bool atomic = klass->as_array_klass()->is_elem_atomic();\n+    \/\/ Even though MyValue is final, [LMyValue is not exact because null-free [LMyValue is a subtype.\n+    bool xk = etype->klass_is_exact() && !etype->is_inlinetypeptr();\n+    const TypeAry* arr0 = TypeAry::make(etype, TypeInt::POS, \/* stable= *\/ false, \/* flat= *\/ false, not_flat, not_null_free, atomic);\n@@ -3680,2 +3843,2 @@\n-    \/\/ slam nulls down in the subarrays.\n-    const TypeAryPtr* arr = TypeAryPtr::make(TypePtr::BotPTR, arr0, nullptr, xk, 0);\n+    \/\/ slam nullptrs down in the subarrays.\n+    const TypeAryPtr* arr = TypeAryPtr::make(TypePtr::BotPTR, arr0, nullptr, xk, Offset(0));\n@@ -3686,1 +3849,2 @@\n-    const TypeAry* arr0 = TypeAry::make(etype, TypeInt::POS);\n+    const TypeAry* arr0 = TypeAry::make(etype, TypeInt::POS,\n+                                        \/* stable= *\/ false, \/* flat= *\/ false, \/* not_flat= *\/ true, \/* not_null_free= *\/ true);\n@@ -3689,1 +3853,12 @@\n-    const TypeAryPtr* arr = TypeAryPtr::make(TypePtr::BotPTR, arr0, klass, true, 0);\n+    const TypeAryPtr* arr = TypeAryPtr::make(TypePtr::BotPTR, arr0, klass, true, Offset(0));\n+    return arr;\n+  } else if (klass->is_flat_array_klass()) {\n+    const TypeOopPtr* etype = TypeOopPtr::make_from_klass_raw(klass->as_array_klass()->element_klass(), trust_interfaces);\n+    const bool is_null_free = klass->as_array_klass()->is_elem_null_free();\n+    if (is_null_free) {\n+      etype = etype->join_speculative(NOTNULL)->is_oopptr();\n+    }\n+    bool atomic = klass->as_array_klass()->is_elem_atomic();\n+    const TypeAry* arr0 = TypeAry::make(etype, TypeInt::POS, \/* stable= *\/ false, \/* flat= *\/ true, \/* not_flat= *\/ false, \/* not_null_free= *\/ false, atomic);\n+    const bool exact = is_null_free; \/\/ Only exact if null-free because \"null-free [LMyValue <: null-able [LMyValue\".\n+    const TypeAryPtr* arr = TypeAryPtr::make(TypePtr::BotPTR, arr0, klass, exact, Offset(0));\n@@ -3705,2 +3880,2 @@\n-  if (klass->is_instance_klass()) {\n-    \/\/ Element is an instance\n+  if (klass->is_instance_klass() || klass->is_inlinetype()) {\n+    \/\/ Element is an instance or inline type\n@@ -3710,1 +3885,1 @@\n-      return TypeInstPtr::make(TypePtr::NotNull, klass, true, nullptr, 0);\n+      return TypeInstPtr::make(TypePtr::NotNull, klass, true, nullptr, Offset(0));\n@@ -3714,3 +3889,9 @@\n-    const TypeOopPtr *etype =\n-      TypeOopPtr::make_from_klass_raw(klass->as_obj_array_klass()->element_klass(), trust_interfaces);\n-    const TypeAry* arr0 = TypeAry::make(etype, TypeInt::make(o->as_array()->length()));\n+    const TypeOopPtr* etype = TypeOopPtr::make_from_klass_raw(klass->as_array_klass()->element_klass(), trust_interfaces);\n+    bool is_flat = o->as_array()->is_flat();\n+    bool is_null_free = o->as_array()->is_null_free();\n+    if (is_null_free) {\n+      etype = etype->join_speculative(TypePtr::NOTNULL)->is_oopptr();\n+    }\n+    bool is_atomic = o->as_array()->is_atomic();\n+    const TypeAry* arr0 = TypeAry::make(etype, TypeInt::make(o->as_array()->length()), \/* stable= *\/ false, \/* flat= *\/ false,\n+                                        \/* not_flat= *\/ !is_flat, \/* not_null_free= *\/ !is_null_free, \/* atomic= *\/ is_atomic);\n@@ -3721,1 +3902,1 @@\n-      return TypeAryPtr::make(TypePtr::Constant, o, arr0, klass, true, 0);\n+      return TypeAryPtr::make(TypePtr::Constant, o, arr0, klass, true, Offset(0));\n@@ -3723,1 +3904,1 @@\n-      return TypeAryPtr::make(TypePtr::NotNull, arr0, klass, true, 0);\n+      return TypeAryPtr::make(TypePtr::NotNull, arr0, klass, true, Offset(0));\n@@ -3727,3 +3908,3 @@\n-    const Type* etype =\n-      (Type*)get_const_basic_type(klass->as_type_array_klass()->element_type());\n-    const TypeAry* arr0 = TypeAry::make(etype, TypeInt::make(o->as_array()->length()));\n+    const Type* etype = (Type*)get_const_basic_type(klass->as_type_array_klass()->element_type());\n+    const TypeAry* arr0 = TypeAry::make(etype, TypeInt::make(o->as_array()->length()), \/* stable= *\/ false, \/* flat= *\/ false,\n+                                        \/* not_flat= *\/ true, \/* not_null_free= *\/ true);\n@@ -3733,1 +3914,18 @@\n-      return TypeAryPtr::make(TypePtr::Constant, o, arr0, klass, true, 0);\n+      return TypeAryPtr::make(TypePtr::Constant, o, arr0, klass, true, Offset(0));\n+    } else {\n+      return TypeAryPtr::make(TypePtr::NotNull, arr0, klass, true, Offset(0));\n+    }\n+  } else if (klass->is_flat_array_klass()) {\n+    const TypeOopPtr* etype = TypeOopPtr::make_from_klass_raw(klass->as_array_klass()->element_klass(), trust_interfaces);\n+    bool is_null_free = o->as_array()->is_null_free();\n+    if (is_null_free) {\n+      etype = etype->join_speculative(TypePtr::NOTNULL)->is_oopptr();\n+    }\n+    bool is_atomic = o->as_array()->is_atomic();\n+    const TypeAry* arr0 = TypeAry::make(etype, TypeInt::make(o->as_array()->length()), \/* stable= *\/ false, \/* flat= *\/ true,\n+                                        \/* not_flat= *\/ false, \/* not_null_free= *\/ !is_null_free, \/* atomic= *\/ is_atomic);\n+    \/\/ We used to pass NotNull in here, asserting that the sub-arrays\n+    \/\/ are all not-null.  This is not true in generally, as code can\n+    \/\/ slam nullptrs down in the subarrays.\n+    if (make_constant) {\n+      return TypeAryPtr::make(TypePtr::Constant, o, arr0, klass, true, Offset(0));\n@@ -3735,1 +3933,1 @@\n-      return TypeAryPtr::make(TypePtr::NotNull, arr0, klass, true, 0);\n+      return TypeAryPtr::make(TypePtr::NotNull, arr0, klass, true, Offset(0));\n@@ -3746,1 +3944,1 @@\n-  assert( _offset >= 0, \"\" );\n+  assert(offset() >= 0, \"\");\n@@ -3748,1 +3946,1 @@\n-  if (_offset != 0) {\n+  if (offset() != 0) {\n@@ -3807,6 +4005,1 @@\n-  switch( _offset ) {\n-  case OffsetTop: st->print(\"+top\"); break;\n-  case OffsetBot: st->print(\"+any\"); break;\n-  case         0: break;\n-  default:        st->print(\"+%d\",_offset); break;\n-  }\n+  _offset.dump2(st);\n@@ -3829,1 +4022,1 @@\n-  return (_offset == 0) && !below_centerline(_ptr);\n+  return (offset() == 0) && !below_centerline(_ptr);\n@@ -3838,1 +4031,1 @@\n-  return make(_ptr, offset, _instance_id, with_offset_speculative(offset), _inline_depth);\n+  return make(_ptr, Offset(offset), _instance_id, with_offset_speculative(offset), _inline_depth);\n@@ -3951,3 +4144,4 @@\n-TypeInstPtr::TypeInstPtr(PTR ptr, ciKlass* k, const TypeInterfaces* interfaces, bool xk, ciObject* o, int off,\n-                         int instance_id, const TypePtr* speculative, int inline_depth)\n-  : TypeOopPtr(InstPtr, ptr, k, interfaces, xk, o, off, instance_id, speculative, inline_depth) {\n+TypeInstPtr::TypeInstPtr(PTR ptr, ciKlass* k, const TypeInterfaces* interfaces, bool xk, ciObject* o, Offset off,\n+                         bool flat_in_array, int instance_id, const TypePtr* speculative, int inline_depth)\n+  : TypeOopPtr(InstPtr, ptr, k, interfaces, xk, o, off, Offset::bottom, instance_id, speculative, inline_depth),\n+    _flat_in_array(flat_in_array) {\n@@ -3958,0 +4152,2 @@\n+  assert(!klass()->maybe_flat_in_array() || flat_in_array, \"Should be flat in array\");\n+  assert(!flat_in_array || can_be_inline_type(), \"Only inline types can be flat in array\");\n@@ -3966,1 +4162,2 @@\n-                                     int offset,\n+                                     Offset offset,\n+                                     bool flat_in_array,\n@@ -3988,0 +4185,3 @@\n+  \/\/ Check if this type is known to be flat in arrays\n+  flat_in_array = flat_in_array || k->maybe_flat_in_array();\n+\n@@ -3990,1 +4190,1 @@\n-    (TypeInstPtr*)(new TypeInstPtr(ptr, k, interfaces, xk, o ,offset, instance_id, speculative, inline_depth))->hashcons();\n+    (TypeInstPtr*)(new TypeInstPtr(ptr, k, interfaces, xk, o, offset, flat_in_array, instance_id, speculative, inline_depth))->hashcons();\n@@ -4056,1 +4256,1 @@\n-  return make(ptr, klass(), _interfaces, klass_is_exact(), ptr == Constant ? const_oop() : nullptr, _offset, _instance_id, _speculative, _inline_depth);\n+  return make(ptr, klass(), _interfaces, klass_is_exact(), ptr == Constant ? const_oop() : nullptr, _offset, _flat_in_array, _instance_id, _speculative, _inline_depth);\n@@ -4067,1 +4267,1 @@\n-  return make(ptr(), klass(), _interfaces, klass_is_exact, const_oop(), _offset, _instance_id, _speculative, _inline_depth);\n+  return make(ptr(), klass(), _interfaces, klass_is_exact, const_oop(), _offset, _flat_in_array, _instance_id, _speculative, _inline_depth);\n@@ -4073,1 +4273,1 @@\n-  return make(_ptr, klass(),  _interfaces, _klass_is_exact, const_oop(), _offset, instance_id, _speculative, _inline_depth);\n+  return make(_ptr, klass(), _interfaces, _klass_is_exact, const_oop(), _offset, _flat_in_array, instance_id, _speculative, _inline_depth);\n@@ -4080,1 +4280,1 @@\n-  int off = meet_offset(tinst->offset());\n+  Offset off = meet_offset(tinst->offset());\n@@ -4105,1 +4305,1 @@\n-    else if (loaded->ptr() == TypePtr::AnyNull)  { return make(ptr, unloaded->klass(), interfaces, false, nullptr, off, instance_id, speculative, depth); }\n+    else if (loaded->ptr() == TypePtr::AnyNull)  { return make(ptr, unloaded->klass(), interfaces, false, nullptr, off, false, instance_id, speculative, depth); }\n@@ -4169,1 +4369,1 @@\n-    int offset = meet_offset(tp->offset());\n+    Offset offset = meet_offset(tp->offset());\n@@ -4178,1 +4378,1 @@\n-                  (ptr == Constant ? const_oop() : nullptr), offset, instance_id, speculative, depth);\n+                  (ptr == Constant ? const_oop() : nullptr), offset, flat_in_array(), instance_id, speculative, depth);\n@@ -4194,1 +4394,1 @@\n-    int offset = meet_offset(tp->offset());\n+    Offset offset = meet_offset(tp->offset());\n@@ -4206,1 +4406,1 @@\n-                  (ptr == Constant ? const_oop() : nullptr), offset, instance_id, speculative, depth);\n+                  (ptr == Constant ? const_oop() : nullptr), offset, flat_in_array(), instance_id, speculative, depth);\n@@ -4234,1 +4434,1 @@\n-    int off = meet_offset(tinst->offset());\n+    Offset off = meet_offset(tinst->offset());\n@@ -4246,0 +4446,1 @@\n+    bool res_flat_in_array = false;\n@@ -4247,1 +4448,1 @@\n-    MeetResult kind = meet_instptr(ptr, interfaces, this, tinst, res_klass, res_xk);\n+    MeetResult kind = meet_instptr(ptr, interfaces, this, tinst, res_klass, res_xk, res_flat_in_array);\n@@ -4288,1 +4489,1 @@\n-      res = make(ptr, res_klass, interfaces, res_xk, o, off, instance_id, speculative, depth);\n+      res = make(ptr, res_klass, interfaces, res_xk, o, off, res_flat_in_array, instance_id, speculative, depth);\n@@ -4300,1 +4501,1 @@\n-                                                            ciKlass*& res_klass, bool& res_xk) {\n+                                                            ciKlass*& res_klass, bool& res_xk, bool& res_flat_in_array) {\n@@ -4303,0 +4504,5 @@\n+  const bool this_flat_in_array = this_type->flat_in_array();\n+  const bool other_flat_in_array = other_type->flat_in_array();\n+  const bool this_not_flat_in_array = this_type->not_flat_in_array();\n+  const bool other_not_flat_in_array = other_type->not_flat_in_array();\n+\n@@ -4313,1 +4519,1 @@\n-  if (ptr != Constant && this_klass->equals(other_klass) && this_xk == other_xk) {\n+  if (ptr != Constant && this_klass->equals(other_klass) && this_xk == other_xk && this_flat_in_array == other_flat_in_array) {\n@@ -4316,0 +4522,1 @@\n+    res_flat_in_array = this_flat_in_array;\n@@ -4348,1 +4555,57 @@\n-  \/\/ Check for subtyping:\n+  \/\/ Flat in Array property _flat_in_array.\n+  \/\/ For simplicity, _flat_in_array is a boolean but we actually have a tri state:\n+  \/\/ - Flat in array       -> flat_in_array()\n+  \/\/ - Not flat in array   -> not_flat_in_array()\n+  \/\/ - Maybe flat in array -> !not_flat_in_array()\n+  \/\/\n+  \/\/ Maybe we should convert _flat_in_array to a proper lattice with four elements at some point:\n+  \/\/\n+  \/\/                  Top\n+  \/\/    Flat in Array     Not Flat in Array\n+  \/\/          Maybe Flat in Array\n+  \/\/\n+  \/\/ where\n+  \/\/     Top = dual(maybe Flat In Array) = \"Flat in Array AND Not Flat in Array\"\n+  \/\/\n+  \/\/ But for now we stick with the current model with _flat_in_array as a boolean.\n+  \/\/\n+  \/\/ When meeting two InstPtr types, we want to have the following behavior:\n+  \/\/\n+  \/\/ (FiA-M) Meet(this, other):\n+  \/\/     'this' and 'other' are either the same klass OR sub klasses:\n+  \/\/\n+  \/\/                yes maybe no\n+  \/\/           yes   y    m    m                      y = Flat in Array\n+  \/\/         maybe   m    m    m                      n = Not Flat in Array\n+  \/\/            no   m    m    n                      m = Maybe Flat in Array\n+  \/\/\n+  \/\/  Join(this, other):\n+  \/\/     (FiA-J-Same) 'this' and 'other' are the SAME klass:\n+  \/\/\n+  \/\/                yes maybe no                      E = Empty set\n+  \/\/           yes   y    y    E                      y = Flat in Array\n+  \/\/         maybe   y    m    m                      n = Not Flat in Array\n+  \/\/            no   E    m    n                      m = Maybe Flat in Array\n+  \/\/\n+  \/\/     (FiA-J-Sub) 'this' and 'other' are SUB klasses:\n+  \/\/\n+  \/\/               yes maybe no   -> Super Klass      E = Empty set\n+  \/\/          yes   y    y    y                       y = Flat in Array\n+  \/\/        maybe   y    m    m                       n = Not Flat in Array\n+  \/\/           no   E    m    n                       m = Maybe Flat in Array\n+  \/\/           |\n+  \/\/           v\n+  \/\/       Sub Klass\n+  \/\/\n+  \/\/     Note the difference when joining a super klass that is not flat in array with a sub klass that is compared to\n+  \/\/     the same klass case. We will take over the flat in array property of the sub klass. This can be done because\n+  \/\/     the super klass could be Object (i.e. not an inline type and thus not flat in array) while the sub klass is a\n+  \/\/     value class which can be flat in array.\n+  \/\/\n+  \/\/     The empty set is only a possible result when matching 'ptr' above the center line (i.e. joining). In this case,\n+  \/\/     we can \"fall hard\" by setting 'ptr' to NotNull such that when we take the dual of that meet above the center\n+  \/\/     line, we get an empty set again.\n+  \/\/\n+  \/\/     Note: When changing to a separate lattice with _flat_in_array we may want to add TypeInst(Klass)Ptr::empty()\n+  \/\/           that returns true when the meet result is FlatInArray::Top (i.e. dual(maybe flat in array)).\n+\n@@ -4351,0 +4614,2 @@\n+  bool flat_in_array = false;\n+  bool is_empty = false;\n@@ -4352,0 +4617,1 @@\n+    \/\/ Same klass\n@@ -4354,1 +4620,6 @@\n-  } else if (!other_xk && this_type->is_meet_subtype_of(other_type)) {\n+    if (above_centerline(ptr)) {\n+      \/\/ Case (FiA-J-Same)\n+      \/\/ One is flat in array and the other not? Result is empty\/\"fall hard\".\n+      is_empty = (this_flat_in_array && other_not_flat_in_array) || (this_not_flat_in_array && other_flat_in_array);\n+    }\n+  } else if (!other_xk && is_meet_subtype_of(this_type, other_type)) {\n@@ -4357,1 +4628,9 @@\n-  } else if(!this_xk && other_type->is_meet_subtype_of(this_type)) {\n+    if (above_centerline(ptr)) {\n+      \/\/ Case (FiA-J-Sub)\n+      is_empty = this_not_flat_in_array && other_flat_in_array;\n+      if (!is_empty) {\n+        bool other_flat_this_maybe_flat = other_flat_in_array && (!this_flat_in_array && !this_not_flat_in_array);\n+        flat_in_array = this_flat_in_array || other_flat_this_maybe_flat;\n+      }\n+    }\n+  } else if (!this_xk && is_meet_subtype_of(other_type, this_type)) {\n@@ -4360,0 +4639,8 @@\n+    if (above_centerline(ptr)) {\n+      \/\/ Case (FiA-J-Sub)\n+      is_empty = this_flat_in_array && other_not_flat_in_array;\n+      if (!is_empty) {\n+        bool this_flat_other_maybe_flat = this_flat_in_array && (!other_flat_in_array && !other_not_flat_in_array);\n+        flat_in_array = other_flat_in_array || this_flat_other_maybe_flat;\n+      }\n+    }\n@@ -4362,2 +4649,4 @@\n-  if (subtype) {\n-    if (above_centerline(ptr)) { \/\/ both are up?\n+\n+  if (subtype && !is_empty) {\n+    if (above_centerline(ptr)) {\n+      \/\/ Both types are empty.\n@@ -4366,0 +4655,5 @@\n+      \/\/ Case (FiA-J-Sub)\n+      bool other_flat_this_maybe_flat = other_flat_in_array && (!this_flat_in_array && !this_not_flat_in_array);\n+      flat_in_array = this_flat_in_array || other_flat_this_maybe_flat;\n+      \/\/ One is flat in array and the other not? Result is empty\/\"fall hard\".\n+      is_empty = (this_flat_in_array && other_not_flat_in_array) || (this_not_flat_in_array && other_flat_in_array);\n@@ -4367,1 +4661,2 @@\n-      this_type = other_type; \/\/ tinst is down; keep down man\n+      \/\/ this_type is empty while other_type is not. Take other_type.\n+      this_type = other_type;\n@@ -4369,0 +4664,1 @@\n+      flat_in_array = other_flat_in_array;\n@@ -4370,0 +4666,1 @@\n+      \/\/ other_type is empty while this_type is not. Take this_type.\n@@ -4371,1 +4668,1 @@\n-      other_xk = this_xk;\n+      flat_in_array = this_flat_in_array;\n@@ -4373,0 +4670,1 @@\n+      \/\/ this_type and other_type are both non-empty.\n@@ -4374,0 +4672,3 @@\n+      \/\/ Case (FiA-M)\n+      \/\/ Meeting two types below the center line: Only flat in array if both are.\n+      flat_in_array = this_flat_in_array && other_flat_in_array;\n@@ -4378,1 +4679,1 @@\n-  if (this_type->is_same_java_type_as(other_type)) {\n+  if (this_type->is_same_java_type_as(other_type) && !is_empty) {\n@@ -4384,0 +4685,1 @@\n+    res_flat_in_array = flat_in_array;\n@@ -4400,0 +4702,1 @@\n+  res_flat_in_array = this_flat_in_array && other_flat_in_array;\n@@ -4404,0 +4707,4 @@\n+template<class T> bool TypePtr::is_meet_subtype_of(const T* sub_type, const T* super_type) {\n+  return sub_type->is_meet_subtype_of(super_type) && !(super_type->flat_in_array() && sub_type->not_flat_in_array());\n+}\n+\n@@ -4411,1 +4718,0 @@\n-\n@@ -4420,1 +4726,1 @@\n-  return new TypeInstPtr(dual_ptr(), klass(), _interfaces, klass_is_exact(), const_oop(), dual_offset(), dual_instance_id(), dual_speculative(), dual_inline_depth());\n+  return new TypeInstPtr(dual_ptr(), klass(), _interfaces, klass_is_exact(), const_oop(), dual_offset(), flat_in_array(), dual_instance_id(), dual_speculative(), dual_inline_depth());\n@@ -4429,0 +4735,1 @@\n+    flat_in_array() == p->flat_in_array() &&\n@@ -4436,1 +4743,1 @@\n-  return klass()->hash() + TypeOopPtr::hash() + _interfaces->hash();\n+  return klass()->hash() + TypeOopPtr::hash() + _interfaces->hash() + (uint)flat_in_array();\n@@ -4490,5 +4797,1 @@\n-  if( _offset ) {               \/\/ Dump offset, if any\n-    if( _offset == OffsetBot )      st->print(\"+any\");\n-    else if( _offset == OffsetTop ) st->print(\"+unknown\");\n-    else st->print(\"+%d\", _offset);\n-  }\n+  _offset.dump2(st);\n@@ -4497,0 +4800,5 @@\n+\n+  if (flat_in_array() && !klass()->is_inlinetype()) {\n+    st->print(\" (flat in array)\");\n+  }\n+\n@@ -4509,1 +4817,1 @@\n-  return make(_ptr, klass(), _interfaces, klass_is_exact(), const_oop(), xadd_offset(offset),\n+  return make(_ptr, klass(), _interfaces, klass_is_exact(), const_oop(), xadd_offset(offset), flat_in_array(),\n@@ -4514,1 +4822,1 @@\n-  return make(_ptr, klass(), _interfaces, klass_is_exact(), const_oop(), offset,\n+  return make(_ptr, klass(), _interfaces, klass_is_exact(), const_oop(), Offset(offset), flat_in_array(),\n@@ -4523,1 +4831,1 @@\n-  return make(_ptr, klass(), _interfaces, klass_is_exact(), const_oop(), _offset,\n+  return make(_ptr, klass(), _interfaces, klass_is_exact(), const_oop(), _offset, flat_in_array(),\n@@ -4528,1 +4836,1 @@\n-  return make(_ptr, klass(), _interfaces, klass_is_exact(), const_oop(), _offset, _instance_id, speculative, _inline_depth);\n+  return make(_ptr, klass(), _interfaces, klass_is_exact(), const_oop(), _offset, flat_in_array(), _instance_id, speculative, _inline_depth);\n@@ -4535,1 +4843,1 @@\n-  return make(_ptr, klass(), _interfaces, klass_is_exact(), const_oop(), _offset, _instance_id, _speculative, depth);\n+  return make(_ptr, klass(), _interfaces, klass_is_exact(), const_oop(), _offset, flat_in_array(), _instance_id, _speculative, depth);\n@@ -4540,1 +4848,5 @@\n-  return make(_ptr, klass(), _interfaces, klass_is_exact(), const_oop(), _offset, instance_id, _speculative, _inline_depth);\n+  return make(_ptr, klass(), _interfaces, klass_is_exact(), const_oop(), _offset, flat_in_array(), instance_id, _speculative, _inline_depth);\n+}\n+\n+const TypeInstPtr *TypeInstPtr::cast_to_flat_in_array() const {\n+  return make(_ptr, klass(), _interfaces, klass_is_exact(), const_oop(), _offset, true, _instance_id, _speculative, _inline_depth);\n@@ -4554,1 +4866,1 @@\n-  return TypeInstKlassPtr::make(xk ? TypePtr::Constant : TypePtr::NotNull, klass(), _interfaces, 0);\n+  return TypeInstKlassPtr::make(xk ? TypePtr::Constant : TypePtr::NotNull, klass(), _interfaces, Offset(0), flat_in_array());\n@@ -4599,1 +4911,0 @@\n-\n@@ -4622,10 +4933,11 @@\n-const TypeAryPtr* TypeAryPtr::RANGE;\n-const TypeAryPtr* TypeAryPtr::OOPS;\n-const TypeAryPtr* TypeAryPtr::NARROWOOPS;\n-const TypeAryPtr* TypeAryPtr::BYTES;\n-const TypeAryPtr* TypeAryPtr::SHORTS;\n-const TypeAryPtr* TypeAryPtr::CHARS;\n-const TypeAryPtr* TypeAryPtr::INTS;\n-const TypeAryPtr* TypeAryPtr::LONGS;\n-const TypeAryPtr* TypeAryPtr::FLOATS;\n-const TypeAryPtr* TypeAryPtr::DOUBLES;\n+const TypeAryPtr *TypeAryPtr::RANGE;\n+const TypeAryPtr *TypeAryPtr::OOPS;\n+const TypeAryPtr *TypeAryPtr::NARROWOOPS;\n+const TypeAryPtr *TypeAryPtr::BYTES;\n+const TypeAryPtr *TypeAryPtr::SHORTS;\n+const TypeAryPtr *TypeAryPtr::CHARS;\n+const TypeAryPtr *TypeAryPtr::INTS;\n+const TypeAryPtr *TypeAryPtr::LONGS;\n+const TypeAryPtr *TypeAryPtr::FLOATS;\n+const TypeAryPtr *TypeAryPtr::DOUBLES;\n+const TypeAryPtr *TypeAryPtr::INLINES;\n@@ -4634,1 +4946,1 @@\n-const TypeAryPtr *TypeAryPtr::make(PTR ptr, const TypeAry *ary, ciKlass* k, bool xk, int offset,\n+const TypeAryPtr* TypeAryPtr::make(PTR ptr, const TypeAry *ary, ciKlass* k, bool xk, Offset offset, Offset field_offset,\n@@ -4644,1 +4956,1 @@\n-  return (TypeAryPtr*)(new TypeAryPtr(ptr, nullptr, ary, k, xk, offset, instance_id, false, speculative, inline_depth))->hashcons();\n+  return (TypeAryPtr*)(new TypeAryPtr(ptr, nullptr, ary, k, xk, offset, field_offset, instance_id, false, speculative, inline_depth))->hashcons();\n@@ -4648,1 +4960,1 @@\n-const TypeAryPtr *TypeAryPtr::make(PTR ptr, ciObject* o, const TypeAry *ary, ciKlass* k, bool xk, int offset,\n+const TypeAryPtr* TypeAryPtr::make(PTR ptr, ciObject* o, const TypeAry *ary, ciKlass* k, bool xk, Offset offset, Offset field_offset,\n@@ -4660,1 +4972,1 @@\n-  return (TypeAryPtr*)(new TypeAryPtr(ptr, o, ary, k, xk, offset, instance_id, is_autobox_cache, speculative, inline_depth))->hashcons();\n+  return (TypeAryPtr*)(new TypeAryPtr(ptr, o, ary, k, xk, offset, field_offset, instance_id, is_autobox_cache, speculative, inline_depth))->hashcons();\n@@ -4666,1 +4978,1 @@\n-  return make(ptr, ptr == Constant ? const_oop() : nullptr, _ary, klass(), klass_is_exact(), _offset, _instance_id, _speculative, _inline_depth);\n+  return make(ptr, ptr == Constant ? const_oop() : nullptr, _ary, klass(), klass_is_exact(), _offset, _field_offset, _instance_id, _speculative, _inline_depth, _is_autobox_cache);\n@@ -4674,1 +4986,1 @@\n-  return make(ptr(), const_oop(), _ary, klass(), klass_is_exact, _offset, _instance_id, _speculative, _inline_depth);\n+  return make(ptr(), const_oop(), _ary, klass(), klass_is_exact, _offset, _field_offset, _instance_id, _speculative, _inline_depth, _is_autobox_cache);\n@@ -4680,1 +4992,1 @@\n-  return make(_ptr, const_oop(), _ary, klass(), _klass_is_exact, _offset, instance_id, _speculative, _inline_depth);\n+  return make(_ptr, const_oop(), _ary, klass(), _klass_is_exact, _offset, _field_offset, instance_id, _speculative, _inline_depth, _is_autobox_cache);\n@@ -4738,2 +5050,65 @@\n-  const TypeAry* new_ary = TypeAry::make(elem(), new_size, is_stable());\n-  return make(ptr(), const_oop(), new_ary, klass(), klass_is_exact(), _offset, _instance_id, _speculative, _inline_depth);\n+  const TypeAry* new_ary = TypeAry::make(elem(), new_size, is_stable(), is_flat(), is_not_flat(), is_not_null_free(), is_atomic());\n+  return make(ptr(), const_oop(), new_ary, klass(), klass_is_exact(), _offset, _field_offset, _instance_id, _speculative, _inline_depth, _is_autobox_cache);\n+}\n+\n+\/\/-------------------------------cast_to_not_flat------------------------------\n+const TypeAryPtr* TypeAryPtr::cast_to_not_flat(bool not_flat) const {\n+  if (not_flat == is_not_flat()) {\n+    return this;\n+  }\n+  assert(!not_flat || !is_flat(), \"inconsistency\");\n+  const TypeAry* new_ary = TypeAry::make(elem(), size(), is_stable(), is_flat(), not_flat, is_not_null_free(), is_atomic());\n+  const TypeAryPtr* res = make(ptr(), const_oop(), new_ary, klass(), klass_is_exact(), _offset, _field_offset, _instance_id, _speculative, _inline_depth, _is_autobox_cache);\n+  \/\/ We keep the speculative part if it contains information about flat-\/nullability.\n+  \/\/ Make sure it's removed if it's not better than the non-speculative type anymore.\n+  if (res->speculative() == res->remove_speculative()) {\n+    return res->remove_speculative();\n+  }\n+  return res;\n+}\n+\n+\/\/-------------------------------cast_to_not_null_free-------------------------\n+const TypeAryPtr* TypeAryPtr::cast_to_not_null_free(bool not_null_free) const {\n+  if (not_null_free == is_not_null_free()) {\n+    return this;\n+  }\n+  assert(!not_null_free || !is_null_free(), \"inconsistency\");\n+  const TypeAry* new_ary = TypeAry::make(elem(), size(), is_stable(), is_flat(), is_not_flat(), not_null_free, is_atomic());\n+  const TypeAryPtr* res = make(ptr(), const_oop(), new_ary, klass(), klass_is_exact(), _offset, _field_offset,\n+                               _instance_id, _speculative, _inline_depth, _is_autobox_cache);\n+  \/\/ We keep the speculative part if it contains information about flat-\/nullability.\n+  \/\/ Make sure it's removed if it's not better than the non-speculative type anymore.\n+  if (res->speculative() == res->remove_speculative()) {\n+    return res->remove_speculative();\n+  }\n+  return res;\n+}\n+\n+\/\/---------------------------------update_properties---------------------------\n+const TypeAryPtr* TypeAryPtr::update_properties(const TypeAryPtr* from) const {\n+  if ((from->is_flat()          && is_not_flat()) ||\n+      (from->is_not_flat()      && is_flat()) ||\n+      (from->is_null_free()     && is_not_null_free()) ||\n+      (from->is_not_null_free() && is_null_free())) {\n+    return nullptr; \/\/ Inconsistent properties\n+  }\n+  const TypeAryPtr* res = this;\n+  if (from->is_not_null_free()) {\n+    res = res->cast_to_not_null_free();\n+  }\n+  if (from->is_not_flat()) {\n+    res = res->cast_to_not_flat();\n+  }\n+  return res;\n+}\n+\n+jint TypeAryPtr::flat_layout_helper() const {\n+  return klass()->as_flat_array_klass()->layout_helper();\n+}\n+\n+int TypeAryPtr::flat_elem_size() const {\n+  return klass()->as_flat_array_klass()->element_byte_size();\n+}\n+\n+int TypeAryPtr::flat_log_elem_size() const {\n+  return klass()->as_flat_array_klass()->log2_element_size();\n@@ -4755,1 +5130,1 @@\n-  const TypeAry* new_ary = TypeAry::make(elem, size(), stable);\n+  const TypeAry* new_ary = TypeAry::make(elem, size(), stable, is_flat(), is_not_flat(), is_not_null_free(), is_atomic());\n@@ -4757,1 +5132,1 @@\n-  return make(ptr(), const_oop(), new_ary, klass(), klass_is_exact(), _offset, _instance_id, _speculative, _inline_depth);\n+  return make(ptr(), const_oop(), new_ary, klass(), klass_is_exact(), _offset, _field_offset, _instance_id, _speculative, _inline_depth, _is_autobox_cache);\n@@ -4777,2 +5152,2 @@\n-  const TypeAry* new_ary = TypeAry::make(etype, size(), is_stable());\n-  return make(ptr(), const_oop(), new_ary, klass(), klass_is_exact(), _offset, _instance_id, _speculative, _inline_depth, \/*is_autobox_cache=*\/true);\n+  const TypeAry* new_ary = TypeAry::make(etype, size(), is_stable(), is_flat(), is_not_flat(), is_not_null_free(), is_atomic());\n+  return make(ptr(), const_oop(), new_ary, klass(), klass_is_exact(), _offset, _field_offset, _instance_id, _speculative, _inline_depth, \/*is_autobox_cache=*\/true);\n@@ -4787,1 +5162,2 @@\n-    TypeOopPtr::eq(p);  \/\/ Check sub-parts\n+    TypeOopPtr::eq(p) &&\/\/ Check sub-parts\n+    _field_offset == p->_field_offset;\n@@ -4793,1 +5169,1 @@\n-  return (uint)(uintptr_t)_ary + TypeOopPtr::hash();\n+  return (uint)(uintptr_t)_ary + TypeOopPtr::hash() + _field_offset.get();\n@@ -4840,1 +5216,1 @@\n-    int offset = meet_offset(tp->offset());\n+    Offset offset = meet_offset(tp->offset());\n@@ -4849,1 +5225,1 @@\n-                  _ary, _klass, _klass_is_exact, offset, instance_id, speculative, depth);\n+                  _ary, _klass, _klass_is_exact, offset, _field_offset, instance_id, speculative, depth);\n@@ -4863,1 +5239,1 @@\n-    int offset = meet_offset(tp->offset());\n+    Offset offset = meet_offset(tp->offset());\n@@ -4879,1 +5255,1 @@\n-                  _ary, _klass, _klass_is_exact, offset, instance_id, speculative, depth);\n+                  _ary, _klass, _klass_is_exact, offset, _field_offset, instance_id, speculative, depth);\n@@ -4893,1 +5269,2 @@\n-    int off = meet_offset(tap->offset());\n+    Offset off = meet_offset(tap->offset());\n+    Offset field_off = meet_field_offset(tap->field_offset());\n@@ -4907,0 +5284,4 @@\n+    bool res_flat = false;\n+    bool res_not_flat = false;\n+    bool res_not_null_free = false;\n+    bool res_atomic = false;\n@@ -4908,1 +5289,1 @@\n-    if (meet_aryptr(ptr, elem, this, tap, res_klass, res_xk) == NOT_SUBTYPE) {\n+    if (meet_aryptr(ptr, elem, this, tap, res_klass, res_xk, res_flat, res_not_flat, res_not_null_free, res_atomic) == NOT_SUBTYPE) {\n@@ -4910,0 +5291,14 @@\n+    } else if (this->is_flat() != tap->is_flat()) {\n+      \/\/ Meeting flat inline type array with non-flat array. Adjust (field) offset accordingly.\n+      if (tary->_flat) {\n+        \/\/ Result is in a flat representation\n+        off = Offset(is_flat() ? offset() : tap->offset());\n+        field_off = is_flat() ? field_offset() : tap->field_offset();\n+      } else if (below_centerline(ptr)) {\n+        \/\/ Result is in a non-flat representation\n+        off = Offset(flat_offset()).meet(Offset(tap->flat_offset()));\n+        field_off = (field_off == Offset::top) ? Offset::top : Offset::bottom;\n+      } else if (flat_offset() == tap->flat_offset()) {\n+        off = Offset(!is_flat() ? offset() : tap->offset());\n+        field_off = !is_flat() ? field_offset() : tap->field_offset();\n+      }\n@@ -4927,1 +5322,1 @@\n-    return make(ptr, o, TypeAry::make(elem, tary->_size, tary->_stable), res_klass, res_xk, off, instance_id, speculative, depth);\n+    return make(ptr, o, TypeAry::make(elem, tary->_size, tary->_stable, res_flat, res_not_flat, res_not_null_free, res_atomic), res_klass, res_xk, off, field_off, instance_id, speculative, depth);\n@@ -4933,1 +5328,1 @@\n-    int offset = meet_offset(tp->offset());\n+    Offset offset = meet_offset(tp->offset());\n@@ -4948,2 +5343,2 @@\n-      if (tp->klass()->equals(ciEnv::current()->Object_klass()) && this_interfaces->contains(tp_interfaces) && !tp->klass_is_exact()) {\n-        return TypeAryPtr::make(ptr, _ary, _klass, _klass_is_exact, offset, instance_id, speculative, depth);\n+      if (tp->klass()->equals(ciEnv::current()->Object_klass()) && this_interfaces->contains(tp_interfaces) && !tp->klass_is_exact() && !tp->flat_in_array()) {\n+        return TypeAryPtr::make(ptr, _ary, _klass, _klass_is_exact, offset, _field_offset, instance_id, speculative, depth);\n@@ -4955,1 +5350,1 @@\n-        return TypeInstPtr::make(ptr, ciEnv::current()->Object_klass(), interfaces, false, nullptr,offset, instance_id, speculative, depth);\n+        return TypeInstPtr::make(ptr, ciEnv::current()->Object_klass(), interfaces, false, nullptr, offset, false, instance_id, speculative, depth);\n@@ -4967,1 +5362,1 @@\n-        if (tp->klass()->equals(ciEnv::current()->Object_klass()) && this_interfaces->contains(tp_interfaces) && !tp->klass_is_exact()) {\n+        if (tp->klass()->equals(ciEnv::current()->Object_klass()) && this_interfaces->contains(tp_interfaces) && !tp->klass_is_exact() && !tp->flat_in_array()) {\n@@ -4970,1 +5365,1 @@\n-                      _ary, _klass, _klass_is_exact, offset, instance_id, speculative, depth);\n+                      _ary, _klass, _klass_is_exact, offset, _field_offset, instance_id, speculative, depth);\n@@ -4982,1 +5377,1 @@\n-      return TypeInstPtr::make(ptr, ciEnv::current()->Object_klass(), interfaces, false, nullptr, offset, instance_id, speculative, depth);\n+      return TypeInstPtr::make(ptr, ciEnv::current()->Object_klass(), interfaces, false, nullptr, offset, false, instance_id, speculative, depth);\n@@ -4991,2 +5386,2 @@\n-template<class T> TypePtr::MeetResult TypePtr::meet_aryptr(PTR& ptr, const Type*& elem, const T* this_ary,\n-                                                           const T* other_ary, ciKlass*& res_klass, bool& res_xk) {\n+template<class T> TypePtr::MeetResult TypePtr::meet_aryptr(PTR& ptr, const Type*& elem, const T* this_ary, const T* other_ary,\n+                                                           ciKlass*& res_klass, bool& res_xk, bool &res_flat, bool& res_not_flat, bool& res_not_null_free, bool &res_atomic) {\n@@ -5002,0 +5397,9 @@\n+  bool this_flat = this_ary->is_flat();\n+  bool this_not_flat = this_ary->is_not_flat();\n+  bool other_flat = other_ary->is_flat();\n+  bool other_not_flat = other_ary->is_not_flat();\n+  bool this_not_null_free = this_ary->is_not_null_free();\n+  bool other_not_null_free = other_ary->is_not_null_free();\n+  bool this_atomic = this_ary->is_atomic();\n+  bool other_atomic = other_ary->is_atomic();\n+  const bool same_nullness = this_ary->is_null_free() == other_ary->is_null_free();\n@@ -5004,0 +5408,6 @@\n+  res_flat = this_flat && other_flat;\n+  bool res_null_free = this_ary->is_null_free() && other_ary->is_null_free();\n+  res_not_flat = this_not_flat && other_not_flat;\n+  res_not_null_free = this_not_null_free && other_not_null_free;\n+  res_atomic = this_atomic && other_atomic;\n+\n@@ -5007,3 +5417,3 @@\n-    if (this_top_or_bottom)\n-      res_klass = other_klass;\n-    else if (other_top_or_bottom || other_klass == this_klass) {\n+      if (this_top_or_bottom) {\n+        res_klass = other_klass;\n+      } else if (other_top_or_bottom || other_klass == this_klass) {\n@@ -5051,0 +5461,3 @@\n+        if (this_ary->is_flat()) {\n+          elem = this_ary->elem();\n+        }\n@@ -5054,1 +5467,1 @@\n-      return result;\n+      break;\n@@ -5056,1 +5469,3 @@\n-      if (this_ptr == Constant) {\n+      if (this_ptr == Constant && same_nullness) {\n+        \/\/ Only exact if same nullness since:\n+        \/\/     null-free [LMyValue <: nullable [LMyValue.\n@@ -5058,1 +5473,1 @@\n-      } else if(above_centerline(this_ptr)) {\n+      } else if (above_centerline(this_ptr)) {\n@@ -5063,0 +5478,5 @@\n+        \/\/ Even though MyValue is final, [LMyValue is only exact if the array\n+        \/\/ is (not) null-free due to null-free [LMyValue <: null-able [LMyValue.\n+        if (res_xk && !res_null_free && !res_not_null_free) {\n+          res_xk = false;\n+        }\n@@ -5064,1 +5484,1 @@\n-      return result;\n+      break;\n@@ -5071,0 +5491,3 @@\n+        if (other_ary->is_flat()) {\n+          elem = other_ary->elem();\n+        }\n@@ -5074,0 +5497,5 @@\n+        \/\/ Even though MyValue is final, [LMyValue is only exact if the array\n+        \/\/ is (not) null-free due to null-free [LMyValue <: null-able [LMyValue.\n+        if (res_xk && !res_null_free && !res_not_null_free) {\n+          res_xk = false;\n+        }\n@@ -5075,1 +5503,1 @@\n-      return result;\n+      break;\n@@ -5088,1 +5516,11 @@\n-  return new TypeAryPtr(dual_ptr(), _const_oop, _ary->dual()->is_ary(),_klass, _klass_is_exact, dual_offset(), dual_instance_id(), is_autobox_cache(), dual_speculative(), dual_inline_depth());\n+  bool xk = _klass_is_exact;\n+  return new TypeAryPtr(dual_ptr(), _const_oop, _ary->dual()->is_ary(), _klass, xk, dual_offset(), dual_field_offset(), dual_instance_id(), is_autobox_cache(), dual_speculative(), dual_inline_depth());\n+}\n+\n+Type::Offset TypeAryPtr::meet_field_offset(const Type::Offset offset) const {\n+  return _field_offset.meet(offset);\n+}\n+\n+\/\/------------------------------dual_offset------------------------------------\n+Type::Offset TypeAryPtr::dual_field_offset() const {\n+  return _field_offset.dual();\n@@ -5116,1 +5554,23 @@\n-  if( _offset != 0 ) {\n+  if (is_flat()) {\n+    st->print(\":flat\");\n+    st->print(\"(\");\n+    _field_offset.dump2(st);\n+    st->print(\")\");\n+  } else if (is_not_flat()) {\n+    st->print(\":not_flat\");\n+  }\n+  if (is_null_free()) {\n+    st->print(\":null free\");\n+  }\n+  if (is_atomic()) {\n+    st->print(\":atomic\");\n+  }\n+  if (Verbose) {\n+    if (is_not_flat()) {\n+      st->print(\":not flat\");\n+    }\n+    if (is_not_null_free()) {\n+      st->print(\":nullable\");\n+    }\n+  }\n+  if (offset() != 0) {\n@@ -5119,3 +5579,3 @@\n-    if( _offset == OffsetTop )       st->print(\"+undefined\");\n-    else if( _offset == OffsetBot )  st->print(\"+any\");\n-    else if( _offset < header_size ) st->print(\"+%d\", _offset);\n+    if( _offset == Offset::top )       st->print(\"+undefined\");\n+    else if( _offset == Offset::bottom )  st->print(\"+any\");\n+    else if( offset() < header_size ) st->print(\"+%d\", offset());\n@@ -5127,1 +5587,1 @@\n-        st->print(\"[%d]\", (_offset - header_size)\/elem_size);\n+        st->print(\"[%d]\", (offset() - header_size)\/elem_size);\n@@ -5144,0 +5604,4 @@\n+  \/\/ FIXME: Does this belong here? Or in the meet code itself?\n+  if (is_flat() && is_not_flat()) {\n+    return true;\n+  }\n@@ -5149,1 +5613,1 @@\n-  return make(_ptr, _const_oop, _ary, _klass, _klass_is_exact, xadd_offset(offset), _instance_id, add_offset_speculative(offset), _inline_depth);\n+  return make(_ptr, _const_oop, _ary, _klass, _klass_is_exact, xadd_offset(offset), _field_offset, _instance_id, add_offset_speculative(offset), _inline_depth, _is_autobox_cache);\n@@ -5153,1 +5617,1 @@\n-  return make(_ptr, _const_oop, _ary, _klass, _klass_is_exact, offset, _instance_id, with_offset_speculative(offset), _inline_depth);\n+  return make(_ptr, _const_oop, _ary, _klass, _klass_is_exact, Offset(offset), _field_offset, _instance_id, with_offset_speculative(offset), _inline_depth, _is_autobox_cache);\n@@ -5157,1 +5621,1 @@\n-  return make(_ptr, _const_oop, ary, _klass, _klass_is_exact, _offset, _instance_id, _speculative, _inline_depth);\n+  return make(_ptr, _const_oop, ary, _klass, _klass_is_exact, _offset, _field_offset, _instance_id, _speculative, _inline_depth, _is_autobox_cache);\n@@ -5165,1 +5629,14 @@\n-  return make(_ptr, _const_oop, _ary->remove_speculative()->is_ary(), _klass, _klass_is_exact, _offset, _instance_id, nullptr, _inline_depth);\n+  return make(_ptr, _const_oop, _ary->remove_speculative()->is_ary(), _klass, _klass_is_exact, _offset, _field_offset, _instance_id, nullptr, _inline_depth, _is_autobox_cache);\n+}\n+\n+const Type* TypeAryPtr::cleanup_speculative() const {\n+  if (speculative() == nullptr) {\n+    return this;\n+  }\n+  \/\/ Keep speculative part if it contains information about flat-\/nullability\n+  const TypeAryPtr* spec_aryptr = speculative()->isa_aryptr();\n+  if (spec_aryptr != nullptr && !above_centerline(spec_aryptr->ptr()) &&\n+      (spec_aryptr->is_not_flat() || spec_aryptr->is_not_null_free())) {\n+    return this;\n+  }\n+  return TypeOopPtr::cleanup_speculative();\n@@ -5172,1 +5649,44 @@\n-  return make(_ptr, _const_oop, _ary->remove_speculative()->is_ary(), _klass, _klass_is_exact, _offset, _instance_id, _speculative, depth);\n+  return make(_ptr, _const_oop, _ary->remove_speculative()->is_ary(), _klass, _klass_is_exact, _offset, _field_offset, _instance_id, _speculative, depth, _is_autobox_cache);\n+}\n+\n+const TypeAryPtr* TypeAryPtr::with_field_offset(int offset) const {\n+  return make(_ptr, _const_oop, _ary->remove_speculative()->is_ary(), _klass, _klass_is_exact, _offset, Offset(offset), _instance_id, _speculative, _inline_depth, _is_autobox_cache);\n+}\n+\n+const TypePtr* TypeAryPtr::add_field_offset_and_offset(intptr_t offset) const {\n+  int adj = 0;\n+  if (is_flat() && offset != Type::OffsetBot && offset != Type::OffsetTop) {\n+    if (_offset.get() != OffsetBot && _offset.get() != OffsetTop) {\n+      adj = _offset.get();\n+      offset += _offset.get();\n+    }\n+    uint header = arrayOopDesc::base_offset_in_bytes(T_OBJECT);\n+    if (_field_offset.get() != OffsetBot && _field_offset.get() != OffsetTop) {\n+      offset += _field_offset.get();\n+      if (_offset.get() == OffsetBot || _offset.get() == OffsetTop) {\n+        offset += header;\n+      }\n+    }\n+    if (elem()->make_oopptr()->is_inlinetypeptr() && (offset >= (intptr_t)header || offset < 0)) {\n+      \/\/ Try to get the field of the inline type array element we are pointing to\n+      ciInlineKlass* vk = elem()->inline_klass();\n+      int shift = flat_log_elem_size();\n+      int mask = (1 << shift) - 1;\n+      intptr_t field_offset = ((offset - header) & mask);\n+      ciField* field = vk->get_field_by_offset(field_offset + vk->payload_offset(), false);\n+      if (field != nullptr || field_offset == vk->null_marker_offset_in_payload()) {\n+        return with_field_offset(field_offset)->add_offset(offset - field_offset - adj);\n+      }\n+    }\n+  }\n+  return add_offset(offset - adj);\n+}\n+\n+\/\/ Return offset incremented by field_offset for flat inline type arrays\n+int TypeAryPtr::flat_offset() const {\n+  int offset = _offset.get();\n+  if (offset != Type::OffsetBot && offset != Type::OffsetTop &&\n+      _field_offset != Offset::bottom && _field_offset != Offset::top) {\n+    offset += _field_offset.get();\n+  }\n+  return offset;\n@@ -5177,1 +5697,1 @@\n-  return make(_ptr, _const_oop, _ary->remove_speculative()->is_ary(), _klass, _klass_is_exact, _offset, instance_id, _speculative, _inline_depth);\n+  return make(_ptr, _const_oop, _ary->remove_speculative()->is_ary(), _klass, _klass_is_exact, _offset, _field_offset, instance_id, _speculative, _inline_depth);\n@@ -5182,0 +5702,1 @@\n+\n@@ -5275,1 +5796,0 @@\n-\n@@ -5359,1 +5879,1 @@\n-  return (_offset == 0) && !below_centerline(_ptr);\n+  return (offset() == 0) && !below_centerline(_ptr);\n@@ -5379,1 +5899,1 @@\n-  assert( _offset >= 0, \"\" );\n+  assert(offset() >= 0, \"\");\n@@ -5381,1 +5901,1 @@\n-  if (_offset != 0) {\n+  if (offset() != 0) {\n@@ -5435,1 +5955,1 @@\n-    int offset = meet_offset(tp->offset());\n+    Offset offset = meet_offset(tp->offset());\n@@ -5463,1 +5983,1 @@\n-    int offset = meet_offset(tp->offset());\n+    Offset offset = meet_offset(tp->offset());\n@@ -5496,1 +6016,1 @@\n-  switch( _offset ) {\n+  switch (offset()) {\n@@ -5500,1 +6020,1 @@\n-  default:        st->print(\"+%d\",_offset); break;\n+  default:        st->print(\"+%d\",offset()); break;\n@@ -5510,1 +6030,1 @@\n-TypeMetadataPtr::TypeMetadataPtr(PTR ptr, ciMetadata* metadata, int offset):\n+TypeMetadataPtr::TypeMetadataPtr(PTR ptr, ciMetadata* metadata, Offset offset):\n@@ -5515,1 +6035,1 @@\n-  return make(Constant, m, 0);\n+  return make(Constant, m, Offset(0));\n@@ -5518,1 +6038,1 @@\n-  return make(Constant, m, 0);\n+  return make(Constant, m, Offset(0));\n@@ -5523,1 +6043,1 @@\n-const TypeMetadataPtr *TypeMetadataPtr::make(PTR ptr, ciMetadata* m, int offset) {\n+const TypeMetadataPtr* TypeMetadataPtr::make(PTR ptr, ciMetadata* m, Offset offset) {\n@@ -5534,1 +6054,6 @@\n-    if (elem->is_klassptr()->klass_is_exact()) {\n+    if (elem->is_klassptr()->klass_is_exact() &&\n+        \/\/ Even though MyValue is final, [LMyValue is only exact if the array\n+        \/\/ is (not) null-free due to null-free [LMyValue <: null-able [LMyValue.\n+        \/\/ TODO 8350865 If we know that the array can't be null-free, it's allowed to be exact, right?\n+        \/\/ If so, we should add '|| is_not_null_free()'\n+        (is_null_free() || !_ary->_elem->make_oopptr()->is_inlinetypeptr())) {\n@@ -5538,1 +6063,1 @@\n-  return TypeAryKlassPtr::make(xk ? TypePtr::Constant : TypePtr::NotNull, elem, klass(), 0);\n+  return TypeAryKlassPtr::make(xk ? TypePtr::Constant : TypePtr::NotNull, elem, klass(), Offset(0), is_not_flat(), is_not_null_free(), is_flat(), is_null_free(), is_atomic(), is_flat() || is_null_free());\n@@ -5541,1 +6066,1 @@\n-const TypeKlassPtr* TypeKlassPtr::make(ciKlass *klass, InterfaceHandling interface_handling) {\n+const TypeKlassPtr* TypeKlassPtr::make(ciKlass* klass, InterfaceHandling interface_handling) {\n@@ -5548,1 +6073,1 @@\n-const TypeKlassPtr* TypeKlassPtr::make(PTR ptr, ciKlass* klass, int offset, InterfaceHandling interface_handling) {\n+const TypeKlassPtr* TypeKlassPtr::make(PTR ptr, ciKlass* klass, Offset offset, InterfaceHandling interface_handling) {\n@@ -5556,3 +6081,1 @@\n-\n-\/\/------------------------------TypeKlassPtr-----------------------------------\n-TypeKlassPtr::TypeKlassPtr(TYPES t, PTR ptr, ciKlass* klass, const TypeInterfaces* interfaces, int offset)\n+TypeKlassPtr::TypeKlassPtr(TYPES t, PTR ptr, ciKlass* klass, const TypeInterfaces* interfaces, Offset offset)\n@@ -5561,1 +6084,1 @@\n-         klass->is_type_array_klass() || !klass->as_obj_array_klass()->base_element_klass()->is_interface(), \"no interface here\");\n+         klass->is_type_array_klass() || klass->is_flat_array_klass() || !klass->as_obj_array_klass()->base_element_klass()->is_interface(), \"no interface here\");\n@@ -5600,1 +6123,1 @@\n-  return (_offset == 0) && !below_centerline(_ptr);\n+  return (offset() == 0) && !below_centerline(_ptr);\n@@ -5630,1 +6153,1 @@\n-  assert( _offset >= 0, \"\" );\n+  assert( offset() >= 0, \"\" );\n@@ -5632,1 +6155,1 @@\n-  if (_offset != 0) {\n+  if (offset() != 0) {\n@@ -5676,5 +6199,2 @@\n-\n-  if (_offset) {               \/\/ Dump offset, if any\n-    if (_offset == OffsetBot)      { st->print(\"+any\"); }\n-    else if (_offset == OffsetTop) { st->print(\"+unknown\"); }\n-    else                            { st->print(\"+%d\", _offset); }\n+  if (Verbose) {\n+    if (isa_instklassptr() && is_instklassptr()->flat_in_array()) st->print(\":flat in array\");\n@@ -5682,1 +6202,1 @@\n-\n+  _offset.dump2(st);\n@@ -5684,0 +6204,4 @@\n+\n+  if (flat_in_array() && !klass()->is_inlinetype()) {\n+    st->print(\" (flat in array)\");\n+  }\n@@ -5698,0 +6222,1 @@\n+    flat_in_array() == p->flat_in_array() &&\n@@ -5702,1 +6227,1 @@\n-  return klass()->hash() + TypeKlassPtr::hash();\n+  return klass()->hash() + TypeKlassPtr::hash() + (uint)flat_in_array();\n@@ -5705,1 +6230,3 @@\n-const TypeInstKlassPtr *TypeInstKlassPtr::make(PTR ptr, ciKlass* k, const TypeInterfaces* interfaces, int offset) {\n+const TypeInstKlassPtr *TypeInstKlassPtr::make(PTR ptr, ciKlass* k, const TypeInterfaces* interfaces, Offset offset, bool flat_in_array) {\n+  flat_in_array = flat_in_array || k->maybe_flat_in_array();\n+\n@@ -5707,1 +6234,1 @@\n-    (TypeInstKlassPtr*)(new TypeInstKlassPtr(ptr, k, interfaces, offset))->hashcons();\n+    (TypeInstKlassPtr*)(new TypeInstKlassPtr(ptr, k, interfaces, offset, flat_in_array))->hashcons();\n@@ -5714,2 +6241,2 @@\n-const TypePtr* TypeInstKlassPtr::add_offset( intptr_t offset ) const {\n-  return make( _ptr, klass(), _interfaces, xadd_offset(offset) );\n+const TypePtr *TypeInstKlassPtr::add_offset( intptr_t offset ) const {\n+  return make(_ptr, klass(), _interfaces, xadd_offset(offset), flat_in_array());\n@@ -5719,1 +6246,1 @@\n-  return make(_ptr, klass(), _interfaces, offset);\n+  return make(_ptr, klass(), _interfaces, Offset(offset), flat_in_array());\n@@ -5726,1 +6253,1 @@\n-  return make(ptr, _klass, _interfaces, _offset);\n+  return make(ptr, _klass, _interfaces, _offset, flat_in_array());\n@@ -5742,1 +6269,1 @@\n-  return make(klass_is_exact ? Constant : NotNull, k, _interfaces, _offset);\n+  return make(klass_is_exact ? Constant : NotNull, k, _interfaces, _offset, flat_in_array());\n@@ -5774,1 +6301,1 @@\n-  return TypeInstPtr::make(TypePtr::BotPTR, k, interfaces, xk, nullptr, 0);\n+  return TypeInstPtr::make(TypePtr::BotPTR, k, interfaces, xk, nullptr, Offset(0), flat_in_array() && !klass()->is_inlinetype());\n@@ -5810,1 +6337,1 @@\n-    int offset = meet_offset(tp->offset());\n+    Offset offset = meet_offset(tp->offset());\n@@ -5818,1 +6345,1 @@\n-      return make( ptr, klass(), _interfaces, offset );\n+      return make(ptr, klass(), _interfaces, offset, flat_in_array());\n@@ -5831,1 +6358,1 @@\n-    return TypePtr::BOTTOM;\n+      return TypePtr::BOTTOM;\n@@ -5851,1 +6378,1 @@\n-    int  off     = meet_offset(tkls->offset());\n+    Offset  off     = meet_offset(tkls->offset());\n@@ -5857,1 +6384,2 @@\n-    switch(meet_instptr(ptr, interfaces, this, tkls, res_klass, res_xk)) {\n+    bool res_flat_in_array = false;\n+    switch(meet_instptr(ptr, interfaces, this, tkls, res_klass, res_xk, res_flat_in_array)) {\n@@ -5865,1 +6393,1 @@\n-        const Type* res = make(ptr, res_klass, interfaces, off);\n+        const Type* res = make(ptr, res_klass, interfaces, off, res_flat_in_array);\n@@ -5874,1 +6402,1 @@\n-    int offset = meet_offset(tp->offset());\n+    Offset offset = meet_offset(tp->offset());\n@@ -5887,1 +6415,1 @@\n-        return TypeAryKlassPtr::make(ptr, tp->elem(), tp->klass(), offset);\n+        return TypeAryKlassPtr::make(ptr, tp->elem(), tp->klass(), offset, tp->is_not_flat(), tp->is_not_null_free(), tp->is_flat(), tp->is_null_free(), tp->is_atomic(), tp->is_vm_type());\n@@ -5892,1 +6420,1 @@\n-        return make(ptr, ciEnv::current()->Object_klass(), interfaces, offset);\n+        return make(ptr, ciEnv::current()->Object_klass(), interfaces, offset, false);\n@@ -5906,2 +6434,1 @@\n-          return TypeAryKlassPtr::make(ptr,\n-                                       tp->elem(), tp->klass(), offset);\n+          return TypeAryKlassPtr::make(ptr, tp->elem(), tp->klass(), offset, tp->is_not_flat(), tp->is_not_null_free(), tp->is_flat(), tp->is_null_free(), tp->is_atomic(), tp->is_vm_type());\n@@ -5915,1 +6442,1 @@\n-      return make(ptr, ciEnv::current()->Object_klass(), interfaces, offset);\n+      return make(ptr, ciEnv::current()->Object_klass(), interfaces, offset, false);\n@@ -5927,1 +6454,1 @@\n-  return new TypeInstKlassPtr(dual_ptr(), klass(), _interfaces, dual_offset());\n+  return new TypeInstKlassPtr(dual_ptr(), klass(), _interfaces, dual_offset(), flat_in_array());\n@@ -6041,0 +6568,3 @@\n+bool TypeInstKlassPtr::can_be_inline_array() const {\n+  return _klass->equals(ciEnv::current()->Object_klass()) && TypeAryKlassPtr::_array_interfaces->contains(_interfaces);\n+}\n@@ -6042,2 +6572,2 @@\n-const TypeAryKlassPtr *TypeAryKlassPtr::make(PTR ptr, const Type* elem, ciKlass* k, int offset) {\n-  return (TypeAryKlassPtr*)(new TypeAryKlassPtr(ptr, elem, k, offset))->hashcons();\n+bool TypeAryKlassPtr::can_be_inline_array() const {\n+  return _elem->isa_instklassptr() && _elem->is_instklassptr()->_klass->can_be_inline_klass();\n@@ -6046,1 +6576,13 @@\n-const TypeAryKlassPtr *TypeAryKlassPtr::make(PTR ptr, ciKlass* k, int offset, InterfaceHandling interface_handling) {\n+bool TypeInstPtr::can_be_inline_array() const {\n+  return _klass->equals(ciEnv::current()->Object_klass()) && TypeAryPtr::_array_interfaces->contains(_interfaces);\n+}\n+\n+bool TypeAryPtr::can_be_inline_array() const {\n+  return elem()->make_ptr() && elem()->make_ptr()->isa_instptr() && elem()->make_ptr()->is_instptr()->_klass->can_be_inline_klass();\n+}\n+\n+const TypeAryKlassPtr *TypeAryKlassPtr::make(PTR ptr, const Type* elem, ciKlass* k, Offset offset, bool not_flat, bool not_null_free, bool flat, bool null_free, bool atomic, bool vm_type) {\n+  return (TypeAryKlassPtr*)(new TypeAryKlassPtr(ptr, elem, k, offset, not_flat, not_null_free, flat, null_free, atomic, vm_type))->hashcons();\n+}\n+\n+const TypeAryKlassPtr* TypeAryKlassPtr::make(PTR ptr, ciKlass* k, Offset offset, InterfaceHandling interface_handling, bool not_flat, bool not_null_free, bool flat, bool null_free, bool atomic, bool vm_type) {\n@@ -6050,2 +6592,2 @@\n-    const TypeKlassPtr *etype = TypeKlassPtr::make(eklass, interface_handling)->cast_to_exactness(false);\n-    return TypeAryKlassPtr::make(ptr, etype, nullptr, offset);\n+    const TypeKlassPtr* etype = TypeKlassPtr::make(eklass, interface_handling)->cast_to_exactness(false);\n+    return TypeAryKlassPtr::make(ptr, etype, nullptr, offset, not_flat, not_null_free, flat, null_free, atomic, vm_type);\n@@ -6055,1 +6597,5 @@\n-    return TypeAryKlassPtr::make(ptr, etype, k, offset);\n+    return TypeAryKlassPtr::make(ptr, etype, k, offset, not_flat, not_null_free, flat, null_free, atomic);\n+  } else if (k->is_flat_array_klass()) {\n+    ciKlass* eklass = k->as_flat_array_klass()->element_klass();\n+    const TypeKlassPtr* etype = TypeKlassPtr::make(eklass, interface_handling)->cast_to_exactness(false);\n+    return TypeAryKlassPtr::make(ptr, etype, k, offset, not_flat, not_null_free, flat, null_free, atomic, vm_type);\n@@ -6062,2 +6608,26 @@\n-const TypeAryKlassPtr* TypeAryKlassPtr::make(ciKlass* klass, InterfaceHandling interface_handling) {\n-  return TypeAryKlassPtr::make(Constant, klass, 0, interface_handling);\n+const TypeAryKlassPtr* TypeAryKlassPtr::make(PTR ptr, ciKlass* k, Offset offset, InterfaceHandling interface_handling, bool vm_type) {\n+  bool flat = k->is_flat_array_klass();\n+  bool null_free = k->as_array_klass()->is_elem_null_free();\n+  bool atomic = k->as_array_klass()->is_elem_atomic();\n+\n+  bool not_inline = k->is_type_array_klass() || !k->as_array_klass()->element_klass()->can_be_inline_klass(false);\n+  bool not_null_free = (ptr == Constant) ? !null_free : not_inline;\n+  bool not_flat = (ptr == Constant) ? !flat : (!UseArrayFlattening || not_inline ||\n+                   (k->as_array_klass()->element_klass() != nullptr &&\n+                    k->as_array_klass()->element_klass()->is_inlinetype() &&\n+                   !k->as_array_klass()->element_klass()->maybe_flat_in_array()));\n+\n+  return TypeAryKlassPtr::make(ptr, k, offset, interface_handling, not_flat, not_null_free, flat, null_free, atomic, vm_type);\n+}\n+\n+const TypeAryKlassPtr* TypeAryKlassPtr::make(ciKlass* klass, InterfaceHandling interface_handling, bool vm_type) {\n+  return TypeAryKlassPtr::make(Constant, klass, Offset(0), interface_handling, vm_type);\n+}\n+\n+const TypeAryKlassPtr* TypeAryKlassPtr::get_vm_type(bool vm_type) const {\n+  ciKlass* eklass = elem()->is_klassptr()->exact_klass_helper();\n+  if (elem()->isa_aryklassptr()) {\n+    eklass = exact_klass()->as_obj_array_klass()->element_klass();\n+  }\n+  ciKlass* array_klass = ciArrayKlass::make(eklass, is_null_free(), is_atomic(), true);\n+  return make(_ptr, array_klass, Offset(0), trust_interfaces, vm_type);\n@@ -6072,0 +6642,6 @@\n+    _flat == p->_flat &&\n+    _not_flat == p->_not_flat &&\n+    _null_free == p->_null_free &&\n+    _not_null_free == p->_not_null_free &&\n+    _atomic == p->_atomic &&\n+    _vm_type == p->_vm_type &&\n@@ -6078,1 +6654,2 @@\n-  return (uint)(uintptr_t)_elem + TypeKlassPtr::hash();\n+  return (uint)(uintptr_t)_elem + TypeKlassPtr::hash() + (uint)(_not_flat ? 43 : 0) +\n+      (uint)(_not_null_free ? 44 : 0) + (uint)(_flat ? 45 : 0) + (uint)(_null_free ? 46 : 0)  + (uint)(_atomic ? 47 : 0) + (uint)(_vm_type ? 48 : 0);\n@@ -6094,2 +6671,9 @@\n-  if ((tinst = el->isa_instptr()) != nullptr) {\n-    \/\/ Leave k_ary at null.\n+  if (is_flat() && el->is_inlinetypeptr()) {\n+    \/\/ Klass is required by TypeAryPtr::flat_layout_helper() and others\n+    if (el->inline_klass() != nullptr) {\n+      \/\/ TODO 8350865 We assume atomic if the atomic layout is available, use is_atomic() here\n+      bool atomic = is_null_free() ? el->inline_klass()->has_atomic_layout() : el->inline_klass()->has_nullable_atomic_layout();\n+      k_ary = ciArrayKlass::make(el->inline_klass(), is_null_free(), atomic, true);\n+    }\n+  } else if ((tinst = el->isa_instptr()) != nullptr) {\n+    \/\/ Leave k_ary at nullptr.\n@@ -6097,1 +6681,1 @@\n-    \/\/ Leave k_ary at null.\n+    \/\/ Leave k_ary at nullptr.\n@@ -6145,1 +6729,1 @@\n-    k = ciObjArrayKlass::make(k);\n+    k = ciArrayKlass::make(k, is_null_free(), is_atomic(), is_flat() || is_null_free());\n@@ -6165,1 +6749,1 @@\n-  return make(_ptr, elem(), klass(), xadd_offset(offset));\n+  return make(_ptr, elem(), klass(), xadd_offset(offset), is_not_flat(), is_not_null_free(), _flat, _null_free, _atomic, _vm_type);\n@@ -6169,1 +6753,1 @@\n-  return make(_ptr, elem(), klass(), offset);\n+  return make(_ptr, elem(), klass(), Offset(offset), is_not_flat(), is_not_null_free(), _flat, _null_free, _atomic, _vm_type);\n@@ -6176,1 +6760,1 @@\n-  return make(ptr, elem(), _klass, _offset);\n+  return make(ptr, elem(), _klass, _offset, is_not_flat(), is_not_null_free(), _flat, _null_free, _atomic, _vm_type);\n@@ -6184,0 +6768,7 @@\n+  \/\/ Even though MyValue is final, [LMyValue is only exact if the array\n+  \/\/ is (not) null-free due to null-free [LMyValue <: null-able [LMyValue.\n+  \/\/ TODO 8350865 If we know that the array can't be null-free, it's allowed to be exact, right?\n+  \/\/ If so, we should add '&& !is_not_null_free()'\n+  if (tk->isa_instklassptr() && tk->klass()->is_inlinetype() && !is_null_free()) {\n+    return false;\n+  }\n@@ -6190,1 +6781,4 @@\n-  if (must_be_exact()) return this;  \/\/ cannot clear xk\n+  if (must_be_exact() && !klass_is_exact) return this;  \/\/ cannot clear xk\n+  if (klass_is_exact == this->klass_is_exact()) {\n+    return this;\n+  }\n@@ -6196,1 +6790,19 @@\n-  return make(klass_is_exact ? Constant : NotNull, elem, k, _offset);\n+  bool not_flat = is_not_flat();\n+  bool not_null_free = is_not_null_free();\n+  if (_elem->isa_klassptr()) {\n+    if (klass_is_exact || _elem->isa_aryklassptr()) {\n+      assert((!is_null_free() && !is_flat()) ||\n+             _elem->is_klassptr()->klass()->is_abstract() || _elem->is_klassptr()->klass()->is_java_lang_Object(),\n+             \"null-free (or flat) concrete inline type arrays should always be exact\");\n+      \/\/ An array can't be null-free (or flat) if the klass is exact\n+      not_null_free = true;\n+      not_flat = true;\n+    } else {\n+      \/\/ Klass is not exact (anymore), re-compute null-free\/flat properties\n+      const TypeOopPtr* exact_etype = TypeOopPtr::make_from_klass_unique(_elem->is_instklassptr()->instance_klass());\n+      bool not_inline = !exact_etype->can_be_inline_type();\n+      not_null_free = not_inline;\n+      not_flat = !UseArrayFlattening || not_inline || (exact_etype->is_inlinetypeptr() && !exact_etype->inline_klass()->maybe_flat_in_array());\n+    }\n+  }\n+  return make(klass_is_exact ? Constant : NotNull, elem, k, _offset, not_flat, not_null_free, _flat, _null_free, _atomic, _vm_type);\n@@ -6199,1 +6811,0 @@\n-\n@@ -6213,1 +6824,5 @@\n-  return TypeAryPtr::make(TypePtr::BotPTR, TypeAry::make(el, TypeInt::POS), k, xk, 0);\n+  bool null_free = _null_free;\n+  if (null_free && el->isa_ptr()) {\n+    el = el->is_ptr()->join_speculative(TypePtr::NOTNULL);\n+  }\n+  return TypeAryPtr::make(TypePtr::BotPTR, TypeAry::make(el, TypeInt::POS, false, is_flat(), is_not_flat(), is_not_null_free(), is_atomic()), k, xk, Offset(0));\n@@ -6250,1 +6865,1 @@\n-    int offset = meet_offset(tp->offset());\n+    Offset offset = meet_offset(tp->offset());\n@@ -6258,1 +6873,1 @@\n-      return make( ptr, _elem, klass(), offset );\n+      return make(ptr, _elem, klass(), offset, is_not_flat(), is_not_null_free(), is_flat(), is_null_free(), is_atomic(), is_vm_type());\n@@ -6291,1 +6906,1 @@\n-    int off = meet_offset(tap->offset());\n+    Offset off = meet_offset(tap->offset());\n@@ -6293,1 +6908,0 @@\n-\n@@ -6297,1 +6911,6 @@\n-    meet_aryptr(ptr, elem, this, tap, res_klass, res_xk);\n+    bool res_flat = false;\n+    bool res_not_flat = false;\n+    bool res_not_null_free = false;\n+    bool res_atomic = false;\n+    MeetResult res = meet_aryptr(ptr, elem, this, tap,\n+                                 res_klass, res_xk, res_flat, res_not_flat, res_not_null_free, res_atomic);\n@@ -6299,1 +6918,28 @@\n-    return make(ptr, elem, res_klass, off);\n+    bool flat = meet_flat(tap->_flat);\n+    bool null_free = meet_null_free(tap->_null_free);\n+    bool atomic = meet_atomic(tap->_atomic);\n+    bool vm_type = _vm_type && tap->_vm_type;\n+    if (res == NOT_SUBTYPE) {\n+      flat = false;\n+      null_free = false;\n+      atomic = false;\n+      vm_type = false;\n+    } else if (res == SUBTYPE) {\n+      if (above_centerline(tap->ptr()) && !above_centerline(this->ptr())) {\n+        flat = _flat;\n+        null_free = _null_free;\n+        atomic = _atomic;\n+        vm_type = _vm_type;\n+      } else if (above_centerline(this->ptr()) && !above_centerline(tap->ptr())) {\n+        flat = tap->_flat;\n+        null_free = tap->_null_free;\n+        atomic = tap->_atomic;\n+        vm_type = tap->_vm_type;\n+      } else if (above_centerline(this->ptr()) && above_centerline(tap->ptr())) {\n+        flat = _flat || tap->_flat;\n+        null_free = _null_free || tap->_null_free;\n+        atomic = _atomic || tap->_atomic;\n+        vm_type = _vm_type || tap->_vm_type;\n+      }\n+    }\n+    return make(ptr, elem, res_klass, off, res_not_flat, res_not_null_free, flat, null_free, atomic, vm_type);\n@@ -6303,1 +6949,1 @@\n-    int offset = meet_offset(tp->offset());\n+    Offset offset = meet_offset(tp->offset());\n@@ -6317,1 +6963,1 @@\n-        return TypeAryKlassPtr::make(ptr, _elem, _klass, offset);\n+        return TypeAryKlassPtr::make(ptr, _elem, _klass, offset, is_not_flat(), is_not_null_free(), is_flat(), is_null_free(), is_atomic(), is_vm_type());\n@@ -6322,1 +6968,1 @@\n-        return TypeInstKlassPtr::make(ptr, ciEnv::current()->Object_klass(), interfaces, offset);\n+        return TypeInstKlassPtr::make(ptr, ciEnv::current()->Object_klass(), interfaces, offset, false);\n@@ -6337,1 +6983,1 @@\n-          return make(ptr, _elem, _klass, offset);\n+          return make(ptr, _elem, _klass, offset, is_not_flat(), is_not_null_free(), is_flat(), is_null_free(), is_atomic(), is_vm_type());\n@@ -6345,1 +6991,1 @@\n-      return TypeInstKlassPtr::make(ptr, ciEnv::current()->Object_klass(), interfaces, offset);\n+      return TypeInstKlassPtr::make(ptr, ciEnv::current()->Object_klass(), interfaces, offset, false);\n@@ -6383,0 +7029,3 @@\n+    if (other->is_null_free() && !this_one->is_null_free()) {\n+      return false; \/\/ A nullable array can't be a subtype of a null-free array\n+    }\n@@ -6475,1 +7124,1 @@\n-  return new TypeAryKlassPtr(dual_ptr(), elem()->dual(), klass(), dual_offset());\n+  return new TypeAryKlassPtr(dual_ptr(), elem()->dual(), klass(), dual_offset(), !is_not_flat(), !is_not_null_free(), dual_flat(), dual_null_free(), dual_atomic(), _vm_type);\n@@ -6485,1 +7134,1 @@\n-    k = ciObjArrayKlass::make(k);\n+    k = ciArrayKlass::make(k, is_null_free(), is_atomic(), _vm_type);\n@@ -6532,5 +7181,7 @@\n-\n-  if( _offset ) {               \/\/ Dump offset, if any\n-    if( _offset == OffsetBot )      { st->print(\"+any\"); }\n-    else if( _offset == OffsetTop ) { st->print(\"+unknown\"); }\n-    else                            { st->print(\"+%d\", _offset); }\n+  if (_flat) st->print(\":flat\");\n+  if (_null_free) st->print(\":null free\");\n+  if (_atomic) st->print(\":atomic\");\n+  if (_vm_type) st->print(\":vm_type\");\n+  if (Verbose) {\n+    if (_not_flat) st->print(\":not flat\");\n+    if (_not_null_free) st->print(\":nullable\");\n@@ -6539,0 +7190,2 @@\n+  _offset.dump2(st);\n+\n@@ -6557,2 +7210,14 @@\n-const TypeFunc *TypeFunc::make( const TypeTuple *domain, const TypeTuple *range ) {\n-  return (TypeFunc*)(new TypeFunc(domain,range))->hashcons();\n+const TypeFunc *TypeFunc::make(const TypeTuple *domain_sig, const TypeTuple* domain_cc,\n+                               const TypeTuple *range_sig, const TypeTuple *range_cc) {\n+  return (TypeFunc*)(new TypeFunc(domain_sig, domain_cc, range_sig, range_cc))->hashcons();\n+}\n+\n+const TypeFunc *TypeFunc::make(const TypeTuple *domain, const TypeTuple *range) {\n+  return make(domain, domain, range, range);\n+}\n+\n+\/\/------------------------------osr_domain-----------------------------\n+const TypeTuple* osr_domain() {\n+  const Type **fields = TypeTuple::fields(2);\n+  fields[TypeFunc::Parms+0] = TypeRawPtr::BOTTOM;  \/\/ address of osr buffer\n+  return TypeTuple::make(TypeFunc::Parms+1, fields);\n@@ -6562,1 +7227,1 @@\n-const TypeFunc *TypeFunc::make(ciMethod* method) {\n+const TypeFunc* TypeFunc::make(ciMethod* method, bool is_osr_compilation) {\n@@ -6564,7 +7229,24 @@\n-  const TypeFunc* tf = C->last_tf(method); \/\/ check cache\n-  if (tf != nullptr)  return tf;  \/\/ The hit rate here is almost 50%.\n-  const TypeTuple *domain;\n-  if (method->is_static()) {\n-    domain = TypeTuple::make_domain(nullptr, method->signature(), ignore_interfaces);\n-  } else {\n-    domain = TypeTuple::make_domain(method->holder(), method->signature(), ignore_interfaces);\n+  const TypeFunc* tf = nullptr;\n+  if (!is_osr_compilation) {\n+    tf = C->last_tf(method); \/\/ check cache\n+    if (tf != nullptr)  return tf;  \/\/ The hit rate here is almost 50%.\n+  }\n+  \/\/ Inline types are not passed\/returned by reference, instead each field of\n+  \/\/ the inline type is passed\/returned as an argument. We maintain two views of\n+  \/\/ the argument\/return list here: one based on the signature (with an inline\n+  \/\/ type argument\/return as a single slot), one based on the actual calling\n+  \/\/ convention (with an inline type argument\/return as a list of its fields).\n+  bool has_scalar_args = method->has_scalarized_args() && !is_osr_compilation;\n+  \/\/ Fall back to the non-scalarized calling convention when compiling a call via a mismatching method\n+  if (method != C->method() && method->get_Method()->mismatch()) {\n+    has_scalar_args = false;\n+  }\n+  const TypeTuple* domain_sig = is_osr_compilation ? osr_domain() : TypeTuple::make_domain(method, ignore_interfaces, false);\n+  const TypeTuple* domain_cc = has_scalar_args ? TypeTuple::make_domain(method, ignore_interfaces, true) : domain_sig;\n+  ciSignature* sig = method->signature();\n+  bool has_scalar_ret = !method->is_native() && sig->return_type()->is_inlinetype() && sig->return_type()->as_inline_klass()->can_be_returned_as_fields();\n+  const TypeTuple* range_sig = TypeTuple::make_range(sig, ignore_interfaces, false);\n+  const TypeTuple* range_cc = has_scalar_ret ? TypeTuple::make_range(sig, ignore_interfaces, true) : range_sig;\n+  tf = TypeFunc::make(domain_sig, domain_cc, range_sig, range_cc);\n+  if (!is_osr_compilation) {\n+    C->set_last_tf(method, tf);  \/\/ fill cache\n@@ -6572,3 +7254,0 @@\n-  const TypeTuple *range  = TypeTuple::make_range(method->signature(), ignore_interfaces);\n-  tf = TypeFunc::make(domain, range);\n-  C->set_last_tf(method, tf);  \/\/ fill cache\n@@ -6609,2 +7288,4 @@\n-  return _domain == a->_domain &&\n-    _range == a->_range;\n+  return _domain_sig == a->_domain_sig &&\n+    _domain_cc == a->_domain_cc &&\n+    _range_sig == a->_range_sig &&\n+    _range_cc == a->_range_cc;\n@@ -6616,1 +7297,1 @@\n-  return (uint)(uintptr_t)_domain + (uint)(uintptr_t)_range;\n+  return (uint)(intptr_t)_domain_sig + (uint)(intptr_t)_domain_cc + (uint)(intptr_t)_range_sig + (uint)(intptr_t)_range_cc;\n@@ -6623,1 +7304,1 @@\n-  if( _range->cnt() <= Parms )\n+  if( _range_sig->cnt() <= Parms )\n@@ -6627,2 +7308,2 @@\n-    for (i = Parms; i < _range->cnt()-1; i++) {\n-      _range->field_at(i)->dump2(d,depth,st);\n+    for (i = Parms; i < _range_sig->cnt()-1; i++) {\n+      _range_sig->field_at(i)->dump2(d,depth,st);\n@@ -6631,1 +7312,1 @@\n-    _range->field_at(i)->dump2(d,depth,st);\n+    _range_sig->field_at(i)->dump2(d,depth,st);\n@@ -6640,3 +7321,3 @@\n-  if (Parms < _domain->cnt())\n-    _domain->field_at(Parms)->dump2(d,depth-1,st);\n-  for (uint i = Parms+1; i < _domain->cnt(); i++) {\n+  if (Parms < _domain_sig->cnt())\n+    _domain_sig->field_at(Parms)->dump2(d,depth-1,st);\n+  for (uint i = Parms+1; i < _domain_sig->cnt(); i++) {\n@@ -6644,1 +7325,1 @@\n-    _domain->field_at(i)->dump2(d,depth-1,st);\n+    _domain_sig->field_at(i)->dump2(d,depth-1,st);\n@@ -6664,1 +7345,1 @@\n-  if (range()->cnt() == TypeFunc::Parms) {\n+  if (range_sig()->cnt() == TypeFunc::Parms) {\n@@ -6667,1 +7348,1 @@\n-  return range()->field_at(TypeFunc::Parms)->basic_type();\n+  return range_sig()->field_at(TypeFunc::Parms)->basic_type();\n","filename":"src\/hotspot\/share\/opto\/type.cpp","additions":1033,"deletions":352,"binary":false,"changes":1385,"status":"modified"},{"patch":"@@ -61,0 +61,1 @@\n+#include \"oops\/flatArrayKlass.hpp\"\n@@ -68,0 +69,1 @@\n+#include \"oops\/refArrayOop.inline.hpp\"\n@@ -416,0 +418,148 @@\n+static void validate_array_arguments(Klass* elmClass, jint len, TRAPS) {\n+  if (len < 0) {\n+    THROW_MSG(vmSymbols::java_lang_IllegalArgumentException(), \"Array length is negative\");\n+  }\n+  elmClass->initialize(CHECK);\n+  if (elmClass->is_array_klass() || elmClass->is_identity_class()) {\n+    THROW_MSG(vmSymbols::java_lang_IllegalArgumentException(), \"Element class is not a value class\");\n+  }\n+  if (elmClass->is_abstract()) {\n+    THROW_MSG(vmSymbols::java_lang_IllegalArgumentException(), \"Element class is abstract\");\n+  }\n+}\n+\n+JVM_ENTRY(jarray, JVM_CopyOfSpecialArray(JNIEnv *env, jarray orig, jint from, jint to))\n+  oop o = JNIHandles::resolve_non_null(orig);\n+  assert(o->is_array(), \"Must be\");\n+  oop array = nullptr;\n+  arrayOop org = (arrayOop)o;\n+  arrayHandle oh(THREAD, org);\n+  ObjArrayKlass* ak = ObjArrayKlass::cast(org->klass());\n+  InlineKlass* vk = InlineKlass::cast(ak->element_klass());\n+  int len = to - from;  \/\/ length of the new array\n+  if (ak->is_null_free_array_klass()) {\n+    if ((len != 0) && (from >= org->length() || to > org->length())) {\n+      THROW_MSG_NULL(vmSymbols::java_lang_IllegalArgumentException(), \"Copying of null-free array with uninitialized elements\");\n+    }\n+  }\n+  if (org->is_flatArray()) {\n+    FlatArrayKlass* fak = FlatArrayKlass::cast(org->klass());\n+    LayoutKind lk = fak->layout_kind();\n+    ArrayKlass::ArrayProperties props = ArrayKlass::ArrayProperties::DEFAULT;\n+    switch(lk) {\n+      case LayoutKind::ATOMIC_FLAT:\n+        props = ArrayKlass::ArrayProperties::NULL_RESTRICTED;\n+      break;\n+      case LayoutKind::NON_ATOMIC_FLAT:\n+        props = (ArrayKlass::ArrayProperties)(ArrayKlass::ArrayProperties::NULL_RESTRICTED | ArrayKlass::ArrayProperties::NON_ATOMIC);\n+      break;\n+      case LayoutKind::NULLABLE_ATOMIC_FLAT:\n+      props = ArrayKlass::ArrayProperties::NON_ATOMIC;\n+      break;\n+      default:\n+        ShouldNotReachHere();\n+    }\n+    array = oopFactory::new_flatArray(vk, len, props, lk, CHECK_NULL);\n+    arrayHandle ah(THREAD, (arrayOop)array);\n+    int end = to < oh()->length() ? to : oh()->length();\n+    for (int i = from; i < end; i++) {\n+      void* src = ((flatArrayOop)oh())->value_at_addr(i, fak->layout_helper());\n+      void* dst = ((flatArrayOop)ah())->value_at_addr(i - from, fak->layout_helper());\n+      vk->copy_payload_to_addr(src, dst, lk, false);\n+    }\n+    array = ah();\n+  } else {\n+    ArrayKlass::ArrayProperties props = org->is_null_free_array() ? ArrayKlass::ArrayProperties::NULL_RESTRICTED : ArrayKlass::ArrayProperties::DEFAULT;\n+    array = oopFactory::new_objArray(vk, len, props,  CHECK_NULL);\n+    int end = to < oh()->length() ? to : oh()->length();\n+    for (int i = from; i < end; i++) {\n+      if (i < ((objArrayOop)oh())->length()) {\n+        ((objArrayOop)array)->obj_at_put(i - from, ((objArrayOop)oh())->obj_at(i));\n+      } else {\n+        assert(!ak->is_null_free_array_klass(), \"Must be a nullable array\");\n+        ((objArrayOop)array)->obj_at_put(i - from, nullptr);\n+      }\n+    }\n+  }\n+  return (jarray) JNIHandles::make_local(THREAD, array);\n+JVM_END\n+\n+JVM_ENTRY(jarray, JVM_NewNullRestrictedNonAtomicArray(JNIEnv *env, jclass elmClass, jint len, jobject initVal))\n+  oop mirror = JNIHandles::resolve_non_null(elmClass);\n+  oop init = JNIHandles::resolve(initVal);\n+  if (init == nullptr) {\n+    THROW_MSG_NULL(vmSymbols::java_lang_IllegalArgumentException(), \"Initial value cannot be null\");\n+  }\n+  Handle init_h(THREAD, init);\n+  Klass* klass = java_lang_Class::as_Klass(mirror);\n+  if (klass != init_h()->klass()) {\n+    THROW_MSG_NULL(vmSymbols::java_lang_IllegalArgumentException(), \"Type mismatch between array and initial value\");\n+  }\n+  validate_array_arguments(klass, len, CHECK_NULL);\n+  InlineKlass* vk = InlineKlass::cast(klass);\n+  ArrayKlass::ArrayProperties props = (ArrayKlass::ArrayProperties)(ArrayKlass::ArrayProperties::NON_ATOMIC | ArrayKlass::ArrayProperties::NULL_RESTRICTED);\n+  objArrayOop array = oopFactory::new_objArray(klass, len, props, CHECK_NULL);\n+  for (int i = 0; i < len; i++) {\n+    array->obj_at_put(i, init_h() \/*, CHECK_NULL*\/ );\n+  }\n+  return (jarray) JNIHandles::make_local(THREAD, array);\n+JVM_END\n+\n+JVM_ENTRY(jarray, JVM_NewNullRestrictedAtomicArray(JNIEnv *env, jclass elmClass, jint len, jobject initVal))\n+  oop mirror = JNIHandles::resolve_non_null(elmClass);\n+  oop init = JNIHandles::resolve(initVal);\n+  if (init == nullptr) {\n+    THROW_MSG_NULL(vmSymbols::java_lang_IllegalArgumentException(), \"Initial value cannot be null\");\n+  }\n+  Handle init_h(THREAD, init);\n+  Klass* klass = java_lang_Class::as_Klass(mirror);\n+  if (klass != init_h()->klass()) {\n+    THROW_MSG_NULL(vmSymbols::java_lang_IllegalArgumentException(), \"Type mismatch between array and initial value\");\n+  }\n+  validate_array_arguments(klass, len, CHECK_NULL);\n+  InlineKlass* vk = InlineKlass::cast(klass);\n+  ArrayKlass::ArrayProperties props = (ArrayKlass::ArrayProperties)(ArrayKlass::ArrayProperties::NULL_RESTRICTED);\n+  objArrayOop array = oopFactory::new_objArray(klass, len, props, CHECK_NULL);\n+  for (int i = 0; i < len; i++) {\n+    array->obj_at_put(i, init_h() \/*, CHECK_NULL*\/ );\n+  }\n+  return (jarray) JNIHandles::make_local(THREAD, array);\n+JVM_END\n+\n+JVM_ENTRY(jarray, JVM_NewNullableAtomicArray(JNIEnv *env, jclass elmClass, jint len))\n+  oop mirror = JNIHandles::resolve_non_null(elmClass);\n+  Klass* klass = java_lang_Class::as_Klass(mirror);\n+  klass->initialize(CHECK_NULL);\n+  validate_array_arguments(klass, len, CHECK_NULL);\n+  InlineKlass* vk = InlineKlass::cast(klass);\n+  ArrayKlass::ArrayProperties props = (ArrayKlass::ArrayProperties)(ArrayKlass::ArrayProperties::DEFAULT);\n+  objArrayOop array = oopFactory::new_objArray(klass, len, props, CHECK_NULL);\n+  return (jarray) JNIHandles::make_local(THREAD, array);\n+JVM_END\n+\n+JVM_ENTRY(jboolean, JVM_IsFlatArray(JNIEnv *env, jobject obj))\n+  arrayOop oop = arrayOop(JNIHandles::resolve_non_null(obj));\n+  return oop->is_flatArray();\n+JVM_END\n+\n+JVM_ENTRY(jboolean, JVM_IsNullRestrictedArray(JNIEnv *env, jobject obj))\n+  arrayOop oop = arrayOop(JNIHandles::resolve_non_null(obj));\n+  return oop->is_null_free_array();\n+JVM_END\n+\n+JVM_ENTRY(jboolean, JVM_IsAtomicArray(JNIEnv *env, jobject obj))\n+  \/\/ There are multiple cases where an array can\/must support atomic access:\n+  \/\/   - the array is a reference array\n+  \/\/   - the array uses an atomic flat layout: NULLABLE_ATOMIC_FLAT or ATOMIC_FLAT\n+  \/\/   - the array is flat and its component type is naturally atomic\n+  arrayOop oop = arrayOop(JNIHandles::resolve_non_null(obj));\n+  if (oop->is_refArray()) return true;\n+  if (oop->is_flatArray()) {\n+    FlatArrayKlass* fak = FlatArrayKlass::cast(oop->klass());\n+    if (fak->layout_kind() == LayoutKind::ATOMIC_FLAT || fak->layout_kind() == LayoutKind::NULLABLE_ATOMIC_FLAT) {\n+      return true;\n+    }\n+    if (fak->element_klass()->is_naturally_atomic()) return true;\n+  }\n+  return false;\n+JVM_END\n@@ -624,2 +774,22 @@\n-  return handle == nullptr ? 0 :\n-         checked_cast<jint>(ObjectSynchronizer::FastHashCode (THREAD, JNIHandles::resolve_non_null(handle)));\n+  if (handle == nullptr) {\n+    return 0;\n+  }\n+  oop obj = JNIHandles::resolve_non_null(handle);\n+  if (EnableValhalla && obj->klass()->is_inline_klass()) {\n+      JavaValue result(T_INT);\n+      JavaCallArguments args;\n+      Handle ho(THREAD, obj);\n+      args.push_oop(ho);\n+      methodHandle method(THREAD, Universe::value_object_hash_code_method());\n+      JavaCalls::call(&result, method, &args, THREAD);\n+      if (HAS_PENDING_EXCEPTION) {\n+        if (!PENDING_EXCEPTION->is_a(vmClasses::Error_klass())) {\n+          Handle e(THREAD, PENDING_EXCEPTION);\n+          CLEAR_PENDING_EXCEPTION;\n+          THROW_MSG_CAUSE_(vmSymbols::java_lang_InternalError(), \"Internal error in hashCode\", e, false);\n+        }\n+      }\n+      return result.get_jint();\n+  } else {\n+    return checked_cast<jint>(ObjectSynchronizer::FastHashCode(THREAD, obj));\n+  }\n@@ -673,0 +843,6 @@\n+  if (klass->is_inline_klass()) {\n+    \/\/ Value instances have no identity, so return the current instance instead of allocating a new one\n+    \/\/ Value classes cannot have finalizers, so the method can return immediately\n+    return JNIHandles::make_local(THREAD, obj());\n+  }\n+\n@@ -1167,1 +1343,2 @@\n-    size = InstanceKlass::cast(klass)->local_interfaces()->length();\n+    InstanceKlass* ik = InstanceKlass::cast(klass);\n+    size = ik->local_interfaces()->length();\n@@ -1180,1 +1357,2 @@\n-      Klass* k = InstanceKlass::cast(klass)->local_interfaces()->at(index);\n+      InstanceKlass* ik = InstanceKlass::cast(klass);\n+      Klass* k = ik->local_interfaces()->at(index);\n@@ -1201,1 +1379,0 @@\n-\n@@ -1685,1 +1862,1 @@\n-    if (want_constructor && !method->is_object_initializer()) {\n+    if (want_constructor && !method->is_object_constructor()) {\n@@ -1689,1 +1866,1 @@\n-        (method->is_object_initializer() || method->is_static_initializer() ||\n+        (method->is_object_constructor() || method->is_class_initializer() ||\n@@ -1717,0 +1894,1 @@\n+        assert(method->is_object_constructor(), \"must be\");\n@@ -1999,1 +2177,1 @@\n-  if (m->is_object_initializer()) {\n+  if (m->is_object_constructor()) {\n@@ -2002,1 +2180,0 @@\n-    \/\/ new_method accepts <clinit> as Method here\n@@ -2452,1 +2629,1 @@\n-  return method->name() == vmSymbols::object_initializer_name();\n+  return method->is_object_constructor();\n@@ -3260,0 +3437,4 @@\n+JVM_LEAF(jboolean, JVM_IsValhallaEnabled(void))\n+  return EnableValhalla ? JNI_TRUE : JNI_FALSE;\n+JVM_END\n+\n@@ -3339,1 +3520,3 @@\n-    objArrayHandle args(THREAD, objArrayOop(JNIHandles::resolve(args0)));\n+    objArrayHandle args(THREAD, (objArrayOop)JNIHandles::resolve(args0));\n+    assert(args() == nullptr || !args->is_flatArray(), \"args are never flat or are they???\");\n+\n@@ -3359,0 +3542,2 @@\n+  objArrayHandle args(THREAD, (objArrayOop)JNIHandles::resolve(args0));\n+  assert(args() == nullptr || !args->is_flatArray(), \"args are never flat or are they???\");\n@@ -3360,1 +3545,0 @@\n-  objArrayHandle args(THREAD, objArrayOop(JNIHandles::resolve(args0)));\n@@ -3600,0 +3784,1 @@\n+  refArrayHandle rah(THREAD, (refArrayOop)ah()); \/\/ j.l.Thread is an identity class, arrays are always reference arrays\n@@ -3605,1 +3790,1 @@\n-    oop thread_obj = ah->obj_at(i);\n+    oop thread_obj = rah->obj_at(i);\n","filename":"src\/hotspot\/share\/prims\/jvm.cpp","additions":198,"deletions":13,"binary":false,"changes":211,"status":"modified"},{"patch":"@@ -26,0 +26,1 @@\n+#include \"classfile\/vmClasses.hpp\"\n@@ -474,0 +475,20 @@\n+\/\/ LoadableDescriptors {\n+\/\/   u2 attribute_name_index;\n+\/\/   u4 attribute_length;\n+\/\/   u2 number_of_descriptors;\n+\/\/   u2 descriptors[number_of_descriptors];\n+\/\/ }\n+void JvmtiClassFileReconstituter::write_loadable_descriptors_attribute() {\n+  Array<u2>* loadable_descriptors = ik()->loadable_descriptors();\n+  int number_of_descriptors = loadable_descriptors->length();\n+  int length = sizeof(u2) * (1 + number_of_descriptors); \/\/ '1 +' is for number_of_descriptors field\n+\n+  write_attribute_name_index(\"LoadableDescriptors\");\n+  write_u4(length);\n+  write_u2(checked_cast<u2>(number_of_descriptors));\n+  for (int i = 0; i < number_of_descriptors; i++) {\n+    u2 utf8_index = loadable_descriptors->at(i);\n+    write_u2(utf8_index);\n+  }\n+}\n+\n@@ -554,1 +575,6 @@\n-    write_u2(iter.inner_access_flags());\n+    u2 flags = iter.inner_access_flags();\n+    \/\/ ClassFileParser may add identity to inner class attributes, so remove it.\n+    if (!ik()->supports_inline_types()) {\n+      flags &= ~JVM_ACC_IDENTITY;;\n+    }\n+    write_u2(flags);\n@@ -813,0 +839,3 @@\n+  if (ik()->loadable_descriptors() != Universe::the_empty_short_array()) {\n+    ++attr_count;\n+  }\n@@ -843,0 +872,3 @@\n+  if (ik()->loadable_descriptors() != Universe::the_empty_short_array()) {\n+    write_loadable_descriptors_attribute();\n+  }\n@@ -1032,1 +1064,1 @@\n-      case Bytecodes::_putfield        :  {\n+      case Bytecodes::_putfield        : {\n","filename":"src\/hotspot\/share\/prims\/jvmtiClassFileReconstituter.cpp","additions":34,"deletions":2,"binary":false,"changes":36,"status":"modified"},{"patch":"@@ -30,0 +30,1 @@\n+#include \"classfile\/klassFactory.hpp\"\n@@ -34,1 +35,0 @@\n-#include \"classfile\/klassFactory.hpp\"\n@@ -615,2 +615,1 @@\n-    \/\/ At this stage JVM_CONSTANT_UnresolvedClassInError should not be\n-    \/\/ here\n+    \/\/ At this stage JVM_CONSTANT_UnresolvedClassInError should not be here\n@@ -1933,0 +1932,6 @@\n+  \/\/ rewrite constant pool references in the LoadableDescriptors attribute:\n+  if (!rewrite_cp_refs_in_loadable_descriptors_attribute(scratch_class)) {\n+    \/\/ propagate failure back to caller\n+    return false;\n+  }\n+\n@@ -2081,0 +2086,13 @@\n+\/\/ Rewrite constant pool references in the LoadableDescriptors attribute.\n+bool VM_RedefineClasses::rewrite_cp_refs_in_loadable_descriptors_attribute(\n+       InstanceKlass* scratch_class) {\n+\n+  Array<u2>* loadable_descriptors = scratch_class->loadable_descriptors();\n+  assert(loadable_descriptors != nullptr, \"unexpected null loadable_descriptors\");\n+  for (int i = 0; i < loadable_descriptors->length(); i++) {\n+    u2 cp_index = loadable_descriptors->at(i);\n+    loadable_descriptors->at_put(i, find_new_index(cp_index));\n+  }\n+  return true;\n+}\n+\n@@ -3268,0 +3286,8 @@\n+   if (frame_type == 246) {  \/\/ EARLY_LARVAL\n+     \/\/ rewrite_cp_refs in  unset fields and fall through.\n+     rewrite_cp_refs_in_early_larval_stackmaps(stackmap_p, stackmap_end, calc_number_of_entries, frame_type);\n+     \/\/ The larval frames point to the next frame, so advance to the next frame and fall through.\n+     frame_type = *stackmap_p;\n+     stackmap_p++;\n+   }\n+\n@@ -3477,0 +3503,23 @@\n+void VM_RedefineClasses::rewrite_cp_refs_in_early_larval_stackmaps(\n+       address& stackmap_p_ref, address stackmap_end, u2 frame_i,\n+       u1 frame_type) {\n+\n+    u2 num_early_larval_stackmaps = Bytes::get_Java_u2(stackmap_p_ref);\n+    stackmap_p_ref += 2;\n+\n+    for (u2 i = 0; i < num_early_larval_stackmaps; i++) {\n+\n+      u2 name_and_ref_index = Bytes::get_Java_u2(stackmap_p_ref);\n+      u2 new_cp_index = find_new_index(name_and_ref_index);\n+      if (new_cp_index != 0) {\n+        log_debug(redefine, class, stackmap)(\"mapped old name_and_ref_index=%d\", name_and_ref_index);\n+        Bytes::put_Java_u2(stackmap_p_ref, new_cp_index);\n+        name_and_ref_index = new_cp_index;\n+      }\n+      log_debug(redefine, class, stackmap)\n+        (\"frame_i=%u, frame_type=%u, name_and_ref_index=%d\", frame_i, frame_type, name_and_ref_index);\n+\n+      stackmap_p_ref += 2;\n+    }\n+} \/\/ rewrite_cp_refs_in_early_larval_stackmaps\n+\n","filename":"src\/hotspot\/share\/prims\/jvmtiRedefineClasses.cpp","additions":52,"deletions":3,"binary":false,"changes":55,"status":"modified"},{"patch":"@@ -55,0 +55,1 @@\n+#include \"memory\/iterator.inline.hpp\"\n@@ -61,0 +62,1 @@\n+#include \"oops\/access.hpp\"\n@@ -63,0 +65,1 @@\n+#include \"oops\/compressedOops.inline.hpp\"\n@@ -70,0 +73,1 @@\n+#include \"oops\/objArrayOop.inline.hpp\"\n@@ -88,0 +92,1 @@\n+#include \"runtime\/keepStackGCProcessed.hpp\"\n@@ -1952,0 +1957,103 @@\n+WB_ENTRY(jobjectArray, WB_getObjectsViaKlassOopMaps(JNIEnv* env, jobject wb, jobject thing))\n+  oop aoop = JNIHandles::resolve(thing);\n+  if (!aoop->is_instance()) {\n+    return nullptr;\n+  }\n+  instanceHandle ih(THREAD, (instanceOop) aoop);\n+  InstanceKlass* klass = InstanceKlass::cast(ih->klass());\n+  if (klass->nonstatic_oop_map_count() == 0) {\n+    return nullptr;\n+  }\n+  const OopMapBlock* map = klass->start_of_nonstatic_oop_maps();\n+  const OopMapBlock* const end = map + klass->nonstatic_oop_map_count();\n+  int oop_count = 0;\n+  while (map < end) {\n+    oop_count += map->count();\n+    map++;\n+  }\n+\n+  objArrayHandle result_array =\n+      oopFactory::new_objArray_handle(vmClasses::Object_klass(), oop_count, CHECK_NULL);\n+  map = klass->start_of_nonstatic_oop_maps();\n+  int index = 0;\n+  while (map < end) {\n+    int offset = map->offset();\n+    for (unsigned int j = 0; j < map->count(); j++) {\n+      result_array->obj_at_put(index++, ih->obj_field(offset));\n+      offset += heapOopSize;\n+    }\n+    map++;\n+  }\n+  return (jobjectArray)JNIHandles::make_local(THREAD, result_array());\n+WB_END\n+\n+\/\/ Collect Object oops but not value objects...loaded from heap\n+class CollectObjectOops : public BasicOopIterateClosure {\n+  public:\n+  GrowableArray<Handle>* _array;\n+\n+  CollectObjectOops() {\n+      _array = new GrowableArray<Handle>(128);\n+  }\n+\n+  void add_oop(oop o) {\n+    Handle oh = Handle(Thread::current(), o);\n+    if (oh != nullptr && oh->is_inline_type()) {\n+      oh->oop_iterate(this);\n+    } else {\n+      _array->append(oh);\n+    }\n+  }\n+\n+  template <class T> inline void add_oop(T* p) { add_oop(HeapAccess<>::oop_load(p)); }\n+  void do_oop(oop* o) { add_oop(o); }\n+  void do_oop(narrowOop* v) { add_oop(v); }\n+\n+  jobjectArray create_jni_result(JNIEnv* env, TRAPS) {\n+    objArrayHandle result_array =\n+        oopFactory::new_objArray_handle(vmClasses::Object_klass(), _array->length(), CHECK_NULL);\n+    for (int i = 0 ; i < _array->length(); i++) {\n+      result_array->obj_at_put(i, _array->at(i)());\n+    }\n+    return (jobjectArray)JNIHandles::make_local(THREAD, result_array());\n+  }\n+};\n+\n+\/\/ Collect Object oops but not value objects...loaded from frames\n+class CollectFrameObjectOops : public BasicOopIterateClosure {\n+ public:\n+  CollectObjectOops _collect;\n+\n+  template <class T> inline void add_oop(T* p) { _collect.add_oop(RawAccess<>::oop_load(p)); }\n+  void do_oop(oop* o) { add_oop(o); }\n+  void do_oop(narrowOop* v) { add_oop(v); }\n+\n+  jobjectArray create_jni_result(JNIEnv* env, TRAPS) {\n+    return _collect.create_jni_result(env, THREAD);\n+  }\n+};\n+\n+\/\/ Collect Object oops for the given oop, iterate through value objects\n+WB_ENTRY(jobjectArray, WB_getObjectsViaOopIterator(JNIEnv* env, jobject wb, jobject thing))\n+  ResourceMark rm(thread);\n+  Handle objh(thread, JNIHandles::resolve(thing));\n+  CollectObjectOops collectOops;\n+  objh->oop_iterate(&collectOops);\n+  return collectOops.create_jni_result(env, THREAD);\n+WB_END\n+\n+\/\/ Collect Object oops for the given frame deep, iterate through value objects\n+WB_ENTRY(jobjectArray, WB_getObjectsViaFrameOopIterator(JNIEnv* env, jobject wb, jint depth))\n+  KeepStackGCProcessedMark ksgcpm(THREAD);\n+  ResourceMark rm(THREAD);\n+  CollectFrameObjectOops collectOops;\n+  StackFrameStream sfs(thread, true \/* update *\/, true \/* process_frames *\/);\n+  while (depth > 0) { \/\/ Skip the native WB API frame\n+    sfs.next();\n+    frame* f = sfs.current();\n+    f->oops_do(&collectOops, nullptr, sfs.register_map());\n+    depth--;\n+  }\n+  return collectOops.create_jni_result(env, THREAD);\n+WB_END\n+\n@@ -2918,0 +3026,6 @@\n+  {CC\"getObjectsViaKlassOopMaps0\",\n+      CC\"(Ljava\/lang\/Object;)[Ljava\/lang\/Object;\",    (void*)&WB_getObjectsViaKlassOopMaps},\n+  {CC\"getObjectsViaOopIterator0\",\n+          CC\"(Ljava\/lang\/Object;)[Ljava\/lang\/Object;\",(void*)&WB_getObjectsViaOopIterator},\n+  {CC\"getObjectsViaFrameOopIterator\",\n+      CC\"(I)[Ljava\/lang\/Object;\",                     (void*)&WB_getObjectsViaFrameOopIterator},\n","filename":"src\/hotspot\/share\/prims\/whitebox.cpp","additions":114,"deletions":0,"binary":false,"changes":114,"status":"modified"},{"patch":"@@ -78,0 +78,1 @@\n+#include <string.h>\n@@ -365,0 +366,12 @@\n+bool Arguments::patching_migrated_classes(const char* property, const char* value) {\n+  if (strncmp(property, MODULE_PROPERTY_PREFIX, MODULE_PROPERTY_PREFIX_LEN) == 0) {\n+    const char* property_suffix = property + MODULE_PROPERTY_PREFIX_LEN;\n+    if (matches_property_suffix(property_suffix, PATCH, PATCH_LEN)) {\n+      if (strcmp(value, \"java.base-valueclasses.jar\")) {\n+        return true;\n+      }\n+    }\n+  }\n+  return false;\n+}\n+\n@@ -1786,1 +1799,0 @@\n-static unsigned int patch_mod_count = 0;\n@@ -1793,1 +1805,1 @@\n-  if (!CDSConfig::check_vm_args_consistency(patch_mod_javabase, mode_flag_cmd_line)) {\n+  if (!CDSConfig::check_vm_args_consistency(mode_flag_cmd_line)) {\n@@ -2069,1 +2081,1 @@\n-      add_patch_mod_prefix(module_name, module_equal + 1);\n+      add_patch_mod_prefix(module_name, module_equal + 1, false \/* no append *\/, false \/* no cds *\/);\n@@ -2071,3 +2083,0 @@\n-      if (!create_numbered_module_property(\"jdk.module.patch\", patch_mod_tail, patch_mod_count++)) {\n-        return JNI_ENOMEM;\n-      }\n@@ -2081,0 +2090,64 @@\n+\/\/ VALUECLASS_STR must match string used in the build\n+#define VALUECLASS_STR \"valueclasses\"\n+#define VALUECLASS_JAR \"-\" VALUECLASS_STR \".jar\"\n+\n+\/\/ Finalize --patch-module args and --enable-preview related to value class module patches.\n+\/\/ Create all numbered properties passing module patches.\n+int Arguments::finalize_patch_module() {\n+  \/\/ If --enable-preview and EnableValhalla is true, each module may have value classes that\n+  \/\/ are to be patched into the module.\n+  \/\/ For each <module>-valueclasses.jar in <JAVA_HOME>\/lib\/valueclasses\/\n+  \/\/ appends the equivalent of --patch-module <module>=<JAVA_HOME>\/lib\/valueclasses\/<module>-valueclasses.jar\n+  if (enable_preview() && EnableValhalla) {\n+    char * valueclasses_dir = AllocateHeap(JVM_MAXPATHLEN, mtArguments);\n+    const char * fileSep = os::file_separator();\n+\n+    jio_snprintf(valueclasses_dir, JVM_MAXPATHLEN, \"%s%slib%s\" VALUECLASS_STR \"%s\",\n+                 Arguments::get_java_home(), fileSep, fileSep, fileSep);\n+    DIR* dir = os::opendir(valueclasses_dir);\n+    if (dir != nullptr) {\n+      char * module_name = AllocateHeap(JVM_MAXPATHLEN, mtArguments);\n+      char * path = AllocateHeap(JVM_MAXPATHLEN, mtArguments);\n+\n+      for (dirent * entry = os::readdir(dir); entry != nullptr; entry = os::readdir(dir)) {\n+        \/\/ Test if file ends-with \"-valueclasses.jar\"\n+        int len = (int)strlen(entry->d_name) - (sizeof(VALUECLASS_JAR) - 1);\n+        if (len <= 0 || strcmp(&entry->d_name[len], VALUECLASS_JAR) != 0) {\n+          continue;         \/\/ too short or not the expected suffix\n+        }\n+\n+        strcpy(module_name, entry->d_name);\n+        module_name[len] = '\\0';     \/\/ truncate to just module-name\n+\n+        jio_snprintf(path, JVM_MAXPATHLEN, \"%s%s\", valueclasses_dir, &entry->d_name);\n+        add_patch_mod_prefix(module_name, path, true \/* append *\/, true \/* cds OK*\/);\n+        log_info(class)(\"--enable-preview appending value classes for module %s: %s\", module_name, entry->d_name);\n+      }\n+      FreeHeap(module_name);\n+      FreeHeap(path);\n+      os::closedir(dir);\n+    }\n+    FreeHeap(valueclasses_dir);\n+  }\n+\n+  \/\/ Create numbered properties for each module that has been patched either\n+  \/\/ by --patch-module or --enable-preview\n+  \/\/ Format is \"jdk.module.patch.<n>=<module_name>=<path>\"\n+  if (_patch_mod_prefix != nullptr) {\n+    char * prop_value = AllocateHeap(JVM_MAXPATHLEN + JVM_MAXPATHLEN + 1, mtArguments);\n+    unsigned int patch_mod_count = 0;\n+\n+    for (GrowableArrayIterator<ModulePatchPath *> it = _patch_mod_prefix->begin();\n+            it != _patch_mod_prefix->end(); ++it) {\n+      jio_snprintf(prop_value, JVM_MAXPATHLEN + JVM_MAXPATHLEN + 1, \"%s=%s\",\n+                   (*it)->module_name(), (*it)->path_string());\n+      if (!create_numbered_module_property(\"jdk.module.patch\", prop_value, patch_mod_count++)) {\n+        FreeHeap(prop_value);\n+        return JNI_ENOMEM;\n+      }\n+    }\n+    FreeHeap(prop_value);\n+  }\n+  return JNI_OK;\n+}\n+\n@@ -2349,0 +2422,4 @@\n+      \/\/ --enable-preview enables Valhalla, EnableValhalla VM option will eventually be removed before integration\n+      if (FLAG_SET_CMDLINE(EnableValhalla, true) != JVMFlag::SUCCESS) {\n+        return JNI_EINVAL;\n+      }\n@@ -2828,10 +2905,5 @@\n-void Arguments::add_patch_mod_prefix(const char* module_name, const char* path) {\n-  \/\/ For java.base check for duplicate --patch-module options being specified on the command line.\n-  \/\/ This check is only required for java.base, all other duplicate module specifications\n-  \/\/ will be checked during module system initialization.  The module system initialization\n-  \/\/ will throw an ExceptionInInitializerError if this situation occurs.\n-  if (strcmp(module_name, JAVA_BASE_NAME) == 0) {\n-    if (patch_mod_javabase) {\n-      vm_exit_during_initialization(\"Cannot specify \" JAVA_BASE_NAME \" more than once to --patch-module\");\n-    } else {\n-      patch_mod_javabase = true;\n+void Arguments::add_patch_mod_prefix(const char* module_name, const char* path, bool allow_append, bool allow_cds) {\n+  if (!allow_cds) {\n+    CDSConfig::set_module_patching_disables_cds();\n+    if (strcmp(module_name, JAVA_BASE_NAME) == 0) {\n+      CDSConfig::set_java_base_module_patching_disables_cds();\n@@ -2846,1 +2918,18 @@\n-  _patch_mod_prefix->push(new ModulePatchPath(module_name, path));\n+  \/\/ Scan patches for matching module\n+  int i = _patch_mod_prefix->find_if([&](ModulePatchPath* patch) {\n+    return (strcmp(module_name, patch->module_name()) == 0);\n+  });\n+  if (i == -1) {\n+    _patch_mod_prefix->push(new ModulePatchPath(module_name, path));\n+  } else {\n+    if (allow_append) {\n+      \/\/ append path to existing module entry\n+      _patch_mod_prefix->at(i)->append_path(path);\n+    } else {\n+      if (strcmp(module_name, JAVA_BASE_NAME) == 0) {\n+        vm_exit_during_initialization(\"Cannot specify \" JAVA_BASE_NAME \" more than once to --patch-module\");\n+      } else {\n+        vm_exit_during_initialization(\"Cannot specify a module more than once to --patch-module\", module_name);\n+      }\n+    }\n+  }\n@@ -2959,1 +3048,2 @@\n-  if (!check_vm_args_consistency()) {\n+  \/\/ finalize --module-patch and related --enable-preview\n+  if (finalize_patch_module() != JNI_OK) {\n@@ -2963,0 +3053,3 @@\n+  if (!check_vm_args_consistency()) {\n+    return JNI_ERR;\n+  }\n@@ -3851,0 +3944,12 @@\n+  if (!EnableValhalla || (is_interpreter_only() && !CDSConfig::is_dumping_archive() && !UseSharedSpaces)) {\n+    \/\/ Disable calling convention optimizations if inline types are not supported.\n+    \/\/ Also these aren't useful in -Xint. However, don't disable them when dumping or using\n+    \/\/ the CDS archive, as the values must match between dumptime and runtime.\n+    FLAG_SET_DEFAULT(InlineTypePassFieldsAsArgs, false);\n+    FLAG_SET_DEFAULT(InlineTypeReturnedAsFields, false);\n+  }\n+  if (!UseNonAtomicValueFlattening && !UseNullableValueFlattening && !UseAtomicValueFlattening) {\n+    \/\/ Flattening is disabled\n+    FLAG_SET_DEFAULT(UseArrayFlattening, false);\n+    FLAG_SET_DEFAULT(UseFieldFlattening, false);\n+  }\n","filename":"src\/hotspot\/share\/runtime\/arguments.cpp","additions":123,"deletions":18,"binary":false,"changes":141,"status":"modified"},{"patch":"@@ -53,0 +53,2 @@\n+#include \"oops\/flatArrayKlass.hpp\"\n+#include \"oops\/flatArrayOop.hpp\"\n@@ -58,0 +60,1 @@\n+#include \"oops\/inlineKlass.inline.hpp\"\n@@ -303,0 +306,18 @@\n+\n+static Klass* get_refined_array_klass(Klass* k, frame* fr, RegisterMap* map, ObjectValue* sv, TRAPS) {\n+  \/\/ If it's an array, get the properties\n+  if (k->is_array_klass() && !k->is_typeArray_klass()) {\n+    assert(!k->is_refArray_klass() && !k->is_flatArray_klass(), \"Unexpected refined klass\");\n+    nmethod* nm = fr->cb()->as_nmethod_or_null();\n+    if (nm->is_compiled_by_c2()) {\n+      assert(sv->has_properties(), \"Property information is missing\");\n+      ArrayKlass::ArrayProperties props = static_cast<ArrayKlass::ArrayProperties>(StackValue::create_stack_value(fr, map, sv->properties())->get_jint());\n+      k = ObjArrayKlass::cast(k)->klass_with_properties(props, THREAD);\n+    } else {\n+      \/\/ TODO Graal needs to be fixed. Just go with the default properties for now\n+      k = ObjArrayKlass::cast(k)->klass_with_properties(ArrayKlass::ArrayProperties::DEFAULT, THREAD);\n+    }\n+  }\n+  return k;\n+}\n+\n@@ -304,2 +325,2 @@\n-static void print_objects(JavaThread* deoptee_thread,\n-                          GrowableArray<ScopeValue*>* objects, bool realloc_failures) {\n+static void print_objects(JavaThread* deoptee_thread, frame* deoptee, RegisterMap* map,\n+                          GrowableArray<ScopeValue*>* objects, bool realloc_failures, TRAPS) {\n@@ -321,0 +342,1 @@\n+    k = get_refined_array_klass(k, deoptee, map, sv, THREAD);\n@@ -354,2 +376,13 @@\n-  bool save_oop_result = chunk->at(0)->scope()->return_oop() && !thread->popframe_forcing_deopt_reexecution() && (exec_mode == Deoptimization::Unpack_deopt);\n-  Handle return_value;\n+  ScopeDesc* scope = chunk->at(0)->scope();\n+  bool save_oop_result = scope->return_oop() && !thread->popframe_forcing_deopt_reexecution() && (exec_mode == Deoptimization::Unpack_deopt);\n+  \/\/ In case of the return of multiple values, we must take care\n+  \/\/ of all oop return values.\n+  GrowableArray<Handle> return_oops;\n+  InlineKlass* vk = nullptr;\n+  if (save_oop_result && scope->return_scalarized()) {\n+    vk = InlineKlass::returned_inline_klass(map);\n+    if (vk != nullptr) {\n+      vk->save_oop_fields(map, return_oops);\n+      save_oop_result = false;\n+    }\n+  }\n@@ -361,1 +394,1 @@\n-    return_value = Handle(thread, result);\n+    return_oops.push(Handle(thread, result));\n@@ -368,1 +401,1 @@\n-  if (objects != nullptr) {\n+  if (objects != nullptr || vk != nullptr) {\n@@ -373,1 +406,9 @@\n-      realloc_failures = Deoptimization::realloc_objects(thread, &deoptee, &map, objects, CHECK_AND_CLEAR_(true));\n+      if (vk != nullptr) {\n+        realloc_failures = Deoptimization::realloc_inline_type_result(vk, map, return_oops, CHECK_AND_CLEAR_(true));\n+      }\n+      if (objects != nullptr) {\n+        realloc_failures = realloc_failures || Deoptimization::realloc_objects(thread, &deoptee, &map, objects, CHECK_AND_CLEAR_(true));\n+        guarantee(compiled_method != nullptr, \"deopt must be associated with an nmethod\");\n+        bool is_jvmci = compiled_method->is_compiled_by_jvmci();\n+        Deoptimization::reassign_fields(&deoptee, &map, objects, realloc_failures, is_jvmci, CHECK_AND_CLEAR_(true));\n+      }\n@@ -378,1 +419,9 @@\n-      realloc_failures = Deoptimization::realloc_objects(thread, &deoptee, &map, objects, THREAD);\n+      if (vk != nullptr) {\n+        realloc_failures = Deoptimization::realloc_inline_type_result(vk, map, return_oops, THREAD);\n+      }\n+      if (objects != nullptr) {\n+        realloc_failures = realloc_failures || Deoptimization::realloc_objects(thread, &deoptee, &map, objects, THREAD);\n+        guarantee(compiled_method != nullptr, \"deopt must be associated with an nmethod\");\n+        bool is_jvmci = compiled_method->is_compiled_by_jvmci();\n+        Deoptimization::reassign_fields(&deoptee, &map, objects, realloc_failures, is_jvmci, THREAD);\n+      }\n@@ -381,5 +430,2 @@\n-    guarantee(compiled_method != nullptr, \"deopt must be associated with an nmethod\");\n-    bool is_jvmci = compiled_method->is_compiled_by_jvmci();\n-    Deoptimization::reassign_fields(&deoptee, &map, objects, realloc_failures, is_jvmci);\n-    if (TraceDeoptimization) {\n-      print_objects(deoptee_thread, objects, realloc_failures);\n+    if (TraceDeoptimization && objects != nullptr) {\n+      print_objects(deoptee_thread, &deoptee, &map, objects, realloc_failures, thread);\n@@ -388,1 +434,1 @@\n-  if (save_oop_result) {\n+  if (save_oop_result || vk != nullptr) {\n@@ -390,1 +436,2 @@\n-    deoptee.set_saved_oop_result(&map, return_value());\n+    assert(return_oops.length() == 1, \"no inline type\");\n+    deoptee.set_saved_oop_result(&map, return_oops.pop()());\n@@ -725,1 +772,1 @@\n-  \/\/ If the sender is deoptimized the we must retrieve the address of the handler\n+  \/\/ If the sender is deoptimized we must retrieve the address of the handler\n@@ -1241,2 +1288,10 @@\n-\n-    oop obj = nullptr;\n+    k = get_refined_array_klass(k, fr, reg_map, sv, THREAD);\n+\n+    \/\/ Check if the object may be null and has an additional null_marker input that needs\n+    \/\/ to be checked before using the field values. Skip re-allocation if it is null.\n+    if (k->is_inline_klass() && sv->has_properties()) {\n+      jint null_marker = StackValue::create_stack_value(fr, reg_map, sv->properties())->get_jint();\n+      if (null_marker == 0) {\n+        continue;\n+      }\n+    }\n@@ -1245,0 +1300,1 @@\n+    oop obj = nullptr;\n@@ -1272,0 +1328,4 @@\n+    } else if (k->is_flatArray_klass()) {\n+      FlatArrayKlass* ak = FlatArrayKlass::cast(k);\n+      \/\/ Inline type array must be zeroed because not all memory is reassigned\n+      obj = ak->allocate_instance(sv->field_size(), ak->properties(), THREAD);\n@@ -1278,2 +1338,2 @@\n-    } else if (k->is_objArray_klass()) {\n-      ObjArrayKlass* ak = ObjArrayKlass::cast(k);\n+    } else if (k->is_refArray_klass()) {\n+      RefArrayKlass* ak = RefArrayKlass::cast(k);\n@@ -1281,1 +1341,1 @@\n-      obj = ak->allocate_instance(sv->field_size(), THREAD);\n+      obj = ak->allocate_instance(sv->field_size(), ak->properties(), THREAD);\n@@ -1303,0 +1363,15 @@\n+\/\/ We're deoptimizing at the return of a call, inline type fields are\n+\/\/ in registers. When we go back to the interpreter, it will expect a\n+\/\/ reference to an inline type instance. Allocate and initialize it from\n+\/\/ the register values here.\n+bool Deoptimization::realloc_inline_type_result(InlineKlass* vk, const RegisterMap& map, GrowableArray<Handle>& return_oops, TRAPS) {\n+  oop new_vt = vk->realloc_result(map, return_oops, THREAD);\n+  if (new_vt == nullptr) {\n+    CLEAR_PENDING_EXCEPTION;\n+    THROW_OOP_(Universe::out_of_memory_error_realloc_objects(), true);\n+  }\n+  return_oops.clear();\n+  return_oops.push(Handle(THREAD, new_vt));\n+  return false;\n+}\n+\n@@ -1468,0 +1543,3 @@\n+  InstanceKlass* _klass;\n+  bool _is_flat;\n+  bool _is_null_free;\n@@ -1469,4 +1547,1 @@\n-  ReassignedField() {\n-    _offset = 0;\n-    _type = T_ILLEGAL;\n-  }\n+  ReassignedField() : _offset(0), _type(T_ILLEGAL), _klass(nullptr), _is_flat(false), _is_null_free(false) { }\n@@ -1486,0 +1561,6 @@\n+      if (fs.is_flat()) {\n+        field._is_flat = true;\n+        field._is_null_free = fs.is_null_free_inline_type();\n+        \/\/ Resolve klass of flat inline type field\n+        field._klass = InlineKlass::cast(klass->get_inline_type_field_klass(fs.index()));\n+      }\n@@ -1492,2 +1573,3 @@\n-\/\/ Restore fields of an eliminated instance object employing the same field order used by the compiler.\n-static int reassign_fields_by_klass(InstanceKlass* klass, frame* fr, RegisterMap* reg_map, ObjectValue* sv, int svIndex, oop obj, bool is_jvmci) {\n+\/\/ Restore fields of an eliminated instance object employing the same field order used by the\n+\/\/ compiler when it scalarizes an object at safepoints.\n+static int reassign_fields_by_klass(InstanceKlass* klass, frame* fr, RegisterMap* reg_map, ObjectValue* sv, int svIndex, oop obj, bool is_jvmci, int base_offset, TRAPS) {\n@@ -1496,0 +1578,19 @@\n+    BasicType type = fields->at(i)._type;\n+    int offset = base_offset + fields->at(i)._offset;\n+    \/\/ Check for flat inline type field before accessing the ScopeValue because it might not have any fields\n+    if (fields->at(i)._is_flat) {\n+      \/\/ Recursively re-assign flat inline type fields\n+      InstanceKlass* vk = fields->at(i)._klass;\n+      assert(vk != nullptr, \"must be resolved\");\n+      offset -= InlineKlass::cast(vk)->payload_offset(); \/\/ Adjust offset to omit oop header\n+      svIndex = reassign_fields_by_klass(vk, fr, reg_map, sv, svIndex, obj, is_jvmci, offset, CHECK_0);\n+      if (!fields->at(i)._is_null_free) {\n+        ScopeValue* scope_field = sv->field_at(svIndex);\n+        StackValue* value = StackValue::create_stack_value(fr, reg_map, scope_field);\n+        int nm_offset = offset + InlineKlass::cast(vk)->null_marker_offset();\n+        obj->bool_field_put(nm_offset, value->get_jint() & 1);\n+        svIndex++;\n+      }\n+      continue; \/\/ Continue because we don't need to increment svIndex\n+    }\n+\n@@ -1498,3 +1599,2 @@\n-    int offset = fields->at(i)._offset;\n-    BasicType type = fields->at(i)._type;\n-      case T_OBJECT: case T_ARRAY:\n+      case T_OBJECT:\n+      case T_ARRAY:\n@@ -1572,0 +1672,1 @@\n+\n@@ -1575,0 +1676,14 @@\n+\/\/ restore fields of an eliminated inline type array\n+void Deoptimization::reassign_flat_array_elements(frame* fr, RegisterMap* reg_map, ObjectValue* sv, flatArrayOop obj, FlatArrayKlass* vak, bool is_jvmci, TRAPS) {\n+  InlineKlass* vk = vak->element_klass();\n+  assert(vk->maybe_flat_in_array(), \"should only be used for flat inline type arrays\");\n+  \/\/ Adjust offset to omit oop header\n+  int base_offset = arrayOopDesc::base_offset_in_bytes(T_FLAT_ELEMENT) - InlineKlass::cast(vk)->payload_offset();\n+  \/\/ Initialize all elements of the flat inline type array\n+  for (int i = 0; i < sv->field_size(); i++) {\n+    ScopeValue* val = sv->field_at(i);\n+    int offset = base_offset + (i << Klass::layout_helper_log2_element_size(vak->layout_helper()));\n+    reassign_fields_by_klass(vk, fr, reg_map, val->as_ObjectValue(), 0, (oop)obj, is_jvmci, offset, CHECK);\n+  }\n+}\n+\n@@ -1576,1 +1691,1 @@\n-void Deoptimization::reassign_fields(frame* fr, RegisterMap* reg_map, GrowableArray<ScopeValue*>* objects, bool realloc_failures, bool is_jvmci) {\n+void Deoptimization::reassign_fields(frame* fr, RegisterMap* reg_map, GrowableArray<ScopeValue*>* objects, bool realloc_failures, bool is_jvmci, TRAPS) {\n@@ -1581,0 +1696,2 @@\n+    k = get_refined_array_klass(k, fr, reg_map, sv, THREAD);\n+\n@@ -1582,1 +1699,1 @@\n-    assert(obj.not_null() || realloc_failures, \"reallocation was missed\");\n+    assert(obj.not_null() || realloc_failures || sv->has_properties(), \"reallocation was missed\");\n@@ -1620,1 +1737,4 @@\n-      reassign_fields_by_klass(ik, fr, reg_map, sv, 0, obj(), is_jvmci);\n+      reassign_fields_by_klass(ik, fr, reg_map, sv, 0, obj(), is_jvmci, 0, CHECK);\n+    } else if (k->is_flatArray_klass()) {\n+      FlatArrayKlass* vak = FlatArrayKlass::cast(k);\n+      reassign_flat_array_elements(fr, reg_map, sv, (flatArrayOop) obj(), vak, is_jvmci, CHECK);\n@@ -1624,1 +1744,1 @@\n-    } else if (k->is_objArray_klass()) {\n+    } else if (k->is_refArray_klass()) {\n@@ -1816,1 +1936,1 @@\n-  \/\/ Deoptimize only if the frame comes from compile code.\n+  \/\/ Deoptimize only if the frame comes from compiled code.\n","filename":"src\/hotspot\/share\/runtime\/deoptimization.cpp","additions":155,"deletions":35,"binary":false,"changes":190,"status":"modified"},{"patch":"@@ -816,1 +816,1 @@\n-  develop(bool, PrintFieldLayout, false,                                    \\\n+  product(bool, PrintFieldLayout, false, DIAGNOSTIC,                        \\\n@@ -819,0 +819,27 @@\n+  product(bool, PrintInlineLayout, false, DIAGNOSTIC,                       \\\n+          \"Print field layout for each inline type or class with inline fields\") \\\n+                                                                            \\\n+  product(bool, PrintFlatArrayLayout, false, DIAGNOSTIC,                    \\\n+          \"Print array layout for each inline type array\")                  \\\n+                                                                            \\\n+  product(bool, UseArrayFlattening, true,                                   \\\n+          \"Allow the VM to flatten arrays\")                                 \\\n+                                                                            \\\n+  product(bool, UseFieldFlattening, true,                                   \\\n+          \"Allow the VM to flatten value fields\")                           \\\n+                                                                            \\\n+  product(bool, UseNonAtomicValueFlattening, true,                          \\\n+          \"Allow the JVM to flatten some non-atomic null-free values\")      \\\n+                                                                            \\\n+  product(bool, UseNullableValueFlattening, true,                           \\\n+          \"Allow the JVM to flatten some nullable values\")                  \\\n+                                                                            \\\n+  product(bool, UseAtomicValueFlattening, true,                             \\\n+          \"Allow the JVM to flatten some atomic values\")                    \\\n+                                                                            \\\n+  product(intx, FlatArrayElementMaxOops, 4,                                 \\\n+          \"Max nof embedded object references in an inline type to flatten, <0 no limit\")  \\\n+                                                                            \\\n+  develop(ccstrlist, PrintInlineKlassFields, \"\",                            \\\n+          \"Print fields collected by InlineKlass::collect_fields\")          \\\n+                                                                            \\\n@@ -1781,0 +1808,3 @@\n+  product(bool, IgnoreAssertUnsetFields, false, DIAGNOSTIC,                           \\\n+          \"Ignore assert_unset_fields\")                                     \\\n+                                                                            \\\n@@ -1952,0 +1982,20 @@\n+  product(bool, EnableValhalla, true,                                       \\\n+          \"Enable experimental Valhalla features\")                          \\\n+                                                                            \\\n+  product_pd(bool, InlineTypePassFieldsAsArgs,                              \\\n+          \"Pass each inline type field as an argument at calls\")            \\\n+                                                                            \\\n+  product_pd(bool, InlineTypeReturnedAsFields,                              \\\n+          \"Return fields instead of an inline type reference\")              \\\n+                                                                            \\\n+  develop(bool, StressCallingConvention, false,                             \\\n+          \"Stress the scalarized calling convention.\")                      \\\n+                                                                            \\\n+  develop(bool, PreloadClasses, true,                                       \\\n+          \"Preloading all classes from the LoadableDescriptors attribute\")  \\\n+                                                                            \\\n+  product(ccstrlist, ForceNonTearable, \"\", DIAGNOSTIC,                      \\\n+          \"List of inline classes which are forced to be atomic \"           \\\n+          \"(whitespace and commas separate names, \"                         \\\n+          \"and leading and trailing stars '*' are wildcards)\")              \\\n+                                                                            \\\n","filename":"src\/hotspot\/share\/runtime\/globals.hpp","additions":51,"deletions":1,"binary":false,"changes":52,"status":"modified"},{"patch":"@@ -50,0 +50,1 @@\n+#include \"oops\/inlineKlass.hpp\"\n","filename":"src\/hotspot\/share\/runtime\/javaThread.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -116,0 +116,1 @@\n+  template(ClassPrintLayout)                      \\\n","filename":"src\/hotspot\/share\/runtime\/vmOperation.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -62,0 +62,2 @@\n+#include \"oops\/flatArrayKlass.hpp\"\n+#include \"oops\/inlineKlass.hpp\"\n@@ -940,1 +942,3 @@\n-           declare_type(ObjArrayKlass, ArrayKlass)                        \\\n+           declare_type(ObjArrayKlass, ArrayKlass)                        \\\n+             declare_type(FlatArrayKlass, ArrayKlass)                     \\\n+             declare_type(RefArrayKlass, ArrayKlass)                      \\\n@@ -943,0 +947,1 @@\n+        declare_type(InlineKlass, InstanceKlass)                          \\\n@@ -1407,1 +1412,1 @@\n-  declare_constant(Klass::_lh_array_tag_obj_value)                        \\\n+  declare_constant(Klass::_lh_array_tag_ref_value)                        \\\n","filename":"src\/hotspot\/share\/runtime\/vmStructs.cpp","additions":7,"deletions":2,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -1553,0 +1553,6 @@\n+            public boolean isNullRestrictedField(MethodHandle mh) {\n+                var memberName = mh.internalMemberName();\n+                assert memberName.isField();\n+                return memberName.isNullRestricted();\n+            }\n+\n@@ -1643,0 +1649,4 @@\n+            @Override\n+            public MethodHandle assertAsType(MethodHandle original, MethodType assertedType) {\n+                return original.viewAsType(assertedType, false);\n+            }\n","filename":"src\/java.base\/share\/classes\/java\/lang\/invoke\/MethodHandleImpl.java","additions":10,"deletions":0,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -35,0 +35,2 @@\n+import java.util.Objects;\n+\n@@ -40,0 +42,8 @@\n+ *\n+ * <div class=\"preview-block\">\n+ *      <div class=\"preview-comment\">\n+ *          The referent must have {@linkplain Objects#hasIdentity(Object) object identity}.\n+ *          When preview features are enabled, attempts to create a reference\n+ *          to a {@linkplain Class#isValue value object} result in an {@link IdentityException}.\n+ *      <\/div>\n+ * <\/div>\n@@ -554,0 +564,3 @@\n+        if (referent != null) {\n+            Objects.requireIdentity(referent);\n+        }\n","filename":"src\/java.base\/share\/classes\/java\/lang\/ref\/Reference.java","additions":13,"deletions":0,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -85,0 +85,12 @@\n+compiler\/jvmci\/jdk.vm.ci.hotspot.test\/src\/jdk\/vm\/ci\/hotspot\/test\/MemoryAccessProviderTest.java 8350208 generic-all\n+compiler\/jvmci\/jdk.vm.ci.hotspot.test\/src\/jdk\/vm\/ci\/hotspot\/test\/TestHotSpotResolvedJavaField.java 8350208 generic-all\n+\n+# Valhalla\n+compiler\/codegen\/TestRedundantLea.java#StringInflate  8367518 generic-all\n+compiler\/codegen\/TestRedundantLea.java#StoreNParallel 8367518 generic-all\n+compiler\/codegen\/TestRedundantLea.java#StoreNSerial   8367518 generic-all\n+compiler\/gcbarriers\/TestImplicitNullChecks.java 8367338 generic-all\n+compiler\/regalloc\/TestVerifyRegisterAllocator.java 8365895 windows-x64\n+compiler\/types\/TestArrayManyDimensions.java 8365895 windows-x64\n+compiler\/types\/correctness\/OffTest.java 8365895 windows-x64\n+\n@@ -104,0 +116,1 @@\n+runtime\/cds\/appcds\/redefineClass\/RedefineRunningMethods_Shared.java  8304168 generic-all\n@@ -123,0 +136,49 @@\n+\n+# Valhalla\n+runtime\/AccModule\/ConstModule.java 8294051 generic-all\n+runtime\/valhalla\/inlinetypes\/CircularityTest.java 8349037 generic-all\n+runtime\/valhalla\/inlinetypes\/classloading\/ConcurrentClassLoadingTest.java 8367412 linux-aarch64\n+runtime\/valhalla\/inlinetypes\/verifier\/StrictInstanceFieldsTest.java CODETOOLS-7904031 generic-all\n+runtime\/valhalla\/inlinetypes\/verifier\/StrictStaticFieldsTest.java CODETOOLS-7904031 generic-all\n+\n+runtime\/cds\/TestDefaultArchiveLoading.java#coops_nocoh            8366774           generic-all\n+runtime\/cds\/TestDefaultArchiveLoading.java#nocoops_nocoh          8366774           generic-all\n+runtime\/cds\/appcds\/jcmd\/JCmdTestDynamicDump.java                  8367398           windows-x64\n+\n+# Valhalla + COH\n+compiler\/c2\/autovectorization\/TestIndexOverflowIR.java                          8348568 generic-all\n+compiler\/c2\/irTests\/stringopts\/TestArrayCopySelect.java                         8348568 generic-all\n+compiler\/c2\/irTests\/TestVectorConditionalMove.java                              8348568 generic-all\n+compiler\/c2\/irTests\/TestVectorizationMismatchedAccess.java                      8348568 generic-all\n+compiler\/c2\/irTests\/TestVectorizationNotRun.java                                8348568 generic-all\n+compiler\/c2\/TestCastX2NotProcessedIGVN.java                                     8348568 generic-all\n+compiler\/loopopts\/superword\/TestAlignVector.java                                8348568 generic-all\n+compiler\/loopopts\/superword\/TestAlignVector.java#NoAlignVector-COH              8348568 generic-all\n+compiler\/loopopts\/superword\/TestAlignVector.java#VerifyAlignVector-COH          8348568 generic-all\n+compiler\/loopopts\/superword\/TestIndependentPacksWithCyclicDependency.java       8348568 generic-all\n+compiler\/loopopts\/superword\/TestMulAddS2I.java                                  8348568 generic-all\n+compiler\/loopopts\/superword\/TestScheduleReordersScalarMemops.java               8348568 generic-all\n+compiler\/loopopts\/superword\/TestSplitPacks.java                                 8348568 generic-all\n+compiler\/loopopts\/superword\/TestUnorderedReductionPartialVectorization.java     8348568 generic-all\n+compiler\/vectorization\/TestFloatConversionsVector.java                          8348568 generic-all\n+compiler\/vectorization\/runner\/ArrayTypeConvertTest.java                         8348568 generic-all\n+compiler\/vectorization\/runner\/LoopCombinedOpTest.java                           8348568 generic-all\n+compiler\/vectorization\/runner\/VectorizationTestRunner.java                      8348568 generic-all\n+runtime\/FieldLayout\/TestOopMapSizeMinimal.java#no_coops_ccptr_coh               8348568 generic-all\n+\n+gc\/stress\/gcbasher\/TestGCBasherWithParallel.java                                8348568 generic-all\n+\n+gtest\/CompressedKlassGtest.java#use-zero-based-encoding-coh                     8348568 generic-all\n+gtest\/CompressedKlassGtest.java#use-zero-based-encoding-coh-large-class-space   8348568 generic-all\n+gtest\/MetaspaceGtests.java#UseCompactObjectHeaders                              8348568 generic-all\n+\n+runtime\/CompressedOops\/CompressedClassPointersEncodingScheme.java               8348568 generic-all\n+runtime\/FieldLayout\/BaseOffsets.java#no-coops-with-coh                          8348568 generic-all\n+runtime\/FieldLayout\/BaseOffsets.java#with-coop--with-coh                        8348568 generic-all\n+runtime\/cds\/TestDefaultArchiveLoading.java#coops_coh                            8348568 generic-all\n+runtime\/cds\/TestDefaultArchiveLoading.java#nocoops_coh                          8348568 generic-all\n+runtime\/cds\/appcds\/TestZGCWithCDS.java                                          8348568 generic-all\n+\n+# Valhalla + AOT\n+runtime\/cds\/appcds\/methodHandles\/MethodHandlesGeneralTest.java#aot              8367408 generic-all\n+\n@@ -150,0 +212,79 @@\n+# Valhalla TODO:\n+serviceability\/jvmti\/valhalla\/HeapDump\/HeapDump.java 8317416 generic-all\n+\n+serviceability\/sa\/ClhsdbCDSCore.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbCDSJstackPrintAll.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbFindPC.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbInspect.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbLongConstant.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbJdis.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbJstack.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbPrintAll.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbPrintAs.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbPrintStatics.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbSource.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbSymbol.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbWhere.java 8190936 generic-all\n+serviceability\/sa\/JhsdbThreadInfoTest.java 8190936 generic-all\n+serviceability\/sa\/TestClassDump.java 8190936 generic-all\n+serviceability\/sa\/TestClhsdbJstackLock.java 8190936 generic-all\n+serviceability\/sa\/TestCpoolForInvokeDynamic.java 8190936 generic-all\n+serviceability\/sa\/TestHeapDumpForInvokeDynamic.java 8190936 generic-all\n+serviceability\/sa\/TestHeapDumpForLargeArray.java 8190936 generic-all\n+serviceability\/sa\/TestIntConstant.java 8190936 generic-all\n+serviceability\/sa\/TestJhsdbJstackLock.java 8190936 generic-all\n+serviceability\/sa\/TestJmapCore.java 8190936 generic-all\n+serviceability\/sa\/TestJmapCoreMetaspace.java 8190936 generic-all\n+serviceability\/sa\/TestPrintMdo.java 8190936 generic-all\n+serviceability\/sa\/jmap-hprof\/JMapHProfLargeHeapTest.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbDumpclass.java 8190936 generic-all\n+\n+compiler\/stringopts\/TestStackedConcatsAppendUncommonTrap.java 8367405 generic-all\n+\n+# Array Changes TODO\n+serviceability\/sa\/CDSJMapClstats.java 8365722 generic-all\n+serviceability\/sa\/ClhsdbClasses.java 8365722 generic-all\n+serviceability\/sa\/sadebugd\/DisableRegistryTest.java 8365722 generic-all\n+serviceability\/sa\/ClhsdbDumpheap.java 8365722 generic-all\n+serviceability\/sa\/sadebugd\/ClhsdbTestConnectArgument.java 8365722 generic-all\n+serviceability\/sa\/sadebugd\/DebugdConnectTest.java 8365722 generic-all\n+serviceability\/sa\/ClhsdbJhisto.java 8365722 generic-all\n+serviceability\/sa\/ClhsdbJstack.java#id1 8365722 generic-all\n+serviceability\/sa\/ClhsdbJstackWithConcurrentLock.java 8365722 generic-all\n+serviceability\/sa\/ClhsdbJstackXcompStress.java 8365722 generic-all\n+serviceability\/sa\/ClhsdbPstack.java#process 8365722 generic-all\n+serviceability\/sa\/ClhsdbPstack.java#core 8365722 generic-all\n+serviceability\/sa\/ClhsdbScanOops.java#id0 8365722 generic-all\n+serviceability\/sa\/ClhsdbScanOops.java#id1 8365722 generic-all\n+serviceability\/sa\/DeadlockDetectionTest.java 8365722 generic-all\n+serviceability\/sa\/ClhsdbJstack.java#id0 8365722 generic-all\n+serviceability\/sa\/TestInstanceKlassSize.java 8365722 generic-all\n+serviceability\/sa\/TestSysProps.java 8365722 generic-all\n+serviceability\/sa\/sadebugd\/ClhsdbAttachToDebugServer.java 8365722 generic-all\n+resourcehogs\/serviceability\/sa\/TestHeapDumpForLargeArray.java 8365722 generic-all\n+serviceability\/HeapDump\/DuplicateArrayClassesTest.java 8365722 generic-all\n+\n+resourcehogs\/serviceability\/sa\/ClhsdbRegionDetailsScanOopsForG1.java 8366806 generic-all\n+serviceability\/jvmti\/valhalla\/GetSetLocal\/ValueGetSetLocal.java 8366806 generic-all\n+vmTestbase\/nsk\/jdi\/Accessible\/isPackagePrivate\/accipp001\/TestDescription.java 8366806 generic-all\n+vmTestbase\/nsk\/jdi\/Accessible\/isPrivate\/isPrivate001\/TestDescription.java 8366806 generic-all\n+vmTestbase\/nsk\/jdi\/Accessible\/isProtected\/isProtected001\/TestDescription.java 8366806 generic-all\n+vmTestbase\/nsk\/jdi\/Accessible\/isPublic\/isPublic001\/TestDescription.java 8366806 generic-all\n+vmTestbase\/nsk\/jdi\/ClassObjectReference\/reflectedType\/reflectype001\/TestDescription.java 8366806 generic-all\n+vmTestbase\/nsk\/jdi\/ClassObjectReference\/toString\/tostring001\/TestDescription.java 8366806 generic-all\n+vmTestbase\/nsk\/jdi\/ReferenceType\/classObject\/classobj001\/TestDescription.java 8366806 generic-all\n+vmTestbase\/nsk\/jdi\/ReferenceType\/equals\/equals001\/TestDescription.java 8366806 generic-all\n+vmTestbase\/nsk\/jdi\/ReferenceType\/genericSignature\/genericSignature001\/TestDescription.java 8366806 generic-all\n+vmTestbase\/nsk\/jdi\/ReferenceType\/genericSignature\/genericSignature002\/TestDescription.java 8366806 generic-all\n+vmTestbase\/nsk\/jdi\/ReferenceType\/hashCode\/hashcode001\/TestDescription.java 8366806 generic-all\n+vmTestbase\/nsk\/jdi\/ReferenceType\/isFinal\/isfinal001\/TestDescription.java 8366806 generic-all\n+vmTestbase\/nsk\/jdi\/ReferenceType\/isStatic\/isstatic001\/TestDescription.java 8366806 generic-all\n+vmTestbase\/nsk\/jdi\/ReferenceType\/isStatic\/isstatic002\/TestDescription.java 8366806 generic-all\n+vmTestbase\/nsk\/jdi\/ReferenceType\/name\/name001\/TestDescription.java 8366806 generic-all\n+vmTestbase\/nsk\/jdi\/ReferenceType\/nestedTypes\/nestedtypes002\/TestDescription.java 8366806 generic-all\n+vmTestbase\/nsk\/jdi\/ReferenceType\/sourceName\/sourcename003\/TestDescription.java 8366806 generic-all\n+vmTestbase\/nsk\/jdi\/VirtualMachineManager\/connectedVirtualMachines\/convm001\/TestDescription.java 8366806 generic-all\n+vmTestbase\/nsk\/jdi\/VirtualMachineManager\/connectedVirtualMachines\/convm002\/TestDescription.java 8366806 generic-all\n+vmTestbase\/nsk\/jdi\/VirtualMachineManager\/connectedVirtualMachines\/convm003\/TestDescription.java 8366806 generic-all\n+vmTestbase\/nsk\/jvmti\/scenarios\/events\/EM04\/em04t001\/TestDescription.java 8366806 generic-all\n+\n@@ -188,0 +329,2 @@\n+vmTestbase\/vm\/mlvm\/hiddenloader\/stress\/byteMutation\/Test.java 8317172 generic-all\n+\n","filename":"test\/hotspot\/jtreg\/ProblemList.txt","additions":143,"deletions":0,"binary":false,"changes":143,"status":"modified"},{"patch":"@@ -31,0 +31,1 @@\n+import compiler.valhalla.inlinetypes.InlineTypeIRNode;\n@@ -90,1 +91,1 @@\n-    private static final String PREFIX = \"_#\";\n+    public static final String PREFIX = \"_#\";\n@@ -105,2 +106,2 @@\n-    private static final String STORE_OF_CLASS_POSTFIX = \"(:|\\\\+)\\\\S* \\\\*\" + END;\n-    private static final String LOAD_OF_CLASS_POSTFIX = \"(:|\\\\+)\\\\S* \\\\*\" + END;\n+    private static final String STORE_OF_CLASS_POSTFIX = \"( \\\\([^\\\\)]+\\\\))?(:|\\\\+)\\\\S* \\\\*\" + END;\n+    private static final String LOAD_OF_CLASS_POSTFIX = \"( \\\\([^\\\\)]+\\\\))?(:|\\\\+)\\\\S* \\\\*\" + END;\n@@ -155,0 +156,6 @@\n+    \/\/ Valhalla: Make sure that all Valhalla specific IR nodes are also properly initialized. Doing it here also\n+    \/\/           ensures that the Flag VM is able to pick up the correct compile phases.\n+    static {\n+        InlineTypeIRNode.forceStaticInitialization();\n+    }\n+\n@@ -386,2 +393,6 @@\n-        String regex = START + \"Allocate\\\\b\" + MID + \"allocationKlass:.*\\\\b\" + IS_REPLACED + \"\\\\s.*\" + END;\n-        macroNodes(ALLOC_OF, regex);\n+        allocateOfNodes(ALLOC_OF, IS_REPLACED);\n+    }\n+\n+    public static void allocateOfNodes(String irNodePlaceholder, String allocatee) {\n+        String regex = START + \"Allocate\\\\b\" + MID + \"allocationKlass:.*\\\\b\" + allocatee + \"\\\\s.*\" + END;\n+        macroNodes(irNodePlaceholder, regex);\n@@ -398,0 +409,4 @@\n+        allocateArrayOfNodes(ALLOC_ARRAY_OF, IS_REPLACED);\n+    }\n+\n+    public static void allocateArrayOfNodes(String irNodePlaceholder, String allocatee) {\n@@ -416,1 +431,1 @@\n-        String name_part = \"\\\\[+.(\" + partial_name_prefix + \")?\" + IS_REPLACED + \";\";\n+        String name_part = \"\\\\[+.(\" + partial_name_prefix + \")?\" + allocatee + \";\";\n@@ -418,1 +433,1 @@\n-        macroNodes(ALLOC_ARRAY_OF, regex);\n+        macroNodes(irNodePlaceholder, regex);\n@@ -483,1 +498,1 @@\n-        callOfNodes(CALL_OF, \"Call.*\");\n+        callOfNodes(CALL_OF, \"Call.*\", IS_REPLACED + \" \" );\n@@ -488,1 +503,6 @@\n-        callOfNodes(CALL_OF_METHOD, \"Call.*Java\");\n+        callOfNodes(CALL_OF_METHOD, \"Call.*Java\", IS_REPLACED + \" \");\n+    }\n+\n+    public static final String STATIC_CALL = PREFIX + \"STATIC_CALL\" + POSTFIX;\n+    static {\n+        beforeMatchingNameRegex(STATIC_CALL, \"CallStaticJava\");\n@@ -493,1 +513,19 @@\n-        callOfNodes(STATIC_CALL_OF_METHOD, \"CallStaticJava\");\n+        staticCallOfMethodNodes(STATIC_CALL_OF_METHOD, IS_REPLACED + \" \");\n+    }\n+\n+    public static void staticCallOfMethodNodes(String irNodePlaceholder, String calleeRegex) {\n+        callOfNodes(irNodePlaceholder, \"CallStaticJava\", calleeRegex);\n+    }\n+\n+    public static final String CALL_LEAF_NO_FP = PREFIX + \"CALL_LEAF_NO_FP\" + POSTFIX;\n+    static {\n+        beforeMatchingNameRegex(CALL_LEAF_NO_FP, \"CallLeafNoFP\");\n+    }\n+\n+    public static final String CALL_LEAF_NO_FP_OF_METHOD = COMPOSITE_PREFIX + \"CALL_LEAF_NO_FP_OF_METHOD\" + POSTFIX;\n+    static {\n+        callLeafNoFpOfMethodNodes(CALL_LEAF_NO_FP_OF_METHOD, IS_REPLACED);\n+    }\n+\n+    public static void callLeafNoFpOfMethodNodes(String irNodePlaceholder, String calleeRegex) {\n+        callOfNodes(irNodePlaceholder, \"CallLeafNoFP\", calleeRegex);\n@@ -590,0 +628,5 @@\n+    public static final String CMP_N = PREFIX + \"CMP_N\" + POSTFIX;\n+    static {\n+        beforeMatchingNameRegex(CMP_N, \"CmpN\");\n+    }\n+\n@@ -709,1 +752,1 @@\n-        callOfNodes(DYNAMIC_CALL_OF_METHOD, \"CallDynamicJava\");\n+        callOfNodes(DYNAMIC_CALL_OF_METHOD, \"CallDynamicJava\", IS_REPLACED);\n@@ -845,0 +888,5 @@\n+    public static final String INLINE_TYPE = PREFIX + \"INLINE_TYPE\" + POSTFIX;\n+    static {\n+        beforeMatchingNameRegex(INLINE_TYPE, \"InlineType\");\n+    }\n+\n@@ -891,1 +939,5 @@\n-        loadOfNodes(LOAD_OF_CLASS, \"Load(B|UB|S|US|I|L|F|D|P|N)\");\n+        anyLoadOfNodes(LOAD_OF_CLASS, IS_REPLACED);\n+    }\n+\n+    public static void anyLoadOfNodes(String irNodePlaceholder, String fieldHolder) {\n+        loadOfNodes(irNodePlaceholder, \"Load(B|UB|S|US|I|L|F|D|P|N)\", fieldHolder);\n@@ -901,1 +953,1 @@\n-        loadOfNodes(LOAD_B_OF_CLASS, \"LoadB\");\n+        loadOfNodes(LOAD_B_OF_CLASS, \"LoadB\", IS_REPLACED);\n@@ -911,1 +963,1 @@\n-        loadOfNodes(LOAD_D_OF_CLASS, \"LoadD\");\n+        loadOfNodes(LOAD_D_OF_CLASS, \"LoadD\", IS_REPLACED);\n@@ -921,1 +973,1 @@\n-        loadOfNodes(LOAD_F_OF_CLASS, \"LoadF\");\n+        loadOfNodes(LOAD_F_OF_CLASS, \"LoadF\", IS_REPLACED);\n@@ -931,1 +983,1 @@\n-        loadOfNodes(LOAD_I_OF_CLASS, \"LoadI\");\n+        loadOfNodes(LOAD_I_OF_CLASS, \"LoadI\", IS_REPLACED);\n@@ -956,1 +1008,1 @@\n-        loadOfNodes(LOAD_L_OF_CLASS, \"LoadL\");\n+        loadOfNodes(LOAD_L_OF_CLASS, \"LoadL\", IS_REPLACED);\n@@ -966,1 +1018,1 @@\n-        loadOfNodes(LOAD_N_OF_CLASS, \"LoadN\");\n+        loadOfNodes(LOAD_N_OF_CLASS, \"LoadN\", IS_REPLACED);\n@@ -982,1 +1034,1 @@\n-        loadOfNodes(LOAD_P_OF_CLASS, \"LoadP\");\n+        loadOfNodes(LOAD_P_OF_CLASS, \"LoadP\", IS_REPLACED);\n@@ -992,1 +1044,1 @@\n-        loadOfNodes(LOAD_S_OF_CLASS, \"LoadS\");\n+        loadOfNodes(LOAD_S_OF_CLASS, \"LoadS\", IS_REPLACED);\n@@ -1002,1 +1054,1 @@\n-        loadOfNodes(LOAD_UB_OF_CLASS, \"LoadUB\");\n+        loadOfNodes(LOAD_UB_OF_CLASS, \"LoadUB\", IS_REPLACED);\n@@ -1012,1 +1064,1 @@\n-        loadOfNodes(LOAD_US_OF_CLASS, \"LoadUS\");\n+        loadOfNodes(LOAD_US_OF_CLASS, \"LoadUS\", IS_REPLACED);\n@@ -1901,1 +1953,1 @@\n-        storeOfNodes(STORE_B_OF_CLASS, \"StoreB\");\n+        storeOfNodes(STORE_B_OF_CLASS, \"StoreB\", IS_REPLACED);\n@@ -1911,1 +1963,1 @@\n-        storeOfNodes(STORE_C_OF_CLASS, \"StoreC\");\n+        storeOfNodes(STORE_C_OF_CLASS, \"StoreC\", IS_REPLACED);\n@@ -1921,1 +1973,1 @@\n-        storeOfNodes(STORE_D_OF_CLASS, \"StoreD\");\n+        storeOfNodes(STORE_D_OF_CLASS, \"StoreD\", IS_REPLACED);\n@@ -1931,1 +1983,1 @@\n-        storeOfNodes(STORE_F_OF_CLASS, \"StoreF\");\n+        storeOfNodes(STORE_F_OF_CLASS, \"StoreF\", IS_REPLACED);\n@@ -1941,1 +1993,1 @@\n-        storeOfNodes(STORE_I_OF_CLASS, \"StoreI\");\n+        storeOfNodes(STORE_I_OF_CLASS, \"StoreI\", IS_REPLACED);\n@@ -1951,1 +2003,1 @@\n-        storeOfNodes(STORE_L_OF_CLASS, \"StoreL\");\n+        storeOfNodes(STORE_L_OF_CLASS, \"StoreL\", IS_REPLACED);\n@@ -1961,1 +2013,1 @@\n-        storeOfNodes(STORE_N_OF_CLASS, \"StoreN\");\n+        storeOfNodes(STORE_N_OF_CLASS, \"StoreN\", IS_REPLACED);\n@@ -1966,1 +2018,5 @@\n-        storeOfNodes(STORE_OF_CLASS, \"Store(B|C|S|I|L|F|D|P|N)\");\n+        anyStoreOfNodes(STORE_OF_CLASS, IS_REPLACED);\n+    }\n+\n+    public static void anyStoreOfNodes(String irNodePlaceholder, String fieldHolder) {\n+        storeOfNodes(irNodePlaceholder, \"Store(B|C|S|I|L|F|D|P|N)\", fieldHolder);\n@@ -1982,1 +2038,1 @@\n-        storeOfNodes(STORE_P_OF_CLASS, \"StoreP\");\n+        storeOfNodes(STORE_P_OF_CLASS, \"StoreP\", IS_REPLACED);\n@@ -2072,1 +2128,2 @@\n-        beforeMatchingNameRegex(SUBTYPE_CHECK, \"SubTypeCheck\");\n+        String regex = START + \"SubTypeCheck\" + MID + END;\n+        macroNodes(SUBTYPE_CHECK, regex);\n@@ -2854,1 +2911,1 @@\n-    private static void beforeMatching(String irNodePlaceholder, String regex) {\n+    public static void beforeMatching(String irNodePlaceholder, String regex) {\n@@ -2890,2 +2947,2 @@\n-    private static void callOfNodes(String irNodePlaceholder, String callRegex) {\n-        String regex = START + callRegex + MID + IS_REPLACED + \" \" +  END;\n+    private static void callOfNodes(String irNodePlaceholder, String callRegex, String calleeRegex) {\n+        String regex = START + callRegex + MID + calleeRegex + END;\n@@ -2899,1 +2956,1 @@\n-    private static void optoOnly(String irNodePlaceholder, String regex) {\n+    public static void optoOnly(String irNodePlaceholder, String regex) {\n@@ -2972,2 +3029,2 @@\n-    private static void loadOfNodes(String irNodePlaceholder, String irNodeRegex) {\n-        String regex = START + irNodeRegex + MID + \"@\\\\S*\" + IS_REPLACED + LOAD_OF_CLASS_POSTFIX;\n+    private static void loadOfNodes(String irNodePlaceholder, String irNodeRegex, String loadee) {\n+        String regex = START + irNodeRegex + MID + \"@(\\\\w+: ?)*[\\\\w\/]*\\\\b\" + loadee + LOAD_OF_CLASS_POSTFIX;\n@@ -2977,2 +3034,2 @@\n-    private static void storeOfNodes(String irNodePlaceholder, String irNodeRegex) {\n-        String regex = START + irNodeRegex + MID + \"@\\\\S*\" + IS_REPLACED + STORE_OF_CLASS_POSTFIX;\n+    private static void storeOfNodes(String irNodePlaceholder, String irNodeRegex, String storee) {\n+        String regex = START + irNodeRegex + MID + \"@(\\\\w+: ?)*[\\\\w\/]*\\\\b\" + storee + STORE_OF_CLASS_POSTFIX;\n","filename":"test\/hotspot\/jtreg\/compiler\/lib\/ir_framework\/IRNode.java","additions":97,"deletions":40,"binary":false,"changes":137,"status":"modified"},{"patch":"@@ -538,0 +538,5 @@\n+java\/lang\/ModuleLayer\/LayerControllerTest.java                  8337048 generic-all\n+java\/lang\/ModuleLayer\/BasicLayerTest.java                       8337048 generic-all\n+\n+java\/lang\/Thread\/virtual\/stress\/Skynet.java#default             8342977 generic-all\n+\n@@ -555,0 +560,1 @@\n+com\/sun\/management\/HotSpotDiagnosticMXBean\/DumpThreadsWithEliminatedLock.java 8360599 generic-all\n@@ -580,0 +586,2 @@\n+java\/net\/CookieHandler\/B6644726.java                            8365811 generic-all\n+\n@@ -735,0 +743,4 @@\n+com\/sun\/jdi\/cds\/CDSBreakpointTest.java                          8304168 generic-all\n+com\/sun\/jdi\/cds\/CDSDeleteAllBkptsTest.java                      8304168 generic-all\n+com\/sun\/jdi\/cds\/CDSFieldWatchpoints.java                        8304168 generic-all\n+\n@@ -836,0 +848,1 @@\n+\n@@ -843,0 +856,19 @@\n+############################################################################\n+\n+# valhalla\n+jdk\/classfile\/AccessFlagsTest.java 8366270 generic-all\n+\n+java\/lang\/invoke\/VarHandles\/VarHandleTestMethodHandleAccessValue.java 8367346 generic-all\n+\n+jdk\/jfr\/event\/runtime\/TestSyncOnValueBasedClassEvent.java 8328777 generic-all\n+\n+jdk\/jfr\/event\/runtime\/TestClassLoaderStatsEvent.java 8366820 generic-all\n+\n+com\/sun\/jdi\/valhalla\/FieldWatchpointsTest.java 8366806 generic-all\n+com\/sun\/jdi\/valhalla\/ValueArrayReferenceTest.java 8366806 generic-all\n+com\/sun\/jdi\/valhalla\/ValueClassTypeTest.java 8366806 generic-all\n+sun\/tools\/jhsdb\/BasicLauncherTest.java 8366806 generic-all\n+sun\/tools\/jhsdb\/HeapDumpTest.java 8366806 generic-all\n+sun\/tools\/jhsdb\/HeapDumpTestWithActiveProcess.java 8366806 generic-all\n+sun\/tools\/jhsdb\/JShellHeapDumpTest.java 8366806 generic-all\n+\n","filename":"test\/jdk\/ProblemList.txt","additions":32,"deletions":0,"binary":false,"changes":32,"status":"modified"}]}