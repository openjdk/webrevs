{"files":[{"patch":"@@ -158,0 +158,1 @@\n+JVM_IsIdentityClass\n@@ -160,0 +161,1 @@\n+JVM_IsValhallaEnabled\n","filename":"make\/data\/hotspot-symbols\/symbols-unix","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -751,0 +751,1 @@\n+BUILD_HOTSPOT_JTREG_LIBRARIES_CFLAGS_libobjmonusage007 := $(NSK_JVMTI_AGENT_INCLUDES)\n@@ -1460,0 +1461,1 @@\n+  BUILD_HOTSPOT_JTREG_LIBRARIES_LIBS_libobjmonusage007 += -lpthread\n","filename":"make\/test\/JtregNativeHotspot.gmk","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -36,0 +36,1 @@\n+#include \"ci\/ciInlineKlass.hpp\"\n@@ -42,0 +43,1 @@\n+#include \"oops\/oop.inline.hpp\"\n@@ -454,1 +456,1 @@\n-  __ remove_frame(initial_frame_size_in_bytes());\n+  __ remove_frame(initial_frame_size_in_bytes(), needs_stack_repair());\n@@ -498,0 +500,28 @@\n+  if (InlineTypeReturnedAsFields) {\n+    \/\/ Check if we are returning an non-null inline type and load its fields into registers\n+    ciType* return_type = compilation()->method()->return_type();\n+    if (return_type->is_inlinetype()) {\n+      ciInlineKlass* vk = return_type->as_inline_klass();\n+      if (vk->can_be_returned_as_fields()) {\n+        address unpack_handler = vk->unpack_handler();\n+        assert(unpack_handler != nullptr, \"must be\");\n+        __ far_call(RuntimeAddress(unpack_handler));\n+      }\n+    } else if (return_type->is_instance_klass() && (!return_type->is_loaded() || StressCallingConvention)) {\n+      Label skip;\n+      __ test_oop_is_not_inline_type(r0, rscratch2, skip);\n+\n+      \/\/ Load fields from a buffered value with an inline class specific handler\n+      __ load_klass(rscratch1 \/*dst*\/, r0 \/*src*\/);\n+      __ ldr(rscratch1, Address(rscratch1, InstanceKlass::adr_inlineklass_fixed_block_offset()));\n+      __ ldr(rscratch1, Address(rscratch1, InlineKlass::unpack_handler_offset()));\n+      \/\/ Unpack handler can be null if inline type is not scalarizable in returns\n+      __ cbz(rscratch1, skip);\n+      __ blr(rscratch1);\n+\n+      __ bind(skip);\n+    }\n+    \/\/ At this point, r0 points to the value object (for interpreter or C1 caller).\n+    \/\/ The fields of the object are copied into registers (for C2 caller).\n+  }\n+\n@@ -499,1 +529,1 @@\n-  __ remove_frame(initial_frame_size_in_bytes());\n+  __ remove_frame(initial_frame_size_in_bytes(), needs_stack_repair());\n@@ -511,0 +541,4 @@\n+int LIR_Assembler::store_inline_type_fields_to_buf(ciInlineKlass* vk) {\n+  return (__ store_inline_type_fields_to_buf(vk, false));\n+}\n+\n@@ -557,3 +591,1 @@\n-        if (patch_code == lir_patch_none) {\n-          jobject2reg(c->as_jobject(), dest->as_register());\n-        } else {\n+        if (patch_code != lir_patch_none) {\n@@ -561,0 +593,2 @@\n+        } else {\n+          jobject2reg(c->as_jobject(), dest->as_register());\n@@ -670,0 +704,2 @@\n+    \/\/ Non-null case is not handled on aarch64 but handled on x86\n+    \/\/ FIXME: do we need to add it here?\n@@ -1020,0 +1056,14 @@\n+void LIR_Assembler::move(LIR_Opr src, LIR_Opr dst) {\n+  assert(dst->is_cpu_register(), \"must be\");\n+  assert(dst->type() == src->type(), \"must be\");\n+\n+  if (src->is_cpu_register()) {\n+    reg2reg(src, dst);\n+  } else if (src->is_stack()) {\n+    stack2reg(src, dst, dst->type());\n+  } else if (src->is_constant()) {\n+    const2reg(src, dst, lir_patch_none, nullptr);\n+  } else {\n+    ShouldNotReachHere();\n+  }\n+}\n@@ -1211,1 +1261,1 @@\n-  if (UseSlowPath ||\n+  if (UseSlowPath || op->is_null_free() ||\n@@ -1316,22 +1366,16 @@\n-  if (should_profile) {\n-    Register mdo  = klass_RInfo;\n-    __ mov_metadata(mdo, md->constant_encoding());\n-    Label not_null;\n-    __ cbnz(obj, not_null);\n-    \/\/ Object is null; update MDO and exit\n-    Address data_addr\n-      = __ form_address(rscratch2, mdo,\n-                        md->byte_offset_of_slot(data, DataLayout::flags_offset()),\n-                        0);\n-    __ ldrb(rscratch1, data_addr);\n-    __ orr(rscratch1, rscratch1, BitData::null_seen_byte_constant());\n-    __ strb(rscratch1, data_addr);\n-    __ b(*obj_is_null);\n-    __ bind(not_null);\n-\n-    Label update_done;\n-    Register recv = k_RInfo;\n-    __ load_klass(recv, obj);\n-    type_profile_helper(mdo, md, data, recv, &update_done);\n-    Address counter_addr(mdo, md->byte_offset_of_slot(data, CounterData::count_offset()));\n-    __ addptr(counter_addr, DataLayout::counter_increment);\n+  if (op->need_null_check()) {\n+    if (should_profile) {\n+      Register mdo  = klass_RInfo;\n+      __ mov_metadata(mdo, md->constant_encoding());\n+      Label not_null;\n+      __ cbnz(obj, not_null);\n+      \/\/ Object is null; update MDO and exit\n+      Address data_addr\n+        = __ form_address(rscratch2, mdo,\n+                          md->byte_offset_of_slot(data, DataLayout::flags_offset()),\n+                          0);\n+      __ ldrb(rscratch1, data_addr);\n+      __ orr(rscratch1, rscratch1, BitData::null_seen_byte_constant());\n+      __ strb(rscratch1, data_addr);\n+      __ b(*obj_is_null);\n+      __ bind(not_null);\n@@ -1339,3 +1383,11 @@\n-    __ bind(update_done);\n-  } else {\n-    __ cbz(obj, *obj_is_null);\n+      Label update_done;\n+      Register recv = k_RInfo;\n+      __ load_klass(recv, obj);\n+      type_profile_helper(mdo, md, data, recv, &update_done);\n+      Address counter_addr(mdo, md->byte_offset_of_slot(data, CounterData::count_offset()));\n+      __ addptr(counter_addr, DataLayout::counter_increment);\n+\n+      __ bind(update_done);\n+    } else {\n+      __ cbz(obj, *obj_is_null);\n+    }\n@@ -1500,0 +1552,127 @@\n+void LIR_Assembler::emit_opFlattenedArrayCheck(LIR_OpFlattenedArrayCheck* op) {\n+  \/\/ We are loading\/storing from\/to an array that *may* be a flat array (the\n+  \/\/ declared type is Object[], abstract[], interface[] or VT.ref[]).\n+  \/\/ If this array is a flat array, take the slow path.\n+\n+  Register klass = op->tmp()->as_register();\n+  if (UseArrayMarkWordCheck) {\n+    __ test_flat_array_oop(op->array()->as_register(), op->tmp()->as_register(), *op->stub()->entry());\n+  } else {\n+    __ load_klass(klass, op->array()->as_register());\n+    __ ldrw(klass, Address(klass, Klass::layout_helper_offset()));\n+    __ tst(klass, Klass::_lh_array_tag_flat_value_bit_inplace);\n+    __ br(Assembler::NE, *op->stub()->entry());\n+  }\n+  if (!op->value()->is_illegal()) {\n+    \/\/ The array is not a flat array, but it might be null-free. If we are storing\n+    \/\/ a null into a null-free array, take the slow path (which will throw NPE).\n+    Label skip;\n+    __ cbnz(op->value()->as_register(), skip);\n+    if (UseArrayMarkWordCheck) {\n+      __ test_null_free_array_oop(op->array()->as_register(), op->tmp()->as_register(), *op->stub()->entry());\n+    } else {\n+      __ tst(klass, Klass::_lh_null_free_array_bit_inplace);\n+      __ br(Assembler::NE, *op->stub()->entry());\n+    }\n+    __ bind(skip);\n+  }\n+}\n+\n+void LIR_Assembler::emit_opNullFreeArrayCheck(LIR_OpNullFreeArrayCheck* op) {\n+  \/\/ We are storing into an array that *may* be null-free (the declared type is\n+  \/\/ Object[], abstract[], interface[] or VT.ref[]).\n+  if (UseArrayMarkWordCheck) {\n+    Label test_mark_word;\n+    Register tmp = op->tmp()->as_register();\n+    __ ldr(tmp, Address(op->array()->as_register(), oopDesc::mark_offset_in_bytes()));\n+    __ tst(tmp, markWord::unlocked_value);\n+    __ br(Assembler::NE, test_mark_word);\n+    __ load_prototype_header(tmp, op->array()->as_register());\n+    __ bind(test_mark_word);\n+    __ tst(tmp, markWord::null_free_array_bit_in_place);\n+  } else {\n+    Register klass = op->tmp()->as_register();\n+    __ load_klass(klass, op->array()->as_register());\n+    __ ldr(klass, Address(klass, Klass::layout_helper_offset()));\n+    __ tst(klass, Klass::_lh_null_free_array_bit_inplace);\n+  }\n+}\n+\n+void LIR_Assembler::emit_opSubstitutabilityCheck(LIR_OpSubstitutabilityCheck* op) {\n+  Label L_oops_equal;\n+  Label L_oops_not_equal;\n+  Label L_end;\n+\n+  Register left  = op->left()->as_register();\n+  Register right = op->right()->as_register();\n+\n+  __ cmp(left, right);\n+  __ br(Assembler::EQ, L_oops_equal);\n+\n+  \/\/ (1) Null check -- if one of the operands is null, the other must not be null (because\n+  \/\/     the two references are not equal), so they are not substitutable,\n+  \/\/     FIXME: do null check only if the operand is nullable\n+  {\n+    __ cbz(left, L_oops_not_equal);\n+    __ cbz(right, L_oops_not_equal);\n+  }\n+\n+  ciKlass* left_klass = op->left_klass();\n+  ciKlass* right_klass = op->right_klass();\n+\n+  \/\/ (2) Inline type check -- if either of the operands is not a inline type,\n+  \/\/     they are not substitutable. We do this only if we are not sure that the\n+  \/\/     operands are inline type\n+  if ((left_klass == nullptr || right_klass == nullptr) ||\/\/ The klass is still unloaded, or came from a Phi node.\n+      !left_klass->is_inlinetype() || !right_klass->is_inlinetype()) {\n+    Register tmp1  = op->tmp1()->as_register();\n+    __ mov(tmp1, markWord::inline_type_pattern);\n+    __ ldr(rscratch1, Address(left, oopDesc::mark_offset_in_bytes()));\n+    __ andr(tmp1, tmp1, rscratch1);\n+    __ ldr(rscratch1, Address(right, oopDesc::mark_offset_in_bytes()));\n+    __ andr(tmp1, tmp1, rscratch1);\n+    __ cmp(tmp1, (u1)markWord::inline_type_pattern);\n+    __ br(Assembler::NE, L_oops_not_equal);\n+  }\n+\n+  \/\/ (3) Same klass check: if the operands are of different klasses, they are not substitutable.\n+  if (left_klass != nullptr && left_klass->is_inlinetype() && left_klass == right_klass) {\n+    \/\/ No need to load klass -- the operands are statically known to be the same inline klass.\n+    __ b(*op->stub()->entry());\n+  } else {\n+    Register left_klass_op = op->left_klass_op()->as_register();\n+    Register right_klass_op = op->right_klass_op()->as_register();\n+\n+    if (UseCompressedClassPointers) {\n+      __ ldrw(left_klass_op,  Address(left,  oopDesc::klass_offset_in_bytes()));\n+      __ ldrw(right_klass_op, Address(right, oopDesc::klass_offset_in_bytes()));\n+      __ cmpw(left_klass_op, right_klass_op);\n+    } else {\n+      __ ldr(left_klass_op,  Address(left,  oopDesc::klass_offset_in_bytes()));\n+      __ ldr(right_klass_op, Address(right, oopDesc::klass_offset_in_bytes()));\n+      __ cmp(left_klass_op, right_klass_op);\n+    }\n+\n+    __ br(Assembler::EQ, *op->stub()->entry()); \/\/ same klass -> do slow check\n+    \/\/ fall through to L_oops_not_equal\n+  }\n+\n+  __ bind(L_oops_not_equal);\n+  move(op->not_equal_result(), op->result_opr());\n+  __ b(L_end);\n+\n+  __ bind(L_oops_equal);\n+  move(op->equal_result(), op->result_opr());\n+  __ b(L_end);\n+\n+  \/\/ We've returned from the stub. R0 contains 0x0 IFF the two\n+  \/\/ operands are not substitutable. (Don't compare against 0x1 in case the\n+  \/\/ C compiler is naughty)\n+  __ bind(*op->stub()->continuation());\n+  __ cbz(r0, L_oops_not_equal); \/\/ (call_stub() == 0x0) -> not_equal\n+  move(op->equal_result(), op->result_opr()); \/\/ (call_stub() != 0x0) -> equal\n+  \/\/ fall-through\n+  __ bind(L_end);\n+}\n+\n+\n@@ -2017,1 +2196,1 @@\n-  add_call_info(code_offset(), op->info());\n+  add_call_info(code_offset(), op->info(), op->maybe_return_as_fields());\n@@ -2028,1 +2207,1 @@\n-  add_call_info(code_offset(), op->info());\n+  add_call_info(code_offset(), op->info(), op->maybe_return_as_fields());\n@@ -2191,0 +2370,22 @@\n+void LIR_Assembler::arraycopy_inlinetype_check(Register obj, Register tmp, CodeStub* slow_path, bool is_dest, bool null_check) {\n+  if (null_check) {\n+    __ cbz(obj, *slow_path->entry());\n+  }\n+  if (UseArrayMarkWordCheck) {\n+    if (is_dest) {\n+      __ test_null_free_array_oop(obj, tmp, *slow_path->entry());\n+    } else {\n+      __ test_flat_array_oop(obj, tmp, *slow_path->entry());\n+    }\n+  } else {\n+    __ load_klass(tmp, obj);\n+    __ ldr(tmp, Address(tmp, Klass::layout_helper_offset()));\n+    if (is_dest) {\n+      \/\/ Take the slow path if it's a null_free destination array, in case the source array contains nulls.\n+      __ tst(tmp, Klass::_lh_null_free_array_bit_inplace);\n+    } else {\n+      __ tst(tmp, Klass::_lh_array_tag_flat_value_bit_inplace);\n+    }\n+    __ br(Assembler::NE, *slow_path->entry());\n+  }\n+}\n@@ -2209,0 +2410,6 @@\n+  if (flags & LIR_OpArrayCopy::always_slow_path) {\n+    __ b(*stub->entry());\n+    __ bind(*stub->continuation());\n+    return;\n+  }\n+\n@@ -2262,0 +2469,9 @@\n+  \/\/ Handle inline type arrays\n+  if (flags & LIR_OpArrayCopy::src_inlinetype_check) {\n+    arraycopy_inlinetype_check(src, tmp, stub, false, (flags & LIR_OpArrayCopy::src_null_check));\n+  }\n+\n+  if (flags & LIR_OpArrayCopy::dst_inlinetype_check) {\n+    arraycopy_inlinetype_check(dst, tmp, stub, true, (flags & LIR_OpArrayCopy::dst_null_check));\n+  }\n+\n@@ -2836,0 +3052,20 @@\n+void LIR_Assembler::emit_profile_inline_type(LIR_OpProfileInlineType* op) {\n+  Register obj = op->obj()->as_register();\n+  Register tmp = op->tmp()->as_pointer_register();\n+  bool not_null = op->not_null();\n+  int flag = op->flag();\n+\n+  Label not_inline_type;\n+  if (!not_null) {\n+    __ cbz(obj, not_inline_type);\n+  }\n+\n+  __ test_oop_is_not_inline_type(obj, tmp, not_inline_type);\n+\n+  Address mdo_addr = as_Address(op->mdp()->as_address_ptr(), rscratch2);\n+  __ ldrb(rscratch1, mdo_addr);\n+  __ orr(rscratch1, rscratch1, flag);\n+  __ strb(rscratch1, mdo_addr);\n+\n+  __ bind(not_inline_type);\n+}\n@@ -2976,0 +3212,4 @@\n+void LIR_Assembler::check_orig_pc() {\n+  __ ldr(rscratch2, frame_map()->address_for_orig_pc_addr());\n+  __ cmp(rscratch2, (u1)NULL_WORD);\n+}\n","filename":"src\/hotspot\/cpu\/aarch64\/c1_LIRAssembler_aarch64.cpp","additions":273,"deletions":33,"binary":false,"changes":306,"status":"modified"},{"patch":"@@ -36,0 +36,1 @@\n+#include \"oops\/constMethodFlags.hpp\"\n@@ -39,0 +40,1 @@\n+#include \"oops\/inlineKlass.hpp\"\n@@ -274,0 +276,63 @@\n+void InterpreterMacroAssembler::allocate_instance(Register klass, Register new_obj,\n+                                                  Register t1, Register t2,\n+                                                  bool clear_fields, Label& alloc_failed) {\n+  MacroAssembler::allocate_instance(klass, new_obj, t1, t2, clear_fields, alloc_failed);\n+  {\n+    SkipIfEqual skip_if(this, &DTraceAllocProbes, 0);\n+    \/\/ Trigger dtrace event for fastpath\n+    push(atos);\n+    call_VM_leaf(CAST_FROM_FN_PTR(address, static_cast<int (*)(oopDesc*)>(SharedRuntime::dtrace_object_alloc)), new_obj);\n+    pop(atos);\n+  }\n+}\n+\n+void InterpreterMacroAssembler::read_flat_field(Register holder_klass,\n+                                                Register field_index, Register field_offset,\n+                                                Register temp, Register obj) {\n+  Label alloc_failed, empty_value, done;\n+  const Register src = field_offset;\n+  const Register alloc_temp = rscratch1;\n+  const Register dst_temp   = temp;\n+  assert_different_registers(obj, holder_klass, field_index, field_offset, dst_temp);\n+\n+  \/\/ Grab the inline field klass\n+  push(holder_klass);\n+  const Register field_klass = holder_klass;\n+  get_inline_type_field_klass(holder_klass, field_index, field_klass);\n+\n+  \/\/check for empty value klass\n+  test_klass_is_empty_inline_type(field_klass, dst_temp, empty_value);\n+\n+  \/\/ allocate buffer\n+  push(obj); \/\/ save holder\n+  allocate_instance(field_klass, obj, alloc_temp, dst_temp, false, alloc_failed);\n+\n+  \/\/ Have an oop instance buffer, copy into it\n+  data_for_oop(obj, dst_temp, field_klass);\n+  pop(alloc_temp);             \/\/ restore holder\n+  lea(src, Address(alloc_temp, field_offset));\n+  \/\/ call_VM_leaf, clobbers a few regs, save restore new obj\n+  push(obj);\n+  access_value_copy(IS_DEST_UNINITIALIZED, src, dst_temp, field_klass);\n+  pop(obj);\n+  pop(holder_klass);\n+  b(done);\n+\n+  bind(empty_value);\n+  get_empty_inline_type_oop(field_klass, dst_temp, obj);\n+  pop(holder_klass);\n+  b(done);\n+\n+  bind(alloc_failed);\n+  pop(obj);\n+  pop(holder_klass);\n+  call_VM(obj, CAST_FROM_FN_PTR(address, InterpreterRuntime::read_flat_field),\n+          obj, field_index, holder_klass);\n+\n+  bind(done);\n+\n+  \/\/ Ensure the stores to copy the inline field contents are visible\n+  \/\/ before any subsequent store that publishes this reference.\n+  membar(Assembler::StoreStore);\n+}\n+\n@@ -320,1 +385,2 @@\n-                                                  Label& ok_is_subtype) {\n+                                                  Label& ok_is_subtype,\n+                                                  bool profile) {\n@@ -326,1 +392,3 @@\n-  profile_typecheck(r2, Rsub_klass, r5); \/\/ blows r2, reloads r5\n+  if (profile) {\n+    profile_typecheck(r2, Rsub_klass, r5); \/\/ blows r2, reloads r5\n+  }\n@@ -692,0 +760,1 @@\n+\n@@ -717,0 +786,31 @@\n+  if (state == atos && InlineTypeReturnedAsFields) {\n+    \/\/ Check if we are returning an non-null inline type and load its fields into registers\n+    Label skip;\n+    test_oop_is_not_inline_type(r0, rscratch2, skip);\n+\n+    \/\/ Load fields from a buffered value with an inline class specific handler\n+    load_klass(rscratch1 \/*dst*\/, r0 \/*src*\/);\n+    ldr(rscratch1, Address(rscratch1, InstanceKlass::adr_inlineklass_fixed_block_offset()));\n+    ldr(rscratch1, Address(rscratch1, InlineKlass::unpack_handler_offset()));\n+    \/\/ Unpack handler can be null if inline type is not scalarizable in returns\n+    cbz(rscratch1, skip);\n+\n+    blr(rscratch1);\n+#ifdef ASSERT\n+    \/\/ TODO 8284443 Enable\n+    if (StressCallingConvention && false) {\n+      Label skip_stress;\n+      ldr(rscratch1, Address(rfp, frame::interpreter_frame_method_offset * wordSize));\n+      ldrw(rscratch1, Address(rscratch1, Method::flags_offset()));\n+      tstw(rscratch1, ConstMethodFlags::has_scalarized_return_flag());\n+      br(Assembler::EQ, skip_stress);\n+      load_klass(r0, r0);\n+      orr(r0, r0, 1);\n+      bind(skip_stress);\n+    }\n+#endif\n+    bind(skip);\n+    \/\/ Check above kills sender esp in rscratch2. Reload it.\n+    ldr(rscratch2, Address(rfp, frame::interpreter_frame_sender_sp_offset * wordSize));\n+  }\n+\n@@ -776,0 +876,4 @@\n+      if (EnableValhalla) {\n+        \/\/ Mask inline_type bit such that we go to the slow path if object is an inline type\n+        andr(swap_reg, swap_reg, ~((int) markWord::inline_type_bit_in_place));\n+      }\n@@ -1150,1 +1254,1 @@\n-void InterpreterMacroAssembler::profile_not_taken_branch(Register mdp) {\n+void InterpreterMacroAssembler::profile_not_taken_branch(Register mdp, bool acmp) {\n@@ -1162,1 +1266,1 @@\n-    update_mdp_by_constant(mdp, in_bytes(BranchData::branch_data_size()));\n+    update_mdp_by_constant(mdp, acmp ? in_bytes(ACmpData::acmp_data_size()) : in_bytes(BranchData::branch_data_size()));\n@@ -1485,0 +1589,79 @@\n+void InterpreterMacroAssembler::profile_array(Register mdp,\n+                                              Register array,\n+                                              Register tmp) {\n+  if (ProfileInterpreter) {\n+    Label profile_continue;\n+\n+    \/\/ If no method data exists, go to profile_continue.\n+    test_method_data_pointer(mdp, profile_continue);\n+\n+    mov(tmp, array);\n+    profile_obj_type(tmp, Address(mdp, in_bytes(ArrayLoadStoreData::array_offset())));\n+\n+    Label not_flat;\n+    test_non_flat_array_oop(array, tmp, not_flat);\n+\n+    set_mdp_flag_at(mdp, ArrayLoadStoreData::flat_array_byte_constant());\n+\n+    bind(not_flat);\n+\n+    Label not_null_free;\n+    test_non_null_free_array_oop(array, tmp, not_null_free);\n+\n+    set_mdp_flag_at(mdp, ArrayLoadStoreData::null_free_array_byte_constant());\n+\n+    bind(not_null_free);\n+\n+    bind(profile_continue);\n+  }\n+}\n+\n+void InterpreterMacroAssembler::profile_element(Register mdp,\n+                                                Register element,\n+                                                Register tmp) {\n+  if (ProfileInterpreter) {\n+    Label profile_continue;\n+\n+    \/\/ If no method data exists, go to profile_continue.\n+    test_method_data_pointer(mdp, profile_continue);\n+\n+    mov(tmp, element);\n+    profile_obj_type(tmp, Address(mdp, in_bytes(ArrayLoadStoreData::element_offset())));\n+\n+    \/\/ The method data pointer needs to be updated.\n+    update_mdp_by_constant(mdp, in_bytes(ArrayLoadStoreData::array_load_store_data_size()));\n+\n+    bind(profile_continue);\n+  }\n+}\n+\n+void InterpreterMacroAssembler::profile_acmp(Register mdp,\n+                                             Register left,\n+                                             Register right,\n+                                             Register tmp) {\n+  if (ProfileInterpreter) {\n+    Label profile_continue;\n+\n+    \/\/ If no method data exists, go to profile_continue.\n+    test_method_data_pointer(mdp, profile_continue);\n+\n+    mov(tmp, left);\n+    profile_obj_type(tmp, Address(mdp, in_bytes(ACmpData::left_offset())));\n+\n+    Label left_not_inline_type;\n+    test_oop_is_not_inline_type(left, tmp, left_not_inline_type);\n+    set_mdp_flag_at(mdp, ACmpData::left_inline_type_byte_constant());\n+    bind(left_not_inline_type);\n+\n+    mov(tmp, right);\n+    profile_obj_type(tmp, Address(mdp, in_bytes(ACmpData::right_offset())));\n+\n+    Label right_not_inline_type;\n+    test_oop_is_not_inline_type(right, tmp, right_not_inline_type);\n+    set_mdp_flag_at(mdp, ACmpData::right_inline_type_byte_constant());\n+    bind(right_not_inline_type);\n+\n+    bind(profile_continue);\n+  }\n+}\n+\n@@ -1735,1 +1918,1 @@\n-        assert(ReturnTypeEntry::static_cell_count() < TypeStackSlotEntries::per_arg_count(), \"can't move past ret type\");\n+        assert(SingleTypeEntry::static_cell_count() < TypeStackSlotEntries::per_arg_count(), \"can't move past ret type\");\n@@ -1781,1 +1964,1 @@\n-    Address mdo_ret_addr(mdp, -in_bytes(ReturnTypeEntry::size()));\n+    Address mdo_ret_addr(mdp, -in_bytes(SingleTypeEntry::size()));\n","filename":"src\/hotspot\/cpu\/aarch64\/interp_masm_aarch64.cpp","additions":189,"deletions":6,"binary":false,"changes":195,"status":"modified"},{"patch":"@@ -149,0 +149,22 @@\n+  \/\/ Kills t1 and t2, perserves klass, return allocation in new_obj\n+  void allocate_instance(Register klass, Register new_obj,\n+                         Register t1, Register t2,\n+                         bool clear_fields, Label& alloc_failed);\n+\n+  \/\/ Allocate instance in \"obj\" and read in the content of the inline field\n+  \/\/ NOTES:\n+  \/\/   - input holder object via \"obj\", which must be r0,\n+  \/\/     will return new instance via the same reg\n+  \/\/   - assumes holder_klass and valueKlass field klass have both been resolved\n+  void read_flat_field(Register holder_klass,\n+                       Register field_index, Register field_offset,\n+                       Register temp,  Register obj = r0);\n+\n+  \/\/ Allocate value buffer in \"obj\" and read in flat element at the given index\n+  \/\/ NOTES:\n+  \/\/   - Return via \"obj\" must be r0\n+  \/\/   - kills all given regs\n+  void read_flat_element(Register array, Register index,\n+                         Register t1, Register t2,\n+                         Register obj = r0);\n+\n@@ -194,1 +216,1 @@\n-  void gen_subtype_check( Register sub_klass, Label &ok_is_subtype );\n+  void gen_subtype_check( Register sub_klass, Label &ok_is_subtype, bool profile = true);\n@@ -286,1 +308,1 @@\n-  void profile_not_taken_branch(Register mdp);\n+  void profile_not_taken_branch(Register mdp, bool acmp = false);\n@@ -299,0 +321,3 @@\n+  void profile_array(Register mdp, Register array, Register tmp);\n+  void profile_element(Register mdp, Register element, Register tmp);\n+  void profile_acmp(Register mdp, Register left, Register right, Register tmp);\n","filename":"src\/hotspot\/cpu\/aarch64\/interp_masm_aarch64.hpp","additions":27,"deletions":2,"binary":false,"changes":29,"status":"modified"},{"patch":"@@ -32,0 +32,1 @@\n+#include \"ci\/ciInlineKlass.hpp\"\n@@ -51,0 +52,1 @@\n+#include \"oops\/resolvedFieldEntry.hpp\"\n@@ -57,0 +59,1 @@\n+#include \"runtime\/signature_cc.hpp\"\n@@ -59,0 +62,1 @@\n+#include \"vmreg_aarch64.inline.hpp\"\n@@ -1124,0 +1128,35 @@\n+void MacroAssembler::get_default_value_oop(Register inline_klass, Register temp_reg, Register obj) {\n+#ifdef ASSERT\n+  {\n+    Label done_check;\n+    test_klass_is_inline_type(inline_klass, temp_reg, done_check);\n+    stop(\"get_default_value_oop from non inline type klass\");\n+    bind(done_check);\n+  }\n+#endif\n+  Register offset = temp_reg;\n+  \/\/ Getting the offset of the pre-allocated default value\n+  ldr(offset, Address(inline_klass, in_bytes(InstanceKlass::adr_inlineklass_fixed_block_offset())));\n+  ldr(offset, Address(offset, in_bytes(InlineKlass::default_value_offset_offset())));\n+\n+  \/\/ Getting the mirror\n+  ldr(obj, Address(inline_klass, in_bytes(Klass::java_mirror_offset())));\n+  resolve_oop_handle(obj, inline_klass, temp_reg);\n+\n+  \/\/ Getting the pre-allocated default value from the mirror\n+  Address field(obj, offset);\n+  load_heap_oop(obj, field, inline_klass, rscratch2);\n+}\n+\n+void MacroAssembler::get_empty_inline_type_oop(Register inline_klass, Register temp_reg, Register obj) {\n+#ifdef ASSERT\n+  {\n+    Label done_check;\n+    test_klass_is_empty_inline_type(inline_klass, temp_reg, done_check);\n+    stop(\"get_empty_value from non-empty inline klass\");\n+    bind(done_check);\n+  }\n+#endif\n+  get_default_value_oop(inline_klass, temp_reg, obj);\n+}\n+\n@@ -1577,1 +1616,5 @@\n-  if (!VerifyOops) return;\n+  if (!VerifyOops || VerifyAdapterSharing) {\n+    \/\/ Below address of the code string confuses VerifyAdapterSharing\n+    \/\/ because it may differ between otherwise equivalent adapters.\n+    return;\n+  }\n@@ -1610,1 +1653,5 @@\n-  if (!VerifyOops) return;\n+  if (!VerifyOops || VerifyAdapterSharing) {\n+    \/\/ Below address of the code string confuses VerifyAdapterSharing\n+    \/\/ because it may differ between otherwise equivalent adapters.\n+    return;\n+  }\n@@ -1708,0 +1755,4 @@\n+void MacroAssembler::super_call_VM_leaf(address entry_point) {\n+  MacroAssembler::call_VM_leaf_base(entry_point, 1);\n+}\n+\n@@ -1753,0 +1804,110 @@\n+void MacroAssembler::test_markword_is_inline_type(Register markword, Label& is_inline_type) {\n+  assert_different_registers(markword, rscratch2);\n+  andr(markword, markword, markWord::inline_type_mask_in_place);\n+  mov(rscratch2, markWord::inline_type_pattern);\n+  cmp(markword, rscratch2);\n+  br(Assembler::EQ, is_inline_type);\n+}\n+\n+void MacroAssembler::test_klass_is_inline_type(Register klass, Register temp_reg, Label& is_inline_type) {\n+  ldrw(temp_reg, Address(klass, Klass::access_flags_offset()));\n+  andr(temp_reg, temp_reg, JVM_ACC_VALUE);\n+  cbnz(temp_reg, is_inline_type);\n+}\n+\n+void MacroAssembler::test_oop_is_not_inline_type(Register object, Register tmp, Label& not_inline_type) {\n+  assert_different_registers(tmp, rscratch1);\n+  cbz(object, not_inline_type);\n+  const int is_inline_type_mask = markWord::inline_type_pattern;\n+  ldr(tmp, Address(object, oopDesc::mark_offset_in_bytes()));\n+  mov(rscratch1, is_inline_type_mask);\n+  andr(tmp, tmp, rscratch1);\n+  cmp(tmp, rscratch1);\n+  br(Assembler::NE, not_inline_type);\n+}\n+\n+void MacroAssembler::test_klass_is_empty_inline_type(Register klass, Register temp_reg, Label& is_empty_inline_type) {\n+#ifdef ASSERT\n+  {\n+    Label done_check;\n+    test_klass_is_inline_type(klass, temp_reg, done_check);\n+    stop(\"test_klass_is_empty_inline_type with non inline type klass\");\n+    bind(done_check);\n+  }\n+#endif\n+  ldrw(temp_reg, Address(klass, InstanceKlass::misc_flags_offset()));\n+  andr(temp_reg, temp_reg, InstanceKlassFlags::is_empty_inline_type_value());\n+  cbnz(temp_reg, is_empty_inline_type);\n+}\n+\n+void MacroAssembler::test_field_is_null_free_inline_type(Register flags, Register temp_reg, Label& is_null_free_inline_type) {\n+  assert(temp_reg == noreg, \"not needed\"); \/\/ keep signature uniform with x86\n+  tbnz(flags, ResolvedFieldEntry::is_null_free_inline_type_shift, is_null_free_inline_type);\n+}\n+\n+void MacroAssembler::test_field_is_not_null_free_inline_type(Register flags, Register temp_reg, Label& not_null_free_inline_type) {\n+  assert(temp_reg == noreg, \"not needed\"); \/\/ keep signature uniform with x86\n+  tbz(flags, ResolvedFieldEntry::is_null_free_inline_type_shift, not_null_free_inline_type);\n+}\n+\n+void MacroAssembler::test_field_is_flat(Register flags, Register temp_reg, Label& is_flat) {\n+  assert(temp_reg == noreg, \"not needed\"); \/\/ keep signature uniform with x86\n+  tbnz(flags, ResolvedFieldEntry::is_flat_shift, is_flat);\n+}\n+\n+void MacroAssembler::test_oop_prototype_bit(Register oop, Register temp_reg, int32_t test_bit, bool jmp_set, Label& jmp_label) {\n+  Label test_mark_word;\n+  \/\/ load mark word\n+  ldr(temp_reg, Address(oop, oopDesc::mark_offset_in_bytes()));\n+  \/\/ check displaced\n+  tst(temp_reg, markWord::unlocked_value);\n+  br(Assembler::NE, test_mark_word);\n+  \/\/ slow path use klass prototype\n+  load_prototype_header(temp_reg, oop);\n+\n+  bind(test_mark_word);\n+  andr(temp_reg, temp_reg, test_bit);\n+  if (jmp_set) {\n+    cbnz(temp_reg, jmp_label);\n+  } else {\n+    cbz(temp_reg, jmp_label);\n+  }\n+}\n+\n+void MacroAssembler::test_flat_array_oop(Register oop, Register temp_reg, Label& is_flat_array) {\n+  test_oop_prototype_bit(oop, temp_reg, markWord::flat_array_bit_in_place, true, is_flat_array);\n+}\n+\n+void MacroAssembler::test_non_flat_array_oop(Register oop, Register temp_reg,\n+                                                  Label&is_non_flat_array) {\n+  test_oop_prototype_bit(oop, temp_reg, markWord::flat_array_bit_in_place, false, is_non_flat_array);\n+}\n+\n+void MacroAssembler::test_null_free_array_oop(Register oop, Register temp_reg, Label& is_null_free_array) {\n+  test_oop_prototype_bit(oop, temp_reg, markWord::null_free_array_bit_in_place, true, is_null_free_array);\n+}\n+\n+void MacroAssembler::test_non_null_free_array_oop(Register oop, Register temp_reg, Label&is_non_null_free_array) {\n+  test_oop_prototype_bit(oop, temp_reg, markWord::null_free_array_bit_in_place, false, is_non_null_free_array);\n+}\n+\n+void MacroAssembler::test_flat_array_layout(Register lh, Label& is_flat_array) {\n+  tst(lh, Klass::_lh_array_tag_flat_value_bit_inplace);\n+  br(Assembler::NE, is_flat_array);\n+}\n+\n+void MacroAssembler::test_non_flat_array_layout(Register lh, Label& is_non_flat_array) {\n+  tst(lh, Klass::_lh_array_tag_flat_value_bit_inplace);\n+  br(Assembler::EQ, is_non_flat_array);\n+}\n+\n+void MacroAssembler::test_null_free_array_layout(Register lh, Label& is_null_free_array) {\n+  tst(lh, Klass::_lh_null_free_array_bit_inplace);\n+  br(Assembler::NE, is_null_free_array);\n+}\n+\n+void MacroAssembler::test_non_null_free_array_layout(Register lh, Label& is_non_null_free_array) {\n+  tst(lh, Klass::_lh_null_free_array_bit_inplace);\n+  br(Assembler::EQ, is_non_null_free_array);\n+}\n+\n@@ -4418,0 +4579,8 @@\n+void MacroAssembler::load_metadata(Register dst, Register src) {\n+  if (UseCompressedClassPointers) {\n+    ldrw(dst, Address(src, oopDesc::klass_offset_in_bytes()));\n+  } else {\n+    ldr(dst, Address(src, oopDesc::klass_offset_in_bytes()));\n+  }\n+}\n+\n@@ -4476,0 +4645,5 @@\n+void MacroAssembler::load_prototype_header(Register dst, Register src) {\n+  load_klass(dst, src);\n+  ldr(dst, Address(dst, Klass::prototype_header_offset()));\n+}\n+\n@@ -4800,0 +4974,40 @@\n+void MacroAssembler::access_value_copy(DecoratorSet decorators, Register src, Register dst,\n+                                       Register inline_klass) {\n+  BarrierSetAssembler* bs = BarrierSet::barrier_set()->barrier_set_assembler();\n+  bs->value_copy(this, decorators, src, dst, inline_klass);\n+}\n+\n+void MacroAssembler::first_field_offset(Register inline_klass, Register offset) {\n+  ldr(offset, Address(inline_klass, InstanceKlass::adr_inlineklass_fixed_block_offset()));\n+  ldrw(offset, Address(offset, InlineKlass::first_field_offset_offset()));\n+}\n+\n+void MacroAssembler::data_for_oop(Register oop, Register data, Register inline_klass) {\n+  \/\/ ((address) (void*) o) + vk->first_field_offset();\n+  Register offset = (data == oop) ? rscratch1 : data;\n+  first_field_offset(inline_klass, offset);\n+  if (data == oop) {\n+    add(data, data, offset);\n+  } else {\n+    lea(data, Address(oop, offset));\n+  }\n+}\n+\n+void MacroAssembler::data_for_value_array_index(Register array, Register array_klass,\n+                                                Register index, Register data) {\n+  assert_different_registers(array, array_klass, index);\n+  assert_different_registers(rscratch1, array, index);\n+\n+  \/\/ array->base() + (index << Klass::layout_helper_log2_element_size(lh));\n+  ldrw(rscratch1, Address(array_klass, Klass::layout_helper_offset()));\n+\n+  \/\/ Klass::layout_helper_log2_element_size(lh)\n+  \/\/ (lh >> _lh_log2_element_size_shift) & _lh_log2_element_size_mask;\n+  lsr(rscratch1, rscratch1, Klass::_lh_log2_element_size_shift);\n+  andr(rscratch1, rscratch1, Klass::_lh_log2_element_size_mask);\n+  lslv(index, index, rscratch1);\n+\n+  add(data, array, index);\n+  add(data, data, arrayOopDesc::base_offset_in_bytes(T_PRIMITIVE_OBJECT));\n+}\n+\n@@ -4876,0 +5090,96 @@\n+\/\/ Object \/ value buffer allocation...\n+void MacroAssembler::allocate_instance(Register klass, Register new_obj,\n+                                       Register t1, Register t2,\n+                                       bool clear_fields, Label& alloc_failed)\n+{\n+  Label done, initialize_header, initialize_object, slow_case, slow_case_no_pop;\n+  Register layout_size = t1;\n+  assert(new_obj == r0, \"needs to be r0\");\n+  assert_different_registers(klass, new_obj, t1, t2);\n+\n+  \/\/ get instance_size in InstanceKlass (scaled to a count of bytes)\n+  ldrw(layout_size, Address(klass, Klass::layout_helper_offset()));\n+  \/\/ test to see if it has a finalizer or is malformed in some way\n+  tst(layout_size, Klass::_lh_instance_slow_path_bit);\n+  br(Assembler::NE, slow_case_no_pop);\n+\n+  \/\/ Allocate the instance:\n+  \/\/  If TLAB is enabled:\n+  \/\/    Try to allocate in the TLAB.\n+  \/\/    If fails, go to the slow path.\n+  \/\/    Initialize the allocation.\n+  \/\/    Exit.\n+  \/\/\n+  \/\/  Go to slow path.\n+\n+  if (UseTLAB) {\n+    push(klass);\n+    tlab_allocate(new_obj, layout_size, 0, klass, t2, slow_case);\n+    if (ZeroTLAB || (!clear_fields)) {\n+      \/\/ the fields have been already cleared\n+      b(initialize_header);\n+    } else {\n+      \/\/ initialize both the header and fields\n+      b(initialize_object);\n+    }\n+\n+    if (clear_fields) {\n+      \/\/ The object is initialized before the header.  If the object size is\n+      \/\/ zero, go directly to the header initialization.\n+      bind(initialize_object);\n+      subs(layout_size, layout_size, sizeof(oopDesc));\n+      br(Assembler::EQ, initialize_header);\n+\n+      \/\/ Initialize topmost object field, divide size by 8, check if odd and\n+      \/\/ test if zero.\n+\n+  #ifdef ASSERT\n+      \/\/ make sure instance_size was multiple of 8\n+      Label L;\n+      tst(layout_size, 7);\n+      br(Assembler::EQ, L);\n+      stop(\"object size is not multiple of 8 - adjust this code\");\n+      bind(L);\n+      \/\/ must be > 0, no extra check needed here\n+  #endif\n+\n+      lsr(layout_size, layout_size, LogBytesPerLong);\n+\n+      \/\/ initialize remaining object fields: instance_size was a multiple of 8\n+      {\n+        Label loop;\n+        Register base = t2;\n+\n+        bind(loop);\n+        add(rscratch1, new_obj, layout_size, Assembler::LSL, LogBytesPerLong);\n+        str(zr, Address(rscratch1, sizeof(oopDesc) - 1*oopSize));\n+        subs(layout_size, layout_size, 1);\n+        br(Assembler::NE, loop);\n+      }\n+    } \/\/ clear_fields\n+\n+    \/\/ initialize object header only.\n+    bind(initialize_header);\n+    pop(klass);\n+    Register mark_word = t2;\n+    ldr(mark_word, Address(klass, Klass::prototype_header_offset()));\n+    str(mark_word, Address(new_obj, oopDesc::mark_offset_in_bytes ()));\n+    store_klass_gap(new_obj, zr);  \/\/ zero klass gap for compressed oops\n+    mov(t2, klass);         \/\/ preserve klass\n+    store_klass(new_obj, t2);  \/\/ src klass reg is potentially compressed\n+\n+    \/\/ TODO: Valhalla removed SharedRuntime::dtrace_object_alloc from here ?\n+\n+    b(done);\n+  }\n+\n+  if (UseTLAB) {\n+    bind(slow_case);\n+    pop(klass);\n+  }\n+  bind(slow_case_no_pop);\n+  b(alloc_failed);\n+\n+  bind(done);\n+}\n+\n@@ -4915,0 +5225,13 @@\n+void MacroAssembler::get_inline_type_field_klass(Register klass, Register index, Register inline_klass) {\n+  ldr(inline_klass, Address(klass, InstanceKlass::inline_type_field_klasses_offset()));\n+#ifdef ASSERT\n+  {\n+    Label done;\n+    cbnz(inline_klass, done);\n+    stop(\"get_inline_type_field_klass contains no inline klass\");\n+    bind(done);\n+  }\n+#endif\n+  ldr(inline_klass, Address(inline_klass, index, Address::lsl(3)));\n+}\n+\n@@ -5040,0 +5363,51 @@\n+void MacroAssembler::remove_frame(int initial_framesize, bool needs_stack_repair) {\n+  if (needs_stack_repair) {\n+    \/\/ Remove the extension of the caller's frame used for inline type unpacking\n+    \/\/\n+    \/\/ Right now the stack looks like this:\n+    \/\/\n+    \/\/ | Arguments from caller     |\n+    \/\/ |---------------------------|  <-- caller's SP\n+    \/\/ | Saved LR #1               |\n+    \/\/ | Saved FP #1               |\n+    \/\/ |---------------------------|\n+    \/\/ | Extension space for       |\n+    \/\/ |   inline arg (un)packing  |\n+    \/\/ |---------------------------|  <-- start of this method's frame\n+    \/\/ | Saved LR #2               |\n+    \/\/ | Saved FP #2               |\n+    \/\/ |---------------------------|  <-- FP\n+    \/\/ | sp_inc                    |\n+    \/\/ | method locals             |\n+    \/\/ |---------------------------|  <-- SP\n+    \/\/\n+    \/\/ There are two copies of FP and LR on the stack. They will be identical\n+    \/\/ unless the caller has been deoptimized, in which case LR #1 will be patched\n+    \/\/ to point at the deopt blob, and LR #2 will still point into the old method.\n+    \/\/\n+    \/\/ The sp_inc stack slot holds the total size of the frame including the\n+    \/\/ extension space minus two words for the saved FP and LR.\n+\n+    int sp_inc_offset = initial_framesize - 3 * wordSize;  \/\/ Immediately below saved LR and FP\n+\n+    ldr(rscratch1, Address(sp, sp_inc_offset));\n+    add(sp, sp, rscratch1);\n+    ldp(rfp, lr, Address(post(sp, 2 * wordSize)));\n+  } else {\n+    remove_frame(initial_framesize);\n+  }\n+}\n+\n+void MacroAssembler::save_stack_increment(int sp_inc, int frame_size) {\n+  int real_frame_size = frame_size + sp_inc;\n+  assert(sp_inc == 0 || sp_inc > 2*wordSize, \"invalid sp_inc value\");\n+  assert(real_frame_size >= 2*wordSize, \"frame size must include FP\/LR space\");\n+  assert((real_frame_size & (StackAlignmentInBytes-1)) == 0, \"frame size not aligned\");\n+\n+  int sp_inc_offset = frame_size - 3 * wordSize;  \/\/ Immediately below saved LR and FP\n+\n+  \/\/ Subtract two words for the saved FP and LR as these will be popped\n+  \/\/ separately. See remove_frame above.\n+  mov(rscratch1, real_frame_size - 2*wordSize);\n+  str(rscratch1, Address(sp, sp_inc_offset));\n+}\n@@ -5954,0 +6328,441 @@\n+#ifdef COMPILER2\n+\/\/ C2 compiled method's prolog code\n+\/\/ Moved here from aarch64.ad to support Valhalla code belows\n+void MacroAssembler::verified_entry(Compile* C, int sp_inc) {\n+  if (C->clinit_barrier_on_entry()) {\n+    assert(!C->method()->holder()->is_not_initialized(), \"initialization should have been started\");\n+\n+    Label L_skip_barrier;\n+\n+    mov_metadata(rscratch2, C->method()->holder()->constant_encoding());\n+    clinit_barrier(rscratch2, rscratch1, &L_skip_barrier);\n+    far_jump(RuntimeAddress(SharedRuntime::get_handle_wrong_method_stub()));\n+    bind(L_skip_barrier);\n+  }\n+\n+  if (C->max_vector_size() > 0) {\n+    reinitialize_ptrue();\n+  }\n+\n+  int bangsize = C->output()->bang_size_in_bytes();\n+  if (C->output()->need_stack_bang(bangsize))\n+    generate_stack_overflow_check(bangsize);\n+\n+  \/\/ n.b. frame size includes space for return pc and rfp\n+  const long framesize = C->output()->frame_size_in_bytes();\n+  build_frame(framesize);\n+\n+  if (C->needs_stack_repair()) {\n+    save_stack_increment(sp_inc, framesize);\n+  }\n+\n+  if (VerifyStackAtCalls) {\n+    Unimplemented();\n+  }\n+}\n+#endif \/\/ COMPILER2\n+\n+int MacroAssembler::store_inline_type_fields_to_buf(ciInlineKlass* vk, bool from_interpreter) {\n+  assert(InlineTypeReturnedAsFields, \"Inline types should never be returned as fields\");\n+  \/\/ An inline type might be returned. If fields are in registers we\n+  \/\/ need to allocate an inline type instance and initialize it with\n+  \/\/ the value of the fields.\n+  Label skip;\n+  \/\/ We only need a new buffered inline type if a new one is not returned\n+  tbz(r0, 0, skip);\n+  int call_offset = -1;\n+\n+  \/\/ Be careful not to clobber r1-7 which hold returned fields\n+  \/\/ Also do not use callee-saved registers as these may be live in the interpreter\n+  Register tmp1 = r13, tmp2 = r14, klass = r15, r0_preserved = r12;\n+\n+  \/\/ The following code is similar to allocate_instance but has some slight differences,\n+  \/\/ e.g. object size is always not zero, sometimes it's constant; storing klass ptr after\n+  \/\/ allocating is not necessary if vk != nullptr, etc. allocate_instance is not aware of these.\n+  Label slow_case;\n+  \/\/ 1. Try to allocate a new buffered inline instance either from TLAB or eden space\n+  mov(r0_preserved, r0); \/\/ save r0 for slow_case since *_allocate may corrupt it when allocation failed\n+\n+  if (vk != nullptr) {\n+    \/\/ Called from C1, where the return type is statically known.\n+    movptr(klass, (intptr_t)vk->get_InlineKlass());\n+    jint obj_size = vk->layout_helper();\n+    assert(obj_size != Klass::_lh_neutral_value, \"inline class in return type must have been resolved\");\n+    if (UseTLAB) {\n+      tlab_allocate(r0, noreg, obj_size, tmp1, tmp2, slow_case);\n+    } else {\n+      b(slow_case);\n+    }\n+  } else {\n+    \/\/ Call from interpreter. R0 contains ((the InlineKlass* of the return type) | 0x01)\n+    andr(klass, r0, -2);\n+    ldrw(tmp2, Address(klass, Klass::layout_helper_offset()));\n+    if (UseTLAB) {\n+      tlab_allocate(r0, tmp2, 0, tmp1, tmp2, slow_case);\n+    } else {\n+      b(slow_case);\n+    }\n+  }\n+  if (UseTLAB) {\n+    \/\/ 2. Initialize buffered inline instance header\n+    Register buffer_obj = r0;\n+    mov(rscratch1, (intptr_t)markWord::inline_type_prototype().value());\n+    str(rscratch1, Address(buffer_obj, oopDesc::mark_offset_in_bytes()));\n+    store_klass_gap(buffer_obj, zr);\n+    if (vk == nullptr) {\n+      \/\/ store_klass corrupts klass, so save it for later use (interpreter case only).\n+      mov(tmp1, klass);\n+    }\n+    store_klass(buffer_obj, klass);\n+    \/\/ 3. Initialize its fields with an inline class specific handler\n+    if (vk != nullptr) {\n+      far_call(RuntimeAddress(vk->pack_handler())); \/\/ no need for call info as this will not safepoint.\n+    } else {\n+      \/\/ tmp1 holds klass preserved above\n+      ldr(tmp1, Address(tmp1, InstanceKlass::adr_inlineklass_fixed_block_offset()));\n+      ldr(tmp1, Address(tmp1, InlineKlass::pack_handler_offset()));\n+      blr(tmp1);\n+    }\n+\n+    membar(Assembler::StoreStore);\n+    b(skip);\n+  } else {\n+    \/\/ Must have already branched to slow_case above.\n+    DEBUG_ONLY(should_not_reach_here());\n+  }\n+  bind(slow_case);\n+  \/\/ We failed to allocate a new inline type, fall back to a runtime\n+  \/\/ call. Some oop field may be live in some registers but we can't\n+  \/\/ tell. That runtime call will take care of preserving them\n+  \/\/ across a GC if there's one.\n+  mov(r0, r0_preserved);\n+\n+  if (from_interpreter) {\n+    super_call_VM_leaf(StubRoutines::store_inline_type_fields_to_buf());\n+  } else {\n+    far_call(RuntimeAddress(StubRoutines::store_inline_type_fields_to_buf()));\n+    call_offset = offset();\n+  }\n+  membar(Assembler::StoreStore);\n+\n+  bind(skip);\n+  return call_offset;\n+}\n+\n+\/\/ Move a value between registers\/stack slots and update the reg_state\n+bool MacroAssembler::move_helper(VMReg from, VMReg to, BasicType bt, RegState reg_state[]) {\n+  assert(from->is_valid() && to->is_valid(), \"source and destination must be valid\");\n+  if (reg_state[to->value()] == reg_written) {\n+    return true; \/\/ Already written\n+  }\n+\n+  if (from != to && bt != T_VOID) {\n+    if (reg_state[to->value()] == reg_readonly) {\n+      return false; \/\/ Not yet writable\n+    }\n+    if (from->is_reg()) {\n+      if (to->is_reg()) {\n+        if (from->is_Register() && to->is_Register()) {\n+          mov(to->as_Register(), from->as_Register());\n+        } else if (from->is_FloatRegister() && to->is_FloatRegister()) {\n+          fmovd(to->as_FloatRegister(), from->as_FloatRegister());\n+        } else {\n+          ShouldNotReachHere();\n+        }\n+      } else {\n+        int st_off = to->reg2stack() * VMRegImpl::stack_slot_size;\n+        Address to_addr = Address(sp, st_off);\n+        if (from->is_FloatRegister()) {\n+          if (bt == T_DOUBLE) {\n+             strd(from->as_FloatRegister(), to_addr);\n+          } else {\n+             assert(bt == T_FLOAT, \"must be float\");\n+             strs(from->as_FloatRegister(), to_addr);\n+          }\n+        } else {\n+          str(from->as_Register(), to_addr);\n+        }\n+      }\n+    } else {\n+      Address from_addr = Address(sp, from->reg2stack() * VMRegImpl::stack_slot_size);\n+      if (to->is_reg()) {\n+        if (to->is_FloatRegister()) {\n+          if (bt == T_DOUBLE) {\n+            ldrd(to->as_FloatRegister(), from_addr);\n+          } else {\n+            assert(bt == T_FLOAT, \"must be float\");\n+            ldrs(to->as_FloatRegister(), from_addr);\n+          }\n+        } else {\n+          ldr(to->as_Register(), from_addr);\n+        }\n+      } else {\n+        int st_off = to->reg2stack() * VMRegImpl::stack_slot_size;\n+        ldr(rscratch1, from_addr);\n+        str(rscratch1, Address(sp, st_off));\n+      }\n+    }\n+  }\n+\n+  \/\/ Update register states\n+  reg_state[from->value()] = reg_writable;\n+  reg_state[to->value()] = reg_written;\n+  return true;\n+}\n+\n+\/\/ Calculate the extra stack space required for packing or unpacking inline\n+\/\/ args and adjust the stack pointer\n+int MacroAssembler::extend_stack_for_inline_args(int args_on_stack) {\n+  int sp_inc = args_on_stack * VMRegImpl::stack_slot_size;\n+  sp_inc = align_up(sp_inc, StackAlignmentInBytes);\n+  assert(sp_inc > 0, \"sanity\");\n+\n+  \/\/ Save a copy of the FP and LR here for deoptimization patching and frame walking\n+  stp(rfp, lr, Address(pre(sp, -2 * wordSize)));\n+\n+  \/\/ Adjust the stack pointer. This will be repaired on return by MacroAssembler::remove_frame\n+  if (sp_inc < (1 << 9)) {\n+    sub(sp, sp, sp_inc);   \/\/ Fits in an immediate\n+  } else {\n+    mov(rscratch1, sp_inc);\n+    sub(sp, sp, rscratch1);\n+  }\n+\n+  return sp_inc + 2 * wordSize;  \/\/ Account for the FP\/LR space\n+}\n+\n+\/\/ Read all fields from an inline type oop and store the values in registers\/stack slots\n+bool MacroAssembler::unpack_inline_helper(const GrowableArray<SigEntry>* sig, int& sig_index,\n+                                          VMReg from, int& from_index, VMRegPair* to, int to_count, int& to_index,\n+                                          RegState reg_state[]) {\n+  assert(sig->at(sig_index)._bt == T_VOID, \"should be at end delimiter\");\n+  assert(from->is_valid(), \"source must be valid\");\n+  bool progress = false;\n+#ifdef ASSERT\n+  const int start_offset = offset();\n+#endif\n+\n+  Label L_null, L_notNull;\n+  \/\/ Don't use r14 as tmp because it's used for spilling (see MacroAssembler::spill_reg_for)\n+  Register tmp1 = r10;\n+  Register tmp2 = r11;\n+  Register fromReg = noreg;\n+  ScalarizedInlineArgsStream stream(sig, sig_index, to, to_count, to_index, -1);\n+  bool done = true;\n+  bool mark_done = true;\n+  VMReg toReg;\n+  BasicType bt;\n+  \/\/ Check if argument requires a null check\n+  bool null_check = false;\n+  VMReg nullCheckReg;\n+  while (stream.next(nullCheckReg, bt)) {\n+    if (sig->at(stream.sig_index())._offset == -1) {\n+      null_check = true;\n+      break;\n+    }\n+  }\n+  stream.reset(sig_index, to_index);\n+  while (stream.next(toReg, bt)) {\n+    assert(toReg->is_valid(), \"destination must be valid\");\n+    int idx = (int)toReg->value();\n+    if (reg_state[idx] == reg_readonly) {\n+      if (idx != from->value()) {\n+        mark_done = false;\n+      }\n+      done = false;\n+      continue;\n+    } else if (reg_state[idx] == reg_written) {\n+      continue;\n+    }\n+    assert(reg_state[idx] == reg_writable, \"must be writable\");\n+    reg_state[idx] = reg_written;\n+    progress = true;\n+\n+    if (fromReg == noreg) {\n+      if (from->is_reg()) {\n+        fromReg = from->as_Register();\n+      } else {\n+        int st_off = from->reg2stack() * VMRegImpl::stack_slot_size;\n+        ldr(tmp1, Address(sp, st_off));\n+        fromReg = tmp1;\n+      }\n+      if (null_check) {\n+        \/\/ Nullable inline type argument, emit null check\n+        cbz(fromReg, L_null);\n+      }\n+    }\n+    int off = sig->at(stream.sig_index())._offset;\n+    if (off == -1) {\n+      assert(null_check, \"Missing null check at\");\n+      if (toReg->is_stack()) {\n+        int st_off = toReg->reg2stack() * VMRegImpl::stack_slot_size;\n+        mov(tmp2, 1);\n+        str(tmp2, Address(sp, st_off));\n+      } else {\n+        mov(toReg->as_Register(), 1);\n+      }\n+      continue;\n+    }\n+    assert(off > 0, \"offset in object should be positive\");\n+    Address fromAddr = Address(fromReg, off);\n+    if (!toReg->is_FloatRegister()) {\n+      Register dst = toReg->is_stack() ? tmp2 : toReg->as_Register();\n+      if (is_reference_type(bt)) {\n+        load_heap_oop(dst, fromAddr, rscratch1, rscratch2);\n+      } else {\n+        bool is_signed = (bt != T_CHAR) && (bt != T_BOOLEAN);\n+        load_sized_value(dst, fromAddr, type2aelembytes(bt), is_signed);\n+      }\n+      if (toReg->is_stack()) {\n+        int st_off = toReg->reg2stack() * VMRegImpl::stack_slot_size;\n+        str(dst, Address(sp, st_off));\n+      }\n+    } else if (bt == T_DOUBLE) {\n+      ldrd(toReg->as_FloatRegister(), fromAddr);\n+    } else {\n+      assert(bt == T_FLOAT, \"must be float\");\n+      ldrs(toReg->as_FloatRegister(), fromAddr);\n+    }\n+  }\n+  if (progress && null_check) {\n+    if (done) {\n+      b(L_notNull);\n+      bind(L_null);\n+      \/\/ Set IsInit field to zero to signal that the argument is null.\n+      \/\/ Also set all oop fields to zero to make the GC happy.\n+      stream.reset(sig_index, to_index);\n+      while (stream.next(toReg, bt)) {\n+        if (sig->at(stream.sig_index())._offset == -1 ||\n+            bt == T_OBJECT || bt == T_ARRAY) {\n+          if (toReg->is_stack()) {\n+            int st_off = toReg->reg2stack() * VMRegImpl::stack_slot_size;\n+            str(zr, Address(sp, st_off));\n+          } else {\n+            mov(toReg->as_Register(), zr);\n+          }\n+        }\n+      }\n+      bind(L_notNull);\n+    } else {\n+      bind(L_null);\n+    }\n+  }\n+\n+  sig_index = stream.sig_index();\n+  to_index = stream.regs_index();\n+\n+  if (mark_done && reg_state[from->value()] != reg_written) {\n+    \/\/ This is okay because no one else will write to that slot\n+    reg_state[from->value()] = reg_writable;\n+  }\n+  from_index--;\n+  assert(progress || (start_offset == offset()), \"should not emit code\");\n+  return done;\n+}\n+\n+\/\/ Pack fields back into an inline type oop\n+bool MacroAssembler::pack_inline_helper(const GrowableArray<SigEntry>* sig, int& sig_index, int vtarg_index,\n+                                        VMRegPair* from, int from_count, int& from_index, VMReg to,\n+                                        RegState reg_state[], Register val_array) {\n+  assert(sig->at(sig_index)._bt == T_METADATA, \"should be at delimiter\");\n+  assert(to->is_valid(), \"destination must be valid\");\n+\n+  if (reg_state[to->value()] == reg_written) {\n+    skip_unpacked_fields(sig, sig_index, from, from_count, from_index);\n+    return true; \/\/ Already written\n+  }\n+\n+  \/\/ The GC barrier expanded by store_heap_oop below may call into the\n+  \/\/ runtime so use callee-saved registers for any values that need to be\n+  \/\/ preserved. The GC barrier assembler should take care of saving the\n+  \/\/ Java argument registers.\n+  \/\/ TODO 8284443 Isn't it an issue if below code uses r14 as tmp when it contains a spilled value?\n+  \/\/ Be careful with r14 because it's used for spilling (see MacroAssembler::spill_reg_for).\n+  Register val_obj_tmp = r21;\n+  Register from_reg_tmp = r22;\n+  Register tmp1 = r14;\n+  Register tmp2 = r13;\n+  Register tmp3 = r12;\n+  Register val_obj = to->is_stack() ? val_obj_tmp : to->as_Register();\n+\n+  assert_different_registers(val_obj_tmp, from_reg_tmp, tmp1, tmp2, tmp3, val_array);\n+\n+  if (reg_state[to->value()] == reg_readonly) {\n+    if (!is_reg_in_unpacked_fields(sig, sig_index, to, from, from_count, from_index)) {\n+      skip_unpacked_fields(sig, sig_index, from, from_count, from_index);\n+      return false; \/\/ Not yet writable\n+    }\n+    val_obj = val_obj_tmp;\n+  }\n+\n+  int index = arrayOopDesc::base_offset_in_bytes(T_OBJECT) + vtarg_index * type2aelembytes(T_OBJECT);\n+  load_heap_oop(val_obj, Address(val_array, index), tmp1, tmp2);\n+\n+  ScalarizedInlineArgsStream stream(sig, sig_index, from, from_count, from_index);\n+  VMReg fromReg;\n+  BasicType bt;\n+  Label L_null;\n+  while (stream.next(fromReg, bt)) {\n+    assert(fromReg->is_valid(), \"source must be valid\");\n+    reg_state[fromReg->value()] = reg_writable;\n+\n+    int off = sig->at(stream.sig_index())._offset;\n+    if (off == -1) {\n+      \/\/ Nullable inline type argument, emit null check\n+      Label L_notNull;\n+      if (fromReg->is_stack()) {\n+        int ld_off = fromReg->reg2stack() * VMRegImpl::stack_slot_size;\n+        ldrb(tmp2, Address(sp, ld_off));\n+        cbnz(tmp2, L_notNull);\n+      } else {\n+        cbnz(fromReg->as_Register(), L_notNull);\n+      }\n+      mov(val_obj, 0);\n+      b(L_null);\n+      bind(L_notNull);\n+      continue;\n+    }\n+\n+    assert(off > 0, \"offset in object should be positive\");\n+    size_t size_in_bytes = is_java_primitive(bt) ? type2aelembytes(bt) : wordSize;\n+\n+    \/\/ Pack the scalarized field into the value object.\n+    Address dst(val_obj, off);\n+\n+    if (!fromReg->is_FloatRegister()) {\n+      Register src;\n+      if (fromReg->is_stack()) {\n+        src = from_reg_tmp;\n+        int ld_off = fromReg->reg2stack() * VMRegImpl::stack_slot_size;\n+        load_sized_value(src, Address(sp, ld_off), size_in_bytes, \/* is_signed *\/ false);\n+      } else {\n+        src = fromReg->as_Register();\n+      }\n+      assert_different_registers(dst.base(), src, tmp1, tmp2, tmp3, val_array);\n+      if (is_reference_type(bt)) {\n+        store_heap_oop(dst, src, tmp1, tmp2, tmp3, IN_HEAP | ACCESS_WRITE | IS_DEST_UNINITIALIZED);\n+      } else {\n+        store_sized_value(dst, src, size_in_bytes);\n+      }\n+    } else if (bt == T_DOUBLE) {\n+      strd(fromReg->as_FloatRegister(), dst);\n+    } else {\n+      assert(bt == T_FLOAT, \"must be float\");\n+      strs(fromReg->as_FloatRegister(), dst);\n+    }\n+  }\n+  bind(L_null);\n+  sig_index = stream.sig_index();\n+  from_index = stream.regs_index();\n+\n+  assert(reg_state[to->value()] == reg_writable, \"must have already been read\");\n+  bool success = move_helper(val_obj->as_VMReg(), to, T_OBJECT, reg_state);\n+  assert(success, \"to register must be writeable\");\n+\n+  return true;\n+}\n+\n+VMReg MacroAssembler::spill_reg_for(VMReg reg) {\n+  return (reg->is_FloatRegister()) ? v8->as_VMReg() : r14->as_VMReg();\n+}\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/macroAssembler_aarch64.cpp","additions":817,"deletions":2,"binary":false,"changes":819,"status":"modified"},{"patch":"@@ -35,0 +35,1 @@\n+#include \"utilities\/macros.hpp\"\n@@ -36,0 +37,4 @@\n+#include \"runtime\/signature.hpp\"\n+\n+\n+class ciInlineKlass;\n@@ -617,0 +622,31 @@\n+  \/\/ markWord tests, kills markWord reg\n+  void test_markword_is_inline_type(Register markword, Label& is_inline_type);\n+\n+  \/\/ inlineKlass queries, kills temp_reg\n+  void test_klass_is_inline_type(Register klass, Register temp_reg, Label& is_inline_type);\n+  void test_klass_is_empty_inline_type(Register klass, Register temp_reg, Label& is_empty_inline_type);\n+  void test_oop_is_not_inline_type(Register object, Register tmp, Label& not_inline_type);\n+\n+  \/\/ Get the default value oop for the given InlineKlass\n+  void get_default_value_oop(Register inline_klass, Register temp_reg, Register obj);\n+  \/\/ The empty value oop, for the given InlineKlass (\"empty\" as in no instance fields)\n+  \/\/ get_default_value_oop with extra assertion for empty inline klass\n+  void get_empty_inline_type_oop(Register inline_klass, Register temp_reg, Register obj);\n+\n+  void test_field_is_null_free_inline_type(Register flags, Register temp_reg, Label& is_null_free);\n+  void test_field_is_not_null_free_inline_type(Register flags, Register temp_reg, Label& not_null_free);\n+  void test_field_is_flat(Register flags, Register temp_reg, Label& is_flat);\n+\n+  \/\/ Check oops for special arrays, i.e. flat arrays and\/or null-free arrays\n+  void test_oop_prototype_bit(Register oop, Register temp_reg, int32_t test_bit, bool jmp_set, Label& jmp_label);\n+  void test_flat_array_oop(Register klass, Register temp_reg, Label& is_flat_array);\n+  void test_non_flat_array_oop(Register oop, Register temp_reg, Label&is_non_flat_array);\n+  void test_null_free_array_oop(Register oop, Register temp_reg, Label& is_null_free_array);\n+  void test_non_null_free_array_oop(Register oop, Register temp_reg, Label&is_non_null_free_array);\n+\n+  \/\/ Check array klass layout helper for flat or null-free arrays...\n+  void test_flat_array_layout(Register lh, Label& is_flat_array);\n+  void test_non_flat_array_layout(Register lh, Label& is_non_flat_array);\n+  void test_null_free_array_layout(Register lh, Label& is_null_free_array);\n+  void test_non_null_free_array_layout(Register lh, Label& is_non_null_free_array);\n+\n@@ -853,0 +889,2 @@\n+  void load_metadata(Register dst, Register src);\n+\n@@ -867,0 +905,9 @@\n+  void access_value_copy(DecoratorSet decorators, Register src, Register dst, Register inline_klass);\n+\n+  \/\/ inline type data payload offsets...\n+  void first_field_offset(Register inline_klass, Register offset);\n+  void data_for_oop(Register oop, Register data, Register inline_klass);\n+  \/\/ get data payload ptr a flat value array at index, kills rcx and index\n+  void data_for_value_array_index(Register array, Register array_klass,\n+                                  Register index, Register data);\n+\n@@ -880,0 +927,2 @@\n+  void load_prototype_header(Register dst, Register src);\n+\n@@ -927,0 +976,9 @@\n+\n+  \/\/ Object \/ value buffer allocation...\n+  \/\/ Allocate instance of klass, assumes klass initialized by caller\n+  \/\/ new_obj prefers to be rax\n+  \/\/ Kills t1 and t2, perserves klass, return allocation in new_obj (rsi on LP64)\n+  void allocate_instance(Register klass, Register new_obj,\n+                         Register t1, Register t2,\n+                         bool clear_fields, Label& alloc_failed);\n+\n@@ -937,0 +995,3 @@\n+  \/\/ For field \"index\" within \"klass\", return inline_klass ...\n+  void get_inline_type_field_klass(Register klass, Register index, Register inline_klass);\n+\n@@ -1329,0 +1390,18 @@\n+  void verified_entry(Compile* C, int sp_inc);\n+\n+  \/\/ Inline type specific methods\n+  #include \"asm\/macroAssembler_common.hpp\"\n+\n+  int store_inline_type_fields_to_buf(ciInlineKlass* vk, bool from_interpreter = true);\n+  bool move_helper(VMReg from, VMReg to, BasicType bt, RegState reg_state[]);\n+  bool unpack_inline_helper(const GrowableArray<SigEntry>* sig, int& sig_index,\n+                            VMReg from, int& from_index, VMRegPair* to, int to_count, int& to_index,\n+                            RegState reg_state[]);\n+  bool pack_inline_helper(const GrowableArray<SigEntry>* sig, int& sig_index, int vtarg_index,\n+                          VMRegPair* from, int from_count, int& from_index, VMReg to,\n+                          RegState reg_state[], Register val_array);\n+  int extend_stack_for_inline_args(int args_on_stack);\n+  void remove_frame(int initial_framesize, bool needs_stack_repair);\n+  VMReg spill_reg_for(VMReg reg);\n+  void save_stack_increment(int sp_inc, int frame_size);\n+\n@@ -1393,0 +1472,2 @@\n+  void fill_words(Register base, uint64_t cnt, Register value);\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/macroAssembler_aarch64.hpp","additions":81,"deletions":0,"binary":false,"changes":81,"status":"modified"},{"patch":"@@ -44,0 +44,1 @@\n+#include \"oops\/inlineKlass.hpp\"\n@@ -474,0 +475,5 @@\n+\n+  if (state == atos && InlineTypeReturnedAsFields) {\n+    __ store_inline_type_fields_to_buf(nullptr, true);\n+  }\n+\n@@ -589,0 +595,1 @@\n+  case T_PRIMITIVE_OBJECT: \/\/ fall through (value types are handled with oops)\n","filename":"src\/hotspot\/cpu\/aarch64\/templateInterpreterGenerator_aarch64.cpp","additions":7,"deletions":0,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -174,0 +174,1 @@\n+  case Bytecodes::_fast_qputfield:\n@@ -340,0 +341,1 @@\n+  __ andr(r3, r3, ~JVM_CONSTANT_QDescBit);\n@@ -755,4 +757,4 @@\n-    \/\/ ??? convention: move array into r3 for exception message\n-  __ mov(r3, array);\n-  __ mov(rscratch1, Interpreter::_throw_ArrayIndexOutOfBoundsException_entry);\n-  __ br(rscratch1);\n+  \/\/ ??? convention: move array into r3 for exception message\n+   __ mov(r3, array);\n+   __ mov(rscratch1, Interpreter::_throw_ArrayIndexOutOfBoundsException_entry);\n+   __ br(rscratch1);\n@@ -818,5 +820,17 @@\n-  __ add(r1, r1, arrayOopDesc::base_offset_in_bytes(T_OBJECT) >> LogBytesPerHeapOop);\n-  do_oop_load(_masm,\n-              Address(r0, r1, Address::uxtw(LogBytesPerHeapOop)),\n-              r0,\n-              IS_ARRAY);\n+  __ profile_array(r2, r0, r4);\n+  if (UseFlatArray) {\n+    Label is_flat_array, done;\n+\n+    __ test_flat_array_oop(r0, r8 \/*temp*\/, is_flat_array);\n+    __ add(r1, r1, arrayOopDesc::base_offset_in_bytes(T_OBJECT) >> LogBytesPerHeapOop);\n+    do_oop_load(_masm, Address(r0, r1, Address::uxtw(LogBytesPerHeapOop)), r0, IS_ARRAY);\n+\n+    __ b(done);\n+    __ bind(is_flat_array);\n+    __ call_VM(r0, CAST_FROM_FN_PTR(address, InterpreterRuntime::value_array_load), r0, r1);\n+    __ bind(done);\n+  } else {\n+    __ add(r1, r1, arrayOopDesc::base_offset_in_bytes(T_OBJECT) >> LogBytesPerHeapOop);\n+    do_oop_load(_masm, Address(r0, r1, Address::uxtw(LogBytesPerHeapOop)), r0, IS_ARRAY);\n+  }\n+  __ profile_element(r2, r0, r4);\n@@ -1109,1 +1123,1 @@\n-  Label is_null, ok_is_subtype, done;\n+  Label is_null, is_flat_array, ok_is_subtype, done;\n@@ -1116,2 +1130,4 @@\n-  Address element_address(r3, r4, Address::uxtw(LogBytesPerHeapOop));\n-\n+\n+  __ profile_array(r4, r3, r5);\n+  __ profile_element(r4, r0, r5);\n+\n@@ -1120,0 +1136,2 @@\n+  Address element_address(r3, r4, Address::uxtw(LogBytesPerHeapOop));\n+  \/\/ Be careful not to clobber r4 below\n@@ -1124,0 +1142,8 @@\n+  \/\/ Move array class to r5\n+  __ load_klass(r5, r3);\n+\n+  if (UseFlatArray) {\n+    __ ldrw(r6, Address(r5, Klass::layout_helper_offset()));\n+    __ test_flat_array_layout(r6, is_flat_array);\n+  }\n+\n@@ -1126,4 +1152,3 @@\n-  \/\/ Move superklass into r0\n-  __ load_klass(r0, r3);\n-  __ ldr(r0, Address(r0,\n-                     ObjArrayKlass::element_klass_offset()));\n+\n+  \/\/ Move array element superklass into r0\n+  __ ldr(r0, Address(r5, ObjArrayKlass::element_klass_offset()));\n@@ -1134,1 +1159,3 @@\n-  __ gen_subtype_check(r1, ok_is_subtype);\n+\n+  \/\/ is \"r1 <: r0\" ? (value subclass <: array element superclass)\n+  __ gen_subtype_check(r1, ok_is_subtype, false);\n@@ -1151,1 +1178,12 @@\n-  __ profile_null_seen(r2);\n+  if (EnablePrimitiveClasses) {\n+    Label is_null_into_value_array_npe, store_null;\n+\n+    \/\/ No way to store null in flat null-free array\n+    __ test_null_free_array_oop(r3, r8, is_null_into_value_array_npe);\n+    __ b(store_null);\n+\n+    __ bind(is_null_into_value_array_npe);\n+    __ b(ExternalAddress(Interpreter::_throw_NullPointerException_entry));\n+\n+    __ bind(store_null);\n+  }\n@@ -1155,0 +1193,41 @@\n+  __ b(done);\n+\n+  if (UseFlatArray) {\n+     Label is_type_ok;\n+    __ bind(is_flat_array); \/\/ Store non-null value to flat\n+\n+    \/\/ Simplistic type check...\n+    \/\/ r0 - value, r2 - index, r3 - array.\n+\n+    \/\/ Profile the not-null value's klass.\n+    \/\/ Load value class\n+     __ load_klass(r1, r0);\n+\n+    \/\/ Move element klass into r7\n+     __ ldr(r7, Address(r5, ArrayKlass::element_klass_offset()));\n+\n+    \/\/ flat value array needs exact type match\n+    \/\/ is \"r1 == r7\" (value subclass == array element superclass)\n+\n+     __ cmp(r7, r1);\n+     __ br(Assembler::EQ, is_type_ok);\n+\n+     __ b(ExternalAddress(Interpreter::_throw_ArrayStoreException_entry));\n+\n+     __ bind(is_type_ok);\n+    \/\/ r1: value's klass\n+    \/\/ r3: array\n+    \/\/ r5: array klass\n+    __ test_klass_is_empty_inline_type(r1, r7, done);\n+\n+    \/\/ calc dst for copy\n+    __ ldrw(r7, at_tos_p1()); \/\/ index\n+    __ data_for_value_array_index(r3, r5, r7, r7);\n+\n+    \/\/ ...and src for copy\n+    __ ldr(r6, at_tos());  \/\/ value\n+    __ data_for_oop(r6, r6, r1);\n+\n+    __ mov(r4, r1);  \/\/ Shuffle arguments to avoid conflict with c_rarg1\n+    __ access_value_copy(IN_HEAP, r6, r7, r4);\n+  }\n@@ -1959,2 +2038,1 @@\n-void TemplateTable::if_acmp(Condition cc)\n-{\n+void TemplateTable::if_acmp(Condition cc) {\n@@ -1963,1 +2041,1 @@\n-  Label not_taken;\n+  Label taken, not_taken;\n@@ -1965,0 +2043,38 @@\n+\n+  __ profile_acmp(r2, r1, r0, r4);\n+\n+  Register is_inline_type_mask = rscratch1;\n+  __ mov(is_inline_type_mask, markWord::inline_type_pattern);\n+\n+  if (EnableValhalla) {\n+    __ cmp(r1, r0);\n+    __ br(Assembler::EQ, (cc == equal) ? taken : not_taken);\n+\n+    \/\/ might be substitutable, test if either r0 or r1 is null\n+    __ andr(r2, r0, r1);\n+    __ cbz(r2, (cc == equal) ? not_taken : taken);\n+\n+    \/\/ and both are values ?\n+    __ ldr(r2, Address(r1, oopDesc::mark_offset_in_bytes()));\n+    __ andr(r2, r2, is_inline_type_mask);\n+    __ ldr(r4, Address(r0, oopDesc::mark_offset_in_bytes()));\n+    __ andr(r4, r4, is_inline_type_mask);\n+    __ andr(r2, r2, r4);\n+    __ cmp(r2,  is_inline_type_mask);\n+    __ br(Assembler::NE, (cc == equal) ? not_taken : taken);\n+\n+    \/\/ same value klass ?\n+    __ load_metadata(r2, r1);\n+    __ load_metadata(r4, r0);\n+    __ cmp(r2, r4);\n+    __ br(Assembler::NE, (cc == equal) ? not_taken : taken);\n+\n+    \/\/ Know both are the same type, let's test for substitutability...\n+    if (cc == equal) {\n+      invoke_is_substitutable(r0, r1, taken, not_taken);\n+    } else {\n+      invoke_is_substitutable(r0, r1, not_taken, taken);\n+    }\n+    __ stop(\"Not reachable\");\n+  }\n+\n@@ -1967,0 +2083,1 @@\n+  __ bind(taken);\n@@ -1969,1 +2086,10 @@\n-  __ profile_not_taken_branch(r0);\n+  __ profile_not_taken_branch(r0, true);\n+}\n+\n+void TemplateTable::invoke_is_substitutable(Register aobj, Register bobj,\n+                                            Label& is_subst, Label& not_subst) {\n+\n+  __ call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::is_substitutable), aobj, bobj);\n+  \/\/ Restored... r0 answer, jmp to outcome...\n+  __ cbz(r0, not_subst);\n+  __ b(is_subst);\n@@ -1972,0 +2098,1 @@\n+\n@@ -2373,1 +2500,1 @@\n-                                          ConstantPoolCacheEntry::f2_offset())));\n+                                      ConstantPoolCacheEntry::f2_offset())));\n@@ -2376,1 +2503,1 @@\n-                                           ConstantPoolCacheEntry::flags_offset())));\n+                                         ConstantPoolCacheEntry::flags_offset())));\n@@ -2538,1 +2665,1 @@\n-  const Register cache     = r4;\n+  const Register cache     = r2;\n@@ -2540,0 +2667,3 @@\n+  const Register klass     = r5;\n+  const Register inline_klass = r7;\n+  const Register field_index = r23;\n@@ -2548,0 +2678,5 @@\n+\n+  \/\/ Valhalla extras\n+  __ load_unsigned_short(field_index, Address(cache, in_bytes(ResolvedFieldEntry::field_index_offset())));\n+  __ ldr(klass, Address(cache, ResolvedFieldEntry::field_holder_offset()));\n+\n@@ -2606,4 +2741,67 @@\n-  do_oop_load(_masm, field, r0, IN_HEAP);\n-  __ push(atos);\n-  if (rc == may_rewrite) {\n-    patch_bytecode(Bytecodes::_fast_agetfield, bc, r1);\n+  if (!EnablePrimitiveClasses) {\n+    do_oop_load(_masm, field, r0, IN_HEAP);\n+    __ push(atos);\n+    if (rc == may_rewrite) {\n+      patch_bytecode(Bytecodes::_fast_agetfield, bc, r1);\n+    }\n+    __ b(Done);\n+  } else { \/\/ Valhalla\n+    if (is_static) {\n+      __ load_heap_oop(r0, field, rscratch1, rscratch2);\n+      Label is_null_free_inline_type, uninitialized;\n+      \/\/ Issue below if the static field has not been initialized yet\n+      __ test_field_is_null_free_inline_type(flags, noreg \/*temp*\/, is_null_free_inline_type);\n+        \/\/ field is not a null free inline type\n+        __ push(atos);\n+        __ b(Done);\n+      \/\/ field is a null free inline type, must not return null even if uninitialized\n+      __ bind(is_null_free_inline_type);\n+        __ cbz(r0, uninitialized);\n+          __ push(atos);\n+          __ b(Done);\n+        __ bind(uninitialized);\n+          Label slow_case, finish;\n+          __ ldrb(rscratch1, Address(klass, InstanceKlass::init_state_offset()));\n+          __ cmp(rscratch1, (u1)InstanceKlass::fully_initialized);\n+          __ br(Assembler::NE, slow_case);\n+          __ get_default_value_oop(klass, off \/* temp *\/, r0);\n+        __ b(finish);\n+        __ bind(slow_case);\n+          __ call_VM(r0, CAST_FROM_FN_PTR(address, InterpreterRuntime::uninitialized_static_inline_type_field), obj, cache);\n+          __ bind(finish);\n+          __ verify_oop(r0);\n+          __ push(atos);\n+          __ b(Done);\n+    } else {\n+      Label is_flat, nonnull, is_inline_type, rewrite_inline;\n+      __ test_field_is_null_free_inline_type(flags, noreg \/*temp*\/, is_inline_type);\n+        \/\/ Non-inline field case\n+        __ load_heap_oop(r0, field, rscratch1, rscratch2);\n+        __ push(atos);\n+        if (rc == may_rewrite) {\n+          patch_bytecode(Bytecodes::_fast_agetfield, bc, r1);\n+        }\n+        __ b(Done);\n+      __ bind(is_inline_type);\n+        __ test_field_is_flat(flags, noreg \/* temp *\/, is_flat);\n+         \/\/ field is not flat\n+          __ load_heap_oop(r0, field, rscratch1, rscratch2);\n+          __ cbnz(r0, nonnull);\n+            __ get_inline_type_field_klass(klass, field_index, inline_klass);\n+            __ get_default_value_oop(inline_klass, klass \/* temp *\/, r0);\n+          __ bind(nonnull);\n+          __ verify_oop(r0);\n+          __ push(atos);\n+          __ b(rewrite_inline);\n+        __ bind(is_flat);\n+        \/\/ field is flat\n+          __ mov(r0, obj);\n+          __ read_flat_field(klass, field_index, off, inline_klass \/* temp *\/, r0);\n+          __ verify_oop(r0);\n+          __ push(atos);\n+      __ bind(rewrite_inline);\n+      if (rc == may_rewrite) {\n+        patch_bytecode(Bytecodes::_fast_qgetfield, bc, r1);\n+      }\n+      __ b(Done);\n+    }\n@@ -2611,1 +2809,0 @@\n-  __ b(Done);\n@@ -2772,1 +2969,1 @@\n-  const Register flags     = r0;\n+  const Register flags     = r6;\n@@ -2774,0 +2971,1 @@\n+  const Register inline_klass = r5;\n@@ -2780,2 +2978,0 @@\n-  __ mov(r5, flags);\n-\n@@ -2784,1 +2980,1 @@\n-    __ tbz(r5, ResolvedFieldEntry::is_volatile_shift, notVolatile);\n+    __ tbz(flags, ResolvedFieldEntry::is_volatile_shift, notVolatile);\n@@ -2833,8 +3029,54 @@\n-    __ pop(atos);\n-    if (!is_static) pop_and_check_object(obj);\n-    \/\/ Store into the field\n-    do_oop_store(_masm, field, r0, IN_HEAP);\n-    if (rc == may_rewrite) {\n-      patch_bytecode(Bytecodes::_fast_aputfield, bc, r1, true, byte_no);\n-    }\n-    __ b(Done);\n+     if (!EnablePrimitiveClasses) {\n+      __ pop(atos);\n+      if (!is_static) pop_and_check_object(obj);\n+      \/\/ Store into the field\n+      do_oop_store(_masm, field, r0, IN_HEAP);\n+      if (rc == may_rewrite) {\n+        patch_bytecode(Bytecodes::_fast_aputfield, bc, r1, true, byte_no);\n+      }\n+      __ b(Done);\n+     } else { \/\/ Valhalla\n+      __ pop(atos);\n+      if (is_static) {\n+        Label is_inline_type;\n+         __ test_field_is_not_null_free_inline_type(flags, noreg \/* temp *\/, is_inline_type);\n+         __ null_check(r0);\n+         __ bind(is_inline_type);\n+         do_oop_store(_masm, field, r0, IN_HEAP);\n+         __ b(Done);\n+      } else {\n+        Label is_inline_type, is_flat, rewrite_not_inline, rewrite_inline;\n+        __ test_field_is_null_free_inline_type(flags, noreg \/*temp*\/, is_inline_type);\n+        \/\/ Not an inline type\n+        pop_and_check_object(obj);\n+        \/\/ Store into the field\n+        do_oop_store(_masm, field, r0, IN_HEAP);\n+        __ bind(rewrite_not_inline);\n+        if (rc == may_rewrite) {\n+          patch_bytecode(Bytecodes::_fast_aputfield, bc, r19, true, byte_no);\n+        }\n+        __ b(Done);\n+        \/\/ Implementation of the inline type semantic\n+        __ bind(is_inline_type);\n+        __ null_check(r0);\n+        __ test_field_is_flat(flags, noreg \/*temp*\/, is_flat);\n+        \/\/ field is not flat\n+        pop_and_check_object(obj);\n+        \/\/ Store into the field\n+        do_oop_store(_masm, field, r0, IN_HEAP);\n+        __ b(rewrite_inline);\n+        __ bind(is_flat);\n+        \/\/ field is flat\n+        pop_and_check_object(obj);\n+        assert_different_registers(r0, inline_klass, obj, off);\n+        __ load_klass(inline_klass, r0);\n+        __ data_for_oop(r0, r0, inline_klass);\n+        __ add(obj, obj, off);\n+        __ access_value_copy(IN_HEAP, r0, obj, inline_klass);\n+        __ bind(rewrite_inline);\n+        if (rc == may_rewrite) {\n+          patch_bytecode(Bytecodes::_fast_qputfield, bc, r19, true, byte_no);\n+        }\n+        __ b(Done);\n+      }\n+     }  \/\/ Valhalla\n@@ -2945,1 +3187,1 @@\n-    __ tbz(r5, ResolvedFieldEntry::is_volatile_shift, notVolatile);\n+    __ tbz(flags, ResolvedFieldEntry::is_volatile_shift, notVolatile);\n@@ -2979,0 +3221,1 @@\n+    case Bytecodes::_fast_qputfield: \/\/fall through\n@@ -3005,0 +3248,1 @@\n+    case Bytecodes::_fast_qputfield: \/\/fall through\n@@ -3055,0 +3299,17 @@\n+  case Bytecodes::_fast_qputfield: \/\/fall through\n+   {\n+      Label is_flat, done;\n+      __ null_check(r0);\n+      __ test_field_is_flat(r3, noreg \/* temp *\/, is_flat);\n+      \/\/ field is not flat\n+      do_oop_store(_masm, field, r0, IN_HEAP);\n+      __ b(done);\n+      __ bind(is_flat);\n+      \/\/ field is flat\n+      __ load_klass(r4, r0);\n+      __ data_for_oop(r0, r0, r4);\n+      __ lea(rscratch1, field);\n+      __ access_value_copy(IN_HEAP, r0, rscratch1, r4);\n+      __ bind(done);\n+    }\n+    break;\n@@ -3150,0 +3411,24 @@\n+  case Bytecodes::_fast_qgetfield:\n+    {\n+      Register index = r4, klass = r5, inline_klass = r6, tmp = r7;\n+      Label is_flat, nonnull, Done;\n+      __ test_field_is_flat(r3, noreg \/* temp *\/, is_flat);\n+        \/\/ field is not flat\n+        __ load_heap_oop(r0, field, rscratch1, rscratch2);\n+        __ cbnz(r0, nonnull);\n+          __ load_unsigned_short(index, Address(r2, in_bytes(ResolvedFieldEntry::field_index_offset())));\n+          __ ldr(klass, Address(r2, in_bytes(ResolvedFieldEntry::field_holder_offset())));\n+          __ get_inline_type_field_klass(klass, index, inline_klass);\n+          __ get_default_value_oop(inline_klass, tmp \/* temp *\/, r0);\n+        __ bind(nonnull);\n+        __ verify_oop(r0);\n+        __ b(Done);\n+      __ bind(is_flat);\n+      \/\/ field is flat\n+        __ load_unsigned_short(index, Address(r2, in_bytes(ResolvedFieldEntry::field_index_offset())));\n+        __ ldr(klass, Address(r2, in_bytes(ResolvedFieldEntry::field_holder_offset())));\n+        __ read_flat_field(klass, index, r1, tmp \/* temp *\/, r0);\n+        __ verify_oop(r0);\n+      __ bind(Done);\n+    }\n+    break;\n@@ -3574,0 +3859,1 @@\n+  Label is_not_value;\n@@ -3590,0 +3876,8 @@\n+  __ ldrb(rscratch1, Address(r4, InstanceKlass::kind_offset()));\n+  __ cmp(rscratch1, (u1)Klass::InlineKlassKind);\n+  __ br(Assembler::NE, is_not_value);\n+\n+  __ call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::throw_InstantiationError));\n+\n+  __ bind(is_not_value);\n+\n@@ -3596,57 +3890,2 @@\n-  \/\/ get instance_size in InstanceKlass (scaled to a count of bytes)\n-  __ ldrw(r3,\n-          Address(r4,\n-                  Klass::layout_helper_offset()));\n-  \/\/ test to see if it has a finalizer or is malformed in some way\n-  __ tbnz(r3, exact_log2(Klass::_lh_instance_slow_path_bit), slow_case);\n-\n-  \/\/ Allocate the instance:\n-  \/\/  If TLAB is enabled:\n-  \/\/    Try to allocate in the TLAB.\n-  \/\/    If fails, go to the slow path.\n-  \/\/    Initialize the allocation.\n-  \/\/    Exit.\n-  \/\/\n-  \/\/  Go to slow path.\n-\n-  if (UseTLAB) {\n-    __ tlab_allocate(r0, r3, 0, noreg, r1, slow_case);\n-\n-    if (ZeroTLAB) {\n-      \/\/ the fields have been already cleared\n-      __ b(initialize_header);\n-    }\n-\n-    \/\/ The object is initialized before the header.  If the object size is\n-    \/\/ zero, go directly to the header initialization.\n-    __ sub(r3, r3, sizeof(oopDesc));\n-    __ cbz(r3, initialize_header);\n-\n-    \/\/ Initialize object fields\n-    {\n-      __ add(r2, r0, sizeof(oopDesc));\n-      Label loop;\n-      __ bind(loop);\n-      __ str(zr, Address(__ post(r2, BytesPerLong)));\n-      __ sub(r3, r3, BytesPerLong);\n-      __ cbnz(r3, loop);\n-    }\n-\n-    \/\/ initialize object header only.\n-    __ bind(initialize_header);\n-    __ mov(rscratch1, (intptr_t)markWord::prototype().value());\n-    __ str(rscratch1, Address(r0, oopDesc::mark_offset_in_bytes()));\n-    __ store_klass_gap(r0, zr);  \/\/ zero klass gap for compressed oops\n-    __ store_klass(r0, r4);      \/\/ store klass last\n-\n-    {\n-      SkipIfEqual skip(_masm, &DTraceAllocProbes, false);\n-      \/\/ Trigger dtrace event for fastpath\n-      __ push(atos); \/\/ save the return value\n-      __ call_VM_leaf(\n-           CAST_FROM_FN_PTR(address, static_cast<int (*)(oopDesc*)>(SharedRuntime::dtrace_object_alloc)), r0);\n-      __ pop(atos); \/\/ restore the return value\n-\n-    }\n-    __ b(done);\n-  }\n+  __ allocate_instance(r4, r0, r3, r1, true, slow_case);\n+  __ b(done);\n@@ -3667,0 +3906,24 @@\n+void TemplateTable::aconst_init() {\n+  transition(vtos, atos);\n+  __ get_unsigned_2_byte_index_at_bcp(c_rarg2, 1);\n+  __ get_constant_pool(c_rarg1);\n+  call_VM(r0, CAST_FROM_FN_PTR(address, InterpreterRuntime::aconst_init),\n+          c_rarg1, c_rarg2);\n+  __ verify_oop(r0);\n+  \/\/ Must prevent reordering of stores for object initialization with stores that publish the new object.\n+  __ membar(Assembler::StoreStore);\n+}\n+\n+void TemplateTable::withfield() {\n+  transition(vtos, atos);\n+\n+  resolve_cache_and_index_for_field(f2_byte, c_rarg1 \/*cache*\/, c_rarg2 \/*index*\/);\n+  __ lea(c_rarg2, at_tos());\n+  call_VM(r1, CAST_FROM_FN_PTR(address, InterpreterRuntime::withfield), c_rarg1, c_rarg2);\n+  \/\/ new value type is returned in r1\n+  \/\/ stack adjustment is returned in r0\n+  __ verify_oop(r1);\n+  __ add(esp, esp, r0);\n+  __ mov(r0, r1);\n+}\n+\n@@ -3706,0 +3969,1 @@\n+  __ andr(r1, r1, ~JVM_CONSTANT_QDescBit);\n@@ -3737,0 +4001,3 @@\n+  __ b(done);\n+  __ bind(is_null);\n+\n@@ -3739,4 +4006,16 @@\n-    __ b(done);\n-    __ bind(is_null);\n-  } else {\n-    __ bind(is_null);   \/\/ same as 'done'\n+\n+  if (EnablePrimitiveClasses) {\n+    \/\/ Get cpool & tags index\n+    __ get_cpool_and_tags(r2, r3); \/\/ r2=cpool, r3=tags array\n+    __ get_unsigned_2_byte_index_at_bcp(r19, 1); \/\/ r19=index\n+     \/\/ See if bytecode has already been quicked\n+    __ add(rscratch1, r3, Array<u1>::base_offset_in_bytes());\n+    __ lea(r1, Address(rscratch1, r19));\n+    __ ldarb(r1, r1);\n+    \/\/ See if CP entry is a Q-descriptor\n+    __ andr (r1, r1, JVM_CONSTANT_QDescBit);\n+    __ cmp(r1, (u1) JVM_CONSTANT_QDescBit);\n+    __ br(Assembler::NE, done);\n+    __ b(ExternalAddress(Interpreter::_throw_NullPointerException_entry));\n+  }\n+\n@@ -3760,0 +4039,1 @@\n+  __ andr(r1, r1, ~JVM_CONSTANT_QDescBit);\n@@ -3863,0 +4143,4 @@\n+  Label is_inline_type;\n+  __ ldr(rscratch1, Address(r0, oopDesc::mark_offset_in_bytes()));\n+  __ test_markword_is_inline_type(rscratch1, is_inline_type);\n+\n@@ -3957,0 +4241,5 @@\n+\n+  __ bind(is_inline_type);\n+  __ call_VM(noreg, CAST_FROM_FN_PTR(address,\n+                    InterpreterRuntime::throw_illegal_monitor_state_exception));\n+  __ should_not_reach_here();\n@@ -3967,0 +4256,12 @@\n+  const int is_inline_type_mask = markWord::inline_type_pattern;\n+  Label has_identity;\n+  __ ldr(rscratch1, Address(r0, oopDesc::mark_offset_in_bytes()));\n+  __ mov(rscratch2, is_inline_type_mask);\n+  __ andr(rscratch1, rscratch1, rscratch2);\n+  __ cmp(rscratch1, rscratch2);\n+  __ br(Assembler::NE, has_identity);\n+  __ call_VM(noreg, CAST_FROM_FN_PTR(address,\n+                     InterpreterRuntime::throw_illegal_monitor_state_exception));\n+  __ should_not_reach_here();\n+  __ bind(has_identity);\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/templateTable_aarch64.cpp","additions":405,"deletions":104,"binary":false,"changes":509,"status":"modified"},{"patch":"@@ -50,1 +50,1 @@\n-VtableStub* VtableStubs::create_vtable_stub(int vtable_index) {\n+VtableStub* VtableStubs::create_vtable_stub(int vtable_index, bool caller_is_c1) {\n@@ -53,1 +53,1 @@\n-  VtableStub* s = new(stub_code_length) VtableStub(true, vtable_index);\n+  VtableStub* s = new(stub_code_length) VtableStub(true, vtable_index, caller_is_c1);\n@@ -66,0 +66,4 @@\n+\/\/ No variance was detected in vtable stub sizes. Setting index_dependent_slop == 0 will unveil any deviation from this observation.\n+  const int index_dependent_slop     = 0;\n+  ByteSize  entry_offset = caller_is_c1 ? Method::from_compiled_inline_offset() :  Method::from_compiled_inline_ro_offset();\n+\n@@ -119,1 +123,1 @@\n-    __ ldr(rscratch1, Address(rmethod, Method::from_compiled_offset()));\n+    __ ldr(rscratch1, Address(rmethod, entry_offset));\n@@ -130,1 +134,1 @@\n-  __ ldr(rscratch1, Address(rmethod, Method::from_compiled_offset()));\n+  __ ldr(rscratch1, Address(rmethod, entry_offset));\n@@ -134,1 +138,2 @@\n-  bookkeeping(masm, tty, s, npe_addr, ame_addr, true, vtable_index, slop_bytes, 0);\n+  slop_bytes += index_dependent_slop; \/\/ add'l slop for size variance due to large itable offsets\n+  bookkeeping(masm, tty, s, npe_addr, ame_addr, true, vtable_index, slop_bytes, index_dependent_slop);\n@@ -140,1 +145,1 @@\n-VtableStub* VtableStubs::create_itable_stub(int itable_index) {\n+VtableStub* VtableStubs::create_itable_stub(int itable_index, bool caller_is_c1) {\n@@ -143,1 +148,1 @@\n-  VtableStub* s = new(stub_code_length) VtableStub(false, itable_index);\n+  VtableStub* s = new(stub_code_length) VtableStub(false, itable_index, caller_is_c1);\n@@ -156,0 +161,4 @@\n+  const int index_dependent_slop = (itable_index == 0) ? 4 :     \/\/ code size change with transition from 8-bit to 32-bit constant (@index == 16).\n+                                   (itable_index < 16) ? 3 : 0;  \/\/ index == 0 generates even shorter code.\n+  ByteSize  entry_offset = caller_is_c1 ? Method::from_compiled_inline_offset() :  Method::from_compiled_inline_ro_offset();\n+\n@@ -210,1 +219,1 @@\n-    __ ldr(rscratch1, Address(rmethod, Method::from_compiled_offset()));\n+    __ ldr(rscratch1, Address(rmethod, entry_offset));\n@@ -220,1 +229,1 @@\n-  __ ldr(rscratch1, Address(rmethod, Method::from_compiled_offset()));\n+  __ ldr(rscratch1, Address(rmethod, entry_offset));\n@@ -233,1 +242,2 @@\n-  bookkeeping(masm, tty, s, npe_addr, ame_addr, false, itable_index, slop_bytes, 0);\n+  slop_bytes += index_dependent_slop; \/\/ add'l slop for size variance due to large itable offsets\n+  bookkeeping(masm, tty, s, npe_addr, ame_addr, false, itable_index, slop_bytes, index_dependent_slop);\n","filename":"src\/hotspot\/cpu\/aarch64\/vtableStubs_aarch64.cpp","additions":20,"deletions":10,"binary":false,"changes":30,"status":"modified"},{"patch":"@@ -1895,1 +1895,1 @@\n-        assert(ReturnTypeEntry::static_cell_count() < TypeStackSlotEntries::per_arg_count(),\n+        assert(SingleTypeEntry::static_cell_count() < TypeStackSlotEntries::per_arg_count(),\n@@ -1936,1 +1936,1 @@\n-    profile_obj_type(ret, R28_mdx, -in_bytes(ReturnTypeEntry::size()), tmp1, tmp2);\n+    profile_obj_type(ret, R28_mdx, -in_bytes(SingleTypeEntry::size()), tmp1, tmp2);\n","filename":"src\/hotspot\/cpu\/ppc\/interp_masm_ppc_64.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -1782,1 +1782,1 @@\n-        assert(ReturnTypeEntry::static_cell_count() < TypeStackSlotEntries::per_arg_count(), \"can't move past ret type\");\n+        assert(SingleTypeEntry::static_cell_count() < TypeStackSlotEntries::per_arg_count(), \"can't move past ret type\");\n@@ -1831,1 +1831,1 @@\n-    Address mdo_ret_addr(mdp, -in_bytes(ReturnTypeEntry::size()));\n+    Address mdo_ret_addr(mdp, -in_bytes(SingleTypeEntry::size()));\n","filename":"src\/hotspot\/cpu\/s390\/interp_masm_s390.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -35,0 +35,1 @@\n+#include \"ci\/ciInlineKlass.hpp\"\n@@ -40,0 +41,1 @@\n+#include \"oops\/oop.inline.hpp\"\n@@ -482,1 +484,1 @@\n-  __ remove_frame(initial_frame_size_in_bytes());\n+  __ remove_frame(initial_frame_size_in_bytes(), needs_stack_repair());\n@@ -519,0 +521,31 @@\n+  if (InlineTypeReturnedAsFields) {\n+  #ifndef _LP64\n+     Unimplemented();\n+  #endif\n+    \/\/ Check if we are returning an non-null inline type and load its fields into registers\n+    ciType* return_type = compilation()->method()->return_type();\n+    if (return_type->is_inlinetype()) {\n+      ciInlineKlass* vk = return_type->as_inline_klass();\n+      if (vk->can_be_returned_as_fields()) {\n+        address unpack_handler = vk->unpack_handler();\n+        assert(unpack_handler != nullptr, \"must be\");\n+        __ call(RuntimeAddress(unpack_handler));\n+      }\n+    } else if (return_type->is_instance_klass() && (!return_type->is_loaded() || StressCallingConvention)) {\n+      Label skip;\n+      __ test_oop_is_not_inline_type(rax, rscratch1, skip);\n+\n+      \/\/ Load fields from a buffered value with an inline class specific handler\n+      __ load_klass(rdi, rax, rscratch1);\n+      __ movptr(rdi, Address(rdi, InstanceKlass::adr_inlineklass_fixed_block_offset()));\n+      __ movptr(rdi, Address(rdi, InlineKlass::unpack_handler_offset()));\n+      \/\/ Unpack handler can be null if inline type is not scalarizable in returns\n+      __ testptr(rdi, rdi);\n+      __ jcc(Assembler::zero, skip);\n+      __ call(rdi);\n+\n+      __ bind(skip);\n+    }\n+    \/\/ At this point, rax points to the value object (for interpreter or C1 caller).\n+    \/\/ The fields of the object are copied into registers (for C2 caller).\n+  }\n@@ -521,1 +554,1 @@\n-  __ remove_frame(initial_frame_size_in_bytes());\n+  __ remove_frame(initial_frame_size_in_bytes(), needs_stack_repair());\n@@ -543,0 +576,4 @@\n+int LIR_Assembler::store_inline_type_fields_to_buf(ciInlineKlass* vk) {\n+  return (__ store_inline_type_fields_to_buf(vk, false));\n+}\n+\n@@ -1617,1 +1654,1 @@\n-  if (UseSlowPath ||\n+  if (UseSlowPath || op->is_null_free() ||\n@@ -1715,24 +1752,26 @@\n-  __ testptr(obj, obj);\n-  if (op->should_profile()) {\n-    Label not_null;\n-    Register mdo  = klass_RInfo;\n-    __ mov_metadata(mdo, md->constant_encoding());\n-    __ jccb(Assembler::notEqual, not_null);\n-    \/\/ Object is null; update MDO and exit\n-    Address data_addr(mdo, md->byte_offset_of_slot(data, DataLayout::flags_offset()));\n-    int header_bits = BitData::null_seen_byte_constant();\n-    __ orb(data_addr, header_bits);\n-    __ jmp(*obj_is_null);\n-    __ bind(not_null);\n-\n-    Label update_done;\n-    Register recv = k_RInfo;\n-    __ load_klass(recv, obj, tmp_load_klass);\n-    type_profile_helper(mdo, md, data, recv, &update_done);\n-\n-    Address nonprofiled_receiver_count_addr(mdo, md->byte_offset_of_slot(data, CounterData::count_offset()));\n-    __ addptr(nonprofiled_receiver_count_addr, DataLayout::counter_increment);\n-\n-    __ bind(update_done);\n-  } else {\n-    __ jcc(Assembler::equal, *obj_is_null);\n+  if (op->need_null_check()) {\n+    __ testptr(obj, obj);\n+    if (op->should_profile()) {\n+      Label not_null;\n+      Register mdo  = klass_RInfo;\n+      __ mov_metadata(mdo, md->constant_encoding());\n+      __ jccb(Assembler::notEqual, not_null);\n+      \/\/ Object is null; update MDO and exit\n+      Address data_addr(mdo, md->byte_offset_of_slot(data, DataLayout::flags_offset()));\n+      int header_bits = BitData::null_seen_byte_constant();\n+      __ orb(data_addr, header_bits);\n+      __ jmp(*obj_is_null);\n+      __ bind(not_null);\n+\n+      Label update_done;\n+      Register recv = k_RInfo;\n+      __ load_klass(recv, obj, tmp_load_klass);\n+      type_profile_helper(mdo, md, data, recv, &update_done);\n+\n+      Address nonprofiled_receiver_count_addr(mdo, md->byte_offset_of_slot(data, CounterData::count_offset()));\n+      __ addptr(nonprofiled_receiver_count_addr, DataLayout::counter_increment);\n+\n+      __ bind(update_done);\n+    } else {\n+      __ jcc(Assembler::equal, *obj_is_null);\n+    }\n@@ -1930,0 +1969,124 @@\n+void LIR_Assembler::emit_opFlattenedArrayCheck(LIR_OpFlattenedArrayCheck* op) {\n+  \/\/ We are loading\/storing from\/to an array that *may* be a flat array (the\n+  \/\/ declared type is Object[], abstract[], interface[] or VT.ref[]).\n+  \/\/ If this array is a flat array, take the slow path.\n+  Register klass = op->tmp()->as_register();\n+  if (UseArrayMarkWordCheck) {\n+    __ test_flat_array_oop(op->array()->as_register(), op->tmp()->as_register(), *op->stub()->entry());\n+  } else {\n+    Register tmp_load_klass = LP64_ONLY(rscratch1) NOT_LP64(noreg);\n+    __ load_klass(klass, op->array()->as_register(), tmp_load_klass);\n+    __ movl(klass, Address(klass, Klass::layout_helper_offset()));\n+    __ testl(klass, Klass::_lh_array_tag_flat_value_bit_inplace);\n+    __ jcc(Assembler::notZero, *op->stub()->entry());\n+  }\n+  if (!op->value()->is_illegal()) {\n+    \/\/ The array is not a flat array, but it might be null-free. If we are storing\n+    \/\/ a null into a null-free array, take the slow path (which will throw NPE).\n+    Label skip;\n+    __ cmpptr(op->value()->as_register(), NULL_WORD);\n+    __ jcc(Assembler::notEqual, skip);\n+    if (UseArrayMarkWordCheck) {\n+      __ test_null_free_array_oop(op->array()->as_register(), op->tmp()->as_register(), *op->stub()->entry());\n+    } else {\n+      __ testl(klass, Klass::_lh_null_free_array_bit_inplace);\n+      __ jcc(Assembler::notZero, *op->stub()->entry());\n+    }\n+    __ bind(skip);\n+  }\n+}\n+\n+void LIR_Assembler::emit_opNullFreeArrayCheck(LIR_OpNullFreeArrayCheck* op) {\n+  \/\/ We are storing into an array that *may* be null-free (the declared type is\n+  \/\/ Object[], abstract[], interface[] or VT.ref[]).\n+  if (UseArrayMarkWordCheck) {\n+    Label test_mark_word;\n+    Register tmp = op->tmp()->as_register();\n+    __ movptr(tmp, Address(op->array()->as_register(), oopDesc::mark_offset_in_bytes()));\n+    __ testl(tmp, markWord::unlocked_value);\n+    __ jccb(Assembler::notZero, test_mark_word);\n+    __ load_prototype_header(tmp, op->array()->as_register(), rscratch1);\n+    __ bind(test_mark_word);\n+    __ testl(tmp, markWord::null_free_array_bit_in_place);\n+  } else {\n+    Register tmp_load_klass = LP64_ONLY(rscratch1) NOT_LP64(noreg);\n+    Register klass = op->tmp()->as_register();\n+    __ load_klass(klass, op->array()->as_register(), tmp_load_klass);\n+    __ movl(klass, Address(klass, Klass::layout_helper_offset()));\n+    __ testl(klass, Klass::_lh_null_free_array_bit_inplace);\n+  }\n+}\n+\n+void LIR_Assembler::emit_opSubstitutabilityCheck(LIR_OpSubstitutabilityCheck* op) {\n+  Label L_oops_equal;\n+  Label L_oops_not_equal;\n+  Label L_end;\n+\n+  Register left  = op->left()->as_register();\n+  Register right = op->right()->as_register();\n+\n+  __ cmpptr(left, right);\n+  __ jcc(Assembler::equal, L_oops_equal);\n+\n+  \/\/ (1) Null check -- if one of the operands is null, the other must not be null (because\n+  \/\/     the two references are not equal), so they are not substitutable,\n+  \/\/     FIXME: do null check only if the operand is nullable\n+  __ testptr(left, right);\n+  __ jcc(Assembler::zero, L_oops_not_equal);\n+\n+  ciKlass* left_klass = op->left_klass();\n+  ciKlass* right_klass = op->right_klass();\n+\n+  \/\/ (2) Inline type check -- if either of the operands is not a inline type,\n+  \/\/     they are not substitutable. We do this only if we are not sure that the\n+  \/\/     operands are inline type\n+  if ((left_klass == nullptr || right_klass == nullptr) ||\/\/ The klass is still unloaded, or came from a Phi node.\n+      !left_klass->is_inlinetype() || !right_klass->is_inlinetype()) {\n+    Register tmp1  = op->tmp1()->as_register();\n+    __ movptr(tmp1, (intptr_t)markWord::inline_type_pattern);\n+    __ andptr(tmp1, Address(left, oopDesc::mark_offset_in_bytes()));\n+    __ andptr(tmp1, Address(right, oopDesc::mark_offset_in_bytes()));\n+    __ cmpptr(tmp1, (intptr_t)markWord::inline_type_pattern);\n+    __ jcc(Assembler::notEqual, L_oops_not_equal);\n+  }\n+\n+  \/\/ (3) Same klass check: if the operands are of different klasses, they are not substitutable.\n+  if (left_klass != nullptr && left_klass->is_inlinetype() && left_klass == right_klass) {\n+    \/\/ No need to load klass -- the operands are statically known to be the same inline klass.\n+    __ jmp(*op->stub()->entry());\n+  } else {\n+    Register left_klass_op = op->left_klass_op()->as_register();\n+    Register right_klass_op = op->right_klass_op()->as_register();\n+\n+    if (UseCompressedClassPointers) {\n+      __ movl(left_klass_op,  Address(left,  oopDesc::klass_offset_in_bytes()));\n+      __ movl(right_klass_op, Address(right, oopDesc::klass_offset_in_bytes()));\n+      __ cmpl(left_klass_op, right_klass_op);\n+    } else {\n+      __ movptr(left_klass_op,  Address(left,  oopDesc::klass_offset_in_bytes()));\n+      __ movptr(right_klass_op, Address(right, oopDesc::klass_offset_in_bytes()));\n+      __ cmpptr(left_klass_op, right_klass_op);\n+    }\n+\n+    __ jcc(Assembler::equal, *op->stub()->entry()); \/\/ same klass -> do slow check\n+    \/\/ fall through to L_oops_not_equal\n+  }\n+\n+  __ bind(L_oops_not_equal);\n+  move(op->not_equal_result(), op->result_opr());\n+  __ jmp(L_end);\n+\n+  __ bind(L_oops_equal);\n+  move(op->equal_result(), op->result_opr());\n+  __ jmp(L_end);\n+\n+  \/\/ We've returned from the stub. RAX contains 0x0 IFF the two\n+  \/\/ operands are not substitutable. (Don't compare against 0x1 in case the\n+  \/\/ C compiler is naughty)\n+  __ bind(*op->stub()->continuation());\n+  __ cmpl(rax, 0);\n+  __ jcc(Assembler::equal, L_oops_not_equal); \/\/ (call_stub() == 0x0) -> not_equal\n+  move(op->equal_result(), op->result_opr()); \/\/ (call_stub() != 0x0) -> equal\n+  \/\/ fall-through\n+  __ bind(L_end);\n+}\n@@ -1990,0 +2153,15 @@\n+void LIR_Assembler::move(LIR_Opr src, LIR_Opr dst) {\n+  assert(dst->is_cpu_register(), \"must be\");\n+  assert(dst->type() == src->type(), \"must be\");\n+\n+  if (src->is_cpu_register()) {\n+    reg2reg(src, dst);\n+  } else if (src->is_stack()) {\n+    stack2reg(src, dst, dst->type());\n+  } else if (src->is_constant()) {\n+    const2reg(src, dst, lir_patch_none, nullptr);\n+  } else {\n+    ShouldNotReachHere();\n+  }\n+}\n+\n@@ -2867,1 +3045,1 @@\n-  add_call_info(code_offset(), op->info());\n+  add_call_info(code_offset(), op->info(), op->maybe_return_as_fields());\n@@ -2874,1 +3052,1 @@\n-  add_call_info(code_offset(), op->info());\n+  add_call_info(code_offset(), op->info(), op->maybe_return_as_fields());\n@@ -3055,0 +3233,26 @@\n+void LIR_Assembler::arraycopy_inlinetype_check(Register obj, Register tmp, CodeStub* slow_path, bool is_dest, bool null_check) {\n+  if (null_check) {\n+    __ testptr(obj, obj);\n+    __ jcc(Assembler::zero, *slow_path->entry());\n+  }\n+  if (UseArrayMarkWordCheck) {\n+    if (is_dest) {\n+      __ test_null_free_array_oop(obj, tmp, *slow_path->entry());\n+    } else {\n+      __ test_flat_array_oop(obj, tmp, *slow_path->entry());\n+    }\n+  } else {\n+    Register tmp_load_klass = LP64_ONLY(rscratch1) NOT_LP64(noreg);\n+    __ load_klass(tmp, obj, tmp_load_klass);\n+    __ movl(tmp, Address(tmp, Klass::layout_helper_offset()));\n+    if (is_dest) {\n+      \/\/ Take the slow path if it's a null_free destination array, in case the source array contains nullptrs.\n+      __ testl(tmp, Klass::_lh_null_free_array_bit_inplace);\n+    } else {\n+      __ testl(tmp, Klass::_lh_array_tag_flat_value_bit_inplace);\n+    }\n+    __ jcc(Assembler::notZero, *slow_path->entry());\n+  }\n+}\n+\n+\n@@ -3073,0 +3277,6 @@\n+  if (flags & LIR_OpArrayCopy::always_slow_path) {\n+    __ jmp(*stub->entry());\n+    __ bind(*stub->continuation());\n+    return;\n+  }\n+\n@@ -3166,0 +3376,8 @@\n+  \/\/ Handle inline type arrays\n+  if (flags & LIR_OpArrayCopy::src_inlinetype_check) {\n+    arraycopy_inlinetype_check(src, tmp, stub, false, (flags & LIR_OpArrayCopy::src_null_check));\n+  }\n+  if (flags & LIR_OpArrayCopy::dst_inlinetype_check) {\n+    arraycopy_inlinetype_check(dst, tmp, stub, true, (flags & LIR_OpArrayCopy::dst_null_check));\n+  }\n+\n@@ -3767,0 +3985,20 @@\n+void LIR_Assembler::emit_profile_inline_type(LIR_OpProfileInlineType* op) {\n+  Register obj = op->obj()->as_register();\n+  Register tmp = op->tmp()->as_pointer_register();\n+  Address mdo_addr = as_Address(op->mdp()->as_address_ptr());\n+  bool not_null = op->not_null();\n+  int flag = op->flag();\n+\n+  Label not_inline_type;\n+  if (!not_null) {\n+    __ testptr(obj, obj);\n+    __ jccb(Assembler::zero, not_inline_type);\n+  }\n+\n+  __ test_oop_is_not_inline_type(obj, tmp, not_inline_type);\n+\n+  __ orb(mdo_addr, flag);\n+\n+  __ bind(not_inline_type);\n+}\n+\n@@ -4030,0 +4268,3 @@\n+void LIR_Assembler::check_orig_pc() {\n+  __ cmpptr(frame_map()->address_for_orig_pc_addr(), NULL_WORD);\n+}\n","filename":"src\/hotspot\/cpu\/x86\/c1_LIRAssembler_x86.cpp","additions":270,"deletions":29,"binary":false,"changes":299,"status":"modified"},{"patch":"@@ -32,0 +32,1 @@\n+#include \"oops\/constMethodFlags.hpp\"\n@@ -35,0 +36,1 @@\n+#include \"oops\/inlineKlass.hpp\"\n@@ -155,1 +157,1 @@\n-        assert(ReturnTypeEntry::static_cell_count() < TypeStackSlotEntries::per_arg_count(), \"can't move past ret type\");\n+        assert(SingleTypeEntry::static_cell_count() < TypeStackSlotEntries::per_arg_count(), \"can't move past ret type\");\n@@ -200,1 +202,1 @@\n-    Address mdo_ret_addr(mdp, -in_bytes(ReturnTypeEntry::size()));\n+    Address mdo_ret_addr(mdp, -in_bytes(SingleTypeEntry::size()));\n@@ -560,1 +562,2 @@\n-                                                  Label& ok_is_subtype) {\n+                                                  Label& ok_is_subtype,\n+                                                  bool profile) {\n@@ -568,1 +571,3 @@\n-  profile_typecheck(rcx, Rsub_klass, rdi); \/\/ blows rcx, reloads rdi\n+  if (profile) {\n+    profile_typecheck(rcx, Rsub_klass, rdi); \/\/ blows rcx, reloads rdi\n+  }\n@@ -1020,1 +1025,1 @@\n- \/\/ get method access flags\n+  \/\/ get method access flags\n@@ -1144,4 +1149,2 @@\n-  \/\/ remove activation\n-  \/\/ get sender sp\n-  movptr(rbx,\n-         Address(rbp, frame::interpreter_frame_sender_sp_offset * wordSize));\n+    movptr(rbx,\n+               Address(rbp, frame::interpreter_frame_sender_sp_offset * wordSize));\n@@ -1171,0 +1174,40 @@\n+\n+  \/\/ remove activation\n+  \/\/ get sender sp\n+  movptr(rbx,\n+         Address(rbp, frame::interpreter_frame_sender_sp_offset * wordSize));\n+\n+  if (state == atos && InlineTypeReturnedAsFields) {\n+    \/\/ Check if we are returning an non-null inline type and load its fields into registers\n+    Label skip;\n+    test_oop_is_not_inline_type(rax, rscratch1, skip);\n+\n+#ifndef _LP64\n+    super_call_VM_leaf(StubRoutines::load_inline_type_fields_in_regs());\n+#else\n+    \/\/ Load fields from a buffered value with an inline class specific handler\n+    load_klass(rdi, rax, rscratch1);\n+    movptr(rdi, Address(rdi, InstanceKlass::adr_inlineklass_fixed_block_offset()));\n+    movptr(rdi, Address(rdi, InlineKlass::unpack_handler_offset()));\n+    \/\/ Unpack handler can be null if inline type is not scalarizable in returns\n+    testptr(rdi, rdi);\n+    jcc(Assembler::zero, skip);\n+    call(rdi);\n+#endif\n+#ifdef ASSERT\n+    \/\/ TODO 8284443 Enable\n+    if (StressCallingConvention && false) {\n+      Label skip_stress;\n+      movptr(rscratch1, Address(rbp, frame::interpreter_frame_method_offset * wordSize));\n+      movl(rscratch1, Address(rscratch1, Method::flags_offset()));\n+      testl(rcx, ConstMethodFlags::has_scalarized_return_flag());\n+      jcc(Assembler::zero, skip_stress);\n+      load_klass(rax, rax, rscratch1);\n+      orptr(rax, 1);\n+      bind(skip_stress);\n+    }\n+#endif\n+    \/\/ call above kills the value in rbx. Reload it.\n+    movptr(rbx, Address(rbp, frame::interpreter_frame_sender_sp_offset * wordSize));\n+    bind(skip);\n+  }\n@@ -1191,0 +1234,106 @@\n+void InterpreterMacroAssembler::allocate_instance(Register klass, Register new_obj,\n+                                                  Register t1, Register t2,\n+                                                  bool clear_fields, Label& alloc_failed) {\n+  MacroAssembler::allocate_instance(klass, new_obj, t1, t2, clear_fields, alloc_failed);\n+  {\n+    SkipIfEqual skip_if(this, &DTraceAllocProbes, 0, rscratch1);\n+    \/\/ Trigger dtrace event for fastpath\n+    push(atos);\n+    call_VM_leaf(CAST_FROM_FN_PTR(address, static_cast<int (*)(oopDesc*)>(SharedRuntime::dtrace_object_alloc)), new_obj);\n+    pop(atos);\n+  }\n+}\n+\n+\n+void InterpreterMacroAssembler::read_flat_field(Register holder_klass,\n+                                                Register field_index, Register field_offset,\n+                                                Register obj) {\n+  Label alloc_failed, empty_value, done;\n+  const Register src = field_offset;\n+  const Register alloc_temp = LP64_ONLY(rscratch1) NOT_LP64(rsi);\n+  const Register dst_temp   = LP64_ONLY(rscratch2) NOT_LP64(rdi);\n+  assert_different_registers(obj, holder_klass, field_index, field_offset, dst_temp);\n+\n+  \/\/ Grap the inline field klass\n+  push(holder_klass);\n+  const Register field_klass = holder_klass;\n+  get_inline_type_field_klass(holder_klass, field_index, field_klass);\n+\n+  \/\/check for empty value klass\n+  test_klass_is_empty_inline_type(field_klass, dst_temp, empty_value);\n+\n+  \/\/ allocate buffer\n+  push(obj); \/\/ save holder\n+  allocate_instance(field_klass, obj, alloc_temp, dst_temp, false, alloc_failed);\n+\n+  \/\/ Have an oop instance buffer, copy into it\n+  data_for_oop(obj, dst_temp, field_klass);\n+  pop(alloc_temp);             \/\/ restore holder\n+  lea(src, Address(alloc_temp, field_offset));\n+  \/\/ call_VM_leaf, clobbers a few regs, save restore new obj\n+  push(obj);\n+  access_value_copy(IS_DEST_UNINITIALIZED, src, dst_temp, field_klass);\n+  pop(obj);\n+  pop(holder_klass);\n+  jmp(done);\n+\n+  bind(empty_value);\n+  get_empty_inline_type_oop(field_klass, dst_temp, obj);\n+  pop(holder_klass);\n+  jmp(done);\n+\n+  bind(alloc_failed);\n+  pop(obj);\n+  pop(holder_klass);\n+  call_VM(obj, CAST_FROM_FN_PTR(address, InterpreterRuntime::read_flat_field),\n+          obj, field_index, holder_klass);\n+\n+  bind(done);\n+}\n+\n+void InterpreterMacroAssembler::read_flat_element(Register array, Register index,\n+                                                  Register t1, Register t2,\n+                                                  Register obj) {\n+  assert_different_registers(array, index, t1, t2);\n+  Label alloc_failed, empty_value, done;\n+  const Register array_klass = t2;\n+  const Register elem_klass = t1;\n+  const Register alloc_temp = LP64_ONLY(rscratch1) NOT_LP64(rsi);\n+  const Register dst_temp   = LP64_ONLY(rscratch2) NOT_LP64(rdi);\n+\n+  \/\/ load in array->klass()->element_klass()\n+  Register tmp_load_klass = LP64_ONLY(rscratch1) NOT_LP64(noreg);\n+  load_klass(array_klass, array, tmp_load_klass);\n+  movptr(elem_klass, Address(array_klass, ArrayKlass::element_klass_offset()));\n+\n+  \/\/check for empty value klass\n+  test_klass_is_empty_inline_type(elem_klass, dst_temp, empty_value);\n+\n+  \/\/ calc source into \"array_klass\" and free up some regs\n+  const Register src = array_klass;\n+  push(index); \/\/ preserve index reg in case alloc_failed\n+  data_for_value_array_index(array, array_klass, index, src);\n+\n+  allocate_instance(elem_klass, obj, alloc_temp, dst_temp, false, alloc_failed);\n+  \/\/ Have an oop instance buffer, copy into it\n+  store_ptr(0, obj); \/\/ preserve obj (overwrite index, no longer needed)\n+  data_for_oop(obj, dst_temp, elem_klass);\n+  access_value_copy(IS_DEST_UNINITIALIZED, src, dst_temp, elem_klass);\n+  pop(obj);\n+  jmp(done);\n+\n+  bind(empty_value);\n+  get_empty_inline_type_oop(elem_klass, dst_temp, obj);\n+  jmp(done);\n+\n+  bind(alloc_failed);\n+  pop(index);\n+  if (array == c_rarg2) {\n+    mov(elem_klass, array);\n+    array = elem_klass;\n+  }\n+  call_VM(obj, CAST_FROM_FN_PTR(address, InterpreterRuntime::value_array_load), array, index);\n+\n+  bind(done);\n+}\n+\n@@ -1246,0 +1395,4 @@\n+      if (EnableValhalla) {\n+        \/\/ Mask inline_type bit such that we go to the slow path if object is an inline type\n+        andptr(swap_reg, ~((int) markWord::inline_type_bit_in_place));\n+      }\n@@ -1618,1 +1771,1 @@\n-void InterpreterMacroAssembler::profile_not_taken_branch(Register mdp) {\n+void InterpreterMacroAssembler::profile_not_taken_branch(Register mdp, bool acmp) {\n@@ -1630,1 +1783,1 @@\n-    update_mdp_by_constant(mdp, in_bytes(BranchData::branch_data_size()));\n+    update_mdp_by_constant(mdp, acmp ? in_bytes(ACmpData::acmp_data_size()): in_bytes(BranchData::branch_data_size()));\n@@ -1965,0 +2118,78 @@\n+void InterpreterMacroAssembler::profile_array(Register mdp,\n+                                              Register array,\n+                                              Register tmp) {\n+  if (ProfileInterpreter) {\n+    Label profile_continue;\n+\n+    \/\/ If no method data exists, go to profile_continue.\n+    test_method_data_pointer(mdp, profile_continue);\n+\n+    mov(tmp, array);\n+    profile_obj_type(tmp, Address(mdp, in_bytes(ArrayLoadStoreData::array_offset())));\n+\n+    Label not_flat;\n+    test_non_flat_array_oop(array, tmp, not_flat);\n+\n+    set_mdp_flag_at(mdp, ArrayLoadStoreData::flat_array_byte_constant());\n+\n+    bind(not_flat);\n+\n+    Label not_null_free;\n+    test_non_null_free_array_oop(array, tmp, not_null_free);\n+\n+    set_mdp_flag_at(mdp, ArrayLoadStoreData::null_free_array_byte_constant());\n+\n+    bind(not_null_free);\n+\n+    bind(profile_continue);\n+  }\n+}\n+\n+void InterpreterMacroAssembler::profile_element(Register mdp,\n+                                                Register element,\n+                                                Register tmp) {\n+  if (ProfileInterpreter) {\n+    Label profile_continue;\n+\n+    \/\/ If no method data exists, go to profile_continue.\n+    test_method_data_pointer(mdp, profile_continue);\n+\n+    mov(tmp, element);\n+    profile_obj_type(tmp, Address(mdp, in_bytes(ArrayLoadStoreData::element_offset())));\n+\n+    \/\/ The method data pointer needs to be updated.\n+    update_mdp_by_constant(mdp, in_bytes(ArrayLoadStoreData::array_load_store_data_size()));\n+\n+    bind(profile_continue);\n+  }\n+}\n+\n+void InterpreterMacroAssembler::profile_acmp(Register mdp,\n+                                             Register left,\n+                                             Register right,\n+                                             Register tmp) {\n+  if (ProfileInterpreter) {\n+    Label profile_continue;\n+\n+    \/\/ If no method data exists, go to profile_continue.\n+    test_method_data_pointer(mdp, profile_continue);\n+\n+    mov(tmp, left);\n+    profile_obj_type(tmp, Address(mdp, in_bytes(ACmpData::left_offset())));\n+\n+    Label left_not_inline_type;\n+    test_oop_is_not_inline_type(left, tmp, left_not_inline_type);\n+    set_mdp_flag_at(mdp, ACmpData::left_inline_type_byte_constant());\n+    bind(left_not_inline_type);\n+\n+    mov(tmp, right);\n+    profile_obj_type(tmp, Address(mdp, in_bytes(ACmpData::right_offset())));\n+\n+    Label right_not_inline_type;\n+    test_oop_is_not_inline_type(right, tmp, right_not_inline_type);\n+    set_mdp_flag_at(mdp, ACmpData::right_inline_type_byte_constant());\n+    bind(right_not_inline_type);\n+\n+    bind(profile_continue);\n+  }\n+}\n","filename":"src\/hotspot\/cpu\/x86\/interp_masm_x86.cpp","additions":242,"deletions":11,"binary":false,"changes":253,"status":"modified"},{"patch":"@@ -192,1 +192,1 @@\n-  void gen_subtype_check( Register sub_klass, Label &ok_is_subtype );\n+  void gen_subtype_check(Register sub_klass, Label &ok_is_subtype, bool profile = true);\n@@ -232,0 +232,23 @@\n+  \/\/ Kills t1 and t2, perserves klass, return allocation in new_obj\n+  void allocate_instance(Register klass, Register new_obj,\n+                         Register t1, Register t2,\n+                         bool clear_fields, Label& alloc_failed);\n+  \/\/ Allocate instance in \"obj\" and read in the content of the inline field\n+  \/\/ NOTES:\n+  \/\/   - input holder object via \"obj\", which must be rax,\n+  \/\/     will return new instance via the same reg\n+  \/\/   - assumes holder_klass and valueKlass field klass have both been resolved\n+  \/\/   - 32 bits: kills rdi and rsi\n+  void read_flat_field(Register holder_klass,\n+                       Register field_index, Register field_offset,\n+                       Register obj = rax);\n+\n+  \/\/ Allocate value buffer in \"obj\" and read in flat element at the given index\n+  \/\/ NOTES:\n+  \/\/   - Return via \"obj\" must be rax\n+  \/\/   - kills all given regs\n+  \/\/   - 32 bits: kills rdi and rsi\n+  void read_flat_element(Register array, Register index,\n+                         Register t1, Register t2,\n+                         Register obj = rax);\n+\n@@ -270,1 +293,1 @@\n-  void profile_not_taken_branch(Register mdp);\n+  void profile_not_taken_branch(Register mdp, bool acmp = false);\n@@ -283,0 +306,3 @@\n+  void profile_array(Register mdp, Register array, Register tmp);\n+  void profile_element(Register mdp, Register element, Register tmp);\n+  void profile_acmp(Register mdp, Register left, Register right, Register tmp);\n","filename":"src\/hotspot\/cpu\/x86\/interp_masm_x86.hpp","additions":28,"deletions":2,"binary":false,"changes":30,"status":"modified"},{"patch":"@@ -26,0 +26,1 @@\n+#include \"asm\/assembler.hpp\"\n@@ -39,0 +40,2 @@\n+#include \"utilities\/macros.hpp\"\n+#include \"vmreg_x86.inline.hpp\"\n@@ -317,5 +320,9 @@\n-  \/\/ T_OBJECT, T_LONG, T_FLOAT or T_DOUBLE is treated as T_INT)\n-  __ movptr(c_rarg0, result);\n-  Label is_long, is_float, is_double, exit;\n-  __ movl(c_rarg1, result_type);\n-  __ cmpl(c_rarg1, T_OBJECT);\n+  \/\/ T_OBJECT, T_PRIMITIVE_OBJECT, T_LONG, T_FLOAT or T_DOUBLE is treated as T_INT)\n+  __ movptr(r13, result);\n+  Label is_long, is_float, is_double, check_prim, exit;\n+  __ movl(rbx, result_type);\n+  __ cmpl(rbx, T_OBJECT);\n+  __ jcc(Assembler::equal, check_prim);\n+  __ cmpl(rbx, T_PRIMITIVE_OBJECT);\n+  __ jcc(Assembler::equal, check_prim);\n+  __ cmpl(rbx, T_LONG);\n@@ -323,3 +330,1 @@\n-  __ cmpl(c_rarg1, T_LONG);\n-  __ jcc(Assembler::equal, is_long);\n-  __ cmpl(c_rarg1, T_FLOAT);\n+  __ cmpl(rbx, T_FLOAT);\n@@ -327,1 +332,1 @@\n-  __ cmpl(c_rarg1, T_DOUBLE);\n+  __ cmpl(rbx, T_DOUBLE);\n@@ -333,1 +338,1 @@\n-    __ cmpl(c_rarg1, T_INT);\n+    __ cmpl(rbx, T_INT);\n@@ -341,1 +346,1 @@\n-  __ movl(Address(c_rarg0, 0), rax);\n+  __ movl(Address(r13, 0), rax);\n@@ -405,0 +410,13 @@\n+  __ BIND(check_prim);\n+  if (InlineTypeReturnedAsFields) {\n+    \/\/ Check for scalarized return value\n+    __ testptr(rax, 1);\n+    __ jcc(Assembler::zero, is_long);\n+    \/\/ Load pack handler address\n+    __ andptr(rax, -2);\n+    __ movptr(rax, Address(rax, InstanceKlass::adr_inlineklass_fixed_block_offset()));\n+    __ movptr(rbx, Address(rax, InlineKlass::pack_handler_jobject_offset()));\n+    \/\/ Call pack handler to initialize the buffer\n+    __ call(rbx);\n+    __ jmp(exit);\n+  }\n@@ -406,1 +424,1 @@\n-  __ movq(Address(c_rarg0, 0), rax);\n+  __ movq(Address(r13, 0), rax);\n@@ -410,1 +428,1 @@\n-  __ movflt(Address(c_rarg0, 0), xmm0);\n+  __ movflt(Address(r13, 0), xmm0);\n@@ -414,1 +432,1 @@\n-  __ movdbl(Address(c_rarg0, 0), xmm0);\n+  __ movdbl(Address(r13, 0), xmm0);\n@@ -3934,0 +3952,10 @@\n+  \/\/ Generate these first because they are called from other stubs\n+  if (InlineTypeReturnedAsFields) {\n+    StubRoutines::_load_inline_type_fields_in_regs =\n+      generate_return_value_stub(CAST_FROM_FN_PTR(address, SharedRuntime::load_inline_type_fields_in_regs),\n+                                 \"load_inline_type_fields_in_regs\", false);\n+    StubRoutines::_store_inline_type_fields_to_buf =\n+      generate_return_value_stub(CAST_FROM_FN_PTR(address, SharedRuntime::store_inline_type_fields_to_buf),\n+                                 \"store_inline_type_fields_to_buf\", true);\n+  }\n+\n@@ -3997,0 +4025,144 @@\n+\/\/ Call here from the interpreter or compiled code to either load\n+\/\/ multiple returned values from the inline type instance being\n+\/\/ returned to registers or to store returned values to a newly\n+\/\/ allocated inline type instance.\n+\/\/ Register is a class, but it would be assigned numerical value.\n+\/\/ \"0\" is assigned for xmm0. Thus we need to ignore -Wnonnull.\n+PRAGMA_DIAG_PUSH\n+PRAGMA_NONNULL_IGNORED\n+address StubGenerator::generate_return_value_stub(address destination, const char* name, bool has_res) {\n+  \/\/ We need to save all registers the calling convention may use so\n+  \/\/ the runtime calls read or update those registers. This needs to\n+  \/\/ be in sync with SharedRuntime::java_return_convention().\n+  enum layout {\n+    pad_off = frame::arg_reg_save_area_bytes\/BytesPerInt, pad_off_2,\n+    rax_off, rax_off_2,\n+    j_rarg5_off, j_rarg5_2,\n+    j_rarg4_off, j_rarg4_2,\n+    j_rarg3_off, j_rarg3_2,\n+    j_rarg2_off, j_rarg2_2,\n+    j_rarg1_off, j_rarg1_2,\n+    j_rarg0_off, j_rarg0_2,\n+    j_farg0_off, j_farg0_2,\n+    j_farg1_off, j_farg1_2,\n+    j_farg2_off, j_farg2_2,\n+    j_farg3_off, j_farg3_2,\n+    j_farg4_off, j_farg4_2,\n+    j_farg5_off, j_farg5_2,\n+    j_farg6_off, j_farg6_2,\n+    j_farg7_off, j_farg7_2,\n+    rbp_off, rbp_off_2,\n+    return_off, return_off_2,\n+\n+    framesize\n+  };\n+\n+  CodeBuffer buffer(name, 1000, 512);\n+  MacroAssembler* _masm = new MacroAssembler(&buffer);\n+\n+  int frame_size_in_bytes = align_up(framesize*BytesPerInt, 16);\n+  assert(frame_size_in_bytes == framesize*BytesPerInt, \"misaligned\");\n+  int frame_size_in_slots = frame_size_in_bytes \/ BytesPerInt;\n+  int frame_size_in_words = frame_size_in_bytes \/ wordSize;\n+\n+  OopMapSet *oop_maps = new OopMapSet();\n+  OopMap* map = new OopMap(frame_size_in_slots, 0);\n+\n+  map->set_callee_saved(VMRegImpl::stack2reg(rax_off), rax->as_VMReg());\n+  map->set_callee_saved(VMRegImpl::stack2reg(j_rarg5_off), j_rarg5->as_VMReg());\n+  map->set_callee_saved(VMRegImpl::stack2reg(j_rarg4_off), j_rarg4->as_VMReg());\n+  map->set_callee_saved(VMRegImpl::stack2reg(j_rarg3_off), j_rarg3->as_VMReg());\n+  map->set_callee_saved(VMRegImpl::stack2reg(j_rarg2_off), j_rarg2->as_VMReg());\n+  map->set_callee_saved(VMRegImpl::stack2reg(j_rarg1_off), j_rarg1->as_VMReg());\n+  map->set_callee_saved(VMRegImpl::stack2reg(j_rarg0_off), j_rarg0->as_VMReg());\n+  map->set_callee_saved(VMRegImpl::stack2reg(j_farg0_off), j_farg0->as_VMReg());\n+  map->set_callee_saved(VMRegImpl::stack2reg(j_farg1_off), j_farg1->as_VMReg());\n+  map->set_callee_saved(VMRegImpl::stack2reg(j_farg2_off), j_farg2->as_VMReg());\n+  map->set_callee_saved(VMRegImpl::stack2reg(j_farg3_off), j_farg3->as_VMReg());\n+  map->set_callee_saved(VMRegImpl::stack2reg(j_farg4_off), j_farg4->as_VMReg());\n+  map->set_callee_saved(VMRegImpl::stack2reg(j_farg5_off), j_farg5->as_VMReg());\n+  map->set_callee_saved(VMRegImpl::stack2reg(j_farg6_off), j_farg6->as_VMReg());\n+  map->set_callee_saved(VMRegImpl::stack2reg(j_farg7_off), j_farg7->as_VMReg());\n+\n+  int start = __ offset();\n+\n+  __ subptr(rsp, frame_size_in_bytes - 8 \/* return address*\/);\n+\n+  __ movptr(Address(rsp, rbp_off * BytesPerInt), rbp);\n+  __ movdbl(Address(rsp, j_farg7_off * BytesPerInt), j_farg7);\n+  __ movdbl(Address(rsp, j_farg6_off * BytesPerInt), j_farg6);\n+  __ movdbl(Address(rsp, j_farg5_off * BytesPerInt), j_farg5);\n+  __ movdbl(Address(rsp, j_farg4_off * BytesPerInt), j_farg4);\n+  __ movdbl(Address(rsp, j_farg3_off * BytesPerInt), j_farg3);\n+  __ movdbl(Address(rsp, j_farg2_off * BytesPerInt), j_farg2);\n+  __ movdbl(Address(rsp, j_farg1_off * BytesPerInt), j_farg1);\n+  __ movdbl(Address(rsp, j_farg0_off * BytesPerInt), j_farg0);\n+\n+  __ movptr(Address(rsp, j_rarg0_off * BytesPerInt), j_rarg0);\n+  __ movptr(Address(rsp, j_rarg1_off * BytesPerInt), j_rarg1);\n+  __ movptr(Address(rsp, j_rarg2_off * BytesPerInt), j_rarg2);\n+  __ movptr(Address(rsp, j_rarg3_off * BytesPerInt), j_rarg3);\n+  __ movptr(Address(rsp, j_rarg4_off * BytesPerInt), j_rarg4);\n+  __ movptr(Address(rsp, j_rarg5_off * BytesPerInt), j_rarg5);\n+  __ movptr(Address(rsp, rax_off * BytesPerInt), rax);\n+\n+  int frame_complete = __ offset();\n+\n+  __ set_last_Java_frame(noreg, noreg, nullptr, rscratch1);\n+\n+  __ mov(c_rarg0, r15_thread);\n+  __ mov(c_rarg1, rax);\n+\n+  __ call(RuntimeAddress(destination));\n+\n+  \/\/ Set an oopmap for the call site.\n+\n+  oop_maps->add_gc_map( __ offset() - start, map);\n+\n+  \/\/ clear last_Java_sp\n+  __ reset_last_Java_frame(false);\n+\n+  __ movptr(rbp, Address(rsp, rbp_off * BytesPerInt));\n+  __ movdbl(j_farg7, Address(rsp, j_farg7_off * BytesPerInt));\n+  __ movdbl(j_farg6, Address(rsp, j_farg6_off * BytesPerInt));\n+  __ movdbl(j_farg5, Address(rsp, j_farg5_off * BytesPerInt));\n+  __ movdbl(j_farg4, Address(rsp, j_farg4_off * BytesPerInt));\n+  __ movdbl(j_farg3, Address(rsp, j_farg3_off * BytesPerInt));\n+  __ movdbl(j_farg2, Address(rsp, j_farg2_off * BytesPerInt));\n+  __ movdbl(j_farg1, Address(rsp, j_farg1_off * BytesPerInt));\n+  __ movdbl(j_farg0, Address(rsp, j_farg0_off * BytesPerInt));\n+\n+  __ movptr(j_rarg0, Address(rsp, j_rarg0_off * BytesPerInt));\n+  __ movptr(j_rarg1, Address(rsp, j_rarg1_off * BytesPerInt));\n+  __ movptr(j_rarg2, Address(rsp, j_rarg2_off * BytesPerInt));\n+  __ movptr(j_rarg3, Address(rsp, j_rarg3_off * BytesPerInt));\n+  __ movptr(j_rarg4, Address(rsp, j_rarg4_off * BytesPerInt));\n+  __ movptr(j_rarg5, Address(rsp, j_rarg5_off * BytesPerInt));\n+  __ movptr(rax, Address(rsp, rax_off * BytesPerInt));\n+\n+  __ addptr(rsp, frame_size_in_bytes-8);\n+\n+  \/\/ check for pending exceptions\n+  Label pending;\n+  __ cmpptr(Address(r15_thread, Thread::pending_exception_offset()), (int32_t)NULL_WORD);\n+  __ jcc(Assembler::notEqual, pending);\n+\n+  if (has_res) {\n+    __ get_vm_result(rax, r15_thread);\n+  }\n+\n+  __ ret(0);\n+\n+  __ bind(pending);\n+\n+  __ movptr(rax, Address(r15_thread, Thread::pending_exception_offset()));\n+  __ jump(RuntimeAddress(StubRoutines::forward_exception_entry()));\n+\n+  \/\/ -------------\n+  \/\/ make sure all code is generated\n+  _masm->flush();\n+\n+  RuntimeStub* stub = RuntimeStub::new_runtime_stub(name, &buffer, frame_complete, frame_size_in_words, oop_maps, false);\n+  return stub->entry_point();\n+}\n+\n","filename":"src\/hotspot\/cpu\/x86\/stubGenerator_x86_64.cpp","additions":186,"deletions":14,"binary":false,"changes":200,"status":"modified"},{"patch":"@@ -42,0 +42,1 @@\n+#include \"oops\/inlineKlass.hpp\"\n@@ -67,1 +68,1 @@\n-int TemplateInterpreter::InterpreterCodeSize = JVMCI_ONLY(268) NOT_JVMCI(256) * 1024;\n+int TemplateInterpreter::InterpreterCodeSize = JVMCI_ONLY(280) NOT_JVMCI(268) * 1024;\n@@ -211,2 +212,2 @@\n-  __ movptr(rcx, Address(rbp, frame::interpreter_frame_last_sp_offset * wordSize));\n-  __ lea(rsp, Address(rbp, rcx, Address::times_ptr));\n+  __ movptr(rscratch1, Address(rbp, frame::interpreter_frame_last_sp_offset * wordSize));\n+  __ lea(rsp, Address(rbp, rscratch1, Address::times_ptr));\n@@ -216,0 +217,4 @@\n+  if (state == atos && InlineTypeReturnedAsFields) {\n+    __ store_inline_type_fields_to_buf(NULL);\n+  }\n+\n@@ -363,0 +368,1 @@\n+  case T_PRIMITIVE_OBJECT: \/\/ fall through (inline types are handled with oops)\n","filename":"src\/hotspot\/cpu\/x86\/templateInterpreterGenerator_x86.cpp","additions":9,"deletions":3,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -40,0 +40,1 @@\n+#include \"oops\/inlineKlass.hpp\"\n@@ -185,0 +186,1 @@\n+  case Bytecodes::_fast_qputfield:\n@@ -383,0 +385,1 @@\n+  __ andl(rdx, ~JVM_CONSTANT_QDescBit);\n@@ -832,9 +835,27 @@\n-  \/\/ rax: index\n-  \/\/ rdx: array\n-  index_check(rdx, rax); \/\/ kills rbx\n-  do_oop_load(_masm,\n-              Address(rdx, rax,\n-                      UseCompressedOops ? Address::times_4 : Address::times_ptr,\n-                      arrayOopDesc::base_offset_in_bytes(T_OBJECT)),\n-              rax,\n-              IS_ARRAY);\n+  Register array = rdx;\n+  Register index = rax;\n+\n+  index_check(array, index); \/\/ kills rbx\n+  __ profile_array(rbx, array, rcx);\n+  if (UseFlatArray) {\n+    Label is_flat_array, done;\n+    __ test_flat_array_oop(array, rbx, is_flat_array);\n+    do_oop_load(_masm,\n+                Address(array, index,\n+                        UseCompressedOops ? Address::times_4 : Address::times_ptr,\n+                        arrayOopDesc::base_offset_in_bytes(T_OBJECT)),\n+                rax,\n+                IS_ARRAY);\n+    __ jmp(done);\n+    __ bind(is_flat_array);\n+    __ read_flat_element(array, index, rbx, rcx, rax);\n+    __ bind(done);\n+  } else {\n+    do_oop_load(_masm,\n+                Address(array, index,\n+                        UseCompressedOops ? Address::times_4 : Address::times_ptr,\n+                        arrayOopDesc::base_offset_in_bytes(T_OBJECT)),\n+                rax,\n+                IS_ARRAY);\n+  }\n+  __ profile_element(rbx, rax, rcx);\n@@ -1126,1 +1147,1 @@\n-  Label is_null, ok_is_subtype, done;\n+  Label is_null, is_flat_array, ok_is_subtype, done;\n@@ -1138,0 +1159,4 @@\n+\n+  __ profile_array(rdi, rdx, rbx);\n+  __ profile_element(rdi, rax, rbx);\n+\n@@ -1141,0 +1166,7 @@\n+  \/\/ Move array class to rdi\n+  __ load_klass(rdi, rdx, rscratch1);\n+  if (UseFlatArray) {\n+    __ movl(rbx, Address(rdi, Klass::layout_helper_offset()));\n+    __ test_flat_array_layout(rbx, is_flat_array);\n+  }\n+\n@@ -1143,3 +1175,2 @@\n-  \/\/ Move superklass into rax\n-  __ load_klass(rax, rdx, rscratch1);\n-  __ movptr(rax, Address(rax,\n+  \/\/ Move array element superklass into rax\n+  __ movptr(rax, Address(rdi,\n@@ -1150,1 +1181,2 @@\n-  __ gen_subtype_check(rbx, ok_is_subtype);\n+  \/\/ is \"rbx <: rax\" ? (value subclass <: array element superclass)\n+  __ gen_subtype_check(rbx, ok_is_subtype, false);\n@@ -1168,1 +1200,9 @@\n-  __ profile_null_seen(rbx);\n+  if (EnablePrimitiveClasses) {\n+    Label is_null_into_value_array_npe, store_null;\n+\n+    \/\/ No way to store null in null-free array\n+    __ test_null_free_array_oop(rdx, rbx, is_null_into_value_array_npe);\n+    __ jmp(store_null);\n+\n+    __ bind(is_null_into_value_array_npe);\n+    __ jump(ExternalAddress(Interpreter::_throw_NullPointerException_entry));\n@@ -1170,0 +1210,2 @@\n+    __ bind(store_null);\n+  }\n@@ -1172,0 +1214,28 @@\n+  __ jmp(done);\n+\n+  if (UseFlatArray) {\n+    Label is_type_ok;\n+    __ bind(is_flat_array); \/\/ Store non-null value to flat\n+\n+    \/\/ Simplistic type check...\n+\n+    \/\/ Profile the not-null value's klass.\n+    __ load_klass(rbx, rax, rscratch1);\n+    \/\/ Move element klass into rax\n+    __ movptr(rax, Address(rdi, ArrayKlass::element_klass_offset()));\n+    \/\/ flat value array needs exact type match\n+    \/\/ is \"rax == rbx\" (value subclass == array element superclass)\n+    __ cmpptr(rax, rbx);\n+    __ jccb(Assembler::equal, is_type_ok);\n+\n+    __ jump(ExternalAddress(Interpreter::_throw_ArrayStoreException_entry));\n+\n+    __ bind(is_type_ok);\n+    \/\/ rbx: value's klass\n+    \/\/ rdx: array\n+    \/\/ rdi: array klass\n+    __ test_klass_is_empty_inline_type(rbx, rax, done);\n+\n+    \/\/ calc dst for copy\n+    __ movl(rax, at_tos_p1()); \/\/ index\n+    __ data_for_value_array_index(rdx, rdi, rax, rax);\n@@ -1173,0 +1243,6 @@\n+    \/\/ ...and src for copy\n+    __ movptr(rcx, at_tos());  \/\/ value\n+    __ data_for_oop(rcx, rcx, rbx);\n+\n+    __ access_value_copy(IN_HEAP, rcx, rax, rbx);\n+  }\n@@ -2341,1 +2417,1 @@\n-  Label not_taken;\n+  Label taken, not_taken;\n@@ -2343,0 +2419,36 @@\n+\n+  __ profile_acmp(rbx, rdx, rax, rcx);\n+\n+  const int is_inline_type_mask = markWord::inline_type_pattern;\n+  if (EnableValhalla) {\n+    __ cmpoop(rdx, rax);\n+    __ jcc(Assembler::equal, (cc == equal) ? taken : not_taken);\n+\n+    \/\/ might be substitutable, test if either rax or rdx is null\n+    __ testptr(rax, rax);\n+    __ jcc(Assembler::zero, (cc == equal) ? not_taken : taken);\n+    __ testptr(rdx, rdx);\n+    __ jcc(Assembler::zero, (cc == equal) ? not_taken : taken);\n+\n+    \/\/ and both are values ?\n+    __ movptr(rbx, Address(rdx, oopDesc::mark_offset_in_bytes()));\n+    __ andptr(rbx, Address(rax, oopDesc::mark_offset_in_bytes()));\n+    __ andptr(rbx, is_inline_type_mask);\n+    __ cmpptr(rbx, is_inline_type_mask);\n+    __ jcc(Assembler::notEqual, (cc == equal) ? not_taken : taken);\n+\n+    \/\/ same value klass ?\n+    __ load_metadata(rbx, rdx);\n+    __ load_metadata(rcx, rax);\n+    __ cmpptr(rbx, rcx);\n+    __ jcc(Assembler::notEqual, (cc == equal) ? not_taken : taken);\n+\n+    \/\/ Know both are the same type, let's test for substitutability...\n+    if (cc == equal) {\n+      invoke_is_substitutable(rax, rdx, taken, not_taken);\n+    } else {\n+      invoke_is_substitutable(rax, rdx, not_taken, taken);\n+    }\n+    __ stop(\"Not reachable\");\n+  }\n+\n@@ -2345,0 +2457,1 @@\n+  __ bind(taken);\n@@ -2347,1 +2460,10 @@\n-  __ profile_not_taken_branch(rax);\n+  __ profile_not_taken_branch(rax, true);\n+}\n+\n+void TemplateTable::invoke_is_substitutable(Register aobj, Register bobj,\n+                                            Label& is_subst, Label& not_subst) {\n+  __ call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::is_substitutable), aobj, bobj);\n+  \/\/ Restored...rax answer, jmp to outcome...\n+  __ testl(rax, rax);\n+  __ jcc(Assembler::zero, not_subst);\n+  __ jmp(is_subst);\n@@ -2617,1 +2739,2 @@\n-  __ remove_activation(state, rbcp);\n+\n+  __ remove_activation(state, rbcp, true, true, true);\n@@ -2933,1 +3056,1 @@\n-  const Register obj   = LP64_ONLY(c_rarg3) NOT_LP64(rcx);\n+  const Register obj   = LP64_ONLY(r9) NOT_LP64(rcx);\n@@ -2945,2 +3068,0 @@\n-  if (!is_static) pop_and_check_object(obj);\n-\n@@ -2949,1 +3070,1 @@\n-  Label Done, notByte, notBool, notInt, notShort, notChar, notLong, notFloat, notObj;\n+  Label Done, notByte, notBool, notInt, notShort, notChar, notLong, notFloat, notObj, notInlineType;\n@@ -2957,0 +3078,1 @@\n+  if (!is_static) pop_and_check_object(obj);\n@@ -2968,1 +3090,1 @@\n-\n+   if (!is_static) pop_and_check_object(obj);\n@@ -2983,4 +3105,80 @@\n-  do_oop_load(_masm, field, rax);\n-  __ push(atos);\n-  if (!is_static && rc == may_rewrite) {\n-    patch_bytecode(Bytecodes::_fast_agetfield, bc, rbx);\n+  if (!EnablePrimitiveClasses) {\n+    if (!is_static) pop_and_check_object(obj);\n+    do_oop_load(_masm, field, rax);\n+    __ push(atos);\n+    if (!is_static && rc == may_rewrite) {\n+      patch_bytecode(Bytecodes::_fast_agetfield, bc, rbx);\n+    }\n+    __ jmp(Done);\n+  } else {\n+    if (is_static) {\n+      __ load_heap_oop(rax, field);\n+      Label is_null_free_inline_type, uninitialized;\n+      \/\/ Issue below if the static field has not been initialized yet\n+      __ test_field_is_null_free_inline_type(flags, rscratch1, is_null_free_inline_type);\n+        \/\/ field is not a null free inline type\n+        __ push(atos);\n+        __ jmp(Done);\n+      \/\/ field is a null free inline type, must not return null even if uninitialized\n+      __ bind(is_null_free_inline_type);\n+          __ testptr(rax, rax);\n+        __ jcc(Assembler::zero, uninitialized);\n+          __ push(atos);\n+          __ jmp(Done);\n+        __ bind(uninitialized);\n+#ifdef _LP64\n+          Label slow_case, finish;\n+          __ movptr(rbx, Address(obj, java_lang_Class::klass_offset()));\n+          __ cmpb(Address(rbx, InstanceKlass::init_state_offset()), InstanceKlass::fully_initialized);\n+          __ jcc(Assembler::notEqual, slow_case);\n+        __ get_default_value_oop(rbx, rscratch1, rax);\n+        __ jmp(finish);\n+        __ bind(slow_case);\n+#endif \/\/ LP64\n+          __ call_VM(rax, CAST_FROM_FN_PTR(address, InterpreterRuntime::uninitialized_static_inline_type_field),\n+                obj, cache);\n+#ifdef _LP64\n+          __ bind(finish);\n+  #endif \/\/ _LP64\n+        __ verify_oop(rax);\n+        __ push(atos);\n+        __ jmp(Done);\n+    } else {\n+      Label is_flat, nonnull, is_inline_type, rewrite_inline;\n+      __ test_field_is_null_free_inline_type(flags, rscratch1, is_inline_type);\n+      \/\/ field is not a null free inline type\n+      pop_and_check_object(obj);\n+      __ load_heap_oop(rax, field);\n+      __ push(atos);\n+      if (rc == may_rewrite) {\n+        patch_bytecode(Bytecodes::_fast_agetfield, bc, rbx);\n+      }\n+      __ jmp(Done);\n+      __ bind(is_inline_type);\n+      __ test_field_is_flat(flags, rscratch1, is_flat);\n+          \/\/ field is not flat\n+          pop_and_check_object(obj);\n+          __ load_heap_oop(rax, field);\n+          __ testptr(rax, rax);\n+          __ jcc(Assembler::notZero, nonnull);\n+            __ load_unsigned_short(flags, Address(cache, in_bytes(ResolvedFieldEntry::field_index_offset())));\n+            __ movptr(rcx, Address(cache, ResolvedFieldEntry::field_holder_offset()));\n+            __ get_inline_type_field_klass(rcx, flags, rbx);\n+            __ get_default_value_oop(rbx, rcx, rax);\n+          __ bind(nonnull);\n+          __ verify_oop(rax);\n+          __ push(atos);\n+          __ jmp(rewrite_inline);\n+        __ bind(is_flat);\n+          pop_and_check_object(rax);\n+          __ load_unsigned_short(flags, Address(cache, in_bytes(ResolvedFieldEntry::field_index_offset())));\n+          __ movptr(rcx, Address(cache, ResolvedFieldEntry::field_holder_offset()));\n+          __ read_flat_field(rcx, flags, rbx, rax);\n+          __ verify_oop(rax);\n+          __ push(atos);\n+      __ bind(rewrite_inline);\n+      if (rc == may_rewrite) {\n+        patch_bytecode(Bytecodes::_fast_qgetfield, bc, rbx);\n+      }\n+        __ jmp(Done);\n+    }\n@@ -2988,1 +3186,0 @@\n-  __ jmp(Done);\n@@ -2991,0 +3188,3 @@\n+\n+  if (!is_static) pop_and_check_object(obj);\n+\n@@ -3090,0 +3290,16 @@\n+void TemplateTable::withfield() {\n+  transition(vtos, atos);\n+\n+  Register cache = LP64_ONLY(c_rarg1) NOT_LP64(rcx);\n+  Register index = LP64_ONLY(c_rarg2) NOT_LP64(rdx);\n+\n+  resolve_cache_and_index_for_field(f2_byte, cache, index);\n+\n+  __ lea(rax, at_tos());\n+  __ call_VM(rbx, CAST_FROM_FN_PTR(address, InterpreterRuntime::withfield), cache, rax);\n+  \/\/ new value type is returned in rbx\n+  \/\/ stack adjustment is returned in rax\n+  __ verify_oop(rbx);\n+  __ addptr(rsp, rax);\n+  __ movptr(rax, rbx);\n+}\n@@ -3172,1 +3388,1 @@\n-  const Register flags = rax;\n+  const Register flags = r9;\n@@ -3185,2 +3401,3 @@\n-  __ andl(flags, (1 << ResolvedFieldEntry::is_volatile_shift));\n-  __ testl(flags, flags);\n+  __ movl(rscratch1, flags);\n+  __ andl(rscratch1, (1 << ResolvedFieldEntry::is_volatile_shift));\n+  __ testl(rscratch1, rscratch1);\n@@ -3189,1 +3406,1 @@\n-  putfield_or_static_helper(byte_no, is_static, rc, obj, off, tos_state);\n+  putfield_or_static_helper(byte_no, is_static, rc, obj, off, tos_state, flags);\n@@ -3195,1 +3412,1 @@\n-  putfield_or_static_helper(byte_no, is_static, rc, obj, off, tos_state);\n+  putfield_or_static_helper(byte_no, is_static, rc, obj, off, tos_state, flags);\n@@ -3201,1 +3418,1 @@\n-                                              Register obj, Register off, Register tos_state) {\n+                                              Register obj, Register off, Register tos_state, Register flags) {\n@@ -3208,1 +3425,1 @@\n-        notLong, notFloat, notObj;\n+        notLong, notFloat, notObj, notInlineType;\n@@ -3249,6 +3466,53 @@\n-    __ pop(atos);\n-    if (!is_static) pop_and_check_object(obj);\n-    \/\/ Store into the field\n-    do_oop_store(_masm, field, rax);\n-    if (!is_static && rc == may_rewrite) {\n-      patch_bytecode(Bytecodes::_fast_aputfield, bc, rbx, true, byte_no);\n+    if (!EnablePrimitiveClasses) {\n+      __ pop(atos);\n+      if (!is_static) pop_and_check_object(obj);\n+      \/\/ Store into the field\n+      do_oop_store(_masm, field, rax);\n+      if (!is_static && rc == may_rewrite) {\n+        patch_bytecode(Bytecodes::_fast_aputfield, bc, rbx, true, byte_no);\n+      }\n+      __ jmp(Done);\n+    } else {\n+      __ pop(atos);\n+      if (is_static) {\n+        Label is_inline_type;\n+        __ test_field_is_not_null_free_inline_type(flags, rscratch1, is_inline_type);\n+        __ null_check(rax);\n+        __ bind(is_inline_type);\n+        do_oop_store(_masm, field, rax);\n+        __ jmp(Done);\n+      } else {\n+        Label is_inline_type, is_flat, rewrite_not_inline, rewrite_inline;\n+        __ test_field_is_null_free_inline_type(flags, rscratch1, is_inline_type);\n+        \/\/ Not an inline type\n+        pop_and_check_object(obj);\n+        \/\/ Store into the field\n+        do_oop_store(_masm, field, rax);\n+        __ bind(rewrite_not_inline);\n+        if (rc == may_rewrite) {\n+          patch_bytecode(Bytecodes::_fast_aputfield, bc, rbx, true, byte_no);\n+        }\n+        __ jmp(Done);\n+        \/\/ Implementation of the inline type semantic\n+        __ bind(is_inline_type);\n+        __ null_check(rax);\n+        __ test_field_is_flat(flags, rscratch1, is_flat);\n+        \/\/ field is not flat\n+        pop_and_check_object(obj);\n+        \/\/ Store into the field\n+        do_oop_store(_masm, field, rax);\n+        __ jmp(rewrite_inline);\n+        __ bind(is_flat);\n+        \/\/ field is flat\n+        pop_and_check_object(obj);\n+        assert_different_registers(rax, rdx, obj, off);\n+        __ load_klass(rdx, rax, rscratch1);\n+        __ data_for_oop(rax, rax, rdx);\n+        __ addptr(obj, off);\n+        __ access_value_copy(IN_HEAP, rax, obj, rdx);\n+        __ bind(rewrite_inline);\n+        if (rc == may_rewrite) {\n+          patch_bytecode(Bytecodes::_fast_qputfield, bc, rbx, true, byte_no);\n+        }\n+        __ jmp(Done);\n+      }\n@@ -3256,1 +3520,0 @@\n-    __ jmp(Done);\n@@ -3395,0 +3658,1 @@\n+    case Bytecodes::_fast_qputfield: \/\/fall through\n@@ -3420,0 +3684,1 @@\n+    case Bytecodes::_fast_qputfield: \/\/ fall through\n@@ -3447,2 +3712,1 @@\n-  \/\/ RBX: field offset, RCX: RAX: TOS, RDX: flags\n-  __ andl(rdx, (1 << ResolvedFieldEntry::is_volatile_shift));\n+  \/\/ RBX: field offset, RCX: RAX: TOS, RDX: flags\n@@ -3458,1 +3722,3 @@\n-  __ testl(rdx, rdx);\n+  __ movl(rscratch2, rdx);  \/\/ saving flags for is_flat test\n+  __ andl(rscratch2, (1 << ResolvedFieldEntry::is_volatile_shift));\n+  __ testl(rscratch2, rscratch2);\n@@ -3461,1 +3727,1 @@\n-  fast_storefield_helper(field, rax);\n+  fast_storefield_helper(field, rax, rdx);\n@@ -3467,1 +3733,1 @@\n-  fast_storefield_helper(field, rax);\n+  fast_storefield_helper(field, rax, rdx);\n@@ -3472,1 +3738,1 @@\n-void TemplateTable::fast_storefield_helper(Address field, Register rax) {\n+void TemplateTable::fast_storefield_helper(Address field, Register rax, Register flags) {\n@@ -3476,0 +3742,17 @@\n+  case Bytecodes::_fast_qputfield:\n+    {\n+      Label is_flat, done;\n+      __ null_check(rax);\n+      __ test_field_is_flat(flags, rscratch1, is_flat);\n+      \/\/ field is not flat\n+      do_oop_store(_masm, field, rax);\n+      __ jmp(done);\n+      __ bind(is_flat);\n+      \/\/ field is flat\n+      __ load_klass(rdx, rax, rscratch1);\n+      __ data_for_oop(rax, rax, rdx);\n+      __ lea(rcx, field);\n+      __ access_value_copy(IN_HEAP, rax, rcx, rdx);\n+      __ bind(done);\n+    }\n+    break;\n@@ -3477,1 +3760,3 @@\n-    do_oop_store(_masm, field, rax);\n+    {\n+      do_oop_store(_masm, field, rax);\n+    }\n@@ -3539,1 +3824,1 @@\n-  __ load_sized_value(rbx, Address(rcx, in_bytes(ResolvedFieldEntry::field_offset_offset())), sizeof(int), true \/*is_signed*\/);\n+  __ load_sized_value(rdx, Address(rcx, in_bytes(ResolvedFieldEntry::field_offset_offset())), sizeof(int), true \/*is_signed*\/);\n@@ -3544,1 +3829,1 @@\n-  Address field(rax, rbx, Address::times_1);\n+  Address field(rax, rdx, Address::times_1);\n@@ -3548,0 +3833,27 @@\n+  case Bytecodes::_fast_qgetfield:\n+    {\n+      Label is_flat, nonnull, Done;\n+      __ load_unsigned_byte(rscratch1, Address(rcx, in_bytes(ResolvedFieldEntry::flags_offset())));\n+      __ test_field_is_flat(rscratch1, rscratch2, is_flat);\n+        \/\/ field is not flat\n+        __ load_heap_oop(rax, field);\n+        __ testptr(rax, rax);\n+        __ jcc(Assembler::notZero, nonnull);\n+          __ load_unsigned_short(rdx, Address(rcx, in_bytes(ResolvedFieldEntry::field_index_offset())));\n+          __ movptr(rcx, Address(rcx, ResolvedFieldEntry::field_holder_offset()));\n+          __ get_inline_type_field_klass(rcx, rdx, rbx);\n+          __ get_default_value_oop(rbx, rcx, rax);\n+        __ bind(nonnull);\n+        __ verify_oop(rax);\n+        __ jmp(Done);\n+      __ bind(is_flat);\n+      \/\/ field is flat\n+        __ push(rdx); \/\/ save offset\n+        __ load_unsigned_short(rdx, Address(rcx, in_bytes(ResolvedFieldEntry::field_index_offset())));\n+        __ movptr(rcx, Address(rcx, ResolvedFieldEntry::field_holder_offset()));\n+        __ pop(rbx); \/\/ restore offset\n+        __ read_flat_field(rcx, rdx, rbx, rax);\n+      __ bind(Done);\n+      __ verify_oop(rax);\n+    }\n+    break;\n@@ -4011,2 +4323,1 @@\n-  Label slow_case_no_pop;\n-  Label initialize_header;\n+  Label is_not_value;\n@@ -4022,1 +4333,1 @@\n-  __ jcc(Assembler::notEqual, slow_case_no_pop);\n+  __ jcc(Assembler::notEqual, slow_case);\n@@ -4026,1 +4337,7 @@\n-  __ push(rcx);  \/\/ save the contexts of klass for initializing the header\n+\n+  __ cmpb(Address(rcx, InstanceKlass::kind_offset()), Klass::InlineKlassKind);\n+  __ jcc(Assembler::notEqual, is_not_value);\n+\n+  __ call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::throw_InstantiationError));\n+\n+  __ bind(is_not_value);\n@@ -4029,1 +4346,0 @@\n-  \/\/ make sure klass is fully initialized\n@@ -4033,14 +4349,2 @@\n-  \/\/ get instance_size in InstanceKlass (scaled to a count of bytes)\n-  __ movl(rdx, Address(rcx, Klass::layout_helper_offset()));\n-  \/\/ test to see if it has a finalizer or is malformed in some way\n-  __ testl(rdx, Klass::_lh_instance_slow_path_bit);\n-  __ jcc(Assembler::notZero, slow_case);\n-\n-  \/\/ Allocate the instance:\n-  \/\/  If TLAB is enabled:\n-  \/\/    Try to allocate in the TLAB.\n-  \/\/    If fails, go to the slow path.\n-  \/\/    Initialize the allocation.\n-  \/\/    Exit.\n-  \/\/\n-  \/\/  Go to slow path.\n+  __ allocate_instance(rcx, rax, rdx, rbx, true, slow_case);\n+  __ jmp(done);\n@@ -4048,1 +4352,2 @@\n-  const Register thread = LP64_ONLY(r15_thread) NOT_LP64(rcx);\n+  \/\/ slow case\n+  __ bind(slow_case);\n@@ -4050,7 +4355,2 @@\n-  if (UseTLAB) {\n-    NOT_LP64(__ get_thread(thread);)\n-    __ tlab_allocate(thread, rax, rdx, 0, rcx, rbx, slow_case);\n-    if (ZeroTLAB) {\n-      \/\/ the fields have been already cleared\n-      __ jmp(initialize_header);\n-    }\n+  Register rarg1 = LP64_ONLY(c_rarg1) NOT_LP64(rax);\n+  Register rarg2 = LP64_ONLY(c_rarg2) NOT_LP64(rdx);\n@@ -4058,4 +4358,4 @@\n-    \/\/ The object is initialized before the header.  If the object size is\n-    \/\/ zero, go directly to the header initialization.\n-    __ decrement(rdx, sizeof(oopDesc));\n-    __ jcc(Assembler::zero, initialize_header);\n+  __ get_constant_pool(rarg1);\n+  __ get_unsigned_2_byte_index_at_bcp(rarg2, 1);\n+  call_VM(rax, CAST_FROM_FN_PTR(address, InterpreterRuntime::_new), rarg1, rarg2);\n+   __ verify_oop(rax);\n@@ -4063,4 +4363,3 @@\n-    \/\/ Initialize topmost object field, divide rdx by 8, check if odd and\n-    \/\/ test if zero.\n-    __ xorl(rcx, rcx);    \/\/ use zero reg to clear memory (shorter code)\n-    __ shrl(rdx, LogBytesPerLong); \/\/ divide by 2*oopSize and set carry flag if odd\n+  \/\/ continue\n+  __ bind(done);\n+}\n@@ -4068,10 +4367,2 @@\n-    \/\/ rdx must have been multiple of 8\n-#ifdef ASSERT\n-    \/\/ make sure rdx was multiple of 8\n-    Label L;\n-    \/\/ Ignore partial flag stall after shrl() since it is debug VM\n-    __ jcc(Assembler::carryClear, L);\n-    __ stop(\"object size is not multiple of 2 - adjust this code\");\n-    __ bind(L);\n-    \/\/ rdx must be > 0, no extra check needed here\n-#endif\n+void TemplateTable::aconst_init() {\n+  transition(vtos, atos);\n@@ -4079,8 +4370,3 @@\n-    \/\/ initialize remaining object fields: rdx was a multiple of 8\n-    { Label loop;\n-    __ bind(loop);\n-    __ movptr(Address(rax, rdx, Address::times_8, sizeof(oopDesc) - 1*oopSize), rcx);\n-    NOT_LP64(__ movptr(Address(rax, rdx, Address::times_8, sizeof(oopDesc) - 2*oopSize), rcx));\n-    __ decrement(rdx);\n-    __ jcc(Assembler::notZero, loop);\n-    }\n+  Label slow_case;\n+  Label done;\n+  Label is_value;\n@@ -4088,10 +4374,2 @@\n-    \/\/ initialize object header only.\n-    __ bind(initialize_header);\n-    __ movptr(Address(rax, oopDesc::mark_offset_in_bytes()),\n-              (intptr_t)markWord::prototype().value()); \/\/ header\n-    __ pop(rcx);   \/\/ get saved klass back in the register.\n-#ifdef _LP64\n-    __ xorl(rsi, rsi); \/\/ use zero reg to clear memory (shorter code)\n-    __ store_klass_gap(rax, rsi);  \/\/ zero klass gap for compressed oops\n-#endif\n-    __ store_klass(rax, rcx, rscratch1);  \/\/ klass\n+  __ get_unsigned_2_byte_index_at_bcp(rdx, 1);\n+  __ get_cpool_and_tags(rcx, rax);\n@@ -4099,8 +4377,6 @@\n-    {\n-      SkipIfEqual skip_if(_masm, &DTraceAllocProbes, 0, rscratch1);\n-      \/\/ Trigger dtrace event for fastpath\n-      __ push(atos);\n-      __ call_VM_leaf(\n-           CAST_FROM_FN_PTR(address, static_cast<int (*)(oopDesc*)>(SharedRuntime::dtrace_object_alloc)), rax);\n-      __ pop(atos);\n-    }\n+  \/\/ Make sure the class we're about to instantiate has been resolved.\n+  \/\/ This is done before loading InstanceKlass to be consistent with the order\n+  \/\/ how Constant Pool is updated (see ConstantPool::klass_at_put)\n+  const int tags_offset = Array<u1>::base_offset_in_bytes();\n+  __ cmpb(Address(rax, rdx, Address::times_1, tags_offset), JVM_CONSTANT_Class);\n+  __ jcc(Assembler::notEqual, slow_case);\n@@ -4108,2 +4384,18 @@\n-    __ jmp(done);\n-  }\n+  \/\/ get InstanceKlass\n+  __ load_resolved_klass_at_index(rcx, rcx, rdx);\n+\n+  __ cmpb(Address(rcx, InstanceKlass::kind_offset()), Klass::InlineKlassKind);\n+  __ jcc(Assembler::equal, is_value);\n+\n+  \/\/ in the future, aconst_init will just return null instead of throwing an exception\n+  __ call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::throw_IncompatibleClassChangeError));\n+\n+  __ bind(is_value);\n+\n+  \/\/ make sure klass is fully initialized\n+  __ cmpb(Address(rcx, InstanceKlass::init_state_offset()), InstanceKlass::fully_initialized);\n+  __ jcc(Assembler::notEqual, slow_case);\n+\n+  \/\/ have a resolved InlineKlass in rcx, return the default value oop from it\n+  __ get_default_value_oop(rcx, rdx, rax);\n+  __ jmp(done);\n@@ -4111,4 +4403,1 @@\n-  \/\/ slow case\n-  __ pop(rcx);   \/\/ restore stack pointer to what it was when we came in.\n-  __ bind(slow_case_no_pop);\n-  Register rarg1 = LP64_ONLY(c_rarg1) NOT_LP64(rax);\n+  Register rarg1 = LP64_ONLY(c_rarg1) NOT_LP64(rcx);\n@@ -4119,3 +4408,4 @@\n-  __ get_constant_pool(rarg1);\n-  call_VM(rax, CAST_FROM_FN_PTR(address, InterpreterRuntime::_new), rarg1, rarg2);\n-   __ verify_oop(rax);\n+  __ get_constant_pool(rarg1);\n+\n+  call_VM(rax, CAST_FROM_FN_PTR(address, InterpreterRuntime::aconst_init),\n+      rarg1, rarg2);\n@@ -4124,1 +4414,1 @@\n-  \/\/ continue\n+  __ verify_oop(rax);\n@@ -4163,4 +4453,5 @@\n-  __ cmpb(Address(rdx, rbx,\n-                  Address::times_1,\n-                  Array<u1>::base_offset_in_bytes()),\n-          JVM_CONSTANT_Class);\n+  __ movzbl(rdx, Address(rdx, rbx,\n+      Address::times_1,\n+      Array<u1>::base_offset_in_bytes()));\n+  __ andl (rdx, ~JVM_CONSTANT_QDescBit);\n+  __ cmpl(rdx, JVM_CONSTANT_Class);\n@@ -4204,0 +4495,3 @@\n+  __ jmp(done);\n+\n+  __ bind(is_null);\n@@ -4207,4 +4501,15 @@\n-    __ jmp(done);\n-    __ bind(is_null);\n-  } else {\n-    __ bind(is_null);   \/\/ same as 'done'\n+\n+  if (EnablePrimitiveClasses) {\n+    \/\/ Get cpool & tags index\n+    __ get_cpool_and_tags(rcx, rdx); \/\/ rcx=cpool, rdx=tags array\n+    __ get_unsigned_2_byte_index_at_bcp(rbx, 1); \/\/ rbx=index\n+    \/\/ See if CP entry is a Q-descriptor\n+    __ movzbl(rcx, Address(rdx, rbx,\n+        Address::times_1,\n+        Array<u1>::base_offset_in_bytes()));\n+    __ andl (rcx, JVM_CONSTANT_QDescBit);\n+    __ cmpl(rcx, JVM_CONSTANT_QDescBit);\n+    __ jcc(Assembler::notEqual, done);\n+    __ jump(ExternalAddress(Interpreter::_throw_NullPointerException_entry));\n+  }\n+\n@@ -4226,4 +4531,5 @@\n-  __ cmpb(Address(rdx, rbx,\n-                  Address::times_1,\n-                  Array<u1>::base_offset_in_bytes()),\n-          JVM_CONSTANT_Class);\n+  __ movzbl(rdx, Address(rdx, rbx,\n+        Address::times_1,\n+        Array<u1>::base_offset_in_bytes()));\n+  __ andl (rdx, ~JVM_CONSTANT_QDescBit);\n+  __ cmpl(rdx, JVM_CONSTANT_Class);\n@@ -4281,1 +4587,0 @@\n-\n@@ -4343,0 +4648,4 @@\n+  Label is_inline_type;\n+  __ movptr(rbx, Address(rax, oopDesc::mark_offset_in_bytes()));\n+  __ test_markword_is_inline_type(rbx, is_inline_type);\n+\n@@ -4432,0 +4741,5 @@\n+\n+  __ bind(is_inline_type);\n+  __ call_VM(noreg, CAST_FROM_FN_PTR(address,\n+                    InterpreterRuntime::throw_illegal_monitor_state_exception));\n+  __ should_not_reach_here();\n@@ -4440,0 +4754,11 @@\n+  const int is_inline_type_mask = markWord::inline_type_pattern;\n+  Label has_identity;\n+  __ movptr(rbx, Address(rax, oopDesc::mark_offset_in_bytes()));\n+  __ andptr(rbx, is_inline_type_mask);\n+  __ cmpl(rbx, is_inline_type_mask);\n+  __ jcc(Assembler::notEqual, has_identity);\n+  __ call_VM(noreg, CAST_FROM_FN_PTR(address,\n+                     InterpreterRuntime::throw_illegal_monitor_state_exception));\n+  __ should_not_reach_here();\n+  __ bind(has_identity);\n+\n","filename":"src\/hotspot\/cpu\/x86\/templateTable_x86.cpp","additions":470,"deletions":145,"binary":false,"changes":615,"status":"modified"},{"patch":"@@ -33,0 +33,2 @@\n+#include \"ci\/ciFlatArrayKlass.hpp\"\n+#include \"ci\/ciInlineKlass.hpp\"\n@@ -728,0 +730,11 @@\n+  \/\/ Record this newly allocated object\n+  void new_instance(NewInlineTypeInstance* object) {\n+    int index = _newobjects.length();\n+    _newobjects.append(object);\n+    if (_fields.at_grow(index, nullptr) == nullptr) {\n+      _fields.at_put(index, new FieldBuffer());\n+    } else {\n+      _fields.at(index)->kill();\n+    }\n+  }\n+\n@@ -1027,0 +1040,7 @@\n+  if (x->as_NewInlineTypeInstance() != nullptr && x->as_NewInlineTypeInstance()->in_larval_state()) {\n+    if (x->as_NewInlineTypeInstance()->on_stack_count() == 1) {\n+      x->as_NewInlineTypeInstance()->set_not_larva_anymore();\n+    } else {\n+      x->as_NewInlineTypeInstance()->increment_on_stack_count();\n+    }\n+  }\n@@ -1033,0 +1053,3 @@\n+  if (x->as_NewInlineTypeInstance() != nullptr) {\n+    x->as_NewInlineTypeInstance()->set_local_index(index);\n+  }\n@@ -1061,0 +1084,3 @@\n+  if (x->as_NewInlineTypeInstance() != nullptr) {\n+    x->as_NewInlineTypeInstance()->set_local_index(index);\n+  }\n@@ -1066,1 +1092,9 @@\n-  ValueStack* state_before = copy_state_indexed_access();\n+  ValueStack* state_before = nullptr;\n+  int array_idx = state()->stack_size() - 2;\n+  if (type == T_OBJECT && state()->stack_at(array_idx)->maybe_flat_array()) {\n+    \/\/ Save the entire state and re-execute on deopt when accessing flat arrays\n+    state_before = copy_state_before();\n+    state_before->set_should_reexecute(true);\n+  } else {\n+    state_before = copy_state_indexed_access();\n+  }\n@@ -1078,1 +1112,61 @@\n-  push(as_ValueType(type), append(new LoadIndexed(array, index, length, type, state_before)));\n+\n+  bool need_membar = false;\n+  LoadIndexed* load_indexed = nullptr;\n+  Instruction* result = nullptr;\n+  if (array->is_loaded_flat_array()) {\n+    ciType* array_type = array->declared_type();\n+    ciInlineKlass* elem_klass = array_type->as_flat_array_klass()->element_klass()->as_inline_klass();\n+\n+    bool can_delay_access = false;\n+    ciBytecodeStream s(method());\n+    s.force_bci(bci());\n+    s.next();\n+    if (s.cur_bc() == Bytecodes::_getfield) {\n+      bool will_link;\n+      ciField* next_field = s.get_field(will_link);\n+      bool next_needs_patching = !next_field->holder()->is_initialized() ||\n+                                 !next_field->will_link(method(), Bytecodes::_getfield) ||\n+                                 PatchALot;\n+      can_delay_access = C1UseDelayedFlattenedFieldReads && !next_needs_patching;\n+    }\n+    if (can_delay_access) {\n+      \/\/ potentially optimizable array access, storing information for delayed decision\n+      LoadIndexed* li = new LoadIndexed(array, index, length, type, state_before);\n+      DelayedLoadIndexed* dli = new DelayedLoadIndexed(li, state_before);\n+      li->set_delayed(dli);\n+      set_pending_load_indexed(dli);\n+      return; \/\/ Nothing else to do for now\n+    } else {\n+      if (elem_klass->is_empty()) {\n+        \/\/ No need to create a new instance, the default instance will be used instead\n+        load_indexed = new LoadIndexed(array, index, length, type, state_before);\n+        apush(append(load_indexed));\n+      } else {\n+        NewInlineTypeInstance* new_instance = new NewInlineTypeInstance(elem_klass, state_before);\n+        _memory->new_instance(new_instance);\n+        apush(append_split(new_instance));\n+        load_indexed = new LoadIndexed(array, index, length, type, state_before);\n+        load_indexed->set_vt(new_instance);\n+        \/\/ The LoadIndexed node will initialise this instance by copying from\n+        \/\/ the flat field.  Ensure these stores are visible before any\n+        \/\/ subsequent store that publishes this reference.\n+        need_membar = true;\n+      }\n+    }\n+  } else {\n+    load_indexed = new LoadIndexed(array, index, length, type, state_before);\n+    if (profile_array_accesses() && is_reference_type(type)) {\n+      compilation()->set_would_profile(true);\n+      load_indexed->set_should_profile(true);\n+      load_indexed->set_profiled_method(method());\n+      load_indexed->set_profiled_bci(bci());\n+    }\n+  }\n+  result = append(load_indexed);\n+  if (need_membar) {\n+    append(new MemBar(lir_membar_storestore));\n+  }\n+  assert(!load_indexed->should_profile() || load_indexed == result, \"should not be optimized out\");\n+  if (!array->is_loaded_flat_array()) {\n+    push(as_ValueType(type), result);\n+  }\n@@ -1084,1 +1178,9 @@\n-  ValueStack* state_before = copy_state_indexed_access();\n+  ValueStack* state_before = nullptr;\n+  int array_idx = state()->stack_size() - 3;\n+  if (type == T_OBJECT && state()->stack_at(array_idx)->maybe_flat_array()) {\n+    \/\/ Save the entire state and re-execute on deopt when accessing flat arrays\n+    state_before = copy_state_before();\n+    state_before->set_should_reexecute(true);\n+  } else {\n+    state_before = copy_state_indexed_access();\n+  }\n@@ -1109,5 +1211,2 @@\n-  StoreIndexed* result = new StoreIndexed(array, index, length, type, value, state_before, check_boolean);\n-  append(result);\n-  _memory->store_value(value);\n-  if (type == T_OBJECT && is_profiling()) {\n-    \/\/ Note that we'd collect profile data in this method if we wanted it.\n+  StoreIndexed* store_indexed = new StoreIndexed(array, index, length, type, value, state_before, check_boolean);\n+  if (profile_array_accesses() && is_reference_type(type) && !array->is_loaded_flat_array()) {\n@@ -1116,6 +1215,3 @@\n-\n-    if (profile_checkcasts()) {\n-      result->set_profiled_method(method());\n-      result->set_profiled_bci(bci());\n-      result->set_should_profile(true);\n-    }\n+    store_indexed->set_should_profile(true);\n+    store_indexed->set_profiled_method(method());\n+    store_indexed->set_profiled_bci(bci());\n@@ -1123,0 +1219,3 @@\n+  Instruction* result = append(store_indexed);\n+  assert(!store_indexed->should_profile() || store_indexed == result, \"should not be optimized out\");\n+  _memory->store_value(value);\n@@ -1125,1 +1224,0 @@\n-\n@@ -1129,1 +1227,2 @@\n-      { state()->raw_pop();\n+      { Value w = state()->raw_pop();\n+        update_larva_stack_count(w);\n@@ -1133,2 +1232,4 @@\n-      { state()->raw_pop();\n-        state()->raw_pop();\n+      { Value w1 = state()->raw_pop();\n+        Value w2 = state()->raw_pop();\n+        update_larva_stack_count(w1);\n+        update_larva_stack_count(w2);\n@@ -1139,0 +1240,1 @@\n+        update_larval_state(w);\n@@ -1146,0 +1248,1 @@\n+        update_larval_state(w1);\n@@ -1155,0 +1258,11 @@\n+        \/\/ special handling for the dup_x2\/pop sequence (see JDK-8251046)\n+        if (w1 != nullptr && w1->as_NewInlineTypeInstance() != nullptr) {\n+          ciBytecodeStream s(method());\n+          s.force_bci(bci());\n+          s.next();\n+          if (s.cur_bc() != Bytecodes::_pop) {\n+            w1->as_NewInlineTypeInstance()->set_not_larva_anymore();\n+          } else {\n+            w1->as_NewInlineTypeInstance()->increment_on_stack_count();\n+          }\n+        }\n@@ -1164,0 +1278,2 @@\n+        update_larval_state(w1);\n+        update_larval_state(w2);\n@@ -1174,0 +1290,2 @@\n+        update_larval_state(w1);\n+        update_larval_state(w2);\n@@ -1186,0 +1304,2 @@\n+        update_larval_state(w1);\n+        update_larval_state(w2);\n@@ -1313,0 +1433,27 @@\n+\n+  bool subst_check = false;\n+  if (EnableValhalla && (stream()->cur_bc() == Bytecodes::_if_acmpeq || stream()->cur_bc() == Bytecodes::_if_acmpne)) {\n+    ValueType* left_vt = x->type();\n+    ValueType* right_vt = y->type();\n+    if (left_vt->is_object()) {\n+      assert(right_vt->is_object(), \"must be\");\n+      ciKlass* left_klass = x->as_loaded_klass_or_null();\n+      ciKlass* right_klass = y->as_loaded_klass_or_null();\n+\n+      if (left_klass == nullptr || right_klass == nullptr) {\n+        \/\/ The klass is still unloaded, or came from a Phi node. Go slow case;\n+        subst_check = true;\n+      } else if (left_klass->can_be_inline_klass() || right_klass->can_be_inline_klass()) {\n+        \/\/ Either operand may be a value object, but we're not sure. Go slow case;\n+        subst_check = true;\n+      } else {\n+        \/\/ No need to do substitutability check\n+      }\n+    }\n+  }\n+  if ((stream()->cur_bc() == Bytecodes::_if_acmpeq || stream()->cur_bc() == Bytecodes::_if_acmpne) &&\n+      is_profiling() && profile_branches()) {\n+    compilation()->set_would_profile(true);\n+    append(new ProfileACmpTypes(method(), bci(), x, y));\n+  }\n+\n@@ -1315,1 +1462,1 @@\n-  Instruction *i = append(new If(x, cond, false, y, tsux, fsux, (is_bb || compilation()->is_optimistic()) ? state_before : nullptr, is_bb));\n+  Instruction *i = append(new If(x, cond, false, y, tsux, fsux, (is_bb || compilation()->is_optimistic() || subst_check) ? state_before : nullptr, is_bb, subst_check));\n@@ -1566,1 +1713,1 @@\n-  if (method()->name() == ciSymbols::object_initializer_name() &&\n+  if ((method()->is_object_constructor() || method()->is_static_vnew_factory()) &&\n@@ -1717,0 +1864,13 @@\n+void GraphBuilder::copy_inline_content(ciInlineKlass* vk, Value src, int src_off, Value dest, int dest_off, ValueStack* state_before, ciField* enclosing_field) {\n+  for (int i = 0; i < vk->nof_nonstatic_fields(); i++) {\n+    ciField* inner_field = vk->nonstatic_field_at(i);\n+    assert(!inner_field->is_flat(), \"the iteration over nested fields is handled by the loop itself\");\n+    int off = inner_field->offset_in_bytes() - vk->first_field_offset();\n+    LoadField* load = new LoadField(src, src_off + off, inner_field, false, state_before, false);\n+    Value replacement = append(load);\n+    StoreField* store = new StoreField(dest, dest_off + off, inner_field, replacement, false, state_before, false);\n+    store->set_enclosing_field(enclosing_field);\n+    append(store);\n+  }\n+}\n+\n@@ -1723,0 +1883,1 @@\n+\n@@ -1726,1 +1887,1 @@\n-                              PatchALot;\n+                              (!field->is_flat() && PatchALot);\n@@ -1745,1 +1906,1 @@\n-  if (field->is_final() && (code == Bytecodes::_putfield)) {\n+  if (field->is_final() && code == Bytecodes::_putfield) {\n@@ -1756,1 +1917,1 @@\n-  const int offset = !needs_patching ? field->offset_in_bytes() : -1;\n+  int offset = !needs_patching ? field->offset_in_bytes() : -1;\n@@ -1766,0 +1927,4 @@\n+      } else if (field->is_null_free() && field->type()->as_instance_klass()->is_initialized() &&\n+                 field->type()->as_inline_klass()->is_empty()) {\n+        \/\/ Loading from a field of an empty inline type. Just return the default instance.\n+        constant = new Constant(new InstanceConstant(field->type()->as_inline_klass()->default_instance()));\n@@ -1773,2 +1938,3 @@\n-        push(type, append(new LoadField(append(obj), offset, field, true,\n-                                        state_before, needs_patching)));\n+        LoadField* load_field = new LoadField(append(obj), offset, field, true,\n+                                        state_before, needs_patching);\n+        push(type, append(load_field));\n@@ -1783,1 +1949,1 @@\n-      if (field->type()->basic_type() == T_BOOLEAN) {\n+      if (field_type == T_BOOLEAN) {\n@@ -1787,0 +1953,4 @@\n+      if (field->is_null_free() && field->type()->is_loaded() && field->type()->as_inline_klass()->is_empty()) {\n+        \/\/ Storing to a field of an empty inline type. Ignore.\n+        break;\n+      }\n@@ -1793,14 +1963,31 @@\n-      obj = apop();\n-      ObjectType* obj_type = obj->type()->as_ObjectType();\n-      if (field->is_constant() && obj_type->is_constant() && !PatchALot) {\n-        ciObject* const_oop = obj_type->constant_value();\n-        if (!const_oop->is_null_object() && const_oop->is_loaded()) {\n-          ciConstant field_value = field->constant_value_of(const_oop);\n-          if (field_value.is_valid()) {\n-            constant = make_constant(field_value, field);\n-            \/\/ For CallSite objects add a dependency for invalidation of the optimization.\n-            if (field->is_call_site_target()) {\n-              ciCallSite* call_site = const_oop->as_call_site();\n-              if (!call_site->is_fully_initialized_constant_call_site()) {\n-                ciMethodHandle* target = field_value.as_object()->as_method_handle();\n-                dependency_recorder()->assert_call_site_target_value(call_site, target);\n+      if (state_before == nullptr && field->is_flat()) {\n+        \/\/ Save the entire state and re-execute on deopt when accessing flat fields\n+        assert(Interpreter::bytecode_should_reexecute(code), \"should reexecute\");\n+        state_before = copy_state_before();\n+      }\n+      if (!has_pending_field_access() && !has_pending_load_indexed()) {\n+        obj = apop();\n+        ObjectType* obj_type = obj->type()->as_ObjectType();\n+        if (field->is_null_free() && field->type()->as_instance_klass()->is_initialized()\n+            && field->type()->as_inline_klass()->is_empty()) {\n+          \/\/ Loading from a field of an empty inline type. Just return the default instance.\n+          null_check(obj);\n+          constant = new Constant(new InstanceConstant(field->type()->as_inline_klass()->default_instance()));\n+        } else if (field->is_constant() && !field->is_flat() && obj_type->is_constant() && !PatchALot) {\n+          ciObject* const_oop = obj_type->constant_value();\n+          if (!const_oop->is_null_object() && const_oop->is_loaded()) {\n+            ciConstant field_value = field->constant_value_of(const_oop);\n+            if (field_value.is_valid()) {\n+              if (field->is_null_free() && field_value.is_null_or_zero()) {\n+                \/\/ Non-flat inline type field. Replace null by the default value.\n+                constant = new Constant(new InstanceConstant(field->type()->as_inline_klass()->default_instance()));\n+              } else {\n+                constant = make_constant(field_value, field);\n+              }\n+              \/\/ For CallSite objects add a dependency for invalidation of the optimization.\n+              if (field->is_call_site_target()) {\n+                ciCallSite* call_site = const_oop->as_call_site();\n+                if (!call_site->is_fully_initialized_constant_call_site()) {\n+                  ciMethodHandle* target = field_value.as_object()->as_method_handle();\n+                  dependency_recorder()->assert_call_site_target_value(call_site, target);\n+                }\n@@ -1818,19 +2005,15 @@\n-        LoadField* load = new LoadField(obj, offset, field, false, state_before, needs_patching);\n-        Value replacement = !needs_patching ? _memory->load(load) : load;\n-        if (replacement != load) {\n-          assert(replacement->is_linked() || !replacement->can_be_linked(), \"should already by linked\");\n-          \/\/ Writing an (integer) value to a boolean, byte, char or short field includes an implicit narrowing\n-          \/\/ conversion. Emit an explicit conversion here to get the correct field value after the write.\n-          BasicType bt = field->type()->basic_type();\n-          switch (bt) {\n-          case T_BOOLEAN:\n-          case T_BYTE:\n-            replacement = append(new Convert(Bytecodes::_i2b, replacement, as_ValueType(bt)));\n-            break;\n-          case T_CHAR:\n-            replacement = append(new Convert(Bytecodes::_i2c, replacement, as_ValueType(bt)));\n-            break;\n-          case T_SHORT:\n-            replacement = append(new Convert(Bytecodes::_i2s, replacement, as_ValueType(bt)));\n-            break;\n-          default:\n+        if (!field->is_flat()) {\n+          if (has_pending_field_access()) {\n+            assert(!needs_patching, \"Can't patch delayed field access\");\n+            obj = pending_field_access()->obj();\n+            offset += pending_field_access()->offset() - field->holder()->as_inline_klass()->first_field_offset();\n+            field = pending_field_access()->holder()->get_field_by_offset(offset, false);\n+            assert(field != nullptr, \"field not found\");\n+            set_pending_field_access(nullptr);\n+          } else if (has_pending_load_indexed()) {\n+            assert(!needs_patching, \"Can't patch delayed field access\");\n+            pending_load_indexed()->update(field, offset - field->holder()->as_inline_klass()->first_field_offset());\n+            LoadIndexed* li = pending_load_indexed()->load_instr();\n+            li->set_type(type);\n+            push(type, append(li));\n+            set_pending_load_indexed(nullptr);\n@@ -1839,1 +2022,24 @@\n-          push(type, replacement);\n+          LoadField* load = new LoadField(obj, offset, field, false, state_before, needs_patching);\n+          Value replacement = !needs_patching ? _memory->load(load) : load;\n+          if (replacement != load) {\n+            assert(replacement->is_linked() || !replacement->can_be_linked(), \"should already by linked\");\n+            \/\/ Writing an (integer) value to a boolean, byte, char or short field includes an implicit narrowing\n+            \/\/ conversion. Emit an explicit conversion here to get the correct field value after the write.\n+            switch (field_type) {\n+            case T_BOOLEAN:\n+            case T_BYTE:\n+              replacement = append(new Convert(Bytecodes::_i2b, replacement, type));\n+              break;\n+            case T_CHAR:\n+              replacement = append(new Convert(Bytecodes::_i2c, replacement, type));\n+              break;\n+            case T_SHORT:\n+              replacement = append(new Convert(Bytecodes::_i2s, replacement, type));\n+              break;\n+            default:\n+              break;\n+            }\n+            push(type, replacement);\n+          } else {\n+            push(type, append(load));\n+          }\n@@ -1841,1 +2047,66 @@\n-          push(type, append(load));\n+          \/\/ Look at the next bytecode to check if we can delay the field access\n+          bool can_delay_access = false;\n+          ciBytecodeStream s(method());\n+          s.force_bci(bci());\n+          s.next();\n+          if (s.cur_bc() == Bytecodes::_getfield && !needs_patching) {\n+            ciField* next_field = s.get_field(will_link);\n+            bool next_needs_patching = !next_field->holder()->is_loaded() ||\n+                                       !next_field->will_link(method(), Bytecodes::_getfield) ||\n+                                       PatchALot;\n+            can_delay_access = C1UseDelayedFlattenedFieldReads && !next_needs_patching;\n+          }\n+          if (can_delay_access) {\n+            if (has_pending_load_indexed()) {\n+              pending_load_indexed()->update(field, offset - field->holder()->as_inline_klass()->first_field_offset());\n+            } else if (has_pending_field_access()) {\n+              pending_field_access()->inc_offset(offset - field->holder()->as_inline_klass()->first_field_offset());\n+            } else {\n+              null_check(obj);\n+              DelayedFieldAccess* dfa = new DelayedFieldAccess(obj, field->holder(), field->offset_in_bytes());\n+              set_pending_field_access(dfa);\n+            }\n+          } else {\n+            ciInlineKlass* inline_klass = field->type()->as_inline_klass();\n+            scope()->set_wrote_final();\n+            scope()->set_wrote_fields();\n+            bool need_membar = false;\n+            if (inline_klass->is_initialized() && inline_klass->is_empty()) {\n+              apush(append(new Constant(new InstanceConstant(inline_klass->default_instance()))));\n+              if (has_pending_field_access()) {\n+                set_pending_field_access(nullptr);\n+              } else if (has_pending_load_indexed()) {\n+                set_pending_load_indexed(nullptr);\n+              }\n+            } else if (has_pending_load_indexed()) {\n+              assert(!needs_patching, \"Can't patch delayed field access\");\n+              pending_load_indexed()->update(field, offset - field->holder()->as_inline_klass()->first_field_offset());\n+              NewInlineTypeInstance* vt = new NewInlineTypeInstance(inline_klass, pending_load_indexed()->state_before());\n+              _memory->new_instance(vt);\n+              pending_load_indexed()->load_instr()->set_vt(vt);\n+              apush(append_split(vt));\n+              append(pending_load_indexed()->load_instr());\n+              set_pending_load_indexed(nullptr);\n+              need_membar = true;\n+            } else {\n+              NewInlineTypeInstance* new_instance = new NewInlineTypeInstance(inline_klass, state_before);\n+              _memory->new_instance(new_instance);\n+              apush(append_split(new_instance));\n+              assert(!needs_patching, \"Can't patch flat inline type field access\");\n+              if (has_pending_field_access()) {\n+                copy_inline_content(inline_klass, pending_field_access()->obj(),\n+                                    pending_field_access()->offset() + field->offset_in_bytes() - field->holder()->as_inline_klass()->first_field_offset(),\n+                                    new_instance, inline_klass->first_field_offset(), state_before);\n+                set_pending_field_access(nullptr);\n+              } else {\n+                copy_inline_content(inline_klass, obj, field->offset_in_bytes(), new_instance, inline_klass->first_field_offset(), state_before);\n+              }\n+              need_membar = true;\n+            }\n+            if (need_membar) {\n+              \/\/ If we allocated a new instance ensure the stores to copy the\n+              \/\/ field contents are visible before any subsequent store that\n+              \/\/ publishes this reference.\n+              append(new MemBar(lir_membar_storestore));\n+            }\n+          }\n@@ -1852,1 +2123,1 @@\n-      if (field->type()->basic_type() == T_BOOLEAN) {\n+      if (field_type == T_BOOLEAN) {\n@@ -1856,4 +2127,13 @@\n-      StoreField* store = new StoreField(obj, offset, field, val, false, state_before, needs_patching);\n-      if (!needs_patching) store = _memory->store(store);\n-      if (store != nullptr) {\n-        append(store);\n+      if (field->is_null_free() && field->type()->is_loaded() && field->type()->as_inline_klass()->is_empty()) {\n+        \/\/ Storing to a field of an empty inline type. Ignore.\n+        null_check(obj);\n+      } else if (!field->is_flat()) {\n+        StoreField* store = new StoreField(obj, offset, field, val, false, state_before, needs_patching);\n+        if (!needs_patching) store = _memory->store(store);\n+        if (store != nullptr) {\n+          append(store);\n+        }\n+      } else {\n+        assert(!needs_patching, \"Can't patch flat inline type field access\");\n+        ciInlineKlass* inline_klass = field->type()->as_inline_klass();\n+        copy_inline_content(inline_klass, val, inline_klass->first_field_offset(), obj, offset, state_before, field);\n@@ -1869,0 +2149,74 @@\n+\/\/ Baseline version of withfield, allocate every time\n+void GraphBuilder::withfield(int field_index) {\n+  \/\/ Save the entire state and re-execute on deopt\n+  ValueStack* state_before = copy_state_before();\n+  state_before->set_should_reexecute(true);\n+\n+  bool will_link;\n+  ciField* field_modify = stream()->get_field(will_link);\n+  ciInstanceKlass* holder = field_modify->holder();\n+  BasicType field_type = field_modify->type()->basic_type();\n+  ValueType* type = as_ValueType(field_type);\n+  Value val = pop(type);\n+  Value obj = apop();\n+  null_check(obj);\n+\n+  if (!holder->is_loaded() || !holder->is_inlinetype() || !will_link) {\n+    apush(append_split(new Deoptimize(holder, state_before)));\n+    return;\n+  }\n+\n+  \/\/ call will_link again to determine if the field is valid.\n+  const bool needs_patching = !field_modify->will_link(method(), Bytecodes::_withfield) ||\n+                              (!field_modify->is_flat() && PatchALot);\n+  const int offset_modify = !needs_patching ? field_modify->offset_in_bytes() : -1;\n+\n+  scope()->set_wrote_final();\n+  scope()->set_wrote_fields();\n+\n+  NewInlineTypeInstance* new_instance;\n+  if (obj->as_NewInlineTypeInstance() != nullptr && obj->as_NewInlineTypeInstance()->in_larval_state()) {\n+    new_instance = obj->as_NewInlineTypeInstance();\n+    apush(append_split(new_instance));\n+  } else {\n+    new_instance = new NewInlineTypeInstance(holder->as_inline_klass(), state_before);\n+    _memory->new_instance(new_instance);\n+    apush(append_split(new_instance));\n+\n+    \/\/ Initialize fields which are not modified\n+    for (int i = 0; i < holder->nof_nonstatic_fields(); i++) {\n+      ciField* field = holder->nonstatic_field_at(i);\n+      int offset = field->offset_in_bytes();\n+      \/\/ Don't use offset_modify here, it might be set to -1 if needs_patching\n+      if (offset != field_modify->offset_in_bytes()) {\n+        if (field->is_flat()) {\n+          ciInlineKlass* vk = field->type()->as_inline_klass();\n+          if (!vk->is_empty()) {\n+            copy_inline_content(vk, obj, offset, new_instance, vk->first_field_offset(), state_before, field);\n+          }\n+        } else {\n+          LoadField* load = new LoadField(obj, offset, field, false, state_before, false);\n+          Value replacement = append(load);\n+          StoreField* store = new StoreField(new_instance, offset, field, replacement, false, state_before, false);\n+          append(store);\n+        }\n+      }\n+    }\n+  }\n+\n+  \/\/ Field to modify\n+  if (field_type == T_BOOLEAN) {\n+    Value mask = append(new Constant(new IntConstant(1)));\n+    val = append(new LogicOp(Bytecodes::_iand, val, mask));\n+  }\n+  if (field_modify->is_flat()) {\n+    assert(!needs_patching, \"Can't patch flat inline type field access\");\n+    ciInlineKlass* vk = field_modify->type()->as_inline_klass();\n+    if (!vk->is_empty()) {\n+      copy_inline_content(vk, val, vk->first_field_offset(), new_instance, offset_modify, state_before, field_modify);\n+    }\n+  } else {\n+    StoreField* store = new StoreField(new_instance, offset_modify, field_modify, val, false, state_before, needs_patching);\n+    append(store);\n+  }\n+}\n@@ -1986,1 +2340,1 @@\n-    } else if (bc_raw == Bytecodes::_invokespecial && !target->is_object_initializer() && calling_klass->is_interface()) {\n+    } else if (bc_raw == Bytecodes::_invokespecial && !target->is_object_constructor() && calling_klass->is_interface()) {\n@@ -2222,1 +2576,2 @@\n-  Invoke* result = new Invoke(code, result_type, recv, args, target, state_before);\n+  Invoke* result = new Invoke(code, result_type, recv, args, target, state_before,\n+                              declared_signature->returns_null_free_inline_type());\n@@ -2244,0 +2599,11 @@\n+void GraphBuilder::default_value(int klass_index) {\n+  bool will_link;\n+  ciKlass* klass = stream()->get_klass(will_link);\n+  if (!stream()->is_unresolved_klass() && klass->is_inlinetype() &&\n+      klass->as_inline_klass()->is_initialized()) {\n+    ciInlineKlass* vk = klass->as_inline_klass();\n+    apush(append(new Constant(new InstanceConstant(vk->default_instance()))));\n+  } else {\n+    apush(append_split(new Deoptimize(klass, copy_state_before())));\n+  }\n+}\n@@ -2253,0 +2619,1 @@\n+  bool null_free = stream()->has_Q_signature();\n@@ -2254,1 +2621,1 @@\n-  NewArray* n = new NewObjectArray(klass, ipop(), state_before);\n+  NewArray* n = new NewObjectArray(klass, ipop(), state_before, null_free);\n@@ -2278,0 +2645,1 @@\n+  bool null_free = stream()->has_Q_signature();\n@@ -2279,1 +2647,1 @@\n-  CheckCast* c = new CheckCast(klass, apop(), state_before);\n+  CheckCast* c = new CheckCast(klass, apop(), state_before, null_free);\n@@ -2317,0 +2685,19 @@\n+  bool maybe_inlinetype = false;\n+  if (bci == InvocationEntryBci) {\n+    \/\/ Called by GraphBuilder::inline_sync_entry.\n+#ifdef ASSERT\n+    ciType* obj_type = x->declared_type();\n+    assert(obj_type == nullptr || !obj_type->is_inlinetype(), \"inline types cannot have synchronized methods\");\n+#endif\n+  } else {\n+    \/\/ We are compiling a monitorenter bytecode\n+    if (EnableValhalla) {\n+      ciType* obj_type = x->declared_type();\n+      if (obj_type == nullptr || obj_type->as_klass()->can_be_inline_klass()) {\n+        \/\/ If we're (possibly) locking on an inline type, check for markWord::always_locked_pattern\n+        \/\/ and throw IMSE. (obj_type is null for Phi nodes, so let's just be conservative).\n+        maybe_inlinetype = true;\n+      }\n+    }\n+  }\n+\n@@ -2320,1 +2707,1 @@\n-  append_with_bci(new MonitorEnter(x, state()->lock(x), state_before), bci);\n+  append_with_bci(new MonitorEnter(x, state()->lock(x), state_before, maybe_inlinetype), bci);\n@@ -2455,1 +2842,1 @@\n-  if (value->as_NewArray() != nullptr || value->as_NewInstance() != nullptr) {\n+  if (value->as_NewArray() != nullptr || value->as_NewInstance() != nullptr || value->as_NewInlineTypeInstance() != nullptr) {\n@@ -2468,0 +2855,1 @@\n+    if (value->is_null_free()) return;\n@@ -2493,1 +2881,3 @@\n-    assert(cur_bci == SynchronizationEntryBCI || cur_bci == cur_scope_data->stream()->cur_bci(), \"invalid bci\");\n+    assert(cur_bci == SynchronizationEntryBCI || cur_bci == cur_scope_data->stream()->cur_bci()\n+           || has_pending_field_access() || has_pending_load_indexed(), \"invalid bci\");\n+\n@@ -2981,0 +3371,2 @@\n+      case Bytecodes::_aconst_init   : default_value(s.get_index_u2()); break;\n+      case Bytecodes::_withfield      : withfield(s.get_index_u2()); break;\n@@ -3269,1 +3661,2 @@\n-    state->store_local(idx, new Local(method()->holder(), objectType, idx, true));\n+    state->store_local(idx, new Local(method()->holder(), objectType, idx,\n+             \/*receiver*\/ true, \/*null_free*\/ method()->holder()->is_flat_array_klass()));\n@@ -3281,1 +3674,1 @@\n-    state->store_local(idx, new Local(type, vt, idx, false));\n+    state->store_local(idx, new Local(type, vt, idx, false, sig->is_null_free_at(i)));\n@@ -3301,0 +3694,2 @@\n+  , _pending_field_access(nullptr)\n+  , _pending_load_indexed(nullptr)\n","filename":"src\/hotspot\/share\/c1\/c1_GraphBuilder.cpp","additions":470,"deletions":75,"binary":false,"changes":545,"status":"modified"},{"patch":"@@ -34,0 +34,2 @@\n+#include \"ci\/ciFlatArrayKlass.hpp\"\n+#include \"ci\/ciInlineKlass.hpp\"\n@@ -216,0 +218,2 @@\n+  assert(!_gen->in_conditional_code(), \"LIRItem cannot be loaded in conditional code\");\n+\n@@ -611,1 +615,2 @@\n-void LIRGenerator::monitor_enter(LIR_Opr object, LIR_Opr lock, LIR_Opr hdr, LIR_Opr scratch, int monitor_no, CodeEmitInfo* info_for_exception, CodeEmitInfo* info) {\n+void LIRGenerator::monitor_enter(LIR_Opr object, LIR_Opr lock, LIR_Opr hdr, LIR_Opr scratch, int monitor_no,\n+                                 CodeEmitInfo* info_for_exception, CodeEmitInfo* info, CodeStub* throw_imse_stub) {\n@@ -614,1 +619,1 @@\n-  CodeStub* slow_path = new MonitorEnterStub(object, lock, info);\n+  CodeStub* slow_path = new MonitorEnterStub(object, lock, info, throw_imse_stub, scratch);\n@@ -617,1 +622,1 @@\n-  __ lock_object(hdr, object, lock, scratch, slow_path, info_for_exception);\n+  __ lock_object(hdr, object, lock, scratch, slow_path, info_for_exception, throw_imse_stub);\n@@ -641,4 +646,9 @@\n-void LIRGenerator::new_instance(LIR_Opr dst, ciInstanceKlass* klass, bool is_unresolved, LIR_Opr scratch1, LIR_Opr scratch2, LIR_Opr scratch3, LIR_Opr scratch4, LIR_Opr klass_reg, CodeEmitInfo* info) {\n-  klass2reg_with_patching(klass_reg, klass, info, is_unresolved);\n-  \/\/ If klass is not loaded we do not know if the klass has finalizers:\n-  if (UseFastNewInstance && klass->is_loaded()\n+void LIRGenerator::new_instance(LIR_Opr dst, ciInstanceKlass* klass, bool is_unresolved, bool allow_inline, LIR_Opr scratch1, LIR_Opr scratch2, LIR_Opr scratch3, LIR_Opr scratch4, LIR_Opr klass_reg, CodeEmitInfo* info) {\n+  if (allow_inline) {\n+    assert(!is_unresolved && klass->is_loaded(), \"inline type klass should be resolved\");\n+    __ metadata2reg(klass->constant_encoding(), klass_reg);\n+  } else {\n+    klass2reg_with_patching(klass_reg, klass, info, is_unresolved);\n+  }\n+  \/\/ If klass is not loaded we do not know if the klass has finalizers or is an unexpected inline klass\n+  if (UseFastNewInstance && klass->is_loaded() && (allow_inline || !klass->is_inlinetype())\n@@ -658,2 +668,2 @@\n-    CodeStub* slow_path = new NewInstanceStub(klass_reg, dst, klass, info, Runtime1::new_instance_id);\n-    __ branch(lir_cond_always, slow_path);\n+    CodeStub* slow_path = new NewInstanceStub(klass_reg, dst, klass, info, allow_inline ? Runtime1::new_instance_id : Runtime1::new_instance_no_inline_id);\n+    __ jump(slow_path);\n@@ -759,0 +769,10 @@\n+  if (!src->is_loaded_flat_array() && !dst->is_loaded_flat_array()) {\n+    flags &= ~LIR_OpArrayCopy::always_slow_path;\n+  }\n+  if (!src->maybe_flat_array()) {\n+    flags &= ~LIR_OpArrayCopy::src_inlinetype_check;\n+  }\n+  if (!dst->maybe_flat_array() && !dst->maybe_null_free_array()) {\n+    flags &= ~LIR_OpArrayCopy::dst_inlinetype_check;\n+  }\n+\n@@ -1564,2 +1584,4 @@\n-  _constants.append(c);\n-  _reg_for_constants.append(result);\n+  if (!in_conditional_code()) {\n+    _constants.append(c);\n+    _reg_for_constants.append(result);\n+  }\n@@ -1569,0 +1591,6 @@\n+void LIRGenerator::set_in_conditional_code(bool v) {\n+  assert(v != _in_conditional_code, \"must change state\");\n+  _in_conditional_code = v;\n+}\n+\n+\n@@ -1660,0 +1688,5 @@\n+  if (!inline_type_field_access_prolog(x)) {\n+    \/\/ Field store will always deopt due to unloaded field or holder klass\n+    return;\n+  }\n+\n@@ -1681,0 +1714,173 @@\n+\/\/ FIXME -- I can't find any other way to pass an address to access_load_at().\n+class TempResolvedAddress: public Instruction {\n+ public:\n+  TempResolvedAddress(ValueType* type, LIR_Opr addr) : Instruction(type) {\n+    set_operand(addr);\n+  }\n+  virtual void input_values_do(ValueVisitor*) {}\n+  virtual void visit(InstructionVisitor* v)   {}\n+  virtual const char* name() const  { return \"TempResolvedAddress\"; }\n+};\n+\n+LIR_Opr LIRGenerator::get_and_load_element_address(LIRItem& array, LIRItem& index) {\n+  ciType* array_type = array.value()->declared_type();\n+  ciFlatArrayKlass* flat_array_klass = array_type->as_flat_array_klass();\n+  assert(flat_array_klass->is_loaded(), \"must be\");\n+\n+  int array_header_size = flat_array_klass->array_header_in_bytes();\n+  int shift = flat_array_klass->log2_element_size();\n+\n+#ifndef _LP64\n+  LIR_Opr index_op = new_register(T_INT);\n+  \/\/ FIXME -- on 32-bit, the shift below can overflow, so we need to check that\n+  \/\/ the top (shift+1) bits of index_op must be zero, or\n+  \/\/ else throw ArrayIndexOutOfBoundsException\n+  if (index.result()->is_constant()) {\n+    jint const_index = index.result()->as_jint();\n+    __ move(LIR_OprFact::intConst(const_index << shift), index_op);\n+  } else {\n+    __ shift_left(index_op, shift, index.result());\n+  }\n+#else\n+  LIR_Opr index_op = new_register(T_LONG);\n+  if (index.result()->is_constant()) {\n+    jint const_index = index.result()->as_jint();\n+    __ move(LIR_OprFact::longConst(const_index << shift), index_op);\n+  } else {\n+    __ convert(Bytecodes::_i2l, index.result(), index_op);\n+    \/\/ Need to shift manually, as LIR_Address can scale only up to 3.\n+    __ shift_left(index_op, shift, index_op);\n+  }\n+#endif\n+\n+  LIR_Opr elm_op = new_pointer_register();\n+  LIR_Address* elm_address = generate_address(array.result(), index_op, 0, array_header_size, T_ADDRESS);\n+  __ leal(LIR_OprFact::address(elm_address), elm_op);\n+  return elm_op;\n+}\n+\n+void LIRGenerator::access_sub_element(LIRItem& array, LIRItem& index, LIR_Opr& result, ciField* field, int sub_offset) {\n+  assert(field != nullptr, \"Need a subelement type specified\");\n+\n+  \/\/ Find the starting address of the source (inside the array)\n+  LIR_Opr elm_op = get_and_load_element_address(array, index);\n+\n+  BasicType subelt_type = field->type()->basic_type();\n+  TempResolvedAddress* elm_resolved_addr = new TempResolvedAddress(as_ValueType(subelt_type), elm_op);\n+  LIRItem elm_item(elm_resolved_addr, this);\n+\n+  DecoratorSet decorators = IN_HEAP;\n+  access_load_at(decorators, subelt_type,\n+                     elm_item, LIR_OprFact::intConst(sub_offset), result,\n+                     nullptr, nullptr);\n+\n+  if (field->is_null_free()) {\n+    assert(field->type()->is_loaded(), \"Must be\");\n+    assert(field->type()->is_inlinetype(), \"Must be if loaded\");\n+    assert(field->type()->as_inline_klass()->is_initialized(), \"Must be\");\n+    LabelObj* L_end = new LabelObj();\n+    __ cmp(lir_cond_notEqual, result, LIR_OprFact::oopConst(nullptr));\n+    __ branch(lir_cond_notEqual, L_end->label());\n+    set_in_conditional_code(true);\n+    Constant* default_value = new Constant(new InstanceConstant(field->type()->as_inline_klass()->default_instance()));\n+    if (default_value->is_pinned()) {\n+      __ move(LIR_OprFact::value_type(default_value->type()), result);\n+    } else {\n+      __ move(load_constant(default_value), result);\n+    }\n+    __ branch_destination(L_end->label());\n+    set_in_conditional_code(false);\n+  }\n+}\n+\n+void LIRGenerator::access_flat_array(bool is_load, LIRItem& array, LIRItem& index, LIRItem& obj_item,\n+                                          ciField* field, int sub_offset) {\n+  assert(sub_offset == 0 || field != nullptr, \"Sanity check\");\n+\n+  \/\/ Find the starting address of the source (inside the array)\n+  LIR_Opr elm_op = get_and_load_element_address(array, index);\n+\n+  ciInlineKlass* elem_klass = nullptr;\n+  if (field != nullptr) {\n+    elem_klass = field->type()->as_inline_klass();\n+  } else {\n+    elem_klass = array.value()->declared_type()->as_flat_array_klass()->element_klass()->as_inline_klass();\n+  }\n+  for (int i = 0; i < elem_klass->nof_nonstatic_fields(); i++) {\n+    ciField* inner_field = elem_klass->nonstatic_field_at(i);\n+    assert(!inner_field->is_flat(), \"flat fields must have been expanded\");\n+    int obj_offset = inner_field->offset_in_bytes();\n+    int elm_offset = obj_offset - elem_klass->first_field_offset() + sub_offset; \/\/ object header is not stored in array.\n+    BasicType field_type = inner_field->type()->basic_type();\n+\n+    \/\/ Types which are smaller than int are still passed in an int register.\n+    BasicType reg_type = field_type;\n+    switch (reg_type) {\n+    case T_BYTE:\n+    case T_BOOLEAN:\n+    case T_SHORT:\n+    case T_CHAR:\n+      reg_type = T_INT;\n+      break;\n+    default:\n+      break;\n+    }\n+\n+    LIR_Opr temp = new_register(reg_type);\n+    TempResolvedAddress* elm_resolved_addr = new TempResolvedAddress(as_ValueType(field_type), elm_op);\n+    LIRItem elm_item(elm_resolved_addr, this);\n+\n+    DecoratorSet decorators = IN_HEAP;\n+    if (is_load) {\n+      access_load_at(decorators, field_type,\n+                     elm_item, LIR_OprFact::intConst(elm_offset), temp,\n+                     nullptr, nullptr);\n+      access_store_at(decorators, field_type,\n+                      obj_item, LIR_OprFact::intConst(obj_offset), temp,\n+                      nullptr, nullptr);\n+    } else {\n+      access_load_at(decorators, field_type,\n+                     obj_item, LIR_OprFact::intConst(obj_offset), temp,\n+                     nullptr, nullptr);\n+      access_store_at(decorators, field_type,\n+                      elm_item, LIR_OprFact::intConst(elm_offset), temp,\n+                      nullptr, nullptr);\n+    }\n+  }\n+}\n+\n+void LIRGenerator::check_flat_array(LIR_Opr array, LIR_Opr value, CodeStub* slow_path) {\n+  LIR_Opr tmp = new_register(T_METADATA);\n+  __ check_flat_array(array, value, tmp, slow_path);\n+}\n+\n+void LIRGenerator::check_null_free_array(LIRItem& array, LIRItem& value, CodeEmitInfo* info) {\n+  LabelObj* L_end = new LabelObj();\n+  LIR_Opr tmp = new_register(T_METADATA);\n+  __ check_null_free_array(array.result(), tmp);\n+  __ branch(lir_cond_equal, L_end->label());\n+  __ null_check(value.result(), info);\n+  __ branch_destination(L_end->label());\n+}\n+\n+bool LIRGenerator::needs_flat_array_store_check(StoreIndexed* x) {\n+  if (x->elt_type() == T_OBJECT && x->array()->maybe_flat_array()) {\n+    ciType* type = x->value()->declared_type();\n+    if (type != nullptr && type->is_klass()) {\n+      ciKlass* klass = type->as_klass();\n+      if (!klass->can_be_inline_klass() || (klass->is_inlinetype() && !klass->as_inline_klass()->flat_in_array())) {\n+        \/\/ This is known to be a non-flat object. If the array is a flat array,\n+        \/\/ it will be caught by the code generated by array_store_check().\n+        return false;\n+      }\n+    }\n+    \/\/ We're not 100% sure, so let's do the flat_array_store_check.\n+    return true;\n+  }\n+  return false;\n+}\n+\n+bool LIRGenerator::needs_null_free_array_store_check(StoreIndexed* x) {\n+  return x->elt_type() == T_OBJECT && x->array()->maybe_null_free_array();\n+}\n+\n@@ -1683,0 +1889,2 @@\n+  assert(x->elt_type() != T_ARRAY, \"never used\");\n+  bool is_loaded_flat_array = x->array()->is_loaded_flat_array();\n@@ -1686,3 +1894,3 @@\n-  bool needs_store_check = obj_store && (x->value()->as_Constant() == nullptr ||\n-                                         !get_jobject_constant(x->value())->is_null_object() ||\n-                                         x->should_profile());\n+  bool needs_store_check = obj_store && !(is_loaded_flat_array && x->is_exact_flat_array_store()) &&\n+                                        (x->value()->as_Constant() == nullptr ||\n+                                         !get_jobject_constant(x->value())->is_null_object());\n@@ -1701,2 +1909,3 @@\n-\n-  if (needs_store_check || x->check_boolean()) {\n+\n+  if (needs_store_check || x->check_boolean()\n+      || is_loaded_flat_array || needs_flat_array_store_check(x) || needs_null_free_array_store_check(x)) {\n@@ -1731,0 +1940,16 @@\n+  if (x->should_profile()) {\n+    if (x->array()->is_loaded_flat_array()) {\n+      \/\/ No need to profile a store to a flat array of known type. This can happen if\n+      \/\/ the type only became known after optimizations (for example, after the PhiSimplifier).\n+      x->set_should_profile(false);\n+    } else {\n+      ciMethodData* md = nullptr;\n+      ciArrayLoadStoreData* load_store = nullptr;\n+      profile_array_type(x, md, load_store);\n+      if (x->array()->maybe_null_free_array()) {\n+        profile_null_free_array(array, md, load_store);\n+      }\n+      profile_element_type(x->value(), md, load_store);\n+    }\n+  }\n+\n@@ -1733,1 +1958,1 @@\n-    array_store_check(value.result(), array.result(), store_check_info, x->profiled_method(), x->profiled_bci());\n+    array_store_check(value.result(), array.result(), store_check_info, nullptr, -1);\n@@ -1736,4 +1961,21 @@\n-  DecoratorSet decorators = IN_HEAP | IS_ARRAY;\n-  if (x->check_boolean()) {\n-    decorators |= C1_MASK_BOOLEAN;\n-  }\n+  if (is_loaded_flat_array) {\n+    if (!x->value()->is_null_free()) {\n+      __ null_check(value.result(), new CodeEmitInfo(range_check_info));\n+    }\n+    \/\/ If array element is an empty inline type, no need to copy anything\n+    if (!x->array()->declared_type()->as_flat_array_klass()->element_klass()->as_inline_klass()->is_empty()) {\n+      access_flat_array(false, array, index, value);\n+    }\n+  } else {\n+    StoreFlattenedArrayStub* slow_path = nullptr;\n+\n+    if (needs_flat_array_store_check(x)) {\n+      \/\/ Check if we indeed have a flat array\n+      index.load_item();\n+      slow_path = new StoreFlattenedArrayStub(array.result(), index.result(), value.result(), state_for(x, x->state_before()));\n+      check_flat_array(array.result(), value.result(), slow_path);\n+      set_in_conditional_code(true);\n+    } else if (needs_null_free_array_store_check(x)) {\n+      CodeEmitInfo* info = new CodeEmitInfo(range_check_info);\n+      check_null_free_array(array, value, info);\n+    }\n@@ -1741,2 +1983,12 @@\n-  access_store_at(decorators, x->elt_type(), array, index.result(), value.result(),\n-                  nullptr, null_check_info);\n+    DecoratorSet decorators = IN_HEAP | IS_ARRAY;\n+    if (x->check_boolean()) {\n+      decorators |= C1_MASK_BOOLEAN;\n+    }\n+\n+    access_store_at(decorators, x->elt_type(), array, index.result(), value.result(),\n+                    nullptr, null_check_info);\n+    if (slow_path != nullptr) {\n+      __ branch_destination(slow_path->continuation());\n+      set_in_conditional_code(false);\n+    }\n+  }\n@@ -1823,0 +2075,25 @@\n+bool LIRGenerator::inline_type_field_access_prolog(AccessField* x) {\n+  ciField* field = x->field();\n+  assert(!field->is_flat(), \"Flattened field access should have been expanded\");\n+  if (!field->is_null_free()) {\n+    return true; \/\/ Not an inline type field\n+  }\n+  \/\/ Deoptimize if the access is non-static and requires patching (holder not loaded\n+  \/\/ or not accessible) because then we only have partial field information and the\n+  \/\/ field could be flat (see ciField constructor).\n+  bool could_be_flat = !x->is_static() && x->needs_patching();\n+  \/\/ Deoptimize if we load from a static field with an uninitialized type because we\n+  \/\/ need to throw an exception if initialization of the type failed.\n+  bool not_initialized = x->is_static() && x->as_LoadField() != nullptr &&\n+      !field->type()->as_instance_klass()->is_initialized();\n+  if (could_be_flat || not_initialized) {\n+    CodeEmitInfo* info = state_for(x, x->state_before());\n+    CodeStub* stub = new DeoptimizeStub(new CodeEmitInfo(info),\n+                                        Deoptimization::Reason_unloaded,\n+                                        Deoptimization::Action_make_not_entrant);\n+    __ jump(stub);\n+    return false;\n+  }\n+  return true;\n+}\n+\n@@ -1852,0 +2129,7 @@\n+  if (!inline_type_field_access_prolog(x)) {\n+    \/\/ Field load will always deopt due to unloaded field or holder klass\n+    LIR_Opr result = rlock_result(x, field_type);\n+    __ move(LIR_OprFact::oopConst(nullptr), result);\n+    return;\n+  }\n+\n@@ -1880,0 +2164,34 @@\n+\n+  ciField* field = x->field();\n+  if (field->is_null_free()) {\n+    \/\/ Load from non-flat inline type field requires\n+    \/\/ a null check to replace null with the default value.\n+    ciInstanceKlass* holder = field->holder();\n+    if (field->is_static() && holder->is_loaded()) {\n+      ciObject* val = holder->java_mirror()->field_value(field).as_object();\n+      if (!val->is_null_object()) {\n+        \/\/ Static field is initialized, we don't need to perform a null check.\n+        return;\n+      }\n+    }\n+    ciInlineKlass* inline_klass = field->type()->as_inline_klass();\n+    if (inline_klass->is_initialized()) {\n+      LabelObj* L_end = new LabelObj();\n+      __ cmp(lir_cond_notEqual, result, LIR_OprFact::oopConst(nullptr));\n+      __ branch(lir_cond_notEqual, L_end->label());\n+      set_in_conditional_code(true);\n+      Constant* default_value = new Constant(new InstanceConstant(inline_klass->default_instance()));\n+      if (default_value->is_pinned()) {\n+        __ move(LIR_OprFact::value_type(default_value->type()), result);\n+      } else {\n+        __ move(load_constant(default_value), result);\n+      }\n+      __ branch_destination(L_end->label());\n+      set_in_conditional_code(false);\n+    } else {\n+      info = state_for(x, x->state_before());\n+      __ cmp(lir_cond_equal, result, LIR_OprFact::oopConst(nullptr));\n+      __ branch(lir_cond_equal, new DeoptimizeStub(info, Deoptimization::Reason_uninitialized,\n+                                                         Deoptimization::Action_make_not_entrant));\n+    }\n+  }\n@@ -2024,1 +2342,68 @@\n-  DecoratorSet decorators = IN_HEAP | IS_ARRAY;\n+  ciMethodData* md = nullptr;\n+  ciArrayLoadStoreData* load_store = nullptr;\n+  if (x->should_profile()) {\n+    if (x->array()->is_loaded_flat_array()) {\n+      \/\/ No need to profile a load from a flat array of known type. This can happen if\n+      \/\/ the type only became known after optimizations (for example, after the PhiSimplifier).\n+      x->set_should_profile(false);\n+    } else {\n+      profile_array_type(x, md, load_store);\n+    }\n+  }\n+\n+  Value element;\n+  if (x->vt() != nullptr) {\n+    assert(x->array()->is_loaded_flat_array(), \"must be\");\n+    \/\/ Find the destination address (of the NewInlineTypeInstance).\n+    LIRItem obj_item(x->vt(), this);\n+\n+    access_flat_array(true, array, index, obj_item,\n+                      x->delayed() == nullptr ? 0 : x->delayed()->field(),\n+                      x->delayed() == nullptr ? 0 : x->delayed()->offset());\n+    set_no_result(x);\n+  } else if (x->delayed() != nullptr) {\n+    assert(x->array()->is_loaded_flat_array(), \"must be\");\n+    LIR_Opr result = rlock_result(x, x->delayed()->field()->type()->basic_type());\n+    access_sub_element(array, index, result, x->delayed()->field(), x->delayed()->offset());\n+  } else if (x->array() != nullptr && x->array()->is_loaded_flat_array() &&\n+             x->array()->declared_type()->as_flat_array_klass()->element_klass()->as_inline_klass()->is_initialized() &&\n+             x->array()->declared_type()->as_flat_array_klass()->element_klass()->as_inline_klass()->is_empty()) {\n+    \/\/ Load the default instance instead of reading the element\n+    ciInlineKlass* elem_klass = x->array()->declared_type()->as_flat_array_klass()->element_klass()->as_inline_klass();\n+    LIR_Opr result = rlock_result(x, x->elt_type());\n+    assert(elem_klass->is_initialized(), \"Must be\");\n+    Constant* default_value = new Constant(new InstanceConstant(elem_klass->default_instance()));\n+    if (default_value->is_pinned()) {\n+      __ move(LIR_OprFact::value_type(default_value->type()), result);\n+    } else {\n+      __ move(load_constant(default_value), result);\n+    }\n+  } else {\n+    LIR_Opr result = rlock_result(x, x->elt_type());\n+    LoadFlattenedArrayStub* slow_path = nullptr;\n+\n+    if (x->should_profile() && x->array()->maybe_null_free_array()) {\n+      profile_null_free_array(array, md, load_store);\n+    }\n+\n+    if (x->elt_type() == T_OBJECT && x->array()->maybe_flat_array()) {\n+      assert(x->delayed() == nullptr, \"Delayed LoadIndexed only apply to loaded_flat_arrays\");\n+      index.load_item();\n+      \/\/ if we are loading from a flat array, load it using a runtime call\n+      slow_path = new LoadFlattenedArrayStub(array.result(), index.result(), result, state_for(x, x->state_before()));\n+      check_flat_array(array.result(), LIR_OprFact::illegalOpr, slow_path);\n+      set_in_conditional_code(true);\n+    }\n+\n+    DecoratorSet decorators = IN_HEAP | IS_ARRAY;\n+    access_load_at(decorators, x->elt_type(),\n+                   array, index.result(), result,\n+                   nullptr, null_check_info);\n+\n+    if (slow_path != nullptr) {\n+      __ branch_destination(slow_path->continuation());\n+      set_in_conditional_code(false);\n+    }\n+\n+    element = x;\n+  }\n@@ -2026,4 +2411,3 @@\n-  LIR_Opr result = rlock_result(x, x->elt_type());\n-  access_load_at(decorators, x->elt_type(),\n-                 array, index.result(), result,\n-                 nullptr, null_check_info);\n+  if (x->should_profile()) {\n+    profile_element_type(element, md, load_store);\n+  }\n@@ -2032,0 +2416,12 @@\n+void LIRGenerator::do_Deoptimize(Deoptimize* x) {\n+  \/\/ This happens only when a class X uses the withfield\/aconst_init bytecode\n+  \/\/ to refer to an inline class V, where V has not yet been loaded\/resolved.\n+  \/\/ This is not a common case. Let's just deoptimize.\n+  CodeEmitInfo* info = state_for(x, x->state_before());\n+  CodeStub* stub = new DeoptimizeStub(new CodeEmitInfo(info),\n+                                      Deoptimization::Reason_unloaded,\n+                                      Deoptimization::Action_make_not_entrant);\n+  __ jump(stub);\n+  LIR_Opr reg = rlock_result(x, T_OBJECT);\n+  __ move(LIR_OprFact::oopConst(nullptr), reg);\n+}\n@@ -2531,1 +2927,1 @@\n-  if (do_update) {\n+  if (do_update && signature_at_call_k != nullptr) {\n@@ -2616,0 +3012,46 @@\n+void LIRGenerator::profile_flags(ciMethodData* md, ciProfileData* data, int flag, LIR_Condition condition) {\n+  assert(md != nullptr && data != nullptr, \"should have been initialized\");\n+  LIR_Opr mdp = new_register(T_METADATA);\n+  __ metadata2reg(md->constant_encoding(), mdp);\n+  LIR_Address* addr = new LIR_Address(mdp, md->byte_offset_of_slot(data, DataLayout::flags_offset()), T_BYTE);\n+  LIR_Opr flags = new_register(T_INT);\n+  __ move(addr, flags);\n+  if (condition != lir_cond_always) {\n+    LIR_Opr update = new_register(T_INT);\n+    __ cmove(condition, LIR_OprFact::intConst(0), LIR_OprFact::intConst(flag), update, T_INT);\n+  } else {\n+    __ logical_or(flags, LIR_OprFact::intConst(flag), flags);\n+  }\n+  __ store(flags, addr);\n+}\n+\n+void LIRGenerator::profile_null_free_array(LIRItem array, ciMethodData* md, ciArrayLoadStoreData* load_store) {\n+  assert(compilation()->profile_array_accesses(), \"array access profiling is disabled\");\n+  LabelObj* L_end = new LabelObj();\n+  LIR_Opr tmp = new_register(T_METADATA);\n+  __ check_null_free_array(array.result(), tmp);\n+\n+  profile_flags(md, load_store, ArrayLoadStoreData::null_free_array_byte_constant(), lir_cond_equal);\n+}\n+\n+void LIRGenerator::profile_array_type(AccessIndexed* x, ciMethodData*& md, ciArrayLoadStoreData*& load_store) {\n+  assert(compilation()->profile_array_accesses(), \"array access profiling is disabled\");\n+  int bci = x->profiled_bci();\n+  md = x->profiled_method()->method_data();\n+  assert(md != nullptr, \"Sanity\");\n+  ciProfileData* data = md->bci_to_data(bci);\n+  assert(data != nullptr && data->is_ArrayLoadStoreData(), \"incorrect profiling entry\");\n+  load_store = (ciArrayLoadStoreData*)data;\n+  LIR_Opr mdp = LIR_OprFact::illegalOpr;\n+  profile_type(md, md->byte_offset_of_slot(load_store, ArrayLoadStoreData::array_offset()), 0,\n+               load_store->array()->type(), x->array(), mdp, true, nullptr, nullptr);\n+}\n+\n+void LIRGenerator::profile_element_type(Value element, ciMethodData* md, ciArrayLoadStoreData* load_store) {\n+  assert(compilation()->profile_array_accesses(), \"array access profiling is disabled\");\n+  assert(md != nullptr && load_store != nullptr, \"should have been initialized\");\n+  LIR_Opr mdp = LIR_OprFact::illegalOpr;\n+  profile_type(md, md->byte_offset_of_slot(load_store, ArrayLoadStoreData::element_offset()), 0,\n+               load_store->element()->type(), element, mdp, false, nullptr, nullptr);\n+}\n+\n@@ -2696,0 +3138,8 @@\n+  if (method()->has_scalarized_args()) {\n+    \/\/ Check if deoptimization was triggered (i.e. orig_pc was set) while buffering scalarized inline type arguments\n+    \/\/ in the entry point (see comments in frame::deoptimize). If so, deoptimize only now that we have the right state.\n+    CodeEmitInfo* info = new CodeEmitInfo(scope()->start()->state()->copy(ValueStack::StateBefore, 0), nullptr, false);\n+    CodeStub* deopt_stub = new DeoptimizeStub(info, Deoptimization::Reason_none, Deoptimization::Action_none);\n+    __ append(new LIR_Op0(lir_check_orig_pc));\n+    __ branch(lir_cond_notEqual, deopt_stub);\n+  }\n@@ -2711,0 +3161,13 @@\n+void LIRGenerator::invoke_load_one_argument(LIRItem* param, LIR_Opr loc) {\n+  if (loc->is_register()) {\n+    param->load_item_force(loc);\n+  } else {\n+    LIR_Address* addr = loc->as_address_ptr();\n+    param->load_for_store(addr->type());\n+    if (addr->type() == T_OBJECT) {\n+      __ move_wide(param->result(), addr);\n+    } else {\n+      __ move(param->result(), addr);\n+    }\n+  }\n+}\n@@ -2718,10 +3181,1 @@\n-    if (loc->is_register()) {\n-      param->load_item_force(loc);\n-    } else {\n-      LIR_Address* addr = loc->as_address_ptr();\n-      param->load_for_store(addr->type());\n-      if (addr->type() == T_OBJECT) {\n-        __ move_wide(param->result(), addr);\n-      } else\n-        __ move(param->result(), addr);\n-    }\n+    invoke_load_one_argument(param, loc);\n@@ -2893,1 +3347,1 @@\n-  if (can_inline_as_constant(right.value())) {\n+  if (can_inline_as_constant(right.value()) && !x->substitutability_check()) {\n@@ -2896,0 +3350,1 @@\n+    \/\/ substitutability_check() needs to use right as a base register.\n@@ -2903,3 +3358,60 @@\n-  LIR_Opr reg = rlock_result(x);\n-  __ cmp(lir_cond(x->cond()), left.result(), right.result());\n-  __ cmove(lir_cond(x->cond()), t_val.result(), f_val.result(), reg, as_BasicType(x->x()->type()));\n+  if (x->substitutability_check()) {\n+    substitutability_check(x, left, right, t_val, f_val);\n+  } else {\n+    LIR_Opr reg = rlock_result(x);\n+    __ cmp(lir_cond(x->cond()), left.result(), right.result());\n+    __ cmove(lir_cond(x->cond()), t_val.result(), f_val.result(), reg, as_BasicType(x->x()->type()));\n+  }\n+}\n+\n+void LIRGenerator::substitutability_check(IfOp* x, LIRItem& left, LIRItem& right, LIRItem& t_val, LIRItem& f_val) {\n+  assert(x->cond() == If::eql || x->cond() == If::neq, \"must be\");\n+  bool is_acmpeq = (x->cond() == If::eql);\n+  LIR_Opr equal_result     = is_acmpeq ? t_val.result() : f_val.result();\n+  LIR_Opr not_equal_result = is_acmpeq ? f_val.result() : t_val.result();\n+  LIR_Opr result = rlock_result(x);\n+  CodeEmitInfo* info = state_for(x, x->state_before());\n+\n+  substitutability_check_common(x->x(), x->y(), left, right, equal_result, not_equal_result, result, info);\n+}\n+\n+void LIRGenerator::substitutability_check(If* x, LIRItem& left, LIRItem& right) {\n+  LIR_Opr equal_result     = LIR_OprFact::intConst(1);\n+  LIR_Opr not_equal_result = LIR_OprFact::intConst(0);\n+  LIR_Opr result = new_register(T_INT);\n+  CodeEmitInfo* info = state_for(x, x->state_before());\n+\n+  substitutability_check_common(x->x(), x->y(), left, right, equal_result, not_equal_result, result, info);\n+\n+  assert(x->cond() == If::eql || x->cond() == If::neq, \"must be\");\n+  __ cmp(lir_cond(x->cond()), result, equal_result);\n+}\n+\n+void LIRGenerator::substitutability_check_common(Value left_val, Value right_val, LIRItem& left, LIRItem& right,\n+                                                 LIR_Opr equal_result, LIR_Opr not_equal_result, LIR_Opr result,\n+                                                 CodeEmitInfo* info) {\n+  LIR_Opr tmp1 = LIR_OprFact::illegalOpr;\n+  LIR_Opr tmp2 = LIR_OprFact::illegalOpr;\n+  LIR_Opr left_klass_op = LIR_OprFact::illegalOpr;\n+  LIR_Opr right_klass_op = LIR_OprFact::illegalOpr;\n+\n+  ciKlass* left_klass  = left_val ->as_loaded_klass_or_null();\n+  ciKlass* right_klass = right_val->as_loaded_klass_or_null();\n+\n+  if ((left_klass == nullptr || right_klass == nullptr) ||\/\/ The klass is still unloaded, or came from a Phi node.\n+      !left_klass->is_inlinetype() || !right_klass->is_inlinetype()) {\n+    init_temps_for_substitutability_check(tmp1, tmp2);\n+  }\n+\n+  if (left_klass != nullptr && left_klass->is_inlinetype() && left_klass == right_klass) {\n+    \/\/ No need to load klass -- the operands are statically known to be the same inline klass.\n+  } else {\n+    BasicType t_klass = UseCompressedOops ? T_INT : T_METADATA;\n+    left_klass_op = new_register(t_klass);\n+    right_klass_op = new_register(t_klass);\n+  }\n+\n+  CodeStub* slow_path = new SubstitutabilityCheckStub(left.result(), right.result(), info);\n+  __ substitutability_check(result, left.result(), right.result(), equal_result, not_equal_result,\n+                            tmp1, tmp2,\n+                            left_klass, right_klass, left_klass_op, right_klass_op, info, slow_path);\n@@ -3180,1 +3692,1 @@\n-    ciReturnTypeEntry* ret = data->is_CallTypeData() ? ((ciCallTypeData*)data)->ret() : ((ciVirtualCallTypeData*)data)->ret();\n+    ciSingleTypeEntry* ret = data->is_CallTypeData() ? ((ciCallTypeData*)data)->ret() : ((ciVirtualCallTypeData*)data)->ret();\n@@ -3201,0 +3713,47 @@\n+bool LIRGenerator::profile_inline_klass(ciMethodData* md, ciProfileData* data, Value value, int flag) {\n+  ciKlass* klass = value->as_loaded_klass_or_null();\n+  if (klass != nullptr) {\n+    if (klass->is_inlinetype()) {\n+      profile_flags(md, data, flag, lir_cond_always);\n+    } else if (klass->can_be_inline_klass()) {\n+      return false;\n+    }\n+  } else {\n+    return false;\n+  }\n+  return true;\n+}\n+\n+\n+void LIRGenerator::do_ProfileACmpTypes(ProfileACmpTypes* x) {\n+  ciMethod* method = x->method();\n+  assert(method != nullptr, \"method should be set if branch is profiled\");\n+  ciMethodData* md = method->method_data_or_null();\n+  assert(md != nullptr, \"Sanity\");\n+  ciProfileData* data = md->bci_to_data(x->bci());\n+  assert(data != nullptr, \"must have profiling data\");\n+  assert(data->is_ACmpData(), \"need BranchData for two-way branches\");\n+  ciACmpData* acmp = (ciACmpData*)data;\n+  LIR_Opr mdp = LIR_OprFact::illegalOpr;\n+  profile_type(md, md->byte_offset_of_slot(acmp, ACmpData::left_offset()), 0,\n+               acmp->left()->type(), x->left(), mdp, !x->left_maybe_null(), nullptr, nullptr);\n+  int flags_offset = md->byte_offset_of_slot(data, DataLayout::flags_offset());\n+  if (!profile_inline_klass(md, acmp, x->left(), ACmpData::left_inline_type_byte_constant())) {\n+    LIR_Opr mdp = new_register(T_METADATA);\n+    __ metadata2reg(md->constant_encoding(), mdp);\n+    LIRItem value(x->left(), this);\n+    value.load_item();\n+    __ profile_inline_type(new LIR_Address(mdp, flags_offset, T_INT), value.result(), ACmpData::left_inline_type_byte_constant(), new_register(T_INT), !x->left_maybe_null());\n+  }\n+  profile_type(md, md->byte_offset_of_slot(acmp, ACmpData::left_offset()),\n+               in_bytes(ACmpData::right_offset()) - in_bytes(ACmpData::left_offset()),\n+               acmp->right()->type(), x->right(), mdp, !x->right_maybe_null(), nullptr, nullptr);\n+  if (!profile_inline_klass(md, acmp, x->right(), ACmpData::right_inline_type_byte_constant())) {\n+    LIR_Opr mdp = new_register(T_METADATA);\n+    __ metadata2reg(md->constant_encoding(), mdp);\n+    LIRItem value(x->right(), this);\n+    value.load_item();\n+    __ profile_inline_type(new LIR_Address(mdp, flags_offset, T_INT), value.result(), ACmpData::right_inline_type_byte_constant(), new_register(T_INT), !x->left_maybe_null());\n+  }\n+}\n+\n","filename":"src\/hotspot\/share\/c1\/c1_LIRGenerator.cpp","additions":603,"deletions":44,"binary":false,"changes":647,"status":"modified"},{"patch":"@@ -53,0 +53,1 @@\n+#include \"runtime\/sharedRuntime.hpp\"\n@@ -662,0 +663,33 @@\n+bool ciMethod::array_access_profiled_type(int bci, ciKlass*& array_type, ciKlass*& element_type, ProfilePtrKind& element_ptr, bool &flat_array, bool &null_free_array) {\n+  if (method_data() != nullptr && method_data()->is_mature()) {\n+    ciProfileData* data = method_data()->bci_to_data(bci);\n+    if (data != nullptr && data->is_ArrayLoadStoreData()) {\n+      ciArrayLoadStoreData* array_access = (ciArrayLoadStoreData*)data->as_ArrayLoadStoreData();\n+      array_type = array_access->array()->valid_type();\n+      element_type = array_access->element()->valid_type();\n+      element_ptr = array_access->element()->ptr_kind();\n+      flat_array = array_access->flat_array();\n+      null_free_array = array_access->null_free_array();\n+      return true;\n+    }\n+  }\n+  return false;\n+}\n+\n+bool ciMethod::acmp_profiled_type(int bci, ciKlass*& left_type, ciKlass*& right_type, ProfilePtrKind& left_ptr, ProfilePtrKind& right_ptr, bool &left_inline_type, bool &right_inline_type) {\n+  if (method_data() != nullptr && method_data()->is_mature()) {\n+    ciProfileData* data = method_data()->bci_to_data(bci);\n+    if (data != nullptr && data->is_ACmpData()) {\n+      ciACmpData* acmp = (ciACmpData*)data->as_ACmpData();\n+      left_type = acmp->left()->valid_type();\n+      right_type = acmp->right()->valid_type();\n+      left_ptr = acmp->left()->ptr_kind();\n+      right_ptr = acmp->right()->ptr_kind();\n+      left_inline_type = acmp->left_inline_type();\n+      right_inline_type = acmp->right_inline_type();\n+      return true;\n+    }\n+  }\n+  return false;\n+}\n+\n@@ -962,1 +996,1 @@\n-\/\/ ciMethod::is_object_initializer\n+\/\/ ciMethod::is_object_constructor\n@@ -964,2 +998,15 @@\n-bool ciMethod::is_object_initializer() const {\n-   return name() == ciSymbols::object_initializer_name();\n+bool ciMethod::is_object_constructor() const {\n+   return (name() == ciSymbols::object_initializer_name()\n+           && signature()->return_type()->is_void());\n+   \/\/ Note:  We can't test is_static, because that would\n+   \/\/ require the method to be loaded.  Sometimes it isn't.\n+}\n+\n+\/\/ ------------------------------------------------------------------\n+\/\/ ciMethod::is_static_vnew_factory\n+\/\/\n+bool ciMethod::is_static_vnew_factory() const {\n+   return (name() == ciSymbols::inline_factory_name()\n+           && !signature()->return_type()->is_void());\n+   \/\/ Note:  We can't test is_static, because that would\n+   \/\/ require the method to be loaded.  Sometimes it isn't.\n@@ -1233,1 +1280,1 @@\n-bool ciMethod::is_initializer () const {         FETCH_FLAG_FROM_VM(is_initializer); }\n+bool ciMethod::is_object_constructor_or_class_initializer() const { FETCH_FLAG_FROM_VM(is_object_constructor_or_class_initializer); }\n@@ -1480,0 +1527,18 @@\n+\n+bool ciMethod::is_scalarized_arg(int idx) const {\n+  VM_ENTRY_MARK;\n+  return get_Method()->is_scalarized_arg(idx);\n+}\n+\n+bool ciMethod::has_scalarized_args() const {\n+  VM_ENTRY_MARK;\n+  return get_Method()->has_scalarized_args();\n+}\n+\n+const GrowableArray<SigEntry>* ciMethod::get_sig_cc() const {\n+  VM_ENTRY_MARK;\n+  if (get_Method()->adapter() == nullptr) {\n+    return nullptr;\n+  }\n+  return get_Method()->adapter()->get_sig_cc();\n+}\n","filename":"src\/hotspot\/share\/ci\/ciMethod.cpp","additions":69,"deletions":4,"binary":false,"changes":73,"status":"modified"},{"patch":"@@ -206,1 +206,1 @@\n-    if (*start == JVM_SIGNATURE_CLASS) {\n+    if (*start == JVM_SIGNATURE_CLASS || *start == JVM_SIGNATURE_PRIMITIVE_OBJECT) {\n","filename":"src\/hotspot\/share\/classfile\/classLoader.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -69,0 +69,1 @@\n+#include \"oops\/inlineKlass.inline.hpp\"\n@@ -450,0 +451,10 @@\n+void ClassLoaderData::inline_classes_do(void f(InlineKlass*)) {\n+  \/\/ Lock-free access requires load_acquire\n+  for (Klass* k = Atomic::load_acquire(&_klasses); k != nullptr; k = k->next_link()) {\n+    if (k->is_inline_klass()) {\n+      f(InlineKlass::cast(k));\n+    }\n+    assert(k != k->next_link(), \"no loops!\");\n+  }\n+}\n+\n@@ -601,0 +612,2 @@\n+  inline_classes_do(InlineKlass::cleanup);\n+\n@@ -895,1 +908,5 @@\n-        MetadataFactory::free_metadata(this, (InstanceKlass*)m);\n+        if (!((Klass*)m)->is_inline_klass()) {\n+          MetadataFactory::free_metadata(this, (InstanceKlass*)m);\n+        } else {\n+          MetadataFactory::free_metadata(this, (InlineKlass*)m);\n+        }\n","filename":"src\/hotspot\/share\/classfile\/classLoaderData.cpp","additions":18,"deletions":1,"binary":false,"changes":19,"status":"modified"},{"patch":"@@ -55,0 +55,2 @@\n+#include \"oops\/flatArrayKlass.hpp\"\n+#include \"oops\/inlineKlass.inline.hpp\"\n@@ -56,1 +58,1 @@\n-#include \"oops\/instanceMirrorKlass.hpp\"\n+#include \"oops\/instanceMirrorKlass.inline.hpp\"\n@@ -790,0 +792,2 @@\n+int java_lang_Class::_primary_mirror_offset;\n+int java_lang_Class::_secondary_mirror_offset;\n@@ -991,1 +995,10 @@\n-    if (k->is_typeArray_klass()) {\n+    if (k->is_flatArray_klass()) {\n+      Klass* element_klass = (Klass*) FlatArrayKlass::cast(k)->element_klass();\n+      assert(element_klass->is_inline_klass(), \"Must be inline type component\");\n+      if (is_scratch) {\n+        comp_mirror = Handle(THREAD, HeapShared::scratch_java_mirror(element_klass));\n+      } else {\n+        InlineKlass* vk = InlineKlass::cast(element_klass);\n+        comp_mirror = Handle(THREAD, vk->val_mirror());\n+      }\n+    } else if (k->is_typeArray_klass()) {\n@@ -1002,0 +1015,5 @@\n+      oop comp_oop = element_klass->java_mirror();\n+      if (element_klass->is_inline_klass()) {\n+        InlineKlass* ik = InlineKlass::cast(element_klass);\n+        comp_oop = k->name()->is_Q_array_signature() ? ik->val_mirror() : ik->ref_mirror();\n+      }\n@@ -1005,1 +1023,1 @@\n-        comp_mirror = Handle(THREAD, element_klass->java_mirror());\n+        comp_mirror = Handle(THREAD, comp_oop);\n@@ -1067,0 +1085,6 @@\n+\n+    if (k->is_inline_klass()) {\n+      oop secondary_mirror = create_secondary_mirror(k, mirror, CHECK);\n+      set_primary_mirror(mirror(), mirror());\n+      set_secondary_mirror(mirror(), secondary_mirror);\n+    }\n@@ -1075,0 +1099,19 @@\n+\/\/ Create the secondary mirror for inline class. Sets all the fields of this java.lang.Class\n+\/\/ instance with the same value as the primary mirror\n+oop java_lang_Class::create_secondary_mirror(Klass* k, Handle mirror, TRAPS) {\n+  assert(k->is_inline_klass(), \"primitive class\");\n+  \/\/ Allocate mirror (java.lang.Class instance)\n+  oop mirror_oop = InstanceMirrorKlass::cast(vmClasses::Class_klass())->allocate_instance(k, CHECK_0);\n+  Handle secondary_mirror(THREAD, mirror_oop);\n+\n+  java_lang_Class::set_klass(secondary_mirror(), k);\n+  java_lang_Class::set_static_oop_field_count(secondary_mirror(), static_oop_field_count(mirror()));\n+\n+  set_protection_domain(secondary_mirror(), protection_domain(mirror()));\n+  set_class_loader(secondary_mirror(), class_loader(mirror()));\n+  \/\/ ## handle if java.base is not yet defined\n+  set_module(secondary_mirror(), module(mirror()));\n+  set_primary_mirror(secondary_mirror(), mirror());\n+  set_secondary_mirror(secondary_mirror(), secondary_mirror());\n+  return secondary_mirror();\n+}\n@@ -1089,3 +1132,8 @@\n-  if (k->class_loader() != nullptr &&\n-      k->class_loader() != SystemDictionary::java_platform_loader() &&\n-      k->class_loader() != SystemDictionary::java_system_loader()) {\n+  \/\/ Inline classes encapsulate two mirror objects, a value mirror (primitive value mirror)\n+  \/\/ and a reference mirror (primitive class mirror), skip over scratch mirror allocation\n+  \/\/ for inline classes, they will not be part of shared archive and will be created while\n+  \/\/ restoring unshared fileds. Refer Klass::restore_unshareable_info() for more details.\n+  if (k->is_inline_klass() ||\n+      (k->class_loader() != nullptr &&\n+       k->class_loader() != SystemDictionary::java_platform_loader() &&\n+       k->class_loader() != SystemDictionary::java_system_loader())) {\n@@ -1200,0 +1248,20 @@\n+oop java_lang_Class::primary_mirror(oop java_class) {\n+  assert(_primary_mirror_offset != 0, \"must be set\");\n+  return java_class->obj_field(_primary_mirror_offset);\n+}\n+\n+void java_lang_Class::set_primary_mirror(oop java_class, oop mirror) {\n+  assert(_primary_mirror_offset != 0, \"must be set\");\n+  java_class->obj_field_put(_primary_mirror_offset, mirror);\n+}\n+\n+oop java_lang_Class::secondary_mirror(oop java_class) {\n+  assert(_secondary_mirror_offset != 0, \"must be set\");\n+  return java_class->obj_field(_secondary_mirror_offset);\n+}\n+\n+void java_lang_Class::set_secondary_mirror(oop java_class, oop mirror) {\n+  assert(_secondary_mirror_offset != 0, \"must be set\");\n+  java_class->obj_field_put(_secondary_mirror_offset, mirror);\n+}\n+\n@@ -1284,0 +1352,1 @@\n+  bool is_Q_descriptor = false;\n@@ -1289,0 +1358,1 @@\n+    is_Q_descriptor = k->is_inline_klass() && is_secondary_mirror(java_class);\n@@ -1295,1 +1365,3 @@\n-  if (is_instance)  st->print(\"L\");\n+  if (is_instance)  {\n+    st->print(is_Q_descriptor ? \"Q\" : \"L\");\n+  }\n@@ -1316,2 +1388,7 @@\n-      const char* sigstr = k->signature_name();\n-      int         siglen = (int) strlen(sigstr);\n+      const char* sigstr;\n+      if (k->is_inline_klass() && is_secondary_mirror(java_class)) {\n+        sigstr = InlineKlass::cast(k)->val_signature_name();\n+      } else {\n+        sigstr = k->signature_name();\n+      }\n+      int siglen = (int) strlen(sigstr);\n@@ -1405,0 +1482,2 @@\n+  macro(_primary_mirror_offset,      k, \"primaryType\",         class_signature,       false); \\\n+  macro(_secondary_mirror_offset,    k, \"secondaryType\",       class_signature,       false); \\\n@@ -2622,1 +2701,1 @@\n-      if (method->name() == vmSymbols::object_initializer_name() &&\n+      if (method->is_object_constructor() &&\n@@ -2981,2 +3060,2 @@\n-  if (m->is_initializer()) {\n-    flags |= java_lang_invoke_MemberName::MN_IS_CONSTRUCTOR;\n+  if (m->is_object_constructor()) {\n+    flags |= java_lang_invoke_MemberName::MN_IS_OBJECT_CONSTRUCTOR;\n@@ -4290,1 +4369,1 @@\n-  return (flags(mname) & (MN_IS_METHOD | MN_IS_CONSTRUCTOR)) > 0;\n+  return (flags(mname) & (MN_IS_METHOD | MN_IS_OBJECT_CONSTRUCTOR)) > 0;\n","filename":"src\/hotspot\/share\/classfile\/javaClasses.cpp","additions":92,"deletions":13,"binary":false,"changes":105,"status":"modified"},{"patch":"@@ -233,0 +233,3 @@\n+  static int _primary_mirror_offset;\n+  static int _secondary_mirror_offset;\n+\n@@ -246,0 +249,3 @@\n+  static void set_primary_mirror(oop java_class, oop comp_mirror);\n+  static void set_secondary_mirror(oop java_class, oop comp_mirror);\n+\n@@ -260,0 +266,1 @@\n+  static oop  create_secondary_mirror(Klass* k, Handle mirror, TRAPS);\n@@ -289,0 +296,3 @@\n+  static int component_mirror_offset()     { CHECK_INIT(_component_mirror_offset); }\n+  static int primary_mirror_offset()       { CHECK_INIT(_primary_mirror_offset); }\n+  static int secondary_mirror_offset()     { CHECK_INIT(_secondary_mirror_offset); }\n@@ -296,0 +306,5 @@\n+  static oop  primary_mirror(oop java_class);\n+  static oop  secondary_mirror(oop java_class);\n+  static bool is_primary_mirror(oop java_class);\n+  static bool is_secondary_mirror(oop java_class);\n+\n@@ -301,2 +316,0 @@\n-  static int component_mirror_offset() { return _component_mirror_offset; }\n-\n@@ -1284,1 +1297,1 @@\n-    MN_IS_CONSTRUCTOR        = 0x00020000, \/\/ constructor\n+    MN_IS_OBJECT_CONSTRUCTOR = 0x00020000, \/\/ constructor\n@@ -1290,0 +1303,1 @@\n+    MN_FLAT_FIELD            = 0x00800000, \/\/ flat field\n@@ -1853,1 +1867,0 @@\n-\n","filename":"src\/hotspot\/share\/classfile\/javaClasses.hpp","additions":17,"deletions":4,"binary":false,"changes":21,"status":"modified"},{"patch":"@@ -306,0 +306,18 @@\n+inline bool java_lang_Class::is_primary_mirror(oop java_class) {\n+  Klass* k = as_Klass(java_class);\n+  if (k->is_inline_klass()) {\n+    return java_class == primary_mirror(java_class);\n+  } else {\n+    return true;\n+  }\n+}\n+\n+inline bool java_lang_Class::is_secondary_mirror(oop java_class) {\n+  Klass* k = as_Klass(java_class);\n+  if (k->is_inline_klass()) {\n+    return java_class == secondary_mirror(java_class);\n+  } else {\n+    return false;\n+  }\n+}\n+\n","filename":"src\/hotspot\/share\/classfile\/javaClasses.inline.hpp","additions":18,"deletions":0,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -58,0 +58,1 @@\n+#include \"oops\/fieldStreams.inline.hpp\"\n@@ -69,0 +70,1 @@\n+#include \"oops\/inlineKlass.inline.hpp\"\n@@ -77,0 +79,1 @@\n+#include \"runtime\/os.hpp\"\n@@ -337,1 +340,1 @@\n-      \/\/ Ignore wrapping L and ;.\n+      \/\/ Ignore wrapping L and ; (and Q and ; for value types).\n@@ -366,1 +369,8 @@\n-      k = k->array_klass(ndims, CHECK_NULL);\n+      if (class_name->is_Q_array_signature()) {\n+        if (!k->is_inline_klass()) {\n+          THROW_MSG_NULL(vmSymbols::java_lang_IncompatibleClassChangeError(), \"L\/Q mismatch on bottom type\");\n+        }\n+        k = InlineKlass::cast(k)->value_array_klass(ndims, CHECK_NULL);\n+      } else {\n+        k = k->array_klass(ndims, CHECK_NULL);\n+      }\n@@ -493,0 +503,43 @@\n+Klass* SystemDictionary::resolve_inline_type_field_or_fail(Symbol* signature,\n+                                                           Handle class_loader,\n+                                                           Handle protection_domain,\n+                                                           bool throw_error,\n+                                                           TRAPS) {\n+  Symbol* class_name = signature->fundamental_name(THREAD);\n+  class_loader = Handle(THREAD, java_lang_ClassLoader::non_reflection_class_loader(class_loader()));\n+  ClassLoaderData* loader_data = class_loader_data(class_loader);\n+  bool throw_circularity_error = false;\n+  PlaceholderEntry* oldprobe;\n+\n+  {\n+    MutexLocker mu(THREAD, SystemDictionary_lock);\n+    oldprobe = PlaceholderTable::get_entry(class_name, loader_data);\n+    if (oldprobe != nullptr &&\n+      oldprobe->check_seen_thread(THREAD, PlaceholderTable::PRIMITIVE_OBJECT_FIELD)) {\n+      throw_circularity_error = true;\n+\n+    } else {\n+      PlaceholderTable::find_and_add(class_name, loader_data,\n+                                   PlaceholderTable::PRIMITIVE_OBJECT_FIELD, nullptr, THREAD);\n+    }\n+  }\n+\n+  Klass* klass = nullptr;\n+  if (!throw_circularity_error) {\n+    klass = SystemDictionary::resolve_or_fail(class_name, class_loader,\n+                                               protection_domain, true, THREAD);\n+  } else {\n+    ResourceMark rm(THREAD);\n+    THROW_MSG_NULL(vmSymbols::java_lang_ClassCircularityError(), class_name->as_C_string());\n+  }\n+\n+  {\n+    MutexLocker mu(THREAD, SystemDictionary_lock);\n+    PlaceholderTable::find_and_remove(class_name, loader_data,\n+                                      PlaceholderTable::PRIMITIVE_OBJECT_FIELD, THREAD);\n+  }\n+\n+  class_name->decrement_refcount();\n+  return klass;\n+}\n+\n@@ -578,1 +631,1 @@\n-         !Signature::has_envelope(name), \"invalid class name\");\n+         !Signature::has_envelope(name), \"invalid class name: %s\", name == nullptr ? \"nullptr\" : name->as_C_string());\n@@ -783,1 +836,1 @@\n-    if (t != T_OBJECT) {\n+    if (t != T_OBJECT && t != T_PRIMITIVE_OBJECT) {\n@@ -789,1 +842,5 @@\n-      k = k->array_klass_or_null(ndims);\n+      if (class_name->is_Q_array_signature()) {\n+        k = InlineKlass::cast(k)->value_array_klass_or_null(ndims);\n+      } else {\n+        k = k->array_klass_or_null(ndims);\n+      }\n@@ -1143,0 +1200,19 @@\n+\n+  if (ik->has_inline_type_fields()) {\n+    for (AllFieldStream fs(ik); !fs.done(); fs.next()) {\n+      Symbol* sig = fs.signature();\n+      if (fs.is_null_free_inline_type()) {\n+        if (!fs.access_flags().is_static()) {\n+          \/\/ Pre-load inline class\n+          Klass* real_k = SystemDictionary::resolve_inline_type_field_or_fail(sig,\n+            class_loader, protection_domain, true, CHECK_NULL);\n+          Klass* k = ik->get_inline_type_field_klass_or_null(fs.index());\n+          if (real_k != k) {\n+            \/\/ oops, the app has substituted a different version of k!\n+            return nullptr;\n+          }\n+        }\n+      }\n+    }\n+  }\n+\n@@ -1178,0 +1254,1 @@\n+\n@@ -1727,1 +1804,1 @@\n-    if (t != T_OBJECT) {\n+    if (t != T_OBJECT && t != T_PRIMITIVE_OBJECT) {\n@@ -1735,1 +1812,5 @@\n-      klass = klass->array_klass_or_null(ndims);\n+      if (class_name->is_Q_array_signature()) {\n+        klass = InlineKlass::cast(klass)->value_array_klass_or_null(ndims);\n+      } else {\n+        klass = klass->array_klass_or_null(ndims);\n+      }\n","filename":"src\/hotspot\/share\/classfile\/systemDictionary.cpp","additions":88,"deletions":7,"binary":false,"changes":95,"status":"modified"},{"patch":"@@ -133,0 +133,1 @@\n+  do_klass(ValueObjectMethods_klass,                    java_lang_runtime_ValueObjectMethods                  ) \\\n","filename":"src\/hotspot\/share\/classfile\/vmClassMacros.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -184,0 +184,1 @@\n+  template(tag_preload,                               \"Preload\")                                  \\\n@@ -394,0 +395,1 @@\n+  template(inline_factory_name,                       \"<vnew>\")                                   \\\n@@ -535,0 +537,2 @@\n+  template(default_value_name,                        \".default\")                                 \\\n+  template(empty_marker_name,                         \".empty\")                                   \\\n@@ -611,0 +615,1 @@\n+  template(class_class_signature,                     \"(Ljava\/lang\/Class;)Ljava\/lang\/Class;\")     \\\n@@ -624,0 +629,1 @@\n+  template(object_object_boolean_signature,           \"(Ljava\/lang\/Object;Ljava\/lang\/Object;)Z\") \\\n@@ -767,0 +773,2 @@\n+  template(primaryType_name,                           \"primaryType\")                                             \\\n+  template(secondaryType_name,                         \"secondaryType\")                                           \\\n@@ -793,0 +801,5 @@\n+  template(java_lang_runtime_ValueObjectMethods,            \"java\/lang\/runtime\/ValueObjectMethods\")               \\\n+  template(isSubstitutable_name,                            \"isSubstitutable\")                                    \\\n+  template(valueObjectHashCode_name,                        \"valueObjectHashCode\")                                \\\n+  template(jdk_internal_value_PrimitiveClass,               \"jdk\/internal\/value\/PrimitiveClass\")                  \\\n+                                                                                                                  \\\n","filename":"src\/hotspot\/share\/classfile\/vmSymbols.hpp","additions":13,"deletions":0,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -384,1 +384,12 @@\n-      signature    = callee->signature();\n+      signature = callee->signature();\n+\n+      \/\/ If inline types are passed as fields, use the extended signature\n+      \/\/ which contains the types of all (oop) fields of the inline type.\n+      if (is_compiled_by_c2() && callee->has_scalarized_args()) {\n+        const GrowableArray<SigEntry>* sig = callee->adapter()->get_sig_cc();\n+        assert(sig != nullptr, \"sig should never be null\");\n+        TempNewSymbol tmp_sig = SigEntry::create_symbol(sig);\n+        has_receiver = false; \/\/ The extended signature contains the receiver type\n+        fr.oops_compiled_arguments_do(tmp_sig, has_receiver, has_appendix, reg_map, f);\n+        return;\n+      }\n","filename":"src\/hotspot\/share\/code\/compiledMethod.cpp","additions":12,"deletions":1,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -1707,1 +1707,2 @@\n-      || m->number_of_breakpoints() > 0) {\n+      || m->number_of_breakpoints() > 0\n+      || m->mismatch()) {\n","filename":"src\/hotspot\/share\/code\/dependencies.cpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -683,0 +683,6 @@\n+\n+    assert(!method->has_scalarized_args(), \"scalarized native wrappers not supported yet\"); \/\/ for the next 3 fields\n+    _inline_entry_point       = _entry_point;\n+    _verified_inline_entry_point = _verified_entry_point;\n+    _verified_inline_ro_entry_point = _verified_entry_point;\n+\n@@ -868,0 +874,3 @@\n+    _inline_entry_point       = code_begin()         + offsets->value(CodeOffsets::Inline_Entry);\n+    _verified_inline_entry_point = code_begin()      + offsets->value(CodeOffsets::Verified_Inline_Entry);\n+    _verified_inline_ro_entry_point = code_begin()   + offsets->value(CodeOffsets::Verified_Inline_Entry_RO);\n@@ -3017,0 +3026,1 @@\n+  if (pos == inline_entry_point())                                      label = \"[Inline Entry Point]\";\n@@ -3018,0 +3028,2 @@\n+  if (pos == verified_inline_entry_point())                             label = \"[Verified Inline Entry Point]\";\n+  if (pos == verified_inline_ro_entry_point())                          label = \"[Verified Inline Entry Point (RO)]\";\n@@ -3027,0 +3039,10 @@\n+static int maybe_print_entry_label(outputStream* stream, address pos, address entry, const char* label) {\n+  if (pos == entry) {\n+    stream->bol();\n+    stream->print_cr(\"%s\", label);\n+    return 1;\n+  } else {\n+    return 0;\n+  }\n+}\n+\n@@ -3029,33 +3051,12 @@\n-    const char* label = nmethod_section_label(block_begin);\n-    if (label != nullptr) {\n-      stream->bol();\n-      stream->print_cr(\"%s\", label);\n-    }\n-  }\n-\n-  if (block_begin == entry_point()) {\n-    Method* m = method();\n-    if (m != nullptr) {\n-      stream->print(\"  # \");\n-      m->print_value_on(stream);\n-      stream->cr();\n-    }\n-    if (m != nullptr && !is_osr_method()) {\n-      ResourceMark rm;\n-      int sizeargs = m->size_of_parameters();\n-      BasicType* sig_bt = NEW_RESOURCE_ARRAY(BasicType, sizeargs);\n-      VMRegPair* regs   = NEW_RESOURCE_ARRAY(VMRegPair, sizeargs);\n-      {\n-        int sig_index = 0;\n-        if (!m->is_static())\n-          sig_bt[sig_index++] = T_OBJECT; \/\/ 'this'\n-        for (SignatureStream ss(m->signature()); !ss.at_return_type(); ss.next()) {\n-          BasicType t = ss.type();\n-          sig_bt[sig_index++] = t;\n-          if (type2size[t] == 2) {\n-            sig_bt[sig_index++] = T_VOID;\n-          } else {\n-            assert(type2size[t] == 1, \"size is 1 or 2\");\n-          }\n-        }\n-        assert(sig_index == sizeargs, \"\");\n+    int n = 0;\n+    \/\/ Multiple entry points may be at the same position. Print them all.\n+    n += maybe_print_entry_label(stream, block_begin, entry_point(),                    \"[Entry Point]\");\n+    n += maybe_print_entry_label(stream, block_begin, inline_entry_point(),             \"[Inline Entry Point]\");\n+    n += maybe_print_entry_label(stream, block_begin, verified_entry_point(),           \"[Verified Entry Point]\");\n+    n += maybe_print_entry_label(stream, block_begin, verified_inline_entry_point(),    \"[Verified Inline Entry Point]\");\n+    n += maybe_print_entry_label(stream, block_begin, verified_inline_ro_entry_point(), \"[Verified Inline Entry Point (RO)]\");\n+    if (n == 0) {\n+      const char* label = nmethod_section_label(block_begin);\n+      if (label != nullptr) {\n+        stream->bol();\n+        stream->print_cr(\"%s\", label);\n@@ -3063,54 +3064,63 @@\n-      const char* spname = \"sp\"; \/\/ make arch-specific?\n-      intptr_t out_preserve = SharedRuntime::java_calling_convention(sig_bt, regs, sizeargs);\n-      int stack_slot_offset = this->frame_size() * wordSize;\n-      int tab1 = 14, tab2 = 24;\n-      int sig_index = 0;\n-      int arg_index = (m->is_static() ? 0 : -1);\n-      bool did_old_sp = false;\n-      for (SignatureStream ss(m->signature()); !ss.at_return_type(); ) {\n-        bool at_this = (arg_index == -1);\n-        bool at_old_sp = false;\n-        BasicType t = (at_this ? T_OBJECT : ss.type());\n-        assert(t == sig_bt[sig_index], \"sigs in sync\");\n-        if (at_this)\n-          stream->print(\"  # this: \");\n-        else\n-          stream->print(\"  # parm%d: \", arg_index);\n-        stream->move_to(tab1);\n-        VMReg fst = regs[sig_index].first();\n-        VMReg snd = regs[sig_index].second();\n-        if (fst->is_reg()) {\n-          stream->print(\"%s\", fst->name());\n-          if (snd->is_valid())  {\n-            stream->print(\":%s\", snd->name());\n-          }\n-        } else if (fst->is_stack()) {\n-          int offset = fst->reg2stack() * VMRegImpl::stack_slot_size + stack_slot_offset;\n-          if (offset == stack_slot_offset)  at_old_sp = true;\n-          stream->print(\"[%s+0x%x]\", spname, offset);\n-        } else {\n-          stream->print(\"reg%d:%d??\", (int)(intptr_t)fst, (int)(intptr_t)snd);\n-        }\n-        stream->print(\" \");\n-        stream->move_to(tab2);\n-        stream->print(\"= \");\n-        if (at_this) {\n-          m->method_holder()->print_value_on(stream);\n-        } else {\n-          bool did_name = false;\n-          if (!at_this && ss.is_reference()) {\n-            Symbol* name = ss.as_symbol();\n-            name->print_value_on(stream);\n-            did_name = true;\n-          }\n-          if (!did_name)\n-            stream->print(\"%s\", type2name(t));\n-        }\n-        if (at_old_sp) {\n-          stream->print(\"  (%s of caller)\", spname);\n-          did_old_sp = true;\n-        }\n-        stream->cr();\n-        sig_index += type2size[t];\n-        arg_index += 1;\n-        if (!at_this)  ss.next();\n+    }\n+  }\n+\n+  Method* m = method();\n+  if (m == nullptr || is_osr_method()) {\n+    return;\n+  }\n+\n+  \/\/ Print the name of the method (only once)\n+  address low = MIN4(entry_point(), verified_entry_point(), verified_inline_entry_point(), verified_inline_ro_entry_point());\n+  low = MIN2(low, inline_entry_point());\n+  assert(low != 0, \"sanity\");\n+  if (block_begin == low) {\n+    stream->print(\"  # \");\n+    m->print_value_on(stream);\n+    stream->cr();\n+  }\n+\n+  \/\/ Print the arguments for the 3 types of verified entry points\n+  CompiledEntrySignature ces(m);\n+  ces.compute_calling_conventions(false);\n+  const GrowableArray<SigEntry>* sig_cc;\n+  const VMRegPair* regs;\n+  if (block_begin == verified_entry_point()) {\n+    sig_cc = ces.sig_cc();\n+    regs = ces.regs_cc();\n+  } else if (block_begin == verified_inline_entry_point()) {\n+    sig_cc = ces.sig();\n+    regs = ces.regs();\n+  } else if (block_begin == verified_inline_ro_entry_point()) {\n+    sig_cc = ces.sig_cc_ro();\n+    regs = ces.regs_cc_ro();\n+  } else {\n+    return;\n+  }\n+\n+  bool has_this = !m->is_static();\n+  if (ces.has_inline_recv() && block_begin == verified_entry_point()) {\n+    \/\/ <this> argument is scalarized for verified_entry_point()\n+    has_this = false;\n+  }\n+  const char* spname = \"sp\"; \/\/ make arch-specific?\n+  int stack_slot_offset = this->frame_size() * wordSize;\n+  int tab1 = 14, tab2 = 24;\n+  int sig_index = 0;\n+  int arg_index = has_this ? -1 : 0;\n+  bool did_old_sp = false;\n+  for (ExtendedSignature sig = ExtendedSignature(sig_cc, SigEntryFilter()); !sig.at_end(); ++sig) {\n+    bool at_this = (arg_index == -1);\n+    bool at_old_sp = false;\n+    BasicType t = (*sig)._bt;\n+    if (at_this) {\n+      stream->print(\"  # this: \");\n+    } else {\n+      stream->print(\"  # parm%d: \", arg_index);\n+    }\n+    stream->move_to(tab1);\n+    VMReg fst = regs[sig_index].first();\n+    VMReg snd = regs[sig_index].second();\n+    if (fst->is_reg()) {\n+      stream->print(\"%s\", fst->name());\n+      if (snd->is_valid())  {\n+        stream->print(\":%s\", snd->name());\n@@ -3118,6 +3128,18 @@\n-      if (!did_old_sp) {\n-        stream->print(\"  # \");\n-        stream->move_to(tab1);\n-        stream->print(\"[%s+0x%x]\", spname, stack_slot_offset);\n-        stream->print(\"  (%s of caller)\", spname);\n-        stream->cr();\n+    } else if (fst->is_stack()) {\n+      int offset = fst->reg2stack() * VMRegImpl::stack_slot_size + stack_slot_offset;\n+      if (offset == stack_slot_offset)  at_old_sp = true;\n+      stream->print(\"[%s+0x%x]\", spname, offset);\n+    } else {\n+      stream->print(\"reg%d:%d??\", (int)(intptr_t)fst, (int)(intptr_t)snd);\n+    }\n+    stream->print(\" \");\n+    stream->move_to(tab2);\n+    stream->print(\"= \");\n+    if (at_this) {\n+      m->method_holder()->print_value_on(stream);\n+    } else {\n+      bool did_name = false;\n+      if (is_reference_type(t)) {\n+        Symbol* name = (*sig)._symbol;\n+        name->print_value_on(stream);\n+        did_name = true;\n@@ -3125,0 +3147,2 @@\n+      if (!did_name)\n+        stream->print(\"%s\", type2name(t));\n@@ -3126,0 +3150,14 @@\n+    if (at_old_sp) {\n+      stream->print(\"  (%s of caller)\", spname);\n+      did_old_sp = true;\n+    }\n+    stream->cr();\n+    sig_index += type2size[t];\n+    arg_index += 1;\n+  }\n+  if (!did_old_sp) {\n+    stream->print(\"  # \");\n+    stream->move_to(tab1);\n+    stream->print(\"[%s+0x%x]\", spname, stack_slot_offset);\n+    stream->print(\"  (%s of caller)\", spname);\n+    stream->cr();\n@@ -3249,1 +3287,1 @@\n-      st->print(\" {reexecute=%d rethrow=%d return_oop=%d}\", sd->should_reexecute(), sd->rethrow_exception(), sd->return_oop());\n+      st->print(\" {reexecute=%d rethrow=%d return_oop=%d return_scalarized=%d}\", sd->should_reexecute(), sd->rethrow_exception(), sd->return_oop(), sd->return_scalarized());\n","filename":"src\/hotspot\/share\/code\/nmethod.cpp","additions":132,"deletions":94,"binary":false,"changes":226,"status":"modified"},{"patch":"@@ -1237,1 +1237,1 @@\n-          if (vfst.method()->is_static_initializer() ||\n+        if (vfst.method()->is_class_initializer() ||\n","filename":"src\/hotspot\/share\/compiler\/compileBroker.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -45,0 +45,1 @@\n+#include \"oops\/flatArrayKlass.inline.hpp\"\n","filename":"src\/hotspot\/share\/gc\/parallel\/psPromotionManager.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -174,0 +174,3 @@\n+JNIEXPORT jboolean JNICALL\n+JVM_IsValhallaEnabled(void);\n+\n@@ -425,1 +428,1 @@\n- * Find a class from a boot class loader. Returns NULL if class not found.\n+ * Find a class from a boot class loader. Returns nullptr if class not found.\n@@ -576,0 +579,3 @@\n+JNIEXPORT jboolean JNICALL\n+JVM_IsIdentityClass(JNIEnv *env, jclass cls);\n+\n","filename":"src\/hotspot\/share\/include\/jvm.h","additions":7,"deletions":1,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -28,0 +28,1 @@\n+#include \"classfile\/systemDictionary.hpp\"\n@@ -48,0 +49,3 @@\n+#include \"oops\/flatArrayKlass.hpp\"\n+#include \"oops\/flatArrayOop.inline.hpp\"\n+#include \"oops\/inlineKlass.inline.hpp\"\n@@ -80,0 +84,1 @@\n+#include \"utilities\/globalDefinitions.hpp\"\n@@ -161,1 +166,3 @@\n-  oop java_class = klass->java_mirror();\n+  oop java_class = tag.is_Qdescriptor_klass()\n+                      ? InlineKlass::cast(klass)->val_mirror()\n+                      : klass->java_mirror();\n@@ -225,0 +232,4 @@\n+  if (klass->is_inline_klass()) {\n+    THROW(vmSymbols::java_lang_InstantiationError());\n+  }\n+\n@@ -249,0 +260,198 @@\n+JRT_ENTRY(void, InterpreterRuntime::aconst_init(JavaThread* current, ConstantPool* pool, int index))\n+  \/\/ Getting the InlineKlass\n+  Klass* k = pool->klass_at(index, CHECK);\n+  if (!k->is_inline_klass()) {\n+    \/\/ inconsistency with 'new' which throws an InstantiationError\n+    \/\/ in the future, aconst_init will just return null instead of throwing an exception\n+    THROW(vmSymbols::java_lang_IncompatibleClassChangeError());\n+  }\n+  assert(k->is_inline_klass(), \"aconst_init argument must be the inline type class\");\n+  InlineKlass* vklass = InlineKlass::cast(k);\n+\n+  vklass->initialize(CHECK);\n+  oop res = vklass->default_value();\n+  current->set_vm_result(res);\n+JRT_END\n+\n+JRT_ENTRY(int, InterpreterRuntime::withfield(JavaThread* current, ResolvedFieldEntry* entry, uintptr_t ptr))\n+  assert(entry->is_valid(), \"Invalid ResolvedFieldEntry\");\n+  oop obj = nullptr;\n+  int recv_offset = type2size[as_BasicType((TosState)entry->tos_state())];\n+  assert(frame::interpreter_frame_expression_stack_direction() == -1, \"currently is -1 on all platforms\");\n+  int ret_adj = (recv_offset + type2size[T_OBJECT] )* AbstractInterpreter::stackElementSize;\n+  int offset = entry->field_offset();\n+  obj = (oopDesc*)(((uintptr_t*)ptr)[recv_offset * Interpreter::stackElementWords]);\n+  if (obj == nullptr) {\n+    THROW_(vmSymbols::java_lang_NullPointerException(), ret_adj);\n+  }\n+  assert(oopDesc::is_oop(obj), \"Verifying receiver\");\n+  assert(obj->klass()->is_inline_klass(), \"Must have been checked during resolution\");\n+  instanceHandle old_value_h(THREAD, (instanceOop)obj);\n+  oop ref = nullptr;\n+  if (entry->tos_state() == atos) {\n+    ref = *(oopDesc**)ptr;\n+  }\n+  Handle ref_h(THREAD, ref);\n+  InlineKlass* ik = InlineKlass::cast(old_value_h()->klass());\n+  \/\/ Ensure that the class is initialized or being initialized\n+  \/\/ If the class is in error state, the creation of a new value should not be allowed\n+  ik->initialize(CHECK_(ret_adj));\n+\n+  bool can_skip = false;\n+  switch(entry->tos_state()) {\n+    case ztos:\n+      if (old_value_h()->bool_field(offset) == (jboolean)(*(jint*)ptr)) can_skip = true;\n+      break;\n+    case btos:\n+      if (old_value_h()->byte_field(offset) == (jbyte)(*(jint*)ptr)) can_skip = true;\n+      break;\n+    case ctos:\n+      if (old_value_h()->char_field(offset) == (jchar)(*(jint*)ptr)) can_skip = true;\n+      break;\n+    case stos:\n+      if (old_value_h()->short_field(offset) == (jshort)(*(jint*)ptr)) can_skip = true;\n+      break;\n+    case itos:\n+      if (old_value_h()->int_field(offset) == *(jint*)ptr) can_skip = true;\n+      break;\n+    case ltos:\n+      if (old_value_h()->long_field(offset) == *(jlong*)ptr) can_skip = true;\n+      break;\n+    case ftos:\n+      if (memcmp(old_value_h()->field_addr<jfloat>(offset), (jfloat*)ptr, sizeof(jfloat)) == 0) can_skip = true;\n+      break;\n+    case dtos:\n+      if (memcmp(old_value_h()->field_addr<jdouble>(offset), (jdouble*)ptr, sizeof(jdouble)) == 0) can_skip = true;\n+      break;\n+    case atos:\n+      if (!entry->is_flat() && old_value_h()->obj_field(offset) == ref_h()) can_skip = true;\n+      break;\n+    default:\n+      break;\n+  }\n+  if (can_skip) {\n+    current->set_vm_result(old_value_h());\n+    return ret_adj;\n+  }\n+\n+  instanceOop new_value = ik->allocate_instance_buffer(CHECK_(ret_adj));\n+  Handle new_value_h = Handle(THREAD, new_value);\n+  ik->inline_copy_oop_to_new_oop(old_value_h(), new_value_h());\n+  switch(entry->tos_state()) {\n+    case ztos:\n+      new_value_h()->bool_field_put(offset, (jboolean)(*(jint*)ptr));\n+      break;\n+    case btos:\n+      new_value_h()->byte_field_put(offset, (jbyte)(*(jint*)ptr));\n+      break;\n+    case ctos:\n+      new_value_h()->char_field_put(offset, (jchar)(*(jint*)ptr));\n+      break;\n+    case stos:\n+      new_value_h()->short_field_put(offset, (jshort)(*(jint*)ptr));\n+      break;\n+    case itos:\n+      new_value_h()->int_field_put(offset, (*(jint*)ptr));\n+      break;\n+    case ltos:\n+      new_value_h()->long_field_put(offset, *(jlong*)ptr);\n+      break;\n+    case ftos:\n+      new_value_h()->float_field_put(offset, *(jfloat*)ptr);\n+      break;\n+    case dtos:\n+      new_value_h()->double_field_put(offset, *(jdouble*)ptr);\n+      break;\n+    case atos:\n+      {\n+        if (entry->is_null_free_inline_type())  {\n+          if (!entry->is_flat()) {\n+            if (ref_h() == nullptr) {\n+              THROW_(vmSymbols::java_lang_NullPointerException(), ret_adj);\n+            }\n+            new_value_h()->obj_field_put(offset, ref_h());\n+          } else {\n+            int field_index = entry->field_index();\n+            InlineKlass* field_ik = InlineKlass::cast(ik->get_inline_type_field_klass(field_index));\n+            field_ik->write_flat_field(new_value_h(), offset, ref_h(), CHECK_(ret_adj));\n+          }\n+        } else {\n+          new_value_h()->obj_field_put(offset, ref_h());\n+        }\n+      }\n+      break;\n+    default:\n+      ShouldNotReachHere();\n+  }\n+  current->set_vm_result(new_value_h());\n+  return ret_adj;\n+JRT_END\n+\n+JRT_ENTRY(void, InterpreterRuntime::uninitialized_static_inline_type_field(JavaThread* current, oopDesc* mirror, ResolvedFieldEntry* entry))\n+  \/\/ The interpreter tries to access an inline static field that has not been initialized.\n+  \/\/ This situation can happen in different scenarios:\n+  \/\/   1 - if the load or initialization of the field failed during step 8 of\n+  \/\/       the initialization of the holder of the field, in this case the access to the field\n+  \/\/       must fail\n+  \/\/   2 - it can also happen when the initialization of the holder class triggered the initialization of\n+  \/\/       another class which accesses this field in its static initializer, in this case the\n+  \/\/       access must succeed to allow circularity\n+  \/\/ The code below tries to load and initialize the field's class again before returning the default value.\n+  \/\/ If the field was not initialized because of an error, an exception should be thrown.\n+  \/\/ If the class is being initialized, the default value is returned.\n+  assert(entry->is_valid(), \"Invalid ResolvedFieldEntry\");\n+  instanceHandle mirror_h(THREAD, (instanceOop)mirror);\n+  InstanceKlass* klass = entry->field_holder();\n+  u2 index = entry->field_index();\n+  assert(klass == java_lang_Class::as_Klass(mirror), \"Not the field holder klass\");\n+  assert(klass->field_is_null_free_inline_type(index), \"Sanity check\");\n+  if (klass->is_being_initialized() && klass->is_init_thread(THREAD)) {\n+    int offset = klass->field_offset(index);\n+    Klass* field_k = klass->get_inline_type_field_klass_or_null(index);\n+    if (field_k == nullptr) {\n+      field_k = SystemDictionary::resolve_or_fail(klass->field_signature(index)->fundamental_name(THREAD),\n+          Handle(THREAD, klass->class_loader()),\n+          Handle(THREAD, klass->protection_domain()),\n+          true, CHECK);\n+      assert(field_k != nullptr, \"Should have been loaded or an exception thrown above\");\n+      klass->set_inline_type_field_klass(index, field_k);\n+    }\n+    field_k->initialize(CHECK);\n+    oop defaultvalue = InlineKlass::cast(field_k)->default_value();\n+    \/\/ It is safe to initialize the static field because 1) the current thread is the initializing thread\n+    \/\/ and is the only one that can access it, and 2) the field is actually not initialized (i.e. null)\n+    \/\/ otherwise the JVM should not be executing this code.\n+    mirror_h()->obj_field_put(offset, defaultvalue);\n+    current->set_vm_result(defaultvalue);\n+  } else {\n+    assert(klass->is_in_error_state(), \"If not initializing, initialization must have failed to get there\");\n+    ResourceMark rm(THREAD);\n+    const char* desc = \"Could not initialize class \";\n+    const char* className = klass->external_name();\n+    size_t msglen = strlen(desc) + strlen(className) + 1;\n+    char* message = NEW_RESOURCE_ARRAY(char, msglen);\n+    if (nullptr == message) {\n+      \/\/ Out of memory: can't create detailed error message\n+      THROW_MSG(vmSymbols::java_lang_NoClassDefFoundError(), className);\n+    } else {\n+      jio_snprintf(message, msglen, \"%s%s\", desc, className);\n+      THROW_MSG(vmSymbols::java_lang_NoClassDefFoundError(), message);\n+    }\n+  }\n+JRT_END\n+\n+JRT_ENTRY(void, InterpreterRuntime::read_flat_field(JavaThread* current, oopDesc* obj, int index, Klass* field_holder))\n+  Handle obj_h(THREAD, obj);\n+\n+  assert(oopDesc::is_oop(obj), \"Sanity check\");\n+\n+  assert(field_holder->is_instance_klass(), \"Sanity check\");\n+  InstanceKlass* klass = InstanceKlass::cast(field_holder);\n+\n+  assert(klass->field_is_flat(index), \"Sanity check\");\n+\n+  InlineKlass* field_vklass = InlineKlass::cast(klass->get_inline_type_field_klass(index));\n+\n+  oop res = field_vklass->read_flat_field(obj_h(), klass->field_offset(index), CHECK);\n+  current->set_vm_result(res);\n+JRT_END\n@@ -258,1 +467,8 @@\n-  objArrayOop obj = oopFactory::new_objArray(klass, size, CHECK);\n+  bool      is_qtype_desc = pool->tag_at(index).is_Qdescriptor_klass();\n+  arrayOop obj;\n+  if ((!klass->is_array_klass()) && is_qtype_desc) { \/\/ Logically creates elements, ensure klass init\n+    klass->initialize(CHECK);\n+    obj = oopFactory::new_valueArray(klass, size, CHECK);\n+  } else {\n+    obj = oopFactory::new_objArray(klass, size, CHECK);\n+  }\n@@ -262,0 +478,10 @@\n+JRT_ENTRY(void, InterpreterRuntime::value_array_load(JavaThread* current, arrayOopDesc* array, int index))\n+  flatArrayHandle vah(current, (flatArrayOop)array);\n+  oop value_holder = flatArrayOopDesc::value_alloc_copy_from_index(vah, index, CHECK);\n+  current->set_vm_result(value_holder);\n+JRT_END\n+\n+JRT_ENTRY(void, InterpreterRuntime::value_array_store(JavaThread* current, void* val, arrayOopDesc* array, int index))\n+  assert(val != nullptr, \"can't store null into flat array\");\n+  ((flatArrayOop)array)->value_copy_to_index(cast_to_oop(val), index);\n+JRT_END\n@@ -267,2 +493,3 @@\n-  int          i = last_frame.get_index_u2(Bytecodes::_multianewarray);\n-  Klass* klass   = constants->klass_at(i, CHECK);\n+  int i = last_frame.get_index_u2(Bytecodes::_multianewarray);\n+  Klass* klass = constants->klass_at(i, CHECK);\n+  bool is_qtype = klass->name()->is_Q_array_signature();\n@@ -273,0 +500,4 @@\n+  if (is_qtype) { \/\/ Logically creates elements, ensure klass init\n+    klass->initialize(CHECK);\n+  }\n+\n@@ -297,0 +528,23 @@\n+JRT_ENTRY(jboolean, InterpreterRuntime::is_substitutable(JavaThread* current, oopDesc* aobj, oopDesc* bobj))\n+  assert(oopDesc::is_oop(aobj) && oopDesc::is_oop(bobj), \"must be valid oops\");\n+\n+  Handle ha(THREAD, aobj);\n+  Handle hb(THREAD, bobj);\n+  JavaValue result(T_BOOLEAN);\n+  JavaCallArguments args;\n+  args.push_oop(ha);\n+  args.push_oop(hb);\n+  methodHandle method(current, Universe::is_substitutable_method());\n+  JavaCalls::call(&result, method, &args, THREAD);\n+  if (HAS_PENDING_EXCEPTION) {\n+    \/\/ Something really bad happened because isSubstitutable() should not throw exceptions\n+    \/\/ If it is an error, just let it propagate\n+    \/\/ If it is an exception, wrap it into an InternalError\n+    if (!PENDING_EXCEPTION->is_a(vmClasses::Error_klass())) {\n+      Handle e(THREAD, PENDING_EXCEPTION);\n+      CLEAR_PENDING_EXCEPTION;\n+      THROW_MSG_CAUSE_(vmSymbols::java_lang_InternalError(), \"Internal error in substitutability test\", e, false);\n+    }\n+  }\n+  return result.get_jboolean();\n+JRT_END\n@@ -633,0 +887,4 @@\n+JRT_ENTRY(void, InterpreterRuntime::throw_InstantiationError(JavaThread* current))\n+  THROW(vmSymbols::java_lang_InstantiationError());\n+JRT_END\n+\n@@ -666,1 +924,1 @@\n-                    bytecode == Bytecodes::_putstatic);\n+                    bytecode == Bytecodes::_putstatic || bytecode == Bytecodes::_withfield);\n@@ -668,0 +926,1 @@\n+  bool is_inline_type  = bytecode == Bytecodes::_withfield;\n@@ -713,3 +972,9 @@\n-    get_code = ((is_static) ? Bytecodes::_getstatic : Bytecodes::_getfield);\n-    if ((is_put && !has_initialized_final_update) || !info.access_flags().is_final()) {\n-      put_code = ((is_static) ? Bytecodes::_putstatic : Bytecodes::_putfield);\n+    if (is_static) {\n+      get_code = Bytecodes::_getstatic;\n+    } else {\n+      get_code = Bytecodes::_getfield;\n+    }\n+    if (is_put && is_inline_type) {\n+        put_code = ((is_static) ? Bytecodes::_putstatic : Bytecodes::_withfield);\n+    } else if ((is_put && !has_initialized_final_update) || !info.access_flags().is_final()) {\n+        put_code = ((is_static) ? Bytecodes::_putstatic : Bytecodes::_putfield);\n@@ -720,1 +985,2 @@\n-  entry->set_flags(info.access_flags().is_final(), info.access_flags().is_volatile());\n+  entry->set_flags(info.access_flags().is_final(), info.access_flags().is_volatile(),\n+                   info.is_flat(), info.is_null_free_inline_type());\n@@ -977,0 +1243,1 @@\n+  case Bytecodes::_withfield:\n@@ -1168,0 +1435,1 @@\n+  assert(entry->is_valid(), \"Invalid ResolvedFieldEntry\");\n@@ -1175,0 +1443,1 @@\n+  bool is_flat = entry->is_flat();\n@@ -1183,1 +1452,1 @@\n-  jfieldID fid = jfieldIDWorkaround::to_jfieldID(field_holder, entry->field_offset(), is_static);\n+  jfieldID fid = jfieldIDWorkaround::to_jfieldID(field_holder, entry->field_offset(), is_static, is_flat);\n@@ -1191,0 +1460,1 @@\n+  assert(entry->is_valid(), \"Invalid ResolvedFieldEntry\");\n@@ -1212,0 +1482,7 @@\n+\n+  \/\/ Both Q-signatures and L-signatures are mapped to atos\n+  ik->field_is_null_free_inline_type(index);\n+  if (entry->tos_state() == atos && ik->field_is_null_free_inline_type(index)) {\n+    sig_type = JVM_SIGNATURE_PRIMITIVE_OBJECT;\n+  }\n+\n@@ -1213,0 +1490,1 @@\n+  bool is_flat = entry->is_flat();\n@@ -1215,1 +1493,1 @@\n-  jfieldID fid = jfieldIDWorkaround::to_jfieldID(ik, entry->field_offset(), is_static);\n+  jfieldID fid = jfieldIDWorkaround::to_jfieldID(ik, entry->field_offset(), is_static, is_flat);\n","filename":"src\/hotspot\/share\/interpreter\/interpreterRuntime.cpp","additions":289,"deletions":11,"binary":false,"changes":300,"status":"modified"},{"patch":"@@ -184,1 +184,1 @@\n-  nonstatic_field(InstanceKlass,               _misc_flags._flags,                            u2)                                    \\\n+  nonstatic_field(InstanceKlass,               _misc_flags._flags,                            u4)                                    \\\n@@ -640,0 +640,2 @@\n+  declare_constant(DataLayout::array_load_store_data_tag)                 \\\n+  declare_constant(DataLayout::acmp_data_tag)                             \\\n","filename":"src\/hotspot\/share\/jvmci\/vmStructs_jvmci.cpp","additions":3,"deletions":1,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -73,0 +73,1 @@\n+#include \"oops\/inlineKlass.hpp\"\n@@ -169,0 +170,4 @@\n+bool InstanceKlass::field_is_null_free_inline_type(int index) const {\n+  return field(index).field_flags().is_null_free_inline_type();\n+}\n+\n@@ -441,1 +446,3 @@\n-                                       parser.is_interface());\n+                                       parser.is_interface(),\n+                                       parser.has_inline_fields() ? parser.java_fields_count() : 0,\n+                                       parser.is_inline_type());\n@@ -463,0 +470,3 @@\n+  } else if (parser.is_inline_type()) {\n+    \/\/ inline type\n+    ik = new (loader_data, size, THREAD) InlineKlass(parser);\n@@ -474,0 +484,6 @@\n+#ifdef ASSERT\n+  ik->bounds_check((address) ik->start_of_vtable(), false, size);\n+  ik->bounds_check((address) ik->start_of_itable(), false, size);\n+  ik->bounds_check((address) ik->end_of_itable(), true, size);\n+  ik->bounds_check((address) ik->end_of_nonstatic_oop_maps(), true, size);\n+#endif \/\/ASSERT\n@@ -477,0 +493,23 @@\n+#ifndef PRODUCT\n+bool InstanceKlass::bounds_check(address addr, bool edge_ok, intptr_t size_in_bytes) const {\n+  const char* bad = nullptr;\n+  address end = nullptr;\n+  if (addr < (address)this) {\n+    bad = \"before\";\n+  } else if (addr == (address)this) {\n+    if (edge_ok)  return true;\n+    bad = \"just before\";\n+  } else if (addr == (end = (address)this + sizeof(intptr_t) * (size_in_bytes < 0 ? size() : size_in_bytes))) {\n+    if (edge_ok)  return true;\n+    bad = \"just after\";\n+  } else if (addr > end) {\n+    bad = \"after\";\n+  } else {\n+    return true;\n+  }\n+  tty->print_cr(\"%s object bounds: \" INTPTR_FORMAT \" [\" INTPTR_FORMAT \"..\" INTPTR_FORMAT \"]\",\n+      bad, (intptr_t)addr, (intptr_t)this, (intptr_t)end);\n+  Verbose = WizardMode = true; this->print(); \/\/@@\n+  return false;\n+}\n+#endif \/\/PRODUCT\n@@ -516,1 +555,4 @@\n-  _init_thread(nullptr)\n+  _init_thread(nullptr),\n+  _inline_type_field_klasses(nullptr),\n+  _preload_classes(nullptr),\n+  _adr_inlineklass_fixed_block(nullptr)\n@@ -523,0 +565,3 @@\n+  if (parser.has_inline_fields()) {\n+    set_has_inline_type_fields();\n+  }\n@@ -527,0 +572,4 @@\n+\n+  if (has_inline_type_fields()) {\n+    _inline_type_field_klasses = (const Klass**) adr_inline_type_field_klasses();\n+  }\n@@ -696,0 +745,6 @@\n+  if (preload_classes() != nullptr &&\n+      preload_classes() != Universe::the_empty_short_array() &&\n+      !preload_classes()->is_shared()) {\n+    MetadataFactory::free_array<jushort>(loader_data, preload_classes());\n+  }\n+\n@@ -852,0 +907,78 @@\n+\n+  \/\/ If a class declares a method that uses an inline class as an argument\n+  \/\/ type or return inline type, this inline class must be loaded during the\n+  \/\/ linking of this class because size and properties of the inline class\n+  \/\/ must be known in order to be able to perform inline type optimizations.\n+  \/\/ The implementation below is an approximation of this rule, the code\n+  \/\/ iterates over all methods of the current class (including overridden\n+  \/\/ methods), not only the methods declared by this class. This\n+  \/\/ approximation makes the code simpler, and doesn't change the semantic\n+  \/\/ because classes declaring methods overridden by the current class are\n+  \/\/ linked (and have performed their own pre-loading) before the linking\n+  \/\/ of the current class.\n+\n+\n+  \/\/ Note:\n+  \/\/ Inline class types are loaded during\n+  \/\/ the loading phase (see ClassFileParser::post_process_parsed_stream()).\n+  \/\/ Inline class types used as element types for array creation\n+  \/\/ are not pre-loaded. Their loading is triggered by either anewarray\n+  \/\/ or multianewarray bytecodes.\n+\n+  \/\/ Could it be possible to do the following processing only if the\n+  \/\/ class uses inline types?\n+  if (EnableValhalla) {\n+    ResourceMark rm(THREAD);\n+    if (EnablePrimitiveClasses) {\n+      for (int i = 0; i < methods()->length(); i++) {\n+        Method* m = methods()->at(i);\n+        for (SignatureStream ss(m->signature()); !ss.is_done(); ss.next()) {\n+          if (ss.is_reference()) {\n+            if (ss.is_array()) {\n+              continue;\n+            }\n+            if (ss.type() == T_PRIMITIVE_OBJECT) {\n+              Symbol* symb = ss.as_symbol();\n+              if (symb == name()) continue;\n+              oop loader = class_loader();\n+              oop protection_domain = this->protection_domain();\n+              Klass* klass = SystemDictionary::resolve_or_fail(symb,\n+                                                              Handle(THREAD, loader), Handle(THREAD, protection_domain), true,\n+                                                              CHECK_false);\n+              if (klass == nullptr) {\n+                THROW_(vmSymbols::java_lang_LinkageError(), false);\n+              }\n+              if (!klass->is_inline_klass()) {\n+                Exceptions::fthrow(\n+                  THREAD_AND_LOCATION,\n+                  vmSymbols::java_lang_IncompatibleClassChangeError(),\n+                  \"class %s is not an inline type\",\n+                  klass->external_name());\n+              }\n+            }\n+          }\n+        }\n+      }\n+    }\n+    \/\/ Aggressively preloading all classes from the Preload attribute\n+    if (preload_classes() != nullptr) {\n+      for (int i = 0; i < preload_classes()->length(); i++) {\n+        if (constants()->tag_at(preload_classes()->at(i)).is_klass()) continue;\n+        Symbol* class_name = constants()->klass_at_noresolve(preload_classes()->at(i));\n+        if (class_name == name()) continue;\n+        oop loader = class_loader();\n+        oop protection_domain = this->protection_domain();\n+        Klass* klass = SystemDictionary::resolve_or_null(class_name,\n+                                                          Handle(THREAD, loader), Handle(THREAD, protection_domain), THREAD);\n+        if (HAS_PENDING_EXCEPTION) {\n+          CLEAR_PENDING_EXCEPTION;\n+        }\n+        if (klass != nullptr) {\n+          log_info(class, preload)(\"Preloading class %s during linking of class %s because of its Preload attribute\", class_name->as_C_string(), name()->as_C_string());\n+        } else {\n+          log_warning(class, preload)(\"Preloading of class %s during linking of class %s (Preload attribute) failed\", class_name->as_C_string(), name()->as_C_string());\n+        }\n+      }\n+    }\n+  }\n+\n@@ -1113,0 +1246,19 @@\n+  \/\/ Pre-allocating an instance of the default value\n+  if (is_inline_klass()) {\n+      InlineKlass* vk = InlineKlass::cast(this);\n+      oop val = vk->allocate_instance(THREAD);\n+      if (HAS_PENDING_EXCEPTION) {\n+          Handle e(THREAD, PENDING_EXCEPTION);\n+          CLEAR_PENDING_EXCEPTION;\n+          {\n+              EXCEPTION_MARK;\n+              add_initialization_error(THREAD, e);\n+              \/\/ Locks object, set state, and notify all waiting threads\n+              set_initialization_state_and_notify(initialization_error, THREAD);\n+              CLEAR_PENDING_EXCEPTION;\n+          }\n+          THROW_OOP(e());\n+      }\n+      vk->set_default_value(val);\n+  }\n+\n@@ -1145,1 +1297,41 @@\n-\n+  \/\/ Initialize classes of inline fields\n+  if (EnablePrimitiveClasses) {\n+    for (AllFieldStream fs(this); !fs.done(); fs.next()) {\n+      if (fs.is_null_free_inline_type()) {\n+        Klass* klass = get_inline_type_field_klass_or_null(fs.index());\n+        if (fs.access_flags().is_static() && klass == nullptr) {\n+          klass = SystemDictionary::resolve_or_fail(field_signature(fs.index())->fundamental_name(THREAD),\n+              Handle(THREAD, class_loader()),\n+              Handle(THREAD, protection_domain()),\n+              true, THREAD);\n+          set_inline_type_field_klass(fs.index(), klass);\n+        }\n+\n+        if (!HAS_PENDING_EXCEPTION) {\n+          assert(klass != nullptr, \"Must  be\");\n+          InstanceKlass::cast(klass)->initialize(THREAD);\n+          if (fs.access_flags().is_static()) {\n+            if (java_mirror()->obj_field(fs.offset()) == nullptr) {\n+              java_mirror()->obj_field_put(fs.offset(), InlineKlass::cast(klass)->default_value());\n+            }\n+          }\n+        }\n+\n+        if (HAS_PENDING_EXCEPTION) {\n+          Handle e(THREAD, PENDING_EXCEPTION);\n+          CLEAR_PENDING_EXCEPTION;\n+          {\n+            EXCEPTION_MARK;\n+            add_initialization_error(THREAD, e);\n+            \/\/ Locks object, set state, and notify all waiting threads\n+            set_initialization_state_and_notify(initialization_error, THREAD);\n+            CLEAR_PENDING_EXCEPTION;\n+          }\n+          THROW_OOP(e());\n+        }\n+      }\n+    }\n+  }\n+\n+\n+  \/\/ Step 9\n@@ -1168,1 +1360,1 @@\n-  \/\/ Step 9\n+  \/\/ Step 10\n@@ -1174,1 +1366,1 @@\n-    \/\/ Step 10 and 11\n+    \/\/ Step 11 and 12\n@@ -1500,1 +1692,2 @@\n-        ObjArrayKlass* k = ObjArrayKlass::allocate_objArray_klass(class_loader_data(), 1, this, CHECK_NULL);\n+        ObjArrayKlass* k = ObjArrayKlass::allocate_objArray_klass(class_loader_data(), 1, this,\n+                                                                  false, false, CHECK_NULL);\n@@ -1507,2 +1700,2 @@\n-  ObjArrayKlass* oak = array_klasses();\n-  return oak->array_klass(n, THREAD);\n+  ArrayKlass* ak = array_klasses();\n+  return ak->array_klass(n, THREAD);\n@@ -1513,2 +1706,2 @@\n-  ObjArrayKlass* oak = array_klasses_acquire();\n-  if (oak == nullptr) {\n+  ArrayKlass* ak = array_klasses_acquire();\n+  if (ak == nullptr) {\n@@ -1517,1 +1710,1 @@\n-    return oak->array_klass_or_null(n);\n+    return ak->array_klass_or_null(n);\n@@ -1534,1 +1727,1 @@\n-  if (clinit != nullptr && clinit->has_valid_initializer_flags()) {\n+  if (clinit != nullptr && clinit->is_class_initializer()) {\n@@ -1583,1 +1776,1 @@\n-    MutexLocker x(OopMapCacheAlloc_lock);\n+    MutexLocker x(OopMapCacheAlloc_lock,  Mutex::_no_safepoint_check_flag);\n@@ -1595,4 +1788,0 @@\n-bool InstanceKlass::contains_field_offset(int offset) {\n-  fieldDescriptor fd;\n-  return find_field_from_offset(offset, false, &fd);\n-}\n@@ -1680,0 +1869,9 @@\n+bool InstanceKlass::contains_field_offset(int offset) {\n+  if (this->is_inline_klass()) {\n+    InlineKlass* vk = InlineKlass::cast(this);\n+    return offset >= vk->first_field_offset() && offset < (vk->first_field_offset() + vk->get_exact_size_in_bytes());\n+  } else {\n+    fieldDescriptor fd;\n+    return find_field_from_offset(offset, false, &fd);\n+  }\n+}\n@@ -2071,0 +2269,4 @@\n+    if (name == vmSymbols::object_initializer_name() ||\n+        name == vmSymbols::inline_factory_name()) {\n+      break;  \/\/ <init> and <vnew> is never inherited\n+    }\n@@ -2551,0 +2753,1 @@\n+  it->push(&_preload_classes);\n@@ -2552,0 +2755,6 @@\n+\n+  if (has_inline_type_fields()) {\n+    for (int i = 0; i < java_fields_count(); i++) {\n+      it->push(&((Klass**)adr_inline_type_field_klasses())[i]);\n+    }\n+  }\n@@ -2595,1 +2804,9 @@\n-  \/\/ These are not allocated from metaspace. They are safe to set to null.\n+  if (has_inline_type_fields()) {\n+    for (AllFieldStream fs(this); !fs.done(); fs.next()) {\n+      if (fs.is_null_free_inline_type()) {\n+        reset_inline_type_field_klass(fs.index());\n+      }\n+    }\n+  }\n+\n+  \/\/ These are not allocated from metaspace. They are safe to set to nullptr.\n@@ -2679,0 +2896,4 @@\n+  if (is_inline_klass()) {\n+    InlineKlass::cast(this)->initialize_calling_convention(CHECK);\n+  }\n+\n@@ -2707,1 +2928,1 @@\n-    assert(this == array_klasses()->bottom_klass(), \"sanity\");\n+    assert(this == ObjArrayKlass::cast(array_klasses())->bottom_klass(), \"sanity\");\n@@ -2906,0 +3127,2 @@\n+  return signature_name_of_carrier(JVM_SIGNATURE_CLASS);\n+}\n@@ -2907,0 +3130,1 @@\n+const char* InstanceKlass::signature_name_of_carrier(char c) const {\n@@ -2913,1 +3137,1 @@\n-  \/\/ Add L as type indicator\n+  \/\/ Add L or Q as type indicator\n@@ -2915,1 +3139,1 @@\n-  dest[dest_index++] = JVM_SIGNATURE_CLASS;\n+  dest[dest_index++] = c;\n@@ -3257,2 +3481,1 @@\n-  \/\/ Remember to strip ACC_SUPER bit\n-  return (access & (~JVM_ACC_SUPER)) & JVM_ACC_WRITTEN_FLAGS;\n+  return (access & JVM_ACC_WRITTEN_FLAGS);\n@@ -3512,1 +3735,4 @@\n-static void print_vtable(intptr_t* start, int len, outputStream* st) {\n+static void print_vtable(address self, intptr_t* start, int len, outputStream* st) {\n+  ResourceMark rm;\n+  int* forward_refs = NEW_RESOURCE_ARRAY(int, len);\n+  for (int i = 0; i < len; i++)  forward_refs[i] = 0;\n@@ -3516,0 +3742,5 @@\n+    if (forward_refs[i] != 0) {\n+      int from = forward_refs[i];\n+      int off = (int) start[from];\n+      st->print(\" (offset %d <= [%d])\", off, from);\n+    }\n@@ -3519,0 +3750,6 @@\n+    } else if (self != nullptr && e > 0 && e < 0x10000) {\n+      address location = self + e;\n+      int index = (int)((intptr_t*)location - start);\n+      st->print(\" (offset %d => [%d])\", (int)e, index);\n+      if (index >= 0 && index < len)\n+        forward_refs[index] = i;\n@@ -3525,1 +3762,22 @@\n-  return print_vtable(reinterpret_cast<intptr_t*>(start), len, st);\n+  return print_vtable(nullptr, reinterpret_cast<intptr_t*>(start), len, st);\n+}\n+\n+template<typename T>\n+ static void print_array_on(outputStream* st, Array<T>* array) {\n+   if (array == nullptr) { st->print_cr(\"nullptr\"); return; }\n+   array->print_value_on(st); st->cr();\n+   if (Verbose || WizardMode) {\n+     for (int i = 0; i < array->length(); i++) {\n+       st->print(\"%d : \", i); array->at(i)->print_value_on(st); st->cr();\n+     }\n+   }\n+ }\n+\n+static void print_array_on(outputStream* st, Array<int>* array) {\n+  if (array == nullptr) { st->print_cr(\"nullptr\"); return; }\n+  array->print_value_on(st); st->cr();\n+  if (Verbose || WizardMode) {\n+    for (int i = 0; i < array->length(); i++) {\n+      st->print(\"%d : %d\", i, array->at(i)); st->cr();\n+    }\n+  }\n@@ -3566,15 +3824,3 @@\n-  st->print(BULLET\"methods:           \"); methods()->print_value_on(st);                  st->cr();\n-  if (Verbose || WizardMode) {\n-    Array<Method*>* method_array = methods();\n-    for (int i = 0; i < method_array->length(); i++) {\n-      st->print(\"%d : \", i); method_array->at(i)->print_value(); st->cr();\n-    }\n-  }\n-  st->print(BULLET\"method ordering:   \"); method_ordering()->print_value_on(st);      st->cr();\n-  st->print(BULLET\"default_methods:   \"); default_methods()->print_value_on(st);      st->cr();\n-  if (Verbose && default_methods() != nullptr) {\n-    Array<Method*>* method_array = default_methods();\n-    for (int i = 0; i < method_array->length(); i++) {\n-      st->print(\"%d : \", i); method_array->at(i)->print_value(); st->cr();\n-    }\n-  }\n+  st->print(BULLET\"methods:           \"); print_array_on(st, methods());\n+  st->print(BULLET\"method ordering:   \"); print_array_on(st, method_ordering());\n+  st->print(BULLET\"default_methods:   \"); print_array_on(st, default_methods());\n@@ -3582,1 +3828,1 @@\n-    st->print(BULLET\"default vtable indices:   \"); default_vtable_indices()->print_value_on(st);       st->cr();\n+    st->print(BULLET\"default vtable indices:   \"); print_array_on(st, default_vtable_indices());\n@@ -3584,2 +3830,2 @@\n-  st->print(BULLET\"local interfaces:  \"); local_interfaces()->print_value_on(st);      st->cr();\n-  st->print(BULLET\"trans. interfaces: \"); transitive_interfaces()->print_value_on(st); st->cr();\n+  st->print(BULLET\"local interfaces:  \"); print_array_on(st, local_interfaces());\n+  st->print(BULLET\"trans. interfaces: \"); print_array_on(st, transitive_interfaces());\n@@ -3631,0 +3877,1 @@\n+  st->print(BULLET\"preload classes:     \"); preload_classes()->print_value_on(st); st->cr();\n@@ -3641,1 +3888,1 @@\n-  if (itable_length() > 0 && (Verbose || WizardMode))  print_vtable(start_of_itable(), itable_length(), st);\n+  if (itable_length() > 0 && (Verbose || WizardMode))  print_vtable(nullptr, start_of_itable(), itable_length(), st);\n","filename":"src\/hotspot\/share\/oops\/instanceKlass.cpp","additions":291,"deletions":44,"binary":false,"changes":335,"status":"modified"},{"patch":"@@ -60,0 +60,1 @@\n+#include \"oops\/inlineKlass.inline.hpp\"\n@@ -118,1 +119,0 @@\n-\n@@ -156,0 +156,5 @@\n+address Method::get_c2i_inline_entry() {\n+  assert(adapter() != nullptr, \"must have\");\n+  return adapter()->get_c2i_inline_entry();\n+}\n+\n@@ -161,0 +166,5 @@\n+address Method::get_c2i_unverified_inline_entry() {\n+  assert(adapter() != nullptr, \"must have\");\n+  return adapter()->get_c2i_unverified_inline_entry();\n+}\n+\n@@ -655,0 +665,13 @@\n+\/\/ InlineKlass the method is declared to return. This must not\n+\/\/ safepoint as it is called with references live on the stack at\n+\/\/ locations the GC is unaware of.\n+InlineKlass* Method::returns_inline_type(Thread* thread) const {\n+  assert(InlineTypeReturnedAsFields, \"Inline types should never be returned as fields\");\n+  NoSafepointVerifier nsv;\n+  SignatureStream ss(signature());\n+  while (!ss.at_return_type()) {\n+    ss.next();\n+  }\n+  return ss.as_inline_klass(method_holder());\n+}\n+\n@@ -660,1 +683,1 @@\n-  \/\/   aload_0\n+  \/\/   aload_0, _fast_aload_0, or _nofast_aload_0\n@@ -684,1 +707,2 @@\n-  if (cb[0] != Bytecodes::_aload_0 || cb[1] != Bytecodes::_invokespecial || cb[last] != Bytecodes::_return) {\n+  if ((cb[0] != Bytecodes::_aload_0 && cb[0] != Bytecodes::_fast_aload_0 && cb[0] != Bytecodes::_nofast_aload_0) ||\n+       cb[1] != Bytecodes::_invokespecial || cb[last] != Bytecodes::_return) {\n@@ -846,0 +870,5 @@\n+  if (has_scalarized_return()) {\n+    \/\/ Don't treat this as (trivial) getter method because the\n+    \/\/ inline type should be returned in a scalarized form.\n+    return false;\n+  }\n@@ -867,0 +896,5 @@\n+  if (has_scalarized_args()) {\n+    \/\/ Don't treat this as (trivial) setter method because the\n+    \/\/ inline type argument should be passed in a scalarized form.\n+    return false;\n+  }\n@@ -877,1 +911,2 @@\n-          Bytecodes::is_return(java_code_at(last_index)));\n+          Bytecodes::is_return(java_code_at(last_index)) &&\n+          !has_scalarized_args());\n@@ -880,2 +915,2 @@\n-bool Method::is_initializer() const {\n-  return is_object_initializer() || is_static_initializer();\n+bool Method::is_object_constructor_or_class_initializer() const {\n+  return (is_object_constructor() || is_class_initializer());\n@@ -884,6 +919,1 @@\n-bool Method::has_valid_initializer_flags() const {\n-  return (is_static() ||\n-          method_holder()->major_version() < 51);\n-}\n-\n-bool Method::is_static_initializer() const {\n+bool Method::is_class_initializer() const {\n@@ -893,2 +923,3 @@\n-  return name() == vmSymbols::class_initializer_name() &&\n-         has_valid_initializer_flags();\n+  return (name() == vmSymbols::class_initializer_name() &&\n+          (is_static() ||\n+           method_holder()->major_version() < 51));\n@@ -897,2 +928,8 @@\n-bool Method::is_object_initializer() const {\n-   return name() == vmSymbols::object_initializer_name();\n+\/\/ A method named <init>, is a classic object constructor.\n+bool Method::is_object_constructor() const {\n+  return name() == vmSymbols::object_initializer_name();\n+}\n+\n+\/\/ A method named <vnew> is a factory for an inline class.\n+bool Method::is_static_vnew_factory() const {\n+  return name() == vmSymbols::inline_factory_name();\n@@ -956,1 +993,1 @@\n-  if( constants()->tag_at(klass_index).is_unresolved_klass() ) {\n+  if( constants()->tag_at(klass_index).is_unresolved_klass()) {\n@@ -972,1 +1009,3 @@\n-    if (constants()->tag_at(klass_index).is_unresolved_klass()) return false;\n+    if (constants()->tag_at(klass_index).is_unresolved_klass()) {\n+      return false;\n+    }\n@@ -1141,0 +1180,2 @@\n+    _from_compiled_inline_entry = nullptr;\n+    _from_compiled_inline_ro_entry = nullptr;\n@@ -1143,0 +1184,2 @@\n+    _from_compiled_inline_entry = adapter()->get_c2i_inline_entry();\n+    _from_compiled_inline_ro_entry = adapter()->get_c2i_inline_ro_entry();\n@@ -1174,0 +1217,2 @@\n+  _from_compiled_inline_entry = nullptr;\n+  _from_compiled_inline_ro_entry = nullptr;\n@@ -1227,0 +1272,4 @@\n+  if (InlineTypeReturnedAsFields && returns_inline_type(THREAD) && !has_scalarized_return()) {\n+    assert(!constMethod()->is_shared(), \"Cannot update shared const objects\");\n+    set_has_scalarized_return();\n+  }\n@@ -1266,0 +1315,2 @@\n+  mh->_from_compiled_inline_entry = adapter->get_c2i_inline_entry();\n+  mh->_from_compiled_inline_ro_entry = adapter->get_c2i_inline_ro_entry();\n@@ -1282,0 +1333,12 @@\n+address Method::verified_inline_code_entry() {\n+  debug_only(NoSafepointVerifier nsv;)\n+  assert(_from_compiled_inline_entry != nullptr, \"must be set\");\n+  return _from_compiled_inline_entry;\n+}\n+\n+address Method::verified_inline_ro_code_entry() {\n+  debug_only(NoSafepointVerifier nsv;)\n+  assert(_from_compiled_inline_ro_entry != nullptr, \"must be set\");\n+  return _from_compiled_inline_ro_entry;\n+}\n+\n@@ -1313,0 +1376,2 @@\n+  mh->_from_compiled_inline_entry = code->verified_inline_entry_point();\n+  mh->_from_compiled_inline_ro_entry = code->verified_inline_ro_entry_point();\n@@ -2289,0 +2354,25 @@\n+bool Method::is_scalarized_arg(int idx) const {\n+  if (!has_scalarized_args()) {\n+    return false;\n+  }\n+  \/\/ Search through signature and check if argument is wrapped in T_METADATA\/T_VOID\n+  int depth = 0;\n+  const GrowableArray<SigEntry>* sig = adapter()->get_sig_cc();\n+  for (int i = 0; i < sig->length(); i++) {\n+    BasicType bt = sig->at(i)._bt;\n+    if (bt == T_METADATA) {\n+      depth++;\n+    }\n+    if (idx == 0) {\n+      break; \/\/ Argument found\n+    }\n+    if (bt == T_VOID && (sig->at(i-1)._bt != T_LONG && sig->at(i-1)._bt != T_DOUBLE)) {\n+      depth--;\n+    }\n+    if (depth == 0 && bt != T_LONG && bt != T_DOUBLE) {\n+      idx--; \/\/ Advance to next argument\n+    }\n+  }\n+  return depth != 0;\n+}\n+\n@@ -2321,0 +2411,4 @@\n+#ifdef ASSERT\n+  if (valid_itable_index())\n+    st->print_cr(\" - itable index:      %d\",   itable_index());\n+#endif\n@@ -2328,1 +2422,3 @@\n-  st->print_cr(\" - compiled entry     \" PTR_FORMAT, p2i(from_compiled_entry()));\n+  st->print_cr(\" - compiled entry           \" PTR_FORMAT, p2i(from_compiled_entry()));\n+  st->print_cr(\" - compiled inline entry    \" PTR_FORMAT, p2i(from_compiled_inline_entry()));\n+  st->print_cr(\" - compiled inline ro entry \" PTR_FORMAT, p2i(from_compiled_inline_ro_entry()));\n@@ -2398,0 +2494,1 @@\n+  if (WizardMode) access_flags().print_on(st);\n","filename":"src\/hotspot\/share\/oops\/method.cpp","additions":116,"deletions":19,"binary":false,"changes":135,"status":"modified"},{"patch":"@@ -96,1 +96,3 @@\n-  volatile address _from_compiled_entry;        \/\/ Cache of: _code ? _code->entry_point() : _adapter->c2i_entry()\n+  volatile address _from_compiled_entry;           \/\/ Cache of: _code ? _code->verified_entry_point()           : _adapter->c2i_entry()\n+  volatile address _from_compiled_inline_ro_entry; \/\/ Cache of: _code ? _code->verified_inline_ro_entry_point() : _adapter->c2i_inline_ro_entry()\n+  volatile address _from_compiled_inline_entry;    \/\/ Cache of: _code ? _code->verified_inline_entry_point()    : _adapter->c2i_inline_entry()\n@@ -135,0 +137,2 @@\n+  address from_compiled_inline_ro_entry() const;\n+  address from_compiled_inline_entry() const;\n@@ -352,0 +356,2 @@\n+  address verified_inline_code_entry();\n+  address verified_inline_ro_code_entry();\n@@ -370,1 +376,7 @@\n-    _from_compiled_entry =  entry;\n+    _from_compiled_entry = entry;\n+  }\n+  void set_from_compiled_inline_ro_entry(address entry) {\n+    _from_compiled_inline_ro_entry = entry;\n+  }\n+  void set_from_compiled_inline_entry(address entry) {\n+    _from_compiled_inline_entry = entry;\n@@ -375,0 +387,1 @@\n+  address get_c2i_inline_entry();\n@@ -376,0 +389,1 @@\n+  address get_c2i_unverified_inline_entry();\n@@ -490,1 +504,1 @@\n-  bool is_returning_fp() const                   { BasicType r = result_type(); return (r == T_FLOAT || r == T_DOUBLE); }\n+  InlineKlass* returns_inline_type(Thread* thread) const;\n@@ -569,6 +583,0 @@\n-  \/\/ returns true if the method is an initializer (<init> or <clinit>).\n-  bool is_initializer() const;\n-\n-  \/\/ returns true if the method is static OR if the classfile version < 51\n-  bool has_valid_initializer_flags() const;\n-\n@@ -577,1 +585,8 @@\n-  bool is_static_initializer() const;\n+  bool is_class_initializer() const;\n+\n+  \/\/ returns true if the method name is <init> and the method is not a static factory\n+  bool is_object_constructor() const;\n+\n+  \/\/ returns true if the method is an object constructor or class initializer\n+  \/\/ (non-static <init> or <clinit>), but false for factories (static <vnew>).\n+  bool is_object_constructor_or_class_initializer() const;\n@@ -579,2 +594,2 @@\n-  \/\/ returns true if the method name is <init>\n-  bool is_object_initializer() const;\n+  \/\/ returns true if the method name is <vnew> and the method is static\n+  bool is_static_vnew_factory() const;\n@@ -602,0 +617,2 @@\n+  static ByteSize from_compiled_inline_offset()  { return byte_offset_of(Method, _from_compiled_inline_entry); }\n+  static ByteSize from_compiled_inline_ro_offset(){ return byte_offset_of(Method, _from_compiled_inline_ro_entry); }\n@@ -603,0 +620,1 @@\n+  static ByteSize flags_offset()                 { return byte_offset_of(Method, _flags); }\n@@ -760,0 +778,17 @@\n+  bool has_scalarized_args() const { return constMethod()->has_scalarized_args(); }\n+  void set_has_scalarized_args() { constMethod()->set_has_scalarized_args(); }\n+\n+  bool has_scalarized_return() const { return constMethod()->has_scalarized_return(); }\n+  void set_has_scalarized_return() { constMethod()->set_has_scalarized_return(); }\n+\n+  bool is_scalarized_arg(int idx) const;\n+\n+  bool c1_needs_stack_repair() const { return constMethod()->c1_needs_stack_repair(); }\n+  void set_c1_needs_stack_repair() { constMethod()->set_c1_needs_stack_repair(); }\n+\n+  bool c2_needs_stack_repair() const { return constMethod()->c2_needs_stack_repair(); }\n+  void set_c2_needs_stack_repair() { constMethod()->set_c2_needs_stack_repair(); }\n+\n+  bool mismatch() const { return constMethod()->mismatch(); }\n+  void set_mismatch() { constMethod()->set_mismatch(); }\n+\n","filename":"src\/hotspot\/share\/oops\/method.hpp","additions":47,"deletions":12,"binary":false,"changes":59,"status":"modified"},{"patch":"@@ -38,0 +38,8 @@\n+inline address Method::from_compiled_inline_ro_entry() const {\n+  return Atomic::load_acquire(&_from_compiled_inline_ro_entry);\n+}\n+\n+inline address Method::from_compiled_inline_entry() const {\n+  return Atomic::load_acquire(&_from_compiled_inline_entry);\n+}\n+\n","filename":"src\/hotspot\/share\/oops\/method.inline.hpp","additions":8,"deletions":0,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -144,1 +144,1 @@\n-    st->print(\"flags(%d) \", flags);\n+    st->print(\"flags(%d) %p\/%d\", flags, data(), in_bytes(DataLayout::flags_offset()));\n@@ -214,1 +214,1 @@\n-  assert(TypeStackSlotEntries::per_arg_count() > ReturnTypeEntry::static_cell_count(), \"code to test for arguments\/results broken\");\n+  assert(TypeStackSlotEntries::per_arg_count() > SingleTypeEntry::static_cell_count(), \"code to test for arguments\/results broken\");\n@@ -224,1 +224,1 @@\n-    ret_cell = ReturnTypeEntry::static_cell_count();\n+    ret_cell = SingleTypeEntry::static_cell_count();\n@@ -327,1 +327,1 @@\n-void ReturnTypeEntry::clean_weak_klass_links(bool always_clean) {\n+void SingleTypeEntry::clean_weak_klass_links(bool always_clean) {\n@@ -365,1 +365,1 @@\n-void ReturnTypeEntry::print_data_on(outputStream* st) const {\n+void SingleTypeEntry::print_data_on(outputStream* st) const {\n@@ -526,0 +526,4 @@\n+  if (data()->flags()) {\n+    tty->cr();\n+    tab(st);\n+  }\n@@ -651,0 +655,21 @@\n+void ArrayLoadStoreData::print_data_on(outputStream* st, const char* extra) const {\n+  print_shared(st, \"ArrayLoadStore\", extra);\n+  st->cr();\n+  tab(st, true);\n+  st->print(\"array\");\n+  _array.print_data_on(st);\n+  tab(st, true);\n+  st->print(\"element\");\n+  _element.print_data_on(st);\n+}\n+\n+void ACmpData::print_data_on(outputStream* st, const char* extra) const {\n+  BranchData::print_data_on(st, extra);\n+  tab(st, true);\n+  st->print(\"left\");\n+  _left.print_data_on(st);\n+  tab(st, true);\n+  st->print(\"right\");\n+  _right.print_data_on(st);\n+}\n+\n@@ -672,1 +697,0 @@\n-  case Bytecodes::_aastore:\n@@ -678,0 +702,3 @@\n+  case Bytecodes::_aaload:\n+  case Bytecodes::_aastore:\n+    return ArrayLoadStoreData::static_cell_count();\n@@ -717,2 +744,0 @@\n-  case Bytecodes::_if_acmpeq:\n-  case Bytecodes::_if_acmpne:\n@@ -722,0 +747,3 @@\n+  case Bytecodes::_if_acmpne:\n+  case Bytecodes::_if_acmpeq:\n+    return ACmpData::static_cell_count();\n@@ -780,0 +808,1 @@\n+  case Bytecodes::_aaload:\n@@ -993,1 +1022,0 @@\n-  case Bytecodes::_aastore:\n@@ -1002,0 +1030,5 @@\n+  case Bytecodes::_aaload:\n+  case Bytecodes::_aastore:\n+    cell_count = ArrayLoadStoreData::static_cell_count();\n+    tag = DataLayout::array_load_store_data_tag;\n+    break;\n@@ -1073,2 +1106,0 @@\n-  case Bytecodes::_if_acmpeq:\n-  case Bytecodes::_if_acmpne:\n@@ -1080,0 +1111,5 @@\n+  case Bytecodes::_if_acmpeq:\n+  case Bytecodes::_if_acmpne:\n+    cell_count = ACmpData::static_cell_count();\n+    tag = DataLayout::acmp_data_tag;\n+    break;\n@@ -1147,0 +1183,4 @@\n+  case DataLayout::array_load_store_data_tag:\n+    return ((new ArrayLoadStoreData(this))->cell_count());\n+  case DataLayout::acmp_data_tag:\n+    return ((new ACmpData(this))->cell_count());\n@@ -1181,0 +1221,4 @@\n+  case DataLayout::array_load_store_data_tag:\n+    return new ArrayLoadStoreData(this);\n+  case DataLayout::acmp_data_tag:\n+    return new ACmpData(this);\n","filename":"src\/hotspot\/share\/oops\/methodData.cpp","additions":55,"deletions":11,"binary":false,"changes":66,"status":"modified"},{"patch":"@@ -132,1 +132,3 @@\n-    speculative_trap_data_tag\n+    speculative_trap_data_tag,\n+    array_load_store_data_tag,\n+    acmp_data_tag\n@@ -268,0 +270,1 @@\n+class       ACmpData;\n@@ -273,0 +276,1 @@\n+class   ArrayLoadStoreData;\n@@ -280,1 +284,1 @@\n-  friend class ReturnTypeEntry;\n+  friend class SingleTypeEntry;\n@@ -401,0 +405,2 @@\n+  virtual bool is_ArrayLoadStoreData() const { return false; }\n+  virtual bool is_ACmpData()           const { return false; }\n@@ -459,0 +465,8 @@\n+  ArrayLoadStoreData* as_ArrayLoadStoreData() const {\n+    assert(is_ArrayLoadStoreData(), \"wrong type\");\n+    return is_ArrayLoadStoreData() ? (ArrayLoadStoreData*)this : nullptr;\n+  }\n+  ACmpData* as_ACmpData() const {\n+    assert(is_ACmpData(), \"wrong type\");\n+    return is_ACmpData() ? (ACmpData*)this : nullptr;\n+  }\n@@ -613,1 +627,2 @@\n-      layout->tag() == DataLayout::branch_data_tag, \"wrong type\");\n+      layout->tag() == DataLayout::branch_data_tag ||\n+      layout->tag() == DataLayout::acmp_data_tag, \"wrong type\");\n@@ -849,1 +864,1 @@\n-class ReturnTypeEntry : public TypeEntries {\n+class SingleTypeEntry : public TypeEntries {\n@@ -857,1 +872,1 @@\n-  ReturnTypeEntry(int base_off)\n+  SingleTypeEntry(int base_off)\n@@ -891,1 +906,1 @@\n-\/\/ (TypeStackSlotEntries), a return type (ReturnTypeEntry) and a\n+\/\/ (TypeStackSlotEntries), a return type (SingleTypeEntry) and a\n@@ -945,1 +960,1 @@\n-    return ReturnTypeEntry::size() + in_ByteSize(header_cell_count() * DataLayout::cell_size);\n+    return SingleTypeEntry::size() + in_ByteSize(header_cell_count() * DataLayout::cell_size);\n@@ -960,1 +975,1 @@\n-  ReturnTypeEntry _ret;\n+  SingleTypeEntry _ret;\n@@ -979,1 +994,1 @@\n-    _ret(cell_count() - ReturnTypeEntry::static_cell_count())\n+    _ret(cell_count() - SingleTypeEntry::static_cell_count())\n@@ -992,1 +1007,1 @@\n-  const ReturnTypeEntry* ret() const {\n+  const SingleTypeEntry* ret() const {\n@@ -1234,1 +1249,1 @@\n-  ReturnTypeEntry _ret;\n+  SingleTypeEntry _ret;\n@@ -1253,1 +1268,1 @@\n-    _ret(cell_count() - ReturnTypeEntry::static_cell_count())\n+    _ret(cell_count() - SingleTypeEntry::static_cell_count())\n@@ -1266,1 +1281,1 @@\n-  const ReturnTypeEntry* ret() const {\n+  const SingleTypeEntry* ret() const {\n@@ -1468,1 +1483,1 @@\n-    assert(layout->tag() == DataLayout::branch_data_tag, \"wrong type\");\n+    assert(layout->tag() == DataLayout::branch_data_tag || layout->tag() == DataLayout::acmp_data_tag, \"wrong type\");\n@@ -1822,0 +1837,146 @@\n+  virtual void print_data_on(outputStream* st, const char* extra = nullptr) const;\n+};\n+\n+class ArrayLoadStoreData : public ProfileData {\n+private:\n+  enum {\n+    flat_array_flag = DataLayout::first_flag,\n+    null_free_array_flag = flat_array_flag + 1,\n+  };\n+\n+  SingleTypeEntry _array;\n+  SingleTypeEntry _element;\n+\n+public:\n+  ArrayLoadStoreData(DataLayout* layout) :\n+    ProfileData(layout),\n+    _array(0),\n+    _element(SingleTypeEntry::static_cell_count()) {\n+    assert(layout->tag() == DataLayout::array_load_store_data_tag, \"wrong type\");\n+    _array.set_profile_data(this);\n+    _element.set_profile_data(this);\n+  }\n+\n+  const SingleTypeEntry* array() const {\n+    return &_array;\n+  }\n+\n+  const SingleTypeEntry* element() const {\n+    return &_element;\n+  }\n+\n+  virtual bool is_ArrayLoadStoreData() const { return true; }\n+\n+  static int static_cell_count() {\n+    return SingleTypeEntry::static_cell_count() * 2;\n+  }\n+\n+  virtual int cell_count() const {\n+    return static_cell_count();\n+  }\n+\n+  void set_flat_array() { set_flag_at(flat_array_flag); }\n+  bool flat_array() const { return flag_at(flat_array_flag); }\n+\n+  void set_null_free_array() { set_flag_at(null_free_array_flag); }\n+  bool null_free_array() const { return flag_at(null_free_array_flag); }\n+\n+  \/\/ Code generation support\n+  static int flat_array_byte_constant() {\n+    return flag_number_to_constant(flat_array_flag);\n+  }\n+\n+  static int null_free_array_byte_constant() {\n+    return flag_number_to_constant(null_free_array_flag);\n+  }\n+\n+  static ByteSize array_offset() {\n+    return cell_offset(0);\n+  }\n+\n+  static ByteSize element_offset() {\n+    return cell_offset(SingleTypeEntry::static_cell_count());\n+  }\n+\n+  virtual void clean_weak_klass_links(bool always_clean) {\n+    _array.clean_weak_klass_links(always_clean);\n+    _element.clean_weak_klass_links(always_clean);\n+  }\n+\n+  static ByteSize array_load_store_data_size() {\n+    return cell_offset(static_cell_count());\n+  }\n+\n+  virtual void print_data_on(outputStream* st, const char* extra = nullptr) const;\n+};\n+\n+class ACmpData : public BranchData {\n+private:\n+  enum {\n+    left_inline_type_flag = DataLayout::first_flag,\n+    right_inline_type_flag\n+  };\n+\n+  SingleTypeEntry _left;\n+  SingleTypeEntry _right;\n+\n+public:\n+  ACmpData(DataLayout* layout) :\n+    BranchData(layout),\n+    _left(BranchData::static_cell_count()),\n+    _right(BranchData::static_cell_count() + SingleTypeEntry::static_cell_count()) {\n+    assert(layout->tag() == DataLayout::acmp_data_tag, \"wrong type\");\n+    _left.set_profile_data(this);\n+    _right.set_profile_data(this);\n+  }\n+\n+  const SingleTypeEntry* left() const {\n+    return &_left;\n+  }\n+\n+  const SingleTypeEntry* right() const {\n+    return &_right;\n+  }\n+\n+  virtual bool is_ACmpData() const { return true; }\n+\n+  static int static_cell_count() {\n+    return BranchData::static_cell_count() + SingleTypeEntry::static_cell_count() * 2;\n+  }\n+\n+  virtual int cell_count() const {\n+    return static_cell_count();\n+  }\n+\n+  void set_left_inline_type() { set_flag_at(left_inline_type_flag); }\n+  bool left_inline_type() const { return flag_at(left_inline_type_flag); }\n+\n+  void set_right_inline_type() { set_flag_at(right_inline_type_flag); }\n+  bool right_inline_type() const { return flag_at(right_inline_type_flag); }\n+\n+  \/\/ Code generation support\n+  static int left_inline_type_byte_constant() {\n+    return flag_number_to_constant(left_inline_type_flag);\n+  }\n+\n+  static int right_inline_type_byte_constant() {\n+    return flag_number_to_constant(right_inline_type_flag);\n+  }\n+\n+  static ByteSize left_offset() {\n+    return cell_offset(BranchData::static_cell_count());\n+  }\n+\n+  static ByteSize right_offset() {\n+    return cell_offset(BranchData::static_cell_count() + SingleTypeEntry::static_cell_count());\n+  }\n+\n+  virtual void clean_weak_klass_links(bool always_clean) {\n+    _left.clean_weak_klass_links(always_clean);\n+    _right.clean_weak_klass_links(always_clean);\n+  }\n+\n+  static ByteSize acmp_data_size() {\n+    return cell_offset(static_cell_count());\n+  }\n+\n","filename":"src\/hotspot\/share\/oops\/methodData.hpp","additions":175,"deletions":14,"binary":false,"changes":189,"status":"modified"},{"patch":"@@ -89,1 +89,1 @@\n-  if (callee_method->is_initializer()) {\n+  if (callee_method->is_object_constructor()) {\n@@ -92,1 +92,1 @@\n-  if (caller_method->is_initializer() &&\n+  if (caller_method->is_object_constructor_or_class_initializer() &&\n","filename":"src\/hotspot\/share\/opto\/bytecodeInfo.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -758,0 +758,6 @@\n+  product(bool, UseArrayLoadStoreProfile, true,                             \\\n+          \"Take advantage of profiling at array load\/store\")                \\\n+                                                                            \\\n+  product(bool, UseACmpProfile, true,                                       \\\n+          \"Take advantage of profiling at acmp\")                            \\\n+                                                                            \\\n","filename":"src\/hotspot\/share\/opto\/c2_globals.hpp","additions":6,"deletions":0,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -38,0 +38,1 @@\n+#include \"opto\/inlinetypenode.hpp\"\n@@ -121,1 +122,1 @@\n-  \/\/ paths to facilitate late inlinig.\n+  \/\/ paths to facilitate late inlining.\n@@ -130,0 +131,1 @@\n+      _call_node(nullptr),\n@@ -132,0 +134,8 @@\n+    if (InlineTypeReturnedAsFields && method->is_method_handle_intrinsic()) {\n+      \/\/ If that call has not been optimized by the time optimizations are over,\n+      \/\/ we'll need to add a call to create an inline type instance from the klass\n+      \/\/ returned by the call (see PhaseMacroExpand::expand_mh_intrinsic_return).\n+      \/\/ Separating memory and I\/O projections for exceptions is required to\n+      \/\/ perform that graph transformation.\n+      _separate_io_proj = true;\n+    }\n@@ -146,0 +156,1 @@\n+  PhaseGVN& gvn = kit.gvn();\n@@ -178,1 +189,4 @@\n-  kit.set_arguments_for_java_call(call);\n+  kit.set_arguments_for_java_call(call, is_late_inline());\n+  if (kit.stopped()) {\n+    return kit.transfer_exceptions_into_jvms();\n+  }\n@@ -219,1 +233,0 @@\n-\n@@ -279,0 +292,3 @@\n+  if (kit.stopped()) {\n+    return kit.transfer_exceptions_into_jvms();\n+  }\n@@ -372,0 +388,4 @@\n+  virtual CallGenerator* inline_cg() {\n+    return _inline_cg;\n+  }\n+\n@@ -433,0 +453,8 @@\n+    \/\/ AlwaysIncrementalInline causes for_method_handle_inline() to\n+    \/\/ return a LateInlineCallGenerator. Extract the\n+    \/\/ InlineCallGenerator from it.\n+    if (AlwaysIncrementalInline && cg->is_late_inline() && !cg->is_virtual_late_inline()) {\n+      cg = cg->inline_cg();\n+      assert(cg != nullptr, \"inline call generator expected\");\n+    }\n+\n@@ -595,3 +623,3 @@\n-  const TypeTuple *r = call->tf()->domain();\n-  for (int i1 = 0; i1 < method()->arg_size(); i1++) {\n-    if (call->in(TypeFunc::Parms + i1)->is_top() && r->field_at(TypeFunc::Parms + i1) != Type::HALF) {\n+  const TypeTuple* r = call->tf()->domain_cc();\n+  for (uint i1 = TypeFunc::Parms; i1 < r->cnt(); i1++) {\n+    if (call->in(i1)->is_top() && r->field_at(i1) != Type::HALF) {\n@@ -615,10 +643,8 @@\n-  CallProjections callprojs;\n-  call->extract_projections(&callprojs, true);\n-  if ((callprojs.fallthrough_catchproj == call->in(0)) ||\n-      (callprojs.catchall_catchproj    == call->in(0)) ||\n-      (callprojs.fallthrough_memproj   == call->in(TypeFunc::Memory)) ||\n-      (callprojs.catchall_memproj      == call->in(TypeFunc::Memory)) ||\n-      (callprojs.fallthrough_ioproj    == call->in(TypeFunc::I_O)) ||\n-      (callprojs.catchall_ioproj       == call->in(TypeFunc::I_O)) ||\n-      (callprojs.resproj != nullptr && call->find_edge(callprojs.resproj) != -1) ||\n-      (callprojs.exobj   != nullptr && call->find_edge(callprojs.exobj) != -1)) {\n+  CallProjections* callprojs = call->extract_projections(true);\n+  if ((callprojs->fallthrough_catchproj == call->in(0)) ||\n+      (callprojs->catchall_catchproj    == call->in(0)) ||\n+      (callprojs->fallthrough_memproj   == call->in(TypeFunc::Memory)) ||\n+      (callprojs->catchall_memproj      == call->in(TypeFunc::Memory)) ||\n+      (callprojs->fallthrough_ioproj    == call->in(TypeFunc::I_O)) ||\n+      (callprojs->catchall_ioproj       == call->in(TypeFunc::I_O)) ||\n+      (callprojs->exobj != nullptr && call->find_edge(callprojs->exobj) != -1)) {\n@@ -634,3 +660,12 @@\n-  \/\/ The call is marked as pure (no important side effects), but result isn't used.\n-  \/\/ It's safe to remove the call.\n-  bool result_not_used = (callprojs.resproj == nullptr || callprojs.resproj->outcnt() == 0);\n+\n+  bool result_not_used = true;\n+  for (uint i = 0; i < callprojs->nb_resproj; i++) {\n+    if (callprojs->resproj[i] != nullptr) {\n+      if (callprojs->resproj[i]->outcnt() != 0) {\n+        result_not_used = false;\n+      }\n+      if (call->find_edge(callprojs->resproj[i]) != -1) {\n+        return;\n+      }\n+    }\n+  }\n@@ -639,0 +674,2 @@\n+    \/\/ The call is marked as pure (no important side effects), but result isn't used.\n+    \/\/ It's safe to remove the call.\n@@ -651,0 +688,1 @@\n+    PhaseGVN& gvn = *C->initial_gvn();\n@@ -654,1 +692,1 @@\n-      C->initial_gvn()->set_type_bottom(mem);\n+      gvn.set_type_bottom(mem);\n@@ -658,4 +696,2 @@\n-    uint nargs = method()->arg_size();\n-    Node* top = C->top();\n-    for (uint i1 = 0; i1 < nargs; i1++) {\n-      map->set_req(TypeFunc::Parms + i1, top);\n+    for (uint i1 = TypeFunc::Parms; i1 < r->cnt(); i1++) {\n+      map->set_req(i1, C->top());\n@@ -669,0 +705,6 @@\n+    const TypeTuple* domain_sig = call->_tf->domain_sig();\n+    uint nargs = method()->arg_size();\n+    assert(domain_sig->cnt() - TypeFunc::Parms == nargs, \"inconsistent signature\");\n+\n+    uint j = TypeFunc::Parms;\n+    int arg_num = 0;\n@@ -670,1 +712,14 @@\n-      map->set_argument(jvms, i1, call->in(TypeFunc::Parms + i1));\n+      const Type* t = domain_sig->field_at(TypeFunc::Parms + i1);\n+      if (t->is_inlinetypeptr() && !method()->get_Method()->mismatch() && method()->is_scalarized_arg(arg_num)) {\n+        \/\/ Inline type arguments are not passed by reference: we get an argument per\n+        \/\/ field of the inline type. Build InlineTypeNodes from the inline type arguments.\n+        GraphKit arg_kit(jvms, &gvn);\n+        Node* vt = InlineTypeNode::make_from_multi(&arg_kit, call, t->inline_klass(), j, \/* in= *\/ true, \/* null_free= *\/ !t->maybe_null());\n+        map->set_control(arg_kit.control());\n+        map->set_argument(jvms, i1, vt);\n+      } else {\n+        map->set_argument(jvms, i1, call->in(j++));\n+      }\n+      if (t != Type::HALF) {\n+        arg_num++;\n+      }\n@@ -686,0 +741,20 @@\n+    \/\/ Check if we are late inlining a method handle call that returns an inline type as fields.\n+    Node* buffer_oop = nullptr;\n+    ciMethod* inline_method = inline_cg()->method();\n+    ciType* return_type = inline_method->return_type();\n+    if (!call->tf()->returns_inline_type_as_fields() && is_mh_late_inline() &&\n+        return_type->is_inlinetype() && return_type->as_inline_klass()->can_be_returned_as_fields()) {\n+      \/\/ Allocate a buffer for the inline type returned as fields because the caller expects an oop return.\n+      \/\/ Do this before the method handle call in case the buffer allocation triggers deoptimization and\n+      \/\/ we need to \"re-execute\" the call in the interpreter (to make sure the call is only executed once).\n+      GraphKit arg_kit(jvms, &gvn);\n+      {\n+        PreserveReexecuteState preexecs(&arg_kit);\n+        arg_kit.jvms()->set_should_reexecute(true);\n+        arg_kit.inc_sp(nargs);\n+        Node* klass_node = arg_kit.makecon(TypeKlassPtr::make(return_type->as_inline_klass()));\n+        buffer_oop = arg_kit.new_instance(klass_node, nullptr, nullptr, \/* deoptimize_on_exception *\/ true);\n+      }\n+      jvms = arg_kit.transfer_exceptions_into_jvms();\n+    }\n+\n@@ -714,2 +789,2 @@\n-      C->set_has_loops(C->has_loops() || inline_cg()->method()->has_loops());\n-      C->env()->notice_inlined_method(inline_cg()->method());\n+      C->set_has_loops(C->has_loops() || inline_method->has_loops());\n+      C->env()->notice_inlined_method(inline_method);\n@@ -719,0 +794,54 @@\n+\n+    \/\/ Handle inline type returns\n+    InlineTypeNode* vt = result->isa_InlineType();\n+    if (vt != nullptr) {\n+      if (call->tf()->returns_inline_type_as_fields()) {\n+        vt->replace_call_results(&kit, call, C, inline_method->signature()->returns_null_free_inline_type());\n+      } else if (vt->is_InlineType()) {\n+        \/\/ Result might still be allocated (for example, if it has been stored to a non-flat field)\n+        if (!vt->is_allocated(&kit.gvn())) {\n+          assert(buffer_oop != nullptr, \"should have allocated a buffer\");\n+          RegionNode* region = new RegionNode(3);\n+\n+          \/\/ Check if result is null\n+          Node* null_ctl = kit.top();\n+          if (!inline_method->signature()->returns_null_free_inline_type()) {\n+            kit.null_check_common(vt->get_is_init(), T_INT, false, &null_ctl);\n+          }\n+          region->init_req(1, null_ctl);\n+          PhiNode* oop = PhiNode::make(region, kit.gvn().zerocon(T_OBJECT), TypeInstPtr::make(TypePtr::BotPTR, vt->type()->inline_klass()));\n+          Node* init_mem = kit.reset_memory();\n+          PhiNode* mem = PhiNode::make(region, init_mem, Type::MEMORY, TypePtr::BOTTOM);\n+\n+          \/\/ Not null, initialize the buffer\n+          kit.set_all_memory(init_mem);\n+          vt->store(&kit, buffer_oop, buffer_oop, vt->type()->inline_klass());\n+          \/\/ Do not let stores that initialize this buffer be reordered with a subsequent\n+          \/\/ store that would make this buffer accessible by other threads.\n+          AllocateNode* alloc = AllocateNode::Ideal_allocation(buffer_oop);\n+          assert(alloc != nullptr, \"must have an allocation node\");\n+          kit.insert_mem_bar(Op_MemBarStoreStore, alloc->proj_out_or_null(AllocateNode::RawAddress));\n+          region->init_req(2, kit.control());\n+          oop->init_req(2, buffer_oop);\n+          mem->init_req(2, kit.merged_memory());\n+\n+          \/\/ Update oop input to buffer\n+          kit.gvn().hash_delete(vt);\n+          vt->set_oop(kit.gvn().transform(oop));\n+          vt->set_is_buffered(kit.gvn());\n+          vt = kit.gvn().transform(vt)->as_InlineType();\n+\n+          kit.set_control(kit.gvn().transform(region));\n+          kit.set_all_memory(kit.gvn().transform(mem));\n+          kit.record_for_igvn(region);\n+          kit.record_for_igvn(oop);\n+          kit.record_for_igvn(mem);\n+        }\n+        result = vt;\n+      }\n+      DEBUG_ONLY(buffer_oop = nullptr);\n+    } else {\n+      assert(result->is_top() || !call->tf()->returns_inline_type_as_fields(), \"Unexpected return value\");\n+    }\n+    assert(buffer_oop == nullptr, \"unused buffer allocation\");\n+\n@@ -941,0 +1070,23 @@\n+  \/\/ Allocate inline types if they are merged with objects (similar to Parse::merge_common())\n+  uint tos = kit.jvms()->stkoff() + kit.sp();\n+  uint limit = slow_map->req();\n+  for (uint i = TypeFunc::Parms; i < limit; i++) {\n+    Node* m = kit.map()->in(i);\n+    Node* n = slow_map->in(i);\n+    const Type* t = gvn.type(m)->meet_speculative(gvn.type(n));\n+    \/\/ TODO 8284443 still needed?\n+    if (m->is_InlineType() && !t->is_inlinetypeptr()) {\n+      \/\/ Allocate inline type in fast path\n+      m = m->as_InlineType()->buffer(&kit);\n+      kit.map()->set_req(i, m);\n+    }\n+    if (n->is_InlineType() && !t->is_inlinetypeptr()) {\n+      \/\/ Allocate inline type in slow path\n+      PreserveJVMState pjvms(&kit);\n+      kit.set_map(slow_map);\n+      n = n->as_InlineType()->buffer(&kit);\n+      kit.map()->set_req(i, n);\n+      slow_map = kit.stop();\n+    }\n+  }\n+\n@@ -964,2 +1116,0 @@\n-  uint tos = kit.jvms()->stkoff() + kit.sp();\n-  uint limit = slow_map->req();\n@@ -1001,2 +1151,2 @@\n-  if (IncrementalInlineMH && call_site_count > 0 &&\n-      (input_not_const || !C->inlining_incrementally() || C->over_inlining_cutoff())) {\n+  if (IncrementalInlineMH && (AlwaysIncrementalInline ||\n+                            (call_site_count > 0 && (input_not_const || !C->inlining_incrementally() || C->over_inlining_cutoff())))) {\n@@ -1010,0 +1160,19 @@\n+static void cast_argument(int nargs, int arg_nb, ciType* t, GraphKit& kit, bool null_free) {\n+  PhaseGVN& gvn = kit.gvn();\n+  Node* arg = kit.argument(arg_nb);\n+  const Type* arg_type = arg->bottom_type();\n+  const Type* sig_type = TypeOopPtr::make_from_klass(t->as_klass());\n+  if (t->as_klass()->is_inlinetype() && null_free) {\n+    sig_type = sig_type->filter_speculative(TypePtr::NOTNULL);\n+  }\n+  if (arg_type->isa_oopptr() && !arg_type->higher_equal(sig_type)) {\n+    const Type* narrowed_arg_type = arg_type->filter_speculative(sig_type); \/\/ keep speculative part\n+    arg = gvn.transform(new CheckCastPPNode(kit.control(), arg, narrowed_arg_type));\n+    kit.set_argument(arg_nb, arg);\n+  }\n+  if (sig_type->is_inlinetypeptr()) {\n+    arg = InlineTypeNode::make_from_oop(&kit, arg, sig_type->inline_klass(), !kit.gvn().type(arg)->maybe_null());\n+    kit.set_argument(arg_nb, arg);\n+  }\n+}\n+\n@@ -1061,0 +1230,1 @@\n+      int nargs = callee->arg_size();\n@@ -1062,1 +1232,1 @@\n-      Node* member_name = kit.argument(callee->arg_size() - 1);\n+      Node* member_name = kit.argument(nargs - 1);\n@@ -1082,8 +1252,1 @@\n-          Node* arg = kit.argument(0);\n-          const TypeOopPtr* arg_type = arg->bottom_type()->isa_oopptr();\n-          const Type*       sig_type = TypeOopPtr::make_from_klass(signature->accessing_klass());\n-          if (arg_type != nullptr && !arg_type->higher_equal(sig_type)) {\n-            const Type* recv_type = arg_type->filter_speculative(sig_type); \/\/ keep speculative part\n-            Node* cast_obj = gvn.transform(new CheckCastPPNode(kit.control(), arg, recv_type));\n-            kit.set_argument(0, cast_obj);\n-          }\n+          cast_argument(nargs, 0, signature->accessing_klass(), kit, false);\n@@ -1095,8 +1258,2 @@\n-            Node* arg = kit.argument(receiver_skip + j);\n-            const TypeOopPtr* arg_type = arg->bottom_type()->isa_oopptr();\n-            const Type*       sig_type = TypeOopPtr::make_from_klass(t->as_klass());\n-            if (arg_type != nullptr && !arg_type->higher_equal(sig_type)) {\n-              const Type* narrowed_arg_type = arg_type->filter_speculative(sig_type); \/\/ keep speculative part\n-              Node* cast_obj = gvn.transform(new CheckCastPPNode(kit.control(), arg, narrowed_arg_type));\n-              kit.set_argument(receiver_skip + j, cast_obj);\n-            }\n+            bool null_free = signature->is_null_free_at(i);\n+            cast_argument(nargs, receiver_skip + j, t, kit, null_free);\n@@ -1133,1 +1290,2 @@\n-                                              speculative_receiver_type);\n+                                              speculative_receiver_type,\n+                                              true);\n@@ -1208,1 +1366,1 @@\n-    Node* receiver = kit.null_check_receiver_before_call(method());\n+    kit.null_check_receiver_before_call(method());\n","filename":"src\/hotspot\/share\/opto\/callGenerator.cpp","additions":208,"deletions":50,"binary":false,"changes":258,"status":"modified"},{"patch":"@@ -47,3 +47,2 @@\n-  virtual bool           do_late_inline_check(Compile* C, JVMState* jvms) { ShouldNotReachHere(); return false;  }\n-  virtual CallGenerator* inline_cg()    const                             { ShouldNotReachHere(); return nullptr;}\n-  virtual bool           is_pure_call() const                             { ShouldNotReachHere(); return false;  }\n+  virtual bool           do_late_inline_check(Compile* C, JVMState* jvms) { ShouldNotReachHere(); return false; }\n+  virtual bool           is_pure_call() const                             { ShouldNotReachHere(); return false; }\n@@ -89,0 +88,2 @@\n+  virtual CallGenerator* inline_cg()    const                             { ShouldNotReachHere(); return nullptr;  }\n+\n","filename":"src\/hotspot\/share\/opto\/callGenerator.hpp","additions":4,"deletions":3,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -257,0 +257,2 @@\n+  InlineTypeNode* push_inline_types_through(PhaseGVN* phase, bool can_reshape, ciInlineKlass* vk);\n+\n@@ -438,0 +440,2 @@\n+  bool is_flat_array_check(PhaseTransform* phase, Node** array = nullptr);\n+\n@@ -696,0 +700,1 @@\n+    init_class_id(Class_Blackhole);\n","filename":"src\/hotspot\/share\/opto\/cfgnode.hpp","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -56,0 +56,1 @@\n+#include \"opto\/inlinetypenode.hpp\"\n@@ -399,0 +400,3 @@\n+  if (dead->is_InlineType()) {\n+    remove_inline_type(dead);\n+  }\n@@ -440,0 +444,3 @@\n+    if (n->outcnt() == 0) {\n+      worklist.push(n);\n+    }\n@@ -447,0 +454,6 @@\n+  remove_useless_nodes(_inline_type_nodes,  useful); \/\/ remove useless inline type nodes\n+#ifdef ASSERT\n+  if (_modified_nodes != nullptr) {\n+    _modified_nodes->remove_useless_nodes(useful.member_set());\n+  }\n+#endif\n@@ -623,0 +636,1 @@\n+                  _has_circular_inline_type(false),\n@@ -642,0 +656,1 @@\n+                  _inline_type_nodes (comp_arena(), 8, 0, nullptr),\n@@ -743,4 +758,2 @@\n-      const TypeTuple *domain = StartOSRNode::osr_domain();\n-      const TypeTuple *range = TypeTuple::make_range(method()->signature());\n-      init_tf(TypeFunc::make(domain, range));\n-      StartNode* s = new StartOSRNode(root(), domain);\n+      init_tf(TypeFunc::make(method(), \/* is_osr_compilation = *\/ true));\n+      StartNode* s = new StartOSRNode(root(), tf()->domain_sig());\n@@ -753,1 +766,1 @@\n-      StartNode* s = new StartNode(root(), tf()->domain());\n+      StartNode* s = new StartNode(root(), tf()->domain_cc());\n@@ -880,0 +893,10 @@\n+  if (needs_stack_repair()) {\n+    \/\/ One extra slot for the special stack increment value\n+    next_slot += 2;\n+  }\n+  \/\/ TODO 8284443 Only reserve extra slot if needed\n+  if (InlineTypeReturnedAsFields) {\n+    \/\/ One extra slot to hold the IsInit information for a nullable\n+    \/\/ inline type return if we run out of registers.\n+    next_slot += 2;\n+  }\n@@ -913,0 +936,1 @@\n+    _has_circular_inline_type(false),\n@@ -1042,0 +1066,4 @@\n+  _has_flat_accesses = false;\n+  _flat_accesses_share_alias = true;\n+  _scalarize_in_safepoints = false;\n+\n@@ -1330,1 +1358,2 @@\n-    assert(InlineUnsafeOps || StressReflectiveCode, \"indeterminate pointers come only from unsafe ops\");\n+    bool default_value_load = EnableValhalla && tj->is_instptr()->instance_klass() == ciEnv::current()->Class_klass();\n+    assert(InlineUnsafeOps || StressReflectiveCode || default_value_load, \"indeterminate pointers come only from unsafe ops\");\n@@ -1343,0 +1372,9 @@\n+  if (ta && ta->is_not_flat()) {\n+    \/\/ Erase not flat property for alias analysis.\n+    tj = ta = ta->cast_to_not_flat(false);\n+  }\n+  if (ta && ta->is_not_null_free()) {\n+    \/\/ Erase not null free property for alias analysis.\n+    tj = ta = ta->cast_to_not_null_free(false);\n+  }\n+\n@@ -1356,0 +1394,2 @@\n+    \/\/ For flat inline type array, each field has its own slice so\n+    \/\/ we must include the field offset.\n@@ -1396,1 +1436,1 @@\n-      tj = ta = TypeAryPtr::make(ptr,ta->const_oop(),tary,nullptr,false,offset);\n+      tj = ta = TypeAryPtr::make(ptr,ta->const_oop(),tary,nullptr,false,Type::Offset(offset), ta->field_offset());\n@@ -1400,1 +1440,6 @@\n-      tj = ta = TypeAryPtr::make(ptr,ta->const_oop(),tary,nullptr,false,offset);\n+      tj = ta = TypeAryPtr::make(ptr,ta->const_oop(),tary,nullptr,false,Type::Offset(offset), ta->field_offset());\n+    }\n+    \/\/ Initially all flattened array accesses share a single slice\n+    if (ta->is_flat() && ta->elem() != TypeInstPtr::BOTTOM && _flat_accesses_share_alias) {\n+      const TypeAry* tary = TypeAry::make(TypeInstPtr::BOTTOM, ta->size(), \/* stable= *\/ false, \/* flat= *\/ true);\n+      tj = ta = TypeAryPtr::make(ptr,ta->const_oop(),tary,nullptr,false,Type::Offset(offset), Type::Offset(Type::OffsetBot));\n@@ -1407,1 +1452,1 @@\n-      tj = ta = TypeAryPtr::make(ptr,ta->const_oop(),tary,aklass,false,offset);\n+      tj = ta = TypeAryPtr::make(ptr,ta->const_oop(),tary,aklass,false,Type::Offset(offset), ta->field_offset());\n@@ -1457,1 +1502,1 @@\n-        tj = to = TypeInstPtr::make(TypePtr::BotPTR, env()->Object_klass(), false, nullptr, offset);\n+        tj = to = TypeInstPtr::make(TypePtr::BotPTR, env()->Object_klass(), false, nullptr, Type::Offset(offset));\n@@ -1472,1 +1517,1 @@\n-          tj = to = TypeInstPtr::make(to->ptr(), canonical_holder, true, nullptr, offset, to->instance_id());\n+          tj = to = TypeInstPtr::make(to->ptr(), canonical_holder, true, nullptr, Type::Offset(offset), to->instance_id());\n@@ -1474,1 +1519,1 @@\n-          tj = to = TypeInstPtr::make(to->ptr(), canonical_holder, false, nullptr, offset);\n+          tj = to = TypeInstPtr::make(to->ptr(), canonical_holder, false, nullptr, Type::Offset(offset));\n@@ -1490,1 +1535,1 @@\n-                                       offset);\n+                                       Type::Offset(offset));\n@@ -1496,1 +1541,1 @@\n-        tj = tk = TypeInstKlassPtr::make(TypePtr::NotNull, env()->Object_klass(), offset);\n+        tj = tk = TypeInstKlassPtr::make(TypePtr::NotNull, env()->Object_klass(), Type::Offset(offset));\n@@ -1498,1 +1543,1 @@\n-        tj = tk = TypeAryKlassPtr::make(TypePtr::NotNull, tk->is_aryklassptr()->elem(), k, offset);\n+        tj = tk = TypeAryKlassPtr::make(TypePtr::NotNull, tk->is_aryklassptr()->elem(), k, Type::Offset(offset), tk->is_not_flat(), tk->is_not_null_free(), tk->is_null_free());\n@@ -1501,1 +1546,0 @@\n-\n@@ -1631,1 +1675,1 @@\n-Compile::AliasType* Compile::find_alias_type(const TypePtr* adr_type, bool no_create, ciField* original_field) {\n+Compile::AliasType* Compile::find_alias_type(const TypePtr* adr_type, bool no_create, ciField* original_field, bool uncached) {\n@@ -1636,3 +1680,6 @@\n-  AliasCacheEntry* ace = probe_alias_cache(adr_type);\n-  if (ace->_adr_type == adr_type) {\n-    return alias_type(ace->_index);\n+  AliasCacheEntry* ace = nullptr;\n+  if (!uncached) {\n+    ace = probe_alias_cache(adr_type);\n+    if (ace->_adr_type == adr_type) {\n+      return alias_type(ace->_index);\n+    }\n@@ -1688,0 +1735,1 @@\n+    ciField* field = nullptr;\n@@ -1694,0 +1742,1 @@\n+      const Type* elemtype = flat->is_aryptr()->elem();\n@@ -1695,1 +1744,8 @@\n-        alias_type(idx)->set_element(flat->is_aryptr()->elem());\n+        alias_type(idx)->set_element(elemtype);\n+      }\n+      int field_offset = flat->is_aryptr()->field_offset().get();\n+      if (flat->is_flat() &&\n+          field_offset != Type::OffsetBot) {\n+        ciInlineKlass* vk = elemtype->inline_klass();\n+        field_offset += vk->first_field_offset();\n+        field = vk->get_field_by_offset(field_offset, false);\n@@ -1707,0 +1763,2 @@\n+      if (flat->offset() == in_bytes(Klass::layout_helper_offset()))\n+        alias_type(idx)->set_rewritable(false);\n@@ -1717,1 +1775,0 @@\n-      ciField* field;\n@@ -1724,0 +1781,4 @@\n+      } else if (tinst->is_inlinetypeptr()) {\n+        \/\/ Inline type field\n+        ciInlineKlass* vk = tinst->inline_klass();\n+        field = vk->get_field_by_offset(tinst->offset(), false);\n@@ -1728,7 +1789,14 @@\n-      assert(field == nullptr ||\n-             original_field == nullptr ||\n-             (field->holder() == original_field->holder() &&\n-              field->offset_in_bytes() == original_field->offset_in_bytes() &&\n-              field->is_static() == original_field->is_static()), \"wrong field?\");\n-      \/\/ Set field() and is_rewritable() attributes.\n-      if (field != nullptr)  alias_type(idx)->set_field(field);\n+    }\n+    assert(field == nullptr ||\n+           original_field == nullptr ||\n+           (field->holder() == original_field->holder() &&\n+            field->offset_in_bytes() == original_field->offset_in_bytes() &&\n+            field->is_static() == original_field->is_static()), \"wrong field?\");\n+    \/\/ Set field() and is_rewritable() attributes.\n+    if (field != nullptr) {\n+      alias_type(idx)->set_field(field);\n+      if (flat->isa_aryptr()) {\n+        \/\/ Fields of flat arrays are rewritable although they are declared final\n+        assert(flat->is_flat(), \"must be a flat array\");\n+        alias_type(idx)->set_rewritable(true);\n+      }\n@@ -1739,3 +1807,4 @@\n-  ace->_adr_type = adr_type;\n-  ace->_index    = idx;\n-  assert(alias_type(adr_type) == alias_type(idx),  \"type must be installed\");\n+  if (!uncached) {\n+    ace->_adr_type = adr_type;\n+    ace->_index    = idx;\n+    assert(alias_type(adr_type) == alias_type(idx),  \"type must be installed\");\n@@ -1743,6 +1812,7 @@\n-  \/\/ Might as well try to fill the cache for the flattened version, too.\n-  AliasCacheEntry* face = probe_alias_cache(flat);\n-  if (face->_adr_type == nullptr) {\n-    face->_adr_type = flat;\n-    face->_index    = idx;\n-    assert(alias_type(flat) == alias_type(idx), \"flat type must work too\");\n+    \/\/ Might as well try to fill the cache for the flattened version, too.\n+    AliasCacheEntry* face = probe_alias_cache(flat);\n+    if (face->_adr_type == nullptr) {\n+      face->_adr_type = flat;\n+      face->_index    = idx;\n+      assert(alias_type(flat) == alias_type(idx), \"flat type must work too\");\n+    }\n@@ -1867,0 +1937,412 @@\n+void Compile::add_inline_type(Node* n) {\n+  assert(n->is_InlineType(), \"unexpected node\");\n+  _inline_type_nodes.push(n);\n+}\n+\n+void Compile::remove_inline_type(Node* n) {\n+  assert(n->is_InlineType(), \"unexpected node\");\n+  if (_inline_type_nodes.contains(n)) {\n+    _inline_type_nodes.remove(n);\n+  }\n+}\n+\n+\/\/ Does the return value keep otherwise useless inline type allocations alive?\n+static bool return_val_keeps_allocations_alive(Node* ret_val) {\n+  ResourceMark rm;\n+  Unique_Node_List wq;\n+  wq.push(ret_val);\n+  bool some_allocations = false;\n+  for (uint i = 0; i < wq.size(); i++) {\n+    Node* n = wq.at(i);\n+    if (n->outcnt() > 1) {\n+      \/\/ Some other use for the allocation\n+      return false;\n+    } else if (n->is_InlineType()) {\n+      wq.push(n->in(1));\n+    } else if (n->is_Phi()) {\n+      for (uint j = 1; j < n->req(); j++) {\n+        wq.push(n->in(j));\n+      }\n+    } else if (n->is_CheckCastPP() &&\n+               n->in(1)->is_Proj() &&\n+               n->in(1)->in(0)->is_Allocate()) {\n+      some_allocations = true;\n+    } else if (n->is_CheckCastPP()) {\n+      wq.push(n->in(1));\n+    }\n+  }\n+  return some_allocations;\n+}\n+\n+void Compile::process_inline_types(PhaseIterGVN &igvn, bool remove) {\n+  \/\/ Make sure that the return value does not keep an otherwise unused allocation alive\n+  if (tf()->returns_inline_type_as_fields()) {\n+    Node* ret = nullptr;\n+    for (uint i = 1; i < root()->req(); i++) {\n+      Node* in = root()->in(i);\n+      if (in->Opcode() == Op_Return) {\n+        assert(ret == nullptr, \"only one return\");\n+        ret = in;\n+      }\n+    }\n+    if (ret != nullptr) {\n+      Node* ret_val = ret->in(TypeFunc::Parms);\n+      if (igvn.type(ret_val)->isa_oopptr() &&\n+          return_val_keeps_allocations_alive(ret_val)) {\n+        igvn.replace_input_of(ret, TypeFunc::Parms, InlineTypeNode::tagged_klass(igvn.type(ret_val)->inline_klass(), igvn));\n+        assert(ret_val->outcnt() == 0, \"should be dead now\");\n+        igvn.remove_dead_node(ret_val);\n+      }\n+    }\n+  }\n+  if (_inline_type_nodes.length() == 0) {\n+    return;\n+  }\n+  \/\/ Scalarize inline types in safepoint debug info.\n+  \/\/ Delay this until all inlining is over to avoid getting inconsistent debug info.\n+  set_scalarize_in_safepoints(true);\n+  for (int i = _inline_type_nodes.length()-1; i >= 0; i--) {\n+    _inline_type_nodes.at(i)->as_InlineType()->make_scalar_in_safepoints(&igvn);\n+  }\n+  if (remove) {\n+    \/\/ Remove inline type nodes by replacing them with their oop input\n+    while (_inline_type_nodes.length() > 0) {\n+      InlineTypeNode* vt = _inline_type_nodes.pop()->as_InlineType();\n+      if (vt->outcnt() == 0) {\n+        igvn.remove_dead_node(vt);\n+        continue;\n+      }\n+      for (DUIterator i = vt->outs(); vt->has_out(i); i++) {\n+        DEBUG_ONLY(bool must_be_buffered = false);\n+        Node* u = vt->out(i);\n+        \/\/ Check if any users are blackholes. If so, rewrite them to use either the\n+        \/\/ allocated buffer, or individual components, instead of the inline type node\n+        \/\/ that goes away.\n+        if (u->is_Blackhole()) {\n+          BlackholeNode* bh = u->as_Blackhole();\n+\n+          \/\/ Unlink the old input\n+          int idx = bh->find_edge(vt);\n+          assert(idx != -1, \"The edge should be there\");\n+          bh->del_req(idx);\n+          --i;\n+\n+          if (vt->is_allocated(&igvn)) {\n+            \/\/ Already has the allocated instance, blackhole that\n+            bh->add_req(vt->get_oop());\n+          } else {\n+            \/\/ Not allocated yet, blackhole the components\n+            for (uint c = 0; c < vt->field_count(); c++) {\n+              bh->add_req(vt->field_value(c));\n+            }\n+          }\n+\n+          \/\/ Node modified, record for IGVN\n+          igvn.record_for_igvn(bh);\n+        }\n+#ifdef ASSERT\n+        \/\/ Verify that inline type is buffered when replacing by oop\n+        else if (u->is_InlineType()) {\n+          \/\/ InlineType uses don't need buffering because they are about to be replaced as well\n+        } else if (u->is_Phi()) {\n+          \/\/ TODO 8302217 Remove this once InlineTypeNodes are reliably pushed through\n+        } else {\n+          must_be_buffered = true;\n+        }\n+        if (must_be_buffered && !vt->is_allocated(&igvn)) {\n+          vt->dump(0);\n+          u->dump(0);\n+          assert(false, \"Should have been buffered\");\n+        }\n+#endif\n+      }\n+      igvn.replace_node(vt, vt->get_oop());\n+    }\n+  }\n+  igvn.optimize();\n+}\n+\n+void Compile::adjust_flat_array_access_aliases(PhaseIterGVN& igvn) {\n+  if (!_has_flat_accesses) {\n+    return;\n+  }\n+  \/\/ Initially, all flat array accesses share the same slice to\n+  \/\/ keep dependencies with Object[] array accesses (that could be\n+  \/\/ to a flat array) correct. We're done with parsing so we\n+  \/\/ now know all flat array accesses in this compile\n+  \/\/ unit. Let's move flat array accesses to their own slice,\n+  \/\/ one per element field. This should help memory access\n+  \/\/ optimizations.\n+  ResourceMark rm;\n+  Unique_Node_List wq;\n+  wq.push(root());\n+\n+  Node_List mergememnodes;\n+  Node_List memnodes;\n+\n+  \/\/ Alias index currently shared by all flat memory accesses\n+  int index = get_alias_index(TypeAryPtr::INLINES);\n+\n+  \/\/ Find MergeMem nodes and flat array accesses\n+  for (uint i = 0; i < wq.size(); i++) {\n+    Node* n = wq.at(i);\n+    if (n->is_Mem()) {\n+      const TypePtr* adr_type = nullptr;\n+      if (n->Opcode() == Op_StoreCM) {\n+        adr_type = get_adr_type(get_alias_index(n->in(MemNode::OopStore)->adr_type()));\n+      } else {\n+        adr_type = get_adr_type(get_alias_index(n->adr_type()));\n+      }\n+      if (adr_type == TypeAryPtr::INLINES) {\n+        memnodes.push(n);\n+      }\n+    } else if (n->is_MergeMem()) {\n+      MergeMemNode* mm = n->as_MergeMem();\n+      if (mm->memory_at(index) != mm->base_memory()) {\n+        mergememnodes.push(n);\n+      }\n+    }\n+    for (uint j = 0; j < n->req(); j++) {\n+      Node* m = n->in(j);\n+      if (m != nullptr) {\n+        wq.push(m);\n+      }\n+    }\n+  }\n+\n+  if (memnodes.size() > 0) {\n+    _flat_accesses_share_alias = false;\n+\n+    \/\/ We are going to change the slice for the flat array\n+    \/\/ accesses so we need to clear the cache entries that refer to\n+    \/\/ them.\n+    for (uint i = 0; i < AliasCacheSize; i++) {\n+      AliasCacheEntry* ace = &_alias_cache[i];\n+      if (ace->_adr_type != nullptr &&\n+          ace->_adr_type->is_flat()) {\n+        ace->_adr_type = nullptr;\n+        ace->_index = (i != 0) ? 0 : AliasIdxTop; \/\/ Make sure the nullptr adr_type resolves to AliasIdxTop\n+      }\n+    }\n+\n+    \/\/ Find what aliases we are going to add\n+    int start_alias = num_alias_types()-1;\n+    int stop_alias = 0;\n+\n+    for (uint i = 0; i < memnodes.size(); i++) {\n+      Node* m = memnodes.at(i);\n+      const TypePtr* adr_type = nullptr;\n+      if (m->Opcode() == Op_StoreCM) {\n+        adr_type = m->in(MemNode::OopStore)->adr_type();\n+        if (adr_type != TypeAryPtr::INLINES) {\n+          \/\/ store was optimized out and we lost track of the adr_type\n+          Node* clone = new StoreCMNode(m->in(MemNode::Control), m->in(MemNode::Memory), m->in(MemNode::Address),\n+                                        m->adr_type(), m->in(MemNode::ValueIn), m->in(MemNode::OopStore),\n+                                        get_alias_index(adr_type));\n+          igvn.register_new_node_with_optimizer(clone);\n+          igvn.replace_node(m, clone);\n+        }\n+      } else {\n+        adr_type = m->adr_type();\n+#ifdef ASSERT\n+        m->as_Mem()->set_adr_type(adr_type);\n+#endif\n+      }\n+      int idx = get_alias_index(adr_type);\n+      start_alias = MIN2(start_alias, idx);\n+      stop_alias = MAX2(stop_alias, idx);\n+    }\n+\n+    assert(stop_alias >= start_alias, \"should have expanded aliases\");\n+\n+    Node_Stack stack(0);\n+#ifdef ASSERT\n+    VectorSet seen(Thread::current()->resource_area());\n+#endif\n+    \/\/ Now let's fix the memory graph so each flat array access\n+    \/\/ is moved to the right slice. Start from the MergeMem nodes.\n+    uint last = unique();\n+    for (uint i = 0; i < mergememnodes.size(); i++) {\n+      MergeMemNode* current = mergememnodes.at(i)->as_MergeMem();\n+      Node* n = current->memory_at(index);\n+      MergeMemNode* mm = nullptr;\n+      do {\n+        \/\/ Follow memory edges through memory accesses, phis and\n+        \/\/ narrow membars and push nodes on the stack. Once we hit\n+        \/\/ bottom memory, we pop element off the stack one at a\n+        \/\/ time, in reverse order, and move them to the right slice\n+        \/\/ by changing their memory edges.\n+        if ((n->is_Phi() && n->adr_type() != TypePtr::BOTTOM) || n->is_Mem() || n->adr_type() == TypeAryPtr::INLINES) {\n+          assert(!seen.test_set(n->_idx), \"\");\n+          \/\/ Uses (a load for instance) will need to be moved to the\n+          \/\/ right slice as well and will get a new memory state\n+          \/\/ that we don't know yet. The use could also be the\n+          \/\/ backedge of a loop. We put a place holder node between\n+          \/\/ the memory node and its uses. We replace that place\n+          \/\/ holder with the correct memory state once we know it,\n+          \/\/ i.e. when nodes are popped off the stack. Using the\n+          \/\/ place holder make the logic work in the presence of\n+          \/\/ loops.\n+          if (n->outcnt() > 1) {\n+            Node* place_holder = nullptr;\n+            assert(!n->has_out_with(Op_Node), \"\");\n+            for (DUIterator k = n->outs(); n->has_out(k); k++) {\n+              Node* u = n->out(k);\n+              if (u != current && u->_idx < last) {\n+                bool success = false;\n+                for (uint l = 0; l < u->req(); l++) {\n+                  if (!stack.is_empty() && u == stack.node() && l == stack.index()) {\n+                    continue;\n+                  }\n+                  Node* in = u->in(l);\n+                  if (in == n) {\n+                    if (place_holder == nullptr) {\n+                      place_holder = new Node(1);\n+                      place_holder->init_req(0, n);\n+                    }\n+                    igvn.replace_input_of(u, l, place_holder);\n+                    success = true;\n+                  }\n+                }\n+                if (success) {\n+                  --k;\n+                }\n+              }\n+            }\n+          }\n+          if (n->is_Phi()) {\n+            stack.push(n, 1);\n+            n = n->in(1);\n+          } else if (n->is_Mem()) {\n+            stack.push(n, n->req());\n+            n = n->in(MemNode::Memory);\n+          } else {\n+            assert(n->is_Proj() && n->in(0)->Opcode() == Op_MemBarCPUOrder, \"\");\n+            stack.push(n, n->req());\n+            n = n->in(0)->in(TypeFunc::Memory);\n+          }\n+        } else {\n+          assert(n->adr_type() == TypePtr::BOTTOM || (n->Opcode() == Op_Node && n->_idx >= last) || (n->is_Proj() && n->in(0)->is_Initialize()), \"\");\n+          \/\/ Build a new MergeMem node to carry the new memory state\n+          \/\/ as we build it. IGVN should fold extraneous MergeMem\n+          \/\/ nodes.\n+          mm = MergeMemNode::make(n);\n+          igvn.register_new_node_with_optimizer(mm);\n+          while (stack.size() > 0) {\n+            Node* m = stack.node();\n+            uint idx = stack.index();\n+            if (m->is_Mem()) {\n+              \/\/ Move memory node to its new slice\n+              const TypePtr* adr_type = m->adr_type();\n+              int alias = get_alias_index(adr_type);\n+              Node* prev = mm->memory_at(alias);\n+              igvn.replace_input_of(m, MemNode::Memory, prev);\n+              mm->set_memory_at(alias, m);\n+            } else if (m->is_Phi()) {\n+              \/\/ We need as many new phis as there are new aliases\n+              igvn.replace_input_of(m, idx, mm);\n+              if (idx == m->req()-1) {\n+                Node* r = m->in(0);\n+                for (uint j = (uint)start_alias; j <= (uint)stop_alias; j++) {\n+                  const TypePtr* adr_type = get_adr_type(j);\n+                  if (!adr_type->isa_aryptr() || !adr_type->is_flat() || j == (uint)index) {\n+                    continue;\n+                  }\n+                  Node* phi = new PhiNode(r, Type::MEMORY, get_adr_type(j));\n+                  igvn.register_new_node_with_optimizer(phi);\n+                  for (uint k = 1; k < m->req(); k++) {\n+                    phi->init_req(k, m->in(k)->as_MergeMem()->memory_at(j));\n+                  }\n+                  mm->set_memory_at(j, phi);\n+                }\n+                Node* base_phi = new PhiNode(r, Type::MEMORY, TypePtr::BOTTOM);\n+                igvn.register_new_node_with_optimizer(base_phi);\n+                for (uint k = 1; k < m->req(); k++) {\n+                  base_phi->init_req(k, m->in(k)->as_MergeMem()->base_memory());\n+                }\n+                mm->set_base_memory(base_phi);\n+              }\n+            } else {\n+              \/\/ This is a MemBarCPUOrder node from\n+              \/\/ Parse::array_load()\/Parse::array_store(), in the\n+              \/\/ branch that handles flat arrays hidden under\n+              \/\/ an Object[] array. We also need one new membar per\n+              \/\/ new alias to keep the unknown access that the\n+              \/\/ membars protect properly ordered with accesses to\n+              \/\/ known flat array.\n+              assert(m->is_Proj(), \"projection expected\");\n+              Node* ctrl = m->in(0)->in(TypeFunc::Control);\n+              igvn.replace_input_of(m->in(0), TypeFunc::Control, top());\n+              for (uint j = (uint)start_alias; j <= (uint)stop_alias; j++) {\n+                const TypePtr* adr_type = get_adr_type(j);\n+                if (!adr_type->isa_aryptr() || !adr_type->is_flat() || j == (uint)index) {\n+                  continue;\n+                }\n+                MemBarNode* mb = new MemBarCPUOrderNode(this, j, nullptr);\n+                igvn.register_new_node_with_optimizer(mb);\n+                Node* mem = mm->memory_at(j);\n+                mb->init_req(TypeFunc::Control, ctrl);\n+                mb->init_req(TypeFunc::Memory, mem);\n+                ctrl = new ProjNode(mb, TypeFunc::Control);\n+                igvn.register_new_node_with_optimizer(ctrl);\n+                mem = new ProjNode(mb, TypeFunc::Memory);\n+                igvn.register_new_node_with_optimizer(mem);\n+                mm->set_memory_at(j, mem);\n+              }\n+              igvn.replace_node(m->in(0)->as_Multi()->proj_out(TypeFunc::Control), ctrl);\n+            }\n+            if (idx < m->req()-1) {\n+              idx += 1;\n+              stack.set_index(idx);\n+              n = m->in(idx);\n+              break;\n+            }\n+            \/\/ Take care of place holder nodes\n+            if (m->has_out_with(Op_Node)) {\n+              Node* place_holder = m->find_out_with(Op_Node);\n+              if (place_holder != nullptr) {\n+                Node* mm_clone = mm->clone();\n+                igvn.register_new_node_with_optimizer(mm_clone);\n+                Node* hook = new Node(1);\n+                hook->init_req(0, mm);\n+                igvn.replace_node(place_holder, mm_clone);\n+                hook->destruct(&igvn);\n+              }\n+              assert(!m->has_out_with(Op_Node), \"place holder should be gone now\");\n+            }\n+            stack.pop();\n+          }\n+        }\n+      } while(stack.size() > 0);\n+      \/\/ Fix the memory state at the MergeMem we started from\n+      igvn.rehash_node_delayed(current);\n+      for (uint j = (uint)start_alias; j <= (uint)stop_alias; j++) {\n+        const TypePtr* adr_type = get_adr_type(j);\n+        if (!adr_type->isa_aryptr() || !adr_type->is_flat()) {\n+          continue;\n+        }\n+        current->set_memory_at(j, mm);\n+      }\n+      current->set_memory_at(index, current->base_memory());\n+    }\n+    igvn.optimize();\n+  }\n+  print_method(PHASE_SPLIT_INLINES_ARRAY, 2);\n+#ifdef ASSERT\n+  if (!_flat_accesses_share_alias) {\n+    wq.clear();\n+    wq.push(root());\n+    for (uint i = 0; i < wq.size(); i++) {\n+      Node* n = wq.at(i);\n+      assert(n->adr_type() != TypeAryPtr::INLINES, \"should have been removed from the graph\");\n+      for (uint j = 0; j < n->req(); j++) {\n+        Node* m = n->in(j);\n+        if (m != nullptr) {\n+          wq.push(m);\n+        }\n+      }\n+    }\n+  }\n+#endif\n+}\n+\n@@ -2152,1 +2634,4 @@\n-  assert(_modified_nodes == nullptr, \"not allowed\");\n+#ifdef ASSERT\n+  Unique_Node_List* modified_nodes = _modified_nodes;\n+  _modified_nodes = nullptr;\n+#endif\n@@ -2165,0 +2650,1 @@\n+  DEBUG_ONLY( _modified_nodes = modified_nodes; )\n@@ -2302,0 +2788,5 @@\n+  \/\/ Process inline type nodes now that all inlining is over\n+  process_inline_types(igvn);\n+\n+  adjust_flat_array_access_aliases(igvn);\n+\n@@ -2418,0 +2909,8 @@\n+  assert(_late_inlines.length() == 0 || IncrementalInlineMH || IncrementalInlineVirtual, \"not empty\");\n+\n+  if (_late_inlines.length() > 0) {\n+    \/\/ More opportunities to optimize virtual and MH calls.\n+    \/\/ Though it's maybe too late to perform inlining, strength-reducing them to direct calls is still an option.\n+    process_late_inline_calls_no_inline(igvn);\n+  }\n+\n@@ -2428,0 +2927,4 @@\n+  \/\/ Process inline type nodes again and remove them. From here\n+  \/\/ on we don't need to keep track of field values anymore.\n+  process_inline_types(igvn, \/* remove= *\/ true);\n+\n@@ -2443,0 +2946,1 @@\n+  DEBUG_ONLY( _late_inlines.clear(); )\n@@ -2445,8 +2949,0 @@\n-\n-  assert(_late_inlines.length() == 0 || IncrementalInlineMH || IncrementalInlineVirtual, \"not empty\");\n-\n-  if (_late_inlines.length() > 0) {\n-    \/\/ More opportunities to optimize virtual and MH calls.\n-    \/\/ Though it's maybe too late to perform inlining, strength-reducing them to direct calls is still an option.\n-    process_late_inline_calls_no_inline(igvn);\n-  }\n@@ -3075,0 +3571,1 @@\n+\n@@ -3227,1 +3724,16 @@\n-      n->add_prec(prec);\n+      if (prec->is_MergeMem()) {\n+        MergeMemNode* mm = prec->as_MergeMem();\n+        Node* base = mm->base_memory();\n+        for (int i = AliasIdxRaw + 1; i < num_alias_types(); i++) {\n+          const TypePtr* adr_type = get_adr_type(i);\n+          if (adr_type->is_flat()) {\n+            Node* m = mm->memory_at(i);\n+            n->add_prec(m);\n+          }\n+        }\n+        if (mm->outcnt() == 0) {\n+          mm->disconnect_inputs(this);\n+        }\n+      } else {\n+        n->add_prec(prec);\n+      }\n@@ -3823,0 +4335,7 @@\n+#ifdef ASSERT\n+  case Op_InlineType: {\n+    n->dump(-1);\n+    assert(false, \"inline type node was not removed\");\n+    break;\n+  }\n+#endif\n@@ -4204,2 +4723,2 @@\n-      if (accessing_method->is_static_initializer() ||\n-          accessing_method->is_object_initializer() ||\n+      if (accessing_method->is_class_initializer() ||\n+          accessing_method->is_object_constructor() ||\n@@ -4213,1 +4732,1 @@\n-      if (accessing_method->is_static_initializer()) {\n+      if (accessing_method->is_class_initializer()) {\n@@ -4269,0 +4788,1 @@\n+               (n->is_Allocate() && i >= AllocateNode::InlineType) ||\n@@ -4271,1 +4791,1 @@\n-              \"only region, phi, arraycopy, unlock or membar nodes have null data edges\");\n+              \"only region, phi, arraycopy, allocate, unlock or membar nodes have null data edges\");\n@@ -4397,0 +4917,8 @@\n+\n+    \/\/ Do not fold the subtype check to an array klass pointer comparison for [V? arrays.\n+    \/\/ [QMyValue is a subtype of [LMyValue but the klass for [QMyValue is not equal to\n+    \/\/ the klass for [LMyValue. Perform a full test.\n+    if (!superk->is_aryklassptr()->is_null_free() && superk->is_aryklassptr()->elem()->isa_instklassptr() &&\n+        superk->is_aryklassptr()->elem()->is_instklassptr()->instance_klass()->is_inlinetype()) {\n+      return SSC_full_test;\n+    }\n@@ -4957,0 +5485,21 @@\n+Node* Compile::optimize_acmp(PhaseGVN* phase, Node* a, Node* b) {\n+  const TypeInstPtr* ta = phase->type(a)->isa_instptr();\n+  const TypeInstPtr* tb = phase->type(b)->isa_instptr();\n+  if (!EnableValhalla || ta == nullptr || tb == nullptr ||\n+      ta->is_zero_type() || tb->is_zero_type() ||\n+      !ta->can_be_inline_type() || !tb->can_be_inline_type()) {\n+    \/\/ Use old acmp if one operand is null or not an inline type\n+    return new CmpPNode(a, b);\n+  } else if (ta->is_inlinetypeptr() || tb->is_inlinetypeptr()) {\n+    \/\/ We know that one operand is an inline type. Therefore,\n+    \/\/ new acmp will only return true if both operands are nullptr.\n+    \/\/ Check if both operands are null by or'ing the oops.\n+    a = phase->transform(new CastP2XNode(nullptr, a));\n+    b = phase->transform(new CastP2XNode(nullptr, b));\n+    a = phase->transform(new OrXNode(a, b));\n+    return new CmpXNode(a, phase->MakeConX(0));\n+  }\n+  \/\/ Use new acmp\n+  return nullptr;\n+}\n+\n","filename":"src\/hotspot\/share\/opto\/compile.cpp","additions":601,"deletions":52,"binary":false,"changes":653,"status":"modified"},{"patch":"@@ -54,0 +54,1 @@\n+class CallNode;\n@@ -95,0 +96,1 @@\n+class InlineTypeNode;\n@@ -324,0 +326,1 @@\n+  bool                  _has_circular_inline_type; \/\/ True if method loads an inline type with a circular, non-flat field\n@@ -350,0 +353,3 @@\n+  bool                  _has_flat_accesses;     \/\/ Any known flat array accesses?\n+  bool                  _flat_accesses_share_alias; \/\/ Initially all flat array share a single slice\n+  bool                  _scalarize_in_safepoints; \/\/ Scalarize inline types in safepoint debug info\n@@ -365,0 +371,1 @@\n+  GrowableArray<Node*>  _inline_type_nodes;     \/\/ List of InlineType nodes\n@@ -624,0 +631,2 @@\n+  bool              has_circular_inline_type() const { return _has_circular_inline_type; }\n+  void          set_has_circular_inline_type(bool z) { _has_circular_inline_type = z; }\n@@ -660,0 +669,10 @@\n+  void          set_flat_accesses()              { _has_flat_accesses = true; }\n+  bool          flat_accesses_share_alias() const { return _flat_accesses_share_alias; }\n+  void          set_flat_accesses_share_alias(bool z) { _flat_accesses_share_alias = z; }\n+  bool          scalarize_in_safepoints() const { return _scalarize_in_safepoints; }\n+  void          set_scalarize_in_safepoints(bool z) { _scalarize_in_safepoints = z; }\n+\n+  \/\/ Support for scalarized inline type calling convention\n+  bool              has_scalarized_args() const  { return _method != nullptr && _method->has_scalarized_args(); }\n+  bool              needs_stack_repair()  const  { return _method != nullptr && _method->get_Method()->c2_needs_stack_repair(); }\n+\n@@ -776,0 +795,7 @@\n+  \/\/ Keep track of inline type nodes for later processing\n+  void add_inline_type(Node* n);\n+  void remove_inline_type(Node* n);\n+  void process_inline_types(PhaseIterGVN &igvn, bool remove = false);\n+\n+  void adjust_flat_array_access_aliases(PhaseIterGVN& igvn);\n+\n@@ -913,1 +939,1 @@\n-  AliasType*        alias_type(const TypePtr* adr_type, ciField* field = nullptr) { return find_alias_type(adr_type, false, field); }\n+  AliasType*        alias_type(const TypePtr* adr_type, ciField* field = nullptr, bool uncached = false) { return find_alias_type(adr_type, false, field, uncached); }\n@@ -917,1 +943,1 @@\n-  int               get_alias_index(const TypePtr* at)  { return alias_type(at)->index(); }\n+  int               get_alias_index(const TypePtr* at, bool uncached = false) { return alias_type(at, nullptr, uncached)->index(); }\n@@ -1160,1 +1186,1 @@\n-  AliasType* find_alias_type(const TypePtr* adr_type, bool no_create, ciField* field);\n+  AliasType* find_alias_type(const TypePtr* adr_type, bool no_create, ciField* field, bool uncached = false);\n@@ -1236,1 +1262,3 @@\n-  \/\/ Auxiliary methods for randomized fuzzing\/stressing\n+  Node* optimize_acmp(PhaseGVN* phase, Node* a, Node* b);\n+\n+  \/\/ Auxiliary method for randomized fuzzing\/stressing\n","filename":"src\/hotspot\/share\/opto\/compile.hpp","additions":32,"deletions":4,"binary":false,"changes":36,"status":"modified"},{"patch":"@@ -41,0 +41,1 @@\n+#include \"opto\/inlinetypenode.hpp\"\n@@ -600,1 +601,1 @@\n-  if (iter().cur_bc_raw() == Bytecodes::_invokespecial && !orig_callee->is_object_initializer()) {\n+  if (iter().cur_bc_raw() == Bytecodes::_invokespecial && !orig_callee->is_object_constructor()) {\n@@ -730,1 +731,1 @@\n-          \/\/ It's OK for a method  to return a value that is discarded.\n+          \/\/ It's OK for a method to return a value that is discarded.\n@@ -742,0 +743,3 @@\n+            if (declared_signature->returns_null_free_inline_type()) {\n+              sig_type = sig_type->join_speculative(TypePtr::NOTNULL);\n+            }\n@@ -788,0 +792,5 @@\n+    if (rtype->is_inlinetype() && !peek()->is_InlineType()) {\n+      Node* retnode = pop();\n+      retnode = InlineTypeNode::make_from_oop(this, retnode, rtype->as_inline_klass(), !gvn().type(retnode)->maybe_null());\n+      push_node(T_OBJECT, retnode);\n+    }\n","filename":"src\/hotspot\/share\/opto\/doCall.cpp","additions":11,"deletions":2,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -26,0 +26,2 @@\n+#include \"ci\/ciFlatArrayKlass.hpp\"\n+#include \"ci\/ciInlineKlass.hpp\"\n@@ -40,0 +42,1 @@\n+#include \"opto\/inlinetypenode.hpp\"\n@@ -43,0 +46,1 @@\n+#include \"opto\/narrowptrnode.hpp\"\n@@ -56,1 +60,1 @@\n-GraphKit::GraphKit(JVMState* jvms)\n+GraphKit::GraphKit(JVMState* jvms, PhaseGVN* gvn)\n@@ -59,1 +63,1 @@\n-    _gvn(*C->initial_gvn()),\n+    _gvn((gvn != nullptr) ? *gvn : *C->initial_gvn()),\n@@ -62,0 +66,1 @@\n+  assert(gvn == nullptr || !gvn->is_IterGVN() || gvn->is_IterGVN()->delay_transform(), \"delay transform should be enabled\");\n@@ -65,0 +70,7 @@\n+#ifdef ASSERT\n+  if (_gvn.is_IterGVN() != nullptr) {\n+    assert(_gvn.is_IterGVN()->delay_transform(), \"Transformation must be delayed if IterGVN is used\");\n+    \/\/ Save the initial size of _for_igvn worklist for verification (see ~GraphKit)\n+    _worklist_size = _gvn.C->igvn_worklist()->size();\n+  }\n+#endif\n@@ -860,1 +872,1 @@\n-           (is_anewarray && code == Bytecodes::_multianewarray);\n+           (is_anewarray && (code == Bytecodes::_multianewarray));\n@@ -1120,0 +1132,9 @@\n+  case Bytecodes::_withfield: {\n+    bool ignored_will_link;\n+    ciField* field = method()->get_field_at_bci(bci(), ignored_will_link);\n+    int      size  = field->type()->size();\n+    inputs = size+1;\n+    depth = rsize() - inputs;\n+    break;\n+  }\n+\n@@ -1202,1 +1223,1 @@\n-  return _gvn.transform(LoadKlassNode::make(_gvn, nullptr, immutable_memory(), k_adr, TypeInstPtr::KLASS));\n+  return _gvn.transform(LoadKlassNode::make(_gvn, nullptr, immutable_memory(), k_adr, TypeInstPtr::KLASS, TypeInstKlassPtr::OBJECT));\n@@ -1251,1 +1272,2 @@\n-                                  bool speculative) {\n+                                  bool speculative,\n+                                  bool is_init_check) {\n@@ -1256,0 +1278,22 @@\n+  if (value->is_InlineType()) {\n+    \/\/ Null checking a scalarized but nullable inline type. Check the IsInit\n+    \/\/ input instead of the oop input to avoid keeping buffer allocations alive.\n+    InlineTypeNode* vtptr = value->as_InlineType();\n+    while (vtptr->get_oop()->is_InlineType()) {\n+      vtptr = vtptr->get_oop()->as_InlineType();\n+    }\n+    null_check_common(vtptr->get_is_init(), T_INT, assert_null, null_control, speculative, true);\n+    if (stopped()) {\n+      return top();\n+    }\n+    if (assert_null) {\n+      \/\/ TODO 8284443 Scalarize here (this currently leads to compilation bailouts)\n+      \/\/ vtptr = InlineTypeNode::make_null(_gvn, vtptr->type()->inline_klass());\n+      \/\/ replace_in_map(value, vtptr);\n+      \/\/ return vtptr;\n+      return null();\n+    }\n+    bool do_replace_in_map = (null_control == nullptr || (*null_control) == top());\n+    return cast_not_null(value, do_replace_in_map);\n+  }\n+\n@@ -1359,1 +1403,1 @@\n-  } else if (type == T_OBJECT) {\n+  } else if (type == T_OBJECT || is_init_check) {\n@@ -1433,1 +1477,0 @@\n-\n@@ -1437,0 +1480,9 @@\n+  if (obj->is_InlineType()) {\n+    Node* vt = obj->clone();\n+    vt->as_InlineType()->set_is_init(_gvn);\n+    vt = _gvn.transform(vt);\n+    if (do_replace_in_map) {\n+      replace_in_map(obj, vt);\n+    }\n+    return vt;\n+  }\n@@ -1563,0 +1615,1 @@\n+\n@@ -1609,1 +1662,2 @@\n-                                DecoratorSet decorators) {\n+                                DecoratorSet decorators,\n+                                bool safe_for_replace) {\n@@ -1622,0 +1676,7 @@\n+  if (val->is_InlineType()) {\n+    \/\/ Store to non-flat field. Buffer the inline type and make sure\n+    \/\/ the store is re-executed if the allocation triggers deoptimization.\n+    PreserveReexecuteState preexecs(this);\n+    jvms()->set_should_reexecute(true);\n+    val = val->as_InlineType()->buffer(this, safe_for_replace);\n+  }\n@@ -1638,1 +1699,2 @@\n-                               DecoratorSet decorators) {\n+                               DecoratorSet decorators,\n+                               Node* ctl) {\n@@ -1644,1 +1706,1 @@\n-  C2ParseAccess access(this, decorators | C2_READ_ACCESS, bt, obj, addr);\n+  C2ParseAccess access(this, decorators | C2_READ_ACCESS, bt, obj, addr, ctl);\n@@ -1749,1 +1811,2 @@\n-  uint shift  = exact_log2(type2aelembytes(elembt));\n+  const TypeAryPtr* arytype = _gvn.type(ary)->is_aryptr();\n+  uint shift = arytype->is_flat() ? arytype->flat_log_elem_size() : exact_log2(type2aelembytes(elembt));\n@@ -1781,6 +1844,45 @@\n-void GraphKit::set_arguments_for_java_call(CallJavaNode* call) {\n-  \/\/ Add the call arguments:\n-  uint nargs = call->method()->arg_size();\n-  for (uint i = 0; i < nargs; i++) {\n-    Node* arg = argument(i);\n-    call->init_req(i + TypeFunc::Parms, arg);\n+void GraphKit::set_arguments_for_java_call(CallJavaNode* call, bool is_late_inline) {\n+  PreserveReexecuteState preexecs(this);\n+  if (EnableValhalla) {\n+    \/\/ Make sure the call is \"re-executed\", if buffering of inline type arguments triggers deoptimization.\n+    \/\/ At this point, the call hasn't been executed yet, so we will only ever execute the call once.\n+    jvms()->set_should_reexecute(true);\n+    int arg_size = method()->get_declared_signature_at_bci(bci())->arg_size_for_bc(java_bc());\n+    inc_sp(arg_size);\n+  }\n+  \/\/ Add the call arguments\n+  const TypeTuple* domain = call->tf()->domain_sig();\n+  uint nargs = domain->cnt();\n+  int arg_num = 0;\n+  for (uint i = TypeFunc::Parms, idx = TypeFunc::Parms; i < nargs; i++) {\n+    Node* arg = argument(i-TypeFunc::Parms);\n+    const Type* t = domain->field_at(i);\n+    \/\/ TODO 8284443 A static call to a mismatched method should still be scalarized\n+    if (t->is_inlinetypeptr() && !call->method()->get_Method()->mismatch() && call->method()->is_scalarized_arg(arg_num)) {\n+      \/\/ We don't pass inline type arguments by reference but instead pass each field of the inline type\n+      if (!arg->is_InlineType()) {\n+        assert(_gvn.type(arg)->is_zero_type() && !t->inline_klass()->is_null_free(), \"Unexpected argument type\");\n+        arg = InlineTypeNode::make_from_oop(this, arg, t->inline_klass(), t->inline_klass()->is_null_free());\n+      }\n+      InlineTypeNode* vt = arg->as_InlineType();\n+      vt->pass_fields(this, call, idx, true, !t->maybe_null());\n+      \/\/ If an inline type argument is passed as fields, attach the Method* to the call site\n+      \/\/ to be able to access the extended signature later via attached_method_before_pc().\n+      \/\/ For example, see CompiledMethod::preserve_callee_argument_oops().\n+      call->set_override_symbolic_info(true);\n+      \/\/ Register an evol dependency on the callee method to make sure that this method is deoptimized and\n+      \/\/ re-compiled with a non-scalarized calling convention if the callee method is later marked as mismatched.\n+      C->dependencies()->assert_evol_method(call->method());\n+      arg_num++;\n+      continue;\n+    } else if (arg->is_InlineType()) {\n+      \/\/ Pass inline type argument via oop to callee\n+      arg = arg->as_InlineType()->buffer(this);\n+      if (!is_late_inline) {\n+        arg = arg->as_InlineType()->get_oop();\n+      }\n+    }\n+    if (t != Type::HALF) {\n+      arg_num++;\n+    }\n+    call->init_req(idx++, arg);\n@@ -1824,7 +1926,0 @@\n-  \/\/ Capture the return value, if any.\n-  Node* ret;\n-  if (call->method() == nullptr ||\n-      call->method()->return_type()->basic_type() == T_VOID)\n-        ret = top();\n-  else  ret = _gvn.transform(new ProjNode(call, TypeFunc::Parms));\n-\n@@ -1843,0 +1938,22 @@\n+\n+  \/\/ Capture the return value, if any.\n+  Node* ret;\n+  if (call->method() == nullptr || call->method()->return_type()->basic_type() == T_VOID) {\n+    ret = top();\n+  } else if (call->tf()->returns_inline_type_as_fields()) {\n+    \/\/ Return of multiple values (inline type fields): we create a\n+    \/\/ InlineType node, each field is a projection from the call.\n+    ciInlineKlass* vk = call->method()->return_type()->as_inline_klass();\n+    uint base_input = TypeFunc::Parms;\n+    ret = InlineTypeNode::make_from_multi(this, call, vk, base_input, false, call->method()->signature()->returns_null_free_inline_type());\n+  } else {\n+    ret = _gvn.transform(new ProjNode(call, TypeFunc::Parms));\n+    ciType* t = call->method()->return_type();\n+    if (t->is_klass()) {\n+      const Type* type = TypeOopPtr::make_from_klass(t->as_klass());\n+      if (type->is_inlinetypeptr()) {\n+        ret = InlineTypeNode::make_from_oop(this, ret, type->inline_klass(), type->inline_klass()->is_null_free());\n+      }\n+    }\n+  }\n+\n@@ -1933,2 +2050,1 @@\n-  CallProjections callprojs;\n-  call->extract_projections(&callprojs, true);\n+  CallProjections* callprojs = call->extract_projections(true);\n@@ -1943,2 +2059,2 @@\n-  if (callprojs.fallthrough_catchproj != nullptr) {\n-    C->gvn_replace_by(callprojs.fallthrough_catchproj, final_ctl);\n+  if (callprojs->fallthrough_catchproj != nullptr) {\n+    C->gvn_replace_by(callprojs->fallthrough_catchproj, final_ctl);\n@@ -1946,1 +2062,1 @@\n-  if (callprojs.fallthrough_memproj != nullptr) {\n+  if (callprojs->fallthrough_memproj != nullptr) {\n@@ -1951,1 +2067,1 @@\n-    C->gvn_replace_by(callprojs.fallthrough_memproj,   final_mem);\n+    C->gvn_replace_by(callprojs->fallthrough_memproj,   final_mem);\n@@ -1954,2 +2070,2 @@\n-  if (callprojs.fallthrough_ioproj != nullptr) {\n-    C->gvn_replace_by(callprojs.fallthrough_ioproj,    final_io);\n+  if (callprojs->fallthrough_ioproj != nullptr) {\n+    C->gvn_replace_by(callprojs->fallthrough_ioproj,    final_io);\n@@ -1959,2 +2075,6 @@\n-  if (callprojs.resproj != nullptr && result != nullptr) {\n-    C->gvn_replace_by(callprojs.resproj, result);\n+  if (callprojs->resproj[0] != nullptr && result != nullptr) {\n+    \/\/ If the inlined code is dead, the result projections for an inline type returned as\n+    \/\/ fields have not been replaced. They will go away once the call is replaced by TOP below.\n+    assert(callprojs->nb_resproj == 1 || (call->tf()->returns_inline_type_as_fields() && stopped()),\n+           \"unexpected number of results\");\n+    C->gvn_replace_by(callprojs->resproj[0], result);\n@@ -1965,2 +2085,2 @@\n-    if (callprojs.catchall_catchproj != nullptr) {\n-      C->gvn_replace_by(callprojs.catchall_catchproj, C->top());\n+    if (callprojs->catchall_catchproj != nullptr) {\n+      C->gvn_replace_by(callprojs->catchall_catchproj, C->top());\n@@ -1968,2 +2088,2 @@\n-    if (callprojs.catchall_memproj != nullptr) {\n-      C->gvn_replace_by(callprojs.catchall_memproj,   C->top());\n+    if (callprojs->catchall_memproj != nullptr) {\n+      C->gvn_replace_by(callprojs->catchall_memproj,   C->top());\n@@ -1971,2 +2091,2 @@\n-    if (callprojs.catchall_ioproj != nullptr) {\n-      C->gvn_replace_by(callprojs.catchall_ioproj,    C->top());\n+    if (callprojs->catchall_ioproj != nullptr) {\n+      C->gvn_replace_by(callprojs->catchall_ioproj,    C->top());\n@@ -1975,2 +2095,2 @@\n-    if (callprojs.exobj != nullptr) {\n-      C->gvn_replace_by(callprojs.exobj, C->top());\n+    if (callprojs->exobj != nullptr) {\n+      C->gvn_replace_by(callprojs->exobj, C->top());\n@@ -1987,2 +2107,2 @@\n-    if (callprojs.catchall_catchproj != nullptr) {\n-      C->gvn_replace_by(callprojs.catchall_catchproj, ekit.control());\n+    if (callprojs->catchall_catchproj != nullptr) {\n+      C->gvn_replace_by(callprojs->catchall_catchproj, ekit.control());\n@@ -1991,1 +2111,1 @@\n-    if (callprojs.catchall_memproj != nullptr) {\n+    if (callprojs->catchall_memproj != nullptr) {\n@@ -1993,1 +2113,1 @@\n-      C->gvn_replace_by(callprojs.catchall_memproj,   ex_mem);\n+      C->gvn_replace_by(callprojs->catchall_memproj,   ex_mem);\n@@ -1996,2 +2116,2 @@\n-    if (callprojs.catchall_ioproj != nullptr) {\n-      C->gvn_replace_by(callprojs.catchall_ioproj,    ekit.i_o());\n+    if (callprojs->catchall_ioproj != nullptr) {\n+      C->gvn_replace_by(callprojs->catchall_ioproj,    ekit.i_o());\n@@ -2001,2 +2121,2 @@\n-    if (callprojs.exobj != nullptr) {\n-      C->gvn_replace_by(callprojs.exobj, ex_oop);\n+    if (callprojs->exobj != nullptr) {\n+      C->gvn_replace_by(callprojs->exobj, ex_oop);\n@@ -2016,1 +2136,1 @@\n-  if (callprojs.fallthrough_catchproj != nullptr && !final_ctl->is_top() && do_replaced_nodes) {\n+  if (callprojs->fallthrough_catchproj != nullptr && !final_ctl->is_top() && do_replaced_nodes) {\n@@ -2215,1 +2335,1 @@\n-    const TypePtr* ptr = (ptr_kind == ProfileMaybeNull && current_type->speculative_maybe_null()) ? TypePtr::BOTTOM : TypePtr::NOTNULL;\n+    const TypePtr* ptr = (ptr_kind != ProfileNeverNull && current_type->speculative_maybe_null()) ? TypePtr::BOTTOM : TypePtr::NOTNULL;\n@@ -2238,1 +2358,1 @@\n-    const TypeOopPtr* spec_type = TypeOopPtr::make(TypePtr::BotPTR, Type::OffsetBot, TypeOopPtr::InstanceBot, speculative);\n+    const TypeOopPtr* spec_type = TypeOopPtr::make(TypePtr::BotPTR, Type::Offset::bottom, TypeOopPtr::InstanceBot, speculative);\n@@ -2272,2 +2392,9 @@\n-      if (!data->as_BitData()->null_seen()) {\n-        ptr_kind = ProfileNeverNull;\n+      if (java_bc() == Bytecodes::_aastore) {\n+        ciKlass* array_type = nullptr;\n+        ciKlass* element_type = nullptr;\n+        ProfilePtrKind element_ptr = ProfileMaybeNull;\n+        bool flat_array = true;\n+        bool null_free_array = true;\n+        method()->array_access_profiled_type(bci(), array_type, element_type, element_ptr, flat_array, null_free_array);\n+        exact_kls = element_type;\n+        ptr_kind = element_ptr;\n@@ -2275,7 +2402,11 @@\n-        assert(data->is_ReceiverTypeData(), \"bad profile data type\");\n-        ciReceiverTypeData* call = (ciReceiverTypeData*)data->as_ReceiverTypeData();\n-        uint i = 0;\n-        for (; i < call->row_limit(); i++) {\n-          ciKlass* receiver = call->receiver(i);\n-          if (receiver != nullptr) {\n-            break;\n+        if (!data->as_BitData()->null_seen()) {\n+          ptr_kind = ProfileNeverNull;\n+        } else {\n+          assert(data->is_ReceiverTypeData(), \"bad profile data type\");\n+          ciReceiverTypeData* call = (ciReceiverTypeData*)data->as_ReceiverTypeData();\n+          uint i = 0;\n+          for (; i < call->row_limit(); i++) {\n+            ciKlass* receiver = call->receiver(i);\n+            if (receiver != nullptr) {\n+              break;\n+            }\n@@ -2283,0 +2414,1 @@\n+          ptr_kind = (i == call->row_limit()) ? ProfileAlwaysNull : ProfileMaybeNull;\n@@ -2284,1 +2416,0 @@\n-        ptr_kind = (i == call->row_limit()) ? ProfileAlwaysNull : ProfileMaybeNull;\n@@ -2303,1 +2434,1 @@\n-  int             nargs = tf->domain()->cnt() - TypeFunc::Parms;\n+  int             nargs = tf->domain_sig()->cnt() - TypeFunc::Parms;\n@@ -2306,1 +2437,1 @@\n-    const Type *targ = tf->domain()->field_at(j + TypeFunc::Parms);\n+    const Type *targ = tf->domain_sig()->field_at(j + TypeFunc::Parms);\n@@ -2360,1 +2491,1 @@\n-    int             nargs = tf->domain()->cnt() - TypeFunc::Parms;\n+    int             nargs = tf->domain_sig()->cnt() - TypeFunc::Parms;\n@@ -2362,1 +2493,1 @@\n-      const Type *targ = tf->domain()->field_at(j + TypeFunc::Parms);\n+      const Type *targ = tf->domain_sig()->field_at(j + TypeFunc::Parms);\n@@ -2511,1 +2642,1 @@\n-    uint num_bits = call_type->range()->field_at(TypeFunc::Parms)->is_vect()->length_in_bytes() * BitsPerByte;\n+    uint num_bits = call_type->range_sig()->field_at(TypeFunc::Parms)->is_vect()->length_in_bytes() * BitsPerByte;\n@@ -2591,0 +2722,1 @@\n+\n@@ -2887,0 +3019,5 @@\n+  const Type* sub_t = _gvn.type(obj_or_subklass);\n+  if (sub_t->make_oopptr() != nullptr && sub_t->make_oopptr()->is_inlinetypeptr()) {\n+    sub_t = TypeKlassPtr::make(sub_t->inline_klass());\n+    obj_or_subklass = makecon(sub_t);\n+  }\n@@ -2893,1 +3030,1 @@\n-    if (!_gvn.type(obj_or_subklass)->isa_klassptr()) {\n+    if (!sub_t->isa_klassptr()) {\n@@ -2911,2 +3048,1 @@\n-                                    float prob,\n-                                    Node* *casted_receiver) {\n+                                    float prob, Node* *casted_receiver) {\n@@ -2914,1 +3050,12 @@\n-\n+  Node* fail = top();\n+  const Type* rec_t = _gvn.type(receiver);\n+  if (rec_t->is_inlinetypeptr()) {\n+    if (klass->equals(rec_t->inline_klass())) {\n+      (*casted_receiver) = receiver; \/\/ Always passes\n+    } else {\n+      (*casted_receiver) = top();    \/\/ Always fails\n+      fail = control();\n+      set_control(top());\n+    }\n+    return fail;\n+  }\n@@ -2917,6 +3064,1 @@\n-  Node* want_klass = makecon(tklass);\n-  Node* cmp = _gvn.transform(new CmpPNode(recv_klass, want_klass));\n-  Node* bol = _gvn.transform(new BoolNode(cmp, BoolTest::eq));\n-  IfNode* iff = create_and_xform_if(control(), bol, prob, COUNT_UNKNOWN);\n-  set_control( _gvn.transform(new IfTrueNode (iff)));\n-  Node* fail = _gvn.transform(new IfFalseNode(iff));\n+  fail = type_check(recv_klass, tklass, prob);\n@@ -2926,2 +3068,2 @@\n-    const TypeOopPtr* recvx_type = tklass->as_instance_type();\n-    assert(recvx_type->klass_is_exact(), \"\");\n+    const TypeOopPtr* recv_xtype = tklass->as_instance_type();\n+    assert(recv_xtype->klass_is_exact(), \"\");\n@@ -2929,1 +3071,1 @@\n-    if (!receiver_type->higher_equal(recvx_type)) { \/\/ ignore redundant casts\n+    if (!receiver_type->higher_equal(recv_xtype)) { \/\/ ignore redundant casts\n@@ -2932,2 +3074,7 @@\n-      Node* cast = new CheckCastPPNode(control(), receiver, recvx_type);\n-      (*casted_receiver) = _gvn.transform(cast);\n+      Node* cast = new CheckCastPPNode(control(), receiver, recv_xtype);\n+      Node* res = _gvn.transform(cast);\n+      if (recv_xtype->is_inlinetypeptr()) {\n+        assert(!gvn().type(res)->maybe_null(), \"receiver should never be null\");\n+        res = InlineTypeNode::make_from_oop(this, res, recv_xtype->inline_klass());\n+      }\n+      (*casted_receiver) = res;\n@@ -2942,0 +3089,11 @@\n+Node* GraphKit::type_check(Node* recv_klass, const TypeKlassPtr* tklass,\n+                           float prob) {\n+  Node* want_klass = makecon(tklass);\n+  Node* cmp = _gvn.transform(new CmpPNode(recv_klass, want_klass));\n+  Node* bol = _gvn.transform(new BoolNode(cmp, BoolTest::eq));\n+  IfNode* iff = create_and_xform_if(control(), bol, prob, COUNT_UNKNOWN);\n+  set_control(_gvn.transform(new IfTrueNode (iff)));\n+  Node* fail = _gvn.transform(new IfFalseNode(iff));\n+  return fail;\n+}\n+\n@@ -2954,3 +3112,6 @@\n-    if (!receiver_type->higher_equal(recv_type)) { \/\/ ignore redundant casts\n-      Node* cast = new CheckCastPPNode(control(), receiver, recv_type);\n-      (*casted_receiver) = _gvn.transform(cast);\n+    if (receiver_type != nullptr && !receiver_type->higher_equal(recv_type)) { \/\/ ignore redundant casts\n+      Node* cast = _gvn.transform(new CheckCastPPNode(control(), receiver, recv_type));\n+      if (recv_type->is_inlinetypeptr()) {\n+        cast = InlineTypeNode::make_from_oop(this, cast, recv_type->inline_klass());\n+      }\n+      (*casted_receiver) = cast;\n@@ -2986,0 +3147,3 @@\n+    if (java_bc() == Bytecodes::_aastore) {\n+      return ((ciArrayLoadStoreData*)data->as_ArrayLoadStoreData())->element()->ptr_kind() == ProfileNeverNull;\n+    }\n@@ -3065,1 +3229,14 @@\n-  ciKlass* exact_kls = spec_klass == nullptr ? profile_has_unique_klass() : spec_klass;\n+  ciKlass* exact_kls = spec_klass;\n+  if (exact_kls == nullptr) {\n+    if (java_bc() == Bytecodes::_aastore) {\n+      ciKlass* array_type = nullptr;\n+      ciKlass* element_type = nullptr;\n+      ProfilePtrKind element_ptr = ProfileMaybeNull;\n+      bool flat_array = true;\n+      bool null_free_array = true;\n+      method()->array_access_profiled_type(bci(), array_type, element_type, element_ptr, flat_array, null_free_array);\n+      exact_kls = element_type;\n+    } else {\n+      exact_kls = profile_has_unique_klass();\n+    }\n+  }\n@@ -3195,1 +3372,1 @@\n-    if (subk->is_loaded()) {\n+    if (subk != nullptr && subk->is_loaded()) {\n@@ -3251,2 +3428,1 @@\n-Node* GraphKit::gen_checkcast(Node *obj, Node* superklass,\n-                              Node* *failure_control) {\n+Node* GraphKit::gen_checkcast(Node *obj, Node* superklass, Node* *failure_control, bool null_free) {\n@@ -3256,0 +3432,2 @@\n+  bool safe_for_replace = (failure_control == nullptr);\n+  assert(!null_free || toop->is_inlinetypeptr(), \"must be an inline type pointer\");\n@@ -3264,3 +3442,10 @@\n-    const TypeOopPtr* objtp = _gvn.type(obj)->isa_oopptr();\n-    if (objtp != nullptr) {\n-      switch (C->static_subtype_check(tk, objtp->as_klass_type())) {\n+    const TypeKlassPtr* kptr = nullptr;\n+    const Type* t = _gvn.type(obj);\n+    if (t->isa_oop_ptr()) {\n+      kptr = t->is_oopptr()->as_klass_type();\n+    } else if (obj->is_InlineType()) {\n+      ciInlineKlass* vk = t->inline_klass();\n+      kptr = TypeInstKlassPtr::make(TypePtr::NotNull, vk, Type::Offset(0));\n+    }\n+    if (kptr != nullptr) {\n+      switch (C->static_subtype_check(tk, kptr)) {\n@@ -3271,1 +3456,7 @@\n-        return record_profiled_receiver_for_speculation(obj);\n+        obj = record_profiled_receiver_for_speculation(obj);\n+        if (null_free) {\n+          assert(safe_for_replace, \"must be\");\n+          obj = null_check(obj);\n+        }\n+        assert(stopped() || !toop->is_inlinetypeptr() || obj->is_InlineType(), \"should have been scalarized\");\n+        return obj;\n@@ -3273,0 +3464,4 @@\n+        if (null_free) {\n+          assert(safe_for_replace, \"must be\");\n+          obj = null_check(obj);\n+        }\n@@ -3274,2 +3469,1 @@\n-        \/\/ A non-null value will always produce an exception.\n-        if (!objtp->maybe_null()) {\n+        if (t->isa_oopptr() != nullptr && !t->is_oopptr()->maybe_null()) {\n@@ -3292,1 +3486,0 @@\n-  bool safe_for_replace = false;\n@@ -3297,2 +3490,3 @@\n-    data = method()->method_data()->bci_to_data(bci());\n-    safe_for_replace = true;\n+    if (method()->method_data()->is_mature()) {\n+      data = method()->method_data()->bci_to_data(bci());\n+    }\n@@ -3305,0 +3499,3 @@\n+  _gvn.set_type(region, Type::CONTROL);\n+  _gvn.set_type(phi, toop);\n+\n@@ -3312,0 +3509,7 @@\n+  if (obj->is_InlineType()) {\n+    \/\/ Re-execute if buffering during triggers deoptimization\n+    PreserveReexecuteState preexecs(this);\n+    jvms()->set_should_reexecute(true);\n+    obj = obj->as_InlineType()->buffer(this, safe_for_replace);\n+  }\n+\n@@ -3314,1 +3518,7 @@\n-  Node* not_null_obj = null_check_oop(obj, &null_ctl, never_see_null, safe_for_replace, speculative_not_null);\n+  Node* not_null_obj = nullptr;\n+  if (null_free) {\n+    assert(safe_for_replace, \"must be\");\n+    not_null_obj = null_check(obj);\n+  } else {\n+    not_null_obj = null_check_oop(obj, &null_ctl, never_see_null, safe_for_replace, speculative_not_null);\n+  }\n@@ -3319,0 +3529,3 @@\n+    if (toop->is_inlinetypeptr()) {\n+      return InlineTypeNode::make_null(_gvn, toop->inline_klass());\n+    }\n@@ -3354,1 +3567,1 @@\n-    Node* not_subtype_ctrl = gen_subtype_check(not_null_obj, superklass );\n+    Node* not_subtype_ctrl = gen_subtype_check(not_null_obj, superklass);\n@@ -3363,0 +3576,6 @@\n+        Node* obj_klass = nullptr;\n+        if (not_null_obj->is_InlineType()) {\n+          obj_klass = makecon(TypeKlassPtr::make(_gvn.type(not_null_obj)->inline_klass()));\n+        } else {\n+          obj_klass = load_object_klass(not_null_obj);\n+        }\n@@ -3393,1 +3612,122 @@\n-  return record_profiled_receiver_for_speculation(res);\n+  bool not_inline = !toop->can_be_inline_type();\n+  bool not_flat_in_array = !UseFlatArray || not_inline || (toop->is_inlinetypeptr() && !toop->inline_klass()->flat_in_array());\n+  if (EnableValhalla && not_flat_in_array) {\n+    \/\/ Check if obj has been loaded from an array\n+    obj = obj->isa_DecodeN() ? obj->in(1) : obj;\n+    Node* array = nullptr;\n+    if (obj->isa_Load()) {\n+      Node* address = obj->in(MemNode::Address);\n+      if (address->isa_AddP()) {\n+        array = address->as_AddP()->in(AddPNode::Base);\n+      }\n+    } else if (obj->is_Phi()) {\n+      Node* region = obj->in(0);\n+      \/\/ TODO make this more robust (see JDK-8231346)\n+      if (region->req() == 3 && region->in(2) != nullptr && region->in(2)->in(0) != nullptr) {\n+        IfNode* iff = region->in(2)->in(0)->isa_If();\n+        if (iff != nullptr) {\n+          iff->is_flat_array_check(&_gvn, &array);\n+        }\n+      }\n+    }\n+    if (array != nullptr) {\n+      const TypeAryPtr* ary_t = _gvn.type(array)->isa_aryptr();\n+      if (ary_t != nullptr && !ary_t->is_flat()) {\n+        if (!ary_t->is_not_null_free() && not_inline) {\n+          \/\/ Casting array element to a non-inline-type, mark array as not null-free.\n+          Node* cast = _gvn.transform(new CheckCastPPNode(control(), array, ary_t->cast_to_not_null_free()));\n+          replace_in_map(array, cast);\n+        } else if (!ary_t->is_not_flat()) {\n+          \/\/ Casting array element to a non-flat type, mark array as not flat.\n+          Node* cast = _gvn.transform(new CheckCastPPNode(control(), array, ary_t->cast_to_not_flat()));\n+          replace_in_map(array, cast);\n+        }\n+      }\n+    }\n+  }\n+\n+  if (!stopped() && !res->is_InlineType()) {\n+    res = record_profiled_receiver_for_speculation(res);\n+    if (toop->is_inlinetypeptr()) {\n+      Node* vt = InlineTypeNode::make_from_oop(this, res, toop->inline_klass(), !gvn().type(res)->maybe_null());\n+      res = vt;\n+      if (safe_for_replace) {\n+        replace_in_map(obj, vt);\n+        replace_in_map(not_null_obj, vt);\n+        replace_in_map(res, vt);\n+      }\n+    }\n+  }\n+  return res;\n+}\n+\n+Node* GraphKit::inline_type_test(Node* obj, bool is_inline) {\n+  Node* mark_adr = basic_plus_adr(obj, oopDesc::mark_offset_in_bytes());\n+  Node* mark = make_load(nullptr, mark_adr, TypeX_X, TypeX_X->basic_type(), MemNode::unordered);\n+  Node* mask = MakeConX(markWord::inline_type_pattern);\n+  Node* masked = _gvn.transform(new AndXNode(mark, mask));\n+  Node* cmp = _gvn.transform(new CmpXNode(masked, mask));\n+  return _gvn.transform(new BoolNode(cmp, is_inline ? BoolTest::eq : BoolTest::ne));\n+}\n+\n+Node* GraphKit::is_val_mirror(Node* mirror) {\n+  Node* p = basic_plus_adr(mirror, java_lang_Class::secondary_mirror_offset());\n+  Node* secondary_mirror = access_load_at(mirror, p, _gvn.type(p)->is_ptr(), TypeInstPtr::MIRROR->cast_to_ptr_type(TypePtr::BotPTR), T_OBJECT, IN_HEAP);\n+  Node* cmp = _gvn.transform(new CmpPNode(mirror, secondary_mirror));\n+  return _gvn.transform(new BoolNode(cmp, BoolTest::eq));\n+}\n+\n+Node* GraphKit::array_lh_test(Node* klass, jint mask, jint val, bool eq) {\n+  Node* lh_adr = basic_plus_adr(klass, in_bytes(Klass::layout_helper_offset()));\n+  \/\/ Make sure to use immutable memory here to enable hoisting the check out of loops\n+  Node* lh_val = _gvn.transform(LoadNode::make(_gvn, nullptr, immutable_memory(), lh_adr, lh_adr->bottom_type()->is_ptr(), TypeInt::INT, T_INT, MemNode::unordered));\n+  Node* masked = _gvn.transform(new AndINode(lh_val, intcon(mask)));\n+  Node* cmp = _gvn.transform(new CmpINode(masked, intcon(val)));\n+  return _gvn.transform(new BoolNode(cmp, eq ? BoolTest::eq : BoolTest::ne));\n+}\n+\n+Node* GraphKit::flat_array_test(Node* array_or_klass, bool flat) {\n+  \/\/ We can't use immutable memory here because the mark word is mutable.\n+  \/\/ PhaseIdealLoop::move_flat_array_check_out_of_loop will make sure the\n+  \/\/ check is moved out of loops (mainly to enable loop unswitching).\n+  Node* mem = UseArrayMarkWordCheck ? memory(Compile::AliasIdxRaw) : immutable_memory();\n+  Node* cmp = _gvn.transform(new FlatArrayCheckNode(C, mem, array_or_klass));\n+  record_for_igvn(cmp); \/\/ Give it a chance to be optimized out by IGVN\n+  return _gvn.transform(new BoolNode(cmp, flat ? BoolTest::eq : BoolTest::ne));\n+}\n+\n+Node* GraphKit::null_free_array_test(Node* klass, bool null_free) {\n+  return array_lh_test(klass, Klass::_lh_null_free_array_bit_inplace, 0, !null_free);\n+}\n+\n+\/\/ Deoptimize if 'ary' is a null-free inline type array and 'val' is null\n+Node* GraphKit::inline_array_null_guard(Node* ary, Node* val, int nargs, bool safe_for_replace) {\n+  RegionNode* region = new RegionNode(3);\n+  Node* null_ctl = top();\n+  null_check_oop(val, &null_ctl);\n+  if (null_ctl != top()) {\n+    PreserveJVMState pjvms(this);\n+    set_control(null_ctl);\n+    {\n+      \/\/ Deoptimize if null-free array\n+      BuildCutout unless(this, null_free_array_test(load_object_klass(ary), \/* null_free = *\/ false), PROB_MAX);\n+      inc_sp(nargs);\n+      uncommon_trap(Deoptimization::Reason_null_check,\n+                    Deoptimization::Action_none);\n+    }\n+    region->init_req(1, control());\n+  }\n+  region->init_req(2, control());\n+  set_control(_gvn.transform(region));\n+  record_for_igvn(region);\n+  if (_gvn.type(val) == TypePtr::NULL_PTR) {\n+    \/\/ Since we were just successfully storing null, the array can't be null free.\n+    const TypeAryPtr* ary_t = _gvn.type(ary)->is_aryptr();\n+    ary_t = ary_t->cast_to_not_null_free();\n+    Node* cast = _gvn.transform(new CheckCastPPNode(control(), ary, ary_t));\n+    if (safe_for_replace) {\n+      replace_in_map(ary, cast);\n+    }\n+    ary = cast;\n+  }\n+  return ary;\n@@ -3461,0 +3801,1 @@\n+\n@@ -3529,0 +3870,1 @@\n+  assert(!obj->is_InlineType(), \"should not unlock on inline type\");\n@@ -3569,1 +3911,8 @@\n-    if (xklass || (klass_t->isa_aryklassptr() && klass_t->is_aryklassptr()->elem() != Type::BOTTOM)) {\n+    bool can_be_flat = false;\n+    const TypeAryPtr* ary_type = klass_t->as_instance_type()->isa_aryptr();\n+    if (UseFlatArray && !xklass && ary_type != nullptr && !ary_type->is_null_free()) {\n+      \/\/ The runtime type of [LMyValue might be [QMyValue due to [QMyValue <: [LMyValue. Don't constant fold.\n+      const TypeOopPtr* elem = ary_type->elem()->make_oopptr();\n+      can_be_flat = ary_type->can_be_inline_array() && (!elem->is_inlinetypeptr() || elem->inline_klass()->flat_in_array());\n+    }\n+    if (!can_be_flat && (xklass || (klass_t->isa_aryklassptr() && klass_t->is_aryklassptr()->elem() != Type::BOTTOM))) {\n@@ -3571,2 +3920,4 @@\n-      if (klass_t->isa_aryklassptr()) {\n-        BasicType elem = klass_t->as_instance_type()->isa_aryptr()->elem()->array_element_basic_type();\n+      if (klass_t->is_flat()) {\n+        lhelper = ary_type->flat_layout_helper();\n+      } else if (klass_t->isa_aryklassptr()) {\n+        BasicType elem = ary_type->elem()->array_element_basic_type();\n@@ -3601,1 +3952,3 @@\n-  kit.set_memory(init_out_raw, alias_idx);\n+  if (init_out_raw != nullptr) {\n+    kit.set_memory(init_out_raw, alias_idx);\n+  }\n@@ -3640,0 +3993,1 @@\n+    _gvn.set_type(minit_in, Type::MEMORY);\n@@ -3647,3 +4001,28 @@\n-      const TypePtr* telemref = oop_type->add_offset(Type::OffsetBot);\n-      int            elemidx  = C->get_alias_index(telemref);\n-      hook_memory_on_init(*this, elemidx, minit_in, minit_out);\n+      const TypeAryPtr* arytype = oop_type->is_aryptr();\n+      if (arytype->is_flat()) {\n+        \/\/ Initially all flat array accesses share a single slice\n+        \/\/ but that changes after parsing. Prepare the memory graph so\n+        \/\/ it can optimize flat array accesses properly once they\n+        \/\/ don't share a single slice.\n+        assert(C->flat_accesses_share_alias(), \"should be set at parse time\");\n+        C->set_flat_accesses_share_alias(false);\n+        ciInlineKlass* vk = arytype->elem()->inline_klass();\n+        for (int i = 0, len = vk->nof_nonstatic_fields(); i < len; i++) {\n+          ciField* field = vk->nonstatic_field_at(i);\n+          if (field->offset_in_bytes() >= TrackedInitializationLimit * HeapWordSize)\n+            continue;  \/\/ do not bother to track really large numbers of fields\n+          int off_in_vt = field->offset_in_bytes() - vk->first_field_offset();\n+          const TypePtr* adr_type = arytype->with_field_offset(off_in_vt)->add_offset(Type::OffsetBot);\n+          int fieldidx = C->get_alias_index(adr_type, true);\n+          \/\/ Pass nullptr for init_out. Having per flat array element field memory edges as uses of the Initialize node\n+          \/\/ can result in per flat array field Phis to be created which confuses the logic of\n+          \/\/ Compile::adjust_flat_array_access_aliases().\n+          hook_memory_on_init(*this, fieldidx, minit_in, nullptr);\n+        }\n+        C->set_flat_accesses_share_alias(true);\n+        hook_memory_on_init(*this, C->get_alias_index(TypeAryPtr::INLINES), minit_in, minit_out);\n+      } else {\n+        const TypePtr* telemref = oop_type->add_offset(Type::OffsetBot);\n+        int            elemidx  = C->get_alias_index(telemref);\n+        hook_memory_on_init(*this, elemidx, minit_in, minit_out);\n+      }\n@@ -3651,0 +4030,1 @@\n+      set_memory(minit_out, C->get_alias_index(oop_type)); \/\/ mark word\n@@ -3701,1 +4081,2 @@\n-                             bool deoptimize_on_exception) {\n+                             bool deoptimize_on_exception,\n+                             InlineTypeNode* inline_type_node) {\n@@ -3708,1 +4089,1 @@\n-  int   layout_is_con = (layout_val == nullptr);\n+  bool  layout_is_con = (layout_val == nullptr);\n@@ -3759,1 +4140,1 @@\n-  \/\/ since GC and deoptimization can happened.\n+  \/\/ since GC and deoptimization can happen.\n@@ -3766,1 +4147,1 @@\n-                                         initial_slow_test);\n+                                         initial_slow_test, inline_type_node);\n@@ -3772,1 +4153,1 @@\n-\/\/ helper for both newarray and anewarray\n+\/\/ helper for newarray and anewarray\n@@ -3782,1 +4163,1 @@\n-  int   layout_is_con = (layout_val == nullptr);\n+  bool  layout_is_con = (layout_val == nullptr);\n@@ -3812,1 +4193,1 @@\n-    fast_size_limit <<= (LogBytesPerLong - log2_esize);\n+    fast_size_limit <<= MAX2(LogBytesPerLong - log2_esize, 0);\n@@ -3830,1 +4211,1 @@\n-    BasicType etype  = Klass::layout_helper_element_type(layout_con);\n+    bool is_flat_array = Klass::layout_helper_is_flatArray(layout_con);\n@@ -3833,1 +4214,1 @@\n-    assert((hsize & right_n_bits(eshift)) == 0, \"hsize is pre-rounded\");\n+    assert(is_flat_array || (hsize & right_n_bits(eshift)) == 0, \"hsize is pre-rounded\");\n@@ -3917,1 +4298,1 @@\n-  \/\/ since GC and deoptimization can happened.\n+  \/\/ since GC and deoptimization can happen.\n@@ -3926,1 +4307,60 @@\n-  const TypeOopPtr* ary_type = _gvn.type(klass_node)->is_klassptr()->as_instance_type();\n+  const TypeKlassPtr* ary_klass = _gvn.type(klass_node)->isa_klassptr();\n+  const TypeOopPtr* ary_type = ary_klass->as_instance_type();\n+  const TypeAryPtr* ary_ptr = ary_type->isa_aryptr();\n+\n+  \/\/ Inline type array variants:\n+  \/\/ - null-ok:              MyValue.ref[] (ciObjArrayKlass \"[LMyValue\")\n+  \/\/ - null-free:            MyValue.val[] (ciObjArrayKlass \"[QMyValue\")\n+  \/\/ - null-free, flat     : MyValue.val[] (ciFlatArrayKlass \"[QMyValue\")\n+  \/\/ Check if array is a null-free, non-flat inline type array\n+  \/\/ that needs to be initialized with the default inline type.\n+  Node* default_value = nullptr;\n+  Node* raw_default_value = nullptr;\n+  if (ary_ptr != nullptr && ary_ptr->klass_is_exact()) {\n+    \/\/ Array type is known\n+    if (ary_ptr->is_null_free() && !ary_ptr->is_flat()) {\n+      ciInlineKlass* vk = ary_ptr->elem()->inline_klass();\n+      default_value = InlineTypeNode::default_oop(gvn(), vk);\n+    }\n+  } else if (ary_type->can_be_inline_array()) {\n+    \/\/ Array type is not known, add runtime checks\n+    assert(!ary_klass->klass_is_exact(), \"unexpected exact type\");\n+    Node* r = new RegionNode(3);\n+    default_value = new PhiNode(r, TypeInstPtr::BOTTOM);\n+\n+    Node* bol = array_lh_test(klass_node, Klass::_lh_array_tag_flat_value_bit_inplace | Klass::_lh_null_free_array_bit_inplace, Klass::_lh_null_free_array_bit_inplace);\n+    IfNode* iff = create_and_map_if(control(), bol, PROB_FAIR, COUNT_UNKNOWN);\n+\n+    \/\/ Null-free, non-flat inline type array, initialize with the default value\n+    set_control(_gvn.transform(new IfTrueNode(iff)));\n+    Node* p = basic_plus_adr(klass_node, in_bytes(ArrayKlass::element_klass_offset()));\n+    Node* eklass = _gvn.transform(LoadKlassNode::make(_gvn, control(), immutable_memory(), p, TypeInstPtr::KLASS));\n+    Node* adr_fixed_block_addr = basic_plus_adr(eklass, in_bytes(InstanceKlass::adr_inlineklass_fixed_block_offset()));\n+    Node* adr_fixed_block = make_load(control(), adr_fixed_block_addr, TypeRawPtr::NOTNULL, T_ADDRESS, MemNode::unordered);\n+    Node* default_value_offset_addr = basic_plus_adr(adr_fixed_block, in_bytes(InlineKlass::default_value_offset_offset()));\n+    Node* default_value_offset = make_load(control(), default_value_offset_addr, TypeInt::INT, T_INT, MemNode::unordered);\n+    Node* elem_mirror = load_mirror_from_klass(eklass);\n+    Node* default_value_addr = basic_plus_adr(elem_mirror, ConvI2X(default_value_offset));\n+    Node* val = access_load_at(elem_mirror, default_value_addr, TypeInstPtr::MIRROR, TypeInstPtr::NOTNULL, T_OBJECT, IN_HEAP);\n+    r->init_req(1, control());\n+    default_value->init_req(1, val);\n+\n+    \/\/ Otherwise initialize with all zero\n+    r->init_req(2, _gvn.transform(new IfFalseNode(iff)));\n+    default_value->init_req(2, null());\n+\n+    set_control(_gvn.transform(r));\n+    default_value = _gvn.transform(default_value);\n+  }\n+  if (default_value != nullptr) {\n+    if (UseCompressedOops) {\n+      \/\/ With compressed oops, the 64-bit init value is built from two 32-bit compressed oops\n+      default_value = _gvn.transform(new EncodePNode(default_value, default_value->bottom_type()->make_narrowoop()));\n+      Node* lower = _gvn.transform(new CastP2XNode(control(), default_value));\n+      Node* upper = _gvn.transform(new LShiftLNode(lower, intcon(32)));\n+      raw_default_value = _gvn.transform(new OrLNode(lower, upper));\n+    } else {\n+      raw_default_value = _gvn.transform(new CastP2XNode(control(), default_value));\n+    }\n+  }\n+\n@@ -3941,2 +4381,2 @@\n-                            length, valid_length_test);\n-\n+                            length, valid_length_test,\n+                            default_value, raw_default_value);\n@@ -4089,1 +4529,1 @@\n-                                                     false, nullptr, 0);\n+                                                     false, nullptr, Type::Offset(0));\n@@ -4092,2 +4532,2 @@\n-                                                  TypeAry::make(TypeInt::BYTE, TypeInt::POS),\n-                                                  ciTypeArrayKlass::make(T_BYTE), true, 0);\n+                                                  TypeAry::make(TypeInt::BYTE, TypeInt::POS, false, false, true, true),\n+                                                  ciTypeArrayKlass::make(T_BYTE), true, Type::Offset(0));\n@@ -4106,1 +4546,1 @@\n-                                                     false, nullptr, 0);\n+                                                     false, nullptr, Type::Offset(0));\n@@ -4118,1 +4558,1 @@\n-                                                     false, nullptr, 0);\n+                                                     false, nullptr, Type::Offset(0));\n@@ -4128,1 +4568,1 @@\n-                                                     false, nullptr, 0);\n+                                                     false, nullptr, Type::Offset(0));\n@@ -4241,1 +4681,7 @@\n-    return makecon(con_type);\n+    Node* con = makecon(con_type);\n+    if (field->type()->is_inlinetype()) {\n+      con = InlineTypeNode::make_from_oop(this, con, field->type()->as_inline_klass(), field->is_null_free());\n+    } else if (con_type->is_inlinetypeptr()) {\n+      con = InlineTypeNode::make_from_oop(this, con, con_type->inline_klass(), field->is_null_free());\n+    }\n+    return con;\n@@ -4245,0 +4691,9 @@\n+\n+\/\/---------------------------load_mirror_from_klass----------------------------\n+\/\/ Given a klass oop, load its java mirror (a java.lang.Class oop).\n+Node* GraphKit::load_mirror_from_klass(Node* klass) {\n+  Node* p = basic_plus_adr(klass, in_bytes(Klass::java_mirror_offset()));\n+  Node* load = make_load(nullptr, p, TypeRawPtr::NOTNULL, T_ADDRESS, MemNode::unordered);\n+  \/\/ mirror = ((OopHandle)mirror)->resolve();\n+  return access_load(load, TypeInstPtr::MIRROR, T_OBJECT, IN_NATIVE);\n+}\n","filename":"src\/hotspot\/share\/opto\/graphKit.cpp","additions":584,"deletions":129,"binary":false,"changes":713,"status":"modified"},{"patch":"@@ -1239,0 +1239,17 @@\n+\/\/ Returns true if this IfNode belongs to a flat array check\n+\/\/ and returns the corresponding array in the 'array' parameter.\n+bool IfNode::is_flat_array_check(PhaseTransform* phase, Node** array) {\n+  Node* bol = in(1);\n+  if (!bol->is_Bool()) {\n+    return false;\n+  }\n+  Node* cmp = bol->in(1);\n+  if (cmp->isa_FlatArrayCheck()) {\n+    if (array != nullptr) {\n+      *array = cmp->in(FlatArrayCheckNode::ArrayOrKlass);\n+    }\n+    return true;\n+  }\n+  return false;\n+}\n+\n","filename":"src\/hotspot\/share\/opto\/ifnode.cpp","additions":17,"deletions":0,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -27,0 +27,1 @@\n+#include \"ci\/ciFlatArrayKlass.hpp\"\n@@ -325,0 +326,2 @@\n+  case vmIntrinsics::_makePrivateBuffer:        return inline_unsafe_make_private_buffer();\n+  case vmIntrinsics::_finishPrivateBuffer:      return inline_unsafe_finish_private_buffer();\n@@ -334,0 +337,1 @@\n+  case vmIntrinsics::_getValue:                 return inline_unsafe_access(!is_store, T_OBJECT,   Relaxed, false, true);\n@@ -344,0 +348,1 @@\n+  case vmIntrinsics::_putValue:                 return inline_unsafe_access( is_store, T_OBJECT,   Relaxed, false, true);\n@@ -507,0 +512,1 @@\n+  case vmIntrinsics::_isFlattenedArray:         return inline_unsafe_isFlattenedArray();\n@@ -530,0 +536,5 @@\n+  case vmIntrinsics::_asPrimaryType:\n+  case vmIntrinsics::_asPrimaryTypeArg:\n+  case vmIntrinsics::_asValueType:\n+  case vmIntrinsics::_asValueTypeArg:           return inline_primitive_Class_conversion(intrinsic_id());\n+\n@@ -2204,0 +2215,1 @@\n+  bool null_free = false;\n@@ -2209,0 +2221,1 @@\n+      null_free = alias_type->field()->is_null_free();\n@@ -2217,0 +2230,1 @@\n+      null_free = adr_type->is_aryptr()->is_null_free();\n@@ -2229,0 +2243,3 @@\n+    if (null_free) {\n+      result = result->join_speculative(TypePtr::NOTNULL)->is_oopptr();\n+    }\n@@ -2259,1 +2276,1 @@\n-bool LibraryCallKit::inline_unsafe_access(bool is_store, const BasicType type, const AccessKind kind, const bool unaligned) {\n+bool LibraryCallKit::inline_unsafe_access(bool is_store, const BasicType type, const AccessKind kind, const bool unaligned, const bool is_flat) {\n@@ -2284,1 +2301,1 @@\n-      assert(sig->count() == 2, \"oop getter has 2 arguments\");\n+      assert(sig->count() == 2 || (is_flat && sig->count() == 3), \"oop getter has 2 or 3 arguments\");\n@@ -2290,1 +2307,1 @@\n-      assert(sig->count() == 3, \"oop putter has 3 arguments\");\n+      assert(sig->count() == 3 || (is_flat && sig->count() == 4), \"oop putter has 3 arguments\");\n@@ -2316,0 +2333,55 @@\n+\n+  ciInlineKlass* inline_klass = nullptr;\n+  if (is_flat) {\n+    const TypeInstPtr* cls = _gvn.type(argument(4))->isa_instptr();\n+    if (cls == nullptr || cls->const_oop() == nullptr) {\n+      return false;\n+    }\n+    ciType* mirror_type = cls->const_oop()->as_instance()->java_mirror_type();\n+    if (!mirror_type->is_inlinetype()) {\n+      return false;\n+    }\n+    inline_klass = mirror_type->as_inline_klass();\n+  }\n+\n+  if (base->is_InlineType()) {\n+    InlineTypeNode* vt = base->as_InlineType();\n+    if (is_store) {\n+      if (!vt->is_allocated(&_gvn)) {\n+        return false;\n+      }\n+      base = vt->get_oop();\n+    } else {\n+      if (offset->is_Con()) {\n+        long off = find_long_con(offset, 0);\n+        ciInlineKlass* vk = vt->type()->inline_klass();\n+        if ((long)(int)off != off || !vk->contains_field_offset(off)) {\n+          return false;\n+        }\n+\n+        ciField* field = vk->get_non_flat_field_by_offset(off);\n+        if (field != nullptr) {\n+          BasicType bt = type2field[field->type()->basic_type()];\n+          if (bt == T_ARRAY || bt == T_NARROWOOP) {\n+            bt = T_OBJECT;\n+          }\n+          if (bt == type && (!field->is_flat() || field->type() == inline_klass)) {\n+            Node* value = vt->field_value_by_offset(off, false);\n+            if (value->is_InlineType()) {\n+              value = value->as_InlineType()->adjust_scalarization_depth(this);\n+            }\n+            set_result(value);\n+            return true;\n+          }\n+        }\n+      }\n+      {\n+        \/\/ Re-execute the unsafe access if allocation triggers deoptimization.\n+        PreserveReexecuteState preexecs(this);\n+        jvms()->set_should_reexecute(true);\n+        vt = vt->buffer(this);\n+      }\n+      base = vt->get_oop();\n+    }\n+  }\n+\n@@ -2326,1 +2398,1 @@\n-    if (type != T_OBJECT) {\n+    if (type != T_OBJECT && (inline_klass == nullptr || !inline_klass->has_object_fields())) {\n@@ -2344,1 +2416,1 @@\n-  Node* val = is_store ? argument(4) : nullptr;\n+  Node* val = is_store ? argument(4 + (is_flat ? 1 : 0)) : nullptr;\n@@ -2365,1 +2437,22 @@\n-  BasicType bt = alias_type->basic_type();\n+  BasicType bt = T_ILLEGAL;\n+  ciField* field = nullptr;\n+  if (adr_type->isa_instptr()) {\n+    const TypeInstPtr* instptr = adr_type->is_instptr();\n+    ciInstanceKlass* k = instptr->instance_klass();\n+    int off = instptr->offset();\n+    if (instptr->const_oop() != nullptr &&\n+        k == ciEnv::current()->Class_klass() &&\n+        instptr->offset() >= (k->size_helper() * wordSize)) {\n+      k = instptr->const_oop()->as_instance()->java_lang_Class_klass()->as_instance_klass();\n+      field = k->get_field_by_offset(off, true);\n+    } else {\n+      field = k->get_non_flat_field_by_offset(off);\n+    }\n+    if (field != nullptr) {\n+      bt = type2field[field->type()->basic_type()];\n+    }\n+    assert(bt == alias_type->basic_type() || is_flat, \"should match\");\n+  } else {\n+    bt = alias_type->basic_type();\n+  }\n+\n@@ -2388,0 +2481,23 @@\n+  if (is_flat) {\n+    if (adr_type->isa_instptr()) {\n+      if (field == nullptr || field->type() != inline_klass) {\n+        mismatched = true;\n+      }\n+    } else if (adr_type->isa_aryptr()) {\n+      const Type* elem = adr_type->is_aryptr()->elem();\n+      if (!adr_type->is_flat() || elem->inline_klass() != inline_klass) {\n+        mismatched = true;\n+      }\n+    } else {\n+      mismatched = true;\n+    }\n+    if (is_store) {\n+      const Type* val_t = _gvn.type(val);\n+      if (!val_t->is_inlinetypeptr() || val_t->inline_klass() != inline_klass) {\n+        set_map(old_map);\n+        set_sp(old_sp);\n+        return false;\n+      }\n+    }\n+  }\n+\n@@ -2389,1 +2505,1 @@\n-  assert(!mismatched || alias_type->adr_type()->is_oopptr(), \"off-heap access can't be mismatched\");\n+  assert(!mismatched || is_flat || alias_type->adr_type()->is_oopptr(), \"off-heap access can't be mismatched\");\n@@ -2401,4 +2517,6 @@\n-  if (!is_store && type == T_OBJECT) {\n-    const TypeOopPtr* tjp = sharpen_unsafe_type(alias_type, adr_type);\n-    if (tjp != nullptr) {\n-      value_type = tjp;\n+  if (!is_store) {\n+    if (type == T_OBJECT && !is_flat) {\n+      const TypeOopPtr* tjp = sharpen_unsafe_type(alias_type, adr_type);\n+      if (tjp != nullptr) {\n+        value_type = tjp;\n+      }\n@@ -2420,2 +2538,2 @@\n-    ciField* field = alias_type->field();\n-    if (heap_base_oop != top() && field != nullptr && field->is_constant() && !mismatched) {\n+\n+    if (heap_base_oop != top() && field != nullptr && field->is_constant() && !field->is_flat() && !mismatched) {\n@@ -2427,1 +2545,16 @@\n-      p = access_load_at(heap_base_oop, adr, adr_type, value_type, type, decorators);\n+      if (is_flat) {\n+        if (adr_type->isa_instptr() && !mismatched) {\n+          ciInstanceKlass* holder = adr_type->is_instptr()->instance_klass();\n+          int offset = adr_type->is_instptr()->offset();\n+          p = InlineTypeNode::make_from_flat(this, inline_klass, base, base, holder, offset, decorators);\n+        } else {\n+          p = InlineTypeNode::make_from_flat(this, inline_klass, base, adr, nullptr, 0, decorators);\n+        }\n+      } else {\n+        p = access_load_at(heap_base_oop, adr, adr_type, value_type, type, decorators);\n+        const TypeOopPtr* ptr = value_type->make_oopptr();\n+        if (ptr != nullptr && ptr->is_inlinetypeptr()) {\n+          \/\/ Load a non-flattened inline type from memory\n+          p = InlineTypeNode::make_from_oop(this, p, ptr->inline_klass(), !ptr->maybe_null());\n+        }\n+      }\n@@ -2465,1 +2598,32 @@\n-    access_store_at(heap_base_oop, adr, adr_type, val, value_type, type, decorators);\n+    if (is_flat) {\n+      if (adr_type->isa_instptr() && !mismatched) {\n+        ciInstanceKlass* holder = adr_type->is_instptr()->instance_klass();\n+        int offset = adr_type->is_instptr()->offset();\n+        val->as_InlineType()->store_flat(this, base, base, holder, offset, decorators);\n+      } else {\n+        val->as_InlineType()->store_flat(this, base, adr, nullptr, 0, decorators);\n+      }\n+    } else {\n+      access_store_at(heap_base_oop, adr, adr_type, val, value_type, type, decorators);\n+    }\n+  }\n+\n+  if (argument(1)->is_InlineType() && is_store) {\n+    InlineTypeNode* value = InlineTypeNode::make_from_oop(this, base, _gvn.type(argument(1))->inline_klass());\n+    value = value->make_larval(this, false);\n+    replace_in_map(argument(1), value);\n+  }\n+\n+  return true;\n+}\n+\n+bool LibraryCallKit::inline_unsafe_make_private_buffer() {\n+  Node* receiver = argument(0);\n+  Node* value = argument(1);\n+  if (!value->is_InlineType()) {\n+    return false;\n+  }\n+\n+  receiver = null_check(receiver);\n+  if (stopped()) {\n+    return true;\n@@ -2468,0 +2632,25 @@\n+  set_result(value->as_InlineType()->make_larval(this, true));\n+  return true;\n+}\n+\n+bool LibraryCallKit::inline_unsafe_finish_private_buffer() {\n+  Node* receiver = argument(0);\n+  Node* buffer = argument(1);\n+  if (!buffer->is_InlineType()) {\n+    return false;\n+  }\n+  InlineTypeNode* vt = buffer->as_InlineType();\n+  if (!vt->is_allocated(&_gvn)) {\n+    return false;\n+  }\n+  \/\/ TODO 8239003 Why is this needed?\n+  if (AllocateNode::Ideal_allocation(vt->get_oop()) == nullptr) {\n+    return false;\n+  }\n+\n+  receiver = null_check(receiver);\n+  if (stopped()) {\n+    return true;\n+  }\n+\n+  set_result(vt->finish_larval(this));\n@@ -2676,0 +2865,13 @@\n+    if (oldval != nullptr && oldval->is_InlineType()) {\n+      \/\/ Re-execute the unsafe access if allocation triggers deoptimization.\n+      PreserveReexecuteState preexecs(this);\n+      jvms()->set_should_reexecute(true);\n+      oldval = oldval->as_InlineType()->buffer(this)->get_oop();\n+    }\n+    if (newval != nullptr && newval->is_InlineType()) {\n+      \/\/ Re-execute the unsafe access if allocation triggers deoptimization.\n+      PreserveReexecuteState preexecs(this);\n+      jvms()->set_should_reexecute(true);\n+      newval = newval->as_InlineType()->buffer(this)->get_oop();\n+    }\n+\n@@ -2862,2 +3064,7 @@\n-\n-  Node* obj = new_instance(kls, test);\n+  Node* obj = nullptr;\n+  const TypeInstKlassPtr* tkls = _gvn.type(kls)->isa_instklassptr();\n+  if (tkls != nullptr && tkls->instance_klass()->is_inlinetype()) {\n+    obj = InlineTypeNode::make_default(_gvn, tkls->instance_klass()->as_inline_klass())->buffer(this);\n+  } else {\n+    obj = new_instance(kls, test);\n+  }\n@@ -3618,1 +3825,1 @@\n-  const TypeAry* arr0 = TypeAry::make(etype, TypeInt::POS);\n+  const TypeAry* arr0 = TypeAry::make(etype, TypeInt::POS, \/* stable= *\/ false, \/* flat= *\/ false, \/* not_flat= *\/ true, \/* not_null_free= *\/ true);\n@@ -3623,1 +3830,1 @@\n-  const Type* objects_type = TypeAryPtr::make(TypePtr::BotPTR, arr0, objects_klass, xk, 0);\n+  const Type* objects_type = TypeAryPtr::make(TypePtr::BotPTR, arr0, objects_klass, xk, TypeAryPtr::Offset(0));\n@@ -3659,9 +3866,0 @@\n-\/\/---------------------------load_mirror_from_klass----------------------------\n-\/\/ Given a klass oop, load its java mirror (a java.lang.Class oop).\n-Node* LibraryCallKit::load_mirror_from_klass(Node* klass) {\n-  Node* p = basic_plus_adr(klass, in_bytes(Klass::java_mirror_offset()));\n-  Node* load = make_load(nullptr, p, TypeRawPtr::NOTNULL, T_ADDRESS, MemNode::unordered);\n-  \/\/ mirror = ((OopHandle)mirror)->resolve();\n-  return access_load(load, TypeInstPtr::MIRROR, T_OBJECT, IN_NATIVE);\n-}\n-\n@@ -3710,0 +3908,1 @@\n+\n@@ -3903,0 +4102,31 @@\n+\/\/-------------------------inline_primitive_Class_conversion-------------------\n+\/\/               Class<T> java.lang.Class                  .asPrimaryType()\n+\/\/ public static Class<T> jdk.internal.value.PrimitiveClass.asPrimaryType(Class<T>)\n+\/\/               Class<T> java.lang.Class                  .asValueType()\n+\/\/ public static Class<T> jdk.internal.value.PrimitiveClass.asValueType(Class<T>)\n+bool LibraryCallKit::inline_primitive_Class_conversion(vmIntrinsics::ID id) {\n+  Node* mirror = argument(0); \/\/ Receiver\/argument Class\n+  const TypeInstPtr* mirror_con = _gvn.type(mirror)->isa_instptr();\n+  if (mirror_con == nullptr) {\n+    return false;\n+  }\n+\n+  bool is_val_mirror = true;\n+  ciType* tm = mirror_con->java_mirror_type(&is_val_mirror);\n+  if (tm != nullptr) {\n+    Node* result = mirror;\n+    if ((id == vmIntrinsics::_asPrimaryType || id == vmIntrinsics::_asPrimaryTypeArg) && is_val_mirror) {\n+      result = _gvn.makecon(TypeInstPtr::make(tm->as_inline_klass()->ref_mirror()));\n+    } else if (id == vmIntrinsics::_asValueType || id == vmIntrinsics::_asValueTypeArg) {\n+      if (!tm->is_inlinetype()) {\n+        return false; \/\/ Throw UnsupportedOperationException\n+      } else if (!is_val_mirror) {\n+        result = _gvn.makecon(TypeInstPtr::make(tm->as_inline_klass()->val_mirror()));\n+      }\n+    }\n+    set_result(result);\n+    return true;\n+  }\n+  return false;\n+}\n+\n@@ -3918,1 +4148,2 @@\n-  ciType* tm = mirror_con->java_mirror_type();\n+  bool requires_null_check = false;\n+  ciType* tm = mirror_con->java_mirror_type(&requires_null_check);\n@@ -3928,0 +4159,3 @@\n+        if (requires_null_check) {\n+          obj = null_check(obj);\n+        }\n@@ -3948,0 +4182,3 @@\n+  if (requires_null_check) {\n+    obj = null_check(obj);\n+  }\n@@ -3954,2 +4191,2 @@\n-  \/\/ Not-subtype or the mirror's klass ptr is null (in case it is a primitive).\n-  enum { _bad_type_path = 1, _prim_path = 2, PATH_LIMIT };\n+  \/\/ Not-subtype or the mirror's klass ptr is nullptr (in case it is a primitive).\n+  enum { _bad_type_path = 1, _prim_path = 2, _npe_path = 3, PATH_LIMIT };\n@@ -3965,0 +4202,2 @@\n+  Node* io = i_o();\n+  Node* mem = merged_memory();\n@@ -3966,0 +4205,21 @@\n+    if (EnableValhalla && !requires_null_check) {\n+      \/\/ Check if we are casting to QMyValue\n+      Node* ctrl_val_mirror = generate_fair_guard(is_val_mirror(mirror), nullptr);\n+      if (ctrl_val_mirror != nullptr) {\n+        RegionNode* r = new RegionNode(3);\n+        record_for_igvn(r);\n+        r->init_req(1, control());\n+\n+        \/\/ Casting to QMyValue, check for null\n+        set_control(ctrl_val_mirror);\n+        { \/\/ PreserveJVMState because null check replaces obj in map\n+          PreserveJVMState pjvms(this);\n+          Node* null_ctr = top();\n+          null_check_oop(obj, &null_ctr);\n+          region->init_req(_npe_path, null_ctr);\n+          r->init_req(2, control());\n+        }\n+        set_control(_gvn.transform(r));\n+      }\n+    }\n+\n@@ -3972,1 +4232,2 @@\n-      region->in(_bad_type_path) != top()) {\n+      region->in(_bad_type_path) != top() ||\n+      region->in(_npe_path) != top()) {\n@@ -3976,0 +4237,3 @@\n+    \/\/ Set IO and memory because gen_checkcast may override them when buffering inline types\n+    set_i_o(io);\n+    set_all_memory(mem);\n@@ -4009,0 +4273,1 @@\n+  RegionNode* prim_region = new RegionNode(2);\n@@ -4011,0 +4276,1 @@\n+  record_for_igvn(prim_region);\n@@ -4035,2 +4301,5 @@\n-    int prim_path = (which_arg == 0 ? _prim_0_path : _prim_1_path);\n-    region->init_req(prim_path, null_ctl);\n+    if (which_arg == 0) {\n+      prim_region->init_req(1, null_ctl);\n+    } else {\n+      region->init_req(_prim_1_path, null_ctl);\n+    }\n@@ -4046,0 +4315,3 @@\n+    \/\/ If superc is an inline mirror, we also need to check if superc == subc because LMyValue\n+    \/\/ is not a subtype of QMyValue but due to subk == superk the subtype check will pass.\n+    generate_fair_guard(is_val_mirror(args[0]), prim_region);\n@@ -4053,1 +4325,2 @@\n-  set_control(region->in(_prim_0_path)); \/\/ go back to first null check\n+  \/\/ This path is also used if superc is a value mirror.\n+  set_control(_gvn.transform(prim_region));\n@@ -4058,1 +4331,1 @@\n-    generate_guard(bol_eq, region, PROB_FAIR);\n+    generate_fair_guard(bol_eq, region);\n@@ -4089,2 +4362,1 @@\n-Node* LibraryCallKit::generate_array_guard_common(Node* kls, RegionNode* region,\n-                                                  bool obj_array, bool not_array) {\n+Node* LibraryCallKit::generate_array_guard_common(Node* kls, RegionNode* region, ArrayKind kind) {\n@@ -4096,9 +4368,0 @@\n-  \/\/ If obj_array\/non_array==false\/false:\n-  \/\/ Branch around if the given klass is in fact an array (either obj or prim).\n-  \/\/ If obj_array\/non_array==false\/true:\n-  \/\/ Branch around if the given klass is not an array klass of any kind.\n-  \/\/ If obj_array\/non_array==true\/true:\n-  \/\/ Branch around if the kls is not an oop array (kls is int[], String, etc.)\n-  \/\/ If obj_array\/non_array==true\/false:\n-  \/\/ Branch around if the kls is an oop array (Object[] or subtype)\n-  \/\/\n@@ -4109,4 +4372,11 @@\n-    bool query = (obj_array\n-                  ? Klass::layout_helper_is_objArray(layout_con)\n-                  : Klass::layout_helper_is_array(layout_con));\n-    if (query == not_array) {\n+    bool query = 0;\n+    switch(kind) {\n+      case ObjectArray:    query = Klass::layout_helper_is_objArray(layout_con); break;\n+      case NonObjectArray: query = !Klass::layout_helper_is_objArray(layout_con); break;\n+      case TypeArray:      query = Klass::layout_helper_is_typeArray(layout_con); break;\n+      case AnyArray:       query = Klass::layout_helper_is_array(layout_con); break;\n+      case NonArray:       query = !Klass::layout_helper_is_array(layout_con); break;\n+      default:\n+        ShouldNotReachHere();\n+    }\n+    if (!query) {\n@@ -4122,0 +4392,21 @@\n+  unsigned int value = 0;\n+  BoolTest::mask btest = BoolTest::illegal;\n+  switch(kind) {\n+    case ObjectArray:\n+    case NonObjectArray: {\n+      value = Klass::_lh_array_tag_obj_value;\n+      layout_val = _gvn.transform(new RShiftINode(layout_val, intcon(Klass::_lh_array_tag_shift)));\n+      btest = (kind == ObjectArray) ? BoolTest::eq : BoolTest::ne;\n+      break;\n+    }\n+    case TypeArray: {\n+      value = Klass::_lh_array_tag_type_value;\n+      layout_val = _gvn.transform(new RShiftINode(layout_val, intcon(Klass::_lh_array_tag_shift)));\n+      btest = BoolTest::eq;\n+      break;\n+    }\n+    case AnyArray:    value = Klass::_lh_neutral_value; btest = BoolTest::lt; break;\n+    case NonArray:    value = Klass::_lh_neutral_value; btest = BoolTest::gt; break;\n+    default:\n+      ShouldNotReachHere();\n+  }\n@@ -4123,4 +4414,1 @@\n-  jint  nval = (obj_array\n-                ? (jint)(Klass::_lh_array_tag_type_value\n-                   <<    Klass::_lh_array_tag_shift)\n-                : Klass::_lh_neutral_value);\n+  jint nval = (jint)value;\n@@ -4128,3 +4416,0 @@\n-  BoolTest::mask btest = BoolTest::lt;  \/\/ correct for testing is_[obj]array\n-  \/\/ invert the test if we are looking for a non-array\n-  if (not_array)  btest = BoolTest(btest).negate();\n@@ -4137,1 +4422,1 @@\n-\/\/ private static native Object java.lang.reflect.newArray(Class<?> componentType, int length);\n+\/\/ private static native Object java.lang.reflect.Array.newArray(Class<?> componentType, int length);\n@@ -4283,1 +4568,13 @@\n-    Node* not_objArray = generate_non_objArray_guard(klass_node, bailout);\n+    \/\/ Inline type array may have object field that would require a\n+    \/\/ write barrier. Conservatively, go to slow path.\n+    \/\/ TODO 8251971: Optimize for the case when flat src\/dst are later found\n+    \/\/ to not contain oops (i.e., move this check to the macro expansion phase).\n+    BarrierSetC2* bs = BarrierSet::barrier_set()->barrier_set_c2();\n+    const TypeAryPtr* orig_t = _gvn.type(original)->isa_aryptr();\n+    const TypeKlassPtr* tklass = _gvn.type(klass_node)->is_klassptr();\n+    bool exclude_flat = UseFlatArray && bs->array_copy_requires_gc_barriers(true, T_OBJECT, false, false, BarrierSetC2::Parsing) &&\n+                        \/\/ Can src array be flat and contain oops?\n+                        (orig_t == nullptr || (!orig_t->is_not_flat() && (!orig_t->is_flat() || orig_t->elem()->inline_klass()->contains_oops()))) &&\n+                        \/\/ Can dest array be flat and contain oops?\n+                        tklass->can_be_inline_array() && (!tklass->is_flat() || tklass->is_aryklassptr()->elem()->is_instklassptr()->instance_klass()->as_inline_klass()->contains_oops());\n+    Node* not_objArray = exclude_flat ? generate_non_objArray_guard(klass_node, bailout) : generate_typeArray_guard(klass_node, bailout);\n@@ -4287,1 +4584,1 @@\n-      const Type* akls = TypeKlassPtr::make(TypePtr::NotNull, ak, 0\/*offset*\/);\n+      const Type* akls = TypeKlassPtr::make(TypePtr::NotNull, ak, Type::Offset(0));\n@@ -4308,0 +4605,32 @@\n+    \/\/ Handle inline type arrays\n+    bool can_validate = !too_many_traps(Deoptimization::Reason_class_check);\n+    if (!stopped()) {\n+      orig_t = _gvn.type(original)->isa_aryptr();\n+      if (orig_t != nullptr && orig_t->is_flat()) {\n+        \/\/ Src is flat, check that dest is flat as well\n+        if (exclude_flat) {\n+          \/\/ Dest can't be flat, bail out\n+          bailout->add_req(control());\n+          set_control(top());\n+        } else {\n+          generate_fair_guard(flat_array_test(klass_node, \/* flat = *\/ false), bailout);\n+        }\n+      } else if (UseFlatArray && (orig_t == nullptr || !orig_t->is_not_flat()) &&\n+                 \/\/ If dest is flat, src must be flat as well (guaranteed by src <: dest check if validated).\n+                 ((!tklass->is_flat() && tklass->can_be_inline_array()) || !can_validate)) {\n+        \/\/ Src might be flat and dest might not be flat. Go to the slow path if src is flat.\n+        \/\/ TODO 8251971: Optimize for the case when src\/dest are later found to be both flat.\n+        generate_fair_guard(flat_array_test(load_object_klass(original)), bailout);\n+        if (orig_t != nullptr) {\n+          orig_t = orig_t->cast_to_not_flat();\n+          original = _gvn.transform(new CheckCastPPNode(control(), original, orig_t));\n+        }\n+      }\n+      if (!can_validate) {\n+        \/\/ No validation. The subtype check emitted at macro expansion time will not go to the slow\n+        \/\/ path but call checkcast_arraycopy which can not handle flat\/null-free inline type arrays.\n+        \/\/ TODO 8251971: Optimize for the case when src\/dest are later found to be both flat\/null-free.\n+        generate_fair_guard(null_free_array_test(klass_node), bailout);\n+      }\n+    }\n+\n@@ -4350,1 +4679,1 @@\n-      if (!too_many_traps(Deoptimization::Reason_class_check)) {\n+      if (can_validate) {\n@@ -4436,1 +4765,1 @@\n-    const TypeTuple* range = tf->range();\n+    const TypeTuple* range = tf->range_cc();\n@@ -4440,1 +4769,1 @@\n-    tf = TypeFunc::make(tf->domain(), new_range);\n+    tf = TypeFunc::make(tf->domain_cc(), new_range);\n@@ -4497,1 +4826,6 @@\n-  Node* obj = nullptr;\n+  Node* obj = argument(0);\n+\n+  if (gvn().type(obj)->is_inlinetypeptr()) {\n+    return false;\n+  }\n+\n@@ -4507,1 +4841,0 @@\n-    obj = argument(0);\n@@ -4547,1 +4880,2 @@\n-  Node *lock_mask      = _gvn.MakeConX(markWord::lock_mask_in_place);\n+  \/\/ This also serves as guard against inline types\n+  Node *lock_mask      = _gvn.MakeConX(markWord::inline_type_mask_in_place);\n@@ -4613,1 +4947,10 @@\n-  Node* obj = null_check_receiver();\n+  Node* obj = argument(0);\n+  if (obj->is_InlineType()) {\n+    const Type* t = _gvn.type(obj);\n+    if (t->maybe_null()) {\n+      null_check(obj);\n+    }\n+    set_result(makecon(TypeInstPtr::make(t->inline_klass()->java_mirror())));\n+    return true;\n+  }\n+  obj = null_check_receiver();\n@@ -4914,0 +5257,14 @@\n+\/\/----------------------inline_unsafe_isFlattenedArray-------------------\n+\/\/ public native boolean Unsafe.isFlattenedArray(Class<?> arrayClass);\n+\/\/ This intrinsic exploits assumptions made by the native implementation\n+\/\/ (arrayClass is neither null nor primitive) to avoid unnecessary null checks.\n+bool LibraryCallKit::inline_unsafe_isFlattenedArray() {\n+  Node* cls = argument(1);\n+  Node* p = basic_plus_adr(cls, java_lang_Class::klass_offset());\n+  Node* kls = _gvn.transform(LoadKlassNode::make(_gvn, nullptr, immutable_memory(), p,\n+                                                 TypeRawPtr::BOTTOM, TypeInstKlassPtr::OBJECT));\n+  Node* result = flat_array_test(kls);\n+  set_result(result);\n+  return true;\n+}\n+\n@@ -4978,1 +5335,2 @@\n-    Node* obj = null_check_receiver();\n+    Node* obj = argument(0);\n+    obj = null_check_receiver();\n@@ -4988,1 +5346,2 @@\n-        obj_type->speculative_type()->is_instance_klass()) {\n+        obj_type->speculative_type()->is_instance_klass() &&\n+        !obj_type->speculative_type()->is_inlinetype()) {\n@@ -5018,0 +5377,5 @@\n+    \/\/ We only go to the fast case code if we pass a number of guards.\n+    \/\/ The paths which do not pass are accumulated in the slow_region.\n+    RegionNode* slow_region = new RegionNode(1);\n+    record_for_igvn(slow_region);\n+\n@@ -5023,3 +5387,0 @@\n-      Node* obj_length = load_array_length(obj);\n-      Node* obj_size  = nullptr;\n-      Node* alloc_obj = new_array(obj_klass, obj_length, 0, &obj_size, \/*deoptimize_on_exception=*\/true);\n@@ -5028,20 +5389,7 @@\n-      if (bs->array_copy_requires_gc_barriers(true, T_OBJECT, true, false, BarrierSetC2::Parsing)) {\n-        \/\/ If it is an oop array, it requires very special treatment,\n-        \/\/ because gc barriers are required when accessing the array.\n-        Node* is_obja = generate_objArray_guard(obj_klass, (RegionNode*)nullptr);\n-        if (is_obja != nullptr) {\n-          PreserveJVMState pjvms2(this);\n-          set_control(is_obja);\n-          \/\/ Generate a direct call to the right arraycopy function(s).\n-          \/\/ Clones are always tightly coupled.\n-          ArrayCopyNode* ac = ArrayCopyNode::make(this, true, obj, intcon(0), alloc_obj, intcon(0), obj_length, true, false);\n-          ac->set_clone_oop_array();\n-          Node* n = _gvn.transform(ac);\n-          assert(n == ac, \"cannot disappear\");\n-          ac->connect_outputs(this, \/*deoptimize_on_exception=*\/true);\n-\n-          result_reg->init_req(_objArray_path, control());\n-          result_val->init_req(_objArray_path, alloc_obj);\n-          result_i_o ->set_req(_objArray_path, i_o());\n-          result_mem ->set_req(_objArray_path, reset_memory());\n-        }\n+      const TypeAryPtr* ary_ptr = obj_type->isa_aryptr();\n+      if (UseFlatArray && bs->array_copy_requires_gc_barriers(true, T_OBJECT, true, false, BarrierSetC2::Expansion) &&\n+          obj_type->can_be_inline_array() &&\n+          (ary_ptr == nullptr || (!ary_ptr->is_not_flat() && (!ary_ptr->is_flat() || ary_ptr->elem()->inline_klass()->contains_oops())))) {\n+        \/\/ Flat inline type array may have object field that would require a\n+        \/\/ write barrier. Conservatively, go to slow path.\n+        generate_fair_guard(flat_array_test(obj_klass), slow_region);\n@@ -5049,7 +5397,0 @@\n-      \/\/ Otherwise, there are no barriers to worry about.\n-      \/\/ (We can dispense with card marks if we know the allocation\n-      \/\/  comes out of eden (TLAB)...  In fact, ReduceInitialCardMarks\n-      \/\/  causes the non-eden paths to take compensating steps to\n-      \/\/  simulate a fresh allocation, so that no further\n-      \/\/  card marks are required in compiled code to initialize\n-      \/\/  the object.)\n@@ -5058,7 +5399,43 @@\n-        copy_to_clone(obj, alloc_obj, obj_size, true);\n-\n-        \/\/ Present the results of the copy.\n-        result_reg->init_req(_array_path, control());\n-        result_val->init_req(_array_path, alloc_obj);\n-        result_i_o ->set_req(_array_path, i_o());\n-        result_mem ->set_req(_array_path, reset_memory());\n+        Node* obj_length = load_array_length(obj);\n+        Node* obj_size  = nullptr;\n+        Node* alloc_obj = new_array(obj_klass, obj_length, 0, &obj_size, \/*deoptimize_on_exception=*\/true);\n+\n+        BarrierSetC2* bs = BarrierSet::barrier_set()->barrier_set_c2();\n+        if (bs->array_copy_requires_gc_barriers(true, T_OBJECT, true, false, BarrierSetC2::Parsing)) {\n+          \/\/ If it is an oop array, it requires very special treatment,\n+          \/\/ because gc barriers are required when accessing the array.\n+          Node* is_obja = generate_objArray_guard(obj_klass, (RegionNode*)nullptr);\n+          if (is_obja != nullptr) {\n+            PreserveJVMState pjvms2(this);\n+            set_control(is_obja);\n+            \/\/ Generate a direct call to the right arraycopy function(s).\n+            \/\/ Clones are always tightly coupled.\n+            ArrayCopyNode* ac = ArrayCopyNode::make(this, true, obj, intcon(0), alloc_obj, intcon(0), obj_length, true, false);\n+            ac->set_clone_oop_array();\n+            Node* n = _gvn.transform(ac);\n+            assert(n == ac, \"cannot disappear\");\n+            ac->connect_outputs(this, \/*deoptimize_on_exception=*\/true);\n+\n+            result_reg->init_req(_objArray_path, control());\n+            result_val->init_req(_objArray_path, alloc_obj);\n+            result_i_o ->set_req(_objArray_path, i_o());\n+            result_mem ->set_req(_objArray_path, reset_memory());\n+          }\n+        }\n+        \/\/ Otherwise, there are no barriers to worry about.\n+        \/\/ (We can dispense with card marks if we know the allocation\n+        \/\/  comes out of eden (TLAB)...  In fact, ReduceInitialCardMarks\n+        \/\/  causes the non-eden paths to take compensating steps to\n+        \/\/  simulate a fresh allocation, so that no further\n+        \/\/  card marks are required in compiled code to initialize\n+        \/\/  the object.)\n+\n+        if (!stopped()) {\n+          copy_to_clone(obj, alloc_obj, obj_size, true);\n+\n+          \/\/ Present the results of the copy.\n+          result_reg->init_req(_array_path, control());\n+          result_val->init_req(_array_path, alloc_obj);\n+          result_i_o ->set_req(_array_path, i_o());\n+          result_mem ->set_req(_array_path, reset_memory());\n+        }\n@@ -5068,4 +5445,0 @@\n-    \/\/ We only go to the instance fast case code if we pass a number of guards.\n-    \/\/ The paths which do not pass are accumulated in the slow_region.\n-    RegionNode* slow_region = new RegionNode(1);\n-    record_for_igvn(slow_region);\n@@ -5241,2 +5614,1 @@\n-    CallProjections callprojs;\n-    alloc->extract_projections(&callprojs, true);\n+    CallProjections* callprojs = alloc->extract_projections(true);\n@@ -5245,1 +5617,1 @@\n-    C->gvn_replace_by(callprojs.fallthrough_ioproj, alloc->in(TypeFunc::I_O));\n+    C->gvn_replace_by(callprojs->fallthrough_ioproj, alloc->in(TypeFunc::I_O));\n@@ -5287,1 +5659,1 @@\n-    set_i_o(callprojs.fallthrough_ioproj);\n+    set_i_o(callprojs->fallthrough_ioproj);\n@@ -5488,1 +5860,1 @@\n-    if (src_elem == dest_elem && src_elem == T_OBJECT) {\n+    if (src_elem == dest_elem && top_src->is_flat() == top_dest->is_flat() && src_elem == T_OBJECT) {\n@@ -5515,0 +5887,2 @@\n+          src_type = _gvn.type(src);\n+          top_src = src_type->isa_aryptr();\n@@ -5518,0 +5892,2 @@\n+          dest_type = _gvn.type(dest);\n+          top_dest = dest_type->isa_aryptr();\n@@ -5533,2 +5909,1 @@\n-      can_emit_guards &&\n-      !src->is_top() && !dest->is_top()) {\n+      can_emit_guards && !src->is_top() && !dest->is_top()) {\n@@ -5577,0 +5952,2 @@\n+      slow_region->add_req(not_subtype_ctrl);\n+    }\n@@ -5578,6 +5955,28 @@\n-      if (not_subtype_ctrl != top()) {\n-        PreserveJVMState pjvms(this);\n-        set_control(not_subtype_ctrl);\n-        uncommon_trap(Deoptimization::Reason_intrinsic,\n-                      Deoptimization::Action_make_not_entrant);\n-        assert(stopped(), \"Should be stopped\");\n+    const TypeKlassPtr* dest_klass_t = _gvn.type(dest_klass)->is_klassptr();\n+    const Type* toop = dest_klass_t->cast_to_exactness(false)->as_instance_type();\n+    src = _gvn.transform(new CheckCastPPNode(control(), src, toop));\n+    src_type = _gvn.type(src);\n+    top_src  = src_type->isa_aryptr();\n+\n+    \/\/ Handle flat inline type arrays (null-free arrays are handled by the subtype check above)\n+    if (!stopped() && UseFlatArray) {\n+      \/\/ If dest is flat, src must be flat as well (guaranteed by src <: dest check). Handle flat src here.\n+      assert(top_dest == nullptr || !top_dest->is_flat() || top_src->is_flat(), \"src array must be flat\");\n+      if (top_src != nullptr && top_src->is_flat()) {\n+        \/\/ Src is flat, check that dest is flat as well\n+        if (top_dest != nullptr && !top_dest->is_flat()) {\n+          generate_fair_guard(flat_array_test(dest_klass, \/* flat = *\/ false), slow_region);\n+          \/\/ Since dest is flat and src <: dest, dest must have the same type as src.\n+          top_dest = top_src->cast_to_exactness(false);\n+          assert(top_dest->is_flat(), \"dest must be flat\");\n+          dest = _gvn.transform(new CheckCastPPNode(control(), dest, top_dest));\n+        }\n+      } else if (top_src == nullptr || !top_src->is_not_flat()) {\n+        \/\/ Src might be flat and dest might not be flat. Go to the slow path if src is flat.\n+        \/\/ TODO 8251971: Optimize for the case when src\/dest are later found to be both flat.\n+        assert(top_dest == nullptr || !top_dest->is_flat(), \"dest array must not be flat\");\n+        generate_fair_guard(flat_array_test(src), slow_region);\n+        if (top_src != nullptr) {\n+          top_src = top_src->cast_to_not_flat();\n+          src = _gvn.transform(new CheckCastPPNode(control(), src, top_src));\n+        }\n@@ -5586,0 +5985,1 @@\n+\n@@ -5593,4 +5993,0 @@\n-\n-    const TypeKlassPtr* dest_klass_t = _gvn.type(dest_klass)->is_klassptr();\n-    const Type *toop = dest_klass_t->cast_to_exactness(false)->as_instance_type();\n-    src = _gvn.transform(new CheckCastPPNode(control(), src, toop));\n","filename":"src\/hotspot\/share\/opto\/library_call.cpp","additions":524,"deletions":128,"binary":false,"changes":652,"status":"modified"},{"patch":"@@ -80,1 +80,2 @@\n-         LoopNestLongOuterLoop = 1<<16 };\n+         LoopNestLongOuterLoop = 1<<16,\n+         FlatArrays            = 1<<17};\n@@ -103,0 +104,1 @@\n+  bool is_flat_arrays() const { return _loop_flags & FlatArrays; }\n@@ -116,0 +118,1 @@\n+  void mark_flat_arrays() { _loop_flags |= FlatArrays; }\n@@ -1410,3 +1413,3 @@\n-                                        Node_List &old_new,\n-                                        IfNode* unswitch_iff,\n-                                        CloneLoopMode mode);\n+                                      Node_List &old_new,\n+                                      Node_List &unswitch_iffs,\n+                                      CloneLoopMode mode);\n@@ -1430,1 +1433,1 @@\n-  IfNode* find_unswitching_candidate(const IdealLoopTree *loop) const;\n+  IfNode* find_unswitching_candidate(const IdealLoopTree *loop, Node_List& unswitch_iffs) const;\n@@ -1550,0 +1553,1 @@\n+  void move_flat_array_check_out_of_loop(Node* n);\n@@ -1551,0 +1555,1 @@\n+  bool flat_array_element_type_check(Node *n);\n","filename":"src\/hotspot\/share\/opto\/loopnode.hpp","additions":10,"deletions":5,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -36,0 +36,1 @@\n+#include \"opto\/inlinetypenode.hpp\"\n@@ -65,0 +66,6 @@\n+  \/\/ Inline types should not be split through Phis because they cannot be merged\n+  \/\/ through Phi nodes but each value input needs to be merged individually.\n+  if (n->is_InlineType()) {\n+    return nullptr;\n+  }\n+\n@@ -728,0 +735,4 @@\n+      if (inp->isa_InlineType()) {\n+        \/\/ TODO 8302217 This prevents PhiNode::push_inline_types_through\n+        return nullptr;\n+      }\n@@ -1053,0 +1064,49 @@\n+\/\/ If UseArrayMarkWordCheck is enabled, we can't use immutable memory for the flat array check\n+\/\/ because we are loading the mark word which is mutable. Although the bits we are interested in\n+\/\/ are immutable (we check for markWord::unlocked_value), we need to use raw memory to not break\n+\/\/ anti dependency analysis. Below code will attempt to still move flat array checks out of loops,\n+\/\/ mainly to enable loop unswitching.\n+void PhaseIdealLoop::move_flat_array_check_out_of_loop(Node* n) {\n+  \/\/ Skip checks for more than one array\n+  if (n->req() > 3) {\n+    return;\n+  }\n+  Node* mem = n->in(FlatArrayCheckNode::Memory);\n+  Node* array = n->in(FlatArrayCheckNode::ArrayOrKlass)->uncast();\n+  IdealLoopTree* check_loop = get_loop(get_ctrl(n));\n+  IdealLoopTree* ary_loop = get_loop(get_ctrl(array));\n+\n+  \/\/ Check if array is loop invariant\n+  if (!check_loop->is_member(ary_loop)) {\n+    \/\/ Walk up memory graph from the check until we leave the loop\n+    VectorSet wq;\n+    wq.set(mem->_idx);\n+    while (check_loop->is_member(get_loop(ctrl_or_self(mem)))) {\n+      if (mem->is_Phi()) {\n+        mem = mem->in(1);\n+      } else if (mem->is_MergeMem()) {\n+        mem = mem->as_MergeMem()->memory_at(Compile::AliasIdxRaw);\n+      } else if (mem->is_Proj()) {\n+        mem = mem->in(0);\n+      } else if (mem->is_MemBar() || mem->is_SafePoint()) {\n+        mem = mem->in(TypeFunc::Memory);\n+      } else if (mem->is_Store() || mem->is_LoadStore() || mem->is_ClearArray()) {\n+        mem = mem->in(MemNode::Memory);\n+      } else {\n+#ifdef ASSERT\n+        mem->dump();\n+#endif\n+        ShouldNotReachHere();\n+      }\n+      if (wq.test_set(mem->_idx)) {\n+        return;\n+      }\n+    }\n+    \/\/ Replace memory input and re-compute ctrl to move the check out of the loop\n+    _igvn.replace_input_of(n, 1, mem);\n+    set_ctrl_and_loop(n, get_early_ctrl(n));\n+    Node* bol = n->unique_out();\n+    set_ctrl_and_loop(bol, get_early_ctrl(bol));\n+  }\n+}\n+\n@@ -1065,0 +1125,6 @@\n+\n+  if (UseArrayMarkWordCheck && n->isa_FlatArrayCheck()) {\n+    move_flat_array_check_out_of_loop(n);\n+    return n;\n+  }\n+\n@@ -1342,0 +1408,98 @@\n+bool PhaseIdealLoop::flat_array_element_type_check(Node *n) {\n+  \/\/ If the CmpP is a subtype check for a value that has just been\n+  \/\/ loaded from an array, the subtype check guarantees the value\n+  \/\/ can't be stored in a flat array and the load of the value\n+  \/\/ happens with a flat array check then: push the type check\n+  \/\/ through the phi of the flat array check. This needs special\n+  \/\/ logic because the subtype check's input is not a phi but a\n+  \/\/ LoadKlass that must first be cloned through the phi.\n+  if (n->Opcode() != Op_CmpP) {\n+    return false;\n+  }\n+\n+  Node* klassptr = n->in(1);\n+  Node* klasscon = n->in(2);\n+\n+  if (klassptr->is_DecodeNarrowPtr()) {\n+    klassptr = klassptr->in(1);\n+  }\n+\n+  if (klassptr->Opcode() != Op_LoadKlass && klassptr->Opcode() != Op_LoadNKlass) {\n+    return false;\n+  }\n+\n+  if (!klasscon->is_Con()) {\n+    return false;\n+  }\n+\n+  Node* addr = klassptr->in(MemNode::Address);\n+\n+  if (!addr->is_AddP()) {\n+    return false;\n+  }\n+\n+  intptr_t offset;\n+  Node* obj = AddPNode::Ideal_base_and_offset(addr, &_igvn, offset);\n+\n+  if (obj == nullptr) {\n+    return false;\n+  }\n+\n+  assert(obj != nullptr && addr->in(AddPNode::Base) == addr->in(AddPNode::Address), \"malformed AddP?\");\n+  if (obj->Opcode() == Op_CastPP) {\n+    obj = obj->in(1);\n+  }\n+\n+  if (!obj->is_Phi()) {\n+    return false;\n+  }\n+\n+  Node* region = obj->in(0);\n+\n+  Node* phi = PhiNode::make_blank(region, n->in(1));\n+  for (uint i = 1; i < region->req(); i++) {\n+    Node* in = obj->in(i);\n+    Node* ctrl = region->in(i);\n+    if (addr->in(AddPNode::Base) != obj) {\n+      Node* cast = addr->in(AddPNode::Base);\n+      assert(cast->Opcode() == Op_CastPP && cast->in(0) != nullptr, \"inconsistent subgraph\");\n+      Node* cast_clone = cast->clone();\n+      cast_clone->set_req(0, ctrl);\n+      cast_clone->set_req(1, in);\n+      register_new_node(cast_clone, ctrl);\n+      const Type* tcast = cast_clone->Value(&_igvn);\n+      _igvn.set_type(cast_clone, tcast);\n+      cast_clone->as_Type()->set_type(tcast);\n+      in = cast_clone;\n+    }\n+    Node* addr_clone = addr->clone();\n+    addr_clone->set_req(AddPNode::Base, in);\n+    addr_clone->set_req(AddPNode::Address, in);\n+    register_new_node(addr_clone, ctrl);\n+    _igvn.set_type(addr_clone, addr_clone->Value(&_igvn));\n+    Node* klassptr_clone = klassptr->clone();\n+    klassptr_clone->set_req(2, addr_clone);\n+    register_new_node(klassptr_clone, ctrl);\n+    _igvn.set_type(klassptr_clone, klassptr_clone->Value(&_igvn));\n+    if (klassptr != n->in(1)) {\n+      Node* decode = n->in(1);\n+      assert(decode->is_DecodeNarrowPtr(), \"inconsistent subgraph\");\n+      Node* decode_clone = decode->clone();\n+      decode_clone->set_req(1, klassptr_clone);\n+      register_new_node(decode_clone, ctrl);\n+      _igvn.set_type(decode_clone, decode_clone->Value(&_igvn));\n+      klassptr_clone = decode_clone;\n+    }\n+    phi->set_req(i, klassptr_clone);\n+  }\n+  register_new_node(phi, region);\n+  Node* orig = n->in(1);\n+  _igvn.replace_input_of(n, 1, phi);\n+  split_if_with_blocks_post(n);\n+  if (n->outcnt() != 0) {\n+    _igvn.replace_input_of(n, 1, orig);\n+    _igvn.remove_dead_node(phi);\n+  }\n+  return true;\n+}\n+\n@@ -1348,0 +1512,4 @@\n+  if (flat_array_element_type_check(n)) {\n+    return;\n+  }\n+\n@@ -1481,0 +1649,5 @@\n+\n+  \/\/ Remove multiple allocations of the same inline type\n+  if (n->is_InlineType()) {\n+    n->as_InlineType()->remove_redundant_allocations(this);\n+  }\n@@ -1939,1 +2112,9 @@\n-  Node *sample_cmp = sample_bool->in(1);\n+  Node* sample_cmp = sample_bool->in(1);\n+  const Type* t = Type::TOP;\n+  const TypePtr* at = nullptr;\n+  if (sample_cmp->is_FlatArrayCheck()) {\n+    \/\/ Left input of a FlatArrayCheckNode is memory, set the (adr) type of the phi accordingly\n+    assert(sample_cmp->in(1)->bottom_type() == Type::MEMORY, \"unexpected input type\");\n+    t = Type::MEMORY;\n+    at = TypeRawPtr::BOTTOM;\n+  }\n@@ -1942,1 +2123,1 @@\n-  PhiNode *phi1 = new PhiNode(phi->in(0), Type::TOP);\n+  PhiNode *phi1 = new PhiNode(phi->in(0), t, at);\n","filename":"src\/hotspot\/share\/opto\/loopopts.cpp","additions":183,"deletions":2,"binary":false,"changes":185,"status":"modified"},{"patch":"@@ -26,0 +26,1 @@\n+#include \"ci\/ciFlatArrayKlass.hpp\"\n@@ -39,0 +40,1 @@\n+#include \"opto\/inlinetypenode.hpp\"\n@@ -56,0 +58,1 @@\n+#include \"runtime\/stubRoutines.hpp\"\n@@ -85,12 +88,0 @@\n-void PhaseMacroExpand::migrate_outs(Node *old, Node *target) {\n-  assert(old != nullptr, \"sanity\");\n-  for (DUIterator_Fast imax, i = old->fast_outs(imax); i < imax; i++) {\n-    Node* use = old->fast_out(i);\n-    _igvn.rehash_node_delayed(use);\n-    imax -= replace_input(use, old, target);\n-    \/\/ back up iterator\n-    --i;\n-  }\n-  assert(old->outcnt() == 0, \"all uses must be deleted\");\n-}\n-\n@@ -159,1 +150,1 @@\n-  bs->eliminate_gc_barrier(this, p2x);\n+  bs->eliminate_gc_barrier(&_igvn, p2x);\n@@ -213,1 +204,1 @@\n-        int adr_offset = atype->offset();\n+        int adr_offset = atype->flat_offset();\n@@ -258,1 +249,1 @@\n-   } else if (mem->Opcode() == Op_StrInflatedCopy) {\n+    } else if (mem->Opcode() == Op_StrInflatedCopy) {\n@@ -303,1 +294,5 @@\n-      const TypePtr* adr_type = nullptr;\n+      Node* base = ac->in(ArrayCopyNode::Src);\n+      const TypeAryPtr* adr_type = _igvn.type(base)->is_aryptr();\n+      if (adr_type->is_flat()) {\n+        shift = adr_type->flat_log_elem_size();\n+      }\n@@ -306,2 +301,2 @@\n-        Node* base = ac->in(ArrayCopyNode::Src);\n-        adr_type = _igvn.type(base)->is_ptr()->add_offset(off);\n+        adr_type = _igvn.type(adr)->is_aryptr();\n+        assert(adr_type == _igvn.type(base)->is_aryptr()->add_field_offset_and_offset(off), \"incorrect address type\");\n@@ -311,1 +306,1 @@\n-          return value_from_mem(ac->in(TypeFunc::Memory), ctl, ft, ftype, adr_type->isa_oopptr(), alloc);\n+          return value_from_mem(ac->in(TypeFunc::Memory), ctl, ft, ftype, adr_type, alloc);\n@@ -314,0 +309,5 @@\n+        if (ac->in(ArrayCopyNode::Src) == ac->in(ArrayCopyNode::Dest)) {\n+          \/\/ Non constant offset in the array: we can't statically\n+          \/\/ determine the value\n+          return nullptr;\n+        }\n@@ -321,7 +321,5 @@\n-        Node* base = ac->in(ArrayCopyNode::Src);\n-        adr_type = _igvn.type(base)->is_ptr()->add_offset(Type::OffsetBot);\n-        if (ac->in(ArrayCopyNode::Src) == ac->in(ArrayCopyNode::Dest)) {\n-          \/\/ Non constant offset in the array: we can't statically\n-          \/\/ determine the value\n-          return nullptr;\n-        }\n+        \/\/ In the case of a flat inline type array, each field has its\n+        \/\/ own slice so we need to extract the field being accessed from\n+        \/\/ the address computation\n+        adr_type = adr_type->add_field_offset_and_offset(offset)->add_offset(Type::OffsetBot)->is_aryptr();\n+        adr = _igvn.transform(new CastPPNode(adr, adr_type));\n@@ -338,0 +336,1 @@\n+      assert(res->isa_DecodeN(), \"should be narrow oop\");\n@@ -353,1 +352,1 @@\n-  int offset = adr_t->offset();\n+  int offset = adr_t->flat_offset();\n@@ -388,1 +387,1 @@\n-    } else  {\n+    } else {\n@@ -392,1 +391,7 @@\n-        values.at_put(j, _igvn.zerocon(ft));\n+        Node* default_value = alloc->in(AllocateNode::DefaultValue);\n+        if (default_value != nullptr) {\n+          values.at_put(j, default_value);\n+        } else {\n+          assert(alloc->in(AllocateNode::RawDefaultValue) == nullptr, \"default value may not be null\");\n+          values.at_put(j, _igvn.zerocon(ft));\n+        }\n@@ -412,1 +417,7 @@\n-        values.at_put(j, _igvn.zerocon(ft));\n+        Node* default_value = alloc->in(AllocateNode::DefaultValue);\n+        if (default_value != nullptr) {\n+          values.at_put(j, default_value);\n+        } else {\n+          assert(alloc->in(AllocateNode::RawDefaultValue) == nullptr, \"default value may not be null\");\n+          values.at_put(j, _igvn.zerocon(ft));\n+        }\n@@ -456,1 +467,1 @@\n-  int offset = adr_t->offset();\n+  int offset = adr_t->flat_offset();\n@@ -458,1 +469,0 @@\n-  Node *alloc_ctrl = alloc->in(TypeFunc::Control);\n@@ -474,1 +484,1 @@\n-        done = true; \/\/ Something go wrong.\n+        done = true; \/\/ Something went wrong.\n@@ -484,1 +494,1 @@\n-             atype->is_known_instance_field() && atype->offset() == offset &&\n+             atype->is_known_instance_field() && atype->flat_offset() == offset &&\n@@ -517,0 +527,5 @@\n+      Node* default_value = alloc->in(AllocateNode::DefaultValue);\n+      if (default_value != nullptr) {\n+        return default_value;\n+      }\n+      assert(alloc->in(AllocateNode::RawDefaultValue) == nullptr, \"default value may not be null\");\n@@ -548,1 +563,1 @@\n-  \/\/ Something go wrong.\n+  \/\/ Something went wrong.\n@@ -552,0 +567,42 @@\n+\/\/ Search the last value stored into the inline type's fields.\n+Node* PhaseMacroExpand::inline_type_from_mem(Node* mem, Node* ctl, ciInlineKlass* vk, const TypeAryPtr* adr_type, int offset, AllocateNode* alloc) {\n+  \/\/ Subtract the offset of the first field to account for the missing oop header\n+  offset -= vk->first_field_offset();\n+  \/\/ Create a new InlineTypeNode and retrieve the field values from memory\n+  InlineTypeNode* vt = InlineTypeNode::make_uninitialized(_igvn, vk);\n+  transform_later(vt);\n+  for (int i = 0; i < vk->nof_declared_nonstatic_fields(); ++i) {\n+    ciType* field_type = vt->field_type(i);\n+    int field_offset = offset + vt->field_offset(i);\n+    Node* value = nullptr;\n+    if (vt->field_is_flat(i)) {\n+      value = inline_type_from_mem(mem, ctl, field_type->as_inline_klass(), adr_type, field_offset, alloc);\n+    } else {\n+      const Type* ft = Type::get_const_type(field_type);\n+      BasicType bt = type2field[field_type->basic_type()];\n+      if (UseCompressedOops && !is_java_primitive(bt)) {\n+        ft = ft->make_narrowoop();\n+        bt = T_NARROWOOP;\n+      }\n+      \/\/ Each inline type field has its own memory slice\n+      adr_type = adr_type->with_field_offset(field_offset);\n+      value = value_from_mem(mem, ctl, bt, ft, adr_type, alloc);\n+      if (value != nullptr && ft->isa_narrowoop()) {\n+        assert(UseCompressedOops, \"unexpected narrow oop\");\n+        if (value->is_EncodeP()) {\n+          value = value->in(1);\n+        } else {\n+          value = transform_later(new DecodeNNode(value, value->get_ptr_type()));\n+        }\n+      }\n+    }\n+    if (value != nullptr) {\n+      vt->set_field_value(i, value);\n+    } else {\n+      \/\/ We might have reached the TrackedInitializationLimit\n+      return nullptr;\n+    }\n+  }\n+  return vt;\n+}\n+\n@@ -561,0 +618,1 @@\n+  Unique_Node_List worklist;\n@@ -569,0 +627,1 @@\n+    worklist.push(res);\n@@ -582,1 +641,1 @@\n-  if (can_eliminate && res != nullptr) {\n+  while (can_eliminate && worklist.size() > 0) {\n@@ -584,2 +643,2 @@\n-    for (DUIterator_Fast jmax, j = res->fast_outs(jmax);\n-                               j < jmax && can_eliminate; j++) {\n+    res = worklist.pop();\n+    for (DUIterator_Fast jmax, j = res->fast_outs(jmax); j < jmax && can_eliminate; j++) {\n@@ -633,0 +692,21 @@\n+      } else if (use->is_InlineType() && use->as_InlineType()->get_oop() == res) {\n+        \/\/ Look at uses\n+        for (DUIterator_Fast kmax, k = use->fast_outs(kmax); k < kmax; k++) {\n+          Node* u = use->fast_out(k);\n+          if (u->is_InlineType()) {\n+            \/\/ Use in flat field can be eliminated\n+            InlineTypeNode* vt = u->as_InlineType();\n+            for (uint i = 0; i < vt->field_count(); ++i) {\n+              if (vt->field_value(i) == use && !vt->field_is_flat(i)) {\n+                can_eliminate = false; \/\/ Use in non-flat field\n+                break;\n+              }\n+            }\n+          } else {\n+            \/\/ Add other uses to the worklist to process individually\n+            worklist.push(u);\n+          }\n+        }\n+      } else if (use->Opcode() == Op_StoreX && use->in(MemNode::Address) == res) {\n+        \/\/ Store to mark word of inline type larval buffer\n+        assert(res_type->is_inlinetypeptr(), \"Unexpected store to mark word\");\n@@ -652,0 +732,3 @@\n+      } else {\n+        assert(use->Opcode() == Op_CastP2X, \"should be\");\n+        assert(!use->has_out_with(Op_OrL), \"should have been removed because oop is never null\");\n@@ -664,1 +747,1 @@\n-    } else if (alloc->_is_scalar_replaceable) {\n+    } else {\n@@ -729,1 +812,2 @@\n-SafePointScalarObjectNode* PhaseMacroExpand::create_scalarized_object_description(AllocateNode *alloc, SafePointNode* sfpt) {\n+SafePointScalarObjectNode* PhaseMacroExpand::create_scalarized_object_description(AllocateNode *alloc, SafePointNode* sfpt,\n+                                                                                  Unique_Node_List* value_worklist) {\n@@ -761,0 +845,4 @@\n+      if (res_type->is_flat()) {\n+        \/\/ Flat inline type array\n+        element_size = res_type->is_aryptr()->flat_elem_size();\n+      }\n@@ -777,0 +865,1 @@\n+      assert(!field->is_flat(), \"flat inline type fields should not have safepoint uses\");\n@@ -802,3 +891,9 @@\n-    const TypeOopPtr *field_addr_type = res_type->add_offset(offset)->isa_oopptr();\n-\n-    Node *field_val = value_from_mem(sfpt->memory(), sfpt->control(), basic_elem_type, field_type, field_addr_type, alloc);\n+    Node* field_val = nullptr;\n+    const TypeOopPtr* field_addr_type = res_type->add_offset(offset)->isa_oopptr();\n+    if (res_type->is_flat()) {\n+      ciInlineKlass* vk = res_type->is_aryptr()->elem()->inline_klass();\n+      assert(vk->flat_in_array(), \"must be flat in array\");\n+      field_val = inline_type_from_mem(sfpt->memory(), sfpt->control(), vk, field_addr_type->isa_aryptr(), 0, alloc);\n+    } else {\n+      field_val = value_from_mem(sfpt->memory(), sfpt->control(), basic_elem_type, field_type, field_addr_type, alloc);\n+    }\n@@ -841,1 +936,1 @@\n-      } else {\n+      } else if (!field_val->is_InlineType()) {\n@@ -845,0 +940,4 @@\n+    if (field_val->is_InlineType()) {\n+      \/\/ Keep track of inline types to scalarize them later\n+      value_worklist->push(field_val);\n+    }\n@@ -858,0 +957,4 @@\n+  const TypeOopPtr* res_type = nullptr;\n+  if (res != nullptr) { \/\/ Could be null when there are no users\n+    res_type = _igvn.type(res)->isa_oopptr();\n+  }\n@@ -860,0 +963,2 @@\n+  assert(safepoints.length() == 0 || !res_type->is_inlinetypeptr(), \"Inline type allocations should not have safepoint uses\");\n+  Unique_Node_List value_worklist;\n@@ -862,1 +967,1 @@\n-    SafePointScalarObjectNode* sobj = create_scalarized_object_description(alloc, sfpt);\n+    SafePointScalarObjectNode* sobj = create_scalarized_object_description(alloc, sfpt, &value_worklist);\n@@ -878,1 +983,8 @@\n-\n+  \/\/ Scalarize inline types that were added to the safepoint.\n+  \/\/ Don't allow linking a constant oop (if available) for flat array elements\n+  \/\/ because Deoptimization::reassign_flat_array_elements needs field values.\n+  bool allow_oop = (res_type != nullptr) && !res_type->is_flat();\n+  for (uint i = 0; i < value_worklist.size(); ++i) {\n+    InlineTypeNode* vt = value_worklist.at(i)->as_InlineType();\n+    vt->make_scalar_in_safepoints(&_igvn, allow_oop);\n+  }\n@@ -894,1 +1006,2 @@\n-void PhaseMacroExpand::process_users_of_allocation(CallNode *alloc) {\n+void PhaseMacroExpand::process_users_of_allocation(CallNode *alloc, bool inline_alloc) {\n+  Unique_Node_List worklist;\n@@ -897,0 +1010,4 @@\n+    worklist.push(res);\n+  }\n+  while (worklist.size() > 0) {\n+    res = worklist.pop();\n@@ -906,10 +1023,7 @@\n-#ifdef ASSERT\n-            \/\/ Verify that there is no dependent MemBarVolatile nodes,\n-            \/\/ they should be removed during IGVN, see MemBarNode::Ideal().\n-            for (DUIterator_Fast pmax, p = n->fast_outs(pmax);\n-                                       p < pmax; p++) {\n-              Node* mb = n->fast_out(p);\n-              assert(mb->is_Initialize() || !mb->is_MemBar() ||\n-                     mb->req() <= MemBarNode::Precedent ||\n-                     mb->in(MemBarNode::Precedent) != n,\n-                     \"MemBarVolatile should be eliminated for non-escaping object\");\n+            for (DUIterator_Fast pmax, p = n->fast_outs(pmax); p < pmax; p++) {\n+              MemBarNode* mb = n->fast_out(p)->isa_MemBar();\n+              if (mb != nullptr && mb->req() <= MemBarNode::Precedent && mb->in(MemBarNode::Precedent) == n) {\n+                \/\/ MemBarVolatiles should have been removed by MemBarNode::Ideal() for non-inline allocations\n+                assert(inline_alloc, \"MemBarVolatile should be eliminated for non-escaping object\");\n+                mb->remove(&_igvn);\n+              }\n@@ -917,1 +1031,0 @@\n-#endif\n@@ -941,2 +1054,1 @@\n-          CallProjections callprojs;\n-          ac->extract_projections(&callprojs, true);\n+          CallProjections* callprojs = ac->extract_projections(true);\n@@ -944,3 +1056,3 @@\n-          _igvn.replace_node(callprojs.fallthrough_ioproj, ac->in(TypeFunc::I_O));\n-          _igvn.replace_node(callprojs.fallthrough_memproj, ac->in(TypeFunc::Memory));\n-          _igvn.replace_node(callprojs.fallthrough_catchproj, ac->in(TypeFunc::Control));\n+          _igvn.replace_node(callprojs->fallthrough_ioproj, ac->in(TypeFunc::I_O));\n+          _igvn.replace_node(callprojs->fallthrough_memproj, ac->in(TypeFunc::Memory));\n+          _igvn.replace_node(callprojs->fallthrough_catchproj, ac->in(TypeFunc::Control));\n@@ -963,0 +1075,19 @@\n+      } else if (use->is_InlineType()) {\n+        assert(use->as_InlineType()->get_oop() == res, \"unexpected inline type ptr use\");\n+        \/\/ Cut off oop input and remove known instance id from type\n+        _igvn.rehash_node_delayed(use);\n+        use->as_InlineType()->set_oop(_igvn.zerocon(T_OBJECT));\n+        const TypeOopPtr* toop = _igvn.type(use)->is_oopptr()->cast_to_instance_id(TypeOopPtr::InstanceBot);\n+        _igvn.set_type(use, toop);\n+        use->as_InlineType()->set_type(toop);\n+        \/\/ Process users\n+        for (DUIterator_Fast kmax, k = use->fast_outs(kmax); k < kmax; k++) {\n+          Node* u = use->fast_out(k);\n+          if (!u->is_InlineType()) {\n+            worklist.push(u);\n+          }\n+        }\n+      } else if (use->Opcode() == Op_StoreX && use->in(MemNode::Address) == res) {\n+        \/\/ Store to mark word of inline type larval buffer\n+        assert(inline_alloc, \"Unexpected store to mark word\");\n+        _igvn.replace_node(use, use->in(MemNode::Memory));\n@@ -975,1 +1106,1 @@\n-  if (_callprojs.resproj != nullptr && _callprojs.resproj->outcnt() != 0) {\n+  if (_callprojs->resproj[0] != nullptr && _callprojs->resproj[0]->outcnt() != 0) {\n@@ -979,2 +1110,2 @@\n-    for (DUIterator_Fast jmax, j = _callprojs.resproj->fast_outs(jmax);  j < jmax; j++) {\n-      Node* use = _callprojs.resproj->fast_out(j);\n+    for (DUIterator_Fast jmax, j = _callprojs->resproj[0]->fast_outs(jmax);  j < jmax; j++) {\n+      Node* use = _callprojs->resproj[0]->fast_out(j);\n@@ -987,3 +1118,3 @@\n-    for (DUIterator_Last jmin, j = _callprojs.resproj->last_outs(jmin); j >= jmin; ) {\n-      Node* use = _callprojs.resproj->last_out(j);\n-      uint oc1 = _callprojs.resproj->outcnt();\n+    for (DUIterator_Last jmin, j = _callprojs->resproj[0]->last_outs(jmin); j >= jmin; ) {\n+      Node* use = _callprojs->resproj[0]->last_out(j);\n+      uint oc1 = _callprojs->resproj[0]->outcnt();\n@@ -1000,1 +1131,1 @@\n-          assert(tmp == nullptr || tmp == _callprojs.fallthrough_catchproj, \"allocation control projection\");\n+          assert(tmp == nullptr || tmp == _callprojs->fallthrough_catchproj, \"allocation control projection\");\n@@ -1008,1 +1139,1 @@\n-            assert(mem->in(TypeFunc::Memory) == _callprojs.fallthrough_memproj, \"allocation memory projection\");\n+            assert(mem->in(TypeFunc::Memory) == _callprojs->fallthrough_memproj, \"allocation memory projection\");\n@@ -1010,1 +1141,1 @@\n-            assert(mem == _callprojs.fallthrough_memproj, \"allocation memory projection\");\n+            assert(mem == _callprojs->fallthrough_memproj, \"allocation memory projection\");\n@@ -1015,0 +1146,4 @@\n+      } else if (use->Opcode() == Op_MemBarStoreStore) {\n+        \/\/ Inline type buffer allocations are followed by a membar\n+        assert(inline_alloc, \"Unexpected MemBarStoreStore\");\n+        use->as_MemBar()->remove(&_igvn);\n@@ -1018,1 +1153,1 @@\n-      j -= (oc1 - _callprojs.resproj->outcnt());\n+      j -= (oc1 - _callprojs->resproj[0]->outcnt());\n@@ -1021,2 +1156,2 @@\n-  if (_callprojs.fallthrough_catchproj != nullptr) {\n-    _igvn.replace_node(_callprojs.fallthrough_catchproj, alloc->in(TypeFunc::Control));\n+  if (_callprojs->fallthrough_catchproj != nullptr) {\n+    _igvn.replace_node(_callprojs->fallthrough_catchproj, alloc->in(TypeFunc::Control));\n@@ -1024,2 +1159,2 @@\n-  if (_callprojs.fallthrough_memproj != nullptr) {\n-    _igvn.replace_node(_callprojs.fallthrough_memproj, alloc->in(TypeFunc::Memory));\n+  if (_callprojs->fallthrough_memproj != nullptr) {\n+    _igvn.replace_node(_callprojs->fallthrough_memproj, alloc->in(TypeFunc::Memory));\n@@ -1027,2 +1162,2 @@\n-  if (_callprojs.catchall_memproj != nullptr) {\n-    _igvn.replace_node(_callprojs.catchall_memproj, C->top());\n+  if (_callprojs->catchall_memproj != nullptr) {\n+    _igvn.replace_node(_callprojs->catchall_memproj, C->top());\n@@ -1030,2 +1165,2 @@\n-  if (_callprojs.fallthrough_ioproj != nullptr) {\n-    _igvn.replace_node(_callprojs.fallthrough_ioproj, alloc->in(TypeFunc::I_O));\n+  if (_callprojs->fallthrough_ioproj != nullptr) {\n+    _igvn.replace_node(_callprojs->fallthrough_ioproj, alloc->in(TypeFunc::I_O));\n@@ -1033,2 +1168,2 @@\n-  if (_callprojs.catchall_ioproj != nullptr) {\n-    _igvn.replace_node(_callprojs.catchall_ioproj, C->top());\n+  if (_callprojs->catchall_ioproj != nullptr) {\n+    _igvn.replace_node(_callprojs->catchall_ioproj, C->top());\n@@ -1036,2 +1171,2 @@\n-  if (_callprojs.catchall_catchproj != nullptr) {\n-    _igvn.replace_node(_callprojs.catchall_catchproj, C->top());\n+  if (_callprojs->catchall_catchproj != nullptr) {\n+    _igvn.replace_node(_callprojs->catchall_catchproj, C->top());\n@@ -1047,1 +1182,1 @@\n-  if (!EliminateAllocations || !alloc->_is_non_escaping) {\n+  if (!EliminateAllocations) {\n@@ -1052,1 +1187,8 @@\n-  Node* res = alloc->result_cast();\n+\n+  \/\/ Attempt to eliminate inline type buffer allocations\n+  \/\/ regardless of usage and escape\/replaceable status.\n+  bool inline_alloc = tklass->isa_instklassptr() &&\n+                      tklass->is_instklassptr()->instance_klass()->is_inlinetype();\n+  if (!alloc->_is_non_escaping && !inline_alloc) {\n+    return false;\n+  }\n@@ -1055,1 +1197,2 @@\n-  bool boxing_alloc = C->eliminate_boxing() &&\n+  Node* res = alloc->result_cast();\n+  bool boxing_alloc = (res == nullptr) && C->eliminate_boxing() &&\n@@ -1062,1 +1205,1 @@\n-  alloc->extract_projections(&_callprojs, false \/*separate_io_proj*\/, false \/*do_asserts*\/);\n+  _callprojs = alloc->extract_projections(false \/*separate_io_proj*\/, false \/*do_asserts*\/);\n@@ -1070,1 +1213,1 @@\n-    assert(res == nullptr, \"sanity\");\n+    assert(res == nullptr || inline_alloc, \"sanity\");\n@@ -1075,0 +1218,1 @@\n+      assert(!inline_alloc, \"Inline type allocations should not have safepoint uses\");\n@@ -1095,1 +1239,1 @@\n-  process_users_of_allocation(alloc);\n+  process_users_of_allocation(alloc, inline_alloc);\n@@ -1117,1 +1261,1 @@\n-  boxing->extract_projections(&_callprojs, false \/*separate_io_proj*\/, false \/*do_asserts*\/);\n+  _callprojs = boxing->extract_projections(false \/*separate_io_proj*\/, false \/*do_asserts*\/);\n@@ -1119,1 +1263,1 @@\n-  const TypeTuple* r = boxing->tf()->range();\n+  const TypeTuple* r = boxing->tf()->range_sig();\n@@ -1303,1 +1447,1 @@\n-    IfNode *toobig_iff = new IfNode(ctrl, initial_slow_test, PROB_MIN, COUNT_UNKNOWN);\n+    IfNode* toobig_iff = new IfNode(ctrl, initial_slow_test, PROB_MIN, COUNT_UNKNOWN);\n@@ -1306,1 +1450,1 @@\n-    Node *toobig_true = new IfTrueNode( toobig_iff );\n+    Node* toobig_true = new IfTrueNode(toobig_iff);\n@@ -1309,1 +1453,1 @@\n-    toobig_false = new IfFalseNode( toobig_iff );\n+    toobig_false = new IfFalseNode(toobig_iff);\n@@ -1348,0 +1492,1 @@\n+\n@@ -1405,0 +1550,3 @@\n+  } else {\n+    \/\/ Let the runtime know if this is a larval allocation\n+    call->init_req(TypeFunc::Parms+1, _igvn.intcon(alloc->_larval));\n@@ -1436,1 +1584,1 @@\n-  call->extract_projections(&_callprojs, false \/*separate_io_proj*\/, false \/*do_asserts*\/);\n+  _callprojs = call->extract_projections(false \/*separate_io_proj*\/, false \/*do_asserts*\/);\n@@ -1442,2 +1590,2 @@\n-  if (expand_fast_path && _callprojs.fallthrough_memproj != nullptr) {\n-    migrate_outs(_callprojs.fallthrough_memproj, result_phi_rawmem);\n+  if (expand_fast_path && _callprojs->fallthrough_memproj != nullptr) {\n+    _igvn.replace_in_uses(_callprojs->fallthrough_memproj, result_phi_rawmem);\n@@ -1447,4 +1595,4 @@\n-  if (_callprojs.catchall_memproj != nullptr ) {\n-    if (_callprojs.fallthrough_memproj == nullptr) {\n-      _callprojs.fallthrough_memproj = new ProjNode(call, TypeFunc::Memory);\n-      transform_later(_callprojs.fallthrough_memproj);\n+  if (_callprojs->catchall_memproj != nullptr) {\n+    if (_callprojs->fallthrough_memproj == nullptr) {\n+      _callprojs->fallthrough_memproj = new ProjNode(call, TypeFunc::Memory);\n+      transform_later(_callprojs->fallthrough_memproj);\n@@ -1452,2 +1600,2 @@\n-    migrate_outs(_callprojs.catchall_memproj, _callprojs.fallthrough_memproj);\n-    _igvn.remove_dead_node(_callprojs.catchall_memproj);\n+    _igvn.replace_in_uses(_callprojs->catchall_memproj, _callprojs->fallthrough_memproj);\n+    _igvn.remove_dead_node(_callprojs->catchall_memproj);\n@@ -1461,2 +1609,2 @@\n-  if (_callprojs.fallthrough_ioproj != nullptr) {\n-    migrate_outs(_callprojs.fallthrough_ioproj, result_phi_i_o);\n+  if (_callprojs->fallthrough_ioproj != nullptr) {\n+    _igvn.replace_in_uses(_callprojs->fallthrough_ioproj, result_phi_i_o);\n@@ -1466,4 +1614,4 @@\n-  if (_callprojs.catchall_ioproj != nullptr ) {\n-    if (_callprojs.fallthrough_ioproj == nullptr) {\n-      _callprojs.fallthrough_ioproj = new ProjNode(call, TypeFunc::I_O);\n-      transform_later(_callprojs.fallthrough_ioproj);\n+  if (_callprojs->catchall_ioproj != nullptr) {\n+    if (_callprojs->fallthrough_ioproj == nullptr) {\n+      _callprojs->fallthrough_ioproj = new ProjNode(call, TypeFunc::I_O);\n+      transform_later(_callprojs->fallthrough_ioproj);\n@@ -1471,2 +1619,2 @@\n-    migrate_outs(_callprojs.catchall_ioproj, _callprojs.fallthrough_ioproj);\n-    _igvn.remove_dead_node(_callprojs.catchall_ioproj);\n+    _igvn.replace_in_uses(_callprojs->catchall_ioproj, _callprojs->fallthrough_ioproj);\n+    _igvn.remove_dead_node(_callprojs->catchall_ioproj);\n@@ -1491,2 +1639,2 @@\n-  if (_callprojs.fallthrough_catchproj != nullptr) {\n-    ctrl = _callprojs.fallthrough_catchproj->clone();\n+  if (_callprojs->fallthrough_catchproj != nullptr) {\n+    ctrl = _callprojs->fallthrough_catchproj->clone();\n@@ -1494,1 +1642,1 @@\n-    _igvn.replace_node(_callprojs.fallthrough_catchproj, result_region);\n+    _igvn.replace_node(_callprojs->fallthrough_catchproj, result_region);\n@@ -1499,1 +1647,1 @@\n-  if (_callprojs.resproj == nullptr) {\n+  if (_callprojs->resproj[0] == nullptr) {\n@@ -1503,1 +1651,1 @@\n-    slow_result = _callprojs.resproj->clone();\n+    slow_result = _callprojs->resproj[0]->clone();\n@@ -1505,1 +1653,1 @@\n-    _igvn.replace_node(_callprojs.resproj, result_phi_rawoop);\n+    _igvn.replace_node(_callprojs->resproj[0], result_phi_rawoop);\n@@ -1515,1 +1663,1 @@\n-  result_phi_rawmem->init_req(slow_result_path, _callprojs.fallthrough_memproj);\n+  result_phi_rawmem->init_req(slow_result_path, _callprojs->fallthrough_memproj);\n@@ -1527,4 +1675,4 @@\n-  alloc->extract_projections(&_callprojs, false \/*separate_io_proj*\/, false \/*do_asserts*\/);\n-  if (_callprojs.resproj != nullptr) {\n-    for (DUIterator_Fast imax, i = _callprojs.resproj->fast_outs(imax); i < imax; i++) {\n-      Node* use = _callprojs.resproj->fast_out(i);\n+  _callprojs = alloc->extract_projections(false \/*separate_io_proj*\/, false \/*do_asserts*\/);\n+  if (_callprojs->resproj[0] != nullptr) {\n+    for (DUIterator_Fast imax, i = _callprojs->resproj[0]->fast_outs(imax); i < imax; i++) {\n+      Node* use = _callprojs->resproj[0]->fast_out(i);\n@@ -1535,2 +1683,2 @@\n-    assert(_callprojs.resproj->outcnt() == 0, \"all uses must be deleted\");\n-    _igvn.remove_dead_node(_callprojs.resproj);\n+    assert(_callprojs->resproj[0]->outcnt() == 0, \"all uses must be deleted\");\n+    _igvn.remove_dead_node(_callprojs->resproj[0]);\n@@ -1538,3 +1686,3 @@\n-  if (_callprojs.fallthrough_catchproj != nullptr) {\n-    migrate_outs(_callprojs.fallthrough_catchproj, ctrl);\n-    _igvn.remove_dead_node(_callprojs.fallthrough_catchproj);\n+  if (_callprojs->fallthrough_catchproj != nullptr) {\n+    _igvn.replace_in_uses(_callprojs->fallthrough_catchproj, ctrl);\n+    _igvn.remove_dead_node(_callprojs->fallthrough_catchproj);\n@@ -1542,3 +1690,3 @@\n-  if (_callprojs.catchall_catchproj != nullptr) {\n-    _igvn.rehash_node_delayed(_callprojs.catchall_catchproj);\n-    _callprojs.catchall_catchproj->set_req(0, top());\n+  if (_callprojs->catchall_catchproj != nullptr) {\n+    _igvn.rehash_node_delayed(_callprojs->catchall_catchproj);\n+    _callprojs->catchall_catchproj->set_req(0, top());\n@@ -1546,2 +1694,2 @@\n-  if (_callprojs.fallthrough_proj != nullptr) {\n-    Node* catchnode = _callprojs.fallthrough_proj->unique_ctrl_out();\n+  if (_callprojs->fallthrough_proj != nullptr) {\n+    Node* catchnode = _callprojs->fallthrough_proj->unique_ctrl_out();\n@@ -1549,1 +1697,1 @@\n-    _igvn.remove_dead_node(_callprojs.fallthrough_proj);\n+    _igvn.remove_dead_node(_callprojs->fallthrough_proj);\n@@ -1551,3 +1699,3 @@\n-  if (_callprojs.fallthrough_memproj != nullptr) {\n-    migrate_outs(_callprojs.fallthrough_memproj, mem);\n-    _igvn.remove_dead_node(_callprojs.fallthrough_memproj);\n+  if (_callprojs->fallthrough_memproj != nullptr) {\n+    _igvn.replace_in_uses(_callprojs->fallthrough_memproj, mem);\n+    _igvn.remove_dead_node(_callprojs->fallthrough_memproj);\n@@ -1555,3 +1703,3 @@\n-  if (_callprojs.fallthrough_ioproj != nullptr) {\n-    migrate_outs(_callprojs.fallthrough_ioproj, i_o);\n-    _igvn.remove_dead_node(_callprojs.fallthrough_ioproj);\n+  if (_callprojs->fallthrough_ioproj != nullptr) {\n+    _igvn.replace_in_uses(_callprojs->fallthrough_ioproj, i_o);\n+    _igvn.remove_dead_node(_callprojs->fallthrough_ioproj);\n@@ -1559,3 +1707,3 @@\n-  if (_callprojs.catchall_memproj != nullptr) {\n-    _igvn.rehash_node_delayed(_callprojs.catchall_memproj);\n-    _callprojs.catchall_memproj->set_req(0, top());\n+  if (_callprojs->catchall_memproj != nullptr) {\n+    _igvn.rehash_node_delayed(_callprojs->catchall_memproj);\n+    _callprojs->catchall_memproj->set_req(0, top());\n@@ -1563,3 +1711,3 @@\n-  if (_callprojs.catchall_ioproj != nullptr) {\n-    _igvn.rehash_node_delayed(_callprojs.catchall_ioproj);\n-    _callprojs.catchall_ioproj->set_req(0, top());\n+  if (_callprojs->catchall_ioproj != nullptr) {\n+    _igvn.rehash_node_delayed(_callprojs->catchall_ioproj);\n+    _callprojs->catchall_ioproj->set_req(0, top());\n@@ -1683,5 +1831,4 @@\n-Node*\n-PhaseMacroExpand::initialize_object(AllocateNode* alloc,\n-                                    Node* control, Node* rawmem, Node* object,\n-                                    Node* klass_node, Node* length,\n-                                    Node* size_in_bytes) {\n+Node* PhaseMacroExpand::initialize_object(AllocateNode* alloc,\n+                                          Node* control, Node* rawmem, Node* object,\n+                                          Node* klass_node, Node* length,\n+                                          Node* size_in_bytes) {\n@@ -1690,1 +1837,1 @@\n-  Node* mark_node = alloc->make_ideal_mark(&_igvn, object, control, rawmem);\n+  Node* mark_node = alloc->make_ideal_mark(&_igvn, control, rawmem);\n@@ -1726,0 +1873,2 @@\n+                                            alloc->in(AllocateNode::DefaultValue),\n+                                            alloc->in(AllocateNode::RawDefaultValue),\n@@ -2095,0 +2244,43 @@\n+void PhaseMacroExpand::inline_type_guard(Node** ctrl, LockNode* lock) {\n+  Node* obj = lock->obj_node();\n+  const TypePtr* obj_type = _igvn.type(obj)->make_ptr();\n+  if (!obj_type->can_be_inline_type()) {\n+    return;\n+  }\n+  Node* mark = make_load(*ctrl, lock->memory(), obj, oopDesc::mark_offset_in_bytes(), TypeX_X, TypeX_X->basic_type());\n+  Node* value_mask = _igvn.MakeConX(markWord::inline_type_pattern);\n+  Node* is_value = _igvn.transform(new AndXNode(mark, value_mask));\n+  Node* cmp = _igvn.transform(new CmpXNode(is_value, value_mask));\n+  Node* bol = _igvn.transform(new BoolNode(cmp, BoolTest::eq));\n+  Node* unc_ctrl = generate_slow_guard(ctrl, bol, nullptr);\n+\n+  int trap_request = Deoptimization::make_trap_request(Deoptimization::Reason_class_check, Deoptimization::Action_none);\n+  address call_addr = SharedRuntime::uncommon_trap_blob()->entry_point();\n+  const TypePtr* no_memory_effects = nullptr;\n+  CallNode* unc = new CallStaticJavaNode(OptoRuntime::uncommon_trap_Type(), call_addr, \"uncommon_trap\",\n+                                         no_memory_effects);\n+  unc->init_req(TypeFunc::Control, unc_ctrl);\n+  unc->init_req(TypeFunc::I_O, lock->i_o());\n+  unc->init_req(TypeFunc::Memory, lock->memory());\n+  unc->init_req(TypeFunc::FramePtr,  lock->in(TypeFunc::FramePtr));\n+  unc->init_req(TypeFunc::ReturnAdr, lock->in(TypeFunc::ReturnAdr));\n+  unc->init_req(TypeFunc::Parms+0, _igvn.intcon(trap_request));\n+  unc->set_cnt(PROB_UNLIKELY_MAG(4));\n+  unc->copy_call_debug_info(&_igvn, lock);\n+\n+  assert(unc->peek_monitor_box() == lock->box_node(), \"wrong monitor\");\n+  assert((obj_type->is_inlinetypeptr() && unc->peek_monitor_obj()->is_SafePointScalarObject()) ||\n+         (obj->is_InlineType() && obj->in(1) == unc->peek_monitor_obj()) ||\n+         (obj == unc->peek_monitor_obj()), \"wrong monitor\");\n+\n+  \/\/ pop monitor and push obj back on stack: we trap before the monitorenter\n+  unc->pop_monitor();\n+  unc->grow_stack(unc->jvms(), 1);\n+  unc->set_stack(unc->jvms(), unc->jvms()->stk_size()-1, obj);\n+  _igvn.register_new_node_with_optimizer(unc);\n+\n+  unc_ctrl = _igvn.transform(new ProjNode(unc, TypeFunc::Control));\n+  Node* halt = _igvn.transform(new HaltNode(unc_ctrl, lock->in(TypeFunc::FramePtr), \"monitor enter on inline type\"));\n+  _igvn.add_input_to(C->root(), halt);\n+}\n+\n@@ -2126,1 +2318,1 @@\n-  alock->extract_projections(&_callprojs, false \/*separate_io_proj*\/, false \/*do_asserts*\/);\n+  _callprojs = alock->extract_projections(false \/*separate_io_proj*\/, false \/*do_asserts*\/);\n@@ -2130,2 +2322,2 @@\n-         _callprojs.fallthrough_proj != nullptr &&\n-         _callprojs.fallthrough_memproj != nullptr,\n+         _callprojs->fallthrough_proj != nullptr &&\n+         _callprojs->fallthrough_memproj != nullptr,\n@@ -2134,2 +2326,2 @@\n-  Node* fallthroughproj = _callprojs.fallthrough_proj;\n-  Node* memproj_fallthrough = _callprojs.fallthrough_memproj;\n+  Node* fallthroughproj = _callprojs->fallthrough_proj;\n+  Node* memproj_fallthrough = _callprojs->fallthrough_memproj;\n@@ -2141,0 +2333,3 @@\n+    \/\/ Deoptimize and re-execute if object is an inline type\n+    inline_type_guard(&ctrl, alock->as_Lock());\n+\n@@ -2201,0 +2396,3 @@\n+  \/\/ Deoptimize and re-execute if object is an inline type\n+  inline_type_guard(&slow_path, lock);\n+\n@@ -2206,1 +2404,1 @@\n-  call->extract_projections(&_callprojs, false \/*separate_io_proj*\/, false \/*do_asserts*\/);\n+  _callprojs = call->extract_projections(false \/*separate_io_proj*\/, false \/*do_asserts*\/);\n@@ -2212,2 +2410,2 @@\n-  assert(_callprojs.fallthrough_ioproj == nullptr && _callprojs.catchall_ioproj == nullptr &&\n-         _callprojs.catchall_memproj == nullptr && _callprojs.catchall_catchproj == nullptr, \"Unexpected projection from Lock\");\n+  assert(_callprojs->fallthrough_ioproj == nullptr && _callprojs->catchall_ioproj == nullptr &&\n+         _callprojs->catchall_memproj == nullptr && _callprojs->catchall_catchproj == nullptr, \"Unexpected projection from Lock\");\n@@ -2218,1 +2416,1 @@\n-  Node *slow_ctrl = _callprojs.fallthrough_proj->clone();\n+  Node *slow_ctrl = _callprojs->fallthrough_proj->clone();\n@@ -2220,2 +2418,2 @@\n-  _igvn.hash_delete(_callprojs.fallthrough_proj);\n-  _callprojs.fallthrough_proj->disconnect_inputs(C);\n+  _igvn.hash_delete(_callprojs->fallthrough_proj);\n+  _callprojs->fallthrough_proj->disconnect_inputs(C);\n@@ -2225,1 +2423,1 @@\n-  _igvn.replace_node(_callprojs.fallthrough_proj, region);\n+  _igvn.replace_node(_callprojs->fallthrough_proj, region);\n@@ -2233,1 +2431,1 @@\n-  _igvn.replace_node(_callprojs.fallthrough_memproj, mem_phi);\n+  _igvn.replace_node(_callprojs->fallthrough_memproj, mem_phi);\n@@ -2266,3 +2464,3 @@\n-  call->extract_projections(&_callprojs, false \/*separate_io_proj*\/, false \/*do_asserts*\/);\n-  assert(_callprojs.fallthrough_ioproj == nullptr && _callprojs.catchall_ioproj == nullptr &&\n-         _callprojs.catchall_memproj == nullptr && _callprojs.catchall_catchproj == nullptr, \"Unexpected projection from Lock\");\n+  _callprojs = call->extract_projections(false \/*separate_io_proj*\/, false \/*do_asserts*\/);\n+  assert(_callprojs->fallthrough_ioproj == nullptr && _callprojs->catchall_ioproj == nullptr &&\n+         _callprojs->catchall_memproj == nullptr && _callprojs->catchall_catchproj == nullptr, \"Unexpected projection from Lock\");\n@@ -2274,1 +2472,1 @@\n-  Node *slow_ctrl = _callprojs.fallthrough_proj->clone();\n+  Node *slow_ctrl = _callprojs->fallthrough_proj->clone();\n@@ -2276,2 +2474,2 @@\n-  _igvn.hash_delete(_callprojs.fallthrough_proj);\n-  _callprojs.fallthrough_proj->disconnect_inputs(C);\n+  _igvn.hash_delete(_callprojs->fallthrough_proj);\n+  _callprojs->fallthrough_proj->disconnect_inputs(C);\n@@ -2281,1 +2479,1 @@\n-  _igvn.replace_node(_callprojs.fallthrough_proj, region);\n+  _igvn.replace_node(_callprojs->fallthrough_proj, region);\n@@ -2288,1 +2486,1 @@\n-  _igvn.replace_node(_callprojs.fallthrough_memproj, mem_phi);\n+  _igvn.replace_node(_callprojs->fallthrough_memproj, mem_phi);\n@@ -2291,0 +2489,211 @@\n+\/\/ An inline type might be returned from the call but we don't know its\n+\/\/ type. Either we get a buffered inline type (and nothing needs to be done)\n+\/\/ or one of the values being returned is the klass of the inline type\n+\/\/ and we need to allocate an inline type instance of that type and\n+\/\/ initialize it with other values being returned. In that case, we\n+\/\/ first try a fast path allocation and initialize the value with the\n+\/\/ inline klass's pack handler or we fall back to a runtime call.\n+void PhaseMacroExpand::expand_mh_intrinsic_return(CallStaticJavaNode* call) {\n+  assert(call->method()->is_method_handle_intrinsic(), \"must be a method handle intrinsic call\");\n+  Node* ret = call->proj_out_or_null(TypeFunc::Parms);\n+  if (ret == nullptr) {\n+    return;\n+  }\n+  const TypeFunc* tf = call->_tf;\n+  const TypeTuple* domain = OptoRuntime::store_inline_type_fields_Type()->domain_cc();\n+  const TypeFunc* new_tf = TypeFunc::make(tf->domain_sig(), tf->domain_cc(), tf->range_sig(), domain);\n+  call->_tf = new_tf;\n+  \/\/ Make sure the change of type is applied before projections are processed by igvn\n+  _igvn.set_type(call, call->Value(&_igvn));\n+  _igvn.set_type(ret, ret->Value(&_igvn));\n+\n+  \/\/ Before any new projection is added:\n+  CallProjections* projs = call->extract_projections(true, true);\n+\n+  \/\/ Create temporary hook nodes that will be replaced below.\n+  \/\/ Add an input to prevent hook nodes from being dead.\n+  Node* ctl = new Node(call);\n+  Node* mem = new Node(ctl);\n+  Node* io = new Node(ctl);\n+  Node* ex_ctl = new Node(ctl);\n+  Node* ex_mem = new Node(ctl);\n+  Node* ex_io = new Node(ctl);\n+  Node* res = new Node(ctl);\n+\n+  \/\/ Allocate a new buffered inline type only if a new one is not returned\n+  Node* cast = transform_later(new CastP2XNode(ctl, res));\n+  Node* mask = MakeConX(0x1);\n+  Node* masked = transform_later(new AndXNode(cast, mask));\n+  Node* cmp = transform_later(new CmpXNode(masked, mask));\n+  Node* bol = transform_later(new BoolNode(cmp, BoolTest::eq));\n+  IfNode* allocation_iff = new IfNode(ctl, bol, PROB_MAX, COUNT_UNKNOWN);\n+  transform_later(allocation_iff);\n+  Node* allocation_ctl = transform_later(new IfTrueNode(allocation_iff));\n+  Node* no_allocation_ctl = transform_later(new IfFalseNode(allocation_iff));\n+  Node* no_allocation_res = transform_later(new CheckCastPPNode(no_allocation_ctl, res, TypeInstPtr::BOTTOM));\n+\n+  \/\/ Try to allocate a new buffered inline instance either from TLAB or eden space\n+  Node* needgc_ctrl = nullptr; \/\/ needgc means slowcase, i.e. allocation failed\n+  CallLeafNoFPNode* handler_call;\n+  const bool alloc_in_place = UseTLAB;\n+  if (alloc_in_place) {\n+    Node* fast_oop_ctrl = nullptr;\n+    Node* fast_oop_rawmem = nullptr;\n+    Node* mask2 = MakeConX(-2);\n+    Node* masked2 = transform_later(new AndXNode(cast, mask2));\n+    Node* rawklassptr = transform_later(new CastX2PNode(masked2));\n+    Node* klass_node = transform_later(new CheckCastPPNode(allocation_ctl, rawklassptr, TypeInstKlassPtr::OBJECT_OR_NULL));\n+    Node* layout_val = make_load(nullptr, mem, klass_node, in_bytes(Klass::layout_helper_offset()), TypeInt::INT, T_INT);\n+    Node* size_in_bytes = ConvI2X(layout_val);\n+    BarrierSetC2* bs = BarrierSet::barrier_set()->barrier_set_c2();\n+    Node* fast_oop = bs->obj_allocate(this, mem, allocation_ctl, size_in_bytes, io, needgc_ctrl,\n+                                      fast_oop_ctrl, fast_oop_rawmem,\n+                                      AllocateInstancePrefetchLines);\n+    \/\/ Allocation succeed, initialize buffered inline instance header firstly,\n+    \/\/ and then initialize its fields with an inline class specific handler\n+    Node* mark_node = makecon(TypeRawPtr::make((address)markWord::inline_type_prototype().value()));\n+    fast_oop_rawmem = make_store(fast_oop_ctrl, fast_oop_rawmem, fast_oop, oopDesc::mark_offset_in_bytes(), mark_node, T_ADDRESS);\n+    fast_oop_rawmem = make_store(fast_oop_ctrl, fast_oop_rawmem, fast_oop, oopDesc::klass_offset_in_bytes(), klass_node, T_METADATA);\n+    if (UseCompressedClassPointers) {\n+      fast_oop_rawmem = make_store(fast_oop_ctrl, fast_oop_rawmem, fast_oop, oopDesc::klass_gap_offset_in_bytes(), intcon(0), T_INT);\n+    }\n+    Node* fixed_block  = make_load(fast_oop_ctrl, fast_oop_rawmem, klass_node, in_bytes(InstanceKlass::adr_inlineklass_fixed_block_offset()), TypeRawPtr::BOTTOM, T_ADDRESS);\n+    Node* pack_handler = make_load(fast_oop_ctrl, fast_oop_rawmem, fixed_block, in_bytes(InlineKlass::pack_handler_offset()), TypeRawPtr::BOTTOM, T_ADDRESS);\n+    handler_call = new CallLeafNoFPNode(OptoRuntime::pack_inline_type_Type(),\n+                                        nullptr,\n+                                        \"pack handler\",\n+                                        TypeRawPtr::BOTTOM);\n+    handler_call->init_req(TypeFunc::Control, fast_oop_ctrl);\n+    handler_call->init_req(TypeFunc::Memory, fast_oop_rawmem);\n+    handler_call->init_req(TypeFunc::I_O, top());\n+    handler_call->init_req(TypeFunc::FramePtr, call->in(TypeFunc::FramePtr));\n+    handler_call->init_req(TypeFunc::ReturnAdr, top());\n+    handler_call->init_req(TypeFunc::Parms, pack_handler);\n+    handler_call->init_req(TypeFunc::Parms+1, fast_oop);\n+  } else {\n+    needgc_ctrl = allocation_ctl;\n+  }\n+\n+  \/\/ Allocation failed, fall back to a runtime call\n+  CallStaticJavaNode* slow_call = new CallStaticJavaNode(OptoRuntime::store_inline_type_fields_Type(),\n+                                                         StubRoutines::store_inline_type_fields_to_buf(),\n+                                                         \"store_inline_type_fields\",\n+                                                         TypePtr::BOTTOM);\n+  slow_call->init_req(TypeFunc::Control, needgc_ctrl);\n+  slow_call->init_req(TypeFunc::Memory, mem);\n+  slow_call->init_req(TypeFunc::I_O, io);\n+  slow_call->init_req(TypeFunc::FramePtr, call->in(TypeFunc::FramePtr));\n+  slow_call->init_req(TypeFunc::ReturnAdr, call->in(TypeFunc::ReturnAdr));\n+  slow_call->init_req(TypeFunc::Parms, res);\n+\n+  Node* slow_ctl = transform_later(new ProjNode(slow_call, TypeFunc::Control));\n+  Node* slow_mem = transform_later(new ProjNode(slow_call, TypeFunc::Memory));\n+  Node* slow_io = transform_later(new ProjNode(slow_call, TypeFunc::I_O));\n+  Node* slow_res = transform_later(new ProjNode(slow_call, TypeFunc::Parms));\n+  Node* slow_catc = transform_later(new CatchNode(slow_ctl, slow_io, 2));\n+  Node* slow_norm = transform_later(new CatchProjNode(slow_catc, CatchProjNode::fall_through_index, CatchProjNode::no_handler_bci));\n+  Node* slow_excp = transform_later(new CatchProjNode(slow_catc, CatchProjNode::catch_all_index,    CatchProjNode::no_handler_bci));\n+\n+  Node* ex_r = new RegionNode(3);\n+  Node* ex_mem_phi = new PhiNode(ex_r, Type::MEMORY, TypePtr::BOTTOM);\n+  Node* ex_io_phi = new PhiNode(ex_r, Type::ABIO);\n+  ex_r->init_req(1, slow_excp);\n+  ex_mem_phi->init_req(1, slow_mem);\n+  ex_io_phi->init_req(1, slow_io);\n+  ex_r->init_req(2, ex_ctl);\n+  ex_mem_phi->init_req(2, ex_mem);\n+  ex_io_phi->init_req(2, ex_io);\n+  transform_later(ex_r);\n+  transform_later(ex_mem_phi);\n+  transform_later(ex_io_phi);\n+\n+  \/\/ We don't know how many values are returned. This assumes the\n+  \/\/ worst case, that all available registers are used.\n+  for (uint i = TypeFunc::Parms+1; i < domain->cnt(); i++) {\n+    if (domain->field_at(i) == Type::HALF) {\n+      slow_call->init_req(i, top());\n+      if (alloc_in_place) {\n+        handler_call->init_req(i+1, top());\n+      }\n+      continue;\n+    }\n+    Node* proj = transform_later(new ProjNode(call, i));\n+    slow_call->init_req(i, proj);\n+    if (alloc_in_place) {\n+      handler_call->init_req(i+1, proj);\n+    }\n+  }\n+  \/\/ We can safepoint at that new call\n+  slow_call->copy_call_debug_info(&_igvn, call);\n+  transform_later(slow_call);\n+  if (alloc_in_place) {\n+    transform_later(handler_call);\n+  }\n+\n+  Node* fast_ctl = nullptr;\n+  Node* fast_res = nullptr;\n+  MergeMemNode* fast_mem = nullptr;\n+  if (alloc_in_place) {\n+    fast_ctl = transform_later(new ProjNode(handler_call, TypeFunc::Control));\n+    Node* rawmem = transform_later(new ProjNode(handler_call, TypeFunc::Memory));\n+    fast_res = transform_later(new ProjNode(handler_call, TypeFunc::Parms));\n+    fast_mem = MergeMemNode::make(mem);\n+    fast_mem->set_memory_at(Compile::AliasIdxRaw, rawmem);\n+    transform_later(fast_mem);\n+  }\n+\n+  Node* r = new RegionNode(alloc_in_place ? 4 : 3);\n+  Node* mem_phi = new PhiNode(r, Type::MEMORY, TypePtr::BOTTOM);\n+  Node* io_phi = new PhiNode(r, Type::ABIO);\n+  Node* res_phi = new PhiNode(r, TypeInstPtr::BOTTOM);\n+  r->init_req(1, no_allocation_ctl);\n+  mem_phi->init_req(1, mem);\n+  io_phi->init_req(1, io);\n+  res_phi->init_req(1, no_allocation_res);\n+  r->init_req(2, slow_norm);\n+  mem_phi->init_req(2, slow_mem);\n+  io_phi->init_req(2, slow_io);\n+  res_phi->init_req(2, slow_res);\n+  if (alloc_in_place) {\n+    r->init_req(3, fast_ctl);\n+    mem_phi->init_req(3, fast_mem);\n+    io_phi->init_req(3, io);\n+    res_phi->init_req(3, fast_res);\n+  }\n+  transform_later(r);\n+  transform_later(mem_phi);\n+  transform_later(io_phi);\n+  transform_later(res_phi);\n+\n+  \/\/ Do not let stores that initialize this buffer be reordered with a subsequent\n+  \/\/ store that would make this buffer accessible by other threads.\n+  MemBarNode* mb = MemBarNode::make(C, Op_MemBarStoreStore, Compile::AliasIdxBot);\n+  transform_later(mb);\n+  mb->init_req(TypeFunc::Memory, mem_phi);\n+  mb->init_req(TypeFunc::Control, r);\n+  r = new ProjNode(mb, TypeFunc::Control);\n+  transform_later(r);\n+  mem_phi = new ProjNode(mb, TypeFunc::Memory);\n+  transform_later(mem_phi);\n+\n+  assert(projs->nb_resproj == 1, \"unexpected number of results\");\n+  _igvn.replace_in_uses(projs->fallthrough_catchproj, r);\n+  _igvn.replace_in_uses(projs->fallthrough_memproj, mem_phi);\n+  _igvn.replace_in_uses(projs->fallthrough_ioproj, io_phi);\n+  _igvn.replace_in_uses(projs->resproj[0], res_phi);\n+  _igvn.replace_in_uses(projs->catchall_catchproj, ex_r);\n+  _igvn.replace_in_uses(projs->catchall_memproj, ex_mem_phi);\n+  _igvn.replace_in_uses(projs->catchall_ioproj, ex_io_phi);\n+  \/\/ The CatchNode should not use the ex_io_phi. Re-connect it to the catchall_ioproj.\n+  Node* cn = projs->fallthrough_catchproj->in(0);\n+  _igvn.replace_input_of(cn, 1, projs->catchall_ioproj);\n+\n+  _igvn.replace_node(ctl, projs->fallthrough_catchproj);\n+  _igvn.replace_node(mem, projs->fallthrough_memproj);\n+  _igvn.replace_node(io, projs->fallthrough_ioproj);\n+  _igvn.replace_node(res, projs->resproj[0]);\n+  _igvn.replace_node(ex_ctl, projs->catchall_catchproj);\n+  _igvn.replace_node(ex_mem, projs->catchall_memproj);\n+  _igvn.replace_node(ex_io, projs->catchall_ioproj);\n+ }\n+\n@@ -2316,1 +2725,1 @@\n-      subklass = _igvn.transform(LoadKlassNode::make(_igvn, nullptr, C->immutable_memory(), k_adr, TypeInstPtr::KLASS));\n+      subklass = _igvn.transform(LoadKlassNode::make(_igvn, nullptr, C->immutable_memory(), k_adr, TypeInstPtr::KLASS, TypeInstKlassPtr::OBJECT));\n@@ -2328,0 +2737,113 @@\n+\/\/ FlatArrayCheckNode (array1 array2 ...) is expanded into:\n+\/\/\n+\/\/ long mark = array1.mark | array2.mark | ...;\n+\/\/ long locked_bit = markWord::unlocked_value & array1.mark & array2.mark & ...;\n+\/\/ if (locked_bit == 0) {\n+\/\/   \/\/ One array is locked, load prototype header from the klass\n+\/\/   mark = array1.klass.proto | array2.klass.proto | ...\n+\/\/ }\n+\/\/ if ((mark & markWord::flat_array_bit_in_place) == 0) {\n+\/\/    ...\n+\/\/ }\n+void PhaseMacroExpand::expand_flatarraycheck_node(FlatArrayCheckNode* check) {\n+  bool array_inputs = _igvn.type(check->in(FlatArrayCheckNode::ArrayOrKlass))->isa_oopptr() != nullptr;\n+  if (UseArrayMarkWordCheck && array_inputs) {\n+    Node* mark = MakeConX(0);\n+    Node* locked_bit = MakeConX(markWord::unlocked_value);\n+    Node* mem = check->in(FlatArrayCheckNode::Memory);\n+    for (uint i = FlatArrayCheckNode::ArrayOrKlass; i < check->req(); ++i) {\n+      Node* ary = check->in(i);\n+      const TypeOopPtr* t = _igvn.type(ary)->isa_oopptr();\n+      assert(t != nullptr, \"Mixing array and klass inputs\");\n+      assert(!t->is_flat() && !t->is_not_flat(), \"Should have been optimized out\");\n+      Node* mark_adr = basic_plus_adr(ary, oopDesc::mark_offset_in_bytes());\n+      Node* mark_load = _igvn.transform(LoadNode::make(_igvn, nullptr, mem, mark_adr, mark_adr->bottom_type()->is_ptr(), TypeX_X, TypeX_X->basic_type(), MemNode::unordered));\n+      mark = _igvn.transform(new OrXNode(mark, mark_load));\n+      locked_bit = _igvn.transform(new AndXNode(locked_bit, mark_load));\n+    }\n+    assert(!mark->is_Con(), \"Should have been optimized out\");\n+    Node* cmp = _igvn.transform(new CmpXNode(locked_bit, MakeConX(0)));\n+    Node* is_unlocked = _igvn.transform(new BoolNode(cmp, BoolTest::ne));\n+\n+    \/\/ BoolNode might be shared, replace each if user\n+    Node* old_bol = check->unique_out();\n+    assert(old_bol->is_Bool() && old_bol->as_Bool()->_test._test == BoolTest::ne, \"unexpected condition\");\n+    for (DUIterator_Last imin, i = old_bol->last_outs(imin); i >= imin; --i) {\n+      IfNode* old_iff = old_bol->last_out(i)->as_If();\n+      Node* ctrl = old_iff->in(0);\n+      RegionNode* region = new RegionNode(3);\n+      Node* mark_phi = new PhiNode(region, TypeX_X);\n+\n+      \/\/ Check if array is unlocked\n+      IfNode* iff = _igvn.transform(new IfNode(ctrl, is_unlocked, PROB_MAX, COUNT_UNKNOWN))->as_If();\n+\n+      \/\/ Unlocked: Use bits from mark word\n+      region->init_req(1, _igvn.transform(new IfTrueNode(iff)));\n+      mark_phi->init_req(1, mark);\n+\n+      \/\/ Locked: Load prototype header from klass\n+      ctrl = _igvn.transform(new IfFalseNode(iff));\n+      Node* proto = MakeConX(0);\n+      for (uint i = FlatArrayCheckNode::ArrayOrKlass; i < check->req(); ++i) {\n+        Node* ary = check->in(i);\n+        \/\/ Make loads control dependent to make sure they are only executed if array is locked\n+        Node* klass_adr = basic_plus_adr(ary, oopDesc::klass_offset_in_bytes());\n+        Node* klass = _igvn.transform(LoadKlassNode::make(_igvn, ctrl, C->immutable_memory(), klass_adr, TypeInstPtr::KLASS, TypeInstKlassPtr::OBJECT));\n+        Node* proto_adr = basic_plus_adr(klass, in_bytes(Klass::prototype_header_offset()));\n+        Node* proto_load = _igvn.transform(LoadNode::make(_igvn, ctrl, C->immutable_memory(), proto_adr, proto_adr->bottom_type()->is_ptr(), TypeX_X, TypeX_X->basic_type(), MemNode::unordered));\n+        proto = _igvn.transform(new OrXNode(proto, proto_load));\n+      }\n+      region->init_req(2, ctrl);\n+      mark_phi->init_req(2, proto);\n+\n+      \/\/ Check if flat array bits are set\n+      Node* mask = MakeConX(markWord::flat_array_bit_in_place);\n+      Node* masked = _igvn.transform(new AndXNode(_igvn.transform(mark_phi), mask));\n+      cmp = _igvn.transform(new CmpXNode(masked, MakeConX(0)));\n+      Node* is_not_flat = _igvn.transform(new BoolNode(cmp, BoolTest::eq));\n+\n+      ctrl = _igvn.transform(region);\n+      iff = _igvn.transform(new IfNode(ctrl, is_not_flat, PROB_MAX, COUNT_UNKNOWN))->as_If();\n+      _igvn.replace_node(old_iff, iff);\n+    }\n+    _igvn.replace_node(check, C->top());\n+  } else {\n+    \/\/ Fall back to layout helper check\n+    Node* lhs = intcon(0);\n+    for (uint i = FlatArrayCheckNode::ArrayOrKlass; i < check->req(); ++i) {\n+      Node* array_or_klass = check->in(i);\n+      Node* klass = nullptr;\n+      const TypePtr* t = _igvn.type(array_or_klass)->is_ptr();\n+      assert(!t->is_flat() && !t->is_not_flat(), \"Should have been optimized out\");\n+      if (t->isa_oopptr() != nullptr) {\n+        Node* klass_adr = basic_plus_adr(array_or_klass, oopDesc::klass_offset_in_bytes());\n+        klass = transform_later(LoadKlassNode::make(_igvn, nullptr, C->immutable_memory(), klass_adr, TypeInstPtr::KLASS, TypeInstKlassPtr::OBJECT));\n+      } else {\n+        assert(t->isa_klassptr(), \"Unexpected input type\");\n+        klass = array_or_klass;\n+      }\n+      Node* lh_addr = basic_plus_adr(klass, in_bytes(Klass::layout_helper_offset()));\n+      Node* lh_val = _igvn.transform(LoadNode::make(_igvn, nullptr, C->immutable_memory(), lh_addr, lh_addr->bottom_type()->is_ptr(), TypeInt::INT, T_INT, MemNode::unordered));\n+      lhs = _igvn.transform(new OrINode(lhs, lh_val));\n+    }\n+    Node* masked = transform_later(new AndINode(lhs, intcon(Klass::_lh_array_tag_flat_value_bit_inplace)));\n+    Node* cmp = transform_later(new CmpINode(masked, intcon(0)));\n+    Node* bol = transform_later(new BoolNode(cmp, BoolTest::eq));\n+    Node* m2b = transform_later(new Conv2BNode(masked));\n+    \/\/ The matcher expects the input to If nodes to be produced by a Bool(CmpI..)\n+    \/\/ pattern, but the input to other potential users (e.g. Phi) to be some\n+    \/\/ other pattern (e.g. a Conv2B node, possibly idealized as a CMoveI).\n+    Node* old_bol = check->unique_out();\n+    for (DUIterator_Last imin, i = old_bol->last_outs(imin); i >= imin; --i) {\n+      Node* user = old_bol->last_out(i);\n+      for (uint j = 0; j < user->req(); j++) {\n+        Node* n = user->in(j);\n+        if (n == old_bol) {\n+          _igvn.replace_input_of(user, j, user->is_If() ? bol : m2b);\n+        }\n+      }\n+    }\n+    _igvn.replace_node(check, C->top());\n+  }\n+}\n+\n@@ -2388,2 +2910,5 @@\n-      case Node::Class_CallStaticJava:\n-        success = eliminate_boxing_node(n->as_CallStaticJava());\n+      case Node::Class_CallStaticJava: {\n+        CallStaticJavaNode* call = n->as_CallStaticJava();\n+        if (!call->method()->is_method_handle_intrinsic()) {\n+          success = eliminate_boxing_node(n->as_CallStaticJava());\n+        }\n@@ -2391,0 +2916,1 @@\n+      }\n@@ -2404,0 +2930,2 @@\n+      case Node::Class_FlatArrayCheck:\n+        break;\n@@ -2446,4 +2974,7 @@\n-        \/\/ Remove it from macro list and put on IGVN worklist to optimize.\n-        C->remove_macro_node(n);\n-        _igvn._worklist.push(n);\n-        success = true;\n+        CallStaticJavaNode* call = n->as_CallStaticJava();\n+        if (!call->method()->is_method_handle_intrinsic()) {\n+          \/\/ Remove it from macro list and put on IGVN worklist to optimize.\n+          C->remove_macro_node(n);\n+          _igvn._worklist.push(n);\n+          success = true;\n+        }\n@@ -2564,0 +3095,7 @@\n+    case Node::Class_CallStaticJava:\n+      expand_mh_intrinsic_return(n->as_CallStaticJava());\n+      C->remove_macro_node(n);\n+      break;\n+    case Node::Class_FlatArrayCheck:\n+      expand_flatarraycheck_node(n->as_FlatArrayCheck());\n+      break;\n","filename":"src\/hotspot\/share\/opto\/macro.cpp","additions":719,"deletions":181,"binary":false,"changes":900,"status":"modified"},{"patch":"@@ -26,0 +26,1 @@\n+#include \"ci\/ciFlatArrayKlass.hpp\"\n@@ -142,1 +143,1 @@\n-inline Node* PhaseMacroExpand::generate_slow_guard(Node** ctrl, Node* test, RegionNode* region) {\n+Node* PhaseMacroExpand::generate_slow_guard(Node** ctrl, Node* test, RegionNode* region) {\n@@ -146,0 +147,4 @@\n+inline Node* PhaseMacroExpand::generate_fair_guard(Node** ctrl, Node* test, RegionNode* region) {\n+  return generate_guard(ctrl, test, region, PROB_FAIR);\n+}\n+\n@@ -286,0 +291,20 @@\n+Node* PhaseMacroExpand::array_lh_test(Node* array, jint mask) {\n+  Node* klass_adr = basic_plus_adr(array, oopDesc::klass_offset_in_bytes());\n+  Node* klass = transform_later(LoadKlassNode::make(_igvn, nullptr, C->immutable_memory(), klass_adr, TypeInstPtr::KLASS, TypeInstKlassPtr::OBJECT));\n+  Node* lh_addr = basic_plus_adr(klass, in_bytes(Klass::layout_helper_offset()));\n+  Node* lh_val = _igvn.transform(LoadNode::make(_igvn, nullptr, C->immutable_memory(), lh_addr, lh_addr->bottom_type()->is_ptr(), TypeInt::INT, T_INT, MemNode::unordered));\n+  Node* masked = transform_later(new AndINode(lh_val, intcon(mask)));\n+  Node* cmp = transform_later(new CmpINode(masked, intcon(0)));\n+  return transform_later(new BoolNode(cmp, BoolTest::ne));\n+}\n+\n+Node* PhaseMacroExpand::generate_flat_array_guard(Node** ctrl, Node* array, RegionNode* region) {\n+  assert(UseFlatArray, \"can never be flat\");\n+  return generate_fair_guard(ctrl, array_lh_test(array, Klass::_lh_array_tag_flat_value_bit_inplace), region);\n+}\n+\n+Node* PhaseMacroExpand::generate_null_free_array_guard(Node** ctrl, Node* array, RegionNode* region) {\n+  assert(EnableValhalla, \"can never be null free\");\n+  return generate_fair_guard(ctrl, array_lh_test(array, Klass::_lh_null_free_array_bit_inplace), region);\n+}\n+\n@@ -380,0 +405,1 @@\n+                                           Node* dest_length,\n@@ -391,0 +417,2 @@\n+  Node* default_value = nullptr;\n+  Node* raw_default_value = nullptr;\n@@ -421,0 +449,2 @@\n+      default_value = alloc->in(AllocateNode::DefaultValue);\n+      raw_default_value = alloc->in(AllocateNode::RawDefaultValue);\n@@ -490,1 +520,0 @@\n-      Node* dest_length = alloc->in(AllocateNode::ALength);\n@@ -497,1 +526,3 @@\n-                             adr_type, dest, basic_elem_type,\n+                             adr_type, dest,\n+                             default_value, raw_default_value,\n+                             basic_elem_type,\n@@ -528,1 +559,0 @@\n-    Node* dest_length = alloc->in(AllocateNode::ALength);\n@@ -534,1 +564,3 @@\n-                           adr_type, dest, basic_elem_type,\n+                           adr_type, dest,\n+                           default_value, raw_default_value,\n+                           basic_elem_type,\n@@ -583,1 +615,3 @@\n-                             adr_type, dest, basic_elem_type,\n+                             adr_type, dest,\n+                             default_value, raw_default_value,\n+                             basic_elem_type,\n@@ -593,1 +627,3 @@\n-                             adr_type, dest, basic_elem_type,\n+                             adr_type, dest,\n+                             default_value, raw_default_value,\n+                             basic_elem_type,\n@@ -771,1 +807,3 @@\n-                           adr_type, dest, basic_elem_type,\n+                           adr_type, dest,\n+                           default_value, raw_default_value,\n+                           basic_elem_type,\n@@ -833,3 +871,3 @@\n-  _igvn.replace_node(_callprojs.fallthrough_memproj, out_mem);\n-  if (_callprojs.fallthrough_ioproj != nullptr) {\n-    _igvn.replace_node(_callprojs.fallthrough_ioproj, *io);\n+  _igvn.replace_node(_callprojs->fallthrough_memproj, out_mem);\n+  if (_callprojs->fallthrough_ioproj != nullptr) {\n+    _igvn.replace_node(_callprojs->fallthrough_ioproj, *io);\n@@ -837,1 +875,1 @@\n-  _igvn.replace_node(_callprojs.fallthrough_catchproj, *ctrl);\n+  _igvn.replace_node(_callprojs->fallthrough_catchproj, *ctrl);\n@@ -877,0 +915,2 @@\n+                                            Node* val,\n+                                            Node* raw_val,\n@@ -915,1 +955,1 @@\n-    mem = ClearArrayNode::clear_memory(ctrl, mem, dest,\n+    mem = ClearArrayNode::clear_memory(ctrl, mem, dest, val, raw_val,\n@@ -920,1 +960,1 @@\n-    mem = ClearArrayNode::clear_memory(ctrl, mem, dest,\n+    mem = ClearArrayNode::clear_memory(ctrl, mem, dest, val, raw_val,\n@@ -933,1 +973,1 @@\n-    mem = ClearArrayNode::clear_memory(ctrl, mem, dest,\n+    mem = ClearArrayNode::clear_memory(ctrl, mem, dest, val, raw_val,\n@@ -962,1 +1002,7 @@\n-        mem = StoreNode::make(_igvn, ctrl, mem, p1, adr_type, intcon(0), T_INT, MemNode::unordered);\n+        if (val == nullptr) {\n+          assert(raw_val == nullptr, \"val may not be null\");\n+          mem = StoreNode::make(_igvn, ctrl, mem, p1, adr_type, intcon(0), T_INT, MemNode::unordered);\n+        } else {\n+          assert(_igvn.type(val)->isa_narrowoop(), \"should be narrow oop\");\n+          mem = new StoreNNode(ctrl, mem, p1, adr_type, val, MemNode::unordered);\n+        }\n@@ -967,1 +1013,1 @@\n-    mem = ClearArrayNode::clear_memory(ctrl, mem, dest,\n+    mem = ClearArrayNode::clear_memory(ctrl, mem, dest, raw_val,\n@@ -1083,2 +1129,2 @@\n-  call->extract_projections(&_callprojs, false \/*separate_io_proj*\/, false \/*do_asserts*\/);\n-  *ctrl = _callprojs.fallthrough_catchproj->clone();\n+  _callprojs = call->extract_projections(false \/*separate_io_proj*\/, false \/*do_asserts*\/);\n+  *ctrl = _callprojs->fallthrough_catchproj->clone();\n@@ -1087,1 +1133,1 @@\n-  Node* m = _callprojs.fallthrough_memproj->clone();\n+  Node* m = _callprojs->fallthrough_memproj->clone();\n@@ -1101,3 +1147,3 @@\n-  \/\/ could be null. Skip clone and update null fallthrough_ioproj.\n-  if (_callprojs.fallthrough_ioproj != nullptr) {\n-    *io = _callprojs.fallthrough_ioproj->clone();\n+  \/\/ could be nullptr. Skip clone and update nullptr fallthrough_ioproj.\n+  if (_callprojs->fallthrough_ioproj != nullptr) {\n+    *io = _callprojs->fallthrough_ioproj->clone();\n@@ -1235,0 +1281,36 @@\n+const TypePtr* PhaseMacroExpand::adjust_for_flat_array(const TypeAryPtr* top_dest, Node*& src_offset,\n+                                                       Node*& dest_offset, Node*& length, BasicType& dest_elem,\n+                                                       Node*& dest_length) {\n+#ifdef ASSERT\n+  BarrierSetC2* bs = BarrierSet::barrier_set()->barrier_set_c2();\n+  bool needs_barriers = top_dest->elem()->inline_klass()->contains_oops() &&\n+    bs->array_copy_requires_gc_barriers(dest_length != nullptr, T_OBJECT, false, false, BarrierSetC2::Optimization);\n+  assert(!needs_barriers || StressReflectiveCode, \"Flat arracopy would require GC barriers\");\n+#endif\n+  int elem_size = top_dest->flat_elem_size();\n+  if (elem_size >= 8) {\n+    if (elem_size > 8) {\n+      \/\/ treat as array of long but scale length, src offset and dest offset\n+      assert((elem_size % 8) == 0, \"not a power of 2?\");\n+      int factor = elem_size \/ 8;\n+      length = transform_later(new MulINode(length, intcon(factor)));\n+      src_offset = transform_later(new MulINode(src_offset, intcon(factor)));\n+      dest_offset = transform_later(new MulINode(dest_offset, intcon(factor)));\n+      if (dest_length != nullptr) {\n+        dest_length = transform_later(new MulINode(dest_length, intcon(factor)));\n+      }\n+      elem_size = 8;\n+    }\n+    dest_elem = T_LONG;\n+  } else if (elem_size == 4) {\n+    dest_elem = T_INT;\n+  } else if (elem_size == 2) {\n+    dest_elem = T_CHAR;\n+  } else if (elem_size == 1) {\n+    dest_elem = T_BYTE;\n+  } else {\n+    ShouldNotReachHere();\n+  }\n+  return TypeRawPtr::BOTTOM;\n+}\n+\n@@ -1252,3 +1334,14 @@\n-    Node* mem = ac->in(TypeFunc::Memory);\n-    merge_mem = MergeMemNode::make(mem);\n-    transform_later(merge_mem);\n+    const Type* src_type = _igvn.type(src);\n+    const Type* dest_type = _igvn.type(dest);\n+    const TypeAryPtr* top_src = src_type->isa_aryptr();\n+    const TypeAryPtr* top_dest = dest_type->isa_aryptr();\n+    BasicType dest_elem = T_OBJECT;\n+    if (top_dest != nullptr && top_dest->elem() != Type::BOTTOM) {\n+      dest_elem = top_dest->elem()->array_element_basic_type();\n+    }\n+    if (is_reference_type(dest_elem, true)) dest_elem = T_OBJECT;\n+\n+    if (top_src != nullptr && top_src->is_flat()) {\n+      \/\/ If src is flat, dest is guaranteed to be flat as well\n+      top_dest = top_src;\n+    }\n@@ -1257,0 +1350,1 @@\n+    Node* dest_length = nullptr;\n@@ -1260,0 +1354,1 @@\n+      dest_length = alloc->in(AllocateNode::ALength);\n@@ -1262,3 +1357,16 @@\n-    const TypePtr* adr_type = _igvn.type(dest)->is_oopptr()->add_offset(Type::OffsetBot);\n-    if (ac->_dest_type != TypeOopPtr::BOTTOM) {\n-      adr_type = ac->_dest_type->add_offset(Type::OffsetBot)->is_ptr();\n+    Node* mem = ac->in(TypeFunc::Memory);\n+    const TypePtr* adr_type = nullptr;\n+    if (top_dest->is_flat()) {\n+      assert(dest_length != nullptr || StressReflectiveCode, \"must be tightly coupled\");\n+      \/\/ Copy to a flat array modifies multiple memory slices. Conservatively insert a barrier\n+      \/\/ on all slices to prevent writes into the source from floating below the arraycopy.\n+      insert_mem_bar(&ctrl, &mem, Op_MemBarCPUOrder);\n+      adr_type = adjust_for_flat_array(top_dest, src_offset, dest_offset, length, dest_elem, dest_length);\n+    } else {\n+      adr_type = dest_type->is_oopptr()->add_offset(Type::OffsetBot);\n+      if (ac->_dest_type != TypeOopPtr::BOTTOM) {\n+        adr_type = ac->_dest_type->add_offset(Type::OffsetBot)->is_ptr();\n+      }\n+      if (ac->_src_type != ac->_dest_type) {\n+        adr_type = TypeRawPtr::BOTTOM;\n+      }\n@@ -1266,0 +1374,3 @@\n+    merge_mem = MergeMemNode::make(mem);\n+    transform_later(merge_mem);\n+\n@@ -1267,1 +1378,1 @@\n-                       adr_type, T_OBJECT,\n+                       adr_type, dest_elem,\n@@ -1269,0 +1380,1 @@\n+                       dest_length,\n@@ -1270,1 +1382,0 @@\n-\n@@ -1304,3 +1415,1 @@\n-  if (ac->is_arraycopy_validated() &&\n-      dest_elem != T_CONFLICT &&\n-      src_elem == T_CONFLICT) {\n+  if (ac->is_arraycopy_validated() && dest_elem != T_CONFLICT && src_elem == T_CONFLICT) {\n@@ -1325,0 +1434,1 @@\n+                                   nullptr,\n@@ -1335,1 +1445,8 @@\n-  if (src_elem != dest_elem || dest_elem == T_VOID) {\n+  \/\/\n+  \/\/ We have no stub to copy flat inline type arrays with oop\n+  \/\/ fields if we need to emit write barriers.\n+  \/\/\n+  BarrierSetC2* bs = BarrierSet::barrier_set()->barrier_set_c2();\n+  if (src_elem != dest_elem || top_src->is_flat() != top_dest->is_flat() || dest_elem == T_VOID ||\n+      (top_src->is_flat() && top_dest->elem()->inline_klass()->contains_oops() &&\n+       bs->array_copy_requires_gc_barriers(alloc != nullptr, T_OBJECT, false, false, BarrierSetC2::Optimization))) {\n@@ -1343,3 +1460,3 @@\n-    _igvn.replace_node(_callprojs.fallthrough_memproj, merge_mem);\n-    if (_callprojs.fallthrough_ioproj != nullptr) {\n-      _igvn.replace_node(_callprojs.fallthrough_ioproj, io);\n+    _igvn.replace_node(_callprojs->fallthrough_memproj, merge_mem);\n+    if (_callprojs->fallthrough_ioproj != nullptr) {\n+      _igvn.replace_node(_callprojs->fallthrough_ioproj, io);\n@@ -1347,1 +1464,1 @@\n-    _igvn.replace_node(_callprojs.fallthrough_catchproj, ctrl);\n+    _igvn.replace_node(_callprojs->fallthrough_catchproj, ctrl);\n@@ -1364,4 +1481,5 @@\n-  {\n-    Node* mem = ac->in(TypeFunc::Memory);\n-    merge_mem = MergeMemNode::make(mem);\n-    transform_later(merge_mem);\n+  Node* mem = ac->in(TypeFunc::Memory);\n+  if (top_dest->is_flat()) {\n+    \/\/ Copy to a flat array modifies multiple memory slices. Conservatively insert a barrier\n+    \/\/ on all slices to prevent writes into the source from floating below the arraycopy.\n+    insert_mem_bar(&ctrl, &mem, Op_MemBarCPUOrder);\n@@ -1369,0 +1487,2 @@\n+  merge_mem = MergeMemNode::make(mem);\n+  transform_later(merge_mem);\n@@ -1409,0 +1529,15 @@\n+\n+    \/\/ Handle inline type arrays\n+    if (!top_src->is_flat()) {\n+      if (UseFlatArray && !top_src->is_not_flat()) {\n+        \/\/ Src might be flat and dest might not be flat. Go to the slow path if src is flat.\n+        generate_flat_array_guard(&ctrl, src, slow_region);\n+      }\n+      if (EnableValhalla) {\n+        \/\/ No validation. The subtype check emitted at macro expansion time will not go to the slow\n+        \/\/ path but call checkcast_arraycopy which can not handle flat\/null-free inline type arrays.\n+        generate_null_free_array_guard(&ctrl, dest, slow_region);\n+      }\n+    } else {\n+      assert(top_dest->is_flat(), \"dest array must be flat\");\n+    }\n@@ -1410,0 +1545,1 @@\n+\n@@ -1412,1 +1548,5 @@\n-  if (ac->_dest_type != TypeOopPtr::BOTTOM) {\n+  Node* dest_length = (alloc != nullptr) ? alloc->in(AllocateNode::ALength) : nullptr;\n+\n+  if (top_dest->is_flat()) {\n+    adr_type = adjust_for_flat_array(top_dest, src_offset, dest_offset, length, dest_elem, dest_length);\n+  } else if (ac->_dest_type != TypeOopPtr::BOTTOM) {\n@@ -1421,0 +1561,1 @@\n+                     dest_length,\n@@ -1423,1 +1564,2 @@\n-                     false, ac->has_negative_length_guard(), slow_region);\n+                     false, ac->has_negative_length_guard(),\n+                     slow_region);\n","filename":"src\/hotspot\/share\/opto\/macroArrayCopy.cpp","additions":187,"deletions":45,"binary":false,"changes":232,"status":"modified"},{"patch":"@@ -50,0 +50,21 @@\n+  \/\/ FIXME: shouldn't this be encoded in helper methods of the type system (maybe_java_subtype_of() etc.?)\n+  \/\/ Similar to logic in CmpPNode::sub()\n+  bool unrelated_classes = false;\n+  \/\/ Handle inline type arrays\n+  if (subk->flat_in_array() && superk->not_flat_in_array()) {\n+    \/\/ The subtype is in flat arrays and the supertype is not in flat arrays. Must be unrelated.\n+    unrelated_classes = true;\n+  } else if (subk->is_not_flat() && superk->is_flat()) {\n+    \/\/ The subtype is a non-flat array and the supertype is a flat array. Must be unrelated.\n+    unrelated_classes = true;\n+  } else if (subk->is_not_null_free() && superk->is_null_free()) {\n+    \/\/ The subtype is a nullable array and the supertype is null-free array. Must be unrelated.\n+    unrelated_classes = true;\n+  }\n+  if (unrelated_classes) {\n+    TypePtr::PTR jp = sub_t->is_ptr()->join_ptr(super_t->is_ptr()->_ptr);\n+    if (jp != TypePtr::Null && jp != TypePtr::BotPTR) {\n+      return TypeInt::CC_GT;\n+    }\n+  }\n+\n","filename":"src\/hotspot\/share\/opto\/subtypenode.cpp","additions":21,"deletions":0,"binary":false,"changes":21,"status":"modified"},{"patch":"@@ -59,0 +59,1 @@\n+#include \"oops\/flatArrayKlass.hpp\"\n@@ -626,2 +627,22 @@\n-  return handle == nullptr ? 0 :\n-         checked_cast<jint>(ObjectSynchronizer::FastHashCode (THREAD, JNIHandles::resolve_non_null(handle)));\n+  if (handle == nullptr) {\n+    return 0;\n+  }\n+  oop obj = JNIHandles::resolve_non_null(handle);\n+  if (EnableValhalla && obj->klass()->is_inline_klass()) {\n+      JavaValue result(T_INT);\n+      JavaCallArguments args;\n+      Handle ho(THREAD, obj);\n+      args.push_oop(ho);\n+      methodHandle method(THREAD, Universe::value_object_hash_code_method());\n+      JavaCalls::call(&result, method, &args, THREAD);\n+      if (HAS_PENDING_EXCEPTION) {\n+        if (!PENDING_EXCEPTION->is_a(vmClasses::Error_klass())) {\n+          Handle e(THREAD, PENDING_EXCEPTION);\n+          CLEAR_PENDING_EXCEPTION;\n+          THROW_MSG_CAUSE_(vmSymbols::java_lang_InternalError(), \"Internal error in hashCode\", e, false);\n+        }\n+      }\n+      return result.get_jint();\n+  } else {\n+    return checked_cast<jint>(ObjectSynchronizer::FastHashCode(THREAD, obj));\n+  }\n@@ -679,0 +700,1 @@\n+       klass->is_inline_klass() ||\n@@ -1195,1 +1217,2 @@\n-    size = InstanceKlass::cast(klass)->local_interfaces()->length();\n+    InstanceKlass* ik = InstanceKlass::cast(klass);\n+    size = ik->local_interfaces()->length();\n@@ -1197,1 +1220,1 @@\n-    assert(klass->is_objArray_klass() || klass->is_typeArray_klass(), \"Illegal mirror klass\");\n+    assert(klass->is_objArray_klass() || klass->is_typeArray_klass() || klass->is_flatArray_klass(), \"Illegal mirror klass\");\n@@ -1208,1 +1231,2 @@\n-      Klass* k = InstanceKlass::cast(klass)->local_interfaces()->at(index);\n+      InstanceKlass* ik = InstanceKlass::cast(klass);\n+      Klass* k = ik->local_interfaces()->at(index);\n@@ -1243,0 +1267,13 @@\n+JVM_ENTRY(jboolean, JVM_IsIdentityClass(JNIEnv *env, jclass cls))\n+  oop mirror = JNIHandles::resolve_non_null(cls);\n+  if (java_lang_Class::is_primitive(mirror)) {\n+    return JNI_FALSE;\n+  }\n+  Klass* k = java_lang_Class::as_Klass(mirror);\n+  if (EnableValhalla) {\n+    return k->is_array_klass() || k->is_identity_class();\n+  } else {\n+    return k->is_interface() ? JNI_FALSE : JNI_TRUE;\n+  }\n+JVM_END\n+\n@@ -1857,0 +1894,2 @@\n+  bool is_ctor = (method->is_object_constructor() ||\n+                  method->is_static_vnew_factory());\n@@ -1858,1 +1897,1 @@\n-    return (method->is_initializer() && !method->is_static());\n+    return is_ctor;\n@@ -1860,1 +1899,3 @@\n-    return  (!method->is_initializer() && !method->is_overpass());\n+    return (!is_ctor &&\n+            !method->is_class_initializer() &&\n+            !method->is_overpass());\n@@ -1923,0 +1964,2 @@\n+        assert(method->is_object_constructor() ||\n+               method->is_static_vnew_factory(), \"must be\");\n@@ -2205,3 +2248,1 @@\n-  if (!m->is_initializer() || m->is_static()) {\n-    method = Reflection::new_method(m, true, CHECK_NULL);\n-  } else {\n+  if (m->is_object_constructor() || m->is_static_vnew_factory()) {\n@@ -2209,0 +2250,2 @@\n+  } else {\n+    method = Reflection::new_method(m, true, CHECK_NULL);\n@@ -2479,0 +2522,37 @@\n+\/\/ Arrays support \/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n+\n+JVM_ENTRY(jboolean, JVM_ArrayIsAccessAtomic(JNIEnv *env, jclass unused, jobject array))\n+  oop o = JNIHandles::resolve(array);\n+  Klass* k = o->klass();\n+  if ((o == nullptr) || (!k->is_array_klass())) {\n+    THROW_0(vmSymbols::java_lang_IllegalArgumentException());\n+  }\n+  return ArrayKlass::cast(k)->element_access_is_atomic();\n+JVM_END\n+\n+JVM_ENTRY(jobject, JVM_ArrayEnsureAccessAtomic(JNIEnv *env, jclass unused, jobject array))\n+  oop o = JNIHandles::resolve(array);\n+  Klass* k = o->klass();\n+  if ((o == nullptr) || (!k->is_array_klass())) {\n+    THROW_0(vmSymbols::java_lang_IllegalArgumentException());\n+  }\n+  if (k->is_flatArray_klass()) {\n+    FlatArrayKlass* vk = FlatArrayKlass::cast(k);\n+    if (!vk->element_access_is_atomic()) {\n+      \/**\n+       * Need to decide how to implement:\n+       *\n+       * 1) Change to objArrayOop layout, therefore oop->klass() differs so\n+       * then \"<atomic>[Qfoo;\" klass needs to subclass \"[Qfoo;\" to pass through\n+       * \"checkcast\" & \"instanceof\"\n+       *\n+       * 2) Use extra header in the flatArrayOop to flag atomicity required and\n+       * possibly per instance lock structure. Said info, could be placed in\n+       * \"trailer\" rather than disturb the current arrayOop\n+       *\/\n+      Unimplemented();\n+    }\n+  }\n+  return array;\n+JVM_END\n+\n@@ -2641,1 +2721,1 @@\n-  return method->name() == vmSymbols::object_initializer_name();\n+  return method->is_object_constructor();\n@@ -3479,0 +3559,4 @@\n+JVM_LEAF(jboolean, JVM_IsValhallaEnabled(void))\n+  return EnableValhalla ? JNI_TRUE : JNI_FALSE;\n+JVM_END\n+\n@@ -3560,1 +3644,1 @@\n-    objArrayHandle args(THREAD, objArrayOop(JNIHandles::resolve(args0)));\n+    objArrayHandle args = oopFactory::ensure_objArray(JNIHandles::resolve(args0), CHECK_NULL);\n@@ -3580,0 +3664,1 @@\n+  objArrayHandle args = oopFactory::ensure_objArray(JNIHandles::resolve(args0), CHECK_NULL);\n@@ -3581,1 +3666,0 @@\n-  objArrayHandle args(THREAD, objArrayOop(JNIHandles::resolve(args0)));\n","filename":"src\/hotspot\/share\/prims\/jvm.cpp","additions":97,"deletions":13,"binary":false,"changes":110,"status":"modified"},{"patch":"@@ -2353,1 +2353,1 @@\n-  if (sig_type == JVM_SIGNATURE_CLASS) {\n+  if (sig_type == JVM_SIGNATURE_CLASS || sig_type == JVM_SIGNATURE_PRIMITIVE_OBJECT) {\n","filename":"src\/hotspot\/share\/prims\/jvmtiExport.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -134,6 +134,6 @@\n-  IS_METHOD            = java_lang_invoke_MemberName::MN_IS_METHOD,\n-  IS_CONSTRUCTOR       = java_lang_invoke_MemberName::MN_IS_CONSTRUCTOR,\n-  IS_FIELD             = java_lang_invoke_MemberName::MN_IS_FIELD,\n-  IS_TYPE              = java_lang_invoke_MemberName::MN_IS_TYPE,\n-  CALLER_SENSITIVE     = java_lang_invoke_MemberName::MN_CALLER_SENSITIVE,\n-  TRUSTED_FINAL        = java_lang_invoke_MemberName::MN_TRUSTED_FINAL,\n+  IS_METHOD             = java_lang_invoke_MemberName::MN_IS_METHOD,\n+  IS_OBJECT_CONSTRUCTOR = java_lang_invoke_MemberName::MN_IS_OBJECT_CONSTRUCTOR,\n+  IS_FIELD              = java_lang_invoke_MemberName::MN_IS_FIELD,\n+  IS_TYPE               = java_lang_invoke_MemberName::MN_IS_TYPE,\n+  CALLER_SENSITIVE      = java_lang_invoke_MemberName::MN_CALLER_SENSITIVE,\n+  TRUSTED_FINAL         = java_lang_invoke_MemberName::MN_TRUSTED_FINAL,\n@@ -141,6 +141,7 @@\n-  REFERENCE_KIND_SHIFT = java_lang_invoke_MemberName::MN_REFERENCE_KIND_SHIFT,\n-  REFERENCE_KIND_MASK  = java_lang_invoke_MemberName::MN_REFERENCE_KIND_MASK,\n-  LM_UNCONDITIONAL     = java_lang_invoke_MemberName::MN_UNCONDITIONAL_MODE,\n-  LM_MODULE            = java_lang_invoke_MemberName::MN_MODULE_MODE,\n-  LM_TRUSTED           = java_lang_invoke_MemberName::MN_TRUSTED_MODE,\n-  ALL_KINDS      = IS_METHOD | IS_CONSTRUCTOR | IS_FIELD | IS_TYPE\n+  FLATTENED             = java_lang_invoke_MemberName::MN_FLAT_FIELD,\n+  REFERENCE_KIND_SHIFT  = java_lang_invoke_MemberName::MN_REFERENCE_KIND_SHIFT,\n+  REFERENCE_KIND_MASK   = java_lang_invoke_MemberName::MN_REFERENCE_KIND_MASK,\n+  LM_UNCONDITIONAL      = java_lang_invoke_MemberName::MN_UNCONDITIONAL_MODE,\n+  LM_MODULE             = java_lang_invoke_MemberName::MN_MODULE_MODE,\n+  LM_TRUSTED            = java_lang_invoke_MemberName::MN_TRUSTED_MODE,\n+  ALL_KINDS      = IS_METHOD | IS_OBJECT_CONSTRUCTOR | IS_FIELD | IS_TYPE\n@@ -157,1 +158,1 @@\n-    flags |= IS_CONSTRUCTOR;\n+    flags |= IS_OBJECT_CONSTRUCTOR;\n@@ -176,1 +177,1 @@\n-    case IS_CONSTRUCTOR:\n+    case IS_OBJECT_CONSTRUCTOR:\n@@ -318,2 +319,2 @@\n-    } else if (m->is_initializer()) {\n-      flags |= IS_CONSTRUCTOR | (JVM_REF_invokeSpecial << REFERENCE_KIND_SHIFT);\n+    } else if (m->is_object_constructor()) {\n+      flags |= IS_OBJECT_CONSTRUCTOR | (JVM_REF_invokeSpecial << REFERENCE_KIND_SHIFT);\n@@ -358,0 +359,1 @@\n+  if (fd.is_flat()) flags |= FLATTENED;;\n@@ -811,1 +813,1 @@\n-  case IS_CONSTRUCTOR:\n+  case IS_OBJECT_CONSTRUCTOR:\n@@ -817,1 +819,1 @@\n-        if (name == vmSymbols::object_initializer_name()) {\n+        if (name == vmSymbols::object_initializer_name() && type->is_void_method_signature()) {\n@@ -819,0 +821,2 @@\n+        } else if (name == vmSymbols::inline_factory_name()) {\n+          LinkResolver::resolve_static_call(result, link_info, false, THREAD);\n@@ -880,1 +884,1 @@\n-  case IS_CONSTRUCTOR:\n+  case IS_OBJECT_CONSTRUCTOR:\n@@ -1002,1 +1006,1 @@\n-    template(java_lang_invoke_MemberName,MN_IS_CONSTRUCTOR) \\\n+    template(java_lang_invoke_MemberName,MN_IS_OBJECT_CONSTRUCTOR) \\\n@@ -1008,0 +1012,1 @@\n+    template(java_lang_invoke_MemberName,MN_FLAT_FIELD) \\\n@@ -1145,1 +1150,1 @@\n-               (flags & ALL_KINDS) == IS_CONSTRUCTOR) {\n+               (flags & ALL_KINDS) == IS_OBJECT_CONSTRUCTOR) {\n","filename":"src\/hotspot\/share\/prims\/methodHandles.cpp","additions":26,"deletions":21,"binary":false,"changes":47,"status":"modified"},{"patch":"@@ -55,0 +55,1 @@\n+#include \"memory\/iterator.inline.hpp\"\n@@ -62,0 +63,1 @@\n+#include \"oops\/compressedOops.inline.hpp\"\n@@ -68,0 +70,1 @@\n+#include \"oops\/objArrayOop.inline.hpp\"\n@@ -1896,0 +1899,87 @@\n+WB_ENTRY(jobjectArray, WB_getObjectsViaKlassOopMaps(JNIEnv* env, jobject wb, jobject thing))\n+  oop aoop = JNIHandles::resolve(thing);\n+  if (!aoop->is_instance()) {\n+    return nullptr;\n+  }\n+  instanceHandle ih(THREAD, (instanceOop) aoop);\n+  InstanceKlass* klass = InstanceKlass::cast(ih->klass());\n+  if (klass->nonstatic_oop_map_count() == 0) {\n+    return nullptr;\n+  }\n+  const OopMapBlock* map = klass->start_of_nonstatic_oop_maps();\n+  const OopMapBlock* const end = map + klass->nonstatic_oop_map_count();\n+  int oop_count = 0;\n+  while (map < end) {\n+    oop_count += map->count();\n+    map++;\n+  }\n+\n+  objArrayHandle result_array =\n+      oopFactory::new_objArray_handle(vmClasses::Object_klass(), oop_count, CHECK_NULL);\n+  map = klass->start_of_nonstatic_oop_maps();\n+  int index = 0;\n+  while (map < end) {\n+    int offset = map->offset();\n+    for (unsigned int j = 0; j < map->count(); j++) {\n+      result_array->obj_at_put(index++, ih->obj_field(offset));\n+      offset += heapOopSize;\n+    }\n+    map++;\n+  }\n+  return (jobjectArray)JNIHandles::make_local(THREAD, result_array());\n+WB_END\n+\n+class CollectOops : public BasicOopIterateClosure {\n+ public:\n+  GrowableArray<Handle>* array;\n+\n+  jobjectArray create_jni_result(JNIEnv* env, TRAPS) {\n+    objArrayHandle result_array =\n+        oopFactory::new_objArray_handle(vmClasses::Object_klass(), array->length(), CHECK_NULL);\n+    for (int i = 0 ; i < array->length(); i++) {\n+      result_array->obj_at_put(i, array->at(i)());\n+    }\n+    return (jobjectArray)JNIHandles::make_local(THREAD, result_array());\n+  }\n+\n+  void add_oop(oop o) {\n+    Handle oh = Handle(Thread::current(), o);\n+    \/\/ Value might be oop, but JLS can't see as Object, just iterate through it...\n+    if (oh != nullptr && oh->is_inline_type()) {\n+      oh->oop_iterate(this);\n+    } else {\n+      array->append(oh);\n+    }\n+  }\n+\n+  void do_oop(oop* o) { add_oop(HeapAccess<>::oop_load(o)); }\n+  void do_oop(narrowOop* v) { add_oop(HeapAccess<>::oop_load(v)); }\n+};\n+\n+\n+WB_ENTRY(jobjectArray, WB_getObjectsViaOopIterator(JNIEnv* env, jobject wb, jobject thing))\n+  ResourceMark rm(thread);\n+  Handle objh(thread, JNIHandles::resolve(thing));\n+  GrowableArray<Handle>* array = new GrowableArray<Handle>(128);\n+  CollectOops collectOops;\n+  collectOops.array = array;\n+  objh->oop_iterate(&collectOops);\n+  return collectOops.create_jni_result(env, THREAD);\n+WB_END\n+\n+WB_ENTRY(jobjectArray, WB_getObjectsViaFrameOopIterator(JNIEnv* env, jobject wb, jint depth))\n+  ResourceMark rm(THREAD);\n+  GrowableArray<Handle>* array = new GrowableArray<Handle>(128);\n+  CollectOops collectOops;\n+  collectOops.array = array;\n+  StackFrameStream sfs(thread, false \/* update *\/, true \/* process_frames *\/);\n+  while (depth > 0) { \/\/ Skip the native WB API frame\n+    sfs.next();\n+    frame* f = sfs.current();\n+    f->oops_do(&collectOops, nullptr, sfs.register_map());\n+    depth--;\n+  }\n+  return collectOops.create_jni_result(env, THREAD);\n+WB_END\n+\n+\n@@ -2808,0 +2898,6 @@\n+  {CC\"getObjectsViaKlassOopMaps0\",\n+      CC\"(Ljava\/lang\/Object;)[Ljava\/lang\/Object;\",    (void*)&WB_getObjectsViaKlassOopMaps},\n+  {CC\"getObjectsViaOopIterator0\",\n+          CC\"(Ljava\/lang\/Object;)[Ljava\/lang\/Object;\",(void*)&WB_getObjectsViaOopIterator},\n+  {CC\"getObjectsViaFrameOopIterator\",\n+      CC\"(I)[Ljava\/lang\/Object;\",                     (void*)&WB_getObjectsViaFrameOopIterator},\n","filename":"src\/hotspot\/share\/prims\/whitebox.cpp","additions":96,"deletions":0,"binary":false,"changes":96,"status":"modified"},{"patch":"@@ -52,0 +52,2 @@\n+#include \"oops\/flatArrayKlass.hpp\"\n+#include \"oops\/flatArrayOop.hpp\"\n@@ -57,0 +59,1 @@\n+#include \"oops\/inlineKlass.inline.hpp\"\n@@ -308,1 +311,1 @@\n-    assert(obj.not_null() || realloc_failures, \"reallocation was missed\");\n+    assert(obj.not_null() || k->is_inline_klass() || realloc_failures, \"reallocation was missed\");\n@@ -310,1 +313,5 @@\n-      st.print(\" allocation failed\");\n+      if (k->is_inline_klass()) {\n+        st.print(\" is null\");\n+      } else {\n+        st.print(\" allocation failed\");\n+      }\n@@ -344,2 +351,13 @@\n-  bool save_oop_result = chunk->at(0)->scope()->return_oop() && !thread->popframe_forcing_deopt_reexecution() && (exec_mode == Deoptimization::Unpack_deopt);\n-  Handle return_value;\n+  ScopeDesc* scope = chunk->at(0)->scope();\n+  bool save_oop_result = scope->return_oop() && !thread->popframe_forcing_deopt_reexecution() && (exec_mode == Deoptimization::Unpack_deopt);\n+  \/\/ In case of the return of multiple values, we must take care\n+  \/\/ of all oop return values.\n+  GrowableArray<Handle> return_oops;\n+  InlineKlass* vk = nullptr;\n+  if (save_oop_result && scope->return_scalarized()) {\n+    vk = InlineKlass::returned_inline_klass(map);\n+    if (vk != nullptr) {\n+      vk->save_oop_fields(map, return_oops);\n+      save_oop_result = false;\n+    }\n+  }\n@@ -351,1 +369,1 @@\n-    return_value = Handle(thread, result);\n+    return_oops.push(Handle(thread, result));\n@@ -358,1 +376,1 @@\n-  if (objects != nullptr) {\n+  if (objects != nullptr || vk != nullptr) {\n@@ -363,1 +381,8 @@\n-      realloc_failures = Deoptimization::realloc_objects(thread, &deoptee, &map, objects, CHECK_AND_CLEAR_(true));\n+      if (vk != nullptr) {\n+        realloc_failures = Deoptimization::realloc_inline_type_result(vk, map, return_oops, CHECK_AND_CLEAR_(true));\n+      }\n+      if (objects != nullptr) {\n+        realloc_failures = realloc_failures || Deoptimization::realloc_objects(thread, &deoptee, &map, objects, CHECK_AND_CLEAR_(true));\n+        bool skip_internal = (compiled_method != nullptr) && !compiled_method->is_compiled_by_jvmci();\n+        Deoptimization::reassign_fields(&deoptee, &map, objects, realloc_failures, skip_internal, CHECK_AND_CLEAR_(true));\n+      }\n@@ -368,1 +393,8 @@\n-      realloc_failures = Deoptimization::realloc_objects(thread, &deoptee, &map, objects, THREAD);\n+      if (vk != nullptr) {\n+        realloc_failures = Deoptimization::realloc_inline_type_result(vk, map, return_oops, THREAD);\n+      }\n+      if (objects != nullptr) {\n+        realloc_failures = realloc_failures || Deoptimization::realloc_objects(thread, &deoptee, &map, objects, THREAD);\n+        bool skip_internal = (compiled_method != nullptr) && !compiled_method->is_compiled_by_jvmci();\n+        Deoptimization::reassign_fields(&deoptee, &map, objects, realloc_failures, skip_internal, THREAD);\n+      }\n@@ -371,2 +403,0 @@\n-    bool skip_internal = (compiled_method != nullptr) && !compiled_method->is_compiled_by_jvmci();\n-    Deoptimization::reassign_fields(&deoptee, &map, objects, realloc_failures, skip_internal);\n@@ -377,1 +407,1 @@\n-  if (save_oop_result) {\n+  if (save_oop_result || vk != nullptr) {\n@@ -379,1 +409,2 @@\n-    deoptee.set_saved_oop_result(&map, return_value());\n+    assert(return_oops.length() == 1, \"no inline type\");\n+    deoptee.set_saved_oop_result(&map, return_oops.pop()());\n@@ -714,1 +745,1 @@\n-  \/\/ If the sender is deoptimized the we must retrieve the address of the handler\n+  \/\/ If the sender is deoptimized we must retrieve the address of the handler\n@@ -1214,2 +1245,11 @@\n-\n-    oop obj = nullptr;\n+    \/\/ Check if the object may be null and has an additional is_init input that needs\n+    \/\/ to be checked before using the field values. Skip re-allocation if it is null.\n+    if (sv->maybe_null()) {\n+      assert(k->is_inline_klass(), \"must be an inline klass\");\n+      jint is_init = StackValue::create_stack_value(fr, reg_map, sv->is_init())->get_jint();\n+      if (is_init == 0) {\n+        continue;\n+      }\n+    }\n+\n+    oop obj = nullptr;\n@@ -1248,0 +1288,4 @@\n+    } else if (k->is_flatArray_klass()) {\n+      FlatArrayKlass* ak = FlatArrayKlass::cast(k);\n+      \/\/ Inline type array must be zeroed because not all memory is reassigned\n+      obj = ak->allocate(sv->field_size(), THREAD);\n@@ -1277,0 +1321,15 @@\n+\/\/ We're deoptimizing at the return of a call, inline type fields are\n+\/\/ in registers. When we go back to the interpreter, it will expect a\n+\/\/ reference to an inline type instance. Allocate and initialize it from\n+\/\/ the register values here.\n+bool Deoptimization::realloc_inline_type_result(InlineKlass* vk, const RegisterMap& map, GrowableArray<Handle>& return_oops, TRAPS) {\n+  oop new_vt = vk->realloc_result(map, return_oops, THREAD);\n+  if (new_vt == nullptr) {\n+    CLEAR_PENDING_EXCEPTION;\n+    THROW_OOP_(Universe::out_of_memory_error_realloc_objects(), true);\n+  }\n+  return_oops.clear();\n+  return_oops.push(Handle(THREAD, new_vt));\n+  return false;\n+}\n+\n@@ -1442,0 +1501,2 @@\n+  InstanceKlass* _klass;\n+  bool _is_flat;\n@@ -1443,4 +1504,1 @@\n-  ReassignedField() {\n-    _offset = 0;\n-    _type = T_ILLEGAL;\n-  }\n+  ReassignedField() : _offset(0), _type(T_ILLEGAL), _klass(nullptr), _is_flat(false) { }\n@@ -1455,1 +1513,1 @@\n-static int reassign_fields_by_klass(InstanceKlass* klass, frame* fr, RegisterMap* reg_map, ObjectValue* sv, int svIndex, oop obj, bool skip_internal) {\n+static int reassign_fields_by_klass(InstanceKlass* klass, frame* fr, RegisterMap* reg_map, ObjectValue* sv, int svIndex, oop obj, bool skip_internal, int base_offset, TRAPS) {\n@@ -1464,0 +1522,9 @@\n+        if (fs.is_null_free_inline_type()) {\n+          if (fs.is_flat()) {\n+            field._is_flat = true;\n+            \/\/ Resolve klass of flat inline type field\n+            field._klass = InlineKlass::cast(klass->get_inline_type_field_klass(fs.index()));\n+          } else {\n+            field._type = T_OBJECT;  \/\/ Can be removed once Q-descriptors have been removed.\n+          }\n+        }\n@@ -1471,0 +1538,11 @@\n+    BasicType type = fields->at(i)._type;\n+    int offset = base_offset + fields->at(i)._offset;\n+    \/\/ Check for flat inline type field before accessing the ScopeValue because it might not have any fields\n+    if (fields->at(i)._is_flat) {\n+      \/\/ Recursively re-assign flat inline type fields\n+      InstanceKlass* vk = fields->at(i)._klass;\n+      assert(vk != nullptr, \"must be resolved\");\n+      offset -= InlineKlass::cast(vk)->first_field_offset(); \/\/ Adjust offset to omit oop header\n+      svIndex = reassign_fields_by_klass(vk, fr, reg_map, sv, svIndex, obj, skip_internal, offset, CHECK_0);\n+      continue; \/\/ Continue because we don't need to increment svIndex\n+    }\n@@ -1473,3 +1551,2 @@\n-    int offset = fields->at(i)._offset;\n-    BasicType type = fields->at(i)._type;\n-      case T_OBJECT: case T_ARRAY:\n+      case T_OBJECT:\n+      case T_ARRAY:\n@@ -1550,0 +1627,14 @@\n+\/\/ restore fields of an eliminated inline type array\n+void Deoptimization::reassign_flat_array_elements(frame* fr, RegisterMap* reg_map, ObjectValue* sv, flatArrayOop obj, FlatArrayKlass* vak, bool skip_internal, TRAPS) {\n+  InlineKlass* vk = vak->element_klass();\n+  assert(vk->flat_array(), \"should only be used for flat inline type arrays\");\n+  \/\/ Adjust offset to omit oop header\n+  int base_offset = arrayOopDesc::base_offset_in_bytes(T_PRIMITIVE_OBJECT) - InlineKlass::cast(vk)->first_field_offset();\n+  \/\/ Initialize all elements of the flat inline type array\n+  for (int i = 0; i < sv->field_size(); i++) {\n+    ScopeValue* val = sv->field_at(i);\n+    int offset = base_offset + (i << Klass::layout_helper_log2_element_size(vak->layout_helper()));\n+    reassign_fields_by_klass(vk, fr, reg_map, val->as_ObjectValue(), 0, (oop)obj, skip_internal, offset, CHECK);\n+  }\n+}\n+\n@@ -1551,1 +1642,1 @@\n-void Deoptimization::reassign_fields(frame* fr, RegisterMap* reg_map, GrowableArray<ScopeValue*>* objects, bool realloc_failures, bool skip_internal) {\n+void Deoptimization::reassign_fields(frame* fr, RegisterMap* reg_map, GrowableArray<ScopeValue*>* objects, bool realloc_failures, bool skip_internal, TRAPS) {\n@@ -1557,1 +1648,1 @@\n-    assert(obj.not_null() || realloc_failures, \"reallocation was missed\");\n+    assert(obj.not_null() || realloc_failures || sv->maybe_null(), \"reallocation was missed\");\n@@ -1597,1 +1688,4 @@\n-      reassign_fields_by_klass(ik, fr, reg_map, sv, 0, obj(), skip_internal);\n+      reassign_fields_by_klass(ik, fr, reg_map, sv, 0, obj(), skip_internal, 0, CHECK);\n+    } else if (k->is_flatArray_klass()) {\n+      FlatArrayKlass* vak = FlatArrayKlass::cast(k);\n+      reassign_flat_array_elements(fr, reg_map, sv, (flatArrayOop) obj(), vak, skip_internal, CHECK);\n@@ -1757,1 +1851,1 @@\n-  \/\/ Deoptimize only if the frame comes from compile code.\n+  \/\/ Deoptimize only if the frame comes from compiled code.\n","filename":"src\/hotspot\/share\/runtime\/deoptimization.cpp","additions":121,"deletions":27,"binary":false,"changes":148,"status":"modified"},{"patch":"@@ -266,0 +266,3 @@\n+  static address _load_inline_type_fields_in_regs;\n+  static address _store_inline_type_fields_to_buf;\n+\n@@ -483,0 +486,3 @@\n+\n+  static address load_inline_type_fields_in_regs() { return _load_inline_type_fields_in_regs; }\n+  static address store_inline_type_fields_to_buf() { return _store_inline_type_fields_to_buf; }\n","filename":"src\/hotspot\/share\/runtime\/stubRoutines.hpp","additions":6,"deletions":0,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -28,0 +28,1 @@\n+import jdk.internal.misc.PreviewFeatures;\n@@ -38,0 +39,1 @@\n+import java.util.HashSet;\n@@ -317,1 +319,2 @@\n-        cw.visit(CLASSFILE_VERSION, ACC_SUPER + ACC_FINAL + ACC_SYNTHETIC,\n+        int version = CLASSFILE_VERSION | (PreviewFeatures.isEnabled() ? Opcodes.V_PREVIEW : 0);\n+        cw.visit(version, ACC_SUPER + ACC_FINAL + ACC_SYNTHETIC,\n@@ -355,0 +358,10 @@\n+        \/\/ generate Preload attribute if it references any value class\n+        PreloadAttributeBuilder builder = new PreloadAttributeBuilder(targetClass);\n+        builder.add(factoryType)\n+               .add(interfaceMethodType)\n+               .add(implMethodType)\n+               .add(dynamicMethodType)\n+               .add(altMethods);\n+        if (!builder.isEmpty())\n+            cw.visitAttribute(builder.build());\n+\n@@ -569,0 +582,66 @@\n+    \/*\n+     * Preload attribute builder\n+     *\/\n+    static class PreloadAttributeBuilder {\n+        private final Set<Class<?>> preloadClasses = new HashSet<>();\n+        PreloadAttributeBuilder(Class<?> targetClass) {\n+            if (requiresPreload(targetClass)) {\n+                preloadClasses.add(targetClass);\n+            }\n+        }\n+\n+        \/*\n+         * Add the value types referenced in the given MethodType.\n+         *\/\n+        PreloadAttributeBuilder add(MethodType mt) {\n+            \/\/ parameter types\n+            for (Class<?> paramType : mt.ptypes()) {\n+                if (requiresPreload(paramType)) {\n+                    preloadClasses.add(paramType);\n+                }\n+            }\n+            \/\/ return type\n+            if (requiresPreload(mt.returnType())) {\n+                preloadClasses.add(mt.returnType());\n+            }\n+            return this;\n+        }\n+\n+        PreloadAttributeBuilder add(MethodType... mtypes) {\n+            for (MethodType mt : mtypes) {\n+                add(mt);\n+            }\n+            return this;\n+        }\n+\n+        boolean requiresPreload(Class<?> cls) {\n+            Class<?> c = cls;\n+            while (c.isArray()) {\n+                c = c.getComponentType();\n+            }\n+            return c.isValue();\n+        }\n+\n+        boolean isEmpty() {\n+            return preloadClasses.isEmpty();\n+        }\n+\n+        Attribute build() {\n+            return new Attribute(\"Preload\") {\n+                @Override\n+                protected ByteVector write(ClassWriter cw,\n+                                           byte[] code,\n+                                           int len,\n+                                           int maxStack,\n+                                           int maxLocals) {\n+                    ByteVector attr = new ByteVector();\n+                    attr.putShort(preloadClasses.size());\n+                    for (Class<?> c : preloadClasses) {\n+                        attr.putShort(cw.newClass(Type.getInternalName(c)));\n+                    }\n+                    return attr;\n+                }\n+            };\n+        }\n+    }\n+\n","filename":"src\/java.base\/share\/classes\/java\/lang\/invoke\/InnerClassLambdaMetafactory.java","additions":80,"deletions":1,"binary":false,"changes":81,"status":"modified"},{"patch":"@@ -28,0 +28,1 @@\n+import jdk.internal.value.PrimitiveClass;\n@@ -94,1 +95,2 @@\n-     *  For a constructor, it is always {@code \"<init>\"}.\n+     *  For an identity object constructor, it is {@code \"<init>\"}.\n+     *  For a value class static factory method, it is {@code \"<vnew>\"}.\n@@ -184,2 +186,3 @@\n-        if (isConstructor() && getReferenceKind() == REF_newInvokeSpecial)\n-            return itype.changeReturnType(clazz);\n+        Class<?> c = PrimitiveClass.isPrimitiveClass(clazz) ? PrimitiveClass.asValueType(clazz) : clazz;\n+        if (isObjectConstructor() && getReferenceKind() == REF_newInvokeSpecial)\n+            return itype.changeReturnType(c);\n@@ -187,1 +190,1 @@\n-            return itype.insertParameterTypes(0, clazz);\n+            return itype.insertParameterTypes(0, c);\n@@ -250,1 +253,1 @@\n-        } else if (isConstructor()) {\n+        } else if (isObjectConstructor()) {\n@@ -390,0 +393,2 @@\n+        \/\/ all fields declared in a value type are effectively final\n+        assert(!clazz.isValue() || !isField() || Modifier.isFinal(flags));\n@@ -411,5 +416,6 @@\n-    static final int BRIDGE    = 0x00000040;\n-    static final int VARARGS   = 0x00000080;\n-    static final int SYNTHETIC = 0x00001000;\n-    static final int ANNOTATION= 0x00002000;\n-    static final int ENUM      = 0x00004000;\n+    static final int BRIDGE      = 0x00000040;\n+    static final int VARARGS     = 0x00000080;\n+    static final int SYNTHETIC   = 0x00001000;\n+    static final int ANNOTATION  = 0x00002000;\n+    static final int ENUM        = 0x00004000;\n+\n@@ -429,1 +435,14 @@\n-    static final String CONSTRUCTOR_NAME = \"<init>\";  \/\/ the ever-popular\n+    \/** Query whether this member is a flattened field *\/\n+    public boolean isFlattened() { return (flags & MN_FLATTENED) == MN_FLATTENED; }\n+\n+    \/** Query whether this member is a field of a primitive class. *\/\n+    public boolean isInlineableField()  {\n+        if (isField()) {\n+            Class<?> type = getFieldType();\n+            return PrimitiveClass.isPrimitiveValueType(type) || (type.isValue() && !PrimitiveClass.isPrimitiveClass(type));\n+        }\n+        return false;\n+    }\n+\n+    static final String CONSTRUCTOR_NAME = \"<init>\";\n+    static final String VALUE_FACTORY_NAME = \"<vnew>\";  \/\/ the ever-popular\n@@ -436,6 +455,6 @@\n-            IS_METHOD        = MN_IS_METHOD,        \/\/ method (not constructor)\n-            IS_CONSTRUCTOR   = MN_IS_CONSTRUCTOR,   \/\/ constructor\n-            IS_FIELD         = MN_IS_FIELD,         \/\/ field\n-            IS_TYPE          = MN_IS_TYPE,          \/\/ nested type\n-            CALLER_SENSITIVE = MN_CALLER_SENSITIVE, \/\/ @CallerSensitive annotation detected\n-            TRUSTED_FINAL    = MN_TRUSTED_FINAL;    \/\/ trusted final field\n+            IS_METHOD             = MN_IS_METHOD,              \/\/ method (not object constructor)\n+            IS_OBJECT_CONSTRUCTOR = MN_IS_OBJECT_CONSTRUCTOR,  \/\/ object constructor\n+            IS_FIELD              = MN_IS_FIELD,               \/\/ field\n+            IS_TYPE               = MN_IS_TYPE,                \/\/ nested type\n+            CALLER_SENSITIVE      = MN_CALLER_SENSITIVE,       \/\/ @CallerSensitive annotation detected\n+            TRUSTED_FINAL         = MN_TRUSTED_FINAL;    \/\/ trusted final field\n@@ -444,2 +463,2 @@\n-    static final int ALL_KINDS = IS_METHOD | IS_CONSTRUCTOR | IS_FIELD | IS_TYPE;\n-    static final int IS_INVOCABLE = IS_METHOD | IS_CONSTRUCTOR;\n+    static final int ALL_KINDS = IS_METHOD | IS_OBJECT_CONSTRUCTOR | IS_FIELD | IS_TYPE;\n+    static final int IS_INVOCABLE = IS_METHOD | IS_OBJECT_CONSTRUCTOR;\n@@ -456,2 +475,6 @@\n-    public boolean isConstructor() {\n-        return allFlagsSet(IS_CONSTRUCTOR);\n+    public boolean isObjectConstructor() {\n+        return allFlagsSet(IS_OBJECT_CONSTRUCTOR);\n+    }\n+    \/** Query whether this member is an object constructor or static <init> factory *\/\n+    public boolean isStaticValueFactoryMethod() {\n+        return VALUE_FACTORY_NAME.equals(name) && isMethod();\n@@ -459,0 +482,1 @@\n+\n@@ -582,1 +606,1 @@\n-    public MemberName asConstructor() {\n+    public MemberName asObjectConstructor() {\n@@ -618,3 +642,8 @@\n-        this.name = CONSTRUCTOR_NAME;\n-        if (this.type == null)\n-            this.type = new Object[] { void.class, ctor.getParameterTypes() };\n+        this.name = this.clazz.isValue() ? VALUE_FACTORY_NAME : CONSTRUCTOR_NAME;\n+        if (this.type == null) {\n+            Class<?> rtype = void.class;\n+            if (isStatic()) {  \/\/ a value class static factory, not a true constructor\n+                rtype = getDeclaringClass();\n+            }\n+            this.type = new Object[] { rtype, ctor.getParameterTypes() };\n+        }\n@@ -752,1 +781,2 @@\n-     *  It will be a constructor if and only if the name is {@code \"<init>\"}.\n+     *  It will be an object constructor if and only if the name is {@code \"<init>\"}.\n+     *  It will be a value class instance factory method if and only if the name is {@code \"<vnew>\"}.\n@@ -758,1 +788,1 @@\n-        int initFlags = (name != null && name.equals(CONSTRUCTOR_NAME) ? IS_CONSTRUCTOR : IS_METHOD);\n+        int initFlags = CONSTRUCTOR_NAME.equals(name) ? IS_OBJECT_CONSTRUCTOR : IS_METHOD;\n@@ -776,1 +806,1 @@\n-            kindFlags = IS_CONSTRUCTOR;\n+            kindFlags = IS_OBJECT_CONSTRUCTOR;\n@@ -894,1 +924,1 @@\n-        else if (isConstructor())\n+        else if (isObjectConstructor())\n@@ -907,1 +937,1 @@\n-        else if (isConstructor())\n+        else if (isObjectConstructor())\n","filename":"src\/java.base\/share\/classes\/java\/lang\/invoke\/MemberName.java","additions":60,"deletions":30,"binary":false,"changes":90,"status":"modified"},{"patch":"@@ -113,9 +113,10 @@\n-            MN_IS_METHOD           = 0x00010000, \/\/ method (not constructor)\n-            MN_IS_CONSTRUCTOR      = 0x00020000, \/\/ constructor\n-            MN_IS_FIELD            = 0x00040000, \/\/ field\n-            MN_IS_TYPE             = 0x00080000, \/\/ nested type\n-            MN_CALLER_SENSITIVE    = 0x00100000, \/\/ @CallerSensitive annotation detected\n-            MN_TRUSTED_FINAL       = 0x00200000, \/\/ trusted final field\n-            MN_HIDDEN_MEMBER       = 0x00400000, \/\/ members defined in a hidden class or with @Hidden\n-            MN_REFERENCE_KIND_SHIFT = 24, \/\/ refKind\n-            MN_REFERENCE_KIND_MASK = 0x0F000000 >> MN_REFERENCE_KIND_SHIFT;\n+            MN_IS_METHOD             = 0x00010000, \/\/ method (not object constructor)\n+            MN_IS_OBJECT_CONSTRUCTOR = 0x00020000, \/\/ object constructor\n+            MN_IS_FIELD              = 0x00040000, \/\/ field\n+            MN_IS_TYPE               = 0x00080000, \/\/ nested type\n+            MN_CALLER_SENSITIVE      = 0x00100000, \/\/ @CallerSensitive annotation detected\n+            MN_TRUSTED_FINAL         = 0x00200000, \/\/ trusted final field\n+            MN_HIDDEN_MEMBER         = 0x00400000, \/\/ members defined in a hidden class or with @Hidden\n+            MN_FLATTENED             = 0x00800000, \/\/ flattened field\n+            MN_REFERENCE_KIND_SHIFT  = 24, \/\/ refKind\n+            MN_REFERENCE_KIND_MASK   = 0x0F000000 >> MN_REFERENCE_KIND_SHIFT;\n@@ -175,1 +176,1 @@\n-    static boolean refKindIsConstructor(byte refKind) {\n+    static boolean refKindIsObjectConstructor(byte refKind) {\n@@ -603,1 +604,1 @@\n-            sb.append(getCharType(pt));\n+            sb.append(getCharErasedType(pt));\n@@ -605,1 +606,1 @@\n-        sb.append('_').append(getCharType(guardType.returnType()));\n+        sb.append('_').append(getCharErasedType(guardType.returnType()));\n@@ -608,1 +609,1 @@\n-    static char getCharType(Class<?> pt) {\n+    static char getCharErasedType(Class<?> pt) {\n","filename":"src\/java.base\/share\/classes\/java\/lang\/invoke\/MethodHandleNatives.java","additions":14,"deletions":13,"binary":false,"changes":27,"status":"modified"},{"patch":"@@ -1065,0 +1065,1 @@\n+    int ACC_IDENTITY = 0x0020;\n@@ -1071,0 +1072,1 @@\n+    int ACC_VALUE    = 0x0040;\n","filename":"src\/java.base\/share\/classes\/jdk\/internal\/classfile\/Classfile.java","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -187,0 +187,1 @@\n+        allowPrimitiveClasses = Feature.PRIMITIVE_CLASSES.allowedInSource(source) && options.isSet(\"enablePrimitiveClasses\");\n@@ -230,0 +231,4 @@\n+    \/** Are primitive classes allowed\n+     *\/\n+    private final boolean allowPrimitiveClasses;\n+\n@@ -632,0 +637,5 @@\n+        } else {\n+            if (allowPrimitiveClasses && found.hasTag(CLASS)) {\n+                if (inferenceContext != infer.emptyContext)\n+                    checkParameterizationByPrimitiveClass(pos, found);\n+            }\n@@ -762,0 +772,51 @@\n+    void checkConstraintsOfValueClass(DiagnosticPosition pos, ClassSymbol c) {\n+        for (Type st : types.closure(c.type)) {\n+            if (st == null || st.tsym == null || st.tsym.kind == ERR)\n+                continue;\n+            if  (st.tsym == syms.objectType.tsym || st.tsym == syms.recordType.tsym || st.isInterface())\n+                continue;\n+            if (!st.tsym.isAbstract()) {\n+                if (c != st.tsym) {\n+                    log.error(pos, Errors.ConcreteSupertypeForValueClass(c, st));\n+                }\n+                continue;\n+            }\n+            \/\/ dealing with an abstract value or value super class below.\n+            Fragment fragment = c.isAbstract() && c.isValueClass() && c == st.tsym ? Fragments.AbstractValueClass(c) : Fragments.SuperclassOfValueClass(c, st);\n+            if ((st.tsym.flags() & HASINITBLOCK) != 0) {\n+                log.error(pos, Errors.AbstractValueClassDeclaresInitBlock(fragment));\n+            }\n+            Type encl = st.getEnclosingType();\n+            if (encl != null && encl.hasTag(CLASS)) {\n+                log.error(pos, Errors.AbstractValueClassCannotBeInner(fragment));\n+            }\n+            for (Symbol s : st.tsym.members().getSymbols(NON_RECURSIVE)) {\n+                switch (s.kind) {\n+                case VAR:\n+                    if ((s.flags() & STATIC) == 0) {\n+                        log.error(pos, Errors.InstanceFieldNotAllowed(s, fragment));\n+                    }\n+                    break;\n+                case MTH:\n+                    if ((s.flags() & (SYNCHRONIZED | STATIC)) == SYNCHRONIZED) {\n+                        log.error(pos, Errors.SuperClassMethodCannotBeSynchronized(s, c, st));\n+                    } else if (s.isInitOrVNew()) {\n+                        MethodSymbol m = (MethodSymbol)s;\n+                        if (m.getParameters().size() > 0) {\n+                            log.error(pos, Errors.AbstractValueClassConstructorCannotTakeArguments(m, fragment));\n+                        } else if (m.getTypeParameters().size() > 0) {\n+                            log.error(pos, Errors.AbstractValueClassConstructorCannotBeGeneric(m, fragment));\n+                        } else if (m.type.getThrownTypes().size() > 0) {\n+                            log.error(pos, Errors.AbstractValueClassConstructorCannotThrow(m, fragment));\n+                        } else if (protection(m.flags()) > protection(m.owner.flags())) {\n+                            log.error(pos, Errors.AbstractValueClassConstructorHasWeakerAccess(m, fragment));\n+                        } else if ((m.flags() & EMPTYNOARGCONSTR) == 0) {\n+                                log.error(pos, Errors.AbstractValueClassNoArgConstructorMustBeEmpty(m, fragment));\n+                        }\n+                    }\n+                    break;\n+                }\n+            }\n+        }\n+    }\n+\n@@ -764,2 +825,2 @@\n-    Type checkConstructorRefType(DiagnosticPosition pos, Type t) {\n-        t = checkClassOrArrayType(pos, t);\n+    Type checkConstructorRefType(JCExpression expr, Type t) {\n+        t = checkClassOrArrayType(expr, t);\n@@ -768,1 +829,1 @@\n-                log.error(pos, Errors.AbstractCantBeInstantiated(t.tsym));\n+                log.error(expr, Errors.AbstractCantBeInstantiated(t.tsym));\n@@ -771,1 +832,1 @@\n-                log.error(pos, Errors.EnumCantBeInstantiated);\n+                log.error(expr, Errors.EnumCantBeInstantiated);\n@@ -774,1 +835,10 @@\n-                t = checkClassType(pos, t, true);\n+                \/\/ Projection types may not be mentioned in constructor references\n+                if (expr.hasTag(SELECT)) {\n+                    JCFieldAccess fieldAccess = (JCFieldAccess) expr;\n+                    if (allowPrimitiveClasses && fieldAccess.selected.type.isPrimitiveClass() &&\n+                            (fieldAccess.name == names.ref || fieldAccess.name == names.val)) {\n+                        log.error(expr, Errors.ProjectionCantBeInstantiated);\n+                        t = types.createErrorType(t);\n+                    }\n+                }\n+                t = checkClassType(expr, t, true);\n@@ -778,1 +848,1 @@\n-                log.error(pos, Errors.GenericArrayCreation);\n+                log.error(expr, Errors.GenericArrayCreation);\n@@ -809,0 +879,1 @@\n+     *  @param primitiveClassOK       If false, a primitive class does not qualify\n@@ -810,2 +881,2 @@\n-    Type checkRefType(DiagnosticPosition pos, Type t) {\n-        if (t.isReference())\n+    Type checkRefType(DiagnosticPosition pos, Type t, boolean primitiveClassOK) {\n+        if (t.isReference() && (!allowPrimitiveClasses || primitiveClassOK || !t.isPrimitiveClass()))\n@@ -819,0 +890,31 @@\n+    \/** Check that type is an identity type, i.e. not a primitive\/value type\n+     *  nor its reference projection. When not discernible statically,\n+     *  give it the benefit of doubt and defer to runtime.\n+     *\n+     *  @param pos           Position to be used for error reporting.\n+     *  @param t             The type to be checked.\n+     *\/\n+    void checkIdentityType(DiagnosticPosition pos, Type t) {\n+        if (t.hasTag(TYPEVAR)) {\n+            t = types.skipTypeVars(t, false);\n+        }\n+        if (t.isIntersection()) {\n+            IntersectionClassType ict = (IntersectionClassType)t;\n+            for (Type component : ict.getExplicitComponents()) {\n+                checkIdentityType(pos, component);\n+            }\n+            return;\n+        }\n+        if (t.isPrimitive() || t.isValueClass() || t.isValueInterface() || t.isReferenceProjection())\n+            typeTagError(pos, diags.fragment(Fragments.TypeReqIdentity), t);\n+    }\n+\n+    \/** Check that type is a reference type, i.e. a class, interface or array type\n+     *  or a type variable.\n+     *  @param pos           Position to be used for error reporting.\n+     *  @param t             The type to be checked.\n+     *\/\n+    Type checkRefType(DiagnosticPosition pos, Type t) {\n+        return checkRefType(pos, t, true);\n+    }\n+\n@@ -827,1 +929,1 @@\n-            l.head = checkRefType(tl.head.pos(), l.head);\n+            l.head = checkRefType(tl.head.pos(), l.head, false);\n@@ -863,0 +965,49 @@\n+    void checkParameterizationByPrimitiveClass(DiagnosticPosition pos, Type t) {\n+        parameterizationByPrimitiveClassChecker.visit(t, pos);\n+    }\n+\n+    \/** parameterizationByPrimitiveClassChecker: A type visitor that descends down the given type looking for instances of primitive classes\n+     *  being used as type arguments and issues error against those usages.\n+     *\/\n+    private final Types.SimpleVisitor<Void, DiagnosticPosition> parameterizationByPrimitiveClassChecker =\n+            new Types.SimpleVisitor<Void, DiagnosticPosition>() {\n+\n+        @Override\n+        public Void visitType(Type t, DiagnosticPosition pos) {\n+            return null;\n+        }\n+\n+        @Override\n+        public Void visitClassType(ClassType t, DiagnosticPosition pos) {\n+            for (Type targ : t.allparams()) {\n+                if (allowPrimitiveClasses && targ.isPrimitiveClass()) {\n+                    log.error(pos, Errors.GenericParameterizationWithPrimitiveClass(t));\n+                }\n+                visit(targ, pos);\n+            }\n+            return null;\n+        }\n+\n+        @Override\n+        public Void visitTypeVar(TypeVar t, DiagnosticPosition pos) {\n+             return null;\n+        }\n+\n+        @Override\n+        public Void visitCapturedType(CapturedType t, DiagnosticPosition pos) {\n+            return null;\n+        }\n+\n+        @Override\n+        public Void visitArrayType(ArrayType t, DiagnosticPosition pos) {\n+            return visit(t.elemtype, pos);\n+        }\n+\n+        @Override\n+        public Void visitWildcardType(WildcardType t, DiagnosticPosition pos) {\n+            return visit(t.type, pos);\n+        }\n+    };\n+\n+\n+\n@@ -995,1 +1146,1 @@\n-                (s.isConstructor() ||\n+                (s.isInitOrVNew() ||\n@@ -1011,1 +1162,5 @@\n-        return types.upward(t, types.captures(t)).baseType();\n+        Type varType = types.upward(t, types.captures(t)).baseType();\n+        if (allowPrimitiveClasses && varType.hasTag(CLASS)) {\n+            checkParameterizationByPrimitiveClass(pos, varType);\n+        }\n+        return varType;\n@@ -1034,0 +1189,1 @@\n+        \/\/ TODO - is enum so <init>\n@@ -1209,1 +1365,1 @@\n-            else\n+            else {\n@@ -1211,0 +1367,4 @@\n+                if (sym.owner.type.isValueClass() && (flags & STATIC) == 0) {\n+                    implicit |= FINAL;\n+                }\n+            }\n@@ -1213,1 +1373,1 @@\n-            if (sym.name == names.init) {\n+            if (names.isInitOrVNew(sym.name)) {\n@@ -1236,1 +1396,2 @@\n-                mask = RecordMethodFlags;\n+                mask = ((sym.owner.flags_field & VALUE_CLASS) != 0 && (flags & Flags.STATIC) == 0) ?\n+                        RecordMethodFlags & ~SYNCHRONIZED : RecordMethodFlags;\n@@ -1238,1 +1399,3 @@\n-                mask = MethodFlags;\n+                \/\/ value objects do not have an associated monitor\/lock\n+                mask = ((sym.owner.flags_field & VALUE_CLASS) != 0 && (flags & Flags.STATIC) == 0) ?\n+                        MethodFlags & ~SYNCHRONIZED : MethodFlags;\n@@ -1255,1 +1418,1 @@\n-                mask = staticOrImplicitlyStatic && allowRecords && (flags & ANNOTATION) == 0 ? StaticLocalFlags : LocalClassFlags;\n+                mask = staticOrImplicitlyStatic && allowRecords && (flags & ANNOTATION) == 0 ? ExtendedStaticLocalClassFlags : ExtendedLocalClassFlags;\n@@ -1275,2 +1438,2 @@\n-                \/\/ enums can't be declared abstract, final, sealed or non-sealed\n-                mask &= ~(ABSTRACT | FINAL | SEALED | NON_SEALED);\n+                \/\/ enums can't be declared abstract, final, sealed or non-sealed or primitive\/value\n+                mask &= ~(ABSTRACT | FINAL | SEALED | NON_SEALED | PRIMITIVE_CLASS | VALUE_CLASS);\n@@ -1289,0 +1452,17 @@\n+\n+            \/\/ primitive classes are implicitly final value classes.\n+            if ((flags & PRIMITIVE_CLASS) != 0)\n+                implicit |= VALUE_CLASS | FINAL;\n+\n+            \/\/ concrete value classes are implicitly final\n+            if ((flags & (ABSTRACT | INTERFACE | VALUE_CLASS)) == VALUE_CLASS) {\n+                implicit |= FINAL;\n+                if ((flags & NON_SEALED) != 0) {\n+                    \/\/ cant declare a final value class non-sealed\n+                    log.error(pos,\n+                            Errors.ModNotAllowedHere(asFlagSet(NON_SEALED)));\n+                }\n+            }\n+\n+            \/\/ TYPs can't be declared synchronized\n+            mask &= ~SYNCHRONIZED;\n@@ -1317,1 +1497,5 @@\n-                               FINAL | NATIVE | SYNCHRONIZED)\n+                               FINAL | NATIVE | SYNCHRONIZED | PRIMITIVE_CLASS)\n+                 &&\n+                 checkDisjoint(pos, flags,\n+                        IDENTITY_TYPE,\n+                        PRIMITIVE_CLASS | VALUE_CLASS)\n@@ -1327,1 +1511,1 @@\n-                 checkDisjoint(pos, flags,\n+                 checkDisjoint(pos, (flags | implicit), \/\/ complain against volatile & implcitly final entities too.\n@@ -1343,1 +1527,7 @@\n-                                ANNOTATION)) {\n+                                ANNOTATION)\n+                 && checkDisjoint(pos, flags,\n+                                IDENTITY_TYPE,\n+                                ANNOTATION)\n+                && checkDisjoint(pos, flags,\n+                                VALUE_CLASS,\n+                                ANNOTATION) ) {\n@@ -1516,1 +1706,2 @@\n-                tree.selected.type.isParameterized()) {\n+                tree.selected.type.isParameterized() &&\n+                    (tree.name != names.ref || !tree.type.isReferenceProjection())) {\n@@ -1520,0 +1711,2 @@\n+                \/\/ Tolerate the pseudo-select V.ref: V<T>.ref will be static if V<T> is and\n+                \/\/ should not be confused as selecting a static member of a parameterized type.\n@@ -1583,1 +1776,1 @@\n-                    env.enclMethod != null && env.enclMethod.name == names.init;\n+                    env.enclMethod != null && names.isInitOrVNew(env.enclMethod.name);\n@@ -2184,1 +2377,1 @@\n-                (env.info.isAnonymousDiamond && !m.isConstructor() && !m.isPrivate());\n+                (env.info.isAnonymousDiamond && !m.isInitOrVNew() && !m.isPrivate());\n@@ -2340,0 +2533,39 @@\n+    \/\/ A primitive class cannot contain a field of its own type either or indirectly.\n+    void checkNonCyclicMembership(JCClassDecl tree) {\n+        if (allowPrimitiveClasses) {\n+            Assert.check((tree.sym.flags_field & LOCKED) == 0);\n+            try {\n+                tree.sym.flags_field |= LOCKED;\n+                for (List<? extends JCTree> l = tree.defs; l.nonEmpty(); l = l.tail) {\n+                    if (l.head.hasTag(VARDEF)) {\n+                        JCVariableDecl field = (JCVariableDecl) l.head;\n+                        if (cyclePossible(field.sym)) {\n+                            checkNonCyclicMembership((ClassSymbol) field.type.tsym, field.pos());\n+                        }\n+                    }\n+                }\n+            } finally {\n+                tree.sym.flags_field &= ~LOCKED;\n+            }\n+        }\n+    }\n+    \/\/ where\n+    private void checkNonCyclicMembership(ClassSymbol c, DiagnosticPosition pos) {\n+        if ((c.flags_field & LOCKED) != 0) {\n+            log.error(pos, Errors.CyclicPrimitiveClassMembership(c));\n+            return;\n+        }\n+        try {\n+            c.flags_field |= LOCKED;\n+            for (Symbol fld : c.members().getSymbols(s -> s.kind == VAR && cyclePossible((VarSymbol) s), NON_RECURSIVE)) {\n+                checkNonCyclicMembership((ClassSymbol) fld.type.tsym, pos);\n+            }\n+        } finally {\n+            c.flags_field &= ~LOCKED;\n+        }\n+    }\n+        \/\/ where\n+        private boolean cyclePossible(VarSymbol symbol) {\n+            return (symbol.flags() & STATIC) == 0 && allowPrimitiveClasses && symbol.type.isPrimitiveClass();\n+        }\n+\n@@ -2588,0 +2820,22 @@\n+\n+        boolean cIsValue = (c.tsym.flags() & VALUE_CLASS) != 0;\n+        boolean cHasIdentity = (c.tsym.flags() & IDENTITY_TYPE) != 0;\n+        Type identitySuper = null, valueSuper = null;\n+        for (Type t : types.closure(c)) {\n+            if (t != c) {\n+                if ((t.tsym.flags() & IDENTITY_TYPE) != 0)\n+                    identitySuper = t;\n+                else if ((t.tsym.flags() & VALUE_CLASS) != 0)\n+                    valueSuper = t;\n+                if (cIsValue &&  identitySuper != null) {\n+                    log.error(pos, Errors.ValueTypeHasIdentitySuperType(c, identitySuper));\n+                    break;\n+                } else if (cHasIdentity &&  valueSuper != null) {\n+                    log.error(pos, Errors.IdentityTypeHasValueSuperType(c, valueSuper));\n+                    break;\n+                } else if (identitySuper != null && valueSuper != null) {\n+                    log.error(pos, Errors.MutuallyIncompatibleSupers(c, identitySuper, valueSuper));\n+                    break;\n+                }\n+            }\n+        }\n@@ -2683,1 +2937,1 @@\n-                     !s.isConstructor();\n+                     !s.isInitOrVNew();\n@@ -2742,1 +2996,1 @@\n-                     !s.isConstructor();\n+                     !s.isInitOrVNew();\n@@ -3625,1 +3879,1 @@\n-                if (s.kind == MTH && !s.isConstructor())\n+                if (s.kind == MTH && !s.isInitOrVNew())\n@@ -3633,1 +3887,1 @@\n-                if (s.kind == MTH && s.isConstructor())\n+                if (s.kind == MTH && s.isInitOrVNew())\n@@ -3652,1 +3906,1 @@\n-                        (s.kind == MTH && !s.isConstructor() &&\n+                        (s.kind == MTH && !s.isInitOrVNew() &&\n@@ -3654,1 +3908,1 @@\n-                        (s.kind == MTH && s.isConstructor())) {\n+                        (s.kind == MTH && s.isInitOrVNew())) {\n@@ -4989,1 +5243,1 @@\n-                    if (sym.isConstructor() &&\n+                    if (sym.isInitOrVNew() &&\n@@ -5017,1 +5271,1 @@\n-                        if (sym.isConstructor()) {\n+                        if (sym.isInitOrVNew()) {\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/comp\/Check.java","additions":286,"deletions":32,"binary":false,"changes":318,"status":"modified"},{"patch":"@@ -67,0 +67,1 @@\n+    private static final Object[] NO_STATIC_ARGS = new Object[0];\n@@ -81,0 +82,1 @@\n+    private final TransValues transValues;\n@@ -117,0 +119,1 @@\n+        transValues = TransValues.instance(context);\n@@ -134,0 +137,2 @@\n+        Source source = Source.instance(context);\n+        allowPrimitiveClasses = Source.Feature.PRIMITIVE_CLASSES.allowedInSource(source) && options.isSet(\"enablePrimitiveClasses\");\n@@ -178,0 +183,2 @@\n+    boolean allowPrimitiveClasses;\n+\n@@ -267,0 +274,14 @@\n+    \/** Insert a reference to given type in the constant pool,\n+     *  checking for an array with too many dimensions;\n+     *  return the reference's index.\n+     *  @param type   The type for which a reference is inserted.\n+     *\/\n+    int makeRef(DiagnosticPosition pos, Type type, boolean emitQtype) {\n+        checkDimension(pos, type);\n+        if (emitQtype) {\n+            return poolWriter.putClass(new ConstantPoolQType(type, types));\n+        } else {\n+            return poolWriter.putClass(type);\n+        }\n+    }\n+\n@@ -273,1 +294,1 @@\n-        return poolWriter.putClass(checkDimension(pos, type));\n+        return makeRef(pos, type, false);\n@@ -327,1 +348,1 @@\n-        else items.makeMemberItem(msym, name == names.init).invoke();\n+        else items.makeMemberItem(msym, names.isInitOrVNew(name)).invoke();\n@@ -551,1 +572,1 @@\n-        if (md.name == names.init && TreeInfo.isInitialConstructor(md)) {\n+        if (names.isInitOrVNew(md.name) && TreeInfo.isInitialConstructor(md)) {\n@@ -954,1 +975,1 @@\n-            if (meth.isConstructor()) {\n+            if (meth.isInitOrVNew()) {\n@@ -994,0 +1015,3 @@\n+                    } else if (env.enclMethod.sym.isValueObjectFactory()) {\n+                        items.makeLocalItem(env.enclMethod.factoryProduct).load();\n+                        code.emitop0(areturn);\n@@ -1048,1 +1072,2 @@\n-                                        poolWriter);\n+                                        poolWriter,\n+                                        allowPrimitiveClasses);\n@@ -1058,1 +1083,1 @@\n-                if (meth.isConstructor() && selfType != syms.objectType)\n+                if (meth.isInitOrVNew() && selfType != syms.objectType)\n@@ -1097,0 +1122,4 @@\n+        Type localType = v.erasure(types);\n+        if (localType.requiresPreload(env.enclClass.sym)) {\n+            poolWriter.enterPreloadClass((ClassSymbol) localType.tsym);\n+        }\n@@ -1145,0 +1174,31 @@\n+    public void visitWithField(JCWithField tree) {\n+        switch(tree.field.getTag()) {\n+            case IDENT:\n+                Symbol sym = ((JCIdent) tree.field).sym;\n+                items.makeThisItem().load();\n+                genExpr(tree.value, tree.field.type).load();\n+                sym = binaryQualifier(sym, env.enclClass.type);\n+                code.emitop2(withfield, sym, PoolWriter::putMember);\n+                result = items.makeStackItem(tree.type);\n+                break;\n+            case SELECT:\n+                JCFieldAccess fieldAccess = (JCFieldAccess) tree.field;\n+                sym = TreeInfo.symbol(fieldAccess);\n+                \/\/ JDK-8207332: To maintain the order of side effects, must compute value ahead of field\n+                genExpr(tree.value, tree.field.type).load();\n+                genExpr(fieldAccess.selected, fieldAccess.selected.type).load();\n+                if (Code.width(tree.field.type) == 2) {\n+                    code.emitop0(dup_x2);\n+                    code.emitop0(pop);\n+                } else {\n+                    code.emitop0(swap);\n+                }\n+                sym = binaryQualifier(sym, fieldAccess.selected.type);\n+                code.emitop2(withfield, sym, PoolWriter::putMember);\n+                result = items.makeStackItem(tree.type);\n+                break;\n+            default:\n+                Assert.check(false);\n+        }\n+    }\n+\n@@ -2070,1 +2130,1 @@\n-                code.emitAnewarray(makeRef(pos, elemtype), type);\n+                code.emitAnewarray(makeRef(pos, elemtype, elemtype.isPrimitiveClass()), type);\n@@ -2296,0 +2356,1 @@\n+        \/\/ primitive reference conversion is a nop when we bifurcate the primitive class, as the VM sees a subtyping relationship.\n@@ -2298,2 +2359,9 @@\n-           types.asSuper(tree.expr.type, tree.clazz.type.tsym) == null) {\n-            code.emitop2(checkcast, checkDimension(tree.pos(), tree.clazz.type), PoolWriter::putClass);\n+            (!tree.clazz.type.isReferenceProjection() || !types.isSameType(tree.clazz.type.valueProjection(), tree.expr.type) || true) &&\n+           !types.isSubtype(tree.expr.type, tree.clazz.type)) {\n+            checkDimension(tree.pos(), tree.clazz.type);\n+            if (tree.clazz.type.isPrimitiveClass()) {\n+                code.emitop2(checkcast, new ConstantPoolQType(tree.clazz.type, types), PoolWriter::putClass);\n+            } else {\n+                code.emitop2(checkcast, tree.clazz.type, PoolWriter::putClass);\n+            }\n+\n@@ -2361,1 +2429,1 @@\n-            code.emitLdc((LoadableConstant)checkDimension(tree.pos(), tree.selected.type));\n+            code.emitLdc((LoadableConstant) tree.selected.type, makeRef(tree.pos(), tree.selected.type, tree.selected.type.isPrimitiveClass()));\n@@ -2364,1 +2432,1 @@\n-       }\n+        }\n@@ -2420,0 +2488,12 @@\n+    public void visitDefaultValue(JCDefaultValue tree) {\n+        if (tree.type.isValueClass()) {\n+            code.emitop2(aconst_init, checkDimension(tree.pos(), tree.type), PoolWriter::putClass);\n+        } else if (tree.type.isReference()) {\n+            code.emitop0(aconst_null);\n+        } else {\n+            code.emitop0(zero(Code.typecode(tree.type)));\n+        }\n+        result = items.makeStackItem(tree.type);\n+        return;\n+    }\n+\n@@ -2476,0 +2556,1 @@\n+            cdef = transValues.translateTopLevelClass(cdef, make);\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/jvm\/Gen.java","additions":92,"deletions":11,"binary":false,"changes":103,"status":"modified"},{"patch":"@@ -108,0 +108,2 @@\n+            new UnknownProfileData(this, config.dataLayoutArrayLoadStoreDataTag),\n+            new UnknownProfileData(this, config.dataLayoutACmpDataTag),\n","filename":"src\/jdk.internal.vm.ci\/share\/classes\/jdk\/vm\/ci\/hotspot\/HotSpotMethodData.java","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -100,1 +100,1 @@\n-    final int instanceKlassMiscFlagsOffset = getFieldOffset(\"InstanceKlass::_misc_flags._flags\", Integer.class, \"u2\");\n+    final int instanceKlassMiscFlagsOffset = getFieldOffset(\"InstanceKlass::_misc_flags._flags\", Integer.class, \"u4\");\n@@ -305,0 +305,2 @@\n+    final int dataLayoutArrayLoadStoreDataTag = getConstant(\"DataLayout::array_load_store_data_tag\", Integer.class);\n+    final int dataLayoutACmpDataTag = getConstant(\"DataLayout::acmp_data_tag\", Integer.class);\n","filename":"src\/jdk.internal.vm.ci\/share\/classes\/jdk\/vm\/ci\/hotspot\/HotSpotVMConfig.java","additions":3,"deletions":1,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -28,0 +28,1 @@\n+import jdk.internal.value.PrimitiveClass;\n@@ -33,1 +34,0 @@\n-import java.lang.invoke.MethodHandles;\n@@ -654,0 +654,3 @@\n+        if (PrimitiveClass.isPrimitiveClass(f.getDeclaringClass())) {\n+            throw new UnsupportedOperationException(\"can't get field offset on a primitive class: \" + f);\n+        }\n@@ -693,0 +696,3 @@\n+        if (PrimitiveClass.isPrimitiveClass(f.getDeclaringClass())) {\n+            throw new UnsupportedOperationException(\"can't get static field offset on a primitive class: \" + f);\n+        }\n@@ -724,0 +730,3 @@\n+        if (PrimitiveClass.isPrimitiveClass(f.getDeclaringClass())) {\n+            throw new UnsupportedOperationException(\"can't get base address on a primitive class: \" + f);\n+        }\n","filename":"src\/jdk.unsupported\/share\/classes\/sun\/misc\/Unsafe.java","additions":10,"deletions":1,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -94,0 +94,1 @@\n+runtime\/cds\/appcds\/redefineClass\/RedefineRunningMethods_Shared.java  8304168 generic-all\n@@ -115,0 +116,5 @@\n+# Valhalla\n+runtime\/AccModule\/ConstModule.java 8294051 generic-all\n+runtime\/valhalla\/inlinetypes\/InlineOops.java#ZGen 8313607 linux-aarch64,macosx-aarch64\n+runtime\/cds\/CDSMapTest.java 8314993 generic-all\n+\n@@ -136,0 +142,29 @@\n+# Valhalla TODO:\n+serviceability\/jvmti\/Valhalla\/HeapDump\/HeapDump.java 8317416 generic-all\n+\n+serviceability\/sa\/ClhsdbCDSCore.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbCDSJstackPrintAll.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbFindPC.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbInspect.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbLongConstant.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbJdis.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbJstack.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbPrintAll.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbPrintAs.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbPrintStatics.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbSource.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbSymbol.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbWhere.java 8190936 generic-all\n+serviceability\/sa\/JhsdbThreadInfoTest.java 8190936 generic-all\n+serviceability\/sa\/TestClassDump.java 8190936 generic-all\n+serviceability\/sa\/TestClhsdbJstackLock.java 8190936 generic-all\n+serviceability\/sa\/TestCpoolForInvokeDynamic.java 8190936 generic-all\n+serviceability\/sa\/TestHeapDumpForInvokeDynamic.java 8190936 generic-all\n+serviceability\/sa\/TestHeapDumpForLargeArray.java 8190936 generic-all\n+serviceability\/sa\/TestIntConstant.java 8190936 generic-all\n+serviceability\/sa\/TestJhsdbJstackLock.java 8190936 generic-all\n+serviceability\/sa\/TestJmapCore.java 8190936 generic-all\n+serviceability\/sa\/TestJmapCoreMetaspace.java 8190936 generic-all\n+serviceability\/sa\/TestPrintMdo.java 8190936 generic-all\n+serviceability\/sa\/jmap-hprof\/JMapHProfLargeHeapTest.java 8190936 generic-all\n+\n@@ -177,0 +212,2 @@\n+\n+vmTestbase\/vm\/mlvm\/hiddenloader\/stress\/byteMutation\/Test.java 8317172 generic-all\n","filename":"test\/hotspot\/jtreg\/ProblemList.txt","additions":37,"deletions":0,"binary":false,"changes":37,"status":"modified"},{"patch":"@@ -189,0 +189,1 @@\n+            \"-XX:-InlineTypeReturnedAsFields\", \/\/ TODO Remove this once 8284443 fixed handling of unloaded return types\n","filename":"test\/hotspot\/jtreg\/compiler\/c2\/unloaded\/TestInlineUnloaded.java","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -279,1 +279,1 @@\n-        String optoRegex = \"(.*precise .*\\\\R((.*(?i:mov|mv|xorl|nop|spill).*|\\\\s*)\\\\R)*.*(?i:call,static).*wrapper for: _new_instance_Java\" + END;\n+        String optoRegex = \"(.*precise .*\\\\R((.*(?i:mov|mv|xorl|nop|spill|pushq|popq).*|\\\\s*)\\\\R)*.*(?i:call,static).*wrapper for: _new_instance_Java\" + END;\n@@ -285,1 +285,1 @@\n-        String regex = \"(.*precise .*\" + IS_REPLACED + \":.*\\\\R((.*(?i:mov|mv|xorl|nop|spill).*|\\\\s*)\\\\R)*.*(?i:call,static).*wrapper for: _new_instance_Java\" + END;\n+        String regex = \"(.*precise .*\" + IS_REPLACED + \":.*\\\\R((.*(?i:mov|mv|xorl|nop|spill|pushq|popq).*|\\\\s*)\\\\R)*.*(?i:call,static).*wrapper for: _new_instance_Java\" + END;\n@@ -291,1 +291,1 @@\n-        String optoRegex = \"(.*precise \\\\[.*\\\\R((.*(?i:mov|mv|xor|nop|spill).*|\\\\s*|.*(LGHI|LI).*)\\\\R)*.*(?i:call,static).*wrapper for: _new_array_Java\" + END;\n+        String optoRegex = \"(.*precise \\\\[.*\\\\R((.*(?i:mov|mv|xor|nop|spill|pushq|popq).*|\\\\s*|.*(LGHI|LI).*)\\\\R)*.*(?i:call,static).*wrapper for: _new_array_Java\" + END;\n@@ -297,1 +297,1 @@\n-        String regex = \"(.*precise \\\\[.*\" + IS_REPLACED + \":.*\\\\R((.*(?i:mov|mv|xorl|nop|spill).*|\\\\s*|.*(LGHI|LI).*)\\\\R)*.*(?i:call,static).*wrapper for: _new_array_Java\" + END;\n+        String regex = \"(.*precise \\\\[.*\" + IS_REPLACED + \":.*\\\\R((.*(?i:mov|mv|xorl|nop|spill|pushq|popq).*|\\\\s*|.*(LGHI|LI).*)\\\\R)*.*(?i:call,static).*wrapper for: _new_array_Java\" + END;\n@@ -544,0 +544,5 @@\n+    public static final String INLINE_TYPE = PREFIX + \"INLINE_TYPE\" + POSTFIX;\n+    static {\n+        beforeMatchingNameRegex(INLINE_TYPE, \"InlineType\");\n+    }\n+\n@@ -2160,1 +2165,1 @@\n-    private static void beforeMatching(String irNodePlaceholder, String regex) {\n+    public static void beforeMatching(String irNodePlaceholder, String regex) {\n@@ -2206,1 +2211,1 @@\n-    private static void optoOnly(String irNodePlaceholder, String regex) {\n+    public static void optoOnly(String irNodePlaceholder, String regex) {\n","filename":"test\/hotspot\/jtreg\/compiler\/lib\/ir_framework\/IRNode.java","additions":11,"deletions":6,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -704,0 +704,4 @@\n+com\/sun\/jdi\/cds\/CDSBreakpointTest.java                          8304168 generic-all\n+com\/sun\/jdi\/cds\/CDSDeleteAllBkptsTest.java                      8304168 generic-all\n+com\/sun\/jdi\/cds\/CDSFieldWatchpoints.java                        8304168 generic-all\n+\n@@ -757,0 +761,6 @@\n+jdk\/classfile\/SwapTest.java                                     8308778 generic-all\n+jdk\/classfile\/LowAdaptTest.java                                 8308778 generic-all\n+jdk\/classfile\/BuilderBlockTest.java                             8308778 generic-all\n+jdk\/classfile\/BuilderTryCatchTest.java                          8308778 generic-all\n+jdk\/classfile\/PrimitiveClassConstantTest.java                   8310649 generic-all\n+\n@@ -796,0 +806,4 @@\n+\n+# valhalla\n+valhalla\/valuetypes\/ObjectMethods.java 8317150 generic-all\n+valhalla\/valuetypes\/SubstitutabilityTest.java 8317151 generic-all\n","filename":"test\/jdk\/ProblemList.txt","additions":14,"deletions":0,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -56,1 +56,1 @@\n-        { anonClasses.put(getClass().getName(), 0); }\n+        { anonClasses.put(getClass().getName(), Classfile.ACC_IDENTITY); }\n@@ -61,1 +61,1 @@\n-            { anonClasses.put(getClass().getName(), 0); }\n+            { anonClasses.put(getClass().getName(), Classfile.ACC_IDENTITY); }\n@@ -69,1 +69,1 @@\n-            { anonClasses.put(getClass().getName(), 0); }\n+            { anonClasses.put(getClass().getName(), Classfile.ACC_IDENTITY); }\n@@ -74,1 +74,1 @@\n-        { anonClasses.put(getClass().getName(), 0); }\n+        { anonClasses.put(getClass().getName(), Classfile.ACC_IDENTITY); }\n@@ -79,1 +79,1 @@\n-            { anonClasses.put(getClass().getName(), 0); }\n+            { anonClasses.put(getClass().getName(), Classfile.ACC_IDENTITY); }\n@@ -87,1 +87,1 @@\n-            { anonClasses.put(getClass().getName(), 0); }\n+            { anonClasses.put(getClass().getName(), Classfile.ACC_IDENTITY); }\n@@ -112,2 +112,2 @@\n-                   Classfile.ACC_SYNTHETIC | Classfile.ACC_ANNOTATION | Classfile.ACC_ENUM;\n-        int classExpected = (expected & mask) | Classfile.ACC_SUPER;\n+                   Classfile.ACC_SYNTHETIC | Classfile.ACC_ANNOTATION | Classfile.ACC_ENUM | Classfile.ACC_IDENTITY;\n+        int classExpected = (expected & mask);\n","filename":"test\/langtools\/tools\/javac\/AnonymousClass\/AnonymousClassFlags.java","additions":8,"deletions":8,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -53,0 +53,5 @@\n+            if ((cf.flags().flagsMask() & (Classfile.ACC_SYNTHETIC | Classfile.ACC_VALUE | Classfile.ACC_ABSTRACT)) == Classfile.ACC_SYNTHETIC) {\n+                if ((cf.flags().flagsMask() & Classfile.ACC_IDENTITY) == 0) {\n+                    throw new IllegalStateException(\"Missing ACC_IDENTITY on synthetic concrete identity class: \" + cf.thisClass().asInternalName());\n+                }\n+            }\n","filename":"test\/langtools\/tools\/javac\/classfiles\/InnerClasses\/SyntheticClasses.java","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2010, 2018, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2010, 2019, Oracle and\/or its affiliates. All rights reserved.\n","filename":"test\/langtools\/tools\/javac\/diags\/CheckResourceKeys.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -107,1 +107,1 @@\n-            \"clone\", \"finalize\", \"getClass\", \"hashCode\",\n+            \"clone\", \"finalize\", \"getClass\", \"hashCode\", \"isValueObject\",\n","filename":"test\/langtools\/tools\/javac\/records\/RecordCompilationTests.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2017, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -76,0 +76,3 @@\n+    \/\/ Size of the instance in the object of the class is inlined.\n+    \/\/ Calculated lazily.\n+    private int inlinedInstanceSize = -1;\n@@ -117,0 +120,40 @@\n+\n+        \/\/ Resolve inlined fields. Should be done before resolveSuperclass for correct field counting\n+        Snapshot.ClassInlinedFields[] inlinedFields = snapshot.findClassInlinedFields(id);\n+        if (inlinedFields != null) {\n+            int newCount = fields.length;\n+            for (Snapshot.ClassInlinedFields f: inlinedFields) {\n+                if (f.synthFieldCount == 0) {\n+                    \/\/ Empty primitive class. just skip it - no data there.\n+                    continue;\n+                }\n+                JavaHeapObject clazz = snapshot.findThing(f.fieldClassID);\n+                if (clazz instanceof JavaClass fieldClass) {\n+                    fieldClass.resolve(snapshot);\n+\n+                    \/\/ Set new field.\n+                    fields[f.fieldIndex] = new InlinedJavaField(f.fieldName, 'Q' + fieldClass.getName() + ';', fieldClass);\n+                    newCount -= (f.synthFieldCount - 1);\n+                    \/\/ Reset invalid fields.\n+                    for (int i = 1; i < f.synthFieldCount; i++) {\n+                        fields[f.fieldIndex + i] = null;\n+                    }\n+                } else {\n+                    \/\/ The field class not found.\n+                    System.out.println(\"WARNING: class of inlined field not found:\" + getName() + \".\" + f.fieldName);\n+                }\n+            }\n+\n+            \/\/ Set new fields.\n+            JavaField[] newFields = new JavaField[newCount];\n+            int oldIndex = 0;\n+            for (int i = 0; i < newFields.length; i++) {\n+                while (fields[oldIndex] == null) {\n+                    oldIndex++;\n+                }\n+                newFields[i] = fields[oldIndex];\n+                oldIndex++;\n+            }\n+            fields = newFields;\n+        }\n+\n@@ -131,0 +174,1 @@\n+\n@@ -380,0 +424,43 @@\n+    public int getInlinedInstanceSize() {\n+        if (inlinedInstanceSize < 0) {\n+            int size = 0;\n+            for (JavaField f: fields) {\n+                if (f instanceof InlinedJavaField inlinedField) {\n+                    size += inlinedField.getInlinedFieldClass().getInlinedInstanceSize();\n+                } else {\n+                    char sig = f.getSignature().charAt(0);\n+                    switch (sig) {\n+                        case 'Q': {\n+                            System.out.println(\"WARNING: (getInlinedInstanceSize) field \"\n+                                    + getClazz().getName() + \".\" + f.getName()\n+                                    + \" is not inlined, but has Q-signature: \" + f.getSignature());\n+                        } \/\/ continue as 'L' object\n+                        case 'L':\n+                        case '[':\n+                            size += mySnapshot.getIdentifierSize();\n+                            break;\n+                        case 'B':\n+                        case 'Z':\n+                            size += 1;\n+                            break;\n+                        case 'C':\n+                        case 'S':\n+                            size += 2;\n+                            break;\n+                        case 'I':\n+                        case 'F':\n+                            size += 4;\n+                            break;\n+                        case 'J':\n+                        case 'D':\n+                            size += 8;\n+                            break;\n+                        default:\n+                            throw new RuntimeException(\"unknown field type: \" + sig);\n+                    }\n+                }\n+            }\n+            inlinedInstanceSize = size;\n+        }\n+        return inlinedInstanceSize;\n+    }\n","filename":"test\/lib\/jdk\/test\/lib\/hprof\/model\/JavaClass.java","additions":88,"deletions":1,"binary":false,"changes":89,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2017, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -45,0 +45,6 @@\n+\n+    public JavaObject(Object clazz, long offset) {\n+        super(offset);\n+        this.clazz = clazz;\n+    }\n+\n@@ -52,2 +58,1 @@\n-        super(offset);\n-        this.clazz = makeId(classID);\n+        this(makeId(classID), offset);\n@@ -214,1 +219,1 @@\n-    protected final long readValueLength() throws IOException {\n+    protected long readValueLength() throws IOException {\n@@ -224,1 +229,1 @@\n-    private long dataStartOffset() {\n+    protected long dataStartOffset() {\n@@ -258,59 +263,68 @@\n-                switch (sig) {\n-                    case 'L':\n-                    case '[': {\n-                        long id = objectIdAt(offset);\n-                        offset += idSize();\n-                        JavaObjectRef ref = new JavaObjectRef(id);\n-                        fieldValues[target+fieldNo] = ref.dereference(snapshot, f, verbose);\n-                        break;\n-                    }\n-                    case 'Z': {\n-                        byte value = byteAt(offset);\n-                        offset++;\n-                        fieldValues[target+fieldNo] = new JavaBoolean(value != 0);\n-                        break;\n-                    }\n-                    case 'B': {\n-                        byte value = byteAt(offset);\n-                        offset++;\n-                        fieldValues[target+fieldNo] = new JavaByte(value);\n-                        break;\n-                    }\n-                    case 'S': {\n-                        short value = shortAt(offset);\n-                        offset += 2;\n-                        fieldValues[target+fieldNo] = new JavaShort(value);\n-                        break;\n-                    }\n-                    case 'C': {\n-                        char value = charAt(offset);\n-                        offset += 2;\n-                        fieldValues[target+fieldNo] = new JavaChar(value);\n-                        break;\n-                    }\n-                    case 'I': {\n-                        int value = intAt(offset);\n-                        offset += 4;\n-                        fieldValues[target+fieldNo] = new JavaInt(value);\n-                        break;\n-                    }\n-                    case 'J': {\n-                        long value = longAt(offset);\n-                        offset += 8;\n-                        fieldValues[target+fieldNo] = new JavaLong(value);\n-                        break;\n-                    }\n-                    case 'F': {\n-                        float value = floatAt(offset);\n-                        offset += 4;\n-                        fieldValues[target+fieldNo] = new JavaFloat(value);\n-                        break;\n-                    }\n-                    case 'D': {\n-                        double value = doubleAt(offset);\n-                        offset += 8;\n-                        fieldValues[target+fieldNo] = new JavaDouble(value);\n-                        break;\n-                    }\n-                    default:\n-                        throw new RuntimeException(\"invalid signature: \" + sig);\n+                if (f instanceof InlinedJavaField inlinedField) {\n+                    JavaClass fieldClass = inlinedField.getInlinedFieldClass();\n+                    fieldValues[target+fieldNo] = new InlinedJavaObject(fieldClass, offset);\n+                    offset += fieldClass.getInlinedInstanceSize();\n+                } else {\n+                    switch (sig) {\n+                        case 'Q': {\n+                            warn(\"(parseFields) field \" + getClazz().getName() + \".\" + f.getName()\n+                                    + \" is not inlined, but has Q-signature: \" + f.getSignature());\n+                        } \/\/ continue as 'L' object\n+                        case 'L':\n+                        case '[': {\n+                            long id = objectIdAt(offset);\n+                            offset += idSize();\n+                            JavaObjectRef ref = new JavaObjectRef(id);\n+                            fieldValues[target + fieldNo] = ref.dereference(snapshot, f, verbose);\n+                            break;\n+                        }\n+                        case 'Z': {\n+                            byte value = byteAt(offset);\n+                            offset++;\n+                            fieldValues[target + fieldNo] = new JavaBoolean(value != 0);\n+                            break;\n+                        }\n+                        case 'B': {\n+                            byte value = byteAt(offset);\n+                            offset++;\n+                            fieldValues[target + fieldNo] = new JavaByte(value);\n+                            break;\n+                        }\n+                        case 'S': {\n+                            short value = shortAt(offset);\n+                            offset += 2;\n+                            fieldValues[target + fieldNo] = new JavaShort(value);\n+                            break;\n+                        }\n+                        case 'C': {\n+                            char value = charAt(offset);\n+                            offset += 2;\n+                            fieldValues[target + fieldNo] = new JavaChar(value);\n+                            break;\n+                        }\n+                        case 'I': {\n+                            int value = intAt(offset);\n+                            offset += 4;\n+                            fieldValues[target + fieldNo] = new JavaInt(value);\n+                            break;\n+                        }\n+                        case 'J': {\n+                            long value = longAt(offset);\n+                            offset += 8;\n+                            fieldValues[target + fieldNo] = new JavaLong(value);\n+                            break;\n+                        }\n+                        case 'F': {\n+                            float value = floatAt(offset);\n+                            offset += 4;\n+                            fieldValues[target + fieldNo] = new JavaFloat(value);\n+                            break;\n+                        }\n+                        case 'D': {\n+                            double value = doubleAt(offset);\n+                            offset += 8;\n+                            fieldValues[target + fieldNo] = new JavaDouble(value);\n+                            break;\n+                        }\n+                        default:\n+                            throw new RuntimeException(\"invalid signature: \" + sig);\n@@ -318,0 +332,1 @@\n+                    }\n@@ -319,4 +334,4 @@\n-        } catch (IOException exp) {\n-            System.err.println(\"lazy read failed at offset \" + offset);\n-            exp.printStackTrace();\n-            return Snapshot.EMPTY_JAVATHING_ARRAY;\n+            } catch (IOException exp) {\n+                System.err.println(\"lazy read failed at offset \" + offset);\n+                exp.printStackTrace();\n+                return Snapshot.EMPTY_JAVATHING_ARRAY;\n","filename":"test\/lib\/jdk\/test\/lib\/hprof\/model\/JavaObject.java","additions":83,"deletions":68,"binary":false,"changes":151,"status":"modified"},{"patch":"@@ -37,1 +37,2 @@\n- * An array of values, that is, an array of ints, boolean, floats or the like.\n+ * An array of values, that is, an array of ints, boolean, floats, etc.\n+ * or flat array of primitive objects.\n@@ -44,2 +45,2 @@\n-    private static String arrayTypeName(byte sig) {\n-        switch (sig) {\n+    private static int elementSize(byte type) {\n+        switch (type) {\n@@ -47,2 +48,1 @@\n-                return \"byte[]\";\n-                return \"boolean[]\";\n+                return 1;\n@@ -51,2 +51,1 @@\n-                return \"char[]\";\n-                return \"short[]\";\n+                return 2;\n@@ -55,2 +54,1 @@\n-                return \"int[]\";\n-                return \"float[]\";\n+                return 4;\n@@ -59,20 +57,0 @@\n-                return \"long[]\";\n-                return \"double[]\";\n-            default:\n-                throw new RuntimeException(\"invalid array element sig: \" + sig);\n-        }\n-    }\n-\n-    private static int elementSize(byte type) {\n-        switch (type) {\n-            case T_BYTE:\n-            case T_BOOLEAN:\n-                return 1;\n-            case T_CHAR:\n-            case T_SHORT:\n-                return 2;\n-            case T_INT:\n-            case T_FLOAT:\n-                return 4;\n-            case T_LONG:\n-            case T_DOUBLE:\n@@ -102,1 +80,1 @@\n-        return len * elementSize(getElementType());\n+        return len * elementSize(getRealElementType());\n@@ -174,0 +152,7 @@\n+                case 'Q': {\n+                    for (int i = 0; i < len; i++) {\n+                        res[i] = new InlinedJavaObject(flatArrayElementClass, offset);\n+                        offset += flatArrayElementClass.getInlinedInstanceSize();\n+                    }\n+                    return res;\n+                }\n@@ -184,0 +169,2 @@\n+    private long objID;\n+\n@@ -199,1 +186,4 @@\n-    public JavaValueArray(byte elementSignature, long offset) {\n+    \/\/ Flat array support.\n+    private JavaClass flatArrayElementClass;\n+\n+    public JavaValueArray(long id, byte elementSignature, long offset) {\n@@ -201,0 +191,1 @@\n+        this.objID = id;\n@@ -208,0 +199,8 @@\n+    public boolean isFlatArray() {\n+        return flatArrayElementClass != null;\n+    }\n+\n+    public JavaClass getFlatElementClazz() {\n+        return flatArrayElementClass;\n+    }\n+\n@@ -216,4 +215,17 @@\n-        byte elementSig = getElementType();\n-        clazz = snapshot.findClass(arrayTypeName(elementSig));\n-        if (clazz == null) {\n-            clazz = snapshot.getArrayClass(\"\" + ((char) elementSig));\n+\n+        byte elementType = getElementType();\n+        String elementSig = \"\" + (char)elementType;\n+        \/\/ Check if this is a flat array of primitive objects.\n+        Number elementClassID = snapshot.findFlatArrayElementType(objID);\n+        if (elementClassID != null) {\n+            \/\/ This is flat array.\n+            JavaHeapObject elementClazz = snapshot.findThing(getIdValue(elementClassID));\n+            if (elementClazz instanceof JavaClass elementJavaClazz) {\n+                flatArrayElementClass = elementJavaClazz;\n+                \/\/ need to resolve the element class\n+                flatArrayElementClass.resolve(snapshot);\n+                elementSig = \"Q\" + flatArrayElementClass.getName() + \";\";\n+            } else {\n+                \/\/ The class not found.\n+                System.out.println(\"WARNING: flat array element class not found\");\n+            }\n@@ -221,0 +233,1 @@\n+        clazz = snapshot.getArrayClass(elementSig);\n@@ -246,0 +259,3 @@\n+            case 'Q':\n+                divider = flatArrayElementClass.getInlinedInstanceSize();\n+                break;\n@@ -260,0 +276,4 @@\n+        return isFlatArray() ? (byte)'Q' : getRealElementType();\n+    }\n+\n+    private byte getRealElementType() {\n@@ -282,1 +302,1 @@\n-        if (elementSignature == 'C')  {\n+        if (elementSignature == 'C' && !isFlatArray())  {\n@@ -341,0 +361,4 @@\n+                    case 'Q': {\n+                        InlinedJavaObject obj = (InlinedJavaObject)things[i];\n+                        result.append(obj);\n+                    }\n","filename":"test\/lib\/jdk\/test\/lib\/hprof\/model\/JavaValueArray.java","additions":60,"deletions":36,"binary":false,"changes":96,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2017, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -72,0 +72,4 @@\n+    private Map<Number, Number> flatArrays = new HashMap<>();\n+\n+    private Map<Number, ClassInlinedFields[]> inlinedFields = new HashMap<>();\n+\n@@ -205,0 +209,35 @@\n+    public void addFlatArray(long objID, long elementClassID) {\n+        flatArrays.put(makeId(objID), makeId(elementClassID));\n+    }\n+\n+    \/**\n+     * @return null if the array is not flat array\n+     *\/\n+    Number findFlatArrayElementType(long arrayObjectID) {\n+        return flatArrays.get(makeId(arrayObjectID));\n+    }\n+\n+    public static class ClassInlinedFields {\n+        final int fieldIndex;\n+        final int synthFieldCount;\n+        final String fieldName;\n+        final long fieldClassID;\n+\n+        public ClassInlinedFields(int fieldIndex, int synthFieldCount, String fieldName, long fieldClassID) {\n+            this.fieldIndex = fieldIndex;\n+            this.synthFieldCount = synthFieldCount;\n+            this.fieldName = fieldName;\n+            this.fieldClassID =  fieldClassID;\n+        }\n+    }\n+\n+    public void addClassInlinedFields(long classID, ClassInlinedFields[] fields) {\n+        inlinedFields.put(makeId(classID), fields);\n+    }\n+\n+    \/**\n+     * @return null if the class has no inlined fields\n+     *\/\n+    ClassInlinedFields[] findClassInlinedFields(long classID) {\n+        return inlinedFields.get(makeId(classID));\n+    }\n","filename":"test\/lib\/jdk\/test\/lib\/hprof\/model\/Snapshot.java","additions":40,"deletions":1,"binary":false,"changes":41,"status":"modified"},{"patch":"@@ -99,0 +99,6 @@\n+    static final int HPROF_FLAT_ARRAYS          = 0x12;\n+    static final int HPROF_INLINED_FIELDS       = 0x13;\n+\n+    static final int HPROF_FLAT_ARRAY           = 0x01;\n+    static final int HPROF_CLASS_WITH_INLINED_FIELDS = 0x01;\n+\n@@ -245,1 +251,1 @@\n-                    if (dumpsToSkip <= 0) {\n+                    if (dumpsToSkip == 0) {\n@@ -254,1 +260,0 @@\n-                        return snapshot;\n@@ -264,3 +269,3 @@\n-                        if (dumpsToSkip <= 0) {\n-                            skipBytes(length);  \/\/ should be no-op\n-                            return snapshot;\n+                        if (dumpsToSkip == 0) {\n+                            \/\/ update dumpsToSkip to skip other dumps\n+                            dumpsToSkip--;\n@@ -281,1 +286,1 @@\n-                        if (dumpsToSkip <= 0) {\n+                        if (dumpsToSkip == 0) {\n@@ -355,0 +360,9 @@\n+                case HPROF_FLAT_ARRAYS: {\n+                    readFlatArrays(length);\n+                    break;\n+                }\n+                case HPROF_INLINED_FIELDS: {\n+                    readInlinedFields(length);\n+                    break;\n+                }\n+\n@@ -853,1 +867,1 @@\n-            JavaValueArray va = new JavaValueArray(primitiveSignature, start);\n+            JavaValueArray va = new JavaValueArray(id, primitiveSignature, start);\n@@ -903,0 +917,52 @@\n+    private void readFlatArrays(long length) throws IOException {\n+        while (length > 0) {\n+            byte tag = in.readByte();\n+            length--;\n+            switch (tag) {\n+                case HPROF_FLAT_ARRAY: {\n+                    long objId = readID();\n+                    length -= identifierSize;\n+                    long elementClassId = readID();\n+                    length -= identifierSize;\n+                    snapshot.addFlatArray(objId, elementClassId);\n+                    break;\n+                }\n+                default: {\n+                    throw new IOException(\"Invalid tag \" + tag);\n+                }\n+            }\n+        }\n+    }\n+\n+    private void readInlinedFields(long length) throws IOException {\n+        while (length > 0) {\n+            byte tag = in.readByte();\n+            length--;\n+            switch (tag) {\n+                case HPROF_CLASS_WITH_INLINED_FIELDS: {\n+                    long classID = readID();\n+                    length -= identifierSize;\n+                    int fieldNum = in.readUnsignedShort();\n+                    length -= 2;\n+                    Snapshot.ClassInlinedFields[] fields = new Snapshot.ClassInlinedFields[fieldNum];\n+                    for (int i = 0; i < fieldNum; i++) {\n+                        int fieldIndex = in.readUnsignedShort();\n+                        length -= 2;\n+                        int synthFieldCount = in.readUnsignedShort();\n+                        length -= 2;\n+                        String fieldName = getNameFromID(readID());\n+                        length -= identifierSize;\n+                        long fieldClassId = readID();\n+                        length -= identifierSize;\n+                        fields[i] = new Snapshot.ClassInlinedFields(fieldIndex, synthFieldCount, fieldName, fieldClassId);\n+                    }\n+                    snapshot.addClassInlinedFields(classID, fields);\n+                    break;\n+                }\n+                default: {\n+                    throw new IOException(\"Invalid tag \" + tag);\n+                }\n+            }\n+        }\n+    }\n+\n","filename":"test\/lib\/jdk\/test\/lib\/hprof\/parser\/HprofReader.java","additions":73,"deletions":7,"binary":false,"changes":80,"status":"modified"}]}