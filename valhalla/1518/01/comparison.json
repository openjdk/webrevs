{"files":[{"patch":"@@ -388,0 +388,2 @@\n+macro(LoadFlat)\n+macro(StoreFlat)\n","filename":"src\/hotspot\/share\/opto\/classes.hpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -411,0 +411,3 @@\n+  if (dead->Opcode() == Op_LoadFlat || dead->Opcode() == Op_StoreFlat) {\n+    remove_flat_access(dead);\n+  }\n@@ -470,0 +473,1 @@\n+  remove_useless_nodes(_flat_access_nodes, useful);  \/\/ remove useless flat access nodes\n@@ -677,0 +681,1 @@\n+      _flat_access_nodes(comp_arena(), 8, 0, nullptr),\n@@ -2083,0 +2088,29 @@\n+void Compile::add_flat_access(Node* n) {\n+  assert(n != nullptr && (n->Opcode() == Op_LoadFlat || n->Opcode() == Op_StoreFlat), \"unexpected node %s\", n == nullptr ? \"nullptr\" : n->Name());\n+  assert(!_flat_access_nodes.contains(n), \"duplicate insertion\");\n+  _flat_access_nodes.push(n);\n+}\n+\n+void Compile::remove_flat_access(Node* n) {\n+  assert(n != nullptr && (n->Opcode() == Op_LoadFlat || n->Opcode() == Op_StoreFlat), \"unexpected node %s\", n == nullptr ? \"nullptr\" : n->Name());\n+  _flat_access_nodes.remove_if_existing(n);\n+}\n+\n+void Compile::process_flat_accesses(PhaseIterGVN& igvn) {\n+  assert(igvn._worklist.size() == 0, \"should be empty\");\n+  igvn.set_delay_transform(true);\n+  for (int i = _flat_access_nodes.length() - 1; i >= 0; i--) {\n+    Node* n = _flat_access_nodes.at(i);\n+    assert(n != nullptr, \"unexpected nullptr\");\n+    if (n->Opcode() == Op_LoadFlat) {\n+      static_cast<LoadFlatNode*>(n)->expand_atomic(igvn);\n+    } else {\n+      assert(n->Opcode() == Op_StoreFlat, \"unexpected node %s\", n->Name());\n+      static_cast<StoreFlatNode*>(n)->expand_atomic(igvn);\n+    }\n+  }\n+  _flat_access_nodes.clear_and_deallocate();\n+  igvn.set_delay_transform(false);\n+  igvn.optimize();\n+}\n+\n@@ -2931,0 +2965,5 @@\n+  process_flat_accesses(igvn);\n+  if (failing()) {\n+    return;\n+  }\n+\n","filename":"src\/hotspot\/share\/opto\/compile.cpp","additions":39,"deletions":0,"binary":false,"changes":39,"status":"modified"},{"patch":"@@ -47,0 +47,1 @@\n+#include \"utilities\/growableArray.hpp\"\n@@ -386,0 +387,1 @@\n+  GrowableArray<Node*>  _flat_access_nodes;     \/\/ List of LoadFlat and StoreFlat nodes\n@@ -794,0 +796,11 @@\n+  void add_flat_access(Node* n);\n+  void remove_flat_access(Node* n);\n+  void process_flat_accesses(PhaseIterGVN& igvn);\n+\n+  template <class F>\n+  void for_each_flat_access(F consumer) {\n+    for (int i = _flat_access_nodes.length() - 1; i >= 0; i--) {\n+      consumer(_flat_access_nodes.at(i));\n+    }\n+  }\n+\n","filename":"src\/hotspot\/share\/opto\/compile.hpp","additions":13,"deletions":0,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -42,0 +42,1 @@\n+#include \"opto\/opcodes.hpp\"\n@@ -428,1 +429,9 @@\n-  \/\/ 6. Reduce allocation merges used as debug information. This is done after\n+  \/\/ 6. Expand flat accesses if the object does not escape. This adds nodes to\n+  \/\/ the graph, so it has to be after split_unique_types. This expands atomic\n+  \/\/ mismatched accesses (though encapsulated in LoadFlats and StoreFlats) into\n+  \/\/ non-mismatched accesses, so it is better before reduce allocation merges.\n+  if (has_non_escaping_obj) {\n+    optimize_flat_accesses();\n+  }\n+\n+  \/\/ 7. Reduce allocation merges used as debug information. This is done after\n@@ -1682,0 +1691,8 @@\n+    case Op_LoadFlat:\n+      \/\/ Treat LoadFlat similar to an unknown call that receives nothing and produces its results\n+      map_ideal_node(n, phantom_obj);\n+      break;\n+    case Op_StoreFlat:\n+      \/\/ Treat StoreFlat similar to a call that escapes the stored flattened fields\n+      delayed_worklist->push(n);\n+      break;\n@@ -1689,0 +1706,3 @@\n+      } else if (n->as_Proj()->_con >= TypeFunc::Parms && n->in(0)->Opcode() == Op_LoadFlat && igvn->type(n)->isa_ptr()) {\n+        \/\/ Treat LoadFlat outputs similar to a call return value\n+        add_local_var_and_edge(n, PointsToNode::NoEscape, n->in(0), delayed_worklist);\n@@ -1773,1 +1793,1 @@\n-  assert(n->is_Store() || n->is_LoadStore() ||\n+  assert(n->is_Store() || n->is_LoadStore() || n->Opcode() == Op_StoreFlat ||\n@@ -1842,0 +1862,16 @@\n+    case Op_StoreFlat: {\n+      \/\/ StoreFlat globally escapes its stored flattened fields\n+      InlineTypeNode* value = static_cast<StoreFlatNode*>(n)->value();\n+      ciInlineKlass* vk = _igvn->type(value)->inline_klass();\n+      for (int i = 0; i < vk->nof_nonstatic_fields(); i++) {\n+        ciField* field = vk->nonstatic_field_at(i);\n+        if (field->type()->is_primitive_type()) {\n+          continue;\n+        }\n+\n+        Node* field_value = value->field_value_by_offset(field->offset_in_bytes(), true);\n+        PointsToNode* field_value_ptn = ptnode_adr(field_value->_idx);\n+        set_escape_state(field_value_ptn, PointsToNode::GlobalEscape, \"store into a flat field\");\n+      }\n+      break;\n+    }\n@@ -1843,4 +1879,9 @@\n-      \/\/ we are only interested in the oop result projection from a call\n-      assert((n->as_Proj()->_con == TypeFunc::Parms && n->in(0)->as_Call()->returns_pointer()) ||\n-             n->in(0)->as_Call()->tf()->returns_inline_type_as_fields(), \"what kind of oop return is it?\");\n-      add_local_var_and_edge(n, PointsToNode::NoEscape, n->in(0), nullptr);\n+      if (n->in(0)->is_Call()) {\n+        \/\/ we are only interested in the oop result projection from a call\n+        assert((n->as_Proj()->_con == TypeFunc::Parms && n->in(0)->as_Call()->returns_pointer()) ||\n+              n->in(0)->as_Call()->tf()->returns_inline_type_as_fields(), \"what kind of oop return is it?\");\n+        add_local_var_and_edge(n, PointsToNode::NoEscape, n->in(0), nullptr);\n+      } else if (n->in(0)->Opcode() == Op_LoadFlat) {\n+        \/\/ Treat LoadFlat outputs similar to a call return value\n+        add_local_var_and_edge(n, PointsToNode::NoEscape, n->in(0), nullptr);\n+      }\n@@ -3345,0 +3386,29 @@\n+\/\/ Atomic flat accesses on non-escape objects can be optimized to non-atomic accesses\n+void ConnectionGraph::optimize_flat_accesses() {\n+  PhaseIterGVN& igvn = *_igvn;\n+  bool delay = igvn.delay_transform();\n+  igvn.set_delay_transform(true);\n+  igvn.C->for_each_flat_access([&](Node* n) {\n+    Node* ptr = n->in(TypeFunc::Parms);\n+    if (!ptr->is_AddP()) {\n+      return;\n+    }\n+\n+    if (!not_global_escape(ptr->as_AddP()->base_node())) {\n+      return;\n+    }\n+\n+    bool expanded;\n+    if (n->Opcode() == Op_LoadFlat) {\n+      expanded = static_cast<LoadFlatNode*>(n)->expand_non_atomic(igvn);\n+    } else {\n+      assert(n->Opcode() == Op_StoreFlat, \"unexpected node %s\", n->Name());\n+      expanded = static_cast<StoreFlatNode*>(n)->expand_non_atomic(igvn);\n+    }\n+    if (expanded) {\n+      igvn.C->remove_flat_access(n);\n+    }\n+  });\n+  igvn.set_delay_transform(delay);\n+}\n+\n@@ -4712,0 +4782,2 @@\n+      } else if (use->Opcode() == Op_LoadFlat || use->Opcode() == Op_StoreFlat) {\n+        \/\/ These nodes consume and output whole memory states so there is nothing to do here\n","filename":"src\/hotspot\/share\/opto\/escape.cpp","additions":78,"deletions":6,"binary":false,"changes":84,"status":"modified"},{"patch":"@@ -484,0 +484,2 @@\n+  \/\/ Expand flat accesses to accesses to each component if the object does not escape\n+  void optimize_flat_accesses();\n","filename":"src\/hotspot\/share\/opto\/escape.hpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -32,0 +32,1 @@\n+#include \"opto\/compile.hpp\"\n@@ -35,0 +36,1 @@\n+#include \"opto\/memnode.hpp\"\n@@ -36,0 +38,1 @@\n+#include \"opto\/multnode.hpp\"\n@@ -490,145 +493,1 @@\n-\/\/ Get a field value from the payload by shifting it according to the offset\n-static Node* get_payload_value(PhaseGVN* gvn, Node* payload, BasicType bt, BasicType val_bt, int offset) {\n-  \/\/ Shift to the right position in the long value\n-  assert((offset + type2aelembytes(val_bt)) <= type2aelembytes(bt), \"Value does not fit into payload\");\n-  Node* value = nullptr;\n-  Node* shift_val = gvn->intcon(offset << LogBitsPerByte);\n-  if (bt == T_LONG) {\n-    value = gvn->transform(new URShiftLNode(payload, shift_val));\n-    value = gvn->transform(new ConvL2INode(value));\n-  } else {\n-    value = gvn->transform(new URShiftINode(payload, shift_val));\n-  }\n-\n-  if (val_bt == T_INT || val_bt == T_OBJECT || val_bt == T_ARRAY) {\n-    return value;\n-  } else {\n-    \/\/ Make sure to zero unused bits in the 32-bit value\n-    return Compile::narrow_value(val_bt, value, nullptr, gvn, true);\n-  }\n-}\n-\n-\/\/ Convert a payload value to field values\n-void InlineTypeNode::convert_from_payload(GraphKit* kit, BasicType bt, Node* payload, int holder_offset, bool null_free, bool trust_null_free_oop) {\n-  PhaseGVN* gvn = &kit->gvn();\n-  ciInlineKlass* vk = inline_klass();\n-  Node* value = nullptr;\n-  if (!null_free) {\n-    \/\/ Get the null marker\n-    value = get_payload_value(gvn, payload, bt, T_BOOLEAN, holder_offset + vk->null_marker_offset_in_payload());\n-    set_req(NullMarker, value);\n-  }\n-  \/\/ Iterate over the fields and get their values from the payload\n-  for (uint i = 0; i < field_count(); ++i) {\n-    ciType* ft = field_type(i);\n-    bool field_null_free = field_is_null_free(i);\n-    int offset = holder_offset + field_offset(i) - vk->payload_offset();\n-    if (field_is_flat(i)) {\n-      InlineTypeNode* vt = make_uninitialized(*gvn, ft->as_inline_klass(), field_null_free);\n-      vt->convert_from_payload(kit, bt, payload, offset, field_null_free, trust_null_free_oop && field_null_free);\n-      value = gvn->transform(vt);\n-    } else {\n-      value = get_payload_value(gvn, payload, bt, ft->basic_type(), offset);\n-      if (!ft->is_primitive_type()) {\n-        \/\/ Narrow oop field\n-        assert(UseCompressedOops && bt == T_LONG, \"Naturally atomic\");\n-        const Type* val_type = Type::get_const_type(ft);\n-        if (trust_null_free_oop && field_null_free) {\n-          val_type = val_type->join_speculative(TypePtr::NOTNULL);\n-        }\n-        value = gvn->transform(new CastI2NNode(kit->control(), value, val_type->make_narrowoop()));\n-        value = gvn->transform(new DecodeNNode(value, val_type->make_narrowoop()));\n-\n-        \/\/ Similar to CheckCastPP nodes with raw input, CastI2N nodes require special handling in 'PhaseCFG::schedule_late' to ensure the\n-        \/\/ register allocator does not move the CastI2N below a safepoint. This is necessary to avoid having the raw pointer span a safepoint,\n-        \/\/ making it opaque to the GC. Unlike CheckCastPPs, which need extra handling in 'Scheduling::ComputeRegisterAntidependencies' due to\n-        \/\/ scalarization, CastI2N nodes are always used by a load if scalarization happens which inherently keeps them pinned above the safepoint.\n-\n-        if (ft->is_inlinetype()) {\n-          GrowableArray<ciType*> visited;\n-          value = make_from_oop_impl(kit, value, ft->as_inline_klass(), visited);\n-        }\n-      }\n-    }\n-    set_field_value(i, value);\n-  }\n-}\n-\n-\/\/ Set a field value in the payload by shifting it according to the offset\n-static Node* set_payload_value(PhaseGVN* gvn, Node* payload, BasicType bt, Node* value, BasicType val_bt, int offset) {\n-  assert((offset + type2aelembytes(val_bt)) <= type2aelembytes(bt), \"Value does not fit into payload\");\n-\n-  \/\/ Make sure to zero unused bits in the 32-bit value\n-  if (val_bt == T_BYTE || val_bt == T_BOOLEAN) {\n-    value = gvn->transform(new AndINode(value, gvn->intcon(0xFF)));\n-  } else if (val_bt == T_CHAR || val_bt == T_SHORT) {\n-    value = gvn->transform(new AndINode(value, gvn->intcon(0xFFFF)));\n-  } else if (val_bt == T_FLOAT) {\n-    value = gvn->transform(new MoveF2INode(value));\n-  } else {\n-    assert(val_bt == T_INT, \"Unsupported type: %s\", type2name(val_bt));\n-  }\n-\n-  Node* shift_val = gvn->intcon(offset << LogBitsPerByte);\n-  if (bt == T_LONG) {\n-    \/\/ Convert to long and remove the sign bit (the backend will fold this and emit a zero extend i2l)\n-    value = gvn->transform(new ConvI2LNode(value));\n-    value = gvn->transform(new AndLNode(value, gvn->longcon(0xFFFFFFFF)));\n-\n-    Node* shift_value = gvn->transform(new LShiftLNode(value, shift_val));\n-    payload = new OrLNode(shift_value, payload);\n-  } else {\n-    Node* shift_value = gvn->transform(new LShiftINode(value, shift_val));\n-    payload = new OrINode(shift_value, payload);\n-  }\n-  return gvn->transform(payload);\n-}\n-\n-\/\/ Convert the field values to a payload value of type 'bt'\n-Node* InlineTypeNode::convert_to_payload(GraphKit* kit, BasicType bt, Node* payload, int holder_offset, bool null_free, int null_marker_offset, int& oop_off_1, int& oop_off_2) const {\n-  PhaseGVN* gvn = &kit->gvn();\n-  Node* value = nullptr;\n-  if (!null_free) {\n-    \/\/ Set the null marker\n-    value = get_null_marker();\n-    payload = set_payload_value(gvn, payload, bt, value, T_BOOLEAN, null_marker_offset);\n-  }\n-  \/\/ Iterate over the fields and add their values to the payload\n-  for (uint i = 0; i < field_count(); ++i) {\n-    value = field_value(i);\n-    int inner_offset = field_offset(i) - inline_klass()->payload_offset();\n-    int offset = holder_offset + inner_offset;\n-    if (field_is_flat(i)) {\n-      null_marker_offset = holder_offset + field_null_marker_offset(i) - inline_klass()->payload_offset();\n-      payload = value->as_InlineType()->convert_to_payload(kit, bt, payload, offset, field_is_null_free(i), null_marker_offset, oop_off_1, oop_off_2);\n-    } else {\n-      ciType* ft = field_type(i);\n-      BasicType field_bt = ft->basic_type();\n-      if (!ft->is_primitive_type()) {\n-        \/\/ Narrow oop field\n-        assert(UseCompressedOops && bt == T_LONG, \"Naturally atomic\");\n-        assert(inner_offset != -1, \"sanity\");\n-        if (oop_off_1 == -1) {\n-          oop_off_1 = inner_offset;\n-        } else {\n-          assert(oop_off_2 == -1, \"already set\");\n-          oop_off_2 = inner_offset;\n-        }\n-        const Type* val_type = Type::get_const_type(ft)->make_narrowoop();\n-        if (value->is_InlineType()) {\n-          PreserveReexecuteState preexecs(kit);\n-          kit->jvms()->set_should_reexecute(true);\n-          value = value->as_InlineType()->buffer(kit, false);\n-        }\n-        value = gvn->transform(new EncodePNode(value, val_type));\n-        value = gvn->transform(new CastP2XNode(kit->control(), value));\n-        value = gvn->transform(new ConvL2INode(value));\n-        field_bt = T_INT;\n-      }\n-      payload = set_payload_value(gvn, payload, bt, value, field_bt, offset);\n-    }\n-  }\n-  return payload;\n-}\n-\n-void InlineTypeNode::store_flat(GraphKit* kit, Node* base, Node* ptr, bool atomic, bool immutable_memory, bool null_free, DecoratorSet decorators) const {\n+void InlineTypeNode::store_flat(GraphKit* kit, Node* base, Node* ptr, bool atomic, bool immutable_memory, bool null_free, DecoratorSet decorators) {\n@@ -657,33 +516,1 @@\n-  \/\/ Convert to a payload value <= 64-bit and write atomically.\n-  \/\/ The payload might contain at most two oop fields that must be narrow because otherwise they would be 64-bit\n-  \/\/ in size and would then be written by a \"normal\" oop store. If the payload contains oops, its size is always\n-  \/\/ 64-bit because the next smaller (power-of-two) size would be 32-bit which could only hold one narrow oop that\n-  \/\/ would then be written by a normal narrow oop store. These properties are asserted in 'convert_to_payload'.\n-  assert(!immutable_memory, \"immutable memory does not need explicit atomic access\");\n-  BasicType store_bt = vk->atomic_size_to_basic_type(null_free);\n-  Node* payload = (store_bt == T_LONG) ? kit->longcon(0) : kit->intcon(0);\n-  int oop_off_1 = -1;\n-  int oop_off_2 = -1;\n-  payload = convert_to_payload(kit, store_bt, payload, 0, null_free, vk->null_marker_offset_in_payload(), oop_off_1, oop_off_2);\n-  if (!UseG1GC || oop_off_1 == -1) {\n-    \/\/ No oop fields or no late barrier expansion. Emit an atomic store of the payload and add GC barriers if needed.\n-    assert(oop_off_2 == -1 || !UseG1GC, \"sanity\");\n-    \/\/ ZGC does not support compressed oops, so only one oop can be in the payload which is written by a \"normal\" oop store.\n-    assert((oop_off_1 == -1 && oop_off_2 == -1) || !UseZGC, \"ZGC does not support embedded oops in flat fields\");\n-    const Type* val_type = Type::get_const_basic_type(store_bt);\n-    kit->insert_mem_bar(Op_MemBarCPUOrder);\n-    kit->access_store_at(base, ptr, TypeRawPtr::BOTTOM, payload, val_type, store_bt, decorators | C2_MISMATCHED, true, this);\n-    kit->insert_mem_bar(Op_MemBarCPUOrder);\n-  } else {\n-    \/\/ Contains oops and requires late barrier expansion. Emit a special store node that allows to emit GC barriers in the backend.\n-    assert(UseG1GC, \"Unexpected GC\");\n-    assert(store_bt == T_LONG, \"Unexpected payload type\");\n-    \/\/ If one oop, set the offset (if no offset is set, two oops are assumed by the backend)\n-    Node* oop_offset = (oop_off_2 == -1) ? kit->intcon(oop_off_1) : nullptr;\n-    kit->insert_mem_bar(Op_MemBarCPUOrder);\n-    Node* mem = kit->reset_memory();\n-    kit->set_all_memory(mem);\n-    Node* st = kit->gvn().transform(new StoreLSpecialNode(kit->control(), mem, ptr, TypeRawPtr::BOTTOM, payload, oop_offset, MemNode::unordered));\n-    kit->set_memory(st, TypeRawPtr::BOTTOM);\n-    kit->insert_mem_bar(Op_MemBarCPUOrder);\n-  }\n+  StoreFlatNode::store(kit, ptr, this, null_free, (decorators & C2_MISMATCHED) != 0);\n@@ -692,1 +519,1 @@\n-void InlineTypeNode::store_flat_array(GraphKit* kit, Node* base, Node* idx) const {\n+void InlineTypeNode::store_flat_array(GraphKit* kit, Node* base, Node* idx) {\n@@ -940,1 +767,1 @@\n-Node* InlineTypeNode::allocate_fields(GraphKit* kit) {\n+InlineTypeNode* InlineTypeNode::allocate_fields(GraphKit* kit) {\n@@ -1214,9 +1041,1 @@\n-  InlineTypeNode* vt = make_uninitialized(kit->gvn(), vk, null_free);\n-  BasicType load_bt = vk->atomic_size_to_basic_type(null_free);\n-  decorators |= C2_MISMATCHED | C2_CONTROL_DEPENDENT_LOAD;\n-  const Type* val_type = Type::get_const_basic_type(load_bt);\n-  kit->insert_mem_bar(Op_MemBarCPUOrder);\n-  Node* payload = kit->access_load_at(base, ptr, TypeRawPtr::BOTTOM, val_type, load_bt, decorators, kit->control());\n-  kit->insert_mem_bar(Op_MemBarCPUOrder);\n-  vt->convert_from_payload(kit, load_bt, kit->gvn().transform(payload), 0, null_free, trust_null_free_oop);\n-  return gvn.transform(vt)->as_InlineType();\n+  return LoadFlatNode::load(kit, vk, ptr, null_free, trust_null_free_oop, (decorators & C2_MISMATCHED) != 0);\n@@ -1653,0 +1472,410 @@\n+\n+InlineTypeNode* LoadFlatNode::load(GraphKit* kit, ciInlineKlass* vk, Node* ptr, bool null_free, bool trust_null_free_oop, bool mismatched) {\n+  int output_type_size = vk->nof_nonstatic_fields() + (null_free ? 0 : 1);\n+  const Type** output_types = TypeTuple::fields(output_type_size);\n+  collect_field_types(vk, output_types + TypeFunc::Parms, 0, output_type_size, null_free, trust_null_free_oop);\n+  const TypeTuple* type = TypeTuple::make(output_type_size + TypeFunc::Parms, output_types);\n+\n+  LoadFlatNode* load = new LoadFlatNode(vk, type, null_free, mismatched);\n+  load->init_req(TypeFunc::Control, kit->control());\n+  load->init_req(TypeFunc::Memory, kit->reset_memory());\n+  load->init_req(TypeFunc::Parms, ptr);\n+  load = static_cast<LoadFlatNode*>(kit->gvn().transform(load));\n+\n+  kit->set_control(kit->gvn().transform(new ProjNode(load, TypeFunc::Control)));\n+  kit->set_all_memory(kit->gvn().transform(new ProjNode(load, TypeFunc::Memory)));\n+  return load->collect_projs(kit, vk, TypeFunc::Parms, null_free);\n+}\n+\n+bool LoadFlatNode::expand_non_atomic(PhaseIterGVN& igvn) {\n+  if (_mismatched) {\n+    return false;\n+  }\n+\n+  Node* ctrl = in(TypeFunc::Control);\n+  Node* mem = in(TypeFunc::Memory);\n+  AddPNode* ptr = this->ptr()->as_AddP();\n+  Node* base = ptr->base_node();\n+\n+  for (int i = 0; i < _vk->nof_nonstatic_fields(); i++) {\n+    ProjNode* proj_out = proj_out_or_null(TypeFunc::Parms + i);\n+    if (proj_out == nullptr) {\n+      continue;\n+    }\n+\n+    ciField* field = _vk->nonstatic_field_at(i);\n+    Node* field_ptr = igvn.transform(new AddPNode(base, ptr, igvn.MakeConX(field->offset_in_bytes() - _vk->payload_offset())));\n+    const TypePtr* field_ptr_type = field_ptr->Value(&igvn)->is_ptr();\n+    igvn.set_type(field_ptr, field_ptr_type);\n+    Node* field_value = LoadNode::make(igvn, ctrl, mem, field_ptr, field_ptr_type, igvn.type(proj_out), field->type()->basic_type(), MemNode::unordered, LoadNode::UnknownControl);\n+    field_value = igvn.transform(field_value);\n+    igvn.replace_node(proj_out, field_value);\n+  }\n+\n+  if (!_null_free) {\n+    ProjNode* proj_out = proj_out_or_null(TypeFunc::Parms + _vk->nof_nonstatic_fields());\n+    if (proj_out != nullptr) {\n+      Node* null_marker_ptr = igvn.transform(new AddPNode(base, ptr, igvn.MakeConX(_vk->null_marker_offset_in_payload())));\n+      const TypePtr* null_marker_ptr_type = null_marker_ptr->Value(&igvn)->is_ptr();\n+      igvn.set_type(null_marker_ptr, null_marker_ptr_type);\n+      Node* null_marker_value = LoadNode::make(igvn, ctrl, mem, null_marker_ptr, null_marker_ptr_type, TypeInt::BOOL, T_BOOLEAN, MemNode::unordered, LoadNode::UnknownControl);\n+      null_marker_value = igvn.transform(null_marker_value);\n+      igvn.replace_node(proj_out, null_marker_value);\n+    }\n+  }\n+\n+  Node* old_ctrl = proj_out_or_null(TypeFunc::Control);\n+  if (old_ctrl != nullptr) {\n+    igvn.replace_node(old_ctrl, ctrl);\n+  }\n+  Node* old_mem = proj_out_or_null(TypeFunc::Memory);\n+  if (old_mem != nullptr) {\n+    igvn.replace_node(old_mem, mem);\n+  }\n+  return true;\n+}\n+\n+void LoadFlatNode::expand_atomic(PhaseIterGVN& igvn) {\n+  Node* ctrl = in(TypeFunc::Control);\n+  Node* mem = in(TypeFunc::Memory);\n+  Node* ptr = this->ptr();\n+\n+  BasicType payload_bt = _vk->atomic_size_to_basic_type(_null_free);\n+  Node* in_membar = MemBarNode::make(igvn.C, Op_MemBarCPUOrder);\n+  in_membar->init_req(TypeFunc::Control, ctrl);\n+  in_membar->init_req(TypeFunc::Memory, mem);\n+  in_membar = igvn.transform(in_membar);\n+\n+  ctrl = igvn.transform(new ProjNode(in_membar, TypeFunc::Control));\n+  mem = igvn.transform(new ProjNode(in_membar, TypeFunc::Memory));\n+  Node* payload = LoadNode::make(igvn, ctrl, mem, ptr, TypeRawPtr::BOTTOM, Type::get_const_basic_type(payload_bt), payload_bt, MemNode::unordered, LoadNode::Pinned, true, false, true);\n+  payload = igvn.transform(payload);\n+\n+  Node* out_membar = MemBarNode::make(igvn.C, Op_MemBarCPUOrder);\n+  out_membar->init_req(TypeFunc::Control, ctrl);\n+  out_membar->init_req(TypeFunc::Memory, mem);\n+  out_membar = igvn.transform(out_membar);\n+\n+  ctrl = igvn.transform(new ProjNode(out_membar, TypeFunc::Control));\n+  Node* old_ctrl = proj_out_or_null(TypeFunc::Control);\n+  if (old_ctrl != nullptr) {\n+    igvn.replace_node(old_ctrl, ctrl);\n+  }\n+  Node* old_mem = proj_out_or_null(TypeFunc::Memory);\n+  if (old_mem != nullptr) {\n+    mem = igvn.transform(new ProjNode(out_membar, TypeFunc::Memory));\n+    igvn.replace_node(old_mem, mem);\n+  }\n+\n+  expand_projs_atomic(igvn, ctrl, payload);\n+}\n+\n+void LoadFlatNode::collect_field_types(ciInlineKlass* vk, const Type** field_types, int idx, int limit, bool null_free, bool trust_null_free_oop) {\n+  assert(null_free || !trust_null_free_oop, \"cannot trust null-free oop when the holder object is not null-free\");\n+  for (int i = 0; i < vk->nof_declared_nonstatic_fields(); i++) {\n+    ciField* field = vk->declared_nonstatic_field_at(i);\n+    if (field->is_flat()) {\n+      ciInlineKlass* field_klass = field->type()->as_inline_klass();\n+      collect_field_types(field_klass, field_types, idx, limit, field->is_null_free(), trust_null_free_oop && field->is_null_free());\n+      idx += field_klass->nof_nonstatic_fields() + (field->is_null_free() ? 0 : 1);\n+      continue;\n+    }\n+\n+    const Type* field_type = Type::get_const_type(field->type());\n+    if (trust_null_free_oop && field->is_null_free()) {\n+      field_type = field_type->filter(TypePtr::NOTNULL);\n+    }\n+\n+    assert(idx >= 0 && idx < limit, \"field type out of bounds, %d - %d\", idx, limit);\n+    field_types[idx] = field_type;\n+    idx++;\n+  }\n+\n+  if (!null_free) {\n+    assert(idx >= 0 && idx < limit, \"field type out of bounds, %d - %d\", idx, limit);\n+    field_types[idx] = TypeInt::BOOL;\n+  }\n+}\n+\n+\/\/ Create an InlineTypeNode from a LoadFlatNode with its fields being extracted from the\n+\/\/ LoadFlatNode\n+InlineTypeNode* LoadFlatNode::collect_projs(GraphKit* kit, ciInlineKlass* vk, int proj_con, bool null_free) {\n+  PhaseGVN& gvn = kit->gvn();\n+  InlineTypeNode* res = InlineTypeNode::make_uninitialized(gvn, vk, null_free);\n+  for (int i = 0; i < vk->nof_declared_nonstatic_fields(); i++) {\n+    ciField* field = vk->declared_nonstatic_field_at(i);\n+    Node* field_value;\n+    if (field->is_flat()) {\n+      ciInlineKlass* field_klass = field->type()->as_inline_klass();\n+      field_value = collect_projs(kit, field_klass, proj_con, field->is_null_free());\n+      proj_con += field_klass->nof_nonstatic_fields() + (field->is_null_free() ? 0 : 1);\n+    } else {\n+      field_value = gvn.transform(new ProjNode(this, proj_con));\n+      if (field->type()->is_inlinetype()) {\n+        field_value = InlineTypeNode::make_from_oop(kit, field_value, field->type()->as_inline_klass());\n+      }\n+      proj_con++;\n+    }\n+    res->set_field_value(i, field_value);\n+  }\n+\n+  if (null_free) {\n+    res->set_null_marker(gvn);\n+  } else {\n+    res->set_null_marker(gvn, gvn.transform(new ProjNode(this, proj_con)));\n+  }\n+  return gvn.transform(res)->as_InlineType();\n+}\n+\n+\/\/ Extract the values of the flattened fields from the loaded payload\n+void LoadFlatNode::expand_projs_atomic(PhaseIterGVN& igvn, Node* ctrl, Node* payload) {\n+  BasicType payload_bt = _vk->atomic_size_to_basic_type(_null_free);\n+  for (int i = 0; i < _vk->nof_nonstatic_fields(); i++) {\n+    ProjNode* proj_out = proj_out_or_null(TypeFunc::Parms + i);\n+    if (proj_out == nullptr) {\n+      continue;\n+    }\n+\n+    ciField* field = _vk->nonstatic_field_at(i);\n+    int field_offset = field->offset_in_bytes() - _vk->payload_offset();\n+    const Type* field_type = igvn.type(proj_out);\n+    Node* field_value = get_payload_value(igvn, ctrl, payload_bt, payload, field_type, field->type()->basic_type(), field_offset);\n+    igvn.replace_node(proj_out, field_value);\n+  }\n+\n+  if (!_null_free) {\n+    ProjNode* proj_out = proj_out_or_null(TypeFunc::Parms + _vk->nof_nonstatic_fields());\n+    if (proj_out == nullptr) {\n+      return;\n+    }\n+\n+    int null_marker_offset = _vk->null_marker_offset_in_payload();\n+    Node* null_marker_value = get_payload_value(igvn, ctrl, payload_bt, payload, TypeInt::BOOL, T_BOOLEAN, null_marker_offset);\n+    igvn.replace_node(proj_out, null_marker_value);\n+  }\n+}\n+\n+Node* LoadFlatNode::get_payload_value(PhaseIterGVN& igvn, Node* ctrl, BasicType payload_bt, Node* payload, const Type* value_type, BasicType value_bt, int offset) {\n+  assert((offset + type2aelembytes(value_bt)) <= type2aelembytes(payload_bt), \"Value does not fit into payload\");\n+  Node* value = nullptr;\n+  \/\/ Shift to the right position in the long value\n+  Node* shift_val = igvn.intcon(offset << LogBitsPerByte);\n+  if (payload_bt == T_LONG) {\n+    value = igvn.transform(new URShiftLNode(payload, shift_val));\n+    value = igvn.transform(new ConvL2INode(value));\n+  } else {\n+    value = igvn.transform(new URShiftINode(payload, shift_val));\n+  }\n+\n+  if (value_bt == T_INT) {\n+    return value;\n+  } else if (!is_java_primitive(value_bt)) {\n+    assert(UseCompressedOops && payload_bt == T_LONG, \"Naturally atomic\");\n+    value = igvn.transform(new CastI2NNode(ctrl, value, value_type->make_narrowoop()));\n+    value = igvn.transform(new DecodeNNode(value, value_type));\n+\n+    \/\/ Similar to CheckCastPP nodes with raw input, CastI2N nodes require special handling in 'PhaseCFG::schedule_late' to ensure the\n+    \/\/ register allocator does not move the CastI2N below a safepoint. This is necessary to avoid having the raw pointer span a safepoint,\n+    \/\/ making it opaque to the GC. Unlike CheckCastPPs, which need extra handling in 'Scheduling::ComputeRegisterAntidependencies' due to\n+    \/\/ scalarization, CastI2N nodes are always used by a load if scalarization happens which inherently keeps them pinned above the safepoint.\n+    return value;\n+  } else {\n+    \/\/ Make sure to zero unused bits in the 32-bit value\n+    return Compile::narrow_value(value_bt, value, nullptr, &igvn, true);\n+  }\n+}\n+\n+void StoreFlatNode::store(GraphKit* kit, Node* ptr, InlineTypeNode* value, bool null_free, bool mismatched) {\n+  value = value->allocate_fields(kit);\n+  Node* store = new StoreFlatNode(null_free, mismatched);\n+  store->init_req(TypeFunc::Control, kit->control());\n+  store->init_req(TypeFunc::Memory, kit->reset_memory());\n+  store->init_req(TypeFunc::Parms, ptr);\n+  store->init_req(TypeFunc::Parms + 1, value);\n+\n+  store = kit->gvn().transform(store);\n+  kit->set_control(kit->gvn().transform(new ProjNode(store, TypeFunc::Control)));\n+  kit->set_all_memory(kit->gvn().transform(new ProjNode(store, TypeFunc::Memory)));\n+}\n+\n+bool StoreFlatNode::expand_non_atomic(PhaseIterGVN& igvn) {\n+  if (_mismatched) {\n+    return false;\n+  }\n+\n+  Node* ctrl = in(TypeFunc::Control);\n+  Node* mem = in(TypeFunc::Memory);\n+  AddPNode* ptr = this->ptr()->as_AddP();\n+  Node* base = ptr->base_node();\n+  InlineTypeNode* value = this->value();\n+\n+  ciInlineKlass* vk = igvn.type(value)->inline_klass();\n+  MergeMemNode* new_mem = MergeMemNode::make(mem);\n+  for (int i = 0; i < vk->nof_nonstatic_fields(); i++) {\n+    ciField* field = vk->nonstatic_field_at(i);\n+    Node* field_ptr = igvn.transform(new AddPNode(base, ptr, igvn.MakeConX(field->offset_in_bytes() - vk->payload_offset())));\n+    const TypePtr* field_ptr_type = field_ptr->Value(&igvn)->is_ptr();\n+    igvn.set_type(field_ptr, field_ptr_type);\n+    Node* field_value = value->field_value_by_offset(field->offset_in_bytes(), true);\n+    Node* store = StoreNode::make(igvn, ctrl, mem, field_ptr, field_ptr_type, field_value, field->type()->basic_type(), MemNode::unordered);\n+    store = igvn.transform(store);\n+    new_mem->set_memory_at(igvn.C->get_alias_index(field_ptr_type), store);\n+  }\n+\n+  if (!_null_free) {\n+    Node* null_marker_ptr = igvn.transform(new AddPNode(base, ptr, igvn.MakeConX(vk->null_marker_offset_in_payload())));\n+    const TypePtr* null_marker_ptr_type = null_marker_ptr->Value(&igvn)->is_ptr();\n+    igvn.set_type(null_marker_ptr, null_marker_ptr_type);\n+    Node* null_marker_value = value->get_null_marker();\n+    Node* store = StoreNode::make(igvn, ctrl, mem, null_marker_ptr, null_marker_ptr_type, null_marker_value, T_BOOLEAN, MemNode::unordered);\n+    store = igvn.transform(store);\n+    new_mem->set_memory_at(igvn.C->get_alias_index(null_marker_ptr_type), store);\n+  }\n+\n+  mem = igvn.transform(new_mem);\n+  Node* old_ctrl = proj_out_or_null(TypeFunc::Control);\n+  if (old_ctrl != nullptr) {\n+    igvn.replace_node(old_ctrl, ctrl);\n+  }\n+  Node* old_mem = proj_out_or_null(TypeFunc::Memory);\n+  if (old_mem != nullptr) {\n+    igvn.replace_node(old_mem, mem);\n+  }\n+  return true;\n+}\n+\n+void StoreFlatNode::expand_atomic(PhaseIterGVN& igvn) {\n+  \/\/ Convert to a payload value <= 64-bit and write atomically.\n+  \/\/ The payload might contain at most two oop fields that must be narrow because otherwise they would be 64-bit\n+  \/\/ in size and would then be written by a \"normal\" oop store. If the payload contains oops, its size is always\n+  \/\/ 64-bit because the next smaller (power-of-two) size would be 32-bit which could only hold one narrow oop that\n+  \/\/ would then be written by a normal narrow oop store. These properties are asserted in 'convert_to_payload'.\n+  Node* ctrl = in(TypeFunc::Control);\n+  Node* mem = in(TypeFunc::Memory);\n+  Node* ptr = this->ptr();\n+  InlineTypeNode* value = this->value();\n+\n+  int oop_off_1 = -1;\n+  int oop_off_2 = -1;\n+  Node* payload = convert_to_payload(igvn, ctrl, value, _null_free, oop_off_1, oop_off_2);\n+\n+  ciInlineKlass* vk = igvn.type(value)->inline_klass();\n+  BasicType payload_bt = vk->atomic_size_to_basic_type(_null_free);\n+  Node* in_membar = MemBarNode::make(igvn.C, Op_MemBarCPUOrder);\n+  in_membar->init_req(TypeFunc::Control, ctrl);\n+  in_membar->init_req(TypeFunc::Memory, mem);\n+  in_membar = igvn.transform(in_membar);\n+\n+  ctrl = igvn.transform(new ProjNode(in_membar, TypeFunc::Control));\n+  mem = igvn.transform(new ProjNode(in_membar, TypeFunc::Memory));\n+  if (!UseG1GC || oop_off_1 == -1) {\n+    \/\/ No oop fields or no late barrier expansion. Emit an atomic store of the payload and add GC barriers if needed.\n+    assert(oop_off_2 == -1 || !UseG1GC, \"sanity\");\n+    \/\/ ZGC does not support compressed oops, so only one oop can be in the payload which is written by a \"normal\" oop store.\n+    assert((oop_off_1 == -1 && oop_off_2 == -1) || !UseZGC, \"ZGC does not support embedded oops in flat fields\");\n+    const Type* val_type = Type::get_const_basic_type(payload_bt);\n+    Node* store = StoreNode::make(igvn, ctrl, mem, ptr, TypeRawPtr::BOTTOM, payload, payload_bt, MemNode::unordered, true);\n+    store = igvn.transform(store);\n+    mem = MergeMemNode::make(mem);\n+    mem->set_req(Compile::AliasIdxRaw, store);\n+    mem = igvn.transform(mem);\n+  } else {\n+    \/\/ Contains oops and requires late barrier expansion. Emit a special store node that allows to emit GC barriers in the backend.\n+    assert(UseG1GC, \"Unexpected GC\");\n+    assert(payload_bt == T_LONG, \"Unexpected payload type\");\n+    \/\/ If one oop, set the offset (if no offset is set, two oops are assumed by the backend)\n+    Node* oop_offset = (oop_off_2 == -1) ? igvn.intcon(oop_off_1) : nullptr;\n+    Node* store = igvn.transform(new StoreLSpecialNode(ctrl, mem, ptr, TypeRawPtr::BOTTOM, payload, oop_offset, MemNode::unordered));\n+    mem = MergeMemNode::make(mem);\n+    mem->set_req(Compile::AliasIdxRaw, store);\n+    mem = igvn.transform(mem);\n+  }\n+\n+  Node* out_membar = MemBarNode::make(igvn.C, Op_MemBarCPUOrder);\n+  out_membar->init_req(TypeFunc::Control, ctrl);\n+  out_membar->init_req(TypeFunc::Memory, mem);\n+  out_membar = igvn.transform(out_membar);\n+\n+  Node* old_ctrl = proj_out_or_null(TypeFunc::Control);\n+  if (old_ctrl != nullptr) {\n+    ctrl = igvn.transform(new ProjNode(out_membar, TypeFunc::Control));\n+    igvn.replace_node(old_ctrl, ctrl);\n+  }\n+  Node* old_mem = proj_out_or_null(TypeFunc::Memory);\n+  if (old_mem != nullptr) {\n+    mem = igvn.transform(new ProjNode(out_membar, TypeFunc::Memory));\n+    igvn.replace_node(old_mem, mem);\n+  }\n+}\n+\n+\/\/ Convert the field values to a payload value of type 'bt'\n+Node* StoreFlatNode::convert_to_payload(PhaseIterGVN& igvn, Node* ctrl, InlineTypeNode* value, bool null_free, int& oop_off_1, int& oop_off_2) {\n+  ciInlineKlass* vk = igvn.type(value)->inline_klass();\n+  BasicType payload_bt = vk->atomic_size_to_basic_type(null_free);\n+  Node* payload = igvn.zerocon(payload_bt);\n+  if (!null_free) {\n+    \/\/ Set the null marker\n+    payload = set_payload_value(igvn, payload_bt, payload, T_BOOLEAN, value->get_null_marker(), vk->null_marker_offset_in_payload());\n+  }\n+\n+  \/\/ Iterate over the fields and add their values to the payload\n+  for (int i = 0; i < vk->nof_nonstatic_fields(); i++) {\n+    ciField* field = vk->nonstatic_field_at(i);\n+    Node* field_value = value->field_value_by_offset(field->offset_in_bytes(), true);\n+    ciType* field_klass = field->type();\n+    BasicType field_bt = field_klass->basic_type();\n+    int field_offset_in_payload = field->offset_in_bytes() - vk->payload_offset();\n+    if (!field_klass->is_primitive_type()) {\n+      \/\/ Narrow oop field\n+      assert(UseCompressedOops && payload_bt == T_LONG, \"Naturally atomic\");\n+      if (oop_off_1 == -1) {\n+        oop_off_1 = field_offset_in_payload;\n+      } else {\n+        assert(oop_off_2 == -1, \"already set\");\n+        oop_off_2 = field_offset_in_payload;\n+      }\n+\n+      const Type* val_type = Type::get_const_type(field_klass)->make_narrowoop();\n+      if (field_value->is_InlineType()) {\n+        assert(field_value->as_InlineType()->is_allocated(&igvn), \"must be allocated\");\n+      }\n+\n+      field_value = igvn.transform(new EncodePNode(field_value, val_type));\n+      field_value = igvn.transform(new CastP2XNode(ctrl, field_value));\n+      field_value = igvn.transform(new ConvL2INode(field_value));\n+      field_bt = T_INT;\n+    }\n+    payload = set_payload_value(igvn, payload_bt, payload, field_bt, field_value, field_offset_in_payload);\n+  }\n+\n+  return payload;\n+}\n+\n+Node* StoreFlatNode::set_payload_value(PhaseIterGVN& igvn, BasicType payload_bt, Node* payload, BasicType val_bt, Node* value, int offset) {\n+  assert((offset + type2aelembytes(val_bt)) <= type2aelembytes(payload_bt), \"Value does not fit into payload\");\n+\n+  \/\/ Make sure to zero unused bits in the 32-bit value\n+  if (val_bt == T_BYTE || val_bt == T_BOOLEAN) {\n+    value = igvn.transform(new AndINode(value, igvn.intcon(0xFF)));\n+  } else if (val_bt == T_CHAR || val_bt == T_SHORT) {\n+    value = igvn.transform(new AndINode(value, igvn.intcon(0xFFFF)));\n+  } else if (val_bt == T_FLOAT) {\n+    value = igvn.transform(new MoveF2INode(value));\n+  } else {\n+    assert(val_bt == T_INT, \"Unsupported type: %s\", type2name(val_bt));\n+  }\n+\n+  Node* shift_val = igvn.intcon(offset << LogBitsPerByte);\n+  if (payload_bt == T_LONG) {\n+    \/\/ Convert to long and remove the sign bit (the backend will fold this and emit a zero extend i2l)\n+    value = igvn.transform(new ConvI2LNode(value));\n+    value = igvn.transform(new AndLNode(value, igvn.longcon(0xFFFFFFFF)));\n+\n+    Node* shift_value = igvn.transform(new LShiftLNode(value, shift_val));\n+    payload = new OrLNode(shift_value, payload);\n+  } else {\n+    Node* shift_value = igvn.transform(new LShiftINode(value, shift_val));\n+    payload = new OrINode(shift_value, payload);\n+  }\n+  return igvn.transform(payload);\n+}\n","filename":"src\/hotspot\/share\/opto\/inlinetypenode.cpp","additions":418,"deletions":189,"binary":false,"changes":607,"status":"modified"},{"patch":"@@ -31,1 +31,1 @@\n-#include \"opto\/connode.hpp\"\n+#include \"opto\/compile.hpp\"\n@@ -33,0 +33,1 @@\n+#include \"opto\/multnode.hpp\"\n@@ -83,3 +84,0 @@\n-  void convert_from_payload(GraphKit* kit, BasicType bt, Node* payload, int holder_offset, bool null_free, bool trust_null_free_oop);\n-  Node* convert_to_payload(GraphKit* kit, BasicType bt, Node* payload, int holder_offset, bool null_free, int null_marker_offset, int& oop_off_1, int& oop_off_2) const;\n-\n@@ -138,1 +136,1 @@\n-  void store_flat(GraphKit* kit, Node* base, Node* ptr, bool atomic, bool immutable_memory, bool null_free, DecoratorSet decorators) const;\n+  void store_flat(GraphKit* kit, Node* base, Node* ptr, bool atomic, bool immutable_memory, bool null_free, DecoratorSet decorators);\n@@ -140,1 +138,1 @@\n-  void store_flat_array(GraphKit* kit, Node* base, Node* idx) const;\n+  void store_flat_array(GraphKit* kit, Node* base, Node* idx);\n@@ -152,1 +150,1 @@\n-  Node* allocate_fields(GraphKit* kit);\n+  InlineTypeNode* allocate_fields(GraphKit* kit);\n@@ -173,0 +171,77 @@\n+\/\/ Load from a flat element, the node produces 1 Proj output for each flattened field of the flat\n+\/\/ element. The order of the Proj node is the same as that of _vk->_nonstatic_fields, and the null\n+\/\/ marker if existing will be the last Proj output. This node acts as if the load happens\n+\/\/ atomically and will be expanded to loading the whole payload and extracting the flattened fields\n+\/\/ from the loaded payload. In special cases, such as when the object from which this load read\n+\/\/ does not escape, this node can be expanded to multiple loads from each flattened field.\n+\/\/ This node allows us to replace its results with the value from a matching store because the\n+\/\/ payload value cannot be directly propagated if it contains oops. This effect, in turns, allows\n+\/\/ objects with atomic flat fields to be scalar replaced.\n+class LoadFlatNode final : public MultiNode {\n+private:\n+  ciInlineKlass* _vk;\n+  const TypeTuple* _type;\n+  bool _null_free;\n+  bool _mismatched;\n+\n+public:\n+  static InlineTypeNode* load(GraphKit* kit, ciInlineKlass* vk, Node* ptr, bool null_free, bool trust_null_free_oop, bool mismatched);\n+  Node* ptr() { return in(TypeFunc::Parms); }\n+  bool expand_non_atomic(PhaseIterGVN& igvn);\n+  void expand_atomic(PhaseIterGVN& igvn);\n+\n+private:\n+  LoadFlatNode(ciInlineKlass* vk, const TypeTuple* type, bool null_free, bool mismatched)\n+    : MultiNode(TypeFunc::Parms + 1), _vk(vk), _type(type), _null_free(null_free), _mismatched(mismatched) {\n+    Compile::current()->add_flat_access(this);\n+  }\n+\n+  virtual int Opcode() const override;\n+  virtual const Type* bottom_type() const override { return _type; }\n+  virtual const TypePtr* adr_type() const override { return TypePtr::BOTTOM; }\n+  virtual uint size_of() const override { return sizeof(LoadFlatNode); }\n+  virtual uint hash() const override { return MultiNode::hash() + _vk->hash() + _null_free; }\n+  virtual bool cmp(const Node& other) const override {\n+    const LoadFlatNode& f = static_cast<const LoadFlatNode&>(other);\n+    return MultiNode::cmp(f) && _vk == f._vk && _null_free == f._null_free;\n+  }\n+\n+  static void collect_field_types(ciInlineKlass* vk, const Type** field_types, int idx, int limit, bool null_free, bool trust_null_free_oop);\n+  InlineTypeNode* collect_projs(GraphKit* kit, ciInlineKlass* vk, int proj_con, bool null_free);\n+  void expand_projs_atomic(PhaseIterGVN& gvn, Node* ctrl, Node* payload);\n+  static Node* get_payload_value(PhaseIterGVN& igvn, Node* ctrl, BasicType payload_bt, Node* payload, const Type* value_type, BasicType value_bt, int offset);\n+};\n+\n+\/\/ Store an InlineTypeNode to a flat element, the store acts as if it is atomic. Similar to\n+\/\/ LoadFlatNode, this node is expanded to storing a payload creating from the field values of the\n+\/\/ InlineTypeNode, and under special circumstances, when there is no racing access to the field,\n+\/\/ this node can be expanded to multiple stores to each flattened field.\n+\/\/ The purposes of this node complement those of LoadFlatNode.\n+class StoreFlatNode final : public MultiNode {\n+private:\n+  bool _null_free;\n+  bool _mismatched;\n+\n+public:\n+  static void store(GraphKit* kit, Node* ptr, InlineTypeNode* obj, bool null_free, bool mismatched);\n+  Node* ptr() { return in(TypeFunc::Parms); }\n+  InlineTypeNode* value() { return in(TypeFunc::Parms + 1)->as_InlineType(); }\n+  bool expand_non_atomic(PhaseIterGVN& igvn);\n+  void expand_atomic(PhaseIterGVN& igvn);\n+\n+private:\n+  StoreFlatNode(bool null_free, bool mismatched) : MultiNode(TypeFunc::Parms + 2), _null_free(null_free), _mismatched(mismatched) {\n+    Compile::current()->add_flat_access(this);\n+  }\n+\n+  virtual int Opcode() const override;\n+  virtual const Type* bottom_type() const override { return TypeTuple::MEMBAR; }\n+  virtual const TypePtr* adr_type() const override { return TypePtr::BOTTOM; }\n+  virtual uint size_of() const override { return sizeof(StoreFlatNode); }\n+  virtual uint hash() const override { return MultiNode::hash() + _null_free; }\n+  virtual bool cmp(const Node& other) const override { return MultiNode::cmp(other) && _null_free == static_cast<const StoreFlatNode&>(other)._null_free; }\n+\n+  static Node* convert_to_payload(PhaseIterGVN& igvn, Node* ctrl, InlineTypeNode* value, bool null_free, int& oop_off_1, int& oop_off_2);\n+  static Node* set_payload_value(PhaseIterGVN& igvn, BasicType payload_bt, Node* payload, BasicType val_bt, Node* value, int offset);\n+};\n+\n","filename":"src\/hotspot\/share\/opto\/inlinetypenode.hpp","additions":82,"deletions":7,"binary":false,"changes":89,"status":"modified"},{"patch":"@@ -41,0 +41,1 @@\n+#include \"opto\/opcodes.hpp\"\n@@ -1129,1 +1130,1 @@\n-      } else if (mem->is_MemBar() || mem->is_SafePoint()) {\n+      } else if (mem->is_MemBar() || mem->is_SafePoint() || mem->Opcode() == Op_LoadFlat || mem->Opcode() == Op_StoreFlat) {\n","filename":"src\/hotspot\/share\/opto\/loopopts.cpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -49,0 +49,1 @@\n+#include \"opto\/opcodes.hpp\"\n@@ -194,0 +195,2 @@\n+      } else if (in->Opcode() == Op_LoadFlat || in->Opcode() == Op_StoreFlat) {\n+        mem = in->in(TypeFunc::Memory);\n","filename":"src\/hotspot\/share\/opto\/macro.cpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -39,0 +39,1 @@\n+#include \"opto\/opcodes.hpp\"\n@@ -58,0 +59,1 @@\n+#include \"utilities\/globalDefinitions.hpp\"\n@@ -305,0 +307,5 @@\n+      } else if (proj_in->Opcode() == Op_LoadFlat || proj_in->Opcode() == Op_StoreFlat) {\n+        if (is_strict_final_load) {\n+          \/\/ LoadFlat and StoreFlat cannot happen to strict final fields\n+          result = proj_in->in(TypeFunc::Memory);\n+        }\n@@ -308,1 +315,1 @@\n-        assert(false, \"unexpected projection\");\n+        assert(false, \"unexpected projection of %s\", proj_in->Name());\n@@ -1089,0 +1096,1 @@\n+  case T_ARRAY:\n@@ -1102,1 +1110,1 @@\n-    ShouldNotReachHere();\n+    assert(false, \"unexpected basic type %s\", type2name(bt));\n@@ -2905,0 +2913,1 @@\n+  case T_ARRAY:\n@@ -2920,1 +2929,1 @@\n-    ShouldNotReachHere();\n+    assert(false, \"unexpected basic type %s\", type2name(bt));\n","filename":"src\/hotspot\/share\/opto\/memnode.cpp","additions":12,"deletions":3,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -570,0 +570,3 @@\n+  if (n->Opcode() == Op_LoadFlat || n->Opcode() == Op_StoreFlat) {\n+    C->add_flat_access(n);\n+  }\n","filename":"src\/hotspot\/share\/opto\/node.cpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -1649,1 +1649,1 @@\n-  int base_offset = arrayOopDesc::base_offset_in_bytes(T_FLAT_ELEMENT) - InlineKlass::cast(vk)->payload_offset();\n+  int base_offset = arrayOopDesc::base_offset_in_bytes(T_FLAT_ELEMENT) - vk->payload_offset();\n@@ -1652,1 +1652,1 @@\n-    ScopeValue* val = sv->field_at(i);\n+    ObjectValue* val = sv->field_at(i)->as_ObjectValue();\n@@ -1654,1 +1654,10 @@\n-    reassign_fields_by_klass(vk, fr, reg_map, val->as_ObjectValue(), 0, (oop)obj, is_jvmci, offset, CHECK);\n+    reassign_fields_by_klass(vk, fr, reg_map, val, 0, (oop)obj, is_jvmci, offset, CHECK);\n+    if (!obj->is_null_free_array()) {\n+      jboolean null_marker_value;\n+      if (val->maybe_null()) {\n+        null_marker_value = StackValue::create_stack_value(fr, reg_map, val->null_marker())->get_jint() & 1;\n+      } else {\n+        null_marker_value = 1;\n+      }\n+      obj->bool_field_put(offset + vk->null_marker_offset(), null_marker_value);\n+    }\n","filename":"src\/hotspot\/share\/runtime\/deoptimization.cpp","additions":12,"deletions":3,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -0,0 +1,102 @@\n+\/*\n+ * Copyright (c) 2025, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+package compiler.valhalla.inlinetypes;\n+\n+import compiler.lib.ir_framework.*;\n+import jdk.internal.vm.annotation.NullRestricted;\n+import jdk.internal.vm.annotation.Strict;\n+\n+\/*\n+ * @test\n+ * @bug 8364191\n+ * @summary Test the removal of allocations of objects with atomic flat fields\n+ * @library \/test\/lib \/\n+ * @enablePreview\n+ * @modules java.base\/jdk.internal.value\n+ *          java.base\/jdk.internal.vm.annotation\n+ * @run main compiler.valhalla.inlinetypes.TestScalarReplaceFlatFields\n+ *\/\n+public class TestScalarReplaceFlatFields {\n+    @DontInline\n+    private static void call() {}\n+\n+    static value class V0 {\n+        byte v1;\n+        byte v2;\n+\n+        V0(int v1, int v2) {\n+            this.v1 = (byte) v1;\n+            this.v2 = (byte) v2;\n+        }\n+    }\n+\n+    static value class V1 {\n+        V0 v;\n+        short s;\n+\n+        V1(V0 v, int s) {\n+            this.v = v;\n+            this.s = (short) s;\n+        }\n+    }\n+\n+    static class Holder {\n+        @Strict\n+        @NullRestricted\n+        V1 v1;\n+        V1 v2;\n+\n+        Holder(V1 v1, V1 v2) {\n+            this.v1 = v1;\n+            this.v2 = v2;\n+        }\n+    }\n+\n+    @Test\n+    @IR(failOn = IRNode.ALLOC)\n+    @Arguments(values = {Argument.RANDOM_EACH})\n+    private static int testField(int v) {\n+        V1 v1 = new V1(null, v);\n+        V1 v2 = new V1(new V0(v, v), v);\n+        Holder h = new Holder(v1, v2);\n+        call();\n+        return h.v1.s;\n+    }\n+\n+    @Test\n+    @IR(failOn = IRNode.ALLOC)\n+    @Arguments(values = {Argument.RANDOM_EACH, Argument.RANDOM_EACH})\n+    private static int testArray(int v1, int v2) {\n+        V1[] array = new V1[2];\n+        array[0] = new V1(null, v1);\n+        array[1] = new V1(new V0(v1, v2), v2);\n+        call();\n+        return array[1].v.v1;\n+    }\n+\n+    public static void main(String[] args) {\n+        InlineTypes.getFramework()\n+                .addScenarios(InlineTypes.DEFAULT_SCENARIOS)\n+                .start();\n+    }\n+}\n","filename":"test\/hotspot\/jtreg\/compiler\/valhalla\/inlinetypes\/TestScalarReplaceFlatFields.java","additions":102,"deletions":0,"binary":false,"changes":102,"status":"added"}]}