{"files":[{"patch":"@@ -705,0 +705,2 @@\n+# Default disabled within Valhalla until support added (JDK-8348568)\n+#\n@@ -707,1 +709,1 @@\n-  UTIL_ARG_ENABLE(NAME: cds-archive-coh, DEFAULT: auto, RESULT: BUILD_CDS_ARCHIVE_COH,\n+  UTIL_ARG_ENABLE(NAME: cds-archive-coh, DEFAULT: false, RESULT: BUILD_CDS_ARCHIVE_COH,\n","filename":"make\/autoconf\/jdk-options.m4","additions":3,"deletions":1,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -1492,0 +1492,1 @@\n+        args = concat(args, \"--with-version-pre=\" + version_numbers.get(\"DEFAULT_PROMOTED_VERSION_PRE\"));\n","filename":"make\/conf\/jib-profiles.js","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -42,1 +42,1 @@\n-DEFAULT_PROMOTED_VERSION_PRE=ea\n+DEFAULT_PROMOTED_VERSION_PRE=lworld5ea\n","filename":"make\/conf\/version-numbers.conf","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -35,0 +35,1 @@\n+#include \"ci\/ciInlineKlass.hpp\"\n@@ -41,0 +42,1 @@\n+#include \"oops\/oop.inline.hpp\"\n@@ -433,1 +435,1 @@\n-  __ remove_frame(initial_frame_size_in_bytes());\n+  __ remove_frame(initial_frame_size_in_bytes(), needs_stack_repair());\n@@ -477,0 +479,28 @@\n+  if (InlineTypeReturnedAsFields) {\n+    \/\/ Check if we are returning an non-null inline type and load its fields into registers\n+    ciType* return_type = compilation()->method()->return_type();\n+    if (return_type->is_inlinetype()) {\n+      ciInlineKlass* vk = return_type->as_inline_klass();\n+      if (vk->can_be_returned_as_fields()) {\n+        address unpack_handler = vk->unpack_handler();\n+        assert(unpack_handler != nullptr, \"must be\");\n+        __ far_call(RuntimeAddress(unpack_handler));\n+      }\n+    } else if (return_type->is_instance_klass() && (!return_type->is_loaded() || StressCallingConvention)) {\n+      Label skip;\n+      __ test_oop_is_not_inline_type(r0, rscratch2, skip);\n+\n+      \/\/ Load fields from a buffered value with an inline class specific handler\n+      __ load_klass(rscratch1 \/*dst*\/, r0 \/*src*\/);\n+      __ ldr(rscratch1, Address(rscratch1, InstanceKlass::adr_inlineklass_fixed_block_offset()));\n+      __ ldr(rscratch1, Address(rscratch1, InlineKlass::unpack_handler_offset()));\n+      \/\/ Unpack handler can be null if inline type is not scalarizable in returns\n+      __ cbz(rscratch1, skip);\n+      __ blr(rscratch1);\n+\n+      __ bind(skip);\n+    }\n+    \/\/ At this point, r0 points to the value object (for interpreter or C1 caller).\n+    \/\/ The fields of the object are copied into registers (for C2 caller).\n+  }\n+\n@@ -478,1 +508,1 @@\n-  __ remove_frame(initial_frame_size_in_bytes());\n+  __ remove_frame(initial_frame_size_in_bytes(), needs_stack_repair());\n@@ -490,0 +520,4 @@\n+int LIR_Assembler::store_inline_type_fields_to_buf(ciInlineKlass* vk) {\n+  return (__ store_inline_type_fields_to_buf(vk, false));\n+}\n+\n@@ -536,3 +570,1 @@\n-        if (patch_code == lir_patch_none) {\n-          jobject2reg(c->as_jobject(), dest->as_register());\n-        } else {\n+        if (patch_code != lir_patch_none) {\n@@ -540,0 +572,2 @@\n+        } else {\n+          jobject2reg(c->as_jobject(), dest->as_register());\n@@ -649,0 +683,2 @@\n+    \/\/ Non-null case is not handled on aarch64 but handled on x86\n+    \/\/ FIXME: do we need to add it here?\n@@ -996,0 +1032,14 @@\n+void LIR_Assembler::move(LIR_Opr src, LIR_Opr dst) {\n+  assert(dst->is_cpu_register(), \"must be\");\n+  assert(dst->type() == src->type(), \"must be\");\n+\n+  if (src->is_cpu_register()) {\n+    reg2reg(src, dst);\n+  } else if (src->is_stack()) {\n+    stack2reg(src, dst, dst->type());\n+  } else if (src->is_constant()) {\n+    const2reg(src, dst, lir_patch_none, nullptr);\n+  } else {\n+    ShouldNotReachHere();\n+  }\n+}\n@@ -1187,1 +1237,1 @@\n-  if (UseSlowPath ||\n+  if (UseSlowPath || op->is_null_free() ||\n@@ -1299,22 +1349,16 @@\n-  if (should_profile) {\n-    Register mdo  = klass_RInfo;\n-    __ mov_metadata(mdo, md->constant_encoding());\n-    Label not_null;\n-    __ cbnz(obj, not_null);\n-    \/\/ Object is null; update MDO and exit\n-    Address data_addr\n-      = __ form_address(rscratch2, mdo,\n-                        md->byte_offset_of_slot(data, DataLayout::flags_offset()),\n-                        0);\n-    __ ldrb(rscratch1, data_addr);\n-    __ orr(rscratch1, rscratch1, BitData::null_seen_byte_constant());\n-    __ strb(rscratch1, data_addr);\n-    __ b(*obj_is_null);\n-    __ bind(not_null);\n-\n-    Label update_done;\n-    Register recv = k_RInfo;\n-    __ load_klass(recv, obj);\n-    type_profile_helper(mdo, md, data, recv, &update_done);\n-    Address counter_addr(mdo, md->byte_offset_of_slot(data, CounterData::count_offset()));\n-    __ addptr(counter_addr, DataLayout::counter_increment);\n+  if (op->need_null_check()) {\n+    if (should_profile) {\n+      Register mdo  = klass_RInfo;\n+      __ mov_metadata(mdo, md->constant_encoding());\n+      Label not_null;\n+      __ cbnz(obj, not_null);\n+      \/\/ Object is null; update MDO and exit\n+      Address data_addr\n+        = __ form_address(rscratch2, mdo,\n+                          md->byte_offset_of_slot(data, DataLayout::flags_offset()),\n+                          0);\n+      __ ldrb(rscratch1, data_addr);\n+      __ orr(rscratch1, rscratch1, BitData::null_seen_byte_constant());\n+      __ strb(rscratch1, data_addr);\n+      __ b(*obj_is_null);\n+      __ bind(not_null);\n@@ -1322,3 +1366,11 @@\n-    __ bind(update_done);\n-  } else {\n-    __ cbz(obj, *obj_is_null);\n+      Label update_done;\n+      Register recv = k_RInfo;\n+      __ load_klass(recv, obj);\n+      type_profile_helper(mdo, md, data, recv, &update_done);\n+      Address counter_addr(mdo, md->byte_offset_of_slot(data, CounterData::count_offset()));\n+      __ addptr(counter_addr, DataLayout::counter_increment);\n+\n+      __ bind(update_done);\n+    } else {\n+      __ cbz(obj, *obj_is_null);\n+    }\n@@ -1482,0 +1534,106 @@\n+void LIR_Assembler::emit_opFlattenedArrayCheck(LIR_OpFlattenedArrayCheck* op) {\n+  \/\/ We are loading\/storing from\/to an array that *may* be a flat array (the\n+  \/\/ declared type is Object[], abstract[], interface[] or VT.ref[]).\n+  \/\/ If this array is a flat array, take the slow path.\n+  __ test_flat_array_oop(op->array()->as_register(), op->tmp()->as_register(), *op->stub()->entry());\n+  if (!op->value()->is_illegal()) {\n+    \/\/ The array is not a flat array, but it might be null-free. If we are storing\n+    \/\/ a null into a null-free array, take the slow path (which will throw NPE).\n+    Label skip;\n+    __ cbnz(op->value()->as_register(), skip);\n+    __ test_null_free_array_oop(op->array()->as_register(), op->tmp()->as_register(), *op->stub()->entry());\n+    __ bind(skip);\n+  }\n+}\n+\n+void LIR_Assembler::emit_opNullFreeArrayCheck(LIR_OpNullFreeArrayCheck* op) {\n+  \/\/ We are storing into an array that *may* be null-free (the declared type is\n+  \/\/ Object[], abstract[], interface[] or VT.ref[]).\n+  Label test_mark_word;\n+  Register tmp = op->tmp()->as_register();\n+  __ ldr(tmp, Address(op->array()->as_register(), oopDesc::mark_offset_in_bytes()));\n+  __ tst(tmp, markWord::unlocked_value);\n+  __ br(Assembler::NE, test_mark_word);\n+  __ load_prototype_header(tmp, op->array()->as_register());\n+  __ bind(test_mark_word);\n+  __ tst(tmp, markWord::null_free_array_bit_in_place);\n+}\n+\n+void LIR_Assembler::emit_opSubstitutabilityCheck(LIR_OpSubstitutabilityCheck* op) {\n+  Label L_oops_equal;\n+  Label L_oops_not_equal;\n+  Label L_end;\n+\n+  Register left  = op->left()->as_register();\n+  Register right = op->right()->as_register();\n+\n+  __ cmp(left, right);\n+  __ br(Assembler::EQ, L_oops_equal);\n+\n+  \/\/ (1) Null check -- if one of the operands is null, the other must not be null (because\n+  \/\/     the two references are not equal), so they are not substitutable,\n+  \/\/     FIXME: do null check only if the operand is nullable\n+  {\n+    __ cbz(left, L_oops_not_equal);\n+    __ cbz(right, L_oops_not_equal);\n+  }\n+\n+  ciKlass* left_klass = op->left_klass();\n+  ciKlass* right_klass = op->right_klass();\n+\n+  \/\/ (2) Inline type check -- if either of the operands is not a inline type,\n+  \/\/     they are not substitutable. We do this only if we are not sure that the\n+  \/\/     operands are inline type\n+  if ((left_klass == nullptr || right_klass == nullptr) ||\/\/ The klass is still unloaded, or came from a Phi node.\n+      !left_klass->is_inlinetype() || !right_klass->is_inlinetype()) {\n+    Register tmp1  = op->tmp1()->as_register();\n+    __ mov(tmp1, markWord::inline_type_pattern);\n+    __ ldr(rscratch1, Address(left, oopDesc::mark_offset_in_bytes()));\n+    __ andr(tmp1, tmp1, rscratch1);\n+    __ ldr(rscratch1, Address(right, oopDesc::mark_offset_in_bytes()));\n+    __ andr(tmp1, tmp1, rscratch1);\n+    __ cmp(tmp1, (u1)markWord::inline_type_pattern);\n+    __ br(Assembler::NE, L_oops_not_equal);\n+  }\n+\n+  \/\/ (3) Same klass check: if the operands are of different klasses, they are not substitutable.\n+  if (left_klass != nullptr && left_klass->is_inlinetype() && left_klass == right_klass) {\n+    \/\/ No need to load klass -- the operands are statically known to be the same inline klass.\n+    __ b(*op->stub()->entry());\n+  } else {\n+    Register left_klass_op = op->left_klass_op()->as_register();\n+    Register right_klass_op = op->right_klass_op()->as_register();\n+\n+    if (UseCompressedClassPointers) {\n+      __ ldrw(left_klass_op,  Address(left,  oopDesc::klass_offset_in_bytes()));\n+      __ ldrw(right_klass_op, Address(right, oopDesc::klass_offset_in_bytes()));\n+      __ cmpw(left_klass_op, right_klass_op);\n+    } else {\n+      __ ldr(left_klass_op,  Address(left,  oopDesc::klass_offset_in_bytes()));\n+      __ ldr(right_klass_op, Address(right, oopDesc::klass_offset_in_bytes()));\n+      __ cmp(left_klass_op, right_klass_op);\n+    }\n+\n+    __ br(Assembler::EQ, *op->stub()->entry()); \/\/ same klass -> do slow check\n+    \/\/ fall through to L_oops_not_equal\n+  }\n+\n+  __ bind(L_oops_not_equal);\n+  move(op->not_equal_result(), op->result_opr());\n+  __ b(L_end);\n+\n+  __ bind(L_oops_equal);\n+  move(op->equal_result(), op->result_opr());\n+  __ b(L_end);\n+\n+  \/\/ We've returned from the stub. R0 contains 0x0 IFF the two\n+  \/\/ operands are not substitutable. (Don't compare against 0x1 in case the\n+  \/\/ C compiler is naughty)\n+  __ bind(*op->stub()->continuation());\n+  __ cbz(r0, L_oops_not_equal); \/\/ (call_stub() == 0x0) -> not_equal\n+  move(op->equal_result(), op->result_opr()); \/\/ (call_stub() != 0x0) -> equal\n+  \/\/ fall-through\n+  __ bind(L_end);\n+}\n+\n+\n@@ -1995,1 +2153,1 @@\n-  add_call_info(code_offset(), op->info());\n+  add_call_info(code_offset(), op->info(), op->maybe_return_as_fields());\n@@ -2006,1 +2164,1 @@\n-  add_call_info(code_offset(), op->info());\n+  add_call_info(code_offset(), op->info(), op->maybe_return_as_fields());\n@@ -2169,0 +2327,12 @@\n+void LIR_Assembler::arraycopy_inlinetype_check(Register obj, Register tmp, CodeStub* slow_path, bool is_dest, bool null_check) {\n+  if (null_check) {\n+    __ cbz(obj, *slow_path->entry());\n+  }\n+  if (is_dest) {\n+    __ test_null_free_array_oop(obj, tmp, *slow_path->entry());\n+    \/\/ TODO 8350865 Flat no longer implies null-free, so we need to check for flat dest. Can we do better here?\n+    __ test_flat_array_oop(obj, tmp, *slow_path->entry());\n+  } else {\n+    __ test_flat_array_oop(obj, tmp, *slow_path->entry());\n+  }\n+}\n@@ -2187,0 +2357,6 @@\n+  if (flags & LIR_OpArrayCopy::always_slow_path) {\n+    __ b(*stub->entry());\n+    __ bind(*stub->continuation());\n+    return;\n+  }\n+\n@@ -2240,0 +2416,8 @@\n+  \/\/ Handle inline type arrays\n+  if (flags & LIR_OpArrayCopy::src_inlinetype_check) {\n+    arraycopy_inlinetype_check(src, tmp, stub, false, (flags & LIR_OpArrayCopy::src_null_check));\n+  }\n+  if (flags & LIR_OpArrayCopy::dst_inlinetype_check) {\n+    arraycopy_inlinetype_check(dst, tmp, stub, true, (flags & LIR_OpArrayCopy::dst_null_check));\n+  }\n+\n@@ -2794,0 +2978,20 @@\n+void LIR_Assembler::emit_profile_inline_type(LIR_OpProfileInlineType* op) {\n+  Register obj = op->obj()->as_register();\n+  Register tmp = op->tmp()->as_pointer_register();\n+  bool not_null = op->not_null();\n+  int flag = op->flag();\n+\n+  Label not_inline_type;\n+  if (!not_null) {\n+    __ cbz(obj, not_inline_type);\n+  }\n+\n+  __ test_oop_is_not_inline_type(obj, tmp, not_inline_type);\n+\n+  Address mdo_addr = as_Address(op->mdp()->as_address_ptr(), rscratch2);\n+  __ ldrb(rscratch1, mdo_addr);\n+  __ orr(rscratch1, rscratch1, flag);\n+  __ strb(rscratch1, mdo_addr);\n+\n+  __ bind(not_inline_type);\n+}\n@@ -2933,0 +3137,4 @@\n+void LIR_Assembler::check_orig_pc() {\n+  __ ldr(rscratch2, frame_map()->address_for_orig_pc_addr());\n+  __ cmp(rscratch2, (u1)NULL_WORD);\n+}\n","filename":"src\/hotspot\/cpu\/aarch64\/c1_LIRAssembler_aarch64.cpp","additions":241,"deletions":33,"binary":false,"changes":274,"status":"modified"},{"patch":"@@ -77,0 +77,3 @@\n+  void arraycopy_inlinetype_check(Register obj, Register tmp, CodeStub* slow_path, bool is_dest, bool null_check);\n+  void move(LIR_Opr src, LIR_Opr dst);\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/c1_LIRAssembler_aarch64.hpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -35,0 +35,1 @@\n+#include \"ci\/ciInlineKlass.hpp\"\n@@ -105,0 +106,6 @@\n+void LIRGenerator::init_temps_for_substitutability_check(LIR_Opr& tmp1, LIR_Opr& tmp2) {\n+  tmp1 = new_register(T_INT);\n+  tmp2 = LIR_OprFact::illegalOpr;\n+}\n+\n+\n@@ -326,0 +333,6 @@\n+\n+  CodeStub* throw_ie_stub =\n+      x->maybe_inlinetype() ?\n+      new SimpleExceptionStub(C1StubId::throw_identity_exception_id, obj.result(), state_for(x)) :\n+      nullptr;\n+\n@@ -330,1 +343,1 @@\n-                        x->monitor_no(), info_for_exception, info);\n+                x->monitor_no(), info_for_exception, info, throw_ie_stub);\n@@ -1130,1 +1143,1 @@\n-  CodeEmitInfo* info = state_for(x, x->state());\n+  CodeEmitInfo* info = state_for(x, x->needs_state_before() ? x->state_before() : x->state());\n@@ -1133,5 +1146,6 @@\n-                       FrameMap::r10_oop_opr,\n-                       FrameMap::r11_oop_opr,\n-                       FrameMap::r4_oop_opr,\n-                       LIR_OprFact::illegalOpr,\n-                       FrameMap::r3_metadata_opr, info);\n+               \/* allow_inline *\/ false,\n+               FrameMap::r10_oop_opr,\n+               FrameMap::r11_oop_opr,\n+               FrameMap::r4_oop_opr,\n+               LIR_OprFact::illegalOpr,\n+               FrameMap::r3_metadata_opr, info);\n@@ -1193,2 +1207,2 @@\n-  CodeStub* slow_path = new NewObjectArrayStub(klass_reg, len, reg, info);\n-  ciKlass* obj = (ciKlass*) ciObjArrayKlass::make(x->klass());\n+  ciKlass* obj = (ciKlass*) x->exact_type();\n+  CodeStub* slow_path = new NewObjectArrayStub(klass_reg, len, reg, info, x->is_null_free());\n@@ -1198,0 +1212,1 @@\n+\n@@ -1199,1 +1214,1 @@\n-  __ allocate_array(reg, len, tmp1, tmp2, tmp3, tmp4, T_OBJECT, klass_reg, slow_path);\n+  __ allocate_array(reg, len, tmp1, tmp2, tmp3, tmp4, T_OBJECT, klass_reg, slow_path, true, x->is_null_free());\n@@ -1275,0 +1290,3 @@\n+  if (x->is_null_free()) {\n+    __ null_check(obj.result(), new CodeEmitInfo(info_for_exception));\n+  }\n@@ -1293,0 +1311,2 @@\n+\n+\n@@ -1296,1 +1316,2 @@\n-               x->profiled_method(), x->profiled_bci());\n+               x->profiled_method(), x->profiled_bci(), x->is_null_free());\n+\n@@ -1379,1 +1400,6 @@\n-  __ cmp(lir_cond(cond), left, right);\n+  if (x->substitutability_check()) {\n+    substitutability_check(x, *xin, *yin);\n+  } else {\n+    __ cmp(lir_cond(cond), left, right);\n+  }\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/c1_LIRGenerator_aarch64.cpp","additions":38,"deletions":12,"binary":false,"changes":50,"status":"modified"},{"patch":"@@ -35,0 +35,1 @@\n+#include \"oops\/constMethodFlags.hpp\"\n@@ -38,0 +39,1 @@\n+#include \"oops\/inlineKlass.hpp\"\n@@ -211,0 +213,52 @@\n+void InterpreterMacroAssembler::allocate_instance(Register klass, Register new_obj,\n+                                                  Register t1, Register t2,\n+                                                  bool clear_fields, Label& alloc_failed) {\n+  MacroAssembler::allocate_instance(klass, new_obj, t1, t2, clear_fields, alloc_failed);\n+  if (DTraceMethodProbes) {\n+      \/\/ Trigger dtrace event for fastpath\n+    push(atos);\n+    call_VM_leaf(CAST_FROM_FN_PTR(address, static_cast<int (*)(oopDesc*)>(SharedRuntime::dtrace_object_alloc)), new_obj);\n+    pop(atos);\n+  }\n+}\n+\n+void InterpreterMacroAssembler::read_flat_field(Register entry,\n+                                                Register field_index, Register field_offset,\n+                                                Register temp, Register obj) {\n+  Label alloc_failed, done;\n+  const Register src = field_offset;\n+  const Register alloc_temp = r10;\n+  const Register dst_temp   = field_index;\n+  const Register layout_info = temp;\n+  assert_different_registers(obj, entry, field_index, field_offset, temp, alloc_temp);\n+\n+  \/\/ Grab the inline field klass\n+  ldr(rscratch1, Address(entry, in_bytes(ResolvedFieldEntry::field_holder_offset())));\n+  inline_layout_info(rscratch1, field_index, layout_info);\n+\n+  const Register field_klass = dst_temp;\n+  ldr(field_klass, Address(layout_info, in_bytes(InlineLayoutInfo::klass_offset())));\n+\n+  \/\/ allocate buffer\n+  push(obj); \/\/ save holder\n+  allocate_instance(field_klass, obj, alloc_temp, rscratch2, false, alloc_failed);\n+\n+  \/\/ Have an oop instance buffer, copy into it\n+  payload_address(obj, dst_temp, field_klass);  \/\/ danger, uses rscratch1\n+  pop(alloc_temp);             \/\/ restore holder\n+  lea(src, Address(alloc_temp, field_offset));\n+  \/\/ call_VM_leaf, clobbers a few regs, save restore new obj\n+  push(obj);\n+  flat_field_copy(IS_DEST_UNINITIALIZED, src, dst_temp, layout_info);\n+  pop(obj);\n+  b(done);\n+\n+  bind(alloc_failed);\n+  pop(obj);\n+  call_VM(obj, CAST_FROM_FN_PTR(address, InterpreterRuntime::read_flat_field),\n+          obj, entry);\n+\n+  bind(done);\n+  membar(Assembler::StoreStore);\n+}\n+\n@@ -245,1 +299,2 @@\n-                                                  Label& ok_is_subtype) {\n+                                                  Label& ok_is_subtype,\n+                                                  bool profile) {\n@@ -251,1 +306,3 @@\n-  profile_typecheck(r2, Rsub_klass, r5); \/\/ blows r2, reloads r5\n+  if (profile) {\n+    profile_typecheck(r2, Rsub_klass, r5); \/\/ blows r2, reloads r5\n+  }\n@@ -625,0 +682,1 @@\n+\n@@ -650,0 +708,31 @@\n+  if (state == atos && InlineTypeReturnedAsFields) {\n+    \/\/ Check if we are returning an non-null inline type and load its fields into registers\n+    Label skip;\n+    test_oop_is_not_inline_type(r0, rscratch2, skip);\n+\n+    \/\/ Load fields from a buffered value with an inline class specific handler\n+    load_klass(rscratch1 \/*dst*\/, r0 \/*src*\/);\n+    ldr(rscratch1, Address(rscratch1, InstanceKlass::adr_inlineklass_fixed_block_offset()));\n+    ldr(rscratch1, Address(rscratch1, InlineKlass::unpack_handler_offset()));\n+    \/\/ Unpack handler can be null if inline type is not scalarizable in returns\n+    cbz(rscratch1, skip);\n+\n+    blr(rscratch1);\n+#ifdef ASSERT\n+    \/\/ TODO 8284443 Enable\n+    if (StressCallingConvention && false) {\n+      Label skip_stress;\n+      ldr(rscratch1, Address(rfp, frame::interpreter_frame_method_offset * wordSize));\n+      ldrw(rscratch1, Address(rscratch1, Method::flags_offset()));\n+      tstw(rscratch1, MethodFlags::has_scalarized_return_flag());\n+      br(Assembler::EQ, skip_stress);\n+      load_klass(r0, r0);\n+      orr(r0, r0, 1);\n+      bind(skip_stress);\n+    }\n+#endif\n+    bind(skip);\n+    \/\/ Check above kills sender esp in rscratch2. Reload it.\n+    ldr(rscratch2, Address(rfp, frame::interpreter_frame_sender_sp_offset * wordSize));\n+  }\n+\n@@ -710,0 +799,4 @@\n+      if (EnableValhalla) {\n+        \/\/ Mask inline_type bit such that we go to the slow path if object is an inline type\n+        andr(swap_reg, swap_reg, ~((int) markWord::inline_type_bit_in_place));\n+      }\n@@ -1063,1 +1156,1 @@\n-void InterpreterMacroAssembler::profile_not_taken_branch(Register mdp) {\n+void InterpreterMacroAssembler::profile_not_taken_branch(Register mdp, bool acmp) {\n@@ -1075,1 +1168,1 @@\n-    update_mdp_by_constant(mdp, in_bytes(BranchData::branch_data_size()));\n+    update_mdp_by_constant(mdp, acmp ? in_bytes(ACmpData::acmp_data_size()) : in_bytes(BranchData::branch_data_size()));\n@@ -1398,0 +1491,114 @@\n+template <class ArrayData> void InterpreterMacroAssembler::profile_array_type(Register mdp,\n+                                                                              Register array,\n+                                                                              Register tmp) {\n+  if (ProfileInterpreter) {\n+    Label profile_continue;\n+\n+    \/\/ If no method data exists, go to profile_continue.\n+    test_method_data_pointer(mdp, profile_continue);\n+\n+    mov(tmp, array);\n+    profile_obj_type(tmp, Address(mdp, in_bytes(ArrayData::array_offset())));\n+\n+    Label not_flat;\n+    test_non_flat_array_oop(array, tmp, not_flat);\n+\n+    set_mdp_flag_at(mdp, ArrayData::flat_array_byte_constant());\n+\n+    bind(not_flat);\n+\n+    Label not_null_free;\n+    test_non_null_free_array_oop(array, tmp, not_null_free);\n+\n+    set_mdp_flag_at(mdp, ArrayData::null_free_array_byte_constant());\n+\n+    bind(not_null_free);\n+\n+    bind(profile_continue);\n+  }\n+}\n+\n+template void InterpreterMacroAssembler::profile_array_type<ArrayLoadData>(Register mdp,\n+                                                                           Register array,\n+                                                                           Register tmp);\n+template void InterpreterMacroAssembler::profile_array_type<ArrayStoreData>(Register mdp,\n+                                                                            Register array,\n+                                                                            Register tmp);\n+\n+void InterpreterMacroAssembler::profile_multiple_element_types(Register mdp, Register element, Register tmp, const Register tmp2) {\n+  if (ProfileInterpreter) {\n+    Label profile_continue;\n+\n+    \/\/ If no method data exists, go to profile_continue.\n+    test_method_data_pointer(mdp, profile_continue);\n+\n+    Label done, update;\n+    cbnz(element, update);\n+    set_mdp_flag_at(mdp, BitData::null_seen_byte_constant());\n+    b(done);\n+\n+    bind(update);\n+    load_klass(tmp, element);\n+\n+    \/\/ Record the object type.\n+    record_klass_in_profile(tmp, mdp, tmp2);\n+\n+    bind(done);\n+\n+    \/\/ The method data pointer needs to be updated.\n+    update_mdp_by_constant(mdp, in_bytes(ArrayStoreData::array_store_data_size()));\n+\n+    bind(profile_continue);\n+  }\n+}\n+\n+\n+void InterpreterMacroAssembler::profile_element_type(Register mdp,\n+                                                     Register element,\n+                                                     Register tmp) {\n+  if (ProfileInterpreter) {\n+    Label profile_continue;\n+\n+    \/\/ If no method data exists, go to profile_continue.\n+    test_method_data_pointer(mdp, profile_continue);\n+\n+    mov(tmp, element);\n+    profile_obj_type(tmp, Address(mdp, in_bytes(ArrayLoadData::element_offset())));\n+\n+    \/\/ The method data pointer needs to be updated.\n+    update_mdp_by_constant(mdp, in_bytes(ArrayLoadData::array_load_data_size()));\n+\n+    bind(profile_continue);\n+  }\n+}\n+\n+void InterpreterMacroAssembler::profile_acmp(Register mdp,\n+                                             Register left,\n+                                             Register right,\n+                                             Register tmp) {\n+  if (ProfileInterpreter) {\n+    Label profile_continue;\n+\n+    \/\/ If no method data exists, go to profile_continue.\n+    test_method_data_pointer(mdp, profile_continue);\n+\n+    mov(tmp, left);\n+    profile_obj_type(tmp, Address(mdp, in_bytes(ACmpData::left_offset())));\n+\n+    Label left_not_inline_type;\n+    test_oop_is_not_inline_type(left, tmp, left_not_inline_type);\n+    set_mdp_flag_at(mdp, ACmpData::left_inline_type_byte_constant());\n+    bind(left_not_inline_type);\n+\n+    mov(tmp, right);\n+    profile_obj_type(tmp, Address(mdp, in_bytes(ACmpData::right_offset())));\n+\n+    Label right_not_inline_type;\n+    test_oop_is_not_inline_type(right, tmp, right_not_inline_type);\n+    set_mdp_flag_at(mdp, ACmpData::right_inline_type_byte_constant());\n+    bind(right_not_inline_type);\n+\n+    bind(profile_continue);\n+  }\n+}\n+\n@@ -1696,1 +1903,1 @@\n-        assert(ReturnTypeEntry::static_cell_count() < TypeStackSlotEntries::per_arg_count(), \"can't move past ret type\");\n+        assert(SingleTypeEntry::static_cell_count() < TypeStackSlotEntries::per_arg_count(), \"can't move past ret type\");\n@@ -1742,1 +1949,1 @@\n-    Address mdo_ret_addr(mdp, -in_bytes(ReturnTypeEntry::size()));\n+    Address mdo_ret_addr(mdp, -in_bytes(SingleTypeEntry::size()));\n","filename":"src\/hotspot\/cpu\/aarch64\/interp_masm_aarch64.cpp","additions":213,"deletions":6,"binary":false,"changes":219,"status":"modified"},{"patch":"@@ -147,0 +147,22 @@\n+  \/\/ Kills t1 and t2, perserves klass, return allocation in new_obj\n+  void allocate_instance(Register klass, Register new_obj,\n+                         Register t1, Register t2,\n+                         bool clear_fields, Label& alloc_failed);\n+\n+  \/\/ Allocate instance in \"obj\" and read in the content of the inline field\n+  \/\/ NOTES:\n+  \/\/   - input holder object via \"obj\", which must be r0,\n+  \/\/     will return new instance via the same reg\n+  \/\/   - assumes holder_klass and valueKlass field klass have both been resolved\n+  void read_flat_field(Register entry,\n+                       Register field_index, Register field_offset,\n+                       Register temp, Register obj = r0);\n+\n+  \/\/ Allocate value buffer in \"obj\" and read in flat element at the given index\n+  \/\/ NOTES:\n+  \/\/   - Return via \"obj\" must be r0\n+  \/\/   - kills all given regs\n+  void read_flat_element(Register array, Register index,\n+                         Register t1, Register t2,\n+                         Register obj = r0);\n+\n@@ -191,1 +213,1 @@\n-  void gen_subtype_check( Register sub_klass, Label &ok_is_subtype );\n+  void gen_subtype_check( Register sub_klass, Label &ok_is_subtype, bool profile = true);\n@@ -283,1 +305,1 @@\n-  void profile_not_taken_branch(Register mdp);\n+  void profile_not_taken_branch(Register mdp, bool acmp = false);\n@@ -296,0 +318,4 @@\n+  template <class ArrayData> void profile_array_type(Register mdp, Register array, Register tmp);\n+  void profile_multiple_element_types(Register mdp, Register element, Register tmp, Register tmp2);\n+  void profile_element_type(Register mdp, Register element, Register tmp);\n+  void profile_acmp(Register mdp, Register left, Register right, Register tmp);\n","filename":"src\/hotspot\/cpu\/aarch64\/interp_masm_aarch64.hpp","additions":28,"deletions":2,"binary":false,"changes":30,"status":"modified"},{"patch":"@@ -44,0 +44,1 @@\n+#include \"oops\/inlineKlass.hpp\"\n@@ -470,0 +471,5 @@\n+\n+  if (state == atos && InlineTypeReturnedAsFields) {\n+    __ store_inline_type_fields_to_buf(nullptr, true);\n+  }\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/templateInterpreterGenerator_aarch64.cpp","additions":6,"deletions":0,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -2573,0 +2573,4 @@\n+void LIR_Assembler::emit_profile_inline_type(LIR_OpProfileInlineType* op) {\n+  Unimplemented();\n+}\n+\n","filename":"src\/hotspot\/cpu\/arm\/c1_LIRAssembler_arm.cpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -3200,0 +3200,3 @@\n+void LIR_Assembler::emit_profile_inline_type(LIR_OpProfileInlineType* op) {\n+  Unimplemented();\n+}\n","filename":"src\/hotspot\/cpu\/ppc\/c1_LIRAssembler_ppc.cpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -1835,1 +1835,1 @@\n-        assert(ReturnTypeEntry::static_cell_count() < TypeStackSlotEntries::per_arg_count(),\n+        assert(SingleTypeEntry::static_cell_count() < TypeStackSlotEntries::per_arg_count(),\n@@ -1876,1 +1876,1 @@\n-    profile_obj_type(ret, R28_mdx, -in_bytes(ReturnTypeEntry::size()), tmp1, tmp2);\n+    profile_obj_type(ret, R28_mdx, -in_bytes(SingleTypeEntry::size()), tmp1, tmp2);\n","filename":"src\/hotspot\/cpu\/ppc\/interp_masm_ppc_64.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -3072,0 +3072,4 @@\n+void LIR_Assembler::emit_profile_inline_type(LIR_OpProfileInlineType* op) {\n+  Unimplemented();\n+}\n+\n","filename":"src\/hotspot\/cpu\/s390\/c1_LIRAssembler_s390.cpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -1763,1 +1763,1 @@\n-        assert(ReturnTypeEntry::static_cell_count() < TypeStackSlotEntries::per_arg_count(), \"can't move past ret type\");\n+        assert(SingleTypeEntry::static_cell_count() < TypeStackSlotEntries::per_arg_count(), \"can't move past ret type\");\n@@ -1812,1 +1812,1 @@\n-    Address mdo_ret_addr(mdp, -in_bytes(ReturnTypeEntry::size()));\n+    Address mdo_ret_addr(mdp, -in_bytes(SingleTypeEntry::size()));\n","filename":"src\/hotspot\/cpu\/s390\/interp_masm_s390.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -32,0 +32,1 @@\n+#include \"oops\/objArrayKlass.hpp\"\n@@ -172,0 +173,73 @@\n+\/\/ Implementation of LoadFlattenedArrayStub\n+\n+LoadFlattenedArrayStub::LoadFlattenedArrayStub(LIR_Opr array, LIR_Opr index, LIR_Opr result, CodeEmitInfo* info) {\n+  _array = array;\n+  _index = index;\n+  _result = result;\n+  \/\/ Tell the register allocator that the runtime call will scratch rax.\n+  _scratch_reg = FrameMap::rax_oop_opr;\n+  _info = new CodeEmitInfo(info);\n+}\n+\n+void LoadFlattenedArrayStub::emit_code(LIR_Assembler* ce) {\n+  assert(__ rsp_offset() == 0, \"frame size should be fixed\");\n+  __ bind(_entry);\n+  ce->store_parameter(_array->as_register(), 1);\n+  ce->store_parameter(_index->as_register(), 0);\n+  __ call(RuntimeAddress(Runtime1::entry_for(C1StubId::load_flat_array_id)));\n+  ce->add_call_info_here(_info);\n+  ce->verify_oop_map(_info);\n+  if (_result->as_register() != rax) {\n+    __ movptr(_result->as_register(), rax);\n+  }\n+  __ jmp(_continuation);\n+}\n+\n+\n+\/\/ Implementation of StoreFlattenedArrayStub\n+\n+StoreFlattenedArrayStub::StoreFlattenedArrayStub(LIR_Opr array, LIR_Opr index, LIR_Opr value, CodeEmitInfo* info) {\n+  _array = array;\n+  _index = index;\n+  _value = value;\n+  \/\/ Tell the register allocator that the runtime call will scratch rax.\n+  _scratch_reg = FrameMap::rax_oop_opr;\n+  _info = new CodeEmitInfo(info);\n+}\n+\n+\n+void StoreFlattenedArrayStub::emit_code(LIR_Assembler* ce) {\n+  assert(__ rsp_offset() == 0, \"frame size should be fixed\");\n+  __ bind(_entry);\n+  ce->store_parameter(_array->as_register(), 2);\n+  ce->store_parameter(_index->as_register(), 1);\n+  ce->store_parameter(_value->as_register(), 0);\n+  __ call(RuntimeAddress(Runtime1::entry_for(C1StubId::store_flat_array_id)));\n+  ce->add_call_info_here(_info);\n+  ce->verify_oop_map(_info);\n+  __ jmp(_continuation);\n+}\n+\n+\n+\/\/ Implementation of SubstitutabilityCheckStub\n+\n+SubstitutabilityCheckStub::SubstitutabilityCheckStub(LIR_Opr left, LIR_Opr right, CodeEmitInfo* info) {\n+  _left = left;\n+  _right = right;\n+  \/\/ Tell the register allocator that the runtime call will scratch rax.\n+  _scratch_reg = FrameMap::rax_oop_opr;\n+  _info = new CodeEmitInfo(info);\n+}\n+\n+void SubstitutabilityCheckStub::emit_code(LIR_Assembler* ce) {\n+  assert(__ rsp_offset() == 0, \"frame size should be fixed\");\n+  __ bind(_entry);\n+  ce->store_parameter(_left->as_register(), 1);\n+  ce->store_parameter(_right->as_register(), 0);\n+  __ call(RuntimeAddress(Runtime1::entry_for(C1StubId::substitutability_check_id)));\n+  ce->add_call_info_here(_info);\n+  ce->verify_oop_map(_info);\n+  __ jmp(_continuation);\n+}\n+\n+\n@@ -224,1 +298,2 @@\n-NewObjectArrayStub::NewObjectArrayStub(LIR_Opr klass_reg, LIR_Opr length, LIR_Opr result, CodeEmitInfo* info) {\n+NewObjectArrayStub::NewObjectArrayStub(LIR_Opr klass_reg, LIR_Opr length, LIR_Opr result,\n+                                       CodeEmitInfo* info, bool is_null_free) {\n@@ -229,0 +304,1 @@\n+  _is_null_free = is_null_free;\n@@ -237,1 +313,5 @@\n-  __ call(RuntimeAddress(Runtime1::entry_for(C1StubId::new_object_array_id)));\n+  if (_is_null_free) {\n+    __ call(RuntimeAddress(Runtime1::entry_for(C1StubId::new_null_free_array_id)));\n+  } else {\n+    __ call(RuntimeAddress(Runtime1::entry_for(C1StubId::new_object_array_id)));\n+  }\n@@ -247,0 +327,9 @@\n+  if (_throw_ie_stub != nullptr) {\n+    \/\/ When we come here, _obj_reg has already been checked to be non-null.\n+    const int is_value_mask = markWord::inline_type_pattern;\n+    Register mark = _scratch_reg->as_register();\n+    __ movptr(mark, Address(_obj_reg->as_register(), oopDesc::mark_offset_in_bytes()));\n+    __ andptr(mark, is_value_mask);\n+    __ cmpl(mark, is_value_mask);\n+    __ jcc(Assembler::equal, *_throw_ie_stub->entry());\n+  }\n","filename":"src\/hotspot\/cpu\/x86\/c1_CodeStubs_x86.cpp","additions":91,"deletions":2,"binary":false,"changes":93,"status":"modified"},{"patch":"@@ -34,0 +34,1 @@\n+#include \"ci\/ciInlineKlass.hpp\"\n@@ -39,0 +40,1 @@\n+#include \"oops\/oop.inline.hpp\"\n@@ -446,1 +448,1 @@\n-  __ remove_frame(initial_frame_size_in_bytes());\n+  __ remove_frame(initial_frame_size_in_bytes(), needs_stack_repair());\n@@ -483,0 +485,31 @@\n+  if (InlineTypeReturnedAsFields) {\n+  #ifndef _LP64\n+     Unimplemented();\n+  #endif\n+    \/\/ Check if we are returning an non-null inline type and load its fields into registers\n+    ciType* return_type = compilation()->method()->return_type();\n+    if (return_type->is_inlinetype()) {\n+      ciInlineKlass* vk = return_type->as_inline_klass();\n+      if (vk->can_be_returned_as_fields()) {\n+        address unpack_handler = vk->unpack_handler();\n+        assert(unpack_handler != nullptr, \"must be\");\n+        __ call(RuntimeAddress(unpack_handler));\n+      }\n+    } else if (return_type->is_instance_klass() && (!return_type->is_loaded() || StressCallingConvention)) {\n+      Label skip;\n+      __ test_oop_is_not_inline_type(rax, rscratch1, skip);\n+\n+      \/\/ Load fields from a buffered value with an inline class specific handler\n+      __ load_klass(rdi, rax, rscratch1);\n+      __ movptr(rdi, Address(rdi, InstanceKlass::adr_inlineklass_fixed_block_offset()));\n+      __ movptr(rdi, Address(rdi, InlineKlass::unpack_handler_offset()));\n+      \/\/ Unpack handler can be null if inline type is not scalarizable in returns\n+      __ testptr(rdi, rdi);\n+      __ jcc(Assembler::zero, skip);\n+      __ call(rdi);\n+\n+      __ bind(skip);\n+    }\n+    \/\/ At this point, rax points to the value object (for interpreter or C1 caller).\n+    \/\/ The fields of the object are copied into registers (for C2 caller).\n+  }\n@@ -485,1 +518,1 @@\n-  __ remove_frame(initial_frame_size_in_bytes());\n+  __ remove_frame(initial_frame_size_in_bytes(), needs_stack_repair());\n@@ -507,0 +540,4 @@\n+int LIR_Assembler::store_inline_type_fields_to_buf(ciInlineKlass* vk) {\n+  return (__ store_inline_type_fields_to_buf(vk, false));\n+}\n+\n@@ -1401,1 +1438,1 @@\n-  if (UseSlowPath ||\n+  if (UseSlowPath || op->is_null_free() ||\n@@ -1500,24 +1537,26 @@\n-  __ testptr(obj, obj);\n-  if (op->should_profile()) {\n-    Label not_null;\n-    Register mdo  = klass_RInfo;\n-    __ mov_metadata(mdo, md->constant_encoding());\n-    __ jccb(Assembler::notEqual, not_null);\n-    \/\/ Object is null; update MDO and exit\n-    Address data_addr(mdo, md->byte_offset_of_slot(data, DataLayout::flags_offset()));\n-    int header_bits = BitData::null_seen_byte_constant();\n-    __ orb(data_addr, header_bits);\n-    __ jmp(*obj_is_null);\n-    __ bind(not_null);\n-\n-    Label update_done;\n-    Register recv = k_RInfo;\n-    __ load_klass(recv, obj, tmp_load_klass);\n-    type_profile_helper(mdo, md, data, recv, &update_done);\n-\n-    Address nonprofiled_receiver_count_addr(mdo, md->byte_offset_of_slot(data, CounterData::count_offset()));\n-    __ addptr(nonprofiled_receiver_count_addr, DataLayout::counter_increment);\n-\n-    __ bind(update_done);\n-  } else {\n-    __ jcc(Assembler::equal, *obj_is_null);\n+  if (op->need_null_check()) {\n+    __ testptr(obj, obj);\n+    if (op->should_profile()) {\n+      Label not_null;\n+      Register mdo  = klass_RInfo;\n+      __ mov_metadata(mdo, md->constant_encoding());\n+      __ jccb(Assembler::notEqual, not_null);\n+      \/\/ Object is null; update MDO and exit\n+      Address data_addr(mdo, md->byte_offset_of_slot(data, DataLayout::flags_offset()));\n+      int header_bits = BitData::null_seen_byte_constant();\n+      __ orb(data_addr, header_bits);\n+      __ jmp(*obj_is_null);\n+      __ bind(not_null);\n+\n+      Label update_done;\n+      Register recv = k_RInfo;\n+      __ load_klass(recv, obj, tmp_load_klass);\n+      type_profile_helper(mdo, md, data, recv, &update_done);\n+\n+      Address nonprofiled_receiver_count_addr(mdo, md->byte_offset_of_slot(data, CounterData::count_offset()));\n+      __ addptr(nonprofiled_receiver_count_addr, DataLayout::counter_increment);\n+\n+      __ bind(update_done);\n+    } else {\n+      __ jcc(Assembler::equal, *obj_is_null);\n+    }\n@@ -1715,0 +1754,103 @@\n+void LIR_Assembler::emit_opFlattenedArrayCheck(LIR_OpFlattenedArrayCheck* op) {\n+  \/\/ We are loading\/storing from\/to an array that *may* be a flat array (the\n+  \/\/ declared type is Object[], abstract[], interface[] or VT.ref[]).\n+  \/\/ If this array is a flat array, take the slow path.\n+  __ test_flat_array_oop(op->array()->as_register(), op->tmp()->as_register(), *op->stub()->entry());\n+  if (!op->value()->is_illegal()) {\n+    \/\/ TODO 8350865 This is also used for profiling code, right? And in that case we don't care about null but just want to know if the array is flat or not.\n+    \/\/ The array is not a flat array, but it might be null-free. If we are storing\n+    \/\/ a null into a null-free array, take the slow path (which will throw NPE).\n+    Label skip;\n+    __ cmpptr(op->value()->as_register(), NULL_WORD);\n+    __ jcc(Assembler::notEqual, skip);\n+    __ test_null_free_array_oop(op->array()->as_register(), op->tmp()->as_register(), *op->stub()->entry());\n+    __ bind(skip);\n+  }\n+}\n+\n+void LIR_Assembler::emit_opNullFreeArrayCheck(LIR_OpNullFreeArrayCheck* op) {\n+  \/\/ We are storing into an array that *may* be null-free (the declared type is\n+  \/\/ Object[], abstract[], interface[] or VT.ref[]).\n+  Label test_mark_word;\n+  Register tmp = op->tmp()->as_register();\n+  __ movptr(tmp, Address(op->array()->as_register(), oopDesc::mark_offset_in_bytes()));\n+  __ testl(tmp, markWord::unlocked_value);\n+  __ jccb(Assembler::notZero, test_mark_word);\n+  __ load_prototype_header(tmp, op->array()->as_register(), rscratch1);\n+  __ bind(test_mark_word);\n+  __ testl(tmp, markWord::null_free_array_bit_in_place);\n+}\n+\n+void LIR_Assembler::emit_opSubstitutabilityCheck(LIR_OpSubstitutabilityCheck* op) {\n+  Label L_oops_equal;\n+  Label L_oops_not_equal;\n+  Label L_end;\n+\n+  Register left  = op->left()->as_register();\n+  Register right = op->right()->as_register();\n+\n+  __ cmpptr(left, right);\n+  __ jcc(Assembler::equal, L_oops_equal);\n+\n+  \/\/ (1) Null check -- if one of the operands is null, the other must not be null (because\n+  \/\/     the two references are not equal), so they are not substitutable,\n+  \/\/     FIXME: do null check only if the operand is nullable\n+  __ testptr(left, right);\n+  __ jcc(Assembler::zero, L_oops_not_equal);\n+\n+  ciKlass* left_klass = op->left_klass();\n+  ciKlass* right_klass = op->right_klass();\n+\n+  \/\/ (2) Inline type check -- if either of the operands is not a inline type,\n+  \/\/     they are not substitutable. We do this only if we are not sure that the\n+  \/\/     operands are inline type\n+  if ((left_klass == nullptr || right_klass == nullptr) ||\/\/ The klass is still unloaded, or came from a Phi node.\n+      !left_klass->is_inlinetype() || !right_klass->is_inlinetype()) {\n+    Register tmp1  = op->tmp1()->as_register();\n+    __ movptr(tmp1, (intptr_t)markWord::inline_type_pattern);\n+    __ andptr(tmp1, Address(left, oopDesc::mark_offset_in_bytes()));\n+    __ andptr(tmp1, Address(right, oopDesc::mark_offset_in_bytes()));\n+    __ cmpptr(tmp1, (intptr_t)markWord::inline_type_pattern);\n+    __ jcc(Assembler::notEqual, L_oops_not_equal);\n+  }\n+\n+  \/\/ (3) Same klass check: if the operands are of different klasses, they are not substitutable.\n+  if (left_klass != nullptr && left_klass->is_inlinetype() && left_klass == right_klass) {\n+    \/\/ No need to load klass -- the operands are statically known to be the same inline klass.\n+    __ jmp(*op->stub()->entry());\n+  } else {\n+    Register left_klass_op = op->left_klass_op()->as_register();\n+    Register right_klass_op = op->right_klass_op()->as_register();\n+\n+    if (UseCompressedClassPointers) {\n+      __ movl(left_klass_op,  Address(left,  oopDesc::klass_offset_in_bytes()));\n+      __ movl(right_klass_op, Address(right, oopDesc::klass_offset_in_bytes()));\n+      __ cmpl(left_klass_op, right_klass_op);\n+    } else {\n+      __ movptr(left_klass_op,  Address(left,  oopDesc::klass_offset_in_bytes()));\n+      __ movptr(right_klass_op, Address(right, oopDesc::klass_offset_in_bytes()));\n+      __ cmpptr(left_klass_op, right_klass_op);\n+    }\n+\n+    __ jcc(Assembler::equal, *op->stub()->entry()); \/\/ same klass -> do slow check\n+    \/\/ fall through to L_oops_not_equal\n+  }\n+\n+  __ bind(L_oops_not_equal);\n+  move(op->not_equal_result(), op->result_opr());\n+  __ jmp(L_end);\n+\n+  __ bind(L_oops_equal);\n+  move(op->equal_result(), op->result_opr());\n+  __ jmp(L_end);\n+\n+  \/\/ We've returned from the stub. RAX contains 0x0 IFF the two\n+  \/\/ operands are not substitutable. (Don't compare against 0x1 in case the\n+  \/\/ C compiler is naughty)\n+  __ bind(*op->stub()->continuation());\n+  __ cmpl(rax, 0);\n+  __ jcc(Assembler::equal, L_oops_not_equal); \/\/ (call_stub() == 0x0) -> not_equal\n+  move(op->equal_result(), op->result_opr()); \/\/ (call_stub() != 0x0) -> equal\n+  \/\/ fall-through\n+  __ bind(L_end);\n+}\n@@ -1775,0 +1917,15 @@\n+void LIR_Assembler::move(LIR_Opr src, LIR_Opr dst) {\n+  assert(dst->is_cpu_register(), \"must be\");\n+  assert(dst->type() == src->type(), \"must be\");\n+\n+  if (src->is_cpu_register()) {\n+    reg2reg(src, dst);\n+  } else if (src->is_stack()) {\n+    stack2reg(src, dst, dst->type());\n+  } else if (src->is_constant()) {\n+    const2reg(src, dst, lir_patch_none, nullptr);\n+  } else {\n+    ShouldNotReachHere();\n+  }\n+}\n+\n@@ -2487,1 +2644,1 @@\n-  add_call_info(code_offset(), op->info());\n+  add_call_info(code_offset(), op->info(), op->maybe_return_as_fields());\n@@ -2494,1 +2651,1 @@\n-  add_call_info(code_offset(), op->info());\n+  add_call_info(code_offset(), op->info(), op->maybe_return_as_fields());\n@@ -2675,0 +2832,15 @@\n+void LIR_Assembler::arraycopy_inlinetype_check(Register obj, Register tmp, CodeStub* slow_path, bool is_dest, bool null_check) {\n+  if (null_check) {\n+    __ testptr(obj, obj);\n+    __ jcc(Assembler::zero, *slow_path->entry());\n+  }\n+  if (is_dest) {\n+    __ test_null_free_array_oop(obj, tmp, *slow_path->entry());\n+    \/\/ TODO 8350865 Flat no longer implies null-free, so we need to check for flat dest. Can we do better here?\n+    __ test_flat_array_oop(obj, tmp, *slow_path->entry());\n+  } else {\n+    __ test_flat_array_oop(obj, tmp, *slow_path->entry());\n+  }\n+}\n+\n+\n@@ -2694,0 +2866,6 @@\n+  if (flags & LIR_OpArrayCopy::always_slow_path) {\n+    __ jmp(*stub->entry());\n+    __ bind(*stub->continuation());\n+    return;\n+  }\n+\n@@ -2787,0 +2965,8 @@\n+  \/\/ Handle inline type arrays\n+  if (flags & LIR_OpArrayCopy::src_inlinetype_check) {\n+    arraycopy_inlinetype_check(src, tmp, stub, false, (flags & LIR_OpArrayCopy::src_null_check));\n+  }\n+  if (flags & LIR_OpArrayCopy::dst_inlinetype_check) {\n+    arraycopy_inlinetype_check(dst, tmp, stub, true, (flags & LIR_OpArrayCopy::dst_null_check));\n+  }\n+\n@@ -3399,0 +3585,20 @@\n+void LIR_Assembler::emit_profile_inline_type(LIR_OpProfileInlineType* op) {\n+  Register obj = op->obj()->as_register();\n+  Register tmp = op->tmp()->as_pointer_register();\n+  Address mdo_addr = as_Address(op->mdp()->as_address_ptr());\n+  bool not_null = op->not_null();\n+  int flag = op->flag();\n+\n+  Label not_inline_type;\n+  if (!not_null) {\n+    __ testptr(obj, obj);\n+    __ jccb(Assembler::zero, not_inline_type);\n+  }\n+\n+  __ test_oop_is_not_inline_type(obj, tmp, not_inline_type);\n+\n+  __ orb(mdo_addr, flag);\n+\n+  __ bind(not_inline_type);\n+}\n+\n@@ -3612,0 +3818,3 @@\n+void LIR_Assembler::check_orig_pc() {\n+  __ cmpptr(frame_map()->address_for_orig_pc_addr(), NULL_WORD);\n+}\n","filename":"src\/hotspot\/cpu\/x86\/c1_LIRAssembler_x86.cpp","additions":238,"deletions":29,"binary":false,"changes":267,"status":"modified"},{"patch":"@@ -54,0 +54,3 @@\n+  void arraycopy_inlinetype_check(Register obj, Register tmp, CodeStub* slow_path, bool is_dest, bool null_check);\n+  void move(LIR_Opr src, LIR_Opr dst);\n+\n","filename":"src\/hotspot\/cpu\/x86\/c1_LIRAssembler_x86.hpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -33,0 +33,1 @@\n+#include \"ci\/ciInlineKlass.hpp\"\n@@ -113,0 +114,13 @@\n+void LIRGenerator::init_temps_for_substitutability_check(LIR_Opr& tmp1, LIR_Opr& tmp2) {\n+  \/\/ We just need one 32-bit temp register for x86\/x64, to check whether both\n+  \/\/ oops have markWord::always_locked_pattern. See LIR_Assembler::emit_opSubstitutabilityCheck().\n+  \/\/ @temp = %r10d\n+  \/\/ mov $0x405, %r10d\n+  \/\/ and (%left), %r10d   \/* if need to check left *\/\n+  \/\/ and (%right), %r10d  \/* if need to check right *\/\n+  \/\/ cmp $0x405, $r10d\n+  \/\/ jne L_oops_not_equal\n+  tmp1 = new_register(T_INT);\n+  tmp2 = LIR_OprFact::illegalOpr;\n+}\n+\n@@ -308,0 +322,6 @@\n+  \/\/ Need a scratch register for inline types on x86\n+  LIR_Opr scratch = LIR_OprFact::illegalOpr;\n+  if ((LockingMode == LM_LIGHTWEIGHT) ||\n+      (EnableValhalla && x->maybe_inlinetype())) {\n+    scratch = new_register(T_ADDRESS);\n+  }\n@@ -313,0 +333,6 @@\n+\n+  CodeStub* throw_ie_stub = x->maybe_inlinetype() ?\n+      new SimpleExceptionStub(C1StubId::throw_identity_exception_id,\n+                              obj.result(), state_for(x))\n+    : nullptr;\n+\n@@ -316,3 +342,2 @@\n-  LIR_Opr tmp = LockingMode == LM_LIGHTWEIGHT ? new_register(T_ADDRESS) : LIR_OprFact::illegalOpr;\n-  monitor_enter(obj.result(), lock, syncTempOpr(), tmp,\n-                        x->monitor_no(), info_for_exception, info);\n+  monitor_enter(obj.result(), lock, syncTempOpr(), scratch,\n+                x->monitor_no(), info_for_exception, info, throw_ie_stub);\n@@ -1291,1 +1316,1 @@\n-  CodeEmitInfo* info = state_for(x, x->state());\n+  CodeEmitInfo* info = state_for(x, x->needs_state_before() ? x->state_before() : x->state());\n@@ -1294,5 +1319,6 @@\n-                       FrameMap::rcx_oop_opr,\n-                       FrameMap::rdi_oop_opr,\n-                       FrameMap::rsi_oop_opr,\n-                       LIR_OprFact::illegalOpr,\n-                       FrameMap::rdx_metadata_opr, info);\n+               !x->is_unresolved() && x->klass()->is_inlinetype(),\n+               FrameMap::rcx_oop_opr,\n+               FrameMap::rdi_oop_opr,\n+               FrameMap::rsi_oop_opr,\n+               LIR_OprFact::illegalOpr,\n+               FrameMap::rdx_metadata_opr, info);\n@@ -1303,1 +1329,0 @@\n-\n@@ -1356,2 +1381,2 @@\n-  CodeStub* slow_path = new NewObjectArrayStub(klass_reg, len, reg, info);\n-  ciKlass* obj = (ciKlass*) ciObjArrayKlass::make(x->klass());\n+  ciKlass* obj = (ciKlass*) x->exact_type();\n+  CodeStub* slow_path = new NewObjectArrayStub(klass_reg, len, reg, info, x->is_null_free());\n@@ -1362,1 +1387,1 @@\n-  __ allocate_array(reg, len, tmp1, tmp2, tmp3, tmp4, T_OBJECT, klass_reg, slow_path);\n+  __ allocate_array(reg, len, tmp1, tmp2, tmp3, tmp4, T_OBJECT, klass_reg, slow_path, true, x->is_null_free());\n@@ -1441,0 +1466,4 @@\n+  if (x->is_null_free()) {\n+    __ null_check(obj.result(), new CodeEmitInfo(info_for_exception));\n+  }\n+\n@@ -1459,1 +1488,1 @@\n-               x->profiled_method(), x->profiled_bci());\n+               x->profiled_method(), x->profiled_bci(), x->is_null_free());\n@@ -1515,1 +1544,1 @@\n-  } else if (tag == longTag || tag == floatTag || tag == doubleTag) {\n+  } else if (tag == longTag || tag == floatTag || tag == doubleTag || x->substitutability_check()) {\n@@ -1535,1 +1564,5 @@\n-  __ cmp(lir_cond(cond), left, right);\n+  if (x->substitutability_check()) {\n+    substitutability_check(x, *xin, *yin);\n+  } else {\n+    __ cmp(lir_cond(cond), left, right);\n+  }\n","filename":"src\/hotspot\/cpu\/x86\/c1_LIRGenerator_x86.cpp","additions":49,"deletions":16,"binary":false,"changes":65,"status":"modified"},{"patch":"@@ -37,0 +37,1 @@\n+#include \"runtime\/frame.inline.hpp\"\n@@ -80,0 +81,4 @@\n+    if (EnableValhalla) {\n+      \/\/ Mask inline_type bit such that we go to the slow path if object is an inline type\n+      andptr(hdr, ~((int) markWord::inline_type_bit_in_place));\n+    }\n@@ -172,2 +177,1 @@\n-#ifdef _LP64\n-  if (UseCompactObjectHeaders) {\n+  if (UseCompactObjectHeaders || EnableValhalla) {\n@@ -176,1 +180,1 @@\n-  } else if (UseCompressedClassPointers) { \/\/ Take care not to kill klass\n+  } else {\n@@ -178,0 +182,3 @@\n+  }\n+#ifdef _LP64\n+  if (UseCompressedClassPointers) { \/\/ Take care not to kill klass\n@@ -181,1 +188,1 @@\n-  } else\n+  } else if (!UseCompactObjectHeaders)\n@@ -184,1 +191,0 @@\n-    movptr(Address(obj, oopDesc::mark_offset_in_bytes()), checked_cast<int32_t>(markWord::prototype().value()));\n@@ -322,2 +328,20 @@\n-void C1_MacroAssembler::build_frame(int frame_size_in_bytes, int bang_size_in_bytes) {\n-  assert(bang_size_in_bytes >= frame_size_in_bytes, \"stack bang size incorrect\");\n+void C1_MacroAssembler::build_frame_helper(int frame_size_in_bytes, int sp_offset_for_orig_pc, int sp_inc, bool reset_orig_pc, bool needs_stack_repair) {\n+  push(rbp);\n+  if (PreserveFramePointer) {\n+    mov(rbp, rsp);\n+  }\n+  decrement(rsp, frame_size_in_bytes);\n+\n+  if (needs_stack_repair) {\n+    \/\/ Save stack increment (also account for fixed framesize and rbp)\n+    assert((sp_inc & (StackAlignmentInBytes-1)) == 0, \"stack increment not aligned\");\n+    int real_frame_size = sp_inc + frame_size_in_bytes + wordSize;\n+    movptr(Address(rsp, frame_size_in_bytes - wordSize), real_frame_size);\n+  }\n+  if (reset_orig_pc) {\n+    \/\/ Zero orig_pc to detect deoptimization during buffering in the entry points\n+    movptr(Address(rsp, sp_offset_for_orig_pc), 0);\n+  }\n+}\n+\n+void C1_MacroAssembler::build_frame(int frame_size_in_bytes, int bang_size_in_bytes, int sp_offset_for_orig_pc, bool needs_stack_repair, bool has_scalarized_args, Label* verified_inline_entry_label) {\n@@ -329,0 +353,1 @@\n+  assert(bang_size_in_bytes >= frame_size_in_bytes, \"stack bang size incorrect\");\n@@ -331,5 +356,1 @@\n-  push(rbp);\n-  if (PreserveFramePointer) {\n-    mov(rbp, rsp);\n-  }\n-  decrement(rsp, frame_size_in_bytes); \/\/ does not emit code for frame_size == 0\n+  build_frame_helper(frame_size_in_bytes, sp_offset_for_orig_pc, 0, has_scalarized_args, needs_stack_repair);\n@@ -340,5 +361,4 @@\n-}\n-\n-void C1_MacroAssembler::remove_frame(int frame_size_in_bytes) {\n-  increment(rsp, frame_size_in_bytes);  \/\/ Does not emit code for frame_size == 0\n-  pop(rbp);\n+  if (verified_inline_entry_label != nullptr) {\n+    \/\/ Jump here from the scalarized entry points that already created the frame.\n+    bind(*verified_inline_entry_label);\n+  }\n@@ -348,1 +368,0 @@\n-\n@@ -365,0 +384,58 @@\n+int C1_MacroAssembler::scalarized_entry(const CompiledEntrySignature* ces, int frame_size_in_bytes, int bang_size_in_bytes, int sp_offset_for_orig_pc, Label& verified_inline_entry_label, bool is_inline_ro_entry) {\n+  assert(InlineTypePassFieldsAsArgs, \"sanity\");\n+  \/\/ Make sure there is enough stack space for this method's activation.\n+  assert(bang_size_in_bytes >= frame_size_in_bytes, \"stack bang size incorrect\");\n+  generate_stack_overflow_check(bang_size_in_bytes);\n+\n+  GrowableArray<SigEntry>* sig    = ces->sig();\n+  GrowableArray<SigEntry>* sig_cc = is_inline_ro_entry ? ces->sig_cc_ro() : ces->sig_cc();\n+  VMRegPair* regs      = ces->regs();\n+  VMRegPair* regs_cc   = is_inline_ro_entry ? ces->regs_cc_ro() : ces->regs_cc();\n+  int args_on_stack    = ces->args_on_stack();\n+  int args_on_stack_cc = is_inline_ro_entry ? ces->args_on_stack_cc_ro() : ces->args_on_stack_cc();\n+\n+  assert(sig->length() <= sig_cc->length(), \"Zero-sized inline class not allowed!\");\n+  BasicType* sig_bt = NEW_RESOURCE_ARRAY(BasicType, sig_cc->length());\n+  int args_passed = sig->length();\n+  int args_passed_cc = SigEntry::fill_sig_bt(sig_cc, sig_bt);\n+\n+  \/\/ Create a temp frame so we can call into the runtime. It must be properly set up to accommodate GC.\n+  build_frame_helper(frame_size_in_bytes, sp_offset_for_orig_pc, 0, true, ces->c1_needs_stack_repair());\n+\n+  \/\/ The runtime call might safepoint, make sure nmethod entry barrier is executed\n+  BarrierSetAssembler* bs = BarrierSet::barrier_set()->barrier_set_assembler();\n+  \/\/ C1 code is not hot enough to micro optimize the nmethod entry barrier with an out-of-line stub\n+  bs->nmethod_entry_barrier(this, nullptr \/* slow_path *\/, nullptr \/* continuation *\/);\n+\n+  \/\/ FIXME -- call runtime only if we cannot in-line allocate all the incoming inline type args.\n+  movptr(rbx, (intptr_t)(ces->method()));\n+  if (is_inline_ro_entry) {\n+    call(RuntimeAddress(Runtime1::entry_for(C1StubId::buffer_inline_args_no_receiver_id)));\n+  } else {\n+    call(RuntimeAddress(Runtime1::entry_for(C1StubId::buffer_inline_args_id)));\n+  }\n+  int rt_call_offset = offset();\n+\n+  \/\/ Remove the temp frame\n+  addptr(rsp, frame_size_in_bytes);\n+  pop(rbp);\n+\n+  \/\/ Check if we need to extend the stack for packing\n+  int sp_inc = 0;\n+  if (args_on_stack > args_on_stack_cc) {\n+    sp_inc = extend_stack_for_inline_args(args_on_stack);\n+  }\n+\n+  shuffle_inline_args(true, is_inline_ro_entry, sig_cc,\n+                      args_passed_cc, args_on_stack_cc, regs_cc, \/\/ from\n+                      args_passed, args_on_stack, regs,          \/\/ to\n+                      sp_inc, rax);\n+\n+  \/\/ Create the real frame. Below jump will then skip over the stack banging and frame\n+  \/\/ setup code in the verified_inline_entry (which has a different real_frame_size).\n+  build_frame_helper(frame_size_in_bytes, sp_offset_for_orig_pc, sp_inc, false, ces->c1_needs_stack_repair());\n+\n+  jmp(verified_inline_entry_label);\n+  return rt_call_offset;\n+}\n+\n","filename":"src\/hotspot\/cpu\/x86\/c1_MacroAssembler_x86.cpp","additions":95,"deletions":18,"binary":false,"changes":113,"status":"modified"},{"patch":"@@ -31,0 +31,1 @@\n+#include \"oops\/constMethodFlags.hpp\"\n@@ -34,0 +35,1 @@\n+#include \"oops\/inlineKlass.hpp\"\n@@ -168,1 +170,1 @@\n-        assert(ReturnTypeEntry::static_cell_count() < TypeStackSlotEntries::per_arg_count(), \"can't move past ret type\");\n+        assert(SingleTypeEntry::static_cell_count() < TypeStackSlotEntries::per_arg_count(), \"can't move past ret type\");\n@@ -213,1 +215,1 @@\n-    Address mdo_ret_addr(mdp, -in_bytes(ReturnTypeEntry::size()));\n+    Address mdo_ret_addr(mdp, -in_bytes(SingleTypeEntry::size()));\n@@ -520,1 +522,2 @@\n-                                                  Label& ok_is_subtype) {\n+                                                  Label& ok_is_subtype,\n+                                                  bool profile) {\n@@ -528,1 +531,3 @@\n-  profile_typecheck(rcx, Rsub_klass, rdi); \/\/ blows rcx, reloads rdi\n+  if (profile) {\n+    profile_typecheck(rcx, Rsub_klass, rdi); \/\/ blows rcx, reloads rdi\n+  }\n@@ -829,1 +834,1 @@\n- \/\/ get method access flags\n+  \/\/ get method access flags\n@@ -951,4 +956,2 @@\n-  \/\/ remove activation\n-  \/\/ get sender sp\n-  movptr(rbx,\n-         Address(rbp, frame::interpreter_frame_sender_sp_offset * wordSize));\n+    movptr(rbx,\n+               Address(rbp, frame::interpreter_frame_sender_sp_offset * wordSize));\n@@ -976,0 +979,40 @@\n+\n+  \/\/ remove activation\n+  \/\/ get sender sp\n+  movptr(rbx,\n+         Address(rbp, frame::interpreter_frame_sender_sp_offset * wordSize));\n+\n+  if (state == atos && InlineTypeReturnedAsFields) {\n+    \/\/ Check if we are returning an non-null inline type and load its fields into registers\n+    Label skip;\n+    test_oop_is_not_inline_type(rax, rscratch1, skip);\n+\n+#ifndef _LP64\n+    super_call_VM_leaf(StubRoutines::load_inline_type_fields_in_regs());\n+#else\n+    \/\/ Load fields from a buffered value with an inline class specific handler\n+    load_klass(rdi, rax, rscratch1);\n+    movptr(rdi, Address(rdi, InstanceKlass::adr_inlineklass_fixed_block_offset()));\n+    movptr(rdi, Address(rdi, InlineKlass::unpack_handler_offset()));\n+    \/\/ Unpack handler can be null if inline type is not scalarizable in returns\n+    testptr(rdi, rdi);\n+    jcc(Assembler::zero, skip);\n+    call(rdi);\n+#endif\n+#ifdef ASSERT\n+    \/\/ TODO 8284443 Enable\n+    if (StressCallingConvention && false) {\n+      Label skip_stress;\n+      movptr(rscratch1, Address(rbp, frame::interpreter_frame_method_offset * wordSize));\n+      movl(rscratch1, Address(rscratch1, Method::flags_offset()));\n+      testl(rcx, MethodFlags::has_scalarized_return_flag());\n+      jcc(Assembler::zero, skip_stress);\n+      load_klass(rax, rax, rscratch1);\n+      orptr(rax, 1);\n+      bind(skip_stress);\n+    }\n+#endif\n+    \/\/ call above kills the value in rbx. Reload it.\n+    movptr(rbx, Address(rbp, frame::interpreter_frame_sender_sp_offset * wordSize));\n+    bind(skip);\n+  }\n@@ -996,0 +1039,54 @@\n+void InterpreterMacroAssembler::allocate_instance(Register klass, Register new_obj,\n+                                                  Register t1, Register t2,\n+                                                  bool clear_fields, Label& alloc_failed) {\n+  MacroAssembler::allocate_instance(klass, new_obj, t1, t2, clear_fields, alloc_failed);\n+  if (DTraceMethodProbes) {\n+    \/\/ Trigger dtrace event for fastpath\n+    push(atos);\n+    call_VM_leaf(CAST_FROM_FN_PTR(address, static_cast<int (*)(oopDesc*)>(SharedRuntime::dtrace_object_alloc)), new_obj);\n+    pop(atos);\n+  }\n+}\n+\n+void InterpreterMacroAssembler::read_flat_field(Register entry, Register tmp1, Register tmp2, Register obj) {\n+  Label alloc_failed, done;\n+  const Register alloc_temp = LP64_ONLY(rscratch1) NOT_LP64(rsi);\n+  const Register dst_temp   = LP64_ONLY(rscratch2) NOT_LP64(rdi);\n+  assert_different_registers(obj, entry, tmp1, tmp2, dst_temp, r8, r9);\n+\n+  \/\/ FIXME: code below could be re-written to better use InlineLayoutInfo data structure\n+  \/\/ see aarch64 version\n+\n+  \/\/ Grap the inline field klass\n+  const Register field_klass = tmp1;\n+  load_unsigned_short(tmp2, Address(entry, in_bytes(ResolvedFieldEntry::field_index_offset())));\n+  movptr(tmp1, Address(entry, ResolvedFieldEntry::field_holder_offset()));\n+  get_inline_type_field_klass(tmp1, tmp2, field_klass);\n+\n+  \/\/ allocate buffer\n+  push(obj);  \/\/ push object being read from     \/\/ FIXME spilling on stack could probably be avoided by using tmp2\n+  allocate_instance(field_klass, obj, alloc_temp, dst_temp, false, alloc_failed);\n+\n+  \/\/ Have an oop instance buffer, copy into it\n+  load_unsigned_short(r9, Address(entry, in_bytes(ResolvedFieldEntry::field_index_offset())));\n+  movptr(r8, Address(entry, in_bytes(ResolvedFieldEntry::field_holder_offset())));\n+  inline_layout_info(r8, r9, r8); \/\/ holder, index, info => InlineLayoutInfo into r8\n+\n+  payload_addr(obj, dst_temp, field_klass);\n+  pop(alloc_temp);             \/\/ restore object being read from\n+  load_sized_value(tmp2, Address(entry, in_bytes(ResolvedFieldEntry::field_offset_offset())), sizeof(int), true \/*is_signed*\/);\n+  lea(tmp2, Address(alloc_temp, tmp2));\n+  \/\/ call_VM_leaf, clobbers a few regs, save restore new obj\n+  push(obj);\n+  \/\/ access_value_copy(IS_DEST_UNINITIALIZED, tmp2, dst_temp, field_klass);\n+  flat_field_copy(IS_DEST_UNINITIALIZED, tmp2, dst_temp, r8);\n+  pop(obj);\n+  jmp(done);\n+\n+  bind(alloc_failed);\n+  pop(obj);\n+  call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::read_flat_field),\n+          obj, entry);\n+  get_vm_result(obj, r15_thread);\n+  bind(done);\n+}\n@@ -1042,0 +1139,4 @@\n+      if (EnableValhalla) {\n+        \/\/ Mask inline_type bit such that we go to the slow path if object is an inline type\n+        andptr(swap_reg, ~((int) markWord::inline_type_bit_in_place));\n+      }\n@@ -1393,1 +1494,1 @@\n-void InterpreterMacroAssembler::profile_not_taken_branch(Register mdp) {\n+void InterpreterMacroAssembler::profile_not_taken_branch(Register mdp, bool acmp) {\n@@ -1405,1 +1506,1 @@\n-    update_mdp_by_constant(mdp, in_bytes(BranchData::branch_data_size()));\n+    update_mdp_by_constant(mdp, acmp ? in_bytes(ACmpData::acmp_data_size()): in_bytes(BranchData::branch_data_size()));\n@@ -1468,1 +1569,1 @@\n-    record_klass_in_profile(receiver, mdp, reg2, true);\n+    record_klass_in_profile(receiver, mdp, reg2);\n@@ -1488,4 +1589,3 @@\n-void InterpreterMacroAssembler::record_klass_in_profile_helper(\n-                                        Register receiver, Register mdp,\n-                                        Register reg2, int start_row,\n-                                        Label& done, bool is_virtual_call) {\n+void InterpreterMacroAssembler::record_klass_in_profile_helper(Register receiver, Register mdp,\n+                                                               Register reg2, int start_row,\n+                                                               Label& done) {\n@@ -1595,3 +1695,1 @@\n-void InterpreterMacroAssembler::record_klass_in_profile(Register receiver,\n-                                                        Register mdp, Register reg2,\n-                                                        bool is_virtual_call) {\n+void InterpreterMacroAssembler::record_klass_in_profile(Register receiver, Register mdp, Register reg2) {\n@@ -1601,1 +1699,1 @@\n-  record_klass_in_profile_helper(receiver, mdp, reg2, 0, done, is_virtual_call);\n+  record_klass_in_profile_helper(receiver, mdp, reg2, 0, done);\n@@ -1678,1 +1776,1 @@\n-      record_klass_in_profile(klass, mdp, reg2, false);\n+      record_klass_in_profile(klass, mdp, reg2);\n@@ -1738,0 +1836,114 @@\n+template <class ArrayData> void InterpreterMacroAssembler::profile_array_type(Register mdp,\n+                                                                              Register array,\n+                                                                              Register tmp) {\n+  if (ProfileInterpreter) {\n+    Label profile_continue;\n+\n+    \/\/ If no method data exists, go to profile_continue.\n+    test_method_data_pointer(mdp, profile_continue);\n+\n+    mov(tmp, array);\n+    profile_obj_type(tmp, Address(mdp, in_bytes(ArrayData::array_offset())));\n+\n+    Label not_flat;\n+    test_non_flat_array_oop(array, tmp, not_flat);\n+\n+    set_mdp_flag_at(mdp, ArrayData::flat_array_byte_constant());\n+\n+    bind(not_flat);\n+\n+    Label not_null_free;\n+    test_non_null_free_array_oop(array, tmp, not_null_free);\n+\n+    set_mdp_flag_at(mdp, ArrayData::null_free_array_byte_constant());\n+\n+    bind(not_null_free);\n+\n+    bind(profile_continue);\n+  }\n+}\n+\n+template void InterpreterMacroAssembler::profile_array_type<ArrayLoadData>(Register mdp,\n+                                                                           Register array,\n+                                                                           Register tmp);\n+template void InterpreterMacroAssembler::profile_array_type<ArrayStoreData>(Register mdp,\n+                                                                            Register array,\n+                                                                            Register tmp);\n+\n+\n+void InterpreterMacroAssembler::profile_multiple_element_types(Register mdp, Register element, Register tmp, const Register tmp2) {\n+  if (ProfileInterpreter) {\n+    Label profile_continue;\n+\n+    \/\/ If no method data exists, go to profile_continue.\n+    test_method_data_pointer(mdp, profile_continue);\n+\n+    Label done, update;\n+    testptr(element, element);\n+    jccb(Assembler::notZero, update);\n+    set_mdp_flag_at(mdp, BitData::null_seen_byte_constant());\n+    jmp(done);\n+\n+    bind(update);\n+    load_klass(tmp, element, rscratch1);\n+\n+    \/\/ Record the object type.\n+    record_klass_in_profile(tmp, mdp, tmp2);\n+\n+    bind(done);\n+\n+    \/\/ The method data pointer needs to be updated.\n+    update_mdp_by_constant(mdp, in_bytes(ArrayStoreData::array_store_data_size()));\n+\n+    bind(profile_continue);\n+  }\n+}\n+\n+void InterpreterMacroAssembler::profile_element_type(Register mdp,\n+                                                     Register element,\n+                                                     Register tmp) {\n+  if (ProfileInterpreter) {\n+    Label profile_continue;\n+\n+    \/\/ If no method data exists, go to profile_continue.\n+    test_method_data_pointer(mdp, profile_continue);\n+\n+    mov(tmp, element);\n+    profile_obj_type(tmp, Address(mdp, in_bytes(ArrayLoadData::element_offset())));\n+\n+    \/\/ The method data pointer needs to be updated.\n+    update_mdp_by_constant(mdp, in_bytes(ArrayLoadData::array_load_data_size()));\n+\n+    bind(profile_continue);\n+  }\n+}\n+\n+void InterpreterMacroAssembler::profile_acmp(Register mdp,\n+                                             Register left,\n+                                             Register right,\n+                                             Register tmp) {\n+  if (ProfileInterpreter) {\n+    Label profile_continue;\n+\n+    \/\/ If no method data exists, go to profile_continue.\n+    test_method_data_pointer(mdp, profile_continue);\n+\n+    mov(tmp, left);\n+    profile_obj_type(tmp, Address(mdp, in_bytes(ACmpData::left_offset())));\n+\n+    Label left_not_inline_type;\n+    test_oop_is_not_inline_type(left, tmp, left_not_inline_type);\n+    set_mdp_flag_at(mdp, ACmpData::left_inline_type_byte_constant());\n+    bind(left_not_inline_type);\n+\n+    mov(tmp, right);\n+    profile_obj_type(tmp, Address(mdp, in_bytes(ACmpData::right_offset())));\n+\n+    Label right_not_inline_type;\n+    test_oop_is_not_inline_type(right, tmp, right_not_inline_type);\n+    set_mdp_flag_at(mdp, ACmpData::right_inline_type_byte_constant());\n+    bind(right_not_inline_type);\n+\n+    bind(profile_continue);\n+  }\n+}\n","filename":"src\/hotspot\/cpu\/x86\/interp_masm_x86.cpp","additions":233,"deletions":21,"binary":false,"changes":254,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -166,1 +166,1 @@\n-  void gen_subtype_check( Register sub_klass, Label &ok_is_subtype );\n+  void gen_subtype_check(Register sub_klass, Label &ok_is_subtype, bool profile = true);\n@@ -206,0 +206,14 @@\n+  \/\/ Kills t1 and t2, preserves klass, return allocation in new_obj\n+  void allocate_instance(Register klass, Register new_obj,\n+                         Register t1, Register t2,\n+                         bool clear_fields, Label& alloc_failed);\n+\n+  \/\/ Allocate instance in \"obj\" and read in the content of the inline field\n+  \/\/ NOTES:\n+  \/\/   - input holder object via \"obj\", which must be rax,\n+  \/\/     will return new instance via the same reg\n+  \/\/   - assumes holder_klass and valueKlass field klass have both been resolved\n+  void read_flat_field(Register entry,\n+                       Register tmp1, Register tmp2,\n+                       Register obj = rax);\n+\n@@ -228,5 +242,2 @@\n-  void record_klass_in_profile(Register receiver, Register mdp,\n-                               Register reg2, bool is_virtual_call);\n-  void record_klass_in_profile_helper(Register receiver, Register mdp,\n-                                      Register reg2, int start_row,\n-                                      Label& done, bool is_virtual_call);\n+  void record_klass_in_profile(Register receiver, Register mdp, Register reg2);\n+  void record_klass_in_profile_helper(Register receiver, Register mdp, Register reg2, int start_row, Label &done);\n@@ -244,1 +255,1 @@\n-  void profile_not_taken_branch(Register mdp);\n+  void profile_not_taken_branch(Register mdp, bool acmp = false);\n@@ -257,0 +268,5 @@\n+  template <class ArrayData> void profile_array_type(Register mdp, Register array, Register tmp);\n+\n+  void profile_multiple_element_types(Register mdp, Register element, Register tmp, const Register tmp2);\n+  void profile_element_type(Register mdp, Register element, Register tmp);\n+  void profile_acmp(Register mdp, Register left, Register right, Register tmp);\n","filename":"src\/hotspot\/cpu\/x86\/interp_masm_x86.hpp","additions":24,"deletions":8,"binary":false,"changes":32,"status":"modified"},{"patch":"@@ -30,0 +30,1 @@\n+#include \"ci\/ciInlineKlass.hpp\"\n@@ -45,0 +46,1 @@\n+#include \"oops\/resolvedFieldEntry.hpp\"\n@@ -55,0 +57,1 @@\n+#include \"runtime\/signature_cc.hpp\"\n@@ -58,0 +61,4 @@\n+#include \"vmreg_x86.inline.hpp\"\n+#ifdef COMPILER2\n+#include \"opto\/output.hpp\"\n+#endif\n@@ -1739,0 +1746,4 @@\n+void MacroAssembler::super_call_VM_leaf(address entry_point) {\n+  MacroAssembler::call_VM_leaf_base(entry_point, 1);\n+}\n+\n@@ -3016,0 +3027,111 @@\n+void MacroAssembler::test_markword_is_inline_type(Register markword, Label& is_inline_type) {\n+  andptr(markword, markWord::inline_type_mask_in_place);\n+  cmpptr(markword, markWord::inline_type_pattern);\n+  jcc(Assembler::equal, is_inline_type);\n+}\n+\n+void MacroAssembler::test_klass_is_inline_type(Register klass, Register temp_reg, Label& is_inline_type) {\n+  load_unsigned_short(temp_reg, Address(klass, Klass::access_flags_offset()));\n+  testl(temp_reg, JVM_ACC_IDENTITY);\n+  jcc(Assembler::zero, is_inline_type);\n+}\n+\n+void MacroAssembler::test_oop_is_not_inline_type(Register object, Register tmp, Label& not_inline_type) {\n+  testptr(object, object);\n+  jcc(Assembler::zero, not_inline_type);\n+  const int is_inline_type_mask = markWord::inline_type_pattern;\n+  movptr(tmp, Address(object, oopDesc::mark_offset_in_bytes()));\n+  andptr(tmp, is_inline_type_mask);\n+  cmpptr(tmp, is_inline_type_mask);\n+  jcc(Assembler::notEqual, not_inline_type);\n+}\n+\n+void MacroAssembler::test_field_is_null_free_inline_type(Register flags, Register temp_reg, Label& is_null_free_inline_type) {\n+  movl(temp_reg, flags);\n+  testl(temp_reg, 1 << ResolvedFieldEntry::is_null_free_inline_type_shift);\n+  jcc(Assembler::notEqual, is_null_free_inline_type);\n+}\n+\n+void MacroAssembler::test_field_is_not_null_free_inline_type(Register flags, Register temp_reg, Label& not_null_free_inline_type) {\n+  movl(temp_reg, flags);\n+  testl(temp_reg, 1 << ResolvedFieldEntry::is_null_free_inline_type_shift);\n+  jcc(Assembler::equal, not_null_free_inline_type);\n+}\n+\n+void MacroAssembler::test_field_is_flat(Register flags, Register temp_reg, Label& is_flat) {\n+  movl(temp_reg, flags);\n+  testl(temp_reg, 1 << ResolvedFieldEntry::is_flat_shift);\n+  jcc(Assembler::notEqual, is_flat);\n+}\n+\n+void MacroAssembler::test_field_has_null_marker(Register flags, Register temp_reg, Label& has_null_marker) {\n+  movl(temp_reg, flags);\n+  testl(temp_reg, 1 << ResolvedFieldEntry::has_null_marker_shift);\n+  jcc(Assembler::notEqual, has_null_marker);\n+}\n+\n+void MacroAssembler::test_oop_prototype_bit(Register oop, Register temp_reg, int32_t test_bit, bool jmp_set, Label& jmp_label) {\n+  Label test_mark_word;\n+  \/\/ load mark word\n+  movptr(temp_reg, Address(oop, oopDesc::mark_offset_in_bytes()));\n+  \/\/ check displaced\n+  testl(temp_reg, markWord::unlocked_value);\n+  jccb(Assembler::notZero, test_mark_word);\n+  \/\/ slow path use klass prototype\n+  push(rscratch1);\n+  load_prototype_header(temp_reg, oop, rscratch1);\n+  pop(rscratch1);\n+\n+  bind(test_mark_word);\n+  testl(temp_reg, test_bit);\n+  jcc((jmp_set) ? Assembler::notZero : Assembler::zero, jmp_label);\n+}\n+\n+void MacroAssembler::test_flat_array_oop(Register oop, Register temp_reg,\n+                                         Label& is_flat_array) {\n+#ifdef _LP64\n+  test_oop_prototype_bit(oop, temp_reg, markWord::flat_array_bit_in_place, true, is_flat_array);\n+#else\n+  load_klass(temp_reg, oop, noreg);\n+  movl(temp_reg, Address(temp_reg, Klass::layout_helper_offset()));\n+  test_flat_array_layout(temp_reg, is_flat_array);\n+#endif\n+}\n+\n+void MacroAssembler::test_non_flat_array_oop(Register oop, Register temp_reg,\n+                                             Label& is_non_flat_array) {\n+#ifdef _LP64\n+  test_oop_prototype_bit(oop, temp_reg, markWord::flat_array_bit_in_place, false, is_non_flat_array);\n+#else\n+  load_klass(temp_reg, oop, noreg);\n+  movl(temp_reg, Address(temp_reg, Klass::layout_helper_offset()));\n+  test_non_flat_array_layout(temp_reg, is_non_flat_array);\n+#endif\n+}\n+\n+void MacroAssembler::test_null_free_array_oop(Register oop, Register temp_reg, Label&is_null_free_array) {\n+#ifdef _LP64\n+  test_oop_prototype_bit(oop, temp_reg, markWord::null_free_array_bit_in_place, true, is_null_free_array);\n+#else\n+  Unimplemented();\n+#endif\n+}\n+\n+void MacroAssembler::test_non_null_free_array_oop(Register oop, Register temp_reg, Label&is_non_null_free_array) {\n+#ifdef _LP64\n+  test_oop_prototype_bit(oop, temp_reg, markWord::null_free_array_bit_in_place, false, is_non_null_free_array);\n+#else\n+  Unimplemented();\n+#endif\n+}\n+\n+void MacroAssembler::test_flat_array_layout(Register lh, Label& is_flat_array) {\n+  testl(lh, Klass::_lh_array_tag_flat_value_bit_inplace);\n+  jcc(Assembler::notZero, is_flat_array);\n+}\n+\n+void MacroAssembler::test_non_flat_array_layout(Register lh, Label& is_non_flat_array) {\n+  testl(lh, Klass::_lh_array_tag_flat_value_bit_inplace);\n+  jcc(Assembler::zero, is_non_flat_array);\n+}\n+\n@@ -4203,0 +4325,120 @@\n+\/\/ Object \/ value buffer allocation...\n+\/\/\n+\/\/ Kills klass and rsi on LP64\n+void MacroAssembler::allocate_instance(Register klass, Register new_obj,\n+                                       Register t1, Register t2,\n+                                       bool clear_fields, Label& alloc_failed)\n+{\n+  Label done, initialize_header, initialize_object, slow_case, slow_case_no_pop;\n+  Register layout_size = t1;\n+  assert(new_obj == rax, \"needs to be rax\");\n+  assert_different_registers(klass, new_obj, t1, t2);\n+\n+  \/\/ get instance_size in InstanceKlass (scaled to a count of bytes)\n+  movl(layout_size, Address(klass, Klass::layout_helper_offset()));\n+  \/\/ test to see if it is malformed in some way\n+  testl(layout_size, Klass::_lh_instance_slow_path_bit);\n+  jcc(Assembler::notZero, slow_case_no_pop);\n+\n+  \/\/ Allocate the instance:\n+  \/\/  If TLAB is enabled:\n+  \/\/    Try to allocate in the TLAB.\n+  \/\/    If fails, go to the slow path.\n+  \/\/  Else If inline contiguous allocations are enabled:\n+  \/\/    Try to allocate in eden.\n+  \/\/    If fails due to heap end, go to slow path.\n+  \/\/\n+  \/\/  If TLAB is enabled OR inline contiguous is enabled:\n+  \/\/    Initialize the allocation.\n+  \/\/    Exit.\n+  \/\/\n+  \/\/  Go to slow path.\n+\n+  push(klass);\n+  const Register thread = r15_thread;\n+\n+  if (UseTLAB) {\n+    tlab_allocate(thread, new_obj, layout_size, 0, klass, t2, slow_case);\n+    if (ZeroTLAB || (!clear_fields)) {\n+      \/\/ the fields have been already cleared\n+      jmp(initialize_header);\n+    } else {\n+      \/\/ initialize both the header and fields\n+      jmp(initialize_object);\n+    }\n+  } else {\n+    jmp(slow_case);\n+  }\n+\n+  \/\/ If UseTLAB is true, the object is created above and there is an initialize need.\n+  \/\/ Otherwise, skip and go to the slow path.\n+  if (UseTLAB) {\n+    if (clear_fields) {\n+      \/\/ The object is initialized before the header.  If the object size is\n+      \/\/ zero, go directly to the header initialization.\n+      bind(initialize_object);\n+      if (UseCompactObjectHeaders) {\n+        assert(is_aligned(oopDesc::base_offset_in_bytes(), BytesPerLong), \"oop base offset must be 8-byte-aligned\");\n+        decrement(layout_size, oopDesc::base_offset_in_bytes());\n+      } else {\n+        decrement(layout_size, sizeof(oopDesc));\n+      }\n+      jcc(Assembler::zero, initialize_header);\n+\n+      \/\/ Initialize topmost object field, divide size by 8, check if odd and\n+      \/\/ test if zero.\n+      Register zero = klass;\n+      xorl(zero, zero);    \/\/ use zero reg to clear memory (shorter code)\n+      shrl(layout_size, LogBytesPerLong); \/\/ divide by 2*oopSize and set carry flag if odd\n+\n+  #ifdef ASSERT\n+      \/\/ make sure instance_size was multiple of 8\n+      Label L;\n+      \/\/ Ignore partial flag stall after shrl() since it is debug VM\n+      jcc(Assembler::carryClear, L);\n+      stop(\"object size is not multiple of 2 - adjust this code\");\n+      bind(L);\n+      \/\/ must be > 0, no extra check needed here\n+  #endif\n+\n+      \/\/ initialize remaining object fields: instance_size was a multiple of 8\n+      {\n+        Label loop;\n+        bind(loop);\n+        int header_size_bytes = oopDesc::header_size() * HeapWordSize;\n+        assert(is_aligned(header_size_bytes, BytesPerLong), \"oop header size must be 8-byte-aligned\");\n+        movptr(Address(new_obj, layout_size, Address::times_8, header_size_bytes - 1*oopSize), zero);\n+        decrement(layout_size);\n+        jcc(Assembler::notZero, loop);\n+      }\n+    } \/\/ clear_fields\n+\n+    \/\/ initialize object header only.\n+    bind(initialize_header);\n+    if (UseCompactObjectHeaders || EnableValhalla) {\n+      pop(klass);\n+      Register mark_word = t2;\n+      movptr(mark_word, Address(klass, Klass::prototype_header_offset()));\n+      movptr(Address(new_obj, oopDesc::mark_offset_in_bytes ()), mark_word);\n+    } else {\n+     movptr(Address(new_obj, oopDesc::mark_offset_in_bytes()),\n+            (intptr_t)markWord::prototype().value()); \/\/ header\n+     pop(klass);   \/\/ get saved klass back in the register.\n+    }\n+    if (!UseCompactObjectHeaders) {\n+      xorl(rsi, rsi);                 \/\/ use zero reg to clear memory (shorter code)\n+      store_klass_gap(new_obj, rsi);  \/\/ zero klass gap for compressed oops\n+      movptr(t2, klass);         \/\/ preserve klass\n+      store_klass(new_obj, t2, rscratch1);  \/\/ src klass reg is potentially compressed\n+    }\n+    jmp(done);\n+  }\n+\n+  bind(slow_case);\n+  pop(klass);\n+  bind(slow_case_no_pop);\n+  jmp(alloc_failed);\n+\n+  bind(done);\n+}\n+\n@@ -4461,0 +4703,27 @@\n+void MacroAssembler::get_inline_type_field_klass(Register holder_klass, Register index, Register inline_klass) {\n+  inline_layout_info(holder_klass, index, inline_klass);\n+  movptr(inline_klass, Address(inline_klass, InlineLayoutInfo::klass_offset()));\n+}\n+\n+void MacroAssembler::inline_layout_info(Register holder_klass, Register index, Register layout_info) {\n+  movptr(layout_info, Address(holder_klass, InstanceKlass::inline_layout_info_array_offset()));\n+#ifdef ASSERT\n+  {\n+    Label done;\n+    cmpptr(layout_info, 0);\n+    jcc(Assembler::notEqual, done);\n+    stop(\"inline_layout_info_array is null\");\n+    bind(done);\n+  }\n+#endif\n+\n+  InlineLayoutInfo array[2];\n+  int size = (char*)&array[1] - (char*)&array[0]; \/\/ computing size of array elements\n+  if (is_power_of_2(size)) {\n+    shll(index, log2i_exact(size)); \/\/ Scale index by power of 2\n+  } else {\n+    imull(index, index, size); \/\/ Scale the index to be the entry index * array_element_size\n+  }\n+  lea(layout_info, Address(layout_info, index, Address::times_1, Array<InlineLayoutInfo>::base_offset_in_bytes()));\n+}\n+\n@@ -5530,1 +5799,5 @@\n-  if (!VerifyOops) return;\n+  if (!VerifyOops || VerifyAdapterSharing) {\n+    \/\/ Below address of the code string confuses VerifyAdapterSharing\n+    \/\/ because it may differ between otherwise equivalent adapters.\n+    return;\n+  }\n@@ -5591,1 +5864,5 @@\n-  if (!VerifyOops) return;\n+  if (!VerifyOops || VerifyAdapterSharing) {\n+    \/\/ Below address of the code string confuses VerifyAdapterSharing\n+    \/\/ because it may differ between otherwise equivalent adapters.\n+    return;\n+  }\n@@ -6078,0 +6355,13 @@\n+void MacroAssembler::load_metadata(Register dst, Register src) {\n+#ifdef _LP64\n+  if (UseCompactObjectHeaders) {\n+    load_narrow_klass_compact(dst, src);\n+  } else if (UseCompressedClassPointers) {\n+    movl(dst, Address(src, oopDesc::klass_offset_in_bytes()));\n+  } else\n+#endif\n+  {\n+    movptr(dst, Address(src, oopDesc::klass_offset_in_bytes()));\n+  }\n+}\n+\n@@ -6103,0 +6393,5 @@\n+void MacroAssembler::load_prototype_header(Register dst, Register src, Register tmp) {\n+  load_klass(dst, src, tmp);\n+  movptr(dst, Address(dst, Klass::prototype_header_offset()));\n+}\n+\n@@ -6175,0 +6470,40 @@\n+void MacroAssembler::flat_field_copy(DecoratorSet decorators, Register src, Register dst,\n+                                     Register inline_layout_info) {\n+  BarrierSetAssembler* bs = BarrierSet::barrier_set()->barrier_set_assembler();\n+  bs->flat_field_copy(this, decorators, src, dst, inline_layout_info);\n+}\n+\n+void MacroAssembler::payload_offset(Register inline_klass, Register offset) {\n+  movptr(offset, Address(inline_klass, InstanceKlass::adr_inlineklass_fixed_block_offset()));\n+  movl(offset, Address(offset, InlineKlass::payload_offset_offset()));\n+}\n+\n+void MacroAssembler::payload_addr(Register oop, Register data, Register inline_klass) {\n+  \/\/ ((address) (void*) o) + vk->payload_offset();\n+  Register offset = (data == oop) ? rscratch1 : data;\n+  payload_offset(inline_klass, offset);\n+  if (data == oop) {\n+    addptr(data, offset);\n+  } else {\n+    lea(data, Address(oop, offset));\n+  }\n+}\n+\n+void MacroAssembler::data_for_value_array_index(Register array, Register array_klass,\n+                                                Register index, Register data) {\n+  assert(index != rcx, \"index needs to shift by rcx\");\n+  assert_different_registers(array, array_klass, index);\n+  assert_different_registers(rcx, array, index);\n+\n+  \/\/ array->base() + (index << Klass::layout_helper_log2_element_size(lh));\n+  movl(rcx, Address(array_klass, Klass::layout_helper_offset()));\n+\n+  \/\/ Klass::layout_helper_log2_element_size(lh)\n+  \/\/ (lh >> _lh_log2_element_size_shift) & _lh_log2_element_size_mask;\n+  shrl(rcx, Klass::_lh_log2_element_size_shift);\n+  andl(rcx, Klass::_lh_log2_element_size_mask);\n+  shlptr(index); \/\/ index << rcx\n+\n+  lea(data, Address(array, index, Address::times_1, arrayOopDesc::base_offset_in_bytes(T_FLAT_ELEMENT)));\n+}\n+\n@@ -6523,1 +6858,1 @@\n-void MacroAssembler::xmm_clear_mem(Register base, Register cnt, Register rtmp, XMMRegister xtmp, KRegister mask) {\n+void MacroAssembler::xmm_clear_mem(Register base, Register cnt, Register val, XMMRegister xtmp, KRegister mask) {\n@@ -6529,1 +6864,1 @@\n-    vpxor(xtmp, xtmp, xtmp, AVX_512bit);\n+    evpbroadcastq(xtmp, val, AVX_512bit);\n@@ -6531,1 +6866,3 @@\n-    vpxor(xtmp, xtmp, xtmp, AVX_256bit);\n+    movdq(xtmp, val);\n+    punpcklqdq(xtmp, xtmp);\n+    vinserti128_high(xtmp, xtmp);\n@@ -6533,1 +6870,2 @@\n-    pxor(xtmp, xtmp);\n+    movdq(xtmp, val);\n+    punpcklqdq(xtmp, xtmp);\n@@ -6556,1 +6894,1 @@\n-    fill64_masked(3, base, 0, xtmp, mask, cnt, rtmp, true);\n+    fill64_masked(3, base, 0, xtmp, mask, cnt, val, true);\n@@ -6575,1 +6913,1 @@\n-    fill32_masked(3, base, 0, xtmp, mask, cnt, rtmp);\n+    fill32_masked(3, base, 0, xtmp, mask, cnt, val);\n@@ -6588,0 +6926,406 @@\n+int MacroAssembler::store_inline_type_fields_to_buf(ciInlineKlass* vk, bool from_interpreter) {\n+  assert(InlineTypeReturnedAsFields, \"Inline types should never be returned as fields\");\n+  \/\/ An inline type might be returned. If fields are in registers we\n+  \/\/ need to allocate an inline type instance and initialize it with\n+  \/\/ the value of the fields.\n+  Label skip;\n+  \/\/ We only need a new buffered inline type if a new one is not returned\n+  testptr(rax, 1);\n+  jcc(Assembler::zero, skip);\n+  int call_offset = -1;\n+\n+#ifdef _LP64\n+  \/\/ The following code is similar to allocate_instance but has some slight differences,\n+  \/\/ e.g. object size is always not zero, sometimes it's constant; storing klass ptr after\n+  \/\/ allocating is not necessary if vk != nullptr, etc. allocate_instance is not aware of these.\n+  Label slow_case;\n+  \/\/ 1. Try to allocate a new buffered inline instance either from TLAB or eden space\n+  mov(rscratch1, rax); \/\/ save rax for slow_case since *_allocate may corrupt it when allocation failed\n+  if (vk != nullptr) {\n+    \/\/ Called from C1, where the return type is statically known.\n+    movptr(rbx, (intptr_t)vk->get_InlineKlass());\n+    jint lh = vk->layout_helper();\n+    assert(lh != Klass::_lh_neutral_value, \"inline class in return type must have been resolved\");\n+    if (UseTLAB && !Klass::layout_helper_needs_slow_path(lh)) {\n+      tlab_allocate(r15_thread, rax, noreg, lh, r13, r14, slow_case);\n+    } else {\n+      jmp(slow_case);\n+    }\n+  } else {\n+    \/\/ Call from interpreter. RAX contains ((the InlineKlass* of the return type) | 0x01)\n+    mov(rbx, rax);\n+    andptr(rbx, -2);\n+    if (UseTLAB) {\n+      movl(r14, Address(rbx, Klass::layout_helper_offset()));\n+      testl(r14, Klass::_lh_instance_slow_path_bit);\n+      jcc(Assembler::notZero, slow_case);\n+      tlab_allocate(r15_thread, rax, r14, 0, r13, r14, slow_case);\n+    } else {\n+      jmp(slow_case);\n+    }\n+  }\n+  if (UseTLAB) {\n+    \/\/ 2. Initialize buffered inline instance header\n+    Register buffer_obj = rax;\n+    if (UseCompactObjectHeaders) {\n+      Register mark_word = r13;\n+      movptr(mark_word, Address(rbx, Klass::prototype_header_offset()));\n+      movptr(Address(buffer_obj, oopDesc::mark_offset_in_bytes ()), mark_word);\n+    } else {\n+      movptr(Address(buffer_obj, oopDesc::mark_offset_in_bytes()), (intptr_t)markWord::inline_type_prototype().value());\n+      xorl(r13, r13);\n+      store_klass_gap(buffer_obj, r13);\n+      if (vk == nullptr) {\n+        \/\/ store_klass corrupts rbx(klass), so save it in r13 for later use (interpreter case only).\n+        mov(r13, rbx);\n+      }\n+      store_klass(buffer_obj, rbx, rscratch1);\n+    }\n+    \/\/ 3. Initialize its fields with an inline class specific handler\n+    if (vk != nullptr) {\n+      call(RuntimeAddress(vk->pack_handler())); \/\/ no need for call info as this will not safepoint.\n+    } else {\n+      movptr(rbx, Address(r13, InstanceKlass::adr_inlineklass_fixed_block_offset()));\n+      movptr(rbx, Address(rbx, InlineKlass::pack_handler_offset()));\n+      call(rbx);\n+    }\n+    jmp(skip);\n+  }\n+  bind(slow_case);\n+  \/\/ We failed to allocate a new inline type, fall back to a runtime\n+  \/\/ call. Some oop field may be live in some registers but we can't\n+  \/\/ tell. That runtime call will take care of preserving them\n+  \/\/ across a GC if there's one.\n+  mov(rax, rscratch1);\n+#endif\n+\n+  if (from_interpreter) {\n+    super_call_VM_leaf(StubRoutines::store_inline_type_fields_to_buf());\n+  } else {\n+    call(RuntimeAddress(StubRoutines::store_inline_type_fields_to_buf()));\n+    call_offset = offset();\n+  }\n+\n+  bind(skip);\n+  return call_offset;\n+}\n+\n+\/\/ Move a value between registers\/stack slots and update the reg_state\n+bool MacroAssembler::move_helper(VMReg from, VMReg to, BasicType bt, RegState reg_state[]) {\n+  assert(from->is_valid() && to->is_valid(), \"source and destination must be valid\");\n+  if (reg_state[to->value()] == reg_written) {\n+    return true; \/\/ Already written\n+  }\n+  if (from != to && bt != T_VOID) {\n+    if (reg_state[to->value()] == reg_readonly) {\n+      return false; \/\/ Not yet writable\n+    }\n+    if (from->is_reg()) {\n+      if (to->is_reg()) {\n+        if (from->is_XMMRegister()) {\n+          if (bt == T_DOUBLE) {\n+            movdbl(to->as_XMMRegister(), from->as_XMMRegister());\n+          } else {\n+            assert(bt == T_FLOAT, \"must be float\");\n+            movflt(to->as_XMMRegister(), from->as_XMMRegister());\n+          }\n+        } else {\n+          movq(to->as_Register(), from->as_Register());\n+        }\n+      } else {\n+        int st_off = to->reg2stack() * VMRegImpl::stack_slot_size + wordSize;\n+        Address to_addr = Address(rsp, st_off);\n+        if (from->is_XMMRegister()) {\n+          if (bt == T_DOUBLE) {\n+            movdbl(to_addr, from->as_XMMRegister());\n+          } else {\n+            assert(bt == T_FLOAT, \"must be float\");\n+            movflt(to_addr, from->as_XMMRegister());\n+          }\n+        } else {\n+          movq(to_addr, from->as_Register());\n+        }\n+      }\n+    } else {\n+      Address from_addr = Address(rsp, from->reg2stack() * VMRegImpl::stack_slot_size + wordSize);\n+      if (to->is_reg()) {\n+        if (to->is_XMMRegister()) {\n+          if (bt == T_DOUBLE) {\n+            movdbl(to->as_XMMRegister(), from_addr);\n+          } else {\n+            assert(bt == T_FLOAT, \"must be float\");\n+            movflt(to->as_XMMRegister(), from_addr);\n+          }\n+        } else {\n+          movq(to->as_Register(), from_addr);\n+        }\n+      } else {\n+        int st_off = to->reg2stack() * VMRegImpl::stack_slot_size + wordSize;\n+        movq(r13, from_addr);\n+        movq(Address(rsp, st_off), r13);\n+      }\n+    }\n+  }\n+  \/\/ Update register states\n+  reg_state[from->value()] = reg_writable;\n+  reg_state[to->value()] = reg_written;\n+  return true;\n+}\n+\n+\/\/ Calculate the extra stack space required for packing or unpacking inline\n+\/\/ args and adjust the stack pointer\n+int MacroAssembler::extend_stack_for_inline_args(int args_on_stack) {\n+  \/\/ Two additional slots to account for return address\n+  int sp_inc = (args_on_stack + 2) * VMRegImpl::stack_slot_size;\n+  sp_inc = align_up(sp_inc, StackAlignmentInBytes);\n+  \/\/ Save the return address, adjust the stack (make sure it is properly\n+  \/\/ 16-byte aligned) and copy the return address to the new top of the stack.\n+  \/\/ The stack will be repaired on return (see MacroAssembler::remove_frame).\n+  assert(sp_inc > 0, \"sanity\");\n+  pop(r13);\n+  subptr(rsp, sp_inc);\n+  push(r13);\n+  return sp_inc;\n+}\n+\n+\/\/ Read all fields from an inline type buffer and store the field values in registers\/stack slots.\n+bool MacroAssembler::unpack_inline_helper(const GrowableArray<SigEntry>* sig, int& sig_index,\n+                                          VMReg from, int& from_index, VMRegPair* to, int to_count, int& to_index,\n+                                          RegState reg_state[]) {\n+  assert(sig->at(sig_index)._bt == T_VOID, \"should be at end delimiter\");\n+  assert(from->is_valid(), \"source must be valid\");\n+  bool progress = false;\n+#ifdef ASSERT\n+  const int start_offset = offset();\n+#endif\n+\n+  Label L_null, L_notNull;\n+  \/\/ Don't use r14 as tmp because it's used for spilling (see MacroAssembler::spill_reg_for)\n+  Register tmp1 = r10;\n+  Register tmp2 = r13;\n+  Register fromReg = noreg;\n+  ScalarizedInlineArgsStream stream(sig, sig_index, to, to_count, to_index, -1);\n+  bool done = true;\n+  bool mark_done = true;\n+  VMReg toReg;\n+  BasicType bt;\n+  \/\/ Check if argument requires a null check\n+  bool null_check = false;\n+  VMReg nullCheckReg;\n+  while (stream.next(nullCheckReg, bt)) {\n+    if (sig->at(stream.sig_index())._offset == -1) {\n+      null_check = true;\n+      break;\n+    }\n+  }\n+  stream.reset(sig_index, to_index);\n+  while (stream.next(toReg, bt)) {\n+    assert(toReg->is_valid(), \"destination must be valid\");\n+    int idx = (int)toReg->value();\n+    if (reg_state[idx] == reg_readonly) {\n+      if (idx != from->value()) {\n+        mark_done = false;\n+      }\n+      done = false;\n+      continue;\n+    } else if (reg_state[idx] == reg_written) {\n+      continue;\n+    }\n+    assert(reg_state[idx] == reg_writable, \"must be writable\");\n+    reg_state[idx] = reg_written;\n+    progress = true;\n+\n+    if (fromReg == noreg) {\n+      if (from->is_reg()) {\n+        fromReg = from->as_Register();\n+      } else {\n+        int st_off = from->reg2stack() * VMRegImpl::stack_slot_size + wordSize;\n+        movq(tmp1, Address(rsp, st_off));\n+        fromReg = tmp1;\n+      }\n+      if (null_check) {\n+        \/\/ Nullable inline type argument, emit null check\n+        testptr(fromReg, fromReg);\n+        jcc(Assembler::zero, L_null);\n+      }\n+    }\n+    int off = sig->at(stream.sig_index())._offset;\n+    if (off == -1) {\n+      assert(null_check, \"Missing null check at\");\n+      if (toReg->is_stack()) {\n+        int st_off = toReg->reg2stack() * VMRegImpl::stack_slot_size + wordSize;\n+        movq(Address(rsp, st_off), 1);\n+      } else {\n+        movq(toReg->as_Register(), 1);\n+      }\n+      continue;\n+    }\n+    assert(off > 0, \"offset in object should be positive\");\n+    Address fromAddr = Address(fromReg, off);\n+    if (!toReg->is_XMMRegister()) {\n+      Register dst = toReg->is_stack() ? tmp2 : toReg->as_Register();\n+      if (is_reference_type(bt)) {\n+        load_heap_oop(dst, fromAddr);\n+      } else {\n+        bool is_signed = (bt != T_CHAR) && (bt != T_BOOLEAN);\n+        load_sized_value(dst, fromAddr, type2aelembytes(bt), is_signed);\n+      }\n+      if (toReg->is_stack()) {\n+        int st_off = toReg->reg2stack() * VMRegImpl::stack_slot_size + wordSize;\n+        movq(Address(rsp, st_off), dst);\n+      }\n+    } else if (bt == T_DOUBLE) {\n+      movdbl(toReg->as_XMMRegister(), fromAddr);\n+    } else {\n+      assert(bt == T_FLOAT, \"must be float\");\n+      movflt(toReg->as_XMMRegister(), fromAddr);\n+    }\n+  }\n+  if (progress && null_check) {\n+    if (done) {\n+      jmp(L_notNull);\n+      bind(L_null);\n+      \/\/ Set IsInit field to zero to signal that the argument is null.\n+      \/\/ Also set all oop fields to zero to make the GC happy.\n+      stream.reset(sig_index, to_index);\n+      while (stream.next(toReg, bt)) {\n+        if (sig->at(stream.sig_index())._offset == -1 ||\n+            bt == T_OBJECT || bt == T_ARRAY) {\n+          if (toReg->is_stack()) {\n+            int st_off = toReg->reg2stack() * VMRegImpl::stack_slot_size + wordSize;\n+            movq(Address(rsp, st_off), 0);\n+          } else {\n+            xorq(toReg->as_Register(), toReg->as_Register());\n+          }\n+        }\n+      }\n+      bind(L_notNull);\n+    } else {\n+      bind(L_null);\n+    }\n+  }\n+\n+  sig_index = stream.sig_index();\n+  to_index = stream.regs_index();\n+\n+  if (mark_done && reg_state[from->value()] != reg_written) {\n+    \/\/ This is okay because no one else will write to that slot\n+    reg_state[from->value()] = reg_writable;\n+  }\n+  from_index--;\n+  assert(progress || (start_offset == offset()), \"should not emit code\");\n+  return done;\n+}\n+\n+bool MacroAssembler::pack_inline_helper(const GrowableArray<SigEntry>* sig, int& sig_index, int vtarg_index,\n+                                        VMRegPair* from, int from_count, int& from_index, VMReg to,\n+                                        RegState reg_state[], Register val_array) {\n+  assert(sig->at(sig_index)._bt == T_METADATA, \"should be at delimiter\");\n+  assert(to->is_valid(), \"destination must be valid\");\n+\n+  if (reg_state[to->value()] == reg_written) {\n+    skip_unpacked_fields(sig, sig_index, from, from_count, from_index);\n+    return true; \/\/ Already written\n+  }\n+\n+  \/\/ TODO 8284443 Isn't it an issue if below code uses r14 as tmp when it contains a spilled value?\n+  \/\/ Be careful with r14 because it's used for spilling (see MacroAssembler::spill_reg_for).\n+  Register val_obj_tmp = r11;\n+  Register from_reg_tmp = r14;\n+  Register tmp1 = r10;\n+  Register tmp2 = r13;\n+  Register tmp3 = rbx;\n+  Register val_obj = to->is_stack() ? val_obj_tmp : to->as_Register();\n+\n+  assert_different_registers(val_obj_tmp, from_reg_tmp, tmp1, tmp2, tmp3, val_array);\n+\n+  if (reg_state[to->value()] == reg_readonly) {\n+    if (!is_reg_in_unpacked_fields(sig, sig_index, to, from, from_count, from_index)) {\n+      skip_unpacked_fields(sig, sig_index, from, from_count, from_index);\n+      return false; \/\/ Not yet writable\n+    }\n+    val_obj = val_obj_tmp;\n+  }\n+\n+  int index = arrayOopDesc::base_offset_in_bytes(T_OBJECT) + vtarg_index * type2aelembytes(T_OBJECT);\n+  load_heap_oop(val_obj, Address(val_array, index));\n+\n+  ScalarizedInlineArgsStream stream(sig, sig_index, from, from_count, from_index);\n+  VMReg fromReg;\n+  BasicType bt;\n+  Label L_null;\n+  while (stream.next(fromReg, bt)) {\n+    assert(fromReg->is_valid(), \"source must be valid\");\n+    reg_state[fromReg->value()] = reg_writable;\n+\n+    int off = sig->at(stream.sig_index())._offset;\n+    if (off == -1) {\n+      \/\/ Nullable inline type argument, emit null check\n+      Label L_notNull;\n+      if (fromReg->is_stack()) {\n+        int ld_off = fromReg->reg2stack() * VMRegImpl::stack_slot_size + wordSize;\n+        testb(Address(rsp, ld_off), 1);\n+      } else {\n+        testb(fromReg->as_Register(), 1);\n+      }\n+      jcc(Assembler::notZero, L_notNull);\n+      movptr(val_obj, 0);\n+      jmp(L_null);\n+      bind(L_notNull);\n+      continue;\n+    }\n+\n+    assert(off > 0, \"offset in object should be positive\");\n+    size_t size_in_bytes = is_java_primitive(bt) ? type2aelembytes(bt) : wordSize;\n+\n+    Address dst(val_obj, off);\n+    if (!fromReg->is_XMMRegister()) {\n+      Register src;\n+      if (fromReg->is_stack()) {\n+        src = from_reg_tmp;\n+        int ld_off = fromReg->reg2stack() * VMRegImpl::stack_slot_size + wordSize;\n+        load_sized_value(src, Address(rsp, ld_off), size_in_bytes, \/* is_signed *\/ false);\n+      } else {\n+        src = fromReg->as_Register();\n+      }\n+      assert_different_registers(dst.base(), src, tmp1, tmp2, tmp3, val_array);\n+      if (is_reference_type(bt)) {\n+        store_heap_oop(dst, src, tmp1, tmp2, tmp3, IN_HEAP | ACCESS_WRITE | IS_DEST_UNINITIALIZED);\n+      } else {\n+        store_sized_value(dst, src, size_in_bytes);\n+      }\n+    } else if (bt == T_DOUBLE) {\n+      movdbl(dst, fromReg->as_XMMRegister());\n+    } else {\n+      assert(bt == T_FLOAT, \"must be float\");\n+      movflt(dst, fromReg->as_XMMRegister());\n+    }\n+  }\n+  bind(L_null);\n+  sig_index = stream.sig_index();\n+  from_index = stream.regs_index();\n+\n+  assert(reg_state[to->value()] == reg_writable, \"must have already been read\");\n+  bool success = move_helper(val_obj->as_VMReg(), to, T_OBJECT, reg_state);\n+  assert(success, \"to register must be writeable\");\n+  return true;\n+}\n+\n+VMReg MacroAssembler::spill_reg_for(VMReg reg) {\n+  return reg->is_XMMRegister() ? xmm8->as_VMReg() : r14->as_VMReg();\n+}\n+\n+void MacroAssembler::remove_frame(int initial_framesize, bool needs_stack_repair) {\n+  assert((initial_framesize & (StackAlignmentInBytes-1)) == 0, \"frame size not aligned\");\n+  if (needs_stack_repair) {\n+    movq(rbp, Address(rsp, initial_framesize));\n+    \/\/ The stack increment resides just below the saved rbp\n+    addq(rsp, Address(rsp, initial_framesize - wordSize));\n+  } else {\n+    if (initial_framesize > 0) {\n+      addq(rsp, initial_framesize);\n+    }\n+    pop(rbp);\n+  }\n+}\n+\n@@ -6677,2 +7421,2 @@\n-void MacroAssembler::clear_mem(Register base, Register cnt, Register tmp, XMMRegister xtmp,\n-                               bool is_large, KRegister mask) {\n+void MacroAssembler::clear_mem(Register base, Register cnt, Register val, XMMRegister xtmp,\n+                               bool is_large, bool word_copy_only, KRegister mask) {\n@@ -6683,1 +7427,1 @@\n-  assert(tmp==rax,   \"tmp register must be eax for rep stos\");\n+  assert(val==rax,   \"val register must be eax for rep stos\");\n@@ -6689,3 +7433,0 @@\n-  if (!is_large || !UseXMMForObjInit) {\n-    xorptr(tmp, tmp);\n-  }\n@@ -6705,1 +7446,1 @@\n-    movptr(Address(base, cnt, Address::times_ptr), tmp);\n+    movptr(Address(base, cnt, Address::times_ptr), val);\n@@ -6714,1 +7455,1 @@\n-  if (UseFastStosb) {\n+  if (UseFastStosb && !word_copy_only) {\n@@ -6718,1 +7459,1 @@\n-    xmm_clear_mem(base, cnt, tmp, xtmp, mask);\n+    xmm_clear_mem(base, cnt, val, xtmp, mask);\n@@ -10839,0 +11580,4 @@\n+  if (EnableValhalla) {\n+    \/\/ Mask inline_type bit such that we go to the slow path if object is an inline type\n+    andptr(reg_rax, ~((int) markWord::inline_type_bit_in_place));\n+  }\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.cpp","additions":762,"deletions":17,"binary":false,"changes":779,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -33,0 +33,1 @@\n+#include \"runtime\/signature.hpp\"\n@@ -36,0 +37,2 @@\n+class ciInlineKlass;\n+\n@@ -105,0 +108,23 @@\n+  \/\/ markWord tests, kills markWord reg\n+  void test_markword_is_inline_type(Register markword, Label& is_inline_type);\n+\n+  \/\/ inlineKlass queries, kills temp_reg\n+  void test_klass_is_inline_type(Register klass, Register temp_reg, Label& is_inline_type);\n+  void test_oop_is_not_inline_type(Register object, Register tmp, Label& not_inline_type);\n+\n+  void test_field_is_null_free_inline_type(Register flags, Register temp_reg, Label& is_null_free);\n+  void test_field_is_not_null_free_inline_type(Register flags, Register temp_reg, Label& not_null_free);\n+  void test_field_is_flat(Register flags, Register temp_reg, Label& is_flat);\n+  void test_field_has_null_marker(Register flags, Register temp_reg, Label& has_null_marker);\n+\n+  \/\/ Check oops for special arrays, i.e. flat arrays and\/or null-free arrays\n+  void test_oop_prototype_bit(Register oop, Register temp_reg, int32_t test_bit, bool jmp_set, Label& jmp_label);\n+  void test_flat_array_oop(Register oop, Register temp_reg, Label& is_flat_array);\n+  void test_non_flat_array_oop(Register oop, Register temp_reg, Label& is_non_flat_array);\n+  void test_null_free_array_oop(Register oop, Register temp_reg, Label& is_null_free_array);\n+  void test_non_null_free_array_oop(Register oop, Register temp_reg, Label& is_non_null_free_array);\n+\n+  \/\/ Check array klass layout helper for flat or null-free arrays...\n+  void test_flat_array_layout(Register lh, Label& is_flat_array);\n+  void test_non_flat_array_layout(Register lh, Label& is_non_flat_array);\n+\n@@ -374,0 +400,3 @@\n+\n+  \/\/ Load oopDesc._metadata without decode (useful for direct Klass* compare from oops)\n+  void load_metadata(Register dst, Register src);\n@@ -393,0 +422,9 @@\n+  void flat_field_copy(DecoratorSet decorators, Register src, Register dst, Register inline_layout_info);\n+\n+  \/\/ inline type data payload offsets...\n+  void payload_offset(Register inline_klass, Register offset);\n+  void payload_addr(Register oop, Register data, Register inline_klass);\n+  \/\/ get data payload ptr a flat value array at index, kills rcx and index\n+  void data_for_value_array_index(Register array, Register array_klass,\n+                                  Register index, Register data);\n+\n@@ -404,0 +442,2 @@\n+  void load_prototype_header(Register dst, Register src, Register tmp);\n+\n@@ -605,0 +645,9 @@\n+\n+  \/\/ Object \/ value buffer allocation...\n+  \/\/ Allocate instance of klass, assumes klass initialized by caller\n+  \/\/ new_obj prefers to be rax\n+  \/\/ Kills t1 and t2, perserves klass, return allocation in new_obj (rsi on LP64)\n+  void allocate_instance(Register klass, Register new_obj,\n+                         Register t1, Register t2,\n+                         bool clear_fields, Label& alloc_failed);\n+\n@@ -616,0 +665,5 @@\n+  \/\/ For field \"index\" within \"klass\", return inline_klass ...\n+  void get_inline_type_field_klass(Register klass, Register index, Register inline_klass);\n+\n+  void inline_layout_info(Register klass, Register index, Register layout_info);\n+\n@@ -866,1 +920,2 @@\n-  void andptr(Register src1, Register src2) { LP64_ONLY(andq(src1, src2)) NOT_LP64(andl(src1, src2)) ; }\n+  void andptr(Register dst, Register src) { LP64_ONLY(andq(dst, src)) NOT_LP64(andl(dst, src)) ; }\n+  void andptr(Register dst, Address src) { LP64_ONLY(andq(dst, src)) NOT_LP64(andl(dst, src)) ; }\n@@ -2082,0 +2137,15 @@\n+  \/\/ Inline type specific methods\n+  #include \"asm\/macroAssembler_common.hpp\"\n+\n+  int store_inline_type_fields_to_buf(ciInlineKlass* vk, bool from_interpreter = true);\n+  bool move_helper(VMReg from, VMReg to, BasicType bt, RegState reg_state[]);\n+  bool unpack_inline_helper(const GrowableArray<SigEntry>* sig, int& sig_index,\n+                            VMReg from, int& from_index, VMRegPair* to, int to_count, int& to_index,\n+                            RegState reg_state[]);\n+  bool pack_inline_helper(const GrowableArray<SigEntry>* sig, int& sig_index, int vtarg_index,\n+                          VMRegPair* from, int from_count, int& from_index, VMReg to,\n+                          RegState reg_state[], Register val_array);\n+  int extend_stack_for_inline_args(int args_on_stack);\n+  void remove_frame(int initial_framesize, bool needs_stack_repair);\n+  VMReg spill_reg_for(VMReg reg);\n+\n@@ -2084,1 +2154,1 @@\n-  void clear_mem(Register base, Register cnt, Register rtmp, XMMRegister xtmp, bool is_large, KRegister mask=knoreg);\n+  void clear_mem(Register base, Register cnt, Register val, XMMRegister xtmp, bool is_large, bool word_copy_only, KRegister mask=knoreg);\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.hpp","additions":73,"deletions":3,"binary":false,"changes":76,"status":"modified"},{"patch":"@@ -25,0 +25,1 @@\n+#include \"asm\/assembler.hpp\"\n@@ -41,0 +42,2 @@\n+#include \"utilities\/macros.hpp\"\n+#include \"vmreg_x86.inline.hpp\"\n@@ -304,4 +307,6 @@\n-  __ movptr(c_rarg0, result);\n-  Label is_long, is_float, is_double, exit;\n-  __ movl(c_rarg1, result_type);\n-  __ cmpl(c_rarg1, T_OBJECT);\n+  __ movptr(r13, result);\n+  Label is_long, is_float, is_double, check_prim, exit;\n+  __ movl(rbx, result_type);\n+  __ cmpl(rbx, T_OBJECT);\n+  __ jcc(Assembler::equal, check_prim);\n+  __ cmpl(rbx, T_LONG);\n@@ -309,3 +314,1 @@\n-  __ cmpl(c_rarg1, T_LONG);\n-  __ jcc(Assembler::equal, is_long);\n-  __ cmpl(c_rarg1, T_FLOAT);\n+  __ cmpl(rbx, T_FLOAT);\n@@ -313,1 +316,1 @@\n-  __ cmpl(c_rarg1, T_DOUBLE);\n+  __ cmpl(rbx, T_DOUBLE);\n@@ -319,1 +322,1 @@\n-    __ cmpl(c_rarg1, T_INT);\n+    __ cmpl(rbx, T_INT);\n@@ -327,1 +330,1 @@\n-  __ movl(Address(c_rarg0, 0), rax);\n+  __ movl(Address(r13, 0), rax);\n@@ -385,0 +388,13 @@\n+  __ BIND(check_prim);\n+  if (InlineTypeReturnedAsFields) {\n+    \/\/ Check for scalarized return value\n+    __ testptr(rax, 1);\n+    __ jcc(Assembler::zero, is_long);\n+    \/\/ Load pack handler address\n+    __ andptr(rax, -2);\n+    __ movptr(rax, Address(rax, InstanceKlass::adr_inlineklass_fixed_block_offset()));\n+    __ movptr(rbx, Address(rax, InlineKlass::pack_handler_jobject_offset()));\n+    \/\/ Call pack handler to initialize the buffer\n+    __ call(rbx);\n+    __ jmp(exit);\n+  }\n@@ -386,1 +402,1 @@\n-  __ movq(Address(c_rarg0, 0), rax);\n+  __ movq(Address(r13, 0), rax);\n@@ -390,1 +406,1 @@\n-  __ movflt(Address(c_rarg0, 0), xmm0);\n+  __ movflt(Address(r13, 0), xmm0);\n@@ -394,1 +410,1 @@\n-  __ movdbl(Address(c_rarg0, 0), xmm0);\n+  __ movdbl(Address(r13, 0), xmm0);\n@@ -4068,0 +4084,10 @@\n+  \/\/ Generate these first because they are called from other stubs\n+  if (InlineTypeReturnedAsFields) {\n+    StubRoutines::_load_inline_type_fields_in_regs =\n+      generate_return_value_stub(CAST_FROM_FN_PTR(address, SharedRuntime::load_inline_type_fields_in_regs),\n+                                 \"load_inline_type_fields_in_regs\", false);\n+    StubRoutines::_store_inline_type_fields_to_buf =\n+      generate_return_value_stub(CAST_FROM_FN_PTR(address, SharedRuntime::store_inline_type_fields_to_buf),\n+                                 \"store_inline_type_fields_to_buf\", true);\n+  }\n+\n@@ -4120,0 +4146,144 @@\n+\/\/ Call here from the interpreter or compiled code to either load\n+\/\/ multiple returned values from the inline type instance being\n+\/\/ returned to registers or to store returned values to a newly\n+\/\/ allocated inline type instance.\n+\/\/ Register is a class, but it would be assigned numerical value.\n+\/\/ \"0\" is assigned for xmm0. Thus we need to ignore -Wnonnull.\n+PRAGMA_DIAG_PUSH\n+PRAGMA_NONNULL_IGNORED\n+address StubGenerator::generate_return_value_stub(address destination, const char* name, bool has_res) {\n+  \/\/ We need to save all registers the calling convention may use so\n+  \/\/ the runtime calls read or update those registers. This needs to\n+  \/\/ be in sync with SharedRuntime::java_return_convention().\n+  enum layout {\n+    pad_off = frame::arg_reg_save_area_bytes\/BytesPerInt, pad_off_2,\n+    rax_off, rax_off_2,\n+    j_rarg5_off, j_rarg5_2,\n+    j_rarg4_off, j_rarg4_2,\n+    j_rarg3_off, j_rarg3_2,\n+    j_rarg2_off, j_rarg2_2,\n+    j_rarg1_off, j_rarg1_2,\n+    j_rarg0_off, j_rarg0_2,\n+    j_farg0_off, j_farg0_2,\n+    j_farg1_off, j_farg1_2,\n+    j_farg2_off, j_farg2_2,\n+    j_farg3_off, j_farg3_2,\n+    j_farg4_off, j_farg4_2,\n+    j_farg5_off, j_farg5_2,\n+    j_farg6_off, j_farg6_2,\n+    j_farg7_off, j_farg7_2,\n+    rbp_off, rbp_off_2,\n+    return_off, return_off_2,\n+\n+    framesize\n+  };\n+\n+  CodeBuffer buffer(name, 1000, 512);\n+  MacroAssembler* _masm = new MacroAssembler(&buffer);\n+\n+  int frame_size_in_bytes = align_up(framesize*BytesPerInt, 16);\n+  assert(frame_size_in_bytes == framesize*BytesPerInt, \"misaligned\");\n+  int frame_size_in_slots = frame_size_in_bytes \/ BytesPerInt;\n+  int frame_size_in_words = frame_size_in_bytes \/ wordSize;\n+\n+  OopMapSet *oop_maps = new OopMapSet();\n+  OopMap* map = new OopMap(frame_size_in_slots, 0);\n+\n+  map->set_callee_saved(VMRegImpl::stack2reg(rax_off), rax->as_VMReg());\n+  map->set_callee_saved(VMRegImpl::stack2reg(j_rarg5_off), j_rarg5->as_VMReg());\n+  map->set_callee_saved(VMRegImpl::stack2reg(j_rarg4_off), j_rarg4->as_VMReg());\n+  map->set_callee_saved(VMRegImpl::stack2reg(j_rarg3_off), j_rarg3->as_VMReg());\n+  map->set_callee_saved(VMRegImpl::stack2reg(j_rarg2_off), j_rarg2->as_VMReg());\n+  map->set_callee_saved(VMRegImpl::stack2reg(j_rarg1_off), j_rarg1->as_VMReg());\n+  map->set_callee_saved(VMRegImpl::stack2reg(j_rarg0_off), j_rarg0->as_VMReg());\n+  map->set_callee_saved(VMRegImpl::stack2reg(j_farg0_off), j_farg0->as_VMReg());\n+  map->set_callee_saved(VMRegImpl::stack2reg(j_farg1_off), j_farg1->as_VMReg());\n+  map->set_callee_saved(VMRegImpl::stack2reg(j_farg2_off), j_farg2->as_VMReg());\n+  map->set_callee_saved(VMRegImpl::stack2reg(j_farg3_off), j_farg3->as_VMReg());\n+  map->set_callee_saved(VMRegImpl::stack2reg(j_farg4_off), j_farg4->as_VMReg());\n+  map->set_callee_saved(VMRegImpl::stack2reg(j_farg5_off), j_farg5->as_VMReg());\n+  map->set_callee_saved(VMRegImpl::stack2reg(j_farg6_off), j_farg6->as_VMReg());\n+  map->set_callee_saved(VMRegImpl::stack2reg(j_farg7_off), j_farg7->as_VMReg());\n+\n+  int start = __ offset();\n+\n+  __ subptr(rsp, frame_size_in_bytes - 8 \/* return address*\/);\n+\n+  __ movptr(Address(rsp, rbp_off * BytesPerInt), rbp);\n+  __ movdbl(Address(rsp, j_farg7_off * BytesPerInt), j_farg7);\n+  __ movdbl(Address(rsp, j_farg6_off * BytesPerInt), j_farg6);\n+  __ movdbl(Address(rsp, j_farg5_off * BytesPerInt), j_farg5);\n+  __ movdbl(Address(rsp, j_farg4_off * BytesPerInt), j_farg4);\n+  __ movdbl(Address(rsp, j_farg3_off * BytesPerInt), j_farg3);\n+  __ movdbl(Address(rsp, j_farg2_off * BytesPerInt), j_farg2);\n+  __ movdbl(Address(rsp, j_farg1_off * BytesPerInt), j_farg1);\n+  __ movdbl(Address(rsp, j_farg0_off * BytesPerInt), j_farg0);\n+\n+  __ movptr(Address(rsp, j_rarg0_off * BytesPerInt), j_rarg0);\n+  __ movptr(Address(rsp, j_rarg1_off * BytesPerInt), j_rarg1);\n+  __ movptr(Address(rsp, j_rarg2_off * BytesPerInt), j_rarg2);\n+  __ movptr(Address(rsp, j_rarg3_off * BytesPerInt), j_rarg3);\n+  __ movptr(Address(rsp, j_rarg4_off * BytesPerInt), j_rarg4);\n+  __ movptr(Address(rsp, j_rarg5_off * BytesPerInt), j_rarg5);\n+  __ movptr(Address(rsp, rax_off * BytesPerInt), rax);\n+\n+  int frame_complete = __ offset();\n+\n+  __ set_last_Java_frame(noreg, noreg, nullptr, rscratch1);\n+\n+  __ mov(c_rarg0, r15_thread);\n+  __ mov(c_rarg1, rax);\n+\n+  __ call(RuntimeAddress(destination));\n+\n+  \/\/ Set an oopmap for the call site.\n+\n+  oop_maps->add_gc_map( __ offset() - start, map);\n+\n+  \/\/ clear last_Java_sp\n+  __ reset_last_Java_frame(false);\n+\n+  __ movptr(rbp, Address(rsp, rbp_off * BytesPerInt));\n+  __ movdbl(j_farg7, Address(rsp, j_farg7_off * BytesPerInt));\n+  __ movdbl(j_farg6, Address(rsp, j_farg6_off * BytesPerInt));\n+  __ movdbl(j_farg5, Address(rsp, j_farg5_off * BytesPerInt));\n+  __ movdbl(j_farg4, Address(rsp, j_farg4_off * BytesPerInt));\n+  __ movdbl(j_farg3, Address(rsp, j_farg3_off * BytesPerInt));\n+  __ movdbl(j_farg2, Address(rsp, j_farg2_off * BytesPerInt));\n+  __ movdbl(j_farg1, Address(rsp, j_farg1_off * BytesPerInt));\n+  __ movdbl(j_farg0, Address(rsp, j_farg0_off * BytesPerInt));\n+\n+  __ movptr(j_rarg0, Address(rsp, j_rarg0_off * BytesPerInt));\n+  __ movptr(j_rarg1, Address(rsp, j_rarg1_off * BytesPerInt));\n+  __ movptr(j_rarg2, Address(rsp, j_rarg2_off * BytesPerInt));\n+  __ movptr(j_rarg3, Address(rsp, j_rarg3_off * BytesPerInt));\n+  __ movptr(j_rarg4, Address(rsp, j_rarg4_off * BytesPerInt));\n+  __ movptr(j_rarg5, Address(rsp, j_rarg5_off * BytesPerInt));\n+  __ movptr(rax, Address(rsp, rax_off * BytesPerInt));\n+\n+  __ addptr(rsp, frame_size_in_bytes-8);\n+\n+  \/\/ check for pending exceptions\n+  Label pending;\n+  __ cmpptr(Address(r15_thread, Thread::pending_exception_offset()), (int32_t)NULL_WORD);\n+  __ jcc(Assembler::notEqual, pending);\n+\n+  if (has_res) {\n+    __ get_vm_result(rax, r15_thread);\n+  }\n+\n+  __ ret(0);\n+\n+  __ bind(pending);\n+\n+  __ movptr(rax, Address(r15_thread, Thread::pending_exception_offset()));\n+  __ jump(RuntimeAddress(StubRoutines::forward_exception_entry()));\n+\n+  \/\/ -------------\n+  \/\/ make sure all code is generated\n+  _masm->flush();\n+\n+  RuntimeStub* stub = RuntimeStub::new_runtime_stub(name, &buffer, frame_complete, frame_size_in_words, oop_maps, false);\n+  return stub->entry_point();\n+}\n+\n","filename":"src\/hotspot\/cpu\/x86\/stubGenerator_x86_64.cpp","additions":183,"deletions":13,"binary":false,"changes":196,"status":"modified"},{"patch":"@@ -41,0 +41,1 @@\n+#include \"oops\/inlineKlass.hpp\"\n@@ -66,1 +67,1 @@\n-int TemplateInterpreter::InterpreterCodeSize = JVMCI_ONLY(268) NOT_JVMCI(256) * 1024;\n+int TemplateInterpreter::InterpreterCodeSize = JVMCI_ONLY(280) NOT_JVMCI(268) * 1024;\n@@ -178,2 +179,2 @@\n-  __ movptr(rcx, Address(rbp, frame::interpreter_frame_last_sp_offset * wordSize));\n-  __ lea(rsp, Address(rbp, rcx, Address::times_ptr));\n+  __ movptr(rscratch1, Address(rbp, frame::interpreter_frame_last_sp_offset * wordSize));\n+  __ lea(rsp, Address(rbp, rscratch1, Address::times_ptr));\n@@ -184,0 +185,4 @@\n+  if (state == atos && InlineTypeReturnedAsFields) {\n+    __ store_inline_type_fields_to_buf(nullptr);\n+  }\n+\n","filename":"src\/hotspot\/cpu\/x86\/templateInterpreterGenerator_x86.cpp","additions":8,"deletions":3,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -39,0 +39,1 @@\n+#include \"oops\/inlineKlass.hpp\"\n@@ -170,0 +171,1 @@\n+  case Bytecodes::_fast_vputfield:\n@@ -786,9 +788,28 @@\n-  \/\/ rax: index\n-  \/\/ rdx: array\n-  index_check(rdx, rax); \/\/ kills rbx\n-  do_oop_load(_masm,\n-              Address(rdx, rax,\n-                      UseCompressedOops ? Address::times_4 : Address::times_ptr,\n-                      arrayOopDesc::base_offset_in_bytes(T_OBJECT)),\n-              rax,\n-              IS_ARRAY);\n+  Register array = rdx;\n+  Register index = rax;\n+\n+  index_check(array, index); \/\/ kills rbx\n+  __ profile_array_type<ArrayLoadData>(rbx, array, rcx);\n+  if (UseArrayFlattening) {\n+    Label is_flat_array, done;\n+    __ test_flat_array_oop(array, rbx, is_flat_array);\n+    do_oop_load(_masm,\n+                Address(array, index,\n+                        UseCompressedOops ? Address::times_4 : Address::times_ptr,\n+                        arrayOopDesc::base_offset_in_bytes(T_OBJECT)),\n+                rax,\n+                IS_ARRAY);\n+    __ jmp(done);\n+    __ bind(is_flat_array);\n+    __ movptr(rcx, array);\n+    call_VM(rax, CAST_FROM_FN_PTR(address, InterpreterRuntime::flat_array_load), rcx, index);\n+    __ bind(done);\n+  } else {\n+    do_oop_load(_masm,\n+                Address(array, index,\n+                        UseCompressedOops ? Address::times_4 : Address::times_ptr,\n+                        arrayOopDesc::base_offset_in_bytes(T_OBJECT)),\n+                rax,\n+                IS_ARRAY);\n+  }\n+  __ profile_element_type(rbx, rax, rcx);\n@@ -1068,1 +1089,1 @@\n-  Label is_null, ok_is_subtype, done;\n+  Label is_null, is_flat_array, ok_is_subtype, done;\n@@ -1080,0 +1101,4 @@\n+\n+  __ profile_array_type<ArrayStoreData>(rdi, rdx, rbx);\n+  __ profile_multiple_element_types(rdi, rax, rbx, rcx);\n+\n@@ -1083,0 +1108,7 @@\n+  \/\/ Move array class to rdi\n+  __ load_klass(rdi, rdx, rscratch1);\n+  if (UseArrayFlattening) {\n+    __ movl(rbx, Address(rdi, Klass::layout_helper_offset()));\n+    __ test_flat_array_layout(rbx, is_flat_array);\n+  }\n+\n@@ -1085,3 +1117,2 @@\n-  \/\/ Move superklass into rax\n-  __ load_klass(rax, rdx, rscratch1);\n-  __ movptr(rax, Address(rax,\n+  \/\/ Move array element superklass into rax\n+  __ movptr(rax, Address(rdi,\n@@ -1092,1 +1123,2 @@\n-  __ gen_subtype_check(rbx, ok_is_subtype);\n+  \/\/ is \"rbx <: rax\" ? (value subclass <: array element superclass)\n+  __ gen_subtype_check(rbx, ok_is_subtype, false);\n@@ -1110,1 +1142,16 @@\n-  __ profile_null_seen(rbx);\n+  if (EnableValhalla) {\n+    Label write_null_to_null_free_array, store_null;\n+\n+      \/\/ Move array class to rdi\n+    __ load_klass(rdi, rdx, rscratch1);\n+    if (UseArrayFlattening) {\n+      __ movl(rbx, Address(rdi, Klass::layout_helper_offset()));\n+      __ test_flat_array_layout(rbx, is_flat_array);\n+    }\n+\n+    \/\/ No way to store null in null-free array\n+    __ test_null_free_array_oop(rdx, rbx, write_null_to_null_free_array);\n+    __ jmp(store_null);\n+\n+    __ bind(write_null_to_null_free_array);\n+    __ jump(RuntimeAddress(Interpreter::_throw_NullPointerException_entry));\n@@ -1112,0 +1159,2 @@\n+    __ bind(store_null);\n+  }\n@@ -1114,0 +1163,9 @@\n+  __ jmp(done);\n+\n+  if (UseArrayFlattening) {\n+    Label is_type_ok;\n+    __ bind(is_flat_array); \/\/ Store non-null value to flat\n+\n+    __ movptr(rax, at_tos());\n+    __ movl(rcx, at_tos_p1()); \/\/ index\n+    __ movptr(rdx, at_tos_p2()); \/\/ array\n@@ -1115,0 +1173,2 @@\n+    call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::flat_array_store), rax, rdx, rcx);\n+  }\n@@ -1925,1 +1985,1 @@\n-  Label not_taken;\n+  Label taken, not_taken;\n@@ -1927,0 +1987,36 @@\n+\n+  __ profile_acmp(rbx, rdx, rax, rcx);\n+\n+  const int is_inline_type_mask = markWord::inline_type_pattern;\n+  if (EnableValhalla) {\n+    __ cmpoop(rdx, rax);\n+    __ jcc(Assembler::equal, (cc == equal) ? taken : not_taken);\n+\n+    \/\/ might be substitutable, test if either rax or rdx is null\n+    __ testptr(rax, rax);\n+    __ jcc(Assembler::zero, (cc == equal) ? not_taken : taken);\n+    __ testptr(rdx, rdx);\n+    __ jcc(Assembler::zero, (cc == equal) ? not_taken : taken);\n+\n+    \/\/ and both are values ?\n+    __ movptr(rbx, Address(rdx, oopDesc::mark_offset_in_bytes()));\n+    __ andptr(rbx, Address(rax, oopDesc::mark_offset_in_bytes()));\n+    __ andptr(rbx, is_inline_type_mask);\n+    __ cmpptr(rbx, is_inline_type_mask);\n+    __ jcc(Assembler::notEqual, (cc == equal) ? not_taken : taken);\n+\n+    \/\/ same value klass ?\n+    __ load_metadata(rbx, rdx);\n+    __ load_metadata(rcx, rax);\n+    __ cmpptr(rbx, rcx);\n+    __ jcc(Assembler::notEqual, (cc == equal) ? not_taken : taken);\n+\n+    \/\/ Know both are the same type, let's test for substitutability...\n+    if (cc == equal) {\n+      invoke_is_substitutable(rax, rdx, taken, not_taken);\n+    } else {\n+      invoke_is_substitutable(rax, rdx, not_taken, taken);\n+    }\n+    __ stop(\"Not reachable\");\n+  }\n+\n@@ -1929,0 +2025,1 @@\n+  __ bind(taken);\n@@ -1931,1 +2028,10 @@\n-  __ profile_not_taken_branch(rax);\n+  __ profile_not_taken_branch(rax, true);\n+}\n+\n+void TemplateTable::invoke_is_substitutable(Register aobj, Register bobj,\n+                                            Label& is_subst, Label& not_subst) {\n+  __ call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::is_substitutable), aobj, bobj);\n+  \/\/ Restored...rax answer, jmp to outcome...\n+  __ testl(rax, rax);\n+  __ jcc(Assembler::zero, not_subst);\n+  __ jmp(is_subst);\n@@ -2185,1 +2291,2 @@\n-  __ remove_activation(state, rbcp);\n+\n+  __ remove_activation(state, rbcp, true, true, true);\n@@ -2547,1 +2654,1 @@\n-  const Register obj   = c_rarg3;\n+  const Register obj   = r9;\n@@ -2559,2 +2666,0 @@\n-  if (!is_static) pop_and_check_object(obj);\n-\n@@ -2563,1 +2668,1 @@\n-  Label Done, notByte, notBool, notInt, notShort, notChar, notLong, notFloat, notObj;\n+  Label Done, notByte, notBool, notInt, notShort, notChar, notLong, notFloat, notObj, notInlineType;\n@@ -2571,0 +2676,1 @@\n+  if (!is_static) pop_and_check_object(obj);\n@@ -2582,1 +2688,1 @@\n-\n+   if (!is_static) pop_and_check_object(obj);\n@@ -2597,4 +2703,67 @@\n-  do_oop_load(_masm, field, rax);\n-  __ push(atos);\n-  if (!is_static && rc == may_rewrite) {\n-    patch_bytecode(Bytecodes::_fast_agetfield, bc, rbx);\n+  if (!EnableValhalla) {\n+    if (!is_static) pop_and_check_object(obj);\n+    do_oop_load(_masm, field, rax);\n+    __ push(atos);\n+    if (!is_static && rc == may_rewrite) {\n+      patch_bytecode(Bytecodes::_fast_agetfield, bc, rbx);\n+    }\n+    __ jmp(Done);\n+  } else {\n+    if (is_static) {\n+      __ load_heap_oop(rax, field);\n+      Label is_null_free_inline_type, uninitialized;\n+      \/\/ Issue below if the static field has not been initialized yet\n+      __ test_field_is_null_free_inline_type(flags, rscratch1, is_null_free_inline_type);\n+        \/\/ field is not a null free inline type\n+        __ push(atos);\n+        __ jmp(Done);\n+      \/\/ field is a null free inline type, must not return null even if uninitialized\n+      __ bind(is_null_free_inline_type);\n+          __ testptr(rax, rax);\n+        __ jcc(Assembler::zero, uninitialized);\n+          __ push(atos);\n+          __ jmp(Done);\n+        __ bind(uninitialized);\n+          __ jump(RuntimeAddress(Interpreter::_throw_NPE_UninitializedField_entry));\n+    } else {\n+      Label is_flat, nonnull, is_null_free_inline_type, rewrite_inline, has_null_marker;\n+      __ test_field_is_null_free_inline_type(flags, rscratch1, is_null_free_inline_type);\n+      __ test_field_has_null_marker(flags, rscratch1, has_null_marker);\n+      \/\/ field is not a null free inline type\n+      pop_and_check_object(obj);\n+      __ load_heap_oop(rax, field);\n+      __ push(atos);\n+      if (rc == may_rewrite) {\n+        patch_bytecode(Bytecodes::_fast_agetfield, bc, rbx);\n+      }\n+      __ jmp(Done);\n+      __ bind(is_null_free_inline_type);\n+      __ test_field_is_flat(flags, rscratch1, is_flat);\n+          \/\/ field is not flat\n+          pop_and_check_object(obj);\n+          __ load_heap_oop(rax, field);\n+          __ testptr(rax, rax);\n+          __ jcc(Assembler::notZero, nonnull);\n+          __ jump(RuntimeAddress(Interpreter::_throw_NPE_UninitializedField_entry));\n+          __ bind(nonnull);\n+          __ verify_oop(rax);\n+          __ push(atos);\n+          __ jmp(rewrite_inline);\n+        __ bind(is_flat);\n+          pop_and_check_object(rax);\n+          __ read_flat_field(rcx, rdx, rbx, rax);\n+          __ verify_oop(rax);\n+          __ push(atos);\n+          __ jmp(rewrite_inline);\n+      __ bind(has_null_marker);\n+        pop_and_check_object(rax);\n+        __ load_field_entry(rcx, rbx);\n+        call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::read_nullable_flat_field), rax, rcx);\n+        __ get_vm_result(rax, r15_thread);\n+        __ push(atos);\n+      __ bind(rewrite_inline);\n+      if (rc == may_rewrite) {\n+        patch_bytecode(Bytecodes::_fast_vgetfield, bc, rbx);\n+      }\n+        __ jmp(Done);\n+    }\n@@ -2602,1 +2771,0 @@\n-  __ jmp(Done);\n@@ -2605,0 +2773,3 @@\n+\n+  if (!is_static) pop_and_check_object(obj);\n+\n@@ -2704,1 +2875,0 @@\n-\n@@ -2766,1 +2936,1 @@\n-  const Register flags = rax;\n+  const Register flags = r9;\n@@ -2779,2 +2949,3 @@\n-  __ andl(flags, (1 << ResolvedFieldEntry::is_volatile_shift));\n-  __ testl(flags, flags);\n+  __ movl(rscratch1, flags);\n+  __ andl(rscratch1, (1 << ResolvedFieldEntry::is_volatile_shift));\n+  __ testl(rscratch1, rscratch1);\n@@ -2783,1 +2954,1 @@\n-  putfield_or_static_helper(byte_no, is_static, rc, obj, off, tos_state);\n+  putfield_or_static_helper(byte_no, is_static, rc, obj, off, tos_state, flags);\n@@ -2789,1 +2960,1 @@\n-  putfield_or_static_helper(byte_no, is_static, rc, obj, off, tos_state);\n+  putfield_or_static_helper(byte_no, is_static, rc, obj, off, tos_state, flags);\n@@ -2795,1 +2966,1 @@\n-                                              Register obj, Register off, Register tos_state) {\n+                                              Register obj, Register off, Register tos_state, Register flags) {\n@@ -2801,1 +2972,1 @@\n-        notLong, notFloat, notObj;\n+        notLong, notFloat, notObj, notInlineType;\n@@ -2842,6 +3013,63 @@\n-    __ pop(atos);\n-    if (!is_static) pop_and_check_object(obj);\n-    \/\/ Store into the field\n-    do_oop_store(_masm, field, rax);\n-    if (!is_static && rc == may_rewrite) {\n-      patch_bytecode(Bytecodes::_fast_aputfield, bc, rbx, true, byte_no);\n+    if (!EnableValhalla) {\n+      __ pop(atos);\n+      if (!is_static) pop_and_check_object(obj);\n+      \/\/ Store into the field\n+      do_oop_store(_masm, field, rax);\n+      if (!is_static && rc == may_rewrite) {\n+        patch_bytecode(Bytecodes::_fast_aputfield, bc, rbx, true, byte_no);\n+      }\n+      __ jmp(Done);\n+    } else {\n+      __ pop(atos);\n+      if (is_static) {\n+        Label is_inline_type;\n+        __ test_field_is_not_null_free_inline_type(flags, rscratch1, is_inline_type);\n+        __ null_check(rax);\n+        __ bind(is_inline_type);\n+        do_oop_store(_masm, field, rax);\n+        __ jmp(Done);\n+      } else {\n+        Label is_null_free_inline_type, is_flat, has_null_marker,\n+              write_null, rewrite_not_inline, rewrite_inline;\n+        __ test_field_is_null_free_inline_type(flags, rscratch1, is_null_free_inline_type);\n+        __ test_field_has_null_marker(flags, rscratch1, has_null_marker);\n+          \/\/ Not an inline type\n+          pop_and_check_object(obj);\n+          \/\/ Store into the field\n+          do_oop_store(_masm, field, rax);\n+          __ bind(rewrite_not_inline);\n+          if (rc == may_rewrite) {\n+            patch_bytecode(Bytecodes::_fast_aputfield, bc, rbx, true, byte_no);\n+          }\n+          __ jmp(Done);\n+        \/\/ Implementation of the inline type semantic\n+        __ bind(is_null_free_inline_type);\n+          __ null_check(rax);\n+          __ test_field_is_flat(flags, rscratch1, is_flat);\n+            \/\/ field is not flat\n+            pop_and_check_object(obj);\n+            \/\/ Store into the field\n+            do_oop_store(_masm, field, rax);\n+          __ jmp(rewrite_inline);\n+          __ bind(is_flat);\n+            \/\/ field is flat\n+            __ load_unsigned_short(rdx, Address(rcx, in_bytes(ResolvedFieldEntry::field_index_offset())));\n+            __ movptr(r9, Address(rcx, in_bytes(ResolvedFieldEntry::field_holder_offset())));\n+            pop_and_check_object(obj);  \/\/ obj = rcx\n+            __ load_klass(r8, rax, rscratch1);\n+            __ payload_addr(rax, rax, r8);\n+            __ addptr(obj, off);\n+            __ inline_layout_info(r9, rdx, rbx);\n+            \/\/ because we use InlineLayoutInfo, we need special value access code specialized for fields (arrays will need a different API)\n+            __ flat_field_copy(IN_HEAP, rax, obj, rbx);\n+            __ jmp(rewrite_inline);\n+        __ bind(has_null_marker); \/\/ has null marker means the field is flat with a null marker\n+          pop_and_check_object(rbx);\n+          __ load_field_entry(rcx, rdx);\n+          call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::write_nullable_flat_field), rbx, rax, rcx);\n+        __ bind(rewrite_inline);\n+        if (rc == may_rewrite) {\n+          patch_bytecode(Bytecodes::_fast_vputfield, bc, rbx, true, byte_no);\n+        }\n+        __ jmp(Done);\n+      }\n@@ -2849,1 +3077,0 @@\n-    __ jmp(Done);\n@@ -2986,0 +3213,1 @@\n+    case Bytecodes::_fast_vputfield: \/\/fall through\n@@ -3009,0 +3237,1 @@\n+    case Bytecodes::_fast_vputfield: \/\/ fall through\n@@ -3027,2 +3256,0 @@\n-  Register cache = rcx;\n-\n@@ -3035,3 +3262,1 @@\n-  load_resolved_field_entry(noreg, cache, rax, rbx, rdx);\n-  \/\/ RBX: field offset, RAX: TOS, RDX: flags\n-  __ andl(rdx, (1 << ResolvedFieldEntry::is_volatile_shift));\n+  load_resolved_field_entry(noreg, rcx, rax, rbx, rdx);\n@@ -3039,0 +3264,1 @@\n+  \/\/ RBX: field offset, RCX: RAX: TOS, RDX: flags\n@@ -3047,1 +3273,3 @@\n-  __ testl(rdx, rdx);\n+  __ movl(rscratch2, rdx);  \/\/ saving flags for is_flat test\n+  __ andl(rscratch2, (1 << ResolvedFieldEntry::is_volatile_shift));\n+  __ testl(rscratch2, rscratch2);\n@@ -3050,1 +3278,1 @@\n-  fast_storefield_helper(field, rax);\n+  fast_storefield_helper(field, rax, rdx);\n@@ -3056,1 +3284,1 @@\n-  fast_storefield_helper(field, rax);\n+  fast_storefield_helper(field, rax, rdx);\n@@ -3061,1 +3289,3 @@\n-void TemplateTable::fast_storefield_helper(Address field, Register rax) {\n+void TemplateTable::fast_storefield_helper(Address field, Register rax, Register flags) {\n+\n+  \/\/ DANGER: 'field' argument depends on rcx and rbx\n@@ -3065,0 +3295,27 @@\n+  case Bytecodes::_fast_vputfield:\n+    {\n+      Label is_flat, has_null_marker, write_null, done;\n+      __ test_field_has_null_marker(flags, rscratch1, has_null_marker);\n+      \/\/ Null free field cases: flat or not flat\n+      __ null_check(rax);\n+      __ test_field_is_flat(flags, rscratch1, is_flat);\n+        \/\/ field is not flat\n+        do_oop_store(_masm, field, rax);\n+        __ jmp(done);\n+      __ bind(is_flat);\n+        __ load_field_entry(r8, r9);\n+        __ load_unsigned_short(r9, Address(r8, in_bytes(ResolvedFieldEntry::field_index_offset())));\n+        __ movptr(r8, Address(r8, in_bytes(ResolvedFieldEntry::field_holder_offset())));\n+        __ inline_layout_info(r8, r9, r8);\n+        __ load_klass(rdx, rax, rscratch1);\n+        __ payload_addr(rax, rax, rdx);\n+        __ lea(rcx, field);\n+        __ flat_field_copy(IN_HEAP, rax, rcx, r8);\n+        __ jmp(done);\n+      __ bind(has_null_marker); \/\/ has null marker means the field is flat with a null marker\n+        __ movptr(rbx, rcx);\n+        __ load_field_entry(rcx, rdx);\n+        call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::write_nullable_flat_field), rbx, rax, rcx);\n+      __ bind(done);\n+    }\n+    break;\n@@ -3066,1 +3323,3 @@\n-    do_oop_store(_masm, field, rax);\n+    {\n+      do_oop_store(_masm, field, rax);\n+    }\n@@ -3122,1 +3381,1 @@\n-  __ load_sized_value(rbx, Address(rcx, in_bytes(ResolvedFieldEntry::field_offset_offset())), sizeof(int), true \/*is_signed*\/);\n+  __ load_sized_value(rdx, Address(rcx, in_bytes(ResolvedFieldEntry::field_offset_offset())), sizeof(int), true \/*is_signed*\/);\n@@ -3127,1 +3386,1 @@\n-  Address field(rax, rbx, Address::times_1);\n+  Address field(rax, rdx, Address::times_1);\n@@ -3131,0 +3390,26 @@\n+  case Bytecodes::_fast_vgetfield:\n+    {\n+      Label is_flat, nonnull, Done, has_null_marker;\n+      __ load_unsigned_byte(rscratch1, Address(rcx, in_bytes(ResolvedFieldEntry::flags_offset())));\n+      __ test_field_has_null_marker(rscratch1, rscratch2, has_null_marker);\n+      __ test_field_is_flat(rscratch1, rscratch2, is_flat);\n+        \/\/ field is not flat\n+        __ load_heap_oop(rax, field);\n+        __ testptr(rax, rax);\n+        __ jcc(Assembler::notZero, nonnull);\n+          __ jump(RuntimeAddress(Interpreter::_throw_NPE_UninitializedField_entry));\n+        __ bind(nonnull);\n+        __ verify_oop(rax);\n+        __ jmp(Done);\n+      __ bind(is_flat);\n+      \/\/ field is flat\n+        __ read_flat_field(rcx, rdx, rbx, rax);\n+        __ jmp(Done);\n+      __ bind(has_null_marker);\n+        \/\/ rax = instance, rcx = resolved entry\n+        call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::read_nullable_flat_field), rax, rcx);\n+        __ get_vm_result(rax, r15_thread);\n+      __ bind(Done);\n+      __ verify_oop(rax);\n+    }\n+    break;\n@@ -3555,2 +3840,0 @@\n-  Label slow_case_no_pop;\n-  Label initialize_header;\n@@ -3566,1 +3849,1 @@\n-  __ jcc(Assembler::notEqual, slow_case_no_pop);\n+  __ jcc(Assembler::notEqual, slow_case);\n@@ -3570,1 +3853,0 @@\n-  __ push(rcx);  \/\/ save the contexts of klass for initializing the header\n@@ -3576,73 +3858,1 @@\n-  \/\/ get instance_size in InstanceKlass (scaled to a count of bytes)\n-  __ movl(rdx, Address(rcx, Klass::layout_helper_offset()));\n-  \/\/ test to see if it is malformed in some way\n-  __ testl(rdx, Klass::_lh_instance_slow_path_bit);\n-  __ jcc(Assembler::notZero, slow_case);\n-\n-  \/\/ Allocate the instance:\n-  \/\/  If TLAB is enabled:\n-  \/\/    Try to allocate in the TLAB.\n-  \/\/    If fails, go to the slow path.\n-  \/\/    Initialize the allocation.\n-  \/\/    Exit.\n-  \/\/\n-  \/\/  Go to slow path.\n-\n-  if (UseTLAB) {\n-    __ tlab_allocate(r15_thread, rax, rdx, 0, rcx, rbx, slow_case);\n-    if (ZeroTLAB) {\n-      \/\/ the fields have been already cleared\n-      __ jmp(initialize_header);\n-    }\n-\n-    \/\/ The object is initialized before the header.  If the object size is\n-    \/\/ zero, go directly to the header initialization.\n-    if (UseCompactObjectHeaders) {\n-      assert(is_aligned(oopDesc::base_offset_in_bytes(), BytesPerLong), \"oop base offset must be 8-byte-aligned\");\n-      __ decrement(rdx, oopDesc::base_offset_in_bytes());\n-    } else {\n-      __ decrement(rdx, sizeof(oopDesc));\n-    }\n-    __ jcc(Assembler::zero, initialize_header);\n-\n-    \/\/ Initialize topmost object field, divide rdx by 8, check if odd and\n-    \/\/ test if zero.\n-    __ xorl(rcx, rcx);    \/\/ use zero reg to clear memory (shorter code)\n-    __ shrl(rdx, LogBytesPerLong); \/\/ divide by 2*oopSize and set carry flag if odd\n-\n-    \/\/ rdx must have been multiple of 8\n-#ifdef ASSERT\n-    \/\/ make sure rdx was multiple of 8\n-    Label L;\n-    \/\/ Ignore partial flag stall after shrl() since it is debug VM\n-    __ jcc(Assembler::carryClear, L);\n-    __ stop(\"object size is not multiple of 2 - adjust this code\");\n-    __ bind(L);\n-    \/\/ rdx must be > 0, no extra check needed here\n-#endif\n-\n-    \/\/ initialize remaining object fields: rdx was a multiple of 8\n-    { Label loop;\n-    __ bind(loop);\n-    int header_size_bytes = oopDesc::header_size() * HeapWordSize;\n-    assert(is_aligned(header_size_bytes, BytesPerLong), \"oop header size must be 8-byte-aligned\");\n-    __ movptr(Address(rax, rdx, Address::times_8, header_size_bytes - 1*oopSize), rcx);\n-    __ decrement(rdx);\n-    __ jcc(Assembler::notZero, loop);\n-    }\n-\n-    \/\/ initialize object header only.\n-    __ bind(initialize_header);\n-    if (UseCompactObjectHeaders) {\n-      __ pop(rcx);   \/\/ get saved klass back in the register.\n-      __ movptr(rbx, Address(rcx, Klass::prototype_header_offset()));\n-      __ movptr(Address(rax, oopDesc::mark_offset_in_bytes()), rbx);\n-    } else {\n-      __ movptr(Address(rax, oopDesc::mark_offset_in_bytes()),\n-                (intptr_t)markWord::prototype().value()); \/\/ header\n-      __ pop(rcx);   \/\/ get saved klass back in the register.\n-      __ xorl(rsi, rsi); \/\/ use zero reg to clear memory (shorter code)\n-      __ store_klass_gap(rax, rsi);  \/\/ zero klass gap for compressed oops\n-      __ store_klass(rax, rcx, rscratch1);  \/\/ klass\n-    }\n-\n+  __ allocate_instance(rcx, rax, rdx, rbx, true, slow_case);\n@@ -3657,3 +3867,1 @@\n-\n-    __ jmp(done);\n-  }\n+  __ jmp(done);\n@@ -3663,2 +3871,0 @@\n-  __ pop(rcx);   \/\/ restore stack pointer to what it was when we came in.\n-  __ bind(slow_case_no_pop);\n@@ -3706,4 +3912,4 @@\n-  __ cmpb(Address(rdx, rbx,\n-                  Address::times_1,\n-                  Array<u1>::base_offset_in_bytes()),\n-          JVM_CONSTANT_Class);\n+  __ movzbl(rdx, Address(rdx, rbx,\n+      Address::times_1,\n+      Array<u1>::base_offset_in_bytes()));\n+  __ cmpl(rdx, JVM_CONSTANT_Class);\n@@ -3740,0 +3946,3 @@\n+  __ jmp(done);\n+\n+  __ bind(is_null);\n@@ -3743,4 +3952,1 @@\n-    __ jmp(done);\n-    __ bind(is_null);\n-  } else {\n-    __ bind(is_null);   \/\/ same as 'done'\n+\n@@ -3762,4 +3968,4 @@\n-  __ cmpb(Address(rdx, rbx,\n-                  Address::times_1,\n-                  Array<u1>::base_offset_in_bytes()),\n-          JVM_CONSTANT_Class);\n+  __ movzbl(rdx, Address(rdx, rbx,\n+        Address::times_1,\n+        Array<u1>::base_offset_in_bytes()));\n+  __ cmpl(rdx, JVM_CONSTANT_Class);\n@@ -3810,1 +4016,0 @@\n-\n@@ -3870,0 +4075,4 @@\n+  Label is_inline_type;\n+  __ movptr(rbx, Address(rax, oopDesc::mark_offset_in_bytes()));\n+  __ test_markword_is_inline_type(rbx, is_inline_type);\n+\n@@ -3962,0 +4171,5 @@\n+\n+  __ bind(is_inline_type);\n+  __ call_VM(noreg, CAST_FROM_FN_PTR(address,\n+                    InterpreterRuntime::throw_identity_exception), rax);\n+  __ should_not_reach_here();\n@@ -3970,0 +4184,11 @@\n+  const int is_inline_type_mask = markWord::inline_type_pattern;\n+  Label has_identity;\n+  __ movptr(rbx, Address(rax, oopDesc::mark_offset_in_bytes()));\n+  __ andptr(rbx, is_inline_type_mask);\n+  __ cmpl(rbx, is_inline_type_mask);\n+  __ jcc(Assembler::notEqual, has_identity);\n+  __ call_VM(noreg, CAST_FROM_FN_PTR(address,\n+                     InterpreterRuntime::throw_illegal_monitor_state_exception));\n+  __ should_not_reach_here();\n+  __ bind(has_identity);\n+\n","filename":"src\/hotspot\/cpu\/x86\/templateTable_x86.cpp","additions":375,"deletions":150,"binary":false,"changes":525,"status":"modified"},{"patch":"@@ -1859,1 +1859,1 @@\n-  if (!UseFastStosb && UseSSE >= 2 && UseUnalignedLoadStores) {\n+  if (UseSSE >= 2 && UseUnalignedLoadStores) {\n","filename":"src\/hotspot\/cpu\/x86\/vm_version_x86.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -781,0 +781,1 @@\n+       !strcmp(_matrule->_rChild->_opType,\"CastI2N\")      ||\n@@ -811,1 +812,1 @@\n-  return  false;\n+  return false;\n@@ -902,1 +903,2 @@\n-      strcmp(_matrule->_opType,\"Halt\"      )==0 )\n+      strcmp(_matrule->_opType,\"Halt\"      )==0 ||\n+      strcmp(_matrule->_opType,\"CallLeafNoFP\")==0)\n@@ -3646,1 +3648,1 @@\n-    \"StoreI\",\"StoreL\",\"StoreP\",\"StoreN\",\"StoreNKlass\",\"StoreD\",\"StoreF\" ,\n+    \"StoreI\",\"StoreL\",\"StoreLSpecial\",\"StoreP\",\"StoreN\",\"StoreNKlass\",\"StoreD\",\"StoreF\" ,\n","filename":"src\/hotspot\/share\/adlc\/formssel.cpp","additions":5,"deletions":3,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -283,1 +283,1 @@\n-  if (!x->mismatched() && array != nullptr && index != nullptr) {\n+  if (!x->should_profile() && !x->mismatched() && array != nullptr && index != nullptr) {\n@@ -651,1 +651,2 @@\n-      if (!is_interface && klass->is_subtype_of(x->klass())) {\n+      if (!is_interface && klass->is_subtype_of(x->klass()) && (!x->is_null_free() || obj->is_null_free())) {\n+        assert(!x->klass()->is_inlinetype() || x->klass() == klass, \"Inline klasses can't have subtypes\");\n@@ -656,2 +657,2 @@\n-    \/\/ checkcast of null returns null\n-    if (obj->as_Constant() && obj->type()->as_ObjectType()->constant_value()->is_null_object()) {\n+    \/\/ checkcast of null returns null for non null-free klasses\n+    if (!x->is_null_free() && obj->is_null_obj()) {\n@@ -671,1 +672,1 @@\n-    if (obj->as_Constant() && obj->type()->as_ObjectType()->constant_value()->is_null_object()) {\n+    if (obj->as_Constant() && obj->is_null_obj()) {\n@@ -848,2 +849,3 @@\n-void Canonicalizer::do_ProfileInvoke  (ProfileInvoke*   x) {}\n-void Canonicalizer::do_RuntimeCall    (RuntimeCall*     x) {}\n+void Canonicalizer::do_ProfileInvoke    (ProfileInvoke* x) {}\n+void Canonicalizer::do_ProfileACmpTypes (ProfileACmpTypes* x) {}\n+void Canonicalizer::do_RuntimeCall      (RuntimeCall* x) {}\n","filename":"src\/hotspot\/share\/c1\/c1_Canonicalizer.cpp","additions":9,"deletions":7,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -96,0 +96,1 @@\n+  virtual void do_ProfileACmpTypes(ProfileACmpTypes*  x);\n","filename":"src\/hotspot\/share\/c1\/c1_Canonicalizer.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -261,0 +261,77 @@\n+class LoadFlattenedArrayStub: public CodeStub {\n+ private:\n+  LIR_Opr          _array;\n+  LIR_Opr          _index;\n+  LIR_Opr          _result;\n+  LIR_Opr          _scratch_reg;\n+  CodeEmitInfo*    _info;\n+\n+ public:\n+  LoadFlattenedArrayStub(LIR_Opr array, LIR_Opr index, LIR_Opr result, CodeEmitInfo* info);\n+  virtual void emit_code(LIR_Assembler* e);\n+  virtual CodeEmitInfo* info() const             { return _info; }\n+  virtual void visit(LIR_OpVisitState* visitor) {\n+    visitor->do_slow_case(_info);\n+    visitor->do_input(_array);\n+    visitor->do_input(_index);\n+    visitor->do_output(_result);\n+    if (_scratch_reg != LIR_OprFact::illegalOpr) {\n+      visitor->do_temp(_scratch_reg);\n+    }\n+  }\n+\n+#ifndef PRODUCT\n+  virtual void print_name(outputStream* out) const { out->print(\"LoadFlattenedArrayStub\"); }\n+#endif \/\/ PRODUCT\n+};\n+\n+\n+class StoreFlattenedArrayStub: public CodeStub {\n+ private:\n+  LIR_Opr          _array;\n+  LIR_Opr          _index;\n+  LIR_Opr          _value;\n+  LIR_Opr          _scratch_reg;\n+  CodeEmitInfo*    _info;\n+\n+ public:\n+  StoreFlattenedArrayStub(LIR_Opr array, LIR_Opr index, LIR_Opr value, CodeEmitInfo* info);\n+  virtual void emit_code(LIR_Assembler* e);\n+  virtual CodeEmitInfo* info() const             { return _info; }\n+  virtual void visit(LIR_OpVisitState* visitor) {\n+    visitor->do_slow_case(_info);\n+    visitor->do_input(_array);\n+    visitor->do_input(_index);\n+    visitor->do_input(_value);\n+    if (_scratch_reg != LIR_OprFact::illegalOpr) {\n+      visitor->do_temp(_scratch_reg);\n+    }\n+  }\n+#ifndef PRODUCT\n+  virtual void print_name(outputStream* out) const { out->print(\"StoreFlattenedArrayStub\"); }\n+#endif \/\/ PRODUCT\n+};\n+\n+class SubstitutabilityCheckStub: public CodeStub {\n+ private:\n+  LIR_Opr          _left;\n+  LIR_Opr          _right;\n+  LIR_Opr          _scratch_reg;\n+  CodeEmitInfo*    _info;\n+ public:\n+  SubstitutabilityCheckStub(LIR_Opr left, LIR_Opr right, CodeEmitInfo* info);\n+  virtual void emit_code(LIR_Assembler* e);\n+  virtual CodeEmitInfo* info() const             { return _info; }\n+  virtual void visit(LIR_OpVisitState* visitor) {\n+    visitor->do_slow_case(_info);\n+    visitor->do_input(_left);\n+    visitor->do_input(_right);\n+    if (_scratch_reg != LIR_OprFact::illegalOpr) {\n+      visitor->do_temp(_scratch_reg);\n+    }\n+  }\n+#ifndef PRODUCT\n+  virtual void print_name(outputStream* out) const { out->print(\"SubstitutabilityCheckStub\"); }\n+#endif \/\/ PRODUCT\n+};\n+\n@@ -313,1 +390,1 @@\n-\n+  bool           _is_null_free;\n@@ -315,1 +392,1 @@\n-  NewObjectArrayStub(LIR_Opr klass_reg, LIR_Opr length, LIR_Opr result, CodeEmitInfo* info);\n+  NewObjectArrayStub(LIR_Opr klass_reg, LIR_Opr length, LIR_Opr result, CodeEmitInfo* info, bool is_null_free);\n@@ -350,0 +427,2 @@\n+  CodeStub* _throw_ie_stub;\n+  LIR_Opr _scratch_reg;\n@@ -352,1 +431,2 @@\n-  MonitorEnterStub(LIR_Opr obj_reg, LIR_Opr lock_reg, CodeEmitInfo* info)\n+  MonitorEnterStub(LIR_Opr obj_reg, LIR_Opr lock_reg, CodeEmitInfo* info,\n+                   CodeStub* throw_ie_stub = nullptr, LIR_Opr scratch_reg = LIR_OprFact::illegalOpr)\n@@ -355,0 +435,5 @@\n+    _scratch_reg = scratch_reg;\n+    _throw_ie_stub = throw_ie_stub;\n+    if (_throw_ie_stub != nullptr) {\n+      assert(_scratch_reg != LIR_OprFact::illegalOpr, \"must be\");\n+    }\n@@ -364,0 +449,3 @@\n+    if (_scratch_reg != LIR_OprFact::illegalOpr) {\n+      visitor->do_temp(_scratch_reg);\n+    }\n","filename":"src\/hotspot\/share\/c1\/c1_CodeStubs.hpp","additions":91,"deletions":3,"binary":false,"changes":94,"status":"modified"},{"patch":"@@ -588,0 +588,1 @@\n+, _compiled_entry_signature(method->get_Method())\n@@ -605,1 +606,0 @@\n-\n@@ -608,0 +608,6 @@\n+  {\n+    ResetNoHandleMark rnhm; \/\/ Huh? Required when doing class lookup of the Q-types\n+    \/\/ TODO 8284443 Should only be computed once\n+    _compiled_entry_signature.compute_calling_conventions(false);\n+  }\n+\n","filename":"src\/hotspot\/share\/c1\/c1_Compilation.cpp","additions":7,"deletions":1,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -187,1 +187,1 @@\n-  bool finalize_frame(int nof_slots);\n+  bool finalize_frame(int nof_slots, bool needs_stack_repair);\n@@ -213,0 +213,3 @@\n+  Address address_for_orig_pc_addr() const {\n+    return make_new_address(sp_offset_for_monitor_base(_num_monitors));\n+  }\n","filename":"src\/hotspot\/share\/c1\/c1_FrameMap.hpp","additions":4,"deletions":1,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -32,0 +32,2 @@\n+#include \"ci\/ciFlatArrayKlass.hpp\"\n+#include \"ci\/ciInlineKlass.hpp\"\n@@ -1050,1 +1052,9 @@\n-  ValueStack* state_before = copy_state_indexed_access();\n+  ValueStack* state_before = nullptr;\n+  int array_idx = state()->stack_size() - 2;\n+  if (type == T_OBJECT && state()->stack_at(array_idx)->maybe_flat_array()) {\n+    \/\/ Save the entire state and re-execute on deopt when accessing flat arrays\n+    state_before = copy_state_before();\n+    state_before->set_should_reexecute(true);\n+  } else {\n+    state_before = copy_state_indexed_access();\n+  }\n@@ -1062,1 +1072,56 @@\n-  push(as_ValueType(type), append(new LoadIndexed(array, index, length, type, state_before)));\n+\n+  bool need_membar = false;\n+  LoadIndexed* load_indexed = nullptr;\n+  Instruction* result = nullptr;\n+  if (array->is_loaded_flat_array()) {\n+    \/\/ TODO 8350865 This is currently dead code\n+    ciType* array_type = array->declared_type();\n+    ciInlineKlass* elem_klass = array_type->as_flat_array_klass()->element_klass()->as_inline_klass();\n+\n+    bool can_delay_access = false;\n+    ciBytecodeStream s(method());\n+    s.force_bci(bci());\n+    s.next();\n+    if (s.cur_bc() == Bytecodes::_getfield) {\n+      bool will_link;\n+      ciField* next_field = s.get_field(will_link);\n+      bool next_needs_patching = !next_field->holder()->is_initialized() ||\n+                                 !next_field->will_link(method(), Bytecodes::_getfield) ||\n+                                 PatchALot;\n+      can_delay_access = C1UseDelayedFlattenedFieldReads && !next_needs_patching;\n+    }\n+    if (can_delay_access) {\n+      \/\/ potentially optimizable array access, storing information for delayed decision\n+      LoadIndexed* li = new LoadIndexed(array, index, length, type, state_before);\n+      DelayedLoadIndexed* dli = new DelayedLoadIndexed(li, state_before);\n+      li->set_delayed(dli);\n+      set_pending_load_indexed(dli);\n+      return; \/\/ Nothing else to do for now\n+    } else {\n+      NewInstance* new_instance = new NewInstance(elem_klass, state_before, false, true);\n+      _memory->new_instance(new_instance);\n+      apush(append_split(new_instance));\n+      load_indexed = new LoadIndexed(array, index, length, type, state_before);\n+      load_indexed->set_vt(new_instance);\n+      \/\/ The LoadIndexed node will initialise this instance by copying from\n+      \/\/ the flat field.  Ensure these stores are visible before any\n+      \/\/ subsequent store that publishes this reference.\n+      need_membar = true;\n+    }\n+  } else {\n+    load_indexed = new LoadIndexed(array, index, length, type, state_before);\n+    if (profile_array_accesses() && is_reference_type(type)) {\n+      compilation()->set_would_profile(true);\n+      load_indexed->set_should_profile(true);\n+      load_indexed->set_profiled_method(method());\n+      load_indexed->set_profiled_bci(bci());\n+    }\n+  }\n+  result = append(load_indexed);\n+  if (need_membar) {\n+    append(new MemBar(lir_membar_storestore));\n+  }\n+  assert(!load_indexed->should_profile() || load_indexed == result, \"should not be optimized out\");\n+  if (!array->is_loaded_flat_array()) {\n+    push(as_ValueType(type), result);\n+  }\n@@ -1068,1 +1133,9 @@\n-  ValueStack* state_before = copy_state_indexed_access();\n+  ValueStack* state_before = nullptr;\n+  int array_idx = state()->stack_size() - 3;\n+  if (type == T_OBJECT && state()->stack_at(array_idx)->maybe_flat_array()) {\n+    \/\/ Save the entire state and re-execute on deopt when accessing flat arrays\n+    state_before = copy_state_before();\n+    state_before->set_should_reexecute(true);\n+  } else {\n+    state_before = copy_state_indexed_access();\n+  }\n@@ -1093,5 +1166,2 @@\n-  StoreIndexed* result = new StoreIndexed(array, index, length, type, value, state_before, check_boolean);\n-  append(result);\n-  _memory->store_value(value);\n-  if (type == T_OBJECT && is_profiling()) {\n-    \/\/ Note that we'd collect profile data in this method if we wanted it.\n+  StoreIndexed* store_indexed = new StoreIndexed(array, index, length, type, value, state_before, check_boolean);\n+  if (profile_array_accesses() && is_reference_type(type) && !array->is_loaded_flat_array()) {\n@@ -1100,6 +1170,3 @@\n-\n-    if (profile_checkcasts()) {\n-      result->set_profiled_method(method());\n-      result->set_profiled_bci(bci());\n-      result->set_should_profile(true);\n-    }\n+    store_indexed->set_should_profile(true);\n+    store_indexed->set_profiled_method(method());\n+    store_indexed->set_profiled_bci(bci());\n@@ -1107,0 +1174,3 @@\n+  Instruction* result = append(store_indexed);\n+  assert(!store_indexed->should_profile() || store_indexed == result, \"should not be optimized out\");\n+  _memory->store_value(value);\n@@ -1109,1 +1179,0 @@\n-\n@@ -1113,1 +1182,1 @@\n-      { state()->raw_pop();\n+      { Value w = state()->raw_pop();\n@@ -1117,2 +1186,2 @@\n-      { state()->raw_pop();\n-        state()->raw_pop();\n+      { Value w1 = state()->raw_pop();\n+        Value w2 = state()->raw_pop();\n@@ -1294,0 +1363,27 @@\n+\n+  bool subst_check = false;\n+  if (EnableValhalla && (stream()->cur_bc() == Bytecodes::_if_acmpeq || stream()->cur_bc() == Bytecodes::_if_acmpne)) {\n+    ValueType* left_vt = x->type();\n+    ValueType* right_vt = y->type();\n+    if (left_vt->is_object()) {\n+      assert(right_vt->is_object(), \"must be\");\n+      ciKlass* left_klass = x->as_loaded_klass_or_null();\n+      ciKlass* right_klass = y->as_loaded_klass_or_null();\n+\n+      if (left_klass == nullptr || right_klass == nullptr) {\n+        \/\/ The klass is still unloaded, or came from a Phi node. Go slow case;\n+        subst_check = true;\n+      } else if (left_klass->can_be_inline_klass() || right_klass->can_be_inline_klass()) {\n+        \/\/ Either operand may be a value object, but we're not sure. Go slow case;\n+        subst_check = true;\n+      } else {\n+        \/\/ No need to do substitutability check\n+      }\n+    }\n+  }\n+  if ((stream()->cur_bc() == Bytecodes::_if_acmpeq || stream()->cur_bc() == Bytecodes::_if_acmpne) &&\n+      is_profiling() && profile_branches()) {\n+    compilation()->set_would_profile(true);\n+    append(new ProfileACmpTypes(method(), bci(), x, y));\n+  }\n+\n@@ -1296,1 +1392,1 @@\n-  Instruction *i = append(new If(x, cond, false, y, tsux, fsux, (is_bb || compilation()->is_optimistic()) ? state_before : nullptr, is_bb));\n+  Instruction *i = append(new If(x, cond, false, y, tsux, fsux, (is_bb || compilation()->is_optimistic() || subst_check) ? state_before : nullptr, is_bb, subst_check));\n@@ -1551,1 +1647,1 @@\n-  if (method()->name() == ciSymbols::object_initializer_name() &&\n+  if (method()->is_object_constructor() &&\n@@ -1702,0 +1798,25 @@\n+void GraphBuilder::copy_inline_content(ciInlineKlass* vk, Value src, int src_off, Value dest, int dest_off, ValueStack* state_before, ciField* enclosing_field) {\n+  for (int i = 0; i < vk->nof_declared_nonstatic_fields(); i++) {\n+    ciField* field = vk->declared_nonstatic_field_at(i);\n+    int offset = field->offset_in_bytes() - vk->payload_offset();\n+    if (field->is_flat()) {\n+      bool needs_atomic_access = !field->is_null_free() || field->is_volatile();\n+      assert(!needs_atomic_access, \"Atomic access in non-atomic container\");\n+      copy_inline_content(field->type()->as_inline_klass(), src, src_off + offset, dest, dest_off + offset, state_before, enclosing_field);\n+      if (!field->is_null_free()) {\n+        \/\/ Nullable, copy the null marker using Unsafe because null markers are no real fields\n+        int null_marker_offset = field->null_marker_offset() - vk->payload_offset();\n+        Value offset = append(new Constant(new LongConstant(src_off + null_marker_offset)));\n+        Value nm = append(new UnsafeGet(T_BOOLEAN, src, offset, false));\n+        offset = append(new Constant(new LongConstant(dest_off + null_marker_offset)));\n+        append(new UnsafePut(T_BOOLEAN, dest, offset, nm, false));\n+      }\n+    } else {\n+      Value value = append(new LoadField(src, src_off + offset, field, false, state_before, false));\n+      StoreField* store = new StoreField(dest, dest_off + offset, field, value, false, state_before, false);\n+      store->set_enclosing_field(enclosing_field);\n+      append(store);\n+    }\n+  }\n+}\n+\n@@ -1708,0 +1829,1 @@\n+\n@@ -1711,1 +1833,1 @@\n-                              PatchALot;\n+                              (!field->is_flat() && PatchALot);\n@@ -1743,1 +1865,1 @@\n-  const int offset = !needs_patching ? field->offset_in_bytes() : -1;\n+  int offset = !needs_patching ? field->offset_in_bytes() : -1;\n@@ -1760,2 +1882,3 @@\n-        push(type, append(new LoadField(append(obj), offset, field, true,\n-                                        state_before, needs_patching)));\n+        LoadField* load_field = new LoadField(append(obj), offset, field, true,\n+                                        state_before, needs_patching);\n+        push(type, append(load_field));\n@@ -1770,1 +1893,1 @@\n-      if (field->type()->basic_type() == T_BOOLEAN) {\n+      if (field_type == T_BOOLEAN) {\n@@ -1774,0 +1897,7 @@\n+      if (field->is_null_free()) {\n+        null_check(val);\n+      }\n+      if (field->is_null_free() && field->type()->is_loaded() && field->type()->as_inline_klass()->is_empty() && (!method()->is_class_initializer() || field->is_flat())) {\n+        \/\/ Storing to a field of an empty, null-free inline type that is already initialized. Ignore.\n+        break;\n+      }\n@@ -1780,14 +1910,21 @@\n-      obj = apop();\n-      ObjectType* obj_type = obj->type()->as_ObjectType();\n-      if (field->is_constant() && obj_type->is_constant() && !PatchALot) {\n-        ciObject* const_oop = obj_type->constant_value();\n-        if (!const_oop->is_null_object() && const_oop->is_loaded()) {\n-          ciConstant field_value = field->constant_value_of(const_oop);\n-          if (field_value.is_valid()) {\n-            constant = make_constant(field_value, field);\n-            \/\/ For CallSite objects add a dependency for invalidation of the optimization.\n-            if (field->is_call_site_target()) {\n-              ciCallSite* call_site = const_oop->as_call_site();\n-              if (!call_site->is_fully_initialized_constant_call_site()) {\n-                ciMethodHandle* target = field_value.as_object()->as_method_handle();\n-                dependency_recorder()->assert_call_site_target_value(call_site, target);\n+      if (state_before == nullptr && field->is_flat()) {\n+        \/\/ Save the entire state and re-execute on deopt when accessing flat fields\n+        assert(Interpreter::bytecode_should_reexecute(code), \"should reexecute\");\n+        state_before = copy_state_before();\n+      }\n+      if (!has_pending_field_access() && !has_pending_load_indexed()) {\n+        obj = apop();\n+        ObjectType* obj_type = obj->type()->as_ObjectType();\n+        if (field->is_constant() && !field->is_flat() && obj_type->is_constant() && !PatchALot) {\n+          ciObject* const_oop = obj_type->constant_value();\n+          if (!const_oop->is_null_object() && const_oop->is_loaded()) {\n+            ciConstant field_value = field->constant_value_of(const_oop);\n+            if (field_value.is_valid()) {\n+              constant = make_constant(field_value, field);\n+              \/\/ For CallSite objects add a dependency for invalidation of the optimization.\n+              if (field->is_call_site_target()) {\n+                ciCallSite* call_site = const_oop->as_call_site();\n+                if (!call_site->is_fully_initialized_constant_call_site()) {\n+                  ciMethodHandle* target = field_value.as_object()->as_method_handle();\n+                  dependency_recorder()->assert_call_site_target_value(call_site, target);\n+                }\n@@ -1805,19 +1942,15 @@\n-        LoadField* load = new LoadField(obj, offset, field, false, state_before, needs_patching);\n-        Value replacement = !needs_patching ? _memory->load(load) : load;\n-        if (replacement != load) {\n-          assert(replacement->is_linked() || !replacement->can_be_linked(), \"should already by linked\");\n-          \/\/ Writing an (integer) value to a boolean, byte, char or short field includes an implicit narrowing\n-          \/\/ conversion. Emit an explicit conversion here to get the correct field value after the write.\n-          BasicType bt = field->type()->basic_type();\n-          switch (bt) {\n-          case T_BOOLEAN:\n-          case T_BYTE:\n-            replacement = append(new Convert(Bytecodes::_i2b, replacement, as_ValueType(bt)));\n-            break;\n-          case T_CHAR:\n-            replacement = append(new Convert(Bytecodes::_i2c, replacement, as_ValueType(bt)));\n-            break;\n-          case T_SHORT:\n-            replacement = append(new Convert(Bytecodes::_i2s, replacement, as_ValueType(bt)));\n-            break;\n-          default:\n+        if (!field->is_flat()) {\n+          if (has_pending_field_access()) {\n+            assert(!needs_patching, \"Can't patch delayed field access\");\n+            obj = pending_field_access()->obj();\n+            offset += pending_field_access()->offset() - field->holder()->as_inline_klass()->payload_offset();\n+            field = pending_field_access()->holder()->get_field_by_offset(offset, false);\n+            assert(field != nullptr, \"field not found\");\n+            set_pending_field_access(nullptr);\n+          } else if (has_pending_load_indexed()) {\n+            assert(!needs_patching, \"Can't patch delayed field access\");\n+            pending_load_indexed()->update(field, offset - field->holder()->as_inline_klass()->payload_offset());\n+            LoadIndexed* li = pending_load_indexed()->load_instr();\n+            li->set_type(type);\n+            push(type, append(li));\n+            set_pending_load_indexed(nullptr);\n@@ -1826,1 +1959,24 @@\n-          push(type, replacement);\n+          LoadField* load = new LoadField(obj, offset, field, false, state_before, needs_patching);\n+          Value replacement = !needs_patching ? _memory->load(load) : load;\n+          if (replacement != load) {\n+            assert(replacement->is_linked() || !replacement->can_be_linked(), \"should already by linked\");\n+            \/\/ Writing an (integer) value to a boolean, byte, char or short field includes an implicit narrowing\n+            \/\/ conversion. Emit an explicit conversion here to get the correct field value after the write.\n+            switch (field_type) {\n+            case T_BOOLEAN:\n+            case T_BYTE:\n+              replacement = append(new Convert(Bytecodes::_i2b, replacement, type));\n+              break;\n+            case T_CHAR:\n+              replacement = append(new Convert(Bytecodes::_i2c, replacement, type));\n+              break;\n+            case T_SHORT:\n+              replacement = append(new Convert(Bytecodes::_i2s, replacement, type));\n+              break;\n+            default:\n+              break;\n+            }\n+            push(type, replacement);\n+          } else {\n+            push(type, append(load));\n+          }\n@@ -1828,1 +1984,78 @@\n-          push(type, append(load));\n+          \/\/ Flat field\n+          assert(!needs_patching, \"Can't patch flat inline type field access\");\n+          ciInlineKlass* inline_klass = field->type()->as_inline_klass();\n+          bool is_naturally_atomic = inline_klass->nof_declared_nonstatic_fields() <= 1;\n+          bool needs_atomic_access = !field->is_null_free() || (field->is_volatile() && !is_naturally_atomic);\n+          if (needs_atomic_access) {\n+            assert(!has_pending_field_access(), \"Pending field accesses are not supported\");\n+            LoadField* load = new LoadField(obj, offset, field, false, state_before, needs_patching);\n+            push(type, append(load));\n+          } else {\n+            assert(field->is_null_free(), \"must be null-free\");\n+            \/\/ Look at the next bytecode to check if we can delay the field access\n+            bool can_delay_access = false;\n+            ciBytecodeStream s(method());\n+            s.force_bci(bci());\n+            s.next();\n+            if (s.cur_bc() == Bytecodes::_getfield && !needs_patching) {\n+              ciField* next_field = s.get_field(will_link);\n+              bool next_needs_patching = !next_field->holder()->is_loaded() ||\n+                                         !next_field->will_link(method(), Bytecodes::_getfield) ||\n+                                         PatchALot;\n+              \/\/ We can't update the offset for atomic accesses\n+              bool next_needs_atomic_access = !next_field->is_null_free() || next_field->is_volatile();\n+              can_delay_access = C1UseDelayedFlattenedFieldReads && !next_needs_patching && !next_needs_atomic_access;\n+            }\n+            if (can_delay_access) {\n+              if (has_pending_load_indexed()) {\n+                pending_load_indexed()->update(field, offset - field->holder()->as_inline_klass()->payload_offset());\n+              } else if (has_pending_field_access()) {\n+                pending_field_access()->inc_offset(offset - field->holder()->as_inline_klass()->payload_offset());\n+              } else {\n+                null_check(obj);\n+                DelayedFieldAccess* dfa = new DelayedFieldAccess(obj, field->holder(), field->offset_in_bytes(), state_before);\n+                set_pending_field_access(dfa);\n+              }\n+            } else {\n+              scope()->set_wrote_final();\n+              scope()->set_wrote_fields();\n+              bool need_membar = false;\n+              if (has_pending_load_indexed()) {\n+                assert(!needs_patching, \"Can't patch delayed field access\");\n+                pending_load_indexed()->update(field, offset - field->holder()->as_inline_klass()->payload_offset());\n+                NewInstance* vt = new NewInstance(inline_klass, pending_load_indexed()->state_before(), false, true);\n+                _memory->new_instance(vt);\n+                pending_load_indexed()->load_instr()->set_vt(vt);\n+                apush(append_split(vt));\n+                append(pending_load_indexed()->load_instr());\n+                set_pending_load_indexed(nullptr);\n+                need_membar = true;\n+              } else {\n+                if (has_pending_field_access()) {\n+                  state_before = pending_field_access()->state_before();\n+                }\n+                NewInstance* new_instance = new NewInstance(inline_klass, state_before, false, true);\n+                _memory->new_instance(new_instance);\n+                apush(append_split(new_instance));\n+                if (has_pending_field_access()) {\n+                  copy_inline_content(inline_klass, pending_field_access()->obj(),\n+                                      pending_field_access()->offset() + field->offset_in_bytes() - field->holder()->as_inline_klass()->payload_offset(),\n+                                      new_instance, inline_klass->payload_offset(), state_before);\n+                  set_pending_field_access(nullptr);\n+                } else {\n+                  if (field->type()->as_instance_klass()->is_initialized() && field->type()->as_inline_klass()->is_empty()) {\n+                    \/\/ Needs an explicit null check because below code does not perform any actual load if there are no fields\n+                    null_check(obj);\n+                  }\n+                  copy_inline_content(inline_klass, obj, field->offset_in_bytes(), new_instance, inline_klass->payload_offset(), state_before);\n+                }\n+                need_membar = true;\n+              }\n+              if (need_membar) {\n+                \/\/ If we allocated a new instance ensure the stores to copy the\n+                \/\/ field contents are visible before any subsequent store that\n+                \/\/ publishes this reference.\n+                append(new MemBar(lir_membar_storestore));\n+              }\n+            }\n+          }\n@@ -1839,1 +2072,1 @@\n-      if (field->type()->basic_type() == T_BOOLEAN) {\n+      if (field_type == T_BOOLEAN) {\n@@ -1843,4 +2076,29 @@\n-      StoreField* store = new StoreField(obj, offset, field, val, false, state_before, needs_patching);\n-      if (!needs_patching) store = _memory->store(store);\n-      if (store != nullptr) {\n-        append(store);\n+\n+      if (field->is_null_free() && field->type()->is_loaded() && field->type()->as_inline_klass()->is_empty() && (!method()->is_object_constructor() || field->is_flat())) {\n+        \/\/ Storing to a field of an empty, null-free inline type that is already initialized. Ignore.\n+        null_check(obj);\n+        null_check(val);\n+      } else if (!field->is_flat()) {\n+        if (field->is_null_free()) {\n+          null_check(val);\n+        }\n+        StoreField* store = new StoreField(obj, offset, field, val, false, state_before, needs_patching);\n+        if (!needs_patching) store = _memory->store(store);\n+        if (store != nullptr) {\n+          append(store);\n+        }\n+      } else {\n+        \/\/ Flat field\n+        assert(!needs_patching, \"Can't patch flat inline type field access\");\n+        ciInlineKlass* inline_klass = field->type()->as_inline_klass();\n+        bool is_naturally_atomic = inline_klass->nof_declared_nonstatic_fields() <= 1;\n+        bool needs_atomic_access = !field->is_null_free() || (field->is_volatile() && !is_naturally_atomic);\n+        if (needs_atomic_access) {\n+          if (field->is_null_free()) {\n+            null_check(val);\n+          }\n+          append(new StoreField(obj, offset, field, val, false, state_before, needs_patching));\n+        } else {\n+          assert(field->is_null_free(), \"must be null-free\");\n+          copy_inline_content(inline_klass, val, inline_klass->payload_offset(), obj, offset, state_before, field);\n+        }\n@@ -1856,1 +2114,0 @@\n-\n@@ -1973,1 +2230,1 @@\n-    } else if (bc_raw == Bytecodes::_invokespecial && !target->is_object_initializer() && calling_klass->is_interface()) {\n+    } else if (bc_raw == Bytecodes::_invokespecial && !target->is_object_constructor() && calling_klass->is_interface()) {\n@@ -2229,1 +2486,1 @@\n-  NewInstance* new_instance = new NewInstance(klass->as_instance_klass(), state_before, stream()->is_unresolved_klass());\n+  NewInstance* new_instance = new NewInstance(klass->as_instance_klass(), state_before, stream()->is_unresolved_klass(), false);\n@@ -2234,1 +2491,0 @@\n-\n@@ -2307,0 +2563,19 @@\n+  bool maybe_inlinetype = false;\n+  if (bci == InvocationEntryBci) {\n+    \/\/ Called by GraphBuilder::inline_sync_entry.\n+#ifdef ASSERT\n+    ciType* obj_type = x->declared_type();\n+    assert(obj_type == nullptr || !obj_type->is_inlinetype(), \"inline types cannot have synchronized methods\");\n+#endif\n+  } else {\n+    \/\/ We are compiling a monitorenter bytecode\n+    if (EnableValhalla) {\n+      ciType* obj_type = x->declared_type();\n+      if (obj_type == nullptr || obj_type->as_klass()->can_be_inline_klass()) {\n+        \/\/ If we're (possibly) locking on an inline type, check for markWord::always_locked_pattern\n+        \/\/ and throw IMSE. (obj_type is null for Phi nodes, so let's just be conservative).\n+        maybe_inlinetype = true;\n+      }\n+    }\n+  }\n+\n@@ -2309,1 +2584,1 @@\n-  append_with_bci(new MonitorEnter(x, state()->lock(x), state_before), bci);\n+  append_with_bci(new MonitorEnter(x, state()->lock(x), state_before, maybe_inlinetype), bci);\n@@ -2434,0 +2709,1 @@\n+    if (value->is_null_free()) return;\n@@ -2459,1 +2735,3 @@\n-    assert(cur_bci == SynchronizationEntryBCI || cur_bci == cur_scope_data->stream()->cur_bci(), \"invalid bci\");\n+    assert(cur_bci == SynchronizationEntryBCI || cur_bci == cur_scope_data->stream()->cur_bci()\n+           || has_pending_field_access() || has_pending_load_indexed(), \"invalid bci\");\n+\n@@ -3241,1 +3519,2 @@\n-    state->store_local(idx, new Local(method()->holder(), objectType, idx, true));\n+    state->store_local(idx, new Local(method()->holder(), objectType, idx,\n+             \/*receiver*\/ true, \/*null_free*\/ method()->holder()->is_flat_array_klass()));\n@@ -3253,1 +3532,1 @@\n-    state->store_local(idx, new Local(type, vt, idx, false));\n+    state->store_local(idx, new Local(type, vt, idx, false, false));\n@@ -3273,0 +3552,2 @@\n+  , _pending_field_access(nullptr)\n+  , _pending_load_indexed(nullptr)\n","filename":"src\/hotspot\/share\/c1\/c1_GraphBuilder.cpp","additions":354,"deletions":73,"binary":false,"changes":427,"status":"modified"},{"patch":"@@ -38,0 +38,18 @@\n+class DelayedFieldAccess : public CompilationResourceObj {\n+private:\n+  Value            _obj;\n+  ciInstanceKlass* _holder;\n+  int              _offset;\n+  ValueStack*      _state_before;\n+\n+public:\n+  DelayedFieldAccess(Value obj, ciInstanceKlass* holder, int offset, ValueStack* state_before)\n+  : _obj(obj), _holder(holder) , _offset(offset), _state_before(state_before) { }\n+\n+  Value obj() const { return _obj; }\n+  ciInstanceKlass* holder() const { return _holder; }\n+  int offset() const { return _offset; }\n+  void inc_offset(int offset) { _offset += offset; }\n+  ValueStack* state_before() const { return _state_before; }\n+};\n+\n@@ -195,0 +213,4 @@\n+  \/\/ support for optimization of accesses to flat fields and flat arrays\n+  DelayedFieldAccess* _pending_field_access;\n+  DelayedLoadIndexed* _pending_load_indexed;\n+\n@@ -212,0 +234,6 @@\n+  bool              has_pending_field_access()   { return _pending_field_access != nullptr; }\n+  DelayedFieldAccess* pending_field_access()     { return _pending_field_access; }\n+  void              set_pending_field_access(DelayedFieldAccess* delayed) { _pending_field_access = delayed; }\n+  bool              has_pending_load_indexed()   { return _pending_load_indexed != nullptr; }\n+  DelayedLoadIndexed* pending_load_indexed()     { return _pending_load_indexed; }\n+  void              set_pending_load_indexed(DelayedLoadIndexed* delayed) { _pending_load_indexed = delayed; }\n@@ -269,0 +297,3 @@\n+  \/\/ inline types\n+  void copy_inline_content(ciInlineKlass* vk, Value src, int src_off, Value dest, int dest_off, ValueStack* state_before, ciField* encloding_field = nullptr);\n+\n@@ -398,0 +429,1 @@\n+  bool profile_array_accesses(){ return _compilation->profile_array_accesses();}\n","filename":"src\/hotspot\/share\/c1\/c1_GraphBuilder.hpp","additions":32,"deletions":0,"binary":false,"changes":32,"status":"modified"},{"patch":"@@ -174,0 +174,3 @@\n+  if (_should_reexecute) {\n+    return true;\n+  }\n@@ -183,1 +186,0 @@\n-\n@@ -217,1 +219,1 @@\n-void CodeEmitInfo::record_debug_info(DebugInformationRecorder* recorder, int pc_offset) {\n+void CodeEmitInfo::record_debug_info(DebugInformationRecorder* recorder, int pc_offset, bool maybe_return_as_fields) {\n@@ -221,1 +223,1 @@\n-  _scope_debug_info->record_debug_info(recorder, pc_offset, reexecute, _is_method_handle_invoke);\n+  _scope_debug_info->record_debug_info(recorder, pc_offset, reexecute, _is_method_handle_invoke, maybe_return_as_fields);\n","filename":"src\/hotspot\/share\/c1\/c1_IR.cpp","additions":5,"deletions":3,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -29,0 +29,2 @@\n+#include \"ci\/ciFlatArrayKlass.hpp\"\n+#include \"ci\/ciInlineKlass.hpp\"\n@@ -108,1 +110,1 @@\n-  ciType* t =  declared_type();\n+  ciType* t = declared_type();\n@@ -115,0 +117,60 @@\n+ciKlass* Instruction::as_loaded_klass_or_null() const {\n+  ciType* type = declared_type();\n+  if (type != nullptr && type->is_klass()) {\n+    ciKlass* klass = type->as_klass();\n+    if (klass->is_loaded()) {\n+      return klass;\n+    }\n+  }\n+  return nullptr;\n+}\n+\n+bool Instruction::is_loaded_flat_array() const {\n+  if (UseArrayFlattening) {\n+    ciType* type = declared_type();\n+    return type != nullptr && type->is_flat_array_klass();\n+  }\n+  return false;\n+}\n+\n+bool Instruction::maybe_flat_array() {\n+  if (UseArrayFlattening) {\n+    ciType* type = declared_type();\n+    if (type != nullptr) {\n+      if (type->is_obj_array_klass()) {\n+        \/\/ Due to array covariance, the runtime type might be a flat array.\n+        ciKlass* element_klass = type->as_obj_array_klass()->element_klass();\n+        if (element_klass->can_be_inline_klass() && (!element_klass->is_inlinetype() || element_klass->as_inline_klass()->flat_in_array())) {\n+          return true;\n+        }\n+      } else if (type->is_flat_array_klass()) {\n+        return true;\n+      } else if (type->is_klass() && type->as_klass()->is_java_lang_Object()) {\n+        \/\/ This can happen as a parameter to System.arraycopy()\n+        return true;\n+      }\n+    } else {\n+      \/\/ Type info gets lost during Phi merging (Phi, IfOp, etc), but we might be storing into a\n+      \/\/ flat array, so we should do a runtime check.\n+      return true;\n+    }\n+  }\n+  return false;\n+}\n+\n+bool Instruction::maybe_null_free_array() {\n+  ciType* type = declared_type();\n+  if (type != nullptr) {\n+    if (type->is_obj_array_klass()) {\n+      \/\/ Due to array covariance, the runtime type might be a null-free array.\n+      if (type->as_obj_array_klass()->can_be_inline_array_klass()) {\n+        return true;\n+      }\n+    }\n+  } else {\n+    \/\/ Type info gets lost during Phi merging (Phi, IfOp, etc), but we might be storing into a\n+    \/\/ null-free array, so we should do a runtime check.\n+    return true;\n+  }\n+  return false;\n+}\n@@ -175,1 +237,1 @@\n-  if (array_type != nullptr) {\n+  if (delayed() == nullptr && array_type != nullptr) {\n@@ -189,1 +251,3 @@\n-\n+  if (delayed() != nullptr) {\n+    return delayed()->field()->type();\n+  }\n@@ -200,0 +264,14 @@\n+bool StoreIndexed::is_exact_flat_array_store() const {\n+  if (array()->is_loaded_flat_array() && value()->as_Constant() == nullptr && value()->declared_type() != nullptr) {\n+    ciKlass* element_klass = array()->declared_type()->as_flat_array_klass()->element_klass();\n+    ciKlass* actual_klass = value()->declared_type()->as_klass();\n+\n+    \/\/ The following check can fail with inlining:\n+    \/\/     void test45_inline(Object[] oa, Object o, int index) { oa[index] = o; }\n+    \/\/     void test45(MyValue1[] va, int index, MyValue2 v) { test45_inline(va, v, index); }\n+    if (element_klass == actual_klass) {\n+      return true;\n+    }\n+  }\n+  return false;\n+}\n@@ -211,1 +289,5 @@\n-  return ciObjArrayKlass::make(klass());\n+  return ciArrayKlass::make(klass());\n+}\n+\n+ciType* NewMultiArray::exact_type() const {\n+  return _klass;\n@@ -321,0 +403,26 @@\n+StoreField::StoreField(Value obj, int offset, ciField* field, Value value, bool is_static,\n+                       ValueStack* state_before, bool needs_patching)\n+  : AccessField(obj, offset, field, is_static, state_before, needs_patching)\n+  , _value(value)\n+  , _enclosing_field(nullptr)\n+{\n+#ifdef ASSERT\n+  AssertValues assert_value;\n+  values_do(&assert_value);\n+#endif\n+  pin();\n+}\n+\n+StoreIndexed::StoreIndexed(Value array, Value index, Value length, BasicType elt_type, Value value,\n+                           ValueStack* state_before, bool check_boolean, bool mismatched)\n+  : AccessIndexed(array, index, length, elt_type, state_before, mismatched)\n+  , _value(value), _check_boolean(check_boolean)\n+{\n+#ifdef ASSERT\n+  AssertValues assert_value;\n+  values_do(&assert_value);\n+#endif\n+  pin();\n+}\n+\n+\n@@ -347,1 +455,2 @@\n-    ValueType* t = argument_at(i)->type();\n+    Value v = argument_at(i);\n+    ValueType* t = v->type();\n@@ -992,0 +1101,1 @@\n+\n","filename":"src\/hotspot\/share\/c1\/c1_Instruction.cpp","additions":115,"deletions":5,"binary":false,"changes":120,"status":"modified"},{"patch":"@@ -77,0 +77,1 @@\n+class     Deoptimize;\n@@ -100,0 +101,1 @@\n+class   ProfileACmpTypes;\n@@ -194,0 +196,1 @@\n+  virtual void do_ProfileACmpTypes(ProfileACmpTypes*  x) = 0;\n@@ -210,3 +213,4 @@\n-#define HASH2(x1, x2        )                    ((HASH1(x1        ) << 7) ^ HASH1(x2))\n-#define HASH3(x1, x2, x3    )                    ((HASH2(x1, x2    ) << 7) ^ HASH1(x3))\n-#define HASH4(x1, x2, x3, x4)                    ((HASH3(x1, x2, x3) << 7) ^ HASH1(x4))\n+#define HASH2(x1, x2        )                    ((HASH1(x1            ) << 7) ^ HASH1(x2))\n+#define HASH3(x1, x2, x3    )                    ((HASH2(x1, x2        ) << 7) ^ HASH1(x3))\n+#define HASH4(x1, x2, x3, x4)                    ((HASH3(x1, x2, x3    ) << 7) ^ HASH1(x4))\n+#define HASH5(x1, x2, x3, x4, x5)                ((HASH4(x1, x2, x3, x4) << 7) ^ HASH1(x5))\n@@ -271,0 +275,15 @@\n+#define HASHING4(class_name, enabled, f1, f2, f3, f4) \\\n+  virtual intx hash() const {                         \\\n+    return (enabled) ? HASH5(name(), f1, f2, f3, f4) : 0; \\\n+  }                                                   \\\n+  virtual bool is_equal(Value v) const {              \\\n+    if (!(enabled)  ) return false;                   \\\n+    class_name* _v = v->as_##class_name();            \\\n+    if (_v == nullptr  ) return false;                   \\\n+    if (f1 != _v->f1) return false;                   \\\n+    if (f2 != _v->f2) return false;                   \\\n+    if (f3 != _v->f3) return false;                   \\\n+    if (f4 != _v->f4) return false;                   \\\n+    return true;                                      \\\n+  }                                                   \\\n+\n@@ -293,0 +312,1 @@\n+  friend class GraphBuilder;\n@@ -345,0 +365,1 @@\n+    NeverNullFlag,          \/\/ For \"Q\" signatures\n@@ -435,0 +456,2 @@\n+  void set_null_free(bool f)                     { set_flag(NeverNullFlag, f); }\n+  bool is_null_free() const                      { return check_flag(NeverNullFlag); }\n@@ -445,0 +468,1 @@\n+  ciKlass* as_loaded_klass_or_null() const;\n@@ -489,0 +513,4 @@\n+  bool is_loaded_flat_array() const;\n+  bool maybe_flat_array();\n+  bool maybe_null_free_array();\n+\n@@ -687,1 +715,1 @@\n-  Local(ciType* declared, ValueType* type, int index, bool receiver)\n+  Local(ciType* declared, ValueType* type, int index, bool receiver, bool null_free)\n@@ -693,0 +721,1 @@\n+    set_null_free(null_free);\n@@ -816,1 +845,3 @@\n-  {}\n+  {\n+    set_null_free(field->is_null_free());\n+  }\n@@ -828,0 +859,1 @@\n+  ciField* _enclosing_field;   \/\/ enclosing field (the flat one) for nested fields\n@@ -832,7 +864,1 @@\n-             ValueStack* state_before, bool needs_patching)\n-  : AccessField(obj, offset, field, is_static, state_before, needs_patching)\n-  , _value(value)\n-  {\n-    ASSERT_VALUES\n-    pin();\n-  }\n+             ValueStack* state_before, bool needs_patching);\n@@ -842,0 +868,2 @@\n+  ciField* enclosing_field() const               { return _enclosing_field; }\n+  void set_enclosing_field(ciField* field)       { _enclosing_field = field; }\n@@ -899,0 +927,2 @@\n+  ciMethod* _profiled_method;\n+  int       _profiled_bci;\n@@ -908,0 +938,1 @@\n+  , _profiled_method(nullptr), _profiled_bci(0)\n@@ -923,0 +954,9 @@\n+  \/\/ Helpers for MethodData* profiling\n+  void set_should_profile(bool value)                { set_flag(ProfileMDOFlag, value); }\n+  void set_profiled_method(ciMethod* method)         { _profiled_method = method;   }\n+  void set_profiled_bci(int bci)                     { _profiled_bci = bci;         }\n+  bool      should_profile() const                   { return check_flag(ProfileMDOFlag); }\n+  ciMethod* profiled_method() const                  { return _profiled_method;     }\n+  int       profiled_bci() const                     { return _profiled_bci;        }\n+\n+\n@@ -927,0 +967,1 @@\n+class DelayedLoadIndexed;\n@@ -931,0 +972,2 @@\n+  NewInstance* _vt;\n+  DelayedLoadIndexed* _delayed;\n@@ -936,1 +979,1 @@\n-  , _explicit_null_check(nullptr) {}\n+  , _explicit_null_check(nullptr), _vt(nullptr), _delayed(nullptr) {}\n@@ -948,0 +991,6 @@\n+  NewInstance* vt() const { return _vt; }\n+  void set_vt(NewInstance* vt) { _vt = vt; }\n+\n+  DelayedLoadIndexed* delayed() const { return _delayed; }\n+  void set_delayed(DelayedLoadIndexed* delayed) { _delayed = delayed; }\n+\n@@ -949,1 +998,1 @@\n-  HASHING3(LoadIndexed, true, elt_type(), array()->subst(), index()->subst())\n+  HASHING4(LoadIndexed, delayed() == nullptr && !should_profile(), elt_type(), array()->subst(), index()->subst(), vt())\n@@ -952,0 +1001,23 @@\n+class DelayedLoadIndexed : public CompilationResourceObj {\n+private:\n+  LoadIndexed* _load_instr;\n+  ValueStack* _state_before;\n+  ciField* _field;\n+  int _offset;\n+ public:\n+  DelayedLoadIndexed(LoadIndexed* load, ValueStack* state_before)\n+  : _load_instr(load)\n+  , _state_before(state_before)\n+  , _field(nullptr)\n+  , _offset(0) { }\n+\n+  void update(ciField* field, int offset) {\n+    _field = field;\n+    _offset += offset;\n+  }\n+\n+  LoadIndexed* load_instr() const { return _load_instr; }\n+  ValueStack* state_before() const { return _state_before; }\n+  ciField* field() const { return _field; }\n+  int offset() const { return _offset; }\n+};\n@@ -957,2 +1029,0 @@\n-  ciMethod* _profiled_method;\n-  int       _profiled_bci;\n@@ -964,7 +1034,1 @@\n-               bool check_boolean, bool mismatched = false)\n-  : AccessIndexed(array, index, length, elt_type, state_before, mismatched)\n-  , _value(value), _profiled_method(nullptr), _profiled_bci(0), _check_boolean(check_boolean)\n-  {\n-    ASSERT_VALUES\n-    pin();\n-  }\n+               bool check_boolean, bool mismatched = false);\n@@ -975,7 +1039,3 @@\n-  \/\/ Helpers for MethodData* profiling\n-  void set_should_profile(bool value)                { set_flag(ProfileMDOFlag, value); }\n-  void set_profiled_method(ciMethod* method)         { _profiled_method = method;   }\n-  void set_profiled_bci(int bci)                     { _profiled_bci = bci;         }\n-  bool      should_profile() const                   { return check_flag(ProfileMDOFlag); }\n-  ciMethod* profiled_method() const                  { return _profiled_method;     }\n-  int       profiled_bci() const                     { return _profiled_bci;        }\n+\n+  \/\/ Flattened array support\n+  bool is_exact_flat_array_store() const;\n@@ -1092,0 +1152,1 @@\n+  bool _substitutability_check;\n@@ -1095,1 +1156,1 @@\n-  IfOp(Value x, Condition cond, Value y, Value tval, Value fval)\n+  IfOp(Value x, Condition cond, Value y, Value tval, Value fval, ValueStack* state_before, bool substitutability_check)\n@@ -1099,0 +1160,1 @@\n+  , _substitutability_check(substitutability_check)\n@@ -1102,0 +1164,1 @@\n+    set_state_before(state_before);\n@@ -1110,1 +1173,1 @@\n-\n+  bool substitutability_check() const             { return _substitutability_check; }\n@@ -1267,0 +1330,1 @@\n+  bool _needs_state_before;\n@@ -1270,1 +1334,1 @@\n-  NewInstance(ciInstanceKlass* klass, ValueStack* state_before, bool is_unresolved)\n+  NewInstance(ciInstanceKlass* klass, ValueStack* state_before, bool is_unresolved, bool needs_state_before)\n@@ -1272,1 +1336,1 @@\n-  , _klass(klass), _is_unresolved(is_unresolved)\n+  , _klass(klass), _is_unresolved(is_unresolved), _needs_state_before(needs_state_before)\n@@ -1278,0 +1342,1 @@\n+  bool needs_state_before() const                { return _needs_state_before; }\n@@ -1287,1 +1352,0 @@\n-\n@@ -1341,1 +1405,2 @@\n-  NewObjectArray(ciKlass* klass, Value length, ValueStack* state_before) : NewArray(length, state_before), _klass(klass) {}\n+  NewObjectArray(ciKlass* klass, Value length, ValueStack* state_before)\n+  : NewArray(length, state_before), _klass(klass) { }\n@@ -1376,0 +1441,2 @@\n+\n+  ciType* exact_type() const;\n@@ -1423,1 +1490,1 @@\n-  : TypeCheck(klass, obj, objectType, state_before) {}\n+  : TypeCheck(klass, obj, objectType, state_before) { }\n@@ -1481,0 +1548,1 @@\n+  bool _maybe_inlinetype;\n@@ -1483,1 +1551,1 @@\n-  MonitorEnter(Value obj, int monitor_no, ValueStack* state_before)\n+  MonitorEnter(Value obj, int monitor_no, ValueStack* state_before, bool maybe_inlinetype)\n@@ -1485,0 +1553,1 @@\n+  , _maybe_inlinetype(maybe_inlinetype)\n@@ -1489,0 +1558,3 @@\n+  \/\/ accessors\n+  bool maybe_inlinetype() const                   { return _maybe_inlinetype; }\n+\n@@ -1944,0 +2016,1 @@\n+  bool        _substitutability_check;\n@@ -1947,1 +2020,1 @@\n-  If(Value x, Condition cond, bool unordered_is_true, Value y, BlockBegin* tsux, BlockBegin* fsux, ValueStack* state_before, bool is_safepoint)\n+  If(Value x, Condition cond, bool unordered_is_true, Value y, BlockBegin* tsux, BlockBegin* fsux, ValueStack* state_before, bool is_safepoint, bool substitutability_check=false)\n@@ -1955,0 +2028,1 @@\n+  , _substitutability_check(substitutability_check)\n@@ -1989,0 +2063,1 @@\n+  bool substitutability_check() const              { return _substitutability_check; }\n@@ -2299,1 +2374,1 @@\n-    \/\/ The ProfileType has side-effects and must occur precisely where located\n+    \/\/ The ProfileReturnType has side-effects and must occur precisely where located\n@@ -2315,0 +2390,42 @@\n+LEAF(ProfileACmpTypes, Instruction)\n+ private:\n+  ciMethod*        _method;\n+  int              _bci;\n+  Value            _left;\n+  Value            _right;\n+  bool             _left_maybe_null;\n+  bool             _right_maybe_null;\n+\n+ public:\n+  ProfileACmpTypes(ciMethod* method, int bci, Value left, Value right)\n+    : Instruction(voidType)\n+    , _method(method)\n+    , _bci(bci)\n+    , _left(left)\n+    , _right(right)\n+  {\n+    \/\/ The ProfileACmp has side-effects and must occur precisely where located\n+    pin();\n+    _left_maybe_null = true;\n+    _right_maybe_null = true;\n+  }\n+\n+  ciMethod* method()             const { return _method; }\n+  int bci()                      const { return _bci; }\n+  Value left()                   const { return _left; }\n+  Value right()                  const { return _right; }\n+  bool left_maybe_null()         const { return _left_maybe_null; }\n+  bool right_maybe_null()        const { return _right_maybe_null; }\n+  void set_left_maybe_null(bool v)     { _left_maybe_null = v; }\n+  void set_right_maybe_null(bool v)    { _right_maybe_null = v; }\n+\n+  virtual void input_values_do(ValueVisitor* f)   {\n+    if (_left != nullptr) {\n+      f->visit(&_left);\n+    }\n+    if (_right != nullptr) {\n+      f->visit(&_right);\n+    }\n+  }\n+};\n+\n","filename":"src\/hotspot\/share\/c1\/c1_Instruction.hpp","additions":157,"deletions":40,"binary":false,"changes":197,"status":"modified"},{"patch":"@@ -28,0 +28,1 @@\n+#include \"ci\/ciInlineKlass.hpp\"\n@@ -383,1 +384,6 @@\n-  output()->print(\" (%c)\", type2char(x->elt_type()));\n+  if (x->delayed() != nullptr) {\n+    output()->print(\" +%d\", x->delayed()->offset());\n+    output()->print(\" (%c)\", type2char(x->delayed()->field()->type()->basic_type()));\n+  } else {\n+    output()->print(\" (%c)\", type2char(x->elt_type()));\n+  }\n@@ -497,1 +503,0 @@\n-\n@@ -517,1 +522,0 @@\n-\n@@ -848,0 +852,1 @@\n+\n@@ -855,0 +860,7 @@\n+void InstructionPrinter::do_ProfileACmpTypes(ProfileACmpTypes* x) {\n+  output()->print(\"profile acmp types \");\n+  print_value(x->left());\n+  output()->print(\", \");\n+  print_value(x->right());\n+}\n+\n","filename":"src\/hotspot\/share\/c1\/c1_InstructionPrinter.cpp","additions":15,"deletions":3,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -128,0 +128,1 @@\n+  virtual void do_ProfileACmpTypes(ProfileACmpTypes*  x);\n","filename":"src\/hotspot\/share\/c1\/c1_InstructionPrinter.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -30,0 +30,1 @@\n+#include \"ci\/ciInlineKlass.hpp\"\n@@ -289,1 +290,1 @@\n-                                 CodeStub* stub)\n+                                 CodeStub* stub, bool need_null_check)\n@@ -305,0 +306,1 @@\n+  , _need_null_check(need_null_check)\n@@ -332,0 +334,1 @@\n+  , _need_null_check(true)\n@@ -341,0 +344,31 @@\n+LIR_OpFlattenedArrayCheck::LIR_OpFlattenedArrayCheck(LIR_Opr array, LIR_Opr value, LIR_Opr tmp, CodeStub* stub)\n+  : LIR_Op(lir_flat_array_check, LIR_OprFact::illegalOpr, nullptr)\n+  , _array(array)\n+  , _value(value)\n+  , _tmp(tmp)\n+  , _stub(stub) {}\n+\n+\n+LIR_OpNullFreeArrayCheck::LIR_OpNullFreeArrayCheck(LIR_Opr array, LIR_Opr tmp)\n+  : LIR_Op(lir_null_free_array_check, LIR_OprFact::illegalOpr, nullptr)\n+  , _array(array)\n+  , _tmp(tmp) {}\n+\n+\n+LIR_OpSubstitutabilityCheck::LIR_OpSubstitutabilityCheck(LIR_Opr result, LIR_Opr left, LIR_Opr right, LIR_Opr equal_result, LIR_Opr not_equal_result,\n+                                                         LIR_Opr tmp1, LIR_Opr tmp2,\n+                                                         ciKlass* left_klass, ciKlass* right_klass, LIR_Opr left_klass_op, LIR_Opr right_klass_op,\n+                                                         CodeEmitInfo* info, CodeStub* stub)\n+  : LIR_Op(lir_substitutability_check, result, info)\n+  , _left(left)\n+  , _right(right)\n+  , _equal_result(equal_result)\n+  , _not_equal_result(not_equal_result)\n+  , _tmp1(tmp1)\n+  , _tmp2(tmp2)\n+  , _left_klass(left_klass)\n+  , _right_klass(right_klass)\n+  , _left_klass_op(left_klass_op)\n+  , _right_klass_op(right_klass_op)\n+  , _stub(stub) {}\n+\n@@ -414,0 +448,1 @@\n+    case lir_check_orig_pc:            \/\/ result and info always invalid\n@@ -797,0 +832,1 @@\n+      do_stub(opLock->_throw_ie_stub);\n@@ -833,0 +869,47 @@\n+\/\/ LIR_OpFlattenedArrayCheck\n+    case lir_flat_array_check: {\n+      assert(op->as_OpFlattenedArrayCheck() != nullptr, \"must be\");\n+      LIR_OpFlattenedArrayCheck* opFlattenedArrayCheck = (LIR_OpFlattenedArrayCheck*)op;\n+\n+      if (opFlattenedArrayCheck->_array->is_valid()) do_input(opFlattenedArrayCheck->_array);\n+      if (opFlattenedArrayCheck->_value->is_valid()) do_input(opFlattenedArrayCheck->_value);\n+      if (opFlattenedArrayCheck->_tmp->is_valid())   do_temp(opFlattenedArrayCheck->_tmp);\n+\n+      do_stub(opFlattenedArrayCheck->_stub);\n+\n+      break;\n+    }\n+\n+\/\/ LIR_OpNullFreeArrayCheck\n+    case lir_null_free_array_check: {\n+      assert(op->as_OpNullFreeArrayCheck() != nullptr, \"must be\");\n+      LIR_OpNullFreeArrayCheck* opNullFreeArrayCheck = (LIR_OpNullFreeArrayCheck*)op;\n+\n+      if (opNullFreeArrayCheck->_array->is_valid()) do_input(opNullFreeArrayCheck->_array);\n+      if (opNullFreeArrayCheck->_tmp->is_valid())   do_temp(opNullFreeArrayCheck->_tmp);\n+      break;\n+    }\n+\n+\/\/ LIR_OpSubstitutabilityCheck\n+    case lir_substitutability_check: {\n+      assert(op->as_OpSubstitutabilityCheck() != nullptr, \"must be\");\n+      LIR_OpSubstitutabilityCheck* opSubstitutabilityCheck = (LIR_OpSubstitutabilityCheck*)op;\n+                                                                do_input(opSubstitutabilityCheck->_left);\n+                                                                do_temp (opSubstitutabilityCheck->_left);\n+                                                                do_input(opSubstitutabilityCheck->_right);\n+                                                                do_temp (opSubstitutabilityCheck->_right);\n+                                                                do_input(opSubstitutabilityCheck->_equal_result);\n+                                                                do_temp (opSubstitutabilityCheck->_equal_result);\n+                                                                do_input(opSubstitutabilityCheck->_not_equal_result);\n+                                                                do_temp (opSubstitutabilityCheck->_not_equal_result);\n+      if (opSubstitutabilityCheck->_tmp1->is_valid())           do_temp(opSubstitutabilityCheck->_tmp1);\n+      if (opSubstitutabilityCheck->_tmp2->is_valid())           do_temp(opSubstitutabilityCheck->_tmp2);\n+      if (opSubstitutabilityCheck->_left_klass_op->is_valid())  do_temp(opSubstitutabilityCheck->_left_klass_op);\n+      if (opSubstitutabilityCheck->_right_klass_op->is_valid()) do_temp(opSubstitutabilityCheck->_right_klass_op);\n+      if (opSubstitutabilityCheck->_result->is_valid())         do_output(opSubstitutabilityCheck->_result);\n+\n+      do_info(opSubstitutabilityCheck->_info);\n+      do_stub(opSubstitutabilityCheck->_stub);\n+      break;\n+    }\n+\n@@ -910,1 +993,12 @@\n-  default:\n+\n+    \/\/ LIR_OpProfileInlineType:\n+    case lir_profile_inline_type: {\n+      assert(op->as_OpProfileInlineType() != nullptr, \"must be\");\n+      LIR_OpProfileInlineType* opProfileInlineType = (LIR_OpProfileInlineType*)op;\n+\n+      do_input(opProfileInlineType->_mdp); do_temp(opProfileInlineType->_mdp);\n+      do_input(opProfileInlineType->_obj);\n+      do_temp(opProfileInlineType->_tmp);\n+      break;\n+    }\n+default:\n@@ -983,0 +1077,28 @@\n+bool LIR_OpJavaCall::maybe_return_as_fields(ciInlineKlass** vk_ret) const {\n+  ciType* return_type = method()->return_type();\n+  if (InlineTypeReturnedAsFields) {\n+    if (return_type->is_inlinetype()) {\n+      ciInlineKlass* vk = return_type->as_inline_klass();\n+      if (vk->can_be_returned_as_fields()) {\n+        if (vk_ret != nullptr) {\n+          *vk_ret = vk;\n+        }\n+        return true;\n+      }\n+    } else if (return_type->is_instance_klass() &&\n+               (method()->is_method_handle_intrinsic() || !return_type->is_loaded() ||\n+                StressCallingConvention)) {\n+      \/\/ An inline type might be returned from the call but we don't know its type.\n+      \/\/ This can happen with method handle intrinsics or when the return type is\n+      \/\/ not loaded (method holder is not loaded or preload attribute is missing).\n+      \/\/ If an inline type is returned, we either get an oop to a buffer and nothing\n+      \/\/ needs to be done or one of the values being returned is the klass of the\n+      \/\/ inline type (RAX on x64, with LSB set to 1) and we need to allocate an inline\n+      \/\/ type instance of that type and initialize it with the fields values being\n+      \/\/ returned in other registers.\n+      return true;\n+    }\n+  }\n+  return false;\n+}\n+\n@@ -1046,0 +1168,18 @@\n+void LIR_OpFlattenedArrayCheck::emit_code(LIR_Assembler* masm) {\n+  masm->emit_opFlattenedArrayCheck(this);\n+  if (stub() != nullptr) {\n+    masm->append_code_stub(stub());\n+  }\n+}\n+\n+void LIR_OpNullFreeArrayCheck::emit_code(LIR_Assembler* masm) {\n+  masm->emit_opNullFreeArrayCheck(this);\n+}\n+\n+void LIR_OpSubstitutabilityCheck::emit_code(LIR_Assembler* masm) {\n+  masm->emit_opSubstitutabilityCheck(this);\n+  if (stub() != nullptr) {\n+    masm->append_code_stub(stub());\n+  }\n+}\n+\n@@ -1063,0 +1203,3 @@\n+  if (throw_ie_stub()) {\n+    masm->append_code_stub(throw_ie_stub());\n+  }\n@@ -1087,0 +1230,4 @@\n+void LIR_OpProfileInlineType::emit_code(LIR_Assembler* masm) {\n+  masm->emit_profile_inline_type(this);\n+}\n+\n@@ -1362,1 +1509,1 @@\n-void LIR_List::allocate_array(LIR_Opr dst, LIR_Opr len, LIR_Opr t1,LIR_Opr t2, LIR_Opr t3,LIR_Opr t4, BasicType type, LIR_Opr klass, CodeStub* stub, bool zero_array) {\n+void LIR_List::allocate_array(LIR_Opr dst, LIR_Opr len, LIR_Opr t1,LIR_Opr t2, LIR_Opr t3,LIR_Opr t4, BasicType type, LIR_Opr klass, CodeStub* stub, bool zero_array, bool is_null_free) {\n@@ -1373,1 +1520,2 @@\n-                           zero_array));\n+                           zero_array,\n+                           is_null_free));\n@@ -1411,1 +1559,1 @@\n-void LIR_List::lock_object(LIR_Opr hdr, LIR_Opr obj, LIR_Opr lock, LIR_Opr scratch, CodeStub* stub, CodeEmitInfo* info) {\n+void LIR_List::lock_object(LIR_Opr hdr, LIR_Opr obj, LIR_Opr lock, LIR_Opr scratch, CodeStub* stub, CodeEmitInfo* info, CodeStub* throw_ie_stub) {\n@@ -1419,1 +1567,2 @@\n-                    info));\n+                    info,\n+                    throw_ie_stub));\n@@ -1444,1 +1593,4 @@\n-                          ciMethod* profiled_method, int profiled_bci) {\n+                          ciMethod* profiled_method, int profiled_bci, bool is_null_free) {\n+  \/\/ If klass is non-nullable,  LIRGenerator::do_CheckCast has already performed null-check\n+  \/\/ on the object.\n+  bool need_null_check = !is_null_free;\n@@ -1446,1 +1598,2 @@\n-                                           tmp1, tmp2, tmp3, fast_check, info_for_exception, info_for_patch, stub);\n+                                           tmp1, tmp2, tmp3, fast_check, info_for_exception, info_for_patch, stub,\n+                                           need_null_check);\n@@ -1468,0 +1621,1 @@\n+  \/\/ FIXME -- if the types of the array and\/or the object are known statically, we can avoid loading the klass\n@@ -1489,0 +1643,21 @@\n+void LIR_List::check_flat_array(LIR_Opr array, LIR_Opr value, LIR_Opr tmp, CodeStub* stub) {\n+  LIR_OpFlattenedArrayCheck* c = new LIR_OpFlattenedArrayCheck(array, value, tmp, stub);\n+  append(c);\n+}\n+\n+void LIR_List::check_null_free_array(LIR_Opr array, LIR_Opr tmp) {\n+  LIR_OpNullFreeArrayCheck* c = new LIR_OpNullFreeArrayCheck(array, tmp);\n+  append(c);\n+}\n+\n+void LIR_List::substitutability_check(LIR_Opr result, LIR_Opr left, LIR_Opr right, LIR_Opr equal_result, LIR_Opr not_equal_result,\n+                                      LIR_Opr tmp1, LIR_Opr tmp2,\n+                                      ciKlass* left_klass, ciKlass* right_klass, LIR_Opr left_klass_op, LIR_Opr right_klass_op,\n+                                      CodeEmitInfo* info, CodeStub* stub) {\n+  LIR_OpSubstitutabilityCheck* c = new LIR_OpSubstitutabilityCheck(result, left, right, equal_result, not_equal_result,\n+                                                                   tmp1, tmp2,\n+                                                                   left_klass, right_klass, left_klass_op, right_klass_op,\n+                                                                   info, stub);\n+  append(c);\n+}\n+\n@@ -1704,0 +1879,1 @@\n+     case lir_check_orig_pc:         s = \"check_orig_pc\"; break;\n@@ -1769,0 +1945,6 @@\n+     \/\/ LIR_OpFlattenedArrayCheck\n+     case lir_flat_array_check:      s = \"flat_array_check\"; break;\n+     \/\/ LIR_OpNullFreeArrayCheck\n+     case lir_null_free_array_check: s = \"null_free_array_check\"; break;\n+     \/\/ LIR_OpSubstitutabilityCheck\n+     case lir_substitutability_check: s = \"substitutability_check\"; break;\n@@ -1777,0 +1959,2 @@\n+     \/\/ LIR_OpProfileInlineType\n+     case lir_profile_inline_type:   s = \"profile_inline_type\"; break;\n@@ -2002,0 +2186,38 @@\n+void LIR_OpFlattenedArrayCheck::print_instr(outputStream* out) const {\n+  array()->print(out);                   out->print(\" \");\n+  value()->print(out);                   out->print(\" \");\n+  tmp()->print(out);                     out->print(\" \");\n+  if (stub() != nullptr) {\n+    out->print(\"[label:\" INTPTR_FORMAT \"]\", p2i(stub()->entry()));\n+  }\n+}\n+\n+void LIR_OpNullFreeArrayCheck::print_instr(outputStream* out) const {\n+  array()->print(out);                   out->print(\" \");\n+  tmp()->print(out);                     out->print(\" \");\n+}\n+\n+void LIR_OpSubstitutabilityCheck::print_instr(outputStream* out) const {\n+  result_opr()->print(out);              out->print(\" \");\n+  left()->print(out);                    out->print(\" \");\n+  right()->print(out);                   out->print(\" \");\n+  equal_result()->print(out);            out->print(\" \");\n+  not_equal_result()->print(out);        out->print(\" \");\n+  tmp1()->print(out);                    out->print(\" \");\n+  tmp2()->print(out);                    out->print(\" \");\n+  if (left_klass() == nullptr) {\n+    out->print(\"unknown \");\n+  } else {\n+    left_klass()->print(out);            out->print(\" \");\n+  }\n+  if (right_klass() == nullptr) {\n+    out->print(\"unknown \");\n+  } else {\n+    right_klass()->print(out);           out->print(\" \");\n+  }\n+  left_klass_op()->print(out);           out->print(\" \");\n+  right_klass_op()->print(out);          out->print(\" \");\n+  if (stub() != nullptr) {\n+    out->print(\"[label:\" INTPTR_FORMAT \"]\", p2i(stub()->entry()));\n+  }\n+}\n@@ -2077,0 +2299,8 @@\n+\/\/ LIR_OpProfileInlineType\n+void LIR_OpProfileInlineType::print_instr(outputStream* out) const {\n+  out->print(\" flag = %x \", flag());\n+  mdp()->print(out);          out->print(\" \");\n+  obj()->print(out);          out->print(\" \");\n+  tmp()->print(out);          out->print(\" \");\n+}\n+\n","filename":"src\/hotspot\/share\/c1\/c1_LIR.cpp","additions":238,"deletions":8,"binary":false,"changes":246,"status":"modified"},{"patch":"@@ -893,0 +893,3 @@\n+class    LIR_OpFlattenedArrayCheck;\n+class    LIR_OpNullFreeArrayCheck;\n+class    LIR_OpSubstitutabilityCheck;\n@@ -897,0 +900,1 @@\n+class    LIR_OpProfileInlineType;\n@@ -920,0 +924,1 @@\n+      , lir_check_orig_pc\n@@ -996,0 +1001,9 @@\n+  , begin_opFlattenedArrayCheck\n+    , lir_flat_array_check\n+  , end_opFlattenedArrayCheck\n+  , begin_opNullFreeArrayCheck\n+    , lir_null_free_array_check\n+  , end_opNullFreeArrayCheck\n+  , begin_opSubstitutabilityCheck\n+    , lir_substitutability_check\n+  , end_opSubstitutabilityCheck\n@@ -1004,0 +1018,1 @@\n+    , lir_profile_inline_type\n@@ -1143,0 +1158,3 @@\n+  virtual LIR_OpFlattenedArrayCheck* as_OpFlattenedArrayCheck() { return nullptr; }\n+  virtual LIR_OpNullFreeArrayCheck* as_OpNullFreeArrayCheck() { return nullptr; }\n+  virtual LIR_OpSubstitutabilityCheck* as_OpSubstitutabilityCheck() { return nullptr; }\n@@ -1147,0 +1165,1 @@\n+  virtual LIR_OpProfileInlineType* as_OpProfileInlineType() { return nullptr; }\n@@ -1219,0 +1238,2 @@\n+\n+  bool maybe_return_as_fields(ciInlineKlass** vk = nullptr) const;\n@@ -1270,1 +1291,4 @@\n-    all_flags              = (1 << 12) - 1\n+    always_slow_path       = 1 << 12,\n+    src_inlinetype_check   = 1 << 13,\n+    dst_inlinetype_check   = 1 << 14,\n+    all_flags              = (1 << 15) - 1\n@@ -1528,0 +1552,1 @@\n+  bool          _need_null_check;\n@@ -1532,1 +1557,1 @@\n-                  CodeEmitInfo* info_for_exception, CodeEmitInfo* info_for_patch, CodeStub* stub);\n+                  CodeEmitInfo* info_for_exception, CodeEmitInfo* info_for_patch, CodeStub* stub, bool need_null_check = true);\n@@ -1554,1 +1579,1 @@\n-\n+  bool      need_null_check() const              { return _need_null_check;   }\n@@ -1561,0 +1586,76 @@\n+\/\/ LIR_OpFlattenedArrayCheck\n+class LIR_OpFlattenedArrayCheck: public LIR_Op {\n+ friend class LIR_OpVisitState;\n+\n+ private:\n+  LIR_Opr       _array;\n+  LIR_Opr       _value;\n+  LIR_Opr       _tmp;\n+  CodeStub*     _stub;\n+public:\n+  LIR_OpFlattenedArrayCheck(LIR_Opr array, LIR_Opr value, LIR_Opr tmp, CodeStub* stub);\n+  LIR_Opr array() const                          { return _array;         }\n+  LIR_Opr value() const                          { return _value;         }\n+  LIR_Opr tmp() const                            { return _tmp;           }\n+  CodeStub* stub() const                         { return _stub;          }\n+\n+  virtual void emit_code(LIR_Assembler* masm);\n+  virtual LIR_OpFlattenedArrayCheck* as_OpFlattenedArrayCheck() { return this; }\n+  virtual void print_instr(outputStream* out) const PRODUCT_RETURN;\n+};\n+\n+\/\/ LIR_OpNullFreeArrayCheck\n+class LIR_OpNullFreeArrayCheck: public LIR_Op {\n+ friend class LIR_OpVisitState;\n+\n+ private:\n+  LIR_Opr       _array;\n+  LIR_Opr       _tmp;\n+public:\n+  LIR_OpNullFreeArrayCheck(LIR_Opr array, LIR_Opr tmp);\n+  LIR_Opr array() const                          { return _array;         }\n+  LIR_Opr tmp() const                            { return _tmp;           }\n+\n+  virtual void emit_code(LIR_Assembler* masm);\n+  virtual LIR_OpNullFreeArrayCheck* as_OpNullFreeArrayCheck() { return this; }\n+  virtual void print_instr(outputStream* out) const PRODUCT_RETURN;\n+};\n+\n+class LIR_OpSubstitutabilityCheck: public LIR_Op {\n+ friend class LIR_OpVisitState;\n+\n+ private:\n+  LIR_Opr       _left;\n+  LIR_Opr       _right;\n+  LIR_Opr       _equal_result;\n+  LIR_Opr       _not_equal_result;\n+  LIR_Opr       _tmp1;\n+  LIR_Opr       _tmp2;\n+  ciKlass*      _left_klass;\n+  ciKlass*      _right_klass;\n+  LIR_Opr       _left_klass_op;\n+  LIR_Opr       _right_klass_op;\n+  CodeStub*     _stub;\n+public:\n+  LIR_OpSubstitutabilityCheck(LIR_Opr result, LIR_Opr left, LIR_Opr right, LIR_Opr equal_result, LIR_Opr not_equal_result,\n+                              LIR_Opr tmp1, LIR_Opr tmp2,\n+                              ciKlass* left_klass, ciKlass* right_klass, LIR_Opr left_klass_op, LIR_Opr right_klass_op,\n+                              CodeEmitInfo* info, CodeStub* stub);\n+\n+  LIR_Opr left() const             { return _left; }\n+  LIR_Opr right() const            { return _right; }\n+  LIR_Opr equal_result() const     { return _equal_result; }\n+  LIR_Opr not_equal_result() const { return _not_equal_result; }\n+  LIR_Opr tmp1() const             { return _tmp1; }\n+  LIR_Opr tmp2() const             { return _tmp2; }\n+  ciKlass* left_klass() const      { return _left_klass; }\n+  ciKlass* right_klass() const     { return _right_klass; }\n+  LIR_Opr left_klass_op() const    { return _left_klass_op; }\n+  LIR_Opr right_klass_op() const   { return _right_klass_op; }\n+  CodeStub* stub() const           { return _stub; }\n+\n+  virtual void emit_code(LIR_Assembler* masm);\n+  virtual LIR_OpSubstitutabilityCheck* as_OpSubstitutabilityCheck() { return this; }\n+  virtual void print_instr(outputStream* out) const PRODUCT_RETURN;\n+};\n+\n@@ -1719,0 +1820,1 @@\n+  bool      _is_null_free;\n@@ -1721,1 +1823,1 @@\n-  LIR_OpAllocArray(LIR_Opr klass, LIR_Opr len, LIR_Opr result, LIR_Opr t1, LIR_Opr t2, LIR_Opr t3, LIR_Opr t4, BasicType type, CodeStub* stub, bool zero_array)\n+  LIR_OpAllocArray(LIR_Opr klass, LIR_Opr len, LIR_Opr result, LIR_Opr t1, LIR_Opr t2, LIR_Opr t3, LIR_Opr t4, BasicType type, CodeStub* stub, bool zero_array, bool is_null_free)\n@@ -1731,1 +1833,2 @@\n-    , _zero_array(zero_array) {}\n+    , _zero_array(zero_array)\n+    , _is_null_free(is_null_free) {}\n@@ -1742,1 +1845,2 @@\n-  bool zero_array()   const                      { return _zero_array;  }\n+  bool      zero_array()   const                 { return _zero_array;  }\n+  bool      is_null_free() const                 { return _is_null_free;}\n@@ -1849,0 +1953,1 @@\n+  CodeStub* _throw_ie_stub;\n@@ -1850,1 +1955,1 @@\n-  LIR_OpLock(LIR_Code code, LIR_Opr hdr, LIR_Opr obj, LIR_Opr lock, LIR_Opr scratch, CodeStub* stub, CodeEmitInfo* info)\n+  LIR_OpLock(LIR_Code code, LIR_Opr hdr, LIR_Opr obj, LIR_Opr lock, LIR_Opr scratch, CodeStub* stub, CodeEmitInfo* info, CodeStub* throw_ie_stub=nullptr)\n@@ -1856,1 +1961,2 @@\n-    , _stub(stub)                      {}\n+    , _stub(stub)\n+    , _throw_ie_stub(throw_ie_stub)                    {}\n@@ -1863,0 +1969,1 @@\n+  CodeStub* throw_ie_stub() const                { return _throw_ie_stub; }\n@@ -2047,0 +2154,32 @@\n+\/\/ LIR_OpProfileInlineType\n+class LIR_OpProfileInlineType : public LIR_Op {\n+ friend class LIR_OpVisitState;\n+\n+ private:\n+  LIR_Opr      _mdp;\n+  LIR_Opr      _obj;\n+  int          _flag;\n+  LIR_Opr      _tmp;\n+  bool         _not_null;      \/\/ true if we know statically that _obj cannot be null\n+\n+ public:\n+  \/\/ Destroys recv\n+  LIR_OpProfileInlineType(LIR_Opr mdp, LIR_Opr obj, int flag, LIR_Opr tmp, bool not_null)\n+    : LIR_Op(lir_profile_inline_type, LIR_OprFact::illegalOpr, nullptr)  \/\/ no result, no info\n+    , _mdp(mdp)\n+    , _obj(obj)\n+    , _flag(flag)\n+    , _tmp(tmp)\n+    , _not_null(not_null) { }\n+\n+  LIR_Opr      mdp()              const             { return _mdp;              }\n+  LIR_Opr      obj()              const             { return _obj;              }\n+  int          flag()             const             { return _flag;             }\n+  LIR_Opr      tmp()              const             { return _tmp;              }\n+  bool         not_null()         const             { return _not_null;         }\n+\n+  virtual void emit_code(LIR_Assembler* masm);\n+  virtual LIR_OpProfileInlineType* as_OpProfileInlineType() { return this; }\n+  virtual void print_instr(outputStream* out) const PRODUCT_RETURN;\n+};\n+\n@@ -2271,1 +2410,1 @@\n-  void allocate_array(LIR_Opr dst, LIR_Opr len, LIR_Opr t1,LIR_Opr t2, LIR_Opr t3,LIR_Opr t4, BasicType type, LIR_Opr klass, CodeStub* stub, bool zero_array = true);\n+  void allocate_array(LIR_Opr dst, LIR_Opr len, LIR_Opr t1,LIR_Opr t2, LIR_Opr t3,LIR_Opr t4, BasicType type, LIR_Opr klass, CodeStub* stub, bool zero_array = true, bool is_null_free = false);\n@@ -2318,1 +2457,1 @@\n-  void lock_object(LIR_Opr hdr, LIR_Opr obj, LIR_Opr lock, LIR_Opr scratch, CodeStub* stub, CodeEmitInfo* info);\n+  void lock_object(LIR_Opr hdr, LIR_Opr obj, LIR_Opr lock, LIR_Opr scratch, CodeStub* stub, CodeEmitInfo* info, CodeStub* throw_ie_stub=nullptr);\n@@ -2328,0 +2467,6 @@\n+  void check_flat_array(LIR_Opr array, LIR_Opr value, LIR_Opr tmp, CodeStub* stub);\n+  void check_null_free_array(LIR_Opr array, LIR_Opr tmp);\n+  void substitutability_check(LIR_Opr result, LIR_Opr left, LIR_Opr right, LIR_Opr equal_result, LIR_Opr not_equal_result,\n+                              LIR_Opr tmp1, LIR_Opr tmp2,\n+                              ciKlass* left_klass, ciKlass* right_klass, LIR_Opr left_klass_op, LIR_Opr right_klass_op,\n+                              CodeEmitInfo* info, CodeStub* stub);\n@@ -2332,1 +2477,1 @@\n-                  ciMethod* profiled_method, int profiled_bci);\n+                  ciMethod* profiled_method, int profiled_bci, bool is_null_free);\n@@ -2340,0 +2485,3 @@\n+  void profile_inline_type(LIR_Address* mdp, LIR_Opr obj, int flag, LIR_Opr tmp, bool not_null) {\n+    append(new LIR_OpProfileInlineType(LIR_OprFact::address(mdp), obj, flag, tmp, not_null));\n+  }\n","filename":"src\/hotspot\/share\/c1\/c1_LIR.hpp","additions":159,"deletions":11,"binary":false,"changes":170,"status":"modified"},{"patch":"@@ -32,0 +32,1 @@\n+#include \"ci\/ciInlineKlass.hpp\"\n@@ -35,0 +36,1 @@\n+#include \"runtime\/sharedRuntime.hpp\"\n@@ -120,0 +122,1 @@\n+  _verified_inline_entry.reset();\n@@ -331,1 +334,0 @@\n-\n@@ -341,2 +343,1 @@\n-\n-void LIR_Assembler::add_call_info(int pc_offset, CodeEmitInfo* cinfo) {\n+void LIR_Assembler::add_call_info(int pc_offset, CodeEmitInfo* cinfo, bool maybe_return_as_fields) {\n@@ -344,1 +345,1 @@\n-  cinfo->record_debug_info(compilation()->debug_info_recorder(), pc_offset);\n+  cinfo->record_debug_info(compilation()->debug_info_recorder(), pc_offset, maybe_return_as_fields);\n@@ -488,0 +489,6 @@\n+\n+  ciInlineKlass* vk = nullptr;\n+  if (op->maybe_return_as_fields(&vk)) {\n+    int offset = store_inline_type_fields_to_buf(vk);\n+    add_call_info(offset, op->info(), true);\n+  }\n@@ -587,0 +594,135 @@\n+void LIR_Assembler::add_scalarized_entry_info(int pc_offset) {\n+  flush_debug_info(pc_offset);\n+  DebugInformationRecorder* debug_info = compilation()->debug_info_recorder();\n+  \/\/ The VEP and VIEP(RO) of a C1-compiled method call buffer_inline_args_xxx()\n+  \/\/ before doing any argument shuffling. This call may cause GC. When GC happens,\n+  \/\/ all the parameters are still as passed by the caller, so we just use\n+  \/\/ map->set_include_argument_oops() inside frame::sender_for_compiled_frame(RegisterMap* map).\n+  \/\/ There's no need to build a GC map here.\n+  OopMap* oop_map = new OopMap(0, 0);\n+  debug_info->add_safepoint(pc_offset, oop_map);\n+  DebugToken* locvals = debug_info->create_scope_values(nullptr); \/\/ FIXME is this needed (for Java debugging to work properly??)\n+  DebugToken* expvals = debug_info->create_scope_values(nullptr); \/\/ FIXME is this needed (for Java debugging to work properly??)\n+  DebugToken* monvals = debug_info->create_monitor_values(nullptr); \/\/ FIXME: need testing with synchronized method\n+  bool reexecute = false;\n+  bool return_oop = false; \/\/ This flag will be ignored since it used only for C2 with escape analysis.\n+  bool rethrow_exception = false;\n+  bool is_method_handle_invoke = false;\n+  debug_info->describe_scope(pc_offset, methodHandle(), method(), 0, reexecute, rethrow_exception, is_method_handle_invoke, return_oop, false, locvals, expvals, monvals);\n+  debug_info->end_safepoint(pc_offset);\n+}\n+\n+\/\/ The entries points of C1-compiled methods can have the following types:\n+\/\/ (1) Methods with no inline type args\n+\/\/ (2) Methods with inline type receiver but no inline type args\n+\/\/     VIEP_RO is the same as VIEP\n+\/\/ (3) Methods with non-inline type receiver and some inline type args\n+\/\/     VIEP_RO is the same as VEP\n+\/\/ (4) Methods with inline type receiver and other inline type args\n+\/\/     Separate VEP, VIEP and VIEP_RO\n+\/\/\n+\/\/ (1)               (2)                 (3)                    (4)\n+\/\/ UEP\/UIEP:         VEP:                UEP:                   UEP:\n+\/\/   check_icache      pack receiver       check_icache           check_icache\n+\/\/ VEP\/VIEP\/VIEP_RO    jump to VIEP      VEP\/VIEP_RO:           VIEP_RO:\n+\/\/   body            UEP\/UIEP:             pack inline args       pack inline args (except receiver)\n+\/\/                     check_icache        jump to VIEP           jump to VIEP\n+\/\/                   VIEP\/VIEP_RO        UIEP:                  VEP:\n+\/\/                     body                check_icache           pack all inline args\n+\/\/                                       VIEP:                    jump to VIEP\n+\/\/                                         body                 UIEP:\n+\/\/                                                                check_icache\n+\/\/                                                              VIEP:\n+\/\/                                                                body\n+void LIR_Assembler::emit_std_entries() {\n+  offsets()->set_value(CodeOffsets::OSR_Entry, _masm->offset());\n+\n+  _masm->align(CodeEntryAlignment);\n+  const CompiledEntrySignature* ces = compilation()->compiled_entry_signature();\n+  if (ces->has_scalarized_args()) {\n+    assert(InlineTypePassFieldsAsArgs && method()->get_Method()->has_scalarized_args(), \"must be\");\n+    CodeOffsets::Entries ro_entry_type = ces->c1_inline_ro_entry_type();\n+\n+    \/\/ UEP: check icache and fall-through\n+    if (ro_entry_type != CodeOffsets::Verified_Inline_Entry) {\n+      offsets()->set_value(CodeOffsets::Entry, _masm->offset());\n+      if (needs_icache(method())) {\n+        check_icache();\n+      }\n+    }\n+\n+    \/\/ VIEP_RO: pack all value parameters, except the receiver\n+    if (ro_entry_type == CodeOffsets::Verified_Inline_Entry_RO) {\n+      emit_std_entry(CodeOffsets::Verified_Inline_Entry_RO, ces);\n+    }\n+\n+    \/\/ VEP: pack all value parameters\n+    _masm->align(CodeEntryAlignment);\n+    emit_std_entry(CodeOffsets::Verified_Entry, ces);\n+\n+    \/\/ UIEP: check icache and fall-through\n+    _masm->align(CodeEntryAlignment);\n+    offsets()->set_value(CodeOffsets::Inline_Entry, _masm->offset());\n+    if (ro_entry_type == CodeOffsets::Verified_Inline_Entry) {\n+      \/\/ Special case if we have VIEP == VIEP(RO):\n+      \/\/ this means UIEP (called by C1) == UEP (called by C2).\n+      offsets()->set_value(CodeOffsets::Entry, _masm->offset());\n+    }\n+    if (needs_icache(method())) {\n+      check_icache();\n+    }\n+\n+    \/\/ VIEP: all value parameters are passed as refs - no packing.\n+    emit_std_entry(CodeOffsets::Verified_Inline_Entry, nullptr);\n+\n+    if (ro_entry_type != CodeOffsets::Verified_Inline_Entry_RO) {\n+      \/\/ The VIEP(RO) is the same as VEP or VIEP\n+      assert(ro_entry_type == CodeOffsets::Verified_Entry ||\n+             ro_entry_type == CodeOffsets::Verified_Inline_Entry, \"must be\");\n+      offsets()->set_value(CodeOffsets::Verified_Inline_Entry_RO,\n+                           offsets()->value(ro_entry_type));\n+    }\n+  } else {\n+    \/\/ All 3 entries are the same (no inline type packing)\n+    offsets()->set_value(CodeOffsets::Entry, _masm->offset());\n+    offsets()->set_value(CodeOffsets::Inline_Entry, _masm->offset());\n+    if (needs_icache(method())) {\n+      check_icache();\n+    }\n+    emit_std_entry(CodeOffsets::Verified_Inline_Entry, nullptr);\n+    offsets()->set_value(CodeOffsets::Verified_Entry, offsets()->value(CodeOffsets::Verified_Inline_Entry));\n+    offsets()->set_value(CodeOffsets::Verified_Inline_Entry_RO, offsets()->value(CodeOffsets::Verified_Inline_Entry));\n+  }\n+}\n+\n+void LIR_Assembler::emit_std_entry(CodeOffsets::Entries entry, const CompiledEntrySignature* ces) {\n+  offsets()->set_value(entry, _masm->offset());\n+  _masm->verified_entry(compilation()->directive()->BreakAtExecuteOption);\n+  switch (entry) {\n+  case CodeOffsets::Verified_Entry: {\n+    if (needs_clinit_barrier_on_entry(method())) {\n+      clinit_barrier(method());\n+    }\n+    int rt_call_offset = _masm->verified_entry(ces, initial_frame_size_in_bytes(), bang_size_in_bytes(), in_bytes(frame_map()->sp_offset_for_orig_pc()), _verified_inline_entry);\n+    add_scalarized_entry_info(rt_call_offset);\n+    break;\n+  }\n+  case CodeOffsets::Verified_Inline_Entry_RO: {\n+    assert(!needs_clinit_barrier_on_entry(method()), \"can't be static\");\n+    int rt_call_offset = _masm->verified_inline_ro_entry(ces, initial_frame_size_in_bytes(), bang_size_in_bytes(), in_bytes(frame_map()->sp_offset_for_orig_pc()), _verified_inline_entry);\n+    add_scalarized_entry_info(rt_call_offset);\n+    break;\n+  }\n+  case CodeOffsets::Verified_Inline_Entry: {\n+    if (needs_clinit_barrier_on_entry(method())) {\n+      clinit_barrier(method());\n+    }\n+    build_frame();\n+    offsets()->set_value(CodeOffsets::Frame_Complete, _masm->offset());\n+    break;\n+  }\n+  default:\n+    ShouldNotReachHere();\n+    break;\n+  }\n+}\n@@ -599,15 +741,2 @@\n-    case lir_std_entry: {\n-      \/\/ init offsets\n-      offsets()->set_value(CodeOffsets::OSR_Entry, _masm->offset());\n-      if (needs_icache(compilation()->method())) {\n-        int offset = check_icache();\n-        offsets()->set_value(CodeOffsets::Entry, offset);\n-      }\n-      _masm->align(CodeEntryAlignment);\n-      offsets()->set_value(CodeOffsets::Verified_Entry, _masm->offset());\n-      _masm->verified_entry(compilation()->directive()->BreakAtExecuteOption);\n-      if (needs_clinit_barrier_on_entry(compilation()->method())) {\n-        clinit_barrier(compilation()->method());\n-      }\n-      build_frame();\n-      offsets()->set_value(CodeOffsets::Frame_Complete, _masm->offset());\n+    case lir_std_entry:\n+      emit_std_entries();\n@@ -615,1 +744,0 @@\n-    }\n@@ -668,0 +796,4 @@\n+    case lir_check_orig_pc:\n+      check_orig_pc();\n+      break;\n+\n@@ -753,1 +885,2 @@\n-  _masm->build_frame(initial_frame_size_in_bytes(), bang_size_in_bytes());\n+  _masm->build_frame(initial_frame_size_in_bytes(), bang_size_in_bytes(), in_bytes(frame_map()->sp_offset_for_orig_pc()),\n+                     needs_stack_repair(), method()->has_scalarized_args(), &_verified_inline_entry);\n","filename":"src\/hotspot\/share\/c1\/c1_LIRAssembler.cpp","additions":154,"deletions":21,"binary":false,"changes":175,"status":"modified"},{"patch":"@@ -34,0 +34,1 @@\n+class CompiledEntrySignature;\n@@ -50,0 +51,1 @@\n+  Label              _verified_inline_entry;\n@@ -94,0 +96,4 @@\n+  bool needs_stack_repair() const {\n+    return compilation()->needs_stack_repair();\n+  }\n+\n@@ -101,1 +107,1 @@\n-  void add_call_info(int pc_offset, CodeEmitInfo* cinfo);\n+  void add_call_info(int pc_offset, CodeEmitInfo* cinfo, bool maybe_return_as_fields = false);\n@@ -199,0 +205,3 @@\n+  void emit_opFlattenedArrayCheck(LIR_OpFlattenedArrayCheck* op);\n+  void emit_opNullFreeArrayCheck(LIR_OpNullFreeArrayCheck* op);\n+  void emit_opSubstitutabilityCheck(LIR_OpSubstitutabilityCheck* op);\n@@ -207,0 +216,1 @@\n+  void emit_profile_inline_type(LIR_OpProfileInlineType* op);\n@@ -208,0 +218,3 @@\n+  void emit_std_entries();\n+  void emit_std_entry(CodeOffsets::Entries entry, const CompiledEntrySignature* ces);\n+  void add_scalarized_entry_info(int call_offset);\n@@ -228,0 +241,1 @@\n+  int  store_inline_type_fields_to_buf(ciInlineKlass* vk);\n@@ -254,0 +268,1 @@\n+  void check_orig_pc();\n","filename":"src\/hotspot\/share\/c1\/c1_LIRAssembler.hpp","additions":16,"deletions":1,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -33,0 +33,2 @@\n+#include \"ci\/ciFlatArrayKlass.hpp\"\n+#include \"ci\/ciInlineKlass.hpp\"\n@@ -218,0 +220,2 @@\n+  assert(!_gen->in_conditional_code(), \"LIRItem cannot be loaded in conditional code\");\n+\n@@ -625,1 +629,2 @@\n-void LIRGenerator::monitor_enter(LIR_Opr object, LIR_Opr lock, LIR_Opr hdr, LIR_Opr scratch, int monitor_no, CodeEmitInfo* info_for_exception, CodeEmitInfo* info) {\n+void LIRGenerator::monitor_enter(LIR_Opr object, LIR_Opr lock, LIR_Opr hdr, LIR_Opr scratch, int monitor_no,\n+                                 CodeEmitInfo* info_for_exception, CodeEmitInfo* info, CodeStub* throw_ie_stub) {\n@@ -628,1 +633,1 @@\n-  CodeStub* slow_path = new MonitorEnterStub(object, lock, info);\n+  CodeStub* slow_path = new MonitorEnterStub(object, lock, info, throw_ie_stub, scratch);\n@@ -631,1 +636,1 @@\n-  __ lock_object(hdr, object, lock, scratch, slow_path, info_for_exception);\n+  __ lock_object(hdr, object, lock, scratch, slow_path, info_for_exception, throw_ie_stub);\n@@ -655,4 +660,9 @@\n-void LIRGenerator::new_instance(LIR_Opr dst, ciInstanceKlass* klass, bool is_unresolved, LIR_Opr scratch1, LIR_Opr scratch2, LIR_Opr scratch3, LIR_Opr scratch4, LIR_Opr klass_reg, CodeEmitInfo* info) {\n-  klass2reg_with_patching(klass_reg, klass, info, is_unresolved);\n-  \/\/ If klass is not loaded we do not know if the klass has finalizers:\n-  if (UseFastNewInstance && klass->is_loaded()\n+void LIRGenerator::new_instance(LIR_Opr dst, ciInstanceKlass* klass, bool is_unresolved, bool allow_inline, LIR_Opr scratch1, LIR_Opr scratch2, LIR_Opr scratch3, LIR_Opr scratch4, LIR_Opr klass_reg, CodeEmitInfo* info) {\n+  if (allow_inline) {\n+    assert(!is_unresolved && klass->is_loaded(), \"inline type klass should be resolved\");\n+    __ metadata2reg(klass->constant_encoding(), klass_reg);\n+  } else {\n+    klass2reg_with_patching(klass_reg, klass, info, is_unresolved);\n+  }\n+  \/\/ If klass is not loaded we do not know if the klass has finalizers or is an unexpected inline klass\n+  if (UseFastNewInstance && klass->is_loaded() && (allow_inline || !klass->is_inlinetype())\n@@ -673,1 +683,1 @@\n-    __ branch(lir_cond_always, slow_path);\n+    __ jump(slow_path);\n@@ -773,0 +783,10 @@\n+  if (!src->is_loaded_flat_array() && !dst->is_loaded_flat_array()) {\n+    flags &= ~LIR_OpArrayCopy::always_slow_path;\n+  }\n+  if (!src->maybe_flat_array()) {\n+    flags &= ~LIR_OpArrayCopy::src_inlinetype_check;\n+  }\n+  if (!dst->maybe_flat_array() && !dst->maybe_null_free_array()) {\n+    flags &= ~LIR_OpArrayCopy::dst_inlinetype_check;\n+  }\n+\n@@ -1465,1 +1485,1 @@\n-  for (int i = 0; i < _constants.length(); i++) {\n+  for (int i = 0; i < _constants.length() && !in_conditional_code(); i++) {\n@@ -1490,2 +1510,4 @@\n-  _constants.append(c);\n-  _reg_for_constants.append(result);\n+  if (!in_conditional_code()) {\n+    _constants.append(c);\n+    _reg_for_constants.append(result);\n+  }\n@@ -1495,0 +1517,6 @@\n+void LIRGenerator::set_in_conditional_code(bool v) {\n+  assert(v != _in_conditional_code, \"must change state\");\n+  _in_conditional_code = v;\n+}\n+\n+\n@@ -1512,0 +1540,8 @@\n+\/\/ Returns a int\/long value with the null marker bit set\n+static LIR_Opr null_marker_mask(BasicType bt, ciField* field) {\n+  assert(field->null_marker_offset() != -1, \"field does not have null marker\");\n+  int nm_offset = field->null_marker_offset() - field->offset_in_bytes();\n+  jlong null_marker = 1ULL << (nm_offset << LogBitsPerByte);\n+  return (bt == T_LONG) ? LIR_OprFact::longConst(null_marker) : LIR_OprFact::intConst(null_marker);\n+}\n+\n@@ -1541,0 +1577,1 @@\n+  ciField* field = x->field();\n@@ -1542,1 +1579,1 @@\n-  bool is_volatile = x->field()->is_volatile();\n+  bool is_volatile = field->is_volatile();\n@@ -1563,10 +1600,2 @@\n-  if (is_volatile || needs_patching) {\n-    \/\/ load item if field is volatile (fewer special cases for volatiles)\n-    \/\/ load item if field not initialized\n-    \/\/ load item if field not constant\n-    \/\/ because of code patching we cannot inline constants\n-    if (field_type == T_BYTE || field_type == T_BOOLEAN) {\n-      value.load_byte_item();\n-    } else  {\n-      value.load_item();\n-    }\n+  if (field->is_flat()) {\n+    value.load_item();\n@@ -1574,1 +1603,13 @@\n-    value.load_for_store(field_type);\n+    if (is_volatile || needs_patching) {\n+      \/\/ load item if field is volatile (fewer special cases for volatiles)\n+      \/\/ load item if field not initialized\n+      \/\/ load item if field not constant\n+      \/\/ because of code patching we cannot inline constants\n+      if (field_type == T_BYTE || field_type == T_BOOLEAN) {\n+        value.load_byte_item();\n+      } else  {\n+        value.load_item();\n+      }\n+    } else {\n+      value.load_for_store(field_type);\n+    }\n@@ -1603,0 +1644,43 @@\n+  if (field->is_flat()) {\n+    ciInlineKlass* vk = field->type()->as_inline_klass();\n+\n+#ifdef ASSERT\n+    bool is_naturally_atomic = vk->nof_declared_nonstatic_fields() <= 1;\n+    bool needs_atomic_access = !field->is_null_free() || (field->is_volatile() && !is_naturally_atomic);\n+    assert(needs_atomic_access, \"No atomic access required\");\n+    \/\/ ZGC does not support compressed oops, so only one oop can be in the payload which is written by a \"normal\" oop store.\n+    assert(!vk->contains_oops() || !UseZGC, \"ZGC does not support embedded oops in flat fields\");\n+#endif\n+\n+    \/\/ Zero the payload\n+    BasicType bt = vk->atomic_size_to_basic_type(field->is_null_free());\n+    LIR_Opr payload = new_register((bt == T_LONG) ? bt : T_INT);\n+    LIR_Opr zero = (bt == T_LONG) ? LIR_OprFact::longConst(0) : LIR_OprFact::intConst(0);\n+    __ move(zero, payload);\n+\n+    bool is_constant_null = value.is_constant() && value.value()->is_null_obj();\n+    if (!is_constant_null) {\n+      LabelObj* L_isNull = new LabelObj();\n+      bool needs_null_check = !value.is_constant() || value.value()->is_null_obj();\n+      if (needs_null_check) {\n+        __ cmp(lir_cond_equal, value.result(), LIR_OprFact::oopConst(nullptr));\n+        __ branch(lir_cond_equal, L_isNull->label());\n+      }\n+      \/\/ Load payload (if not empty) and set null marker (if not null-free)\n+      if (!vk->is_empty()) {\n+        access_load_at(decorators, bt, value, LIR_OprFact::intConst(vk->payload_offset()), payload);\n+      }\n+      if (!field->is_null_free()) {\n+        __ logical_or(payload, null_marker_mask(bt, field), payload);\n+      }\n+      if (needs_null_check) {\n+        __ branch_destination(L_isNull->label());\n+      }\n+    }\n+    access_store_at(decorators, bt, object, LIR_OprFact::intConst(x->offset()), payload,\n+                    \/\/ Make sure to emit an implicit null check and pass the information\n+                    \/\/ that this is a flat store that might require gc barriers for oop fields.\n+                    info != nullptr ? new CodeEmitInfo(info) : nullptr, info, vk);\n+    return;\n+  }\n+\n@@ -1607,0 +1691,155 @@\n+\/\/ FIXME -- I can't find any other way to pass an address to access_load_at().\n+class TempResolvedAddress: public Instruction {\n+ public:\n+  TempResolvedAddress(ValueType* type, LIR_Opr addr) : Instruction(type) {\n+    set_operand(addr);\n+  }\n+  virtual void input_values_do(ValueVisitor*) {}\n+  virtual void visit(InstructionVisitor* v)   {}\n+  virtual const char* name() const  { return \"TempResolvedAddress\"; }\n+};\n+\n+LIR_Opr LIRGenerator::get_and_load_element_address(LIRItem& array, LIRItem& index) {\n+  ciType* array_type = array.value()->declared_type();\n+  ciFlatArrayKlass* flat_array_klass = array_type->as_flat_array_klass();\n+  assert(flat_array_klass->is_loaded(), \"must be\");\n+\n+  int array_header_size = flat_array_klass->array_header_in_bytes();\n+  int shift = flat_array_klass->log2_element_size();\n+\n+#ifndef _LP64\n+  LIR_Opr index_op = new_register(T_INT);\n+  \/\/ FIXME -- on 32-bit, the shift below can overflow, so we need to check that\n+  \/\/ the top (shift+1) bits of index_op must be zero, or\n+  \/\/ else throw ArrayIndexOutOfBoundsException\n+  if (index.result()->is_constant()) {\n+    jint const_index = index.result()->as_jint();\n+    __ move(LIR_OprFact::intConst(const_index << shift), index_op);\n+  } else {\n+    __ shift_left(index_op, shift, index.result());\n+  }\n+#else\n+  LIR_Opr index_op = new_register(T_LONG);\n+  if (index.result()->is_constant()) {\n+    jint const_index = index.result()->as_jint();\n+    __ move(LIR_OprFact::longConst(const_index << shift), index_op);\n+  } else {\n+    __ convert(Bytecodes::_i2l, index.result(), index_op);\n+    \/\/ Need to shift manually, as LIR_Address can scale only up to 3.\n+    __ shift_left(index_op, shift, index_op);\n+  }\n+#endif\n+\n+  LIR_Opr elm_op = new_pointer_register();\n+  LIR_Address* elm_address = generate_address(array.result(), index_op, 0, array_header_size, T_ADDRESS);\n+  __ leal(LIR_OprFact::address(elm_address), elm_op);\n+  return elm_op;\n+}\n+\n+void LIRGenerator::access_sub_element(LIRItem& array, LIRItem& index, LIR_Opr& result, ciField* field, int sub_offset) {\n+  assert(field != nullptr, \"Need a subelement type specified\");\n+\n+  \/\/ Find the starting address of the source (inside the array)\n+  LIR_Opr elm_op = get_and_load_element_address(array, index);\n+\n+  BasicType subelt_type = field->type()->basic_type();\n+  TempResolvedAddress* elm_resolved_addr = new TempResolvedAddress(as_ValueType(subelt_type), elm_op);\n+  LIRItem elm_item(elm_resolved_addr, this);\n+\n+  DecoratorSet decorators = IN_HEAP;\n+  access_load_at(decorators, subelt_type,\n+                     elm_item, LIR_OprFact::intConst(sub_offset), result,\n+                     nullptr, nullptr);\n+}\n+\n+void LIRGenerator::access_flat_array(bool is_load, LIRItem& array, LIRItem& index, LIRItem& obj_item,\n+                                          ciField* field, int sub_offset) {\n+  assert(sub_offset == 0 || field != nullptr, \"Sanity check\");\n+\n+  \/\/ Find the starting address of the source (inside the array)\n+  LIR_Opr elm_op = get_and_load_element_address(array, index);\n+\n+  ciInlineKlass* elem_klass = nullptr;\n+  if (field != nullptr) {\n+    elem_klass = field->type()->as_inline_klass();\n+  } else {\n+    elem_klass = array.value()->declared_type()->as_flat_array_klass()->element_klass()->as_inline_klass();\n+  }\n+  for (int i = 0; i < elem_klass->nof_nonstatic_fields(); i++) {\n+    ciField* inner_field = elem_klass->nonstatic_field_at(i);\n+    assert(!inner_field->is_flat(), \"flat fields must have been expanded\");\n+    int obj_offset = inner_field->offset_in_bytes();\n+    int elm_offset = obj_offset - elem_klass->payload_offset() + sub_offset; \/\/ object header is not stored in array.\n+    BasicType field_type = inner_field->type()->basic_type();\n+\n+    \/\/ Types which are smaller than int are still passed in an int register.\n+    BasicType reg_type = field_type;\n+    switch (reg_type) {\n+    case T_BYTE:\n+    case T_BOOLEAN:\n+    case T_SHORT:\n+    case T_CHAR:\n+      reg_type = T_INT;\n+      break;\n+    default:\n+      break;\n+    }\n+\n+    LIR_Opr temp = new_register(reg_type);\n+    TempResolvedAddress* elm_resolved_addr = new TempResolvedAddress(as_ValueType(field_type), elm_op);\n+    LIRItem elm_item(elm_resolved_addr, this);\n+\n+    DecoratorSet decorators = IN_HEAP;\n+    if (is_load) {\n+      access_load_at(decorators, field_type,\n+                     elm_item, LIR_OprFact::intConst(elm_offset), temp,\n+                     nullptr, nullptr);\n+      access_store_at(decorators, field_type,\n+                      obj_item, LIR_OprFact::intConst(obj_offset), temp,\n+                      nullptr, nullptr);\n+    } else {\n+      access_load_at(decorators, field_type,\n+                     obj_item, LIR_OprFact::intConst(obj_offset), temp,\n+                     nullptr, nullptr);\n+      access_store_at(decorators, field_type,\n+                      elm_item, LIR_OprFact::intConst(elm_offset), temp,\n+                      nullptr, nullptr);\n+    }\n+  }\n+}\n+\n+void LIRGenerator::check_flat_array(LIR_Opr array, LIR_Opr value, CodeStub* slow_path) {\n+  LIR_Opr tmp = new_register(T_METADATA);\n+  __ check_flat_array(array, value, tmp, slow_path);\n+}\n+\n+void LIRGenerator::check_null_free_array(LIRItem& array, LIRItem& value, CodeEmitInfo* info) {\n+  LabelObj* L_end = new LabelObj();\n+  LIR_Opr tmp = new_register(T_METADATA);\n+  __ check_null_free_array(array.result(), tmp);\n+  __ branch(lir_cond_equal, L_end->label());\n+  __ null_check(value.result(), info);\n+  __ branch_destination(L_end->label());\n+}\n+\n+bool LIRGenerator::needs_flat_array_store_check(StoreIndexed* x) {\n+  if (x->elt_type() == T_OBJECT && x->array()->maybe_flat_array()) {\n+    ciType* type = x->value()->declared_type();\n+    if (type != nullptr && type->is_klass()) {\n+      ciKlass* klass = type->as_klass();\n+      if (!klass->can_be_inline_klass() || (klass->is_inlinetype() && !klass->as_inline_klass()->flat_in_array())) {\n+        \/\/ This is known to be a non-flat object. If the array is a flat array,\n+        \/\/ it will be caught by the code generated by array_store_check().\n+        return false;\n+      }\n+    }\n+    \/\/ We're not 100% sure, so let's do the flat_array_store_check.\n+    return true;\n+  }\n+  return false;\n+}\n+\n+bool LIRGenerator::needs_null_free_array_store_check(StoreIndexed* x) {\n+  return x->elt_type() == T_OBJECT && x->array()->maybe_null_free_array();\n+}\n+\n@@ -1609,0 +1848,2 @@\n+  assert(x->elt_type() != T_ARRAY, \"never used\");\n+  bool is_loaded_flat_array = x->array()->is_loaded_flat_array();\n@@ -1612,3 +1853,3 @@\n-  bool needs_store_check = obj_store && (x->value()->as_Constant() == nullptr ||\n-                                         !get_jobject_constant(x->value())->is_null_object() ||\n-                                         x->should_profile());\n+  bool needs_store_check = obj_store && !(is_loaded_flat_array && x->is_exact_flat_array_store()) &&\n+                                        (x->value()->as_Constant() == nullptr ||\n+                                         !get_jobject_constant(x->value())->is_null_object());\n@@ -1627,2 +1868,3 @@\n-\n-  if (needs_store_check || x->check_boolean()) {\n+\n+  if (needs_store_check || x->check_boolean()\n+      || is_loaded_flat_array || needs_flat_array_store_check(x) || needs_null_free_array_store_check(x)) {\n@@ -1657,0 +1899,20 @@\n+  if (x->should_profile()) {\n+    if (is_loaded_flat_array) {\n+      \/\/ No need to profile a store to a flat array of known type. This can happen if\n+      \/\/ the type only became known after optimizations (for example, after the PhiSimplifier).\n+      x->set_should_profile(false);\n+    } else {\n+      int bci = x->profiled_bci();\n+      ciMethodData* md = x->profiled_method()->method_data();\n+      assert(md != nullptr, \"Sanity\");\n+      ciProfileData* data = md->bci_to_data(bci);\n+      assert(data != nullptr && data->is_ArrayStoreData(), \"incorrect profiling entry\");\n+      ciArrayStoreData* store_data = (ciArrayStoreData*)data;\n+      profile_array_type(x, md, store_data);\n+      assert(store_data->is_ArrayStoreData(), \"incorrect profiling entry\");\n+      if (x->array()->maybe_null_free_array()) {\n+        profile_null_free_array(array, md, store_data);\n+      }\n+    }\n+  }\n+\n@@ -1662,4 +1924,11 @@\n-  DecoratorSet decorators = IN_HEAP | IS_ARRAY;\n-  if (x->check_boolean()) {\n-    decorators |= C1_MASK_BOOLEAN;\n-  }\n+  if (is_loaded_flat_array) {\n+    \/\/ TODO 8350865 This is currently dead code\n+    if (!x->value()->is_null_free()) {\n+      __ null_check(value.result(), new CodeEmitInfo(range_check_info));\n+    }\n+    \/\/ If array element is an empty inline type, no need to copy anything\n+    if (!x->array()->declared_type()->as_flat_array_klass()->element_klass()->as_inline_klass()->is_empty()) {\n+      access_flat_array(false, array, index, value);\n+    }\n+  } else {\n+    StoreFlattenedArrayStub* slow_path = nullptr;\n@@ -1667,2 +1936,22 @@\n-  access_store_at(decorators, x->elt_type(), array, index.result(), value.result(),\n-                  nullptr, null_check_info);\n+    if (needs_flat_array_store_check(x)) {\n+      \/\/ Check if we indeed have a flat array\n+      index.load_item();\n+      slow_path = new StoreFlattenedArrayStub(array.result(), index.result(), value.result(), state_for(x, x->state_before()));\n+      check_flat_array(array.result(), value.result(), slow_path);\n+      set_in_conditional_code(true);\n+    } else if (needs_null_free_array_store_check(x)) {\n+      CodeEmitInfo* info = new CodeEmitInfo(range_check_info);\n+      check_null_free_array(array, value, info);\n+    }\n+\n+    DecoratorSet decorators = IN_HEAP | IS_ARRAY;\n+    if (x->check_boolean()) {\n+      decorators |= C1_MASK_BOOLEAN;\n+    }\n+\n+    access_store_at(decorators, x->elt_type(), array, index.result(), value.result(), nullptr, null_check_info);\n+    if (slow_path != nullptr) {\n+      __ branch_destination(slow_path->continuation());\n+      set_in_conditional_code(false);\n+    }\n+  }\n@@ -1697,1 +1986,2 @@\n-                                   CodeEmitInfo* patch_info, CodeEmitInfo* store_emit_info) {\n+                                   CodeEmitInfo* patch_info, CodeEmitInfo* store_emit_info,\n+                                   ciInlineKlass* vk) {\n@@ -1699,1 +1989,1 @@\n-  LIRAccess access(this, decorators, base, offset, type, patch_info, store_emit_info);\n+  LIRAccess access(this, decorators, base, offset, type, patch_info, store_emit_info, vk);\n@@ -1750,0 +2040,1 @@\n+  ciField* field = x->field();\n@@ -1751,1 +2042,1 @@\n-  bool is_volatile = x->field()->is_volatile();\n+  bool is_volatile = field->is_volatile();\n@@ -1802,0 +2093,37 @@\n+  if (field->is_flat()) {\n+    ciInlineKlass* vk = field->type()->as_inline_klass();\n+#ifdef ASSERT\n+    bool is_naturally_atomic = vk->nof_declared_nonstatic_fields() <= 1;\n+    bool needs_atomic_access = !field->is_null_free() || (field->is_volatile() && !is_naturally_atomic);\n+    assert(needs_atomic_access, \"No atomic access required\");\n+    assert(x->state_before() != nullptr, \"Needs state before\");\n+#endif\n+\n+    \/\/ Allocate buffer (we can't easily do this conditionally on the null check below\n+    \/\/ because branches added in the LIR are opaque to the register allocator).\n+    NewInstance* buffer = new NewInstance(vk, x->state_before(), false, true);\n+    do_NewInstance(buffer);\n+    LIRItem dest(buffer, this);\n+\n+    \/\/ Copy the payload to the buffer\n+    BasicType bt = vk->atomic_size_to_basic_type(field->is_null_free());\n+    LIR_Opr payload = new_register((bt == T_LONG) ? bt : T_INT);\n+    access_load_at(decorators, bt, object, LIR_OprFact::intConst(field->offset_in_bytes()), payload,\n+                   \/\/ Make sure to emit an implicit null check\n+                   info ? new CodeEmitInfo(info) : nullptr, info);\n+    access_store_at(decorators, bt, dest, LIR_OprFact::intConst(vk->payload_offset()), payload);\n+\n+    if (field->is_null_free()) {\n+      set_result(x, buffer->operand());\n+    } else {\n+      \/\/ Check the null marker and set result to null if it's not set\n+      __ logical_and(payload, null_marker_mask(bt, field), payload);\n+      __ cmp(lir_cond_equal, payload, (bt == T_LONG) ? LIR_OprFact::longConst(0) : LIR_OprFact::intConst(0));\n+      __ cmove(lir_cond_equal, LIR_OprFact::oopConst(nullptr), buffer->operand(), rlock_result(x), T_OBJECT);\n+    }\n+\n+    \/\/ Ensure the copy is visible before any subsequent store that publishes the buffer.\n+    __ membar_storestore();\n+    return;\n+  }\n+\n@@ -1950,1 +2278,61 @@\n-  DecoratorSet decorators = IN_HEAP | IS_ARRAY;\n+  ciMethodData* md = nullptr;\n+  ciArrayLoadData* load_data = nullptr;\n+  if (x->should_profile()) {\n+    if (x->array()->is_loaded_flat_array()) {\n+      \/\/ No need to profile a load from a flat array of known type. This can happen if\n+      \/\/ the type only became known after optimizations (for example, after the PhiSimplifier).\n+      x->set_should_profile(false);\n+    } else {\n+      int bci = x->profiled_bci();\n+      md = x->profiled_method()->method_data();\n+      assert(md != nullptr, \"Sanity\");\n+      ciProfileData* data = md->bci_to_data(bci);\n+      assert(data != nullptr && data->is_ArrayLoadData(), \"incorrect profiling entry\");\n+      load_data = (ciArrayLoadData*)data;\n+      profile_array_type(x, md, load_data);\n+    }\n+  }\n+\n+  Value element;\n+  if (x->vt() != nullptr) {\n+    assert(x->array()->is_loaded_flat_array(), \"must be\");\n+    \/\/ Find the destination address (of the NewInlineTypeInstance).\n+    LIRItem obj_item(x->vt(), this);\n+\n+    access_flat_array(true, array, index, obj_item,\n+                      x->delayed() == nullptr ? 0 : x->delayed()->field(),\n+                      x->delayed() == nullptr ? 0 : x->delayed()->offset());\n+    set_no_result(x);\n+  } else if (x->delayed() != nullptr) {\n+    assert(x->array()->is_loaded_flat_array(), \"must be\");\n+    LIR_Opr result = rlock_result(x, x->delayed()->field()->type()->basic_type());\n+    access_sub_element(array, index, result, x->delayed()->field(), x->delayed()->offset());\n+  } else {\n+    LIR_Opr result = rlock_result(x, x->elt_type());\n+    LoadFlattenedArrayStub* slow_path = nullptr;\n+\n+    if (x->should_profile() && x->array()->maybe_null_free_array()) {\n+      profile_null_free_array(array, md, load_data);\n+    }\n+\n+    if (x->elt_type() == T_OBJECT && x->array()->maybe_flat_array()) {\n+      assert(x->delayed() == nullptr, \"Delayed LoadIndexed only apply to loaded_flat_arrays\");\n+      index.load_item();\n+      \/\/ if we are loading from a flat array, load it using a runtime call\n+      slow_path = new LoadFlattenedArrayStub(array.result(), index.result(), result, state_for(x, x->state_before()));\n+      check_flat_array(array.result(), LIR_OprFact::illegalOpr, slow_path);\n+      set_in_conditional_code(true);\n+    }\n+\n+    DecoratorSet decorators = IN_HEAP | IS_ARRAY;\n+    access_load_at(decorators, x->elt_type(),\n+                   array, index.result(), result,\n+                   nullptr, null_check_info);\n+\n+    if (slow_path != nullptr) {\n+      __ branch_destination(slow_path->continuation());\n+      set_in_conditional_code(false);\n+    }\n+\n+    element = x;\n+  }\n@@ -1952,4 +2340,3 @@\n-  LIR_Opr result = rlock_result(x, x->elt_type());\n-  access_load_at(decorators, x->elt_type(),\n-                 array, index.result(), result,\n-                 nullptr, null_check_info);\n+  if (x->should_profile()) {\n+    profile_element_type(element, md, load_data);\n+  }\n@@ -2438,1 +2825,1 @@\n-  if (do_update) {\n+  if (do_update && signature_at_call_k != nullptr) {\n@@ -2523,0 +2910,40 @@\n+void LIRGenerator::profile_flags(ciMethodData* md, ciProfileData* data, int flag, LIR_Condition condition) {\n+  assert(md != nullptr && data != nullptr, \"should have been initialized\");\n+  LIR_Opr mdp = new_register(T_METADATA);\n+  __ metadata2reg(md->constant_encoding(), mdp);\n+  LIR_Address* addr = new LIR_Address(mdp, md->byte_offset_of_slot(data, DataLayout::flags_offset()), T_BYTE);\n+  LIR_Opr flags = new_register(T_INT);\n+  __ move(addr, flags);\n+  if (condition != lir_cond_always) {\n+    LIR_Opr update = new_register(T_INT);\n+    __ cmove(condition, LIR_OprFact::intConst(0), LIR_OprFact::intConst(flag), update, T_INT);\n+  } else {\n+    __ logical_or(flags, LIR_OprFact::intConst(flag), flags);\n+  }\n+  __ store(flags, addr);\n+}\n+\n+template <class ArrayData> void LIRGenerator::profile_null_free_array(LIRItem array, ciMethodData* md, ArrayData* load_store) {\n+  assert(compilation()->profile_array_accesses(), \"array access profiling is disabled\");\n+  LabelObj* L_end = new LabelObj();\n+  LIR_Opr tmp = new_register(T_METADATA);\n+  __ check_null_free_array(array.result(), tmp);\n+\n+  profile_flags(md, load_store, ArrayStoreData::null_free_array_byte_constant(), lir_cond_equal);\n+}\n+\n+template <class ArrayData> void LIRGenerator::profile_array_type(AccessIndexed* x, ciMethodData*& md, ArrayData*& load_store) {\n+  assert(compilation()->profile_array_accesses(), \"array access profiling is disabled\");\n+  LIR_Opr mdp = LIR_OprFact::illegalOpr;\n+  profile_type(md, md->byte_offset_of_slot(load_store, ArrayData::array_offset()), 0,\n+               load_store->array()->type(), x->array(), mdp, true, nullptr, nullptr);\n+}\n+\n+void LIRGenerator::profile_element_type(Value element, ciMethodData* md, ciArrayLoadData* load_data) {\n+  assert(compilation()->profile_array_accesses(), \"array access profiling is disabled\");\n+  assert(md != nullptr && load_data != nullptr, \"should have been initialized\");\n+  LIR_Opr mdp = LIR_OprFact::illegalOpr;\n+  profile_type(md, md->byte_offset_of_slot(load_data, ArrayLoadData::element_offset()), 0,\n+               load_data->element()->type(), element, mdp, false, nullptr, nullptr);\n+}\n+\n@@ -2605,0 +3032,8 @@\n+  if (method()->has_scalarized_args()) {\n+    \/\/ Check if deoptimization was triggered (i.e. orig_pc was set) while buffering scalarized inline type arguments\n+    \/\/ in the entry point (see comments in frame::deoptimize). If so, deoptimize only now that we have the right state.\n+    CodeEmitInfo* info = new CodeEmitInfo(scope()->start()->state()->copy(ValueStack::StateBefore, 0), nullptr, false);\n+    CodeStub* deopt_stub = new DeoptimizeStub(info, Deoptimization::Reason_none, Deoptimization::Action_none);\n+    __ append(new LIR_Op0(lir_check_orig_pc));\n+    __ branch(lir_cond_notEqual, deopt_stub);\n+  }\n@@ -2620,0 +3055,13 @@\n+void LIRGenerator::invoke_load_one_argument(LIRItem* param, LIR_Opr loc) {\n+  if (loc->is_register()) {\n+    param->load_item_force(loc);\n+  } else {\n+    LIR_Address* addr = loc->as_address_ptr();\n+    param->load_for_store(addr->type());\n+    if (addr->type() == T_OBJECT) {\n+      __ move_wide(param->result(), addr);\n+    } else {\n+      __ move(param->result(), addr);\n+    }\n+  }\n+}\n@@ -2627,10 +3075,1 @@\n-    if (loc->is_register()) {\n-      param->load_item_force(loc);\n-    } else {\n-      LIR_Address* addr = loc->as_address_ptr();\n-      param->load_for_store(addr->type());\n-      if (addr->type() == T_OBJECT) {\n-        __ move_wide(param->result(), addr);\n-      } else\n-        __ move(param->result(), addr);\n-    }\n+    invoke_load_one_argument(param, loc);\n@@ -2802,1 +3241,1 @@\n-  if (can_inline_as_constant(right.value())) {\n+  if (can_inline_as_constant(right.value()) && !x->substitutability_check()) {\n@@ -2805,0 +3244,1 @@\n+    \/\/ substitutability_check() needs to use right as a base register.\n@@ -2812,3 +3252,60 @@\n-  LIR_Opr reg = rlock_result(x);\n-  __ cmp(lir_cond(x->cond()), left.result(), right.result());\n-  __ cmove(lir_cond(x->cond()), t_val.result(), f_val.result(), reg, as_BasicType(x->x()->type()));\n+  if (x->substitutability_check()) {\n+    substitutability_check(x, left, right, t_val, f_val);\n+  } else {\n+    LIR_Opr reg = rlock_result(x);\n+    __ cmp(lir_cond(x->cond()), left.result(), right.result());\n+    __ cmove(lir_cond(x->cond()), t_val.result(), f_val.result(), reg, as_BasicType(x->x()->type()));\n+  }\n+}\n+\n+void LIRGenerator::substitutability_check(IfOp* x, LIRItem& left, LIRItem& right, LIRItem& t_val, LIRItem& f_val) {\n+  assert(x->cond() == If::eql || x->cond() == If::neq, \"must be\");\n+  bool is_acmpeq = (x->cond() == If::eql);\n+  LIR_Opr equal_result     = is_acmpeq ? t_val.result() : f_val.result();\n+  LIR_Opr not_equal_result = is_acmpeq ? f_val.result() : t_val.result();\n+  LIR_Opr result = rlock_result(x);\n+  CodeEmitInfo* info = state_for(x, x->state_before());\n+\n+  substitutability_check_common(x->x(), x->y(), left, right, equal_result, not_equal_result, result, info);\n+}\n+\n+void LIRGenerator::substitutability_check(If* x, LIRItem& left, LIRItem& right) {\n+  LIR_Opr equal_result     = LIR_OprFact::intConst(1);\n+  LIR_Opr not_equal_result = LIR_OprFact::intConst(0);\n+  LIR_Opr result = new_register(T_INT);\n+  CodeEmitInfo* info = state_for(x, x->state_before());\n+\n+  substitutability_check_common(x->x(), x->y(), left, right, equal_result, not_equal_result, result, info);\n+\n+  assert(x->cond() == If::eql || x->cond() == If::neq, \"must be\");\n+  __ cmp(lir_cond(x->cond()), result, equal_result);\n+}\n+\n+void LIRGenerator::substitutability_check_common(Value left_val, Value right_val, LIRItem& left, LIRItem& right,\n+                                                 LIR_Opr equal_result, LIR_Opr not_equal_result, LIR_Opr result,\n+                                                 CodeEmitInfo* info) {\n+  LIR_Opr tmp1 = LIR_OprFact::illegalOpr;\n+  LIR_Opr tmp2 = LIR_OprFact::illegalOpr;\n+  LIR_Opr left_klass_op = LIR_OprFact::illegalOpr;\n+  LIR_Opr right_klass_op = LIR_OprFact::illegalOpr;\n+\n+  ciKlass* left_klass  = left_val ->as_loaded_klass_or_null();\n+  ciKlass* right_klass = right_val->as_loaded_klass_or_null();\n+\n+  if ((left_klass == nullptr || right_klass == nullptr) ||\/\/ The klass is still unloaded, or came from a Phi node.\n+      !left_klass->is_inlinetype() || !right_klass->is_inlinetype()) {\n+    init_temps_for_substitutability_check(tmp1, tmp2);\n+  }\n+\n+  if (left_klass != nullptr && left_klass->is_inlinetype() && left_klass == right_klass) {\n+    \/\/ No need to load klass -- the operands are statically known to be the same inline klass.\n+  } else {\n+    BasicType t_klass = UseCompressedOops ? T_INT : T_METADATA;\n+    left_klass_op = new_register(t_klass);\n+    right_klass_op = new_register(t_klass);\n+  }\n+\n+  CodeStub* slow_path = new SubstitutabilityCheckStub(left.result(), right.result(), info);\n+  __ substitutability_check(result, left.result(), right.result(), equal_result, not_equal_result,\n+                            tmp1, tmp2,\n+                            left_klass, right_klass, left_klass_op, right_klass_op, info, slow_path);\n@@ -3088,1 +3585,1 @@\n-    ciReturnTypeEntry* ret = data->is_CallTypeData() ? ((ciCallTypeData*)data)->ret() : ((ciVirtualCallTypeData*)data)->ret();\n+    ciSingleTypeEntry* ret = data->is_CallTypeData() ? ((ciCallTypeData*)data)->ret() : ((ciVirtualCallTypeData*)data)->ret();\n@@ -3109,0 +3606,47 @@\n+bool LIRGenerator::profile_inline_klass(ciMethodData* md, ciProfileData* data, Value value, int flag) {\n+  ciKlass* klass = value->as_loaded_klass_or_null();\n+  if (klass != nullptr) {\n+    if (klass->is_inlinetype()) {\n+      profile_flags(md, data, flag, lir_cond_always);\n+    } else if (klass->can_be_inline_klass()) {\n+      return false;\n+    }\n+  } else {\n+    return false;\n+  }\n+  return true;\n+}\n+\n+\n+void LIRGenerator::do_ProfileACmpTypes(ProfileACmpTypes* x) {\n+  ciMethod* method = x->method();\n+  assert(method != nullptr, \"method should be set if branch is profiled\");\n+  ciMethodData* md = method->method_data_or_null();\n+  assert(md != nullptr, \"Sanity\");\n+  ciProfileData* data = md->bci_to_data(x->bci());\n+  assert(data != nullptr, \"must have profiling data\");\n+  assert(data->is_ACmpData(), \"need BranchData for two-way branches\");\n+  ciACmpData* acmp = (ciACmpData*)data;\n+  LIR_Opr mdp = LIR_OprFact::illegalOpr;\n+  profile_type(md, md->byte_offset_of_slot(acmp, ACmpData::left_offset()), 0,\n+               acmp->left()->type(), x->left(), mdp, !x->left_maybe_null(), nullptr, nullptr);\n+  int flags_offset = md->byte_offset_of_slot(data, DataLayout::flags_offset());\n+  if (!profile_inline_klass(md, acmp, x->left(), ACmpData::left_inline_type_byte_constant())) {\n+    LIR_Opr mdp = new_register(T_METADATA);\n+    __ metadata2reg(md->constant_encoding(), mdp);\n+    LIRItem value(x->left(), this);\n+    value.load_item();\n+    __ profile_inline_type(new LIR_Address(mdp, flags_offset, T_INT), value.result(), ACmpData::left_inline_type_byte_constant(), new_register(T_INT), !x->left_maybe_null());\n+  }\n+  profile_type(md, md->byte_offset_of_slot(acmp, ACmpData::left_offset()),\n+               in_bytes(ACmpData::right_offset()) - in_bytes(ACmpData::left_offset()),\n+               acmp->right()->type(), x->right(), mdp, !x->right_maybe_null(), nullptr, nullptr);\n+  if (!profile_inline_klass(md, acmp, x->right(), ACmpData::right_inline_type_byte_constant())) {\n+    LIR_Opr mdp = new_register(T_METADATA);\n+    __ metadata2reg(md->constant_encoding(), mdp);\n+    LIRItem value(x->right(), this);\n+    value.load_item();\n+    __ profile_inline_type(new LIR_Address(mdp, flags_offset, T_INT), value.result(), ACmpData::right_inline_type_byte_constant(), new_register(T_INT), !x->left_maybe_null());\n+  }\n+}\n+\n","filename":"src\/hotspot\/share\/c1\/c1_LIRGenerator.cpp","additions":602,"deletions":58,"binary":false,"changes":660,"status":"modified"},{"patch":"@@ -172,0 +172,1 @@\n+  bool          _in_conditional_code;\n@@ -198,0 +199,1 @@\n+  void set_in_conditional_code(bool v);\n@@ -217,0 +219,1 @@\n+  bool in_conditional_code() { return _in_conditional_code; }\n@@ -275,0 +278,13 @@\n+  void access_flat_array(bool is_load, LIRItem& array, LIRItem& index, LIRItem& obj_item, ciField* field = nullptr, int offset = 0);\n+  void access_sub_element(LIRItem& array, LIRItem& index, LIR_Opr& result, ciField* field, int sub_offset);\n+  LIR_Opr get_and_load_element_address(LIRItem& array, LIRItem& index);\n+  bool needs_flat_array_store_check(StoreIndexed* x);\n+  void check_flat_array(LIR_Opr array, LIR_Opr value, CodeStub* slow_path);\n+  bool needs_null_free_array_store_check(StoreIndexed* x);\n+  void check_null_free_array(LIRItem& array, LIRItem& value,  CodeEmitInfo* info);\n+  void substitutability_check(IfOp* x, LIRItem& left, LIRItem& right, LIRItem& t_val, LIRItem& f_val);\n+  void substitutability_check(If* x, LIRItem& left, LIRItem& right);\n+  void substitutability_check_common(Value left_val, Value right_val, LIRItem& left, LIRItem& right,\n+                                     LIR_Opr equal_result, LIR_Opr not_equal_result, LIR_Opr result, CodeEmitInfo* info);\n+  void init_temps_for_substitutability_check(LIR_Opr& tmp1, LIR_Opr& tmp2);\n+\n@@ -291,1 +307,1 @@\n-                       CodeEmitInfo* patch_info = nullptr, CodeEmitInfo* store_emit_info = nullptr);\n+                       CodeEmitInfo* patch_info = nullptr, CodeEmitInfo* store_emit_info = nullptr, ciInlineKlass* vk = nullptr);\n@@ -328,1 +344,1 @@\n-\n+  void invoke_load_one_argument(LIRItem* param, LIR_Opr loc);\n@@ -364,1 +380,1 @@\n-  void monitor_enter (LIR_Opr object, LIR_Opr lock, LIR_Opr hdr, LIR_Opr scratch, int monitor_no, CodeEmitInfo* info_for_exception, CodeEmitInfo* info);\n+  void monitor_enter (LIR_Opr object, LIR_Opr lock, LIR_Opr hdr, LIR_Opr scratch, int monitor_no, CodeEmitInfo* info_for_exception, CodeEmitInfo* info, CodeStub* throw_ie_stub);\n@@ -367,1 +383,1 @@\n-  void new_instance    (LIR_Opr  dst, ciInstanceKlass* klass, bool is_unresolved, LIR_Opr  scratch1, LIR_Opr  scratch2, LIR_Opr  scratch3,  LIR_Opr scratch4, LIR_Opr  klass_reg, CodeEmitInfo* info);\n+  void new_instance(LIR_Opr dst, ciInstanceKlass* klass, bool is_unresolved, bool allow_inline, LIR_Opr scratch1, LIR_Opr scratch2, LIR_Opr scratch3, LIR_Opr scratch4, LIR_Opr klass_reg, CodeEmitInfo* info);\n@@ -479,0 +495,5 @@\n+  void profile_flags(ciMethodData* md, ciProfileData* load_store, int flag, LIR_Condition condition = lir_cond_always);\n+  template <class ArrayData> void profile_null_free_array(LIRItem array, ciMethodData* md, ArrayData* load_store);\n+  template <class ArrayData> void profile_array_type(AccessIndexed* x, ciMethodData*& md, ArrayData*& load_store);\n+  void profile_element_type(Value element, ciMethodData* md, ciArrayLoadData* load_store);\n+  bool profile_inline_klass(ciMethodData* md, ciProfileData* data, Value value, int flag);\n@@ -506,0 +527,1 @@\n+    , _in_conditional_code(false)\n@@ -587,0 +609,1 @@\n+  virtual void do_ProfileACmpTypes(ProfileACmpTypes* x);\n","filename":"src\/hotspot\/share\/c1\/c1_LIRGenerator.hpp","additions":27,"deletions":4,"binary":false,"changes":31,"status":"modified"},{"patch":"@@ -65,1 +65,1 @@\n-static int type2spill_size[T_CONFLICT+1]={ -1, 0, 0, 0, 1, 1, 1, 2, 1, 1, 1, 2, 2, 2, 0, 2,  1, 2, 1, -1};\n+static int type2spill_size[T_CONFLICT+1]={ -1, 0, 0, 0, 1, 1, 1, 2, 1, 1, 1, 2, 2, 2, 2, 0, 2,  1, 2, 1, -1};\n@@ -67,1 +67,1 @@\n-static int type2spill_size[T_CONFLICT+1]={ -1, 0, 0, 0, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 0, 1, -1, 1, 1, -1};\n+static int type2spill_size[T_CONFLICT+1]={ -1, 0, 0, 0, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 1, 0, 1, -1, 1, 1, -1};\n@@ -259,1 +259,1 @@\n-  if (!frame_map()->finalize_frame(max_spills())) {\n+  if (!frame_map()->finalize_frame(max_spills(), compilation()->needs_stack_repair())) {\n@@ -2929,1 +2929,1 @@\n-  return new IRScopeDebugInfo(cur_scope, cur_state->bci(), locals, expressions, monitors, caller_debug_info);\n+  return new IRScopeDebugInfo(cur_scope, cur_state->bci(), locals, expressions, monitors, caller_debug_info, cur_state->should_reexecute());\n","filename":"src\/hotspot\/share\/c1\/c1_LinearScan.cpp","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -89,1 +89,2 @@\n-  Value make_ifop(Value x, Instruction::Condition cond, Value y, Value tval, Value fval);\n+  Value make_ifop(Value x, Instruction::Condition cond, Value y, Value tval, Value fval,\n+                  ValueStack* state_before, bool substitutability_check);\n@@ -218,1 +219,2 @@\n-  Value result = make_ifop(if_->x(), if_->cond(), if_->y(), t_value, f_value);\n+  Value result = make_ifop(if_->x(), if_->cond(), if_->y(), t_value, f_value,\n+                           if_->state_before(), if_->substitutability_check());\n@@ -273,1 +275,2 @@\n-Value CE_Eliminator::make_ifop(Value x, Instruction::Condition cond, Value y, Value tval, Value fval) {\n+Value CE_Eliminator::make_ifop(Value x, Instruction::Condition cond, Value y, Value tval, Value fval,\n+                               ValueStack* state_before, bool substitutability_check) {\n@@ -275,1 +278,1 @@\n-    return new IfOp(x, cond, y, tval, fval);\n+    return new IfOp(x, cond, y, tval, fval, state_before, substitutability_check);\n@@ -310,1 +313,1 @@\n-            return new IfOp(x_ifop->x(), x_ifop_cond, x_ifop->y(), new_tval, new_fval);\n+            return new IfOp(x_ifop->x(), x_ifop_cond, x_ifop->y(), new_tval, new_fval, state_before, substitutability_check);\n@@ -326,1 +329,1 @@\n-  return new IfOp(x, cond, y, tval, fval);\n+  return new IfOp(x, cond, y, tval, fval, state_before, substitutability_check);\n@@ -466,1 +469,1 @@\n-      if (con && ifop) {\n+      if (con && ifop && !ifop->substitutability_check()) {\n@@ -491,1 +494,1 @@\n-                                 tblock, fblock, if_->state_before(), if_->is_safepoint());\n+                                 tblock, fblock, if_->state_before(), if_->is_safepoint(), ifop->substitutability_check());\n@@ -585,0 +588,1 @@\n+  void do_ProfileACmpTypes(ProfileACmpTypes*  x);\n@@ -713,0 +717,1 @@\n+  void handle_ProfileACmpTypes(ProfileACmpTypes* x);\n@@ -771,0 +776,1 @@\n+void NullCheckVisitor::do_ProfileACmpTypes(ProfileACmpTypes* x) { nce()->handle_ProfileACmpTypes(x); }\n@@ -1200,0 +1206,5 @@\n+void NullCheckEliminator::handle_ProfileACmpTypes(ProfileACmpTypes* x) {\n+  x->set_left_maybe_null(!set_contains(x->left()));\n+  x->set_right_maybe_null(!set_contains(x->right()));\n+}\n+\n","filename":"src\/hotspot\/share\/c1\/c1_Optimizer.cpp","additions":19,"deletions":8,"binary":false,"changes":27,"status":"modified"},{"patch":"@@ -162,0 +162,1 @@\n+    void do_ProfileACmpTypes(ProfileACmpTypes*  x) { \/* nothing to do *\/ };\n","filename":"src\/hotspot\/share\/c1\/c1_RangeCheckElimination.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -52,0 +52,2 @@\n+#include \"oops\/flatArrayKlass.hpp\"\n+#include \"oops\/flatArrayOop.inline.hpp\"\n@@ -122,0 +124,1 @@\n+uint Runtime1::_new_null_free_array_slowcase_cnt = 0;\n@@ -124,0 +127,5 @@\n+uint Runtime1::_load_flat_array_slowcase_cnt = 0;\n+uint Runtime1::_store_flat_array_slowcase_cnt = 0;\n+uint Runtime1::_substitutability_check_slowcase_cnt = 0;\n+uint Runtime1::_buffer_inline_args_slowcase_cnt = 0;\n+uint Runtime1::_buffer_inline_args_no_receiver_slowcase_cnt = 0;\n@@ -133,0 +141,2 @@\n+uint Runtime1::_throw_illegal_monitor_state_exception_count = 0;\n+uint Runtime1::_throw_identity_exception_count = 0;\n@@ -358,2 +368,1 @@\n-\n-JRT_ENTRY(void, Runtime1::new_instance(JavaThread* current, Klass* klass))\n+static void allocate_instance(JavaThread* current, Klass* klass, TRAPS) {\n@@ -362,1 +371,1 @@\n-    _new_instance_slowcase_cnt++;\n+    Runtime1::_new_instance_slowcase_cnt++;\n@@ -376,0 +385,3 @@\n+JRT_ENTRY(void, Runtime1::new_instance(JavaThread* current, Klass* klass))\n+  allocate_instance(current, klass, CHECK);\n+JRT_END\n@@ -410,1 +422,1 @@\n-  Klass* elem_klass = ObjArrayKlass::cast(array_klass)->element_klass();\n+  Klass* elem_klass = ArrayKlass::cast(array_klass)->element_klass();\n@@ -421,0 +433,29 @@\n+JRT_ENTRY(void, Runtime1::new_null_free_array(JavaThread* current, Klass* array_klass, jint length))\n+  NOT_PRODUCT(_new_null_free_array_slowcase_cnt++;)\n+  \/\/ TODO 8350865 This is dead code since 8325660 because null-free arrays can only be created via the factory methods that are not yet implemented in C1. Should probably be fixed by 8265122.\n+\n+  \/\/ Note: no handle for klass needed since they are not used\n+  \/\/       anymore after new_objArray() and no GC can happen before.\n+  \/\/       (This may have to change if this code changes!)\n+  assert(array_klass->is_klass(), \"not a class\");\n+  Handle holder(THREAD, array_klass->klass_holder()); \/\/ keep the klass alive\n+  Klass* elem_klass = ArrayKlass::cast(array_klass)->element_klass();\n+  assert(elem_klass->is_inline_klass(), \"must be\");\n+  InlineKlass* vk = InlineKlass::cast(elem_klass);\n+  \/\/ Logically creates elements, ensure klass init\n+  elem_klass->initialize(CHECK);\n+  arrayOop obj= nullptr;\n+  if (UseArrayFlattening && vk->has_non_atomic_layout()) {\n+    obj = oopFactory::new_flatArray(elem_klass, length, LayoutKind::NON_ATOMIC_FLAT, CHECK);\n+  } else {\n+    obj = oopFactory::new_null_free_objArray(elem_klass, length, CHECK);\n+  }\n+  current->set_vm_result(obj);\n+  \/\/ This is pretty rare but this runtime patch is stressful to deoptimization\n+  \/\/ if we deoptimize here so force a deopt to stress the path.\n+  if (DeoptimizeALot) {\n+    deopt_caller(current);\n+  }\n+JRT_END\n+\n+\n@@ -435,0 +476,96 @@\n+static void profile_flat_array(JavaThread* current, bool load, bool null_free) {\n+  ResourceMark rm(current);\n+  vframeStream vfst(current, true);\n+  assert(!vfst.at_end(), \"Java frame must exist\");\n+  \/\/ Check if array access profiling is enabled\n+  if (vfst.nm()->comp_level() != CompLevel_full_profile || !C1UpdateMethodData) {\n+    return;\n+  }\n+  int bci = vfst.bci();\n+  Method* method = vfst.method();\n+  MethodData* md = method->method_data();\n+  if (md != nullptr) {\n+    \/\/ Lock to access ProfileData, and ensure lock is not broken by a safepoint\n+    MutexLocker ml(md->extra_data_lock(), Mutex::_no_safepoint_check_flag);\n+\n+    ProfileData* data = md->bci_to_data(bci);\n+    assert(data != nullptr, \"incorrect profiling entry\");\n+    if (data->is_ArrayLoadData()) {\n+      assert(load, \"should be an array load\");\n+      ArrayLoadData* load_data = (ArrayLoadData*) data;\n+      load_data->set_flat_array();\n+      if (null_free) {\n+        load_data->set_null_free_array();\n+      }\n+    } else {\n+      assert(data->is_ArrayStoreData(), \"\");\n+      assert(!load, \"should be an array store\");\n+      ArrayStoreData* store_data = (ArrayStoreData*) data;\n+      store_data->set_flat_array();\n+      if (null_free) {\n+        store_data->set_null_free_array();\n+      }\n+    }\n+  }\n+}\n+\n+JRT_ENTRY(void, Runtime1::load_flat_array(JavaThread* current, flatArrayOopDesc* array, int index))\n+  assert(array->klass()->is_flatArray_klass(), \"should not be called\");\n+  profile_flat_array(current, true, array->is_null_free_array());\n+\n+  NOT_PRODUCT(_load_flat_array_slowcase_cnt++;)\n+  assert(array->length() > 0 && index < array->length(), \"already checked\");\n+  flatArrayHandle vah(current, array);\n+  oop obj = array->read_value_from_flat_array(index, CHECK);\n+  current->set_vm_result(obj);\n+JRT_END\n+\n+JRT_ENTRY(void, Runtime1::store_flat_array(JavaThread* current, flatArrayOopDesc* array, int index, oopDesc* value))\n+  \/\/ TOOD 8350865 We can call here with a non-flat array because of LIR_Assembler::emit_opFlattenedArrayCheck\n+  if (array->klass()->is_flatArray_klass()) {\n+    profile_flat_array(current, false, array->is_null_free_array());\n+  }\n+\n+  NOT_PRODUCT(_store_flat_array_slowcase_cnt++;)\n+  if (value == nullptr && array->is_null_free_array()) {\n+    SharedRuntime::throw_and_post_jvmti_exception(current, vmSymbols::java_lang_NullPointerException());\n+  } else {\n+    assert(array->klass()->is_flatArray_klass(), \"should not be called\");\n+    array->write_value_to_flat_array(value, index, CHECK);\n+  }\n+JRT_END\n+\n+JRT_ENTRY(int, Runtime1::substitutability_check(JavaThread* current, oopDesc* left, oopDesc* right))\n+  NOT_PRODUCT(_substitutability_check_slowcase_cnt++;)\n+  JavaCallArguments args;\n+  args.push_oop(Handle(THREAD, left));\n+  args.push_oop(Handle(THREAD, right));\n+  JavaValue result(T_BOOLEAN);\n+  JavaCalls::call_static(&result,\n+                         vmClasses::ValueObjectMethods_klass(),\n+                         vmSymbols::isSubstitutable_name(),\n+                         vmSymbols::object_object_boolean_signature(),\n+                         &args, CHECK_0);\n+  return result.get_jboolean() ? 1 : 0;\n+JRT_END\n+\n+\n+extern \"C\" void ps();\n+\n+void Runtime1::buffer_inline_args_impl(JavaThread* current, Method* m, bool allocate_receiver) {\n+  JavaThread* THREAD = current;\n+  methodHandle method(current, m); \/\/ We are inside the verified_entry or verified_inline_ro_entry of this method.\n+  oop obj = SharedRuntime::allocate_inline_types_impl(current, method, allocate_receiver, CHECK);\n+  current->set_vm_result(obj);\n+}\n+\n+JRT_ENTRY(void, Runtime1::buffer_inline_args(JavaThread* current, Method* method))\n+  NOT_PRODUCT(_buffer_inline_args_slowcase_cnt++;)\n+  buffer_inline_args_impl(current, method, true);\n+JRT_END\n+\n+JRT_ENTRY(void, Runtime1::buffer_inline_args_no_receiver(JavaThread* current, Method* method))\n+  NOT_PRODUCT(_buffer_inline_args_no_receiver_slowcase_cnt++;)\n+  buffer_inline_args_impl(current, method, false);\n+JRT_END\n+\n@@ -758,0 +895,13 @@\n+JRT_ENTRY(void, Runtime1::throw_illegal_monitor_state_exception(JavaThread* current))\n+  NOT_PRODUCT(_throw_illegal_monitor_state_exception_count++;)\n+  ResourceMark rm(current);\n+  SharedRuntime::throw_and_post_jvmti_exception(current, vmSymbols::java_lang_IllegalMonitorStateException());\n+JRT_END\n+\n+JRT_ENTRY(void, Runtime1::throw_identity_exception(JavaThread* current, oopDesc* object))\n+  NOT_PRODUCT(_throw_identity_exception_count++;)\n+  ResourceMark rm(current);\n+  char* message = SharedRuntime::generate_identity_exception_message(current, object->klass());\n+  SharedRuntime::throw_and_post_jvmti_exception(current, vmSymbols::java_lang_IdentityException(), message);\n+JRT_END\n+\n@@ -963,0 +1113,2 @@\n+  bool deoptimize_for_null_free = false;\n+  bool deoptimize_for_flat = false;\n@@ -1006,0 +1158,10 @@\n+    \/\/ The field we are patching is null-free. Deoptimize and regenerate\n+    \/\/ the compiled code if we patch a putfield\/putstatic because it\n+    \/\/ does not contain the required null check.\n+    deoptimize_for_null_free = result.is_null_free_inline_type() && (field_access.is_putfield() || field_access.is_putstatic());\n+\n+    \/\/ The field we are patching is flat. Deoptimize and regenerate\n+    \/\/ the compiled code which can't handle the layout of the flat\n+    \/\/ field because it was unknown at compile time.\n+    deoptimize_for_flat = result.is_flat();\n+\n@@ -1078,1 +1240,1 @@\n-  if (deoptimize_for_volatile || deoptimize_for_atomic) {\n+  if (deoptimize_for_volatile || deoptimize_for_atomic || deoptimize_for_null_free || deoptimize_for_flat) {\n@@ -1089,0 +1251,6 @@\n+      if (deoptimize_for_null_free) {\n+        tty->print_cr(\"Deoptimizing for patching null-free field reference\");\n+      }\n+      if (deoptimize_for_flat) {\n+        tty->print_cr(\"Deoptimizing for patching flat field reference\");\n+      }\n@@ -1539,0 +1707,1 @@\n+  tty->print_cr(\" _new_null_free_array_slowcase_cnt: %u\", _new_null_free_array_slowcase_cnt);\n@@ -1541,0 +1710,6 @@\n+  tty->print_cr(\" _load_flat_array_slowcase_cnt:   %u\", _load_flat_array_slowcase_cnt);\n+  tty->print_cr(\" _store_flat_array_slowcase_cnt:  %u\", _store_flat_array_slowcase_cnt);\n+  tty->print_cr(\" _substitutability_check_slowcase_cnt: %u\", _substitutability_check_slowcase_cnt);\n+  tty->print_cr(\" _buffer_inline_args_slowcase_cnt:%u\", _buffer_inline_args_slowcase_cnt);\n+  tty->print_cr(\" _buffer_inline_args_no_receiver_slowcase_cnt:%u\", _buffer_inline_args_no_receiver_slowcase_cnt);\n+\n@@ -1551,0 +1726,2 @@\n+  tty->print_cr(\" _throw_illegal_monitor_state_exception_count:  %u:\", _throw_illegal_monitor_state_exception_count);\n+  tty->print_cr(\" _throw_identity_exception_count:               %u:\", _throw_identity_exception_count);\n","filename":"src\/hotspot\/share\/c1\/c1_Runtime1.cpp","additions":182,"deletions":5,"binary":false,"changes":187,"status":"modified"},{"patch":"@@ -150,0 +150,3 @@\n+      if (x->enclosing_field() != nullptr) {\n+        kill_field(x->enclosing_field(), true);\n+      }\n@@ -208,0 +211,1 @@\n+  void do_ProfileACmpTypes(ProfileACmpTypes*  x) { \/* nothing to do *\/ }\n","filename":"src\/hotspot\/share\/c1\/c1_ValueMap.hpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -37,0 +37,1 @@\n+, _should_reexecute(false)\n@@ -45,1 +46,1 @@\n-ValueStack::ValueStack(ValueStack* copy_from, Kind kind, int bci)\n+ValueStack::ValueStack(ValueStack* copy_from, Kind kind, int bci, bool reexecute)\n@@ -50,0 +51,1 @@\n+  , _should_reexecute(reexecute)\n@@ -213,1 +215,0 @@\n-\n","filename":"src\/hotspot\/share\/c1\/c1_ValueStack.cpp","additions":3,"deletions":2,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -312,1 +312,4 @@\n-          \"print control flow graph to a separate file during compilation\")\n+          \"print control flow graph to a separate file during compilation\") \\\n+                                                                            \\\n+  develop(bool, C1UseDelayedFlattenedFieldReads, true,                      \\\n+          \"Use delayed reads of flat fields to reduce heap buffering\")\n","filename":"src\/hotspot\/share\/c1\/c1_globals.hpp","additions":4,"deletions":1,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -35,0 +35,1 @@\n+#include \"runtime\/globals.hpp\"\n@@ -101,0 +102,31 @@\n+#define CDS_MUST_MATCH_FLAGS_DO(f) \\\n+  f(EnableValhalla) \\\n+  f(UseArrayFlattening) \\\n+  f(UseFieldFlattening) \\\n+  f(InlineTypePassFieldsAsArgs) \\\n+  f(InlineTypeReturnedAsFields) \\\n+  f(UseNonAtomicValueFlattening) \\\n+  f(UseAtomicValueFlattening) \\\n+  f(UseNullableValueFlattening)\n+\n+\n+class CDSMustMatchFlags {\n+private:\n+  size_t _max_name_width;\n+#define DECLARE_CDS_MUST_MATCH_FLAG(n) \\\n+  decltype(n) _v_##n;\n+  CDS_MUST_MATCH_FLAGS_DO(DECLARE_CDS_MUST_MATCH_FLAG);\n+#undef DECLARE_CDS_MUST_MATCH_FLAG\n+\n+  inline static void do_print(outputStream* st, bool v);\n+  inline static void do_print(outputStream* st, intx v);\n+  inline static void do_print(outputStream* st, uintx v);\n+  inline static void do_print(outputStream* st, double v);\n+  void print_info() const;\n+\n+public:\n+  void init();\n+  bool runtime_check() const;\n+  void print(outputStream* st) const;\n+};\n+\n@@ -143,0 +175,2 @@\n+  bool   _has_valhalla_patched_classes; \/\/ Is this archived dumped with --enable-preview -XX:+EnableValhalla?\n+  CDSMustMatchFlags _must_match;        \/\/ These flags must be the same between dumptime and runtime\n@@ -239,0 +273,4 @@\n+  bool check_must_match_flags() const {\n+    return _must_match.runtime_check();\n+  }\n+\n","filename":"src\/hotspot\/share\/cds\/filemap.hpp","additions":38,"deletions":0,"binary":false,"changes":38,"status":"modified"},{"patch":"@@ -76,0 +76,2 @@\n+#include \"oops\/flatArrayKlass.hpp\"\n+#include \"oops\/inlineKlass.hpp\"\n@@ -120,1 +122,1 @@\n-\/\/ [0] All classes are loaded in MetaspaceShared::preload_classes(). All metadata are\n+\/\/ [0] All classes are loaded in MetaspaceShared::loadable_descriptors(). All metadata are\n@@ -862,1 +864,1 @@\n-void MetaspaceShared::preload_classes(TRAPS) {\n+void MetaspaceShared::loadable_descriptors(TRAPS) {\n@@ -909,1 +911,1 @@\n-    preload_classes(CHECK);\n+    loadable_descriptors(CHECK);\n@@ -1064,0 +1066,5 @@\n+  if (CDSConfig::is_valhalla_preview()) {\n+    log_info(cds)(\"Archived java heap is not yet supported with Valhalla preview\");\n+    return;\n+  }\n+\n","filename":"src\/hotspot\/share\/cds\/metaspaceShared.cpp","additions":10,"deletions":3,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -83,1 +83,1 @@\n-  static void preload_classes(TRAPS) NOT_CDS_RETURN;\n+  static void loadable_descriptors(TRAPS) NOT_CDS_RETURN;\n","filename":"src\/hotspot\/share\/cds\/metaspaceShared.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -557,0 +557,3 @@\n+        \/\/ If the array is a flat array, a larger part of it is modified than\n+        \/\/ the size of a reference. However, if OFFSET_ANY is given as\n+        \/\/ parameter to set_modified(), size is not taken into account.\n","filename":"src\/hotspot\/share\/ci\/bcEscapeAnalyzer.cpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -26,0 +26,2 @@\n+#include \"ci\/ciFlatArrayKlass.hpp\"\n+#include \"ci\/ciInlineKlass.hpp\"\n@@ -29,0 +31,1 @@\n+#include \"oops\/inlineKlass.inline.hpp\"\n@@ -62,1 +65,1 @@\n-    return as_obj_array_klass()->element_klass()->as_klass();\n+    return element_klass()->as_klass();\n@@ -74,1 +77,1 @@\n-  } else {\n+  } else if (is_obj_array_klass()) {\n@@ -80,0 +83,2 @@\n+  } else {\n+    return as_flat_array_klass()->base_element_klass();\n@@ -99,1 +104,1 @@\n-ciArrayKlass* ciArrayKlass::make(ciType* element_type) {\n+ciArrayKlass* ciArrayKlass::make(ciType* element_type, bool flat, bool null_free, bool atomic) {\n@@ -102,2 +107,45 @@\n-  } else {\n-    return ciObjArrayKlass::make(element_type->as_klass());\n+\n+  ciKlass* klass = element_type->as_klass();\n+  assert(!null_free || !klass->is_loaded() || klass->is_inlinetype() || klass->is_abstract() ||\n+         klass->is_java_lang_Object(), \"only value classes are null free\");\n+  if (klass->is_loaded() && klass->is_inlinetype()) {\n+    GUARDED_VM_ENTRY(\n+      EXCEPTION_CONTEXT;\n+      Klass* ak = nullptr;\n+      InlineKlass* vk = InlineKlass::cast(klass->get_Klass());\n+      if (flat && vk->flat_array()) {\n+        LayoutKind lk;\n+        if (null_free) {\n+          if (vk->is_naturally_atomic()) {\n+            atomic = vk->has_atomic_layout();\n+          }\n+          if (!atomic && !vk->has_non_atomic_layout()) {\n+            \/\/ TODO 8350865 Impossible type\n+            lk = vk->has_atomic_layout() ? LayoutKind::ATOMIC_FLAT : LayoutKind::NULLABLE_ATOMIC_FLAT;\n+          } else {\n+            lk = atomic ? LayoutKind::ATOMIC_FLAT : LayoutKind::NON_ATOMIC_FLAT;\n+          }\n+        } else {\n+          if (!vk->has_nullable_atomic_layout()) {\n+            \/\/ TODO 8350865 Impossible type, null-able flat is always atomic.\n+            lk = vk->has_atomic_layout() ? LayoutKind::ATOMIC_FLAT : LayoutKind::NON_ATOMIC_FLAT;\n+          } else {\n+            lk = LayoutKind::NULLABLE_ATOMIC_FLAT;\n+          }\n+        }\n+        ak = vk->flat_array_klass(lk, THREAD);\n+      } else if (null_free) {\n+        ak = vk->null_free_reference_array(THREAD);\n+      } else {\n+        return ciObjArrayKlass::make(klass);\n+      }\n+      if (HAS_PENDING_EXCEPTION) {\n+        CLEAR_PENDING_EXCEPTION;\n+      } else if (ak->is_flatArray_klass()) {\n+        return CURRENT_THREAD_ENV->get_flat_array_klass(ak);\n+      } else if (ak->is_objArray_klass()) {\n+        return CURRENT_THREAD_ENV->get_obj_array_klass(ak);\n+      }\n+    )\n+  }\n+  return ciObjArrayKlass::make(klass);\n@@ -107,0 +155,14 @@\n+int ciArrayKlass::array_header_in_bytes() {\n+  return get_ArrayKlass()->array_header_in_bytes();\n+}\n+\n+ciInstance* ciArrayKlass::component_mirror_instance() const {\n+  GUARDED_VM_ENTRY(\n+    oop component_mirror = ArrayKlass::cast(get_Klass())->component_mirror();\n+    return CURRENT_ENV->get_instance(component_mirror);\n+  )\n+}\n+\n+bool ciArrayKlass::is_elem_null_free() const {\n+  GUARDED_VM_ENTRY(return get_Klass()->is_null_free_array_klass();)\n+}\n","filename":"src\/hotspot\/share\/ci\/ciArrayKlass.cpp","additions":67,"deletions":5,"binary":false,"changes":72,"status":"modified"},{"patch":"@@ -28,0 +28,1 @@\n+#include \"ci\/ciInlineKlass.hpp\"\n@@ -479,1 +480,2 @@\n-      (sym->char_at(1) == JVM_SIGNATURE_ARRAY || sym->char_at(1) == JVM_SIGNATURE_CLASS)) {\n+      (sym->char_at(1) == JVM_SIGNATURE_ARRAY ||\n+       sym->char_at(1) == JVM_SIGNATURE_CLASS )) {\n@@ -492,1 +494,1 @@\n-      return ciObjArrayKlass::make_impl(elem_klass);\n+      return ciArrayKlass::make(elem_klass);\n@@ -518,0 +520,4 @@\n+  int i = 0;\n+  while (sym->char_at(i) == JVM_SIGNATURE_ARRAY) {\n+    i++;\n+  }\n","filename":"src\/hotspot\/share\/ci\/ciEnv.cpp","additions":8,"deletions":2,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -202,0 +202,4 @@\n+  ciFlatArrayKlass* get_flat_array_klass(Klass* o) {\n+    if (o == nullptr) return nullptr;\n+    return get_metadata(o)->as_flat_array_klass();\n+  }\n@@ -503,0 +507,4 @@\n+  ciWrapper* make_null_free_wrapper(ciType* type) {\n+    return _factory->make_null_free_wrapper(type);\n+  }\n+\n","filename":"src\/hotspot\/share\/ci\/ciEnv.hpp","additions":8,"deletions":0,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -42,1 +42,1 @@\n-ciType* ciInstance::java_mirror_type() {\n+ciType* ciInstance::java_mirror_type(bool* is_null_free_array) {\n@@ -55,0 +55,3 @@\n+    if (is_null_free_array != nullptr && (k->is_array_klass() && k->is_null_free_array_klass())) {\n+      *is_null_free_array = true;\n+    }\n@@ -110,1 +113,3 @@\n-  assert(field->is_static() || klass()->is_subclass_of(field->holder()), \"invalid access - must be subclass\");\n+  assert(field->is_static() || field->holder()->is_inlinetype() || klass()->is_subclass_of(field->holder()),\n+         \"invalid access - must be subclass\");\n+\n","filename":"src\/hotspot\/share\/ci\/ciInstance.cpp","additions":7,"deletions":2,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -26,0 +26,1 @@\n+#include \"ci\/ciInlineKlass.hpp\"\n@@ -30,0 +31,1 @@\n+#include \"classfile\/systemDictionary.hpp\"\n@@ -35,0 +37,1 @@\n+#include \"oops\/inlineKlass.inline.hpp\"\n@@ -117,2 +120,3 @@\n-                                 jobject loader)\n-  : ciKlass(name, T_OBJECT)\n+                                 jobject loader,\n+                                 BasicType bt)\n+  : ciKlass(name, bt)\n@@ -123,1 +127,1 @@\n-  _nonstatic_fields = nullptr;\n+  _nonstatic_fields = nullptr;         \/\/ initialized lazily by compute_nonstatic_fields\n@@ -321,1 +325,1 @@\n-    _flags.print_klass_flags();\n+    _flags.print_klass_flags(st);\n@@ -325,1 +329,1 @@\n-      _super->print_name();\n+      _super->print_name_on(st);\n@@ -416,0 +420,23 @@\n+ciField* ciInstanceKlass::get_non_flat_field_by_offset(int field_offset) {\n+  if (super() != nullptr && super()->has_nonstatic_fields()) {\n+    ciField* f = super()->get_non_flat_field_by_offset(field_offset);\n+    if (f != nullptr) {\n+      return f;\n+    }\n+  }\n+\n+  VM_ENTRY_MARK;\n+  InstanceKlass* k = get_instanceKlass();\n+  Arena* arena = CURRENT_ENV->arena();\n+  for (JavaFieldStream fs(k); !fs.done(); fs.next()) {\n+    if (fs.access_flags().is_static())  continue;\n+    fieldDescriptor& fd = fs.field_descriptor();\n+    if (fd.offset() == field_offset) {\n+      ciField* f = new (arena) ciField(&fd);\n+      return f;\n+    }\n+  }\n+\n+  return nullptr;\n+}\n+\n@@ -430,0 +457,4 @@\n+static int sort_field_by_offset(ciField** a, ciField** b) {\n+  return (*a)->offset_in_bytes() - (*b)->offset_in_bytes();\n+  \/\/ (no worries about 32-bit overflow...)\n+}\n@@ -469,3 +500,1 @@\n-  int flen = fields->length();\n-\n-  return flen;\n+  return fields->length();\n@@ -475,3 +504,1 @@\n-GrowableArray<ciField*>*\n-ciInstanceKlass::compute_nonstatic_fields_impl(GrowableArray<ciField*>*\n-                                               super_fields) {\n+GrowableArray<ciField*>* ciInstanceKlass::compute_nonstatic_fields_impl(GrowableArray<ciField*>* super_fields, bool is_flat) {\n@@ -489,1 +516,1 @@\n-  if (flen == 0) {\n+  if (flen == 0 && !is_inlinetype()) {\n@@ -503,2 +530,22 @@\n-    ciField* field = new (arena) ciField(&fd);\n-    fields->append(field);\n+    if (fd.is_flat() && is_flat) {\n+      \/\/ Inline type fields are embedded\n+      int field_offset = fd.offset();\n+      \/\/ Get InlineKlass and adjust number of fields\n+      Klass* k = get_instanceKlass()->get_inline_type_field_klass(fd.index());\n+      ciInlineKlass* vk = CURRENT_ENV->get_klass(k)->as_inline_klass();\n+      flen += vk->nof_nonstatic_fields() - 1;\n+      \/\/ Iterate over fields of the flat inline type and copy them to 'this'\n+      for (int i = 0; i < vk->nof_nonstatic_fields(); ++i) {\n+        ciField* flat_field = vk->nonstatic_field_at(i);\n+        \/\/ Adjust offset to account for missing oop header\n+        int offset = field_offset + (flat_field->offset_in_bytes() - vk->payload_offset());\n+        \/\/ A flat field can be treated as final if the non-flat\n+        \/\/ field is declared final or the holder klass is an inline type itself.\n+        bool is_final = fd.is_final() || is_inlinetype();\n+        ciField* field = new (arena) ciField(flat_field, this, offset, is_final);\n+        fields->append(field);\n+      }\n+    } else {\n+      ciField* field = new (arena) ciField(&fd);\n+      fields->append(field);\n+    }\n@@ -507,0 +554,1 @@\n+  fields->sort(sort_field_by_offset);\n@@ -613,0 +661,16 @@\n+bool ciInstanceKlass::can_be_inline_klass(bool is_exact) {\n+  if (!EnableValhalla) {\n+    return false;\n+  }\n+  if (!is_loaded() || is_inlinetype()) {\n+    \/\/ Not loaded or known to be an inline klass\n+    return true;\n+  }\n+  if (!is_exact) {\n+    \/\/ Not exact, check if this is a valid super for an inline klass\n+    VM_ENTRY_MARK;\n+    return !get_instanceKlass()->access_flags().is_identity_class() || is_java_lang_Object() ;\n+  }\n+  return false;\n+}\n+\n@@ -621,1 +685,2 @@\n-class StaticFinalFieldPrinter : public FieldClosure {\n+class StaticFieldPrinter : public FieldClosure {\n+protected:\n@@ -623,0 +688,8 @@\n+public:\n+  StaticFieldPrinter(outputStream* out) :\n+    _out(out) {\n+  }\n+  void do_field_helper(fieldDescriptor* fd, oop obj, bool is_flat);\n+};\n+\n+class StaticFinalFieldPrinter : public StaticFieldPrinter {\n@@ -626,2 +699,1 @@\n-    _out(out),\n-    _holder(holder) {\n+    StaticFieldPrinter(out), _holder(holder) {\n@@ -632,46 +704,59 @@\n-      oop mirror = fd->field_holder()->java_mirror();\n-      _out->print(\"staticfield %s %s %s \", _holder, fd->name()->as_quoted_ascii(), fd->signature()->as_quoted_ascii());\n-      BasicType field_type = fd->field_type();\n-      switch (field_type) {\n-        case T_BYTE:    _out->print_cr(\"%d\", mirror->byte_field(fd->offset()));   break;\n-        case T_BOOLEAN: _out->print_cr(\"%d\", mirror->bool_field(fd->offset()));   break;\n-        case T_SHORT:   _out->print_cr(\"%d\", mirror->short_field(fd->offset()));  break;\n-        case T_CHAR:    _out->print_cr(\"%d\", mirror->char_field(fd->offset()));   break;\n-        case T_INT:     _out->print_cr(\"%d\", mirror->int_field(fd->offset()));    break;\n-        case T_LONG:    _out->print_cr(INT64_FORMAT, (int64_t)(mirror->long_field(fd->offset())));   break;\n-        case T_FLOAT: {\n-          float f = mirror->float_field(fd->offset());\n-          _out->print_cr(\"%d\", *(int*)&f);\n-          break;\n-        }\n-        case T_DOUBLE: {\n-          double d = mirror->double_field(fd->offset());\n-          _out->print_cr(INT64_FORMAT, *(int64_t*)&d);\n-          break;\n-        }\n-        case T_ARRAY:  \/\/ fall-through\n-        case T_OBJECT: {\n-          oop value =  mirror->obj_field_acquire(fd->offset());\n-          if (value == nullptr) {\n-            if (field_type == T_ARRAY) {\n-              _out->print(\"%d\", -1);\n-            }\n-            _out->cr();\n-          } else if (value->is_instance()) {\n-            assert(field_type == T_OBJECT, \"\");\n-            if (value->is_a(vmClasses::String_klass())) {\n-              const char* ascii_value = java_lang_String::as_quoted_ascii(value);\n-              _out->print_cr(\"\\\"%s\\\"\", (ascii_value != nullptr) ? ascii_value : \"\");\n-            } else {\n-              const char* klass_name  = value->klass()->name()->as_quoted_ascii();\n-              _out->print_cr(\"%s\", klass_name);\n-            }\n-          } else if (value->is_array()) {\n-            typeArrayOop ta = (typeArrayOop)value;\n-            _out->print(\"%d\", ta->length());\n-            if (value->is_objArray()) {\n-              objArrayOop oa = (objArrayOop)value;\n-              const char* klass_name  = value->klass()->name()->as_quoted_ascii();\n-              _out->print(\" %s\", klass_name);\n-            }\n-            _out->cr();\n+      InstanceKlass* holder = fd->field_holder();\n+      oop mirror = holder->java_mirror();\n+      _out->print(\"staticfield %s %s \", _holder, fd->name()->as_quoted_ascii());\n+      BasicType bt = fd->field_type();\n+      if (bt != T_OBJECT && bt != T_ARRAY) {\n+        _out->print(\"%s \", fd->signature()->as_quoted_ascii());\n+      }\n+      do_field_helper(fd, mirror, false);\n+      _out->cr();\n+    }\n+  }\n+};\n+\n+class InlineTypeFieldPrinter : public StaticFieldPrinter {\n+  oop _obj;\n+public:\n+  InlineTypeFieldPrinter(outputStream* out, oop obj) :\n+    StaticFieldPrinter(out), _obj(obj) {\n+  }\n+  void do_field(fieldDescriptor* fd) {\n+    do_field_helper(fd, _obj, true);\n+    _out->print(\" \");\n+  }\n+};\n+\n+void StaticFieldPrinter::do_field_helper(fieldDescriptor* fd, oop mirror, bool is_flat) {\n+  BasicType field_type = fd->field_type();\n+  switch (field_type) {\n+    case T_BYTE:    _out->print(\"%d\", mirror->byte_field(fd->offset()));   break;\n+    case T_BOOLEAN: _out->print(\"%d\", mirror->bool_field(fd->offset()));   break;\n+    case T_SHORT:   _out->print(\"%d\", mirror->short_field(fd->offset()));  break;\n+    case T_CHAR:    _out->print(\"%d\", mirror->char_field(fd->offset()));   break;\n+    case T_INT:     _out->print(\"%d\", mirror->int_field(fd->offset()));    break;\n+    case T_LONG:    _out->print(INT64_FORMAT, (int64_t)(mirror->long_field(fd->offset())));   break;\n+    case T_FLOAT: {\n+      float f = mirror->float_field(fd->offset());\n+      _out->print(\"%d\", *(int*)&f);\n+      break;\n+    }\n+    case T_DOUBLE: {\n+      double d = mirror->double_field(fd->offset());\n+      _out->print(INT64_FORMAT, *(int64_t*)&d);\n+      break;\n+    }\n+    case T_ARRAY:  \/\/ fall-through\n+    case T_OBJECT:\n+      if (!fd->is_null_free_inline_type()) {\n+        _out->print(\"%s \", fd->signature()->as_quoted_ascii());\n+        oop value =  mirror->obj_field_acquire(fd->offset());\n+        if (value == nullptr) {\n+          if (field_type == T_ARRAY) {\n+            _out->print(\"%d\", -1);\n+          }\n+          _out->cr();\n+        } else if (value->is_instance()) {\n+          assert(field_type == T_OBJECT, \"\");\n+          if (value->is_a(vmClasses::String_klass())) {\n+            const char* ascii_value = java_lang_String::as_quoted_ascii(value);\n+            _out->print(\"\\\"%s\\\"\", (ascii_value != nullptr) ? ascii_value : \"\");\n@@ -679,1 +764,2 @@\n-            ShouldNotReachHere();\n+            const char* klass_name  = value->klass()->name()->as_quoted_ascii();\n+            _out->print(\"%s\", klass_name);\n@@ -681,3 +767,9 @@\n-          break;\n-        }\n-        default:\n+        } else if (value->is_array()) {\n+          typeArrayOop ta = (typeArrayOop)value;\n+          _out->print(\"%d\", ta->length());\n+          if (value->is_objArray() || value->is_flatArray()) {\n+            objArrayOop oa = (objArrayOop)value;\n+            const char* klass_name  = value->klass()->name()->as_quoted_ascii();\n+            _out->print(\" %s\", klass_name);\n+          }\n+        } else {\n@@ -686,1 +778,26 @@\n-    }\n+        break;\n+      } else {\n+        \/\/ handling of null free inline type\n+        ResetNoHandleMark rnhm;\n+        Thread* THREAD = Thread::current();\n+        SignatureStream ss(fd->signature(), false);\n+        Symbol* name = ss.as_symbol();\n+        assert(!HAS_PENDING_EXCEPTION, \"can resolve klass?\");\n+        InstanceKlass* holder = fd->field_holder();\n+        InstanceKlass* k = SystemDictionary::find_instance_klass(THREAD, name,\n+                                                                 Handle(THREAD, holder->class_loader()));\n+        assert(k != nullptr && !HAS_PENDING_EXCEPTION, \"can resolve klass?\");\n+        InlineKlass* vk = InlineKlass::cast(k);\n+        oop obj;\n+        if (is_flat) {\n+          int field_offset = fd->offset() - vk->payload_offset();\n+          obj = cast_to_oop(cast_from_oop<address>(mirror) + field_offset);\n+        } else {\n+          obj = mirror->obj_field_acquire(fd->offset());\n+        }\n+        InlineTypeFieldPrinter print_field(_out, obj);\n+        vk->do_nonstatic_fields(&print_field);\n+        break;\n+      }\n+    default:\n+      ShouldNotReachHere();\n@@ -688,1 +805,1 @@\n-};\n+}\n","filename":"src\/hotspot\/share\/ci\/ciInstanceKlass.cpp","additions":186,"deletions":69,"binary":false,"changes":255,"status":"modified"},{"patch":"@@ -53,0 +53,1 @@\n+#include \"runtime\/sharedRuntime.hpp\"\n@@ -663,0 +664,54 @@\n+bool ciMethod::array_access_profiled_type(int bci, ciKlass*& array_type, ciKlass*& element_type, ProfilePtrKind& element_ptr, bool &flat_array, bool &null_free_array) {\n+  if (method_data() != nullptr && method_data()->is_mature()) {\n+    ciProfileData* data = method_data()->bci_to_data(bci);\n+    if (data != nullptr) {\n+      if (data->is_ArrayLoadData()) {\n+        ciArrayLoadData* array_access = (ciArrayLoadData*) data->as_ArrayLoadData();\n+        array_type = array_access->array()->valid_type();\n+        element_type = array_access->element()->valid_type();\n+        element_ptr = array_access->element()->ptr_kind();\n+        flat_array = array_access->flat_array();\n+        null_free_array = array_access->null_free_array();\n+        return true;\n+      } else if (data->is_ArrayStoreData()) {\n+        ciArrayStoreData* array_access = (ciArrayStoreData*) data->as_ArrayStoreData();\n+        array_type = array_access->array()->valid_type();\n+        flat_array = array_access->flat_array();\n+        null_free_array = array_access->null_free_array();\n+        ciCallProfile call_profile = call_profile_at_bci(bci);\n+        if (call_profile.morphism() == 1) {\n+          element_type = call_profile.receiver(0);\n+        } else {\n+          element_type = nullptr;\n+        }\n+        if (!array_access->null_seen()) {\n+          element_ptr = ProfileNeverNull;\n+        } else if (call_profile.count() == 0) {\n+          element_ptr = ProfileAlwaysNull;\n+        } else {\n+          element_ptr = ProfileMaybeNull;\n+        }\n+        return true;\n+      }\n+    }\n+  }\n+  return false;\n+}\n+\n+bool ciMethod::acmp_profiled_type(int bci, ciKlass*& left_type, ciKlass*& right_type, ProfilePtrKind& left_ptr, ProfilePtrKind& right_ptr, bool &left_inline_type, bool &right_inline_type) {\n+  if (method_data() != nullptr && method_data()->is_mature()) {\n+    ciProfileData* data = method_data()->bci_to_data(bci);\n+    if (data != nullptr && data->is_ACmpData()) {\n+      ciACmpData* acmp = (ciACmpData*)data->as_ACmpData();\n+      left_type = acmp->left()->valid_type();\n+      right_type = acmp->right()->valid_type();\n+      left_ptr = acmp->left()->ptr_kind();\n+      right_ptr = acmp->right()->ptr_kind();\n+      left_inline_type = acmp->left_inline_type();\n+      right_inline_type = acmp->right_inline_type();\n+      return true;\n+    }\n+  }\n+  return false;\n+}\n+\n@@ -969,1 +1024,1 @@\n-\/\/ ciMethod::is_object_initializer\n+\/\/ ciMethod::is_object_constructor\n@@ -971,2 +1026,5 @@\n-bool ciMethod::is_object_initializer() const {\n-   return name() == ciSymbols::object_initializer_name();\n+bool ciMethod::is_object_constructor() const {\n+   return (name() == ciSymbols::object_initializer_name()\n+           && signature()->return_type()->is_void());\n+   \/\/ Note:  We can't test is_static, because that would\n+   \/\/ require the method to be loaded.  Sometimes it isn't.\n@@ -1494,0 +1552,19 @@\n+\n+bool ciMethod::is_scalarized_arg(int idx) const {\n+  VM_ENTRY_MARK;\n+  return get_Method()->is_scalarized_arg(idx);\n+}\n+\n+bool ciMethod::has_scalarized_args() const {\n+  VM_ENTRY_MARK;\n+  return get_Method()->has_scalarized_args();\n+}\n+\n+const GrowableArray<SigEntry>* ciMethod::get_sig_cc() const {\n+  VM_ENTRY_MARK;\n+  if (get_Method()->adapter() == nullptr) {\n+    return nullptr;\n+  }\n+  return get_Method()->adapter()->get_sig_cc();\n+}\n+\n","filename":"src\/hotspot\/share\/ci\/ciMethod.cpp","additions":80,"deletions":3,"binary":false,"changes":83,"status":"modified"},{"patch":"@@ -26,0 +26,3 @@\n+#include \"ci\/ciFlatArray.hpp\"\n+#include \"ci\/ciFlatArrayKlass.hpp\"\n+#include \"ci\/ciInlineKlass.hpp\"\n@@ -367,0 +370,3 @@\n+  } else if (o->is_flatArray()) {\n+    flatArrayHandle h_ta(THREAD, (flatArrayOop)o);\n+    return new (arena()) ciFlatArray(h_ta);\n@@ -386,1 +392,3 @@\n-    if (k->is_instance_klass()) {\n+    if (k->is_inline_klass()) {\n+      return new (arena()) ciInlineKlass(k);\n+    } else if (k->is_instance_klass()) {\n@@ -389,0 +397,2 @@\n+    } else if (k->is_flatArray_klass()) {\n+      return new (arena()) ciFlatArrayKlass(k);\n@@ -625,0 +635,6 @@\n+ciWrapper* ciObjectFactory::make_null_free_wrapper(ciType* type) {\n+  ciWrapper* wrapper = new (arena()) ciWrapper(type);\n+  init_ident_of(wrapper);\n+  return wrapper;\n+}\n+\n","filename":"src\/hotspot\/share\/ci\/ciObjectFactory.cpp","additions":17,"deletions":1,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -44,0 +44,1 @@\n+#include \"oops\/inlineKlass.inline.hpp\"\n@@ -514,1 +515,1 @@\n-      \/\/ array\n+      \/\/ TODO 8350865 I think we need to handle null-free\/flat arrays here\n@@ -964,0 +965,1 @@\n+\n@@ -1010,27 +1012,79 @@\n-  \/\/ staticfield <klass> <name> <signature> <value>\n-  \/\/\n-  \/\/ Initialize a class and fill in the value for a static field.\n-  \/\/ This is useful when the compile was dependent on the value of\n-  \/\/ static fields but it's impossible to properly rerun the static\n-  \/\/ initializer.\n-  void process_staticfield(TRAPS) {\n-    InstanceKlass* k = (InstanceKlass *)parse_klass(CHECK);\n-\n-    if (k == nullptr || ReplaySuppressInitializers == 0 ||\n-        (ReplaySuppressInitializers == 2 && k->class_loader() == nullptr)) {\n-      skip_remaining();\n-      return;\n-    }\n-\n-    assert(k->is_initialized(), \"must be\");\n-\n-    const char* field_name = parse_escaped_string();\n-    const char* field_signature = parse_string();\n-    fieldDescriptor fd;\n-    Symbol* name = SymbolTable::new_symbol(field_name);\n-    Symbol* sig = SymbolTable::new_symbol(field_signature);\n-    if (!k->find_local_field(name, sig, &fd) ||\n-        !fd.is_static() ||\n-        fd.has_initial_value()) {\n-      report_error(field_name);\n-      return;\n+  class InlineTypeFieldInitializer : public FieldClosure {\n+    oop _vt;\n+    CompileReplay* _replay;\n+  public:\n+    InlineTypeFieldInitializer(oop vt, CompileReplay* replay)\n+  : _vt(vt), _replay(replay) {}\n+\n+    void do_field(fieldDescriptor* fd) {\n+      BasicType bt = fd->field_type();\n+      const char* string_value = fd->is_null_free_inline_type() ? nullptr : _replay->parse_escaped_string();\n+      switch (bt) {\n+      case T_BYTE: {\n+        int value = atoi(string_value);\n+        _vt->byte_field_put(fd->offset(), value);\n+        break;\n+      }\n+      case T_BOOLEAN: {\n+        int value = atoi(string_value);\n+        _vt->bool_field_put(fd->offset(), value);\n+        break;\n+      }\n+      case T_SHORT: {\n+        int value = atoi(string_value);\n+        _vt->short_field_put(fd->offset(), value);\n+        break;\n+      }\n+      case T_CHAR: {\n+        int value = atoi(string_value);\n+        _vt->char_field_put(fd->offset(), value);\n+        break;\n+      }\n+      case T_INT: {\n+        int value = atoi(string_value);\n+        _vt->int_field_put(fd->offset(), value);\n+        break;\n+      }\n+      case T_LONG: {\n+        jlong value;\n+        if (sscanf(string_value, JLONG_FORMAT, &value) != 1) {\n+          fprintf(stderr, \"Error parsing long: %s\\n\", string_value);\n+          break;\n+        }\n+        _vt->long_field_put(fd->offset(), value);\n+        break;\n+      }\n+      case T_FLOAT: {\n+        float value = atof(string_value);\n+        _vt->float_field_put(fd->offset(), value);\n+        break;\n+      }\n+      case T_DOUBLE: {\n+        double value = atof(string_value);\n+        _vt->double_field_put(fd->offset(), value);\n+        break;\n+      }\n+      case T_ARRAY:\n+      case T_OBJECT:\n+        if (!fd->is_null_free_inline_type()) {\n+          JavaThread* THREAD = JavaThread::current();\n+          bool res = _replay->process_staticfield_reference(string_value, _vt, fd, THREAD);\n+          assert(res, \"should succeed for arrays & objects\");\n+          break;\n+        } else {\n+          InlineKlass* vk = InlineKlass::cast(fd->field_holder()->get_inline_type_field_klass(fd->index()));\n+          if (fd->is_flat()) {\n+            int field_offset = fd->offset() - vk->payload_offset();\n+            oop obj = cast_to_oop(cast_from_oop<address>(_vt) + field_offset);\n+            InlineTypeFieldInitializer init_fields(obj, _replay);\n+            vk->do_nonstatic_fields(&init_fields);\n+          } else {\n+            oop value = vk->allocate_instance(JavaThread::current());\n+            _vt->obj_field_put(fd->offset(), value);\n+          }\n+          break;\n+        }\n+      default: {\n+        fatal(\"Unhandled type: %s\", type2name(bt));\n+      }\n+      }\n@@ -1038,0 +1092,1 @@\n+  };\n@@ -1039,1 +1094,1 @@\n-    oop java_mirror = k->java_mirror();\n+  bool process_staticfield_reference(const char* field_signature, oop java_mirror, fieldDescriptor* fd, TRAPS) {\n@@ -1047,4 +1102,2 @@\n-          ArrayKlass* kelem = (ArrayKlass *)parse_klass(CHECK);\n-          if (kelem == nullptr) {\n-            return;\n-          }\n+          Klass* k = resolve_klass(field_signature, CHECK_(true));\n+          ArrayKlass* kelem = (ArrayKlass *)k;\n@@ -1060,1 +1113,1 @@\n-          value = kelem->multi_allocate(rank, dims, CHECK);\n+          value = kelem->multi_allocate(rank, dims, CHECK_(true));\n@@ -1063,1 +1116,1 @@\n-            value = oopFactory::new_byteArray(length, CHECK);\n+            value = oopFactory::new_byteArray(length, CHECK_(true));\n@@ -1065,1 +1118,1 @@\n-            value = oopFactory::new_boolArray(length, CHECK);\n+            value = oopFactory::new_boolArray(length, CHECK_(true));\n@@ -1067,1 +1120,1 @@\n-            value = oopFactory::new_charArray(length, CHECK);\n+            value = oopFactory::new_charArray(length, CHECK_(true));\n@@ -1069,1 +1122,1 @@\n-            value = oopFactory::new_shortArray(length, CHECK);\n+            value = oopFactory::new_shortArray(length, CHECK_(true));\n@@ -1071,1 +1124,1 @@\n-            value = oopFactory::new_floatArray(length, CHECK);\n+            value = oopFactory::new_floatArray(length, CHECK_(true));\n@@ -1073,1 +1126,1 @@\n-            value = oopFactory::new_doubleArray(length, CHECK);\n+            value = oopFactory::new_doubleArray(length, CHECK_(true));\n@@ -1075,1 +1128,1 @@\n-            value = oopFactory::new_intArray(length, CHECK);\n+            value = oopFactory::new_intArray(length, CHECK_(true));\n@@ -1077,1 +1130,1 @@\n-            value = oopFactory::new_longArray(length, CHECK);\n+            value = oopFactory::new_longArray(length, CHECK_(true));\n@@ -1080,1 +1133,2 @@\n-            Klass* actual_array_klass = parse_klass(CHECK);\n+            Klass* actual_array_klass = parse_klass(CHECK_(true));\n+            \/\/ TODO 8350865 I think we need to handle null-free\/flat arrays here\n@@ -1082,1 +1136,1 @@\n-            value = oopFactory::new_objArray(kelem, length, CHECK);\n+            value = oopFactory::new_objArray(kelem, length, CHECK_(true));\n@@ -1087,0 +1141,14 @@\n+        java_mirror->obj_field_put(fd->offset(), value);\n+        return true;\n+      }\n+    } else if (strcmp(field_signature, \"Ljava\/lang\/String;\") == 0) {\n+      const char* string_value = parse_escaped_string();\n+      Handle value = java_lang_String::create_from_str(string_value, CHECK_(true));\n+      java_mirror->obj_field_put(fd->offset(), value());\n+      return true;\n+    } else if (field_signature[0] == JVM_SIGNATURE_CLASS) {\n+      const char* instance = parse_escaped_string();\n+      oop value = nullptr;\n+      if (instance != nullptr) {\n+        Klass* k = resolve_klass(instance, CHECK_(true));\n+        value = InstanceKlass::cast(k)->allocate_instance(CHECK_(true));\n@@ -1088,0 +1156,76 @@\n+      java_mirror->obj_field_put(fd->offset(), value);\n+      return true;\n+    }\n+    return false;\n+  }\n+\n+  \/\/ Initialize a class and fill in the value for a static field.\n+  \/\/ This is useful when the compile was dependent on the value of\n+  \/\/ static fields but it's impossible to properly rerun the static\n+  \/\/ initializer.\n+  void process_staticfield(TRAPS) {\n+    InstanceKlass* k = (InstanceKlass *)parse_klass(CHECK);\n+\n+    if (k == nullptr || ReplaySuppressInitializers == 0 ||\n+        (ReplaySuppressInitializers == 2 && k->class_loader() == nullptr)) {\n+        skip_remaining();\n+      return;\n+    }\n+\n+    assert(k->is_initialized(), \"must be\");\n+\n+    const char* field_name = parse_escaped_string();\n+    const char* field_signature = parse_string();\n+    fieldDescriptor fd;\n+    Symbol* name = SymbolTable::new_symbol(field_name);\n+    Symbol* sig = SymbolTable::new_symbol(field_signature);\n+    if (!k->find_local_field(name, sig, &fd) ||\n+        !fd.is_static() ||\n+        fd.has_initial_value()) {\n+      report_error(field_name);\n+      return;\n+    }\n+\n+    oop java_mirror = k->java_mirror();\n+    if (strcmp(field_signature, \"I\") == 0) {\n+      const char* string_value = parse_escaped_string();\n+      int value = atoi(string_value);\n+      java_mirror->int_field_put(fd.offset(), value);\n+    } else if (strcmp(field_signature, \"B\") == 0) {\n+      const char* string_value = parse_escaped_string();\n+      int value = atoi(string_value);\n+      java_mirror->byte_field_put(fd.offset(), value);\n+    } else if (strcmp(field_signature, \"C\") == 0) {\n+      const char* string_value = parse_escaped_string();\n+      int value = atoi(string_value);\n+      java_mirror->char_field_put(fd.offset(), value);\n+    } else if (strcmp(field_signature, \"S\") == 0) {\n+      const char* string_value = parse_escaped_string();\n+      int value = atoi(string_value);\n+      java_mirror->short_field_put(fd.offset(), value);\n+    } else if (strcmp(field_signature, \"Z\") == 0) {\n+      const char* string_value = parse_escaped_string();\n+      int value = atoi(string_value);\n+      java_mirror->bool_field_put(fd.offset(), value);\n+    } else if (strcmp(field_signature, \"J\") == 0) {\n+      const char* string_value = parse_escaped_string();\n+      jlong value;\n+      if (sscanf(string_value, JLONG_FORMAT, &value) != 1) {\n+        fprintf(stderr, \"Error parsing long: %s\\n\", string_value);\n+        return;\n+      }\n+      java_mirror->long_field_put(fd.offset(), value);\n+    } else if (strcmp(field_signature, \"F\") == 0) {\n+      const char* string_value = parse_escaped_string();\n+      float value = atof(string_value);\n+      java_mirror->float_field_put(fd.offset(), value);\n+    } else if (strcmp(field_signature, \"D\") == 0) {\n+      const char* string_value = parse_escaped_string();\n+      double value = atof(string_value);\n+      java_mirror->double_field_put(fd.offset(), value);\n+    } else if (fd.is_null_free_inline_type()) {\n+      Klass* kelem = resolve_klass(field_signature, CHECK);\n+      InlineKlass* vk = InlineKlass::cast(kelem);\n+      oop value = vk->allocate_instance(CHECK);\n+      InlineTypeFieldInitializer init_fields(value, this);\n+      vk->do_nonstatic_fields(&init_fields);\n@@ -1090,40 +1234,2 @@\n-      const char* string_value = parse_escaped_string();\n-      if (strcmp(field_signature, \"I\") == 0) {\n-        int value = atoi(string_value);\n-        java_mirror->int_field_put(fd.offset(), value);\n-      } else if (strcmp(field_signature, \"B\") == 0) {\n-        int value = atoi(string_value);\n-        java_mirror->byte_field_put(fd.offset(), value);\n-      } else if (strcmp(field_signature, \"C\") == 0) {\n-        int value = atoi(string_value);\n-        java_mirror->char_field_put(fd.offset(), value);\n-      } else if (strcmp(field_signature, \"S\") == 0) {\n-        int value = atoi(string_value);\n-        java_mirror->short_field_put(fd.offset(), value);\n-      } else if (strcmp(field_signature, \"Z\") == 0) {\n-        int value = atoi(string_value);\n-        java_mirror->bool_field_put(fd.offset(), value);\n-      } else if (strcmp(field_signature, \"J\") == 0) {\n-        jlong value;\n-        if (sscanf(string_value, JLONG_FORMAT, &value) != 1) {\n-          fprintf(stderr, \"Error parsing long: %s\\n\", string_value);\n-          return;\n-        }\n-        java_mirror->long_field_put(fd.offset(), value);\n-      } else if (strcmp(field_signature, \"F\") == 0) {\n-        float value = atof(string_value);\n-        java_mirror->float_field_put(fd.offset(), value);\n-      } else if (strcmp(field_signature, \"D\") == 0) {\n-        double value = atof(string_value);\n-        java_mirror->double_field_put(fd.offset(), value);\n-      } else if (strcmp(field_signature, \"Ljava\/lang\/String;\") == 0) {\n-        Handle value = java_lang_String::create_from_str(string_value, CHECK);\n-        java_mirror->obj_field_put(fd.offset(), value());\n-      } else if (field_signature[0] == JVM_SIGNATURE_CLASS) {\n-        oop value = nullptr;\n-        if (string_value != nullptr) {\n-          Klass* k = resolve_klass(string_value, CHECK);\n-          value = InstanceKlass::cast(k)->allocate_instance(CHECK);\n-        }\n-        java_mirror->obj_field_put(fd.offset(), value);\n-      } else {\n+      bool res = process_staticfield_reference(field_signature, java_mirror, &fd, CHECK);\n+      if (!res)  {\n","filename":"src\/hotspot\/share\/ci\/ciReplay.cpp","additions":190,"deletions":84,"binary":false,"changes":274,"status":"modified"},{"patch":"@@ -46,0 +46,1 @@\n+#include \"oops\/fieldStreams.inline.hpp\"\n@@ -51,1 +52,1 @@\n-#include \"runtime\/fieldDescriptor.hpp\"\n+#include \"runtime\/fieldDescriptor.inline.hpp\"\n@@ -69,0 +70,1 @@\n+#define INLINE_TYPE_MAJOR_VERSION                       56\n@@ -482,0 +484,3 @@\n+    case BAD_STRICT_FIELDS:\n+      ss->print(\"Invalid use of strict instance fields\");\n+      break;\n@@ -500,0 +505,7 @@\n+    case WRONG_INLINE_TYPE:\n+      ss->print(\"Type \");\n+      _type.details(ss);\n+      ss->print(\" and type \");\n+      _expected.details(ss);\n+      ss->print(\" must be identical inline types.\");\n+      break;\n@@ -620,0 +632,5 @@\n+static bool supports_strict_fields(InstanceKlass* klass) {\n+  int ver = klass->major_version();\n+  return ver > Verifier::VALUE_TYPES_MAJOR_VERSION ||\n+         (ver == Verifier::VALUE_TYPES_MAJOR_VERSION && klass->minor_version() == Verifier::JAVA_PREVIEW_MINOR_VERSION);\n+}\n@@ -715,0 +732,15 @@\n+  \/\/ Collect the initial strict instance fields\n+  StackMapFrame::AssertUnsetFieldTable* strict_fields = new StackMapFrame::AssertUnsetFieldTable();\n+  if (m->is_object_constructor()) {\n+    for (AllFieldStream fs(m->method_holder()); !fs.done(); fs.next()) {\n+      if (fs.access_flags().is_strict() && !fs.access_flags().is_static()) {\n+        NameAndSig new_field(fs.name(), fs.signature());\n+        if (IgnoreAssertUnsetFields) {\n+          strict_fields->put(new_field, true);\n+        } else {\n+          strict_fields->put(new_field, false);\n+        }\n+      }\n+    }\n+  }\n+\n@@ -716,1 +748,1 @@\n-  StackMapFrame current_frame(max_locals, max_stack, this);\n+  StackMapFrame current_frame(max_locals, max_stack, strict_fields, this);\n@@ -743,1 +775,1 @@\n-  StackMapReader reader(this, &stream, code_data, code_length, &current_frame, max_locals, max_stack, THREAD);\n+  StackMapReader reader(this, &stream, code_data, code_length, &current_frame, max_locals, max_stack, strict_fields, THREAD);\n@@ -1684,1 +1716,1 @@\n-          if (_method->name() == vmSymbols::object_initializer_name() &&\n+          if (_method->is_object_constructor() &&\n@@ -1707,4 +1739,0 @@\n-          verify_invoke_instructions(\n-            &bcs, code_length, &current_frame, (bci >= ex_min && bci < ex_max),\n-            &this_uninit, return_type, cp, &stackmap_table, CHECK_VERIFY(this));\n-          no_control_flow = false; break;\n@@ -1715,1 +1743,1 @@\n-            &this_uninit, return_type, cp, &stackmap_table, CHECK_VERIFY(this));\n+            &this_uninit, cp, &stackmap_table, CHECK_VERIFY(this));\n@@ -1773,2 +1801,2 @@\n-        case Bytecodes::_monitorexit :\n-          current_frame.pop_stack(\n+        case Bytecodes::_monitorexit : {\n+          VerificationType ref = current_frame.pop_stack(\n@@ -1777,0 +1805,1 @@\n+        }\n@@ -2154,1 +2183,1 @@\n-            | (1 << JVM_CONSTANT_String)  | (1 << JVM_CONSTANT_Class)\n+            | (1 << JVM_CONSTANT_String) | (1 << JVM_CONSTANT_Class)\n@@ -2330,1 +2359,1 @@\n-    (!allow_arrays || !ref_class_type.is_array())) {\n+      (!allow_arrays || !ref_class_type.is_array())) {\n@@ -2337,0 +2366,1 @@\n+\n@@ -2378,1 +2408,1 @@\n-      \/\/ The JVMS 2nd edition allows field initialization before the superclass\n+      \/\/ Field initialization is allowed before the superclass\n@@ -2381,4 +2411,24 @@\n-      if (stack_object_type == VerificationType::uninitialized_this_type() &&\n-          target_class_type.equals(current_type()) &&\n-          _klass->find_local_field(field_name, field_sig, &fd)) {\n-        stack_object_type = current_type();\n+      bool is_local_field = _klass->find_local_field(field_name, field_sig, &fd) &&\n+                            target_class_type.equals(current_type());\n+      if (stack_object_type == VerificationType::uninitialized_this_type()) {\n+        if (is_local_field) {\n+          \/\/ Set the type to the current type so the is_assignable check passes.\n+          stack_object_type = current_type();\n+\n+          if (fd.access_flags().is_strict()) {\n+            ResourceMark rm(THREAD);\n+            if (!current_frame->satisfy_unset_field(fd.name(), fd.signature())) {\n+              log_info(verification)(\"Attempting to initialize field not found in initial strict instance fields: %s%s\",\n+                                     fd.name()->as_C_string(), fd.signature()->as_C_string());\n+              verify_error(ErrorContext::bad_strict_fields(bci, current_frame),\n+                           \"Initializing unknown strict field: %s:%s\", fd.name()->as_C_string(), fd.signature()->as_C_string());\n+            }\n+          }\n+        }\n+      } else if (supports_strict_fields(_klass)) {\n+        \/\/ `strict` fields are not writable, but only local fields produce verification errors\n+        if (is_local_field && fd.access_flags().is_strict() && fd.access_flags().is_final()) {\n+          verify_error(ErrorContext::bad_code(bci),\n+                       \"Illegal use of putfield on a strict field\");\n+          return;\n+        }\n@@ -2666,0 +2716,7 @@\n+    } else if (ref_class_type.name() == superk->name()) {\n+      \/\/ Strict final fields must be satisfied by this point\n+      if (!current_frame->verify_unset_fields_satisfied()) {\n+        log_info(verification)(\"Strict instance fields not initialized\");\n+        StackMapFrame::print_strict_fields(current_frame->assert_unset_fields());\n+        current_frame->unsatisfied_strict_fields_error(current_class(), bci);\n+      }\n@@ -2789,1 +2846,1 @@\n-    bool in_try_block, bool *this_uninit, VerificationType return_type,\n+    bool in_try_block, bool *this_uninit,\n@@ -2821,1 +2878,1 @@\n-  \/\/ Get referenced class type\n+  \/\/ Get referenced class\n@@ -2887,1 +2944,2 @@\n-    \/\/ Make sure <init> can only be invoked by invokespecial\n+    \/\/ Make sure:\n+    \/\/   <init> can only be invoked by invokespecial.\n@@ -2889,1 +2947,1 @@\n-        method_name != vmSymbols::object_initializer_name()) {\n+          method_name != vmSymbols::object_initializer_name()) {\n@@ -2897,1 +2955,1 @@\n-                  current_class()->super()->name()))) {\n+                  current_class()->super()->name()))) { \/\/ super() can never be an inline_type.\n@@ -2982,3 +3040,1 @@\n-      \/\/ <init> method must have a void return type\n-      \/* Unreachable?  Class file parser verifies that methods with '<' have\n-       * void return *\/\n+      \/\/ an <init> method must have a void return type\n","filename":"src\/hotspot\/share\/classfile\/verifier.cpp","additions":82,"deletions":26,"binary":false,"changes":108,"status":"modified"},{"patch":"@@ -327,0 +327,8 @@\n+  do_intrinsic(_newNullRestrictedAtomicArray, jdk_internal_value_ValueClass, newNullRestrictedAtomicArray_name, newArray_signature3, F_SN) \\\n+   do_name(     newNullRestrictedAtomicArray_name,                \"newNullRestrictedAtomicArray\")                       \\\n+  do_intrinsic(_newNullRestrictedNonAtomicArray, jdk_internal_value_ValueClass, newNullRestrictedNonAtomicArray_name, newArray_signature3, F_SN) \\\n+   do_name(     newNullRestrictedNonAtomicArray_name,             \"newNullRestrictedNonAtomicArray\")                    \\\n+  do_intrinsic(_newNullableAtomicArray, jdk_internal_value_ValueClass, newNullableAtomicArray_name, newArray_signature2, F_SN) \\\n+   do_name(     newNullableAtomicArray_name,                      \"newNullableAtomicArray\")                             \\\n+   do_signature(newArray_signature2,                              \"(Ljava\/lang\/Class;I)[Ljava\/lang\/Object;\")            \\\n+   do_signature(newArray_signature3,                              \"(Ljava\/lang\/Class;ILjava\/lang\/Object;)[Ljava\/lang\/Object;\") \\\n@@ -658,0 +666,2 @@\n+  do_intrinsic(_isFlatArray,              jdk_internal_misc_Unsafe,     isFlatArray_name, class_boolean_signature, F_RN)         \\\n+   do_name(     isFlatArray_name,                                       \"isFlatArray\")                                           \\\n@@ -709,0 +719,2 @@\n+  do_signature(getValue_signature,        \"(Ljava\/lang\/Object;JLjava\/lang\/Class;)Ljava\/lang\/Object;\")                   \\\n+  do_signature(putValue_signature,        \"(Ljava\/lang\/Object;JLjava\/lang\/Class;Ljava\/lang\/Object;)V\")                  \\\n@@ -719,0 +731,3 @@\n+  do_name(getValue_name,\"getValue\")             do_name(putValue_name,\"putValue\")                                       \\\n+  do_name(makePrivateBuffer_name,\"makePrivateBuffer\")                                                                   \\\n+  do_name(finishPrivateBuffer_name,\"finishPrivateBuffer\")                                                               \\\n@@ -729,0 +744,1 @@\n+  do_intrinsic(_getValue,           jdk_internal_misc_Unsafe,     getValue_name, getValue_signature,             F_RN)  \\\n@@ -738,0 +754,4 @@\n+  do_intrinsic(_putValue,           jdk_internal_misc_Unsafe,     putValue_name, putValue_signature,             F_RN)  \\\n+                                                                                                                        \\\n+  do_intrinsic(_makePrivateBuffer,  jdk_internal_misc_Unsafe,     makePrivateBuffer_name, object_object_signature, F_RN)   \\\n+  do_intrinsic(_finishPrivateBuffer,  jdk_internal_misc_Unsafe,   finishPrivateBuffer_name, object_object_signature, F_RN) \\\n","filename":"src\/hotspot\/share\/classfile\/vmIntrinsics.hpp","additions":20,"deletions":0,"binary":false,"changes":20,"status":"modified"},{"patch":"@@ -1264,1 +1264,1 @@\n-          if (vfst.method()->is_static_initializer() ||\n+        if (vfst.method()->is_class_initializer() ||\n","filename":"src\/hotspot\/share\/compiler\/compileBroker.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -36,0 +36,1 @@\n+#include \"oops\/inlineKlass.hpp\"\n","filename":"src\/hotspot\/share\/compiler\/oopMap.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -512,1 +512,2 @@\n-    if (klass->is_array_klass()) {\n+    \/\/ CMH: Valhalla flat arrays can split this work up, but for now, doesn't\n+    if (klass->is_array_klass() && !klass->is_flatArray_klass()) {\n","filename":"src\/hotspot\/share\/gc\/g1\/g1ParScanThreadState.cpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -78,0 +78,1 @@\n+#include \"oops\/flatArrayKlass.inline.hpp\"\n@@ -2453,0 +2454,3 @@\n+    \/\/ Read the klass before the copying, since it might destroy the klass (i.e. overlapping copy)\n+    \/\/ and if partial copy, the destination klass may not be copied yet\n+    Klass* klass = cast_to_oop(source())->klass();\n@@ -2454,1 +2458,1 @@\n-    cast_to_oop(copy_destination())->init_mark();\n+    cast_to_oop(copy_destination())->set_mark(Klass::default_prototype_header(klass));\n","filename":"src\/hotspot\/share\/gc\/parallel\/psParallelCompact.cpp","additions":5,"deletions":1,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -51,0 +51,4 @@\n+Node* C2ParseAccess::control() const {\n+  return _ctl == nullptr ? _kit->control() : _ctl;\n+}\n+\n@@ -204,1 +208,1 @@\n-    Node* control = control_dependent ? kit->control() : nullptr;\n+    Node* control = control_dependent ? parse_access.control() : nullptr;\n@@ -865,1 +869,1 @@\n-  phase->igvn().replace_node(ac, call);\n+  phase->replace_node(ac, call);\n@@ -889,1 +893,1 @@\n-  phase->igvn().replace_node(ac, call);\n+  phase->replace_node(ac, call);\n","filename":"src\/hotspot\/share\/gc\/shared\/c2\/barrierSetC2.cpp","additions":7,"deletions":3,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -174,0 +174,1 @@\n+    Interpreter::_throw_NPE_UninitializedField_entry         = generate_exception_handler(\"java\/lang\/NullPointerException\", \"Uninitialized null-restricted field\");\n","filename":"src\/hotspot\/share\/interpreter\/templateInterpreterGenerator.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -89,1 +89,4 @@\n-    if (!mh->is_native() && !mh->is_static() && !mh->is_object_initializer() && !mh->is_static_initializer()) {\n+    if (!mh->is_native() &&\n+        !mh->is_static() &&\n+        !mh->is_object_constructor() &&\n+        !mh->is_class_initializer()) {\n","filename":"src\/hotspot\/share\/jvmci\/jvmciCompiler.cpp","additions":4,"deletions":1,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -56,0 +56,1 @@\n+#include \"oops\/flatArrayKlass.hpp\"\n@@ -265,1 +266,1 @@\n-      \/\/ All of these should have been reverted back to ClassIndex before calling\n+      \/\/ All of these should have been reverted back to Unresolved before calling\n@@ -642,0 +643,6 @@\n+void check_is_inline_type(Klass* k, TRAPS) {\n+  if (!k->is_inline_klass()) {\n+    THROW(vmSymbols::java_lang_IncompatibleClassChangeError());\n+  }\n+}\n+\n@@ -680,0 +687,1 @@\n+  bool inline_type_signature = false;\n@@ -688,0 +696,3 @@\n+  if (inline_type_signature) {\n+    name->decrement_refcount();\n+  }\n@@ -696,0 +707,16 @@\n+  if (!HAS_PENDING_EXCEPTION && inline_type_signature) {\n+    check_is_inline_type(k, THREAD);\n+  }\n+\n+  if (!HAS_PENDING_EXCEPTION) {\n+    Klass* bottom_klass = nullptr;\n+    if (k->is_objArray_klass()) {\n+      bottom_klass = ObjArrayKlass::cast(k)->bottom_klass();\n+      assert(bottom_klass != nullptr, \"Should be set\");\n+      assert(bottom_klass->is_instance_klass() || bottom_klass->is_typeArray_klass(), \"Sanity check\");\n+    } else if (k->is_flatArray_klass()) {\n+      bottom_klass = FlatArrayKlass::cast(k)->element_klass();\n+      assert(bottom_klass != nullptr, \"Should be set\");\n+    }\n+  }\n+\n","filename":"src\/hotspot\/share\/oops\/constantPool.cpp","additions":28,"deletions":1,"binary":false,"changes":29,"status":"modified"},{"patch":"@@ -141,1 +141,1 @@\n-    if (!is_static)\n+    if (!is_static) {\n@@ -143,0 +143,1 @@\n+    }\n@@ -1603,0 +1604,1 @@\n+    case Bytecodes::_invokeinterface:\n@@ -1604,4 +1606,3 @@\n-    case Bytecodes::_invokespecial:     do_method(false, false, itr->get_index_u2(), itr->bci(), itr->code()); break;\n-    case Bytecodes::_invokestatic:      do_method(true,  false, itr->get_index_u2(), itr->bci(), itr->code()); break;\n-    case Bytecodes::_invokedynamic:     do_method(true,  false, itr->get_index_u4(), itr->bci(), itr->code()); break;\n-    case Bytecodes::_invokeinterface:   do_method(false, true,  itr->get_index_u2(), itr->bci(), itr->code()); break;\n+    case Bytecodes::_invokespecial:     do_method(false, itr->get_index_u2(), itr->bci(), itr->code()); break;\n+    case Bytecodes::_invokestatic:      do_method(true , itr->get_index_u2(), itr->bci(), itr->code()); break;\n+    case Bytecodes::_invokedynamic:     do_method(true , itr->get_index_u4(), itr->bci(), itr->code()); break;\n@@ -1627,0 +1628,1 @@\n+\n@@ -1952,1 +1954,3 @@\n-  if (!is_static) in[i++] = CellTypeState::ref;\n+  if (!is_static) {\n+    in[i++] = CellTypeState::ref;\n+  }\n@@ -1958,1 +1962,1 @@\n-void GenerateOopMap::do_method(int is_static, int is_interface, int idx, int bci, Bytecodes::Code bc) {\n+void GenerateOopMap::do_method(int is_static, int idx, int bci, Bytecodes::Code bc) {\n","filename":"src\/hotspot\/share\/oops\/generateOopMap.cpp","additions":11,"deletions":7,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -695,0 +695,6 @@\n+  if (tp->isa_aryptr()) {\n+    \/\/ In the case of a flat inline type array, each field has its\n+    \/\/ own slice so we need to extract the field being accessed from\n+    \/\/ the address computation\n+    return tp->is_aryptr()->add_field_offset_and_offset(txoffset);\n+  }\n@@ -715,0 +721,6 @@\n+  if (p1->isa_aryptr()) {\n+    \/\/ In the case of a flat inline type array, each field has its\n+    \/\/ own slice so we need to extract the field being accessed from\n+    \/\/ the address computation\n+    return p1->is_aryptr()->add_field_offset_and_offset(p2offset);\n+  }\n","filename":"src\/hotspot\/share\/opto\/addnode.cpp","additions":12,"deletions":0,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -792,0 +792,6 @@\n+  product(bool, UseArrayLoadStoreProfile, true,                             \\\n+          \"Take advantage of profiling at array load\/store\")                \\\n+                                                                            \\\n+  product(bool, UseACmpProfile, true,                                       \\\n+          \"Take advantage of profiling at acmp\")                            \\\n+                                                                            \\\n","filename":"src\/hotspot\/share\/opto\/c2_globals.hpp","additions":6,"deletions":0,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -74,0 +74,1 @@\n+macro(CastI2N)\n@@ -189,0 +190,1 @@\n+macro(FlatArrayCheck)\n@@ -359,0 +361,1 @@\n+macro(StoreLSpecial)\n@@ -384,0 +387,1 @@\n+macro(InlineType)\n","filename":"src\/hotspot\/share\/opto\/classes.hpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -30,0 +30,1 @@\n+#include \"opto\/inlinetypenode.hpp\"\n@@ -68,0 +69,1 @@\n+\/\/------------------------------Ideal------------------------------------------\n@@ -69,0 +71,6 @@\n+  if (in(1)->is_InlineType()) {\n+    \/\/ Null checking a scalarized but nullable inline type. Check the IsInit\n+    \/\/ input instead of the oop input to avoid keeping buffer allocations alive.\n+    set_req_X(1, in(1)->as_InlineType()->get_is_init(), phase);\n+    return this;\n+  }\n","filename":"src\/hotspot\/share\/opto\/convertnode.cpp","additions":8,"deletions":0,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -32,0 +32,1 @@\n+#include \"jvm_io.h\"\n@@ -40,0 +41,1 @@\n+#include \"opto\/inlinetypenode.hpp\"\n@@ -148,1 +150,15 @@\n-  if (allow_inline && allow_intrinsics) {\n+  if (callee->intrinsic_id() == vmIntrinsics::_makePrivateBuffer || callee->intrinsic_id() == vmIntrinsics::_finishPrivateBuffer) {\n+    \/\/ These methods must be inlined so that we don't have larval value objects crossing method\n+    \/\/ boundaries\n+    assert(!call_does_dispatch, \"callee should not be virtual %s\", callee->name()->as_utf8());\n+    CallGenerator* cg = find_intrinsic(callee, call_does_dispatch);\n+\n+    if (cg == nullptr) {\n+      \/\/ This is probably because the intrinsics is disabled from the command line\n+      char reason[256];\n+      jio_snprintf(reason, sizeof(reason), \"cannot find an intrinsics for %s\", callee->name()->as_utf8());\n+      C->record_method_not_compilable(reason);\n+      return nullptr;\n+    }\n+    return cg;\n+  } else if (allow_inline && allow_intrinsics) {\n@@ -604,1 +620,1 @@\n-  if (iter().cur_bc_raw() == Bytecodes::_invokespecial && !orig_callee->is_object_initializer()) {\n+  if (iter().cur_bc_raw() == Bytecodes::_invokespecial && !orig_callee->is_object_constructor()) {\n@@ -646,0 +662,4 @@\n+  if (failing()) {\n+    return;\n+  }\n+  assert(cg != nullptr, \"must find a CallGenerator for callee %s\", callee->name()->as_utf8());\n@@ -732,1 +752,1 @@\n-          \/\/ It's OK for a method  to return a value that is discarded.\n+          \/\/ It's OK for a method to return a value that is discarded.\n@@ -790,0 +810,53 @@\n+    if (rtype->is_inlinetype() && !peek()->is_InlineType()) {\n+      Node* retnode = pop();\n+      retnode = InlineTypeNode::make_from_oop(this, retnode, rtype->as_inline_klass());\n+      push_node(T_OBJECT, retnode);\n+    }\n+\n+    \/\/ Note that:\n+    \/\/ - The caller map is the state just before the call of the currently parsed method with all arguments\n+    \/\/   on the stack. Therefore, we have caller_map->arg(0) == this.\n+    \/\/ - local(0) contains the updated receiver after calling an inline type constructor.\n+    \/\/ - Abstract value classes are not ciInlineKlass instances and thus abstract_value_klass->is_inlinetype() is false.\n+    \/\/   We use the bottom type of the receiver node to determine if we have a value class or not.\n+    const bool is_current_method_inline_type_constructor =\n+        \/\/ Is current method a constructor (i.e <init>)?\n+        _method->is_object_constructor() &&\n+        \/\/ Is the holder of the current constructor method an inline type?\n+        _caller->map()->argument(_caller, 0)->bottom_type()->is_inlinetypeptr();\n+    assert(!is_current_method_inline_type_constructor || !cg->method()->is_object_constructor() || receiver != nullptr,\n+           \"must have valid receiver after calling another constructor\");\n+    if (is_current_method_inline_type_constructor &&\n+        \/\/ Is the just called method an inline type constructor?\n+        cg->method()->is_object_constructor() && receiver->bottom_type()->is_inlinetypeptr() &&\n+         \/\/ AND:\n+         \/\/ 1) ... invoked on the same receiver? Then it's another constructor on the same object doing the initialization.\n+        (receiver == _caller->map()->argument(_caller, 0) ||\n+         \/\/ 2) ... abstract? Then it's the call to the super constructor which eventually calls Object.<init> to\n+         \/\/                    finish the initialization of this larval.\n+         cg->method()->holder()->is_abstract() ||\n+         \/\/ 3) ... Object.<init>? Then we know it's the final call to finish the larval initialization. Other\n+         \/\/        Object.<init> calls would have a non-inline-type receiver which we already excluded in the check above.\n+         cg->method()->holder()->is_java_lang_Object())\n+        ) {\n+      assert(local(0)->is_InlineType() && receiver->bottom_type()->is_inlinetypeptr() && receiver->is_InlineType() &&\n+             _caller->map()->argument(_caller, 0)->bottom_type()->inline_klass() == receiver->bottom_type()->inline_klass(),\n+             \"Unexpected receiver\");\n+      InlineTypeNode* updated_receiver = local(0)->as_InlineType();\n+      InlineTypeNode* cloned_updated_receiver = updated_receiver->clone_if_required(&_gvn, _map);\n+      cloned_updated_receiver->set_is_larval(false);\n+      cloned_updated_receiver = _gvn.transform(cloned_updated_receiver)->as_InlineType();\n+      \/\/ Receiver updated by the just called constructor. We need to update the map to make the effect visible. After\n+      \/\/ the super() call, only the updated receiver in local(0) will be used from now on. Therefore, we do not need\n+      \/\/ to update the original receiver 'receiver' but only the 'updated_receiver'.\n+      replace_in_map(updated_receiver, cloned_updated_receiver);\n+\n+      if (_caller->has_method()) {\n+        \/\/ If the current method is inlined, we also need to update the exit map to propagate the updated receiver\n+        \/\/ to the caller map.\n+        Node* receiver_in_caller = _caller->map()->argument(_caller, 0);\n+        assert(receiver_in_caller->bottom_type()->inline_klass() == receiver->bottom_type()->inline_klass(),\n+               \"Receiver type mismatch\");\n+        _exits.map()->replace_edge(receiver_in_caller, cloned_updated_receiver, &_gvn);\n+      }\n+    }\n","filename":"src\/hotspot\/share\/opto\/doCall.cpp","additions":76,"deletions":3,"binary":false,"changes":79,"status":"modified"},{"patch":"@@ -25,0 +25,2 @@\n+#include \"ci\/ciFlatArrayKlass.hpp\"\n+#include \"ci\/ciInlineKlass.hpp\"\n@@ -34,0 +36,1 @@\n+#include \"oops\/flatArrayKlass.hpp\"\n@@ -39,0 +42,1 @@\n+#include \"opto\/inlinetypenode.hpp\"\n@@ -42,0 +46,1 @@\n+#include \"opto\/narrowptrnode.hpp\"\n@@ -55,1 +60,1 @@\n-GraphKit::GraphKit(JVMState* jvms)\n+GraphKit::GraphKit(JVMState* jvms, PhaseGVN* gvn)\n@@ -58,1 +63,1 @@\n-    _gvn(*C->initial_gvn()),\n+    _gvn((gvn != nullptr) ? *gvn : *C->initial_gvn()),\n@@ -61,0 +66,1 @@\n+  assert(gvn == nullptr || !gvn->is_IterGVN() || gvn->is_IterGVN()->delay_transform(), \"delay transform should be enabled\");\n@@ -64,0 +70,7 @@\n+#ifdef ASSERT\n+  if (_gvn.is_IterGVN() != nullptr) {\n+    assert(_gvn.is_IterGVN()->delay_transform(), \"Transformation must be delayed if IterGVN is used\");\n+    \/\/ Save the initial size of _for_igvn worklist for verification (see ~GraphKit)\n+    _worklist_size = _gvn.C->igvn_worklist()->size();\n+  }\n+#endif\n@@ -861,1 +874,1 @@\n-           (is_anewarray && code == Bytecodes::_multianewarray);\n+           (is_anewarray && (code == Bytecodes::_multianewarray));\n@@ -952,0 +965,2 @@\n+\n+  JVMState* callee_jvms = nullptr;\n@@ -977,2 +992,10 @@\n-      for (j = 0; j < l; j++)\n-        call->set_req(p++, in_map->in(k+j));\n+      for (j = 0; j < l; j++) {\n+        Node* val = in_map->in(k + j);\n+        \/\/ Check if there's a larval that has been written in the callee state (constructor) and update it in the caller state\n+        if (callee_jvms != nullptr && val->is_InlineType() && val->as_InlineType()->is_larval() &&\n+            callee_jvms->method()->is_object_constructor() && val == in_map->argument(in_jvms, 0) &&\n+            val->bottom_type()->is_inlinetypeptr()) {\n+          val = callee_jvms->map()->local(callee_jvms, 0); \/\/ Receiver\n+        }\n+        call->set_req(p++, val);\n+      }\n@@ -988,2 +1011,10 @@\n-      for (j = 0; j < l; j++)\n-        call->set_req(p++, in_map->in(k+j));\n+      for (j = 0; j < l; j++) {\n+        Node* val = in_map->in(k + j);\n+        \/\/ Check if there's a larval that has been written in the callee state (constructor) and update it in the caller state\n+        if (callee_jvms != nullptr && val->is_InlineType() && val->as_InlineType()->is_larval() &&\n+            callee_jvms->method()->is_object_constructor() && val == in_map->argument(in_jvms, 0) &&\n+            val->bottom_type()->is_inlinetypeptr()) {\n+          val = callee_jvms->map()->local(callee_jvms, 0); \/\/ Receiver\n+        }\n+        call->set_req(p++, val);\n+      }\n@@ -1028,0 +1059,1 @@\n+    callee_jvms = out_jvms;\n@@ -1203,1 +1235,1 @@\n-  return _gvn.transform(LoadKlassNode::make(_gvn, immutable_memory(), k_adr, TypeInstPtr::KLASS));\n+  return _gvn.transform(LoadKlassNode::make(_gvn, immutable_memory(), k_adr, TypeInstPtr::KLASS, TypeInstKlassPtr::OBJECT));\n@@ -1252,1 +1284,2 @@\n-                                  bool speculative) {\n+                                  bool speculative,\n+                                  bool is_init_check) {\n@@ -1257,0 +1290,23 @@\n+  if (value->is_InlineType()) {\n+    \/\/ Null checking a scalarized but nullable inline type. Check the IsInit\n+    \/\/ input instead of the oop input to avoid keeping buffer allocations alive.\n+    InlineTypeNode* vtptr = value->as_InlineType();\n+    while (vtptr->get_oop()->is_InlineType()) {\n+      vtptr = vtptr->get_oop()->as_InlineType();\n+    }\n+    null_check_common(vtptr->get_is_init(), T_INT, assert_null, null_control, speculative, true);\n+    if (stopped()) {\n+      return top();\n+    }\n+    if (assert_null) {\n+      \/\/ TODO 8284443 Scalarize here (this currently leads to compilation bailouts)\n+      \/\/ vtptr = InlineTypeNode::make_null(_gvn, vtptr->type()->inline_klass());\n+      \/\/ replace_in_map(value, vtptr);\n+      \/\/ return vtptr;\n+      replace_in_map(value, null());\n+      return null();\n+    }\n+    bool do_replace_in_map = (null_control == nullptr || (*null_control) == top());\n+    return cast_not_null(value, do_replace_in_map);\n+  }\n+\n@@ -1360,1 +1416,1 @@\n-  } else if (type == T_OBJECT) {\n+  } else if (type == T_OBJECT || is_init_check) {\n@@ -1434,1 +1490,0 @@\n-\n@@ -1438,0 +1493,9 @@\n+  if (obj->is_InlineType()) {\n+    Node* vt = obj->isa_InlineType()->clone_if_required(&gvn(), map(), do_replace_in_map);\n+    vt->as_InlineType()->set_is_init(_gvn);\n+    vt = _gvn.transform(vt);\n+    if (do_replace_in_map) {\n+      replace_in_map(obj, vt);\n+    }\n+    return vt;\n+  }\n@@ -1566,0 +1630,1 @@\n+\n@@ -1619,1 +1684,3 @@\n-                                DecoratorSet decorators) {\n+                                DecoratorSet decorators,\n+                                bool safe_for_replace,\n+                                const InlineTypeNode* vt) {\n@@ -1632,0 +1699,7 @@\n+  if (val->is_InlineType()) {\n+    \/\/ Store to non-flat field. Buffer the inline type and make sure\n+    \/\/ the store is re-executed if the allocation triggers deoptimization.\n+    PreserveReexecuteState preexecs(this);\n+    jvms()->set_should_reexecute(true);\n+    val = val->as_InlineType()->buffer(this, safe_for_replace);\n+  }\n@@ -1635,1 +1709,1 @@\n-  C2ParseAccess access(this, decorators | C2_WRITE_ACCESS, bt, obj, addr);\n+  C2ParseAccess access(this, decorators | C2_WRITE_ACCESS, bt, obj, addr, nullptr, vt);\n@@ -1648,1 +1722,2 @@\n-                               DecoratorSet decorators) {\n+                               DecoratorSet decorators,\n+                               Node* ctl) {\n@@ -1654,1 +1729,1 @@\n-  C2ParseAccess access(this, decorators | C2_READ_ACCESS, bt, obj, addr);\n+  C2ParseAccess access(this, decorators | C2_READ_ACCESS, bt, obj, addr, ctl);\n@@ -1759,1 +1834,12 @@\n-  uint shift  = exact_log2(type2aelembytes(elembt));\n+  const TypeAryPtr* arytype = _gvn.type(ary)->is_aryptr();\n+  uint shift;\n+  if (arytype->is_flat() && arytype->klass_is_exact()) {\n+    \/\/ We can only determine the flat array layout statically if the klass is exact. Otherwise, we could have different\n+    \/\/ value classes at runtime with a potentially different layout. The caller needs to fall back to call\n+    \/\/ load\/store_unknown_inline_Type() at runtime. We could return a sentinel node for the non-exact case but that\n+    \/\/ might mess with other GVN transformations in between. Thus, we just continue in the else branch normally, even\n+    \/\/ though we don't need the address node in this case and throw it away again.\n+    shift = arytype->flat_log_elem_size();\n+  } else {\n+    shift = exact_log2(type2aelembytes(elembt));\n+  }\n@@ -1776,0 +1862,10 @@\n+Node* GraphKit::flat_array_element_address(Node*& array, Node* idx, ciInlineKlass* vk, bool is_null_free,\n+                                           bool is_not_null_free, bool is_atomic) {\n+  ciArrayKlass* array_klass = ciArrayKlass::make(vk, \/* flat *\/ true, is_null_free, is_atomic);\n+  const TypeAryPtr* arytype = TypeOopPtr::make_from_klass(array_klass)->isa_aryptr();\n+  arytype = arytype->cast_to_exactness(true);\n+  arytype = arytype->cast_to_not_null_free(is_not_null_free);\n+  array = _gvn.transform(new CheckCastPPNode(control(), array, arytype));\n+  return array_element_address(array, idx, T_FLAT_ELEMENT, arytype->size(), control());\n+}\n+\n@@ -1791,6 +1887,63 @@\n-void GraphKit::set_arguments_for_java_call(CallJavaNode* call) {\n-  \/\/ Add the call arguments:\n-  uint nargs = call->method()->arg_size();\n-  for (uint i = 0; i < nargs; i++) {\n-    Node* arg = argument(i);\n-    call->init_req(i + TypeFunc::Parms, arg);\n+void GraphKit::set_arguments_for_java_call(CallJavaNode* call, bool is_late_inline) {\n+  PreserveReexecuteState preexecs(this);\n+  if (EnableValhalla) {\n+    \/\/ Make sure the call is \"re-executed\", if buffering of inline type arguments triggers deoptimization.\n+    \/\/ At this point, the call hasn't been executed yet, so we will only ever execute the call once.\n+    jvms()->set_should_reexecute(true);\n+    int arg_size = method()->get_declared_signature_at_bci(bci())->arg_size_for_bc(java_bc());\n+    inc_sp(arg_size);\n+  }\n+  \/\/ Add the call arguments\n+  const TypeTuple* domain = call->tf()->domain_sig();\n+  uint nargs = domain->cnt();\n+  int arg_num = 0;\n+  for (uint i = TypeFunc::Parms, idx = TypeFunc::Parms; i < nargs; i++) {\n+    Node* arg = argument(i-TypeFunc::Parms);\n+    const Type* t = domain->field_at(i);\n+    \/\/ TODO 8284443 A static call to a mismatched method should still be scalarized\n+    if (t->is_inlinetypeptr() && !call->method()->get_Method()->mismatch() && call->method()->is_scalarized_arg(arg_num)) {\n+      \/\/ We don't pass inline type arguments by reference but instead pass each field of the inline type\n+      if (!arg->is_InlineType()) {\n+        assert(_gvn.type(arg)->is_zero_type() && !t->inline_klass()->is_null_free(), \"Unexpected argument type\");\n+        arg = InlineTypeNode::make_from_oop(this, arg, t->inline_klass());\n+      }\n+      InlineTypeNode* vt = arg->as_InlineType();\n+      vt->pass_fields(this, call, idx, true, !t->maybe_null());\n+      \/\/ If an inline type argument is passed as fields, attach the Method* to the call site\n+      \/\/ to be able to access the extended signature later via attached_method_before_pc().\n+      \/\/ For example, see CompiledMethod::preserve_callee_argument_oops().\n+      call->set_override_symbolic_info(true);\n+      \/\/ Register an evol dependency on the callee method to make sure that this method is deoptimized and\n+      \/\/ re-compiled with a non-scalarized calling convention if the callee method is later marked as mismatched.\n+      C->dependencies()->assert_evol_method(call->method());\n+      arg_num++;\n+      continue;\n+    } else if (arg->is_InlineType()) {\n+      \/\/ Pass inline type argument via oop to callee\n+      InlineTypeNode* inline_type = arg->as_InlineType();\n+      const ciMethod* method = call->method();\n+      ciInstanceKlass* holder = method->holder();\n+      const bool is_receiver = (i == TypeFunc::Parms);\n+      const bool is_abstract_or_object_klass_constructor = method->is_object_constructor() &&\n+                                                           (holder->is_abstract() || holder->is_java_lang_Object());\n+      const bool is_larval_receiver_on_super_constructor = is_receiver && is_abstract_or_object_klass_constructor;\n+      bool must_init_buffer = true;\n+      \/\/ We always need to buffer inline types when they are escaping. However, we can skip the actual initialization\n+      \/\/ of the buffer if the inline type is a larval because we are going to update the buffer anyway which requires\n+      \/\/ us to create a new one. But there is one special case where we are still required to initialize the buffer:\n+      \/\/ When we have a larval receiver invoked on an abstract (value class) constructor or the Object constructor (that\n+      \/\/ is not going to be inlined). After this call, the larval is completely initialized and thus not a larval anymore.\n+      \/\/ We therefore need to force an initialization of the buffer to not lose all the field writes so far in case the\n+      \/\/ buffer needs to be used (e.g. to read from when deoptimizing at runtime) or further updated in abstract super\n+      \/\/ value class constructors which could have more fields to be initialized. Note that we do not need to\n+      \/\/ initialize the buffer when invoking another constructor in the same class on a larval receiver because we\n+      \/\/ have not initialized any fields, yet (this is done completely by the other constructor call).\n+      if (inline_type->is_larval() && !is_larval_receiver_on_super_constructor) {\n+        must_init_buffer = false;\n+      }\n+      arg = inline_type->buffer(this, true, must_init_buffer);\n+    }\n+    if (t != Type::HALF) {\n+      arg_num++;\n+    }\n+    call->init_req(idx++, arg);\n@@ -1834,7 +1987,0 @@\n-  \/\/ Capture the return value, if any.\n-  Node* ret;\n-  if (call->method() == nullptr ||\n-      call->method()->return_type()->basic_type() == T_VOID)\n-        ret = top();\n-  else  ret = _gvn.transform(new ProjNode(call, TypeFunc::Parms));\n-\n@@ -1853,0 +1999,36 @@\n+\n+  \/\/ Capture the return value, if any.\n+  Node* ret;\n+  if (call->method() == nullptr || call->method()->return_type()->basic_type() == T_VOID) {\n+    ret = top();\n+  } else if (call->tf()->returns_inline_type_as_fields()) {\n+    \/\/ Return of multiple values (inline type fields): we create a\n+    \/\/ InlineType node, each field is a projection from the call.\n+    ciInlineKlass* vk = call->method()->return_type()->as_inline_klass();\n+    uint base_input = TypeFunc::Parms;\n+    ret = InlineTypeNode::make_from_multi(this, call, vk, base_input, false, false);\n+  } else {\n+    ret = _gvn.transform(new ProjNode(call, TypeFunc::Parms));\n+    ciType* t = call->method()->return_type();\n+    if (t->is_klass()) {\n+      const Type* type = TypeOopPtr::make_from_klass(t->as_klass());\n+      if (type->is_inlinetypeptr()) {\n+        ret = InlineTypeNode::make_from_oop(this, ret, type->inline_klass());\n+      }\n+    }\n+  }\n+\n+  \/\/ We just called the constructor on a value type receiver. Reload it from the buffer\n+  ciMethod* method = call->method();\n+  if (method->is_object_constructor() && !method->holder()->is_java_lang_Object()) {\n+    InlineTypeNode* inline_type_receiver = call->in(TypeFunc::Parms)->isa_InlineType();\n+    if (inline_type_receiver != nullptr) {\n+      assert(inline_type_receiver->is_larval(), \"must be larval\");\n+      assert(inline_type_receiver->is_allocated(&gvn()), \"larval must be buffered\");\n+      InlineTypeNode* reloaded = InlineTypeNode::make_from_oop(this, inline_type_receiver->get_oop(),\n+                                                               inline_type_receiver->bottom_type()->inline_klass());\n+      assert(!reloaded->is_larval(), \"should not be larval anymore\");\n+      replace_in_map(inline_type_receiver, reloaded);\n+    }\n+  }\n+\n@@ -1943,2 +2125,1 @@\n-  CallProjections callprojs;\n-  call->extract_projections(&callprojs, true, do_asserts);\n+  CallProjections* callprojs = call->extract_projections(true, do_asserts);\n@@ -1953,2 +2134,2 @@\n-  if (callprojs.fallthrough_catchproj != nullptr) {\n-    C->gvn_replace_by(callprojs.fallthrough_catchproj, final_ctl);\n+  if (callprojs->fallthrough_catchproj != nullptr) {\n+    C->gvn_replace_by(callprojs->fallthrough_catchproj, final_ctl);\n@@ -1956,1 +2137,1 @@\n-  if (callprojs.fallthrough_memproj != nullptr) {\n+  if (callprojs->fallthrough_memproj != nullptr) {\n@@ -1961,1 +2142,1 @@\n-    C->gvn_replace_by(callprojs.fallthrough_memproj,   final_mem);\n+    C->gvn_replace_by(callprojs->fallthrough_memproj,   final_mem);\n@@ -1964,2 +2145,2 @@\n-  if (callprojs.fallthrough_ioproj != nullptr) {\n-    C->gvn_replace_by(callprojs.fallthrough_ioproj,    final_io);\n+  if (callprojs->fallthrough_ioproj != nullptr) {\n+    C->gvn_replace_by(callprojs->fallthrough_ioproj,    final_io);\n@@ -1969,2 +2150,6 @@\n-  if (callprojs.resproj != nullptr && result != nullptr) {\n-    C->gvn_replace_by(callprojs.resproj, result);\n+  if (callprojs->resproj[0] != nullptr && result != nullptr) {\n+    \/\/ If the inlined code is dead, the result projections for an inline type returned as\n+    \/\/ fields have not been replaced. They will go away once the call is replaced by TOP below.\n+    assert(callprojs->nb_resproj == 1 || (call->tf()->returns_inline_type_as_fields() && stopped()),\n+           \"unexpected number of results\");\n+    C->gvn_replace_by(callprojs->resproj[0], result);\n@@ -1975,2 +2160,2 @@\n-    if (callprojs.catchall_catchproj != nullptr) {\n-      C->gvn_replace_by(callprojs.catchall_catchproj, C->top());\n+    if (callprojs->catchall_catchproj != nullptr) {\n+      C->gvn_replace_by(callprojs->catchall_catchproj, C->top());\n@@ -1978,2 +2163,2 @@\n-    if (callprojs.catchall_memproj != nullptr) {\n-      C->gvn_replace_by(callprojs.catchall_memproj,   C->top());\n+    if (callprojs->catchall_memproj != nullptr) {\n+      C->gvn_replace_by(callprojs->catchall_memproj,   C->top());\n@@ -1981,2 +2166,2 @@\n-    if (callprojs.catchall_ioproj != nullptr) {\n-      C->gvn_replace_by(callprojs.catchall_ioproj,    C->top());\n+    if (callprojs->catchall_ioproj != nullptr) {\n+      C->gvn_replace_by(callprojs->catchall_ioproj,    C->top());\n@@ -1985,2 +2170,2 @@\n-    if (callprojs.exobj != nullptr) {\n-      C->gvn_replace_by(callprojs.exobj, C->top());\n+    if (callprojs->exobj != nullptr) {\n+      C->gvn_replace_by(callprojs->exobj, C->top());\n@@ -1997,2 +2182,2 @@\n-    if (callprojs.catchall_catchproj != nullptr) {\n-      C->gvn_replace_by(callprojs.catchall_catchproj, ekit.control());\n+    if (callprojs->catchall_catchproj != nullptr) {\n+      C->gvn_replace_by(callprojs->catchall_catchproj, ekit.control());\n@@ -2001,1 +2186,1 @@\n-    if (callprojs.catchall_memproj != nullptr) {\n+    if (callprojs->catchall_memproj != nullptr) {\n@@ -2003,1 +2188,1 @@\n-      C->gvn_replace_by(callprojs.catchall_memproj,   ex_mem);\n+      C->gvn_replace_by(callprojs->catchall_memproj,   ex_mem);\n@@ -2006,2 +2191,2 @@\n-    if (callprojs.catchall_ioproj != nullptr) {\n-      C->gvn_replace_by(callprojs.catchall_ioproj,    ekit.i_o());\n+    if (callprojs->catchall_ioproj != nullptr) {\n+      C->gvn_replace_by(callprojs->catchall_ioproj,    ekit.i_o());\n@@ -2011,2 +2196,2 @@\n-    if (callprojs.exobj != nullptr) {\n-      C->gvn_replace_by(callprojs.exobj, ex_oop);\n+    if (callprojs->exobj != nullptr) {\n+      C->gvn_replace_by(callprojs->exobj, ex_oop);\n@@ -2026,1 +2211,1 @@\n-  if (callprojs.fallthrough_catchproj != nullptr && !final_ctl->is_top() && do_replaced_nodes) {\n+  if (callprojs->fallthrough_catchproj != nullptr && !final_ctl->is_top() && do_replaced_nodes) {\n@@ -2226,1 +2411,1 @@\n-    const TypePtr* ptr = (ptr_kind == ProfileMaybeNull && current_type->speculative_maybe_null()) ? TypePtr::BOTTOM : TypePtr::NOTNULL;\n+    const TypePtr* ptr = (ptr_kind != ProfileNeverNull && current_type->speculative_maybe_null()) ? TypePtr::BOTTOM : TypePtr::NOTNULL;\n@@ -2249,1 +2434,1 @@\n-    const TypeOopPtr* spec_type = TypeOopPtr::make(TypePtr::BotPTR, Type::OffsetBot, TypeOopPtr::InstanceBot, speculative);\n+    const TypeOopPtr* spec_type = TypeOopPtr::make(TypePtr::BotPTR, Type::Offset::bottom, TypeOopPtr::InstanceBot, speculative);\n@@ -2283,2 +2468,9 @@\n-      if (!data->as_BitData()->null_seen()) {\n-        ptr_kind = ProfileNeverNull;\n+      if (java_bc() == Bytecodes::_aastore) {\n+        ciKlass* array_type = nullptr;\n+        ciKlass* element_type = nullptr;\n+        ProfilePtrKind element_ptr = ProfileMaybeNull;\n+        bool flat_array = true;\n+        bool null_free_array = true;\n+        method()->array_access_profiled_type(bci(), array_type, element_type, element_ptr, flat_array, null_free_array);\n+        exact_kls = element_type;\n+        ptr_kind = element_ptr;\n@@ -2286,7 +2478,11 @@\n-        assert(data->is_ReceiverTypeData(), \"bad profile data type\");\n-        ciReceiverTypeData* call = (ciReceiverTypeData*)data->as_ReceiverTypeData();\n-        uint i = 0;\n-        for (; i < call->row_limit(); i++) {\n-          ciKlass* receiver = call->receiver(i);\n-          if (receiver != nullptr) {\n-            break;\n+        if (!data->as_BitData()->null_seen()) {\n+          ptr_kind = ProfileNeverNull;\n+        } else {\n+          assert(data->is_ReceiverTypeData(), \"bad profile data type\");\n+          ciReceiverTypeData* call = (ciReceiverTypeData*)data->as_ReceiverTypeData();\n+          uint i = 0;\n+          for (; i < call->row_limit(); i++) {\n+            ciKlass* receiver = call->receiver(i);\n+            if (receiver != nullptr) {\n+              break;\n+            }\n@@ -2294,0 +2490,1 @@\n+          ptr_kind = (i == call->row_limit()) ? ProfileAlwaysNull : ProfileMaybeNull;\n@@ -2295,1 +2492,0 @@\n-        ptr_kind = (i == call->row_limit()) ? ProfileAlwaysNull : ProfileMaybeNull;\n@@ -2314,1 +2510,1 @@\n-  int             nargs = tf->domain()->cnt() - TypeFunc::Parms;\n+  int             nargs = tf->domain_sig()->cnt() - TypeFunc::Parms;\n@@ -2317,1 +2513,1 @@\n-    const Type *targ = tf->domain()->field_at(j + TypeFunc::Parms);\n+    const Type *targ = tf->domain_sig()->field_at(j + TypeFunc::Parms);\n@@ -2477,1 +2673,1 @@\n-    uint num_bits = call_type->range()->field_at(TypeFunc::Parms)->is_vect()->length_in_bytes() * BitsPerByte;\n+    uint num_bits = call_type->range_sig()->field_at(TypeFunc::Parms)->is_vect()->length_in_bytes() * BitsPerByte;\n@@ -2557,0 +2753,1 @@\n+\n@@ -2859,0 +3056,5 @@\n+  const Type* sub_t = _gvn.type(obj_or_subklass);\n+  if (sub_t->make_oopptr() != nullptr && sub_t->make_oopptr()->is_inlinetypeptr()) {\n+    sub_t = TypeKlassPtr::make(sub_t->inline_klass());\n+    obj_or_subklass = makecon(sub_t);\n+  }\n@@ -2864,1 +3066,1 @@\n-    if (!_gvn.type(obj_or_subklass)->isa_klassptr()) {\n+    if (!sub_t->isa_klassptr()) {\n@@ -2882,2 +3084,1 @@\n-                                    float prob,\n-                                    Node* *casted_receiver) {\n+                                    float prob, Node* *casted_receiver) {\n@@ -2885,1 +3086,12 @@\n-\n+  Node* fail = top();\n+  const Type* rec_t = _gvn.type(receiver);\n+  if (rec_t->is_inlinetypeptr()) {\n+    if (klass->equals(rec_t->inline_klass())) {\n+      (*casted_receiver) = receiver; \/\/ Always passes\n+    } else {\n+      (*casted_receiver) = top();    \/\/ Always fails\n+      fail = control();\n+      set_control(top());\n+    }\n+    return fail;\n+  }\n@@ -2888,6 +3100,1 @@\n-  Node* want_klass = makecon(tklass);\n-  Node* cmp = _gvn.transform(new CmpPNode(recv_klass, want_klass));\n-  Node* bol = _gvn.transform(new BoolNode(cmp, BoolTest::eq));\n-  IfNode* iff = create_and_xform_if(control(), bol, prob, COUNT_UNKNOWN);\n-  set_control( _gvn.transform(new IfTrueNode (iff)));\n-  Node* fail = _gvn.transform(new IfFalseNode(iff));\n+  fail = type_check(recv_klass, tklass, prob);\n@@ -2897,2 +3104,2 @@\n-    const TypeOopPtr* recvx_type = tklass->as_instance_type();\n-    assert(recvx_type->klass_is_exact(), \"\");\n+    const TypeOopPtr* recv_xtype = tklass->as_instance_type();\n+    assert(recv_xtype->klass_is_exact(), \"\");\n@@ -2900,1 +3107,1 @@\n-    if (!receiver_type->higher_equal(recvx_type)) { \/\/ ignore redundant casts\n+    if (!receiver_type->higher_equal(recv_xtype)) { \/\/ ignore redundant casts\n@@ -2903,2 +3110,7 @@\n-      Node* cast = new CheckCastPPNode(control(), receiver, recvx_type);\n-      (*casted_receiver) = _gvn.transform(cast);\n+      Node* cast = new CheckCastPPNode(control(), receiver, recv_xtype);\n+      Node* res = _gvn.transform(cast);\n+      if (recv_xtype->is_inlinetypeptr()) {\n+        assert(!gvn().type(res)->maybe_null(), \"receiver should never be null\");\n+        res = InlineTypeNode::make_from_oop(this, res, recv_xtype->inline_klass());\n+      }\n+      (*casted_receiver) = res;\n@@ -2913,0 +3125,11 @@\n+Node* GraphKit::type_check(Node* recv_klass, const TypeKlassPtr* tklass,\n+                           float prob) {\n+  Node* want_klass = makecon(tklass);\n+  Node* cmp = _gvn.transform(new CmpPNode(recv_klass, want_klass));\n+  Node* bol = _gvn.transform(new BoolNode(cmp, BoolTest::eq));\n+  IfNode* iff = create_and_xform_if(control(), bol, prob, COUNT_UNKNOWN);\n+  set_control(_gvn.transform(new IfTrueNode (iff)));\n+  Node* fail = _gvn.transform(new IfFalseNode(iff));\n+  return fail;\n+}\n+\n@@ -2925,3 +3148,6 @@\n-    if (!receiver_type->higher_equal(recv_type)) { \/\/ ignore redundant casts\n-      Node* cast = new CheckCastPPNode(control(), receiver, recv_type);\n-      (*casted_receiver) = _gvn.transform(cast);\n+    if (receiver_type != nullptr && !receiver_type->higher_equal(recv_type)) { \/\/ ignore redundant casts\n+      Node* cast = _gvn.transform(new CheckCastPPNode(control(), receiver, recv_type));\n+      if (recv_type->is_inlinetypeptr()) {\n+        cast = InlineTypeNode::make_from_oop(this, cast, recv_type->inline_klass());\n+      }\n+      (*casted_receiver) = cast;\n@@ -3036,1 +3262,14 @@\n-  ciKlass* exact_kls = spec_klass == nullptr ? profile_has_unique_klass() : spec_klass;\n+  ciKlass* exact_kls = spec_klass;\n+  if (exact_kls == nullptr) {\n+    if (java_bc() == Bytecodes::_aastore) {\n+      ciKlass* array_type = nullptr;\n+      ciKlass* element_type = nullptr;\n+      ProfilePtrKind element_ptr = ProfileMaybeNull;\n+      bool flat_array = true;\n+      bool null_free_array = true;\n+      method()->array_access_profiled_type(bci(), array_type, element_type, element_ptr, flat_array, null_free_array);\n+      exact_kls = element_type;\n+    } else {\n+      exact_kls = profile_has_unique_klass();\n+    }\n+  }\n@@ -3166,1 +3405,1 @@\n-    if (subk->is_loaded()) {\n+    if (subk != nullptr && subk->is_loaded()) {\n@@ -3222,2 +3461,1 @@\n-Node* GraphKit::gen_checkcast(Node *obj, Node* superklass,\n-                              Node* *failure_control) {\n+Node* GraphKit::gen_checkcast(Node* obj, Node* superklass, Node* *failure_control, bool null_free) {\n@@ -3226,0 +3464,13 @@\n+  const Type* obj_type = _gvn.type(obj);\n+  if (obj_type->is_inlinetypeptr() && !obj_type->maybe_null() && klass_ptr_type->klass_is_exact() && obj_type->inline_klass() == klass_ptr_type->exact_klass(true)) {\n+    \/\/ Special case: larval inline objects must not be scalarized. They are also generally not\n+    \/\/ allowed to participate in most operations except as the first operand of putfield, or as an\n+    \/\/ argument to a constructor invocation with it being a receiver, Unsafe::putXXX with it being\n+    \/\/ the first argument, or Unsafe::finishPrivateBuffer. This allows us to aggressively scalarize\n+    \/\/ value objects in all other places. This special case comes from the limitation of the Java\n+    \/\/ language, Unsafe::makePrivateBuffer returns an Object that is checkcast-ed to the concrete\n+    \/\/ value type. We must do this first because C->static_subtype_check may do nothing when\n+    \/\/ StressReflectiveCode is set.\n+    return obj;\n+  }\n+\n@@ -3228,0 +3479,2 @@\n+  bool safe_for_replace = (failure_control == nullptr);\n+  assert(!null_free || toop->can_be_inline_type(), \"must be an inline type pointer\");\n@@ -3236,3 +3489,10 @@\n-    const TypeOopPtr* objtp = _gvn.type(obj)->isa_oopptr();\n-    if (objtp != nullptr) {\n-      switch (C->static_subtype_check(improved_klass_ptr_type, objtp->as_klass_type())) {\n+    const TypeKlassPtr* kptr = nullptr;\n+    if (obj_type->isa_oop_ptr()) {\n+      kptr = obj_type->is_oopptr()->as_klass_type();\n+    } else if (obj->is_InlineType()) {\n+      ciInlineKlass* vk = obj_type->inline_klass();\n+      kptr = TypeInstKlassPtr::make(TypePtr::NotNull, vk, Type::Offset(0));\n+    }\n+\n+    if (kptr != nullptr) {\n+      switch (C->static_subtype_check(improved_klass_ptr_type, kptr)) {\n@@ -3243,1 +3503,7 @@\n-        return record_profiled_receiver_for_speculation(obj);\n+        obj = record_profiled_receiver_for_speculation(obj);\n+        if (null_free) {\n+          assert(safe_for_replace, \"must be\");\n+          obj = null_check(obj);\n+        }\n+        assert(stopped() || !toop->is_inlinetypeptr() || obj->is_InlineType(), \"should have been scalarized\");\n+        return obj;\n@@ -3245,0 +3511,4 @@\n+        if (null_free) {\n+          assert(safe_for_replace, \"must be\");\n+          obj = null_check(obj);\n+        }\n@@ -3246,2 +3516,1 @@\n-        \/\/ A non-null value will always produce an exception.\n-        if (!objtp->maybe_null()) {\n+        if (obj_type->isa_oopptr() != nullptr && !obj_type->is_oopptr()->maybe_null()) {\n@@ -3264,1 +3533,0 @@\n-  bool safe_for_replace = false;\n@@ -3269,2 +3537,3 @@\n-    data = method()->method_data()->bci_to_data(bci());\n-    safe_for_replace = true;\n+    if (method()->method_data()->is_mature()) {\n+      data = method()->method_data()->bci_to_data(bci());\n+    }\n@@ -3277,0 +3546,3 @@\n+  _gvn.set_type(region, Type::CONTROL);\n+  _gvn.set_type(phi, toop);\n+\n@@ -3284,0 +3556,7 @@\n+  if (obj->is_InlineType()) {\n+    \/\/ Re-execute if buffering during triggers deoptimization\n+    PreserveReexecuteState preexecs(this);\n+    jvms()->set_should_reexecute(true);\n+    obj = obj->as_InlineType()->buffer(this, safe_for_replace);\n+  }\n+\n@@ -3286,1 +3565,7 @@\n-  Node* not_null_obj = null_check_oop(obj, &null_ctl, never_see_null, safe_for_replace, speculative_not_null);\n+  Node* not_null_obj = nullptr;\n+  if (null_free) {\n+    assert(safe_for_replace, \"must be\");\n+    not_null_obj = null_check(obj);\n+  } else {\n+    not_null_obj = null_check_oop(obj, &null_ctl, never_see_null, safe_for_replace, speculative_not_null);\n+  }\n@@ -3291,0 +3576,3 @@\n+    if (toop->is_inlinetypeptr()) {\n+      return InlineTypeNode::make_null(_gvn, toop->inline_klass());\n+    }\n@@ -3328,0 +3616,3 @@\n+      \/\/ Only improve the super class for constants which allows subsequent sub type checks to possibly be commoned up.\n+      \/\/ The other non-constant cases cannot be improved with a cast node here since they could be folded to top.\n+      \/\/ Additionally, the benefit would only be minor in non-constant cases.\n@@ -3331,1 +3622,0 @@\n-\n@@ -3339,0 +3629,6 @@\n+        Node* obj_klass = nullptr;\n+        if (not_null_obj->is_InlineType()) {\n+          obj_klass = makecon(TypeKlassPtr::make(_gvn.type(not_null_obj)->inline_klass()));\n+        } else {\n+          obj_klass = load_object_klass(not_null_obj);\n+        }\n@@ -3369,1 +3665,164 @@\n-  return record_profiled_receiver_for_speculation(res);\n+  bool not_inline = !toop->can_be_inline_type();\n+  bool not_flat_in_array = !UseArrayFlattening || not_inline || (toop->is_inlinetypeptr() && !toop->inline_klass()->flat_in_array());\n+  if (EnableValhalla && (not_inline || not_flat_in_array)) {\n+    \/\/ Check if obj has been loaded from an array\n+    obj = obj->isa_DecodeN() ? obj->in(1) : obj;\n+    Node* array = nullptr;\n+    if (obj->isa_Load()) {\n+      Node* address = obj->in(MemNode::Address);\n+      if (address->isa_AddP()) {\n+        array = address->as_AddP()->in(AddPNode::Base);\n+      }\n+    } else if (obj->is_Phi()) {\n+      Node* region = obj->in(0);\n+      \/\/ TODO make this more robust (see JDK-8231346)\n+      if (region->req() == 3 && region->in(2) != nullptr && region->in(2)->in(0) != nullptr) {\n+        IfNode* iff = region->in(2)->in(0)->isa_If();\n+        if (iff != nullptr) {\n+          iff->is_flat_array_check(&_gvn, &array);\n+        }\n+      }\n+    }\n+    if (array != nullptr) {\n+      const TypeAryPtr* ary_t = _gvn.type(array)->isa_aryptr();\n+      if (ary_t != nullptr) {\n+        if (!ary_t->is_not_null_free() && !ary_t->is_null_free() && not_inline) {\n+          \/\/ Casting array element to a non-inline-type, mark array as not null-free.\n+          Node* cast = _gvn.transform(new CheckCastPPNode(control(), array, ary_t->cast_to_not_null_free()));\n+          replace_in_map(array, cast);\n+          array = cast;\n+        }\n+        if (!ary_t->is_not_flat() && !ary_t->is_flat() && not_flat_in_array) {\n+          \/\/ Casting array element to a non-flat-in-array type, mark array as not flat.\n+          Node* cast = _gvn.transform(new CheckCastPPNode(control(), array, ary_t->cast_to_not_flat()));\n+          replace_in_map(array, cast);\n+          array = cast;\n+        }\n+      }\n+    }\n+  }\n+\n+  if (!stopped() && !res->is_InlineType()) {\n+    res = record_profiled_receiver_for_speculation(res);\n+    if (toop->is_inlinetypeptr()) {\n+      Node* vt = InlineTypeNode::make_from_oop(this, res, toop->inline_klass());\n+      res = vt;\n+      if (safe_for_replace) {\n+        replace_in_map(obj, vt);\n+        replace_in_map(not_null_obj, vt);\n+        replace_in_map(res, vt);\n+      }\n+    }\n+  }\n+  return res;\n+}\n+\n+Node* GraphKit::mark_word_test(Node* obj, uintptr_t mask_val, bool eq, bool check_lock) {\n+  \/\/ Load markword\n+  Node* mark_adr = basic_plus_adr(obj, oopDesc::mark_offset_in_bytes());\n+  Node* mark = make_load(nullptr, mark_adr, TypeX_X, TypeX_X->basic_type(), MemNode::unordered);\n+  if (check_lock) {\n+    \/\/ Check if obj is locked\n+    Node* locked_bit = MakeConX(markWord::unlocked_value);\n+    locked_bit = _gvn.transform(new AndXNode(locked_bit, mark));\n+    Node* cmp = _gvn.transform(new CmpXNode(locked_bit, MakeConX(0)));\n+    Node* is_unlocked = _gvn.transform(new BoolNode(cmp, BoolTest::ne));\n+    IfNode* iff = new IfNode(control(), is_unlocked, PROB_MAX, COUNT_UNKNOWN);\n+    _gvn.transform(iff);\n+    Node* locked_region = new RegionNode(3);\n+    Node* mark_phi = new PhiNode(locked_region, TypeX_X);\n+\n+    \/\/ Unlocked: Use bits from mark word\n+    locked_region->init_req(1, _gvn.transform(new IfTrueNode(iff)));\n+    mark_phi->init_req(1, mark);\n+\n+    \/\/ Locked: Load prototype header from klass\n+    set_control(_gvn.transform(new IfFalseNode(iff)));\n+    \/\/ Make loads control dependent to make sure they are only executed if array is locked\n+    Node* klass_adr = basic_plus_adr(obj, oopDesc::klass_offset_in_bytes());\n+    Node* klass = _gvn.transform(LoadKlassNode::make(_gvn, C->immutable_memory(), klass_adr, TypeInstPtr::KLASS, TypeInstKlassPtr::OBJECT));\n+    Node* proto_adr = basic_plus_adr(klass, in_bytes(Klass::prototype_header_offset()));\n+    Node* proto = _gvn.transform(LoadNode::make(_gvn, control(), C->immutable_memory(), proto_adr, proto_adr->bottom_type()->is_ptr(), TypeX_X, TypeX_X->basic_type(), MemNode::unordered));\n+\n+    locked_region->init_req(2, control());\n+    mark_phi->init_req(2, proto);\n+    set_control(_gvn.transform(locked_region));\n+    record_for_igvn(locked_region);\n+\n+    mark = mark_phi;\n+  }\n+\n+  \/\/ Now check if mark word bits are set\n+  Node* mask = MakeConX(mask_val);\n+  Node* masked = _gvn.transform(new AndXNode(_gvn.transform(mark), mask));\n+  record_for_igvn(masked); \/\/ Give it a chance to be optimized out by IGVN\n+  Node* cmp = _gvn.transform(new CmpXNode(masked, mask));\n+  return _gvn.transform(new BoolNode(cmp, eq ? BoolTest::eq : BoolTest::ne));\n+}\n+\n+Node* GraphKit::inline_type_test(Node* obj, bool is_inline) {\n+  return mark_word_test(obj, markWord::inline_type_pattern, is_inline, \/* check_lock = *\/ false);\n+}\n+\n+Node* GraphKit::flat_array_test(Node* array_or_klass, bool flat) {\n+  \/\/ We can't use immutable memory here because the mark word is mutable.\n+  \/\/ PhaseIdealLoop::move_flat_array_check_out_of_loop will make sure the\n+  \/\/ check is moved out of loops (mainly to enable loop unswitching).\n+  Node* cmp = _gvn.transform(new FlatArrayCheckNode(C, memory(Compile::AliasIdxRaw), array_or_klass));\n+  record_for_igvn(cmp); \/\/ Give it a chance to be optimized out by IGVN\n+  return _gvn.transform(new BoolNode(cmp, flat ? BoolTest::eq : BoolTest::ne));\n+}\n+\n+Node* GraphKit::null_free_array_test(Node* array, bool null_free) {\n+  return mark_word_test(array, markWord::null_free_array_bit_in_place, null_free);\n+}\n+\n+Node* GraphKit::null_free_atomic_array_test(Node* array, ciInlineKlass* vk) {\n+  assert(vk->has_atomic_layout() || vk->has_non_atomic_layout(), \"Can't be null-free and flat\");\n+\n+  \/\/ TODO 8350865 Add a stress flag to always access atomic if layout exists?\n+  if (!vk->has_non_atomic_layout()) {\n+    return intcon(1); \/\/ Always atomic\n+  } else if (!vk->has_atomic_layout()) {\n+    return intcon(0); \/\/ Never atomic\n+  }\n+\n+  Node* array_klass = load_object_klass(array);\n+  int layout_kind_offset = in_bytes(FlatArrayKlass::layout_kind_offset());\n+  Node* layout_kind_addr = basic_plus_adr(array_klass, array_klass, layout_kind_offset);\n+  Node* layout_kind = make_load(nullptr, layout_kind_addr, TypeInt::INT, T_INT, MemNode::unordered);\n+  Node* cmp = _gvn.transform(new CmpINode(layout_kind, intcon((int)LayoutKind::ATOMIC_FLAT)));\n+  return _gvn.transform(new BoolNode(cmp, BoolTest::eq));\n+}\n+\n+\/\/ Deoptimize if 'ary' is a null-free inline type array and 'val' is null\n+Node* GraphKit::inline_array_null_guard(Node* ary, Node* val, int nargs, bool safe_for_replace) {\n+  RegionNode* region = new RegionNode(3);\n+  Node* null_ctl = top();\n+  null_check_oop(val, &null_ctl);\n+  if (null_ctl != top()) {\n+    PreserveJVMState pjvms(this);\n+    set_control(null_ctl);\n+    {\n+      \/\/ Deoptimize if null-free array\n+      BuildCutout unless(this, null_free_array_test(ary, \/* null_free = *\/ false), PROB_MAX);\n+      inc_sp(nargs);\n+      uncommon_trap(Deoptimization::Reason_null_check,\n+                    Deoptimization::Action_none);\n+    }\n+    region->init_req(1, control());\n+  }\n+  region->init_req(2, control());\n+  set_control(_gvn.transform(region));\n+  record_for_igvn(region);\n+  if (_gvn.type(val) == TypePtr::NULL_PTR) {\n+    \/\/ Since we were just successfully storing null, the array can't be null free.\n+    const TypeAryPtr* ary_t = _gvn.type(ary)->is_aryptr();\n+    ary_t = ary_t->cast_to_not_null_free();\n+    Node* cast = _gvn.transform(new CheckCastPPNode(control(), ary, ary_t));\n+    if (safe_for_replace) {\n+      replace_in_map(ary, cast);\n+    }\n+    ary = cast;\n+  }\n+  return ary;\n@@ -3437,0 +3896,1 @@\n+\n@@ -3505,0 +3965,1 @@\n+  assert(!obj->is_InlineType(), \"should not unlock on inline type\");\n@@ -3545,1 +4006,8 @@\n-    if (xklass || (klass_t->isa_aryklassptr() && klass_t->is_aryklassptr()->elem() != Type::BOTTOM)) {\n+    bool can_be_flat = false;\n+    const TypeAryPtr* ary_type = klass_t->as_instance_type()->isa_aryptr();\n+    if (UseArrayFlattening && !xklass && ary_type != nullptr && !ary_type->is_null_free()) {\n+      \/\/ Don't constant fold if the runtime type might be a flat array but the static type is not.\n+      const TypeOopPtr* elem = ary_type->elem()->make_oopptr();\n+      can_be_flat = ary_type->can_be_inline_array() && (!elem->is_inlinetypeptr() || elem->inline_klass()->flat_in_array());\n+    }\n+    if (!can_be_flat && (xklass || (klass_t->isa_aryklassptr() && klass_t->is_aryklassptr()->elem() != Type::BOTTOM))) {\n@@ -3547,2 +4015,4 @@\n-      if (klass_t->isa_aryklassptr()) {\n-        BasicType elem = klass_t->as_instance_type()->isa_aryptr()->elem()->array_element_basic_type();\n+      if (klass_t->is_flat()) {\n+        lhelper = ary_type->flat_layout_helper();\n+      } else if (klass_t->isa_aryklassptr()) {\n+        BasicType elem = ary_type->elem()->array_element_basic_type();\n@@ -3577,1 +4047,3 @@\n-  kit.set_memory(init_out_raw, alias_idx);\n+  if (init_out_raw != nullptr) {\n+    kit.set_memory(init_out_raw, alias_idx);\n+  }\n@@ -3616,0 +4088,1 @@\n+    _gvn.set_type(minit_in, Type::MEMORY);\n@@ -3623,3 +4096,28 @@\n-      const TypePtr* telemref = oop_type->add_offset(Type::OffsetBot);\n-      int            elemidx  = C->get_alias_index(telemref);\n-      hook_memory_on_init(*this, elemidx, minit_in, minit_out);\n+      const TypeAryPtr* arytype = oop_type->is_aryptr();\n+      if (arytype->is_flat()) {\n+        \/\/ Initially all flat array accesses share a single slice\n+        \/\/ but that changes after parsing. Prepare the memory graph so\n+        \/\/ it can optimize flat array accesses properly once they\n+        \/\/ don't share a single slice.\n+        assert(C->flat_accesses_share_alias(), \"should be set at parse time\");\n+        C->set_flat_accesses_share_alias(false);\n+        ciInlineKlass* vk = arytype->elem()->inline_klass();\n+        for (int i = 0, len = vk->nof_nonstatic_fields(); i < len; i++) {\n+          ciField* field = vk->nonstatic_field_at(i);\n+          if (field->offset_in_bytes() >= TrackedInitializationLimit * HeapWordSize)\n+            continue;  \/\/ do not bother to track really large numbers of fields\n+          int off_in_vt = field->offset_in_bytes() - vk->payload_offset();\n+          const TypePtr* adr_type = arytype->with_field_offset(off_in_vt)->add_offset(Type::OffsetBot);\n+          int fieldidx = C->get_alias_index(adr_type, true);\n+          \/\/ Pass nullptr for init_out. Having per flat array element field memory edges as uses of the Initialize node\n+          \/\/ can result in per flat array field Phis to be created which confuses the logic of\n+          \/\/ Compile::adjust_flat_array_access_aliases().\n+          hook_memory_on_init(*this, fieldidx, minit_in, nullptr);\n+        }\n+        C->set_flat_accesses_share_alias(true);\n+        hook_memory_on_init(*this, C->get_alias_index(TypeAryPtr::INLINES), minit_in, minit_out);\n+      } else {\n+        const TypePtr* telemref = oop_type->add_offset(Type::OffsetBot);\n+        int            elemidx  = C->get_alias_index(telemref);\n+        hook_memory_on_init(*this, elemidx, minit_in, minit_out);\n+      }\n@@ -3627,0 +4125,1 @@\n+      set_memory(minit_out, C->get_alias_index(oop_type)); \/\/ mark word\n@@ -3677,1 +4176,2 @@\n-                             bool deoptimize_on_exception) {\n+                             bool deoptimize_on_exception,\n+                             InlineTypeNode* inline_type_node) {\n@@ -3684,1 +4184,1 @@\n-  int   layout_is_con = (layout_val == nullptr);\n+  bool  layout_is_con = (layout_val == nullptr);\n@@ -3735,1 +4235,1 @@\n-  \/\/ since GC and deoptimization can happened.\n+  \/\/ since GC and deoptimization can happen.\n@@ -3742,1 +4242,1 @@\n-                                         initial_slow_test);\n+                                         initial_slow_test, inline_type_node);\n@@ -3748,1 +4248,1 @@\n-\/\/ helper for both newarray and anewarray\n+\/\/ helper for newarray and anewarray\n@@ -3758,1 +4258,2 @@\n-                          bool deoptimize_on_exception) {\n+                          bool deoptimize_on_exception,\n+                          Node* init_val) {\n@@ -3761,1 +4262,1 @@\n-  int   layout_is_con = (layout_val == nullptr);\n+  bool  layout_is_con = (layout_val == nullptr);\n@@ -3791,1 +4292,1 @@\n-    fast_size_limit <<= (LogBytesPerLong - log2_esize);\n+    fast_size_limit <<= MAX2(LogBytesPerLong - log2_esize, 0);\n@@ -3808,0 +4309,1 @@\n+    bool is_flat_array = Klass::layout_helper_is_flatArray(layout_con);\n@@ -3810,1 +4312,1 @@\n-    assert((hsize & right_n_bits(eshift)) == 0, \"hsize is pre-rounded\");\n+    assert(is_flat_array || (hsize & right_n_bits(eshift)) == 0, \"hsize is pre-rounded\");\n@@ -3897,1 +4399,1 @@\n-  \/\/ since GC and deoptimization can happened.\n+  \/\/ since GC and deoptimization can happen.\n@@ -3906,1 +4408,21 @@\n-  const TypeOopPtr* ary_type = _gvn.type(klass_node)->is_klassptr()->as_instance_type();\n+  const TypeKlassPtr* ary_klass = _gvn.type(klass_node)->isa_klassptr();\n+  const TypeOopPtr* ary_type = ary_klass->as_instance_type();\n+\n+  Node* raw_init_value = nullptr;\n+  if (init_val != nullptr) {\n+    \/\/ TODO 8350865 Fast non-zero init not implemented yet for flat, null-free arrays\n+    if (ary_type->is_flat()) {\n+      initial_slow_test = intcon(1);\n+    }\n+\n+    if (UseCompressedOops) {\n+      \/\/ With compressed oops, the 64-bit init value is built from two 32-bit compressed oops\n+      init_val = _gvn.transform(new EncodePNode(init_val, init_val->bottom_type()->make_narrowoop()));\n+      Node* lower = _gvn.transform(new CastP2XNode(control(), init_val));\n+      Node* upper = _gvn.transform(new LShiftLNode(lower, intcon(32)));\n+      raw_init_value = _gvn.transform(new OrLNode(lower, upper));\n+    } else {\n+      raw_init_value = _gvn.transform(new CastP2XNode(control(), init_val));\n+    }\n+  }\n+\n@@ -3921,2 +4443,2 @@\n-                            length, valid_length_test);\n-\n+                            length, valid_length_test,\n+                            init_val, raw_init_value);\n@@ -4070,1 +4592,1 @@\n-                                                     false, nullptr, 0);\n+                                                     false, nullptr, Type::Offset(0));\n@@ -4073,2 +4595,2 @@\n-                                                  TypeAry::make(TypeInt::BYTE, TypeInt::POS),\n-                                                  ciTypeArrayKlass::make(T_BYTE), true, 0);\n+                                                  TypeAry::make(TypeInt::BYTE, TypeInt::POS, false, false, true, true),\n+                                                  ciTypeArrayKlass::make(T_BYTE), true, Type::Offset(0));\n@@ -4087,1 +4609,1 @@\n-                                                     false, nullptr, 0);\n+                                                     false, nullptr, Type::Offset(0));\n@@ -4099,1 +4621,1 @@\n-                                                     false, nullptr, 0);\n+                                                     false, nullptr, Type::Offset(0));\n@@ -4109,1 +4631,1 @@\n-                                                     false, nullptr, 0);\n+                                                     false, nullptr, Type::Offset(0));\n@@ -4222,1 +4744,7 @@\n-    return makecon(con_type);\n+    Node* con = makecon(con_type);\n+    if (field->type()->is_inlinetype()) {\n+      con = InlineTypeNode::make_from_oop(this, con, field->type()->as_inline_klass());\n+    } else if (con_type->is_inlinetypeptr()) {\n+      con = InlineTypeNode::make_from_oop(this, con, con_type->inline_klass());\n+    }\n+    return con;\n@@ -4227,0 +4755,9 @@\n+\/\/---------------------------load_mirror_from_klass----------------------------\n+\/\/ Given a klass oop, load its java mirror (a java.lang.Class oop).\n+Node* GraphKit::load_mirror_from_klass(Node* klass) {\n+  Node* p = basic_plus_adr(klass, in_bytes(Klass::java_mirror_offset()));\n+  Node* load = make_load(nullptr, p, TypeRawPtr::NOTNULL, T_ADDRESS, MemNode::unordered);\n+  \/\/ mirror = ((OopHandle)mirror)->resolve();\n+  return access_load(load, TypeInstPtr::MIRROR, T_OBJECT, IN_NATIVE);\n+}\n+\n@@ -4228,1 +4765,1 @@\n-  const TypeOopPtr* obj_type = obj->bottom_type()->isa_oopptr();\n+  const Type* obj_type = obj->bottom_type();\n@@ -4230,1 +4767,1 @@\n-  if (obj_type != nullptr && sig_type->is_loaded() && !obj_type->higher_equal(sig_type)) {\n+  if (obj_type->isa_oopptr() && sig_type->is_loaded() && !obj_type->higher_equal(sig_type)) {\n@@ -4233,1 +4770,4 @@\n-    return casted_obj;\n+    obj = casted_obj;\n+  }\n+  if (sig_type->is_inlinetypeptr()) {\n+    obj = InlineTypeNode::make_from_oop(this, obj, sig_type->inline_klass());\n","filename":"src\/hotspot\/share\/opto\/graphKit.cpp","additions":675,"deletions":135,"binary":false,"changes":810,"status":"modified"},{"patch":"@@ -36,0 +36,1 @@\n+#include \"opto\/inlinetypenode.hpp\"\n@@ -69,0 +70,3 @@\n+#ifdef ASSERT\n+  uint              _worklist_size;\n+#endif\n@@ -81,1 +85,1 @@\n-  GraphKit(JVMState* jvms);     \/\/ the JVM state on which to operate\n+  GraphKit(JVMState* jvms, PhaseGVN* gvn = nullptr);     \/\/ the JVM state on which to operate\n@@ -87,0 +91,7 @@\n+#if 0\n+    \/\/ During incremental inlining, the Node_Array of the C->for_igvn() worklist and the IGVN\n+    \/\/ worklist are shared but the _in_worklist VectorSet is not. To avoid inconsistencies,\n+    \/\/ we should not add nodes to the _for_igvn worklist when using IGVN for the GraphKit.\n+    assert((_gvn.is_IterGVN() == nullptr) || (_gvn.C->for_igvn()->size() == _worklist_size),\n+           \"GraphKit should not modify _for_igvn worklist after parsing\");\n+#endif\n@@ -97,1 +108,1 @@\n-  void record_for_igvn(Node* n) const { C->record_for_igvn(n); }  \/\/ delegate to Compile\n+  void record_for_igvn(Node* n) const { _gvn.record_for_igvn(n); }\n@@ -365,1 +376,2 @@\n-                          bool speculative = false);\n+                          bool speculative = false,\n+                          bool is_init_check = false);\n@@ -370,1 +382,0 @@\n-    assert(argument(0)->bottom_type()->isa_ptr(), \"must be\");\n@@ -572,1 +583,3 @@\n-                        DecoratorSet decorators);\n+                        DecoratorSet decorators,\n+                        bool safe_for_replace = true,\n+                        const InlineTypeNode* vt = nullptr);\n@@ -579,1 +592,2 @@\n-                       DecoratorSet decorators);\n+                       DecoratorSet decorators,\n+                       Node* ctl = nullptr);\n@@ -632,0 +646,2 @@\n+  Node* flat_array_element_address(Node*& array, Node* idx, ciInlineKlass* vk, bool is_null_free,\n+                                   bool is_not_null_free, bool is_atomic);\n@@ -671,1 +687,1 @@\n-  void  set_arguments_for_java_call(CallJavaNode* call);\n+  void  set_arguments_for_java_call(CallJavaNode* call, bool is_late_inline = false);\n@@ -801,2 +817,9 @@\n-  Node* gen_checkcast( Node *subobj, Node* superkls,\n-                       Node* *failure_control = nullptr );\n+  Node* gen_checkcast(Node *subobj, Node* superkls, Node* *failure_control = nullptr, bool null_free = false);\n+\n+  \/\/ Inline types\n+  Node* mark_word_test(Node* obj, uintptr_t mask_val, bool eq, bool check_lock = true);\n+  Node* inline_type_test(Node* obj, bool is_inline = true);\n+  Node* flat_array_test(Node* array_or_klass, bool flat = true);\n+  Node* null_free_array_test(Node* array, bool null_free = true);\n+  Node* null_free_atomic_array_test(Node* array, ciInlineKlass* vk);\n+  Node* inline_array_null_guard(Node* ary, Node* val, int nargs, bool safe_for_replace = false);\n@@ -811,0 +834,1 @@\n+  Node* type_check(Node* recv_klass, const TypeKlassPtr* tklass, float prob);\n@@ -824,1 +848,2 @@\n-                     bool deoptimize_on_exception = false);\n+                     bool deoptimize_on_exception = false,\n+                     InlineTypeNode* inline_type_node = nullptr);\n@@ -827,1 +852,2 @@\n-                  bool deoptimize_on_exception = false);\n+                  bool deoptimize_on_exception = false,\n+                  Node* init_val = nullptr);\n@@ -861,0 +887,1 @@\n+  Node* load_mirror_from_klass(Node* klass);\n","filename":"src\/hotspot\/share\/opto\/graphKit.hpp","additions":38,"deletions":11,"binary":false,"changes":49,"status":"modified"},{"patch":"@@ -50,1 +50,0 @@\n-  assert(!_gvn.is_IterGVN(), \"IdealKit can't be used during Optimize phase\");\n@@ -82,1 +81,0 @@\n-\n@@ -86,0 +84,4 @@\n+  if_then(bol, prob, cnt, push_new_state);\n+}\n+\n+void IdealKit::if_then(Node* bol, float prob, float cnt, bool push_new_state) {\n@@ -296,1 +298,1 @@\n-    C->record_for_igvn(n);\n+    gvn().record_for_igvn(n);\n@@ -305,1 +307,1 @@\n-  C->record_for_igvn(n);\n+  gvn().record_for_igvn(n);\n@@ -505,2 +507,2 @@\n-  if (slow_call_type->range()->cnt() > TypeFunc::Parms) {\n-    assert(slow_call_type->range()->cnt() == TypeFunc::Parms+1, \"only one return value\");\n+  if (slow_call_type->range_sig()->cnt() > TypeFunc::Parms) {\n+    assert(slow_call_type->range_sig()->cnt() == TypeFunc::Parms+1, \"only one return value\");\n","filename":"src\/hotspot\/share\/opto\/idealKit.cpp","additions":8,"deletions":6,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -26,0 +26,1 @@\n+#include \"ci\/ciFlatArrayKlass.hpp\"\n@@ -318,0 +319,2 @@\n+  case vmIntrinsics::_makePrivateBuffer:        return inline_unsafe_make_private_buffer();\n+  case vmIntrinsics::_finishPrivateBuffer:      return inline_unsafe_finish_private_buffer();\n@@ -327,0 +330,1 @@\n+  case vmIntrinsics::_getValue:                 return inline_unsafe_access(!is_store, T_OBJECT,   Relaxed, false, true);\n@@ -337,0 +341,1 @@\n+  case vmIntrinsics::_putValue:                 return inline_unsafe_access( is_store, T_OBJECT,   Relaxed, false, true);\n@@ -503,0 +508,1 @@\n+  case vmIntrinsics::_isFlatArray:              return inline_unsafe_isFlatArray();\n@@ -515,0 +521,3 @@\n+  case vmIntrinsics::_newNullRestrictedNonAtomicArray: return inline_newArray(\/* null_free *\/ true, \/* atomic *\/ false);\n+  case vmIntrinsics::_newNullRestrictedAtomicArray: return inline_newArray(\/* null_free *\/ true, \/* atomic *\/ true);\n+  case vmIntrinsics::_newNullableAtomicArray:     return inline_newArray(\/* null_free *\/ false, \/* atomic *\/ true);\n@@ -2286,0 +2295,1 @@\n+  bool null_free = false;\n@@ -2291,0 +2301,1 @@\n+      null_free = alias_type->field()->is_null_free();\n@@ -2299,0 +2310,1 @@\n+      null_free = adr_type->is_aryptr()->is_null_free();\n@@ -2311,0 +2323,3 @@\n+    if (null_free) {\n+      result = result->join_speculative(TypePtr::NOTNULL)->is_oopptr();\n+    }\n@@ -2341,1 +2356,1 @@\n-bool LibraryCallKit::inline_unsafe_access(bool is_store, const BasicType type, const AccessKind kind, const bool unaligned) {\n+bool LibraryCallKit::inline_unsafe_access(bool is_store, const BasicType type, const AccessKind kind, const bool unaligned, const bool is_flat) {\n@@ -2366,1 +2381,1 @@\n-      assert(sig->count() == 2, \"oop getter has 2 arguments\");\n+      assert(sig->count() == 2 || (is_flat && sig->count() == 3), \"oop getter has 2 or 3 arguments\");\n@@ -2372,1 +2387,1 @@\n-      assert(sig->count() == 3, \"oop putter has 3 arguments\");\n+      assert(sig->count() == 3 || (is_flat && sig->count() == 4), \"oop putter has 3 arguments\");\n@@ -2398,0 +2413,55 @@\n+\n+  ciInlineKlass* inline_klass = nullptr;\n+  if (is_flat) {\n+    const TypeInstPtr* cls = _gvn.type(argument(4))->isa_instptr();\n+    if (cls == nullptr || cls->const_oop() == nullptr) {\n+      return false;\n+    }\n+    ciType* mirror_type = cls->const_oop()->as_instance()->java_mirror_type();\n+    if (!mirror_type->is_inlinetype()) {\n+      return false;\n+    }\n+    inline_klass = mirror_type->as_inline_klass();\n+  }\n+\n+  if (base->is_InlineType()) {\n+    InlineTypeNode* vt = base->as_InlineType();\n+    if (is_store) {\n+      if (!vt->is_allocated(&_gvn)) {\n+        return false;\n+      }\n+      base = vt->get_oop();\n+    } else {\n+      if (offset->is_Con()) {\n+        long off = find_long_con(offset, 0);\n+        ciInlineKlass* vk = vt->type()->inline_klass();\n+        if ((long)(int)off != off || !vk->contains_field_offset(off)) {\n+          return false;\n+        }\n+\n+        ciField* field = vk->get_non_flat_field_by_offset(off);\n+        if (field != nullptr) {\n+          BasicType bt = type2field[field->type()->basic_type()];\n+          if (bt == T_ARRAY || bt == T_NARROWOOP) {\n+            bt = T_OBJECT;\n+          }\n+          if (bt == type && (!field->is_flat() || field->type() == inline_klass)) {\n+            Node* value = vt->field_value_by_offset(off, false);\n+            if (value->is_InlineType()) {\n+              value = value->as_InlineType()->adjust_scalarization_depth(this);\n+            }\n+            set_result(value);\n+            return true;\n+          }\n+        }\n+      }\n+      {\n+        \/\/ Re-execute the unsafe access if allocation triggers deoptimization.\n+        PreserveReexecuteState preexecs(this);\n+        jvms()->set_should_reexecute(true);\n+        vt = vt->buffer(this);\n+      }\n+      base = vt->get_oop();\n+    }\n+  }\n+\n@@ -2409,1 +2479,1 @@\n-    if (type != T_OBJECT) {\n+    if (type != T_OBJECT && (inline_klass == nullptr || !inline_klass->has_object_fields())) {\n@@ -2427,1 +2497,1 @@\n-  Node* val = is_store ? argument(4) : nullptr;\n+  Node* val = is_store ? argument(4 + (is_flat ? 1 : 0)) : nullptr;\n@@ -2448,1 +2518,29 @@\n-  BasicType bt = alias_type->basic_type();\n+  BasicType bt = T_ILLEGAL;\n+  ciField* field = nullptr;\n+  if (adr_type->isa_instptr()) {\n+    const TypeInstPtr* instptr = adr_type->is_instptr();\n+    ciInstanceKlass* k = instptr->instance_klass();\n+    int off = instptr->offset();\n+    if (instptr->const_oop() != nullptr &&\n+        k == ciEnv::current()->Class_klass() &&\n+        instptr->offset() >= (k->size_helper() * wordSize)) {\n+      k = instptr->const_oop()->as_instance()->java_lang_Class_klass()->as_instance_klass();\n+      field = k->get_field_by_offset(off, true);\n+    } else {\n+      field = k->get_non_flat_field_by_offset(off);\n+    }\n+    if (field != nullptr) {\n+      bt = type2field[field->type()->basic_type()];\n+    }\n+    if (bt != alias_type->basic_type()) {\n+      \/\/ Type mismatch. Is it an access to a nested flat field?\n+      field = k->get_field_by_offset(off, false);\n+      if (field != nullptr) {\n+        bt = type2field[field->type()->basic_type()];\n+      }\n+    }\n+    assert(bt == alias_type->basic_type() || is_flat, \"should match\");\n+  } else {\n+    bt = alias_type->basic_type();\n+  }\n+\n@@ -2471,0 +2569,23 @@\n+  if (is_flat) {\n+    if (adr_type->isa_instptr()) {\n+      if (field == nullptr || field->type() != inline_klass) {\n+        mismatched = true;\n+      }\n+    } else if (adr_type->isa_aryptr()) {\n+      const Type* elem = adr_type->is_aryptr()->elem();\n+      if (!adr_type->is_flat() || elem->inline_klass() != inline_klass) {\n+        mismatched = true;\n+      }\n+    } else {\n+      mismatched = true;\n+    }\n+    if (is_store) {\n+      const Type* val_t = _gvn.type(val);\n+      if (!val_t->is_inlinetypeptr() || val_t->inline_klass() != inline_klass) {\n+        set_map(old_map);\n+        set_sp(old_sp);\n+        return false;\n+      }\n+    }\n+  }\n+\n@@ -2472,1 +2593,1 @@\n-  assert(!mismatched || alias_type->adr_type()->is_oopptr(), \"off-heap access can't be mismatched\");\n+  assert(!mismatched || is_flat || alias_type->adr_type()->is_oopptr(), \"off-heap access can't be mismatched\");\n@@ -2484,4 +2605,6 @@\n-  if (!is_store && type == T_OBJECT) {\n-    const TypeOopPtr* tjp = sharpen_unsafe_type(alias_type, adr_type);\n-    if (tjp != nullptr) {\n-      value_type = tjp;\n+  if (!is_store) {\n+    if (type == T_OBJECT && !is_flat) {\n+      const TypeOopPtr* tjp = sharpen_unsafe_type(alias_type, adr_type);\n+      if (tjp != nullptr) {\n+        value_type = tjp;\n+      }\n@@ -2503,2 +2626,2 @@\n-    ciField* field = alias_type->field();\n-    if (heap_base_oop != top() && field != nullptr && field->is_constant() && !mismatched) {\n+\n+    if (heap_base_oop != top() && field != nullptr && field->is_constant() && !field->is_flat() && !mismatched) {\n@@ -2510,1 +2633,16 @@\n-      p = access_load_at(heap_base_oop, adr, adr_type, value_type, type, decorators);\n+      if (is_flat) {\n+        if (adr_type->isa_instptr() && !mismatched) {\n+          ciInstanceKlass* holder = adr_type->is_instptr()->instance_klass();\n+          int offset = adr_type->is_instptr()->offset();\n+          p = InlineTypeNode::make_from_flat(this, inline_klass, base, base, nullptr, holder, offset, false, -1, decorators);\n+        } else {\n+          p = InlineTypeNode::make_from_flat(this, inline_klass, base, adr, nullptr, nullptr, 0, false, -1, decorators);\n+        }\n+      } else {\n+        p = access_load_at(heap_base_oop, adr, adr_type, value_type, type, decorators);\n+        const TypeOopPtr* ptr = value_type->make_oopptr();\n+        if (ptr != nullptr && ptr->is_inlinetypeptr()) {\n+          \/\/ Load a non-flattened inline type from memory\n+          p = InlineTypeNode::make_from_oop(this, p, ptr->inline_klass());\n+        }\n+      }\n@@ -2548,1 +2686,68 @@\n-    access_store_at(heap_base_oop, adr, adr_type, val, value_type, type, decorators);\n+    if (is_flat) {\n+      if (adr_type->isa_instptr() && !mismatched) {\n+        ciInstanceKlass* holder = adr_type->is_instptr()->instance_klass();\n+        int offset = adr_type->is_instptr()->offset();\n+        val->as_InlineType()->store_flat(this, base, base, nullptr, holder, offset, false, -1, decorators);\n+      } else {\n+        val->as_InlineType()->store_flat(this, base, adr, nullptr, nullptr, 0, false, -1, decorators);\n+      }\n+    } else {\n+      access_store_at(heap_base_oop, adr, adr_type, val, value_type, type, decorators);\n+    }\n+  }\n+\n+  if (argument(1)->is_InlineType() && is_store) {\n+    InlineTypeNode* value = InlineTypeNode::make_from_oop(this, base, _gvn.type(argument(1))->inline_klass());\n+    value = value->make_larval(this, false);\n+    replace_in_map(argument(1), value);\n+  }\n+\n+  return true;\n+}\n+\n+bool LibraryCallKit::inline_unsafe_make_private_buffer() {\n+  Node* receiver = argument(0);\n+  Node* value = argument(1);\n+\n+  const Type* type = gvn().type(value);\n+  if (!type->is_inlinetypeptr()) {\n+    C->record_method_not_compilable(\"value passed to Unsafe::makePrivateBuffer is not of a constant value type\");\n+    return false;\n+  }\n+\n+  null_check(receiver);\n+  if (stopped()) {\n+    return true;\n+  }\n+\n+  value = null_check(value);\n+  if (stopped()) {\n+    return true;\n+  }\n+\n+  ciInlineKlass* vk = type->inline_klass();\n+  Node* klass = makecon(TypeKlassPtr::make(vk));\n+  Node* obj = new_instance(klass);\n+  AllocateNode::Ideal_allocation(obj)->_larval = true;\n+\n+  assert(value->is_InlineType(), \"must be an InlineTypeNode\");\n+  value->as_InlineType()->store(this, obj, obj, vk);\n+\n+  set_result(obj);\n+  return true;\n+}\n+\n+bool LibraryCallKit::inline_unsafe_finish_private_buffer() {\n+  Node* receiver = argument(0);\n+  Node* buffer = argument(1);\n+\n+  const Type* type = gvn().type(buffer);\n+  if (!type->is_inlinetypeptr()) {\n+    C->record_method_not_compilable(\"value passed to Unsafe::finishPrivateBuffer is not of a constant value type\");\n+    return false;\n+  }\n+\n+  AllocateNode* alloc = AllocateNode::Ideal_allocation(buffer);\n+  if (alloc == nullptr) {\n+    C->record_method_not_compilable(\"value passed to Unsafe::finishPrivateBuffer must be allocated by Unsafe::makePrivateBuffer\");\n+    return false;\n@@ -2551,0 +2756,14 @@\n+  null_check(receiver);\n+  if (stopped()) {\n+    return true;\n+  }\n+\n+  \/\/ Unset the larval bit in the object header\n+  Node* old_header = make_load(control(), buffer, TypeX_X, TypeX_X->basic_type(), MemNode::unordered, LoadNode::Pinned);\n+  Node* new_header = gvn().transform(new AndXNode(old_header, MakeConX(~markWord::larval_bit_in_place)));\n+  access_store_at(buffer, buffer, type->is_ptr(), new_header, TypeX_X, TypeX_X->basic_type(), MO_UNORDERED | IN_HEAP);\n+\n+  \/\/ We must ensure that the buffer is properly published\n+  insert_mem_bar(Op_MemBarStoreStore, alloc->proj_out(AllocateNode::RawAddress));\n+  assert(!type->maybe_null(), \"result of an allocation should not be null\");\n+  set_result(InlineTypeNode::make_from_oop(this, buffer, type->inline_klass(), false));\n@@ -2759,0 +2978,13 @@\n+    if (oldval != nullptr && oldval->is_InlineType()) {\n+      \/\/ Re-execute the unsafe access if allocation triggers deoptimization.\n+      PreserveReexecuteState preexecs(this);\n+      jvms()->set_should_reexecute(true);\n+      oldval = oldval->as_InlineType()->buffer(this)->get_oop();\n+    }\n+    if (newval != nullptr && newval->is_InlineType()) {\n+      \/\/ Re-execute the unsafe access if allocation triggers deoptimization.\n+      PreserveReexecuteState preexecs(this);\n+      jvms()->set_should_reexecute(true);\n+      newval = newval->as_InlineType()->buffer(this)->get_oop();\n+    }\n+\n@@ -2945,2 +3177,7 @@\n-\n-  Node* obj = new_instance(kls, test);\n+  Node* obj = nullptr;\n+  const TypeInstKlassPtr* tkls = _gvn.type(kls)->isa_instklassptr();\n+  if (tkls != nullptr && tkls->instance_klass()->is_inlinetype()) {\n+    obj = InlineTypeNode::make_all_zero(_gvn, tkls->instance_klass()->as_inline_klass())->buffer(this);\n+  } else {\n+    obj = new_instance(kls, test);\n+  }\n@@ -3727,1 +3964,1 @@\n-  const TypeAry* arr0 = TypeAry::make(etype, TypeInt::POS);\n+  const TypeAry* arr0 = TypeAry::make(etype, TypeInt::POS, \/* stable= *\/ false, \/* flat= *\/ false, \/* not_flat= *\/ true, \/* not_null_free= *\/ true);\n@@ -3732,1 +3969,1 @@\n-  const Type* objects_type = TypeAryPtr::make(TypePtr::BotPTR, arr0, objects_klass, xk, 0);\n+  const Type* objects_type = TypeAryPtr::make(TypePtr::BotPTR, arr0, objects_klass, xk, TypeAryPtr::Offset(0));\n@@ -3856,9 +4093,0 @@\n-\/\/---------------------------load_mirror_from_klass----------------------------\n-\/\/ Given a klass oop, load its java mirror (a java.lang.Class oop).\n-Node* LibraryCallKit::load_mirror_from_klass(Node* klass) {\n-  Node* p = basic_plus_adr(klass, in_bytes(Klass::java_mirror_offset()));\n-  Node* load = make_load(nullptr, p, TypeRawPtr::NOTNULL, T_ADDRESS, MemNode::unordered);\n-  \/\/ mirror = ((OopHandle)mirror)->resolve();\n-  return access_load(load, TypeInstPtr::MIRROR, T_OBJECT, IN_NATIVE);\n-}\n-\n@@ -3908,0 +4136,1 @@\n+\n@@ -4066,0 +4295,1 @@\n+\n@@ -4081,1 +4311,2 @@\n-  ciType* tm = mirror_con->java_mirror_type();\n+  bool is_null_free_array = false;\n+  ciType* tm = mirror_con->java_mirror_type(&is_null_free_array);\n@@ -4088,1 +4319,5 @@\n-      int static_res = C->static_subtype_check(TypeKlassPtr::make(tm->as_klass(), Type::trust_interfaces), tp->as_klass_type());\n+      const TypeKlassPtr* tklass = TypeKlassPtr::make(tm->as_klass(), Type::trust_interfaces);\n+      if (is_null_free_array) {\n+        tklass = tklass->is_aryklassptr()->cast_to_null_free();\n+      }\n+      int static_res = C->static_subtype_check(tklass, tp->as_klass_type());\n@@ -4117,2 +4352,2 @@\n-  \/\/ Not-subtype or the mirror's klass ptr is null (in case it is a primitive).\n-  enum { _bad_type_path = 1, _prim_path = 2, PATH_LIMIT };\n+  \/\/ Not-subtype or the mirror's klass ptr is nullptr (in case it is a primitive).\n+  enum { _bad_type_path = 1, _prim_path = 2, _npe_path = 3, PATH_LIMIT };\n@@ -4128,0 +4363,2 @@\n+  Node* io = i_o();\n+  Node* mem = merged_memory();\n@@ -4129,0 +4366,1 @@\n+\n@@ -4135,1 +4373,2 @@\n-      region->in(_bad_type_path) != top()) {\n+      region->in(_bad_type_path) != top() ||\n+      region->in(_npe_path) != top()) {\n@@ -4139,0 +4378,3 @@\n+    \/\/ Set IO and memory because gen_checkcast may override them when buffering inline types\n+    set_i_o(io);\n+    set_all_memory(mem);\n@@ -4172,0 +4414,1 @@\n+  RegionNode* prim_region = new RegionNode(2);\n@@ -4174,0 +4417,1 @@\n+  record_for_igvn(prim_region);\n@@ -4198,2 +4442,5 @@\n-    int prim_path = (which_arg == 0 ? _prim_0_path : _prim_1_path);\n-    region->init_req(prim_path, null_ctl);\n+    if (which_arg == 0) {\n+      prim_region->init_req(1, null_ctl);\n+    } else {\n+      region->init_req(_prim_1_path, null_ctl);\n+    }\n@@ -4209,1 +4456,0 @@\n-    \/\/ now we have a successful reference subtype check\n@@ -4216,1 +4462,2 @@\n-  set_control(region->in(_prim_0_path)); \/\/ go back to first null check\n+  \/\/ This path is also used if superc is a value mirror.\n+  set_control(_gvn.transform(prim_region));\n@@ -4221,1 +4468,1 @@\n-    generate_guard(bol_eq, region, PROB_FAIR);\n+    generate_fair_guard(bol_eq, region);\n@@ -4252,2 +4499,1 @@\n-Node* LibraryCallKit::generate_array_guard_common(Node* kls, RegionNode* region,\n-                                                  bool obj_array, bool not_array, Node** obj) {\n+Node* LibraryCallKit::generate_array_guard_common(Node* kls, RegionNode* region, ArrayKind kind, Node** obj) {\n@@ -4259,9 +4505,0 @@\n-  \/\/ If obj_array\/non_array==false\/false:\n-  \/\/ Branch around if the given klass is in fact an array (either obj or prim).\n-  \/\/ If obj_array\/non_array==false\/true:\n-  \/\/ Branch around if the given klass is not an array klass of any kind.\n-  \/\/ If obj_array\/non_array==true\/true:\n-  \/\/ Branch around if the kls is not an oop array (kls is int[], String, etc.)\n-  \/\/ If obj_array\/non_array==true\/false:\n-  \/\/ Branch around if the kls is an oop array (Object[] or subtype)\n-  \/\/\n@@ -4272,4 +4509,11 @@\n-    bool query = (obj_array\n-                  ? Klass::layout_helper_is_objArray(layout_con)\n-                  : Klass::layout_helper_is_array(layout_con));\n-    if (query == not_array) {\n+    bool query = 0;\n+    switch(kind) {\n+      case ObjectArray:    query = Klass::layout_helper_is_objArray(layout_con); break;\n+      case NonObjectArray: query = !Klass::layout_helper_is_objArray(layout_con); break;\n+      case TypeArray:      query = Klass::layout_helper_is_typeArray(layout_con); break;\n+      case AnyArray:       query = Klass::layout_helper_is_array(layout_con); break;\n+      case NonArray:       query = !Klass::layout_helper_is_array(layout_con); break;\n+      default:\n+        ShouldNotReachHere();\n+    }\n+    if (!query) {\n@@ -4285,0 +4529,21 @@\n+  unsigned int value = 0;\n+  BoolTest::mask btest = BoolTest::illegal;\n+  switch(kind) {\n+    case ObjectArray:\n+    case NonObjectArray: {\n+      value = Klass::_lh_array_tag_obj_value;\n+      layout_val = _gvn.transform(new RShiftINode(layout_val, intcon(Klass::_lh_array_tag_shift)));\n+      btest = (kind == ObjectArray) ? BoolTest::eq : BoolTest::ne;\n+      break;\n+    }\n+    case TypeArray: {\n+      value = Klass::_lh_array_tag_type_value;\n+      layout_val = _gvn.transform(new RShiftINode(layout_val, intcon(Klass::_lh_array_tag_shift)));\n+      btest = BoolTest::eq;\n+      break;\n+    }\n+    case AnyArray:    value = Klass::_lh_neutral_value; btest = BoolTest::lt; break;\n+    case NonArray:    value = Klass::_lh_neutral_value; btest = BoolTest::gt; break;\n+    default:\n+      ShouldNotReachHere();\n+  }\n@@ -4286,4 +4551,1 @@\n-  jint  nval = (obj_array\n-                ? (jint)(Klass::_lh_array_tag_type_value\n-                   <<    Klass::_lh_array_tag_shift)\n-                : Klass::_lh_neutral_value);\n+  jint nval = (jint)value;\n@@ -4291,3 +4553,0 @@\n-  BoolTest::mask btest = BoolTest::lt;  \/\/ correct for testing is_[obj]array\n-  \/\/ invert the test if we are looking for a non-array\n-  if (not_array)  btest = BoolTest(btest).negate();\n@@ -4296,1 +4555,1 @@\n-  Node* is_array_ctrl = not_array ? control() : ctrl;\n+  Node* is_array_ctrl = kind == NonArray ? control() : ctrl;\n@@ -4310,0 +4569,59 @@\n+\/\/ public static native Object[] newNullRestrictedAtomicArray(Class<?> componentType, int length, Object initVal);\n+\/\/ public static native Object[] newNullRestrictedNonAtomicArray(Class<?> componentType, int length, Object initVal);\n+\/\/ public static native Object[] newNullableAtomicArray(Class<?> componentType, int length);\n+bool LibraryCallKit::inline_newArray(bool null_free, bool atomic) {\n+  assert(null_free || atomic, \"nullable implies atomic\");\n+  Node* componentType = argument(0);\n+  Node* length = argument(1);\n+  Node* init_val = null_free ? argument(2) : nullptr;\n+\n+  const TypeInstPtr* tp = _gvn.type(componentType)->isa_instptr();\n+  if (tp != nullptr) {\n+    ciInstanceKlass* ik = tp->instance_klass();\n+    if (ik == C->env()->Class_klass()) {\n+      ciType* t = tp->java_mirror_type();\n+      if (t != nullptr && t->is_inlinetype()) {\n+        ciInlineKlass* vk = t->as_inline_klass();\n+        bool flat = vk->flat_in_array();\n+        if (flat && atomic) {\n+          \/\/ Only flat if we have a corresponding atomic layout\n+          flat = null_free ? vk->has_atomic_layout() : vk->has_nullable_atomic_layout();\n+        }\n+        \/\/ TODO 8350865 refactor\n+        if (flat && !atomic) {\n+          flat = vk->has_non_atomic_layout();\n+        }\n+\n+        \/\/ TOOD 8350865 ZGC needs card marks on initializing oop stores\n+        if (UseZGC && null_free && !flat) {\n+          return false;\n+        }\n+\n+        ciArrayKlass* array_klass = ciArrayKlass::make(t, flat, null_free, atomic);\n+        if (array_klass->is_loaded() && array_klass->element_klass()->as_inline_klass()->is_initialized()) {\n+          const TypeAryKlassPtr* array_klass_type = TypeKlassPtr::make(array_klass, Type::trust_interfaces)->is_aryklassptr();\n+          if (null_free) {\n+            if (init_val->is_InlineType()) {\n+              if (array_klass_type->is_flat() && init_val->as_InlineType()->is_all_zero(&gvn(), \/* flat *\/ true)) {\n+                \/\/ Zeroing is enough because the init value is the all-zero value\n+                init_val = nullptr;\n+              } else {\n+                init_val = init_val->as_InlineType()->buffer(this);\n+              }\n+            }\n+            \/\/ TODO 8350865 Should we add a check of the init_val type (maybe in debug only + halt)?\n+          }\n+          Node* obj = new_array(makecon(array_klass_type), length, 0, nullptr, false, init_val);\n+          const TypeAryPtr* arytype = gvn().type(obj)->is_aryptr();\n+          assert(arytype->is_null_free() == null_free, \"inconsistency\");\n+          assert(arytype->is_not_null_free() == !null_free, \"inconsistency\");\n+          assert(arytype->is_flat() == flat, \"inconsistency\");\n+          assert(arytype->is_aryptr()->is_not_flat() == !flat, \"inconsistency\");\n+          set_result(obj);\n+          return true;\n+        }\n+      }\n+    }\n+  }\n+  return false;\n+}\n@@ -4312,1 +4630,1 @@\n-\/\/ private static native Object java.lang.reflect.newArray(Class<?> componentType, int length);\n+\/\/ private static native Object java.lang.reflect.Array.newArray(Class<?> componentType, int length);\n@@ -4458,1 +4776,13 @@\n-    Node* not_objArray = generate_non_objArray_guard(klass_node, bailout);\n+    \/\/ Inline type array may have object field that would require a\n+    \/\/ write barrier. Conservatively, go to slow path.\n+    \/\/ TODO 8251971: Optimize for the case when flat src\/dst are later found\n+    \/\/ to not contain oops (i.e., move this check to the macro expansion phase).\n+    BarrierSetC2* bs = BarrierSet::barrier_set()->barrier_set_c2();\n+    const TypeAryPtr* orig_t = _gvn.type(original)->isa_aryptr();\n+    const TypeKlassPtr* tklass = _gvn.type(klass_node)->is_klassptr();\n+    bool exclude_flat = UseArrayFlattening && bs->array_copy_requires_gc_barriers(true, T_OBJECT, false, false, BarrierSetC2::Parsing) &&\n+                        \/\/ Can src array be flat and contain oops?\n+                        (orig_t == nullptr || (!orig_t->is_not_flat() && (!orig_t->is_flat() || orig_t->elem()->inline_klass()->contains_oops()))) &&\n+                        \/\/ Can dest array be flat and contain oops?\n+                        tklass->can_be_inline_array() && (!tklass->is_flat() || tklass->is_aryklassptr()->elem()->is_instklassptr()->instance_klass()->as_inline_klass()->contains_oops());\n+    Node* not_objArray = exclude_flat ? generate_non_objArray_guard(klass_node, bailout) : generate_typeArray_guard(klass_node, bailout);\n@@ -4462,1 +4792,1 @@\n-      const Type* akls = TypeKlassPtr::make(TypePtr::NotNull, ak, 0\/*offset*\/);\n+      const Type* akls = TypeKlassPtr::make(TypePtr::NotNull, ak, Type::Offset(0));\n@@ -4482,0 +4812,39 @@\n+    \/\/ Handle inline type arrays\n+    bool can_validate = !too_many_traps(Deoptimization::Reason_class_check);\n+    if (!stopped()) {\n+      \/\/ TODO JDK-8329224\n+      if (!orig_t->is_null_free()) {\n+        \/\/ Not statically known to be null free, add a check\n+        generate_fair_guard(null_free_array_test(original), bailout);\n+      }\n+      orig_t = _gvn.type(original)->isa_aryptr();\n+      if (orig_t != nullptr && orig_t->is_flat()) {\n+        \/\/ Src is flat, check that dest is flat as well\n+        if (exclude_flat) {\n+          \/\/ Dest can't be flat, bail out\n+          bailout->add_req(control());\n+          set_control(top());\n+        } else {\n+          generate_fair_guard(flat_array_test(klass_node, \/* flat = *\/ false), bailout);\n+        }\n+        \/\/ TODO 8350865 This is not correct anymore. Write tests and fix logic similar to arraycopy.\n+      } else if (UseArrayFlattening && (orig_t == nullptr || !orig_t->is_not_flat()) &&\n+                 \/\/ If dest is flat, src must be flat as well (guaranteed by src <: dest check if validated).\n+                 ((!tklass->is_flat() && tklass->can_be_inline_array()) || !can_validate)) {\n+        \/\/ Src might be flat and dest might not be flat. Go to the slow path if src is flat.\n+        \/\/ TODO 8251971: Optimize for the case when src\/dest are later found to be both flat.\n+        generate_fair_guard(flat_array_test(load_object_klass(original)), bailout);\n+        if (orig_t != nullptr) {\n+          orig_t = orig_t->cast_to_not_flat();\n+          original = _gvn.transform(new CheckCastPPNode(control(), original, orig_t));\n+        }\n+      }\n+      if (!can_validate) {\n+        \/\/ No validation. The subtype check emitted at macro expansion time will not go to the slow\n+        \/\/ path but call checkcast_arraycopy which can not handle flat\/null-free inline type arrays.\n+        \/\/ TODO 8251971: Optimize for the case when src\/dest are later found to be both flat\/null-free.\n+        generate_fair_guard(flat_array_test(klass_node), bailout);\n+        generate_fair_guard(null_free_array_test(original), bailout);\n+      }\n+    }\n+\n@@ -4527,1 +4896,1 @@\n-      if (!too_many_traps(Deoptimization::Reason_class_check)) {\n+      if (can_validate) {\n@@ -4613,1 +4982,1 @@\n-    const TypeTuple* range = tf->range();\n+    const TypeTuple* range = tf->range_cc();\n@@ -4617,1 +4986,1 @@\n-    tf = TypeFunc::make(tf->domain(), new_range);\n+    tf = TypeFunc::make(tf->domain_cc(), new_range);\n@@ -4674,1 +5043,8 @@\n-  Node* obj = nullptr;\n+  Node* obj = argument(0);\n+\n+  \/\/ Don't intrinsify hashcode on inline types for now.\n+  \/\/ The \"is locked\" runtime check below also serves as inline type check and goes to the slow path.\n+  if (gvn().type(obj)->is_inlinetypeptr()) {\n+    return false;\n+  }\n+\n@@ -4684,1 +5060,0 @@\n-    obj = argument(0);\n@@ -4725,1 +5100,2 @@\n-    Node *lock_mask      = _gvn.MakeConX(markWord::lock_mask_in_place);\n+  \/\/ This also serves as guard against inline types\n+    Node *lock_mask      = _gvn.MakeConX(markWord::inline_type_mask_in_place);\n@@ -4800,1 +5176,10 @@\n-  Node* obj = null_check_receiver();\n+  Node* obj = argument(0);\n+  if (obj->is_InlineType()) {\n+    const Type* t = _gvn.type(obj);\n+    if (t->maybe_null()) {\n+      null_check(obj);\n+    }\n+    set_result(makecon(TypeInstPtr::make(t->inline_klass()->java_mirror())));\n+    return true;\n+  }\n+  obj = null_check_receiver();\n@@ -5152,0 +5537,14 @@\n+\/\/----------------------inline_unsafe_isFlatArray------------------------\n+\/\/ public native boolean Unsafe.isFlatArray(Class<?> arrayClass);\n+\/\/ This intrinsic exploits assumptions made by the native implementation\n+\/\/ (arrayClass is neither null nor primitive) to avoid unnecessary null checks.\n+bool LibraryCallKit::inline_unsafe_isFlatArray() {\n+  Node* cls = argument(1);\n+  Node* p = basic_plus_adr(cls, java_lang_Class::klass_offset());\n+  Node* kls = _gvn.transform(LoadKlassNode::make(_gvn, immutable_memory(), p,\n+                                                 TypeRawPtr::BOTTOM, TypeInstKlassPtr::OBJECT));\n+  Node* result = flat_array_test(kls);\n+  set_result(result);\n+  return true;\n+}\n+\n@@ -5222,1 +5621,2 @@\n-    Node* obj = null_check_receiver();\n+    Node* obj = argument(0);\n+    obj = null_check_receiver();\n@@ -5226,0 +5626,6 @@\n+    if (obj_type->is_inlinetypeptr()) {\n+      \/\/ If the object to clone is an inline type, we can simply return it (i.e. a nop) since inline types have\n+      \/\/ no identity.\n+      set_result(obj);\n+      return true;\n+    }\n@@ -5232,1 +5638,2 @@\n-        obj_type->speculative_type()->is_instance_klass()) {\n+        obj_type->speculative_type()->is_instance_klass() &&\n+        !obj_type->speculative_type()->is_inlinetype()) {\n@@ -5262,0 +5669,5 @@\n+    \/\/ We only go to the fast case code if we pass a number of guards.\n+    \/\/ The paths which do not pass are accumulated in the slow_region.\n+    RegionNode* slow_region = new RegionNode(1);\n+    record_for_igvn(slow_region);\n+\n@@ -5268,3 +5680,0 @@\n-      Node* obj_length = load_array_length(array_obj);\n-      Node* array_size = nullptr; \/\/ Size of the array without object alignment padding.\n-      Node* alloc_obj = new_array(obj_klass, obj_length, 0, &array_size, \/*deoptimize_on_exception=*\/true);\n@@ -5273,20 +5682,7 @@\n-      if (bs->array_copy_requires_gc_barriers(true, T_OBJECT, true, false, BarrierSetC2::Parsing)) {\n-        \/\/ If it is an oop array, it requires very special treatment,\n-        \/\/ because gc barriers are required when accessing the array.\n-        Node* is_obja = generate_objArray_guard(obj_klass, (RegionNode*)nullptr);\n-        if (is_obja != nullptr) {\n-          PreserveJVMState pjvms2(this);\n-          set_control(is_obja);\n-          \/\/ Generate a direct call to the right arraycopy function(s).\n-          \/\/ Clones are always tightly coupled.\n-          ArrayCopyNode* ac = ArrayCopyNode::make(this, true, array_obj, intcon(0), alloc_obj, intcon(0), obj_length, true, false);\n-          ac->set_clone_oop_array();\n-          Node* n = _gvn.transform(ac);\n-          assert(n == ac, \"cannot disappear\");\n-          ac->connect_outputs(this, \/*deoptimize_on_exception=*\/true);\n-\n-          result_reg->init_req(_objArray_path, control());\n-          result_val->init_req(_objArray_path, alloc_obj);\n-          result_i_o ->set_req(_objArray_path, i_o());\n-          result_mem ->set_req(_objArray_path, reset_memory());\n-        }\n+      const TypeAryPtr* ary_ptr = obj_type->isa_aryptr();\n+      if (UseArrayFlattening && bs->array_copy_requires_gc_barriers(true, T_OBJECT, true, false, BarrierSetC2::Expansion) &&\n+          obj_type->can_be_inline_array() &&\n+          (ary_ptr == nullptr || (!ary_ptr->is_not_flat() && (!ary_ptr->is_flat() || ary_ptr->elem()->inline_klass()->contains_oops())))) {\n+        \/\/ Flat inline type array may have object field that would require a\n+        \/\/ write barrier. Conservatively, go to slow path.\n+        generate_fair_guard(flat_array_test(obj_klass), slow_region);\n@@ -5294,7 +5690,0 @@\n-      \/\/ Otherwise, there are no barriers to worry about.\n-      \/\/ (We can dispense with card marks if we know the allocation\n-      \/\/  comes out of eden (TLAB)...  In fact, ReduceInitialCardMarks\n-      \/\/  causes the non-eden paths to take compensating steps to\n-      \/\/  simulate a fresh allocation, so that no further\n-      \/\/  card marks are required in compiled code to initialize\n-      \/\/  the object.)\n@@ -5303,7 +5692,43 @@\n-        copy_to_clone(array_obj, alloc_obj, array_size, true);\n-\n-        \/\/ Present the results of the copy.\n-        result_reg->init_req(_array_path, control());\n-        result_val->init_req(_array_path, alloc_obj);\n-        result_i_o ->set_req(_array_path, i_o());\n-        result_mem ->set_req(_array_path, reset_memory());\n+        Node* obj_length = load_array_length(array_obj);\n+        Node* array_size = nullptr; \/\/ Size of the array without object alignment padding.\n+        Node* alloc_obj = new_array(obj_klass, obj_length, 0, &array_size, \/*deoptimize_on_exception=*\/true);\n+\n+        BarrierSetC2* bs = BarrierSet::barrier_set()->barrier_set_c2();\n+        if (bs->array_copy_requires_gc_barriers(true, T_OBJECT, true, false, BarrierSetC2::Parsing)) {\n+          \/\/ If it is an oop array, it requires very special treatment,\n+          \/\/ because gc barriers are required when accessing the array.\n+          Node* is_obja = generate_objArray_guard(obj_klass, (RegionNode*)nullptr);\n+          if (is_obja != nullptr) {\n+            PreserveJVMState pjvms2(this);\n+            set_control(is_obja);\n+            \/\/ Generate a direct call to the right arraycopy function(s).\n+            \/\/ Clones are always tightly coupled.\n+            ArrayCopyNode* ac = ArrayCopyNode::make(this, true, array_obj, intcon(0), alloc_obj, intcon(0), obj_length, true, false);\n+            ac->set_clone_oop_array();\n+            Node* n = _gvn.transform(ac);\n+            assert(n == ac, \"cannot disappear\");\n+            ac->connect_outputs(this, \/*deoptimize_on_exception=*\/true);\n+\n+            result_reg->init_req(_objArray_path, control());\n+            result_val->init_req(_objArray_path, alloc_obj);\n+            result_i_o ->set_req(_objArray_path, i_o());\n+            result_mem ->set_req(_objArray_path, reset_memory());\n+          }\n+        }\n+        \/\/ Otherwise, there are no barriers to worry about.\n+        \/\/ (We can dispense with card marks if we know the allocation\n+        \/\/  comes out of eden (TLAB)...  In fact, ReduceInitialCardMarks\n+        \/\/  causes the non-eden paths to take compensating steps to\n+        \/\/  simulate a fresh allocation, so that no further\n+        \/\/  card marks are required in compiled code to initialize\n+        \/\/  the object.)\n+\n+        if (!stopped()) {\n+          copy_to_clone(obj, alloc_obj, array_size, true);\n+\n+          \/\/ Present the results of the copy.\n+          result_reg->init_req(_array_path, control());\n+          result_val->init_req(_array_path, alloc_obj);\n+          result_i_o ->set_req(_array_path, i_o());\n+          result_mem ->set_req(_array_path, reset_memory());\n+        }\n@@ -5313,4 +5738,0 @@\n-    \/\/ We only go to the instance fast case code if we pass a number of guards.\n-    \/\/ The paths which do not pass are accumulated in the slow_region.\n-    RegionNode* slow_region = new RegionNode(1);\n-    record_for_igvn(slow_region);\n@@ -5448,0 +5869,12 @@\n+  int adjustment = 1;\n+  const TypeAryKlassPtr* ary_klass_ptr = alloc->in(AllocateNode::KlassNode)->bottom_type()->is_aryklassptr();\n+  if (ary_klass_ptr->is_null_free()) {\n+    \/\/ A null-free, tightly coupled array allocation can only come from LibraryCallKit::inline_newArray which\n+    \/\/ also requires the componentType and initVal on stack for re-execution.\n+    \/\/ Re-create and push the componentType.\n+    ciArrayKlass* klass = ary_klass_ptr->exact_klass()->as_array_klass();\n+    ciInstance* instance = klass->component_mirror_instance();\n+    const TypeInstPtr* t_instance = TypeInstPtr::make(instance);\n+    sfpt->ins_req(old_jvms->stkoff() + old_jvms->sp(), makecon(t_instance));\n+    adjustment++;\n+  }\n@@ -5449,5 +5882,16 @@\n-  sfpt->ins_req(old_jvms->stkoff() + old_jvms->sp(), alloc->in(AllocateNode::ALength));\n-  old_jvms->set_sp(old_jvms->sp()+1);\n-  old_jvms->set_monoff(old_jvms->monoff()+1);\n-  old_jvms->set_scloff(old_jvms->scloff()+1);\n-  old_jvms->set_endoff(old_jvms->endoff()+1);\n+  sfpt->ins_req(old_jvms->stkoff() + old_jvms->sp() + adjustment - 1, alloc->in(AllocateNode::ALength));\n+  if (ary_klass_ptr->is_null_free()) {\n+    \/\/ Re-create and push the initVal.\n+    Node* init_val = alloc->in(AllocateNode::InitValue);\n+    if (init_val == nullptr) {\n+      init_val = InlineTypeNode::make_all_zero(_gvn, ary_klass_ptr->elem()->is_instklassptr()->instance_klass()->as_inline_klass());\n+    } else if (UseCompressedOops) {\n+      init_val = _gvn.transform(new DecodeNNode(init_val, init_val->bottom_type()->make_ptr()));\n+    }\n+    sfpt->ins_req(old_jvms->stkoff() + old_jvms->sp() + adjustment, init_val);\n+    adjustment++;\n+  }\n+  old_jvms->set_sp(old_jvms->sp() + adjustment);\n+  old_jvms->set_monoff(old_jvms->monoff() + adjustment);\n+  old_jvms->set_scloff(old_jvms->scloff() + adjustment);\n+  old_jvms->set_endoff(old_jvms->endoff() + adjustment);\n@@ -5486,2 +5930,1 @@\n-    CallProjections callprojs;\n-    alloc->extract_projections(&callprojs, true);\n+    CallProjections* callprojs = alloc->extract_projections(true);\n@@ -5490,1 +5933,1 @@\n-    C->gvn_replace_by(callprojs.fallthrough_ioproj, alloc->in(TypeFunc::I_O));\n+    C->gvn_replace_by(callprojs->fallthrough_ioproj, alloc->in(TypeFunc::I_O));\n@@ -5532,1 +5975,1 @@\n-    set_i_o(callprojs.fallthrough_ioproj);\n+    set_i_o(callprojs->fallthrough_ioproj);\n@@ -5870,1 +6313,1 @@\n-    if (src_elem == dest_elem && src_elem == T_OBJECT) {\n+    if (src_elem == dest_elem && top_src->is_flat() == top_dest->is_flat() && src_elem == T_OBJECT) {\n@@ -5897,0 +6340,2 @@\n+          src_type = _gvn.type(src);\n+          top_src = src_type->isa_aryptr();\n@@ -5900,0 +6345,2 @@\n+          dest_type = _gvn.type(dest);\n+          top_dest = dest_type->isa_aryptr();\n@@ -5915,2 +6362,1 @@\n-      can_emit_guards &&\n-      !src->is_top() && !dest->is_top()) {\n+      can_emit_guards && !src->is_top() && !dest->is_top()) {\n@@ -5959,0 +6405,6 @@\n+      slow_region->add_req(not_subtype_ctrl);\n+    }\n+\n+    \/\/ TODO 8350865 Fix below logic. Also handle atomicity.\n+    generate_fair_guard(flat_array_test(src), slow_region);\n+    generate_fair_guard(flat_array_test(dest), slow_region);\n@@ -5960,6 +6412,28 @@\n-      if (not_subtype_ctrl != top()) {\n-        PreserveJVMState pjvms(this);\n-        set_control(not_subtype_ctrl);\n-        uncommon_trap(Deoptimization::Reason_intrinsic,\n-                      Deoptimization::Action_make_not_entrant);\n-        assert(stopped(), \"Should be stopped\");\n+    const TypeKlassPtr* dest_klass_t = _gvn.type(dest_klass)->is_klassptr();\n+    const Type* toop = dest_klass_t->cast_to_exactness(false)->as_instance_type();\n+    src = _gvn.transform(new CheckCastPPNode(control(), src, toop));\n+    src_type = _gvn.type(src);\n+    top_src  = src_type->isa_aryptr();\n+\n+    \/\/ Handle flat inline type arrays (null-free arrays are handled by the subtype check above)\n+    if (!stopped() && UseArrayFlattening) {\n+      \/\/ If dest is flat, src must be flat as well (guaranteed by src <: dest check). Handle flat src here.\n+      assert(top_dest == nullptr || !top_dest->is_flat() || top_src->is_flat(), \"src array must be flat\");\n+      if (top_src != nullptr && top_src->is_flat()) {\n+        \/\/ Src is flat, check that dest is flat as well\n+        if (top_dest != nullptr && !top_dest->is_flat()) {\n+          generate_fair_guard(flat_array_test(dest_klass, \/* flat = *\/ false), slow_region);\n+          \/\/ Since dest is flat and src <: dest, dest must have the same type as src.\n+          top_dest = top_src->cast_to_exactness(false);\n+          assert(top_dest->is_flat(), \"dest must be flat\");\n+          dest = _gvn.transform(new CheckCastPPNode(control(), dest, top_dest));\n+        }\n+      } else if (top_src == nullptr || !top_src->is_not_flat()) {\n+        \/\/ Src might be flat and dest might not be flat. Go to the slow path if src is flat.\n+        \/\/ TODO 8251971: Optimize for the case when src\/dest are later found to be both flat.\n+        assert(top_dest == nullptr || !top_dest->is_flat(), \"dest array must not be flat\");\n+        generate_fair_guard(flat_array_test(src), slow_region);\n+        if (top_src != nullptr) {\n+          top_src = top_src->cast_to_not_flat();\n+          src = _gvn.transform(new CheckCastPPNode(control(), src, top_src));\n+        }\n@@ -5968,0 +6442,1 @@\n+\n@@ -5975,4 +6450,0 @@\n-\n-    const TypeKlassPtr* dest_klass_t = _gvn.type(dest_klass)->is_klassptr();\n-    const Type *toop = dest_klass_t->cast_to_exactness(false)->as_instance_type();\n-    src = _gvn.transform(new CheckCastPPNode(control(), src, toop));\n","filename":"src\/hotspot\/share\/opto\/library_call.cpp","additions":607,"deletions":136,"binary":false,"changes":743,"status":"modified"},{"patch":"@@ -31,0 +31,1 @@\n+#include \"opto\/inlinetypenode.hpp\"\n@@ -108,2 +109,3 @@\n-    if (!stopped() && result() != nullptr) {\n-      if (result()->is_top()) {\n+    Node* res = result();\n+    if (!stopped() && res != nullptr) {\n+      if (res->is_top()) {\n@@ -113,2 +115,9 @@\n-      BasicType bt = result()->bottom_type()->basic_type();\n-      push_node(bt, result());\n+      BasicType bt = res->bottom_type()->basic_type();\n+      if (C->inlining_incrementally() && res->is_InlineType()) {\n+        \/\/ The caller expects an oop when incrementally inlining an intrinsic that returns an\n+        \/\/ inline type. Make sure the call is re-executed if the allocation triggers a deoptimization.\n+        PreserveReexecuteState preexecs(this);\n+        jvms()->set_should_reexecute(true);\n+        res = res->as_InlineType()->buffer(this);\n+      }\n+      push_node(bt, res);\n@@ -145,1 +154,0 @@\n-  Node* load_mirror_from_klass(Node* klass);\n@@ -169,0 +177,9 @@\n+\n+  enum ArrayKind {\n+    AnyArray,\n+    NonArray,\n+    ObjectArray,\n+    NonObjectArray,\n+    TypeArray\n+  };\n+\n@@ -170,0 +187,1 @@\n+\n@@ -171,1 +189,1 @@\n-    return generate_array_guard_common(kls, region, false, false, obj);\n+    return generate_array_guard_common(kls, region, AnyArray, obj);\n@@ -174,1 +192,1 @@\n-    return generate_array_guard_common(kls, region, false, true, obj);\n+    return generate_array_guard_common(kls, region, NonArray, obj);\n@@ -177,1 +195,1 @@\n-    return generate_array_guard_common(kls, region, true, false, obj);\n+    return generate_array_guard_common(kls, region, ObjectArray, obj);\n@@ -180,1 +198,4 @@\n-    return generate_array_guard_common(kls, region, true, true, obj);\n+    return generate_array_guard_common(kls, region, NonObjectArray, obj);\n+  }\n+  Node* generate_typeArray_guard(Node* kls, RegionNode* region, Node** obj = nullptr) {\n+    return generate_array_guard_common(kls, region, TypeArray, obj);\n@@ -182,2 +203,1 @@\n-  Node* generate_array_guard_common(Node* kls, RegionNode* region,\n-                                    bool obj_array, bool not_array, Node** obj = nullptr);\n+  Node* generate_array_guard_common(Node* kls, RegionNode* region, ArrayKind kind, Node** obj = nullptr);\n@@ -231,1 +251,1 @@\n-  bool inline_unsafe_access(bool is_store, BasicType type, AccessKind kind, bool is_unaligned);\n+  bool inline_unsafe_access(bool is_store, BasicType type, AccessKind kind, bool is_unaligned, bool is_flat = false);\n@@ -235,0 +255,1 @@\n+  bool inline_newArray(bool null_free, bool atomic);\n@@ -238,0 +259,3 @@\n+  bool inline_unsafe_isFlatArray();\n+  bool inline_unsafe_make_private_buffer();\n+  bool inline_unsafe_finish_private_buffer();\n@@ -264,0 +288,1 @@\n+  bool inline_primitive_Class_conversion(vmIntrinsics::ID id);\n","filename":"src\/hotspot\/share\/opto\/library_call.hpp","additions":37,"deletions":12,"binary":false,"changes":49,"status":"modified"},{"patch":"@@ -46,0 +46,1 @@\n+class UnswitchCandidate;\n@@ -88,1 +89,1 @@\n-       };\n+         FlatArrays            = 1<<18};\n@@ -111,0 +112,1 @@\n+  bool is_flat_arrays() const { return _loop_flags & FlatArrays; }\n@@ -124,0 +126,1 @@\n+  void mark_flat_arrays() { _loop_flags |= FlatArrays; }\n@@ -727,0 +730,1 @@\n+  bool no_unswitch_candidate() const;\n@@ -1470,1 +1474,2 @@\n-  IfNode* find_unswitch_candidate(const IdealLoopTree* loop) const;\n+  IfNode* find_unswitch_candidates(const IdealLoopTree* loop, Node_List& flat_array_checks) const;\n+  IfNode* find_unswitch_candidate_from_idoms(const IdealLoopTree* loop) const;\n@@ -1477,1 +1482,1 @@\n-                                   const UnswitchedLoopSelector& unswitched_loop_selector);\n+                                   const UnswitchCandidate& unswitch_candidate, const IfNode* loop_selector);\n@@ -1485,0 +1490,1 @@\n+                                            const UnswitchCandidate& unswitch_candidate,\n@@ -1626,0 +1632,1 @@\n+  void move_flat_array_check_out_of_loop(Node* n);\n@@ -1627,0 +1634,1 @@\n+  bool flat_array_element_type_check(Node *n);\n@@ -1816,0 +1824,2 @@\n+  void collect_flat_array_checks(const IdealLoopTree* loop, Node_List& flat_array_checks) const;\n+\n","filename":"src\/hotspot\/share\/opto\/loopnode.hpp","additions":13,"deletions":3,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -26,0 +26,1 @@\n+#include \"ci\/ciSymbols.hpp\"\n@@ -38,0 +39,2 @@\n+#include \"opto\/idealKit.hpp\"\n+#include \"opto\/inlinetypenode.hpp\"\n@@ -52,0 +55,17 @@\n+Node* Parse::record_profile_for_speculation_at_array_load(Node* ld) {\n+  \/\/ Feed unused profile data to type speculation\n+  if (UseTypeSpeculation && UseArrayLoadStoreProfile) {\n+    ciKlass* array_type = nullptr;\n+    ciKlass* element_type = nullptr;\n+    ProfilePtrKind element_ptr = ProfileMaybeNull;\n+    bool flat_array = true;\n+    bool null_free_array = true;\n+    method()->array_access_profiled_type(bci(), array_type, element_type, element_ptr, flat_array, null_free_array);\n+    if (element_type != nullptr || element_ptr != ProfileMaybeNull) {\n+      ld = record_profile_for_speculation(ld, element_type, element_ptr);\n+    }\n+  }\n+  return ld;\n+}\n+\n+\n@@ -55,1 +75,0 @@\n-  bool big_val = bt == T_DOUBLE || bt == T_LONG;\n@@ -59,2 +78,69 @@\n-  pop();                      \/\/ index (already used)\n-  Node* array = pop();        \/\/ the array itself\n+  Node* array_index = pop();\n+  Node* array = pop();\n+\n+  \/\/ Handle inline type arrays\n+  const TypeOopPtr* element_ptr = elemtype->make_oopptr();\n+  const TypeAryPtr* array_type = _gvn.type(array)->is_aryptr();\n+\n+  if (!array_type->is_not_flat()) {\n+    \/\/ Cannot statically determine if array is a flat array, emit runtime check\n+    assert(UseArrayFlattening && is_reference_type(bt) && element_ptr->can_be_inline_type() &&\n+           (!element_ptr->is_inlinetypeptr() || element_ptr->inline_klass()->flat_in_array()), \"array can't be flat\");\n+    IdealKit ideal(this);\n+    IdealVariable res(ideal);\n+    ideal.declarations_done();\n+    ideal.if_then(flat_array_test(array, \/* flat = *\/ false)); {\n+      \/\/ Non-flat array\n+      sync_kit(ideal);\n+      if (!array_type->is_flat()) {\n+        assert(array_type->is_flat() || control()->in(0)->as_If()->is_flat_array_check(&_gvn), \"Should be found\");\n+        const TypeAryPtr* adr_type = TypeAryPtr::get_array_body_type(bt);\n+        DecoratorSet decorator_set = IN_HEAP | IS_ARRAY | C2_CONTROL_DEPENDENT_LOAD;\n+        if (needs_range_check(array_type->size(), array_index)) {\n+          \/\/ We've emitted a RangeCheck but now insert an additional check between the range check and the actual load.\n+          \/\/ We cannot pin the load to two separate nodes. Instead, we pin it conservatively here such that it cannot\n+          \/\/ possibly float above the range check at any point.\n+          decorator_set |= C2_UNKNOWN_CONTROL_LOAD;\n+        }\n+        Node* ld = access_load_at(array, adr, adr_type, element_ptr, bt, decorator_set);\n+        if (element_ptr->is_inlinetypeptr()) {\n+          ld = InlineTypeNode::make_from_oop(this, ld, element_ptr->inline_klass());\n+        }\n+        ideal.set(res, ld);\n+      }\n+      ideal.sync_kit(this);\n+    } ideal.else_(); {\n+      \/\/ Flat array\n+      sync_kit(ideal);\n+      if (!array_type->is_not_flat()) {\n+        if (element_ptr->is_inlinetypeptr()) {\n+          \/\/ Element type is known, cast and load from flat array layout.\n+          ciInlineKlass* vk = element_ptr->inline_klass();\n+          bool is_null_free = array_type->is_null_free() || !vk->has_nullable_atomic_layout();\n+          bool is_not_null_free = array_type->is_not_null_free() || (!vk->has_atomic_layout() && !vk->has_non_atomic_layout());\n+          if (is_null_free) {\n+            \/\/ TODO 8350865 Impossible type\n+            is_not_null_free = false;\n+          }\n+          bool is_naturally_atomic = is_null_free && vk->nof_declared_nonstatic_fields() <= 1;\n+          bool may_need_atomicity = !is_naturally_atomic && ((!is_not_null_free && vk->has_atomic_layout()) || (!is_null_free && vk->has_nullable_atomic_layout()));\n+\n+          adr = flat_array_element_address(array, array_index, vk, is_null_free, is_not_null_free, may_need_atomicity);\n+          int nm_offset = is_null_free ? -1 : vk->null_marker_offset_in_payload();\n+          Node* vt = InlineTypeNode::make_from_flat(this, vk, array, adr, array_index, nullptr, 0, may_need_atomicity, nm_offset);\n+          ideal.set(res, vt);\n+        } else {\n+          \/\/ Element type is unknown, and thus we cannot statically determine the exact flat array layout. Emit a\n+          \/\/ runtime call to correctly load the inline type element from the flat array.\n+          Node* inline_type = load_from_unknown_flat_array(array, array_index, element_ptr);\n+          ideal.set(res, inline_type);\n+        }\n+      }\n+      ideal.sync_kit(this);\n+    } ideal.end_if();\n+    sync_kit(ideal);\n+    Node* ld = _gvn.transform(ideal.value(res));\n+    ld = record_profile_for_speculation_at_array_load(ld);\n+    push_node(bt, ld);\n+    return;\n+  }\n@@ -66,1 +152,0 @@\n-\n@@ -69,4 +154,5 @@\n-  if (big_val) {\n-    push_pair(ld);\n-  } else {\n-    push(ld);\n+  ld = record_profile_for_speculation_at_array_load(ld);\n+  \/\/ Loading an inline type from a non-flat array\n+  if (element_ptr != nullptr && element_ptr->is_inlinetypeptr()) {\n+    assert(!array_type->is_null_free() || !element_ptr->maybe_null(), \"inline type array elements should never be null\");\n+    ld = InlineTypeNode::make_from_oop(this, ld, element_ptr->inline_klass());\n@@ -74,0 +160,1 @@\n+  push_node(bt, ld);\n@@ -76,0 +163,28 @@\n+Node* Parse::load_from_unknown_flat_array(Node* array, Node* array_index, const TypeOopPtr* element_ptr) {\n+  \/\/ Below membars keep this access to an unknown flat array correctly\n+  \/\/ ordered with other unknown and known flat array accesses.\n+  insert_mem_bar_volatile(Op_MemBarCPUOrder, C->get_alias_index(TypeAryPtr::INLINES));\n+\n+  Node* call = nullptr;\n+  {\n+    \/\/ Re-execute flat array load if runtime call triggers deoptimization\n+    PreserveReexecuteState preexecs(this);\n+    jvms()->set_bci(_bci);\n+    jvms()->set_should_reexecute(true);\n+    inc_sp(2);\n+    kill_dead_locals();\n+    call = make_runtime_call(RC_NO_LEAF | RC_NO_IO,\n+                             OptoRuntime::load_unknown_inline_Type(),\n+                             OptoRuntime::load_unknown_inline_Java(),\n+                             nullptr, TypeRawPtr::BOTTOM,\n+                             array, array_index);\n+  }\n+  make_slow_call_ex(call, env()->Throwable_klass(), false);\n+  Node* buffer = _gvn.transform(new ProjNode(call, TypeFunc::Parms));\n+\n+  insert_mem_bar_volatile(Op_MemBarCPUOrder, C->get_alias_index(TypeAryPtr::INLINES));\n+\n+  \/\/ Keep track of the information that the inline type is in flat arrays\n+  const Type* unknown_value = element_ptr->is_instptr()->cast_to_flat_in_array();\n+  return _gvn.transform(new CheckCastPPNode(control(), buffer, unknown_value));\n+}\n@@ -80,2 +195,1 @@\n-  bool big_val = bt == T_DOUBLE || bt == T_LONG;\n-  Node* adr = array_addressing(bt, big_val ? 2 : 1, elemtype);\n+  Node* adr = array_addressing(bt, type2size[bt], elemtype);\n@@ -83,0 +197,1 @@\n+  Node* stored_value_casted = nullptr;\n@@ -84,1 +199,1 @@\n-    array_store_check();\n+    stored_value_casted = array_store_check(adr, elemtype);\n@@ -89,8 +204,6 @@\n-  Node* val;                  \/\/ Oop to store\n-  if (big_val) {\n-    val = pop_pair();\n-  } else {\n-    val = pop();\n-  }\n-  pop();                      \/\/ index (already used)\n-  Node* array = pop();        \/\/ the array itself\n+  Node* const stored_value = pop_node(bt); \/\/ Value to store\n+  Node* const array_index = pop();         \/\/ Index in the array\n+  Node* array = pop();                     \/\/ The array itself\n+\n+  const TypeAryPtr* array_type = _gvn.type(array)->is_aryptr();\n+  const TypeAryPtr* adr_type = TypeAryPtr::get_array_body_type(bt);\n@@ -100,2 +213,60 @@\n-  }\n-  const TypeAryPtr* adr_type = TypeAryPtr::get_array_body_type(bt);\n+  } else if (bt == T_OBJECT) {\n+    elemtype = elemtype->make_oopptr();\n+    const Type* stored_value_casted_type = _gvn.type(stored_value_casted);\n+    \/\/ Based on the value to be stored, try to determine if the array is not null-free and\/or not flat.\n+    \/\/ This is only legal for non-null stores because the array_store_check always passes for null, even\n+    \/\/ if the array is null-free. Null stores are handled in GraphKit::inline_array_null_guard().\n+    bool not_inline = !stored_value_casted_type->maybe_null() && !stored_value_casted_type->is_oopptr()->can_be_inline_type();\n+    bool not_null_free = not_inline;\n+    bool not_flat = not_inline || ( stored_value_casted_type->is_inlinetypeptr() &&\n+                                   !stored_value_casted_type->inline_klass()->flat_in_array());\n+    if (!array_type->is_not_null_free() && not_null_free) {\n+      \/\/ Storing a non-inline type, mark array as not null-free.\n+      array_type = array_type->cast_to_not_null_free();\n+      Node* cast = _gvn.transform(new CheckCastPPNode(control(), array, array_type));\n+      replace_in_map(array, cast);\n+      array = cast;\n+    }\n+    if (!array_type->is_not_flat() && not_flat) {\n+      \/\/ Storing to a non-flat array, mark array as not flat.\n+      array_type = array_type->cast_to_not_flat();\n+      Node* cast = _gvn.transform(new CheckCastPPNode(control(), array, array_type));\n+      replace_in_map(array, cast);\n+      array = cast;\n+    }\n+\n+    if (!array_type->is_flat() && array_type->is_null_free()) {\n+      \/\/ Store to non-flat null-free inline type array (elements can never be null)\n+      assert(!stored_value_casted_type->maybe_null(), \"should be guaranteed by array store check\");\n+      if (elemtype->is_inlinetypeptr() && elemtype->inline_klass()->is_empty()) {\n+        \/\/ Ignore empty inline stores, array is already initialized.\n+        return;\n+      }\n+    } else if (!array_type->is_not_flat()) {\n+      \/\/ Array might be a flat array, emit runtime checks (for nullptr, a simple inline_array_null_guard is sufficient).\n+      assert(UseArrayFlattening && !not_flat && elemtype->is_oopptr()->can_be_inline_type() &&\n+             (!array_type->klass_is_exact() || array_type->is_flat()), \"array can't be a flat array\");\n+      \/\/ TODO 8350865 Depending on the available layouts, we can avoid this check in below flat\/not-flat branches. Also the safe_for_replace arg is now always true.\n+      array = inline_array_null_guard(array, stored_value_casted, 3, true);\n+      IdealKit ideal(this);\n+      ideal.if_then(flat_array_test(array, \/* flat = *\/ false)); {\n+        \/\/ Non-flat array\n+        if (!array_type->is_flat()) {\n+          sync_kit(ideal);\n+          assert(array_type->is_flat() || ideal.ctrl()->in(0)->as_If()->is_flat_array_check(&_gvn), \"Should be found\");\n+          inc_sp(3);\n+          access_store_at(array, adr, adr_type, stored_value_casted, elemtype, bt, MO_UNORDERED | IN_HEAP | IS_ARRAY, false);\n+          dec_sp(3);\n+          ideal.sync_kit(this);\n+        }\n+      } ideal.else_(); {\n+        \/\/ Flat array\n+        sync_kit(ideal);\n+        if (!array_type->is_not_flat()) {\n+          \/\/ Try to determine the inline klass type of the stored value\n+          ciInlineKlass* vk = nullptr;\n+          if (stored_value_casted_type->is_inlinetypeptr()) {\n+            vk = stored_value_casted_type->inline_klass();\n+          } else if (elemtype->is_inlinetypeptr()) {\n+            vk = elemtype->inline_klass();\n+          }\n@@ -103,1 +274,42 @@\n-  access_store_at(array, adr, adr_type, val, elemtype, bt, MO_UNORDERED | IN_HEAP | IS_ARRAY);\n+          if (vk != nullptr) {\n+            \/\/ Element type is known, cast and store to flat array layout.\n+            bool is_null_free = array_type->is_null_free() || !vk->has_nullable_atomic_layout();\n+            bool is_not_null_free = array_type->is_not_null_free() || (!vk->has_atomic_layout() && !vk->has_non_atomic_layout());\n+            if (is_null_free) {\n+              \/\/ TODO 8350865 Impossible type\n+              is_not_null_free = false;\n+            }\n+            bool is_naturally_atomic = is_null_free && vk->nof_declared_nonstatic_fields() <= 1;\n+            bool may_need_atomicity = !is_naturally_atomic && ((!is_not_null_free && vk->has_atomic_layout()) || (!is_null_free && vk->has_nullable_atomic_layout()));\n+\n+            \/\/ Re-execute flat array store if buffering triggers deoptimization\n+            PreserveReexecuteState preexecs(this);\n+            jvms()->set_should_reexecute(true);\n+            inc_sp(3);\n+\n+            if (!stored_value_casted->is_InlineType()) {\n+              assert(_gvn.type(stored_value_casted) == TypePtr::NULL_PTR, \"Unexpected value\");\n+              stored_value_casted = InlineTypeNode::make_null(_gvn, vk);\n+            }\n+            adr = flat_array_element_address(array, array_index, vk, is_null_free, is_not_null_free, may_need_atomicity);\n+            int nm_offset = is_null_free ? -1 : vk->null_marker_offset_in_payload();\n+            stored_value_casted->as_InlineType()->store_flat(this, array, adr, array_index, nullptr, 0, may_need_atomicity, nm_offset, MO_UNORDERED | IN_HEAP | IS_ARRAY);\n+          } else {\n+            \/\/ Element type is unknown, emit a runtime call since the flat array layout is not statically known.\n+            store_to_unknown_flat_array(array, array_index, stored_value_casted);\n+          }\n+        }\n+        ideal.sync_kit(this);\n+      }\n+      ideal.end_if();\n+      sync_kit(ideal);\n+      return;\n+    } else if (!array_type->is_not_null_free()) {\n+      \/\/ Array is not flat but may be null free\n+      assert(elemtype->is_oopptr()->can_be_inline_type(), \"array can't be null-free\");\n+      array = inline_array_null_guard(array, stored_value_casted, 3, true);\n+    }\n+  }\n+  inc_sp(3);\n+  access_store_at(array, adr, adr_type, stored_value, elemtype, bt, MO_UNORDERED | IN_HEAP | IS_ARRAY);\n+  dec_sp(3);\n@@ -106,0 +318,25 @@\n+\/\/ Emit a runtime call to store to a flat array whose element type is either unknown (i.e. we do not know the flat\n+\/\/ array layout) or not exact (could have different flat array layouts at runtime).\n+void Parse::store_to_unknown_flat_array(Node* array, Node* const idx, Node* non_null_stored_value) {\n+  \/\/ Below membars keep this access to an unknown flat array correctly\n+  \/\/ ordered with other unknown and known flat array accesses.\n+  insert_mem_bar_volatile(Op_MemBarCPUOrder, C->get_alias_index(TypeAryPtr::INLINES));\n+\n+  Node* call = nullptr;\n+  {\n+    \/\/ Re-execute flat array store if runtime call triggers deoptimization\n+    PreserveReexecuteState preexecs(this);\n+    jvms()->set_bci(_bci);\n+    jvms()->set_should_reexecute(true);\n+    inc_sp(3);\n+    kill_dead_locals();\n+    call = make_runtime_call(RC_NO_LEAF | RC_NO_IO,\n+                      OptoRuntime::store_unknown_inline_Type(),\n+                      OptoRuntime::store_unknown_inline_Java(),\n+                      nullptr, TypeRawPtr::BOTTOM,\n+                      non_null_stored_value, array, idx);\n+  }\n+  make_slow_call_ex(call, env()->Throwable_klass(), false);\n+\n+  insert_mem_bar_volatile(Op_MemBarCPUOrder, C->get_alias_index(TypeAryPtr::INLINES));\n+}\n@@ -134,11 +371,0 @@\n-  \/\/ Check for big class initializers with all constant offsets\n-  \/\/ feeding into a known-size array.\n-  const TypeInt* idxtype = _gvn.type(idx)->is_int();\n-  \/\/ See if the highest idx value is less than the lowest array bound,\n-  \/\/ and if the idx value cannot be negative:\n-  bool need_range_check = true;\n-  if (idxtype->_hi < sizetype->_lo && idxtype->_lo >= 0) {\n-    need_range_check = false;\n-    if (C->log() != nullptr)   C->log()->elem(\"observe that='!need_range_check'\");\n-  }\n-\n@@ -156,12 +382,1 @@\n-  \/\/ Do the range check\n-  if (need_range_check) {\n-    Node* tst;\n-    if (sizetype->_hi <= 0) {\n-      \/\/ The greatest array bound is negative, so we can conclude that we're\n-      \/\/ compiling unreachable code, but the unsigned compare trick used below\n-      \/\/ only works with non-negative lengths.  Instead, hack \"tst\" to be zero so\n-      \/\/ the uncommon_trap path will always be taken.\n-      tst = _gvn.intcon(0);\n-    } else {\n-      \/\/ Range is constant in array-oop, so we can use the original state of mem\n-      Node* len = load_array_length(ary);\n+  ary = create_speculative_inline_type_array_checks(ary, arytype, elemtype);\n@@ -169,31 +384,4 @@\n-      \/\/ Test length vs index (standard trick using unsigned compare)\n-      Node* chk = _gvn.transform( new CmpUNode(idx, len) );\n-      BoolTest::mask btest = BoolTest::lt;\n-      tst = _gvn.transform( new BoolNode(chk, btest) );\n-    }\n-    RangeCheckNode* rc = new RangeCheckNode(control(), tst, PROB_MAX, COUNT_UNKNOWN);\n-    _gvn.set_type(rc, rc->Value(&_gvn));\n-    if (!tst->is_Con()) {\n-      record_for_igvn(rc);\n-    }\n-    set_control(_gvn.transform(new IfTrueNode(rc)));\n-    \/\/ Branch to failure if out of bounds\n-    {\n-      PreserveJVMState pjvms(this);\n-      set_control(_gvn.transform(new IfFalseNode(rc)));\n-      if (C->allow_range_check_smearing()) {\n-        \/\/ Do not use builtin_throw, since range checks are sometimes\n-        \/\/ made more stringent by an optimistic transformation.\n-        \/\/ This creates \"tentative\" range checks at this point,\n-        \/\/ which are not guaranteed to throw exceptions.\n-        \/\/ See IfNode::Ideal, is_range_check, adjust_check.\n-        uncommon_trap(Deoptimization::Reason_range_check,\n-                      Deoptimization::Action_make_not_entrant,\n-                      nullptr, \"range_check\");\n-      } else {\n-        \/\/ If we have already recompiled with the range-check-widening\n-        \/\/ heroic optimization turned off, then we must really be throwing\n-        \/\/ range check exceptions.\n-        builtin_throw(Deoptimization::Reason_range_check);\n-      }\n-    }\n+  if (needs_range_check(sizetype, idx)) {\n+    create_range_check(idx, ary, sizetype);\n+  } else if (C->log() != nullptr) {\n+    C->log()->elem(\"observe that='!need_range_check'\");\n@@ -201,0 +389,1 @@\n+\n@@ -212,0 +401,199 @@\n+\/\/ Check if we need a range check for an array access. This is the case if the index is either negative or if it could\n+\/\/ be greater or equal the smallest possible array size (i.e. out-of-bounds).\n+bool Parse::needs_range_check(const TypeInt* size_type, const Node* index) const {\n+  const TypeInt* index_type = _gvn.type(index)->is_int();\n+  return index_type->_hi >= size_type->_lo || index_type->_lo < 0;\n+}\n+\n+void Parse::create_range_check(Node* idx, Node* ary, const TypeInt* sizetype) {\n+  Node* tst;\n+  if (sizetype->_hi <= 0) {\n+    \/\/ The greatest array bound is negative, so we can conclude that we're\n+    \/\/ compiling unreachable code, but the unsigned compare trick used below\n+    \/\/ only works with non-negative lengths.  Instead, hack \"tst\" to be zero so\n+    \/\/ the uncommon_trap path will always be taken.\n+    tst = _gvn.intcon(0);\n+  } else {\n+    \/\/ Range is constant in array-oop, so we can use the original state of mem\n+    Node* len = load_array_length(ary);\n+\n+    \/\/ Test length vs index (standard trick using unsigned compare)\n+    Node* chk = _gvn.transform(new CmpUNode(idx, len) );\n+    BoolTest::mask btest = BoolTest::lt;\n+    tst = _gvn.transform(new BoolNode(chk, btest) );\n+  }\n+  RangeCheckNode* rc = new RangeCheckNode(control(), tst, PROB_MAX, COUNT_UNKNOWN);\n+  _gvn.set_type(rc, rc->Value(&_gvn));\n+  if (!tst->is_Con()) {\n+    record_for_igvn(rc);\n+  }\n+  set_control(_gvn.transform(new IfTrueNode(rc)));\n+  \/\/ Branch to failure if out of bounds\n+  {\n+    PreserveJVMState pjvms(this);\n+    set_control(_gvn.transform(new IfFalseNode(rc)));\n+    if (C->allow_range_check_smearing()) {\n+      \/\/ Do not use builtin_throw, since range checks are sometimes\n+      \/\/ made more stringent by an optimistic transformation.\n+      \/\/ This creates \"tentative\" range checks at this point,\n+      \/\/ which are not guaranteed to throw exceptions.\n+      \/\/ See IfNode::Ideal, is_range_check, adjust_check.\n+      uncommon_trap(Deoptimization::Reason_range_check,\n+                    Deoptimization::Action_make_not_entrant,\n+                    nullptr, \"range_check\");\n+    } else {\n+      \/\/ If we have already recompiled with the range-check-widening\n+      \/\/ heroic optimization turned off, then we must really be throwing\n+      \/\/ range check exceptions.\n+      builtin_throw(Deoptimization::Reason_range_check);\n+    }\n+  }\n+}\n+\n+\/\/ For inline type arrays, we can use the profiling information for array accesses to speculate on the type, flatness,\n+\/\/ and null-freeness. We can either prepare the speculative type for later uses or emit explicit speculative checks with\n+\/\/ traps now. In the latter case, the speculative type guarantees can avoid additional runtime checks later (e.g.\n+\/\/ non-null-free implies non-flat which allows us to remove flatness checks). This makes the graph simpler.\n+Node* Parse::create_speculative_inline_type_array_checks(Node* array, const TypeAryPtr* array_type,\n+                                                         const Type*& element_type) {\n+  if (!array_type->is_flat() && !array_type->is_not_flat()) {\n+    \/\/ For arrays that might be flat, speculate that the array has the exact type reported in the profile data such that\n+    \/\/ we can rely on a fixed memory layout (i.e. either a flat layout or not).\n+    array = cast_to_speculative_array_type(array, array_type, element_type);\n+  } else if (UseTypeSpeculation && UseArrayLoadStoreProfile) {\n+    \/\/ Array is known to be either flat or not flat. If possible, update the speculative type by using the profile data\n+    \/\/ at this bci.\n+    array = cast_to_profiled_array_type(array);\n+  }\n+\n+  \/\/ Even though the type does not tell us whether we have an inline type array or not, we can still check the profile data\n+  \/\/ whether we have a non-null-free or non-flat array. Speculating on a non-null-free array doesn't help aaload but could\n+  \/\/ be profitable for a subsequent aastore.\n+  if (!array_type->is_null_free() && !array_type->is_not_null_free()) {\n+    array = speculate_non_null_free_array(array, array_type);\n+  }\n+  if (!array_type->is_flat() && !array_type->is_not_flat()) {\n+    array = speculate_non_flat_array(array, array_type);\n+  }\n+  return array;\n+}\n+\n+\/\/ Speculate that the array has the exact type reported in the profile data. We emit a trap when this turns out to be\n+\/\/ wrong. On the fast path, we add a CheckCastPP to use the exact type.\n+Node* Parse::cast_to_speculative_array_type(Node* const array, const TypeAryPtr*& array_type, const Type*& element_type) {\n+  Deoptimization::DeoptReason reason = Deoptimization::Reason_speculate_class_check;\n+  ciKlass* speculative_array_type = array_type->speculative_type();\n+  if (too_many_traps_or_recompiles(reason) || speculative_array_type == nullptr) {\n+    \/\/ No speculative type, check profile data at this bci\n+    speculative_array_type = nullptr;\n+    reason = Deoptimization::Reason_class_check;\n+    if (UseArrayLoadStoreProfile && !too_many_traps_or_recompiles(reason)) {\n+      ciKlass* profiled_element_type = nullptr;\n+      ProfilePtrKind element_ptr = ProfileMaybeNull;\n+      bool flat_array = true;\n+      bool null_free_array = true;\n+      method()->array_access_profiled_type(bci(), speculative_array_type, profiled_element_type, element_ptr, flat_array,\n+                                           null_free_array);\n+    }\n+  }\n+  if (speculative_array_type != nullptr) {\n+    \/\/ Speculate that this array has the exact type reported by profile data\n+    Node* casted_array = nullptr;\n+    DEBUG_ONLY(Node* old_control = control();)\n+    Node* slow_ctl = type_check_receiver(array, speculative_array_type, 1.0, &casted_array);\n+    if (stopped()) {\n+      \/\/ The check always fails and therefore profile information is incorrect. Don't use it.\n+      assert(old_control == slow_ctl, \"type check should have been removed\");\n+      set_control(slow_ctl);\n+    } else if (!slow_ctl->is_top()) {\n+      { PreserveJVMState pjvms(this);\n+        set_control(slow_ctl);\n+        uncommon_trap_exact(reason, Deoptimization::Action_maybe_recompile);\n+      }\n+      replace_in_map(array, casted_array);\n+      array_type = _gvn.type(casted_array)->is_aryptr();\n+      element_type = array_type->elem();\n+      return casted_array;\n+    }\n+  }\n+  return array;\n+}\n+\n+\/\/ Create a CheckCastPP when the speculative type can improve the current type.\n+Node* Parse::cast_to_profiled_array_type(Node* const array) {\n+  ciKlass* array_type = nullptr;\n+  ciKlass* element_type = nullptr;\n+  ProfilePtrKind element_ptr = ProfileMaybeNull;\n+  bool flat_array = true;\n+  bool null_free_array = true;\n+  method()->array_access_profiled_type(bci(), array_type, element_type, element_ptr, flat_array, null_free_array);\n+  if (array_type != nullptr) {\n+    return record_profile_for_speculation(array, array_type, ProfileMaybeNull);\n+  }\n+  return array;\n+}\n+\n+\/\/ Speculate that the array is non-null-free. We emit a trap when this turns out to be\n+\/\/ wrong. On the fast path, we add a CheckCastPP to use the non-null-free type.\n+Node* Parse::speculate_non_null_free_array(Node* const array, const TypeAryPtr*& array_type) {\n+  bool null_free_array = true;\n+  Deoptimization::DeoptReason reason = Deoptimization::Reason_none;\n+  if (array_type->speculative() != nullptr &&\n+      array_type->speculative()->is_aryptr()->is_not_null_free() &&\n+      !too_many_traps_or_recompiles(Deoptimization::Reason_speculate_class_check)) {\n+    null_free_array = false;\n+    reason = Deoptimization::Reason_speculate_class_check;\n+  } else if (UseArrayLoadStoreProfile && !too_many_traps_or_recompiles(Deoptimization::Reason_class_check)) {\n+    ciKlass* profiled_array_type = nullptr;\n+    ciKlass* profiled_element_type = nullptr;\n+    ProfilePtrKind element_ptr = ProfileMaybeNull;\n+    bool flat_array = true;\n+    method()->array_access_profiled_type(bci(), profiled_array_type, profiled_element_type, element_ptr, flat_array,\n+                                         null_free_array);\n+    reason = Deoptimization::Reason_class_check;\n+  }\n+  if (!null_free_array) {\n+    { \/\/ Deoptimize if null-free array\n+      BuildCutout unless(this, null_free_array_test(array, \/* null_free = *\/ false), PROB_MAX);\n+      uncommon_trap_exact(reason, Deoptimization::Action_maybe_recompile);\n+    }\n+    assert(!stopped(), \"null-free array should have been caught earlier\");\n+    Node* casted_array = _gvn.transform(new CheckCastPPNode(control(), array, array_type->cast_to_not_null_free()));\n+    replace_in_map(array, casted_array);\n+    array_type = _gvn.type(casted_array)->is_aryptr();\n+    return casted_array;\n+  }\n+  return array;\n+}\n+\n+\/\/ Speculate that the array is non-flat. We emit a trap when this turns out to be wrong.\n+\/\/ On the fast path, we add a CheckCastPP to use the non-flat type.\n+Node* Parse::speculate_non_flat_array(Node* const array, const TypeAryPtr* const array_type) {\n+  bool flat_array = true;\n+  Deoptimization::DeoptReason reason = Deoptimization::Reason_none;\n+  if (array_type->speculative() != nullptr &&\n+      array_type->speculative()->is_aryptr()->is_not_flat() &&\n+      !too_many_traps_or_recompiles(Deoptimization::Reason_speculate_class_check)) {\n+    flat_array = false;\n+    reason = Deoptimization::Reason_speculate_class_check;\n+  } else if (UseArrayLoadStoreProfile && !too_many_traps_or_recompiles(reason)) {\n+    ciKlass* profiled_array_type = nullptr;\n+    ciKlass* profiled_element_type = nullptr;\n+    ProfilePtrKind element_ptr = ProfileMaybeNull;\n+    bool null_free_array = true;\n+    method()->array_access_profiled_type(bci(), profiled_array_type, profiled_element_type, element_ptr, flat_array,\n+                                         null_free_array);\n+    reason = Deoptimization::Reason_class_check;\n+  }\n+  if (!flat_array) {\n+    { \/\/ Deoptimize if flat array\n+      BuildCutout unless(this, flat_array_test(array, \/* flat = *\/ false), PROB_MAX);\n+      uncommon_trap_exact(reason, Deoptimization::Action_maybe_recompile);\n+    }\n+    assert(!stopped(), \"flat array should have been caught earlier\");\n+    Node* casted_array = _gvn.transform(new CheckCastPPNode(control(), array, array_type->cast_to_not_flat()));\n+    replace_in_map(array, casted_array);\n+    return casted_array;\n+  }\n+  return array;\n+}\n@@ -1448,1 +1836,1 @@\n-void Parse::do_if(BoolTest::mask btest, Node* c) {\n+void Parse::do_if(BoolTest::mask btest, Node* c, bool can_trap, bool new_path, Node** ctrl_taken) {\n@@ -1539,2 +1927,2 @@\n-      if (C->eliminate_boxing()) {\n-        \/\/ Mark the successor block as parsed\n+      if (C->eliminate_boxing() && !new_path) {\n+        \/\/ Mark the successor block as parsed (if we haven't created a new path)\n@@ -1544,1 +1932,1 @@\n-      adjust_map_after_if(taken_btest, c, prob, branch_block);\n+      adjust_map_after_if(taken_btest, c, prob, branch_block, can_trap);\n@@ -1546,1 +1934,9 @@\n-        merge(target_bci);\n+        if (new_path) {\n+          \/\/ Merge by using a new path\n+          merge_new_path(target_bci);\n+        } else if (ctrl_taken != nullptr) {\n+          \/\/ Don't merge but save taken branch to be wired by caller\n+          *ctrl_taken = control();\n+        } else {\n+          merge(target_bci);\n+        }\n@@ -1555,1 +1951,1 @@\n-  if (stopped()) {\n+  if (stopped() && ctrl_taken == nullptr) {\n@@ -1557,1 +1953,1 @@\n-      \/\/ Mark the successor block as parsed\n+      \/\/ Mark the successor block as parsed (if caller does not re-wire control flow)\n@@ -1561,1 +1957,1 @@\n-    adjust_map_after_if(untaken_btest, c, untaken_prob, next_block);\n+    adjust_map_after_if(untaken_btest, c, untaken_prob, next_block, can_trap);\n@@ -1569,0 +1965,404 @@\n+\n+static ProfilePtrKind speculative_ptr_kind(const TypeOopPtr* t) {\n+  if (t->speculative() == nullptr) {\n+    return ProfileUnknownNull;\n+  }\n+  if (t->speculative_always_null()) {\n+    return ProfileAlwaysNull;\n+  }\n+  if (t->speculative_maybe_null()) {\n+    return ProfileMaybeNull;\n+  }\n+  return ProfileNeverNull;\n+}\n+\n+void Parse::acmp_always_null_input(Node* input, const TypeOopPtr* tinput, BoolTest::mask btest, Node* eq_region) {\n+  inc_sp(2);\n+  Node* cast = null_check_common(input, T_OBJECT, true, nullptr,\n+                                 !too_many_traps_or_recompiles(Deoptimization::Reason_speculate_null_check) &&\n+                                 speculative_ptr_kind(tinput) == ProfileAlwaysNull);\n+  dec_sp(2);\n+  if (btest == BoolTest::ne) {\n+    {\n+      PreserveJVMState pjvms(this);\n+      replace_in_map(input, cast);\n+      int target_bci = iter().get_dest();\n+      merge(target_bci);\n+    }\n+    record_for_igvn(eq_region);\n+    set_control(_gvn.transform(eq_region));\n+  } else {\n+    replace_in_map(input, cast);\n+  }\n+}\n+\n+Node* Parse::acmp_null_check(Node* input, const TypeOopPtr* tinput, ProfilePtrKind input_ptr, Node*& null_ctl) {\n+  inc_sp(2);\n+  null_ctl = top();\n+  Node* cast = null_check_oop(input, &null_ctl,\n+                              input_ptr == ProfileNeverNull || (input_ptr == ProfileUnknownNull && !too_many_traps_or_recompiles(Deoptimization::Reason_null_check)),\n+                              false,\n+                              speculative_ptr_kind(tinput) == ProfileNeverNull &&\n+                              !too_many_traps_or_recompiles(Deoptimization::Reason_speculate_null_check));\n+  dec_sp(2);\n+  assert(!stopped(), \"null input should have been caught earlier\");\n+  return cast;\n+}\n+\n+void Parse::acmp_known_non_inline_type_input(Node* input, const TypeOopPtr* tinput, ProfilePtrKind input_ptr, ciKlass* input_type, BoolTest::mask btest, Node* eq_region) {\n+  Node* ne_region = new RegionNode(1);\n+  Node* null_ctl;\n+  Node* cast = acmp_null_check(input, tinput, input_ptr, null_ctl);\n+  ne_region->add_req(null_ctl);\n+\n+  Node* slow_ctl = type_check_receiver(cast, input_type, 1.0, &cast);\n+  {\n+    PreserveJVMState pjvms(this);\n+    inc_sp(2);\n+    set_control(slow_ctl);\n+    Deoptimization::DeoptReason reason;\n+    if (tinput->speculative_type() != nullptr && !too_many_traps_or_recompiles(Deoptimization::Reason_speculate_class_check)) {\n+      reason = Deoptimization::Reason_speculate_class_check;\n+    } else {\n+      reason = Deoptimization::Reason_class_check;\n+    }\n+    uncommon_trap_exact(reason, Deoptimization::Action_maybe_recompile);\n+  }\n+  ne_region->add_req(control());\n+\n+  record_for_igvn(ne_region);\n+  set_control(_gvn.transform(ne_region));\n+  if (btest == BoolTest::ne) {\n+    {\n+      PreserveJVMState pjvms(this);\n+      if (null_ctl == top()) {\n+        replace_in_map(input, cast);\n+      }\n+      int target_bci = iter().get_dest();\n+      merge(target_bci);\n+    }\n+    record_for_igvn(eq_region);\n+    set_control(_gvn.transform(eq_region));\n+  } else {\n+    if (null_ctl == top()) {\n+      replace_in_map(input, cast);\n+    }\n+    set_control(_gvn.transform(ne_region));\n+  }\n+}\n+\n+void Parse::acmp_unknown_non_inline_type_input(Node* input, const TypeOopPtr* tinput, ProfilePtrKind input_ptr, BoolTest::mask btest, Node* eq_region) {\n+  Node* ne_region = new RegionNode(1);\n+  Node* null_ctl;\n+  Node* cast = acmp_null_check(input, tinput, input_ptr, null_ctl);\n+  ne_region->add_req(null_ctl);\n+\n+  {\n+    BuildCutout unless(this, inline_type_test(cast, \/* is_inline = *\/ false), PROB_MAX);\n+    inc_sp(2);\n+    uncommon_trap_exact(Deoptimization::Reason_class_check, Deoptimization::Action_maybe_recompile);\n+  }\n+\n+  ne_region->add_req(control());\n+\n+  record_for_igvn(ne_region);\n+  set_control(_gvn.transform(ne_region));\n+  if (btest == BoolTest::ne) {\n+    {\n+      PreserveJVMState pjvms(this);\n+      if (null_ctl == top()) {\n+        replace_in_map(input, cast);\n+      }\n+      int target_bci = iter().get_dest();\n+      merge(target_bci);\n+    }\n+    record_for_igvn(eq_region);\n+    set_control(_gvn.transform(eq_region));\n+  } else {\n+    if (null_ctl == top()) {\n+      replace_in_map(input, cast);\n+    }\n+    set_control(_gvn.transform(ne_region));\n+  }\n+}\n+\n+void Parse::do_acmp(BoolTest::mask btest, Node* left, Node* right) {\n+  ciKlass* left_type = nullptr;\n+  ciKlass* right_type = nullptr;\n+  ProfilePtrKind left_ptr = ProfileUnknownNull;\n+  ProfilePtrKind right_ptr = ProfileUnknownNull;\n+  bool left_inline_type = true;\n+  bool right_inline_type = true;\n+\n+  \/\/ Leverage profiling at acmp\n+  if (UseACmpProfile) {\n+    method()->acmp_profiled_type(bci(), left_type, right_type, left_ptr, right_ptr, left_inline_type, right_inline_type);\n+    if (too_many_traps_or_recompiles(Deoptimization::Reason_class_check)) {\n+      left_type = nullptr;\n+      right_type = nullptr;\n+      left_inline_type = true;\n+      right_inline_type = true;\n+    }\n+    if (too_many_traps_or_recompiles(Deoptimization::Reason_null_check)) {\n+      left_ptr = ProfileUnknownNull;\n+      right_ptr = ProfileUnknownNull;\n+    }\n+  }\n+\n+  if (UseTypeSpeculation) {\n+    record_profile_for_speculation(left, left_type, left_ptr);\n+    record_profile_for_speculation(right, right_type, right_ptr);\n+  }\n+\n+  if (!EnableValhalla) {\n+    Node* cmp = CmpP(left, right);\n+    cmp = optimize_cmp_with_klass(cmp);\n+    do_if(btest, cmp);\n+    return;\n+  }\n+\n+  \/\/ Check for equality before potentially allocating\n+  if (left == right) {\n+    do_if(btest, makecon(TypeInt::CC_EQ));\n+    return;\n+  }\n+\n+  \/\/ Allocate inline type operands and re-execute on deoptimization\n+  if (left->is_InlineType()) {\n+    if (_gvn.type(right)->is_zero_type() ||\n+        (right->is_InlineType() && _gvn.type(right->as_InlineType()->get_is_init())->is_zero_type())) {\n+      \/\/ Null checking a scalarized but nullable inline type. Check the IsInit\n+      \/\/ input instead of the oop input to avoid keeping buffer allocations alive.\n+      Node* cmp = CmpI(left->as_InlineType()->get_is_init(), intcon(0));\n+      do_if(btest, cmp);\n+      return;\n+    } else {\n+      PreserveReexecuteState preexecs(this);\n+      inc_sp(2);\n+      jvms()->set_should_reexecute(true);\n+      left = left->as_InlineType()->buffer(this)->get_oop();\n+    }\n+  }\n+  if (right->is_InlineType()) {\n+    PreserveReexecuteState preexecs(this);\n+    inc_sp(2);\n+    jvms()->set_should_reexecute(true);\n+    right = right->as_InlineType()->buffer(this)->get_oop();\n+  }\n+\n+  \/\/ First, do a normal pointer comparison\n+  const TypeOopPtr* tleft = _gvn.type(left)->isa_oopptr();\n+  const TypeOopPtr* tright = _gvn.type(right)->isa_oopptr();\n+  Node* cmp = CmpP(left, right);\n+  cmp = optimize_cmp_with_klass(cmp);\n+  if (tleft == nullptr || !tleft->can_be_inline_type() ||\n+      tright == nullptr || !tright->can_be_inline_type()) {\n+    \/\/ This is sufficient, if one of the operands can't be an inline type\n+    do_if(btest, cmp);\n+    return;\n+  }\n+\n+  \/\/ Don't add traps to unstable if branches because additional checks are required to\n+  \/\/ decide if the operands are equal\/substitutable and we therefore shouldn't prune\n+  \/\/ branches for one if based on the profiling of the acmp branches.\n+  \/\/ Also, OptimizeUnstableIf would set an incorrect re-rexecution state because it\n+  \/\/ assumes that there is a 1-1 mapping between the if and the acmp branches and that\n+  \/\/ hitting a trap means that we will take the corresponding acmp branch on re-execution.\n+  const bool can_trap = true;\n+\n+  Node* eq_region = nullptr;\n+  if (btest == BoolTest::eq) {\n+    do_if(btest, cmp, !can_trap, true);\n+    if (stopped()) {\n+      \/\/ Pointers are equal, operands must be equal\n+      return;\n+    }\n+  } else {\n+    assert(btest == BoolTest::ne, \"only eq or ne\");\n+    Node* is_not_equal = nullptr;\n+    eq_region = new RegionNode(3);\n+    {\n+      PreserveJVMState pjvms(this);\n+      \/\/ Pointers are not equal, but more checks are needed to determine if the operands are (not) substitutable\n+      do_if(btest, cmp, !can_trap, false, &is_not_equal);\n+      if (!stopped()) {\n+        eq_region->init_req(1, control());\n+      }\n+    }\n+    if (is_not_equal == nullptr || is_not_equal->is_top()) {\n+      record_for_igvn(eq_region);\n+      set_control(_gvn.transform(eq_region));\n+      return;\n+    }\n+    set_control(is_not_equal);\n+  }\n+\n+  \/\/ Prefer speculative types if available\n+  if (!too_many_traps_or_recompiles(Deoptimization::Reason_speculate_class_check)) {\n+    if (tleft->speculative_type() != nullptr) {\n+      left_type = tleft->speculative_type();\n+    }\n+    if (tright->speculative_type() != nullptr) {\n+      right_type = tright->speculative_type();\n+    }\n+  }\n+\n+  if (speculative_ptr_kind(tleft) != ProfileMaybeNull && speculative_ptr_kind(tleft) != ProfileUnknownNull) {\n+    ProfilePtrKind speculative_left_ptr = speculative_ptr_kind(tleft);\n+    if (speculative_left_ptr == ProfileAlwaysNull && !too_many_traps_or_recompiles(Deoptimization::Reason_speculate_null_assert)) {\n+      left_ptr = speculative_left_ptr;\n+    } else if (speculative_left_ptr == ProfileNeverNull && !too_many_traps_or_recompiles(Deoptimization::Reason_speculate_null_check)) {\n+      left_ptr = speculative_left_ptr;\n+    }\n+  }\n+  if (speculative_ptr_kind(tright) != ProfileMaybeNull && speculative_ptr_kind(tright) != ProfileUnknownNull) {\n+    ProfilePtrKind speculative_right_ptr = speculative_ptr_kind(tright);\n+    if (speculative_right_ptr == ProfileAlwaysNull && !too_many_traps_or_recompiles(Deoptimization::Reason_speculate_null_assert)) {\n+      right_ptr = speculative_right_ptr;\n+    } else if (speculative_right_ptr == ProfileNeverNull && !too_many_traps_or_recompiles(Deoptimization::Reason_speculate_null_check)) {\n+      right_ptr = speculative_right_ptr;\n+    }\n+  }\n+\n+  if (left_ptr == ProfileAlwaysNull) {\n+    \/\/ Comparison with null. Assert the input is indeed null and we're done.\n+    acmp_always_null_input(left, tleft, btest, eq_region);\n+    return;\n+  }\n+  if (right_ptr == ProfileAlwaysNull) {\n+    \/\/ Comparison with null. Assert the input is indeed null and we're done.\n+    acmp_always_null_input(right, tright, btest, eq_region);\n+    return;\n+  }\n+  if (left_type != nullptr && !left_type->is_inlinetype()) {\n+    \/\/ Comparison with an object of known type\n+    acmp_known_non_inline_type_input(left, tleft, left_ptr, left_type, btest, eq_region);\n+    return;\n+  }\n+  if (right_type != nullptr && !right_type->is_inlinetype()) {\n+    \/\/ Comparison with an object of known type\n+    acmp_known_non_inline_type_input(right, tright, right_ptr, right_type, btest, eq_region);\n+    return;\n+  }\n+  if (!left_inline_type) {\n+    \/\/ Comparison with an object known not to be an inline type\n+    acmp_unknown_non_inline_type_input(left, tleft, left_ptr, btest, eq_region);\n+    return;\n+  }\n+  if (!right_inline_type) {\n+    \/\/ Comparison with an object known not to be an inline type\n+    acmp_unknown_non_inline_type_input(right, tright, right_ptr, btest, eq_region);\n+    return;\n+  }\n+\n+  \/\/ Pointers are not equal, check if first operand is non-null\n+  Node* ne_region = new RegionNode(6);\n+  Node* null_ctl;\n+  Node* not_null_right = acmp_null_check(right, tright, right_ptr, null_ctl);\n+  ne_region->init_req(1, null_ctl);\n+\n+  \/\/ First operand is non-null, check if it is an inline type\n+  Node* is_value = inline_type_test(not_null_right);\n+  IfNode* is_value_iff = create_and_map_if(control(), is_value, PROB_FAIR, COUNT_UNKNOWN);\n+  Node* not_value = _gvn.transform(new IfFalseNode(is_value_iff));\n+  ne_region->init_req(2, not_value);\n+  set_control(_gvn.transform(new IfTrueNode(is_value_iff)));\n+\n+  \/\/ The first operand is an inline type, check if the second operand is non-null\n+  Node* not_null_left = acmp_null_check(left, tleft, left_ptr, null_ctl);\n+  ne_region->init_req(3, null_ctl);\n+\n+  \/\/ Check if both operands are of the same class.\n+  Node* kls_left = load_object_klass(not_null_left);\n+  Node* kls_right = load_object_klass(not_null_right);\n+  Node* kls_cmp = CmpP(kls_left, kls_right);\n+  Node* kls_bol = _gvn.transform(new BoolNode(kls_cmp, BoolTest::ne));\n+  IfNode* kls_iff = create_and_map_if(control(), kls_bol, PROB_FAIR, COUNT_UNKNOWN);\n+  Node* kls_ne = _gvn.transform(new IfTrueNode(kls_iff));\n+  set_control(_gvn.transform(new IfFalseNode(kls_iff)));\n+  ne_region->init_req(4, kls_ne);\n+\n+  if (stopped()) {\n+    record_for_igvn(ne_region);\n+    set_control(_gvn.transform(ne_region));\n+    if (btest == BoolTest::ne) {\n+      {\n+        PreserveJVMState pjvms(this);\n+        int target_bci = iter().get_dest();\n+        merge(target_bci);\n+      }\n+      record_for_igvn(eq_region);\n+      set_control(_gvn.transform(eq_region));\n+    }\n+    return;\n+  }\n+\n+  \/\/ Both operands are values types of the same class, we need to perform a\n+  \/\/ substitutability test. Delegate to ValueObjectMethods::isSubstitutable().\n+  Node* ne_io_phi = PhiNode::make(ne_region, i_o());\n+  Node* mem = reset_memory();\n+  Node* ne_mem_phi = PhiNode::make(ne_region, mem);\n+\n+  Node* eq_io_phi = nullptr;\n+  Node* eq_mem_phi = nullptr;\n+  if (eq_region != nullptr) {\n+    eq_io_phi = PhiNode::make(eq_region, i_o());\n+    eq_mem_phi = PhiNode::make(eq_region, mem);\n+  }\n+\n+  set_all_memory(mem);\n+\n+  kill_dead_locals();\n+  ciMethod* subst_method = ciEnv::current()->ValueObjectMethods_klass()->find_method(ciSymbols::isSubstitutable_name(), ciSymbols::object_object_boolean_signature());\n+  CallStaticJavaNode *call = new CallStaticJavaNode(C, TypeFunc::make(subst_method), SharedRuntime::get_resolve_static_call_stub(), subst_method);\n+  call->set_override_symbolic_info(true);\n+  call->init_req(TypeFunc::Parms, not_null_left);\n+  call->init_req(TypeFunc::Parms+1, not_null_right);\n+  inc_sp(2);\n+  set_edges_for_java_call(call, false, false);\n+  Node* ret = set_results_for_java_call(call, false, true);\n+  dec_sp(2);\n+\n+  \/\/ Test the return value of ValueObjectMethods::isSubstitutable()\n+  \/\/ This is the last check, do_if can emit traps now.\n+  Node* subst_cmp = _gvn.transform(new CmpINode(ret, intcon(1)));\n+  Node* ctl = C->top();\n+  if (btest == BoolTest::eq) {\n+    PreserveJVMState pjvms(this);\n+    do_if(btest, subst_cmp, can_trap);\n+    if (!stopped()) {\n+      ctl = control();\n+    }\n+  } else {\n+    assert(btest == BoolTest::ne, \"only eq or ne\");\n+    PreserveJVMState pjvms(this);\n+    do_if(btest, subst_cmp, can_trap, false, &ctl);\n+    if (!stopped()) {\n+      eq_region->init_req(2, control());\n+      eq_io_phi->init_req(2, i_o());\n+      eq_mem_phi->init_req(2, reset_memory());\n+    }\n+  }\n+  ne_region->init_req(5, ctl);\n+  ne_io_phi->init_req(5, i_o());\n+  ne_mem_phi->init_req(5, reset_memory());\n+\n+  record_for_igvn(ne_region);\n+  set_control(_gvn.transform(ne_region));\n+  set_i_o(_gvn.transform(ne_io_phi));\n+  set_all_memory(_gvn.transform(ne_mem_phi));\n+\n+  if (btest == BoolTest::ne) {\n+    {\n+      PreserveJVMState pjvms(this);\n+      int target_bci = iter().get_dest();\n+      merge(target_bci);\n+    }\n+\n+    record_for_igvn(eq_region);\n+    set_control(_gvn.transform(eq_region));\n+    set_i_o(_gvn.transform(eq_io_phi));\n+    set_all_memory(_gvn.transform(eq_mem_phi));\n+  }\n+}\n+\n@@ -1641,1 +2441,1 @@\n-void Parse::adjust_map_after_if(BoolTest::mask btest, Node* c, float prob, Block* path) {\n+void Parse::adjust_map_after_if(BoolTest::mask btest, Node* c, float prob, Block* path, bool can_trap) {\n@@ -1653,1 +2453,1 @@\n-  if (path_is_suitable_for_uncommon_trap(prob)) {\n+  if (can_trap && path_is_suitable_for_uncommon_trap(prob)) {\n@@ -1750,0 +2550,3 @@\n+            if (tboth->is_inlinetypeptr()) {\n+              ccast = InlineTypeNode::make_from_oop(this, ccast, tboth->exact_klass(true)->as_inline_klass());\n+            }\n@@ -1854,0 +2657,4 @@\n+        if (obj->is_InlineType()) {\n+          assert(obj->as_InlineType()->is_allocated(&_gvn), \"must be allocated\");\n+          obj = obj->as_InlineType()->get_oop();\n+        }\n@@ -2651,14 +3458,20 @@\n-    if (!_gvn.type(b)->speculative_maybe_null() &&\n-        !too_many_traps(Deoptimization::Reason_speculate_null_check)) {\n-      inc_sp(1);\n-      Node* null_ctl = top();\n-      b = null_check_oop(b, &null_ctl, true, true, true);\n-      assert(null_ctl->is_top(), \"no null control here\");\n-      dec_sp(1);\n-    } else if (_gvn.type(b)->speculative_always_null() &&\n-               !too_many_traps(Deoptimization::Reason_speculate_null_assert)) {\n-      inc_sp(1);\n-      b = null_assert(b);\n-      dec_sp(1);\n-    }\n-    c = _gvn.transform( new CmpPNode(b, a) );\n+    if (b->is_InlineType()) {\n+      \/\/ Null checking a scalarized but nullable inline type. Check the IsInit\n+      \/\/ input instead of the oop input to avoid keeping buffer allocations alive\n+      c = _gvn.transform(new CmpINode(b->as_InlineType()->get_is_init(), zerocon(T_INT)));\n+    } else {\n+      if (!_gvn.type(b)->speculative_maybe_null() &&\n+          !too_many_traps(Deoptimization::Reason_speculate_null_check)) {\n+        inc_sp(1);\n+        Node* null_ctl = top();\n+        b = null_check_oop(b, &null_ctl, true, true, true);\n+        assert(null_ctl->is_top(), \"no null control here\");\n+        dec_sp(1);\n+      } else if (_gvn.type(b)->speculative_always_null() &&\n+                 !too_many_traps(Deoptimization::Reason_speculate_null_assert)) {\n+        inc_sp(1);\n+        b = null_assert(b);\n+        dec_sp(1);\n+      }\n+      c = _gvn.transform( new CmpPNode(b, a) );\n+    }\n@@ -2675,3 +3488,1 @@\n-    c = _gvn.transform( new CmpPNode(b, a) );\n-    c = optimize_cmp_with_klass(c);\n-    do_if(btest, c);\n+    do_acmp(btest, b, a);\n@@ -2732,1 +3543,1 @@\n-    do_anewarray();\n+    do_newarray();\n","filename":"src\/hotspot\/share\/opto\/parse2.cpp","additions":915,"deletions":104,"binary":false,"changes":1019,"status":"modified"},{"patch":"@@ -47,0 +47,2 @@\n+#include \"oops\/flatArrayKlass.hpp\"\n+#include \"oops\/flatArrayOop.inline.hpp\"\n@@ -199,0 +201,1 @@\n+const TypeFunc* OptoRuntime::_new_array_nozero_Type               = nullptr;\n@@ -324,1 +327,1 @@\n-JRT_BLOCK_ENTRY(void, OptoRuntime::new_instance_C(Klass* klass, JavaThread* current))\n+JRT_BLOCK_ENTRY(void, OptoRuntime::new_instance_C(Klass* klass, bool is_larval, JavaThread* current))\n@@ -344,1 +347,5 @@\n-    oop result = InstanceKlass::cast(klass)->allocate_instance(THREAD);\n+    instanceOop result = InstanceKlass::cast(klass)->allocate_instance(THREAD);\n+    if (is_larval) {\n+      \/\/ Check if this is a larval buffer allocation\n+      result->set_mark(result->mark().enter_larval_state());\n+    }\n@@ -362,1 +369,1 @@\n-JRT_BLOCK_ENTRY(void, OptoRuntime::new_array_C(Klass* array_type, int len, JavaThread* current))\n+JRT_BLOCK_ENTRY(void, OptoRuntime::new_array_C(Klass* array_type, int len, oopDesc* init_val, JavaThread* current))\n@@ -371,0 +378,1 @@\n+  Handle h_init_val(current, init_val); \/\/ keep the init_val object alive\n@@ -372,1 +380,12 @@\n-  if (array_type->is_typeArray_klass()) {\n+  if (array_type->is_flatArray_klass()) {\n+    Handle holder(current, array_type->klass_holder()); \/\/ keep the array klass alive\n+    FlatArrayKlass* fak = FlatArrayKlass::cast(array_type);\n+    InlineKlass* vk = fak->element_klass();\n+    result = oopFactory::new_flatArray(vk, len, fak->layout_kind(), THREAD);\n+    if (array_type->is_null_free_array_klass() && !h_init_val.is_null()) {\n+      \/\/ Null-free arrays need to be initialized\n+      for (int i = 0; i < len; i++) {\n+        vk->write_value_to_addr(h_init_val(), ((flatArrayOop)result)->value_at_addr(i, fak->layout_helper()), fak->layout_kind(), true, CHECK);\n+      }\n+    }\n+  } else if (array_type->is_typeArray_klass()) {\n@@ -378,5 +397,8 @@\n-    \/\/ Although the oopFactory likes to work with the elem_type,\n-    \/\/ the compiler prefers the array_type, since it must already have\n-    \/\/ that latter value in hand for the fast path.\n-    Klass* elem_type = ObjArrayKlass::cast(array_type)->element_klass();\n-    result = oopFactory::new_objArray(elem_type, len, THREAD);\n+    ObjArrayKlass* array_klass = ObjArrayKlass::cast(array_type);\n+    result = array_klass->allocate(len, THREAD);\n+    if (array_type->is_null_free_array_klass() && !h_init_val.is_null()) {\n+      \/\/ Null-free arrays need to be initialized\n+      for (int i = 0; i < len; i++) {\n+        ((objArrayOop)result)->obj_at_put(i, h_init_val());\n+      }\n+    }\n@@ -580,1 +602,1 @@\n-  const Type **fields = TypeTuple::fields(1);\n+  const Type **fields = TypeTuple::fields(2);\n@@ -582,1 +604,2 @@\n-  const TypeTuple *domain = TypeTuple::make(TypeFunc::Parms+1, fields);\n+  fields[TypeFunc::Parms+1] = TypeInt::BOOL;        \/\/ is_larval\n+  const TypeTuple *domain = TypeTuple::make(TypeFunc::Parms+2, fields);\n@@ -625,0 +648,17 @@\n+  \/\/ create input type (domain)\n+  const Type **fields = TypeTuple::fields(3);\n+  fields[TypeFunc::Parms+0] = TypeInstPtr::NOTNULL;   \/\/ element klass\n+  fields[TypeFunc::Parms+1] = TypeInt::INT;       \/\/ array size\n+  fields[TypeFunc::Parms+2] = TypeInstPtr::NOTNULL;       \/\/ init value\n+  const TypeTuple *domain = TypeTuple::make(TypeFunc::Parms+3, fields);\n+\n+  \/\/ create result type (range)\n+  fields = TypeTuple::fields(1);\n+  fields[TypeFunc::Parms+0] = TypeRawPtr::NOTNULL; \/\/ Returned oop\n+\n+  const TypeTuple *range = TypeTuple::make(TypeFunc::Parms+1, fields);\n+\n+  return TypeFunc::make(domain, range);\n+}\n+\n+static const TypeFunc* make_new_array_nozero_Type() {\n@@ -700,1 +740,1 @@\n-  return TypeFunc::make(domain,range);\n+  return TypeFunc::make(domain, range);\n@@ -1938,1 +1978,1 @@\n-  return TypeFunc::make(domain,range);\n+  return TypeFunc::make(domain, range);\n@@ -1970,1 +2010,1 @@\n-  return TypeFunc::make(domain,range);\n+  return TypeFunc::make(domain, range);\n@@ -1986,1 +2026,1 @@\n-  return TypeFunc::make(domain,range);\n+  return TypeFunc::make(domain, range);\n@@ -2077,0 +2117,1 @@\n+  _new_array_nozero_Type              = make_new_array_nozero_Type();\n@@ -2172,0 +2213,105 @@\n+\n+const TypeFunc *OptoRuntime::store_inline_type_fields_Type() {\n+  \/\/ create input type (domain)\n+  uint total = SharedRuntime::java_return_convention_max_int + SharedRuntime::java_return_convention_max_float*2;\n+  const Type **fields = TypeTuple::fields(total);\n+  \/\/ We don't know the number of returned values and their\n+  \/\/ types. Assume all registers available to the return convention\n+  \/\/ are used.\n+  fields[TypeFunc::Parms] = TypePtr::BOTTOM;\n+  uint i = 1;\n+  for (; i < SharedRuntime::java_return_convention_max_int; i++) {\n+    fields[TypeFunc::Parms+i] = TypeInt::INT;\n+  }\n+  for (; i < total; i+=2) {\n+    fields[TypeFunc::Parms+i] = Type::DOUBLE;\n+    fields[TypeFunc::Parms+i+1] = Type::HALF;\n+  }\n+  const TypeTuple* domain = TypeTuple::make(TypeFunc::Parms + total, fields);\n+\n+  \/\/ create result type (range)\n+  fields = TypeTuple::fields(1);\n+  fields[TypeFunc::Parms+0] = TypeInstPtr::NOTNULL;\n+\n+  const TypeTuple *range = TypeTuple::make(TypeFunc::Parms+1,fields);\n+\n+  return TypeFunc::make(domain, range);\n+}\n+\n+const TypeFunc *OptoRuntime::pack_inline_type_Type() {\n+  \/\/ create input type (domain)\n+  uint total = 1 + SharedRuntime::java_return_convention_max_int + SharedRuntime::java_return_convention_max_float*2;\n+  const Type **fields = TypeTuple::fields(total);\n+  \/\/ We don't know the number of returned values and their\n+  \/\/ types. Assume all registers available to the return convention\n+  \/\/ are used.\n+  fields[TypeFunc::Parms] = TypeRawPtr::BOTTOM;\n+  fields[TypeFunc::Parms+1] = TypeRawPtr::BOTTOM;\n+  uint i = 2;\n+  for (; i < SharedRuntime::java_return_convention_max_int+1; i++) {\n+    fields[TypeFunc::Parms+i] = TypeInt::INT;\n+  }\n+  for (; i < total; i+=2) {\n+    fields[TypeFunc::Parms+i] = Type::DOUBLE;\n+    fields[TypeFunc::Parms+i+1] = Type::HALF;\n+  }\n+  const TypeTuple* domain = TypeTuple::make(TypeFunc::Parms + total, fields);\n+\n+  \/\/ create result type (range)\n+  fields = TypeTuple::fields(1);\n+  fields[TypeFunc::Parms+0] = TypeInstPtr::NOTNULL;\n+\n+  const TypeTuple *range = TypeTuple::make(TypeFunc::Parms+1,fields);\n+\n+  return TypeFunc::make(domain, range);\n+}\n+\n+JRT_BLOCK_ENTRY(void, OptoRuntime::load_unknown_inline_C(flatArrayOopDesc* array, int index, JavaThread* current))\n+  JRT_BLOCK;\n+  oop buffer = array->read_value_from_flat_array(index, THREAD);\n+  deoptimize_caller_frame(current, HAS_PENDING_EXCEPTION);\n+  current->set_vm_result(buffer);\n+  JRT_BLOCK_END;\n+JRT_END\n+\n+const TypeFunc* OptoRuntime::load_unknown_inline_Type() {\n+  \/\/ create input type (domain)\n+  const Type** fields = TypeTuple::fields(2);\n+  fields[TypeFunc::Parms] = TypeOopPtr::NOTNULL;\n+  fields[TypeFunc::Parms+1] = TypeInt::POS;\n+\n+  const TypeTuple* domain = TypeTuple::make(TypeFunc::Parms+2, fields);\n+\n+  \/\/ create result type (range)\n+  fields = TypeTuple::fields(1);\n+  fields[TypeFunc::Parms] = TypeInstPtr::NOTNULL;\n+\n+  const TypeTuple* range = TypeTuple::make(TypeFunc::Parms+1, fields);\n+\n+  return TypeFunc::make(domain, range);\n+}\n+\n+JRT_BLOCK_ENTRY(void, OptoRuntime::store_unknown_inline_C(instanceOopDesc* buffer, flatArrayOopDesc* array, int index, JavaThread* current))\n+  JRT_BLOCK;\n+  array->write_value_to_flat_array(buffer, index, THREAD);\n+  if (HAS_PENDING_EXCEPTION) {\n+      fatal(\"This entry must be changed to be a non-leaf entry because writing to a flat array can now throw an exception\");\n+  }\n+  JRT_BLOCK_END;\n+JRT_END\n+\n+const TypeFunc* OptoRuntime::store_unknown_inline_Type() {\n+  \/\/ create input type (domain)\n+  const Type** fields = TypeTuple::fields(3);\n+  fields[TypeFunc::Parms] = TypeInstPtr::NOTNULL;\n+  fields[TypeFunc::Parms+1] = TypeOopPtr::NOTNULL;\n+  fields[TypeFunc::Parms+2] = TypeInt::POS;\n+\n+  const TypeTuple* domain = TypeTuple::make(TypeFunc::Parms+3, fields);\n+\n+  \/\/ create result type (range)\n+  fields = TypeTuple::fields(0);\n+  const TypeTuple* range = TypeTuple::make(TypeFunc::Parms, fields);\n+\n+  return TypeFunc::make(domain, range);\n+}\n","filename":"src\/hotspot\/share\/opto\/runtime.cpp","additions":161,"deletions":15,"binary":false,"changes":176,"status":"modified"},{"patch":"@@ -32,0 +32,1 @@\n+#include \"opto\/inlinetypenode.hpp\"\n@@ -917,1 +918,8 @@\n-Node *CmpLNode::Ideal( PhaseGVN *phase, bool can_reshape ) {\n+\/\/------------------------------Ideal------------------------------------------\n+Node* CmpLNode::Ideal(PhaseGVN* phase, bool can_reshape) {\n+  Node* a = nullptr;\n+  Node* b = nullptr;\n+  if (is_double_null_check(phase, a, b) && (phase->type(a)->is_zero_type() || phase->type(b)->is_zero_type())) {\n+    \/\/ Degraded to a simple null check, use old acmp\n+    return new CmpPNode(a, b);\n+  }\n@@ -928,0 +936,25 @@\n+\/\/ Match double null check emitted by Compile::optimize_acmp()\n+bool CmpLNode::is_double_null_check(PhaseGVN* phase, Node*& a, Node*& b) const {\n+  if (in(1)->Opcode() == Op_OrL &&\n+      in(1)->in(1)->Opcode() == Op_CastP2X &&\n+      in(1)->in(2)->Opcode() == Op_CastP2X &&\n+      in(2)->bottom_type()->is_zero_type()) {\n+    assert(EnableValhalla, \"unexpected double null check\");\n+    a = in(1)->in(1)->in(1);\n+    b = in(1)->in(2)->in(1);\n+    return true;\n+  }\n+  return false;\n+}\n+\n+\/\/------------------------------Value------------------------------------------\n+const Type* CmpLNode::Value(PhaseGVN* phase) const {\n+  Node* a = nullptr;\n+  Node* b = nullptr;\n+  if (is_double_null_check(phase, a, b) && (!phase->type(a)->maybe_null() || !phase->type(b)->maybe_null())) {\n+    \/\/ One operand is never nullptr, emit constant false\n+    return TypeInt::CC_GT;\n+  }\n+  return SubNode::Value(phase);\n+}\n+\n@@ -1053,1 +1086,16 @@\n-\n+    if (!unrelated_classes) {\n+      \/\/ Handle inline type arrays\n+      if ((r0->flat_in_array() && r1->not_flat_in_array()) ||\n+          (r1->flat_in_array() && r0->not_flat_in_array())) {\n+        \/\/ One type is in flat arrays but the other type is not. Must be unrelated.\n+        unrelated_classes = true;\n+      } else if ((r0->is_not_flat() && r1->is_flat()) ||\n+                 (r1->is_not_flat() && r0->is_flat())) {\n+        \/\/ One type is a non-flat array and the other type is a flat array. Must be unrelated.\n+        unrelated_classes = true;\n+      } else if ((r0->is_not_null_free() && r1->is_null_free()) ||\n+                 (r1->is_not_null_free() && r0->is_null_free())) {\n+        \/\/ One type is a nullable array and the other type is a null-free array. Must be unrelated.\n+        unrelated_classes = true;\n+      }\n+    }\n@@ -1138,1 +1186,8 @@\n-Node *CmpPNode::Ideal( PhaseGVN *phase, bool can_reshape ) {\n+Node* CmpPNode::Ideal(PhaseGVN *phase, bool can_reshape) {\n+  \/\/ TODO 8284443 in(1) could be cast?\n+  if (in(1)->is_InlineType() && phase->type(in(2))->is_zero_type()) {\n+    \/\/ Null checking a scalarized but nullable inline type. Check the IsInit\n+    \/\/ input instead of the oop input to avoid keeping buffer allocations alive.\n+    return new CmpINode(in(1)->as_InlineType()->get_is_init(), phase->intcon(0));\n+  }\n+\n@@ -1210,0 +1265,7 @@\n+  \/\/ Do not fold the subtype check to an array klass pointer comparison for null-able inline type arrays\n+  \/\/ because null-free [LMyValue <: null-able [LMyValue but the klasses are different. Perform a full test.\n+  if (superklass->is_obj_array_klass() && !superklass->as_array_klass()->is_elem_null_free() &&\n+      superklass->as_array_klass()->element_klass()->is_inlinetype()) {\n+    return nullptr;\n+  }\n+\n@@ -1353,0 +1415,37 @@\n+\/\/=============================================================================\n+\/\/------------------------------Value------------------------------------------\n+const Type* FlatArrayCheckNode::Value(PhaseGVN* phase) const {\n+  bool all_not_flat = true;\n+  for (uint i = ArrayOrKlass; i < req(); ++i) {\n+    const Type* t = phase->type(in(i));\n+    if (t == Type::TOP) {\n+      return Type::TOP;\n+    }\n+    if (t->is_ptr()->is_flat()) {\n+      \/\/ One of the input arrays is flat, check always passes\n+      return TypeInt::CC_EQ;\n+    } else if (!t->is_ptr()->is_not_flat()) {\n+      \/\/ One of the input arrays might be flat\n+      all_not_flat = false;\n+    }\n+  }\n+  if (all_not_flat) {\n+    \/\/ None of the input arrays can be flat, check always fails\n+    return TypeInt::CC_GT;\n+  }\n+  return TypeInt::CC;\n+}\n+\n+\/\/------------------------------Ideal------------------------------------------\n+Node* FlatArrayCheckNode::Ideal(PhaseGVN* phase, bool can_reshape) {\n+  bool changed = false;\n+  \/\/ Remove inputs that are known to be non-flat\n+  for (uint i = ArrayOrKlass; i < req(); ++i) {\n+    const Type* t = phase->type(in(i));\n+    if (t->isa_ptr() && t->is_ptr()->is_not_flat()) {\n+      del_req(i--);\n+      changed = true;\n+    }\n+  }\n+  return changed ? this : nullptr;\n+}\n","filename":"src\/hotspot\/share\/opto\/subnode.cpp","additions":102,"deletions":3,"binary":false,"changes":105,"status":"modified"},{"patch":"@@ -220,1 +220,2 @@\n-  virtual Node *Ideal(PhaseGVN *phase, bool can_reshape);\n+  virtual Node* Ideal(PhaseGVN* phase, bool can_reshape);\n+  virtual const Type* Value(PhaseGVN* phase) const;\n@@ -222,0 +223,1 @@\n+  bool is_double_null_check(PhaseGVN* phase, Node*& a, Node*& b) const;\n@@ -312,0 +314,20 @@\n+\/\/--------------------------FlatArrayCheckNode---------------------------------\n+\/\/ Returns true if one of the input array objects or array klass ptrs (there\n+\/\/ can be multiple) is flat.\n+class FlatArrayCheckNode : public CmpNode {\n+public:\n+  enum {\n+    Control,\n+    Memory,\n+    ArrayOrKlass\n+  };\n+  FlatArrayCheckNode(Compile* C, Node* mem, Node* array_or_klass) : CmpNode(mem, array_or_klass) {\n+    init_class_id(Class_FlatArrayCheck);\n+    init_flags(Flag_is_macro);\n+    C->add_macro_node(this);\n+  }\n+  virtual int Opcode() const;\n+  virtual const Type* sub(const Type*, const Type*) const { ShouldNotReachHere(); return nullptr; }\n+  const Type* Value(PhaseGVN* phase) const;\n+  virtual Node* Ideal(PhaseGVN* phase, bool can_reshape);\n+};\n","filename":"src\/hotspot\/share\/opto\/subnode.hpp","additions":23,"deletions":1,"binary":false,"changes":24,"status":"modified"},{"patch":"@@ -56,0 +56,2 @@\n+#include \"oops\/flatArrayOop.inline.hpp\"\n+#include \"oops\/inlineKlass.inline.hpp\"\n@@ -423,0 +425,1 @@\n+  bool is_flat = InstanceKlass::cast(k1)->field_is_flat(slot);\n@@ -424,1 +427,1 @@\n-  ret = jfieldIDWorkaround::to_instance_jfieldID(k1, offset);\n+  ret = jfieldIDWorkaround::to_instance_jfieldID(k1, offset, is_flat);\n@@ -441,1 +444,1 @@\n-  if (m->is_object_initializer()) {\n+  if (m->is_object_constructor()) {\n@@ -800,1 +803,1 @@\n-    case T_OBJECT:      push_object(va_arg(_ap, jobject)); break;\n+    case T_OBJECT: push_object(va_arg(_ap, jobject)); break;\n@@ -840,1 +843,2 @@\n-    case T_OBJECT:      push_object((_ap++)->l); break;\n+    case T_OBJECT:\n+    case T_FLAT_ELEMENT: push_object((_ap++)->l); break;\n@@ -967,1 +971,7 @@\n-  instanceOop i = InstanceKlass::allocate_instance(JNIHandles::resolve_non_null(clazz), CHECK_NULL);\n+  oop clazzoop = JNIHandles::resolve_non_null(clazz);\n+  Klass* k = java_lang_Class::as_Klass(clazzoop);\n+  if (k == nullptr || k->is_inline_klass()) {\n+    ResourceMark rm(THREAD);\n+    THROW_(vmSymbols::java_lang_InstantiationException(), nullptr);\n+  }\n+  instanceOop i = InstanceKlass::allocate_instance(clazzoop, CHECK_NULL);\n@@ -981,1 +991,8 @@\n-  instanceOop i = InstanceKlass::allocate_instance(JNIHandles::resolve_non_null(clazz), CHECK_NULL);\n+  oop clazzoop = JNIHandles::resolve_non_null(clazz);\n+  Klass* k = java_lang_Class::as_Klass(clazzoop);\n+  if (k == nullptr) {\n+    ResourceMark rm(THREAD);\n+    THROW_(vmSymbols::java_lang_InstantiationException(), nullptr);\n+  }\n+\n+  instanceOop i = InstanceKlass::allocate_instance(clazzoop, CHECK_NULL);\n@@ -986,0 +1003,1 @@\n+\n@@ -987,1 +1005,1 @@\n-JNI_END\n+  JNI_END\n@@ -999,1 +1017,8 @@\n-  instanceOop i = InstanceKlass::allocate_instance(JNIHandles::resolve_non_null(clazz), CHECK_NULL);\n+  oop clazzoop = JNIHandles::resolve_non_null(clazz);\n+  Klass* k = java_lang_Class::as_Klass(clazzoop);\n+  if (k == nullptr) {\n+    ResourceMark rm(THREAD);\n+    THROW_(vmSymbols::java_lang_InstantiationException(), nullptr);\n+  }\n+\n+  instanceOop i = InstanceKlass::allocate_instance(clazzoop, CHECK_NULL);\n@@ -1004,0 +1029,1 @@\n+\n@@ -1017,1 +1043,8 @@\n-  instanceOop i = InstanceKlass::allocate_instance(JNIHandles::resolve_non_null(clazz), CHECK_NULL);\n+  oop clazzoop = JNIHandles::resolve_non_null(clazz);\n+  Klass* k = java_lang_Class::as_Klass(clazzoop);\n+  if (k == nullptr) {\n+    ResourceMark rm(THREAD);\n+    THROW_(vmSymbols::java_lang_InstantiationException(), nullptr);\n+  }\n+\n+  instanceOop i = InstanceKlass::allocate_instance(clazzoop, CHECK_NULL);\n@@ -1025,0 +1058,1 @@\n+\n@@ -1775,1 +1809,1 @@\n-  ret = jfieldIDWorkaround::to_instance_jfieldID(k, fd.offset());\n+  ret = jfieldIDWorkaround::to_instance_jfieldID(k, fd.offset(), fd.is_flat());\n@@ -1785,0 +1819,1 @@\n+  oop res = nullptr;\n@@ -1790,2 +1825,15 @@\n-  oop loaded_obj = HeapAccess<ON_UNKNOWN_OOP_REF>::oop_load_at(o, offset);\n-  jobject ret = JNIHandles::make_local(THREAD, loaded_obj);\n+  if (!jfieldIDWorkaround::is_flat_jfieldID(fieldID)) {\n+    res = HeapAccess<ON_UNKNOWN_OOP_REF>::oop_load_at(o, offset);\n+  } else {\n+    assert(k->is_instance_klass(), \"Only instance can have flat fields\");\n+    InstanceKlass* ik = InstanceKlass::cast(k);\n+    fieldDescriptor fd;\n+    bool found = ik->find_field_from_offset(offset, false, &fd);  \/\/ performance bottleneck\n+    assert(found, \"Field not found\");\n+    InstanceKlass* holder = fd.field_holder();\n+    assert(holder->field_is_flat(fd.index()), \"Must be\");\n+    InlineLayoutInfo* li = holder->inline_layout_info_adr(fd.index());\n+    InlineKlass* field_vklass = li->klass();\n+    res = field_vklass->read_payload_from_addr(o, ik->field_offset(fd.index()), li->kind(), CHECK_NULL);\n+  }\n+  jobject ret = JNIHandles::make_local(THREAD, res);\n@@ -1796,2 +1844,0 @@\n-\n-\n@@ -1883,1 +1929,22 @@\n-  HeapAccess<ON_UNKNOWN_OOP_REF>::oop_store_at(o, offset, JNIHandles::resolve(value));\n+  if (!jfieldIDWorkaround::is_flat_jfieldID(fieldID)) {\n+    oop v = JNIHandles::resolve(value);\n+    if (v == nullptr) {\n+      InstanceKlass *ik = InstanceKlass::cast(k);\n+      fieldDescriptor fd;\n+      ik->find_field_from_offset(offset, false, &fd);\n+      if (fd.is_null_free_inline_type()) {\n+        THROW_MSG(vmSymbols::java_lang_NullPointerException(), \"Cannot store null in a null-restricted field\");\n+      }\n+    }\n+    HeapAccess<ON_UNKNOWN_OOP_REF>::oop_store_at(o, offset, v);\n+  } else {\n+    assert(k->is_instance_klass(), \"Only instances can have flat fields\");\n+    InstanceKlass* ik = InstanceKlass::cast(k);\n+    fieldDescriptor fd;\n+    ik->find_field_from_offset(offset, false, &fd);\n+    InstanceKlass* holder = fd.field_holder();\n+    InlineLayoutInfo* li = holder->inline_layout_info_adr(fd.index());\n+    InlineKlass* vklass = li->klass();\n+    oop v = JNIHandles::resolve(value);\n+    vklass->write_value_to_addr(v, ((char*)(oopDesc*)o) + offset, li->kind(), true, CHECK);\n+  }\n@@ -2308,4 +2375,12 @@\n-  objArrayOop a = objArrayOop(JNIHandles::resolve_non_null(array));\n-  if (a->is_within_bounds(index)) {\n-    ret = JNIHandles::make_local(THREAD, a->obj_at(index));\n-    return ret;\n+  oop res = nullptr;\n+  arrayOop arr((arrayOop)JNIHandles::resolve_non_null(array));\n+  if (arr->is_within_bounds(index)) {\n+    if (arr->is_flatArray()) {\n+      flatArrayOop a = flatArrayOop(JNIHandles::resolve_non_null(array));\n+      res = a->read_value_from_flat_array(index, CHECK_NULL);\n+      assert(res != nullptr || !arr->is_null_free_array(), \"Invalid value\");\n+    } else {\n+      assert(arr->is_objArray(), \"If not a valueArray. must be an objArray\");\n+      objArrayOop a = objArrayOop(JNIHandles::resolve_non_null(array));\n+      res = a->obj_at(index);\n+    }\n@@ -2315,1 +2390,1 @@\n-    ss.print(\"Index %d out of bounds for length %d\", index, a->length());\n+    ss.print(\"Index %d out of bounds for length %d\", index,arr->length());\n@@ -2318,0 +2393,2 @@\n+  ret = JNIHandles::make_local(THREAD, res);\n+  return ret;\n@@ -2327,24 +2404,40 @@\n-  objArrayOop a = objArrayOop(JNIHandles::resolve_non_null(array));\n-  oop v = JNIHandles::resolve(value);\n-  if (a->is_within_bounds(index)) {\n-    if (v == nullptr || v->is_a(ObjArrayKlass::cast(a->klass())->element_klass())) {\n-      a->obj_at_put(index, v);\n-    } else {\n-      ResourceMark rm(THREAD);\n-      stringStream ss;\n-      Klass *bottom_kl = ObjArrayKlass::cast(a->klass())->bottom_klass();\n-      ss.print(\"type mismatch: can not store %s to %s[%d]\",\n-               v->klass()->external_name(),\n-               bottom_kl->is_typeArray_klass() ? type2name_tab[ArrayKlass::cast(bottom_kl)->element_type()] : bottom_kl->external_name(),\n-               index);\n-      for (int dims = ArrayKlass::cast(a->klass())->dimension(); dims > 1; --dims) {\n-        ss.print(\"[]\");\n-      }\n-      THROW_MSG(vmSymbols::java_lang_ArrayStoreException(), ss.as_string());\n-    }\n-  } else {\n-    ResourceMark rm(THREAD);\n-    stringStream ss;\n-    ss.print(\"Index %d out of bounds for length %d\", index, a->length());\n-    THROW_MSG(vmSymbols::java_lang_ArrayIndexOutOfBoundsException(), ss.as_string());\n-  }\n+   bool oob = false;\n+   int length = -1;\n+   oop res = nullptr;\n+   arrayOop arr((arrayOop)JNIHandles::resolve_non_null(array));\n+   if (arr->is_within_bounds(index)) {\n+     if (arr->is_flatArray()) {\n+       flatArrayOop a = flatArrayOop(JNIHandles::resolve_non_null(array));\n+       oop v = JNIHandles::resolve(value);\n+       FlatArrayKlass* vaklass = FlatArrayKlass::cast(a->klass());\n+       InlineKlass* element_vklass = vaklass->element_klass();\n+       a->write_value_to_flat_array(v, index, CHECK);\n+     } else {\n+       assert(arr->is_objArray(), \"If not a valueArray. must be an objArray\");\n+       objArrayOop a = objArrayOop(JNIHandles::resolve_non_null(array));\n+       oop v = JNIHandles::resolve(value);\n+       if (v == nullptr || v->is_a(ObjArrayKlass::cast(a->klass())->element_klass())) {\n+         if (v == nullptr && ObjArrayKlass::cast(a->klass())->is_null_free_array_klass()) {\n+           THROW_MSG(vmSymbols::java_lang_NullPointerException(), \"Cannot store null in a null-restricted array\");\n+         }\n+         a->obj_at_put(index, v);\n+       } else {\n+         ResourceMark rm(THREAD);\n+         stringStream ss;\n+         Klass *bottom_kl = ObjArrayKlass::cast(a->klass())->bottom_klass();\n+         ss.print(\"type mismatch: can not store %s to %s[%d]\",\n+             v->klass()->external_name(),\n+             bottom_kl->is_typeArray_klass() ? type2name_tab[ArrayKlass::cast(bottom_kl)->element_type()] : bottom_kl->external_name(),\n+                 index);\n+         for (int dims = ArrayKlass::cast(a->klass())->dimension(); dims > 1; --dims) {\n+           ss.print(\"[]\");\n+         }\n+         THROW_MSG(vmSymbols::java_lang_ArrayStoreException(), ss.as_string());\n+       }\n+     }\n+   } else {\n+     ResourceMark rm(THREAD);\n+     stringStream ss;\n+     ss.print(\"Index %d out of bounds for length %d\", index, arr->length());\n+     THROW_MSG(vmSymbols::java_lang_ArrayIndexOutOfBoundsException(), ss.as_string());\n+   }\n@@ -2726,1 +2819,1 @@\n-  ObjectSynchronizer::jni_enter(obj, thread);\n+  ObjectSynchronizer::jni_enter(obj, CHECK_(JNI_ERR));\n","filename":"src\/hotspot\/share\/prims\/jni.cpp","additions":138,"deletions":45,"binary":false,"changes":183,"status":"modified"},{"patch":"@@ -55,0 +55,1 @@\n+#include \"memory\/iterator.inline.hpp\"\n@@ -61,0 +62,1 @@\n+#include \"oops\/access.hpp\"\n@@ -63,0 +65,1 @@\n+#include \"oops\/compressedOops.inline.hpp\"\n@@ -70,0 +73,1 @@\n+#include \"oops\/objArrayOop.inline.hpp\"\n@@ -88,0 +92,1 @@\n+#include \"runtime\/keepStackGCProcessed.hpp\"\n@@ -1949,0 +1954,103 @@\n+WB_ENTRY(jobjectArray, WB_getObjectsViaKlassOopMaps(JNIEnv* env, jobject wb, jobject thing))\n+  oop aoop = JNIHandles::resolve(thing);\n+  if (!aoop->is_instance()) {\n+    return nullptr;\n+  }\n+  instanceHandle ih(THREAD, (instanceOop) aoop);\n+  InstanceKlass* klass = InstanceKlass::cast(ih->klass());\n+  if (klass->nonstatic_oop_map_count() == 0) {\n+    return nullptr;\n+  }\n+  const OopMapBlock* map = klass->start_of_nonstatic_oop_maps();\n+  const OopMapBlock* const end = map + klass->nonstatic_oop_map_count();\n+  int oop_count = 0;\n+  while (map < end) {\n+    oop_count += map->count();\n+    map++;\n+  }\n+\n+  objArrayHandle result_array =\n+      oopFactory::new_objArray_handle(vmClasses::Object_klass(), oop_count, CHECK_NULL);\n+  map = klass->start_of_nonstatic_oop_maps();\n+  int index = 0;\n+  while (map < end) {\n+    int offset = map->offset();\n+    for (unsigned int j = 0; j < map->count(); j++) {\n+      result_array->obj_at_put(index++, ih->obj_field(offset));\n+      offset += heapOopSize;\n+    }\n+    map++;\n+  }\n+  return (jobjectArray)JNIHandles::make_local(THREAD, result_array());\n+WB_END\n+\n+\/\/ Collect Object oops but not value objects...loaded from heap\n+class CollectObjectOops : public BasicOopIterateClosure {\n+  public:\n+  GrowableArray<Handle>* _array;\n+\n+  CollectObjectOops() {\n+      _array = new GrowableArray<Handle>(128);\n+  }\n+\n+  void add_oop(oop o) {\n+    Handle oh = Handle(Thread::current(), o);\n+    if (oh != nullptr && oh->is_inline_type()) {\n+      oh->oop_iterate(this);\n+    } else {\n+      _array->append(oh);\n+    }\n+  }\n+\n+  template <class T> inline void add_oop(T* p) { add_oop(HeapAccess<>::oop_load(p)); }\n+  void do_oop(oop* o) { add_oop(o); }\n+  void do_oop(narrowOop* v) { add_oop(v); }\n+\n+  jobjectArray create_jni_result(JNIEnv* env, TRAPS) {\n+    objArrayHandle result_array =\n+        oopFactory::new_objArray_handle(vmClasses::Object_klass(), _array->length(), CHECK_NULL);\n+    for (int i = 0 ; i < _array->length(); i++) {\n+      result_array->obj_at_put(i, _array->at(i)());\n+    }\n+    return (jobjectArray)JNIHandles::make_local(THREAD, result_array());\n+  }\n+};\n+\n+\/\/ Collect Object oops but not value objects...loaded from frames\n+class CollectFrameObjectOops : public BasicOopIterateClosure {\n+ public:\n+  CollectObjectOops _collect;\n+\n+  template <class T> inline void add_oop(T* p) { _collect.add_oop(RawAccess<>::oop_load(p)); }\n+  void do_oop(oop* o) { add_oop(o); }\n+  void do_oop(narrowOop* v) { add_oop(v); }\n+\n+  jobjectArray create_jni_result(JNIEnv* env, TRAPS) {\n+    return _collect.create_jni_result(env, THREAD);\n+  }\n+};\n+\n+\/\/ Collect Object oops for the given oop, iterate through value objects\n+WB_ENTRY(jobjectArray, WB_getObjectsViaOopIterator(JNIEnv* env, jobject wb, jobject thing))\n+  ResourceMark rm(thread);\n+  Handle objh(thread, JNIHandles::resolve(thing));\n+  CollectObjectOops collectOops;\n+  objh->oop_iterate(&collectOops);\n+  return collectOops.create_jni_result(env, THREAD);\n+WB_END\n+\n+\/\/ Collect Object oops for the given frame deep, iterate through value objects\n+WB_ENTRY(jobjectArray, WB_getObjectsViaFrameOopIterator(JNIEnv* env, jobject wb, jint depth))\n+  KeepStackGCProcessedMark ksgcpm(THREAD);\n+  ResourceMark rm(THREAD);\n+  CollectFrameObjectOops collectOops;\n+  StackFrameStream sfs(thread, true \/* update *\/, true \/* process_frames *\/);\n+  while (depth > 0) { \/\/ Skip the native WB API frame\n+    sfs.next();\n+    frame* f = sfs.current();\n+    f->oops_do(&collectOops, nullptr, sfs.register_map());\n+    depth--;\n+  }\n+  return collectOops.create_jni_result(env, THREAD);\n+WB_END\n+\n@@ -2911,0 +3019,6 @@\n+  {CC\"getObjectsViaKlassOopMaps0\",\n+      CC\"(Ljava\/lang\/Object;)[Ljava\/lang\/Object;\",    (void*)&WB_getObjectsViaKlassOopMaps},\n+  {CC\"getObjectsViaOopIterator0\",\n+          CC\"(Ljava\/lang\/Object;)[Ljava\/lang\/Object;\",(void*)&WB_getObjectsViaOopIterator},\n+  {CC\"getObjectsViaFrameOopIterator\",\n+      CC\"(I)[Ljava\/lang\/Object;\",                     (void*)&WB_getObjectsViaFrameOopIterator},\n","filename":"src\/hotspot\/share\/prims\/whitebox.cpp","additions":114,"deletions":0,"binary":false,"changes":114,"status":"modified"},{"patch":"@@ -77,0 +77,1 @@\n+#include <string.h>\n@@ -1776,1 +1777,0 @@\n-static unsigned int patch_mod_count = 0;\n@@ -1783,1 +1783,1 @@\n-  if (!CDSConfig::check_vm_args_consistency(patch_mod_javabase, mode_flag_cmd_line)) {\n+  if (!CDSConfig::check_vm_args_consistency(mode_flag_cmd_line)) {\n@@ -2082,1 +2082,1 @@\n-      add_patch_mod_prefix(module_name, module_equal + 1);\n+      add_patch_mod_prefix(module_name, module_equal + 1, false \/* no append *\/, false \/* no cds *\/);\n@@ -2084,3 +2084,0 @@\n-      if (!create_numbered_module_property(\"jdk.module.patch\", patch_mod_tail, patch_mod_count++)) {\n-        return JNI_ENOMEM;\n-      }\n@@ -2094,0 +2091,64 @@\n+\/\/ VALUECLASS_STR must match string used in the build\n+#define VALUECLASS_STR \"valueclasses\"\n+#define VALUECLASS_JAR \"-\" VALUECLASS_STR \".jar\"\n+\n+\/\/ Finalize --patch-module args and --enable-preview related to value class module patches.\n+\/\/ Create all numbered properties passing module patches.\n+int Arguments::finalize_patch_module() {\n+  \/\/ If --enable-preview and EnableValhalla is true, each module may have value classes that\n+  \/\/ are to be patched into the module.\n+  \/\/ For each <module>-valueclasses.jar in <JAVA_HOME>\/lib\/valueclasses\/\n+  \/\/ appends the equivalent of --patch-module <module>=<JAVA_HOME>\/lib\/valueclasses\/<module>-valueclasses.jar\n+  if (enable_preview() && EnableValhalla) {\n+    char * valueclasses_dir = AllocateHeap(JVM_MAXPATHLEN, mtArguments);\n+    const char * fileSep = os::file_separator();\n+\n+    jio_snprintf(valueclasses_dir, JVM_MAXPATHLEN, \"%s%slib%s\" VALUECLASS_STR \"%s\",\n+                 Arguments::get_java_home(), fileSep, fileSep, fileSep);\n+    DIR* dir = os::opendir(valueclasses_dir);\n+    if (dir != nullptr) {\n+      char * module_name = AllocateHeap(JVM_MAXPATHLEN, mtArguments);\n+      char * path = AllocateHeap(JVM_MAXPATHLEN, mtArguments);\n+\n+      for (dirent * entry = os::readdir(dir); entry != nullptr; entry = os::readdir(dir)) {\n+        \/\/ Test if file ends-with \"-valueclasses.jar\"\n+        int len = (int)strlen(entry->d_name) - (sizeof(VALUECLASS_JAR) - 1);\n+        if (len <= 0 || strcmp(&entry->d_name[len], VALUECLASS_JAR) != 0) {\n+          continue;         \/\/ too short or not the expected suffix\n+        }\n+\n+        strcpy(module_name, entry->d_name);\n+        module_name[len] = '\\0';     \/\/ truncate to just module-name\n+\n+        jio_snprintf(path, JVM_MAXPATHLEN, \"%s%s\", valueclasses_dir, &entry->d_name);\n+        add_patch_mod_prefix(module_name, path, true \/* append *\/, true \/* cds OK*\/);\n+        log_info(class)(\"--enable-preview appending value classes for module %s: %s\", module_name, entry->d_name);\n+      }\n+      FreeHeap(module_name);\n+      FreeHeap(path);\n+      os::closedir(dir);\n+    }\n+    FreeHeap(valueclasses_dir);\n+  }\n+\n+  \/\/ Create numbered properties for each module that has been patched either\n+  \/\/ by --patch-module or --enable-preview\n+  \/\/ Format is \"jdk.module.patch.<n>=<module_name>=<path>\"\n+  if (_patch_mod_prefix != nullptr) {\n+    char * prop_value = AllocateHeap(JVM_MAXPATHLEN + JVM_MAXPATHLEN + 1, mtArguments);\n+    unsigned int patch_mod_count = 0;\n+\n+    for (GrowableArrayIterator<ModulePatchPath *> it = _patch_mod_prefix->begin();\n+            it != _patch_mod_prefix->end(); ++it) {\n+      jio_snprintf(prop_value, JVM_MAXPATHLEN + JVM_MAXPATHLEN + 1, \"%s=%s\",\n+                   (*it)->module_name(), (*it)->path_string());\n+      if (!create_numbered_module_property(\"jdk.module.patch\", prop_value, patch_mod_count++)) {\n+        FreeHeap(prop_value);\n+        return JNI_ENOMEM;\n+      }\n+    }\n+    FreeHeap(prop_value);\n+  }\n+  return JNI_OK;\n+}\n+\n@@ -2349,0 +2410,4 @@\n+      \/\/ --enable-preview enables Valhalla, EnableValhalla VM option will eventually be removed before integration\n+      if (FLAG_SET_CMDLINE(EnableValhalla, true) != JVMFlag::SUCCESS) {\n+        return JNI_EINVAL;\n+      }\n@@ -2828,10 +2893,5 @@\n-void Arguments::add_patch_mod_prefix(const char* module_name, const char* path) {\n-  \/\/ For java.base check for duplicate --patch-module options being specified on the command line.\n-  \/\/ This check is only required for java.base, all other duplicate module specifications\n-  \/\/ will be checked during module system initialization.  The module system initialization\n-  \/\/ will throw an ExceptionInInitializerError if this situation occurs.\n-  if (strcmp(module_name, JAVA_BASE_NAME) == 0) {\n-    if (patch_mod_javabase) {\n-      vm_exit_during_initialization(\"Cannot specify \" JAVA_BASE_NAME \" more than once to --patch-module\");\n-    } else {\n-      patch_mod_javabase = true;\n+void Arguments::add_patch_mod_prefix(const char* module_name, const char* path, bool allow_append, bool allow_cds) {\n+  if (!allow_cds) {\n+    CDSConfig::set_module_patching_disables_cds();\n+    if (strcmp(module_name, JAVA_BASE_NAME) == 0) {\n+      CDSConfig::set_java_base_module_patching_disables_cds();\n@@ -2846,1 +2906,18 @@\n-  _patch_mod_prefix->push(new ModulePatchPath(module_name, path));\n+  \/\/ Scan patches for matching module\n+  int i = _patch_mod_prefix->find_if([&](ModulePatchPath* patch) {\n+    return (strcmp(module_name, patch->module_name()) == 0);\n+  });\n+  if (i == -1) {\n+    _patch_mod_prefix->push(new ModulePatchPath(module_name, path));\n+  } else {\n+    if (allow_append) {\n+      \/\/ append path to existing module entry\n+      _patch_mod_prefix->at(i)->append_path(path);\n+    } else {\n+      if (strcmp(module_name, JAVA_BASE_NAME) == 0) {\n+        vm_exit_during_initialization(\"Cannot specify \" JAVA_BASE_NAME \" more than once to --patch-module\");\n+      } else {\n+        vm_exit_during_initialization(\"Cannot specify a module more than once to --patch-module\", module_name);\n+      }\n+    }\n+  }\n@@ -2959,1 +3036,2 @@\n-  if (!check_vm_args_consistency()) {\n+  \/\/ finalize --module-patch and related --enable-preview\n+  if (finalize_patch_module() != JNI_OK) {\n@@ -2963,0 +3041,3 @@\n+  if (!check_vm_args_consistency()) {\n+    return JNI_ERR;\n+  }\n@@ -3764,0 +3845,12 @@\n+  if (!EnableValhalla || (is_interpreter_only() && !CDSConfig::is_dumping_archive() && !UseSharedSpaces)) {\n+    \/\/ Disable calling convention optimizations if inline types are not supported.\n+    \/\/ Also these aren't useful in -Xint. However, don't disable them when dumping or using\n+    \/\/ the CDS archive, as the values must match between dumptime and runtime.\n+    FLAG_SET_DEFAULT(InlineTypePassFieldsAsArgs, false);\n+    FLAG_SET_DEFAULT(InlineTypeReturnedAsFields, false);\n+  }\n+  if (!UseNonAtomicValueFlattening && !UseNullableValueFlattening && !UseAtomicValueFlattening) {\n+    \/\/ Flattening is disabled\n+    FLAG_SET_DEFAULT(UseArrayFlattening, false);\n+    FLAG_SET_DEFAULT(UseFieldFlattening, false);\n+  }\n","filename":"src\/hotspot\/share\/runtime\/arguments.cpp","additions":111,"deletions":18,"binary":false,"changes":129,"status":"modified"},{"patch":"@@ -50,0 +50,1 @@\n+#include \"oops\/inlineKlass.hpp\"\n","filename":"src\/hotspot\/share\/runtime\/javaThread.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -150,0 +150,1 @@\n+  oop           _return_buffered_value; \/\/ buffered value being returned\n@@ -787,0 +788,3 @@\n+  oop return_buffered_value() const              { return _return_buffered_value; }\n+  void set_return_buffered_value(oop val)        { _return_buffered_value = val; }\n+\n@@ -853,0 +857,1 @@\n+  static ByteSize return_buffered_value_offset() { return byte_offset_of(JavaThread, _return_buffered_value); }\n","filename":"src\/hotspot\/share\/runtime\/javaThread.hpp","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -44,0 +44,1 @@\n+#include \"oops\/inlineKlass.hpp\"\n@@ -785,0 +786,1 @@\n+    ResourceMark rm;\n@@ -786,1 +788,2 @@\n-    bool return_oop = nm->method()->is_returning_oop();\n+    Method* method = nm->method();\n+    bool return_oop = method->is_returning_oop();\n@@ -788,1 +791,16 @@\n-    Handle return_value;\n+    GrowableArray<Handle> return_values;\n+    InlineKlass* vk = nullptr;\n+    if (return_oop && InlineTypeReturnedAsFields &&\n+        (method->result_type() == T_OBJECT)) {\n+      \/\/ Check if an inline type is returned as fields\n+      vk = InlineKlass::returned_inline_klass(map);\n+      if (vk != nullptr) {\n+        \/\/ We're at a safepoint at the return of a method that returns\n+        \/\/ multiple values. We must make sure we preserve the oop values\n+        \/\/ across the safepoint.\n+        assert(vk == method->returns_inline_type(thread()), \"bad inline klass\");\n+        vk->save_oop_fields(map, return_values);\n+        return_oop = false;\n+      }\n+    }\n+\n@@ -795,1 +813,1 @@\n-      return_value = Handle(self, result);\n+      return_values.push(Handle(self, result));\n@@ -809,1 +827,4 @@\n-      caller_fr.set_saved_oop_result(&map, return_value());\n+      assert(return_values.length() == 1, \"only one return value\");\n+      caller_fr.set_saved_oop_result(&map, return_values.pop()());\n+    } else if (vk != nullptr) {\n+      vk->restore_oop_results(map, return_values);\n","filename":"src\/hotspot\/share\/runtime\/safepoint.cpp","additions":25,"deletions":4,"binary":false,"changes":29,"status":"modified"},{"patch":"@@ -411,0 +411,1 @@\n+  initialize_class(vmSymbols::java_lang_IdentityException(), CHECK);\n","filename":"src\/hotspot\/share\/runtime\/threads.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -29,0 +29,2 @@\n+#include \"oops\/array.hpp\"\n+#include \"oops\/oop.hpp\"\n@@ -105,0 +107,1 @@\n+template <typename E, typename UnaryPredicate> class GrowableArrayFilterIterator;\n@@ -483,0 +486,6 @@\n+  void appendAll(const Array<E>* l) {\n+    for (int i = 0; i < l->length(); i++) {\n+      this->at_put_grow(this->_len, l->at(i), E());\n+    }\n+  }\n+\n@@ -859,0 +868,1 @@\n+  template <typename F, typename UnaryPredicate> friend class GrowableArrayFilterIterator;\n@@ -885,0 +895,54 @@\n+\/\/ Custom STL-style iterator to iterate over elements of a GrowableArray that satisfy a given predicate\n+template <typename E, class UnaryPredicate>\n+class GrowableArrayFilterIterator : public StackObj {\n+  friend class GrowableArrayView<E>;\n+\n+ private:\n+  const GrowableArrayView<E>* _array; \/\/ GrowableArray we iterate over\n+  int _position;                      \/\/ Current position in the GrowableArray\n+  UnaryPredicate _predicate;          \/\/ Unary predicate the elements of the GrowableArray should satisfy\n+\n+ public:\n+  GrowableArrayFilterIterator(const GrowableArray<E>* array, UnaryPredicate filter_predicate) :\n+      _array(array), _position(0), _predicate(filter_predicate) {\n+    \/\/ Advance to first element satisfying the predicate\n+    while(!at_end() && !_predicate(_array->at(_position))) {\n+      ++_position;\n+    }\n+  }\n+\n+  GrowableArrayFilterIterator<E, UnaryPredicate>& operator++() {\n+    do {\n+      \/\/ Advance to next element satisfying the predicate\n+      ++_position;\n+    } while(!at_end() && !_predicate(_array->at(_position)));\n+    return *this;\n+  }\n+\n+  E operator*() { return _array->at(_position); }\n+\n+  bool operator==(const GrowableArrayIterator<E>& rhs)  {\n+    assert(_array == rhs._array, \"iterator belongs to different array\");\n+    return _position == rhs._position;\n+  }\n+\n+  bool operator!=(const GrowableArrayIterator<E>& rhs)  {\n+    assert(_array == rhs._array, \"iterator belongs to different array\");\n+    return _position != rhs._position;\n+  }\n+\n+  bool operator==(const GrowableArrayFilterIterator<E, UnaryPredicate>& rhs)  {\n+    assert(_array == rhs._array, \"iterator belongs to different array\");\n+    return _position == rhs._position;\n+  }\n+\n+  bool operator!=(const GrowableArrayFilterIterator<E, UnaryPredicate>& rhs)  {\n+    assert(_array == rhs._array, \"iterator belongs to different array\");\n+    return _position != rhs._position;\n+  }\n+\n+  bool at_end() const {\n+    return _array == nullptr || _position == _array->end()._position;\n+  }\n+};\n+\n","filename":"src\/hotspot\/share\/utilities\/growableArray.hpp","additions":64,"deletions":0,"binary":false,"changes":64,"status":"modified"},{"patch":"@@ -69,0 +69,1 @@\n+compiler\/jvmci\/jdk.vm.ci.code.test\/src\/jdk\/vm\/ci\/code\/test\/VirtualObjectDebugInfoTest.java 8354366 generic-all\n@@ -77,0 +78,3 @@\n+compiler\/c2\/irTests\/scalarReplacement\/ScalarReplacementWithGCBarrierTests.java 8342488 generic-all\n+compiler\/c2\/TestMergeStores.java#id1 8348959 generic-all\n+\n@@ -80,0 +84,4 @@\n+compiler\/startup\/StartupOutput.java 8354404 generic-all\n+compiler\/types\/TestSubTypeCheckUniqueSubclass.java 8354408 generic-all\n+compiler\/valhalla\/inlinetypes\/TestAllocationMergeAndFolding.java 8354283 generic-all\n+\n@@ -106,0 +114,1 @@\n+runtime\/cds\/appcds\/redefineClass\/RedefineRunningMethods_Shared.java  8304168 generic-all\n@@ -127,0 +136,40 @@\n+\n+# Valhalla\n+runtime\/AccModule\/ConstModule.java 8294051 generic-all\n+runtime\/valhalla\/inlinetypes\/CircularityTest.java 8349037 generic-all\n+runtime\/valhalla\/inlinetypes\/PreloadCircularityTest.java 8349631 linux-aarch64-debug\n+runtime\/valhalla\/inlinetypes\/ValuePreloadTest.java 8349630 linux-aarch64-debug\n+compiler\/gcbarriers\/TestG1BarrierGeneration.java 8343420 generic-all\n+\n+# Valhalla + COH\n+compiler\/c2\/autovectorization\/TestIndexOverflowIR.java                          8348568 generic-all\n+compiler\/c2\/irTests\/TestVectorConditionalMove.java                              8348568 generic-all\n+compiler\/c2\/irTests\/TestVectorizationMismatchedAccess.java                      8348568 generic-all\n+compiler\/c2\/irTests\/TestVectorizationNotRun.java                                8348568 generic-all\n+compiler\/c2\/TestCastX2NotProcessedIGVN.java                                     8348568 generic-all\n+compiler\/loopopts\/superword\/TestAlignVector.java                                8348568 generic-all\n+compiler\/loopopts\/superword\/TestAlignVector.java#NoAlignVector-COH              8348568 generic-all\n+compiler\/loopopts\/superword\/TestAlignVector.java#VerifyAlignVector-COH          8348568 generic-all\n+compiler\/loopopts\/superword\/TestIndependentPacksWithCyclicDependency.java       8348568 generic-all\n+compiler\/loopopts\/superword\/TestMulAddS2I.java                                  8348568 generic-all\n+compiler\/loopopts\/superword\/TestScheduleReordersScalarMemops.java               8348568 generic-all\n+compiler\/loopopts\/superword\/TestSplitPacks.java                                 8348568 generic-all\n+compiler\/loopopts\/superword\/TestUnorderedReductionPartialVectorization.java     8348568 generic-all\n+compiler\/vectorization\/TestFloatConversionsVector.java                          8348568 generic-all\n+compiler\/vectorization\/runner\/ArrayTypeConvertTest.java                         8348568 generic-all\n+compiler\/vectorization\/runner\/LoopCombinedOpTest.java                           8348568 generic-all\n+compiler\/vectorization\/runner\/VectorizationTestRunner.java                      8348568 generic-all\n+\n+gc\/stress\/gcbasher\/TestGCBasherWithParallel.java                                8348568 generic-all\n+\n+gtest\/CompressedKlassGtest.java#use-zero-based-encoding-coh                     8348568 generic-all\n+gtest\/CompressedKlassGtest.java#use-zero-based-encoding-coh-large-class-space   8348568 generic-all\n+gtest\/MetaspaceGtests.java#UseCompactObjectHeaders                              8348568 generic-all\n+\n+runtime\/CompressedOops\/CompressedClassPointersEncodingScheme.java               8348568 generic-all\n+runtime\/FieldLayout\/BaseOffsets.java#no-coops-with-coh                          8348568 generic-all\n+runtime\/FieldLayout\/BaseOffsets.java#with-coop--with-coh                        8348568 generic-all\n+runtime\/cds\/TestDefaultArchiveLoading.java#coops_coh                            8348568 generic-all\n+runtime\/cds\/TestDefaultArchiveLoading.java#nocoops_coh                          8348568 generic-all\n+runtime\/cds\/appcds\/TestZGCWithCDS.java                                          8348568 generic-all\n+\n@@ -152,0 +201,31 @@\n+# Valhalla TODO:\n+serviceability\/jvmti\/valhalla\/HeapDump\/HeapDump.java 8317416 generic-all\n+\n+serviceability\/sa\/ClhsdbCDSCore.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbCDSJstackPrintAll.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbFindPC.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbInspect.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbLongConstant.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbJdis.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbJstack.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbPrintAll.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbPrintAs.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbPrintStatics.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbSource.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbSymbol.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbWhere.java 8190936 generic-all\n+serviceability\/sa\/JhsdbThreadInfoTest.java 8190936 generic-all\n+serviceability\/sa\/TestClassDump.java 8190936 generic-all\n+serviceability\/sa\/TestClhsdbJstackLock.java 8190936 generic-all\n+serviceability\/sa\/TestCpoolForInvokeDynamic.java 8190936 generic-all\n+serviceability\/sa\/TestHeapDumpForInvokeDynamic.java 8190936 generic-all\n+serviceability\/sa\/TestHeapDumpForLargeArray.java 8190936 generic-all\n+serviceability\/sa\/TestIntConstant.java 8190936 generic-all\n+serviceability\/sa\/TestJhsdbJstackLock.java 8190936 generic-all\n+serviceability\/sa\/TestJmapCore.java 8190936 generic-all\n+serviceability\/sa\/TestJmapCoreMetaspace.java 8190936 generic-all\n+serviceability\/sa\/TestPrintMdo.java 8190936 generic-all\n+serviceability\/sa\/jmap-hprof\/JMapHProfLargeHeapTest.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbDumpclass.java 8190936 generic-all\n+\n+\n@@ -188,0 +268,2 @@\n+vmTestbase\/vm\/mlvm\/hiddenloader\/stress\/byteMutation\/Test.java 8317172 generic-all\n+\n","filename":"test\/hotspot\/jtreg\/ProblemList.txt","additions":82,"deletions":0,"binary":false,"changes":82,"status":"modified"},{"patch":"@@ -49,1 +49,1 @@\n-  runtime\n+  runtime \\\n@@ -65,0 +65,8 @@\n+hotspot_valhalla = \\\n+  runtime\/valhalla \\\n+  compiler\/valhalla \\\n+  serviceability\/jvmti\/valhalla\n+\n+hotspot_valhalla_runtime = \\\n+  runtime\/valhalla\n+\n@@ -208,0 +216,1 @@\n+  compiler\/valhalla\/ \\\n@@ -249,0 +258,7 @@\n+\n+tier1_compiler_no_valhalla = \\\n+  :tier1_compiler_1 \\\n+  :tier1_compiler_2 \\\n+  :tier1_compiler_3 \\\n+  -compiler\/valhalla\n+\n@@ -401,0 +417,4 @@\n+tier1_runtime_no_valhalla = \\\n+  :tier1_runtime \\\n+  -runtime\/valhalla\n+\n","filename":"test\/hotspot\/jtreg\/TEST.groups","additions":21,"deletions":1,"binary":false,"changes":22,"status":"modified"},{"patch":"@@ -31,0 +31,1 @@\n+import compiler.valhalla.inlinetypes.InlineTypeIRNode;\n@@ -155,0 +156,6 @@\n+    \/\/ Valhalla: Make sure that all Valhalla specific IR nodes are also properly initialized. Doing it here also\n+    \/\/           ensures that the Flag VM is able to pick up the correct compile phases.\n+    static {\n+        InlineTypeIRNode.forceStaticInitialization();\n+    }\n+\n@@ -825,0 +832,5 @@\n+    public static final String INLINE_TYPE = PREFIX + \"INLINE_TYPE\" + POSTFIX;\n+    static {\n+        beforeMatchingNameRegex(INLINE_TYPE, \"InlineType\");\n+    }\n+\n@@ -1961,1 +1973,1 @@\n-        beforeMatchingNameRegex(SUBTYPE_CHECK, \"SubTypeCheck\");\n+        macroNodes(SUBTYPE_CHECK, \"SubTypeCheck\");\n@@ -2678,1 +2690,1 @@\n-    private static void beforeMatching(String irNodePlaceholder, String regex) {\n+    public static void beforeMatching(String irNodePlaceholder, String regex) {\n@@ -2723,1 +2735,1 @@\n-    private static void optoOnly(String irNodePlaceholder, String regex) {\n+    public static void optoOnly(String irNodePlaceholder, String regex) {\n","filename":"test\/hotspot\/jtreg\/compiler\/lib\/ir_framework\/IRNode.java","additions":15,"deletions":3,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -72,2 +72,2 @@\n-                                             TestCommon.list(mainClass),\n-                                             unlockArg, logArg, nmtArg);\n+                TestCommon.list(mainClass),\n+                unlockArg, logArg, nmtArg);\n@@ -77,1 +77,1 @@\n-            .assertNormalExit(output -> {\n+                .assertNormalExit(output -> {\n","filename":"test\/hotspot\/jtreg\/runtime\/cds\/appcds\/ArchiveRelocationTest.java","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -33,1 +33,4 @@\n- * @build Hello\n+ * @enablePreview\n+ * @modules java.base\/jdk.internal.value\n+ *          java.base\/jdk.internal.vm.annotation\n+ * @compile ..\/test-classes\/HelloInlineClassApp.java ..\/test-classes\/HelloRelocation.java\n@@ -35,1 +38,1 @@\n- * @run driver jdk.test.lib.helpers.ClassFileInstaller -jar hello.jar Hello\n+ * @run driver jdk.test.lib.helpers.ClassFileInstaller -jar hello.jar HelloRelocation HelloInlineClassApp HelloInlineClassApp$Point HelloInlineClassApp$Rectangle  HelloInlineClassApp$ValueRecord\n@@ -81,1 +84,1 @@\n-        String mainClass = \"Hello\";\n+        String mainClass = \"HelloRelocation\";\n@@ -97,1 +100,1 @@\n-        TestCommon.dumpBaseArchive(baseArchiveName, unlockArg, logArg)\n+        TestCommon.dumpBaseArchive(baseArchiveName, \"--enable-preview\", unlockArg, logArg)\n@@ -103,0 +106,1 @@\n+              \"--enable-preview\",\n@@ -116,0 +120,1 @@\n+             \"--enable-preview\",\n","filename":"test\/hotspot\/jtreg\/runtime\/cds\/appcds\/dynamicArchive\/DynamicArchiveRelocationTest.java","additions":9,"deletions":4,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -521,0 +521,5 @@\n+java\/lang\/ModuleLayer\/LayerControllerTest.java                  8337048 generic-all\n+java\/lang\/ModuleLayer\/BasicLayerTest.java                       8337048 generic-all\n+\n+java\/lang\/Thread\/virtual\/stress\/Skynet.java#default             8342977 generic-all\n+\n@@ -720,0 +725,4 @@\n+com\/sun\/jdi\/cds\/CDSBreakpointTest.java                          8304168 generic-all\n+com\/sun\/jdi\/cds\/CDSDeleteAllBkptsTest.java                      8304168 generic-all\n+com\/sun\/jdi\/cds\/CDSFieldWatchpoints.java                        8304168 generic-all\n+\n@@ -764,0 +773,6 @@\n+jdk\/classfile\/SwapTest.java                                     8308778 generic-all\n+jdk\/classfile\/LowAdaptTest.java                                 8308778 generic-all\n+jdk\/classfile\/BuilderBlockTest.java                             8308778 generic-all\n+jdk\/classfile\/BuilderTryCatchTest.java                          8308778 generic-all\n+jdk\/classfile\/PrimitiveClassConstantTest.java                   8310649 generic-all\n+\n@@ -811,0 +826,4 @@\n+\n+# valhalla\n+jdk\/jfr\/event\/runtime\/TestSyncOnValueBasedClassEvent.java 8328777 generic-all\n+\n","filename":"test\/jdk\/ProblemList.txt","additions":19,"deletions":0,"binary":false,"changes":19,"status":"modified"},{"patch":"@@ -150,0 +150,14 @@\n+  private native Object[] getObjectsViaKlassOopMaps0(Object thing);\n+  public Object[] getObjectsViaKlassOopMaps(Object thing) {\n+    Objects.requireNonNull(thing);\n+    return getObjectsViaKlassOopMaps0(thing);\n+  }\n+\n+  private native Object[] getObjectsViaOopIterator0(Object thing);\n+  public Object[] getObjectsViaOopIterator(Object thing) {\n+    Objects.requireNonNull(thing);\n+    return getObjectsViaOopIterator0(thing);\n+  }\n+\n+  public native Object[] getObjectsViaFrameOopIterator(int depth);\n+\n","filename":"test\/lib\/jdk\/test\/whitebox\/WhiteBox.java","additions":14,"deletions":0,"binary":false,"changes":14,"status":"modified"}]}