{"files":[{"patch":"@@ -970,0 +970,23 @@\n+################################################################################\n+#\n+# Setup a hook to invoke a script that runs for file produced by the native\n+# compilation steps, after linking.\n+# Parameter is the path to the script to be called.\n+#\n+AC_DEFUN([JDKOPT_SETUP_SIGNING_HOOK],\n+[\n+  UTIL_ARG_WITH(NAME: signing-hook, TYPE: executable,\n+      OPTIONAL: true, DEFAULT: \"\",\n+      DESC: [specify path to script used to code sign native binaries]\n+  )\n+\n+  AC_MSG_CHECKING([for signing hook])\n+  if test \"x$SIGNING_HOOK\" != x; then\n+    UTIL_FIXUP_EXECUTABLE(SIGNING_HOOK)\n+    AC_MSG_RESULT([$SIGNING_HOOK])\n+  else\n+    AC_MSG_RESULT([none])\n+  fi\n+  AC_SUBST(SIGNING_HOOK)\n+])\n+\n","filename":"make\/autoconf\/jdk-options.m4","additions":23,"deletions":0,"binary":false,"changes":23,"status":"modified"},{"patch":"@@ -394,2 +394,2 @@\n-    common.boot_jdk_version = \"23\";\n-    common.boot_jdk_build_number = \"37\";\n+    common.boot_jdk_version = \"24\";\n+    common.boot_jdk_build_number = \"36\";\n","filename":"make\/conf\/jib-profiles.js","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n-# Copyright (c) 2011, 2024, Oracle and\/or its affiliates. All rights reserved.\n+# Copyright (c) 2011, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -40,1 +40,1 @@\n-DEFAULT_ACCEPTABLE_BOOT_VERSIONS=\"23 24 25\"\n+DEFAULT_ACCEPTABLE_BOOT_VERSIONS=\"24 25\"\n","filename":"make\/conf\/version-numbers.conf","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -621,1 +621,1 @@\n-        reg2stack(FrameMap::rscratch1_opr, dest, c->type(), false);\n+        reg2stack(FrameMap::rscratch1_opr, dest, c->type());\n@@ -628,1 +628,1 @@\n-      reg2stack(FrameMap::rscratch1_opr, dest, c->type(), false);\n+      reg2stack(FrameMap::rscratch1_opr, dest, c->type());\n@@ -755,1 +755,1 @@\n-void LIR_Assembler::reg2stack(LIR_Opr src, LIR_Opr dest, BasicType type, bool pop_fpu_stack) {\n+void LIR_Assembler::reg2stack(LIR_Opr src, LIR_Opr dest, BasicType type) {\n@@ -791,1 +791,1 @@\n-void LIR_Assembler::reg2mem(LIR_Opr src, LIR_Opr dest, BasicType type, LIR_PatchCode patch_code, CodeEmitInfo* info, bool pop_fpu_stack, bool wide) {\n+void LIR_Assembler::reg2mem(LIR_Opr src, LIR_Opr dest, BasicType type, LIR_PatchCode patch_code, CodeEmitInfo* info, bool wide) {\n@@ -944,1 +944,1 @@\n-  reg2stack(temp, dest, dest->type(), false);\n+  reg2stack(temp, dest, dest->type());\n@@ -1759,1 +1759,1 @@\n-void LIR_Assembler::arith_op(LIR_Code code, LIR_Opr left, LIR_Opr right, LIR_Opr dest, CodeEmitInfo* info, bool pop_fpu_stack) {\n+void LIR_Assembler::arith_op(LIR_Code code, LIR_Opr left, LIR_Opr right, LIR_Opr dest, CodeEmitInfo* info) {\n@@ -1930,3 +1930,0 @@\n-void LIR_Assembler::arith_fpu_implementation(LIR_Code code, int left_index, int right_index, int dest_index, bool pop_fpu_stack) { Unimplemented(); }\n-\n-\n@@ -3056,2 +3053,1 @@\n-    move_op(src, dest, type, lir_patch_none, info,\n-            \/*pop_fpu_stack*\/false, \/*wide*\/false);\n+    move_op(src, dest, type, lir_patch_none, info, \/*wide*\/false);\n","filename":"src\/hotspot\/cpu\/aarch64\/c1_LIRAssembler_aarch64.cpp","additions":7,"deletions":11,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -36,2 +36,0 @@\n-  void arith_fpu_implementation(LIR_Code code, int left_index, int right_index, int dest_index, bool pop_fpu_stack);\n-\n","filename":"src\/hotspot\/cpu\/aarch64\/c1_LIRAssembler_aarch64.hpp","additions":0,"deletions":2,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -423,1 +423,1 @@\n-  set_result(x, round_item(reg));\n+  set_result(x, reg);\n","filename":"src\/hotspot\/cpu\/aarch64\/c1_LIRGenerator_aarch64.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -1611,3 +1611,0 @@\n-void InterpreterMacroAssembler::verify_FPU(int stack_depth, TosState state) { ; }\n-\n-\n","filename":"src\/hotspot\/cpu\/aarch64\/interp_masm_aarch64.cpp","additions":0,"deletions":3,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -332,2 +332,0 @@\n-  \/\/ only if +VerifyFPU  && (state == ftos || state == dtos)\n-  void verify_FPU(int stack_depth, TosState state = ftos);\n","filename":"src\/hotspot\/cpu\/aarch64\/interp_masm_aarch64.hpp","additions":0,"deletions":2,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -1809,6 +1809,0 @@\n-\/\/ Not supported\n-address TemplateInterpreterGenerator::generate_Float_intBitsToFloat_entry() { return nullptr; }\n-address TemplateInterpreterGenerator::generate_Float_floatToRawIntBits_entry() { return nullptr; }\n-address TemplateInterpreterGenerator::generate_Double_longBitsToDouble_entry() { return nullptr; }\n-address TemplateInterpreterGenerator::generate_Double_doubleToRawLongBits_entry() { return nullptr; }\n-\n","filename":"src\/hotspot\/cpu\/aarch64\/templateInterpreterGenerator_aarch64.cpp","additions":0,"deletions":6,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -457,1 +457,1 @@\n-void LIR_Assembler::reg2stack(LIR_Opr src, LIR_Opr dest, BasicType type, bool pop_fpu_stack) {\n+void LIR_Assembler::reg2stack(LIR_Opr src, LIR_Opr dest, BasicType type) {\n@@ -496,1 +496,1 @@\n-                            bool pop_fpu_stack, bool wide) {\n+                            bool wide) {\n@@ -1515,1 +1515,1 @@\n-void LIR_Assembler::arith_op(LIR_Code code, LIR_Opr left, LIR_Opr right, LIR_Opr dest, CodeEmitInfo* info, bool pop_fpu_stack) {\n+void LIR_Assembler::arith_op(LIR_Code code, LIR_Opr left, LIR_Opr right, LIR_Opr dest, CodeEmitInfo* info) {\n","filename":"src\/hotspot\/cpu\/arm\/c1_LIRAssembler_arm.cpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -1206,1 +1206,1 @@\n-void LIR_Assembler::reg2stack(LIR_Opr from_reg, LIR_Opr dest, BasicType type, bool pop_fpu_stack) {\n+void LIR_Assembler::reg2stack(LIR_Opr from_reg, LIR_Opr dest, BasicType type) {\n@@ -1249,1 +1249,1 @@\n-                            LIR_PatchCode patch_code, CodeEmitInfo* info, bool pop_fpu_stack,\n+                            LIR_PatchCode patch_code, CodeEmitInfo* info,\n@@ -1620,1 +1620,1 @@\n-                             CodeEmitInfo* info, bool pop_fpu_stack) {\n+                             CodeEmitInfo* info) {\n","filename":"src\/hotspot\/cpu\/ppc\/c1_LIRAssembler_ppc.cpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -2384,6 +2384,0 @@\n-void InterpreterMacroAssembler::verify_FPU(int stack_depth, TosState state) {\n-  if (VerifyFPU) {\n-    unimplemented(\"verfiyFPU\");\n-  }\n-}\n-\n","filename":"src\/hotspot\/cpu\/ppc\/interp_masm_ppc_64.cpp","additions":0,"deletions":6,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -1019,1 +1019,1 @@\n-void LIR_Assembler::reg2stack(LIR_Opr src, LIR_Opr dest, BasicType type, bool pop_fpu_stack) {\n+void LIR_Assembler::reg2stack(LIR_Opr src, LIR_Opr dest, BasicType type) {\n@@ -1077,1 +1077,1 @@\n-                            LIR_PatchCode patch_code, CodeEmitInfo* info, bool pop_fpu_stack,\n+                            LIR_PatchCode patch_code, CodeEmitInfo* info,\n@@ -1504,1 +1504,1 @@\n-                             CodeEmitInfo* info, bool pop_fpu_stack) {\n+                             CodeEmitInfo* info) {\n","filename":"src\/hotspot\/cpu\/s390\/c1_LIRAssembler_s390.cpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -96,2 +96,0 @@\n-  verify_FPU(1, state);\n-\n@@ -2192,6 +2190,0 @@\n-\n-void InterpreterMacroAssembler::verify_FPU(int stack_depth, TosState state) {\n-  if (VerifyFPU) {\n-    unimplemented(\"verifyFPU\");\n-  }\n-}\n","filename":"src\/hotspot\/cpu\/s390\/interp_masm_s390.cpp","additions":0,"deletions":8,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -452,10 +452,4 @@\n-    Register tmp2 = rbx;\n-    __ push(tmp2);\n-    \/\/ Load without verification to keep code size small. We need it because\n-    \/\/ begin_initialized_entry_offset has to fit in a byte. Also, we know it's not null.\n-    __ movptr(tmp2, Address(_obj, java_lang_Class::klass_offset()));\n-    __ get_thread(tmp);\n-    __ cmpptr(tmp, Address(tmp2, InstanceKlass::init_thread_offset()));\n-    __ pop(tmp2);\n-    __ pop(tmp);\n-    __ jcc(Assembler::notEqual, call_patch);\n+    __ movptr(tmp, Address(_obj, java_lang_Class::klass_offset()));\n+    __ cmpptr(r15_thread, Address(tmp, InstanceKlass::init_thread_offset()));\n+    __ pop(tmp); \/\/ pop it right away, no matter which path we take\n+    __ jccb(Assembler::notEqual, call_patch);\n","filename":"src\/hotspot\/cpu\/x86\/c1_CodeStubs_x86.cpp","additions":4,"deletions":10,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -166,18 +166,0 @@\n-#ifndef _LP64\n-void LIR_Assembler::fpop() {\n-  __ fpop();\n-}\n-\n-void LIR_Assembler::fxch(int i) {\n-  __ fxch(i);\n-}\n-\n-void LIR_Assembler::fld(int i) {\n-  __ fld_s(i);\n-}\n-\n-void LIR_Assembler::ffree(int i) {\n-  __ ffree(i);\n-}\n-#endif \/\/ !_LP64\n-\n@@ -642,1 +624,1 @@\n-        if (LP64_ONLY(UseAVX <= 2 &&) c->is_zero_float()) {\n+        if (UseAVX <= 2 && c->is_zero_float()) {\n@@ -649,12 +631,0 @@\n-#ifndef _LP64\n-        assert(dest->is_single_fpu(), \"must be\");\n-        assert(dest->fpu_regnr() == 0, \"dest must be TOS\");\n-        if (c->is_zero_float()) {\n-          __ fldz();\n-        } else if (c->is_one_float()) {\n-          __ fld1();\n-        } else {\n-          __ fld_s (InternalAddress(float_constant(c->as_jfloat())));\n-        }\n-#else\n-#endif \/\/ !_LP64\n@@ -668,1 +638,1 @@\n-        if (LP64_ONLY(UseAVX <= 2 &&) c->is_zero_double()) {\n+        if (UseAVX <= 2 && c->is_zero_double()) {\n@@ -675,12 +645,0 @@\n-#ifndef _LP64\n-        assert(dest->is_double_fpu(), \"must be\");\n-        assert(dest->fpu_regnrLo() == 0, \"dest must be TOS\");\n-        if (c->is_zero_double()) {\n-          __ fldz();\n-        } else if (c->is_one_double()) {\n-          __ fld1();\n-        } else {\n-          __ fld_d (InternalAddress(double_constant(c->as_jdouble())));\n-        }\n-#else\n-#endif \/\/ !_LP64\n@@ -881,17 +839,0 @@\n-#ifndef _LP64\n-    \/\/ special moves from fpu-register to xmm-register\n-    \/\/ necessary for method results\n-  } else if (src->is_single_xmm() && !dest->is_single_xmm()) {\n-    __ movflt(Address(rsp, 0), src->as_xmm_float_reg());\n-    __ fld_s(Address(rsp, 0));\n-  } else if (src->is_double_xmm() && !dest->is_double_xmm()) {\n-    __ movdbl(Address(rsp, 0), src->as_xmm_double_reg());\n-    __ fld_d(Address(rsp, 0));\n-  } else if (dest->is_single_xmm() && !src->is_single_xmm()) {\n-    __ fstp_s(Address(rsp, 0));\n-    __ movflt(dest->as_xmm_float_reg(), Address(rsp, 0));\n-  } else if (dest->is_double_xmm() && !src->is_double_xmm()) {\n-    __ fstp_d(Address(rsp, 0));\n-    __ movdbl(dest->as_xmm_double_reg(), Address(rsp, 0));\n-#endif \/\/ !_LP64\n-\n@@ -906,7 +847,0 @@\n-#ifndef _LP64\n-    \/\/ move between fpu-registers (no instruction necessary because of fpu-stack)\n-  } else if (dest->is_single_fpu() || dest->is_double_fpu()) {\n-    assert(src->is_single_fpu() || src->is_double_fpu(), \"must match\");\n-    assert(src->fpu() == dest->fpu(), \"currently should be nothing to do\");\n-#endif \/\/ !_LP64\n-\n@@ -918,1 +852,1 @@\n-void LIR_Assembler::reg2stack(LIR_Opr src, LIR_Opr dest, BasicType type, bool pop_fpu_stack) {\n+void LIR_Assembler::reg2stack(LIR_Opr src, LIR_Opr dest, BasicType type) {\n@@ -947,14 +881,0 @@\n-#ifndef _LP64\n-  } else if (src->is_single_fpu()) {\n-    assert(src->fpu_regnr() == 0, \"argument must be on TOS\");\n-    Address dst_addr = frame_map()->address_for_slot(dest->single_stack_ix());\n-    if (pop_fpu_stack)     __ fstp_s (dst_addr);\n-    else                   __ fst_s  (dst_addr);\n-\n-  } else if (src->is_double_fpu()) {\n-    assert(src->fpu_regnrLo() == 0, \"argument must be on TOS\");\n-    Address dst_addr = frame_map()->address_for_slot(dest->double_stack_ix());\n-    if (pop_fpu_stack)     __ fstp_d (dst_addr);\n-    else                   __ fst_d  (dst_addr);\n-#endif \/\/ !_LP64\n-\n@@ -967,1 +887,1 @@\n-void LIR_Assembler::reg2mem(LIR_Opr src, LIR_Opr dest, BasicType type, LIR_PatchCode patch_code, CodeEmitInfo* info, bool pop_fpu_stack, bool wide) {\n+void LIR_Assembler::reg2mem(LIR_Opr src, LIR_Opr dest, BasicType type, LIR_PatchCode patch_code, CodeEmitInfo* info, bool wide) {\n@@ -994,1 +914,0 @@\n-#ifdef _LP64\n@@ -997,10 +916,0 @@\n-#else\n-      if (src->is_single_xmm()) {\n-        __ movflt(as_Address(to_addr), src->as_xmm_float_reg());\n-      } else {\n-        assert(src->is_single_fpu(), \"must be\");\n-        assert(src->fpu_regnr() == 0, \"argument must be on TOS\");\n-        if (pop_fpu_stack)      __ fstp_s(as_Address(to_addr));\n-        else                    __ fst_s (as_Address(to_addr));\n-      }\n-#endif \/\/ _LP64\n@@ -1011,1 +920,0 @@\n-#ifdef _LP64\n@@ -1014,10 +922,0 @@\n-#else\n-      if (src->is_double_xmm()) {\n-        __ movdbl(as_Address(to_addr), src->as_xmm_double_reg());\n-      } else {\n-        assert(src->is_double_fpu(), \"must be\");\n-        assert(src->fpu_regnrLo() == 0, \"argument must be on TOS\");\n-        if (pop_fpu_stack)      __ fstp_d(as_Address(to_addr));\n-        else                    __ fst_d (as_Address(to_addr));\n-      }\n-#endif \/\/ _LP64\n@@ -1140,12 +1038,0 @@\n-#ifndef _LP64\n-  } else if (dest->is_single_fpu()) {\n-    assert(dest->fpu_regnr() == 0, \"dest must be TOS\");\n-    Address src_addr = frame_map()->address_for_slot(src->single_stack_ix());\n-    __ fld_s(src_addr);\n-\n-  } else if (dest->is_double_fpu()) {\n-    assert(dest->fpu_regnrLo() == 0, \"dest must be TOS\");\n-    Address src_addr = frame_map()->address_for_slot(src->double_stack_ix());\n-    __ fld_d(src_addr);\n-#endif \/\/ _LP64\n-\n@@ -1234,6 +1120,0 @@\n-#ifndef _LP64\n-        assert(dest->is_single_fpu(), \"must be\");\n-        assert(dest->fpu_regnr() == 0, \"dest must be TOS\");\n-        __ fld_s(from_addr);\n-#else\n-#endif \/\/ !LP64\n@@ -1249,6 +1129,0 @@\n-#ifndef _LP64\n-        assert(dest->is_double_fpu(), \"must be\");\n-        assert(dest->fpu_regnrLo() == 0, \"dest must be TOS\");\n-        __ fld_d(from_addr);\n-#else\n-#endif \/\/ !LP64\n@@ -1497,2 +1371,0 @@\n-\n-#ifdef _LP64\n@@ -1538,68 +1410,0 @@\n-#else\n-    case Bytecodes::_f2d:\n-    case Bytecodes::_d2f:\n-      if (dest->is_single_xmm()) {\n-        __ cvtsd2ss(dest->as_xmm_float_reg(), src->as_xmm_double_reg());\n-      } else if (dest->is_double_xmm()) {\n-        __ cvtss2sd(dest->as_xmm_double_reg(), src->as_xmm_float_reg());\n-      } else {\n-        assert(src->fpu() == dest->fpu(), \"register must be equal\");\n-        \/\/ do nothing (float result is rounded later through spilling)\n-      }\n-      break;\n-\n-    case Bytecodes::_i2f:\n-    case Bytecodes::_i2d:\n-      if (dest->is_single_xmm()) {\n-        __ cvtsi2ssl(dest->as_xmm_float_reg(), src->as_register());\n-      } else if (dest->is_double_xmm()) {\n-        __ cvtsi2sdl(dest->as_xmm_double_reg(), src->as_register());\n-      } else {\n-        assert(dest->fpu() == 0, \"result must be on TOS\");\n-        __ movl(Address(rsp, 0), src->as_register());\n-        __ fild_s(Address(rsp, 0));\n-      }\n-      break;\n-\n-    case Bytecodes::_l2f:\n-    case Bytecodes::_l2d:\n-      assert(!dest->is_xmm_register(), \"result in xmm register not supported (no SSE instruction present)\");\n-      assert(dest->fpu() == 0, \"result must be on TOS\");\n-      __ movptr(Address(rsp, 0),          src->as_register_lo());\n-      __ movl(Address(rsp, BytesPerWord), src->as_register_hi());\n-      __ fild_d(Address(rsp, 0));\n-      \/\/ float result is rounded later through spilling\n-      break;\n-\n-    case Bytecodes::_f2i:\n-    case Bytecodes::_d2i:\n-      if (src->is_single_xmm()) {\n-        __ cvttss2sil(dest->as_register(), src->as_xmm_float_reg());\n-      } else if (src->is_double_xmm()) {\n-        __ cvttsd2sil(dest->as_register(), src->as_xmm_double_reg());\n-      } else {\n-        assert(src->fpu() == 0, \"input must be on TOS\");\n-        __ fldcw(ExternalAddress(StubRoutines::x86::addr_fpu_cntrl_wrd_trunc()));\n-        __ fist_s(Address(rsp, 0));\n-        __ movl(dest->as_register(), Address(rsp, 0));\n-        __ fldcw(ExternalAddress(StubRoutines::x86::addr_fpu_cntrl_wrd_std()));\n-      }\n-      \/\/ IA32 conversion instructions do not match JLS for overflow, underflow and NaN -> fixup in stub\n-      assert(op->stub() != nullptr, \"stub required\");\n-      __ cmpl(dest->as_register(), 0x80000000);\n-      __ jcc(Assembler::equal, *op->stub()->entry());\n-      __ bind(*op->stub()->continuation());\n-      break;\n-\n-    case Bytecodes::_f2l:\n-    case Bytecodes::_d2l:\n-      assert(!src->is_xmm_register(), \"input in xmm register not supported (no SSE instruction present)\");\n-      assert(src->fpu() == 0, \"input must be on TOS\");\n-      assert(dest == FrameMap::long0_opr, \"runtime stub places result in these registers\");\n-\n-      \/\/ instruction sequence too long to inline it here\n-      {\n-        __ call(RuntimeAddress(Runtime1::entry_for(C1StubId::fpu2long_stub_id)));\n-      }\n-      break;\n-#endif \/\/ _LP64\n@@ -2192,1 +1996,1 @@\n-void LIR_Assembler::arith_op(LIR_Code code, LIR_Opr left, LIR_Opr right, LIR_Opr dest, CodeEmitInfo* info, bool pop_fpu_stack) {\n+void LIR_Assembler::arith_op(LIR_Code code, LIR_Opr left, LIR_Opr right, LIR_Opr dest, CodeEmitInfo* info) {\n@@ -2373,74 +2177,0 @@\n-#ifndef _LP64\n-  } else if (left->is_single_fpu()) {\n-    assert(dest->is_single_fpu(),  \"fpu stack allocation required\");\n-\n-    if (right->is_single_fpu()) {\n-      arith_fpu_implementation(code, left->fpu_regnr(), right->fpu_regnr(), dest->fpu_regnr(), pop_fpu_stack);\n-\n-    } else {\n-      assert(left->fpu_regnr() == 0, \"left must be on TOS\");\n-      assert(dest->fpu_regnr() == 0, \"dest must be on TOS\");\n-\n-      Address raddr;\n-      if (right->is_single_stack()) {\n-        raddr = frame_map()->address_for_slot(right->single_stack_ix());\n-      } else if (right->is_constant()) {\n-        address const_addr = float_constant(right->as_jfloat());\n-        assert(const_addr != nullptr, \"incorrect float\/double constant maintenance\");\n-        \/\/ hack for now\n-        raddr = __ as_Address(InternalAddress(const_addr));\n-      } else {\n-        ShouldNotReachHere();\n-      }\n-\n-      switch (code) {\n-        case lir_add: __ fadd_s(raddr); break;\n-        case lir_sub: __ fsub_s(raddr); break;\n-        case lir_mul: __ fmul_s(raddr); break;\n-        case lir_div: __ fdiv_s(raddr); break;\n-        default:      ShouldNotReachHere();\n-      }\n-    }\n-\n-  } else if (left->is_double_fpu()) {\n-    assert(dest->is_double_fpu(),  \"fpu stack allocation required\");\n-\n-    if (code == lir_mul || code == lir_div) {\n-      \/\/ Double values require special handling for strictfp mul\/div on x86\n-      __ fld_x(ExternalAddress(StubRoutines::x86::addr_fpu_subnormal_bias1()));\n-      __ fmulp(left->fpu_regnrLo() + 1);\n-    }\n-\n-    if (right->is_double_fpu()) {\n-      arith_fpu_implementation(code, left->fpu_regnrLo(), right->fpu_regnrLo(), dest->fpu_regnrLo(), pop_fpu_stack);\n-\n-    } else {\n-      assert(left->fpu_regnrLo() == 0, \"left must be on TOS\");\n-      assert(dest->fpu_regnrLo() == 0, \"dest must be on TOS\");\n-\n-      Address raddr;\n-      if (right->is_double_stack()) {\n-        raddr = frame_map()->address_for_slot(right->double_stack_ix());\n-      } else if (right->is_constant()) {\n-        \/\/ hack for now\n-        raddr = __ as_Address(InternalAddress(double_constant(right->as_jdouble())));\n-      } else {\n-        ShouldNotReachHere();\n-      }\n-\n-      switch (code) {\n-        case lir_add: __ fadd_d(raddr); break;\n-        case lir_sub: __ fsub_d(raddr); break;\n-        case lir_mul: __ fmul_d(raddr); break;\n-        case lir_div: __ fdiv_d(raddr); break;\n-        default: ShouldNotReachHere();\n-      }\n-    }\n-\n-    if (code == lir_mul || code == lir_div) {\n-      \/\/ Double values require special handling for strictfp mul\/div on x86\n-      __ fld_x(ExternalAddress(StubRoutines::x86::addr_fpu_subnormal_bias2()));\n-      __ fmulp(dest->fpu_regnrLo() + 1);\n-    }\n-#endif \/\/ !_LP64\n-\n@@ -2488,58 +2218,0 @@\n-#ifndef _LP64\n-void LIR_Assembler::arith_fpu_implementation(LIR_Code code, int left_index, int right_index, int dest_index, bool pop_fpu_stack) {\n-  assert(pop_fpu_stack  || (left_index     == dest_index || right_index     == dest_index), \"invalid LIR\");\n-  assert(!pop_fpu_stack || (left_index - 1 == dest_index || right_index - 1 == dest_index), \"invalid LIR\");\n-  assert(left_index == 0 || right_index == 0, \"either must be on top of stack\");\n-\n-  bool left_is_tos = (left_index == 0);\n-  bool dest_is_tos = (dest_index == 0);\n-  int non_tos_index = (left_is_tos ? right_index : left_index);\n-\n-  switch (code) {\n-    case lir_add:\n-      if (pop_fpu_stack)       __ faddp(non_tos_index);\n-      else if (dest_is_tos)    __ fadd (non_tos_index);\n-      else                     __ fadda(non_tos_index);\n-      break;\n-\n-    case lir_sub:\n-      if (left_is_tos) {\n-        if (pop_fpu_stack)     __ fsubrp(non_tos_index);\n-        else if (dest_is_tos)  __ fsub  (non_tos_index);\n-        else                   __ fsubra(non_tos_index);\n-      } else {\n-        if (pop_fpu_stack)     __ fsubp (non_tos_index);\n-        else if (dest_is_tos)  __ fsubr (non_tos_index);\n-        else                   __ fsuba (non_tos_index);\n-      }\n-      break;\n-\n-    case lir_mul:\n-      if (pop_fpu_stack)       __ fmulp(non_tos_index);\n-      else if (dest_is_tos)    __ fmul (non_tos_index);\n-      else                     __ fmula(non_tos_index);\n-      break;\n-\n-    case lir_div:\n-      if (left_is_tos) {\n-        if (pop_fpu_stack)     __ fdivrp(non_tos_index);\n-        else if (dest_is_tos)  __ fdiv  (non_tos_index);\n-        else                   __ fdivra(non_tos_index);\n-      } else {\n-        if (pop_fpu_stack)     __ fdivp (non_tos_index);\n-        else if (dest_is_tos)  __ fdivr (non_tos_index);\n-        else                   __ fdiva (non_tos_index);\n-      }\n-      break;\n-\n-    case lir_rem:\n-      assert(left_is_tos && dest_is_tos && right_index == 1, \"must be guaranteed by FPU stack allocation\");\n-      __ fremr(noreg);\n-      break;\n-\n-    default:\n-      ShouldNotReachHere();\n-  }\n-}\n-#endif \/\/ _LP64\n-\n@@ -2567,9 +2239,0 @@\n-#ifndef _LP64\n-  } else if (value->is_double_fpu()) {\n-    assert(value->fpu_regnrLo() == 0 && dest->fpu_regnrLo() == 0, \"both must be on TOS\");\n-    switch(code) {\n-      case lir_abs   : __ fabs() ; break;\n-      case lir_sqrt  : __ fsqrt(); break;\n-      default      : ShouldNotReachHere();\n-    }\n-#endif \/\/ !_LP64\n@@ -2893,7 +2556,0 @@\n-#ifndef _LP64\n-  } else if(opr1->is_single_fpu() || opr1->is_double_fpu()) {\n-    assert(opr1->is_fpu_register() && opr1->fpu() == 0, \"currently left-hand side must be on TOS (relax this restriction)\");\n-    assert(opr2->is_fpu_register(), \"both must be registers\");\n-    __ fcmp(noreg, opr2->fpu(), op->fpu_pop_count() > 0, op->fpu_pop_count() > 1);\n-#endif \/\/ LP64\n-\n@@ -2942,9 +2598,0 @@\n-#ifdef _LP64\n-#else\n-      assert(left->is_single_fpu() || left->is_double_fpu(), \"must be\");\n-      assert(right->is_single_fpu() || right->is_double_fpu(), \"must match\");\n-\n-      assert(left->fpu() == 0, \"left must be on TOS\");\n-      __ fcmp2int(dst->as_register(), code == lir_ucmp_fd2i, right->fpu(),\n-                  op->fpu_pop_count() > 0, op->fpu_pop_count() > 1);\n-#endif \/\/ LP64\n@@ -4013,7 +3660,0 @@\n-#ifndef _LP64\n-  } else if (left->is_single_fpu() || left->is_double_fpu()) {\n-    assert(left->fpu() == 0, \"arg must be on TOS\");\n-    assert(dest->fpu() == 0, \"dest must be TOS\");\n-    __ fchs();\n-#endif \/\/ !_LP64\n-\n@@ -4088,23 +3728,0 @@\n-\n-#ifndef _LP64\n-  } else if (src->is_double_fpu()) {\n-    assert(src->fpu_regnrLo() == 0, \"must be TOS\");\n-    if (dest->is_double_stack()) {\n-      __ fistp_d(frame_map()->address_for_slot(dest->double_stack_ix()));\n-    } else if (dest->is_address()) {\n-      __ fistp_d(as_Address(dest->as_address_ptr()));\n-    } else {\n-      ShouldNotReachHere();\n-    }\n-\n-  } else if (dest->is_double_fpu()) {\n-    assert(dest->fpu_regnrLo() == 0, \"must be TOS\");\n-    if (src->is_double_stack()) {\n-      __ fild_d(frame_map()->address_for_slot(src->double_stack_ix()));\n-    } else if (src->is_address()) {\n-      __ fild_d(as_Address(src->as_address_ptr()));\n-    } else {\n-      ShouldNotReachHere();\n-    }\n-#endif \/\/ !_LP64\n-\n","filename":"src\/hotspot\/cpu\/x86\/c1_LIRAssembler_x86.cpp","additions":5,"deletions":388,"binary":false,"changes":393,"status":"modified"},{"patch":"@@ -64,9 +64,0 @@\n-#ifndef _LP64\n-  void arith_fpu_implementation(LIR_Code code, int left_index, int right_index, int dest_index, bool pop_fpu_stack);\n-\n-  void fpop();\n-  void fxch(int i);\n-  void fld(int i);\n-  void ffree(int i);\n-#endif \/\/ !_LP64\n-\n","filename":"src\/hotspot\/cpu\/x86\/c1_LIRAssembler_x86.hpp","additions":0,"deletions":9,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -96,1 +96,0 @@\n-#ifdef _LP64\n@@ -99,4 +98,0 @@\n-#else\n-    case floatTag:   opr = UseSSE >= 1 ? FrameMap::xmm0_float_opr  : FrameMap::fpu0_float_opr;  break;\n-    case doubleTag:  opr = UseSSE >= 2 ? FrameMap::xmm0_double_opr : FrameMap::fpu0_double_opr;  break;\n-#endif \/\/ _LP64\n@@ -373,1 +368,1 @@\n-  set_result(x, round_item(reg));\n+  set_result(x, reg);\n@@ -461,1 +456,1 @@\n-    set_result(x, round_item(reg));\n+    set_result(x, reg);\n@@ -482,1 +477,1 @@\n-  set_result(x, round_item(reg));\n+  set_result(x, reg);\n@@ -898,56 +893,0 @@\n-#ifndef _LP64\n-  LIR_Opr tmp = FrameMap::fpu0_double_opr;\n-  result_reg = tmp;\n-  switch(x->id()) {\n-    case vmIntrinsics::_dexp:\n-      if (StubRoutines::dexp() != nullptr) {\n-        __ call_runtime_leaf(StubRoutines::dexp(), getThreadTemp(), result_reg, cc->args());\n-      } else {\n-        __ call_runtime_leaf(CAST_FROM_FN_PTR(address, SharedRuntime::dexp), getThreadTemp(), result_reg, cc->args());\n-      }\n-      break;\n-    case vmIntrinsics::_dlog:\n-      if (StubRoutines::dlog() != nullptr) {\n-        __ call_runtime_leaf(StubRoutines::dlog(), getThreadTemp(), result_reg, cc->args());\n-      } else {\n-        __ call_runtime_leaf(CAST_FROM_FN_PTR(address, SharedRuntime::dlog), getThreadTemp(), result_reg, cc->args());\n-      }\n-      break;\n-    case vmIntrinsics::_dlog10:\n-      if (StubRoutines::dlog10() != nullptr) {\n-       __ call_runtime_leaf(StubRoutines::dlog10(), getThreadTemp(), result_reg, cc->args());\n-      } else {\n-        __ call_runtime_leaf(CAST_FROM_FN_PTR(address, SharedRuntime::dlog10), getThreadTemp(), result_reg, cc->args());\n-      }\n-      break;\n-    case vmIntrinsics::_dpow:\n-      if (StubRoutines::dpow() != nullptr) {\n-        __ call_runtime_leaf(StubRoutines::dpow(), getThreadTemp(), result_reg, cc->args());\n-      } else {\n-        __ call_runtime_leaf(CAST_FROM_FN_PTR(address, SharedRuntime::dpow), getThreadTemp(), result_reg, cc->args());\n-      }\n-      break;\n-    case vmIntrinsics::_dsin:\n-      if (VM_Version::supports_sse2() && StubRoutines::dsin() != nullptr) {\n-        __ call_runtime_leaf(StubRoutines::dsin(), getThreadTemp(), result_reg, cc->args());\n-      } else {\n-        __ call_runtime_leaf(CAST_FROM_FN_PTR(address, SharedRuntime::dsin), getThreadTemp(), result_reg, cc->args());\n-      }\n-      break;\n-    case vmIntrinsics::_dcos:\n-      if (VM_Version::supports_sse2() && StubRoutines::dcos() != nullptr) {\n-        __ call_runtime_leaf(StubRoutines::dcos(), getThreadTemp(), result_reg, cc->args());\n-      } else {\n-        __ call_runtime_leaf(CAST_FROM_FN_PTR(address, SharedRuntime::dcos), getThreadTemp(), result_reg, cc->args());\n-      }\n-      break;\n-    case vmIntrinsics::_dtan:\n-      if (StubRoutines::dtan() != nullptr) {\n-        __ call_runtime_leaf(StubRoutines::dtan(), getThreadTemp(), result_reg, cc->args());\n-      } else {\n-        __ call_runtime_leaf(CAST_FROM_FN_PTR(address, SharedRuntime::dtan), getThreadTemp(), result_reg, cc->args());\n-      }\n-      break;\n-    default:  ShouldNotReachHere();\n-  }\n-#else\n@@ -1012,1 +951,1 @@\n-#endif \/\/ _LP64\n+\n@@ -1302,14 +1241,0 @@\n-#ifndef _LP64\n-\/\/ _i2l, _i2f, _i2d, _l2i, _l2f, _l2d, _f2i, _f2l, _f2d, _d2i, _d2l, _d2f\n-\/\/ _i2b, _i2c, _i2s\n-static LIR_Opr fixed_register_for(BasicType type) {\n-  switch (type) {\n-    case T_FLOAT:  return FrameMap::fpu0_float_opr;\n-    case T_DOUBLE: return FrameMap::fpu0_double_opr;\n-    case T_INT:    return FrameMap::rax_opr;\n-    case T_LONG:   return FrameMap::long0_opr;\n-    default:       ShouldNotReachHere(); return LIR_OprFact::illegalOpr;\n-  }\n-}\n-#endif\n-\n","filename":"src\/hotspot\/cpu\/x86\/c1_LIRGenerator_x86.cpp","additions":4,"deletions":79,"binary":false,"changes":83,"status":"modified"},{"patch":"@@ -333,6 +333,0 @@\n-#if !defined(_LP64) && defined(COMPILER2)\n-  if (UseSSE < 2 && !CompilerConfig::is_c1_only_no_jvmci()) {\n-      \/\/ c2 leaves fpu stack dirty. Clean it on entry\n-      empty_FPU_stack();\n-    }\n-#endif \/\/ !_LP64 && COMPILER2\n","filename":"src\/hotspot\/cpu\/x86\/c1_MacroAssembler_x86.cpp","additions":0,"deletions":6,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -58,4 +58,0 @@\n-#ifdef _LP64\n-#else\n-  assert_different_registers(obj, mdo_addr.base(), mdo_addr.index());\n-#endif\n@@ -77,2 +73,0 @@\n-#ifdef _LP64\n-#endif\n@@ -94,1 +88,1 @@\n-#ifdef _LP64\n+\n@@ -102,1 +96,0 @@\n-#endif\n@@ -319,1 +312,0 @@\n-  NOT_LP64(assert(java_thread == noreg , \"not expecting a precomputed java thread\");)\n@@ -340,1 +332,0 @@\n-#ifdef _LP64\n@@ -392,7 +383,0 @@\n-#else\n-void InterpreterMacroAssembler::call_VM_preemptable(Register oop_result,\n-                         address entry_point,\n-                         Register arg_1) {\n-  MacroAssembler::call_VM(oop_result, entry_point, arg_1);\n-}\n-#endif  \/\/ _LP64\n@@ -409,2 +393,1 @@\n-    Register pop_cond = NOT_LP64(java_thread) \/\/ Not clear if any other register is available on 32 bit\n-                        LP64_ONLY(c_rarg0);\n+    Register pop_cond = c_rarg0;\n@@ -421,1 +404,0 @@\n-    NOT_LP64(get_thread(java_thread);)\n@@ -426,3 +408,1 @@\n-  Register thread = LP64_ONLY(r15_thread) NOT_LP64(rcx);\n-  NOT_LP64(get_thread(thread);)\n-  movptr(rcx, Address(thread, JavaThread::jvmti_thread_state_offset()));\n+  movptr(rcx, Address(r15_thread, JavaThread::jvmti_thread_state_offset()));\n@@ -432,1 +412,1 @@\n-#ifdef _LP64\n+\n@@ -448,23 +428,1 @@\n-  \/\/ Clean up tos value in the thread object\n-  movl(tos_addr, ilgl);\n-  movl(val_addr, NULL_WORD);\n-#else\n-  const Address val_addr1(rcx, JvmtiThreadState::earlyret_value_offset()\n-                             + in_ByteSize(wordSize));\n-  switch (state) {\n-    case atos: movptr(rax, oop_addr);\n-               movptr(oop_addr, NULL_WORD);\n-               interp_verify_oop(rax, state);         break;\n-    case ltos:\n-               movl(rdx, val_addr1);               \/\/ fall through\n-    case btos:                                     \/\/ fall through\n-    case ztos:                                     \/\/ fall through\n-    case ctos:                                     \/\/ fall through\n-    case stos:                                     \/\/ fall through\n-    case itos: movl(rax, val_addr);                   break;\n-    case ftos: load_float(val_addr);                  break;\n-    case dtos: load_double(val_addr);                 break;\n-    case vtos: \/* nothing to do *\/                    break;\n-    default  : ShouldNotReachHere();\n-  }\n-#endif \/\/ _LP64\n+\n@@ -474,1 +432,0 @@\n-  NOT_LP64(movptr(val_addr1, NULL_WORD);)\n@@ -481,2 +438,2 @@\n-    Register tmp = LP64_ONLY(c_rarg0) NOT_LP64(java_thread);\n-    Register rthread = LP64_ONLY(r15_thread) NOT_LP64(java_thread);\n+    Register tmp = c_rarg0;\n+    Register rthread = r15_thread;\n@@ -497,2 +454,0 @@\n-    NOT_LP64(get_thread(java_thread);)\n-#ifdef _LP64\n@@ -502,4 +457,0 @@\n-#else\n-    pushl(Address(tmp, JvmtiThreadState::earlyret_tos_offset()));\n-    call_VM_leaf(CAST_FROM_FN_PTR(address, Interpreter::remove_activation_early_entry), 1);\n-#endif \/\/ _LP64\n@@ -508,1 +459,0 @@\n-    NOT_LP64(get_thread(java_thread);)\n@@ -590,17 +540,0 @@\n-#ifndef _LP64\n-void InterpreterMacroAssembler::f2ieee() {\n-  if (IEEEPrecision) {\n-    fstp_s(Address(rsp, 0));\n-    fld_s(Address(rsp, 0));\n-  }\n-}\n-\n-\n-void InterpreterMacroAssembler::d2ieee() {\n-  if (IEEEPrecision) {\n-    fstp_d(Address(rsp, 0));\n-    fld_d(Address(rsp, 0));\n-  }\n-}\n-#endif \/\/ _LP64\n-\n@@ -645,1 +578,0 @@\n-#ifdef _LP64\n@@ -696,99 +628,0 @@\n-#else\n-void InterpreterMacroAssembler::pop_i(Register r) {\n-  pop(r);\n-}\n-\n-void InterpreterMacroAssembler::pop_l(Register lo, Register hi) {\n-  pop(lo);\n-  pop(hi);\n-}\n-\n-void InterpreterMacroAssembler::pop_f() {\n-  fld_s(Address(rsp, 0));\n-  addptr(rsp, 1 * wordSize);\n-}\n-\n-void InterpreterMacroAssembler::pop_d() {\n-  fld_d(Address(rsp, 0));\n-  addptr(rsp, 2 * wordSize);\n-}\n-\n-\n-void InterpreterMacroAssembler::pop(TosState state) {\n-  switch (state) {\n-    case atos: pop_ptr(rax);                                 break;\n-    case btos:                                               \/\/ fall through\n-    case ztos:                                               \/\/ fall through\n-    case ctos:                                               \/\/ fall through\n-    case stos:                                               \/\/ fall through\n-    case itos: pop_i(rax);                                   break;\n-    case ltos: pop_l(rax, rdx);                              break;\n-    case ftos:\n-      if (UseSSE >= 1) {\n-        pop_f(xmm0);\n-      } else {\n-        pop_f();\n-      }\n-      break;\n-    case dtos:\n-      if (UseSSE >= 2) {\n-        pop_d(xmm0);\n-      } else {\n-        pop_d();\n-      }\n-      break;\n-    case vtos: \/* nothing to do *\/                           break;\n-    default  : ShouldNotReachHere();\n-  }\n-  interp_verify_oop(rax, state);\n-}\n-\n-\n-void InterpreterMacroAssembler::push_l(Register lo, Register hi) {\n-  push(hi);\n-  push(lo);\n-}\n-\n-void InterpreterMacroAssembler::push_f() {\n-  \/\/ Do not schedule for no AGI! Never write beyond rsp!\n-  subptr(rsp, 1 * wordSize);\n-  fstp_s(Address(rsp, 0));\n-}\n-\n-void InterpreterMacroAssembler::push_d() {\n-  \/\/ Do not schedule for no AGI! Never write beyond rsp!\n-  subptr(rsp, 2 * wordSize);\n-  fstp_d(Address(rsp, 0));\n-}\n-\n-\n-void InterpreterMacroAssembler::push(TosState state) {\n-  interp_verify_oop(rax, state);\n-  switch (state) {\n-    case atos: push_ptr(rax); break;\n-    case btos:                                               \/\/ fall through\n-    case ztos:                                               \/\/ fall through\n-    case ctos:                                               \/\/ fall through\n-    case stos:                                               \/\/ fall through\n-    case itos: push_i(rax);                                    break;\n-    case ltos: push_l(rax, rdx);                               break;\n-    case ftos:\n-      if (UseSSE >= 1) {\n-        push_f(xmm0);\n-      } else {\n-        push_f();\n-      }\n-      break;\n-    case dtos:\n-      if (UseSSE >= 2) {\n-        push_d(xmm0);\n-      } else {\n-        push_d();\n-      }\n-      break;\n-    case vtos: \/* nothing to do *\/                             break;\n-    default  : ShouldNotReachHere();\n-  }\n-}\n-#endif \/\/ _LP64\n-\n@@ -829,3 +662,1 @@\n-    LP64_ONLY(temp = r15_thread;)\n-    NOT_LP64(get_thread(temp);)\n-    cmpb(Address(temp, JavaThread::interp_only_mode_offset()), 0);\n+    cmpb(Address(r15_thread, JavaThread::interp_only_mode_offset()), 0);\n@@ -854,1 +685,0 @@\n-  verify_FPU(1, state);\n@@ -872,1 +702,0 @@\n-#ifdef _LP64\n@@ -887,21 +716,0 @@\n-\n-#else\n-  Address index(noreg, rbx, Address::times_ptr);\n-  if (table != safepoint_table && generate_poll) {\n-    NOT_PRODUCT(block_comment(\"Thread-local Safepoint poll\"));\n-    Label no_safepoint;\n-    const Register thread = rcx;\n-    get_thread(thread);\n-    testb(Address(thread, JavaThread::polling_word_offset()), SafepointMechanism::poll_bit());\n-\n-    jccb(Assembler::zero, no_safepoint);\n-    ArrayAddress dispatch_addr(ExternalAddress((address)safepoint_table), index);\n-    jump(dispatch_addr, noreg);\n-    bind(no_safepoint);\n-  }\n-\n-  {\n-    ArrayAddress dispatch_addr(ExternalAddress((address)table), index);\n-    jump(dispatch_addr, noreg);\n-  }\n-#endif \/\/ _LP64\n@@ -959,3 +767,1 @@\n-  LP64_ONLY(movsbl(result, result);)\n-  NOT_LP64(shll(result, 24);)      \/\/ truncate upper 24 bits\n-  NOT_LP64(sarl(result, 24);)      \/\/ and sign-extend byte\n+  movsbl(result, result);\n@@ -967,2 +773,1 @@\n-  LP64_ONLY(movzwl(result, result);)\n-  NOT_LP64(andl(result, 0xFFFF);)  \/\/ truncate upper 16 bits\n+  movzwl(result, result);\n@@ -974,3 +779,1 @@\n-  LP64_ONLY(movswl(result, result);)\n-  NOT_LP64(shll(result, 16);)      \/\/ truncate upper 16 bits\n-  NOT_LP64(sarl(result, 16);)      \/\/ and sign-extend short\n+  movswl(result, result);\n@@ -1006,6 +809,3 @@\n-  const Register rthread = LP64_ONLY(r15_thread) NOT_LP64(rcx);\n-  const Register robj    = LP64_ONLY(c_rarg1) NOT_LP64(rdx);\n-  const Register rmon    = LP64_ONLY(c_rarg1) NOT_LP64(rcx);\n-                              \/\/ monitor pointers need different register\n-                              \/\/ because rdx may have the result in it\n-  NOT_LP64(get_thread(rthread);)\n+  const Register rthread = r15_thread;\n+  const Register robj    = c_rarg1;\n+  const Register rmon    = c_rarg1;\n@@ -1024,1 +824,0 @@\n-  NOT_LP64(get_thread(rthread);) \/\/ call_VM clobbered it, restore\n@@ -1065,1 +864,0 @@\n-    NOT_LP64(empty_FPU_stack();)  \/\/ remove possible return value from FPU-stack, otherwise stack could overflow\n@@ -1074,1 +872,0 @@\n-      NOT_LP64(empty_FPU_stack();)\n@@ -1116,1 +913,0 @@\n-      NOT_LP64(empty_FPU_stack();)\n@@ -1132,1 +928,0 @@\n-        NOT_LP64(empty_FPU_stack();)\n@@ -1165,1 +960,1 @@\n-    Register rthread = LP64_ONLY(r15_thread) NOT_LP64(rcx);\n+    Register rthread = r15_thread;\n@@ -1168,2 +963,0 @@\n-    NOT_LP64(get_thread(rthread);)\n-\n@@ -1309,2 +1102,1 @@\n-  assert(lock_reg == LP64_ONLY(c_rarg1) NOT_LP64(rdx),\n-         \"The argument is only for looks. It must be c_rarg1\");\n+  assert(lock_reg == c_rarg1, \"The argument is only for looks. It must be c_rarg1\");\n@@ -1321,1 +1113,1 @@\n-    const Register obj_reg = LP64_ONLY(c_rarg3) NOT_LP64(rcx); \/\/ Will contain the oop\n+    const Register obj_reg = c_rarg3; \/\/ Will contain the oop\n@@ -1339,1 +1131,0 @@\n-#ifdef _LP64\n@@ -1342,4 +1133,0 @@\n-#else\n-      \/\/ Lacking registers and thread on x86_32. Always take slow path.\n-      jmp(slow_case);\n-#endif\n@@ -1367,1 +1154,1 @@\n-      const int zero_bits = LP64_ONLY(7) NOT_LP64(3);\n+      const int zero_bits = 7;\n@@ -1432,2 +1219,1 @@\n-  assert(lock_reg == LP64_ONLY(c_rarg1) NOT_LP64(rdx),\n-         \"The argument is only for looks. It must be c_rarg1\");\n+  assert(lock_reg == c_rarg1, \"The argument is only for looks. It must be c_rarg1\");\n@@ -1441,2 +1227,2 @@\n-    const Register header_reg = LP64_ONLY(c_rarg2) NOT_LP64(rbx);  \/\/ Will contain the old oopMark\n-    const Register obj_reg    = LP64_ONLY(c_rarg3) NOT_LP64(rcx);  \/\/ Will contain the oop\n+    const Register header_reg = c_rarg2;  \/\/ Will contain the old oopMark\n+    const Register obj_reg    = c_rarg3;  \/\/ Will contain the oop\n@@ -1459,5 +1245,0 @@\n-#ifdef _LP64\n-#else\n-      \/\/ Lacking registers and thread on x86_32. Always take slow path.\n-      jmp(slow_case);\n-#endif\n@@ -1540,2 +1321,2 @@\n-  Register arg3_reg = LP64_ONLY(c_rarg3) NOT_LP64(rcx);\n-  Register arg2_reg = LP64_ONLY(c_rarg2) NOT_LP64(rdx);\n+  Register arg3_reg = c_rarg3;\n+  Register arg2_reg = c_rarg2;\n@@ -1996,2 +1777,0 @@\n-      NOT_LP64(assert(reg2 == rdi, \"we know how to fix this blown reg\");)\n-      NOT_LP64(restore_locals();)         \/\/ Restore EDI\n@@ -2179,8 +1958,0 @@\n-void InterpreterMacroAssembler::verify_FPU(int stack_depth, TosState state) {\n-#ifndef _LP64\n-  if ((state == ftos && UseSSE < 1) ||\n-      (state == dtos && UseSSE < 2)) {\n-    MacroAssembler::verify_FPU(stack_depth);\n-  }\n-#endif\n-}\n@@ -2207,2 +1978,2 @@\n-  Register rthread = LP64_ONLY(r15_thread) NOT_LP64(rcx);\n-  Register rarg = LP64_ONLY(c_rarg1) NOT_LP64(rbx);\n+  Register rthread = r15_thread;\n+  Register rarg = c_rarg1;\n@@ -2211,1 +1982,0 @@\n-    NOT_LP64(get_thread(rthread);)\n@@ -2221,1 +1991,0 @@\n-    NOT_LP64(get_thread(rthread);)\n@@ -2229,1 +1998,0 @@\n-    NOT_LP64(get_thread(rthread);)\n@@ -2243,2 +2011,2 @@\n-  Register rthread = LP64_ONLY(r15_thread) NOT_LP64(rcx);\n-  Register rarg = LP64_ONLY(c_rarg1) NOT_LP64(rbx);\n+  Register rthread = r15_thread;\n+  Register rarg = c_rarg1;\n@@ -2254,1 +2022,0 @@\n-    NOT_LP64(get_thread(rthread);)\n@@ -2266,1 +2033,0 @@\n-    NOT_LP64(get_thread(rthread);)\n","filename":"src\/hotspot\/cpu\/x86\/interp_masm_x86.cpp","additions":27,"deletions":261,"binary":false,"changes":288,"status":"modified"},{"patch":"@@ -56,2 +56,2 @@\n-    _locals_register(LP64_ONLY(r14) NOT_LP64(rdi)),\n-    _bcp_register(LP64_ONLY(r13) NOT_LP64(rsi)) {}\n+    _locals_register(r14),\n+    _bcp_register(r13) {}\n@@ -124,3 +124,0 @@\n-  NOT_LP64(void f2ieee();)        \/\/ truncate ftos to 32bits\n-  NOT_LP64(void d2ieee();)        \/\/ truncate dtos to 64bits\n-\n@@ -146,1 +143,0 @@\n-#ifdef _LP64\n@@ -149,9 +145,0 @@\n-#else\n-  void pop_l(Register lo = rax, Register hi = rdx);\n-  void pop_f();\n-  void pop_d();\n-\n-  void push_l(Register lo = rax, Register hi = rdx);\n-  void push_d();\n-  void push_f();\n-#endif \/\/ _LP64\n@@ -171,1 +158,0 @@\n-    NOT_LP64(empty_FPU_stack());\n@@ -292,2 +278,0 @@\n-  \/\/ only if +VerifyFPU  && (state == ftos || state == dtos)\n-  void verify_FPU(int stack_depth, TosState state = ftos);\n","filename":"src\/hotspot\/cpu\/x86\/interp_masm_x86.hpp","additions":2,"deletions":18,"binary":false,"changes":20,"status":"modified"},{"patch":"@@ -2734,0 +2734,54 @@\n+void MacroAssembler::vmovdqu(XMMRegister dst, XMMRegister src, int vector_len) {\n+  if (vector_len == AVX_512bit) {\n+    evmovdquq(dst, src, AVX_512bit);\n+  } else if (vector_len == AVX_256bit) {\n+    vmovdqu(dst, src);\n+  } else {\n+    movdqu(dst, src);\n+  }\n+}\n+\n+void MacroAssembler::vmovdqu(Address dst, XMMRegister src, int vector_len) {\n+  if (vector_len == AVX_512bit) {\n+    evmovdquq(dst, src, AVX_512bit);\n+  } else if (vector_len == AVX_256bit) {\n+    vmovdqu(dst, src);\n+  } else {\n+    movdqu(dst, src);\n+  }\n+}\n+\n+void MacroAssembler::vmovdqu(XMMRegister dst, Address src, int vector_len) {\n+  if (vector_len == AVX_512bit) {\n+    evmovdquq(dst, src, AVX_512bit);\n+  } else if (vector_len == AVX_256bit) {\n+    vmovdqu(dst, src);\n+  } else {\n+    movdqu(dst, src);\n+  }\n+}\n+\n+void MacroAssembler::vmovdqa(XMMRegister dst, AddressLiteral src, Register rscratch) {\n+  assert(rscratch != noreg || always_reachable(src), \"missing\");\n+\n+  if (reachable(src)) {\n+    vmovdqa(dst, as_Address(src));\n+  }\n+  else {\n+    lea(rscratch, src);\n+    vmovdqa(dst, Address(rscratch, 0));\n+  }\n+}\n+\n+void MacroAssembler::vmovdqa(XMMRegister dst, AddressLiteral src, int vector_len, Register rscratch) {\n+  assert(rscratch != noreg || always_reachable(src), \"missing\");\n+\n+  if (vector_len == AVX_512bit) {\n+    evmovdqaq(dst, src, AVX_512bit, rscratch);\n+  } else if (vector_len == AVX_256bit) {\n+    vmovdqa(dst, src, rscratch);\n+  } else {\n+    movdqa(dst, src, rscratch);\n+  }\n+}\n+\n@@ -2858,0 +2912,23 @@\n+void MacroAssembler::evmovdqaq(XMMRegister dst, KRegister mask, AddressLiteral src, bool merge, int vector_len, Register rscratch) {\n+  assert(rscratch != noreg || always_reachable(src), \"missing\");\n+\n+  if (reachable(src)) {\n+    Assembler::evmovdqaq(dst, mask, as_Address(src), merge, vector_len);\n+  } else {\n+    lea(rscratch, src);\n+    Assembler::evmovdqaq(dst, mask, Address(rscratch, 0), merge, vector_len);\n+  }\n+}\n+\n+void MacroAssembler::evmovdqaq(XMMRegister dst, AddressLiteral src, int vector_len, Register rscratch) {\n+  assert(rscratch != noreg || always_reachable(src), \"missing\");\n+\n+  if (reachable(src)) {\n+    Assembler::evmovdqaq(dst, as_Address(src), vector_len);\n+  } else {\n+    lea(rscratch, src);\n+    Assembler::evmovdqaq(dst, Address(rscratch, 0), vector_len);\n+  }\n+}\n+\n+\n@@ -4281,6 +4358,1 @@\n-  const Register thread = LP64_ONLY(r15_thread) NOT_LP64(klass);\n-#ifndef _LP64\n-  if (UseTLAB) {\n-    get_thread(thread);\n-  }\n-#endif \/\/ _LP64\n+  const Register thread = r15_thread;\n@@ -4339,1 +4411,0 @@\n-        NOT_LP64(movptr(Address(new_obj, layout_size, Address::times_8, header_size_bytes - 2*oopSize), zero));\n@@ -4358,1 +4429,0 @@\n-#ifdef _LP64\n@@ -4361,1 +4431,0 @@\n-#endif\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.cpp","additions":78,"deletions":9,"binary":false,"changes":87,"status":"modified"},{"patch":"@@ -1406,0 +1406,8 @@\n+  void vmovdqu(XMMRegister dst, XMMRegister    src, int vector_len);\n+  void vmovdqu(XMMRegister dst, Address        src, int vector_len);\n+  void vmovdqu(Address     dst, XMMRegister    src, int vector_len);\n+\n+  \/\/ AVX Aligned forms\n+  using Assembler::vmovdqa;\n+  void vmovdqa(XMMRegister dst, AddressLiteral src,                 Register rscratch = noreg);\n+  void vmovdqa(XMMRegister dst, AddressLiteral src, int vector_len, Register rscratch = noreg);\n@@ -1462,0 +1470,1 @@\n+  void evmovdqaq(XMMRegister dst, AddressLiteral src, int vector_len, Register rscratch = noreg);\n@@ -1471,0 +1480,1 @@\n+  void evmovdqaq(XMMRegister dst, KRegister mask, AddressLiteral src, bool merge, int vector_len, Register rscratch = noreg);\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.hpp","additions":10,"deletions":0,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -4485,1 +4485,1 @@\n-  if (VM_Version::is_intel() && (VM_Version::supports_avx512dq() || VM_Version::supports_avx2())) {\n+  if (VM_Version::supports_avx512dq() || VM_Version::supports_avx2()) {\n@@ -4496,1 +4496,1 @@\n-      snprintf(ebuf_, sizeof(ebuf_), VM_Version::supports_avx512dq() ? \"avx512_sort\" : \"avx2_sort\");\n+      snprintf(ebuf_, sizeof(ebuf_), VM_Version::supports_avx512_simd_sort() ? \"avx512_sort\" : \"avx2_sort\");\n@@ -4499,1 +4499,1 @@\n-      snprintf(ebuf_, sizeof(ebuf_), VM_Version::supports_avx512dq() ? \"avx512_partition\" : \"avx2_partition\");\n+      snprintf(ebuf_, sizeof(ebuf_), VM_Version::supports_avx512_simd_sort() ? \"avx512_partition\" : \"avx2_partition\");\n","filename":"src\/hotspot\/cpu\/x86\/stubGenerator_x86_64.cpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -67,1 +67,0 @@\n-#ifdef AMD64\n@@ -69,3 +68,0 @@\n-#else\n-int TemplateInterpreter::InterpreterCodeSize = 224 * 1024;\n-#endif \/\/ AMD64\n@@ -74,2 +70,2 @@\n-static const Register rbcp     = LP64_ONLY(r13) NOT_LP64(rsi);\n-static const Register rlocals  = LP64_ONLY(r14) NOT_LP64(rdi);\n+static const Register rbcp     = r13;\n+static const Register rlocals  = r14;\n@@ -124,1 +120,0 @@\n-  Register rarg = NOT_LP64(rax) LP64_ONLY(c_rarg1);\n@@ -129,1 +124,1 @@\n-             rarg, rbx);\n+             c_rarg1, rbx);\n@@ -137,2 +132,1 @@\n-  Register rarg = NOT_LP64(rax) LP64_ONLY(c_rarg1);\n-  __ pop(rarg);\n+  __ pop(c_rarg1);\n@@ -148,1 +142,1 @@\n-             rarg);\n+             c_rarg1);\n@@ -157,3 +151,0 @@\n-  Register rarg = NOT_LP64(rax) LP64_ONLY(c_rarg1);\n-  Register rarg2 = NOT_LP64(rbx) LP64_ONLY(c_rarg2);\n-\n@@ -162,1 +153,1 @@\n-    __ pop(rarg2);\n+    __ pop(c_rarg2);\n@@ -168,1 +159,1 @@\n-  __ lea(rarg, ExternalAddress((address)name));\n+  __ lea(c_rarg1, ExternalAddress((address)name));\n@@ -173,1 +164,1 @@\n-               rarg, rarg2);\n+               c_rarg1, c_rarg2);\n@@ -175,1 +166,1 @@\n-    __ lea(rarg2, ExternalAddress((address)message));\n+    __ lea(c_rarg2, ExternalAddress((address)message));\n@@ -178,1 +169,1 @@\n-               rarg, rarg2);\n+               c_rarg1, c_rarg2);\n@@ -188,24 +179,0 @@\n-#ifndef _LP64\n-#ifdef COMPILER2\n-  \/\/ The FPU stack is clean if UseSSE >= 2 but must be cleaned in other cases\n-  if ((state == ftos && UseSSE < 1) || (state == dtos && UseSSE < 2)) {\n-    for (int i = 1; i < 8; i++) {\n-        __ ffree(i);\n-    }\n-  } else if (UseSSE < 2) {\n-    __ empty_FPU_stack();\n-  }\n-#endif \/\/ COMPILER2\n-  if ((state == ftos && UseSSE < 1) || (state == dtos && UseSSE < 2)) {\n-    __ MacroAssembler::verify_FPU(1, \"generate_return_entry_for compiled\");\n-  } else {\n-    __ MacroAssembler::verify_FPU(0, \"generate_return_entry_for compiled\");\n-  }\n-\n-  if (state == ftos) {\n-    __ MacroAssembler::verify_FPU(UseSSE >= 1 ? 0 : 1, \"generate_return_entry_for in interpreter\");\n-  } else if (state == dtos) {\n-    __ MacroAssembler::verify_FPU(UseSSE >= 2 ? 0 : 1, \"generate_return_entry_for in interpreter\");\n-  }\n-#endif \/\/ _LP64\n-\n@@ -244,3 +211,1 @@\n-   const Register java_thread = NOT_LP64(rcx) LP64_ONLY(r15_thread);\n-     NOT_LP64(__ get_thread(java_thread));\n-     __ check_and_handle_popframe(java_thread);\n+     __ check_and_handle_popframe(r15_thread);\n@@ -250,2 +215,1 @@\n-     NOT_LP64(__ get_thread(java_thread));\n-     __ check_and_handle_earlyret(java_thread);\n+     __ check_and_handle_earlyret(r15_thread);\n@@ -263,8 +227,0 @@\n-#ifndef _LP64\n-  if (state == ftos) {\n-    __ MacroAssembler::verify_FPU(UseSSE >= 1 ? 0 : 1, \"generate_deopt_entry_for in interpreter\");\n-  } else if (state == dtos) {\n-    __ MacroAssembler::verify_FPU(UseSSE >= 2 ? 0 : 1, \"generate_deopt_entry_for in interpreter\");\n-  }\n-#endif \/\/ _LP64\n-\n@@ -275,2 +231,1 @@\n-  const Register thread = NOT_LP64(rcx) LP64_ONLY(r15_thread);\n-  NOT_LP64(__ get_thread(thread));\n+  const Register thread = r15_thread;\n@@ -327,4 +282,0 @@\n-#ifndef _LP64\n-  case T_CHAR   : __ andptr(rax, 0xFFFF);    break;\n-#else\n-#endif \/\/ _LP64\n@@ -337,27 +288,0 @@\n-#ifndef _LP64\n-  case T_DOUBLE :\n-  case T_FLOAT  :\n-    { const Register t = InterpreterRuntime::SignatureHandlerGenerator::temp();\n-      __ pop(t);                            \/\/ remove return address first\n-      \/\/ Must return a result for interpreter or compiler. In SSE\n-      \/\/ mode, results are returned in xmm0 and the FPU stack must\n-      \/\/ be empty.\n-      if (type == T_FLOAT && UseSSE >= 1) {\n-        \/\/ Load ST0\n-        __ fld_d(Address(rsp, 0));\n-        \/\/ Store as float and empty fpu stack\n-        __ fstp_s(Address(rsp, 0));\n-        \/\/ and reload\n-        __ movflt(xmm0, Address(rsp, 0));\n-      } else if (type == T_DOUBLE && UseSSE >= 2 ) {\n-        __ movdbl(xmm0, Address(rsp, 0));\n-      } else {\n-        \/\/ restore ST0\n-        __ fld_d(Address(rsp, 0));\n-      }\n-      \/\/ and pop the temp\n-      __ addptr(rsp, 2 * wordSize);\n-      __ push(t);                           \/\/ restore return address\n-    }\n-    break;\n-#else\n@@ -366,1 +290,0 @@\n-#endif \/\/ _LP64\n@@ -475,2 +398,1 @@\n-  Register rarg = NOT_LP64(rax) LP64_ONLY(c_rarg1);\n-  __ movl(rarg, 0);\n+  __ movl(c_rarg1, 0);\n@@ -480,1 +402,1 @@\n-             rarg);\n+             c_rarg1);\n@@ -531,6 +453,1 @@\n-  const Register thread = NOT_LP64(rsi) LP64_ONLY(r15_thread);\n-#ifndef _LP64\n-  __ push(thread);\n-  __ get_thread(thread);\n-#endif\n-  const Address stack_limit(thread, JavaThread::stack_overflow_limit_offset());\n+  const Address stack_limit(r15_thread, JavaThread::stack_overflow_limit_offset());\n@@ -560,1 +477,0 @@\n-  NOT_LP64(__ pop(rsi));  \/\/ get saved bcp\n@@ -576,1 +492,0 @@\n-  NOT_LP64(__ pop(rsi));\n@@ -639,3 +554,2 @@\n-  const Register lockreg = NOT_LP64(rdx) LP64_ONLY(c_rarg1);\n-  __ movptr(lockreg, rsp); \/\/ object address\n-  __ lock_object(lockreg);\n+  __ movptr(c_rarg1, rsp); \/\/ object address\n+  __ lock_object(c_rarg1);\n@@ -655,2 +569,2 @@\n-  __ push(rax);        \/\/ save return address\n-  __ enter();          \/\/ save old & set new rbp\n+  __ push(rax);         \/\/ save return address\n+  __ enter();           \/\/ save old & set new rbp\n@@ -658,3 +572,9 @@\n-  __ push(NULL_WORD); \/\/ leave last_sp as null\n-  __ movptr(rbcp, Address(rbx, Method::const_offset()));      \/\/ get ConstMethod*\n-  __ lea(rbcp, Address(rbcp, ConstMethod::codes_offset())); \/\/ get codebase\n+\n+  \/\/ Resolve ConstMethod* -> ConstantPool*.\n+  \/\/ Get codebase, while we still have ConstMethod*.\n+  \/\/ Save ConstantPool* in rax for later use.\n+  __ movptr(rax, Address(rbx, Method::const_offset()));\n+  __ lea(rbcp, Address(rax, ConstMethod::codes_offset()));\n+  __ movptr(rax, Address(rax, ConstMethod::constants_offset()));\n+\n+  __ push(NULL_WORD);  \/\/ leave last_sp as null\n@@ -662,2 +582,6 @@\n-  \/\/ Get mirror and store it in the frame as GC root for this Method*\n-  __ load_mirror(rdx, rbx, rscratch2);\n+\n+  \/\/ Get mirror and store it in the frame as GC root for this Method*.\n+  \/\/ rax is still ConstantPool*, resolve ConstantPool* -> InstanceKlass* -> Java mirror.\n+  __ movptr(rdx, Address(rax, ConstantPool::pool_holder_offset()));\n+  __ movptr(rdx, Address(rdx, in_bytes(Klass::java_mirror_offset())));\n+  __ resolve_oop_handle(rdx, rscratch2);\n@@ -665,0 +589,1 @@\n+\n@@ -669,1 +594,1 @@\n-    __ jcc(Assembler::zero, method_data_continue);\n+    __ jccb(Assembler::zero, method_data_continue);\n@@ -674,1 +599,1 @@\n-    __ push(0);\n+    __ push(NULL_WORD);\n@@ -677,4 +602,3 @@\n-  __ movptr(rdx, Address(rbx, Method::const_offset()));\n-  __ movptr(rdx, Address(rdx, ConstMethod::constants_offset()));\n-  __ movptr(rdx, Address(rdx, ConstantPool::cache_offset()));\n-  __ push(rdx); \/\/ set constant pool cache\n+  \/\/ rax is still ConstantPool*, set the constant pool cache\n+  __ movptr(rdx, Address(rax, ConstantPool::cache_offset()));\n+  __ push(rdx);\n@@ -688,1 +612,1 @@\n-    __ push(0); \/\/ no bcp\n+    __ push(NULL_WORD); \/\/ no bcp\n@@ -736,4 +660,0 @@\n-  \/\/ Preserve the sender sp in case the load barrier\n-  \/\/ calls the runtime\n-  NOT_LP64(__ push(rsi));\n-\n@@ -745,3 +665,1 @@\n-  const Register sender_sp = NOT_LP64(rsi) LP64_ONLY(r13);\n-  NOT_LP64(__ pop(rsi));      \/\/ get sender sp\n-  __ mov(rsp, sender_sp);     \/\/ set sp to sender sp\n+  __ mov(rsp, r13);           \/\/ set sp to sender sp\n@@ -772,5 +690,1 @@\n-  const Register thread = NOT_LP64(rsi) LP64_ONLY(r15_thread);\n-#ifndef _LP64\n-  __ push(thread);\n-  __ get_thread(thread);\n-#endif\n+  const Register thread = r15_thread;\n@@ -808,4 +722,0 @@\n-\n-#ifndef _LP64\n-  __ pop(thread);\n-#endif\n@@ -885,3 +795,1 @@\n-  const Register thread1 = NOT_LP64(rax) LP64_ONLY(r15_thread);\n-  NOT_LP64(__ get_thread(thread1));\n-  const Address do_not_unlock_if_synchronized(thread1,\n+  const Address do_not_unlock_if_synchronized(r15_thread,\n@@ -903,1 +811,0 @@\n-  NOT_LP64(__ get_thread(thread1));\n@@ -945,2 +852,2 @@\n-  const Register thread = NOT_LP64(rdi) LP64_ONLY(r15_thread);\n-  const Register t      = NOT_LP64(rcx) LP64_ONLY(r11);\n+  const Register thread = r15_thread;\n+  const Register t      = r11;\n@@ -953,6 +860,0 @@\n-#ifndef _LP64\n-  __ shlptr(t, Interpreter::logStackElementSize); \/\/ Convert parameter count to bytes.\n-  __ addptr(t, 2*wordSize);     \/\/ allocate two more slots for JNIEnv and possible mirror\n-  __ subptr(rsp, t);\n-  __ andptr(rsp, -(StackAlignmentInBytes)); \/\/ gcc needs 16 byte aligned stacks to do XMM intrinsics\n-#else\n@@ -964,1 +865,0 @@\n-#endif \/\/ _LP64\n@@ -986,1 +886,1 @@\n-  assert(InterpreterRuntime::SignatureHandlerGenerator::temp() == NOT_LP64(t) LP64_ONLY(rscratch1),\n+  assert(InterpreterRuntime::SignatureHandlerGenerator::temp() == rscratch1,\n@@ -1015,4 +915,0 @@\n-#ifndef _LP64\n-    __ lea(t, Address(rbp, frame::interpreter_frame_oop_temp_offset * wordSize));\n-    __ movptr(Address(rsp, wordSize), t);\n-#else\n@@ -1021,1 +917,0 @@\n-#endif \/\/ _LP64\n@@ -1042,10 +937,0 @@\n-#ifndef _LP64\n-   __ get_thread(thread);\n-   __ lea(t, Address(thread, JavaThread::jni_environment_offset()));\n-   __ movptr(Address(rsp, 0), t);\n-\n-   \/\/ set_last_Java_frame_before_call\n-   \/\/ It is enough that the pc()\n-   \/\/ points into the right code segment. It does not have to be the correct return pc.\n-   __ set_last_Java_frame(thread, noreg, rbp, __ pc(), noreg);\n-#else\n@@ -1060,1 +945,0 @@\n-#endif \/\/ _LP64\n@@ -1096,28 +980,0 @@\n-#ifndef _LP64\n-  \/\/ save potential result in ST(0) & rdx:rax\n-  \/\/ (if result handler is the T_FLOAT or T_DOUBLE handler, result must be in ST0 -\n-  \/\/ the check is necessary to avoid potential Intel FPU overflow problems by saving\/restoring 'empty' FPU registers)\n-  \/\/ It is safe to do this push because state is _thread_in_native and return address will be found\n-  \/\/ via _last_native_pc and not via _last_jave_sp\n-\n-  \/\/ NOTE: the order of these push(es) is known to frame::interpreter_frame_result.\n-  \/\/ If the order changes or anything else is added to the stack the code in\n-  \/\/ interpreter_frame_result will have to be changed.\n-\n-  { Label L;\n-    Label push_double;\n-    ExternalAddress float_handler(AbstractInterpreter::result_handler(T_FLOAT));\n-    ExternalAddress double_handler(AbstractInterpreter::result_handler(T_DOUBLE));\n-    __ cmpptr(Address(rbp, (frame::interpreter_frame_result_handler_offset)*wordSize),\n-              float_handler.addr(), noreg);\n-    __ jcc(Assembler::equal, push_double);\n-    __ cmpptr(Address(rbp, (frame::interpreter_frame_result_handler_offset)*wordSize),\n-              double_handler.addr(), noreg);\n-    __ jcc(Assembler::notEqual, L);\n-    __ bind(push_double);\n-    __ push_d(); \/\/ FP values are returned using the FPU, so push FPU contents (even if UseSSE > 0).\n-    __ bind(L);\n-  }\n-#else\n-#endif \/\/ _LP64\n-\n@@ -1128,1 +984,0 @@\n-  NOT_LP64(__ get_thread(thread));\n@@ -1138,6 +993,0 @@\n-#ifndef _LP64\n-  if (AlwaysRestoreFPU) {\n-    \/\/  Make sure the control word is correct.\n-    __ fldcw(ExternalAddress(StubRoutines::x86::addr_fpu_cntrl_wrd_std()));\n-  }\n-#endif \/\/ _LP64\n@@ -1163,7 +1012,0 @@\n-#ifndef _LP64\n-    __ push(thread);\n-    __ call(RuntimeAddress(CAST_FROM_FN_PTR(address,\n-                                            JavaThread::check_special_condition_for_native_trans)));\n-    __ increment(rsp, wordSize);\n-    __ get_thread(thread);\n-#else\n@@ -1177,1 +1019,0 @@\n-#endif \/\/ _LP64\n@@ -1184,1 +1025,0 @@\n-#ifdef _LP64\n@@ -1200,1 +1040,0 @@\n-#endif \/\/ _LP64\n@@ -1242,4 +1081,0 @@\n-#ifndef _LP64\n-    __ call(RuntimeAddress(CAST_FROM_FN_PTR(address, SharedRuntime::reguard_yellow_pages)));\n-    __ popa();\n-#else\n@@ -1253,1 +1088,0 @@\n-#endif \/\/ _LP64\n@@ -1301,1 +1135,1 @@\n-      const Register regmon = NOT_LP64(rdx) LP64_ONLY(c_rarg1);\n+      const Register regmon = c_rarg1;\n@@ -1333,1 +1167,1 @@\n-  LP64_ONLY( __ pop(dtos));\n+  __ pop(dtos);\n@@ -1426,1 +1260,1 @@\n-    __ jcc(Assembler::lessEqual, exit); \/\/ do nothing if rdx <= 0\n+    __ jccb(Assembler::lessEqual, exit); \/\/ do nothing if rdx <= 0\n@@ -1430,1 +1264,1 @@\n-    __ jcc(Assembler::greater, loop);\n+    __ jccb(Assembler::greater, loop);\n@@ -1462,3 +1296,1 @@\n-  const Register thread = NOT_LP64(rax) LP64_ONLY(r15_thread);\n-  NOT_LP64(__ get_thread(thread));\n-  const Address do_not_unlock_if_synchronized(thread,\n+  const Address do_not_unlock_if_synchronized(r15_thread,\n@@ -1482,1 +1314,0 @@\n-  NOT_LP64(__ get_thread(thread));\n@@ -1549,1 +1380,1 @@\n-  LP64_ONLY(__ reinit_heapbase());  \/\/ restore r12 as heapbase.\n+  __ reinit_heapbase();  \/\/ restore r12 as heapbase.\n@@ -1556,2 +1387,1 @@\n-  Register rarg = NOT_LP64(rax) LP64_ONLY(c_rarg1);\n-  LP64_ONLY(__ mov(c_rarg1, rax));\n+  __ mov(c_rarg1, rax);\n@@ -1566,1 +1396,1 @@\n-             rarg);\n+             c_rarg1);\n@@ -1596,2 +1426,1 @@\n-  const Register thread = NOT_LP64(rcx) LP64_ONLY(r15_thread);\n-  NOT_LP64(__ get_thread(thread));\n+  const Register thread = r15_thread;\n@@ -1614,2 +1443,1 @@\n-    Register rarg = NOT_LP64(rdx) LP64_ONLY(c_rarg1);\n-    __ movptr(rarg, Address(rbp, frame::return_addr_offset * wordSize));\n+    __ movptr(c_rarg1, Address(rbp, frame::return_addr_offset * wordSize));\n@@ -1617,1 +1445,1 @@\n-                               InterpreterRuntime::interpreter_contains), rarg);\n+                               InterpreterRuntime::interpreter_contains), c_rarg1);\n@@ -1632,1 +1460,0 @@\n-    NOT_LP64(__ get_thread(thread));\n@@ -1645,1 +1472,0 @@\n-    NOT_LP64(__ get_thread(thread));\n@@ -1671,10 +1497,0 @@\n-#ifndef _LP64\n-  __ mov(rax, rsp);\n-  __ movptr(rbx, Address(rbp, frame::interpreter_frame_last_sp_offset * wordSize));\n-  __ lea(rbx, Address(rbp, rbx, Address::times_ptr));\n-  __ get_thread(thread);\n-  \/\/ PC must point into interpreter here\n-  __ set_last_Java_frame(thread, noreg, rbp, __ pc(), noreg);\n-  __ super_call_VM_leaf(CAST_FROM_FN_PTR(address, InterpreterRuntime::popframe_move_outgoing_args), thread, rax, rbx);\n-  __ get_thread(thread);\n-#else\n@@ -1687,1 +1503,0 @@\n-#endif\n@@ -1704,1 +1519,0 @@\n-  NOT_LP64(__ get_thread(thread));\n@@ -1738,1 +1552,0 @@\n-  NOT_LP64(__ get_thread(thread));\n@@ -1743,1 +1556,0 @@\n-  NOT_LP64(__ get_thread(thread));\n@@ -1779,3 +1591,1 @@\n-  const Register thread = NOT_LP64(rcx) LP64_ONLY(r15_thread);\n-  NOT_LP64(__ get_thread(thread));\n-  __ movptr(rcx, Address(thread, JavaThread::jvmti_thread_state_offset()));\n+  __ movptr(rcx, Address(r15_thread, JavaThread::jvmti_thread_state_offset()));\n@@ -1812,8 +1622,0 @@\n-#ifndef _LP64\n-  fep = __ pc();     \/\/ ftos entry point\n-      __ push(ftos);\n-      __ jmpb(L);\n-  dep = __ pc();     \/\/ dtos entry point\n-      __ push(dtos);\n-      __ jmpb(L);\n-#else\n@@ -1826,1 +1628,0 @@\n-#endif \/\/ _LP64\n@@ -1845,13 +1646,0 @@\n-#ifndef _LP64\n-  \/\/ prepare expression stack\n-  __ pop(rcx);          \/\/ pop return address so expression stack is 'pure'\n-  __ push(state);       \/\/ save tosca\n-\n-  \/\/ pass tosca registers as arguments & call tracer\n-  __ call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::trace_bytecode), rcx, rax, rdx);\n-  __ mov(rcx, rax);     \/\/ make sure return address is not destroyed by pop(state)\n-  __ pop(state);        \/\/ restore tosca\n-\n-  \/\/ return\n-  __ jmp(rcx);\n-#else\n@@ -1876,1 +1664,0 @@\n-#endif \/\/ _LP64\n@@ -1882,4 +1669,0 @@\n-  #ifndef _LP64\n-  __ incrementl(ExternalAddress((address) &BytecodeCounter::_counter_value), rscratch1);\n-  #else\n-  #endif\n@@ -1912,3 +1695,0 @@\n-#ifndef _LP64\n-  __ call(RuntimeAddress(Interpreter::trace_code(t->tos_in())));\n-#else\n@@ -1920,1 +1700,0 @@\n-#endif \/\/ _LP64\n@@ -1926,5 +1705,0 @@\n-  #ifndef _LP64\n-  __ cmp32(ExternalAddress((address) &BytecodeCounter::_counter_value),\n-           StopInterpreterAt,\n-           rscratch1);\n-  #else\n@@ -1933,1 +1707,0 @@\n-  #endif\n","filename":"src\/hotspot\/cpu\/x86\/templateInterpreterGenerator_x86.cpp","additions":60,"deletions":287,"binary":false,"changes":347,"status":"modified"},{"patch":"@@ -55,2 +55,2 @@\n-static const Register rbcp     = LP64_ONLY(r13) NOT_LP64(rsi);\n-static const Register rlocals  = LP64_ONLY(r14) NOT_LP64(rdi);\n+static const Register rbcp     = r13;\n+static const Register rlocals  = r14;\n@@ -67,6 +67,0 @@\n-#ifndef _LP64\n-static inline Address haddress(int n) {\n-  return iaddress(n + 0);\n-}\n-#endif\n-\n@@ -93,6 +87,0 @@\n-#ifndef _LP64\n-static inline Address haddress(Register r)       {\n-  return Address(rlocals, r, Interpreter::stackElementScale(), Interpreter::local_offset_in_bytes(0));\n-}\n-#endif\n-\n@@ -160,4 +148,1 @@\n-  __ store_heap_oop(dst, val,\n-                    NOT_LP64(rdx) LP64_ONLY(rscratch2),\n-                    NOT_LP64(rbx) LP64_ONLY(r9),\n-                    NOT_LP64(rsi) LP64_ONLY(r8), decorators);\n+  __ store_heap_oop(dst, val, rscratch2, r9, r8, decorators);\n@@ -290,4 +275,0 @@\n-#ifndef _LP64\n-  assert(value >= 0, \"check this code\");\n-  __ xorptr(rdx, rdx);\n-#endif\n@@ -317,8 +298,0 @@\n-#ifdef _LP64\n-#else\n-           if (value == 0) { __ fldz();\n-    } else if (value == 1) { __ fld1();\n-    } else if (value == 2) { __ fld1(); __ fld1(); __ faddp(); \/\/ should do a better solution here\n-    } else                 { ShouldNotReachHere();\n-    }\n-#endif \/\/ _LP64\n@@ -345,7 +318,0 @@\n-#ifdef _LP64\n-#else\n-           if (value == 0) { __ fldz();\n-    } else if (value == 1) { __ fld1();\n-    } else                 { ShouldNotReachHere();\n-    }\n-#endif\n@@ -370,1 +336,1 @@\n-  Register rarg = NOT_LP64(rcx) LP64_ONLY(c_rarg1);\n+  Register rarg = c_rarg1;\n@@ -438,1 +404,1 @@\n-  Register rarg = NOT_LP64(rcx) LP64_ONLY(c_rarg1);\n+  Register rarg = c_rarg1;\n@@ -501,1 +467,0 @@\n-  NOT_LP64(__ movptr(rdx, Address(rcx, rbx, Address::times_ptr, base_offset + 1 * wordSize)));\n@@ -515,1 +480,1 @@\n-  const Register rarg = NOT_LP64(rcx) LP64_ONLY(c_rarg1);\n+  const Register rarg = c_rarg1;\n@@ -518,7 +483,0 @@\n-#ifndef _LP64\n-  \/\/ borrow rdi from locals\n-  __ get_thread(rdi);\n-  __ get_vm_result_2(flags, rdi);\n-  __ restore_locals();\n-#else\n-#endif\n@@ -600,1 +558,0 @@\n-      NOT_LP64(__ movptr(rdx, field.plus_disp(4)));\n@@ -641,2 +598,2 @@\n-    const Register bc = LP64_ONLY(c_rarg3) NOT_LP64(rcx);\n-    LP64_ONLY(assert(rbx != bc, \"register damaged\"));\n+    const Register bc = c_rarg3;\n+    assert(rbx != bc, \"register damaged\");\n@@ -698,1 +655,0 @@\n-  NOT_LP64(__ movl(rdx, haddress(rbx)));\n@@ -736,1 +692,0 @@\n-  NOT_LP64(__ movl(rdx, haddress(rbx)));\n@@ -777,1 +732,1 @@\n-  __ mov(NOT_LP64(rax) LP64_ONLY(c_rarg1), array);\n+  __ mov(c_rarg1, array);\n@@ -798,1 +753,0 @@\n-  NOT_LP64(__ mov(rbx, rax));\n@@ -918,1 +872,0 @@\n-  NOT_LP64(__ movptr(rdx, haddress(n)));\n@@ -970,2 +923,2 @@\n-    const Register bc = LP64_ONLY(c_rarg3) NOT_LP64(rcx);\n-    LP64_ONLY(assert(rbx != bc, \"register damaged\"));\n+    const Register bc = c_rarg3;\n+    assert(rbx != bc, \"register damaged\");\n@@ -1025,1 +978,0 @@\n-  NOT_LP64(__ movptr(haddress(rbx), rdx));\n@@ -1056,2 +1008,1 @@\n-  NOT_LP64(__ pop_l(rax, rdx));\n-  LP64_ONLY(__ pop_l());\n+  __ pop_l();\n@@ -1060,1 +1011,0 @@\n-  NOT_LP64(__ movl(haddress(rbx), rdx));\n@@ -1064,1 +1014,0 @@\n-#ifdef _LP64\n@@ -1069,3 +1018,0 @@\n-#else\n-  wide_istore();\n-#endif\n@@ -1075,1 +1021,0 @@\n-#ifdef _LP64\n@@ -1080,3 +1025,0 @@\n-#else\n-  wide_lstore();\n-#endif\n@@ -1287,1 +1229,0 @@\n-  NOT_LP64(__ movptr(haddress(n), rdx));\n@@ -1426,1 +1367,0 @@\n-#ifdef _LP64\n@@ -1435,12 +1375,0 @@\n-#else\n-  __ pop_l(rbx, rcx);\n-  switch (op) {\n-    case add  : __ addl(rax, rbx); __ adcl(rdx, rcx); break;\n-    case sub  : __ subl(rbx, rax); __ sbbl(rcx, rdx);\n-                __ mov (rax, rbx); __ mov (rdx, rcx); break;\n-    case _and : __ andl(rax, rbx); __ andl(rdx, rcx); break;\n-    case _or  : __ orl (rax, rbx); __ orl (rdx, rcx); break;\n-    case _xor : __ xorl(rax, rbx); __ xorl(rdx, rcx); break;\n-    default   : ShouldNotReachHere();\n-  }\n-#endif\n@@ -1474,1 +1402,0 @@\n-#ifdef _LP64\n@@ -1477,7 +1404,0 @@\n-#else\n-  __ pop_l(rbx, rcx);\n-  __ push(rcx); __ push(rbx);\n-  __ push(rdx); __ push(rax);\n-  __ lmul(2 * wordSize, 0);\n-  __ addptr(rsp, 4 * wordSize);  \/\/ take off temporaries\n-#endif\n@@ -1488,1 +1408,0 @@\n-#ifdef _LP64\n@@ -1500,11 +1419,0 @@\n-#else\n-  __ pop_l(rbx, rcx);\n-  __ push(rcx); __ push(rbx);\n-  __ push(rdx); __ push(rax);\n-  \/\/ check if y = 0\n-  __ orl(rax, rdx);\n-  __ jump_cc(Assembler::zero,\n-             RuntimeAddress(Interpreter::_throw_ArithmeticException_entry));\n-  __ call_VM_leaf(CAST_FROM_FN_PTR(address, SharedRuntime::ldiv));\n-  __ addptr(rsp, 4 * wordSize);  \/\/ take off temporaries\n-#endif\n@@ -1515,1 +1423,0 @@\n-#ifdef _LP64\n@@ -1527,11 +1434,0 @@\n-#else\n-  __ pop_l(rbx, rcx);\n-  __ push(rcx); __ push(rbx);\n-  __ push(rdx); __ push(rax);\n-  \/\/ check if y = 0\n-  __ orl(rax, rdx);\n-  __ jump_cc(Assembler::zero,\n-             RuntimeAddress(Interpreter::_throw_ArithmeticException_entry));\n-  __ call_VM_leaf(CAST_FROM_FN_PTR(address, SharedRuntime::lrem));\n-  __ addptr(rsp, 4 * wordSize);\n-#endif\n@@ -1543,1 +1439,0 @@\n-  #ifdef _LP64\n@@ -1546,4 +1441,0 @@\n-#else\n-  __ pop_l(rax, rdx);                            \/\/ get shift value\n-  __ lshl(rdx, rax);\n-#endif\n@@ -1553,1 +1444,0 @@\n-#ifdef _LP64\n@@ -1558,6 +1448,0 @@\n-#else\n-  transition(itos, ltos);\n-  __ mov(rcx, rax);                              \/\/ get shift count\n-  __ pop_l(rax, rdx);                            \/\/ get shift value\n-  __ lshr(rdx, rax, true);\n-#endif\n@@ -1568,1 +1452,0 @@\n-#ifdef _LP64\n@@ -1572,5 +1455,0 @@\n-#else\n-  __ mov(rcx, rax);                              \/\/ get shift count\n-  __ pop_l(rax, rdx);                            \/\/ get shift value\n-  __ lshr(rdx, rax);\n-#endif\n@@ -1608,7 +1486,0 @@\n-      \/\/\n-      \/\/ On x86_32 platforms the FPU is used to perform the modulo operation. The\n-      \/\/ reason is that on 32-bit Windows the sign of modulo operations diverges from\n-      \/\/ what is considered the standard (e.g., -0.0f % -3.14f is 0.0f (and not -0.0f).\n-      \/\/ The fprem instruction used on x86_32 is functionally equivalent to\n-      \/\/ SharedRuntime::frem in that it returns a NaN.\n-#ifdef _LP64\n@@ -1618,10 +1489,0 @@\n-#else \/\/ !_LP64\n-      __ push_f(xmm0);\n-      __ pop_f();\n-      __ fld_s(at_rsp());\n-      __ fremr(rax);\n-      __ f2ieee();\n-      __ pop(rax);  \/\/ pop second operand off the stack\n-      __ push_f();\n-      __ pop_f(xmm0);\n-#endif \/\/ _LP64\n@@ -1634,13 +1495,0 @@\n-#ifdef _LP64\n-#else \/\/ !_LP64\n-    switch (op) {\n-    case add: __ fadd_s (at_rsp());                break;\n-    case sub: __ fsubr_s(at_rsp());                break;\n-    case mul: __ fmul_s (at_rsp());                break;\n-    case div: __ fdivr_s(at_rsp());                break;\n-    case rem: __ fld_s  (at_rsp()); __ fremr(rax); break;\n-    default : ShouldNotReachHere();\n-    }\n-    __ f2ieee();\n-    __ pop(rax);  \/\/ pop second operand off the stack\n-#endif \/\/ _LP64\n@@ -1675,3 +1523,2 @@\n-      \/\/ SharedRuntime::drem method (on x86_64 platforms) or using the\n-      \/\/ FPU (on x86_32 platforms) for the same reasons as mentioned in fop2().\n-#ifdef _LP64\n+      \/\/ SharedRuntime::drem method on x86_64 platforms for the same reasons\n+      \/\/ as mentioned in fop2().\n@@ -1681,11 +1528,0 @@\n-#else \/\/ !_LP64\n-      __ push_d(xmm0);\n-      __ pop_d();\n-      __ fld_d(at_rsp());\n-      __ fremr(rax);\n-      __ d2ieee();\n-      __ pop(rax);\n-      __ pop(rdx);\n-      __ push_d();\n-      __ pop_d(xmm0);\n-#endif \/\/ _LP64\n@@ -1698,31 +1534,0 @@\n-#ifdef _LP64\n-#else \/\/ !_LP64\n-    switch (op) {\n-    case add: __ fadd_d (at_rsp());                break;\n-    case sub: __ fsubr_d(at_rsp());                break;\n-    case mul: {\n-      \/\/ strict semantics\n-      __ fld_x(ExternalAddress(StubRoutines::x86::addr_fpu_subnormal_bias1()));\n-      __ fmulp();\n-      __ fmul_d (at_rsp());\n-      __ fld_x(ExternalAddress(StubRoutines::x86::addr_fpu_subnormal_bias2()));\n-      __ fmulp();\n-      break;\n-    }\n-    case div: {\n-      \/\/ strict semantics\n-      __ fld_x(ExternalAddress(StubRoutines::x86::addr_fpu_subnormal_bias1()));\n-      __ fmul_d (at_rsp());\n-      __ fdivrp();\n-      __ fld_x(ExternalAddress(StubRoutines::x86::addr_fpu_subnormal_bias2()));\n-      __ fmulp();\n-      break;\n-    }\n-    case rem: __ fld_d  (at_rsp()); __ fremr(rax); break;\n-    default : ShouldNotReachHere();\n-    }\n-    __ d2ieee();\n-    \/\/ Pop double precision number from rsp.\n-    __ pop(rax);\n-    __ pop(rdx);\n-#endif \/\/ _LP64\n@@ -1740,2 +1545,1 @@\n-  LP64_ONLY(__ negq(rax));\n-  NOT_LP64(__ lneg(rdx, rax));\n+  __ negq(rax);\n@@ -1765,2 +1569,1 @@\n-    LP64_ONLY(ShouldNotReachHere());\n-    NOT_LP64(__ fchs());\n+    ShouldNotReachHere();\n@@ -1777,4 +1580,0 @@\n-#ifdef _LP64\n-#else\n-    __ fchs();\n-#endif\n@@ -1804,1 +1603,0 @@\n-#ifdef _LP64\n@@ -1932,192 +1730,0 @@\n-#else \/\/ !_LP64\n-  \/\/ Checking\n-#ifdef ASSERT\n-  { TosState tos_in  = ilgl;\n-    TosState tos_out = ilgl;\n-    switch (bytecode()) {\n-      case Bytecodes::_i2l: \/\/ fall through\n-      case Bytecodes::_i2f: \/\/ fall through\n-      case Bytecodes::_i2d: \/\/ fall through\n-      case Bytecodes::_i2b: \/\/ fall through\n-      case Bytecodes::_i2c: \/\/ fall through\n-      case Bytecodes::_i2s: tos_in = itos; break;\n-      case Bytecodes::_l2i: \/\/ fall through\n-      case Bytecodes::_l2f: \/\/ fall through\n-      case Bytecodes::_l2d: tos_in = ltos; break;\n-      case Bytecodes::_f2i: \/\/ fall through\n-      case Bytecodes::_f2l: \/\/ fall through\n-      case Bytecodes::_f2d: tos_in = ftos; break;\n-      case Bytecodes::_d2i: \/\/ fall through\n-      case Bytecodes::_d2l: \/\/ fall through\n-      case Bytecodes::_d2f: tos_in = dtos; break;\n-      default             : ShouldNotReachHere();\n-    }\n-    switch (bytecode()) {\n-      case Bytecodes::_l2i: \/\/ fall through\n-      case Bytecodes::_f2i: \/\/ fall through\n-      case Bytecodes::_d2i: \/\/ fall through\n-      case Bytecodes::_i2b: \/\/ fall through\n-      case Bytecodes::_i2c: \/\/ fall through\n-      case Bytecodes::_i2s: tos_out = itos; break;\n-      case Bytecodes::_i2l: \/\/ fall through\n-      case Bytecodes::_f2l: \/\/ fall through\n-      case Bytecodes::_d2l: tos_out = ltos; break;\n-      case Bytecodes::_i2f: \/\/ fall through\n-      case Bytecodes::_l2f: \/\/ fall through\n-      case Bytecodes::_d2f: tos_out = ftos; break;\n-      case Bytecodes::_i2d: \/\/ fall through\n-      case Bytecodes::_l2d: \/\/ fall through\n-      case Bytecodes::_f2d: tos_out = dtos; break;\n-      default             : ShouldNotReachHere();\n-    }\n-    transition(tos_in, tos_out);\n-  }\n-#endif \/\/ ASSERT\n-\n-  \/\/ Conversion\n-  \/\/ (Note: use push(rcx)\/pop(rcx) for 1\/2-word stack-ptr manipulation)\n-  switch (bytecode()) {\n-    case Bytecodes::_i2l:\n-      __ extend_sign(rdx, rax);\n-      break;\n-    case Bytecodes::_i2f:\n-      if (UseSSE >= 1) {\n-        __ cvtsi2ssl(xmm0, rax);\n-      } else {\n-        __ push(rax);          \/\/ store int on tos\n-        __ fild_s(at_rsp());   \/\/ load int to ST0\n-        __ f2ieee();           \/\/ truncate to float size\n-        __ pop(rcx);           \/\/ adjust rsp\n-      }\n-      break;\n-    case Bytecodes::_i2d:\n-      if (UseSSE >= 2) {\n-        __ cvtsi2sdl(xmm0, rax);\n-      } else {\n-      __ push(rax);          \/\/ add one slot for d2ieee()\n-      __ push(rax);          \/\/ store int on tos\n-      __ fild_s(at_rsp());   \/\/ load int to ST0\n-      __ d2ieee();           \/\/ truncate to double size\n-      __ pop(rcx);           \/\/ adjust rsp\n-      __ pop(rcx);\n-      }\n-      break;\n-    case Bytecodes::_i2b:\n-      __ shll(rax, 24);      \/\/ truncate upper 24 bits\n-      __ sarl(rax, 24);      \/\/ and sign-extend byte\n-      LP64_ONLY(__ movsbl(rax, rax));\n-      break;\n-    case Bytecodes::_i2c:\n-      __ andl(rax, 0xFFFF);  \/\/ truncate upper 16 bits\n-      LP64_ONLY(__ movzwl(rax, rax));\n-      break;\n-    case Bytecodes::_i2s:\n-      __ shll(rax, 16);      \/\/ truncate upper 16 bits\n-      __ sarl(rax, 16);      \/\/ and sign-extend short\n-      LP64_ONLY(__ movswl(rax, rax));\n-      break;\n-    case Bytecodes::_l2i:\n-      \/* nothing to do *\/\n-      break;\n-    case Bytecodes::_l2f:\n-      \/\/ On 64-bit platforms, the cvtsi2ssq instruction is used to convert\n-      \/\/ 64-bit long values to floats. On 32-bit platforms it is not possible\n-      \/\/ to use that instruction with 64-bit operands, therefore the FPU is\n-      \/\/ used to perform the conversion.\n-      __ push(rdx);          \/\/ store long on tos\n-      __ push(rax);\n-      __ fild_d(at_rsp());   \/\/ load long to ST0\n-      __ f2ieee();           \/\/ truncate to float size\n-      __ pop(rcx);           \/\/ adjust rsp\n-      __ pop(rcx);\n-      if (UseSSE >= 1) {\n-        __ push_f();\n-        __ pop_f(xmm0);\n-      }\n-      break;\n-    case Bytecodes::_l2d:\n-      \/\/ On 32-bit platforms the FPU is used for conversion because on\n-      \/\/ 32-bit platforms it is not not possible to use the cvtsi2sdq\n-      \/\/ instruction with 64-bit operands.\n-      __ push(rdx);          \/\/ store long on tos\n-      __ push(rax);\n-      __ fild_d(at_rsp());   \/\/ load long to ST0\n-      __ d2ieee();           \/\/ truncate to double size\n-      __ pop(rcx);           \/\/ adjust rsp\n-      __ pop(rcx);\n-      if (UseSSE >= 2) {\n-        __ push_d();\n-        __ pop_d(xmm0);\n-      }\n-      break;\n-    case Bytecodes::_f2i:\n-      \/\/ SharedRuntime::f2i does not differentiate between sNaNs and qNaNs\n-      \/\/ as it returns 0 for any NaN.\n-      if (UseSSE >= 1) {\n-        __ push_f(xmm0);\n-      } else {\n-        __ push(rcx);          \/\/ reserve space for argument\n-        __ fstp_s(at_rsp());   \/\/ pass float argument on stack\n-      }\n-      __ call_VM_leaf(CAST_FROM_FN_PTR(address, SharedRuntime::f2i), 1);\n-      break;\n-    case Bytecodes::_f2l:\n-      \/\/ SharedRuntime::f2l does not differentiate between sNaNs and qNaNs\n-      \/\/ as it returns 0 for any NaN.\n-      if (UseSSE >= 1) {\n-       __ push_f(xmm0);\n-      } else {\n-        __ push(rcx);          \/\/ reserve space for argument\n-        __ fstp_s(at_rsp());   \/\/ pass float argument on stack\n-      }\n-      __ call_VM_leaf(CAST_FROM_FN_PTR(address, SharedRuntime::f2l), 1);\n-      break;\n-    case Bytecodes::_f2d:\n-      if (UseSSE < 1) {\n-        \/* nothing to do *\/\n-      } else if (UseSSE == 1) {\n-        __ push_f(xmm0);\n-        __ pop_f();\n-      } else { \/\/ UseSSE >= 2\n-        __ cvtss2sd(xmm0, xmm0);\n-      }\n-      break;\n-    case Bytecodes::_d2i:\n-      if (UseSSE >= 2) {\n-        __ push_d(xmm0);\n-      } else {\n-        __ push(rcx);          \/\/ reserve space for argument\n-        __ push(rcx);\n-        __ fstp_d(at_rsp());   \/\/ pass double argument on stack\n-      }\n-      __ call_VM_leaf(CAST_FROM_FN_PTR(address, SharedRuntime::d2i), 2);\n-      break;\n-    case Bytecodes::_d2l:\n-      if (UseSSE >= 2) {\n-        __ push_d(xmm0);\n-      } else {\n-        __ push(rcx);          \/\/ reserve space for argument\n-        __ push(rcx);\n-        __ fstp_d(at_rsp());   \/\/ pass double argument on stack\n-      }\n-      __ call_VM_leaf(CAST_FROM_FN_PTR(address, SharedRuntime::d2l), 2);\n-      break;\n-    case Bytecodes::_d2f:\n-      if (UseSSE <= 1) {\n-        __ push(rcx);          \/\/ reserve space for f2ieee()\n-        __ f2ieee();           \/\/ truncate to float size\n-        __ pop(rcx);           \/\/ adjust rsp\n-        if (UseSSE == 1) {\n-          \/\/ The cvtsd2ss instruction is not available if UseSSE==1, therefore\n-          \/\/ the conversion is performed using the FPU in this case.\n-          __ push_f();\n-          __ pop_f(xmm0);\n-        }\n-      } else { \/\/ UseSSE >= 2\n-        __ cvtsd2ss(xmm0, xmm0);\n-      }\n-      break;\n-    default             :\n-      ShouldNotReachHere();\n-  }\n-#endif \/\/ _LP64\n@@ -2128,1 +1734,0 @@\n-#ifdef _LP64\n@@ -2137,7 +1742,0 @@\n-#else\n-\n-  \/\/ y = rdx:rax\n-  __ pop_l(rbx, rcx);             \/\/ get x = rcx:rbx\n-  __ lcmp2int(rcx, rbx, rdx, rax);\/\/ rcx := cmp(x, y)\n-  __ mov(rax, rcx);\n-#endif\n@@ -2175,11 +1773,0 @@\n-#ifdef _LP64\n-#else \/\/ !_LP64\n-    if (is_float) {\n-      __ fld_s(at_rsp());\n-    } else {\n-      __ fld_d(at_rsp());\n-      __ pop(rdx);\n-    }\n-    __ pop(rcx);\n-    __ fcmp2int(rax, unordered_result < 0);\n-#endif \/\/ _LP64\n@@ -2211,1 +1798,1 @@\n-  LP64_ONLY(__ movl2ptr(rdx, rdx));\n+  __ movl2ptr(rdx, rdx);\n@@ -2330,2 +1917,0 @@\n-      NOT_LP64(__ get_thread(rcx));\n-\n@@ -2335,2 +1920,1 @@\n-      LP64_ONLY(__ mov(j_rarg0, rax));\n-      NOT_LP64(__ mov(rcx, rax));\n+      __ mov(j_rarg0, rax);\n@@ -2341,2 +1925,2 @@\n-      const Register retaddr   = LP64_ONLY(j_rarg2) NOT_LP64(rdi);\n-      const Register sender_sp = LP64_ONLY(j_rarg1) NOT_LP64(rdx);\n+      const Register retaddr   = j_rarg2;\n+      const Register sender_sp = j_rarg1;\n@@ -2459,2 +2043,1 @@\n-  LP64_ONLY(__ movslq(rbx, iaddress(rbx))); \/\/ get return bci, compute return bcp\n-  NOT_LP64(__ movptr(rbx, iaddress(rbx)));\n+  __ movslq(rbx, iaddress(rbx)); \/\/ get return bci, compute return bcp\n@@ -2504,1 +2087,1 @@\n-  LP64_ONLY(__ movl2ptr(rdx, rdx));\n+  __ movl2ptr(rdx, rdx);\n@@ -2594,2 +2177,0 @@\n-  NOT_LP64(__ save_bcp());\n-\n@@ -2652,4 +2233,1 @@\n-  LP64_ONLY(__ movslq(j, j));\n-\n-  NOT_LP64(__ restore_bcp());\n-  NOT_LP64(__ restore_locals());                           \/\/ restore rdi\n+  __ movslq(j, j);\n@@ -2666,4 +2244,1 @@\n-  LP64_ONLY(__ movslq(j, j));\n-\n-  NOT_LP64(__ restore_bcp());\n-  NOT_LP64(__ restore_locals());\n+  __ movslq(j, j);\n@@ -2684,1 +2259,1 @@\n-    Register robj = LP64_ONLY(c_rarg1) NOT_LP64(rax);\n+    Register robj = c_rarg1;\n@@ -2699,6 +2274,0 @@\n-#ifdef _LP64\n-#else\n-    const Register thread = rdi;\n-    __ get_thread(thread);\n-    __ testb(Address(thread, JavaThread::polling_word_offset()), SafepointMechanism::poll_bit());\n-#endif\n@@ -2804,1 +2373,1 @@\n-    const Register thread = LP64_ONLY(r15_thread) NOT_LP64(noreg);\n+    const Register thread = r15_thread;\n@@ -2931,1 +2500,0 @@\n-#ifdef _LP64\n@@ -2934,3 +2502,0 @@\n-#else\n-    __ movptr(index, ArrayAddress(table, Address(noreg, index, Address::times_ptr)));\n-#endif \/\/ _LP64\n@@ -3089,1 +2654,1 @@\n-  const Register obj   = LP64_ONLY(r9) NOT_LP64(rcx);\n+  const Register obj   = r9;\n@@ -3095,1 +2660,1 @@\n-  const Register bc    = LP64_ONLY(c_rarg3) NOT_LP64(rcx); \/\/ uses same reg as obj, so don't mix them\n+  const Register bc    = c_rarg3; \/\/ uses same reg as obj, so don't mix them\n@@ -3255,1 +2820,1 @@\n-  LP64_ONLY(if (!is_static && rc == may_rewrite) patch_bytecode(Bytecodes::_fast_lgetfield, bc, rbx));\n+  if (!is_static && rc == may_rewrite) patch_bytecode(Bytecodes::_fast_lgetfield, bc, rbx);\n@@ -3314,3 +2879,3 @@\n-  const Register entry = LP64_ONLY(c_rarg2) NOT_LP64(rax); \/\/ ResolvedFieldEntry\n-  const Register obj = LP64_ONLY(c_rarg1) NOT_LP64(rbx);   \/\/ Object pointer\n-  const Register value = LP64_ONLY(c_rarg3) NOT_LP64(rcx); \/\/ JValue object\n+  const Register entry = c_rarg2; \/\/ ResolvedFieldEntry\n+  const Register obj = c_rarg1;   \/\/ Object pointer\n+  const Register value = c_rarg3; \/\/ JValue object\n@@ -3338,4 +2903,0 @@\n-#ifndef _LP64\n-      Label two_word, valsize_known;\n-#endif\n-#ifdef _LP64\n@@ -3350,16 +2911,0 @@\n-#else\n-      __ mov(obj, rsp);\n-      __ cmpl(value, ltos);\n-      __ jccb(Assembler::equal, two_word);\n-      __ cmpl(value, dtos);\n-      __ jccb(Assembler::equal, two_word);\n-      __ addptr(obj, Interpreter::expr_offset_in_bytes(1)); \/\/ one word jvalue (not ltos, dtos)\n-      __ jmpb(valsize_known);\n-\n-      __ bind(two_word);\n-      __ addptr(obj, Interpreter::expr_offset_in_bytes(2)); \/\/ two words jvalue\n-\n-      __ bind(valsize_known);\n-      \/\/ setup object pointer\n-      __ movptr(obj, Address(obj, 0));\n-#endif\n@@ -3425,1 +2970,0 @@\n-  NOT_LP64( const Address hi(obj, off, Address::times_1, 1*wordSize);)\n@@ -3431,1 +2975,1 @@\n-  const Register bc    = LP64_ONLY(c_rarg3) NOT_LP64(rcx);\n+  const Register bc    = c_rarg3;\n@@ -3590,1 +3134,0 @@\n-#ifdef _LP64\n@@ -3594,1 +3137,0 @@\n-#endif \/\/ _LP64\n@@ -3655,1 +3197,1 @@\n-  const Register scratch = LP64_ONLY(c_rarg3) NOT_LP64(rcx);\n+  const Register scratch = c_rarg3;\n@@ -3687,2 +3229,1 @@\n-    LP64_ONLY(__ load_field_entry(c_rarg2, rax));\n-    NOT_LP64(__ load_field_entry(rax, rdx));\n+    __ load_field_entry(c_rarg2, rax);\n@@ -3693,2 +3234,1 @@\n-    LP64_ONLY(__ call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::post_field_modification), rbx, c_rarg2, c_rarg3));\n-    NOT_LP64(__ call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::post_field_modification), rbx, rax, rcx));\n+    __ call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::post_field_modification), rbx, c_rarg2, c_rarg3);\n@@ -3788,4 +3328,0 @@\n-#ifdef _LP64\n-#else\n-  __ stop(\"should not be rewritten\");\n-#endif\n@@ -3832,2 +3368,1 @@\n-    LP64_ONLY(__ load_field_entry(c_rarg2, rcx));\n-    NOT_LP64(__ load_field_entry(rcx, rdx));\n+    __ load_field_entry(c_rarg2, rcx);\n@@ -3836,1 +3371,1 @@\n-    LP64_ONLY(__ mov(c_rarg1, rax));\n+    __ mov(c_rarg1, rax);\n@@ -3839,2 +3374,1 @@\n-    LP64_ONLY(__ call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::post_field_access), c_rarg1, c_rarg2));\n-    NOT_LP64(__ call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::post_field_access), rax, rcx));\n+    __ call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::post_field_access), c_rarg1, c_rarg2);\n@@ -3887,4 +3421,0 @@\n-#ifdef _LP64\n-#else\n-  __ stop(\"should not be rewritten\");\n-#endif\n@@ -3996,1 +3526,0 @@\n-#ifdef _LP64\n@@ -3999,3 +3528,0 @@\n-#else\n-    __ movptr(flags, ArrayAddress(table, Address(noreg, flags, Address::times_ptr)));\n-#endif \/\/ _LP64\n@@ -4233,1 +3759,0 @@\n-#ifdef _LP64\n@@ -4238,4 +3763,0 @@\n-#else\n-  recvKlass = rdx;\n-  Register method    = rcx;\n-#endif\n@@ -4253,1 +3774,3 @@\n-  LP64_ONLY( if (recvKlass != rdx) { __ movq(recvKlass, rdx); } )\n+  if (recvKlass != rdx) {\n+    __ movq(recvKlass, rdx);\n+  }\n@@ -4333,1 +3856,0 @@\n-#ifdef _LP64\n@@ -4336,4 +3858,0 @@\n-#else\n-  __ cmpb(Address(rcx, InstanceKlass::init_state_offset()), InstanceKlass::fully_initialized);\n-  __ jcc(Assembler::notEqual, slow_case);\n-#endif\n@@ -4353,6 +3871,3 @@\n-  Register rarg1 = LP64_ONLY(c_rarg1) NOT_LP64(rax);\n-  Register rarg2 = LP64_ONLY(c_rarg2) NOT_LP64(rdx);\n-\n-  __ get_constant_pool(rarg1);\n-  __ get_unsigned_2_byte_index_at_bcp(rarg2, 1);\n-  call_VM(rax, CAST_FROM_FN_PTR(address, InterpreterRuntime::_new), rarg1, rarg2);\n+  __ get_constant_pool(c_rarg1);\n+  __ get_unsigned_2_byte_index_at_bcp(c_rarg2, 1);\n+  call_VM(rax, CAST_FROM_FN_PTR(address, InterpreterRuntime::_new), c_rarg1, c_rarg2);\n@@ -4368,2 +3883,1 @@\n-  Register rarg1 = LP64_ONLY(c_rarg1) NOT_LP64(rdx);\n-  __ load_unsigned_byte(rarg1, at_bcp(1));\n+  __ load_unsigned_byte(c_rarg1, at_bcp(1));\n@@ -4371,1 +3885,1 @@\n-          rarg1, rax);\n+          c_rarg1, rax);\n@@ -4377,5 +3891,2 @@\n-  Register rarg1 = LP64_ONLY(c_rarg1) NOT_LP64(rcx);\n-  Register rarg2 = LP64_ONLY(c_rarg2) NOT_LP64(rdx);\n-\n-  __ get_unsigned_2_byte_index_at_bcp(rarg2, 1);\n-  __ get_constant_pool(rarg1);\n+  __ get_unsigned_2_byte_index_at_bcp(c_rarg2, 1);\n+  __ get_constant_pool(c_rarg1);\n@@ -4383,1 +3894,1 @@\n-          rarg1, rarg2, rax);\n+          c_rarg1, c_rarg2, rax);\n@@ -4410,7 +3921,0 @@\n-#ifndef _LP64\n-  \/\/ borrow rdi from locals\n-  __ get_thread(rdi);\n-  __ get_vm_result_2(rax, rdi);\n-  __ restore_locals();\n-#else\n-#endif\n@@ -4474,7 +3978,0 @@\n-#ifndef _LP64\n-  \/\/ borrow rdi from locals\n-  __ get_thread(rdi);\n-  __ get_vm_result_2(rax, rdi);\n-  __ restore_locals();\n-#else\n-#endif\n@@ -4528,3 +4025,1 @@\n-  Register rarg = LP64_ONLY(c_rarg1) NOT_LP64(rcx);\n-\n-  __ get_method(rarg);\n+  __ get_method(c_rarg1);\n@@ -4535,1 +4030,1 @@\n-             rarg, rbcp);\n+             c_rarg1, rbcp);\n@@ -4539,1 +4034,1 @@\n-  __ get_method(rarg);\n+  __ get_method(c_rarg1);\n@@ -4542,1 +4037,1 @@\n-             rarg, rbcp);\n+             c_rarg1, rbcp);\n@@ -4592,3 +4087,3 @@\n-  Register rtop = LP64_ONLY(c_rarg3) NOT_LP64(rcx);\n-  Register rbot = LP64_ONLY(c_rarg2) NOT_LP64(rbx);\n-  Register rmon = LP64_ONLY(c_rarg1) NOT_LP64(rdx);\n+  Register rtop = c_rarg3;\n+  Register rbot = c_rarg2;\n+  Register rmon = c_rarg1;\n@@ -4706,2 +4201,2 @@\n-  Register rtop = LP64_ONLY(c_rarg1) NOT_LP64(rdx);\n-  Register rbot = LP64_ONLY(c_rarg2) NOT_LP64(rbx);\n+  Register rtop = c_rarg1;\n+  Register rbot = c_rarg2;\n@@ -4761,1 +4256,0 @@\n-  Register rarg = LP64_ONLY(c_rarg1) NOT_LP64(rax);\n@@ -4766,2 +4260,2 @@\n-  __ lea(rarg, Address(rsp, rax, Interpreter::stackElementScale(), -wordSize));\n-  call_VM(rax, CAST_FROM_FN_PTR(address, InterpreterRuntime::multianewarray), rarg);\n+  __ lea(c_rarg1, Address(rsp, rax, Interpreter::stackElementScale(), -wordSize));\n+  call_VM(rax, CAST_FROM_FN_PTR(address, InterpreterRuntime::multianewarray), c_rarg1);\n","filename":"src\/hotspot\/cpu\/x86\/templateTable_x86.cpp","additions":61,"deletions":567,"binary":false,"changes":628,"status":"modified"},{"patch":"@@ -1409,1 +1409,1 @@\n-  if (supports_avx512ifma() && supports_avx512vlbw()) {\n+  if ((supports_avx512ifma() && supports_avx512vlbw()) || supports_avxifma()) {\n","filename":"src\/hotspot\/cpu\/x86\/vm_version_x86.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -4230,2 +4230,0 @@\n-        strcmp(opType,\"RoundDouble\")==0 ||\n-        strcmp(opType,\"RoundFloat\")==0 ||\n","filename":"src\/hotspot\/share\/adlc\/formssel.cpp","additions":0,"deletions":2,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -844,1 +844,0 @@\n-void Canonicalizer::do_RoundFP        (RoundFP*         x) {}\n","filename":"src\/hotspot\/share\/c1\/c1_Canonicalizer.cpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -91,1 +91,0 @@\n-  virtual void do_RoundFP        (RoundFP*         x);\n","filename":"src\/hotspot\/share\/c1\/c1_Canonicalizer.hpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -29,1 +29,1 @@\n-#include \"c1\/c1_IR.hpp\"\n+#include \"c1\/c1_IR.hpp\"\n","filename":"src\/hotspot\/share\/c1\/c1_CodeStubs.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -28,1 +28,1 @@\n-#include \"c1\/c1_LIRAssembler.hpp\"\n+#include \"c1\/c1_LIRAssembler.hpp\"\n@@ -37,2 +37,0 @@\n-#include \"compiler\/compilerDirectives.hpp\"\n-#include \"compiler\/compileTask.hpp\"\n@@ -42,0 +40,1 @@\n+#include \"compiler\/compileTask.hpp\"\n","filename":"src\/hotspot\/share\/c1\/c1_Compilation.cpp","additions":2,"deletions":3,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -32,0 +32,1 @@\n+#include \"oops\/compressedOops.hpp\"\n@@ -35,1 +36,0 @@\n-#include \"oops\/compressedOops.hpp\"\n","filename":"src\/hotspot\/share\/c1\/c1_FrameMap.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -25,1 +25,1 @@\n-#include \"c1\/c1_CFGPrinter.hpp\"\n+#include \"c1\/c1_CFGPrinter.hpp\"\n@@ -678,11 +678,0 @@\n-    if (strict_fp_requires_explicit_rounding && load->type()->is_float_kind()) {\n-#ifdef IA32\n-      if (UseSSE < 2) {\n-        \/\/ can't skip load since value might get rounded as a side effect\n-        return load;\n-      }\n-#else\n-      Unimplemented();\n-#endif \/\/ IA32\n-    }\n-\n@@ -1057,1 +1046,1 @@\n-  state->store_local(index, round_fp(x));\n+  state->store_local(index, x);\n@@ -1276,4 +1265,1 @@\n-  \/\/ Note: currently single-precision floating-point rounding on Intel is handled at the LIRGenerator level\n-  res = append(res);\n-  res = round_fp(res);\n-  push(type, res);\n+  push(type, append(res));\n@@ -2488,1 +2474,1 @@\n-    push(result_type, round_fp(result));\n+    push(result_type, result);\n@@ -2634,23 +2620,0 @@\n-Value GraphBuilder::round_fp(Value fp_value) {\n-  if (strict_fp_requires_explicit_rounding) {\n-#ifdef IA32\n-    \/\/ no rounding needed if SSE2 is used\n-    if (UseSSE < 2) {\n-      \/\/ Must currently insert rounding node for doubleword values that\n-      \/\/ are results of expressions (i.e., not loads from memory or\n-      \/\/ constants)\n-      if (fp_value->type()->tag() == doubleTag &&\n-          fp_value->as_Constant() == nullptr &&\n-          fp_value->as_Local() == nullptr &&       \/\/ method parameters need no rounding\n-          fp_value->as_RoundFP() == nullptr) {\n-        return append(new RoundFP(fp_value));\n-      }\n-    }\n-#else\n-    Unimplemented();\n-#endif \/\/ IA32\n-  }\n-  return fp_value;\n-}\n-\n-\n","filename":"src\/hotspot\/share\/c1\/c1_GraphBuilder.cpp","additions":4,"deletions":41,"binary":false,"changes":45,"status":"modified"},{"patch":"@@ -28,1 +28,1 @@\n-#include \"c1\/c1_IR.hpp\"\n+#include \"c1\/c1_IR.hpp\"\n@@ -297,1 +297,0 @@\n-  Value round_fp(Value fp_value);\n","filename":"src\/hotspot\/share\/c1\/c1_GraphBuilder.hpp","additions":1,"deletions":2,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -28,1 +28,1 @@\n-#include \"c1\/c1_IR.hpp\"\n+#include \"c1\/c1_IR.hpp\"\n","filename":"src\/hotspot\/share\/c1\/c1_IR.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -25,1 +25,0 @@\n-#include \"c1\/c1_IR.hpp\"\n@@ -28,0 +27,1 @@\n+#include \"c1\/c1_IR.hpp\"\n","filename":"src\/hotspot\/share\/c1\/c1_Instruction.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -95,1 +95,0 @@\n-class   RoundFP;\n@@ -192,1 +191,0 @@\n-  virtual void do_RoundFP        (RoundFP*         x) = 0;\n@@ -587,1 +585,0 @@\n-  virtual RoundFP*          as_RoundFP()         { return nullptr; }\n@@ -1685,1 +1682,0 @@\n-  intArray*      _fpu_stack_state;               \/\/ For x86 FPU code generation with UseLinearScan\n@@ -1732,1 +1728,0 @@\n-  , _fpu_stack_state(nullptr)\n@@ -1760,1 +1755,0 @@\n-  intArray* fpu_stack_state() const              { return _fpu_stack_state;    }\n@@ -1783,1 +1777,0 @@\n-  void set_fpu_stack_state(intArray* state)      { _fpu_stack_state = state;  }\n@@ -2220,24 +2213,0 @@\n-\/\/ Models needed rounding for floating-point values on Intel.\n-\/\/ Currently only used to represent rounding of double-precision\n-\/\/ values stored into local variables, but could be used to model\n-\/\/ intermediate rounding of single-precision values as well.\n-LEAF(RoundFP, Instruction)\n- private:\n-  Value _input;             \/\/ floating-point value to be rounded\n-\n- public:\n-  RoundFP(Value input)\n-  : Instruction(input->type()) \/\/ Note: should not be used for constants\n-  , _input(input)\n-  {\n-    ASSERT_VALUES\n-  }\n-\n-  \/\/ accessors\n-  Value input() const                            { return _input; }\n-\n-  \/\/ generic\n-  virtual void input_values_do(ValueVisitor* f)   { f->visit(&_input); }\n-};\n-\n-\n","filename":"src\/hotspot\/share\/c1\/c1_Instruction.hpp","additions":0,"deletions":31,"binary":false,"changes":31,"status":"modified"},{"patch":"@@ -25,1 +25,0 @@\n-#include \"classfile\/vmSymbols.hpp\"\n@@ -32,0 +31,1 @@\n+#include \"classfile\/vmSymbols.hpp\"\n@@ -786,6 +786,0 @@\n-\n-void InstructionPrinter::do_RoundFP(RoundFP* x) {\n-  output()->print(\"round_fp \");\n-  print_value(x->input());\n-}\n-\n","filename":"src\/hotspot\/share\/c1\/c1_InstructionPrinter.cpp","additions":1,"deletions":7,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -28,1 +28,1 @@\n-#include \"c1\/c1_IR.hpp\"\n+#include \"c1\/c1_IR.hpp\"\n@@ -123,1 +123,0 @@\n-  virtual void do_RoundFP        (RoundFP*         x);\n","filename":"src\/hotspot\/share\/c1\/c1_InstructionPrinter.hpp","additions":1,"deletions":2,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -440,1 +440,0 @@\n-    case lir_fpop_raw:                 \/\/ result and info always invalid\n@@ -481,2 +480,0 @@\n-    case lir_fxch:           \/\/ input always valid, result and info always invalid\n-    case lir_fld:            \/\/ input always valid, result and info always invalid\n@@ -588,14 +585,0 @@\n-\/\/ LIR_OpRoundFP;\n-    case lir_roundfp: {\n-      assert(op->as_OpRoundFP() != nullptr, \"must be\");\n-      LIR_OpRoundFP* opRoundFP = (LIR_OpRoundFP*)op;\n-\n-      assert(op->_info == nullptr, \"info not used by this instruction\");\n-      assert(opRoundFP->_tmp->is_illegal(), \"not used\");\n-      do_input(opRoundFP->_opr);\n-      do_output(opRoundFP->_result);\n-\n-      break;\n-    }\n-\n-\n@@ -1894,1 +1877,0 @@\n-     case lir_fpop_raw:              s = \"fpop_raw\";      break;\n@@ -1899,2 +1881,0 @@\n-     case lir_fxch:                  s = \"fxch\";          break;\n-     case lir_fld:                   s = \"fld\";           break;\n@@ -1910,1 +1890,0 @@\n-     case lir_roundfp:               s = \"roundfp\";       break;\n@@ -2163,6 +2142,0 @@\n-void LIR_OpRoundFP::print_instr(outputStream* out) const {\n-  _opr->print(out);         out->print(\" \");\n-  tmp()->print(out);        out->print(\" \");\n-  result_opr()->print(out); out->print(\" \");\n-}\n-\n","filename":"src\/hotspot\/share\/c1\/c1_LIR.cpp","additions":0,"deletions":27,"binary":false,"changes":27,"status":"modified"},{"patch":"@@ -46,1 +46,0 @@\n-class FpuStackSim;\n@@ -239,2 +238,1 @@\n-    , is_fpu_stack_offset_bits = 1        \/\/ used in assertion checking on x86 for FPU stack slot allocation\n-                       + is_xmm_bits + last_use_bits + is_fpu_stack_offset_bits\n+                       + is_xmm_bits + last_use_bits\n@@ -252,2 +250,1 @@\n-    , is_fpu_stack_offset_shift = last_use_shift + last_use_bits\n-    , virtual_shift  = is_fpu_stack_offset_shift + is_fpu_stack_offset_bits\n+    , virtual_shift = last_use_shift + last_use_bits\n@@ -271,1 +268,0 @@\n-    , is_fpu_stack_offset_mask = right_n_bits(is_fpu_stack_offset_bits) << is_fpu_stack_offset_shift\n@@ -276,1 +272,1 @@\n-    , no_type_mask   = (int)(~(type_mask | last_use_mask | is_fpu_stack_offset_mask))\n+    , no_type_mask   = (int)(~(type_mask | last_use_mask))\n@@ -429,2 +425,0 @@\n-  bool is_fpu_stack_offset() const { assert(is_register(), \"only works for registers\"); return (value() & is_fpu_stack_offset_mask) != 0; }\n-  LIR_Opr make_fpu_stack_offset()  { assert(is_register(), \"only works for registers\"); return (LIR_Opr)(value() | is_fpu_stack_offset_mask); }\n@@ -887,1 +881,0 @@\n-class      LIR_OpRoundFP;\n@@ -920,1 +913,0 @@\n-      , lir_fpop_raw\n@@ -935,2 +927,0 @@\n-      , lir_fxch\n-      , lir_fld\n@@ -946,1 +936,0 @@\n-      , lir_roundfp\n@@ -1090,1 +1079,0 @@\n-  int           _fpu_pop_count;\n@@ -1110,1 +1098,0 @@\n-    , _fpu_pop_count(0)\n@@ -1124,1 +1111,0 @@\n-    , _fpu_pop_count(0)\n@@ -1145,5 +1131,0 @@\n-  \/\/ FPU stack simulation helpers -- only used on Intel\n-  void set_fpu_pop_count(int count)           { assert(count >= 0 && count <= 1, \"currently only 0 and 1 are valid\"); _fpu_pop_count = count; }\n-  int  fpu_pop_count() const                  { return _fpu_pop_count; }\n-  bool pop_fpu_stack()                        { return _fpu_pop_count > 0; }\n-\n@@ -1165,1 +1146,0 @@\n-  virtual LIR_OpRoundFP* as_OpRoundFP() { return nullptr; }\n@@ -1554,17 +1534,0 @@\n-\/\/ LIR_OpRoundFP\n-class LIR_OpRoundFP : public LIR_Op1 {\n- friend class LIR_OpVisitState;\n-\n- private:\n-  LIR_Opr _tmp;\n-\n- public:\n-  LIR_OpRoundFP(LIR_Opr reg, LIR_Opr stack_loc_temp, LIR_Opr result)\n-    : LIR_Op1(lir_roundfp, reg, result)\n-    , _tmp(stack_loc_temp) {}\n-\n-  LIR_Opr tmp() const                            { return _tmp; }\n-  virtual LIR_OpRoundFP* as_OpRoundFP()          { return this; }\n-  void print_instr(outputStream* out) const PRODUCT_RETURN;\n-};\n-\n@@ -2347,1 +2310,0 @@\n-  void roundfp(LIR_Opr reg, LIR_Opr stack_loc_temp, LIR_Opr result) { append(new LIR_OpRoundFP(reg, stack_loc_temp, result)); }\n","filename":"src\/hotspot\/share\/c1\/c1_LIR.hpp","additions":3,"deletions":41,"binary":false,"changes":44,"status":"modified"},{"patch":"@@ -495,13 +495,0 @@\n-\n-#if defined(IA32) && defined(COMPILER2)\n-  \/\/ C2 leave fpu stack dirty clean it\n-  if (UseSSE < 2 && !CompilerConfig::is_c1_only_no_jvmci()) {\n-    int i;\n-    for ( i = 1; i <= 7 ; i++ ) {\n-      ffree(i);\n-    }\n-    if (!op->result_opr()->is_float_kind()) {\n-      ffree(0);\n-    }\n-  }\n-#endif \/\/ IA32 && COMPILER2\n@@ -524,1 +511,1 @@\n-                op->patch_code(), op->info(), op->pop_fpu_stack(),\n+                op->patch_code(), op->info(),\n@@ -529,6 +516,0 @@\n-    case lir_roundfp: {\n-      LIR_OpRoundFP* round_op = op->as_OpRoundFP();\n-      roundfp_op(round_op->in_opr(), round_op->tmp(), round_op->result_opr(), round_op->pop_fpu_stack());\n-      break;\n-    }\n-\n@@ -858,1 +839,0 @@\n-      assert(op->fpu_pop_count() < 2, \"\");\n@@ -864,2 +844,1 @@\n-        op->info(),\n-        op->fpu_pop_count() == 1);\n+        op->info());\n@@ -911,11 +890,1 @@\n-void LIR_Assembler::roundfp_op(LIR_Opr src, LIR_Opr tmp, LIR_Opr dest, bool pop_fpu_stack) {\n-  assert(strict_fp_requires_explicit_rounding, \"not required\");\n-  assert((src->is_single_fpu() && dest->is_single_stack()) ||\n-         (src->is_double_fpu() && dest->is_double_stack()),\n-         \"round_fp: rounds register -> stack location\");\n-\n-  reg2stack (src, dest, src->type(), pop_fpu_stack);\n-}\n-\n-\n-void LIR_Assembler::move_op(LIR_Opr src, LIR_Opr dest, BasicType type, LIR_PatchCode patch_code, CodeEmitInfo* info, bool pop_fpu_stack, bool wide) {\n+void LIR_Assembler::move_op(LIR_Opr src, LIR_Opr dest, BasicType type, LIR_PatchCode patch_code, CodeEmitInfo* info, bool wide) {\n@@ -928,1 +897,1 @@\n-      reg2stack(src, dest, type, pop_fpu_stack);\n+      reg2stack(src, dest, type);\n@@ -930,1 +899,1 @@\n-      reg2mem(src, dest, type, patch_code, info, pop_fpu_stack, wide);\n+      reg2mem(src, dest, type, patch_code, info, wide);\n","filename":"src\/hotspot\/share\/c1\/c1_LIRAssembler.cpp","additions":5,"deletions":36,"binary":false,"changes":41,"status":"modified"},{"patch":"@@ -175,1 +175,1 @@\n-  void reg2stack  (LIR_Opr src, LIR_Opr dest, BasicType type, bool pop_fpu_stack);\n+  void reg2stack  (LIR_Opr src, LIR_Opr dest, BasicType type);\n@@ -179,1 +179,1 @@\n-                   bool pop_fpu_stack, bool wide);\n+                   bool wide);\n@@ -222,1 +222,1 @@\n-  void arith_op(LIR_Code code, LIR_Opr left, LIR_Opr right, LIR_Opr dest, CodeEmitInfo* info, bool pop_fpu_stack);\n+  void arith_op(LIR_Code code, LIR_Opr left, LIR_Opr right, LIR_Opr dest, CodeEmitInfo* info);\n@@ -231,2 +231,1 @@\n-  void roundfp_op(LIR_Opr src, LIR_Opr tmp, LIR_Opr dest, bool pop_fpu_stack);\n-               LIR_PatchCode patch_code, CodeEmitInfo* info, bool pop_fpu_stack, bool wide);\n+               LIR_PatchCode patch_code, CodeEmitInfo* info, bool wide);\n","filename":"src\/hotspot\/share\/c1\/c1_LIRAssembler.hpp","additions":4,"deletions":5,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -904,21 +904,0 @@\n-LIR_Opr LIRGenerator::round_item(LIR_Opr opr) {\n-  assert(opr->is_register(), \"why spill if item is not register?\");\n-\n-  if (strict_fp_requires_explicit_rounding) {\n-#ifdef IA32\n-    if (UseSSE < 1 && opr->is_single_fpu()) {\n-      LIR_Opr result = new_register(T_FLOAT);\n-      set_vreg_flag(result, must_start_in_memory);\n-      assert(opr->is_register(), \"only a register can be spilled\");\n-      assert(opr->value_type()->is_float(), \"rounding only for floats available\");\n-      __ roundfp(opr, LIR_OprFact::illegalOpr, result);\n-      return result;\n-    }\n-#else\n-    Unimplemented();\n-#endif \/\/ IA32\n-  }\n-  return opr;\n-}\n-\n-\n@@ -2443,19 +2422,0 @@\n-void LIRGenerator::do_RoundFP(RoundFP* x) {\n-  assert(strict_fp_requires_explicit_rounding, \"not required\");\n-\n-  LIRItem input(x->input(), this);\n-  input.load_item();\n-  LIR_Opr input_opr = input.result();\n-  assert(input_opr->is_register(), \"why round if value is not in a register?\");\n-  assert(input_opr->is_single_fpu() || input_opr->is_double_fpu(), \"input should be floating-point value\");\n-  if (input_opr->is_single_fpu()) {\n-    set_result(x, round_item(input_opr)); \/\/ This code path not currently taken\n-  } else {\n-    LIR_Opr result = new_register(T_DOUBLE);\n-    set_vreg_flag(result, must_start_in_memory);\n-    __ roundfp(input_opr, LIR_OprFact::illegalOpr, result);\n-    set_result(x, result);\n-  }\n-}\n-\n-\n","filename":"src\/hotspot\/share\/c1\/c1_LIRGenerator.cpp","additions":0,"deletions":40,"binary":false,"changes":40,"status":"modified"},{"patch":"@@ -239,1 +239,0 @@\n-  LIR_Opr round_item(LIR_Opr opr);\n@@ -604,1 +603,0 @@\n-  virtual void do_RoundFP        (RoundFP*         x);\n","filename":"src\/hotspot\/share\/c1\/c1_LIRGenerator.hpp","additions":0,"deletions":2,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -30,1 +30,1 @@\n-#include \"c1\/c1_LIRGenerator.hpp\"\n+#include \"c1\/c1_LIRGenerator.hpp\"\n@@ -94,3 +94,0 @@\n-#ifdef IA32\n- , _fpu_stack_allocator(nullptr)\n-#endif\n@@ -1871,2 +1868,1 @@\n-  if ((reg < nof_regs && interval->always_in_memory()) ||\n-      (use_fpu_stack_allocation() && reg >= pd_first_fpu_reg && reg <= pd_last_fpu_reg)) {\n+  if ((reg < nof_regs && interval->always_in_memory())) {\n@@ -1874,1 +1870,1 @@\n-    \/\/ in the following two cases:\n+    \/\/ in the following case:\n@@ -1878,2 +1874,0 @@\n-    \/\/ * the interval would be on the fpu stack at the begin of the exception handler\n-    \/\/   this is not allowed because of the complicated fpu stack handling on Intel\n@@ -2668,8 +2662,1 @@\n-#ifdef IA32\n-    \/\/ the exact location of fpu stack values is only known\n-    \/\/ during fpu stack allocation, so the stack allocator object\n-    \/\/ must be present\n-    assert(use_fpu_stack_allocation(), \"should not have float stack values without fpu stack allocation (all floats must be SSE2)\");\n-    assert(_fpu_stack_allocator != nullptr, \"must be present\");\n-    opr = _fpu_stack_allocator->to_fpu_stack(opr);\n-#elif defined(AMD64)\n+#if defined(AMD64)\n@@ -2678,1 +2665,0 @@\n-\n@@ -2781,10 +2767,0 @@\n-#ifdef IA32\n-      \/\/ the exact location of fpu stack values is only known\n-      \/\/ during fpu stack allocation, so the stack allocator object\n-      \/\/ must be present\n-      assert(use_fpu_stack_allocation(), \"should not have float stack values without fpu stack allocation (all floats must be SSE2)\");\n-      assert(_fpu_stack_allocator != nullptr, \"must be present\");\n-      opr = _fpu_stack_allocator->to_fpu_stack(opr);\n-\n-      assert(opr->fpu_regnrLo() == opr->fpu_regnrHi(), \"assumed in calculation (only fpu_regnrLo is used)\");\n-#endif\n@@ -3024,9 +3000,3 @@\n-      if (!use_fpu_stack_allocation()) {\n-        \/\/ compute debug information if fpu stack allocation is not needed.\n-        \/\/ when fpu stack allocation is needed, the debug information can not\n-        \/\/ be computed here because the exact location of fpu operands is not known\n-        \/\/ -> debug information is created inside the fpu stack allocator\n-        int n = visitor.info_count();\n-        for (int k = 0; k < n; k++) {\n-          compute_debug_info(visitor.info_at(k), op_id);\n-        }\n+      int n = visitor.info_count();\n+      for (int k = 0; k < n; k++) {\n+        compute_debug_info(visitor.info_at(k), op_id);\n@@ -3129,8 +3099,0 @@\n-  { TIME_LINEAR_SCAN(timer_allocate_fpu_stack);\n-\n-    if (use_fpu_stack_allocation()) {\n-      allocate_fpu_stack(); \/\/ Only has effect on Intel\n-      NOT_PRODUCT(print_lir(2, \"LIR after FPU stack allocation:\"));\n-    }\n-  }\n-\n@@ -5891,1 +5853,0 @@\n-    \/\/ used for lir_roundfp: rounding is done by store to stack and reload later\n@@ -6011,14 +5972,0 @@\n-\n-  } else if (op1->code() == lir_fxch && op2->code() == lir_fxch) {\n-    assert(op1->as_Op1() != nullptr, \"fxch must be LIR_Op1\");\n-    assert(op2->as_Op1() != nullptr, \"fxch must be LIR_Op1\");\n-    LIR_Op1* fxch1 = (LIR_Op1*)op1;\n-    LIR_Op1* fxch2 = (LIR_Op1*)op2;\n-    if (fxch1->in_opr()->as_jint() == fxch2->in_opr()->as_jint()) {\n-      \/\/ equal FPU stack operations can be optimized\n-      return false;\n-    }\n-\n-  } else if (op1->code() == lir_fpop_raw && op2->code() == lir_fpop_raw) {\n-    \/\/ equal FPU stack operations can be optimized\n-    return false;\n@@ -6544,1 +6491,0 @@\n-    case counter_fpu_stack:       return \"fpu-stack\";\n@@ -6765,4 +6711,0 @@\n-        case lir_fpop_raw:\n-        case lir_fxch:\n-        case lir_fld:             inc_counter(counter_fpu_stack); break;\n-\n@@ -6773,1 +6715,0 @@\n-        case lir_roundfp:\n@@ -6822,1 +6763,0 @@\n-    case timer_allocate_fpu_stack:       return \"Allocate FPU Stack\";\n","filename":"src\/hotspot\/share\/c1\/c1_LinearScan.cpp","additions":7,"deletions":67,"binary":false,"changes":74,"status":"modified"},{"patch":"@@ -28,0 +28,1 @@\n+#include \"compiler\/compileLog.hpp\"\n@@ -30,1 +31,0 @@\n-#include \"compiler\/compileLog.hpp\"\n@@ -583,1 +583,0 @@\n-  void do_RoundFP        (RoundFP*         x);\n@@ -770,1 +769,0 @@\n-void NullCheckVisitor::do_RoundFP        (RoundFP*         x) {}\n","filename":"src\/hotspot\/share\/c1\/c1_Optimizer.cpp","additions":1,"deletions":3,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -157,1 +157,0 @@\n-    void do_RoundFP        (RoundFP*         x) { \/* nothing to do *\/ };\n","filename":"src\/hotspot\/share\/c1\/c1_RangeCheckElimination.hpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -54,1 +54,1 @@\n-#include \"oops\/objArrayOop.inline.hpp\"\n+#include \"oops\/objArrayOop.inline.hpp\"\n","filename":"src\/hotspot\/share\/c1\/c1_Runtime1.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -209,1 +209,0 @@\n-  void do_RoundFP        (RoundFP*         x) { \/* nothing to do *\/ }\n","filename":"src\/hotspot\/share\/c1\/c1_ValueMap.hpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -25,1 +25,1 @@\n-#include \"c1\/c1_IR.hpp\"\n+#include \"c1\/c1_IR.hpp\"\n","filename":"src\/hotspot\/share\/c1\/c1_ValueStack.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -283,6 +283,0 @@\n-  develop(bool, TraceFPUStack, false,                                       \\\n-          \"Trace emulation of the FPU stack (intel only)\")                  \\\n-                                                                            \\\n-  develop(bool, TraceFPURegisterUsage, false,                               \\\n-          \"Trace usage of FPU registers at start of blocks (intel only)\")   \\\n-                                                                            \\\n","filename":"src\/hotspot\/share\/c1\/c1_globals.hpp","additions":0,"deletions":6,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -440,1 +440,1 @@\n-  char* mapped_base()    const { return first_core_region()->mapped_base(); }\n+  char* mapped_base()    const { return header()->mapped_base_address();    }\n","filename":"src\/hotspot\/share\/cds\/filemap.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -154,0 +154,4 @@\n+size_t MetaspaceShared::protection_zone_size() {\n+  return os::cds_core_region_alignment();\n+}\n+\n@@ -693,4 +697,3 @@\n-class CollectCLDClosure : public CLDClosure {\n-  GrowableArray<ClassLoaderData*> _loaded_cld;\n-  GrowableArray<OopHandle> _loaded_cld_handles; \/\/ keep the CLDs alive\n-  Thread* _current_thread;\n+class CollectClassesForLinking : public KlassClosure {\n+  GrowableArray<OopHandle> _mirrors;\n+\n@@ -698,4 +701,3 @@\n-  CollectCLDClosure(Thread* thread) : _current_thread(thread) {}\n-  ~CollectCLDClosure() {\n-    for (int i = 0; i < _loaded_cld_handles.length(); i++) {\n-      _loaded_cld_handles.at(i).release(Universe::vm_global());\n+  ~CollectClassesForLinking() {\n+    for (int i = 0; i < _mirrors.length(); i++) {\n+      _mirrors.at(i).release(Universe::vm_global());\n@@ -704,0 +706,1 @@\n+\n@@ -706,2 +709,0 @@\n-    _loaded_cld.append(cld);\n-    _loaded_cld_handles.append(OopHandle(Universe::vm_global(), cld->holder()));\n@@ -710,2 +711,7 @@\n-  int nof_cld() const                { return _loaded_cld.length(); }\n-  ClassLoaderData* cld_at(int index) { return _loaded_cld.at(index); }\n+  void do_klass(Klass* k) {\n+    if (k->is_instance_klass()) {\n+      _mirrors.append(OopHandle(Universe::vm_global(), k->java_mirror()));\n+    }\n+  }\n+\n+  const GrowableArray<OopHandle>* mirrors() const { return &_mirrors; }\n@@ -752,10 +758,0 @@\n-  \/\/ Collect all loaded ClassLoaderData.\n-  CollectCLDClosure collect_cld(THREAD);\n-  {\n-    \/\/ ClassLoaderDataGraph::loaded_cld_do requires ClassLoaderDataGraph_lock.\n-    \/\/ We cannot link the classes while holding this lock (or else we may run into deadlock).\n-    \/\/ Therefore, we need to first collect all the CLDs, and then link their classes after\n-    \/\/ releasing the lock.\n-    MutexLocker lock(ClassLoaderDataGraph_lock);\n-    ClassLoaderDataGraph::loaded_cld_do(&collect_cld);\n-  }\n@@ -764,0 +760,11 @@\n+    CollectClassesForLinking collect_classes;\n+    {\n+      \/\/ ClassLoaderDataGraph::loaded_classes_do_keepalive() requires ClassLoaderDataGraph_lock.\n+      \/\/ We cannot link the classes while holding this lock (or else we may run into deadlock).\n+      \/\/ Therefore, we need to first collect all the classes, keeping them alive by\n+      \/\/ holding onto their java_mirrors in global OopHandles. We then link the classes after\n+      \/\/ releasing the lock.\n+      MutexLocker lock(ClassLoaderDataGraph_lock);\n+      ClassLoaderDataGraph::loaded_classes_do_keepalive(&collect_classes);\n+    }\n+\n@@ -765,9 +772,6 @@\n-    for (int i = 0; i < collect_cld.nof_cld(); i++) {\n-      ClassLoaderData* cld = collect_cld.cld_at(i);\n-      for (Klass* klass = cld->klasses(); klass != nullptr; klass = klass->next_link()) {\n-        if (klass->is_instance_klass()) {\n-          InstanceKlass* ik = InstanceKlass::cast(klass);\n-          if (may_be_eagerly_linked(ik)) {\n-            has_linked |= link_class_for_cds(ik, CHECK);\n-          }\n-        }\n+    const GrowableArray<OopHandle>* mirrors = collect_classes.mirrors();\n+    for (int i = 0; i < mirrors->length(); i++) {\n+      OopHandle mirror = mirrors->at(i);\n+      InstanceKlass* ik = InstanceKlass::cast(java_lang_Class::as_Klass(mirror.resolve()));\n+      if (may_be_eagerly_linked(ik)) {\n+        has_linked |= link_class_for_cds(ik, CHECK);\n@@ -1144,0 +1148,2 @@\n+    log_info(cds)(\"ArchiveRelocationMode: %d\", ArchiveRelocationMode);\n+\n@@ -1261,0 +1267,1 @@\n+  size_t prot_zone_size = 0;\n@@ -1272,0 +1279,4 @@\n+    if (Metaspace::using_class_space()) {\n+      prot_zone_size = protection_zone_size();\n+    }\n+\n@@ -1277,0 +1288,3 @@\n+      assert(archive_space_rs.base() == mapped_base_address &&\n+          archive_space_rs.size() > protection_zone_size(),\n+          \"Archive space must lead and include the protection zone\");\n@@ -1279,1 +1293,1 @@\n-      assert(class_space_rs.is_reserved(),\n+      assert(class_space_rs.is_reserved() && class_space_rs.size() > 0,\n@@ -1292,2 +1306,3 @@\n-    log_info(cds)(\"Reserved archive_space_rs [\" INTPTR_FORMAT \" - \" INTPTR_FORMAT \"] (%zu) bytes\",\n-                   p2i(archive_space_rs.base()), p2i(archive_space_rs.end()), archive_space_rs.size());\n+    log_info(cds)(\"Reserved archive_space_rs [\" INTPTR_FORMAT \" - \" INTPTR_FORMAT \"] (%zu) bytes%s\",\n+                   p2i(archive_space_rs.base()), p2i(archive_space_rs.end()), archive_space_rs.size(),\n+                   (prot_zone_size > 0 ? \" (includes protection zone)\" : \"\"));\n@@ -1320,0 +1335,17 @@\n+        \/\/ The protection zone is part of the archive:\n+        \/\/ See comment above, the Windows way of loading CDS is to mmap the individual\n+        \/\/ parts of the archive into the address region we just vacated. The protection\n+        \/\/ zone will not be mapped (and, in fact, does not exist as physical region in\n+        \/\/ the archive). Therefore, after removing the archive space above, we must\n+        \/\/ re-reserve the protection zone part lest something else gets mapped into that\n+        \/\/ area later.\n+        if (prot_zone_size > 0) {\n+          assert(prot_zone_size >= os::vm_allocation_granularity(), \"must be\"); \/\/ not just page size!\n+          char* p = os::attempt_reserve_memory_at(mapped_base_address, prot_zone_size,\n+                                                  false, MemTag::mtClassShared);\n+          assert(p == mapped_base_address || p == nullptr, \"must be\");\n+          if (p == nullptr) {\n+            log_debug(cds)(\"Failed to re-reserve protection zone\");\n+            return MAP_ARCHIVE_MMAP_FAILURE;\n+          }\n+        }\n@@ -1322,0 +1354,10 @@\n+\n+    if (prot_zone_size > 0) {\n+      os::commit_memory(mapped_base_address, prot_zone_size, false); \/\/ will later be protected\n+      \/\/ Before mapping the core regions into the newly established address space, we mark\n+      \/\/ start and the end of the future protection zone with canaries. That way we easily\n+      \/\/ catch mapping errors (accidentally mapping data into the future protection zone).\n+      *(mapped_base_address) = 'P';\n+      *(mapped_base_address + prot_zone_size - 1) = 'P';\n+    }\n+\n@@ -1365,32 +1407,34 @@\n-        if (Metaspace::using_class_space()) {\n-          \/\/ Set up ccs in metaspace.\n-          Metaspace::initialize_class_space(class_space_rs);\n-\n-          \/\/ Set up compressed Klass pointer encoding: the encoding range must\n-          \/\/  cover both archive and class space.\n-          address cds_base = (address)static_mapinfo->mapped_base();\n-          address ccs_end = (address)class_space_rs.end();\n-          assert(ccs_end > cds_base, \"Sanity check\");\n-          if (INCLUDE_CDS_JAVA_HEAP || UseCompactObjectHeaders) {\n-            \/\/ The CDS archive may contain narrow Klass IDs that were precomputed at archive generation time:\n-            \/\/ - every archived java object header (only if INCLUDE_CDS_JAVA_HEAP)\n-            \/\/ - every archived Klass' prototype   (only if +UseCompactObjectHeaders)\n-            \/\/\n-            \/\/ In order for those IDs to still be valid, we need to dictate base and shift: base should be the\n-            \/\/ mapping start, shift the shift used at archive generation time.\n-            address precomputed_narrow_klass_base = cds_base;\n-            const int precomputed_narrow_klass_shift = ArchiveBuilder::precomputed_narrow_klass_shift();\n-            CompressedKlassPointers::initialize_for_given_encoding(\n-              cds_base, ccs_end - cds_base, \/\/ Klass range\n-              precomputed_narrow_klass_base, precomputed_narrow_klass_shift \/\/ precomputed encoding, see ArchiveBuilder\n-            );\n-          } else {\n-            \/\/ Let JVM freely chose encoding base and shift\n-            CompressedKlassPointers::initialize (\n-              cds_base, ccs_end - cds_base \/\/ Klass range\n-              );\n-          }\n-          \/\/ map_or_load_heap_region() compares the current narrow oop and klass encodings\n-          \/\/ with the archived ones, so it must be done after all encodings are determined.\n-          static_mapinfo->map_or_load_heap_region();\n-        }\n+    if (Metaspace::using_class_space()) {\n+      assert(prot_zone_size > 0 &&\n+             *(mapped_base_address) == 'P' &&\n+             *(mapped_base_address + prot_zone_size - 1) == 'P',\n+             \"Protection zone was overwritten?\");\n+      \/\/ Set up ccs in metaspace.\n+      Metaspace::initialize_class_space(class_space_rs);\n+\n+      \/\/ Set up compressed Klass pointer encoding: the encoding range must\n+      \/\/  cover both archive and class space.\n+      const address encoding_base = (address)mapped_base_address;\n+      const address klass_range_start = encoding_base + prot_zone_size;\n+      const size_t klass_range_size = (address)class_space_rs.end() - klass_range_start;\n+      if (INCLUDE_CDS_JAVA_HEAP || UseCompactObjectHeaders) {\n+        \/\/ The CDS archive may contain narrow Klass IDs that were precomputed at archive generation time:\n+        \/\/ - every archived java object header (only if INCLUDE_CDS_JAVA_HEAP)\n+        \/\/ - every archived Klass' prototype   (only if +UseCompactObjectHeaders)\n+        \/\/\n+        \/\/ In order for those IDs to still be valid, we need to dictate base and shift: base should be the\n+        \/\/ mapping start (including protection zone), shift should be the shift used at archive generation time.\n+        CompressedKlassPointers::initialize_for_given_encoding(\n+          klass_range_start, klass_range_size,\n+          encoding_base, ArchiveBuilder::precomputed_narrow_klass_shift() \/\/ precomputed encoding, see ArchiveBuilder\n+        );\n+      } else {\n+        \/\/ Let JVM freely choose encoding base and shift\n+        CompressedKlassPointers::initialize(klass_range_start, klass_range_size);\n+      }\n+      CompressedKlassPointers::establish_protection_zone(encoding_base, prot_zone_size);\n+\n+      \/\/ map_or_load_heap_region() compares the current narrow oop and klass encodings\n+      \/\/ with the archived ones, so it must be done after all encodings are determined.\n+      static_mapinfo->map_or_load_heap_region();\n+    }\n@@ -1478,1 +1522,0 @@\n-  assert(static_mapinfo->mapping_base_offset() == 0, \"Must be\");\n@@ -1499,1 +1542,1 @@\n-      MemTracker::record_virtual_memory_tag(archive_space_rs.base(), mtClassShared);\n+      MemTracker::record_virtual_memory_tag(archive_space_rs, mtClassShared);\n@@ -1572,3 +1615,2 @@\n-    \/\/ NMT: fix up the space tags\n-    MemTracker::record_virtual_memory_tag(archive_space_rs.base(), mtClassShared);\n-    MemTracker::record_virtual_memory_tag(class_space_rs.base(), mtClass);\n+    MemTracker::record_virtual_memory_tag(archive_space_rs, mtClassShared);\n+    MemTracker::record_virtual_memory_tag(class_space_rs, mtClass);\n","filename":"src\/hotspot\/share\/cds\/metaspaceShared.cpp","additions":113,"deletions":71,"binary":false,"changes":184,"status":"modified"},{"patch":"@@ -142,0 +142,1 @@\n+  static size_t protection_zone_size();\n","filename":"src\/hotspot\/share\/cds\/metaspaceShared.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -25,1 +25,0 @@\n-#include \"classfile\/vmIntrinsics.hpp\"\n@@ -31,0 +30,1 @@\n+#include \"classfile\/vmIntrinsics.hpp\"\n","filename":"src\/hotspot\/share\/ci\/bcEscapeAnalyzer.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -31,1 +31,1 @@\n-#include \"oops\/inlineKlass.inline.hpp\"\n+#include \"oops\/inlineKlass.inline.hpp\"\n","filename":"src\/hotspot\/share\/ci\/ciArrayKlass.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -46,1 +46,1 @@\n-#include \"compiler\/compilerEvent.hpp\"\n+#include \"compiler\/compilerEvent.hpp\"\n","filename":"src\/hotspot\/share\/ci\/ciEnv.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -34,0 +34,1 @@\n+#include \"compiler\/cHeapStringHolder.hpp\"\n@@ -36,1 +37,0 @@\n-#include \"compiler\/cHeapStringHolder.hpp\"\n","filename":"src\/hotspot\/share\/ci\/ciEnv.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -25,1 +25,0 @@\n-#include \"classfile\/javaClasses.inline.hpp\"\n@@ -32,0 +31,1 @@\n+#include \"classfile\/javaClasses.inline.hpp\"\n","filename":"src\/hotspot\/share\/ci\/ciInstance.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -36,0 +36,2 @@\n+#include \"oops\/fieldStreams.inline.hpp\"\n+#include \"oops\/inlineKlass.inline.hpp\"\n@@ -39,2 +41,0 @@\n-#include \"oops\/fieldStreams.inline.hpp\"\n-#include \"oops\/inlineKlass.inline.hpp\"\n","filename":"src\/hotspot\/share\/ci\/ciInstanceKlass.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -31,0 +31,1 @@\n+#include \"ci\/ciReplay.hpp\"\n@@ -33,1 +34,0 @@\n-#include \"ci\/ciReplay.hpp\"\n","filename":"src\/hotspot\/share\/ci\/ciMethod.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -28,0 +28,1 @@\n+#include \"ci\/ciInlineKlass.hpp\"\n@@ -46,1 +47,0 @@\n-#include \"ci\/ciInlineKlass.hpp\"\n","filename":"src\/hotspot\/share\/ci\/ciObjectFactory.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -25,0 +25,1 @@\n+#include \"ci\/ciKlass.hpp\"\n@@ -28,1 +29,0 @@\n-#include \"ci\/ciKlass.hpp\"\n","filename":"src\/hotspot\/share\/ci\/ciReplay.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -780,1 +780,0 @@\n-    ResourceMark rm(THREAD);\n@@ -823,1 +822,0 @@\n-        ResourceMark rm(THREAD);\n","filename":"src\/hotspot\/share\/classfile\/verifier.cpp","additions":0,"deletions":2,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -543,1 +543,1 @@\n-  do_name(intPolyMult_name, \"multImpl\")                                                                                     \\\n+  do_name(intPolyMult_name, \"mult\")                                                                                     \\\n","filename":"src\/hotspot\/share\/classfile\/vmIntrinsics.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -42,1 +42,1 @@\n-#include \"jvm.h\"\n+#include \"jvm.h\"\n@@ -49,1 +49,1 @@\n-#include \"oops\/methodData.hpp\"\n+#include \"oops\/methodData.hpp\"\n@@ -1447,24 +1447,0 @@\n-#if defined(IA32) && !defined(ZERO)\n-      \/\/ The following native methods:\n-      \/\/\n-      \/\/ java.lang.Float.intBitsToFloat\n-      \/\/ java.lang.Float.floatToRawIntBits\n-      \/\/ java.lang.Double.longBitsToDouble\n-      \/\/ java.lang.Double.doubleToRawLongBits\n-      \/\/\n-      \/\/ are called through the interpreter even if interpreter native stubs\n-      \/\/ are not preferred (i.e., calling through adapter handlers is preferred).\n-      \/\/ The reason is that on x86_32 signaling NaNs (sNaNs) are not preserved\n-      \/\/ if the version of the methods from the native libraries is called.\n-      \/\/ As the interpreter and the C2-intrinsified version of the methods preserves\n-      \/\/ sNaNs, that would result in an inconsistent way of handling of sNaNs.\n-      if ((UseSSE >= 1 &&\n-          (method->intrinsic_id() == vmIntrinsics::_intBitsToFloat ||\n-           method->intrinsic_id() == vmIntrinsics::_floatToRawIntBits)) ||\n-          (UseSSE >= 2 &&\n-           (method->intrinsic_id() == vmIntrinsics::_longBitsToDouble ||\n-            method->intrinsic_id() == vmIntrinsics::_doubleToRawLongBits))) {\n-        return nullptr;\n-      }\n-#endif \/\/ IA32 && !ZERO\n-\n","filename":"src\/hotspot\/share\/compiler\/compileBroker.cpp","additions":2,"deletions":26,"binary":false,"changes":28,"status":"modified"},{"patch":"@@ -36,1 +36,1 @@\n-#include \"oops\/inlineKlass.hpp\"\n+#include \"oops\/inlineKlass.hpp\"\n","filename":"src\/hotspot\/share\/compiler\/oopMap.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -618,0 +618,6 @@\n+void G1ParScanThreadState::record_evacuation_failed_region(G1HeapRegion* r, uint worker_id, bool cause_pinned) {\n+  if (_evac_failure_regions->record(worker_id, r->hrm_index(), cause_pinned)) {\n+    G1HeapRegionPrinter::evac_failure(r);\n+  }\n+}\n+\n@@ -627,3 +633,1 @@\n-    if (_evac_failure_regions->record(_worker_id, r->hrm_index(), cause_pinned)) {\n-      G1HeapRegionPrinter::evac_failure(r);\n-    }\n+    record_evacuation_failed_region(r, _worker_id, cause_pinned);\n","filename":"src\/hotspot\/share\/gc\/g1\/g1ParScanThreadState.cpp","additions":7,"deletions":3,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -260,1 +260,1 @@\n-  MemTracker::record_virtual_memory_tag((address)rs.base(), mtGC);\n+  MemTracker::record_virtual_memory_tag(rs, mtGC);\n","filename":"src\/hotspot\/share\/gc\/parallel\/psParallelCompact.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -156,5 +156,0 @@\n-    if (bt == T_DOUBLE) {\n-      Node* new_val = kit->dprecision_rounding(val.node());\n-      val.set_node(new_val);\n-    }\n-\n","filename":"src\/hotspot\/share\/gc\/shared\/c2\/barrierSetC2.cpp","additions":0,"deletions":5,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -236,5 +236,0 @@\n-  native_method_entry(java_lang_Float_intBitsToFloat)\n-  native_method_entry(java_lang_Float_floatToRawIntBits)\n-  native_method_entry(java_lang_Double_longBitsToDouble)\n-  native_method_entry(java_lang_Double_doubleToRawLongBits)\n-\n@@ -380,1 +375,0 @@\n-  __ verify_FPU(1, t->tos_in());\n@@ -491,11 +485,0 @@\n-\n-  \/\/ On x86_32 platforms, a special entry is generated for the following four methods.\n-  \/\/ On other platforms the native entry is used to enter these methods.\n-  case Interpreter::java_lang_Float_intBitsToFloat\n-                                           : entry_point = generate_Float_intBitsToFloat_entry(); break;\n-  case Interpreter::java_lang_Float_floatToRawIntBits\n-                                           : entry_point = generate_Float_floatToRawIntBits_entry(); break;\n-  case Interpreter::java_lang_Double_longBitsToDouble\n-                                           : entry_point = generate_Double_longBitsToDouble_entry(); break;\n-  case Interpreter::java_lang_Double_doubleToRawLongBits\n-                                           : entry_point = generate_Double_doubleToRawLongBits_entry(); break;\n","filename":"src\/hotspot\/share\/interpreter\/templateInterpreterGenerator.cpp","additions":0,"deletions":17,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -24,0 +24,1 @@\n+#include \"classfile\/moduleEntry.hpp\"\n@@ -25,0 +26,1 @@\n+#include \"classfile\/vmSymbols.hpp\"\n@@ -27,2 +29,0 @@\n-#include \"classfile\/moduleEntry.hpp\"\n-#include \"classfile\/vmSymbols.hpp\"\n","filename":"src\/hotspot\/share\/jvmci\/jvmciCompiler.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -459,1 +459,1 @@\n-  if (CDSConfig::is_dumping_final_static_archive() && resolved_references() != nullptr) {\n+  if (CDSConfig::is_dumping_final_static_archive() && CDSConfig::is_dumping_heap() && resolved_references() != nullptr) {\n","filename":"src\/hotspot\/share\/oops\/constantPool.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -1283,1 +1283,0 @@\n-  ResourceMark rm;\n","filename":"src\/hotspot\/share\/oops\/generateOopMap.cpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -869,0 +869,6 @@\n+  \/\/ If either input is all ones, the output is all ones.\n+  \/\/ x | ~0 == ~0 <==> x | -1 == -1\n+  if (r0 == TypeInt::MINUS_1 || r1 == TypeInt::MINUS_1) {\n+    return TypeInt::MINUS_1;\n+  }\n+\n@@ -926,0 +932,6 @@\n+  \/\/ If either input is all ones, the output is all ones.\n+  \/\/ x | ~0 == ~0 <==> x | -1 == -1\n+  if (r0 == TypeLong::MINUS_1 || r1 == TypeLong::MINUS_1) {\n+    return TypeLong::MINUS_1;\n+  }\n+\n","filename":"src\/hotspot\/share\/opto\/addnode.cpp","additions":12,"deletions":0,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2000, 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2000, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -241,1 +241,1 @@\n-          \"Generate a predicate to select fast\/slow loop versions\")         \\\n+          \"Move checks with uncommon trap out of loops.\")                   \\\n@@ -788,1 +788,3 @@\n-          \"Move predicates out of loops based on profiling data\")           \\\n+          \"Move checks with an uncommon trap out of loops based on \"        \\\n+          \"profiling data. \"                                                \\\n+          \"Requires UseLoopPredicate to be turned on (default).\")           \\\n","filename":"src\/hotspot\/share\/opto\/c2_globals.hpp","additions":5,"deletions":3,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -318,1 +318,0 @@\n-macro(RoundDouble)\n@@ -321,1 +320,0 @@\n-macro(RoundFloat)\n","filename":"src\/hotspot\/share\/opto\/classes.hpp","additions":0,"deletions":2,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -192,10 +192,0 @@\n-\/\/------------------------------Ideal------------------------------------------\n-\/\/ If converting to an int type, skip any rounding nodes\n-Node *ConvD2INode::Ideal(PhaseGVN *phase, bool can_reshape) {\n-  if (in(1)->Opcode() == Op_RoundDouble) {\n-    set_req(1, in(1)->in(1));\n-    return this;\n-  }\n-  return nullptr;\n-}\n-\n@@ -228,10 +218,0 @@\n-\/\/------------------------------Ideal------------------------------------------\n-\/\/ If converting to an int type, skip any rounding nodes\n-Node *ConvD2LNode::Ideal(PhaseGVN *phase, bool can_reshape) {\n-  if (in(1)->Opcode() == Op_RoundDouble) {\n-    set_req(1, in(1)->in(1));\n-    return this;\n-  }\n-  return nullptr;\n-}\n-\n@@ -311,10 +291,0 @@\n-\/\/------------------------------Ideal------------------------------------------\n-\/\/ If converting to an int type, skip any rounding nodes\n-Node *ConvF2INode::Ideal(PhaseGVN *phase, bool can_reshape) {\n-  if (in(1)->Opcode() == Op_RoundFloat) {\n-    set_req(1, in(1)->in(1));\n-    return this;\n-  }\n-  return nullptr;\n-}\n-\n@@ -340,10 +310,0 @@\n-\/\/------------------------------Ideal------------------------------------------\n-\/\/ If converting to an int type, skip any rounding nodes\n-Node *ConvF2LNode::Ideal(PhaseGVN *phase, bool can_reshape) {\n-  if (in(1)->Opcode() == Op_RoundFloat) {\n-    set_req(1, in(1)->in(1));\n-    return this;\n-  }\n-  return nullptr;\n-}\n-\n@@ -876,46 +836,0 @@\n-\n-\n-\/\/=============================================================================\n-\/\/------------------------------Identity---------------------------------------\n-\/\/ Remove redundant roundings\n-Node* RoundFloatNode::Identity(PhaseGVN* phase) {\n-  assert(Matcher::strict_fp_requires_explicit_rounding, \"should only generate for Intel\");\n-  \/\/ Do not round constants\n-  if (phase->type(in(1))->base() == Type::FloatCon)  return in(1);\n-  int op = in(1)->Opcode();\n-  \/\/ Redundant rounding\n-  if( op == Op_RoundFloat ) return in(1);\n-  \/\/ Already rounded\n-  if( op == Op_Parm ) return in(1);\n-  if( op == Op_LoadF ) return in(1);\n-  return this;\n-}\n-\n-\/\/------------------------------Value------------------------------------------\n-const Type* RoundFloatNode::Value(PhaseGVN* phase) const {\n-  return phase->type( in(1) );\n-}\n-\n-\/\/=============================================================================\n-\/\/------------------------------Identity---------------------------------------\n-\/\/ Remove redundant roundings.  Incoming arguments are already rounded.\n-Node* RoundDoubleNode::Identity(PhaseGVN* phase) {\n-  assert(Matcher::strict_fp_requires_explicit_rounding, \"should only generate for Intel\");\n-  \/\/ Do not round constants\n-  if (phase->type(in(1))->base() == Type::DoubleCon)  return in(1);\n-  int op = in(1)->Opcode();\n-  \/\/ Redundant rounding\n-  if( op == Op_RoundDouble ) return in(1);\n-  \/\/ Already rounded\n-  if( op == Op_Parm ) return in(1);\n-  if( op == Op_LoadD ) return in(1);\n-  if( op == Op_ConvF2D ) return in(1);\n-  if( op == Op_ConvI2D ) return in(1);\n-  return this;\n-}\n-\n-\/\/------------------------------Value------------------------------------------\n-const Type* RoundDoubleNode::Value(PhaseGVN* phase) const {\n-  return phase->type( in(1) );\n-}\n-\n","filename":"src\/hotspot\/share\/opto\/convertnode.cpp","additions":0,"deletions":86,"binary":false,"changes":86,"status":"modified"},{"patch":"@@ -671,2 +671,0 @@\n-  \/\/ Round double arguments before call\n-  round_double_arguments(cg->method());\n","filename":"src\/hotspot\/share\/opto\/doCall.cpp","additions":0,"deletions":2,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -603,1 +603,1 @@\n-    if (failing()) { stop(); return; }  \/\/ exception allocation might fail\n+    \/\/ If we have a preconstructed exception object, use it.\n@@ -2563,45 +2563,0 @@\n-void GraphKit::round_double_arguments(ciMethod* dest_method) {\n-  if (Matcher::strict_fp_requires_explicit_rounding) {\n-    \/\/ (Note:  TypeFunc::make has a cache that makes this fast.)\n-    const TypeFunc* tf    = TypeFunc::make(dest_method);\n-    int             nargs = tf->domain_sig()->cnt() - TypeFunc::Parms;\n-    for (int j = 0; j < nargs; j++) {\n-      const Type *targ = tf->domain_sig()->field_at(j + TypeFunc::Parms);\n-      if (targ->basic_type() == T_DOUBLE) {\n-        \/\/ If any parameters are doubles, they must be rounded before\n-        \/\/ the call, dprecision_rounding does gvn.transform\n-        Node *arg = argument(j);\n-        arg = dprecision_rounding(arg);\n-        set_argument(j, arg);\n-      }\n-    }\n-  }\n-}\n-\n-\/\/ rounding for strict float precision conformance\n-Node* GraphKit::precision_rounding(Node* n) {\n-  if (Matcher::strict_fp_requires_explicit_rounding) {\n-#ifdef IA32\n-    if (UseSSE == 0) {\n-      return _gvn.transform(new RoundFloatNode(nullptr, n));\n-    }\n-#else\n-    Unimplemented();\n-#endif \/\/ IA32\n-  }\n-  return n;\n-}\n-\n-\/\/ rounding for strict double precision conformance\n-Node* GraphKit::dprecision_rounding(Node *n) {\n-  if (Matcher::strict_fp_requires_explicit_rounding) {\n-#ifdef IA32\n-    if (UseSSE < 2) {\n-      return _gvn.transform(new RoundDoubleNode(nullptr, n));\n-    }\n-#else\n-    Unimplemented();\n-#endif \/\/ IA32\n-  }\n-  return n;\n-}\n@@ -4607,3 +4562,3 @@\n-  }\n-  if (UseProfiledLoopPredicate) {\n-    add_parse_predicate(Deoptimization::Reason_profile_predicate, nargs);\n+    if (UseProfiledLoopPredicate) {\n+      add_parse_predicate(Deoptimization::Reason_profile_predicate, nargs);\n+    }\n","filename":"src\/hotspot\/share\/opto\/graphKit.cpp","additions":4,"deletions":49,"binary":false,"changes":53,"status":"modified"},{"patch":"@@ -771,9 +771,0 @@\n-  \/\/ Helper function to round double arguments before a call\n-  void round_double_arguments(ciMethod* dest_method);\n-\n-  \/\/ rounding for strict float precision conformance\n-  Node* precision_rounding(Node* n);\n-\n-  \/\/ rounding for strict double precision conformance\n-  Node* dprecision_rounding(Node* n);\n-\n","filename":"src\/hotspot\/share\/opto\/graphKit.hpp","additions":0,"deletions":9,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -168,8 +168,6 @@\n-  if (UseLoopPredicate) {\n-    \/\/ Sync IdealKit and graphKit.\n-    gkit->sync_kit(*this);\n-    \/\/ Add Parse Predicates.\n-    gkit->add_parse_predicates(nargs);\n-    \/\/ Update IdealKit memory.\n-    sync_kit(gkit);\n-  }\n+  \/\/ Sync IdealKit and graphKit.\n+  gkit->sync_kit(*this);\n+  \/\/ Add Parse Predicates.\n+  gkit->add_parse_predicates(nargs);\n+  \/\/ Update IdealKit memory.\n+  sync_kit(gkit);\n","filename":"src\/hotspot\/share\/opto\/idealKit.cpp","additions":6,"deletions":8,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -1733,14 +1733,0 @@\n-\/\/--------------------------round_double_node--------------------------------\n-\/\/ Round a double node if necessary.\n-Node* LibraryCallKit::round_double_node(Node* n) {\n-  if (Matcher::strict_fp_requires_explicit_rounding) {\n-#ifdef IA32\n-    if (UseSSE < 2) {\n-      n = _gvn.transform(new RoundDoubleNode(nullptr, n));\n-    }\n-#else\n-    Unimplemented();\n-#endif \/\/ IA32\n-  }\n-  return n;\n-}\n@@ -1755,1 +1741,1 @@\n-  Node* arg = round_double_node(argument(0));\n+  Node* arg = argument(0);\n@@ -1766,1 +1752,1 @@\n-  case vmIntrinsics::_dcopySign: n = CopySignDNode::make(_gvn, arg, round_double_node(argument(2))); break;\n+  case vmIntrinsics::_dcopySign: n = CopySignDNode::make(_gvn, arg, argument(2)); break;\n@@ -1800,2 +1786,2 @@\n-  Node* a = round_double_node(argument(0));\n-  Node* b = (call_type == OptoRuntime::Math_DD_D_Type()) ? round_double_node(argument(2)) : nullptr;\n+  Node* a = argument(0);\n+  Node* b = (call_type == OptoRuntime::Math_DD_D_Type()) ? argument(2) : nullptr;\n@@ -1819,1 +1805,1 @@\n-  Node* exp = round_double_node(argument(2));\n+  Node* exp = argument(2);\n@@ -1824,1 +1810,1 @@\n-      Node* base = round_double_node(argument(0));\n+      Node* base = argument(0);\n@@ -1829,1 +1815,1 @@\n-      Node* base = round_double_node(argument(0));\n+      Node* base = argument(0);\n@@ -1975,2 +1961,2 @@\n-      a = round_double_node(argument(0));\n-      b = round_double_node(argument(2));\n+      a = argument(0);\n+      b = argument(2);\n@@ -8957,3 +8943,3 @@\n-    a = round_double_node(argument(0));\n-    b = round_double_node(argument(2));\n-    c = round_double_node(argument(4));\n+    a = argument(0);\n+    b = argument(2);\n+    c = argument(4);\n","filename":"src\/hotspot\/share\/opto\/library_call.cpp","additions":12,"deletions":26,"binary":false,"changes":38,"status":"modified"},{"patch":"@@ -225,1 +225,0 @@\n-  Node* round_double_node(Node* n);\n","filename":"src\/hotspot\/share\/opto\/library_call.hpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -1000,1 +1000,1 @@\n-                                           const NodeInLoopBody& _node_in_loop_body, bool clone_template);\n+                                           const NodeInLoopBody& _node_in_loop_body, bool kill_old_template);\n@@ -1003,1 +1003,2 @@\n-                                                        const NodeInLoopBody& _node_in_loop_body, bool clone_template);\n+                                                        const NodeInLoopBody& _node_in_loop_body,\n+                                                        bool kill_old_template);\n","filename":"src\/hotspot\/share\/opto\/loopnode.hpp","additions":3,"deletions":2,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -2872,1 +2872,1 @@\n-    set_pair_local( 0, dprecision_rounding(pop_pair()) );\n+    set_pair_local( 0, pop_pair() );\n@@ -2875,1 +2875,1 @@\n-    set_pair_local( 1, dprecision_rounding(pop_pair()) );\n+    set_pair_local( 1, pop_pair() );\n@@ -2878,1 +2878,1 @@\n-    set_pair_local( 2, dprecision_rounding(pop_pair()) );\n+    set_pair_local( 2, pop_pair() );\n@@ -2881,1 +2881,1 @@\n-    set_pair_local( 3, dprecision_rounding(pop_pair()) );\n+    set_pair_local( 3, pop_pair() );\n@@ -2884,1 +2884,1 @@\n-    set_pair_local( iter().get_index(), dprecision_rounding(pop_pair()) );\n+    set_pair_local( iter().get_index(), pop_pair() );\n@@ -3066,2 +3066,1 @@\n-    d = precision_rounding(c);\n-    push( d );\n+    push(c);\n@@ -3074,2 +3073,1 @@\n-    d = precision_rounding(c);\n-    push( d );\n+    push(c);\n@@ -3082,2 +3080,1 @@\n-    d = precision_rounding(c);\n-    push( d );\n+    push(c);\n@@ -3090,2 +3087,1 @@\n-    d = precision_rounding(c);\n-    push( d );\n+    push(c);\n@@ -3141,2 +3137,0 @@\n-    \/\/ This breaks _227_mtrt (speed & correctness) and _222_mpegaudio (speed)\n-    \/\/b = _gvn.transform(new RoundFloatNode(nullptr, b) );\n@@ -3150,5 +3144,0 @@\n-      \/\/ For x86_32.ad, FILD doesn't restrict precision to 24 or 53 bits.\n-      \/\/ Rather than storing the result into an FP register then pushing\n-      \/\/ out to memory to round, the machine instruction that implements\n-      \/\/ ConvL2D is responsible for rounding.\n-      \/\/ c = precision_rounding(b);\n@@ -3164,2 +3153,0 @@\n-    \/\/ For x86_32.ad, rounding is always necessary (see _l2f above).\n-    \/\/ c = dprecision_rounding(b);\n@@ -3185,2 +3172,1 @@\n-    d = dprecision_rounding(c);\n-    push_pair( d );\n+    push_pair(c);\n@@ -3193,2 +3179,1 @@\n-    d = dprecision_rounding(c);\n-    push_pair( d );\n+    push_pair(c);\n@@ -3201,2 +3186,1 @@\n-    d = dprecision_rounding(c);\n-    push_pair( d );\n+    push_pair(c);\n@@ -3209,2 +3193,1 @@\n-    d = dprecision_rounding(c);\n-    push_pair( d );\n+    push_pair(c);\n@@ -3395,2 +3378,1 @@\n-    c = precision_rounding(b);\n-    push (b);\n+    push(b);\n","filename":"src\/hotspot\/share\/opto\/parse2.cpp","additions":14,"deletions":32,"binary":false,"changes":46,"status":"modified"},{"patch":"@@ -1732,1 +1732,0 @@\n-    ResourceMark rm;\n","filename":"src\/hotspot\/share\/opto\/runtime.cpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -2149,9 +2149,3 @@\n-Node* ReverseINode::Identity(PhaseGVN* phase) {\n-  if (in(1)->Opcode() == Op_ReverseI) {\n-    return in(1)->in(1);\n-  }\n-  return this;\n-}\n-\n-Node* ReverseLNode::Identity(PhaseGVN* phase) {\n-  if (in(1)->Opcode() == Op_ReverseL) {\n+Node* InvolutionNode::Identity(PhaseGVN* phase) {\n+  \/\/ Op ( Op x ) => x\n+  if (in(1)->Opcode() == Opcode()) {\n","filename":"src\/hotspot\/share\/opto\/subnode.cpp","additions":3,"deletions":9,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -464,0 +464,7 @@\n+\/\/------------------------------InvolutionNode----------------------------------\n+\/\/ Represents a self-inverse operation, i.e., op(op(x)) = x for any x\n+class InvolutionNode : public Node {\n+public:\n+  InvolutionNode(Node* in) : Node(nullptr, in) {}\n+  virtual Node* Identity(PhaseGVN* phase);\n+};\n@@ -466,1 +473,1 @@\n-class NegNode : public Node {\n+class NegNode : public InvolutionNode {\n@@ -468,1 +475,1 @@\n-  NegNode(Node* in1) : Node(nullptr, in1) {\n+  NegNode(Node* in1) : InvolutionNode(in1) {\n@@ -581,1 +588,1 @@\n-class ReverseBytesINode : public Node {\n+class ReverseBytesINode : public InvolutionNode {\n@@ -583,1 +590,1 @@\n-  ReverseBytesINode(Node* in) : Node(nullptr, in) {}\n+  ReverseBytesINode(Node* in) : InvolutionNode(in) {}\n@@ -591,1 +598,1 @@\n-class ReverseBytesLNode : public Node {\n+class ReverseBytesLNode : public InvolutionNode {\n@@ -593,1 +600,1 @@\n-  ReverseBytesLNode(Node* in) : Node(nullptr, in) {}\n+  ReverseBytesLNode(Node* in) : InvolutionNode(in) {}\n@@ -601,1 +608,1 @@\n-class ReverseBytesUSNode : public Node {\n+class ReverseBytesUSNode : public InvolutionNode {\n@@ -603,1 +610,1 @@\n-  ReverseBytesUSNode(Node* in1) : Node(nullptr, in1) {}\n+  ReverseBytesUSNode(Node* in1) : InvolutionNode(in1) {}\n@@ -611,1 +618,1 @@\n-class ReverseBytesSNode : public Node {\n+class ReverseBytesSNode : public InvolutionNode {\n@@ -613,1 +620,1 @@\n-  ReverseBytesSNode(Node* in) : Node(nullptr, in) {}\n+  ReverseBytesSNode(Node* in) : InvolutionNode(in) {}\n@@ -621,1 +628,1 @@\n-class ReverseINode : public Node {\n+class ReverseINode : public InvolutionNode {\n@@ -623,1 +630,1 @@\n-  ReverseINode(Node* in) : Node(nullptr, in) {}\n+  ReverseINode(Node* in) : InvolutionNode(in) {}\n@@ -627,1 +634,0 @@\n-  virtual Node* Identity(PhaseGVN* phase);\n@@ -633,1 +639,1 @@\n-class ReverseLNode : public Node {\n+class ReverseLNode : public InvolutionNode {\n@@ -635,1 +641,1 @@\n-  ReverseLNode(Node* in) : Node(nullptr, in) {}\n+  ReverseLNode(Node* in) : InvolutionNode(in) {}\n@@ -639,1 +645,0 @@\n-  virtual Node* Identity(PhaseGVN* phase);\n","filename":"src\/hotspot\/share\/opto\/subnode.hpp","additions":21,"deletions":16,"binary":false,"changes":37,"status":"modified"},{"patch":"@@ -3900,0 +3900,4 @@\n+  \/\/ Set the _monitor_owner_id to the next thread_id temporarily while initialization runs.\n+  \/\/ Do it now before we make this thread visible in Threads::add().\n+  thread->set_monitor_owner_id(ThreadIdentifier::next());\n+\n@@ -3939,0 +3943,3 @@\n+  \/\/ Update the _monitor_owner_id with the tid value.\n+  thread->set_monitor_owner_id(java_lang_Thread::thread_id(thread->threadObj()));\n+\n","filename":"src\/hotspot\/share\/prims\/jni.cpp","additions":7,"deletions":0,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -333,0 +333,11 @@\n+WB_ENTRY(void, WB_DecodeNKlassAndAccessKlass(JNIEnv* env, jobject o, jint nKlass))\n+  assert(UseCompressedClassPointers, \"Should only call for UseCompressedClassPointers\");\n+  const narrowKlass nk = (narrowKlass)nKlass;\n+  const Klass* const k = CompressedKlassPointers::decode_not_null_without_asserts(nKlass);\n+  printf(\"WB_DecodeNKlassAndAccessKlass: nk %u k \" PTR_FORMAT \"\\n\", nk, p2i(k));\n+  printf(\"Will attempt to crash now...\\n\");\n+  fflush(stdout); \/\/ flush now - we will crash below\n+  \/\/ Access k by calling a virtual function - will result in loading the vtable from *k\n+  k->print_on(tty);\n+WB_END\n+\n@@ -2844,0 +2855,1 @@\n+  {CC\"decodeNKlassAndAccessKlass\",CC\"(I)V\",            (void*)&WB_DecodeNKlassAndAccessKlass},\n","filename":"src\/hotspot\/share\/prims\/whitebox.cpp","additions":12,"deletions":0,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -335,2 +335,1 @@\n-    if (matches_property_suffix(property_suffix, ADDEXPORTS, ADDEXPORTS_LEN) ||\n-        matches_property_suffix(property_suffix, ADDREADS, ADDREADS_LEN) ||\n+    if (matches_property_suffix(property_suffix, ADDREADS, ADDREADS_LEN) ||\n@@ -347,1 +346,2 @@\n-      if (matches_property_suffix(property_suffix, PATH, PATH_LEN) ||\n+      if (matches_property_suffix(property_suffix, ADDEXPORTS, ADDEXPORTS_LEN) ||\n+          matches_property_suffix(property_suffix, PATH, PATH_LEN) ||\n@@ -3893,0 +3893,7 @@\n+#ifdef COMPILER2\n+  if (!FLAG_IS_DEFAULT(UseLoopPredicate) && !UseLoopPredicate && UseProfiledLoopPredicate) {\n+    warning(\"Disabling UseProfiledLoopPredicate since UseLoopPredicate is turned off.\");\n+    FLAG_SET_ERGO(UseProfiledLoopPredicate, false);\n+  }\n+#endif \/\/ COMPILER2\n+\n","filename":"src\/hotspot\/share\/runtime\/arguments.cpp","additions":10,"deletions":3,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -240,2 +240,0 @@\n-  \/\/ Set the _monitor_owner_id to the next thread_id temporarily while initialization runs.\n-  set_monitor_owner_id(ThreadIdentifier::next());\n@@ -267,2 +265,0 @@\n-  \/\/ Update the _monitor_owner_id with the tid value.\n-  set_monitor_owner_id(java_lang_Thread::thread_id(thread_oop()));\n","filename":"src\/hotspot\/share\/runtime\/javaThread.cpp","additions":0,"deletions":4,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -174,1 +174,1 @@\n-    assert(id >= ThreadIdentifier::initial() && id < ThreadIdentifier::current(), \"\");\n+    ThreadIdentifier::verify_id(id);\n@@ -177,1 +177,5 @@\n-  int64_t monitor_owner_id() const { return _monitor_owner_id; }\n+  int64_t monitor_owner_id() const {\n+    int64_t id = _monitor_owner_id;\n+    ThreadIdentifier::verify_id(id);\n+    return id;\n+  }\n","filename":"src\/hotspot\/share\/runtime\/javaThread.hpp","additions":6,"deletions":2,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -155,1 +155,0 @@\n-      ResourceMark rm;\n@@ -168,1 +167,0 @@\n-    ResourceMark rm;\n","filename":"src\/hotspot\/share\/runtime\/safepoint.cpp","additions":0,"deletions":2,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -1035,0 +1035,1 @@\n+  assert(p->monitor_owner_id() != 0, \"should be set\");\n","filename":"src\/hotspot\/share\/runtime\/threads.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -823,4 +823,0 @@\n-    if (max == 0) {\n-      return nullptr;\n-    }\n-\n","filename":"src\/hotspot\/share\/utilities\/growableArray.hpp","additions":1,"deletions":5,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -180,0 +180,2 @@\n+serviceability\/attach\/AttachAPIv2\/StreamingOutputTest.java 8352392 aix-ppc64\n+\n","filename":"test\/hotspot\/jtreg\/ProblemList.txt","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -150,0 +150,1 @@\n+  sources \\\n@@ -435,0 +436,2 @@\n+ -runtime\/cds\/appcds\/jigsaw\/ExactOptionMatch.java \\\n+ -runtime\/cds\/appcds\/jigsaw\/modulepath\/AddExports.java \\\n@@ -550,0 +553,1 @@\n+ -runtime\/cds\/appcds\/jigsaw\/ExactOptionMatch.java \\\n@@ -551,0 +555,1 @@\n+ -runtime\/cds\/appcds\/jigsaw\/modulepath\/AddExports.java \\\n@@ -565,0 +570,1 @@\n+ -runtime\/cds\/appcds\/methodHandles \\\n@@ -642,3 +648,0 @@\n-tier1_sources = \\\n-   sources\n-\n@@ -651,1 +654,0 @@\n-  :tier1_sources\n","filename":"test\/hotspot\/jtreg\/TEST.groups","additions":6,"deletions":4,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -1414,0 +1414,10 @@\n+    public static final String NEG_F = PREFIX + \"NEG_F\" + POSTFIX;\n+    static {\n+        beforeMatchingNameRegex(NEG_F, \"NegF\");\n+    }\n+\n+    public static final String NEG_D = PREFIX + \"NEG_D\" + POSTFIX;\n+    static {\n+        beforeMatchingNameRegex(NEG_D, \"NegD\");\n+    }\n+\n@@ -1445,0 +1455,10 @@\n+    public static final String OR_I = PREFIX + \"OR_I\" + POSTFIX;\n+    static {\n+        beforeMatchingNameRegex(OR_I, \"OrI\");\n+    }\n+\n+    public static final String OR_L = PREFIX + \"OR_L\" + POSTFIX;\n+    static {\n+        beforeMatchingNameRegex(OR_L, \"OrL\");\n+    }\n+\n@@ -1534,0 +1554,15 @@\n+    public static final String LOOP_PARSE_PREDICATE = PREFIX + \"LOOP_PARSE_PREDICATE\" + POSTFIX;\n+    static {\n+        parsePredicateNodes(LOOP_PARSE_PREDICATE, \"Loop\");\n+    }\n+\n+    public static final String LOOP_LIMIT_CHECK_PARSE_PREDICATE = PREFIX + \"LOOP_LIMIT_CHECK_PARSE_PREDICATE\" + POSTFIX;\n+    static {\n+        parsePredicateNodes(LOOP_LIMIT_CHECK_PARSE_PREDICATE, \"Loop Limit Check\");\n+    }\n+\n+    public static final String PROFILED_LOOP_PARSE_PREDICATE = PREFIX + \"PROFILED_LOOP_PARSE_PREDICATE\" + POSTFIX;\n+    static {\n+        parsePredicateNodes(PROFILED_LOOP_PARSE_PREDICATE, \"Profiled Loop\");\n+    }\n+\n@@ -2756,0 +2791,7 @@\n+    private static void parsePredicateNodes(String irNodePlaceholder, String label) {\n+        String regex = START + \"ParsePredicate\" + MID + \"#\" + label + \"[ ]*!jvms:\" + END;\n+        IR_NODE_MAPPINGS.put(irNodePlaceholder, new SinglePhaseRangeEntry(CompilePhase.AFTER_PARSING, regex,\n+                                                                          CompilePhase.AFTER_PARSING,\n+                                                                          CompilePhase.PHASEIDEALLOOP_ITERATIONS));\n+    }\n+\n","filename":"test\/hotspot\/jtreg\/compiler\/lib\/ir_framework\/IRNode.java","additions":42,"deletions":0,"binary":false,"changes":42,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2019, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2019, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -64,2 +64,3 @@\n-        String forceRelocation = \"-XX:ArchiveRelocationMode=1\";\n-        String runRelocArg  = run_reloc  ? forceRelocation : \"-showversion\";\n+        String maybeRelocation = \"-XX:ArchiveRelocationMode=0\";\n+        String alwaysRelocation = \"-XX:ArchiveRelocationMode=1\";\n+        String runRelocArg  = run_reloc  ? alwaysRelocation : maybeRelocation;\n@@ -78,1 +79,4 @@\n-                        output.shouldContain(\"Try to map archive(s) at an alternative address\");\n+                        output.shouldContain(\"ArchiveRelocationMode == 1: always map archive(s) at an alternative address\")\n+                              .shouldContain(\"Try to map archive(s) at an alternative address\");\n+                    } else {\n+                        output.shouldContain(\"ArchiveRelocationMode: 0\");\n","filename":"test\/hotspot\/jtreg\/runtime\/cds\/appcds\/ArchiveRelocationTest.java","additions":8,"deletions":4,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2019, 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2019, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -85,3 +85,4 @@\n-        String forceRelocation = \"-XX:ArchiveRelocationMode=1\";\n-        String dumpTopRelocArg  = dump_top_reloc  ? forceRelocation : \"-showversion\";\n-        String runRelocArg      = run_reloc       ? forceRelocation : \"-showversion\";\n+        String maybeRelocation = \"-XX:ArchiveRelocationMode=0\";\n+        String alwaysRelocation = \"-XX:ArchiveRelocationMode=1\";\n+        String dumpTopRelocArg  = dump_top_reloc  ? alwaysRelocation : maybeRelocation;\n+        String runRelocArg      = run_reloc       ? alwaysRelocation : maybeRelocation;\n@@ -95,0 +96,1 @@\n+        String relocationModeMsg = \"ArchiveRelocationMode: 0\";\n@@ -112,0 +114,2 @@\n+                    } else {\n+                        output.shouldContain(relocationModeMsg);\n@@ -124,0 +128,2 @@\n+                    } else {\n+                        output.shouldContain(relocationModeMsg);\n","filename":"test\/hotspot\/jtreg\/runtime\/cds\/appcds\/dynamicArchive\/DynamicArchiveRelocationTest.java","additions":10,"deletions":4,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -768,3 +768,0 @@\n-jdk\/jfr\/api\/consumer\/streaming\/TestJVMCrash.java                8344671 macosx-all\n-jdk\/jfr\/api\/consumer\/streaming\/TestJVMExit.java                 8344671 macosx-all\n-jdk\/jfr\/api\/consumer\/streaming\/TestOutOfProcessMigration.java   8344671 macosx-all\n@@ -798,0 +795,1 @@\n+javax\/swing\/JInternalFrame\/bug4134077.java 8184985 windows-all\n","filename":"test\/jdk\/ProblemList.txt","additions":1,"deletions":3,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2012, 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2012, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -659,0 +659,2 @@\n+\n+  public native void decodeNKlassAndAccessKlass(int nKlass);\n","filename":"test\/lib\/jdk\/test\/whitebox\/WhiteBox.java","additions":3,"deletions":1,"binary":false,"changes":4,"status":"modified"}]}