{"files":[{"patch":"@@ -58,4 +58,4 @@\n-          - arm\n-          - s390x\n-          - ppc64le\n-          - riscv64\n+          # - arm\n+          # - s390x\n+          # - ppc64le\n+          # - riscv64\n@@ -69,25 +69,25 @@\n-          - target-cpu: arm\n-            gnu-arch: arm\n-            debian-arch: armhf\n-            debian-repository: https:\/\/httpredir.debian.org\/debian\/\n-            debian-version: trixie\n-            tolerate-sysroot-errors: false\n-            gnu-abi: eabihf\n-          - target-cpu: s390x\n-            gnu-arch: s390x\n-            debian-arch: s390x\n-            debian-repository: https:\/\/httpredir.debian.org\/debian\/\n-            debian-version: trixie\n-            tolerate-sysroot-errors: false\n-          - target-cpu: ppc64le\n-            gnu-arch: powerpc64le\n-            debian-arch: ppc64el\n-            debian-repository: https:\/\/httpredir.debian.org\/debian\/\n-            debian-version: trixie\n-            tolerate-sysroot-errors: false\n-          - target-cpu: riscv64\n-            gnu-arch: riscv64\n-            debian-arch: riscv64\n-            debian-repository: https:\/\/httpredir.debian.org\/debian\/\n-            debian-version: trixie\n-            tolerate-sysroot-errors: false\n+          # - target-cpu: arm\n+          #   gnu-arch: arm\n+          #   debian-arch: armhf\n+          #   debian-repository: https:\/\/httpredir.debian.org\/debian\/\n+          #   debian-version: trixie\n+          #   tolerate-sysroot-errors: false\n+          #   gnu-abi: eabihf\n+          # - target-cpu: s390x\n+          #   gnu-arch: s390x\n+          #   debian-arch: s390x\n+          #   debian-repository: https:\/\/httpredir.debian.org\/debian\/\n+          #   debian-version: trixie\n+          #   tolerate-sysroot-errors: false\n+          # - target-cpu: ppc64le\n+          #   gnu-arch: powerpc64le\n+          #   debian-arch: ppc64el\n+          #   debian-repository: https:\/\/httpredir.debian.org\/debian\/\n+          #   debian-version: trixie\n+          #   tolerate-sysroot-errors: false\n+          # - target-cpu: riscv64\n+          #   gnu-arch: riscv64\n+          #   debian-arch: riscv64\n+          #   debian-repository: https:\/\/httpredir.debian.org\/debian\/\n+          #   debian-version: trixie\n+          #   tolerate-sysroot-errors: false\n","filename":".github\/workflows\/build-cross-compile.yml","additions":29,"deletions":29,"binary":false,"changes":58,"status":"modified"},{"patch":"@@ -146,0 +146,1 @@\n+          --disable-jvm-feature-shenandoahgc\n","filename":".github\/workflows\/build-linux.yml","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -219,14 +219,14 @@\n-  build-linux-x64-hs-zero:\n-    name: linux-x64-hs-zero\n-    needs: prepare\n-    uses: .\/.github\/workflows\/build-linux.yml\n-    with:\n-      platform: linux-x64\n-      make-target: 'hotspot'\n-      debug-levels: '[ \"debug\" ]'\n-      gcc-major-version: '10'\n-      extra-conf-options: '--with-jvm-variants=zero --disable-precompiled-headers'\n-      configure-arguments: ${{ github.event.inputs.configure-arguments }}\n-      make-arguments: ${{ github.event.inputs.make-arguments }}\n-      dry-run: ${{ needs.prepare.outputs.dry-run == 'true' }}\n-    if: needs.prepare.outputs.linux-x64-variants == 'true'\n+  # build-linux-x64-hs-zero:\n+  #   name: linux-x64-hs-zero\n+  #   needs: prepare\n+  #   uses: .\/.github\/workflows\/build-linux.yml\n+  #   with:\n+  #     platform: linux-x64\n+  #     make-target: 'hotspot'\n+  #     debug-levels: '[ \"debug\" ]'\n+  #     gcc-major-version: '10'\n+  #     extra-conf-options: '--with-jvm-variants=zero --disable-precompiled-headers'\n+  #     configure-arguments: ${{ github.event.inputs.configure-arguments }}\n+  #     make-arguments: ${{ github.event.inputs.make-arguments }}\n+  #     dry-run: ${{ needs.prepare.outputs.dry-run == 'true' }}\n+  #   if: needs.prepare.outputs.linux-x64-variants == 'true'\n","filename":".github\/workflows\/main.yml","additions":14,"deletions":14,"binary":false,"changes":28,"status":"modified"},{"patch":"@@ -141,0 +141,1 @@\n+# Param3 - _valhalla, or empty\n@@ -148,2 +149,2 @@\n-  $1_$2_DUMP_EXTRA_ARG := $$($1_$2_COOPS_OPTION) $$($1_$2_COH_OPTION)\n-  $1_$2_DUMP_TYPE      := $(if $(findstring _nocoops, $2),-NOCOOPS,)$(if $(findstring _coh, $2),-COH,)\n+  $1_$2_$3_DUMP_EXTRA_ARG := $$($1_$2_COOPS_OPTION) $$($1_$2_COH_OPTION) $(if $(findstring _valhalla, $3), --enable-preview,)\n+  $1_$2_$3_DUMP_TYPE      := $(if $(findstring _nocoops, $2),-NOCOOPS,)$(if $(findstring _coh, $2),-COH,)$(if $(findstring _valhalla, $3), -VALHALLA,)\n@@ -151,1 +152,1 @@\n-  $1_$2_CDS_DUMP_FLAGS := $(CDS_DUMP_FLAGS) $(if $(filter g1gc, $(JVM_FEATURES_$1)), -XX:+UseG1GC)\n+  $1_$2_$3_CDS_DUMP_FLAGS := $(CDS_DUMP_FLAGS) $(if $(filter g1gc, $(JVM_FEATURES_$1)), -XX:+UseG1GC)\n@@ -154,1 +155,1 @@\n-    $1_$2_CDS_ARCHIVE := bin\/$1\/classes$2.jsa\n+    $1_$2_$3_CDS_ARCHIVE := bin\/$1\/classes$2$3.jsa\n@@ -156,1 +157,1 @@\n-    $1_$2_CDS_ARCHIVE := lib\/$1\/classes$2.jsa\n+    $1_$2_$3_CDS_ARCHIVE := lib\/$1\/classes$2$3.jsa\n@@ -167,3 +168,3 @@\n-  $$(eval $$(call SetupExecute, $1_$2_gen_cds_archive_jdk, \\\n-      WARN := Creating CDS$$($1_$2_DUMP_TYPE) archive for jdk image for $1, \\\n-      INFO := Using CDS flags for $1: $$($1_$2_CDS_DUMP_FLAGS), \\\n+  $$(eval $$(call SetupExecute, $1_$2_$3_gen_cds_archive_jdk, \\\n+      WARN := Creating CDS$$($1_$2_$3_DUMP_TYPE) archive for jdk image for $1, \\\n+      INFO := Using CDS flags for $1: $$($1_$2_$3_CDS_DUMP_FLAGS), \\\n@@ -171,1 +172,1 @@\n-      OUTPUT_FILE := $$(JDK_IMAGE_DIR)\/$$($1_$2_CDS_ARCHIVE), \\\n+      OUTPUT_FILE := $$(JDK_IMAGE_DIR)\/$$($1_$2_$3_CDS_ARCHIVE), \\\n@@ -174,2 +175,2 @@\n-          -XX:SharedArchiveFile=$$(JDK_IMAGE_DIR)\/$$($1_$2_CDS_ARCHIVE) \\\n-          -$1 $$($1_$2_DUMP_EXTRA_ARG) $$($1_$2_CDS_DUMP_FLAGS) $$(LOG_INFO), \\\n+          -XX:SharedArchiveFile=$$(JDK_IMAGE_DIR)\/$$($1_$2_$3_CDS_ARCHIVE) \\\n+          -$1 $$($1_$2_$3_DUMP_EXTRA_ARG) $$($1_$2_$3_CDS_DUMP_FLAGS) $$(LOG_INFO), \\\n@@ -178,1 +179,1 @@\n-  JDK_TARGETS += $$($1_$2_gen_cds_archive_jdk)\n+  JDK_TARGETS += $$($1_$2_$3_gen_cds_archive_jdk)\n@@ -180,3 +181,3 @@\n-  $$(eval $$(call SetupExecute, $1_$2_gen_cds_archive_jre, \\\n-      WARN := Creating CDS$$($1_$2_DUMP_TYPE) archive for jre image for $1, \\\n-      INFO := Using CDS flags for $1: $$($1_$2_CDS_DUMP_FLAGS), \\\n+  $$(eval $$(call SetupExecute, $1_$2_$3_gen_cds_archive_jre, \\\n+      WARN := Creating CDS$$($1_$2_$3_DUMP_TYPE) archive for jre image for $1, \\\n+      INFO := Using CDS flags for $1: $$($1_$2_$3_CDS_DUMP_FLAGS), \\\n@@ -184,1 +185,1 @@\n-      OUTPUT_FILE := $$(JRE_IMAGE_DIR)\/$$($1_$2_CDS_ARCHIVE), \\\n+      OUTPUT_FILE := $$(JRE_IMAGE_DIR)\/$$($1_$2_$3_CDS_ARCHIVE), \\\n@@ -187,2 +188,2 @@\n-          -XX:SharedArchiveFile=$$(JRE_IMAGE_DIR)\/$$($1_$2_CDS_ARCHIVE) \\\n-          -$1 $$($1_$2_DUMP_EXTRA_ARG) $$($1_$2_CDS_DUMP_FLAGS) $$(LOG_INFO), \\\n+          -XX:SharedArchiveFile=$$(JRE_IMAGE_DIR)\/$$($1_$2_$3_CDS_ARCHIVE) \\\n+          -$1 $$($1_$2_$3_DUMP_EXTRA_ARG) $$($1_$2_$3_CDS_DUMP_FLAGS) $$(LOG_INFO), \\\n@@ -191,1 +192,1 @@\n-  JRE_TARGETS += $$($1_$2_gen_cds_archive_jre)\n+  JRE_TARGETS += $$($1_$2_$3_gen_cds_archive_jre)\n@@ -196,1 +197,2 @@\n-    $(eval $(call CreateCDSArchive,$v,)) \\\n+    $(eval $(call CreateCDSArchive,$v,,)) \\\n+    $(eval $(call CreateCDSArchive,$v,,_valhalla)) \\\n@@ -201,1 +203,2 @@\n-      $(eval $(call CreateCDSArchive,$v,_nocoops)) \\\n+      $(eval $(call CreateCDSArchive,$v,_nocoops,)) \\\n+      $(eval $(call CreateCDSArchive,$v,_nocoops,_valhalla)) \\\n","filename":"make\/Images.gmk","additions":24,"deletions":21,"binary":false,"changes":45,"status":"modified"},{"patch":"@@ -40,0 +40,1 @@\n+include gensrc\/GensrcValueClasses.gmk\n","filename":"make\/modules\/java.base\/Gensrc.gmk","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -32,1 +32,1 @@\n-# DISABLED_WARNINGS_java +=\n+DISABLED_WARNINGS_java += initialization\n","filename":"make\/modules\/java.base\/Java.gmk","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -1693,0 +1693,3 @@\n+  } else if (_entry_point == nullptr) {\n+    \/\/ See CallLeafNoFPIndirect\n+    return 1 * NativeInstruction::instruction_size;\n@@ -1801,12 +1804,1 @@\n-  \/\/ n.b. frame size includes space for return pc and rfp\n-  const int framesize = C->output()->frame_size_in_bytes();\n-\n-  if (C->clinit_barrier_on_entry()) {\n-    assert(!C->method()->holder()->is_not_initialized(), \"initialization should have been started\");\n-\n-    Label L_skip_barrier;\n-    __ mov_metadata(rscratch2, C->method()->holder()->constant_encoding());\n-    __ clinit_barrier(rscratch2, rscratch1, &L_skip_barrier);\n-    __ far_jump(RuntimeAddress(SharedRuntime::get_handle_wrong_method_stub()));\n-    __ bind(L_skip_barrier);\n-  }\n+  __ verified_entry(C, 0);\n@@ -1815,2 +1807,2 @@\n-  if (C->max_vector_size() > 0) {\n-    __ reinitialize_ptrue();\n+  if (C->stub_function() == nullptr) {\n+    __ entry_barrier();\n@@ -1819,25 +1811,2 @@\n-  int bangsize = C->output()->bang_size_in_bytes();\n-  if (C->output()->need_stack_bang(bangsize))\n-    __ generate_stack_overflow_check(bangsize);\n-\n-  __ build_frame(framesize);\n-\n-  if (C->stub_function() == nullptr) {\n-    BarrierSetAssembler* bs = BarrierSet::barrier_set()->barrier_set_assembler();\n-    \/\/ Dummy labels for just measuring the code size\n-    Label dummy_slow_path;\n-    Label dummy_continuation;\n-    Label dummy_guard;\n-    Label* slow_path = &dummy_slow_path;\n-    Label* continuation = &dummy_continuation;\n-    Label* guard = &dummy_guard;\n-    if (!Compile::current()->output()->in_scratch_emit_size()) {\n-      \/\/ Use real labels from actual stub when not emitting code for the purpose of measuring its size\n-      C2EntryBarrierStub* stub = new (Compile::current()->comp_arena()) C2EntryBarrierStub();\n-      Compile::current()->output()->add_stub(stub);\n-      slow_path = &stub->entry();\n-      continuation = &stub->continuation();\n-      guard = &stub->guard();\n-    }\n-    \/\/ In the C2 code, we move the non-hot part of nmethod entry barriers out-of-line to a stub.\n-    bs->nmethod_entry_barrier(masm, slow_path, continuation, guard);\n+  if (!Compile::current()->output()->in_scratch_emit_size()) {\n+    __ bind(*_verified_entry);\n@@ -1860,6 +1829,0 @@\n-uint MachPrologNode::size(PhaseRegAlloc* ra_) const\n-{\n-  return MachNode::size(ra_); \/\/ too many variables; just compute it\n-                              \/\/ the hard way\n-}\n-\n@@ -1908,1 +1871,1 @@\n-  __ remove_frame(framesize);\n+  __ remove_frame(framesize, C->needs_stack_repair());\n@@ -1927,5 +1890,0 @@\n-uint MachEpilogNode::size(PhaseRegAlloc *ra_) const {\n-  \/\/ Variable size. Determine dynamically.\n-  return MachNode::size(ra_);\n-}\n-\n@@ -2230,1 +2188,12 @@\n-\/\/=============================================================================\n+\/\/\/=============================================================================\n+#ifndef PRODUCT\n+void MachVEPNode::format(PhaseRegAlloc* ra_, outputStream* st) const\n+{\n+  st->print_cr(\"# MachVEPNode\");\n+  if (!_verified) {\n+    st->print_cr(\"\\t load_class\");\n+  } else {\n+    st->print_cr(\"\\t unpack_inline_arg\");\n+  }\n+}\n+#endif\n@@ -2232,0 +2201,27 @@\n+void MachVEPNode::emit(C2_MacroAssembler *masm, PhaseRegAlloc* ra_) const\n+{\n+  if (!_verified) {\n+    __ ic_check(1);\n+  } else {\n+    \/\/ TODO 8284443 Avoid creation of temporary frame\n+    if (ra_->C->stub_function() == nullptr) {\n+      __ verified_entry(ra_->C, 0);\n+      __ entry_barrier();\n+      int framesize = ra_->C->output()->frame_slots() << LogBytesPerInt;\n+      __ remove_frame(framesize, false);\n+    }\n+    \/\/ Unpack inline type args passed as oop and then jump to\n+    \/\/ the verified entry point (skipping the unverified entry).\n+    int sp_inc = __ unpack_inline_args(ra_->C, _receiver_only);\n+    \/\/ Emit code for verified entry and save increment for stack repair on return\n+    __ verified_entry(ra_->C, sp_inc);\n+    if (Compile::current()->output()->in_scratch_emit_size()) {\n+      Label dummy_verified_entry;\n+      __ b(dummy_verified_entry);\n+    } else {\n+      __ b(*_verified_entry);\n+    }\n+  }\n+}\n+\n+\/\/=============================================================================\n@@ -2254,5 +2250,0 @@\n-uint MachUEPNode::size(PhaseRegAlloc* ra_) const\n-{\n-  return MachNode::size(ra_);\n-}\n-\n@@ -3684,0 +3675,31 @@\n+    if (tf()->returns_inline_type_as_fields() && !_method->is_method_handle_intrinsic() && _method->return_type()->is_loaded()) {\n+      \/\/ The last return value is not set by the callee but used to pass the null marker to compiled code.\n+      \/\/ Search for the corresponding projection, get the register and emit code that initialized it.\n+      uint con = (tf()->range_cc()->cnt() - 1);\n+      for (DUIterator_Fast imax, i = fast_outs(imax); i < imax; i++) {\n+        ProjNode* proj = fast_out(i)->as_Proj();\n+        if (proj->_con == con) {\n+          \/\/ Set null marker if r0 is non-null (a non-null value is returned buffered or scalarized)\n+          OptoReg::Name optoReg = ra_->get_reg_first(proj);\n+          VMReg reg = OptoReg::as_VMReg(optoReg, ra_->_framesize, OptoReg::reg2stack(ra_->_matcher._new_SP));\n+          Register toReg = reg->is_reg() ? reg->as_Register() : rscratch1;\n+          __ cmp(r0, zr);\n+          __ cset(toReg, Assembler::NE);\n+          if (reg->is_stack()) {\n+            int st_off = reg->reg2stack() * VMRegImpl::stack_slot_size;\n+            __ str(toReg, Address(sp, st_off));\n+          }\n+          break;\n+        }\n+      }\n+      if (return_value_is_used()) {\n+        \/\/ An inline type is returned as fields in multiple registers.\n+        \/\/ R0 either contains an oop if the inline type is buffered or a pointer\n+        \/\/ to the corresponding InlineKlass with the lowest bit set to 1. Zero r0\n+        \/\/ if the lowest bit is set to allow C2 to use the oop after null checking.\n+        \/\/ r0 &= (r0 & 1) - 1\n+        __ andr(rscratch1, r0, 0x1);\n+        __ sub(rscratch1, rscratch1, 0x1);\n+        __ andr(r0, r0, rscratch1);\n+      }\n+    }\n@@ -6865,1 +6887,1 @@\n-    \"mov  $dst, $con\\t# ptr\\n\\t\"\n+    \"mov  $dst, $con\\t# ptr\"\n@@ -8058,0 +8080,30 @@\n+instruct castI2N(iRegNNoSp dst, iRegI src) %{\n+  match(Set dst (CastI2N src));\n+\n+  ins_cost(INSN_COST);\n+  format %{ \"mov $dst, $src\\t# int -> narrow ptr\" %}\n+\n+  ins_encode %{\n+    if ($dst$$reg != $src$$reg) {\n+      __ mov(as_Register($dst$$reg), as_Register($src$$reg));\n+    }\n+  %}\n+\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct castN2X(iRegLNoSp dst, iRegN src) %{\n+  match(Set dst (CastP2X src));\n+\n+  ins_cost(INSN_COST);\n+  format %{ \"mov $dst, $src\\t# ptr -> long\" %}\n+\n+  ins_encode %{\n+    if ($dst$$reg != $src$$reg) {\n+      __ mov(as_Register($dst$$reg), as_Register($src$$reg));\n+    }\n+  %}\n+\n+  ins_pipe(ialu_reg);\n+%}\n+\n@@ -14046,1 +14098,1 @@\n-instruct clearArray_reg_reg(iRegL_R11 cnt, iRegP_R10 base, Universe dummy, rFlagsReg cr)\n+instruct clearArray_reg_reg_immL0(iRegL_R11 cnt, iRegP_R10 base, immL0 zero, Universe dummy, rFlagsReg cr)\n@@ -14048,1 +14100,1 @@\n-  match(Set dummy (ClearArray cnt base));\n+  match(Set dummy (ClearArray (Binary cnt base) zero));\n@@ -14065,0 +14117,16 @@\n+instruct clearArray_reg_reg(iRegL_R11 cnt, iRegP_R10 base, iRegL val, Universe dummy, rFlagsReg cr)\n+%{\n+  predicate(((ClearArrayNode*)n)->word_copy_only());\n+  match(Set dummy (ClearArray (Binary cnt base) val));\n+  effect(USE_KILL cnt, USE_KILL base, KILL cr);\n+\n+  ins_cost(4 * INSN_COST);\n+  format %{ \"ClearArray $cnt, $base, $val\" %}\n+\n+  ins_encode %{\n+    __ fill_words($base$$Register, $cnt$$Register, $val$$Register);\n+  %}\n+\n+  ins_pipe(pipe_class_memory);\n+%}\n+\n@@ -14068,1 +14136,2 @@\n-            < (uint64_t)(BlockZeroingLowLimit >> LogBytesPerWord));\n+            < (uint64_t)(BlockZeroingLowLimit >> LogBytesPerWord)\n+            && !((ClearArrayNode*)n)->word_copy_only());\n@@ -15385,0 +15454,18 @@\n+\/\/ entry point is null, target holds the address to call\n+instruct CallLeafNoFPIndirect(iRegP target)\n+%{\n+  predicate(n->as_Call()->entry_point() == nullptr);\n+\n+  match(CallLeafNoFP target);\n+\n+  ins_cost(CALL_COST);\n+\n+  format %{ \"CALL, runtime leaf nofp indirect $target\" %}\n+\n+  ins_encode %{\n+    __ blr($target$$Register);\n+  %}\n+\n+  ins_pipe(pipe_class_call);\n+%}\n+\n@@ -15387,0 +15474,2 @@\n+  predicate(n->as_Call()->entry_point() != nullptr);\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/aarch64.ad","additions":150,"deletions":61,"binary":false,"changes":211,"status":"modified"},{"patch":"@@ -331,4 +331,11 @@\n-    __ ldr(j_rarg2, result);\n-    Label is_long, is_float, is_double, exit;\n-    __ ldr(j_rarg1, result_type);\n-    __ cmp(j_rarg1, (u1)T_OBJECT);\n+    \/\/ All of j_rargN may be used to return inline type fields so be careful\n+    \/\/ not to clobber those.\n+    \/\/ SharedRuntime::generate_buffered_inline_type_adapter() knows the register\n+    \/\/ assignment of Rresult below.\n+    Register Rresult = r14, Rresult_type = r15;\n+    __ ldr(Rresult, result);\n+    Label is_long, is_float, is_double, check_prim, exit;\n+    __ ldr(Rresult_type, result_type);\n+    __ cmp(Rresult_type, (u1)T_OBJECT);\n+    __ br(Assembler::EQ, check_prim);\n+    __ cmp(Rresult_type, (u1)T_LONG);\n@@ -336,3 +343,1 @@\n-    __ cmp(j_rarg1, (u1)T_LONG);\n-    __ br(Assembler::EQ, is_long);\n-    __ cmp(j_rarg1, (u1)T_FLOAT);\n+    __ cmp(Rresult_type, (u1)T_FLOAT);\n@@ -340,1 +345,1 @@\n-    __ cmp(j_rarg1, (u1)T_DOUBLE);\n+    __ cmp(Rresult_type, (u1)T_DOUBLE);\n@@ -344,1 +349,1 @@\n-    __ strw(r0, Address(j_rarg2));\n+    __ strw(r0, Address(Rresult));\n@@ -396,0 +401,11 @@\n+    __ BIND(check_prim);\n+    if (InlineTypeReturnedAsFields) {\n+      \/\/ Check for scalarized return value\n+      __ tbz(r0, 0, is_long);\n+      \/\/ Load pack handler address\n+      __ andr(rscratch1, r0, -2);\n+      __ ldr(rscratch1, Address(rscratch1, InstanceKlass::adr_inlineklass_fixed_block_offset()));\n+      __ ldr(rscratch1, Address(rscratch1, InlineKlass::pack_handler_jobject_offset()));\n+      __ blr(rscratch1);\n+      __ b(exit);\n+    }\n@@ -398,1 +414,1 @@\n-    __ str(r0, Address(j_rarg2, 0));\n+    __ str(r0, Address(Rresult, 0));\n@@ -402,1 +418,1 @@\n-    __ strs(j_farg0, Address(j_rarg2, 0));\n+    __ strs(j_farg0, Address(Rresult, 0));\n@@ -406,1 +422,1 @@\n-    __ strd(j_farg0, Address(j_rarg2, 0));\n+    __ strd(j_farg0, Address(Rresult, 0));\n@@ -2227,0 +2243,6 @@\n+    \/\/ Check for flat inline type array -> return -1\n+    __ test_flat_array_oop(src, rscratch2, L_failed);\n+\n+    \/\/ Check for null-free (non-flat) inline type array -> handle as object array\n+    __ test_null_free_array_oop(src, rscratch2, L_objArray);\n+\n@@ -10525,0 +10547,24 @@\n+  static void save_return_registers(MacroAssembler* masm) {\n+    if (InlineTypeReturnedAsFields) {\n+      masm->push(RegSet::range(r0, r7), sp);\n+      masm->sub(sp, sp, 4 * wordSize);\n+      masm->st1(v0, v1, v2, v3, masm->T1D, Address(sp));\n+      masm->sub(sp, sp, 4 * wordSize);\n+      masm->st1(v4, v5, v6, v7, masm->T1D, Address(sp));\n+    } else {\n+      masm->fmovd(rscratch1, v0);\n+      masm->stp(rscratch1, r0, Address(masm->pre(sp, -2 * wordSize)));\n+    }\n+  }\n+\n+  static void restore_return_registers(MacroAssembler* masm) {\n+    if (InlineTypeReturnedAsFields) {\n+      masm->ld1(v4, v5, v6, v7, masm->T1D, Address(masm->post(sp, 4 * wordSize)));\n+      masm->ld1(v0, v1, v2, v3, masm->T1D, Address(masm->post(sp, 4 * wordSize)));\n+      masm->pop(RegSet::range(r0, r7), sp);\n+    } else {\n+      masm->ldp(rscratch1, r0, Address(masm->post(sp, 2 * wordSize)));\n+      masm->fmovd(v0, rscratch1);\n+    }\n+  }\n+\n@@ -10539,2 +10585,1 @@\n-      __ fmovd(rscratch1, v0);\n-      __ stp(rscratch1, r0, Address(__ pre(sp, -2 * wordSize)));\n+      save_return_registers(_masm);\n@@ -10549,2 +10594,1 @@\n-      __ ldp(rscratch1, r0, Address(__ post(sp, 2 * wordSize)));\n-      __ fmovd(v0, rscratch1);\n+      restore_return_registers(_masm);\n@@ -10569,2 +10613,1 @@\n-      __ fmovd(rscratch1, v0);\n-      __ stp(rscratch1, r0, Address(__ pre(sp, -2 * wordSize)));\n+      save_return_registers(_masm);\n@@ -10581,2 +10624,1 @@\n-      __ ldp(rscratch1, r0, Address(__ post(sp, 2 * wordSize)));\n-      __ fmovd(v0, rscratch1);\n+      restore_return_registers(_masm);\n@@ -11719,0 +11761,128 @@\n+  \/\/ Call here from the interpreter or compiled code to either load\n+  \/\/ multiple returned values from the inline type instance being\n+  \/\/ returned to registers or to store returned values to a newly\n+  \/\/ allocated inline type instance.\n+  address generate_return_value_stub(address destination, const char* name, bool has_res) {\n+    \/\/ We need to save all registers the calling convention may use so\n+    \/\/ the runtime calls read or update those registers. This needs to\n+    \/\/ be in sync with SharedRuntime::java_return_convention().\n+    \/\/ n.b. aarch64 asserts that frame::arg_reg_save_area_bytes == 0\n+    enum layout {\n+      j_rarg7_off = 0, j_rarg7_2,    \/\/ j_rarg7 is r0\n+      j_rarg6_off, j_rarg6_2,\n+      j_rarg5_off, j_rarg5_2,\n+      j_rarg4_off, j_rarg4_2,\n+      j_rarg3_off, j_rarg3_2,\n+      j_rarg2_off, j_rarg2_2,\n+      j_rarg1_off, j_rarg1_2,\n+      j_rarg0_off, j_rarg0_2,\n+\n+      j_farg7_off, j_farg7_2,\n+      j_farg6_off, j_farg6_2,\n+      j_farg5_off, j_farg5_2,\n+      j_farg4_off, j_farg4_2,\n+      j_farg3_off, j_farg3_2,\n+      j_farg2_off, j_farg2_2,\n+      j_farg1_off, j_farg1_2,\n+      j_farg0_off, j_farg0_2,\n+\n+      rfp_off, rfp_off2,\n+      return_off, return_off2,\n+\n+      framesize \/\/ inclusive of return address\n+    };\n+\n+    CodeBuffer code(name, 512, 64);\n+    MacroAssembler* masm = new MacroAssembler(&code);\n+\n+    int frame_size_in_bytes = align_up(framesize*BytesPerInt, 16);\n+    assert(frame_size_in_bytes == framesize*BytesPerInt, \"misaligned\");\n+    int frame_size_in_slots = frame_size_in_bytes \/ BytesPerInt;\n+    int frame_size_in_words = frame_size_in_bytes \/ wordSize;\n+\n+    OopMapSet* oop_maps = new OopMapSet();\n+    OopMap* map = new OopMap(frame_size_in_slots, 0);\n+\n+    map->set_callee_saved(VMRegImpl::stack2reg(j_rarg7_off), j_rarg7->as_VMReg());\n+    map->set_callee_saved(VMRegImpl::stack2reg(j_rarg6_off), j_rarg6->as_VMReg());\n+    map->set_callee_saved(VMRegImpl::stack2reg(j_rarg5_off), j_rarg5->as_VMReg());\n+    map->set_callee_saved(VMRegImpl::stack2reg(j_rarg4_off), j_rarg4->as_VMReg());\n+    map->set_callee_saved(VMRegImpl::stack2reg(j_rarg3_off), j_rarg3->as_VMReg());\n+    map->set_callee_saved(VMRegImpl::stack2reg(j_rarg2_off), j_rarg2->as_VMReg());\n+    map->set_callee_saved(VMRegImpl::stack2reg(j_rarg1_off), j_rarg1->as_VMReg());\n+    map->set_callee_saved(VMRegImpl::stack2reg(j_rarg0_off), j_rarg0->as_VMReg());\n+\n+    map->set_callee_saved(VMRegImpl::stack2reg(j_farg0_off), j_farg0->as_VMReg());\n+    map->set_callee_saved(VMRegImpl::stack2reg(j_farg1_off), j_farg1->as_VMReg());\n+    map->set_callee_saved(VMRegImpl::stack2reg(j_farg2_off), j_farg2->as_VMReg());\n+    map->set_callee_saved(VMRegImpl::stack2reg(j_farg3_off), j_farg3->as_VMReg());\n+    map->set_callee_saved(VMRegImpl::stack2reg(j_farg4_off), j_farg4->as_VMReg());\n+    map->set_callee_saved(VMRegImpl::stack2reg(j_farg5_off), j_farg5->as_VMReg());\n+    map->set_callee_saved(VMRegImpl::stack2reg(j_farg6_off), j_farg6->as_VMReg());\n+    map->set_callee_saved(VMRegImpl::stack2reg(j_farg7_off), j_farg7->as_VMReg());\n+\n+    address start = __ pc();\n+\n+    __ enter(); \/\/ Save FP and LR before call\n+\n+    __ stpd(j_farg1, j_farg0, Address(__ pre(sp, -2 * wordSize)));\n+    __ stpd(j_farg3, j_farg2, Address(__ pre(sp, -2 * wordSize)));\n+    __ stpd(j_farg5, j_farg4, Address(__ pre(sp, -2 * wordSize)));\n+    __ stpd(j_farg7, j_farg6, Address(__ pre(sp, -2 * wordSize)));\n+\n+    __ stp(j_rarg1, j_rarg0, Address(__ pre(sp, -2 * wordSize)));\n+    __ stp(j_rarg3, j_rarg2, Address(__ pre(sp, -2 * wordSize)));\n+    __ stp(j_rarg5, j_rarg4, Address(__ pre(sp, -2 * wordSize)));\n+    __ stp(j_rarg7, j_rarg6, Address(__ pre(sp, -2 * wordSize)));\n+\n+    int frame_complete = __ offset();\n+\n+    \/\/ Set up last_Java_sp and last_Java_fp\n+    address the_pc = __ pc();\n+    __ set_last_Java_frame(sp, noreg, the_pc, rscratch1);\n+\n+    \/\/ Call runtime\n+    __ mov(c_rarg1, r0);\n+    __ mov(c_rarg0, rthread);\n+\n+    __ mov(rscratch1, destination);\n+    __ blr(rscratch1);\n+\n+    oop_maps->add_gc_map(the_pc - start, map);\n+\n+    __ reset_last_Java_frame(false);\n+\n+    __ ldp(j_rarg7, j_rarg6, Address(__ post(sp, 2 * wordSize)));\n+    __ ldp(j_rarg5, j_rarg4, Address(__ post(sp, 2 * wordSize)));\n+    __ ldp(j_rarg3, j_rarg2, Address(__ post(sp, 2 * wordSize)));\n+    __ ldp(j_rarg1, j_rarg0, Address(__ post(sp, 2 * wordSize)));\n+\n+    __ ldpd(j_farg7, j_farg6, Address(__ post(sp, 2 * wordSize)));\n+    __ ldpd(j_farg5, j_farg4, Address(__ post(sp, 2 * wordSize)));\n+    __ ldpd(j_farg3, j_farg2, Address(__ post(sp, 2 * wordSize)));\n+    __ ldpd(j_farg1, j_farg0, Address(__ post(sp, 2 * wordSize)));\n+\n+    __ leave();\n+\n+    \/\/ check for pending exceptions\n+    Label pending;\n+    __ ldr(rscratch1, Address(rthread, in_bytes(Thread::pending_exception_offset())));\n+    __ cbnz(rscratch1, pending);\n+\n+    if (has_res) {\n+      __ get_vm_result_oop(r0, rthread);\n+    }\n+\n+    __ ret(lr);\n+\n+    __ bind(pending);\n+    __ far_jump(RuntimeAddress(StubRoutines::forward_exception_entry()));\n+\n+    \/\/ -------------\n+    \/\/ make sure all code is generated\n+    masm->flush();\n+\n+    RuntimeStub* stub = RuntimeStub::new_runtime_stub(name, &code, frame_complete, frame_size_in_words, oop_maps, false);\n+    return stub->entry_point();\n+  }\n+\n@@ -11767,0 +11937,8 @@\n+\n+    if (InlineTypeReturnedAsFields) {\n+      StubRoutines::_load_inline_type_fields_in_regs =\n+         generate_return_value_stub(CAST_FROM_FN_PTR(address, SharedRuntime::load_inline_type_fields_in_regs), \"load_inline_type_fields_in_regs\", false);\n+      StubRoutines::_store_inline_type_fields_to_buf =\n+         generate_return_value_stub(CAST_FROM_FN_PTR(address, SharedRuntime::store_inline_type_fields_to_buf), \"store_inline_type_fields_to_buf\", true);\n+    }\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/stubGenerator_aarch64.cpp","additions":198,"deletions":20,"binary":false,"changes":218,"status":"modified"},{"patch":"@@ -238,2 +238,18 @@\n-  \/\/ Determine and save the live input values\n-  __ push_call_clobbered_registers();\n+  if (EnableValhalla && InlineTypePassFieldsAsArgs) {\n+    \/\/ Barriers might be emitted when converting between (scalarized) calling conventions for inline\n+    \/\/ types. Save all argument registers before calling into the runtime.\n+    \/\/ TODO 8366717: use push_set() (see JDK-8283327 push\/pop_call_clobbered_registers & aarch64 )\n+    __ pusha();\n+    __ subptr(rsp, 64);\n+    __ movdbl(Address(rsp, 0),  j_farg0);\n+    __ movdbl(Address(rsp, 8),  j_farg1);\n+    __ movdbl(Address(rsp, 16), j_farg2);\n+    __ movdbl(Address(rsp, 24), j_farg3);\n+    __ movdbl(Address(rsp, 32), j_farg4);\n+    __ movdbl(Address(rsp, 40), j_farg5);\n+    __ movdbl(Address(rsp, 48), j_farg6);\n+    __ movdbl(Address(rsp, 56), j_farg7);\n+  } else {\n+    \/\/ Determine and save the live input values\n+    __ push_call_clobbered_registers();\n+  }\n@@ -266,1 +282,15 @@\n-  __ pop_call_clobbered_registers();\n+  if (EnableValhalla && InlineTypePassFieldsAsArgs) {\n+    \/\/ Restore registers\n+    __ movdbl(j_farg0, Address(rsp, 0));\n+    __ movdbl(j_farg1, Address(rsp, 8));\n+    __ movdbl(j_farg2, Address(rsp, 16));\n+    __ movdbl(j_farg3, Address(rsp, 24));\n+    __ movdbl(j_farg4, Address(rsp, 32));\n+    __ movdbl(j_farg5, Address(rsp, 40));\n+    __ movdbl(j_farg6, Address(rsp, 48));\n+    __ movdbl(j_farg7, Address(rsp, 56));\n+    __ addptr(rsp, 64);\n+    __ popa();\n+  } else {\n+    __ pop_call_clobbered_registers();\n+  }\n@@ -383,0 +413,1 @@\n+  bool dest_uninitialized = (decorators & IS_DEST_UNINITIALIZED) != 0;\n@@ -384,1 +415,1 @@\n-  bool needs_pre_barrier = as_normal;\n+  bool needs_pre_barrier = as_normal && !dest_uninitialized;\n","filename":"src\/hotspot\/cpu\/x86\/gc\/g1\/g1BarrierSetAssembler_x86.cpp","additions":35,"deletions":4,"binary":false,"changes":39,"status":"modified"},{"patch":"@@ -1652,0 +1652,4 @@\n+  if (_entry_point == nullptr) {\n+    \/\/ CallLeafNoFPInDirect\n+    return 3; \/\/ callq (register)\n+  }\n@@ -1658,0 +1662,1 @@\n+\n@@ -1883,6 +1888,1 @@\n-  int framesize = C->output()->frame_size_in_bytes();\n-  int bangsize = C->output()->bang_size_in_bytes();\n-\n-  if (C->clinit_barrier_on_entry()) {\n-    assert(VM_Version::supports_fast_class_init_checks(), \"sanity\");\n-    assert(!C->method()->holder()->is_not_initialized(), \"initialization should have been started\");\n+  __ verified_entry(C);\n@@ -1890,9 +1890,2 @@\n-    Label L_skip_barrier;\n-    Register klass = rscratch1;\n-\n-    __ mov_metadata(klass, C->method()->holder()->constant_encoding());\n-    __ clinit_barrier(klass, &L_skip_barrier \/*L_fast_path*\/);\n-\n-    __ jump(RuntimeAddress(SharedRuntime::get_handle_wrong_method_stub())); \/\/ slow path\n-\n-    __ bind(L_skip_barrier);\n+  if (ra_->C->stub_function() == nullptr) {\n+    __ entry_barrier();\n@@ -1901,1 +1894,3 @@\n-  __ verified_entry(framesize, C->output()->need_stack_bang(bangsize)?bangsize:0, false, C->stub_function() != nullptr);\n+  if (!Compile::current()->output()->in_scratch_emit_size()) {\n+    __ bind(*_verified_entry);\n+  }\n@@ -1913,5 +1908,0 @@\n-uint MachPrologNode::size(PhaseRegAlloc* ra_) const\n-{\n-  return MachNode::size(ra_); \/\/ too many variables; just compute it\n-                              \/\/ the hard way\n-}\n@@ -1965,13 +1955,3 @@\n-  int framesize = C->output()->frame_size_in_bytes();\n-  assert((framesize & (StackAlignmentInBytes-1)) == 0, \"frame size not aligned\");\n-  \/\/ Remove word for return adr already pushed\n-  \/\/ and RBP\n-  framesize -= 2*wordSize;\n-\n-  \/\/ Note that VerifyStackAtCalls' Majik cookie does not change the frame size popped here\n-\n-  if (framesize) {\n-    __ addq(rsp, framesize);\n-  }\n-\n-  __ popq(rbp);\n+  \/\/ Subtract two words to account for return address and rbp\n+  int initial_framesize = C->output()->frame_size_in_bytes() - 2*wordSize;\n+  __ remove_frame(initial_framesize, C->needs_stack_repair());\n@@ -1996,6 +1976,0 @@\n-uint MachEpilogNode::size(PhaseRegAlloc* ra_) const\n-{\n-  return MachNode::size(ra_); \/\/ too many variables; just compute it\n-                              \/\/ the hard way\n-}\n-\n@@ -2603,0 +2577,43 @@\n+\/\/=============================================================================\n+#ifndef PRODUCT\n+void MachVEPNode::format(PhaseRegAlloc* ra_, outputStream* st) const\n+{\n+  st->print_cr(\"MachVEPNode\");\n+}\n+#endif\n+\n+void MachVEPNode::emit(C2_MacroAssembler* masm, PhaseRegAlloc* ra_) const\n+{\n+  CodeBuffer* cbuf = masm->code();\n+  uint insts_size = cbuf->insts_size();\n+  if (!_verified) {\n+    __ ic_check(1);\n+  } else {\n+    \/\/ TODO 8284443 Avoid creation of temporary frame\n+    if (ra_->C->stub_function() == nullptr) {\n+      __ verified_entry(ra_->C, 0);\n+      __ entry_barrier();\n+      int initial_framesize = ra_->C->output()->frame_size_in_bytes() - 2*wordSize;\n+      __ remove_frame(initial_framesize, false);\n+    }\n+    \/\/ Unpack inline type args passed as oop and then jump to\n+    \/\/ the verified entry point (skipping the unverified entry).\n+    int sp_inc = __ unpack_inline_args(ra_->C, _receiver_only);\n+    \/\/ Emit code for verified entry and save increment for stack repair on return\n+    __ verified_entry(ra_->C, sp_inc);\n+    if (Compile::current()->output()->in_scratch_emit_size()) {\n+      Label dummy_verified_entry;\n+      __ jmp(dummy_verified_entry);\n+    } else {\n+      __ jmp(*_verified_entry);\n+    }\n+  }\n+  \/* WARNING these NOPs are critical so that verified entry point is properly\n+     4 bytes aligned for patching by NativeJump::patch_verified_entry() *\/\n+  int nops_cnt = 4 - ((cbuf->insts_size() - insts_size) & 0x3);\n+  nops_cnt &= 0x3; \/\/ Do not add nops if code is aligned.\n+  if (nops_cnt > 0) {\n+    __ nop(nops_cnt);\n+  }\n+}\n+\n@@ -2623,6 +2640,0 @@\n-uint MachUEPNode::size(PhaseRegAlloc* ra_) const\n-{\n-  return MachNode::size(ra_); \/\/ too many variables; just compute it\n-                              \/\/ the hard way\n-}\n-\n@@ -4603,0 +4614,33 @@\n+    if (tf()->returns_inline_type_as_fields() && !_method->is_method_handle_intrinsic() && _method->return_type()->is_loaded()) {\n+      \/\/ The last return value is not set by the callee but used to pass the null marker to compiled code.\n+      \/\/ Search for the corresponding projection, get the register and emit code that initialized it.\n+      uint con = (tf()->range_cc()->cnt() - 1);\n+      for (DUIterator_Fast imax, i = fast_outs(imax); i < imax; i++) {\n+        ProjNode* proj = fast_out(i)->as_Proj();\n+        if (proj->_con == con) {\n+          \/\/ Set null marker if rax is non-null (a non-null value is returned buffered or scalarized)\n+          OptoReg::Name optoReg = ra_->get_reg_first(proj);\n+          VMReg reg = OptoReg::as_VMReg(optoReg, ra_->_framesize, OptoReg::reg2stack(ra_->_matcher._new_SP));\n+          Register toReg = reg->is_reg() ? reg->as_Register() : rscratch1;\n+          __ testq(rax, rax);\n+          __ setb(Assembler::notZero, toReg);\n+          __ movzbl(toReg, toReg);\n+          if (reg->is_stack()) {\n+            int st_off = reg->reg2stack() * VMRegImpl::stack_slot_size;\n+            __ movq(Address(rsp, st_off), toReg);\n+          }\n+          break;\n+        }\n+      }\n+      if (return_value_is_used()) {\n+        \/\/ An inline type is returned as fields in multiple registers.\n+        \/\/ Rax either contains an oop if the inline type is buffered or a pointer\n+        \/\/ to the corresponding InlineKlass with the lowest bit set to 1. Zero rax\n+        \/\/ if the lowest bit is set to allow C2 to use the oop after null checking.\n+        \/\/ rax &= (rax & 1) - 1\n+        __ movptr(rscratch1, rax);\n+        __ andptr(rscratch1, 0x1);\n+        __ subptr(rscratch1, 0x1);\n+        __ andptr(rax, rscratch1);\n+      }\n+    }\n@@ -5765,0 +5809,16 @@\n+\/\/ Indirect Narrow Oop Operand\n+operand indCompressedOop(rRegN reg) %{\n+  predicate(UseCompressedOops && (CompressedOops::shift() == Address::times_8));\n+  constraint(ALLOC_IN_RC(ptr_reg));\n+  match(DecodeN reg);\n+\n+  op_cost(10);\n+  format %{\"[R12 + $reg << 3] (compressed oop addressing)\" %}\n+  interface(MEMORY_INTER) %{\n+    base(0xc); \/\/ R12\n+    index($reg);\n+    scale(0x3);\n+    disp(0x0);\n+  %}\n+%}\n+\n@@ -6241,1 +6301,1 @@\n-               indCompressedOopOffset,\n+               indCompressedOop, indCompressedOopOffset,\n@@ -8839,0 +8899,26 @@\n+instruct castI2N(rRegN dst, rRegI src)\n+%{\n+  match(Set dst (CastI2N src));\n+\n+  format %{ \"movq    $dst, $src\\t# int -> narrow ptr\" %}\n+  ins_encode %{\n+    if ($dst$$reg != $src$$reg) {\n+      __ movl($dst$$Register, $src$$Register);\n+    }\n+  %}\n+  ins_pipe(ialu_reg_reg); \/\/ XXX\n+%}\n+\n+instruct castN2X(rRegL dst, rRegN src)\n+%{\n+  match(Set dst (CastP2X src));\n+\n+  format %{ \"movq    $dst, $src\\t# ptr -> long\" %}\n+  ins_encode %{\n+    if ($dst$$reg != $src$$reg) {\n+      __ movptr($dst$$Register, $src$$Register);\n+    }\n+  %}\n+  ins_pipe(ialu_reg_reg); \/\/ XXX\n+%}\n+\n@@ -15085,0 +15171,1 @@\n+\n@@ -15087,1 +15174,1 @@\n-instruct rep_stos(rcx_RegL cnt, rdi_RegP base, regD tmp, rax_RegI zero,\n+instruct rep_stos(rcx_RegL cnt, rdi_RegP base, regD tmp, rax_RegL val,\n@@ -15090,3 +15177,120 @@\n-  predicate(!((ClearArrayNode*)n)->is_large() && (UseAVX <= 2));\n-  match(Set dummy (ClearArray cnt base));\n-  effect(USE_KILL cnt, USE_KILL base, TEMP tmp, KILL zero, KILL cr);\n+  predicate(!((ClearArrayNode*)n)->is_large() && !((ClearArrayNode*)n)->word_copy_only() && (UseAVX <= 2));\n+  match(Set dummy (ClearArray (Binary cnt base) val));\n+  effect(USE_KILL cnt, USE_KILL base, TEMP tmp, USE_KILL val, KILL cr);\n+\n+  format %{ $$template\n+    $$emit$$\"cmp     InitArrayShortSize,rcx\\n\\t\"\n+    $$emit$$\"jg      LARGE\\n\\t\"\n+    $$emit$$\"dec     rcx\\n\\t\"\n+    $$emit$$\"js      DONE\\t# Zero length\\n\\t\"\n+    $$emit$$\"mov     rax,(rdi,rcx,8)\\t# LOOP\\n\\t\"\n+    $$emit$$\"dec     rcx\\n\\t\"\n+    $$emit$$\"jge     LOOP\\n\\t\"\n+    $$emit$$\"jmp     DONE\\n\\t\"\n+    $$emit$$\"# LARGE:\\n\\t\"\n+    if (UseFastStosb) {\n+       $$emit$$\"shlq    rcx,3\\t# Convert doublewords to bytes\\n\\t\"\n+       $$emit$$\"rep     stosb\\t# Store rax to *rdi++ while rcx--\\n\\t\"\n+    } else if (UseXMMForObjInit) {\n+       $$emit$$\"movdq   $tmp, $val\\n\\t\"\n+       $$emit$$\"punpcklqdq $tmp, $tmp\\n\\t\"\n+       $$emit$$\"vinserti128_high $tmp, $tmp\\n\\t\"\n+       $$emit$$\"jmpq    L_zero_64_bytes\\n\\t\"\n+       $$emit$$\"# L_loop:\\t# 64-byte LOOP\\n\\t\"\n+       $$emit$$\"vmovdqu $tmp,(rax)\\n\\t\"\n+       $$emit$$\"vmovdqu $tmp,0x20(rax)\\n\\t\"\n+       $$emit$$\"add     0x40,rax\\n\\t\"\n+       $$emit$$\"# L_zero_64_bytes:\\n\\t\"\n+       $$emit$$\"sub     0x8,rcx\\n\\t\"\n+       $$emit$$\"jge     L_loop\\n\\t\"\n+       $$emit$$\"add     0x4,rcx\\n\\t\"\n+       $$emit$$\"jl      L_tail\\n\\t\"\n+       $$emit$$\"vmovdqu $tmp,(rax)\\n\\t\"\n+       $$emit$$\"add     0x20,rax\\n\\t\"\n+       $$emit$$\"sub     0x4,rcx\\n\\t\"\n+       $$emit$$\"# L_tail:\\t# Clearing tail bytes\\n\\t\"\n+       $$emit$$\"add     0x4,rcx\\n\\t\"\n+       $$emit$$\"jle     L_end\\n\\t\"\n+       $$emit$$\"dec     rcx\\n\\t\"\n+       $$emit$$\"# L_sloop:\\t# 8-byte short loop\\n\\t\"\n+       $$emit$$\"vmovq   xmm0,(rax)\\n\\t\"\n+       $$emit$$\"add     0x8,rax\\n\\t\"\n+       $$emit$$\"dec     rcx\\n\\t\"\n+       $$emit$$\"jge     L_sloop\\n\\t\"\n+       $$emit$$\"# L_end:\\n\\t\"\n+    } else {\n+       $$emit$$\"rep     stosq\\t# Store rax to *rdi++ while rcx--\\n\\t\"\n+    }\n+    $$emit$$\"# DONE\"\n+  %}\n+  ins_encode %{\n+    __ clear_mem($base$$Register, $cnt$$Register, $val$$Register,\n+                 $tmp$$XMMRegister, false, false);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct rep_stos_word_copy(rcx_RegL cnt, rdi_RegP base, regD tmp, rax_RegL val,\n+                            Universe dummy, rFlagsReg cr)\n+%{\n+  predicate(!((ClearArrayNode*)n)->is_large() && ((ClearArrayNode*)n)->word_copy_only() && (UseAVX <= 2));\n+  match(Set dummy (ClearArray (Binary cnt base) val));\n+  effect(USE_KILL cnt, USE_KILL base, TEMP tmp, USE_KILL val, KILL cr);\n+\n+  format %{ $$template\n+    $$emit$$\"cmp     InitArrayShortSize,rcx\\n\\t\"\n+    $$emit$$\"jg      LARGE\\n\\t\"\n+    $$emit$$\"dec     rcx\\n\\t\"\n+    $$emit$$\"js      DONE\\t# Zero length\\n\\t\"\n+    $$emit$$\"mov     rax,(rdi,rcx,8)\\t# LOOP\\n\\t\"\n+    $$emit$$\"dec     rcx\\n\\t\"\n+    $$emit$$\"jge     LOOP\\n\\t\"\n+    $$emit$$\"jmp     DONE\\n\\t\"\n+    $$emit$$\"# LARGE:\\n\\t\"\n+    if (UseXMMForObjInit) {\n+       $$emit$$\"movdq   $tmp, $val\\n\\t\"\n+       $$emit$$\"punpcklqdq $tmp, $tmp\\n\\t\"\n+       $$emit$$\"vinserti128_high $tmp, $tmp\\n\\t\"\n+       $$emit$$\"jmpq    L_zero_64_bytes\\n\\t\"\n+       $$emit$$\"# L_loop:\\t# 64-byte LOOP\\n\\t\"\n+       $$emit$$\"vmovdqu $tmp,(rax)\\n\\t\"\n+       $$emit$$\"vmovdqu $tmp,0x20(rax)\\n\\t\"\n+       $$emit$$\"add     0x40,rax\\n\\t\"\n+       $$emit$$\"# L_zero_64_bytes:\\n\\t\"\n+       $$emit$$\"sub     0x8,rcx\\n\\t\"\n+       $$emit$$\"jge     L_loop\\n\\t\"\n+       $$emit$$\"add     0x4,rcx\\n\\t\"\n+       $$emit$$\"jl      L_tail\\n\\t\"\n+       $$emit$$\"vmovdqu $tmp,(rax)\\n\\t\"\n+       $$emit$$\"add     0x20,rax\\n\\t\"\n+       $$emit$$\"sub     0x4,rcx\\n\\t\"\n+       $$emit$$\"# L_tail:\\t# Clearing tail bytes\\n\\t\"\n+       $$emit$$\"add     0x4,rcx\\n\\t\"\n+       $$emit$$\"jle     L_end\\n\\t\"\n+       $$emit$$\"dec     rcx\\n\\t\"\n+       $$emit$$\"# L_sloop:\\t# 8-byte short loop\\n\\t\"\n+       $$emit$$\"vmovq   xmm0,(rax)\\n\\t\"\n+       $$emit$$\"add     0x8,rax\\n\\t\"\n+       $$emit$$\"dec     rcx\\n\\t\"\n+       $$emit$$\"jge     L_sloop\\n\\t\"\n+       $$emit$$\"# L_end:\\n\\t\"\n+    } else {\n+       $$emit$$\"rep     stosq\\t# Store rax to *rdi++ while rcx--\\n\\t\"\n+    }\n+    $$emit$$\"# DONE\"\n+  %}\n+  ins_encode %{\n+    __ clear_mem($base$$Register, $cnt$$Register, $val$$Register,\n+                 $tmp$$XMMRegister, false, true);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ Small non-constant length ClearArray for AVX512 targets.\n+instruct rep_stos_evex(rcx_RegL cnt, rdi_RegP base, legRegD tmp, kReg ktmp, rax_RegL val,\n+                       Universe dummy, rFlagsReg cr)\n+%{\n+  predicate(!((ClearArrayNode*)n)->is_large() && !((ClearArrayNode*)n)->word_copy_only() && (UseAVX > 2));\n+  match(Set dummy (ClearArray (Binary cnt base) val));\n+  ins_cost(125);\n+  effect(USE_KILL cnt, USE_KILL base, TEMP tmp, TEMP ktmp, USE_KILL val, KILL cr);\n@@ -15140,2 +15344,2 @@\n-    __ clear_mem($base$$Register, $cnt$$Register, $zero$$Register,\n-                 $tmp$$XMMRegister, false, knoreg);\n+    __ clear_mem($base$$Register, $cnt$$Register, $val$$Register,\n+                 $tmp$$XMMRegister, false, false, $ktmp$$KRegister);\n@@ -15146,3 +15350,2 @@\n-\/\/ Small non-constant length ClearArray for AVX512 targets.\n-instruct rep_stos_evex(rcx_RegL cnt, rdi_RegP base, legRegD tmp, kReg ktmp, rax_RegI zero,\n-                       Universe dummy, rFlagsReg cr)\n+instruct rep_stos_evex_word_copy(rcx_RegL cnt, rdi_RegP base, legRegD tmp, kReg ktmp, rax_RegL val,\n+                                 Universe dummy, rFlagsReg cr)\n@@ -15150,2 +15353,2 @@\n-  predicate(!((ClearArrayNode*)n)->is_large() && (UseAVX > 2));\n-  match(Set dummy (ClearArray cnt base));\n+  predicate(!((ClearArrayNode*)n)->is_large() && ((ClearArrayNode*)n)->word_copy_only() && (UseAVX > 2));\n+  match(Set dummy (ClearArray (Binary cnt base) val));\n@@ -15153,1 +15356,1 @@\n-  effect(USE_KILL cnt, USE_KILL base, TEMP tmp, TEMP ktmp, KILL zero, KILL cr);\n+  effect(USE_KILL cnt, USE_KILL base, TEMP tmp, TEMP ktmp, USE_KILL val, KILL cr);\n@@ -15201,2 +15404,2 @@\n-    __ clear_mem($base$$Register, $cnt$$Register, $zero$$Register,\n-                 $tmp$$XMMRegister, false, $ktmp$$KRegister);\n+    __ clear_mem($base$$Register, $cnt$$Register, $val$$Register,\n+                 $tmp$$XMMRegister, false, true, $ktmp$$KRegister);\n@@ -15208,1 +15411,1 @@\n-instruct rep_stos_large(rcx_RegL cnt, rdi_RegP base, regD tmp, rax_RegI zero,\n+instruct rep_stos_large(rcx_RegL cnt, rdi_RegP base, regD tmp, rax_RegL val,\n@@ -15211,3 +15414,99 @@\n-  predicate((UseAVX <=2) && ((ClearArrayNode*)n)->is_large());\n-  match(Set dummy (ClearArray cnt base));\n-  effect(USE_KILL cnt, USE_KILL base, TEMP tmp, KILL zero, KILL cr);\n+  predicate(((ClearArrayNode*)n)->is_large() && !((ClearArrayNode*)n)->word_copy_only() && (UseAVX <= 2));\n+  match(Set dummy (ClearArray (Binary cnt base) val));\n+  effect(USE_KILL cnt, USE_KILL base, TEMP tmp, USE_KILL val, KILL cr);\n+\n+  format %{ $$template\n+    if (UseFastStosb) {\n+       $$emit$$\"shlq    rcx,3\\t# Convert doublewords to bytes\\n\\t\"\n+       $$emit$$\"rep     stosb\\t# Store rax to *rdi++ while rcx--\"\n+    } else if (UseXMMForObjInit) {\n+       $$emit$$\"movdq   $tmp, $val\\n\\t\"\n+       $$emit$$\"punpcklqdq $tmp, $tmp\\n\\t\"\n+       $$emit$$\"vinserti128_high $tmp, $tmp\\n\\t\"\n+       $$emit$$\"jmpq    L_zero_64_bytes\\n\\t\"\n+       $$emit$$\"# L_loop:\\t# 64-byte LOOP\\n\\t\"\n+       $$emit$$\"vmovdqu $tmp,(rax)\\n\\t\"\n+       $$emit$$\"vmovdqu $tmp,0x20(rax)\\n\\t\"\n+       $$emit$$\"add     0x40,rax\\n\\t\"\n+       $$emit$$\"# L_zero_64_bytes:\\n\\t\"\n+       $$emit$$\"sub     0x8,rcx\\n\\t\"\n+       $$emit$$\"jge     L_loop\\n\\t\"\n+       $$emit$$\"add     0x4,rcx\\n\\t\"\n+       $$emit$$\"jl      L_tail\\n\\t\"\n+       $$emit$$\"vmovdqu $tmp,(rax)\\n\\t\"\n+       $$emit$$\"add     0x20,rax\\n\\t\"\n+       $$emit$$\"sub     0x4,rcx\\n\\t\"\n+       $$emit$$\"# L_tail:\\t# Clearing tail bytes\\n\\t\"\n+       $$emit$$\"add     0x4,rcx\\n\\t\"\n+       $$emit$$\"jle     L_end\\n\\t\"\n+       $$emit$$\"dec     rcx\\n\\t\"\n+       $$emit$$\"# L_sloop:\\t# 8-byte short loop\\n\\t\"\n+       $$emit$$\"vmovq   xmm0,(rax)\\n\\t\"\n+       $$emit$$\"add     0x8,rax\\n\\t\"\n+       $$emit$$\"dec     rcx\\n\\t\"\n+       $$emit$$\"jge     L_sloop\\n\\t\"\n+       $$emit$$\"# L_end:\\n\\t\"\n+    } else {\n+       $$emit$$\"rep     stosq\\t# Store rax to *rdi++ while rcx--\"\n+    }\n+  %}\n+  ins_encode %{\n+    __ clear_mem($base$$Register, $cnt$$Register, $val$$Register,\n+                 $tmp$$XMMRegister, true, false);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct rep_stos_large_word_copy(rcx_RegL cnt, rdi_RegP base, regD tmp, rax_RegL val,\n+                                  Universe dummy, rFlagsReg cr)\n+%{\n+  predicate(((ClearArrayNode*)n)->is_large() && ((ClearArrayNode*)n)->word_copy_only() && (UseAVX <= 2));\n+  match(Set dummy (ClearArray (Binary cnt base) val));\n+  effect(USE_KILL cnt, USE_KILL base, TEMP tmp, USE_KILL val, KILL cr);\n+\n+  format %{ $$template\n+    if (UseXMMForObjInit) {\n+       $$emit$$\"movdq   $tmp, $val\\n\\t\"\n+       $$emit$$\"punpcklqdq $tmp, $tmp\\n\\t\"\n+       $$emit$$\"vinserti128_high $tmp, $tmp\\n\\t\"\n+       $$emit$$\"jmpq    L_zero_64_bytes\\n\\t\"\n+       $$emit$$\"# L_loop:\\t# 64-byte LOOP\\n\\t\"\n+       $$emit$$\"vmovdqu $tmp,(rax)\\n\\t\"\n+       $$emit$$\"vmovdqu $tmp,0x20(rax)\\n\\t\"\n+       $$emit$$\"add     0x40,rax\\n\\t\"\n+       $$emit$$\"# L_zero_64_bytes:\\n\\t\"\n+       $$emit$$\"sub     0x8,rcx\\n\\t\"\n+       $$emit$$\"jge     L_loop\\n\\t\"\n+       $$emit$$\"add     0x4,rcx\\n\\t\"\n+       $$emit$$\"jl      L_tail\\n\\t\"\n+       $$emit$$\"vmovdqu $tmp,(rax)\\n\\t\"\n+       $$emit$$\"add     0x20,rax\\n\\t\"\n+       $$emit$$\"sub     0x4,rcx\\n\\t\"\n+       $$emit$$\"# L_tail:\\t# Clearing tail bytes\\n\\t\"\n+       $$emit$$\"add     0x4,rcx\\n\\t\"\n+       $$emit$$\"jle     L_end\\n\\t\"\n+       $$emit$$\"dec     rcx\\n\\t\"\n+       $$emit$$\"# L_sloop:\\t# 8-byte short loop\\n\\t\"\n+       $$emit$$\"vmovq   xmm0,(rax)\\n\\t\"\n+       $$emit$$\"add     0x8,rax\\n\\t\"\n+       $$emit$$\"dec     rcx\\n\\t\"\n+       $$emit$$\"jge     L_sloop\\n\\t\"\n+       $$emit$$\"# L_end:\\n\\t\"\n+    } else {\n+       $$emit$$\"rep     stosq\\t# Store rax to *rdi++ while rcx--\"\n+    }\n+  %}\n+  ins_encode %{\n+    __ clear_mem($base$$Register, $cnt$$Register, $val$$Register,\n+                 $tmp$$XMMRegister, true, true);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ Large non-constant length ClearArray for AVX512 targets.\n+instruct rep_stos_large_evex(rcx_RegL cnt, rdi_RegP base, legRegD tmp, kReg ktmp, rax_RegL val,\n+                             Universe dummy, rFlagsReg cr)\n+%{\n+  predicate(((ClearArrayNode*)n)->is_large() && !((ClearArrayNode*)n)->word_copy_only() && (UseAVX > 2));\n+  match(Set dummy (ClearArray (Binary cnt base) val));\n+  effect(USE_KILL cnt, USE_KILL base, TEMP tmp, TEMP ktmp, USE_KILL val, KILL cr);\n@@ -15252,2 +15551,2 @@\n-    __ clear_mem($base$$Register, $cnt$$Register, $zero$$Register,\n-                 $tmp$$XMMRegister, true, knoreg);\n+    __ clear_mem($base$$Register, $cnt$$Register, $val$$Register,\n+                 $tmp$$XMMRegister, true, false, $ktmp$$KRegister);\n@@ -15258,3 +15557,2 @@\n-\/\/ Large non-constant length ClearArray for AVX512 targets.\n-instruct rep_stos_large_evex(rcx_RegL cnt, rdi_RegP base, legRegD tmp, kReg ktmp, rax_RegI zero,\n-                             Universe dummy, rFlagsReg cr)\n+instruct rep_stos_large_evex_word_copy(rcx_RegL cnt, rdi_RegP base, legRegD tmp, kReg ktmp, rax_RegL val,\n+                                       Universe dummy, rFlagsReg cr)\n@@ -15262,3 +15560,3 @@\n-  predicate((UseAVX > 2) && ((ClearArrayNode*)n)->is_large());\n-  match(Set dummy (ClearArray cnt base));\n-  effect(USE_KILL cnt, USE_KILL base, TEMP tmp, TEMP ktmp, KILL zero, KILL cr);\n+  predicate(((ClearArrayNode*)n)->is_large() && ((ClearArrayNode*)n)->word_copy_only() && (UseAVX > 2));\n+  match(Set dummy (ClearArray (Binary cnt base) val));\n+  effect(USE_KILL cnt, USE_KILL base, TEMP tmp, TEMP ktmp, USE_KILL val, KILL cr);\n@@ -15303,2 +15601,2 @@\n-    __ clear_mem($base$$Register, $cnt$$Register, $zero$$Register,\n-                 $tmp$$XMMRegister, true, $ktmp$$KRegister);\n+    __ clear_mem($base$$Register, $cnt$$Register, $val$$Register,\n+                 $tmp$$XMMRegister, true, true, $ktmp$$KRegister);\n@@ -15310,1 +15608,1 @@\n-instruct rep_stos_im(immL cnt, rRegP base, regD tmp, rRegI zero, kReg ktmp, Universe dummy, rFlagsReg cr)\n+instruct rep_stos_im(immL cnt, rRegP base, regD tmp, rax_RegL val, kReg ktmp, Universe dummy, rFlagsReg cr)\n@@ -15312,2 +15610,3 @@\n-  predicate(!((ClearArrayNode*)n)->is_large() && (MaxVectorSize >= 32) && VM_Version::supports_avx512vl());\n-  match(Set dummy (ClearArray cnt base));\n+  predicate(!((ClearArrayNode*)n)->is_large() && !((ClearArrayNode*)n)->word_copy_only() &&\n+            ((MaxVectorSize >= 32) && VM_Version::supports_avx512vl()));\n+  match(Set dummy (ClearArray (Binary cnt base) val));\n@@ -15315,1 +15614,1 @@\n-  effect(TEMP tmp, TEMP zero, TEMP ktmp, KILL cr);\n+  effect(TEMP tmp, USE_KILL val, TEMP ktmp, KILL cr);\n@@ -15318,1 +15617,1 @@\n-   __ clear_mem($base$$Register, $cnt$$constant, $zero$$Register, $tmp$$XMMRegister, $ktmp$$KRegister);\n+    __ clear_mem($base$$Register, $cnt$$constant, $val$$Register, $tmp$$XMMRegister, $ktmp$$KRegister);\n@@ -17165,0 +17464,16 @@\n+\/\/ Call runtime without safepoint\n+\/\/ entry point is null, target holds the address to call\n+instruct CallLeafNoFPInDirect(rRegP target)\n+%{\n+  predicate(n->as_Call()->entry_point() == nullptr);\n+  match(CallLeafNoFP target);\n+\n+  ins_cost(300);\n+  format %{ \"call_leaf_nofp,runtime indirect \" %}\n+  ins_encode %{\n+     __ call($target$$Register);\n+  %}\n+\n+  ins_pipe(pipe_slow);\n+%}\n+\n@@ -17168,0 +17483,1 @@\n+  predicate(n->as_Call()->entry_point() != nullptr);\n","filename":"src\/hotspot\/cpu\/x86\/x86.ad","additions":396,"deletions":80,"binary":false,"changes":476,"status":"modified"},{"patch":"@@ -56,0 +56,3 @@\n+                 Inline_Entry,\n+                 Verified_Inline_Entry,\n+                 Verified_Inline_Entry_RO,\n@@ -70,0 +73,1 @@\n+  void check(int e) const { assert(0 <= e && e < max_Entries, \"must be\"); }\n@@ -75,0 +79,3 @@\n+    _values[Inline_Entry  ] = 0;\n+    _values[Verified_Inline_Entry] = -1;\n+    _values[Verified_Inline_Entry_RO] = -1;\n@@ -82,2 +89,2 @@\n-  int value(Entries e) { return _values[e]; }\n-  void set_value(Entries e, int val) { _values[e] = val; }\n+  int value(Entries e) const { check(e); return _values[e]; }\n+  void set_value(Entries e, int val) { check(e); _values[e] = val; }\n","filename":"src\/hotspot\/share\/asm\/codeBuffer.hpp","additions":9,"deletions":2,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -297,1 +297,1 @@\n-  memset(mem, 0, objArrayOopDesc::object_size(element_count));\n+  memset(mem, 0, refArrayOopDesc::object_size(element_count));\n@@ -303,0 +303,1 @@\n+    assert(!EnableValhalla || Universe::objectArrayKlass()->prototype_header() == markWord::prototype(), \"should be the same\");\n@@ -333,1 +334,1 @@\n-  assert(objArrayOopDesc::object_size(max_elem_count)*HeapWordSize == MIN_GC_REGION_ALIGNMENT,\n+  assert(refArrayOopDesc::object_size(max_elem_count)*HeapWordSize == MIN_GC_REGION_ALIGNMENT,\n@@ -448,1 +449,1 @@\n-  size_t byte_size = objArrayOopDesc::object_size(length) * HeapWordSize;\n+  size_t byte_size = refArrayOopDesc::object_size(length) * HeapWordSize;\n@@ -477,0 +478,1 @@\n+    assert(!EnableValhalla || Universe::objectArrayKlass()->prototype_header() == markWord::prototype(), \"should be the same\");\n@@ -731,1 +733,2 @@\n-    fake_oop->set_mark(markWord::prototype().set_narrow_klass(nk));\n+    markWord prototype_header = src_klass->prototype_header().set_narrow_klass(nk);\n+    fake_oop->set_mark(prototype_header);\n@@ -741,1 +744,1 @@\n-  if (!src_obj->fast_no_hash_check()) {\n+  if (!src_obj->fast_no_hash_check() && (!(EnableValhalla && src_obj->mark().is_inline_type()))) {\n@@ -744,1 +747,3 @@\n-      fake_oop->set_mark(markWord::prototype().set_narrow_klass(nk).copy_set_hash(src_hash));\n+      fake_oop->set_mark(fake_oop->mark().copy_set_hash(src_hash));\n+    } else if (EnableValhalla) {\n+      fake_oop->set_mark(src_klass->prototype_header().copy_set_hash(src_hash));\n","filename":"src\/hotspot\/share\/cds\/aotMappedHeapWriter.cpp","additions":11,"deletions":6,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -24,0 +24,1 @@\n+\n@@ -54,0 +55,1 @@\n+#include \"oops\/inlineKlass.inline.hpp\"\n@@ -88,0 +90,1 @@\n+#include \"utilities\/stringUtils.hpp\"\n@@ -157,0 +160,2 @@\n+#define CONSTANT_CLASS_DESCRIPTORS        70\n+\n@@ -195,1 +200,1 @@\n-      case JVM_CONSTANT_Class : {\n+      case JVM_CONSTANT_Class: {\n@@ -499,0 +504,3 @@\n+\n+        Symbol* const name = cp->symbol_at(class_index);\n+        const unsigned int name_len = name->utf8_length();\n@@ -708,1 +716,1 @@\n-            } else if (!Signature::is_void_method(signature)) { \/\/ must have void signature.\n+            } else if (!Signature::is_void_method(signature)) {  \/\/ must have void signature.\n@@ -728,2 +736,3 @@\n-            if (ref_kind == JVM_REF_newInvokeSpecial) {\n-              if (name != vmSymbols::object_initializer_name()) {\n+\n+            if (name != vmSymbols::object_initializer_name()) { \/\/ !<init>\n+              if (ref_kind == JVM_REF_newInvokeSpecial) {\n@@ -735,2 +744,10 @@\n-            } else {\n-              if (name == vmSymbols::object_initializer_name()) {\n+            } else { \/\/ <init>\n+              \/\/ The allowed invocation mode of <init> depends on its signature.\n+              \/\/ This test corresponds to verify_invoke_instructions in the verifier.\n+              const int signature_ref_index =\n+                cp->signature_ref_index_at(name_and_type_ref_index);\n+              const Symbol* const signature = cp->symbol_at(signature_ref_index);\n+              if (signature->is_void_method_signature()\n+                  && ref_kind == JVM_REF_newInvokeSpecial) {\n+                \/\/ OK, could be a constructor call\n+              } else {\n@@ -788,4 +805,13 @@\n-\/\/ Side-effects: populates the _local_interfaces field\n-void ClassFileParser::parse_interfaces(const ClassFileStream* const stream,\n-                                       const int itfs_len,\n-                                       ConstantPool* const cp,\n+static void check_identity_and_value_modifiers(ClassFileParser* current, const InstanceKlass* super_type, TRAPS) {\n+  assert(super_type != nullptr,\"Method doesn't support null super type\");\n+  if (super_type->access_flags().is_identity_class() && !current->access_flags().is_identity_class()\n+      && super_type->name() != vmSymbols::java_lang_Object()) {\n+      THROW_MSG(vmSymbols::java_lang_IncompatibleClassChangeError(),\n+                err_msg(\"Value type %s has an identity type as supertype\",\n+                current->class_name()->as_klass_external_name()));\n+  }\n+}\n+\n+void ClassFileParser::parse_interfaces(const ClassFileStream* stream,\n+                                       int itfs_len,\n+                                       ConstantPool* cp,\n@@ -793,0 +819,6 @@\n+                                       \/\/ FIXME: lots of these functions\n+                                       \/\/ declare their parameters as const,\n+                                       \/\/ which adds only noise to the code.\n+                                       \/\/ Remove the spurious const modifiers.\n+                                       \/\/ Many are of the form \"const int x\"\n+                                       \/\/ or \"T* const x\".\n@@ -800,0 +832,1 @@\n+\n@@ -802,3 +835,2 @@\n-    _local_interfaces = MetadataFactory::new_array<InstanceKlass*>(_loader_data, itfs_len, nullptr, CHECK);\n-\n-    int index;\n+    _local_interface_indexes = new GrowableArray<u2>(itfs_len);\n+    int index = 0;\n@@ -807,1 +839,0 @@\n-      Klass* interf;\n@@ -812,29 +843,1 @@\n-      if (cp->tag_at(interface_index).is_klass()) {\n-        interf = cp->resolved_klass_at(interface_index);\n-      } else {\n-        Symbol* const unresolved_klass  = cp->klass_name_at(interface_index);\n-\n-        \/\/ Don't need to check legal name because it's checked when parsing constant pool.\n-        \/\/ But need to make sure it's not an array type.\n-        guarantee_property(unresolved_klass->char_at(0) != JVM_SIGNATURE_ARRAY,\n-                           \"Bad interface name in class file %s\", CHECK);\n-\n-        \/\/ Call resolve on the interface class name with class circularity checking\n-        interf = SystemDictionary::resolve_super_or_fail(_class_name,\n-                                                         unresolved_klass,\n-                                                         Handle(THREAD, _loader_data->class_loader()),\n-                                                         false, CHECK);\n-      }\n-\n-      if (!interf->is_interface()) {\n-        THROW_MSG(vmSymbols::java_lang_IncompatibleClassChangeError(),\n-                  err_msg(\"class %s can not implement %s, because it is not an interface (%s)\",\n-                          _class_name->as_klass_external_name(),\n-                          interf->external_name(),\n-                          interf->class_in_module_of_loader()));\n-      }\n-\n-      if (InstanceKlass::cast(interf)->has_nonstatic_concrete_methods()) {\n-        *has_nonstatic_concrete_methods = true;\n-      }\n-      _local_interfaces->at_put(index, InstanceKlass::cast(interf));\n+      _local_interface_indexes->at_put_grow(index, interface_index);\n@@ -852,2 +855,1 @@\n-      const InstanceKlass* const k = _local_interfaces->at(index);\n-      Symbol* interface_name = k->name();\n+      Symbol* interface_name = cp->klass_name_at(_local_interface_indexes->at(index));\n@@ -940,0 +942,2 @@\n+    _jdk_internal_LooselyConsistentValue,\n+    _jdk_internal_NullRestricted,\n@@ -1366,1 +1370,1 @@\n-                                   bool is_interface,\n+                                   AccessFlags class_access_flags,\n@@ -1379,0 +1383,2 @@\n+  bool is_inline_type = !class_access_flags.is_identity_class() && !class_access_flags.is_abstract();\n+  bool is_value_class = !class_access_flags.is_identity_class() && !class_access_flags.is_interface();\n@@ -1386,1 +1392,6 @@\n-  const int total_fields = length + num_injected;\n+\n+  \/\/ two more slots are required for inline classes:\n+  \/\/ one for the static field with a reference to the pre-allocated default value\n+  \/\/ one for the field the JVM injects when detecting an empty inline class\n+  const int total_fields = length + num_injected + (is_inline_type ? 2 : 0)\n+                           + ((UseAltSubstitutabilityMethod && is_value_class) ? 1 : 0);\n@@ -1392,0 +1403,1 @@\n+  int instance_fields_count = 0;\n@@ -1397,0 +1409,7 @@\n+    jint recognized_modifiers = JVM_RECOGNIZED_FIELD_MODIFIERS;\n+    if (!supports_inline_types()) {\n+      recognized_modifiers &= ~JVM_ACC_STRICT;\n+    }\n+\n+    const jint flags = cfs->get_u2_fast() & recognized_modifiers;\n+    verify_legal_field_modifiers(flags, class_access_flags, CHECK);\n@@ -1398,2 +1417,0 @@\n-    const jint flags = cfs->get_u2_fast() & JVM_RECOGNIZED_FIELD_MODIFIERS;\n-    verify_legal_field_modifiers(flags, is_interface, CHECK);\n@@ -1416,0 +1433,1 @@\n+    if (!access_flags.is_static()) instance_fields_count++;\n@@ -1423,0 +1441,2 @@\n+    bool is_null_restricted = false;\n+\n@@ -1442,0 +1462,18 @@\n+        if (parsed_annotations.has_annotation(AnnotationCollector::_jdk_internal_NullRestricted)) {\n+          if (!Signature::has_envelope(sig)) {\n+            Exceptions::fthrow(\n+              THREAD_AND_LOCATION,\n+              vmSymbols::java_lang_ClassFormatError(),\n+              \"Illegal use of @jdk.internal.vm.annotation.NullRestricted annotation on field %s.%s with signature %s (primitive types can never be null)\",\n+              class_name()->as_C_string(), name->as_C_string(), sig->as_C_string());\n+          }\n+          const bool is_strict = (flags & JVM_ACC_STRICT) != 0;\n+          if (!is_strict) {\n+            Exceptions::fthrow(\n+              THREAD_AND_LOCATION,\n+              vmSymbols::java_lang_ClassFormatError(),\n+              \"Illegal use of @jdk.internal.vm.annotation.NullRestricted annotation on field %s.%s which doesn't have the @jdk.internal.vm.annotation.Strict annotation\",\n+              class_name()->as_C_string(), name->as_C_string());\n+          }\n+          is_null_restricted = true;\n+        }\n@@ -1464,0 +1502,4 @@\n+    if (is_null_restricted) {\n+      fieldFlags.update_null_free_inline_type(true);\n+    }\n+\n@@ -1480,0 +1522,3 @@\n+    if (access_flags.is_strict() && access_flags.is_static()) {\n+      _has_strict_static_fields = true;\n+    }\n@@ -1484,1 +1529,0 @@\n-  int index = length;\n@@ -1512,3 +1556,2 @@\n-      fi.set_index(index);\n-      _temp_field_info->append(fi);\n-      index++;\n+      int idx = _temp_field_info->append(fi);\n+      _temp_field_info->adr_at(idx)->set_index(idx);\n@@ -1518,1 +1561,33 @@\n-  assert(_temp_field_info->length() == index, \"Must be\");\n+  if (is_inline_type) {\n+    \/\/ Inject static \".null_reset\" field. This is an all-zero value with its null-channel set to zero.\n+    \/\/ IT should never be seen by user code, it is used when writing \"null\" to a nullable flat field\n+    \/\/ The all-zero value ensure that any embedded oop will be set to null, to avoid keeping dead objects\n+    \/\/ alive.\n+    FieldInfo::FieldFlags fflags2(0);\n+    fflags2.update_injected(true);\n+    AccessFlags aflags2(JVM_ACC_STATIC);\n+    FieldInfo fi2(aflags2,\n+                 (u2)vmSymbols::as_int(VM_SYMBOL_ENUM_NAME(null_reset_value_name)),\n+                 (u2)vmSymbols::as_int(VM_SYMBOL_ENUM_NAME(object_signature)),\n+                 0,\n+                 fflags2);\n+    int idx2 = _temp_field_info->append(fi2);\n+    _temp_field_info->adr_at(idx2)->set_index(idx2);\n+    _static_oop_count++;\n+  }\n+  if (!access_flags().is_identity_class() && !access_flags().is_interface()\n+      && _class_name != vmSymbols::java_lang_Object() && UseAltSubstitutabilityMethod) {\n+    \/\/ Acmp map required for abstract and concrete value classes\n+    FieldInfo::FieldFlags fflags2(0);\n+    fflags2.update_injected(true);\n+    fflags2.update_stable(true);\n+    AccessFlags aflags2(JVM_ACC_STATIC | JVM_ACC_FINAL);\n+    FieldInfo fi3(aflags2,\n+                 (u2)vmSymbols::as_int(VM_SYMBOL_ENUM_NAME(acmp_maps_name)),\n+                 (u2)vmSymbols::as_int(VM_SYMBOL_ENUM_NAME(int_array_signature)),\n+                 0,\n+                 fflags2);\n+    int idx2 = _temp_field_info->append(fi3);\n+    _temp_field_info->adr_at(idx2)->set_index(idx2);\n+    _static_oop_count++;\n+  }\n@@ -1898,0 +1973,8 @@\n+    case VM_SYMBOL_ENUM_NAME(jdk_internal_vm_annotation_LooselyConsistentValue_signature): {\n+      if (_location != _in_class)   break; \/\/ only allow for classes\n+      return _jdk_internal_LooselyConsistentValue;\n+    }\n+    case VM_SYMBOL_ENUM_NAME(jdk_internal_vm_annotation_NullRestricted_signature): {\n+      if (_location != _in_field)   break; \/\/ only allow for fields\n+      return _jdk_internal_NullRestricted;\n+    }\n@@ -2134,0 +2217,2 @@\n+                                      bool is_value_class,\n+                                      bool is_abstract_class,\n@@ -2175,1 +2260,1 @@\n-    verify_legal_method_modifiers(flags, is_interface, name, CHECK_NULL);\n+    verify_legal_method_modifiers(flags, access_flags() , name, CHECK_NULL);\n@@ -2183,0 +2268,9 @@\n+  if (EnableValhalla) {\n+    if (((flags & JVM_ACC_SYNCHRONIZED) == JVM_ACC_SYNCHRONIZED)\n+        && ((flags & JVM_ACC_STATIC) == 0 )\n+        && !_access_flags.is_identity_class()) {\n+      classfile_parse_error(\"Invalid synchronized method in non-identity class %s\", THREAD);\n+        return nullptr;\n+    }\n+  }\n+\n@@ -2717,0 +2811,2 @@\n+                                    bool is_value_class,\n+                                    bool is_abstract_type,\n@@ -2741,0 +2837,2 @@\n+                                    is_value_class,\n+                                    is_abstract_type,\n@@ -3006,0 +3104,1 @@\n+\n@@ -3014,0 +3113,1 @@\n+\n@@ -3019,0 +3119,8 @@\n+    if (!supports_inline_types()) {\n+      const bool is_module = (flags & JVM_ACC_MODULE) != 0;\n+      const bool is_interface = (flags & JVM_ACC_INTERFACE) != 0;\n+      if (!is_module && !is_interface) {\n+        flags |= JVM_ACC_IDENTITY;\n+      }\n+    }\n+\n@@ -3124,0 +3232,43 @@\n+u2 ClassFileParser::parse_classfile_loadable_descriptors_attribute(const ClassFileStream* const cfs,\n+                                                                   const u1* const loadable_descriptors_attribute_start,\n+                                                                   TRAPS) {\n+  const u1* const current_mark = cfs->current();\n+  u2 length = 0;\n+  if (loadable_descriptors_attribute_start != nullptr) {\n+    cfs->set_current(loadable_descriptors_attribute_start);\n+    cfs->guarantee_more(2, CHECK_0);  \/\/ length\n+    length = cfs->get_u2_fast();\n+  }\n+  const int size = length;\n+  Array<u2>* const loadable_descriptors = MetadataFactory::new_array<u2>(_loader_data, size, CHECK_0);\n+  _loadable_descriptors = loadable_descriptors;\n+  if (length > 0) {\n+    int index = 0;\n+    cfs->guarantee_more(2 * length, CHECK_0);\n+    for (int n = 0; n < length; n++) {\n+      const u2 descriptor_index = cfs->get_u2_fast();\n+      guarantee_property(\n+        valid_symbol_at(descriptor_index),\n+        \"LoadableDescriptors descriptor_index %u has bad constant type in class file %s\",\n+        descriptor_index, CHECK_0);\n+      Symbol* descriptor = _cp->symbol_at(descriptor_index);\n+      bool valid = legal_field_signature(descriptor, CHECK_0);\n+      if(!valid) {\n+        ResourceMark rm(THREAD);\n+        Exceptions::fthrow(THREAD_AND_LOCATION,\n+          vmSymbols::java_lang_ClassFormatError(),\n+          \"Descriptor from LoadableDescriptors attribute at index \\\"%d\\\" in class %s has illegal signature \\\"%s\\\"\",\n+          descriptor_index, _class_name->as_C_string(), descriptor->as_C_string());\n+        return 0;\n+      }\n+      loadable_descriptors->at_put(index++, descriptor_index);\n+    }\n+    assert(index == size, \"wrong size\");\n+  }\n+\n+  \/\/ Restore buffer's current position.\n+  cfs->set_current(current_mark);\n+\n+  return length;\n+}\n+\n@@ -3374,0 +3525,2 @@\n+  \/\/ Set _loadable_descriptors attribute to default sentinel\n+  _loadable_descriptors = Universe::the_empty_short_array();\n@@ -3380,0 +3533,1 @@\n+  bool parsed_loadable_descriptors_attribute = false;\n@@ -3401,0 +3555,2 @@\n+  const u1* loadable_descriptors_attribute_start = nullptr;\n+  u4  loadable_descriptors_attribute_length = 0;\n@@ -3616,0 +3772,9 @@\n+            if (EnableValhalla && tag == vmSymbols::tag_loadable_descriptors()) {\n+              if (parsed_loadable_descriptors_attribute) {\n+                classfile_parse_error(\"Multiple LoadableDescriptors attributes in class file %s\", CHECK);\n+                return;\n+              }\n+              parsed_loadable_descriptors_attribute = true;\n+              loadable_descriptors_attribute_start = cfs->current();\n+              loadable_descriptors_attribute_length = attribute_length;\n+            }\n@@ -3692,0 +3857,12 @@\n+  if (parsed_loadable_descriptors_attribute) {\n+    const u2 num_classes = parse_classfile_loadable_descriptors_attribute(\n+                            cfs,\n+                            loadable_descriptors_attribute_start,\n+                            CHECK);\n+    if (_need_verify) {\n+      guarantee_property(\n+        loadable_descriptors_attribute_length == sizeof(num_classes) + sizeof(u2) * num_classes,\n+        \"Wrong LoadableDescriptors attribute length in class file %s\", CHECK);\n+    }\n+  }\n+\n@@ -3758,0 +3935,1 @@\n+  this_klass->set_loadable_descriptors(_loadable_descriptors);\n@@ -3761,0 +3939,1 @@\n+  this_klass->set_inline_layout_info_array(_inline_layout_info_array);\n@@ -3799,2 +3978,1 @@\n-                       \"Invalid superclass index %u in class file %s\",\n-                       super_class_index,\n+                       \"Invalid superclass index 0 in class file %s\",\n@@ -3992,0 +4170,6 @@\n+bool ClassFileParser::supports_inline_types() const {\n+  \/\/ Inline types are only supported by class file version 70.65535 and later\n+  return _major_version > JAVA_26_VERSION ||\n+         (_major_version == JAVA_26_VERSION && _minor_version == JAVA_PREVIEW_MINOR_VERSION);\n+}\n+\n@@ -4035,3 +4219,4 @@\n-  } else if (max_transitive_size == local_size) {\n-    \/\/ only local interfaces added, share local interface array\n-    return local_ifs;\n+    \/\/ The three lines below are commented to work around bug JDK-8245487\n+\/\/  } else if (max_transitive_size == local_size) {\n+\/\/    \/\/ only local interfaces added, share local interface array\n+\/\/    return local_ifs;\n@@ -4058,0 +4243,1 @@\n+\n@@ -4088,0 +4274,10 @@\n+    \/\/ The JVMS says that super classes for value types must not have the ACC_IDENTITY\n+    \/\/ flag set. But, java.lang.Object must still be allowed to be a direct super class\n+    \/\/ for a value classes.  So, it is treated as a special case for now.\n+    if (!this_klass->access_flags().is_identity_class() &&\n+        super->name() != vmSymbols::java_lang_Object() &&\n+        super->is_identity_class()) {\n+      classfile_icce_error(\"value class %s cannot inherit from class %s\", super, THREAD);\n+      return;\n+    }\n+\n@@ -4280,1 +4476,1 @@\n-  const bool is_super      = (flags & JVM_ACC_SUPER)      != 0;\n+  const bool is_identity   = (flags & JVM_ACC_IDENTITY)   != 0;\n@@ -4284,0 +4480,2 @@\n+  const bool valid_value_class = is_identity || is_interface ||\n+                                 (supports_inline_types() && (!is_identity && (is_abstract || is_final)));\n@@ -4287,2 +4485,3 @@\n-      (is_interface && major_gte_1_5 && (is_super || is_enum)) ||\n-      (!is_interface && major_gte_1_5 && is_annotation)) {\n+      (is_interface && major_gte_1_5 && (is_identity || is_enum)) ||   \/\/  ACC_SUPER (now ACC_IDENTITY) was illegal for interfaces\n+      (!is_interface && major_gte_1_5 && is_annotation) ||\n+      (!valid_value_class)) {\n@@ -4290,0 +4489,4 @@\n+    const char* class_note = \"\";\n+    if (!valid_value_class) {\n+      class_note = \" (a value class must be final or else abstract)\";\n+    }\n@@ -4383,2 +4586,2 @@\n-void ClassFileParser::verify_legal_field_modifiers(jint flags,\n-                                                   bool is_interface,\n+void ClassFileParser:: verify_legal_field_modifiers(jint flags,\n+                                                   AccessFlags class_access_flags,\n@@ -4396,0 +4599,1 @@\n+  const bool is_strict    = (flags & JVM_ACC_STRICT)    != 0;\n@@ -4398,1 +4602,2 @@\n-  bool is_illegal = false;\n+  const bool is_interface = class_access_flags.is_interface();\n+  const bool is_identity_class = class_access_flags.is_identity_class();\n@@ -4400,9 +4605,30 @@\n-  if (is_interface) {\n-    if (!is_public || !is_static || !is_final || is_private ||\n-        is_protected || is_volatile || is_transient ||\n-        (major_gte_1_5 && is_enum)) {\n-      is_illegal = true;\n-    }\n-  } else { \/\/ not interface\n-    if (has_illegal_visibility(flags) || (is_final && is_volatile)) {\n-      is_illegal = true;\n+  bool is_illegal = false;\n+  const char* error_msg = \"\";\n+\n+  \/\/ There is some overlap in the checks that apply, for example interface fields\n+  \/\/ must be static, static fields can't be strict, and therefore interfaces can't\n+  \/\/ have strict fields. So we don't have to check every possible invalid combination\n+  \/\/ individually as long as all are covered. Once we have found an illegal combination\n+  \/\/ we can stop checking.\n+\n+  if (!is_illegal) {\n+    if (is_interface) {\n+      if (!is_public || !is_static || !is_final || is_private ||\n+          is_protected || is_volatile || is_transient ||\n+          (major_gte_1_5 && is_enum)) {\n+        is_illegal = true;\n+        error_msg = \"interface fields must be public, static and final, and may be synthetic\";\n+      }\n+    } else { \/\/ not interface\n+      if (has_illegal_visibility(flags)) {\n+        is_illegal = true;\n+        error_msg = \"invalid visibility flags for class field\";\n+      } else if (is_final && is_volatile) {\n+        is_illegal = true;\n+        error_msg = \"fields cannot be final and volatile\";\n+      } else if (supports_inline_types()) {\n+        if (!is_identity_class && !is_static && (!is_strict || !is_final)) {\n+          is_illegal = true;\n+          error_msg = \"value class fields must be either non-static final and strict, or static\";\n+        }\n+      }\n@@ -4418,2 +4644,2 @@\n-      \"Illegal field modifiers in class %s: 0x%X\",\n-      _class_name->as_C_string(), flags);\n+      \"Illegal field modifiers (%s) in class %s: 0x%X\",\n+      error_msg, _class_name->as_C_string(), flags);\n@@ -4425,1 +4651,1 @@\n-                                                    bool is_interface,\n+                                                    AccessFlags class_access_flags,\n@@ -4444,0 +4670,4 @@\n+  \/\/ LW401 CR required: removal of value factories support\n+  const bool is_interface    = class_access_flags.is_interface();\n+  const bool is_identity_class = class_access_flags.is_identity_class();\n+  const bool is_abstract_class = class_access_flags.is_abstract();\n@@ -4447,0 +4677,1 @@\n+  const char* class_note = \"\";\n@@ -4486,4 +4717,9 @@\n-        if (is_abstract) {\n-          if ((is_final || is_native || is_private || is_static ||\n-              (major_gte_1_5 && (is_synchronized || (!major_gte_17 && is_strict))))) {\n-            is_illegal = true;\n+        if (!is_identity_class && is_synchronized && !is_static) {\n+          is_illegal = true;\n+          class_note = \" (not an identity class)\";\n+        } else {\n+          if (is_abstract) {\n+            if ((is_final || is_native || is_private || is_static ||\n+                (major_gte_1_5 && (is_synchronized || (!major_gte_17 && is_strict))))) {\n+              is_illegal = true;\n+            }\n@@ -4502,2 +4738,3 @@\n-      \"Method %s in class %s has illegal modifiers: 0x%X\",\n-      name->as_C_string(), _class_name->as_C_string(), flags);\n+      \"Method %s in class %s%s has illegal modifiers: 0x%X\",\n+      name->as_C_string(), _class_name->as_C_string(),\n+      class_note, flags);\n@@ -4561,0 +4798,9 @@\n+bool ClassFileParser::is_class_in_loadable_descriptors_attribute(Symbol *klass) {\n+  if (_loadable_descriptors == nullptr) return false;\n+  for (int i = 0; i < _loadable_descriptors->length(); i++) {\n+        Symbol* class_name = _cp->symbol_at(_loadable_descriptors->at(i));\n+        if (class_name == klass) return true;\n+  }\n+  return false;\n+}\n+\n@@ -4662,1 +4908,2 @@\n-    case JVM_SIGNATURE_CLASS: {\n+    case JVM_SIGNATURE_CLASS:\n+    {\n@@ -4677,1 +4924,1 @@\n-        \/\/ Skip leading 'L' and ignore first appearance of ';'\n+        \/\/ Skip leading 'L' or 'Q' and ignore first appearance of ';'\n@@ -4733,0 +4980,4 @@\n+    } else if ((_major_version >= CONSTANT_CLASS_DESCRIPTORS || _class_name->starts_with(\"jdk\/internal\/reflect\/\"))\n+                   && bytes[length - 1] == ';' ) {\n+      \/\/ Support for L...; descriptors\n+      legal = verify_unqualified_name(bytes + 1, length - 2, LegalClass);\n@@ -4800,1 +5051,2 @@\n-      if (name == vmSymbols::object_initializer_name() || name == vmSymbols::class_initializer_name()) {\n+      if (name == vmSymbols::object_initializer_name() ||\n+          name == vmSymbols::class_initializer_name()) {\n@@ -4827,0 +5079,10 @@\n+bool ClassFileParser::legal_field_signature(const Symbol* signature, TRAPS) const {\n+  const char* const bytes = (const char*)signature->bytes();\n+  const unsigned int length = signature->utf8_length();\n+  const char* const p = skip_over_field_signature(bytes, false, length, CHECK_false);\n+\n+  if (p == nullptr || (p - bytes) != (int)length) {\n+    return false;\n+  }\n+  return true;\n+}\n@@ -4862,3 +5124,3 @@\n-      name->char_at(0) == JVM_SIGNATURE_SPECIAL &&\n-      sig_length > 0 &&\n-      signature->char_at(sig_length - 1) != JVM_SIGNATURE_VOID) {\n+    name->char_at(0) == JVM_SIGNATURE_SPECIAL &&\n+    sig_length > 0 &&\n+    signature->char_at(sig_length - 1) != JVM_SIGNATURE_VOID) {\n@@ -4915,2 +5177,2 @@\n-  assert(_field_info != nullptr, \"invariant\");\n-  return _field_info->_static_field_size;\n+  assert(_layout_info != nullptr, \"invariant\");\n+  return _layout_info->_static_field_size;\n@@ -4920,2 +5182,2 @@\n-  assert(_field_info != nullptr, \"invariant\");\n-  return _field_info->oop_map_blocks->_nonstatic_oop_map_count;\n+  assert(_layout_info != nullptr, \"invariant\");\n+  return _layout_info->oop_map_blocks->_nonstatic_oop_map_count;\n@@ -4925,2 +5187,2 @@\n-  assert(_field_info != nullptr, \"invariant\");\n-  return _field_info->_instance_size;\n+  assert(_layout_info != nullptr, \"invariant\");\n+  return _layout_info->_instance_size;\n@@ -5041,1 +5303,0 @@\n-\n@@ -5064,3 +5325,3 @@\n-  assert(_field_info != nullptr, \"invariant\");\n-  assert(ik->static_field_size() == _field_info->_static_field_size, \"sanity\");\n-  assert(ik->nonstatic_oop_map_count() == _field_info->oop_map_blocks->_nonstatic_oop_map_count,\n+  assert(_layout_info != nullptr, \"invariant\");\n+  assert(ik->static_field_size() == _layout_info->_static_field_size, \"sanity\");\n+  assert(ik->nonstatic_oop_map_count() == _layout_info->oop_map_blocks->_nonstatic_oop_map_count,\n@@ -5070,1 +5331,1 @@\n-  assert(ik->size_helper() == _field_info->_instance_size, \"sanity\");\n+  assert(ik->size_helper() == _layout_info->_instance_size, \"sanity\");\n@@ -5076,2 +5337,12 @@\n-  ik->set_nonstatic_field_size(_field_info->_nonstatic_field_size);\n-  ik->set_has_nonstatic_fields(_field_info->_has_nonstatic_fields);\n+  ik->set_nonstatic_field_size(_layout_info->_nonstatic_field_size);\n+  ik->set_has_nonstatic_fields(_layout_info->_has_nonstatic_fields);\n+  ik->set_has_strict_static_fields(_has_strict_static_fields);\n+\n+  if (_layout_info->_is_naturally_atomic) {\n+    ik->set_is_naturally_atomic();\n+  }\n+\n+  if (_layout_info->_must_be_atomic) {\n+    ik->set_must_be_atomic();\n+  }\n+\n@@ -5083,0 +5354,3 @@\n+  if (ik->is_inline_klass()) {\n+    InlineKlass::cast(ik)->init_fixed_block();\n+  }\n@@ -5097,0 +5371,1 @@\n+  assert(nullptr == _loadable_descriptors, \"invariant\");\n@@ -5100,0 +5375,1 @@\n+  assert(nullptr == _inline_layout_info_array, \"invariant\");\n@@ -5176,1 +5452,1 @@\n-  OopMapBlocksBuilder* oop_map_blocks = _field_info->oop_map_blocks;\n+  OopMapBlocksBuilder* oop_map_blocks = _layout_info->oop_map_blocks;\n@@ -5237,0 +5513,49 @@\n+  if (is_inline_type()) {\n+    InlineKlass* vk = InlineKlass::cast(ik);\n+    vk->set_payload_alignment(_layout_info->_payload_alignment);\n+    vk->set_payload_offset(_layout_info->_payload_offset);\n+    vk->set_payload_size_in_bytes(_layout_info->_payload_size_in_bytes);\n+    vk->set_non_atomic_size_in_bytes(_layout_info->_non_atomic_size_in_bytes);\n+    vk->set_non_atomic_alignment(_layout_info->_non_atomic_alignment);\n+    vk->set_atomic_size_in_bytes(_layout_info->_atomic_layout_size_in_bytes);\n+    vk->set_nullable_size_in_bytes(_layout_info->_nullable_layout_size_in_bytes);\n+    vk->set_null_marker_offset(_layout_info->_null_marker_offset);\n+    vk->set_null_reset_value_offset(_layout_info->_null_reset_value_offset);\n+    if (_layout_info->_is_empty_inline_klass) vk->set_is_empty_inline_type();\n+    vk->initialize_calling_convention(CHECK);\n+  }\n+\n+  if (EnableValhalla && !access_flags().is_identity_class() && !access_flags().is_interface()\n+      && _class_name != vmSymbols::java_lang_Object() && UseAltSubstitutabilityMethod) {\n+    \/\/ Both abstract and concrete value classes need a field map for acmp\n+    ik->set_acmp_maps_offset(_layout_info->_acmp_maps_offset);\n+    \/\/ Current format of acmp maps:\n+    \/\/ All maps are stored contiguously in a single int array because it might\n+    \/\/ be too early to instantiate an Object array (to be investigated)\n+    \/\/ Format is:\n+    \/\/ [number_of_nonoop_entries][offset0][size[0][offset1][size1]...[oop_offset0][oop_offset1]...\n+    \/\/                           ^               ^\n+    \/\/                           |               |\n+    \/\/                           --------------------- Pair of integer describing a segment of\n+    \/\/                                                 contiguous non-oop fields\n+    \/\/ First element is the number of segment of contiguous non-oop fields\n+    \/\/ Then, each segment of contiguous non-oop fields is described by two consecutive elements:\n+    \/\/ the offset then the size.\n+    \/\/ After the last segment of contiguous non-oop fields, oop fields are described, one element\n+    \/\/ per oop field, containing the offset of the field.\n+    int nonoop_acmp_map_size = _layout_info->_nonoop_acmp_map->length() * 2;\n+    int oop_acmp_map_size = _layout_info->_oop_acmp_map->length();\n+    typeArrayOop map = oopFactory::new_intArray(nonoop_acmp_map_size + oop_acmp_map_size + 1, CHECK);\n+    typeArrayHandle map_h(THREAD, map);\n+    map_h->int_at_put(0, _layout_info->_nonoop_acmp_map->length());\n+    for (int i = 0; i < _layout_info->_nonoop_acmp_map->length(); i++) {\n+      map_h->int_at_put(i * 2 + 1, _layout_info->_nonoop_acmp_map->at(i).first);\n+      map_h->int_at_put(i * 2 + 2, _layout_info->_nonoop_acmp_map->at(i).second);\n+    }\n+    int oop_map_start = nonoop_acmp_map_size + 1;\n+    for (int i = 0; i < _layout_info->_oop_acmp_map->length(); i++) {\n+      map_h->int_at_put(oop_map_start + i, _layout_info->_oop_acmp_map->at(i));\n+    }\n+    ik->java_mirror()->obj_field_put(ik->acmp_maps_offset(), map_h());\n+  }\n+\n@@ -5317,0 +5642,1 @@\n+  _loadable_descriptors(nullptr),\n@@ -5319,0 +5645,1 @@\n+  _local_interface_indexes(nullptr),\n@@ -5328,1 +5655,2 @@\n-  _field_info(nullptr),\n+  _layout_info(nullptr),\n+  _inline_layout_info_array(nullptr),\n@@ -5357,0 +5685,5 @@\n+  _has_strict_static_fields(false),\n+  _has_inline_type_fields(false),\n+  _is_naturally_atomic(false),\n+  _must_be_atomic(true),\n+  _has_loosely_consistent_annotation(false),\n@@ -5394,0 +5727,1 @@\n+  _loadable_descriptors = nullptr;\n@@ -5398,0 +5732,1 @@\n+  _inline_layout_info_array = nullptr;\n@@ -5417,0 +5752,4 @@\n+  if (_inline_layout_info_array != nullptr) {\n+    MetadataFactory::free_array<InlineLayoutInfo>(_loader_data, _inline_layout_info_array);\n+  }\n+\n@@ -5439,0 +5778,4 @@\n+  if (_loadable_descriptors != nullptr && _loadable_descriptors != Universe::the_empty_short_array()) {\n+    MetadataFactory::free_array<u2>(_loader_data, _loadable_descriptors);\n+  }\n+\n@@ -5538,0 +5881,9 @@\n+  \/\/ Fixing ACC_SUPER\/ACC_IDENTITY for old class files\n+  if (!supports_inline_types()) {\n+    const bool is_module = (flags & JVM_ACC_MODULE) != 0;\n+    const bool is_interface = (flags & JVM_ACC_INTERFACE) != 0;\n+    if (!is_module && !is_interface) {\n+      flags |= JVM_ACC_IDENTITY;\n+    }\n+  }\n+\n@@ -5641,2 +5993,0 @@\n-  assert(_local_interfaces != nullptr, \"invariant\");\n-\n@@ -5645,1 +5995,1 @@\n-               _access_flags.is_interface(),\n+               _access_flags,\n@@ -5655,1 +6005,3 @@\n-                _access_flags.is_interface(),\n+                is_interface(),\n+                !is_identity_class(),\n+                is_abstract_class(),\n@@ -5743,1 +6095,1 @@\n-    if (_access_flags.is_interface()) {\n+    if (is_interface()) {\n@@ -5765,0 +6117,14 @@\n+    if (_super_klass->is_interface()) {\n+      classfile_icce_error(\"class %s has interface %s as super class\", _super_klass, THREAD);\n+      return;\n+    }\n+\n+    if (_super_klass->is_final()) {\n+      classfile_icce_error(\"class %s cannot inherit from final class %s\", _super_klass, THREAD);\n+      return;\n+    }\n+\n+    if (EnableValhalla) {\n+      check_identity_and_value_modifiers(this, _super_klass, CHECK);\n+    }\n+\n@@ -5768,0 +6134,1 @@\n+  }\n@@ -5769,3 +6136,24 @@\n-    if (_super_klass->is_interface()) {\n-      classfile_icce_error(\"class %s has interface %s as super class\", _super_klass, THREAD);\n-      return;\n+  if (_parsed_annotations->has_annotation(AnnotationCollector::_jdk_internal_LooselyConsistentValue) && _access_flags.is_identity_class()) {\n+    THROW_MSG(vmSymbols::java_lang_ClassFormatError(),\n+          err_msg(\"class %s cannot have annotation jdk.internal.vm.annotation.LooselyConsistentValue, because it is not a value class\",\n+                  _class_name->as_klass_external_name()));\n+  }\n+\n+  \/\/ Determining is the class allows tearing or not (default is not)\n+  if (EnableValhalla && !_access_flags.is_identity_class()) {\n+    if (_parsed_annotations->has_annotation(ClassAnnotationCollector::_jdk_internal_LooselyConsistentValue)\n+        && (_super_klass == vmClasses::Object_klass() || !_super_klass->must_be_atomic())) {\n+      \/\/ Conditions above are not sufficient to determine atomicity requirements,\n+      \/\/ the presence of fields with atomic requirements could force the current class to have atomicy requirements too\n+      \/\/ Marking as not needing atomicity for now, can be updated when computing the fields layout\n+      \/\/ The InstanceKlass must be filled with the value from the FieldLayoutInfo returned by\n+      \/\/ the FieldLayoutBuilder, not with this _must_be_atomic field.\n+      _must_be_atomic = false;\n+    }\n+    \/\/ Apply VM options override\n+    if (*ForceNonTearable != '\\0') {\n+      \/\/ Allow a command line switch to force the same atomicity property:\n+      const char* class_name_str = _class_name->as_C_string();\n+      if (StringUtils::class_list_match(ForceNonTearable, class_name_str)) {\n+        _must_be_atomic = true;\n+      }\n@@ -5775,0 +6163,46 @@\n+  int itfs_len = _local_interface_indexes == nullptr ? 0 : _local_interface_indexes->length();\n+  _local_interfaces = MetadataFactory::new_array<InstanceKlass*>(_loader_data, itfs_len, nullptr, CHECK);\n+  if (_local_interface_indexes != nullptr) {\n+    for (int i = 0; i < _local_interface_indexes->length(); i++) {\n+      u2 interface_index = _local_interface_indexes->at(i);\n+      Klass* interf;\n+      if (cp->tag_at(interface_index).is_klass()) {\n+        interf = cp->resolved_klass_at(interface_index);\n+      } else {\n+        Symbol* const unresolved_klass  = cp->klass_name_at(interface_index);\n+\n+        \/\/ Don't need to check legal name because it's checked when parsing constant pool.\n+        \/\/ But need to make sure it's not an array type.\n+        guarantee_property(unresolved_klass->char_at(0) != JVM_SIGNATURE_ARRAY,\n+                            \"Bad interface name in class file %s\", CHECK);\n+\n+        \/\/ Call resolve on the interface class name with class circularity checking\n+        interf = SystemDictionary::resolve_super_or_fail(\n+                                                  _class_name,\n+                                                  unresolved_klass,\n+                                                  Handle(THREAD, _loader_data->class_loader()),\n+                                                  false,\n+                                                  CHECK);\n+      }\n+\n+      if (!interf->is_interface()) {\n+        THROW_MSG(vmSymbols::java_lang_IncompatibleClassChangeError(),\n+                  err_msg(\"class %s can not implement %s, because it is not an interface (%s)\",\n+                          _class_name->as_klass_external_name(),\n+                          interf->external_name(),\n+                          interf->class_in_module_of_loader()));\n+      }\n+\n+      if (EnableValhalla) {\n+        \/\/ Check modifiers and set carries_identity_modifier\/carries_value_modifier flags\n+        check_identity_and_value_modifiers(this, InstanceKlass::cast(interf), CHECK);\n+      }\n+\n+      if (InstanceKlass::cast(interf)->has_nonstatic_concrete_methods()) {\n+        _has_nonstatic_concrete_methods = true;\n+      }\n+      _local_interfaces->at_put(i, InstanceKlass::cast(interf));\n+    }\n+  }\n+  assert(_local_interfaces != nullptr, \"invariant\");\n+\n@@ -5802,1 +6236,1 @@\n-  _itable_size = _access_flags.is_interface() ? 0 :\n+  _itable_size = is_interface() ? 0 :\n@@ -5807,3 +6241,92 @@\n-  _field_info = new FieldLayoutInfo();\n-  FieldLayoutBuilder lb(class_name(), super_klass(), _cp, \/*_fields*\/ _temp_field_info,\n-                        _parsed_annotations->is_contended(), _field_info);\n+  if (EnableValhalla) {\n+    _inline_layout_info_array = MetadataFactory::new_array<InlineLayoutInfo>(_loader_data,\n+                                                   java_fields_count(),\n+                                                   CHECK);\n+    for (GrowableArrayIterator<FieldInfo> it = _temp_field_info->begin(); it != _temp_field_info->end(); ++it) {\n+      FieldInfo fieldinfo = *it;\n+      if (fieldinfo.access_flags().is_static()) continue;  \/\/ Only non-static fields are processed at load time\n+      Symbol* sig = fieldinfo.signature(cp);\n+      if (fieldinfo.field_flags().is_null_free_inline_type()) {\n+        \/\/ Pre-load classes of null-free fields that are candidate for flattening\n+        TempNewSymbol s = Signature::strip_envelope(sig);\n+        if (s == _class_name) {\n+          THROW_MSG(vmSymbols::java_lang_ClassCircularityError(),\n+                    err_msg(\"Class %s cannot have a null-free non-static field of its own type\", _class_name->as_C_string()));\n+        }\n+        log_info(class, preload)(\"Preloading of class %s during loading of class %s. \"\n+                                  \"Cause: a null-free non-static field is declared with this type\",\n+                                  s->as_C_string(), _class_name->as_C_string());\n+        InstanceKlass* klass = SystemDictionary::resolve_with_circularity_detection(_class_name, s,\n+                                                                                    Handle(THREAD,\n+                                                                                    _loader_data->class_loader()),\n+                                                                                    false, THREAD);\n+        if (HAS_PENDING_EXCEPTION) {\n+          log_info(class, preload)(\"Preloading of class %s during loading of class %s \"\n+                                      \"(cause: null-free non-static field) failed: %s\",\n+                                      s->as_C_string(), _class_name->as_C_string(),\n+                                      PENDING_EXCEPTION->klass()->name()->as_C_string());\n+          return; \/\/ Exception is still pending\n+        }\n+        assert(klass != nullptr, \"Sanity check\");\n+        InstanceKlass::check_can_be_annotated_with_NullRestricted(klass, _class_name, CHECK);\n+        InlineKlass* vk = InlineKlass::cast(klass);\n+        _inline_layout_info_array->adr_at(fieldinfo.index())->set_klass(vk);\n+        log_info(class, preload)(\"Preloading of class %s during loading of class %s \"\n+                                 \"(cause: null-free non-static field) succeeded\",\n+                                 s->as_C_string(), _class_name->as_C_string());\n+      } else if (Signature::has_envelope(sig) && PreloadClasses) {\n+        \/\/ Preloading classes for nullable fields that are listed in the LoadableDescriptors attribute\n+        \/\/ Those classes would be required later for the flattening of nullable inline type fields\n+        TempNewSymbol name = Signature::strip_envelope(sig);\n+        if (name != _class_name && is_class_in_loadable_descriptors_attribute(sig)) {\n+          log_info(class, preload)(\"Preloading of class %s during loading of class %s. \"\n+                                   \"Cause: field type in LoadableDescriptors attribute\",\n+                                   name->as_C_string(), _class_name->as_C_string());\n+          oop loader = loader_data()->class_loader();\n+          Klass* klass = SystemDictionary::resolve_super_or_fail(_class_name, name,\n+                                                                 Handle(THREAD, loader),\n+                                                                 false, THREAD);\n+          if (klass != nullptr) {\n+            if (klass->is_inline_klass()) {\n+              _inline_layout_info_array->adr_at(fieldinfo.index())->set_klass(InlineKlass::cast(klass));\n+              log_info(class, preload)(\"Preloading of class %s during loading of class %s \"\n+                                       \"(cause: field type in LoadableDescriptors attribute) succeeded\",\n+                                       name->as_C_string(), _class_name->as_C_string());\n+            } else {\n+              \/\/ Non value class are allowed by the current spec, but it could be an indication of an issue so let's log a warning\n+              log_info(class, preload)(\"Preloading of class %s during loading of class %s \"\n+                                          \"(cause: field type in LoadableDescriptors attribute) but loaded class is not a value class\",\n+                                          name->as_C_string(), _class_name->as_C_string());\n+            }\n+          } else {\n+            log_info(class, preload)(\"Preloading of class %s during loading of class %s \"\n+                                        \"(cause: field type in LoadableDescriptors attribute) failed : %s\",\n+                                        name->as_C_string(), _class_name->as_C_string(),\n+                                        PENDING_EXCEPTION->klass()->name()->as_C_string());\n+          }\n+          \/\/ Loads triggered by the LoadableDescriptors attribute are speculative, failures must not impact loading of current class\n+          if (HAS_PENDING_EXCEPTION) {\n+            CLEAR_PENDING_EXCEPTION;\n+          }\n+        } else {\n+          \/\/ Just poking the system dictionary to see if the class has already be loaded. Looking for migrated classes\n+          \/\/ used when --enable-preview when jdk isn't compiled with --enable-preview so doesn't include LoadableDescriptors.\n+          \/\/ This is temporary.\n+          oop loader = loader_data()->class_loader();\n+          InstanceKlass* klass = SystemDictionary::find_instance_klass(THREAD, name, Handle(THREAD, loader));\n+          if (klass != nullptr && klass->is_inline_klass()) {\n+            _inline_layout_info_array->adr_at(fieldinfo.index())->set_klass(InlineKlass::cast(klass));\n+            log_info(class, preload)(\"Preloading of class %s during loading of class %s \"\n+                                     \"(cause: field type not in LoadableDescriptors attribute) succeeded\",\n+                                     name->as_C_string(), _class_name->as_C_string());\n+          }\n+        }\n+      }\n+    }\n+  }\n+\n+  _layout_info = new FieldLayoutInfo();\n+  FieldLayoutBuilder lb(class_name(), loader_data(), super_klass(), _cp, \/*_fields*\/ _temp_field_info,\n+      _parsed_annotations->is_contended(), is_inline_type(),\n+      access_flags().is_abstract() && !access_flags().is_identity_class() && !access_flags().is_interface(),\n+      _must_be_atomic, _layout_info, _inline_layout_info_array);\n@@ -5811,0 +6334,1 @@\n+  _has_inline_type_fields = _layout_info->_has_inline_fields;\n@@ -5820,0 +6344,21 @@\n+\n+  \/\/ Strict static fields track initialization status from the beginning of time.\n+  \/\/ After this class runs <clinit>, they will be verified as being \"not unset\".\n+  \/\/ See Step 8 of InstanceKlass::initialize_impl.\n+  if (_has_strict_static_fields) {\n+    bool found_one = false;\n+    for (int i = 0; i < _temp_field_info->length(); i++) {\n+      FieldInfo& fi = *_temp_field_info->adr_at(i);\n+      if (fi.access_flags().is_strict() && fi.access_flags().is_static()) {\n+        found_one = true;\n+        if (fi.initializer_index() != 0) {\n+          \/\/ skip strict static fields with ConstantValue attributes\n+        } else {\n+          _fields_status->adr_at(fi.index())->update_strict_static_unset(true);\n+          _fields_status->adr_at(fi.index())->update_strict_static_unread(true);\n+        }\n+      }\n+    }\n+    assert(found_one == _has_strict_static_fields,\n+           \"correct prediction = %d\", (int)_has_strict_static_fields);\n+  }\n","filename":"src\/hotspot\/share\/classfile\/classFileParser.cpp","additions":662,"deletions":117,"binary":false,"changes":779,"status":"modified"},{"patch":"@@ -35,0 +35,1 @@\n+#include \"utilities\/pair.hpp\"\n@@ -74,0 +75,2 @@\n+  GrowableArray<Pair<int,int>>* _nonoop_acmp_map;\n+  GrowableArray<int>* _oop_acmp_map;\n@@ -77,1 +80,23 @@\n-  bool  _has_nonstatic_fields;\n+  int _payload_alignment;\n+  int _payload_offset;\n+  int _payload_size_in_bytes;\n+  int _non_atomic_size_in_bytes;\n+  int _non_atomic_alignment;\n+  int _atomic_layout_size_in_bytes;\n+  int _nullable_layout_size_in_bytes;\n+  int _null_marker_offset;\n+  int _null_reset_value_offset;\n+  int _acmp_maps_offset;\n+  bool _has_nonstatic_fields;\n+  bool _is_naturally_atomic;\n+  bool _must_be_atomic;\n+  bool _has_inline_fields;\n+  bool _is_empty_inline_klass;\n+  FieldLayoutInfo() : oop_map_blocks(nullptr), _nonoop_acmp_map(nullptr), _oop_acmp_map(nullptr),\n+                      _instance_size(-1), _nonstatic_field_size(-1), _static_field_size(-1),\n+                      _payload_alignment(-1), _payload_offset(-1), _payload_size_in_bytes(-1),\n+                      _non_atomic_size_in_bytes(-1), _non_atomic_alignment(-1),\n+                      _atomic_layout_size_in_bytes(-1), _nullable_layout_size_in_bytes(-1),\n+                      _null_marker_offset(-1), _null_reset_value_offset(-1), _acmp_maps_offset(-1),\n+                      _has_nonstatic_fields(false), _is_naturally_atomic(false), _must_be_atomic(false),\n+                      _has_inline_fields(false), _is_empty_inline_klass(false) { }\n@@ -133,0 +158,1 @@\n+  Array<u2>* _loadable_descriptors;\n@@ -135,0 +161,1 @@\n+  GrowableArray<u2>* _local_interface_indexes;\n@@ -145,1 +172,2 @@\n-  FieldLayoutInfo* _field_info;\n+  FieldLayoutInfo* _layout_info;\n+  Array<InlineLayoutInfo>* _inline_layout_info_array;\n@@ -159,0 +187,1 @@\n+\n@@ -196,0 +225,6 @@\n+  bool _has_strict_static_fields;\n+\n+  bool _has_inline_type_fields;\n+  bool _is_naturally_atomic;\n+  bool _must_be_atomic;\n+  bool _has_loosely_consistent_annotation;\n@@ -262,1 +297,1 @@\n-                    bool is_interface,\n+                    AccessFlags class_access_flags,\n@@ -271,0 +306,2 @@\n+                       bool is_value_class,\n+                       bool is_abstract_class,\n@@ -277,0 +314,2 @@\n+                     bool is_value_class,\n+                     bool is_abstract_class,\n@@ -331,0 +370,4 @@\n+  u2 parse_classfile_loadable_descriptors_attribute(const ClassFileStream* const cfs,\n+                                                    const u1* const loadable_descriptors_attribute_start,\n+                                                    TRAPS);\n+\n@@ -423,0 +466,2 @@\n+  bool legal_field_signature(const Symbol* signature, TRAPS) const;\n+\n@@ -437,1 +482,1 @@\n-  void verify_legal_field_modifiers(jint flags, bool is_interface, TRAPS) const;\n+  void verify_legal_field_modifiers(jint flags, AccessFlags class_access_flags, TRAPS) const;\n@@ -439,1 +484,1 @@\n-                                     bool is_interface,\n+                                     AccessFlags class_access_flags,\n@@ -491,0 +536,3 @@\n+  \/\/ Check if the class file supports inline types\n+  bool supports_inline_types() const;\n+\n@@ -520,0 +568,8 @@\n+  \/\/ Being an inline type means being a concrete value class\n+  bool is_inline_type() const { return !_access_flags.is_identity_class() && !_access_flags.is_interface() && !_access_flags.is_abstract(); }\n+  bool is_abstract_class() const { return _access_flags.is_abstract(); }\n+  bool is_identity_class() const { return _access_flags.is_identity_class(); }\n+  bool has_inline_fields() const { return _has_inline_type_fields; }\n+\n+  u2 java_fields_count() const { return _java_fields_count; }\n+  bool is_abstract() const { return _access_flags.is_abstract(); }\n@@ -533,0 +589,2 @@\n+  bool is_class_in_loadable_descriptors_attribute(Symbol *klass);\n+\n","filename":"src\/hotspot\/share\/classfile\/classFileParser.hpp","additions":63,"deletions":5,"binary":false,"changes":68,"status":"modified"},{"patch":"@@ -55,0 +55,2 @@\n+#include \"oops\/flatArrayKlass.hpp\"\n+#include \"oops\/inlineKlass.inline.hpp\"\n@@ -56,1 +58,1 @@\n-#include \"oops\/instanceMirrorKlass.hpp\"\n+#include \"oops\/instanceMirrorKlass.inline.hpp\"\n@@ -871,0 +873,1 @@\n+int java_lang_Class::_is_identity_offset;\n@@ -1098,0 +1101,1 @@\n+  set_is_identity(mirror(), k->is_array_klass() || k->is_identity_class());\n@@ -1106,1 +1110,9 @@\n-    if (k->is_typeArray_klass()) {\n+    if (k->is_flatArray_klass()) {\n+      Klass* element_klass = (Klass*) FlatArrayKlass::cast(k)->element_klass();\n+      assert(element_klass->is_inline_klass(), \"Must be inline type component\");\n+      if (is_scratch) {\n+        comp_mirror = Handle(THREAD, HeapShared::scratch_java_mirror(element_klass));\n+      } else {\n+        comp_mirror = Handle(THREAD, element_klass->java_mirror());\n+      }\n+    } else if (k->is_typeArray_klass()) {\n@@ -1115,0 +1127,1 @@\n+      assert(!k->is_refArray_klass() || !k->is_flatArray_klass(), \"Must not have mirror\");\n@@ -1117,0 +1130,1 @@\n+      oop comp_oop = element_klass->java_mirror();\n@@ -1120,1 +1134,1 @@\n-        comp_mirror = Handle(THREAD, element_klass->java_mirror());\n+        comp_mirror = Handle(THREAD, comp_oop);\n@@ -1149,1 +1163,0 @@\n-\n@@ -1153,0 +1166,9 @@\n+\n+    if (k->is_refined_objArray_klass()) {\n+      Klass* super_klass = k->super();\n+      assert(super_klass != nullptr, \"Must be\");\n+      Handle mirror(THREAD, super_klass->java_mirror());\n+      k->set_java_mirror(mirror);\n+      return;\n+    }\n+\n@@ -1175,0 +1197,1 @@\n+\n@@ -1198,3 +1221,3 @@\n-  if (k->class_loader() != nullptr &&\n-      k->class_loader() != SystemDictionary::java_platform_loader() &&\n-      k->class_loader() != SystemDictionary::java_system_loader()) {\n+  if ((k->class_loader() != nullptr &&\n+       k->class_loader() != SystemDictionary::java_platform_loader() &&\n+       k->class_loader() != SystemDictionary::java_system_loader())) {\n@@ -1239,1 +1262,0 @@\n-  assert(as_Klass(m) == k, \"must be\");\n@@ -1243,0 +1265,1 @@\n+    assert(as_Klass(m) == k, \"must be\");\n@@ -1249,0 +1272,4 @@\n+  } else {\n+    ObjArrayKlass* objarray_k = (ObjArrayKlass*)as_Klass(m);\n+    \/\/ Mirror should be restored for an ObjArrayKlass or one of its refined array klasses\n+    assert(objarray_k == k || objarray_k->next_refined_array_klass() == k, \"must be\");\n@@ -1381,0 +1408,4 @@\n+void java_lang_Class::set_is_identity(oop java_class, bool value) {\n+  assert(_is_identity_offset != 0, \"must be set\");\n+  java_class->bool_field_put(_is_identity_offset, value);\n+}\n@@ -1422,1 +1453,3 @@\n-  if (is_instance)  st->print(\"L\");\n+  if (is_instance)  {\n+    st->print(\"L\");\n+  }\n@@ -1480,0 +1513,1 @@\n+  assert(!klass->is_refined_objArray_klass(), \"should not be ref or flat array klass\");\n@@ -1539,1 +1573,2 @@\n-  macro(_is_primitive_offset,        k, \"primitive\",           bool_signature,         false);\n+  macro(_is_primitive_offset,        k, \"primitive\",           bool_signature,         false); \\\n+  macro(_is_identity_offset,         k, \"identity\",            bool_signature,         false);\n@@ -2859,1 +2894,1 @@\n-      if (method->name() == vmSymbols::object_initializer_name() &&\n+      if (method->is_object_constructor() &&\n@@ -3218,2 +3253,2 @@\n-  if (m->is_object_initializer()) {\n-    flags |= java_lang_invoke_MemberName::MN_IS_CONSTRUCTOR;\n+  if (m->is_object_constructor()) {\n+    flags |= java_lang_invoke_MemberName::MN_IS_OBJECT_CONSTRUCTOR;\n@@ -3610,1 +3645,1 @@\n-int java_lang_reflect_Field::_trusted_final_offset;\n+int java_lang_reflect_Field::_flags_offset;\n@@ -3620,1 +3655,1 @@\n-  macro(_trusted_final_offset,    k, vmSymbols::trusted_final_name(),    bool_signature,       false); \\\n+  macro(_flags_offset,     k, vmSymbols::flags_name(),     int_signature,    false); \\\n@@ -3685,2 +3720,2 @@\n-void java_lang_reflect_Field::set_trusted_final(oop field) {\n-  field->bool_field_put(_trusted_final_offset, true);\n+void java_lang_reflect_Field::set_flags(oop field, int value) {\n+  field->int_field_put(_flags_offset, value);\n@@ -3986,2 +4021,1 @@\n-int java_lang_boxing_object::_value_offset;\n-int java_lang_boxing_object::_long_value_offset;\n+int* java_lang_boxing_object::_offsets;\n@@ -3989,3 +4023,9 @@\n-#define BOXING_FIELDS_DO(macro) \\\n-  macro(_value_offset,      integerKlass, \"value\", int_signature, false); \\\n-  macro(_long_value_offset, longKlass, \"value\", long_signature, false);\n+#define BOXING_FIELDS_DO(macro)                                                                                                    \\\n+  macro(java_lang_boxing_object::_offsets[T_BOOLEAN - T_BOOLEAN], vmClasses::Boolean_klass(),   \"value\", bool_signature,   false); \\\n+  macro(java_lang_boxing_object::_offsets[T_CHAR - T_BOOLEAN],    vmClasses::Character_klass(), \"value\", char_signature,   false); \\\n+  macro(java_lang_boxing_object::_offsets[T_FLOAT - T_BOOLEAN],   vmClasses::Float_klass(),     \"value\", float_signature,  false); \\\n+  macro(java_lang_boxing_object::_offsets[T_DOUBLE - T_BOOLEAN],  vmClasses::Double_klass(),    \"value\", double_signature, false); \\\n+  macro(java_lang_boxing_object::_offsets[T_BYTE - T_BOOLEAN],    vmClasses::Byte_klass(),      \"value\", byte_signature,   false); \\\n+  macro(java_lang_boxing_object::_offsets[T_SHORT - T_BOOLEAN],   vmClasses::Short_klass(),     \"value\", short_signature,  false); \\\n+  macro(java_lang_boxing_object::_offsets[T_INT - T_BOOLEAN],     vmClasses::Integer_klass(),   \"value\", int_signature,    false); \\\n+  macro(java_lang_boxing_object::_offsets[T_LONG - T_BOOLEAN],    vmClasses::Long_klass(),      \"value\", long_signature,   false);\n@@ -3994,2 +4034,2 @@\n-  InstanceKlass* integerKlass = vmClasses::Integer_klass();\n-  InstanceKlass* longKlass = vmClasses::Long_klass();\n+  assert(T_LONG - T_BOOLEAN == 7, \"Sanity check\");\n+  java_lang_boxing_object::_offsets = NEW_C_HEAP_ARRAY(int, 8, mtInternal);\n@@ -4001,0 +4041,4 @@\n+  if (f->reading()) {\n+    assert(T_LONG - T_BOOLEAN == 7, \"Sanity check\");\n+    java_lang_boxing_object::_offsets = NEW_C_HEAP_ARRAY(int, 8, mtInternal);\n+  }\n@@ -4021,1 +4065,1 @@\n-      box->bool_field_put(_value_offset, value->z);\n+      box->bool_field_put(value_offset(type), value->z);\n@@ -4024,1 +4068,1 @@\n-      box->char_field_put(_value_offset, value->c);\n+      box->char_field_put(value_offset(type), value->c);\n@@ -4027,1 +4071,1 @@\n-      box->float_field_put(_value_offset, value->f);\n+      box->float_field_put(value_offset(type), value->f);\n@@ -4030,1 +4074,1 @@\n-      box->double_field_put(_long_value_offset, value->d);\n+      box->double_field_put(value_offset(type), value->d);\n@@ -4033,1 +4077,1 @@\n-      box->byte_field_put(_value_offset, value->b);\n+      box->byte_field_put(value_offset(type), value->b);\n@@ -4036,1 +4080,1 @@\n-      box->short_field_put(_value_offset, value->s);\n+      box->short_field_put(value_offset(type), value->s);\n@@ -4039,1 +4083,1 @@\n-      box->int_field_put(_value_offset, value->i);\n+      box->int_field_put(value_offset(type), value->i);\n@@ -4042,1 +4086,1 @@\n-      box->long_field_put(_long_value_offset, value->j);\n+      box->long_field_put(value_offset(type), value->j);\n@@ -4064,1 +4108,1 @@\n-    value->z = box->bool_field(_value_offset);\n+    value->z = box->bool_field(value_offset(type));\n@@ -4067,1 +4111,1 @@\n-    value->c = box->char_field(_value_offset);\n+    value->c = box->char_field(value_offset(type));\n@@ -4070,1 +4114,1 @@\n-    value->f = box->float_field(_value_offset);\n+    value->f = box->float_field(value_offset(type));\n@@ -4073,1 +4117,1 @@\n-    value->d = box->double_field(_long_value_offset);\n+    value->d = box->double_field(value_offset(type));\n@@ -4076,1 +4120,1 @@\n-    value->b = box->byte_field(_value_offset);\n+    value->b = box->byte_field(value_offset(type));\n@@ -4079,1 +4123,1 @@\n-    value->s = box->short_field(_value_offset);\n+    value->s = box->short_field(value_offset(type));\n@@ -4082,1 +4126,1 @@\n-    value->i = box->int_field(_value_offset);\n+    value->i = box->int_field(value_offset(type));\n@@ -4085,1 +4129,1 @@\n-    value->j = box->long_field(_long_value_offset);\n+    value->j = box->long_field(value_offset(type));\n@@ -4098,1 +4142,1 @@\n-    box->bool_field_put(_value_offset, value->z);\n+    box->bool_field_put(value_offset(type), value->z);\n@@ -4101,1 +4145,1 @@\n-    box->char_field_put(_value_offset, value->c);\n+    box->char_field_put(value_offset(type), value->c);\n@@ -4104,1 +4148,1 @@\n-    box->float_field_put(_value_offset, value->f);\n+    box->float_field_put(value_offset(type), value->f);\n@@ -4107,1 +4151,1 @@\n-    box->double_field_put(_long_value_offset, value->d);\n+    box->double_field_put(value_offset(type), value->d);\n@@ -4110,1 +4154,1 @@\n-    box->byte_field_put(_value_offset, value->b);\n+    box->byte_field_put(value_offset(type), value->b);\n@@ -4113,1 +4157,1 @@\n-    box->short_field_put(_value_offset, value->s);\n+    box->short_field_put(value_offset(type), value->s);\n@@ -4116,1 +4160,1 @@\n-    box->int_field_put(_value_offset, value->i);\n+    box->int_field_put(value_offset(type), value->i);\n@@ -4119,1 +4163,1 @@\n-    box->long_field_put(_long_value_offset, value->j);\n+    box->long_field_put(value_offset(type), value->j);\n@@ -4515,1 +4559,0 @@\n-\n@@ -4525,1 +4568,1 @@\n-  return (flags(mname) & (MN_IS_METHOD | MN_IS_CONSTRUCTOR)) > 0;\n+  return (flags(mname) & (MN_IS_METHOD | MN_IS_OBJECT_CONSTRUCTOR)) > 0;\n@@ -5518,16 +5561,11 @@\n-#define CHECK_OFFSET(klass_name, cpp_klass_name, field_name, field_sig) \\\n-  valid &= check_offset(klass_name, cpp_klass_name :: _##field_name ## _offset, #field_name, field_sig)\n-\n-#define CHECK_LONG_OFFSET(klass_name, cpp_klass_name, field_name, field_sig) \\\n-  valid &= check_offset(klass_name, cpp_klass_name :: _##long_ ## field_name ## _offset, #field_name, field_sig)\n-\n-  \/\/ Boxed primitive objects (java_lang_boxing_object)\n-\n-  CHECK_OFFSET(\"java\/lang\/Boolean\",   java_lang_boxing_object, value, \"Z\");\n-  CHECK_OFFSET(\"java\/lang\/Character\", java_lang_boxing_object, value, \"C\");\n-  CHECK_OFFSET(\"java\/lang\/Float\",     java_lang_boxing_object, value, \"F\");\n-  CHECK_LONG_OFFSET(\"java\/lang\/Double\", java_lang_boxing_object, value, \"D\");\n-  CHECK_OFFSET(\"java\/lang\/Byte\",      java_lang_boxing_object, value, \"B\");\n-  CHECK_OFFSET(\"java\/lang\/Short\",     java_lang_boxing_object, value, \"S\");\n-  CHECK_OFFSET(\"java\/lang\/Integer\",   java_lang_boxing_object, value, \"I\");\n-  CHECK_LONG_OFFSET(\"java\/lang\/Long\", java_lang_boxing_object, value, \"J\");\n+#define CHECK_OFFSET(klass_name, type, field_sig) \\\n+  valid &= check_offset(klass_name, java_lang_boxing_object::value_offset(type), \"value\", field_sig)\n+\n+  CHECK_OFFSET(\"java\/lang\/Boolean\",   T_BOOLEAN, \"Z\");\n+  CHECK_OFFSET(\"java\/lang\/Character\", T_CHAR,    \"C\");\n+  CHECK_OFFSET(\"java\/lang\/Float\",     T_FLOAT,   \"F\");\n+  CHECK_OFFSET(\"java\/lang\/Double\",    T_DOUBLE,  \"D\");\n+  CHECK_OFFSET(\"java\/lang\/Byte\",      T_BYTE,    \"B\");\n+  CHECK_OFFSET(\"java\/lang\/Short\",     T_SHORT,   \"S\");\n+  CHECK_OFFSET(\"java\/lang\/Integer\",   T_INT,     \"I\");\n+  CHECK_OFFSET(\"java\/lang\/Long\",      T_LONG,    \"J\");\n","filename":"src\/hotspot\/share\/classfile\/javaClasses.cpp","additions":104,"deletions":66,"binary":false,"changes":170,"status":"modified"},{"patch":"@@ -216,0 +216,1 @@\n+  static inline bool is_instance_without_asserts(oop obj);\n@@ -255,0 +256,1 @@\n+\n@@ -262,0 +264,1 @@\n+  static int _is_identity_offset;\n@@ -276,0 +279,1 @@\n+  static void set_is_identity(oop java_class, bool value);\n@@ -323,0 +327,1 @@\n+\n@@ -837,1 +842,1 @@\n-  static int _trusted_final_offset;\n+  static int _flags_offset;\n@@ -865,1 +870,1 @@\n-  static void set_trusted_final(oop field);\n+  static void set_flags(oop field, int value);\n@@ -987,2 +992,1 @@\n-  static int _value_offset;\n-  static int _long_value_offset;\n+  static int* _offsets;\n@@ -1005,1 +1009,3 @@\n-    return is_double_word_type(type) ? _long_value_offset : _value_offset;\n+    assert(type >= T_BOOLEAN && type <= T_LONG, \"BasicType out of range\");\n+    assert(_offsets != nullptr, \"Uninitialized offsets\");\n+    return _offsets[type - T_BOOLEAN];\n@@ -1367,1 +1373,1 @@\n-    MN_IS_CONSTRUCTOR        = 0x00020000, \/\/ constructor\n+    MN_IS_OBJECT_CONSTRUCTOR = 0x00020000, \/\/ constructor\n@@ -1373,0 +1379,1 @@\n+    MN_NULL_RESTRICTED_FIELD = 0x00800000, \/\/ null-restricted field\n@@ -1374,1 +1381,3 @@\n-    MN_REFERENCE_KIND_MASK   = 0x0F000000 >> MN_REFERENCE_KIND_SHIFT,\n+    MN_REFERENCE_KIND_MASK   = 0x0F000000 >> MN_REFERENCE_KIND_SHIFT, \/\/ 4 bits\n+    MN_LAYOUT_SHIFT          = 28, \/\/ field layout\n+    MN_LAYOUT_MASK           = 0x70000000 >> MN_LAYOUT_SHIFT, \/\/ 3 bits\n@@ -1878,1 +1887,0 @@\n-\n","filename":"src\/hotspot\/share\/classfile\/javaClasses.hpp","additions":16,"deletions":8,"binary":false,"changes":24,"status":"modified"},{"patch":"@@ -57,0 +57,2 @@\n+#include \"oops\/fieldStreams.inline.hpp\"\n+#include \"oops\/inlineKlass.inline.hpp\"\n@@ -74,0 +76,1 @@\n+#include \"runtime\/os.hpp\"\n@@ -188,0 +191,15 @@\n+\/\/ These migrated value classes are loaded by the bootstrap class loader but are added to the initiating\n+\/\/ loaders automatically so that fields of these types can be found and potentially flattened during\n+\/\/ field layout.\n+static void add_migrated_value_classes(ClassLoaderData* cld) {\n+  JavaThread* current = JavaThread::current();\n+  auto add_klass = [&] (Symbol* classname) {\n+    InstanceKlass* ik = SystemDictionary::find_instance_klass(current, classname, Handle(current, nullptr));\n+    assert(ik != nullptr, \"Must exist\");\n+    SystemDictionary::add_to_initiating_loader(current, ik, cld);\n+  };\n+\n+  MonitorLocker mu1(SystemDictionary_lock);\n+  vmSymbols::migrated_class_names_do(add_klass);\n+}\n+\n@@ -193,2 +211,10 @@\n-    return (class_loader() == nullptr) ? ClassLoaderData::the_null_class_loader_data() :\n-                                      ClassLoaderDataGraph::find_or_create(class_loader);\n+    if (class_loader() == nullptr) {\n+      return ClassLoaderData::the_null_class_loader_data();\n+    } else {\n+      bool created = false;\n+      ClassLoaderData* cld = ClassLoaderDataGraph::find_or_create(class_loader, created);\n+      if (created && Arguments::enable_preview()) {\n+        add_migrated_value_classes(cld);\n+      }\n+      return cld;\n+    }\n@@ -402,1 +428,2 @@\n-\/\/ during class definition to allow class circularity checking\n+\/\/ during class definition, or may be called for inline field layout processing\n+\/\/ to detect class circularity errors.\n@@ -415,0 +442,2 @@\n+\/\/ inline field layout callers:\n+\/\/    The field's class must be loaded to determine layout.\n@@ -418,1 +447,1 @@\n-\/\/ placeholder for the same thread, class, classloader is found.\n+\/\/ placeholder for the same thread, class, and classloader is found.\n@@ -453,1 +482,1 @@\n-    \/\/ Must check ClassCircularity before resolving next_name (superclass or interface).\n+    \/\/ Must check ClassCircularity before resolving next_name (superclass, interface, field types or speculatively preloaded argument types).\n@@ -477,1 +506,1 @@\n-  \/\/ Resolve the superclass or superinterface, check results on return\n+  \/\/ Resolve the superclass, superinterface, field type or speculatively preloaded argument types and check results on return.\n@@ -915,2 +944,1 @@\n-  assert(!ModuleEntryTable::javabase_moduleEntry()->is_patched(),\n-         \"Cannot use sharing if java.base is patched\");\n+  assert(!CDSConfig::module_patching_disables_cds(), \"Cannot use CDS\");\n@@ -992,1 +1020,1 @@\n-        assert(!mod_entry->is_patched(), \"cannot load archived classes for patched module\");\n+        assert(!CDSConfig::module_patching_disables_cds(), \"Cannot use CDS\");\n@@ -1070,0 +1098,69 @@\n+\/\/ Pre-load class referred to in non-static null-free instance field. These fields trigger MANDATORY loading.\n+\/\/ Some pre-loading does not fail fatally\n+bool SystemDictionary::preload_from_null_free_field(InstanceKlass* ik, Handle class_loader, Symbol* sig, int field_index, TRAPS) {\n+  TempNewSymbol name = Signature::strip_envelope(sig);\n+  log_info(class, preload)(\"Preloading of class %s during loading of shared class %s. \"\n+                           \"Cause: a null-free non-static field is declared with this type\",\n+                           name->as_C_string(), ik->name()->as_C_string());\n+  InstanceKlass* real_k = SystemDictionary::resolve_with_circularity_detection(ik->name(), name,\n+                                                                               class_loader, false, CHECK_false);\n+  if (HAS_PENDING_EXCEPTION) {\n+    log_info(class, preload)(\"Preloading of class %s during loading of class %s \"\n+                                \"(cause: null-free non-static field) failed: %s\",\n+                                name->as_C_string(), ik->name()->as_C_string(),\n+                                PENDING_EXCEPTION->klass()->name()->as_C_string());\n+    return false; \/\/ Exception is still pending\n+  }\n+\n+  InstanceKlass* k = ik->get_inline_type_field_klass_or_null(field_index);\n+  if (real_k != k) {\n+    \/\/ oops, the app has substituted a different version of k! Does not fail fatally\n+    log_info(class, preload)(\"Preloading of class %s during loading of shared class %s \"\n+                                \"(cause: null-free non-static field) failed : \"\n+                                \"app substituted a different version of %s\",\n+                                name->as_C_string(), ik->name()->as_C_string(),\n+                                name->as_C_string());\n+    return false;\n+  }\n+  log_info(class, preload)(\"Preloading of class %s during loading of shared class %s \"\n+                           \"(cause: null-free non-static field) succeeded\",\n+                           name->as_C_string(), ik->name()->as_C_string());\n+\n+  assert(real_k != nullptr, \"Sanity check\");\n+  InstanceKlass::check_can_be_annotated_with_NullRestricted(real_k, ik->name(), CHECK_false);\n+\n+  return true;\n+}\n+\n+\/\/ Tries to pre-load classes referred to in non-static nullable instance fields if they are found in the\n+\/\/ loadable descriptors attribute. If loading fails, we can fail silently.\n+void SystemDictionary::try_preload_from_loadable_descriptors(InstanceKlass* ik, Handle class_loader, Symbol* sig, int field_index, TRAPS) {\n+  TempNewSymbol name = Signature::strip_envelope(sig);\n+  if (name != ik->name() && ik->is_class_in_loadable_descriptors_attribute(sig)) {\n+    log_info(class, preload)(\"Preloading of class %s during loading of shared class %s. \"\n+                             \"Cause: field type in LoadableDescriptors attribute\",\n+                             name->as_C_string(), ik->name()->as_C_string());\n+    InstanceKlass* real_k = SystemDictionary::resolve_with_circularity_detection(ik->name(), name,\n+                                                                                 class_loader, false, THREAD);\n+    if (HAS_PENDING_EXCEPTION) {\n+      CLEAR_PENDING_EXCEPTION;\n+    }\n+\n+    InstanceKlass* k = ik->get_inline_type_field_klass_or_null(field_index);\n+    if (real_k != k) {\n+      \/\/ oops, the app has substituted a different version of k!\n+      log_info(class, preload)(\"Preloading of class %s during loading of shared class %s \"\n+                                  \"(cause: field type in LoadableDescriptors attribute) failed : \"\n+                                  \"app substituted a different version of %s\",\n+                                  name->as_C_string(), ik->name()->as_C_string(),\n+                                  k->name()->as_C_string());\n+      return;\n+    } else if (real_k != nullptr) {\n+      log_info(class, preload)(\"Preloading of class %s during loading of shared class %s \"\n+                               \"(cause: field type in LoadableDescriptors attribute) succeeded\",\n+                                name->as_C_string(), ik->name()->as_C_string());\n+    }\n+  }\n+}\n+\n+\n@@ -1093,0 +1190,21 @@\n+  if (ik->has_inline_type_fields()) {\n+    for (AllFieldStream fs(ik); !fs.done(); fs.next()) {\n+      if (fs.access_flags().is_static()) continue;\n+\n+      Symbol* sig = fs.signature();\n+      int field_index = fs.index();\n+\n+      if (fs.is_null_free_inline_type()) {\n+        \/\/ A false return means that the class didn't load for other reasons than an exception.\n+        bool check = preload_from_null_free_field(ik, class_loader, sig, field_index, CHECK_NULL);\n+        if (!check) {\n+          ik->set_shared_loading_failed();\n+          return nullptr;\n+        }\n+      } else if (Signature::has_envelope(sig)) {\n+          \/\/ Pending exceptions are cleared so we can fail silently\n+          try_preload_from_loadable_descriptors(ik, class_loader, sig, field_index, CHECK_NULL);\n+      }\n+    }\n+  }\n+\n@@ -1128,0 +1246,1 @@\n+\n@@ -1709,1 +1828,0 @@\n-#if INCLUDE_CDS\n@@ -1712,1 +1830,3 @@\n-\/\/ This API should be used only by AOTLinkedClassBulkLoader\n+\/\/ This API is used by AOTLinkedClassBulkLoader and to register boxing\n+\/\/ classes from java.lang in all class loaders to enable more value\n+\/\/ classes optimizations\n@@ -1716,1 +1836,0 @@\n-  assert(CDSConfig::is_using_aot_linked_classes(), \"must be\");\n@@ -1722,2 +1841,3 @@\n-  assert(dictionary->find_class(current, name) == nullptr, \"sanity\");\n-  dictionary->add_klass(current, name, k);\n+  if (dictionary->find_class(current, name) == nullptr) {\n+    dictionary->add_klass(current, name, k);\n+  }\n@@ -1725,1 +1845,0 @@\n-#endif\n","filename":"src\/hotspot\/share\/classfile\/systemDictionary.cpp","additions":134,"deletions":15,"binary":false,"changes":149,"status":"modified"},{"patch":"@@ -142,0 +142,1 @@\n+\n@@ -287,1 +288,1 @@\n-                                       ClassLoaderData* loader_data) NOT_CDS_RETURN;\n+                                       ClassLoaderData* loader_data);\n@@ -333,0 +334,2 @@\n+  static bool preload_from_null_free_field(InstanceKlass* ik, Handle class_loader, Symbol* sig, int field_index, TRAPS);\n+  static void try_preload_from_loadable_descriptors(InstanceKlass* ik, Handle class_loader, Symbol* sig, int field_index, TRAPS);\n","filename":"src\/hotspot\/share\/classfile\/systemDictionary.hpp","additions":4,"deletions":1,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -328,0 +328,14 @@\n+  do_intrinsic(_newNullRestrictedAtomicArray, jdk_internal_value_ValueClass, newNullRestrictedAtomicArray_name, newArray_signature3, F_SN) \\\n+   do_name(     newNullRestrictedAtomicArray_name,                \"newNullRestrictedAtomicArray\")                       \\\n+  do_intrinsic(_newNullRestrictedNonAtomicArray, jdk_internal_value_ValueClass, newNullRestrictedNonAtomicArray_name, newArray_signature3, F_SN) \\\n+   do_name(     newNullRestrictedNonAtomicArray_name,             \"newNullRestrictedNonAtomicArray\")                    \\\n+  do_intrinsic(_newNullableAtomicArray, jdk_internal_value_ValueClass, newNullableAtomicArray_name, newArray_signature2, F_SN) \\\n+   do_name(     newNullableAtomicArray_name,                      \"newNullableAtomicArray\")                             \\\n+   do_signature(newArray_signature2,                              \"(Ljava\/lang\/Class;I)[Ljava\/lang\/Object;\")            \\\n+   do_signature(newArray_signature3,                              \"(Ljava\/lang\/Class;ILjava\/lang\/Object;)[Ljava\/lang\/Object;\") \\\n+  do_intrinsic(_isFlatArray, jdk_internal_value_ValueClass, isFlatArray_name, object_boolean_signature, F_SN)           \\\n+   do_name(     isFlatArray_name,                                 \"isFlatArray\")                                        \\\n+  do_intrinsic(_isNullRestrictedArray, jdk_internal_value_ValueClass, isNullRestrictedArray_name, object_boolean_signature, F_SN) \\\n+   do_name(     isNullRestrictedArray_name,                       \"isNullRestrictedArray\")                              \\\n+  do_intrinsic(_isAtomicArray, jdk_internal_value_ValueClass, isAtomicArray_name, object_boolean_signature, F_SN)       \\\n+   do_name(     isAtomicArray_name,                               \"isAtomicArray\")                                      \\\n@@ -695,0 +709,7 @@\n+  do_intrinsic(_arrayInstanceBaseOffset,  jdk_internal_misc_Unsafe,     arrayInstanceBaseOffset_name, arrayProperties_signature, F_RN) \\\n+   do_name(     arrayInstanceBaseOffset_name,                           \"arrayInstanceBaseOffset0\")                              \\\n+   do_signature(arrayProperties_signature,                              \"([Ljava\/lang\/Object;)I\")                                \\\n+  do_intrinsic(_arrayInstanceIndexScale,  jdk_internal_misc_Unsafe,     _arrayInstanceIndexScale_name, arrayProperties_signature, F_RN) \\\n+   do_name(    _arrayInstanceIndexScale_name,                           \"arrayInstanceIndexScale0\")                              \\\n+  do_intrinsic(_arrayLayout,  jdk_internal_misc_Unsafe,                 _arrayLayout_name, arrayProperties_signature, F_RN)      \\\n+   do_name(    _arrayLayout_name,                                       \"arrayLayout0\")                                          \\\n@@ -731,0 +752,2 @@\n+  do_signature(getFlatValue_signature,    \"(Ljava\/lang\/Object;JILjava\/lang\/Class;)Ljava\/lang\/Object;\")                  \\\n+  do_signature(putFlatValue_signature,    \"(Ljava\/lang\/Object;JILjava\/lang\/Class;Ljava\/lang\/Object;)V\")                 \\\n@@ -741,0 +764,4 @@\n+  do_name(getValue_name,\"getValue\")             do_name(putValue_name,\"putValue\")                                       \\\n+  do_name(getFlatValue_name,\"getFlatValue\")     do_name(putFlatValue_name,\"putFlatValue\")                               \\\n+  do_name(makePrivateBuffer_name,\"makePrivateBuffer\")                                                                   \\\n+  do_name(finishPrivateBuffer_name,\"finishPrivateBuffer\")                                                               \\\n@@ -751,0 +778,1 @@\n+  do_intrinsic(_getFlatValue,       jdk_internal_misc_Unsafe,     getFlatValue_name, getFlatValue_signature,     F_RN)  \\\n@@ -760,0 +788,4 @@\n+  do_intrinsic(_putFlatValue,       jdk_internal_misc_Unsafe,     putFlatValue_name, putFlatValue_signature,     F_RN)  \\\n+                                                                                                                        \\\n+  do_intrinsic(_makePrivateBuffer,  jdk_internal_misc_Unsafe,     makePrivateBuffer_name, object_object_signature, F_RN)   \\\n+  do_intrinsic(_finishPrivateBuffer,  jdk_internal_misc_Unsafe,   finishPrivateBuffer_name, object_object_signature, F_RN) \\\n","filename":"src\/hotspot\/share\/classfile\/vmIntrinsics.hpp","additions":32,"deletions":0,"binary":false,"changes":32,"status":"modified"},{"patch":"@@ -94,0 +94,25 @@\n+  \/* Valhalla migrated classes. *\/                                                                \\\n+  template(java_lang_Number,                          \"java\/lang\/Number\")                         \\\n+  template(java_lang_Record,                          \"java\/lang\/Record\")                         \\\n+  template(java_util_Optional,                        \"java\/util\/Optional\")                       \\\n+  template(java_util_OptionalInt,                     \"java\/util\/OptionalInt\")                    \\\n+  template(java_util_OptionalLong,                    \"java\/util\/OptionalLong\")                   \\\n+  template(java_util_OptionalDouble,                  \"java\/util\/OptionalDouble\")                 \\\n+  template(java_time_LocalDate,                       \"java\/time\/LocalDate\")                      \\\n+  template(java_time_LocalDateTime,                   \"java\/time\/LocalDateTime\")                  \\\n+  template(java_time_LocalTime,                       \"java\/time\/LocalTime\")                      \\\n+  template(java_time_Duration,                        \"java\/time\/Duration\")                       \\\n+  template(java_time_Instant,                         \"java\/time\/Instant\")                        \\\n+  template(java_time_MonthDay,                        \"java\/time\/MonthDay\")                       \\\n+  template(java_time_ZonedDateTime,                   \"java\/time\/ZonedDateTime\")                  \\\n+  template(java_time_OffsetDateTime,                  \"java\/time\/OffsetDateTime\")                 \\\n+  template(java_time_OffsetTime,                      \"java\/time\/OffsetTime\")                     \\\n+  template(java_time_YearMonth,                       \"java\/time\/YearMonth\")                      \\\n+  template(java_time_Year,                            \"java\/time\/Year\")                           \\\n+  template(java_time_Period,                          \"java\/time\/Period\")                         \\\n+  template(java_time_chrono_ChronoLocalDateImpl,      \"java\/time\/chrono\/ChronoLocalDateImpl\")     \\\n+  template(java_time_chrono_MinguoDate,               \"java\/time\/chrono\/MinguoDate\")              \\\n+  template(java_time_chrono_HijrahDate,               \"java\/time\/chrono\/HijrahDate\")              \\\n+  template(java_time_chrono_JapaneseDate,             \"java\/time\/chrono\/JapaneseDate\")            \\\n+  template(java_time_chrono_ThaiBuddhistDate,         \"java\/time\/chrono\/ThaiBuddhistDate\")        \\\n+                                                                                                  \\\n@@ -140,1 +165,0 @@\n-  template(java_lang_Record,                          \"java\/lang\/Record\")                         \\\n@@ -170,0 +194,1 @@\n+  template(tag_loadable_descriptors,                  \"LoadableDescriptors\")                      \\\n@@ -206,0 +231,1 @@\n+  template(java_lang_IdentityException,               \"java\/lang\/IdentityException\")              \\\n@@ -254,0 +280,2 @@\n+  template(jdk_internal_vm_annotation_LooselyConsistentValue_signature,      \"Ljdk\/internal\/vm\/annotation\/LooselyConsistentValue;\") \\\n+  template(jdk_internal_vm_annotation_NullRestricted_signature,              \"Ljdk\/internal\/vm\/annotation\/NullRestricted;\") \\\n@@ -283,1 +311,0 @@\n-  template(trusted_final_name,                        \"trustedFinal\")                             \\\n@@ -505,0 +532,3 @@\n+  template(null_reset_value_name,                     \".null_reset\")                              \\\n+  template(acmp_maps_name,                            \".acmp_maps\")                               \\\n+  template(empty_marker_name,                         \".empty\")                                   \\\n@@ -581,0 +611,1 @@\n+  template(class_class_signature,                     \"(Ljava\/lang\/Class;)Ljava\/lang\/Class;\")     \\\n@@ -590,0 +621,1 @@\n+  template(object_object_boolean_signature,           \"(Ljava\/lang\/Object;Ljava\/lang\/Object;)Z\")                  \\\n@@ -717,0 +749,2 @@\n+  template(primaryType_name,                           \"primaryType\")                                             \\\n+  template(secondaryType_name,                         \"secondaryType\")                                           \\\n@@ -746,0 +780,8 @@\n+  template(java_lang_runtime_ValueObjectMethods,            \"java\/lang\/runtime\/ValueObjectMethods\")               \\\n+  template(isSubstitutable_name,                            \"isSubstitutable\")                                    \\\n+  template(isSubstitutableAlt_name,                         \"isSubstitutableAlt\")                                 \\\n+  template(valueObjectHashCode_name,                        \"valueObjectHashCode\")                                \\\n+  template(valueObjectHashCodeAlt_name,                     \"valueObjectHashCodeAlt\")                             \\\n+  template(jdk_internal_value_PrimitiveClass,               \"jdk\/internal\/value\/PrimitiveClass\")                  \\\n+  template(jdk_internal_value_ValueClass,                   \"jdk\/internal\/value\/ValueClass\")                      \\\n+                                                                                                                  \\\n@@ -829,0 +871,4 @@\n+  static void initialize_migrated_class_names();\n+\n+  static const int _migrated_class_names_length = 31;\n+  static Symbol* _migrated_class_names[_migrated_class_names_length];\n@@ -864,0 +910,7 @@\n+\n+  template<typename Function>\n+  static void migrated_class_names_do(Function f) {\n+     for (int i = 0; i < _migrated_class_names_length; i++) {\n+       f(_migrated_class_names[i]);\n+     }\n+  }\n","filename":"src\/hotspot\/share\/classfile\/vmSymbols.hpp","additions":55,"deletions":2,"binary":false,"changes":57,"status":"modified"},{"patch":"@@ -1279,0 +1279,1 @@\n+  SET_ADDRESS(_extrs, SharedRuntime::allocate_inline_types);\n@@ -1328,0 +1329,8 @@\n+    SET_ADDRESS(_extrs, Runtime1::new_null_free_array);\n+    SET_ADDRESS(_extrs, Runtime1::load_flat_array);\n+    SET_ADDRESS(_extrs, Runtime1::store_flat_array);\n+    SET_ADDRESS(_extrs, Runtime1::substitutability_check);\n+    SET_ADDRESS(_extrs, Runtime1::buffer_inline_args);\n+    SET_ADDRESS(_extrs, Runtime1::buffer_inline_args_no_receiver);\n+    SET_ADDRESS(_extrs, Runtime1::throw_identity_exception);\n+    SET_ADDRESS(_extrs, Runtime1::throw_illegal_monitor_state_exception);\n@@ -1355,0 +1364,2 @@\n+    SET_ADDRESS(_extrs, OptoRuntime::load_unknown_inline_C);\n+    SET_ADDRESS(_extrs, OptoRuntime::store_unknown_inline_C);\n@@ -1385,0 +1396,4 @@\n+  if (UseCompressedOops) {\n+    SET_ADDRESS(_extrs, CompressedOops::base_addr());\n+  }\n+\n","filename":"src\/hotspot\/share\/code\/aotCodeCache.cpp","additions":15,"deletions":0,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -716,0 +716,11 @@\n+\n+      \/\/ If inline types are passed as fields, use the extended signature\n+      \/\/ which contains the types of all (oop) fields of the inline type.\n+      if (is_compiled_by_c2() && callee->has_scalarized_args()) {\n+        const GrowableArray<SigEntry>* sig = callee->adapter()->get_sig_cc();\n+        assert(sig != nullptr, \"sig should never be null\");\n+        TempNewSymbol tmp_sig = SigEntry::create_symbol(sig);\n+        has_receiver = false; \/\/ The extended signature contains the receiver type\n+        fr.oops_compiled_arguments_do(tmp_sig, has_receiver, has_appendix, reg_map, f);\n+        return;\n+      }\n@@ -1246,0 +1257,4 @@\n+  _inline_entry_point             = entry_point();\n+  _verified_inline_entry_point    = verified_entry_point();\n+  _verified_inline_ro_entry_point = verified_entry_point();\n+\n@@ -1285,1 +1300,1 @@\n-\n+    assert(!method->has_scalarized_args(), \"scalarized native wrappers not supported yet\");\n@@ -1756,0 +1771,4 @@\n+    _inline_entry_point             = code_begin() + offsets->value(CodeOffsets::Inline_Entry);\n+    _verified_inline_entry_point    = code_begin() + offsets->value(CodeOffsets::Verified_Inline_Entry);\n+    _verified_inline_ro_entry_point = code_begin() + offsets->value(CodeOffsets::Verified_Inline_Entry_RO);\n+\n@@ -3201,4 +3220,4 @@\n-    if (deps.type() != Dependencies::evol_method)\n-      continue;\n-    Method* method = deps.method_argument(0);\n-    if (method == dependee) return true;\n+    if (Dependencies::has_method_dep(deps.type())) {\n+      Method* method = deps.method_argument(0);\n+      if (method == dependee) return true;\n+    }\n@@ -4042,0 +4061,1 @@\n+  if (pos == inline_entry_point())                                      label = \"[Inline Entry Point]\";\n@@ -4043,0 +4063,2 @@\n+  if (pos == verified_inline_entry_point())                             label = \"[Verified Inline Entry Point]\";\n+  if (pos == verified_inline_ro_entry_point())                          label = \"[Verified Inline Entry Point (RO)]\";\n@@ -4051,0 +4073,10 @@\n+static int maybe_print_entry_label(outputStream* stream, address pos, address entry, const char* label) {\n+  if (pos == entry) {\n+    stream->bol();\n+    stream->print_cr(\"%s\", label);\n+    return 1;\n+  } else {\n+    return 0;\n+  }\n+}\n+\n@@ -4053,4 +4085,13 @@\n-    const char* label = nmethod_section_label(block_begin);\n-    if (label != nullptr) {\n-      stream->bol();\n-      stream->print_cr(\"%s\", label);\n+    int n = 0;\n+    \/\/ Multiple entry points may be at the same position. Print them all.\n+    n += maybe_print_entry_label(stream, block_begin, entry_point(),                    \"[Entry Point]\");\n+    n += maybe_print_entry_label(stream, block_begin, inline_entry_point(),             \"[Inline Entry Point]\");\n+    n += maybe_print_entry_label(stream, block_begin, verified_entry_point(),           \"[Verified Entry Point]\");\n+    n += maybe_print_entry_label(stream, block_begin, verified_inline_entry_point(),    \"[Verified Inline Entry Point]\");\n+    n += maybe_print_entry_label(stream, block_begin, verified_inline_ro_entry_point(), \"[Verified Inline Entry Point (RO)]\");\n+    if (n == 0) {\n+      const char* label = nmethod_section_label(block_begin);\n+      if (label != nullptr) {\n+        stream->bol();\n+        stream->print_cr(\"%s\", label);\n+      }\n@@ -4060,6 +4101,62 @@\n-  if (block_begin == entry_point()) {\n-    Method* m = method();\n-    if (m != nullptr) {\n-      stream->print(\"  # \");\n-      m->print_value_on(stream);\n-      stream->cr();\n+  Method* m = method();\n+  if (m == nullptr || is_osr_method()) {\n+    return;\n+  }\n+\n+  \/\/ Print the name of the method (only once)\n+  address low = MIN3(entry_point(),\n+                     verified_entry_point(),\n+                     inline_entry_point());\n+  \/\/ The verified inline entry point and verified inline RO entry point are not always\n+  \/\/ used. When they are unused. CodeOffsets::Verified_Inline_Entry(_RO) is -1. Hence,\n+  \/\/ the calculated entry point is smaller than the block they are offsetting into.\n+  if (verified_inline_entry_point() >= block_begin) {\n+    low = MIN2(low, verified_inline_entry_point());\n+  }\n+  if (verified_inline_ro_entry_point() >= block_begin) {\n+    low = MIN2(low, verified_inline_ro_entry_point());\n+  }\n+  assert(low != 0, \"sanity\");\n+  if (block_begin == low) {\n+    stream->print(\"  # \");\n+    m->print_value_on(stream);\n+    stream->cr();\n+  }\n+\n+  \/\/ Print the arguments for the 3 types of verified entry points\n+  CompiledEntrySignature ces(m);\n+  ces.compute_calling_conventions(false);\n+  const GrowableArray<SigEntry>* sig_cc;\n+  const VMRegPair* regs;\n+  if (block_begin == verified_entry_point()) {\n+    sig_cc = ces.sig_cc();\n+    regs = ces.regs_cc();\n+  } else if (block_begin == verified_inline_entry_point()) {\n+    sig_cc = ces.sig();\n+    regs = ces.regs();\n+  } else if (block_begin == verified_inline_ro_entry_point()) {\n+    sig_cc = ces.sig_cc_ro();\n+    regs = ces.regs_cc_ro();\n+  } else {\n+    return;\n+  }\n+\n+  bool has_this = !m->is_static();\n+  if (ces.has_inline_recv() && block_begin == verified_entry_point()) {\n+    \/\/ <this> argument is scalarized for verified_entry_point()\n+    has_this = false;\n+  }\n+  const char* spname = \"sp\"; \/\/ make arch-specific?\n+  int stack_slot_offset = this->frame_size() * wordSize;\n+  int tab1 = 14, tab2 = 24;\n+  int sig_index = 0;\n+  int arg_index = has_this ? -1 : 0;\n+  bool did_old_sp = false;\n+  for (ExtendedSignature sig = ExtendedSignature(sig_cc, SigEntryFilter()); !sig.at_end(); ++sig) {\n+    bool at_this = (arg_index == -1);\n+    bool at_old_sp = false;\n+    BasicType t = (*sig)._bt;\n+    if (at_this) {\n+      stream->print(\"  # this: \");\n+    } else {\n+      stream->print(\"  # parm%d: \", arg_index);\n@@ -4067,19 +4164,7 @@\n-    if (m != nullptr && !is_osr_method()) {\n-      ResourceMark rm;\n-      int sizeargs = m->size_of_parameters();\n-      BasicType* sig_bt = NEW_RESOURCE_ARRAY(BasicType, sizeargs);\n-      VMRegPair* regs   = NEW_RESOURCE_ARRAY(VMRegPair, sizeargs);\n-      {\n-        int sig_index = 0;\n-        if (!m->is_static())\n-          sig_bt[sig_index++] = T_OBJECT; \/\/ 'this'\n-        for (SignatureStream ss(m->signature()); !ss.at_return_type(); ss.next()) {\n-          BasicType t = ss.type();\n-          sig_bt[sig_index++] = t;\n-          if (type2size[t] == 2) {\n-            sig_bt[sig_index++] = T_VOID;\n-          } else {\n-            assert(type2size[t] == 1, \"size is 1 or 2\");\n-          }\n-        }\n-        assert(sig_index == sizeargs, \"\");\n+    stream->move_to(tab1);\n+    VMReg fst = regs[sig_index].first();\n+    VMReg snd = regs[sig_index].second();\n+    if (fst->is_reg()) {\n+      stream->print(\"%s\", fst->name());\n+      if (snd->is_valid())  {\n+        stream->print(\":%s\", snd->name());\n@@ -4087,54 +4172,18 @@\n-      const char* spname = \"sp\"; \/\/ make arch-specific?\n-      SharedRuntime::java_calling_convention(sig_bt, regs, sizeargs);\n-      int stack_slot_offset = this->frame_size() * wordSize;\n-      int tab1 = 14, tab2 = 24;\n-      int sig_index = 0;\n-      int arg_index = (m->is_static() ? 0 : -1);\n-      bool did_old_sp = false;\n-      for (SignatureStream ss(m->signature()); !ss.at_return_type(); ) {\n-        bool at_this = (arg_index == -1);\n-        bool at_old_sp = false;\n-        BasicType t = (at_this ? T_OBJECT : ss.type());\n-        assert(t == sig_bt[sig_index], \"sigs in sync\");\n-        if (at_this)\n-          stream->print(\"  # this: \");\n-        else\n-          stream->print(\"  # parm%d: \", arg_index);\n-        stream->move_to(tab1);\n-        VMReg fst = regs[sig_index].first();\n-        VMReg snd = regs[sig_index].second();\n-        if (fst->is_reg()) {\n-          stream->print(\"%s\", fst->name());\n-          if (snd->is_valid())  {\n-            stream->print(\":%s\", snd->name());\n-          }\n-        } else if (fst->is_stack()) {\n-          int offset = fst->reg2stack() * VMRegImpl::stack_slot_size + stack_slot_offset;\n-          if (offset == stack_slot_offset)  at_old_sp = true;\n-          stream->print(\"[%s+0x%x]\", spname, offset);\n-        } else {\n-          stream->print(\"reg%d:%d??\", (int)(intptr_t)fst, (int)(intptr_t)snd);\n-        }\n-        stream->print(\" \");\n-        stream->move_to(tab2);\n-        stream->print(\"= \");\n-        if (at_this) {\n-          m->method_holder()->print_value_on(stream);\n-        } else {\n-          bool did_name = false;\n-          if (!at_this && ss.is_reference()) {\n-            Symbol* name = ss.as_symbol();\n-            name->print_value_on(stream);\n-            did_name = true;\n-          }\n-          if (!did_name)\n-            stream->print(\"%s\", type2name(t));\n-        }\n-        if (at_old_sp) {\n-          stream->print(\"  (%s of caller)\", spname);\n-          did_old_sp = true;\n-        }\n-        stream->cr();\n-        sig_index += type2size[t];\n-        arg_index += 1;\n-        if (!at_this)  ss.next();\n+    } else if (fst->is_stack()) {\n+      int offset = fst->reg2stack() * VMRegImpl::stack_slot_size + stack_slot_offset;\n+      if (offset == stack_slot_offset)  at_old_sp = true;\n+      stream->print(\"[%s+0x%x]\", spname, offset);\n+    } else {\n+      stream->print(\"reg%d:%d??\", (int)(intptr_t)fst, (int)(intptr_t)snd);\n+    }\n+    stream->print(\" \");\n+    stream->move_to(tab2);\n+    stream->print(\"= \");\n+    if (at_this) {\n+      m->method_holder()->print_value_on(stream);\n+    } else {\n+      bool did_name = false;\n+      if (is_reference_type(t)) {\n+        Symbol* name = (*sig)._name;\n+        name->print_value_on(stream);\n+        did_name = true;\n@@ -4142,6 +4191,4 @@\n-      if (!did_old_sp) {\n-        stream->print(\"  # \");\n-        stream->move_to(tab1);\n-        stream->print(\"[%s+0x%x]\", spname, stack_slot_offset);\n-        stream->print(\"  (%s of caller)\", spname);\n-        stream->cr();\n+      if (!did_name)\n+        stream->print(\"%s\", type2name(t));\n+      if ((*sig)._null_marker) {\n+        stream->print(\" (null marker)\");\n@@ -4150,0 +4197,14 @@\n+    if (at_old_sp) {\n+      stream->print(\"  (%s of caller)\", spname);\n+      did_old_sp = true;\n+    }\n+    stream->cr();\n+    sig_index += type2size[t];\n+    arg_index += 1;\n+  }\n+  if (!did_old_sp) {\n+    stream->print(\"  # \");\n+    stream->move_to(tab1);\n+    stream->print(\"[%s+0x%x]\", spname, stack_slot_offset);\n+    stream->print(\"  (%s of caller)\", spname);\n+    stream->cr();\n@@ -4273,1 +4334,1 @@\n-      st->print(\" {reexecute=%d rethrow=%d return_oop=%d}\", sd->should_reexecute(), sd->rethrow_exception(), sd->return_oop());\n+      st->print(\" {reexecute=%d rethrow=%d return_oop=%d return_scalarized=%d}\", sd->should_reexecute(), sd->rethrow_exception(), sd->return_oop(), sd->return_scalarized());\n","filename":"src\/hotspot\/share\/code\/nmethod.cpp","additions":156,"deletions":95,"binary":false,"changes":251,"status":"modified"},{"patch":"@@ -30,0 +30,1 @@\n+#include \"compiler\/compilerDefinitions.hpp\"\n@@ -219,0 +220,4 @@\n+  \/\/ TODO: can these be uint16_t, seem rely on -1 CodeOffset, can change later...\n+  address _inline_entry_point;              \/\/ inline type entry point (unpack all inline type args) with class check\n+  address _verified_inline_entry_point;     \/\/ inline type entry point (unpack all inline type args) without class check\n+  address _verified_inline_ro_entry_point;  \/\/ inline type entry point (unpack receiver only) without class check\n@@ -700,0 +705,3 @@\n+  address inline_entry_point() const              { return _inline_entry_point; }             \/\/ inline type entry point (unpack all inline type args)\n+  address verified_inline_entry_point() const     { return _verified_inline_entry_point; }    \/\/ inline type entry point (unpack all inline type args) without class check\n+  address verified_inline_ro_entry_point() const  { return _verified_inline_ro_entry_point; } \/\/ inline type entry point (only unpack receiver) without class check\n@@ -766,0 +774,10 @@\n+  bool  needs_stack_repair() const {\n+    if (is_compiled_by_c1()) {\n+      return method()->c1_needs_stack_repair();\n+    } else if (is_compiled_by_c2()) {\n+      return method()->c2_needs_stack_repair();\n+    } else {\n+      return false;\n+    }\n+  }\n+\n@@ -1078,3 +1096,4 @@\n-  \/\/ Fast breakpoint support. Tells if this compiled method is\n-  \/\/ dependent on the given method. Returns true if this nmethod\n-  \/\/ corresponds to the given method as well.\n+  \/\/ Tells if this compiled method is dependent on the given method.\n+  \/\/ Returns true if this nmethod corresponds to the given method as well.\n+  \/\/ It is used for fast breakpoint support and updating the calling convention\n+  \/\/ in case of mismatch.\n","filename":"src\/hotspot\/share\/code\/nmethod.hpp","additions":22,"deletions":3,"binary":false,"changes":25,"status":"modified"},{"patch":"@@ -177,0 +177,3 @@\n+JNIEXPORT jboolean JNICALL\n+JVM_IsValhallaEnabled(void);\n+\n@@ -559,0 +562,3 @@\n+JNIEXPORT jboolean JNICALL\n+JVM_IsIdentityClass(JNIEnv *env, jclass cls);\n+\n@@ -1085,0 +1091,21 @@\n+JNIEXPORT jarray JNICALL\n+JVM_CopyOfSpecialArray(JNIEnv *env, jarray orig, jint from, jint to);\n+\n+JNIEXPORT jarray JNICALL\n+JVM_NewNullRestrictedNonAtomicArray(JNIEnv *env, jclass elmClass, jint len, jobject initVal);\n+\n+JNIEXPORT jarray JNICALL\n+JVM_NewNullRestrictedAtomicArray(JNIEnv *env, jclass elmClass, jint len, jobject initVal);\n+\n+JNIEXPORT jarray JNICALL\n+JVM_NewNullableAtomicArray(JNIEnv *env, jclass elmClass, jint len);\n+\n+JNIEXPORT jboolean JNICALL\n+JVM_IsFlatArray(JNIEnv *env, jobject obj);\n+\n+JNIEXPORT jboolean JNICALL\n+JVM_IsNullRestrictedArray(JNIEnv *env, jobject obj);\n+\n+JNIEXPORT jboolean JNICALL\n+JVM_IsAtomicArray(JNIEnv *env, jobject obj);\n+\n","filename":"src\/hotspot\/share\/include\/jvm.h","additions":27,"deletions":0,"binary":false,"changes":27,"status":"modified"},{"patch":"@@ -215,0 +215,2 @@\n+          \/\/ For frames that need stack repair we skip this trick. This is because the stack walking code reads\n+          \/\/ the frame size from the stack, but the memory has already been overwritten by the SafepointBlob.\n@@ -218,1 +220,1 @@\n-          if (is_valid(pc_desc)) {\n+          if (is_valid(pc_desc) && !sampled_nm->needs_stack_repair()) {\n","filename":"src\/hotspot\/share\/jfr\/periodic\/sampling\/jfrThreadSampling.cpp","additions":3,"deletions":1,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -225,1 +225,1 @@\n-  nonstatic_field(InstanceKlass,               _misc_flags._flags,                            u2)                                    \\\n+  nonstatic_field(InstanceKlass,               _misc_flags._flags,                            u4)                                    \\\n@@ -738,0 +738,3 @@\n+  declare_constant(DataLayout::array_store_data_tag)                      \\\n+  declare_constant(DataLayout::array_load_data_tag)                       \\\n+  declare_constant(DataLayout::acmp_data_tag)                             \\\n@@ -818,1 +821,1 @@\n-  declare_constant(Klass::_lh_array_tag_obj_value)                        \\\n+  declare_constant(Klass::_lh_array_tag_ref_value)                        \\\n","filename":"src\/hotspot\/share\/jvmci\/vmStructs_jvmci.cpp","additions":5,"deletions":2,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -284,14 +284,0 @@\n-static markWord make_prototype(const Klass* kls) {\n-  markWord prototype = markWord::prototype();\n-#ifdef _LP64\n-  if (UseCompactObjectHeaders) {\n-    \/\/ With compact object headers, the narrow Klass ID is part of the mark word.\n-    \/\/ We therefore seed the mark word with the narrow Klass ID.\n-    precond(CompressedKlassPointers::is_encodable(kls));\n-    const narrowKlass nk = CompressedKlassPointers::encode(const_cast<Klass*>(kls));\n-    prototype = prototype.set_narrow_klass(nk);\n-  }\n-#endif\n-  return prototype;\n-}\n-\n@@ -310,2 +296,1 @@\n-Klass::Klass(KlassKind kind) : _kind(kind),\n-                               _prototype_header(make_prototype(this)),\n+Klass::Klass(KlassKind kind, markWord prototype_header) : _kind(kind),\n@@ -313,0 +298,1 @@\n+  set_prototype_header(make_prototype_header(this, prototype_header));\n@@ -325,2 +311,2 @@\n-  int  tag   =  isobj ? _lh_array_tag_obj_value : _lh_array_tag_type_value;\n-  int lh = array_layout_helper(tag, hsize, etype, exact_log2(esize));\n+  int  tag   =  isobj ? _lh_array_tag_ref_value : _lh_array_tag_type_value;\n+  int lh = array_layout_helper(tag, false, hsize, etype, exact_log2(esize));\n@@ -330,1 +316,1 @@\n-  assert(layout_helper_is_objArray(lh) == isobj, \"correct kind\");\n+  assert(layout_helper_is_refArray(lh) == isobj, \"correct kind\");\n@@ -1043,4 +1029,2 @@\n-     if (UseCompactObjectHeaders) {\n-       st->print(BULLET\"prototype_header: \" INTPTR_FORMAT, _prototype_header.value());\n-       st->cr();\n-     }\n+     st->print(BULLET\"prototype_header: \" INTPTR_FORMAT, _prototype_header.value());\n+     st->cr();\n","filename":"src\/hotspot\/share\/oops\/klass.cpp","additions":7,"deletions":23,"binary":false,"changes":30,"status":"modified"},{"patch":"@@ -56,0 +56,1 @@\n+#include \"oops\/inlineKlass.inline.hpp\"\n@@ -125,1 +126,0 @@\n-\n@@ -170,0 +170,5 @@\n+address Method::get_c2i_inline_entry() {\n+  assert(adapter() != nullptr, \"must have\");\n+  return adapter()->get_c2i_inline_entry();\n+}\n+\n@@ -178,0 +183,5 @@\n+address Method::get_c2i_unverified_inline_entry() {\n+  assert(adapter() != nullptr, \"must have\");\n+  return adapter()->get_c2i_unverified_inline_entry();\n+}\n+\n@@ -406,1 +416,1 @@\n-  if (!method_holder()->is_rewritten()) {\n+  if (!method_holder()->is_rewritten() || CDSConfig::is_valhalla_preview()) {\n@@ -449,0 +459,2 @@\n+    _from_compiled_inline_entry = _adapter->get_c2i_inline_entry();\n+    _from_compiled_inline_ro_entry = _adapter->get_c2i_inline_ro_entry();\n@@ -738,0 +750,14 @@\n+\/\/ InlineKlass the method is declared to return. This must not\n+\/\/ safepoint as it is called with references live on the stack at\n+\/\/ locations the GC is unaware of.\n+InlineKlass* Method::returns_inline_type() const {\n+  assert(InlineTypeReturnedAsFields, \"Inline types should never be returned as fields\");\n+  if (is_native()) {\n+    return nullptr;\n+  }\n+  NoSafepointVerifier nsv;\n+  SignatureStream ss(signature());\n+  ss.skip_to_return_type();\n+  return ss.as_inline_klass(method_holder());\n+}\n+\n@@ -886,0 +912,5 @@\n+  if (has_scalarized_return()) {\n+    \/\/ Don't treat this as (trivial) getter method because the\n+    \/\/ inline type should be returned in a scalarized form.\n+    return false;\n+  }\n@@ -907,0 +938,5 @@\n+  if (has_scalarized_args()) {\n+    \/\/ Don't treat this as (trivial) setter method because the\n+    \/\/ inline type argument should be passed in a scalarized form.\n+    return false;\n+  }\n@@ -917,6 +953,2 @@\n-          Bytecodes::is_return(java_code_at(last_index)));\n-}\n-\n-bool Method::has_valid_initializer_flags() const {\n-  return (is_static() ||\n-          method_holder()->major_version() < 51);\n+          Bytecodes::is_return(java_code_at(last_index)) &&\n+          !has_scalarized_args());\n@@ -925,1 +957,1 @@\n-bool Method::is_static_initializer() const {\n+bool Method::is_class_initializer() const {\n@@ -929,2 +961,3 @@\n-  return name() == vmSymbols::class_initializer_name() &&\n-         has_valid_initializer_flags();\n+  return (name() == vmSymbols::class_initializer_name() &&\n+          (is_static() ||\n+           method_holder()->major_version() < 51));\n@@ -933,2 +966,3 @@\n-bool Method::is_object_initializer() const {\n-   return name() == vmSymbols::object_initializer_name();\n+\/\/ A method named <init>, is a classic object constructor.\n+bool Method::is_object_constructor() const {\n+  return name() == vmSymbols::object_initializer_name();\n@@ -997,1 +1031,1 @@\n-  if( constants()->tag_at(klass_index).is_unresolved_klass() ) {\n+  if( constants()->tag_at(klass_index).is_unresolved_klass()) {\n@@ -1012,1 +1046,3 @@\n-    if (constants()->tag_at(klass_index).is_unresolved_klass()) return false;\n+    if (constants()->tag_at(klass_index).is_unresolved_klass()) {\n+      return false;\n+    }\n@@ -1180,1 +1216,3 @@\n-    _from_compiled_entry = nullptr;\n+    _from_compiled_entry    = nullptr;\n+    _from_compiled_inline_entry = nullptr;\n+    _from_compiled_inline_ro_entry = nullptr;\n@@ -1182,1 +1220,3 @@\n-    _from_compiled_entry = adapter()->get_c2i_entry();\n+    _from_compiled_entry    = adapter()->get_c2i_entry();\n+    _from_compiled_inline_entry = adapter()->get_c2i_inline_entry();\n+    _from_compiled_inline_ro_entry = adapter()->get_c2i_inline_ro_entry();\n@@ -1216,0 +1256,2 @@\n+  _from_compiled_inline_entry = nullptr;\n+  _from_compiled_inline_ro_entry = nullptr;\n@@ -1247,0 +1289,2 @@\n+  set_has_scalarized_args(false);\n+  set_has_scalarized_return(false);\n@@ -1283,0 +1327,3 @@\n+  if (InlineTypeReturnedAsFields && returns_inline_type() && !has_scalarized_return()) {\n+    set_has_scalarized_return();\n+  }\n@@ -1293,1 +1340,4 @@\n-    h_method->_from_compiled_entry = SharedRuntime::get_handle_wrong_method_abstract_stub();\n+    address wrong_method_abstract = SharedRuntime::get_handle_wrong_method_abstract_stub();\n+    h_method->_from_compiled_entry = wrong_method_abstract;\n+    h_method->_from_compiled_inline_entry = wrong_method_abstract;\n+    h_method->_from_compiled_inline_ro_entry = wrong_method_abstract;\n@@ -1300,0 +1350,2 @@\n+    h_method->_from_compiled_inline_entry = adapter()->get_c2i_inline_entry();\n+    h_method->_from_compiled_inline_ro_entry = adapter()->get_c2i_inline_ro_entry();\n@@ -1355,0 +1407,12 @@\n+address Method::verified_inline_code_entry() {\n+  DEBUG_ONLY(NoSafepointVerifier nsv;)\n+  assert(_from_compiled_inline_entry != nullptr, \"must be set\");\n+  return _from_compiled_inline_entry;\n+}\n+\n+address Method::verified_inline_ro_code_entry() {\n+  DEBUG_ONLY(NoSafepointVerifier nsv;)\n+  assert(_from_compiled_inline_ro_entry != nullptr, \"must be set\");\n+  return _from_compiled_inline_ro_entry;\n+}\n+\n@@ -1386,0 +1450,2 @@\n+  mh->_from_compiled_inline_entry = code->verified_inline_entry_point();\n+  mh->_from_compiled_inline_ro_entry = code->verified_inline_ro_entry_point();\n@@ -1578,0 +1644,2 @@\n+    m->set_from_compiled_inline_entry(m->adapter()->get_c2i_inline_entry());\n+    m->set_from_compiled_inline_ro_entry(m->adapter()->get_c2i_inline_ro_entry());\n@@ -2195,0 +2263,25 @@\n+bool Method::is_scalarized_arg(int idx) const {\n+  if (!has_scalarized_args()) {\n+    return false;\n+  }\n+  \/\/ Search through signature and check if argument is wrapped in T_METADATA\/T_VOID\n+  int depth = 0;\n+  const GrowableArray<SigEntry>* sig = adapter()->get_sig_cc();\n+  for (int i = 0; i < sig->length(); i++) {\n+    BasicType bt = sig->at(i)._bt;\n+    if (bt == T_METADATA) {\n+      depth++;\n+    }\n+    if (idx == 0) {\n+      break; \/\/ Argument found\n+    }\n+    if (bt == T_VOID && (sig->at(i-1)._bt != T_LONG && sig->at(i-1)._bt != T_DOUBLE)) {\n+      depth--;\n+    }\n+    if (depth == 0 && bt != T_LONG && bt != T_DOUBLE) {\n+      idx--; \/\/ Advance to next argument\n+    }\n+  }\n+  return depth != 0;\n+}\n+\n@@ -2220,0 +2313,4 @@\n+#ifdef ASSERT\n+  if (valid_itable_index())\n+    st->print_cr(\" - itable index:      %d\",   itable_index());\n+#endif\n@@ -2227,1 +2324,3 @@\n-  st->print_cr(\" - compiled entry     \" PTR_FORMAT, p2i(from_compiled_entry()));\n+  st->print_cr(\" - compiled entry           \" PTR_FORMAT, p2i(from_compiled_entry()));\n+  st->print_cr(\" - compiled inline entry    \" PTR_FORMAT, p2i(from_compiled_inline_entry()));\n+  st->print_cr(\" - compiled inline ro entry \" PTR_FORMAT, p2i(from_compiled_inline_ro_entry()));\n@@ -2297,0 +2396,1 @@\n+  if (WizardMode) access_flags().print_on(st);\n","filename":"src\/hotspot\/share\/oops\/method.cpp","additions":119,"deletions":19,"binary":false,"changes":138,"status":"modified"},{"patch":"@@ -26,0 +26,1 @@\n+#include \"classfile\/vmIntrinsics.hpp\"\n@@ -132,1 +133,2 @@\n-  bool eliminate_boxing = EliminateAutoBox;\n+  \/\/ TODO 8328675 Re-enable\n+  bool eliminate_boxing = false; \/\/ EliminateAutoBox;\n@@ -661,0 +663,2 @@\n+  case vmIntrinsics::_makePrivateBuffer:\n+  case vmIntrinsics::_finishPrivateBuffer:\n@@ -670,0 +674,1 @@\n+  case vmIntrinsics::_getFlatValue:\n@@ -679,0 +684,1 @@\n+  case vmIntrinsics::_putFlatValue:\n@@ -745,0 +751,3 @@\n+  case vmIntrinsics::_arrayInstanceBaseOffset:\n+  case vmIntrinsics::_arrayInstanceIndexScale:\n+  case vmIntrinsics::_arrayLayout:\n@@ -762,0 +771,6 @@\n+  case vmIntrinsics::_newNullRestrictedNonAtomicArray:\n+  case vmIntrinsics::_newNullRestrictedAtomicArray:\n+  case vmIntrinsics::_newNullableAtomicArray:\n+  case vmIntrinsics::_isFlatArray:\n+  case vmIntrinsics::_isNullRestrictedArray:\n+  case vmIntrinsics::_isAtomicArray:\n","filename":"src\/hotspot\/share\/opto\/c2compiler.cpp","additions":16,"deletions":1,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -32,0 +32,1 @@\n+#include \"oops\/accessDecorators.hpp\"\n@@ -37,0 +38,1 @@\n+#include \"opto\/inlinetypenode.hpp\"\n@@ -121,1 +123,1 @@\n-  \/\/ paths to facilitate late inlinig.\n+  \/\/ paths to facilitate late inlining.\n@@ -130,0 +132,1 @@\n+      _call_node(nullptr),\n@@ -132,0 +135,8 @@\n+    if (InlineTypeReturnedAsFields && method->is_method_handle_intrinsic()) {\n+      \/\/ If that call has not been optimized by the time optimizations are over,\n+      \/\/ we'll need to add a call to create an inline type instance from the klass\n+      \/\/ returned by the call (see PhaseMacroExpand::expand_mh_intrinsic_return).\n+      \/\/ Separating memory and I\/O projections for exceptions is required to\n+      \/\/ perform that graph transformation.\n+      _separate_io_proj = true;\n+    }\n@@ -145,0 +156,1 @@\n+  PhaseGVN& gvn = kit.gvn();\n@@ -173,1 +185,4 @@\n-  kit.set_arguments_for_java_call(call);\n+  kit.set_arguments_for_java_call(call, is_late_inline());\n+  if (kit.stopped()) {\n+    return kit.transfer_exceptions_into_jvms();\n+  }\n@@ -214,1 +229,0 @@\n-\n@@ -272,0 +286,3 @@\n+  if (kit.stopped()) {\n+    return kit.transfer_exceptions_into_jvms();\n+  }\n@@ -356,0 +373,4 @@\n+  virtual CallGenerator* inline_cg() {\n+    return _inline_cg;\n+  }\n+\n@@ -417,0 +438,8 @@\n+    \/\/ AlwaysIncrementalInline causes for_method_handle_inline() to\n+    \/\/ return a LateInlineCallGenerator. Extract the\n+    \/\/ InlineCallGenerator from it.\n+    if (AlwaysIncrementalInline && cg->is_late_inline() && !cg->is_virtual_late_inline()) {\n+      cg = cg->inline_cg();\n+      assert(cg != nullptr, \"inline call generator expected\");\n+    }\n+\n@@ -576,3 +605,3 @@\n-  const TypeTuple *r = call->tf()->domain();\n-  for (int i1 = 0; i1 < method()->arg_size(); i1++) {\n-    if (call->in(TypeFunc::Parms + i1)->is_top() && r->field_at(TypeFunc::Parms + i1) != Type::HALF) {\n+  const TypeTuple* r = call->tf()->domain_cc();\n+  for (uint i1 = TypeFunc::Parms; i1 < r->cnt(); i1++) {\n+    if (call->in(i1)->is_top() && r->field_at(i1) != Type::HALF) {\n@@ -596,1 +625,0 @@\n-  CallProjections callprojs;\n@@ -600,9 +628,8 @@\n-  call->extract_projections(&callprojs, true, do_asserts);\n-  if ((callprojs.fallthrough_catchproj == call->in(0)) ||\n-      (callprojs.catchall_catchproj    == call->in(0)) ||\n-      (callprojs.fallthrough_memproj   == call->in(TypeFunc::Memory)) ||\n-      (callprojs.catchall_memproj      == call->in(TypeFunc::Memory)) ||\n-      (callprojs.fallthrough_ioproj    == call->in(TypeFunc::I_O)) ||\n-      (callprojs.catchall_ioproj       == call->in(TypeFunc::I_O)) ||\n-      (callprojs.resproj != nullptr && call->find_edge(callprojs.resproj) != -1) ||\n-      (callprojs.exobj   != nullptr && call->find_edge(callprojs.exobj) != -1)) {\n+  CallProjections* callprojs = call->extract_projections(true, do_asserts);\n+  if ((callprojs->fallthrough_catchproj == call->in(0)) ||\n+      (callprojs->catchall_catchproj    == call->in(0)) ||\n+      (callprojs->fallthrough_memproj   == call->in(TypeFunc::Memory)) ||\n+      (callprojs->catchall_memproj      == call->in(TypeFunc::Memory)) ||\n+      (callprojs->fallthrough_ioproj    == call->in(TypeFunc::I_O)) ||\n+      (callprojs->catchall_ioproj       == call->in(TypeFunc::I_O)) ||\n+      (callprojs->exobj != nullptr && call->find_edge(callprojs->exobj) != -1)) {\n@@ -618,3 +645,12 @@\n-  \/\/ The call is marked as pure (no important side effects), but result isn't used.\n-  \/\/ It's safe to remove the call.\n-  bool result_not_used = (callprojs.resproj == nullptr || callprojs.resproj->outcnt() == 0);\n+\n+  bool result_not_used = true;\n+  for (uint i = 0; i < callprojs->nb_resproj; i++) {\n+    if (callprojs->resproj[i] != nullptr) {\n+      if (callprojs->resproj[i]->outcnt() != 0) {\n+        result_not_used = false;\n+      }\n+      if (call->find_edge(callprojs->resproj[i]) != -1) {\n+        return;\n+      }\n+    }\n+  }\n@@ -623,0 +659,2 @@\n+    \/\/ The call is marked as pure (no important side effects), but result isn't used.\n+    \/\/ It's safe to remove the call.\n@@ -635,0 +673,1 @@\n+    PhaseGVN& gvn = *C->initial_gvn();\n@@ -638,1 +677,1 @@\n-      C->initial_gvn()->set_type_bottom(mem);\n+      gvn.set_type_bottom(mem);\n@@ -642,4 +681,2 @@\n-    uint nargs = method()->arg_size();\n-    Node* top = C->top();\n-    for (uint i1 = 0; i1 < nargs; i1++) {\n-      map->set_req(TypeFunc::Parms + i1, top);\n+    for (uint i1 = TypeFunc::Parms; i1 < r->cnt(); i1++) {\n+      map->set_req(i1, C->top());\n@@ -653,0 +690,6 @@\n+    const TypeTuple* domain_sig = call->_tf->domain_sig();\n+    uint nargs = method()->arg_size();\n+    assert(domain_sig->cnt() - TypeFunc::Parms == nargs, \"inconsistent signature\");\n+\n+    uint j = TypeFunc::Parms;\n+    int arg_num = 0;\n@@ -654,1 +697,14 @@\n-      map->set_argument(jvms, i1, call->in(TypeFunc::Parms + i1));\n+      const Type* t = domain_sig->field_at(TypeFunc::Parms + i1);\n+      if (t->is_inlinetypeptr() && !method()->get_Method()->mismatch() && method()->is_scalarized_arg(arg_num)) {\n+        \/\/ Inline type arguments are not passed by reference: we get an argument per\n+        \/\/ field of the inline type. Build InlineTypeNodes from the inline type arguments.\n+        GraphKit arg_kit(jvms, &gvn);\n+        Node* vt = InlineTypeNode::make_from_multi(&arg_kit, call, t->inline_klass(), j, \/* in= *\/ true, \/* null_free= *\/ !t->maybe_null());\n+        map->set_control(arg_kit.control());\n+        map->set_argument(jvms, i1, vt);\n+      } else {\n+        map->set_argument(jvms, i1, call->in(j++));\n+      }\n+      if (t != Type::HALF) {\n+        arg_num++;\n+      }\n@@ -665,0 +721,20 @@\n+    \/\/ Check if we are late inlining a method handle call that returns an inline type as fields.\n+    Node* buffer_oop = nullptr;\n+    ciMethod* inline_method = inline_cg()->method();\n+    ciType* return_type = inline_method->return_type();\n+    if (!call->tf()->returns_inline_type_as_fields() &&\n+        return_type->is_inlinetype() && return_type->as_inline_klass()->can_be_returned_as_fields()) {\n+      \/\/ Allocate a buffer for the inline type returned as fields because the caller expects an oop return.\n+      \/\/ Do this before the method handle call in case the buffer allocation triggers deoptimization and\n+      \/\/ we need to \"re-execute\" the call in the interpreter (to make sure the call is only executed once).\n+      GraphKit arg_kit(jvms, &gvn);\n+      {\n+        PreserveReexecuteState preexecs(&arg_kit);\n+        arg_kit.jvms()->set_should_reexecute(true);\n+        arg_kit.inc_sp(nargs);\n+        Node* klass_node = arg_kit.makecon(TypeKlassPtr::make(return_type->as_inline_klass()));\n+        buffer_oop = arg_kit.new_instance(klass_node, nullptr, nullptr, \/* deoptimize_on_exception *\/ true);\n+      }\n+      jvms = arg_kit.transfer_exceptions_into_jvms();\n+    }\n+\n@@ -705,2 +781,2 @@\n-      C->set_has_loops(C->has_loops() || inline_cg()->method()->has_loops());\n-      C->env()->notice_inlined_method(inline_cg()->method());\n+      C->set_has_loops(C->has_loops() || inline_method->has_loops());\n+      C->env()->notice_inlined_method(inline_method);\n@@ -710,0 +786,54 @@\n+\n+    \/\/ Handle inline type returns\n+    InlineTypeNode* vt = result->isa_InlineType();\n+    if (vt != nullptr) {\n+      if (call->tf()->returns_inline_type_as_fields()) {\n+        vt->replace_call_results(&kit, call, C);\n+      } else {\n+        \/\/ Result might still be allocated (for example, if it has been stored to a non-flat field)\n+        if (!vt->is_allocated(&kit.gvn())) {\n+          assert(buffer_oop != nullptr, \"should have allocated a buffer\");\n+          RegionNode* region = new RegionNode(3);\n+\n+          \/\/ Check if result is null\n+          Node* null_ctl = kit.top();\n+          kit.null_check_common(vt->get_null_marker(), T_INT, false, &null_ctl);\n+          region->init_req(1, null_ctl);\n+          PhiNode* oop = PhiNode::make(region, kit.gvn().zerocon(T_OBJECT), TypeInstPtr::make(TypePtr::BotPTR, vt->type()->inline_klass()));\n+          Node* init_mem = kit.reset_memory();\n+          PhiNode* mem = PhiNode::make(region, init_mem, Type::MEMORY, TypePtr::BOTTOM);\n+\n+          \/\/ Not null, initialize the buffer\n+          kit.set_all_memory(init_mem);\n+\n+          Node* payload_ptr = kit.basic_plus_adr(buffer_oop, kit.gvn().type(vt)->inline_klass()->payload_offset());\n+          vt->store_flat(&kit, buffer_oop, payload_ptr, false, true, true, IN_HEAP | MO_UNORDERED);\n+          \/\/ Do not let stores that initialize this buffer be reordered with a subsequent\n+          \/\/ store that would make this buffer accessible by other threads.\n+          AllocateNode* alloc = AllocateNode::Ideal_allocation(buffer_oop);\n+          assert(alloc != nullptr, \"must have an allocation node\");\n+          kit.insert_mem_bar(Op_MemBarStoreStore, alloc->proj_out_or_null(AllocateNode::RawAddress));\n+          region->init_req(2, kit.control());\n+          oop->init_req(2, buffer_oop);\n+          mem->init_req(2, kit.merged_memory());\n+\n+          \/\/ Update oop input to buffer\n+          kit.gvn().hash_delete(vt);\n+          vt->set_oop(kit.gvn(), kit.gvn().transform(oop));\n+          vt->set_is_buffered(kit.gvn());\n+          vt = kit.gvn().transform(vt)->as_InlineType();\n+\n+          kit.set_control(kit.gvn().transform(region));\n+          kit.set_all_memory(kit.gvn().transform(mem));\n+          kit.record_for_igvn(region);\n+          kit.record_for_igvn(oop);\n+          kit.record_for_igvn(mem);\n+        }\n+        result = vt;\n+      }\n+      DEBUG_ONLY(buffer_oop = nullptr);\n+    } else {\n+      assert(result->is_top() || !call->tf()->returns_inline_type_as_fields() || !call->as_CallJava()->method()->return_type()->is_loaded(), \"Unexpected return value\");\n+    }\n+    assert(buffer_oop == nullptr, \"unused buffer allocation\");\n+\n@@ -938,0 +1068,23 @@\n+  \/\/ Allocate inline types if they are merged with objects (similar to Parse::merge_common())\n+  uint tos = kit.jvms()->stkoff() + kit.sp();\n+  uint limit = slow_map->req();\n+  for (uint i = TypeFunc::Parms; i < limit; i++) {\n+    Node* m = kit.map()->in(i);\n+    Node* n = slow_map->in(i);\n+    const Type* t = gvn.type(m)->meet_speculative(gvn.type(n));\n+    \/\/ TODO 8284443 still needed?\n+    if (m->is_InlineType() && !t->is_inlinetypeptr()) {\n+      \/\/ Allocate inline type in fast path\n+      m = m->as_InlineType()->buffer(&kit);\n+      kit.map()->set_req(i, m);\n+    }\n+    if (n->is_InlineType() && !t->is_inlinetypeptr()) {\n+      \/\/ Allocate inline type in slow path\n+      PreserveJVMState pjvms(&kit);\n+      kit.set_map(slow_map);\n+      n = n->as_InlineType()->buffer(&kit);\n+      kit.map()->set_req(i, n);\n+      slow_map = kit.stop();\n+    }\n+  }\n+\n@@ -961,2 +1114,0 @@\n-  uint tos = kit.jvms()->stkoff() + kit.sp();\n-  uint limit = slow_map->req();\n@@ -999,2 +1150,2 @@\n-  if (IncrementalInlineMH && call_site_count > 0 &&\n-      (should_delay || input_not_const || !C->inlining_incrementally() || C->over_inlining_cutoff())) {\n+  if (IncrementalInlineMH && (AlwaysIncrementalInline ||\n+                            (call_site_count > 0 && (should_delay || input_not_const || !C->inlining_incrementally() || C->over_inlining_cutoff())))) {\n@@ -1008,0 +1159,1 @@\n+\n@@ -1055,0 +1207,1 @@\n+      int nargs = callee->arg_size();\n@@ -1056,1 +1209,1 @@\n-      Node* member_name = kit.argument(callee->arg_size() - 1);\n+      Node* member_name = kit.argument(nargs - 1);\n@@ -1126,1 +1279,2 @@\n-                                              speculative_receiver_type);\n+                                              speculative_receiver_type,\n+                                              true);\n@@ -1198,1 +1352,1 @@\n-    Node* receiver = kit.null_check_receiver_before_call(method());\n+    kit.null_check_receiver_before_call(method());\n","filename":"src\/hotspot\/share\/opto\/callGenerator.cpp","additions":188,"deletions":34,"binary":false,"changes":222,"status":"modified"},{"patch":"@@ -26,0 +26,1 @@\n+#include \"ci\/ciFlatArrayKlass.hpp\"\n@@ -37,0 +38,1 @@\n+#include \"opto\/inlinetypenode.hpp\"\n@@ -46,0 +48,1 @@\n+#include \"runtime\/stubRoutines.hpp\"\n@@ -80,1 +83,1 @@\n-Node *StartNode::match( const ProjNode *proj, const Matcher *match ) {\n+Node *StartNode::match(const ProjNode *proj, const Matcher *match, const RegMask* mask) {\n@@ -104,11 +107,0 @@\n-\/\/------------------------------StartOSRNode----------------------------------\n-\/\/ The method start node for an on stack replacement adapter\n-\n-\/\/------------------------------osr_domain-----------------------------\n-const TypeTuple *StartOSRNode::osr_domain() {\n-  const Type **fields = TypeTuple::fields(2);\n-  fields[TypeFunc::Parms+0] = TypeRawPtr::BOTTOM;  \/\/ address of osr buffer\n-\n-  return TypeTuple::make(TypeFunc::Parms+1, fields);\n-}\n-\n@@ -502,0 +494,8 @@\n+      } else if (cik->is_flat_array_klass()) {\n+        ciKlass* cie = cik->as_flat_array_klass()->base_element_klass();\n+        cie->print_name_on(st);\n+        st->print(\"[%d]\", spobj->n_fields());\n+        int ndim = cik->as_array_klass()->dimension() - 1;\n+        while (ndim-- > 0) {\n+          st->print(\"[]\");\n+        }\n@@ -507,0 +507,7 @@\n+        if (iklass != nullptr && iklass->is_inlinetype()) {\n+          Node* null_marker = mcall->in(first_ind++);\n+          if (!null_marker->is_top()) {\n+            st->print(\" [null marker\");\n+            format_helper(regalloc, st, null_marker, \":\", -1, nullptr);\n+          }\n+        }\n@@ -508,1 +515,0 @@\n-        ciField* cifield;\n@@ -511,2 +517,1 @@\n-          cifield = iklass->nonstatic_field_at(0);\n-          cifield->print_name_on(st);\n+          iklass->nonstatic_field_at(0)->print_name_on(st);\n@@ -521,2 +526,1 @@\n-            cifield = iklass->nonstatic_field_at(j);\n-            cifield->print_name_on(st);\n+            iklass->nonstatic_field_at(j)->print_name_on(st);\n@@ -757,1 +761,1 @@\n-const Type *CallNode::bottom_type() const { return tf()->range(); }\n+const Type *CallNode::bottom_type() const { return tf()->range_cc(); }\n@@ -762,1 +766,1 @@\n-  return tf()->range();\n+  return tf()->range_cc();\n@@ -767,0 +771,7 @@\n+  if (_entry_point == StubRoutines::store_inline_type_fields_to_buf()) {\n+    \/\/ The call to that stub is a special case: its inputs are\n+    \/\/ multiple values returned from a call and so it should follow\n+    \/\/ the return convention.\n+    SharedRuntime::java_return_convention(sig_bt, parm_regs, argcnt);\n+    return;\n+  }\n@@ -775,27 +786,26 @@\n-Node *CallNode::match( const ProjNode *proj, const Matcher *match ) {\n-  switch (proj->_con) {\n-  case TypeFunc::Control:\n-  case TypeFunc::I_O:\n-  case TypeFunc::Memory:\n-    return new MachProjNode(this,proj->_con,RegMask::EMPTY,MachProjNode::unmatched_proj);\n-\n-  case TypeFunc::Parms+1:       \/\/ For LONG & DOUBLE returns\n-    assert(tf()->range()->field_at(TypeFunc::Parms+1) == Type::HALF, \"\");\n-    \/\/ 2nd half of doubles and longs\n-    return new MachProjNode(this,proj->_con, RegMask::EMPTY, (uint)OptoReg::Bad);\n-\n-  case TypeFunc::Parms: {       \/\/ Normal returns\n-    uint ideal_reg = tf()->range()->field_at(TypeFunc::Parms)->ideal_reg();\n-    OptoRegPair regs = Opcode() == Op_CallLeafVector\n-      ? match->vector_return_value(ideal_reg)      \/\/ Calls into assembly vector routine\n-      : is_CallRuntime()\n-        ? match->c_return_value(ideal_reg)  \/\/ Calls into C runtime\n-        : match->  return_value(ideal_reg); \/\/ Calls into compiled Java code\n-    RegMask rm = RegMask(regs.first());\n-\n-    if (Opcode() == Op_CallLeafVector) {\n-      \/\/ If the return is in vector, compute appropriate regmask taking into account the whole range\n-      if(ideal_reg >= Op_VecA && ideal_reg <= Op_VecZ) {\n-        if(OptoReg::is_valid(regs.second())) {\n-          for (OptoReg::Name r = regs.first(); r <= regs.second(); r = OptoReg::add(r, 1)) {\n-            rm.insert(r);\n+Node *CallNode::match(const ProjNode *proj, const Matcher *match, const RegMask* mask) {\n+  uint con = proj->_con;\n+  const TypeTuple* range_cc = tf()->range_cc();\n+  if (con >= TypeFunc::Parms) {\n+    if (tf()->returns_inline_type_as_fields()) {\n+      \/\/ The call returns multiple values (inline type fields): we\n+      \/\/ create one projection per returned value.\n+      assert(con <= TypeFunc::Parms+1 || InlineTypeReturnedAsFields, \"only for multi value return\");\n+      uint ideal_reg = range_cc->field_at(con)->ideal_reg();\n+      return new MachProjNode(this, con, mask[con-TypeFunc::Parms], ideal_reg);\n+    } else {\n+      if (con == TypeFunc::Parms) {\n+        uint ideal_reg = range_cc->field_at(TypeFunc::Parms)->ideal_reg();\n+        OptoRegPair regs = Opcode() == Op_CallLeafVector\n+          ? match->vector_return_value(ideal_reg)      \/\/ Calls into assembly vector routine\n+          : match->c_return_value(ideal_reg);\n+        RegMask rm = RegMask(regs.first());\n+\n+        if (Opcode() == Op_CallLeafVector) {\n+          \/\/ If the return is in vector, compute appropriate regmask taking into account the whole range\n+          if(ideal_reg >= Op_VecA && ideal_reg <= Op_VecZ) {\n+            if(OptoReg::is_valid(regs.second())) {\n+              for (OptoReg::Name r = regs.first(); r <= regs.second(); r = OptoReg::add(r, 1)) {\n+                rm.insert(r);\n+              }\n+            }\n@@ -804,0 +814,9 @@\n+\n+        if (OptoReg::is_valid(regs.second())) {\n+          rm.insert(regs.second());\n+        }\n+        return new MachProjNode(this,con,rm,ideal_reg);\n+      } else {\n+        assert(con == TypeFunc::Parms+1, \"only one return value\");\n+        assert(range_cc->field_at(TypeFunc::Parms+1) == Type::HALF, \"\");\n+        return new MachProjNode(this,con, RegMask::EMPTY, (uint)OptoReg::Bad);\n@@ -806,4 +825,0 @@\n-\n-    if( OptoReg::is_valid(regs.second()) )\n-      rm.insert(regs.second());\n-    return new MachProjNode(this,proj->_con,rm,ideal_reg);\n@@ -812,0 +827,6 @@\n+  switch (con) {\n+  case TypeFunc::Control:\n+  case TypeFunc::I_O:\n+  case TypeFunc::Memory:\n+    return new MachProjNode(this,proj->_con,RegMask::EMPTY,MachProjNode::unmatched_proj);\n+\n@@ -832,1 +853,1 @@\n-    const TypeTuple* args = _tf->domain();\n+    const TypeTuple* args = _tf->domain_sig();\n@@ -881,1 +902,1 @@\n-      const TypeTuple* d = tf()->domain();\n+      const TypeTuple* d = tf()->domain_cc();\n@@ -896,2 +917,2 @@\n-bool CallNode::has_non_debug_use(Node *n) {\n-  const TypeTuple * d = tf()->domain();\n+bool CallNode::has_non_debug_use(Node* n) {\n+  const TypeTuple* d = tf()->domain_cc();\n@@ -899,2 +920,1 @@\n-    Node *arg = in(i);\n-    if (arg == n) {\n+    if (in(i) == n) {\n@@ -907,0 +927,11 @@\n+bool CallNode::has_debug_use(Node* n) {\n+  if (jvms() != nullptr) {\n+    for (uint i = jvms()->debug_start(); i < jvms()->debug_end(); i++) {\n+      if (in(i) == n) {\n+        return true;\n+      }\n+    }\n+  }\n+  return false;\n+}\n+\n@@ -938,10 +969,15 @@\n-void CallNode::extract_projections(CallProjections* projs, bool separate_io_proj, bool do_asserts) const {\n-  projs->fallthrough_proj      = nullptr;\n-  projs->fallthrough_catchproj = nullptr;\n-  projs->fallthrough_ioproj    = nullptr;\n-  projs->catchall_ioproj       = nullptr;\n-  projs->catchall_catchproj    = nullptr;\n-  projs->fallthrough_memproj   = nullptr;\n-  projs->catchall_memproj      = nullptr;\n-  projs->resproj               = nullptr;\n-  projs->exobj                 = nullptr;\n+CallProjections* CallNode::extract_projections(bool separate_io_proj, bool do_asserts) const {\n+  uint max_res = TypeFunc::Parms-1;\n+  for (DUIterator_Fast imax, i = fast_outs(imax); i < imax; i++) {\n+    ProjNode *pn = fast_out(i)->as_Proj();\n+    max_res = MAX2(max_res, pn->_con);\n+  }\n+\n+  assert(max_res < _tf->range_cc()->cnt(), \"result out of bounds\");\n+\n+  uint projs_size = sizeof(CallProjections);\n+  if (max_res > TypeFunc::Parms) {\n+    projs_size += (max_res-TypeFunc::Parms)*sizeof(Node*);\n+  }\n+  char* projs_storage = resource_allocate_bytes(projs_size);\n+  CallProjections* projs = new(projs_storage)CallProjections(max_res - TypeFunc::Parms + 1);\n@@ -993,1 +1029,1 @@\n-      projs->resproj = pn;\n+      projs->resproj[0] = pn;\n@@ -996,1 +1032,3 @@\n-      assert(false, \"unexpected projection from allocation node.\");\n+      assert(pn->_con <= max_res, \"unexpected projection from allocation node.\");\n+      projs->resproj[pn->_con-TypeFunc::Parms] = pn;\n+      break;\n@@ -1003,1 +1041,1 @@\n-  assert(projs->fallthrough_proj      != nullptr, \"must be found\");\n+  assert(!do_asserts || projs->fallthrough_proj      != nullptr, \"must be found\");\n@@ -1013,0 +1051,1 @@\n+  return projs;\n@@ -1044,2 +1083,2 @@\n-  uint old_dbg_start = sfpt->is_Call() ? sfpt->as_Call()->tf()->domain()->cnt() : (uint)TypeFunc::Parms+1;\n-  uint new_dbg_start = tf()->domain()->cnt();\n+  uint old_dbg_start = sfpt->is_Call() ? sfpt->as_Call()->tf()->domain_sig()->cnt() : (uint)TypeFunc::Parms+1;\n+  uint new_dbg_start = tf()->domain_sig()->cnt();\n@@ -1086,0 +1125,4 @@\n+  Bytecodes::Code bc = jvms()->method()->java_code_at_bci(jvms()->bci());\n+  if (EnableValhalla && (bc == Bytecodes::_if_acmpeq || bc == Bytecodes::_if_acmpne)) {\n+    return true;\n+  }\n@@ -1128,0 +1171,10 @@\n+  if (can_reshape && uncommon_trap_request() != 0) {\n+    PhaseIterGVN* igvn = phase->is_IterGVN();\n+    if (remove_unknown_flat_array_load(igvn, in(0), in(TypeFunc::Memory), in(TypeFunc::Parms))) {\n+      if (!in(0)->is_Region()) {\n+        igvn->replace_input_of(this, 0, phase->C->top());\n+      }\n+      return this;\n+    }\n+  }\n+\n@@ -1188,0 +1241,122 @@\n+\/\/ Split if can cause the flat array branch of an array load with unknown type (see\n+\/\/ Parse::array_load) to end in an uncommon trap. In that case, the call to\n+\/\/ 'load_unknown_inline' is useless. Replace it with an uncommon trap with the same JVMState.\n+bool CallStaticJavaNode::remove_unknown_flat_array_load(PhaseIterGVN* igvn, Node* ctl, Node* mem, Node* unc_arg) {\n+  if (ctl == nullptr || ctl->is_top() || mem == nullptr || mem->is_top() || !mem->is_MergeMem()) {\n+    return false;\n+  }\n+  if (ctl->is_Region()) {\n+    bool res = false;\n+    for (uint i = 1; i < ctl->req(); i++) {\n+      MergeMemNode* mm = mem->clone()->as_MergeMem();\n+      for (MergeMemStream mms(mm); mms.next_non_empty(); ) {\n+        Node* m = mms.memory();\n+        if (m->is_Phi() && m->in(0) == ctl) {\n+          mms.set_memory(m->in(i));\n+        }\n+      }\n+      if (remove_unknown_flat_array_load(igvn, ctl->in(i), mm, unc_arg)) {\n+        res = true;\n+        if (!ctl->in(i)->is_Region()) {\n+          igvn->replace_input_of(ctl, i, igvn->C->top());\n+        }\n+      }\n+      igvn->remove_dead_node(mm);\n+    }\n+    return res;\n+  }\n+  \/\/ Verify the control flow is ok\n+  Node* call = ctl;\n+  MemBarNode* membar = nullptr;\n+  for (;;) {\n+    if (call == nullptr || call->is_top()) {\n+      return false;\n+    }\n+    if (call->is_Proj() || call->is_Catch() || call->is_MemBar()) {\n+      call = call->in(0);\n+    } else if (call->Opcode() == Op_CallStaticJava && !call->in(0)->is_top() &&\n+               call->as_Call()->entry_point() == OptoRuntime::load_unknown_inline_Java()) {\n+      assert(call->in(0)->is_Proj() && call->in(0)->in(0)->is_MemBar(), \"missing membar\");\n+      membar = call->in(0)->in(0)->as_MemBar();\n+      break;\n+    } else {\n+      return false;\n+    }\n+  }\n+\n+  JVMState* jvms = call->jvms();\n+  if (igvn->C->too_many_traps(jvms->method(), jvms->bci(), Deoptimization::trap_request_reason(uncommon_trap_request()))) {\n+    return false;\n+  }\n+\n+  Node* call_mem = call->in(TypeFunc::Memory);\n+  if (call_mem == nullptr || call_mem->is_top()) {\n+    return false;\n+  }\n+  if (!call_mem->is_MergeMem()) {\n+    call_mem = MergeMemNode::make(call_mem);\n+    igvn->register_new_node_with_optimizer(call_mem);\n+  }\n+\n+  \/\/ Verify that there's no unexpected side effect\n+  for (MergeMemStream mms2(mem->as_MergeMem(), call_mem->as_MergeMem()); mms2.next_non_empty2(); ) {\n+    Node* m1 = mms2.is_empty() ? mms2.base_memory() : mms2.memory();\n+    Node* m2 = mms2.memory2();\n+\n+    for (uint i = 0; i < 100; i++) {\n+      if (m1 == m2) {\n+        break;\n+      } else if (m1->is_Proj()) {\n+        m1 = m1->in(0);\n+      } else if (m1->is_MemBar()) {\n+        m1 = m1->in(TypeFunc::Memory);\n+      } else if (m1->Opcode() == Op_CallStaticJava &&\n+                 m1->as_Call()->entry_point() == OptoRuntime::load_unknown_inline_Java()) {\n+        if (m1 != call) {\n+          return false;\n+        }\n+        break;\n+      } else if (m1->is_MergeMem()) {\n+        MergeMemNode* mm = m1->as_MergeMem();\n+        int idx = mms2.alias_idx();\n+        if (idx == Compile::AliasIdxBot) {\n+          m1 = mm->base_memory();\n+        } else {\n+          m1 = mm->memory_at(idx);\n+        }\n+      } else {\n+        return false;\n+      }\n+    }\n+  }\n+  if (call_mem->outcnt() == 0) {\n+    igvn->remove_dead_node(call_mem);\n+  }\n+\n+  \/\/ Remove membar preceding the call\n+  membar->remove(igvn);\n+\n+  address call_addr = OptoRuntime::uncommon_trap_blob()->entry_point();\n+  CallNode* unc = new CallStaticJavaNode(OptoRuntime::uncommon_trap_Type(), call_addr, \"uncommon_trap\", nullptr);\n+  unc->init_req(TypeFunc::Control, call->in(0));\n+  unc->init_req(TypeFunc::I_O, call->in(TypeFunc::I_O));\n+  unc->init_req(TypeFunc::Memory, call->in(TypeFunc::Memory));\n+  unc->init_req(TypeFunc::FramePtr,  call->in(TypeFunc::FramePtr));\n+  unc->init_req(TypeFunc::ReturnAdr, call->in(TypeFunc::ReturnAdr));\n+  unc->init_req(TypeFunc::Parms+0, unc_arg);\n+  unc->set_cnt(PROB_UNLIKELY_MAG(4));\n+  unc->copy_call_debug_info(igvn, call->as_CallStaticJava());\n+\n+  \/\/ Replace the call with an uncommon trap\n+  igvn->replace_input_of(call, 0, igvn->C->top());\n+\n+  igvn->register_new_node_with_optimizer(unc);\n+\n+  Node* ctrl = igvn->transform(new ProjNode(unc, TypeFunc::Control));\n+  Node* halt = igvn->transform(new HaltNode(ctrl, call->in(TypeFunc::FramePtr), \"uncommon trap returned which should never happen\"));\n+  igvn->add_input_to(igvn->C->root(), halt);\n+\n+  return true;\n+}\n+\n+\n@@ -1303,0 +1478,7 @@\n+  if (_entry_point == nullptr) {\n+    \/\/ The call to that stub is a special case: its inputs are\n+    \/\/ multiple values returned from a call and so it should follow\n+    \/\/ the return convention.\n+    SharedRuntime::java_return_convention(sig_bt, parm_regs, argcnt);\n+    return;\n+  }\n@@ -1308,1 +1490,1 @@\n-  assert(tf()->range()->field_at(TypeFunc::Parms)->is_vect()->length_in_bytes() * BitsPerByte == _num_bits,\n+  assert(tf()->range_sig()->field_at(TypeFunc::Parms)->is_vect()->length_in_bytes() * BitsPerByte == _num_bits,\n@@ -1310,1 +1492,1 @@\n-  const TypeTuple* d = tf()->domain();\n+  const TypeTuple* d = tf()->domain_sig();\n@@ -1346,1 +1528,1 @@\n-      tf()->range(),\n+      tf()->range_cc(),\n@@ -1354,1 +1536,1 @@\n-  for (uint i = TypeFunc::Parms; i < tf()->range()->cnt(); i++) {\n+  for (uint i = TypeFunc::Parms; i < tf()->range_cc()->cnt(); i++) {\n@@ -1385,0 +1567,6 @@\n+uint CallLeafNoFPNode::match_edge(uint idx) const {\n+  \/\/ Null entry point is a special case for which the target is in a\n+  \/\/ register. Need to match that edge.\n+  return entry_point() == nullptr && idx == TypeFunc::Parms;\n+}\n+\n@@ -1435,1 +1623,14 @@\n-  return remove_dead_region(phase, can_reshape) ? this : nullptr;\n+  if (remove_dead_region(phase, can_reshape)) {\n+    return this;\n+  }\n+  \/\/ Scalarize inline types in safepoint debug info.\n+  \/\/ Delay this until all inlining is over to avoid getting inconsistent debug info.\n+  if (phase->C->scalarize_in_safepoints() && can_reshape && jvms() != nullptr) {\n+    for (uint i = jvms()->debug_start(); i < jvms()->debug_end(); i++) {\n+      Node* n = in(i)->uncast();\n+      if (n->is_InlineType()) {\n+        n->as_InlineType()->make_scalar_in_safepoints(phase->is_IterGVN());\n+      }\n+    }\n+  }\n+  return nullptr;\n@@ -1589,1 +1790,1 @@\n-  if (!alloc->is_Allocate() && !(alloc->Opcode() == Op_VectorBox)) {\n+  if (alloc != nullptr && !alloc->is_Allocate() && !(alloc->Opcode() == Op_VectorBox)) {\n@@ -1694,1 +1895,3 @@\n-                           Node *size, Node *klass_node, Node *initial_test)\n+                           Node *size, Node *klass_node,\n+                           Node* initial_test,\n+                           InlineTypeNode* inline_type_node)\n@@ -1702,0 +1905,1 @@\n+  _larval = false;\n@@ -1714,0 +1918,3 @@\n+  init_req( InlineType     , inline_type_node);\n+  \/\/ DefaultValue defaults to nullptr\n+  \/\/ RawDefaultValue defaults to nullptr\n@@ -1719,1 +1926,2 @@\n-  assert(initializer != nullptr && initializer->is_object_initializer(),\n+  assert(initializer != nullptr &&\n+         (initializer->is_object_constructor() || initializer->is_class_initializer()),\n@@ -1731,1 +1939,2 @@\n-Node *AllocateNode::make_ideal_mark(PhaseGVN *phase, Node* obj, Node* control, Node* mem) {\n+\n+Node* AllocateNode::make_ideal_mark(PhaseGVN* phase, Node* control, Node* mem) {\n@@ -1733,1 +1942,1 @@\n-  if (UseCompactObjectHeaders) {\n+  if (UseCompactObjectHeaders || EnableValhalla) {\n@@ -1737,0 +1946,6 @@\n+    if (EnableValhalla) {\n+      mark_node = phase->transform(mark_node);\n+      \/\/ Avoid returning a constant (old node) here because this method is used by LoadNode::Ideal\n+      mark_node = new OrXNode(mark_node, phase->MakeConX(_larval ? markWord::larval_bit_in_place : 0));\n+    }\n+    return mark_node;\n@@ -1738,2 +1953,1 @@\n-    \/\/ For now only enable fast locking for non-array types\n-    mark_node = phase->MakeConX(markWord::prototype().value());\n+    return phase->MakeConX(markWord::prototype().value());\n@@ -1741,1 +1955,0 @@\n-  return mark_node;\n@@ -2137,1 +2350,2 @@\n-  if (can_reshape && EliminateLocks && !is_non_esc_obj()) {\n+  const Type* obj_type = phase->type(obj_node());\n+  if (can_reshape && EliminateLocks && !is_non_esc_obj() && !obj_type->is_inlinetypeptr()) {\n@@ -2338,1 +2552,2 @@\n-  if (can_reshape && EliminateLocks && !is_non_esc_obj()) {\n+  const Type* obj_type = phase->type(obj_node());\n+  if (can_reshape && EliminateLocks && !is_non_esc_obj() && !obj_type->is_inlinetypeptr()) {\n@@ -2418,1 +2633,2 @@\n-    dest_t = dest_t->add_offset(Type::OffsetBot)->is_oopptr();\n+    dest_t = dest_t->is_aryptr()->with_field_offset(Type::OffsetBot)->add_offset(Type::OffsetBot)->is_oopptr();\n+    t_oop = t_oop->is_aryptr()->with_field_offset(Type::OffsetBot);\n","filename":"src\/hotspot\/share\/opto\/callnode.cpp","additions":303,"deletions":87,"binary":false,"changes":390,"status":"modified"},{"patch":"@@ -79,1 +79,1 @@\n-  virtual Node *match( const ProjNode *proj, const Matcher *m );\n+  virtual Node *match(const ProjNode *proj, const Matcher *m, const RegMask* mask);\n@@ -93,1 +93,0 @@\n-  static  const TypeTuple *osr_domain();\n@@ -669,1 +668,1 @@\n-class CallProjections : public StackObj {\n+class CallProjections {\n@@ -678,1 +677,19 @@\n-  Node* resproj;\n+  uint nb_resproj;\n+  Node* resproj[1]; \/\/ at least one projection\n+\n+  CallProjections(uint nbres) {\n+    fallthrough_proj      = nullptr;\n+    fallthrough_catchproj = nullptr;\n+    fallthrough_memproj   = nullptr;\n+    fallthrough_ioproj    = nullptr;\n+    catchall_catchproj    = nullptr;\n+    catchall_memproj      = nullptr;\n+    catchall_ioproj       = nullptr;\n+    exobj                 = nullptr;\n+    nb_resproj            = nbres;\n+    resproj[0]            = nullptr;\n+    for (uint i = 1; i < nb_resproj; i++) {\n+      resproj[i]          = nullptr;\n+    }\n+  }\n+\n@@ -700,1 +717,1 @@\n-    : SafePointNode(tf->domain()->cnt(), jvms, adr_type),\n+    : SafePointNode(tf->domain_cc()->cnt(), jvms, adr_type),\n@@ -727,1 +744,1 @@\n-  virtual Node*       match(const ProjNode* proj, const Matcher* m);\n+  virtual Node*       match(const ProjNode* proj, const Matcher* m, const RegMask* mask);\n@@ -742,0 +759,1 @@\n+  bool                has_debug_use(Node* n);\n@@ -748,2 +766,3 @@\n-    const TypeTuple* r = tf()->range();\n-    return (r->cnt() > TypeFunc::Parms &&\n+    const TypeTuple* r = tf()->range_sig();\n+    return (!tf()->returns_inline_type_as_fields() &&\n+            r->cnt() > TypeFunc::Parms &&\n@@ -756,1 +775,1 @@\n-  void extract_projections(CallProjections* projs, bool separate_io_proj, bool do_asserts = true) const;\n+  CallProjections* extract_projections(bool separate_io_proj, bool do_asserts = true) const;\n@@ -822,0 +841,3 @@\n+\n+  bool remove_unknown_flat_array_load(PhaseIterGVN* igvn, Node* ctl, Node* mem, Node* unc_arg);\n+\n@@ -830,0 +852,11 @@\n+    const TypeTuple *r = tf->range_sig();\n+    if (InlineTypeReturnedAsFields &&\n+        method != nullptr &&\n+        method->is_method_handle_intrinsic() &&\n+        r->cnt() > TypeFunc::Parms &&\n+        r->field_at(TypeFunc::Parms)->isa_oopptr() &&\n+        r->field_at(TypeFunc::Parms)->is_oopptr()->can_be_inline_type()) {\n+      \/\/ Make sure this call is processed by PhaseMacroExpand::expand_mh_intrinsic_return\n+      init_flags(Flag_is_macro);\n+      C->add_macro_node(this);\n+    }\n@@ -967,0 +1000,1 @@\n+  virtual uint match_edge(uint idx) const;\n@@ -1009,0 +1043,3 @@\n+    InlineType,                       \/\/ InlineTypeNode if this is an inline type allocation\n+    InitValue,                        \/\/ Init value for null-free inline type arrays\n+    RawInitValue,                     \/\/ Same as above but as raw machine word\n@@ -1019,0 +1056,3 @@\n+    fields[InlineType] = Type::BOTTOM;\n+    fields[InitValue] = TypeInstPtr::NOTNULL;\n+    fields[RawInitValue] = TypeX_X;\n@@ -1036,0 +1076,1 @@\n+  bool _larval;\n@@ -1039,1 +1080,2 @@\n-               Node *size, Node *klass_node, Node *initial_test);\n+               Node *size, Node *klass_node, Node *initial_test,\n+               InlineTypeNode* inline_type_node = nullptr);\n@@ -1104,1 +1146,1 @@\n-  Node* make_ideal_mark(PhaseGVN *phase, Node* obj, Node* control, Node* mem);\n+  Node* make_ideal_mark(PhaseGVN* phase, Node* control, Node* mem);\n@@ -1116,1 +1158,2 @@\n-                    Node* initial_test, Node* count_val, Node* valid_length_test)\n+                    Node* initial_test, Node* count_val, Node* valid_length_test,\n+                    Node* init_value, Node* raw_init_value)\n@@ -1121,1 +1164,1 @@\n-    set_req(AllocateNode::ALength,        count_val);\n+    set_req(AllocateNode::ALength, count_val);\n@@ -1123,0 +1166,2 @@\n+    init_req(AllocateNode::InitValue, init_value);\n+    init_req(AllocateNode::RawInitValue, raw_init_value);\n@@ -1124,0 +1169,1 @@\n+  virtual uint size_of() const { return sizeof(*this); }\n","filename":"src\/hotspot\/share\/opto\/callnode.hpp","additions":59,"deletions":13,"binary":false,"changes":72,"status":"modified"},{"patch":"@@ -35,0 +35,1 @@\n+#include \"opto\/inlinetypenode.hpp\"\n@@ -523,0 +524,1 @@\n+\n@@ -969,1 +971,2 @@\n-             cmp1->is_SubTypeCheck() || cmp2->is_SubTypeCheck()) {\n+             cmp1->is_SubTypeCheck() || cmp2->is_SubTypeCheck() ||\n+             cmp1->is_FlatArrayCheck() || cmp2->is_FlatArrayCheck()) {\n@@ -1048,1 +1051,1 @@\n-\/\/ note that these functions assume that the _adr_type field is flattened\n+\/\/ note that these functions assume that the _adr_type field is flat\n@@ -1066,1 +1069,1 @@\n-  assert(t != Type::MEMORY || at == flatten_phi_adr_type(at), \"flatten at\");\n+  assert(t != Type::MEMORY || at == flatten_phi_adr_type(at) || (flatten_phi_adr_type(at) == TypeAryPtr::INLINES && Compile::current()->flat_accesses_share_alias()), \"flatten at\");\n@@ -1195,0 +1198,8 @@\n+  \/\/ Flat array element shouldn't get their own memory slice until flat_accesses_share_alias is cleared.\n+  \/\/ It could be the graph has no loads\/stores and flat_accesses_share_alias is never cleared. EA could still\n+  \/\/ creates per element Phis but that wouldn't be a problem as there are no memory accesses for that array.\n+  assert(_adr_type == nullptr || _adr_type->isa_aryptr() == nullptr ||\n+         _adr_type->is_aryptr()->is_known_instance() ||\n+         !_adr_type->is_aryptr()->is_flat() ||\n+         !Compile::current()->flat_accesses_share_alias() ||\n+         _adr_type == TypeAryPtr::INLINES, \"flat array element shouldn't get its own slice yet\");\n@@ -1414,0 +1425,1 @@\n+\n@@ -1468,0 +1480,4 @@\n+  uin = unique_constant_input_recursive(phase);\n+  if (uin != nullptr) {\n+    return uin;\n+  }\n@@ -1567,0 +1583,36 @@\n+\/\/ Find the unique input, try to look recursively through input Phis\n+Node* PhiNode::unique_constant_input_recursive(PhaseGVN* phase) {\n+  if (!phase->is_IterGVN()) {\n+    return nullptr;\n+  }\n+\n+  ResourceMark rm;\n+  Node* unique = nullptr;\n+  Unique_Node_List visited;\n+  visited.push(this);\n+\n+  for (uint visited_idx = 0; visited_idx < visited.size(); visited_idx++) {\n+    Node* current = visited.at(visited_idx);\n+    for (uint i = 1; i < current->req(); i++) {\n+      Node* phi_in = current->in(i);\n+      if (phi_in == nullptr) {\n+        continue;\n+      }\n+\n+      if (phi_in->is_Phi()) {\n+        visited.push(phi_in);\n+      } else {\n+        if (unique == nullptr) {\n+          if (!phi_in->is_Con()) {\n+            return nullptr;\n+          }\n+          unique = phi_in;\n+        } else if (unique != phi_in) {\n+          return nullptr;\n+        }\n+      }\n+    }\n+  }\n+  return unique;\n+}\n+\n@@ -2034,0 +2086,52 @@\n+\/\/ Push inline type input nodes (and null) down through the phi recursively (can handle data loops).\n+InlineTypeNode* PhiNode::push_inline_types_down(PhaseGVN* phase, bool can_reshape, ciInlineKlass* inline_klass) {\n+  assert(inline_klass != nullptr, \"must be\");\n+  InlineTypeNode* vt = InlineTypeNode::make_null(*phase, inline_klass, \/* transform = *\/ false)->clone_with_phis(phase, in(0), nullptr, !_type->maybe_null(), true);\n+  if (can_reshape) {\n+    \/\/ Replace phi right away to be able to use the inline\n+    \/\/ type node when reaching the phi again through data loops.\n+    PhaseIterGVN* igvn = phase->is_IterGVN();\n+    for (DUIterator_Fast imax, i = fast_outs(imax); i < imax; i++) {\n+      Node* u = fast_out(i);\n+      igvn->rehash_node_delayed(u);\n+      imax -= u->replace_edge(this, vt);\n+      --i;\n+    }\n+    igvn->rehash_node_delayed(this);\n+    assert(outcnt() == 0, \"should be dead now\");\n+  }\n+  ResourceMark rm;\n+  Node_List casts;\n+  for (uint i = 1; i < req(); ++i) {\n+    Node* n = in(i);\n+    while (n->is_ConstraintCast()) {\n+      casts.push(n);\n+      n = n->in(1);\n+    }\n+    if (phase->type(n)->is_zero_type()) {\n+      n = InlineTypeNode::make_null(*phase, inline_klass);\n+    } else if (n->is_Phi()) {\n+      assert(can_reshape, \"can only handle phis during IGVN\");\n+      n = phase->transform(n->as_Phi()->push_inline_types_down(phase, can_reshape, inline_klass));\n+    }\n+    while (casts.size() != 0) {\n+      \/\/ Push the cast(s) through the InlineTypeNode\n+      \/\/ TODO 8302217 Can we avoid cloning? See InlineTypeNode::clone_if_required\n+      Node* cast = casts.pop()->clone();\n+      cast->set_req_X(1, n->as_InlineType()->get_oop(), phase);\n+      n = n->clone();\n+      n->as_InlineType()->set_oop(*phase, phase->transform(cast));\n+      n = phase->transform(n);\n+      if (n->is_top()) {\n+        break;\n+      }\n+    }\n+    bool transform = !can_reshape && (i == (req()-1)); \/\/ Transform phis on last merge\n+    assert(n->is_top() || n->is_InlineType(), \"Only InlineType or top at this point.\");\n+    if (n->is_InlineType()) {\n+      vt->merge_with(phase, n->as_InlineType(), i, transform);\n+    } \/\/ else nothing to do: phis above vt created by clone_with_phis are initialized to top already.\n+  }\n+  return vt;\n+}\n+\n@@ -2434,0 +2538,2 @@\n+    \/\/ TODO revisit this with JDK-8247216\n+    bool mergemem_only = true;\n@@ -2449,0 +2555,2 @@\n+      } else {\n+        mergemem_only = false;\n@@ -2472,1 +2580,1 @@\n-    if (!split_always_terminates && adr_type() == TypePtr::BOTTOM &&\n+    if (!mergemem_only && !split_always_terminates && adr_type() == TypePtr::BOTTOM &&\n@@ -2507,1 +2615,1 @@\n-      } else if (split_always_terminates) {\n+      } else if (mergemem_only || split_always_terminates) {\n@@ -2543,0 +2651,5 @@\n+            if (igvn) {\n+              \/\/ TODO revisit this with JDK-8247216\n+              \/\/ Put 'n' on the worklist because it might be modified by MergeMemStream::iteration_setup\n+              igvn->_worklist.push(n);\n+            }\n@@ -2585,1 +2698,1 @@\n-        set_req(i, new_in);\n+        set_req_X(i, new_in, phase->is_IterGVN());\n@@ -2653,0 +2766,5 @@\n+  Node* inline_type = try_push_inline_types_down(phase, can_reshape);\n+  if (inline_type != this) {\n+    return inline_type;\n+  }\n+\n@@ -2696,0 +2814,95 @@\n+\/\/ Check recursively if inputs are either an inline type, constant null\n+\/\/ or another Phi (including self references through data loops). If so,\n+\/\/ push the inline types down through the phis to enable folding of loads.\n+Node* PhiNode::try_push_inline_types_down(PhaseGVN* phase, const bool can_reshape) {\n+  if (!can_be_inline_type()) {\n+    return this;\n+  }\n+\n+  ciInlineKlass* inline_klass;\n+  if (can_push_inline_types_down(phase, can_reshape, inline_klass)) {\n+    assert(inline_klass != nullptr, \"must be\");\n+    return push_inline_types_down(phase, can_reshape, inline_klass);\n+  }\n+  return this;\n+}\n+\n+bool PhiNode::can_push_inline_types_down(PhaseGVN* phase, const bool can_reshape, ciInlineKlass*& inline_klass) {\n+  if (req() <= 2) {\n+    \/\/ Dead phi.\n+    return false;\n+  }\n+  inline_klass = nullptr;\n+\n+  \/\/ TODO 8302217 We need to prevent endless pushing through\n+  bool only_phi = (outcnt() != 0);\n+  for (DUIterator_Fast imax, i = fast_outs(imax); i < imax; i++) {\n+    Node* n = fast_out(i);\n+    if (n->is_InlineType() && n->in(1) == this) {\n+      return false;\n+    }\n+    if (!n->is_Phi()) {\n+      only_phi = false;\n+    }\n+  }\n+  if (only_phi) {\n+    return false;\n+  }\n+\n+  ResourceMark rm;\n+  Unique_Node_List worklist;\n+  worklist.push(this);\n+  Node_List casts;\n+\n+  for (uint next = 0; next < worklist.size(); next++) {\n+    Node* phi = worklist.at(next);\n+    for (uint i = 1; i < phi->req(); i++) {\n+      Node* n = phi->in(i);\n+      if (n == nullptr) {\n+        return false;\n+      }\n+      while (n->is_ConstraintCast()) {\n+        if (n->in(0) != nullptr && n->in(0)->is_top()) {\n+          \/\/ Will die, don't optimize\n+          return false;\n+        }\n+        casts.push(n);\n+        n = n->in(1);\n+      }\n+      const Type* type = phase->type(n);\n+      if (n->is_InlineType() && (inline_klass == nullptr || inline_klass == type->inline_klass())) {\n+        inline_klass = type->inline_klass();\n+      } else if (n->is_Phi() && can_reshape && n->bottom_type()->isa_ptr()) {\n+        worklist.push(n);\n+      } else if (!type->is_zero_type()) {\n+        return false;\n+      }\n+    }\n+  }\n+  if (inline_klass == nullptr) {\n+    return false;\n+  }\n+\n+  \/\/ Check if cast nodes can be pushed through\n+  const Type* t = Type::get_const_type(inline_klass);\n+  while (casts.size() != 0 && t != nullptr) {\n+    Node* cast = casts.pop();\n+    if (t->filter(cast->bottom_type()) == Type::TOP) {\n+      return false;\n+    }\n+  }\n+\n+  return true;\n+}\n+\n+#ifdef ASSERT\n+bool PhiNode::can_push_inline_types_down(PhaseGVN* phase) {\n+  if (!can_be_inline_type()) {\n+    return false;\n+  }\n+\n+  ciInlineKlass* inline_klass;\n+  return can_push_inline_types_down(phase, true, inline_klass);\n+}\n+#endif \/\/ ASSERT\n+\n@@ -3080,0 +3293,6 @@\n+\n+  \/\/ CheckCastPPNode::Ideal() for inline types reuses the exception\n+  \/\/ paths of a call to perform an allocation: we can see a Phi here.\n+  if (in(1)->is_Phi()) {\n+    return this;\n+  }\n","filename":"src\/hotspot\/share\/opto\/cfgnode.cpp","additions":225,"deletions":6,"binary":false,"changes":231,"status":"modified"},{"patch":"@@ -183,0 +183,3 @@\n+  bool can_push_inline_types_down(PhaseGVN* phase, bool can_reshape, ciInlineKlass*& inline_klass);\n+  InlineTypeNode* push_inline_types_down(PhaseGVN* phase, bool can_reshape, ciInlineKlass* inline_klass);\n+\n@@ -232,0 +235,1 @@\n+  Node* unique_constant_input_recursive(PhaseGVN* phase);\n@@ -258,0 +262,7 @@\n+  bool can_be_inline_type() const {\n+    return EnableValhalla && _type->isa_instptr() && _type->is_instptr()->can_be_inline_type();\n+  }\n+\n+  Node* try_push_inline_types_down(PhaseGVN* phase, bool can_reshape);\n+  DEBUG_ONLY(bool can_push_inline_types_down(PhaseGVN* phase);)\n+\n@@ -451,0 +462,2 @@\n+  bool is_flat_array_check(PhaseTransform* phase, Node** array = nullptr);\n+\n@@ -734,0 +747,1 @@\n+    init_class_id(Class_Blackhole);\n","filename":"src\/hotspot\/share\/opto\/cfgnode.hpp","additions":14,"deletions":0,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -1944,1 +1944,1 @@\n-          derived->bottom_type()->make_ptr()->is_ptr()->_offset == 0, \"sanity\");\n+         derived->bottom_type()->make_ptr()->is_ptr()->offset() == 0, \"sanity\");\n@@ -1947,1 +1947,1 @@\n-  if( tj == nullptr || tj->_offset == 0 ) {\n+  if (tj == nullptr || tj->offset() == 0) {\n@@ -2113,1 +2113,1 @@\n-                  derived->bottom_type()->make_ptr()->is_ptr()->_offset == 0, \"sanity\");\n+                 derived->bottom_type()->make_ptr()->is_ptr()->offset() == 0, \"sanity\");\n@@ -2115,1 +2115,1 @@\n-          if( tj && tj->_offset != 0 && tj->isa_oop_ptr() ) {\n+          if (tj && tj->offset() != 0 && tj->isa_oop_ptr()) {\n@@ -2405,1 +2405,1 @@\n-  const TypeTuple *domain = C->tf()->domain();\n+  const TypeTuple *domain = C->tf()->domain_cc();\n@@ -2614,1 +2614,1 @@\n-                  if (is_derived && check->bottom_type()->is_ptr()->_offset != 0) {\n+                  if (is_derived && check->bottom_type()->is_ptr()->offset() != 0) {\n@@ -2618,1 +2618,1 @@\n-                    assert(check->bottom_type()->is_ptr()->_offset == 0, \"Bad base pointer\");\n+                    assert(check->bottom_type()->is_ptr()->offset() == 0, \"Bad base pointer\");\n@@ -2627,1 +2627,1 @@\n-                } else if (check->bottom_type()->is_ptr()->_offset == 0) {\n+                } else if (check->bottom_type()->is_ptr()->offset() == 0) {\n","filename":"src\/hotspot\/share\/opto\/chaitin.cpp","additions":8,"deletions":8,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -62,0 +62,1 @@\n+#include \"opto\/inlinetypenode.hpp\"\n@@ -69,0 +70,1 @@\n+#include \"opto\/movenode.hpp\"\n@@ -408,0 +410,6 @@\n+  if (dead->is_InlineType()) {\n+    remove_inline_type(dead);\n+  }\n+  if (dead->is_LoadFlat() || dead->is_StoreFlat()) {\n+    remove_flat_access(dead);\n+  }\n@@ -455,0 +463,3 @@\n+    if (n->outcnt() == 0) {\n+      worklist.push(n);\n+    }\n@@ -463,0 +474,7 @@\n+  remove_useless_nodes(_inline_type_nodes,  useful); \/\/ remove useless inline type nodes\n+  remove_useless_nodes(_flat_access_nodes, useful);  \/\/ remove useless flat access nodes\n+#ifdef ASSERT\n+  if (_modified_nodes != nullptr) {\n+    _modified_nodes->remove_useless_nodes(useful.member_set());\n+  }\n+#endif\n@@ -649,0 +667,1 @@\n+      _has_circular_inline_type(false),\n@@ -667,0 +686,2 @@\n+      _inline_type_nodes (comp_arena(), 8, 0, nullptr),\n+      _flat_access_nodes(comp_arena(), 8, 0, nullptr),\n@@ -775,4 +796,2 @@\n-      const TypeTuple *domain = StartOSRNode::osr_domain();\n-      const TypeTuple *range = TypeTuple::make_range(method()->signature());\n-      init_tf(TypeFunc::make(domain, range));\n-      StartNode* s = new StartOSRNode(root(), domain);\n+      init_tf(TypeFunc::make(method(), \/* is_osr_compilation = *\/ true));\n+      StartNode* s = new StartOSRNode(root(), tf()->domain_sig());\n@@ -785,1 +804,1 @@\n-      StartNode* s = new StartNode(root(), tf()->domain());\n+      StartNode* s = new StartNode(root(), tf()->domain_cc());\n@@ -886,0 +905,10 @@\n+  if (needs_stack_repair()) {\n+    \/\/ One extra slot for the special stack increment value\n+    next_slot += 2;\n+  }\n+  \/\/ TODO 8284443 Only reserve extra slot if needed\n+  if (InlineTypeReturnedAsFields) {\n+    \/\/ One extra slot to hold the null marker for a nullable\n+    \/\/ inline type return if we run out of registers.\n+    next_slot += 2;\n+  }\n@@ -923,0 +952,1 @@\n+      _has_circular_inline_type(false),\n@@ -1077,0 +1107,4 @@\n+  _has_flat_accesses = false;\n+  _flat_accesses_share_alias = true;\n+  _scalarize_in_safepoints = false;\n+\n@@ -1361,0 +1395,9 @@\n+  if (ta && ta->is_not_flat()) {\n+    \/\/ Erase not flat property for alias analysis.\n+    tj = ta = ta->cast_to_not_flat(false);\n+  }\n+  if (ta && ta->is_not_null_free()) {\n+    \/\/ Erase not null free property for alias analysis.\n+    tj = ta = ta->cast_to_not_null_free(false);\n+  }\n+\n@@ -1374,0 +1417,2 @@\n+    \/\/ For flat inline type array, each field has its own slice so\n+    \/\/ we must include the field offset.\n@@ -1414,1 +1459,1 @@\n-      tj = ta = TypeAryPtr::make(ptr,ta->const_oop(),tary,nullptr,false,offset);\n+      tj = ta = TypeAryPtr::make(ptr,ta->const_oop(),tary,nullptr,false,Type::Offset(offset), ta->field_offset());\n@@ -1418,1 +1463,6 @@\n-      tj = ta = TypeAryPtr::make(ptr,ta->const_oop(),tary,nullptr,false,offset);\n+      tj = ta = TypeAryPtr::make(ptr,ta->const_oop(),tary,nullptr,false,Type::Offset(offset), ta->field_offset());\n+    }\n+    \/\/ Initially all flattened array accesses share a single slice\n+    if (ta->is_flat() && ta->elem() != TypeInstPtr::BOTTOM && _flat_accesses_share_alias) {\n+      const TypeAry* tary = TypeAry::make(TypeInstPtr::BOTTOM, ta->size(), \/* stable= *\/ false, \/* flat= *\/ true);\n+      tj = ta = TypeAryPtr::make(ptr,ta->const_oop(),tary,nullptr,false,Type::Offset(offset), Type::Offset(Type::OffsetBot));\n@@ -1425,1 +1475,1 @@\n-      tj = ta = TypeAryPtr::make(ptr,ta->const_oop(),tary,aklass,false,offset);\n+      tj = ta = TypeAryPtr::make(ptr,ta->const_oop(),tary,aklass,false,Type::Offset(offset), ta->field_offset());\n@@ -1443,0 +1493,1 @@\n+    tj = to = to->cast_to_maybe_flat_in_array(); \/\/ flatten to maybe flat in array\n@@ -1475,1 +1526,1 @@\n-        tj = to = TypeInstPtr::make(TypePtr::BotPTR, env()->Object_klass(), false, nullptr, offset);\n+        tj = to = TypeInstPtr::make(TypePtr::BotPTR, env()->Object_klass(), false, nullptr, Type::Offset(offset));\n@@ -1496,1 +1547,2 @@\n-        assert(tj == TypeInstPtr::make(to->ptr(), canonical_holder, is_known_inst, nullptr, offset, instance_id), \"exact type should be canonical type\");\n+        assert(tj == TypeInstPtr::make(to->ptr(), canonical_holder, is_known_inst, nullptr, Type::Offset(offset), instance_id,\n+                                       TypePtr::MaybeFlat), \"exact type should be canonical type\");\n@@ -1499,1 +1551,2 @@\n-        tj = to = TypeInstPtr::make(to->ptr(), canonical_holder, is_known_inst, nullptr, offset, instance_id);\n+        tj = to = TypeInstPtr::make(to->ptr(), canonical_holder, is_known_inst, nullptr, Type::Offset(offset), instance_id,\n+                                    TypePtr::MaybeFlat);\n@@ -1514,1 +1567,2 @@\n-                                       offset);\n+                                       Type::Offset(offset),\n+                                       TypePtr::MaybeFlat);\n@@ -1520,1 +1574,1 @@\n-        tj = tk = TypeInstKlassPtr::make(TypePtr::NotNull, env()->Object_klass(), offset);\n+        tj = tk = TypeInstKlassPtr::make(TypePtr::NotNull, env()->Object_klass(), Type::Offset(offset), TypePtr::MaybeFlat);\n@@ -1522,1 +1576,1 @@\n-        tj = tk = TypeAryKlassPtr::make(TypePtr::NotNull, tk->is_aryklassptr()->elem(), k, offset);\n+        tj = tk = TypeAryKlassPtr::make(TypePtr::NotNull, tk->is_aryklassptr()->elem(), k, Type::Offset(offset), tk->is_not_flat(), tk->is_not_null_free(), tk->is_flat(), tk->is_null_free(), tk->is_atomic(), tk->is_aryklassptr()->is_refined_type());\n@@ -1525,1 +1579,0 @@\n-\n@@ -1655,1 +1708,1 @@\n-Compile::AliasType* Compile::find_alias_type(const TypePtr* adr_type, bool no_create, ciField* original_field) {\n+Compile::AliasType* Compile::find_alias_type(const TypePtr* adr_type, bool no_create, ciField* original_field, bool uncached) {\n@@ -1660,3 +1713,6 @@\n-  AliasCacheEntry* ace = probe_alias_cache(adr_type);\n-  if (ace->_adr_type == adr_type) {\n-    return alias_type(ace->_index);\n+  AliasCacheEntry* ace = nullptr;\n+  if (!uncached) {\n+    ace = probe_alias_cache(adr_type);\n+    if (ace->_adr_type == adr_type) {\n+      return alias_type(ace->_index);\n+    }\n@@ -1712,0 +1768,1 @@\n+    ciField* field = nullptr;\n@@ -1718,0 +1775,1 @@\n+      const Type* elemtype = flat->is_aryptr()->elem();\n@@ -1719,1 +1777,8 @@\n-        alias_type(idx)->set_element(flat->is_aryptr()->elem());\n+        alias_type(idx)->set_element(elemtype);\n+      }\n+      int field_offset = flat->is_aryptr()->field_offset().get();\n+      if (flat->is_flat() &&\n+          field_offset != Type::OffsetBot) {\n+        ciInlineKlass* vk = elemtype->inline_klass();\n+        field_offset += vk->payload_offset();\n+        field = vk->get_field_by_offset(field_offset, false);\n@@ -1735,0 +1800,2 @@\n+      if (flat->offset() == in_bytes(Klass::layout_helper_offset()))\n+        alias_type(idx)->set_rewritable(false);\n@@ -1745,1 +1812,0 @@\n-      ciField* field;\n@@ -1752,0 +1818,4 @@\n+      } else if (tinst->is_inlinetypeptr()) {\n+        \/\/ Inline type field\n+        ciInlineKlass* vk = tinst->inline_klass();\n+        field = vk->get_field_by_offset(tinst->offset(), false);\n@@ -1756,7 +1826,14 @@\n-      assert(field == nullptr ||\n-             original_field == nullptr ||\n-             (field->holder() == original_field->holder() &&\n-              field->offset_in_bytes() == original_field->offset_in_bytes() &&\n-              field->is_static() == original_field->is_static()), \"wrong field?\");\n-      \/\/ Set field() and is_rewritable() attributes.\n-      if (field != nullptr)  alias_type(idx)->set_field(field);\n+    }\n+    assert(field == nullptr ||\n+           original_field == nullptr ||\n+           (field->holder() == original_field->holder() &&\n+            field->offset_in_bytes() == original_field->offset_in_bytes() &&\n+            field->is_static() == original_field->is_static()), \"wrong field?\");\n+    \/\/ Set field() and is_rewritable() attributes.\n+    if (field != nullptr) {\n+      alias_type(idx)->set_field(field);\n+      if (flat->isa_aryptr()) {\n+        \/\/ Fields of flat arrays are rewritable although they are declared final\n+        assert(flat->is_flat(), \"must be a flat array\");\n+        alias_type(idx)->set_rewritable(true);\n+      }\n@@ -1767,3 +1844,4 @@\n-  ace->_adr_type = adr_type;\n-  ace->_index    = idx;\n-  assert(alias_type(adr_type) == alias_type(idx),  \"type must be installed\");\n+  if (!uncached) {\n+    ace->_adr_type = adr_type;\n+    ace->_index    = idx;\n+    assert(alias_type(adr_type) == alias_type(idx),  \"type must be installed\");\n@@ -1771,6 +1849,7 @@\n-  \/\/ Might as well try to fill the cache for the flattened version, too.\n-  AliasCacheEntry* face = probe_alias_cache(flat);\n-  if (face->_adr_type == nullptr) {\n-    face->_adr_type = flat;\n-    face->_index    = idx;\n-    assert(alias_type(flat) == alias_type(idx), \"flat type must work too\");\n+    \/\/ Might as well try to fill the cache for the flattened version, too.\n+    AliasCacheEntry* face = probe_alias_cache(flat);\n+    if (face->_adr_type == nullptr) {\n+      face->_adr_type = flat;\n+      face->_index    = idx;\n+      assert(alias_type(flat) == alias_type(idx), \"flat type must work too\");\n+    }\n@@ -1898,0 +1977,426 @@\n+void Compile::add_inline_type(Node* n) {\n+  assert(n->is_InlineType(), \"unexpected node\");\n+  _inline_type_nodes.push(n);\n+}\n+\n+void Compile::remove_inline_type(Node* n) {\n+  assert(n->is_InlineType(), \"unexpected node\");\n+  if (_inline_type_nodes.contains(n)) {\n+    _inline_type_nodes.remove(n);\n+  }\n+}\n+\n+\/\/ Does the return value keep otherwise useless inline type allocations alive?\n+static bool return_val_keeps_allocations_alive(Node* ret_val) {\n+  ResourceMark rm;\n+  Unique_Node_List wq;\n+  wq.push(ret_val);\n+  bool some_allocations = false;\n+  for (uint i = 0; i < wq.size(); i++) {\n+    Node* n = wq.at(i);\n+    if (n->outcnt() > 1) {\n+      \/\/ Some other use for the allocation\n+      return false;\n+    } else if (n->is_InlineType()) {\n+      wq.push(n->in(1));\n+    } else if (n->is_Phi()) {\n+      for (uint j = 1; j < n->req(); j++) {\n+        wq.push(n->in(j));\n+      }\n+    } else if (n->is_CheckCastPP() &&\n+               n->in(1)->is_Proj() &&\n+               n->in(1)->in(0)->is_Allocate()) {\n+      some_allocations = true;\n+    } else if (n->is_CheckCastPP()) {\n+      wq.push(n->in(1));\n+    }\n+  }\n+  return some_allocations;\n+}\n+\n+void Compile::process_inline_types(PhaseIterGVN &igvn, bool remove) {\n+  \/\/ Make sure that the return value does not keep an otherwise unused allocation alive\n+  if (tf()->returns_inline_type_as_fields()) {\n+    Node* ret = nullptr;\n+    for (uint i = 1; i < root()->req(); i++) {\n+      Node* in = root()->in(i);\n+      if (in->Opcode() == Op_Return) {\n+        assert(ret == nullptr, \"only one return\");\n+        ret = in;\n+      }\n+    }\n+    if (ret != nullptr) {\n+      Node* ret_val = ret->in(TypeFunc::Parms);\n+      if (igvn.type(ret_val)->isa_oopptr() &&\n+          return_val_keeps_allocations_alive(ret_val)) {\n+        igvn.replace_input_of(ret, TypeFunc::Parms, InlineTypeNode::tagged_klass(igvn.type(ret_val)->inline_klass(), igvn));\n+        assert(ret_val->outcnt() == 0, \"should be dead now\");\n+        igvn.remove_dead_node(ret_val);\n+      }\n+    }\n+  }\n+  if (_inline_type_nodes.length() == 0) {\n+    return;\n+  }\n+  \/\/ Scalarize inline types in safepoint debug info.\n+  \/\/ Delay this until all inlining is over to avoid getting inconsistent debug info.\n+  set_scalarize_in_safepoints(true);\n+  for (int i = _inline_type_nodes.length()-1; i >= 0; i--) {\n+    InlineTypeNode* vt = _inline_type_nodes.at(i)->as_InlineType();\n+    vt->make_scalar_in_safepoints(&igvn);\n+    igvn.record_for_igvn(vt);\n+  }\n+  if (remove) {\n+    \/\/ Remove inline type nodes by replacing them with their oop input\n+    while (_inline_type_nodes.length() > 0) {\n+      InlineTypeNode* vt = _inline_type_nodes.pop()->as_InlineType();\n+      if (vt->outcnt() == 0) {\n+        igvn.remove_dead_node(vt);\n+        continue;\n+      }\n+      for (DUIterator i = vt->outs(); vt->has_out(i); i++) {\n+        DEBUG_ONLY(bool must_be_buffered = false);\n+        Node* u = vt->out(i);\n+        \/\/ Check if any users are blackholes. If so, rewrite them to use either the\n+        \/\/ allocated buffer, or individual components, instead of the inline type node\n+        \/\/ that goes away.\n+        if (u->is_Blackhole()) {\n+          BlackholeNode* bh = u->as_Blackhole();\n+\n+          \/\/ Unlink the old input\n+          int idx = bh->find_edge(vt);\n+          assert(idx != -1, \"The edge should be there\");\n+          bh->del_req(idx);\n+          --i;\n+\n+          if (vt->is_allocated(&igvn)) {\n+            \/\/ Already has the allocated instance, blackhole that\n+            bh->add_req(vt->get_oop());\n+          } else {\n+            \/\/ Not allocated yet, blackhole the components\n+            for (uint c = 0; c < vt->field_count(); c++) {\n+              bh->add_req(vt->field_value(c));\n+            }\n+          }\n+\n+          \/\/ Node modified, record for IGVN\n+          igvn.record_for_igvn(bh);\n+        }\n+#ifdef ASSERT\n+        \/\/ Verify that inline type is buffered when replacing by oop\n+        else if (u->is_InlineType()) {\n+          \/\/ InlineType uses don't need buffering because they are about to be replaced as well\n+        } else if (u->is_Phi()) {\n+          \/\/ TODO 8302217 Remove this once InlineTypeNodes are reliably pushed through\n+        } else {\n+          must_be_buffered = true;\n+        }\n+        if (must_be_buffered && !vt->is_allocated(&igvn)) {\n+          vt->dump(0);\n+          u->dump(0);\n+          assert(false, \"Should have been buffered\");\n+        }\n+#endif\n+      }\n+      igvn.replace_node(vt, vt->get_oop());\n+    }\n+  }\n+  igvn.optimize();\n+}\n+\n+void Compile::add_flat_access(Node* n) {\n+  assert(n != nullptr && (n->Opcode() == Op_LoadFlat || n->Opcode() == Op_StoreFlat), \"unexpected node %s\", n == nullptr ? \"nullptr\" : n->Name());\n+  assert(!_flat_access_nodes.contains(n), \"duplicate insertion\");\n+  _flat_access_nodes.push(n);\n+}\n+\n+void Compile::remove_flat_access(Node* n) {\n+  assert(n != nullptr && (n->Opcode() == Op_LoadFlat || n->Opcode() == Op_StoreFlat), \"unexpected node %s\", n == nullptr ? \"nullptr\" : n->Name());\n+  _flat_access_nodes.remove_if_existing(n);\n+}\n+\n+void Compile::process_flat_accesses(PhaseIterGVN& igvn) {\n+  assert(igvn._worklist.size() == 0, \"should be empty\");\n+  igvn.set_delay_transform(true);\n+  for (int i = _flat_access_nodes.length() - 1; i >= 0; i--) {\n+    Node* n = _flat_access_nodes.at(i);\n+    assert(n != nullptr, \"unexpected nullptr\");\n+    if (n->is_LoadFlat()) {\n+      n->as_LoadFlat()->expand_atomic(igvn);\n+    } else {\n+      n->as_StoreFlat()->expand_atomic(igvn);\n+    }\n+  }\n+  _flat_access_nodes.clear_and_deallocate();\n+  igvn.set_delay_transform(false);\n+  igvn.optimize();\n+}\n+\n+void Compile::adjust_flat_array_access_aliases(PhaseIterGVN& igvn) {\n+  if (!_has_flat_accesses) {\n+    return;\n+  }\n+  \/\/ Initially, all flat array accesses share the same slice to\n+  \/\/ keep dependencies with Object[] array accesses (that could be\n+  \/\/ to a flat array) correct. We're done with parsing so we\n+  \/\/ now know all flat array accesses in this compile\n+  \/\/ unit. Let's move flat array accesses to their own slice,\n+  \/\/ one per element field. This should help memory access\n+  \/\/ optimizations.\n+  ResourceMark rm;\n+  Unique_Node_List wq;\n+  wq.push(root());\n+\n+  Node_List mergememnodes;\n+  Node_List memnodes;\n+\n+  \/\/ Alias index currently shared by all flat memory accesses\n+  int index = get_alias_index(TypeAryPtr::INLINES);\n+\n+  \/\/ Find MergeMem nodes and flat array accesses\n+  for (uint i = 0; i < wq.size(); i++) {\n+    Node* n = wq.at(i);\n+    if (n->is_Mem()) {\n+      const TypePtr* adr_type = nullptr;\n+      adr_type = get_adr_type(get_alias_index(n->adr_type()));\n+      if (adr_type == TypeAryPtr::INLINES) {\n+        memnodes.push(n);\n+      }\n+    } else if (n->is_MergeMem()) {\n+      MergeMemNode* mm = n->as_MergeMem();\n+      if (mm->memory_at(index) != mm->base_memory()) {\n+        mergememnodes.push(n);\n+      }\n+    }\n+    for (uint j = 0; j < n->req(); j++) {\n+      Node* m = n->in(j);\n+      if (m != nullptr) {\n+        wq.push(m);\n+      }\n+    }\n+  }\n+\n+  if (memnodes.size() > 0) {\n+    _flat_accesses_share_alias = false;\n+\n+    \/\/ We are going to change the slice for the flat array\n+    \/\/ accesses so we need to clear the cache entries that refer to\n+    \/\/ them.\n+    for (uint i = 0; i < AliasCacheSize; i++) {\n+      AliasCacheEntry* ace = &_alias_cache[i];\n+      if (ace->_adr_type != nullptr &&\n+          ace->_adr_type->is_flat()) {\n+        ace->_adr_type = nullptr;\n+        ace->_index = (i != 0) ? 0 : AliasIdxTop; \/\/ Make sure the nullptr adr_type resolves to AliasIdxTop\n+      }\n+    }\n+\n+    \/\/ Find what aliases we are going to add\n+    int start_alias = num_alias_types()-1;\n+    int stop_alias = 0;\n+\n+    for (uint i = 0; i < memnodes.size(); i++) {\n+      Node* m = memnodes.at(i);\n+      const TypePtr* adr_type = nullptr;\n+      adr_type = m->adr_type();\n+#ifdef ASSERT\n+      m->as_Mem()->set_adr_type(adr_type);\n+#endif\n+      int idx = get_alias_index(adr_type);\n+      start_alias = MIN2(start_alias, idx);\n+      stop_alias = MAX2(stop_alias, idx);\n+    }\n+\n+    assert(stop_alias >= start_alias, \"should have expanded aliases\");\n+\n+    Node_Stack stack(0);\n+#ifdef ASSERT\n+    VectorSet seen(Thread::current()->resource_area());\n+#endif\n+    \/\/ Now let's fix the memory graph so each flat array access\n+    \/\/ is moved to the right slice. Start from the MergeMem nodes.\n+    uint last = unique();\n+    for (uint i = 0; i < mergememnodes.size(); i++) {\n+      MergeMemNode* current = mergememnodes.at(i)->as_MergeMem();\n+      Node* n = current->memory_at(index);\n+      MergeMemNode* mm = nullptr;\n+      do {\n+        \/\/ Follow memory edges through memory accesses, phis and\n+        \/\/ narrow membars and push nodes on the stack. Once we hit\n+        \/\/ bottom memory, we pop element off the stack one at a\n+        \/\/ time, in reverse order, and move them to the right slice\n+        \/\/ by changing their memory edges.\n+        if ((n->is_Phi() && n->adr_type() != TypePtr::BOTTOM) || n->is_Mem() || n->adr_type() == TypeAryPtr::INLINES) {\n+          assert(!seen.test_set(n->_idx), \"\");\n+          \/\/ Uses (a load for instance) will need to be moved to the\n+          \/\/ right slice as well and will get a new memory state\n+          \/\/ that we don't know yet. The use could also be the\n+          \/\/ backedge of a loop. We put a place holder node between\n+          \/\/ the memory node and its uses. We replace that place\n+          \/\/ holder with the correct memory state once we know it,\n+          \/\/ i.e. when nodes are popped off the stack. Using the\n+          \/\/ place holder make the logic work in the presence of\n+          \/\/ loops.\n+          if (n->outcnt() > 1) {\n+            Node* place_holder = nullptr;\n+            assert(!n->has_out_with(Op_Node), \"\");\n+            for (DUIterator k = n->outs(); n->has_out(k); k++) {\n+              Node* u = n->out(k);\n+              if (u != current && u->_idx < last) {\n+                bool success = false;\n+                for (uint l = 0; l < u->req(); l++) {\n+                  if (!stack.is_empty() && u == stack.node() && l == stack.index()) {\n+                    continue;\n+                  }\n+                  Node* in = u->in(l);\n+                  if (in == n) {\n+                    if (place_holder == nullptr) {\n+                      place_holder = new Node(1);\n+                      place_holder->init_req(0, n);\n+                    }\n+                    igvn.replace_input_of(u, l, place_holder);\n+                    success = true;\n+                  }\n+                }\n+                if (success) {\n+                  --k;\n+                }\n+              }\n+            }\n+          }\n+          if (n->is_Phi()) {\n+            stack.push(n, 1);\n+            n = n->in(1);\n+          } else if (n->is_Mem()) {\n+            stack.push(n, n->req());\n+            n = n->in(MemNode::Memory);\n+          } else {\n+            assert(n->is_Proj() && n->in(0)->Opcode() == Op_MemBarCPUOrder, \"\");\n+            stack.push(n, n->req());\n+            n = n->in(0)->in(TypeFunc::Memory);\n+          }\n+        } else {\n+          assert(n->adr_type() == TypePtr::BOTTOM || (n->Opcode() == Op_Node && n->_idx >= last) || (n->is_Proj() && n->in(0)->is_Initialize()), \"\");\n+          \/\/ Build a new MergeMem node to carry the new memory state\n+          \/\/ as we build it. IGVN should fold extraneous MergeMem\n+          \/\/ nodes.\n+          mm = MergeMemNode::make(n);\n+          igvn.register_new_node_with_optimizer(mm);\n+          while (stack.size() > 0) {\n+            Node* m = stack.node();\n+            uint idx = stack.index();\n+            if (m->is_Mem()) {\n+              \/\/ Move memory node to its new slice\n+              const TypePtr* adr_type = m->adr_type();\n+              int alias = get_alias_index(adr_type);\n+              Node* prev = mm->memory_at(alias);\n+              igvn.replace_input_of(m, MemNode::Memory, prev);\n+              mm->set_memory_at(alias, m);\n+            } else if (m->is_Phi()) {\n+              \/\/ We need as many new phis as there are new aliases\n+              igvn.replace_input_of(m, idx, mm);\n+              if (idx == m->req()-1) {\n+                Node* r = m->in(0);\n+                for (uint j = (uint)start_alias; j <= (uint)stop_alias; j++) {\n+                  const TypePtr* adr_type = get_adr_type(j);\n+                  if (!adr_type->isa_aryptr() || !adr_type->is_flat() || j == (uint)index) {\n+                    continue;\n+                  }\n+                  Node* phi = new PhiNode(r, Type::MEMORY, get_adr_type(j));\n+                  igvn.register_new_node_with_optimizer(phi);\n+                  for (uint k = 1; k < m->req(); k++) {\n+                    phi->init_req(k, m->in(k)->as_MergeMem()->memory_at(j));\n+                  }\n+                  mm->set_memory_at(j, phi);\n+                }\n+                Node* base_phi = new PhiNode(r, Type::MEMORY, TypePtr::BOTTOM);\n+                igvn.register_new_node_with_optimizer(base_phi);\n+                for (uint k = 1; k < m->req(); k++) {\n+                  base_phi->init_req(k, m->in(k)->as_MergeMem()->base_memory());\n+                }\n+                mm->set_base_memory(base_phi);\n+              }\n+            } else {\n+              \/\/ This is a MemBarCPUOrder node from\n+              \/\/ Parse::array_load()\/Parse::array_store(), in the\n+              \/\/ branch that handles flat arrays hidden under\n+              \/\/ an Object[] array. We also need one new membar per\n+              \/\/ new alias to keep the unknown access that the\n+              \/\/ membars protect properly ordered with accesses to\n+              \/\/ known flat array.\n+              assert(m->is_Proj(), \"projection expected\");\n+              Node* ctrl = m->in(0)->in(TypeFunc::Control);\n+              igvn.replace_input_of(m->in(0), TypeFunc::Control, top());\n+              for (uint j = (uint)start_alias; j <= (uint)stop_alias; j++) {\n+                const TypePtr* adr_type = get_adr_type(j);\n+                if (!adr_type->isa_aryptr() || !adr_type->is_flat() || j == (uint)index) {\n+                  continue;\n+                }\n+                MemBarNode* mb = new MemBarCPUOrderNode(this, j, nullptr);\n+                igvn.register_new_node_with_optimizer(mb);\n+                Node* mem = mm->memory_at(j);\n+                mb->init_req(TypeFunc::Control, ctrl);\n+                mb->init_req(TypeFunc::Memory, mem);\n+                ctrl = new ProjNode(mb, TypeFunc::Control);\n+                igvn.register_new_node_with_optimizer(ctrl);\n+                mem = new ProjNode(mb, TypeFunc::Memory);\n+                igvn.register_new_node_with_optimizer(mem);\n+                mm->set_memory_at(j, mem);\n+              }\n+              igvn.replace_node(m->in(0)->as_Multi()->proj_out(TypeFunc::Control), ctrl);\n+            }\n+            if (idx < m->req()-1) {\n+              idx += 1;\n+              stack.set_index(idx);\n+              n = m->in(idx);\n+              break;\n+            }\n+            \/\/ Take care of place holder nodes\n+            if (m->has_out_with(Op_Node)) {\n+              Node* place_holder = m->find_out_with(Op_Node);\n+              if (place_holder != nullptr) {\n+                Node* mm_clone = mm->clone();\n+                igvn.register_new_node_with_optimizer(mm_clone);\n+                Node* hook = new Node(1);\n+                hook->init_req(0, mm);\n+                igvn.replace_node(place_holder, mm_clone);\n+                hook->destruct(&igvn);\n+              }\n+              assert(!m->has_out_with(Op_Node), \"place holder should be gone now\");\n+            }\n+            stack.pop();\n+          }\n+        }\n+      } while(stack.size() > 0);\n+      \/\/ Fix the memory state at the MergeMem we started from\n+      igvn.rehash_node_delayed(current);\n+      for (uint j = (uint)start_alias; j <= (uint)stop_alias; j++) {\n+        const TypePtr* adr_type = get_adr_type(j);\n+        if (!adr_type->isa_aryptr() || !adr_type->is_flat()) {\n+          continue;\n+        }\n+        current->set_memory_at(j, mm);\n+      }\n+      current->set_memory_at(index, current->base_memory());\n+    }\n+    igvn.optimize();\n+  }\n+  print_method(PHASE_SPLIT_INLINES_ARRAY, 2);\n+#ifdef ASSERT\n+  if (!_flat_accesses_share_alias) {\n+    wq.clear();\n+    wq.push(root());\n+    for (uint i = 0; i < wq.size(); i++) {\n+      Node* n = wq.at(i);\n+      assert(n->adr_type() != TypeAryPtr::INLINES, \"should have been removed from the graph\");\n+      for (uint j = 0; j < n->req(); j++) {\n+        Node* m = n->in(j);\n+        if (m != nullptr) {\n+          wq.push(m);\n+        }\n+      }\n+    }\n+  }\n+#endif\n+}\n+\n@@ -2016,1 +2521,1 @@\n-        if (!live_locals.at(i) && !local->is_top() && local != lhs && local!= rhs) {\n+        if (!live_locals.at(i) && !local->is_top() && local != lhs && local != rhs) {\n@@ -2031,1 +2536,1 @@\n-    \/\/ keep the mondified trap for late query\n+    \/\/ keep the modified trap for late query\n@@ -2243,1 +2748,4 @@\n-  assert(_modified_nodes == nullptr, \"not allowed\");\n+#ifdef ASSERT\n+  Unique_Node_List* modified_nodes = _modified_nodes;\n+  _modified_nodes = nullptr;\n+#endif\n@@ -2256,0 +2764,1 @@\n+  DEBUG_ONLY( _modified_nodes = modified_nodes; )\n@@ -2399,0 +2908,5 @@\n+  \/\/ Process inline type nodes now that all inlining is over\n+  process_inline_types(igvn);\n+\n+  adjust_flat_array_access_aliases(igvn);\n+\n@@ -2401,0 +2915,11 @@\n+  if (C->macro_count() > 0) {\n+    \/\/ Eliminate some macro nodes before EA to reduce analysis pressure\n+    PhaseMacroExpand mexp(igvn);\n+    mexp.eliminate_macro_nodes(\/* eliminate_locks= *\/ false);\n+    if (failing()) {\n+      return;\n+    }\n+    igvn.set_delay_transform(false);\n+    print_method(PHASE_ITER_GVN_AFTER_ELIMINATION, 2);\n+  }\n+\n@@ -2411,1 +2936,14 @@\n-      if (failing())  return;\n+      if (failing()) {\n+        return;\n+      }\n+      print_method(PHASE_PHASEIDEAL_BEFORE_EA, 2);\n+      if (C->macro_count() > 0) {\n+        \/\/ Eliminate some macro nodes before EA to reduce analysis pressure\n+        PhaseMacroExpand mexp(igvn);\n+        mexp.eliminate_macro_nodes(\/* eliminate_locks= *\/ false);\n+        if (failing()) {\n+          return;\n+        }\n+        igvn.set_delay_transform(false);\n+        print_method(PHASE_ITER_GVN_AFTER_ELIMINATION, 2);\n+      }\n@@ -2413,0 +2951,1 @@\n+\n@@ -2414,1 +2953,0 @@\n-    print_method(PHASE_PHASEIDEAL_BEFORE_EA, 2);\n@@ -2432,1 +2970,3 @@\n-        if (failing()) return;\n+        if (failing()) {\n+          return;\n+        }\n@@ -2436,3 +2976,0 @@\n-        igvn.optimize();\n-        if (failing()) return;\n-\n@@ -2453,0 +2990,5 @@\n+  process_flat_accesses(igvn);\n+  if (failing()) {\n+    return;\n+  }\n+\n@@ -2529,0 +3071,8 @@\n+  assert(_late_inlines.length() == 0 || IncrementalInlineMH || IncrementalInlineVirtual, \"not empty\");\n+\n+  if (_late_inlines.length() > 0) {\n+    \/\/ More opportunities to optimize virtual and MH calls.\n+    \/\/ Though it's maybe too late to perform inlining, strength-reducing them to direct calls is still an option.\n+    process_late_inline_calls_no_inline(igvn);\n+  }\n+\n@@ -2531,0 +3081,7 @@\n+    PhaseMacroExpand mex(igvn);\n+    \/\/ Last attempt to eliminate macro nodes.\n+    mex.eliminate_macro_nodes();\n+    if (failing()) {\n+      return;\n+    }\n+\n@@ -2532,1 +3089,0 @@\n-    PhaseMacroExpand  mex(igvn);\n@@ -2552,0 +3108,4 @@\n+  \/\/ Process inline type nodes again and remove them. From here\n+  \/\/ on we don't need to keep track of field values anymore.\n+  process_inline_types(igvn, \/* remove= *\/ true);\n+\n@@ -2568,0 +3128,1 @@\n+  DEBUG_ONLY( _late_inlines.clear(); )\n@@ -2570,9 +3131,0 @@\n-\n-  assert(_late_inlines.length() == 0 || IncrementalInlineMH || IncrementalInlineVirtual, \"not empty\");\n-\n-  if (_late_inlines.length() > 0) {\n-    \/\/ More opportunities to optimize virtual and MH calls.\n-    \/\/ Though it's maybe too late to perform inlining, strength-reducing them to direct calls is still an option.\n-    process_late_inline_calls_no_inline(igvn);\n-    if (failing())  return;\n-  }\n@@ -3324,1 +3876,1 @@\n-      for (unsigned int i = TypeFunc::Parms; i < call->tf()->domain()->cnt(); i++) {\n+      for (unsigned int i = TypeFunc::Parms; i < call->tf()->domain_sig()->cnt(); i++) {\n@@ -3370,0 +3922,1 @@\n+  case Op_StoreLSpecial:\n@@ -3922,0 +4475,5 @@\n+  case Op_InlineType: {\n+    n->dump(-1);\n+    assert(false, \"inline type node was not removed\");\n+    break;\n+  }\n@@ -4297,2 +4855,2 @@\n-      if (accessing_method->is_static_initializer() ||\n-          accessing_method->is_object_initializer() ||\n+      if (accessing_method->is_class_initializer() ||\n+          accessing_method->is_object_constructor() ||\n@@ -4306,1 +4864,1 @@\n-      if (accessing_method->is_static_initializer()) {\n+      if (accessing_method->is_class_initializer()) {\n@@ -4376,0 +4934,1 @@\n+               (n->is_Allocate() && i >= AllocateNode::InlineType) ||\n@@ -4378,1 +4937,1 @@\n-              \"only region, phi, arraycopy, unlock or membar nodes have null data edges\");\n+              \"only region, phi, arraycopy, allocate, unlock or membar nodes have null data edges\");\n@@ -4529,0 +5088,7 @@\n+\n+    \/\/ Do not fold the subtype check to an array klass pointer comparison for null-able inline type arrays\n+    \/\/ because null-free [LMyValue <: null-able [LMyValue but the klasses are different. Perform a full test.\n+    if (!superk->is_aryklassptr()->is_null_free() && superk->is_aryklassptr()->elem()->isa_instklassptr() &&\n+        superk->is_aryklassptr()->elem()->is_instklassptr()->instance_klass()->is_inlinetype()) {\n+      return SSC_full_test;\n+    }\n@@ -5012,0 +5578,21 @@\n+Node* Compile::optimize_acmp(PhaseGVN* phase, Node* a, Node* b) {\n+  const TypeInstPtr* ta = phase->type(a)->isa_instptr();\n+  const TypeInstPtr* tb = phase->type(b)->isa_instptr();\n+  if (!EnableValhalla || ta == nullptr || tb == nullptr ||\n+      ta->is_zero_type() || tb->is_zero_type() ||\n+      !ta->can_be_inline_type() || !tb->can_be_inline_type()) {\n+    \/\/ Use old acmp if one operand is null or not an inline type\n+    return new CmpPNode(a, b);\n+  } else if (ta->is_inlinetypeptr() || tb->is_inlinetypeptr()) {\n+    \/\/ We know that one operand is an inline type. Therefore,\n+    \/\/ new acmp will only return true if both operands are nullptr.\n+    \/\/ Check if both operands are null by or'ing the oops.\n+    a = phase->transform(new CastP2XNode(nullptr, a));\n+    b = phase->transform(new CastP2XNode(nullptr, b));\n+    a = phase->transform(new OrXNode(a, b));\n+    return new CmpXNode(a, phase->MakeConX(0));\n+  }\n+  \/\/ Use new acmp\n+  return nullptr;\n+}\n+\n@@ -5367,0 +5954,2 @@\n+  } else if (bt == T_FLOAT) {\n+    result = new MoveI2FNode(value);\n","filename":"src\/hotspot\/share\/opto\/compile.cpp","additions":650,"deletions":61,"binary":false,"changes":711,"status":"modified"},{"patch":"@@ -48,0 +48,1 @@\n+#include \"utilities\/growableArray.hpp\"\n@@ -56,0 +57,1 @@\n+class CallNode;\n@@ -99,0 +101,1 @@\n+class InlineTypeNode;\n@@ -344,0 +347,1 @@\n+  bool                  _has_circular_inline_type; \/\/ True if method loads an inline type with a circular, non-flat field\n@@ -370,0 +374,3 @@\n+  bool                  _has_flat_accesses;     \/\/ Any known flat array accesses?\n+  bool                  _flat_accesses_share_alias; \/\/ Initially all flat array share a single slice\n+  bool                  _scalarize_in_safepoints; \/\/ Scalarize inline types in safepoint debug info\n@@ -388,0 +395,2 @@\n+  GrowableArray<Node*>  _inline_type_nodes;     \/\/ List of InlineType nodes\n+  GrowableArray<Node*>  _flat_access_nodes;     \/\/ List of LoadFlat and StoreFlat nodes\n@@ -623,0 +632,2 @@\n+  bool              has_circular_inline_type() const { return _has_circular_inline_type; }\n+  void          set_has_circular_inline_type(bool z) { _has_circular_inline_type = z; }\n@@ -655,0 +666,10 @@\n+  void          set_flat_accesses()              { _has_flat_accesses = true; }\n+  bool          flat_accesses_share_alias() const { return _flat_accesses_share_alias; }\n+  void          set_flat_accesses_share_alias(bool z) { _flat_accesses_share_alias = z; }\n+  bool          scalarize_in_safepoints() const { return _scalarize_in_safepoints; }\n+  void          set_scalarize_in_safepoints(bool z) { _scalarize_in_safepoints = z; }\n+\n+  \/\/ Support for scalarized inline type calling convention\n+  bool              has_scalarized_args() const  { return _method != nullptr && _method->has_scalarized_args(); }\n+  bool              needs_stack_repair()  const  { return _method != nullptr && _method->get_Method()->c2_needs_stack_repair(); }\n+\n@@ -786,0 +807,18 @@\n+  \/\/ Keep track of inline type nodes for later processing\n+  void add_inline_type(Node* n);\n+  void remove_inline_type(Node* n);\n+  void process_inline_types(PhaseIterGVN &igvn, bool remove = false);\n+\n+  void add_flat_access(Node* n);\n+  void remove_flat_access(Node* n);\n+  void process_flat_accesses(PhaseIterGVN& igvn);\n+\n+  template <class F>\n+  void for_each_flat_access(F consumer) {\n+    for (int i = _flat_access_nodes.length() - 1; i >= 0; i--) {\n+      consumer(_flat_access_nodes.at(i));\n+    }\n+  }\n+\n+  void adjust_flat_array_access_aliases(PhaseIterGVN& igvn);\n+\n@@ -968,1 +1007,1 @@\n-  AliasType*        alias_type(const TypePtr* adr_type, ciField* field = nullptr) { return find_alias_type(adr_type, false, field); }\n+  AliasType*        alias_type(const TypePtr* adr_type, ciField* field = nullptr, bool uncached = false) { return find_alias_type(adr_type, false, field, uncached); }\n@@ -972,1 +1011,1 @@\n-  int               get_alias_index(const TypePtr* at)  { return alias_type(at)->index(); }\n+  int               get_alias_index(const TypePtr* at, bool uncached = false) { return alias_type(at, nullptr, uncached)->index(); }\n@@ -1214,1 +1253,1 @@\n-  AliasType* find_alias_type(const TypePtr* adr_type, bool no_create, ciField* field);\n+  AliasType* find_alias_type(const TypePtr* adr_type, bool no_create, ciField* field, bool uncached = false);\n@@ -1304,1 +1343,3 @@\n-  \/\/ Auxiliary methods for randomized fuzzing\/stressing\n+  Node* optimize_acmp(PhaseGVN* phase, Node* a, Node* b);\n+\n+  \/\/ Auxiliary method for randomized fuzzing\/stressing\n","filename":"src\/hotspot\/share\/opto\/compile.hpp","additions":45,"deletions":4,"binary":false,"changes":49,"status":"modified"},{"patch":"@@ -28,0 +28,1 @@\n+#include \"classfile\/vmIntrinsics.hpp\"\n@@ -32,0 +33,1 @@\n+#include \"jvm_io.h\"\n@@ -40,0 +42,1 @@\n+#include \"opto\/inlinetypenode.hpp\"\n@@ -89,0 +92,57 @@\n+static bool arg_can_be_larval(ciMethod* callee, int arg_idx) {\n+  if (callee->is_object_constructor() && arg_idx == 0) {\n+    return true;\n+  }\n+\n+  if (arg_idx != 1 || callee->intrinsic_id() == vmIntrinsicID::_none) {\n+    return false;\n+  }\n+\n+  switch (callee->intrinsic_id()) {\n+    case vmIntrinsicID::_finishPrivateBuffer:\n+    case vmIntrinsicID::_putBoolean:\n+    case vmIntrinsicID::_putBooleanOpaque:\n+    case vmIntrinsicID::_putBooleanRelease:\n+    case vmIntrinsicID::_putBooleanVolatile:\n+    case vmIntrinsicID::_putByte:\n+    case vmIntrinsicID::_putByteOpaque:\n+    case vmIntrinsicID::_putByteRelease:\n+    case vmIntrinsicID::_putByteVolatile:\n+    case vmIntrinsicID::_putChar:\n+    case vmIntrinsicID::_putCharOpaque:\n+    case vmIntrinsicID::_putCharRelease:\n+    case vmIntrinsicID::_putCharUnaligned:\n+    case vmIntrinsicID::_putCharVolatile:\n+    case vmIntrinsicID::_putShort:\n+    case vmIntrinsicID::_putShortOpaque:\n+    case vmIntrinsicID::_putShortRelease:\n+    case vmIntrinsicID::_putShortUnaligned:\n+    case vmIntrinsicID::_putShortVolatile:\n+    case vmIntrinsicID::_putInt:\n+    case vmIntrinsicID::_putIntOpaque:\n+    case vmIntrinsicID::_putIntRelease:\n+    case vmIntrinsicID::_putIntUnaligned:\n+    case vmIntrinsicID::_putIntVolatile:\n+    case vmIntrinsicID::_putLong:\n+    case vmIntrinsicID::_putLongOpaque:\n+    case vmIntrinsicID::_putLongRelease:\n+    case vmIntrinsicID::_putLongUnaligned:\n+    case vmIntrinsicID::_putLongVolatile:\n+    case vmIntrinsicID::_putFloat:\n+    case vmIntrinsicID::_putFloatOpaque:\n+    case vmIntrinsicID::_putFloatRelease:\n+    case vmIntrinsicID::_putFloatVolatile:\n+    case vmIntrinsicID::_putDouble:\n+    case vmIntrinsicID::_putDoubleOpaque:\n+    case vmIntrinsicID::_putDoubleRelease:\n+    case vmIntrinsicID::_putDoubleVolatile:\n+    case vmIntrinsicID::_putReference:\n+    case vmIntrinsicID::_putReferenceOpaque:\n+    case vmIntrinsicID::_putReferenceRelease:\n+    case vmIntrinsicID::_putReferenceVolatile:\n+      return true;\n+    default:\n+      return false;\n+  }\n+}\n+\n@@ -149,1 +209,15 @@\n-  if (allow_inline && allow_intrinsics) {\n+  if (callee->intrinsic_id() == vmIntrinsics::_makePrivateBuffer || callee->intrinsic_id() == vmIntrinsics::_finishPrivateBuffer) {\n+    \/\/ These methods must be inlined so that we don't have larval value objects crossing method\n+    \/\/ boundaries\n+    assert(!call_does_dispatch, \"callee should not be virtual %s\", callee->name()->as_utf8());\n+    CallGenerator* cg = find_intrinsic(callee, call_does_dispatch);\n+\n+    if (cg == nullptr) {\n+      \/\/ This is probably because the intrinsics is disabled from the command line\n+      char reason[256];\n+      jio_snprintf(reason, sizeof(reason), \"cannot find an intrinsics for %s\", callee->name()->as_utf8());\n+      C->record_method_not_compilable(reason);\n+      return nullptr;\n+    }\n+    return cg;\n+  } else if (allow_inline && allow_intrinsics) {\n@@ -617,1 +691,1 @@\n-  if (iter().cur_bc_raw() == Bytecodes::_invokespecial && !orig_callee->is_object_initializer()) {\n+  if (iter().cur_bc_raw() == Bytecodes::_invokespecial && !orig_callee->is_object_constructor()) {\n@@ -645,0 +719,9 @@\n+  \/\/ Scalarize value objects passed into this invocation if we know that they are not larval\n+  for (int arg_idx = 0; arg_idx < nargs; arg_idx++) {\n+    if (arg_can_be_larval(callee, arg_idx)) {\n+      continue;\n+    }\n+\n+    cast_to_non_larval(peek(nargs - 1 - arg_idx));\n+  }\n+\n@@ -659,0 +742,4 @@\n+  if (failing()) {\n+    return;\n+  }\n+  assert(cg != nullptr, \"must find a CallGenerator for callee %s\", callee->name()->as_utf8());\n@@ -745,1 +832,1 @@\n-          \/\/ It's OK for a method  to return a value that is discarded.\n+          \/\/ It's OK for a method to return a value that is discarded.\n@@ -803,0 +890,21 @@\n+\n+    if (!rtype->is_void() && cg->method()->intrinsic_id() != vmIntrinsicID::_makePrivateBuffer) {\n+      Node* retnode = peek();\n+      const Type* rettype = gvn().type(retnode);\n+      if (rettype->is_inlinetypeptr() && !retnode->is_InlineType()) {\n+        retnode = InlineTypeNode::make_from_oop(this, retnode, rettype->inline_klass());\n+        dec_sp(1);\n+        push(retnode);\n+      }\n+    }\n+\n+    if (cg->method()->is_object_constructor() && receiver != nullptr && gvn().type(receiver)->is_inlinetypeptr()) {\n+      InlineTypeNode* non_larval = InlineTypeNode::make_from_oop(this, receiver, gvn().type(receiver)->inline_klass());\n+      \/\/ Relinquish the oop input, we will delay the allocation to the point it is needed, see the\n+      \/\/ comments in InlineTypeNode::Ideal for more details\n+      non_larval = non_larval->clone_if_required(&gvn(), nullptr);\n+      non_larval->set_oop(gvn(), null());\n+      non_larval->set_is_buffered(gvn(), false);\n+      non_larval = gvn().transform(non_larval)->as_InlineType();\n+      map()->replace_edge(receiver, non_larval);\n+    }\n","filename":"src\/hotspot\/share\/opto\/doCall.cpp","additions":111,"deletions":3,"binary":false,"changes":114,"status":"modified"},{"patch":"@@ -26,0 +26,3 @@\n+#include \"ci\/ciArrayKlass.hpp\"\n+#include \"ci\/ciFlatArrayKlass.hpp\"\n+#include \"ci\/ciInstanceKlass.hpp\"\n@@ -32,0 +35,1 @@\n+#include \"gc\/shared\/c2\/barrierSetC2.hpp\"\n@@ -34,0 +38,1 @@\n+#include \"oops\/accessDecorators.hpp\"\n@@ -35,0 +40,1 @@\n+#include \"oops\/layoutKind.hpp\"\n@@ -43,0 +49,1 @@\n+#include \"opto\/graphKit.hpp\"\n@@ -44,0 +51,1 @@\n+#include \"opto\/inlinetypenode.hpp\"\n@@ -49,0 +57,1 @@\n+#include \"opto\/opcodes.hpp\"\n@@ -53,0 +62,1 @@\n+#include \"opto\/type.hpp\"\n@@ -62,0 +72,1 @@\n+#include \"utilities\/globalDefinitions.hpp\"\n@@ -322,0 +333,2 @@\n+  case vmIntrinsics::_makePrivateBuffer:        return inline_unsafe_make_private_buffer();\n+  case vmIntrinsics::_finishPrivateBuffer:      return inline_unsafe_finish_private_buffer();\n@@ -412,0 +425,3 @@\n+  case vmIntrinsics::_getFlatValue:             return inline_unsafe_flat_access(!is_store, Relaxed);\n+  case vmIntrinsics::_putFlatValue:             return inline_unsafe_flat_access( is_store, Relaxed);\n+\n@@ -471,0 +487,4 @@\n+  case vmIntrinsics::_arrayInstanceBaseOffset:  return inline_arrayInstanceBaseOffset();\n+  case vmIntrinsics::_arrayInstanceIndexScale:  return inline_arrayInstanceIndexScale();\n+  case vmIntrinsics::_arrayLayout:              return inline_arrayLayout();\n+\n@@ -519,0 +539,6 @@\n+  case vmIntrinsics::_newNullRestrictedNonAtomicArray: return inline_newArray(\/* null_free *\/ true, \/* atomic *\/ false);\n+  case vmIntrinsics::_newNullRestrictedAtomicArray: return inline_newArray(\/* null_free *\/ true, \/* atomic *\/ true);\n+  case vmIntrinsics::_newNullableAtomicArray:     return inline_newArray(\/* null_free *\/ false, \/* atomic *\/ true);\n+  case vmIntrinsics::_isFlatArray:              return inline_getArrayProperties(IsFlat);\n+  case vmIntrinsics::_isNullRestrictedArray:    return inline_getArrayProperties(IsNullRestricted);\n+  case vmIntrinsics::_isAtomicArray:            return inline_getArrayProperties(IsAtomic);\n@@ -2336,0 +2362,1 @@\n+  bool null_free = false;\n@@ -2341,0 +2368,1 @@\n+      null_free = alias_type->field()->is_null_free();\n@@ -2347,1 +2375,1 @@\n-    if (adr_type->offset() >= objArrayOopDesc::base_offset_in_bytes()) {\n+    if (adr_type->offset() >= refArrayOopDesc::base_offset_in_bytes()) {\n@@ -2349,0 +2377,1 @@\n+      null_free = adr_type->is_aryptr()->is_null_free();\n@@ -2361,0 +2390,3 @@\n+    if (null_free) {\n+      result = result->join_speculative(TypePtr::NOTNULL)->is_oopptr();\n+    }\n@@ -2489,0 +2521,36 @@\n+\n+  if (base->is_InlineType()) {\n+    assert(!is_store, \"InlineTypeNodes are non-larval value objects\");\n+    InlineTypeNode* vt = base->as_InlineType();\n+    if (offset->is_Con()) {\n+      long off = find_long_con(offset, 0);\n+      ciInlineKlass* vk = vt->type()->inline_klass();\n+      if ((long)(int)off != off || !vk->contains_field_offset(off)) {\n+        return false;\n+      }\n+\n+      ciField* field = vk->get_non_flat_field_by_offset(off);\n+      if (field != nullptr) {\n+        BasicType bt = type2field[field->type()->basic_type()];\n+        if (bt == T_ARRAY || bt == T_NARROWOOP) {\n+          bt = T_OBJECT;\n+        }\n+        if (bt == type && !field->is_flat()) {\n+          Node* value = vt->field_value_by_offset(off, false);\n+          if (value->is_InlineType()) {\n+            value = value->as_InlineType()->adjust_scalarization_depth(this);\n+          }\n+          set_result(value);\n+          return true;\n+        }\n+      }\n+    }\n+    {\n+      \/\/ Re-execute the unsafe access if allocation triggers deoptimization.\n+      PreserveReexecuteState preexecs(this);\n+      jvms()->set_should_reexecute(true);\n+      vt = vt->buffer(this);\n+    }\n+    base = vt->get_oop();\n+  }\n+\n@@ -2532,1 +2600,29 @@\n-  BasicType bt = alias_type->basic_type();\n+  BasicType bt = T_ILLEGAL;\n+  ciField* field = nullptr;\n+  if (adr_type->isa_instptr()) {\n+    const TypeInstPtr* instptr = adr_type->is_instptr();\n+    ciInstanceKlass* k = instptr->instance_klass();\n+    int off = instptr->offset();\n+    if (instptr->const_oop() != nullptr &&\n+        k == ciEnv::current()->Class_klass() &&\n+        instptr->offset() >= (k->size_helper() * wordSize)) {\n+      k = instptr->const_oop()->as_instance()->java_lang_Class_klass()->as_instance_klass();\n+      field = k->get_field_by_offset(off, true);\n+    } else {\n+      field = k->get_non_flat_field_by_offset(off);\n+    }\n+    if (field != nullptr) {\n+      bt = type2field[field->type()->basic_type()];\n+    }\n+    if (bt != alias_type->basic_type()) {\n+      \/\/ Type mismatch. Is it an access to a nested flat field?\n+      field = k->get_field_by_offset(off, false);\n+      if (field != nullptr) {\n+        bt = type2field[field->type()->basic_type()];\n+      }\n+    }\n+    assert(bt == alias_type->basic_type(), \"should match\");\n+  } else {\n+    bt = alias_type->basic_type();\n+  }\n+\n@@ -2566,4 +2662,6 @@\n-  if (!is_store && type == T_OBJECT) {\n-    const TypeOopPtr* tjp = sharpen_unsafe_type(alias_type, adr_type);\n-    if (tjp != nullptr) {\n-      value_type = tjp;\n+  if (!is_store) {\n+    if (type == T_OBJECT) {\n+      const TypeOopPtr* tjp = sharpen_unsafe_type(alias_type, adr_type);\n+      if (tjp != nullptr) {\n+        value_type = tjp;\n+      }\n@@ -2585,2 +2683,2 @@\n-    ciField* field = alias_type->field();\n-    if (heap_base_oop != top() && field != nullptr && field->is_constant() && !mismatched) {\n+\n+    if (heap_base_oop != top() && field != nullptr && field->is_constant() && !field->is_flat() && !mismatched) {\n@@ -2593,0 +2691,5 @@\n+      const TypeOopPtr* ptr = value_type->make_oopptr();\n+      if (ptr != nullptr && ptr->is_inlinetypeptr()) {\n+        \/\/ Load a non-flattened inline type from memory\n+        p = InlineTypeNode::make_from_oop(this, p, ptr->inline_klass());\n+      }\n@@ -2636,0 +2739,235 @@\n+bool LibraryCallKit::inline_unsafe_flat_access(bool is_store, AccessKind kind) {\n+#ifdef ASSERT\n+  {\n+    ResourceMark rm;\n+    \/\/ Check the signatures.\n+    ciSignature* sig = callee()->signature();\n+    assert(sig->type_at(0)->basic_type() == T_OBJECT, \"base should be object, but is %s\", type2name(sig->type_at(0)->basic_type()));\n+    assert(sig->type_at(1)->basic_type() == T_LONG, \"offset should be long, but is %s\", type2name(sig->type_at(1)->basic_type()));\n+    assert(sig->type_at(2)->basic_type() == T_INT, \"layout kind should be int, but is %s\", type2name(sig->type_at(3)->basic_type()));\n+    assert(sig->type_at(3)->basic_type() == T_OBJECT, \"value klass should be object, but is %s\", type2name(sig->type_at(4)->basic_type()));\n+    if (is_store) {\n+      assert(sig->return_type()->basic_type() == T_VOID, \"putter must not return a value, but returns %s\", type2name(sig->return_type()->basic_type()));\n+      assert(sig->count() == 5, \"flat putter should have 5 arguments, but has %d\", sig->count());\n+      assert(sig->type_at(4)->basic_type() == T_OBJECT, \"put value should be object, but is %s\", type2name(sig->type_at(5)->basic_type()));\n+    } else {\n+      assert(sig->return_type()->basic_type() == T_OBJECT, \"getter must return an object, but returns %s\", type2name(sig->return_type()->basic_type()));\n+      assert(sig->count() == 4, \"flat getter should have 4 arguments, but has %d\", sig->count());\n+    }\n+ }\n+#endif \/\/ ASSERT\n+\n+  assert(kind == Relaxed, \"Only plain accesses for now\");\n+  if (callee()->is_static()) {\n+    \/\/ caller must have the capability!\n+    return false;\n+  }\n+  C->set_has_unsafe_access(true);\n+\n+  const TypeInstPtr* value_klass_node = _gvn.type(argument(5))->isa_instptr();\n+  if (value_klass_node == nullptr || value_klass_node->const_oop() == nullptr) {\n+    \/\/ parameter valueType is not a constant\n+    return false;\n+  }\n+  ciType* mirror_type = value_klass_node->const_oop()->as_instance()->java_mirror_type();\n+  if (!mirror_type->is_inlinetype()) {\n+    \/\/ Dead code\n+    return false;\n+  }\n+  ciInlineKlass* value_klass = mirror_type->as_inline_klass();\n+\n+  const TypeInt* layout_type = _gvn.type(argument(4))->isa_int();\n+  if (layout_type == nullptr || !layout_type->is_con()) {\n+    \/\/ parameter layoutKind is not a constant\n+    return false;\n+  }\n+  assert(layout_type->get_con() >= static_cast<int>(LayoutKind::REFERENCE) &&\n+         layout_type->get_con() <= static_cast<int>(LayoutKind::UNKNOWN),\n+         \"invalid layoutKind %d\", layout_type->get_con());\n+  LayoutKind layout = static_cast<LayoutKind>(layout_type->get_con());\n+  assert(layout == LayoutKind::REFERENCE || layout == LayoutKind::NON_ATOMIC_FLAT ||\n+         layout == LayoutKind::ATOMIC_FLAT || layout == LayoutKind::NULLABLE_ATOMIC_FLAT,\n+         \"unexpected layoutKind %d\", layout_type->get_con());\n+\n+  null_check(argument(0));\n+  if (stopped()) {\n+    return true;\n+  }\n+\n+  Node* base = must_be_not_null(argument(1), true);\n+  Node* offset = argument(2);\n+  const Type* base_type = _gvn.type(base);\n+\n+  Node* ptr;\n+  bool immutable_memory = false;\n+  DecoratorSet decorators = C2_UNSAFE_ACCESS | IN_HEAP | MO_UNORDERED;\n+  if (base_type->isa_instptr()) {\n+    const TypeLong* offset_type = _gvn.type(offset)->isa_long();\n+    if (offset_type == nullptr || !offset_type->is_con()) {\n+      \/\/ Offset into a non-array should be a constant\n+      decorators |= C2_MISMATCHED;\n+    } else {\n+      int offset_con = checked_cast<int>(offset_type->get_con());\n+      ciInstanceKlass* base_klass = base_type->is_instptr()->instance_klass();\n+      ciField* field = base_klass->get_non_flat_field_by_offset(offset_con);\n+      if (field == nullptr) {\n+        assert(!base_klass->is_final(), \"non-existence field at offset %d of class %s\", offset_con, base_klass->name()->as_utf8());\n+        decorators |= C2_MISMATCHED;\n+      } else {\n+        assert(field->type() == value_klass, \"field at offset %d of %s is of type %s, but valueType is %s\",\n+               offset_con, base_klass->name()->as_utf8(), field->type()->name(), value_klass->name()->as_utf8());\n+        immutable_memory = field->is_strict() && field->is_final();\n+\n+        if (base->is_InlineType()) {\n+          assert(!is_store, \"Cannot store into a non-larval value object\");\n+          set_result(base->as_InlineType()->field_value_by_offset(offset_con, false));\n+          return true;\n+        }\n+      }\n+    }\n+\n+    if (base->is_InlineType()) {\n+      assert(!is_store, \"Cannot store into a non-larval value object\");\n+      base = base->as_InlineType()->buffer(this, true);\n+    }\n+    ptr = basic_plus_adr(base, ConvL2X(offset));\n+  } else if (base_type->isa_aryptr()) {\n+    decorators |= IS_ARRAY;\n+    if (layout == LayoutKind::REFERENCE) {\n+      if (!base_type->is_aryptr()->is_not_flat()) {\n+        const TypeAryPtr* array_type = base_type->is_aryptr()->cast_to_not_flat();\n+        Node* new_base = _gvn.transform(new CastPPNode(control(), base, array_type, ConstraintCastNode::StrongDependency));\n+        replace_in_map(base, new_base);\n+        base = new_base;\n+      }\n+      ptr = basic_plus_adr(base, ConvL2X(offset));\n+    } else {\n+      if (UseArrayFlattening) {\n+        \/\/ Flat array must have an exact type\n+        bool is_null_free = !LayoutKindHelper::is_nullable_flat(layout);\n+        bool is_atomic = LayoutKindHelper::is_atomic_flat(layout);\n+        Node* new_base = cast_to_flat_array(base, value_klass, is_null_free, !is_null_free, is_atomic);\n+        replace_in_map(base, new_base);\n+        base = new_base;\n+        ptr = basic_plus_adr(base, ConvL2X(offset));\n+        const TypeAryPtr* ptr_type = _gvn.type(ptr)->is_aryptr();\n+        if (ptr_type->field_offset().get() != 0) {\n+          ptr = _gvn.transform(new CastPPNode(control(), ptr, ptr_type->with_field_offset(0), ConstraintCastNode::StrongDependency));\n+        }\n+      } else {\n+        uncommon_trap(Deoptimization::Reason_intrinsic,\n+                      Deoptimization::Action_none);\n+        return true;\n+      }\n+    }\n+  } else {\n+    decorators |= C2_MISMATCHED;\n+    ptr = basic_plus_adr(base, ConvL2X(offset));\n+  }\n+\n+  if (is_store) {\n+    Node* value = argument(6);\n+    const Type* value_type = _gvn.type(value);\n+    if (!value_type->is_inlinetypeptr()) {\n+      value_type = Type::get_const_type(value_klass)->filter_speculative(value_type);\n+      Node* new_value = _gvn.transform(new CastPPNode(control(), value, value_type, ConstraintCastNode::StrongDependency));\n+      new_value = InlineTypeNode::make_from_oop(this, new_value, value_klass);\n+      replace_in_map(value, new_value);\n+      value = new_value;\n+    }\n+\n+    assert(value_type->inline_klass() == value_klass, \"value is of type %s while valueType is %s\", value_type->inline_klass()->name()->as_utf8(), value_klass->name()->as_utf8());\n+    if (layout == LayoutKind::REFERENCE) {\n+      const TypePtr* ptr_type = (decorators & C2_MISMATCHED) != 0 ? TypeRawPtr::BOTTOM : _gvn.type(ptr)->is_ptr();\n+      access_store_at(base, ptr, ptr_type, value, value_type, T_OBJECT, decorators);\n+    } else {\n+      bool atomic = LayoutKindHelper::is_atomic_flat(layout);\n+      bool null_free = !LayoutKindHelper::is_nullable_flat(layout);\n+      value->as_InlineType()->store_flat(this, base, ptr, atomic, immutable_memory, null_free, decorators);\n+    }\n+\n+    return true;\n+  } else {\n+    decorators |= (C2_CONTROL_DEPENDENT_LOAD | C2_UNKNOWN_CONTROL_LOAD);\n+    InlineTypeNode* result;\n+    if (layout == LayoutKind::REFERENCE) {\n+      const TypePtr* ptr_type = (decorators & C2_MISMATCHED) != 0 ? TypeRawPtr::BOTTOM : _gvn.type(ptr)->is_ptr();\n+      Node* oop = access_load_at(base, ptr, ptr_type, Type::get_const_type(value_klass), T_OBJECT, decorators);\n+      result = InlineTypeNode::make_from_oop(this, oop, value_klass);\n+    } else {\n+      bool atomic = LayoutKindHelper::is_atomic_flat(layout);\n+      bool null_free = !LayoutKindHelper::is_nullable_flat(layout);\n+      result = InlineTypeNode::make_from_flat(this, value_klass, base, ptr, atomic, immutable_memory, null_free, decorators);\n+    }\n+\n+    set_result(result);\n+    return true;\n+  }\n+}\n+\n+bool LibraryCallKit::inline_unsafe_make_private_buffer() {\n+  Node* receiver = argument(0);\n+  Node* value = argument(1);\n+\n+  const Type* type = gvn().type(value);\n+  if (!type->is_inlinetypeptr()) {\n+    C->record_method_not_compilable(\"value passed to Unsafe::makePrivateBuffer is not of a constant value type\");\n+    return false;\n+  }\n+\n+  null_check(receiver);\n+  if (stopped()) {\n+    return true;\n+  }\n+\n+  value = null_check(value);\n+  if (stopped()) {\n+    return true;\n+  }\n+\n+  ciInlineKlass* vk = type->inline_klass();\n+  Node* klass = makecon(TypeKlassPtr::make(vk));\n+  Node* obj = new_instance(klass);\n+  AllocateNode::Ideal_allocation(obj)->_larval = true;\n+\n+  assert(value->is_InlineType(), \"must be an InlineTypeNode\");\n+  Node* payload_ptr = basic_plus_adr(obj, vk->payload_offset());\n+  value->as_InlineType()->store_flat(this, obj, payload_ptr, false, true, true, IN_HEAP | MO_UNORDERED);\n+\n+  set_result(obj);\n+  return true;\n+}\n+\n+bool LibraryCallKit::inline_unsafe_finish_private_buffer() {\n+  Node* receiver = argument(0);\n+  Node* buffer = argument(1);\n+\n+  const Type* type = gvn().type(buffer);\n+  if (!type->is_inlinetypeptr()) {\n+    C->record_method_not_compilable(\"value passed to Unsafe::finishPrivateBuffer is not of a constant value type\");\n+    return false;\n+  }\n+\n+  AllocateNode* alloc = AllocateNode::Ideal_allocation(buffer);\n+  if (alloc == nullptr) {\n+    C->record_method_not_compilable(\"value passed to Unsafe::finishPrivateBuffer must be allocated by Unsafe::makePrivateBuffer\");\n+    return false;\n+  }\n+\n+  null_check(receiver);\n+  if (stopped()) {\n+    return true;\n+  }\n+\n+  \/\/ Unset the larval bit in the object header\n+  Node* old_header = make_load(control(), buffer, TypeX_X, TypeX_X->basic_type(), MemNode::unordered, LoadNode::Pinned);\n+  Node* new_header = gvn().transform(new AndXNode(old_header, MakeConX(~markWord::larval_bit_in_place)));\n+  access_store_at(buffer, buffer, type->is_ptr(), new_header, TypeX_X, TypeX_X->basic_type(), MO_UNORDERED | IN_HEAP);\n+\n+  \/\/ We must ensure that the buffer is properly published\n+  insert_mem_bar(Op_MemBarStoreStore, alloc->proj_out(AllocateNode::RawAddress));\n+  assert(!type->maybe_null(), \"result of an allocation should not be null\");\n+  set_result(InlineTypeNode::make_from_oop(this, buffer, type->inline_klass()));\n+  return true;\n+}\n+\n@@ -2838,0 +3176,13 @@\n+    if (oldval != nullptr && oldval->is_InlineType()) {\n+      \/\/ Re-execute the unsafe access if allocation triggers deoptimization.\n+      PreserveReexecuteState preexecs(this);\n+      jvms()->set_should_reexecute(true);\n+      oldval = oldval->as_InlineType()->buffer(this)->get_oop();\n+    }\n+    if (newval != nullptr && newval->is_InlineType()) {\n+      \/\/ Re-execute the unsafe access if allocation triggers deoptimization.\n+      PreserveReexecuteState preexecs(this);\n+      jvms()->set_should_reexecute(true);\n+      newval = newval->as_InlineType()->buffer(this)->get_oop();\n+    }\n+\n@@ -2906,0 +3257,72 @@\n+\/\/ private native int arrayInstanceBaseOffset0(Object[] array);\n+bool LibraryCallKit::inline_arrayInstanceBaseOffset() {\n+  Node* array = argument(1);\n+  Node* klass_node = load_object_klass(array);\n+\n+  jint  layout_con = Klass::_lh_neutral_value;\n+  Node* layout_val = get_layout_helper(klass_node, layout_con);\n+  int   layout_is_con = (layout_val == nullptr);\n+\n+  Node* header_size = nullptr;\n+  if (layout_is_con) {\n+    int hsize = Klass::layout_helper_header_size(layout_con);\n+    header_size = intcon(hsize);\n+  } else {\n+    Node* hss = intcon(Klass::_lh_header_size_shift);\n+    Node* hsm = intcon(Klass::_lh_header_size_mask);\n+    header_size = _gvn.transform(new URShiftINode(layout_val, hss));\n+    header_size = _gvn.transform(new AndINode(header_size, hsm));\n+  }\n+  set_result(header_size);\n+  return true;\n+}\n+\n+\/\/ private native int arrayInstanceIndexScale0(Object[] array);\n+bool LibraryCallKit::inline_arrayInstanceIndexScale() {\n+  Node* array = argument(1);\n+  Node* klass_node = load_object_klass(array);\n+\n+  jint  layout_con = Klass::_lh_neutral_value;\n+  Node* layout_val = get_layout_helper(klass_node, layout_con);\n+  int   layout_is_con = (layout_val == nullptr);\n+\n+  Node* element_size = nullptr;\n+  if (layout_is_con) {\n+    int log_element_size  = Klass::layout_helper_log2_element_size(layout_con);\n+    int elem_size = 1 << log_element_size;\n+    element_size = intcon(elem_size);\n+  } else {\n+    Node* ess = intcon(Klass::_lh_log2_element_size_shift);\n+    Node* esm = intcon(Klass::_lh_log2_element_size_mask);\n+    Node* log_element_size = _gvn.transform(new URShiftINode(layout_val, ess));\n+    log_element_size = _gvn.transform(new AndINode(log_element_size, esm));\n+    element_size = _gvn.transform(new LShiftINode(intcon(1), log_element_size));\n+  }\n+  set_result(element_size);\n+  return true;\n+}\n+\n+\/\/ private native int arrayLayout0(Object[] array);\n+bool LibraryCallKit::inline_arrayLayout() {\n+  RegionNode* region = new RegionNode(2);\n+  Node* phi = new PhiNode(region, TypeInt::POS);\n+\n+  Node* array = argument(1);\n+  Node* klass_node = load_object_klass(array);\n+  generate_refArray_guard(klass_node, region);\n+  if (region->req() == 3) {\n+    phi->add_req(intcon((jint)LayoutKind::REFERENCE));\n+  }\n+\n+  int layout_kind_offset = in_bytes(FlatArrayKlass::layout_kind_offset());\n+  Node* layout_kind_addr = basic_plus_adr(klass_node, klass_node, layout_kind_offset);\n+  Node* layout_kind = make_load(nullptr, layout_kind_addr, TypeInt::POS, T_INT, MemNode::unordered);\n+\n+  region->init_req(1, control());\n+  phi->init_req(1, layout_kind);\n+\n+  set_control(_gvn.transform(region));\n+  set_result(_gvn.transform(phi));\n+  return true;\n+}\n+\n@@ -3024,2 +3447,7 @@\n-\n-  Node* obj = new_instance(kls, test);\n+  Node* obj = nullptr;\n+  const TypeInstKlassPtr* tkls = _gvn.type(kls)->isa_instklassptr();\n+  if (tkls != nullptr && tkls->instance_klass()->is_inlinetype()) {\n+    obj = InlineTypeNode::make_all_zero(_gvn, tkls->instance_klass()->as_inline_klass())->buffer(this);\n+  } else {\n+    obj = new_instance(kls, test);\n+  }\n@@ -3840,1 +4268,1 @@\n-  const TypeAry* arr0 = TypeAry::make(etype, TypeInt::POS);\n+  const TypeAry* arr0 = TypeAry::make(etype, TypeInt::POS, \/* stable= *\/ false, \/* flat= *\/ false, \/* not_flat= *\/ true, \/* not_null_free= *\/ true);\n@@ -3845,1 +4273,1 @@\n-  const Type* objects_type = TypeAryPtr::make(TypePtr::BotPTR, arr0, objects_klass, xk, 0);\n+  const Type* objects_type = TypeAryPtr::make(TypePtr::BotPTR, arr0, objects_klass, xk, TypeAryPtr::Offset(0));\n@@ -3969,9 +4397,0 @@\n-\/\/---------------------------load_mirror_from_klass----------------------------\n-\/\/ Given a klass oop, load its java mirror (a java.lang.Class oop).\n-Node* LibraryCallKit::load_mirror_from_klass(Node* klass) {\n-  Node* p = basic_plus_adr(klass, in_bytes(Klass::java_mirror_offset()));\n-  Node* load = make_load(nullptr, p, TypeRawPtr::NOTNULL, T_ADDRESS, MemNode::unordered);\n-  \/\/ mirror = ((OopHandle)mirror)->resolve();\n-  return access_load(load, TypeInstPtr::MIRROR, T_OBJECT, IN_NATIVE);\n-}\n-\n@@ -4021,0 +4440,1 @@\n+\n@@ -4142,10 +4562,12 @@\n-    p = basic_plus_adr(kls, in_bytes(Klass::super_offset()));\n-    kls = _gvn.transform(LoadKlassNode::make(_gvn, immutable_memory(), p, TypeRawPtr::BOTTOM, TypeInstKlassPtr::OBJECT_OR_NULL));\n-    null_ctl = top();\n-    kls = null_check_oop(kls, &null_ctl);\n-    if (null_ctl != top()) {\n-      \/\/ If the guard is taken, Object.superClass is null (both klass and mirror).\n-      region->add_req(null_ctl);\n-      phi   ->add_req(null());\n-    }\n-      query_value = load_mirror_from_klass(kls);\n+      p = basic_plus_adr(kls, in_bytes(Klass::super_offset()));\n+      kls = _gvn.transform(LoadKlassNode::make(_gvn, immutable_memory(), p, TypeRawPtr::BOTTOM, TypeInstKlassPtr::OBJECT_OR_NULL));\n+      null_ctl = top();\n+      kls = null_check_oop(kls, &null_ctl);\n+      if (null_ctl != top()) {\n+        \/\/ If the guard is taken, Object.superClass is null (both klass and mirror).\n+        region->add_req(null_ctl);\n+        phi   ->add_req(null());\n+      }\n+      if (!stopped()) {\n+        query_value = load_mirror_from_klass(kls);\n+      }\n@@ -4170,0 +4592,1 @@\n+\n@@ -4192,1 +4615,2 @@\n-      int static_res = C->static_subtype_check(TypeKlassPtr::make(tm->as_klass(), Type::trust_interfaces), tp->as_klass_type());\n+      const TypeKlassPtr* tklass = TypeKlassPtr::make(tm->as_klass(), Type::trust_interfaces);\n+      int static_res = C->static_subtype_check(tklass, tp->as_klass_type());\n@@ -4221,2 +4645,2 @@\n-  \/\/ Not-subtype or the mirror's klass ptr is null (in case it is a primitive).\n-  enum { _bad_type_path = 1, _prim_path = 2, PATH_LIMIT };\n+  \/\/ Not-subtype or the mirror's klass ptr is nullptr (in case it is a primitive).\n+  enum { _bad_type_path = 1, _prim_path = 2, _npe_path = 3, PATH_LIMIT };\n@@ -4232,0 +4656,2 @@\n+  Node* io = i_o();\n+  Node* mem = merged_memory();\n@@ -4233,0 +4659,1 @@\n+\n@@ -4239,1 +4666,2 @@\n-      region->in(_bad_type_path) != top()) {\n+      region->in(_bad_type_path) != top() ||\n+      region->in(_npe_path) != top()) {\n@@ -4243,0 +4671,3 @@\n+    \/\/ Set IO and memory because gen_checkcast may override them when buffering inline types\n+    set_i_o(io);\n+    set_all_memory(mem);\n@@ -4276,0 +4707,1 @@\n+  RegionNode* prim_region = new RegionNode(2);\n@@ -4278,0 +4710,1 @@\n+  record_for_igvn(prim_region);\n@@ -4302,2 +4735,5 @@\n-    int prim_path = (which_arg == 0 ? _prim_0_path : _prim_1_path);\n-    region->init_req(prim_path, null_ctl);\n+    if (which_arg == 0) {\n+      prim_region->init_req(1, null_ctl);\n+    } else {\n+      region->init_req(_prim_1_path, null_ctl);\n+    }\n@@ -4313,1 +4749,0 @@\n-    \/\/ now we have a successful reference subtype check\n@@ -4320,1 +4755,2 @@\n-  set_control(region->in(_prim_0_path)); \/\/ go back to first null check\n+  \/\/ This path is also used if superc is a value mirror.\n+  set_control(_gvn.transform(prim_region));\n@@ -4325,1 +4761,1 @@\n-    generate_guard(bol_eq, region, PROB_FAIR);\n+    generate_fair_guard(bol_eq, region);\n@@ -4356,2 +4792,1 @@\n-Node* LibraryCallKit::generate_array_guard_common(Node* kls, RegionNode* region,\n-                                                  bool obj_array, bool not_array, Node** obj) {\n+Node* LibraryCallKit::generate_array_guard_common(Node* kls, RegionNode* region, ArrayKind kind, Node** obj) {\n@@ -4363,9 +4798,0 @@\n-  \/\/ If obj_array\/non_array==false\/false:\n-  \/\/ Branch around if the given klass is in fact an array (either obj or prim).\n-  \/\/ If obj_array\/non_array==false\/true:\n-  \/\/ Branch around if the given klass is not an array klass of any kind.\n-  \/\/ If obj_array\/non_array==true\/true:\n-  \/\/ Branch around if the kls is not an oop array (kls is int[], String, etc.)\n-  \/\/ If obj_array\/non_array==true\/false:\n-  \/\/ Branch around if the kls is an oop array (Object[] or subtype)\n-  \/\/\n@@ -4376,4 +4802,11 @@\n-    bool query = (obj_array\n-                  ? Klass::layout_helper_is_objArray(layout_con)\n-                  : Klass::layout_helper_is_array(layout_con));\n-    if (query == not_array) {\n+    bool query = 0;\n+    switch(kind) {\n+      case RefArray:       query = Klass::layout_helper_is_refArray(layout_con); break;\n+      case NonRefArray:    query = !Klass::layout_helper_is_refArray(layout_con); break;\n+      case TypeArray:      query = Klass::layout_helper_is_typeArray(layout_con); break;\n+      case AnyArray:       query = Klass::layout_helper_is_array(layout_con); break;\n+      case NonArray:       query = !Klass::layout_helper_is_array(layout_con); break;\n+      default:\n+        ShouldNotReachHere();\n+    }\n+    if (!query) {\n@@ -4389,0 +4822,21 @@\n+  unsigned int value = 0;\n+  BoolTest::mask btest = BoolTest::illegal;\n+  switch(kind) {\n+    case RefArray:\n+    case NonRefArray: {\n+      value = Klass::_lh_array_tag_ref_value;\n+      layout_val = _gvn.transform(new RShiftINode(layout_val, intcon(Klass::_lh_array_tag_shift)));\n+      btest = (kind == RefArray) ? BoolTest::eq : BoolTest::ne;\n+      break;\n+    }\n+    case TypeArray: {\n+      value = Klass::_lh_array_tag_type_value;\n+      layout_val = _gvn.transform(new RShiftINode(layout_val, intcon(Klass::_lh_array_tag_shift)));\n+      btest = BoolTest::eq;\n+      break;\n+    }\n+    case AnyArray:    value = Klass::_lh_neutral_value; btest = BoolTest::lt; break;\n+    case NonArray:    value = Klass::_lh_neutral_value; btest = BoolTest::gt; break;\n+    default:\n+      ShouldNotReachHere();\n+  }\n@@ -4390,4 +4844,1 @@\n-  jint  nval = (obj_array\n-                ? (jint)(Klass::_lh_array_tag_type_value\n-                   <<    Klass::_lh_array_tag_shift)\n-                : Klass::_lh_neutral_value);\n+  jint nval = (jint)value;\n@@ -4395,3 +4846,0 @@\n-  BoolTest::mask btest = BoolTest::lt;  \/\/ correct for testing is_[obj]array\n-  \/\/ invert the test if we are looking for a non-array\n-  if (not_array)  btest = BoolTest(btest).negate();\n@@ -4400,1 +4848,1 @@\n-  Node* is_array_ctrl = not_array ? control() : ctrl;\n+  Node* is_array_ctrl = kind == NonArray ? control() : ctrl;\n@@ -4409,0 +4857,131 @@\n+\/\/ public static native Object[] ValueClass::newNullRestrictedAtomicArray(Class<?> componentType, int length, Object initVal);\n+\/\/ public static native Object[] ValueClass::newNullRestrictedNonAtomicArray(Class<?> componentType, int length, Object initVal);\n+\/\/ public static native Object[] ValueClass::newNullableAtomicArray(Class<?> componentType, int length);\n+bool LibraryCallKit::inline_newArray(bool null_free, bool atomic) {\n+  assert(null_free || atomic, \"nullable implies atomic\");\n+  Node* componentType = argument(0);\n+  Node* length = argument(1);\n+  Node* init_val = null_free ? argument(2) : nullptr;\n+\n+  const TypeInstPtr* tp = _gvn.type(componentType)->isa_instptr();\n+  if (tp != nullptr) {\n+    ciInstanceKlass* ik = tp->instance_klass();\n+    if (ik == C->env()->Class_klass()) {\n+      ciType* t = tp->java_mirror_type();\n+      if (t != nullptr && t->is_inlinetype()) {\n+\n+        ciArrayKlass* array_klass = ciArrayKlass::make(t, null_free, atomic, true);\n+        assert(array_klass->is_elem_null_free() == null_free, \"inconsistency\");\n+        assert(array_klass->is_elem_atomic() == atomic, \"inconsistency\");\n+\n+        \/\/ TOOD 8350865 ZGC needs card marks on initializing oop stores\n+        if (UseZGC && null_free && !array_klass->is_flat_array_klass()) {\n+          return false;\n+        }\n+\n+        if (array_klass->is_loaded() && array_klass->element_klass()->as_inline_klass()->is_initialized()) {\n+          const TypeAryKlassPtr* array_klass_type = TypeAryKlassPtr::make(array_klass, Type::trust_interfaces, true);\n+          if (null_free) {\n+            if (init_val->is_InlineType()) {\n+              if (array_klass_type->is_flat() && init_val->as_InlineType()->is_all_zero(&gvn(), \/* flat *\/ true)) {\n+                \/\/ Zeroing is enough because the init value is the all-zero value\n+                init_val = nullptr;\n+              } else {\n+                init_val = init_val->as_InlineType()->buffer(this);\n+              }\n+            }\n+            \/\/ TODO 8350865 Should we add a check of the init_val type (maybe in debug only + halt)?\n+          }\n+          Node* obj = new_array(makecon(array_klass_type), length, 0, nullptr, false, init_val);\n+          const TypeAryPtr* arytype = gvn().type(obj)->is_aryptr();\n+          assert(arytype->is_null_free() == null_free, \"inconsistency\");\n+          assert(arytype->is_not_null_free() == !null_free, \"inconsistency\");\n+          assert(arytype->is_atomic() == atomic, \"inconsistency\");\n+          set_result(obj);\n+          return true;\n+        }\n+      }\n+    }\n+  }\n+  return false;\n+}\n+\n+\/\/ public static native boolean ValueClass::isFlatArray(Object array);\n+\/\/ public static native boolean ValueClass::isNullRestrictedArray(Object array);\n+\/\/ public static native boolean ValueClass::isAtomicArray(Object array);\n+bool LibraryCallKit::inline_getArrayProperties(ArrayPropertiesCheck check) {\n+  Node* array = argument(0);\n+\n+  Node* bol;\n+  switch(check) {\n+    case IsFlat:\n+      \/\/ TODO 8350865 Use the object version here instead of loading the klass\n+      \/\/ The problem is that PhaseMacroExpand::expand_flatarraycheck_node can only handle some IR shapes and will fail, for example, if the bol is directly wired to a ReturnNode\n+      bol = flat_array_test(load_object_klass(array));\n+      break;\n+    case IsNullRestricted:\n+      bol = null_free_array_test(array);\n+      break;\n+    case IsAtomic:\n+      \/\/ TODO 8350865 Implement this. It's a bit more complicated, see conditions in JVM_IsAtomicArray\n+      \/\/ Enable TestIntrinsics::test87\/88 once this is implemented\n+      \/\/ bol = null_free_atomic_array_test\n+      return false;\n+    default:\n+      ShouldNotReachHere();\n+  }\n+\n+  Node* res = gvn().transform(new CMoveINode(bol, intcon(0), intcon(1), TypeInt::BOOL));\n+  set_result(res);\n+  return true;\n+}\n+\n+\/\/ Load the default refined array klass from an ObjArrayKlass. This relies on the first entry in the\n+\/\/ '_next_refined_array_klass' linked list being the default (see ObjArrayKlass::klass_with_properties).\n+Node* LibraryCallKit::load_default_refined_array_klass(Node* klass_node, bool type_array_guard) {\n+  RegionNode* region = new RegionNode(2);\n+  Node* phi = new PhiNode(region, TypeInstKlassPtr::OBJECT_OR_NULL);\n+\n+  if (type_array_guard) {\n+    generate_typeArray_guard(klass_node, region);\n+    if (region->req() == 3) {\n+      phi->add_req(klass_node);\n+    }\n+  }\n+  Node* adr_refined_klass = basic_plus_adr(klass_node, in_bytes(ObjArrayKlass::next_refined_array_klass_offset()));\n+  Node* refined_klass = _gvn.transform(LoadKlassNode::make(_gvn, immutable_memory(), adr_refined_klass, TypeRawPtr::BOTTOM, TypeInstKlassPtr::OBJECT_OR_NULL));\n+\n+  \/\/ Can be null if not initialized yet, just deopt\n+  Node* null_ctl = top();\n+  refined_klass = null_check_oop(refined_klass, &null_ctl, \/* never_see_null= *\/ true);\n+\n+  region->init_req(1, control());\n+  phi->init_req(1, refined_klass);\n+\n+  set_control(_gvn.transform(region));\n+  return _gvn.transform(phi);\n+}\n+\n+\/\/ Load the non-refined array klass from an ObjArrayKlass.\n+Node* LibraryCallKit::load_non_refined_array_klass(Node* klass_node) {\n+  const TypeAryKlassPtr* ary_klass_ptr = _gvn.type(klass_node)->isa_aryklassptr();\n+  if (ary_klass_ptr != nullptr && ary_klass_ptr->klass_is_exact()) {\n+    return _gvn.makecon(ary_klass_ptr->cast_to_refined_array_klass_ptr(false));\n+  }\n+\n+  RegionNode* region = new RegionNode(2);\n+  Node* phi = new PhiNode(region, TypeInstKlassPtr::OBJECT);\n+\n+  generate_typeArray_guard(klass_node, region);\n+  if (region->req() == 3) {\n+    phi->add_req(klass_node);\n+  }\n+  Node* super_adr = basic_plus_adr(klass_node, in_bytes(Klass::super_offset()));\n+  Node* super_klass = _gvn.transform(LoadKlassNode::make(_gvn, immutable_memory(), super_adr, TypeRawPtr::BOTTOM, TypeInstKlassPtr::OBJECT));\n+\n+  region->init_req(1, control());\n+  phi->init_req(1, super_klass);\n+\n+  set_control(_gvn.transform(region));\n+  return _gvn.transform(phi);\n+}\n@@ -4411,1 +4990,1 @@\n-\/\/ private static native Object java.lang.reflect.newArray(Class<?> componentType, int length);\n+\/\/ private static native Object java.lang.reflect.Array.newArray(Class<?> componentType, int length);\n@@ -4469,0 +5048,3 @@\n+\n+    klass_node = load_default_refined_array_klass(klass_node);\n+\n@@ -4557,1 +5139,16 @@\n-    Node* not_objArray = generate_non_objArray_guard(klass_node, bailout);\n+    \/\/ Inline type array may have object field that would require a\n+    \/\/ write barrier. Conservatively, go to slow path.\n+    \/\/ TODO 8251971: Optimize for the case when flat src\/dst are later found\n+    \/\/ to not contain oops (i.e., move this check to the macro expansion phase).\n+    BarrierSetC2* bs = BarrierSet::barrier_set()->barrier_set_c2();\n+    const TypeAryPtr* orig_t = _gvn.type(original)->isa_aryptr();\n+    const TypeKlassPtr* tklass = _gvn.type(klass_node)->is_klassptr();\n+    bool exclude_flat = UseArrayFlattening && bs->array_copy_requires_gc_barriers(true, T_OBJECT, false, false, BarrierSetC2::Parsing) &&\n+                        \/\/ Can src array be flat and contain oops?\n+                        (orig_t == nullptr || (!orig_t->is_not_flat() && (!orig_t->is_flat() || orig_t->elem()->inline_klass()->contains_oops()))) &&\n+                        \/\/ Can dest array be flat and contain oops?\n+                        tklass->can_be_inline_array() && (!tklass->is_flat() || tklass->is_aryklassptr()->elem()->is_instklassptr()->instance_klass()->as_inline_klass()->contains_oops());\n+    Node* not_objArray = exclude_flat ? generate_non_refArray_guard(klass_node, bailout) : generate_typeArray_guard(klass_node, bailout);\n+\n+    Node* refined_klass_node = load_default_refined_array_klass(klass_node, \/* type_array_guard= *\/ false);\n+\n@@ -4561,3 +5158,3 @@\n-      const Type* akls = TypeKlassPtr::make(TypePtr::NotNull, ak, 0\/*offset*\/);\n-      Node* cast = new CastPPNode(control(), klass_node, akls);\n-      klass_node = _gvn.transform(cast);\n+      const Type* akls = TypeKlassPtr::make(TypePtr::NotNull, ak, Type::Offset(0));\n+      Node* cast = new CastPPNode(control(), refined_klass_node, akls);\n+      refined_klass_node = _gvn.transform(cast);\n@@ -4581,0 +5178,39 @@\n+    \/\/ Handle inline type arrays\n+    bool can_validate = !too_many_traps(Deoptimization::Reason_class_check);\n+    if (!stopped()) {\n+      \/\/ TODO 8251971\n+      if (!orig_t->is_null_free()) {\n+        \/\/ Not statically known to be null free, add a check\n+        generate_fair_guard(null_free_array_test(original), bailout);\n+      }\n+      orig_t = _gvn.type(original)->isa_aryptr();\n+      if (orig_t != nullptr && orig_t->is_flat()) {\n+        \/\/ Src is flat, check that dest is flat as well\n+        if (exclude_flat) {\n+          \/\/ Dest can't be flat, bail out\n+          bailout->add_req(control());\n+          set_control(top());\n+        } else {\n+          generate_fair_guard(flat_array_test(refined_klass_node, \/* flat = *\/ false), bailout);\n+        }\n+        \/\/ TODO 8350865 This is not correct anymore. Write tests and fix logic similar to arraycopy.\n+      } else if (UseArrayFlattening && (orig_t == nullptr || !orig_t->is_not_flat()) &&\n+                 \/\/ If dest is flat, src must be flat as well (guaranteed by src <: dest check if validated).\n+                 ((!tklass->is_flat() && tklass->can_be_inline_array()) || !can_validate)) {\n+        \/\/ Src might be flat and dest might not be flat. Go to the slow path if src is flat.\n+        \/\/ TODO 8251971: Optimize for the case when src\/dest are later found to be both flat.\n+        generate_fair_guard(flat_array_test(load_object_klass(original)), bailout);\n+        if (orig_t != nullptr) {\n+          orig_t = orig_t->cast_to_not_flat();\n+          original = _gvn.transform(new CheckCastPPNode(control(), original, orig_t));\n+        }\n+      }\n+      if (!can_validate) {\n+        \/\/ No validation. The subtype check emitted at macro expansion time will not go to the slow\n+        \/\/ path but call checkcast_arraycopy which can not handle flat\/null-free inline type arrays.\n+        \/\/ TODO 8251971: Optimize for the case when src\/dest are later found to be both flat\/null-free.\n+        generate_fair_guard(flat_array_test(refined_klass_node), bailout);\n+        generate_fair_guard(null_free_array_test(original), bailout);\n+      }\n+    }\n+\n@@ -4626,1 +5262,1 @@\n-      if (!too_many_traps(Deoptimization::Reason_class_check)) {\n+      if (can_validate) {\n@@ -4640,1 +5276,1 @@\n-        newcopy = new_array(klass_node, length, 0);  \/\/ no arguments to push\n+        newcopy = new_array(refined_klass_node, length, 0);  \/\/ no arguments to push\n@@ -4712,1 +5348,1 @@\n-    const TypeTuple* range = tf->range();\n+    const TypeTuple* range = tf->range_cc();\n@@ -4716,1 +5352,1 @@\n-    tf = TypeFunc::make(tf->domain(), new_range);\n+    tf = TypeFunc::make(tf->domain_cc(), new_range);\n@@ -4773,1 +5409,8 @@\n-  Node* obj = nullptr;\n+  Node* obj = argument(0);\n+\n+  \/\/ Don't intrinsify hashcode on inline types for now.\n+  \/\/ The \"is locked\" runtime check also subsumes the inline type check (as inline types cannot be locked) and goes to the slow path.\n+  if (gvn().type(obj)->is_inlinetypeptr()) {\n+    return false;\n+  }\n+\n@@ -4783,1 +5426,0 @@\n-    obj = argument(0);\n@@ -4824,0 +5466,2 @@\n+    \/\/ We cannot use the inline type mask as this may check bits that are overriden\n+    \/\/ by an object monitor's pointer when inflating locking.\n@@ -4891,1 +5535,10 @@\n-  Node* obj = null_check_receiver();\n+  Node* obj = argument(0);\n+  if (obj->is_InlineType()) {\n+    const Type* t = _gvn.type(obj);\n+    if (t->maybe_null()) {\n+      null_check(obj);\n+    }\n+    set_result(makecon(TypeInstPtr::make(t->inline_klass()->java_mirror())));\n+    return true;\n+  }\n+  obj = null_check_receiver();\n@@ -5313,1 +5966,2 @@\n-    Node* obj = null_check_receiver();\n+    Node* obj = argument(0);\n+    obj = null_check_receiver();\n@@ -5317,0 +5971,6 @@\n+    if (obj_type->is_inlinetypeptr()) {\n+      \/\/ If the object to clone is an inline type, we can simply return it (i.e. a nop) since inline types have\n+      \/\/ no identity.\n+      set_result(obj);\n+      return true;\n+    }\n@@ -5323,1 +5983,2 @@\n-        obj_type->speculative_type()->is_instance_klass()) {\n+        obj_type->speculative_type()->is_instance_klass() &&\n+        !obj_type->speculative_type()->is_inlinetype()) {\n@@ -5353,0 +6014,5 @@\n+    \/\/ We only go to the fast case code if we pass a number of guards.\n+    \/\/ The paths which do not pass are accumulated in the slow_region.\n+    RegionNode* slow_region = new RegionNode(1);\n+    record_for_igvn(slow_region);\n+\n@@ -5359,3 +6025,0 @@\n-      Node* obj_length = load_array_length(array_obj);\n-      Node* array_size = nullptr; \/\/ Size of the array without object alignment padding.\n-      Node* alloc_obj = new_array(obj_klass, obj_length, 0, &array_size, \/*deoptimize_on_exception=*\/true);\n@@ -5364,20 +6027,7 @@\n-      if (bs->array_copy_requires_gc_barriers(true, T_OBJECT, true, false, BarrierSetC2::Parsing)) {\n-        \/\/ If it is an oop array, it requires very special treatment,\n-        \/\/ because gc barriers are required when accessing the array.\n-        Node* is_obja = generate_objArray_guard(obj_klass, (RegionNode*)nullptr);\n-        if (is_obja != nullptr) {\n-          PreserveJVMState pjvms2(this);\n-          set_control(is_obja);\n-          \/\/ Generate a direct call to the right arraycopy function(s).\n-          \/\/ Clones are always tightly coupled.\n-          ArrayCopyNode* ac = ArrayCopyNode::make(this, true, array_obj, intcon(0), alloc_obj, intcon(0), obj_length, true, false);\n-          ac->set_clone_oop_array();\n-          Node* n = _gvn.transform(ac);\n-          assert(n == ac, \"cannot disappear\");\n-          ac->connect_outputs(this, \/*deoptimize_on_exception=*\/true);\n-\n-          result_reg->init_req(_objArray_path, control());\n-          result_val->init_req(_objArray_path, alloc_obj);\n-          result_i_o ->set_req(_objArray_path, i_o());\n-          result_mem ->set_req(_objArray_path, reset_memory());\n-        }\n+      const TypeAryPtr* ary_ptr = obj_type->isa_aryptr();\n+      if (UseArrayFlattening && bs->array_copy_requires_gc_barriers(true, T_OBJECT, true, false, BarrierSetC2::Expansion) &&\n+          obj_type->can_be_inline_array() &&\n+          (ary_ptr == nullptr || (!ary_ptr->is_not_flat() && (!ary_ptr->is_flat() || ary_ptr->elem()->inline_klass()->contains_oops())))) {\n+        \/\/ Flat inline type array may have object field that would require a\n+        \/\/ write barrier. Conservatively, go to slow path.\n+        generate_fair_guard(flat_array_test(obj_klass), slow_region);\n@@ -5385,7 +6035,0 @@\n-      \/\/ Otherwise, there are no barriers to worry about.\n-      \/\/ (We can dispense with card marks if we know the allocation\n-      \/\/  comes out of eden (TLAB)...  In fact, ReduceInitialCardMarks\n-      \/\/  causes the non-eden paths to take compensating steps to\n-      \/\/  simulate a fresh allocation, so that no further\n-      \/\/  card marks are required in compiled code to initialize\n-      \/\/  the object.)\n@@ -5394,7 +6037,43 @@\n-        copy_to_clone(array_obj, alloc_obj, array_size, true);\n-\n-        \/\/ Present the results of the copy.\n-        result_reg->init_req(_array_path, control());\n-        result_val->init_req(_array_path, alloc_obj);\n-        result_i_o ->set_req(_array_path, i_o());\n-        result_mem ->set_req(_array_path, reset_memory());\n+        Node* obj_length = load_array_length(array_obj);\n+        Node* array_size = nullptr; \/\/ Size of the array without object alignment padding.\n+        Node* alloc_obj = new_array(obj_klass, obj_length, 0, &array_size, \/*deoptimize_on_exception=*\/true);\n+\n+        BarrierSetC2* bs = BarrierSet::barrier_set()->barrier_set_c2();\n+        if (bs->array_copy_requires_gc_barriers(true, T_OBJECT, true, false, BarrierSetC2::Parsing)) {\n+          \/\/ If it is an oop array, it requires very special treatment,\n+          \/\/ because gc barriers are required when accessing the array.\n+          Node* is_obja = generate_refArray_guard(obj_klass, (RegionNode*)nullptr);\n+          if (is_obja != nullptr) {\n+            PreserveJVMState pjvms2(this);\n+            set_control(is_obja);\n+            \/\/ Generate a direct call to the right arraycopy function(s).\n+            \/\/ Clones are always tightly coupled.\n+            ArrayCopyNode* ac = ArrayCopyNode::make(this, true, array_obj, intcon(0), alloc_obj, intcon(0), obj_length, true, false);\n+            ac->set_clone_oop_array();\n+            Node* n = _gvn.transform(ac);\n+            assert(n == ac, \"cannot disappear\");\n+            ac->connect_outputs(this, \/*deoptimize_on_exception=*\/true);\n+\n+            result_reg->init_req(_objArray_path, control());\n+            result_val->init_req(_objArray_path, alloc_obj);\n+            result_i_o ->set_req(_objArray_path, i_o());\n+            result_mem ->set_req(_objArray_path, reset_memory());\n+          }\n+        }\n+        \/\/ Otherwise, there are no barriers to worry about.\n+        \/\/ (We can dispense with card marks if we know the allocation\n+        \/\/  comes out of eden (TLAB)...  In fact, ReduceInitialCardMarks\n+        \/\/  causes the non-eden paths to take compensating steps to\n+        \/\/  simulate a fresh allocation, so that no further\n+        \/\/  card marks are required in compiled code to initialize\n+        \/\/  the object.)\n+\n+        if (!stopped()) {\n+          copy_to_clone(obj, alloc_obj, array_size, true);\n+\n+          \/\/ Present the results of the copy.\n+          result_reg->init_req(_array_path, control());\n+          result_val->init_req(_array_path, alloc_obj);\n+          result_i_o ->set_req(_array_path, i_o());\n+          result_mem ->set_req(_array_path, reset_memory());\n+        }\n@@ -5404,4 +6083,0 @@\n-    \/\/ We only go to the instance fast case code if we pass a number of guards.\n-    \/\/ The paths which do not pass are accumulated in the slow_region.\n-    RegionNode* slow_region = new RegionNode(1);\n-    record_for_igvn(slow_region);\n@@ -5539,0 +6214,12 @@\n+  int adjustment = 1;\n+  const TypeAryKlassPtr* ary_klass_ptr = alloc->in(AllocateNode::KlassNode)->bottom_type()->is_aryklassptr();\n+  if (ary_klass_ptr->is_null_free()) {\n+    \/\/ A null-free, tightly coupled array allocation can only come from LibraryCallKit::inline_newArray which\n+    \/\/ also requires the componentType and initVal on stack for re-execution.\n+    \/\/ Re-create and push the componentType.\n+    ciArrayKlass* klass = ary_klass_ptr->exact_klass()->as_array_klass();\n+    ciInstance* instance = klass->component_mirror_instance();\n+    const TypeInstPtr* t_instance = TypeInstPtr::make(instance);\n+    sfpt->ins_req(old_jvms->stkoff() + old_jvms->sp(), makecon(t_instance));\n+    adjustment++;\n+  }\n@@ -5540,5 +6227,16 @@\n-  sfpt->ins_req(old_jvms->stkoff() + old_jvms->sp(), alloc->in(AllocateNode::ALength));\n-  old_jvms->set_sp(old_jvms->sp()+1);\n-  old_jvms->set_monoff(old_jvms->monoff()+1);\n-  old_jvms->set_scloff(old_jvms->scloff()+1);\n-  old_jvms->set_endoff(old_jvms->endoff()+1);\n+  sfpt->ins_req(old_jvms->stkoff() + old_jvms->sp() + adjustment - 1, alloc->in(AllocateNode::ALength));\n+  if (ary_klass_ptr->is_null_free()) {\n+    \/\/ Re-create and push the initVal.\n+    Node* init_val = alloc->in(AllocateNode::InitValue);\n+    if (init_val == nullptr) {\n+      init_val = InlineTypeNode::make_all_zero(_gvn, ary_klass_ptr->elem()->is_instklassptr()->instance_klass()->as_inline_klass());\n+    } else if (UseCompressedOops) {\n+      init_val = _gvn.transform(new DecodeNNode(init_val, init_val->bottom_type()->make_ptr()));\n+    }\n+    sfpt->ins_req(old_jvms->stkoff() + old_jvms->sp() + adjustment, init_val);\n+    adjustment++;\n+  }\n+  old_jvms->set_sp(old_jvms->sp() + adjustment);\n+  old_jvms->set_monoff(old_jvms->monoff() + adjustment);\n+  old_jvms->set_scloff(old_jvms->scloff() + adjustment);\n+  old_jvms->set_endoff(old_jvms->endoff() + adjustment);\n@@ -5577,2 +6275,1 @@\n-    CallProjections callprojs;\n-    alloc->extract_projections(&callprojs, true);\n+    CallProjections* callprojs = alloc->extract_projections(true);\n@@ -5581,1 +6278,1 @@\n-    C->gvn_replace_by(callprojs.fallthrough_ioproj, alloc->in(TypeFunc::I_O));\n+    C->gvn_replace_by(callprojs->fallthrough_ioproj, alloc->in(TypeFunc::I_O));\n@@ -5623,1 +6320,1 @@\n-    set_i_o(callprojs.fallthrough_ioproj);\n+    set_i_o(callprojs->fallthrough_ioproj);\n@@ -5973,1 +6670,1 @@\n-    if (src_elem == dest_elem && src_elem == T_OBJECT) {\n+    if (src_elem == dest_elem && top_src->is_flat() == top_dest->is_flat() && src_elem == T_OBJECT) {\n@@ -6000,0 +6697,2 @@\n+          src_type = _gvn.type(src);\n+          top_src = src_type->isa_aryptr();\n@@ -6003,0 +6702,2 @@\n+          dest_type = _gvn.type(dest);\n+          top_dest = dest_type->isa_aryptr();\n@@ -6018,2 +6719,1 @@\n-      can_emit_guards &&\n-      !src->is_top() && !dest->is_top()) {\n+      can_emit_guards && !src->is_top() && !dest->is_top()) {\n@@ -6060,0 +6760,1 @@\n+    Node* refined_dest_klass = dest_klass;\n@@ -6061,0 +6762,1 @@\n+      dest_klass = load_non_refined_array_klass(refined_dest_klass);\n@@ -6062,8 +6764,1 @@\n-\n-      if (not_subtype_ctrl != top()) {\n-        PreserveJVMState pjvms(this);\n-        set_control(not_subtype_ctrl);\n-        uncommon_trap(Deoptimization::Reason_intrinsic,\n-                      Deoptimization::Action_make_not_entrant);\n-        assert(stopped(), \"Should be stopped\");\n-      }\n+      slow_region->add_req(not_subtype_ctrl);\n@@ -6071,0 +6766,21 @@\n+\n+    \/\/ TODO 8350865 Improve this. What about atomicity? Make sure this is always folded for type arrays.\n+    \/\/ If destination is null-restricted, source must be null-restricted as well: src_null_restricted || !dst_null_restricted\n+    Node* src_klass = load_object_klass(src);\n+    Node* adr_prop_src = basic_plus_adr(src_klass, in_bytes(ArrayKlass::properties_offset()));\n+    Node* prop_src = _gvn.transform(LoadNode::make(_gvn, control(), immutable_memory(), adr_prop_src, TypeRawPtr::BOTTOM, TypeInt::INT, T_INT, MemNode::unordered));\n+    Node* adr_prop_dest = basic_plus_adr(refined_dest_klass, in_bytes(ArrayKlass::properties_offset()));\n+    Node* prop_dest = _gvn.transform(LoadNode::make(_gvn, control(), immutable_memory(), adr_prop_dest, TypeRawPtr::BOTTOM, TypeInt::INT, T_INT, MemNode::unordered));\n+\n+    prop_dest = _gvn.transform(new XorINode(prop_dest, intcon(ArrayKlass::ArrayProperties::NULL_RESTRICTED)));\n+    prop_src = _gvn.transform(new OrINode(prop_dest, prop_src));\n+    prop_src = _gvn.transform(new AndINode(prop_src, intcon(ArrayKlass::ArrayProperties::NULL_RESTRICTED)));\n+\n+    Node* chk = _gvn.transform(new CmpINode(prop_src, intcon(ArrayKlass::ArrayProperties::NULL_RESTRICTED)));\n+    Node* tst = _gvn.transform(new BoolNode(chk, BoolTest::ne));\n+    generate_fair_guard(tst, slow_region);\n+\n+    \/\/ TODO 8350865 This is too strong\n+    generate_fair_guard(flat_array_test(src), slow_region);\n+    generate_fair_guard(flat_array_test(dest), slow_region);\n+\n@@ -6079,2 +6795,2 @@\n-    const TypeKlassPtr* dest_klass_t = _gvn.type(dest_klass)->is_klassptr();\n-    const Type *toop = dest_klass_t->cast_to_exactness(false)->as_instance_type();\n+    const TypeKlassPtr* dest_klass_t = _gvn.type(refined_dest_klass)->is_klassptr();\n+    const Type* toop = dest_klass_t->cast_to_exactness(false)->as_instance_type();\n@@ -6089,0 +6805,3 @@\n+  Node* dest_klass = load_object_klass(dest);\n+  dest_klass = load_non_refined_array_klass(dest_klass);\n+\n@@ -6093,1 +6812,1 @@\n-                                          load_object_klass(src), load_object_klass(dest),\n+                                          load_object_klass(src), dest_klass,\n","filename":"src\/hotspot\/share\/opto\/library_call.cpp","additions":860,"deletions":141,"binary":false,"changes":1001,"status":"modified"},{"patch":"@@ -34,0 +34,1 @@\n+#include \"opto\/inlinetypenode.hpp\"\n@@ -111,2 +112,3 @@\n-    if (!stopped() && result() != nullptr) {\n-      if (result()->is_top()) {\n+    Node* res = result();\n+    if (!stopped() && res != nullptr) {\n+      if (res->is_top()) {\n@@ -116,2 +118,9 @@\n-      BasicType bt = result()->bottom_type()->basic_type();\n-      push_node(bt, result());\n+      BasicType bt = res->bottom_type()->basic_type();\n+      if (C->inlining_incrementally() && res->is_InlineType()) {\n+        \/\/ The caller expects an oop when incrementally inlining an intrinsic that returns an\n+        \/\/ inline type. Make sure the call is re-executed if the allocation triggers a deoptimization.\n+        PreserveReexecuteState preexecs(this);\n+        jvms()->set_should_reexecute(true);\n+        res = res->as_InlineType()->buffer(this);\n+      }\n+      push_node(bt, res);\n@@ -172,1 +181,0 @@\n-  Node* load_mirror_from_klass(Node* klass);\n@@ -190,0 +198,3 @@\n+  Node* load_default_refined_array_klass(Node* klass_node, bool type_array_guard = true);\n+  Node* load_non_refined_array_klass(Node* klass_node);\n+\n@@ -196,0 +207,9 @@\n+\n+  enum ArrayKind {\n+    AnyArray,\n+    NonArray,\n+    RefArray,\n+    NonRefArray,\n+    TypeArray\n+  };\n+\n@@ -197,0 +217,1 @@\n+\n@@ -198,1 +219,1 @@\n-    return generate_array_guard_common(kls, region, false, false, obj);\n+    return generate_array_guard_common(kls, region, AnyArray, obj);\n@@ -201,1 +222,4 @@\n-    return generate_array_guard_common(kls, region, false, true, obj);\n+    return generate_array_guard_common(kls, region, NonArray, obj);\n+  }\n+  Node* generate_refArray_guard(Node* kls, RegionNode* region, Node** obj = nullptr) {\n+    return generate_array_guard_common(kls, region, RefArray, obj);\n@@ -203,2 +227,2 @@\n-  Node* generate_objArray_guard(Node* kls, RegionNode* region, Node** obj = nullptr) {\n-    return generate_array_guard_common(kls, region, true, false, obj);\n+  Node* generate_non_refArray_guard(Node* kls, RegionNode* region, Node** obj = nullptr) {\n+    return generate_array_guard_common(kls, region, NonRefArray, obj);\n@@ -206,2 +230,2 @@\n-  Node* generate_non_objArray_guard(Node* kls, RegionNode* region, Node** obj = nullptr) {\n-    return generate_array_guard_common(kls, region, true, true, obj);\n+  Node* generate_typeArray_guard(Node* kls, RegionNode* region, Node** obj = nullptr) {\n+    return generate_array_guard_common(kls, region, TypeArray, obj);\n@@ -209,2 +233,1 @@\n-  Node* generate_array_guard_common(Node* kls, RegionNode* region,\n-                                    bool obj_array, bool not_array, Node** obj = nullptr);\n+  Node* generate_array_guard_common(Node* kls, RegionNode* region, ArrayKind kind, Node** obj = nullptr);\n@@ -259,0 +282,1 @@\n+  bool inline_unsafe_flat_access(bool is_store, AccessKind kind);\n@@ -262,0 +286,3 @@\n+  bool inline_newArray(bool null_free, bool atomic);\n+  typedef enum { IsFlat, IsNullRestricted, IsAtomic } ArrayPropertiesCheck;\n+  bool inline_getArrayProperties(ArrayPropertiesCheck check);\n@@ -265,0 +292,2 @@\n+  bool inline_unsafe_make_private_buffer();\n+  bool inline_unsafe_finish_private_buffer();\n@@ -293,0 +322,1 @@\n+  bool inline_primitive_Class_conversion(vmIntrinsics::ID id);\n@@ -322,0 +352,3 @@\n+  bool inline_arrayInstanceBaseOffset();\n+  bool inline_arrayInstanceIndexScale();\n+  bool inline_arrayLayout();\n","filename":"src\/hotspot\/share\/opto\/library_call.hpp","additions":46,"deletions":13,"binary":false,"changes":59,"status":"modified"},{"patch":"@@ -419,0 +419,16 @@\n+  if (tp->isa_aryptr()) {\n+    \/\/ In the case of a flat inline type array, each field has its\n+    \/\/ own slice so we need to extract the field being accessed from\n+    \/\/ the address computation\n+    if (offset == Type::OffsetBot) {\n+      Node* base;\n+      Node* index;\n+      const MachOper* oper = memory_inputs(base, index);\n+      if (oper != (MachOper*)-1) {\n+        offset = oper->constant_disp();\n+        return tp->is_aryptr()->add_field_offset_and_offset(offset)->add_offset(Type::OffsetBot);\n+      }\n+    }\n+    return tp->is_aryptr()->add_field_offset_and_offset(offset);\n+  }\n+\n@@ -490,0 +506,5 @@\n+  \/\/ Never rematerialize CastI2N because it might \"hide\" narrow oops from a safepoint\n+  if (ideal_Opcode() == Op_CastI2N) {\n+    return false;\n+  }\n+\n@@ -722,2 +743,2 @@\n-const Type *MachCallNode::bottom_type() const { return tf()->range(); }\n-const Type* MachCallNode::Value(PhaseGVN* phase) const { return tf()->range(); }\n+const Type *MachCallNode::bottom_type() const { return tf()->range_cc(); }\n+const Type* MachCallNode::Value(PhaseGVN* phase) const { return tf()->range_cc(); }\n@@ -734,2 +755,1 @@\n-#ifndef _LP64\n-  if (tf()->range()->cnt() == TypeFunc::Parms) {\n+  if (tf()->range_sig()->cnt() == TypeFunc::Parms) {\n@@ -751,1 +771,0 @@\n-#endif\n@@ -757,1 +776,1 @@\n-  const TypeTuple *r = tf()->range();\n+  const TypeTuple *r = tf()->range_sig();\n@@ -762,0 +781,4 @@\n+bool MachCallNode::returns_scalarized() const {\n+  return tf()->returns_inline_type_as_fields();\n+}\n+\n@@ -766,1 +789,6 @@\n-  if (idx < tf()->domain()->cnt()) {\n+  if (entry_point() == nullptr && idx == TypeFunc::Parms) {\n+    \/\/ Null entry point is a special cast where the target of the call\n+    \/\/ is in a register.\n+    return MachNode::in_RegMask(idx);\n+  }\n+  if (idx < tf()->domain_sig()->cnt()) {\n@@ -797,1 +825,1 @@\n-  if (idx < tf()->domain()->cnt()) {\n+  if (idx < tf()->domain_cc()->cnt()) {\n","filename":"src\/hotspot\/share\/opto\/machnode.cpp","additions":36,"deletions":8,"binary":false,"changes":44,"status":"modified"},{"patch":"@@ -54,0 +54,1 @@\n+class MachVEPNode;\n@@ -513,0 +514,30 @@\n+\/\/------------------------------MachVEPNode-----------------------------------\n+\/\/ Machine Inline Type Entry Point Node\n+class MachVEPNode : public MachIdealNode {\n+public:\n+  Label* _verified_entry;\n+\n+  MachVEPNode(Label* verified_entry, bool verified, bool receiver_only) :\n+    _verified_entry(verified_entry),\n+    _verified(verified),\n+    _receiver_only(receiver_only) {\n+    init_class_id(Class_MachVEP);\n+  }\n+  virtual bool cmp(const Node &n) const {\n+    return (_verified_entry == ((MachVEPNode&)n)._verified_entry) &&\n+           (_verified == ((MachVEPNode&)n)._verified) &&\n+           (_receiver_only == ((MachVEPNode&)n)._receiver_only) &&\n+           MachIdealNode::cmp(n);\n+  }\n+  virtual uint size_of() const { return sizeof(*this); }\n+  virtual void emit(C2_MacroAssembler *masm, PhaseRegAlloc* ra_) const;\n+\n+#ifndef PRODUCT\n+  virtual const char* Name() const { return \"InlineType Entry-Point\"; }\n+  virtual void format(PhaseRegAlloc*, outputStream* st) const;\n+#endif\n+private:\n+  bool   _verified;\n+  bool   _receiver_only;\n+};\n+\n@@ -519,1 +550,0 @@\n-  virtual uint size(PhaseRegAlloc *ra_) const;\n@@ -531,1 +561,9 @@\n-  MachPrologNode( ) {}\n+  Label* _verified_entry;\n+\n+  MachPrologNode(Label* verified_entry) : _verified_entry(verified_entry) {\n+    init_class_id(Class_MachProlog);\n+  }\n+  virtual bool cmp(const Node &n) const {\n+    return (_verified_entry == ((MachPrologNode&)n)._verified_entry) && MachIdealNode::cmp(n);\n+  }\n+  virtual uint size_of() const { return sizeof(*this); }\n@@ -533,1 +571,0 @@\n-  virtual uint size(PhaseRegAlloc *ra_) const;\n@@ -550,1 +587,0 @@\n-  virtual uint size(PhaseRegAlloc *ra_) const;\n@@ -942,1 +978,1 @@\n-  NOT_LP64(bool return_value_is_used() const;)\n+  bool return_value_is_used() const;\n@@ -946,0 +982,1 @@\n+  bool returns_scalarized() const;\n","filename":"src\/hotspot\/share\/opto\/machnode.hpp","additions":42,"deletions":5,"binary":false,"changes":47,"status":"modified"},{"patch":"@@ -169,0 +169,45 @@\n+\/\/ Array of RegMask, one per returned values (inline type instances can\n+\/\/ be returned as multiple return values, one per field)\n+RegMask* Matcher::return_values_mask(const TypeFunc* tf) {\n+  const TypeTuple* range = tf->range_cc();\n+  uint cnt = range->cnt() - TypeFunc::Parms;\n+  if (cnt == 0) {\n+    return nullptr;\n+  }\n+  RegMask* mask = NEW_RESOURCE_ARRAY(RegMask, cnt);\n+  BasicType* sig_bt = NEW_RESOURCE_ARRAY(BasicType, cnt);\n+  VMRegPair* vm_parm_regs = NEW_RESOURCE_ARRAY(VMRegPair, cnt);\n+  for (uint i = 0; i < cnt; i++) {\n+    sig_bt[i] = range->field_at(i+TypeFunc::Parms)->basic_type();\n+    new (mask + i) RegMask();\n+  }\n+\n+  int regs = SharedRuntime::java_return_convention(sig_bt, vm_parm_regs, cnt);\n+  if (regs <= 0) {\n+    \/\/ We ran out of registers to store the null marker for a nullable inline type return.\n+    \/\/ Since it is only set in the 'call_epilog', we can simply put it on the stack.\n+    assert(tf->returns_inline_type_as_fields(), \"should have been tested during graph construction\");\n+    \/\/ TODO 8284443 Can we teach the register allocator to reserve a stack slot instead?\n+    \/\/ mask[--cnt] = STACK_ONLY_mask does not work (test with -XX:+StressGCM)\n+    int slot = C->fixed_slots() - 2;\n+    if (C->needs_stack_repair()) {\n+      slot -= 2; \/\/ Account for stack increment value\n+    }\n+    mask[--cnt].clear();\n+    mask[cnt].insert(OptoReg::stack2reg(slot));\n+  }\n+  for (uint i = 0; i < cnt; i++) {\n+    mask[i].clear();\n+\n+    OptoReg::Name reg1 = OptoReg::as_OptoReg(vm_parm_regs[i].first());\n+    if (OptoReg::is_valid(reg1)) {\n+      mask[i].insert(reg1);\n+    }\n+    OptoReg::Name reg2 = OptoReg::as_OptoReg(vm_parm_regs[i].second());\n+    if (OptoReg::is_valid(reg2)) {\n+      mask[i].insert(reg2);\n+    }\n+  }\n+\n+  return mask;\n+}\n@@ -189,15 +234,3 @@\n-  \/\/ Map a Java-signature return type into return register-value\n-  \/\/ machine registers for 0, 1 and 2 returned values.\n-  const TypeTuple *range = C->tf()->range();\n-  if( range->cnt() > TypeFunc::Parms ) { \/\/ If not a void function\n-    \/\/ Get ideal-register return type\n-    uint ireg = range->field_at(TypeFunc::Parms)->ideal_reg();\n-    \/\/ Get machine return register\n-    uint sop = C->start()->Opcode();\n-    OptoRegPair regs = return_value(ireg);\n-\n-    \/\/ And mask for same\n-    _return_value_mask.assignFrom(RegMask(regs.first()));\n-    if( OptoReg::is_valid(regs.second()) )\n-      _return_value_mask.insert(regs.second());\n-  }\n+  \/\/ Map Java-signature return types into return register-value\n+  \/\/ machine registers.\n+  _return_values_mask = return_values_mask(C->tf());\n@@ -211,1 +244,1 @@\n-  const TypeTuple *domain = C->tf()->domain();\n+  const TypeTuple *domain = C->tf()->domain_cc();\n@@ -482,0 +515,1 @@\n+\n@@ -702,1 +736,1 @@\n-  uint ret_edge_cnt = TypeFunc::Parms + ((C->tf()->range()->cnt() == TypeFunc::Parms) ? 0 : 1);\n+  uint ret_edge_cnt = C->tf()->range_cc()->cnt();\n@@ -704,4 +738,2 @@\n-  \/\/ Returns have 0 or 1 returned values depending on call signature.\n-  \/\/ Return register is specified by return_value in the AD file.\n-  if (ret_edge_cnt > TypeFunc::Parms) {\n-    ret_rms[TypeFunc::Parms + 0].assignFrom(_return_value_mask);\n+  for (i = TypeFunc::Parms; i < ret_edge_cnt; i++) {\n+    ret_rms[i].assignFrom(_return_values_mask[i-TypeFunc::Parms]);\n@@ -780,1 +812,1 @@\n-  int proj_cnt = C->tf()->domain()->cnt();\n+  int proj_cnt = C->tf()->domain_cc()->cnt();\n@@ -1060,1 +1092,5 @@\n-                m = n->in(0)->as_Multi()->match( n->as_Proj(), this );\n+                RegMask* mask = nullptr;\n+                if (n->in(0)->is_Call() && n->in(0)->as_Call()->tf()->returns_inline_type_as_fields()) {\n+                  mask = return_values_mask(n->in(0)->as_Call()->tf());\n+                }\n+                m = n->in(0)->as_Multi()->match(n->as_Proj(), this, mask);\n@@ -1196,1 +1232,1 @@\n-    domain = call->tf()->domain();\n+    domain = call->tf()->domain_cc();\n@@ -1272,1 +1308,4 @@\n-  int argcnt = cnt - TypeFunc::Parms;\n+  \/\/ Null entry point is a special cast where the target of the call\n+  \/\/ is in a register.\n+  int adj = (call != nullptr && call->entry_point() == nullptr) ? 1 : 0;\n+  int argcnt = cnt - TypeFunc::Parms - adj;\n@@ -1278,1 +1317,1 @@\n-      sig_bt[i] = domain->field_at(i+TypeFunc::Parms)->basic_type();\n+      sig_bt[i] = domain->field_at(i+TypeFunc::Parms+adj)->basic_type();\n@@ -1319,1 +1358,1 @@\n-      RegMask *rm = &mcall->_in_rms[i+TypeFunc::Parms];\n+      RegMask *rm = &mcall->_in_rms[i+TypeFunc::Parms+adj];\n@@ -1337,2 +1376,3 @@\n-      if (OptoReg::is_valid(reg1))\n-        rm->insert(reg1);\n+      if (OptoReg::is_valid(reg1)) {\n+        rm->insert( reg1 );\n+      }\n@@ -1341,2 +1381,3 @@\n-      if (OptoReg::is_valid(reg2))\n-        rm->insert(reg2);\n+      if (OptoReg::is_valid(reg2)) {\n+        rm->insert( reg2 );\n+      }\n@@ -1358,2 +1399,2 @@\n-    uint r_cnt = mcall->tf()->range()->cnt();\n-    MachProjNode* proj = new MachProjNode(mcall, r_cnt + 10000, RegMask::EMPTY, MachProjNode::fat_proj);\n+    uint r_cnt = mcall->tf()->range_sig()->cnt();\n+    MachProjNode *proj = new MachProjNode( mcall, r_cnt+10000, RegMask::EMPTY, MachProjNode::fat_proj );\n@@ -1376,1 +1417,1 @@\n-         (mcall->jvms()->debug_start() + mcall->_jvmadj == mcall->tf()->domain()->cnt()), \"\");\n+         (mcall->jvms()->debug_start() + mcall->_jvmadj == mcall->tf()->domain_cc()->cnt()), \"\");\n@@ -2065,1 +2106,1 @@\n-      for (int i = n->req() - 1; i >= 0; --i) { \/\/ For my children\n+      for (int i = n->len() - 1; i >= 0; --i) { \/\/ For my children\n@@ -2376,0 +2417,7 @@\n+    case Op_ClearArray: {\n+      Node* pair = new BinaryNode(n->in(2), n->in(3));\n+      n->set_req(2, pair);\n+      n->set_req(3, n->in(4));\n+      n->del_req(4);\n+      break;\n+    }\n@@ -2425,0 +2473,8 @@\n+    case Op_StoreLSpecial: {\n+      if (n->req() > (MemNode::ValueIn + 1) && n->in(MemNode::ValueIn + 1) != nullptr) {\n+        Node* pair = new BinaryNode(n->in(MemNode::ValueIn), n->in(MemNode::ValueIn + 1));\n+        n->set_req(MemNode::ValueIn, pair);\n+        n->del_req(MemNode::ValueIn + 1);\n+      }\n+      break;\n+    }\n","filename":"src\/hotspot\/share\/opto\/matcher.cpp","additions":91,"deletions":35,"binary":false,"changes":126,"status":"modified"},{"patch":"@@ -268,0 +268,2 @@\n+  RegMask* return_values_mask(const TypeFunc* tf);\n+\n@@ -411,1 +413,1 @@\n-  RegMask                     _return_value_mask;\n+  RegMask*            _return_values_mask;\n","filename":"src\/hotspot\/share\/opto\/matcher.hpp","additions":3,"deletions":1,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -90,0 +90,1 @@\n+class FlatArrayCheckNode;\n@@ -122,0 +123,1 @@\n+class MachPrologNode;\n@@ -128,0 +130,1 @@\n+class MachVEPNode;\n@@ -186,0 +189,3 @@\n+class InlineTypeNode;\n+class LoadFlatNode;\n+class StoreFlatNode;\n@@ -692,0 +698,2 @@\n+        DEFINE_CLASS_ID(LoadFlat,  SafePoint, 1)\n+        DEFINE_CLASS_ID(StoreFlat, SafePoint, 2)\n@@ -708,0 +716,1 @@\n+        DEFINE_CLASS_ID(Blackhole,        MemBar, 2)\n@@ -729,0 +738,2 @@\n+      DEFINE_CLASS_ID(MachProlog,       Mach, 8)\n+      DEFINE_CLASS_ID(MachVEP,          Mach, 9)\n@@ -761,1 +772,2 @@\n-      DEFINE_CLASS_ID(Con, Type, 8)\n+      DEFINE_CLASS_ID(InlineType, Type, 8)\n+      DEFINE_CLASS_ID(Con, Type, 9)\n@@ -763,2 +775,2 @@\n-      DEFINE_CLASS_ID(SafePointScalarMerge, Type, 9)\n-      DEFINE_CLASS_ID(Convert, Type, 10)\n+      DEFINE_CLASS_ID(SafePointScalarMerge, Type, 10)\n+      DEFINE_CLASS_ID(Convert, Type, 11)\n@@ -803,3 +815,4 @@\n-        DEFINE_CLASS_ID(FastLock,   Cmp, 0)\n-        DEFINE_CLASS_ID(FastUnlock, Cmp, 1)\n-        DEFINE_CLASS_ID(SubTypeCheck,Cmp, 2)\n+        DEFINE_CLASS_ID(FastLock,       Cmp, 0)\n+        DEFINE_CLASS_ID(FastUnlock,     Cmp, 1)\n+        DEFINE_CLASS_ID(SubTypeCheck,   Cmp, 2)\n+        DEFINE_CLASS_ID(FlatArrayCheck, Cmp, 3)\n@@ -914,0 +927,1 @@\n+  DEFINE_CLASS_QUERY(Blackhole)\n@@ -947,0 +961,1 @@\n+  DEFINE_CLASS_QUERY(FlatArrayCheck)\n@@ -979,0 +994,1 @@\n+  DEFINE_CLASS_QUERY(MachProlog)\n@@ -985,0 +1001,1 @@\n+  DEFINE_CLASS_QUERY(MachVEP)\n@@ -1023,0 +1040,3 @@\n+  DEFINE_CLASS_QUERY(InlineType)\n+  DEFINE_CLASS_QUERY(LoadFlat)\n+  DEFINE_CLASS_QUERY(StoreFlat)\n","filename":"src\/hotspot\/share\/opto\/node.hpp","additions":26,"deletions":6,"binary":false,"changes":32,"status":"modified"},{"patch":"@@ -47,0 +47,2 @@\n+#include \"oops\/flatArrayKlass.hpp\"\n+#include \"oops\/flatArrayOop.inline.hpp\"\n@@ -173,0 +175,1 @@\n+const TypeFunc* OptoRuntime::_new_array_nozero_Type               = nullptr;\n@@ -301,1 +304,1 @@\n-JRT_BLOCK_ENTRY(void, OptoRuntime::new_instance_C(Klass* klass, JavaThread* current))\n+JRT_BLOCK_ENTRY(void, OptoRuntime::new_instance_C(Klass* klass, bool is_larval, JavaThread* current))\n@@ -321,1 +324,5 @@\n-    oop result = InstanceKlass::cast(klass)->allocate_instance(THREAD);\n+    instanceOop result = InstanceKlass::cast(klass)->allocate_instance(THREAD);\n+    if (is_larval) {\n+      \/\/ Check if this is a larval buffer allocation\n+      result->set_mark(result->mark().enter_larval_state());\n+    }\n@@ -339,1 +346,1 @@\n-JRT_BLOCK_ENTRY(void, OptoRuntime::new_array_C(Klass* array_type, int len, JavaThread* current))\n+JRT_BLOCK_ENTRY(void, OptoRuntime::new_array_C(Klass* array_type, int len, oopDesc* init_val, JavaThread* current))\n@@ -348,0 +355,1 @@\n+  Handle h_init_val(current, init_val); \/\/ keep the init_val object alive\n@@ -349,1 +357,13 @@\n-  if (array_type->is_typeArray_klass()) {\n+  if (array_type->is_flatArray_klass()) {\n+    Handle holder(current, array_type->klass_holder()); \/\/ keep the array klass alive\n+    FlatArrayKlass* fak = FlatArrayKlass::cast(array_type);\n+    InlineKlass* vk = fak->element_klass();\n+    ArrayKlass::ArrayProperties props = ArrayKlass::array_properties_from_layout(fak->layout_kind());\n+    result = oopFactory::new_flatArray(vk, len, props, fak->layout_kind(), THREAD);\n+    if (array_type->is_null_free_array_klass() && !h_init_val.is_null()) {\n+      \/\/ Null-free arrays need to be initialized\n+      for (int i = 0; i < len; i++) {\n+        vk->write_value_to_addr(h_init_val(), ((flatArrayOop)result)->value_at_addr(i, fak->layout_helper()), fak->layout_kind(), true, CHECK);\n+      }\n+    }\n+  } else if (array_type->is_typeArray_klass()) {\n@@ -355,5 +375,7 @@\n-    \/\/ Although the oopFactory likes to work with the elem_type,\n-    \/\/ the compiler prefers the array_type, since it must already have\n-    \/\/ that latter value in hand for the fast path.\n-    Klass* elem_type = ObjArrayKlass::cast(array_type)->element_klass();\n-    result = oopFactory::new_objArray(elem_type, len, THREAD);\n+    result = oopFactory::new_refArray(array_type, len, THREAD);\n+    if (array_type->is_null_free_array_klass() && !h_init_val.is_null()) {\n+      \/\/ Null-free arrays need to be initialized\n+      for (int i = 0; i < len; i++) {\n+        ((objArrayOop)result)->obj_at_put(i, h_init_val());\n+      }\n+    }\n@@ -577,1 +599,1 @@\n-  const Type **fields = TypeTuple::fields(1);\n+  const Type **fields = TypeTuple::fields(2);\n@@ -579,1 +601,2 @@\n-  const TypeTuple *domain = TypeTuple::make(TypeFunc::Parms+1, fields);\n+  fields[TypeFunc::Parms+1] = TypeInt::BOOL;        \/\/ is_larval\n+  const TypeTuple *domain = TypeTuple::make(TypeFunc::Parms+2, fields);\n@@ -620,0 +643,17 @@\n+  \/\/ create input type (domain)\n+  const Type **fields = TypeTuple::fields(3);\n+  fields[TypeFunc::Parms+0] = TypeInstPtr::NOTNULL;   \/\/ element klass\n+  fields[TypeFunc::Parms+1] = TypeInt::INT;       \/\/ array size\n+  fields[TypeFunc::Parms+2] = TypeInstPtr::NOTNULL;       \/\/ init value\n+  const TypeTuple *domain = TypeTuple::make(TypeFunc::Parms+3, fields);\n+\n+  \/\/ create result type (range)\n+  fields = TypeTuple::fields(1);\n+  fields[TypeFunc::Parms+0] = TypeRawPtr::NOTNULL; \/\/ Returned oop\n+\n+  const TypeTuple *range = TypeTuple::make(TypeFunc::Parms+1, fields);\n+\n+  return TypeFunc::make(domain, range);\n+}\n+\n+static const TypeFunc* make_new_array_nozero_Type() {\n@@ -695,1 +735,1 @@\n-  return TypeFunc::make(domain,range);\n+  return TypeFunc::make(domain, range);\n@@ -2129,1 +2169,1 @@\n-  return TypeFunc::make(domain,range);\n+  return TypeFunc::make(domain, range);\n@@ -2161,1 +2201,1 @@\n-  return TypeFunc::make(domain,range);\n+  return TypeFunc::make(domain, range);\n@@ -2177,1 +2217,1 @@\n-  return TypeFunc::make(domain,range);\n+  return TypeFunc::make(domain, range);\n@@ -2268,0 +2308,1 @@\n+  _new_array_nozero_Type              = make_new_array_nozero_Type();\n@@ -2366,0 +2407,105 @@\n+\n+const TypeFunc *OptoRuntime::store_inline_type_fields_Type() {\n+  \/\/ create input type (domain)\n+  uint total = SharedRuntime::java_return_convention_max_int + SharedRuntime::java_return_convention_max_float*2;\n+  const Type **fields = TypeTuple::fields(total);\n+  \/\/ We don't know the number of returned values and their\n+  \/\/ types. Assume all registers available to the return convention\n+  \/\/ are used.\n+  fields[TypeFunc::Parms] = TypePtr::BOTTOM;\n+  uint i = 1;\n+  for (; i < SharedRuntime::java_return_convention_max_int; i++) {\n+    fields[TypeFunc::Parms+i] = TypeInt::INT;\n+  }\n+  for (; i < total; i+=2) {\n+    fields[TypeFunc::Parms+i] = Type::DOUBLE;\n+    fields[TypeFunc::Parms+i+1] = Type::HALF;\n+  }\n+  const TypeTuple* domain = TypeTuple::make(TypeFunc::Parms + total, fields);\n+\n+  \/\/ create result type (range)\n+  fields = TypeTuple::fields(1);\n+  fields[TypeFunc::Parms+0] = TypeInstPtr::BOTTOM;\n+\n+  const TypeTuple *range = TypeTuple::make(TypeFunc::Parms+1,fields);\n+\n+  return TypeFunc::make(domain, range);\n+}\n+\n+const TypeFunc *OptoRuntime::pack_inline_type_Type() {\n+  \/\/ create input type (domain)\n+  uint total = 1 + SharedRuntime::java_return_convention_max_int + SharedRuntime::java_return_convention_max_float*2;\n+  const Type **fields = TypeTuple::fields(total);\n+  \/\/ We don't know the number of returned values and their\n+  \/\/ types. Assume all registers available to the return convention\n+  \/\/ are used.\n+  fields[TypeFunc::Parms] = TypeRawPtr::BOTTOM;\n+  fields[TypeFunc::Parms+1] = TypeRawPtr::BOTTOM;\n+  uint i = 2;\n+  for (; i < SharedRuntime::java_return_convention_max_int+1; i++) {\n+    fields[TypeFunc::Parms+i] = TypeInt::INT;\n+  }\n+  for (; i < total; i+=2) {\n+    fields[TypeFunc::Parms+i] = Type::DOUBLE;\n+    fields[TypeFunc::Parms+i+1] = Type::HALF;\n+  }\n+  const TypeTuple* domain = TypeTuple::make(TypeFunc::Parms + total, fields);\n+\n+  \/\/ create result type (range)\n+  fields = TypeTuple::fields(1);\n+  fields[TypeFunc::Parms+0] = TypeInstPtr::NOTNULL;\n+\n+  const TypeTuple *range = TypeTuple::make(TypeFunc::Parms+1,fields);\n+\n+  return TypeFunc::make(domain, range);\n+}\n+\n+JRT_BLOCK_ENTRY(void, OptoRuntime::load_unknown_inline_C(flatArrayOopDesc* array, int index, JavaThread* current))\n+  JRT_BLOCK;\n+  oop buffer = array->obj_at(index, THREAD);\n+  deoptimize_caller_frame(current, HAS_PENDING_EXCEPTION);\n+  current->set_vm_result_oop(buffer);\n+  JRT_BLOCK_END;\n+JRT_END\n+\n+const TypeFunc* OptoRuntime::load_unknown_inline_Type() {\n+  \/\/ create input type (domain)\n+  const Type** fields = TypeTuple::fields(2);\n+  fields[TypeFunc::Parms] = TypeOopPtr::NOTNULL;\n+  fields[TypeFunc::Parms+1] = TypeInt::POS;\n+\n+  const TypeTuple* domain = TypeTuple::make(TypeFunc::Parms+2, fields);\n+\n+  \/\/ create result type (range)\n+  fields = TypeTuple::fields(1);\n+  fields[TypeFunc::Parms] = TypeInstPtr::BOTTOM;\n+\n+  const TypeTuple* range = TypeTuple::make(TypeFunc::Parms+1, fields);\n+\n+  return TypeFunc::make(domain, range);\n+}\n+\n+JRT_BLOCK_ENTRY(void, OptoRuntime::store_unknown_inline_C(instanceOopDesc* buffer, flatArrayOopDesc* array, int index, JavaThread* current))\n+  JRT_BLOCK;\n+  array->obj_at_put(index, buffer, THREAD);\n+  if (HAS_PENDING_EXCEPTION) {\n+      fatal(\"This entry must be changed to be a non-leaf entry because writing to a flat array can now throw an exception\");\n+  }\n+  JRT_BLOCK_END;\n+JRT_END\n+\n+const TypeFunc* OptoRuntime::store_unknown_inline_Type() {\n+  \/\/ create input type (domain)\n+  const Type** fields = TypeTuple::fields(3);\n+  fields[TypeFunc::Parms] = TypeInstPtr::NOTNULL;\n+  fields[TypeFunc::Parms+1] = TypeOopPtr::NOTNULL;\n+  fields[TypeFunc::Parms+2] = TypeInt::POS;\n+\n+  const TypeTuple* domain = TypeTuple::make(TypeFunc::Parms+3, fields);\n+\n+  \/\/ create result type (range)\n+  fields = TypeTuple::fields(0);\n+  const TypeTuple* range = TypeTuple::make(TypeFunc::Parms, fields);\n+\n+  return TypeFunc::make(domain, range);\n+}\n","filename":"src\/hotspot\/share\/opto\/runtime.cpp","additions":161,"deletions":15,"binary":false,"changes":176,"status":"modified"},{"patch":"@@ -128,0 +128,1 @@\n+  static const TypeFunc* _new_array_nozero_Type;\n@@ -212,1 +213,1 @@\n-  static void new_instance_C(Klass* instance_klass, JavaThread* current);\n+  static void new_instance_C(Klass* instance_klass, bool is_larval, JavaThread* current);\n@@ -215,1 +216,1 @@\n-  static void new_array_C(Klass* array_klass, int len, JavaThread* current);\n+  static void new_array_C(Klass* array_klass, int len, oopDesc* init_val, JavaThread* current);\n@@ -264,0 +265,2 @@\n+  static void load_unknown_inline_C(flatArrayOopDesc* array, int index, JavaThread* current);\n+  static void store_unknown_inline_C(instanceOopDesc* buffer, flatArrayOopDesc* array, int index, JavaThread* current);\n@@ -296,0 +299,2 @@\n+  static address load_unknown_inline_Java()              { return _load_unknown_inline_Java; }\n+  static address store_unknown_inline_Java()             { return _store_unknown_inline_Java; }\n@@ -327,1 +332,2 @@\n-    return new_array_Type();\n+    assert(_new_array_nozero_Type != nullptr, \"should be initialized\");\n+    return _new_array_nozero_Type;\n@@ -748,0 +754,6 @@\n+  static const TypeFunc* load_unknown_inline_Type();\n+  static const TypeFunc* store_unknown_inline_Type();\n+\n+  static const TypeFunc* store_inline_type_fields_Type();\n+  static const TypeFunc* pack_inline_type_Type();\n+\n","filename":"src\/hotspot\/share\/opto\/runtime.hpp","additions":15,"deletions":3,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -61,0 +61,1 @@\n+#include \"oops\/flatArrayKlass.hpp\"\n@@ -68,0 +69,1 @@\n+#include \"oops\/refArrayOop.inline.hpp\"\n@@ -417,0 +419,133 @@\n+static void validate_array_arguments(Klass* elmClass, jint len, TRAPS) {\n+  if (len < 0) {\n+    THROW_MSG(vmSymbols::java_lang_IllegalArgumentException(), \"Array length is negative\");\n+  }\n+  elmClass->initialize(CHECK);\n+  if (elmClass->is_array_klass() || elmClass->is_identity_class()) {\n+    THROW_MSG(vmSymbols::java_lang_IllegalArgumentException(), \"Element class is not a value class\");\n+  }\n+  if (elmClass->is_abstract()) {\n+    THROW_MSG(vmSymbols::java_lang_IllegalArgumentException(), \"Element class is abstract\");\n+  }\n+}\n+\n+JVM_ENTRY(jarray, JVM_CopyOfSpecialArray(JNIEnv *env, jarray orig, jint from, jint to))\n+  oop o = JNIHandles::resolve_non_null(orig);\n+  assert(o->is_array(), \"Must be\");\n+  oop array = nullptr;\n+  arrayOop org = (arrayOop)o;\n+  arrayHandle oh(THREAD, org);\n+  ObjArrayKlass* ak = ObjArrayKlass::cast(org->klass());\n+  InlineKlass* vk = InlineKlass::cast(ak->element_klass());\n+  int len = to - from;  \/\/ length of the new array\n+  if (ak->is_null_free_array_klass()) {\n+    if ((len != 0) && (from >= org->length() || to > org->length())) {\n+      THROW_MSG_NULL(vmSymbols::java_lang_IllegalArgumentException(), \"Copying of null-free array with uninitialized elements\");\n+    }\n+  }\n+  if (org->is_flatArray()) {\n+    FlatArrayKlass* fak = FlatArrayKlass::cast(org->klass());\n+    LayoutKind lk = fak->layout_kind();\n+    ArrayKlass::ArrayProperties props = ArrayKlass::array_properties_from_layout(lk);\n+    array = oopFactory::new_flatArray(vk, len, props, lk, CHECK_NULL);\n+    arrayHandle ah(THREAD, (arrayOop)array);\n+    int end = to < oh()->length() ? to : oh()->length();\n+    for (int i = from; i < end; i++) {\n+      void* src = ((flatArrayOop)oh())->value_at_addr(i, fak->layout_helper());\n+      void* dst = ((flatArrayOop)ah())->value_at_addr(i - from, fak->layout_helper());\n+      vk->copy_payload_to_addr(src, dst, lk, false);\n+    }\n+    array = ah();\n+  } else {\n+    ArrayKlass::ArrayProperties props = org->is_null_free_array() ? ArrayKlass::ArrayProperties::NULL_RESTRICTED : ArrayKlass::ArrayProperties::DEFAULT;\n+    array = oopFactory::new_objArray(vk, len, props,  CHECK_NULL);\n+    int end = to < oh()->length() ? to : oh()->length();\n+    for (int i = from; i < end; i++) {\n+      if (i < ((objArrayOop)oh())->length()) {\n+        ((objArrayOop)array)->obj_at_put(i - from, ((objArrayOop)oh())->obj_at(i));\n+      } else {\n+        assert(!ak->is_null_free_array_klass(), \"Must be a nullable array\");\n+        ((objArrayOop)array)->obj_at_put(i - from, nullptr);\n+      }\n+    }\n+  }\n+  return (jarray) JNIHandles::make_local(THREAD, array);\n+JVM_END\n+\n+JVM_ENTRY(jarray, JVM_NewNullRestrictedNonAtomicArray(JNIEnv *env, jclass elmClass, jint len, jobject initVal))\n+  oop mirror = JNIHandles::resolve_non_null(elmClass);\n+  oop init = JNIHandles::resolve(initVal);\n+  if (init == nullptr) {\n+    THROW_MSG_NULL(vmSymbols::java_lang_IllegalArgumentException(), \"Initial value cannot be null\");\n+  }\n+  Handle init_h(THREAD, init);\n+  Klass* klass = java_lang_Class::as_Klass(mirror);\n+  if (klass != init_h()->klass()) {\n+    THROW_MSG_NULL(vmSymbols::java_lang_IllegalArgumentException(), \"Type mismatch between array and initial value\");\n+  }\n+  validate_array_arguments(klass, len, CHECK_NULL);\n+  InlineKlass* vk = InlineKlass::cast(klass);\n+  ArrayKlass::ArrayProperties props = (ArrayKlass::ArrayProperties)(ArrayKlass::ArrayProperties::NON_ATOMIC | ArrayKlass::ArrayProperties::NULL_RESTRICTED);\n+  objArrayOop array = oopFactory::new_objArray(klass, len, props, CHECK_NULL);\n+  for (int i = 0; i < len; i++) {\n+    array->obj_at_put(i, init_h() \/*, CHECK_NULL*\/ );\n+  }\n+  return (jarray) JNIHandles::make_local(THREAD, array);\n+JVM_END\n+\n+JVM_ENTRY(jarray, JVM_NewNullRestrictedAtomicArray(JNIEnv *env, jclass elmClass, jint len, jobject initVal))\n+  oop mirror = JNIHandles::resolve_non_null(elmClass);\n+  oop init = JNIHandles::resolve(initVal);\n+  if (init == nullptr) {\n+    THROW_MSG_NULL(vmSymbols::java_lang_IllegalArgumentException(), \"Initial value cannot be null\");\n+  }\n+  Handle init_h(THREAD, init);\n+  Klass* klass = java_lang_Class::as_Klass(mirror);\n+  if (klass != init_h()->klass()) {\n+    THROW_MSG_NULL(vmSymbols::java_lang_IllegalArgumentException(), \"Type mismatch between array and initial value\");\n+  }\n+  validate_array_arguments(klass, len, CHECK_NULL);\n+  InlineKlass* vk = InlineKlass::cast(klass);\n+  ArrayKlass::ArrayProperties props = (ArrayKlass::ArrayProperties)(ArrayKlass::ArrayProperties::NULL_RESTRICTED);\n+  objArrayOop array = oopFactory::new_objArray(klass, len, props, CHECK_NULL);\n+  for (int i = 0; i < len; i++) {\n+    array->obj_at_put(i, init_h() \/*, CHECK_NULL*\/ );\n+  }\n+  return (jarray) JNIHandles::make_local(THREAD, array);\n+JVM_END\n+\n+JVM_ENTRY(jarray, JVM_NewNullableAtomicArray(JNIEnv *env, jclass elmClass, jint len))\n+  oop mirror = JNIHandles::resolve_non_null(elmClass);\n+  Klass* klass = java_lang_Class::as_Klass(mirror);\n+  klass->initialize(CHECK_NULL);\n+  validate_array_arguments(klass, len, CHECK_NULL);\n+  InlineKlass* vk = InlineKlass::cast(klass);\n+  ArrayKlass::ArrayProperties props = (ArrayKlass::ArrayProperties)(ArrayKlass::ArrayProperties::DEFAULT);\n+  objArrayOop array = oopFactory::new_objArray(klass, len, props, CHECK_NULL);\n+  return (jarray) JNIHandles::make_local(THREAD, array);\n+JVM_END\n+\n+JVM_ENTRY(jboolean, JVM_IsFlatArray(JNIEnv *env, jobject obj))\n+  arrayOop oop = arrayOop(JNIHandles::resolve_non_null(obj));\n+  return oop->is_flatArray();\n+JVM_END\n+\n+JVM_ENTRY(jboolean, JVM_IsNullRestrictedArray(JNIEnv *env, jobject obj))\n+  arrayOop oop = arrayOop(JNIHandles::resolve_non_null(obj));\n+  return oop->is_null_free_array();\n+JVM_END\n+\n+JVM_ENTRY(jboolean, JVM_IsAtomicArray(JNIEnv *env, jobject obj))\n+  \/\/ There are multiple cases where an array can\/must support atomic access:\n+  \/\/   - the array is a reference array\n+  \/\/   - the array uses an atomic flat layout: NULLABLE_ATOMIC_FLAT or ATOMIC_FLAT\n+  \/\/   - the array is flat and its component type is naturally atomic\n+  arrayOop oop = arrayOop(JNIHandles::resolve_non_null(obj));\n+  if (oop->is_refArray()) return true;\n+  if (oop->is_flatArray()) {\n+    FlatArrayKlass* fak = FlatArrayKlass::cast(oop->klass());\n+    if (LayoutKindHelper::is_atomic_flat(fak->layout_kind())) return true;\n+    if (fak->element_klass()->is_naturally_atomic()) return true;\n+  }\n+  return false;\n+JVM_END\n@@ -625,2 +760,23 @@\n-  return handle == nullptr ? 0 :\n-         checked_cast<jint>(ObjectSynchronizer::FastHashCode (THREAD, JNIHandles::resolve_non_null(handle)));\n+  if (handle == nullptr) {\n+    return 0;\n+  }\n+  oop obj = JNIHandles::resolve_non_null(handle);\n+  if (EnableValhalla && obj->klass()->is_inline_klass()) {\n+      JavaValue result(T_INT);\n+      JavaCallArguments args;\n+      Handle ho(THREAD, obj);\n+      args.push_oop(ho);\n+      methodHandle method(THREAD, UseAltSubstitutabilityMethod\n+              ? Universe::value_object_hash_codeAlt_method() : Universe::value_object_hash_code_method());\n+      JavaCalls::call(&result, method, &args, THREAD);\n+      if (HAS_PENDING_EXCEPTION) {\n+        if (!PENDING_EXCEPTION->is_a(vmClasses::Error_klass())) {\n+          Handle e(THREAD, PENDING_EXCEPTION);\n+          CLEAR_PENDING_EXCEPTION;\n+          THROW_MSG_CAUSE_(vmSymbols::java_lang_InternalError(), \"Internal error in hashCode\", e, false);\n+        }\n+      }\n+      return result.get_jint();\n+  } else {\n+    return checked_cast<jint>(ObjectSynchronizer::FastHashCode(THREAD, obj));\n+  }\n@@ -674,0 +830,6 @@\n+  if (klass->is_inline_klass()) {\n+    \/\/ Value instances have no identity, so return the current instance instead of allocating a new one\n+    \/\/ Value classes cannot have finalizers, so the method can return immediately\n+    return JNIHandles::make_local(THREAD, obj());\n+  }\n+\n@@ -1166,1 +1328,2 @@\n-    size = InstanceKlass::cast(klass)->local_interfaces()->length();\n+    InstanceKlass* ik = InstanceKlass::cast(klass);\n+    size = ik->local_interfaces()->length();\n@@ -1200,1 +1363,0 @@\n-\n@@ -1681,1 +1843,1 @@\n-    if (want_constructor && !method->is_object_initializer()) {\n+    if (want_constructor && !method->is_object_constructor()) {\n@@ -1685,1 +1847,1 @@\n-        (method->is_object_initializer() || method->is_static_initializer() ||\n+        (method->is_object_constructor() || method->is_class_initializer() ||\n@@ -1713,0 +1875,1 @@\n+        assert(method->is_object_constructor(), \"must be\");\n@@ -1973,1 +2136,1 @@\n-  if (m->is_object_initializer()) {\n+  if (m->is_object_constructor()) {\n@@ -1976,1 +2139,0 @@\n-    \/\/ new_method accepts <clinit> as Method here\n@@ -2423,1 +2585,1 @@\n-  return method->name() == vmSymbols::object_initializer_name();\n+  return method->is_object_constructor();\n@@ -3217,0 +3379,4 @@\n+JVM_LEAF(jboolean, JVM_IsValhallaEnabled(void))\n+  return EnableValhalla ? JNI_TRUE : JNI_FALSE;\n+JVM_END\n+\n@@ -3296,1 +3462,3 @@\n-    objArrayHandle args(THREAD, objArrayOop(JNIHandles::resolve(args0)));\n+    objArrayHandle args(THREAD, (objArrayOop)JNIHandles::resolve(args0));\n+    assert(args() == nullptr || !args->is_flatArray(), \"args are never flat or are they???\");\n+\n@@ -3316,0 +3484,2 @@\n+  objArrayHandle args(THREAD, (objArrayOop)JNIHandles::resolve(args0));\n+  assert(args() == nullptr || !args->is_flatArray(), \"args are never flat or are they???\");\n@@ -3317,1 +3487,0 @@\n-  objArrayHandle args(THREAD, objArrayOop(JNIHandles::resolve(args0)));\n@@ -3554,0 +3723,1 @@\n+  refArrayHandle rah(THREAD, (refArrayOop)ah()); \/\/ j.l.Thread is an identity class, arrays are always reference arrays\n@@ -3559,1 +3729,1 @@\n-    oop thread_obj = ah->obj_at(i);\n+    oop thread_obj = rah->obj_at(i);\n","filename":"src\/hotspot\/share\/prims\/jvm.cpp","additions":182,"deletions":12,"binary":false,"changes":194,"status":"modified"},{"patch":"@@ -2732,4 +2732,0 @@\n-  if (!java_lang_Class::is_primitive(k_mirror)) {\n-    \/\/ Reset the deleted  ACC_SUPER bit (deleted in compute_modifier_flags()).\n-    result |= JVM_ACC_SUPER;\n-  }\n@@ -2853,1 +2849,2 @@\n-                                                     flds.access_flags().is_static());\n+                                                     flds.access_flags().is_static(),\n+                                                     flds.field_descriptor().is_flat());\n@@ -2891,2 +2888,3 @@\n-    Array<InstanceKlass*>* interface_list = InstanceKlass::cast(k)->local_interfaces();\n-    const int result_length = (interface_list == nullptr ? 0 : interface_list->length());\n+    InstanceKlass* ik = InstanceKlass::cast(k);\n+    Array<InstanceKlass*>* interface_list = ik->local_interfaces();\n+    int result_length = (interface_list == nullptr ? 0 : interface_list->length());\n@@ -3088,3 +3086,6 @@\n-  {\n-    jint result = (jint) mirror->identity_hash();\n-    *hash_code_ptr = result;\n+  if (mirror->is_inline_type()) {\n+    \/\/ For inline types, use the klass as a hash code.\n+    \/\/ TBD to improve this (see also JvmtiTagMapKey::get_hash for similar case).\n+    *hash_code_ptr = (jint)((int64_t)mirror->klass() >> 3);\n+  } else {\n+    *hash_code_ptr = (jint)mirror->identity_hash();\n","filename":"src\/hotspot\/share\/prims\/jvmtiEnv.cpp","additions":11,"deletions":10,"binary":false,"changes":21,"status":"modified"},{"patch":"@@ -74,0 +74,61 @@\n+\n+\/\/ Helper class to store objects to visit.\n+class JvmtiHeapwalkVisitStack {\n+private:\n+  enum {\n+    initial_visit_stack_size = 4000\n+  };\n+\n+  GrowableArray<JvmtiHeapwalkObject>* _visit_stack;\n+  JVMTIBitSet _bitset;\n+\n+  static GrowableArray<JvmtiHeapwalkObject>* create_visit_stack() {\n+    return new (mtServiceability) GrowableArray<JvmtiHeapwalkObject>(initial_visit_stack_size, mtServiceability);\n+  }\n+\n+public:\n+  JvmtiHeapwalkVisitStack(): _visit_stack(create_visit_stack()) {\n+  }\n+  ~JvmtiHeapwalkVisitStack() {\n+    if (_visit_stack != nullptr) {\n+      delete _visit_stack;\n+    }\n+  }\n+\n+  bool is_empty() const {\n+    return _visit_stack->is_empty();\n+  }\n+\n+  void push(const JvmtiHeapwalkObject& obj) {\n+    _visit_stack->push(obj);\n+  }\n+\n+  \/\/ If the object hasn't been visited then push it onto the visit stack\n+  \/\/ so that it will be visited later.\n+  void check_for_visit(const JvmtiHeapwalkObject& obj) {\n+    if (!is_visited(obj)) {\n+      _visit_stack->push(obj);\n+    }\n+  }\n+\n+  JvmtiHeapwalkObject pop() {\n+    return _visit_stack->pop();\n+  }\n+\n+  bool is_visited(const JvmtiHeapwalkObject& obj) \/*const*\/ { \/\/ TODO: _bitset.is_marked() should be const\n+    \/\/ The method is called only for objects from visit_stack to ensure an object is not visited twice.\n+    \/\/ Flat objects can be added to visit_stack only when we visit their holder object, so we cannot get duplicate reference to it.\n+    if (obj.is_flat()) {\n+      return false;\n+    }\n+    return _bitset.is_marked(obj.obj());\n+  }\n+\n+  void mark_visited(const JvmtiHeapwalkObject& obj) {\n+    if (!obj.is_flat()) {\n+      _bitset.mark_obj(obj.obj());\n+    }\n+  }\n+};\n+\n+\n@@ -81,1 +142,2 @@\n-  _posting_events(false) {\n+  _posting_events(false),\n+  _converting_flat_object(false) {\n@@ -87,0 +149,1 @@\n+  _flat_hashmap = new JvmtiFlatTagMapTable();\n@@ -102,0 +165,1 @@\n+  delete _flat_hashmap;\n@@ -110,0 +174,1 @@\n+  _flat_hashmap->clear();\n@@ -128,6 +193,1 @@\n-\/\/ iterate over all entries in the tag map.\n-void JvmtiTagMap::entry_iterate(JvmtiTagMapKeyClosure* closure) {\n-  hashmap()->entry_iterate(closure);\n-}\n-\n-bool JvmtiTagMap::is_empty() {\n+bool JvmtiTagMap::is_empty() const {\n@@ -136,1 +196,1 @@\n-  return hashmap()->is_empty();\n+  return _hashmap->is_empty() && _flat_hashmap->is_empty();\n@@ -170,5 +230,170 @@\n-\/\/ Return the tag value for an object, or 0 if the object is\n-\/\/ not tagged\n-\/\/\n-static inline jlong tag_for(JvmtiTagMap* tag_map, oop o) {\n-  return tag_map->hashmap()->find(o);\n+\/\/ Converts entries from JvmtiFlatTagMapTable to JvmtiTagMapTable in batches.\n+\/\/   1. (JvmtiTagMap is locked)\n+\/\/      reads entries from JvmtiFlatTagMapTable (describe flat value objects);\n+\/\/   2. (JvmtiTagMap is unlocked)\n+\/\/      creates heap-allocated copies of the flat object;\n+\/\/   3. (JvmtiTagMap is locked)\n+\/\/      ensures source entry still exists, removes it from JvmtiFlatTagMapTable, adds new entry to JvmtiTagMapTable.\n+\/\/ If some error occurs in step 2 (OOM?), the process stops.\n+class JvmtiTagMapFlatEntryConverter: public StackObj {\n+private:\n+  struct Entry {\n+    \/\/ source flat value object\n+    Handle holder;\n+    int offset;\n+    InlineKlass* inline_klass;\n+    LayoutKind layout_kind;\n+    \/\/ converted heap-allocated object\n+    Handle dst;\n+\n+    Entry(): holder(), offset(0), inline_klass(nullptr), dst() {}\n+    Entry(Handle holder, int offset, InlineKlass* inline_klass, LayoutKind lk)\n+      : holder(holder), offset(offset), inline_klass(inline_klass), layout_kind(lk), dst() {}\n+  };\n+\n+  int _batch_size;\n+  GrowableArray<Entry> _entries;\n+  bool _has_error;\n+\n+public:\n+  JvmtiTagMapFlatEntryConverter(int batch_size): _batch_size(batch_size), _entries(batch_size, mtServiceability), _has_error(false) { }\n+  ~JvmtiTagMapFlatEntryConverter() {}\n+\n+  \/\/ returns false if there is nothing to convert\n+  bool import_entries(JvmtiFlatTagMapTable* table) {\n+    if (_has_error) {\n+      \/\/ stop the process to avoid infinite loop\n+      return false;\n+    }\n+\n+    class Importer: public JvmtiFlatTagMapKeyClosure {\n+    private:\n+      GrowableArray<Entry>& _entries;\n+      int _batch_size;\n+    public:\n+      Importer(GrowableArray<Entry>& entries, int batch_size): _entries(entries), _batch_size(batch_size) {}\n+\n+      bool do_entry(JvmtiFlatTagMapKey& key, jlong& tag) {\n+        Entry entry(Handle(Thread::current(), key.holder()), key.offset(), key.inline_klass(), key.layout_kind());\n+        _entries.append(entry);\n+\n+        return _entries.length() < _batch_size;\n+      }\n+    } importer(_entries, _batch_size);\n+    table->entry_iterate(&importer);\n+\n+    return !_entries.is_empty();\n+  }\n+\n+  void convert() {\n+    for (int i = 0; i < _entries.length(); i++) {\n+      EXCEPTION_MARK;\n+      Entry& entry = _entries.at(i);\n+      oop obj = entry.inline_klass->read_payload_from_addr(entry.holder(), entry.offset, entry.layout_kind, JavaThread::current());\n+\n+      if (HAS_PENDING_EXCEPTION) {\n+        tty->print_cr(\"Exception in JvmtiTagMapFlatEntryConverter: \");\n+        java_lang_Throwable::print(PENDING_EXCEPTION, tty);\n+        tty->cr();\n+        CLEAR_PENDING_EXCEPTION;\n+        \/\/ stop the conversion\n+        _has_error = true;\n+      } else {\n+        entry.dst = Handle(Thread::current(), obj);\n+      }\n+    }\n+  }\n+\n+  \/\/ returns number of converted entries\n+  int move(JvmtiFlatTagMapTable* src_table, JvmtiTagMapTable* dst_table) {\n+    int count = 0;\n+    for (int i = 0; i < _entries.length(); i++) {\n+      Entry& entry = _entries.at(i);\n+      if (entry.dst() == nullptr) {\n+        \/\/ some error during conversion, skip the entry\n+        continue;\n+      }\n+      JvmtiHeapwalkObject obj(entry.holder(), entry.offset, entry.inline_klass, entry.layout_kind);\n+      jlong tag = src_table->remove(obj);\n+\n+      if (tag != 0) { \/\/ ensure the entry is still in the src_table\n+        dst_table->add(entry.dst(), tag);\n+        count++;\n+      } else {\n+\n+      }\n+    }\n+    \/\/ and clean the array\n+    _entries.clear();\n+    return count;\n+  }\n+};\n+\n+\n+void JvmtiTagMap::convert_flat_object_entries() {\n+  Thread* current = Thread::current();\n+  assert(current->is_Java_thread(), \"must be executed on JavaThread\");\n+\n+  log_debug(jvmti, table)(\"convert_flat_object_entries, main table size = %d, flat table size = %d\",\n+                          _hashmap->number_of_entries(), _flat_hashmap->number_of_entries());\n+\n+  {\n+    MonitorLocker ml(lock(), Mutex::_no_safepoint_check_flag);\n+    \/\/ If another thread is converting, let it finish.\n+    while (_converting_flat_object) {\n+      ml.wait();\n+    }\n+    if (_flat_hashmap->is_empty()) {\n+      \/\/ nothing to convert\n+      return;\n+    }\n+    _converting_flat_object = true;\n+  }\n+\n+  const int BATCH_SIZE = 1024;\n+  JvmtiTagMapFlatEntryConverter converter(BATCH_SIZE);\n+\n+  int count = 0;\n+  while (true) {\n+    HandleMark hm(current);\n+    {\n+      MonitorLocker ml(lock(), Mutex::_no_safepoint_check_flag);\n+      if (!converter.import_entries(_flat_hashmap)) {\n+        break;\n+      }\n+    }\n+    \/\/ Convert flat objects to heap-allocated without table lock (so agent callbacks can get\/set tags).\n+    converter.convert();\n+    {\n+      MonitorLocker ml(lock(), Mutex::_no_safepoint_check_flag);\n+      count += converter.move(_flat_hashmap, _hashmap);\n+    }\n+  }\n+\n+  log_info(jvmti, table)(\"%d flat value objects are converted, flat table size = %d\",\n+                         count, _flat_hashmap->number_of_entries());\n+  {\n+    MonitorLocker ml(lock(), Mutex::_no_safepoint_check_flag);\n+    _converting_flat_object = false;\n+    ml.notify_all();\n+  }\n+}\n+\n+jlong JvmtiTagMap::find(const JvmtiHeapwalkObject& obj) const {\n+  jlong tag = _hashmap->find(obj);\n+  if (tag == 0 && obj.is_value()) {\n+    tag = _flat_hashmap->find(obj);\n+  }\n+  return tag;\n+}\n+\n+void JvmtiTagMap::add(const JvmtiHeapwalkObject& obj, jlong tag) {\n+  if (obj.is_flat()) {\n+    \/\/ we may have tag for equal (non-flat) object in _hashmap, try to update it 1st\n+    if (!_hashmap->update(obj, tag)) {\n+      \/\/ no entry in _hashmap, add to _flat_hashmap\n+      _flat_hashmap->add(obj, tag);\n+    }\n+  } else {\n+    _hashmap->add(obj, tag);\n+  }\n@@ -177,0 +402,9 @@\n+void JvmtiTagMap::remove(const JvmtiHeapwalkObject& obj) {\n+  if (!_hashmap->remove(obj)) {\n+    if (obj.is_value()) {\n+      _flat_hashmap->remove(obj);\n+    }\n+  }\n+}\n+\n+\n@@ -195,2 +429,1 @@\n-  JvmtiTagMapTable* _hashmap;\n-  oop _o;\n+  const JvmtiHeapwalkObject& _o;\n@@ -205,2 +438,2 @@\n-  void inline post_callback_tag_update(oop o, JvmtiTagMapTable* hashmap,\n-                                       jlong obj_tag);\n+  void inline post_callback_tag_update(const JvmtiHeapwalkObject& o, JvmtiTagMap* tag_map, jlong obj_tag);\n+\n@@ -208,1 +441,3 @@\n-  CallbackWrapper(JvmtiTagMap* tag_map, oop o) {\n+  CallbackWrapper(JvmtiTagMap* tag_map, const JvmtiHeapwalkObject& o)\n+    : _tag_map(tag_map), _o(o)\n+  {\n@@ -212,8 +447,8 @@\n-    \/\/ object to tag\n-    _o = o;\n-\n-    _obj_size = (jlong)_o->size() * wordSize;\n-\n-    \/\/ record the context\n-    _tag_map = tag_map;\n-    _hashmap = tag_map->hashmap();\n+    if (!o.is_flat()) {\n+      \/\/ common case: we have oop\n+      _obj_size = (jlong)o.obj()->size() * wordSize;\n+    } else {\n+      \/\/ flat value object, we know its InstanceKlass\n+      assert(_o.inline_klass() != nullptr, \"must be\");\n+      _obj_size = _o.inline_klass()->size() * wordSize;;\n+    }\n@@ -223,1 +458,1 @@\n-    _obj_tag = _hashmap->find(_o);\n+    _obj_tag = _tag_map->find(_o);\n@@ -228,1 +463,1 @@\n-    _klass_tag = tag_for(tag_map, _o->klass()->java_mirror());\n+    _klass_tag = _tag_map->find(_o.klass()->java_mirror());\n@@ -232,1 +467,1 @@\n-    post_callback_tag_update(_o, _hashmap, _obj_tag);\n+    post_callback_tag_update(_o, _tag_map, _obj_tag);\n@@ -242,2 +477,2 @@\n-void inline CallbackWrapper::post_callback_tag_update(oop o,\n-                                                      JvmtiTagMapTable* hashmap,\n+void inline CallbackWrapper::post_callback_tag_update(const JvmtiHeapwalkObject& o,\n+                                                      JvmtiTagMap* tag_map,\n@@ -247,1 +482,1 @@\n-    hashmap->remove(o);\n+    tag_map->remove(o);\n@@ -252,1 +487,1 @@\n-    hashmap->add(o, obj_tag);\n+    tag_map->add(o, obj_tag);\n@@ -274,0 +509,1 @@\n+  const JvmtiHeapwalkObject& _referrer;\n@@ -275,2 +511,0 @@\n-  JvmtiTagMapTable* _referrer_hashmap;\n-  oop _referrer;\n@@ -284,2 +518,2 @@\n-  TwoOopCallbackWrapper(JvmtiTagMap* tag_map, oop referrer, oop o) :\n-    CallbackWrapper(tag_map, o)\n+  TwoOopCallbackWrapper(JvmtiTagMap* tag_map, const JvmtiHeapwalkObject& referrer, const JvmtiHeapwalkObject& o) :\n+    CallbackWrapper(tag_map, o), _referrer(referrer)\n@@ -294,5 +528,1 @@\n-      _referrer = referrer;\n-      \/\/ record the context\n-      _referrer_hashmap = tag_map->hashmap();\n-\n-      _referrer_obj_tag = _referrer_hashmap->find(_referrer);\n+      _referrer_obj_tag = tag_map->find(_referrer);\n@@ -304,1 +534,1 @@\n-      _referrer_klass_tag = tag_for(tag_map, _referrer->klass()->java_mirror());\n+      _referrer_klass_tag = tag_map->find(_referrer.klass()->java_mirror());\n@@ -311,1 +541,1 @@\n-                               _referrer_hashmap,\n+                               tag_map(),\n@@ -339,3 +569,1 @@\n-\n-  JvmtiTagMapTable* hashmap = _hashmap;\n-\n+  JvmtiHeapwalkObject obj(o);\n@@ -345,1 +573,1 @@\n-    hashmap->remove(o);\n+    _hashmap->remove(obj);\n@@ -349,1 +577,1 @@\n-    hashmap->add(o, tag);\n+    add(obj, tag);\n@@ -365,1 +593,1 @@\n-  return tag_for(this, o);\n+  return find(o);\n@@ -378,0 +606,2 @@\n+  InlineKlass* _inline_klass; \/\/ nullptr for heap object\n+  LayoutKind _layout_kind;\n@@ -379,2 +609,12 @@\n-  ClassFieldDescriptor(int index, char type, int offset) :\n-    _field_index(index), _field_offset(offset), _field_type(type) {\n+  ClassFieldDescriptor(int index, const FieldStreamBase& fld) :\n+      _field_index(index), _field_offset(fld.offset()), _field_type(fld.signature()->char_at(0)) {\n+    if (fld.is_flat()) {\n+      const fieldDescriptor& fd = fld.field_descriptor();\n+      InstanceKlass* holder_klass = fd.field_holder();\n+      InlineLayoutInfo* layout_info = holder_klass->inline_layout_info_adr(fd.index());\n+      _inline_klass = layout_info->klass();\n+      _layout_kind = layout_info->kind();\n+    } else {\n+      _inline_klass = nullptr;\n+      _layout_kind = LayoutKind::REFERENCE;\n+    }\n@@ -385,0 +625,3 @@\n+  bool is_flat()     const  { return _inline_klass != nullptr; }\n+  InlineKlass* inline_klass() const { return _inline_klass; }\n+  LayoutKind layout_kind() const { return _layout_kind; }\n@@ -403,1 +646,1 @@\n-  void add(int index, char type, int offset);\n+  void add(int index, const FieldStreamBase& fld);\n@@ -414,1 +657,1 @@\n-  static ClassFieldMap* create_map_of_instance_fields(oop obj);\n+  static ClassFieldMap* create_map_of_instance_fields(Klass* k);\n@@ -439,2 +682,2 @@\n-void ClassFieldMap::add(int index, char type, int offset) {\n-  ClassFieldDescriptor* field = new ClassFieldDescriptor(index, type, offset);\n+void ClassFieldMap::add(int index, const FieldStreamBase& fld) {\n+  ClassFieldDescriptor* field = new ClassFieldDescriptor(index, fld);\n@@ -464,1 +707,1 @@\n-    field_map->add(index, fld.signature()->char_at(0), fld.offset());\n+    field_map->add(index, fld);\n@@ -473,2 +716,2 @@\n-ClassFieldMap* ClassFieldMap::create_map_of_instance_fields(oop obj) {\n-  InstanceKlass* ik = InstanceKlass::cast(obj->klass());\n+ClassFieldMap* ClassFieldMap::create_map_of_instance_fields(Klass* k) {\n+  InstanceKlass* ik = InstanceKlass::cast(k);\n@@ -493,1 +736,1 @@\n-      field_map->add(start_index + index, fld.signature()->char_at(0), fld.offset());\n+      field_map->add(start_index + index, fld);\n@@ -523,1 +766,1 @@\n-  \/\/ returns the field map for a given object (returning map cached\n+  \/\/ returns the field map for a given klass (returning map cached\n@@ -525,1 +768,1 @@\n-  static ClassFieldMap* get_map_of_instance_fields(oop obj);\n+  static ClassFieldMap* get_map_of_instance_fields(Klass* k);\n@@ -577,1 +820,1 @@\n-\/\/ returns the instance field map for the given object\n+\/\/ returns the instance field map for the given klass\n@@ -579,1 +822,1 @@\n-ClassFieldMap* JvmtiCachedClassFieldMap::get_map_of_instance_fields(oop obj) {\n+ClassFieldMap* JvmtiCachedClassFieldMap::get_map_of_instance_fields(Klass *k) {\n@@ -583,1 +826,0 @@\n-  Klass* k = obj->klass();\n@@ -592,1 +834,1 @@\n-    ClassFieldMap* field_map = ClassFieldMap::create_map_of_instance_fields(obj);\n+    ClassFieldMap* field_map = ClassFieldMap::create_map_of_instance_fields(k);\n@@ -644,1 +886,1 @@\n-static inline bool is_filtered_by_klass_filter(oop obj, Klass* klass_filter) {\n+static inline bool is_filtered_by_klass_filter(const JvmtiHeapwalkObject& obj, Klass* klass_filter) {\n@@ -646,1 +888,1 @@\n-    if (obj->klass() != klass_filter) {\n+    if (obj.klass() != klass_filter) {\n@@ -677,1 +919,1 @@\n-                                         oop str,\n+                                         const JvmtiHeapwalkObject& obj,\n@@ -680,0 +922,2 @@\n+  assert(!obj.is_flat(), \"cannot be flat\");\n+  oop str = obj.obj();\n@@ -728,1 +972,1 @@\n-                                                  oop obj,\n+                                                  const JvmtiHeapwalkObject& obj,\n@@ -731,1 +975,2 @@\n-  assert(obj->is_typeArray(), \"not a primitive array\");\n+  assert(!obj.is_flat(), \"cannot be flat\");\n+  assert(obj.obj()->is_typeArray(), \"not a primitive array\");\n@@ -734,1 +979,1 @@\n-  typeArrayOop array = typeArrayOop(obj);\n+  typeArrayOop array = typeArrayOop(obj.obj());\n@@ -824,1 +1069,1 @@\n-  oop obj,\n+  const JvmtiHeapwalkObject& obj,\n@@ -832,1 +1077,1 @@\n-  ClassFieldMap* fields = JvmtiCachedClassFieldMap::get_map_of_instance_fields(obj);\n+  ClassFieldMap* fields = JvmtiCachedClassFieldMap::get_map_of_instance_fields(obj.klass());\n@@ -846,3 +1091,2 @@\n-    \/\/ get offset and field value\n-    int offset = field->field_offset();\n-    address addr = cast_from_oop<address>(obj) + offset;\n+    \/\/ get field value\n+    address addr = cast_from_oop<address>(obj.obj()) + obj.offset() + field->field_offset();\n@@ -962,1 +1206,2 @@\n-  CallbackWrapper wrapper(tag_map(), o);\n+  JvmtiHeapwalkObject wrapper_obj(o);\n+  CallbackWrapper wrapper(tag_map(), wrapper_obj);\n@@ -1014,0 +1259,4 @@\n+  void visit_object(const JvmtiHeapwalkObject& obj);\n+  void visit_flat_fields(const JvmtiHeapwalkObject& obj);\n+  void visit_flat_array_elements(const JvmtiHeapwalkObject& obj);\n+\n@@ -1029,1 +1278,1 @@\n-  void do_object(oop o);\n+  void do_object(oop obj);\n@@ -1038,4 +1287,1 @@\n-  \/\/ apply class filter\n-  if (is_filtered_by_klass_filter(obj, klass())) return;\n-\n-  if (obj->klass()->java_mirror() == nullptr) {\n+  if (obj != nullptr && obj->klass()->java_mirror() == nullptr) {\n@@ -1048,0 +1294,7 @@\n+  visit_object(obj);\n+}\n+\n+void IterateThroughHeapObjectClosure::visit_object(const JvmtiHeapwalkObject& obj) {\n+  \/\/ apply class filter\n+  if (is_filtered_by_klass_filter(obj, klass())) return;\n+\n@@ -1057,2 +1310,2 @@\n-  bool is_array = obj->is_array();\n-  int len = is_array ? arrayOop(obj)->length() : -1;\n+  bool is_array = obj.klass()->is_array_klass();\n+  int len = is_array ? arrayOop(obj.obj())->length() : -1;\n@@ -1072,1 +1325,1 @@\n-  if (callbacks()->primitive_field_callback != nullptr && obj->is_instance()) {\n+  if (callbacks()->primitive_field_callback != nullptr && obj.klass()->is_instance_klass()) {\n@@ -1075,1 +1328,2 @@\n-    if (obj->klass() == vmClasses::Class_klass()) {\n+    if (obj.klass() == vmClasses::Class_klass()) {\n+      assert(!obj.is_flat(), \"Class object cannot be flattened\");\n@@ -1077,3 +1331,3 @@\n-                                                                    obj,\n-                                                                    cb,\n-                                                                    (void*)user_data());\n+                                                              obj.obj(),\n+                                                              cb,\n+                                                              (void*)user_data());\n@@ -1082,3 +1336,3 @@\n-                                                                      obj,\n-                                                                      cb,\n-                                                                      (void*)user_data());\n+                                                                obj,\n+                                                                cb,\n+                                                                (void*)user_data());\n@@ -1092,1 +1346,1 @@\n-      obj->klass() == vmClasses::String_klass()) {\n+      obj.klass() == vmClasses::String_klass()) {\n@@ -1097,1 +1351,1 @@\n-                (void*)user_data() );\n+                (void*)user_data());\n@@ -1104,1 +1358,1 @@\n-      obj->is_typeArray()) {\n+      obj.klass()->is_typeArray_klass()) {\n@@ -1109,1 +1363,1 @@\n-               (void*)user_data() );\n+               (void*)user_data());\n@@ -1112,1 +1366,80 @@\n-};\n+  \/\/ All info for the object is reported.\n+\n+  \/\/ If the object has flat fields, report them as heap objects.\n+  if (obj.klass()->is_instance_klass()) {\n+    if (InstanceKlass::cast(obj.klass())->has_inline_type_fields()) {\n+      visit_flat_fields(obj);\n+      \/\/ check if iteration has been halted\n+      if (is_iteration_aborted()) {\n+        return;\n+      }\n+    }\n+  }\n+  \/\/ If the object is flat array, report all elements as heap objects.\n+  if (is_array && obj.obj()->is_flatArray()) {\n+    assert(!obj.is_flat(), \"Array object cannot be flattened\");\n+    visit_flat_array_elements(obj);\n+  }\n+}\n+\n+void IterateThroughHeapObjectClosure::visit_flat_fields(const JvmtiHeapwalkObject& obj) {\n+  \/\/ iterate over instance fields\n+  ClassFieldMap* fields = JvmtiCachedClassFieldMap::get_map_of_instance_fields(obj.klass());\n+  for (int i = 0; i < fields->field_count(); i++) {\n+    ClassFieldDescriptor* field = fields->field_at(i);\n+    \/\/ skip non-flat and (for safety) primitive fields\n+    if (!field->is_flat() || is_primitive_field_type(field->field_type())) {\n+      continue;\n+    }\n+\n+    int field_offset = field->field_offset();\n+    if (obj.is_flat()) {\n+      \/\/ the object is inlined, its fields are stored without the header\n+      field_offset += obj.offset() - obj.inline_klass()->payload_offset();\n+    }\n+    \/\/ check for possible nulls\n+    if (LayoutKindHelper::is_nullable_flat(field->layout_kind())) {\n+      address payload = cast_from_oop<address>(obj.obj()) + field_offset;\n+      if (field->inline_klass()->is_payload_marked_as_null(payload)) {\n+        continue;\n+      }\n+    }\n+    JvmtiHeapwalkObject field_obj(obj.obj(), field_offset, field->inline_klass(), field->layout_kind());\n+\n+    visit_object(field_obj);\n+\n+    \/\/ check if iteration has been halted\n+    if (is_iteration_aborted()) {\n+      return;\n+    }\n+  }\n+}\n+\n+void IterateThroughHeapObjectClosure::visit_flat_array_elements(const JvmtiHeapwalkObject& obj) {\n+  assert(!obj.is_flat() && obj.obj()->is_flatArray() , \"sanity check\");\n+  flatArrayOop array = flatArrayOop(obj.obj());\n+  FlatArrayKlass* faklass = FlatArrayKlass::cast(array->klass());\n+  InlineKlass* vk = InlineKlass::cast(faklass->element_klass());\n+  bool need_null_check = LayoutKindHelper::is_nullable_flat(faklass->layout_kind());\n+\n+  for (int index = 0; index < array->length(); index++) {\n+    address addr = (address)array->value_at_addr(index, faklass->layout_helper());\n+    \/\/ check for null\n+    if (need_null_check) {\n+      if (vk->is_payload_marked_as_null(addr)) {\n+        continue;\n+      }\n+    }\n+\n+    \/\/ offset in the array oop\n+    int offset = (int)(addr - cast_from_oop<address>(array));\n+    JvmtiHeapwalkObject elem(obj.obj(), offset, vk, faklass->layout_kind());\n+\n+    visit_object(elem);\n+\n+    \/\/ check if iteration has been halted\n+    if (is_iteration_aborted()) {\n+      return;\n+    }\n+  }\n+}\n@@ -1138,0 +1471,2 @@\n+  convert_flat_object_entries();\n+\n@@ -1165,0 +1500,2 @@\n+  convert_flat_object_entries();\n+\n@@ -1178,1 +1515,1 @@\n-    hashmap()->remove_dead_entries(objects);\n+    _hashmap->remove_dead_entries(objects);\n@@ -1331,0 +1668,3 @@\n+  \/\/ ensure flat object conversion is completed\n+  convert_flat_object_entries();\n+\n@@ -1339,1 +1679,1 @@\n-    entry_iterate(&collector);\n+    _hashmap->entry_iterate(&collector);\n@@ -1379,1 +1719,1 @@\n-  oop _last_referrer;\n+  JvmtiHeapwalkObject _last_referrer;\n@@ -1392,1 +1732,1 @@\n-    _last_referrer(nullptr),\n+    _last_referrer(),\n@@ -1401,2 +1741,2 @@\n-  oop last_referrer() const               { return _last_referrer; }\n-  void set_last_referrer(oop referrer)    { _last_referrer = referrer; }\n+  JvmtiHeapwalkObject last_referrer() const    { return _last_referrer; }\n+  void set_last_referrer(const JvmtiHeapwalkObject& referrer) { _last_referrer = referrer; }\n@@ -1475,2 +1815,1 @@\n-  static GrowableArray<oop>* _visit_stack;\n-  static JVMTIBitSet* _bitset;\n+  static JvmtiHeapwalkVisitStack* _visit_stack;\n@@ -1481,1 +1820,1 @@\n-  static GrowableArray<oop>* visit_stack()             { return _visit_stack; }\n+  static JvmtiHeapwalkVisitStack* visit_stack()        { return _visit_stack; }\n@@ -1485,2 +1824,2 @@\n-  static inline bool check_for_visit(oop obj) {\n-    if (!_bitset->is_marked(obj)) visit_stack()->push(obj);\n+  static inline bool check_for_visit(const JvmtiHeapwalkObject&obj) {\n+    visit_stack()->check_for_visit(obj);\n@@ -1490,0 +1829,10 @@\n+  \/\/ return element count if the obj is array, -1 otherwise\n+  static jint get_array_length(const JvmtiHeapwalkObject& obj) {\n+    if (!obj.klass()->is_array_klass()) {\n+      return -1;\n+    }\n+    assert(!obj.is_flat(), \"array cannot be flat\");\n+    return (jint)arrayOop(obj.obj())->length();\n+  }\n+\n+\n@@ -1492,1 +1841,1 @@\n-    (jvmtiHeapRootKind root_kind, oop obj);\n+    (jvmtiHeapRootKind root_kind, const JvmtiHeapwalkObject& obj);\n@@ -1495,1 +1844,1 @@\n-     int slot, oop obj);\n+     int slot, const JvmtiHeapwalkObject& obj);\n@@ -1497,1 +1846,1 @@\n-    (jvmtiObjectReferenceKind ref_kind, oop referrer, oop referree, jint index);\n+    (jvmtiObjectReferenceKind ref_kind, const JvmtiHeapwalkObject& referrer, const JvmtiHeapwalkObject& referree, jint index);\n@@ -1501,1 +1850,1 @@\n-    (jvmtiHeapReferenceKind ref_kind, oop obj);\n+    (jvmtiHeapReferenceKind ref_kind, const JvmtiHeapwalkObject& obj);\n@@ -1504,1 +1853,1 @@\n-     jmethodID method, jlocation bci, jint slot, oop obj);\n+     jmethodID method, jlocation bci, jint slot, const JvmtiHeapwalkObject& obj);\n@@ -1506,1 +1855,1 @@\n-    (jvmtiHeapReferenceKind ref_kind, oop referrer, oop referree, jint index);\n+    (jvmtiHeapReferenceKind ref_kind, const JvmtiHeapwalkObject& referrer, const JvmtiHeapwalkObject& referree, jint index);\n@@ -1510,1 +1859,1 @@\n-    (jvmtiHeapReferenceKind ref_kind, oop obj, jint index, address addr, char type);\n+    (jvmtiHeapReferenceKind ref_kind, const JvmtiHeapwalkObject& obj, jint index, address addr, char type);\n@@ -1515,1 +1864,0 @@\n-                                             GrowableArray<oop>* visit_stack,\n@@ -1518,1 +1866,1 @@\n-                                             JVMTIBitSet* bitset);\n+                                             JvmtiHeapwalkVisitStack* visit_stack);\n@@ -1522,1 +1870,0 @@\n-                                                GrowableArray<oop>* visit_stack,\n@@ -1525,1 +1872,1 @@\n-                                                JVMTIBitSet* bitset);\n+                                                JvmtiHeapwalkVisitStack* visit_stack);\n@@ -1528,1 +1875,1 @@\n-  static inline bool report_simple_root(jvmtiHeapReferenceKind kind, oop o);\n+  static inline bool report_simple_root(jvmtiHeapReferenceKind kind, const JvmtiHeapwalkObject& o);\n@@ -1530,1 +1877,1 @@\n-    jmethodID m, oop o);\n+    jmethodID m, const JvmtiHeapwalkObject& o);\n@@ -1532,1 +1879,1 @@\n-    jmethodID method, jlocation bci, jint slot, oop o);\n+    jmethodID method, jlocation bci, jint slot, const JvmtiHeapwalkObject& o);\n@@ -1535,14 +1882,14 @@\n-  static inline bool report_array_element_reference(oop referrer, oop referree, jint index);\n-  static inline bool report_class_reference(oop referrer, oop referree);\n-  static inline bool report_class_loader_reference(oop referrer, oop referree);\n-  static inline bool report_signers_reference(oop referrer, oop referree);\n-  static inline bool report_protection_domain_reference(oop referrer, oop referree);\n-  static inline bool report_superclass_reference(oop referrer, oop referree);\n-  static inline bool report_interface_reference(oop referrer, oop referree);\n-  static inline bool report_static_field_reference(oop referrer, oop referree, jint slot);\n-  static inline bool report_field_reference(oop referrer, oop referree, jint slot);\n-  static inline bool report_constant_pool_reference(oop referrer, oop referree, jint index);\n-  static inline bool report_primitive_array_values(oop array);\n-  static inline bool report_string_value(oop str);\n-  static inline bool report_primitive_instance_field(oop o, jint index, address value, char type);\n-  static inline bool report_primitive_static_field(oop o, jint index, address value, char type);\n+  static inline bool report_array_element_reference(const JvmtiHeapwalkObject& referrer, const JvmtiHeapwalkObject& referree, jint index);\n+  static inline bool report_class_reference(const JvmtiHeapwalkObject& referrer, const JvmtiHeapwalkObject& referree);\n+  static inline bool report_class_loader_reference(const JvmtiHeapwalkObject& referrer, const JvmtiHeapwalkObject& referree);\n+  static inline bool report_signers_reference(const JvmtiHeapwalkObject& referrer, const JvmtiHeapwalkObject& referree);\n+  static inline bool report_protection_domain_reference(const JvmtiHeapwalkObject& referrer, const JvmtiHeapwalkObject& referree);\n+  static inline bool report_superclass_reference(const JvmtiHeapwalkObject& referrer, const JvmtiHeapwalkObject& referree);\n+  static inline bool report_interface_reference(const JvmtiHeapwalkObject& referrer, const JvmtiHeapwalkObject& referree);\n+  static inline bool report_static_field_reference(const JvmtiHeapwalkObject& referrer, const JvmtiHeapwalkObject& referree, jint slot);\n+  static inline bool report_field_reference(const JvmtiHeapwalkObject& referrer, const JvmtiHeapwalkObject& referree, jint slot);\n+  static inline bool report_constant_pool_reference(const JvmtiHeapwalkObject& referrer, const JvmtiHeapwalkObject& referree, jint index);\n+  static inline bool report_primitive_array_values(const JvmtiHeapwalkObject& array);\n+  static inline bool report_string_value(const JvmtiHeapwalkObject& str);\n+  static inline bool report_primitive_instance_field(const JvmtiHeapwalkObject& o, jint index, address value, char type);\n+  static inline bool report_primitive_static_field(const JvmtiHeapwalkObject& o, jint index, address value, char type);\n@@ -1557,2 +1904,1 @@\n-GrowableArray<oop>* CallbackInvoker::_visit_stack;\n-JVMTIBitSet* CallbackInvoker::_bitset;\n+JvmtiHeapwalkVisitStack* CallbackInvoker::_visit_stack;\n@@ -1562,1 +1908,0 @@\n-                                                     GrowableArray<oop>* visit_stack,\n@@ -1565,1 +1910,1 @@\n-                                                     JVMTIBitSet* bitset) {\n+                                                     JvmtiHeapwalkVisitStack* visit_stack) {\n@@ -1567,1 +1912,0 @@\n-  _visit_stack = visit_stack;\n@@ -1572,1 +1916,1 @@\n-  _bitset = bitset;\n+  _visit_stack = visit_stack;\n@@ -1577,1 +1921,0 @@\n-                                                        GrowableArray<oop>* visit_stack,\n@@ -1580,1 +1923,1 @@\n-                                                        JVMTIBitSet* bitset) {\n+                                                        JvmtiHeapwalkVisitStack* visit_stack) {\n@@ -1582,1 +1925,0 @@\n-  _visit_stack = visit_stack;\n@@ -1587,1 +1929,1 @@\n-  _bitset = bitset;\n+  _visit_stack = visit_stack;\n@@ -1592,1 +1934,1 @@\n-inline bool CallbackInvoker::invoke_basic_heap_root_callback(jvmtiHeapRootKind root_kind, oop obj) {\n+inline bool CallbackInvoker::invoke_basic_heap_root_callback(jvmtiHeapRootKind root_kind, const JvmtiHeapwalkObject& obj) {\n@@ -1619,1 +1961,1 @@\n-                                                             oop obj) {\n+                                                             const JvmtiHeapwalkObject& obj) {\n@@ -1646,2 +1988,2 @@\n-                                                                    oop referrer,\n-                                                                    oop referree,\n+                                                                    const JvmtiHeapwalkObject& referrer,\n+                                                                    const JvmtiHeapwalkObject& referree,\n@@ -1658,1 +2000,1 @@\n-    referrer_tag = tag_for(tag_map(), referrer);\n+    referrer_tag = tag_map()->find(referrer);\n@@ -1690,1 +2032,1 @@\n-                                                                oop obj) {\n+                                                                const JvmtiHeapwalkObject& obj) {\n@@ -1715,1 +2057,1 @@\n-  jint len = (jint)(obj->is_array() ? arrayOop(obj)->length() : -1);\n+  jint len = get_array_length(obj);\n@@ -1744,1 +2086,1 @@\n-                                                                oop obj) {\n+                                                                const JvmtiHeapwalkObject& obj) {\n@@ -1778,1 +2120,1 @@\n-  jint len = (jint)(obj->is_array() ? arrayOop(obj)->length() : -1);\n+  jint len = get_array_length(obj);\n@@ -1811,2 +2153,2 @@\n-                                                                       oop referrer,\n-                                                                       oop obj,\n+                                                                       const JvmtiHeapwalkObject& referrer,\n+                                                                       const JvmtiHeapwalkObject& obj,\n@@ -1845,1 +2187,1 @@\n-  jint len = (jint)(obj->is_array() ? arrayOop(obj)->length() : -1);\n+  jint len = get_array_length(obj);\n@@ -1868,1 +2210,1 @@\n-inline bool CallbackInvoker::report_simple_root(jvmtiHeapReferenceKind kind, oop obj) {\n+inline bool CallbackInvoker::report_simple_root(jvmtiHeapReferenceKind kind, const JvmtiHeapwalkObject& obj) {\n@@ -1884,2 +2226,2 @@\n-inline bool CallbackInvoker::report_primitive_array_values(oop obj) {\n-  assert(obj->is_typeArray(), \"not a primitive array\");\n+inline bool CallbackInvoker::report_primitive_array_values(const JvmtiHeapwalkObject& obj) {\n+  assert(obj.klass()->is_typeArray_klass(), \"not a primitive array\");\n@@ -1913,2 +2255,2 @@\n-inline bool CallbackInvoker::report_string_value(oop str) {\n-  assert(str->klass() == vmClasses::String_klass(), \"not a string\");\n+inline bool CallbackInvoker::report_string_value(const JvmtiHeapwalkObject& str) {\n+  assert(str.klass() == vmClasses::String_klass(), \"not a string\");\n@@ -1943,1 +2285,1 @@\n-                                                    oop obj,\n+                                                    const JvmtiHeapwalkObject& obj,\n@@ -1991,1 +2333,1 @@\n-inline bool CallbackInvoker::report_primitive_instance_field(oop obj,\n+inline bool CallbackInvoker::report_primitive_instance_field(const JvmtiHeapwalkObject& obj,\n@@ -2003,1 +2345,1 @@\n-inline bool CallbackInvoker::report_primitive_static_field(oop obj,\n+inline bool CallbackInvoker::report_primitive_static_field(const JvmtiHeapwalkObject& obj,\n@@ -2015,1 +2357,1 @@\n-inline bool CallbackInvoker::report_jni_local_root(jlong thread_tag, jlong tid, jint depth, jmethodID m, oop obj) {\n+inline bool CallbackInvoker::report_jni_local_root(jlong thread_tag, jlong tid, jint depth, jmethodID m, const JvmtiHeapwalkObject& obj) {\n@@ -2042,1 +2384,1 @@\n-                                                   oop obj) {\n+                                                   const JvmtiHeapwalkObject& obj) {\n@@ -2063,1 +2405,1 @@\n-inline bool CallbackInvoker::report_class_reference(oop referrer, oop referree) {\n+inline bool CallbackInvoker::report_class_reference(const JvmtiHeapwalkObject& referrer, const JvmtiHeapwalkObject& referree) {\n@@ -2072,1 +2414,1 @@\n-inline bool CallbackInvoker::report_class_loader_reference(oop referrer, oop referree) {\n+inline bool CallbackInvoker::report_class_loader_reference(const JvmtiHeapwalkObject& referrer, const JvmtiHeapwalkObject& referree) {\n@@ -2081,1 +2423,1 @@\n-inline bool CallbackInvoker::report_signers_reference(oop referrer, oop referree) {\n+inline bool CallbackInvoker::report_signers_reference(const JvmtiHeapwalkObject& referrer, const JvmtiHeapwalkObject& referree) {\n@@ -2090,1 +2432,1 @@\n-inline bool CallbackInvoker::report_protection_domain_reference(oop referrer, oop referree) {\n+inline bool CallbackInvoker::report_protection_domain_reference(const JvmtiHeapwalkObject& referrer, const JvmtiHeapwalkObject& referree) {\n@@ -2099,1 +2441,1 @@\n-inline bool CallbackInvoker::report_superclass_reference(oop referrer, oop referree) {\n+inline bool CallbackInvoker::report_superclass_reference(const JvmtiHeapwalkObject& referrer, const JvmtiHeapwalkObject& referree) {\n@@ -2109,1 +2451,1 @@\n-inline bool CallbackInvoker::report_interface_reference(oop referrer, oop referree) {\n+inline bool CallbackInvoker::report_interface_reference(const JvmtiHeapwalkObject& referrer, const JvmtiHeapwalkObject& referree) {\n@@ -2118,1 +2460,1 @@\n-inline bool CallbackInvoker::report_static_field_reference(oop referrer, oop referree, jint slot) {\n+inline bool CallbackInvoker::report_static_field_reference(const JvmtiHeapwalkObject& referrer, const JvmtiHeapwalkObject& referree, jint slot) {\n@@ -2127,1 +2469,1 @@\n-inline bool CallbackInvoker::report_array_element_reference(oop referrer, oop referree, jint index) {\n+inline bool CallbackInvoker::report_array_element_reference(const JvmtiHeapwalkObject& referrer, const JvmtiHeapwalkObject& referree, jint index) {\n@@ -2136,1 +2478,1 @@\n-inline bool CallbackInvoker::report_field_reference(oop referrer, oop referree, jint slot) {\n+inline bool CallbackInvoker::report_field_reference(const JvmtiHeapwalkObject& referrer, const JvmtiHeapwalkObject& referree, jint slot) {\n@@ -2145,1 +2487,1 @@\n-inline bool CallbackInvoker::report_constant_pool_reference(oop referrer, oop referree, jint index) {\n+inline bool CallbackInvoker::report_constant_pool_reference(const JvmtiHeapwalkObject& referrer, const JvmtiHeapwalkObject& referree, jint index) {\n@@ -2305,1 +2647,1 @@\n-  _thread_tag = tag_for(_tag_map, _threadObj);\n+  _thread_tag = _tag_map->find(_threadObj);\n@@ -2438,4 +2780,0 @@\n-  enum {\n-    initial_visit_stack_size = 4000\n-  };\n-\n@@ -2445,3 +2783,1 @@\n-  GrowableArray<oop>* _visit_stack;                 \/\/ the visit stack\n-\n-  JVMTIBitSet _bitset;\n+  JvmtiHeapwalkVisitStack _visit_stack;\n@@ -2458,4 +2794,0 @@\n-  GrowableArray<oop>* create_visit_stack() {\n-    return new (mtServiceability) GrowableArray<oop>(initial_visit_stack_size, mtServiceability);\n-  }\n-\n@@ -2473,1 +2805,1 @@\n-  GrowableArray<oop>* visit_stack() const          { return _visit_stack; }\n+  JvmtiHeapwalkVisitStack* visit_stack()           { return &_visit_stack; }\n@@ -2476,4 +2808,5 @@\n-  inline bool iterate_over_array(oop o);\n-  inline bool iterate_over_type_array(oop o);\n-  inline bool iterate_over_class(oop o);\n-  inline bool iterate_over_object(oop o);\n+  inline bool iterate_over_array(const JvmtiHeapwalkObject& o);\n+  inline bool iterate_over_flat_array(const JvmtiHeapwalkObject& o);\n+  inline bool iterate_over_type_array(const JvmtiHeapwalkObject& o);\n+  inline bool iterate_over_class(const JvmtiHeapwalkObject& o);\n+  inline bool iterate_over_object(const JvmtiHeapwalkObject& o);\n@@ -2488,1 +2821,1 @@\n-  inline bool visit(oop o);\n+  inline bool visit(const JvmtiHeapwalkObject& o);\n@@ -2522,3 +2855,1 @@\n-  _visit_stack = create_visit_stack();\n-\n-  CallbackInvoker::initialize_for_basic_heap_walk(tag_map, _visit_stack, user_data, callbacks, &_bitset);\n+  CallbackInvoker::initialize_for_basic_heap_walk(tag_map, user_data, callbacks, &_visit_stack);\n@@ -2540,2 +2871,1 @@\n-  _visit_stack = create_visit_stack();\n-  CallbackInvoker::initialize_for_advanced_heap_walk(tag_map, _visit_stack, user_data, callbacks, &_bitset);\n+  CallbackInvoker::initialize_for_advanced_heap_walk(tag_map, user_data, callbacks, &_visit_stack);\n@@ -2546,5 +2876,0 @@\n-  if (_following_object_refs) {\n-    assert(_visit_stack != nullptr, \"checking\");\n-    delete _visit_stack;\n-    _visit_stack = nullptr;\n-  }\n@@ -2555,2 +2880,3 @@\n-inline bool VM_HeapWalkOperation::iterate_over_array(oop o) {\n-  objArrayOop array = objArrayOop(o);\n+inline bool VM_HeapWalkOperation::iterate_over_array(const JvmtiHeapwalkObject& o) {\n+  assert(!o.is_flat(), \"Array object cannot be flattened\");\n+  objArrayOop array = objArrayOop(o.obj());\n@@ -2580,0 +2906,38 @@\n+\/\/ similar to iterate_over_array(), but itrates over flat array\n+inline bool VM_HeapWalkOperation::iterate_over_flat_array(const JvmtiHeapwalkObject& o) {\n+  assert(!o.is_flat(), \"Array object cannot be flattened\");\n+  flatArrayOop array = flatArrayOop(o.obj());\n+  FlatArrayKlass* faklass = FlatArrayKlass::cast(array->klass());\n+  InlineKlass* vk = InlineKlass::cast(faklass->element_klass());\n+  bool need_null_check = LayoutKindHelper::is_nullable_flat(faklass->layout_kind());\n+\n+  \/\/ array reference to its class\n+  oop mirror = faklass->java_mirror();\n+  if (!CallbackInvoker::report_class_reference(o, mirror)) {\n+    return false;\n+  }\n+\n+  \/\/ iterate over the array and report each reference to a\n+  \/\/ non-null element\n+  for (int index = 0; index < array->length(); index++) {\n+    address addr = (address)array->value_at_addr(index, faklass->layout_helper());\n+\n+    \/\/ check for null\n+    if (need_null_check) {\n+      if (vk->is_payload_marked_as_null(addr)) {\n+        continue;\n+      }\n+    }\n+\n+    \/\/ offset in the array oop\n+    int offset = (int)(addr - cast_from_oop<address>(array));\n+    JvmtiHeapwalkObject elem(o.obj(), offset, vk, faklass->layout_kind());\n+\n+    \/\/ report the array reference\n+    if (!CallbackInvoker::report_array_element_reference(o, elem, index)) {\n+      return false;\n+    }\n+  }\n+  return true;\n+}\n+\n@@ -2581,2 +2945,3 @@\n-inline bool VM_HeapWalkOperation::iterate_over_type_array(oop o) {\n-  Klass* k = o->klass();\n+inline bool VM_HeapWalkOperation::iterate_over_type_array(const JvmtiHeapwalkObject& o) {\n+  assert(!o.is_flat(), \"Array object cannot be flattened\");\n+  Klass* k = o.klass();\n@@ -2616,1 +2981,3 @@\n-inline bool VM_HeapWalkOperation::iterate_over_class(oop java_class) {\n+inline bool VM_HeapWalkOperation::iterate_over_class(const JvmtiHeapwalkObject& o) {\n+  assert(!o.is_flat(), \"Klass object cannot be flattened\");\n+  Klass* klass = java_lang_Class::as_Klass(o.obj());\n@@ -2618,1 +2985,0 @@\n-  Klass* klass = java_lang_Class::as_Klass(java_class);\n@@ -2629,1 +2995,2 @@\n-    oop mirror = klass->java_mirror();\n+    oop mirror_oop = klass->java_mirror();\n+    JvmtiHeapwalkObject mirror(mirror_oop);\n@@ -2718,2 +3085,2 @@\n-        oop fld_o = mirror->obj_field(field->field_offset());\n-        assert(verify_static_oop(ik, mirror, field->field_offset()), \"sanity check\");\n+        oop fld_o = mirror_oop->obj_field(field->field_offset());\n+        assert(verify_static_oop(ik, mirror_oop, field->field_offset()), \"sanity check\");\n@@ -2729,1 +3096,1 @@\n-           address addr = cast_from_oop<address>(mirror) + field->field_offset();\n+           address addr = cast_from_oop<address>(mirror_oop) + field->field_offset();\n@@ -2749,1 +3116,1 @@\n-inline bool VM_HeapWalkOperation::iterate_over_object(oop o) {\n+inline bool VM_HeapWalkOperation::iterate_over_object(const JvmtiHeapwalkObject& o) {\n@@ -2751,1 +3118,1 @@\n-  if (!CallbackInvoker::report_class_reference(o, o->klass()->java_mirror())) {\n+  if (!CallbackInvoker::report_class_reference(o, o.klass()->java_mirror())) {\n@@ -2756,1 +3123,1 @@\n-  ClassFieldMap* field_map = JvmtiCachedClassFieldMap::get_map_of_instance_fields(o);\n+  ClassFieldMap* field_map = JvmtiCachedClassFieldMap::get_map_of_instance_fields(o.klass());\n@@ -2760,0 +3127,6 @@\n+    int slot = field->field_index();\n+    int field_offset = field->field_offset();\n+    if (o.is_flat()) {\n+      \/\/ the object is inlined, its fields are stored without the header\n+      field_offset += o.offset() - o.inline_klass()->payload_offset();\n+    }\n@@ -2761,7 +3134,10 @@\n-      oop fld_o = o->obj_field_access<AS_NO_KEEPALIVE | ON_UNKNOWN_OOP_REF>(field->field_offset());\n-      \/\/ ignore any objects that aren't visible to profiler\n-      if (fld_o != nullptr) {\n-        assert(Universe::heap()->is_in(fld_o), \"unsafe code should not \"\n-               \"have references to Klass* anymore\");\n-        int slot = field->field_index();\n-        if (!CallbackInvoker::report_field_reference(o, fld_o, slot)) {\n+      if (field->is_flat()) {\n+        \/\/ check for possible nulls\n+        if (LayoutKindHelper::is_nullable_flat(field->layout_kind())) {\n+          address payload = cast_from_oop<address>(o.obj()) + field_offset;\n+          if (field->inline_klass()->is_payload_marked_as_null(payload)) {\n+            continue;\n+          }\n+        }\n+        JvmtiHeapwalkObject field_obj(o.obj(), field_offset, field->inline_klass(), field->layout_kind());\n+        if (!CallbackInvoker::report_field_reference(o, field_obj, slot)) {\n@@ -2770,0 +3146,9 @@\n+      } else {\n+        oop fld_o = o.obj()->obj_field_access<AS_NO_KEEPALIVE | ON_UNKNOWN_OOP_REF>(field_offset);\n+        \/\/ ignore any objects that aren't visible to profiler\n+        if (fld_o != nullptr) {\n+          assert(Universe::heap()->is_in(fld_o), \"unsafe code should not have references to Klass* anymore\");\n+          if (!CallbackInvoker::report_field_reference(o, fld_o, slot)) {\n+            return false;\n+          }\n+        }\n@@ -2774,2 +3159,1 @@\n-        address addr = cast_from_oop<address>(o) + field->field_offset();\n-        int slot = field->field_index();\n+        address addr = cast_from_oop<address>(o.obj()) + field_offset;\n@@ -2785,1 +3169,1 @@\n-      o->klass() == vmClasses::String_klass()) {\n+      o.klass() == vmClasses::String_klass()) {\n@@ -2854,1 +3238,1 @@\n-    blk->set_context(tag_for(_tag_map, threadObj), java_lang_Thread::thread_id(threadObj), 0, (jmethodID)nullptr);\n+    blk->set_context(_tag_map->find(threadObj), java_lang_Thread::thread_id(threadObj), 0, (jmethodID)nullptr);\n@@ -2954,1 +3338,1 @@\n-bool VM_HeapWalkOperation::visit(oop o) {\n+bool VM_HeapWalkOperation::visit(const JvmtiHeapwalkObject& o) {\n@@ -2956,2 +3340,2 @@\n-  assert(!_bitset.is_marked(o), \"can't visit same object more than once\");\n-  _bitset.mark_obj(o);\n+  assert(!visit_stack()->is_visited(o), \"can't visit same object more than once\");\n+  visit_stack()->mark_visited(o);\n@@ -2959,0 +3343,1 @@\n+  Klass* klass = o.klass();\n@@ -2960,3 +3345,4 @@\n-  if (o->is_instance()) {\n-    if (o->klass() == vmClasses::Class_klass()) {\n-      if (!java_lang_Class::is_primitive(o)) {\n+  if (klass->is_instance_klass()) {\n+    if (klass == vmClasses::Class_klass()) {\n+      assert(!o.is_flat(), \"Class object cannot be flattened\");\n+      if (!java_lang_Class::is_primitive(o.obj())) {\n@@ -2969,2 +3355,3 @@\n-      if (initial_object().is_null() && java_lang_VirtualThread::is_subclass(o->klass())) {\n-        if (!collect_vthread_stack_refs(o)) {\n+      if (initial_object().is_null() && java_lang_VirtualThread::is_subclass(klass)) {\n+        assert(!o.is_flat(), \"VirtualThread object cannot be flattened\");\n+        if (!collect_vthread_stack_refs(o.obj())) {\n@@ -2978,0 +3365,5 @@\n+  \/\/ flat object array\n+  if (klass->is_flatArray_klass()) {\n+      return iterate_over_flat_array(o);\n+  }\n+\n@@ -2979,1 +3371,1 @@\n-  if (o->is_objArray()) {\n+  if (klass->is_objArray_klass()) {\n@@ -2984,1 +3376,1 @@\n-  if (o->is_typeArray()) {\n+  if (klass->is_typeArray_klass()) {\n@@ -3016,2 +3408,2 @@\n-      oop o = visit_stack()->pop();\n-      if (!_bitset.is_marked(o)) {\n+      const JvmtiHeapwalkObject o = visit_stack()->pop();\n+      if (!visit_stack()->is_visited(o)) {\n@@ -3046,0 +3438,2 @@\n+  convert_flat_object_entries();\n+\n@@ -3068,0 +3462,2 @@\n+  convert_flat_object_entries();\n+\n@@ -3100,0 +3496,2 @@\n+  convert_flat_object_entries();\n+\n","filename":"src\/hotspot\/share\/prims\/jvmtiTagMap.cpp","additions":641,"deletions":243,"binary":false,"changes":884,"status":"modified"},{"patch":"@@ -55,0 +55,1 @@\n+#include \"memory\/iterator.inline.hpp\"\n@@ -63,0 +64,1 @@\n+#include \"oops\/access.hpp\"\n@@ -65,0 +67,1 @@\n+#include \"oops\/compressedOops.inline.hpp\"\n@@ -90,0 +93,1 @@\n+#include \"runtime\/keepStackGCProcessed.hpp\"\n@@ -2015,0 +2019,103 @@\n+WB_ENTRY(jobjectArray, WB_getObjectsViaKlassOopMaps(JNIEnv* env, jobject wb, jobject thing))\n+  oop aoop = JNIHandles::resolve(thing);\n+  if (!aoop->is_instance()) {\n+    return nullptr;\n+  }\n+  instanceHandle ih(THREAD, (instanceOop) aoop);\n+  InstanceKlass* klass = InstanceKlass::cast(ih->klass());\n+  if (klass->nonstatic_oop_map_count() == 0) {\n+    return nullptr;\n+  }\n+  const OopMapBlock* map = klass->start_of_nonstatic_oop_maps();\n+  const OopMapBlock* const end = map + klass->nonstatic_oop_map_count();\n+  int oop_count = 0;\n+  while (map < end) {\n+    oop_count += map->count();\n+    map++;\n+  }\n+\n+  objArrayHandle result_array =\n+      oopFactory::new_objArray_handle(vmClasses::Object_klass(), oop_count, CHECK_NULL);\n+  map = klass->start_of_nonstatic_oop_maps();\n+  int index = 0;\n+  while (map < end) {\n+    int offset = map->offset();\n+    for (unsigned int j = 0; j < map->count(); j++) {\n+      result_array->obj_at_put(index++, ih->obj_field(offset));\n+      offset += heapOopSize;\n+    }\n+    map++;\n+  }\n+  return (jobjectArray)JNIHandles::make_local(THREAD, result_array());\n+WB_END\n+\n+\/\/ Collect Object oops but not value objects...loaded from heap\n+class CollectObjectOops : public BasicOopIterateClosure {\n+  public:\n+  GrowableArray<Handle>* _array;\n+\n+  CollectObjectOops() {\n+      _array = new GrowableArray<Handle>(128);\n+  }\n+\n+  void add_oop(oop o) {\n+    Handle oh = Handle(Thread::current(), o);\n+    if (oh != nullptr && oh->is_inline_type()) {\n+      oh->oop_iterate(this);\n+    } else {\n+      _array->append(oh);\n+    }\n+  }\n+\n+  template <class T> inline void add_oop(T* p) { add_oop(HeapAccess<>::oop_load(p)); }\n+  void do_oop(oop* o) { add_oop(o); }\n+  void do_oop(narrowOop* v) { add_oop(v); }\n+\n+  jobjectArray create_jni_result(JNIEnv* env, TRAPS) {\n+    objArrayHandle result_array =\n+        oopFactory::new_objArray_handle(vmClasses::Object_klass(), _array->length(), CHECK_NULL);\n+    for (int i = 0 ; i < _array->length(); i++) {\n+      result_array->obj_at_put(i, _array->at(i)());\n+    }\n+    return (jobjectArray)JNIHandles::make_local(THREAD, result_array());\n+  }\n+};\n+\n+\/\/ Collect Object oops but not value objects...loaded from frames\n+class CollectFrameObjectOops : public BasicOopIterateClosure {\n+ public:\n+  CollectObjectOops _collect;\n+\n+  template <class T> inline void add_oop(T* p) { _collect.add_oop(RawAccess<>::oop_load(p)); }\n+  void do_oop(oop* o) { add_oop(o); }\n+  void do_oop(narrowOop* v) { add_oop(v); }\n+\n+  jobjectArray create_jni_result(JNIEnv* env, TRAPS) {\n+    return _collect.create_jni_result(env, THREAD);\n+  }\n+};\n+\n+\/\/ Collect Object oops for the given oop, iterate through value objects\n+WB_ENTRY(jobjectArray, WB_getObjectsViaOopIterator(JNIEnv* env, jobject wb, jobject thing))\n+  ResourceMark rm(thread);\n+  Handle objh(thread, JNIHandles::resolve(thing));\n+  CollectObjectOops collectOops;\n+  objh->oop_iterate(&collectOops);\n+  return collectOops.create_jni_result(env, THREAD);\n+WB_END\n+\n+\/\/ Collect Object oops for the given frame deep, iterate through value objects\n+WB_ENTRY(jobjectArray, WB_getObjectsViaFrameOopIterator(JNIEnv* env, jobject wb, jint depth))\n+  KeepStackGCProcessedMark ksgcpm(THREAD);\n+  ResourceMark rm(THREAD);\n+  CollectFrameObjectOops collectOops;\n+  StackFrameStream sfs(thread, true \/* update *\/, true \/* process_frames *\/);\n+  while (depth > 0) { \/\/ Skip the native WB API frame\n+    sfs.next();\n+    frame* f = sfs.current();\n+    f->oops_do(&collectOops, nullptr, sfs.register_map());\n+    depth--;\n+  }\n+  return collectOops.create_jni_result(env, THREAD);\n+WB_END\n+\n@@ -3032,0 +3139,6 @@\n+  {CC\"getObjectsViaKlassOopMaps0\",\n+      CC\"(Ljava\/lang\/Object;)[Ljava\/lang\/Object;\",    (void*)&WB_getObjectsViaKlassOopMaps},\n+  {CC\"getObjectsViaOopIterator0\",\n+          CC\"(Ljava\/lang\/Object;)[Ljava\/lang\/Object;\",(void*)&WB_getObjectsViaOopIterator},\n+  {CC\"getObjectsViaFrameOopIterator\",\n+      CC\"(I)[Ljava\/lang\/Object;\",                     (void*)&WB_getObjectsViaFrameOopIterator},\n","filename":"src\/hotspot\/share\/prims\/whitebox.cpp","additions":113,"deletions":0,"binary":false,"changes":113,"status":"modified"},{"patch":"@@ -79,0 +79,2 @@\n+#include <string.h>\n+\n@@ -371,0 +373,12 @@\n+bool Arguments::patching_migrated_classes(const char* property, const char* value) {\n+  if (strncmp(property, MODULE_PROPERTY_PREFIX, MODULE_PROPERTY_PREFIX_LEN) == 0) {\n+    const char* property_suffix = property + MODULE_PROPERTY_PREFIX_LEN;\n+    if (matches_property_suffix(property_suffix, PATCH, PATCH_LEN)) {\n+      if (strcmp(value, \"java.base-valueclasses.jar\")) {\n+        return true;\n+      }\n+    }\n+  }\n+  return false;\n+}\n+\n@@ -1816,1 +1830,0 @@\n-static unsigned int patch_mod_count = 0;\n@@ -1824,1 +1837,1 @@\n-  if (!CDSConfig::check_vm_args_consistency(patch_mod_javabase, mode_flag_cmd_line)) {\n+  if (!CDSConfig::check_vm_args_consistency(mode_flag_cmd_line)) {\n@@ -1991,0 +2004,4 @@\n+  if (UseAltSubstitutabilityMethod) {\n+    no_shared_spaces(\"Alternate substitutability method doesn't work with CDS yet\");\n+  }\n+\n@@ -2073,1 +2090,1 @@\n-      add_patch_mod_prefix(module_name, module_equal + 1);\n+      add_patch_mod_prefix(module_name, module_equal + 1, false \/* no append *\/, false \/* no cds *\/);\n@@ -2075,3 +2092,0 @@\n-      if (!create_numbered_module_property(\"jdk.module.patch\", patch_mod_tail, patch_mod_count++)) {\n-        return JNI_ENOMEM;\n-      }\n@@ -2085,0 +2099,82 @@\n+\/\/ Temporary system property to disable preview patching and enable the new preview mode\n+\/\/ feature for testing\/development. Once the preview mode feature is finished, the value\n+\/\/ will be always 'true' and this code, and all related dead-code can be removed.\n+#define DISABLE_PREVIEW_PATCHING_DEFAULT false\n+\n+bool Arguments::disable_preview_patching() {\n+  const char* prop = get_property(\"DISABLE_PREVIEW_PATCHING\");\n+  return (prop != nullptr)\n+      ? strncmp(prop, \"true\", strlen(\"true\")) == 0\n+      : DISABLE_PREVIEW_PATCHING_DEFAULT;\n+}\n+\n+\/\/ VALUECLASS_STR must match string used in the build\n+#define VALUECLASS_STR \"valueclasses\"\n+#define VALUECLASS_JAR \"-\" VALUECLASS_STR \".jar\"\n+\n+\/\/ Finalize --patch-module args and --enable-preview related to value class module patches.\n+\/\/ Create all numbered properties passing module patches.\n+int Arguments::finalize_patch_module() {\n+  \/\/ If --enable-preview and EnableValhalla is true, modules may have preview mode resources.\n+  bool enable_valhalla_preview = enable_preview() && EnableValhalla;\n+  \/\/ Whether to use module patching, or the new preview mode feature for preview resources.\n+  bool disable_patching = disable_preview_patching();\n+\n+  \/\/ This must be called, even with 'false', to enable resource lookup from JImage.\n+  ClassLoader::init_jimage(disable_patching && enable_valhalla_preview);\n+\n+  \/\/ For each <module>-valueclasses.jar in <JAVA_HOME>\/lib\/valueclasses\/\n+  \/\/ appends the equivalent of --patch-module <module>=<JAVA_HOME>\/lib\/valueclasses\/<module>-valueclasses.jar\n+  if (!disable_patching && enable_valhalla_preview) {\n+    char * valueclasses_dir = AllocateHeap(JVM_MAXPATHLEN, mtArguments);\n+    const char * fileSep = os::file_separator();\n+\n+    jio_snprintf(valueclasses_dir, JVM_MAXPATHLEN, \"%s%slib%s\" VALUECLASS_STR \"%s\",\n+                 Arguments::get_java_home(), fileSep, fileSep, fileSep);\n+    DIR* dir = os::opendir(valueclasses_dir);\n+    if (dir != nullptr) {\n+      char * module_name = AllocateHeap(JVM_MAXPATHLEN, mtArguments);\n+      char * path = AllocateHeap(JVM_MAXPATHLEN, mtArguments);\n+\n+      for (dirent * entry = os::readdir(dir); entry != nullptr; entry = os::readdir(dir)) {\n+        \/\/ Test if file ends-with \"-valueclasses.jar\"\n+        int len = (int)strlen(entry->d_name) - (sizeof(VALUECLASS_JAR) - 1);\n+        if (len <= 0 || strcmp(&entry->d_name[len], VALUECLASS_JAR) != 0) {\n+          continue;         \/\/ too short or not the expected suffix\n+        }\n+\n+        strcpy(module_name, entry->d_name);\n+        module_name[len] = '\\0';     \/\/ truncate to just module-name\n+\n+        jio_snprintf(path, JVM_MAXPATHLEN, \"%s%s\", valueclasses_dir, &entry->d_name);\n+        add_patch_mod_prefix(module_name, path, true \/* append *\/, true \/* cds OK*\/);\n+        log_info(class)(\"--enable-preview appending value classes for module %s: %s\", module_name, entry->d_name);\n+      }\n+      FreeHeap(module_name);\n+      FreeHeap(path);\n+      os::closedir(dir);\n+    }\n+    FreeHeap(valueclasses_dir);\n+  }\n+\n+  \/\/ Create numbered properties for each module that has been patched either\n+  \/\/ by --patch-module (or --enable-preview if disable_patching is false).\n+  \/\/ Format is \"jdk.module.patch.<n>=<module_name>=<path>\"\n+  if (_patch_mod_prefix != nullptr) {\n+    char * prop_value = AllocateHeap(JVM_MAXPATHLEN + JVM_MAXPATHLEN + 1, mtArguments);\n+    unsigned int patch_mod_count = 0;\n+\n+    for (GrowableArrayIterator<ModulePatchPath *> it = _patch_mod_prefix->begin();\n+            it != _patch_mod_prefix->end(); ++it) {\n+      jio_snprintf(prop_value, JVM_MAXPATHLEN + JVM_MAXPATHLEN + 1, \"%s=%s\",\n+                   (*it)->module_name(), (*it)->path_string());\n+      if (!create_numbered_module_property(\"jdk.module.patch\", prop_value, patch_mod_count++)) {\n+        FreeHeap(prop_value);\n+        return JNI_ENOMEM;\n+      }\n+    }\n+    FreeHeap(prop_value);\n+  }\n+  return JNI_OK;\n+}\n+\n@@ -2366,0 +2462,4 @@\n+      \/\/ --enable-preview enables Valhalla, EnableValhalla VM option will eventually be removed before integration\n+      if (FLAG_SET_CMDLINE(EnableValhalla, true) != JVMFlag::SUCCESS) {\n+        return JNI_EINVAL;\n+      }\n@@ -2872,10 +2972,5 @@\n-void Arguments::add_patch_mod_prefix(const char* module_name, const char* path) {\n-  \/\/ For java.base check for duplicate --patch-module options being specified on the command line.\n-  \/\/ This check is only required for java.base, all other duplicate module specifications\n-  \/\/ will be checked during module system initialization.  The module system initialization\n-  \/\/ will throw an ExceptionInInitializerError if this situation occurs.\n-  if (strcmp(module_name, JAVA_BASE_NAME) == 0) {\n-    if (patch_mod_javabase) {\n-      vm_exit_during_initialization(\"Cannot specify \" JAVA_BASE_NAME \" more than once to --patch-module\");\n-    } else {\n-      patch_mod_javabase = true;\n+void Arguments::add_patch_mod_prefix(const char* module_name, const char* path, bool allow_append, bool allow_cds) {\n+  if (!allow_cds) {\n+    CDSConfig::set_module_patching_disables_cds();\n+    if (strcmp(module_name, JAVA_BASE_NAME) == 0) {\n+      CDSConfig::set_java_base_module_patching_disables_cds();\n@@ -2890,1 +2985,18 @@\n-  _patch_mod_prefix->push(new ModulePatchPath(module_name, path));\n+  \/\/ Scan patches for matching module\n+  int i = _patch_mod_prefix->find_if([&](ModulePatchPath* patch) {\n+    return (strcmp(module_name, patch->module_name()) == 0);\n+  });\n+  if (i == -1) {\n+    _patch_mod_prefix->push(new ModulePatchPath(module_name, path));\n+  } else {\n+    if (allow_append) {\n+      \/\/ append path to existing module entry\n+      _patch_mod_prefix->at(i)->append_path(path);\n+    } else {\n+      if (strcmp(module_name, JAVA_BASE_NAME) == 0) {\n+        vm_exit_during_initialization(\"Cannot specify \" JAVA_BASE_NAME \" more than once to --patch-module\");\n+      } else {\n+        vm_exit_during_initialization(\"Cannot specify a module more than once to --patch-module\", module_name);\n+      }\n+    }\n+  }\n@@ -3003,1 +3115,2 @@\n-  if (!check_vm_args_consistency()) {\n+  \/\/ finalize --module-patch and related --enable-preview\n+  if (finalize_patch_module() != JNI_OK) {\n@@ -3007,0 +3120,3 @@\n+  if (!check_vm_args_consistency()) {\n+    return JNI_ERR;\n+  }\n@@ -3892,0 +4008,12 @@\n+  if (!EnableValhalla || (is_interpreter_only() && !CDSConfig::is_dumping_archive() && !UseSharedSpaces)) {\n+    \/\/ Disable calling convention optimizations if inline types are not supported.\n+    \/\/ Also these aren't useful in -Xint. However, don't disable them when dumping or using\n+    \/\/ the CDS archive, as the values must match between dumptime and runtime.\n+    FLAG_SET_DEFAULT(InlineTypePassFieldsAsArgs, false);\n+    FLAG_SET_DEFAULT(InlineTypeReturnedAsFields, false);\n+  }\n+  if (!UseNonAtomicValueFlattening && !UseNullableValueFlattening && !UseAtomicValueFlattening) {\n+    \/\/ Flattening is disabled\n+    FLAG_SET_DEFAULT(UseArrayFlattening, false);\n+    FLAG_SET_DEFAULT(UseFieldFlattening, false);\n+  }\n","filename":"src\/hotspot\/share\/runtime\/arguments.cpp","additions":146,"deletions":18,"binary":false,"changes":164,"status":"modified"},{"patch":"@@ -154,6 +154,0 @@\n-#ifndef PRODUCT\n-static jlong java_tid(JavaThread* thread) {\n-  return java_lang_Thread::thread_id(thread->threadObj());\n-}\n-#endif\n-\n@@ -389,3 +383,2 @@\n-  log_develop_debug(continuations)(\"continuation_bottom_sender: [\" JLONG_FORMAT \"] [%d] callee: \" INTPTR_FORMAT\n-    \" sender_sp: \" INTPTR_FORMAT,\n-    java_tid(thread), thread->osthread()->thread_id(), p2i(callee.sp()), p2i(sender_sp));\n+  log_develop_debug(continuations)(\"continuation_bottom_sender: [%d] callee: \" INTPTR_FORMAT \" sender_sp: \" INTPTR_FORMAT,\n+      thread->osthread()->thread_id(), p2i(callee.sp()), p2i(sender_sp));\n","filename":"src\/hotspot\/share\/runtime\/continuation.cpp","additions":2,"deletions":9,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -472,1 +472,1 @@\n-  template<typename FKind> frame new_heap_frame(frame& f, frame& caller);\n+  template<typename FKind> frame new_heap_frame(frame& f, frame& caller, int size_adjust = 0);\n@@ -474,1 +474,1 @@\n-  inline void patch_pd(frame& callee, const frame& caller);\n+  inline void patch_pd(frame& callee, const frame& caller, bool is_bottom_frame);\n@@ -1184,1 +1184,1 @@\n-  patch_pd(hf, caller);\n+  patch_pd(hf, caller, is_bottom_frame);\n@@ -1281,2 +1281,24 @@\n-  const int argsize = ContinuationHelper::CompiledFrame::stack_argsize(f) + frame::metadata_words_at_top;\n-  const int fsize = pointer_delta_as_int(stack_frame_bottom + argsize, stack_frame_top);\n+  int argsize = ContinuationHelper::CompiledFrame::stack_argsize(f) + frame::metadata_words_at_top;\n+  int fsize = pointer_delta_as_int(stack_frame_bottom + argsize, stack_frame_top);\n+\n+  int real_frame_size = 0;\n+  bool augmented = f.was_augmented_on_entry(real_frame_size);\n+  if (augmented) {\n+    \/\/ The args reside inside the frame so clear argsize. If the caller is compiled,\n+    \/\/ this will cause the stack arguments passed by the caller to be freezed when\n+    \/\/ freezing the caller frame itself. If the caller is interpreted this will have\n+    \/\/ the effect of discarding the arg area created in the i2c stub.\n+    argsize = 0;\n+    fsize = real_frame_size - (callee_interpreted ? 0 : callee_argsize);\n+#ifdef ASSERT\n+    nmethod* nm = f.cb()->as_nmethod();\n+    Method* method = nm->method();\n+    address return_pc = ContinuationHelper::CompiledFrame::return_pc(f);\n+    CodeBlob* caller_cb = CodeCache::find_blob_fast(return_pc);\n+    assert(nm->is_compiled_by_c2() || (caller_cb->is_nmethod() && caller_cb->as_nmethod()->is_compiled_by_c2()), \"caller or callee should be c2 compiled\");\n+    assert((!caller_cb->is_nmethod() && nm->is_compiled_by_c2()) ||\n+           (nm->compiler_type() != caller_cb->as_nmethod()->compiler_type()) ||\n+           (nm->is_compiled_by_c2() && !method->is_static() && method->method_holder()->is_inline_klass()),\n+           \"frame should not be extended\");\n+#endif\n+  }\n@@ -1284,1 +1306,1 @@\n-  log_develop_trace(continuations)(\"recurse_freeze_compiled_frame %s _size: %d fsize: %d argsize: %d\",\n+  log_develop_trace(continuations)(\"recurse_freeze_compiled_frame %s _size: %d fsize: %d argsize: %d augmented: %d\",\n@@ -1287,1 +1309,1 @@\n-                             _freeze_size, fsize, argsize);\n+                             _freeze_size, fsize, argsize, augmented);\n@@ -1298,0 +1320,1 @@\n+  assert(!is_bottom_frame || !augmented, \"thaw extended frame without caller?\");\n@@ -1301,1 +1324,1 @@\n-  frame hf = new_heap_frame<ContinuationHelper::CompiledFrame>(f, caller);\n+  frame hf = new_heap_frame<ContinuationHelper::CompiledFrame>(f, caller, augmented ? real_frame_size - f.cb()->as_nmethod()->frame_size() : 0);\n@@ -2081,0 +2104,1 @@\n+  int remove_scalarized_frames(StackChunkFrameStream<ChunkFrames::CompiledOnly>& scfs, int &argsize);\n@@ -2106,1 +2130,1 @@\n-  inline void patch(frame& f, const frame& caller, bool bottom);\n+  inline void patch(frame& f, const frame& caller, bool bottom, bool augmented = false);\n@@ -2116,1 +2140,1 @@\n-  template<typename FKind> frame new_stack_frame(const frame& hf, frame& caller, bool bottom);\n+  template<typename FKind> frame new_stack_frame(const frame& hf, frame& caller, bool bottom, int size_adjust = 0);\n@@ -2193,0 +2217,14 @@\n+int ThawBase::remove_scalarized_frames(StackChunkFrameStream<ChunkFrames::CompiledOnly>& f, int &argsize) {\n+  intptr_t* top = f.sp();\n+\n+  while (f.cb()->as_nmethod()->needs_stack_repair()) {\n+    f.next(SmallRegisterMap::instance_no_args(), false \/* stop *\/);\n+  }\n+  assert(!f.is_done(), \"\");\n+  assert(f.is_compiled(), \"\");\n+\n+  intptr_t* bottom = f.sp() + f.cb()->frame_size();\n+  argsize = f.stack_argsize();\n+  return bottom - top;\n+}\n+\n@@ -2214,3 +2252,0 @@\n-    frame_size += f.cb()->frame_size();\n-    argsize = f.stack_argsize();\n-\n@@ -2223,0 +2258,9 @@\n+\n+    if (f.cb()->as_nmethod()->needs_stack_repair()) {\n+      frame_size += remove_scalarized_frames(f, argsize);\n+    } else {\n+      frame_size += f.cb()->frame_size();\n+      argsize = f.stack_argsize();\n+    }\n+  } else if (f.cb()->as_nmethod()->needs_stack_repair()) {\n+    frame_size = remove_scalarized_frames(f, argsize);\n@@ -2522,0 +2566,1 @@\n+  CodeBlob* cb = _stream.cb();\n@@ -2526,3 +2571,8 @@\n-  \/\/ we never leave a compiled caller of an interpreted frame as the top frame in the chunk\n-  \/\/ as it makes detecting that situation and adjusting unextended_sp tricky\n-  if (num_frames == 1 && !_stream.is_done() && FKind::interpreted && _stream.is_compiled()) {\n+  \/\/ We never leave a compiled caller of an interpreted frame as the top frame in the chunk\n+  \/\/ as it makes detecting that situation and adjusting unextended_sp tricky. We also always\n+  \/\/ thaw the caller of a frame that needs_stack_repair, as it would otherwise complicate things:\n+  \/\/ - Regardless of whether the frame was extended or not, we would need to copy the right arg\n+  \/\/   size if its greater than the one given by the normal method signature (non-scalarized).\n+  \/\/ - If the frame was indeed extended, leaving its caller as the top frame would complicate walking\n+  \/\/   the chunk (we need unextended_sp, but we only have sp).\n+  if (num_frames == 1 && !_stream.is_done() && ((FKind::interpreted && _stream.is_compiled()) || (FKind::compiled && cb->as_nmethod_or_null()->needs_stack_repair()))) {\n@@ -2588,1 +2638,1 @@\n-inline void ThawBase::patch(frame& f, const frame& caller, bool bottom) {\n+inline void ThawBase::patch(frame& f, const frame& caller, bool bottom, bool augmented) {\n@@ -2593,1 +2643,1 @@\n-  } else {\n+  } else if (caller.is_compiled_frame()){\n@@ -2596,1 +2646,1 @@\n-    ContinuationHelper::Frame::patch_pc(caller, caller.raw_pc());\n+    ContinuationHelper::Frame::patch_pc(caller, caller.raw_pc(), augmented \/*callee_augmented*\/);\n@@ -2829,0 +2879,10 @@\n+  int fsize = 0;\n+  int added_argsize = 0;\n+  bool augmented = hf.was_augmented_on_entry(fsize);\n+  if (!augmented) {\n+    added_argsize = (is_bottom_frame || caller.is_interpreted_frame()) ? hf.compiled_frame_stack_argsize() : 0;\n+    fsize += added_argsize;\n+  }\n+  assert(!is_bottom_frame || !augmented, \"\");\n+\n+\n@@ -2832,1 +2892,3 @@\n-  frame f = new_stack_frame<ContinuationHelper::CompiledFrame>(hf, caller, is_bottom_frame);\n+  frame f = new_stack_frame<ContinuationHelper::CompiledFrame>(hf, caller, is_bottom_frame, augmented ? fsize - hf.cb()->frame_size() : 0);\n+  assert(f.cb()->frame_size() == (int)(caller.sp() - f.sp()), \"\");\n+\n@@ -2835,5 +2897,0 @@\n-\n-  const int added_argsize = (is_bottom_frame || caller.is_interpreted_frame()) ? hf.compiled_frame_stack_argsize() : 0;\n-  int fsize = ContinuationHelper::CompiledFrame::size(hf) + added_argsize;\n-  assert(fsize <= (int)(caller.unextended_sp() - f.unextended_sp()), \"\");\n-\n@@ -2852,1 +2909,1 @@\n-  patch(f, caller, is_bottom_frame);\n+  patch(f, caller, is_bottom_frame, augmented);\n","filename":"src\/hotspot\/share\/runtime\/continuationFreezeThaw.cpp","additions":83,"deletions":26,"binary":false,"changes":109,"status":"modified"},{"patch":"@@ -816,1 +816,1 @@\n-  develop(bool, PrintFieldLayout, false,                                    \\\n+  product(bool, PrintFieldLayout, false, DIAGNOSTIC,                        \\\n@@ -819,0 +819,27 @@\n+  product(bool, PrintInlineLayout, false, DIAGNOSTIC,                       \\\n+          \"Print field layout for each inline type or class with inline fields\") \\\n+                                                                            \\\n+  product(bool, PrintFlatArrayLayout, false, DIAGNOSTIC,                    \\\n+          \"Print array layout for each inline type array\")                  \\\n+                                                                            \\\n+  product(bool, UseArrayFlattening, true,                                   \\\n+          \"Allow the VM to flatten arrays\")                                 \\\n+                                                                            \\\n+  product(bool, UseFieldFlattening, true,                                   \\\n+          \"Allow the VM to flatten value fields\")                           \\\n+                                                                            \\\n+  product(bool, UseNonAtomicValueFlattening, true,                          \\\n+          \"Allow the JVM to flatten some non-atomic null-free values\")      \\\n+                                                                            \\\n+  product(bool, UseNullableValueFlattening, true,                           \\\n+          \"Allow the JVM to flatten some nullable values\")                  \\\n+                                                                            \\\n+  product(bool, UseAtomicValueFlattening, true,                             \\\n+          \"Allow the JVM to flatten some atomic values\")                    \\\n+                                                                            \\\n+  product(intx, FlatArrayElementMaxOops, 4,                                 \\\n+          \"Max nof embedded object references in an inline type to flatten, <0 no limit\")  \\\n+                                                                            \\\n+  develop(ccstrlist, PrintInlineKlassFields, \"\",                            \\\n+          \"Print fields collected by InlineKlass::collect_fields\")          \\\n+                                                                            \\\n@@ -1778,0 +1805,3 @@\n+  product(bool, IgnoreAssertUnsetFields, false, DIAGNOSTIC,                           \\\n+          \"Ignore assert_unset_fields\")                                     \\\n+                                                                            \\\n@@ -1949,0 +1979,20 @@\n+  product(bool, EnableValhalla, true,                                       \\\n+          \"Enable experimental Valhalla features\")                          \\\n+                                                                            \\\n+  product_pd(bool, InlineTypePassFieldsAsArgs,                              \\\n+          \"Pass each inline type field as an argument at calls\")            \\\n+                                                                            \\\n+  product_pd(bool, InlineTypeReturnedAsFields,                              \\\n+          \"Return fields instead of an inline type reference\")              \\\n+                                                                            \\\n+  develop(bool, StressCallingConvention, false,                             \\\n+          \"Stress the scalarized calling convention.\")                      \\\n+                                                                            \\\n+  develop(bool, PreloadClasses, true,                                       \\\n+          \"Preloading all classes from the LoadableDescriptors attribute\")  \\\n+                                                                            \\\n+  product(ccstrlist, ForceNonTearable, \"\", DIAGNOSTIC,                      \\\n+          \"List of inline classes which are forced to be atomic \"           \\\n+          \"(whitespace and commas separate names, \"                         \\\n+          \"and leading and trailing stars '*' are wildcards)\")              \\\n+                                                                            \\\n@@ -2007,0 +2057,3 @@\n+  product(bool, UseAltSubstitutabilityMethod, false,                        \\\n+          \"Use alternate version of the isSubstitutable method to \"         \\\n+          \"compare value class instances\")                                  \\\n","filename":"src\/hotspot\/share\/runtime\/globals.hpp","additions":54,"deletions":1,"binary":false,"changes":55,"status":"modified"},{"patch":"@@ -50,0 +50,1 @@\n+#include \"oops\/inlineKlass.hpp\"\n","filename":"src\/hotspot\/share\/runtime\/javaThread.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -151,0 +151,1 @@\n+  oop           _return_buffered_value; \/\/ buffered value being returned\n@@ -821,0 +822,3 @@\n+  oop return_buffered_value() const              { return _return_buffered_value; }\n+  void set_return_buffered_value(oop val)        { _return_buffered_value = val; }\n+\n@@ -883,0 +887,1 @@\n+  static ByteSize return_buffered_value_offset() { return byte_offset_of(JavaThread, _return_buffered_value); }\n","filename":"src\/hotspot\/share\/runtime\/javaThread.hpp","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -48,0 +48,1 @@\n+#include \"memory\/oopFactory.hpp\"\n@@ -51,0 +52,3 @@\n+#include \"oops\/access.hpp\"\n+#include \"oops\/fieldStreams.inline.hpp\"\n+#include \"oops\/inlineKlass.inline.hpp\"\n@@ -54,0 +58,1 @@\n+#include \"oops\/objArrayOop.inline.hpp\"\n@@ -73,0 +78,1 @@\n+#include \"runtime\/signature.hpp\"\n@@ -1230,0 +1236,16 @@\n+  \/\/ Substitutability test implementation piggy backs on static call resolution\n+  Bytecodes::Code code = caller->java_code_at(bci);\n+  if (code == Bytecodes::_if_acmpeq || code == Bytecodes::_if_acmpne) {\n+    bc = Bytecodes::_invokestatic;\n+    methodHandle attached_method(THREAD, extract_attached_method(vfst));\n+    assert(attached_method.not_null(), \"must have attached method\");\n+    vmClasses::ValueObjectMethods_klass()->initialize(CHECK_NH);\n+    LinkResolver::resolve_invoke(callinfo, receiver, attached_method, bc, false, CHECK_NH);\n+#ifdef ASSERT\n+    Symbol* subst_method_name = UseAltSubstitutabilityMethod ? vmSymbols::isSubstitutableAlt_name() : vmSymbols::isSubstitutable_name();\n+    Method* is_subst = vmClasses::ValueObjectMethods_klass()->find_method(subst_method_name, vmSymbols::object_object_boolean_signature());\n+    assert(callinfo.selected_method() == is_subst, \"must be isSubstitutable method\");\n+#endif\n+    return receiver;\n+  }\n+\n@@ -1265,0 +1287,6 @@\n+    } else {\n+      assert(attached_method->has_scalarized_args(), \"invalid use of attached method\");\n+      if (!attached_method->method_holder()->is_inline_klass()) {\n+        \/\/ Ignore the attached method in this case to not confuse below code\n+        attached_method = methodHandle(current, nullptr);\n+      }\n@@ -1273,0 +1301,1 @@\n+  bool check_null_and_abstract = true;\n@@ -1286,2 +1315,3 @@\n-    if (attached_method.is_null()) {\n-      Method* callee = bytecode.static_target(CHECK_NH);\n+    Method* callee = attached_method();\n+    if (callee == nullptr) {\n+      callee = bytecode.static_target(CHECK_NH);\n@@ -1292,7 +1322,17 @@\n-\n-    \/\/ Retrieve from a compiled argument list\n-    receiver = Handle(current, callerFrame.retrieve_receiver(&reg_map2));\n-    assert(oopDesc::is_oop_or_null(receiver()), \"\");\n-\n-    if (receiver.is_null()) {\n-      THROW_(vmSymbols::java_lang_NullPointerException(), nullHandle);\n+    bool caller_is_c1 = callerFrame.is_compiled_frame() && callerFrame.cb()->as_nmethod()->is_compiled_by_c1();\n+    if (!caller_is_c1 && callee->is_scalarized_arg(0)) {\n+      \/\/ If the receiver is an inline type that is passed as fields, no oop is available\n+      \/\/ Resolve the call without receiver null checking.\n+      assert(!callee->mismatch(), \"calls with inline type receivers should never mismatch\");\n+      assert(attached_method.not_null() && !attached_method->is_abstract(), \"must have non-abstract attached method\");\n+      if (bc == Bytecodes::_invokeinterface) {\n+        bc = Bytecodes::_invokevirtual; \/\/ C2 optimistically replaces interface calls by virtual calls\n+      }\n+      check_null_and_abstract = false;\n+    } else {\n+      \/\/ Retrieve from a compiled argument list\n+      receiver = Handle(current, callerFrame.retrieve_receiver(&reg_map2));\n+      assert(oopDesc::is_oop_or_null(receiver()), \"\");\n+      if (receiver.is_null()) {\n+        THROW_(vmSymbols::java_lang_NullPointerException(), nullHandle);\n+      }\n@@ -1305,1 +1345,1 @@\n-    LinkResolver::resolve_invoke(callinfo, receiver, attached_method, bc, CHECK_NH);\n+    LinkResolver::resolve_invoke(callinfo, receiver, attached_method, bc, check_null_and_abstract, CHECK_NH);\n@@ -1314,1 +1354,1 @@\n-  if (has_receiver) {\n+  if (has_receiver && check_null_and_abstract) {\n@@ -1342,1 +1382,1 @@\n-methodHandle SharedRuntime::find_callee_method(TRAPS) {\n+methodHandle SharedRuntime::find_callee_method(bool& caller_does_not_scalarize, TRAPS) {\n@@ -1368,0 +1408,4 @@\n+    \/\/ Calls via mismatching methods are always non-scalarized\n+    if (callinfo.resolved_method()->mismatch()) {\n+      caller_does_not_scalarize = true;\n+    }\n@@ -1375,1 +1419,1 @@\n-methodHandle SharedRuntime::resolve_helper(bool is_virtual, bool is_optimized, TRAPS) {\n+methodHandle SharedRuntime::resolve_helper(bool is_virtual, bool is_optimized, bool& caller_does_not_scalarize, TRAPS) {\n@@ -1398,0 +1442,4 @@\n+  \/\/ Calls via mismatching methods are always non-scalarized\n+  if (caller_nm->is_compiled_by_c1() || call_info.resolved_method()->mismatch()) {\n+    caller_does_not_scalarize = true;\n+  }\n@@ -1416,1 +1464,1 @@\n-    tty->print(\"resolving %s%s (%s) call to\",\n+    tty->print(\"resolving %s%s (%s) %s call to\",\n@@ -1418,1 +1466,1 @@\n-               Bytecodes::name(invoke_code));\n+               Bytecodes::name(invoke_code), (caller_does_not_scalarize) ? \"non-scalar\" : \"\");\n@@ -1457,1 +1505,1 @@\n-    inline_cache->update(&call_info, receiver->klass());\n+    inline_cache->update(&call_info, receiver->klass(), caller_does_not_scalarize);\n@@ -1461,1 +1509,1 @@\n-    callsite->set(callee_method);\n+    callsite->set(callee_method, caller_does_not_scalarize);\n@@ -1481,0 +1529,1 @@\n+  bool caller_does_not_scalarize = false;\n@@ -1482,1 +1531,1 @@\n-    callee_method = SharedRuntime::handle_ic_miss_helper(CHECK_NULL);\n+    callee_method = SharedRuntime::handle_ic_miss_helper(caller_does_not_scalarize, CHECK_NULL);\n@@ -1487,1 +1536,1 @@\n-  return get_resolved_entry(current, callee_method);\n+  return get_resolved_entry(current, callee_method, false, false, caller_does_not_scalarize);\n@@ -1528,1 +1577,5 @@\n-      return callee->get_c2i_entry();\n+      if (caller_frame.is_interpreted_frame()) {\n+        return callee->get_c2i_inline_entry();\n+      } else {\n+        return callee->get_c2i_entry();\n+      }\n@@ -1534,0 +1587,3 @@\n+  bool is_static_call = false;\n+  bool is_optimized = false;\n+  bool caller_does_not_scalarize = false;\n@@ -1536,1 +1592,1 @@\n-    callee_method = SharedRuntime::reresolve_call_site(CHECK_NULL);\n+    callee_method = SharedRuntime::reresolve_call_site(is_optimized, caller_does_not_scalarize, CHECK_NULL);\n@@ -1540,1 +1596,1 @@\n-  return get_resolved_entry(current, callee_method);\n+  return get_resolved_entry(current, callee_method, callee_method->is_static(), is_optimized, caller_does_not_scalarize);\n@@ -1579,1 +1635,2 @@\n-address SharedRuntime::get_resolved_entry(JavaThread* current, methodHandle callee_method) {\n+address SharedRuntime::get_resolved_entry(JavaThread* current, methodHandle callee_method,\n+                                          bool is_static_call, bool is_optimized, bool caller_does_not_scalarize) {\n@@ -1585,2 +1642,11 @@\n-  assert(callee_method->verified_code_entry() != nullptr, \" Jump to zero!\");\n-  return callee_method->verified_code_entry();\n+\n+  if (caller_does_not_scalarize) {\n+    assert(callee_method->verified_inline_code_entry() != nullptr, \"Jump to zero!\");\n+    return callee_method->verified_inline_code_entry();\n+  } else if (is_static_call || is_optimized) {\n+    assert(callee_method->verified_code_entry() != nullptr, \"Jump to zero!\");\n+    return callee_method->verified_code_entry();\n+  } else {\n+    assert(callee_method->verified_inline_ro_code_entry() != nullptr, \"Jump to zero!\");\n+    return callee_method->verified_inline_ro_code_entry();\n+  }\n@@ -1592,0 +1658,1 @@\n+  bool caller_does_not_scalarize = false;\n@@ -1594,1 +1661,1 @@\n-    callee_method = SharedRuntime::resolve_helper(false, false, CHECK_NULL);\n+    callee_method = SharedRuntime::resolve_helper(false, false, caller_does_not_scalarize, CHECK_NULL);\n@@ -1598,1 +1665,1 @@\n-  return get_resolved_entry(current, callee_method);\n+  return get_resolved_entry(current, callee_method, true, false, caller_does_not_scalarize);\n@@ -1604,0 +1671,1 @@\n+  bool caller_does_not_scalarize = false;\n@@ -1605,1 +1673,1 @@\n-    callee_method = SharedRuntime::resolve_helper(true, false, CHECK_NULL);\n+    callee_method = SharedRuntime::resolve_helper(true, false, caller_does_not_scalarize, CHECK_NULL);\n@@ -1609,1 +1677,1 @@\n-  return get_resolved_entry(current, callee_method);\n+  return get_resolved_entry(current, callee_method, false, false, caller_does_not_scalarize);\n@@ -1617,0 +1685,1 @@\n+  bool caller_does_not_scalarize = false;\n@@ -1618,1 +1687,1 @@\n-    callee_method = SharedRuntime::resolve_helper(true, true, CHECK_NULL);\n+    callee_method = SharedRuntime::resolve_helper(true, true, caller_does_not_scalarize, CHECK_NULL);\n@@ -1622,1 +1691,1 @@\n-  return get_resolved_entry(current, callee_method);\n+  return get_resolved_entry(current, callee_method, false, true, caller_does_not_scalarize);\n@@ -1625,1 +1694,3 @@\n-methodHandle SharedRuntime::handle_ic_miss_helper(TRAPS) {\n+\n+\n+methodHandle SharedRuntime::handle_ic_miss_helper(bool& caller_does_not_scalarize, TRAPS) {\n@@ -1643,1 +1714,1 @@\n-    tty->print(\"IC miss (%s) call to\", Bytecodes::name(bc));\n+    tty->print(\"IC miss (%s) %s call to\", Bytecodes::name(bc), (caller_does_not_scalarize) ? \"non-scalar\" : \"\");\n@@ -1675,0 +1746,4 @@\n+  \/\/ Calls via mismatching methods are always non-scalarized\n+  if (caller_nm->is_compiled_by_c1() || call_info.resolved_method()->mismatch()) {\n+    caller_does_not_scalarize = true;\n+  }\n@@ -1678,1 +1753,1 @@\n-  inline_cache->update(&call_info, receiver()->klass());\n+  inline_cache->update(&call_info, receiver()->klass(), caller_does_not_scalarize);\n@@ -1689,1 +1764,1 @@\n-methodHandle SharedRuntime::reresolve_call_site(TRAPS) {\n+methodHandle SharedRuntime::reresolve_call_site(bool& is_optimized, bool& caller_does_not_scalarize, TRAPS) {\n@@ -1699,8 +1774,21 @@\n-\n-  \/\/ Do nothing if the frame isn't a live compiled frame.\n-  \/\/ nmethod could be deoptimized by the time we get here\n-  \/\/ so no update to the caller is needed.\n-\n-  if ((caller.is_compiled_frame() && !caller.is_deoptimized_frame()) ||\n-      (caller.is_native_frame() && caller.cb()->as_nmethod()->method()->is_continuation_enter_intrinsic())) {\n-\n+  if (caller.is_compiled_frame()) {\n+    caller_does_not_scalarize = caller.cb()->as_nmethod()->is_compiled_by_c1();\n+  }\n+  assert(!caller.is_interpreted_frame(), \"must be compiled\");\n+\n+  \/\/ If the frame isn't a live compiled frame (i.e. deoptimized by the time we get here), no IC clearing must be done\n+  \/\/ for the caller. However, when the caller is C2 compiled and the callee a C1 or C2 compiled method, then we still\n+  \/\/ need to figure out whether it was an optimized virtual call with an inline type receiver. Otherwise, we end up\n+  \/\/ using the wrong method entry point and accidentally skip the buffering of the receiver.\n+  methodHandle callee_method = find_callee_method(caller_does_not_scalarize, CHECK_(methodHandle()));\n+  const bool caller_is_compiled_and_not_deoptimized = caller.is_compiled_frame() && !caller.is_deoptimized_frame();\n+  const bool caller_is_continuation_enter_intrinsic =\n+    caller.is_native_frame() && caller.cb()->as_nmethod()->method()->is_continuation_enter_intrinsic();\n+  const bool do_IC_clearing = caller_is_compiled_and_not_deoptimized || caller_is_continuation_enter_intrinsic;\n+\n+  const bool callee_compiled_with_scalarized_receiver = callee_method->has_compiled_code() &&\n+                                                        !callee_method()->is_static() &&\n+                                                        callee_method()->is_scalarized_arg(0);\n+  const bool compute_is_optimized = !caller_does_not_scalarize && callee_compiled_with_scalarized_receiver;\n+\n+  if (do_IC_clearing || compute_is_optimized) {\n@@ -1739,0 +1827,1 @@\n+        is_optimized = false;\n@@ -1741,0 +1830,1 @@\n+            assert(callee_method->is_static(), \"must be\");\n@@ -1742,2 +1832,5 @@\n-            CompiledDirectCall* cdc = CompiledDirectCall::at(call_addr);\n-            cdc->set_to_clean();\n+            is_optimized = (iter.type() == relocInfo::opt_virtual_call_type);\n+            if (do_IC_clearing) {\n+              CompiledDirectCall* cdc = CompiledDirectCall::at(call_addr);\n+              cdc->set_to_clean();\n+            }\n@@ -1746,4 +1839,5 @@\n-\n-            \/\/ compiled, dispatched call (which used to call an interpreted method)\n-            CompiledIC* inline_cache = CompiledIC_at(caller_nm, call_addr);\n-            inline_cache->set_to_clean();\n+            if (do_IC_clearing) {\n+              \/\/ compiled, dispatched call (which used to call an interpreted method)\n+              CompiledIC* inline_cache = CompiledIC_at(caller_nm, call_addr);\n+              inline_cache->set_to_clean();\n+            }\n@@ -1760,3 +1854,0 @@\n-  methodHandle callee_method = find_callee_method(CHECK_(methodHandle()));\n-\n-\n@@ -1768,1 +1859,1 @@\n-    tty->print(\"handle_wrong_method reresolving call to\");\n+    tty->print(\"handle_wrong_method reresolving %s call to\", (caller_does_not_scalarize) ? \"non-scalar\" : \"\");\n@@ -1974,0 +2065,15 @@\n+char* SharedRuntime::generate_identity_exception_message(JavaThread* current, Klass* klass) {\n+  assert(klass->is_inline_klass(), \"Must be a concrete value class\");\n+  const char* desc = \"Cannot synchronize on an instance of value class \";\n+  const char* className = klass->external_name();\n+  size_t msglen = strlen(desc) + strlen(className) + 1;\n+  char* message = NEW_RESOURCE_ARRAY(char, msglen);\n+  if (nullptr == message) {\n+    \/\/ Out of memory: can't create detailed error message\n+    message = const_cast<char*>(klass->external_name());\n+  } else {\n+    jio_snprintf(message, msglen, \"%s%s\", desc, className);\n+  }\n+  return message;\n+}\n+\n@@ -2207,5 +2313,30 @@\n- private:\n-  enum {\n-    _basic_type_bits = 4,\n-    _basic_type_mask = right_n_bits(_basic_type_bits),\n-    _basic_types_per_int = BitsPerInt \/ _basic_type_bits,\n+public:\n+  class Element {\n+  private:\n+    \/\/ The highest byte is the type of the argument. The remaining bytes contain the offset of the\n+    \/\/ field if it is flattened in the calling convention, -1 otherwise.\n+    juint _payload;\n+\n+    static constexpr int offset_bit_width = 24;\n+    static constexpr juint offset_bit_mask = (1 << offset_bit_width) - 1;\n+  public:\n+    Element(BasicType bt, int offset) : _payload((static_cast<juint>(bt) << offset_bit_width) | (juint(offset) & offset_bit_mask)) {\n+      assert(offset >= -1 && offset < jint(offset_bit_mask), \"invalid offset %d\", offset);\n+    }\n+\n+    BasicType bt() const {\n+      return static_cast<BasicType>(_payload >> offset_bit_width);\n+    }\n+\n+    int offset() const {\n+      juint res = _payload & offset_bit_mask;\n+      return res == offset_bit_mask ? -1 : res;\n+    }\n+\n+    juint hash() const {\n+      return _payload;\n+    }\n+\n+    bool operator!=(const Element& other) const {\n+      return _payload != other._payload;\n+    }\n@@ -2213,3 +2344,3 @@\n-  \/\/ TO DO:  Consider integrating this with a more global scheme for compressing signatures.\n-  \/\/ For now, 4 bits per components (plus T_VOID gaps after double\/long) is not excessive.\n-  int _length;\n+private:\n+  const bool _has_ro_adapter;\n+  const int _length;\n@@ -2219,2 +2350,8 @@\n-  int* data_pointer() {\n-    return (int*)((address)this + data_offset());\n+  Element* data_pointer() {\n+    return reinterpret_cast<Element*>(reinterpret_cast<address>(this) + data_offset());\n+  }\n+\n+  const Element& element_at(int index) {\n+    assert(index < length(), \"index %d out of bounds for length %d\", index, length());\n+    Element* data = data_pointer();\n+    return data[index];\n@@ -2224,6 +2361,5 @@\n-  AdapterFingerPrint(int total_args_passed, BasicType* sig_bt, int len) {\n-    int* data = data_pointer();\n-    \/\/ Pack the BasicTypes with 8 per int\n-    assert(len == length(total_args_passed), \"sanity\");\n-    _length = len;\n-    int sig_index = 0;\n+  AdapterFingerPrint(const GrowableArray<SigEntry>* sig, bool has_ro_adapter)\n+    : _has_ro_adapter(has_ro_adapter), _length(total_args_passed_in_sig(sig)) {\n+    Element* data = data_pointer();\n+    BasicType prev_bt = T_ILLEGAL;\n+    int vt_count = 0;\n@@ -2231,5 +2367,15 @@\n-      int value = 0;\n-      for (int byte = 0; sig_index < total_args_passed && byte < _basic_types_per_int; byte++) {\n-        int bt = adapter_encoding(sig_bt[sig_index++]);\n-        assert((bt & _basic_type_mask) == bt, \"must fit in 4 bits\");\n-        value = (value << _basic_type_bits) | bt;\n+      const SigEntry& sig_entry = sig->at(index);\n+      BasicType bt = sig_entry._bt;\n+      if (bt == T_METADATA) {\n+        \/\/ Found start of inline type in signature\n+        assert(InlineTypePassFieldsAsArgs, \"unexpected start of inline type\");\n+        vt_count++;\n+      } else if (bt == T_VOID && prev_bt != T_LONG && prev_bt != T_DOUBLE) {\n+        \/\/ Found end of inline type in signature\n+        assert(InlineTypePassFieldsAsArgs, \"unexpected end of inline type\");\n+        vt_count--;\n+        assert(vt_count >= 0, \"invalid vt_count\");\n+      } else if (vt_count == 0) {\n+        \/\/ Widen fields that are not part of a scalarized inline type argument\n+        assert(sig_entry._offset == -1, \"invalid offset for argument that is not a flattened field %d\", sig_entry._offset);\n+        bt = adapter_encoding(bt);\n@@ -2237,1 +2383,3 @@\n-      data[index] = value;\n+\n+      ::new(&data[index]) Element(bt, sig_entry._offset);\n+      prev_bt = bt;\n@@ -2239,0 +2387,1 @@\n+    assert(vt_count == 0, \"invalid vt_count\");\n@@ -2246,2 +2395,2 @@\n-  static int length(int total_args) {\n-    return (total_args + (_basic_types_per_int-1)) \/ _basic_types_per_int;\n+  static int total_args_passed_in_sig(const GrowableArray<SigEntry>* sig) {\n+    return (sig != nullptr) ? sig->length() : 0;\n@@ -2251,1 +2400,1 @@\n-    return (int)heap_word_size(sizeof(AdapterFingerPrint) + (len * sizeof(int)));\n+    return (int)heap_word_size(sizeof(AdapterFingerPrint) + (len * sizeof(Element)));\n@@ -2257,1 +2406,1 @@\n-  static int adapter_encoding(BasicType in) {\n+  static BasicType adapter_encoding(BasicType in) {\n@@ -2263,1 +2412,1 @@\n-        \/\/ There are all promoted to T_INT in the calling convention\n+        \/\/ They are all promoted to T_INT in the calling convention\n@@ -2296,0 +2445,1 @@\n+public:\n@@ -2299,10 +2449,1 @@\n-      unsigned val = (unsigned)value(i);\n-      \/\/ args are packed so that first\/lower arguments are in the highest\n-      \/\/ bits of each int value, so iterate from highest to the lowest\n-      for (int j = 32 - _basic_type_bits; j >= 0; j -= _basic_type_bits) {\n-        unsigned v = (val >> j) & _basic_type_mask;\n-        if (v == 0) {\n-          continue;\n-        }\n-        function(v);\n-      }\n+      function(element_at(i));\n@@ -2312,3 +2453,2 @@\n- public:\n-  static AdapterFingerPrint* allocate(int total_args_passed, BasicType* sig_bt) {\n-    int len = length(total_args_passed);\n+  static AdapterFingerPrint* allocate(const GrowableArray<SigEntry>* sig, bool has_ro_adapter = false) {\n+    int len = total_args_passed_in_sig(sig);\n@@ -2316,1 +2456,1 @@\n-    AdapterFingerPrint* afp = new (size_in_bytes) AdapterFingerPrint(total_args_passed, sig_bt, len);\n+    AdapterFingerPrint* afp = new (size_in_bytes) AdapterFingerPrint(sig, has_ro_adapter);\n@@ -2325,3 +2465,2 @@\n-  int value(int index) {\n-    int* data = data_pointer();\n-    return data[index];\n+  bool has_ro_adapter() const {\n+    return _has_ro_adapter;\n@@ -2330,1 +2469,1 @@\n-  int length() {\n+  int length() const {\n@@ -2337,1 +2476,1 @@\n-      int v = value(i);\n+      const Element& v = element_at(i);\n@@ -2339,1 +2478,1 @@\n-      hash = ((hash << 8) ^ v ^ (hash >> 5)) + 3;\n+      hash = ((hash << 8) ^ v.hash() ^ (hash >> 5)) + 3;\n@@ -2346,1 +2485,6 @@\n-    st.print(\"0x\");\n+    st.print(\"{\");\n+    if (_has_ro_adapter) {\n+      st.print(\"has_ro_adapter\");\n+    } else {\n+      st.print(\"no_ro_adapter\");\n+    }\n@@ -2348,1 +2492,3 @@\n-      st.print(\"%x\", value(i));\n+      st.print(\", \");\n+      const Element& elem = element_at(i);\n+      st.print(\"{%s, %d}\", type2name(elem.bt()), elem.offset());\n@@ -2350,0 +2496,1 @@\n+    st.print(\"}\");\n@@ -2356,1 +2503,1 @@\n-    iterate_args([&] (int arg) {\n+    iterate_args([&] (const Element& arg) {\n@@ -2359,1 +2506,1 @@\n-        if (arg == T_VOID) {\n+        if (arg.bt() == T_VOID) {\n@@ -2365,7 +2512,4 @@\n-      switch (arg) {\n-        case T_INT:    st.print(\"I\");    break;\n-        case T_LONG:   long_prev = true; break;\n-        case T_FLOAT:  st.print(\"F\");    break;\n-        case T_DOUBLE: st.print(\"D\");    break;\n-        case T_VOID:   break;\n-        default: ShouldNotReachHere();\n+      if (arg.bt() == T_LONG) {\n+        long_prev = true;\n+      } else if (arg.bt() != T_VOID) {\n+        st.print(\"%c\", type2char(arg.bt()));\n@@ -2380,52 +2524,3 @@\n-  BasicType* as_basic_type(int& nargs) {\n-    nargs = 0;\n-    GrowableArray<BasicType> btarray;\n-    bool long_prev = false;\n-\n-    iterate_args([&] (int arg) {\n-      if (long_prev) {\n-        long_prev = false;\n-        if (arg == T_VOID) {\n-          btarray.append(T_LONG);\n-        } else {\n-          btarray.append(T_OBJECT); \/\/ it could be T_ARRAY; it shouldn't matter\n-        }\n-      }\n-      switch (arg) {\n-        case T_INT: \/\/ fallthrough\n-        case T_FLOAT: \/\/ fallthrough\n-        case T_DOUBLE:\n-        case T_VOID:\n-          btarray.append((BasicType)arg);\n-          break;\n-        case T_LONG:\n-          long_prev = true;\n-          break;\n-        default: ShouldNotReachHere();\n-      }\n-    });\n-\n-    if (long_prev) {\n-      btarray.append(T_OBJECT);\n-    }\n-\n-    nargs = btarray.length();\n-    BasicType* sig_bt = NEW_RESOURCE_ARRAY(BasicType, nargs);\n-    int index = 0;\n-    GrowableArrayIterator<BasicType> iter = btarray.begin();\n-    while (iter != btarray.end()) {\n-      sig_bt[index++] = *iter;\n-      ++iter;\n-    }\n-    assert(index == btarray.length(), \"sanity check\");\n-#ifdef ASSERT\n-    {\n-      AdapterFingerPrint* compare_fp = AdapterFingerPrint::allocate(nargs, sig_bt);\n-      assert(this->equals(compare_fp), \"sanity check\");\n-      AdapterFingerPrint::deallocate(compare_fp);\n-    }\n-#endif\n-    return sig_bt;\n-  }\n-\n-    if (other->_length != _length) {\n+    if (other->_has_ro_adapter != _has_ro_adapter) {\n+      return false;\n+    } else if (other->_length != _length) {\n@@ -2436,1 +2531,1 @@\n-        if (value(i) != other->value(i)) {\n+        if (element_at(i) != other->element_at(i)) {\n@@ -2479,1 +2574,1 @@\n-AdapterHandlerEntry* AdapterHandlerLibrary::lookup(int total_args_passed, BasicType* sig_bt) {\n+AdapterHandlerEntry* AdapterHandlerLibrary::lookup(const GrowableArray<SigEntry>* sig, bool has_ro_adapter) {\n@@ -2482,1 +2577,1 @@\n-  AdapterFingerPrint* fp = AdapterFingerPrint::allocate(total_args_passed, sig_bt);\n+  AdapterFingerPrint* fp = AdapterFingerPrint::allocate(sig, has_ro_adapter);\n@@ -2539,1 +2634,1 @@\n-static const int AdapterHandlerLibrary_size = 16*K;\n+static const int AdapterHandlerLibrary_size = 48*K;\n@@ -2587,1 +2682,3 @@\n-    _no_arg_handler = create_adapter(0, nullptr);\n+    CompiledEntrySignature no_args;\n+    no_args.compute_calling_conventions();\n+    _no_arg_handler = create_adapter(no_args, true);\n@@ -2589,2 +2686,4 @@\n-    BasicType obj_args[] = { T_OBJECT };\n-    _obj_arg_handler = create_adapter(1, obj_args);\n+    CompiledEntrySignature obj_args;\n+    SigEntry::add_entry(obj_args.sig(), T_OBJECT);\n+    obj_args.compute_calling_conventions();\n+    _obj_arg_handler = create_adapter(obj_args, true);\n@@ -2592,2 +2691,4 @@\n-    BasicType int_args[] = { T_INT };\n-    _int_arg_handler = create_adapter(1, int_args);\n+    CompiledEntrySignature int_args;\n+    SigEntry::add_entry(int_args.sig(), T_INT);\n+    int_args.compute_calling_conventions();\n+    _int_arg_handler = create_adapter(int_args, true);\n@@ -2595,2 +2696,5 @@\n-    BasicType obj_int_args[] = { T_OBJECT, T_INT };\n-    _obj_int_arg_handler = create_adapter(2, obj_int_args);\n+    CompiledEntrySignature obj_int_args;\n+    SigEntry::add_entry(obj_int_args.sig(), T_OBJECT);\n+    SigEntry::add_entry(obj_int_args.sig(), T_INT);\n+    obj_int_args.compute_calling_conventions();\n+    _obj_int_arg_handler = create_adapter(obj_int_args, true);\n@@ -2598,2 +2702,5 @@\n-    BasicType obj_obj_args[] = { T_OBJECT, T_OBJECT };\n-    _obj_obj_arg_handler = create_adapter(2, obj_obj_args);\n+    CompiledEntrySignature obj_obj_args;\n+    SigEntry::add_entry(obj_obj_args.sig(), T_OBJECT);\n+    SigEntry::add_entry(obj_obj_args.sig(), T_OBJECT);\n+    obj_obj_args.compute_calling_conventions();\n+    _obj_obj_arg_handler = create_adapter(obj_obj_args, true);\n@@ -2633,0 +2740,3 @@\n+      if (InlineTypePassFieldsAsArgs && method->method_holder()->is_inline_klass()) {\n+        return nullptr;\n+      }\n@@ -2636,1 +2746,10 @@\n-      case JVM_SIGNATURE_CLASS:\n+      case JVM_SIGNATURE_CLASS: {\n+        if (InlineTypePassFieldsAsArgs) {\n+          SignatureStream ss(method->signature());\n+          InlineKlass* vk = ss.as_inline_klass(method->method_holder());\n+          if (vk != nullptr) {\n+            return nullptr;\n+          }\n+        }\n+        return _obj_arg_handler;\n+      }\n@@ -2647,1 +2766,1 @@\n-             !method->is_static()) {\n+             !method->is_static() && (!InlineTypePassFieldsAsArgs || !method->method_holder()->is_inline_klass())) {\n@@ -2649,1 +2768,10 @@\n-      case JVM_SIGNATURE_CLASS:\n+      case JVM_SIGNATURE_CLASS: {\n+        if (InlineTypePassFieldsAsArgs) {\n+          SignatureStream ss(method->signature());\n+          InlineKlass* vk = ss.as_inline_klass(method->method_holder());\n+          if (vk != nullptr) {\n+            return nullptr;\n+          }\n+        }\n+        return _obj_obj_arg_handler;\n+      }\n@@ -2663,5 +2791,9 @@\n-class AdapterSignatureIterator : public SignatureIterator {\n- private:\n-  BasicType stack_sig_bt[16];\n-  BasicType* sig_bt;\n-  int index;\n+CompiledEntrySignature::CompiledEntrySignature(Method* method) :\n+  _method(method), _num_inline_args(0), _has_inline_recv(false),\n+  _regs(nullptr), _regs_cc(nullptr), _regs_cc_ro(nullptr),\n+  _args_on_stack(0), _args_on_stack_cc(0), _args_on_stack_cc_ro(0),\n+  _c1_needs_stack_repair(false), _c2_needs_stack_repair(false), _supers(nullptr) {\n+  _sig = new GrowableArray<SigEntry>((method != nullptr) ? method->size_of_parameters() : 1);\n+  _sig_cc = new GrowableArray<SigEntry>((method != nullptr) ? method->size_of_parameters() : 1);\n+  _sig_cc_ro = new GrowableArray<SigEntry>((method != nullptr) ? method->size_of_parameters() : 1);\n+}\n@@ -2669,11 +2801,24 @@\n- public:\n-  AdapterSignatureIterator(Symbol* signature,\n-                           fingerprint_t fingerprint,\n-                           bool is_static,\n-                           int total_args_passed) :\n-    SignatureIterator(signature, fingerprint),\n-    index(0)\n-  {\n-    sig_bt = (total_args_passed <= 16) ? stack_sig_bt : NEW_RESOURCE_ARRAY(BasicType, total_args_passed);\n-    if (!is_static) { \/\/ Pass in receiver first\n-      sig_bt[index++] = T_OBJECT;\n+\/\/ See if we can save space by sharing the same entry for VIEP and VIEP(RO),\n+\/\/ or the same entry for VEP and VIEP(RO).\n+CodeOffsets::Entries CompiledEntrySignature::c1_inline_ro_entry_type() const {\n+  if (!has_scalarized_args()) {\n+    \/\/ VEP\/VIEP\/VIEP(RO) all share the same entry. There's no packing.\n+    return CodeOffsets::Verified_Entry;\n+  }\n+  if (_method->is_static()) {\n+    \/\/ Static methods don't need VIEP(RO)\n+    return CodeOffsets::Verified_Entry;\n+  }\n+\n+  if (has_inline_recv()) {\n+    if (num_inline_args() == 1) {\n+      \/\/ Share same entry for VIEP and VIEP(RO).\n+      \/\/ This is quite common: we have an instance method in an InlineKlass that has\n+      \/\/ no inline type args other than <this>.\n+      return CodeOffsets::Verified_Inline_Entry;\n+    } else {\n+      assert(num_inline_args() > 1, \"must be\");\n+      \/\/ No sharing:\n+      \/\/   VIEP(RO) -- <this> is passed as object\n+      \/\/   VEP      -- <this> is passed as fields\n+      return CodeOffsets::Verified_Inline_Entry_RO;\n@@ -2681,1 +2826,0 @@\n-    do_parameters_on(this);\n@@ -2684,2 +2828,9 @@\n-  BasicType* basic_types() {\n-    return sig_bt;\n+  \/\/ Either a static method, or <this> is not an inline type\n+  if (args_on_stack_cc() != args_on_stack_cc_ro()) {\n+    \/\/ No sharing:\n+    \/\/ Some arguments are passed on the stack, and we have inserted reserved entries\n+    \/\/ into the VEP, but we never insert reserved entries into the VIEP(RO).\n+    return CodeOffsets::Verified_Inline_Entry_RO;\n+  } else {\n+    \/\/ Share same entry for VEP and VIEP(RO).\n+    return CodeOffsets::Verified_Entry;\n@@ -2687,0 +2838,1 @@\n+}\n@@ -2688,3 +2840,39 @@\n-#ifdef ASSERT\n-  int slots() {\n-    return index;\n+\/\/ Returns all super methods (transitive) in classes and interfaces that are overridden by the current method.\n+GrowableArray<Method*>* CompiledEntrySignature::get_supers() {\n+  if (_supers != nullptr) {\n+    return _supers;\n+  }\n+  _supers = new GrowableArray<Method*>();\n+  \/\/ Skip private, static, and <init> methods\n+  if (_method->is_private() || _method->is_static() || _method->is_object_constructor()) {\n+    return _supers;\n+  }\n+  Symbol* name = _method->name();\n+  Symbol* signature = _method->signature();\n+  const Klass* holder = _method->method_holder()->super();\n+  Symbol* holder_name = holder->name();\n+  ThreadInVMfromUnknown tiv;\n+  JavaThread* current = JavaThread::current();\n+  HandleMark hm(current);\n+  Handle loader(current, _method->method_holder()->class_loader());\n+\n+  \/\/ Walk up the class hierarchy and search for super methods\n+  while (holder != nullptr) {\n+    Method* super_method = holder->lookup_method(name, signature);\n+    if (super_method == nullptr) {\n+      break;\n+    }\n+    if (!super_method->is_static() && !super_method->is_private() &&\n+        (!super_method->is_package_private() ||\n+         super_method->method_holder()->is_same_class_package(loader(), holder_name))) {\n+      _supers->push(super_method);\n+    }\n+    holder = super_method->method_holder()->super();\n+  }\n+  \/\/ Search interfaces for super methods\n+  Array<InstanceKlass*>* interfaces = _method->method_holder()->transitive_interfaces();\n+  for (int i = 0; i < interfaces->length(); ++i) {\n+    Method* m = interfaces->at(i)->lookup_method(name, signature);\n+    if (m != nullptr && !m->is_static() && m->is_public()) {\n+      _supers->push(m);\n+    }\n@@ -2692,0 +2880,50 @@\n+  return _supers;\n+}\n+\n+\/\/ Iterate over arguments and compute scalarized and non-scalarized signatures\n+void CompiledEntrySignature::compute_calling_conventions(bool init) {\n+  bool has_scalarized = false;\n+  if (_method != nullptr) {\n+    InstanceKlass* holder = _method->method_holder();\n+    int arg_num = 0;\n+    if (!_method->is_static()) {\n+      \/\/ We shouldn't scalarize 'this' in a value class constructor\n+      if (holder->is_inline_klass() && InlineKlass::cast(holder)->can_be_passed_as_fields() && !_method->is_object_constructor() &&\n+          (init || _method->is_scalarized_arg(arg_num))) {\n+        _sig_cc->appendAll(InlineKlass::cast(holder)->extended_sig());\n+        has_scalarized = true;\n+        _has_inline_recv = true;\n+        _num_inline_args++;\n+      } else {\n+        SigEntry::add_entry(_sig_cc, T_OBJECT, holder->name());\n+      }\n+      SigEntry::add_entry(_sig, T_OBJECT, holder->name());\n+      SigEntry::add_entry(_sig_cc_ro, T_OBJECT, holder->name());\n+      arg_num++;\n+    }\n+    for (SignatureStream ss(_method->signature()); !ss.at_return_type(); ss.next()) {\n+      BasicType bt = ss.type();\n+      if (bt == T_OBJECT) {\n+        InlineKlass* vk = ss.as_inline_klass(holder);\n+        if (vk != nullptr && vk->can_be_passed_as_fields() && (init || _method->is_scalarized_arg(arg_num))) {\n+          \/\/ Check for a calling convention mismatch with super method(s)\n+          bool scalar_super = false;\n+          bool non_scalar_super = false;\n+          GrowableArray<Method*>* supers = get_supers();\n+          for (int i = 0; i < supers->length(); ++i) {\n+            Method* super_method = supers->at(i);\n+            if (super_method->is_scalarized_arg(arg_num)) {\n+              scalar_super = true;\n+            } else {\n+              non_scalar_super = true;\n+            }\n+          }\n+#ifdef ASSERT\n+          \/\/ Randomly enable below code paths for stress testing\n+          bool stress = init && StressCallingConvention;\n+          if (stress && (os::random() & 1) == 1) {\n+            non_scalar_super = true;\n+            if ((os::random() & 1) == 1) {\n+              scalar_super = true;\n+            }\n+          }\n@@ -2693,0 +2931,50 @@\n+          if (non_scalar_super) {\n+            \/\/ Found a super method with a non-scalarized argument. Fall back to the non-scalarized calling convention.\n+            if (scalar_super) {\n+              \/\/ Found non-scalar *and* scalar super methods. We can't handle both.\n+              \/\/ Mark the scalar method as mismatch and re-compile call sites to use non-scalarized calling convention.\n+              for (int i = 0; i < supers->length(); ++i) {\n+                Method* super_method = supers->at(i);\n+                if (super_method->is_scalarized_arg(arg_num) DEBUG_ONLY(|| (stress && (os::random() & 1) == 1))) {\n+                  super_method->set_mismatch();\n+                  MutexLocker ml(Compile_lock, Mutex::_safepoint_check_flag);\n+                  JavaThread* thread = JavaThread::current();\n+                  HandleMark hm(thread);\n+                  methodHandle mh(thread, super_method);\n+                  DeoptimizationScope deopt_scope;\n+                  CodeCache::mark_for_deoptimization(&deopt_scope, mh());\n+                  deopt_scope.deoptimize_marked();\n+                }\n+              }\n+            }\n+            \/\/ Fall back to non-scalarized calling convention\n+            SigEntry::add_entry(_sig_cc, T_OBJECT, ss.as_symbol());\n+            SigEntry::add_entry(_sig_cc_ro, T_OBJECT, ss.as_symbol());\n+          } else {\n+            _num_inline_args++;\n+            has_scalarized = true;\n+            int last = _sig_cc->length();\n+            int last_ro = _sig_cc_ro->length();\n+            _sig_cc->appendAll(vk->extended_sig());\n+            _sig_cc_ro->appendAll(vk->extended_sig());\n+            if (bt == T_OBJECT) {\n+              \/\/ Nullable inline type argument, insert InlineTypeNode::NullMarker field right after T_METADATA delimiter\n+              _sig_cc->insert_before(last+1, SigEntry(T_BOOLEAN, -1, nullptr, true));\n+              _sig_cc_ro->insert_before(last_ro+1, SigEntry(T_BOOLEAN, -1, nullptr, true));\n+            }\n+          }\n+        } else {\n+          SigEntry::add_entry(_sig_cc, T_OBJECT, ss.as_symbol());\n+          SigEntry::add_entry(_sig_cc_ro, T_OBJECT, ss.as_symbol());\n+        }\n+        bt = T_OBJECT;\n+      } else {\n+        SigEntry::add_entry(_sig_cc, ss.type(), ss.as_symbol());\n+        SigEntry::add_entry(_sig_cc_ro, ss.type(), ss.as_symbol());\n+      }\n+      SigEntry::add_entry(_sig, bt, ss.as_symbol());\n+      if (bt != T_VOID) {\n+        arg_num++;\n+      }\n+    }\n+  }\n@@ -2694,1 +2982,3 @@\n- private:\n+  \/\/ Compute the non-scalarized calling convention\n+  _regs = NEW_RESOURCE_ARRAY(VMRegPair, _sig->length());\n+  _args_on_stack = SharedRuntime::java_calling_convention(_sig, _regs);\n@@ -2696,5 +2986,16 @@\n-  friend class SignatureIterator;  \/\/ so do_parameters_on can call do_type\n-  void do_type(BasicType type) {\n-    sig_bt[index++] = type;\n-    if (type == T_LONG || type == T_DOUBLE) {\n-      sig_bt[index++] = T_VOID; \/\/ Longs & doubles take 2 Java slots\n+  \/\/ Compute the scalarized calling conventions if there are scalarized inline types in the signature\n+  if (has_scalarized && !_method->is_native()) {\n+    _regs_cc = NEW_RESOURCE_ARRAY(VMRegPair, _sig_cc->length());\n+    _args_on_stack_cc = SharedRuntime::java_calling_convention(_sig_cc, _regs_cc);\n+\n+    _regs_cc_ro = NEW_RESOURCE_ARRAY(VMRegPair, _sig_cc_ro->length());\n+    _args_on_stack_cc_ro = SharedRuntime::java_calling_convention(_sig_cc_ro, _regs_cc_ro);\n+\n+    _c1_needs_stack_repair = (_args_on_stack_cc < _args_on_stack) || (_args_on_stack_cc_ro < _args_on_stack);\n+    _c2_needs_stack_repair = (_args_on_stack_cc > _args_on_stack) || (_args_on_stack_cc > _args_on_stack_cc_ro);\n+\n+    \/\/ Upper bound on stack arguments to avoid hitting the argument limit and\n+    \/\/ bailing out of compilation (\"unsupported incoming calling sequence\").\n+    \/\/ TODO we need a reasonable limit (flag?) here\n+    if (MAX2(_args_on_stack_cc, _args_on_stack_cc_ro) <= 60) {\n+      return; \/\/ Success\n@@ -2703,1 +3004,130 @@\n-};\n+  \/\/ No scalarized args\n+  _sig_cc = _sig;\n+  _regs_cc = _regs;\n+  _args_on_stack_cc = _args_on_stack;\n+\n+  _sig_cc_ro = _sig;\n+  _regs_cc_ro = _regs;\n+  _args_on_stack_cc_ro = _args_on_stack;\n+}\n+\n+void CompiledEntrySignature::initialize_from_fingerprint(AdapterFingerPrint* fingerprint) {\n+  _has_inline_recv = fingerprint->has_ro_adapter();\n+\n+  int value_object_count = 0;\n+  BasicType prev_bt = T_ILLEGAL;\n+  bool has_scalarized_arguments = false;\n+  bool long_prev = false;\n+  int long_prev_offset = -1;\n+\n+  fingerprint->iterate_args([&] (const AdapterFingerPrint::Element& arg) {\n+    BasicType bt = arg.bt();\n+    int offset = arg.offset();\n+\n+    if (long_prev) {\n+      long_prev = false;\n+      BasicType bt_to_add;\n+      if (bt == T_VOID) {\n+        bt_to_add = T_LONG;\n+      } else {\n+        bt_to_add = T_OBJECT;\n+      }\n+      if (value_object_count == 0) {\n+        SigEntry::add_entry(_sig, bt_to_add);\n+      }\n+      SigEntry::add_entry(_sig_cc, bt_to_add, nullptr, long_prev_offset);\n+      SigEntry::add_entry(_sig_cc_ro, bt_to_add, nullptr, long_prev_offset);\n+    }\n+\n+    switch (bt) {\n+      case T_VOID:\n+        if (prev_bt != T_LONG && prev_bt != T_DOUBLE) {\n+          assert(InlineTypePassFieldsAsArgs, \"unexpected end of inline type\");\n+          value_object_count--;\n+          SigEntry::add_entry(_sig_cc, T_VOID, nullptr, offset);\n+          SigEntry::add_entry(_sig_cc_ro, T_VOID, nullptr, offset);\n+          assert(value_object_count >= 0, \"invalid value object count\");\n+        } else {\n+          \/\/ Nothing to add for _sig: We already added an addition T_VOID in add_entry() when adding T_LONG or T_DOUBLE.\n+        }\n+        break;\n+      case T_INT:\n+      case T_FLOAT:\n+      case T_DOUBLE:\n+        if (value_object_count == 0) {\n+          SigEntry::add_entry(_sig, bt);\n+        }\n+        SigEntry::add_entry(_sig_cc, bt, nullptr, offset);\n+        SigEntry::add_entry(_sig_cc_ro, bt, nullptr, offset);\n+        break;\n+      case T_LONG:\n+        long_prev = true;\n+        long_prev_offset = offset;\n+        break;\n+      case T_BOOLEAN:\n+      case T_CHAR:\n+      case T_BYTE:\n+      case T_SHORT:\n+      case T_OBJECT:\n+      case T_ARRAY:\n+        assert(value_object_count > 0, \"must be value object field\");\n+        SigEntry::add_entry(_sig_cc, bt, nullptr, offset);\n+        SigEntry::add_entry(_sig_cc_ro, bt, nullptr, offset);\n+        break;\n+      case T_METADATA:\n+        assert(InlineTypePassFieldsAsArgs, \"unexpected start of inline type\");\n+        if (value_object_count == 0) {\n+          SigEntry::add_entry(_sig, T_OBJECT);\n+        }\n+        SigEntry::add_entry(_sig_cc, T_METADATA, nullptr, offset);\n+        SigEntry::add_entry(_sig_cc_ro, T_METADATA, nullptr, offset);\n+        value_object_count++;\n+        has_scalarized_arguments = true;\n+        break;\n+      default: {\n+        fatal(\"Unexpected BasicType: %s\", basictype_to_str(bt));\n+      }\n+    }\n+    prev_bt = bt;\n+  });\n+\n+  if (long_prev) {\n+    \/\/ If previous bt was T_LONG and we reached the end of the signature, we know that it must be a T_OBJECT.\n+    SigEntry::add_entry(_sig, T_OBJECT);\n+    SigEntry::add_entry(_sig_cc, T_OBJECT);\n+    SigEntry::add_entry(_sig_cc_ro, T_OBJECT);\n+  }\n+  assert(value_object_count == 0, \"invalid value object count\");\n+\n+  _regs = NEW_RESOURCE_ARRAY(VMRegPair, _sig->length());\n+  _args_on_stack = SharedRuntime::java_calling_convention(_sig, _regs);\n+\n+  \/\/ Compute the scalarized calling conventions if there are scalarized inline types in the signature\n+  if (has_scalarized_arguments) {\n+    _regs_cc = NEW_RESOURCE_ARRAY(VMRegPair, _sig_cc->length());\n+    _args_on_stack_cc = SharedRuntime::java_calling_convention(_sig_cc, _regs_cc);\n+\n+    _regs_cc_ro = NEW_RESOURCE_ARRAY(VMRegPair, _sig_cc_ro->length());\n+    _args_on_stack_cc_ro = SharedRuntime::java_calling_convention(_sig_cc_ro, _regs_cc_ro);\n+\n+    _c1_needs_stack_repair = (_args_on_stack_cc < _args_on_stack) || (_args_on_stack_cc_ro < _args_on_stack);\n+    _c2_needs_stack_repair = (_args_on_stack_cc > _args_on_stack) || (_args_on_stack_cc > _args_on_stack_cc_ro);\n+  } else {\n+    \/\/ No scalarized args\n+    _sig_cc = _sig;\n+    _regs_cc = _regs;\n+    _args_on_stack_cc = _args_on_stack;\n+\n+    _sig_cc_ro = _sig;\n+    _regs_cc_ro = _regs;\n+    _args_on_stack_cc_ro = _args_on_stack;\n+  }\n+\n+#ifdef ASSERT\n+  {\n+    AdapterFingerPrint* compare_fp = AdapterFingerPrint::allocate(_sig_cc, _has_inline_recv);\n+    assert(fingerprint->equals(compare_fp), \"%s - %s\", fingerprint->as_string(), compare_fp->as_string());\n+    AdapterFingerPrint::deallocate(compare_fp);\n+  }\n+#endif\n+}\n@@ -2711,1 +3141,1 @@\n-void AdapterHandlerLibrary::verify_adapter_sharing(int total_args_passed, BasicType* sig_bt, AdapterHandlerEntry* cached_entry) {\n+void AdapterHandlerLibrary::verify_adapter_sharing(CompiledEntrySignature& ces, AdapterHandlerEntry* cached_entry) {\n@@ -2714,1 +3144,1 @@\n-  AdapterHandlerEntry* comparison_entry = create_adapter(total_args_passed, sig_bt, true);\n+  AdapterHandlerEntry* comparison_entry = create_adapter(ces, false, true);\n@@ -2739,2 +3169,13 @@\n-  \/\/ Fill in the signature array, for the calling-convention call.\n-  int total_args_passed = method->size_of_parameters(); \/\/ All args on stack\n+  CompiledEntrySignature ces(method());\n+  ces.compute_calling_conventions();\n+  if (ces.has_scalarized_args()) {\n+    if (!method->has_scalarized_args()) {\n+      method->set_has_scalarized_args();\n+    }\n+    if (ces.c1_needs_stack_repair()) {\n+      method->set_c1_needs_stack_repair();\n+    }\n+    if (ces.c2_needs_stack_repair() && !method->c2_needs_stack_repair()) {\n+      method->set_c2_needs_stack_repair();\n+    }\n+  }\n@@ -2742,4 +3183,0 @@\n-  AdapterSignatureIterator si(method->signature(), method->constMethod()->fingerprint(),\n-                              method->is_static(), total_args_passed);\n-  assert(si.slots() == total_args_passed, \"\");\n-  BasicType* sig_bt = si.basic_types();\n@@ -2750,1 +3187,1 @@\n-    entry = lookup(total_args_passed, sig_bt);\n+    entry = lookup(ces.sig_cc(), ces.has_inline_recv());\n@@ -2758,1 +3195,1 @@\n-        verify_adapter_sharing(total_args_passed, sig_bt, entry);\n+        verify_adapter_sharing(ces, entry);\n@@ -2762,1 +3199,1 @@\n-      entry = create_adapter(total_args_passed, sig_bt);\n+      entry = create_adapter(ces, \/* allocate_code_blob *\/ true);\n@@ -2817,0 +3254,2 @@\n+  entry_offset[AdapterBlob::C2I_Inline] = entry_address[AdapterBlob::C2I_Inline] - entry_address[AdapterBlob::I2C];\n+  entry_offset[AdapterBlob::C2I_Inline_RO] = entry_address[AdapterBlob::C2I_Inline_RO] - entry_address[AdapterBlob::I2C];\n@@ -2818,0 +3257,1 @@\n+  entry_offset[AdapterBlob::C2I_Unverified_Inline] = entry_address[AdapterBlob::C2I_Unverified_Inline] - entry_address[AdapterBlob::I2C];\n@@ -2826,2 +3266,2 @@\n-                                                  int total_args_passed,\n-                                                  BasicType* sig_bt,\n+                                                  CompiledEntrySignature& ces,\n+                                                  bool allocate_code_blob,\n@@ -2834,0 +3274,1 @@\n+  AdapterBlob* adapter_blob = nullptr;\n@@ -2840,2 +3281,1 @@\n-  VMRegPair stack_regs[16];\n-  VMRegPair* regs = (total_args_passed <= 16) ? stack_regs : NEW_RESOURCE_ARRAY(VMRegPair, total_args_passed);\n+  address entry_address[AdapterBlob::ENTRY_COUNT];\n@@ -2844,7 +3284,17 @@\n-  int comp_args_on_stack = SharedRuntime::java_calling_convention(sig_bt, regs, total_args_passed);\n-  address entry_address[AdapterBlob::ENTRY_COUNT];\n-                                         total_args_passed,\n-                                         comp_args_on_stack,\n-                                         sig_bt,\n-                                         regs,\n-                                         entry_address);\n+                                         ces.args_on_stack(),\n+                                         ces.sig(),\n+                                         ces.regs(),\n+                                         ces.sig_cc(),\n+                                         ces.regs_cc(),\n+                                         ces.sig_cc_ro(),\n+                                         ces.regs_cc_ro(),\n+                                         entry_address,\n+                                         adapter_blob,\n+                                         allocate_code_blob);\n+\n+  if (ces.has_scalarized_args()) {\n+    \/\/ Save a C heap allocated version of the scalarized signature and store it in the adapter\n+    GrowableArray<SigEntry>* heap_sig = new (mtInternal) GrowableArray<SigEntry>(ces.sig_cc()->length(), mtInternal);\n+    heap_sig->appendAll(ces.sig_cc());\n+    handler->set_sig_cc(heap_sig);\n+  }\n@@ -2864,1 +3314,0 @@\n-  AdapterBlob* adapter_blob = AdapterBlob::create(&buffer, entry_offset);\n@@ -2891,2 +3340,2 @@\n-AdapterHandlerEntry* AdapterHandlerLibrary::create_adapter(int total_args_passed,\n-                                                           BasicType* sig_bt,\n+AdapterHandlerEntry* AdapterHandlerLibrary::create_adapter(CompiledEntrySignature& ces,\n+                                                           bool allocate_code_blob,\n@@ -2894,1 +3343,6 @@\n-  AdapterFingerPrint* fp = AdapterFingerPrint::allocate(total_args_passed, sig_bt);\n+  AdapterFingerPrint* fp = AdapterFingerPrint::allocate(ces.sig_cc(), ces.has_inline_recv());\n+#ifdef ASSERT\n+  \/\/ Verify that we can successfully restore the compiled entry signature object.\n+  CompiledEntrySignature ces_verify;\n+  ces_verify.initialize_from_fingerprint(fp);\n+#endif\n@@ -2896,1 +3350,1 @@\n-  if (!generate_adapter_code(handler, total_args_passed, sig_bt, is_transient)) {\n+  if (!generate_adapter_code(handler, ces, allocate_code_blob, is_transient)) {\n@@ -2999,3 +3453,3 @@\n-    int nargs;\n-    BasicType* bt = _fingerprint->as_basic_type(nargs);\n-    if (!AdapterHandlerLibrary::generate_adapter_code(this, nargs, bt, \/* is_transient *\/ false)) {\n+    CompiledEntrySignature ces;\n+    ces.initialize_from_fingerprint(_fingerprint);\n+    if (!AdapterHandlerLibrary::generate_adapter_code(this, ces, true, false)) {\n@@ -3039,13 +3493,26 @@\n-  _no_arg_handler = lookup(0, nullptr);\n-\n-  BasicType obj_args[] = { T_OBJECT };\n-  _obj_arg_handler = lookup(1, obj_args);\n-\n-  BasicType int_args[] = { T_INT };\n-  _int_arg_handler = lookup(1, int_args);\n-\n-  BasicType obj_int_args[] = { T_OBJECT, T_INT };\n-  _obj_int_arg_handler = lookup(2, obj_int_args);\n-\n-  BasicType obj_obj_args[] = { T_OBJECT, T_OBJECT };\n-  _obj_obj_arg_handler = lookup(2, obj_obj_args);\n+  ResourceMark rm;\n+  CompiledEntrySignature no_args;\n+  no_args.compute_calling_conventions();\n+  _no_arg_handler = lookup(no_args.sig_cc(), no_args.has_inline_recv());\n+\n+  CompiledEntrySignature obj_args;\n+  SigEntry::add_entry(obj_args.sig(), T_OBJECT);\n+  obj_args.compute_calling_conventions();\n+  _obj_arg_handler = lookup(obj_args.sig_cc(), obj_args.has_inline_recv());\n+\n+  CompiledEntrySignature int_args;\n+  SigEntry::add_entry(int_args.sig(), T_INT);\n+  int_args.compute_calling_conventions();\n+  _int_arg_handler = lookup(int_args.sig_cc(), int_args.has_inline_recv());\n+\n+  CompiledEntrySignature obj_int_args;\n+  SigEntry::add_entry(obj_int_args.sig(), T_OBJECT);\n+  SigEntry::add_entry(obj_int_args.sig(), T_INT);\n+  obj_int_args.compute_calling_conventions();\n+  _obj_int_arg_handler = lookup(obj_int_args.sig_cc(), obj_int_args.has_inline_recv());\n+\n+  CompiledEntrySignature obj_obj_args;\n+  SigEntry::add_entry(obj_obj_args.sig(), T_OBJECT);\n+  SigEntry::add_entry(obj_obj_args.sig(), T_OBJECT);\n+  obj_obj_args.compute_calling_conventions();\n+  _obj_obj_arg_handler = lookup(obj_obj_args.sig_cc(), obj_obj_args.has_inline_recv());\n@@ -3080,0 +3547,3 @@\n+  if (_sig_cc != nullptr) {\n+    delete _sig_cc;\n+  }\n@@ -3167,0 +3637,1 @@\n+      BasicType stack_sig_bt[16];\n@@ -3168,0 +3639,1 @@\n+      BasicType* sig_bt = (total_args_passed <= 16) ? stack_sig_bt : NEW_RESOURCE_ARRAY(BasicType, total_args_passed);\n@@ -3170,5 +3642,13 @@\n-      AdapterSignatureIterator si(method->signature(), method->constMethod()->fingerprint(),\n-                              method->is_static(), total_args_passed);\n-      BasicType* sig_bt = si.basic_types();\n-      assert(si.slots() == total_args_passed, \"\");\n-      BasicType ret_type = si.return_type();\n+      int i = 0;\n+      if (!method->is_static()) {  \/\/ Pass in receiver first\n+        sig_bt[i++] = T_OBJECT;\n+      }\n+      SignatureStream ss(method->signature());\n+      for (; !ss.at_return_type(); ss.next()) {\n+        sig_bt[i++] = ss.type();  \/\/ Collect remaining bits of signature\n+        if (ss.type() == T_LONG || ss.type() == T_DOUBLE) {\n+          sig_bt[i++] = T_VOID;   \/\/ Longs & doubles take 2 Java slots\n+        }\n+      }\n+      assert(i == total_args_passed, \"\");\n+      BasicType ret_type = ss.type();\n@@ -3422,1 +3902,4 @@\n-    st->print(\" c2iUV: \" INTPTR_FORMAT, p2i(get_c2i_unverified_entry()));\n+    st->print(\" c2iVE: \" INTPTR_FORMAT, p2i(get_c2i_inline_entry()));\n+    st->print(\" c2iVROE: \" INTPTR_FORMAT, p2i(get_c2i_inline_ro_entry()));\n+    st->print(\" c2iUE: \" INTPTR_FORMAT, p2i(get_c2i_unverified_entry()));\n+    st->print(\" c2iUVE: \" INTPTR_FORMAT, p2i(get_c2i_unverified_inline_entry()));\n@@ -3509,0 +3992,195 @@\n+\n+\/\/ We are at a compiled code to interpreter call. We need backing\n+\/\/ buffers for all inline type arguments. Allocate an object array to\n+\/\/ hold them (convenient because once we're done with it we don't have\n+\/\/ to worry about freeing it).\n+oop SharedRuntime::allocate_inline_types_impl(JavaThread* current, methodHandle callee, bool allocate_receiver, TRAPS) {\n+  assert(InlineTypePassFieldsAsArgs, \"no reason to call this\");\n+  ResourceMark rm;\n+\n+  int nb_slots = 0;\n+  InstanceKlass* holder = callee->method_holder();\n+  allocate_receiver &= !callee->is_static() && holder->is_inline_klass() && callee->is_scalarized_arg(0);\n+  if (allocate_receiver) {\n+    nb_slots++;\n+  }\n+  int arg_num = callee->is_static() ? 0 : 1;\n+  for (SignatureStream ss(callee->signature()); !ss.at_return_type(); ss.next()) {\n+    BasicType bt = ss.type();\n+    if (bt == T_OBJECT && callee->is_scalarized_arg(arg_num)) {\n+      nb_slots++;\n+    }\n+    if (bt != T_VOID) {\n+      arg_num++;\n+    }\n+  }\n+  objArrayOop array_oop = oopFactory::new_objectArray(nb_slots, CHECK_NULL);\n+  objArrayHandle array(THREAD, array_oop);\n+  arg_num = callee->is_static() ? 0 : 1;\n+  int i = 0;\n+  if (allocate_receiver) {\n+    InlineKlass* vk = InlineKlass::cast(holder);\n+    oop res = vk->allocate_instance(CHECK_NULL);\n+    array->obj_at_put(i++, res);\n+  }\n+  for (SignatureStream ss(callee->signature()); !ss.at_return_type(); ss.next()) {\n+    BasicType bt = ss.type();\n+    if (bt == T_OBJECT && callee->is_scalarized_arg(arg_num)) {\n+      InlineKlass* vk = ss.as_inline_klass(holder);\n+      assert(vk != nullptr, \"Unexpected klass\");\n+      oop res = vk->allocate_instance(CHECK_NULL);\n+      array->obj_at_put(i++, res);\n+    }\n+    if (bt != T_VOID) {\n+      arg_num++;\n+    }\n+  }\n+  return array();\n+}\n+\n+JRT_ENTRY(void, SharedRuntime::allocate_inline_types(JavaThread* current, Method* callee_method, bool allocate_receiver))\n+  methodHandle callee(current, callee_method);\n+  oop array = SharedRuntime::allocate_inline_types_impl(current, callee, allocate_receiver, CHECK);\n+  current->set_vm_result_oop(array);\n+  current->set_vm_result_metadata(callee()); \/\/ TODO: required to keep callee live?\n+JRT_END\n+\n+\/\/ We're returning from an interpreted method: load each field into a\n+\/\/ register following the calling convention\n+JRT_LEAF(void, SharedRuntime::load_inline_type_fields_in_regs(JavaThread* current, oopDesc* res))\n+{\n+  assert(res->klass()->is_inline_klass(), \"only inline types here\");\n+  ResourceMark rm;\n+  RegisterMap reg_map(current,\n+                      RegisterMap::UpdateMap::include,\n+                      RegisterMap::ProcessFrames::include,\n+                      RegisterMap::WalkContinuation::skip);\n+  frame stubFrame = current->last_frame();\n+  frame callerFrame = stubFrame.sender(&reg_map);\n+  assert(callerFrame.is_interpreted_frame(), \"should be coming from interpreter\");\n+\n+  InlineKlass* vk = InlineKlass::cast(res->klass());\n+\n+  const Array<SigEntry>* sig_vk = vk->extended_sig();\n+  const Array<VMRegPair>* regs = vk->return_regs();\n+\n+  if (regs == nullptr) {\n+    \/\/ The fields of the inline klass don't fit in registers, bail out\n+    return;\n+  }\n+\n+  int j = 1;\n+  for (int i = 0; i < sig_vk->length(); i++) {\n+    BasicType bt = sig_vk->at(i)._bt;\n+    if (bt == T_METADATA) {\n+      continue;\n+    }\n+    if (bt == T_VOID) {\n+      if (sig_vk->at(i-1)._bt == T_LONG ||\n+          sig_vk->at(i-1)._bt == T_DOUBLE) {\n+        j++;\n+      }\n+      continue;\n+    }\n+    int off = sig_vk->at(i)._offset;\n+    assert(off > 0, \"offset in object should be positive\");\n+    VMRegPair pair = regs->at(j);\n+    address loc = reg_map.location(pair.first(), nullptr);\n+    switch(bt) {\n+    case T_BOOLEAN:\n+      *(jboolean*)loc = res->bool_field(off);\n+      break;\n+    case T_CHAR:\n+      *(jchar*)loc = res->char_field(off);\n+      break;\n+    case T_BYTE:\n+      *(jbyte*)loc = res->byte_field(off);\n+      break;\n+    case T_SHORT:\n+      *(jshort*)loc = res->short_field(off);\n+      break;\n+    case T_INT: {\n+      *(jint*)loc = res->int_field(off);\n+      break;\n+    }\n+    case T_LONG:\n+#ifdef _LP64\n+      *(intptr_t*)loc = res->long_field(off);\n+#else\n+      Unimplemented();\n+#endif\n+      break;\n+    case T_OBJECT:\n+    case T_ARRAY: {\n+      *(oop*)loc = res->obj_field(off);\n+      break;\n+    }\n+    case T_FLOAT:\n+      *(jfloat*)loc = res->float_field(off);\n+      break;\n+    case T_DOUBLE:\n+      *(jdouble*)loc = res->double_field(off);\n+      break;\n+    default:\n+      ShouldNotReachHere();\n+    }\n+    j++;\n+  }\n+  assert(j == regs->length(), \"missed a field?\");\n+\n+#ifdef ASSERT\n+  VMRegPair pair = regs->at(0);\n+  address loc = reg_map.location(pair.first(), nullptr);\n+  assert(*(oopDesc**)loc == res, \"overwritten object\");\n+#endif\n+\n+  current->set_vm_result_oop(res);\n+}\n+JRT_END\n+\n+\/\/ We've returned to an interpreted method, the interpreter needs a\n+\/\/ reference to an inline type instance. Allocate it and initialize it\n+\/\/ from field's values in registers.\n+JRT_BLOCK_ENTRY(void, SharedRuntime::store_inline_type_fields_to_buf(JavaThread* current, intptr_t res))\n+{\n+  ResourceMark rm;\n+  RegisterMap reg_map(current,\n+                      RegisterMap::UpdateMap::include,\n+                      RegisterMap::ProcessFrames::include,\n+                      RegisterMap::WalkContinuation::skip);\n+  frame stubFrame = current->last_frame();\n+  frame callerFrame = stubFrame.sender(&reg_map);\n+\n+#ifdef ASSERT\n+  InlineKlass* verif_vk = InlineKlass::returned_inline_klass(reg_map);\n+#endif\n+\n+  if (!is_set_nth_bit(res, 0)) {\n+    \/\/ We're not returning with inline type fields in registers (the\n+    \/\/ calling convention didn't allow it for this inline klass)\n+    assert(!Metaspace::contains((void*)res), \"should be oop or pointer in buffer area\");\n+    current->set_vm_result_oop((oopDesc*)res);\n+    assert(verif_vk == nullptr, \"broken calling convention\");\n+    return;\n+  }\n+\n+  clear_nth_bit(res, 0);\n+  InlineKlass* vk = (InlineKlass*)res;\n+  assert(verif_vk == vk, \"broken calling convention\");\n+  assert(Metaspace::contains((void*)res), \"should be klass\");\n+\n+  \/\/ Allocate handles for every oop field so they are safe in case of\n+  \/\/ a safepoint when allocating\n+  GrowableArray<Handle> handles;\n+  vk->save_oop_fields(reg_map, handles);\n+\n+  \/\/ It's unsafe to safepoint until we are here\n+  JRT_BLOCK;\n+  {\n+    JavaThread* THREAD = current;\n+    oop vt = vk->realloc_result(reg_map, handles, CHECK);\n+    current->set_vm_result_oop(vt);\n+  }\n+  JRT_BLOCK_END;\n+}\n+JRT_END\n","filename":"src\/hotspot\/share\/runtime\/sharedRuntime.cpp","additions":935,"deletions":257,"binary":false,"changes":1192,"status":"modified"},{"patch":"@@ -28,0 +28,1 @@\n+#include \"asm\/codeBuffer.hpp\"\n@@ -36,0 +37,1 @@\n+#include \"runtime\/signature.hpp\"\n@@ -42,0 +44,1 @@\n+class SigEntry;\n@@ -371,0 +374,2 @@\n+  static char* generate_identity_exception_message(JavaThread* thr, Klass* klass);\n+\n@@ -373,1 +378,1 @@\n-  static methodHandle resolve_helper(bool is_virtual, bool is_optimized, TRAPS);\n+  static methodHandle resolve_helper(bool is_virtual, bool is_optimized, bool& caller_does_not_scalarize, TRAPS);\n@@ -383,1 +388,1 @@\n-  static methodHandle reresolve_call_site(TRAPS);\n+  static methodHandle reresolve_call_site(bool& is_optimized, bool& caller_does_not_scalarize, TRAPS);\n@@ -387,1 +392,1 @@\n-  static methodHandle handle_ic_miss_helper(TRAPS);\n+  static methodHandle handle_ic_miss_helper(bool& caller_does_not_scalarize, TRAPS);\n@@ -390,1 +395,1 @@\n-  static methodHandle find_callee_method(TRAPS);\n+  static methodHandle find_callee_method(bool& caller_does_not_scalarize, TRAPS);\n@@ -418,0 +423,8 @@\n+  static int java_calling_convention(const GrowableArray<SigEntry>* sig, VMRegPair* regs) {\n+    BasicType* sig_bt = NEW_RESOURCE_ARRAY(BasicType, sig->length());\n+    int total_args_passed = SigEntry::fill_sig_bt(sig, sig_bt);\n+    return java_calling_convention(sig_bt, regs, total_args_passed);\n+  }\n+  static int java_return_convention(const BasicType* sig_bt, VMRegPair* regs, int total_args_passed);\n+  static const uint java_return_convention_max_int;\n+  static const uint java_return_convention_max_float;\n@@ -464,1 +477,1 @@\n-  static void generate_i2c2i_adapters(MacroAssembler *_masm,\n+  static void generate_i2c2i_adapters(MacroAssembler* _masm,\n@@ -466,4 +479,9 @@\n-                                      int max_arg,\n-                                      const BasicType *sig_bt,\n-                                      const VMRegPair *regs,\n-                                      address entry_address[AdapterBlob::ENTRY_COUNT]);\n+                                      const GrowableArray<SigEntry>* sig,\n+                                      const VMRegPair* regs,\n+                                      const GrowableArray<SigEntry>* sig_cc,\n+                                      const VMRegPair* regs_cc,\n+                                      const GrowableArray<SigEntry>* sig_cc_ro,\n+                                      const VMRegPair* regs_cc_ro,\n+                                      address entry_address[AdapterBlob::ENTRY_COUNT],\n+                                      AdapterBlob*& new_adapter,\n+                                      bool allocate_code_blob);\n@@ -472,2 +490,1 @@\n-                              int total_args_passed,\n-                              const BasicType *sig_bt,\n+                              const GrowableArray<SigEntry>* sig,\n@@ -546,1 +563,2 @@\n-  static address get_resolved_entry        (JavaThread* current, methodHandle callee_method);\n+  static address get_resolved_entry        (JavaThread* current, methodHandle callee_method,\n+                                            bool is_static_call, bool is_optimized, bool caller_does_not_scalarize);\n@@ -551,0 +569,3 @@\n+  static void load_inline_type_fields_in_regs(JavaThread* current, oopDesc* res);\n+  static void store_inline_type_fields_to_buf(JavaThread* current, intptr_t res);\n+\n@@ -561,0 +582,2 @@\n+  static void allocate_inline_types(JavaThread* current, Method* callee, bool allocate_receiver);\n+  static oop allocate_inline_types_impl(JavaThread* current, methodHandle callee, bool allocate_receiver, TRAPS);\n@@ -564,0 +587,1 @@\n+  static BufferedInlineTypeBlob* generate_buffered_inline_type_adapter(const InlineKlass* vk);\n@@ -702,1 +726,1 @@\n-  static const int ENTRIES_COUNT = 4;\n+  static const int ENTRIES_COUNT = 7;\n@@ -712,0 +736,3 @@\n+  \/\/ Support for scalarized inline type calling convention\n+  const GrowableArray<SigEntry>* _sig_cc;\n+\n@@ -723,1 +750,2 @@\n-    _linked(false)\n+    _linked(false),\n+    _sig_cc(nullptr)\n@@ -774,0 +802,18 @@\n+  address get_c2i_inline_entry() const {\n+#ifndef ZERO\n+    assert(_adapter_blob != nullptr, \"must be\");\n+    return _adapter_blob->c2i_inline_entry();\n+#else\n+    return nullptr;\n+#endif \/\/ ZERO\n+  }\n+\n+  address get_c2i_inline_ro_entry() const {\n+#ifndef ZERO\n+    assert(_adapter_blob != nullptr, \"must be\");\n+    return _adapter_blob->c2i_inline_ro_entry();\n+#else\n+    return nullptr;\n+#endif \/\/ ZERO\n+  }\n+\n@@ -783,0 +829,9 @@\n+  address get_c2i_unverified_inline_entry() const {\n+#ifndef ZERO\n+    assert(_adapter_blob != nullptr, \"must be\");\n+    return _adapter_blob->c2i_unverified_inline_entry();\n+#else\n+    return nullptr;\n+#endif \/\/ ZERO\n+  }\n+\n@@ -795,0 +850,4 @@\n+  \/\/ Support for scalarized inline type calling convention\n+  void set_sig_cc(const GrowableArray<SigEntry>* sig)  { _sig_cc = sig; }\n+  const GrowableArray<SigEntry>* get_sig_cc()    const { return _sig_cc; }\n+\n@@ -819,0 +878,2 @@\n+class CompiledEntrySignature;\n+\n@@ -837,2 +898,2 @@\n-  static AdapterHandlerEntry* create_adapter(int total_args_passed,\n-                                             BasicType* sig_bt,\n+  static AdapterHandlerEntry* create_adapter(CompiledEntrySignature& ces,\n+                                             bool allocate_code_blob,\n@@ -849,1 +910,1 @@\n-  static AdapterHandlerEntry* lookup(int total_args_passed, BasicType* sig_bt);\n+  static AdapterHandlerEntry* lookup(const GrowableArray<SigEntry>* sig, bool has_ro_adapter = false);\n@@ -851,2 +912,2 @@\n-                                    int total_args_passed,\n-                                    BasicType* sig_bt,\n+                                    CompiledEntrySignature& ces,\n+                                    bool allocate_code_blob,\n@@ -856,1 +917,1 @@\n-  static void verify_adapter_sharing(int total_args_passed, BasicType* sig_bt, AdapterHandlerEntry* cached);\n+  static void verify_adapter_sharing(CompiledEntrySignature& ces, AdapterHandlerEntry* cached_entry);\n@@ -874,0 +935,60 @@\n+\/\/ Utility class for computing the calling convention of the 3 types\n+\/\/ of compiled method entries:\n+\/\/     Method::_from_compiled_entry               - sig_cc\n+\/\/     Method::_from_compiled_inline_ro_entry     - sig_cc_ro\n+\/\/     Method::_from_compiled_inline_entry        - sig\n+class CompiledEntrySignature : public StackObj {\n+  Method* _method;\n+  int  _num_inline_args;\n+  bool _has_inline_recv;\n+  GrowableArray<SigEntry>* _sig;\n+  GrowableArray<SigEntry>* _sig_cc;\n+  GrowableArray<SigEntry>* _sig_cc_ro;\n+  VMRegPair* _regs;\n+  VMRegPair* _regs_cc;\n+  VMRegPair* _regs_cc_ro;\n+\n+  int _args_on_stack;\n+  int _args_on_stack_cc;\n+  int _args_on_stack_cc_ro;\n+\n+  bool _c1_needs_stack_repair;\n+  bool _c2_needs_stack_repair;\n+\n+  GrowableArray<Method*>* _supers;\n+\n+public:\n+  Method* method()                     const { return _method; }\n+\n+  \/\/ Used by Method::_from_compiled_inline_entry\n+  GrowableArray<SigEntry>* sig()       const { return _sig; }\n+\n+  \/\/ Used by Method::_from_compiled_entry\n+  GrowableArray<SigEntry>* sig_cc()    const { return _sig_cc; }\n+\n+  \/\/ Used by Method::_from_compiled_inline_ro_entry\n+  GrowableArray<SigEntry>* sig_cc_ro() const { return _sig_cc_ro; }\n+\n+  VMRegPair* regs()                    const { return _regs; }\n+  VMRegPair* regs_cc()                 const { return _regs_cc; }\n+  VMRegPair* regs_cc_ro()              const { return _regs_cc_ro; }\n+\n+  int args_on_stack()                  const { return _args_on_stack; }\n+  int args_on_stack_cc()               const { return _args_on_stack_cc; }\n+  int args_on_stack_cc_ro()            const { return _args_on_stack_cc_ro; }\n+\n+  int  num_inline_args()               const { return _num_inline_args; }\n+  bool has_inline_recv()               const { return _has_inline_recv; }\n+\n+  bool has_scalarized_args()           const { return _sig != _sig_cc; }\n+  bool c1_needs_stack_repair()         const { return _c1_needs_stack_repair; }\n+  bool c2_needs_stack_repair()         const { return _c2_needs_stack_repair; }\n+  CodeOffsets::Entries c1_inline_ro_entry_type() const;\n+\n+  GrowableArray<Method*>* get_supers();\n+\n+  CompiledEntrySignature(Method* method = nullptr);\n+  void compute_calling_conventions(bool init = true);\n+  void initialize_from_fingerprint(AdapterFingerPrint* fingerprint);\n+};\n+\n","filename":"src\/hotspot\/share\/runtime\/sharedRuntime.hpp","additions":141,"deletions":20,"binary":false,"changes":161,"status":"modified"},{"patch":"@@ -138,0 +138,1 @@\n+  do_blob(new_null_free_array)                                         \\\n@@ -139,0 +140,5 @@\n+  do_blob(load_flat_array)                                             \\\n+  do_blob(store_flat_array)                                            \\\n+  do_blob(substitutability_check)                                      \\\n+  do_blob(buffer_inline_args)                                          \\\n+  do_blob(buffer_inline_args_no_receiver)                              \\\n@@ -145,0 +151,2 @@\n+  do_blob(throw_illegal_monitor_state_exception)                       \\\n+  do_blob(throw_identity_exception)                                    \\\n@@ -233,0 +241,2 @@\n+  do_stub(load_unknown_inline, 0, true, false)                         \\\n+  do_stub(store_unknown_inline, 0, true, false)                        \\\n","filename":"src\/hotspot\/share\/runtime\/stubDeclarations.hpp","additions":10,"deletions":0,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -28,0 +28,2 @@\n+import java.lang.classfile.ClassFile;\n+\n@@ -386,1 +388,2 @@\n-    ; \/\/ Reduce code churn when appending new constants\n+\n+    \/\/ Reduce code churn when appending new constants\n@@ -391,0 +394,4 @@\n+    \/\/\/ The preview features of Valhalla.\n+    \/\/\/ @since 26\n+    CURRENT_PREVIEW_FEATURES(ClassFile.latestMajorVersion());\n+\n","filename":"src\/java.base\/share\/classes\/java\/lang\/reflect\/ClassFileFormatVersion.java","additions":8,"deletions":1,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -231,0 +231,4 @@\n+javac.opt.Xlint.desc.initialization=\\\n+    Warn about code in identity classes that wouldn''t be allowed in early\\n\\\n+\\                         construction due to a this dependency.\n+\n@@ -237,0 +241,3 @@\n+javac.opt.Xlint.desc.migration=\\\n+    Warn about issues related to migration of JDK classes.\n+\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/resources\/javac.properties","additions":7,"deletions":0,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2015, 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2015, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -376,15 +376,16 @@\n-        ImageProvider imageProvider =\n-                createImageProvider(config,\n-                                    null,\n-                                    IGNORE_SIGNING_DEFAULT,\n-                                    false,\n-                                    null,\n-                                    false,\n-                                    new OptionsValues(),\n-                                    null);\n-\n-        \/\/ Then create the Plugin Stack\n-        ImagePluginStack stack = ImagePluginConfiguration.parseConfiguration(plugins);\n-\n-        \/\/Ask the stack to proceed;\n-        stack.operate(imageProvider);\n+        try (ImageHelper imageProvider =\n+                     createImageProvider(config,\n+                             null,\n+                             IGNORE_SIGNING_DEFAULT,\n+                             false,\n+                             null,\n+                             false,\n+                             new OptionsValues(),\n+                             null)) {\n+\n+            \/\/ Then create the Plugin Stack\n+            ImagePluginStack stack = ImagePluginConfiguration.parseConfiguration(plugins);\n+\n+            \/\/ Ask the stack to proceed;\n+            stack.operate(imageProvider);\n+        }\n@@ -514,16 +515,18 @@\n-        ImageHelper imageProvider = createImageProvider(config,\n-                                                        options.packagedModulesPath,\n-                                                        options.ignoreSigning,\n-                                                        options.bindServices,\n-                                                        options.endian,\n-                                                        options.verbose,\n-                                                        options,\n-                                                        log);\n-\n-        \/\/ Then create the Plugin Stack\n-        ImagePluginStack stack = ImagePluginConfiguration.parseConfiguration(\n-            taskHelper.getPluginsConfig(options.output, options.launchers,\n-                    imageProvider.targetPlatform));\n-\n-        \/\/Ask the stack to proceed\n-        stack.operate(imageProvider);\n+        try (ImageHelper imageProvider = createImageProvider(config,\n+                options.packagedModulesPath,\n+                options.ignoreSigning,\n+                options.bindServices,\n+                options.endian,\n+                options.verbose,\n+                options,\n+                log)) {\n+            \/\/ Then create the Plugin Stack\n+            ImagePluginStack stack = ImagePluginConfiguration.parseConfiguration(\n+                    taskHelper.getPluginsConfig(\n+                            options.output,\n+                            options.launchers,\n+                            imageProvider.targetPlatform));\n+\n+            \/\/Ask the stack to proceed\n+            stack.operate(imageProvider);\n+        }\n@@ -1057,4 +1060,5 @@\n-    private static record ImageHelper(Set<Archive> archives,\n-                                      Platform targetPlatform,\n-                                      Path packagedModulesPath,\n-                                      boolean generateRuntimeImage) implements ImageProvider {\n+    private record ImageHelper(Set<Archive> archives,\n+                               Platform targetPlatform,\n+                               Path packagedModulesPath,\n+                               boolean generateRuntimeImage)\n+            implements ImageProvider, AutoCloseable {\n@@ -1076,0 +1080,20 @@\n+\n+        @Override\n+        public void close() throws IOException {\n+            List<IOException> thrown = null;\n+            for (Archive archive : archives) {\n+                try {\n+                    archive.close();\n+                } catch (IOException ex) {\n+                    if (thrown == null) {\n+                        thrown = new ArrayList<>();\n+                    }\n+                    thrown.add(ex);\n+                }\n+            }\n+            if (thrown != null) {\n+                IOException ex = new IOException(\"Archives could not be closed\", thrown.getFirst());\n+                thrown.subList(1, thrown.size()).forEach(ex::addSuppressed);\n+                throw ex;\n+            }\n+        }\n","filename":"src\/jdk.jlink\/share\/classes\/jdk\/tools\/jlink\/internal\/JlinkTask.java","additions":60,"deletions":36,"binary":false,"changes":96,"status":"modified"},{"patch":"@@ -79,0 +79,3 @@\n+compiler\/jvmci\/jdk.vm.ci.hotspot.test\/src\/jdk\/vm\/ci\/hotspot\/test\/MemoryAccessProviderTest.java 8350208 generic-all\n+compiler\/jvmci\/jdk.vm.ci.hotspot.test\/src\/jdk\/vm\/ci\/hotspot\/test\/TestHotSpotResolvedJavaField.java 8350208 generic-all\n+\n@@ -81,0 +84,24 @@\n+# Valhalla\n+compiler\/whitebox\/DeoptimizeRelocatedNMethod.java 8370571 generic-all\n+compiler\/whitebox\/RelocateNMethodMultiplePaths.java#C1 8370571 generic-all\n+compiler\/whitebox\/RelocateNMethodMultiplePaths.java#C2 8370571 generic-all\n+compiler\/whitebox\/RelocateNMethodMultiplePaths.java#G1C1 8370571 generic-all\n+compiler\/whitebox\/RelocateNMethodMultiplePaths.java#G1C2 8370571 generic-all\n+compiler\/whitebox\/RelocateNMethodMultiplePaths.java#ParallelC1 8370571 generic-all\n+compiler\/whitebox\/RelocateNMethodMultiplePaths.java#ParallelC2 8370571 generic-all\n+compiler\/whitebox\/RelocateNMethodMultiplePaths.java#SerialC1 8370571 generic-all\n+compiler\/whitebox\/RelocateNMethodMultiplePaths.java#SerialC2 8370571 generic-all\n+compiler\/whitebox\/RelocateNMethodMultiplePaths.java#ZGCC1 8370571 generic-all\n+compiler\/whitebox\/RelocateNMethodMultiplePaths.java#ZGCC2 8370571 generic-all\n+compiler\/whitebox\/StressNMethodRelocation.java 8370571 generic-all\n+compiler\/valhalla\/inlinetypes\/TestC1.java             8372341 generic-all\n+compiler\/valhalla\/inlinetypes\/TestCastMismatch.java   8372341 generic-all\n+compiler\/valhalla\/inlinetypes\/TestGetfieldChains.java 8372341 generic-all\n+compiler\/codegen\/TestRedundantLea.java#GetAndSet          8361089 generic-all\n+compiler\/codegen\/TestRedundantLea.java#StringEquals       8361089 generic-all\n+compiler\/codegen\/TestRedundantLea.java#StringInflate      8361089 generic-all\n+compiler\/codegen\/TestRedundantLea.java#RegexFind          8361089 generic-all\n+compiler\/codegen\/TestRedundantLea.java#StoreNSerial       8361089 generic-all\n+compiler\/codegen\/TestRedundantLea.java#StoreNParallel     8361089 generic-all\n+compiler\/codegen\/TestRedundantLea.java#Spill              8361089 generic-all\n+\n@@ -103,0 +130,1 @@\n+runtime\/cds\/appcds\/redefineClass\/RedefineRunningMethods_Shared.java  8304168 generic-all\n@@ -122,0 +150,16 @@\n+\n+# Valhalla\n+runtime\/valhalla\/inlinetypes\/verifier\/StrictInstanceFieldsTest.java CODETOOLS-7904031 generic-all\n+runtime\/valhalla\/inlinetypes\/verifier\/StrictStaticFieldsTest.java CODETOOLS-7904031 generic-all\n+\n+runtime\/cds\/TestDefaultArchiveLoading.java#coops_nocoh            8366774           generic-all\n+runtime\/cds\/TestDefaultArchiveLoading.java#nocoops_nocoh          8366774           generic-all\n+\n+# Valhalla + AOT\n+runtime\/cds\/appcds\/aotCache\/HelloAOTCache.java                                  8369043 generic-aarch64\n+runtime\/cds\/appcds\/aotCode\/AOTCodeFlags.java                                    8369043 generic-aarch64\n+runtime\/cds\/appcds\/methodHandles\/MethodHandlesGeneralTest.java#aot              8367408 generic-all\n+runtime\/cds\/appcds\/resolvedConstants\/ResolvedConstants.java#aot                 8371456 generic-all\n+runtime\/cds\/appcds\/resolvedConstants\/ResolvedConstants.java#static              8371456 generic-all\n+runtime\/cds\/appcds\/dynamicArchive\/DynamicArchiveRelocationTest.java             8372265 generic-all\n+runtime\/cds\/appcds\/dynamicArchive\/HelloDynamicInlineClass.java                  8372265 generic-all\n@@ -150,0 +194,57 @@\n+\n+# Valhalla TODO:\n+serviceability\/sa\/ClhsdbCDSCore.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbCDSJstackPrintAll.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbFindPC.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbInspect.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbLongConstant.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbJdis.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbJstack.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbPrintAll.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbPrintAs.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbPrintStatics.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbSource.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbSymbol.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbWhere.java 8190936 generic-all\n+serviceability\/sa\/JhsdbThreadInfoTest.java 8190936 generic-all\n+serviceability\/sa\/TestClassDump.java 8190936 generic-all\n+serviceability\/sa\/TestClhsdbJstackLock.java 8190936 generic-all\n+serviceability\/sa\/TestCpoolForInvokeDynamic.java 8190936 generic-all\n+serviceability\/sa\/TestHeapDumpForInvokeDynamic.java 8190936 generic-all\n+serviceability\/sa\/TestHeapDumpForLargeArray.java 8190936 generic-all\n+serviceability\/sa\/TestIntConstant.java 8190936 generic-all\n+serviceability\/sa\/TestJhsdbJstackLock.java 8190936 generic-all\n+serviceability\/sa\/TestJmapCore.java 8190936 generic-all\n+serviceability\/sa\/TestJmapCoreMetaspace.java 8190936 generic-all\n+serviceability\/sa\/TestPrintMdo.java 8190936 generic-all\n+serviceability\/sa\/jmap-hprof\/JMapHProfLargeHeapTest.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbDumpclass.java 8190936 generic-all\n+\n+# Array Changes TODO\n+serviceability\/sa\/CDSJMapClstats.java 8365722 generic-all\n+serviceability\/sa\/ClhsdbClasses.java 8365722 generic-all\n+serviceability\/sa\/sadebugd\/DisableRegistryTest.java 8365722 generic-all\n+serviceability\/sa\/ClhsdbDumpheap.java 8365722 generic-all\n+serviceability\/sa\/sadebugd\/ClhsdbTestConnectArgument.java 8365722 generic-all\n+serviceability\/sa\/sadebugd\/DebugdConnectTest.java 8365722 generic-all\n+serviceability\/sa\/ClhsdbJhisto.java 8365722 generic-all\n+serviceability\/sa\/ClhsdbJstack.java#id1 8365722 generic-all\n+serviceability\/sa\/ClhsdbJstackWithConcurrentLock.java 8365722 generic-all\n+serviceability\/sa\/TestJhsdbJstackWithVirtualThread.java 8365722 generic-all\n+serviceability\/sa\/ClhsdbJstackXcompStress.java 8365722 generic-all\n+serviceability\/sa\/ClhsdbPstack.java#process 8365722 generic-all\n+serviceability\/sa\/ClhsdbPstack.java#core 8365722 generic-all\n+serviceability\/sa\/ClhsdbScanOops.java#id0 8365722 generic-all\n+serviceability\/sa\/ClhsdbScanOops.java#id1 8365722 generic-all\n+serviceability\/sa\/ClhsdbScanOops.java#serial 8365722 generic-all\n+serviceability\/sa\/ClhsdbScanOops.java#parallel 8365722 generic-all\n+serviceability\/sa\/DeadlockDetectionTest.java 8365722 generic-all\n+serviceability\/sa\/ClhsdbJstack.java#id0 8365722 generic-all\n+serviceability\/sa\/TestInstanceKlassSize.java 8365722 generic-all\n+serviceability\/sa\/TestSysProps.java 8365722 generic-all\n+serviceability\/sa\/sadebugd\/ClhsdbAttachToDebugServer.java 8365722 generic-all\n+resourcehogs\/serviceability\/sa\/TestHeapDumpForLargeArray.java 8365722 generic-all\n+\n+resourcehogs\/serviceability\/sa\/ClhsdbRegionDetailsScanOopsForG1.java 8190936 generic-all\n+vmTestbase\/nsk\/jvmti\/scenarios\/events\/EM04\/em04t001\/TestDescription.java 8367590 generic-all\n+\n@@ -189,0 +290,2 @@\n+vmTestbase\/vm\/mlvm\/hiddenloader\/stress\/byteMutation\/Test.java 8317172 generic-all\n+\n","filename":"test\/hotspot\/jtreg\/ProblemList.txt","additions":103,"deletions":0,"binary":false,"changes":103,"status":"modified"},{"patch":"@@ -31,0 +31,1 @@\n+import compiler.valhalla.inlinetypes.InlineTypeIRNode;\n@@ -90,1 +91,1 @@\n-    private static final String PREFIX = \"_#\";\n+    public static final String PREFIX = \"_#\";\n@@ -153,0 +154,6 @@\n+    \/\/ Valhalla: Make sure that all Valhalla specific IR nodes are also properly initialized. Doing it here also\n+    \/\/           ensures that the Flag VM is able to pick up the correct compile phases.\n+    static {\n+        InlineTypeIRNode.forceStaticInitialization();\n+    }\n+\n@@ -384,2 +391,6 @@\n-        String regex = START + \"Allocate\\\\b\" + MID + \"allocationKlass:.*\\\\b\" + IS_REPLACED + \"\\\\s.*\" + END;\n-        macroNodes(ALLOC_OF, regex);\n+        allocateOfNodes(ALLOC_OF, IS_REPLACED);\n+    }\n+\n+    public static void allocateOfNodes(String irNodePlaceholder, String allocatee) {\n+        String regex = START + \"Allocate\\\\b\" + MID + \"allocationKlass:.*\\\\b\" + allocatee + \"\\\\s.*\" + END;\n+        macroNodes(irNodePlaceholder, regex);\n@@ -396,0 +407,4 @@\n+        allocateArrayOfNodes(ALLOC_ARRAY_OF, IS_REPLACED);\n+    }\n+\n+    public static void allocateArrayOfNodes(String irNodePlaceholder, String allocatee) {\n@@ -414,1 +429,1 @@\n-        String name_part = \"\\\\[+.(\" + partial_name_prefix + \")?\" + IS_REPLACED + \";\";\n+        String name_part = \"\\\\[+.(\" + partial_name_prefix + \")?\" + allocatee + \";\";\n@@ -416,1 +431,1 @@\n-        macroNodes(ALLOC_ARRAY_OF, regex);\n+        macroNodes(irNodePlaceholder, regex);\n@@ -481,1 +496,1 @@\n-        callOfNodes(CALL_OF, \"Call.*\");\n+        callOfNodes(CALL_OF, \"Call.*\", IS_REPLACED + \" \" );\n@@ -486,1 +501,6 @@\n-        callOfNodes(CALL_OF_METHOD, \"Call.*Java\");\n+        callOfNodes(CALL_OF_METHOD, \"Call.*Java\", IS_REPLACED + \" \");\n+    }\n+\n+    public static final String STATIC_CALL = PREFIX + \"STATIC_CALL\" + POSTFIX;\n+    static {\n+        beforeMatchingNameRegex(STATIC_CALL, \"CallStaticJava\");\n@@ -491,1 +511,19 @@\n-        callOfNodes(STATIC_CALL_OF_METHOD, \"CallStaticJava\");\n+        staticCallOfMethodNodes(STATIC_CALL_OF_METHOD, IS_REPLACED + \" \");\n+    }\n+\n+    public static void staticCallOfMethodNodes(String irNodePlaceholder, String calleeRegex) {\n+        callOfNodes(irNodePlaceholder, \"CallStaticJava\", calleeRegex);\n+    }\n+\n+    public static final String CALL_LEAF_NO_FP = PREFIX + \"CALL_LEAF_NO_FP\" + POSTFIX;\n+    static {\n+        beforeMatchingNameRegex(CALL_LEAF_NO_FP, \"CallLeafNoFP\");\n+    }\n+\n+    public static final String CALL_LEAF_NO_FP_OF_METHOD = COMPOSITE_PREFIX + \"CALL_LEAF_NO_FP_OF_METHOD\" + POSTFIX;\n+    static {\n+        callLeafNoFpOfMethodNodes(CALL_LEAF_NO_FP_OF_METHOD, IS_REPLACED);\n+    }\n+\n+    public static void callLeafNoFpOfMethodNodes(String irNodePlaceholder, String calleeRegex) {\n+        callOfNodes(irNodePlaceholder, \"CallLeafNoFP\", calleeRegex);\n@@ -588,0 +626,5 @@\n+    public static final String CMP_N = PREFIX + \"CMP_N\" + POSTFIX;\n+    static {\n+        beforeMatchingNameRegex(CMP_N, \"CmpN\");\n+    }\n+\n@@ -742,1 +785,1 @@\n-        callOfNodes(DYNAMIC_CALL_OF_METHOD, \"CallDynamicJava\");\n+        callOfNodes(DYNAMIC_CALL_OF_METHOD, \"CallDynamicJava\", IS_REPLACED);\n@@ -878,0 +921,5 @@\n+    public static final String INLINE_TYPE = PREFIX + \"INLINE_TYPE\" + POSTFIX;\n+    static {\n+        beforeMatchingNameRegex(INLINE_TYPE, \"InlineType\");\n+    }\n+\n@@ -924,1 +972,5 @@\n-        loadOfNodes(LOAD_OF_CLASS, \"Load(B|UB|S|US|I|L|F|D|P|N)\");\n+        anyLoadOfNodes(LOAD_OF_CLASS, IS_REPLACED);\n+    }\n+\n+    public static void anyLoadOfNodes(String irNodePlaceholder, String fieldHolder) {\n+        loadOfNodes(irNodePlaceholder, \"Load(B|UB|S|US|I|L|F|D|P|N)\", fieldHolder);\n@@ -934,1 +986,1 @@\n-        loadOfNodes(LOAD_B_OF_CLASS, \"LoadB\");\n+        loadOfNodes(LOAD_B_OF_CLASS, \"LoadB\", IS_REPLACED);\n@@ -944,1 +996,1 @@\n-        loadOfNodes(LOAD_D_OF_CLASS, \"LoadD\");\n+        loadOfNodes(LOAD_D_OF_CLASS, \"LoadD\", IS_REPLACED);\n@@ -954,1 +1006,1 @@\n-        loadOfNodes(LOAD_F_OF_CLASS, \"LoadF\");\n+        loadOfNodes(LOAD_F_OF_CLASS, \"LoadF\", IS_REPLACED);\n@@ -964,1 +1016,1 @@\n-        loadOfNodes(LOAD_I_OF_CLASS, \"LoadI\");\n+        loadOfNodes(LOAD_I_OF_CLASS, \"LoadI\", IS_REPLACED);\n@@ -989,1 +1041,1 @@\n-        loadOfNodes(LOAD_L_OF_CLASS, \"LoadL\");\n+        loadOfNodes(LOAD_L_OF_CLASS, \"LoadL\", IS_REPLACED);\n@@ -999,1 +1051,1 @@\n-        loadOfNodes(LOAD_N_OF_CLASS, \"LoadN\");\n+        loadOfNodes(LOAD_N_OF_CLASS, \"LoadN\", IS_REPLACED);\n@@ -1015,1 +1067,1 @@\n-        loadOfNodes(LOAD_P_OF_CLASS, \"LoadP\");\n+        loadOfNodes(LOAD_P_OF_CLASS, \"LoadP\", IS_REPLACED);\n@@ -1025,1 +1077,1 @@\n-        loadOfNodes(LOAD_S_OF_CLASS, \"LoadS\");\n+        loadOfNodes(LOAD_S_OF_CLASS, \"LoadS\", IS_REPLACED);\n@@ -1035,1 +1087,1 @@\n-        loadOfNodes(LOAD_UB_OF_CLASS, \"LoadUB\");\n+        loadOfNodes(LOAD_UB_OF_CLASS, \"LoadUB\", IS_REPLACED);\n@@ -1045,1 +1097,1 @@\n-        loadOfNodes(LOAD_US_OF_CLASS, \"LoadUS\");\n+        loadOfNodes(LOAD_US_OF_CLASS, \"LoadUS\", IS_REPLACED);\n@@ -1965,1 +2017,1 @@\n-        storeOfNodes(STORE_B_OF_CLASS, \"StoreB\");\n+        storeOfNodes(STORE_B_OF_CLASS, \"StoreB\", IS_REPLACED);\n@@ -1975,1 +2027,1 @@\n-        storeOfNodes(STORE_C_OF_CLASS, \"StoreC\");\n+        storeOfNodes(STORE_C_OF_CLASS, \"StoreC\", IS_REPLACED);\n@@ -1985,1 +2037,1 @@\n-        storeOfNodes(STORE_D_OF_CLASS, \"StoreD\");\n+        storeOfNodes(STORE_D_OF_CLASS, \"StoreD\", IS_REPLACED);\n@@ -1995,1 +2047,1 @@\n-        storeOfNodes(STORE_F_OF_CLASS, \"StoreF\");\n+        storeOfNodes(STORE_F_OF_CLASS, \"StoreF\", IS_REPLACED);\n@@ -2005,1 +2057,1 @@\n-        storeOfNodes(STORE_I_OF_CLASS, \"StoreI\");\n+        storeOfNodes(STORE_I_OF_CLASS, \"StoreI\", IS_REPLACED);\n@@ -2015,1 +2067,1 @@\n-        storeOfNodes(STORE_L_OF_CLASS, \"StoreL\");\n+        storeOfNodes(STORE_L_OF_CLASS, \"StoreL\", IS_REPLACED);\n@@ -2025,1 +2077,1 @@\n-        storeOfNodes(STORE_N_OF_CLASS, \"StoreN\");\n+        storeOfNodes(STORE_N_OF_CLASS, \"StoreN\", IS_REPLACED);\n@@ -2030,1 +2082,5 @@\n-        storeOfNodes(STORE_OF_CLASS, \"Store(B|C|S|I|L|F|D|P|N)\");\n+        anyStoreOfNodes(STORE_OF_CLASS, IS_REPLACED);\n+    }\n+\n+    public static void anyStoreOfNodes(String irNodePlaceholder, String fieldHolder) {\n+        storeOfNodes(irNodePlaceholder, \"Store(B|C|S|I|L|F|D|P|N)\", fieldHolder);\n@@ -2046,1 +2102,1 @@\n-        storeOfNodes(STORE_P_OF_CLASS, \"StoreP\");\n+        storeOfNodes(STORE_P_OF_CLASS, \"StoreP\", IS_REPLACED);\n@@ -2146,1 +2202,2 @@\n-        beforeMatchingNameRegex(SUBTYPE_CHECK, \"SubTypeCheck\");\n+        String regex = START + \"SubTypeCheck\" + MID + END;\n+        macroNodes(SUBTYPE_CHECK, regex);\n@@ -3098,1 +3155,1 @@\n-    private static void beforeMatching(String irNodePlaceholder, String regex) {\n+    public static void beforeMatching(String irNodePlaceholder, String regex) {\n@@ -3134,2 +3191,2 @@\n-    private static void callOfNodes(String irNodePlaceholder, String callRegex) {\n-        String regex = START + callRegex + MID + IS_REPLACED + \" \" +  END;\n+    private static void callOfNodes(String irNodePlaceholder, String callRegex, String calleeRegex) {\n+        String regex = START + callRegex + MID + calleeRegex + END;\n@@ -3143,1 +3200,1 @@\n-    private static void optoOnly(String irNodePlaceholder, String regex) {\n+    public static void optoOnly(String irNodePlaceholder, String regex) {\n@@ -3237,2 +3294,2 @@\n-    private static void loadOfNodes(String irNodePlaceholder, String irNodeRegex) {\n-        String regex = START + irNodeRegex + MID + LOAD_STORE_PREFIX + IS_REPLACED + LOAD_STORE_SUFFIX + END;\n+    private static void loadOfNodes(String irNodePlaceholder, String irNodeRegex, String loadee) {\n+        String regex = START + irNodeRegex + MID + LOAD_STORE_PREFIX + loadee + LOAD_STORE_SUFFIX + END;\n@@ -3242,2 +3299,2 @@\n-    private static void storeOfNodes(String irNodePlaceholder, String irNodeRegex) {\n-        String regex = START + irNodeRegex + MID + LOAD_STORE_PREFIX + IS_REPLACED + LOAD_STORE_SUFFIX + END;\n+    private static void storeOfNodes(String irNodePlaceholder, String irNodeRegex, String storee) {\n+        String regex = START + irNodeRegex + MID + LOAD_STORE_PREFIX + storee + LOAD_STORE_SUFFIX + END;\n","filename":"test\/hotspot\/jtreg\/compiler\/lib\/ir_framework\/IRNode.java","additions":95,"deletions":38,"binary":false,"changes":133,"status":"modified"},{"patch":"@@ -777,2 +777,11 @@\n-    @IR(counts = {IRNode.STORE_OF_FIELD, \"myClassEmpty\", \"1\", IRNode.STORE_OF_CLASS, \"GoodCount\", \"1\",\n-                  IRNode.STORE_OF_CLASS, \"\/GoodCount\", \"1\", IRNode.STORE_OF_CLASS, \"MyClassEmpty\", \"0\"},\n+    @IR(counts = {\n+            IRNode.STORE_OF_FIELD, \"myClassEmpty\", \"1\",\n+            IRNode.STORE_OF_CLASS, \"oodCount\", \"0\",\n+            IRNode.STORE_OF_CLASS, \"GoodCount\", \"1\",\n+            IRNode.STORE_OF_CLASS, \"\/GoodCount\", \"1\",\n+            IRNode.STORE_OF_CLASS, \"tests\/GoodCount\", \"1\",\n+            IRNode.STORE_OF_CLASS, \"\/tests\/GoodCount\", \"1\",\n+            IRNode.STORE_OF_CLASS, \"ir_framework\/tests\/GoodCount\", \"1\",\n+            IRNode.STORE_OF_CLASS, \"\/ir_framework\/tests\/GoodCount\", \"0\",\n+            IRNode.STORE_OF_CLASS, \"MyClassEmpty\", \"0\"\n+        },\n","filename":"test\/hotspot\/jtreg\/testlibrary_tests\/ir_framework\/tests\/TestIRMatching.java","additions":11,"deletions":2,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -532,0 +532,5 @@\n+java\/lang\/ModuleLayer\/LayerControllerTest.java                  8337048 generic-all\n+java\/lang\/ModuleLayer\/BasicLayerTest.java                       8337048 generic-all\n+\n+java\/lang\/Thread\/virtual\/stress\/Skynet.java#default             8342977 generic-all\n+\n@@ -549,0 +554,1 @@\n+com\/sun\/management\/HotSpotDiagnosticMXBean\/DumpThreadsWithEliminatedLock.java 8360599 generic-all\n@@ -570,0 +576,2 @@\n+java\/net\/CookieHandler\/B6644726.java                            8365811 generic-all\n+\n@@ -720,0 +728,4 @@\n+com\/sun\/jdi\/cds\/CDSBreakpointTest.java                          8304168 generic-all\n+com\/sun\/jdi\/cds\/CDSDeleteAllBkptsTest.java                      8304168 generic-all\n+com\/sun\/jdi\/cds\/CDSFieldWatchpoints.java                        8304168 generic-all\n+\n@@ -815,0 +827,1 @@\n+\n@@ -820,0 +833,16 @@\n+\n+############################################################################\n+\n+# valhalla\n+java\/lang\/invoke\/VarHandles\/VarHandleTestMethodHandleAccessValue.java 8367346 generic-all\n+\n+jdk\/classfile\/AccessFlagsTest.java 8366270 generic-all\n+jdk\/jfr\/event\/runtime\/TestClassLoaderStatsEvent.java 8366820 generic-all\n+\n+jdk\/internal\/vm\/Continuation\/Fuzz.java#default 8370177 generic-aarch64\n+jdk\/internal\/vm\/Continuation\/Fuzz.java#preserve-fp 8370177 generic-aarch64\n+\n+sun\/tools\/jhsdb\/BasicLauncherTest.java 8366806 generic-all\n+sun\/tools\/jhsdb\/HeapDumpTest.java 8366806 generic-all\n+sun\/tools\/jhsdb\/HeapDumpTestWithActiveProcess.java 8366806 generic-all\n+sun\/tools\/jhsdb\/JShellHeapDumpTest.java 8366806 generic-all\n","filename":"test\/jdk\/ProblemList.txt","additions":29,"deletions":0,"binary":false,"changes":29,"status":"modified"},{"patch":"@@ -36,0 +36,1 @@\n+ * @ignore Verifier error\n","filename":"test\/langtools\/tools\/javac\/launcher\/SourceLauncherTest.java","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -161,0 +161,14 @@\n+  private native Object[] getObjectsViaKlassOopMaps0(Object thing);\n+  public Object[] getObjectsViaKlassOopMaps(Object thing) {\n+    Objects.requireNonNull(thing);\n+    return getObjectsViaKlassOopMaps0(thing);\n+  }\n+\n+  private native Object[] getObjectsViaOopIterator0(Object thing);\n+  public Object[] getObjectsViaOopIterator(Object thing) {\n+    Objects.requireNonNull(thing);\n+    return getObjectsViaOopIterator0(thing);\n+  }\n+\n+  public native Object[] getObjectsViaFrameOopIterator(int depth);\n+\n@@ -739,4 +753,7 @@\n-  private final List<Function<String,Object>> flagsGetters = Arrays.asList(\n-    this::getBooleanVMFlag, this::getIntVMFlag, this::getUintVMFlag,\n-    this::getIntxVMFlag, this::getUintxVMFlag, this::getUint64VMFlag,\n-    this::getSizeTVMFlag, this::getStringVMFlag, this::getDoubleVMFlag);\n+  private final List<Function<String,Object>> flagsGetters;\n+  {\n+      flagsGetters = Arrays.asList(\n+          this::getBooleanVMFlag, this::getIntVMFlag, this::getUintVMFlag,\n+          this::getIntxVMFlag, this::getUintxVMFlag, this::getUint64VMFlag,\n+          this::getSizeTVMFlag, this::getStringVMFlag, this::getDoubleVMFlag);\n+  }\n@@ -781,0 +798,1 @@\n+  @SuppressWarnings(\"initialization\")\n","filename":"test\/lib\/jdk\/test\/whitebox\/WhiteBox.java","additions":22,"deletions":4,"binary":false,"changes":26,"status":"modified"}]}