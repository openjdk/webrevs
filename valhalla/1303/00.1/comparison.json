{"files":[{"patch":"@@ -1657,0 +1657,3 @@\n+  } else if (_entry_point == nullptr) {\n+    \/\/ See CallLeafNoFPIndirect\n+    return 1 * NativeInstruction::instruction_size;\n@@ -1765,3 +1768,0 @@\n-  \/\/ n.b. frame size includes space for return pc and rfp\n-  const int framesize = C->output()->frame_size_in_bytes();\n-\n@@ -1772,10 +1772,1 @@\n-  if (C->clinit_barrier_on_entry()) {\n-    assert(!C->method()->holder()->is_not_initialized(), \"initialization should have been started\");\n-\n-    Label L_skip_barrier;\n-\n-    __ mov_metadata(rscratch2, C->method()->holder()->constant_encoding());\n-    __ clinit_barrier(rscratch2, rscratch1, &L_skip_barrier);\n-    __ far_jump(RuntimeAddress(SharedRuntime::get_handle_wrong_method_stub()));\n-    __ bind(L_skip_barrier);\n-  }\n+  __ verified_entry(C, 0);\n@@ -1783,2 +1774,2 @@\n-  if (C->max_vector_size() > 0) {\n-    __ reinitialize_ptrue();\n+  if (C->stub_function() == nullptr) {\n+    __ entry_barrier();\n@@ -1787,27 +1778,2 @@\n-  int bangsize = C->output()->bang_size_in_bytes();\n-  if (C->output()->need_stack_bang(bangsize))\n-    __ generate_stack_overflow_check(bangsize);\n-\n-  __ build_frame(framesize);\n-\n-  if (C->stub_function() == nullptr) {\n-    BarrierSetAssembler* bs = BarrierSet::barrier_set()->barrier_set_assembler();\n-    if (BarrierSet::barrier_set()->barrier_set_nmethod() != nullptr) {\n-      \/\/ Dummy labels for just measuring the code size\n-      Label dummy_slow_path;\n-      Label dummy_continuation;\n-      Label dummy_guard;\n-      Label* slow_path = &dummy_slow_path;\n-      Label* continuation = &dummy_continuation;\n-      Label* guard = &dummy_guard;\n-      if (!Compile::current()->output()->in_scratch_emit_size()) {\n-        \/\/ Use real labels from actual stub when not emitting code for the purpose of measuring its size\n-        C2EntryBarrierStub* stub = new (Compile::current()->comp_arena()) C2EntryBarrierStub();\n-        Compile::current()->output()->add_stub(stub);\n-        slow_path = &stub->entry();\n-        continuation = &stub->continuation();\n-        guard = &stub->guard();\n-      }\n-      \/\/ In the C2 code, we move the non-hot part of nmethod entry barriers out-of-line to a stub.\n-      bs->nmethod_entry_barrier(masm, slow_path, continuation, guard);\n-    }\n+  if (!Compile::current()->output()->in_scratch_emit_size()) {\n+    __ bind(*_verified_entry);\n@@ -1830,6 +1796,0 @@\n-uint MachPrologNode::size(PhaseRegAlloc* ra_) const\n-{\n-  return MachNode::size(ra_); \/\/ too many variables; just compute it\n-                              \/\/ the hard way\n-}\n-\n@@ -1878,1 +1838,1 @@\n-  __ remove_frame(framesize);\n+  __ remove_frame(framesize, C->needs_stack_repair());\n@@ -1897,5 +1857,0 @@\n-uint MachEpilogNode::size(PhaseRegAlloc *ra_) const {\n-  \/\/ Variable size. Determine dynamically.\n-  return MachNode::size(ra_);\n-}\n-\n@@ -2197,1 +2152,42 @@\n-\/\/=============================================================================\n+\/\/\/=============================================================================\n+#ifndef PRODUCT\n+void MachVEPNode::format(PhaseRegAlloc* ra_, outputStream* st) const\n+{\n+  st->print_cr(\"# MachVEPNode\");\n+  if (!_verified) {\n+    st->print_cr(\"\\t load_class\");\n+  } else {\n+    st->print_cr(\"\\t unpack_inline_arg\");\n+  }\n+}\n+#endif\n+\n+void MachVEPNode::emit(C2_MacroAssembler *masm, PhaseRegAlloc* ra_) const\n+{\n+  if (!_verified) {\n+    __ ic_check(1);\n+  } else {\n+    \/\/ insert a nop at the start of the prolog so we can patch in a\n+    \/\/ branch if we need to invalidate the method later\n+    __ nop();\n+\n+    \/\/ TODO 8284443 Avoid creation of temporary frame\n+    if (ra_->C->stub_function() == nullptr) {\n+      __ verified_entry(ra_->C, 0);\n+      __ entry_barrier();\n+      int framesize = ra_->C->output()->frame_slots() << LogBytesPerInt;\n+      __ remove_frame(framesize, false);\n+    }\n+    \/\/ Unpack inline type args passed as oop and then jump to\n+    \/\/ the verified entry point (skipping the unverified entry).\n+    int sp_inc = __ unpack_inline_args(ra_->C, _receiver_only);\n+    \/\/ Emit code for verified entry and save increment for stack repair on return\n+    __ verified_entry(ra_->C, sp_inc);\n+    if (Compile::current()->output()->in_scratch_emit_size()) {\n+      Label dummy_verified_entry;\n+      __ b(dummy_verified_entry);\n+    } else {\n+      __ b(*_verified_entry);\n+    }\n+  }\n+}\n@@ -2199,0 +2195,1 @@\n+\/\/=============================================================================\n@@ -2221,5 +2218,0 @@\n-uint MachUEPNode::size(PhaseRegAlloc* ra_) const\n-{\n-  return MachNode::size(ra_);\n-}\n-\n@@ -3677,0 +3669,31 @@\n+    if (tf()->returns_inline_type_as_fields() && !_method->is_method_handle_intrinsic()) {\n+      \/\/ The last return value is not set by the callee but used to pass IsInit information to compiled code.\n+      \/\/ Search for the corresponding projection, get the register and emit code that initialized it.\n+      uint con = (tf()->range_cc()->cnt() - 1);\n+      for (DUIterator_Fast imax, i = fast_outs(imax); i < imax; i++) {\n+        ProjNode* proj = fast_out(i)->as_Proj();\n+        if (proj->_con == con) {\n+          \/\/ Set IsInit if r0 is non-null (a non-null value is returned buffered or scalarized)\n+          OptoReg::Name optoReg = ra_->get_reg_first(proj);\n+          VMReg reg = OptoReg::as_VMReg(optoReg, ra_->_framesize, OptoReg::reg2stack(ra_->_matcher._new_SP));\n+          Register toReg = reg->is_reg() ? reg->as_Register() : rscratch1;\n+          __ cmp(r0, zr);\n+          __ cset(toReg, Assembler::NE);\n+          if (reg->is_stack()) {\n+            int st_off = reg->reg2stack() * VMRegImpl::stack_slot_size;\n+            __ str(toReg, Address(sp, st_off));\n+          }\n+          break;\n+        }\n+      }\n+      if (return_value_is_used()) {\n+        \/\/ An inline type is returned as fields in multiple registers.\n+        \/\/ R0 either contains an oop if the inline type is buffered or a pointer\n+        \/\/ to the corresponding InlineKlass with the lowest bit set to 1. Zero r0\n+        \/\/ if the lowest bit is set to allow C2 to use the oop after null checking.\n+        \/\/ r0 &= (r0 & 1) - 1\n+        __ andr(rscratch1, r0, 0x1);\n+        __ sub(rscratch1, rscratch1, 0x1);\n+        __ andr(r0, r0, rscratch1);\n+      }\n+    }\n@@ -6736,1 +6759,1 @@\n-    \"mov  $dst, $con\\t# ptr\\n\\t\"\n+    \"mov  $dst, $con\\t# ptr\"\n@@ -7932,0 +7955,15 @@\n+instruct castN2X(iRegLNoSp dst, iRegN src) %{\n+  match(Set dst (CastP2X src));\n+\n+  ins_cost(INSN_COST);\n+  format %{ \"mov $dst, $src\\t# ptr -> long\" %}\n+\n+  ins_encode %{\n+    if ($dst$$reg != $src$$reg) {\n+      __ mov(as_Register($dst$$reg), as_Register($src$$reg));\n+    }\n+  %}\n+\n+  ins_pipe(ialu_reg);\n+%}\n+\n@@ -14757,1 +14795,1 @@\n-instruct clearArray_reg_reg(iRegL_R11 cnt, iRegP_R10 base, Universe dummy, rFlagsReg cr)\n+instruct clearArray_reg_reg_immL0(iRegL_R11 cnt, iRegP_R10 base, immL0 zero, Universe dummy, rFlagsReg cr)\n@@ -14759,1 +14797,1 @@\n-  match(Set dummy (ClearArray cnt base));\n+  match(Set dummy (ClearArray (Binary cnt base) zero));\n@@ -14776,0 +14814,16 @@\n+instruct clearArray_reg_reg(iRegL_R11 cnt, iRegP_R10 base, iRegL val, Universe dummy, rFlagsReg cr)\n+%{\n+  predicate(((ClearArrayNode*)n)->word_copy_only());\n+  match(Set dummy (ClearArray (Binary cnt base) val));\n+  effect(USE_KILL cnt, USE_KILL base, KILL cr);\n+\n+  ins_cost(4 * INSN_COST);\n+  format %{ \"ClearArray $cnt, $base, $val\" %}\n+\n+  ins_encode %{\n+    __ fill_words($base$$Register, $cnt$$Register, $val$$Register);\n+  %}\n+\n+  ins_pipe(pipe_class_memory);\n+%}\n+\n@@ -14779,1 +14833,2 @@\n-            < (uint64_t)(BlockZeroingLowLimit >> LogBytesPerWord));\n+            < (uint64_t)(BlockZeroingLowLimit >> LogBytesPerWord)\n+            && !((ClearArrayNode*)n)->word_copy_only());\n@@ -16114,0 +16169,18 @@\n+\/\/ entry point is null, target holds the address to call\n+instruct CallLeafNoFPIndirect(iRegP target)\n+%{\n+  predicate(n->as_Call()->entry_point() == nullptr);\n+\n+  match(CallLeafNoFP target);\n+\n+  ins_cost(CALL_COST);\n+\n+  format %{ \"CALL, runtime leaf nofp indirect $target\" %}\n+\n+  ins_encode %{\n+    __ blr($target$$Register);\n+  %}\n+\n+  ins_pipe(pipe_class_call);\n+%}\n+\n@@ -16116,0 +16189,2 @@\n+  predicate(n->as_Call()->entry_point() != nullptr);\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/aarch64.ad","additions":139,"deletions":64,"binary":false,"changes":203,"status":"modified"},{"patch":"@@ -30,0 +30,1 @@\n+#include \"ci\/ciInlineKlass.hpp\"\n@@ -50,0 +51,1 @@\n+#include \"oops\/resolvedFieldEntry.hpp\"\n@@ -56,0 +58,1 @@\n+#include \"runtime\/signature_cc.hpp\"\n@@ -59,0 +62,1 @@\n+#include \"vmreg_aarch64.inline.hpp\"\n@@ -1174,0 +1178,36 @@\n+void MacroAssembler::get_default_value_oop(Register inline_klass, Register temp_reg, Register obj) {\n+  assert_different_registers(inline_klass, temp_reg, obj, rscratch2);\n+#ifdef ASSERT\n+  {\n+    Label done_check;\n+    test_klass_is_inline_type(inline_klass, temp_reg, done_check);\n+    stop(\"get_default_value_oop from non inline type klass\");\n+    bind(done_check);\n+  }\n+#endif\n+  Register offset = temp_reg;\n+  \/\/ Getting the offset of the pre-allocated default value\n+  ldr(offset, Address(inline_klass, in_bytes(InstanceKlass::adr_inlineklass_fixed_block_offset())));\n+  load_sized_value(offset, Address(offset, in_bytes(InlineKlass::default_value_offset_offset())), sizeof(int), true \/*is_signed*\/);\n+\n+  \/\/ Getting the mirror\n+  ldr(obj, Address(inline_klass, in_bytes(Klass::java_mirror_offset())));\n+  resolve_oop_handle(obj, inline_klass, rscratch2);\n+\n+  \/\/ Getting the pre-allocated default value from the mirror\n+  Address field(obj, offset);\n+  load_heap_oop(obj, field, inline_klass, rscratch2);\n+}\n+\n+void MacroAssembler::get_empty_inline_type_oop(Register inline_klass, Register temp_reg, Register obj) {\n+#ifdef ASSERT\n+  {\n+    Label done_check;\n+    test_klass_is_empty_inline_type(inline_klass, temp_reg, done_check);\n+    stop(\"get_empty_value from non-empty inline klass\");\n+    bind(done_check);\n+  }\n+#endif\n+  get_default_value_oop(inline_klass, temp_reg, obj);\n+}\n+\n@@ -1862,1 +1902,5 @@\n-  if (!VerifyOops) return;\n+  if (!VerifyOops || VerifyAdapterSharing) {\n+    \/\/ Below address of the code string confuses VerifyAdapterSharing\n+    \/\/ because it may differ between otherwise equivalent adapters.\n+    return;\n+  }\n@@ -1895,1 +1939,5 @@\n-  if (!VerifyOops) return;\n+  if (!VerifyOops || VerifyAdapterSharing) {\n+    \/\/ Below address of the code string confuses VerifyAdapterSharing\n+    \/\/ because it may differ between otherwise equivalent adapters.\n+    return;\n+  }\n@@ -1993,0 +2041,4 @@\n+void MacroAssembler::super_call_VM_leaf(address entry_point) {\n+  MacroAssembler::call_VM_leaf_base(entry_point, 1);\n+}\n+\n@@ -2038,0 +2090,105 @@\n+void MacroAssembler::test_markword_is_inline_type(Register markword, Label& is_inline_type) {\n+  assert_different_registers(markword, rscratch2);\n+  andr(markword, markword, markWord::inline_type_mask_in_place);\n+  mov(rscratch2, markWord::inline_type_pattern);\n+  cmp(markword, rscratch2);\n+  br(Assembler::EQ, is_inline_type);\n+}\n+\n+void MacroAssembler::test_klass_is_inline_type(Register klass, Register temp_reg, Label& is_inline_type) {\n+  ldrw(temp_reg, Address(klass, Klass::access_flags_offset()));\n+  andr(temp_reg, temp_reg, JVM_ACC_IDENTITY);\n+  cbz(temp_reg, is_inline_type);\n+}\n+\n+void MacroAssembler::test_oop_is_not_inline_type(Register object, Register tmp, Label& not_inline_type) {\n+  assert_different_registers(tmp, rscratch1);\n+  cbz(object, not_inline_type);\n+  const int is_inline_type_mask = markWord::inline_type_pattern;\n+  ldr(tmp, Address(object, oopDesc::mark_offset_in_bytes()));\n+  mov(rscratch1, is_inline_type_mask);\n+  andr(tmp, tmp, rscratch1);\n+  cmp(tmp, rscratch1);\n+  br(Assembler::NE, not_inline_type);\n+}\n+\n+void MacroAssembler::test_klass_is_empty_inline_type(Register klass, Register temp_reg, Label& is_empty_inline_type) {\n+#ifdef ASSERT\n+  {\n+    Label done_check;\n+    test_klass_is_inline_type(klass, temp_reg, done_check);\n+    stop(\"test_klass_is_empty_inline_type with non inline type klass\");\n+    bind(done_check);\n+  }\n+#endif\n+  ldrw(temp_reg, Address(klass, InstanceKlass::misc_flags_offset()));\n+  andr(temp_reg, temp_reg, InstanceKlassFlags::is_empty_inline_type_value());\n+  cbnz(temp_reg, is_empty_inline_type);\n+}\n+\n+void MacroAssembler::test_field_is_null_free_inline_type(Register flags, Register temp_reg, Label& is_null_free_inline_type) {\n+  assert(temp_reg == noreg, \"not needed\"); \/\/ keep signature uniform with x86\n+  tbnz(flags, ResolvedFieldEntry::is_null_free_inline_type_shift, is_null_free_inline_type);\n+}\n+\n+void MacroAssembler::test_field_is_not_null_free_inline_type(Register flags, Register temp_reg, Label& not_null_free_inline_type) {\n+  assert(temp_reg == noreg, \"not needed\"); \/\/ keep signature uniform with x86\n+  tbz(flags, ResolvedFieldEntry::is_null_free_inline_type_shift, not_null_free_inline_type);\n+}\n+\n+void MacroAssembler::test_field_is_flat(Register flags, Register temp_reg, Label& is_flat) {\n+  assert(temp_reg == noreg, \"not needed\"); \/\/ keep signature uniform with x86\n+  tbnz(flags, ResolvedFieldEntry::is_flat_shift, is_flat);\n+}\n+\n+void MacroAssembler::test_field_has_null_marker(Register flags, Register temp_reg, Label& has_null_marker) {\n+  assert(temp_reg == noreg, \"not needed\"); \/\/ keep signature uniform with x86\n+  tbnz(flags, ResolvedFieldEntry::has_null_marker_shift, has_null_marker);\n+}\n+\n+void MacroAssembler::test_oop_prototype_bit(Register oop, Register temp_reg, int32_t test_bit, bool jmp_set, Label& jmp_label) {\n+  Label test_mark_word;\n+  \/\/ load mark word\n+  ldr(temp_reg, Address(oop, oopDesc::mark_offset_in_bytes()));\n+  \/\/ check displaced\n+  tst(temp_reg, markWord::unlocked_value);\n+  br(Assembler::NE, test_mark_word);\n+  \/\/ slow path use klass prototype\n+  load_prototype_header(temp_reg, oop);\n+\n+  bind(test_mark_word);\n+  andr(temp_reg, temp_reg, test_bit);\n+  if (jmp_set) {\n+    cbnz(temp_reg, jmp_label);\n+  } else {\n+    cbz(temp_reg, jmp_label);\n+  }\n+}\n+\n+void MacroAssembler::test_flat_array_oop(Register oop, Register temp_reg, Label& is_flat_array) {\n+  test_oop_prototype_bit(oop, temp_reg, markWord::flat_array_bit_in_place, true, is_flat_array);\n+}\n+\n+void MacroAssembler::test_non_flat_array_oop(Register oop, Register temp_reg,\n+                                                  Label&is_non_flat_array) {\n+  test_oop_prototype_bit(oop, temp_reg, markWord::flat_array_bit_in_place, false, is_non_flat_array);\n+}\n+\n+void MacroAssembler::test_null_free_array_oop(Register oop, Register temp_reg, Label& is_null_free_array) {\n+  test_oop_prototype_bit(oop, temp_reg, markWord::null_free_array_bit_in_place, true, is_null_free_array);\n+}\n+\n+void MacroAssembler::test_non_null_free_array_oop(Register oop, Register temp_reg, Label&is_non_null_free_array) {\n+  test_oop_prototype_bit(oop, temp_reg, markWord::null_free_array_bit_in_place, false, is_non_null_free_array);\n+}\n+\n+void MacroAssembler::test_flat_array_layout(Register lh, Label& is_flat_array) {\n+  tst(lh, Klass::_lh_array_tag_flat_value_bit_inplace);\n+  br(Assembler::NE, is_flat_array);\n+}\n+\n+void MacroAssembler::test_non_flat_array_layout(Register lh, Label& is_non_flat_array) {\n+  tst(lh, Klass::_lh_array_tag_flat_value_bit_inplace);\n+  br(Assembler::EQ, is_non_flat_array);\n+}\n+\n@@ -4842,0 +4999,8 @@\n+void MacroAssembler::load_metadata(Register dst, Register src) {\n+  if (UseCompressedClassPointers) {\n+    ldrw(dst, Address(src, oopDesc::klass_offset_in_bytes()));\n+  } else {\n+    ldr(dst, Address(src, oopDesc::klass_offset_in_bytes()));\n+  }\n+}\n+\n@@ -4917,0 +5082,5 @@\n+void MacroAssembler::load_prototype_header(Register dst, Register src) {\n+  load_klass(dst, src);\n+  ldr(dst, Address(dst, Klass::prototype_header_offset()));\n+}\n+\n@@ -5243,0 +5413,46 @@\n+void MacroAssembler::access_value_copy(DecoratorSet decorators, Register src, Register dst,\n+                                       Register inline_klass) {\n+  BarrierSetAssembler* bs = BarrierSet::barrier_set()->barrier_set_assembler();\n+  bs->value_copy(this, decorators, src, dst, inline_klass);\n+}\n+\n+void MacroAssembler::flat_field_copy(DecoratorSet decorators, Register src, Register dst,\n+                                     Register inline_layout_info) {\n+  BarrierSetAssembler* bs = BarrierSet::barrier_set()->barrier_set_assembler();\n+  bs->flat_field_copy(this, decorators, src, dst, inline_layout_info);\n+}\n+\n+void MacroAssembler::first_field_offset(Register inline_klass, Register offset) {\n+  ldr(offset, Address(inline_klass, InstanceKlass::adr_inlineklass_fixed_block_offset()));\n+  ldrw(offset, Address(offset, InlineKlass::first_field_offset_offset()));\n+}\n+\n+void MacroAssembler::data_for_oop(Register oop, Register data, Register inline_klass) {\n+  \/\/ ((address) (void*) o) + vk->first_field_offset();\n+  Register offset = (data == oop) ? rscratch1 : data;\n+  first_field_offset(inline_klass, offset);\n+  if (data == oop) {\n+    add(data, data, offset);\n+  } else {\n+    lea(data, Address(oop, offset));\n+  }\n+}\n+\n+void MacroAssembler::data_for_value_array_index(Register array, Register array_klass,\n+                                                Register index, Register data) {\n+  assert_different_registers(array, array_klass, index);\n+  assert_different_registers(rscratch1, array, index);\n+\n+  \/\/ array->base() + (index << Klass::layout_helper_log2_element_size(lh));\n+  ldrw(rscratch1, Address(array_klass, Klass::layout_helper_offset()));\n+\n+  \/\/ Klass::layout_helper_log2_element_size(lh)\n+  \/\/ (lh >> _lh_log2_element_size_shift) & _lh_log2_element_size_mask;\n+  lsr(rscratch1, rscratch1, Klass::_lh_log2_element_size_shift);\n+  andr(rscratch1, rscratch1, Klass::_lh_log2_element_size_mask);\n+  lslv(index, index, rscratch1);\n+\n+  add(data, array, index);\n+  add(data, data, arrayOopDesc::base_offset_in_bytes(T_PRIMITIVE_OBJECT));\n+}\n+\n@@ -5319,0 +5535,96 @@\n+\/\/ Object \/ value buffer allocation...\n+void MacroAssembler::allocate_instance(Register klass, Register new_obj,\n+                                       Register t1, Register t2,\n+                                       bool clear_fields, Label& alloc_failed)\n+{\n+  Label done, initialize_header, initialize_object, slow_case, slow_case_no_pop;\n+  Register layout_size = t1;\n+  assert(new_obj == r0, \"needs to be r0\");\n+  assert_different_registers(klass, new_obj, t1, t2);\n+\n+  \/\/ get instance_size in InstanceKlass (scaled to a count of bytes)\n+  ldrw(layout_size, Address(klass, Klass::layout_helper_offset()));\n+  \/\/ test to see if it is malformed in some way\n+  tst(layout_size, Klass::_lh_instance_slow_path_bit);\n+  br(Assembler::NE, slow_case_no_pop);\n+\n+  \/\/ Allocate the instance:\n+  \/\/  If TLAB is enabled:\n+  \/\/    Try to allocate in the TLAB.\n+  \/\/    If fails, go to the slow path.\n+  \/\/    Initialize the allocation.\n+  \/\/    Exit.\n+  \/\/\n+  \/\/  Go to slow path.\n+\n+  if (UseTLAB) {\n+    push(klass);\n+    tlab_allocate(new_obj, layout_size, 0, klass, t2, slow_case);\n+    if (ZeroTLAB || (!clear_fields)) {\n+      \/\/ the fields have been already cleared\n+      b(initialize_header);\n+    } else {\n+      \/\/ initialize both the header and fields\n+      b(initialize_object);\n+    }\n+\n+    if (clear_fields) {\n+      \/\/ The object is initialized before the header.  If the object size is\n+      \/\/ zero, go directly to the header initialization.\n+      bind(initialize_object);\n+      subs(layout_size, layout_size, sizeof(oopDesc));\n+      br(Assembler::EQ, initialize_header);\n+\n+      \/\/ Initialize topmost object field, divide size by 8, check if odd and\n+      \/\/ test if zero.\n+\n+  #ifdef ASSERT\n+      \/\/ make sure instance_size was multiple of 8\n+      Label L;\n+      tst(layout_size, 7);\n+      br(Assembler::EQ, L);\n+      stop(\"object size is not multiple of 8 - adjust this code\");\n+      bind(L);\n+      \/\/ must be > 0, no extra check needed here\n+  #endif\n+\n+      lsr(layout_size, layout_size, LogBytesPerLong);\n+\n+      \/\/ initialize remaining object fields: instance_size was a multiple of 8\n+      {\n+        Label loop;\n+        Register base = t2;\n+\n+        bind(loop);\n+        add(rscratch1, new_obj, layout_size, Assembler::LSL, LogBytesPerLong);\n+        str(zr, Address(rscratch1, sizeof(oopDesc) - 1*oopSize));\n+        subs(layout_size, layout_size, 1);\n+        br(Assembler::NE, loop);\n+      }\n+    } \/\/ clear_fields\n+\n+    \/\/ initialize object header only.\n+    bind(initialize_header);\n+    pop(klass);\n+    Register mark_word = t2;\n+    ldr(mark_word, Address(klass, Klass::prototype_header_offset()));\n+    str(mark_word, Address(new_obj, oopDesc::mark_offset_in_bytes ()));\n+    store_klass_gap(new_obj, zr);  \/\/ zero klass gap for compressed oops\n+    mov(t2, klass);         \/\/ preserve klass\n+    store_klass(new_obj, t2);  \/\/ src klass reg is potentially compressed\n+\n+    \/\/ TODO: Valhalla removed SharedRuntime::dtrace_object_alloc from here ?\n+\n+    b(done);\n+  }\n+\n+  if (UseTLAB) {\n+    bind(slow_case);\n+    pop(klass);\n+  }\n+  bind(slow_case_no_pop);\n+  b(alloc_failed);\n+\n+  bind(done);\n+}\n+\n@@ -5358,0 +5670,20 @@\n+void MacroAssembler::get_inline_type_field_klass(Register holder_klass, Register index, Register inline_klass) {\n+  inline_layout_info(holder_klass, index, inline_klass);\n+  ldr(inline_klass, Address(inline_klass, InlineLayoutInfo::klass_offset()));\n+}\n+\n+void MacroAssembler::inline_layout_info(Register holder_klass, Register index, Register layout_info) {\n+  assert_different_registers(holder_klass, index, layout_info);\n+  InlineLayoutInfo array[2];\n+  int size = (char*)&array[1] - (char*)&array[0]; \/\/ computing size of array elements\n+  if (is_power_of_2(size)) {\n+    lsl(index, index, log2i_exact(size)); \/\/ Scale index by power of 2\n+  } else {\n+    mov(layout_info, size);\n+    mul(index, index, layout_info); \/\/ Scale the index to be the entry index * array_element_size\n+  }\n+  ldr(layout_info, Address(holder_klass, InstanceKlass::inline_layout_info_array_offset()));\n+  add(layout_info, layout_info, Array<InlineLayoutInfo>::base_offset_in_bytes());\n+  lea(layout_info, Address(layout_info, index));\n+}\n+\n@@ -5483,0 +5815,51 @@\n+void MacroAssembler::remove_frame(int initial_framesize, bool needs_stack_repair) {\n+  if (needs_stack_repair) {\n+    \/\/ Remove the extension of the caller's frame used for inline type unpacking\n+    \/\/\n+    \/\/ Right now the stack looks like this:\n+    \/\/\n+    \/\/ | Arguments from caller     |\n+    \/\/ |---------------------------|  <-- caller's SP\n+    \/\/ | Saved LR #1               |\n+    \/\/ | Saved FP #1               |\n+    \/\/ |---------------------------|\n+    \/\/ | Extension space for       |\n+    \/\/ |   inline arg (un)packing  |\n+    \/\/ |---------------------------|  <-- start of this method's frame\n+    \/\/ | Saved LR #2               |\n+    \/\/ | Saved FP #2               |\n+    \/\/ |---------------------------|  <-- FP\n+    \/\/ | sp_inc                    |\n+    \/\/ | method locals             |\n+    \/\/ |---------------------------|  <-- SP\n+    \/\/\n+    \/\/ There are two copies of FP and LR on the stack. They will be identical\n+    \/\/ unless the caller has been deoptimized, in which case LR #1 will be patched\n+    \/\/ to point at the deopt blob, and LR #2 will still point into the old method.\n+    \/\/\n+    \/\/ The sp_inc stack slot holds the total size of the frame including the\n+    \/\/ extension space minus two words for the saved FP and LR.\n+\n+    int sp_inc_offset = initial_framesize - 3 * wordSize;  \/\/ Immediately below saved LR and FP\n+\n+    ldr(rscratch1, Address(sp, sp_inc_offset));\n+    add(sp, sp, rscratch1);\n+    ldp(rfp, lr, Address(post(sp, 2 * wordSize)));\n+  } else {\n+    remove_frame(initial_framesize);\n+  }\n+}\n+\n+void MacroAssembler::save_stack_increment(int sp_inc, int frame_size) {\n+  int real_frame_size = frame_size + sp_inc;\n+  assert(sp_inc == 0 || sp_inc > 2*wordSize, \"invalid sp_inc value\");\n+  assert(real_frame_size >= 2*wordSize, \"frame size must include FP\/LR space\");\n+  assert((real_frame_size & (StackAlignmentInBytes-1)) == 0, \"frame size not aligned\");\n+\n+  int sp_inc_offset = frame_size - 3 * wordSize;  \/\/ Immediately below saved LR and FP\n+\n+  \/\/ Subtract two words for the saved FP and LR as these will be popped\n+  \/\/ separately. See remove_frame above.\n+  mov(rscratch1, real_frame_size - 2*wordSize);\n+  str(rscratch1, Address(sp, sp_inc_offset));\n+}\n@@ -6396,0 +6779,443 @@\n+#ifdef COMPILER2\n+\/\/ C2 compiled method's prolog code\n+\/\/ Moved here from aarch64.ad to support Valhalla code belows\n+void MacroAssembler::verified_entry(Compile* C, int sp_inc) {\n+  if (C->clinit_barrier_on_entry()) {\n+    assert(!C->method()->holder()->is_not_initialized(), \"initialization should have been started\");\n+\n+    Label L_skip_barrier;\n+\n+    mov_metadata(rscratch2, C->method()->holder()->constant_encoding());\n+    clinit_barrier(rscratch2, rscratch1, &L_skip_barrier);\n+    far_jump(RuntimeAddress(SharedRuntime::get_handle_wrong_method_stub()));\n+    bind(L_skip_barrier);\n+  }\n+\n+  if (C->max_vector_size() > 0) {\n+    reinitialize_ptrue();\n+  }\n+\n+  int bangsize = C->output()->bang_size_in_bytes();\n+  if (C->output()->need_stack_bang(bangsize))\n+    generate_stack_overflow_check(bangsize);\n+\n+  \/\/ n.b. frame size includes space for return pc and rfp\n+  const long framesize = C->output()->frame_size_in_bytes();\n+  build_frame(framesize);\n+\n+  if (C->needs_stack_repair()) {\n+    save_stack_increment(sp_inc, framesize);\n+  }\n+\n+  if (VerifyStackAtCalls) {\n+    Unimplemented();\n+  }\n+}\n+#endif \/\/ COMPILER2\n+\n+int MacroAssembler::store_inline_type_fields_to_buf(ciInlineKlass* vk, bool from_interpreter) {\n+  assert(InlineTypeReturnedAsFields, \"Inline types should never be returned as fields\");\n+  \/\/ An inline type might be returned. If fields are in registers we\n+  \/\/ need to allocate an inline type instance and initialize it with\n+  \/\/ the value of the fields.\n+  Label skip;\n+  \/\/ We only need a new buffered inline type if a new one is not returned\n+  tbz(r0, 0, skip);\n+  int call_offset = -1;\n+\n+  \/\/ Be careful not to clobber r1-7 which hold returned fields\n+  \/\/ Also do not use callee-saved registers as these may be live in the interpreter\n+  Register tmp1 = r13, tmp2 = r14, klass = r15, r0_preserved = r12;\n+\n+  \/\/ The following code is similar to allocate_instance but has some slight differences,\n+  \/\/ e.g. object size is always not zero, sometimes it's constant; storing klass ptr after\n+  \/\/ allocating is not necessary if vk != nullptr, etc. allocate_instance is not aware of these.\n+  Label slow_case;\n+  \/\/ 1. Try to allocate a new buffered inline instance either from TLAB or eden space\n+  mov(r0_preserved, r0); \/\/ save r0 for slow_case since *_allocate may corrupt it when allocation failed\n+\n+  if (vk != nullptr) {\n+    \/\/ Called from C1, where the return type is statically known.\n+    movptr(klass, (intptr_t)vk->get_InlineKlass());\n+    jint lh = vk->layout_helper();\n+    assert(lh != Klass::_lh_neutral_value, \"inline class in return type must have been resolved\");\n+    if (UseTLAB && !Klass::layout_helper_needs_slow_path(lh)) {\n+      tlab_allocate(r0, noreg, lh, tmp1, tmp2, slow_case);\n+    } else {\n+      b(slow_case);\n+    }\n+  } else {\n+    \/\/ Call from interpreter. R0 contains ((the InlineKlass* of the return type) | 0x01)\n+    andr(klass, r0, -2);\n+    if (UseTLAB) {\n+      ldrw(tmp2, Address(klass, Klass::layout_helper_offset()));\n+      tst(tmp2, Klass::_lh_instance_slow_path_bit);\n+      br(Assembler::NE, slow_case);\n+      tlab_allocate(r0, tmp2, 0, tmp1, tmp2, slow_case);\n+    } else {\n+      b(slow_case);\n+    }\n+  }\n+  if (UseTLAB) {\n+    \/\/ 2. Initialize buffered inline instance header\n+    Register buffer_obj = r0;\n+    mov(rscratch1, (intptr_t)markWord::inline_type_prototype().value());\n+    str(rscratch1, Address(buffer_obj, oopDesc::mark_offset_in_bytes()));\n+    store_klass_gap(buffer_obj, zr);\n+    if (vk == nullptr) {\n+      \/\/ store_klass corrupts klass, so save it for later use (interpreter case only).\n+      mov(tmp1, klass);\n+    }\n+    store_klass(buffer_obj, klass);\n+    \/\/ 3. Initialize its fields with an inline class specific handler\n+    if (vk != nullptr) {\n+      far_call(RuntimeAddress(vk->pack_handler())); \/\/ no need for call info as this will not safepoint.\n+    } else {\n+      \/\/ tmp1 holds klass preserved above\n+      ldr(tmp1, Address(tmp1, InstanceKlass::adr_inlineklass_fixed_block_offset()));\n+      ldr(tmp1, Address(tmp1, InlineKlass::pack_handler_offset()));\n+      blr(tmp1);\n+    }\n+\n+    membar(Assembler::StoreStore);\n+    b(skip);\n+  } else {\n+    \/\/ Must have already branched to slow_case above.\n+    DEBUG_ONLY(should_not_reach_here());\n+  }\n+  bind(slow_case);\n+  \/\/ We failed to allocate a new inline type, fall back to a runtime\n+  \/\/ call. Some oop field may be live in some registers but we can't\n+  \/\/ tell. That runtime call will take care of preserving them\n+  \/\/ across a GC if there's one.\n+  mov(r0, r0_preserved);\n+\n+  if (from_interpreter) {\n+    super_call_VM_leaf(StubRoutines::store_inline_type_fields_to_buf());\n+  } else {\n+    far_call(RuntimeAddress(StubRoutines::store_inline_type_fields_to_buf()));\n+    call_offset = offset();\n+  }\n+  membar(Assembler::StoreStore);\n+\n+  bind(skip);\n+  return call_offset;\n+}\n+\n+\/\/ Move a value between registers\/stack slots and update the reg_state\n+bool MacroAssembler::move_helper(VMReg from, VMReg to, BasicType bt, RegState reg_state[]) {\n+  assert(from->is_valid() && to->is_valid(), \"source and destination must be valid\");\n+  if (reg_state[to->value()] == reg_written) {\n+    return true; \/\/ Already written\n+  }\n+\n+  if (from != to && bt != T_VOID) {\n+    if (reg_state[to->value()] == reg_readonly) {\n+      return false; \/\/ Not yet writable\n+    }\n+    if (from->is_reg()) {\n+      if (to->is_reg()) {\n+        if (from->is_Register() && to->is_Register()) {\n+          mov(to->as_Register(), from->as_Register());\n+        } else if (from->is_FloatRegister() && to->is_FloatRegister()) {\n+          fmovd(to->as_FloatRegister(), from->as_FloatRegister());\n+        } else {\n+          ShouldNotReachHere();\n+        }\n+      } else {\n+        int st_off = to->reg2stack() * VMRegImpl::stack_slot_size;\n+        Address to_addr = Address(sp, st_off);\n+        if (from->is_FloatRegister()) {\n+          if (bt == T_DOUBLE) {\n+             strd(from->as_FloatRegister(), to_addr);\n+          } else {\n+             assert(bt == T_FLOAT, \"must be float\");\n+             strs(from->as_FloatRegister(), to_addr);\n+          }\n+        } else {\n+          str(from->as_Register(), to_addr);\n+        }\n+      }\n+    } else {\n+      Address from_addr = Address(sp, from->reg2stack() * VMRegImpl::stack_slot_size);\n+      if (to->is_reg()) {\n+        if (to->is_FloatRegister()) {\n+          if (bt == T_DOUBLE) {\n+            ldrd(to->as_FloatRegister(), from_addr);\n+          } else {\n+            assert(bt == T_FLOAT, \"must be float\");\n+            ldrs(to->as_FloatRegister(), from_addr);\n+          }\n+        } else {\n+          ldr(to->as_Register(), from_addr);\n+        }\n+      } else {\n+        int st_off = to->reg2stack() * VMRegImpl::stack_slot_size;\n+        ldr(rscratch1, from_addr);\n+        str(rscratch1, Address(sp, st_off));\n+      }\n+    }\n+  }\n+\n+  \/\/ Update register states\n+  reg_state[from->value()] = reg_writable;\n+  reg_state[to->value()] = reg_written;\n+  return true;\n+}\n+\n+\/\/ Calculate the extra stack space required for packing or unpacking inline\n+\/\/ args and adjust the stack pointer\n+int MacroAssembler::extend_stack_for_inline_args(int args_on_stack) {\n+  int sp_inc = args_on_stack * VMRegImpl::stack_slot_size;\n+  sp_inc = align_up(sp_inc, StackAlignmentInBytes);\n+  assert(sp_inc > 0, \"sanity\");\n+\n+  \/\/ Save a copy of the FP and LR here for deoptimization patching and frame walking\n+  stp(rfp, lr, Address(pre(sp, -2 * wordSize)));\n+\n+  \/\/ Adjust the stack pointer. This will be repaired on return by MacroAssembler::remove_frame\n+  if (sp_inc < (1 << 9)) {\n+    sub(sp, sp, sp_inc);   \/\/ Fits in an immediate\n+  } else {\n+    mov(rscratch1, sp_inc);\n+    sub(sp, sp, rscratch1);\n+  }\n+\n+  return sp_inc + 2 * wordSize;  \/\/ Account for the FP\/LR space\n+}\n+\n+\/\/ Read all fields from an inline type oop and store the values in registers\/stack slots\n+bool MacroAssembler::unpack_inline_helper(const GrowableArray<SigEntry>* sig, int& sig_index,\n+                                          VMReg from, int& from_index, VMRegPair* to, int to_count, int& to_index,\n+                                          RegState reg_state[]) {\n+  assert(sig->at(sig_index)._bt == T_VOID, \"should be at end delimiter\");\n+  assert(from->is_valid(), \"source must be valid\");\n+  bool progress = false;\n+#ifdef ASSERT\n+  const int start_offset = offset();\n+#endif\n+\n+  Label L_null, L_notNull;\n+  \/\/ Don't use r14 as tmp because it's used for spilling (see MacroAssembler::spill_reg_for)\n+  Register tmp1 = r10;\n+  Register tmp2 = r11;\n+  Register fromReg = noreg;\n+  ScalarizedInlineArgsStream stream(sig, sig_index, to, to_count, to_index, -1);\n+  bool done = true;\n+  bool mark_done = true;\n+  VMReg toReg;\n+  BasicType bt;\n+  \/\/ Check if argument requires a null check\n+  bool null_check = false;\n+  VMReg nullCheckReg;\n+  while (stream.next(nullCheckReg, bt)) {\n+    if (sig->at(stream.sig_index())._offset == -1) {\n+      null_check = true;\n+      break;\n+    }\n+  }\n+  stream.reset(sig_index, to_index);\n+  while (stream.next(toReg, bt)) {\n+    assert(toReg->is_valid(), \"destination must be valid\");\n+    int idx = (int)toReg->value();\n+    if (reg_state[idx] == reg_readonly) {\n+      if (idx != from->value()) {\n+        mark_done = false;\n+      }\n+      done = false;\n+      continue;\n+    } else if (reg_state[idx] == reg_written) {\n+      continue;\n+    }\n+    assert(reg_state[idx] == reg_writable, \"must be writable\");\n+    reg_state[idx] = reg_written;\n+    progress = true;\n+\n+    if (fromReg == noreg) {\n+      if (from->is_reg()) {\n+        fromReg = from->as_Register();\n+      } else {\n+        int st_off = from->reg2stack() * VMRegImpl::stack_slot_size;\n+        ldr(tmp1, Address(sp, st_off));\n+        fromReg = tmp1;\n+      }\n+      if (null_check) {\n+        \/\/ Nullable inline type argument, emit null check\n+        cbz(fromReg, L_null);\n+      }\n+    }\n+    int off = sig->at(stream.sig_index())._offset;\n+    if (off == -1) {\n+      assert(null_check, \"Missing null check at\");\n+      if (toReg->is_stack()) {\n+        int st_off = toReg->reg2stack() * VMRegImpl::stack_slot_size;\n+        mov(tmp2, 1);\n+        str(tmp2, Address(sp, st_off));\n+      } else {\n+        mov(toReg->as_Register(), 1);\n+      }\n+      continue;\n+    }\n+    assert(off > 0, \"offset in object should be positive\");\n+    Address fromAddr = Address(fromReg, off);\n+    if (!toReg->is_FloatRegister()) {\n+      Register dst = toReg->is_stack() ? tmp2 : toReg->as_Register();\n+      if (is_reference_type(bt)) {\n+        load_heap_oop(dst, fromAddr, rscratch1, rscratch2);\n+      } else {\n+        bool is_signed = (bt != T_CHAR) && (bt != T_BOOLEAN);\n+        load_sized_value(dst, fromAddr, type2aelembytes(bt), is_signed);\n+      }\n+      if (toReg->is_stack()) {\n+        int st_off = toReg->reg2stack() * VMRegImpl::stack_slot_size;\n+        str(dst, Address(sp, st_off));\n+      }\n+    } else if (bt == T_DOUBLE) {\n+      ldrd(toReg->as_FloatRegister(), fromAddr);\n+    } else {\n+      assert(bt == T_FLOAT, \"must be float\");\n+      ldrs(toReg->as_FloatRegister(), fromAddr);\n+    }\n+  }\n+  if (progress && null_check) {\n+    if (done) {\n+      b(L_notNull);\n+      bind(L_null);\n+      \/\/ Set IsInit field to zero to signal that the argument is null.\n+      \/\/ Also set all oop fields to zero to make the GC happy.\n+      stream.reset(sig_index, to_index);\n+      while (stream.next(toReg, bt)) {\n+        if (sig->at(stream.sig_index())._offset == -1 ||\n+            bt == T_OBJECT || bt == T_ARRAY) {\n+          if (toReg->is_stack()) {\n+            int st_off = toReg->reg2stack() * VMRegImpl::stack_slot_size;\n+            str(zr, Address(sp, st_off));\n+          } else {\n+            mov(toReg->as_Register(), zr);\n+          }\n+        }\n+      }\n+      bind(L_notNull);\n+    } else {\n+      bind(L_null);\n+    }\n+  }\n+\n+  sig_index = stream.sig_index();\n+  to_index = stream.regs_index();\n+\n+  if (mark_done && reg_state[from->value()] != reg_written) {\n+    \/\/ This is okay because no one else will write to that slot\n+    reg_state[from->value()] = reg_writable;\n+  }\n+  from_index--;\n+  assert(progress || (start_offset == offset()), \"should not emit code\");\n+  return done;\n+}\n+\n+\/\/ Pack fields back into an inline type oop\n+bool MacroAssembler::pack_inline_helper(const GrowableArray<SigEntry>* sig, int& sig_index, int vtarg_index,\n+                                        VMRegPair* from, int from_count, int& from_index, VMReg to,\n+                                        RegState reg_state[], Register val_array) {\n+  assert(sig->at(sig_index)._bt == T_METADATA, \"should be at delimiter\");\n+  assert(to->is_valid(), \"destination must be valid\");\n+\n+  if (reg_state[to->value()] == reg_written) {\n+    skip_unpacked_fields(sig, sig_index, from, from_count, from_index);\n+    return true; \/\/ Already written\n+  }\n+\n+  \/\/ The GC barrier expanded by store_heap_oop below may call into the\n+  \/\/ runtime so use callee-saved registers for any values that need to be\n+  \/\/ preserved. The GC barrier assembler should take care of saving the\n+  \/\/ Java argument registers.\n+  \/\/ TODO 8284443 Isn't it an issue if below code uses r14 as tmp when it contains a spilled value?\n+  \/\/ Be careful with r14 because it's used for spilling (see MacroAssembler::spill_reg_for).\n+  Register val_obj_tmp = r21;\n+  Register from_reg_tmp = r22;\n+  Register tmp1 = r14;\n+  Register tmp2 = r13;\n+  Register tmp3 = r12;\n+  Register val_obj = to->is_stack() ? val_obj_tmp : to->as_Register();\n+\n+  assert_different_registers(val_obj_tmp, from_reg_tmp, tmp1, tmp2, tmp3, val_array);\n+\n+  if (reg_state[to->value()] == reg_readonly) {\n+    if (!is_reg_in_unpacked_fields(sig, sig_index, to, from, from_count, from_index)) {\n+      skip_unpacked_fields(sig, sig_index, from, from_count, from_index);\n+      return false; \/\/ Not yet writable\n+    }\n+    val_obj = val_obj_tmp;\n+  }\n+\n+  int index = arrayOopDesc::base_offset_in_bytes(T_OBJECT) + vtarg_index * type2aelembytes(T_OBJECT);\n+  load_heap_oop(val_obj, Address(val_array, index), tmp1, tmp2);\n+\n+  ScalarizedInlineArgsStream stream(sig, sig_index, from, from_count, from_index);\n+  VMReg fromReg;\n+  BasicType bt;\n+  Label L_null;\n+  while (stream.next(fromReg, bt)) {\n+    assert(fromReg->is_valid(), \"source must be valid\");\n+    reg_state[fromReg->value()] = reg_writable;\n+\n+    int off = sig->at(stream.sig_index())._offset;\n+    if (off == -1) {\n+      \/\/ Nullable inline type argument, emit null check\n+      Label L_notNull;\n+      if (fromReg->is_stack()) {\n+        int ld_off = fromReg->reg2stack() * VMRegImpl::stack_slot_size;\n+        ldrb(tmp2, Address(sp, ld_off));\n+        cbnz(tmp2, L_notNull);\n+      } else {\n+        cbnz(fromReg->as_Register(), L_notNull);\n+      }\n+      mov(val_obj, 0);\n+      b(L_null);\n+      bind(L_notNull);\n+      continue;\n+    }\n+\n+    assert(off > 0, \"offset in object should be positive\");\n+    size_t size_in_bytes = is_java_primitive(bt) ? type2aelembytes(bt) : wordSize;\n+\n+    \/\/ Pack the scalarized field into the value object.\n+    Address dst(val_obj, off);\n+\n+    if (!fromReg->is_FloatRegister()) {\n+      Register src;\n+      if (fromReg->is_stack()) {\n+        src = from_reg_tmp;\n+        int ld_off = fromReg->reg2stack() * VMRegImpl::stack_slot_size;\n+        load_sized_value(src, Address(sp, ld_off), size_in_bytes, \/* is_signed *\/ false);\n+      } else {\n+        src = fromReg->as_Register();\n+      }\n+      assert_different_registers(dst.base(), src, tmp1, tmp2, tmp3, val_array);\n+      if (is_reference_type(bt)) {\n+        store_heap_oop(dst, src, tmp1, tmp2, tmp3, IN_HEAP | ACCESS_WRITE | IS_DEST_UNINITIALIZED);\n+      } else {\n+        store_sized_value(dst, src, size_in_bytes);\n+      }\n+    } else if (bt == T_DOUBLE) {\n+      strd(fromReg->as_FloatRegister(), dst);\n+    } else {\n+      assert(bt == T_FLOAT, \"must be float\");\n+      strs(fromReg->as_FloatRegister(), dst);\n+    }\n+  }\n+  bind(L_null);\n+  sig_index = stream.sig_index();\n+  from_index = stream.regs_index();\n+\n+  assert(reg_state[to->value()] == reg_writable, \"must have already been read\");\n+  bool success = move_helper(val_obj->as_VMReg(), to, T_OBJECT, reg_state);\n+  assert(success, \"to register must be writeable\");\n+\n+  return true;\n+}\n+\n+VMReg MacroAssembler::spill_reg_for(VMReg reg) {\n+  return (reg->is_FloatRegister()) ? v8->as_VMReg() : r14->as_VMReg();\n+}\n+\n@@ -6794,0 +7620,4 @@\n+  if (EnableValhalla) {\n+    \/\/ Mask inline_type bit such that we go to the slow path if object is an inline type\n+    andr(mark, mark, ~((int) markWord::inline_type_bit_in_place));\n+  }\n","filename":"src\/hotspot\/cpu\/aarch64\/macroAssembler_aarch64.cpp","additions":832,"deletions":2,"binary":false,"changes":834,"status":"modified"},{"patch":"@@ -3212,0 +3212,3 @@\n+void LIR_Assembler::emit_profile_inline_type(LIR_OpProfileInlineType* op) {\n+  Unimplemented();\n+}\n","filename":"src\/hotspot\/cpu\/ppc\/c1_LIRAssembler_ppc.cpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -53,1 +53,20 @@\n-void C2_MacroAssembler::verified_entry(int framesize, int stack_bang_size, bool fp_mode_24b, bool is_stub) {\n+void C2_MacroAssembler::verified_entry(Compile* C, int sp_inc) {\n+  if (C->clinit_barrier_on_entry()) {\n+    assert(VM_Version::supports_fast_class_init_checks(), \"sanity\");\n+    assert(!C->method()->holder()->is_not_initialized(), \"initialization should have been started\");\n+\n+    Label L_skip_barrier;\n+    Register klass = rscratch1;\n+\n+    mov_metadata(klass, C->method()->holder()->constant_encoding());\n+    clinit_barrier(klass, r15_thread, &L_skip_barrier \/*L_fast_path*\/);\n+\n+    jump(RuntimeAddress(SharedRuntime::get_handle_wrong_method_stub())); \/\/ slow path\n+\n+    bind(L_skip_barrier);\n+  }\n+\n+  int framesize = C->output()->frame_size_in_bytes();\n+  int bangsize = C->output()->bang_size_in_bytes();\n+  bool fp_mode_24b = false;\n+  int stack_bang_size = C->output()->need_stack_bang(bangsize) ? bangsize : 0;\n@@ -106,0 +125,6 @@\n+  if (C->needs_stack_repair()) {\n+    \/\/ Save stack increment just below the saved rbp (also account for fixed framesize and rbp)\n+    assert((sp_inc & (StackAlignmentInBytes-1)) == 0, \"stack increment not aligned\");\n+    movptr(Address(rsp, framesize - wordSize), sp_inc + framesize + wordSize);\n+  }\n+\n@@ -134,0 +159,1 @@\n+}\n@@ -135,17 +161,15 @@\n-  if (!is_stub) {\n-    BarrierSetAssembler* bs = BarrierSet::barrier_set()->barrier_set_assembler();\n- #ifdef _LP64\n-    if (BarrierSet::barrier_set()->barrier_set_nmethod() != nullptr) {\n-      \/\/ We put the non-hot code of the nmethod entry barrier out-of-line in a stub.\n-      Label dummy_slow_path;\n-      Label dummy_continuation;\n-      Label* slow_path = &dummy_slow_path;\n-      Label* continuation = &dummy_continuation;\n-      if (!Compile::current()->output()->in_scratch_emit_size()) {\n-        \/\/ Use real labels from actual stub when not emitting code for the purpose of measuring its size\n-        C2EntryBarrierStub* stub = new (Compile::current()->comp_arena()) C2EntryBarrierStub();\n-        Compile::current()->output()->add_stub(stub);\n-        slow_path = &stub->entry();\n-        continuation = &stub->continuation();\n-      }\n-      bs->nmethod_entry_barrier(this, slow_path, continuation);\n+void C2_MacroAssembler::entry_barrier() {\n+  BarrierSetAssembler* bs = BarrierSet::barrier_set()->barrier_set_assembler();\n+#ifdef _LP64\n+  if (BarrierSet::barrier_set()->barrier_set_nmethod() != nullptr) {\n+    \/\/ We put the non-hot code of the nmethod entry barrier out-of-line in a stub.\n+    Label dummy_slow_path;\n+    Label dummy_continuation;\n+    Label* slow_path = &dummy_slow_path;\n+    Label* continuation = &dummy_continuation;\n+    if (!Compile::current()->output()->in_scratch_emit_size()) {\n+      \/\/ Use real labels from actual stub when not emitting code for the purpose of measuring its size\n+      C2EntryBarrierStub* stub = new (Compile::current()->comp_arena()) C2EntryBarrierStub();\n+      Compile::current()->output()->add_stub(stub);\n+      slow_path = &stub->entry();\n+      continuation = &stub->continuation();\n@@ -153,0 +177,2 @@\n+    bs->nmethod_entry_barrier(this, slow_path, continuation);\n+  }\n@@ -154,2 +180,2 @@\n-    \/\/ Don't bother with out-of-line nmethod entry barrier stub for x86_32.\n-    bs->nmethod_entry_barrier(this, nullptr \/* slow_path *\/, nullptr \/* continuation *\/);\n+  \/\/ Don't bother with out-of-line nmethod entry barrier stub for x86_32.\n+  bs->nmethod_entry_barrier(this, nullptr \/* slow_path *\/, nullptr \/* continuation *\/);\n@@ -157,1 +183,0 @@\n-  }\n@@ -295,0 +320,4 @@\n+    if (EnableValhalla) {\n+      \/\/ Mask inline_type bit such that we go to the slow path if object is an inline type\n+      andptr(tmpReg, ~((int) markWord::inline_type_bit_in_place));\n+    }\n","filename":"src\/hotspot\/cpu\/x86\/c2_MacroAssembler_x86.cpp","additions":50,"deletions":21,"binary":false,"changes":71,"status":"modified"},{"patch":"@@ -32,1 +32,1 @@\n-  void verified_entry(int framesize, int stack_bang_size, bool fp_mode_24b, bool is_stub);\n+  void verified_entry(Compile* C, int sp_inc = 0);\n@@ -34,0 +34,1 @@\n+  void entry_barrier();\n","filename":"src\/hotspot\/cpu\/x86\/c2_MacroAssembler_x86.hpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -2808,0 +2808,33 @@\n+    if (tf()->returns_inline_type_as_fields() && !_method->is_method_handle_intrinsic()) {\n+      \/\/ The last return value is not set by the callee but used to pass IsInit information to compiled code.\n+      \/\/ Search for the corresponding projection, get the register and emit code that initialized it.\n+      uint con = (tf()->range_cc()->cnt() - 1);\n+      for (DUIterator_Fast imax, i = fast_outs(imax); i < imax; i++) {\n+        ProjNode* proj = fast_out(i)->as_Proj();\n+        if (proj->_con == con) {\n+          \/\/ Set IsInit if rax is non-null (a non-null value is returned buffered or scalarized)\n+          OptoReg::Name optoReg = ra_->get_reg_first(proj);\n+          VMReg reg = OptoReg::as_VMReg(optoReg, ra_->_framesize, OptoReg::reg2stack(ra_->_matcher._new_SP));\n+          Register toReg = reg->is_reg() ? reg->as_Register() : rscratch1;\n+          __ testq(rax, rax);\n+          __ setb(Assembler::notZero, toReg);\n+          __ movzbl(toReg, toReg);\n+          if (reg->is_stack()) {\n+            int st_off = reg->reg2stack() * VMRegImpl::stack_slot_size;\n+            __ movq(Address(rsp, st_off), toReg);\n+          }\n+          break;\n+        }\n+      }\n+      if (return_value_is_used()) {\n+        \/\/ An inline type is returned as fields in multiple registers.\n+        \/\/ Rax either contains an oop if the inline type is buffered or a pointer\n+        \/\/ to the corresponding InlineKlass with the lowest bit set to 1. Zero rax\n+        \/\/ if the lowest bit is set to allow C2 to use the oop after null checking.\n+        \/\/ rax &= (rax & 1) - 1\n+        __ movptr(rscratch1, rax);\n+        __ andptr(rscratch1, 0x1);\n+        __ subptr(rscratch1, 0x1);\n+        __ andptr(rax, rscratch1);\n+      }\n+    }\n","filename":"src\/hotspot\/cpu\/x86\/x86.ad","additions":33,"deletions":0,"binary":false,"changes":33,"status":"modified"},{"patch":"@@ -613,4 +613,1 @@\n-  int framesize = C->output()->frame_size_in_bytes();\n-  int bangsize = C->output()->bang_size_in_bytes();\n-\n-  __ verified_entry(framesize, C->output()->need_stack_bang(bangsize)?bangsize:0, C->in_24_bit_fp_mode(), C->stub_function() != nullptr);\n+  __ verified_entry(C);\n","filename":"src\/hotspot\/cpu\/x86\/x86_32.ad","additions":1,"deletions":4,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -606,0 +606,4 @@\n+  if (_entry_point == nullptr) {\n+    \/\/ CallLeafNoFPInDirect\n+    return 3; \/\/ callq (register)\n+  }\n@@ -612,0 +616,1 @@\n+\n@@ -837,14 +842,1 @@\n-  int framesize = C->output()->frame_size_in_bytes();\n-  int bangsize = C->output()->bang_size_in_bytes();\n-\n-  if (C->clinit_barrier_on_entry()) {\n-    assert(VM_Version::supports_fast_class_init_checks(), \"sanity\");\n-    assert(!C->method()->holder()->is_not_initialized(), \"initialization should have been started\");\n-\n-    Label L_skip_barrier;\n-    Register klass = rscratch1;\n-\n-    __ mov_metadata(klass, C->method()->holder()->constant_encoding());\n-    __ clinit_barrier(klass, r15_thread, &L_skip_barrier \/*L_fast_path*\/);\n-\n-    __ jump(RuntimeAddress(SharedRuntime::get_handle_wrong_method_stub())); \/\/ slow path\n+  __ verified_entry(C);\n@@ -852,1 +844,2 @@\n-    __ bind(L_skip_barrier);\n+  if (ra_->C->stub_function() == nullptr) {\n+    __ entry_barrier();\n@@ -855,1 +848,3 @@\n-  __ verified_entry(framesize, C->output()->need_stack_bang(bangsize)?bangsize:0, false, C->stub_function() != nullptr);\n+  if (!Compile::current()->output()->in_scratch_emit_size()) {\n+    __ bind(*_verified_entry);\n+  }\n@@ -867,6 +862,0 @@\n-uint MachPrologNode::size(PhaseRegAlloc* ra_) const\n-{\n-  return MachNode::size(ra_); \/\/ too many variables; just compute it\n-                              \/\/ the hard way\n-}\n-\n@@ -919,13 +908,3 @@\n-  int framesize = C->output()->frame_size_in_bytes();\n-  assert((framesize & (StackAlignmentInBytes-1)) == 0, \"frame size not aligned\");\n-  \/\/ Remove word for return adr already pushed\n-  \/\/ and RBP\n-  framesize -= 2*wordSize;\n-\n-  \/\/ Note that VerifyStackAtCalls' Majik cookie does not change the frame size popped here\n-\n-  if (framesize) {\n-    __ addq(rsp, framesize);\n-  }\n-\n-  __ popq(rbp);\n+  \/\/ Subtract two words to account for return address and rbp\n+  int initial_framesize = C->output()->frame_size_in_bytes() - 2*wordSize;\n+  __ remove_frame(initial_framesize, C->needs_stack_repair());\n@@ -950,6 +929,0 @@\n-uint MachEpilogNode::size(PhaseRegAlloc* ra_) const\n-{\n-  return MachNode::size(ra_); \/\/ too many variables; just compute it\n-                              \/\/ the hard way\n-}\n-\n@@ -1553,0 +1526,43 @@\n+\/\/=============================================================================\n+#ifndef PRODUCT\n+void MachVEPNode::format(PhaseRegAlloc* ra_, outputStream* st) const\n+{\n+  st->print_cr(\"MachVEPNode\");\n+}\n+#endif\n+\n+void MachVEPNode::emit(C2_MacroAssembler* masm, PhaseRegAlloc* ra_) const\n+{\n+  CodeBuffer* cbuf = masm->code();\n+  uint insts_size = cbuf->insts_size();\n+  if (!_verified) {\n+    __ ic_check(1);\n+  } else {\n+    \/\/ TODO 8284443 Avoid creation of temporary frame\n+    if (ra_->C->stub_function() == nullptr) {\n+      __ verified_entry(ra_->C, 0);\n+      __ entry_barrier();\n+      int initial_framesize = ra_->C->output()->frame_size_in_bytes() - 2*wordSize;\n+      __ remove_frame(initial_framesize, false);\n+    }\n+    \/\/ Unpack inline type args passed as oop and then jump to\n+    \/\/ the verified entry point (skipping the unverified entry).\n+    int sp_inc = __ unpack_inline_args(ra_->C, _receiver_only);\n+    \/\/ Emit code for verified entry and save increment for stack repair on return\n+    __ verified_entry(ra_->C, sp_inc);\n+    if (Compile::current()->output()->in_scratch_emit_size()) {\n+      Label dummy_verified_entry;\n+      __ jmp(dummy_verified_entry);\n+    } else {\n+      __ jmp(*_verified_entry);\n+    }\n+  }\n+  \/* WARNING these NOPs are critical so that verified entry point is properly\n+     4 bytes aligned for patching by NativeJump::patch_verified_entry() *\/\n+  int nops_cnt = 4 - ((cbuf->insts_size() - insts_size) & 0x3);\n+  nops_cnt &= 0x3; \/\/ Do not add nops if code is aligned.\n+  if (nops_cnt > 0) {\n+    __ nop(nops_cnt);\n+  }\n+}\n+\n@@ -1573,7 +1589,0 @@\n-uint MachUEPNode::size(PhaseRegAlloc* ra_) const\n-{\n-  return MachNode::size(ra_); \/\/ too many variables; just compute it\n-                              \/\/ the hard way\n-}\n-\n-\n@@ -3062,0 +3071,16 @@\n+\/\/ Indirect Narrow Oop Operand\n+operand indCompressedOop(rRegN reg) %{\n+  predicate(UseCompressedOops && (CompressedOops::shift() == Address::times_8));\n+  constraint(ALLOC_IN_RC(ptr_reg));\n+  match(DecodeN reg);\n+\n+  op_cost(10);\n+  format %{\"[R12 + $reg << 3] (compressed oop addressing)\" %}\n+  interface(MEMORY_INTER) %{\n+    base(0xc); \/\/ R12\n+    index($reg);\n+    scale(0x3);\n+    disp(0x0);\n+  %}\n+%}\n+\n@@ -3408,1 +3433,1 @@\n-               indCompressedOopOffset,\n+               indCompressedOop, indCompressedOopOffset,\n@@ -5916,0 +5941,13 @@\n+instruct castN2X(rRegL dst, rRegN src)\n+%{\n+  match(Set dst (CastP2X src));\n+\n+  format %{ \"movq    $dst, $src\\t# ptr -> long\" %}\n+  ins_encode %{\n+    if ($dst$$reg != $src$$reg) {\n+      __ movptr($dst$$Register, $src$$Register);\n+    }\n+  %}\n+  ins_pipe(ialu_reg_reg); \/\/ XXX\n+%}\n+\n@@ -10464,0 +10502,1 @@\n+\n@@ -10466,1 +10505,1 @@\n-instruct rep_stos(rcx_RegL cnt, rdi_RegP base, regD tmp, rax_RegI zero,\n+instruct rep_stos(rcx_RegL cnt, rdi_RegP base, regD tmp, rax_RegL val,\n@@ -10469,3 +10508,120 @@\n-  predicate(!((ClearArrayNode*)n)->is_large() && (UseAVX <= 2));\n-  match(Set dummy (ClearArray cnt base));\n-  effect(USE_KILL cnt, USE_KILL base, TEMP tmp, KILL zero, KILL cr);\n+  predicate(!((ClearArrayNode*)n)->is_large() && !((ClearArrayNode*)n)->word_copy_only() && (UseAVX <= 2));\n+  match(Set dummy (ClearArray (Binary cnt base) val));\n+  effect(USE_KILL cnt, USE_KILL base, TEMP tmp, USE_KILL val, KILL cr);\n+\n+  format %{ $$template\n+    $$emit$$\"cmp     InitArrayShortSize,rcx\\n\\t\"\n+    $$emit$$\"jg      LARGE\\n\\t\"\n+    $$emit$$\"dec     rcx\\n\\t\"\n+    $$emit$$\"js      DONE\\t# Zero length\\n\\t\"\n+    $$emit$$\"mov     rax,(rdi,rcx,8)\\t# LOOP\\n\\t\"\n+    $$emit$$\"dec     rcx\\n\\t\"\n+    $$emit$$\"jge     LOOP\\n\\t\"\n+    $$emit$$\"jmp     DONE\\n\\t\"\n+    $$emit$$\"# LARGE:\\n\\t\"\n+    if (UseFastStosb) {\n+       $$emit$$\"shlq    rcx,3\\t# Convert doublewords to bytes\\n\\t\"\n+       $$emit$$\"rep     stosb\\t# Store rax to *rdi++ while rcx--\\n\\t\"\n+    } else if (UseXMMForObjInit) {\n+       $$emit$$\"movdq   $tmp, $val\\n\\t\"\n+       $$emit$$\"punpcklqdq $tmp, $tmp\\n\\t\"\n+       $$emit$$\"vinserti128_high $tmp, $tmp\\n\\t\"\n+       $$emit$$\"jmpq    L_zero_64_bytes\\n\\t\"\n+       $$emit$$\"# L_loop:\\t# 64-byte LOOP\\n\\t\"\n+       $$emit$$\"vmovdqu $tmp,(rax)\\n\\t\"\n+       $$emit$$\"vmovdqu $tmp,0x20(rax)\\n\\t\"\n+       $$emit$$\"add     0x40,rax\\n\\t\"\n+       $$emit$$\"# L_zero_64_bytes:\\n\\t\"\n+       $$emit$$\"sub     0x8,rcx\\n\\t\"\n+       $$emit$$\"jge     L_loop\\n\\t\"\n+       $$emit$$\"add     0x4,rcx\\n\\t\"\n+       $$emit$$\"jl      L_tail\\n\\t\"\n+       $$emit$$\"vmovdqu $tmp,(rax)\\n\\t\"\n+       $$emit$$\"add     0x20,rax\\n\\t\"\n+       $$emit$$\"sub     0x4,rcx\\n\\t\"\n+       $$emit$$\"# L_tail:\\t# Clearing tail bytes\\n\\t\"\n+       $$emit$$\"add     0x4,rcx\\n\\t\"\n+       $$emit$$\"jle     L_end\\n\\t\"\n+       $$emit$$\"dec     rcx\\n\\t\"\n+       $$emit$$\"# L_sloop:\\t# 8-byte short loop\\n\\t\"\n+       $$emit$$\"vmovq   xmm0,(rax)\\n\\t\"\n+       $$emit$$\"add     0x8,rax\\n\\t\"\n+       $$emit$$\"dec     rcx\\n\\t\"\n+       $$emit$$\"jge     L_sloop\\n\\t\"\n+       $$emit$$\"# L_end:\\n\\t\"\n+    } else {\n+       $$emit$$\"rep     stosq\\t# Store rax to *rdi++ while rcx--\\n\\t\"\n+    }\n+    $$emit$$\"# DONE\"\n+  %}\n+  ins_encode %{\n+    __ clear_mem($base$$Register, $cnt$$Register, $val$$Register,\n+                 $tmp$$XMMRegister, false, false);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct rep_stos_word_copy(rcx_RegL cnt, rdi_RegP base, regD tmp, rax_RegL val,\n+                            Universe dummy, rFlagsReg cr)\n+%{\n+  predicate(!((ClearArrayNode*)n)->is_large() && ((ClearArrayNode*)n)->word_copy_only() && (UseAVX <= 2));\n+  match(Set dummy (ClearArray (Binary cnt base) val));\n+  effect(USE_KILL cnt, USE_KILL base, TEMP tmp, USE_KILL val, KILL cr);\n+\n+  format %{ $$template\n+    $$emit$$\"cmp     InitArrayShortSize,rcx\\n\\t\"\n+    $$emit$$\"jg      LARGE\\n\\t\"\n+    $$emit$$\"dec     rcx\\n\\t\"\n+    $$emit$$\"js      DONE\\t# Zero length\\n\\t\"\n+    $$emit$$\"mov     rax,(rdi,rcx,8)\\t# LOOP\\n\\t\"\n+    $$emit$$\"dec     rcx\\n\\t\"\n+    $$emit$$\"jge     LOOP\\n\\t\"\n+    $$emit$$\"jmp     DONE\\n\\t\"\n+    $$emit$$\"# LARGE:\\n\\t\"\n+    if (UseXMMForObjInit) {\n+       $$emit$$\"movdq   $tmp, $val\\n\\t\"\n+       $$emit$$\"punpcklqdq $tmp, $tmp\\n\\t\"\n+       $$emit$$\"vinserti128_high $tmp, $tmp\\n\\t\"\n+       $$emit$$\"jmpq    L_zero_64_bytes\\n\\t\"\n+       $$emit$$\"# L_loop:\\t# 64-byte LOOP\\n\\t\"\n+       $$emit$$\"vmovdqu $tmp,(rax)\\n\\t\"\n+       $$emit$$\"vmovdqu $tmp,0x20(rax)\\n\\t\"\n+       $$emit$$\"add     0x40,rax\\n\\t\"\n+       $$emit$$\"# L_zero_64_bytes:\\n\\t\"\n+       $$emit$$\"sub     0x8,rcx\\n\\t\"\n+       $$emit$$\"jge     L_loop\\n\\t\"\n+       $$emit$$\"add     0x4,rcx\\n\\t\"\n+       $$emit$$\"jl      L_tail\\n\\t\"\n+       $$emit$$\"vmovdqu $tmp,(rax)\\n\\t\"\n+       $$emit$$\"add     0x20,rax\\n\\t\"\n+       $$emit$$\"sub     0x4,rcx\\n\\t\"\n+       $$emit$$\"# L_tail:\\t# Clearing tail bytes\\n\\t\"\n+       $$emit$$\"add     0x4,rcx\\n\\t\"\n+       $$emit$$\"jle     L_end\\n\\t\"\n+       $$emit$$\"dec     rcx\\n\\t\"\n+       $$emit$$\"# L_sloop:\\t# 8-byte short loop\\n\\t\"\n+       $$emit$$\"vmovq   xmm0,(rax)\\n\\t\"\n+       $$emit$$\"add     0x8,rax\\n\\t\"\n+       $$emit$$\"dec     rcx\\n\\t\"\n+       $$emit$$\"jge     L_sloop\\n\\t\"\n+       $$emit$$\"# L_end:\\n\\t\"\n+    } else {\n+       $$emit$$\"rep     stosq\\t# Store rax to *rdi++ while rcx--\\n\\t\"\n+    }\n+    $$emit$$\"# DONE\"\n+  %}\n+  ins_encode %{\n+    __ clear_mem($base$$Register, $cnt$$Register, $val$$Register,\n+                 $tmp$$XMMRegister, false, true);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ Small non-constant length ClearArray for AVX512 targets.\n+instruct rep_stos_evex(rcx_RegL cnt, rdi_RegP base, legRegD tmp, kReg ktmp, rax_RegL val,\n+                       Universe dummy, rFlagsReg cr)\n+%{\n+  predicate(!((ClearArrayNode*)n)->is_large() && !((ClearArrayNode*)n)->word_copy_only() && (UseAVX > 2));\n+  match(Set dummy (ClearArray (Binary cnt base) val));\n+  ins_cost(125);\n+  effect(USE_KILL cnt, USE_KILL base, TEMP tmp, TEMP ktmp, USE_KILL val, KILL cr);\n@@ -10519,2 +10675,2 @@\n-    __ clear_mem($base$$Register, $cnt$$Register, $zero$$Register,\n-                 $tmp$$XMMRegister, false, knoreg);\n+    __ clear_mem($base$$Register, $cnt$$Register, $val$$Register,\n+                 $tmp$$XMMRegister, false, false, $ktmp$$KRegister);\n@@ -10525,3 +10681,2 @@\n-\/\/ Small non-constant length ClearArray for AVX512 targets.\n-instruct rep_stos_evex(rcx_RegL cnt, rdi_RegP base, legRegD tmp, kReg ktmp, rax_RegI zero,\n-                       Universe dummy, rFlagsReg cr)\n+instruct rep_stos_evex_word_copy(rcx_RegL cnt, rdi_RegP base, legRegD tmp, kReg ktmp, rax_RegL val,\n+                                 Universe dummy, rFlagsReg cr)\n@@ -10529,2 +10684,2 @@\n-  predicate(!((ClearArrayNode*)n)->is_large() && (UseAVX > 2));\n-  match(Set dummy (ClearArray cnt base));\n+  predicate(!((ClearArrayNode*)n)->is_large() && ((ClearArrayNode*)n)->word_copy_only() && (UseAVX > 2));\n+  match(Set dummy (ClearArray (Binary cnt base) val));\n@@ -10532,1 +10687,1 @@\n-  effect(USE_KILL cnt, USE_KILL base, TEMP tmp, TEMP ktmp, KILL zero, KILL cr);\n+  effect(USE_KILL cnt, USE_KILL base, TEMP tmp, TEMP ktmp, USE_KILL val, KILL cr);\n@@ -10580,2 +10735,2 @@\n-    __ clear_mem($base$$Register, $cnt$$Register, $zero$$Register,\n-                 $tmp$$XMMRegister, false, $ktmp$$KRegister);\n+    __ clear_mem($base$$Register, $cnt$$Register, $val$$Register,\n+                 $tmp$$XMMRegister, false, true, $ktmp$$KRegister);\n@@ -10587,1 +10742,1 @@\n-instruct rep_stos_large(rcx_RegL cnt, rdi_RegP base, regD tmp, rax_RegI zero,\n+instruct rep_stos_large(rcx_RegL cnt, rdi_RegP base, regD tmp, rax_RegL val,\n@@ -10590,3 +10745,99 @@\n-  predicate((UseAVX <=2) && ((ClearArrayNode*)n)->is_large());\n-  match(Set dummy (ClearArray cnt base));\n-  effect(USE_KILL cnt, USE_KILL base, TEMP tmp, KILL zero, KILL cr);\n+  predicate(((ClearArrayNode*)n)->is_large() && !((ClearArrayNode*)n)->word_copy_only() && (UseAVX <= 2));\n+  match(Set dummy (ClearArray (Binary cnt base) val));\n+  effect(USE_KILL cnt, USE_KILL base, TEMP tmp, USE_KILL val, KILL cr);\n+\n+  format %{ $$template\n+    if (UseFastStosb) {\n+       $$emit$$\"shlq    rcx,3\\t# Convert doublewords to bytes\\n\\t\"\n+       $$emit$$\"rep     stosb\\t# Store rax to *rdi++ while rcx--\"\n+    } else if (UseXMMForObjInit) {\n+       $$emit$$\"movdq   $tmp, $val\\n\\t\"\n+       $$emit$$\"punpcklqdq $tmp, $tmp\\n\\t\"\n+       $$emit$$\"vinserti128_high $tmp, $tmp\\n\\t\"\n+       $$emit$$\"jmpq    L_zero_64_bytes\\n\\t\"\n+       $$emit$$\"# L_loop:\\t# 64-byte LOOP\\n\\t\"\n+       $$emit$$\"vmovdqu $tmp,(rax)\\n\\t\"\n+       $$emit$$\"vmovdqu $tmp,0x20(rax)\\n\\t\"\n+       $$emit$$\"add     0x40,rax\\n\\t\"\n+       $$emit$$\"# L_zero_64_bytes:\\n\\t\"\n+       $$emit$$\"sub     0x8,rcx\\n\\t\"\n+       $$emit$$\"jge     L_loop\\n\\t\"\n+       $$emit$$\"add     0x4,rcx\\n\\t\"\n+       $$emit$$\"jl      L_tail\\n\\t\"\n+       $$emit$$\"vmovdqu $tmp,(rax)\\n\\t\"\n+       $$emit$$\"add     0x20,rax\\n\\t\"\n+       $$emit$$\"sub     0x4,rcx\\n\\t\"\n+       $$emit$$\"# L_tail:\\t# Clearing tail bytes\\n\\t\"\n+       $$emit$$\"add     0x4,rcx\\n\\t\"\n+       $$emit$$\"jle     L_end\\n\\t\"\n+       $$emit$$\"dec     rcx\\n\\t\"\n+       $$emit$$\"# L_sloop:\\t# 8-byte short loop\\n\\t\"\n+       $$emit$$\"vmovq   xmm0,(rax)\\n\\t\"\n+       $$emit$$\"add     0x8,rax\\n\\t\"\n+       $$emit$$\"dec     rcx\\n\\t\"\n+       $$emit$$\"jge     L_sloop\\n\\t\"\n+       $$emit$$\"# L_end:\\n\\t\"\n+    } else {\n+       $$emit$$\"rep     stosq\\t# Store rax to *rdi++ while rcx--\"\n+    }\n+  %}\n+  ins_encode %{\n+    __ clear_mem($base$$Register, $cnt$$Register, $val$$Register,\n+                 $tmp$$XMMRegister, true, false);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct rep_stos_large_word_copy(rcx_RegL cnt, rdi_RegP base, regD tmp, rax_RegL val,\n+                                  Universe dummy, rFlagsReg cr)\n+%{\n+  predicate(((ClearArrayNode*)n)->is_large() && ((ClearArrayNode*)n)->word_copy_only() && (UseAVX <= 2));\n+  match(Set dummy (ClearArray (Binary cnt base) val));\n+  effect(USE_KILL cnt, USE_KILL base, TEMP tmp, USE_KILL val, KILL cr);\n+\n+  format %{ $$template\n+    if (UseXMMForObjInit) {\n+       $$emit$$\"movdq   $tmp, $val\\n\\t\"\n+       $$emit$$\"punpcklqdq $tmp, $tmp\\n\\t\"\n+       $$emit$$\"vinserti128_high $tmp, $tmp\\n\\t\"\n+       $$emit$$\"jmpq    L_zero_64_bytes\\n\\t\"\n+       $$emit$$\"# L_loop:\\t# 64-byte LOOP\\n\\t\"\n+       $$emit$$\"vmovdqu $tmp,(rax)\\n\\t\"\n+       $$emit$$\"vmovdqu $tmp,0x20(rax)\\n\\t\"\n+       $$emit$$\"add     0x40,rax\\n\\t\"\n+       $$emit$$\"# L_zero_64_bytes:\\n\\t\"\n+       $$emit$$\"sub     0x8,rcx\\n\\t\"\n+       $$emit$$\"jge     L_loop\\n\\t\"\n+       $$emit$$\"add     0x4,rcx\\n\\t\"\n+       $$emit$$\"jl      L_tail\\n\\t\"\n+       $$emit$$\"vmovdqu $tmp,(rax)\\n\\t\"\n+       $$emit$$\"add     0x20,rax\\n\\t\"\n+       $$emit$$\"sub     0x4,rcx\\n\\t\"\n+       $$emit$$\"# L_tail:\\t# Clearing tail bytes\\n\\t\"\n+       $$emit$$\"add     0x4,rcx\\n\\t\"\n+       $$emit$$\"jle     L_end\\n\\t\"\n+       $$emit$$\"dec     rcx\\n\\t\"\n+       $$emit$$\"# L_sloop:\\t# 8-byte short loop\\n\\t\"\n+       $$emit$$\"vmovq   xmm0,(rax)\\n\\t\"\n+       $$emit$$\"add     0x8,rax\\n\\t\"\n+       $$emit$$\"dec     rcx\\n\\t\"\n+       $$emit$$\"jge     L_sloop\\n\\t\"\n+       $$emit$$\"# L_end:\\n\\t\"\n+    } else {\n+       $$emit$$\"rep     stosq\\t# Store rax to *rdi++ while rcx--\"\n+    }\n+  %}\n+  ins_encode %{\n+    __ clear_mem($base$$Register, $cnt$$Register, $val$$Register,\n+                 $tmp$$XMMRegister, true, true);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ Large non-constant length ClearArray for AVX512 targets.\n+instruct rep_stos_large_evex(rcx_RegL cnt, rdi_RegP base, legRegD tmp, kReg ktmp, rax_RegL val,\n+                             Universe dummy, rFlagsReg cr)\n+%{\n+  predicate(((ClearArrayNode*)n)->is_large() && !((ClearArrayNode*)n)->word_copy_only() && (UseAVX > 2));\n+  match(Set dummy (ClearArray (Binary cnt base) val));\n+  effect(USE_KILL cnt, USE_KILL base, TEMP tmp, TEMP ktmp, USE_KILL val, KILL cr);\n@@ -10631,2 +10882,2 @@\n-    __ clear_mem($base$$Register, $cnt$$Register, $zero$$Register,\n-                 $tmp$$XMMRegister, true, knoreg);\n+    __ clear_mem($base$$Register, $cnt$$Register, $val$$Register,\n+                 $tmp$$XMMRegister, true, false, $ktmp$$KRegister);\n@@ -10637,3 +10888,2 @@\n-\/\/ Large non-constant length ClearArray for AVX512 targets.\n-instruct rep_stos_large_evex(rcx_RegL cnt, rdi_RegP base, legRegD tmp, kReg ktmp, rax_RegI zero,\n-                             Universe dummy, rFlagsReg cr)\n+instruct rep_stos_large_evex_word_copy(rcx_RegL cnt, rdi_RegP base, legRegD tmp, kReg ktmp, rax_RegL val,\n+                                       Universe dummy, rFlagsReg cr)\n@@ -10641,3 +10891,3 @@\n-  predicate((UseAVX > 2) && ((ClearArrayNode*)n)->is_large());\n-  match(Set dummy (ClearArray cnt base));\n-  effect(USE_KILL cnt, USE_KILL base, TEMP tmp, TEMP ktmp, KILL zero, KILL cr);\n+  predicate(((ClearArrayNode*)n)->is_large() && ((ClearArrayNode*)n)->word_copy_only() && (UseAVX > 2));\n+  match(Set dummy (ClearArray (Binary cnt base) val));\n+  effect(USE_KILL cnt, USE_KILL base, TEMP tmp, TEMP ktmp, USE_KILL val, KILL cr);\n@@ -10682,2 +10932,2 @@\n-    __ clear_mem($base$$Register, $cnt$$Register, $zero$$Register,\n-                 $tmp$$XMMRegister, true, $ktmp$$KRegister);\n+    __ clear_mem($base$$Register, $cnt$$Register, $val$$Register,\n+                 $tmp$$XMMRegister, true, true, $ktmp$$KRegister);\n@@ -10689,1 +10939,1 @@\n-instruct rep_stos_im(immL cnt, rRegP base, regD tmp, rRegI zero, kReg ktmp, Universe dummy, rFlagsReg cr)\n+instruct rep_stos_im(immL cnt, rRegP base, regD tmp, rax_RegL val, kReg ktmp, Universe dummy, rFlagsReg cr)\n@@ -10691,2 +10941,3 @@\n-  predicate(!((ClearArrayNode*)n)->is_large() && (MaxVectorSize >= 32) && VM_Version::supports_avx512vl());\n-  match(Set dummy (ClearArray cnt base));\n+  predicate(!((ClearArrayNode*)n)->is_large() && !((ClearArrayNode*)n)->word_copy_only() &&\n+            ((MaxVectorSize >= 32) && VM_Version::supports_avx512vl()));\n+  match(Set dummy (ClearArray (Binary cnt base) val));\n@@ -10694,1 +10945,1 @@\n-  effect(TEMP tmp, TEMP zero, TEMP ktmp, KILL cr);\n+  effect(TEMP tmp, USE_KILL val, TEMP ktmp, KILL cr);\n@@ -10697,1 +10948,1 @@\n-   __ clear_mem($base$$Register, $cnt$$constant, $zero$$Register, $tmp$$XMMRegister, $ktmp$$KRegister);\n+    __ clear_mem($base$$Register, $cnt$$constant, $val$$Register, $tmp$$XMMRegister, $ktmp$$KRegister);\n@@ -12498,0 +12749,15 @@\n+\/\/ entry point is null, target holds the address to call\n+instruct CallLeafNoFPInDirect(rRegP target)\n+%{\n+  predicate(n->as_Call()->entry_point() == nullptr);\n+  match(CallLeafNoFP target);\n+\n+  ins_cost(300);\n+  format %{ \"call_leaf_nofp,runtime indirect \" %}\n+  ins_encode %{\n+     __ call($target$$Register);\n+  %}\n+\n+  ins_pipe(pipe_slow);\n+%}\n+\n@@ -12500,0 +12766,1 @@\n+  predicate(n->as_Call()->entry_point() != nullptr);\n","filename":"src\/hotspot\/cpu\/x86\/x86_64.ad","additions":349,"deletions":82,"binary":false,"changes":431,"status":"modified"},{"patch":"@@ -811,1 +811,1 @@\n-  return  false;\n+  return false;\n@@ -902,1 +902,2 @@\n-      strcmp(_matrule->_opType,\"Halt\"      )==0 )\n+      strcmp(_matrule->_opType,\"Halt\"      )==0 ||\n+      strcmp(_matrule->_opType,\"CallLeafNoFP\")==0)\n","filename":"src\/hotspot\/share\/adlc\/formssel.cpp","additions":3,"deletions":2,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -56,0 +56,2 @@\n+#include \"oops\/flatArrayKlass.hpp\"\n+#include \"oops\/flatArrayOop.inline.hpp\"\n@@ -128,0 +130,1 @@\n+uint Runtime1::_new_null_free_array_slowcase_cnt = 0;\n@@ -130,0 +133,5 @@\n+uint Runtime1::_load_flat_array_slowcase_cnt = 0;\n+uint Runtime1::_store_flat_array_slowcase_cnt = 0;\n+uint Runtime1::_substitutability_check_slowcase_cnt = 0;\n+uint Runtime1::_buffer_inline_args_slowcase_cnt = 0;\n+uint Runtime1::_buffer_inline_args_no_receiver_slowcase_cnt = 0;\n@@ -139,0 +147,2 @@\n+uint Runtime1::_throw_illegal_monitor_state_exception_count = 0;\n+uint Runtime1::_throw_identity_exception_count = 0;\n@@ -358,2 +368,1 @@\n-\n-JRT_ENTRY(void, Runtime1::new_instance(JavaThread* current, Klass* klass))\n+static void allocate_instance(JavaThread* current, Klass* klass, TRAPS) {\n@@ -362,1 +371,1 @@\n-    _new_instance_slowcase_cnt++;\n+    Runtime1::_new_instance_slowcase_cnt++;\n@@ -371,2 +380,8 @@\n-  \/\/ allocate instance and return via TLS\n-  oop obj = h->allocate_instance(CHECK);\n+  oop obj = nullptr;\n+  if (h->is_inline_klass() &&  InlineKlass::cast(h)->is_empty_inline_type()) {\n+    obj = InlineKlass::cast(h)->default_value();\n+    assert(obj != nullptr, \"default value must exist\");\n+  } else {\n+    \/\/ allocate instance and return via TLS\n+    obj = h->allocate_instance(CHECK);\n+  }\n@@ -376,0 +391,3 @@\n+JRT_ENTRY(void, Runtime1::new_instance(JavaThread* current, Klass* klass))\n+  allocate_instance(current, klass, CHECK);\n+JRT_END\n@@ -410,1 +428,1 @@\n-  Klass* elem_klass = ObjArrayKlass::cast(array_klass)->element_klass();\n+  Klass* elem_klass = ArrayKlass::cast(array_klass)->element_klass();\n@@ -421,0 +439,22 @@\n+JRT_ENTRY(void, Runtime1::new_null_free_array(JavaThread* current, Klass* array_klass, jint length))\n+  NOT_PRODUCT(_new_null_free_array_slowcase_cnt++;)\n+\n+  \/\/ Note: no handle for klass needed since they are not used\n+  \/\/       anymore after new_objArray() and no GC can happen before.\n+  \/\/       (This may have to change if this code changes!)\n+  assert(array_klass->is_klass(), \"not a class\");\n+  Handle holder(THREAD, array_klass->klass_holder()); \/\/ keep the klass alive\n+  Klass* elem_klass = ArrayKlass::cast(array_klass)->element_klass();\n+  assert(elem_klass->is_inline_klass(), \"must be\");\n+  \/\/ Logically creates elements, ensure klass init\n+  elem_klass->initialize(CHECK);\n+  arrayOop obj = oopFactory::new_valueArray(elem_klass, length, CHECK);\n+  current->set_vm_result(obj);\n+  \/\/ This is pretty rare but this runtime patch is stressful to deoptimization\n+  \/\/ if we deoptimize here so force a deopt to stress the path.\n+  if (DeoptimizeALot) {\n+    deopt_caller(current);\n+  }\n+JRT_END\n+\n+\n@@ -435,0 +475,92 @@\n+static void profile_flat_array(JavaThread* current, bool load) {\n+  ResourceMark rm(current);\n+  vframeStream vfst(current, true);\n+  assert(!vfst.at_end(), \"Java frame must exist\");\n+  \/\/ Check if array access profiling is enabled\n+  if (vfst.nm()->comp_level() != CompLevel_full_profile || !C1UpdateMethodData) {\n+    return;\n+  }\n+  int bci = vfst.bci();\n+  Method* method = vfst.method();\n+  MethodData* md = method->method_data();\n+  if (md != nullptr) {\n+    \/\/ Lock to access ProfileData, and ensure lock is not broken by a safepoint\n+    MutexLocker ml(md->extra_data_lock(), Mutex::_no_safepoint_check_flag);\n+\n+    ProfileData* data = md->bci_to_data(bci);\n+    assert(data != nullptr, \"incorrect profiling entry\");\n+    if (data->is_ArrayLoadData()) {\n+      assert(load, \"should be an array load\");\n+      ArrayLoadData* load_data = (ArrayLoadData*) data;\n+      load_data->set_flat_array();\n+    } else {\n+      assert(data->is_ArrayStoreData(), \"\");\n+      assert(!load, \"should be an array store\");\n+      ArrayStoreData* store_data = (ArrayStoreData*) data;\n+      store_data->set_flat_array();\n+    }\n+  }\n+}\n+\n+JRT_ENTRY(void, Runtime1::load_flat_array(JavaThread* current, flatArrayOopDesc* array, int index))\n+  assert(array->klass()->is_flatArray_klass(), \"should not be called\");\n+  profile_flat_array(current, true);\n+\n+  NOT_PRODUCT(_load_flat_array_slowcase_cnt++;)\n+  assert(array->length() > 0 && index < array->length(), \"already checked\");\n+  flatArrayHandle vah(current, array);\n+  oop obj = flatArrayOopDesc::value_alloc_copy_from_index(vah, index, CHECK);\n+  current->set_vm_result(obj);\n+JRT_END\n+\n+\n+JRT_ENTRY(void, Runtime1::store_flat_array(JavaThread* current, flatArrayOopDesc* array, int index, oopDesc* value))\n+  if (array->klass()->is_flatArray_klass()) {\n+    profile_flat_array(current, false);\n+  }\n+\n+  NOT_PRODUCT(_store_flat_array_slowcase_cnt++;)\n+  if (value == nullptr) {\n+    assert(array->klass()->is_flatArray_klass() || array->klass()->is_null_free_array_klass(), \"should not be called\");\n+    SharedRuntime::throw_and_post_jvmti_exception(current, vmSymbols::java_lang_NullPointerException());\n+  } else {\n+    assert(array->klass()->is_flatArray_klass(), \"should not be called\");\n+    array->value_copy_to_index(value, index, LayoutKind::PAYLOAD); \/\/ Non atomic is currently the only layout supported by flat arrays\n+  }\n+JRT_END\n+\n+\n+JRT_ENTRY(int, Runtime1::substitutability_check(JavaThread* current, oopDesc* left, oopDesc* right))\n+  NOT_PRODUCT(_substitutability_check_slowcase_cnt++;)\n+  JavaCallArguments args;\n+  args.push_oop(Handle(THREAD, left));\n+  args.push_oop(Handle(THREAD, right));\n+  JavaValue result(T_BOOLEAN);\n+  JavaCalls::call_static(&result,\n+                         vmClasses::ValueObjectMethods_klass(),\n+                         vmSymbols::isSubstitutable_name(),\n+                         vmSymbols::object_object_boolean_signature(),\n+                         &args, CHECK_0);\n+  return result.get_jboolean() ? 1 : 0;\n+JRT_END\n+\n+\n+extern \"C\" void ps();\n+\n+void Runtime1::buffer_inline_args_impl(JavaThread* current, Method* m, bool allocate_receiver) {\n+  JavaThread* THREAD = current;\n+  methodHandle method(current, m); \/\/ We are inside the verified_entry or verified_inline_ro_entry of this method.\n+  oop obj = SharedRuntime::allocate_inline_types_impl(current, method, allocate_receiver, CHECK);\n+  current->set_vm_result(obj);\n+}\n+\n+JRT_ENTRY(void, Runtime1::buffer_inline_args(JavaThread* current, Method* method))\n+  NOT_PRODUCT(_buffer_inline_args_slowcase_cnt++;)\n+  buffer_inline_args_impl(current, method, true);\n+JRT_END\n+\n+JRT_ENTRY(void, Runtime1::buffer_inline_args_no_receiver(JavaThread* current, Method* method))\n+  NOT_PRODUCT(_buffer_inline_args_no_receiver_slowcase_cnt++;)\n+  buffer_inline_args_impl(current, method, false);\n+JRT_END\n+\n@@ -758,0 +890,13 @@\n+JRT_ENTRY(void, Runtime1::throw_illegal_monitor_state_exception(JavaThread* current))\n+  NOT_PRODUCT(_throw_illegal_monitor_state_exception_count++;)\n+  ResourceMark rm(current);\n+  SharedRuntime::throw_and_post_jvmti_exception(current, vmSymbols::java_lang_IllegalMonitorStateException());\n+JRT_END\n+\n+JRT_ENTRY(void, Runtime1::throw_identity_exception(JavaThread* current, oopDesc* object))\n+  NOT_PRODUCT(_throw_identity_exception_count++;)\n+  ResourceMark rm(current);\n+  char* message = SharedRuntime::generate_identity_exception_message(current, object->klass());\n+  SharedRuntime::throw_and_post_jvmti_exception(current, vmSymbols::java_lang_IdentityException(), message);\n+JRT_END\n+\n@@ -963,0 +1108,2 @@\n+  bool deoptimize_for_null_free = false;\n+  bool deoptimize_for_flat = false;\n@@ -1006,0 +1153,10 @@\n+    \/\/ The field we are patching is null-free. Deoptimize and regenerate\n+    \/\/ the compiled code if we patch a putfield\/putstatic because it\n+    \/\/ does not contain the required null check.\n+    deoptimize_for_null_free = result.is_null_free_inline_type() && (field_access.is_putfield() || field_access.is_putstatic());\n+\n+    \/\/ The field we are patching is flat. Deoptimize and regenerate\n+    \/\/ the compiled code which can't handle the layout of the flat\n+    \/\/ field because it was unknown at compile time.\n+    deoptimize_for_flat = result.is_flat();\n+\n@@ -1078,1 +1235,1 @@\n-  if (deoptimize_for_volatile || deoptimize_for_atomic) {\n+  if (deoptimize_for_volatile || deoptimize_for_atomic || deoptimize_for_null_free || deoptimize_for_flat) {\n@@ -1089,0 +1246,6 @@\n+      if (deoptimize_for_null_free) {\n+        tty->print_cr(\"Deoptimizing for patching null-free field reference\");\n+      }\n+      if (deoptimize_for_flat) {\n+        tty->print_cr(\"Deoptimizing for patching flat field reference\");\n+      }\n@@ -1539,0 +1702,1 @@\n+  tty->print_cr(\" _new_null_free_array_slowcase_cnt: %u\", _new_null_free_array_slowcase_cnt);\n@@ -1541,0 +1705,6 @@\n+  tty->print_cr(\" _load_flat_array_slowcase_cnt:   %u\", _load_flat_array_slowcase_cnt);\n+  tty->print_cr(\" _store_flat_array_slowcase_cnt:  %u\", _store_flat_array_slowcase_cnt);\n+  tty->print_cr(\" _substitutability_check_slowcase_cnt: %u\", _substitutability_check_slowcase_cnt);\n+  tty->print_cr(\" _buffer_inline_args_slowcase_cnt:%u\", _buffer_inline_args_slowcase_cnt);\n+  tty->print_cr(\" _buffer_inline_args_no_receiver_slowcase_cnt:%u\", _buffer_inline_args_no_receiver_slowcase_cnt);\n+\n@@ -1551,0 +1721,2 @@\n+  tty->print_cr(\" _throw_illegal_monitor_state_exception_count:  %u:\", _throw_illegal_monitor_state_exception_count);\n+  tty->print_cr(\" _throw_identity_exception_count:               %u:\", _throw_identity_exception_count);\n","filename":"src\/hotspot\/share\/c1\/c1_Runtime1.cpp","additions":179,"deletions":7,"binary":false,"changes":186,"status":"modified"},{"patch":"@@ -32,0 +32,2 @@\n+#include \"oops\/inlineKlass.hpp\"\n+#include \"oops\/instanceKlass.hpp\"\n@@ -36,1 +38,0 @@\n-\n@@ -53,0 +54,3 @@\n+\n+#define MAX_ATOMIC_OP_SIZE sizeof(uint64_t)\n+\n@@ -57,6 +61,7 @@\n-    EMPTY,         \/\/ empty slot, space is taken from this to allocate fields\n-    RESERVED,      \/\/ reserved for JVM usage (for instance object header)\n-    PADDING,       \/\/ padding (because of alignment constraints or @Contended)\n-    REGULAR,       \/\/ primitive or oop field (including non-flattened inline fields)\n-    FLATTENED,     \/\/ flattened field\n-    INHERITED      \/\/ field(s) inherited from super classes\n+    EMPTY,                 \/\/ empty slot, space is taken from this to allocate fields\n+    RESERVED,              \/\/ reserved for JVM usage (for instance object header)\n+    PADDING,               \/\/ padding (because of alignment constraints or @Contended)\n+    REGULAR,               \/\/ primitive or oop field (including not flat inline type fields)\n+    FLAT,                  \/\/ flat field\n+    INHERITED,             \/\/ field(s) inherited from super classes\n+    NULL_MARKER            \/\/ stores the null marker for a flat field\n@@ -68,1 +73,3 @@\n-  Kind _kind;\n+  InlineKlass* _inline_klass;\n+  Kind _block_kind;\n+  LayoutKind _layout_kind;\n@@ -73,1 +80,1 @@\n-  bool _is_reference;\n+  int _null_marker_offset;\n@@ -77,1 +84,2 @@\n-  LayoutRawBlock(int index, Kind kind, int size, int alignment, bool is_reference = false);\n+\n+  LayoutRawBlock(int index, Kind kind, int size, int alignment);\n@@ -82,1 +90,2 @@\n-  Kind kind() const { return _kind; }\n+  Kind block_kind() const { return _block_kind; }\n+  void set_block_kind(LayoutRawBlock::Kind kind) { _block_kind = kind; } \/\/ Dangerous operation, is only used by remove_null_marker();\n@@ -95,1 +104,14 @@\n-  bool is_reference() const { return _is_reference; }\n+  void set_field_index(int field_index) {\n+    assert(_field_index == -1, \"Must not be initialized\");\n+    _field_index = field_index;\n+  }\n+  InlineKlass* inline_klass() const {\n+    assert(_inline_klass != nullptr, \"Must be initialized\");\n+    return _inline_klass;\n+  }\n+  void set_inline_klass(InlineKlass* inline_klass) { _inline_klass = inline_klass; }\n+  void set_null_marker_offset(int offset) { _null_marker_offset = offset; }\n+  int null_marker_offset() const { return _null_marker_offset; }\n+\n+  LayoutKind layout_kind() const { return _layout_kind; }\n+  void set_layout_kind(LayoutKind kind) { _layout_kind = kind; }\n@@ -111,1 +133,0 @@\n-\n@@ -118,1 +139,1 @@\n-\/\/ oop, or flattened.\n+\/\/ oop, or flat.\n@@ -124,1 +145,3 @@\n-  GrowableArray<LayoutRawBlock*>* _primitive_fields;\n+\n+  GrowableArray<LayoutRawBlock*>* _small_primitive_fields;\n+  GrowableArray<LayoutRawBlock*>* _big_primitive_fields;\n@@ -135,1 +158,2 @@\n-  GrowableArray<LayoutRawBlock*>* primitive_fields() const { return _primitive_fields; }\n+  GrowableArray<LayoutRawBlock*>* small_primitive_fields() const { return _small_primitive_fields; }\n+  GrowableArray<LayoutRawBlock*>* big_primitive_fields() const { return _big_primitive_fields; }\n@@ -142,0 +166,2 @@\n+  void add_flat_field(int idx, InlineKlass* vk, LayoutKind lk, int size, int alignment);\n+  void add_block(LayoutRawBlock** list, LayoutRawBlock* block);\n@@ -143,0 +169,3 @@\n+ private:\n+  void add_to_small_primitive_list(LayoutRawBlock* block);\n+  void add_to_big_primitive_list(LayoutRawBlock* block);\n@@ -164,0 +193,1 @@\n+  Array<InlineLayoutInfo>* _inline_layout_info_array;\n@@ -168,0 +198,7 @@\n+  int _super_first_field_offset;\n+  int _super_alignment;\n+  int _super_min_align_required;\n+  int _default_value_offset;  \/\/ offset of the default value in class mirror, only for static layout of inline classes\n+  int _null_reset_value_offset;    \/\/ offset of the reset value in class mirror, only for static layout of inline classes\n+  bool _super_has_fields;\n+  bool _has_inherited_fields;\n@@ -170,1 +207,1 @@\n-  FieldLayout(GrowableArray<FieldInfo>* field_info, ConstantPool* cp);\n+  FieldLayout(GrowableArray<FieldInfo>* field_info, Array<InlineLayoutInfo>* inline_layout_info_array, ConstantPool* cp);\n@@ -176,1 +213,1 @@\n-    while (block->kind() != LayoutRawBlock::EMPTY) {\n+    while (block->block_kind() != LayoutRawBlock::EMPTY) {\n@@ -182,1 +219,3 @@\n-  LayoutRawBlock* start() { return _start; }\n+  LayoutRawBlock* blocks() const { return _blocks; }\n+\n+  LayoutRawBlock* start() const { return _start; }\n@@ -184,1 +223,14 @@\n-  LayoutRawBlock* last_block() { return _last; }\n+  LayoutRawBlock* last_block() const  { return _last; }\n+  int super_first_field_offset() const { return _super_first_field_offset; }\n+  int super_alignment() const { return _super_alignment; }\n+  int super_min_align_required() const { return _super_min_align_required; }\n+  int default_value_offset() const {\n+    assert(_default_value_offset != -1, \"Must have been set\");\n+    return _default_value_offset;\n+  }\n+  int null_reset_value_offset() const {\n+    assert(_null_reset_value_offset != -1, \"Must have been set\");\n+    return _null_reset_value_offset;\n+  }\n+  bool super_has_fields() const { return _super_has_fields; }\n+  bool has_inherited_fields() const { return _has_inherited_fields; }\n@@ -195,1 +247,4 @@\n-  void print(outputStream* output, bool is_static, const InstanceKlass* super);\n+  void shift_fields(int shift);\n+  LayoutRawBlock* find_null_marker();\n+  void remove_null_marker();\n+  void print(outputStream* output, bool is_static, const InstanceKlass* super, Array<InlineLayoutInfo>* inline_fields);\n@@ -200,5 +255,4 @@\n-\/\/ This class has three methods to generate layout: one for regular classes\n-\/\/ and two for classes with hard coded offsets (java,lang.ref.Reference\n-\/\/ and the boxing classes). The rationale for having multiple methods\n-\/\/ is that each kind of class has a different set goals regarding\n-\/\/ its layout, so instead of mixing several layout strategies into a\n+\/\/ This class has two methods to generate layout: one for identity classes\n+\/\/ and one for inline classes. The rational for having two methods\n+\/\/ is that each kind of classes has a different set goals regarding\n+\/\/ its layout, so instead of mixing two layout strategies into a\n@@ -221,1 +275,1 @@\n-\/\/  can vary with the allocation strategy.\n+\/\/  differ for inline classes and identity classes.\n@@ -224,1 +278,1 @@\n- private:\n+ private:\n@@ -227,0 +281,1 @@\n+  ClassLoaderData* _loader_data;\n@@ -231,0 +286,1 @@\n+  Array<InlineLayoutInfo>* _inline_layout_info_array;\n@@ -237,1 +293,13 @@\n-  int _alignment;\n+  int _payload_alignment;\n+  int _first_field_offset;\n+  int _null_marker_offset; \/\/ if any, -1 means no internal null marker\n+  int _payload_size_in_bytes;\n+  int _non_atomic_layout_size_in_bytes;\n+  int _non_atomic_layout_alignment;\n+  int _atomic_layout_size_in_bytes;\n+  int _nullable_layout_size_in_bytes;\n+  int _fields_size_sum;\n+  int _declared_non_static_fields_count;\n+  bool _has_non_naturally_atomic_fields;\n+  bool _is_naturally_atomic;\n+  bool _must_be_atomic;\n@@ -239,1 +307,8 @@\n-  bool _is_contended; \/\/ is a contended class?\n+  bool _has_inline_type_fields;\n+  bool _is_contended;\n+  bool _is_inline_type;\n+  bool _is_abstract_value;\n+  bool _has_flattening_information;\n+  bool _is_empty_inline_class;\n+\n+  FieldGroup* get_or_create_contended_group(int g);\n@@ -242,2 +317,3 @@\n-  FieldLayoutBuilder(const Symbol* classname, const InstanceKlass* super_klass, ConstantPool* constant_pool,\n-                     GrowableArray<FieldInfo>* field_info, bool is_contended, FieldLayoutInfo* info);\n+  FieldLayoutBuilder(const Symbol* classname, ClassLoaderData* loader_data, const InstanceKlass* super_klass, ConstantPool* constant_pool,\n+                     GrowableArray<FieldInfo>* field_info, bool is_contended, bool is_inline_type, bool is_abstract_value,\n+                     bool must_be_atomic, FieldLayoutInfo* info, Array<InlineLayoutInfo>* inline_layout_info_array);\n@@ -245,4 +321,12 @@\n-  int get_alignment() {\n-    assert(_alignment != -1, \"Uninitialized\");\n-    return _alignment;\n-  }\n+  int first_field_offset() const               { assert(_first_field_offset != -1, \"Uninitialized\"); return _first_field_offset; }\n+  int  payload_layout_size_in_bytes() const    { return _payload_size_in_bytes; }\n+  int  payload_layout_alignment() const        { assert(_payload_alignment != -1, \"Uninitialized\"); return _payload_alignment; }\n+  bool has_non_atomic_flat_layout() const      { return _non_atomic_layout_size_in_bytes != -1; }\n+  int  non_atomic_layout_size_in_bytes() const { return _non_atomic_layout_size_in_bytes; }\n+  int  non_atomic_layout_alignment() const     { return _non_atomic_layout_alignment; }\n+  bool has_atomic_layout() const               { return _atomic_layout_size_in_bytes != -1; }\n+  int  atomic_layout_size_in_bytes() const     { return _atomic_layout_size_in_bytes; }\n+  bool has_nullable_layout() const             { return _nullable_layout_size_in_bytes != -1; }\n+  int  nullable_layout_size_in_bytes() const   { return _nullable_layout_size_in_bytes; }\n+  int  null_marker_offset() const              { return _null_marker_offset; }\n+  bool is_empty_inline_class() const           { return _is_empty_inline_class; }\n@@ -252,0 +336,1 @@\n+  void compute_inline_class_layout();\n@@ -254,1 +339,1 @@\n- private:\n+ protected:\n@@ -258,1 +343,4 @@\n-  FieldGroup* get_or_create_contended_group(int g);\n+  void inline_class_field_sorting();\n+  void add_flat_field_oopmap(OopMapBlocksBuilder* nonstatic_oop_map, InlineKlass* vk, int offset);\n+  void register_embedded_oops_from_list(OopMapBlocksBuilder* nonstatic_oop_maps, GrowableArray<LayoutRawBlock*>* list);\n+  void register_embedded_oops(OopMapBlocksBuilder* nonstatic_oop_maps, FieldGroup* group);\n","filename":"src\/hotspot\/share\/classfile\/fieldLayoutBuilder.hpp","additions":126,"deletions":38,"binary":false,"changes":164,"status":"modified"},{"patch":"@@ -68,0 +68,1 @@\n+#define INLINE_TYPE_MAJOR_VERSION                       56\n@@ -286,1 +287,1 @@\n-    \/\/ We need to skip the following four for bootstraping\n+    \/\/ We need to skip the following four for bootstrapping\n@@ -505,0 +506,7 @@\n+    case WRONG_INLINE_TYPE:\n+      ss->print(\"Type \");\n+      _type.details(ss);\n+      ss->print(\" and type \");\n+      _expected.details(ss);\n+      ss->print(\" must be identical inline types.\");\n+      break;\n@@ -625,0 +633,5 @@\n+static bool supports_strict_fields(InstanceKlass* klass) {\n+  int ver = klass->major_version();\n+  return ver > Verifier::VALUE_TYPES_MAJOR_VERSION ||\n+         (ver == Verifier::VALUE_TYPES_MAJOR_VERSION && klass->minor_version() == Verifier::JAVA_PREVIEW_MINOR_VERSION);\n+}\n@@ -1692,1 +1705,1 @@\n-          if (_method->name() == vmSymbols::object_initializer_name() &&\n+          if (_method->is_object_constructor() &&\n@@ -1715,4 +1728,0 @@\n-          verify_invoke_instructions(\n-            &bcs, code_length, &current_frame, (bci >= ex_min && bci < ex_max),\n-            &this_uninit, return_type, cp, &stackmap_table, CHECK_VERIFY(this));\n-          no_control_flow = false; break;\n@@ -1723,1 +1732,1 @@\n-            &this_uninit, return_type, cp, &stackmap_table, CHECK_VERIFY(this));\n+            &this_uninit, cp, &stackmap_table, CHECK_VERIFY(this));\n@@ -1781,2 +1790,2 @@\n-        case Bytecodes::_monitorexit :\n-          current_frame.pop_stack(\n+        case Bytecodes::_monitorexit : {\n+          VerificationType ref = current_frame.pop_stack(\n@@ -1785,0 +1794,1 @@\n+        }\n@@ -2049,0 +2059,1 @@\n+\n@@ -2163,1 +2174,1 @@\n-            | (1 << JVM_CONSTANT_String)  | (1 << JVM_CONSTANT_Class)\n+            | (1 << JVM_CONSTANT_String) | (1 << JVM_CONSTANT_Class)\n@@ -2339,1 +2350,1 @@\n-    (!allow_arrays || !ref_class_type.is_array())) {\n+      (!allow_arrays || !ref_class_type.is_array())) {\n@@ -2346,0 +2357,1 @@\n+\n@@ -2387,1 +2399,1 @@\n-      \/\/ The JVMS 2nd edition allows field initialization before the superclass\n+      \/\/ Field initialization is allowed before the superclass\n@@ -2390,4 +2402,14 @@\n-      if (stack_object_type == VerificationType::uninitialized_this_type() &&\n-          target_class_type.equals(current_type()) &&\n-          _klass->find_local_field(field_name, field_sig, &fd)) {\n-        stack_object_type = current_type();\n+      bool is_local_field = _klass->find_local_field(field_name, field_sig, &fd) &&\n+                            target_class_type.equals(current_type());\n+      if (stack_object_type == VerificationType::uninitialized_this_type()) {\n+        if (is_local_field) {\n+          \/\/ Set the type to the current type so the is_assignable check passes.\n+          stack_object_type = current_type();\n+        }\n+      } else if (supports_strict_fields(_klass)) {\n+        \/\/ `strict` fields are not writable, but only local fields produce verification errors\n+        if (is_local_field && fd.access_flags().is_strict()) {\n+          verify_error(ErrorContext::bad_code(bci),\n+                       \"Illegal use of putfield on a strict field\");\n+          return;\n+        }\n@@ -2798,1 +2820,1 @@\n-    bool in_try_block, bool *this_uninit, VerificationType return_type,\n+    bool in_try_block, bool *this_uninit,\n@@ -2830,1 +2852,1 @@\n-  \/\/ Get referenced class type\n+  \/\/ Get referenced class\n@@ -2896,1 +2918,2 @@\n-    \/\/ Make sure <init> can only be invoked by invokespecial\n+    \/\/ Make sure:\n+    \/\/   <init> can only be invoked by invokespecial.\n@@ -2898,1 +2921,1 @@\n-        method_name != vmSymbols::object_initializer_name()) {\n+          method_name != vmSymbols::object_initializer_name()) {\n@@ -2906,1 +2929,1 @@\n-                  current_class()->super()->name()))) {\n+                  current_class()->super()->name()))) { \/\/ super() can never be an inline_type.\n@@ -2991,3 +3014,1 @@\n-      \/\/ <init> method must have a void return type\n-      \/* Unreachable?  Class file parser verifies that methods with '<' have\n-       * void return *\/\n+      \/\/ an <init> method must have a void return type\n","filename":"src\/hotspot\/share\/classfile\/verifier.cpp","additions":45,"deletions":24,"binary":false,"changes":69,"status":"modified"},{"patch":"@@ -333,0 +333,3 @@\n+  do_intrinsic(_newNullRestrictedArray,   jdk_internal_value_ValueClass, newNullRestrictedArray_name, newNullRestrictedArray_signature, F_SN) \\\n+   do_signature(newNullRestrictedArray_signature,                 \"(Ljava\/lang\/Class;I)[Ljava\/lang\/Object;\")            \\\n+   do_name(     newNullRestrictedArray_name,                      \"newNullRestrictedArray\")                             \\\n@@ -639,0 +642,2 @@\n+  do_intrinsic(_isFlatArray,              jdk_internal_misc_Unsafe,     isFlatArray_name, class_boolean_signature, F_RN)         \\\n+   do_name(     isFlatArray_name,                                       \"isFlatArray\")                                           \\\n@@ -690,0 +695,2 @@\n+  do_signature(getValue_signature,        \"(Ljava\/lang\/Object;JLjava\/lang\/Class;)Ljava\/lang\/Object;\")                   \\\n+  do_signature(putValue_signature,        \"(Ljava\/lang\/Object;JLjava\/lang\/Class;Ljava\/lang\/Object;)V\")                  \\\n@@ -700,0 +707,3 @@\n+  do_name(getValue_name,\"getValue\")             do_name(putValue_name,\"putValue\")                                       \\\n+  do_name(makePrivateBuffer_name,\"makePrivateBuffer\")                                                                   \\\n+  do_name(finishPrivateBuffer_name,\"finishPrivateBuffer\")                                                               \\\n@@ -710,0 +720,1 @@\n+  do_intrinsic(_getValue,           jdk_internal_misc_Unsafe,     getValue_name, getValue_signature,             F_RN)  \\\n@@ -719,0 +730,4 @@\n+  do_intrinsic(_putValue,           jdk_internal_misc_Unsafe,     putValue_name, putValue_signature,             F_RN)  \\\n+                                                                                                                        \\\n+  do_intrinsic(_makePrivateBuffer,  jdk_internal_misc_Unsafe,     makePrivateBuffer_name, object_object_signature, F_RN)   \\\n+  do_intrinsic(_finishPrivateBuffer,  jdk_internal_misc_Unsafe,   finishPrivateBuffer_name, object_object_signature, F_RN) \\\n","filename":"src\/hotspot\/share\/classfile\/vmIntrinsics.hpp","additions":15,"deletions":0,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -173,0 +173,1 @@\n+  template(tag_loadable_descriptors,                  \"LoadableDescriptors\")                      \\\n@@ -209,0 +210,1 @@\n+  template(java_lang_IdentityException,               \"java\/lang\/IdentityException\")              \\\n@@ -256,0 +258,3 @@\n+  template(jdk_internal_vm_annotation_ImplicitlyConstructible_signature,     \"Ljdk\/internal\/vm\/annotation\/ImplicitlyConstructible;\") \\\n+  template(jdk_internal_vm_annotation_LooselyConsistentValue_signature,      \"Ljdk\/internal\/vm\/annotation\/LooselyConsistentValue;\") \\\n+  template(jdk_internal_vm_annotation_NullRestricted_signature,              \"Ljdk\/internal\/vm\/annotation\/NullRestricted;\") \\\n@@ -287,1 +292,0 @@\n-  template(trusted_final_name,                        \"trustedFinal\")                             \\\n@@ -512,0 +516,3 @@\n+  template(default_value_name,                        \".default\")                                 \\\n+  template(null_reset_value_name,                     \".null_reset\")                              \\\n+  template(empty_marker_name,                         \".empty\")                                   \\\n@@ -583,0 +590,1 @@\n+  template(class_class_signature,                     \"(Ljava\/lang\/Class;)Ljava\/lang\/Class;\")     \\\n@@ -592,0 +600,1 @@\n+  template(object_object_boolean_signature,           \"(Ljava\/lang\/Object;Ljava\/lang\/Object;)Z\")                  \\\n@@ -722,0 +731,2 @@\n+  template(primaryType_name,                           \"primaryType\")                                             \\\n+  template(secondaryType_name,                         \"secondaryType\")                                           \\\n@@ -747,0 +758,6 @@\n+  template(java_lang_runtime_ValueObjectMethods,            \"java\/lang\/runtime\/ValueObjectMethods\")               \\\n+  template(isSubstitutable_name,                            \"isSubstitutable\")                                    \\\n+  template(valueObjectHashCode_name,                        \"valueObjectHashCode\")                                \\\n+  template(jdk_internal_value_PrimitiveClass,               \"jdk\/internal\/value\/PrimitiveClass\")                  \\\n+  template(jdk_internal_value_ValueClass,                   \"jdk\/internal\/value\/ValueClass\")                      \\\n+                                                                                                                  \\\n","filename":"src\/hotspot\/share\/classfile\/vmSymbols.hpp","additions":18,"deletions":1,"binary":false,"changes":19,"status":"modified"},{"patch":"@@ -727,0 +727,11 @@\n+\n+      \/\/ If inline types are passed as fields, use the extended signature\n+      \/\/ which contains the types of all (oop) fields of the inline type.\n+      if (is_compiled_by_c2() && callee->has_scalarized_args()) {\n+        const GrowableArray<SigEntry>* sig = callee->adapter()->get_sig_cc();\n+        assert(sig != nullptr, \"sig should never be null\");\n+        TempNewSymbol tmp_sig = SigEntry::create_symbol(sig);\n+        has_receiver = false; \/\/ The extended signature contains the receiver type\n+        fr.oops_compiled_arguments_do(tmp_sig, has_receiver, has_appendix, reg_map, f);\n+        return;\n+      }\n@@ -1253,0 +1264,4 @@\n+  _inline_entry_point             = entry_point();\n+  _verified_inline_entry_point    = verified_entry_point();\n+  _verified_inline_ro_entry_point = verified_entry_point();\n+\n@@ -1291,1 +1306,1 @@\n-\n+    assert(!method->has_scalarized_args(), \"scalarized native wrappers not supported yet\");\n@@ -1499,0 +1514,3 @@\n+    _inline_entry_point             = code_begin() + offsets->value(CodeOffsets::Inline_Entry);\n+    _verified_inline_entry_point    = code_begin() + offsets->value(CodeOffsets::Verified_Inline_Entry);\n+    _verified_inline_ro_entry_point = code_begin() + offsets->value(CodeOffsets::Verified_Inline_Entry_RO);\n@@ -3674,0 +3692,1 @@\n+  if (pos == inline_entry_point())                                      label = \"[Inline Entry Point]\";\n@@ -3675,0 +3694,2 @@\n+  if (pos == verified_inline_entry_point())                             label = \"[Verified Inline Entry Point]\";\n+  if (pos == verified_inline_ro_entry_point())                          label = \"[Verified Inline Entry Point (RO)]\";\n@@ -3684,0 +3705,10 @@\n+static int maybe_print_entry_label(outputStream* stream, address pos, address entry, const char* label) {\n+  if (pos == entry) {\n+    stream->bol();\n+    stream->print_cr(\"%s\", label);\n+    return 1;\n+  } else {\n+    return 0;\n+  }\n+}\n+\n@@ -3686,33 +3717,12 @@\n-    const char* label = nmethod_section_label(block_begin);\n-    if (label != nullptr) {\n-      stream->bol();\n-      stream->print_cr(\"%s\", label);\n-    }\n-  }\n-\n-  if (block_begin == entry_point()) {\n-    Method* m = method();\n-    if (m != nullptr) {\n-      stream->print(\"  # \");\n-      m->print_value_on(stream);\n-      stream->cr();\n-    }\n-    if (m != nullptr && !is_osr_method()) {\n-      ResourceMark rm;\n-      int sizeargs = m->size_of_parameters();\n-      BasicType* sig_bt = NEW_RESOURCE_ARRAY(BasicType, sizeargs);\n-      VMRegPair* regs   = NEW_RESOURCE_ARRAY(VMRegPair, sizeargs);\n-      {\n-        int sig_index = 0;\n-        if (!m->is_static())\n-          sig_bt[sig_index++] = T_OBJECT; \/\/ 'this'\n-        for (SignatureStream ss(m->signature()); !ss.at_return_type(); ss.next()) {\n-          BasicType t = ss.type();\n-          sig_bt[sig_index++] = t;\n-          if (type2size[t] == 2) {\n-            sig_bt[sig_index++] = T_VOID;\n-          } else {\n-            assert(type2size[t] == 1, \"size is 1 or 2\");\n-          }\n-        }\n-        assert(sig_index == sizeargs, \"\");\n+    int n = 0;\n+    \/\/ Multiple entry points may be at the same position. Print them all.\n+    n += maybe_print_entry_label(stream, block_begin, entry_point(),                    \"[Entry Point]\");\n+    n += maybe_print_entry_label(stream, block_begin, inline_entry_point(),             \"[Inline Entry Point]\");\n+    n += maybe_print_entry_label(stream, block_begin, verified_entry_point(),           \"[Verified Entry Point]\");\n+    n += maybe_print_entry_label(stream, block_begin, verified_inline_entry_point(),    \"[Verified Inline Entry Point]\");\n+    n += maybe_print_entry_label(stream, block_begin, verified_inline_ro_entry_point(), \"[Verified Inline Entry Point (RO)]\");\n+    if (n == 0) {\n+      const char* label = nmethod_section_label(block_begin);\n+      if (label != nullptr) {\n+        stream->bol();\n+        stream->print_cr(\"%s\", label);\n@@ -3720,54 +3730,63 @@\n-      const char* spname = \"sp\"; \/\/ make arch-specific?\n-      SharedRuntime::java_calling_convention(sig_bt, regs, sizeargs);\n-      int stack_slot_offset = this->frame_size() * wordSize;\n-      int tab1 = 14, tab2 = 24;\n-      int sig_index = 0;\n-      int arg_index = (m->is_static() ? 0 : -1);\n-      bool did_old_sp = false;\n-      for (SignatureStream ss(m->signature()); !ss.at_return_type(); ) {\n-        bool at_this = (arg_index == -1);\n-        bool at_old_sp = false;\n-        BasicType t = (at_this ? T_OBJECT : ss.type());\n-        assert(t == sig_bt[sig_index], \"sigs in sync\");\n-        if (at_this)\n-          stream->print(\"  # this: \");\n-        else\n-          stream->print(\"  # parm%d: \", arg_index);\n-        stream->move_to(tab1);\n-        VMReg fst = regs[sig_index].first();\n-        VMReg snd = regs[sig_index].second();\n-        if (fst->is_reg()) {\n-          stream->print(\"%s\", fst->name());\n-          if (snd->is_valid())  {\n-            stream->print(\":%s\", snd->name());\n-          }\n-        } else if (fst->is_stack()) {\n-          int offset = fst->reg2stack() * VMRegImpl::stack_slot_size + stack_slot_offset;\n-          if (offset == stack_slot_offset)  at_old_sp = true;\n-          stream->print(\"[%s+0x%x]\", spname, offset);\n-        } else {\n-          stream->print(\"reg%d:%d??\", (int)(intptr_t)fst, (int)(intptr_t)snd);\n-        }\n-        stream->print(\" \");\n-        stream->move_to(tab2);\n-        stream->print(\"= \");\n-        if (at_this) {\n-          m->method_holder()->print_value_on(stream);\n-        } else {\n-          bool did_name = false;\n-          if (!at_this && ss.is_reference()) {\n-            Symbol* name = ss.as_symbol();\n-            name->print_value_on(stream);\n-            did_name = true;\n-          }\n-          if (!did_name)\n-            stream->print(\"%s\", type2name(t));\n-        }\n-        if (at_old_sp) {\n-          stream->print(\"  (%s of caller)\", spname);\n-          did_old_sp = true;\n-        }\n-        stream->cr();\n-        sig_index += type2size[t];\n-        arg_index += 1;\n-        if (!at_this)  ss.next();\n+    }\n+  }\n+\n+  Method* m = method();\n+  if (m == nullptr || is_osr_method()) {\n+    return;\n+  }\n+\n+  \/\/ Print the name of the method (only once)\n+  address low = MIN4(entry_point(), verified_entry_point(), verified_inline_entry_point(), verified_inline_ro_entry_point());\n+  low = MIN2(low, inline_entry_point());\n+  assert(low != 0, \"sanity\");\n+  if (block_begin == low) {\n+    stream->print(\"  # \");\n+    m->print_value_on(stream);\n+    stream->cr();\n+  }\n+\n+  \/\/ Print the arguments for the 3 types of verified entry points\n+  CompiledEntrySignature ces(m);\n+  ces.compute_calling_conventions(false);\n+  const GrowableArray<SigEntry>* sig_cc;\n+  const VMRegPair* regs;\n+  if (block_begin == verified_entry_point()) {\n+    sig_cc = ces.sig_cc();\n+    regs = ces.regs_cc();\n+  } else if (block_begin == verified_inline_entry_point()) {\n+    sig_cc = ces.sig();\n+    regs = ces.regs();\n+  } else if (block_begin == verified_inline_ro_entry_point()) {\n+    sig_cc = ces.sig_cc_ro();\n+    regs = ces.regs_cc_ro();\n+  } else {\n+    return;\n+  }\n+\n+  bool has_this = !m->is_static();\n+  if (ces.has_inline_recv() && block_begin == verified_entry_point()) {\n+    \/\/ <this> argument is scalarized for verified_entry_point()\n+    has_this = false;\n+  }\n+  const char* spname = \"sp\"; \/\/ make arch-specific?\n+  int stack_slot_offset = this->frame_size() * wordSize;\n+  int tab1 = 14, tab2 = 24;\n+  int sig_index = 0;\n+  int arg_index = has_this ? -1 : 0;\n+  bool did_old_sp = false;\n+  for (ExtendedSignature sig = ExtendedSignature(sig_cc, SigEntryFilter()); !sig.at_end(); ++sig) {\n+    bool at_this = (arg_index == -1);\n+    bool at_old_sp = false;\n+    BasicType t = (*sig)._bt;\n+    if (at_this) {\n+      stream->print(\"  # this: \");\n+    } else {\n+      stream->print(\"  # parm%d: \", arg_index);\n+    }\n+    stream->move_to(tab1);\n+    VMReg fst = regs[sig_index].first();\n+    VMReg snd = regs[sig_index].second();\n+    if (fst->is_reg()) {\n+      stream->print(\"%s\", fst->name());\n+      if (snd->is_valid())  {\n+        stream->print(\":%s\", snd->name());\n@@ -3775,6 +3794,18 @@\n-      if (!did_old_sp) {\n-        stream->print(\"  # \");\n-        stream->move_to(tab1);\n-        stream->print(\"[%s+0x%x]\", spname, stack_slot_offset);\n-        stream->print(\"  (%s of caller)\", spname);\n-        stream->cr();\n+    } else if (fst->is_stack()) {\n+      int offset = fst->reg2stack() * VMRegImpl::stack_slot_size + stack_slot_offset;\n+      if (offset == stack_slot_offset)  at_old_sp = true;\n+      stream->print(\"[%s+0x%x]\", spname, offset);\n+    } else {\n+      stream->print(\"reg%d:%d??\", (int)(intptr_t)fst, (int)(intptr_t)snd);\n+    }\n+    stream->print(\" \");\n+    stream->move_to(tab2);\n+    stream->print(\"= \");\n+    if (at_this) {\n+      m->method_holder()->print_value_on(stream);\n+    } else {\n+      bool did_name = false;\n+      if (is_reference_type(t)) {\n+        Symbol* name = (*sig)._symbol;\n+        name->print_value_on(stream);\n+        did_name = true;\n@@ -3782,0 +3813,2 @@\n+      if (!did_name)\n+        stream->print(\"%s\", type2name(t));\n@@ -3783,0 +3816,14 @@\n+    if (at_old_sp) {\n+      stream->print(\"  (%s of caller)\", spname);\n+      did_old_sp = true;\n+    }\n+    stream->cr();\n+    sig_index += type2size[t];\n+    arg_index += 1;\n+  }\n+  if (!did_old_sp) {\n+    stream->print(\"  # \");\n+    stream->move_to(tab1);\n+    stream->print(\"[%s+0x%x]\", spname, stack_slot_offset);\n+    stream->print(\"  (%s of caller)\", spname);\n+    stream->cr();\n@@ -3906,1 +3953,1 @@\n-      st->print(\" {reexecute=%d rethrow=%d return_oop=%d}\", sd->should_reexecute(), sd->rethrow_exception(), sd->return_oop());\n+      st->print(\" {reexecute=%d rethrow=%d return_oop=%d return_scalarized=%d}\", sd->should_reexecute(), sd->rethrow_exception(), sd->return_oop(), sd->return_scalarized());\n","filename":"src\/hotspot\/share\/code\/nmethod.cpp","additions":142,"deletions":95,"binary":false,"changes":237,"status":"modified"},{"patch":"@@ -216,1 +216,1 @@\n-void G1BarrierSetC2::eliminate_gc_barrier(PhaseMacroExpand* macro, Node* node) const {\n+void G1BarrierSetC2::eliminate_gc_barrier(PhaseIterGVN* igvn, Node* node) const {\n@@ -219,1 +219,0 @@\n-\n","filename":"src\/hotspot\/share\/gc\/g1\/c2\/g1BarrierSetC2.cpp","additions":1,"deletions":2,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -787,1 +787,1 @@\n-    Node* offset = phase->igvn().MakeConX(in_bytes(ShenandoahThreadLocalData::gc_state_offset()));\n+    Node* offset = phase->MakeConX(in_bytes(ShenandoahThreadLocalData::gc_state_offset()));\n@@ -835,1 +835,1 @@\n-    phase->igvn().replace_node(ac, call);\n+    phase->replace_node(ac, call);\n@@ -855,1 +855,1 @@\n-void ShenandoahBarrierSetC2::eliminate_gc_barrier(PhaseMacroExpand* macro, Node* n) const {\n+void ShenandoahBarrierSetC2::eliminate_gc_barrier(PhaseIterGVN* igvn, Node* n) const {\n@@ -857,1 +857,1 @@\n-    shenandoah_eliminate_wb_pre(n, &macro->igvn());\n+    shenandoah_eliminate_wb_pre(n, igvn);\n@@ -988,1 +988,1 @@\n-    uint cnt = ShenandoahBarrierSetC2::write_ref_field_pre_Type()->domain()->cnt();\n+    uint cnt = ShenandoahBarrierSetC2::write_ref_field_pre_Type()->domain_sig()->cnt();\n@@ -1074,1 +1074,1 @@\n-        uint cnt = ShenandoahBarrierSetC2::write_ref_field_pre_Type()->domain()->cnt();\n+        uint cnt = ShenandoahBarrierSetC2::write_ref_field_pre_Type()->domain_sig()->cnt();\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/c2\/shenandoahBarrierSetC2.cpp","additions":6,"deletions":6,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -424,1 +424,1 @@\n-    if (is_reference_type(bt)) {\n+    if (is_reference_type(bt) && !ary_ptr->is_flat()) {\n@@ -450,1 +450,1 @@\n-        length = phase->transform_later(new SubLNode(length, phase->longcon(1))); \/\/ Size is in longs\n+        length = phase->transform_later(new SubXNode(length, phase->longcon(1))); \/\/ Size is in longs\n@@ -814,1 +814,2 @@\n-void ZBarrierSetC2::eliminate_gc_barrier(PhaseMacroExpand* macro, Node* node) const {\n+\n+void ZBarrierSetC2::eliminate_gc_barrier(PhaseIterGVN* igvn, Node* node) const {\n","filename":"src\/hotspot\/share\/gc\/z\/c2\/zBarrierSetC2.cpp","additions":4,"deletions":3,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -134,1 +134,1 @@\n-  virtual void eliminate_gc_barrier(PhaseMacroExpand* macro, Node* node) const;\n+  virtual void eliminate_gc_barrier(PhaseIterGVN* igvn, Node* node) const;\n","filename":"src\/hotspot\/share\/gc\/z\/c2\/zBarrierSetC2.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -212,1 +212,1 @@\n-  nonstatic_field(InstanceKlass,               _misc_flags._flags,                            u2)                                    \\\n+  nonstatic_field(InstanceKlass,               _misc_flags._flags,                            u4)                                    \\\n@@ -683,0 +683,3 @@\n+  declare_constant(DataLayout::array_store_data_tag)                      \\\n+  declare_constant(DataLayout::array_load_data_tag)                       \\\n+  declare_constant(DataLayout::acmp_data_tag)                             \\\n","filename":"src\/hotspot\/share\/jvmci\/vmStructs_jvmci.cpp","additions":4,"deletions":1,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -127,1 +127,2 @@\n-  bool eliminate_boxing = EliminateAutoBox;\n+  \/\/ TODO 8328675 Re-enable\n+  bool eliminate_boxing = false; \/\/ EliminateAutoBox;\n@@ -646,0 +647,2 @@\n+  case vmIntrinsics::_makePrivateBuffer:\n+  case vmIntrinsics::_finishPrivateBuffer:\n@@ -655,0 +658,1 @@\n+  case vmIntrinsics::_getValue:\n@@ -664,0 +668,1 @@\n+  case vmIntrinsics::_putValue:\n@@ -746,0 +751,1 @@\n+  case vmIntrinsics::_isFlatArray:\n@@ -747,0 +753,1 @@\n+  case vmIntrinsics::_newNullRestrictedArray:\n","filename":"src\/hotspot\/share\/opto\/c2compiler.cpp","additions":8,"deletions":1,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -38,0 +38,1 @@\n+#include \"opto\/inlinetypenode.hpp\"\n@@ -123,1 +124,1 @@\n-  \/\/ paths to facilitate late inlinig.\n+  \/\/ paths to facilitate late inlining.\n@@ -132,0 +133,1 @@\n+      _call_node(nullptr),\n@@ -134,0 +136,8 @@\n+    if (InlineTypeReturnedAsFields && method->is_method_handle_intrinsic()) {\n+      \/\/ If that call has not been optimized by the time optimizations are over,\n+      \/\/ we'll need to add a call to create an inline type instance from the klass\n+      \/\/ returned by the call (see PhaseMacroExpand::expand_mh_intrinsic_return).\n+      \/\/ Separating memory and I\/O projections for exceptions is required to\n+      \/\/ perform that graph transformation.\n+      _separate_io_proj = true;\n+    }\n@@ -148,0 +158,1 @@\n+  PhaseGVN& gvn = kit.gvn();\n@@ -180,1 +191,4 @@\n-  kit.set_arguments_for_java_call(call);\n+  kit.set_arguments_for_java_call(call, is_late_inline());\n+  if (kit.stopped()) {\n+    return kit.transfer_exceptions_into_jvms();\n+  }\n@@ -221,1 +235,0 @@\n-\n@@ -281,0 +294,3 @@\n+  if (kit.stopped()) {\n+    return kit.transfer_exceptions_into_jvms();\n+  }\n@@ -374,0 +390,4 @@\n+  virtual CallGenerator* inline_cg() {\n+    return _inline_cg;\n+  }\n+\n@@ -435,0 +455,8 @@\n+    \/\/ AlwaysIncrementalInline causes for_method_handle_inline() to\n+    \/\/ return a LateInlineCallGenerator. Extract the\n+    \/\/ InlineCallGenerator from it.\n+    if (AlwaysIncrementalInline && cg->is_late_inline() && !cg->is_virtual_late_inline()) {\n+      cg = cg->inline_cg();\n+      assert(cg != nullptr, \"inline call generator expected\");\n+    }\n+\n@@ -605,3 +633,3 @@\n-  const TypeTuple *r = call->tf()->domain();\n-  for (int i1 = 0; i1 < method()->arg_size(); i1++) {\n-    if (call->in(TypeFunc::Parms + i1)->is_top() && r->field_at(TypeFunc::Parms + i1) != Type::HALF) {\n+  const TypeTuple* r = call->tf()->domain_cc();\n+  for (uint i1 = TypeFunc::Parms; i1 < r->cnt(); i1++) {\n+    if (call->in(i1)->is_top() && r->field_at(i1) != Type::HALF) {\n@@ -625,1 +653,0 @@\n-  CallProjections callprojs;\n@@ -629,9 +656,8 @@\n-  call->extract_projections(&callprojs, true, do_asserts);\n-  if ((callprojs.fallthrough_catchproj == call->in(0)) ||\n-      (callprojs.catchall_catchproj    == call->in(0)) ||\n-      (callprojs.fallthrough_memproj   == call->in(TypeFunc::Memory)) ||\n-      (callprojs.catchall_memproj      == call->in(TypeFunc::Memory)) ||\n-      (callprojs.fallthrough_ioproj    == call->in(TypeFunc::I_O)) ||\n-      (callprojs.catchall_ioproj       == call->in(TypeFunc::I_O)) ||\n-      (callprojs.resproj != nullptr && call->find_edge(callprojs.resproj) != -1) ||\n-      (callprojs.exobj   != nullptr && call->find_edge(callprojs.exobj) != -1)) {\n+  CallProjections* callprojs = call->extract_projections(true, do_asserts);\n+  if ((callprojs->fallthrough_catchproj == call->in(0)) ||\n+      (callprojs->catchall_catchproj    == call->in(0)) ||\n+      (callprojs->fallthrough_memproj   == call->in(TypeFunc::Memory)) ||\n+      (callprojs->catchall_memproj      == call->in(TypeFunc::Memory)) ||\n+      (callprojs->fallthrough_ioproj    == call->in(TypeFunc::I_O)) ||\n+      (callprojs->catchall_ioproj       == call->in(TypeFunc::I_O)) ||\n+      (callprojs->exobj != nullptr && call->find_edge(callprojs->exobj) != -1)) {\n@@ -647,3 +673,12 @@\n-  \/\/ The call is marked as pure (no important side effects), but result isn't used.\n-  \/\/ It's safe to remove the call.\n-  bool result_not_used = (callprojs.resproj == nullptr || callprojs.resproj->outcnt() == 0);\n+\n+  bool result_not_used = true;\n+  for (uint i = 0; i < callprojs->nb_resproj; i++) {\n+    if (callprojs->resproj[i] != nullptr) {\n+      if (callprojs->resproj[i]->outcnt() != 0) {\n+        result_not_used = false;\n+      }\n+      if (call->find_edge(callprojs->resproj[i]) != -1) {\n+        return;\n+      }\n+    }\n+  }\n@@ -652,0 +687,2 @@\n+    \/\/ The call is marked as pure (no important side effects), but result isn't used.\n+    \/\/ It's safe to remove the call.\n@@ -664,0 +701,1 @@\n+    PhaseGVN& gvn = *C->initial_gvn();\n@@ -667,1 +705,1 @@\n-      C->initial_gvn()->set_type_bottom(mem);\n+      gvn.set_type_bottom(mem);\n@@ -671,4 +709,2 @@\n-    uint nargs = method()->arg_size();\n-    Node* top = C->top();\n-    for (uint i1 = 0; i1 < nargs; i1++) {\n-      map->set_req(TypeFunc::Parms + i1, top);\n+    for (uint i1 = TypeFunc::Parms; i1 < r->cnt(); i1++) {\n+      map->set_req(i1, C->top());\n@@ -682,0 +718,6 @@\n+    const TypeTuple* domain_sig = call->_tf->domain_sig();\n+    uint nargs = method()->arg_size();\n+    assert(domain_sig->cnt() - TypeFunc::Parms == nargs, \"inconsistent signature\");\n+\n+    uint j = TypeFunc::Parms;\n+    int arg_num = 0;\n@@ -683,1 +725,14 @@\n-      map->set_argument(jvms, i1, call->in(TypeFunc::Parms + i1));\n+      const Type* t = domain_sig->field_at(TypeFunc::Parms + i1);\n+      if (t->is_inlinetypeptr() && !method()->get_Method()->mismatch() && method()->is_scalarized_arg(arg_num)) {\n+        \/\/ Inline type arguments are not passed by reference: we get an argument per\n+        \/\/ field of the inline type. Build InlineTypeNodes from the inline type arguments.\n+        GraphKit arg_kit(jvms, &gvn);\n+        Node* vt = InlineTypeNode::make_from_multi(&arg_kit, call, t->inline_klass(), j, \/* in= *\/ true, \/* null_free= *\/ !t->maybe_null());\n+        map->set_control(arg_kit.control());\n+        map->set_argument(jvms, i1, vt);\n+      } else {\n+        map->set_argument(jvms, i1, call->in(j++));\n+      }\n+      if (t != Type::HALF) {\n+        arg_num++;\n+      }\n@@ -702,0 +757,20 @@\n+    \/\/ Check if we are late inlining a method handle call that returns an inline type as fields.\n+    Node* buffer_oop = nullptr;\n+    ciMethod* inline_method = inline_cg()->method();\n+    ciType* return_type = inline_method->return_type();\n+    if (!call->tf()->returns_inline_type_as_fields() && is_mh_late_inline() &&\n+        return_type->is_inlinetype() && return_type->as_inline_klass()->can_be_returned_as_fields()) {\n+      \/\/ Allocate a buffer for the inline type returned as fields because the caller expects an oop return.\n+      \/\/ Do this before the method handle call in case the buffer allocation triggers deoptimization and\n+      \/\/ we need to \"re-execute\" the call in the interpreter (to make sure the call is only executed once).\n+      GraphKit arg_kit(jvms, &gvn);\n+      {\n+        PreserveReexecuteState preexecs(&arg_kit);\n+        arg_kit.jvms()->set_should_reexecute(true);\n+        arg_kit.inc_sp(nargs);\n+        Node* klass_node = arg_kit.makecon(TypeKlassPtr::make(return_type->as_inline_klass()));\n+        buffer_oop = arg_kit.new_instance(klass_node, nullptr, nullptr, \/* deoptimize_on_exception *\/ true);\n+      }\n+      jvms = arg_kit.transfer_exceptions_into_jvms();\n+    }\n+\n@@ -730,2 +805,2 @@\n-      C->set_has_loops(C->has_loops() || inline_cg()->method()->has_loops());\n-      C->env()->notice_inlined_method(inline_cg()->method());\n+      C->set_has_loops(C->has_loops() || inline_method->has_loops());\n+      C->env()->notice_inlined_method(inline_method);\n@@ -735,0 +810,52 @@\n+\n+    \/\/ Handle inline type returns\n+    InlineTypeNode* vt = result->isa_InlineType();\n+    if (vt != nullptr) {\n+      if (call->tf()->returns_inline_type_as_fields()) {\n+        vt->replace_call_results(&kit, call, C);\n+      } else if (vt->is_InlineType()) {\n+        \/\/ Result might still be allocated (for example, if it has been stored to a non-flat field)\n+        if (!vt->is_allocated(&kit.gvn())) {\n+          assert(buffer_oop != nullptr, \"should have allocated a buffer\");\n+          RegionNode* region = new RegionNode(3);\n+\n+          \/\/ Check if result is null\n+          Node* null_ctl = kit.top();\n+          kit.null_check_common(vt->get_is_init(), T_INT, false, &null_ctl);\n+          region->init_req(1, null_ctl);\n+          PhiNode* oop = PhiNode::make(region, kit.gvn().zerocon(T_OBJECT), TypeInstPtr::make(TypePtr::BotPTR, vt->type()->inline_klass()));\n+          Node* init_mem = kit.reset_memory();\n+          PhiNode* mem = PhiNode::make(region, init_mem, Type::MEMORY, TypePtr::BOTTOM);\n+\n+          \/\/ Not null, initialize the buffer\n+          kit.set_all_memory(init_mem);\n+          vt->store(&kit, buffer_oop, buffer_oop, vt->type()->inline_klass());\n+          \/\/ Do not let stores that initialize this buffer be reordered with a subsequent\n+          \/\/ store that would make this buffer accessible by other threads.\n+          AllocateNode* alloc = AllocateNode::Ideal_allocation(buffer_oop);\n+          assert(alloc != nullptr, \"must have an allocation node\");\n+          kit.insert_mem_bar(Op_MemBarStoreStore, alloc->proj_out_or_null(AllocateNode::RawAddress));\n+          region->init_req(2, kit.control());\n+          oop->init_req(2, buffer_oop);\n+          mem->init_req(2, kit.merged_memory());\n+\n+          \/\/ Update oop input to buffer\n+          kit.gvn().hash_delete(vt);\n+          vt->set_oop(kit.gvn(), kit.gvn().transform(oop));\n+          vt->set_is_buffered(kit.gvn());\n+          vt = kit.gvn().transform(vt)->as_InlineType();\n+\n+          kit.set_control(kit.gvn().transform(region));\n+          kit.set_all_memory(kit.gvn().transform(mem));\n+          kit.record_for_igvn(region);\n+          kit.record_for_igvn(oop);\n+          kit.record_for_igvn(mem);\n+        }\n+        result = vt;\n+      }\n+      DEBUG_ONLY(buffer_oop = nullptr);\n+    } else {\n+      assert(result->is_top() || !call->tf()->returns_inline_type_as_fields(), \"Unexpected return value\");\n+    }\n+    assert(buffer_oop == nullptr, \"unused buffer allocation\");\n+\n@@ -957,0 +1084,23 @@\n+  \/\/ Allocate inline types if they are merged with objects (similar to Parse::merge_common())\n+  uint tos = kit.jvms()->stkoff() + kit.sp();\n+  uint limit = slow_map->req();\n+  for (uint i = TypeFunc::Parms; i < limit; i++) {\n+    Node* m = kit.map()->in(i);\n+    Node* n = slow_map->in(i);\n+    const Type* t = gvn.type(m)->meet_speculative(gvn.type(n));\n+    \/\/ TODO 8284443 still needed?\n+    if (m->is_InlineType() && !t->is_inlinetypeptr()) {\n+      \/\/ Allocate inline type in fast path\n+      m = m->as_InlineType()->buffer(&kit);\n+      kit.map()->set_req(i, m);\n+    }\n+    if (n->is_InlineType() && !t->is_inlinetypeptr()) {\n+      \/\/ Allocate inline type in slow path\n+      PreserveJVMState pjvms(&kit);\n+      kit.set_map(slow_map);\n+      n = n->as_InlineType()->buffer(&kit);\n+      kit.map()->set_req(i, n);\n+      slow_map = kit.stop();\n+    }\n+  }\n+\n@@ -980,2 +1130,0 @@\n-  uint tos = kit.jvms()->stkoff() + kit.sp();\n-  uint limit = slow_map->req();\n@@ -1018,2 +1166,2 @@\n-  if (IncrementalInlineMH && call_site_count > 0 &&\n-      (should_delay || input_not_const || !C->inlining_incrementally() || C->over_inlining_cutoff())) {\n+  if (IncrementalInlineMH && (AlwaysIncrementalInline ||\n+                            (call_site_count > 0 && (should_delay || input_not_const || !C->inlining_incrementally() || C->over_inlining_cutoff())))) {\n@@ -1027,0 +1175,1 @@\n+\n@@ -1078,0 +1227,1 @@\n+      int nargs = callee->arg_size();\n@@ -1079,1 +1229,1 @@\n-      Node* member_name = kit.argument(callee->arg_size() - 1);\n+      Node* member_name = kit.argument(nargs - 1);\n@@ -1152,1 +1302,2 @@\n-                                              speculative_receiver_type);\n+                                              speculative_receiver_type,\n+                                              true);\n@@ -1227,1 +1378,1 @@\n-    Node* receiver = kit.null_check_receiver_before_call(method());\n+    kit.null_check_receiver_before_call(method());\n","filename":"src\/hotspot\/share\/opto\/callGenerator.cpp","additions":185,"deletions":34,"binary":false,"changes":219,"status":"modified"},{"patch":"@@ -185,0 +185,1 @@\n+macro(FlatArrayCheck)\n@@ -372,0 +373,1 @@\n+macro(InlineType)\n","filename":"src\/hotspot\/share\/opto\/classes.hpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -60,0 +60,1 @@\n+#include \"opto\/inlinetypenode.hpp\"\n@@ -405,0 +406,3 @@\n+  if (dead->is_InlineType()) {\n+    remove_inline_type(dead);\n+  }\n@@ -446,0 +450,3 @@\n+    if (n->outcnt() == 0) {\n+      worklist.push(n);\n+    }\n@@ -453,0 +460,6 @@\n+  remove_useless_nodes(_inline_type_nodes,  useful); \/\/ remove useless inline type nodes\n+#ifdef ASSERT\n+  if (_modified_nodes != nullptr) {\n+    _modified_nodes->remove_useless_nodes(useful.member_set());\n+  }\n+#endif\n@@ -632,0 +645,1 @@\n+                  _has_circular_inline_type(false),\n@@ -651,0 +665,1 @@\n+                  _inline_type_nodes (comp_arena(), 8, 0, nullptr),\n@@ -757,4 +772,2 @@\n-      const TypeTuple *domain = StartOSRNode::osr_domain();\n-      const TypeTuple *range = TypeTuple::make_range(method()->signature());\n-      init_tf(TypeFunc::make(domain, range));\n-      StartNode* s = new StartOSRNode(root(), domain);\n+      init_tf(TypeFunc::make(method(), \/* is_osr_compilation = *\/ true));\n+      StartNode* s = new StartOSRNode(root(), tf()->domain_sig());\n@@ -767,1 +780,1 @@\n-      StartNode* s = new StartNode(root(), tf()->domain());\n+      StartNode* s = new StartNode(root(), tf()->domain_cc());\n@@ -878,0 +891,10 @@\n+  if (needs_stack_repair()) {\n+    \/\/ One extra slot for the special stack increment value\n+    next_slot += 2;\n+  }\n+  \/\/ TODO 8284443 Only reserve extra slot if needed\n+  if (InlineTypeReturnedAsFields) {\n+    \/\/ One extra slot to hold the IsInit information for a nullable\n+    \/\/ inline type return if we run out of registers.\n+    next_slot += 2;\n+  }\n@@ -912,0 +935,1 @@\n+    _has_circular_inline_type(false),\n@@ -1058,0 +1082,4 @@\n+  _has_flat_accesses = false;\n+  _flat_accesses_share_alias = true;\n+  _scalarize_in_safepoints = false;\n+\n@@ -1329,1 +1357,2 @@\n-    assert(InlineUnsafeOps || StressReflectiveCode, \"indeterminate pointers come only from unsafe ops\");\n+    bool default_value_load = EnableValhalla && tj->is_instptr()->instance_klass() == ciEnv::current()->Class_klass();\n+    assert(InlineUnsafeOps || StressReflectiveCode || default_value_load, \"indeterminate pointers come only from unsafe ops\");\n@@ -1342,0 +1371,9 @@\n+  if (ta && ta->is_not_flat()) {\n+    \/\/ Erase not flat property for alias analysis.\n+    tj = ta = ta->cast_to_not_flat(false);\n+  }\n+  if (ta && ta->is_not_null_free()) {\n+    \/\/ Erase not null free property for alias analysis.\n+    tj = ta = ta->cast_to_not_null_free(false);\n+  }\n+\n@@ -1355,0 +1393,2 @@\n+    \/\/ For flat inline type array, each field has its own slice so\n+    \/\/ we must include the field offset.\n@@ -1395,1 +1435,1 @@\n-      tj = ta = TypeAryPtr::make(ptr,ta->const_oop(),tary,nullptr,false,offset);\n+      tj = ta = TypeAryPtr::make(ptr,ta->const_oop(),tary,nullptr,false,Type::Offset(offset), ta->field_offset());\n@@ -1399,1 +1439,6 @@\n-      tj = ta = TypeAryPtr::make(ptr,ta->const_oop(),tary,nullptr,false,offset);\n+      tj = ta = TypeAryPtr::make(ptr,ta->const_oop(),tary,nullptr,false,Type::Offset(offset), ta->field_offset());\n+    }\n+    \/\/ Initially all flattened array accesses share a single slice\n+    if (ta->is_flat() && ta->elem() != TypeInstPtr::BOTTOM && _flat_accesses_share_alias) {\n+      const TypeAry* tary = TypeAry::make(TypeInstPtr::BOTTOM, ta->size(), \/* stable= *\/ false, \/* flat= *\/ true);\n+      tj = ta = TypeAryPtr::make(ptr,ta->const_oop(),tary,nullptr,false,Type::Offset(offset), Type::Offset(Type::OffsetBot));\n@@ -1406,1 +1451,1 @@\n-      tj = ta = TypeAryPtr::make(ptr,ta->const_oop(),tary,aklass,false,offset);\n+      tj = ta = TypeAryPtr::make(ptr,ta->const_oop(),tary,aklass,false,Type::Offset(offset), ta->field_offset());\n@@ -1456,1 +1501,1 @@\n-        tj = to = TypeInstPtr::make(TypePtr::BotPTR, env()->Object_klass(), false, nullptr, offset);\n+        tj = to = TypeInstPtr::make(TypePtr::BotPTR, env()->Object_klass(), false, nullptr, Type::Offset(offset));\n@@ -1477,1 +1522,1 @@\n-        assert(tj == TypeInstPtr::make(to->ptr(), canonical_holder, is_known_inst, nullptr, offset, instance_id), \"exact type should be canonical type\");\n+        assert(tj == TypeInstPtr::make(to->ptr(), canonical_holder, is_known_inst, nullptr, Type::Offset(offset), instance_id), \"exact type should be canonical type\");\n@@ -1480,1 +1525,1 @@\n-        tj = to = TypeInstPtr::make(to->ptr(), canonical_holder, is_known_inst, nullptr, offset, instance_id);\n+        tj = to = TypeInstPtr::make(to->ptr(), canonical_holder, is_known_inst, nullptr, Type::Offset(offset), instance_id);\n@@ -1495,1 +1540,1 @@\n-                                       offset);\n+                                       Type::Offset(offset));\n@@ -1501,1 +1546,1 @@\n-        tj = tk = TypeInstKlassPtr::make(TypePtr::NotNull, env()->Object_klass(), offset);\n+        tj = tk = TypeInstKlassPtr::make(TypePtr::NotNull, env()->Object_klass(), Type::Offset(offset));\n@@ -1503,1 +1548,1 @@\n-        tj = tk = TypeAryKlassPtr::make(TypePtr::NotNull, tk->is_aryklassptr()->elem(), k, offset);\n+        tj = tk = TypeAryKlassPtr::make(TypePtr::NotNull, tk->is_aryklassptr()->elem(), k, Type::Offset(offset), tk->is_not_flat(), tk->is_not_null_free(), tk->is_null_free());\n@@ -1506,1 +1551,0 @@\n-\n@@ -1636,1 +1680,1 @@\n-Compile::AliasType* Compile::find_alias_type(const TypePtr* adr_type, bool no_create, ciField* original_field) {\n+Compile::AliasType* Compile::find_alias_type(const TypePtr* adr_type, bool no_create, ciField* original_field, bool uncached) {\n@@ -1641,3 +1685,6 @@\n-  AliasCacheEntry* ace = probe_alias_cache(adr_type);\n-  if (ace->_adr_type == adr_type) {\n-    return alias_type(ace->_index);\n+  AliasCacheEntry* ace = nullptr;\n+  if (!uncached) {\n+    ace = probe_alias_cache(adr_type);\n+    if (ace->_adr_type == adr_type) {\n+      return alias_type(ace->_index);\n+    }\n@@ -1693,0 +1740,1 @@\n+    ciField* field = nullptr;\n@@ -1699,0 +1747,1 @@\n+      const Type* elemtype = flat->is_aryptr()->elem();\n@@ -1700,1 +1749,8 @@\n-        alias_type(idx)->set_element(flat->is_aryptr()->elem());\n+        alias_type(idx)->set_element(elemtype);\n+      }\n+      int field_offset = flat->is_aryptr()->field_offset().get();\n+      if (flat->is_flat() &&\n+          field_offset != Type::OffsetBot) {\n+        ciInlineKlass* vk = elemtype->inline_klass();\n+        field_offset += vk->first_field_offset();\n+        field = vk->get_field_by_offset(field_offset, false);\n@@ -1714,0 +1770,2 @@\n+      if (flat->offset() == in_bytes(Klass::layout_helper_offset()))\n+        alias_type(idx)->set_rewritable(false);\n@@ -1724,1 +1782,0 @@\n-      ciField* field;\n@@ -1731,0 +1788,4 @@\n+      } else if (tinst->is_inlinetypeptr()) {\n+        \/\/ Inline type field\n+        ciInlineKlass* vk = tinst->inline_klass();\n+        field = vk->get_field_by_offset(tinst->offset(), false);\n@@ -1735,7 +1796,14 @@\n-      assert(field == nullptr ||\n-             original_field == nullptr ||\n-             (field->holder() == original_field->holder() &&\n-              field->offset_in_bytes() == original_field->offset_in_bytes() &&\n-              field->is_static() == original_field->is_static()), \"wrong field?\");\n-      \/\/ Set field() and is_rewritable() attributes.\n-      if (field != nullptr)  alias_type(idx)->set_field(field);\n+    }\n+    assert(field == nullptr ||\n+           original_field == nullptr ||\n+           (field->holder() == original_field->holder() &&\n+            field->offset_in_bytes() == original_field->offset_in_bytes() &&\n+            field->is_static() == original_field->is_static()), \"wrong field?\");\n+    \/\/ Set field() and is_rewritable() attributes.\n+    if (field != nullptr) {\n+      alias_type(idx)->set_field(field);\n+      if (flat->isa_aryptr()) {\n+        \/\/ Fields of flat arrays are rewritable although they are declared final\n+        assert(flat->is_flat(), \"must be a flat array\");\n+        alias_type(idx)->set_rewritable(true);\n+      }\n@@ -1746,3 +1814,4 @@\n-  ace->_adr_type = adr_type;\n-  ace->_index    = idx;\n-  assert(alias_type(adr_type) == alias_type(idx),  \"type must be installed\");\n+  if (!uncached) {\n+    ace->_adr_type = adr_type;\n+    ace->_index    = idx;\n+    assert(alias_type(adr_type) == alias_type(idx),  \"type must be installed\");\n@@ -1750,6 +1819,7 @@\n-  \/\/ Might as well try to fill the cache for the flattened version, too.\n-  AliasCacheEntry* face = probe_alias_cache(flat);\n-  if (face->_adr_type == nullptr) {\n-    face->_adr_type = flat;\n-    face->_index    = idx;\n-    assert(alias_type(flat) == alias_type(idx), \"flat type must work too\");\n+    \/\/ Might as well try to fill the cache for the flattened version, too.\n+    AliasCacheEntry* face = probe_alias_cache(flat);\n+    if (face->_adr_type == nullptr) {\n+      face->_adr_type = flat;\n+      face->_index    = idx;\n+      assert(alias_type(flat) == alias_type(idx), \"flat type must work too\");\n+    }\n@@ -1875,0 +1945,398 @@\n+void Compile::add_inline_type(Node* n) {\n+  assert(n->is_InlineType(), \"unexpected node\");\n+  _inline_type_nodes.push(n);\n+}\n+\n+void Compile::remove_inline_type(Node* n) {\n+  assert(n->is_InlineType(), \"unexpected node\");\n+  if (_inline_type_nodes.contains(n)) {\n+    _inline_type_nodes.remove(n);\n+  }\n+}\n+\n+\/\/ Does the return value keep otherwise useless inline type allocations alive?\n+static bool return_val_keeps_allocations_alive(Node* ret_val) {\n+  ResourceMark rm;\n+  Unique_Node_List wq;\n+  wq.push(ret_val);\n+  bool some_allocations = false;\n+  for (uint i = 0; i < wq.size(); i++) {\n+    Node* n = wq.at(i);\n+    if (n->outcnt() > 1) {\n+      \/\/ Some other use for the allocation\n+      return false;\n+    } else if (n->is_InlineType()) {\n+      wq.push(n->in(1));\n+    } else if (n->is_Phi()) {\n+      for (uint j = 1; j < n->req(); j++) {\n+        wq.push(n->in(j));\n+      }\n+    } else if (n->is_CheckCastPP() &&\n+               n->in(1)->is_Proj() &&\n+               n->in(1)->in(0)->is_Allocate()) {\n+      some_allocations = true;\n+    } else if (n->is_CheckCastPP()) {\n+      wq.push(n->in(1));\n+    }\n+  }\n+  return some_allocations;\n+}\n+\n+void Compile::process_inline_types(PhaseIterGVN &igvn, bool remove) {\n+  \/\/ Make sure that the return value does not keep an otherwise unused allocation alive\n+  if (tf()->returns_inline_type_as_fields()) {\n+    Node* ret = nullptr;\n+    for (uint i = 1; i < root()->req(); i++) {\n+      Node* in = root()->in(i);\n+      if (in->Opcode() == Op_Return) {\n+        assert(ret == nullptr, \"only one return\");\n+        ret = in;\n+      }\n+    }\n+    if (ret != nullptr) {\n+      Node* ret_val = ret->in(TypeFunc::Parms);\n+      if (igvn.type(ret_val)->isa_oopptr() &&\n+          return_val_keeps_allocations_alive(ret_val)) {\n+        igvn.replace_input_of(ret, TypeFunc::Parms, InlineTypeNode::tagged_klass(igvn.type(ret_val)->inline_klass(), igvn));\n+        assert(ret_val->outcnt() == 0, \"should be dead now\");\n+        igvn.remove_dead_node(ret_val);\n+      }\n+    }\n+  }\n+  if (_inline_type_nodes.length() == 0) {\n+    return;\n+  }\n+  \/\/ Scalarize inline types in safepoint debug info.\n+  \/\/ Delay this until all inlining is over to avoid getting inconsistent debug info.\n+  set_scalarize_in_safepoints(true);\n+  for (int i = _inline_type_nodes.length()-1; i >= 0; i--) {\n+    InlineTypeNode* vt = _inline_type_nodes.at(i)->as_InlineType();\n+    vt->make_scalar_in_safepoints(&igvn);\n+    igvn.record_for_igvn(vt);\n+  }\n+  if (remove) {\n+    \/\/ Remove inline type nodes by replacing them with their oop input\n+    while (_inline_type_nodes.length() > 0) {\n+      InlineTypeNode* vt = _inline_type_nodes.pop()->as_InlineType();\n+      if (vt->outcnt() == 0) {\n+        igvn.remove_dead_node(vt);\n+        continue;\n+      }\n+      for (DUIterator i = vt->outs(); vt->has_out(i); i++) {\n+        DEBUG_ONLY(bool must_be_buffered = false);\n+        Node* u = vt->out(i);\n+        \/\/ Check if any users are blackholes. If so, rewrite them to use either the\n+        \/\/ allocated buffer, or individual components, instead of the inline type node\n+        \/\/ that goes away.\n+        if (u->is_Blackhole()) {\n+          BlackholeNode* bh = u->as_Blackhole();\n+\n+          \/\/ Unlink the old input\n+          int idx = bh->find_edge(vt);\n+          assert(idx != -1, \"The edge should be there\");\n+          bh->del_req(idx);\n+          --i;\n+\n+          if (vt->is_allocated(&igvn)) {\n+            \/\/ Already has the allocated instance, blackhole that\n+            bh->add_req(vt->get_oop());\n+          } else {\n+            \/\/ Not allocated yet, blackhole the components\n+            for (uint c = 0; c < vt->field_count(); c++) {\n+              bh->add_req(vt->field_value(c));\n+            }\n+          }\n+\n+          \/\/ Node modified, record for IGVN\n+          igvn.record_for_igvn(bh);\n+        }\n+#ifdef ASSERT\n+        \/\/ Verify that inline type is buffered when replacing by oop\n+        else if (u->is_InlineType()) {\n+          \/\/ InlineType uses don't need buffering because they are about to be replaced as well\n+        } else if (u->is_Phi()) {\n+          \/\/ TODO 8302217 Remove this once InlineTypeNodes are reliably pushed through\n+        } else {\n+          must_be_buffered = true;\n+        }\n+        if (must_be_buffered && !vt->is_allocated(&igvn)) {\n+          vt->dump(0);\n+          u->dump(0);\n+          assert(false, \"Should have been buffered\");\n+        }\n+#endif\n+      }\n+      igvn.replace_node(vt, vt->get_oop());\n+    }\n+  }\n+  igvn.optimize();\n+}\n+\n+void Compile::adjust_flat_array_access_aliases(PhaseIterGVN& igvn) {\n+  if (!_has_flat_accesses) {\n+    return;\n+  }\n+  \/\/ Initially, all flat array accesses share the same slice to\n+  \/\/ keep dependencies with Object[] array accesses (that could be\n+  \/\/ to a flat array) correct. We're done with parsing so we\n+  \/\/ now know all flat array accesses in this compile\n+  \/\/ unit. Let's move flat array accesses to their own slice,\n+  \/\/ one per element field. This should help memory access\n+  \/\/ optimizations.\n+  ResourceMark rm;\n+  Unique_Node_List wq;\n+  wq.push(root());\n+\n+  Node_List mergememnodes;\n+  Node_List memnodes;\n+\n+  \/\/ Alias index currently shared by all flat memory accesses\n+  int index = get_alias_index(TypeAryPtr::INLINES);\n+\n+  \/\/ Find MergeMem nodes and flat array accesses\n+  for (uint i = 0; i < wq.size(); i++) {\n+    Node* n = wq.at(i);\n+    if (n->is_Mem()) {\n+      const TypePtr* adr_type = nullptr;\n+      adr_type = get_adr_type(get_alias_index(n->adr_type()));\n+      if (adr_type == TypeAryPtr::INLINES) {\n+        memnodes.push(n);\n+      }\n+    } else if (n->is_MergeMem()) {\n+      MergeMemNode* mm = n->as_MergeMem();\n+      if (mm->memory_at(index) != mm->base_memory()) {\n+        mergememnodes.push(n);\n+      }\n+    }\n+    for (uint j = 0; j < n->req(); j++) {\n+      Node* m = n->in(j);\n+      if (m != nullptr) {\n+        wq.push(m);\n+      }\n+    }\n+  }\n+\n+  if (memnodes.size() > 0) {\n+    _flat_accesses_share_alias = false;\n+\n+    \/\/ We are going to change the slice for the flat array\n+    \/\/ accesses so we need to clear the cache entries that refer to\n+    \/\/ them.\n+    for (uint i = 0; i < AliasCacheSize; i++) {\n+      AliasCacheEntry* ace = &_alias_cache[i];\n+      if (ace->_adr_type != nullptr &&\n+          ace->_adr_type->is_flat()) {\n+        ace->_adr_type = nullptr;\n+        ace->_index = (i != 0) ? 0 : AliasIdxTop; \/\/ Make sure the nullptr adr_type resolves to AliasIdxTop\n+      }\n+    }\n+\n+    \/\/ Find what aliases we are going to add\n+    int start_alias = num_alias_types()-1;\n+    int stop_alias = 0;\n+\n+    for (uint i = 0; i < memnodes.size(); i++) {\n+      Node* m = memnodes.at(i);\n+      const TypePtr* adr_type = nullptr;\n+      adr_type = m->adr_type();\n+#ifdef ASSERT\n+      m->as_Mem()->set_adr_type(adr_type);\n+#endif\n+      int idx = get_alias_index(adr_type);\n+      start_alias = MIN2(start_alias, idx);\n+      stop_alias = MAX2(stop_alias, idx);\n+    }\n+\n+    assert(stop_alias >= start_alias, \"should have expanded aliases\");\n+\n+    Node_Stack stack(0);\n+#ifdef ASSERT\n+    VectorSet seen(Thread::current()->resource_area());\n+#endif\n+    \/\/ Now let's fix the memory graph so each flat array access\n+    \/\/ is moved to the right slice. Start from the MergeMem nodes.\n+    uint last = unique();\n+    for (uint i = 0; i < mergememnodes.size(); i++) {\n+      MergeMemNode* current = mergememnodes.at(i)->as_MergeMem();\n+      Node* n = current->memory_at(index);\n+      MergeMemNode* mm = nullptr;\n+      do {\n+        \/\/ Follow memory edges through memory accesses, phis and\n+        \/\/ narrow membars and push nodes on the stack. Once we hit\n+        \/\/ bottom memory, we pop element off the stack one at a\n+        \/\/ time, in reverse order, and move them to the right slice\n+        \/\/ by changing their memory edges.\n+        if ((n->is_Phi() && n->adr_type() != TypePtr::BOTTOM) || n->is_Mem() || n->adr_type() == TypeAryPtr::INLINES) {\n+          assert(!seen.test_set(n->_idx), \"\");\n+          \/\/ Uses (a load for instance) will need to be moved to the\n+          \/\/ right slice as well and will get a new memory state\n+          \/\/ that we don't know yet. The use could also be the\n+          \/\/ backedge of a loop. We put a place holder node between\n+          \/\/ the memory node and its uses. We replace that place\n+          \/\/ holder with the correct memory state once we know it,\n+          \/\/ i.e. when nodes are popped off the stack. Using the\n+          \/\/ place holder make the logic work in the presence of\n+          \/\/ loops.\n+          if (n->outcnt() > 1) {\n+            Node* place_holder = nullptr;\n+            assert(!n->has_out_with(Op_Node), \"\");\n+            for (DUIterator k = n->outs(); n->has_out(k); k++) {\n+              Node* u = n->out(k);\n+              if (u != current && u->_idx < last) {\n+                bool success = false;\n+                for (uint l = 0; l < u->req(); l++) {\n+                  if (!stack.is_empty() && u == stack.node() && l == stack.index()) {\n+                    continue;\n+                  }\n+                  Node* in = u->in(l);\n+                  if (in == n) {\n+                    if (place_holder == nullptr) {\n+                      place_holder = new Node(1);\n+                      place_holder->init_req(0, n);\n+                    }\n+                    igvn.replace_input_of(u, l, place_holder);\n+                    success = true;\n+                  }\n+                }\n+                if (success) {\n+                  --k;\n+                }\n+              }\n+            }\n+          }\n+          if (n->is_Phi()) {\n+            stack.push(n, 1);\n+            n = n->in(1);\n+          } else if (n->is_Mem()) {\n+            stack.push(n, n->req());\n+            n = n->in(MemNode::Memory);\n+          } else {\n+            assert(n->is_Proj() && n->in(0)->Opcode() == Op_MemBarCPUOrder, \"\");\n+            stack.push(n, n->req());\n+            n = n->in(0)->in(TypeFunc::Memory);\n+          }\n+        } else {\n+          assert(n->adr_type() == TypePtr::BOTTOM || (n->Opcode() == Op_Node && n->_idx >= last) || (n->is_Proj() && n->in(0)->is_Initialize()), \"\");\n+          \/\/ Build a new MergeMem node to carry the new memory state\n+          \/\/ as we build it. IGVN should fold extraneous MergeMem\n+          \/\/ nodes.\n+          mm = MergeMemNode::make(n);\n+          igvn.register_new_node_with_optimizer(mm);\n+          while (stack.size() > 0) {\n+            Node* m = stack.node();\n+            uint idx = stack.index();\n+            if (m->is_Mem()) {\n+              \/\/ Move memory node to its new slice\n+              const TypePtr* adr_type = m->adr_type();\n+              int alias = get_alias_index(adr_type);\n+              Node* prev = mm->memory_at(alias);\n+              igvn.replace_input_of(m, MemNode::Memory, prev);\n+              mm->set_memory_at(alias, m);\n+            } else if (m->is_Phi()) {\n+              \/\/ We need as many new phis as there are new aliases\n+              igvn.replace_input_of(m, idx, mm);\n+              if (idx == m->req()-1) {\n+                Node* r = m->in(0);\n+                for (uint j = (uint)start_alias; j <= (uint)stop_alias; j++) {\n+                  const TypePtr* adr_type = get_adr_type(j);\n+                  if (!adr_type->isa_aryptr() || !adr_type->is_flat() || j == (uint)index) {\n+                    continue;\n+                  }\n+                  Node* phi = new PhiNode(r, Type::MEMORY, get_adr_type(j));\n+                  igvn.register_new_node_with_optimizer(phi);\n+                  for (uint k = 1; k < m->req(); k++) {\n+                    phi->init_req(k, m->in(k)->as_MergeMem()->memory_at(j));\n+                  }\n+                  mm->set_memory_at(j, phi);\n+                }\n+                Node* base_phi = new PhiNode(r, Type::MEMORY, TypePtr::BOTTOM);\n+                igvn.register_new_node_with_optimizer(base_phi);\n+                for (uint k = 1; k < m->req(); k++) {\n+                  base_phi->init_req(k, m->in(k)->as_MergeMem()->base_memory());\n+                }\n+                mm->set_base_memory(base_phi);\n+              }\n+            } else {\n+              \/\/ This is a MemBarCPUOrder node from\n+              \/\/ Parse::array_load()\/Parse::array_store(), in the\n+              \/\/ branch that handles flat arrays hidden under\n+              \/\/ an Object[] array. We also need one new membar per\n+              \/\/ new alias to keep the unknown access that the\n+              \/\/ membars protect properly ordered with accesses to\n+              \/\/ known flat array.\n+              assert(m->is_Proj(), \"projection expected\");\n+              Node* ctrl = m->in(0)->in(TypeFunc::Control);\n+              igvn.replace_input_of(m->in(0), TypeFunc::Control, top());\n+              for (uint j = (uint)start_alias; j <= (uint)stop_alias; j++) {\n+                const TypePtr* adr_type = get_adr_type(j);\n+                if (!adr_type->isa_aryptr() || !adr_type->is_flat() || j == (uint)index) {\n+                  continue;\n+                }\n+                MemBarNode* mb = new MemBarCPUOrderNode(this, j, nullptr);\n+                igvn.register_new_node_with_optimizer(mb);\n+                Node* mem = mm->memory_at(j);\n+                mb->init_req(TypeFunc::Control, ctrl);\n+                mb->init_req(TypeFunc::Memory, mem);\n+                ctrl = new ProjNode(mb, TypeFunc::Control);\n+                igvn.register_new_node_with_optimizer(ctrl);\n+                mem = new ProjNode(mb, TypeFunc::Memory);\n+                igvn.register_new_node_with_optimizer(mem);\n+                mm->set_memory_at(j, mem);\n+              }\n+              igvn.replace_node(m->in(0)->as_Multi()->proj_out(TypeFunc::Control), ctrl);\n+            }\n+            if (idx < m->req()-1) {\n+              idx += 1;\n+              stack.set_index(idx);\n+              n = m->in(idx);\n+              break;\n+            }\n+            \/\/ Take care of place holder nodes\n+            if (m->has_out_with(Op_Node)) {\n+              Node* place_holder = m->find_out_with(Op_Node);\n+              if (place_holder != nullptr) {\n+                Node* mm_clone = mm->clone();\n+                igvn.register_new_node_with_optimizer(mm_clone);\n+                Node* hook = new Node(1);\n+                hook->init_req(0, mm);\n+                igvn.replace_node(place_holder, mm_clone);\n+                hook->destruct(&igvn);\n+              }\n+              assert(!m->has_out_with(Op_Node), \"place holder should be gone now\");\n+            }\n+            stack.pop();\n+          }\n+        }\n+      } while(stack.size() > 0);\n+      \/\/ Fix the memory state at the MergeMem we started from\n+      igvn.rehash_node_delayed(current);\n+      for (uint j = (uint)start_alias; j <= (uint)stop_alias; j++) {\n+        const TypePtr* adr_type = get_adr_type(j);\n+        if (!adr_type->isa_aryptr() || !adr_type->is_flat()) {\n+          continue;\n+        }\n+        current->set_memory_at(j, mm);\n+      }\n+      current->set_memory_at(index, current->base_memory());\n+    }\n+    igvn.optimize();\n+  }\n+  print_method(PHASE_SPLIT_INLINES_ARRAY, 2);\n+#ifdef ASSERT\n+  if (!_flat_accesses_share_alias) {\n+    wq.clear();\n+    wq.push(root());\n+    for (uint i = 0; i < wq.size(); i++) {\n+      Node* n = wq.at(i);\n+      assert(n->adr_type() != TypeAryPtr::INLINES, \"should have been removed from the graph\");\n+      for (uint j = 0; j < n->req(); j++) {\n+        Node* m = n->in(j);\n+        if (m != nullptr) {\n+          wq.push(m);\n+        }\n+      }\n+    }\n+  }\n+#endif\n+}\n+\n@@ -1950,1 +2418,1 @@\n-        if (!live_locals.at(i) && !local->is_top() && local != lhs && local!= rhs) {\n+        if (!live_locals.at(i) && !local->is_top() && local != lhs && local != rhs) {\n@@ -1965,1 +2433,1 @@\n-    \/\/ keep the mondified trap for late query\n+    \/\/ keep the modified trap for late query\n@@ -2160,1 +2628,4 @@\n-  assert(_modified_nodes == nullptr, \"not allowed\");\n+#ifdef ASSERT\n+  Unique_Node_List* modified_nodes = _modified_nodes;\n+  _modified_nodes = nullptr;\n+#endif\n@@ -2173,0 +2644,1 @@\n+  DEBUG_ONLY( _modified_nodes = modified_nodes; )\n@@ -2319,0 +2791,5 @@\n+  \/\/ Process inline type nodes now that all inlining is over\n+  process_inline_types(igvn);\n+\n+  adjust_flat_array_access_aliases(igvn);\n+\n@@ -2442,0 +2919,8 @@\n+  assert(_late_inlines.length() == 0 || IncrementalInlineMH || IncrementalInlineVirtual, \"not empty\");\n+\n+  if (_late_inlines.length() > 0) {\n+    \/\/ More opportunities to optimize virtual and MH calls.\n+    \/\/ Though it's maybe too late to perform inlining, strength-reducing them to direct calls is still an option.\n+    process_late_inline_calls_no_inline(igvn);\n+  }\n+\n@@ -2453,0 +2938,4 @@\n+  \/\/ Process inline type nodes again and remove them. From here\n+  \/\/ on we don't need to keep track of field values anymore.\n+  process_inline_types(igvn, \/* remove= *\/ true);\n+\n@@ -2469,0 +2958,1 @@\n+  DEBUG_ONLY( _late_inlines.clear(); )\n@@ -2471,9 +2961,0 @@\n-\n-  assert(_late_inlines.length() == 0 || IncrementalInlineMH || IncrementalInlineVirtual, \"not empty\");\n-\n-  if (_late_inlines.length() > 0) {\n-    \/\/ More opportunities to optimize virtual and MH calls.\n-    \/\/ Though it's maybe too late to perform inlining, strength-reducing them to direct calls is still an option.\n-    process_late_inline_calls_no_inline(igvn);\n-    if (failing())  return;\n-  }\n@@ -3785,0 +4266,7 @@\n+#ifdef ASSERT\n+  case Op_InlineType: {\n+    n->dump(-1);\n+    assert(false, \"inline type node was not removed\");\n+    break;\n+  }\n+#endif\n@@ -4164,2 +4652,2 @@\n-      if (accessing_method->is_static_initializer() ||\n-          accessing_method->is_object_initializer() ||\n+      if (accessing_method->is_class_initializer() ||\n+          accessing_method->is_object_constructor() ||\n@@ -4173,1 +4661,1 @@\n-      if (accessing_method->is_static_initializer()) {\n+      if (accessing_method->is_class_initializer()) {\n@@ -4229,0 +4717,1 @@\n+               (n->is_Allocate() && i >= AllocateNode::InlineType) ||\n@@ -4231,1 +4720,1 @@\n-              \"only region, phi, arraycopy, unlock or membar nodes have null data edges\");\n+              \"only region, phi, arraycopy, allocate, unlock or membar nodes have null data edges\");\n@@ -4365,0 +4854,7 @@\n+\n+    \/\/ Do not fold the subtype check to an array klass pointer comparison for null-able inline type arrays\n+    \/\/ because null-free [LMyValue <: null-able [LMyValue but the klasses are different. Perform a full test.\n+    if (!superk->is_aryklassptr()->is_null_free() && superk->is_aryklassptr()->elem()->isa_instklassptr() &&\n+        superk->is_aryklassptr()->elem()->is_instklassptr()->instance_klass()->is_inlinetype()) {\n+      return SSC_full_test;\n+    }\n@@ -4964,0 +5460,21 @@\n+Node* Compile::optimize_acmp(PhaseGVN* phase, Node* a, Node* b) {\n+  const TypeInstPtr* ta = phase->type(a)->isa_instptr();\n+  const TypeInstPtr* tb = phase->type(b)->isa_instptr();\n+  if (!EnableValhalla || ta == nullptr || tb == nullptr ||\n+      ta->is_zero_type() || tb->is_zero_type() ||\n+      !ta->can_be_inline_type() || !tb->can_be_inline_type()) {\n+    \/\/ Use old acmp if one operand is null or not an inline type\n+    return new CmpPNode(a, b);\n+  } else if (ta->is_inlinetypeptr() || tb->is_inlinetypeptr()) {\n+    \/\/ We know that one operand is an inline type. Therefore,\n+    \/\/ new acmp will only return true if both operands are nullptr.\n+    \/\/ Check if both operands are null by or'ing the oops.\n+    a = phase->transform(new CastP2XNode(nullptr, a));\n+    b = phase->transform(new CastP2XNode(nullptr, b));\n+    a = phase->transform(new OrXNode(a, b));\n+    return new CmpXNode(a, phase->MakeConX(0));\n+  }\n+  \/\/ Use new acmp\n+  return nullptr;\n+}\n+\n","filename":"src\/hotspot\/share\/opto\/compile.cpp","additions":571,"deletions":54,"binary":false,"changes":625,"status":"modified"},{"patch":"@@ -55,0 +55,1 @@\n+class CallNode;\n@@ -97,0 +98,1 @@\n+class InlineTypeNode;\n@@ -333,0 +335,1 @@\n+  bool                  _has_circular_inline_type; \/\/ True if method loads an inline type with a circular, non-flat field\n@@ -360,0 +363,3 @@\n+  bool                  _has_flat_accesses;     \/\/ Any known flat array accesses?\n+  bool                  _flat_accesses_share_alias; \/\/ Initially all flat array share a single slice\n+  bool                  _scalarize_in_safepoints; \/\/ Scalarize inline types in safepoint debug info\n@@ -376,0 +382,1 @@\n+  GrowableArray<Node*>  _inline_type_nodes;     \/\/ List of InlineType nodes\n@@ -644,0 +651,2 @@\n+  bool              has_circular_inline_type() const { return _has_circular_inline_type; }\n+  void          set_has_circular_inline_type(bool z) { _has_circular_inline_type = z; }\n@@ -676,0 +685,10 @@\n+  void          set_flat_accesses()              { _has_flat_accesses = true; }\n+  bool          flat_accesses_share_alias() const { return _flat_accesses_share_alias; }\n+  void          set_flat_accesses_share_alias(bool z) { _flat_accesses_share_alias = z; }\n+  bool          scalarize_in_safepoints() const { return _scalarize_in_safepoints; }\n+  void          set_scalarize_in_safepoints(bool z) { _scalarize_in_safepoints = z; }\n+\n+  \/\/ Support for scalarized inline type calling convention\n+  bool              has_scalarized_args() const  { return _method != nullptr && _method->has_scalarized_args(); }\n+  bool              needs_stack_repair()  const  { return _method != nullptr && _method->get_Method()->c2_needs_stack_repair(); }\n+\n@@ -804,0 +823,7 @@\n+  \/\/ Keep track of inline type nodes for later processing\n+  void add_inline_type(Node* n);\n+  void remove_inline_type(Node* n);\n+  void process_inline_types(PhaseIterGVN &igvn, bool remove = false);\n+\n+  void adjust_flat_array_access_aliases(PhaseIterGVN& igvn);\n+\n@@ -982,1 +1008,1 @@\n-  AliasType*        alias_type(const TypePtr* adr_type, ciField* field = nullptr) { return find_alias_type(adr_type, false, field); }\n+  AliasType*        alias_type(const TypePtr* adr_type, ciField* field = nullptr, bool uncached = false) { return find_alias_type(adr_type, false, field, uncached); }\n@@ -986,1 +1012,1 @@\n-  int               get_alias_index(const TypePtr* at)  { return alias_type(at)->index(); }\n+  int               get_alias_index(const TypePtr* at, bool uncached = false) { return alias_type(at, nullptr, uncached)->index(); }\n@@ -1228,1 +1254,1 @@\n-  AliasType* find_alias_type(const TypePtr* adr_type, bool no_create, ciField* field);\n+  AliasType* find_alias_type(const TypePtr* adr_type, bool no_create, ciField* field, bool uncached = false);\n@@ -1304,1 +1330,3 @@\n-  \/\/ Auxiliary methods for randomized fuzzing\/stressing\n+  Node* optimize_acmp(PhaseGVN* phase, Node* a, Node* b);\n+\n+  \/\/ Auxiliary method for randomized fuzzing\/stressing\n","filename":"src\/hotspot\/share\/opto\/compile.hpp","additions":32,"deletions":4,"binary":false,"changes":36,"status":"modified"},{"patch":"@@ -32,0 +32,1 @@\n+#include \"memory\/metaspace.hpp\"\n@@ -39,0 +40,1 @@\n+#include \"opto\/inlinetypenode.hpp\"\n@@ -167,0 +169,10 @@\n+    if ((n->Opcode() == Op_LoadX || n->Opcode() == Op_StoreX) &&\n+        !n->in(MemNode::Address)->is_AddP() &&\n+        _igvn->type(n->in(MemNode::Address))->isa_oopptr()) {\n+      \/\/ Load\/Store at mark work address is at offset 0 so has no AddP which confuses EA\n+      Node* addp = new AddPNode(n->in(MemNode::Address), n->in(MemNode::Address), _igvn->MakeConX(0));\n+      _igvn->register_new_node_with_optimizer(addp);\n+      _igvn->replace_input_of(n, MemNode::Address, addp);\n+      ideal_nodes.push(addp);\n+      _nodes.at_put_grow(addp->_idx, nullptr, nullptr);\n+    }\n@@ -1250,1 +1262,9 @@\n-      SafePointScalarObjectNode* sobj = mexp.create_scalarized_object_description(alloc, sfpt);\n+      Unique_Node_List value_worklist;\n+#ifdef ASSERT\n+      const Type* res_type = alloc->result_cast()->bottom_type();\n+      if (res_type->is_inlinetypeptr() && !Compile::current()->has_circular_inline_type()) {\n+        PhiNode* phi = ophi->as_Phi();\n+        assert(!ophi->as_Phi()->can_push_inline_types_down(_igvn), \"missed earlier scalarization opportunity\");\n+      }\n+#endif\n+      SafePointScalarObjectNode* sobj = mexp.create_scalarized_object_description(alloc, sfpt, &value_worklist);\n@@ -1252,0 +1272,1 @@\n+        _compile->record_failure(C2Compiler::retry_no_reduce_allocation_merges());\n@@ -1262,0 +1283,9 @@\n+\n+      \/\/ Scalarize inline types that were added to the safepoint.\n+      \/\/ Don't allow linking a constant oop (if available) for flat array elements\n+      \/\/ because Deoptimization::reassign_flat_array_elements needs field values.\n+      const bool allow_oop = !merge_t->is_flat();\n+      for (uint j = 0; j < value_worklist.size(); ++j) {\n+        InlineTypeNode* vt = value_worklist.at(j)->as_InlineType();\n+        vt->make_scalar_in_safepoints(_igvn, allow_oop);\n+      }\n@@ -1460,1 +1490,1 @@\n-      const TypeTuple* d = call->tf()->domain();\n+      const TypeTuple* d = call->tf()->domain_sig();\n@@ -1534,0 +1564,11 @@\n+      } else if (n->as_Call()->tf()->returns_inline_type_as_fields()) {\n+        bool returns_oop = false;\n+        for (DUIterator_Fast imax, i = n->fast_outs(imax); i < imax && !returns_oop; i++) {\n+          ProjNode* pn = n->fast_out(i)->as_Proj();\n+          if (pn->_con >= TypeFunc::Parms && pn->bottom_type()->isa_ptr()) {\n+            returns_oop = true;\n+          }\n+        }\n+        if (returns_oop) {\n+          add_call_node(n->as_Call());\n+        }\n@@ -1565,0 +1606,1 @@\n+    case Op_InlineType:\n@@ -1636,2 +1678,4 @@\n-      if (n->as_Proj()->_con == TypeFunc::Parms && n->in(0)->is_Call() &&\n-          n->in(0)->as_Call()->returns_pointer()) {\n+      if (n->as_Proj()->_con >= TypeFunc::Parms && n->in(0)->is_Call() &&\n+          (n->in(0)->as_Call()->returns_pointer() || n->bottom_type()->isa_ptr())) {\n+        assert((n->as_Proj()->_con == TypeFunc::Parms && n->in(0)->as_Call()->returns_pointer()) ||\n+               n->in(0)->as_Call()->tf()->returns_inline_type_as_fields(), \"what kind of oop return is it?\");\n@@ -1739,0 +1783,1 @@\n+    case Op_InlineType:\n@@ -1793,2 +1838,2 @@\n-      assert(n->as_Proj()->_con == TypeFunc::Parms && n->in(0)->is_Call() &&\n-             n->in(0)->as_Call()->returns_pointer(), \"Unexpected node type\");\n+      assert((n->as_Proj()->_con == TypeFunc::Parms && n->in(0)->as_Call()->returns_pointer()) ||\n+             n->in(0)->as_Call()->tf()->returns_inline_type_as_fields(), \"what kind of oop return is it?\");\n@@ -1970,1 +2015,1 @@\n-  assert(call->returns_pointer(), \"only for call which returns pointer\");\n+  assert(call->returns_pointer() || call->tf()->returns_inline_type_as_fields(), \"only for call which returns pointer\");\n@@ -2046,1 +2091,2 @@\n-      assert(strncmp(name, \"C2 Runtime multianewarray\", 25) == 0, \"TODO: add failed case check\");\n+      assert(strncmp(name, \"C2 Runtime multianewarray\", 25) == 0 ||\n+             strncmp(name, \"C2 Runtime load_unknown_inline\", 30) == 0, \"TODO: add failed case check\");\n@@ -2077,1 +2123,1 @@\n-        const TypeTuple* d = call->tf()->domain();\n+        const TypeTuple* d = call->tf()->domain_cc();\n@@ -2125,1 +2171,1 @@\n-      const TypeTuple * d = call->tf()->domain();\n+      const TypeTuple * d = call->tf()->domain_sig();\n@@ -2156,1 +2202,4 @@\n-                               (aat->isa_aryptr() && (aat->isa_aryptr()->elem() == Type::BOTTOM || aat->isa_aryptr()->elem()->make_oopptr() != nullptr)));\n+                               (aat->isa_aryptr() && (aat->isa_aryptr()->elem() == Type::BOTTOM || aat->isa_aryptr()->elem()->make_oopptr() != nullptr)) ||\n+                               (aat->isa_aryptr() && aat->isa_aryptr()->elem() != nullptr &&\n+                                                               aat->isa_aryptr()->is_flat() &&\n+                                                               aat->isa_aryptr()->elem()->inline_klass()->contains_oops()));\n@@ -2207,0 +2256,3 @@\n+                  strcmp(call->as_CallLeaf()->_name, \"vectorizedMismatch\") == 0 ||\n+                  strcmp(call->as_CallLeaf()->_name, \"load_unknown_inline\") == 0 ||\n+                  strcmp(call->as_CallLeaf()->_name, \"store_unknown_inline\") == 0 ||\n@@ -2273,1 +2325,1 @@\n-        const TypeTuple* d = call->tf()->domain();\n+        const TypeTuple* d = call->tf()->domain_cc();\n@@ -2317,1 +2369,1 @@\n-      const TypeTuple* d = call->tf()->domain();\n+      const TypeTuple* d = call->tf()->domain_cc();\n@@ -2730,0 +2782,1 @@\n+  PointsToNode* init_val = phantom_obj;\n@@ -2735,1 +2788,8 @@\n-    return 0;\n+    if (alloc->as_Allocate()->in(AllocateNode::DefaultValue) != nullptr) {\n+      \/\/ Non-flat inline type arrays are initialized with\n+      \/\/ the default value instead of null. Handle them here.\n+      init_val = ptnode_adr(alloc->as_Allocate()->in(AllocateNode::DefaultValue)->_idx);\n+      assert(init_val != nullptr, \"default value should be registered\");\n+    } else {\n+      return 0;\n+    }\n@@ -2737,1 +2797,2 @@\n-  assert(pta->arraycopy_dst() || alloc->as_CallStaticJava(), \"sanity\");\n+  \/\/ Non-escaped allocation returned from Java or runtime call has unknown values in fields.\n+  assert(pta->arraycopy_dst() || alloc->is_CallStaticJava() || init_val != phantom_obj, \"sanity\");\n@@ -2739,1 +2800,1 @@\n-  if (!pta->arraycopy_dst() && alloc->as_CallStaticJava()->method() == nullptr) {\n+  if (alloc->is_CallStaticJava() && alloc->as_CallStaticJava()->method() == nullptr) {\n@@ -2741,1 +2802,2 @@\n-    assert(strncmp(name, \"C2 Runtime multianewarray\", 25) == 0, \"sanity\");\n+    assert(strncmp(name, \"C2 Runtime multianewarray\", 25) == 0 ||\n+           strncmp(name, \"C2 Runtime load_unknown_inline\", 30) == 0, \"sanity\");\n@@ -2749,1 +2811,1 @@\n-      if (add_edge(field, phantom_obj)) {\n+      if (add_edge(field, init_val)) {\n@@ -2764,1 +2826,1 @@\n-  if (!alloc->is_Allocate()) {\n+  if (!alloc->is_Allocate() || alloc->as_Allocate()->in(AllocateNode::DefaultValue) != nullptr) {\n@@ -2850,1 +2912,1 @@\n-                tty->print_cr(\"----------missed referernce to object-----------\");\n+                tty->print_cr(\"----------missed reference to object------------\");\n@@ -2852,1 +2914,1 @@\n-                tty->print_cr(\"----------object referernced by init store -----\");\n+                tty->print_cr(\"----------object referenced by init store-------\");\n@@ -3169,1 +3231,2 @@\n-          if (can_eliminate_lock(alock)) {\n+          const Type* obj_type = igvn->type(alock->obj_node());\n+          if (can_eliminate_lock(alock) && !obj_type->is_inlinetypeptr()) {\n@@ -3211,5 +3274,10 @@\n-      MemBarNode* mb = MemBarNode::make(C, Op_MemBarCPUOrder, Compile::AliasIdxBot);\n-      mb->init_req(TypeFunc::Memory,  storestore->in(TypeFunc::Memory));\n-      mb->init_req(TypeFunc::Control, storestore->in(TypeFunc::Control));\n-      igvn->register_new_node_with_optimizer(mb);\n-      igvn->replace_node(storestore, mb);\n+      if (alloc->in(AllocateNode::InlineType) != nullptr) {\n+        \/\/ Non-escaping inline type buffer allocations don't require a membar\n+        storestore->as_MemBar()->remove(_igvn);\n+      } else {\n+        MemBarNode* mb = MemBarNode::make(C, Op_MemBarCPUOrder, Compile::AliasIdxBot);\n+        mb->init_req(TypeFunc::Memory,  storestore->in(TypeFunc::Memory));\n+        mb->init_req(TypeFunc::Control, storestore->in(TypeFunc::Control));\n+        igvn->register_new_node_with_optimizer(mb);\n+        igvn->replace_node(storestore, mb);\n+      }\n@@ -3377,0 +3445,1 @@\n+  int field_offset = adr_type->isa_aryptr() ? adr_type->isa_aryptr()->field_offset().get() : Type::OffsetBot;\n@@ -3378,1 +3447,1 @@\n-  if (offset == Type::OffsetBot) {\n+  if (offset == Type::OffsetBot && field_offset == Type::OffsetBot) {\n@@ -3390,1 +3459,1 @@\n-      ciField* field = _compile->alias_type(adr_type->isa_instptr())->field();\n+      ciField* field = _compile->alias_type(adr_type->is_ptr())->field();\n@@ -3409,2 +3478,8 @@\n-        const Type* elemtype = adr_type->isa_aryptr()->elem();\n-        bt = elemtype->array_element_basic_type();\n+        const Type* elemtype = adr_type->is_aryptr()->elem();\n+        if (adr_type->is_aryptr()->is_flat() && field_offset != Type::OffsetBot) {\n+          ciInlineKlass* vk = elemtype->inline_klass();\n+          field_offset += vk->first_field_offset();\n+          bt = vk->get_field_by_offset(field_offset, false)->layout_type();\n+        } else {\n+          bt = elemtype->array_element_basic_type();\n+        }\n@@ -3607,3 +3682,1 @@\n-  const TypePtr *t_ptr = adr_type->isa_ptr();\n-  assert(t_ptr != nullptr, \"must be a pointer type\");\n-  return t_ptr->offset();\n+  return adr_type->is_ptr()->flat_offset();\n@@ -3763,1 +3836,8 @@\n-    t = base_t->add_offset(offs)->is_oopptr();\n+    if (base_t->isa_aryptr() != nullptr) {\n+      \/\/ In the case of a flat inline type array, each field has its\n+      \/\/ own slice so we need to extract the field being accessed from\n+      \/\/ the address computation\n+      t = base_t->isa_aryptr()->add_field_offset_and_offset(offs)->is_oopptr();\n+    } else {\n+      t = base_t->add_offset(offs)->is_oopptr();\n+    }\n@@ -3765,1 +3845,1 @@\n-  int inst_id =  base_t->instance_id();\n+  int inst_id = base_t->instance_id();\n@@ -3779,1 +3859,1 @@\n-  \/\/ It could happened when CHA type is different from MDO type on a dead path\n+  \/\/ It could happen when CHA type is different from MDO type on a dead path\n@@ -3789,1 +3869,12 @@\n-  const TypeOopPtr *tinst = base_t->add_offset(t->offset())->is_oopptr();\n+  const TypePtr* tinst = base_t->add_offset(t->offset());\n+  if (tinst->isa_aryptr() && t->isa_aryptr()) {\n+    \/\/ In the case of a flat inline type array, each field has its\n+    \/\/ own slice so we need to keep track of the field being accessed.\n+    tinst = tinst->is_aryptr()->with_field_offset(t->is_aryptr()->field_offset().get());\n+    \/\/ Keep array properties (not flat\/null-free)\n+    tinst = tinst->is_aryptr()->update_properties(t->is_aryptr());\n+    if (tinst == nullptr) {\n+      return false; \/\/ Skip dead path with inconsistent properties\n+    }\n+  }\n+\n@@ -4495,0 +4586,7 @@\n+          if (tn_t->isa_aryptr()) {\n+            \/\/ Keep array properties (not flat\/null-free)\n+            tinst = tinst->is_aryptr()->update_properties(tn_t->is_aryptr());\n+            if (tinst == nullptr) {\n+              continue; \/\/ Skip dead path with inconsistent properties\n+            }\n+          }\n@@ -4520,1 +4618,1 @@\n-      if(use->is_Mem() && use->in(MemNode::Address) == n) {\n+      if (use->is_Mem() && use->in(MemNode::Address) == n) {\n@@ -4556,0 +4654,3 @@\n+      } else if (use->Opcode() == Op_Return) {\n+        \/\/ Allocation is referenced by field of returned inline type\n+        assert(_compile->tf()->returns_inline_type_as_fields(), \"EA: unexpected reference by ReturnNode\");\n@@ -4569,1 +4670,1 @@\n-              op == Op_SubTypeCheck ||\n+              op == Op_SubTypeCheck || op == Op_InlineType || op == Op_FlatArrayCheck ||\n@@ -4669,0 +4770,3 @@\n+    } else if (n->is_CallLeaf() && n->as_CallLeaf()->_name != nullptr &&\n+               strcmp(n->as_CallLeaf()->_name, \"store_unknown_inline\") == 0) {\n+      n = n->as_CallLeaf()->proj_out(TypeFunc::Memory);\n@@ -4708,1 +4812,1 @@\n-      } else if(use->is_Mem()) {\n+      } else if (use->is_Mem()) {\n@@ -4717,0 +4821,4 @@\n+      } else if (use->is_CallLeaf() && use->as_CallLeaf()->_name != nullptr &&\n+                 strcmp(use->as_CallLeaf()->_name, \"store_unknown_inline\") == 0) {\n+        \/\/ store_unknown_inline overwrites destination array\n+        memnode_worklist.append_if_missing(use);\n@@ -4726,1 +4834,1 @@\n-              op == Op_StrEquals || op == Op_StrIndexOf || op == Op_StrIndexOfChar)) {\n+              op == Op_StrEquals || op == Op_StrIndexOf || op == Op_StrIndexOfChar || op == Op_FlatArrayCheck)) {\n@@ -4827,1 +4935,1 @@\n-  \/\/ chains as is done in split_memory_phi() since they  will\n+  \/\/ chains as is done in split_memory_phi() since they will\n","filename":"src\/hotspot\/share\/opto\/escape.cpp","additions":151,"deletions":43,"binary":false,"changes":194,"status":"modified"},{"patch":"@@ -237,1 +237,1 @@\n-      for (int i = node->req()-1; i >= 0; --i) {\n+      for (int i = node->len()-1; i >= 0; --i) {\n","filename":"src\/hotspot\/share\/opto\/gcm.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -26,0 +26,2 @@\n+#include \"ci\/ciFlatArrayKlass.hpp\"\n+#include \"ci\/ciInlineKlass.hpp\"\n@@ -40,0 +42,1 @@\n+#include \"opto\/inlinetypenode.hpp\"\n@@ -43,0 +46,1 @@\n+#include \"opto\/narrowptrnode.hpp\"\n@@ -56,1 +60,1 @@\n-GraphKit::GraphKit(JVMState* jvms)\n+GraphKit::GraphKit(JVMState* jvms, PhaseGVN* gvn)\n@@ -59,1 +63,1 @@\n-    _gvn(*C->initial_gvn()),\n+    _gvn((gvn != nullptr) ? *gvn : *C->initial_gvn()),\n@@ -62,0 +66,1 @@\n+  assert(gvn == nullptr || !gvn->is_IterGVN() || gvn->is_IterGVN()->delay_transform(), \"delay transform should be enabled\");\n@@ -65,0 +70,7 @@\n+#ifdef ASSERT\n+  if (_gvn.is_IterGVN() != nullptr) {\n+    assert(_gvn.is_IterGVN()->delay_transform(), \"Transformation must be delayed if IterGVN is used\");\n+    \/\/ Save the initial size of _for_igvn worklist for verification (see ~GraphKit)\n+    _worklist_size = _gvn.C->igvn_worklist()->size();\n+  }\n+#endif\n@@ -862,1 +874,1 @@\n-           (is_anewarray && code == Bytecodes::_multianewarray);\n+           (is_anewarray && (code == Bytecodes::_multianewarray));\n@@ -953,0 +965,2 @@\n+\n+  JVMState* callee_jvms = nullptr;\n@@ -978,2 +992,10 @@\n-      for (j = 0; j < l; j++)\n-        call->set_req(p++, in_map->in(k+j));\n+      for (j = 0; j < l; j++) {\n+        Node* val = in_map->in(k + j);\n+        \/\/ Check if there's a larval that has been written in the callee state (constructor) and update it in the caller state\n+        if (callee_jvms != nullptr && val->is_InlineType() && val->as_InlineType()->is_larval() &&\n+            callee_jvms->method()->is_object_constructor() && val == in_map->argument(in_jvms, 0) &&\n+            val->bottom_type()->is_inlinetypeptr()) {\n+          val = callee_jvms->map()->local(callee_jvms, 0); \/\/ Receiver\n+        }\n+        call->set_req(p++, val);\n+      }\n@@ -989,2 +1011,10 @@\n-      for (j = 0; j < l; j++)\n-        call->set_req(p++, in_map->in(k+j));\n+      for (j = 0; j < l; j++) {\n+        Node* val = in_map->in(k + j);\n+        \/\/ Check if there's a larval that has been written in the callee state (constructor) and update it in the caller state\n+        if (callee_jvms != nullptr && val->is_InlineType() && val->as_InlineType()->is_larval() &&\n+            callee_jvms->method()->is_object_constructor() && val == in_map->argument(in_jvms, 0) &&\n+            val->bottom_type()->is_inlinetypeptr()) {\n+          val = callee_jvms->map()->local(callee_jvms, 0); \/\/ Receiver\n+        }\n+        call->set_req(p++, val);\n+      }\n@@ -1029,0 +1059,1 @@\n+    callee_jvms = out_jvms;\n@@ -1204,1 +1235,1 @@\n-  return _gvn.transform(LoadKlassNode::make(_gvn, nullptr, immutable_memory(), k_adr, TypeInstPtr::KLASS));\n+  return _gvn.transform(LoadKlassNode::make(_gvn, nullptr, immutable_memory(), k_adr, TypeInstPtr::KLASS, TypeInstKlassPtr::OBJECT));\n@@ -1253,1 +1284,2 @@\n-                                  bool speculative) {\n+                                  bool speculative,\n+                                  bool is_init_check) {\n@@ -1258,0 +1290,23 @@\n+  if (value->is_InlineType()) {\n+    \/\/ Null checking a scalarized but nullable inline type. Check the IsInit\n+    \/\/ input instead of the oop input to avoid keeping buffer allocations alive.\n+    InlineTypeNode* vtptr = value->as_InlineType();\n+    while (vtptr->get_oop()->is_InlineType()) {\n+      vtptr = vtptr->get_oop()->as_InlineType();\n+    }\n+    null_check_common(vtptr->get_is_init(), T_INT, assert_null, null_control, speculative, true);\n+    if (stopped()) {\n+      return top();\n+    }\n+    if (assert_null) {\n+      \/\/ TODO 8284443 Scalarize here (this currently leads to compilation bailouts)\n+      \/\/ vtptr = InlineTypeNode::make_null(_gvn, vtptr->type()->inline_klass());\n+      \/\/ replace_in_map(value, vtptr);\n+      \/\/ return vtptr;\n+      replace_in_map(value, null());\n+      return null();\n+    }\n+    bool do_replace_in_map = (null_control == nullptr || (*null_control) == top());\n+    return cast_not_null(value, do_replace_in_map);\n+  }\n+\n@@ -1361,1 +1416,1 @@\n-  } else if (type == T_OBJECT) {\n+  } else if (type == T_OBJECT || is_init_check) {\n@@ -1435,1 +1490,0 @@\n-\n@@ -1439,0 +1493,9 @@\n+  if (obj->is_InlineType()) {\n+    Node* vt = obj->isa_InlineType()->clone_if_required(&gvn(), map(), do_replace_in_map);\n+    vt->as_InlineType()->set_is_init(_gvn);\n+    vt = _gvn.transform(vt);\n+    if (do_replace_in_map) {\n+      replace_in_map(obj, vt);\n+    }\n+    return vt;\n+  }\n@@ -1561,1 +1624,2 @@\n-  assert(adr_idx == C->get_alias_index(_gvn.type(adr)->isa_ptr()), \"slice of address and input slice don't match\");\n+  \/\/ Fix 8344108 and renable the commented assert\n+  \/\/assert(adr_idx == C->get_alias_index(_gvn.type(adr)->isa_ptr()), \"slice of address and input slice don't match\");\n@@ -1568,0 +1632,1 @@\n+\n@@ -1591,1 +1656,2 @@\n-  assert(adr_idx == C->get_alias_index(_gvn.type(adr)->isa_ptr()), \"slice of address and input slice don't match\");\n+  \/\/ Fix 8344108 and renable the commented assert\n+  \/\/assert(adr_idx == C->get_alias_index(_gvn.type(adr)->isa_ptr()), \"slice of address and input slice don't match\");\n@@ -1622,1 +1688,2 @@\n-                                DecoratorSet decorators) {\n+                                DecoratorSet decorators,\n+                                bool safe_for_replace) {\n@@ -1635,0 +1702,7 @@\n+  if (val->is_InlineType()) {\n+    \/\/ Store to non-flat field. Buffer the inline type and make sure\n+    \/\/ the store is re-executed if the allocation triggers deoptimization.\n+    PreserveReexecuteState preexecs(this);\n+    jvms()->set_should_reexecute(true);\n+    val = val->as_InlineType()->buffer(this, safe_for_replace);\n+  }\n@@ -1651,1 +1725,2 @@\n-                               DecoratorSet decorators) {\n+                               DecoratorSet decorators,\n+                               Node* ctl) {\n@@ -1657,1 +1732,1 @@\n-  C2ParseAccess access(this, decorators | C2_READ_ACCESS, bt, obj, addr);\n+  C2ParseAccess access(this, decorators | C2_READ_ACCESS, bt, obj, addr, ctl);\n@@ -1762,1 +1837,2 @@\n-  uint shift  = exact_log2(type2aelembytes(elembt));\n+  const TypeAryPtr* arytype = _gvn.type(ary)->is_aryptr();\n+  uint shift = arytype->is_flat() ? arytype->flat_log_elem_size() : exact_log2(type2aelembytes(elembt));\n@@ -1794,6 +1870,63 @@\n-void GraphKit::set_arguments_for_java_call(CallJavaNode* call) {\n-  \/\/ Add the call arguments:\n-  uint nargs = call->method()->arg_size();\n-  for (uint i = 0; i < nargs; i++) {\n-    Node* arg = argument(i);\n-    call->init_req(i + TypeFunc::Parms, arg);\n+void GraphKit::set_arguments_for_java_call(CallJavaNode* call, bool is_late_inline) {\n+  PreserveReexecuteState preexecs(this);\n+  if (EnableValhalla) {\n+    \/\/ Make sure the call is \"re-executed\", if buffering of inline type arguments triggers deoptimization.\n+    \/\/ At this point, the call hasn't been executed yet, so we will only ever execute the call once.\n+    jvms()->set_should_reexecute(true);\n+    int arg_size = method()->get_declared_signature_at_bci(bci())->arg_size_for_bc(java_bc());\n+    inc_sp(arg_size);\n+  }\n+  \/\/ Add the call arguments\n+  const TypeTuple* domain = call->tf()->domain_sig();\n+  uint nargs = domain->cnt();\n+  int arg_num = 0;\n+  for (uint i = TypeFunc::Parms, idx = TypeFunc::Parms; i < nargs; i++) {\n+    Node* arg = argument(i-TypeFunc::Parms);\n+    const Type* t = domain->field_at(i);\n+    \/\/ TODO 8284443 A static call to a mismatched method should still be scalarized\n+    if (t->is_inlinetypeptr() && !call->method()->get_Method()->mismatch() && call->method()->is_scalarized_arg(arg_num)) {\n+      \/\/ We don't pass inline type arguments by reference but instead pass each field of the inline type\n+      if (!arg->is_InlineType()) {\n+        assert(_gvn.type(arg)->is_zero_type() && !t->inline_klass()->is_null_free(), \"Unexpected argument type\");\n+        arg = InlineTypeNode::make_from_oop(this, arg, t->inline_klass(), t->inline_klass()->is_null_free());\n+      }\n+      InlineTypeNode* vt = arg->as_InlineType();\n+      vt->pass_fields(this, call, idx, true, !t->maybe_null());\n+      \/\/ If an inline type argument is passed as fields, attach the Method* to the call site\n+      \/\/ to be able to access the extended signature later via attached_method_before_pc().\n+      \/\/ For example, see CompiledMethod::preserve_callee_argument_oops().\n+      call->set_override_symbolic_info(true);\n+      \/\/ Register an evol dependency on the callee method to make sure that this method is deoptimized and\n+      \/\/ re-compiled with a non-scalarized calling convention if the callee method is later marked as mismatched.\n+      C->dependencies()->assert_evol_method(call->method());\n+      arg_num++;\n+      continue;\n+    } else if (arg->is_InlineType()) {\n+      \/\/ Pass inline type argument via oop to callee\n+      InlineTypeNode* inline_type = arg->as_InlineType();\n+      const ciMethod* method = call->method();\n+      ciInstanceKlass* holder = method->holder();\n+      const bool is_receiver = (i == TypeFunc::Parms);\n+      const bool is_abstract_or_object_klass_constructor = method->is_object_constructor() &&\n+                                                           (holder->is_abstract() || holder->is_java_lang_Object());\n+      const bool is_larval_receiver_on_super_constructor = is_receiver && is_abstract_or_object_klass_constructor;\n+      bool must_init_buffer = true;\n+      \/\/ We always need to buffer inline types when they are escaping. However, we can skip the actual initialization\n+      \/\/ of the buffer if the inline type is a larval because we are going to update the buffer anyway which requires\n+      \/\/ us to create a new one. But there is one special case where we are still required to initialize the buffer:\n+      \/\/ When we have a larval receiver invoked on an abstract (value class) constructor or the Object constructor (that\n+      \/\/ is not going to be inlined). After this call, the larval is completely initialized and thus not a larval anymore.\n+      \/\/ We therefore need to force an initialization of the buffer to not lose all the field writes so far in case the\n+      \/\/ buffer needs to be used (e.g. to read from when deoptimizing at runtime) or further updated in abstract super\n+      \/\/ value class constructors which could have more fields to be initialized. Note that we do not need to\n+      \/\/ initialize the buffer when invoking another constructor in the same class on a larval receiver because we\n+      \/\/ have not initialized any fields, yet (this is done completely by the other constructor call).\n+      if (inline_type->is_larval() && !is_larval_receiver_on_super_constructor) {\n+        must_init_buffer = false;\n+      }\n+      arg = inline_type->buffer(this, true, must_init_buffer);\n+    }\n+    if (t != Type::HALF) {\n+      arg_num++;\n+    }\n+    call->init_req(idx++, arg);\n@@ -1837,7 +1970,0 @@\n-  \/\/ Capture the return value, if any.\n-  Node* ret;\n-  if (call->method() == nullptr ||\n-      call->method()->return_type()->basic_type() == T_VOID)\n-        ret = top();\n-  else  ret = _gvn.transform(new ProjNode(call, TypeFunc::Parms));\n-\n@@ -1856,0 +1982,36 @@\n+\n+  \/\/ Capture the return value, if any.\n+  Node* ret;\n+  if (call->method() == nullptr || call->method()->return_type()->basic_type() == T_VOID) {\n+    ret = top();\n+  } else if (call->tf()->returns_inline_type_as_fields()) {\n+    \/\/ Return of multiple values (inline type fields): we create a\n+    \/\/ InlineType node, each field is a projection from the call.\n+    ciInlineKlass* vk = call->method()->return_type()->as_inline_klass();\n+    uint base_input = TypeFunc::Parms;\n+    ret = InlineTypeNode::make_from_multi(this, call, vk, base_input, false, false);\n+  } else {\n+    ret = _gvn.transform(new ProjNode(call, TypeFunc::Parms));\n+    ciType* t = call->method()->return_type();\n+    if (t->is_klass()) {\n+      const Type* type = TypeOopPtr::make_from_klass(t->as_klass());\n+      if (type->is_inlinetypeptr()) {\n+        ret = InlineTypeNode::make_from_oop(this, ret, type->inline_klass(), type->inline_klass()->is_null_free());\n+      }\n+    }\n+  }\n+\n+  \/\/ We just called the constructor on a value type receiver. Reload it from the buffer\n+  ciMethod* method = call->method();\n+  if (method->is_object_constructor() && !method->holder()->is_java_lang_Object()) {\n+    InlineTypeNode* inline_type_receiver = call->in(TypeFunc::Parms)->isa_InlineType();\n+    if (inline_type_receiver != nullptr) {\n+      assert(inline_type_receiver->is_larval(), \"must be larval\");\n+      assert(inline_type_receiver->is_allocated(&gvn()), \"larval must be buffered\");\n+      InlineTypeNode* reloaded = InlineTypeNode::make_from_oop(this, inline_type_receiver->get_oop(),\n+                                                               inline_type_receiver->bottom_type()->inline_klass(), true);\n+      assert(!reloaded->is_larval(), \"should not be larval anymore\");\n+      replace_in_map(inline_type_receiver, reloaded);\n+    }\n+  }\n+\n@@ -1946,2 +2108,1 @@\n-  CallProjections callprojs;\n-  call->extract_projections(&callprojs, true, do_asserts);\n+  CallProjections* callprojs = call->extract_projections(true, do_asserts);\n@@ -1956,2 +2117,2 @@\n-  if (callprojs.fallthrough_catchproj != nullptr) {\n-    C->gvn_replace_by(callprojs.fallthrough_catchproj, final_ctl);\n+  if (callprojs->fallthrough_catchproj != nullptr) {\n+    C->gvn_replace_by(callprojs->fallthrough_catchproj, final_ctl);\n@@ -1959,1 +2120,1 @@\n-  if (callprojs.fallthrough_memproj != nullptr) {\n+  if (callprojs->fallthrough_memproj != nullptr) {\n@@ -1964,1 +2125,1 @@\n-    C->gvn_replace_by(callprojs.fallthrough_memproj,   final_mem);\n+    C->gvn_replace_by(callprojs->fallthrough_memproj,   final_mem);\n@@ -1967,2 +2128,2 @@\n-  if (callprojs.fallthrough_ioproj != nullptr) {\n-    C->gvn_replace_by(callprojs.fallthrough_ioproj,    final_io);\n+  if (callprojs->fallthrough_ioproj != nullptr) {\n+    C->gvn_replace_by(callprojs->fallthrough_ioproj,    final_io);\n@@ -1972,2 +2133,6 @@\n-  if (callprojs.resproj != nullptr && result != nullptr) {\n-    C->gvn_replace_by(callprojs.resproj, result);\n+  if (callprojs->resproj[0] != nullptr && result != nullptr) {\n+    \/\/ If the inlined code is dead, the result projections for an inline type returned as\n+    \/\/ fields have not been replaced. They will go away once the call is replaced by TOP below.\n+    assert(callprojs->nb_resproj == 1 || (call->tf()->returns_inline_type_as_fields() && stopped()),\n+           \"unexpected number of results\");\n+    C->gvn_replace_by(callprojs->resproj[0], result);\n@@ -1978,2 +2143,2 @@\n-    if (callprojs.catchall_catchproj != nullptr) {\n-      C->gvn_replace_by(callprojs.catchall_catchproj, C->top());\n+    if (callprojs->catchall_catchproj != nullptr) {\n+      C->gvn_replace_by(callprojs->catchall_catchproj, C->top());\n@@ -1981,2 +2146,2 @@\n-    if (callprojs.catchall_memproj != nullptr) {\n-      C->gvn_replace_by(callprojs.catchall_memproj,   C->top());\n+    if (callprojs->catchall_memproj != nullptr) {\n+      C->gvn_replace_by(callprojs->catchall_memproj,   C->top());\n@@ -1984,2 +2149,2 @@\n-    if (callprojs.catchall_ioproj != nullptr) {\n-      C->gvn_replace_by(callprojs.catchall_ioproj,    C->top());\n+    if (callprojs->catchall_ioproj != nullptr) {\n+      C->gvn_replace_by(callprojs->catchall_ioproj,    C->top());\n@@ -1988,2 +2153,2 @@\n-    if (callprojs.exobj != nullptr) {\n-      C->gvn_replace_by(callprojs.exobj, C->top());\n+    if (callprojs->exobj != nullptr) {\n+      C->gvn_replace_by(callprojs->exobj, C->top());\n@@ -2000,2 +2165,2 @@\n-    if (callprojs.catchall_catchproj != nullptr) {\n-      C->gvn_replace_by(callprojs.catchall_catchproj, ekit.control());\n+    if (callprojs->catchall_catchproj != nullptr) {\n+      C->gvn_replace_by(callprojs->catchall_catchproj, ekit.control());\n@@ -2004,1 +2169,1 @@\n-    if (callprojs.catchall_memproj != nullptr) {\n+    if (callprojs->catchall_memproj != nullptr) {\n@@ -2006,1 +2171,1 @@\n-      C->gvn_replace_by(callprojs.catchall_memproj,   ex_mem);\n+      C->gvn_replace_by(callprojs->catchall_memproj,   ex_mem);\n@@ -2009,2 +2174,2 @@\n-    if (callprojs.catchall_ioproj != nullptr) {\n-      C->gvn_replace_by(callprojs.catchall_ioproj,    ekit.i_o());\n+    if (callprojs->catchall_ioproj != nullptr) {\n+      C->gvn_replace_by(callprojs->catchall_ioproj,    ekit.i_o());\n@@ -2014,2 +2179,2 @@\n-    if (callprojs.exobj != nullptr) {\n-      C->gvn_replace_by(callprojs.exobj, ex_oop);\n+    if (callprojs->exobj != nullptr) {\n+      C->gvn_replace_by(callprojs->exobj, ex_oop);\n@@ -2029,1 +2194,1 @@\n-  if (callprojs.fallthrough_catchproj != nullptr && !final_ctl->is_top() && do_replaced_nodes) {\n+  if (callprojs->fallthrough_catchproj != nullptr && !final_ctl->is_top() && do_replaced_nodes) {\n@@ -2230,1 +2395,1 @@\n-    const TypePtr* ptr = (ptr_kind == ProfileMaybeNull && current_type->speculative_maybe_null()) ? TypePtr::BOTTOM : TypePtr::NOTNULL;\n+    const TypePtr* ptr = (ptr_kind != ProfileNeverNull && current_type->speculative_maybe_null()) ? TypePtr::BOTTOM : TypePtr::NOTNULL;\n@@ -2253,1 +2418,1 @@\n-    const TypeOopPtr* spec_type = TypeOopPtr::make(TypePtr::BotPTR, Type::OffsetBot, TypeOopPtr::InstanceBot, speculative);\n+    const TypeOopPtr* spec_type = TypeOopPtr::make(TypePtr::BotPTR, Type::Offset::bottom, TypeOopPtr::InstanceBot, speculative);\n@@ -2287,2 +2452,9 @@\n-      if (!data->as_BitData()->null_seen()) {\n-        ptr_kind = ProfileNeverNull;\n+      if (java_bc() == Bytecodes::_aastore) {\n+        ciKlass* array_type = nullptr;\n+        ciKlass* element_type = nullptr;\n+        ProfilePtrKind element_ptr = ProfileMaybeNull;\n+        bool flat_array = true;\n+        bool null_free_array = true;\n+        method()->array_access_profiled_type(bci(), array_type, element_type, element_ptr, flat_array, null_free_array);\n+        exact_kls = element_type;\n+        ptr_kind = element_ptr;\n@@ -2290,7 +2462,11 @@\n-        assert(data->is_ReceiverTypeData(), \"bad profile data type\");\n-        ciReceiverTypeData* call = (ciReceiverTypeData*)data->as_ReceiverTypeData();\n-        uint i = 0;\n-        for (; i < call->row_limit(); i++) {\n-          ciKlass* receiver = call->receiver(i);\n-          if (receiver != nullptr) {\n-            break;\n+        if (!data->as_BitData()->null_seen()) {\n+          ptr_kind = ProfileNeverNull;\n+        } else {\n+          assert(data->is_ReceiverTypeData(), \"bad profile data type\");\n+          ciReceiverTypeData* call = (ciReceiverTypeData*)data->as_ReceiverTypeData();\n+          uint i = 0;\n+          for (; i < call->row_limit(); i++) {\n+            ciKlass* receiver = call->receiver(i);\n+            if (receiver != nullptr) {\n+              break;\n+            }\n@@ -2298,0 +2474,1 @@\n+          ptr_kind = (i == call->row_limit()) ? ProfileAlwaysNull : ProfileMaybeNull;\n@@ -2299,1 +2476,0 @@\n-        ptr_kind = (i == call->row_limit()) ? ProfileAlwaysNull : ProfileMaybeNull;\n@@ -2318,1 +2494,1 @@\n-  int             nargs = tf->domain()->cnt() - TypeFunc::Parms;\n+  int             nargs = tf->domain_sig()->cnt() - TypeFunc::Parms;\n@@ -2321,1 +2497,1 @@\n-    const Type *targ = tf->domain()->field_at(j + TypeFunc::Parms);\n+    const Type *targ = tf->domain_sig()->field_at(j + TypeFunc::Parms);\n@@ -2375,1 +2551,1 @@\n-    int             nargs = tf->domain()->cnt() - TypeFunc::Parms;\n+    int             nargs = tf->domain_sig()->cnt() - TypeFunc::Parms;\n@@ -2377,1 +2553,1 @@\n-      const Type *targ = tf->domain()->field_at(j + TypeFunc::Parms);\n+      const Type *targ = tf->domain_sig()->field_at(j + TypeFunc::Parms);\n@@ -2526,1 +2702,1 @@\n-    uint num_bits = call_type->range()->field_at(TypeFunc::Parms)->is_vect()->length_in_bytes() * BitsPerByte;\n+    uint num_bits = call_type->range_sig()->field_at(TypeFunc::Parms)->is_vect()->length_in_bytes() * BitsPerByte;\n@@ -2606,0 +2782,1 @@\n+\n@@ -2908,0 +3085,5 @@\n+  const Type* sub_t = _gvn.type(obj_or_subklass);\n+  if (sub_t->make_oopptr() != nullptr && sub_t->make_oopptr()->is_inlinetypeptr()) {\n+    sub_t = TypeKlassPtr::make(sub_t->inline_klass());\n+    obj_or_subklass = makecon(sub_t);\n+  }\n@@ -2913,1 +3095,1 @@\n-    if (!_gvn.type(obj_or_subklass)->isa_klassptr()) {\n+    if (!sub_t->isa_klassptr()) {\n@@ -2931,2 +3113,1 @@\n-                                    float prob,\n-                                    Node* *casted_receiver) {\n+                                    float prob, Node* *casted_receiver) {\n@@ -2934,1 +3115,12 @@\n-\n+  Node* fail = top();\n+  const Type* rec_t = _gvn.type(receiver);\n+  if (rec_t->is_inlinetypeptr()) {\n+    if (klass->equals(rec_t->inline_klass())) {\n+      (*casted_receiver) = receiver; \/\/ Always passes\n+    } else {\n+      (*casted_receiver) = top();    \/\/ Always fails\n+      fail = control();\n+      set_control(top());\n+    }\n+    return fail;\n+  }\n@@ -2937,6 +3129,1 @@\n-  Node* want_klass = makecon(tklass);\n-  Node* cmp = _gvn.transform(new CmpPNode(recv_klass, want_klass));\n-  Node* bol = _gvn.transform(new BoolNode(cmp, BoolTest::eq));\n-  IfNode* iff = create_and_xform_if(control(), bol, prob, COUNT_UNKNOWN);\n-  set_control( _gvn.transform(new IfTrueNode (iff)));\n-  Node* fail = _gvn.transform(new IfFalseNode(iff));\n+  fail = type_check(recv_klass, tklass, prob);\n@@ -2946,2 +3133,2 @@\n-    const TypeOopPtr* recvx_type = tklass->as_instance_type();\n-    assert(recvx_type->klass_is_exact(), \"\");\n+    const TypeOopPtr* recv_xtype = tklass->as_instance_type();\n+    assert(recv_xtype->klass_is_exact(), \"\");\n@@ -2949,1 +3136,1 @@\n-    if (!receiver_type->higher_equal(recvx_type)) { \/\/ ignore redundant casts\n+    if (!receiver_type->higher_equal(recv_xtype)) { \/\/ ignore redundant casts\n@@ -2952,2 +3139,7 @@\n-      Node* cast = new CheckCastPPNode(control(), receiver, recvx_type);\n-      (*casted_receiver) = _gvn.transform(cast);\n+      Node* cast = new CheckCastPPNode(control(), receiver, recv_xtype);\n+      Node* res = _gvn.transform(cast);\n+      if (recv_xtype->is_inlinetypeptr()) {\n+        assert(!gvn().type(res)->maybe_null(), \"receiver should never be null\");\n+        res = InlineTypeNode::make_from_oop(this, res, recv_xtype->inline_klass());\n+      }\n+      (*casted_receiver) = res;\n@@ -2962,0 +3154,11 @@\n+Node* GraphKit::type_check(Node* recv_klass, const TypeKlassPtr* tklass,\n+                           float prob) {\n+  Node* want_klass = makecon(tklass);\n+  Node* cmp = _gvn.transform(new CmpPNode(recv_klass, want_klass));\n+  Node* bol = _gvn.transform(new BoolNode(cmp, BoolTest::eq));\n+  IfNode* iff = create_and_xform_if(control(), bol, prob, COUNT_UNKNOWN);\n+  set_control(_gvn.transform(new IfTrueNode (iff)));\n+  Node* fail = _gvn.transform(new IfFalseNode(iff));\n+  return fail;\n+}\n+\n@@ -2974,3 +3177,6 @@\n-    if (!receiver_type->higher_equal(recv_type)) { \/\/ ignore redundant casts\n-      Node* cast = new CheckCastPPNode(control(), receiver, recv_type);\n-      (*casted_receiver) = _gvn.transform(cast);\n+    if (receiver_type != nullptr && !receiver_type->higher_equal(recv_type)) { \/\/ ignore redundant casts\n+      Node* cast = _gvn.transform(new CheckCastPPNode(control(), receiver, recv_type));\n+      if (recv_type->is_inlinetypeptr()) {\n+        cast = InlineTypeNode::make_from_oop(this, cast, recv_type->inline_klass());\n+      }\n+      (*casted_receiver) = cast;\n@@ -3085,1 +3291,14 @@\n-  ciKlass* exact_kls = spec_klass == nullptr ? profile_has_unique_klass() : spec_klass;\n+  ciKlass* exact_kls = spec_klass;\n+  if (exact_kls == nullptr) {\n+    if (java_bc() == Bytecodes::_aastore) {\n+      ciKlass* array_type = nullptr;\n+      ciKlass* element_type = nullptr;\n+      ProfilePtrKind element_ptr = ProfileMaybeNull;\n+      bool flat_array = true;\n+      bool null_free_array = true;\n+      method()->array_access_profiled_type(bci(), array_type, element_type, element_ptr, flat_array, null_free_array);\n+      exact_kls = element_type;\n+    } else {\n+      exact_kls = profile_has_unique_klass();\n+    }\n+  }\n@@ -3215,1 +3434,1 @@\n-    if (subk->is_loaded()) {\n+    if (subk != nullptr && subk->is_loaded()) {\n@@ -3271,2 +3490,1 @@\n-Node* GraphKit::gen_checkcast(Node *obj, Node* superklass,\n-                              Node* *failure_control) {\n+Node* GraphKit::gen_checkcast(Node *obj, Node* superklass, Node* *failure_control, bool null_free) {\n@@ -3277,0 +3495,2 @@\n+  bool safe_for_replace = (failure_control == nullptr);\n+  assert(!null_free || toop->is_inlinetypeptr(), \"must be an inline type pointer\");\n@@ -3285,3 +3505,10 @@\n-    const TypeOopPtr* objtp = _gvn.type(obj)->isa_oopptr();\n-    if (objtp != nullptr) {\n-      switch (C->static_subtype_check(improved_klass_ptr_type, objtp->as_klass_type())) {\n+    const TypeKlassPtr* kptr = nullptr;\n+    const Type* t = _gvn.type(obj);\n+    if (t->isa_oop_ptr()) {\n+      kptr = t->is_oopptr()->as_klass_type();\n+    } else if (obj->is_InlineType()) {\n+      ciInlineKlass* vk = t->inline_klass();\n+      kptr = TypeInstKlassPtr::make(TypePtr::NotNull, vk, Type::Offset(0));\n+    }\n+    if (kptr != nullptr) {\n+      switch (C->static_subtype_check(improved_klass_ptr_type, kptr)) {\n@@ -3292,1 +3519,7 @@\n-        return record_profiled_receiver_for_speculation(obj);\n+        obj = record_profiled_receiver_for_speculation(obj);\n+        if (null_free) {\n+          assert(safe_for_replace, \"must be\");\n+          obj = null_check(obj);\n+        }\n+        assert(stopped() || !toop->is_inlinetypeptr() || obj->is_InlineType(), \"should have been scalarized\");\n+        return obj;\n@@ -3294,0 +3527,4 @@\n+        if (null_free) {\n+          assert(safe_for_replace, \"must be\");\n+          obj = null_check(obj);\n+        }\n@@ -3295,2 +3532,1 @@\n-        \/\/ A non-null value will always produce an exception.\n-        if (!objtp->maybe_null()) {\n+        if (t->isa_oopptr() != nullptr && !t->is_oopptr()->maybe_null()) {\n@@ -3313,1 +3549,0 @@\n-  bool safe_for_replace = false;\n@@ -3318,2 +3553,3 @@\n-    data = method()->method_data()->bci_to_data(bci());\n-    safe_for_replace = true;\n+    if (method()->method_data()->is_mature()) {\n+      data = method()->method_data()->bci_to_data(bci());\n+    }\n@@ -3326,0 +3562,3 @@\n+  _gvn.set_type(region, Type::CONTROL);\n+  _gvn.set_type(phi, toop);\n+\n@@ -3333,0 +3572,7 @@\n+  if (obj->is_InlineType()) {\n+    \/\/ Re-execute if buffering during triggers deoptimization\n+    PreserveReexecuteState preexecs(this);\n+    jvms()->set_should_reexecute(true);\n+    obj = obj->as_InlineType()->buffer(this, safe_for_replace);\n+  }\n+\n@@ -3335,1 +3581,7 @@\n-  Node* not_null_obj = null_check_oop(obj, &null_ctl, never_see_null, safe_for_replace, speculative_not_null);\n+  Node* not_null_obj = nullptr;\n+  if (null_free) {\n+    assert(safe_for_replace, \"must be\");\n+    not_null_obj = null_check(obj);\n+  } else {\n+    not_null_obj = null_check_oop(obj, &null_ctl, never_see_null, safe_for_replace, speculative_not_null);\n+  }\n@@ -3340,0 +3592,3 @@\n+    if (toop->is_inlinetypeptr()) {\n+      return InlineTypeNode::make_null(_gvn, toop->inline_klass());\n+    }\n@@ -3377,0 +3632,3 @@\n+      \/\/ Only improve the super class for constants which allows subsequent sub type checks to possibly be commoned up.\n+      \/\/ The other non-constant cases cannot be improved with a cast node here since they could be folded to top.\n+      \/\/ Additionally, the benefit would only be minor in non-constant cases.\n@@ -3380,1 +3638,0 @@\n-\n@@ -3388,0 +3645,6 @@\n+        Node* obj_klass = nullptr;\n+        if (not_null_obj->is_InlineType()) {\n+          obj_klass = makecon(TypeKlassPtr::make(_gvn.type(not_null_obj)->inline_klass()));\n+        } else {\n+          obj_klass = load_object_klass(not_null_obj);\n+        }\n@@ -3418,1 +3681,143 @@\n-  return record_profiled_receiver_for_speculation(res);\n+  bool not_inline = !toop->can_be_inline_type();\n+  bool not_flat_in_array = !UseFlatArray || not_inline || (toop->is_inlinetypeptr() && !toop->inline_klass()->flat_in_array());\n+  if (EnableValhalla && not_flat_in_array) {\n+    \/\/ Check if obj has been loaded from an array\n+    obj = obj->isa_DecodeN() ? obj->in(1) : obj;\n+    Node* array = nullptr;\n+    if (obj->isa_Load()) {\n+      Node* address = obj->in(MemNode::Address);\n+      if (address->isa_AddP()) {\n+        array = address->as_AddP()->in(AddPNode::Base);\n+      }\n+    } else if (obj->is_Phi()) {\n+      Node* region = obj->in(0);\n+      \/\/ TODO make this more robust (see JDK-8231346)\n+      if (region->req() == 3 && region->in(2) != nullptr && region->in(2)->in(0) != nullptr) {\n+        IfNode* iff = region->in(2)->in(0)->isa_If();\n+        if (iff != nullptr) {\n+          iff->is_flat_array_check(&_gvn, &array);\n+        }\n+      }\n+    }\n+    if (array != nullptr) {\n+      const TypeAryPtr* ary_t = _gvn.type(array)->isa_aryptr();\n+      if (ary_t != nullptr && !ary_t->is_flat()) {\n+        if (!ary_t->is_not_null_free() && not_inline) {\n+          \/\/ Casting array element to a non-inline-type, mark array as not null-free.\n+          Node* cast = _gvn.transform(new CheckCastPPNode(control(), array, ary_t->cast_to_not_null_free()));\n+          replace_in_map(array, cast);\n+        } else if (!ary_t->is_not_flat()) {\n+          \/\/ Casting array element to a non-flat type, mark array as not flat.\n+          Node* cast = _gvn.transform(new CheckCastPPNode(control(), array, ary_t->cast_to_not_flat()));\n+          replace_in_map(array, cast);\n+        }\n+      }\n+    }\n+  }\n+\n+  if (!stopped() && !res->is_InlineType()) {\n+    res = record_profiled_receiver_for_speculation(res);\n+    if (toop->is_inlinetypeptr()) {\n+      Node* vt = InlineTypeNode::make_from_oop(this, res, toop->inline_klass(), !gvn().type(res)->maybe_null());\n+      res = vt;\n+      if (safe_for_replace) {\n+        replace_in_map(obj, vt);\n+        replace_in_map(not_null_obj, vt);\n+        replace_in_map(res, vt);\n+      }\n+    }\n+  }\n+  return res;\n+}\n+\n+Node* GraphKit::mark_word_test(Node* obj, uintptr_t mask_val, bool eq, bool check_lock) {\n+  \/\/ Load markword\n+  Node* mark_adr = basic_plus_adr(obj, oopDesc::mark_offset_in_bytes());\n+  Node* mark = make_load(nullptr, mark_adr, TypeX_X, TypeX_X->basic_type(), MemNode::unordered);\n+  if (check_lock) {\n+    \/\/ Check if obj is locked\n+    Node* locked_bit = MakeConX(markWord::unlocked_value);\n+    locked_bit = _gvn.transform(new AndXNode(locked_bit, mark));\n+    Node* cmp = _gvn.transform(new CmpXNode(locked_bit, MakeConX(0)));\n+    Node* is_unlocked = _gvn.transform(new BoolNode(cmp, BoolTest::ne));\n+    IfNode* iff = new IfNode(control(), is_unlocked, PROB_MAX, COUNT_UNKNOWN);\n+    _gvn.transform(iff);\n+    Node* locked_region = new RegionNode(3);\n+    Node* mark_phi = new PhiNode(locked_region, TypeX_X);\n+\n+    \/\/ Unlocked: Use bits from mark word\n+    locked_region->init_req(1, _gvn.transform(new IfTrueNode(iff)));\n+    mark_phi->init_req(1, mark);\n+\n+    \/\/ Locked: Load prototype header from klass\n+    set_control(_gvn.transform(new IfFalseNode(iff)));\n+    \/\/ Make loads control dependent to make sure they are only executed if array is locked\n+    Node* klass_adr = basic_plus_adr(obj, oopDesc::klass_offset_in_bytes());\n+    Node* klass = _gvn.transform(LoadKlassNode::make(_gvn, control(), C->immutable_memory(), klass_adr, TypeInstPtr::KLASS, TypeInstKlassPtr::OBJECT));\n+    Node* proto_adr = basic_plus_adr(klass, in_bytes(Klass::prototype_header_offset()));\n+    Node* proto = _gvn.transform(LoadNode::make(_gvn, control(), C->immutable_memory(), proto_adr, proto_adr->bottom_type()->is_ptr(), TypeX_X, TypeX_X->basic_type(), MemNode::unordered));\n+\n+    locked_region->init_req(2, control());\n+    mark_phi->init_req(2, proto);\n+    set_control(_gvn.transform(locked_region));\n+    record_for_igvn(locked_region);\n+\n+    mark = mark_phi;\n+  }\n+\n+  \/\/ Now check if mark word bits are set\n+  Node* mask = MakeConX(mask_val);\n+  Node* masked = _gvn.transform(new AndXNode(_gvn.transform(mark), mask));\n+  record_for_igvn(masked); \/\/ Give it a chance to be optimized out by IGVN\n+  Node* cmp = _gvn.transform(new CmpXNode(masked, mask));\n+  return _gvn.transform(new BoolNode(cmp, eq ? BoolTest::eq : BoolTest::ne));\n+}\n+\n+Node* GraphKit::inline_type_test(Node* obj, bool is_inline) {\n+  return mark_word_test(obj, markWord::inline_type_pattern, is_inline, \/* check_lock = *\/ false);\n+}\n+\n+Node* GraphKit::flat_array_test(Node* array_or_klass, bool flat) {\n+  \/\/ We can't use immutable memory here because the mark word is mutable.\n+  \/\/ PhaseIdealLoop::move_flat_array_check_out_of_loop will make sure the\n+  \/\/ check is moved out of loops (mainly to enable loop unswitching).\n+  Node* cmp = _gvn.transform(new FlatArrayCheckNode(C, memory(Compile::AliasIdxRaw), array_or_klass));\n+  record_for_igvn(cmp); \/\/ Give it a chance to be optimized out by IGVN\n+  return _gvn.transform(new BoolNode(cmp, flat ? BoolTest::eq : BoolTest::ne));\n+}\n+\n+Node* GraphKit::null_free_array_test(Node* array, bool null_free) {\n+  return mark_word_test(array, markWord::null_free_array_bit_in_place, null_free);\n+}\n+\n+\/\/ Deoptimize if 'ary' is a null-free inline type array and 'val' is null\n+Node* GraphKit::inline_array_null_guard(Node* ary, Node* val, int nargs, bool safe_for_replace) {\n+  RegionNode* region = new RegionNode(3);\n+  Node* null_ctl = top();\n+  null_check_oop(val, &null_ctl);\n+  if (null_ctl != top()) {\n+    PreserveJVMState pjvms(this);\n+    set_control(null_ctl);\n+    {\n+      \/\/ Deoptimize if null-free array\n+      BuildCutout unless(this, null_free_array_test(ary, \/* null_free = *\/ false), PROB_MAX);\n+      inc_sp(nargs);\n+      uncommon_trap(Deoptimization::Reason_null_check,\n+                    Deoptimization::Action_none);\n+    }\n+    region->init_req(1, control());\n+  }\n+  region->init_req(2, control());\n+  set_control(_gvn.transform(region));\n+  record_for_igvn(region);\n+  if (_gvn.type(val) == TypePtr::NULL_PTR) {\n+    \/\/ Since we were just successfully storing null, the array can't be null free.\n+    const TypeAryPtr* ary_t = _gvn.type(ary)->is_aryptr();\n+    ary_t = ary_t->cast_to_not_null_free();\n+    Node* cast = _gvn.transform(new CheckCastPPNode(control(), ary, ary_t));\n+    if (safe_for_replace) {\n+      replace_in_map(ary, cast);\n+    }\n+    ary = cast;\n+  }\n+  return ary;\n@@ -3486,0 +3891,1 @@\n+\n@@ -3554,0 +3960,1 @@\n+  assert(!obj->is_InlineType(), \"should not unlock on inline type\");\n@@ -3594,1 +4001,8 @@\n-    if (xklass || (klass_t->isa_aryklassptr() && klass_t->is_aryklassptr()->elem() != Type::BOTTOM)) {\n+    bool can_be_flat = false;\n+    const TypeAryPtr* ary_type = klass_t->as_instance_type()->isa_aryptr();\n+    if (UseFlatArray && !xklass && ary_type != nullptr && !ary_type->is_null_free()) {\n+      \/\/ Don't constant fold if the runtime type might be a flat array but the static type is not.\n+      const TypeOopPtr* elem = ary_type->elem()->make_oopptr();\n+      can_be_flat = ary_type->can_be_inline_array() && (!elem->is_inlinetypeptr() || elem->inline_klass()->flat_in_array());\n+    }\n+    if (!can_be_flat && (xklass || (klass_t->isa_aryklassptr() && klass_t->is_aryklassptr()->elem() != Type::BOTTOM))) {\n@@ -3596,2 +4010,4 @@\n-      if (klass_t->isa_aryklassptr()) {\n-        BasicType elem = klass_t->as_instance_type()->isa_aryptr()->elem()->array_element_basic_type();\n+      if (klass_t->is_flat()) {\n+        lhelper = ary_type->flat_layout_helper();\n+      } else if (klass_t->isa_aryklassptr()) {\n+        BasicType elem = ary_type->elem()->array_element_basic_type();\n@@ -3626,1 +4042,3 @@\n-  kit.set_memory(init_out_raw, alias_idx);\n+  if (init_out_raw != nullptr) {\n+    kit.set_memory(init_out_raw, alias_idx);\n+  }\n@@ -3665,0 +4083,1 @@\n+    _gvn.set_type(minit_in, Type::MEMORY);\n@@ -3672,3 +4091,28 @@\n-      const TypePtr* telemref = oop_type->add_offset(Type::OffsetBot);\n-      int            elemidx  = C->get_alias_index(telemref);\n-      hook_memory_on_init(*this, elemidx, minit_in, minit_out);\n+      const TypeAryPtr* arytype = oop_type->is_aryptr();\n+      if (arytype->is_flat()) {\n+        \/\/ Initially all flat array accesses share a single slice\n+        \/\/ but that changes after parsing. Prepare the memory graph so\n+        \/\/ it can optimize flat array accesses properly once they\n+        \/\/ don't share a single slice.\n+        assert(C->flat_accesses_share_alias(), \"should be set at parse time\");\n+        C->set_flat_accesses_share_alias(false);\n+        ciInlineKlass* vk = arytype->elem()->inline_klass();\n+        for (int i = 0, len = vk->nof_nonstatic_fields(); i < len; i++) {\n+          ciField* field = vk->nonstatic_field_at(i);\n+          if (field->offset_in_bytes() >= TrackedInitializationLimit * HeapWordSize)\n+            continue;  \/\/ do not bother to track really large numbers of fields\n+          int off_in_vt = field->offset_in_bytes() - vk->first_field_offset();\n+          const TypePtr* adr_type = arytype->with_field_offset(off_in_vt)->add_offset(Type::OffsetBot);\n+          int fieldidx = C->get_alias_index(adr_type, true);\n+          \/\/ Pass nullptr for init_out. Having per flat array element field memory edges as uses of the Initialize node\n+          \/\/ can result in per flat array field Phis to be created which confuses the logic of\n+          \/\/ Compile::adjust_flat_array_access_aliases().\n+          hook_memory_on_init(*this, fieldidx, minit_in, nullptr);\n+        }\n+        C->set_flat_accesses_share_alias(true);\n+        hook_memory_on_init(*this, C->get_alias_index(TypeAryPtr::INLINES), minit_in, minit_out);\n+      } else {\n+        const TypePtr* telemref = oop_type->add_offset(Type::OffsetBot);\n+        int            elemidx  = C->get_alias_index(telemref);\n+        hook_memory_on_init(*this, elemidx, minit_in, minit_out);\n+      }\n@@ -3676,0 +4120,1 @@\n+      set_memory(minit_out, C->get_alias_index(oop_type)); \/\/ mark word\n@@ -3726,1 +4171,2 @@\n-                             bool deoptimize_on_exception) {\n+                             bool deoptimize_on_exception,\n+                             InlineTypeNode* inline_type_node) {\n@@ -3733,1 +4179,1 @@\n-  int   layout_is_con = (layout_val == nullptr);\n+  bool  layout_is_con = (layout_val == nullptr);\n@@ -3784,1 +4230,1 @@\n-  \/\/ since GC and deoptimization can happened.\n+  \/\/ since GC and deoptimization can happen.\n@@ -3791,1 +4237,1 @@\n-                                         initial_slow_test);\n+                                         initial_slow_test, inline_type_node);\n@@ -3797,1 +4243,1 @@\n-\/\/ helper for both newarray and anewarray\n+\/\/ helper for newarray and anewarray\n@@ -3810,1 +4256,1 @@\n-  int   layout_is_con = (layout_val == nullptr);\n+  bool  layout_is_con = (layout_val == nullptr);\n@@ -3840,1 +4286,1 @@\n-    fast_size_limit <<= (LogBytesPerLong - log2_esize);\n+    fast_size_limit <<= MAX2(LogBytesPerLong - log2_esize, 0);\n@@ -3857,0 +4303,1 @@\n+    bool is_flat_array = Klass::layout_helper_is_flatArray(layout_con);\n@@ -3859,1 +4306,1 @@\n-    assert((hsize & right_n_bits(eshift)) == 0, \"hsize is pre-rounded\");\n+    assert(is_flat_array || (hsize & right_n_bits(eshift)) == 0, \"hsize is pre-rounded\");\n@@ -3946,1 +4393,1 @@\n-  \/\/ since GC and deoptimization can happened.\n+  \/\/ since GC and deoptimization can happen.\n@@ -3955,1 +4402,29 @@\n-  const TypeOopPtr* ary_type = _gvn.type(klass_node)->is_klassptr()->as_instance_type();\n+  const TypeKlassPtr* ary_klass = _gvn.type(klass_node)->isa_klassptr();\n+  const TypeOopPtr* ary_type = ary_klass->as_instance_type();\n+  const TypeAryPtr* ary_ptr = ary_type->isa_aryptr();\n+\n+  \/\/ Inline type array variants:\n+  \/\/ - null-ok:         ciObjArrayKlass  with is_elem_null_free() = false\n+  \/\/ - null-free:       ciObjArrayKlass  with is_elem_null_free() = true\n+  \/\/ - null-free, flat: ciFlatArrayKlass with is_elem_null_free() = true\n+  \/\/ Check if array is a null-free, non-flat inline type array\n+  \/\/ that needs to be initialized with the default inline type.\n+  Node* default_value = nullptr;\n+  Node* raw_default_value = nullptr;\n+  if (ary_ptr != nullptr && ary_ptr->klass_is_exact()) {\n+    \/\/ Array type is known\n+    if (ary_ptr->is_null_free() && !ary_ptr->is_flat()) {\n+      ciInlineKlass* vk = ary_ptr->elem()->inline_klass();\n+      default_value = InlineTypeNode::default_oop(gvn(), vk);\n+      if (UseCompressedOops) {\n+        \/\/ With compressed oops, the 64-bit init value is built from two 32-bit compressed oops\n+        default_value = _gvn.transform(new EncodePNode(default_value, default_value->bottom_type()->make_narrowoop()));\n+        Node* lower = _gvn.transform(new CastP2XNode(control(), default_value));\n+        Node* upper = _gvn.transform(new LShiftLNode(lower, intcon(32)));\n+        raw_default_value = _gvn.transform(new OrLNode(lower, upper));\n+      } else {\n+        raw_default_value = _gvn.transform(new CastP2XNode(control(), default_value));\n+      }\n+    }\n+  }\n+\n@@ -3970,2 +4445,2 @@\n-                            length, valid_length_test);\n-\n+                            length, valid_length_test,\n+                            default_value, raw_default_value);\n@@ -4118,1 +4593,1 @@\n-                                                     false, nullptr, 0);\n+                                                     false, nullptr, Type::Offset(0));\n@@ -4121,2 +4596,2 @@\n-                                                  TypeAry::make(TypeInt::BYTE, TypeInt::POS),\n-                                                  ciTypeArrayKlass::make(T_BYTE), true, 0);\n+                                                  TypeAry::make(TypeInt::BYTE, TypeInt::POS, false, false, true, true),\n+                                                  ciTypeArrayKlass::make(T_BYTE), true, Type::Offset(0));\n@@ -4135,1 +4610,1 @@\n-                                                     false, nullptr, 0);\n+                                                     false, nullptr, Type::Offset(0));\n@@ -4147,1 +4622,1 @@\n-                                                     false, nullptr, 0);\n+                                                     false, nullptr, Type::Offset(0));\n@@ -4157,1 +4632,1 @@\n-                                                     false, nullptr, 0);\n+                                                     false, nullptr, Type::Offset(0));\n@@ -4270,1 +4745,7 @@\n-    return makecon(con_type);\n+    Node* con = makecon(con_type);\n+    if (field->type()->is_inlinetype()) {\n+      con = InlineTypeNode::make_from_oop(this, con, field->type()->as_inline_klass(), field->is_null_free());\n+    } else if (con_type->is_inlinetypeptr()) {\n+      con = InlineTypeNode::make_from_oop(this, con, con_type->inline_klass(), field->is_null_free());\n+    }\n+    return con;\n@@ -4275,0 +4756,9 @@\n+\/\/---------------------------load_mirror_from_klass----------------------------\n+\/\/ Given a klass oop, load its java mirror (a java.lang.Class oop).\n+Node* GraphKit::load_mirror_from_klass(Node* klass) {\n+  Node* p = basic_plus_adr(klass, in_bytes(Klass::java_mirror_offset()));\n+  Node* load = make_load(nullptr, p, TypeRawPtr::NOTNULL, T_ADDRESS, MemNode::unordered);\n+  \/\/ mirror = ((OopHandle)mirror)->resolve();\n+  return access_load(load, TypeInstPtr::MIRROR, T_OBJECT, IN_NATIVE);\n+}\n+\n@@ -4276,1 +4766,1 @@\n-  const TypeOopPtr* obj_type = obj->bottom_type()->isa_oopptr();\n+  const Type* obj_type = obj->bottom_type();\n@@ -4278,1 +4768,1 @@\n-  if (obj_type != nullptr && sig_type->is_loaded() && !obj_type->higher_equal(sig_type)) {\n+  if (obj_type->isa_oopptr() && sig_type->is_loaded() && !obj_type->higher_equal(sig_type)) {\n@@ -4281,1 +4771,4 @@\n-    return casted_obj;\n+    obj = casted_obj;\n+  }\n+  if (sig_type->is_inlinetypeptr()) {\n+    obj = InlineTypeNode::make_from_oop(this, obj, sig_type->inline_klass(), !gvn().type(obj)->maybe_null());\n","filename":"src\/hotspot\/share\/opto\/graphKit.cpp","additions":630,"deletions":137,"binary":false,"changes":767,"status":"modified"},{"patch":"@@ -36,0 +36,1 @@\n+#include \"opto\/inlinetypenode.hpp\"\n@@ -69,0 +70,3 @@\n+#ifdef ASSERT\n+  uint              _worklist_size;\n+#endif\n@@ -81,1 +85,1 @@\n-  GraphKit(JVMState* jvms);     \/\/ the JVM state on which to operate\n+  GraphKit(JVMState* jvms, PhaseGVN* gvn = nullptr);     \/\/ the JVM state on which to operate\n@@ -87,0 +91,7 @@\n+#if 0\n+    \/\/ During incremental inlining, the Node_Array of the C->for_igvn() worklist and the IGVN\n+    \/\/ worklist are shared but the _in_worklist VectorSet is not. To avoid inconsistencies,\n+    \/\/ we should not add nodes to the _for_igvn worklist when using IGVN for the GraphKit.\n+    assert((_gvn.is_IterGVN() == nullptr) || (_gvn.C->for_igvn()->size() == _worklist_size),\n+           \"GraphKit should not modify _for_igvn worklist after parsing\");\n+#endif\n@@ -97,1 +108,1 @@\n-  void record_for_igvn(Node* n) const { C->record_for_igvn(n); }  \/\/ delegate to Compile\n+  void record_for_igvn(Node* n) const { _gvn.record_for_igvn(n); }\n@@ -365,1 +376,2 @@\n-                          bool speculative = false);\n+                          bool speculative = false,\n+                          bool is_init_check = false);\n@@ -370,1 +382,0 @@\n-    assert(argument(0)->bottom_type()->isa_ptr(), \"must be\");\n@@ -610,1 +621,2 @@\n-                        DecoratorSet decorators);\n+                        DecoratorSet decorators,\n+                        bool safe_for_replace = true);\n@@ -617,1 +629,2 @@\n-                       DecoratorSet decorators);\n+                       DecoratorSet decorators,\n+                       Node* ctl = nullptr);\n@@ -709,1 +722,1 @@\n-  void  set_arguments_for_java_call(CallJavaNode* call);\n+  void  set_arguments_for_java_call(CallJavaNode* call, bool is_late_inline = false);\n@@ -848,2 +861,8 @@\n-  Node* gen_checkcast( Node *subobj, Node* superkls,\n-                       Node* *failure_control = nullptr );\n+  Node* gen_checkcast(Node *subobj, Node* superkls, Node* *failure_control = nullptr, bool null_free = false);\n+\n+  \/\/ Inline types\n+  Node* mark_word_test(Node* obj, uintptr_t mask_val, bool eq, bool check_lock = true);\n+  Node* inline_type_test(Node* obj, bool is_inline = true);\n+  Node* flat_array_test(Node* array_or_klass, bool flat = true);\n+  Node* null_free_array_test(Node* array, bool null_free = true);\n+  Node* inline_array_null_guard(Node* ary, Node* val, int nargs, bool safe_for_replace = false);\n@@ -858,0 +877,1 @@\n+  Node* type_check(Node* recv_klass, const TypeKlassPtr* tklass, float prob);\n@@ -871,1 +891,2 @@\n-                     bool deoptimize_on_exception = false);\n+                     bool deoptimize_on_exception = false,\n+                     InlineTypeNode* inline_type_node = nullptr);\n@@ -908,0 +929,1 @@\n+  Node* load_mirror_from_klass(Node* klass);\n","filename":"src\/hotspot\/share\/opto\/graphKit.hpp","additions":32,"deletions":10,"binary":false,"changes":42,"status":"modified"},{"patch":"@@ -51,1 +51,0 @@\n-  assert(!_gvn.is_IterGVN(), \"IdealKit can't be used during Optimize phase\");\n@@ -83,1 +82,0 @@\n-\n@@ -87,0 +85,4 @@\n+  if_then(bol, prob, cnt, push_new_state);\n+}\n+\n+void IdealKit::if_then(Node* bol, float prob, float cnt, bool push_new_state) {\n@@ -299,1 +301,1 @@\n-    C->record_for_igvn(n);\n+    gvn().record_for_igvn(n);\n@@ -308,1 +310,1 @@\n-  C->record_for_igvn(n);\n+  gvn().record_for_igvn(n);\n@@ -508,2 +510,2 @@\n-  if (slow_call_type->range()->cnt() > TypeFunc::Parms) {\n-    assert(slow_call_type->range()->cnt() == TypeFunc::Parms+1, \"only one return value\");\n+  if (slow_call_type->range_sig()->cnt() > TypeFunc::Parms) {\n+    assert(slow_call_type->range_sig()->cnt() == TypeFunc::Parms+1, \"only one return value\");\n","filename":"src\/hotspot\/share\/opto\/idealKit.cpp","additions":8,"deletions":6,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -166,0 +166,1 @@\n+  void if_then(Node* bol, float prob = PROB_FAIR, float cnt = COUNT_UNKNOWN, bool push_new_state = true);\n","filename":"src\/hotspot\/share\/opto\/idealKit.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -1278,0 +1278,17 @@\n+\/\/ Returns true if this IfNode belongs to a flat array check\n+\/\/ and returns the corresponding array in the 'array' parameter.\n+bool IfNode::is_flat_array_check(PhaseTransform* phase, Node** array) {\n+  Node* bol = in(1);\n+  if (!bol->is_Bool()) {\n+    return false;\n+  }\n+  Node* cmp = bol->in(1);\n+  if (cmp->isa_FlatArrayCheck()) {\n+    if (array != nullptr) {\n+      *array = cmp->in(FlatArrayCheckNode::ArrayOrKlass);\n+    }\n+    return true;\n+  }\n+  return false;\n+}\n+\n","filename":"src\/hotspot\/share\/opto\/ifnode.cpp","additions":17,"deletions":0,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -287,1 +287,1 @@\n-        if (offset == Type::OffsetBot || tptr->_offset == Type::OffsetBot)\n+        if (offset == Type::OffsetBot || tptr->offset() == Type::OffsetBot)\n@@ -289,1 +289,1 @@\n-        offset += tptr->_offset; \/\/ correct if base is offsetted\n+        offset += tptr->offset(); \/\/ correct if base is offsetted\n@@ -326,1 +326,5 @@\n-      Block *inb = get_block_for_node(mach->in(j));\n+      Block* inb = get_block_for_node(mach->in(j));\n+      if (mach->in(j)->is_Con() && mach->in(j)->req() == 1 && inb == get_block_for_node(mach)) {\n+        \/\/ Ignore constant loads scheduled in the same block (we can simply hoist them as well)\n+        continue;\n+      }\n@@ -419,0 +423,21 @@\n+\n+  \/\/ Hoist constant load inputs as well.\n+  for (uint i = 1; i < best->req(); ++i) {\n+    Node* n = best->in(i);\n+    if (n->is_Con() && get_block_for_node(n) == get_block_for_node(best)) {\n+      get_block_for_node(n)->find_remove(n);\n+      block->add_inst(n);\n+      map_node_to_block(n, block);\n+      \/\/ Constant loads may kill flags (for example, when XORing a register).\n+      \/\/ Check for flag-killing projections that also need to be hoisted.\n+      for (DUIterator_Fast jmax, j = n->fast_outs(jmax); j < jmax; j++) {\n+        Node* proj = n->fast_out(j);\n+        if (proj->is_MachProj()) {\n+          get_block_for_node(proj)->find_remove(proj);\n+          block->add_inst(proj);\n+          map_node_to_block(proj, block);\n+        }\n+      }\n+    }\n+  }\n+\n@@ -889,1 +914,1 @@\n-  uint r_cnt = mcall->tf()->range()->cnt();\n+  uint r_cnt = mcall->tf()->range_cc()->cnt();\n","filename":"src\/hotspot\/share\/opto\/lcm.cpp","additions":29,"deletions":4,"binary":false,"changes":33,"status":"modified"},{"patch":"@@ -27,0 +27,1 @@\n+#include \"ci\/ciFlatArrayKlass.hpp\"\n@@ -328,0 +329,2 @@\n+  case vmIntrinsics::_makePrivateBuffer:        return inline_unsafe_make_private_buffer();\n+  case vmIntrinsics::_finishPrivateBuffer:      return inline_unsafe_finish_private_buffer();\n@@ -337,0 +340,1 @@\n+  case vmIntrinsics::_getValue:                 return inline_unsafe_access(!is_store, T_OBJECT,   Relaxed, false, true);\n@@ -347,0 +351,1 @@\n+  case vmIntrinsics::_putValue:                 return inline_unsafe_access( is_store, T_OBJECT,   Relaxed, false, true);\n@@ -514,0 +519,1 @@\n+  case vmIntrinsics::_isFlatArray:              return inline_unsafe_isFlatArray();\n@@ -526,0 +532,1 @@\n+  case vmIntrinsics::_newNullRestrictedArray:   return inline_newNullRestrictedArray();\n@@ -2257,0 +2264,1 @@\n+  bool null_free = false;\n@@ -2262,0 +2270,1 @@\n+      null_free = alias_type->field()->is_null_free();\n@@ -2270,0 +2279,1 @@\n+      null_free = adr_type->is_aryptr()->is_null_free();\n@@ -2282,0 +2292,3 @@\n+    if (null_free) {\n+      result = result->join_speculative(TypePtr::NOTNULL)->is_oopptr();\n+    }\n@@ -2312,1 +2325,1 @@\n-bool LibraryCallKit::inline_unsafe_access(bool is_store, const BasicType type, const AccessKind kind, const bool unaligned) {\n+bool LibraryCallKit::inline_unsafe_access(bool is_store, const BasicType type, const AccessKind kind, const bool unaligned, const bool is_flat) {\n@@ -2337,1 +2350,1 @@\n-      assert(sig->count() == 2, \"oop getter has 2 arguments\");\n+      assert(sig->count() == 2 || (is_flat && sig->count() == 3), \"oop getter has 2 or 3 arguments\");\n@@ -2343,1 +2356,1 @@\n-      assert(sig->count() == 3, \"oop putter has 3 arguments\");\n+      assert(sig->count() == 3 || (is_flat && sig->count() == 4), \"oop putter has 3 arguments\");\n@@ -2369,0 +2382,55 @@\n+\n+  ciInlineKlass* inline_klass = nullptr;\n+  if (is_flat) {\n+    const TypeInstPtr* cls = _gvn.type(argument(4))->isa_instptr();\n+    if (cls == nullptr || cls->const_oop() == nullptr) {\n+      return false;\n+    }\n+    ciType* mirror_type = cls->const_oop()->as_instance()->java_mirror_type();\n+    if (!mirror_type->is_inlinetype()) {\n+      return false;\n+    }\n+    inline_klass = mirror_type->as_inline_klass();\n+  }\n+\n+  if (base->is_InlineType()) {\n+    InlineTypeNode* vt = base->as_InlineType();\n+    if (is_store) {\n+      if (!vt->is_allocated(&_gvn)) {\n+        return false;\n+      }\n+      base = vt->get_oop();\n+    } else {\n+      if (offset->is_Con()) {\n+        long off = find_long_con(offset, 0);\n+        ciInlineKlass* vk = vt->type()->inline_klass();\n+        if ((long)(int)off != off || !vk->contains_field_offset(off)) {\n+          return false;\n+        }\n+\n+        ciField* field = vk->get_non_flat_field_by_offset(off);\n+        if (field != nullptr) {\n+          BasicType bt = type2field[field->type()->basic_type()];\n+          if (bt == T_ARRAY || bt == T_NARROWOOP) {\n+            bt = T_OBJECT;\n+          }\n+          if (bt == type && (!field->is_flat() || field->type() == inline_klass)) {\n+            Node* value = vt->field_value_by_offset(off, false);\n+            if (value->is_InlineType()) {\n+              value = value->as_InlineType()->adjust_scalarization_depth(this);\n+            }\n+            set_result(value);\n+            return true;\n+          }\n+        }\n+      }\n+      {\n+        \/\/ Re-execute the unsafe access if allocation triggers deoptimization.\n+        PreserveReexecuteState preexecs(this);\n+        jvms()->set_should_reexecute(true);\n+        vt = vt->buffer(this);\n+      }\n+      base = vt->get_oop();\n+    }\n+  }\n+\n@@ -2380,1 +2448,1 @@\n-    if (type != T_OBJECT) {\n+    if (type != T_OBJECT && (inline_klass == nullptr || !inline_klass->has_object_fields())) {\n@@ -2398,1 +2466,1 @@\n-  Node* val = is_store ? argument(4) : nullptr;\n+  Node* val = is_store ? argument(4 + (is_flat ? 1 : 0)) : nullptr;\n@@ -2419,1 +2487,22 @@\n-  BasicType bt = alias_type->basic_type();\n+  BasicType bt = T_ILLEGAL;\n+  ciField* field = nullptr;\n+  if (adr_type->isa_instptr()) {\n+    const TypeInstPtr* instptr = adr_type->is_instptr();\n+    ciInstanceKlass* k = instptr->instance_klass();\n+    int off = instptr->offset();\n+    if (instptr->const_oop() != nullptr &&\n+        k == ciEnv::current()->Class_klass() &&\n+        instptr->offset() >= (k->size_helper() * wordSize)) {\n+      k = instptr->const_oop()->as_instance()->java_lang_Class_klass()->as_instance_klass();\n+      field = k->get_field_by_offset(off, true);\n+    } else {\n+      field = k->get_non_flat_field_by_offset(off);\n+    }\n+    if (field != nullptr) {\n+      bt = type2field[field->type()->basic_type()];\n+    }\n+    assert(bt == alias_type->basic_type() || is_flat, \"should match\");\n+  } else {\n+    bt = alias_type->basic_type();\n+  }\n+\n@@ -2442,0 +2531,23 @@\n+  if (is_flat) {\n+    if (adr_type->isa_instptr()) {\n+      if (field == nullptr || field->type() != inline_klass) {\n+        mismatched = true;\n+      }\n+    } else if (adr_type->isa_aryptr()) {\n+      const Type* elem = adr_type->is_aryptr()->elem();\n+      if (!adr_type->is_flat() || elem->inline_klass() != inline_klass) {\n+        mismatched = true;\n+      }\n+    } else {\n+      mismatched = true;\n+    }\n+    if (is_store) {\n+      const Type* val_t = _gvn.type(val);\n+      if (!val_t->is_inlinetypeptr() || val_t->inline_klass() != inline_klass) {\n+        set_map(old_map);\n+        set_sp(old_sp);\n+        return false;\n+      }\n+    }\n+  }\n+\n@@ -2443,1 +2555,1 @@\n-  assert(!mismatched || alias_type->adr_type()->is_oopptr(), \"off-heap access can't be mismatched\");\n+  assert(!mismatched || is_flat || alias_type->adr_type()->is_oopptr(), \"off-heap access can't be mismatched\");\n@@ -2455,4 +2567,6 @@\n-  if (!is_store && type == T_OBJECT) {\n-    const TypeOopPtr* tjp = sharpen_unsafe_type(alias_type, adr_type);\n-    if (tjp != nullptr) {\n-      value_type = tjp;\n+  if (!is_store) {\n+    if (type == T_OBJECT && !is_flat) {\n+      const TypeOopPtr* tjp = sharpen_unsafe_type(alias_type, adr_type);\n+      if (tjp != nullptr) {\n+        value_type = tjp;\n+      }\n@@ -2474,2 +2588,2 @@\n-    ciField* field = alias_type->field();\n-    if (heap_base_oop != top() && field != nullptr && field->is_constant() && !mismatched) {\n+\n+    if (heap_base_oop != top() && field != nullptr && field->is_constant() && !field->is_flat() && !mismatched) {\n@@ -2481,1 +2595,16 @@\n-      p = access_load_at(heap_base_oop, adr, adr_type, value_type, type, decorators);\n+      if (is_flat) {\n+        if (adr_type->isa_instptr() && !mismatched) {\n+          ciInstanceKlass* holder = adr_type->is_instptr()->instance_klass();\n+          int offset = adr_type->is_instptr()->offset();\n+          p = InlineTypeNode::make_from_flat(this, inline_klass, base, base, holder, offset, decorators);\n+        } else {\n+          p = InlineTypeNode::make_from_flat(this, inline_klass, base, adr, nullptr, 0, decorators);\n+        }\n+      } else {\n+        p = access_load_at(heap_base_oop, adr, adr_type, value_type, type, decorators);\n+        const TypeOopPtr* ptr = value_type->make_oopptr();\n+        if (ptr != nullptr && ptr->is_inlinetypeptr()) {\n+          \/\/ Load a non-flattened inline type from memory\n+          p = InlineTypeNode::make_from_oop(this, p, ptr->inline_klass(), !ptr->maybe_null());\n+        }\n+      }\n@@ -2519,1 +2648,17 @@\n-    access_store_at(heap_base_oop, adr, adr_type, val, value_type, type, decorators);\n+    if (is_flat) {\n+      if (adr_type->isa_instptr() && !mismatched) {\n+        ciInstanceKlass* holder = adr_type->is_instptr()->instance_klass();\n+        int offset = adr_type->is_instptr()->offset();\n+        val->as_InlineType()->store_flat(this, base, base, holder, offset, decorators);\n+      } else {\n+        val->as_InlineType()->store_flat(this, base, adr, nullptr, 0, decorators);\n+      }\n+    } else {\n+      access_store_at(heap_base_oop, adr, adr_type, val, value_type, type, decorators);\n+    }\n+  }\n+\n+  if (argument(1)->is_InlineType() && is_store) {\n+    InlineTypeNode* value = InlineTypeNode::make_from_oop(this, base, _gvn.type(argument(1))->inline_klass());\n+    value = value->make_larval(this, false);\n+    replace_in_map(argument(1), value);\n@@ -2525,0 +2670,40 @@\n+bool LibraryCallKit::inline_unsafe_make_private_buffer() {\n+  Node* receiver = argument(0);\n+  Node* value = argument(1);\n+  if (!value->is_InlineType()) {\n+    return false;\n+  }\n+\n+  receiver = null_check(receiver);\n+  if (stopped()) {\n+    return true;\n+  }\n+\n+  set_result(value->as_InlineType()->make_larval(this, true));\n+  return true;\n+}\n+\n+bool LibraryCallKit::inline_unsafe_finish_private_buffer() {\n+  Node* receiver = argument(0);\n+  Node* buffer = argument(1);\n+  if (!buffer->is_InlineType()) {\n+    return false;\n+  }\n+  InlineTypeNode* vt = buffer->as_InlineType();\n+  if (!vt->is_allocated(&_gvn)) {\n+    return false;\n+  }\n+  \/\/ TODO 8239003 Why is this needed?\n+  if (AllocateNode::Ideal_allocation(vt->get_oop()) == nullptr) {\n+    return false;\n+  }\n+\n+  receiver = null_check(receiver);\n+  if (stopped()) {\n+    return true;\n+  }\n+\n+  set_result(vt->finish_larval(this));\n+  return true;\n+}\n+\n@@ -2730,0 +2915,13 @@\n+    if (oldval != nullptr && oldval->is_InlineType()) {\n+      \/\/ Re-execute the unsafe access if allocation triggers deoptimization.\n+      PreserveReexecuteState preexecs(this);\n+      jvms()->set_should_reexecute(true);\n+      oldval = oldval->as_InlineType()->buffer(this)->get_oop();\n+    }\n+    if (newval != nullptr && newval->is_InlineType()) {\n+      \/\/ Re-execute the unsafe access if allocation triggers deoptimization.\n+      PreserveReexecuteState preexecs(this);\n+      jvms()->set_should_reexecute(true);\n+      newval = newval->as_InlineType()->buffer(this)->get_oop();\n+    }\n+\n@@ -2916,2 +3114,7 @@\n-\n-  Node* obj = new_instance(kls, test);\n+  Node* obj = nullptr;\n+  const TypeInstKlassPtr* tkls = _gvn.type(kls)->isa_instklassptr();\n+  if (tkls != nullptr && tkls->instance_klass()->is_inlinetype()) {\n+    obj = InlineTypeNode::make_default(_gvn, tkls->instance_klass()->as_inline_klass())->buffer(this);\n+  } else {\n+    obj = new_instance(kls, test);\n+  }\n@@ -3713,1 +3916,1 @@\n-  const TypeAry* arr0 = TypeAry::make(etype, TypeInt::POS);\n+  const TypeAry* arr0 = TypeAry::make(etype, TypeInt::POS, \/* stable= *\/ false, \/* flat= *\/ false, \/* not_flat= *\/ true, \/* not_null_free= *\/ true);\n@@ -3718,1 +3921,1 @@\n-  const Type* objects_type = TypeAryPtr::make(TypePtr::BotPTR, arr0, objects_klass, xk, 0);\n+  const Type* objects_type = TypeAryPtr::make(TypePtr::BotPTR, arr0, objects_klass, xk, TypeAryPtr::Offset(0));\n@@ -3841,9 +4044,0 @@\n-\/\/---------------------------load_mirror_from_klass----------------------------\n-\/\/ Given a klass oop, load its java mirror (a java.lang.Class oop).\n-Node* LibraryCallKit::load_mirror_from_klass(Node* klass) {\n-  Node* p = basic_plus_adr(klass, in_bytes(Klass::java_mirror_offset()));\n-  Node* load = make_load(nullptr, p, TypeRawPtr::NOTNULL, T_ADDRESS, MemNode::unordered);\n-  \/\/ mirror = ((OopHandle)mirror)->resolve();\n-  return access_load(load, TypeInstPtr::MIRROR, T_OBJECT, IN_NATIVE);\n-}\n-\n@@ -3893,0 +4087,1 @@\n+\n@@ -4094,0 +4289,1 @@\n+\n@@ -4109,1 +4305,2 @@\n-  ciType* tm = mirror_con->java_mirror_type();\n+  bool is_null_free_array = false;\n+  ciType* tm = mirror_con->java_mirror_type(&is_null_free_array);\n@@ -4116,1 +4313,5 @@\n-      int static_res = C->static_subtype_check(TypeKlassPtr::make(tm->as_klass(), Type::trust_interfaces), tp->as_klass_type());\n+      const TypeKlassPtr* tklass = TypeKlassPtr::make(tm->as_klass(), Type::trust_interfaces);\n+      if (is_null_free_array) {\n+        tklass = tklass->is_aryklassptr()->cast_to_null_free();\n+      }\n+      int static_res = C->static_subtype_check(tklass, tp->as_klass_type());\n@@ -4145,2 +4346,2 @@\n-  \/\/ Not-subtype or the mirror's klass ptr is null (in case it is a primitive).\n-  enum { _bad_type_path = 1, _prim_path = 2, PATH_LIMIT };\n+  \/\/ Not-subtype or the mirror's klass ptr is nullptr (in case it is a primitive).\n+  enum { _bad_type_path = 1, _prim_path = 2, _npe_path = 3, PATH_LIMIT };\n@@ -4156,0 +4357,2 @@\n+  Node* io = i_o();\n+  Node* mem = merged_memory();\n@@ -4157,0 +4360,1 @@\n+\n@@ -4163,1 +4367,2 @@\n-      region->in(_bad_type_path) != top()) {\n+      region->in(_bad_type_path) != top() ||\n+      region->in(_npe_path) != top()) {\n@@ -4167,0 +4372,3 @@\n+    \/\/ Set IO and memory because gen_checkcast may override them when buffering inline types\n+    set_i_o(io);\n+    set_all_memory(mem);\n@@ -4200,0 +4408,1 @@\n+  RegionNode* prim_region = new RegionNode(2);\n@@ -4202,0 +4411,1 @@\n+  record_for_igvn(prim_region);\n@@ -4226,2 +4436,5 @@\n-    int prim_path = (which_arg == 0 ? _prim_0_path : _prim_1_path);\n-    region->init_req(prim_path, null_ctl);\n+    if (which_arg == 0) {\n+      prim_region->init_req(1, null_ctl);\n+    } else {\n+      region->init_req(_prim_1_path, null_ctl);\n+    }\n@@ -4237,1 +4450,0 @@\n-    \/\/ now we have a successful reference subtype check\n@@ -4244,1 +4456,2 @@\n-  set_control(region->in(_prim_0_path)); \/\/ go back to first null check\n+  \/\/ This path is also used if superc is a value mirror.\n+  set_control(_gvn.transform(prim_region));\n@@ -4249,1 +4462,1 @@\n-    generate_guard(bol_eq, region, PROB_FAIR);\n+    generate_fair_guard(bol_eq, region);\n@@ -4280,2 +4493,1 @@\n-Node* LibraryCallKit::generate_array_guard_common(Node* kls, RegionNode* region,\n-                                                  bool obj_array, bool not_array) {\n+Node* LibraryCallKit::generate_array_guard_common(Node* kls, RegionNode* region, ArrayKind kind) {\n@@ -4287,9 +4499,0 @@\n-  \/\/ If obj_array\/non_array==false\/false:\n-  \/\/ Branch around if the given klass is in fact an array (either obj or prim).\n-  \/\/ If obj_array\/non_array==false\/true:\n-  \/\/ Branch around if the given klass is not an array klass of any kind.\n-  \/\/ If obj_array\/non_array==true\/true:\n-  \/\/ Branch around if the kls is not an oop array (kls is int[], String, etc.)\n-  \/\/ If obj_array\/non_array==true\/false:\n-  \/\/ Branch around if the kls is an oop array (Object[] or subtype)\n-  \/\/\n@@ -4300,4 +4503,11 @@\n-    bool query = (obj_array\n-                  ? Klass::layout_helper_is_objArray(layout_con)\n-                  : Klass::layout_helper_is_array(layout_con));\n-    if (query == not_array) {\n+    bool query = 0;\n+    switch(kind) {\n+      case ObjectArray:    query = Klass::layout_helper_is_objArray(layout_con); break;\n+      case NonObjectArray: query = !Klass::layout_helper_is_objArray(layout_con); break;\n+      case TypeArray:      query = Klass::layout_helper_is_typeArray(layout_con); break;\n+      case AnyArray:       query = Klass::layout_helper_is_array(layout_con); break;\n+      case NonArray:       query = !Klass::layout_helper_is_array(layout_con); break;\n+      default:\n+        ShouldNotReachHere();\n+    }\n+    if (!query) {\n@@ -4313,0 +4523,21 @@\n+  unsigned int value = 0;\n+  BoolTest::mask btest = BoolTest::illegal;\n+  switch(kind) {\n+    case ObjectArray:\n+    case NonObjectArray: {\n+      value = Klass::_lh_array_tag_obj_value;\n+      layout_val = _gvn.transform(new RShiftINode(layout_val, intcon(Klass::_lh_array_tag_shift)));\n+      btest = (kind == ObjectArray) ? BoolTest::eq : BoolTest::ne;\n+      break;\n+    }\n+    case TypeArray: {\n+      value = Klass::_lh_array_tag_type_value;\n+      layout_val = _gvn.transform(new RShiftINode(layout_val, intcon(Klass::_lh_array_tag_shift)));\n+      btest = BoolTest::eq;\n+      break;\n+    }\n+    case AnyArray:    value = Klass::_lh_neutral_value; btest = BoolTest::lt; break;\n+    case NonArray:    value = Klass::_lh_neutral_value; btest = BoolTest::gt; break;\n+    default:\n+      ShouldNotReachHere();\n+  }\n@@ -4314,4 +4545,1 @@\n-  jint  nval = (obj_array\n-                ? (jint)(Klass::_lh_array_tag_type_value\n-                   <<    Klass::_lh_array_tag_shift)\n-                : Klass::_lh_neutral_value);\n+  jint nval = (jint)value;\n@@ -4319,3 +4547,0 @@\n-  BoolTest::mask btest = BoolTest::lt;  \/\/ correct for testing is_[obj]array\n-  \/\/ invert the test if we are looking for a non-array\n-  if (not_array)  btest = BoolTest(btest).negate();\n@@ -4326,0 +4551,26 @@\n+\/\/-----------------------inline_newNullRestrictedArray--------------------------\n+\/\/ public static native Object[] newNullRestrictedArray(Class<?> componentType, int length);\n+bool LibraryCallKit::inline_newNullRestrictedArray() {\n+  Node* componentType = argument(0);\n+  Node* length = argument(1);\n+\n+  const TypeInstPtr* tp = _gvn.type(componentType)->isa_instptr();\n+  if (tp != nullptr) {\n+    ciInstanceKlass* ik = tp->instance_klass();\n+    if (ik == C->env()->Class_klass()) {\n+      ciType* t = tp->java_mirror_type();\n+      if (t != nullptr && t->is_inlinetype()) {\n+        ciArrayKlass* array_klass = ciArrayKlass::make(t, true);\n+        if (array_klass->is_loaded() && array_klass->element_klass()->as_inline_klass()->is_initialized()) {\n+          const TypeAryKlassPtr* array_klass_type = TypeKlassPtr::make(array_klass, Type::trust_interfaces)->is_aryklassptr();\n+          array_klass_type = array_klass_type->cast_to_null_free();\n+          Node* obj = new_array(makecon(array_klass_type), length, 0, nullptr, false);  \/\/ no arguments to push\n+          set_result(obj);\n+          assert(gvn().type(obj)->is_aryptr()->is_null_free(), \"must be null-free\");\n+          return true;\n+        }\n+      }\n+    }\n+  }\n+  return false;\n+}\n@@ -4328,1 +4579,1 @@\n-\/\/ private static native Object java.lang.reflect.newArray(Class<?> componentType, int length);\n+\/\/ private static native Object java.lang.reflect.Array.newArray(Class<?> componentType, int length);\n@@ -4474,1 +4725,13 @@\n-    Node* not_objArray = generate_non_objArray_guard(klass_node, bailout);\n+    \/\/ Inline type array may have object field that would require a\n+    \/\/ write barrier. Conservatively, go to slow path.\n+    \/\/ TODO 8251971: Optimize for the case when flat src\/dst are later found\n+    \/\/ to not contain oops (i.e., move this check to the macro expansion phase).\n+    BarrierSetC2* bs = BarrierSet::barrier_set()->barrier_set_c2();\n+    const TypeAryPtr* orig_t = _gvn.type(original)->isa_aryptr();\n+    const TypeKlassPtr* tklass = _gvn.type(klass_node)->is_klassptr();\n+    bool exclude_flat = UseFlatArray && bs->array_copy_requires_gc_barriers(true, T_OBJECT, false, false, BarrierSetC2::Parsing) &&\n+                        \/\/ Can src array be flat and contain oops?\n+                        (orig_t == nullptr || (!orig_t->is_not_flat() && (!orig_t->is_flat() || orig_t->elem()->inline_klass()->contains_oops()))) &&\n+                        \/\/ Can dest array be flat and contain oops?\n+                        tklass->can_be_inline_array() && (!tklass->is_flat() || tklass->is_aryklassptr()->elem()->is_instklassptr()->instance_klass()->as_inline_klass()->contains_oops());\n+    Node* not_objArray = exclude_flat ? generate_non_objArray_guard(klass_node, bailout) : generate_typeArray_guard(klass_node, bailout);\n@@ -4478,1 +4741,1 @@\n-      const Type* akls = TypeKlassPtr::make(TypePtr::NotNull, ak, 0\/*offset*\/);\n+      const Type* akls = TypeKlassPtr::make(TypePtr::NotNull, ak, Type::Offset(0));\n@@ -4498,0 +4761,38 @@\n+    \/\/ Handle inline type arrays\n+    bool can_validate = !too_many_traps(Deoptimization::Reason_class_check);\n+    if (!stopped()) {\n+      \/\/ TODO JDK-8329224\n+      if (!orig_t->is_null_free()) {\n+        \/\/ Not statically known to be null free, add a check\n+        generate_fair_guard(null_free_array_test(original), bailout);\n+      }\n+      orig_t = _gvn.type(original)->isa_aryptr();\n+      if (orig_t != nullptr && orig_t->is_flat()) {\n+        \/\/ Src is flat, check that dest is flat as well\n+        if (exclude_flat) {\n+          \/\/ Dest can't be flat, bail out\n+          bailout->add_req(control());\n+          set_control(top());\n+        } else {\n+          generate_fair_guard(flat_array_test(klass_node, \/* flat = *\/ false), bailout);\n+        }\n+      } else if (UseFlatArray && (orig_t == nullptr || !orig_t->is_not_flat()) &&\n+                 \/\/ If dest is flat, src must be flat as well (guaranteed by src <: dest check if validated).\n+                 ((!tklass->is_flat() && tklass->can_be_inline_array()) || !can_validate)) {\n+        \/\/ Src might be flat and dest might not be flat. Go to the slow path if src is flat.\n+        \/\/ TODO 8251971: Optimize for the case when src\/dest are later found to be both flat.\n+        generate_fair_guard(flat_array_test(load_object_klass(original)), bailout);\n+        if (orig_t != nullptr) {\n+          orig_t = orig_t->cast_to_not_flat();\n+          original = _gvn.transform(new CheckCastPPNode(control(), original, orig_t));\n+        }\n+      }\n+      if (!can_validate) {\n+        \/\/ No validation. The subtype check emitted at macro expansion time will not go to the slow\n+        \/\/ path but call checkcast_arraycopy which can not handle flat\/null-free inline type arrays.\n+        \/\/ TODO 8251971: Optimize for the case when src\/dest are later found to be both flat\/null-free.\n+        generate_fair_guard(flat_array_test(klass_node), bailout);\n+        generate_fair_guard(null_free_array_test(original), bailout);\n+      }\n+    }\n+\n@@ -4543,1 +4844,1 @@\n-      if (!too_many_traps(Deoptimization::Reason_class_check)) {\n+      if (can_validate) {\n@@ -4629,1 +4930,1 @@\n-    const TypeTuple* range = tf->range();\n+    const TypeTuple* range = tf->range_cc();\n@@ -4633,1 +4934,1 @@\n-    tf = TypeFunc::make(tf->domain(), new_range);\n+    tf = TypeFunc::make(tf->domain_cc(), new_range);\n@@ -4690,1 +4991,8 @@\n-  Node* obj = nullptr;\n+  Node* obj = argument(0);\n+\n+  \/\/ Don't intrinsify hashcode on inline types for now.\n+  \/\/ The \"is locked\" runtime check below also serves as inline type check and goes to the slow path.\n+  if (gvn().type(obj)->is_inlinetypeptr()) {\n+    return false;\n+  }\n+\n@@ -4700,1 +5008,0 @@\n-    obj = argument(0);\n@@ -4741,1 +5048,2 @@\n-    Node *lock_mask      = _gvn.MakeConX(markWord::lock_mask_in_place);\n+  \/\/ This also serves as guard against inline types\n+    Node *lock_mask      = _gvn.MakeConX(markWord::inline_type_mask_in_place);\n@@ -4816,1 +5124,10 @@\n-  Node* obj = null_check_receiver();\n+  Node* obj = argument(0);\n+  if (obj->is_InlineType()) {\n+    const Type* t = _gvn.type(obj);\n+    if (t->maybe_null()) {\n+      null_check(obj);\n+    }\n+    set_result(makecon(TypeInstPtr::make(t->inline_klass()->java_mirror())));\n+    return true;\n+  }\n+  obj = null_check_receiver();\n@@ -5168,0 +5485,14 @@\n+\/\/----------------------inline_unsafe_isFlatArray------------------------\n+\/\/ public native boolean Unsafe.isFlatArray(Class<?> arrayClass);\n+\/\/ This intrinsic exploits assumptions made by the native implementation\n+\/\/ (arrayClass is neither null nor primitive) to avoid unnecessary null checks.\n+bool LibraryCallKit::inline_unsafe_isFlatArray() {\n+  Node* cls = argument(1);\n+  Node* p = basic_plus_adr(cls, java_lang_Class::klass_offset());\n+  Node* kls = _gvn.transform(LoadKlassNode::make(_gvn, nullptr, immutable_memory(), p,\n+                                                 TypeRawPtr::BOTTOM, TypeInstKlassPtr::OBJECT));\n+  Node* result = flat_array_test(kls);\n+  set_result(result);\n+  return true;\n+}\n+\n@@ -5238,1 +5569,2 @@\n-    Node* obj = null_check_receiver();\n+    Node* obj = argument(0);\n+    obj = null_check_receiver();\n@@ -5242,0 +5574,6 @@\n+    if (obj_type->is_inlinetypeptr()) {\n+      \/\/ If the object to clone is an inline type, we can simply return it (i.e. a nop) since inline types have\n+      \/\/ no identity.\n+      set_result(obj);\n+      return true;\n+    }\n@@ -5248,1 +5586,2 @@\n-        obj_type->speculative_type()->is_instance_klass()) {\n+        obj_type->speculative_type()->is_instance_klass() &&\n+        !obj_type->speculative_type()->is_inlinetype()) {\n@@ -5278,0 +5617,5 @@\n+    \/\/ We only go to the fast case code if we pass a number of guards.\n+    \/\/ The paths which do not pass are accumulated in the slow_region.\n+    RegionNode* slow_region = new RegionNode(1);\n+    record_for_igvn(slow_region);\n+\n@@ -5283,3 +5627,0 @@\n-      Node* obj_length = load_array_length(obj);\n-      Node* array_size = nullptr; \/\/ Size of the array without object alignment padding.\n-      Node* alloc_obj = new_array(obj_klass, obj_length, 0, &array_size, \/*deoptimize_on_exception=*\/true);\n@@ -5288,20 +5629,7 @@\n-      if (bs->array_copy_requires_gc_barriers(true, T_OBJECT, true, false, BarrierSetC2::Parsing)) {\n-        \/\/ If it is an oop array, it requires very special treatment,\n-        \/\/ because gc barriers are required when accessing the array.\n-        Node* is_obja = generate_objArray_guard(obj_klass, (RegionNode*)nullptr);\n-        if (is_obja != nullptr) {\n-          PreserveJVMState pjvms2(this);\n-          set_control(is_obja);\n-          \/\/ Generate a direct call to the right arraycopy function(s).\n-          \/\/ Clones are always tightly coupled.\n-          ArrayCopyNode* ac = ArrayCopyNode::make(this, true, obj, intcon(0), alloc_obj, intcon(0), obj_length, true, false);\n-          ac->set_clone_oop_array();\n-          Node* n = _gvn.transform(ac);\n-          assert(n == ac, \"cannot disappear\");\n-          ac->connect_outputs(this, \/*deoptimize_on_exception=*\/true);\n-\n-          result_reg->init_req(_objArray_path, control());\n-          result_val->init_req(_objArray_path, alloc_obj);\n-          result_i_o ->set_req(_objArray_path, i_o());\n-          result_mem ->set_req(_objArray_path, reset_memory());\n-        }\n+      const TypeAryPtr* ary_ptr = obj_type->isa_aryptr();\n+      if (UseFlatArray && bs->array_copy_requires_gc_barriers(true, T_OBJECT, true, false, BarrierSetC2::Expansion) &&\n+          obj_type->can_be_inline_array() &&\n+          (ary_ptr == nullptr || (!ary_ptr->is_not_flat() && (!ary_ptr->is_flat() || ary_ptr->elem()->inline_klass()->contains_oops())))) {\n+        \/\/ Flat inline type array may have object field that would require a\n+        \/\/ write barrier. Conservatively, go to slow path.\n+        generate_fair_guard(flat_array_test(obj_klass), slow_region);\n@@ -5309,7 +5637,0 @@\n-      \/\/ Otherwise, there are no barriers to worry about.\n-      \/\/ (We can dispense with card marks if we know the allocation\n-      \/\/  comes out of eden (TLAB)...  In fact, ReduceInitialCardMarks\n-      \/\/  causes the non-eden paths to take compensating steps to\n-      \/\/  simulate a fresh allocation, so that no further\n-      \/\/  card marks are required in compiled code to initialize\n-      \/\/  the object.)\n@@ -5318,7 +5639,43 @@\n-        copy_to_clone(obj, alloc_obj, array_size, true);\n-\n-        \/\/ Present the results of the copy.\n-        result_reg->init_req(_array_path, control());\n-        result_val->init_req(_array_path, alloc_obj);\n-        result_i_o ->set_req(_array_path, i_o());\n-        result_mem ->set_req(_array_path, reset_memory());\n+        Node* obj_length = load_array_length(obj);\n+        Node* array_size = nullptr; \/\/ Size of the array without object alignment padding.\n+        Node* alloc_obj = new_array(obj_klass, obj_length, 0, &array_size, \/*deoptimize_on_exception=*\/true);\n+\n+        BarrierSetC2* bs = BarrierSet::barrier_set()->barrier_set_c2();\n+        if (bs->array_copy_requires_gc_barriers(true, T_OBJECT, true, false, BarrierSetC2::Parsing)) {\n+          \/\/ If it is an oop array, it requires very special treatment,\n+          \/\/ because gc barriers are required when accessing the array.\n+          Node* is_obja = generate_objArray_guard(obj_klass, (RegionNode*)nullptr);\n+          if (is_obja != nullptr) {\n+            PreserveJVMState pjvms2(this);\n+            set_control(is_obja);\n+            \/\/ Generate a direct call to the right arraycopy function(s).\n+            \/\/ Clones are always tightly coupled.\n+            ArrayCopyNode* ac = ArrayCopyNode::make(this, true, obj, intcon(0), alloc_obj, intcon(0), obj_length, true, false);\n+            ac->set_clone_oop_array();\n+            Node* n = _gvn.transform(ac);\n+            assert(n == ac, \"cannot disappear\");\n+            ac->connect_outputs(this, \/*deoptimize_on_exception=*\/true);\n+\n+            result_reg->init_req(_objArray_path, control());\n+            result_val->init_req(_objArray_path, alloc_obj);\n+            result_i_o ->set_req(_objArray_path, i_o());\n+            result_mem ->set_req(_objArray_path, reset_memory());\n+          }\n+        }\n+        \/\/ Otherwise, there are no barriers to worry about.\n+        \/\/ (We can dispense with card marks if we know the allocation\n+        \/\/  comes out of eden (TLAB)...  In fact, ReduceInitialCardMarks\n+        \/\/  causes the non-eden paths to take compensating steps to\n+        \/\/  simulate a fresh allocation, so that no further\n+        \/\/  card marks are required in compiled code to initialize\n+        \/\/  the object.)\n+\n+        if (!stopped()) {\n+          copy_to_clone(obj, alloc_obj, array_size, true);\n+\n+          \/\/ Present the results of the copy.\n+          result_reg->init_req(_array_path, control());\n+          result_val->init_req(_array_path, alloc_obj);\n+          result_i_o ->set_req(_array_path, i_o());\n+          result_mem ->set_req(_array_path, reset_memory());\n+        }\n@@ -5328,4 +5685,0 @@\n-    \/\/ We only go to the instance fast case code if we pass a number of guards.\n-    \/\/ The paths which do not pass are accumulated in the slow_region.\n-    RegionNode* slow_region = new RegionNode(1);\n-    record_for_igvn(slow_region);\n@@ -5463,0 +5816,12 @@\n+  int adjustment = 1;\n+  const TypeAryKlassPtr* ary_klass_ptr = alloc->in(AllocateNode::KlassNode)->bottom_type()->is_aryklassptr();\n+  if (ary_klass_ptr->is_null_free()) {\n+    \/\/ A null-free, tightly coupled array allocation can only come from LibraryCallKit::inline_newNullRestrictedArray\n+    \/\/ which requires both the component type and the array length on stack for re-execution. Re-create and push\n+    \/\/ the component type.\n+    ciArrayKlass* klass = ary_klass_ptr->exact_klass()->as_array_klass();\n+    ciInstance* instance = klass->component_mirror_instance();\n+    const TypeInstPtr* t_instance = TypeInstPtr::make(instance);\n+    sfpt->ins_req(old_jvms->stkoff() + old_jvms->sp(), makecon(t_instance));\n+    adjustment++;\n+  }\n@@ -5464,5 +5829,5 @@\n-  sfpt->ins_req(old_jvms->stkoff() + old_jvms->sp(), alloc->in(AllocateNode::ALength));\n-  old_jvms->set_sp(old_jvms->sp()+1);\n-  old_jvms->set_monoff(old_jvms->monoff()+1);\n-  old_jvms->set_scloff(old_jvms->scloff()+1);\n-  old_jvms->set_endoff(old_jvms->endoff()+1);\n+  sfpt->ins_req(old_jvms->stkoff() + old_jvms->sp() + adjustment - 1, alloc->in(AllocateNode::ALength));\n+  old_jvms->set_sp(old_jvms->sp() + adjustment);\n+  old_jvms->set_monoff(old_jvms->monoff() + adjustment);\n+  old_jvms->set_scloff(old_jvms->scloff() + adjustment);\n+  old_jvms->set_endoff(old_jvms->endoff() + adjustment);\n@@ -5501,2 +5866,1 @@\n-    CallProjections callprojs;\n-    alloc->extract_projections(&callprojs, true);\n+    CallProjections* callprojs = alloc->extract_projections(true);\n@@ -5505,1 +5869,1 @@\n-    C->gvn_replace_by(callprojs.fallthrough_ioproj, alloc->in(TypeFunc::I_O));\n+    C->gvn_replace_by(callprojs->fallthrough_ioproj, alloc->in(TypeFunc::I_O));\n@@ -5547,1 +5911,1 @@\n-    set_i_o(callprojs.fallthrough_ioproj);\n+    set_i_o(callprojs->fallthrough_ioproj);\n@@ -5885,1 +6249,1 @@\n-    if (src_elem == dest_elem && src_elem == T_OBJECT) {\n+    if (src_elem == dest_elem && top_src->is_flat() == top_dest->is_flat() && src_elem == T_OBJECT) {\n@@ -5912,0 +6276,2 @@\n+          src_type = _gvn.type(src);\n+          top_src = src_type->isa_aryptr();\n@@ -5915,0 +6281,2 @@\n+          dest_type = _gvn.type(dest);\n+          top_dest = dest_type->isa_aryptr();\n@@ -5930,2 +6298,1 @@\n-      can_emit_guards &&\n-      !src->is_top() && !dest->is_top()) {\n+      can_emit_guards && !src->is_top() && !dest->is_top()) {\n@@ -5974,0 +6341,2 @@\n+      slow_region->add_req(not_subtype_ctrl);\n+    }\n@@ -5975,6 +6344,28 @@\n-      if (not_subtype_ctrl != top()) {\n-        PreserveJVMState pjvms(this);\n-        set_control(not_subtype_ctrl);\n-        uncommon_trap(Deoptimization::Reason_intrinsic,\n-                      Deoptimization::Action_make_not_entrant);\n-        assert(stopped(), \"Should be stopped\");\n+    const TypeKlassPtr* dest_klass_t = _gvn.type(dest_klass)->is_klassptr();\n+    const Type* toop = dest_klass_t->cast_to_exactness(false)->as_instance_type();\n+    src = _gvn.transform(new CheckCastPPNode(control(), src, toop));\n+    src_type = _gvn.type(src);\n+    top_src  = src_type->isa_aryptr();\n+\n+    \/\/ Handle flat inline type arrays (null-free arrays are handled by the subtype check above)\n+    if (!stopped() && UseFlatArray) {\n+      \/\/ If dest is flat, src must be flat as well (guaranteed by src <: dest check). Handle flat src here.\n+      assert(top_dest == nullptr || !top_dest->is_flat() || top_src->is_flat(), \"src array must be flat\");\n+      if (top_src != nullptr && top_src->is_flat()) {\n+        \/\/ Src is flat, check that dest is flat as well\n+        if (top_dest != nullptr && !top_dest->is_flat()) {\n+          generate_fair_guard(flat_array_test(dest_klass, \/* flat = *\/ false), slow_region);\n+          \/\/ Since dest is flat and src <: dest, dest must have the same type as src.\n+          top_dest = top_src->cast_to_exactness(false);\n+          assert(top_dest->is_flat(), \"dest must be flat\");\n+          dest = _gvn.transform(new CheckCastPPNode(control(), dest, top_dest));\n+        }\n+      } else if (top_src == nullptr || !top_src->is_not_flat()) {\n+        \/\/ Src might be flat and dest might not be flat. Go to the slow path if src is flat.\n+        \/\/ TODO 8251971: Optimize for the case when src\/dest are later found to be both flat.\n+        assert(top_dest == nullptr || !top_dest->is_flat(), \"dest array must not be flat\");\n+        generate_fair_guard(flat_array_test(src), slow_region);\n+        if (top_src != nullptr) {\n+          top_src = top_src->cast_to_not_flat();\n+          src = _gvn.transform(new CheckCastPPNode(control(), src, top_src));\n+        }\n@@ -5983,0 +6374,1 @@\n+\n@@ -5990,4 +6382,0 @@\n-\n-    const TypeKlassPtr* dest_klass_t = _gvn.type(dest_klass)->is_klassptr();\n-    const Type *toop = dest_klass_t->cast_to_exactness(false)->as_instance_type();\n-    src = _gvn.transform(new CheckCastPPNode(control(), src, toop));\n","filename":"src\/hotspot\/share\/opto\/library_call.cpp","additions":523,"deletions":135,"binary":false,"changes":658,"status":"modified"},{"patch":"@@ -31,0 +31,1 @@\n+#include \"opto\/inlinetypenode.hpp\"\n@@ -108,3 +109,11 @@\n-    if (!stopped() && result() != nullptr) {\n-      BasicType bt = result()->bottom_type()->basic_type();\n-      push_node(bt, result());\n+    Node* res = result();\n+    if (!stopped() && res != nullptr) {\n+      BasicType bt = res->bottom_type()->basic_type();\n+      if (C->inlining_incrementally() && res->is_InlineType()) {\n+        \/\/ The caller expects an oop when incrementally inlining an intrinsic that returns an\n+        \/\/ inline type. Make sure the call is re-executed if the allocation triggers a deoptimization.\n+        PreserveReexecuteState preexecs(this);\n+        jvms()->set_should_reexecute(true);\n+        res = res->as_InlineType()->buffer(this);\n+      }\n+      push_node(bt, res);\n@@ -141,1 +150,0 @@\n-  Node* load_mirror_from_klass(Node* klass);\n@@ -165,0 +173,9 @@\n+\n+  enum ArrayKind {\n+    AnyArray,\n+    NonArray,\n+    ObjectArray,\n+    NonObjectArray,\n+    TypeArray\n+  };\n+\n@@ -166,0 +183,1 @@\n+\n@@ -167,1 +185,1 @@\n-    return generate_array_guard_common(kls, region, false, false);\n+    return generate_array_guard_common(kls, region, AnyArray);\n@@ -170,1 +188,1 @@\n-    return generate_array_guard_common(kls, region, false, true);\n+    return generate_array_guard_common(kls, region, NonArray);\n@@ -173,1 +191,1 @@\n-    return generate_array_guard_common(kls, region, true, false);\n+    return generate_array_guard_common(kls, region, ObjectArray);\n@@ -176,1 +194,4 @@\n-    return generate_array_guard_common(kls, region, true, true);\n+    return generate_array_guard_common(kls, region, NonObjectArray);\n+  }\n+  Node* generate_typeArray_guard(Node* kls, RegionNode* region) {\n+    return generate_array_guard_common(kls, region, TypeArray);\n@@ -178,2 +199,1 @@\n-  Node* generate_array_guard_common(Node* kls, RegionNode* region,\n-                                    bool obj_array, bool not_array);\n+  Node* generate_array_guard_common(Node* kls, RegionNode* region, ArrayKind kind);\n@@ -229,1 +249,1 @@\n-  bool inline_unsafe_access(bool is_store, BasicType type, AccessKind kind, bool is_unaligned);\n+  bool inline_unsafe_access(bool is_store, BasicType type, AccessKind kind, bool is_unaligned, bool is_flat = false);\n@@ -233,0 +253,1 @@\n+  bool inline_newNullRestrictedArray();\n@@ -236,0 +257,3 @@\n+  bool inline_unsafe_isFlatArray();\n+  bool inline_unsafe_make_private_buffer();\n+  bool inline_unsafe_finish_private_buffer();\n@@ -262,0 +286,1 @@\n+  bool inline_primitive_Class_conversion(vmIntrinsics::ID id);\n","filename":"src\/hotspot\/share\/opto\/library_call.hpp","additions":36,"deletions":11,"binary":false,"changes":47,"status":"modified"},{"patch":"@@ -46,0 +46,1 @@\n+class UnswitchCandidate;\n@@ -82,1 +83,2 @@\n-         LoopNestLongOuterLoop = 1<<16 };\n+         LoopNestLongOuterLoop = 1<<16,\n+         FlatArrays            = 1<<17};\n@@ -105,0 +107,1 @@\n+  bool is_flat_arrays() const { return _loop_flags & FlatArrays; }\n@@ -118,0 +121,1 @@\n+  void mark_flat_arrays() { _loop_flags |= FlatArrays; }\n@@ -693,0 +697,1 @@\n+  bool no_unswitch_candidate() const;\n@@ -1434,1 +1439,2 @@\n-  IfNode* find_unswitch_candidate(const IdealLoopTree* loop) const;\n+  IfNode* find_unswitch_candidates(const IdealLoopTree* loop, Node_List& flat_array_checks) const;\n+  IfNode* find_unswitch_candidate_from_idoms(const IdealLoopTree* loop) const;\n@@ -1441,1 +1447,1 @@\n-                                   const UnswitchedLoopSelector& unswitched_loop_selector);\n+                                   const UnswitchCandidate& unswitch_candidate, const IfNode* loop_selector);\n@@ -1449,0 +1455,1 @@\n+                                            const UnswitchCandidate& unswitch_candidate,\n@@ -1583,0 +1590,1 @@\n+  void move_flat_array_check_out_of_loop(Node* n);\n@@ -1584,0 +1592,1 @@\n+  bool flat_array_element_type_check(Node *n);\n@@ -1793,0 +1802,2 @@\n+  void collect_flat_array_checks(const IdealLoopTree* loop, Node_List& flat_array_checks) const;\n+\n","filename":"src\/hotspot\/share\/opto\/loopnode.hpp","additions":14,"deletions":3,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -36,0 +36,1 @@\n+#include \"opto\/inlinetypenode.hpp\"\n@@ -67,0 +68,6 @@\n+  \/\/ Inline types should not be split through Phis because they cannot be merged\n+  \/\/ through Phi nodes but each value input needs to be merged individually.\n+  if (n->is_InlineType()) {\n+    return nullptr;\n+  }\n+\n@@ -763,0 +770,4 @@\n+      if (inp->isa_InlineType()) {\n+        \/\/ TODO 8302217 This prevents PhiNode::push_inline_types_through\n+        return nullptr;\n+      }\n@@ -1092,0 +1103,48 @@\n+\/\/ We can't use immutable memory for the flat array check because we are loading the mark word which is\n+\/\/ mutable. Although the bits we are interested in are immutable (we check for markWord::unlocked_value),\n+\/\/ we need to use raw memory to not break anti dependency analysis. Below code will attempt to still move\n+\/\/ flat array checks out of loops, mainly to enable loop unswitching.\n+void PhaseIdealLoop::move_flat_array_check_out_of_loop(Node* n) {\n+  \/\/ Skip checks for more than one array\n+  if (n->req() > 3) {\n+    return;\n+  }\n+  Node* mem = n->in(FlatArrayCheckNode::Memory);\n+  Node* array = n->in(FlatArrayCheckNode::ArrayOrKlass)->uncast();\n+  IdealLoopTree* check_loop = get_loop(get_ctrl(n));\n+  IdealLoopTree* ary_loop = get_loop(get_ctrl(array));\n+\n+  \/\/ Check if array is loop invariant\n+  if (!check_loop->is_member(ary_loop)) {\n+    \/\/ Walk up memory graph from the check until we leave the loop\n+    VectorSet wq;\n+    wq.set(mem->_idx);\n+    while (check_loop->is_member(get_loop(ctrl_or_self(mem)))) {\n+      if (mem->is_Phi()) {\n+        mem = mem->in(1);\n+      } else if (mem->is_MergeMem()) {\n+        mem = mem->as_MergeMem()->memory_at(Compile::AliasIdxRaw);\n+      } else if (mem->is_Proj()) {\n+        mem = mem->in(0);\n+      } else if (mem->is_MemBar() || mem->is_SafePoint()) {\n+        mem = mem->in(TypeFunc::Memory);\n+      } else if (mem->is_Store() || mem->is_LoadStore() || mem->is_ClearArray()) {\n+        mem = mem->in(MemNode::Memory);\n+      } else {\n+#ifdef ASSERT\n+        mem->dump();\n+#endif\n+        ShouldNotReachHere();\n+      }\n+      if (wq.test_set(mem->_idx)) {\n+        return;\n+      }\n+    }\n+    \/\/ Replace memory input and re-compute ctrl to move the check out of the loop\n+    _igvn.replace_input_of(n, 1, mem);\n+    set_ctrl_and_loop(n, get_early_ctrl(n));\n+    Node* bol = n->unique_out();\n+    set_ctrl_and_loop(bol, get_early_ctrl(bol));\n+  }\n+}\n+\n@@ -1123,0 +1182,6 @@\n+\n+  if (n->isa_FlatArrayCheck()) {\n+    move_flat_array_check_out_of_loop(n);\n+    return n;\n+  }\n+\n@@ -1402,0 +1467,98 @@\n+bool PhaseIdealLoop::flat_array_element_type_check(Node *n) {\n+  \/\/ If the CmpP is a subtype check for a value that has just been\n+  \/\/ loaded from an array, the subtype check guarantees the value\n+  \/\/ can't be stored in a flat array and the load of the value\n+  \/\/ happens with a flat array check then: push the type check\n+  \/\/ through the phi of the flat array check. This needs special\n+  \/\/ logic because the subtype check's input is not a phi but a\n+  \/\/ LoadKlass that must first be cloned through the phi.\n+  if (n->Opcode() != Op_CmpP) {\n+    return false;\n+  }\n+\n+  Node* klassptr = n->in(1);\n+  Node* klasscon = n->in(2);\n+\n+  if (klassptr->is_DecodeNarrowPtr()) {\n+    klassptr = klassptr->in(1);\n+  }\n+\n+  if (klassptr->Opcode() != Op_LoadKlass && klassptr->Opcode() != Op_LoadNKlass) {\n+    return false;\n+  }\n+\n+  if (!klasscon->is_Con()) {\n+    return false;\n+  }\n+\n+  Node* addr = klassptr->in(MemNode::Address);\n+\n+  if (!addr->is_AddP()) {\n+    return false;\n+  }\n+\n+  intptr_t offset;\n+  Node* obj = AddPNode::Ideal_base_and_offset(addr, &_igvn, offset);\n+\n+  if (obj == nullptr) {\n+    return false;\n+  }\n+\n+  assert(obj != nullptr && addr->in(AddPNode::Base) == addr->in(AddPNode::Address), \"malformed AddP?\");\n+  if (obj->Opcode() == Op_CastPP) {\n+    obj = obj->in(1);\n+  }\n+\n+  if (!obj->is_Phi()) {\n+    return false;\n+  }\n+\n+  Node* region = obj->in(0);\n+\n+  Node* phi = PhiNode::make_blank(region, n->in(1));\n+  for (uint i = 1; i < region->req(); i++) {\n+    Node* in = obj->in(i);\n+    Node* ctrl = region->in(i);\n+    if (addr->in(AddPNode::Base) != obj) {\n+      Node* cast = addr->in(AddPNode::Base);\n+      assert(cast->Opcode() == Op_CastPP && cast->in(0) != nullptr, \"inconsistent subgraph\");\n+      Node* cast_clone = cast->clone();\n+      cast_clone->set_req(0, ctrl);\n+      cast_clone->set_req(1, in);\n+      register_new_node(cast_clone, ctrl);\n+      const Type* tcast = cast_clone->Value(&_igvn);\n+      _igvn.set_type(cast_clone, tcast);\n+      cast_clone->as_Type()->set_type(tcast);\n+      in = cast_clone;\n+    }\n+    Node* addr_clone = addr->clone();\n+    addr_clone->set_req(AddPNode::Base, in);\n+    addr_clone->set_req(AddPNode::Address, in);\n+    register_new_node(addr_clone, ctrl);\n+    _igvn.set_type(addr_clone, addr_clone->Value(&_igvn));\n+    Node* klassptr_clone = klassptr->clone();\n+    klassptr_clone->set_req(2, addr_clone);\n+    register_new_node(klassptr_clone, ctrl);\n+    _igvn.set_type(klassptr_clone, klassptr_clone->Value(&_igvn));\n+    if (klassptr != n->in(1)) {\n+      Node* decode = n->in(1);\n+      assert(decode->is_DecodeNarrowPtr(), \"inconsistent subgraph\");\n+      Node* decode_clone = decode->clone();\n+      decode_clone->set_req(1, klassptr_clone);\n+      register_new_node(decode_clone, ctrl);\n+      _igvn.set_type(decode_clone, decode_clone->Value(&_igvn));\n+      klassptr_clone = decode_clone;\n+    }\n+    phi->set_req(i, klassptr_clone);\n+  }\n+  register_new_node(phi, region);\n+  Node* orig = n->in(1);\n+  _igvn.replace_input_of(n, 1, phi);\n+  split_if_with_blocks_post(n);\n+  if (n->outcnt() != 0) {\n+    _igvn.replace_input_of(n, 1, orig);\n+    _igvn.remove_dead_node(phi);\n+  }\n+  return true;\n+}\n+\n@@ -1408,0 +1571,4 @@\n+  if (flat_array_element_type_check(n)) {\n+    return;\n+  }\n+\n@@ -1555,0 +1722,5 @@\n+\n+  \/\/ Remove multiple allocations of the same inline type\n+  if (n->is_InlineType()) {\n+    n->as_InlineType()->remove_redundant_allocations(this);\n+  }\n@@ -2037,1 +2209,9 @@\n-  Node *sample_cmp = sample_bool->in(1);\n+  Node* sample_cmp = sample_bool->in(1);\n+  const Type* t = Type::TOP;\n+  const TypePtr* at = nullptr;\n+  if (sample_cmp->is_FlatArrayCheck()) {\n+    \/\/ Left input of a FlatArrayCheckNode is memory, set the (adr) type of the phi accordingly\n+    assert(sample_cmp->in(1)->bottom_type() == Type::MEMORY, \"unexpected input type\");\n+    t = Type::MEMORY;\n+    at = TypeRawPtr::BOTTOM;\n+  }\n@@ -2040,1 +2220,1 @@\n-  PhiNode *phi1 = new PhiNode(phi->in(0), Type::TOP);\n+  PhiNode *phi1 = new PhiNode(phi->in(0), t, at);\n","filename":"src\/hotspot\/share\/opto\/loopopts.cpp","additions":182,"deletions":2,"binary":false,"changes":184,"status":"modified"},{"patch":"@@ -188,0 +188,44 @@\n+\/\/ Array of RegMask, one per returned values (inline type instances can\n+\/\/ be returned as multiple return values, one per field)\n+RegMask* Matcher::return_values_mask(const TypeFunc* tf) {\n+  const TypeTuple* range = tf->range_cc();\n+  uint cnt = range->cnt() - TypeFunc::Parms;\n+  if (cnt == 0) {\n+    return nullptr;\n+  }\n+  RegMask* mask = NEW_RESOURCE_ARRAY(RegMask, cnt);\n+  BasicType* sig_bt = NEW_RESOURCE_ARRAY(BasicType, cnt);\n+  VMRegPair* vm_parm_regs = NEW_RESOURCE_ARRAY(VMRegPair, cnt);\n+  for (uint i = 0; i < cnt; i++) {\n+    sig_bt[i] = range->field_at(i+TypeFunc::Parms)->basic_type();\n+  }\n+\n+  int regs = SharedRuntime::java_return_convention(sig_bt, vm_parm_regs, cnt);\n+  if (regs <= 0) {\n+    \/\/ We ran out of registers to store the IsInit information for a nullable inline type return.\n+    \/\/ Since it is only set in the 'call_epilog', we can simply put it on the stack.\n+    assert(tf->returns_inline_type_as_fields(), \"should have been tested during graph construction\");\n+    \/\/ TODO 8284443 Can we teach the register allocator to reserve a stack slot instead?\n+    \/\/ mask[--cnt] = STACK_ONLY_mask does not work (test with -XX:+StressGCM)\n+    int slot = C->fixed_slots() - 2;\n+    if (C->needs_stack_repair()) {\n+      slot -= 2; \/\/ Account for stack increment value\n+    }\n+    mask[--cnt].Clear();\n+    mask[cnt].Insert(OptoReg::stack2reg(slot));\n+  }\n+  for (uint i = 0; i < cnt; i++) {\n+    mask[i].Clear();\n+\n+    OptoReg::Name reg1 = OptoReg::as_OptoReg(vm_parm_regs[i].first());\n+    if (OptoReg::is_valid(reg1)) {\n+      mask[i].Insert(reg1);\n+    }\n+    OptoReg::Name reg2 = OptoReg::as_OptoReg(vm_parm_regs[i].second());\n+    if (OptoReg::is_valid(reg2)) {\n+      mask[i].Insert(reg2);\n+    }\n+  }\n+\n+  return mask;\n+}\n@@ -206,15 +250,3 @@\n-  \/\/ Map a Java-signature return type into return register-value\n-  \/\/ machine registers for 0, 1 and 2 returned values.\n-  const TypeTuple *range = C->tf()->range();\n-  if( range->cnt() > TypeFunc::Parms ) { \/\/ If not a void function\n-    \/\/ Get ideal-register return type\n-    uint ireg = range->field_at(TypeFunc::Parms)->ideal_reg();\n-    \/\/ Get machine return register\n-    uint sop = C->start()->Opcode();\n-    OptoRegPair regs = return_value(ireg);\n-\n-    \/\/ And mask for same\n-    _return_value_mask = RegMask(regs.first());\n-    if( OptoReg::is_valid(regs.second()) )\n-      _return_value_mask.Insert(regs.second());\n-  }\n+  \/\/ Map Java-signature return types into return register-value\n+  \/\/ machine registers.\n+  _return_values_mask = return_values_mask(C->tf());\n@@ -228,1 +260,1 @@\n-  const TypeTuple *domain = C->tf()->domain();\n+  const TypeTuple *domain = C->tf()->domain_cc();\n@@ -551,0 +583,1 @@\n+\n@@ -809,1 +842,1 @@\n-  uint ret_edge_cnt = TypeFunc::Parms + ((C->tf()->range()->cnt() == TypeFunc::Parms) ? 0 : 1);\n+  uint ret_edge_cnt = C->tf()->range_cc()->cnt();\n@@ -811,4 +844,3 @@\n-  \/\/ Returns have 0 or 1 returned values depending on call signature.\n-  \/\/ Return register is specified by return_value in the AD file.\n-  if (ret_edge_cnt > TypeFunc::Parms)\n-    ret_rms[TypeFunc::Parms+0] = _return_value_mask;\n+  for (i = TypeFunc::Parms; i < ret_edge_cnt; i++) {\n+    ret_rms[i] = _return_values_mask[i-TypeFunc::Parms];\n+  }\n@@ -886,1 +918,1 @@\n-  int proj_cnt = C->tf()->domain()->cnt();\n+  int proj_cnt = C->tf()->domain_cc()->cnt();\n@@ -1167,1 +1199,5 @@\n-              m = n->in(0)->as_Multi()->match( n->as_Proj(), this );\n+              RegMask* mask = nullptr;\n+              if (n->in(0)->is_Call() && n->in(0)->as_Call()->tf()->returns_inline_type_as_fields()) {\n+                mask = return_values_mask(n->in(0)->as_Call()->tf());\n+              }\n+              m = n->in(0)->as_Multi()->match(n->as_Proj(), this, mask);\n@@ -1307,1 +1343,1 @@\n-    domain = call->tf()->domain();\n+    domain = call->tf()->domain_cc();\n@@ -1386,1 +1422,4 @@\n-  int argcnt = cnt - TypeFunc::Parms;\n+  \/\/ Null entry point is a special cast where the target of the call\n+  \/\/ is in a register.\n+  int adj = (call != nullptr && call->entry_point() == nullptr) ? 1 : 0;\n+  int argcnt = cnt - TypeFunc::Parms - adj;\n@@ -1392,1 +1431,1 @@\n-      sig_bt[i] = domain->field_at(i+TypeFunc::Parms)->basic_type();\n+      sig_bt[i] = domain->field_at(i+TypeFunc::Parms+adj)->basic_type();\n@@ -1433,1 +1472,1 @@\n-      RegMask *rm = &mcall->_in_rms[i+TypeFunc::Parms];\n+      RegMask *rm = &mcall->_in_rms[i+TypeFunc::Parms+adj];\n@@ -1454,1 +1493,1 @@\n-      if (OptoReg::is_valid(reg1))\n+      if (OptoReg::is_valid(reg1)) {\n@@ -1456,0 +1495,1 @@\n+      }\n@@ -1461,1 +1501,1 @@\n-      if (OptoReg::is_valid(reg2))\n+      if (OptoReg::is_valid(reg2)) {\n@@ -1463,0 +1503,1 @@\n+      }\n@@ -1478,1 +1519,1 @@\n-    uint r_cnt = mcall->tf()->range()->cnt();\n+    uint r_cnt = mcall->tf()->range_sig()->cnt();\n@@ -1500,1 +1541,1 @@\n-         (mcall->jvms()->debug_start() + mcall->_jvmadj == mcall->tf()->domain()->cnt()), \"\");\n+         (mcall->jvms()->debug_start() + mcall->_jvmadj == mcall->tf()->domain_cc()->cnt()), \"\");\n@@ -2188,1 +2229,1 @@\n-      for (int i = n->req() - 1; i >= 0; --i) { \/\/ For my children\n+      for (int i = n->len() - 1; i >= 0; --i) { \/\/ For my children\n@@ -2494,0 +2535,7 @@\n+    }\n+    case Op_ClearArray: {\n+      Node* pair = new BinaryNode(n->in(2), n->in(3));\n+      n->set_req(2, pair);\n+      n->set_req(3, n->in(4));\n+      n->del_req(4);\n+      break;\n","filename":"src\/hotspot\/share\/opto\/matcher.cpp","additions":80,"deletions":32,"binary":false,"changes":112,"status":"modified"},{"patch":"@@ -278,0 +278,2 @@\n+  RegMask* return_values_mask(const TypeFunc* tf);\n+\n@@ -413,1 +415,1 @@\n-  RegMask                     _return_value_mask;\n+  RegMask*            _return_values_mask;\n","filename":"src\/hotspot\/share\/opto\/matcher.hpp","additions":3,"deletions":1,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -27,0 +27,1 @@\n+#include \"ci\/ciFlatArrayKlass.hpp\"\n@@ -28,0 +29,1 @@\n+#include \"classfile\/systemDictionary.hpp\"\n@@ -42,0 +44,1 @@\n+#include \"opto\/inlinetypenode.hpp\"\n@@ -235,0 +238,2 @@\n+                     ->cast_to_not_flat(t_oop->is_aryptr()->is_not_flat())\n+                     ->cast_to_not_null_free(t_oop->is_aryptr()->is_not_null_free())\n@@ -261,1 +266,1 @@\n-               tp->isa_aryptr() &&        tp->offset() == Type::OffsetBot &&\n+        tp->isa_aryptr() &&        tp->offset() == Type::OffsetBot &&\n@@ -1014,1 +1019,1 @@\n-    return (eliminate_boxing && non_volatile) || is_stable_ary;\n+    return (eliminate_boxing && non_volatile) || is_stable_ary || tp->is_inlinetypeptr();\n@@ -1071,1 +1076,1 @@\n-      uint shift  = exact_log2(type2aelembytes(ary_elem));\n+      uint shift  = ary_t->is_flat() ? ary_t->flat_log_elem_size() : exact_log2(type2aelembytes(ary_elem));\n@@ -1190,1 +1195,1 @@\n-        const TypeVect* out_vt = as_LoadVector()->vect_type();\n+        const TypeVect* out_vt = is_Load() ? as_LoadVector()->vect_type() : as_StoreVector()->vect_type();\n@@ -1208,0 +1213,5 @@\n+      Node* default_value = ld_alloc->in(AllocateNode::DefaultValue);\n+      if (default_value != nullptr) {\n+        return default_value;\n+      }\n+      assert(ld_alloc->in(AllocateNode::RawDefaultValue) == nullptr, \"default value may not be null\");\n@@ -1275,0 +1285,17 @@\n+  \/\/ Loading from an InlineType? The InlineType has the values of\n+  \/\/ all fields as input. Look for the field with matching offset.\n+  Node* addr = in(Address);\n+  intptr_t offset;\n+  Node* base = AddPNode::Ideal_base_and_offset(addr, phase, offset);\n+  if (base != nullptr && base->is_InlineType() && offset > oopDesc::klass_offset_in_bytes()) {\n+    Node* value = base->as_InlineType()->field_value_by_offset((int)offset, true);\n+    if (value != nullptr) {\n+      if (Opcode() == Op_LoadN) {\n+        \/\/ Encode oop value if we are loading a narrow oop\n+        assert(!phase->type(value)->isa_narrowoop(), \"should already be decoded\");\n+        value = phase->transform(new EncodePNode(value, bottom_type()));\n+      }\n+      return value;\n+    }\n+  }\n+\n@@ -2059,0 +2086,1 @@\n+        && !ary->is_flat()\n@@ -2094,0 +2122,2 @@\n+            \/\/ Default value load\n+            tp->is_instptr()->instance_klass() == ciEnv::current()->Class_klass() ||\n@@ -2099,1 +2129,3 @@\n-    \/\/ Optimize loads from constant fields.\n+    BasicType bt = memory_type();\n+\n+    \/\/ Optimize loads from constant fields.\n@@ -2103,1 +2135,1 @@\n-      const Type* con_type = Type::make_constant_from_field(const_oop->as_instance(), off, is_unsigned(), memory_type());\n+      const Type* con_type = Type::make_constant_from_field(const_oop->as_instance(), off, is_unsigned(), bt);\n@@ -2238,1 +2270,0 @@\n-\n@@ -2241,1 +2272,10 @@\n-    return TypeX::make(markWord::prototype().value());\n+    if (EnableValhalla) {\n+      \/\/ The mark word may contain property bits (inline, flat, null-free)\n+      Node* klass_node = alloc->in(AllocateNode::KlassNode);\n+      const TypeKlassPtr* tkls = phase->type(klass_node)->isa_klassptr();\n+      if (tkls != nullptr && tkls->is_loaded() && tkls->klass_is_exact()) {\n+        return TypeX::make(tkls->exact_klass()->prototype_header().value());\n+      }\n+    } else {\n+      return TypeX::make(markWord::prototype().value());\n+    }\n@@ -2392,1 +2432,2 @@\n-Node* LoadKlassNode::make(PhaseGVN& gvn, Node* ctl, Node* mem, Node* adr, const TypePtr* at, const TypeKlassPtr* tk) {\n+Node* LoadKlassNode::make(PhaseGVN& gvn, Node* ctl, Node* mem, Node* adr, const TypePtr* at,\n+                          const TypeKlassPtr* tk) {\n@@ -2439,1 +2480,2 @@\n-      ciType* t = tinst->java_mirror_type();\n+      bool is_null_free_array = false;\n+      ciType* t = tinst->java_mirror_type(&is_null_free_array);\n@@ -2449,1 +2491,5 @@\n-          return TypeKlassPtr::make(ciArrayKlass::make(t), Type::trust_interfaces);\n+          const TypeKlassPtr* tklass = TypeKlassPtr::make(ciArrayKlass::make(t), Type::trust_interfaces);\n+          if (is_null_free_array) {\n+            tklass = tklass->is_aryklassptr()->cast_to_null_free();\n+          }\n+          return tklass;\n@@ -2456,1 +2502,5 @@\n-        return TypeKlassPtr::make(t->as_klass(), Type::trust_interfaces);\n+        const TypeKlassPtr* tklass = TypeKlassPtr::make(t->as_klass(), Type::trust_interfaces);\n+        if (is_null_free_array) {\n+          tklass = tklass->is_aryklassptr()->cast_to_null_free();\n+        }\n+        return tklass;\n@@ -2468,1 +2518,1 @@\n-  const TypeAryPtr *tary = tp->isa_aryptr();\n+  const TypeAryPtr* tary = tp->isa_aryptr();\n@@ -3465,2 +3515,2 @@\n-  \/\/ unsafe if I have intervening uses.\n-  {\n+  \/\/ unsafe if I have intervening uses...\n+  if (phase->C->get_adr_type(phase->C->get_alias_index(adr_type())) != TypeAryPtr::INLINES) {\n@@ -3486,0 +3536,1 @@\n+             (Opcode() == Op_StoreL && st->Opcode() == Op_StoreN) ||\n@@ -3612,2 +3663,1 @@\n-  if (result == this &&\n-      ReduceFieldZeroing && phase->type(val)->is_zero_type()) {\n+  if (result == this && ReduceFieldZeroing) {\n@@ -3615,1 +3665,2 @@\n-    if (mem->is_Proj() && mem->in(0)->is_Allocate()) {\n+    if (mem->is_Proj() && mem->in(0)->is_Allocate() &&\n+        (phase->type(val)->is_zero_type() || mem->in(0)->in(AllocateNode::DefaultValue) == val)) {\n@@ -3619,1 +3670,1 @@\n-    if (result == this) {\n+    if (result == this && phase->type(val)->is_zero_type()) {\n@@ -3935,1 +3986,1 @@\n-    return new ClearArrayNode(in(0), in(1), in(2), in(3), true);\n+    return new ClearArrayNode(in(0), in(1), in(2), in(3), in(4), true);\n@@ -3953,1 +4004,1 @@\n-  Node *zero = phase->makecon(TypeLong::ZERO);\n+  Node *val = in(4);\n@@ -3955,1 +4006,1 @@\n-  mem = new StoreLNode(in(0),mem,adr,atp,zero,MemNode::unordered,false);\n+  mem = new StoreLNode(in(0), mem, adr, atp, val, MemNode::unordered, false);\n@@ -3960,1 +4011,1 @@\n-    mem = new StoreLNode(in(0),mem,adr,atp,zero,MemNode::unordered,false);\n+    mem = new StoreLNode(in(0), mem, adr, atp, val, MemNode::unordered, false);\n@@ -3994,0 +4045,2 @@\n+                                   Node* val,\n+                                   Node* raw_val,\n@@ -4004,1 +4057,7 @@\n-    mem = StoreNode::make(*phase, ctl, mem, adr, atp, phase->zerocon(T_INT), T_INT, MemNode::unordered);\n+    if (val != nullptr) {\n+      assert(phase->type(val)->isa_narrowoop(), \"should be narrow oop\");\n+      mem = new StoreNNode(ctl, mem, adr, atp, val, MemNode::unordered);\n+    } else {\n+      assert(raw_val == nullptr, \"val may not be null\");\n+      mem = StoreNode::make(*phase, ctl, mem, adr, atp, phase->zerocon(T_INT), T_INT, MemNode::unordered);\n+    }\n@@ -4011,1 +4070,1 @@\n-  return clear_memory(ctl, mem, dest, phase->MakeConX(offset), end_offset, phase);\n+  return clear_memory(ctl, mem, dest, raw_val, phase->MakeConX(offset), end_offset, phase);\n@@ -4015,0 +4074,1 @@\n+                                   Node* raw_val,\n@@ -4037,1 +4097,4 @@\n-  mem = new ClearArrayNode(ctl, mem, zsize, adr, false);\n+  if (raw_val == nullptr) {\n+    raw_val = phase->MakeConX(0);\n+  }\n+  mem = new ClearArrayNode(ctl, mem, zsize, adr, raw_val, false);\n@@ -4042,0 +4105,2 @@\n+                                   Node* val,\n+                                   Node* raw_val,\n@@ -4056,1 +4121,1 @@\n-    mem = clear_memory(ctl, mem, dest,\n+    mem = clear_memory(ctl, mem, dest, val, raw_val,\n@@ -4063,1 +4128,7 @@\n-    mem = StoreNode::make(*phase, ctl, mem, adr, atp, phase->zerocon(T_INT), T_INT, MemNode::unordered);\n+    if (val != nullptr) {\n+      assert(phase->type(val)->isa_narrowoop(), \"should be narrow oop\");\n+      mem = new StoreNNode(ctl, mem, adr, atp, val, MemNode::unordered);\n+    } else {\n+      assert(raw_val == nullptr, \"val may not be null\");\n+      mem = StoreNode::make(*phase, ctl, mem, adr, atp, phase->zerocon(T_INT), T_INT, MemNode::unordered);\n+    }\n@@ -4209,1 +4280,1 @@\n-Node *MemBarNode::match( const ProjNode *proj, const Matcher *m ) {\n+Node *MemBarNode::match(const ProjNode *proj, const Matcher *m, const RegMask* mask) {\n@@ -4496,1 +4567,3 @@\n-  if (init == nullptr || init->is_complete())  return false;\n+  if (init == nullptr || init->is_complete()) {\n+    return false;\n+  }\n@@ -4680,0 +4753,6 @@\n+                if (base->is_Phi()) {\n+                  \/\/ In rare case, base may be a PhiNode and it may read\n+                  \/\/ the same memory slice between InitializeNode and store.\n+                  failed = true;\n+                  break;\n+                }\n@@ -5266,0 +5345,2 @@\n+                                              allocation()->in(AllocateNode::DefaultValue),\n+                                              allocation()->in(AllocateNode::RawDefaultValue),\n@@ -5325,0 +5406,2 @@\n+                                            allocation()->in(AllocateNode::DefaultValue),\n+                                            allocation()->in(AllocateNode::RawDefaultValue),\n","filename":"src\/hotspot\/share\/opto\/memnode.cpp","additions":113,"deletions":30,"binary":false,"changes":143,"status":"modified"},{"patch":"@@ -129,0 +129,4 @@\n+#ifdef ASSERT\n+  void set_adr_type(const TypePtr* adr_type) { _adr_type = adr_type; }\n+#endif\n+\n@@ -558,1 +562,0 @@\n-\n@@ -1067,0 +1070,1 @@\n+  bool _word_copy_only;\n@@ -1068,2 +1072,3 @@\n-  ClearArrayNode( Node *ctrl, Node *arymem, Node *word_cnt, Node *base, bool is_large)\n-    : Node(ctrl,arymem,word_cnt,base), _is_large(is_large) {\n+  ClearArrayNode( Node *ctrl, Node *arymem, Node *word_cnt, Node *base, Node* val, bool is_large)\n+    : Node(ctrl, arymem, word_cnt, base, val), _is_large(is_large),\n+      _word_copy_only(val->bottom_type()->isa_long() && (!val->bottom_type()->is_long()->is_con() || val->bottom_type()->is_long()->get_con() != 0)) {\n@@ -1081,0 +1086,1 @@\n+  bool word_copy_only() const { return _word_copy_only; }\n@@ -1087,0 +1093,2 @@\n+                            Node* val,\n+                            Node* raw_val,\n@@ -1091,0 +1099,2 @@\n+                            Node* val,\n+                            Node* raw_val,\n@@ -1095,0 +1105,1 @@\n+                            Node* raw_val,\n@@ -1146,1 +1157,1 @@\n-  virtual Node *match( const ProjNode *proj, const Matcher *m );\n+  virtual Node *match(const ProjNode *proj, const Matcher *m, const RegMask* mask);\n","filename":"src\/hotspot\/share\/opto\/memnode.hpp","additions":15,"deletions":4,"binary":false,"changes":19,"status":"modified"},{"patch":"@@ -36,0 +36,1 @@\n+#include \"gc\/shared\/gc_globals.hpp\"\n@@ -249,1 +250,9 @@\n-    _orig_pc_slot = C->fixed_slots() - (sizeof(address) \/ VMRegImpl::stack_slot_size);\n+    int fixed_slots = C->fixed_slots();\n+    if (C->needs_stack_repair()) {\n+      fixed_slots -= 2;\n+    }\n+    \/\/ TODO 8284443 Only reserve extra slot if needed\n+    if (InlineTypeReturnedAsFields) {\n+      fixed_slots -= 2;\n+    }\n+    _orig_pc_slot = fixed_slots - (sizeof(address) \/ VMRegImpl::stack_slot_size);\n@@ -290,1 +299,2 @@\n-  MachPrologNode *prolog = new MachPrologNode();\n+  Label verified_entry;\n+  MachPrologNode* prolog = new MachPrologNode(&verified_entry);\n@@ -296,3 +306,2 @@\n-\n-  if( C->is_osr_compilation() ) {\n-    if( PoisonOSREntry ) {\n+  if (C->is_osr_compilation()) {\n+    if (PoisonOSREntry) {\n@@ -303,3 +312,14 @@\n-    if( C->method() && !C->method()->flags().is_static() ) {\n-      \/\/ Insert unvalidated entry point\n-      C->cfg()->insert( broot, 0, new MachUEPNode() );\n+    if (C->method()) {\n+      if (C->method()->has_scalarized_args()) {\n+        \/\/ Add entry point to unpack all inline type arguments\n+        C->cfg()->insert(broot, 0, new MachVEPNode(&verified_entry, \/* verified *\/ true, \/* receiver_only *\/ false));\n+        if (!C->method()->is_static()) {\n+          \/\/ Add verified\/unverified entry points to only unpack inline type receiver at interface calls\n+          C->cfg()->insert(broot, 0, new MachVEPNode(&verified_entry, \/* verified *\/ false, \/* receiver_only *\/ false));\n+          C->cfg()->insert(broot, 0, new MachVEPNode(&verified_entry, \/* verified *\/ true,  \/* receiver_only *\/ true));\n+          C->cfg()->insert(broot, 0, new MachVEPNode(&verified_entry, \/* verified *\/ false, \/* receiver_only *\/ true));\n+        }\n+      } else if (!C->method()->is_static()) {\n+        \/\/ Insert unvalidated entry point\n+        C->cfg()->insert(broot, 0, new MachUEPNode());\n+      }\n@@ -307,1 +327,0 @@\n-\n@@ -347,0 +366,25 @@\n+  if (!C->is_osr_compilation() && C->has_scalarized_args()) {\n+    \/\/ Compute the offsets of the entry points required by the inline type calling convention\n+    if (!C->method()->is_static()) {\n+      \/\/ We have entries at the beginning of the method, implemented by the first 4 nodes.\n+      \/\/ Entry                     (unverified) @ offset 0\n+      \/\/ Verified_Inline_Entry_RO\n+      \/\/ Inline_Entry              (unverified)\n+      \/\/ Verified_Inline_Entry\n+      uint offset = 0;\n+      _code_offsets.set_value(CodeOffsets::Entry, offset);\n+\n+      offset += ((MachVEPNode*)broot->get_node(0))->size(C->regalloc());\n+      _code_offsets.set_value(CodeOffsets::Verified_Inline_Entry_RO, offset);\n+\n+      offset += ((MachVEPNode*)broot->get_node(1))->size(C->regalloc());\n+      _code_offsets.set_value(CodeOffsets::Inline_Entry, offset);\n+\n+      offset += ((MachVEPNode*)broot->get_node(2))->size(C->regalloc());\n+      _code_offsets.set_value(CodeOffsets::Verified_Inline_Entry, offset);\n+    } else {\n+      _code_offsets.set_value(CodeOffsets::Entry, -1); \/\/ will be patched later\n+      _code_offsets.set_value(CodeOffsets::Verified_Inline_Entry, 0);\n+    }\n+  }\n+\n@@ -508,1 +552,3 @@\n-          mcall->method_set((intptr_t)mcall->entry_point());\n+          if (mcall->entry_point() != nullptr) {\n+            mcall->method_set((intptr_t)mcall->entry_point());\n+          }\n@@ -763,0 +809,17 @@\n+      uint first_ind = spobj->first_index(sfpt->jvms());\n+      \/\/ Nullable, scalarized inline types have an is_init input\n+      \/\/ that needs to be checked before using the field values.\n+      ScopeValue* is_init = nullptr;\n+      if (cik->is_inlinetype()) {\n+        Node* init_node = sfpt->in(first_ind++);\n+        assert(init_node != nullptr, \"is_init node not found\");\n+        if (!init_node->is_top()) {\n+          const TypeInt* init_type = init_node->bottom_type()->is_int();\n+          if (init_node->is_Con()) {\n+            is_init = new ConstantIntValue(init_type->get_con());\n+          } else {\n+            OptoReg::Name init_reg = C->regalloc()->get_reg_first(init_node);\n+            is_init = new_loc_value(C->regalloc(), init_reg, Location::normal);\n+          }\n+        }\n+      }\n@@ -764,1 +827,1 @@\n-                           new ConstantOopWriteValue(cik->java_mirror()->constant_encoding()));\n+                           new ConstantOopWriteValue(cik->java_mirror()->constant_encoding()), true, is_init);\n@@ -767,1 +830,0 @@\n-      uint first_ind = spobj->first_index(sfpt->jvms());\n@@ -1007,0 +1069,1 @@\n+  bool return_scalarized = false;\n@@ -1027,1 +1090,1 @@\n-    if (mcall->returns_pointer()) {\n+    if (mcall->returns_pointer() || mcall->returns_scalarized()) {\n@@ -1030,0 +1093,3 @@\n+    if (mcall->returns_scalarized()) {\n+      return_scalarized = true;\n+    }\n@@ -1195,0 +1261,1 @@\n+      return_scalarized,\n@@ -1569,2 +1636,4 @@\n-          \/\/ This destination address is NOT PC-relative\n-          mcall->method_set((intptr_t)mcall->entry_point());\n+          if (mcall->entry_point() != nullptr) {\n+            \/\/ This destination address is NOT PC-relative\n+            mcall->method_set((intptr_t)mcall->entry_point());\n+          }\n@@ -1711,1 +1780,0 @@\n-\n@@ -3147,0 +3215,13 @@\n+\n+      \/\/ Do not allow a CheckCastPP node whose input is a raw pointer to\n+      \/\/ float past a safepoint.  This can occur when a buffered inline\n+      \/\/ type is allocated in a loop and the CheckCastPP from that\n+      \/\/ allocation is reused outside the loop.  If the use inside the\n+      \/\/ loop is scalarized the CheckCastPP will no longer be connected\n+      \/\/ to the loop safepoint.  See JDK-8264340.\n+      if (m->is_Mach() && m->as_Mach()->ideal_Opcode() == Op_CheckCastPP) {\n+        Node *def = m->in(1);\n+        if (def != nullptr && def->bottom_type()->base() == Type::RawPtr) {\n+          last_safept_node->add_prec(m);\n+        }\n+      }\n@@ -3305,0 +3386,19 @@\n+    if (C->has_scalarized_args()) {\n+      \/\/ Inline type entry points (MachVEPNodes) require lots of space for GC barriers and oop verification\n+      \/\/ when loading object fields from the buffered argument. Increase scratch buffer size accordingly.\n+      ciMethod* method = C->method();\n+      int barrier_size = UseZGC ? 200 : (7 DEBUG_ONLY(+ 37));\n+      int arg_num = 0;\n+      if (!method->is_static()) {\n+        if (method->is_scalarized_arg(arg_num)) {\n+          size += method->holder()->as_inline_klass()->oop_count() * barrier_size;\n+        }\n+        arg_num++;\n+      }\n+      for (ciSignatureStream str(method->signature()); !str.at_return_type(); str.next()) {\n+        if (method->is_scalarized_arg(arg_num)) {\n+          size += str.type()->as_inline_klass()->oop_count() * barrier_size;\n+        }\n+        arg_num++;\n+      }\n+    }\n@@ -3375,1 +3475,2 @@\n-  if (is_branch) \/\/ Restore label.\n+  \/\/ Restore label.\n+  if (is_branch) {\n@@ -3377,0 +3478,1 @@\n+  }\n@@ -3417,6 +3519,9 @@\n-      if (!target->is_static()) {\n-        \/\/ The UEP of an nmethod ensures that the VEP is padded. However, the padding of the UEP is placed\n-        \/\/ before the inline cache check, so we don't have to execute any nop instructions when dispatching\n-        \/\/ through the UEP, yet we can ensure that the VEP is aligned appropriately.\n-        _code_offsets.set_value(CodeOffsets::Entry, _first_block_size - MacroAssembler::ic_check_size());\n-      }\n+      if (_code_offsets.value(CodeOffsets::Verified_Inline_Entry) == -1) {\n+        _code_offsets.set_value(CodeOffsets::Verified_Inline_Entry, _first_block_size);\n+      }\n+      if (_code_offsets.value(CodeOffsets::Verified_Inline_Entry_RO) == -1) {\n+        _code_offsets.set_value(CodeOffsets::Verified_Inline_Entry_RO, _first_block_size);\n+      }\n+      if (_code_offsets.value(CodeOffsets::Entry) == -1) {\n+        _code_offsets.set_value(CodeOffsets::Entry, _first_block_size);\n+      }\n@@ -3428,14 +3533,14 @@\n-                                     entry_bci,\n-                                     &_code_offsets,\n-                                     _orig_pc_slot_offset_in_bytes,\n-                                     code_buffer(),\n-                                     frame_size_in_words(),\n-                                     oop_map_set(),\n-                                     &_handler_table,\n-                                     inc_table(),\n-                                     compiler,\n-                                     has_unsafe_access,\n-                                     SharedRuntime::is_wide_vector(C->max_vector_size()),\n-                                     C->has_monitors(),\n-                                     C->has_scoped_access(),\n-                                     0);\n+                              entry_bci,\n+                              &_code_offsets,\n+                              _orig_pc_slot_offset_in_bytes,\n+                              code_buffer(),\n+                              frame_size_in_words(),\n+                              _oop_map_set,\n+                              &_handler_table,\n+                              inc_table(),\n+                              compiler,\n+                              has_unsafe_access,\n+                              SharedRuntime::is_wide_vector(C->max_vector_size()),\n+                              C->has_monitors(),\n+                              C->has_scoped_access(),\n+                              0);\n","filename":"src\/hotspot\/share\/opto\/output.cpp","additions":142,"deletions":37,"binary":false,"changes":179,"status":"modified"},{"patch":"@@ -26,0 +26,3 @@\n+#include \"ci\/ciFlatArrayKlass.hpp\"\n+#include \"ci\/ciField.hpp\"\n+#include \"ci\/ciInlineKlass.hpp\"\n@@ -52,0 +55,45 @@\n+const Type::Offset Type::Offset::top(Type::OffsetTop);\n+const Type::Offset Type::Offset::bottom(Type::OffsetBot);\n+\n+const Type::Offset Type::Offset::meet(const Type::Offset other) const {\n+  \/\/ Either is 'TOP' offset?  Return the other offset!\n+  if (_offset == OffsetTop) return other;\n+  if (other._offset == OffsetTop) return *this;\n+  \/\/ If either is different, return 'BOTTOM' offset\n+  if (_offset != other._offset) return bottom;\n+  return Offset(_offset);\n+}\n+\n+const Type::Offset Type::Offset::dual() const {\n+  if (_offset == OffsetTop) return bottom;\/\/ Map 'TOP' into 'BOTTOM'\n+  if (_offset == OffsetBot) return top;\/\/ Map 'BOTTOM' into 'TOP'\n+  return Offset(_offset);               \/\/ Map everything else into self\n+}\n+\n+const Type::Offset Type::Offset::add(intptr_t offset) const {\n+  \/\/ Adding to 'TOP' offset?  Return 'TOP'!\n+  if (_offset == OffsetTop || offset == OffsetTop) return top;\n+  \/\/ Adding to 'BOTTOM' offset?  Return 'BOTTOM'!\n+  if (_offset == OffsetBot || offset == OffsetBot) return bottom;\n+  \/\/ Addition overflows or \"accidentally\" equals to OffsetTop? Return 'BOTTOM'!\n+  offset += (intptr_t)_offset;\n+  if (offset != (int)offset || offset == OffsetTop) return bottom;\n+\n+  \/\/ assert( _offset >= 0 && _offset+offset >= 0, \"\" );\n+  \/\/ It is possible to construct a negative offset during PhaseCCP\n+\n+  return Offset((int)offset);        \/\/ Sum valid offsets\n+}\n+\n+void Type::Offset::dump2(outputStream *st) const {\n+  if (_offset == 0) {\n+    return;\n+  } else if (_offset == OffsetTop) {\n+    st->print(\"+top\");\n+  }\n+  else if (_offset == OffsetBot) {\n+    st->print(\"+bot\");\n+  } else if (_offset) {\n+    st->print(\"+%d\", _offset);\n+  }\n+}\n@@ -226,0 +274,3 @@\n+  case T_OBJECT:\n+    return Type::get_const_type(type->unwrap())->join_speculative(type->is_null_free() ? TypePtr::NOTNULL : TypePtr::BOTTOM);\n+\n@@ -538,3 +589,3 @@\n-  TypePtr::NULL_PTR= TypePtr::make(AnyPtr, TypePtr::Null, 0);\n-  TypePtr::NOTNULL = TypePtr::make(AnyPtr, TypePtr::NotNull, OffsetBot);\n-  TypePtr::BOTTOM  = TypePtr::make(AnyPtr, TypePtr::BotPTR, OffsetBot);\n+  TypePtr::NULL_PTR= TypePtr::make(AnyPtr, TypePtr::Null, Offset(0));\n+  TypePtr::NOTNULL = TypePtr::make(AnyPtr, TypePtr::NotNull, Offset::bottom);\n+  TypePtr::BOTTOM  = TypePtr::make(AnyPtr, TypePtr::BotPTR, Offset::bottom);\n@@ -557,1 +608,1 @@\n-                                           false, nullptr, oopDesc::mark_offset_in_bytes());\n+                                           false, nullptr, Offset(oopDesc::mark_offset_in_bytes()));\n@@ -559,2 +610,2 @@\n-                                           false, nullptr, oopDesc::klass_offset_in_bytes());\n-  TypeOopPtr::BOTTOM  = TypeOopPtr::make(TypePtr::BotPTR, OffsetBot, TypeOopPtr::InstanceBot);\n+                                           false, nullptr, Offset(oopDesc::klass_offset_in_bytes()));\n+  TypeOopPtr::BOTTOM  = TypeOopPtr::make(TypePtr::BotPTR, Offset::bottom, TypeOopPtr::InstanceBot);\n@@ -562,1 +613,1 @@\n-  TypeMetadataPtr::BOTTOM = TypeMetadataPtr::make(TypePtr::BotPTR, nullptr, OffsetBot);\n+  TypeMetadataPtr::BOTTOM = TypeMetadataPtr::make(TypePtr::BotPTR, nullptr, Offset::bottom);\n@@ -585,1 +636,1 @@\n-  TypeAryPtr::RANGE   = TypeAryPtr::make( TypePtr::BotPTR, TypeAry::make(Type::BOTTOM,TypeInt::POS), nullptr \/* current->env()->Object_klass() *\/, false, arrayOopDesc::length_offset_in_bytes());\n+  TypeAryPtr::RANGE   = TypeAryPtr::make(TypePtr::BotPTR, TypeAry::make(Type::BOTTOM,TypeInt::POS), nullptr \/* current->env()->Object_klass() *\/, false, Offset(arrayOopDesc::length_offset_in_bytes()));\n@@ -587,1 +638,1 @@\n-  TypeAryPtr::NARROWOOPS = TypeAryPtr::make(TypePtr::BotPTR, TypeAry::make(TypeNarrowOop::BOTTOM, TypeInt::POS), nullptr \/*ciArrayKlass::make(o)*\/,  false,  Type::OffsetBot);\n+  TypeAryPtr::NARROWOOPS = TypeAryPtr::make(TypePtr::BotPTR, TypeAry::make(TypeNarrowOop::BOTTOM, TypeInt::POS), nullptr \/*ciArrayKlass::make(o)*\/,  false,  Offset::bottom);\n@@ -597,1 +648,1 @@\n-    TypeAryPtr::OOPS  = TypeAryPtr::make(TypePtr::BotPTR, TypeAry::make(TypeInstPtr::BOTTOM,TypeInt::POS), nullptr \/*ciArrayKlass::make(o)*\/,  false,  Type::OffsetBot);\n+    TypeAryPtr::OOPS  = TypeAryPtr::make(TypePtr::BotPTR, TypeAry::make(TypeInstPtr::BOTTOM,TypeInt::POS), nullptr \/*ciArrayKlass::make(o)*\/,  false,  Offset::bottom);\n@@ -599,7 +650,8 @@\n-  TypeAryPtr::BYTES   = TypeAryPtr::make(TypePtr::BotPTR, TypeAry::make(TypeInt::BYTE      ,TypeInt::POS), ciTypeArrayKlass::make(T_BYTE),   true,  Type::OffsetBot);\n-  TypeAryPtr::SHORTS  = TypeAryPtr::make(TypePtr::BotPTR, TypeAry::make(TypeInt::SHORT     ,TypeInt::POS), ciTypeArrayKlass::make(T_SHORT),  true,  Type::OffsetBot);\n-  TypeAryPtr::CHARS   = TypeAryPtr::make(TypePtr::BotPTR, TypeAry::make(TypeInt::CHAR      ,TypeInt::POS), ciTypeArrayKlass::make(T_CHAR),   true,  Type::OffsetBot);\n-  TypeAryPtr::INTS    = TypeAryPtr::make(TypePtr::BotPTR, TypeAry::make(TypeInt::INT       ,TypeInt::POS), ciTypeArrayKlass::make(T_INT),    true,  Type::OffsetBot);\n-  TypeAryPtr::LONGS   = TypeAryPtr::make(TypePtr::BotPTR, TypeAry::make(TypeLong::LONG     ,TypeInt::POS), ciTypeArrayKlass::make(T_LONG),   true,  Type::OffsetBot);\n-  TypeAryPtr::FLOATS  = TypeAryPtr::make(TypePtr::BotPTR, TypeAry::make(Type::FLOAT        ,TypeInt::POS), ciTypeArrayKlass::make(T_FLOAT),  true,  Type::OffsetBot);\n-  TypeAryPtr::DOUBLES = TypeAryPtr::make(TypePtr::BotPTR, TypeAry::make(Type::DOUBLE       ,TypeInt::POS), ciTypeArrayKlass::make(T_DOUBLE), true,  Type::OffsetBot);\n+  TypeAryPtr::BYTES   = TypeAryPtr::make(TypePtr::BotPTR, TypeAry::make(TypeInt::BYTE      ,TypeInt::POS), ciTypeArrayKlass::make(T_BYTE),   true,  Offset::bottom);\n+  TypeAryPtr::SHORTS  = TypeAryPtr::make(TypePtr::BotPTR, TypeAry::make(TypeInt::SHORT     ,TypeInt::POS), ciTypeArrayKlass::make(T_SHORT),  true,  Offset::bottom);\n+  TypeAryPtr::CHARS   = TypeAryPtr::make(TypePtr::BotPTR, TypeAry::make(TypeInt::CHAR      ,TypeInt::POS), ciTypeArrayKlass::make(T_CHAR),   true,  Offset::bottom);\n+  TypeAryPtr::INTS    = TypeAryPtr::make(TypePtr::BotPTR, TypeAry::make(TypeInt::INT       ,TypeInt::POS), ciTypeArrayKlass::make(T_INT),    true,  Offset::bottom);\n+  TypeAryPtr::LONGS   = TypeAryPtr::make(TypePtr::BotPTR, TypeAry::make(TypeLong::LONG     ,TypeInt::POS), ciTypeArrayKlass::make(T_LONG),   true,  Offset::bottom);\n+  TypeAryPtr::FLOATS  = TypeAryPtr::make(TypePtr::BotPTR, TypeAry::make(Type::FLOAT        ,TypeInt::POS), ciTypeArrayKlass::make(T_FLOAT),  true,  Offset::bottom);\n+  TypeAryPtr::DOUBLES = TypeAryPtr::make(TypePtr::BotPTR, TypeAry::make(Type::DOUBLE       ,TypeInt::POS), ciTypeArrayKlass::make(T_DOUBLE), true,  Offset::bottom);\n+  TypeAryPtr::INLINES = TypeAryPtr::make(TypePtr::BotPTR, TypeAry::make(TypeInstPtr::BOTTOM,TypeInt::POS, \/* stable= *\/ false, \/* flat= *\/ true), nullptr, false, Offset::bottom);\n@@ -610,0 +662,1 @@\n+  TypeAryPtr::_array_body_type[T_PRIMITIVE_OBJECT] = TypeAryPtr::OOPS;\n@@ -620,2 +673,2 @@\n-  TypeInstKlassPtr::OBJECT = TypeInstKlassPtr::make(TypePtr::NotNull, current->env()->Object_klass(), 0);\n-  TypeInstKlassPtr::OBJECT_OR_NULL = TypeInstKlassPtr::make(TypePtr::BotPTR, current->env()->Object_klass(), 0);\n+  TypeInstKlassPtr::OBJECT = TypeInstKlassPtr::make(TypePtr::NotNull, current->env()->Object_klass(), Offset(0));\n+  TypeInstKlassPtr::OBJECT_OR_NULL = TypeInstKlassPtr::make(TypePtr::BotPTR, current->env()->Object_klass(), Offset(0));\n@@ -660,0 +713,1 @@\n+  _const_basic_type[T_PRIMITIVE_OBJECT] = TypeInstPtr::BOTTOM;\n@@ -676,0 +730,1 @@\n+  _zero_type[T_PRIMITIVE_OBJECT] = TypePtr::NULL_PTR;\n@@ -946,0 +1001,3 @@\n+\n+  \/\/ Verify that:\n+  \/\/      this meet t == t meet this\n@@ -962,0 +1020,6 @@\n+  \/\/ Verify that:\n+  \/\/      !(t meet this)  meet !t ==\n+  \/\/      (!t join !this) meet !t == !t\n+  \/\/ and\n+  \/\/      !(t meet this)  meet !this ==\n+  \/\/      (!t join !this) meet !this == !this\n@@ -2132,0 +2196,12 @@\n+static void collect_inline_fields(ciInlineKlass* vk, const Type** field_array, uint& pos) {\n+  for (int j = 0; j < vk->nof_nonstatic_fields(); j++) {\n+    ciField* field = vk->nonstatic_field_at(j);\n+    BasicType bt = field->type()->basic_type();\n+    const Type* ft = Type::get_const_type(field->type());\n+    field_array[pos++] = ft;\n+    if (type2size[bt] == 2) {\n+      field_array[pos++] = Type::HALF;\n+    }\n+  }\n+}\n+\n@@ -2134,1 +2210,1 @@\n-const TypeTuple *TypeTuple::make_range(ciSignature* sig, InterfaceHandling interface_handling) {\n+const TypeTuple *TypeTuple::make_range(ciSignature* sig, InterfaceHandling interface_handling, bool ret_vt_fields) {\n@@ -2137,0 +2213,5 @@\n+  if (ret_vt_fields) {\n+    arg_cnt = return_type->as_inline_klass()->inline_arg_slots() + 1;\n+    \/\/ InlineTypeNode::IsInit field used for null checking\n+    arg_cnt++;\n+  }\n@@ -2148,0 +2229,11 @@\n+    if (return_type->is_inlinetype() && ret_vt_fields) {\n+      uint pos = TypeFunc::Parms;\n+      field_array[pos++] = get_const_type(return_type); \/\/ Oop might be null when returning as fields\n+      collect_inline_fields(return_type->as_inline_klass(), field_array, pos);\n+      \/\/ InlineTypeNode::IsInit field used for null checking\n+      field_array[pos++] = get_const_basic_type(T_BOOLEAN);\n+      break;\n+    } else {\n+      field_array[TypeFunc::Parms] = get_const_type(return_type, interface_handling)->join_speculative(TypePtr::BOTTOM);\n+    }\n+    break;\n@@ -2166,2 +2258,10 @@\n-const TypeTuple *TypeTuple::make_domain(ciInstanceKlass* recv, ciSignature* sig, InterfaceHandling interface_handling) {\n-  uint arg_cnt = sig->size();\n+const TypeTuple *TypeTuple::make_domain(ciMethod* method, InterfaceHandling interface_handling, bool vt_fields_as_args) {\n+  ciSignature* sig = method->signature();\n+  uint arg_cnt = sig->size() + (method->is_static() ? 0 : 1);\n+  if (vt_fields_as_args) {\n+    arg_cnt = 0;\n+    assert(method->get_sig_cc() != nullptr, \"Should have scalarized signature\");\n+    for (ExtendedSignature sig_cc = ExtendedSignature(method->get_sig_cc(), SigEntryFilter()); !sig_cc.at_end(); ++sig_cc) {\n+      arg_cnt += type2size[(*sig_cc)._bt];\n+    }\n+  }\n@@ -2170,8 +2270,8 @@\n-  const Type **field_array;\n-  if (recv != nullptr) {\n-    arg_cnt++;\n-    field_array = fields(arg_cnt);\n-    \/\/ Use get_const_type here because it respects UseUniqueSubclasses:\n-    field_array[pos++] = get_const_type(recv, interface_handling)->join_speculative(TypePtr::NOTNULL);\n-  } else {\n-    field_array = fields(arg_cnt);\n+  const Type** field_array = fields(arg_cnt);\n+  if (!method->is_static()) {\n+    ciInstanceKlass* recv = method->holder();\n+    if (vt_fields_as_args && recv->is_inlinetype() && recv->as_inline_klass()->can_be_passed_as_fields() && method->is_scalarized_arg(0)) {\n+      collect_inline_fields(recv->as_inline_klass(), field_array, pos);\n+    } else {\n+      field_array[pos++] = get_const_type(recv, interface_handling)->join_speculative(TypePtr::NOTNULL);\n+    }\n@@ -2183,0 +2283,1 @@\n+    BasicType bt = type->basic_type();\n@@ -2184,1 +2285,1 @@\n-    switch (type->basic_type()) {\n+    switch (bt) {\n@@ -2194,0 +2295,8 @@\n+      if (type->is_inlinetype() && vt_fields_as_args && method->is_scalarized_arg(i + (method->is_static() ? 0 : 1))) {\n+        \/\/ InlineTypeNode::IsInit field used for null checking\n+        field_array[pos++] = get_const_basic_type(T_BOOLEAN);\n+        collect_inline_fields(type->as_inline_klass(), field_array, pos);\n+      } else {\n+        field_array[pos++] = get_const_type(type, interface_handling);\n+      }\n+      break;\n@@ -2210,0 +2319,1 @@\n+  assert(pos == TypeFunc::Parms + arg_cnt, \"wrong number of arguments\");\n@@ -2344,1 +2454,2 @@\n-const TypeAry* TypeAry::make(const Type* elem, const TypeInt* size, bool stable) {\n+const TypeAry* TypeAry::make(const Type* elem, const TypeInt* size, bool stable,\n+                             bool flat, bool not_flat, bool not_null_free) {\n@@ -2349,1 +2460,1 @@\n-  return (TypeAry*)(new TypeAry(elem,size,stable))->hashcons();\n+  return (TypeAry*)(new TypeAry(elem, size, stable, flat, not_flat, not_null_free))->hashcons();\n@@ -2371,1 +2482,4 @@\n-                         _stable && a->_stable);\n+                         _stable && a->_stable,\n+                         _flat && a->_flat,\n+                         _not_flat && a->_not_flat,\n+                         _not_null_free && a->_not_null_free);\n@@ -2384,1 +2498,1 @@\n-  return new TypeAry(_elem->dual(), size_dual, !_stable);\n+  return new TypeAry(_elem->dual(), size_dual, !_stable, !_flat, !_not_flat, !_not_null_free);\n@@ -2393,1 +2507,5 @@\n-    _size == a->_size;\n+    _size == a->_size &&\n+    _flat == a->_flat &&\n+    _not_flat == a->_not_flat &&\n+    _not_null_free == a->_not_null_free;\n+\n@@ -2399,1 +2517,2 @@\n-  return (uint)(uintptr_t)_elem + (uint)(uintptr_t)_size + (uint)(_stable ? 43 : 0);\n+  return (uint)(uintptr_t)_elem + (uint)(uintptr_t)_size + (uint)(_stable ? 43 : 0) +\n+      (uint)(_flat ? 44 : 0) + (uint)(_not_flat ? 45 : 0) + (uint)(_not_null_free ? 46 : 0);\n@@ -2406,1 +2525,1 @@\n-  return make(_elem->remove_speculative(), _size, _stable);\n+  return make(_elem->remove_speculative(), _size, _stable, _flat, _not_flat, _not_null_free);\n@@ -2413,1 +2532,1 @@\n-  return make(_elem->cleanup_speculative(), _size, _stable);\n+  return make(_elem->cleanup_speculative(), _size, _stable, _flat, _not_flat, _not_null_free);\n@@ -2432,0 +2551,5 @@\n+  if (_flat) st->print(\"flat:\");\n+  if (Verbose) {\n+    if (_not_flat) st->print(\"not flat:\");\n+    if (_not_null_free) st->print(\"not null free:\");\n+  }\n@@ -2471,2 +2595,10 @@\n-  if (tinst)\n-    return tinst->instance_klass()->is_final();\n+  if (tinst) {\n+    if (tinst->instance_klass()->is_final()) {\n+      \/\/ Even though MyValue is final, [LMyValue is not exact because null-free [LMyValue is a subtype.\n+      if (tinst->is_inlinetypeptr() && (tinst->ptr() == TypePtr::BotPTR || tinst->ptr() == TypePtr::TopPTR)) {\n+        return false;\n+      }\n+      return true;\n+    }\n+    return false;\n+  }\n@@ -2647,1 +2779,1 @@\n-const TypePtr *TypePtr::make(TYPES t, enum PTR ptr, int offset, const TypePtr* speculative, int inline_depth) {\n+const TypePtr* TypePtr::make(TYPES t, enum PTR ptr, Offset offset, const TypePtr* speculative, int inline_depth) {\n@@ -2661,1 +2793,1 @@\n-  return _offset;\n+  return offset();\n@@ -2732,7 +2864,2 @@\n-int TypePtr::meet_offset( int offset ) const {\n-  \/\/ Either is 'TOP' offset?  Return the other offset!\n-  if( _offset == OffsetTop ) return offset;\n-  if( offset == OffsetTop ) return _offset;\n-  \/\/ If either is different, return 'BOTTOM' offset\n-  if( _offset != offset ) return OffsetBot;\n-  return _offset;\n+Type::Offset TypePtr::meet_offset(int offset) const {\n+  return _offset.meet(Offset(offset));\n@@ -2742,4 +2869,2 @@\n-int TypePtr::dual_offset( ) const {\n-  if( _offset == OffsetTop ) return OffsetBot;\/\/ Map 'TOP' into 'BOTTOM'\n-  if( _offset == OffsetBot ) return OffsetTop;\/\/ Map 'BOTTOM' into 'TOP'\n-  return _offset;               \/\/ Map everything else into self\n+Type::Offset TypePtr::dual_offset() const {\n+  return _offset.dual();\n@@ -2758,13 +2883,2 @@\n-int TypePtr::xadd_offset( intptr_t offset ) const {\n-  \/\/ Adding to 'TOP' offset?  Return 'TOP'!\n-  if( _offset == OffsetTop || offset == OffsetTop ) return OffsetTop;\n-  \/\/ Adding to 'BOTTOM' offset?  Return 'BOTTOM'!\n-  if( _offset == OffsetBot || offset == OffsetBot ) return OffsetBot;\n-  \/\/ Addition overflows or \"accidentally\" equals to OffsetTop? Return 'BOTTOM'!\n-  offset += (intptr_t)_offset;\n-  if (offset != (int)offset || offset == OffsetTop) return OffsetBot;\n-\n-  \/\/ assert( _offset >= 0 && _offset+offset >= 0, \"\" );\n-  \/\/ It is possible to construct a negative offset during PhaseCCP\n-\n-  return (int)offset;        \/\/ Sum valid offsets\n+Type::Offset TypePtr::xadd_offset(intptr_t offset) const {\n+  return _offset.add(offset);\n@@ -2779,1 +2893,1 @@\n-  return make(AnyPtr, _ptr, offset, _speculative, _inline_depth);\n+  return make(AnyPtr, _ptr, Offset(offset), _speculative, _inline_depth);\n@@ -2786,1 +2900,1 @@\n-  return _ptr == a->ptr() && _offset == a->offset() && eq_speculative(a) && _inline_depth == a->_inline_depth;\n+  return _ptr == a->ptr() && _offset == a->_offset && eq_speculative(a) && _inline_depth == a->_inline_depth;\n@@ -2792,1 +2906,1 @@\n-  return (uint)_ptr + (uint)_offset + (uint)hash_speculative() + (uint)_inline_depth;\n+  return (uint)_ptr + (uint)offset() + (uint)hash_speculative() + (uint)_inline_depth;\n@@ -3058,3 +3172,1 @@\n-  if( _offset == OffsetTop ) st->print(\"+top\");\n-  else if( _offset == OffsetBot ) st->print(\"+bot\");\n-  else if( _offset ) st->print(\"+%d\", _offset);\n+  _offset.dump2(st);\n@@ -3095,1 +3207,1 @@\n-  return (_offset != OffsetBot) && !below_centerline(_ptr);\n+  return (_offset != Offset::bottom) && !below_centerline(_ptr);\n@@ -3099,1 +3211,1 @@\n-  return (_offset == OffsetTop) || above_centerline(_ptr);\n+  return (_offset == Offset::top) || above_centerline(_ptr);\n@@ -3500,1 +3612,1 @@\n-TypeOopPtr::TypeOopPtr(TYPES t, PTR ptr, ciKlass* k, const TypeInterfaces* interfaces, bool xk, ciObject* o, int offset,\n+TypeOopPtr::TypeOopPtr(TYPES t, PTR ptr, ciKlass* k, const TypeInterfaces* interfaces, bool xk, ciObject* o, Offset offset, Offset field_offset,\n@@ -3516,2 +3628,2 @@\n-      (offset > 0) && xk && (k != nullptr) && k->is_instance_klass()) {\n-    _is_ptr_to_boxed_value = k->as_instance_klass()->is_boxed_value_offset(offset);\n+      (offset.get() > 0) && xk && (k != nullptr) && k->is_instance_klass()) {\n+    _is_ptr_to_boxed_value = k->as_instance_klass()->is_boxed_value_offset(offset.get());\n@@ -3520,2 +3632,2 @@\n-  if (_offset > 0 || _offset == Type::OffsetTop || _offset == Type::OffsetBot) {\n-    if (_offset == oopDesc::klass_offset_in_bytes()) {\n+  if (this->offset() > 0 || this->offset() == Type::OffsetTop || this->offset() == Type::OffsetBot) {\n+    if (this->offset() == oopDesc::klass_offset_in_bytes()) {\n@@ -3527,3 +3639,12 @@\n-    } else if (this->isa_aryptr()) {\n-      _is_ptr_to_narrowoop = (UseCompressedOops && klass()->is_obj_array_klass() &&\n-                             _offset != arrayOopDesc::length_offset_in_bytes());\n+    } else if (UseCompressedOops && this->isa_aryptr() && this->offset() != arrayOopDesc::length_offset_in_bytes()) {\n+      if (klass()->is_obj_array_klass()) {\n+        _is_ptr_to_narrowoop = true;\n+      } else if (klass()->is_flat_array_klass() && field_offset != Offset::top && field_offset != Offset::bottom) {\n+        \/\/ Check if the field of the inline type array element contains oops\n+        ciInlineKlass* vk = klass()->as_flat_array_klass()->element_klass()->as_inline_klass();\n+        int foffset = field_offset.get() + vk->first_field_offset();\n+        ciField* field = vk->get_field_by_offset(foffset, false);\n+        assert(field != nullptr, \"missing field\");\n+        BasicType bt = field->layout_type();\n+        _is_ptr_to_narrowoop = UseCompressedOops && ::is_reference_type(bt);\n+      }\n@@ -3531,1 +3652,0 @@\n-      ciInstanceKlass* ik = klass()->as_instance_klass();\n@@ -3534,1 +3654,1 @@\n-      } else if (_offset == OffsetBot || _offset == OffsetTop) {\n+      } else if (_offset == Offset::bottom || _offset == Offset::top) {\n@@ -3539,3 +3659,2 @@\n-\n-            (_offset == java_lang_Class::klass_offset() ||\n-             _offset == java_lang_Class::array_klass_offset())) {\n+            (this->offset() == java_lang_Class::klass_offset() ||\n+             this->offset() == java_lang_Class::array_klass_offset())) {\n@@ -3547,1 +3666,1 @@\n-                   _offset >= InstanceMirrorKlass::offset_of_static_fields()) {\n+                   this->offset() >= InstanceMirrorKlass::offset_of_static_fields()) {\n@@ -3552,1 +3671,1 @@\n-            field = k->get_field_by_offset(_offset, true);\n+            field = k->get_field_by_offset(this->offset(), true);\n@@ -3563,1 +3682,2 @@\n-          ciField* field = ik->get_field_by_offset(_offset, false);\n+          ciInstanceKlass* ik = klass()->as_instance_klass();\n+          ciField* field = ik->get_field_by_offset(this->offset(), false);\n@@ -3583,2 +3703,2 @@\n-const TypeOopPtr *TypeOopPtr::make(PTR ptr, int offset, int instance_id,\n-                                     const TypePtr* speculative, int inline_depth) {\n+const TypeOopPtr *TypeOopPtr::make(PTR ptr, Offset offset, int instance_id,\n+                                   const TypePtr* speculative, int inline_depth) {\n@@ -3590,1 +3710,1 @@\n-  return (TypeOopPtr*)(new TypeOopPtr(OopPtr, ptr, k, interfaces, xk, o, offset, instance_id, speculative, inline_depth))->hashcons();\n+  return (TypeOopPtr*)(new TypeOopPtr(OopPtr, ptr, k, interfaces, xk, o, offset, Offset::bottom, instance_id, speculative, inline_depth))->hashcons();\n@@ -3615,1 +3735,0 @@\n-\n@@ -3661,1 +3780,1 @@\n-    int offset = meet_offset(tp->offset());\n+    Offset offset = meet_offset(tp->offset());\n@@ -3703,1 +3822,1 @@\n-  return new TypeOopPtr(_base, dual_ptr(), klass(), _interfaces, klass_is_exact(), const_oop(), dual_offset(), dual_instance_id(), dual_speculative(), dual_inline_depth());\n+  return new TypeOopPtr(_base, dual_ptr(), klass(), _interfaces, klass_is_exact(), const_oop(), dual_offset(), Offset::bottom, dual_instance_id(), dual_speculative(), dual_inline_depth());\n@@ -3708,2 +3827,2 @@\n-const TypeOopPtr* TypeOopPtr::make_from_klass_common(ciKlass* klass, bool klass_change, bool try_for_exact, InterfaceHandling interface_handling) {\n-  if (klass->is_instance_klass()) {\n+const TypeOopPtr* TypeOopPtr::make_from_klass_common(ciKlass *klass, bool klass_change, bool try_for_exact, InterfaceHandling interface_handling) {\n+  if (klass->is_instance_klass() || klass->is_inlinetype()) {\n@@ -3736,1 +3855,1 @@\n-    return TypeInstPtr::make(TypePtr::BotPTR, klass, interfaces, klass_is_exact, nullptr, 0);\n+    return TypeInstPtr::make(TypePtr::BotPTR, klass, interfaces, klass_is_exact, nullptr, Offset(0));\n@@ -3738,5 +3857,13 @@\n-    \/\/ Element is an object array. Recursively call ourself.\n-    ciKlass* eklass = klass->as_obj_array_klass()->element_klass();\n-    const TypeOopPtr *etype = TypeOopPtr::make_from_klass_common(eklass, false, try_for_exact, interface_handling);\n-    bool xk = etype->klass_is_exact();\n-    const TypeAry* arr0 = TypeAry::make(etype, TypeInt::POS);\n+    \/\/ Element is an object or inline type array. Recursively call ourself.\n+    const TypeOopPtr* etype = TypeOopPtr::make_from_klass_common(klass->as_array_klass()->element_klass(), \/* klass_change= *\/ false, try_for_exact, interface_handling);\n+    \/\/ Determine null-free\/flat properties\n+    const TypeOopPtr* exact_etype = etype;\n+    if (etype->can_be_inline_type()) {\n+      \/\/ Use exact type if element can be an inline type\n+      exact_etype = TypeOopPtr::make_from_klass_common(klass->as_array_klass()->element_klass(), \/* klass_change= *\/ true, \/* try_for_exact= *\/ true, interface_handling);\n+    }\n+    bool not_null_free = !exact_etype->can_be_inline_type();\n+    bool not_flat = !UseFlatArray || not_null_free || (exact_etype->is_inlinetypeptr() && !exact_etype->inline_klass()->flat_in_array());\n+    \/\/ Even though MyValue is final, [LMyValue is not exact because null-free [LMyValue is a subtype.\n+    bool xk = etype->klass_is_exact() && !etype->is_inlinetypeptr();\n+    const TypeAry* arr0 = TypeAry::make(etype, TypeInt::POS, \/* stable= *\/ false, \/* flat= *\/ false, not_flat, not_null_free);\n@@ -3745,2 +3872,2 @@\n-    \/\/ slam nulls down in the subarrays.\n-    const TypeAryPtr* arr = TypeAryPtr::make(TypePtr::BotPTR, arr0, nullptr, xk, 0);\n+    \/\/ slam nullptrs down in the subarrays.\n+    const TypeAryPtr* arr = TypeAryPtr::make(TypePtr::BotPTR, arr0, nullptr, xk, Offset(0));\n@@ -3751,1 +3878,2 @@\n-    const TypeAry* arr0 = TypeAry::make(etype, TypeInt::POS);\n+    const TypeAry* arr0 = TypeAry::make(etype, TypeInt::POS,\n+                                        \/* stable= *\/ false, \/* flat= *\/ false, \/* not_flat= *\/ true, \/* not_null_free= *\/ true);\n@@ -3754,1 +3882,7 @@\n-    const TypeAryPtr* arr = TypeAryPtr::make(TypePtr::BotPTR, arr0, klass, true, 0);\n+    const TypeAryPtr* arr = TypeAryPtr::make(TypePtr::BotPTR, arr0, klass, true, Offset(0));\n+    return arr;\n+  } else if (klass->is_flat_array_klass()) {\n+    const TypeOopPtr* etype = TypeOopPtr::make_from_klass_raw(klass->as_array_klass()->element_klass(), trust_interfaces);\n+    etype = etype->join_speculative(TypePtr::NOTNULL)->is_oopptr();\n+    const TypeAry* arr0 = TypeAry::make(etype, TypeInt::POS, \/* stable= *\/ false, \/* flat= *\/ true);\n+    const TypeAryPtr* arr = TypeAryPtr::make(TypePtr::BotPTR, arr0, klass, true, Offset(0));\n@@ -3770,2 +3904,2 @@\n-  if (klass->is_instance_klass()) {\n-    \/\/ Element is an instance\n+  if (klass->is_instance_klass() || klass->is_inlinetype()) {\n+    \/\/ Element is an instance or inline type\n@@ -3775,1 +3909,1 @@\n-      return TypeInstPtr::make(TypePtr::NotNull, klass, true, nullptr, 0);\n+      return TypeInstPtr::make(TypePtr::NotNull, klass, true, nullptr, Offset(0));\n@@ -3779,3 +3913,8 @@\n-    const TypeOopPtr *etype =\n-      TypeOopPtr::make_from_klass_raw(klass->as_obj_array_klass()->element_klass(), trust_interfaces);\n-    const TypeAry* arr0 = TypeAry::make(etype, TypeInt::make(o->as_array()->length()));\n+    const TypeOopPtr* etype = TypeOopPtr::make_from_klass_raw(klass->as_array_klass()->element_klass(), trust_interfaces);\n+    bool is_flat = o->as_obj_array()->is_flat();\n+    bool is_null_free = o->as_obj_array()->is_null_free();\n+    if (is_null_free) {\n+      etype = etype->join_speculative(TypePtr::NOTNULL)->is_oopptr();\n+    }\n+    const TypeAry* arr0 = TypeAry::make(etype, TypeInt::make(o->as_array()->length()),\n+                                        \/* stable= *\/ false, \/* flat= *\/ false, \/* not_flat= *\/ !is_flat, \/* not_null_free= *\/ !is_null_free);\n@@ -3786,1 +3925,1 @@\n-      return TypeAryPtr::make(TypePtr::Constant, o, arr0, klass, true, 0);\n+      return TypeAryPtr::make(TypePtr::Constant, o, arr0, klass, true, Offset(0));\n@@ -3788,1 +3927,1 @@\n-      return TypeAryPtr::make(TypePtr::NotNull, arr0, klass, true, 0);\n+      return TypeAryPtr::make(TypePtr::NotNull, arr0, klass, true, Offset(0));\n@@ -3792,3 +3931,3 @@\n-    const Type* etype =\n-      (Type*)get_const_basic_type(klass->as_type_array_klass()->element_type());\n-    const TypeAry* arr0 = TypeAry::make(etype, TypeInt::make(o->as_array()->length()));\n+    const Type* etype = (Type*)get_const_basic_type(klass->as_type_array_klass()->element_type());\n+    const TypeAry* arr0 = TypeAry::make(etype, TypeInt::make(o->as_array()->length()),\n+                                        \/* stable= *\/ false, \/* flat= *\/ false, \/* not_flat= *\/ true, \/* not_null_free= *\/ true);\n@@ -3798,1 +3937,13 @@\n-      return TypeAryPtr::make(TypePtr::Constant, o, arr0, klass, true, 0);\n+      return TypeAryPtr::make(TypePtr::Constant, o, arr0, klass, true, Offset(0));\n+    } else {\n+      return TypeAryPtr::make(TypePtr::NotNull, arr0, klass, true, Offset(0));\n+    }\n+  } else if (klass->is_flat_array_klass()) {\n+    const TypeOopPtr* etype = TypeOopPtr::make_from_klass_raw(klass->as_array_klass()->element_klass(), trust_interfaces);\n+    etype = etype->join_speculative(TypePtr::NOTNULL)->is_oopptr();\n+    const TypeAry* arr0 = TypeAry::make(etype, TypeInt::make(o->as_array()->length()), \/* stable= *\/ false, \/* flat= *\/ true);\n+    \/\/ We used to pass NotNull in here, asserting that the sub-arrays\n+    \/\/ are all not-null.  This is not true in generally, as code can\n+    \/\/ slam nullptrs down in the subarrays.\n+    if (make_constant) {\n+      return TypeAryPtr::make(TypePtr::Constant, o, arr0, klass, true, Offset(0));\n@@ -3800,1 +3951,1 @@\n-      return TypeAryPtr::make(TypePtr::NotNull, arr0, klass, true, 0);\n+      return TypeAryPtr::make(TypePtr::NotNull, arr0, klass, true, Offset(0));\n@@ -3811,1 +3962,1 @@\n-  assert( _offset >= 0, \"\" );\n+  assert(offset() >= 0, \"\");\n@@ -3813,1 +3964,1 @@\n-  if (_offset != 0) {\n+  if (offset() != 0) {\n@@ -3874,6 +4025,1 @@\n-  switch( _offset ) {\n-  case OffsetTop: st->print(\"+top\"); break;\n-  case OffsetBot: st->print(\"+any\"); break;\n-  case         0: break;\n-  default:        st->print(\"+%d\",_offset); break;\n-  }\n+  _offset.dump2(st);\n@@ -3896,1 +4042,1 @@\n-  return (_offset == 0) && !below_centerline(_ptr);\n+  return (offset() == 0) && !below_centerline(_ptr);\n@@ -3905,1 +4051,1 @@\n-  return make(_ptr, offset, _instance_id, with_offset_speculative(offset), _inline_depth);\n+  return make(_ptr, Offset(offset), _instance_id, with_offset_speculative(offset), _inline_depth);\n@@ -4018,3 +4164,4 @@\n-TypeInstPtr::TypeInstPtr(PTR ptr, ciKlass* k, const TypeInterfaces* interfaces, bool xk, ciObject* o, int off,\n-                         int instance_id, const TypePtr* speculative, int inline_depth)\n-  : TypeOopPtr(InstPtr, ptr, k, interfaces, xk, o, off, instance_id, speculative, inline_depth) {\n+TypeInstPtr::TypeInstPtr(PTR ptr, ciKlass* k, const TypeInterfaces* interfaces, bool xk, ciObject* o, Offset off,\n+                         bool flat_in_array, int instance_id, const TypePtr* speculative, int inline_depth)\n+  : TypeOopPtr(InstPtr, ptr, k, interfaces, xk, o, off, Offset::bottom, instance_id, speculative, inline_depth),\n+    _flat_in_array(flat_in_array) {\n@@ -4025,0 +4172,2 @@\n+  assert(!klass()->flat_in_array() || flat_in_array, \"Should be flat in array\");\n+  assert(!flat_in_array || can_be_inline_type(), \"Only inline types can be flat in array\");\n@@ -4033,1 +4182,2 @@\n-                                     int offset,\n+                                     Offset offset,\n+                                     bool flat_in_array,\n@@ -4055,0 +4205,3 @@\n+  \/\/ Check if this type is known to be flat in arrays\n+  flat_in_array = flat_in_array || k->flat_in_array();\n+\n@@ -4057,1 +4210,1 @@\n-    (TypeInstPtr*)(new TypeInstPtr(ptr, k, interfaces, xk, o ,offset, instance_id, speculative, inline_depth))->hashcons();\n+    (TypeInstPtr*)(new TypeInstPtr(ptr, k, interfaces, xk, o, offset, flat_in_array, instance_id, speculative, inline_depth))->hashcons();\n@@ -4123,1 +4276,1 @@\n-  return make(ptr, klass(), _interfaces, klass_is_exact(), ptr == Constant ? const_oop() : nullptr, _offset, _instance_id, _speculative, _inline_depth);\n+  return make(ptr, klass(), _interfaces, klass_is_exact(), ptr == Constant ? const_oop() : nullptr, _offset, _flat_in_array, _instance_id, _speculative, _inline_depth);\n@@ -4134,1 +4287,1 @@\n-  return make(ptr(), klass(), _interfaces, klass_is_exact, const_oop(), _offset, _instance_id, _speculative, _inline_depth);\n+  return make(ptr(), klass(), _interfaces, klass_is_exact, const_oop(), _offset, _flat_in_array, _instance_id, _speculative, _inline_depth);\n@@ -4140,1 +4293,1 @@\n-  return make(_ptr, klass(),  _interfaces, _klass_is_exact, const_oop(), _offset, instance_id, _speculative, _inline_depth);\n+  return make(_ptr, klass(), _interfaces, _klass_is_exact, const_oop(), _offset, _flat_in_array, instance_id, _speculative, _inline_depth);\n@@ -4147,1 +4300,1 @@\n-  int off = meet_offset(tinst->offset());\n+  Offset off = meet_offset(tinst->offset());\n@@ -4172,1 +4325,1 @@\n-    else if (loaded->ptr() == TypePtr::AnyNull)  { return make(ptr, unloaded->klass(), interfaces, false, nullptr, off, instance_id, speculative, depth); }\n+    else if (loaded->ptr() == TypePtr::AnyNull)  { return make(ptr, unloaded->klass(), interfaces, false, nullptr, off, false, instance_id, speculative, depth); }\n@@ -4233,1 +4386,1 @@\n-    int offset = meet_offset(tp->offset());\n+    Offset offset = meet_offset(tp->offset());\n@@ -4242,1 +4395,1 @@\n-                  (ptr == Constant ? const_oop() : nullptr), offset, instance_id, speculative, depth);\n+                  (ptr == Constant ? const_oop() : nullptr), offset, flat_in_array(), instance_id, speculative, depth);\n@@ -4258,1 +4411,1 @@\n-    int offset = meet_offset(tp->offset());\n+    Offset offset = meet_offset(tp->offset());\n@@ -4270,1 +4423,1 @@\n-                  (ptr == Constant ? const_oop() : nullptr), offset, instance_id, speculative, depth);\n+                  (ptr == Constant ? const_oop() : nullptr), offset, flat_in_array(), instance_id, speculative, depth);\n@@ -4298,1 +4451,1 @@\n-    int off = meet_offset(tinst->offset());\n+    Offset off = meet_offset(tinst->offset());\n@@ -4310,0 +4463,1 @@\n+    bool res_flat_in_array = false;\n@@ -4311,1 +4465,1 @@\n-    MeetResult kind = meet_instptr(ptr, interfaces, this, tinst, res_klass, res_xk);\n+    MeetResult kind = meet_instptr(ptr, interfaces, this, tinst, res_klass, res_xk, res_flat_in_array);\n@@ -4352,1 +4506,1 @@\n-      res = make(ptr, res_klass, interfaces, res_xk, o, off, instance_id, speculative, depth);\n+      res = make(ptr, res_klass, interfaces, res_xk, o, off, res_flat_in_array, instance_id, speculative, depth);\n@@ -4364,1 +4518,1 @@\n-                                                            ciKlass*& res_klass, bool& res_xk) {\n+                                                            ciKlass*& res_klass, bool& res_xk, bool& res_flat_in_array) {\n@@ -4367,0 +4521,5 @@\n+  const bool this_flat_in_array = this_type->flat_in_array();\n+  const bool other_flat_in_array = other_type->flat_in_array();\n+  const bool this_not_flat_in_array = this_type->not_flat_in_array();\n+  const bool other_not_flat_in_array = other_type->not_flat_in_array();\n+\n@@ -4377,1 +4536,1 @@\n-  if (ptr != Constant && this_klass->equals(other_klass) && this_xk == other_xk) {\n+  if (ptr != Constant && this_klass->equals(other_klass) && this_xk == other_xk && this_flat_in_array == other_flat_in_array) {\n@@ -4380,0 +4539,1 @@\n+    res_flat_in_array = this_flat_in_array;\n@@ -4413,0 +4573,9 @@\n+  \/\/ Flat in array matrix, yes = y, no = n, maybe = m, top\/empty = T:\n+  \/\/        yes maybe no   -> Super Klass\n+  \/\/   yes   y    y    y\n+  \/\/ maybe   y    m    m\n+  \/\/    no   T    n    n\n+  \/\/    |\n+  \/\/    v\n+  \/\/ Sub Klass\n+\n@@ -4415,0 +4584,1 @@\n+  bool flat_in_array = false;\n@@ -4418,1 +4588,2 @@\n-  } else if (!other_xk && this_type->is_meet_subtype_of(other_type)) {\n+    flat_in_array = below_centerline(ptr) ? (this_flat_in_array && other_flat_in_array) : (this_flat_in_array || other_flat_in_array);\n+  } else if (!other_xk && is_meet_subtype_of(this_type, other_type)) {\n@@ -4421,1 +4592,3 @@\n-  } else if(!this_xk && other_type->is_meet_subtype_of(this_type)) {\n+    bool other_flat_this_maybe_flat = other_flat_in_array && (!this_flat_in_array && !this_not_flat_in_array);\n+    flat_in_array = this_flat_in_array || other_flat_this_maybe_flat;\n+  } else if (!this_xk && is_meet_subtype_of(other_type, this_type)) {\n@@ -4424,0 +4597,2 @@\n+    bool this_flat_other_maybe_flat = this_flat_in_array && (!other_flat_in_array && !other_not_flat_in_array);\n+    flat_in_array = other_flat_in_array || this_flat_other_maybe_flat;\n@@ -4427,1 +4602,2 @@\n-    if (above_centerline(ptr)) { \/\/ both are up?\n+    if (above_centerline(ptr)) {\n+      \/\/ Both types are empty.\n@@ -4431,1 +4607,2 @@\n-      this_type = other_type; \/\/ tinst is down; keep down man\n+      \/\/ this_type is empty while other_type is not. Take other_type.\n+      this_type = other_type;\n@@ -4433,0 +4610,1 @@\n+      flat_in_array = other_flat_in_array;\n@@ -4434,0 +4612,1 @@\n+      \/\/ other_type is empty while this_type is not. Take this_type.\n@@ -4435,1 +4614,1 @@\n-      other_xk = this_xk;\n+      flat_in_array = this_flat_in_array;\n@@ -4437,0 +4616,1 @@\n+      \/\/ this_type and other_type are both non-empty.\n@@ -4448,0 +4628,1 @@\n+    res_flat_in_array = subtype ? flat_in_array : this_flat_in_array;\n@@ -4464,0 +4645,1 @@\n+  res_flat_in_array = this_flat_in_array && other_flat_in_array;\n@@ -4468,0 +4650,4 @@\n+template<class T> bool TypePtr::is_meet_subtype_of(const T* sub_type, const T* super_type) {\n+  return sub_type->is_meet_subtype_of(super_type) && !(super_type->flat_in_array() && sub_type->not_flat_in_array());\n+}\n+\n@@ -4469,1 +4655,1 @@\n-ciType* TypeInstPtr::java_mirror_type() const {\n+ciType* TypeInstPtr::java_mirror_type(bool* is_null_free_array) const {\n@@ -4475,2 +4661,1 @@\n-\n-  return const_oop()->as_instance()->java_mirror_type();\n+  return const_oop()->as_instance()->java_mirror_type(is_null_free_array);\n@@ -4484,1 +4669,1 @@\n-  return new TypeInstPtr(dual_ptr(), klass(), _interfaces, klass_is_exact(), const_oop(), dual_offset(), dual_instance_id(), dual_speculative(), dual_inline_depth());\n+  return new TypeInstPtr(dual_ptr(), klass(), _interfaces, klass_is_exact(), const_oop(), dual_offset(), flat_in_array(), dual_instance_id(), dual_speculative(), dual_inline_depth());\n@@ -4493,0 +4678,1 @@\n+    flat_in_array() == p->flat_in_array() &&\n@@ -4500,1 +4686,1 @@\n-  return klass()->hash() + TypeOopPtr::hash() + _interfaces->hash();\n+  return klass()->hash() + TypeOopPtr::hash() + _interfaces->hash() + (uint)flat_in_array();\n@@ -4554,5 +4740,1 @@\n-  if( _offset ) {               \/\/ Dump offset, if any\n-    if( _offset == OffsetBot )      st->print(\"+any\");\n-    else if( _offset == OffsetTop ) st->print(\"+unknown\");\n-    else st->print(\"+%d\", _offset);\n-  }\n+  _offset.dump2(st);\n@@ -4561,0 +4743,5 @@\n+\n+  if (flat_in_array() && !klass()->is_inlinetype()) {\n+    st->print(\" (flat in array)\");\n+  }\n+\n@@ -4573,1 +4760,1 @@\n-  return make(_ptr, klass(), _interfaces, klass_is_exact(), const_oop(), xadd_offset(offset),\n+  return make(_ptr, klass(), _interfaces, klass_is_exact(), const_oop(), xadd_offset(offset), flat_in_array(),\n@@ -4578,1 +4765,1 @@\n-  return make(_ptr, klass(), _interfaces, klass_is_exact(), const_oop(), offset,\n+  return make(_ptr, klass(), _interfaces, klass_is_exact(), const_oop(), Offset(offset), flat_in_array(),\n@@ -4587,1 +4774,1 @@\n-  return make(_ptr, klass(), _interfaces, klass_is_exact(), const_oop(), _offset,\n+  return make(_ptr, klass(), _interfaces, klass_is_exact(), const_oop(), _offset, flat_in_array(),\n@@ -4592,1 +4779,1 @@\n-  return make(_ptr, klass(), _interfaces, klass_is_exact(), const_oop(), _offset, _instance_id, speculative, _inline_depth);\n+  return make(_ptr, klass(), _interfaces, klass_is_exact(), const_oop(), _offset, flat_in_array(), _instance_id, speculative, _inline_depth);\n@@ -4599,1 +4786,1 @@\n-  return make(_ptr, klass(), _interfaces, klass_is_exact(), const_oop(), _offset, _instance_id, _speculative, depth);\n+  return make(_ptr, klass(), _interfaces, klass_is_exact(), const_oop(), _offset, flat_in_array(), _instance_id, _speculative, depth);\n@@ -4604,1 +4791,5 @@\n-  return make(_ptr, klass(), _interfaces, klass_is_exact(), const_oop(), _offset, instance_id, _speculative, _inline_depth);\n+  return make(_ptr, klass(), _interfaces, klass_is_exact(), const_oop(), _offset, flat_in_array(), instance_id, _speculative, _inline_depth);\n+}\n+\n+const TypeInstPtr *TypeInstPtr::cast_to_flat_in_array() const {\n+  return make(_ptr, klass(), _interfaces, klass_is_exact(), const_oop(), _offset, true, _instance_id, _speculative, _inline_depth);\n@@ -4618,1 +4809,1 @@\n-  return TypeInstKlassPtr::make(xk ? TypePtr::Constant : TypePtr::NotNull, klass(), _interfaces, 0);\n+  return TypeInstKlassPtr::make(xk ? TypePtr::Constant : TypePtr::NotNull, klass(), _interfaces, Offset(0), flat_in_array());\n@@ -4663,1 +4854,0 @@\n-\n@@ -4695,0 +4885,1 @@\n+const TypeAryPtr *TypeAryPtr::INLINES;\n@@ -4697,1 +4888,1 @@\n-const TypeAryPtr *TypeAryPtr::make(PTR ptr, const TypeAry *ary, ciKlass* k, bool xk, int offset,\n+const TypeAryPtr* TypeAryPtr::make(PTR ptr, const TypeAry *ary, ciKlass* k, bool xk, Offset offset, Offset field_offset,\n@@ -4707,1 +4898,4 @@\n-  return (TypeAryPtr*)(new TypeAryPtr(ptr, nullptr, ary, k, xk, offset, instance_id, false, speculative, inline_depth))->hashcons();\n+  if (k != nullptr && k->is_flat_array_klass() && !ary->_flat) {\n+    k = nullptr;\n+  }\n+  return (TypeAryPtr*)(new TypeAryPtr(ptr, nullptr, ary, k, xk, offset, field_offset, instance_id, false, speculative, inline_depth))->hashcons();\n@@ -4711,1 +4905,1 @@\n-const TypeAryPtr *TypeAryPtr::make(PTR ptr, ciObject* o, const TypeAry *ary, ciKlass* k, bool xk, int offset,\n+const TypeAryPtr* TypeAryPtr::make(PTR ptr, ciObject* o, const TypeAry *ary, ciKlass* k, bool xk, Offset offset, Offset field_offset,\n@@ -4723,1 +4917,4 @@\n-  return (TypeAryPtr*)(new TypeAryPtr(ptr, o, ary, k, xk, offset, instance_id, is_autobox_cache, speculative, inline_depth))->hashcons();\n+  if (k != nullptr && k->is_flat_array_klass() && !ary->_flat) {\n+    k = nullptr;\n+  }\n+  return (TypeAryPtr*)(new TypeAryPtr(ptr, o, ary, k, xk, offset, field_offset, instance_id, is_autobox_cache, speculative, inline_depth))->hashcons();\n@@ -4729,1 +4926,1 @@\n-  return make(ptr, ptr == Constant ? const_oop() : nullptr, _ary, klass(), klass_is_exact(), _offset, _instance_id, _speculative, _inline_depth);\n+  return make(ptr, ptr == Constant ? const_oop() : nullptr, _ary, klass(), klass_is_exact(), _offset, _field_offset, _instance_id, _speculative, _inline_depth, _is_autobox_cache);\n@@ -4737,1 +4934,1 @@\n-  return make(ptr(), const_oop(), _ary, klass(), klass_is_exact, _offset, _instance_id, _speculative, _inline_depth);\n+  return make(ptr(), const_oop(), _ary, klass(), klass_is_exact, _offset, _field_offset, _instance_id, _speculative, _inline_depth, _is_autobox_cache);\n@@ -4743,1 +4940,1 @@\n-  return make(_ptr, const_oop(), _ary, klass(), _klass_is_exact, _offset, instance_id, _speculative, _inline_depth);\n+  return make(_ptr, const_oop(), _ary, klass(), _klass_is_exact, _offset, _field_offset, instance_id, _speculative, _inline_depth, _is_autobox_cache);\n@@ -4799,2 +4996,62 @@\n-  const TypeAry* new_ary = TypeAry::make(elem(), new_size, is_stable());\n-  return make(ptr(), const_oop(), new_ary, klass(), klass_is_exact(), _offset, _instance_id, _speculative, _inline_depth);\n+  const TypeAry* new_ary = TypeAry::make(elem(), new_size, is_stable(), is_flat(), is_not_flat(), is_not_null_free());\n+  return make(ptr(), const_oop(), new_ary, klass(), klass_is_exact(), _offset, _field_offset, _instance_id, _speculative, _inline_depth, _is_autobox_cache);\n+}\n+\n+\/\/-------------------------------cast_to_not_flat------------------------------\n+const TypeAryPtr* TypeAryPtr::cast_to_not_flat(bool not_flat) const {\n+  if (not_flat == is_not_flat()) {\n+    return this;\n+  }\n+  assert(!not_flat || !is_flat(), \"inconsistency\");\n+  const TypeAry* new_ary = TypeAry::make(elem(), size(), is_stable(), is_flat(), not_flat, is_not_null_free());\n+  const TypeAryPtr* res = make(ptr(), const_oop(), new_ary, klass(), klass_is_exact(), _offset, _field_offset, _instance_id, _speculative, _inline_depth, _is_autobox_cache);\n+  \/\/ We keep the speculative part if it contains information about flat-\/nullability.\n+  \/\/ Make sure it's removed if it's not better than the non-speculative type anymore.\n+  if (res->speculative() == res->remove_speculative()) {\n+    return res->remove_speculative();\n+  }\n+  return res;\n+}\n+\n+\/\/-------------------------------cast_to_not_null_free-------------------------\n+const TypeAryPtr* TypeAryPtr::cast_to_not_null_free(bool not_null_free) const {\n+  if (not_null_free == is_not_null_free()) {\n+    return this;\n+  }\n+  assert(!not_null_free || !is_flat(), \"inconsistency\");\n+  const TypeAry* new_ary = TypeAry::make(elem(), size(), is_stable(), is_flat(), \/* not_flat= *\/ not_null_free ? true : is_not_flat(), not_null_free);\n+  const TypeAryPtr* res = make(ptr(), const_oop(), new_ary, klass(), klass_is_exact(), _offset, _field_offset,\n+                               _instance_id, _speculative, _inline_depth, _is_autobox_cache);\n+  \/\/ We keep the speculative part if it contains information about flat-\/nullability.\n+  \/\/ Make sure it's removed if it's not better than the non-speculative type anymore.\n+  if (res->speculative() == res->remove_speculative()) {\n+    return res->remove_speculative();\n+  }\n+  return res;\n+}\n+\n+\/\/---------------------------------update_properties---------------------------\n+const TypeAryPtr* TypeAryPtr::update_properties(const TypeAryPtr* from) const {\n+  if ((from->is_flat()          && is_not_flat()) ||\n+      (from->is_not_flat()      && is_flat()) ||\n+      (from->is_null_free()     && is_not_null_free()) ||\n+      (from->is_not_null_free() && is_null_free())) {\n+    return nullptr; \/\/ Inconsistent properties\n+  } else if (from->is_not_null_free()) {\n+    return cast_to_not_null_free(); \/\/ Implies not flat\n+  } else if (from->is_not_flat()) {\n+    return cast_to_not_flat();\n+  }\n+  return this;\n+}\n+\n+jint TypeAryPtr::flat_layout_helper() const {\n+  return klass()->as_flat_array_klass()->layout_helper();\n+}\n+\n+int TypeAryPtr::flat_elem_size() const {\n+  return klass()->as_flat_array_klass()->element_byte_size();\n+}\n+\n+int TypeAryPtr::flat_log_elem_size() const {\n+  return klass()->as_flat_array_klass()->log2_element_size();\n@@ -4816,1 +5073,1 @@\n-  const TypeAry* new_ary = TypeAry::make(elem, size(), stable);\n+  const TypeAry* new_ary = TypeAry::make(elem, size(), stable, is_flat(), is_not_flat(), is_not_null_free());\n@@ -4818,1 +5075,1 @@\n-  return make(ptr(), const_oop(), new_ary, klass(), klass_is_exact(), _offset, _instance_id, _speculative, _inline_depth);\n+  return make(ptr(), const_oop(), new_ary, klass(), klass_is_exact(), _offset, _field_offset, _instance_id, _speculative, _inline_depth, _is_autobox_cache);\n@@ -4838,2 +5095,2 @@\n-  const TypeAry* new_ary = TypeAry::make(etype, size(), is_stable());\n-  return make(ptr(), const_oop(), new_ary, klass(), klass_is_exact(), _offset, _instance_id, _speculative, _inline_depth, \/*is_autobox_cache=*\/true);\n+  const TypeAry* new_ary = TypeAry::make(etype, size(), is_stable(), is_flat(), is_not_flat(), is_not_null_free());\n+  return make(ptr(), const_oop(), new_ary, klass(), klass_is_exact(), _offset, _field_offset, _instance_id, _speculative, _inline_depth, \/*is_autobox_cache=*\/true);\n@@ -4848,1 +5105,2 @@\n-    TypeOopPtr::eq(p);  \/\/ Check sub-parts\n+    TypeOopPtr::eq(p) &&\/\/ Check sub-parts\n+    _field_offset == p->_field_offset;\n@@ -4854,1 +5112,1 @@\n-  return (uint)(uintptr_t)_ary + TypeOopPtr::hash();\n+  return (uint)(uintptr_t)_ary + TypeOopPtr::hash() + _field_offset.get();\n@@ -4898,1 +5156,1 @@\n-    int offset = meet_offset(tp->offset());\n+    Offset offset = meet_offset(tp->offset());\n@@ -4907,1 +5165,1 @@\n-                  _ary, _klass, _klass_is_exact, offset, instance_id, speculative, depth);\n+                  _ary, _klass, _klass_is_exact, offset, _field_offset, instance_id, speculative, depth);\n@@ -4921,1 +5179,1 @@\n-    int offset = meet_offset(tp->offset());\n+    Offset offset = meet_offset(tp->offset());\n@@ -4937,1 +5195,1 @@\n-                  _ary, _klass, _klass_is_exact, offset, instance_id, speculative, depth);\n+                  _ary, _klass, _klass_is_exact, offset, _field_offset, instance_id, speculative, depth);\n@@ -4951,1 +5209,2 @@\n-    int off = meet_offset(tap->offset());\n+    Offset off = meet_offset(tap->offset());\n+    Offset field_off = meet_field_offset(tap->field_offset());\n@@ -4960,0 +5219,3 @@\n+    bool res_flat = false;\n+    bool res_not_flat = false;\n+    bool res_not_null_free = false;\n@@ -4961,1 +5223,1 @@\n-    if (meet_aryptr(ptr, elem, this, tap, res_klass, res_xk) == NOT_SUBTYPE) {\n+    if (meet_aryptr(ptr, elem, this, tap, res_klass, res_xk, res_flat, res_not_flat, res_not_null_free) == NOT_SUBTYPE) {\n@@ -4963,0 +5225,14 @@\n+    } else if (this->is_flat() != tap->is_flat()) {\n+      \/\/ Meeting flat inline type array with non-flat array. Adjust (field) offset accordingly.\n+      if (tary->_flat) {\n+        \/\/ Result is in a flat representation\n+        off = Offset(is_flat() ? offset() : tap->offset());\n+        field_off = is_flat() ? field_offset() : tap->field_offset();\n+      } else if (below_centerline(ptr)) {\n+        \/\/ Result is in a non-flat representation\n+        off = Offset(flat_offset()).meet(Offset(tap->flat_offset()));\n+        field_off = (field_off == Offset::top) ? Offset::top : Offset::bottom;\n+      } else if (flat_offset() == tap->flat_offset()) {\n+        off = Offset(!is_flat() ? offset() : tap->offset());\n+        field_off = !is_flat() ? field_offset() : tap->field_offset();\n+      }\n@@ -4980,1 +5256,1 @@\n-    return make(ptr, o, TypeAry::make(elem, tary->_size, tary->_stable), res_klass, res_xk, off, instance_id, speculative, depth);\n+    return make(ptr, o, TypeAry::make(elem, tary->_size, tary->_stable, res_flat, res_not_flat, res_not_null_free), res_klass, res_xk, off, field_off, instance_id, speculative, depth);\n@@ -4986,1 +5262,1 @@\n-    int offset = meet_offset(tp->offset());\n+    Offset offset = meet_offset(tp->offset());\n@@ -5001,2 +5277,2 @@\n-      if (tp->klass()->equals(ciEnv::current()->Object_klass()) && this_interfaces->contains(tp_interfaces) && !tp->klass_is_exact()) {\n-        return TypeAryPtr::make(ptr, _ary, _klass, _klass_is_exact, offset, instance_id, speculative, depth);\n+      if (tp->klass()->equals(ciEnv::current()->Object_klass()) && this_interfaces->contains(tp_interfaces) && !tp->klass_is_exact() && !tp->flat_in_array()) {\n+        return TypeAryPtr::make(ptr, _ary, _klass, _klass_is_exact, offset, _field_offset, instance_id, speculative, depth);\n@@ -5008,1 +5284,1 @@\n-        return TypeInstPtr::make(ptr, ciEnv::current()->Object_klass(), interfaces, false, nullptr,offset, instance_id, speculative, depth);\n+        return TypeInstPtr::make(ptr, ciEnv::current()->Object_klass(), interfaces, false, nullptr, offset, false, instance_id, speculative, depth);\n@@ -5020,1 +5296,1 @@\n-        if (tp->klass()->equals(ciEnv::current()->Object_klass()) && this_interfaces->contains(tp_interfaces) && !tp->klass_is_exact()) {\n+        if (tp->klass()->equals(ciEnv::current()->Object_klass()) && this_interfaces->contains(tp_interfaces) && !tp->klass_is_exact() && !tp->flat_in_array()) {\n@@ -5023,1 +5299,1 @@\n-                      _ary, _klass, _klass_is_exact, offset, instance_id, speculative, depth);\n+                      _ary, _klass, _klass_is_exact, offset, _field_offset, instance_id, speculative, depth);\n@@ -5035,1 +5311,1 @@\n-      return TypeInstPtr::make(ptr, ciEnv::current()->Object_klass(), interfaces, false, nullptr, offset, instance_id, speculative, depth);\n+      return TypeInstPtr::make(ptr, ciEnv::current()->Object_klass(), interfaces, false, nullptr, offset, false, instance_id, speculative, depth);\n@@ -5044,2 +5320,2 @@\n-template<class T> TypePtr::MeetResult TypePtr::meet_aryptr(PTR& ptr, const Type*& elem, const T* this_ary,\n-                                                           const T* other_ary, ciKlass*& res_klass, bool& res_xk) {\n+template<class T> TypePtr::MeetResult TypePtr::meet_aryptr(PTR& ptr, const Type*& elem, const T* this_ary, const T* other_ary,\n+                                                           ciKlass*& res_klass, bool& res_xk, bool &res_flat, bool& res_not_flat, bool& res_not_null_free) {\n@@ -5055,0 +5331,6 @@\n+  bool this_flat = this_ary->is_flat();\n+  bool this_not_flat = this_ary->is_not_flat();\n+  bool other_flat = other_ary->is_flat();\n+  bool other_not_flat = other_ary->is_not_flat();\n+  bool this_not_null_free = this_ary->is_not_null_free();\n+  bool other_not_null_free = other_ary->is_not_null_free();\n@@ -5057,0 +5339,4 @@\n+  res_flat = this_flat && other_flat;\n+  res_not_flat = this_not_flat && other_not_flat;\n+  res_not_null_free = this_not_null_free && other_not_null_free;\n+\n@@ -5060,3 +5346,3 @@\n-    if (this_top_or_bottom)\n-      res_klass = other_klass;\n-    else if (other_top_or_bottom || other_klass == this_klass) {\n+      if (this_top_or_bottom) {\n+        res_klass = other_klass;\n+      } else if (other_top_or_bottom || other_klass == this_klass) {\n@@ -5104,0 +5390,3 @@\n+        if (this_ary->is_flat()) {\n+          elem = this_ary->elem();\n+        }\n@@ -5107,1 +5396,1 @@\n-      return result;\n+      break;\n@@ -5111,1 +5400,1 @@\n-      } else if(above_centerline(this_ptr)) {\n+      } else if (above_centerline(this_ptr)) {\n@@ -5116,0 +5405,5 @@\n+        \/\/ Even though MyValue is final, [LMyValue is only exact if the array\n+        \/\/ is null-free due to null-free [LMyValue <: null-able [LMyValue.\n+        if (res_xk && !res_not_null_free) {\n+          res_xk = false;\n+        }\n@@ -5117,1 +5411,1 @@\n-      return result;\n+      break;\n@@ -5124,0 +5418,3 @@\n+        if (other_ary->is_flat()) {\n+          elem = other_ary->elem();\n+        }\n@@ -5127,0 +5424,5 @@\n+        \/\/ Even though MyValue is final, [LMyValue is only exact if the array\n+        \/\/ is null-free due to null-free [LMyValue <: null-able [LMyValue.\n+        if (res_xk && !res_not_null_free) {\n+          res_xk = false;\n+        }\n@@ -5128,1 +5430,1 @@\n-      return result;\n+      break;\n@@ -5141,1 +5443,10 @@\n-  return new TypeAryPtr(dual_ptr(), _const_oop, _ary->dual()->is_ary(),_klass, _klass_is_exact, dual_offset(), dual_instance_id(), is_autobox_cache(), dual_speculative(), dual_inline_depth());\n+  return new TypeAryPtr(dual_ptr(), _const_oop, _ary->dual()->is_ary(), _klass, _klass_is_exact, dual_offset(), dual_field_offset(), dual_instance_id(), is_autobox_cache(), dual_speculative(), dual_inline_depth());\n+}\n+\n+Type::Offset TypeAryPtr::meet_field_offset(const Type::Offset offset) const {\n+  return _field_offset.meet(offset);\n+}\n+\n+\/\/------------------------------dual_offset------------------------------------\n+Type::Offset TypeAryPtr::dual_field_offset() const {\n+  return _field_offset.dual();\n@@ -5169,1 +5480,10 @@\n-  if( _offset != 0 ) {\n+  if (is_flat()) {\n+    st->print(\":flat\");\n+    st->print(\"(\");\n+    _field_offset.dump2(st);\n+    st->print(\")\");\n+  }\n+  if (is_null_free()) {\n+    st->print(\":null_free\");\n+  }\n+  if (offset() != 0) {\n@@ -5172,3 +5492,3 @@\n-    if( _offset == OffsetTop )       st->print(\"+undefined\");\n-    else if( _offset == OffsetBot )  st->print(\"+any\");\n-    else if( _offset < header_size ) st->print(\"+%d\", _offset);\n+    if( _offset == Offset::top )       st->print(\"+undefined\");\n+    else if( _offset == Offset::bottom )  st->print(\"+any\");\n+    else if( offset() < header_size ) st->print(\"+%d\", offset());\n@@ -5180,1 +5500,1 @@\n-        st->print(\"[%d]\", (_offset - header_size)\/elem_size);\n+        st->print(\"[%d]\", (offset() - header_size)\/elem_size);\n@@ -5197,0 +5517,4 @@\n+  \/\/ FIXME: Does this belong here? Or in the meet code itself?\n+  if (is_flat() && is_not_flat()) {\n+    return true;\n+  }\n@@ -5202,1 +5526,1 @@\n-  return make(_ptr, _const_oop, _ary, _klass, _klass_is_exact, xadd_offset(offset), _instance_id, add_offset_speculative(offset), _inline_depth);\n+  return make(_ptr, _const_oop, _ary, _klass, _klass_is_exact, xadd_offset(offset), _field_offset, _instance_id, add_offset_speculative(offset), _inline_depth, _is_autobox_cache);\n@@ -5206,1 +5530,1 @@\n-  return make(_ptr, _const_oop, _ary, _klass, _klass_is_exact, offset, _instance_id, with_offset_speculative(offset), _inline_depth);\n+  return make(_ptr, _const_oop, _ary, _klass, _klass_is_exact, Offset(offset), _field_offset, _instance_id, with_offset_speculative(offset), _inline_depth, _is_autobox_cache);\n@@ -5210,1 +5534,1 @@\n-  return make(_ptr, _const_oop, ary, _klass, _klass_is_exact, _offset, _instance_id, _speculative, _inline_depth);\n+  return make(_ptr, _const_oop, ary, _klass, _klass_is_exact, _offset, _field_offset, _instance_id, _speculative, _inline_depth, _is_autobox_cache);\n@@ -5218,1 +5542,14 @@\n-  return make(_ptr, _const_oop, _ary->remove_speculative()->is_ary(), _klass, _klass_is_exact, _offset, _instance_id, nullptr, _inline_depth);\n+  return make(_ptr, _const_oop, _ary->remove_speculative()->is_ary(), _klass, _klass_is_exact, _offset, _field_offset, _instance_id, nullptr, _inline_depth, _is_autobox_cache);\n+}\n+\n+const Type* TypeAryPtr::cleanup_speculative() const {\n+  if (speculative() == nullptr) {\n+    return this;\n+  }\n+  \/\/ Keep speculative part if it contains information about flat-\/nullability\n+  const TypeAryPtr* spec_aryptr = speculative()->isa_aryptr();\n+  if (spec_aryptr != nullptr && !above_centerline(spec_aryptr->ptr()) &&\n+      (spec_aryptr->is_not_flat() || spec_aryptr->is_not_null_free())) {\n+    return this;\n+  }\n+  return TypeOopPtr::cleanup_speculative();\n@@ -5225,1 +5562,44 @@\n-  return make(_ptr, _const_oop, _ary->remove_speculative()->is_ary(), _klass, _klass_is_exact, _offset, _instance_id, _speculative, depth);\n+  return make(_ptr, _const_oop, _ary->remove_speculative()->is_ary(), _klass, _klass_is_exact, _offset, _field_offset, _instance_id, _speculative, depth, _is_autobox_cache);\n+}\n+\n+const TypeAryPtr* TypeAryPtr::with_field_offset(int offset) const {\n+  return make(_ptr, _const_oop, _ary->remove_speculative()->is_ary(), _klass, _klass_is_exact, _offset, Offset(offset), _instance_id, _speculative, _inline_depth, _is_autobox_cache);\n+}\n+\n+const TypePtr* TypeAryPtr::add_field_offset_and_offset(intptr_t offset) const {\n+  int adj = 0;\n+  if (is_flat() && offset != Type::OffsetBot && offset != Type::OffsetTop) {\n+    if (_offset.get() != OffsetBot && _offset.get() != OffsetTop) {\n+      adj = _offset.get();\n+      offset += _offset.get();\n+    }\n+    uint header = arrayOopDesc::base_offset_in_bytes(T_OBJECT);\n+    if (_field_offset.get() != OffsetBot && _field_offset.get() != OffsetTop) {\n+      offset += _field_offset.get();\n+      if (_offset.get() == OffsetBot || _offset.get() == OffsetTop) {\n+        offset += header;\n+      }\n+    }\n+    if (elem()->make_oopptr()->is_inlinetypeptr() && (offset >= (intptr_t)header || offset < 0)) {\n+      \/\/ Try to get the field of the inline type array element we are pointing to\n+      ciInlineKlass* vk = elem()->inline_klass();\n+      int shift = flat_log_elem_size();\n+      int mask = (1 << shift) - 1;\n+      intptr_t field_offset = ((offset - header) & mask);\n+      ciField* field = vk->get_field_by_offset(field_offset + vk->first_field_offset(), false);\n+      if (field != nullptr) {\n+        return with_field_offset(field_offset)->add_offset(offset - field_offset - adj);\n+      }\n+    }\n+  }\n+  return add_offset(offset - adj);\n+}\n+\n+\/\/ Return offset incremented by field_offset for flat inline type arrays\n+int TypeAryPtr::flat_offset() const {\n+  int offset = _offset.get();\n+  if (offset != Type::OffsetBot && offset != Type::OffsetTop &&\n+      _field_offset != Offset::bottom && _field_offset != Offset::top) {\n+    offset += _field_offset.get();\n+  }\n+  return offset;\n@@ -5230,1 +5610,1 @@\n-  return make(_ptr, _const_oop, _ary->remove_speculative()->is_ary(), _klass, _klass_is_exact, _offset, instance_id, _speculative, _inline_depth);\n+  return make(_ptr, _const_oop, _ary->remove_speculative()->is_ary(), _klass, _klass_is_exact, _offset, _field_offset, instance_id, _speculative, _inline_depth);\n@@ -5235,0 +5615,1 @@\n+\n@@ -5325,1 +5706,0 @@\n-\n@@ -5409,1 +5789,1 @@\n-  return (_offset == 0) && !below_centerline(_ptr);\n+  return (offset() == 0) && !below_centerline(_ptr);\n@@ -5429,1 +5809,1 @@\n-  assert( _offset >= 0, \"\" );\n+  assert(offset() >= 0, \"\");\n@@ -5431,1 +5811,1 @@\n-  if (_offset != 0) {\n+  if (offset() != 0) {\n@@ -5482,1 +5862,1 @@\n-    int offset = meet_offset(tp->offset());\n+    Offset offset = meet_offset(tp->offset());\n@@ -5510,1 +5890,1 @@\n-    int offset = meet_offset(tp->offset());\n+    Offset offset = meet_offset(tp->offset());\n@@ -5543,1 +5923,1 @@\n-  switch( _offset ) {\n+  switch (offset()) {\n@@ -5547,1 +5927,1 @@\n-  default:        st->print(\"+%d\",_offset); break;\n+  default:        st->print(\"+%d\",offset()); break;\n@@ -5557,1 +5937,1 @@\n-TypeMetadataPtr::TypeMetadataPtr(PTR ptr, ciMetadata* metadata, int offset):\n+TypeMetadataPtr::TypeMetadataPtr(PTR ptr, ciMetadata* metadata, Offset offset):\n@@ -5562,1 +5942,1 @@\n-  return make(Constant, m, 0);\n+  return make(Constant, m, Offset(0));\n@@ -5565,1 +5945,1 @@\n-  return make(Constant, m, 0);\n+  return make(Constant, m, Offset(0));\n@@ -5570,1 +5950,1 @@\n-const TypeMetadataPtr *TypeMetadataPtr::make(PTR ptr, ciMetadata* m, int offset) {\n+const TypeMetadataPtr* TypeMetadataPtr::make(PTR ptr, ciMetadata* m, Offset offset) {\n@@ -5581,1 +5961,4 @@\n-    if (elem->is_klassptr()->klass_is_exact()) {\n+    if (elem->is_klassptr()->klass_is_exact() &&\n+        \/\/ Even though MyValue is final, [LMyValue is only exact if the array\n+        \/\/ is null-free due to null-free [LMyValue <: null-able [LMyValue.\n+        (is_null_free() || !_ary->_elem->make_oopptr()->is_inlinetypeptr())) {\n@@ -5585,1 +5968,1 @@\n-  return TypeAryKlassPtr::make(xk ? TypePtr::Constant : TypePtr::NotNull, elem, klass(), 0);\n+  return TypeAryKlassPtr::make(xk ? TypePtr::Constant : TypePtr::NotNull, elem, klass(), Offset(0), is_not_flat(), is_not_null_free(), is_null_free());\n@@ -5588,1 +5971,1 @@\n-const TypeKlassPtr* TypeKlassPtr::make(ciKlass *klass, InterfaceHandling interface_handling) {\n+const TypeKlassPtr* TypeKlassPtr::make(ciKlass* klass, InterfaceHandling interface_handling) {\n@@ -5595,1 +5978,1 @@\n-const TypeKlassPtr* TypeKlassPtr::make(PTR ptr, ciKlass* klass, int offset, InterfaceHandling interface_handling) {\n+const TypeKlassPtr* TypeKlassPtr::make(PTR ptr, ciKlass* klass, Offset offset, InterfaceHandling interface_handling) {\n@@ -5603,3 +5986,1 @@\n-\n-\/\/------------------------------TypeKlassPtr-----------------------------------\n-TypeKlassPtr::TypeKlassPtr(TYPES t, PTR ptr, ciKlass* klass, const TypeInterfaces* interfaces, int offset)\n+TypeKlassPtr::TypeKlassPtr(TYPES t, PTR ptr, ciKlass* klass, const TypeInterfaces* interfaces, Offset offset)\n@@ -5608,1 +5989,1 @@\n-         klass->is_type_array_klass() || !klass->as_obj_array_klass()->base_element_klass()->is_interface(), \"no interface here\");\n+         klass->is_type_array_klass() || klass->is_flat_array_klass() || !klass->as_obj_array_klass()->base_element_klass()->is_interface(), \"no interface here\");\n@@ -5647,1 +6028,1 @@\n-  return (_offset == 0) && !below_centerline(_ptr);\n+  return (offset() == 0) && !below_centerline(_ptr);\n@@ -5679,1 +6060,1 @@\n-  assert( _offset >= 0, \"\" );\n+  assert( offset() >= 0, \"\" );\n@@ -5681,1 +6062,1 @@\n-  if (_offset != 0) {\n+  if (offset() != 0) {\n@@ -5725,5 +6106,2 @@\n-\n-  if (_offset) {               \/\/ Dump offset, if any\n-    if (_offset == OffsetBot)      { st->print(\"+any\"); }\n-    else if (_offset == OffsetTop) { st->print(\"+unknown\"); }\n-    else                            { st->print(\"+%d\", _offset); }\n+  if (Verbose) {\n+    if (isa_instklassptr() && is_instklassptr()->flat_in_array()) st->print(\":flat in array\");\n@@ -5731,1 +6109,1 @@\n-\n+  _offset.dump2(st);\n@@ -5747,0 +6125,1 @@\n+    flat_in_array() == p->flat_in_array() &&\n@@ -5751,1 +6130,1 @@\n-  return klass()->hash() + TypeKlassPtr::hash();\n+  return klass()->hash() + TypeKlassPtr::hash() + (uint)flat_in_array();\n@@ -5754,1 +6133,3 @@\n-const TypeInstKlassPtr *TypeInstKlassPtr::make(PTR ptr, ciKlass* k, const TypeInterfaces* interfaces, int offset) {\n+const TypeInstKlassPtr *TypeInstKlassPtr::make(PTR ptr, ciKlass* k, const TypeInterfaces* interfaces, Offset offset, bool flat_in_array) {\n+  flat_in_array = flat_in_array || k->flat_in_array();\n+\n@@ -5756,1 +6137,1 @@\n-    (TypeInstKlassPtr*)(new TypeInstKlassPtr(ptr, k, interfaces, offset))->hashcons();\n+    (TypeInstKlassPtr*)(new TypeInstKlassPtr(ptr, k, interfaces, offset, flat_in_array))->hashcons();\n@@ -5763,2 +6144,2 @@\n-const TypePtr* TypeInstKlassPtr::add_offset( intptr_t offset ) const {\n-  return make( _ptr, klass(), _interfaces, xadd_offset(offset) );\n+const TypePtr *TypeInstKlassPtr::add_offset( intptr_t offset ) const {\n+  return make(_ptr, klass(), _interfaces, xadd_offset(offset), flat_in_array());\n@@ -5768,1 +6149,1 @@\n-  return make(_ptr, klass(), _interfaces, offset);\n+  return make(_ptr, klass(), _interfaces, Offset(offset), flat_in_array());\n@@ -5775,1 +6156,1 @@\n-  return make(ptr, _klass, _interfaces, _offset);\n+  return make(ptr, _klass, _interfaces, _offset, flat_in_array());\n@@ -5791,1 +6172,1 @@\n-  return make(klass_is_exact ? Constant : NotNull, k, _interfaces, _offset);\n+  return make(klass_is_exact ? Constant : NotNull, k, _interfaces, _offset, flat_in_array());\n@@ -5823,1 +6204,1 @@\n-  return TypeInstPtr::make(TypePtr::BotPTR, k, interfaces, xk, nullptr, 0);\n+  return TypeInstPtr::make(TypePtr::BotPTR, k, interfaces, xk, nullptr, Offset(0), flat_in_array() && !klass()->is_inlinetype());\n@@ -5856,1 +6237,1 @@\n-    int offset = meet_offset(tp->offset());\n+    Offset offset = meet_offset(tp->offset());\n@@ -5864,1 +6245,1 @@\n-      return make( ptr, klass(), _interfaces, offset );\n+      return make(ptr, klass(), _interfaces, offset, flat_in_array());\n@@ -5877,1 +6258,1 @@\n-    return TypePtr::BOTTOM;\n+      return TypePtr::BOTTOM;\n@@ -5897,1 +6278,1 @@\n-    int  off     = meet_offset(tkls->offset());\n+    Offset  off     = meet_offset(tkls->offset());\n@@ -5903,1 +6284,2 @@\n-    switch(meet_instptr(ptr, interfaces, this, tkls, res_klass, res_xk)) {\n+    bool res_flat_in_array = false;\n+    switch(meet_instptr(ptr, interfaces, this, tkls, res_klass, res_xk, res_flat_in_array)) {\n@@ -5911,1 +6293,1 @@\n-        const Type* res = make(ptr, res_klass, interfaces, off);\n+        const Type* res = make(ptr, res_klass, interfaces, off, res_flat_in_array);\n@@ -5920,1 +6302,1 @@\n-    int offset = meet_offset(tp->offset());\n+    Offset offset = meet_offset(tp->offset());\n@@ -5933,1 +6315,1 @@\n-        return TypeAryKlassPtr::make(ptr, tp->elem(), tp->klass(), offset);\n+        return TypeAryKlassPtr::make(ptr, tp->elem(), tp->klass(), offset, tp->is_not_flat(), tp->is_not_null_free(), tp->is_null_free());\n@@ -5938,1 +6320,1 @@\n-        return make(ptr, ciEnv::current()->Object_klass(), interfaces, offset);\n+        return make(ptr, ciEnv::current()->Object_klass(), interfaces, offset, false);\n@@ -5952,2 +6334,1 @@\n-          return TypeAryKlassPtr::make(ptr,\n-                                       tp->elem(), tp->klass(), offset);\n+          return TypeAryKlassPtr::make(ptr, tp->elem(), tp->klass(), offset, tp->is_not_flat(), tp->is_not_null_free(), tp->is_null_free());\n@@ -5961,1 +6342,1 @@\n-      return make(ptr, ciEnv::current()->Object_klass(), interfaces, offset);\n+      return make(ptr, ciEnv::current()->Object_klass(), interfaces, offset, false);\n@@ -5973,1 +6354,1 @@\n-  return new TypeInstKlassPtr(dual_ptr(), klass(), _interfaces, dual_offset());\n+  return new TypeInstKlassPtr(dual_ptr(), klass(), _interfaces, dual_offset(), flat_in_array());\n@@ -6074,0 +6455,15 @@\n+bool TypeInstKlassPtr::can_be_inline_array() const {\n+  return _klass->equals(ciEnv::current()->Object_klass()) && TypeAryKlassPtr::_array_interfaces->contains(_interfaces);\n+}\n+\n+bool TypeAryKlassPtr::can_be_inline_array() const {\n+  return _elem->isa_instklassptr() && _elem->is_instklassptr()->_klass->can_be_inline_klass();\n+}\n+\n+bool TypeInstPtr::can_be_inline_array() const {\n+  return _klass->equals(ciEnv::current()->Object_klass()) && TypeAryPtr::_array_interfaces->contains(_interfaces);\n+}\n+\n+bool TypeAryPtr::can_be_inline_array() const {\n+  return elem()->make_ptr() && elem()->make_ptr()->isa_instptr() && elem()->make_ptr()->is_instptr()->_klass->can_be_inline_klass();\n+}\n@@ -6075,2 +6471,2 @@\n-const TypeAryKlassPtr *TypeAryKlassPtr::make(PTR ptr, const Type* elem, ciKlass* k, int offset) {\n-  return (TypeAryKlassPtr*)(new TypeAryKlassPtr(ptr, elem, k, offset))->hashcons();\n+const TypeAryKlassPtr *TypeAryKlassPtr::make(PTR ptr, const Type* elem, ciKlass* k, Offset offset, bool not_flat, bool not_null_free, bool null_free) {\n+  return (TypeAryKlassPtr*)(new TypeAryKlassPtr(ptr, elem, k, offset, not_flat, not_null_free, null_free))->hashcons();\n@@ -6079,1 +6475,1 @@\n-const TypeAryKlassPtr *TypeAryKlassPtr::make(PTR ptr, ciKlass* k, int offset, InterfaceHandling interface_handling) {\n+const TypeAryKlassPtr* TypeAryKlassPtr::make(PTR ptr, ciKlass* k, Offset offset, InterfaceHandling interface_handling, bool not_flat, bool not_null_free, bool null_free) {\n@@ -6083,2 +6479,2 @@\n-    const TypeKlassPtr *etype = TypeKlassPtr::make(eklass, interface_handling)->cast_to_exactness(false);\n-    return TypeAryKlassPtr::make(ptr, etype, nullptr, offset);\n+    const TypeKlassPtr* etype = TypeKlassPtr::make(eklass, interface_handling)->cast_to_exactness(false);\n+    return TypeAryKlassPtr::make(ptr, etype, nullptr, offset, not_flat, not_null_free, null_free);\n@@ -6088,1 +6484,5 @@\n-    return TypeAryKlassPtr::make(ptr, etype, k, offset);\n+    return TypeAryKlassPtr::make(ptr, etype, k, offset, not_flat, not_null_free, null_free);\n+  } else if (k->is_flat_array_klass()) {\n+    ciKlass* eklass = k->as_flat_array_klass()->element_klass();\n+    const TypeKlassPtr* etype = TypeKlassPtr::make(eklass);\n+    return TypeAryKlassPtr::make(ptr, etype, k, offset, not_flat, not_null_free, null_free);\n@@ -6095,0 +6495,11 @@\n+const TypeAryKlassPtr* TypeAryKlassPtr::make(PTR ptr, ciKlass* k, Offset offset, InterfaceHandling interface_handling) {\n+  bool null_free = k->as_array_klass()->is_elem_null_free();\n+  bool not_null_free = (ptr == Constant) ? !null_free : !k->is_flat_array_klass() && (k->is_type_array_klass() || !k->as_array_klass()->element_klass()->can_be_inline_klass(false));\n+\n+  bool not_flat = !UseFlatArray || not_null_free || (k->as_array_klass()->element_klass() != nullptr &&\n+                                                     k->as_array_klass()->element_klass()->is_inlinetype() &&\n+                                                     !k->as_array_klass()->element_klass()->flat_in_array());\n+\n+  return TypeAryKlassPtr::make(ptr, k, offset, interface_handling, not_flat, not_null_free, null_free);\n+}\n+\n@@ -6096,1 +6507,1 @@\n-  return TypeAryKlassPtr::make(Constant, klass, 0, interface_handling);\n+  return TypeAryKlassPtr::make(Constant, klass, Offset(0), interface_handling);\n@@ -6105,0 +6516,3 @@\n+    _not_flat == p->_not_flat &&\n+    _not_null_free == p->_not_null_free &&\n+    _null_free == p->_null_free &&\n@@ -6111,1 +6525,2 @@\n-  return (uint)(uintptr_t)_elem + TypeKlassPtr::hash();\n+  return (uint)(uintptr_t)_elem + TypeKlassPtr::hash() + (uint)(_not_flat ? 43 : 0) +\n+      (uint)(_not_null_free ? 44 : 0) + (uint)(_null_free ? 45 : 0);\n@@ -6127,2 +6542,7 @@\n-  if ((tinst = el->isa_instptr()) != nullptr) {\n-    \/\/ Leave k_ary at null.\n+  if (is_flat() && el->is_inlinetypeptr()) {\n+    \/\/ Klass is required by TypeAryPtr::flat_layout_helper() and others\n+    if (el->inline_klass() != nullptr) {\n+      k_ary = ciArrayKlass::make(el->inline_klass(), \/* null_free *\/ true);\n+    }\n+  } else if ((tinst = el->isa_instptr()) != nullptr) {\n+    \/\/ Leave k_ary at nullptr.\n@@ -6130,1 +6550,1 @@\n-    \/\/ Leave k_ary at null.\n+    \/\/ Leave k_ary at nullptr.\n@@ -6178,1 +6598,1 @@\n-    k = ciObjArrayKlass::make(k);\n+    k = ciArrayKlass::make(k, is_null_free());\n@@ -6198,1 +6618,1 @@\n-  return make(_ptr, elem(), klass(), xadd_offset(offset));\n+  return make(_ptr, elem(), klass(), xadd_offset(offset), is_not_flat(), is_not_null_free(), _null_free);\n@@ -6202,1 +6622,1 @@\n-  return make(_ptr, elem(), klass(), offset);\n+  return make(_ptr, elem(), klass(), Offset(offset), is_not_flat(), is_not_null_free(), _null_free);\n@@ -6209,1 +6629,1 @@\n-  return make(ptr, elem(), _klass, _offset);\n+  return make(ptr, elem(), _klass, _offset, is_not_flat(), is_not_null_free(), _null_free);\n@@ -6217,0 +6637,5 @@\n+  \/\/ Even though MyValue is final, [LMyValue is only exact if the array\n+  \/\/ is null-free due to null-free [LMyValue <: null-able [LMyValue.\n+  if (tk->isa_instklassptr() && tk->klass()->is_inlinetype() && !is_null_free()) {\n+    return false;\n+  }\n@@ -6223,1 +6648,4 @@\n-  if (must_be_exact()) return this;  \/\/ cannot clear xk\n+  if (must_be_exact() && !klass_is_exact) return this;  \/\/ cannot clear xk\n+  if (klass_is_exact == this->klass_is_exact()) {\n+    return this;\n+  }\n@@ -6229,1 +6657,16 @@\n-  return make(klass_is_exact ? Constant : NotNull, elem, k, _offset);\n+  bool not_flat = is_not_flat();\n+  bool not_null_free = is_not_null_free();\n+  if (_elem->isa_klassptr()) {\n+    if (klass_is_exact || _elem->isa_aryklassptr()) {\n+      assert(!is_null_free() && !is_flat(), \"null-free (or flat) inline type arrays should always be exact\");\n+      \/\/ An array can't be null-free (or flat) if the klass is exact\n+      not_null_free = true;\n+      not_flat = true;\n+    } else {\n+      \/\/ Klass is not exact (anymore), re-compute null-free\/flat properties\n+      const TypeOopPtr* exact_etype = TypeOopPtr::make_from_klass_unique(_elem->is_instklassptr()->instance_klass());\n+      not_null_free = !exact_etype->can_be_inline_type();\n+      not_flat = !UseFlatArray || not_null_free || (exact_etype->is_inlinetypeptr() && !exact_etype->inline_klass()->flat_in_array());\n+    }\n+  }\n+  return make(klass_is_exact ? Constant : NotNull, elem, k, _offset, not_flat, not_null_free, _null_free);\n@@ -6232,0 +6675,3 @@\n+const TypeAryKlassPtr* TypeAryKlassPtr::cast_to_null_free() const {\n+  return make(_ptr, elem(), klass(), _offset, is_not_flat(), false, true);\n+}\n@@ -6246,1 +6692,5 @@\n-  return TypeAryPtr::make(TypePtr::BotPTR, TypeAry::make(el, TypeInt::POS), k, xk, 0);\n+  bool null_free = _null_free;\n+  if (null_free && el->isa_ptr()) {\n+    el = el->is_ptr()->join_speculative(TypePtr::NOTNULL);\n+  }\n+  return TypeAryPtr::make(TypePtr::BotPTR, TypeAry::make(el, TypeInt::POS, false, is_flat(), is_not_flat(), is_not_null_free()), k, xk, Offset(0));\n@@ -6280,1 +6730,1 @@\n-    int offset = meet_offset(tp->offset());\n+    Offset offset = meet_offset(tp->offset());\n@@ -6288,1 +6738,1 @@\n-      return make( ptr, _elem, klass(), offset );\n+      return make(ptr, _elem, klass(), offset, is_not_flat(), is_not_null_free(), is_null_free());\n@@ -6321,1 +6771,1 @@\n-    int off = meet_offset(tap->offset());\n+    Offset off = meet_offset(tap->offset());\n@@ -6323,1 +6773,0 @@\n-\n@@ -6327,1 +6776,5 @@\n-    meet_aryptr(ptr, elem, this, tap, res_klass, res_xk);\n+    bool res_flat = false;\n+    bool res_not_flat = false;\n+    bool res_not_null_free = false;\n+    MeetResult res = meet_aryptr(ptr, elem, this, tap,\n+                                 res_klass, res_xk, res_flat, res_not_flat, res_not_null_free);\n@@ -6329,1 +6782,13 @@\n-    return make(ptr, elem, res_klass, off);\n+    bool null_free = meet_null_free(tap->_null_free);\n+    if (res == NOT_SUBTYPE) {\n+      null_free = false;\n+    } else if (res == SUBTYPE) {\n+      if (above_centerline(tap->ptr()) && !above_centerline(this->ptr())) {\n+        null_free = _null_free;\n+      } else if (above_centerline(this->ptr()) && !above_centerline(tap->ptr())) {\n+        null_free = tap->_null_free;\n+      } else if (above_centerline(this->ptr()) && above_centerline(tap->ptr())) {\n+        null_free = _null_free || tap->_null_free;\n+      }\n+    }\n+    return make(ptr, elem, res_klass, off, res_not_flat, res_not_null_free, null_free);\n@@ -6333,1 +6798,1 @@\n-    int offset = meet_offset(tp->offset());\n+    Offset offset = meet_offset(tp->offset());\n@@ -6347,1 +6812,1 @@\n-        return TypeAryKlassPtr::make(ptr, _elem, _klass, offset);\n+        return TypeAryKlassPtr::make(ptr, _elem, _klass, offset, is_not_flat(), is_not_null_free(), is_null_free());\n@@ -6352,1 +6817,1 @@\n-        return TypeInstKlassPtr::make(ptr, ciEnv::current()->Object_klass(), interfaces, offset);\n+        return TypeInstKlassPtr::make(ptr, ciEnv::current()->Object_klass(), interfaces, offset, false);\n@@ -6367,1 +6832,1 @@\n-          return make(ptr, _elem, _klass, offset);\n+          return make(ptr, _elem, _klass, offset, is_not_flat(), is_not_null_free(), is_null_free());\n@@ -6375,1 +6840,1 @@\n-      return TypeInstKlassPtr::make(ptr, ciEnv::current()->Object_klass(), interfaces, offset);\n+      return TypeInstKlassPtr::make(ptr, ciEnv::current()->Object_klass(), interfaces, offset, false);\n@@ -6413,0 +6878,3 @@\n+    if (other->is_null_free() && !this_one->is_null_free()) {\n+      return false; \/\/ A nullable array can't be a subtype of a null-free array\n+    }\n@@ -6505,1 +6973,1 @@\n-  return new TypeAryKlassPtr(dual_ptr(), elem()->dual(), klass(), dual_offset());\n+  return new TypeAryKlassPtr(dual_ptr(), elem()->dual(), klass(), dual_offset(), !is_not_flat(), !is_not_null_free(), dual_null_free());\n@@ -6515,1 +6983,1 @@\n-    k = ciObjArrayKlass::make(k);\n+    k = ciArrayKlass::make(k, _null_free);\n@@ -6562,5 +7030,5 @@\n-\n-  if( _offset ) {               \/\/ Dump offset, if any\n-    if( _offset == OffsetBot )      { st->print(\"+any\"); }\n-    else if( _offset == OffsetTop ) { st->print(\"+unknown\"); }\n-    else                            { st->print(\"+%d\", _offset); }\n+  if (is_flat()) st->print(\":flat\");\n+  if (_null_free) st->print(\":null free\");\n+  if (Verbose) {\n+    if (_not_flat) st->print(\":not flat\");\n+    if (_not_null_free) st->print(\":not null free\");\n@@ -6569,0 +7037,2 @@\n+  _offset.dump2(st);\n+\n@@ -6587,2 +7057,14 @@\n-const TypeFunc *TypeFunc::make( const TypeTuple *domain, const TypeTuple *range ) {\n-  return (TypeFunc*)(new TypeFunc(domain,range))->hashcons();\n+const TypeFunc *TypeFunc::make(const TypeTuple *domain_sig, const TypeTuple* domain_cc,\n+                               const TypeTuple *range_sig, const TypeTuple *range_cc) {\n+  return (TypeFunc*)(new TypeFunc(domain_sig, domain_cc, range_sig, range_cc))->hashcons();\n+}\n+\n+const TypeFunc *TypeFunc::make(const TypeTuple *domain, const TypeTuple *range) {\n+  return make(domain, domain, range, range);\n+}\n+\n+\/\/------------------------------osr_domain-----------------------------\n+const TypeTuple* osr_domain() {\n+  const Type **fields = TypeTuple::fields(2);\n+  fields[TypeFunc::Parms+0] = TypeRawPtr::BOTTOM;  \/\/ address of osr buffer\n+  return TypeTuple::make(TypeFunc::Parms+1, fields);\n@@ -6592,1 +7074,1 @@\n-const TypeFunc *TypeFunc::make(ciMethod* method) {\n+const TypeFunc* TypeFunc::make(ciMethod* method, bool is_osr_compilation) {\n@@ -6594,7 +7076,24 @@\n-  const TypeFunc* tf = C->last_tf(method); \/\/ check cache\n-  if (tf != nullptr)  return tf;  \/\/ The hit rate here is almost 50%.\n-  const TypeTuple *domain;\n-  if (method->is_static()) {\n-    domain = TypeTuple::make_domain(nullptr, method->signature(), ignore_interfaces);\n-  } else {\n-    domain = TypeTuple::make_domain(method->holder(), method->signature(), ignore_interfaces);\n+  const TypeFunc* tf = nullptr;\n+  if (!is_osr_compilation) {\n+    tf = C->last_tf(method); \/\/ check cache\n+    if (tf != nullptr)  return tf;  \/\/ The hit rate here is almost 50%.\n+  }\n+  \/\/ Inline types are not passed\/returned by reference, instead each field of\n+  \/\/ the inline type is passed\/returned as an argument. We maintain two views of\n+  \/\/ the argument\/return list here: one based on the signature (with an inline\n+  \/\/ type argument\/return as a single slot), one based on the actual calling\n+  \/\/ convention (with an inline type argument\/return as a list of its fields).\n+  bool has_scalar_args = method->has_scalarized_args() && !is_osr_compilation;\n+  \/\/ Fall back to the non-scalarized calling convention when compiling a call via a mismatching method\n+  if (method != C->method() && method->get_Method()->mismatch()) {\n+    has_scalar_args = false;\n+  }\n+  const TypeTuple* domain_sig = is_osr_compilation ? osr_domain() : TypeTuple::make_domain(method, ignore_interfaces, false);\n+  const TypeTuple* domain_cc = has_scalar_args ? TypeTuple::make_domain(method, ignore_interfaces, true) : domain_sig;\n+  ciSignature* sig = method->signature();\n+  bool has_scalar_ret = !method->is_native() && sig->return_type()->is_inlinetype() && sig->return_type()->as_inline_klass()->can_be_returned_as_fields();\n+  const TypeTuple* range_sig = TypeTuple::make_range(sig, ignore_interfaces, false);\n+  const TypeTuple* range_cc = has_scalar_ret ? TypeTuple::make_range(sig, ignore_interfaces, true) : range_sig;\n+  tf = TypeFunc::make(domain_sig, domain_cc, range_sig, range_cc);\n+  if (!is_osr_compilation) {\n+    C->set_last_tf(method, tf);  \/\/ fill cache\n@@ -6602,3 +7101,0 @@\n-  const TypeTuple *range  = TypeTuple::make_range(method->signature(), ignore_interfaces);\n-  tf = TypeFunc::make(domain, range);\n-  C->set_last_tf(method, tf);  \/\/ fill cache\n@@ -6639,2 +7135,4 @@\n-  return _domain == a->_domain &&\n-    _range == a->_range;\n+  return _domain_sig == a->_domain_sig &&\n+    _domain_cc == a->_domain_cc &&\n+    _range_sig == a->_range_sig &&\n+    _range_cc == a->_range_cc;\n@@ -6646,1 +7144,1 @@\n-  return (uint)(uintptr_t)_domain + (uint)(uintptr_t)_range;\n+  return (uint)(intptr_t)_domain_sig + (uint)(intptr_t)_domain_cc + (uint)(intptr_t)_range_sig + (uint)(intptr_t)_range_cc;\n@@ -6653,1 +7151,1 @@\n-  if( _range->cnt() <= Parms )\n+  if( _range_sig->cnt() <= Parms )\n@@ -6657,2 +7155,2 @@\n-    for (i = Parms; i < _range->cnt()-1; i++) {\n-      _range->field_at(i)->dump2(d,depth,st);\n+    for (i = Parms; i < _range_sig->cnt()-1; i++) {\n+      _range_sig->field_at(i)->dump2(d,depth,st);\n@@ -6661,1 +7159,1 @@\n-    _range->field_at(i)->dump2(d,depth,st);\n+    _range_sig->field_at(i)->dump2(d,depth,st);\n@@ -6670,3 +7168,3 @@\n-  if (Parms < _domain->cnt())\n-    _domain->field_at(Parms)->dump2(d,depth-1,st);\n-  for (uint i = Parms+1; i < _domain->cnt(); i++) {\n+  if (Parms < _domain_sig->cnt())\n+    _domain_sig->field_at(Parms)->dump2(d,depth-1,st);\n+  for (uint i = Parms+1; i < _domain_sig->cnt(); i++) {\n@@ -6674,1 +7172,1 @@\n-    _domain->field_at(i)->dump2(d,depth-1,st);\n+    _domain_sig->field_at(i)->dump2(d,depth-1,st);\n@@ -6694,1 +7192,1 @@\n-  if (range()->cnt() == TypeFunc::Parms) {\n+  if (range_sig()->cnt() == TypeFunc::Parms) {\n@@ -6697,1 +7195,1 @@\n-  return range()->field_at(TypeFunc::Parms)->basic_type();\n+  return range_sig()->field_at(TypeFunc::Parms)->basic_type();\n","filename":"src\/hotspot\/share\/opto\/type.cpp","additions":835,"deletions":337,"binary":false,"changes":1172,"status":"modified"},{"patch":"@@ -28,0 +28,1 @@\n+#include \"ci\/ciInlineKlass.hpp\"\n@@ -30,0 +31,1 @@\n+#include \"runtime\/sharedRuntime.hpp\"\n@@ -139,0 +141,24 @@\n+  class Offset {\n+  private:\n+    int _offset;\n+\n+  public:\n+    explicit Offset(int offset) : _offset(offset) {}\n+\n+    const Offset meet(const Offset other) const;\n+    const Offset dual() const;\n+    const Offset add(intptr_t offset) const;\n+    bool operator==(const Offset& other) const {\n+      return _offset == other._offset;\n+    }\n+    bool operator!=(const Offset& other) const {\n+      return _offset != other._offset;\n+    }\n+    int get() const { return _offset; }\n+\n+    void dump2(outputStream *st) const;\n+\n+    static const Offset top;\n+    static const Offset bottom;\n+  };\n+\n@@ -329,0 +355,3 @@\n+  bool is_inlinetypeptr() const;\n+  virtual ciInlineKlass* inline_klass() const;\n+\n@@ -727,2 +756,2 @@\n-  static const TypeTuple *make_range(ciSignature *sig, InterfaceHandling interface_handling = ignore_interfaces);\n-  static const TypeTuple *make_domain(ciInstanceKlass* recv, ciSignature *sig, InterfaceHandling interface_handling);\n+  static const TypeTuple *make_range(ciSignature* sig, InterfaceHandling interface_handling = ignore_interfaces, bool ret_vt_fields = false);\n+  static const TypeTuple *make_domain(ciMethod* method, InterfaceHandling interface_handling, bool vt_fields_as_args = false);\n@@ -757,2 +786,2 @@\n-  TypeAry(const Type* elem, const TypeInt* size, bool stable) : Type(Array),\n-      _elem(elem), _size(size), _stable(stable) {}\n+  TypeAry(const Type* elem, const TypeInt* size, bool stable, bool flat, bool not_flat, bool not_null_free) : Type(Array),\n+      _elem(elem), _size(size), _stable(stable), _flat(flat), _not_flat(not_flat), _not_null_free(not_null_free) {}\n@@ -769,0 +798,6 @@\n+\n+  \/\/ Inline type array properties\n+  const bool _flat;             \/\/ Array is flat\n+  const bool _not_flat;         \/\/ Array is never flat\n+  const bool _not_null_free;    \/\/ Array is never null-free\n+\n@@ -772,1 +807,2 @@\n-  static const TypeAry* make(const Type* elem, const TypeInt* size, bool stable = false);\n+  static const TypeAry* make(const Type* elem, const TypeInt* size, bool stable = false,\n+                             bool flat = false, bool not_flat = false, bool not_null_free = false);\n@@ -918,1 +954,1 @@\n-  TypePtr(TYPES t, PTR ptr, int offset,\n+  TypePtr(TYPES t, PTR ptr, Offset offset,\n@@ -974,1 +1010,4 @@\n-                                                            const T* other_type, ciKlass*& res_klass, bool& res_xk);\n+                                                            const T* other_type, ciKlass*& res_klass, bool& res_xk, bool& res_flat_array);\n+ private:\n+  template<class T> static bool is_meet_subtype_of(const T* sub_type, const T* super_type);\n+ protected:\n@@ -977,1 +1016,1 @@\n-                                                  ciKlass*& res_klass, bool& res_xk);\n+                                                  ciKlass*& res_klass, bool& res_xk, bool &res_flat, bool &res_not_flat, bool &res_not_null_free);\n@@ -988,1 +1027,1 @@\n-  const int _offset;            \/\/ Offset into oop, with TOP & BOT\n+  const Offset _offset;         \/\/ Offset into oop, with TOP & BOT\n@@ -991,1 +1030,1 @@\n-  int offset() const { return _offset; }\n+  int offset() const { return _offset.get(); }\n@@ -994,1 +1033,1 @@\n-  static const TypePtr *make(TYPES t, PTR ptr, int offset,\n+  static const TypePtr* make(TYPES t, PTR ptr, Offset offset,\n@@ -1003,1 +1042,1 @@\n-  int xadd_offset( intptr_t offset ) const;\n+  Type::Offset xadd_offset(intptr_t offset) const;\n@@ -1006,0 +1045,1 @@\n+  virtual int flat_offset() const { return offset(); }\n@@ -1013,2 +1053,2 @@\n-  int meet_offset( int offset ) const;\n-  int dual_offset( ) const;\n+  Offset meet_offset(int offset) const;\n+  Offset dual_offset() const;\n@@ -1042,0 +1082,8 @@\n+  virtual bool can_be_inline_type() const { return false; }\n+  virtual bool flat_in_array()      const { return false; }\n+  virtual bool not_flat_in_array()  const { return true; }\n+  virtual bool is_flat()            const { return false; }\n+  virtual bool is_not_flat()        const { return false; }\n+  virtual bool is_null_free()       const { return false; }\n+  virtual bool is_not_null_free()   const { return false; }\n+\n@@ -1059,1 +1107,1 @@\n-  TypeRawPtr( PTR ptr, address bits ) : TypePtr(RawPtr,ptr,0), _bits(bits){}\n+  TypeRawPtr(PTR ptr, address bits) : TypePtr(RawPtr,ptr,Offset(0)), _bits(bits){}\n@@ -1095,1 +1143,1 @@\n- TypeOopPtr(TYPES t, PTR ptr, ciKlass* k, const TypeInterfaces* interfaces, bool xk, ciObject* o, int offset, int instance_id,\n+ TypeOopPtr(TYPES t, PTR ptr, ciKlass* k, const TypeInterfaces* interfaces, bool xk, ciObject* o, Offset offset, Offset field_offset, int instance_id,\n@@ -1136,1 +1184,1 @@\n-  virtual ciKlass* klass() const { return _klass;     }\n+  virtual ciKlass* klass() const { return _klass; }\n@@ -1183,1 +1231,1 @@\n-  static const TypeOopPtr* make(PTR ptr, int offset, int instance_id,\n+  static const TypeOopPtr* make(PTR ptr, Offset offset, int instance_id,\n@@ -1202,1 +1250,4 @@\n-  bool is_known_instance_field() const { return is_known_instance() && _offset >= 0; }\n+  bool is_known_instance_field() const { return is_known_instance() && _offset.get() >= 0; }\n+\n+  virtual bool can_be_inline_type() const { return (_klass == nullptr || _klass->can_be_inline_klass(_klass_is_exact)); }\n+  virtual bool can_be_inline_array() const { ShouldNotReachHere(); return false; }\n@@ -1265,2 +1316,3 @@\n-  TypeInstPtr(PTR ptr, ciKlass* k, const TypeInterfaces* interfaces, bool xk, ciObject* o, int off, int instance_id,\n-              const TypePtr* speculative, int inline_depth);\n+  TypeInstPtr(PTR ptr, ciKlass* k, const TypeInterfaces* interfaces, bool xk, ciObject* o, Offset offset,\n+              bool flat_in_array, int instance_id, const TypePtr* speculative,\n+              int inline_depth);\n@@ -1269,1 +1321,1 @@\n-\n+  bool _flat_in_array; \/\/ Type is flat in arrays\n@@ -1288,1 +1340,1 @@\n-    return make(TypePtr::Constant, k, interfaces, true, o, 0, InstanceBot);\n+    return make(TypePtr::Constant, k, interfaces, true, o, Offset(0));\n@@ -1291,1 +1343,1 @@\n-  static const TypeInstPtr *make(ciObject* o, int offset) {\n+  static const TypeInstPtr *make(ciObject* o, Offset offset) {\n@@ -1294,1 +1346,1 @@\n-    return make(TypePtr::Constant, k, interfaces, true, o, offset, InstanceBot);\n+    return make(TypePtr::Constant, k, interfaces, true, o, offset);\n@@ -1300,1 +1352,1 @@\n-    return make(ptr, klass, interfaces, false, nullptr, 0, InstanceBot);\n+    return make(ptr, klass, interfaces, false, nullptr, Offset(0));\n@@ -1306,1 +1358,1 @@\n-    return make(ptr, klass, interfaces, true, nullptr, 0, InstanceBot);\n+    return make(ptr, klass, interfaces, true, nullptr, Offset(0));\n@@ -1310,1 +1362,1 @@\n-  static const TypeInstPtr *make(PTR ptr, ciKlass* klass, int offset) {\n+  static const TypeInstPtr *make(PTR ptr, ciKlass* klass, Offset offset) {\n@@ -1312,1 +1364,1 @@\n-    return make(ptr, klass, interfaces, false, nullptr, offset, InstanceBot);\n+    return make(ptr, klass, interfaces, false, nullptr, offset);\n@@ -1315,1 +1367,3 @@\n-  static const TypeInstPtr *make(PTR ptr, ciKlass* k, const TypeInterfaces* interfaces, bool xk, ciObject* o, int offset,\n+  \/\/ Make a pointer to an oop.\n+  static const TypeInstPtr* make(PTR ptr, ciKlass* k, const TypeInterfaces* interfaces, bool xk, ciObject* o, Offset offset,\n+                                 bool flat_in_array = false,\n@@ -1320,1 +1374,1 @@\n-  static const TypeInstPtr *make(PTR ptr, ciKlass* k, bool xk, ciObject* o, int offset, int instance_id = InstanceBot) {\n+  static const TypeInstPtr *make(PTR ptr, ciKlass* k, bool xk, ciObject* o, Offset offset, int instance_id = InstanceBot) {\n@@ -1322,1 +1376,1 @@\n-    return make(ptr, k, interfaces, xk, o, offset, instance_id);\n+    return make(ptr, k, interfaces, xk, o, offset, false, instance_id);\n@@ -1331,1 +1385,1 @@\n-  ciType* java_mirror_type() const;\n+  ciType* java_mirror_type(bool* is_null_free_array = nullptr) const;\n@@ -1348,0 +1402,4 @@\n+  virtual const TypeInstPtr* cast_to_flat_in_array() const;\n+  virtual bool flat_in_array() const { return _flat_in_array; }\n+  virtual bool not_flat_in_array() const { return !can_be_inline_type() || (_klass->is_inlinetype() && !flat_in_array()); }\n+\n@@ -1355,0 +1413,2 @@\n+  virtual bool can_be_inline_array() const;\n+\n@@ -1379,0 +1439,1 @@\n+  friend class TypeInstPtr;\n@@ -1380,4 +1441,4 @@\n-  TypeAryPtr( PTR ptr, ciObject* o, const TypeAry *ary, ciKlass* k, bool xk,\n-              int offset, int instance_id, bool is_autobox_cache,\n-              const TypePtr* speculative, int inline_depth)\n-    : TypeOopPtr(AryPtr,ptr,k,_array_interfaces,xk,o,offset, instance_id, speculative, inline_depth),\n+  TypeAryPtr(PTR ptr, ciObject* o, const TypeAry *ary, ciKlass* k, bool xk,\n+             Offset offset, Offset field_offset, int instance_id, bool is_autobox_cache,\n+             const TypePtr* speculative, int inline_depth)\n+    : TypeOopPtr(AryPtr, ptr, k, _array_interfaces, xk, o, offset, field_offset, instance_id, speculative, inline_depth),\n@@ -1385,1 +1446,2 @@\n-    _is_autobox_cache(is_autobox_cache)\n+    _is_autobox_cache(is_autobox_cache),\n+    _field_offset(field_offset)\n@@ -1391,2 +1453,2 @@\n-        _offset != 0 && _offset != arrayOopDesc::length_offset_in_bytes() &&\n-        _offset != arrayOopDesc::klass_offset_in_bytes()) {\n+        _offset.get() != 0 && _offset.get() != arrayOopDesc::length_offset_in_bytes() &&\n+        _offset.get() != arrayOopDesc::klass_offset_in_bytes()) {\n@@ -1401,0 +1463,6 @@\n+  \/\/ For flat inline type arrays, each field of the inline type in\n+  \/\/ the array has its own memory slice so we need to keep track of\n+  \/\/ which field is accessed\n+  const Offset _field_offset;\n+  Offset meet_field_offset(const Type::Offset offset) const;\n+  Offset dual_field_offset() const;\n@@ -1428,0 +1496,6 @@\n+  \/\/ Inline type array properties\n+  bool is_flat()          const { return _ary->_flat; }\n+  bool is_not_flat()      const { return _ary->_not_flat; }\n+  bool is_null_free()     const { return is_flat() || (_ary->_elem->make_ptr() != nullptr && _ary->_elem->make_ptr()->is_inlinetypeptr() && (_ary->_elem->make_ptr()->ptr() == NotNull || _ary->_elem->make_ptr()->ptr() == AnyNull)); }\n+  bool is_not_null_free() const { return _ary->_not_null_free; }\n+\n@@ -1430,1 +1504,2 @@\n-  static const TypeAryPtr *make(PTR ptr, const TypeAry *ary, ciKlass* k, bool xk, int offset,\n+  static const TypeAryPtr* make(PTR ptr, const TypeAry *ary, ciKlass* k, bool xk, Offset offset,\n+                                Offset field_offset = Offset::bottom,\n@@ -1435,1 +1510,2 @@\n-  static const TypeAryPtr *make(PTR ptr, ciObject* o, const TypeAry *ary, ciKlass* k, bool xk, int offset,\n+  static const TypeAryPtr* make(PTR ptr, ciObject* o, const TypeAry *ary, ciKlass* k, bool xk, Offset offset,\n+                                Offset field_offset = Offset::bottom,\n@@ -1438,1 +1514,2 @@\n-                                int inline_depth = InlineDepthBottom, bool is_autobox_cache = false);\n+                                int inline_depth = InlineDepthBottom,\n+                                bool is_autobox_cache = false);\n@@ -1457,0 +1534,1 @@\n+  virtual const Type* cleanup_speculative() const;\n@@ -1464,0 +1542,8 @@\n+  \/\/ Inline type array properties\n+  const TypeAryPtr* cast_to_not_flat(bool not_flat = true) const;\n+  const TypeAryPtr* cast_to_not_null_free(bool not_null_free = true) const;\n+  const TypeAryPtr* update_properties(const TypeAryPtr* new_type) const;\n+  jint flat_layout_helper() const;\n+  int flat_elem_size() const;\n+  int flat_log_elem_size() const;\n+\n@@ -1469,1 +1555,8 @@\n-  static jint max_array_length(BasicType etype) ;\n+  static jint max_array_length(BasicType etype);\n+\n+  int flat_offset() const;\n+  const Offset field_offset() const { return _field_offset; }\n+  const TypeAryPtr* with_field_offset(int offset) const;\n+  const TypePtr* add_field_offset_and_offset(intptr_t offset) const;\n+\n+  virtual bool can_be_inline_type() const { return false; }\n@@ -1472,0 +1565,2 @@\n+  virtual bool can_be_inline_array() const;\n+\n@@ -1483,0 +1578,1 @@\n+  static const TypeAryPtr *INLINES;\n@@ -1501,1 +1597,1 @@\n-  TypeMetadataPtr(PTR ptr, ciMetadata* metadata, int offset);\n+  TypeMetadataPtr(PTR ptr, ciMetadata* metadata, Offset offset);\n@@ -1513,1 +1609,1 @@\n-  static const TypeMetadataPtr* make(PTR ptr, ciMetadata* m, int offset);\n+  static const TypeMetadataPtr* make(PTR ptr, ciMetadata* m, Offset offset);\n@@ -1544,1 +1640,1 @@\n-  TypeKlassPtr(TYPES t, PTR ptr, ciKlass* klass, const TypeInterfaces* interfaces, int offset);\n+  TypeKlassPtr(TYPES t, PTR ptr, ciKlass* klass, const TypeInterfaces* interfaces, Offset offset);\n@@ -1583,1 +1679,1 @@\n-  static const TypeKlassPtr *make(PTR ptr, ciKlass* klass, int offset, InterfaceHandling interface_handling = ignore_interfaces);\n+  static const TypeKlassPtr *make(PTR ptr, ciKlass* klass, Offset offset, InterfaceHandling interface_handling = ignore_interfaces);\n@@ -1602,0 +1698,1 @@\n+  virtual bool can_be_inline_array() const { ShouldNotReachHere(); return false; }\n@@ -1636,2 +1733,2 @@\n-  TypeInstKlassPtr(PTR ptr, ciKlass* klass, const TypeInterfaces* interfaces, int offset)\n-    : TypeKlassPtr(InstKlassPtr, ptr, klass, interfaces, offset) {\n+  TypeInstKlassPtr(PTR ptr, ciKlass* klass, const TypeInterfaces* interfaces, Offset offset, bool flat_in_array)\n+    : TypeKlassPtr(InstKlassPtr, ptr, klass, interfaces, offset), _flat_in_array(flat_in_array) {\n@@ -1643,0 +1740,2 @@\n+  const bool _flat_in_array; \/\/ Type is flat in arrays\n+\n@@ -1654,0 +1753,2 @@\n+  virtual bool can_be_inline_type() const { return (_klass == nullptr || _klass->can_be_inline_klass(klass_is_exact())); }\n+\n@@ -1656,1 +1757,1 @@\n-    return make(TypePtr::Constant, k, interfaces, 0);\n+    return make(TypePtr::Constant, k, interfaces, Offset(0));\n@@ -1658,1 +1759,1 @@\n-  static const TypeInstKlassPtr* make(PTR ptr, ciKlass* k, const TypeInterfaces* interfaces, int offset);\n+  static const TypeInstKlassPtr* make(PTR ptr, ciKlass* k, const TypeInterfaces* interfaces, Offset offset, bool flat_in_array = false);\n@@ -1660,1 +1761,1 @@\n-  static const TypeInstKlassPtr* make(PTR ptr, ciKlass* k, int offset) {\n+  static const TypeInstKlassPtr* make(PTR ptr, ciKlass* k, Offset offset) {\n@@ -1681,0 +1782,5 @@\n+  virtual bool flat_in_array() const { return _flat_in_array; }\n+  virtual bool not_flat_in_array() const { return !_klass->can_be_inline_klass() || (_klass->is_inlinetype() && !flat_in_array()); }\n+\n+  virtual bool can_be_inline_array() const;\n+\n@@ -1695,0 +1801,3 @@\n+  const bool _not_flat;      \/\/ Array is never flat\n+  const bool _not_null_free; \/\/ Array is never null-free\n+  const bool _null_free;\n@@ -1697,3 +1806,3 @@\n-  TypeAryKlassPtr(PTR ptr, const Type *elem, ciKlass* klass, int offset)\n-    : TypeKlassPtr(AryKlassPtr, ptr, klass, _array_interfaces, offset), _elem(elem) {\n-    assert(klass == nullptr || klass->is_type_array_klass() || !klass->as_obj_array_klass()->base_element_klass()->is_interface(), \"\");\n+  TypeAryKlassPtr(PTR ptr, const Type *elem, ciKlass* klass, Offset offset, bool not_flat, int not_null_free, bool null_free)\n+    : TypeKlassPtr(AryKlassPtr, ptr, klass, _array_interfaces, offset), _elem(elem), _not_flat(not_flat), _not_null_free(not_null_free), _null_free(null_free) {\n+    assert(klass == nullptr || klass->is_type_array_klass() || klass->is_flat_array_klass() || !klass->as_obj_array_klass()->base_element_klass()->is_interface(), \"\");\n@@ -1708,0 +1817,8 @@\n+  bool dual_null_free() const {\n+    return _null_free;\n+  }\n+\n+  bool meet_null_free(bool other) const {\n+    return _null_free && other;\n+  }\n+\n@@ -1713,1 +1830,1 @@\n-  static const TypeAryKlassPtr *make(PTR ptr, ciKlass* k, int offset, InterfaceHandling interface_handling);\n+  static const TypeAryKlassPtr* make(PTR ptr, ciKlass* k, Offset offset, InterfaceHandling interface_handling, bool not_flat, bool not_null_free, bool null_free);\n@@ -1721,1 +1838,2 @@\n-  static const TypeAryKlassPtr *make(PTR ptr, const Type *elem, ciKlass* k, int offset);\n+  static const TypeAryKlassPtr* make(PTR ptr, const Type* elem, ciKlass* k, Offset offset, bool not_flat, bool not_null_free, bool null_free);\n+  static const TypeAryKlassPtr* make(PTR ptr, ciKlass* k, Offset offset, InterfaceHandling interface_handling);\n@@ -1733,0 +1851,2 @@\n+  const TypeAryKlassPtr* cast_to_null_free() const;\n+\n@@ -1746,0 +1866,6 @@\n+  bool is_flat()          const { return klass() != nullptr && klass()->is_flat_array_klass(); }\n+  bool is_not_flat()      const { return _not_flat; }\n+  bool is_null_free()     const { return _null_free; }\n+  bool is_not_null_free() const { return _not_null_free; }\n+  virtual bool can_be_inline_array() const;\n+\n@@ -1881,1 +2007,2 @@\n-  TypeFunc( const TypeTuple *domain, const TypeTuple *range ) : Type(Function),  _domain(domain), _range(range) {}\n+  TypeFunc(const TypeTuple *domain_sig, const TypeTuple *domain_cc, const TypeTuple *range_sig, const TypeTuple *range_cc)\n+    : Type(Function), _domain_sig(domain_sig), _domain_cc(domain_cc), _range_sig(range_sig), _range_cc(range_cc) {}\n@@ -1887,2 +2014,13 @@\n-  const TypeTuple* const _domain;     \/\/ Domain of inputs\n-  const TypeTuple* const _range;      \/\/ Range of results\n+  \/\/ Domains of inputs: inline type arguments are not passed by\n+  \/\/ reference, instead each field of the inline type is passed as an\n+  \/\/ argument. We maintain 2 views of the argument list here: one\n+  \/\/ based on the signature (with an inline type argument as a single\n+  \/\/ slot), one based on the actual calling convention (with a value\n+  \/\/ type argument as a list of its fields).\n+  const TypeTuple* const _domain_sig;\n+  const TypeTuple* const _domain_cc;\n+  \/\/ Range of results. Similar to domains: an inline type result can be\n+  \/\/ returned in registers in which case range_cc lists all fields and\n+  \/\/ is the actual calling convention.\n+  const TypeTuple* const _range_sig;\n+  const TypeTuple* const _range_cc;\n@@ -1902,5 +2040,8 @@\n-  const TypeTuple* domain() const { return _domain; }\n-  const TypeTuple* range()  const { return _range; }\n-\n-  static const TypeFunc *make(ciMethod* method);\n-  static const TypeFunc *make(ciSignature signature, const Type* extra);\n+  const TypeTuple* domain_sig() const { return _domain_sig; }\n+  const TypeTuple* domain_cc()  const { return _domain_cc; }\n+  const TypeTuple* range_sig()  const { return _range_sig; }\n+  const TypeTuple* range_cc()   const { return _range_cc; }\n+\n+  static const TypeFunc* make(ciMethod* method, bool is_osr_compilation = false);\n+  static const TypeFunc *make(const TypeTuple* domain_sig, const TypeTuple* domain_cc,\n+                              const TypeTuple* range_sig, const TypeTuple* range_cc);\n@@ -1914,0 +2055,2 @@\n+  bool returns_inline_type_as_fields() const { return range_sig() != range_cc(); }\n+\n@@ -2169,0 +2312,8 @@\n+inline bool Type::is_inlinetypeptr() const {\n+  return isa_instptr() != nullptr && is_instptr()->instance_klass()->is_inlinetype();\n+}\n+\n+inline ciInlineKlass* Type::inline_klass() const {\n+  return make_ptr()->is_instptr()->instance_klass()->as_inline_klass();\n+}\n+\n@@ -2195,0 +2346,1 @@\n+#define CmpUXNode    CmpULNode\n@@ -2213,0 +2365,1 @@\n+#define Op_StoreX    Op_StoreL\n@@ -2241,0 +2394,1 @@\n+#define CmpUXNode    CmpUNode\n@@ -2259,0 +2413,1 @@\n+#define Op_StoreX    Op_StoreI\n","filename":"src\/hotspot\/share\/opto\/type.hpp","additions":221,"deletions":66,"binary":false,"changes":287,"status":"modified"},{"patch":"@@ -146,0 +146,1 @@\n+  oop           _return_buffered_value; \/\/ buffered value being returned\n@@ -737,0 +738,3 @@\n+  oop return_buffered_value() const              { return _return_buffered_value; }\n+  void set_return_buffered_value(oop val)        { _return_buffered_value = val; }\n+\n@@ -803,0 +807,1 @@\n+  static ByteSize return_buffered_value_offset() { return byte_offset_of(JavaThread, _return_buffered_value); }\n","filename":"src\/hotspot\/share\/runtime\/javaThread.hpp","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -400,0 +400,1 @@\n+  initialize_class(vmSymbols::java_lang_IdentityException(), CHECK);\n","filename":"src\/hotspot\/share\/runtime\/threads.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -71,0 +71,2 @@\n+#include \"oops\/flatArrayKlass.hpp\"\n+#include \"oops\/inlineKlass.hpp\"\n@@ -233,1 +235,1 @@\n-  volatile_nonstatic_field(InstanceKlass,      _array_klasses,                                ObjArrayKlass*)                        \\\n+  volatile_nonstatic_field(InstanceKlass,      _array_klasses,                                ArrayKlass*)                        \\\n@@ -1166,0 +1168,1 @@\n+           declare_type(FlatArrayKlass, ArrayKlass)                       \\\n@@ -1169,0 +1172,1 @@\n+        declare_type(InlineKlass, InstanceKlass)                          \\\n@@ -1548,0 +1552,1 @@\n+  declare_c2_type(MachVEPNode, MachIdealNode)                             \\\n","filename":"src\/hotspot\/share\/runtime\/vmStructs.cpp","additions":6,"deletions":1,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -122,0 +122,1 @@\n+  DCmdFactory::register_DCmdFactory(new DCmdFactoryImpl<PrintClassLayoutDCmd>(full_export, true, false));\n@@ -162,1 +163,0 @@\n-\n@@ -958,1 +958,25 @@\n-#endif\n+\n+PrintClassLayoutDCmd::PrintClassLayoutDCmd(outputStream* output, bool heap) :\n+                                       DCmdWithParser(output, heap),\n+  _classname(\"classname\", \"Name of class whose layout should be printed. \",\n+             \"STRING\", true) {\n+  _dcmdparser.add_dcmd_argument(&_classname);\n+}\n+\n+void PrintClassLayoutDCmd::execute(DCmdSource source, TRAPS) {\n+  VM_PrintClassLayout printClassLayoutOp(output(), _classname.value());\n+  VMThread::execute(&printClassLayoutOp);\n+}\n+\n+int PrintClassLayoutDCmd::num_arguments() {\n+  ResourceMark rm;\n+  PrintClassLayoutDCmd* dcmd = new PrintClassLayoutDCmd(nullptr, false);\n+  if (dcmd != nullptr) {\n+    DCmdMark mark(dcmd);\n+    return dcmd->_dcmdparser.num_arguments();\n+  } else {\n+    return 0;\n+  }\n+}\n+\n+#endif \/\/ INCLUDE_SERVICES\n","filename":"src\/hotspot\/share\/services\/diagnosticCommand.cpp","additions":26,"deletions":2,"binary":false,"changes":28,"status":"modified"},{"patch":"@@ -633,0 +633,9 @@\n+\/\/----------------------------------------------------------------------------------------------------\n+\/\/ Prototyping\n+\/\/ \"Code Missing Here\" macro, un-define when integrating back from prototyping stage and break\n+\/\/ compilation on purpose (i.e. \"forget me not\")\n+#define PROTOTYPE\n+#ifdef PROTOTYPE\n+#define CMH(m)\n+#endif\n+\n@@ -718,6 +727,7 @@\n-  T_VOID        = 14,\n-  T_ADDRESS     = 15,\n-  T_NARROWOOP   = 16,\n-  T_METADATA    = 17,\n-  T_NARROWKLASS = 18,\n-  T_CONFLICT    = 19, \/\/ for stack value type with conflicting contents\n+  T_PRIMITIVE_OBJECT = 14, \/\/ Not a true BasicType, only use in headers of flat arrays\n+  T_VOID        = 15,\n+  T_ADDRESS     = 16,\n+  T_NARROWOOP   = 17,\n+  T_METADATA    = 18,\n+  T_NARROWKLASS = 19,\n+  T_CONFLICT    = 20, \/\/ for stack value type with conflicting contents\n@@ -738,0 +748,1 @@\n+    F(JVM_SIGNATURE_PRIMITIVE_OBJECT, T_PRIMITIVE_OBJECT, N) \\\n@@ -767,1 +778,1 @@\n-  return (t == T_OBJECT || t == T_ARRAY || (include_narrow_oop && t == T_NARROWOOP));\n+  return (t == T_OBJECT || t == T_ARRAY || t == T_PRIMITIVE_OBJECT || (include_narrow_oop && t == T_NARROWOOP));\n@@ -825,1 +836,2 @@\n-  T_VOID_size        = 0\n+  T_VOID_size        = 0,\n+  T_PRIMITIVE_OBJECT_size = 1\n@@ -855,0 +867,1 @@\n+  T_PRIMITIVE_OBJECT_aelem_bytes = 8,\n@@ -858,0 +871,1 @@\n+  T_PRIMITIVE_OBJECT_aelem_bytes = 4,\n@@ -950,1 +964,1 @@\n-  vtos = 9,             \/\/ tos not cached\n+  vtos = 9,             \/\/ tos not cached,\n@@ -967,1 +981,1 @@\n-    case T_ARRAY  : \/\/ fall through\n+    case T_ARRAY  :   \/\/ fall through\n@@ -1349,0 +1363,6 @@\n+\/\/ TEMP!!!!\n+\/\/ This should be removed after LW2 arrays are implemented (JDK-8220790).\n+\/\/ It's an alias to (EnableValhalla && (FlatArrayElementMaxSize != 0)),\n+\/\/ which is actually not 100% correct, but works for the current set of C1\/C2\n+\/\/ implementation and test cases.\n+#define UseFlatArray (EnableValhalla && (FlatArrayElementMaxSize != 0))\n","filename":"src\/hotspot\/share\/utilities\/globalDefinitions.hpp","additions":30,"deletions":10,"binary":false,"changes":40,"status":"modified"},{"patch":"@@ -58,0 +58,1 @@\n+import jdk.internal.misc.PreviewFeatures;\n@@ -379,1 +380,1 @@\n-               .withFlags(ACC_FINAL | ACC_SYNTHETIC)\n+               .withFlags((PreviewFeatures.isEnabled() ? ACC_IDENTITY  : 0) | ACC_FINAL | ACC_SYNTHETIC)\n","filename":"src\/java.base\/share\/classes\/java\/lang\/invoke\/MethodHandleProxies.java","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -28,0 +28,1 @@\n+import java.util.Objects;\n@@ -34,0 +35,2 @@\n+ * <p>\n+ * The referent must be an {@linkplain Objects#hasIdentity(Object) identity object}.\n@@ -103,0 +106,2 @@\n+     * @throws IdentityException if the referent is not an\n+     *         {@link java.util.Objects#hasIdentity(Object) identity object}\n","filename":"src\/java.base\/share\/classes\/java\/lang\/ref\/PhantomReference.java","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -35,0 +35,2 @@\n+import java.util.Objects;\n+\n@@ -40,0 +42,3 @@\n+ * <p>\n+ * The referent must be an {@linkplain Objects#hasIdentity(Object) identity object}.\n+ * Attempts to create a reference to a value object result in an {@link IdentityException}.\n@@ -547,0 +552,3 @@\n+        if (referent != null) {\n+            Objects.requireIdentity(referent);\n+        }\n","filename":"src\/java.base\/share\/classes\/java\/lang\/ref\/Reference.java","additions":8,"deletions":0,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -314,0 +314,18 @@\n+    public static final class LoadableDescriptorsMapper extends AbstractAttributeMapper<LoadableDescriptorsAttribute> {\n+        public static final LoadableDescriptorsMapper INSTANCE = new LoadableDescriptorsMapper();\n+\n+        private LoadableDescriptorsMapper() {\n+            super(NAME_LOADABLE_DESCRIPTORS, AttributeStability.CP_REFS);\n+        }\n+\n+        @Override\n+        public LoadableDescriptorsAttribute readAttribute(AttributedElement e, ClassReader cf, int p) {\n+            return new BoundAttribute.BoundLoadableDescriptorsAttribute(cf, this, p);\n+        }\n+\n+        @Override\n+        protected void writeBody(BufWriter buf, LoadableDescriptorsAttribute attr) {\n+            Util.writeListIndices(buf, attr.loadableDescriptors());\n+        }\n+    }\n+\n","filename":"src\/java.base\/share\/classes\/jdk\/internal\/classfile\/impl\/AbstractAttributeMapper.java","additions":18,"deletions":0,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -932,0 +932,3 @@\n+                case LoadableDescriptorsAttribute pa ->\n+                    nodes.add(list(\"loadable descriptors\", \"descriptor\", pa.loadableDescriptors().stream()\n+                            .map(e -> e.stringValue())));\n","filename":"src\/java.base\/share\/classes\/jdk\/internal\/classfile\/impl\/ClassPrinterImpl.java","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -52,0 +52,1 @@\n+import java.lang.classfile.attribute.LoadableDescriptorsAttribute;\n@@ -433,0 +434,16 @@\n+    public static final class UnboundLoadableDescriptorsAttribute\n+            extends UnboundAttribute<LoadableDescriptorsAttribute>\n+            implements LoadableDescriptorsAttribute {\n+        private final List<Utf8Entry> loadableDescriptors;\n+\n+        public UnboundLoadableDescriptorsAttribute(List<Utf8Entry> loadableDescriptors) {\n+            super(Attributes.loadableDescriptors());\n+            this.loadableDescriptors = List.copyOf(loadableDescriptors);\n+        }\n+\n+        @Override\n+        public List<Utf8Entry> loadableDescriptors() {\n+            return loadableDescriptors;\n+        }\n+    }\n+\n","filename":"src\/java.base\/share\/classes\/jdk\/internal\/classfile\/impl\/UnboundAttribute.java","additions":17,"deletions":0,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -230,2 +230,0 @@\n-    public final Type valueBasedType;\n-    public final Type valueBasedInternalType;\n@@ -247,0 +245,10 @@\n+    \/\/ valhalla\n+    public final Type valueBasedType;\n+    public final Type valueBasedInternalType;\n+    public final Type migratedValueClassType;\n+    public final Type migratedValueClassInternalType;\n+    public final Type strictType;\n+    \/** The symbol representing the finalize method on Object *\/\n+    public final MethodSymbol objectFinalize;\n+    public final Type numberType;\n+\n@@ -542,0 +550,6 @@\n+        throwableType = enterClass(\"java.lang.Throwable\");\n+        objectFinalize = new MethodSymbol(PROTECTED,\n+                names.finalize,\n+                new MethodType(List.nil(), voidType,\n+                        List.of(throwableType), methodClass),\n+                objectType.tsym);\n@@ -550,1 +564,0 @@\n-        throwableType = enterClass(\"java.lang.Throwable\");\n@@ -618,0 +631,3 @@\n+        strictType = enterSyntheticAnnotation(\"jdk.internal.vm.annotation.Strict\");\n+        migratedValueClassType = enterClass(\"jdk.internal.MigratedValueClass\");\n+        migratedValueClassInternalType = enterSyntheticAnnotation(\"jdk.internal.MigratedValueClass+Annotation\");\n@@ -640,0 +656,2 @@\n+        numberType = enterClass(\"java.lang.Number\");\n+\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/code\/Symtab.java","additions":21,"deletions":3,"binary":false,"changes":24,"status":"modified"},{"patch":"@@ -80,0 +80,1 @@\n+compiler\/c2\/irTests\/scalarReplacement\/ScalarReplacementWithGCBarrierTests.java  8342488 generic-all\n@@ -106,0 +107,1 @@\n+runtime\/cds\/appcds\/redefineClass\/RedefineRunningMethods_Shared.java  8304168 generic-all\n@@ -126,0 +128,4 @@\n+\n+# Valhalla\n+runtime\/AccModule\/ConstModule.java 8294051 generic-all\n+compiler\/gcbarriers\/TestG1BarrierGeneration.java 8343420 generic-all\n@@ -153,0 +159,31 @@\n+# Valhalla TODO:\n+serviceability\/jvmti\/valhalla\/HeapDump\/HeapDump.java 8317416 generic-all\n+\n+serviceability\/sa\/ClhsdbCDSCore.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbCDSJstackPrintAll.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbFindPC.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbInspect.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbLongConstant.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbJdis.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbJstack.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbPrintAll.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbPrintAs.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbPrintStatics.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbSource.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbSymbol.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbWhere.java 8190936 generic-all\n+serviceability\/sa\/JhsdbThreadInfoTest.java 8190936 generic-all\n+serviceability\/sa\/TestClassDump.java 8190936 generic-all\n+serviceability\/sa\/TestClhsdbJstackLock.java 8190936 generic-all\n+serviceability\/sa\/TestCpoolForInvokeDynamic.java 8190936 generic-all\n+serviceability\/sa\/TestHeapDumpForInvokeDynamic.java 8190936 generic-all\n+serviceability\/sa\/TestHeapDumpForLargeArray.java 8190936 generic-all\n+serviceability\/sa\/TestIntConstant.java 8190936 generic-all\n+serviceability\/sa\/TestJhsdbJstackLock.java 8190936 generic-all\n+serviceability\/sa\/TestJmapCore.java 8190936 generic-all\n+serviceability\/sa\/TestJmapCoreMetaspace.java 8190936 generic-all\n+serviceability\/sa\/TestPrintMdo.java 8190936 generic-all\n+serviceability\/sa\/jmap-hprof\/JMapHProfLargeHeapTest.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbDumpclass.java 8190936 generic-all\n+\n+\n@@ -192,0 +229,2 @@\n+vmTestbase\/vm\/mlvm\/hiddenloader\/stress\/byteMutation\/Test.java 8317172 generic-all\n+\n","filename":"test\/hotspot\/jtreg\/ProblemList.txt","additions":39,"deletions":0,"binary":false,"changes":39,"status":"modified"},{"patch":"@@ -31,0 +31,1 @@\n+import compiler.valhalla.inlinetypes.InlineTypeIRNode;\n@@ -155,0 +156,6 @@\n+    \/\/ Valhalla: Make sure that all Valhalla specific IR nodes are also properly initialized. Doing it here also\n+    \/\/           ensures that the Flag VM is able to pick up the correct compile phases.\n+    static {\n+        InlineTypeIRNode.forceStaticInitialization();\n+    }\n+\n@@ -279,1 +286,1 @@\n-        String optoRegex = \"(.*precise .*\\\\R((.*(?i:mov|mv|xorl|nop|spill).*|\\\\s*)\\\\R)*.*(?i:call,static).*wrapper for: C2 Runtime new_instance\" + END;\n+        String optoRegex = \"(.*precise .*\\\\R((.*(?i:mov|mv|xorl|nop|spill|pushq|popq).*|\\\\s*)\\\\R)*.*(?i:call,static).*wrapper for: C2 Runtime new_instance\" + END;\n@@ -285,1 +292,1 @@\n-        String regex = \"(.*precise .*\" + IS_REPLACED + \":.*\\\\R((.*(?i:mov|mv|xorl|nop|spill).*|\\\\s*)\\\\R)*.*(?i:call,static).*wrapper for: C2 Runtime new_instance\" + END;\n+        String regex = \"(.*precise .*\" + IS_REPLACED + \":.*\\\\R((.*(?i:mov|mv|xorl|nop|spill|pushq|popq).*|\\\\s*)\\\\R)*.*(?i:call,static).*wrapper for: C2 Runtime new_instance\" + END;\n@@ -291,1 +298,1 @@\n-        String optoRegex = \"(.*precise \\\\[.*\\\\R((.*(?i:mov|mv|xor|nop|spill).*|\\\\s*|.*(LGHI|LI).*)\\\\R)*.*(?i:call,static).*wrapper for: C2 Runtime new_array\" + END;\n+        String optoRegex = \"(.*precise \\\\[.*\\\\R((.*(?i:mov|mv|xor|nop|spill|pushq|popq).*|\\\\s*|.*(LGHI|LI).*)\\\\R)*.*(?i:call,static).*wrapper for: C2 Runtime new_array\" + END;\n@@ -297,1 +304,1 @@\n-        String regex = \"(.*precise \\\\[.*\" + IS_REPLACED + \":.*\\\\R((.*(?i:mov|mv|xorl|nop|spill).*|\\\\s*|.*(LGHI|LI).*)\\\\R)*.*(?i:call,static).*wrapper for: C2 Runtime new_array\" + END;\n+        String regex = \"(.*precise \\\\[.*\" + IS_REPLACED + \":.*\\\\R((.*(?i:mov|mv|xorl|nop|spill|pushq|popq).*|\\\\s*|.*(LGHI|LI).*)\\\\R)*.*(?i:call,static).*wrapper for: C2 Runtime new_array\" + END;\n@@ -680,0 +687,5 @@\n+    public static final String INLINE_TYPE = PREFIX + \"INLINE_TYPE\" + POSTFIX;\n+    static {\n+        beforeMatchingNameRegex(INLINE_TYPE, \"InlineType\");\n+    }\n+\n@@ -1701,1 +1713,1 @@\n-        beforeMatchingNameRegex(SUBTYPE_CHECK, \"SubTypeCheck\");\n+        macroNodes(SUBTYPE_CHECK, \"SubTypeCheck\");\n@@ -2386,1 +2398,1 @@\n-    private static void beforeMatching(String irNodePlaceholder, String regex) {\n+    public static void beforeMatching(String irNodePlaceholder, String regex) {\n@@ -2432,1 +2444,1 @@\n-    private static void optoOnly(String irNodePlaceholder, String regex) {\n+    public static void optoOnly(String irNodePlaceholder, String regex) {\n@@ -2483,0 +2495,13 @@\n+    \/**\n+     * Apply a regex that matches a macro node IR node name {@code macroNodeName} exactly on all machine independent\n+     * ideal graph phases up to and including {@link CompilePhase#BEFORE_MACRO_EXPANSION}. By default, we match on\n+     * {@link CompilePhase#BEFORE_MACRO_EXPANSION} when no {@link CompilePhase} is chosen.\n+     *\/\n+    private static void macroNodes(String irNodePlaceholder, String macroNodeName) {\n+        String macroNodeRegex = START + macroNodeName + \"\\\\b\" + MID + END;\n+        IR_NODE_MAPPINGS.put(irNodePlaceholder, new SinglePhaseRangeEntry(CompilePhase.BEFORE_MACRO_EXPANSION,\n+                                                                          macroNodeRegex,\n+                                                                          CompilePhase.BEFORE_STRINGOPTS,\n+                                                                          CompilePhase.BEFORE_MACRO_EXPANSION));\n+    }\n+\n","filename":"test\/hotspot\/jtreg\/compiler\/lib\/ir_framework\/IRNode.java","additions":32,"deletions":7,"binary":false,"changes":39,"status":"modified"},{"patch":"@@ -0,0 +1,4500 @@\n+\/*\n+ * Copyright (c) 2019, 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+package compiler.valhalla.inlinetypes;\n+\n+import compiler.lib.ir_framework.*;\n+import jdk.test.lib.Asserts;\n+import test.java.lang.invoke.lib.OldInstructionHelper;\n+\n+\n+import java.lang.invoke.MethodHandle;\n+import java.lang.invoke.MethodHandles;\n+import java.lang.invoke.MethodType;\n+import java.lang.reflect.Method;\n+import jdk.experimental.bytecode.TypeTag;\n+import java.util.Arrays;\n+import java.util.Objects;\n+\n+import jdk.internal.value.ValueClass;\n+import jdk.internal.vm.annotation.ImplicitlyConstructible;\n+import jdk.internal.vm.annotation.LooselyConsistentValue;\n+import jdk.internal.vm.annotation.NullRestricted;\n+\n+import static compiler.valhalla.inlinetypes.InlineTypeIRNode.*;\n+import static compiler.valhalla.inlinetypes.InlineTypes.*;\n+\n+\/*\n+ * @test\n+ * @key randomness\n+ * @summary Test inline types in LWorld.\n+ * @library \/test\/lib \/test\/jdk\/lib\/testlibrary\/bytecode \/test\/jdk\/java\/lang\/invoke\/common \/\n+ * @requires (os.simpleArch == \"x64\" | os.simpleArch == \"aarch64\")\n+ * @enablePreview\n+ * @modules java.base\/jdk.internal.value\n+ *          java.base\/jdk.internal.vm.annotation\n+ * @build jdk.experimental.bytecode.BasicClassBuilder test.java.lang.invoke.lib.OldInstructionHelper\n+ * @run main\/othervm\/timeout=450 compiler.valhalla.inlinetypes.TestLWorld\n+ *\/\n+\n+@ForceCompileClassInitializer\n+public class TestLWorld {\n+\n+    public static void main(String[] args) {\n+        \/\/ Make sure Test140Value is loaded but not linked\n+        Class<?> class1 = Test140Value.class;\n+        \/\/ Make sure Test141Value is linked but not initialized\n+        Class<?> class2 = Test141Value.class;\n+        class2.getDeclaredFields();\n+\n+        Scenario[] scenarios = InlineTypes.DEFAULT_SCENARIOS;\n+        scenarios[3].addFlags(\"-XX:-MonomorphicArrayCheck\", \"-XX:FlatArrayElementMaxSize=-1\");\n+        scenarios[4].addFlags(\"-XX:-MonomorphicArrayCheck\");\n+\n+        InlineTypes.getFramework()\n+                   .addScenarios(scenarios)\n+                   .addHelperClasses(MyValue1.class,\n+                                     MyValue2.class,\n+                                     MyValue2Inline.class,\n+                                     MyValue3.class,\n+                                     MyValue3Inline.class)\n+                   .start();\n+    }\n+\n+    static {\n+        \/\/ Make sure RuntimeException is loaded to prevent uncommon traps in IR verified tests\n+        RuntimeException tmp = new RuntimeException(\"42\");\n+    }\n+\n+    \/\/ Helper methods\n+\n+    @NullRestricted\n+    private static final MyValue1 testValue1 = MyValue1.createWithFieldsInline(rI, rL);\n+    @NullRestricted\n+    private static final MyValue2 testValue2 = MyValue2.createWithFieldsInline(rI, rD);\n+\n+    protected long hash() {\n+        return testValue1.hash();\n+    }\n+\n+    \/\/ Test passing an inline type as an Object\n+    @DontInline\n+    public Object test1_dontinline1(Object o) {\n+        return o;\n+    }\n+\n+    @DontInline\n+    public MyValue1 test1_dontinline2(Object o) {\n+        return (MyValue1)o;\n+    }\n+\n+    @ForceInline\n+    public Object test1_inline1(Object o) {\n+        return o;\n+    }\n+\n+    @ForceInline\n+    public MyValue1 test1_inline2(Object o) {\n+        return (MyValue1)o;\n+    }\n+\n+    @Test\n+    @IR(failOn = {ALLOC_G})\n+    public MyValue1 test1() {\n+        MyValue1 vt = testValue1;\n+        vt = (MyValue1)test1_dontinline1(vt);\n+        vt =           test1_dontinline2(vt);\n+        vt = (MyValue1)test1_inline1(vt);\n+        vt =           test1_inline2(vt);\n+        return vt;\n+    }\n+\n+    @Run(test = \"test1\")\n+    public void test1_verifier() {\n+        Asserts.assertEQ(test1().hash(), hash());\n+    }\n+\n+    \/\/ Test storing\/loading inline types to\/from Object and inline type fields\n+    Object objectField1 = null;\n+    Object objectField2 = null;\n+    Object objectField3 = null;\n+    Object objectField4 = null;\n+    Object objectField5 = null;\n+    Object objectField6 = null;\n+\n+    @NullRestricted\n+    MyValue1 valueField1 = testValue1;\n+    @NullRestricted\n+    MyValue1 valueField2 = testValue1;\n+    MyValue1 valueField3 = testValue1;\n+    @NullRestricted\n+    MyValue1 valueField4;\n+    MyValue1 valueField5;\n+\n+    static MyValue1 staticValueField1 = testValue1;\n+    @NullRestricted\n+    static MyValue1 staticValueField2 = testValue1;\n+    @NullRestricted\n+    static MyValue1 staticValueField3;\n+    static MyValue1 staticValueField4;\n+\n+    @DontInline\n+    public Object readValueField5() {\n+        return (Object)valueField5;\n+    }\n+\n+    @DontInline\n+    public Object readStaticValueField4() {\n+        return (Object)staticValueField4;\n+    }\n+\n+    @Test\n+    public long test2(MyValue1 vt1, Object vt2) {\n+        objectField1 = vt1;\n+        objectField2 = (MyValue1)vt2;\n+        objectField3 = testValue1;\n+        objectField4 = MyValue1.createWithFieldsDontInline(rI, rL);\n+        objectField5 = valueField1;\n+        objectField6 = valueField3;\n+        valueField1 = (MyValue1)objectField1;\n+        valueField2 = (MyValue1)vt2;\n+        valueField3 = (MyValue1)vt2;\n+        staticValueField1 = (MyValue1)objectField1;\n+        staticValueField2 = (MyValue1)vt1;\n+        \/\/ Don't inline these methods because reading NULL will trigger a deoptimization\n+        if (readValueField5() != null || readStaticValueField4() != null) {\n+            throw new RuntimeException(\"Should be null\");\n+        }\n+        return ((MyValue1)objectField1).hash() + ((MyValue1)objectField2).hash() +\n+               ((MyValue1)objectField3).hash() + ((MyValue1)objectField4).hash() +\n+               ((MyValue1)objectField5).hash() + ((MyValue1)objectField6).hash() +\n+                valueField1.hash() + valueField2.hash() + valueField3.hash() + valueField4.hashPrimitive() +\n+                staticValueField1.hash() + staticValueField2.hash() + staticValueField3.hashPrimitive();\n+    }\n+\n+    @Run(test = \"test2\")\n+    public void test2_verifier() {\n+        MyValue1 vt = testValue1;\n+        MyValue1 def = MyValue1.createDefaultDontInline();\n+        long result = test2(vt, vt);\n+        Asserts.assertEQ(result, 11*vt.hash() + 2*def.hashPrimitive());\n+    }\n+\n+    \/\/ Test merging inline types and objects\n+    @Test\n+    public Object test3(int state) {\n+        Object res = null;\n+        if (state == 0) {\n+            res = new NonValueClass(rI);\n+        } else if (state == 1) {\n+            res = MyValue1.createWithFieldsInline(rI, rL);\n+        } else if (state == 2) {\n+            res = MyValue1.createWithFieldsDontInline(rI, rL);\n+        } else if (state == 3) {\n+            res = (MyValue1)objectField1;\n+        } else if (state == 4) {\n+            res = valueField1;\n+        } else if (state == 5) {\n+            res = null;\n+        } else if (state == 6) {\n+            res = MyValue2.createWithFieldsInline(rI, rD);\n+        } else if (state == 7) {\n+            res = testValue2;\n+        }\n+        return res;\n+    }\n+\n+    @Run(test = \"test3\")\n+    public void test3_verifier() {\n+        objectField1 = valueField1;\n+        Object result = null;\n+        result = test3(0);\n+        Asserts.assertEQ(((NonValueClass)result).x, rI);\n+        result = test3(1);\n+        Asserts.assertEQ(((MyValue1)result).hash(), hash());\n+        result = test3(2);\n+        Asserts.assertEQ(((MyValue1)result).hash(), hash());\n+        result = test3(3);\n+        Asserts.assertEQ(((MyValue1)result).hash(), hash());\n+        result = test3(4);\n+        Asserts.assertEQ(((MyValue1)result).hash(), hash());\n+        result = test3(5);\n+        Asserts.assertEQ(result, null);\n+        result = test3(6);\n+        Asserts.assertEQ(((MyValue2)result).hash(), testValue2.hash());\n+        result = test3(7);\n+        Asserts.assertEQ(((MyValue2)result).hash(), testValue2.hash());\n+    }\n+\n+    \/\/ Test merging inline types and objects in loops\n+    @Test\n+    public Object test4(int iters) {\n+        Object res = new NonValueClass(rI);\n+        for (int i = 0; i < iters; ++i) {\n+            if (res instanceof NonValueClass) {\n+                res = MyValue1.createWithFieldsInline(rI, rL);\n+            } else {\n+                res = MyValue1.createWithFieldsInline(((MyValue1)res).x + 1, rL);\n+            }\n+        }\n+        return res;\n+    }\n+\n+    @Run(test = \"test4\")\n+    public void test4_verifier() {\n+        NonValueClass result1 = (NonValueClass)test4(0);\n+        Asserts.assertEQ(result1.x, rI);\n+        int iters = (Math.abs(rI) % 10) + 1;\n+        MyValue1 result2 = (MyValue1)test4(iters);\n+        MyValue1 vt = MyValue1.createWithFieldsInline(rI + iters - 1, rL);\n+        Asserts.assertEQ(result2.hash(), vt.hash());\n+    }\n+\n+    \/\/ Test inline types in object variables that are live at safepoint\n+    @Test\n+    @IR(failOn = {ALLOC, STORE, LOOP})\n+    public long test5(MyValue1 arg, boolean deopt, Method m) {\n+        Object vt1 = MyValue1.createWithFieldsInline(rI, rL);\n+        Object vt2 = MyValue1.createWithFieldsDontInline(rI, rL);\n+        Object vt3 = arg;\n+        Object vt4 = valueField1;\n+        if (deopt) {\n+            \/\/ uncommon trap\n+            TestFramework.deoptimize(m);\n+        }\n+        return ((MyValue1)vt1).hash() + ((MyValue1)vt2).hash() +\n+               ((MyValue1)vt3).hash() + ((MyValue1)vt4).hash();\n+    }\n+\n+    @Run(test = \"test5\")\n+    public void test5_verifier(RunInfo info) {\n+        long result = test5(valueField1, !info.isWarmUp(), info.getTest());\n+        Asserts.assertEQ(result, 4*hash());\n+    }\n+\n+    \/\/ Test comparing inline types with objects\n+    @Test\n+    public boolean test6(Object arg) {\n+        Object vt = MyValue1.createWithFieldsInline(rI, rL);\n+        if (vt == arg || vt == (Object)valueField1 || vt == objectField1 || vt == null ||\n+            arg == vt || (Object)valueField1 == vt || objectField1 == vt || null == vt) {\n+            return true;\n+        }\n+        return false;\n+    }\n+\n+    @Run(test = \"test6\")\n+    public void test6_verifier() {\n+        boolean result = test6(null);\n+        Asserts.assertFalse(result);\n+    }\n+\n+    \/\/ merge of inline type and non-inline type\n+    @Test\n+    public Object test7(boolean flag) {\n+        Object res = null;\n+        if (flag) {\n+            res = valueField1;\n+        } else {\n+            res = objectField1;\n+        }\n+        return res;\n+    }\n+\n+    @Run(test = \"test7\")\n+    public void test7_verifier() {\n+        test7(true);\n+        test7(false);\n+    }\n+\n+    @Test\n+    public Object test8(boolean flag) {\n+        Object res = null;\n+        if (flag) {\n+            res = objectField1;\n+        } else {\n+            res = valueField1;\n+        }\n+        return res;\n+    }\n+\n+    @Run(test = \"test8\")\n+    public void test8_verifier() {\n+        test8(true);\n+        test8(false);\n+    }\n+\n+    \/\/ merge of inline types in a loop, stored in an object local\n+    @Test\n+    public Object test9() {\n+        Object o = valueField1;\n+        for (int i = 1; i < 100; i *= 2) {\n+            MyValue1 v = (MyValue1)o;\n+            o = MyValue1.setX(v, v.x + 1);\n+        }\n+        return o;\n+    }\n+\n+    @Run(test = \"test9\")\n+    public void test9_verifier() {\n+        test9();\n+    }\n+\n+    \/\/ merge of inline types in an object local\n+    @ForceInline\n+    public Object test10_helper() {\n+        return valueField1;\n+    }\n+\n+    @Test\n+    @IR(failOn = {ALLOC_G})\n+    public void test10(boolean flag) {\n+        Object o = null;\n+        if (flag) {\n+            o = valueField1;\n+        } else {\n+            o = test10_helper();\n+        }\n+        valueField1 = (MyValue1)o;\n+    }\n+\n+    @Run(test = \"test10\")\n+    public void test10_verifier() {\n+        test10(true);\n+        test10(false);\n+    }\n+\n+    \/\/ Interface tests\n+\n+    @DontInline\n+    public MyInterface test11_dontinline1(MyInterface o) {\n+        return o;\n+    }\n+\n+    @DontInline\n+    public MyValue1 test11_dontinline2(MyInterface o) {\n+        return (MyValue1)o;\n+    }\n+\n+    @ForceInline\n+    public MyInterface test11_inline1(MyInterface o) {\n+        return o;\n+    }\n+\n+    @ForceInline\n+    public MyValue1 test11_inline2(MyInterface o) {\n+        return (MyValue1)o;\n+    }\n+\n+    @Test\n+    public MyValue1 test11() {\n+        MyValue1 vt = MyValue1.createWithFieldsInline(rI, rL);\n+        vt = (MyValue1)test11_dontinline1(vt);\n+        vt =           test11_dontinline2(vt);\n+        vt = (MyValue1)test11_inline1(vt);\n+        vt =           test11_inline2(vt);\n+        return vt;\n+    }\n+\n+    @Run(test = \"test11\")\n+    public void test11_verifier() {\n+        Asserts.assertEQ(test11().hash(), hash());\n+    }\n+\n+    \/\/ Test storing\/loading inline types to\/from interface and inline type fields\n+    MyInterface interfaceField1 = null;\n+    MyInterface interfaceField2 = null;\n+    MyInterface interfaceField3 = null;\n+    MyInterface interfaceField4 = null;\n+    MyInterface interfaceField5 = null;\n+    MyInterface interfaceField6 = null;\n+\n+    @DontInline\n+    public MyInterface readValueField5AsInterface() {\n+        return (MyInterface)valueField5;\n+    }\n+\n+    @DontInline\n+    public MyInterface readStaticValueField4AsInterface() {\n+        return (MyInterface)staticValueField4;\n+    }\n+\n+    @Test\n+    public long test12(MyValue1 vt1, MyInterface vt2) {\n+        interfaceField1 = vt1;\n+        interfaceField2 = (MyValue1)vt2;\n+        interfaceField3 = MyValue1.createWithFieldsInline(rI, rL);\n+        interfaceField4 = MyValue1.createWithFieldsDontInline(rI, rL);\n+        interfaceField5 = valueField1;\n+        interfaceField6 = valueField3;\n+        valueField1 = (MyValue1)interfaceField1;\n+        valueField2 = (MyValue1)vt2;\n+        valueField3 = (MyValue1)vt2;\n+        staticValueField1 = (MyValue1)interfaceField1;\n+        staticValueField2 = (MyValue1)vt1;\n+        \/\/ Don't inline these methods because reading NULL will trigger a deoptimization\n+        if (readValueField5AsInterface() != null || readStaticValueField4AsInterface() != null) {\n+            throw new RuntimeException(\"Should be null\");\n+        }\n+        return ((MyValue1)interfaceField1).hash() + ((MyValue1)interfaceField2).hash() +\n+               ((MyValue1)interfaceField3).hash() + ((MyValue1)interfaceField4).hash() +\n+               ((MyValue1)interfaceField5).hash() + ((MyValue1)interfaceField6).hash() +\n+                valueField1.hash() + valueField2.hash() + valueField3.hash() + valueField4.hashPrimitive() +\n+                staticValueField1.hash() + staticValueField2.hash() + staticValueField3.hashPrimitive();\n+    }\n+\n+    @Run(test = \"test12\")\n+    public void test12_verifier() {\n+        MyValue1 vt = testValue1;\n+        MyValue1 def = MyValue1.createDefaultDontInline();\n+        long result = test12(vt, vt);\n+        Asserts.assertEQ(result, 11*vt.hash() + 2*def.hashPrimitive());\n+    }\n+\n+    class MyObject1 implements MyInterface {\n+        public int x;\n+\n+        public MyObject1(int x) {\n+            this.x = x;\n+        }\n+\n+        @ForceInline\n+        public long hash() {\n+            return x;\n+        }\n+    }\n+\n+    \/\/ Test merging inline types and interfaces\n+    @Test\n+    public MyInterface test13(int state) {\n+        MyInterface res = null;\n+        if (state == 0) {\n+            res = new MyObject1(rI);\n+        } else if (state == 1) {\n+            res = MyValue1.createWithFieldsInline(rI, rL);\n+        } else if (state == 2) {\n+            res = MyValue1.createWithFieldsDontInline(rI, rL);\n+        } else if (state == 3) {\n+            res = (MyValue1)objectField1;\n+        } else if (state == 4) {\n+            res = valueField1;\n+        } else if (state == 5) {\n+            res = null;\n+        }\n+        return res;\n+    }\n+\n+    @Run(test = \"test13\")\n+    public void test13_verifier() {\n+        objectField1 = valueField1;\n+        MyInterface result = null;\n+        result = test13(0);\n+        Asserts.assertEQ(((MyObject1)result).x, rI);\n+        result = test13(1);\n+        Asserts.assertEQ(((MyValue1)result).hash(), hash());\n+        result = test13(2);\n+        Asserts.assertEQ(((MyValue1)result).hash(), hash());\n+        result = test13(3);\n+        Asserts.assertEQ(((MyValue1)result).hash(), hash());\n+        result = test13(4);\n+        Asserts.assertEQ(((MyValue1)result).hash(), hash());\n+        result = test13(5);\n+        Asserts.assertEQ(result, null);\n+    }\n+\n+    \/\/ Test merging inline types and interfaces in loops\n+    @Test\n+    public MyInterface test14(int iters) {\n+        MyInterface res = new MyObject1(rI);\n+        for (int i = 0; i < iters; ++i) {\n+            if (res instanceof MyObject1) {\n+                res = MyValue1.createWithFieldsInline(rI, rL);\n+            } else {\n+                res = MyValue1.createWithFieldsInline(((MyValue1)res).x + 1, rL);\n+            }\n+        }\n+        return res;\n+    }\n+\n+    @Run(test = \"test14\")\n+    public void test14_verifier() {\n+        MyObject1 result1 = (MyObject1)test14(0);\n+        Asserts.assertEQ(result1.x, rI);\n+        int iters = (Math.abs(rI) % 10) + 1;\n+        MyValue1 result2 = (MyValue1)test14(iters);\n+        MyValue1 vt = MyValue1.createWithFieldsInline(rI + iters - 1, rL);\n+        Asserts.assertEQ(result2.hash(), vt.hash());\n+    }\n+\n+    \/\/ Test inline types in interface variables that are live at safepoint\n+    @Test\n+    @IR(failOn = {ALLOC, STORE, LOOP})\n+    public long test15(MyValue1 arg, boolean deopt, Method m) {\n+        MyInterface vt1 = MyValue1.createWithFieldsInline(rI, rL);\n+        MyInterface vt2 = MyValue1.createWithFieldsDontInline(rI, rL);\n+        MyInterface vt3 = arg;\n+        MyInterface vt4 = valueField1;\n+        return ((MyValue1)vt1).hash() + ((MyValue1)vt2).hash() +\n+               ((MyValue1)vt3).hash() + ((MyValue1)vt4).hash();\n+    }\n+\n+    @Run(test = \"test15\")\n+    public void test15_verifier(RunInfo info) {\n+        long result = test15(valueField1, !info.isWarmUp(), info.getTest());\n+        Asserts.assertEQ(result, 4*hash());\n+    }\n+\n+    \/\/ Test comparing inline types with interfaces\n+    @Test\n+    public boolean test16(Object arg) {\n+        MyInterface vt = MyValue1.createWithFieldsInline(rI, rL);\n+        if (vt == arg || vt == (MyInterface)valueField1 || vt == interfaceField1 || vt == null ||\n+            arg == vt || (MyInterface)valueField1 == vt || interfaceField1 == vt || null == vt) {\n+            return true;\n+        }\n+        return false;\n+    }\n+\n+    @Run(test = \"test16\")\n+    public void test16_verifier() {\n+        boolean result = test16(null);\n+        Asserts.assertFalse(result);\n+    }\n+\n+    \/\/ Test subtype check when casting to inline type\n+    @Test\n+    @IR(failOn = {ALLOC})\n+    public MyValue1 test17(MyValue1 vt, Object obj) {\n+        try {\n+            vt = (MyValue1)obj;\n+            throw new RuntimeException(\"ClassCastException expected\");\n+        } catch (ClassCastException e) {\n+            \/\/ Expected\n+        }\n+        return vt;\n+    }\n+\n+    @Run(test = \"test17\")\n+    public void test17_verifier() {\n+        MyValue1 vt = testValue1;\n+        MyValue1 result = test17(vt, new NonValueClass(rI));\n+        Asserts.assertEquals(result.hash(), vt.hash());\n+    }\n+\n+    @Test\n+    public MyValue1 test18(MyValue1 vt) {\n+        Object obj = vt;\n+        vt = (MyValue1)obj;\n+        return vt;\n+    }\n+\n+    @Run(test = \"test18\")\n+    public void test18_verifier() {\n+        MyValue1 vt = testValue1;\n+        MyValue1 result = test18(vt);\n+        Asserts.assertEquals(result.hash(), vt.hash());\n+    }\n+\n+    @Test\n+    @IR(failOn = {ALLOC_G})\n+    public void test19(MyValue1 vt) {\n+        if (vt == null) {\n+            return;\n+        }\n+        Object obj = vt;\n+        try {\n+            MyValue2 vt2 = (MyValue2)obj;\n+            throw new RuntimeException(\"ClassCastException expected\");\n+        } catch (ClassCastException e) {\n+            \/\/ Expected\n+        }\n+    }\n+\n+    @Run(test = \"test19\")\n+    public void test19_verifier() {\n+        test19(valueField1);\n+    }\n+\n+    @Test\n+    @IR(failOn = {ALLOC_G})\n+    public void test20(MyValue1 vt) {\n+        if (vt == null) {\n+            return;\n+        }\n+        Object obj = vt;\n+        try {\n+            NonValueClass i = (NonValueClass)obj;\n+            throw new RuntimeException(\"ClassCastException expected\");\n+        } catch (ClassCastException e) {\n+            \/\/ Expected\n+        }\n+    }\n+\n+    @Run(test = \"test20\")\n+    public void test20_verifier() {\n+        test20(valueField1);\n+    }\n+\n+    \/\/ Array tests\n+\n+    private static final MyValue1[] testValue1Array = (MyValue1[])ValueClass.newNullRestrictedArray(MyValue1.class, 3);\n+    static {\n+        for (int i = 0; i < 3; ++i) {\n+            testValue1Array[i] = testValue1;\n+        }\n+    }\n+\n+    private static final MyValue1[][] testValue1Array2 = new MyValue1[][] {testValue1Array,\n+                                                                           testValue1Array,\n+                                                                           testValue1Array};\n+\n+    private static final MyValue2[] testValue2Array = (MyValue2[])ValueClass.newNullRestrictedArray(MyValue2.class, 3);\n+    static {\n+        for (int i = 0; i < 3; ++i) {\n+            testValue2Array[i] = testValue2;\n+        }\n+    }\n+\n+    private static final NonValueClass[] testNonValueArray = new NonValueClass[42];\n+\n+    \/\/ Test load from (flattened) inline type array disguised as object array\n+    @Test\n+    public Object test21(Object[] oa, int index) {\n+        return oa[index];\n+    }\n+\n+    @Run(test = \"test21\")\n+    public void test21_verifier() {\n+        MyValue1 result = (MyValue1)test21(testValue1Array, Math.abs(rI) % 3);\n+        Asserts.assertEQ(result.hash(), hash());\n+    }\n+\n+    \/\/ Test load from (flattened) inline type array disguised as interface array\n+    @Test\n+    public Object test22Interface(MyInterface[] ia, int index) {\n+        return ia[index];\n+    }\n+\n+    @Run(test = \"test22Interface\")\n+    public void test22Interface_verifier() {\n+        MyValue1 result = (MyValue1)test22Interface(testValue1Array, Math.abs(rI) % 3);\n+        Asserts.assertEQ(result.hash(), hash());\n+    }\n+\n+    \/\/ Test load from (flattened) inline type array disguised as abstract array\n+    @Test\n+    public Object test22Abstract(MyAbstract[] ia, int index) {\n+        return ia[index];\n+    }\n+\n+    @Run(test = \"test22Abstract\")\n+    public void test22Abstract_verifier() {\n+        MyValue1 result = (MyValue1)test22Abstract(testValue1Array, Math.abs(rI) % 3);\n+        Asserts.assertEQ(result.hash(), hash());\n+    }\n+\n+    \/\/ Test inline store to (flattened) inline type array disguised as object array\n+    @ForceInline\n+    public void test23_inline(Object[] oa, Object o, int index) {\n+        oa[index] = o;\n+    }\n+\n+    @Test\n+    public void test23(Object[] oa, MyValue1 vt, int index) {\n+        test23_inline(oa, vt, index);\n+    }\n+\n+    @Run(test = \"test23\")\n+    public void test23_verifier() {\n+        int index = Math.abs(rI) % 3;\n+        MyValue1 vt = MyValue1.createWithFieldsInline(rI + 1, rL + 1);\n+        test23(testValue1Array, vt, index);\n+        Asserts.assertEQ(testValue1Array[index].hash(), vt.hash());\n+        testValue1Array[index] = testValue1;\n+        try {\n+            test23(testValue2Array, vt, index);\n+            throw new RuntimeException(\"No ArrayStoreException thrown\");\n+        } catch (ArrayStoreException e) {\n+            \/\/ Expected\n+        }\n+        Asserts.assertEQ(testValue2Array[index].hash(), testValue2.hash());\n+    }\n+\n+    @ForceInline\n+    public void test24_inline(Object[] oa, Object o, int index) {\n+        oa[index] = o;\n+    }\n+\n+    @Test\n+    public void test24(Object[] oa, MyValue1 vt, int index) {\n+        test24_inline(oa, vt, index);\n+    }\n+\n+    @Run(test = \"test24\")\n+    public void test24_verifier() {\n+        int index = Math.abs(rI) % 3;\n+        try {\n+            test24(testNonValueArray, testValue1, index);\n+            throw new RuntimeException(\"No ArrayStoreException thrown\");\n+        } catch (ArrayStoreException e) {\n+            \/\/ Expected\n+        }\n+    }\n+\n+    @ForceInline\n+    public void test25_inline(Object[] oa, Object o, int index) {\n+        oa[index] = o;\n+    }\n+\n+    @Test\n+    public void test25(Object[] oa, MyValue1 vt, int index) {\n+        test25_inline(oa, vt, index);\n+    }\n+\n+    @Run(test = \"test25\")\n+    public void test25_verifier() {\n+        int index = Math.abs(rI) % 3;\n+        try {\n+            test25(null, testValue1, index);\n+            throw new RuntimeException(\"No NPE thrown\");\n+        } catch (NullPointerException e) {\n+            \/\/ Expected\n+        }\n+    }\n+\n+    \/\/ Test inline store to (flattened) inline type array disguised as interface array\n+    @ForceInline\n+    public void test26Interface_inline(MyInterface[] ia, MyInterface i, int index) {\n+        ia[index] = i;\n+    }\n+\n+    @Test\n+    public void test26Interface(MyInterface[] ia, MyValue1 vt, int index) {\n+      test26Interface_inline(ia, vt, index);\n+    }\n+\n+    @Run(test = \"test26Interface\")\n+    public void test26Interface_verifier() {\n+        int index = Math.abs(rI) % 3;\n+        MyValue1 vt = MyValue1.createWithFieldsInline(rI + 1, rL + 1);\n+        test26Interface(testValue1Array, vt, index);\n+        Asserts.assertEQ(testValue1Array[index].hash(), vt.hash());\n+        testValue1Array[index] = testValue1;\n+        try {\n+            test26Interface(testValue2Array, vt, index);\n+            throw new RuntimeException(\"No ArrayStoreException thrown\");\n+        } catch (ArrayStoreException e) {\n+            \/\/ Expected\n+        }\n+        Asserts.assertEQ(testValue2Array[index].hash(), testValue2.hash());\n+    }\n+\n+    @ForceInline\n+    public void test27Interface_inline(MyInterface[] ia, MyInterface i, int index) {\n+        ia[index] = i;\n+    }\n+\n+    @Test\n+    public void test27Interface(MyInterface[] ia, MyValue1 vt, int index) {\n+        test27Interface_inline(ia, vt, index);\n+    }\n+\n+    @Run(test = \"test27Interface\")\n+    public void test27Interface_verifier() {\n+        int index = Math.abs(rI) % 3;\n+        try {\n+            test27Interface(null, testValue1, index);\n+            throw new RuntimeException(\"No NPE thrown\");\n+        } catch (NullPointerException e) {\n+            \/\/ Expected\n+        }\n+    }\n+\n+    \/\/ Test inline store to (flattened) inline type array disguised as abstract array\n+    @ForceInline\n+    public void test26Abstract_inline(MyAbstract[] ia, MyAbstract i, int index) {\n+        ia[index] = i;\n+    }\n+\n+    @Test\n+    public void test26Abstract(MyAbstract[] ia, MyValue1 vt, int index) {\n+      test26Abstract_inline(ia, vt, index);\n+    }\n+\n+    @Run(test = \"test26Abstract\")\n+    public void test26Abstract_verifier() {\n+        int index = Math.abs(rI) % 3;\n+        MyValue1 vt = MyValue1.createWithFieldsInline(rI + 1, rL + 1);\n+        test26Abstract(testValue1Array, vt, index);\n+        Asserts.assertEQ(testValue1Array[index].hash(), vt.hash());\n+        testValue1Array[index] = testValue1;\n+        try {\n+            test26Abstract(testValue2Array, vt, index);\n+            throw new RuntimeException(\"No ArrayStoreException thrown\");\n+        } catch (ArrayStoreException e) {\n+            \/\/ Expected\n+        }\n+        Asserts.assertEQ(testValue2Array[index].hash(), testValue2.hash());\n+    }\n+\n+    @ForceInline\n+    public void test27Abstract_inline(MyAbstract[] ia, MyAbstract i, int index) {\n+        ia[index] = i;\n+    }\n+\n+    @Test\n+    public void test27Abstract(MyAbstract[] ia, MyValue1 vt, int index) {\n+        test27Abstract_inline(ia, vt, index);\n+    }\n+\n+    @Run(test = \"test27Abstract\")\n+    public void test27Abstract_verifier() {\n+        int index = Math.abs(rI) % 3;\n+        try {\n+            test27Abstract(null, testValue1, index);\n+            throw new RuntimeException(\"No NPE thrown\");\n+        } catch (NullPointerException e) {\n+            \/\/ Expected\n+        }\n+    }\n+\n+    \/\/ Test object store to (flattened) inline type array disguised as object array\n+    @ForceInline\n+    public void test28_inline(Object[] oa, Object o, int index) {\n+        oa[index] = o;\n+    }\n+\n+    @Test\n+    public void test28(Object[] oa, Object o, int index) {\n+        test28_inline(oa, o, index);\n+    }\n+\n+    @Run(test = \"test28\")\n+    public void test28_verifier() {\n+        int index = Math.abs(rI) % 3;\n+        MyValue1 vt1 = MyValue1.createWithFieldsInline(rI + 1, rL + 1);\n+        test28(testValue1Array, vt1, index);\n+        Asserts.assertEQ(testValue1Array[index].hash(), vt1.hash());\n+        try {\n+            test28(testValue1Array, testValue2, index);\n+            throw new RuntimeException(\"No ArrayStoreException thrown\");\n+        } catch (ArrayStoreException e) {\n+            \/\/ Expected\n+        }\n+        Asserts.assertEQ(testValue1Array[index].hash(), vt1.hash());\n+        testValue1Array[index] = testValue1;\n+    }\n+\n+    @ForceInline\n+    public void test29_inline(Object[] oa, Object o, int index) {\n+        oa[index] = o;\n+    }\n+\n+    @Test\n+    public void test29(Object[] oa, Object o, int index) {\n+        test29_inline(oa, o, index);\n+    }\n+\n+    @Run(test = \"test29\")\n+    public void test29_verifier() {\n+        int index = Math.abs(rI) % 3;\n+        try {\n+            test29(testValue2Array, testValue1, index);\n+            throw new RuntimeException(\"No ArrayStoreException thrown\");\n+        } catch (ArrayStoreException e) {\n+            \/\/ Expected\n+        }\n+        Asserts.assertEQ(testValue2Array[index].hash(), testValue2.hash());\n+    }\n+\n+    @ForceInline\n+    public void test30_inline(Object[] oa, Object o, int index) {\n+        oa[index] = o;\n+    }\n+\n+    @Test\n+    public void test30(Object[] oa, Object o, int index) {\n+        test30_inline(oa, o, index);\n+    }\n+\n+    @Run(test = \"test30\")\n+    public void test30_verifier() {\n+        int index = Math.abs(rI) % 3;\n+        try {\n+            test30(testNonValueArray, testValue1, index);\n+            throw new RuntimeException(\"No ArrayStoreException thrown\");\n+        } catch (ArrayStoreException e) {\n+            \/\/ Expected\n+        }\n+    }\n+\n+    \/\/ Test inline store to (flattened) inline type array disguised as interface array\n+    @ForceInline\n+    public void test31Interface_inline(MyInterface[] ia, MyInterface i, int index) {\n+        ia[index] = i;\n+    }\n+\n+    @Test\n+    public void test31Interface(MyInterface[] ia, MyInterface i, int index) {\n+        test31Interface_inline(ia, i, index);\n+    }\n+\n+    @Run(test = \"test31Interface\")\n+    public void test31Interface_verifier() {\n+        int index = Math.abs(rI) % 3;\n+        MyValue1 vt1 = MyValue1.createWithFieldsInline(rI + 1, rL + 1);\n+        test31Interface(testValue1Array, vt1, index);\n+        Asserts.assertEQ(testValue1Array[index].hash(), vt1.hash());\n+        try {\n+            test31Interface(testValue1Array, testValue2, index);\n+            throw new RuntimeException(\"No ArrayStoreException thrown\");\n+        } catch (ArrayStoreException e) {\n+            \/\/ Expected\n+        }\n+        Asserts.assertEQ(testValue1Array[index].hash(), vt1.hash());\n+        testValue1Array[index] = testValue1;\n+    }\n+\n+    @ForceInline\n+    public void test32Interface_inline(MyInterface[] ia, MyInterface i, int index) {\n+        ia[index] = i;\n+    }\n+\n+    @Test\n+    public void test32Interface(MyInterface[] ia, MyInterface i, int index) {\n+        test32Interface_inline(ia, i, index);\n+    }\n+\n+    @Run(test = \"test32Interface\")\n+    public void test32Interface_verifier() {\n+        int index = Math.abs(rI) % 3;\n+        try {\n+            test32Interface(testValue2Array, testValue1, index);\n+            throw new RuntimeException(\"No ArrayStoreException thrown\");\n+        } catch (ArrayStoreException e) {\n+            \/\/ Expected\n+        }\n+    }\n+\n+    \/\/ Test inline store to (flattened) inline type array disguised as abstract array\n+    @ForceInline\n+    public void test31Abstract_inline(MyAbstract[] ia, MyAbstract i, int index) {\n+        ia[index] = i;\n+    }\n+\n+    @Test\n+    public void test31Abstract(MyAbstract[] ia, MyAbstract i, int index) {\n+        test31Abstract_inline(ia, i, index);\n+    }\n+\n+    @Run(test = \"test31Abstract\")\n+    public void test31Abstract_verifier() {\n+        int index = Math.abs(rI) % 3;\n+        MyValue1 vt1 = MyValue1.createWithFieldsInline(rI + 1, rL + 1);\n+        test31Abstract(testValue1Array, vt1, index);\n+        Asserts.assertEQ(testValue1Array[index].hash(), vt1.hash());\n+        try {\n+            test31Abstract(testValue1Array, testValue2, index);\n+            throw new RuntimeException(\"No ArrayStoreException thrown\");\n+        } catch (ArrayStoreException e) {\n+            \/\/ Expected\n+        }\n+        Asserts.assertEQ(testValue1Array[index].hash(), vt1.hash());\n+        testValue1Array[index] = testValue1;\n+    }\n+\n+    @ForceInline\n+    public void test32Abstract_inline(MyAbstract[] ia, MyAbstract i, int index) {\n+        ia[index] = i;\n+    }\n+\n+    @Test\n+    public void test32Abstract(MyAbstract[] ia, MyAbstract i, int index) {\n+        test32Abstract_inline(ia, i, index);\n+    }\n+\n+    @Run(test = \"test32Abstract\")\n+    public void test32Abstract_verifier() {\n+        int index = Math.abs(rI) % 3;\n+        try {\n+            test32Abstract(testValue2Array, testValue1, index);\n+            throw new RuntimeException(\"No ArrayStoreException thrown\");\n+        } catch (ArrayStoreException e) {\n+            \/\/ Expected\n+        }\n+    }\n+\n+    \/\/ Test writing null to a (flattened) inline type array disguised as object array\n+    @ForceInline\n+    public void test33_inline(Object[] oa, Object o, int index) {\n+        oa[index] = o;\n+    }\n+\n+    @Test\n+    public void test33(Object[] oa, Object o, int index) {\n+        test33_inline(oa, o, index);\n+    }\n+\n+    @Run(test = \"test33\")\n+    public void test33_verifier() {\n+        int index = Math.abs(rI) % 3;\n+        try {\n+            test33(testValue1Array, null, index);\n+            throw new RuntimeException(\"No NPE thrown\");\n+        } catch (NullPointerException e) {\n+            \/\/ Expected\n+        }\n+        Asserts.assertEQ(testValue1Array[index].hash(), hash());\n+    }\n+\n+    \/\/ Test writing constant null to a (flattened) inline type array disguised as object array\n+\n+    @ForceInline\n+    public void test34_inline(Object[] oa, Object o, int index) {\n+        oa[index] = o;\n+    }\n+\n+    @Test\n+    public void test34(Object[] oa, int index) {\n+        test34_inline(oa, null, index);\n+    }\n+\n+    @Run(test = \"test34\")\n+    public void test34_verifier() {\n+        int index = Math.abs(rI) % 3;\n+        try {\n+            test34(testValue1Array, index);\n+            throw new RuntimeException(\"No NPE thrown\");\n+        } catch (NullPointerException e) {\n+            \/\/ Expected\n+        }\n+        Asserts.assertEQ(testValue1Array[index].hash(), hash());\n+    }\n+\n+    \/\/ Test writing constant null to a (flattened) inline type array\n+\n+    private static final MethodHandle setArrayElementNull = OldInstructionHelper.loadCode(MethodHandles.lookup(),\n+        \"setArrayElementNull\",\n+        MethodType.methodType(void.class, TestLWorld.class, MyValue1[].class, int.class),\n+        CODE -> {\n+            CODE.\n+            aload_1().\n+            iload_2().\n+            aconst_null().\n+            aastore().\n+            return_();\n+        });\n+\n+    @Test\n+    public void test35(MyValue1[] va, int index) throws Throwable {\n+        setArrayElementNull.invoke(this, va, index);\n+    }\n+\n+    @Run(test = \"test35\")\n+    @Warmup(10000)\n+    public void test35_verifier() throws Throwable {\n+        int index = Math.abs(rI) % 3;\n+        try {\n+            test35(testValue1Array, index);\n+            throw new RuntimeException(\"No NPE thrown\");\n+        } catch (NullPointerException e) {\n+            \/\/ Expected\n+        }\n+        Asserts.assertEQ(testValue1Array[index].hash(), hash());\n+    }\n+\n+    \/\/ Test writing an inline type to a null inline type array\n+    @Test\n+    public void test36(MyValue1[] va, MyValue1 vt, int index) {\n+        va[index] = vt;\n+    }\n+\n+    @Run(test = \"test36\")\n+    public void test36_verifier() {\n+        int index = Math.abs(rI) % 3;\n+        try {\n+            test36(null, testValue1Array[index], index);\n+            throw new RuntimeException(\"No NPE thrown\");\n+        } catch (NullPointerException e) {\n+            \/\/ Expected\n+        }\n+    }\n+\n+    \/\/ Test incremental inlining\n+    @ForceInline\n+    public void test37_inline(Object[] oa, Object o, int index) {\n+        oa[index] = o;\n+    }\n+\n+    @Test\n+    @IR(failOn = {ALLOC_G})\n+    public void test37(MyValue1[] va, Object o, int index) {\n+        test37_inline(va, o, index);\n+    }\n+\n+    @Run(test = \"test37\")\n+    public void test37_verifier() {\n+        int index = Math.abs(rI) % 3;\n+        MyValue1 vt1 = MyValue1.createWithFieldsInline(rI + 1, rL + 1);\n+        test37(testValue1Array, vt1, index);\n+        Asserts.assertEQ(testValue1Array[index].hash(), vt1.hash());\n+        try {\n+            test37(testValue1Array, testValue2, index);\n+            throw new RuntimeException(\"No ArrayStoreException thrown\");\n+        } catch (ArrayStoreException e) {\n+            \/\/ Expected\n+        }\n+        Asserts.assertEQ(testValue1Array[index].hash(), vt1.hash());\n+        testValue1Array[index] = testValue1;\n+    }\n+\n+    \/\/ Test merging of inline type arrays\n+\n+    @ForceInline\n+    public Object[] test38_inline() {\n+        return (MyValue1[])ValueClass.newNullRestrictedArray(MyValue1.class, 42);\n+    }\n+\n+    @Test\n+    @IR(failOn = {ALLOC})\n+    public Object[] test38(Object[] oa, Object o, int i1, int i2, int num) {\n+        Object[] result = null;\n+        switch (num) {\n+        case 0:\n+            result = test38_inline();\n+            break;\n+        case 1:\n+            result = oa;\n+            break;\n+        case 2:\n+            result = testValue1Array;\n+            break;\n+        case 3:\n+            result = testValue2Array;\n+            break;\n+        case 4:\n+            result = testNonValueArray;\n+            break;\n+        case 5:\n+            result = null;\n+            break;\n+        case 6:\n+            result = testValue1Array2;\n+            break;\n+        }\n+        result[i1] = result[i2];\n+        result[i2] = o;\n+        return result;\n+    }\n+\n+    @Run(test = \"test38\")\n+    public void test38_verifier() {\n+        int index = Math.abs(rI) % 3;\n+        MyValue1[] va = (MyValue1[])ValueClass.newNullRestrictedArray(MyValue1.class, 42);\n+        Object[] result = test38(null, testValue1, index, index, 0);\n+        Asserts.assertEQ(((MyValue1)result[index]).hash(), testValue1.hash());\n+        result = test38(testValue1Array, testValue1, index, index, 1);\n+        Asserts.assertEQ(((MyValue1)result[index]).hash(), testValue1.hash());\n+        result = test38(null, testValue1, index, index, 2);\n+        Asserts.assertEQ(((MyValue1)result[index]).hash(), testValue1.hash());\n+        result = test38(null, testValue2, index, index, 3);\n+        Asserts.assertEQ(((MyValue2)result[index]).hash(), testValue2.hash());\n+        try {\n+            result = test38(null, null, index, index, 3);\n+            throw new RuntimeException(\"No NPE thrown\");\n+        } catch (NullPointerException e) {\n+            \/\/ Expected\n+        }\n+        result = test38(null, null, index, index, 4);\n+        try {\n+            result = test38(null, testValue1, index, index, 4);\n+            throw new RuntimeException(\"No ArrayStoreException thrown\");\n+        } catch (ArrayStoreException e) {\n+            \/\/ Expected\n+        }\n+        try {\n+            result = test38(null, testValue1, index, index, 5);\n+            throw new RuntimeException(\"No NPE thrown\");\n+        } catch (NullPointerException e) {\n+            \/\/ Expected\n+        }\n+        result = test38(null, testValue1Array, index, index, 6);\n+        Asserts.assertEQ(((MyValue1[][])result)[index][index].hash(), testValue1.hash());\n+    }\n+\n+    @ForceInline\n+    public Object test39_inline() {\n+        return (MyValue1[])ValueClass.newNullRestrictedArray(MyValue1.class, 42);\n+    }\n+\n+    \/\/ Same as above but merging into Object instead of Object[]\n+    @Test\n+    public Object test39(Object oa, Object o, int i1, int i2, int num) {\n+        Object result = null;\n+        switch (num) {\n+        case 0:\n+            result = test39_inline();\n+            break;\n+        case 1:\n+            result = oa;\n+            break;\n+        case 2:\n+            result = testValue1Array;\n+            break;\n+        case 3:\n+            result = testValue2Array;\n+            break;\n+        case 4:\n+            result = testNonValueArray;\n+            break;\n+        case 5:\n+            result = null;\n+            break;\n+        case 6:\n+            result = testValue1;\n+            break;\n+        case 7:\n+            result = testValue2;\n+            break;\n+        case 8:\n+            result = MyValue1.createWithFieldsInline(rI, rL);\n+            break;\n+        case 9:\n+            result = new NonValueClass(42);\n+            break;\n+        case 10:\n+            result = testValue1Array2;\n+            break;\n+        }\n+        if (result instanceof Object[]) {\n+            ((Object[])result)[i1] = ((Object[])result)[i2];\n+            ((Object[])result)[i2] = o;\n+        }\n+        return result;\n+    }\n+\n+    @Run(test = \"test39\")\n+    public void test39_verifier() {\n+        int index = Math.abs(rI) % 3;\n+        MyValue1[] va = (MyValue1[])ValueClass.newNullRestrictedArray(MyValue1.class, 42);\n+        Object result = test39(null, testValue1, index, index, 0);\n+        Asserts.assertEQ(((MyValue1[])result)[index].hash(), testValue1.hash());\n+        result = test39(testValue1Array, testValue1, index, index, 1);\n+        Asserts.assertEQ(((MyValue1[])result)[index].hash(), testValue1.hash());\n+        result = test39(null, testValue1, index, index, 2);\n+        Asserts.assertEQ(((MyValue1[])result)[index].hash(), testValue1.hash());\n+        result = test39(null, testValue2, index, index, 3);\n+        Asserts.assertEQ(((MyValue2[])result)[index].hash(), testValue2.hash());\n+        try {\n+            result = test39(null, null, index, index, 3);\n+            throw new RuntimeException(\"No NPE thrown\");\n+        } catch (NullPointerException e) {\n+            \/\/ Expected\n+        }\n+        result = test39(null, null, index, index, 4);\n+        try {\n+            result = test39(null, testValue1, index, index, 4);\n+            throw new RuntimeException(\"No ArrayStoreException thrown\");\n+        } catch (ArrayStoreException e) {\n+            \/\/ Expected\n+        }\n+        result = test39(null, testValue1, index, index, 5);\n+        Asserts.assertEQ(result, null);\n+        result = test39(null, testValue1, index, index, 6);\n+        Asserts.assertEQ(((MyValue1)result).hash(), testValue1.hash());\n+        result = test39(null, testValue1, index, index, 7);\n+        Asserts.assertEQ(((MyValue2)result).hash(), testValue2.hash());\n+        result = test39(null, testValue1, index, index, 8);\n+        Asserts.assertEQ(((MyValue1)result).hash(), testValue1.hash());\n+        result = test39(null, testValue1, index, index, 9);\n+        Asserts.assertEQ(((NonValueClass)result).x, 42);\n+        result = test39(null, testValue1Array, index, index, 10);\n+        Asserts.assertEQ(((MyValue1[][])result)[index][index].hash(), testValue1.hash());\n+    }\n+\n+    \/\/ Test instanceof with inline types and arrays\n+    @Test\n+    @IR(applyIf = {\"InlineTypePassFieldsAsArgs\", \"true\"},\n+        failOn = {ALLOC_G})\n+    public long test40(Object o, int index) {\n+        if (o instanceof MyValue1) {\n+          return ((MyValue1)o).hashInterpreted();\n+        } else if (o instanceof MyValue1[]) {\n+          return ((MyValue1[])o)[index].hashInterpreted();\n+        } else if (o instanceof MyValue2) {\n+          return ((MyValue2)o).hash();\n+        } else if (o instanceof MyValue2[]) {\n+          return ((MyValue2[])o)[index].hash();\n+        } else if (o instanceof MyValue1[][]) {\n+          return ((MyValue1[][])o)[index][index].hash();\n+        } else if (o instanceof Long) {\n+          return (long)o;\n+        }\n+        return 0;\n+    }\n+\n+    @Run(test = \"test40\")\n+    public void test40_verifier() {\n+        int index = Math.abs(rI) % 3;\n+        long result = test40(testValue1, 0);\n+        Asserts.assertEQ(result, testValue1.hashInterpreted());\n+        result = test40(testValue1Array, index);\n+        Asserts.assertEQ(result, testValue1.hashInterpreted());\n+        result = test40(testValue2, index);\n+        Asserts.assertEQ(result, testValue2.hash());\n+        result = test40(testValue2Array, index);\n+        Asserts.assertEQ(result, testValue2.hash());\n+        result = test40(testValue1Array2, index);\n+        Asserts.assertEQ(result, testValue1.hash());\n+        result = test40(Long.valueOf(42), index);\n+        Asserts.assertEQ(result, 42L);\n+    }\n+\n+    \/\/ Test for bug in Escape Analysis\n+    @DontInline\n+    public void test41_dontinline(Object o) {\n+        Asserts.assertEQ(o, rI);\n+    }\n+\n+    @Test\n+    @IR(failOn = {ALLOC})\n+    public void test41() {\n+        MyValue1[] vals = (MyValue1[])ValueClass.newNullRestrictedArray(MyValue1.class, 1);\n+        vals[0] = testValue1;\n+        test41_dontinline(vals[0].oa[0]);\n+        test41_dontinline(vals[0].oa[0]);\n+    }\n+\n+    @Run(test = \"test41\")\n+    public void test41_verifier() {\n+        test41();\n+    }\n+\n+    \/\/ Test for bug in Escape Analysis\n+    private static final MyValue1 test42VT1 = MyValue1.createWithFieldsInline(rI, rL);\n+    private static final MyValue1 test42VT2 = MyValue1.createWithFieldsInline(rI + 1, rL + 1);\n+\n+    @Test\n+    @IR(failOn = {ALLOC})\n+    public void test42() {\n+        MyValue1[] vals = (MyValue1[])ValueClass.newNullRestrictedArray(MyValue1.class, 2);\n+        vals[0] = test42VT1;\n+        vals[1] = test42VT2;\n+        Asserts.assertEQ(vals[0].hash(), test42VT1.hash());\n+        Asserts.assertEQ(vals[1].hash(), test42VT2.hash());\n+    }\n+\n+    @Run(test = \"test42\")\n+    public void test42_verifier(RunInfo info) {\n+        if (!info.isWarmUp()) test42(); \/\/ We need -Xcomp behavior\n+    }\n+\n+    \/\/ Test for bug in Escape Analysis\n+    @Test\n+    @IR(failOn = {ALLOC})\n+    public long test43(boolean deopt, Method m) {\n+        MyValue1[] vals = (MyValue1[])ValueClass.newNullRestrictedArray(MyValue1.class, 2);\n+        vals[0] = test42VT1;\n+        vals[1] = test42VT2;\n+\n+        if (deopt) {\n+            \/\/ uncommon trap\n+            TestFramework.deoptimize(m);\n+            Asserts.assertEQ(vals[0].hash(), test42VT1.hash());\n+            Asserts.assertEQ(vals[1].hash(), test42VT2.hash());\n+        }\n+\n+        return vals[0].hash();\n+    }\n+\n+    @Run(test = \"test43\")\n+    public void test43_verifier(RunInfo info) {\n+        test43(!info.isWarmUp(), info.getTest());\n+    }\n+\n+    \/\/ Tests writing an array element with a (statically known) incompatible type\n+    private static final MethodHandle setArrayElementIncompatible = OldInstructionHelper.loadCode(MethodHandles.lookup(),\n+        \"setArrayElementIncompatible\",\n+        MethodType.methodType(void.class, TestLWorld.class, MyValue1[].class, int.class, MyValue2.class),\n+        CODE -> {\n+            CODE.\n+            aload_1().\n+            iload_2().\n+            aload_3().\n+            aastore().\n+            return_();\n+        });\n+\n+    @Test\n+    public void test44(MyValue1[] va, int index, MyValue2 v) throws Throwable {\n+        setArrayElementIncompatible.invoke(this, va, index, v);\n+    }\n+\n+    @Run(test = \"test44\")\n+    @Warmup(10000)\n+    public void test44_verifier() throws Throwable {\n+        int index = Math.abs(rI) % 3;\n+        try {\n+            test44(testValue1Array, index, testValue2);\n+            throw new RuntimeException(\"No ArrayStoreException thrown\");\n+        } catch (ArrayStoreException e) {\n+            \/\/ Expected\n+        }\n+        Asserts.assertEQ(testValue1Array[index].hash(), hash());\n+    }\n+\n+    \/\/ Tests writing an array element with a (statically known) incompatible type\n+    @ForceInline\n+    public void test45_inline(Object[] oa, Object o, int index) {\n+        oa[index] = o;\n+    }\n+\n+    @Test\n+    public void test45(MyValue1[] va, int index, MyValue2 v) throws Throwable {\n+        test45_inline(va, v, index);\n+    }\n+\n+    @Run(test = \"test45\")\n+    public void test45_verifier() throws Throwable {\n+        int index = Math.abs(rI) % 3;\n+        try {\n+            test45(testValue1Array, index, testValue2);\n+            throw new RuntimeException(\"No ArrayStoreException thrown\");\n+        } catch (ArrayStoreException e) {\n+            \/\/ Expected\n+        }\n+        Asserts.assertEQ(testValue1Array[index].hash(), hash());\n+    }\n+\n+    \/\/ instanceof tests with inline types\n+    @Test\n+    @IR(failOn = {ALLOC_G})\n+    public boolean test46(MyValue1 vt) {\n+        Object obj = vt;\n+        return obj instanceof MyValue1;\n+    }\n+\n+    @Run(test = \"test46\")\n+    public void test46_verifier() {\n+        MyValue1 vt = testValue1;\n+        boolean result = test46(vt);\n+        Asserts.assertTrue(result);\n+    }\n+\n+    @Test\n+    @IR(failOn = {ALLOC_G})\n+    public boolean test47(MyValue1 vt) {\n+        Object obj = vt;\n+        return obj instanceof MyValue2;\n+    }\n+\n+    @Run(test = \"test47\")\n+    public void test47_verifier() {\n+        MyValue1 vt = testValue1;\n+        boolean result = test47(vt);\n+        Asserts.assertFalse(result);\n+    }\n+\n+    @Test\n+    @IR(failOn = {ALLOC_G})\n+    public boolean test48(Object obj) {\n+        return obj instanceof MyValue1;\n+    }\n+\n+    @Run(test = \"test48\")\n+    public void test48_verifier() {\n+        MyValue1 vt = testValue1;\n+        boolean result = test48(vt);\n+        Asserts.assertTrue(result);\n+    }\n+\n+    @Test\n+    @IR(failOn = {ALLOC_G})\n+    public boolean test49(Object obj) {\n+        return obj instanceof MyValue2;\n+    }\n+\n+    @Run(test = \"test49\")\n+    public void test49_verifier() {\n+        MyValue1 vt = testValue1;\n+        boolean result = test49(vt);\n+        Asserts.assertFalse(result);\n+    }\n+\n+    @Test\n+    @IR(failOn = {ALLOC_G})\n+    public boolean test50(Object obj) {\n+        return obj instanceof MyValue1;\n+    }\n+\n+    @Run(test = \"test50\")\n+    public void test50_verifier() {\n+        Asserts.assertFalse(test49(new NonValueClass(42)));\n+    }\n+\n+    \/\/ Inline type with some non-flattened fields\n+    @ImplicitlyConstructible\n+    @LooselyConsistentValue\n+    value class Test51Value {\n+        Object objectField1;\n+        Object objectField2;\n+        Object objectField3;\n+        Object objectField4;\n+        Object objectField5;\n+        Object objectField6;\n+\n+        @NullRestricted\n+        MyValue1 valueField1;\n+        @NullRestricted\n+        MyValue1 valueField2;\n+        MyValue1 valueField3;\n+        @NullRestricted\n+        MyValue1 valueField4;\n+        MyValue1 valueField5;\n+\n+        public Test51Value() {\n+            objectField1 = null;\n+            objectField2 = null;\n+            objectField3 = null;\n+            objectField4 = null;\n+            objectField5 = null;\n+            objectField6 = null;\n+            valueField1 = testValue1;\n+            valueField2 = testValue1;\n+            valueField3 = testValue1;\n+            valueField4 = MyValue1.createDefaultDontInline();\n+            valueField5 = MyValue1.createDefaultDontInline();\n+        }\n+\n+        public Test51Value(Object o1, Object o2, Object o3, Object o4, Object o5, Object o6,\n+                           MyValue1 vt1, MyValue1 vt2, MyValue1 vt3, MyValue1 vt4, MyValue1 vt5) {\n+            objectField1 = o1;\n+            objectField2 = o2;\n+            objectField3 = o3;\n+            objectField4 = o4;\n+            objectField5 = o5;\n+            objectField6 = o6;\n+            valueField1 = vt1;\n+            valueField2 = vt2;\n+            valueField3 = vt3;\n+            valueField4 = vt4;\n+            valueField5 = vt5;\n+        }\n+\n+        @ForceInline\n+        public long test(Test51Value holder, MyValue1 vt1, Object vt2) {\n+            holder = new Test51Value(vt1, holder.objectField2, holder.objectField3, holder.objectField4, holder.objectField5, holder.objectField6,\n+                                     holder.valueField1, holder.valueField2, holder.valueField3, holder.valueField4, holder.valueField5);\n+            holder = new Test51Value(holder.objectField1, (MyValue1)vt2, holder.objectField3, holder.objectField4, holder.objectField5, holder.objectField6,\n+                                     holder.valueField1, holder.valueField2, holder.valueField3, holder.valueField4, holder.valueField5);\n+            holder = new Test51Value(holder.objectField1, holder.objectField2, testValue1, holder.objectField4, holder.objectField5, holder.objectField6,\n+                                     holder.valueField1, holder.valueField2, holder.valueField3, holder.valueField4, holder.valueField5);\n+            holder = new Test51Value(holder.objectField1, holder.objectField2, holder.objectField3, MyValue1.createWithFieldsDontInline(rI, rL), holder.objectField5, holder.objectField6,\n+                                     holder.valueField1, holder.valueField2, holder.valueField3, holder.valueField4, holder.valueField5);\n+            holder = new Test51Value(holder.objectField1, holder.objectField2, holder.objectField3, holder.objectField4, holder.valueField1, holder.objectField6,\n+                                     holder.valueField1, holder.valueField2, holder.valueField3, holder.valueField4, holder.valueField5);\n+            holder = new Test51Value(holder.objectField1, holder.objectField2, holder.objectField3, holder.objectField4, holder.objectField5, holder.valueField3,\n+                                     holder.valueField1, holder.valueField2, holder.valueField3, holder.valueField4, holder.valueField5);\n+            holder = new Test51Value(holder.objectField1, holder.objectField2, holder.objectField3, holder.objectField4, holder.objectField5, holder.objectField6,\n+                                     (MyValue1)holder.objectField1, holder.valueField2, holder.valueField3, holder.valueField4, holder.valueField5);\n+            holder = new Test51Value(holder.objectField1, holder.objectField2, holder.objectField3, holder.objectField4, holder.objectField5, holder.objectField6,\n+                                     holder.valueField1, (MyValue1)vt2, holder.valueField3, holder.valueField4, holder.valueField5);\n+            holder = new Test51Value(holder.objectField1, holder.objectField2, holder.objectField3, holder.objectField4, holder.objectField5, holder.objectField6,\n+                                     holder.valueField1, holder.valueField2, (MyValue1)vt2, holder.valueField4, holder.valueField5);\n+\n+            return ((MyValue1)holder.objectField1).hash() +\n+                   ((MyValue1)holder.objectField2).hash() +\n+                   ((MyValue1)holder.objectField3).hash() +\n+                   ((MyValue1)holder.objectField4).hash() +\n+                   ((MyValue1)holder.objectField5).hash() +\n+                   ((MyValue1)holder.objectField6).hash() +\n+                   holder.valueField1.hash() +\n+                   holder.valueField2.hash() +\n+                   holder.valueField3.hash() +\n+                   holder.valueField4.hashPrimitive();\n+        }\n+    }\n+\n+    \/\/ Pass arguments via fields to avoid exzessive spilling leading to compilation bailouts\n+    @NullRestricted\n+    static Test51Value test51_arg1;\n+    @NullRestricted\n+    static MyValue1 test51_arg2;\n+    static Object test51_arg3;\n+\n+    \/\/ Same as test2 but with field holder being an inline type\n+    @Test\n+    public long test51() {\n+        return test51_arg1.test(test51_arg1, test51_arg2, test51_arg3);\n+    }\n+\n+    @Run(test = \"test51\")\n+    public void test51_verifier() {\n+        MyValue1 vt = testValue1;\n+        MyValue1 def = MyValue1.createDefaultDontInline();\n+        Test51Value holder = new Test51Value();\n+        Asserts.assertEQ(testValue1.hash(), vt.hash());\n+        Asserts.assertEQ(holder.valueField1.hash(), vt.hash());\n+        test51_arg1 = holder;\n+        test51_arg2 = vt;\n+        test51_arg3 = vt;\n+        long result = test51();\n+        Asserts.assertEQ(result, 9*vt.hash() + def.hashPrimitive());\n+    }\n+\n+    \/\/ Access non-flattened, uninitialized inline type field with inline type holder\n+    @Test\n+    @IR(failOn = {ALLOC})\n+    public void test52(Test51Value holder) {\n+        if ((Object)holder.valueField5 != null) {\n+            throw new RuntimeException(\"Should be null\");\n+        }\n+    }\n+\n+    @Run(test = \"test52\")\n+    public void test52_verifier() {\n+        Test51Value vt = new Test51Value(null, null, null, null, null, null,\n+                                         MyValue1.createDefaultInline(), MyValue1.createDefaultInline(), null, MyValue1.createDefaultInline(), null);\n+        test52(vt);\n+    }\n+\n+    \/\/ Merging inline types of different types\n+    @Test\n+    public Object test53(Object o, boolean b) {\n+        MyValue1 vt = MyValue1.createWithFieldsInline(rI, rL);\n+        return b ? vt : o;\n+    }\n+\n+    @Run(test = \"test53\")\n+    public void test53_verifier() {\n+        test53(new Object(), false);\n+        MyValue1 result = (MyValue1)test53(new Object(), true);\n+        Asserts.assertEQ(result.hash(), hash());\n+    }\n+\n+    @Test\n+    public Object test54(boolean b) {\n+        MyValue1 vt = MyValue1.createWithFieldsInline(rI, rL);\n+        return b ? vt : testValue2;\n+    }\n+\n+    @Run(test = \"test54\")\n+    public void test54_verifier() {\n+        MyValue1 result1 = (MyValue1)test54(true);\n+        Asserts.assertEQ(result1.hash(), hash());\n+        MyValue2 result2 = (MyValue2)test54(false);\n+        Asserts.assertEQ(result2.hash(), testValue2.hash());\n+    }\n+\n+    @Test\n+    public Object test55(boolean b) {\n+        MyValue1 vt1 = MyValue1.createWithFieldsInline(rI, rL);\n+        MyValue2 vt2 = MyValue2.createWithFieldsInline(rI, rD);\n+        return b ? vt1 : vt2;\n+    }\n+\n+    @Run(test = \"test55\")\n+    public void test55_verifier() {\n+        MyValue1 result1 = (MyValue1)test55(true);\n+        Asserts.assertEQ(result1.hash(), hash());\n+        MyValue2 result2 = (MyValue2)test55(false);\n+        Asserts.assertEQ(result2.hash(), testValue2.hash());\n+    }\n+\n+    \/\/ Test synchronization on inline types\n+    @Test\n+    public void test56(Object vt) {\n+        synchronized (vt) {\n+            throw new RuntimeException(\"test56 failed: synchronization on inline type should not succeed\");\n+        }\n+    }\n+\n+    @Run(test = \"test56\")\n+    public void test56_verifier() {\n+        try {\n+            test56(testValue1);\n+            throw new RuntimeException(\"test56 failed: no exception thrown\");\n+        } catch (IdentityException ex) {\n+            \/\/ Expected\n+        }\n+    }\n+\n+    @ForceInline\n+    public void test57_inline(Object vt) {\n+        synchronized (vt) {\n+            throw new RuntimeException(\"test57 failed: synchronization on inline type should not succeed\");\n+        }\n+    }\n+\n+    @Test\n+    @IR(failOn = {ALLOC_G})\n+    public void test57(MyValue1 vt) {\n+        test57_inline(vt);\n+    }\n+\n+    @Run(test = \"test57\")\n+    public void test57_verifier() {\n+        try {\n+            test57(testValue1);\n+            throw new RuntimeException(\"test57 failed: no exception thrown\");\n+        } catch (IdentityException ex) {\n+            \/\/ Expected\n+        }\n+    }\n+\n+    @ForceInline\n+    public void test58_inline(Object vt) {\n+        synchronized (vt) {\n+            throw new RuntimeException(\"test58 failed: synchronization on inline type should not succeed\");\n+        }\n+    }\n+\n+    @Test\n+    @IR(failOn = {ALLOC})\n+    public void test58() {\n+        MyValue1 vt = MyValue1.createWithFieldsInline(rI, rL);\n+        test58_inline(vt);\n+    }\n+\n+    @Run(test = \"test58\")\n+    public void test58_verifier() {\n+        try {\n+            test58();\n+            throw new RuntimeException(\"test58 failed: no exception thrown\");\n+        } catch (IdentityException ex) {\n+            \/\/ Expected\n+        }\n+    }\n+\n+    @Test\n+    public void test59(Object o, boolean b) {\n+        MyValue1 vt = MyValue1.createWithFieldsInline(rI, rL);\n+        Object sync = b ? vt : o;\n+        synchronized (sync) {\n+            if (b) {\n+                throw new RuntimeException(\"test59 failed: synchronization on inline type should not succeed\");\n+            }\n+        }\n+    }\n+\n+    @Run(test = \"test59\")\n+    public void test59_verifier() {\n+        test59(new Object(), false);\n+        try {\n+            test59(new Object(), true);\n+            throw new RuntimeException(\"test59 failed: no exception thrown\");\n+        } catch (IdentityException ex) {\n+            \/\/ Expected\n+        }\n+    }\n+\n+    @Test\n+    public void test60(boolean b) {\n+        MyValue1 vt = MyValue1.createWithFieldsInline(rI, rL);\n+        Object sync = b ? vt : testValue2;\n+        synchronized (sync) {\n+            throw new RuntimeException(\"test60 failed: synchronization on inline type should not succeed\");\n+        }\n+    }\n+\n+    @Run(test = \"test60\")\n+    public void test60_verifier() {\n+        try {\n+            test60(false);\n+            throw new RuntimeException(\"test60 failed: no exception thrown\");\n+        } catch (IdentityException ex) {\n+            \/\/ Expected\n+        }\n+        try {\n+            test60(true);\n+            throw new RuntimeException(\"test60 failed: no exception thrown\");\n+        } catch (IdentityException ex) {\n+            \/\/ Expected\n+        }\n+    }\n+\n+    \/\/ Test catching the IdentityException in compiled code\n+    @Test\n+    public void test61(Object vt) {\n+        boolean thrown = false;\n+        try {\n+            synchronized (vt) {\n+                throw new RuntimeException(\"test61 failed: no exception thrown\");\n+            }\n+        } catch (IdentityException ex) {\n+            thrown = true;\n+        }\n+        if (!thrown) {\n+            throw new RuntimeException(\"test61 failed: no exception thrown\");\n+        }\n+    }\n+\n+    @Run(test = \"test61\")\n+    public void test61_verifier() {\n+        test61(testValue1);\n+    }\n+\n+    @Test\n+    public void test62(Object o) {\n+        try {\n+            synchronized (o) { }\n+        } catch (IdentityException ex) {\n+            \/\/ Expected\n+            return;\n+        }\n+        throw new RuntimeException(\"test62 failed: no exception thrown\");\n+    }\n+\n+    @Run(test = \"test62\")\n+    public void test62_verifier() {\n+        test62(testValue1);\n+    }\n+\n+    \/\/ Test synchronization without any instructions in the synchronized block\n+    @Test\n+    public void test63(Object o) {\n+        synchronized (o) { }\n+    }\n+\n+    @Run(test = \"test63\")\n+    public void test63_verifier() {\n+        try {\n+            test63(testValue1);\n+        } catch (IdentityException ex) {\n+            \/\/ Expected\n+            return;\n+        }\n+        throw new RuntimeException(\"test63 failed: no exception thrown\");\n+    }\n+\n+    \/\/ type system test with interface and inline type\n+    @ForceInline\n+    public MyInterface test64Interface_helper(MyValue1 vt) {\n+        return vt;\n+    }\n+\n+    @Test\n+    public MyInterface test64Interface(MyValue1 vt) {\n+        return test64Interface_helper(vt);\n+    }\n+\n+    @Run(test = \"test64Interface\")\n+    public void test64Interface_verifier() {\n+        test64Interface(testValue1);\n+    }\n+\n+    \/\/ type system test with abstract and inline type\n+    @ForceInline\n+    public MyAbstract test64Abstract_helper(MyValue1 vt) {\n+        return vt;\n+    }\n+\n+    @Test\n+    public MyAbstract test64Abstract(MyValue1 vt) {\n+        return test64Abstract_helper(vt);\n+    }\n+\n+    @Run(test = \"test64Abstract\")\n+    public void test64Abstract_verifier() {\n+        test64Abstract(testValue1);\n+    }\n+\n+    \/\/ Array store tests\n+    @Test\n+    public void test65(Object[] array, MyValue1 vt) {\n+        array[0] = vt;\n+    }\n+\n+    @Run(test = \"test65\")\n+    public void test65_verifier() {\n+        Object[] array = new Object[1];\n+        test65(array, testValue1);\n+        Asserts.assertEQ(((MyValue1)array[0]).hash(), testValue1.hash());\n+    }\n+\n+    @Test\n+    public void test66(Object[] array, MyValue1 vt) {\n+        array[0] = vt;\n+    }\n+\n+    @Run(test = \"test66\")\n+    public void test66_verifier() {\n+        MyValue1[] array = (MyValue1[])ValueClass.newNullRestrictedArray(MyValue1.class, 1);\n+        test66(array, testValue1);\n+        Asserts.assertEQ(array[0].hash(), testValue1.hash());\n+    }\n+\n+    @Test\n+    public void test67(Object[] array, Object vt) {\n+        array[0] = vt;\n+    }\n+\n+    @Run(test = \"test67\")\n+    public void test67_verifier() {\n+        MyValue1[] array = (MyValue1[])ValueClass.newNullRestrictedArray(MyValue1.class, 1);\n+        test67(array, testValue1);\n+        Asserts.assertEQ(array[0].hash(), testValue1.hash());\n+    }\n+\n+    @Test\n+    public void test68(Object[] array, NonValueClass o) {\n+        array[0] = o;\n+    }\n+\n+    @Run(test = \"test68\")\n+    public void test68_verifier() {\n+        NonValueClass[] array = new NonValueClass[1];\n+        NonValueClass obj = new NonValueClass(1);\n+        test68(array, obj);\n+        Asserts.assertEQ(array[0], obj);\n+    }\n+\n+    \/\/ Test convertion between an inline type and java.lang.Object without an allocation\n+    @ForceInline\n+    public Object test69_sum(Object a, Object b) {\n+        int sum = ((MyValue1)a).x + ((MyValue1)b).x;\n+        return MyValue1.setX(((MyValue1)a), sum);\n+    }\n+\n+    @Test\n+    @IR(failOn = {ALLOC_G, STORE})\n+    public int test69(MyValue1[] array) {\n+        MyValue1 result = MyValue1.createDefaultInline();\n+        for (int i = 0; i < array.length; ++i) {\n+            result = (MyValue1)test69_sum(result, array[i]);\n+        }\n+        return result.x;\n+    }\n+\n+    @Run(test = \"test69\")\n+    public void test69_verifier() {\n+        int result = test69(testValue1Array);\n+        Asserts.assertEQ(result, rI * testValue1Array.length);\n+    }\n+\n+    \/\/ Same as test69 but with an Interface\n+    @ForceInline\n+    public MyInterface test70Interface_sum(MyInterface a, MyInterface b) {\n+        int sum = ((MyValue1)a).x + ((MyValue1)b).x;\n+        return MyValue1.setX(((MyValue1)a), sum);\n+    }\n+\n+    @Test\n+    @IR(failOn = {ALLOC_G, STORE})\n+    public int test70Interface(MyValue1[] array) {\n+        MyValue1 result = MyValue1.createDefaultInline();\n+        for (int i = 0; i < array.length; ++i) {\n+            result = (MyValue1)test70Interface_sum(result, array[i]);\n+        }\n+        return result.x;\n+    }\n+\n+    @Run(test = \"test70Interface\")\n+    public void test70Interface_verifier() {\n+        int result = test70Interface(testValue1Array);\n+        Asserts.assertEQ(result, rI * testValue1Array.length);\n+    }\n+\n+    \/\/ Same as test69 but with an Abstract\n+    @ForceInline\n+    public MyAbstract test70Abstract_sum(MyAbstract a, MyAbstract b) {\n+        int sum = ((MyValue1)a).x + ((MyValue1)b).x;\n+        return MyValue1.setX(((MyValue1)a), sum);\n+    }\n+\n+    @Test\n+    @IR(failOn = {ALLOC_G, STORE})\n+    public int test70Abstract(MyValue1[] array) {\n+        MyValue1 result = MyValue1.createDefaultInline();\n+        for (int i = 0; i < array.length; ++i) {\n+            result = (MyValue1)test70Abstract_sum(result, array[i]);\n+        }\n+        return result.x;\n+    }\n+\n+    @Run(test = \"test70Abstract\")\n+    public void test70Abstract_verifier() {\n+        int result = test70Abstract(testValue1Array);\n+        Asserts.assertEQ(result, rI * testValue1Array.length);\n+    }\n+\n+    \/\/ Test that allocated inline type is not used in non-dominated path\n+    public MyValue1 test71_inline(Object obj) {\n+        MyValue1 vt = MyValue1.createWithFieldsInline(rI, rL);\n+        try {\n+            vt = (MyValue1)Objects.requireNonNull(obj);\n+            throw new RuntimeException(\"NullPointerException expected\");\n+        } catch (NullPointerException e) {\n+            \/\/ Expected\n+        }\n+        return vt;\n+    }\n+\n+    @Test\n+    @IR(failOn = {ALLOC})\n+    public MyValue1 test71() {\n+        return test71_inline(null);\n+    }\n+\n+    @Run(test = \"test71\")\n+    public void test71_verifier() {\n+        MyValue1 vt = test71();\n+        Asserts.assertEquals(vt.hash(), hash());\n+    }\n+\n+    \/\/ Test calling a method on an uninitialized inline type\n+    @ImplicitlyConstructible\n+    @LooselyConsistentValue\n+    value class Test72Value {\n+        int x = 0;\n+\n+        public int get() {\n+            return x;\n+        }\n+    }\n+\n+    \/\/ Make sure Test72Value is loaded but not initialized\n+    public void unused(Test72Value vt) { }\n+\n+    @Test\n+    @IR(failOn = {ALLOC_G})\n+    public int test72() {\n+        Test72Value vt = new Test72Value();\n+        return vt.get();\n+    }\n+\n+    @Run(test = \"test72\")\n+    @Warmup(0)\n+    public void test72_verifier() {\n+        int result = test72();\n+        Asserts.assertEquals(result, 0);\n+    }\n+\n+    \/\/ Tests for loading\/storing unkown values\n+    @Test\n+    public Object test73(Object[] va) {\n+        return va[0];\n+    }\n+\n+    @Run(test = \"test73\")\n+    public void test73_verifier() {\n+        MyValue1 vt = (MyValue1)test73(testValue1Array);\n+        Asserts.assertEquals(testValue1Array[0].hash(), vt.hash());\n+    }\n+\n+    @Test\n+    public void test74(Object[] va, Object vt) {\n+        va[0] = vt;\n+    }\n+\n+    @Run(test = \"test74\")\n+    public void test74_verifier() {\n+        MyValue1[] va = (MyValue1[])ValueClass.newNullRestrictedArray(MyValue1.class, 1);\n+        test74(va, testValue1);\n+        Asserts.assertEquals(va[0].hash(), testValue1.hash());\n+    }\n+\n+    \/\/ Verify that mixing instances and arrays with the clone api\n+    \/\/ doesn't break anything\n+    @Test\n+    @IR(failOn = {ALLOC_G})\n+    public Object test75(Object o) {\n+        MyValue1[] va = (MyValue1[])ValueClass.newNullRestrictedArray(MyValue1.class, 1);\n+        Object[] next = va;\n+        Object[] arr = va;\n+        for (int i = 0; i < 10; i++) {\n+            arr = next;\n+            next = new NonValueClass[1];\n+        }\n+        return arr[0];\n+    }\n+\n+    @Run(test = \"test75\")\n+    public void test75_verifier() {\n+        test75(42);\n+    }\n+\n+    \/\/ Casting an NonValueClass to a inline type should throw a ClassCastException\n+    @ForceInline\n+    public MyValue1 test77_helper(Object o) {\n+        return (MyValue1)o;\n+    }\n+\n+    @Test\n+    @IR(failOn = {ALLOC_G})\n+    public MyValue1 test77(NonValueClass obj) throws Throwable {\n+        return test77_helper(obj);\n+    }\n+\n+    @Run(test = \"test77\")\n+    public void test77_verifier() throws Throwable {\n+        try {\n+            test77(new NonValueClass(42));\n+            throw new RuntimeException(\"ClassCastException expected\");\n+        } catch (ClassCastException e) {\n+            \/\/ Expected\n+        } catch (Exception e) {\n+            throw new RuntimeException(\"test77 failed: unexpected exception\", e);\n+        }\n+    }\n+\n+    \/\/ Casting a null NonValueClass to a nullable inline type should not throw\n+    @ForceInline\n+    public MyValue1 test78_helper(Object o) {\n+        return (MyValue1)o;\n+    }\n+\n+    @Test\n+    @IR(failOn = {ALLOC_G})\n+    public MyValue1 test78(NonValueClass obj) throws Throwable {\n+        return test78_helper(obj);\n+    }\n+\n+    @Run(test = \"test78\")\n+    public void test78_verifier() throws Throwable {\n+        try {\n+            test78(null); \/\/ Should not throw\n+        } catch (Exception e) {\n+            throw new RuntimeException(\"test78 failed: unexpected exception\", e);\n+        }\n+    }\n+\n+    \/\/ Casting an NonValueClass to a nullable inline type should throw a ClassCastException\n+    @ForceInline\n+    public MyValue1 test79_helper(Object o) {\n+        return (MyValue1)o;\n+    }\n+\n+    @Test\n+    @IR(failOn = {ALLOC_G})\n+    public MyValue1 test79(NonValueClass obj) throws Throwable {\n+        return test79_helper(obj);\n+    }\n+\n+    @Run(test = \"test79\")\n+    public void test79_verifier() throws Throwable {\n+        try {\n+            test79(new NonValueClass(42));\n+            throw new RuntimeException(\"ClassCastException expected\");\n+        } catch (ClassCastException e) {\n+            \/\/ Expected\n+        } catch (Exception e) {\n+            throw new RuntimeException(\"test79 failed: unexpected exception\", e);\n+        }\n+    }\n+\n+    \/\/ Test flattened field with non-flattenend (but flattenable) inline type field\n+    @ImplicitlyConstructible\n+    @LooselyConsistentValue\n+    static value class Small {\n+        int i;\n+        @NullRestricted\n+        Big big; \/\/ Too big to be flattened\n+\n+        private Small() {\n+            i = rI;\n+            big = new Big();\n+        }\n+    }\n+\n+    @ImplicitlyConstructible\n+    @LooselyConsistentValue\n+    static value class Big {\n+        long l0,l1,l2,l3,l4,l5,l6,l7,l8,l9;\n+        long l10,l11,l12,l13,l14,l15,l16,l17,l18,l19;\n+        long l20,l21,l22,l23,l24,l25,l26,l27,l28,l29;\n+\n+        private Big() {\n+            l0 = l1 = l2 = l3 = l4 = l5 = l6 = l7 = l8 = l9 = rL;\n+            l10 = l11 = l12 = l13 = l14 = l15 = l16 = l17 = l18 = l19 = rL+1;\n+            l20 = l21 = l22 = l23 = l24 = l25 = l26 = l27 = l28 = l29 = rL+2;\n+        }\n+    }\n+\n+    @NullRestricted\n+    Small small = new Small();\n+    @NullRestricted\n+    Small smallDefault;\n+    @NullRestricted\n+    Big big = new Big();\n+    @NullRestricted\n+    Big bigDefault;\n+\n+    @Test\n+    public long test80() {\n+        return small.i + small.big.l0 + smallDefault.i + smallDefault.big.l29 + big.l0 + bigDefault.l29;\n+    }\n+\n+    @Run(test = \"test80\")\n+    public void test80_verifier() throws Throwable {\n+        long result = test80();\n+        Asserts.assertEQ(result, rI + 2*rL);\n+    }\n+\n+    \/\/ Test scalarization with exceptional control flow\n+    public int test81Callee(MyValue1 vt)  {\n+        return vt.x;\n+    }\n+\n+    @Test\n+    @IR(failOn = {ALLOC_G, LOAD, STORE})\n+    public int test81()  {\n+        MyValue1 vt = MyValue1.createWithFieldsInline(rI, rL);\n+        int result = 0;\n+        for (int i = 0; i < 10; i++) {\n+            try {\n+                result += test81Callee(vt);\n+            } catch (NullPointerException npe) {\n+                result += rI;\n+            }\n+        }\n+        return result;\n+    }\n+\n+    @Run(test = \"test81\")\n+    public void test81_verifier() {\n+        int result = test81();\n+        Asserts.assertEQ(result, 10*rI);\n+    }\n+\n+    \/\/ Test check for null free array when storing to inline tpye array\n+    @Test\n+    public void test82(Object[] dst, Object v) {\n+        dst[0] = v;\n+    }\n+\n+    @Run(test = \"test82\")\n+    public void test82_verifier(RunInfo info) {\n+        MyValue2[] dst = (MyValue2[])ValueClass.newNullRestrictedArray(MyValue2.class, 1);\n+        test82(dst, testValue2);\n+        if (!info.isWarmUp()) {\n+            try {\n+                test82(dst, null);\n+                throw new RuntimeException(\"No ArrayStoreException thrown\");\n+            } catch (NullPointerException e) {\n+                \/\/ Expected\n+            }\n+        }\n+    }\n+\n+    @Test\n+    @IR(failOn = {ALLOC_G})\n+    public void test83(Object[] dst, Object v, boolean flag) {\n+        if (dst == null) { \/\/ null check\n+        }\n+        if (flag) {\n+            if (dst.getClass() == MyValue1[].class) { \/\/ trigger split if\n+            }\n+        } else {\n+            dst = (MyValue2[])ValueClass.newNullRestrictedArray(MyValue2.class, 1); \/\/ constant null free property\n+        }\n+        dst[0] = v;\n+    }\n+\n+    @Run(test = \"test83\")\n+    @Warmup(10000)\n+    public void test83_verifier(RunInfo info) {\n+        MyValue2[] dst = (MyValue2[])ValueClass.newNullRestrictedArray(MyValue2.class, 1);\n+        test83(dst, testValue2, false);\n+        test83(dst, testValue2, true);\n+        if (!info.isWarmUp()) {\n+            try {\n+                test83(dst, null, true);\n+                throw new RuntimeException(\"No ArrayStoreException thrown\");\n+            } catch (NullPointerException e) {\n+                \/\/ Expected\n+            }\n+        }\n+    }\n+\n+    private void rerun_and_recompile_for(Method m, int num, Runnable test) {\n+        for (int i = 1; i < num; i++) {\n+            test.run();\n+\n+            if (!TestFramework.isCompiled(m)) {\n+                TestFramework.compile(m, CompLevel.C2);\n+            }\n+        }\n+    }\n+\n+    \/\/ Tests for the Loop Unswitching optimization\n+    \/\/ Should make 2 copies of the loop, one for non flattened arrays, one for other cases.\n+    @Test\n+    @IR(applyIf = {\"FlatArrayElementMaxSize\", \"= -1\"},\n+        counts = {COUNTEDLOOP_MAIN, \"= 2\"})\n+    @IR(applyIf = {\"FlatArrayElementMaxSize\", \"!= -1\"},\n+        counts = {COUNTEDLOOP_MAIN, \"= 1\"})\n+    public void test84(Object[] src, Object[] dst) {\n+        for (int i = 0; i < src.length; i++) {\n+            dst[i] = src[i];\n+        }\n+    }\n+\n+    @Run(test = \"test84\")\n+    @Warmup(0)\n+    public void test84_verifier(RunInfo info) {\n+        MyValue2[] src = (MyValue2[])ValueClass.newNullRestrictedArray(MyValue2.class, 100);\n+        Arrays.fill(src, testValue2);\n+        MyValue2[] dst = (MyValue2[])ValueClass.newNullRestrictedArray(MyValue2.class, 100);\n+        rerun_and_recompile_for(info.getTest(), 10,\n+                                () ->  { test84(src, dst);\n+                                         Asserts.assertTrue(Arrays.equals(src, dst)); });\n+    }\n+\n+    @Test\n+    @IR(applyIfAnd = {\"UseG1GC\", \"true\", \"FlatArrayElementMaxSize\", \"= -1\"},\n+        counts = {COUNTEDLOOP, \"= 2\", LOAD_UNKNOWN_INLINE, \"= 1\"})\n+    @IR(applyIfAnd = {\"UseG1GC\", \"false\", \"FlatArrayElementMaxSize\", \"= -1\"},\n+        counts = {COUNTEDLOOP_MAIN, \"= 2\", LOAD_UNKNOWN_INLINE, \"= 4\"})\n+    public void test85(Object[] src, Object[] dst) {\n+        for (int i = 0; i < src.length; i++) {\n+            dst[i] = src[i];\n+        }\n+    }\n+\n+    @Run(test = \"test85\")\n+    @Warmup(0)\n+    public void test85_verifier(RunInfo info) {\n+        Object[] src = new Object[100];\n+        Arrays.fill(src, new Object());\n+        src[0] = null;\n+        Object[] dst = new Object[100];\n+        rerun_and_recompile_for(info.getTest(), 10,\n+                                () -> { test85(src, dst);\n+                                        Asserts.assertTrue(Arrays.equals(src, dst)); });\n+    }\n+\n+    @Test\n+    @IR(applyIfAnd = {\"UseG1GC\", \"true\", \"FlatArrayElementMaxSize\", \"= -1\"},\n+        counts = {COUNTEDLOOP, \"= 2\"})\n+    @IR(applyIfAnd = {\"UseG1GC\", \"false\", \"FlatArrayElementMaxSize\", \"= -1\"},\n+        counts = {COUNTEDLOOP_MAIN, \"= 2\"})\n+    public void test86(Object[] src, Object[] dst) {\n+        for (int i = 0; i < src.length; i++) {\n+            dst[i] = src[i];\n+        }\n+    }\n+\n+    @Run(test = \"test86\")\n+    @Warmup(0)\n+    public void test86_verifier(RunInfo info) {\n+        MyValue2[] src = (MyValue2[])ValueClass.newNullRestrictedArray(MyValue2.class, 100);\n+        Arrays.fill(src, testValue2);\n+        Object[] dst = new Object[100];\n+        rerun_and_recompile_for(info.getTest(), 10,\n+                                () -> { test86(src, dst);\n+                                        Asserts.assertTrue(Arrays.equals(src, dst)); });\n+    }\n+\n+    @Test\n+    @IR(applyIf = {\"FlatArrayElementMaxSize\", \"= -1\"},\n+        counts = {COUNTEDLOOP_MAIN, \"= 2\"})\n+    @IR(applyIf = {\"FlatArrayElementMaxSize\", \"!= -1\"},\n+        counts = {COUNTEDLOOP_MAIN, \"= 1\"})\n+    public void test87(Object[] src, Object[] dst) {\n+        for (int i = 0; i < src.length; i++) {\n+            dst[i] = src[i];\n+        }\n+    }\n+\n+    @Run(test = \"test87\")\n+    @Warmup(0)\n+    public void test87_verifier(RunInfo info) {\n+        Object[] src = new Object[100];\n+        Arrays.fill(src, testValue2);\n+        MyValue2[] dst = (MyValue2[])ValueClass.newNullRestrictedArray(MyValue2.class, 100);\n+\n+        rerun_and_recompile_for(info.getTest(), 10,\n+                                () -> { test87(src, dst);\n+                                        Asserts.assertTrue(Arrays.equals(src, dst)); });\n+    }\n+\n+    @Test\n+    \/* FIX: JDK-8344532\n+    @IR(applyIf = {\"FlatArrayElementMaxSize\", \"= -1\"},\n+        counts = {COUNTEDLOOP_MAIN, \"= 2\"})\n+    @IR(applyIf = {\"FlatArrayElementMaxSize\", \"!= -1\"},\n+        counts = {COUNTEDLOOP_MAIN, \"= 0\"})\n+    *\/\n+    public void test88(Object[] src1, Object[] dst1, Object[] src2, Object[] dst2) {\n+        for (int i = 0; i < src1.length; i++) {\n+            dst1[i] = src1[i];\n+            dst2[i] = src2[i];\n+        }\n+    }\n+\n+    @Run(test = \"test88\")\n+    @Warmup(0)\n+    public void test88_verifier(RunInfo info) {\n+        MyValue2[] src1 = (MyValue2[])ValueClass.newNullRestrictedArray(MyValue2.class, 100);\n+        Arrays.fill(src1, testValue2);\n+        MyValue2[] dst1 = (MyValue2[])ValueClass.newNullRestrictedArray(MyValue2.class, 100);\n+        Object[] src2 = new Object[100];\n+        Arrays.fill(src2, new Object());\n+        Object[] dst2 = new Object[100];\n+\n+        rerun_and_recompile_for(info.getTest(), 10,\n+                                () -> { test88(src1, dst1, src2, dst2);\n+                                        Asserts.assertTrue(Arrays.equals(src1, dst1));\n+                                        Asserts.assertTrue(Arrays.equals(src2, dst2)); });\n+    }\n+\n+    @Test\n+    public boolean test89(Object obj) {\n+        return obj.getClass() == NonValueClass.class;\n+    }\n+\n+    @Run(test = \"test89\")\n+    public void test89_verifier() {\n+        Asserts.assertTrue(test89(new NonValueClass(42)));\n+        Asserts.assertFalse(test89(new Object()));\n+    }\n+\n+    @Test\n+    public NonValueClass test90(Object obj) {\n+        return (NonValueClass)obj;\n+    }\n+\n+    @Run(test = \"test90\")\n+    public void test90_verifier() {\n+        test90(new NonValueClass(42));\n+        try {\n+            test90(new Object());\n+            throw new RuntimeException(\"ClassCastException expected\");\n+        } catch (ClassCastException e) {\n+            \/\/ Expected\n+        }\n+    }\n+\n+    @Test\n+    public boolean test91(Object obj) {\n+        return obj.getClass() == MyValue2[].class;\n+    }\n+\n+    @Run(test = \"test91\")\n+    public void test91_verifier() {\n+        Asserts.assertFalse(test91((MyValue2[])ValueClass.newNullRestrictedArray(MyValue2.class, 1)));\n+        Asserts.assertTrue(test91(new MyValue2[1]));\n+        Asserts.assertFalse(test91(new Object()));\n+    }\n+\n+    @ImplicitlyConstructible\n+    @LooselyConsistentValue\n+    static value class Test92Value {\n+        int field;\n+\n+        public Test92Value() {\n+            field = 0x42;\n+        }\n+    }\n+\n+    @Test\n+    @IR(applyIf = {\"FlatArrayElementMaxSize\", \"= -1\"},\n+        counts = {CLASS_CHECK_TRAP, \"= 2\"},\n+        failOn = {LOAD_UNKNOWN_INLINE, ALLOC_G, MEMBAR})\n+    public Object test92(Object[] array) {\n+        \/\/ Dummy loops to ensure we run enough passes of split if\n+        for (int i = 0; i < 2; i++) {\n+            for (int j = 0; j < 2; j++) {\n+              for (int k = 0; k < 2; k++) {\n+              }\n+            }\n+        }\n+\n+        return (NonValueClass)array[0];\n+    }\n+\n+    @Run(test = \"test92\")\n+    @Warmup(10000)\n+    public void test92_verifier() {\n+        Object[] array = new Object[1];\n+        Object obj = new NonValueClass(rI);\n+        array[0] = obj;\n+        Object result = test92(array);\n+        Asserts.assertEquals(result, obj);\n+    }\n+\n+    \/\/ If the class check succeeds, the flattened array check that\n+    \/\/ precedes will never succeed and the flat array branch should\n+    \/\/ trigger an uncommon trap.\n+    @Test\n+    public Object test93(Object[] array) {\n+        for (int i = 0; i < 2; i++) {\n+            for (int j = 0; j < 2; j++) {\n+            }\n+        }\n+\n+        Object v = (NonValueClass)array[0];\n+        return v;\n+    }\n+\n+    @Run(test = \"test93\")\n+    @Warmup(10000)\n+    public void test93_verifier(RunInfo info) {\n+        if (info.isWarmUp()) {\n+            Object[] array = new Object[1];\n+            array[0] = new NonValueClass(42);\n+            Object result = test93(array);\n+            Asserts.assertEquals(((NonValueClass)result).x, 42);\n+        } else {\n+            Object[] array = (Test92Value[])ValueClass.newNullRestrictedArray(Test92Value.class, 1);\n+            Method m = info.getTest();\n+            int extra = 3;\n+            for (int j = 0; j < extra; j++) {\n+                for (int i = 0; i < 10; i++) {\n+                    try {\n+                        test93(array);\n+                    } catch (ClassCastException cce) {\n+                    }\n+                }\n+                boolean compiled = TestFramework.isCompiled(m);\n+                boolean compilationSkipped = info.isCompilationSkipped();\n+                Asserts.assertTrue(compilationSkipped || compiled || (j != extra-1));\n+                if (!compilationSkipped && !compiled) {\n+                    TestFramework.compile(m, CompLevel.ANY);\n+                }\n+            }\n+        }\n+    }\n+\n+    @Test\n+    @IR(applyIf = {\"FlatArrayElementMaxSize\", \"= -1\"},\n+        counts = {CLASS_CHECK_TRAP, \"= 2\", LOOP, \"= 1\"},\n+        failOn = {LOAD_UNKNOWN_INLINE, ALLOC_G, MEMBAR})\n+    public int test94(Object[] array) {\n+        int res = 0;\n+        for (int i = 1; i < 4; i *= 2) {\n+            Object v = array[i];\n+            res += ((NonValueClass)v).x;\n+        }\n+        return res;\n+    }\n+\n+    @Run(test = \"test94\")\n+    @Warmup(10000)\n+    public void test94_verifier() {\n+        Object[] array = new Object[4];\n+        Object obj = new NonValueClass(rI);\n+        array[0] = obj;\n+        array[1] = obj;\n+        array[2] = obj;\n+        array[3] = obj;\n+        int result = test94(array);\n+        Asserts.assertEquals(result, rI * 2);\n+    }\n+\n+    @Test\n+    public boolean test95(Object o1, Object o2) {\n+        return o1 == o2;\n+    }\n+\n+    @Run(test = \"test95\")\n+    @Warmup(10000)\n+    public void test95_verifier() {\n+        Object o1 = new Object();\n+        Object o2 = new Object();\n+        Asserts.assertTrue(test95(o1, o1));\n+        Asserts.assertTrue(test95(null, null));\n+        Asserts.assertFalse(test95(o1, null));\n+        Asserts.assertFalse(test95(o1, o2));\n+    }\n+\n+    @Test\n+    public boolean test96(Object o1, Object o2) {\n+        return o1 == o2;\n+    }\n+\n+    @Run(test = \"test96\")\n+    @Warmup(10000)\n+    public void test96_verifier(RunInfo info) {\n+        Object o1 = new Object();\n+        Object o2 = new Object();\n+        Asserts.assertTrue(test96(o1, o1));\n+        Asserts.assertFalse(test96(o1, o2));\n+        if (!info.isWarmUp()) {\n+            Asserts.assertTrue(test96(null, null));\n+            Asserts.assertFalse(test96(o1, null));\n+        }\n+    }\n+\n+    \/\/ Abstract class tests\n+\n+    @DontInline\n+    public MyAbstract test97_dontinline1(MyAbstract o) {\n+        return o;\n+    }\n+\n+    @DontInline\n+    public MyValue1 test97_dontinline2(MyAbstract o) {\n+        return (MyValue1)o;\n+    }\n+\n+    @ForceInline\n+    public MyAbstract test97_inline1(MyAbstract o) {\n+        return o;\n+    }\n+\n+    @ForceInline\n+    public MyValue1 test97_inline2(MyAbstract o) {\n+        return (MyValue1)o;\n+    }\n+\n+    @Test\n+    public MyValue1 test97() {\n+        MyValue1 vt = MyValue1.createWithFieldsInline(rI, rL);\n+        vt = (MyValue1)test97_dontinline1(vt);\n+        vt =           test97_dontinline2(vt);\n+        vt = (MyValue1)test97_inline1(vt);\n+        vt =           test97_inline2(vt);\n+        return vt;\n+    }\n+\n+    @Run(test = \"test97\")\n+    public void test97_verifier() {\n+        Asserts.assertEQ(test97().hash(), hash());\n+    }\n+\n+    \/\/ Test storing\/loading inline types to\/from abstract and inline type fields\n+    MyAbstract abstractField1 = null;\n+    MyAbstract abstractField2 = null;\n+    MyAbstract abstractField3 = null;\n+    MyAbstract abstractField4 = null;\n+    MyAbstract abstractField5 = null;\n+    MyAbstract abstractField6 = null;\n+\n+    @DontInline\n+    public MyAbstract readValueField5AsAbstract() {\n+        return (MyAbstract)valueField5;\n+    }\n+\n+    @DontInline\n+    public MyAbstract readStaticValueField4AsAbstract() {\n+        return (MyAbstract)staticValueField4;\n+    }\n+\n+    @Test\n+    public long test98(MyValue1 vt1, MyAbstract vt2) {\n+        abstractField1 = vt1;\n+        abstractField2 = (MyValue1)vt2;\n+        abstractField3 = MyValue1.createWithFieldsInline(rI, rL);\n+        abstractField4 = MyValue1.createWithFieldsDontInline(rI, rL);\n+        abstractField5 = valueField1;\n+        abstractField6 = valueField3;\n+        valueField1 = (MyValue1)abstractField1;\n+        valueField2 = (MyValue1)vt2;\n+        valueField3 = (MyValue1)vt2;\n+        staticValueField1 = (MyValue1)abstractField1;\n+        staticValueField2 = (MyValue1)vt1;\n+        \/\/ Don't inline these methods because reading NULL will trigger a deoptimization\n+        if (readValueField5AsAbstract() != null || readStaticValueField4AsAbstract() != null) {\n+            throw new RuntimeException(\"Should be null\");\n+        }\n+        return ((MyValue1)abstractField1).hash() + ((MyValue1)abstractField2).hash() +\n+               ((MyValue1)abstractField3).hash() + ((MyValue1)abstractField4).hash() +\n+               ((MyValue1)abstractField5).hash() + ((MyValue1)abstractField6).hash() +\n+                valueField1.hash() + valueField2.hash() + valueField3.hash() + valueField4.hashPrimitive() +\n+                staticValueField1.hash() + staticValueField2.hash() + staticValueField3.hashPrimitive();\n+    }\n+\n+    @Run(test = \"test98\")\n+    public void test98_verifier() {\n+        MyValue1 vt = testValue1;\n+        MyValue1 def = MyValue1.createDefaultDontInline();\n+        long result = test98(vt, vt);\n+        Asserts.assertEQ(result, 11*vt.hash() + 2*def.hashPrimitive());\n+    }\n+\n+    value class MyObject2 extends MyAbstract {\n+        public int x;\n+\n+        public MyObject2(int x) {\n+            this.x = x;\n+        }\n+\n+        @ForceInline\n+        public long hash() {\n+            return x;\n+        }\n+    }\n+\n+    \/\/ Test merging inline types and abstract classes\n+    @Test\n+    public MyAbstract test99(int state) {\n+        MyAbstract res = null;\n+        if (state == 0) {\n+            res = new MyObject2(rI);\n+        } else if (state == 1) {\n+            res = MyValue1.createWithFieldsInline(rI, rL);\n+        } else if (state == 2) {\n+            res = MyValue1.createWithFieldsDontInline(rI, rL);\n+        } else if (state == 3) {\n+            res = (MyValue1)objectField1;\n+        } else if (state == 4) {\n+            res = valueField1;\n+        } else if (state == 5) {\n+            res = null;\n+        }\n+        return res;\n+    }\n+\n+    @Run(test = \"test99\")\n+    public void test99_verifier() {\n+        objectField1 = valueField1;\n+        MyAbstract result = null;\n+        result = test99(0);\n+        Asserts.assertEQ(((MyObject2)result).x, rI);\n+        result = test99(1);\n+        Asserts.assertEQ(((MyValue1)result).hash(), hash());\n+        result = test99(2);\n+        Asserts.assertEQ(((MyValue1)result).hash(), hash());\n+        result = test99(3);\n+        Asserts.assertEQ(((MyValue1)result).hash(), hash());\n+        result = test99(4);\n+        Asserts.assertEQ(((MyValue1)result).hash(), hash());\n+        result = test99(5);\n+        Asserts.assertEQ(result, null);\n+    }\n+\n+    \/\/ Test merging inline types and abstract classes in loops\n+    @Test\n+    public MyAbstract test100(int iters) {\n+        MyAbstract res = new MyObject2(rI);\n+        for (int i = 0; i < iters; ++i) {\n+            if (res instanceof MyObject2) {\n+                res = MyValue1.createWithFieldsInline(rI, rL);\n+            } else {\n+                res = MyValue1.createWithFieldsInline(((MyValue1)res).x + 1, rL);\n+            }\n+        }\n+        return res;\n+    }\n+\n+    @Run(test = \"test100\")\n+    public void test100_verifier() {\n+        MyObject2 result1 = (MyObject2)test100(0);\n+        Asserts.assertEQ(result1.x, rI);\n+        int iters = (Math.abs(rI) % 10) + 1;\n+        MyValue1 result2 = (MyValue1)test100(iters);\n+        MyValue1 vt = MyValue1.createWithFieldsInline(rI + iters - 1, rL);\n+        Asserts.assertEQ(result2.hash(), vt.hash());\n+    }\n+\n+    \/\/ Test inline types in abstract class variables that are live at safepoint\n+    @Test\n+    @IR(failOn = {ALLOC, STORE, LOOP})\n+    public long test101(MyValue1 arg, boolean deopt, Method m) {\n+        MyAbstract vt1 = MyValue1.createWithFieldsInline(rI, rL);\n+        MyAbstract vt2 = MyValue1.createWithFieldsDontInline(rI, rL);\n+        MyAbstract vt3 = arg;\n+        MyAbstract vt4 = valueField1;\n+        if (deopt) {\n+            \/\/ uncommon trap\n+            TestFramework.deoptimize(m);\n+        }\n+        return ((MyValue1)vt1).hash() + ((MyValue1)vt2).hash() +\n+               ((MyValue1)vt3).hash() + ((MyValue1)vt4).hash();\n+    }\n+\n+    @Run(test = \"test101\")\n+    public void test101_verifier(RunInfo info) {\n+        long result = test101(valueField1, !info.isWarmUp(), info.getTest());\n+        Asserts.assertEQ(result, 4*hash());\n+    }\n+\n+    \/\/ Test comparing inline types with abstract classes\n+    @Test\n+    public boolean test102(Object arg) {\n+        MyAbstract vt = MyValue1.createWithFieldsInline(rI, rL);\n+        if (vt == arg || vt == (MyAbstract)valueField1 || vt == abstractField1 || vt == null ||\n+            arg == vt || (MyAbstract)valueField1 == vt || abstractField1 == vt || null == vt) {\n+            return true;\n+        }\n+        return false;\n+    }\n+\n+    @Run(test = \"test102\")\n+    public void test102_verifier() {\n+        boolean result = test102(null);\n+        Asserts.assertFalse(result);\n+    }\n+\n+    \/\/ An abstract class with a non-static field can never be implemented by an inline type\n+    abstract class NoValueImplementors1 {\n+        int field = 42;\n+    }\n+\n+    class MyObject3 extends NoValueImplementors1 {\n+\n+    }\n+\n+    class MyObject4 extends NoValueImplementors1 {\n+\n+    }\n+\n+    \/\/ Loading from an abstract class array does not require a flatness check if the abstract class has a non-static field\n+    @Test\n+    @IR(failOn = {ALLOC_G, MEMBAR, LOAD_UNKNOWN_INLINE, STORE_UNKNOWN_INLINE, INLINE_ARRAY_NULL_GUARD})\n+    public NoValueImplementors1 test103(NoValueImplementors1[] array, int i) {\n+        return array[i];\n+    }\n+\n+    @Run(test = \"test103\")\n+    public void test103_verifier() {\n+        NoValueImplementors1[] array1 = new NoValueImplementors1[3];\n+        MyObject3[] array2 = new MyObject3[3];\n+        MyObject4[] array3 = new MyObject4[3];\n+        NoValueImplementors1 result = test103(array1, 0);\n+        Asserts.assertEquals(result, array1[0]);\n+\n+        result = test103(array2, 1);\n+        Asserts.assertEquals(result, array1[1]);\n+\n+        result = test103(array3, 2);\n+        Asserts.assertEquals(result, array1[2]);\n+    }\n+\n+    \/\/ Storing to an abstract class array does not require a flatness\/null check if the abstract class has a non-static field\n+    @Test\n+    @IR(failOn = {ALLOC_G, LOAD_UNKNOWN_INLINE, STORE_UNKNOWN_INLINE, INLINE_ARRAY_NULL_GUARD})\n+    public NoValueImplementors1 test104(NoValueImplementors1[] array, NoValueImplementors1 v, MyObject3 o, int i) {\n+        array[0] = v;\n+        array[1] = array[0];\n+        array[2] = o;\n+        return array[i];\n+    }\n+\n+    @Run(test = \"test104\")\n+    public void test104_verifier() {\n+        MyObject4 v = new MyObject4();\n+        MyObject3 o = new MyObject3();\n+        NoValueImplementors1[] array1 = new NoValueImplementors1[3];\n+        MyObject3[] array2 = new MyObject3[3];\n+        MyObject4[] array3 = new MyObject4[3];\n+        NoValueImplementors1 result = test104(array1, v, o, 0);\n+        Asserts.assertEquals(array1[0], v);\n+        Asserts.assertEquals(array1[1], v);\n+        Asserts.assertEquals(array1[2], o);\n+        Asserts.assertEquals(result, v);\n+\n+        result = test104(array2, o, o, 1);\n+        Asserts.assertEquals(array2[0], o);\n+        Asserts.assertEquals(array2[1], o);\n+        Asserts.assertEquals(array2[2], o);\n+        Asserts.assertEquals(result, o);\n+\n+        result = test104(array3, v, null, 1);\n+        Asserts.assertEquals(array3[0], v);\n+        Asserts.assertEquals(array3[1], v);\n+        Asserts.assertEquals(array3[2], null);\n+        Asserts.assertEquals(result, v);\n+    }\n+\n+    \/\/ An abstract class with a single, non-inline implementor\n+    abstract class NoValueImplementors2 {\n+\n+    }\n+\n+    class MyObject5 extends NoValueImplementors2 {\n+\n+    }\n+\n+    \/\/ Loading from an abstract class array does not require a flatness check if the abstract class has no inline implementor\n+    @Test\n+    @IR(failOn = {ALLOC_G, MEMBAR, LOAD_UNKNOWN_INLINE, STORE_UNKNOWN_INLINE, INLINE_ARRAY_NULL_GUARD})\n+    public NoValueImplementors2 test105(NoValueImplementors2[] array, int i) {\n+        return array[i];\n+    }\n+\n+    @Run(test = \"test105\")\n+    public void test105_verifier() {\n+        NoValueImplementors2[] array1 = new NoValueImplementors2[3];\n+        MyObject5[] array2 = new MyObject5[3];\n+        NoValueImplementors2 result = test105(array1, 0);\n+        Asserts.assertEquals(result, array1[0]);\n+\n+        result = test105(array2, 1);\n+        Asserts.assertEquals(result, array1[1]);\n+    }\n+\n+    \/\/ Storing to an abstract class array does not require a flatness\/null check if the abstract class has no inline implementor\n+    @Test\n+    @IR(failOn = {ALLOC_G, LOAD_UNKNOWN_INLINE, STORE_UNKNOWN_INLINE, INLINE_ARRAY_NULL_GUARD})\n+    public NoValueImplementors2 test106(NoValueImplementors2[] array, NoValueImplementors2 v, MyObject5 o, int i) {\n+        array[0] = v;\n+        array[1] = array[0];\n+        array[2] = o;\n+        return array[i];\n+    }\n+\n+    @Run(test = \"test106\")\n+    public void test106_verifier() {\n+        MyObject5 v = new MyObject5();\n+        NoValueImplementors2[] array1 = new NoValueImplementors2[3];\n+        MyObject5[] array2 = new MyObject5[3];\n+        NoValueImplementors2 result = test106(array1, v, null, 0);\n+        Asserts.assertEquals(array1[0], v);\n+        Asserts.assertEquals(array1[1], v);\n+        Asserts.assertEquals(array1[2], null);\n+        Asserts.assertEquals(result, v);\n+\n+        result = test106(array2, v, v, 1);\n+        Asserts.assertEquals(array2[0], v);\n+        Asserts.assertEquals(array2[1], v);\n+        Asserts.assertEquals(array2[2], v);\n+        Asserts.assertEquals(result, v);\n+    }\n+\n+    \/\/ More tests for the Loop Unswitching optimization (similar to test84 and following)\n+    Object oFld1, oFld2;\n+\n+    @Test\n+    @IR(applyIfAnd = {\"UseG1GC\", \"true\", \"FlatArrayElementMaxSize\", \"= -1\"},\n+        failOn = {STORE_UNKNOWN_INLINE, INLINE_ARRAY_NULL_GUARD},\n+        counts = {COUNTEDLOOP, \"= 2\", LOAD_UNKNOWN_INLINE, \"= 2\"})\n+    @IR(applyIfAnd = {\"UseG1GC\", \"false\", \"FlatArrayElementMaxSize\", \"= -1\"},\n+        failOn = {STORE_UNKNOWN_INLINE, INLINE_ARRAY_NULL_GUARD},\n+        counts = {COUNTEDLOOP, \"= 3\", LOAD_UNKNOWN_INLINE, \"= 2\"})\n+    public void test107(Object[] src1, Object[] src2) {\n+        for (int i = 0; i < src1.length; i++) {\n+            oFld1 = src1[i];\n+            oFld2 = src2[i];\n+        }\n+    }\n+\n+    @Run(test = \"test107\")\n+    @Warmup(0)\n+    public void test107_verifier(RunInfo info) {\n+        MyValue2[] src1 = (MyValue2[])ValueClass.newNullRestrictedArray(MyValue2.class, 100);\n+        Arrays.fill(src1, testValue2);\n+        Object[] src2 = new Object[100];\n+        Object obj = new Object();\n+        Arrays.fill(src2, obj);\n+        rerun_and_recompile_for(info.getTest(), 10,\n+                                () -> { test107(src1, src2);\n+                                        Asserts.assertEquals(oFld1, testValue2);\n+                                        Asserts.assertEquals(oFld2, obj);\n+                                        test107(src2, src1);\n+                                        Asserts.assertEquals(oFld1, obj);\n+                                        Asserts.assertEquals(oFld2, testValue2);  });\n+    }\n+\n+    @Test\n+    @IR(applyIfAnd = {\"UseG1GC\", \"true\", \"FlatArrayElementMaxSize\", \"= -1\"},\n+        failOn = {LOAD_UNKNOWN_INLINE, INLINE_ARRAY_NULL_GUARD},\n+        counts = {COUNTEDLOOP, \"= 4\", STORE_UNKNOWN_INLINE, \"= 9\"})\n+    @IR(applyIfAnd = {\"UseG1GC\", \"false\", \"FlatArrayElementMaxSize\", \"= -1\"},\n+        failOn = {LOAD_UNKNOWN_INLINE, INLINE_ARRAY_NULL_GUARD},\n+        counts = {COUNTEDLOOP, \"= 4\", STORE_UNKNOWN_INLINE, \"= 12\"})\n+    public void test108(Object[] dst1, Object[] dst2, Object o1, Object o2) {\n+        for (int i = 0; i < dst1.length; i++) {\n+            dst1[i] = o1;\n+            dst2[i] = o2;\n+        }\n+    }\n+\n+    @Run(test = \"test108\")\n+    @Warmup(0)\n+    public void test108_verifier(RunInfo info) {\n+        MyValue2[] dst1 = (MyValue2[])ValueClass.newNullRestrictedArray(MyValue2.class, 100);\n+        Object[] dst2 = new Object[100];\n+        Object o1 = new Object();\n+        rerun_and_recompile_for(info.getTest(), 10,\n+                                () -> { test108(dst1, dst2, testValue2, o1);\n+                                        for (int i = 0; i < dst1.length; i++) {\n+                                            Asserts.assertEquals(dst1[i], testValue2);\n+                                            Asserts.assertEquals(dst2[i], o1);\n+                                        }\n+                                        test108(dst2, dst1, o1, testValue2);\n+                                        for (int i = 0; i < dst1.length; i++) {\n+                                            Asserts.assertEquals(dst1[i], testValue2);\n+                                            Asserts.assertEquals(dst2[i], o1);\n+                                        } });\n+    }\n+\n+    \/\/ Escape analysis tests\n+\n+    static interface WrapperInterface {\n+        long value();\n+\n+        final static WrapperInterface ZERO = new LongWrapper(0);\n+\n+        @ForceInline\n+        static WrapperInterface wrap(long val) {\n+            return (val == 0L) ? ZERO : new LongWrapper(val);\n+        }\n+    }\n+\n+    @ForceCompileClassInitializer\n+    @ImplicitlyConstructible\n+    @LooselyConsistentValue\n+    static value class LongWrapper implements WrapperInterface {\n+        @NullRestricted\n+        final static LongWrapper ZERO = new LongWrapper(0);\n+        private long val;\n+\n+        @ForceInline\n+        LongWrapper(long val) {\n+            this.val = val;\n+        }\n+\n+        @ForceInline\n+        static LongWrapper wrap(long val) {\n+            return (val == 0L) ? ZERO : new LongWrapper(val);\n+        }\n+\n+        @ForceInline\n+        public long value() {\n+            return val;\n+        }\n+    }\n+\n+    static class InterfaceBox {\n+        WrapperInterface content;\n+\n+        @ForceInline\n+        InterfaceBox(WrapperInterface content) {\n+            this.content = content;\n+        }\n+\n+        @ForceInline\n+        static InterfaceBox box_sharp(long val) {\n+            return new InterfaceBox(LongWrapper.wrap(val));\n+        }\n+\n+        @ForceInline\n+        static InterfaceBox box(long val) {\n+            return new InterfaceBox(WrapperInterface.wrap(val));\n+        }\n+    }\n+\n+    static class ObjectBox {\n+        Object content;\n+\n+        @ForceInline\n+        ObjectBox(Object content) {\n+            this.content = content;\n+        }\n+\n+        @ForceInline\n+        static ObjectBox box_sharp(long val) {\n+            return new ObjectBox(LongWrapper.wrap(val));\n+        }\n+\n+        @ForceInline\n+        static ObjectBox box(long val) {\n+            return new ObjectBox(WrapperInterface.wrap(val));\n+        }\n+    }\n+\n+    static class RefBox {\n+        LongWrapper content;\n+\n+        @ForceInline\n+        RefBox(LongWrapper content) {\n+            this.content = content;\n+        }\n+\n+        @ForceInline\n+        static RefBox box_sharp(long val) {\n+            return new RefBox(LongWrapper.wrap(val));\n+        }\n+\n+        @ForceInline\n+        static RefBox box(long val) {\n+            return new RefBox((LongWrapper)WrapperInterface.wrap(val));\n+        }\n+    }\n+\n+    static class InlineBox {\n+        @NullRestricted\n+        LongWrapper content;\n+\n+        @ForceInline\n+        InlineBox(long val) {\n+            this.content = LongWrapper.wrap(val);\n+        }\n+\n+        @ForceInline\n+        static InlineBox box(long val) {\n+            return new InlineBox(val);\n+        }\n+    }\n+\n+    static class GenericBox<T> {\n+        T content;\n+\n+        @ForceInline\n+        static GenericBox<LongWrapper> box_sharp(long val) {\n+            GenericBox<LongWrapper> res = new GenericBox<>();\n+            res.content = LongWrapper.wrap(val);\n+            return res;\n+        }\n+\n+        @ForceInline\n+        static GenericBox<WrapperInterface> box(long val) {\n+            GenericBox<WrapperInterface> res = new GenericBox<>();\n+            res.content = WrapperInterface.wrap(val);\n+            return res;\n+        }\n+    }\n+\n+    long[] lArr = {0L, rL, 0L, rL, 0L, rL, 0L, rL, 0L, rL};\n+\n+    \/\/ Test removal of allocations when inline type instance is wrapped into box object\n+    @Test\n+    @IR(failOn = {ALLOC_G, MEMBAR},\n+        counts = {PREDICATE_TRAP, \"= 1\"})\n+    public long test109() {\n+        long res = 0;\n+        for (int i = 0; i < lArr.length; i++) {\n+            res += InterfaceBox.box(lArr[i]).content.value();\n+        }\n+        return res;\n+    }\n+\n+    @Run(test = \"test109\")\n+    @Warmup(10000) \/\/ Make sure interface calls are inlined\n+    public void test109_verifier() {\n+        long res = test109();\n+        Asserts.assertEquals(res, 5*rL);\n+    }\n+\n+    @Test\n+    @IR(failOn = {ALLOC_G, MEMBAR},\n+        counts = {PREDICATE_TRAP, \"= 1\"})\n+    @IR(failOn = {ALLOC_G, MEMBAR})\n+    public long test109_sharp() {\n+        long res = 0;\n+        for (int i = 0; i < lArr.length; i++) {\n+            res += InterfaceBox.box_sharp(lArr[i]).content.value();\n+        }\n+        return res;\n+    }\n+\n+    @Run(test = \"test109_sharp\")\n+    @Warmup(10000) \/\/ Make sure interface calls are inlined\n+    public void test109_sharp_verifier() {\n+        long res = test109_sharp();\n+        Asserts.assertEquals(res, 5*rL);\n+    }\n+\n+    \/\/ Same as test109 but with ObjectBox\n+    @Test\n+    @IR(failOn = {ALLOC_G, MEMBAR},\n+        counts = {PREDICATE_TRAP, \"= 1\"})\n+    public long test110() {\n+        long res = 0;\n+        for (int i = 0; i < lArr.length; i++) {\n+            res += ((WrapperInterface)ObjectBox.box(lArr[i]).content).value();\n+        }\n+        return res;\n+    }\n+\n+    @Run(test = \"test110\")\n+    @Warmup(10000) \/\/ Make sure interface calls are inlined\n+    public void test110_verifier() {\n+        long res = test110();\n+        Asserts.assertEquals(res, 5*rL);\n+    }\n+\n+    @Test\n+    @IR(failOn = {ALLOC_G, MEMBAR},\n+        counts = {PREDICATE_TRAP, \"= 1\"})\n+    @IR(failOn = {ALLOC_G, MEMBAR})\n+    public long test110_sharp() {\n+        long res = 0;\n+        for (int i = 0; i < lArr.length; i++) {\n+            res += ((WrapperInterface)ObjectBox.box_sharp(lArr[i]).content).value();\n+        }\n+        return res;\n+    }\n+\n+    @Run(test = \"test110_sharp\")\n+    @Warmup(10000) \/\/ Make sure interface calls are inlined\n+    public void test110_sharp_verifier() {\n+        long res = test110_sharp();\n+        Asserts.assertEquals(res, 5*rL);\n+    }\n+\n+    \/\/ Same as test109 but with RefBox\n+    @Test\n+    @IR(failOn = {ALLOC_G, MEMBAR},\n+        counts = {PREDICATE_TRAP, \"= 1\"})\n+    public long test111() {\n+        long res = 0;\n+        for (int i = 0; i < lArr.length; i++) {\n+            res += RefBox.box(lArr[i]).content.value();\n+        }\n+        return res;\n+    }\n+\n+    @Run(test = \"test111\")\n+    public void test111_verifier() {\n+        long res = test111();\n+        Asserts.assertEquals(res, 5*rL);\n+    }\n+\n+    @Test\n+    @IR(failOn = {ALLOC_G, MEMBAR},\n+        counts = {PREDICATE_TRAP, \"= 1\"})\n+    public long test111_sharp() {\n+        long res = 0;\n+        for (int i = 0; i < lArr.length; i++) {\n+            res += RefBox.box_sharp(lArr[i]).content.value();\n+        }\n+        return res;\n+    }\n+\n+    @Run(test = \"test111_sharp\")\n+    public void test111_sharp_verifier() {\n+        long res = test111_sharp();\n+        Asserts.assertEquals(res, 5*rL);\n+    }\n+\n+    \/\/ Same as test109 but with InlineBox\n+    @Test\n+    @IR(failOn = {ALLOC_G, MEMBAR},\n+        counts = {PREDICATE_TRAP, \"= 1\"})\n+    public long test112() {\n+        long res = 0;\n+        for (int i = 0; i < lArr.length; i++) {\n+            res += InlineBox.box(lArr[i]).content.value();\n+        }\n+        return res;\n+    }\n+\n+    @Run(test = \"test112\")\n+    public void test112_verifier() {\n+        long res = test112();\n+        Asserts.assertEquals(res, 5*rL);\n+    }\n+\n+    \/\/ Same as test109 but with GenericBox\n+    @Test\n+    @IR(failOn = {ALLOC_G, MEMBAR},\n+        counts = {PREDICATE_TRAP, \"= 1\"})\n+    public long test113() {\n+        long res = 0;\n+        for (int i = 0; i < lArr.length; i++) {\n+            res += GenericBox.box(lArr[i]).content.value();\n+        }\n+        return res;\n+    }\n+\n+    @Run(test = \"test113\")\n+    @Warmup(10000) \/\/ Make sure interface calls are inlined\n+    public void test113_verifier() {\n+        long res = test113();\n+        Asserts.assertEquals(res, 5*rL);\n+    }\n+\n+    @Test\n+    @IR(failOn = {ALLOC_G, MEMBAR},\n+        counts = {PREDICATE_TRAP, \"= 1\"})\n+    public long test113_sharp() {\n+        long res = 0;\n+        for (int i = 0; i < lArr.length; i++) {\n+            res += GenericBox.box_sharp(lArr[i]).content.value();\n+        }\n+        return res;\n+    }\n+\n+    @Run(test = \"test113_sharp\")\n+    @Warmup(10000) \/\/ Make sure interface calls are inlined\n+    public void test113_sharp_verifier() {\n+        long res = test113_sharp();\n+        Asserts.assertEquals(res, 5*rL);\n+    }\n+\n+    static interface WrapperInterface2 {\n+        public long value();\n+\n+        static final InlineWrapper ZERO = new InlineWrapper(0);\n+\n+        @ForceInline\n+        public static WrapperInterface2 wrap(long val) {\n+            return (val == 0) ? ZERO.content : new LongWrapper2(val);\n+        }\n+\n+        @ForceInline\n+        public static WrapperInterface2 wrap_default(long val) {\n+            return (val == 0) ? new LongWrapper2(0) : new LongWrapper2(val);\n+        }\n+    }\n+\n+    @ImplicitlyConstructible\n+    @LooselyConsistentValue\n+    static value class LongWrapper2 implements WrapperInterface2 {\n+        private long val;\n+\n+        @ForceInline\n+        public LongWrapper2(long val) {\n+            this.val = val;\n+        }\n+\n+        @ForceInline\n+        public long value() {\n+            return val;\n+        }\n+    }\n+\n+    @ImplicitlyConstructible\n+    @LooselyConsistentValue\n+    static value class InlineWrapper {\n+        WrapperInterface2 content;\n+\n+        @ForceInline\n+        public InlineWrapper(long val) {\n+            content = new LongWrapper2(val);\n+        }\n+    }\n+\n+    static class InterfaceBox2 {\n+        WrapperInterface2 content;\n+\n+        @ForceInline\n+        public InterfaceBox2(long val, boolean def) {\n+            this.content = def ? WrapperInterface2.wrap_default(val) : WrapperInterface2.wrap(val);\n+        }\n+\n+        @ForceInline\n+        static InterfaceBox2 box(long val) {\n+            return new InterfaceBox2(val, false);\n+        }\n+\n+        @ForceInline\n+        static InterfaceBox2 box_default(long val) {\n+            return new InterfaceBox2(val, true);\n+        }\n+    }\n+\n+    \/\/ Same as tests above but with ZERO hidden in field of another inline type\n+    @Test\n+    @IR(failOn = {ALLOC_G, MEMBAR},\n+        counts = {PREDICATE_TRAP, \"= 1\"})\n+    public long test114() {\n+        long res = 0;\n+        for (int i = 0; i < lArr.length; i++) {\n+            res += InterfaceBox2.box(lArr[i]).content.value();\n+        }\n+        return res;\n+    }\n+\n+    @Run(test = \"test114\")\n+    @Warmup(10000)\n+    public void test114_verifier() {\n+        long res = test114();\n+        Asserts.assertEquals(res, 5*rL);\n+    }\n+\n+    \/\/ Same as test114 but with default instead of ZERO field\n+    @Test\n+    @IR(failOn = {ALLOC_G, MEMBAR},\n+        counts = {PREDICATE_TRAP, \"= 1\"})\n+    public long test115() {\n+        long res = 0;\n+        for (int i = 0; i < lArr.length; i++) {\n+            res += InterfaceBox2.box_default(lArr[i]).content.value();\n+        }\n+        return res;\n+    }\n+\n+    @Run(test = \"test115\")\n+    @Warmup(10000)\n+    public void test115_verifier() {\n+        long res = test115();\n+        Asserts.assertEquals(res, 5*rL);\n+    }\n+\n+    @NullRestricted\n+    static MyValueEmpty fEmpty1;\n+    static MyValueEmpty fEmpty2 = new MyValueEmpty();\n+    @NullRestricted\n+           MyValueEmpty fEmpty3;\n+           MyValueEmpty fEmpty4 = new MyValueEmpty();\n+\n+    \/\/ Test fields loads\/stores with empty inline types\n+    @Test\n+    @IR(failOn = {ALLOC_G, TRAP})\n+    public void test116() {\n+        fEmpty1 = fEmpty4;\n+        fEmpty2 = fEmpty1;\n+        fEmpty3 = fEmpty2;\n+        fEmpty4 = fEmpty3;\n+    }\n+\n+    @Run(test = \"test116\")\n+    public void test116_verifier() {\n+        test116();\n+        Asserts.assertEquals(fEmpty1, fEmpty2);\n+        Asserts.assertEquals(fEmpty2, fEmpty3);\n+        Asserts.assertEquals(fEmpty3, fEmpty4);\n+    }\n+\n+    \/\/ Test array loads\/stores with empty inline types\n+    @Test\n+    @IR(failOn = {ALLOC_G})\n+    public MyValueEmpty test117(MyValueEmpty[] arr1, MyValueEmpty[] arr2) {\n+        arr1[0] = arr2[0];\n+        arr2[0] = new MyValueEmpty();\n+        return arr1[0];\n+    }\n+\n+    @Run(test = \"test117\")\n+    public void test117_verifier() {\n+        MyValueEmpty[] arr1 = new MyValueEmpty[] { new MyValueEmpty() };\n+        MyValueEmpty res = test117(arr1, arr1);\n+        Asserts.assertEquals(res, new MyValueEmpty());\n+        Asserts.assertEquals(arr1[0], new MyValueEmpty());\n+    }\n+\n+    \/\/ Test acmp with empty inline types\n+    @Test\n+    public boolean test118(MyValueEmpty v1, MyValueEmpty v2, Object o1) {\n+        return (v1 == v2) && (v2 == o1);\n+    }\n+\n+    @Run(test = \"test118\")\n+    public void test118_verifier() {\n+        boolean res = test118(new MyValueEmpty(), new MyValueEmpty(), new MyValueEmpty());\n+        Asserts.assertTrue(res);\n+    }\n+\n+    @ImplicitlyConstructible\n+    @LooselyConsistentValue\n+    static value class EmptyContainer {\n+        @NullRestricted\n+        private MyValueEmpty empty = new MyValueEmpty();\n+    }\n+\n+    @ImplicitlyConstructible\n+    @LooselyConsistentValue\n+    static value class MixedContainer {\n+        public int val = 0;\n+        @NullRestricted\n+        private EmptyContainer empty = new EmptyContainer();\n+    }\n+\n+    @NullRestricted\n+    static final MyValueEmpty empty = new MyValueEmpty();\n+\n+    @NullRestricted\n+    static final EmptyContainer emptyC = new EmptyContainer();\n+\n+    @NullRestricted\n+    static final MixedContainer mixedContainer = new MixedContainer();\n+\n+    \/\/ Test re-allocation of empty inline type array during deoptimization\n+    @Test\n+    @IR(failOn = {ALLOC_G})\n+    public void test119(boolean deopt, Method m) {\n+        MyValueEmpty[]   array1 = new MyValueEmpty[] { empty };\n+        EmptyContainer[] array2 = (EmptyContainer[])ValueClass.newNullRestrictedArray(EmptyContainer.class, 1);\n+        array2[0] = emptyC;\n+        MixedContainer[] array3 = (MixedContainer[])ValueClass.newNullRestrictedArray(MixedContainer.class, 1);\n+        array3[0] = mixedContainer;\n+        if (deopt) {\n+            \/\/ uncommon trap\n+            TestFramework.deoptimize(m);\n+        }\n+        Asserts.assertEquals(array1[0], empty);\n+        Asserts.assertEquals(array2[0], emptyC);\n+        Asserts.assertEquals(array3[0], mixedContainer);\n+    }\n+\n+    @Run(test = \"test119\")\n+    public void test119_verifier(RunInfo info) {\n+        test119(!info.isWarmUp(), info.getTest());\n+    }\n+\n+    \/\/ Test removal of empty inline type field stores\n+    @Test\n+    @IR(failOn = {ALLOC_G, LOAD, STORE, FIELD_ACCESS, NULL_CHECK_TRAP, TRAP})\n+    public void test120() {\n+        fEmpty1 = empty;\n+        fEmpty3 = empty;\n+        \/\/ fEmpty2 and fEmpty4 could be null, store can't be removed\n+    }\n+\n+    @Run(test = \"test120\")\n+    public void test120_verifier() {\n+        test120();\n+        Asserts.assertEquals(fEmpty1, empty);\n+        Asserts.assertEquals(fEmpty2, empty);\n+    }\n+\n+    \/\/ Test removal of empty inline type field loads\n+    @Test\n+    @IR(failOn = {ALLOC_G, LOAD, STORE, FIELD_ACCESS, NULL_CHECK_TRAP, TRAP})\n+    public boolean test121() {\n+        return fEmpty1.equals(fEmpty3);\n+        \/\/ fEmpty2 and fEmpty4 could be null, load can't be removed\n+    }\n+\n+    @Run(test = \"test121\")\n+    public void test121_verifier() {\n+        boolean res = test121();\n+        Asserts.assertTrue(res);\n+    }\n+\n+    \/\/ Verify that empty inline type field loads check for null holder\n+    @Test\n+    @IR(failOn = {ALLOC_G})\n+    public MyValueEmpty test122(TestLWorld t) {\n+        return t.fEmpty3;\n+    }\n+\n+    @Run(test = \"test122\")\n+    public void test122_verifier() {\n+        MyValueEmpty res = test122(this);\n+        Asserts.assertEquals(res, new MyValueEmpty());\n+        try {\n+            test122(null);\n+            throw new RuntimeException(\"No NPE thrown\");\n+        } catch (NullPointerException e) {\n+            \/\/ Expected\n+        }\n+    }\n+\n+    \/\/ Verify that empty inline type field stores check for null holder\n+    @Test\n+    @IR(failOn = {ALLOC_G})\n+    public void test123(TestLWorld t) {\n+        t.fEmpty3 = new MyValueEmpty();\n+    }\n+\n+    @Run(test = \"test123\")\n+    public void test123_verifier() {\n+        test123(this);\n+        Asserts.assertEquals(fEmpty3, new MyValueEmpty());\n+        try {\n+            test123(null);\n+            throw new RuntimeException(\"No NPE thrown\");\n+        } catch (NullPointerException e) {\n+            \/\/ Expected\n+        }\n+    }\n+\n+    \/\/ acmp doesn't need substitutability test when one input is known\n+    \/\/ not to be a value type\n+    @Test\n+    @IR(failOn = SUBSTITUTABILITY_TEST)\n+    public boolean test124(NonValueClass o1, Object o2) {\n+        return o1 == o2;\n+    }\n+\n+    @Run(test = \"test124\")\n+    public void test124_verifier() {\n+        NonValueClass obj = new NonValueClass(rI);\n+        test124(obj, obj);\n+        test124(obj, testValue1);\n+    }\n+\n+    \/\/ acmp doesn't need substitutability test when one input is null\n+    @Test\n+    @IR(failOn = {SUBSTITUTABILITY_TEST})\n+    public boolean test125(Object o1) {\n+        Object o2 = null;\n+        return o1 == o2;\n+    }\n+\n+    @Run(test = \"test125\")\n+    public void test125_verifier() {\n+        test125(testValue1);\n+        test125(null);\n+    }\n+\n+    \/\/ Test inline type that can only be scalarized after loop opts\n+    @Test\n+    @IR(failOn = {ALLOC_G, LOAD, STORE})\n+    public long test126(boolean trap) {\n+        MyValue2 nonNull = MyValue2.createWithFieldsInline(rI, rD);\n+        MyValue2 val = null;\n+\n+        for (int i = 0; i < 4; i++) {\n+            if ((i % 2) == 0) {\n+                val = nonNull;\n+            }\n+        }\n+        \/\/ 'val' is always non-null here but that's only known after loop opts\n+        if (trap) {\n+            \/\/ Uncommon trap with an inline input that can only be scalarized after loop opts\n+            return val.hash();\n+        }\n+        return 0;\n+    }\n+\n+    @Run(test = \"test126\")\n+    @Warmup(10000)\n+    public void test126_verifier(RunInfo info) {\n+        long res = test126(false);\n+        Asserts.assertEquals(res, 0L);\n+        if (!info.isWarmUp()) {\n+            res = test126(true);\n+            Asserts.assertEquals(res, testValue2.hash());\n+        }\n+    }\n+\n+    \/\/ Same as test126 but with interface type\n+    @Test\n+    @IR(failOn = {ALLOC_G, LOAD, STORE})\n+    public long test127(boolean trap) {\n+        MyValue2 nonNull = MyValue2.createWithFieldsInline(rI, rD);\n+        MyInterface val = null;\n+\n+        for (int i = 0; i < 4; i++) {\n+            if ((i % 2) == 0) {\n+                val = nonNull;\n+            }\n+        }\n+        \/\/ 'val' is always non-null here but that's only known after loop opts\n+        if (trap) {\n+            \/\/ Uncommon trap with an inline input that can only be scalarized after loop opts\n+            return val.hash();\n+        }\n+        return 0;\n+    }\n+\n+    @Run(test = \"test127\")\n+    @Warmup(10000)\n+    public void test127_verifier(RunInfo info) {\n+        long res = test127(false);\n+        Asserts.assertEquals(res, 0L);\n+        if (!info.isWarmUp()) {\n+            res = test127(true);\n+            Asserts.assertEquals(res, testValue2.hash());\n+        }\n+    }\n+\n+    \/\/ Test inline type that can only be scalarized after CCP\n+    @Test\n+    @IR(failOn = {ALLOC_G, LOAD, STORE})\n+    public long test128(boolean trap) {\n+        MyValue2 nonNull = MyValue2.createWithFieldsInline(rI, rD);\n+        MyValue2 val = null;\n+\n+        int limit = 2;\n+        for (; limit < 4; limit *= 2);\n+        for (int i = 2; i < limit; i++) {\n+            val = nonNull;\n+        }\n+        \/\/ 'val' is always non-null here but that's only known after CCP\n+        if (trap) {\n+            \/\/ Uncommon trap with an inline input that can only be scalarized after CCP\n+            return val.hash();\n+        }\n+        return 0;\n+    }\n+\n+    @Run(test = \"test128\")\n+    @Warmup(10000)\n+    public void test128_verifier(RunInfo info) {\n+        long res = test128(false);\n+        Asserts.assertEquals(res, 0L);\n+        if (!info.isWarmUp()) {\n+            res = test128(true);\n+            Asserts.assertEquals(res, testValue2.hash());\n+        }\n+    }\n+\n+    \/\/ Same as test128 but with interface type\n+    @Test\n+    @IR(failOn = {ALLOC_G, LOAD, STORE})\n+    public long test129(boolean trap) {\n+        MyValue2 nonNull = MyValue2.createWithFieldsInline(rI, rD);\n+        MyInterface val = null;\n+\n+        int limit = 2;\n+        for (; limit < 4; limit *= 2);\n+        for (int i = 0; i < limit; i++) {\n+            val = nonNull;\n+        }\n+        \/\/ 'val' is always non-null here but that's only known after CCP\n+        if (trap) {\n+            \/\/ Uncommon trap with an inline input that can only be scalarized after CCP\n+            return val.hash();\n+        }\n+        return 0;\n+    }\n+\n+    @Run(test = \"test129\")\n+    @Warmup(10000)\n+    public void test129_verifier(RunInfo info) {\n+        long res = test129(false);\n+        Asserts.assertEquals(res, 0L);\n+        if (!info.isWarmUp()) {\n+            res = test129(true);\n+            Asserts.assertEquals(res, testValue2.hash());\n+        }\n+    }\n+\n+    \/\/ Lock on inline type (known after inlining)\n+    @ForceInline\n+    public Object test130_inlinee() {\n+        return MyValue1.createWithFieldsInline(rI, rL);\n+    }\n+\n+    @Test\n+    @IR(failOn = {LOAD},\n+        \/\/ LockNode keeps MyValue1 allocation alive up until macro expansion which in turn keeps MyValue2\n+        \/\/ alloc alive. Although the MyValue1 allocation is removed (unused), MyValue2 is expanded first\n+        \/\/ and therefore stays.\n+        counts = {ALLOC, \"<= 1\", STORE, \"<= 1\"})\n+    public void test130() {\n+        Object obj = test130_inlinee();\n+        synchronized (obj) {\n+            throw new RuntimeException(\"test130 failed: synchronization on inline type should not succeed\");\n+        }\n+    }\n+\n+    @Run(test = \"test130\")\n+    public void test130_verifier() {\n+        try {\n+            test130();\n+            throw new RuntimeException(\"test130 failed: no exception thrown\");\n+        } catch (IdentityException ex) {\n+            \/\/ Expected\n+        }\n+    }\n+\n+    \/\/ Same as test130 but with field load instead of allocation\n+    @ForceInline\n+    public Object test131_inlinee() {\n+        return testValue1;\n+    }\n+\n+    @Test\n+    @IR(failOn = {ALLOC_G})\n+    public void test131() {\n+        Object obj = test131_inlinee();\n+        synchronized (obj) {\n+            throw new RuntimeException(\"test131 failed: synchronization on inline type should not succeed\");\n+        }\n+    }\n+\n+    @Run(test = \"test131\")\n+    public void test131_verifier() {\n+        try {\n+            test131();\n+            throw new RuntimeException(\"test131 failed: no exception thrown\");\n+        } catch (IdentityException ex) {\n+            \/\/ Expected\n+        }\n+    }\n+\n+    \/\/ Test locking on object that is known to be an inline type only after CCP\n+    @Test\n+    @IR(failOn = {ALLOC, LOAD, STORE})\n+    public void test132() {\n+        MyValue2 vt = MyValue2.createWithFieldsInline(rI, rD);\n+        Object obj = new NonValueClass(42);\n+\n+        int limit = 2;\n+        for (; limit < 4; limit *= 2);\n+        for (int i = 2; i < limit; i++) {\n+            obj = vt;\n+        }\n+        synchronized (obj) {\n+            throw new RuntimeException(\"test132 failed: synchronization on inline type should not succeed\");\n+        }\n+    }\n+\n+    @Run(test = \"test132\")\n+    @Warmup(10000)\n+    public void test132_verifier() {\n+        try {\n+            test132();\n+            throw new RuntimeException(\"test132 failed: no exception thrown\");\n+        } catch (IdentityException ex) {\n+            \/\/ Expected\n+        }\n+    }\n+\n+    \/\/ Test conditional locking on inline type and non-escaping object\n+    @Test\n+    public void test133(boolean b) {\n+        Object obj = b ? new NonValueClass(rI) : MyValue2.createWithFieldsInline(rI, rD);\n+        synchronized (obj) {\n+            if (!b) {\n+                throw new RuntimeException(\"test133 failed: synchronization on inline type should not succeed\");\n+            }\n+        }\n+    }\n+\n+    @Run(test = \"test133\")\n+    public void test133_verifier() {\n+        test133(true);\n+        try {\n+            test133(false);\n+            throw new RuntimeException(\"test133 failed: no exception thrown\");\n+        } catch (IdentityException ex) {\n+            \/\/ Expected\n+        }\n+    }\n+\n+    \/\/ Variant with non-scalarized inline type\n+    @Test\n+    @IR(failOn = {ALLOC_G})\n+    public void test134(boolean b) {\n+        Object obj = null;\n+        if (b) {\n+            obj = MyValue2.createWithFieldsInline(rI, rD);\n+        }\n+        synchronized (obj) {\n+\n+        }\n+    }\n+\n+    @Run(test = \"test134\")\n+    public void test134_verifier() {\n+        try {\n+            test134(true);\n+            throw new RuntimeException(\"test134 failed: no exception thrown\");\n+        } catch (IdentityException ex) {\n+            \/\/ Expected\n+        }\n+    }\n+\n+    \/\/ Test that acmp of the same inline object is removed\n+    @Test\n+    @IR(failOn = {ALLOC_G, LOAD, STORE, NULL_CHECK_TRAP, TRAP})\n+    public boolean test135() {\n+        MyValue1 val = MyValue1.createWithFieldsInline(rI, rL);\n+        return val == val;\n+    }\n+\n+    @Run(test = \"test135\")\n+    public void test135_verifier() {\n+        Asserts.assertTrue(test135());\n+    }\n+\n+    \/\/ Same as test135 but with null\n+    @Test\n+    @IR(failOn = {ALLOC_G, LOAD, STORE, NULL_CHECK_TRAP, TRAP})\n+    public boolean test136(boolean b) {\n+        MyValue1 val = MyValue1.createWithFieldsInline(rI, rL);\n+        if (b) {\n+            val = null;\n+        }\n+        return val == val;\n+    }\n+\n+    @Run(test = \"test136\")\n+    public void test136_verifier() {\n+        Asserts.assertTrue(test136(false));\n+        Asserts.assertTrue(test136(true));\n+    }\n+\n+    \/\/ Test that acmp of different inline objects with same content is removed\n+    @Test\n+    \/\/ TODO 8228361\n+    \/\/ @IR(failOn = {ALLOC_G, LOAD, STORE, NULL_CHECK_TRAP, TRAP})\n+    public boolean test137(int i) {\n+        MyValue2 val1 = MyValue2.createWithFieldsInline(i, rD);\n+        MyValue2 val2 = MyValue2.createWithFieldsInline(i, rD);\n+        return val1 == val2;\n+    }\n+\n+    @Run(test = \"test137\")\n+    public void test137_verifier() {\n+        Asserts.assertTrue(test137(rI));\n+    }\n+\n+    \/\/ Same as test137 but with null\n+    @Test\n+    \/\/ TODO 8228361\n+    \/\/ @IR(failOn = {ALLOC_G, LOAD, STORE, NULL_CHECK_TRAP, TRAP})\n+    public boolean test138(int i, boolean b) {\n+        MyValue2 val1 = MyValue2.createWithFieldsInline(i, rD);\n+        MyValue2 val2 = MyValue2.createWithFieldsInline(i, rD);\n+        if (b) {\n+            val1 = null;\n+            val2 = null;\n+        }\n+        return val1 == val2;\n+    }\n+\n+    @Run(test = \"test138\")\n+    public void test138_verifier() {\n+        Asserts.assertTrue(test138(rI, false));\n+        Asserts.assertTrue(test138(rI, true));\n+    }\n+\n+    @ImplicitlyConstructible\n+    @LooselyConsistentValue\n+    static value class Test139Value {\n+        Object obj = null;\n+        @NullRestricted\n+        MyValueEmpty empty = new MyValueEmpty();\n+    }\n+\n+    @ImplicitlyConstructible\n+    @LooselyConsistentValue\n+    static value class Test139Wrapper {\n+        @NullRestricted\n+        Test139Value value = new Test139Value();\n+    }\n+\n+    @Test\n+    @IR(failOn = {ALLOC_G, LOAD, STORE, TRAP})\n+    public MyValueEmpty test139() {\n+        Test139Wrapper w = new Test139Wrapper();\n+        return w.value.empty;\n+    }\n+\n+    @Run(test = \"test139\")\n+    public void test139_verifier() {\n+        MyValueEmpty empty = test139();\n+        Asserts.assertEquals(empty, new MyValueEmpty());\n+    }\n+\n+    \/\/ Test calling a method on a loaded but not linked inline type\n+    @ImplicitlyConstructible\n+    @LooselyConsistentValue\n+    value class Test140Value {\n+        int x = 0;\n+\n+        public int get() {\n+            return x;\n+        }\n+    }\n+\n+    @Test\n+    @IR(failOn = {ALLOC_G})\n+    public int test140() {\n+        Test140Value vt = new Test140Value();\n+        return vt.get();\n+    }\n+\n+    @Run(test = \"test140\")\n+    @Warmup(0)\n+    public void test140_verifier() {\n+        int result = test140();\n+        Asserts.assertEquals(result, 0);\n+    }\n+\n+    \/\/ Test calling a method on a linked but not initialized inline type\n+    @ImplicitlyConstructible\n+    @LooselyConsistentValue\n+    value class Test141Value {\n+        int x = 0;\n+\n+        public int get() {\n+            return x;\n+        }\n+    }\n+\n+    @Test\n+    @IR(failOn = {ALLOC_G})\n+    public int test141() {\n+        Test141Value vt = new Test141Value();\n+        return vt.get();\n+    }\n+\n+    @Run(test = \"test141\")\n+    @Warmup(0)\n+    public void test141_verifier() {\n+        int result = test141();\n+        Asserts.assertEquals(result, 0);\n+    }\n+\n+    \/\/ Test that virtual calls on inline type receivers are properly inlined\n+    @Test\n+    @IR(failOn = {ALLOC_G, LOAD, STORE})\n+    public long test142() {\n+        MyValue2 nonNull = MyValue2.createWithFieldsInline(rI, rD);\n+        MyInterface val = null;\n+\n+        for (int i = 0; i < 4; i++) {\n+            if ((i % 2) == 0) {\n+                val = nonNull;\n+            }\n+        }\n+        return val.hash();\n+    }\n+\n+    @Run(test = \"test142\")\n+    public void test142_verifier() {\n+        long res = test142();\n+        Asserts.assertEquals(res, testValue2.hash());\n+    }\n+\n+    \/\/ Test merging of buffered default and non-default inline types\n+    @Test\n+    @IR(failOn = {ALLOC_G})\n+    public Object test144(int i) {\n+        if (i == 0) {\n+            return MyValue1.createDefaultInline();\n+        } else if (i == 1) {\n+            return testValue1;\n+        } else {\n+            return MyValue1.createDefaultInline();\n+        }\n+    }\n+\n+    @Run(test = \"test144\")\n+    public void test144_verifier() {\n+        Asserts.assertEquals(test144(0), MyValue1.createDefaultInline());\n+        Asserts.assertEquals(test144(1), testValue1);\n+        Asserts.assertEquals(test144(2), MyValue1.createDefaultInline());\n+    }\n+\n+    \/\/ Tests writing an array element with a (statically known) incompatible type\n+    private static final MethodHandle setArrayElementIncompatibleRef = OldInstructionHelper.loadCode(MethodHandles.lookup(),\n+        \"setArrayElementIncompatibleRef\",\n+        MethodType.methodType(void.class, TestLWorld.class, MyValue1[].class, int.class, MyValue2.class),\n+        CODE -> {\n+            CODE.\n+            aload_1().\n+            iload_2().\n+            aload_3().\n+            aastore().\n+            return_();\n+        });\n+\n+    \/\/ Test inline type connected to result node\n+    @Test\n+    @IR(failOn = {ALLOC_G})\n+    public MyValue1 test146(Object obj) {\n+        return (MyValue1)obj;\n+    }\n+\n+    @Run(test = \"test146\")\n+    @Warmup(10000)\n+    public void test146_verifier() {\n+        Asserts.assertEQ(test146(testValue1), testValue1);\n+    }\n+\n+    @ForceInline\n+    public Object test148_helper(Object obj) {\n+        return (MyValue1)obj;\n+    }\n+\n+    \/\/ Same as test146 but with helper method\n+    @Test\n+    public Object test148(Object obj) {\n+        return test148_helper(obj);\n+    }\n+\n+    @Run(test = \"test148\")\n+    @Warmup(10000)\n+    public void test148_verifier() {\n+        Asserts.assertEQ(test148(testValue1), testValue1);\n+    }\n+\n+    @ForceInline\n+    public Object test149_helper(Object obj) {\n+        return (MyValue1)obj;\n+    }\n+\n+    \/\/ Same as test147 but with helper method\n+    @Test\n+    public Object test149(Object obj) {\n+        return test149_helper(obj);\n+    }\n+\n+    @Run(test = \"test149\")\n+    @Warmup(10000)\n+    public void test149_verifier() {\n+        Asserts.assertEQ(test149(testValue1), testValue1);\n+        Asserts.assertEQ(test149(null), null);\n+    }\n+\n+    \/\/ Test post-parse call devirtualization with inline type receiver\n+    @Test\n+    @IR(applyIf = {\"InlineTypePassFieldsAsArgs\", \"true\"},\n+        failOn = {ALLOC})\n+    @IR(failOn = {compiler.lib.ir_framework.IRNode.DYNAMIC_CALL_OF_METHOD, \"MyValue2::hash\"},\n+        counts = {compiler.lib.ir_framework.IRNode.STATIC_CALL_OF_METHOD, \"MyValue2::hash\", \"= 1\"})\n+    public long test150() {\n+        MyValue2 val = MyValue2.createWithFieldsInline(rI, rD);\n+        MyInterface receiver = MyValue1.createWithFieldsInline(rI, rL);\n+\n+        for (int i = 0; i < 4; i++) {\n+            if ((i % 2) == 0) {\n+                receiver = val;\n+            }\n+        }\n+        \/\/ Trigger post parse call devirtualization (strength-reducing\n+        \/\/ virtual calls to direct calls).\n+        return receiver.hash();\n+    }\n+\n+    @Run(test = \"test150\")\n+    public void test150_verifier() {\n+        Asserts.assertEquals(test150(), testValue2.hash());\n+    }\n+\n+\/\/ TODO 8336003 This triggers #  assert(false) failed: Should have been buffered\n+\/*\n+    \/\/ Same as test150 but with val not being allocated in the scope of the method\n+    @Test\n+    @IR(failOn = {compiler.lib.ir_framework.IRNode.DYNAMIC_CALL_OF_METHOD, \"MyValue2::hash\"},\n+        counts = {compiler.lib.ir_framework.IRNode.STATIC_CALL_OF_METHOD, \"MyValue2::hash\", \"= 1\"})\n+    public long test151(MyValue2 val) {\n+        MyAbstract receiver = MyValue1.createWithFieldsInline(rI, rL);\n+\n+        for (int i = 0; i < 100; i++) {\n+            if ((i % 2) == 0) {\n+                receiver = val;\n+            }\n+        }\n+        \/\/ Trigger post parse call devirtualization (strength-reducing\n+        \/\/ virtual calls to direct calls).\n+        return receiver.hash();\n+    }\n+\n+    @Run(test = \"test151\")\n+    @Warmup(0) \/\/ Make sure there is no receiver type profile\n+    public void test151_verifier() {\n+        Asserts.assertEquals(test151(testValue2), testValue2.hash());\n+    }\n+*\/\n+\n+    static interface MyInterface2 {\n+        public int val();\n+    }\n+\n+    @ImplicitlyConstructible\n+    static abstract value class MyAbstract2 implements MyInterface2 {\n+\n+    }\n+\n+    static class MyClass152 extends MyAbstract2 {\n+        private int val;\n+\n+        @ForceInline\n+        public MyClass152(int val) {\n+            this.val = val;\n+        }\n+\n+        @Override\n+        public int val() {\n+            return val;\n+        }\n+    }\n+\n+    @ImplicitlyConstructible\n+    @LooselyConsistentValue\n+    static value class MyValue152 extends MyAbstract2 {\n+        private int unused = 0; \/\/ Make sure sub-offset of val is field non-zero\n+        private int val;\n+\n+        @ForceInline\n+        public MyValue152(int val) {\n+            this.val = val;\n+        }\n+\n+        @Override\n+        public int val() {\n+            return val;\n+        }\n+    }\n+\n+    @ImplicitlyConstructible\n+    @LooselyConsistentValue\n+    static value class MyWrapper152 {\n+        private int unused = 0; \/\/ Make sure sub-offset of val field is non-zero\n+        @NullRestricted\n+        MyValue152 val;\n+\n+        @ForceInline\n+        public MyWrapper152(MyInterface2 val) {\n+            this.val = (MyValue152)val;\n+        }\n+    }\n+\n+    \/\/ Test that checkcast with speculative type does not break scalarization in return\n+    @Test\n+    public MyWrapper152 test152(MyInterface2 val) {\n+        return new MyWrapper152(val);\n+    }\n+\n+    @Run(test = \"test152\")\n+    @Warmup(10000) \/\/ Make sure profile information is available at cast\n+    public void test152_verifier() {\n+        MyClass152 unused = new MyClass152(rI);\n+        MyValue152 val = new MyValue152(rI);\n+        Asserts.assertEquals(test152(val).val, val);\n+    }\n+\n+    @DontInline\n+    static void test153_helper(MyWrapper152 arg) {\n+\n+    }\n+\n+    \/\/ Test that checkcast with speculative type does not prevent scalarization in args\n+    @Test\n+    public void test153(MyInterface2 val) {\n+        test153_helper(new MyWrapper152(val));\n+    }\n+\n+    @Run(test = \"test153\")\n+    @Warmup(10000) \/\/ Make sure profile information is available at cast\n+    public void test153_verifier() {\n+        MyClass152 unused = new MyClass152(rI);\n+        MyValue152 val = new MyValue152(rI);\n+        test153(val);\n+    }\n+\n+    \/\/ Test that checkcast with speculative type enables scalarization\n+    @Test\n+    @IR(failOn = {ALLOC_G, STORE})\n+    public int test154(Method m, MyInterface2 val, boolean b1, boolean b2) {\n+        MyInterface2 obj = new MyValue152(rI);\n+        if (b1) {\n+            \/\/ Speculative cast to MyValue152 enables scalarization\n+            obj = (MyAbstract2)val;\n+        }\n+        if (b2) {\n+            \/\/ Uncommon trap\n+            TestFramework.deoptimize(m);\n+            return obj.val();\n+        }\n+        return -1;\n+    }\n+\n+    @Run(test = \"test154\")\n+    @Warmup(10000) \/\/ Make sure profile information is available at cast\n+    public void test154_verifier(RunInfo info) {\n+        MyClass152 unused = new MyClass152(rI);\n+        MyValue152 val = new MyValue152(rI);\n+        Asserts.assertEquals(test154(info.getTest(), val, false, false), -1);\n+        Asserts.assertEquals(test154(info.getTest(), val, true, false), -1);\n+        if (!info.isWarmUp()) {\n+            Asserts.assertEquals(test154(info.getTest(), val, false, true), rI);\n+        }\n+    }\n+\n+    \/\/ Same as test154 but with null val\n+    @Test\n+    @IR(failOn = {ALLOC_G, STORE})\n+    public int test155(Method m, MyInterface2 val, boolean b1, boolean b2) {\n+        MyInterface2 obj = new MyValue152(rI);\n+        if (b1) {\n+            \/\/ Speculative cast to MyValue152 enables scalarization\n+            obj = (MyAbstract2)val;\n+        }\n+        if (b2) {\n+            \/\/ Uncommon trap\n+            TestFramework.deoptimize(m);\n+            return obj.val();\n+        }\n+        return -1;\n+    }\n+\n+    @Run(test = \"test155\")\n+    @Warmup(10000) \/\/ Make sure profile information is available at cast\n+    public void test155_verifier(RunInfo info) {\n+        MyClass152 unused = new MyClass152(rI);\n+        MyValue152 val = new MyValue152(rI);\n+        Asserts.assertEquals(test155(info.getTest(), val, false, false), -1);\n+        Asserts.assertEquals(test155(info.getTest(), val, true, false), -1);\n+        Asserts.assertEquals(test155(info.getTest(), null, true, false), -1);\n+        if (!info.isWarmUp()) {\n+            Asserts.assertEquals(test155(info.getTest(), val, false, true), rI);\n+        }\n+    }\n+\n+    @NullRestricted\n+    final static MyValue1 test157Cache = MyValue1.createWithFieldsInline(rI, 0);\n+\n+    \/\/ Test merging buffered inline type from field load with non-buffered inline type\n+    @Test\n+    public MyValue1 test157(long val) {\n+        return (val == 0L) ? test157Cache : MyValue1.createWithFieldsInline(rI, val);\n+    }\n+\n+    @Run(test = \"test157\")\n+    public void test157_verifier() {\n+        Asserts.assertEquals(test157(0), test157Cache);\n+        Asserts.assertEquals(test157(rL).hash(), testValue1.hash());\n+    }\n+\n+    @NullRestricted\n+    static MyValue1 test158Cache = MyValue1.createWithFieldsInline(rI, 0);\n+\n+    \/\/ Same as test157 but with non-final field load\n+    @Test\n+    public MyValue1 test158(long val) {\n+        return (val == 0L) ? test158Cache : MyValue1.createWithFieldsInline(rI, val);\n+    }\n+\n+    @Run(test = \"test158\")\n+    public void test158_verifier() {\n+        Asserts.assertEquals(test158(0), test158Cache);\n+        Asserts.assertEquals(test158(rL).hash(), testValue1.hash());\n+    }\n+\n+    \/\/ Verify that cast that with incompatible types is properly handled\n+    @Test\n+    public void test160(NonValueClass arg) {\n+        Object tmp = arg;\n+        MyValue1 res = (MyValue1)tmp;\n+    }\n+\n+    @Run(test = \"test160\")\n+    @Warmup(10000)\n+    public void test160_verifier(RunInfo info) {\n+        try {\n+            test160(new NonValueClass(42));\n+            throw new RuntimeException(\"No CCE thrown\");\n+        } catch (ClassCastException e) {\n+            \/\/ Expected\n+        }\n+        test160(null);\n+    }\n+\n+    abstract value static class AbstractValueClassSingleSubclass {\n+    }\n+\n+    value static class UniqueValueSubClass extends AbstractValueClassSingleSubclass {\n+        int x = 34;\n+    }\n+\n+    static AbstractValueClassSingleSubclass abstractValueClassSingleSubclass = new UniqueValueSubClass();\n+\n+    @Test\n+    public void testUniqueConcreteValueSubKlass(boolean flag) {\n+        \/\/ C2 should recognize that even though we do not know the exact layout of the underlying inline type of the\n+        \/\/ abstract field abstractValueClassSingleSubclass (i.e. cannot scalarize), we only have a unique concrete sub\n+        \/\/ class from which we know at compile time whether it can be scalarized or not. This unique sub class\n+        \/\/ optimization was missing, resulting in a missing InlineTypeNode assertion failure.\n+        doNothing(abstractValueClassSingleSubclass, flag ? 23 : 34);\n+    }\n+\n+    void doNothing(Object a, int i) {}\n+\n+    @Run(test = \"testUniqueConcreteValueSubKlass\")\n+    public void testUniqueConcreteValueSubKlass_verifier() {\n+        testUniqueConcreteValueSubKlass(true);\n+    }\n+\n+    static value class MyValueContainer {\n+        private final Object value;\n+\n+        private MyValueContainer(Object value) {\n+            this.value = value;\n+        }\n+    }\n+\n+    static value class MyValue161 {\n+        int x = 0;\n+    }\n+\n+    \/\/ Test merging value classes with Object fields\n+    @Test\n+    public MyValueContainer test161(boolean b) {\n+        MyValueContainer res = b ? new MyValueContainer(new MyValue161()) : null;\n+        \/\/ Cast to verify that merged values are of correct type\n+        Object obj = b ? (MyValue161)res.value : null;\n+        return res;\n+    }\n+\n+    @Run(test = \"test161\")\n+    public void test161_verifier() {\n+        Asserts.assertEquals(test161(true), new MyValueContainer(new MyValue161()));\n+        Asserts.assertEquals(test161(false), null);\n+    }\n+\n+    @Test\n+    public MyValueContainer test162(boolean b) {\n+        MyValueContainer res = b ? null : new MyValueContainer(new MyValue161());\n+        \/\/ Cast to verify that merged values are of correct type\n+        Object obj = b ? null : (MyValue161)res.value;\n+        return res;\n+    }\n+\n+    @Run(test = \"test162\")\n+    public void test162_verifier() {\n+        Asserts.assertEquals(test162(true), null);\n+        Asserts.assertEquals(test162(false), new MyValueContainer(new MyValue161()));\n+    }\n+\n+    @Test\n+    public MyValueContainer test163(boolean b) {\n+        MyValueContainer res = b ? new MyValueContainer(new MyValue161()) : new MyValueContainer(null);\n+        \/\/ Cast to verify that merged values are of correct type\n+        Object obj = b ? (MyValue161)res.value : (MyValue161)res.value;\n+        return res;\n+    }\n+\n+    @Run(test = \"test163\")\n+    public void test163_verifier() {\n+        Asserts.assertEquals(test163(true), new MyValueContainer(new MyValue161()));\n+        Asserts.assertEquals(test163(false), new MyValueContainer(null));\n+    }\n+\n+    @Test\n+    public MyValueContainer test164(boolean b) {\n+        MyValueContainer res = b ? new MyValueContainer(null) : new MyValueContainer(new MyValue161());\n+        \/\/ Cast to verify that merged values are of correct type\n+        Object obj = b ? (MyValue161)res.value : (MyValue161)res.value;\n+        return res;\n+    }\n+\n+    @Run(test = \"test164\")\n+    public void test164_verifier() {\n+        Asserts.assertEquals(test164(true), new MyValueContainer(null));\n+        Asserts.assertEquals(test164(false), new MyValueContainer(new MyValue161()));\n+    }\n+\n+    @Test\n+    public MyValueContainer test165(boolean b) {\n+        MyValueContainer res = b ? new MyValueContainer(new MyValue161()) : new MyValueContainer(42);\n+        \/\/ Cast to verify that merged values are of correct type\n+        Object obj = b ? (MyValue161)res.value : (Integer)res.value;\n+        return res;\n+    }\n+\n+    @Run(test = \"test165\")\n+    public void test165_verifier() {\n+        Asserts.assertEquals(test165(true), new MyValueContainer(new MyValue161()));\n+        Asserts.assertEquals(test165(false), new MyValueContainer(42));\n+    }\n+\n+    @Test\n+    public MyValueContainer test166(boolean b) {\n+        MyValueContainer res = b ? new MyValueContainer(42) : new MyValueContainer(new MyValue161());\n+        \/\/ Cast to verify that merged values are of correct type\n+        Object obj = b ? (Integer)res.value : (MyValue161)res.value;\n+        return res;\n+    }\n+\n+    @Run(test = \"test166\")\n+    public void test166_verifier() {\n+        Asserts.assertEquals(test166(true), new MyValueContainer(42));\n+        Asserts.assertEquals(test166(false), new MyValueContainer(new MyValue161()));\n+    }\n+\n+    \/\/ Verify that monitor information in JVMState is correct at method exit\n+    @Test\n+    public synchronized Object test167() {\n+        return MyValue1.createWithFieldsInline(rI, rL); \/\/ Might trigger buffering which requires JVMState\n+    }\n+\n+    @Run(test = \"test167\")\n+    public void test167_verifier() {\n+        Asserts.assertEquals(((MyValue1)test167()).hash(), hash());\n+    }\n+}\n","filename":"test\/hotspot\/jtreg\/compiler\/valhalla\/inlinetypes\/TestLWorld.java","additions":4500,"deletions":0,"binary":false,"changes":4500,"status":"added"},{"patch":"@@ -522,0 +522,3 @@\n+java\/lang\/ModuleLayer\/LayerControllerTest.java                  8337048 generic-all\n+java\/lang\/ModuleLayer\/BasicLayerTest.java                       8337048 generic-all\n+\n@@ -739,0 +742,4 @@\n+com\/sun\/jdi\/cds\/CDSBreakpointTest.java                          8304168 generic-all\n+com\/sun\/jdi\/cds\/CDSDeleteAllBkptsTest.java                      8304168 generic-all\n+com\/sun\/jdi\/cds\/CDSFieldWatchpoints.java                        8304168 generic-all\n+\n@@ -782,0 +789,6 @@\n+jdk\/classfile\/SwapTest.java                                     8308778 generic-all\n+jdk\/classfile\/LowAdaptTest.java                                 8308778 generic-all\n+jdk\/classfile\/BuilderBlockTest.java                             8308778 generic-all\n+jdk\/classfile\/BuilderTryCatchTest.java                          8308778 generic-all\n+jdk\/classfile\/PrimitiveClassConstantTest.java                   8310649 generic-all\n+\n@@ -826,0 +839,3 @@\n+\n+# valhalla\n+jdk\/jfr\/event\/runtime\/TestSyncOnValueBasedClassEvent.java 8328777 generic-all\n","filename":"test\/jdk\/ProblemList.txt","additions":16,"deletions":0,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -130,1 +130,9 @@\n-    jni\/nullCaller\n+    jni\/nullCaller \\\n+    valhalla\n+\n+# valhalla lworld tests\n+jdk_valhalla = \\\n+    java\/lang\/invoke \\\n+    valhalla \\\n+    java\/lang\/instrument\/valhalla\n+\n","filename":"test\/jdk\/TEST.groups","additions":9,"deletions":1,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -387,0 +387,1 @@\n+        vmOptFinalFlag(map, \"TieredCompilation\");\n","filename":"test\/jtreg-ext\/requires\/VMProps.java","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"}]}