{"files":[{"patch":"@@ -54,4 +54,3 @@\n-  JVM_EXCLUDE_FILES += templateInterpreter.cpp \\\n-      templateInterpreterGenerator.cpp bcEscapeAnalyzer.cpp ciTypeFlow.cpp\n-  JVM_CFLAGS_FEATURES += -DZERO \\\n-      -DZERO_LIBARCH='\"$(OPENJDK_TARGET_CPU_LEGACY_LIB)\"' $(LIBFFI_CFLAGS)\n+  JVM_EXCLUDE_FILES += templateInterpreter.cpp templateInterpreterGenerator.cpp \\\n+                       bcEscapeAnalyzer.cpp ciTypeFlow.cpp macroAssembler_common.cpp\n+  JVM_CFLAGS_FEATURES += -DZERO -DZERO_LIBARCH='\"$(OPENJDK_TARGET_CPU_LEGACY_LIB)\"' $(LIBFFI_CFLAGS)\n","filename":"make\/hotspot\/lib\/JvmFeatures.gmk","additions":3,"deletions":4,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -1693,0 +1693,3 @@\n+  } else if (_entry_point == nullptr) {\n+    \/\/ See CallLeafNoFPIndirect\n+    return 1 * NativeInstruction::instruction_size;\n@@ -1801,12 +1804,1 @@\n-  \/\/ n.b. frame size includes space for return pc and rfp\n-  const int framesize = C->output()->frame_size_in_bytes();\n-\n-  if (C->clinit_barrier_on_entry()) {\n-    assert(!C->method()->holder()->is_not_initialized(), \"initialization should have been started\");\n-\n-    Label L_skip_barrier;\n-    __ mov_metadata(rscratch2, C->method()->holder()->constant_encoding());\n-    __ clinit_barrier(rscratch2, rscratch1, &L_skip_barrier);\n-    __ far_jump(RuntimeAddress(SharedRuntime::get_handle_wrong_method_stub()));\n-    __ bind(L_skip_barrier);\n-  }\n+  __ verified_entry(C, 0);\n@@ -1815,2 +1807,2 @@\n-  if (C->max_vector_size() > 0) {\n-    __ reinitialize_ptrue();\n+  if (C->stub_function() == nullptr) {\n+    __ entry_barrier();\n@@ -1819,25 +1811,2 @@\n-  int bangsize = C->output()->bang_size_in_bytes();\n-  if (C->output()->need_stack_bang(bangsize))\n-    __ generate_stack_overflow_check(bangsize);\n-\n-  __ build_frame(framesize);\n-\n-  if (C->stub_function() == nullptr) {\n-    BarrierSetAssembler* bs = BarrierSet::barrier_set()->barrier_set_assembler();\n-    \/\/ Dummy labels for just measuring the code size\n-    Label dummy_slow_path;\n-    Label dummy_continuation;\n-    Label dummy_guard;\n-    Label* slow_path = &dummy_slow_path;\n-    Label* continuation = &dummy_continuation;\n-    Label* guard = &dummy_guard;\n-    if (!Compile::current()->output()->in_scratch_emit_size()) {\n-      \/\/ Use real labels from actual stub when not emitting code for the purpose of measuring its size\n-      C2EntryBarrierStub* stub = new (Compile::current()->comp_arena()) C2EntryBarrierStub();\n-      Compile::current()->output()->add_stub(stub);\n-      slow_path = &stub->entry();\n-      continuation = &stub->continuation();\n-      guard = &stub->guard();\n-    }\n-    \/\/ In the C2 code, we move the non-hot part of nmethod entry barriers out-of-line to a stub.\n-    bs->nmethod_entry_barrier(masm, slow_path, continuation, guard);\n+  if (!Compile::current()->output()->in_scratch_emit_size()) {\n+    __ bind(*_verified_entry);\n@@ -1860,6 +1829,0 @@\n-uint MachPrologNode::size(PhaseRegAlloc* ra_) const\n-{\n-  return MachNode::size(ra_); \/\/ too many variables; just compute it\n-                              \/\/ the hard way\n-}\n-\n@@ -1908,1 +1871,1 @@\n-  __ remove_frame(framesize);\n+  __ remove_frame(framesize, C->needs_stack_repair());\n@@ -1927,5 +1890,0 @@\n-uint MachEpilogNode::size(PhaseRegAlloc *ra_) const {\n-  \/\/ Variable size. Determine dynamically.\n-  return MachNode::size(ra_);\n-}\n-\n@@ -2227,1 +2185,12 @@\n-\/\/=============================================================================\n+\/\/\/=============================================================================\n+#ifndef PRODUCT\n+void MachVEPNode::format(PhaseRegAlloc* ra_, outputStream* st) const\n+{\n+  st->print_cr(\"# MachVEPNode\");\n+  if (!_verified) {\n+    st->print_cr(\"\\t load_class\");\n+  } else {\n+    st->print_cr(\"\\t unpack_inline_arg\");\n+  }\n+}\n+#endif\n@@ -2229,0 +2198,27 @@\n+void MachVEPNode::emit(C2_MacroAssembler *masm, PhaseRegAlloc* ra_) const\n+{\n+  if (!_verified) {\n+    __ ic_check(1);\n+  } else {\n+    \/\/ TODO 8284443 Avoid creation of temporary frame\n+    if (ra_->C->stub_function() == nullptr) {\n+      __ verified_entry(ra_->C, 0);\n+      __ entry_barrier();\n+      int framesize = ra_->C->output()->frame_slots() << LogBytesPerInt;\n+      __ remove_frame(framesize, false);\n+    }\n+    \/\/ Unpack inline type args passed as oop and then jump to\n+    \/\/ the verified entry point (skipping the unverified entry).\n+    int sp_inc = __ unpack_inline_args(ra_->C, _receiver_only);\n+    \/\/ Emit code for verified entry and save increment for stack repair on return\n+    __ verified_entry(ra_->C, sp_inc);\n+    if (Compile::current()->output()->in_scratch_emit_size()) {\n+      Label dummy_verified_entry;\n+      __ b(dummy_verified_entry);\n+    } else {\n+      __ b(*_verified_entry);\n+    }\n+  }\n+}\n+\n+\/\/=============================================================================\n@@ -2251,5 +2247,0 @@\n-uint MachUEPNode::size(PhaseRegAlloc* ra_) const\n-{\n-  return MachNode::size(ra_);\n-}\n-\n@@ -3744,0 +3735,31 @@\n+    if (tf()->returns_inline_type_as_fields() && !_method->is_method_handle_intrinsic() && _method->return_type()->is_loaded()) {\n+      \/\/ The last return value is not set by the callee but used to pass the null marker to compiled code.\n+      \/\/ Search for the corresponding projection, get the register and emit code that initialized it.\n+      uint con = (tf()->range_cc()->cnt() - 1);\n+      for (DUIterator_Fast imax, i = fast_outs(imax); i < imax; i++) {\n+        ProjNode* proj = fast_out(i)->as_Proj();\n+        if (proj->_con == con) {\n+          \/\/ Set null marker if r0 is non-null (a non-null value is returned buffered or scalarized)\n+          OptoReg::Name optoReg = ra_->get_reg_first(proj);\n+          VMReg reg = OptoReg::as_VMReg(optoReg, ra_->_framesize, OptoReg::reg2stack(ra_->_matcher._new_SP));\n+          Register toReg = reg->is_reg() ? reg->as_Register() : rscratch1;\n+          __ cmp(r0, zr);\n+          __ cset(toReg, Assembler::NE);\n+          if (reg->is_stack()) {\n+            int st_off = reg->reg2stack() * VMRegImpl::stack_slot_size;\n+            __ str(toReg, Address(sp, st_off));\n+          }\n+          break;\n+        }\n+      }\n+      if (return_value_is_used()) {\n+        \/\/ An inline type is returned as fields in multiple registers.\n+        \/\/ R0 either contains an oop if the inline type is buffered or a pointer\n+        \/\/ to the corresponding InlineKlass with the lowest bit set to 1. Zero r0\n+        \/\/ if the lowest bit is set to allow C2 to use the oop after null checking.\n+        \/\/ r0 &= (r0 & 1) - 1\n+        __ andr(rscratch1, r0, 0x1);\n+        __ sub(rscratch1, rscratch1, 0x1);\n+        __ andr(r0, r0, rscratch1);\n+      }\n+    }\n@@ -6925,1 +6947,1 @@\n-    \"mov  $dst, $con\\t# ptr\\n\\t\"\n+    \"mov  $dst, $con\\t# ptr\"\n@@ -8118,0 +8140,30 @@\n+instruct castI2N(iRegNNoSp dst, iRegI src) %{\n+  match(Set dst (CastI2N src));\n+\n+  ins_cost(INSN_COST);\n+  format %{ \"mov $dst, $src\\t# int -> narrow ptr\" %}\n+\n+  ins_encode %{\n+    if ($dst$$reg != $src$$reg) {\n+      __ mov(as_Register($dst$$reg), as_Register($src$$reg));\n+    }\n+  %}\n+\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct castN2X(iRegLNoSp dst, iRegN src) %{\n+  match(Set dst (CastP2X src));\n+\n+  ins_cost(INSN_COST);\n+  format %{ \"mov $dst, $src\\t# ptr -> long\" %}\n+\n+  ins_encode %{\n+    if ($dst$$reg != $src$$reg) {\n+      __ mov(as_Register($dst$$reg), as_Register($src$$reg));\n+    }\n+  %}\n+\n+  ins_pipe(ialu_reg);\n+%}\n+\n@@ -15071,1 +15123,1 @@\n-instruct clearArray_reg_reg(iRegL_R11 cnt, iRegP_R10 base, Universe dummy, rFlagsReg cr)\n+instruct clearArray_reg_reg_immL0(iRegL_R11 cnt, iRegP_R10 base, immL0 zero, Universe dummy, rFlagsReg cr)\n@@ -15073,1 +15125,1 @@\n-  match(Set dummy (ClearArray cnt base));\n+  match(Set dummy (ClearArray (Binary cnt base) zero));\n@@ -15090,0 +15142,16 @@\n+instruct clearArray_reg_reg(iRegL_R11 cnt, iRegP_R10 base, iRegL val, Universe dummy, rFlagsReg cr)\n+%{\n+  predicate(((ClearArrayNode*)n)->word_copy_only());\n+  match(Set dummy (ClearArray (Binary cnt base) val));\n+  effect(USE_KILL cnt, USE_KILL base, KILL cr);\n+\n+  ins_cost(4 * INSN_COST);\n+  format %{ \"ClearArray $cnt, $base, $val\" %}\n+\n+  ins_encode %{\n+    __ fill_words($base$$Register, $cnt$$Register, $val$$Register);\n+  %}\n+\n+  ins_pipe(pipe_class_memory);\n+%}\n+\n@@ -15093,1 +15161,2 @@\n-            < (uint64_t)(BlockZeroingLowLimit >> LogBytesPerWord));\n+            < (uint64_t)(BlockZeroingLowLimit >> LogBytesPerWord)\n+            && !((ClearArrayNode*)n)->word_copy_only());\n@@ -16410,0 +16479,18 @@\n+\/\/ entry point is null, target holds the address to call\n+instruct CallLeafNoFPIndirect(iRegP target)\n+%{\n+  predicate(n->as_Call()->entry_point() == nullptr);\n+\n+  match(CallLeafNoFP target);\n+\n+  ins_cost(CALL_COST);\n+\n+  format %{ \"CALL, runtime leaf nofp indirect $target\" %}\n+\n+  ins_encode %{\n+    __ blr($target$$Register);\n+  %}\n+\n+  ins_pipe(pipe_class_call);\n+%}\n+\n@@ -16412,0 +16499,2 @@\n+  predicate(n->as_Call()->entry_point() != nullptr);\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/aarch64.ad","additions":150,"deletions":61,"binary":false,"changes":211,"status":"modified"},{"patch":"@@ -35,0 +35,1 @@\n+#include \"ci\/ciInlineKlass.hpp\"\n@@ -36,0 +37,1 @@\n+#include \"ci\/ciObjArrayKlass.hpp\"\n@@ -41,0 +43,1 @@\n+#include \"oops\/oop.inline.hpp\"\n@@ -429,1 +432,1 @@\n-  __ remove_frame(initial_frame_size_in_bytes());\n+  __ remove_frame(initial_frame_size_in_bytes(), needs_stack_repair());\n@@ -481,0 +484,42 @@\n+  if (InlineTypeReturnedAsFields) {\n+    \/\/ Check if we are returning an non-null inline type and load its fields into registers\n+    ciType* return_type = compilation()->method()->return_type();\n+    if (return_type->is_inlinetype()) {\n+      ciInlineKlass* vk = return_type->as_inline_klass();\n+      if (vk->can_be_returned_as_fields()) {\n+        address unpack_handler = vk->unpack_handler();\n+        assert(unpack_handler != nullptr, \"must be\");\n+        __ far_call(RuntimeAddress(unpack_handler));\n+      }\n+    } else if (return_type->is_instance_klass() && (!return_type->is_loaded() || StressCallingConvention)) {\n+      Label skip;\n+      Label not_null;\n+      __ cbnz(r0, not_null);\n+      \/\/ Returned value is null, zero all return registers because they may belong to oop fields\n+      __ mov(j_rarg1, zr);\n+      __ mov(j_rarg2, zr);\n+      __ mov(j_rarg3, zr);\n+      __ mov(j_rarg4, zr);\n+      __ mov(j_rarg5, zr);\n+      __ mov(j_rarg6, zr);\n+      __ mov(j_rarg7, zr);\n+      __ b(skip);\n+      __ bind(not_null);\n+\n+      \/\/ Check if we are returning an non-null inline type and load its fields into registers\n+      __ test_oop_is_not_inline_type(r0, rscratch2, skip, \/* can_be_null= *\/ false);\n+\n+      \/\/ Load fields from a buffered value with an inline class specific handler\n+      __ load_klass(rscratch1 \/*dst*\/, r0 \/*src*\/);\n+      __ ldr(rscratch1, Address(rscratch1, InstanceKlass::adr_inlineklass_fixed_block_offset()));\n+      __ ldr(rscratch1, Address(rscratch1, InlineKlass::unpack_handler_offset()));\n+      \/\/ Unpack handler can be null if inline type is not scalarizable in returns\n+      __ cbz(rscratch1, skip);\n+      __ blr(rscratch1);\n+\n+      __ bind(skip);\n+    }\n+    \/\/ At this point, r0 points to the value object (for interpreter or C1 caller).\n+    \/\/ The fields of the object are copied into registers (for C2 caller).\n+  }\n+\n@@ -482,1 +527,1 @@\n-  __ remove_frame(initial_frame_size_in_bytes());\n+  __ remove_frame(initial_frame_size_in_bytes(), needs_stack_repair());\n@@ -494,0 +539,4 @@\n+int LIR_Assembler::store_inline_type_fields_to_buf(ciInlineKlass* vk) {\n+  return (__ store_inline_type_fields_to_buf(vk, false));\n+}\n+\n@@ -540,3 +589,1 @@\n-        if (patch_code == lir_patch_none) {\n-          jobject2reg(c->as_jobject(), dest->as_register());\n-        } else {\n+        if (patch_code != lir_patch_none) {\n@@ -544,0 +591,2 @@\n+        } else {\n+          jobject2reg(c->as_jobject(), dest->as_register());\n@@ -653,0 +702,2 @@\n+    \/\/ Non-null case is not handled on aarch64 but handled on x86\n+    \/\/ FIXME: do we need to add it here?\n@@ -1000,0 +1051,14 @@\n+void LIR_Assembler::move(LIR_Opr src, LIR_Opr dst) {\n+  assert(dst->is_cpu_register(), \"must be\");\n+  assert(dst->type() == src->type(), \"must be\");\n+\n+  if (src->is_cpu_register()) {\n+    reg2reg(src, dst);\n+  } else if (src->is_stack()) {\n+    stack2reg(src, dst, dst->type());\n+  } else if (src->is_constant()) {\n+    const2reg(src, dst, lir_patch_none, nullptr);\n+  } else {\n+    ShouldNotReachHere();\n+  }\n+}\n@@ -1191,1 +1256,1 @@\n-  if (UseSlowPath ||\n+  if (UseSlowPath || op->always_slow_path() ||\n@@ -1303,22 +1368,16 @@\n-  if (should_profile) {\n-    Register mdo  = klass_RInfo;\n-    __ mov_metadata(mdo, md->constant_encoding());\n-    Label not_null;\n-    __ cbnz(obj, not_null);\n-    \/\/ Object is null; update MDO and exit\n-    Address data_addr\n-      = __ form_address(rscratch2, mdo,\n-                        md->byte_offset_of_slot(data, DataLayout::flags_offset()),\n-                        0);\n-    __ ldrb(rscratch1, data_addr);\n-    __ orr(rscratch1, rscratch1, BitData::null_seen_byte_constant());\n-    __ strb(rscratch1, data_addr);\n-    __ b(*obj_is_null);\n-    __ bind(not_null);\n-\n-    Label update_done;\n-    Register recv = k_RInfo;\n-    __ load_klass(recv, obj);\n-    type_profile_helper(mdo, md, data, recv, &update_done);\n-    Address counter_addr(mdo, md->byte_offset_of_slot(data, CounterData::count_offset()));\n-    __ addptr(counter_addr, DataLayout::counter_increment);\n+  if (op->need_null_check()) {\n+    if (should_profile) {\n+      Register mdo  = klass_RInfo;\n+      __ mov_metadata(mdo, md->constant_encoding());\n+      Label not_null;\n+      __ cbnz(obj, not_null);\n+      \/\/ Object is null; update MDO and exit\n+      Address data_addr\n+        = __ form_address(rscratch2, mdo,\n+                          md->byte_offset_of_slot(data, DataLayout::flags_offset()),\n+                          0);\n+      __ ldrb(rscratch1, data_addr);\n+      __ orr(rscratch1, rscratch1, BitData::null_seen_byte_constant());\n+      __ strb(rscratch1, data_addr);\n+      __ b(*obj_is_null);\n+      __ bind(not_null);\n@@ -1326,3 +1385,11 @@\n-    __ bind(update_done);\n-  } else {\n-    __ cbz(obj, *obj_is_null);\n+      Label update_done;\n+      Register recv = k_RInfo;\n+      __ load_klass(recv, obj);\n+      type_profile_helper(mdo, md, data, recv, &update_done);\n+      Address counter_addr(mdo, md->byte_offset_of_slot(data, CounterData::count_offset()));\n+      __ addptr(counter_addr, DataLayout::counter_increment);\n+\n+      __ bind(update_done);\n+    } else {\n+      __ cbz(obj, *obj_is_null);\n+    }\n@@ -1339,0 +1406,1 @@\n+    assert(!k->is_loaded() || !k->is_obj_array_klass(), \"Use refined array for a direct pointer comparison\");\n@@ -1361,1 +1429,8 @@\n-        __ cmp(klass_RInfo, k_RInfo);\n+        if (k->is_loaded() && k->is_obj_array_klass()) {\n+          \/\/ For a direct pointer comparison, we need the refined array klass pointer\n+          ciKlass* k_refined = ciObjArrayKlass::make(k->as_obj_array_klass()->element_klass());\n+          __ mov_metadata(rscratch1, k_refined->constant_encoding());\n+          __ cmp(klass_RInfo, rscratch1);\n+        } else {\n+          __ cmp(klass_RInfo, k_RInfo);\n+        }\n@@ -1486,0 +1561,106 @@\n+void LIR_Assembler::emit_opFlattenedArrayCheck(LIR_OpFlattenedArrayCheck* op) {\n+  \/\/ We are loading\/storing from\/to an array that *may* be a flat array (the\n+  \/\/ declared type is Object[], abstract[], interface[] or VT.ref[]).\n+  \/\/ If this array is a flat array, take the slow path.\n+  __ test_flat_array_oop(op->array()->as_register(), op->tmp()->as_register(), *op->stub()->entry());\n+  if (!op->value()->is_illegal()) {\n+    \/\/ The array is not a flat array, but it might be null-free. If we are storing\n+    \/\/ a null into a null-free array, take the slow path (which will throw NPE).\n+    Label skip;\n+    __ cbnz(op->value()->as_register(), skip);\n+    __ test_null_free_array_oop(op->array()->as_register(), op->tmp()->as_register(), *op->stub()->entry());\n+    __ bind(skip);\n+  }\n+}\n+\n+void LIR_Assembler::emit_opNullFreeArrayCheck(LIR_OpNullFreeArrayCheck* op) {\n+  \/\/ We are storing into an array that *may* be null-free (the declared type is\n+  \/\/ Object[], abstract[], interface[] or VT.ref[]).\n+  Label test_mark_word;\n+  Register tmp = op->tmp()->as_register();\n+  __ ldr(tmp, Address(op->array()->as_register(), oopDesc::mark_offset_in_bytes()));\n+  __ tst(tmp, markWord::unlocked_value);\n+  __ br(Assembler::NE, test_mark_word);\n+  __ load_prototype_header(tmp, op->array()->as_register());\n+  __ bind(test_mark_word);\n+  __ tst(tmp, markWord::null_free_array_bit_in_place);\n+}\n+\n+void LIR_Assembler::emit_opSubstitutabilityCheck(LIR_OpSubstitutabilityCheck* op) {\n+  Label L_oops_equal;\n+  Label L_oops_not_equal;\n+  Label L_end;\n+\n+  Register left  = op->left()->as_register();\n+  Register right = op->right()->as_register();\n+\n+  __ cmp(left, right);\n+  __ br(Assembler::EQ, L_oops_equal);\n+\n+  \/\/ (1) Null check -- if one of the operands is null, the other must not be null (because\n+  \/\/     the two references are not equal), so they are not substitutable,\n+  \/\/     FIXME: do null check only if the operand is nullable\n+  {\n+    __ cbz(left, L_oops_not_equal);\n+    __ cbz(right, L_oops_not_equal);\n+  }\n+\n+  ciKlass* left_klass = op->left_klass();\n+  ciKlass* right_klass = op->right_klass();\n+\n+  \/\/ (2) Inline type check -- if either of the operands is not a inline type,\n+  \/\/     they are not substitutable. We do this only if we are not sure that the\n+  \/\/     operands are inline type\n+  if ((left_klass == nullptr || right_klass == nullptr) ||\/\/ The klass is still unloaded, or came from a Phi node.\n+      !left_klass->is_inlinetype() || !right_klass->is_inlinetype()) {\n+    Register tmp1  = op->tmp1()->as_register();\n+    __ mov(tmp1, markWord::inline_type_pattern);\n+    __ ldr(rscratch1, Address(left, oopDesc::mark_offset_in_bytes()));\n+    __ andr(tmp1, tmp1, rscratch1);\n+    __ ldr(rscratch1, Address(right, oopDesc::mark_offset_in_bytes()));\n+    __ andr(tmp1, tmp1, rscratch1);\n+    __ cmp(tmp1, (u1)markWord::inline_type_pattern);\n+    __ br(Assembler::NE, L_oops_not_equal);\n+  }\n+\n+  \/\/ (3) Same klass check: if the operands are of different klasses, they are not substitutable.\n+  if (left_klass != nullptr && left_klass->is_inlinetype() && left_klass == right_klass) {\n+    \/\/ No need to load klass -- the operands are statically known to be the same inline klass.\n+    __ b(*op->stub()->entry());\n+  } else {\n+    Register left_klass_op = op->left_klass_op()->as_register();\n+    Register right_klass_op = op->right_klass_op()->as_register();\n+\n+    if (UseCompressedClassPointers) {\n+      __ ldrw(left_klass_op,  Address(left,  oopDesc::klass_offset_in_bytes()));\n+      __ ldrw(right_klass_op, Address(right, oopDesc::klass_offset_in_bytes()));\n+      __ cmpw(left_klass_op, right_klass_op);\n+    } else {\n+      __ ldr(left_klass_op,  Address(left,  oopDesc::klass_offset_in_bytes()));\n+      __ ldr(right_klass_op, Address(right, oopDesc::klass_offset_in_bytes()));\n+      __ cmp(left_klass_op, right_klass_op);\n+    }\n+\n+    __ br(Assembler::EQ, *op->stub()->entry()); \/\/ same klass -> do slow check\n+    \/\/ fall through to L_oops_not_equal\n+  }\n+\n+  __ bind(L_oops_not_equal);\n+  move(op->not_equal_result(), op->result_opr());\n+  __ b(L_end);\n+\n+  __ bind(L_oops_equal);\n+  move(op->equal_result(), op->result_opr());\n+  __ b(L_end);\n+\n+  \/\/ We've returned from the stub. R0 contains 0x0 IFF the two\n+  \/\/ operands are not substitutable. (Don't compare against 0x1 in case the\n+  \/\/ C compiler is naughty)\n+  __ bind(*op->stub()->continuation());\n+  __ cbz(r0, L_oops_not_equal); \/\/ (call_stub() == 0x0) -> not_equal\n+  move(op->equal_result(), op->result_opr()); \/\/ (call_stub() != 0x0) -> equal\n+  \/\/ fall-through\n+  __ bind(L_end);\n+}\n+\n+\n@@ -1999,1 +2180,1 @@\n-  add_call_info(code_offset(), op->info());\n+  add_call_info(code_offset(), op->info(), op->maybe_return_as_fields());\n@@ -2010,1 +2191,1 @@\n-  add_call_info(code_offset(), op->info());\n+  add_call_info(code_offset(), op->info(), op->maybe_return_as_fields());\n@@ -2173,0 +2354,12 @@\n+void LIR_Assembler::arraycopy_inlinetype_check(Register obj, Register tmp, CodeStub* slow_path, bool is_dest, bool null_check) {\n+  if (null_check) {\n+    __ cbz(obj, *slow_path->entry());\n+  }\n+  if (is_dest) {\n+    __ test_null_free_array_oop(obj, tmp, *slow_path->entry());\n+    \/\/ TODO 8350865 Flat no longer implies null-free, so we need to check for flat dest. Can we do better here?\n+    __ test_flat_array_oop(obj, tmp, *slow_path->entry());\n+  } else {\n+    __ test_flat_array_oop(obj, tmp, *slow_path->entry());\n+  }\n+}\n@@ -2191,0 +2384,6 @@\n+  if (flags & LIR_OpArrayCopy::always_slow_path) {\n+    __ b(*stub->entry());\n+    __ bind(*stub->continuation());\n+    return;\n+  }\n+\n@@ -2244,0 +2443,8 @@\n+  \/\/ Handle inline type arrays\n+  if (flags & LIR_OpArrayCopy::src_inlinetype_check) {\n+    arraycopy_inlinetype_check(src, tmp, stub, false, (flags & LIR_OpArrayCopy::src_null_check));\n+  }\n+  if (flags & LIR_OpArrayCopy::dst_inlinetype_check) {\n+    arraycopy_inlinetype_check(dst, tmp, stub, true, (flags & LIR_OpArrayCopy::dst_null_check));\n+  }\n+\n@@ -2785,0 +2992,20 @@\n+void LIR_Assembler::emit_profile_inline_type(LIR_OpProfileInlineType* op) {\n+  Register obj = op->obj()->as_register();\n+  Register tmp = op->tmp()->as_pointer_register();\n+  bool not_null = op->not_null();\n+  int flag = op->flag();\n+\n+  Label not_inline_type;\n+  if (!not_null) {\n+    __ cbz(obj, not_inline_type);\n+  }\n+\n+  __ test_oop_is_not_inline_type(obj, tmp, not_inline_type);\n+\n+  Address mdo_addr = as_Address(op->mdp()->as_address_ptr(), rscratch2);\n+  __ ldrb(rscratch1, mdo_addr);\n+  __ orr(rscratch1, rscratch1, flag);\n+  __ strb(rscratch1, mdo_addr);\n+\n+  __ bind(not_inline_type);\n+}\n@@ -2924,0 +3151,4 @@\n+void LIR_Assembler::check_orig_pc() {\n+  __ ldr(rscratch2, frame_map()->address_for_orig_pc_addr());\n+  __ cmp(rscratch2, (u1)NULL_WORD);\n+}\n","filename":"src\/hotspot\/cpu\/aarch64\/c1_LIRAssembler_aarch64.cpp","additions":265,"deletions":34,"binary":false,"changes":299,"status":"modified"},{"patch":"@@ -77,0 +77,3 @@\n+  void arraycopy_inlinetype_check(Register obj, Register tmp, CodeStub* slow_path, bool is_dest, bool null_check);\n+  void move(LIR_Opr src, LIR_Opr dst);\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/c1_LIRAssembler_aarch64.hpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -2562,0 +2562,4 @@\n+void LIR_Assembler::emit_profile_inline_type(LIR_OpProfileInlineType* op) {\n+  Unimplemented();\n+}\n+\n","filename":"src\/hotspot\/cpu\/arm\/c1_LIRAssembler_arm.cpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -3133,0 +3133,3 @@\n+void LIR_Assembler::emit_profile_inline_type(LIR_OpProfileInlineType* op) {\n+  Unimplemented();\n+}\n","filename":"src\/hotspot\/cpu\/ppc\/c1_LIRAssembler_ppc.cpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -3069,0 +3069,4 @@\n+void LIR_Assembler::emit_profile_inline_type(LIR_OpProfileInlineType* op) {\n+  Unimplemented();\n+}\n+\n","filename":"src\/hotspot\/cpu\/s390\/c1_LIRAssembler_s390.cpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -34,0 +34,1 @@\n+#include \"ci\/ciInlineKlass.hpp\"\n@@ -35,0 +36,1 @@\n+#include \"ci\/ciObjArrayKlass.hpp\"\n@@ -39,0 +41,1 @@\n+#include \"oops\/oop.inline.hpp\"\n@@ -431,1 +434,1 @@\n-  __ remove_frame(initial_frame_size_in_bytes());\n+  __ remove_frame(initial_frame_size_in_bytes(), needs_stack_repair());\n@@ -476,0 +479,44 @@\n+  if (InlineTypeReturnedAsFields) {\n+  #ifndef _LP64\n+     Unimplemented();\n+  #endif\n+    \/\/ Check if we are returning an non-null inline type and load its fields into registers\n+    ciType* return_type = compilation()->method()->return_type();\n+    if (return_type->is_inlinetype()) {\n+      ciInlineKlass* vk = return_type->as_inline_klass();\n+      if (vk->can_be_returned_as_fields()) {\n+        address unpack_handler = vk->unpack_handler();\n+        assert(unpack_handler != nullptr, \"must be\");\n+        __ call(RuntimeAddress(unpack_handler));\n+      }\n+    } else if (return_type->is_instance_klass() && (!return_type->is_loaded() || StressCallingConvention)) {\n+      Label skip;\n+      Label not_null;\n+      __ testptr(rax, rax);\n+      __ jcc(Assembler::notZero, not_null);\n+      \/\/ Returned value is null, zero all return registers because they may belong to oop fields\n+      __ xorq(j_rarg1, j_rarg1);\n+      __ xorq(j_rarg2, j_rarg2);\n+      __ xorq(j_rarg3, j_rarg3);\n+      __ xorq(j_rarg4, j_rarg4);\n+      __ xorq(j_rarg5, j_rarg5);\n+      __ jmp(skip);\n+      __ bind(not_null);\n+\n+      \/\/ Check if we are returning an non-null inline type and load its fields into registers\n+      __ test_oop_is_not_inline_type(rax, rscratch1, skip, \/* can_be_null= *\/ false);\n+\n+      \/\/ Load fields from a buffered value with an inline class specific handler\n+      __ load_klass(rdi, rax, rscratch1);\n+      __ movptr(rdi, Address(rdi, InstanceKlass::adr_inlineklass_fixed_block_offset()));\n+      __ movptr(rdi, Address(rdi, InlineKlass::unpack_handler_offset()));\n+      \/\/ Unpack handler can be null if inline type is not scalarizable in returns\n+      __ testptr(rdi, rdi);\n+      __ jcc(Assembler::zero, skip);\n+      __ call(rdi);\n+\n+      __ bind(skip);\n+    }\n+    \/\/ At this point, rax points to the value object (for interpreter or C1 caller).\n+    \/\/ The fields of the object are copied into registers (for C2 caller).\n+  }\n@@ -478,1 +525,1 @@\n-  __ remove_frame(initial_frame_size_in_bytes());\n+  __ remove_frame(initial_frame_size_in_bytes(), needs_stack_repair());\n@@ -494,0 +541,4 @@\n+int LIR_Assembler::store_inline_type_fields_to_buf(ciInlineKlass* vk) {\n+  return (__ store_inline_type_fields_to_buf(vk, false));\n+}\n+\n@@ -1232,1 +1283,1 @@\n-  if (UseSlowPath ||\n+  if (UseSlowPath || op->always_slow_path() ||\n@@ -1331,24 +1382,26 @@\n-  __ testptr(obj, obj);\n-  if (op->should_profile()) {\n-    Label not_null;\n-    Register mdo  = klass_RInfo;\n-    __ mov_metadata(mdo, md->constant_encoding());\n-    __ jccb(Assembler::notEqual, not_null);\n-    \/\/ Object is null; update MDO and exit\n-    Address data_addr(mdo, md->byte_offset_of_slot(data, DataLayout::flags_offset()));\n-    int header_bits = BitData::null_seen_byte_constant();\n-    __ orb(data_addr, header_bits);\n-    __ jmp(*obj_is_null);\n-    __ bind(not_null);\n-\n-    Label update_done;\n-    Register recv = k_RInfo;\n-    __ load_klass(recv, obj, tmp_load_klass);\n-    type_profile_helper(mdo, md, data, recv, &update_done);\n-\n-    Address nonprofiled_receiver_count_addr(mdo, md->byte_offset_of_slot(data, CounterData::count_offset()));\n-    __ addptr(nonprofiled_receiver_count_addr, DataLayout::counter_increment);\n-\n-    __ bind(update_done);\n-  } else {\n-    __ jcc(Assembler::equal, *obj_is_null);\n+  if (op->need_null_check()) {\n+    __ testptr(obj, obj);\n+    if (op->should_profile()) {\n+      Label not_null;\n+      Register mdo  = klass_RInfo;\n+      __ mov_metadata(mdo, md->constant_encoding());\n+      __ jccb(Assembler::notEqual, not_null);\n+      \/\/ Object is null; update MDO and exit\n+      Address data_addr(mdo, md->byte_offset_of_slot(data, DataLayout::flags_offset()));\n+      int header_bits = BitData::null_seen_byte_constant();\n+      __ orb(data_addr, header_bits);\n+      __ jmp(*obj_is_null);\n+      __ bind(not_null);\n+\n+      Label update_done;\n+      Register recv = k_RInfo;\n+      __ load_klass(recv, obj, tmp_load_klass);\n+      type_profile_helper(mdo, md, data, recv, &update_done);\n+\n+      Address nonprofiled_receiver_count_addr(mdo, md->byte_offset_of_slot(data, CounterData::count_offset()));\n+      __ addptr(nonprofiled_receiver_count_addr, DataLayout::counter_increment);\n+\n+      __ bind(update_done);\n+    } else {\n+      __ jcc(Assembler::equal, *obj_is_null);\n+    }\n@@ -1365,0 +1418,1 @@\n+    assert(!k->is_loaded() || !k->is_obj_array_klass(), \"Use refined array for a direct pointer comparison\");\n@@ -1389,1 +1443,8 @@\n-        __ cmpptr(klass_RInfo, k_RInfo);\n+        if (k->is_loaded() && k->is_obj_array_klass()) {\n+          \/\/ For a direct pointer comparison, we need the refined array klass pointer\n+          ciKlass* k_refined = ciObjArrayKlass::make(k->as_obj_array_klass()->element_klass());\n+          __ mov_metadata(tmp_load_klass, k_refined->constant_encoding());\n+          __ cmpptr(klass_RInfo, tmp_load_klass);\n+        } else {\n+          __ cmpptr(klass_RInfo, k_RInfo);\n+        }\n@@ -1524,0 +1585,103 @@\n+void LIR_Assembler::emit_opFlattenedArrayCheck(LIR_OpFlattenedArrayCheck* op) {\n+  \/\/ We are loading\/storing from\/to an array that *may* be a flat array (the\n+  \/\/ declared type is Object[], abstract[], interface[] or VT.ref[]).\n+  \/\/ If this array is a flat array, take the slow path.\n+  __ test_flat_array_oop(op->array()->as_register(), op->tmp()->as_register(), *op->stub()->entry());\n+  if (!op->value()->is_illegal()) {\n+    \/\/ TODO 8350865 This is also used for profiling code, right? And in that case we don't care about null but just want to know if the array is flat or not.\n+    \/\/ The array is not a flat array, but it might be null-free. If we are storing\n+    \/\/ a null into a null-free array, take the slow path (which will throw NPE).\n+    Label skip;\n+    __ cmpptr(op->value()->as_register(), NULL_WORD);\n+    __ jcc(Assembler::notEqual, skip);\n+    __ test_null_free_array_oop(op->array()->as_register(), op->tmp()->as_register(), *op->stub()->entry());\n+    __ bind(skip);\n+  }\n+}\n+\n+void LIR_Assembler::emit_opNullFreeArrayCheck(LIR_OpNullFreeArrayCheck* op) {\n+  \/\/ We are storing into an array that *may* be null-free (the declared type is\n+  \/\/ Object[], abstract[], interface[] or VT.ref[]).\n+  Label test_mark_word;\n+  Register tmp = op->tmp()->as_register();\n+  __ movptr(tmp, Address(op->array()->as_register(), oopDesc::mark_offset_in_bytes()));\n+  __ testl(tmp, markWord::unlocked_value);\n+  __ jccb(Assembler::notZero, test_mark_word);\n+  __ load_prototype_header(tmp, op->array()->as_register(), rscratch1);\n+  __ bind(test_mark_word);\n+  __ testl(tmp, markWord::null_free_array_bit_in_place);\n+}\n+\n+void LIR_Assembler::emit_opSubstitutabilityCheck(LIR_OpSubstitutabilityCheck* op) {\n+  Label L_oops_equal;\n+  Label L_oops_not_equal;\n+  Label L_end;\n+\n+  Register left  = op->left()->as_register();\n+  Register right = op->right()->as_register();\n+\n+  __ cmpptr(left, right);\n+  __ jcc(Assembler::equal, L_oops_equal);\n+\n+  \/\/ (1) Null check -- if one of the operands is null, the other must not be null (because\n+  \/\/     the two references are not equal), so they are not substitutable,\n+  \/\/     FIXME: do null check only if the operand is nullable\n+  __ testptr(left, right);\n+  __ jcc(Assembler::zero, L_oops_not_equal);\n+\n+  ciKlass* left_klass = op->left_klass();\n+  ciKlass* right_klass = op->right_klass();\n+\n+  \/\/ (2) Inline type check -- if either of the operands is not a inline type,\n+  \/\/     they are not substitutable. We do this only if we are not sure that the\n+  \/\/     operands are inline type\n+  if ((left_klass == nullptr || right_klass == nullptr) ||\/\/ The klass is still unloaded, or came from a Phi node.\n+      !left_klass->is_inlinetype() || !right_klass->is_inlinetype()) {\n+    Register tmp1  = op->tmp1()->as_register();\n+    __ movptr(tmp1, (intptr_t)markWord::inline_type_pattern);\n+    __ andptr(tmp1, Address(left, oopDesc::mark_offset_in_bytes()));\n+    __ andptr(tmp1, Address(right, oopDesc::mark_offset_in_bytes()));\n+    __ cmpptr(tmp1, (intptr_t)markWord::inline_type_pattern);\n+    __ jcc(Assembler::notEqual, L_oops_not_equal);\n+  }\n+\n+  \/\/ (3) Same klass check: if the operands are of different klasses, they are not substitutable.\n+  if (left_klass != nullptr && left_klass->is_inlinetype() && left_klass == right_klass) {\n+    \/\/ No need to load klass -- the operands are statically known to be the same inline klass.\n+    __ jmp(*op->stub()->entry());\n+  } else {\n+    Register left_klass_op = op->left_klass_op()->as_register();\n+    Register right_klass_op = op->right_klass_op()->as_register();\n+\n+    if (UseCompressedClassPointers) {\n+      __ movl(left_klass_op,  Address(left,  oopDesc::klass_offset_in_bytes()));\n+      __ movl(right_klass_op, Address(right, oopDesc::klass_offset_in_bytes()));\n+      __ cmpl(left_klass_op, right_klass_op);\n+    } else {\n+      __ movptr(left_klass_op,  Address(left,  oopDesc::klass_offset_in_bytes()));\n+      __ movptr(right_klass_op, Address(right, oopDesc::klass_offset_in_bytes()));\n+      __ cmpptr(left_klass_op, right_klass_op);\n+    }\n+\n+    __ jcc(Assembler::equal, *op->stub()->entry()); \/\/ same klass -> do slow check\n+    \/\/ fall through to L_oops_not_equal\n+  }\n+\n+  __ bind(L_oops_not_equal);\n+  move(op->not_equal_result(), op->result_opr());\n+  __ jmp(L_end);\n+\n+  __ bind(L_oops_equal);\n+  move(op->equal_result(), op->result_opr());\n+  __ jmp(L_end);\n+\n+  \/\/ We've returned from the stub. RAX contains 0x0 IFF the two\n+  \/\/ operands are not substitutable. (Don't compare against 0x1 in case the\n+  \/\/ C compiler is naughty)\n+  __ bind(*op->stub()->continuation());\n+  __ cmpl(rax, 0);\n+  __ jcc(Assembler::equal, L_oops_not_equal); \/\/ (call_stub() == 0x0) -> not_equal\n+  move(op->equal_result(), op->result_opr()); \/\/ (call_stub() != 0x0) -> equal\n+  \/\/ fall-through\n+  __ bind(L_end);\n+}\n@@ -1569,0 +1733,15 @@\n+void LIR_Assembler::move(LIR_Opr src, LIR_Opr dst) {\n+  assert(dst->is_cpu_register(), \"must be\");\n+  assert(dst->type() == src->type(), \"must be\");\n+\n+  if (src->is_cpu_register()) {\n+    reg2reg(src, dst);\n+  } else if (src->is_stack()) {\n+    stack2reg(src, dst, dst->type());\n+  } else if (src->is_constant()) {\n+    const2reg(src, dst, lir_patch_none, nullptr);\n+  } else {\n+    ShouldNotReachHere();\n+  }\n+}\n+\n@@ -2179,1 +2358,1 @@\n-  add_call_info(code_offset(), op->info());\n+  add_call_info(code_offset(), op->info(), op->maybe_return_as_fields());\n@@ -2186,1 +2365,1 @@\n-  add_call_info(code_offset(), op->info());\n+  add_call_info(code_offset(), op->info(), op->maybe_return_as_fields());\n@@ -2353,0 +2532,15 @@\n+void LIR_Assembler::arraycopy_inlinetype_check(Register obj, Register tmp, CodeStub* slow_path, bool is_dest, bool null_check) {\n+  if (null_check) {\n+    __ testptr(obj, obj);\n+    __ jcc(Assembler::zero, *slow_path->entry());\n+  }\n+  if (is_dest) {\n+    __ test_null_free_array_oop(obj, tmp, *slow_path->entry());\n+    \/\/ TODO 8350865 Flat no longer implies null-free, so we need to check for flat dest. Can we do better here?\n+    __ test_flat_array_oop(obj, tmp, *slow_path->entry());\n+  } else {\n+    __ test_flat_array_oop(obj, tmp, *slow_path->entry());\n+  }\n+}\n+\n+\n@@ -2372,0 +2566,6 @@\n+  if (flags & LIR_OpArrayCopy::always_slow_path) {\n+    __ jmp(*stub->entry());\n+    __ bind(*stub->continuation());\n+    return;\n+  }\n+\n@@ -2448,0 +2648,8 @@\n+  \/\/ Handle inline type arrays\n+  if (flags & LIR_OpArrayCopy::src_inlinetype_check) {\n+    arraycopy_inlinetype_check(src, tmp, stub, false, (flags & LIR_OpArrayCopy::src_null_check));\n+  }\n+  if (flags & LIR_OpArrayCopy::dst_inlinetype_check) {\n+    arraycopy_inlinetype_check(dst, tmp, stub, true, (flags & LIR_OpArrayCopy::dst_null_check));\n+  }\n+\n@@ -2674,0 +2882,1 @@\n+\n@@ -3010,0 +3219,21 @@\n+void LIR_Assembler::emit_profile_inline_type(LIR_OpProfileInlineType* op) {\n+  Register obj = op->obj()->as_register();\n+  Register tmp = op->tmp()->as_pointer_register();\n+  Address mdo_addr = as_Address(op->mdp()->as_address_ptr());\n+  bool not_null = op->not_null();\n+  int flag = op->flag();\n+\n+  Label not_inline_type;\n+  if (!not_null) {\n+    __ testptr(obj, obj);\n+    __ jccb(Assembler::zero, not_inline_type);\n+  }\n+\n+  __ test_oop_is_not_inline_type(obj, tmp, not_inline_type);\n+\n+  __ orb(mdo_addr, flag);\n+\n+  __ bind(not_inline_type);\n+}\n+\n+\n@@ -3195,0 +3425,3 @@\n+void LIR_Assembler::check_orig_pc() {\n+  __ cmpptr(frame_map()->address_for_orig_pc_addr(), NULL_WORD);\n+}\n","filename":"src\/hotspot\/cpu\/x86\/c1_LIRAssembler_x86.cpp","additions":263,"deletions":30,"binary":false,"changes":293,"status":"modified"},{"patch":"@@ -54,0 +54,3 @@\n+  void arraycopy_inlinetype_check(Register obj, Register tmp, CodeStub* slow_path, bool is_dest, bool null_check);\n+  void move(LIR_Opr src, LIR_Opr dst);\n+\n","filename":"src\/hotspot\/cpu\/x86\/c1_LIRAssembler_x86.hpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -35,0 +35,3 @@\n+#ifdef COMPILER1\n+#include \"c1\/c1_Runtime1.hpp\"\n+#endif\n@@ -432,2 +435,3 @@\n-  \/\/ On Intel the return_address is always the word on the stack\n-  address sender_pc = (address) *(sender_sp-1);\n+#ifdef ASSERT\n+  address sender_pc_copy = (address) *(sender_sp-1);\n+#endif\n@@ -440,0 +444,16 @@\n+  \/\/ Repair the sender sp if the frame has been extended\n+  sender_sp = repair_sender_sp(sender_sp, saved_fp_addr);\n+\n+  \/\/ On Intel the return_address is always the word on the stack\n+  address sender_pc = (address) *(sender_sp-1);\n+\n+#ifdef ASSERT\n+  if (sender_pc != sender_pc_copy) {\n+    \/\/ When extending the stack in the callee method entry to make room for unpacking of value\n+    \/\/ type args, we keep a copy of the sender pc at the expected location in the callee frame.\n+    \/\/ If the sender pc is patched due to deoptimization, the copy is not consistent anymore.\n+    nmethod* nm = CodeCache::find_blob(sender_pc)->as_nmethod();\n+    assert(sender_pc == nm->deopt_handler_entry(), \"unexpected sender pc\");\n+  }\n+#endif\n+\n@@ -444,2 +464,21 @@\n-    if (!_cb->is_nmethod()) { \/\/ compiled frames do not use callee-saved registers\n-      map->set_include_argument_oops(_cb->caller_must_gc_arguments(map->thread()));\n+    bool c1_buffering = false;\n+#ifdef COMPILER1\n+    nmethod* nm = _cb->as_nmethod_or_null();\n+    if (nm != nullptr && nm->is_compiled_by_c1() && nm->method()->has_scalarized_args() &&\n+        pc() < nm->verified_inline_entry_point()) {\n+      \/\/ TODO 8284443 Can't we do that by not passing 'dont_gc_arguments' in case 'StubId::c1_buffer_inline_args_id' in 'Runtime1::generate_code_for'?\n+      \/\/ The VEP and VIEP(RO) of C1-compiled methods call buffer_inline_args_xxx\n+      \/\/ before doing any argument shuffling, so we need to scan the oops\n+      \/\/ as the caller passes them.\n+      c1_buffering = true;\n+#ifdef ASSERT\n+      NativeCall* call = nativeCall_before(pc());\n+      address dest = call->destination();\n+      assert(dest == Runtime1::entry_for(StubId::c1_buffer_inline_args_no_receiver_id) ||\n+             dest == Runtime1::entry_for(StubId::c1_buffer_inline_args_id), \"unexpected safepoint in entry point\");\n+#endif\n+    }\n+#endif\n+    if (!_cb->is_nmethod() || c1_buffering) { \/\/ compiled frames do not use callee-saved registers\n+      bool caller_args = _cb->caller_must_gc_arguments(map->thread()) || c1_buffering;\n+      map->set_include_argument_oops(caller_args);\n","filename":"src\/hotspot\/cpu\/x86\/frame_x86.inline.hpp","additions":43,"deletions":4,"binary":false,"changes":47,"status":"modified"},{"patch":"@@ -33,0 +33,1 @@\n+#include \"runtime\/signature.hpp\"\n@@ -36,0 +37,2 @@\n+class ciInlineKlass;\n+\n@@ -97,0 +100,22 @@\n+  \/\/ markWord tests, kills markWord reg\n+  void test_markword_is_inline_type(Register markword, Label& is_inline_type);\n+\n+  \/\/ inlineKlass queries, kills temp_reg\n+  void test_oop_is_not_inline_type(Register object, Register tmp, Label& not_inline_type, bool can_be_null = true);\n+\n+  void test_field_is_null_free_inline_type(Register flags, Register temp_reg, Label& is_null_free);\n+  void test_field_is_not_null_free_inline_type(Register flags, Register temp_reg, Label& not_null_free);\n+  void test_field_is_flat(Register flags, Register temp_reg, Label& is_flat);\n+  void test_field_has_null_marker(Register flags, Register temp_reg, Label& has_null_marker);\n+\n+  \/\/ Check oops for special arrays, i.e. flat arrays and\/or null-free arrays\n+  void test_oop_prototype_bit(Register oop, Register temp_reg, int32_t test_bit, bool jmp_set, Label& jmp_label);\n+  void test_flat_array_oop(Register oop, Register temp_reg, Label& is_flat_array);\n+  void test_non_flat_array_oop(Register oop, Register temp_reg, Label& is_non_flat_array);\n+  void test_null_free_array_oop(Register oop, Register temp_reg, Label& is_null_free_array);\n+  void test_non_null_free_array_oop(Register oop, Register temp_reg, Label& is_non_null_free_array);\n+\n+  \/\/ Check array klass layout helper for flat or null-free arrays...\n+  void test_flat_array_layout(Register lh, Label& is_flat_array);\n+  void test_non_flat_array_layout(Register lh, Label& is_non_flat_array);\n+\n@@ -350,0 +375,3 @@\n+\n+  \/\/ Load oopDesc._metadata without decode (useful for direct Klass* compare from oops)\n+  void load_metadata(Register dst, Register src);\n@@ -367,0 +395,9 @@\n+  void flat_field_copy(DecoratorSet decorators, Register src, Register dst, Register inline_layout_info);\n+\n+  \/\/ inline type data payload offsets...\n+  void payload_offset(Register inline_klass, Register offset);\n+  void payload_addr(Register oop, Register data, Register inline_klass);\n+  \/\/ get data payload ptr a flat value array at index, kills rcx and index\n+  void data_for_value_array_index(Register array, Register array_klass,\n+                                  Register index, Register data);\n+\n@@ -376,0 +413,2 @@\n+  void load_prototype_header(Register dst, Register src, Register tmp);\n+\n@@ -510,0 +549,9 @@\n+\n+  \/\/ Object \/ value buffer allocation...\n+  \/\/ Allocate instance of klass, assumes klass initialized by caller\n+  \/\/ new_obj prefers to be rax\n+  \/\/ Kills t1 and t2, perserves klass, return allocation in new_obj (rsi on LP64)\n+  void allocate_instance(Register klass, Register new_obj,\n+                         Register t1, Register t2,\n+                         bool clear_fields, Label& alloc_failed);\n+\n@@ -520,0 +568,5 @@\n+  \/\/ For field \"index\" within \"klass\", return inline_klass ...\n+  void get_inline_type_field_klass(Register klass, Register index, Register inline_klass);\n+\n+  void inline_layout_info(Register klass, Register index, Register layout_info);\n+\n@@ -766,0 +819,1 @@\n+  void andptr(Register dst, Address src) { andq(dst, src); }\n@@ -1905,0 +1959,15 @@\n+  \/\/ Inline type specific methods\n+  #include \"asm\/macroAssembler_common.hpp\"\n+\n+  int store_inline_type_fields_to_buf(ciInlineKlass* vk, bool from_interpreter = true);\n+  bool move_helper(VMReg from, VMReg to, BasicType bt, RegState reg_state[]);\n+  bool unpack_inline_helper(const GrowableArray<SigEntry>* sig, int& sig_index,\n+                            VMReg from, int& from_index, VMRegPair* to, int to_count, int& to_index,\n+                            RegState reg_state[]);\n+  bool pack_inline_helper(const GrowableArray<SigEntry>* sig, int& sig_index, int vtarg_index,\n+                          VMRegPair* from, int from_count, int& from_index, VMReg to,\n+                          RegState reg_state[], Register val_array);\n+  int extend_stack_for_inline_args(int args_on_stack);\n+  void remove_frame(int initial_framesize, bool needs_stack_repair);\n+  VMReg spill_reg_for(VMReg reg);\n+\n@@ -1907,1 +1976,1 @@\n-  void clear_mem(Register base, Register cnt, Register rtmp, XMMRegister xtmp, bool is_large, KRegister mask=knoreg);\n+  void clear_mem(Register base, Register cnt, Register val, XMMRegister xtmp, bool is_large, bool word_copy_only, KRegister mask=knoreg);\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.hpp","additions":70,"deletions":1,"binary":false,"changes":71,"status":"modified"},{"patch":"@@ -1794,1 +1794,1 @@\n-  if (!UseFastStosb && UseUnalignedLoadStores) {\n+  if (UseUnalignedLoadStores) {\n","filename":"src\/hotspot\/cpu\/x86\/vm_version_x86.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -1652,0 +1652,4 @@\n+  if (_entry_point == nullptr) {\n+    \/\/ CallLeafNoFPInDirect\n+    return 3; \/\/ callq (register)\n+  }\n@@ -1658,0 +1662,1 @@\n+\n@@ -1883,6 +1888,1 @@\n-  int framesize = C->output()->frame_size_in_bytes();\n-  int bangsize = C->output()->bang_size_in_bytes();\n-\n-  if (C->clinit_barrier_on_entry()) {\n-    assert(VM_Version::supports_fast_class_init_checks(), \"sanity\");\n-    assert(!C->method()->holder()->is_not_initialized(), \"initialization should have been started\");\n+  __ verified_entry(C);\n@@ -1890,9 +1890,2 @@\n-    Label L_skip_barrier;\n-    Register klass = rscratch1;\n-\n-    __ mov_metadata(klass, C->method()->holder()->constant_encoding());\n-    __ clinit_barrier(klass, &L_skip_barrier \/*L_fast_path*\/);\n-\n-    __ jump(RuntimeAddress(SharedRuntime::get_handle_wrong_method_stub())); \/\/ slow path\n-\n-    __ bind(L_skip_barrier);\n+  if (ra_->C->stub_function() == nullptr) {\n+    __ entry_barrier();\n@@ -1901,1 +1894,3 @@\n-  __ verified_entry(framesize, C->output()->need_stack_bang(bangsize)?bangsize:0, false, C->stub_function() != nullptr);\n+  if (!Compile::current()->output()->in_scratch_emit_size()) {\n+    __ bind(*_verified_entry);\n+  }\n@@ -1913,5 +1908,0 @@\n-uint MachPrologNode::size(PhaseRegAlloc* ra_) const\n-{\n-  return MachNode::size(ra_); \/\/ too many variables; just compute it\n-                              \/\/ the hard way\n-}\n@@ -1965,13 +1955,3 @@\n-  int framesize = C->output()->frame_size_in_bytes();\n-  assert((framesize & (StackAlignmentInBytes-1)) == 0, \"frame size not aligned\");\n-  \/\/ Remove word for return adr already pushed\n-  \/\/ and RBP\n-  framesize -= 2*wordSize;\n-\n-  \/\/ Note that VerifyStackAtCalls' Majik cookie does not change the frame size popped here\n-\n-  if (framesize) {\n-    __ addq(rsp, framesize);\n-  }\n-\n-  __ popq(rbp);\n+  \/\/ Subtract two words to account for return address and rbp\n+  int initial_framesize = C->output()->frame_size_in_bytes() - 2*wordSize;\n+  __ remove_frame(initial_framesize, C->needs_stack_repair());\n@@ -1996,6 +1976,0 @@\n-uint MachEpilogNode::size(PhaseRegAlloc* ra_) const\n-{\n-  return MachNode::size(ra_); \/\/ too many variables; just compute it\n-                              \/\/ the hard way\n-}\n-\n@@ -2603,0 +2577,43 @@\n+\/\/=============================================================================\n+#ifndef PRODUCT\n+void MachVEPNode::format(PhaseRegAlloc* ra_, outputStream* st) const\n+{\n+  st->print_cr(\"MachVEPNode\");\n+}\n+#endif\n+\n+void MachVEPNode::emit(C2_MacroAssembler* masm, PhaseRegAlloc* ra_) const\n+{\n+  CodeBuffer* cbuf = masm->code();\n+  uint insts_size = cbuf->insts_size();\n+  if (!_verified) {\n+    __ ic_check(1);\n+  } else {\n+    \/\/ TODO 8284443 Avoid creation of temporary frame\n+    if (ra_->C->stub_function() == nullptr) {\n+      __ verified_entry(ra_->C, 0);\n+      __ entry_barrier();\n+      int initial_framesize = ra_->C->output()->frame_size_in_bytes() - 2*wordSize;\n+      __ remove_frame(initial_framesize, false);\n+    }\n+    \/\/ Unpack inline type args passed as oop and then jump to\n+    \/\/ the verified entry point (skipping the unverified entry).\n+    int sp_inc = __ unpack_inline_args(ra_->C, _receiver_only);\n+    \/\/ Emit code for verified entry and save increment for stack repair on return\n+    __ verified_entry(ra_->C, sp_inc);\n+    if (Compile::current()->output()->in_scratch_emit_size()) {\n+      Label dummy_verified_entry;\n+      __ jmp(dummy_verified_entry);\n+    } else {\n+      __ jmp(*_verified_entry);\n+    }\n+  }\n+  \/* WARNING these NOPs are critical so that verified entry point is properly\n+     4 bytes aligned for patching by NativeJump::patch_verified_entry() *\/\n+  int nops_cnt = 4 - ((cbuf->insts_size() - insts_size) & 0x3);\n+  nops_cnt &= 0x3; \/\/ Do not add nops if code is aligned.\n+  if (nops_cnt > 0) {\n+    __ nop(nops_cnt);\n+  }\n+}\n+\n@@ -2623,6 +2640,0 @@\n-uint MachUEPNode::size(PhaseRegAlloc* ra_) const\n-{\n-  return MachNode::size(ra_); \/\/ too many variables; just compute it\n-                              \/\/ the hard way\n-}\n-\n@@ -4532,0 +4543,33 @@\n+    if (tf()->returns_inline_type_as_fields() && !_method->is_method_handle_intrinsic() && _method->return_type()->is_loaded()) {\n+      \/\/ The last return value is not set by the callee but used to pass the null marker to compiled code.\n+      \/\/ Search for the corresponding projection, get the register and emit code that initialized it.\n+      uint con = (tf()->range_cc()->cnt() - 1);\n+      for (DUIterator_Fast imax, i = fast_outs(imax); i < imax; i++) {\n+        ProjNode* proj = fast_out(i)->as_Proj();\n+        if (proj->_con == con) {\n+          \/\/ Set null marker if rax is non-null (a non-null value is returned buffered or scalarized)\n+          OptoReg::Name optoReg = ra_->get_reg_first(proj);\n+          VMReg reg = OptoReg::as_VMReg(optoReg, ra_->_framesize, OptoReg::reg2stack(ra_->_matcher._new_SP));\n+          Register toReg = reg->is_reg() ? reg->as_Register() : rscratch1;\n+          __ testq(rax, rax);\n+          __ setb(Assembler::notZero, toReg);\n+          __ movzbl(toReg, toReg);\n+          if (reg->is_stack()) {\n+            int st_off = reg->reg2stack() * VMRegImpl::stack_slot_size;\n+            __ movq(Address(rsp, st_off), toReg);\n+          }\n+          break;\n+        }\n+      }\n+      if (return_value_is_used()) {\n+        \/\/ An inline type is returned as fields in multiple registers.\n+        \/\/ Rax either contains an oop if the inline type is buffered or a pointer\n+        \/\/ to the corresponding InlineKlass with the lowest bit set to 1. Zero rax\n+        \/\/ if the lowest bit is set to allow C2 to use the oop after null checking.\n+        \/\/ rax &= (rax & 1) - 1\n+        __ movptr(rscratch1, rax);\n+        __ andptr(rscratch1, 0x1);\n+        __ subptr(rscratch1, 0x1);\n+        __ andptr(rax, rscratch1);\n+      }\n+    }\n@@ -5694,0 +5738,16 @@\n+\/\/ Indirect Narrow Oop Operand\n+operand indCompressedOop(rRegN reg) %{\n+  predicate(UseCompressedOops && (CompressedOops::shift() == Address::times_8));\n+  constraint(ALLOC_IN_RC(ptr_reg));\n+  match(DecodeN reg);\n+\n+  op_cost(10);\n+  format %{\"[R12 + $reg << 3] (compressed oop addressing)\" %}\n+  interface(MEMORY_INTER) %{\n+    base(0xc); \/\/ R12\n+    index($reg);\n+    scale(0x3);\n+    disp(0x0);\n+  %}\n+%}\n+\n@@ -6170,1 +6230,1 @@\n-               indCompressedOopOffset,\n+               indCompressedOop, indCompressedOopOffset,\n@@ -8768,0 +8828,26 @@\n+instruct castI2N(rRegN dst, rRegI src)\n+%{\n+  match(Set dst (CastI2N src));\n+\n+  format %{ \"movq    $dst, $src\\t# int -> narrow ptr\" %}\n+  ins_encode %{\n+    if ($dst$$reg != $src$$reg) {\n+      __ movl($dst$$Register, $src$$Register);\n+    }\n+  %}\n+  ins_pipe(ialu_reg_reg); \/\/ XXX\n+%}\n+\n+instruct castN2X(rRegL dst, rRegN src)\n+%{\n+  match(Set dst (CastP2X src));\n+\n+  format %{ \"movq    $dst, $src\\t# ptr -> long\" %}\n+  ins_encode %{\n+    if ($dst$$reg != $src$$reg) {\n+      __ movptr($dst$$Register, $src$$Register);\n+    }\n+  %}\n+  ins_pipe(ialu_reg_reg); \/\/ XXX\n+%}\n+\n@@ -14991,0 +15077,1 @@\n+\n@@ -14993,1 +15080,1 @@\n-instruct rep_stos(rcx_RegL cnt, rdi_RegP base, regD tmp, rax_RegI zero,\n+instruct rep_stos(rcx_RegL cnt, rdi_RegP base, regD tmp, rax_RegL val,\n@@ -14996,3 +15083,120 @@\n-  predicate(!((ClearArrayNode*)n)->is_large() && (UseAVX <= 2));\n-  match(Set dummy (ClearArray cnt base));\n-  effect(USE_KILL cnt, USE_KILL base, TEMP tmp, KILL zero, KILL cr);\n+  predicate(!((ClearArrayNode*)n)->is_large() && !((ClearArrayNode*)n)->word_copy_only() && (UseAVX <= 2));\n+  match(Set dummy (ClearArray (Binary cnt base) val));\n+  effect(USE_KILL cnt, USE_KILL base, TEMP tmp, USE_KILL val, KILL cr);\n+\n+  format %{ $$template\n+    $$emit$$\"cmp     InitArrayShortSize,rcx\\n\\t\"\n+    $$emit$$\"jg      LARGE\\n\\t\"\n+    $$emit$$\"dec     rcx\\n\\t\"\n+    $$emit$$\"js      DONE\\t# Zero length\\n\\t\"\n+    $$emit$$\"mov     rax,(rdi,rcx,8)\\t# LOOP\\n\\t\"\n+    $$emit$$\"dec     rcx\\n\\t\"\n+    $$emit$$\"jge     LOOP\\n\\t\"\n+    $$emit$$\"jmp     DONE\\n\\t\"\n+    $$emit$$\"# LARGE:\\n\\t\"\n+    if (UseFastStosb) {\n+       $$emit$$\"shlq    rcx,3\\t# Convert doublewords to bytes\\n\\t\"\n+       $$emit$$\"rep     stosb\\t# Store rax to *rdi++ while rcx--\\n\\t\"\n+    } else if (UseXMMForObjInit) {\n+       $$emit$$\"movdq   $tmp, $val\\n\\t\"\n+       $$emit$$\"punpcklqdq $tmp, $tmp\\n\\t\"\n+       $$emit$$\"vinserti128_high $tmp, $tmp\\n\\t\"\n+       $$emit$$\"jmpq    L_zero_64_bytes\\n\\t\"\n+       $$emit$$\"# L_loop:\\t# 64-byte LOOP\\n\\t\"\n+       $$emit$$\"vmovdqu $tmp,(rax)\\n\\t\"\n+       $$emit$$\"vmovdqu $tmp,0x20(rax)\\n\\t\"\n+       $$emit$$\"add     0x40,rax\\n\\t\"\n+       $$emit$$\"# L_zero_64_bytes:\\n\\t\"\n+       $$emit$$\"sub     0x8,rcx\\n\\t\"\n+       $$emit$$\"jge     L_loop\\n\\t\"\n+       $$emit$$\"add     0x4,rcx\\n\\t\"\n+       $$emit$$\"jl      L_tail\\n\\t\"\n+       $$emit$$\"vmovdqu $tmp,(rax)\\n\\t\"\n+       $$emit$$\"add     0x20,rax\\n\\t\"\n+       $$emit$$\"sub     0x4,rcx\\n\\t\"\n+       $$emit$$\"# L_tail:\\t# Clearing tail bytes\\n\\t\"\n+       $$emit$$\"add     0x4,rcx\\n\\t\"\n+       $$emit$$\"jle     L_end\\n\\t\"\n+       $$emit$$\"dec     rcx\\n\\t\"\n+       $$emit$$\"# L_sloop:\\t# 8-byte short loop\\n\\t\"\n+       $$emit$$\"vmovq   xmm0,(rax)\\n\\t\"\n+       $$emit$$\"add     0x8,rax\\n\\t\"\n+       $$emit$$\"dec     rcx\\n\\t\"\n+       $$emit$$\"jge     L_sloop\\n\\t\"\n+       $$emit$$\"# L_end:\\n\\t\"\n+    } else {\n+       $$emit$$\"rep     stosq\\t# Store rax to *rdi++ while rcx--\\n\\t\"\n+    }\n+    $$emit$$\"# DONE\"\n+  %}\n+  ins_encode %{\n+    __ clear_mem($base$$Register, $cnt$$Register, $val$$Register,\n+                 $tmp$$XMMRegister, false, false);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct rep_stos_word_copy(rcx_RegL cnt, rdi_RegP base, regD tmp, rax_RegL val,\n+                            Universe dummy, rFlagsReg cr)\n+%{\n+  predicate(!((ClearArrayNode*)n)->is_large() && ((ClearArrayNode*)n)->word_copy_only() && (UseAVX <= 2));\n+  match(Set dummy (ClearArray (Binary cnt base) val));\n+  effect(USE_KILL cnt, USE_KILL base, TEMP tmp, USE_KILL val, KILL cr);\n+\n+  format %{ $$template\n+    $$emit$$\"cmp     InitArrayShortSize,rcx\\n\\t\"\n+    $$emit$$\"jg      LARGE\\n\\t\"\n+    $$emit$$\"dec     rcx\\n\\t\"\n+    $$emit$$\"js      DONE\\t# Zero length\\n\\t\"\n+    $$emit$$\"mov     rax,(rdi,rcx,8)\\t# LOOP\\n\\t\"\n+    $$emit$$\"dec     rcx\\n\\t\"\n+    $$emit$$\"jge     LOOP\\n\\t\"\n+    $$emit$$\"jmp     DONE\\n\\t\"\n+    $$emit$$\"# LARGE:\\n\\t\"\n+    if (UseXMMForObjInit) {\n+       $$emit$$\"movdq   $tmp, $val\\n\\t\"\n+       $$emit$$\"punpcklqdq $tmp, $tmp\\n\\t\"\n+       $$emit$$\"vinserti128_high $tmp, $tmp\\n\\t\"\n+       $$emit$$\"jmpq    L_zero_64_bytes\\n\\t\"\n+       $$emit$$\"# L_loop:\\t# 64-byte LOOP\\n\\t\"\n+       $$emit$$\"vmovdqu $tmp,(rax)\\n\\t\"\n+       $$emit$$\"vmovdqu $tmp,0x20(rax)\\n\\t\"\n+       $$emit$$\"add     0x40,rax\\n\\t\"\n+       $$emit$$\"# L_zero_64_bytes:\\n\\t\"\n+       $$emit$$\"sub     0x8,rcx\\n\\t\"\n+       $$emit$$\"jge     L_loop\\n\\t\"\n+       $$emit$$\"add     0x4,rcx\\n\\t\"\n+       $$emit$$\"jl      L_tail\\n\\t\"\n+       $$emit$$\"vmovdqu $tmp,(rax)\\n\\t\"\n+       $$emit$$\"add     0x20,rax\\n\\t\"\n+       $$emit$$\"sub     0x4,rcx\\n\\t\"\n+       $$emit$$\"# L_tail:\\t# Clearing tail bytes\\n\\t\"\n+       $$emit$$\"add     0x4,rcx\\n\\t\"\n+       $$emit$$\"jle     L_end\\n\\t\"\n+       $$emit$$\"dec     rcx\\n\\t\"\n+       $$emit$$\"# L_sloop:\\t# 8-byte short loop\\n\\t\"\n+       $$emit$$\"vmovq   xmm0,(rax)\\n\\t\"\n+       $$emit$$\"add     0x8,rax\\n\\t\"\n+       $$emit$$\"dec     rcx\\n\\t\"\n+       $$emit$$\"jge     L_sloop\\n\\t\"\n+       $$emit$$\"# L_end:\\n\\t\"\n+    } else {\n+       $$emit$$\"rep     stosq\\t# Store rax to *rdi++ while rcx--\\n\\t\"\n+    }\n+    $$emit$$\"# DONE\"\n+  %}\n+  ins_encode %{\n+    __ clear_mem($base$$Register, $cnt$$Register, $val$$Register,\n+                 $tmp$$XMMRegister, false, true);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ Small non-constant length ClearArray for AVX512 targets.\n+instruct rep_stos_evex(rcx_RegL cnt, rdi_RegP base, legRegD tmp, kReg ktmp, rax_RegL val,\n+                       Universe dummy, rFlagsReg cr)\n+%{\n+  predicate(!((ClearArrayNode*)n)->is_large() && !((ClearArrayNode*)n)->word_copy_only() && (UseAVX > 2));\n+  match(Set dummy (ClearArray (Binary cnt base) val));\n+  ins_cost(125);\n+  effect(USE_KILL cnt, USE_KILL base, TEMP tmp, TEMP ktmp, USE_KILL val, KILL cr);\n@@ -15046,2 +15250,2 @@\n-    __ clear_mem($base$$Register, $cnt$$Register, $zero$$Register,\n-                 $tmp$$XMMRegister, false, knoreg);\n+    __ clear_mem($base$$Register, $cnt$$Register, $val$$Register,\n+                 $tmp$$XMMRegister, false, false, $ktmp$$KRegister);\n@@ -15052,3 +15256,2 @@\n-\/\/ Small non-constant length ClearArray for AVX512 targets.\n-instruct rep_stos_evex(rcx_RegL cnt, rdi_RegP base, legRegD tmp, kReg ktmp, rax_RegI zero,\n-                       Universe dummy, rFlagsReg cr)\n+instruct rep_stos_evex_word_copy(rcx_RegL cnt, rdi_RegP base, legRegD tmp, kReg ktmp, rax_RegL val,\n+                                 Universe dummy, rFlagsReg cr)\n@@ -15056,2 +15259,2 @@\n-  predicate(!((ClearArrayNode*)n)->is_large() && (UseAVX > 2));\n-  match(Set dummy (ClearArray cnt base));\n+  predicate(!((ClearArrayNode*)n)->is_large() && ((ClearArrayNode*)n)->word_copy_only() && (UseAVX > 2));\n+  match(Set dummy (ClearArray (Binary cnt base) val));\n@@ -15059,1 +15262,1 @@\n-  effect(USE_KILL cnt, USE_KILL base, TEMP tmp, TEMP ktmp, KILL zero, KILL cr);\n+  effect(USE_KILL cnt, USE_KILL base, TEMP tmp, TEMP ktmp, USE_KILL val, KILL cr);\n@@ -15107,2 +15310,2 @@\n-    __ clear_mem($base$$Register, $cnt$$Register, $zero$$Register,\n-                 $tmp$$XMMRegister, false, $ktmp$$KRegister);\n+    __ clear_mem($base$$Register, $cnt$$Register, $val$$Register,\n+                 $tmp$$XMMRegister, false, true, $ktmp$$KRegister);\n@@ -15114,1 +15317,1 @@\n-instruct rep_stos_large(rcx_RegL cnt, rdi_RegP base, regD tmp, rax_RegI zero,\n+instruct rep_stos_large(rcx_RegL cnt, rdi_RegP base, regD tmp, rax_RegL val,\n@@ -15117,3 +15320,99 @@\n-  predicate((UseAVX <=2) && ((ClearArrayNode*)n)->is_large());\n-  match(Set dummy (ClearArray cnt base));\n-  effect(USE_KILL cnt, USE_KILL base, TEMP tmp, KILL zero, KILL cr);\n+  predicate(((ClearArrayNode*)n)->is_large() && !((ClearArrayNode*)n)->word_copy_only() && (UseAVX <= 2));\n+  match(Set dummy (ClearArray (Binary cnt base) val));\n+  effect(USE_KILL cnt, USE_KILL base, TEMP tmp, USE_KILL val, KILL cr);\n+\n+  format %{ $$template\n+    if (UseFastStosb) {\n+       $$emit$$\"shlq    rcx,3\\t# Convert doublewords to bytes\\n\\t\"\n+       $$emit$$\"rep     stosb\\t# Store rax to *rdi++ while rcx--\"\n+    } else if (UseXMMForObjInit) {\n+       $$emit$$\"movdq   $tmp, $val\\n\\t\"\n+       $$emit$$\"punpcklqdq $tmp, $tmp\\n\\t\"\n+       $$emit$$\"vinserti128_high $tmp, $tmp\\n\\t\"\n+       $$emit$$\"jmpq    L_zero_64_bytes\\n\\t\"\n+       $$emit$$\"# L_loop:\\t# 64-byte LOOP\\n\\t\"\n+       $$emit$$\"vmovdqu $tmp,(rax)\\n\\t\"\n+       $$emit$$\"vmovdqu $tmp,0x20(rax)\\n\\t\"\n+       $$emit$$\"add     0x40,rax\\n\\t\"\n+       $$emit$$\"# L_zero_64_bytes:\\n\\t\"\n+       $$emit$$\"sub     0x8,rcx\\n\\t\"\n+       $$emit$$\"jge     L_loop\\n\\t\"\n+       $$emit$$\"add     0x4,rcx\\n\\t\"\n+       $$emit$$\"jl      L_tail\\n\\t\"\n+       $$emit$$\"vmovdqu $tmp,(rax)\\n\\t\"\n+       $$emit$$\"add     0x20,rax\\n\\t\"\n+       $$emit$$\"sub     0x4,rcx\\n\\t\"\n+       $$emit$$\"# L_tail:\\t# Clearing tail bytes\\n\\t\"\n+       $$emit$$\"add     0x4,rcx\\n\\t\"\n+       $$emit$$\"jle     L_end\\n\\t\"\n+       $$emit$$\"dec     rcx\\n\\t\"\n+       $$emit$$\"# L_sloop:\\t# 8-byte short loop\\n\\t\"\n+       $$emit$$\"vmovq   xmm0,(rax)\\n\\t\"\n+       $$emit$$\"add     0x8,rax\\n\\t\"\n+       $$emit$$\"dec     rcx\\n\\t\"\n+       $$emit$$\"jge     L_sloop\\n\\t\"\n+       $$emit$$\"# L_end:\\n\\t\"\n+    } else {\n+       $$emit$$\"rep     stosq\\t# Store rax to *rdi++ while rcx--\"\n+    }\n+  %}\n+  ins_encode %{\n+    __ clear_mem($base$$Register, $cnt$$Register, $val$$Register,\n+                 $tmp$$XMMRegister, true, false);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct rep_stos_large_word_copy(rcx_RegL cnt, rdi_RegP base, regD tmp, rax_RegL val,\n+                                  Universe dummy, rFlagsReg cr)\n+%{\n+  predicate(((ClearArrayNode*)n)->is_large() && ((ClearArrayNode*)n)->word_copy_only() && (UseAVX <= 2));\n+  match(Set dummy (ClearArray (Binary cnt base) val));\n+  effect(USE_KILL cnt, USE_KILL base, TEMP tmp, USE_KILL val, KILL cr);\n+\n+  format %{ $$template\n+    if (UseXMMForObjInit) {\n+       $$emit$$\"movdq   $tmp, $val\\n\\t\"\n+       $$emit$$\"punpcklqdq $tmp, $tmp\\n\\t\"\n+       $$emit$$\"vinserti128_high $tmp, $tmp\\n\\t\"\n+       $$emit$$\"jmpq    L_zero_64_bytes\\n\\t\"\n+       $$emit$$\"# L_loop:\\t# 64-byte LOOP\\n\\t\"\n+       $$emit$$\"vmovdqu $tmp,(rax)\\n\\t\"\n+       $$emit$$\"vmovdqu $tmp,0x20(rax)\\n\\t\"\n+       $$emit$$\"add     0x40,rax\\n\\t\"\n+       $$emit$$\"# L_zero_64_bytes:\\n\\t\"\n+       $$emit$$\"sub     0x8,rcx\\n\\t\"\n+       $$emit$$\"jge     L_loop\\n\\t\"\n+       $$emit$$\"add     0x4,rcx\\n\\t\"\n+       $$emit$$\"jl      L_tail\\n\\t\"\n+       $$emit$$\"vmovdqu $tmp,(rax)\\n\\t\"\n+       $$emit$$\"add     0x20,rax\\n\\t\"\n+       $$emit$$\"sub     0x4,rcx\\n\\t\"\n+       $$emit$$\"# L_tail:\\t# Clearing tail bytes\\n\\t\"\n+       $$emit$$\"add     0x4,rcx\\n\\t\"\n+       $$emit$$\"jle     L_end\\n\\t\"\n+       $$emit$$\"dec     rcx\\n\\t\"\n+       $$emit$$\"# L_sloop:\\t# 8-byte short loop\\n\\t\"\n+       $$emit$$\"vmovq   xmm0,(rax)\\n\\t\"\n+       $$emit$$\"add     0x8,rax\\n\\t\"\n+       $$emit$$\"dec     rcx\\n\\t\"\n+       $$emit$$\"jge     L_sloop\\n\\t\"\n+       $$emit$$\"# L_end:\\n\\t\"\n+    } else {\n+       $$emit$$\"rep     stosq\\t# Store rax to *rdi++ while rcx--\"\n+    }\n+  %}\n+  ins_encode %{\n+    __ clear_mem($base$$Register, $cnt$$Register, $val$$Register,\n+                 $tmp$$XMMRegister, true, true);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ Large non-constant length ClearArray for AVX512 targets.\n+instruct rep_stos_large_evex(rcx_RegL cnt, rdi_RegP base, legRegD tmp, kReg ktmp, rax_RegL val,\n+                             Universe dummy, rFlagsReg cr)\n+%{\n+  predicate(((ClearArrayNode*)n)->is_large() && !((ClearArrayNode*)n)->word_copy_only() && (UseAVX > 2));\n+  match(Set dummy (ClearArray (Binary cnt base) val));\n+  effect(USE_KILL cnt, USE_KILL base, TEMP tmp, TEMP ktmp, USE_KILL val, KILL cr);\n@@ -15158,2 +15457,2 @@\n-    __ clear_mem($base$$Register, $cnt$$Register, $zero$$Register,\n-                 $tmp$$XMMRegister, true, knoreg);\n+    __ clear_mem($base$$Register, $cnt$$Register, $val$$Register,\n+                 $tmp$$XMMRegister, true, false, $ktmp$$KRegister);\n@@ -15164,3 +15463,2 @@\n-\/\/ Large non-constant length ClearArray for AVX512 targets.\n-instruct rep_stos_large_evex(rcx_RegL cnt, rdi_RegP base, legRegD tmp, kReg ktmp, rax_RegI zero,\n-                             Universe dummy, rFlagsReg cr)\n+instruct rep_stos_large_evex_word_copy(rcx_RegL cnt, rdi_RegP base, legRegD tmp, kReg ktmp, rax_RegL val,\n+                                       Universe dummy, rFlagsReg cr)\n@@ -15168,3 +15466,3 @@\n-  predicate((UseAVX > 2) && ((ClearArrayNode*)n)->is_large());\n-  match(Set dummy (ClearArray cnt base));\n-  effect(USE_KILL cnt, USE_KILL base, TEMP tmp, TEMP ktmp, KILL zero, KILL cr);\n+  predicate(((ClearArrayNode*)n)->is_large() && ((ClearArrayNode*)n)->word_copy_only() && (UseAVX > 2));\n+  match(Set dummy (ClearArray (Binary cnt base) val));\n+  effect(USE_KILL cnt, USE_KILL base, TEMP tmp, TEMP ktmp, USE_KILL val, KILL cr);\n@@ -15209,2 +15507,2 @@\n-    __ clear_mem($base$$Register, $cnt$$Register, $zero$$Register,\n-                 $tmp$$XMMRegister, true, $ktmp$$KRegister);\n+    __ clear_mem($base$$Register, $cnt$$Register, $val$$Register,\n+                 $tmp$$XMMRegister, true, true, $ktmp$$KRegister);\n@@ -15216,1 +15514,1 @@\n-instruct rep_stos_im(immL cnt, rRegP base, regD tmp, rRegI zero, kReg ktmp, Universe dummy, rFlagsReg cr)\n+instruct rep_stos_im(immL cnt, rRegP base, regD tmp, rax_RegL val, kReg ktmp, Universe dummy, rFlagsReg cr)\n@@ -15218,2 +15516,3 @@\n-  predicate(!((ClearArrayNode*)n)->is_large() && (MaxVectorSize >= 32) && VM_Version::supports_avx512vl());\n-  match(Set dummy (ClearArray cnt base));\n+  predicate(!((ClearArrayNode*)n)->is_large() && !((ClearArrayNode*)n)->word_copy_only() &&\n+            ((MaxVectorSize >= 32) && VM_Version::supports_avx512vl()));\n+  match(Set dummy (ClearArray (Binary cnt base) val));\n@@ -15221,1 +15520,1 @@\n-  effect(TEMP tmp, TEMP zero, TEMP ktmp, KILL cr);\n+  effect(TEMP tmp, USE_KILL val, TEMP ktmp, KILL cr);\n@@ -15224,1 +15523,1 @@\n-   __ clear_mem($base$$Register, $cnt$$constant, $zero$$Register, $tmp$$XMMRegister, $ktmp$$KRegister);\n+    __ clear_mem($base$$Register, $cnt$$constant, $val$$Register, $tmp$$XMMRegister, $ktmp$$KRegister);\n@@ -17069,0 +17368,16 @@\n+\/\/ Call runtime without safepoint\n+\/\/ entry point is null, target holds the address to call\n+instruct CallLeafNoFPInDirect(rRegP target)\n+%{\n+  predicate(n->as_Call()->entry_point() == nullptr);\n+  match(CallLeafNoFP target);\n+\n+  ins_cost(300);\n+  format %{ \"call_leaf_nofp,runtime indirect \" %}\n+  ins_encode %{\n+     __ call($target$$Register);\n+  %}\n+\n+  ins_pipe(pipe_slow);\n+%}\n+\n@@ -17072,0 +17387,1 @@\n+  predicate(n->as_Call()->entry_point() != nullptr);\n","filename":"src\/hotspot\/cpu\/x86\/x86.ad","additions":396,"deletions":80,"binary":false,"changes":476,"status":"modified"},{"patch":"@@ -80,0 +80,2 @@\n+#include \"oops\/flatArrayKlass.hpp\"\n+#include \"oops\/inlineKlass.hpp\"\n@@ -505,1 +507,1 @@\n-  soc->do_tag(objArrayOopDesc::base_offset_in_bytes());\n+  soc->do_tag(refArrayOopDesc::base_offset_in_bytes());\n@@ -1392,0 +1394,5 @@\n+  if (CDSConfig::is_valhalla_preview()) {\n+    log_info(cds)(\"Archived java heap is not yet supported with Valhalla preview\");\n+    return;\n+  }\n+\n","filename":"src\/hotspot\/share\/cds\/aotMetaspace.cpp","additions":8,"deletions":1,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -28,0 +28,1 @@\n+#include \"ci\/ciInlineKlass.hpp\"\n@@ -479,1 +480,2 @@\n-      (sym->char_at(1) == JVM_SIGNATURE_ARRAY || sym->char_at(1) == JVM_SIGNATURE_CLASS)) {\n+      (sym->char_at(1) == JVM_SIGNATURE_ARRAY ||\n+       sym->char_at(1) == JVM_SIGNATURE_CLASS )) {\n@@ -492,1 +494,1 @@\n-      return ciObjArrayKlass::make_impl(elem_klass);\n+      return ciArrayKlass::make(elem_klass);\n@@ -518,0 +520,4 @@\n+  int i = 0;\n+  while (sym->char_at(i) == JVM_SIGNATURE_ARRAY) {\n+    i++;\n+  }\n","filename":"src\/hotspot\/share\/ci\/ciEnv.cpp","additions":8,"deletions":2,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -24,0 +24,1 @@\n+\n@@ -54,0 +55,1 @@\n+#include \"oops\/inlineKlass.inline.hpp\"\n@@ -88,0 +90,1 @@\n+#include \"utilities\/stringUtils.hpp\"\n@@ -160,0 +163,2 @@\n+#define CONSTANT_CLASS_DESCRIPTORS        70\n+\n@@ -198,1 +203,1 @@\n-      case JVM_CONSTANT_Class : {\n+      case JVM_CONSTANT_Class: {\n@@ -502,0 +507,3 @@\n+\n+        Symbol* const name = cp->symbol_at(class_index);\n+        const unsigned int name_len = name->utf8_length();\n@@ -711,1 +719,1 @@\n-            } else if (!Signature::is_void_method(signature)) { \/\/ must have void signature.\n+            } else if (!Signature::is_void_method(signature)) {  \/\/ must have void signature.\n@@ -731,2 +739,3 @@\n-            if (ref_kind == JVM_REF_newInvokeSpecial) {\n-              if (name != vmSymbols::object_initializer_name()) {\n+\n+            if (name != vmSymbols::object_initializer_name()) { \/\/ !<init>\n+              if (ref_kind == JVM_REF_newInvokeSpecial) {\n@@ -738,2 +747,10 @@\n-            } else {\n-              if (name == vmSymbols::object_initializer_name()) {\n+            } else { \/\/ <init>\n+              \/\/ The allowed invocation mode of <init> depends on its signature.\n+              \/\/ This test corresponds to verify_invoke_instructions in the verifier.\n+              const int signature_ref_index =\n+                cp->signature_ref_index_at(name_and_type_ref_index);\n+              const Symbol* const signature = cp->symbol_at(signature_ref_index);\n+              if (signature->is_void_method_signature()\n+                  && ref_kind == JVM_REF_newInvokeSpecial) {\n+                \/\/ OK, could be a constructor call\n+              } else {\n@@ -791,4 +808,13 @@\n-\/\/ Side-effects: populates the _local_interfaces field\n-void ClassFileParser::parse_interfaces(const ClassFileStream* const stream,\n-                                       const int itfs_len,\n-                                       ConstantPool* const cp,\n+static void check_identity_and_value_modifiers(ClassFileParser* current, const InstanceKlass* super_type, TRAPS) {\n+  assert(super_type != nullptr,\"Method doesn't support null super type\");\n+  if (super_type->access_flags().is_identity_class() && !current->access_flags().is_identity_class()\n+      && super_type->name() != vmSymbols::java_lang_Object()) {\n+      THROW_MSG(vmSymbols::java_lang_IncompatibleClassChangeError(),\n+                err_msg(\"Value type %s has an identity type as supertype\",\n+                current->class_name()->as_klass_external_name()));\n+  }\n+}\n+\n+void ClassFileParser::parse_interfaces(const ClassFileStream* stream,\n+                                       int itfs_len,\n+                                       ConstantPool* cp,\n@@ -796,0 +822,6 @@\n+                                       \/\/ FIXME: lots of these functions\n+                                       \/\/ declare their parameters as const,\n+                                       \/\/ which adds only noise to the code.\n+                                       \/\/ Remove the spurious const modifiers.\n+                                       \/\/ Many are of the form \"const int x\"\n+                                       \/\/ or \"T* const x\".\n@@ -803,0 +835,1 @@\n+\n@@ -805,3 +838,2 @@\n-    _local_interfaces = MetadataFactory::new_array<InstanceKlass*>(_loader_data, itfs_len, nullptr, CHECK);\n-\n-    int index;\n+    _local_interface_indexes = new GrowableArray<u2>(itfs_len);\n+    int index = 0;\n@@ -810,1 +842,0 @@\n-      Klass* interf;\n@@ -815,29 +846,1 @@\n-      if (cp->tag_at(interface_index).is_klass()) {\n-        interf = cp->resolved_klass_at(interface_index);\n-      } else {\n-        Symbol* const unresolved_klass  = cp->klass_name_at(interface_index);\n-\n-        \/\/ Don't need to check legal name because it's checked when parsing constant pool.\n-        \/\/ But need to make sure it's not an array type.\n-        guarantee_property(unresolved_klass->char_at(0) != JVM_SIGNATURE_ARRAY,\n-                           \"Bad interface name in class file %s\", CHECK);\n-\n-        \/\/ Call resolve on the interface class name with class circularity checking\n-        interf = SystemDictionary::resolve_super_or_fail(_class_name,\n-                                                         unresolved_klass,\n-                                                         Handle(THREAD, _loader_data->class_loader()),\n-                                                         false, CHECK);\n-      }\n-\n-      if (!interf->is_interface()) {\n-        THROW_MSG(vmSymbols::java_lang_IncompatibleClassChangeError(),\n-                  err_msg(\"class %s can not implement %s, because it is not an interface (%s)\",\n-                          _class_name->as_klass_external_name(),\n-                          interf->external_name(),\n-                          interf->class_in_module_of_loader()));\n-      }\n-\n-      if (InstanceKlass::cast(interf)->has_nonstatic_concrete_methods()) {\n-        *has_nonstatic_concrete_methods = true;\n-      }\n-      _local_interfaces->at_put(index, InstanceKlass::cast(interf));\n+      _local_interface_indexes->at_put_grow(index, interface_index);\n@@ -855,2 +858,1 @@\n-      const InstanceKlass* const k = _local_interfaces->at(index);\n-      Symbol* interface_name = k->name();\n+      Symbol* interface_name = cp->klass_name_at(_local_interface_indexes->at(index));\n@@ -943,0 +945,2 @@\n+    _jdk_internal_LooselyConsistentValue,\n+    _jdk_internal_NullRestricted,\n@@ -1369,1 +1373,1 @@\n-                                   bool is_interface,\n+                                   AccessFlags class_access_flags,\n@@ -1382,0 +1386,2 @@\n+  bool is_inline_type = !class_access_flags.is_identity_class() && !class_access_flags.is_abstract();\n+  bool is_value_class = !class_access_flags.is_identity_class() && !class_access_flags.is_interface();\n@@ -1389,1 +1395,6 @@\n-  const int total_fields = length + num_injected;\n+\n+  \/\/ two more slots are required for inline classes:\n+  \/\/ one for the static field with a reference to the pre-allocated default value\n+  \/\/ one for the field the JVM injects when detecting an empty inline class\n+  const int total_fields = length + num_injected + (is_inline_type ? 2 : 0)\n+                           + ((UseAltSubstitutabilityMethod && is_value_class) ? 1 : 0);\n@@ -1395,0 +1406,1 @@\n+  int instance_fields_count = 0;\n@@ -1400,0 +1412,7 @@\n+    jint recognized_modifiers = JVM_RECOGNIZED_FIELD_MODIFIERS;\n+    if (!supports_inline_types()) {\n+      recognized_modifiers &= ~JVM_ACC_STRICT;\n+    }\n+\n+    const jint flags = cfs->get_u2_fast() & recognized_modifiers;\n+    verify_legal_field_modifiers(flags, class_access_flags, CHECK);\n@@ -1401,2 +1420,0 @@\n-    const jint flags = cfs->get_u2_fast() & JVM_RECOGNIZED_FIELD_MODIFIERS;\n-    verify_legal_field_modifiers(flags, is_interface, CHECK);\n@@ -1419,0 +1436,1 @@\n+    if (!access_flags.is_static()) instance_fields_count++;\n@@ -1426,0 +1444,2 @@\n+    bool is_null_restricted = false;\n+\n@@ -1445,0 +1465,18 @@\n+        if (parsed_annotations.has_annotation(AnnotationCollector::_jdk_internal_NullRestricted)) {\n+          if (!Signature::has_envelope(sig)) {\n+            Exceptions::fthrow(\n+              THREAD_AND_LOCATION,\n+              vmSymbols::java_lang_ClassFormatError(),\n+              \"Illegal use of @jdk.internal.vm.annotation.NullRestricted annotation on field %s.%s with signature %s (primitive types can never be null)\",\n+              class_name()->as_C_string(), name->as_C_string(), sig->as_C_string());\n+          }\n+          const bool is_strict = (flags & JVM_ACC_STRICT) != 0;\n+          if (!is_strict) {\n+            Exceptions::fthrow(\n+              THREAD_AND_LOCATION,\n+              vmSymbols::java_lang_ClassFormatError(),\n+              \"Illegal use of @jdk.internal.vm.annotation.NullRestricted annotation on field %s.%s which doesn't have the @jdk.internal.vm.annotation.Strict annotation\",\n+              class_name()->as_C_string(), name->as_C_string());\n+          }\n+          is_null_restricted = true;\n+        }\n@@ -1467,0 +1505,4 @@\n+    if (is_null_restricted) {\n+      fieldFlags.update_null_free_inline_type(true);\n+    }\n+\n@@ -1483,0 +1525,3 @@\n+    if (access_flags.is_strict() && access_flags.is_static()) {\n+      _has_strict_static_fields = true;\n+    }\n@@ -1487,1 +1532,0 @@\n-  int index = length;\n@@ -1515,3 +1559,2 @@\n-      fi.set_index(index);\n-      _temp_field_info->append(fi);\n-      index++;\n+      int idx = _temp_field_info->append(fi);\n+      _temp_field_info->adr_at(idx)->set_index(idx);\n@@ -1521,1 +1564,33 @@\n-  assert(_temp_field_info->length() == index, \"Must be\");\n+  if (is_inline_type) {\n+    \/\/ Inject static \".null_reset\" field. This is an all-zero value with its null-channel set to zero.\n+    \/\/ IT should never be seen by user code, it is used when writing \"null\" to a nullable flat field\n+    \/\/ The all-zero value ensure that any embedded oop will be set to null, to avoid keeping dead objects\n+    \/\/ alive.\n+    FieldInfo::FieldFlags fflags2(0);\n+    fflags2.update_injected(true);\n+    AccessFlags aflags2(JVM_ACC_STATIC);\n+    FieldInfo fi2(aflags2,\n+                 (u2)vmSymbols::as_int(VM_SYMBOL_ENUM_NAME(null_reset_value_name)),\n+                 (u2)vmSymbols::as_int(VM_SYMBOL_ENUM_NAME(object_signature)),\n+                 0,\n+                 fflags2);\n+    int idx2 = _temp_field_info->append(fi2);\n+    _temp_field_info->adr_at(idx2)->set_index(idx2);\n+    _static_oop_count++;\n+  }\n+  if (!access_flags().is_identity_class() && !access_flags().is_interface()\n+      && _class_name != vmSymbols::java_lang_Object() && UseAltSubstitutabilityMethod) {\n+    \/\/ Acmp map required for abstract and concrete value classes\n+    FieldInfo::FieldFlags fflags2(0);\n+    fflags2.update_injected(true);\n+    fflags2.update_stable(true);\n+    AccessFlags aflags2(JVM_ACC_STATIC | JVM_ACC_FINAL);\n+    FieldInfo fi3(aflags2,\n+                 (u2)vmSymbols::as_int(VM_SYMBOL_ENUM_NAME(acmp_maps_name)),\n+                 (u2)vmSymbols::as_int(VM_SYMBOL_ENUM_NAME(int_array_signature)),\n+                 0,\n+                 fflags2);\n+    int idx2 = _temp_field_info->append(fi3);\n+    _temp_field_info->adr_at(idx2)->set_index(idx2);\n+    _static_oop_count++;\n+  }\n@@ -1901,0 +1976,8 @@\n+    case VM_SYMBOL_ENUM_NAME(jdk_internal_vm_annotation_LooselyConsistentValue_signature): {\n+      if (_location != _in_class)   break; \/\/ only allow for classes\n+      return _jdk_internal_LooselyConsistentValue;\n+    }\n+    case VM_SYMBOL_ENUM_NAME(jdk_internal_vm_annotation_NullRestricted_signature): {\n+      if (_location != _in_field)   break; \/\/ only allow for fields\n+      return _jdk_internal_NullRestricted;\n+    }\n@@ -2137,0 +2220,2 @@\n+                                      bool is_value_class,\n+                                      bool is_abstract_class,\n@@ -2178,1 +2263,1 @@\n-    verify_legal_method_modifiers(flags, is_interface, name, CHECK_NULL);\n+    verify_legal_method_modifiers(flags, access_flags() , name, CHECK_NULL);\n@@ -2186,0 +2271,9 @@\n+  if (EnableValhalla) {\n+    if (((flags & JVM_ACC_SYNCHRONIZED) == JVM_ACC_SYNCHRONIZED)\n+        && ((flags & JVM_ACC_STATIC) == 0 )\n+        && !_access_flags.is_identity_class()) {\n+      classfile_parse_error(\"Invalid synchronized method in non-identity class %s\", THREAD);\n+        return nullptr;\n+    }\n+  }\n+\n@@ -2720,0 +2814,2 @@\n+                                    bool is_value_class,\n+                                    bool is_abstract_type,\n@@ -2744,0 +2840,2 @@\n+                                    is_value_class,\n+                                    is_abstract_type,\n@@ -3009,0 +3107,1 @@\n+\n@@ -3017,0 +3116,1 @@\n+\n@@ -3022,0 +3122,8 @@\n+    if (!supports_inline_types()) {\n+      const bool is_module = (flags & JVM_ACC_MODULE) != 0;\n+      const bool is_interface = (flags & JVM_ACC_INTERFACE) != 0;\n+      if (!is_module && !is_interface) {\n+        flags |= JVM_ACC_IDENTITY;\n+      }\n+    }\n+\n@@ -3127,0 +3235,43 @@\n+u2 ClassFileParser::parse_classfile_loadable_descriptors_attribute(const ClassFileStream* const cfs,\n+                                                                   const u1* const loadable_descriptors_attribute_start,\n+                                                                   TRAPS) {\n+  const u1* const current_mark = cfs->current();\n+  u2 length = 0;\n+  if (loadable_descriptors_attribute_start != nullptr) {\n+    cfs->set_current(loadable_descriptors_attribute_start);\n+    cfs->guarantee_more(2, CHECK_0);  \/\/ length\n+    length = cfs->get_u2_fast();\n+  }\n+  const int size = length;\n+  Array<u2>* const loadable_descriptors = MetadataFactory::new_array<u2>(_loader_data, size, CHECK_0);\n+  _loadable_descriptors = loadable_descriptors;\n+  if (length > 0) {\n+    int index = 0;\n+    cfs->guarantee_more(2 * length, CHECK_0);\n+    for (int n = 0; n < length; n++) {\n+      const u2 descriptor_index = cfs->get_u2_fast();\n+      guarantee_property(\n+        valid_symbol_at(descriptor_index),\n+        \"LoadableDescriptors descriptor_index %u has bad constant type in class file %s\",\n+        descriptor_index, CHECK_0);\n+      Symbol* descriptor = _cp->symbol_at(descriptor_index);\n+      bool valid = legal_field_signature(descriptor, CHECK_0);\n+      if(!valid) {\n+        ResourceMark rm(THREAD);\n+        Exceptions::fthrow(THREAD_AND_LOCATION,\n+          vmSymbols::java_lang_ClassFormatError(),\n+          \"Descriptor from LoadableDescriptors attribute at index \\\"%d\\\" in class %s has illegal signature \\\"%s\\\"\",\n+          descriptor_index, _class_name->as_C_string(), descriptor->as_C_string());\n+        return 0;\n+      }\n+      loadable_descriptors->at_put(index++, descriptor_index);\n+    }\n+    assert(index == size, \"wrong size\");\n+  }\n+\n+  \/\/ Restore buffer's current position.\n+  cfs->set_current(current_mark);\n+\n+  return length;\n+}\n+\n@@ -3377,0 +3528,2 @@\n+  \/\/ Set _loadable_descriptors attribute to default sentinel\n+  _loadable_descriptors = Universe::the_empty_short_array();\n@@ -3383,0 +3536,1 @@\n+  bool parsed_loadable_descriptors_attribute = false;\n@@ -3404,0 +3558,2 @@\n+  const u1* loadable_descriptors_attribute_start = nullptr;\n+  u4  loadable_descriptors_attribute_length = 0;\n@@ -3619,0 +3775,9 @@\n+            if (EnableValhalla && tag == vmSymbols::tag_loadable_descriptors()) {\n+              if (parsed_loadable_descriptors_attribute) {\n+                classfile_parse_error(\"Multiple LoadableDescriptors attributes in class file %s\", CHECK);\n+                return;\n+              }\n+              parsed_loadable_descriptors_attribute = true;\n+              loadable_descriptors_attribute_start = cfs->current();\n+              loadable_descriptors_attribute_length = attribute_length;\n+            }\n@@ -3695,0 +3860,12 @@\n+  if (parsed_loadable_descriptors_attribute) {\n+    const u2 num_classes = parse_classfile_loadable_descriptors_attribute(\n+                            cfs,\n+                            loadable_descriptors_attribute_start,\n+                            CHECK);\n+    if (_need_verify) {\n+      guarantee_property(\n+        loadable_descriptors_attribute_length == sizeof(num_classes) + sizeof(u2) * num_classes,\n+        \"Wrong LoadableDescriptors attribute length in class file %s\", CHECK);\n+    }\n+  }\n+\n@@ -3761,0 +3938,1 @@\n+  this_klass->set_loadable_descriptors(_loadable_descriptors);\n@@ -3764,0 +3942,1 @@\n+  this_klass->set_inline_layout_info_array(_inline_layout_info_array);\n@@ -3802,2 +3981,1 @@\n-                       \"Invalid superclass index %u in class file %s\",\n-                       super_class_index,\n+                       \"Invalid superclass index 0 in class file %s\",\n@@ -3995,0 +4173,6 @@\n+bool ClassFileParser::supports_inline_types() const {\n+  \/\/ Inline types are only supported by class file version 70.65535 and later\n+  return _major_version > JAVA_26_VERSION ||\n+         (_major_version == JAVA_26_VERSION && _minor_version == JAVA_PREVIEW_MINOR_VERSION);\n+}\n+\n@@ -4038,3 +4222,4 @@\n-  } else if (max_transitive_size == local_size) {\n-    \/\/ only local interfaces added, share local interface array\n-    return local_ifs;\n+    \/\/ The three lines below are commented to work around bug JDK-8245487\n+\/\/  } else if (max_transitive_size == local_size) {\n+\/\/    \/\/ only local interfaces added, share local interface array\n+\/\/    return local_ifs;\n@@ -4061,0 +4246,1 @@\n+\n@@ -4091,0 +4277,10 @@\n+    \/\/ The JVMS says that super classes for value types must not have the ACC_IDENTITY\n+    \/\/ flag set. But, java.lang.Object must still be allowed to be a direct super class\n+    \/\/ for a value classes.  So, it is treated as a special case for now.\n+    if (!this_klass->access_flags().is_identity_class() &&\n+        super->name() != vmSymbols::java_lang_Object() &&\n+        super->is_identity_class()) {\n+      classfile_icce_error(\"value class %s cannot inherit from class %s\", super, THREAD);\n+      return;\n+    }\n+\n@@ -4283,1 +4479,1 @@\n-  const bool is_super      = (flags & JVM_ACC_SUPER)      != 0;\n+  const bool is_identity   = (flags & JVM_ACC_IDENTITY)   != 0;\n@@ -4287,0 +4483,2 @@\n+  const bool valid_value_class = is_identity || is_interface ||\n+                                 (supports_inline_types() && (!is_identity && (is_abstract || is_final)));\n@@ -4290,2 +4488,3 @@\n-      (is_interface && major_gte_1_5 && (is_super || is_enum)) ||\n-      (!is_interface && major_gte_1_5 && is_annotation)) {\n+      (is_interface && major_gte_1_5 && (is_identity || is_enum)) ||   \/\/  ACC_SUPER (now ACC_IDENTITY) was illegal for interfaces\n+      (!is_interface && major_gte_1_5 && is_annotation) ||\n+      (!valid_value_class)) {\n@@ -4293,0 +4492,4 @@\n+    const char* class_note = \"\";\n+    if (!valid_value_class) {\n+      class_note = \" (a value class must be final or else abstract)\";\n+    }\n@@ -4386,2 +4589,2 @@\n-void ClassFileParser::verify_legal_field_modifiers(jint flags,\n-                                                   bool is_interface,\n+void ClassFileParser:: verify_legal_field_modifiers(jint flags,\n+                                                   AccessFlags class_access_flags,\n@@ -4399,0 +4602,1 @@\n+  const bool is_strict    = (flags & JVM_ACC_STRICT)    != 0;\n@@ -4401,1 +4605,2 @@\n-  bool is_illegal = false;\n+  const bool is_interface = class_access_flags.is_interface();\n+  const bool is_identity_class = class_access_flags.is_identity_class();\n@@ -4403,9 +4608,30 @@\n-  if (is_interface) {\n-    if (!is_public || !is_static || !is_final || is_private ||\n-        is_protected || is_volatile || is_transient ||\n-        (major_gte_1_5 && is_enum)) {\n-      is_illegal = true;\n-    }\n-  } else { \/\/ not interface\n-    if (has_illegal_visibility(flags) || (is_final && is_volatile)) {\n-      is_illegal = true;\n+  bool is_illegal = false;\n+  const char* error_msg = \"\";\n+\n+  \/\/ There is some overlap in the checks that apply, for example interface fields\n+  \/\/ must be static, static fields can't be strict, and therefore interfaces can't\n+  \/\/ have strict fields. So we don't have to check every possible invalid combination\n+  \/\/ individually as long as all are covered. Once we have found an illegal combination\n+  \/\/ we can stop checking.\n+\n+  if (!is_illegal) {\n+    if (is_interface) {\n+      if (!is_public || !is_static || !is_final || is_private ||\n+          is_protected || is_volatile || is_transient ||\n+          (major_gte_1_5 && is_enum)) {\n+        is_illegal = true;\n+        error_msg = \"interface fields must be public, static and final, and may be synthetic\";\n+      }\n+    } else { \/\/ not interface\n+      if (has_illegal_visibility(flags)) {\n+        is_illegal = true;\n+        error_msg = \"invalid visibility flags for class field\";\n+      } else if (is_final && is_volatile) {\n+        is_illegal = true;\n+        error_msg = \"fields cannot be final and volatile\";\n+      } else if (supports_inline_types()) {\n+        if (!is_identity_class && !is_static && (!is_strict || !is_final)) {\n+          is_illegal = true;\n+          error_msg = \"value class fields must be either non-static final and strict, or static\";\n+        }\n+      }\n@@ -4421,2 +4647,2 @@\n-      \"Illegal field modifiers in class %s: 0x%X\",\n-      _class_name->as_C_string(), flags);\n+      \"Illegal field modifiers (%s) in class %s: 0x%X\",\n+      error_msg, _class_name->as_C_string(), flags);\n@@ -4428,1 +4654,1 @@\n-                                                    bool is_interface,\n+                                                    AccessFlags class_access_flags,\n@@ -4447,0 +4673,4 @@\n+  \/\/ LW401 CR required: removal of value factories support\n+  const bool is_interface    = class_access_flags.is_interface();\n+  const bool is_identity_class = class_access_flags.is_identity_class();\n+  const bool is_abstract_class = class_access_flags.is_abstract();\n@@ -4450,0 +4680,1 @@\n+  const char* class_note = \"\";\n@@ -4489,4 +4720,9 @@\n-        if (is_abstract) {\n-          if ((is_final || is_native || is_private || is_static ||\n-              (major_gte_1_5 && (is_synchronized || (!major_gte_17 && is_strict))))) {\n-            is_illegal = true;\n+        if (!is_identity_class && is_synchronized && !is_static) {\n+          is_illegal = true;\n+          class_note = \" (not an identity class)\";\n+        } else {\n+          if (is_abstract) {\n+            if ((is_final || is_native || is_private || is_static ||\n+                (major_gte_1_5 && (is_synchronized || (!major_gte_17 && is_strict))))) {\n+              is_illegal = true;\n+            }\n@@ -4505,2 +4741,3 @@\n-      \"Method %s in class %s has illegal modifiers: 0x%X\",\n-      name->as_C_string(), _class_name->as_C_string(), flags);\n+      \"Method %s in class %s%s has illegal modifiers: 0x%X\",\n+      name->as_C_string(), _class_name->as_C_string(),\n+      class_note, flags);\n@@ -4564,0 +4801,9 @@\n+bool ClassFileParser::is_class_in_loadable_descriptors_attribute(Symbol *klass) {\n+  if (_loadable_descriptors == nullptr) return false;\n+  for (int i = 0; i < _loadable_descriptors->length(); i++) {\n+        Symbol* class_name = _cp->symbol_at(_loadable_descriptors->at(i));\n+        if (class_name == klass) return true;\n+  }\n+  return false;\n+}\n+\n@@ -4665,1 +4911,2 @@\n-    case JVM_SIGNATURE_CLASS: {\n+    case JVM_SIGNATURE_CLASS:\n+    {\n@@ -4680,1 +4927,1 @@\n-        \/\/ Skip leading 'L' and ignore first appearance of ';'\n+        \/\/ Skip leading 'L' or 'Q' and ignore first appearance of ';'\n@@ -4736,0 +4983,4 @@\n+    } else if ((_major_version >= CONSTANT_CLASS_DESCRIPTORS || _class_name->starts_with(\"jdk\/internal\/reflect\/\"))\n+                   && bytes[length - 1] == ';' ) {\n+      \/\/ Support for L...; descriptors\n+      legal = verify_unqualified_name(bytes + 1, length - 2, LegalClass);\n@@ -4803,1 +5054,2 @@\n-      if (name == vmSymbols::object_initializer_name() || name == vmSymbols::class_initializer_name()) {\n+      if (name == vmSymbols::object_initializer_name() ||\n+          name == vmSymbols::class_initializer_name()) {\n@@ -4830,0 +5082,10 @@\n+bool ClassFileParser::legal_field_signature(const Symbol* signature, TRAPS) const {\n+  const char* const bytes = (const char*)signature->bytes();\n+  const unsigned int length = signature->utf8_length();\n+  const char* const p = skip_over_field_signature(bytes, false, length, CHECK_false);\n+\n+  if (p == nullptr || (p - bytes) != (int)length) {\n+    return false;\n+  }\n+  return true;\n+}\n@@ -4865,3 +5127,3 @@\n-      name->char_at(0) == JVM_SIGNATURE_SPECIAL &&\n-      sig_length > 0 &&\n-      signature->char_at(sig_length - 1) != JVM_SIGNATURE_VOID) {\n+    name->char_at(0) == JVM_SIGNATURE_SPECIAL &&\n+    sig_length > 0 &&\n+    signature->char_at(sig_length - 1) != JVM_SIGNATURE_VOID) {\n@@ -4918,2 +5180,2 @@\n-  assert(_field_info != nullptr, \"invariant\");\n-  return _field_info->_static_field_size;\n+  assert(_layout_info != nullptr, \"invariant\");\n+  return _layout_info->_static_field_size;\n@@ -4923,2 +5185,2 @@\n-  assert(_field_info != nullptr, \"invariant\");\n-  return _field_info->oop_map_blocks->_nonstatic_oop_map_count;\n+  assert(_layout_info != nullptr, \"invariant\");\n+  return _layout_info->oop_map_blocks->_nonstatic_oop_map_count;\n@@ -4928,2 +5190,2 @@\n-  assert(_field_info != nullptr, \"invariant\");\n-  return _field_info->_instance_size;\n+  assert(_layout_info != nullptr, \"invariant\");\n+  return _layout_info->_instance_size;\n@@ -5044,1 +5306,0 @@\n-\n@@ -5067,3 +5328,3 @@\n-  assert(_field_info != nullptr, \"invariant\");\n-  assert(ik->static_field_size() == _field_info->_static_field_size, \"sanity\");\n-  assert(ik->nonstatic_oop_map_count() == _field_info->oop_map_blocks->_nonstatic_oop_map_count,\n+  assert(_layout_info != nullptr, \"invariant\");\n+  assert(ik->static_field_size() == _layout_info->_static_field_size, \"sanity\");\n+  assert(ik->nonstatic_oop_map_count() == _layout_info->oop_map_blocks->_nonstatic_oop_map_count,\n@@ -5073,1 +5334,1 @@\n-  assert(ik->size_helper() == _field_info->_instance_size, \"sanity\");\n+  assert(ik->size_helper() == _layout_info->_instance_size, \"sanity\");\n@@ -5079,2 +5340,12 @@\n-  ik->set_nonstatic_field_size(_field_info->_nonstatic_field_size);\n-  ik->set_has_nonstatic_fields(_field_info->_has_nonstatic_fields);\n+  ik->set_nonstatic_field_size(_layout_info->_nonstatic_field_size);\n+  ik->set_has_nonstatic_fields(_layout_info->_has_nonstatic_fields);\n+  ik->set_has_strict_static_fields(_has_strict_static_fields);\n+\n+  if (_layout_info->_is_naturally_atomic) {\n+    ik->set_is_naturally_atomic();\n+  }\n+\n+  if (_layout_info->_must_be_atomic) {\n+    ik->set_must_be_atomic();\n+  }\n+\n@@ -5086,0 +5357,3 @@\n+  if (ik->is_inline_klass()) {\n+    InlineKlass::cast(ik)->init_fixed_block();\n+  }\n@@ -5100,0 +5374,1 @@\n+  assert(nullptr == _loadable_descriptors, \"invariant\");\n@@ -5103,0 +5378,1 @@\n+  assert(nullptr == _inline_layout_info_array, \"invariant\");\n@@ -5179,1 +5455,1 @@\n-  OopMapBlocksBuilder* oop_map_blocks = _field_info->oop_map_blocks;\n+  OopMapBlocksBuilder* oop_map_blocks = _layout_info->oop_map_blocks;\n@@ -5240,0 +5516,49 @@\n+  if (is_inline_type()) {\n+    InlineKlass* vk = InlineKlass::cast(ik);\n+    vk->set_payload_alignment(_layout_info->_payload_alignment);\n+    vk->set_payload_offset(_layout_info->_payload_offset);\n+    vk->set_payload_size_in_bytes(_layout_info->_payload_size_in_bytes);\n+    vk->set_non_atomic_size_in_bytes(_layout_info->_non_atomic_size_in_bytes);\n+    vk->set_non_atomic_alignment(_layout_info->_non_atomic_alignment);\n+    vk->set_atomic_size_in_bytes(_layout_info->_atomic_layout_size_in_bytes);\n+    vk->set_nullable_size_in_bytes(_layout_info->_nullable_layout_size_in_bytes);\n+    vk->set_null_marker_offset(_layout_info->_null_marker_offset);\n+    vk->set_null_reset_value_offset(_layout_info->_null_reset_value_offset);\n+    if (_layout_info->_is_empty_inline_klass) vk->set_is_empty_inline_type();\n+    vk->initialize_calling_convention(CHECK);\n+  }\n+\n+  if (EnableValhalla && !access_flags().is_identity_class() && !access_flags().is_interface()\n+      && _class_name != vmSymbols::java_lang_Object() && UseAltSubstitutabilityMethod) {\n+    \/\/ Both abstract and concrete value classes need a field map for acmp\n+    ik->set_acmp_maps_offset(_layout_info->_acmp_maps_offset);\n+    \/\/ Current format of acmp maps:\n+    \/\/ All maps are stored contiguously in a single int array because it might\n+    \/\/ be too early to instantiate an Object array (to be investigated)\n+    \/\/ Format is:\n+    \/\/ [number_of_nonoop_entries][offset0][size[0][offset1][size1]...[oop_offset0][oop_offset1]...\n+    \/\/                           ^               ^\n+    \/\/                           |               |\n+    \/\/                           --------------------- Pair of integer describing a segment of\n+    \/\/                                                 contiguous non-oop fields\n+    \/\/ First element is the number of segment of contiguous non-oop fields\n+    \/\/ Then, each segment of contiguous non-oop fields is described by two consecutive elements:\n+    \/\/ the offset then the size.\n+    \/\/ After the last segment of contiguous non-oop fields, oop fields are described, one element\n+    \/\/ per oop field, containing the offset of the field.\n+    int nonoop_acmp_map_size = _layout_info->_nonoop_acmp_map->length() * 2;\n+    int oop_acmp_map_size = _layout_info->_oop_acmp_map->length();\n+    typeArrayOop map = oopFactory::new_intArray(nonoop_acmp_map_size + oop_acmp_map_size + 1, CHECK);\n+    typeArrayHandle map_h(THREAD, map);\n+    map_h->int_at_put(0, _layout_info->_nonoop_acmp_map->length());\n+    for (int i = 0; i < _layout_info->_nonoop_acmp_map->length(); i++) {\n+      map_h->int_at_put(i * 2 + 1, _layout_info->_nonoop_acmp_map->at(i).first);\n+      map_h->int_at_put(i * 2 + 2, _layout_info->_nonoop_acmp_map->at(i).second);\n+    }\n+    int oop_map_start = nonoop_acmp_map_size + 1;\n+    for (int i = 0; i < _layout_info->_oop_acmp_map->length(); i++) {\n+      map_h->int_at_put(oop_map_start + i, _layout_info->_oop_acmp_map->at(i));\n+    }\n+    ik->java_mirror()->obj_field_put(ik->acmp_maps_offset(), map_h());\n+  }\n+\n@@ -5322,0 +5647,1 @@\n+  _loadable_descriptors(nullptr),\n@@ -5324,0 +5650,1 @@\n+  _local_interface_indexes(nullptr),\n@@ -5333,1 +5660,2 @@\n-  _field_info(nullptr),\n+  _layout_info(nullptr),\n+  _inline_layout_info_array(nullptr),\n@@ -5362,0 +5690,5 @@\n+  _has_strict_static_fields(false),\n+  _has_inline_type_fields(false),\n+  _is_naturally_atomic(false),\n+  _must_be_atomic(true),\n+  _has_loosely_consistent_annotation(false),\n@@ -5399,0 +5732,1 @@\n+  _loadable_descriptors = nullptr;\n@@ -5403,0 +5737,1 @@\n+  _inline_layout_info_array = nullptr;\n@@ -5422,0 +5757,4 @@\n+  if (_inline_layout_info_array != nullptr) {\n+    MetadataFactory::free_array<InlineLayoutInfo>(_loader_data, _inline_layout_info_array);\n+  }\n+\n@@ -5444,0 +5783,4 @@\n+  if (_loadable_descriptors != nullptr && _loadable_descriptors != Universe::the_empty_short_array()) {\n+    MetadataFactory::free_array<u2>(_loader_data, _loadable_descriptors);\n+  }\n+\n@@ -5543,0 +5886,9 @@\n+  \/\/ Fixing ACC_SUPER\/ACC_IDENTITY for old class files\n+  if (!supports_inline_types()) {\n+    const bool is_module = (flags & JVM_ACC_MODULE) != 0;\n+    const bool is_interface = (flags & JVM_ACC_INTERFACE) != 0;\n+    if (!is_module && !is_interface) {\n+      flags |= JVM_ACC_IDENTITY;\n+    }\n+  }\n+\n@@ -5646,2 +5998,0 @@\n-  assert(_local_interfaces != nullptr, \"invariant\");\n-\n@@ -5650,1 +6000,1 @@\n-               _access_flags.is_interface(),\n+               _access_flags,\n@@ -5660,1 +6010,3 @@\n-                _access_flags.is_interface(),\n+                is_interface(),\n+                !is_identity_class(),\n+                is_abstract_class(),\n@@ -5748,1 +6100,1 @@\n-    if (_access_flags.is_interface()) {\n+    if (is_interface()) {\n@@ -5770,0 +6122,14 @@\n+    if (_super_klass->is_interface()) {\n+      classfile_icce_error(\"class %s has interface %s as super class\", _super_klass, THREAD);\n+      return;\n+    }\n+\n+    if (_super_klass->is_final()) {\n+      classfile_icce_error(\"class %s cannot inherit from final class %s\", _super_klass, THREAD);\n+      return;\n+    }\n+\n+    if (EnableValhalla) {\n+      check_identity_and_value_modifiers(this, _super_klass, CHECK);\n+    }\n+\n@@ -5773,0 +6139,1 @@\n+  }\n@@ -5774,3 +6141,24 @@\n-    if (_super_klass->is_interface()) {\n-      classfile_icce_error(\"class %s has interface %s as super class\", _super_klass, THREAD);\n-      return;\n+  if (_parsed_annotations->has_annotation(AnnotationCollector::_jdk_internal_LooselyConsistentValue) && _access_flags.is_identity_class()) {\n+    THROW_MSG(vmSymbols::java_lang_ClassFormatError(),\n+          err_msg(\"class %s cannot have annotation jdk.internal.vm.annotation.LooselyConsistentValue, because it is not a value class\",\n+                  _class_name->as_klass_external_name()));\n+  }\n+\n+  \/\/ Determining is the class allows tearing or not (default is not)\n+  if (EnableValhalla && !_access_flags.is_identity_class()) {\n+    if (_parsed_annotations->has_annotation(ClassAnnotationCollector::_jdk_internal_LooselyConsistentValue)\n+        && (_super_klass == vmClasses::Object_klass() || !_super_klass->must_be_atomic())) {\n+      \/\/ Conditions above are not sufficient to determine atomicity requirements,\n+      \/\/ the presence of fields with atomic requirements could force the current class to have atomicy requirements too\n+      \/\/ Marking as not needing atomicity for now, can be updated when computing the fields layout\n+      \/\/ The InstanceKlass must be filled with the value from the FieldLayoutInfo returned by\n+      \/\/ the FieldLayoutBuilder, not with this _must_be_atomic field.\n+      _must_be_atomic = false;\n+    }\n+    \/\/ Apply VM options override\n+    if (*ForceNonTearable != '\\0') {\n+      \/\/ Allow a command line switch to force the same atomicity property:\n+      const char* class_name_str = _class_name->as_C_string();\n+      if (StringUtils::class_list_match(ForceNonTearable, class_name_str)) {\n+        _must_be_atomic = true;\n+      }\n@@ -5780,0 +6168,46 @@\n+  int itfs_len = _local_interface_indexes == nullptr ? 0 : _local_interface_indexes->length();\n+  _local_interfaces = MetadataFactory::new_array<InstanceKlass*>(_loader_data, itfs_len, nullptr, CHECK);\n+  if (_local_interface_indexes != nullptr) {\n+    for (int i = 0; i < _local_interface_indexes->length(); i++) {\n+      u2 interface_index = _local_interface_indexes->at(i);\n+      Klass* interf;\n+      if (cp->tag_at(interface_index).is_klass()) {\n+        interf = cp->resolved_klass_at(interface_index);\n+      } else {\n+        Symbol* const unresolved_klass  = cp->klass_name_at(interface_index);\n+\n+        \/\/ Don't need to check legal name because it's checked when parsing constant pool.\n+        \/\/ But need to make sure it's not an array type.\n+        guarantee_property(unresolved_klass->char_at(0) != JVM_SIGNATURE_ARRAY,\n+                            \"Bad interface name in class file %s\", CHECK);\n+\n+        \/\/ Call resolve on the interface class name with class circularity checking\n+        interf = SystemDictionary::resolve_super_or_fail(\n+                                                  _class_name,\n+                                                  unresolved_klass,\n+                                                  Handle(THREAD, _loader_data->class_loader()),\n+                                                  false,\n+                                                  CHECK);\n+      }\n+\n+      if (!interf->is_interface()) {\n+        THROW_MSG(vmSymbols::java_lang_IncompatibleClassChangeError(),\n+                  err_msg(\"class %s can not implement %s, because it is not an interface (%s)\",\n+                          _class_name->as_klass_external_name(),\n+                          interf->external_name(),\n+                          interf->class_in_module_of_loader()));\n+      }\n+\n+      if (EnableValhalla) {\n+        \/\/ Check modifiers and set carries_identity_modifier\/carries_value_modifier flags\n+        check_identity_and_value_modifiers(this, InstanceKlass::cast(interf), CHECK);\n+      }\n+\n+      if (InstanceKlass::cast(interf)->has_nonstatic_concrete_methods()) {\n+        _has_nonstatic_concrete_methods = true;\n+      }\n+      _local_interfaces->at_put(i, InstanceKlass::cast(interf));\n+    }\n+  }\n+  assert(_local_interfaces != nullptr, \"invariant\");\n+\n@@ -5807,1 +6241,1 @@\n-  _itable_size = _access_flags.is_interface() ? 0 :\n+  _itable_size = is_interface() ? 0 :\n@@ -5812,3 +6246,92 @@\n-  _field_info = new FieldLayoutInfo();\n-  FieldLayoutBuilder lb(class_name(), super_klass(), _cp, \/*_fields*\/ _temp_field_info,\n-                        _parsed_annotations->is_contended(), _field_info);\n+  if (EnableValhalla) {\n+    _inline_layout_info_array = MetadataFactory::new_array<InlineLayoutInfo>(_loader_data,\n+                                                   java_fields_count(),\n+                                                   CHECK);\n+    for (GrowableArrayIterator<FieldInfo> it = _temp_field_info->begin(); it != _temp_field_info->end(); ++it) {\n+      FieldInfo fieldinfo = *it;\n+      if (fieldinfo.access_flags().is_static()) continue;  \/\/ Only non-static fields are processed at load time\n+      Symbol* sig = fieldinfo.signature(cp);\n+      if (fieldinfo.field_flags().is_null_free_inline_type()) {\n+        \/\/ Pre-load classes of null-free fields that are candidate for flattening\n+        TempNewSymbol s = Signature::strip_envelope(sig);\n+        if (s == _class_name) {\n+          THROW_MSG(vmSymbols::java_lang_ClassCircularityError(),\n+                    err_msg(\"Class %s cannot have a null-free non-static field of its own type\", _class_name->as_C_string()));\n+        }\n+        log_info(class, preload)(\"Preloading of class %s during loading of class %s. \"\n+                                  \"Cause: a null-free non-static field is declared with this type\",\n+                                  s->as_C_string(), _class_name->as_C_string());\n+        InstanceKlass* klass = SystemDictionary::resolve_with_circularity_detection(_class_name, s,\n+                                                                                    Handle(THREAD,\n+                                                                                    _loader_data->class_loader()),\n+                                                                                    false, THREAD);\n+        if (HAS_PENDING_EXCEPTION) {\n+          log_info(class, preload)(\"Preloading of class %s during loading of class %s \"\n+                                      \"(cause: null-free non-static field) failed: %s\",\n+                                      s->as_C_string(), _class_name->as_C_string(),\n+                                      PENDING_EXCEPTION->klass()->name()->as_C_string());\n+          return; \/\/ Exception is still pending\n+        }\n+        assert(klass != nullptr, \"Sanity check\");\n+        InstanceKlass::check_can_be_annotated_with_NullRestricted(klass, _class_name, CHECK);\n+        InlineKlass* vk = InlineKlass::cast(klass);\n+        _inline_layout_info_array->adr_at(fieldinfo.index())->set_klass(vk);\n+        log_info(class, preload)(\"Preloading of class %s during loading of class %s \"\n+                                 \"(cause: null-free non-static field) succeeded\",\n+                                 s->as_C_string(), _class_name->as_C_string());\n+      } else if (Signature::has_envelope(sig) && PreloadClasses) {\n+        \/\/ Preloading classes for nullable fields that are listed in the LoadableDescriptors attribute\n+        \/\/ Those classes would be required later for the flattening of nullable inline type fields\n+        TempNewSymbol name = Signature::strip_envelope(sig);\n+        if (name != _class_name && is_class_in_loadable_descriptors_attribute(sig)) {\n+          log_info(class, preload)(\"Preloading of class %s during loading of class %s. \"\n+                                   \"Cause: field type in LoadableDescriptors attribute\",\n+                                   name->as_C_string(), _class_name->as_C_string());\n+          oop loader = loader_data()->class_loader();\n+          Klass* klass = SystemDictionary::resolve_super_or_fail(_class_name, name,\n+                                                                 Handle(THREAD, loader),\n+                                                                 false, THREAD);\n+          if (klass != nullptr) {\n+            if (klass->is_inline_klass()) {\n+              _inline_layout_info_array->adr_at(fieldinfo.index())->set_klass(InlineKlass::cast(klass));\n+              log_info(class, preload)(\"Preloading of class %s during loading of class %s \"\n+                                       \"(cause: field type in LoadableDescriptors attribute) succeeded\",\n+                                       name->as_C_string(), _class_name->as_C_string());\n+            } else {\n+              \/\/ Non value class are allowed by the current spec, but it could be an indication of an issue so let's log a warning\n+              log_info(class, preload)(\"Preloading of class %s during loading of class %s \"\n+                                          \"(cause: field type in LoadableDescriptors attribute) but loaded class is not a value class\",\n+                                          name->as_C_string(), _class_name->as_C_string());\n+            }\n+          } else {\n+            log_info(class, preload)(\"Preloading of class %s during loading of class %s \"\n+                                        \"(cause: field type in LoadableDescriptors attribute) failed : %s\",\n+                                        name->as_C_string(), _class_name->as_C_string(),\n+                                        PENDING_EXCEPTION->klass()->name()->as_C_string());\n+          }\n+          \/\/ Loads triggered by the LoadableDescriptors attribute are speculative, failures must not impact loading of current class\n+          if (HAS_PENDING_EXCEPTION) {\n+            CLEAR_PENDING_EXCEPTION;\n+          }\n+        } else {\n+          \/\/ Just poking the system dictionary to see if the class has already be loaded. Looking for migrated classes\n+          \/\/ used when --enable-preview when jdk isn't compiled with --enable-preview so doesn't include LoadableDescriptors.\n+          \/\/ This is temporary.\n+          oop loader = loader_data()->class_loader();\n+          InstanceKlass* klass = SystemDictionary::find_instance_klass(THREAD, name, Handle(THREAD, loader));\n+          if (klass != nullptr && klass->is_inline_klass()) {\n+            _inline_layout_info_array->adr_at(fieldinfo.index())->set_klass(InlineKlass::cast(klass));\n+            log_info(class, preload)(\"Preloading of class %s during loading of class %s \"\n+                                     \"(cause: field type not in LoadableDescriptors attribute) succeeded\",\n+                                     name->as_C_string(), _class_name->as_C_string());\n+          }\n+        }\n+      }\n+    }\n+  }\n+\n+  _layout_info = new FieldLayoutInfo();\n+  FieldLayoutBuilder lb(class_name(), loader_data(), super_klass(), _cp, \/*_fields*\/ _temp_field_info,\n+      _parsed_annotations->is_contended(), is_inline_type(),\n+      access_flags().is_abstract() && !access_flags().is_identity_class() && !access_flags().is_interface(),\n+      _must_be_atomic, _layout_info, _inline_layout_info_array);\n@@ -5816,0 +6339,1 @@\n+  _has_inline_type_fields = _layout_info->_has_inline_fields;\n@@ -5825,0 +6349,21 @@\n+\n+  \/\/ Strict static fields track initialization status from the beginning of time.\n+  \/\/ After this class runs <clinit>, they will be verified as being \"not unset\".\n+  \/\/ See Step 8 of InstanceKlass::initialize_impl.\n+  if (_has_strict_static_fields) {\n+    bool found_one = false;\n+    for (int i = 0; i < _temp_field_info->length(); i++) {\n+      FieldInfo& fi = *_temp_field_info->adr_at(i);\n+      if (fi.access_flags().is_strict() && fi.access_flags().is_static()) {\n+        found_one = true;\n+        if (fi.initializer_index() != 0) {\n+          \/\/ skip strict static fields with ConstantValue attributes\n+        } else {\n+          _fields_status->adr_at(fi.index())->update_strict_static_unset(true);\n+          _fields_status->adr_at(fi.index())->update_strict_static_unread(true);\n+        }\n+      }\n+    }\n+    assert(found_one == _has_strict_static_fields,\n+           \"correct prediction = %d\", (int)_has_strict_static_fields);\n+  }\n","filename":"src\/hotspot\/share\/classfile\/classFileParser.cpp","additions":662,"deletions":117,"binary":false,"changes":779,"status":"modified"},{"patch":"@@ -99,1 +99,1 @@\n-\/\/ JimageFile pointer, or null if exploded JDK build.\n+\/\/ JImageFile pointer, or null if exploded JDK build.\n@@ -102,0 +102,9 @@\n+\/\/ JImageMode status to control preview behaviour. JImage_file is unusable\n+\/\/ for normal lookup until (JImage_mode != JIMAGE_MODE_UNINITIALIZED).\n+enum JImageMode {\n+  JIMAGE_MODE_UNINITIALIZED = 0,\n+  JIMAGE_MODE_DEFAULT = 1,\n+  JIMAGE_MODE_ENABLE_PREVIEW = 2\n+};\n+static JImageMode                      JImage_mode            = JIMAGE_MODE_UNINITIALIZED;\n+\n@@ -156,1 +165,1 @@\n-ClassPathEntry* ClassLoader::_jrt_entry = nullptr;\n+ClassPathImageEntry* ClassLoader::_jrt_entry = nullptr;\n@@ -173,9 +182,0 @@\n-static const char* get_jimage_version_string() {\n-  static char version_string[10] = \"\";\n-  if (version_string[0] == '\\0') {\n-    jio_snprintf(version_string, sizeof(version_string), \"%d.%d\",\n-                 VM_Version::vm_major_version(), VM_Version::vm_minor_version());\n-  }\n-  return (const char*)version_string;\n-}\n-\n@@ -236,0 +236,67 @@\n+\/\/ --------------------------------\n+\/\/ The following jimage_xxx static functions encapsulate all JImage_file and JImage_mode access.\n+\/\/ This is done to make it easy to reason about the JImage file state (exists vs initialized etc.).\n+\n+\/\/ Opens the named JImage file and sets the JImage file reference.\n+\/\/ Returns true if opening the JImage file was successful (see also jimage_exists()).\n+static bool jimage_open(const char* modules_path) {\n+  \/\/ Currently 'error' is not set to anything useful, so ignore it here.\n+  jint error;\n+  JImage_file = (*JImageOpen)(modules_path, &error);\n+  return JImage_file != nullptr;\n+}\n+\n+\/\/ Closes and clears the JImage file reference (this will only be called during shutdown).\n+static void jimage_close() {\n+  if (JImage_file != nullptr) {\n+    (*JImageClose)(JImage_file);\n+    JImage_file = nullptr;\n+  }\n+}\n+\n+\/\/ Returns whether a JImage file was opened (but NOT whether it was initialized yet).\n+static bool jimage_exists() {\n+  return JImage_file != nullptr;\n+}\n+\n+\/\/ Returns the JImage file reference (which may or may not be initialized).\n+static JImageFile* jimage_non_null() {\n+  assert(jimage_exists(), \"should have been opened by ClassLoader::lookup_vm_options \"\n+                          \"and remained throughout normal JVM lifetime\");\n+  return JImage_file;\n+}\n+\n+\/\/ Called once to set the access mode for resource (i.e. preview or non-preview) before\n+\/\/ general resource lookup can occur.\n+static void jimage_init(bool enable_preview) {\n+  assert(JImage_mode == JIMAGE_MODE_UNINITIALIZED, \"jimage_init must not be called twice\");\n+  JImage_mode = enable_preview ? JIMAGE_MODE_ENABLE_PREVIEW : JIMAGE_MODE_DEFAULT;\n+}\n+\n+\/\/ Returns true if jimage_init() has been called. Once the JImage file is initialized,\n+\/\/ jimage_is_preview_enabled() can be called to correctly determine the access mode.\n+static bool jimage_is_initialized() {\n+  return jimage_exists() && JImage_mode != JIMAGE_MODE_UNINITIALIZED;\n+}\n+\n+\/\/ Returns the access mode for an initialized JImage file (reflects --enable-preview).\n+static bool jimage_is_preview_enabled() {\n+  assert(jimage_is_initialized(), \"jimage is not initialized\");\n+  return JImage_mode == JIMAGE_MODE_ENABLE_PREVIEW;\n+}\n+\n+\/\/ Looks up the location of a named JImage resource. This \"raw\" lookup function allows\n+\/\/ the preview mode to be manually specified, so must not be accessible outside this\n+\/\/ class. ClassPathImageEntry manages all calls for resources after startup is complete.\n+static JImageLocationRef jimage_find_resource(const char* module_name,\n+                                              const char* file_name,\n+                                              bool is_preview,\n+                                              jlong *size) {\n+  return ((*JImageFindResource)(jimage_non_null(),\n+                                module_name,\n+                                file_name,\n+                                is_preview,\n+                                size));\n+}\n+\/\/ --------------------------------\n+\n@@ -374,15 +441,1 @@\n-JImageFile* ClassPathImageEntry::jimage() const {\n-  return JImage_file;\n-}\n-\n-JImageFile* ClassPathImageEntry::jimage_non_null() const {\n-  assert(ClassLoader::has_jrt_entry(), \"must be\");\n-  assert(jimage() != nullptr, \"should have been opened by ClassLoader::lookup_vm_options \"\n-                           \"and remained throughout normal JVM lifetime\");\n-  return jimage();\n-}\n-\n-  if (jimage() != nullptr) {\n-    (*JImageClose)(jimage());\n-    JImage_file = nullptr;\n-  }\n+  jimage_close();\n@@ -392,1 +445,1 @@\n-ClassPathImageEntry::ClassPathImageEntry(JImageFile* jimage, const char* name) :\n+ClassPathImageEntry::ClassPathImageEntry(const char* name) :\n@@ -394,1 +447,1 @@\n-  guarantee(jimage != nullptr, \"jimage file is null\");\n+  guarantee(jimage_is_initialized(), \"jimage is not initialized\");\n@@ -396,0 +449,1 @@\n+\n@@ -414,0 +468,2 @@\n+  bool is_preview = jimage_is_preview_enabled();\n+\n@@ -422,1 +478,1 @@\n-      location = (*JImageFindResource)(jimage_non_null(), JAVA_BASE_NAME, get_jimage_version_string(), name, &size);\n+      location = jimage_find_resource(JAVA_BASE_NAME, name, is_preview, &size);\n@@ -433,1 +489,1 @@\n-          location = (*JImageFindResource)(jimage_non_null(), module_name, get_jimage_version_string(), name, &size);\n+          location = jimage_find_resource(module_name, name, is_preview, &size);\n@@ -446,1 +502,1 @@\n-    assert(this == (ClassPathImageEntry*)ClassLoader::get_jrt_entry(), \"must be\");\n+    assert(this == ClassLoader::get_jrt_entry(), \"must be\");\n@@ -456,7 +512,0 @@\n-JImageLocationRef ClassLoader::jimage_find_resource(JImageFile* jf,\n-                                                    const char* module_name,\n-                                                    const char* file_name,\n-                                                    jlong &size) {\n-  return ((*JImageFindResource)(jf, module_name, get_jimage_version_string(), file_name, &size));\n-}\n-\n@@ -465,1 +514,1 @@\n-  assert(this == (ClassPathImageEntry*)ClassLoader::get_jrt_entry(), \"must be used for jrt entry\");\n+  assert(this == ClassLoader::get_jrt_entry(), \"must be used for jrt entry\");\n@@ -620,1 +669,1 @@\n-        if (JImage_file != nullptr) {\n+        if (jimage_exists()) {\n@@ -625,1 +674,3 @@\n-          _jrt_entry = new ClassPathImageEntry(JImage_file, canonical_path);\n+          \/\/ Hand over lifecycle control of the JImage file to the _jrt_entry singleton\n+          \/\/ (see ClassPathImageEntry::close_jimage). The image must be initialized by now.\n+          _jrt_entry = new ClassPathImageEntry(canonical_path);\n@@ -627,1 +678,0 @@\n-          assert(_jrt_entry->jimage() != nullptr, \"No java runtime image\");\n@@ -1046,0 +1096,1 @@\n+  bool is_patched = false;\n@@ -1065,2 +1116,16 @@\n-    assert(!CDSConfig::is_dumping_archive(), \"CDS doesn't support --patch-module during dumping\");\n-    stream = search_module_entries(THREAD, _patch_mod_entries, pkg_entry, file_name);\n+    \/\/ At CDS dump time, the --patch-module entries are ignored. That means a\n+    \/\/ class is still loaded from the runtime image even if it might\n+    \/\/ appear in the _patch_mod_entries. The runtime shared class visibility\n+    \/\/ check will determine if a shared class is visible based on the runtime\n+    \/\/ environment, including the runtime --patch-module setting.\n+    if (!CDSConfig::is_valhalla_preview()) {\n+      \/\/ Dynamic dumping requires UseSharedSpaces to be enabled. Since --patch-module\n+      \/\/ is not supported with UseSharedSpaces, we can never come here during dynamic dumping.\n+      assert(!CDSConfig::is_dumping_archive(), \"CDS doesn't support --patch-module during dumping\");\n+    }\n+    if (CDSConfig::is_valhalla_preview() || !CDSConfig::is_dumping_static_archive()) {\n+      stream = search_module_entries(THREAD, _patch_mod_entries, pkg_entry, file_name);\n+      if (stream != nullptr) {\n+        is_patched = true;\n+      }\n+    }\n@@ -1115,0 +1180,3 @@\n+  if (is_patched) {\n+    result->set_shared_classpath_index(0);\n+  }\n@@ -1191,0 +1259,4 @@\n+  if (ik->shared_classpath_index() == 0 && ik->defined_by_boot_loader()) {\n+    return;\n+  }\n+\n@@ -1398,11 +1470,0 @@\n-static char* lookup_vm_resource(JImageFile *jimage, const char *jimage_version, const char *path) {\n-  jlong size;\n-  JImageLocationRef location = (*JImageFindResource)(jimage, \"java.base\", jimage_version, path, &size);\n-  if (location == 0)\n-    return nullptr;\n-  char *val = NEW_C_HEAP_ARRAY(char, size+1, mtClass);\n-  (*JImageGetResource)(jimage, location, val, size);\n-  val[size] = '\\0';\n-  return val;\n-}\n-\n@@ -1411,1 +1472,0 @@\n-  jint error;\n@@ -1419,3 +1479,13 @@\n-  JImage_file =(*JImageOpen)(modules_path, &error);\n-  if (JImage_file == nullptr) {\n-    return nullptr;\n+  if (jimage_open(modules_path)) {\n+    \/\/ Special case where we lookup the options string *before* calling jimage_init().\n+    \/\/ Since VM arguments have not been parsed, and the ClassPathImageEntry singleton\n+    \/\/ has not been created yet, we access the JImage file directly in non-preview mode.\n+    jlong size;\n+    JImageLocationRef location =\n+            jimage_find_resource(JAVA_BASE_NAME, \"jdk\/internal\/vm\/options\", \/* is_preview *\/ false, &size);\n+    if (location != 0) {\n+      char *options = NEW_C_HEAP_ARRAY(char, size+1, mtClass);\n+      (*JImageGetResource)(jimage_non_null(), location, options, size);\n+      options[size] = '\\0';\n+      return options;\n+    }\n@@ -1423,0 +1493,2 @@\n+  return nullptr;\n+}\n@@ -1424,3 +1496,5 @@\n-  const char *jimage_version = get_jimage_version_string();\n-  char *options = lookup_vm_resource(JImage_file, jimage_version, \"jdk\/internal\/vm\/options\");\n-  return options;\n+\/\/ Finishes initializing the JImageFile (if present) by setting the access mode.\n+void ClassLoader::init_jimage(bool enable_preview) {\n+  if (jimage_exists()) {\n+    jimage_init(enable_preview);\n+  }\n@@ -1431,1 +1505,1 @@\n-  if (JImage_file == nullptr) {\n+  if (!jimage_exists()) {\n@@ -1438,0 +1512,1 @@\n+  \/\/ We don't expect preview mode (i.e. --enable-preview) to affect module visibility.\n@@ -1439,2 +1514,1 @@\n-  const char *jimage_version = get_jimage_version_string();\n-  return (*JImageFindResource)(JImage_file, module_name, jimage_version, \"module-info.class\", &size) != 0;\n+  return jimage_find_resource(module_name, \"module-info.class\", \/* is_preview *\/ false, &size) != 0;\n","filename":"src\/hotspot\/share\/classfile\/classLoader.cpp","additions":139,"deletions":65,"binary":false,"changes":204,"status":"modified"},{"patch":"@@ -716,0 +716,11 @@\n+\n+      \/\/ If inline types are passed as fields, use the extended signature\n+      \/\/ which contains the types of all (oop) fields of the inline type.\n+      if (is_compiled_by_c2() && callee->has_scalarized_args()) {\n+        const GrowableArray<SigEntry>* sig = callee->adapter()->get_sig_cc();\n+        assert(sig != nullptr, \"sig should never be null\");\n+        TempNewSymbol tmp_sig = SigEntry::create_symbol(sig);\n+        has_receiver = false; \/\/ The extended signature contains the receiver type\n+        fr.oops_compiled_arguments_do(tmp_sig, has_receiver, has_appendix, reg_map, f);\n+        return;\n+      }\n@@ -1246,0 +1257,4 @@\n+  _inline_entry_point             = entry_point();\n+  _verified_inline_entry_point    = verified_entry_point();\n+  _verified_inline_ro_entry_point = verified_entry_point();\n+\n@@ -1285,1 +1300,1 @@\n-\n+    assert(!method->has_scalarized_args(), \"scalarized native wrappers not supported yet\");\n@@ -1743,0 +1758,4 @@\n+    _inline_entry_point             = code_begin() + offsets->value(CodeOffsets::Inline_Entry);\n+    _verified_inline_entry_point    = code_begin() + offsets->value(CodeOffsets::Verified_Inline_Entry);\n+    _verified_inline_ro_entry_point = code_begin() + offsets->value(CodeOffsets::Verified_Inline_Entry_RO);\n+\n@@ -3188,4 +3207,4 @@\n-    if (deps.type() != Dependencies::evol_method)\n-      continue;\n-    Method* method = deps.method_argument(0);\n-    if (method == dependee) return true;\n+    if (Dependencies::has_method_dep(deps.type())) {\n+      Method* method = deps.method_argument(0);\n+      if (method == dependee) return true;\n+    }\n@@ -4029,0 +4048,1 @@\n+  if (pos == inline_entry_point())                                      label = \"[Inline Entry Point]\";\n@@ -4030,0 +4050,2 @@\n+  if (pos == verified_inline_entry_point())                             label = \"[Verified Inline Entry Point]\";\n+  if (pos == verified_inline_ro_entry_point())                          label = \"[Verified Inline Entry Point (RO)]\";\n@@ -4038,0 +4060,10 @@\n+static int maybe_print_entry_label(outputStream* stream, address pos, address entry, const char* label) {\n+  if (pos == entry) {\n+    stream->bol();\n+    stream->print_cr(\"%s\", label);\n+    return 1;\n+  } else {\n+    return 0;\n+  }\n+}\n+\n@@ -4040,4 +4072,13 @@\n-    const char* label = nmethod_section_label(block_begin);\n-    if (label != nullptr) {\n-      stream->bol();\n-      stream->print_cr(\"%s\", label);\n+    int n = 0;\n+    \/\/ Multiple entry points may be at the same position. Print them all.\n+    n += maybe_print_entry_label(stream, block_begin, entry_point(),                    \"[Entry Point]\");\n+    n += maybe_print_entry_label(stream, block_begin, inline_entry_point(),             \"[Inline Entry Point]\");\n+    n += maybe_print_entry_label(stream, block_begin, verified_entry_point(),           \"[Verified Entry Point]\");\n+    n += maybe_print_entry_label(stream, block_begin, verified_inline_entry_point(),    \"[Verified Inline Entry Point]\");\n+    n += maybe_print_entry_label(stream, block_begin, verified_inline_ro_entry_point(), \"[Verified Inline Entry Point (RO)]\");\n+    if (n == 0) {\n+      const char* label = nmethod_section_label(block_begin);\n+      if (label != nullptr) {\n+        stream->bol();\n+        stream->print_cr(\"%s\", label);\n+      }\n@@ -4047,6 +4088,62 @@\n-  if (block_begin == entry_point()) {\n-    Method* m = method();\n-    if (m != nullptr) {\n-      stream->print(\"  # \");\n-      m->print_value_on(stream);\n-      stream->cr();\n+  Method* m = method();\n+  if (m == nullptr || is_osr_method()) {\n+    return;\n+  }\n+\n+  \/\/ Print the name of the method (only once)\n+  address low = MIN3(entry_point(),\n+                     verified_entry_point(),\n+                     inline_entry_point());\n+  \/\/ The verified inline entry point and verified inline RO entry point are not always\n+  \/\/ used. When they are unused. CodeOffsets::Verified_Inline_Entry(_RO) is -1. Hence,\n+  \/\/ the calculated entry point is smaller than the block they are offsetting into.\n+  if (verified_inline_entry_point() >= block_begin) {\n+    low = MIN2(low, verified_inline_entry_point());\n+  }\n+  if (verified_inline_ro_entry_point() >= block_begin) {\n+    low = MIN2(low, verified_inline_ro_entry_point());\n+  }\n+  assert(low != 0, \"sanity\");\n+  if (block_begin == low) {\n+    stream->print(\"  # \");\n+    m->print_value_on(stream);\n+    stream->cr();\n+  }\n+\n+  \/\/ Print the arguments for the 3 types of verified entry points\n+  CompiledEntrySignature ces(m);\n+  ces.compute_calling_conventions(false);\n+  const GrowableArray<SigEntry>* sig_cc;\n+  const VMRegPair* regs;\n+  if (block_begin == verified_entry_point()) {\n+    sig_cc = ces.sig_cc();\n+    regs = ces.regs_cc();\n+  } else if (block_begin == verified_inline_entry_point()) {\n+    sig_cc = ces.sig();\n+    regs = ces.regs();\n+  } else if (block_begin == verified_inline_ro_entry_point()) {\n+    sig_cc = ces.sig_cc_ro();\n+    regs = ces.regs_cc_ro();\n+  } else {\n+    return;\n+  }\n+\n+  bool has_this = !m->is_static();\n+  if (ces.has_inline_recv() && block_begin == verified_entry_point()) {\n+    \/\/ <this> argument is scalarized for verified_entry_point()\n+    has_this = false;\n+  }\n+  const char* spname = \"sp\"; \/\/ make arch-specific?\n+  int stack_slot_offset = this->frame_size() * wordSize;\n+  int tab1 = 14, tab2 = 24;\n+  int sig_index = 0;\n+  int arg_index = has_this ? -1 : 0;\n+  bool did_old_sp = false;\n+  for (ExtendedSignature sig = ExtendedSignature(sig_cc, SigEntryFilter()); !sig.at_end(); ++sig) {\n+    bool at_this = (arg_index == -1);\n+    bool at_old_sp = false;\n+    BasicType t = (*sig)._bt;\n+    if (at_this) {\n+      stream->print(\"  # this: \");\n+    } else {\n+      stream->print(\"  # parm%d: \", arg_index);\n@@ -4054,19 +4151,7 @@\n-    if (m != nullptr && !is_osr_method()) {\n-      ResourceMark rm;\n-      int sizeargs = m->size_of_parameters();\n-      BasicType* sig_bt = NEW_RESOURCE_ARRAY(BasicType, sizeargs);\n-      VMRegPair* regs   = NEW_RESOURCE_ARRAY(VMRegPair, sizeargs);\n-      {\n-        int sig_index = 0;\n-        if (!m->is_static())\n-          sig_bt[sig_index++] = T_OBJECT; \/\/ 'this'\n-        for (SignatureStream ss(m->signature()); !ss.at_return_type(); ss.next()) {\n-          BasicType t = ss.type();\n-          sig_bt[sig_index++] = t;\n-          if (type2size[t] == 2) {\n-            sig_bt[sig_index++] = T_VOID;\n-          } else {\n-            assert(type2size[t] == 1, \"size is 1 or 2\");\n-          }\n-        }\n-        assert(sig_index == sizeargs, \"\");\n+    stream->move_to(tab1);\n+    VMReg fst = regs[sig_index].first();\n+    VMReg snd = regs[sig_index].second();\n+    if (fst->is_reg()) {\n+      stream->print(\"%s\", fst->name());\n+      if (snd->is_valid())  {\n+        stream->print(\":%s\", snd->name());\n@@ -4074,54 +4159,18 @@\n-      const char* spname = \"sp\"; \/\/ make arch-specific?\n-      SharedRuntime::java_calling_convention(sig_bt, regs, sizeargs);\n-      int stack_slot_offset = this->frame_size() * wordSize;\n-      int tab1 = 14, tab2 = 24;\n-      int sig_index = 0;\n-      int arg_index = (m->is_static() ? 0 : -1);\n-      bool did_old_sp = false;\n-      for (SignatureStream ss(m->signature()); !ss.at_return_type(); ) {\n-        bool at_this = (arg_index == -1);\n-        bool at_old_sp = false;\n-        BasicType t = (at_this ? T_OBJECT : ss.type());\n-        assert(t == sig_bt[sig_index], \"sigs in sync\");\n-        if (at_this)\n-          stream->print(\"  # this: \");\n-        else\n-          stream->print(\"  # parm%d: \", arg_index);\n-        stream->move_to(tab1);\n-        VMReg fst = regs[sig_index].first();\n-        VMReg snd = regs[sig_index].second();\n-        if (fst->is_reg()) {\n-          stream->print(\"%s\", fst->name());\n-          if (snd->is_valid())  {\n-            stream->print(\":%s\", snd->name());\n-          }\n-        } else if (fst->is_stack()) {\n-          int offset = fst->reg2stack() * VMRegImpl::stack_slot_size + stack_slot_offset;\n-          if (offset == stack_slot_offset)  at_old_sp = true;\n-          stream->print(\"[%s+0x%x]\", spname, offset);\n-        } else {\n-          stream->print(\"reg%d:%d??\", (int)(intptr_t)fst, (int)(intptr_t)snd);\n-        }\n-        stream->print(\" \");\n-        stream->move_to(tab2);\n-        stream->print(\"= \");\n-        if (at_this) {\n-          m->method_holder()->print_value_on(stream);\n-        } else {\n-          bool did_name = false;\n-          if (!at_this && ss.is_reference()) {\n-            Symbol* name = ss.as_symbol();\n-            name->print_value_on(stream);\n-            did_name = true;\n-          }\n-          if (!did_name)\n-            stream->print(\"%s\", type2name(t));\n-        }\n-        if (at_old_sp) {\n-          stream->print(\"  (%s of caller)\", spname);\n-          did_old_sp = true;\n-        }\n-        stream->cr();\n-        sig_index += type2size[t];\n-        arg_index += 1;\n-        if (!at_this)  ss.next();\n+    } else if (fst->is_stack()) {\n+      int offset = fst->reg2stack() * VMRegImpl::stack_slot_size + stack_slot_offset;\n+      if (offset == stack_slot_offset)  at_old_sp = true;\n+      stream->print(\"[%s+0x%x]\", spname, offset);\n+    } else {\n+      stream->print(\"reg%d:%d??\", (int)(intptr_t)fst, (int)(intptr_t)snd);\n+    }\n+    stream->print(\" \");\n+    stream->move_to(tab2);\n+    stream->print(\"= \");\n+    if (at_this) {\n+      m->method_holder()->print_value_on(stream);\n+    } else {\n+      bool did_name = false;\n+      if (is_reference_type(t)) {\n+        Symbol* name = (*sig)._name;\n+        name->print_value_on(stream);\n+        did_name = true;\n@@ -4129,6 +4178,4 @@\n-      if (!did_old_sp) {\n-        stream->print(\"  # \");\n-        stream->move_to(tab1);\n-        stream->print(\"[%s+0x%x]\", spname, stack_slot_offset);\n-        stream->print(\"  (%s of caller)\", spname);\n-        stream->cr();\n+      if (!did_name)\n+        stream->print(\"%s\", type2name(t));\n+      if ((*sig)._null_marker) {\n+        stream->print(\" (null marker)\");\n@@ -4137,0 +4184,14 @@\n+    if (at_old_sp) {\n+      stream->print(\"  (%s of caller)\", spname);\n+      did_old_sp = true;\n+    }\n+    stream->cr();\n+    sig_index += type2size[t];\n+    arg_index += 1;\n+  }\n+  if (!did_old_sp) {\n+    stream->print(\"  # \");\n+    stream->move_to(tab1);\n+    stream->print(\"[%s+0x%x]\", spname, stack_slot_offset);\n+    stream->print(\"  (%s of caller)\", spname);\n+    stream->cr();\n@@ -4260,1 +4321,1 @@\n-      st->print(\" {reexecute=%d rethrow=%d return_oop=%d}\", sd->should_reexecute(), sd->rethrow_exception(), sd->return_oop());\n+      st->print(\" {reexecute=%d rethrow=%d return_oop=%d return_scalarized=%d}\", sd->should_reexecute(), sd->rethrow_exception(), sd->return_oop(), sd->return_scalarized());\n","filename":"src\/hotspot\/share\/code\/nmethod.cpp","additions":156,"deletions":95,"binary":false,"changes":251,"status":"modified"},{"patch":"@@ -30,0 +30,1 @@\n+#include \"compiler\/compilerDefinitions.hpp\"\n@@ -219,0 +220,4 @@\n+  \/\/ TODO: can these be uint16_t, seem rely on -1 CodeOffset, can change later...\n+  address _inline_entry_point;              \/\/ inline type entry point (unpack all inline type args) with class check\n+  address _verified_inline_entry_point;     \/\/ inline type entry point (unpack all inline type args) without class check\n+  address _verified_inline_ro_entry_point;  \/\/ inline type entry point (unpack receiver only) without class check\n@@ -697,0 +702,3 @@\n+  address inline_entry_point() const              { return _inline_entry_point; }             \/\/ inline type entry point (unpack all inline type args)\n+  address verified_inline_entry_point() const     { return _verified_inline_entry_point; }    \/\/ inline type entry point (unpack all inline type args) without class check\n+  address verified_inline_ro_entry_point() const  { return _verified_inline_ro_entry_point; } \/\/ inline type entry point (only unpack receiver) without class check\n@@ -763,0 +771,10 @@\n+  bool  needs_stack_repair() const {\n+    if (is_compiled_by_c1()) {\n+      return method()->c1_needs_stack_repair();\n+    } else if (is_compiled_by_c2()) {\n+      return method()->c2_needs_stack_repair();\n+    } else {\n+      return false;\n+    }\n+  }\n+\n@@ -1075,3 +1093,4 @@\n-  \/\/ Fast breakpoint support. Tells if this compiled method is\n-  \/\/ dependent on the given method. Returns true if this nmethod\n-  \/\/ corresponds to the given method as well.\n+  \/\/ Tells if this compiled method is dependent on the given method.\n+  \/\/ Returns true if this nmethod corresponds to the given method as well.\n+  \/\/ It is used for fast breakpoint support and updating the calling convention\n+  \/\/ in case of mismatch.\n","filename":"src\/hotspot\/share\/code\/nmethod.hpp","additions":22,"deletions":3,"binary":false,"changes":25,"status":"modified"},{"patch":"@@ -291,0 +291,1 @@\n+  oop obj_buffer_allocate(Klass* klass, size_t size, TRAPS); \/\/ doesn't clear memory\n","filename":"src\/hotspot\/share\/gc\/shared\/collectedHeap.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -176,1 +176,1 @@\n-    static bool oop_arraycopy_in_heap(arrayOop src_obj, size_t src_offset_in_bytes, T* src_raw,\n+    static void oop_arraycopy_in_heap(arrayOop src_obj, size_t src_offset_in_bytes, T* src_raw,\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahBarrierSet.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -371,1 +371,1 @@\n-bool ShenandoahBarrierSet::AccessBarrier<decorators, BarrierSetT>::oop_arraycopy_in_heap(arrayOop src_obj, size_t src_offset_in_bytes, T* src_raw,\n+void ShenandoahBarrierSet::AccessBarrier<decorators, BarrierSetT>::oop_arraycopy_in_heap(arrayOop src_obj, size_t src_offset_in_bytes, T* src_raw,\n@@ -378,2 +378,4 @@\n-  bs->arraycopy_barrier(src, dst, length);\n-  bool result = Raw::oop_arraycopy_in_heap(src_obj, src_offset_in_bytes, src_raw, dst_obj, dst_offset_in_bytes, dst_raw, length);\n+  bs->arraycopy_barrier(arrayOopDesc::obj_offset_to_raw(src_obj, src_offset_in_bytes, src_raw),\n+                        arrayOopDesc::obj_offset_to_raw(dst_obj, dst_offset_in_bytes, dst_raw),\n+                        length);\n+  Raw::oop_arraycopy_in_heap(src_obj, src_offset_in_bytes, src_raw, dst_obj, dst_offset_in_bytes, dst_raw, length);\n@@ -383,1 +385,0 @@\n-  return result;\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahBarrierSet.inline.hpp","additions":5,"deletions":4,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -215,0 +215,2 @@\n+          \/\/ For frames that need stack repair we skip this trick. This is because the stack walking code reads\n+          \/\/ the frame size from the stack, but the memory has already been overwritten by the SafepointBlob.\n@@ -218,1 +220,1 @@\n-          if (is_valid(pc_desc)) {\n+          if (is_valid(pc_desc) && !sampled_nm->needs_stack_repair()) {\n","filename":"src\/hotspot\/share\/jfr\/periodic\/sampling\/jfrThreadSampling.cpp","additions":3,"deletions":1,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -69,0 +69,1 @@\n+#include \"oops\/refArrayKlass.hpp\"\n@@ -116,0 +117,4 @@\n+static LatestMethodCache _is_substitutable_cache;           \/\/ ValueObjectMethods.isSubstitutable()\n+static LatestMethodCache _value_object_hash_code_cache;     \/\/ ValueObjectMethods.valueObjectHashCode()\n+static LatestMethodCache _is_substitutable_alt_cache;       \/\/ ValueObjectMethods.isSubstitutableAlt()\n+static LatestMethodCache _value_object_hash_code_alt_cache; \/\/ ValueObjectMethods.valueObjectHashCodeAlt()\n@@ -512,2 +517,6 @@\n-    Klass* oak = vmClasses::Object_klass()->array_klass(CHECK);\n-    _objectArrayKlass = ObjArrayKlass::cast(oak);\n+    ArrayKlass* oak = vmClasses::Object_klass()->array_klass(CHECK);\n+    oak->append_to_sibling_list();\n+\n+    \/\/ Create a RefArrayKlass (which is the default) and initialize.\n+    ObjArrayKlass* rak = ObjArrayKlass::cast(oak)->klass_with_properties(ArrayKlass::ArrayProperties::DEFAULT, THREAD);\n+    _objectArrayKlass = rak;\n@@ -515,7 +524,0 @@\n-  \/\/ OLD\n-  \/\/ Add the class to the class hierarchy manually to make sure that\n-  \/\/ its vtable is initialized after core bootstrapping is completed.\n-  \/\/ ---\n-  \/\/ New\n-  \/\/ Have already been initialized.\n-  _objectArrayKlass->append_to_sibling_list();\n@@ -656,0 +658,3 @@\n+\n+  \/\/ This isn't added to the subclass list, so need to reinitialize vtables directly.\n+  Universe::objectArrayKlass()->vtable().initialize_vtable();\n@@ -1080,5 +1085,9 @@\n-Method* Universe::finalizer_register_method()     { return _finalizer_register_cache.get_method(); }\n-Method* Universe::loader_addClass_method()        { return _loader_addClass_cache.get_method(); }\n-Method* Universe::throw_illegal_access_error()    { return _throw_illegal_access_error_cache.get_method(); }\n-Method* Universe::throw_no_such_method_error()    { return _throw_no_such_method_error_cache.get_method(); }\n-Method* Universe::do_stack_walk_method()          { return _do_stack_walk_cache.get_method(); }\n+Method* Universe::finalizer_register_method()        { return _finalizer_register_cache.get_method(); }\n+Method* Universe::loader_addClass_method()           { return _loader_addClass_cache.get_method(); }\n+Method* Universe::throw_illegal_access_error()       { return _throw_illegal_access_error_cache.get_method(); }\n+Method* Universe::throw_no_such_method_error()       { return _throw_no_such_method_error_cache.get_method(); }\n+Method* Universe::do_stack_walk_method()             { return _do_stack_walk_cache.get_method(); }\n+Method* Universe::is_substitutable_method()          { return _is_substitutable_cache.get_method(); }\n+Method* Universe::value_object_hash_code_method()    { return _value_object_hash_code_cache.get_method(); }\n+Method* Universe::is_substitutableAlt_method()       { return _is_substitutable_alt_cache.get_method(); }\n+Method* Universe::value_object_hash_codeAlt_method() { return _value_object_hash_code_alt_cache.get_method(); }\n@@ -1114,0 +1123,19 @@\n+\n+  \/\/ Set up substitutability testing\n+  ResourceMark rm(current);\n+  _is_substitutable_cache.init(current,\n+                          vmClasses::ValueObjectMethods_klass(),\n+                          vmSymbols::isSubstitutable_name()->as_C_string(),\n+                          vmSymbols::object_object_boolean_signature(), true);\n+  _value_object_hash_code_cache.init(current,\n+                          vmClasses::ValueObjectMethods_klass(),\n+                          vmSymbols::valueObjectHashCode_name()->as_C_string(),\n+                          vmSymbols::object_int_signature(), true);\n+  _is_substitutable_alt_cache.init(current,\n+                          vmClasses::ValueObjectMethods_klass(),\n+                          vmSymbols::isSubstitutableAlt_name()->as_C_string(),\n+                          vmSymbols::object_object_boolean_signature(), true);\n+  _value_object_hash_code_alt_cache.init(current,\n+                          vmClasses::ValueObjectMethods_klass(),\n+                          vmSymbols::valueObjectHashCodeAlt_name()->as_C_string(),\n+                          vmSymbols::object_int_signature(), true);\n","filename":"src\/hotspot\/share\/memory\/universe.cpp","additions":42,"deletions":14,"binary":false,"changes":56,"status":"modified"},{"patch":"@@ -256,0 +256,5 @@\n+  static Method*      is_substitutable_method();\n+  static Method*      value_object_hash_code_method();\n+  static Method*      is_substitutableAlt_method();\n+  static Method*      value_object_hash_codeAlt_method();\n+\n","filename":"src\/hotspot\/share\/memory\/universe.hpp","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -54,0 +54,1 @@\n+#include \"oops\/flatArrayKlass.hpp\"\n@@ -59,0 +60,1 @@\n+#include \"oops\/refArrayOop.hpp\"\n@@ -192,1 +194,1 @@\n-  return resolved_references()->replace_if_null(index, new_result);\n+  return refArrayOopDesc::cast(resolved_references())->replace_if_null(index, new_result);\n@@ -263,1 +265,1 @@\n-      \/\/ All of these should have been reverted back to ClassIndex before calling\n+      \/\/ All of these should have been reverted back to Unresolved before calling\n@@ -479,0 +481,1 @@\n+    assert(src_k->is_instance_klass() || src_k->is_typeArray_klass(), \"Sanity check\");\n@@ -625,0 +628,6 @@\n+void check_is_inline_type(Klass* k, TRAPS) {\n+  if (!k->is_inline_klass()) {\n+    THROW(vmSymbols::java_lang_IncompatibleClassChangeError());\n+  }\n+}\n+\n@@ -662,0 +671,1 @@\n+  bool inline_type_signature = false;\n@@ -670,0 +680,3 @@\n+  if (inline_type_signature) {\n+    name->decrement_refcount();\n+  }\n@@ -678,0 +691,16 @@\n+  if (!HAS_PENDING_EXCEPTION && inline_type_signature) {\n+    check_is_inline_type(k, THREAD);\n+  }\n+\n+  if (!HAS_PENDING_EXCEPTION) {\n+    Klass* bottom_klass = nullptr;\n+    if (k->is_objArray_klass()) {\n+      bottom_klass = ObjArrayKlass::cast(k)->bottom_klass();\n+      assert(bottom_klass != nullptr, \"Should be set\");\n+      assert(bottom_klass->is_instance_klass() || bottom_klass->is_typeArray_klass(), \"Sanity check\");\n+    } else if (k->is_flatArray_klass()) {\n+      bottom_klass = FlatArrayKlass::cast(k)->element_klass();\n+      assert(bottom_klass != nullptr, \"Should be set\");\n+    }\n+  }\n+\n","filename":"src\/hotspot\/share\/oops\/constantPool.cpp","additions":31,"deletions":2,"binary":false,"changes":33,"status":"modified"},{"patch":"@@ -275,1 +275,1 @@\n-  \/\/ For temporary use while constructing constant pool\n+  \/\/ For temporary use while constructing constant pool. Used during a retransform\/class redefinition as well.\n","filename":"src\/hotspot\/share\/oops\/constantPool.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -849,0 +849,6 @@\n+  product(bool, UseArrayLoadStoreProfile, true,                             \\\n+          \"Take advantage of profiling at array load\/store\")                \\\n+                                                                            \\\n+  product(bool, UseACmpProfile, true,                                       \\\n+          \"Take advantage of profiling at acmp\")                            \\\n+                                                                            \\\n","filename":"src\/hotspot\/share\/opto\/c2_globals.hpp","additions":6,"deletions":0,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -62,0 +62,1 @@\n+#include \"opto\/inlinetypenode.hpp\"\n@@ -69,0 +70,1 @@\n+#include \"opto\/movenode.hpp\"\n@@ -408,0 +410,6 @@\n+  if (dead->is_InlineType()) {\n+    remove_inline_type(dead);\n+  }\n+  if (dead->is_LoadFlat() || dead->is_StoreFlat()) {\n+    remove_flat_access(dead);\n+  }\n@@ -455,0 +463,3 @@\n+    if (n->outcnt() == 0) {\n+      worklist.push(n);\n+    }\n@@ -463,0 +474,7 @@\n+  remove_useless_nodes(_inline_type_nodes,  useful); \/\/ remove useless inline type nodes\n+  remove_useless_nodes(_flat_access_nodes, useful);  \/\/ remove useless flat access nodes\n+#ifdef ASSERT\n+  if (_modified_nodes != nullptr) {\n+    _modified_nodes->remove_useless_nodes(useful.member_set());\n+  }\n+#endif\n@@ -649,0 +667,1 @@\n+      _has_circular_inline_type(false),\n@@ -667,0 +686,2 @@\n+      _inline_type_nodes (comp_arena(), 8, 0, nullptr),\n+      _flat_access_nodes(comp_arena(), 8, 0, nullptr),\n@@ -775,4 +796,2 @@\n-      const TypeTuple *domain = StartOSRNode::osr_domain();\n-      const TypeTuple *range = TypeTuple::make_range(method()->signature());\n-      init_tf(TypeFunc::make(domain, range));\n-      StartNode* s = new StartOSRNode(root(), domain);\n+      init_tf(TypeFunc::make(method(), \/* is_osr_compilation = *\/ true));\n+      StartNode* s = new StartOSRNode(root(), tf()->domain_sig());\n@@ -785,1 +804,1 @@\n-      StartNode* s = new StartNode(root(), tf()->domain());\n+      StartNode* s = new StartNode(root(), tf()->domain_cc());\n@@ -886,0 +905,10 @@\n+  if (needs_stack_repair()) {\n+    \/\/ One extra slot for the special stack increment value\n+    next_slot += 2;\n+  }\n+  \/\/ TODO 8284443 Only reserve extra slot if needed\n+  if (InlineTypeReturnedAsFields) {\n+    \/\/ One extra slot to hold the null marker for a nullable\n+    \/\/ inline type return if we run out of registers.\n+    next_slot += 2;\n+  }\n@@ -923,0 +952,1 @@\n+      _has_circular_inline_type(false),\n@@ -1077,0 +1107,4 @@\n+  _has_flat_accesses = false;\n+  _flat_accesses_share_alias = true;\n+  _scalarize_in_safepoints = false;\n+\n@@ -1361,0 +1395,9 @@\n+  if (ta && ta->is_not_flat()) {\n+    \/\/ Erase not flat property for alias analysis.\n+    tj = ta = ta->cast_to_not_flat(false);\n+  }\n+  if (ta && ta->is_not_null_free()) {\n+    \/\/ Erase not null free property for alias analysis.\n+    tj = ta = ta->cast_to_not_null_free(false);\n+  }\n+\n@@ -1374,0 +1417,2 @@\n+    \/\/ For flat inline type array, each field has its own slice so\n+    \/\/ we must include the field offset.\n@@ -1414,1 +1459,1 @@\n-      tj = ta = TypeAryPtr::make(ptr,ta->const_oop(),tary,nullptr,false,offset);\n+      tj = ta = TypeAryPtr::make(ptr,ta->const_oop(),tary,nullptr,false,Type::Offset(offset), ta->field_offset());\n@@ -1418,1 +1463,6 @@\n-      tj = ta = TypeAryPtr::make(ptr,ta->const_oop(),tary,nullptr,false,offset);\n+      tj = ta = TypeAryPtr::make(ptr,ta->const_oop(),tary,nullptr,false,Type::Offset(offset), ta->field_offset());\n+    }\n+    \/\/ Initially all flattened array accesses share a single slice\n+    if (ta->is_flat() && ta->elem() != TypeInstPtr::BOTTOM && _flat_accesses_share_alias) {\n+      const TypeAry* tary = TypeAry::make(TypeInstPtr::BOTTOM, ta->size(), \/* stable= *\/ false, \/* flat= *\/ true);\n+      tj = ta = TypeAryPtr::make(ptr,ta->const_oop(),tary,nullptr,false,Type::Offset(offset), Type::Offset(Type::OffsetBot));\n@@ -1425,1 +1475,1 @@\n-      tj = ta = TypeAryPtr::make(ptr,ta->const_oop(),tary,aklass,false,offset);\n+      tj = ta = TypeAryPtr::make(ptr,ta->const_oop(),tary,aklass,false,Type::Offset(offset), ta->field_offset());\n@@ -1443,0 +1493,1 @@\n+    tj = to = to->cast_to_maybe_flat_in_array(); \/\/ flatten to maybe flat in array\n@@ -1475,1 +1526,1 @@\n-        tj = to = TypeInstPtr::make(TypePtr::BotPTR, env()->Object_klass(), false, nullptr, offset);\n+        tj = to = TypeInstPtr::make(TypePtr::BotPTR, env()->Object_klass(), false, nullptr, Type::Offset(offset));\n@@ -1496,1 +1547,2 @@\n-        assert(tj == TypeInstPtr::make(to->ptr(), canonical_holder, is_known_inst, nullptr, offset, instance_id), \"exact type should be canonical type\");\n+        assert(tj == TypeInstPtr::make(to->ptr(), canonical_holder, is_known_inst, nullptr, Type::Offset(offset), instance_id,\n+                                       TypePtr::MaybeFlat), \"exact type should be canonical type\");\n@@ -1499,1 +1551,2 @@\n-        tj = to = TypeInstPtr::make(to->ptr(), canonical_holder, is_known_inst, nullptr, offset, instance_id);\n+        tj = to = TypeInstPtr::make(to->ptr(), canonical_holder, is_known_inst, nullptr, Type::Offset(offset), instance_id,\n+                                    TypePtr::MaybeFlat);\n@@ -1514,1 +1567,2 @@\n-                                       offset);\n+                                       Type::Offset(offset),\n+                                       TypePtr::MaybeFlat);\n@@ -1520,1 +1574,1 @@\n-        tj = tk = TypeInstKlassPtr::make(TypePtr::NotNull, env()->Object_klass(), offset);\n+        tj = tk = TypeInstKlassPtr::make(TypePtr::NotNull, env()->Object_klass(), Type::Offset(offset), TypePtr::MaybeFlat);\n@@ -1522,1 +1576,1 @@\n-        tj = tk = TypeAryKlassPtr::make(TypePtr::NotNull, tk->is_aryklassptr()->elem(), k, offset);\n+        tj = tk = TypeAryKlassPtr::make(TypePtr::NotNull, tk->is_aryklassptr()->elem(), k, Type::Offset(offset), tk->is_not_flat(), tk->is_not_null_free(), tk->is_flat(), tk->is_null_free(), tk->is_atomic(), tk->is_aryklassptr()->is_refined_type());\n@@ -1525,1 +1579,0 @@\n-\n@@ -1655,1 +1708,1 @@\n-Compile::AliasType* Compile::find_alias_type(const TypePtr* adr_type, bool no_create, ciField* original_field) {\n+Compile::AliasType* Compile::find_alias_type(const TypePtr* adr_type, bool no_create, ciField* original_field, bool uncached) {\n@@ -1660,3 +1713,6 @@\n-  AliasCacheEntry* ace = probe_alias_cache(adr_type);\n-  if (ace->_adr_type == adr_type) {\n-    return alias_type(ace->_index);\n+  AliasCacheEntry* ace = nullptr;\n+  if (!uncached) {\n+    ace = probe_alias_cache(adr_type);\n+    if (ace->_adr_type == adr_type) {\n+      return alias_type(ace->_index);\n+    }\n@@ -1712,0 +1768,1 @@\n+    ciField* field = nullptr;\n@@ -1718,0 +1775,1 @@\n+      const Type* elemtype = flat->is_aryptr()->elem();\n@@ -1719,1 +1777,8 @@\n-        alias_type(idx)->set_element(flat->is_aryptr()->elem());\n+        alias_type(idx)->set_element(elemtype);\n+      }\n+      int field_offset = flat->is_aryptr()->field_offset().get();\n+      if (flat->is_flat() &&\n+          field_offset != Type::OffsetBot) {\n+        ciInlineKlass* vk = elemtype->inline_klass();\n+        field_offset += vk->payload_offset();\n+        field = vk->get_field_by_offset(field_offset, false);\n@@ -1735,0 +1800,2 @@\n+      if (flat->offset() == in_bytes(Klass::layout_helper_offset()))\n+        alias_type(idx)->set_rewritable(false);\n@@ -1745,1 +1812,0 @@\n-      ciField* field;\n@@ -1752,0 +1818,4 @@\n+      } else if (tinst->is_inlinetypeptr()) {\n+        \/\/ Inline type field\n+        ciInlineKlass* vk = tinst->inline_klass();\n+        field = vk->get_field_by_offset(tinst->offset(), false);\n@@ -1756,7 +1826,14 @@\n-      assert(field == nullptr ||\n-             original_field == nullptr ||\n-             (field->holder() == original_field->holder() &&\n-              field->offset_in_bytes() == original_field->offset_in_bytes() &&\n-              field->is_static() == original_field->is_static()), \"wrong field?\");\n-      \/\/ Set field() and is_rewritable() attributes.\n-      if (field != nullptr)  alias_type(idx)->set_field(field);\n+    }\n+    assert(field == nullptr ||\n+           original_field == nullptr ||\n+           (field->holder() == original_field->holder() &&\n+            field->offset_in_bytes() == original_field->offset_in_bytes() &&\n+            field->is_static() == original_field->is_static()), \"wrong field?\");\n+    \/\/ Set field() and is_rewritable() attributes.\n+    if (field != nullptr) {\n+      alias_type(idx)->set_field(field);\n+      if (flat->isa_aryptr()) {\n+        \/\/ Fields of flat arrays are rewritable although they are declared final\n+        assert(flat->is_flat(), \"must be a flat array\");\n+        alias_type(idx)->set_rewritable(true);\n+      }\n@@ -1767,3 +1844,4 @@\n-  ace->_adr_type = adr_type;\n-  ace->_index    = idx;\n-  assert(alias_type(adr_type) == alias_type(idx),  \"type must be installed\");\n+  if (!uncached) {\n+    ace->_adr_type = adr_type;\n+    ace->_index    = idx;\n+    assert(alias_type(adr_type) == alias_type(idx),  \"type must be installed\");\n@@ -1771,6 +1849,7 @@\n-  \/\/ Might as well try to fill the cache for the flattened version, too.\n-  AliasCacheEntry* face = probe_alias_cache(flat);\n-  if (face->_adr_type == nullptr) {\n-    face->_adr_type = flat;\n-    face->_index    = idx;\n-    assert(alias_type(flat) == alias_type(idx), \"flat type must work too\");\n+    \/\/ Might as well try to fill the cache for the flattened version, too.\n+    AliasCacheEntry* face = probe_alias_cache(flat);\n+    if (face->_adr_type == nullptr) {\n+      face->_adr_type = flat;\n+      face->_index    = idx;\n+      assert(alias_type(flat) == alias_type(idx), \"flat type must work too\");\n+    }\n@@ -1898,0 +1977,426 @@\n+void Compile::add_inline_type(Node* n) {\n+  assert(n->is_InlineType(), \"unexpected node\");\n+  _inline_type_nodes.push(n);\n+}\n+\n+void Compile::remove_inline_type(Node* n) {\n+  assert(n->is_InlineType(), \"unexpected node\");\n+  if (_inline_type_nodes.contains(n)) {\n+    _inline_type_nodes.remove(n);\n+  }\n+}\n+\n+\/\/ Does the return value keep otherwise useless inline type allocations alive?\n+static bool return_val_keeps_allocations_alive(Node* ret_val) {\n+  ResourceMark rm;\n+  Unique_Node_List wq;\n+  wq.push(ret_val);\n+  bool some_allocations = false;\n+  for (uint i = 0; i < wq.size(); i++) {\n+    Node* n = wq.at(i);\n+    if (n->outcnt() > 1) {\n+      \/\/ Some other use for the allocation\n+      return false;\n+    } else if (n->is_InlineType()) {\n+      wq.push(n->in(1));\n+    } else if (n->is_Phi()) {\n+      for (uint j = 1; j < n->req(); j++) {\n+        wq.push(n->in(j));\n+      }\n+    } else if (n->is_CheckCastPP() &&\n+               n->in(1)->is_Proj() &&\n+               n->in(1)->in(0)->is_Allocate()) {\n+      some_allocations = true;\n+    } else if (n->is_CheckCastPP()) {\n+      wq.push(n->in(1));\n+    }\n+  }\n+  return some_allocations;\n+}\n+\n+void Compile::process_inline_types(PhaseIterGVN &igvn, bool remove) {\n+  \/\/ Make sure that the return value does not keep an otherwise unused allocation alive\n+  if (tf()->returns_inline_type_as_fields()) {\n+    Node* ret = nullptr;\n+    for (uint i = 1; i < root()->req(); i++) {\n+      Node* in = root()->in(i);\n+      if (in->Opcode() == Op_Return) {\n+        assert(ret == nullptr, \"only one return\");\n+        ret = in;\n+      }\n+    }\n+    if (ret != nullptr) {\n+      Node* ret_val = ret->in(TypeFunc::Parms);\n+      if (igvn.type(ret_val)->isa_oopptr() &&\n+          return_val_keeps_allocations_alive(ret_val)) {\n+        igvn.replace_input_of(ret, TypeFunc::Parms, InlineTypeNode::tagged_klass(igvn.type(ret_val)->inline_klass(), igvn));\n+        assert(ret_val->outcnt() == 0, \"should be dead now\");\n+        igvn.remove_dead_node(ret_val);\n+      }\n+    }\n+  }\n+  if (_inline_type_nodes.length() == 0) {\n+    return;\n+  }\n+  \/\/ Scalarize inline types in safepoint debug info.\n+  \/\/ Delay this until all inlining is over to avoid getting inconsistent debug info.\n+  set_scalarize_in_safepoints(true);\n+  for (int i = _inline_type_nodes.length()-1; i >= 0; i--) {\n+    InlineTypeNode* vt = _inline_type_nodes.at(i)->as_InlineType();\n+    vt->make_scalar_in_safepoints(&igvn);\n+    igvn.record_for_igvn(vt);\n+  }\n+  if (remove) {\n+    \/\/ Remove inline type nodes by replacing them with their oop input\n+    while (_inline_type_nodes.length() > 0) {\n+      InlineTypeNode* vt = _inline_type_nodes.pop()->as_InlineType();\n+      if (vt->outcnt() == 0) {\n+        igvn.remove_dead_node(vt);\n+        continue;\n+      }\n+      for (DUIterator i = vt->outs(); vt->has_out(i); i++) {\n+        DEBUG_ONLY(bool must_be_buffered = false);\n+        Node* u = vt->out(i);\n+        \/\/ Check if any users are blackholes. If so, rewrite them to use either the\n+        \/\/ allocated buffer, or individual components, instead of the inline type node\n+        \/\/ that goes away.\n+        if (u->is_Blackhole()) {\n+          BlackholeNode* bh = u->as_Blackhole();\n+\n+          \/\/ Unlink the old input\n+          int idx = bh->find_edge(vt);\n+          assert(idx != -1, \"The edge should be there\");\n+          bh->del_req(idx);\n+          --i;\n+\n+          if (vt->is_allocated(&igvn)) {\n+            \/\/ Already has the allocated instance, blackhole that\n+            bh->add_req(vt->get_oop());\n+          } else {\n+            \/\/ Not allocated yet, blackhole the components\n+            for (uint c = 0; c < vt->field_count(); c++) {\n+              bh->add_req(vt->field_value(c));\n+            }\n+          }\n+\n+          \/\/ Node modified, record for IGVN\n+          igvn.record_for_igvn(bh);\n+        }\n+#ifdef ASSERT\n+        \/\/ Verify that inline type is buffered when replacing by oop\n+        else if (u->is_InlineType()) {\n+          \/\/ InlineType uses don't need buffering because they are about to be replaced as well\n+        } else if (u->is_Phi()) {\n+          \/\/ TODO 8302217 Remove this once InlineTypeNodes are reliably pushed through\n+        } else {\n+          must_be_buffered = true;\n+        }\n+        if (must_be_buffered && !vt->is_allocated(&igvn)) {\n+          vt->dump(0);\n+          u->dump(0);\n+          assert(false, \"Should have been buffered\");\n+        }\n+#endif\n+      }\n+      igvn.replace_node(vt, vt->get_oop());\n+    }\n+  }\n+  igvn.optimize();\n+}\n+\n+void Compile::add_flat_access(Node* n) {\n+  assert(n != nullptr && (n->Opcode() == Op_LoadFlat || n->Opcode() == Op_StoreFlat), \"unexpected node %s\", n == nullptr ? \"nullptr\" : n->Name());\n+  assert(!_flat_access_nodes.contains(n), \"duplicate insertion\");\n+  _flat_access_nodes.push(n);\n+}\n+\n+void Compile::remove_flat_access(Node* n) {\n+  assert(n != nullptr && (n->Opcode() == Op_LoadFlat || n->Opcode() == Op_StoreFlat), \"unexpected node %s\", n == nullptr ? \"nullptr\" : n->Name());\n+  _flat_access_nodes.remove_if_existing(n);\n+}\n+\n+void Compile::process_flat_accesses(PhaseIterGVN& igvn) {\n+  assert(igvn._worklist.size() == 0, \"should be empty\");\n+  igvn.set_delay_transform(true);\n+  for (int i = _flat_access_nodes.length() - 1; i >= 0; i--) {\n+    Node* n = _flat_access_nodes.at(i);\n+    assert(n != nullptr, \"unexpected nullptr\");\n+    if (n->is_LoadFlat()) {\n+      n->as_LoadFlat()->expand_atomic(igvn);\n+    } else {\n+      n->as_StoreFlat()->expand_atomic(igvn);\n+    }\n+  }\n+  _flat_access_nodes.clear_and_deallocate();\n+  igvn.set_delay_transform(false);\n+  igvn.optimize();\n+}\n+\n+void Compile::adjust_flat_array_access_aliases(PhaseIterGVN& igvn) {\n+  if (!_has_flat_accesses) {\n+    return;\n+  }\n+  \/\/ Initially, all flat array accesses share the same slice to\n+  \/\/ keep dependencies with Object[] array accesses (that could be\n+  \/\/ to a flat array) correct. We're done with parsing so we\n+  \/\/ now know all flat array accesses in this compile\n+  \/\/ unit. Let's move flat array accesses to their own slice,\n+  \/\/ one per element field. This should help memory access\n+  \/\/ optimizations.\n+  ResourceMark rm;\n+  Unique_Node_List wq;\n+  wq.push(root());\n+\n+  Node_List mergememnodes;\n+  Node_List memnodes;\n+\n+  \/\/ Alias index currently shared by all flat memory accesses\n+  int index = get_alias_index(TypeAryPtr::INLINES);\n+\n+  \/\/ Find MergeMem nodes and flat array accesses\n+  for (uint i = 0; i < wq.size(); i++) {\n+    Node* n = wq.at(i);\n+    if (n->is_Mem()) {\n+      const TypePtr* adr_type = nullptr;\n+      adr_type = get_adr_type(get_alias_index(n->adr_type()));\n+      if (adr_type == TypeAryPtr::INLINES) {\n+        memnodes.push(n);\n+      }\n+    } else if (n->is_MergeMem()) {\n+      MergeMemNode* mm = n->as_MergeMem();\n+      if (mm->memory_at(index) != mm->base_memory()) {\n+        mergememnodes.push(n);\n+      }\n+    }\n+    for (uint j = 0; j < n->req(); j++) {\n+      Node* m = n->in(j);\n+      if (m != nullptr) {\n+        wq.push(m);\n+      }\n+    }\n+  }\n+\n+  if (memnodes.size() > 0) {\n+    _flat_accesses_share_alias = false;\n+\n+    \/\/ We are going to change the slice for the flat array\n+    \/\/ accesses so we need to clear the cache entries that refer to\n+    \/\/ them.\n+    for (uint i = 0; i < AliasCacheSize; i++) {\n+      AliasCacheEntry* ace = &_alias_cache[i];\n+      if (ace->_adr_type != nullptr &&\n+          ace->_adr_type->is_flat()) {\n+        ace->_adr_type = nullptr;\n+        ace->_index = (i != 0) ? 0 : AliasIdxTop; \/\/ Make sure the nullptr adr_type resolves to AliasIdxTop\n+      }\n+    }\n+\n+    \/\/ Find what aliases we are going to add\n+    int start_alias = num_alias_types()-1;\n+    int stop_alias = 0;\n+\n+    for (uint i = 0; i < memnodes.size(); i++) {\n+      Node* m = memnodes.at(i);\n+      const TypePtr* adr_type = nullptr;\n+      adr_type = m->adr_type();\n+#ifdef ASSERT\n+      m->as_Mem()->set_adr_type(adr_type);\n+#endif\n+      int idx = get_alias_index(adr_type);\n+      start_alias = MIN2(start_alias, idx);\n+      stop_alias = MAX2(stop_alias, idx);\n+    }\n+\n+    assert(stop_alias >= start_alias, \"should have expanded aliases\");\n+\n+    Node_Stack stack(0);\n+#ifdef ASSERT\n+    VectorSet seen(Thread::current()->resource_area());\n+#endif\n+    \/\/ Now let's fix the memory graph so each flat array access\n+    \/\/ is moved to the right slice. Start from the MergeMem nodes.\n+    uint last = unique();\n+    for (uint i = 0; i < mergememnodes.size(); i++) {\n+      MergeMemNode* current = mergememnodes.at(i)->as_MergeMem();\n+      Node* n = current->memory_at(index);\n+      MergeMemNode* mm = nullptr;\n+      do {\n+        \/\/ Follow memory edges through memory accesses, phis and\n+        \/\/ narrow membars and push nodes on the stack. Once we hit\n+        \/\/ bottom memory, we pop element off the stack one at a\n+        \/\/ time, in reverse order, and move them to the right slice\n+        \/\/ by changing their memory edges.\n+        if ((n->is_Phi() && n->adr_type() != TypePtr::BOTTOM) || n->is_Mem() || n->adr_type() == TypeAryPtr::INLINES) {\n+          assert(!seen.test_set(n->_idx), \"\");\n+          \/\/ Uses (a load for instance) will need to be moved to the\n+          \/\/ right slice as well and will get a new memory state\n+          \/\/ that we don't know yet. The use could also be the\n+          \/\/ backedge of a loop. We put a place holder node between\n+          \/\/ the memory node and its uses. We replace that place\n+          \/\/ holder with the correct memory state once we know it,\n+          \/\/ i.e. when nodes are popped off the stack. Using the\n+          \/\/ place holder make the logic work in the presence of\n+          \/\/ loops.\n+          if (n->outcnt() > 1) {\n+            Node* place_holder = nullptr;\n+            assert(!n->has_out_with(Op_Node), \"\");\n+            for (DUIterator k = n->outs(); n->has_out(k); k++) {\n+              Node* u = n->out(k);\n+              if (u != current && u->_idx < last) {\n+                bool success = false;\n+                for (uint l = 0; l < u->req(); l++) {\n+                  if (!stack.is_empty() && u == stack.node() && l == stack.index()) {\n+                    continue;\n+                  }\n+                  Node* in = u->in(l);\n+                  if (in == n) {\n+                    if (place_holder == nullptr) {\n+                      place_holder = new Node(1);\n+                      place_holder->init_req(0, n);\n+                    }\n+                    igvn.replace_input_of(u, l, place_holder);\n+                    success = true;\n+                  }\n+                }\n+                if (success) {\n+                  --k;\n+                }\n+              }\n+            }\n+          }\n+          if (n->is_Phi()) {\n+            stack.push(n, 1);\n+            n = n->in(1);\n+          } else if (n->is_Mem()) {\n+            stack.push(n, n->req());\n+            n = n->in(MemNode::Memory);\n+          } else {\n+            assert(n->is_Proj() && n->in(0)->Opcode() == Op_MemBarCPUOrder, \"\");\n+            stack.push(n, n->req());\n+            n = n->in(0)->in(TypeFunc::Memory);\n+          }\n+        } else {\n+          assert(n->adr_type() == TypePtr::BOTTOM || (n->Opcode() == Op_Node && n->_idx >= last) || (n->is_Proj() && n->in(0)->is_Initialize()), \"\");\n+          \/\/ Build a new MergeMem node to carry the new memory state\n+          \/\/ as we build it. IGVN should fold extraneous MergeMem\n+          \/\/ nodes.\n+          mm = MergeMemNode::make(n);\n+          igvn.register_new_node_with_optimizer(mm);\n+          while (stack.size() > 0) {\n+            Node* m = stack.node();\n+            uint idx = stack.index();\n+            if (m->is_Mem()) {\n+              \/\/ Move memory node to its new slice\n+              const TypePtr* adr_type = m->adr_type();\n+              int alias = get_alias_index(adr_type);\n+              Node* prev = mm->memory_at(alias);\n+              igvn.replace_input_of(m, MemNode::Memory, prev);\n+              mm->set_memory_at(alias, m);\n+            } else if (m->is_Phi()) {\n+              \/\/ We need as many new phis as there are new aliases\n+              igvn.replace_input_of(m, idx, mm);\n+              if (idx == m->req()-1) {\n+                Node* r = m->in(0);\n+                for (uint j = (uint)start_alias; j <= (uint)stop_alias; j++) {\n+                  const TypePtr* adr_type = get_adr_type(j);\n+                  if (!adr_type->isa_aryptr() || !adr_type->is_flat() || j == (uint)index) {\n+                    continue;\n+                  }\n+                  Node* phi = new PhiNode(r, Type::MEMORY, get_adr_type(j));\n+                  igvn.register_new_node_with_optimizer(phi);\n+                  for (uint k = 1; k < m->req(); k++) {\n+                    phi->init_req(k, m->in(k)->as_MergeMem()->memory_at(j));\n+                  }\n+                  mm->set_memory_at(j, phi);\n+                }\n+                Node* base_phi = new PhiNode(r, Type::MEMORY, TypePtr::BOTTOM);\n+                igvn.register_new_node_with_optimizer(base_phi);\n+                for (uint k = 1; k < m->req(); k++) {\n+                  base_phi->init_req(k, m->in(k)->as_MergeMem()->base_memory());\n+                }\n+                mm->set_base_memory(base_phi);\n+              }\n+            } else {\n+              \/\/ This is a MemBarCPUOrder node from\n+              \/\/ Parse::array_load()\/Parse::array_store(), in the\n+              \/\/ branch that handles flat arrays hidden under\n+              \/\/ an Object[] array. We also need one new membar per\n+              \/\/ new alias to keep the unknown access that the\n+              \/\/ membars protect properly ordered with accesses to\n+              \/\/ known flat array.\n+              assert(m->is_Proj(), \"projection expected\");\n+              Node* ctrl = m->in(0)->in(TypeFunc::Control);\n+              igvn.replace_input_of(m->in(0), TypeFunc::Control, top());\n+              for (uint j = (uint)start_alias; j <= (uint)stop_alias; j++) {\n+                const TypePtr* adr_type = get_adr_type(j);\n+                if (!adr_type->isa_aryptr() || !adr_type->is_flat() || j == (uint)index) {\n+                  continue;\n+                }\n+                MemBarNode* mb = new MemBarCPUOrderNode(this, j, nullptr);\n+                igvn.register_new_node_with_optimizer(mb);\n+                Node* mem = mm->memory_at(j);\n+                mb->init_req(TypeFunc::Control, ctrl);\n+                mb->init_req(TypeFunc::Memory, mem);\n+                ctrl = new ProjNode(mb, TypeFunc::Control);\n+                igvn.register_new_node_with_optimizer(ctrl);\n+                mem = new ProjNode(mb, TypeFunc::Memory);\n+                igvn.register_new_node_with_optimizer(mem);\n+                mm->set_memory_at(j, mem);\n+              }\n+              igvn.replace_node(m->in(0)->as_Multi()->proj_out(TypeFunc::Control), ctrl);\n+            }\n+            if (idx < m->req()-1) {\n+              idx += 1;\n+              stack.set_index(idx);\n+              n = m->in(idx);\n+              break;\n+            }\n+            \/\/ Take care of place holder nodes\n+            if (m->has_out_with(Op_Node)) {\n+              Node* place_holder = m->find_out_with(Op_Node);\n+              if (place_holder != nullptr) {\n+                Node* mm_clone = mm->clone();\n+                igvn.register_new_node_with_optimizer(mm_clone);\n+                Node* hook = new Node(1);\n+                hook->init_req(0, mm);\n+                igvn.replace_node(place_holder, mm_clone);\n+                hook->destruct(&igvn);\n+              }\n+              assert(!m->has_out_with(Op_Node), \"place holder should be gone now\");\n+            }\n+            stack.pop();\n+          }\n+        }\n+      } while(stack.size() > 0);\n+      \/\/ Fix the memory state at the MergeMem we started from\n+      igvn.rehash_node_delayed(current);\n+      for (uint j = (uint)start_alias; j <= (uint)stop_alias; j++) {\n+        const TypePtr* adr_type = get_adr_type(j);\n+        if (!adr_type->isa_aryptr() || !adr_type->is_flat()) {\n+          continue;\n+        }\n+        current->set_memory_at(j, mm);\n+      }\n+      current->set_memory_at(index, current->base_memory());\n+    }\n+    igvn.optimize();\n+  }\n+  print_method(PHASE_SPLIT_INLINES_ARRAY, 2);\n+#ifdef ASSERT\n+  if (!_flat_accesses_share_alias) {\n+    wq.clear();\n+    wq.push(root());\n+    for (uint i = 0; i < wq.size(); i++) {\n+      Node* n = wq.at(i);\n+      assert(n->adr_type() != TypeAryPtr::INLINES, \"should have been removed from the graph\");\n+      for (uint j = 0; j < n->req(); j++) {\n+        Node* m = n->in(j);\n+        if (m != nullptr) {\n+          wq.push(m);\n+        }\n+      }\n+    }\n+  }\n+#endif\n+}\n+\n@@ -2016,1 +2521,1 @@\n-        if (!live_locals.at(i) && !local->is_top() && local != lhs && local!= rhs) {\n+        if (!live_locals.at(i) && !local->is_top() && local != lhs && local != rhs) {\n@@ -2031,1 +2536,1 @@\n-    \/\/ keep the mondified trap for late query\n+    \/\/ keep the modified trap for late query\n@@ -2243,1 +2748,4 @@\n-  assert(_modified_nodes == nullptr, \"not allowed\");\n+#ifdef ASSERT\n+  Unique_Node_List* modified_nodes = _modified_nodes;\n+  _modified_nodes = nullptr;\n+#endif\n@@ -2256,0 +2764,1 @@\n+  DEBUG_ONLY( _modified_nodes = modified_nodes; )\n@@ -2399,0 +2908,5 @@\n+  \/\/ Process inline type nodes now that all inlining is over\n+  process_inline_types(igvn);\n+\n+  adjust_flat_array_access_aliases(igvn);\n+\n@@ -2401,0 +2915,11 @@\n+  if (C->macro_count() > 0) {\n+    \/\/ Eliminate some macro nodes before EA to reduce analysis pressure\n+    PhaseMacroExpand mexp(igvn);\n+    mexp.eliminate_macro_nodes(\/* eliminate_locks= *\/ false);\n+    if (failing()) {\n+      return;\n+    }\n+    igvn.set_delay_transform(false);\n+    print_method(PHASE_ITER_GVN_AFTER_ELIMINATION, 2);\n+  }\n+\n@@ -2411,1 +2936,14 @@\n-      if (failing())  return;\n+      if (failing()) {\n+        return;\n+      }\n+      print_method(PHASE_PHASEIDEAL_BEFORE_EA, 2);\n+      if (C->macro_count() > 0) {\n+        \/\/ Eliminate some macro nodes before EA to reduce analysis pressure\n+        PhaseMacroExpand mexp(igvn);\n+        mexp.eliminate_macro_nodes(\/* eliminate_locks= *\/ false);\n+        if (failing()) {\n+          return;\n+        }\n+        igvn.set_delay_transform(false);\n+        print_method(PHASE_ITER_GVN_AFTER_ELIMINATION, 2);\n+      }\n@@ -2413,0 +2951,1 @@\n+\n@@ -2414,1 +2953,0 @@\n-    print_method(PHASE_PHASEIDEAL_BEFORE_EA, 2);\n@@ -2432,1 +2970,3 @@\n-        if (failing()) return;\n+        if (failing()) {\n+          return;\n+        }\n@@ -2436,3 +2976,0 @@\n-        igvn.optimize();\n-        if (failing()) return;\n-\n@@ -2453,0 +2990,5 @@\n+  process_flat_accesses(igvn);\n+  if (failing()) {\n+    return;\n+  }\n+\n@@ -2529,0 +3071,8 @@\n+  assert(_late_inlines.length() == 0 || IncrementalInlineMH || IncrementalInlineVirtual, \"not empty\");\n+\n+  if (_late_inlines.length() > 0) {\n+    \/\/ More opportunities to optimize virtual and MH calls.\n+    \/\/ Though it's maybe too late to perform inlining, strength-reducing them to direct calls is still an option.\n+    process_late_inline_calls_no_inline(igvn);\n+  }\n+\n@@ -2531,0 +3081,7 @@\n+    PhaseMacroExpand mex(igvn);\n+    \/\/ Last attempt to eliminate macro nodes.\n+    mex.eliminate_macro_nodes();\n+    if (failing()) {\n+      return;\n+    }\n+\n@@ -2532,1 +3089,0 @@\n-    PhaseMacroExpand  mex(igvn);\n@@ -2552,0 +3108,4 @@\n+  \/\/ Process inline type nodes again and remove them. From here\n+  \/\/ on we don't need to keep track of field values anymore.\n+  process_inline_types(igvn, \/* remove= *\/ true);\n+\n@@ -2568,0 +3128,1 @@\n+  DEBUG_ONLY( _late_inlines.clear(); )\n@@ -2570,9 +3131,0 @@\n-\n-  assert(_late_inlines.length() == 0 || IncrementalInlineMH || IncrementalInlineVirtual, \"not empty\");\n-\n-  if (_late_inlines.length() > 0) {\n-    \/\/ More opportunities to optimize virtual and MH calls.\n-    \/\/ Though it's maybe too late to perform inlining, strength-reducing them to direct calls is still an option.\n-    process_late_inline_calls_no_inline(igvn);\n-    if (failing())  return;\n-  }\n@@ -3324,1 +3876,1 @@\n-      for (unsigned int i = TypeFunc::Parms; i < call->tf()->domain()->cnt(); i++) {\n+      for (unsigned int i = TypeFunc::Parms; i < call->tf()->domain_sig()->cnt(); i++) {\n@@ -3370,0 +3922,1 @@\n+  case Op_StoreLSpecial:\n@@ -3922,0 +4475,5 @@\n+  case Op_InlineType: {\n+    n->dump(-1);\n+    assert(false, \"inline type node was not removed\");\n+    break;\n+  }\n@@ -4297,2 +4855,2 @@\n-      if (accessing_method->is_static_initializer() ||\n-          accessing_method->is_object_initializer() ||\n+      if (accessing_method->is_class_initializer() ||\n+          accessing_method->is_object_constructor() ||\n@@ -4306,1 +4864,1 @@\n-      if (accessing_method->is_static_initializer()) {\n+      if (accessing_method->is_class_initializer()) {\n@@ -4376,0 +4934,1 @@\n+               (n->is_Allocate() && i >= AllocateNode::InlineType) ||\n@@ -4378,1 +4937,1 @@\n-              \"only region, phi, arraycopy, unlock or membar nodes have null data edges\");\n+              \"only region, phi, arraycopy, allocate, unlock or membar nodes have null data edges\");\n@@ -4529,0 +5088,7 @@\n+\n+    \/\/ Do not fold the subtype check to an array klass pointer comparison for null-able inline type arrays\n+    \/\/ because null-free [LMyValue <: null-able [LMyValue but the klasses are different. Perform a full test.\n+    if (!superk->is_aryklassptr()->is_null_free() && superk->is_aryklassptr()->elem()->isa_instklassptr() &&\n+        superk->is_aryklassptr()->elem()->is_instklassptr()->instance_klass()->is_inlinetype()) {\n+      return SSC_full_test;\n+    }\n@@ -5012,0 +5578,21 @@\n+Node* Compile::optimize_acmp(PhaseGVN* phase, Node* a, Node* b) {\n+  const TypeInstPtr* ta = phase->type(a)->isa_instptr();\n+  const TypeInstPtr* tb = phase->type(b)->isa_instptr();\n+  if (!EnableValhalla || ta == nullptr || tb == nullptr ||\n+      ta->is_zero_type() || tb->is_zero_type() ||\n+      !ta->can_be_inline_type() || !tb->can_be_inline_type()) {\n+    \/\/ Use old acmp if one operand is null or not an inline type\n+    return new CmpPNode(a, b);\n+  } else if (ta->is_inlinetypeptr() || tb->is_inlinetypeptr()) {\n+    \/\/ We know that one operand is an inline type. Therefore,\n+    \/\/ new acmp will only return true if both operands are nullptr.\n+    \/\/ Check if both operands are null by or'ing the oops.\n+    a = phase->transform(new CastP2XNode(nullptr, a));\n+    b = phase->transform(new CastP2XNode(nullptr, b));\n+    a = phase->transform(new OrXNode(a, b));\n+    return new CmpXNode(a, phase->MakeConX(0));\n+  }\n+  \/\/ Use new acmp\n+  return nullptr;\n+}\n+\n@@ -5367,0 +5954,2 @@\n+  } else if (bt == T_FLOAT) {\n+    result = new MoveI2FNode(value);\n","filename":"src\/hotspot\/share\/opto\/compile.cpp","additions":650,"deletions":61,"binary":false,"changes":711,"status":"modified"},{"patch":"@@ -46,0 +46,1 @@\n+class UnswitchCandidate;\n@@ -88,1 +89,1 @@\n-       };\n+         FlatArrays            = 1<<18};\n@@ -111,0 +112,1 @@\n+  bool is_flat_arrays() const { return _loop_flags & FlatArrays; }\n@@ -124,0 +126,1 @@\n+  void mark_flat_arrays() { _loop_flags |= FlatArrays; }\n@@ -756,0 +759,1 @@\n+  bool no_unswitch_candidate() const;\n@@ -1566,1 +1570,2 @@\n-  IfNode* find_unswitch_candidate(const IdealLoopTree* loop) const;\n+  IfNode* find_unswitch_candidates(const IdealLoopTree* loop, Node_List& flat_array_checks) const;\n+  IfNode* find_unswitch_candidate_from_idoms(const IdealLoopTree* loop) const;\n@@ -1573,1 +1578,1 @@\n-                                   const UnswitchedLoopSelector& unswitched_loop_selector);\n+                                   const UnswitchCandidate& unswitch_candidate, const IfNode* loop_selector);\n@@ -1581,0 +1586,1 @@\n+                                            const UnswitchCandidate& unswitch_candidate,\n@@ -1778,0 +1784,1 @@\n+  void move_flat_array_check_out_of_loop(Node* n);\n@@ -1779,0 +1786,1 @@\n+  bool flat_array_element_type_check(Node *n);\n@@ -1970,0 +1978,2 @@\n+  void collect_flat_array_checks(const IdealLoopTree* loop, Node_List& flat_array_checks) const;\n+\n","filename":"src\/hotspot\/share\/opto\/loopnode.hpp","additions":13,"deletions":3,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -34,0 +34,1 @@\n+#include \"opto\/inlinetypenode.hpp\"\n@@ -66,0 +67,6 @@\n+  \/\/ Inline types should not be split through Phis because they cannot be merged\n+  \/\/ through Phi nodes but each value input needs to be merged individually.\n+  if (n->is_InlineType()) {\n+    return nullptr;\n+  }\n+\n@@ -795,0 +802,4 @@\n+      if (inp->isa_InlineType()) {\n+        \/\/ TODO 8302217 This prevents PhiNode::push_inline_types_through\n+        return nullptr;\n+      }\n@@ -1120,0 +1131,48 @@\n+\/\/ We can't use immutable memory for the flat array check because we are loading the mark word which is\n+\/\/ mutable. Although the bits we are interested in are immutable (we check for markWord::unlocked_value),\n+\/\/ we need to use raw memory to not break anti dependency analysis. Below code will attempt to still move\n+\/\/ flat array checks out of loops, mainly to enable loop unswitching.\n+void PhaseIdealLoop::move_flat_array_check_out_of_loop(Node* n) {\n+  \/\/ Skip checks for more than one array\n+  if (n->req() > 3) {\n+    return;\n+  }\n+  Node* mem = n->in(FlatArrayCheckNode::Memory);\n+  Node* array = n->in(FlatArrayCheckNode::ArrayOrKlass)->uncast();\n+  IdealLoopTree* check_loop = get_loop(get_ctrl(n));\n+  IdealLoopTree* ary_loop = get_loop(get_ctrl(array));\n+\n+  \/\/ Check if array is loop invariant\n+  if (!check_loop->is_member(ary_loop)) {\n+    \/\/ Walk up memory graph from the check until we leave the loop\n+    VectorSet wq;\n+    wq.set(mem->_idx);\n+    while (check_loop->is_member(get_loop(ctrl_or_self(mem)))) {\n+      if (mem->is_Phi()) {\n+        mem = mem->in(1);\n+      } else if (mem->is_MergeMem()) {\n+        mem = mem->as_MergeMem()->memory_at(Compile::AliasIdxRaw);\n+      } else if (mem->is_Proj()) {\n+        mem = mem->in(0);\n+      } else if (mem->is_MemBar() || mem->is_SafePoint()) {\n+        mem = mem->in(TypeFunc::Memory);\n+      } else if (mem->is_Store() || mem->is_LoadStore() || mem->is_ClearArray()) {\n+        mem = mem->in(MemNode::Memory);\n+      } else {\n+#ifdef ASSERT\n+        mem->dump();\n+#endif\n+        ShouldNotReachHere();\n+      }\n+      if (wq.test_set(mem->_idx)) {\n+        return;\n+      }\n+    }\n+    \/\/ Replace memory input and re-compute ctrl to move the check out of the loop\n+    _igvn.replace_input_of(n, 1, mem);\n+    set_ctrl_and_loop(n, get_early_ctrl(n));\n+    Node* bol = n->unique_out();\n+    set_ctrl_and_loop(bol, get_early_ctrl(bol));\n+  }\n+}\n+\n@@ -1132,0 +1191,6 @@\n+\n+  if (n->isa_FlatArrayCheck()) {\n+    move_flat_array_check_out_of_loop(n);\n+    return n;\n+  }\n+\n@@ -1404,0 +1469,98 @@\n+bool PhaseIdealLoop::flat_array_element_type_check(Node *n) {\n+  \/\/ If the CmpP is a subtype check for a value that has just been\n+  \/\/ loaded from an array, the subtype check guarantees the value\n+  \/\/ can't be stored in a flat array and the load of the value\n+  \/\/ happens with a flat array check then: push the type check\n+  \/\/ through the phi of the flat array check. This needs special\n+  \/\/ logic because the subtype check's input is not a phi but a\n+  \/\/ LoadKlass that must first be cloned through the phi.\n+  if (n->Opcode() != Op_CmpP) {\n+    return false;\n+  }\n+\n+  Node* klassptr = n->in(1);\n+  Node* klasscon = n->in(2);\n+\n+  if (klassptr->is_DecodeNarrowPtr()) {\n+    klassptr = klassptr->in(1);\n+  }\n+\n+  if (klassptr->Opcode() != Op_LoadKlass && klassptr->Opcode() != Op_LoadNKlass) {\n+    return false;\n+  }\n+\n+  if (!klasscon->is_Con()) {\n+    return false;\n+  }\n+\n+  Node* addr = klassptr->in(MemNode::Address);\n+\n+  if (!addr->is_AddP()) {\n+    return false;\n+  }\n+\n+  intptr_t offset;\n+  Node* obj = AddPNode::Ideal_base_and_offset(addr, &_igvn, offset);\n+\n+  if (obj == nullptr) {\n+    return false;\n+  }\n+\n+  assert(obj != nullptr && addr->in(AddPNode::Base) == addr->in(AddPNode::Address), \"malformed AddP?\");\n+  if (obj->Opcode() == Op_CastPP) {\n+    obj = obj->in(1);\n+  }\n+\n+  if (!obj->is_Phi()) {\n+    return false;\n+  }\n+\n+  Node* region = obj->in(0);\n+\n+  Node* phi = PhiNode::make_blank(region, n->in(1));\n+  for (uint i = 1; i < region->req(); i++) {\n+    Node* in = obj->in(i);\n+    Node* ctrl = region->in(i);\n+    if (addr->in(AddPNode::Base) != obj) {\n+      Node* cast = addr->in(AddPNode::Base);\n+      assert(cast->Opcode() == Op_CastPP && cast->in(0) != nullptr, \"inconsistent subgraph\");\n+      Node* cast_clone = cast->clone();\n+      cast_clone->set_req(0, ctrl);\n+      cast_clone->set_req(1, in);\n+      register_new_node(cast_clone, ctrl);\n+      const Type* tcast = cast_clone->Value(&_igvn);\n+      _igvn.set_type(cast_clone, tcast);\n+      cast_clone->as_Type()->set_type(tcast);\n+      in = cast_clone;\n+    }\n+    Node* addr_clone = addr->clone();\n+    addr_clone->set_req(AddPNode::Base, in);\n+    addr_clone->set_req(AddPNode::Address, in);\n+    register_new_node(addr_clone, ctrl);\n+    _igvn.set_type(addr_clone, addr_clone->Value(&_igvn));\n+    Node* klassptr_clone = klassptr->clone();\n+    klassptr_clone->set_req(2, addr_clone);\n+    register_new_node(klassptr_clone, ctrl);\n+    _igvn.set_type(klassptr_clone, klassptr_clone->Value(&_igvn));\n+    if (klassptr != n->in(1)) {\n+      Node* decode = n->in(1);\n+      assert(decode->is_DecodeNarrowPtr(), \"inconsistent subgraph\");\n+      Node* decode_clone = decode->clone();\n+      decode_clone->set_req(1, klassptr_clone);\n+      register_new_node(decode_clone, ctrl);\n+      _igvn.set_type(decode_clone, decode_clone->Value(&_igvn));\n+      klassptr_clone = decode_clone;\n+    }\n+    phi->set_req(i, klassptr_clone);\n+  }\n+  register_new_node(phi, region);\n+  Node* orig = n->in(1);\n+  _igvn.replace_input_of(n, 1, phi);\n+  split_if_with_blocks_post(n);\n+  if (n->outcnt() != 0) {\n+    _igvn.replace_input_of(n, 1, orig);\n+    _igvn.remove_dead_node(phi);\n+  }\n+  return true;\n+}\n+\n@@ -1410,0 +1573,4 @@\n+  if (flat_array_element_type_check(n)) {\n+    return;\n+  }\n+\n@@ -1553,0 +1720,5 @@\n+\n+  \/\/ Remove multiple allocations of the same inline type\n+  if (n->is_InlineType()) {\n+    n->as_InlineType()->remove_redundant_allocations(this);\n+  }\n@@ -2061,1 +2233,9 @@\n-  Node *sample_cmp = sample_bool->in(1);\n+  Node* sample_cmp = sample_bool->in(1);\n+  const Type* t = Type::TOP;\n+  const TypePtr* at = nullptr;\n+  if (sample_cmp->is_FlatArrayCheck()) {\n+    \/\/ Left input of a FlatArrayCheckNode is memory, set the (adr) type of the phi accordingly\n+    assert(sample_cmp->in(1)->bottom_type() == Type::MEMORY, \"unexpected input type\");\n+    t = Type::MEMORY;\n+    at = TypeRawPtr::BOTTOM;\n+  }\n@@ -2064,1 +2244,1 @@\n-  PhiNode *phi1 = new PhiNode(phi->in(0), Type::TOP);\n+  PhiNode *phi1 = new PhiNode(phi->in(0), t, at);\n","filename":"src\/hotspot\/share\/opto\/loopopts.cpp","additions":182,"deletions":2,"binary":false,"changes":184,"status":"modified"},{"patch":"@@ -90,0 +90,1 @@\n+class FlatArrayCheckNode;\n@@ -122,0 +123,1 @@\n+class MachPrologNode;\n@@ -128,0 +130,1 @@\n+class MachVEPNode;\n@@ -186,0 +189,3 @@\n+class InlineTypeNode;\n+class LoadFlatNode;\n+class StoreFlatNode;\n@@ -692,0 +698,2 @@\n+        DEFINE_CLASS_ID(LoadFlat,  SafePoint, 1)\n+        DEFINE_CLASS_ID(StoreFlat, SafePoint, 2)\n@@ -708,0 +716,1 @@\n+        DEFINE_CLASS_ID(Blackhole,        MemBar, 2)\n@@ -729,0 +738,2 @@\n+      DEFINE_CLASS_ID(MachProlog,       Mach, 8)\n+      DEFINE_CLASS_ID(MachVEP,          Mach, 9)\n@@ -761,1 +772,2 @@\n-      DEFINE_CLASS_ID(Con, Type, 8)\n+      DEFINE_CLASS_ID(InlineType, Type, 8)\n+      DEFINE_CLASS_ID(Con, Type, 9)\n@@ -763,2 +775,2 @@\n-      DEFINE_CLASS_ID(SafePointScalarMerge, Type, 9)\n-      DEFINE_CLASS_ID(Convert, Type, 10)\n+      DEFINE_CLASS_ID(SafePointScalarMerge, Type, 10)\n+      DEFINE_CLASS_ID(Convert, Type, 11)\n@@ -803,3 +815,4 @@\n-        DEFINE_CLASS_ID(FastLock,   Cmp, 0)\n-        DEFINE_CLASS_ID(FastUnlock, Cmp, 1)\n-        DEFINE_CLASS_ID(SubTypeCheck,Cmp, 2)\n+        DEFINE_CLASS_ID(FastLock,       Cmp, 0)\n+        DEFINE_CLASS_ID(FastUnlock,     Cmp, 1)\n+        DEFINE_CLASS_ID(SubTypeCheck,   Cmp, 2)\n+        DEFINE_CLASS_ID(FlatArrayCheck, Cmp, 3)\n@@ -914,0 +927,1 @@\n+  DEFINE_CLASS_QUERY(Blackhole)\n@@ -947,0 +961,1 @@\n+  DEFINE_CLASS_QUERY(FlatArrayCheck)\n@@ -979,0 +994,1 @@\n+  DEFINE_CLASS_QUERY(MachProlog)\n@@ -985,0 +1001,1 @@\n+  DEFINE_CLASS_QUERY(MachVEP)\n@@ -1023,0 +1040,3 @@\n+  DEFINE_CLASS_QUERY(InlineType)\n+  DEFINE_CLASS_QUERY(LoadFlat)\n+  DEFINE_CLASS_QUERY(StoreFlat)\n","filename":"src\/hotspot\/share\/opto\/node.hpp","additions":26,"deletions":6,"binary":false,"changes":32,"status":"modified"},{"patch":"@@ -36,0 +36,1 @@\n+#include \"gc\/shared\/gc_globals.hpp\"\n@@ -232,1 +233,9 @@\n-    _orig_pc_slot = C->fixed_slots() - (sizeof(address) \/ VMRegImpl::stack_slot_size);\n+    int fixed_slots = C->fixed_slots();\n+    if (C->needs_stack_repair()) {\n+      fixed_slots -= 2;\n+    }\n+    \/\/ TODO 8284443 Only reserve extra slot if needed\n+    if (InlineTypeReturnedAsFields) {\n+      fixed_slots -= 2;\n+    }\n+    _orig_pc_slot = fixed_slots - (sizeof(address) \/ VMRegImpl::stack_slot_size);\n@@ -273,1 +282,2 @@\n-  MachPrologNode *prolog = new MachPrologNode();\n+  Label verified_entry;\n+  MachPrologNode* prolog = new MachPrologNode(&verified_entry);\n@@ -279,3 +289,2 @@\n-\n-  if( C->is_osr_compilation() ) {\n-    if( PoisonOSREntry ) {\n+  if (C->is_osr_compilation()) {\n+    if (PoisonOSREntry) {\n@@ -286,3 +295,14 @@\n-    if( C->method() && !C->method()->flags().is_static() ) {\n-      \/\/ Insert unvalidated entry point\n-      C->cfg()->insert( broot, 0, new MachUEPNode() );\n+    if (C->method()) {\n+      if (C->method()->has_scalarized_args()) {\n+        \/\/ Add entry point to unpack all inline type arguments\n+        C->cfg()->insert(broot, 0, new MachVEPNode(&verified_entry, \/* verified *\/ true, \/* receiver_only *\/ false));\n+        if (!C->method()->is_static()) {\n+          \/\/ Add verified\/unverified entry points to only unpack inline type receiver at interface calls\n+          C->cfg()->insert(broot, 0, new MachVEPNode(&verified_entry, \/* verified *\/ false, \/* receiver_only *\/ false));\n+          C->cfg()->insert(broot, 0, new MachVEPNode(&verified_entry, \/* verified *\/ true,  \/* receiver_only *\/ true));\n+          C->cfg()->insert(broot, 0, new MachVEPNode(&verified_entry, \/* verified *\/ false, \/* receiver_only *\/ true));\n+        }\n+      } else if (!C->method()->is_static()) {\n+        \/\/ Insert unvalidated entry point\n+        C->cfg()->insert(broot, 0, new MachUEPNode());\n+      }\n@@ -290,1 +310,0 @@\n-\n@@ -330,0 +349,25 @@\n+  if (!C->is_osr_compilation() && C->has_scalarized_args()) {\n+    \/\/ Compute the offsets of the entry points required by the inline type calling convention\n+    if (!C->method()->is_static()) {\n+      \/\/ We have entries at the beginning of the method, implemented by the first 4 nodes.\n+      \/\/ Entry                     (unverified) @ offset 0\n+      \/\/ Verified_Inline_Entry_RO\n+      \/\/ Inline_Entry              (unverified)\n+      \/\/ Verified_Inline_Entry\n+      uint offset = 0;\n+      _code_offsets.set_value(CodeOffsets::Entry, offset);\n+\n+      offset += ((MachVEPNode*)broot->get_node(0))->size(C->regalloc());\n+      _code_offsets.set_value(CodeOffsets::Verified_Inline_Entry_RO, offset);\n+\n+      offset += ((MachVEPNode*)broot->get_node(1))->size(C->regalloc());\n+      _code_offsets.set_value(CodeOffsets::Inline_Entry, offset);\n+\n+      offset += ((MachVEPNode*)broot->get_node(2))->size(C->regalloc());\n+      _code_offsets.set_value(CodeOffsets::Verified_Inline_Entry, offset);\n+    } else {\n+      _code_offsets.set_value(CodeOffsets::Entry, -1); \/\/ will be patched later\n+      _code_offsets.set_value(CodeOffsets::Verified_Inline_Entry, 0);\n+    }\n+  }\n+\n@@ -491,1 +535,3 @@\n-          mcall->method_set((intptr_t)mcall->entry_point());\n+          if (mcall->entry_point() != nullptr) {\n+            mcall->method_set((intptr_t)mcall->entry_point());\n+          }\n@@ -746,0 +792,29 @@\n+      uint first_ind = spobj->first_index(sfpt->jvms());\n+      \/\/ Nullable, scalarized inline types have a null_marker input\n+      \/\/ that needs to be checked before using the field values.\n+      ScopeValue* properties = nullptr;\n+      if (cik->is_inlinetype()) {\n+        Node* null_marker_node = sfpt->in(first_ind++);\n+        assert(null_marker_node != nullptr, \"null_marker node not found\");\n+        if (!null_marker_node->is_top()) {\n+          const TypeInt* null_marker_type = null_marker_node->bottom_type()->is_int();\n+          if (null_marker_node->is_Con()) {\n+            properties = new ConstantIntValue(null_marker_type->get_con());\n+          } else {\n+            OptoReg::Name null_marker_reg = C->regalloc()->get_reg_first(null_marker_node);\n+            properties = new_loc_value(C->regalloc(), null_marker_reg, Location::normal);\n+          }\n+        }\n+      }\n+      if (cik->is_array_klass() && !cik->is_type_array_klass()) {\n+        jint props = ArrayKlass::ArrayProperties::DEFAULT;\n+        if (cik->as_array_klass()->element_klass()->is_inlinetype()) {\n+          if (cik->as_array_klass()->is_elem_null_free()) {\n+            props |= ArrayKlass::ArrayProperties::NULL_RESTRICTED;\n+          }\n+          if (!cik->as_array_klass()->is_elem_atomic()) {\n+            props |= ArrayKlass::ArrayProperties::NON_ATOMIC;\n+          }\n+        }\n+        properties = new ConstantIntValue(props);\n+      }\n@@ -747,1 +822,1 @@\n-                           new ConstantOopWriteValue(cik->java_mirror()->constant_encoding()));\n+                           new ConstantOopWriteValue(cik->java_mirror()->constant_encoding()), true, properties);\n@@ -750,1 +825,0 @@\n-      uint first_ind = spobj->first_index(sfpt->jvms());\n@@ -996,0 +1070,1 @@\n+  bool return_scalarized = false;\n@@ -1011,1 +1086,1 @@\n-    if (mcall->returns_pointer()) {\n+    if (mcall->returns_pointer() || mcall->returns_scalarized()) {\n@@ -1014,0 +1089,3 @@\n+    if (mcall->returns_scalarized()) {\n+      return_scalarized = true;\n+    }\n@@ -1082,0 +1160,14 @@\n+          assert(!cik->is_inlinetype(), \"Synchronization on value object?\");\n+          ScopeValue* properties = nullptr;\n+          if (cik->is_array_klass() && !cik->is_type_array_klass()) {\n+            jint props = ArrayKlass::ArrayProperties::DEFAULT;\n+            if (cik->as_array_klass()->element_klass()->is_inlinetype()) {\n+              if (cik->as_array_klass()->is_elem_null_free()) {\n+                props |= ArrayKlass::ArrayProperties::NULL_RESTRICTED;\n+              }\n+              if (!cik->as_array_klass()->is_elem_atomic()) {\n+                props |= ArrayKlass::ArrayProperties::NON_ATOMIC;\n+              }\n+            }\n+            properties = new ConstantIntValue(props);\n+          }\n@@ -1083,1 +1175,1 @@\n-                                            new ConstantOopWriteValue(cik->java_mirror()->constant_encoding()));\n+                                            new ConstantOopWriteValue(cik->java_mirror()->constant_encoding()), true, properties);\n@@ -1190,0 +1282,1 @@\n+      return_scalarized,\n@@ -1543,2 +1636,4 @@\n-          \/\/ This destination address is NOT PC-relative\n-          mcall->method_set((intptr_t)mcall->entry_point());\n+          if (mcall->entry_point() != nullptr) {\n+            \/\/ This destination address is NOT PC-relative\n+            mcall->method_set((intptr_t)mcall->entry_point());\n+          }\n@@ -1682,1 +1777,0 @@\n-\n@@ -2937,0 +3031,13 @@\n+\n+      \/\/ Do not allow a CheckCastPP node whose input is a raw pointer to\n+      \/\/ float past a safepoint.  This can occur when a buffered inline\n+      \/\/ type is allocated in a loop and the CheckCastPP from that\n+      \/\/ allocation is reused outside the loop.  If the use inside the\n+      \/\/ loop is scalarized the CheckCastPP will no longer be connected\n+      \/\/ to the loop safepoint.  See JDK-8264340.\n+      if (m->is_Mach() && m->as_Mach()->ideal_Opcode() == Op_CheckCastPP) {\n+        Node *def = m->in(1);\n+        if (def != nullptr && def->bottom_type()->base() == Type::RawPtr) {\n+          last_safept_node->add_prec(m);\n+        }\n+      }\n@@ -3085,0 +3192,19 @@\n+    if (C->has_scalarized_args()) {\n+      \/\/ Inline type entry points (MachVEPNodes) require lots of space for GC barriers and oop verification\n+      \/\/ when loading object fields from the buffered argument. Increase scratch buffer size accordingly.\n+      ciMethod* method = C->method();\n+      int barrier_size = UseZGC ? 200 : (7 DEBUG_ONLY(+ 37));\n+      int arg_num = 0;\n+      if (!method->is_static()) {\n+        if (method->is_scalarized_arg(arg_num)) {\n+          size += method->holder()->as_inline_klass()->oop_count() * barrier_size;\n+        }\n+        arg_num++;\n+      }\n+      for (ciSignatureStream str(method->signature()); !str.at_return_type(); str.next()) {\n+        if (method->is_scalarized_arg(arg_num)) {\n+          size += str.type()->as_inline_klass()->oop_count() * barrier_size;\n+        }\n+        arg_num++;\n+      }\n+    }\n@@ -3155,1 +3281,2 @@\n-  if (is_branch) \/\/ Restore label.\n+  \/\/ Restore label.\n+  if (is_branch) {\n@@ -3157,0 +3284,1 @@\n+  }\n@@ -3197,6 +3325,9 @@\n-      if (!target->is_static()) {\n-        \/\/ The UEP of an nmethod ensures that the VEP is padded. However, the padding of the UEP is placed\n-        \/\/ before the inline cache check, so we don't have to execute any nop instructions when dispatching\n-        \/\/ through the UEP, yet we can ensure that the VEP is aligned appropriately.\n-        _code_offsets.set_value(CodeOffsets::Entry, _first_block_size - MacroAssembler::ic_check_size());\n-      }\n+      if (_code_offsets.value(CodeOffsets::Verified_Inline_Entry) == -1) {\n+        _code_offsets.set_value(CodeOffsets::Verified_Inline_Entry, _first_block_size);\n+      }\n+      if (_code_offsets.value(CodeOffsets::Verified_Inline_Entry_RO) == -1) {\n+        _code_offsets.set_value(CodeOffsets::Verified_Inline_Entry_RO, _first_block_size);\n+      }\n+      if (_code_offsets.value(CodeOffsets::Entry) == -1) {\n+        _code_offsets.set_value(CodeOffsets::Entry, _first_block_size);\n+      }\n@@ -3208,14 +3339,14 @@\n-                                     entry_bci,\n-                                     &_code_offsets,\n-                                     _orig_pc_slot_offset_in_bytes,\n-                                     code_buffer(),\n-                                     frame_size_in_words(),\n-                                     oop_map_set(),\n-                                     &_handler_table,\n-                                     inc_table(),\n-                                     compiler,\n-                                     has_unsafe_access,\n-                                     SharedRuntime::is_wide_vector(C->max_vector_size()),\n-                                     C->has_monitors(),\n-                                     C->has_scoped_access(),\n-                                     0);\n+                              entry_bci,\n+                              &_code_offsets,\n+                              _orig_pc_slot_offset_in_bytes,\n+                              code_buffer(),\n+                              frame_size_in_words(),\n+                              _oop_map_set,\n+                              &_handler_table,\n+                              inc_table(),\n+                              compiler,\n+                              has_unsafe_access,\n+                              SharedRuntime::is_wide_vector(C->max_vector_size()),\n+                              C->has_monitors(),\n+                              C->has_scoped_access(),\n+                              0);\n","filename":"src\/hotspot\/share\/opto\/output.cpp","additions":169,"deletions":38,"binary":false,"changes":207,"status":"modified"},{"patch":"@@ -1184,1 +1184,1 @@\n-  n->dump_bfs(1, nullptr, \"\", &ss);\n+  n->dump_bfs(3, nullptr, \"\", &ss);\n@@ -2077,6 +2077,0 @@\n-  if (_delay_transform) {\n-    \/\/ Register the node but don't optimize for now\n-    register_new_node_with_optimizer(n);\n-    return n;\n-  }\n-\n@@ -2089,0 +2083,6 @@\n+  if (_delay_transform) {\n+    \/\/ Add the node to the worklist but don't optimize for now\n+    _worklist.push(n);\n+    return n;\n+  }\n+\n@@ -2361,0 +2361,13 @@\n+void PhaseIterGVN::replace_in_uses(Node* n, Node* m) {\n+  assert(n != nullptr, \"sanity\");\n+  for (DUIterator_Fast imax, i = n->fast_outs(imax); i < imax; i++) {\n+    Node* u = n->fast_out(i);\n+    if (u != n) {\n+      rehash_node_delayed(u);\n+      int nb = u->replace_edge(n, m);\n+      --i, imax -= nb;\n+    }\n+  }\n+  assert(n->outcnt() == 0, \"all uses must be deleted\");\n+}\n+\n@@ -2416,0 +2429,10 @@\n+  \/\/ AndLNode::Ideal folds GraphKit::mark_word_test patterns. Give it a chance to run.\n+  if (n->is_Load() && use->is_Phi()) {\n+    for (DUIterator_Fast imax, i = use->fast_outs(imax); i < imax; i++) {\n+      Node* u = use->fast_out(i);\n+      if (u->Opcode() == Op_AndL) {\n+        worklist.push(u);\n+      }\n+    }\n+  }\n+\n@@ -2513,0 +2536,9 @@\n+  \/\/ Inline type nodes can have other inline types as users. If an input gets\n+  \/\/ updated, make sure that inline type users get a chance for optimization.\n+  if (use->is_InlineType()) {\n+    for (DUIterator_Fast i2max, i2 = use->fast_outs(i2max); i2 < i2max; i2++) {\n+      Node* u = use->fast_out(i2);\n+      if (u->is_InlineType())\n+        worklist.push(u);\n+    }\n+  }\n@@ -2639,0 +2671,8 @@\n+  if (use_op == Op_CastP2X) {\n+    for (DUIterator_Fast i2max, i2 = use->fast_outs(i2max); i2 < i2max; i2++) {\n+      Node* u = use->fast_out(i2);\n+      if (u->Opcode() == Op_AndX) {\n+        worklist.push(u);\n+      }\n+    }\n+  }\n@@ -2657,0 +2697,10 @@\n+  \/\/ Give CallStaticJavaNode::remove_useless_allocation a chance to run\n+  if (use->is_Region()) {\n+    Node* c = use;\n+    do {\n+      c = c->unique_ctrl_out_or_null();\n+    } while (c != nullptr && c->is_Region());\n+    if (c != nullptr && c->is_CallStaticJava() && c->as_CallStaticJava()->uncommon_trap_request() != 0) {\n+      worklist.push(c);\n+    }\n+  }\n@@ -2776,1 +2826,1 @@\n-    n->dump(1);\n+    n->dump(3);\n@@ -2940,0 +2990,1 @@\n+  push_cast(worklist, use);\n@@ -3051,0 +3102,12 @@\n+  }\n+}\n+\n+void PhaseCCP::push_cast(Unique_Node_List& worklist, const Node* use) {\n+  uint use_op = use->Opcode();\n+  if (use_op == Op_CastP2X) {\n+    for (DUIterator_Fast i2max, i2 = use->fast_outs(i2max); i2 < i2max; i2++) {\n+      Node* u = use->fast_out(i2);\n+      if (u->Opcode() == Op_AndX) {\n+        worklist.push(u);\n+      }\n+    }\n","filename":"src\/hotspot\/share\/opto\/phaseX.cpp","additions":71,"deletions":8,"binary":false,"changes":79,"status":"modified"},{"patch":"@@ -482,1 +482,1 @@\n-  virtual void record_for_igvn(Node *n) { }\n+  virtual void record_for_igvn(Node *n) { _worklist.push(n); }\n@@ -557,0 +557,2 @@\n+  void replace_in_uses(Node* n, Node* m);\n+\n@@ -648,0 +650,1 @@\n+  static void push_cast(Unique_Node_List& worklist, const Node* use);\n","filename":"src\/hotspot\/share\/opto\/phaseX.hpp","additions":4,"deletions":1,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -25,0 +25,3 @@\n+#include \"ci\/ciField.hpp\"\n+#include \"ci\/ciFlatArrayKlass.hpp\"\n+#include \"ci\/ciInlineKlass.hpp\"\n@@ -49,0 +52,1 @@\n+#include \"utilities\/globalDefinitions.hpp\"\n@@ -59,0 +63,45 @@\n+const Type::Offset Type::Offset::top(Type::OffsetTop);\n+const Type::Offset Type::Offset::bottom(Type::OffsetBot);\n+\n+const Type::Offset Type::Offset::meet(const Type::Offset other) const {\n+  \/\/ Either is 'TOP' offset?  Return the other offset!\n+  if (_offset == OffsetTop) return other;\n+  if (other._offset == OffsetTop) return *this;\n+  \/\/ If either is different, return 'BOTTOM' offset\n+  if (_offset != other._offset) return bottom;\n+  return Offset(_offset);\n+}\n+\n+const Type::Offset Type::Offset::dual() const {\n+  if (_offset == OffsetTop) return bottom;\/\/ Map 'TOP' into 'BOTTOM'\n+  if (_offset == OffsetBot) return top;\/\/ Map 'BOTTOM' into 'TOP'\n+  return Offset(_offset);               \/\/ Map everything else into self\n+}\n+\n+const Type::Offset Type::Offset::add(intptr_t offset) const {\n+  \/\/ Adding to 'TOP' offset?  Return 'TOP'!\n+  if (_offset == OffsetTop || offset == OffsetTop) return top;\n+  \/\/ Adding to 'BOTTOM' offset?  Return 'BOTTOM'!\n+  if (_offset == OffsetBot || offset == OffsetBot) return bottom;\n+  \/\/ Addition overflows or \"accidentally\" equals to OffsetTop? Return 'BOTTOM'!\n+  offset += (intptr_t)_offset;\n+  if (offset != (int)offset || offset == OffsetTop) return bottom;\n+\n+  \/\/ assert( _offset >= 0 && _offset+offset >= 0, \"\" );\n+  \/\/ It is possible to construct a negative offset during PhaseCCP\n+\n+  return Offset((int)offset);        \/\/ Sum valid offsets\n+}\n+\n+void Type::Offset::dump2(outputStream *st) const {\n+  if (_offset == 0) {\n+    return;\n+  } else if (_offset == OffsetTop) {\n+    st->print(\"+top\");\n+  }\n+  else if (_offset == OffsetBot) {\n+    st->print(\"+bot\");\n+  } else if (_offset) {\n+    st->print(\"+%d\", _offset);\n+  }\n+}\n@@ -237,0 +286,3 @@\n+  case T_OBJECT:\n+    return Type::get_const_type(type->unwrap())->join_speculative(type->is_null_free() ? TypePtr::NOTNULL : TypePtr::BOTTOM);\n+\n@@ -559,3 +611,3 @@\n-  TypePtr::NULL_PTR= TypePtr::make(AnyPtr, TypePtr::Null, 0);\n-  TypePtr::NOTNULL = TypePtr::make(AnyPtr, TypePtr::NotNull, OffsetBot);\n-  TypePtr::BOTTOM  = TypePtr::make(AnyPtr, TypePtr::BotPTR, OffsetBot);\n+  TypePtr::NULL_PTR= TypePtr::make(AnyPtr, TypePtr::Null, Offset(0));\n+  TypePtr::NOTNULL = TypePtr::make(AnyPtr, TypePtr::NotNull, Offset::bottom);\n+  TypePtr::BOTTOM  = TypePtr::make(AnyPtr, TypePtr::BotPTR, Offset::bottom);\n@@ -578,1 +630,1 @@\n-                                           false, nullptr, oopDesc::mark_offset_in_bytes());\n+                                           false, nullptr, Offset(oopDesc::mark_offset_in_bytes()));\n@@ -580,2 +632,2 @@\n-                                           false, nullptr, oopDesc::klass_offset_in_bytes());\n-  TypeOopPtr::BOTTOM  = TypeOopPtr::make(TypePtr::BotPTR, OffsetBot, TypeOopPtr::InstanceBot);\n+                                           false, nullptr, Offset(oopDesc::klass_offset_in_bytes()));\n+  TypeOopPtr::BOTTOM  = TypeOopPtr::make(TypePtr::BotPTR, Offset::bottom, TypeOopPtr::InstanceBot);\n@@ -583,1 +635,1 @@\n-  TypeMetadataPtr::BOTTOM = TypeMetadataPtr::make(TypePtr::BotPTR, nullptr, OffsetBot);\n+  TypeMetadataPtr::BOTTOM = TypeMetadataPtr::make(TypePtr::BotPTR, nullptr, Offset::bottom);\n@@ -606,2 +658,2 @@\n-  TypeAryPtr::BOTTOM = TypeAryPtr::make(TypePtr::BotPTR, TypeAry::make(Type::BOTTOM, TypeInt::POS), nullptr, false, Type::OffsetBot);\n-  TypeAryPtr::RANGE   = TypeAryPtr::make( TypePtr::BotPTR, TypeAry::make(Type::BOTTOM,TypeInt::POS), nullptr \/* current->env()->Object_klass() *\/, false, arrayOopDesc::length_offset_in_bytes());\n+  TypeAryPtr::BOTTOM = TypeAryPtr::make(TypePtr::BotPTR, TypeAry::make(Type::BOTTOM, TypeInt::POS), nullptr, false, Offset::bottom);\n+  TypeAryPtr::RANGE   = TypeAryPtr::make(TypePtr::BotPTR, TypeAry::make(Type::BOTTOM,TypeInt::POS), nullptr \/* current->env()->Object_klass() *\/, false, Offset(arrayOopDesc::length_offset_in_bytes()));\n@@ -609,1 +661,1 @@\n-  TypeAryPtr::NARROWOOPS = TypeAryPtr::make(TypePtr::BotPTR, TypeAry::make(TypeNarrowOop::BOTTOM, TypeInt::POS), nullptr \/*ciArrayKlass::make(o)*\/,  false,  Type::OffsetBot);\n+  TypeAryPtr::NARROWOOPS = TypeAryPtr::make(TypePtr::BotPTR, TypeAry::make(TypeNarrowOop::BOTTOM, TypeInt::POS), nullptr \/*ciArrayKlass::make(o)*\/,  false,  Offset::bottom);\n@@ -619,1 +671,1 @@\n-    TypeAryPtr::OOPS  = TypeAryPtr::make(TypePtr::BotPTR, TypeAry::make(TypeInstPtr::BOTTOM,TypeInt::POS), nullptr \/*ciArrayKlass::make(o)*\/,  false,  Type::OffsetBot);\n+    TypeAryPtr::OOPS  = TypeAryPtr::make(TypePtr::BotPTR, TypeAry::make(TypeInstPtr::BOTTOM,TypeInt::POS), nullptr \/*ciArrayKlass::make(o)*\/,  false,  Offset::bottom);\n@@ -621,7 +673,8 @@\n-  TypeAryPtr::BYTES   = TypeAryPtr::make(TypePtr::BotPTR, TypeAry::make(TypeInt::BYTE      ,TypeInt::POS), ciTypeArrayKlass::make(T_BYTE),   true,  Type::OffsetBot);\n-  TypeAryPtr::SHORTS  = TypeAryPtr::make(TypePtr::BotPTR, TypeAry::make(TypeInt::SHORT     ,TypeInt::POS), ciTypeArrayKlass::make(T_SHORT),  true,  Type::OffsetBot);\n-  TypeAryPtr::CHARS   = TypeAryPtr::make(TypePtr::BotPTR, TypeAry::make(TypeInt::CHAR      ,TypeInt::POS), ciTypeArrayKlass::make(T_CHAR),   true,  Type::OffsetBot);\n-  TypeAryPtr::INTS    = TypeAryPtr::make(TypePtr::BotPTR, TypeAry::make(TypeInt::INT       ,TypeInt::POS), ciTypeArrayKlass::make(T_INT),    true,  Type::OffsetBot);\n-  TypeAryPtr::LONGS   = TypeAryPtr::make(TypePtr::BotPTR, TypeAry::make(TypeLong::LONG     ,TypeInt::POS), ciTypeArrayKlass::make(T_LONG),   true,  Type::OffsetBot);\n-  TypeAryPtr::FLOATS  = TypeAryPtr::make(TypePtr::BotPTR, TypeAry::make(Type::FLOAT        ,TypeInt::POS), ciTypeArrayKlass::make(T_FLOAT),  true,  Type::OffsetBot);\n-  TypeAryPtr::DOUBLES = TypeAryPtr::make(TypePtr::BotPTR, TypeAry::make(Type::DOUBLE       ,TypeInt::POS), ciTypeArrayKlass::make(T_DOUBLE), true,  Type::OffsetBot);\n+  TypeAryPtr::BYTES   = TypeAryPtr::make(TypePtr::BotPTR, TypeAry::make(TypeInt::BYTE      ,TypeInt::POS), ciTypeArrayKlass::make(T_BYTE),   true,  Offset::bottom);\n+  TypeAryPtr::SHORTS  = TypeAryPtr::make(TypePtr::BotPTR, TypeAry::make(TypeInt::SHORT     ,TypeInt::POS), ciTypeArrayKlass::make(T_SHORT),  true,  Offset::bottom);\n+  TypeAryPtr::CHARS   = TypeAryPtr::make(TypePtr::BotPTR, TypeAry::make(TypeInt::CHAR      ,TypeInt::POS), ciTypeArrayKlass::make(T_CHAR),   true,  Offset::bottom);\n+  TypeAryPtr::INTS    = TypeAryPtr::make(TypePtr::BotPTR, TypeAry::make(TypeInt::INT       ,TypeInt::POS), ciTypeArrayKlass::make(T_INT),    true,  Offset::bottom);\n+  TypeAryPtr::LONGS   = TypeAryPtr::make(TypePtr::BotPTR, TypeAry::make(TypeLong::LONG     ,TypeInt::POS), ciTypeArrayKlass::make(T_LONG),   true,  Offset::bottom);\n+  TypeAryPtr::FLOATS  = TypeAryPtr::make(TypePtr::BotPTR, TypeAry::make(Type::FLOAT        ,TypeInt::POS), ciTypeArrayKlass::make(T_FLOAT),  true,  Offset::bottom);\n+  TypeAryPtr::DOUBLES = TypeAryPtr::make(TypePtr::BotPTR, TypeAry::make(Type::DOUBLE       ,TypeInt::POS), ciTypeArrayKlass::make(T_DOUBLE), true,  Offset::bottom);\n+  TypeAryPtr::INLINES = TypeAryPtr::make(TypePtr::BotPTR, TypeAry::make(TypeInstPtr::BOTTOM,TypeInt::POS, \/* stable= *\/ false, \/* flat= *\/ true), nullptr, false, Offset::bottom);\n@@ -632,0 +685,1 @@\n+  TypeAryPtr::_array_body_type[T_FLAT_ELEMENT] = TypeAryPtr::OOPS;\n@@ -642,2 +696,2 @@\n-  TypeInstKlassPtr::OBJECT = TypeInstKlassPtr::make(TypePtr::NotNull, current->env()->Object_klass(), 0);\n-  TypeInstKlassPtr::OBJECT_OR_NULL = TypeInstKlassPtr::make(TypePtr::BotPTR, current->env()->Object_klass(), 0);\n+  TypeInstKlassPtr::OBJECT = TypeInstKlassPtr::make(TypePtr::NotNull, current->env()->Object_klass(), Offset(0));\n+  TypeInstKlassPtr::OBJECT_OR_NULL = TypeInstKlassPtr::make(TypePtr::BotPTR, current->env()->Object_klass(), Offset(0));\n@@ -682,0 +736,1 @@\n+  _const_basic_type[T_FLAT_ELEMENT] = TypeInstPtr::BOTTOM;\n@@ -698,0 +753,1 @@\n+  _zero_type[T_FLAT_ELEMENT] = TypePtr::NULL_PTR;\n@@ -972,0 +1028,3 @@\n+\n+  \/\/ Verify that:\n+  \/\/      this meet t == t meet this\n@@ -988,0 +1047,9 @@\n+  \/\/ Verify that:\n+  \/\/ 1)     mt_dual meet t_dual    == t_dual\n+  \/\/    which corresponds to\n+  \/\/       !(t meet this)  meet !t ==\n+  \/\/       (!t join !this) meet !t == !t\n+  \/\/ 2)    mt_dual meet this_dual     == this_dual\n+  \/\/    which corresponds to\n+  \/\/       !(t meet this)  meet !this ==\n+  \/\/       (!t join !this) meet !this == !this\n@@ -998,0 +1066,1 @@\n+    \/\/ 1)\n@@ -999,0 +1068,1 @@\n+    \/\/ 2)\n@@ -1000,0 +1070,9 @@\n+    tty->cr();\n+    tty->print_cr(\"Fail: \");\n+    if (t2t != t->_dual) {\n+      tty->print_cr(\"- mt_dual meet t_dual != t_dual\");\n+    }\n+    if (t2this != this->_dual) {\n+      tty->print_cr(\"- mt_dual meet this_dual != this_dual\");\n+    }\n+    tty->cr();\n@@ -1036,0 +1115,3 @@\n+  \/\/ TODO 8350865 This currently triggers a verification failure, the code around \"\/\/ Even though MyValue is final\" needs adjustments\n+  if ((this_t->isa_ptr() && this_t->is_ptr()->is_not_flat()) ||\n+      (this_t->_dual->isa_ptr() && this_t->_dual->is_ptr()->is_not_flat())) return mt;\n@@ -2052,0 +2134,21 @@\n+static void collect_inline_fields(ciInlineKlass* vk, const Type** field_array, uint& pos) {\n+  for (int i = 0; i < vk->nof_declared_nonstatic_fields(); i++) {\n+    ciField* field = vk->declared_nonstatic_field_at(i);\n+    if (field->is_flat()) {\n+      collect_inline_fields(field->type()->as_inline_klass(), field_array, pos);\n+      if (!field->is_null_free()) {\n+        \/\/ Use T_INT instead of T_BOOLEAN here because the upper bits can contain garbage if the holder\n+        \/\/ is null and C2 will only zero them for T_INT assuming that T_BOOLEAN is already canonicalized.\n+        field_array[pos++] = Type::get_const_basic_type(T_INT);\n+      }\n+    } else {\n+      BasicType bt = field->type()->basic_type();\n+      const Type* ft = Type::get_const_type(field->type());\n+      field_array[pos++] = ft;\n+      if (type2size[bt] == 2) {\n+        field_array[pos++] = Type::HALF;\n+      }\n+    }\n+  }\n+}\n+\n@@ -2054,1 +2157,1 @@\n-const TypeTuple *TypeTuple::make_range(ciSignature* sig, InterfaceHandling interface_handling) {\n+const TypeTuple *TypeTuple::make_range(ciSignature* sig, InterfaceHandling interface_handling, bool ret_vt_fields) {\n@@ -2057,0 +2160,5 @@\n+  if (ret_vt_fields) {\n+    arg_cnt = return_type->as_inline_klass()->inline_arg_slots() + 1;\n+    \/\/ InlineTypeNode::NullMarker field used for null checking\n+    arg_cnt++;\n+  }\n@@ -2068,0 +2176,12 @@\n+    if (return_type->is_inlinetype() && ret_vt_fields) {\n+      uint pos = TypeFunc::Parms;\n+      field_array[pos++] = get_const_type(return_type); \/\/ Oop might be null when returning as fields\n+      collect_inline_fields(return_type->as_inline_klass(), field_array, pos);\n+      \/\/ InlineTypeNode::NullMarker field used for null checking\n+      field_array[pos++] = get_const_basic_type(T_BOOLEAN);\n+      assert(pos == (TypeFunc::Parms + arg_cnt), \"out of bounds\");\n+      break;\n+    } else {\n+      field_array[TypeFunc::Parms] = get_const_type(return_type, interface_handling)->join_speculative(TypePtr::BOTTOM);\n+    }\n+    break;\n@@ -2086,2 +2206,10 @@\n-const TypeTuple *TypeTuple::make_domain(ciInstanceKlass* recv, ciSignature* sig, InterfaceHandling interface_handling) {\n-  uint arg_cnt = sig->size();\n+const TypeTuple *TypeTuple::make_domain(ciMethod* method, InterfaceHandling interface_handling, bool vt_fields_as_args) {\n+  ciSignature* sig = method->signature();\n+  uint arg_cnt = sig->size() + (method->is_static() ? 0 : 1);\n+  if (vt_fields_as_args) {\n+    arg_cnt = 0;\n+    assert(method->get_sig_cc() != nullptr, \"Should have scalarized signature\");\n+    for (ExtendedSignature sig_cc = ExtendedSignature(method->get_sig_cc(), SigEntryFilter()); !sig_cc.at_end(); ++sig_cc) {\n+      arg_cnt += type2size[(*sig_cc)._bt];\n+    }\n+  }\n@@ -2090,8 +2218,8 @@\n-  const Type **field_array;\n-  if (recv != nullptr) {\n-    arg_cnt++;\n-    field_array = fields(arg_cnt);\n-    \/\/ Use get_const_type here because it respects UseUniqueSubclasses:\n-    field_array[pos++] = get_const_type(recv, interface_handling)->join_speculative(TypePtr::NOTNULL);\n-  } else {\n-    field_array = fields(arg_cnt);\n+  const Type** field_array = fields(arg_cnt);\n+  if (!method->is_static()) {\n+    ciInstanceKlass* recv = method->holder();\n+    if (vt_fields_as_args && recv->is_inlinetype() && recv->as_inline_klass()->can_be_passed_as_fields() && method->is_scalarized_arg(0)) {\n+      collect_inline_fields(recv->as_inline_klass(), field_array, pos);\n+    } else {\n+      field_array[pos++] = get_const_type(recv, interface_handling)->join_speculative(TypePtr::NOTNULL);\n+    }\n@@ -2103,0 +2231,1 @@\n+    BasicType bt = type->basic_type();\n@@ -2104,1 +2233,1 @@\n-    switch (type->basic_type()) {\n+    switch (bt) {\n@@ -2114,0 +2243,8 @@\n+      if (type->is_inlinetype() && vt_fields_as_args && method->is_scalarized_arg(i + (method->is_static() ? 0 : 1))) {\n+        \/\/ InlineTypeNode::NullMarker field used for null checking\n+        field_array[pos++] = get_const_basic_type(T_BOOLEAN);\n+        collect_inline_fields(type->as_inline_klass(), field_array, pos);\n+      } else {\n+        field_array[pos++] = get_const_type(type, interface_handling);\n+      }\n+      break;\n@@ -2130,0 +2267,1 @@\n+  assert(pos == TypeFunc::Parms + arg_cnt, \"wrong number of arguments\");\n@@ -2264,1 +2402,2 @@\n-const TypeAry* TypeAry::make(const Type* elem, const TypeInt* size, bool stable) {\n+const TypeAry* TypeAry::make(const Type* elem, const TypeInt* size, bool stable,\n+                             bool flat, bool not_flat, bool not_null_free, bool atomic) {\n@@ -2269,1 +2408,1 @@\n-  return (TypeAry*)(new TypeAry(elem,size,stable))->hashcons();\n+  return (TypeAry*)(new TypeAry(elem, size, stable, flat, not_flat, not_null_free, atomic))->hashcons();\n@@ -2296,1 +2435,5 @@\n-                         isize, _stable && a->_stable);\n+                         isize, _stable && a->_stable,\n+                         _flat && a->_flat,\n+                         _not_flat && a->_not_flat,\n+                         _not_null_free && a->_not_null_free,\n+                         _atomic && a->_atomic);\n@@ -2309,1 +2452,1 @@\n-  return new TypeAry(_elem->dual(), size_dual, !_stable);\n+  return new TypeAry(_elem->dual(), size_dual, !_stable, !_flat, !_not_flat, !_not_null_free, !_atomic);\n@@ -2318,1 +2461,6 @@\n-    _size == a->_size;\n+    _size == a->_size &&\n+    _flat == a->_flat &&\n+    _not_flat == a->_not_flat &&\n+    _not_null_free == a->_not_null_free &&\n+    _atomic == a->_atomic;\n+\n@@ -2324,1 +2472,2 @@\n-  return (uint)(uintptr_t)_elem + (uint)(uintptr_t)_size + (uint)(_stable ? 43 : 0);\n+  return (uint)(uintptr_t)_elem + (uint)(uintptr_t)_size + (uint)(_stable ? 43 : 0) +\n+      (uint)(_flat ? 44 : 0) + (uint)(_not_flat ? 45 : 0) + (uint)(_not_null_free ? 46 : 0) + (uint)(_atomic ? 47 : 0);\n@@ -2331,1 +2480,1 @@\n-  return make(_elem->remove_speculative(), _size, _stable);\n+  return make(_elem->remove_speculative(), _size, _stable, _flat, _not_flat, _not_null_free, _atomic);\n@@ -2338,1 +2487,1 @@\n-  return make(_elem->cleanup_speculative(), _size, _stable);\n+  return make(_elem->cleanup_speculative(), _size, _stable, _flat, _not_flat, _not_null_free, _atomic);\n@@ -2357,0 +2506,6 @@\n+  if (_flat) st->print(\"flat:\");\n+  if (Verbose) {\n+    if (_not_flat) st->print(\"not flat:\");\n+    if (_not_null_free) st->print(\"not null free:\");\n+  }\n+  if (_atomic) st->print(\"atomic:\");\n@@ -2396,2 +2551,13 @@\n-  if (tinst)\n-    return tinst->instance_klass()->is_final();\n+  if (tinst) {\n+    if (tinst->instance_klass()->is_final()) {\n+      \/\/ Even though MyValue is final, [LMyValue is only exact if the array\n+      \/\/ is (not) null-free due to null-free [LMyValue <: null-able [LMyValue.\n+      \/\/ TODO 8350865 If we know that the array can't be null-free, it's allowed to be exact, right?\n+      \/\/ If so, we should add '&& !_not_null_free'\n+      if (tinst->is_inlinetypeptr() && (tinst->ptr() != TypePtr::NotNull)) {\n+        return false;\n+      }\n+      return true;\n+    }\n+    return false;\n+  }\n@@ -2572,1 +2738,1 @@\n-const TypePtr *TypePtr::make(TYPES t, enum PTR ptr, int offset, const TypePtr* speculative, int inline_depth) {\n+const TypePtr* TypePtr::make(TYPES t, enum PTR ptr, Offset offset, const TypePtr* speculative, int inline_depth) {\n@@ -2586,1 +2752,1 @@\n-  return _offset;\n+  return offset();\n@@ -2660,7 +2826,2 @@\n-int TypePtr::meet_offset( int offset ) const {\n-  \/\/ Either is 'TOP' offset?  Return the other offset!\n-  if( _offset == OffsetTop ) return offset;\n-  if( offset == OffsetTop ) return _offset;\n-  \/\/ If either is different, return 'BOTTOM' offset\n-  if( _offset != offset ) return OffsetBot;\n-  return _offset;\n+Type::Offset TypePtr::meet_offset(int offset) const {\n+  return _offset.meet(Offset(offset));\n@@ -2670,4 +2831,2 @@\n-int TypePtr::dual_offset( ) const {\n-  if( _offset == OffsetTop ) return OffsetBot;\/\/ Map 'TOP' into 'BOTTOM'\n-  if( _offset == OffsetBot ) return OffsetTop;\/\/ Map 'BOTTOM' into 'TOP'\n-  return _offset;               \/\/ Map everything else into self\n+Type::Offset TypePtr::dual_offset() const {\n+  return _offset.dual();\n@@ -2681,0 +2840,12 @@\n+\n+const TypePtr::FlatInArray TypePtr::flat_in_array_dual[Uninitialized] = {\n+  \/* TopFlat   -> *\/ MaybeFlat,\n+  \/* Flat      -> *\/ NotFlat,\n+  \/* NotFlat   -> *\/ Flat,\n+  \/* MaybeFlat -> *\/ TopFlat\n+};\n+\n+const char* const TypePtr::flat_in_array_msg[Uninitialized] = {\n+  \"TOP flat in array\", \"flat in array\", \"not flat in array\", \"maybe flat in array\"\n+};\n+\n@@ -2686,13 +2857,2 @@\n-int TypePtr::xadd_offset( intptr_t offset ) const {\n-  \/\/ Adding to 'TOP' offset?  Return 'TOP'!\n-  if( _offset == OffsetTop || offset == OffsetTop ) return OffsetTop;\n-  \/\/ Adding to 'BOTTOM' offset?  Return 'BOTTOM'!\n-  if( _offset == OffsetBot || offset == OffsetBot ) return OffsetBot;\n-  \/\/ Addition overflows or \"accidentally\" equals to OffsetTop? Return 'BOTTOM'!\n-  offset += (intptr_t)_offset;\n-  if (offset != (int)offset || offset == OffsetTop) return OffsetBot;\n-\n-  \/\/ assert( _offset >= 0 && _offset+offset >= 0, \"\" );\n-  \/\/ It is possible to construct a negative offset during PhaseCCP\n-\n-  return (int)offset;        \/\/ Sum valid offsets\n+Type::Offset TypePtr::xadd_offset(intptr_t offset) const {\n+  return _offset.add(offset);\n@@ -2707,1 +2867,1 @@\n-  return make(AnyPtr, _ptr, offset, _speculative, _inline_depth);\n+  return make(AnyPtr, _ptr, Offset(offset), _speculative, _inline_depth);\n@@ -2714,1 +2874,1 @@\n-  return _ptr == a->ptr() && _offset == a->offset() && eq_speculative(a) && _inline_depth == a->_inline_depth;\n+  return _ptr == a->ptr() && _offset == a->_offset && eq_speculative(a) && _inline_depth == a->_inline_depth;\n@@ -2720,1 +2880,1 @@\n-  return (uint)_ptr + (uint)_offset + (uint)hash_speculative() + (uint)_inline_depth;\n+  return (uint)_ptr + (uint)offset() + (uint)hash_speculative() + (uint)_inline_depth;\n@@ -2977,0 +3137,33 @@\n+TypePtr::FlatInArray TypePtr::compute_flat_in_array(ciInstanceKlass* instance_klass, bool is_exact) {\n+  if (!instance_klass->can_be_inline_klass(is_exact)) {\n+    \/\/ Definitely not a value class and thus never flat in an array.\n+    return NotFlat;\n+  }\n+  if (instance_klass->is_inlinetype() && instance_klass->as_inline_klass()->is_always_flat_in_array()) {\n+    return Flat;\n+  }\n+  \/\/ We don't know.\n+  return MaybeFlat;\n+}\n+\n+\/\/ Compute flat in array property if we don't know anything about it (i.e. old_flat_in_array == MaybeFlat).\n+TypePtr::FlatInArray TypePtr::compute_flat_in_array_if_unknown(ciInstanceKlass* instance_klass, bool is_exact,\n+  FlatInArray old_flat_in_array) const {\n+  switch (old_flat_in_array) {\n+    case Flat:\n+      assert(can_be_inline_type(), \"only value objects can be flat in array\");\n+      assert(!instance_klass->is_inlinetype() || instance_klass->as_inline_klass()->is_always_flat_in_array(),\n+             \"a value object is only marked flat in array if it's proven to be always flat in array\");\n+      break;\n+    case NotFlat:\n+      assert(!instance_klass->maybe_flat_in_array(), \"cannot be flat\");\n+      break;\n+    case MaybeFlat:\n+      return compute_flat_in_array(instance_klass, is_exact);\n+      break;\n+    default:\n+      break;\n+  }\n+  return old_flat_in_array;\n+}\n+\n@@ -2991,7 +3184,1 @@\n-  if (_offset == OffsetBot) {\n-    st->print(\"+bot\");\n-  } else if (_offset == OffsetTop) {\n-    st->print(\"+top\");\n-  } else {\n-    st->print(\"+%d\", _offset);\n-  }\n+  _offset.dump2(st);\n@@ -3023,0 +3210,16 @@\n+\n+void TypePtr::dump_flat_in_array(FlatInArray flat_in_array, outputStream* st) {\n+  switch (flat_in_array) {\n+    case MaybeFlat:\n+    case NotFlat:\n+      if (!Verbose) {\n+        break;\n+      }\n+    case TopFlat:\n+    case Flat:\n+      st->print(\" (%s)\", flat_in_array_msg[flat_in_array]);\n+      break;\n+    default:\n+      ShouldNotReachHere();\n+  }\n+}\n@@ -3030,1 +3233,1 @@\n-  return (_offset != OffsetBot) && !below_centerline(_ptr);\n+  return (_offset != Offset::bottom) && !below_centerline(_ptr);\n@@ -3034,1 +3237,1 @@\n-  return (_offset == OffsetTop) || above_centerline(_ptr);\n+  return (_offset == Offset::top) || above_centerline(_ptr);\n@@ -3442,1 +3645,1 @@\n-TypeOopPtr::TypeOopPtr(TYPES t, PTR ptr, ciKlass* k, const TypeInterfaces* interfaces, bool xk, ciObject* o, int offset,\n+TypeOopPtr::TypeOopPtr(TYPES t, PTR ptr, ciKlass* k, const TypeInterfaces* interfaces, bool xk, ciObject* o, Offset offset, Offset field_offset,\n@@ -3451,0 +3654,1 @@\n+    _is_ptr_to_strict_final_field(false),\n@@ -3458,2 +3662,11 @@\n-      (offset > 0) && xk && (k != nullptr) && k->is_instance_klass()) {\n-    _is_ptr_to_boxed_value = k->as_instance_klass()->is_boxed_value_offset(offset);\n+      (offset.get() > 0) && xk && (k != nullptr) && k->is_instance_klass()) {\n+    _is_ptr_to_boxed_value = k->as_instance_klass()->is_boxed_value_offset(offset.get());\n+    _is_ptr_to_strict_final_field = _is_ptr_to_boxed_value;\n+  }\n+\n+  if (klass() != nullptr && klass()->is_instance_klass() && klass()->is_loaded() &&\n+      this->offset() != Type::OffsetBot && this->offset() != Type::OffsetTop) {\n+    ciField* field = klass()->as_instance_klass()->get_field_by_offset(this->offset(), false);\n+    if (field != nullptr && field->is_strict() && field->is_final()) {\n+      _is_ptr_to_strict_final_field = true;\n+    }\n@@ -3461,0 +3674,1 @@\n+\n@@ -3462,2 +3676,2 @@\n-  if (_offset > 0 || _offset == Type::OffsetTop || _offset == Type::OffsetBot) {\n-    if (_offset == oopDesc::klass_offset_in_bytes()) {\n+  if (this->offset() > 0 || this->offset() == Type::OffsetTop || this->offset() == Type::OffsetBot) {\n+    if (this->offset() == oopDesc::klass_offset_in_bytes()) {\n@@ -3469,3 +3683,17 @@\n-    } else if (this->isa_aryptr()) {\n-      _is_ptr_to_narrowoop = (UseCompressedOops && klass()->is_obj_array_klass() &&\n-                             _offset != arrayOopDesc::length_offset_in_bytes());\n+    } else if (UseCompressedOops && this->isa_aryptr() && this->offset() != arrayOopDesc::length_offset_in_bytes()) {\n+      if (klass()->is_obj_array_klass()) {\n+        _is_ptr_to_narrowoop = true;\n+      } else if (klass()->is_flat_array_klass() && field_offset != Offset::top && field_offset != Offset::bottom) {\n+        \/\/ Check if the field of the inline type array element contains oops\n+        ciInlineKlass* vk = klass()->as_flat_array_klass()->element_klass()->as_inline_klass();\n+        int foffset = field_offset.get() + vk->payload_offset();\n+        BasicType field_bt;\n+        ciField* field = vk->get_field_by_offset(foffset, false);\n+        if (field != nullptr) {\n+          field_bt = field->layout_type();\n+        } else {\n+          assert(field_offset.get() == vk->null_marker_offset_in_payload(), \"no field or null marker of %s at offset %d\", vk->name()->as_utf8(), foffset);\n+          field_bt = T_BOOLEAN;\n+        }\n+        _is_ptr_to_narrowoop = UseCompressedOops && ::is_reference_type(field_bt);\n+      }\n@@ -3473,1 +3701,0 @@\n-      ciInstanceKlass* ik = klass()->as_instance_klass();\n@@ -3476,1 +3703,1 @@\n-      } else if (_offset == OffsetBot || _offset == OffsetTop) {\n+      } else if (_offset == Offset::bottom || _offset == Offset::top) {\n@@ -3481,3 +3708,2 @@\n-\n-            (_offset == java_lang_Class::klass_offset() ||\n-             _offset == java_lang_Class::array_klass_offset())) {\n+            (this->offset() == java_lang_Class::klass_offset() ||\n+             this->offset() == java_lang_Class::array_klass_offset())) {\n@@ -3489,1 +3715,1 @@\n-                   _offset >= InstanceMirrorKlass::offset_of_static_fields()) {\n+                   this->offset() >= InstanceMirrorKlass::offset_of_static_fields()) {\n@@ -3494,1 +3720,1 @@\n-            basic_elem_type = k->get_field_type_by_offset(_offset, true);\n+            basic_elem_type = k->get_field_type_by_offset(this->offset(), true);\n@@ -3504,1 +3730,2 @@\n-          BasicType basic_elem_type = ik->get_field_type_by_offset(_offset, false);\n+          ciInstanceKlass* ik = klass()->as_instance_klass();\n+          BasicType basic_elem_type = ik->get_field_type_by_offset(this->offset(), false);\n@@ -3519,1 +3746,1 @@\n-#endif\n+#endif \/\/ _LP64\n@@ -3523,2 +3750,2 @@\n-const TypeOopPtr *TypeOopPtr::make(PTR ptr, int offset, int instance_id,\n-                                     const TypePtr* speculative, int inline_depth) {\n+const TypeOopPtr *TypeOopPtr::make(PTR ptr, Offset offset, int instance_id,\n+                                   const TypePtr* speculative, int inline_depth) {\n@@ -3530,1 +3757,1 @@\n-  return (TypeOopPtr*)(new TypeOopPtr(OopPtr, ptr, k, interfaces, xk, o, offset, instance_id, speculative, inline_depth))->hashcons();\n+  return (TypeOopPtr*)(new TypeOopPtr(OopPtr, ptr, k, interfaces, xk, o, offset, Offset::bottom, instance_id, speculative, inline_depth))->hashcons();\n@@ -3555,1 +3782,0 @@\n-\n@@ -3604,1 +3830,1 @@\n-    int offset = meet_offset(tp->offset());\n+    Offset offset = meet_offset(tp->offset());\n@@ -3646,1 +3872,1 @@\n-  return new TypeOopPtr(_base, dual_ptr(), klass(), _interfaces, klass_is_exact(), const_oop(), dual_offset(), dual_instance_id(), dual_speculative(), dual_inline_depth());\n+  return new TypeOopPtr(_base, dual_ptr(), klass(), _interfaces, klass_is_exact(), const_oop(), dual_offset(), Offset::bottom, dual_instance_id(), dual_speculative(), dual_inline_depth());\n@@ -3651,2 +3877,2 @@\n-const TypeOopPtr* TypeOopPtr::make_from_klass_common(ciKlass* klass, bool klass_change, bool try_for_exact, InterfaceHandling interface_handling) {\n-  if (klass->is_instance_klass()) {\n+const TypeOopPtr* TypeOopPtr::make_from_klass_common(ciKlass *klass, bool klass_change, bool try_for_exact, InterfaceHandling interface_handling) {\n+  if (klass->is_instance_klass() || klass->is_inlinetype()) {\n@@ -3658,0 +3884,1 @@\n+    ciInstanceKlass* ik = klass->as_instance_klass();\n@@ -3660,1 +3887,0 @@\n-      ciInstanceKlass* ik = klass->as_instance_klass();\n@@ -3678,0 +3904,1 @@\n+    FlatInArray flat_in_array = compute_flat_in_array(ik, klass_is_exact);\n@@ -3679,1 +3906,1 @@\n-    return TypeInstPtr::make(TypePtr::BotPTR, klass, interfaces, klass_is_exact, nullptr, 0);\n+    return TypeInstPtr::make(TypePtr::BotPTR, klass, interfaces, klass_is_exact, nullptr, Offset(0), flat_in_array);\n@@ -3681,5 +3908,19 @@\n-    \/\/ Element is an object array. Recursively call ourself.\n-    ciKlass* eklass = klass->as_obj_array_klass()->element_klass();\n-    const TypeOopPtr *etype = TypeOopPtr::make_from_klass_common(eklass, false, try_for_exact, interface_handling);\n-    bool xk = etype->klass_is_exact();\n-    const TypeAry* arr0 = TypeAry::make(etype, TypeInt::POS);\n+    \/\/ Element is an object or inline type array. Recursively call ourself.\n+    const TypeOopPtr* etype = TypeOopPtr::make_from_klass_common(klass->as_array_klass()->element_klass(), \/* klass_change= *\/ false, try_for_exact, interface_handling);\n+    \/\/ Determine null-free\/flat properties\n+    const bool is_null_free = klass->as_array_klass()->is_elem_null_free();\n+    if (is_null_free) {\n+      etype = etype->join_speculative(NOTNULL)->is_oopptr();\n+    }\n+    const TypeOopPtr* exact_etype = etype;\n+    if (etype->can_be_inline_type()) {\n+      \/\/ Use exact type if element can be an inline type\n+      exact_etype = TypeOopPtr::make_from_klass_common(klass->as_array_klass()->element_klass(), \/* klass_change= *\/ true, \/* try_for_exact= *\/ true, interface_handling);\n+    }\n+    bool not_inline = !exact_etype->can_be_inline_type();\n+    bool not_null_free = not_inline;\n+    bool not_flat = !UseArrayFlattening || not_inline || (exact_etype->is_inlinetypeptr() && !exact_etype->inline_klass()->maybe_flat_in_array());\n+    bool atomic = klass->as_array_klass()->is_elem_atomic();\n+    \/\/ Even though MyValue is final, [LMyValue is not exact because null-free [LMyValue is a subtype.\n+    bool xk = etype->klass_is_exact() && !etype->is_inlinetypeptr();\n+    const TypeAry* arr0 = TypeAry::make(etype, TypeInt::POS, \/* stable= *\/ false, \/* flat= *\/ false, not_flat, not_null_free, atomic);\n@@ -3688,2 +3929,2 @@\n-    \/\/ slam nulls down in the subarrays.\n-    const TypeAryPtr* arr = TypeAryPtr::make(TypePtr::BotPTR, arr0, nullptr, xk, 0);\n+    \/\/ slam nullptrs down in the subarrays.\n+    const TypeAryPtr* arr = TypeAryPtr::make(TypePtr::BotPTR, arr0, nullptr, xk, Offset(0));\n@@ -3694,1 +3935,2 @@\n-    const TypeAry* arr0 = TypeAry::make(etype, TypeInt::POS);\n+    const TypeAry* arr0 = TypeAry::make(etype, TypeInt::POS,\n+                                        \/* stable= *\/ false, \/* flat= *\/ false, \/* not_flat= *\/ true, \/* not_null_free= *\/ true);\n@@ -3697,1 +3939,12 @@\n-    const TypeAryPtr* arr = TypeAryPtr::make(TypePtr::BotPTR, arr0, klass, true, 0);\n+    const TypeAryPtr* arr = TypeAryPtr::make(TypePtr::BotPTR, arr0, klass, true, Offset(0));\n+    return arr;\n+  } else if (klass->is_flat_array_klass()) {\n+    const TypeOopPtr* etype = TypeOopPtr::make_from_klass_raw(klass->as_array_klass()->element_klass(), trust_interfaces);\n+    const bool is_null_free = klass->as_array_klass()->is_elem_null_free();\n+    if (is_null_free) {\n+      etype = etype->join_speculative(NOTNULL)->is_oopptr();\n+    }\n+    bool atomic = klass->as_array_klass()->is_elem_atomic();\n+    const TypeAry* arr0 = TypeAry::make(etype, TypeInt::POS, \/* stable= *\/ false, \/* flat= *\/ true, \/* not_flat= *\/ false, \/* not_null_free= *\/ false, atomic);\n+    const bool exact = is_null_free; \/\/ Only exact if null-free because \"null-free [LMyValue <: null-able [LMyValue\".\n+    const TypeAryPtr* arr = TypeAryPtr::make(TypePtr::BotPTR, arr0, klass, exact, Offset(0));\n@@ -3713,2 +3966,2 @@\n-  if (klass->is_instance_klass()) {\n-    \/\/ Element is an instance\n+  if (klass->is_instance_klass() || klass->is_inlinetype()) {\n+    \/\/ Element is an instance or inline type\n@@ -3718,1 +3971,1 @@\n-      return TypeInstPtr::make(TypePtr::NotNull, klass, true, nullptr, 0);\n+      return TypeInstPtr::make(TypePtr::NotNull, klass, true, nullptr, Offset(0));\n@@ -3722,3 +3975,9 @@\n-    const TypeOopPtr *etype =\n-      TypeOopPtr::make_from_klass_raw(klass->as_obj_array_klass()->element_klass(), trust_interfaces);\n-    const TypeAry* arr0 = TypeAry::make(etype, TypeInt::make(o->as_array()->length()));\n+    const TypeOopPtr* etype = TypeOopPtr::make_from_klass_raw(klass->as_array_klass()->element_klass(), trust_interfaces);\n+    bool is_flat = o->as_array()->is_flat();\n+    bool is_null_free = o->as_array()->is_null_free();\n+    if (is_null_free) {\n+      etype = etype->join_speculative(TypePtr::NOTNULL)->is_oopptr();\n+    }\n+    bool is_atomic = o->as_array()->is_atomic();\n+    const TypeAry* arr0 = TypeAry::make(etype, TypeInt::make(o->as_array()->length()), \/* stable= *\/ false, \/* flat= *\/ false,\n+                                        \/* not_flat= *\/ !is_flat, \/* not_null_free= *\/ !is_null_free, \/* atomic= *\/ is_atomic);\n@@ -3729,1 +3988,1 @@\n-      return TypeAryPtr::make(TypePtr::Constant, o, arr0, klass, true, 0);\n+      return TypeAryPtr::make(TypePtr::Constant, o, arr0, klass, true, Offset(0));\n@@ -3731,1 +3990,1 @@\n-      return TypeAryPtr::make(TypePtr::NotNull, arr0, klass, true, 0);\n+      return TypeAryPtr::make(TypePtr::NotNull, arr0, klass, true, Offset(0));\n@@ -3735,3 +3994,3 @@\n-    const Type* etype =\n-      (Type*)get_const_basic_type(klass->as_type_array_klass()->element_type());\n-    const TypeAry* arr0 = TypeAry::make(etype, TypeInt::make(o->as_array()->length()));\n+    const Type* etype = (Type*)get_const_basic_type(klass->as_type_array_klass()->element_type());\n+    const TypeAry* arr0 = TypeAry::make(etype, TypeInt::make(o->as_array()->length()), \/* stable= *\/ false, \/* flat= *\/ false,\n+                                        \/* not_flat= *\/ true, \/* not_null_free= *\/ true);\n@@ -3741,1 +4000,1 @@\n-      return TypeAryPtr::make(TypePtr::Constant, o, arr0, klass, true, 0);\n+      return TypeAryPtr::make(TypePtr::Constant, o, arr0, klass, true, Offset(0));\n@@ -3743,1 +4002,18 @@\n-      return TypeAryPtr::make(TypePtr::NotNull, arr0, klass, true, 0);\n+      return TypeAryPtr::make(TypePtr::NotNull, arr0, klass, true, Offset(0));\n+    }\n+  } else if (klass->is_flat_array_klass()) {\n+    const TypeOopPtr* etype = TypeOopPtr::make_from_klass_raw(klass->as_array_klass()->element_klass(), trust_interfaces);\n+    bool is_null_free = o->as_array()->is_null_free();\n+    if (is_null_free) {\n+      etype = etype->join_speculative(TypePtr::NOTNULL)->is_oopptr();\n+    }\n+    bool is_atomic = o->as_array()->is_atomic();\n+    const TypeAry* arr0 = TypeAry::make(etype, TypeInt::make(o->as_array()->length()), \/* stable= *\/ false, \/* flat= *\/ true,\n+                                        \/* not_flat= *\/ false, \/* not_null_free= *\/ !is_null_free, \/* atomic= *\/ is_atomic);\n+    \/\/ We used to pass NotNull in here, asserting that the sub-arrays\n+    \/\/ are all not-null.  This is not true in generally, as code can\n+    \/\/ slam nullptrs down in the subarrays.\n+    if (make_constant) {\n+      return TypeAryPtr::make(TypePtr::Constant, o, arr0, klass, true, Offset(0));\n+    } else {\n+      return TypeAryPtr::make(TypePtr::NotNull, arr0, klass, true, Offset(0));\n@@ -3754,1 +4030,1 @@\n-  assert( _offset >= 0, \"\" );\n+  assert(offset() >= 0, \"\");\n@@ -3756,1 +4032,1 @@\n-  if (_offset != 0) {\n+  if (offset() != 0) {\n@@ -3842,1 +4118,1 @@\n-  return (_offset == 0) && !below_centerline(_ptr);\n+  return (offset() == 0) && !below_centerline(_ptr);\n@@ -3851,1 +4127,1 @@\n-  return make(_ptr, offset, _instance_id, with_offset_speculative(offset), _inline_depth);\n+  return make(_ptr, Offset(offset), _instance_id, with_offset_speculative(offset), _inline_depth);\n@@ -3964,3 +4240,6 @@\n-TypeInstPtr::TypeInstPtr(PTR ptr, ciKlass* k, const TypeInterfaces* interfaces, bool xk, ciObject* o, int off,\n-                         int instance_id, const TypePtr* speculative, int inline_depth)\n-  : TypeOopPtr(InstPtr, ptr, k, interfaces, xk, o, off, instance_id, speculative, inline_depth) {\n+TypeInstPtr::TypeInstPtr(PTR ptr, ciKlass* k, const TypeInterfaces* interfaces, bool xk, ciObject* o, Offset off,\n+                         FlatInArray flat_in_array, int instance_id, const TypePtr* speculative, int inline_depth)\n+  : TypeOopPtr(InstPtr, ptr, k, interfaces, xk, o, off, Offset::bottom, instance_id, speculative, inline_depth),\n+    _flat_in_array(flat_in_array) {\n+\n+  assert(flat_in_array != Uninitialized, \"must be set now\");\n@@ -3979,1 +4258,2 @@\n-                                     int offset,\n+                                     Offset offset,\n+                                     FlatInArray flat_in_array,\n@@ -3991,0 +4271,1 @@\n+  ciInstanceKlass* ik = k->as_instance_klass();\n@@ -3995,1 +4276,0 @@\n-    ciInstanceKlass* ik = k->as_instance_klass();\n@@ -4001,0 +4281,3 @@\n+  if (flat_in_array == Uninitialized) {\n+    flat_in_array = compute_flat_in_array(ik, xk);\n+  }\n@@ -4003,1 +4286,1 @@\n-    (TypeInstPtr*)(new TypeInstPtr(ptr, k, interfaces, xk, o ,offset, instance_id, speculative, inline_depth))->hashcons();\n+    (TypeInstPtr*)(new TypeInstPtr(ptr, k, interfaces, xk, o, offset, flat_in_array, instance_id, speculative, inline_depth))->hashcons();\n@@ -4069,1 +4352,1 @@\n-  return make(ptr, klass(), _interfaces, klass_is_exact(), ptr == Constant ? const_oop() : nullptr, _offset, _instance_id, _speculative, _inline_depth);\n+  return make(ptr, klass(), _interfaces, klass_is_exact(), ptr == Constant ? const_oop() : nullptr, _offset, _flat_in_array, _instance_id, _speculative, _inline_depth);\n@@ -4080,1 +4363,2 @@\n-  return make(ptr(), klass(), _interfaces, klass_is_exact, const_oop(), _offset, _instance_id, _speculative, _inline_depth);\n+  FlatInArray flat_in_array = compute_flat_in_array(ik, klass_is_exact);\n+  return make(ptr(), klass(), _interfaces, klass_is_exact, const_oop(), _offset, flat_in_array, _instance_id, _speculative, _inline_depth);\n@@ -4086,1 +4370,1 @@\n-  return make(_ptr, klass(),  _interfaces, _klass_is_exact, const_oop(), _offset, instance_id, _speculative, _inline_depth);\n+  return make(_ptr, klass(), _interfaces, _klass_is_exact, const_oop(), _offset, _flat_in_array, instance_id, _speculative, _inline_depth);\n@@ -4093,1 +4377,1 @@\n-  int off = meet_offset(tinst->offset());\n+  Offset off = meet_offset(tinst->offset());\n@@ -4118,1 +4402,5 @@\n-    else if (loaded->ptr() == TypePtr::AnyNull)  { return make(ptr, unloaded->klass(), interfaces, false, nullptr, off, instance_id, speculative, depth); }\n+    else if (loaded->ptr() == TypePtr::AnyNull)  {\n+      FlatInArray flat_in_array = meet_flat_in_array(_flat_in_array, tinst->flat_in_array());\n+      return make(ptr, unloaded->klass(), interfaces, false, nullptr, off, flat_in_array, instance_id,\n+                  speculative, depth);\n+    }\n@@ -4182,1 +4470,1 @@\n-    int offset = meet_offset(tp->offset());\n+    Offset offset = meet_offset(tp->offset());\n@@ -4191,1 +4479,1 @@\n-                  (ptr == Constant ? const_oop() : nullptr), offset, instance_id, speculative, depth);\n+                  (ptr == Constant ? const_oop() : nullptr), offset, flat_in_array(), instance_id, speculative, depth);\n@@ -4207,1 +4495,1 @@\n-    int offset = meet_offset(tp->offset());\n+    Offset offset = meet_offset(tp->offset());\n@@ -4219,1 +4507,1 @@\n-                  (ptr == Constant ? const_oop() : nullptr), offset, instance_id, speculative, depth);\n+                  (ptr == Constant ? const_oop() : nullptr), offset, flat_in_array(), instance_id, speculative, depth);\n@@ -4247,1 +4535,1 @@\n-    int off = meet_offset(tinst->offset());\n+    Offset off = meet_offset(tinst->offset());\n@@ -4280,0 +4568,1 @@\n+      FlatInArray flat_in_array = meet_flat_in_array(_flat_in_array, tinst->flat_in_array());\n@@ -4301,1 +4590,1 @@\n-      res = make(ptr, res_klass, interfaces, res_xk, o, off, instance_id, speculative, depth);\n+      res = make(ptr, res_klass, interfaces, res_xk, o, off, flat_in_array, instance_id, speculative, depth);\n@@ -4316,0 +4605,1 @@\n+\n@@ -4361,1 +4651,0 @@\n-  \/\/ Check for subtyping:\n@@ -4365,0 +4654,1 @@\n+    \/\/ Same klass\n@@ -4370,1 +4660,1 @@\n-  } else if(!this_xk && other_type->is_meet_subtype_of(this_type)) {\n+  } else if (!this_xk && other_type->is_meet_subtype_of(this_type)) {\n@@ -4375,2 +4665,3 @@\n-  if (subtype) {\n-    if (above_centerline(ptr)) { \/\/ both are up?\n+  if (subtype != nullptr) {\n+    if (above_centerline(ptr)) {\n+      \/\/ Both types are empty.\n@@ -4380,1 +4671,2 @@\n-      this_type = other_type; \/\/ tinst is down; keep down man\n+      \/\/ this_type is empty while other_type is not. Take other_type.\n+      this_type = other_type;\n@@ -4383,0 +4675,1 @@\n+      \/\/ other_type is empty while this_type is not. Take this_type.\n@@ -4384,1 +4677,1 @@\n-      other_xk = this_xk;\n+      \/\/ this_type and other_type are both non-empty.\n@@ -4413,1 +4706,0 @@\n-\n@@ -4417,0 +4709,34 @@\n+\/\/                Top-Flat    Flat        Not-Flat    Maybe-Flat\n+\/\/ -------------------------------------------------------------\n+\/\/    Top-Flat    Top-Flat    Flat        Not-Flat    Maybe-Flat\n+\/\/        Flat    Flat        Flat        Maybe-Flat  Maybe-Flat\n+\/\/    Not-Flat    Not-Flat    Maybe-Flat  Not-Flat    Maybe-Flat\n+\/\/  Maybe-Flat    Maybe-Flat  Maybe-Flat  Maybe-Flat  Maybe-flat\n+TypePtr::FlatInArray TypePtr::meet_flat_in_array(const FlatInArray left, const FlatInArray right) {\n+  if (left == TopFlat) {\n+    return right;\n+  }\n+  if (right == TopFlat) {\n+    return left;\n+  }\n+  if (left == MaybeFlat || right == MaybeFlat) {\n+    return MaybeFlat;\n+  }\n+\n+  switch (left) {\n+    case Flat:\n+      if (right == Flat) {\n+        return Flat;\n+      }\n+      return MaybeFlat;\n+    case NotFlat:\n+      if (right == NotFlat) {\n+        return NotFlat;\n+      }\n+      return MaybeFlat;\n+    default:\n+      ShouldNotReachHere();\n+      return Uninitialized;\n+  }\n+}\n+\n@@ -4424,1 +4750,0 @@\n-\n@@ -4432,2 +4757,3 @@\n-const Type *TypeInstPtr::xdual() const {\n-  return new TypeInstPtr(dual_ptr(), klass(), _interfaces, klass_is_exact(), const_oop(), dual_offset(), dual_instance_id(), dual_speculative(), dual_inline_depth());\n+const Type* TypeInstPtr::xdual() const {\n+  return new TypeInstPtr(dual_ptr(), klass(), _interfaces, klass_is_exact(), const_oop(), dual_offset(),\n+                         dual_flat_in_array(), dual_instance_id(), dual_speculative(), dual_inline_depth());\n@@ -4442,0 +4768,1 @@\n+    _flat_in_array == p->_flat_in_array &&\n@@ -4448,2 +4775,2 @@\n-uint TypeInstPtr::hash(void) const {\n-  return klass()->hash() + TypeOopPtr::hash() + _interfaces->hash();\n+uint TypeInstPtr::hash() const {\n+  return klass()->hash() + TypeOopPtr::hash() + _interfaces->hash() + static_cast<uint>(_flat_in_array);\n@@ -4493,0 +4820,2 @@\n+  st->print(\" *\");\n+\n@@ -4497,0 +4826,1 @@\n+  dump_flat_in_array(_flat_in_array, st);\n@@ -4500,0 +4830,7 @@\n+bool TypeInstPtr::empty() const {\n+  if (_flat_in_array == TopFlat) {\n+    return true;\n+  }\n+  return TypeOopPtr::empty();\n+}\n+\n@@ -4502,1 +4839,1 @@\n-  return make(_ptr, klass(), _interfaces, klass_is_exact(), const_oop(), xadd_offset(offset),\n+  return make(_ptr, klass(), _interfaces, klass_is_exact(), const_oop(), xadd_offset(offset), _flat_in_array,\n@@ -4507,1 +4844,1 @@\n-  return make(_ptr, klass(), _interfaces, klass_is_exact(), const_oop(), offset,\n+  return make(_ptr, klass(), _interfaces, klass_is_exact(), const_oop(), Offset(offset), _flat_in_array,\n@@ -4516,1 +4853,1 @@\n-  return make(_ptr, klass(), _interfaces, klass_is_exact(), const_oop(), _offset,\n+  return make(_ptr, klass(), _interfaces, klass_is_exact(), const_oop(), _offset, _flat_in_array,\n@@ -4521,1 +4858,1 @@\n-  return make(_ptr, klass(), _interfaces, klass_is_exact(), const_oop(), _offset, _instance_id, speculative, _inline_depth);\n+  return make(_ptr, klass(), _interfaces, klass_is_exact(), const_oop(), _offset, _flat_in_array, _instance_id, speculative, _inline_depth);\n@@ -4528,1 +4865,1 @@\n-  return make(_ptr, klass(), _interfaces, klass_is_exact(), const_oop(), _offset, _instance_id, _speculative, depth);\n+  return make(_ptr, klass(), _interfaces, klass_is_exact(), const_oop(), _offset, _flat_in_array, _instance_id, _speculative, depth);\n@@ -4533,1 +4870,9 @@\n-  return make(_ptr, klass(), _interfaces, klass_is_exact(), const_oop(), _offset, instance_id, _speculative, _inline_depth);\n+  return make(_ptr, klass(), _interfaces, klass_is_exact(), const_oop(), _offset, _flat_in_array, instance_id, _speculative, _inline_depth);\n+}\n+\n+const TypeInstPtr *TypeInstPtr::cast_to_flat_in_array() const {\n+  return make(_ptr, klass(), _interfaces, klass_is_exact(), const_oop(), _offset, Flat, _instance_id, _speculative, _inline_depth);\n+}\n+\n+const TypeInstPtr *TypeInstPtr::cast_to_maybe_flat_in_array() const {\n+  return make(_ptr, klass(), _interfaces, klass_is_exact(), const_oop(), _offset, MaybeFlat, _instance_id, _speculative, _inline_depth);\n@@ -4547,1 +4892,2 @@\n-  return TypeInstKlassPtr::make(xk ? TypePtr::Constant : TypePtr::NotNull, klass(), _interfaces, 0);\n+  FlatInArray flat_in_array = compute_flat_in_array_if_unknown(ik, xk, _flat_in_array);\n+  return TypeInstKlassPtr::make(xk ? TypePtr::Constant : TypePtr::NotNull, klass(), _interfaces, Offset(0), flat_in_array);\n@@ -4592,1 +4938,0 @@\n-\n@@ -4615,10 +4960,11 @@\n-const TypeAryPtr* TypeAryPtr::RANGE;\n-const TypeAryPtr* TypeAryPtr::OOPS;\n-const TypeAryPtr* TypeAryPtr::NARROWOOPS;\n-const TypeAryPtr* TypeAryPtr::BYTES;\n-const TypeAryPtr* TypeAryPtr::SHORTS;\n-const TypeAryPtr* TypeAryPtr::CHARS;\n-const TypeAryPtr* TypeAryPtr::INTS;\n-const TypeAryPtr* TypeAryPtr::LONGS;\n-const TypeAryPtr* TypeAryPtr::FLOATS;\n-const TypeAryPtr* TypeAryPtr::DOUBLES;\n+const TypeAryPtr *TypeAryPtr::RANGE;\n+const TypeAryPtr *TypeAryPtr::OOPS;\n+const TypeAryPtr *TypeAryPtr::NARROWOOPS;\n+const TypeAryPtr *TypeAryPtr::BYTES;\n+const TypeAryPtr *TypeAryPtr::SHORTS;\n+const TypeAryPtr *TypeAryPtr::CHARS;\n+const TypeAryPtr *TypeAryPtr::INTS;\n+const TypeAryPtr *TypeAryPtr::LONGS;\n+const TypeAryPtr *TypeAryPtr::FLOATS;\n+const TypeAryPtr *TypeAryPtr::DOUBLES;\n+const TypeAryPtr *TypeAryPtr::INLINES;\n@@ -4627,1 +4973,1 @@\n-const TypeAryPtr *TypeAryPtr::make(PTR ptr, const TypeAry *ary, ciKlass* k, bool xk, int offset,\n+const TypeAryPtr* TypeAryPtr::make(PTR ptr, const TypeAry *ary, ciKlass* k, bool xk, Offset offset, Offset field_offset,\n@@ -4637,1 +4983,1 @@\n-  return (TypeAryPtr*)(new TypeAryPtr(ptr, nullptr, ary, k, xk, offset, instance_id, false, speculative, inline_depth))->hashcons();\n+  return (TypeAryPtr*)(new TypeAryPtr(ptr, nullptr, ary, k, xk, offset, field_offset, instance_id, false, speculative, inline_depth))->hashcons();\n@@ -4641,1 +4987,1 @@\n-const TypeAryPtr *TypeAryPtr::make(PTR ptr, ciObject* o, const TypeAry *ary, ciKlass* k, bool xk, int offset,\n+const TypeAryPtr* TypeAryPtr::make(PTR ptr, ciObject* o, const TypeAry *ary, ciKlass* k, bool xk, Offset offset, Offset field_offset,\n@@ -4653,1 +4999,1 @@\n-  return (TypeAryPtr*)(new TypeAryPtr(ptr, o, ary, k, xk, offset, instance_id, is_autobox_cache, speculative, inline_depth))->hashcons();\n+  return (TypeAryPtr*)(new TypeAryPtr(ptr, o, ary, k, xk, offset, field_offset, instance_id, is_autobox_cache, speculative, inline_depth))->hashcons();\n@@ -4659,1 +5005,1 @@\n-  return make(ptr, ptr == Constant ? const_oop() : nullptr, _ary, klass(), klass_is_exact(), _offset, _instance_id, _speculative, _inline_depth);\n+  return make(ptr, ptr == Constant ? const_oop() : nullptr, _ary, klass(), klass_is_exact(), _offset, _field_offset, _instance_id, _speculative, _inline_depth, _is_autobox_cache);\n@@ -4667,1 +5013,1 @@\n-  return make(ptr(), const_oop(), _ary, klass(), klass_is_exact, _offset, _instance_id, _speculative, _inline_depth);\n+  return make(ptr(), const_oop(), _ary, klass(), klass_is_exact, _offset, _field_offset, _instance_id, _speculative, _inline_depth, _is_autobox_cache);\n@@ -4673,1 +5019,1 @@\n-  return make(_ptr, const_oop(), _ary, klass(), _klass_is_exact, _offset, instance_id, _speculative, _inline_depth);\n+  return make(_ptr, const_oop(), _ary, klass(), _klass_is_exact, _offset, _field_offset, instance_id, _speculative, _inline_depth, _is_autobox_cache);\n@@ -4731,2 +5077,65 @@\n-  const TypeAry* new_ary = TypeAry::make(elem(), new_size, is_stable());\n-  return make(ptr(), const_oop(), new_ary, klass(), klass_is_exact(), _offset, _instance_id, _speculative, _inline_depth);\n+  const TypeAry* new_ary = TypeAry::make(elem(), new_size, is_stable(), is_flat(), is_not_flat(), is_not_null_free(), is_atomic());\n+  return make(ptr(), const_oop(), new_ary, klass(), klass_is_exact(), _offset, _field_offset, _instance_id, _speculative, _inline_depth, _is_autobox_cache);\n+}\n+\n+\/\/-------------------------------cast_to_not_flat------------------------------\n+const TypeAryPtr* TypeAryPtr::cast_to_not_flat(bool not_flat) const {\n+  if (not_flat == is_not_flat()) {\n+    return this;\n+  }\n+  assert(!not_flat || !is_flat(), \"inconsistency\");\n+  const TypeAry* new_ary = TypeAry::make(elem(), size(), is_stable(), is_flat(), not_flat, is_not_null_free(), is_atomic());\n+  const TypeAryPtr* res = make(ptr(), const_oop(), new_ary, klass(), klass_is_exact(), _offset, _field_offset, _instance_id, _speculative, _inline_depth, _is_autobox_cache);\n+  \/\/ We keep the speculative part if it contains information about flat-\/nullability.\n+  \/\/ Make sure it's removed if it's not better than the non-speculative type anymore.\n+  if (res->speculative() == res->remove_speculative()) {\n+    return res->remove_speculative();\n+  }\n+  return res;\n+}\n+\n+\/\/-------------------------------cast_to_not_null_free-------------------------\n+const TypeAryPtr* TypeAryPtr::cast_to_not_null_free(bool not_null_free) const {\n+  if (not_null_free == is_not_null_free()) {\n+    return this;\n+  }\n+  assert(!not_null_free || !is_null_free(), \"inconsistency\");\n+  const TypeAry* new_ary = TypeAry::make(elem(), size(), is_stable(), is_flat(), is_not_flat(), not_null_free, is_atomic());\n+  const TypeAryPtr* res = make(ptr(), const_oop(), new_ary, klass(), klass_is_exact(), _offset, _field_offset,\n+                               _instance_id, _speculative, _inline_depth, _is_autobox_cache);\n+  \/\/ We keep the speculative part if it contains information about flat-\/nullability.\n+  \/\/ Make sure it's removed if it's not better than the non-speculative type anymore.\n+  if (res->speculative() == res->remove_speculative()) {\n+    return res->remove_speculative();\n+  }\n+  return res;\n+}\n+\n+\/\/---------------------------------update_properties---------------------------\n+const TypeAryPtr* TypeAryPtr::update_properties(const TypeAryPtr* from) const {\n+  if ((from->is_flat()          && is_not_flat()) ||\n+      (from->is_not_flat()      && is_flat()) ||\n+      (from->is_null_free()     && is_not_null_free()) ||\n+      (from->is_not_null_free() && is_null_free())) {\n+    return nullptr; \/\/ Inconsistent properties\n+  }\n+  const TypeAryPtr* res = this;\n+  if (from->is_not_null_free()) {\n+    res = res->cast_to_not_null_free();\n+  }\n+  if (from->is_not_flat()) {\n+    res = res->cast_to_not_flat();\n+  }\n+  return res;\n+}\n+\n+jint TypeAryPtr::flat_layout_helper() const {\n+  return exact_klass()->as_flat_array_klass()->layout_helper();\n+}\n+\n+int TypeAryPtr::flat_elem_size() const {\n+  return exact_klass()->as_flat_array_klass()->element_byte_size();\n+}\n+\n+int TypeAryPtr::flat_log_elem_size() const {\n+  return exact_klass()->as_flat_array_klass()->log2_element_size();\n@@ -4748,1 +5157,1 @@\n-  const TypeAry* new_ary = TypeAry::make(elem, size(), stable);\n+  const TypeAry* new_ary = TypeAry::make(elem, size(), stable, is_flat(), is_not_flat(), is_not_null_free(), is_atomic());\n@@ -4750,1 +5159,1 @@\n-  return make(ptr(), const_oop(), new_ary, klass(), klass_is_exact(), _offset, _instance_id, _speculative, _inline_depth);\n+  return make(ptr(), const_oop(), new_ary, klass(), klass_is_exact(), _offset, _field_offset, _instance_id, _speculative, _inline_depth, _is_autobox_cache);\n@@ -4770,2 +5179,2 @@\n-  const TypeAry* new_ary = TypeAry::make(etype, size(), is_stable());\n-  return make(ptr(), const_oop(), new_ary, klass(), klass_is_exact(), _offset, _instance_id, _speculative, _inline_depth, \/*is_autobox_cache=*\/true);\n+  const TypeAry* new_ary = TypeAry::make(etype, size(), is_stable(), is_flat(), is_not_flat(), is_not_null_free(), is_atomic());\n+  return make(ptr(), const_oop(), new_ary, klass(), klass_is_exact(), _offset, _field_offset, _instance_id, _speculative, _inline_depth, \/*is_autobox_cache=*\/true);\n@@ -4780,1 +5189,2 @@\n-    TypeOopPtr::eq(p);  \/\/ Check sub-parts\n+    TypeOopPtr::eq(p) &&\/\/ Check sub-parts\n+    _field_offset == p->_field_offset;\n@@ -4786,1 +5196,1 @@\n-  return (uint)(uintptr_t)_ary + TypeOopPtr::hash();\n+  return (uint)(uintptr_t)_ary + TypeOopPtr::hash() + _field_offset.get();\n@@ -4833,1 +5243,1 @@\n-    int offset = meet_offset(tp->offset());\n+    Offset offset = meet_offset(tp->offset());\n@@ -4842,1 +5252,1 @@\n-                  _ary, _klass, _klass_is_exact, offset, instance_id, speculative, depth);\n+                  _ary, _klass, _klass_is_exact, offset, _field_offset, instance_id, speculative, depth);\n@@ -4856,1 +5266,1 @@\n-    int offset = meet_offset(tp->offset());\n+    Offset offset = meet_offset(tp->offset());\n@@ -4872,1 +5282,1 @@\n-                  _ary, _klass, _klass_is_exact, offset, instance_id, speculative, depth);\n+                  _ary, _klass, _klass_is_exact, offset, _field_offset, instance_id, speculative, depth);\n@@ -4886,1 +5296,2 @@\n-    int off = meet_offset(tap->offset());\n+    Offset off = meet_offset(tap->offset());\n+    Offset field_off = meet_field_offset(tap->field_offset());\n@@ -4900,0 +5311,4 @@\n+    bool res_flat = false;\n+    bool res_not_flat = false;\n+    bool res_not_null_free = false;\n+    bool res_atomic = false;\n@@ -4901,1 +5316,1 @@\n-    if (meet_aryptr(ptr, elem, this, tap, res_klass, res_xk) == NOT_SUBTYPE) {\n+    if (meet_aryptr(ptr, elem, this, tap, res_klass, res_xk, res_flat, res_not_flat, res_not_null_free, res_atomic) == NOT_SUBTYPE) {\n@@ -4903,0 +5318,14 @@\n+    } else if (this->is_flat() != tap->is_flat()) {\n+      \/\/ Meeting flat inline type array with non-flat array. Adjust (field) offset accordingly.\n+      if (tary->_flat) {\n+        \/\/ Result is in a flat representation\n+        off = Offset(is_flat() ? offset() : tap->offset());\n+        field_off = is_flat() ? field_offset() : tap->field_offset();\n+      } else if (below_centerline(ptr)) {\n+        \/\/ Result is in a non-flat representation\n+        off = Offset(flat_offset()).meet(Offset(tap->flat_offset()));\n+        field_off = (field_off == Offset::top) ? Offset::top : Offset::bottom;\n+      } else if (flat_offset() == tap->flat_offset()) {\n+        off = Offset(!is_flat() ? offset() : tap->offset());\n+        field_off = !is_flat() ? field_offset() : tap->field_offset();\n+      }\n@@ -4920,1 +5349,1 @@\n-    return make(ptr, o, TypeAry::make(elem, tary->_size, tary->_stable), res_klass, res_xk, off, instance_id, speculative, depth);\n+    return make(ptr, o, TypeAry::make(elem, tary->_size, tary->_stable, res_flat, res_not_flat, res_not_null_free, res_atomic), res_klass, res_xk, off, field_off, instance_id, speculative, depth);\n@@ -4926,1 +5355,1 @@\n-    int offset = meet_offset(tp->offset());\n+    Offset offset = meet_offset(tp->offset());\n@@ -4941,2 +5370,14 @@\n-      if (tp->klass()->equals(ciEnv::current()->Object_klass()) && this_interfaces->contains(tp_interfaces) && !tp->klass_is_exact()) {\n-        return TypeAryPtr::make(ptr, _ary, _klass, _klass_is_exact, offset, instance_id, speculative, depth);\n+      \/\/\n+      \/\/ Flat in array:\n+      \/\/ We do\n+      \/\/   dual(TypeAryPtr) MEET dual(TypeInstPtr)\n+      \/\/ If TypeInstPtr is anything else than Object, then the result of the meet is bottom Object (i.e. we could have\n+      \/\/ instances or arrays).\n+      \/\/ If TypeInstPtr is an Object and either\n+      \/\/ - exact\n+      \/\/ - inexact AND flat in array == dual(not flat in array) (i.e. not an array type)\n+      \/\/ then the result of the meet is bottom Object (i.e. we could have instances or arrays).\n+      \/\/ Otherwise, we meet two array pointers and create a new TypeAryPtr.\n+      if (tp->klass()->equals(ciEnv::current()->Object_klass()) && this_interfaces->contains(tp_interfaces) &&\n+          !tp->klass_is_exact() && !tp->is_not_flat_in_array()) {\n+        return TypeAryPtr::make(ptr, _ary, _klass, _klass_is_exact, offset, _field_offset, instance_id, speculative, depth);\n@@ -4948,1 +5389,2 @@\n-        return TypeInstPtr::make(ptr, ciEnv::current()->Object_klass(), interfaces, false, nullptr,offset, instance_id, speculative, depth);\n+        FlatInArray flat_in_array = meet_flat_in_array(NotFlat, tp->flat_in_array());\n+        return TypeInstPtr::make(ptr, ciEnv::current()->Object_klass(), interfaces, false, nullptr, offset, flat_in_array, instance_id, speculative, depth);\n@@ -4952,1 +5394,1 @@\n-    case BotPTR:                \/\/ Fall down to object klass\n+    case BotPTR: { \/\/ Fall down to object klass\n@@ -4960,1 +5402,4 @@\n-        if (tp->klass()->equals(ciEnv::current()->Object_klass()) && this_interfaces->contains(tp_interfaces) && !tp->klass_is_exact()) {\n+\n+        \/\/ Flat in array: We do TypeAryPtr MEET dual(TypeInstPtr), same applies as above in TopPTR\/AnyNull case.\n+        if (tp->klass()->equals(ciEnv::current()->Object_klass()) && this_interfaces->contains(tp_interfaces) &&\n+            !tp->klass_is_exact() && !tp->is_not_flat_in_array()) {\n@@ -4963,1 +5408,1 @@\n-                      _ary, _klass, _klass_is_exact, offset, instance_id, speculative, depth);\n+                      _ary, _klass, _klass_is_exact, offset, _field_offset, instance_id, speculative, depth);\n@@ -4969,1 +5414,1 @@\n-         ptr = NotNull;\n+        ptr = NotNull;\n@@ -4974,0 +5419,2 @@\n+\n+      FlatInArray flat_in_array = meet_flat_in_array(NotFlat, tp->flat_in_array());\n@@ -4975,1 +5422,3 @@\n-      return TypeInstPtr::make(ptr, ciEnv::current()->Object_klass(), interfaces, false, nullptr, offset, instance_id, speculative, depth);\n+      return TypeInstPtr::make(ptr, ciEnv::current()->Object_klass(), interfaces, false, nullptr, offset,\n+                               flat_in_array, instance_id, speculative, depth);\n+    }\n@@ -4984,2 +5433,2 @@\n-template<class T> TypePtr::MeetResult TypePtr::meet_aryptr(PTR& ptr, const Type*& elem, const T* this_ary,\n-                                                           const T* other_ary, ciKlass*& res_klass, bool& res_xk) {\n+template<class T> TypePtr::MeetResult TypePtr::meet_aryptr(PTR& ptr, const Type*& elem, const T* this_ary, const T* other_ary,\n+                                                           ciKlass*& res_klass, bool& res_xk, bool &res_flat, bool& res_not_flat, bool& res_not_null_free, bool &res_atomic) {\n@@ -4995,0 +5444,9 @@\n+  bool this_flat = this_ary->is_flat();\n+  bool this_not_flat = this_ary->is_not_flat();\n+  bool other_flat = other_ary->is_flat();\n+  bool other_not_flat = other_ary->is_not_flat();\n+  bool this_not_null_free = this_ary->is_not_null_free();\n+  bool other_not_null_free = other_ary->is_not_null_free();\n+  bool this_atomic = this_ary->is_atomic();\n+  bool other_atomic = other_ary->is_atomic();\n+  const bool same_nullness = this_ary->is_null_free() == other_ary->is_null_free();\n@@ -4997,0 +5455,6 @@\n+  res_flat = this_flat && other_flat;\n+  bool res_null_free = this_ary->is_null_free() && other_ary->is_null_free();\n+  res_not_flat = this_not_flat && other_not_flat;\n+  res_not_null_free = this_not_null_free && other_not_null_free;\n+  res_atomic = this_atomic && other_atomic;\n+\n@@ -5000,3 +5464,3 @@\n-    if (this_top_or_bottom)\n-      res_klass = other_klass;\n-    else if (other_top_or_bottom || other_klass == this_klass) {\n+      if (this_top_or_bottom) {\n+        res_klass = other_klass;\n+      } else if (other_top_or_bottom || other_klass == this_klass) {\n@@ -5044,0 +5508,3 @@\n+        if (this_ary->is_flat()) {\n+          elem = this_ary->elem();\n+        }\n@@ -5047,1 +5514,1 @@\n-      return result;\n+      break;\n@@ -5049,1 +5516,3 @@\n-      if (this_ptr == Constant) {\n+      if (this_ptr == Constant && same_nullness) {\n+        \/\/ Only exact if same nullness since:\n+        \/\/     null-free [LMyValue <: nullable [LMyValue.\n@@ -5051,1 +5520,1 @@\n-      } else if(above_centerline(this_ptr)) {\n+      } else if (above_centerline(this_ptr)) {\n@@ -5056,0 +5525,6 @@\n+        \/\/ Even though MyValue is final, [LMyValue is only exact if the array\n+        \/\/ is (not) null-free due to null-free [LMyValue <: null-able [LMyValue.\n+        if (res_xk && !res_null_free && !res_not_null_free) {\n+          ptr = NotNull;\n+          res_xk = false;\n+        }\n@@ -5057,1 +5532,1 @@\n-      return result;\n+      break;\n@@ -5064,0 +5539,3 @@\n+        if (other_ary->is_flat()) {\n+          elem = other_ary->elem();\n+        }\n@@ -5067,0 +5545,6 @@\n+        \/\/ Even though MyValue is final, [LMyValue is only exact if the array\n+        \/\/ is (not) null-free due to null-free [LMyValue <: null-able [LMyValue.\n+        if (res_xk && !res_null_free && !res_not_null_free) {\n+          ptr = NotNull;\n+          res_xk = false;\n+        }\n@@ -5068,1 +5552,1 @@\n-      return result;\n+      break;\n@@ -5081,1 +5565,11 @@\n-  return new TypeAryPtr(dual_ptr(), _const_oop, _ary->dual()->is_ary(),_klass, _klass_is_exact, dual_offset(), dual_instance_id(), is_autobox_cache(), dual_speculative(), dual_inline_depth());\n+  bool xk = _klass_is_exact;\n+  return new TypeAryPtr(dual_ptr(), _const_oop, _ary->dual()->is_ary(), _klass, xk, dual_offset(), dual_field_offset(), dual_instance_id(), is_autobox_cache(), dual_speculative(), dual_inline_depth());\n+}\n+\n+Type::Offset TypeAryPtr::meet_field_offset(const Type::Offset offset) const {\n+  return _field_offset.meet(offset);\n+}\n+\n+\/\/------------------------------dual_offset------------------------------------\n+Type::Offset TypeAryPtr::dual_field_offset() const {\n+  return _field_offset.dual();\n@@ -5100,1 +5594,23 @@\n-  if( _offset != 0 ) {\n+  if (is_flat()) {\n+    st->print(\":flat\");\n+    st->print(\"(\");\n+    _field_offset.dump2(st);\n+    st->print(\")\");\n+  } else if (is_not_flat()) {\n+    st->print(\":not_flat\");\n+  }\n+  if (is_null_free()) {\n+    st->print(\":null free\");\n+  }\n+  if (is_atomic()) {\n+    st->print(\":atomic\");\n+  }\n+  if (Verbose) {\n+    if (is_not_flat()) {\n+      st->print(\":not flat\");\n+    }\n+    if (is_not_null_free()) {\n+      st->print(\":nullable\");\n+    }\n+  }\n+  if (offset() != 0) {\n@@ -5103,3 +5619,3 @@\n-    if( _offset == OffsetTop )       st->print(\"+undefined\");\n-    else if( _offset == OffsetBot )  st->print(\"+any\");\n-    else if( _offset < header_size ) st->print(\"+%d\", _offset);\n+    if( _offset == Offset::top )       st->print(\"+undefined\");\n+    else if( _offset == Offset::bottom )  st->print(\"+any\");\n+    else if( offset() < header_size ) st->print(\"+%d\", offset());\n@@ -5111,1 +5627,1 @@\n-        st->print(\"[%d]\", (_offset - header_size)\/elem_size);\n+        st->print(\"[%d]\", (offset() - header_size)\/elem_size);\n@@ -5124,0 +5640,4 @@\n+  \/\/ FIXME: Does this belong here? Or in the meet code itself?\n+  if (is_flat() && is_not_flat()) {\n+    return true;\n+  }\n@@ -5129,1 +5649,1 @@\n-  return make(_ptr, _const_oop, _ary, _klass, _klass_is_exact, xadd_offset(offset), _instance_id, add_offset_speculative(offset), _inline_depth);\n+  return make(_ptr, _const_oop, _ary, _klass, _klass_is_exact, xadd_offset(offset), _field_offset, _instance_id, add_offset_speculative(offset), _inline_depth, _is_autobox_cache);\n@@ -5133,1 +5653,1 @@\n-  return make(_ptr, _const_oop, _ary, _klass, _klass_is_exact, offset, _instance_id, with_offset_speculative(offset), _inline_depth);\n+  return make(_ptr, _const_oop, _ary, _klass, _klass_is_exact, Offset(offset), _field_offset, _instance_id, with_offset_speculative(offset), _inline_depth, _is_autobox_cache);\n@@ -5137,1 +5657,1 @@\n-  return make(_ptr, _const_oop, ary, _klass, _klass_is_exact, _offset, _instance_id, _speculative, _inline_depth);\n+  return make(_ptr, _const_oop, ary, _klass, _klass_is_exact, _offset, _field_offset, _instance_id, _speculative, _inline_depth, _is_autobox_cache);\n@@ -5145,1 +5665,14 @@\n-  return make(_ptr, _const_oop, _ary->remove_speculative()->is_ary(), _klass, _klass_is_exact, _offset, _instance_id, nullptr, _inline_depth);\n+  return make(_ptr, _const_oop, _ary->remove_speculative()->is_ary(), _klass, _klass_is_exact, _offset, _field_offset, _instance_id, nullptr, _inline_depth, _is_autobox_cache);\n+}\n+\n+const Type* TypeAryPtr::cleanup_speculative() const {\n+  if (speculative() == nullptr) {\n+    return this;\n+  }\n+  \/\/ Keep speculative part if it contains information about flat-\/nullability\n+  const TypeAryPtr* spec_aryptr = speculative()->isa_aryptr();\n+  if (spec_aryptr != nullptr && !above_centerline(spec_aryptr->ptr()) &&\n+      (spec_aryptr->is_not_flat() || spec_aryptr->is_not_null_free())) {\n+    return this;\n+  }\n+  return TypeOopPtr::cleanup_speculative();\n@@ -5152,1 +5685,44 @@\n-  return make(_ptr, _const_oop, _ary->remove_speculative()->is_ary(), _klass, _klass_is_exact, _offset, _instance_id, _speculative, depth);\n+  return make(_ptr, _const_oop, _ary->remove_speculative()->is_ary(), _klass, _klass_is_exact, _offset, _field_offset, _instance_id, _speculative, depth, _is_autobox_cache);\n+}\n+\n+const TypeAryPtr* TypeAryPtr::with_field_offset(int offset) const {\n+  return make(_ptr, _const_oop, _ary->remove_speculative()->is_ary(), _klass, _klass_is_exact, _offset, Offset(offset), _instance_id, _speculative, _inline_depth, _is_autobox_cache);\n+}\n+\n+const TypePtr* TypeAryPtr::add_field_offset_and_offset(intptr_t offset) const {\n+  int adj = 0;\n+  if (is_flat() && klass_is_exact() && offset != Type::OffsetBot && offset != Type::OffsetTop) {\n+    if (_offset.get() != OffsetBot && _offset.get() != OffsetTop) {\n+      adj = _offset.get();\n+      offset += _offset.get();\n+    }\n+    uint header = arrayOopDesc::base_offset_in_bytes(T_FLAT_ELEMENT);\n+    if (_field_offset.get() != OffsetBot && _field_offset.get() != OffsetTop) {\n+      offset += _field_offset.get();\n+      if (_offset.get() == OffsetBot || _offset.get() == OffsetTop) {\n+        offset += header;\n+      }\n+    }\n+    if (elem()->make_oopptr()->is_inlinetypeptr() && (offset >= (intptr_t)header || offset < 0)) {\n+      \/\/ Try to get the field of the inline type array element we are pointing to\n+      ciInlineKlass* vk = elem()->inline_klass();\n+      int shift = flat_log_elem_size();\n+      int mask = (1 << shift) - 1;\n+      intptr_t field_offset = ((offset - header) & mask);\n+      ciField* field = vk->get_field_by_offset(field_offset + vk->payload_offset(), false);\n+      if (field != nullptr || field_offset == vk->null_marker_offset_in_payload()) {\n+        return with_field_offset(field_offset)->add_offset(offset - field_offset - adj);\n+      }\n+    }\n+  }\n+  return add_offset(offset - adj);\n+}\n+\n+\/\/ Return offset incremented by field_offset for flat inline type arrays\n+int TypeAryPtr::flat_offset() const {\n+  int offset = _offset.get();\n+  if (offset != Type::OffsetBot && offset != Type::OffsetTop &&\n+      _field_offset != Offset::bottom && _field_offset != Offset::top) {\n+    offset += _field_offset.get();\n+  }\n+  return offset;\n@@ -5157,1 +5733,1 @@\n-  return make(_ptr, _const_oop, _ary->remove_speculative()->is_ary(), _klass, _klass_is_exact, _offset, instance_id, _speculative, _inline_depth);\n+  return make(_ptr, _const_oop, _ary->remove_speculative()->is_ary(), _klass, _klass_is_exact, _offset, _field_offset, instance_id, _speculative, _inline_depth);\n@@ -5162,0 +5738,1 @@\n+\n@@ -5255,1 +5832,0 @@\n-\n@@ -5339,1 +5915,1 @@\n-  return (_offset == 0) && !below_centerline(_ptr);\n+  return (offset() == 0) && !below_centerline(_ptr);\n@@ -5359,1 +5935,1 @@\n-  assert( _offset >= 0, \"\" );\n+  assert(offset() >= 0, \"\");\n@@ -5361,1 +5937,1 @@\n-  if (_offset != 0) {\n+  if (offset() != 0) {\n@@ -5415,1 +5991,1 @@\n-    int offset = meet_offset(tp->offset());\n+    Offset offset = meet_offset(tp->offset());\n@@ -5443,1 +6019,1 @@\n-    int offset = meet_offset(tp->offset());\n+    Offset offset = meet_offset(tp->offset());\n@@ -5487,1 +6063,1 @@\n-TypeMetadataPtr::TypeMetadataPtr(PTR ptr, ciMetadata* metadata, int offset):\n+TypeMetadataPtr::TypeMetadataPtr(PTR ptr, ciMetadata* metadata, Offset offset):\n@@ -5492,1 +6068,1 @@\n-  return make(Constant, m, 0);\n+  return make(Constant, m, Offset(0));\n@@ -5495,1 +6071,1 @@\n-  return make(Constant, m, 0);\n+  return make(Constant, m, Offset(0));\n@@ -5500,1 +6076,1 @@\n-const TypeMetadataPtr *TypeMetadataPtr::make(PTR ptr, ciMetadata* m, int offset) {\n+const TypeMetadataPtr* TypeMetadataPtr::make(PTR ptr, ciMetadata* m, Offset offset) {\n@@ -5511,1 +6087,6 @@\n-    if (elem->is_klassptr()->klass_is_exact()) {\n+    if (elem->is_klassptr()->klass_is_exact() &&\n+        \/\/ Even though MyValue is final, [LMyValue is only exact if the array\n+        \/\/ is (not) null-free due to null-free [LMyValue <: null-able [LMyValue.\n+        \/\/ TODO 8350865 If we know that the array can't be null-free, it's allowed to be exact, right?\n+        \/\/ If so, we should add '|| is_not_null_free()'\n+        (is_null_free() || !_ary->_elem->make_oopptr()->is_inlinetypeptr())) {\n@@ -5515,1 +6096,1 @@\n-  return TypeAryKlassPtr::make(xk ? TypePtr::Constant : TypePtr::NotNull, elem, klass(), 0);\n+  return TypeAryKlassPtr::make(xk ? TypePtr::Constant : TypePtr::NotNull, elem, klass(), Offset(0), is_not_flat(), is_not_null_free(), is_flat(), is_null_free(), is_atomic(), is_flat() || is_null_free());\n@@ -5518,1 +6099,1 @@\n-const TypeKlassPtr* TypeKlassPtr::make(ciKlass *klass, InterfaceHandling interface_handling) {\n+const TypeKlassPtr* TypeKlassPtr::make(ciKlass* klass, InterfaceHandling interface_handling) {\n@@ -5525,1 +6106,1 @@\n-const TypeKlassPtr* TypeKlassPtr::make(PTR ptr, ciKlass* klass, int offset, InterfaceHandling interface_handling) {\n+const TypeKlassPtr* TypeKlassPtr::make(PTR ptr, ciKlass* klass, Offset offset, InterfaceHandling interface_handling) {\n@@ -5533,3 +6114,1 @@\n-\n-\/\/------------------------------TypeKlassPtr-----------------------------------\n-TypeKlassPtr::TypeKlassPtr(TYPES t, PTR ptr, ciKlass* klass, const TypeInterfaces* interfaces, int offset)\n+TypeKlassPtr::TypeKlassPtr(TYPES t, PTR ptr, ciKlass* klass, const TypeInterfaces* interfaces, Offset offset)\n@@ -5538,1 +6117,1 @@\n-         klass->is_type_array_klass() || !klass->as_obj_array_klass()->base_element_klass()->is_interface(), \"no interface here\");\n+         klass->is_type_array_klass() || klass->is_flat_array_klass() || !klass->as_obj_array_klass()->base_element_klass()->is_interface(), \"no interface here\");\n@@ -5577,1 +6156,1 @@\n-  return (_offset == 0) && !below_centerline(_ptr);\n+  return (offset() == 0) && !below_centerline(_ptr);\n@@ -5607,1 +6186,1 @@\n-  assert( _offset >= 0, \"\" );\n+  assert( offset() >= 0, \"\" );\n@@ -5609,1 +6188,1 @@\n-  if (_offset != 0) {\n+  if (offset() != 0) {\n@@ -5634,1 +6213,1 @@\n-  const TypeKlassPtr *p = t->is_klassptr();\n+  const TypeInstKlassPtr* p = t->is_instklassptr();\n@@ -5637,0 +6216,1 @@\n+    _flat_in_array == p->_flat_in_array &&\n@@ -5640,2 +6220,2 @@\n-uint TypeInstKlassPtr::hash(void) const {\n-  return klass()->hash() + TypeKlassPtr::hash();\n+uint TypeInstKlassPtr::hash() const {\n+  return klass()->hash() + TypeKlassPtr::hash() + static_cast<uint>(_flat_in_array);\n@@ -5644,1 +6224,4 @@\n-const TypeInstKlassPtr *TypeInstKlassPtr::make(PTR ptr, ciKlass* k, const TypeInterfaces* interfaces, int offset) {\n+const TypeInstKlassPtr *TypeInstKlassPtr::make(PTR ptr, ciKlass* k, const TypeInterfaces* interfaces, Offset offset, FlatInArray flat_in_array) {\n+  if (flat_in_array == Uninitialized) {\n+    flat_in_array = compute_flat_in_array(k->as_instance_klass(), ptr == Constant);\n+  }\n@@ -5646,1 +6229,1 @@\n-    (TypeInstKlassPtr*)(new TypeInstKlassPtr(ptr, k, interfaces, offset))->hashcons();\n+    (TypeInstKlassPtr*)(new TypeInstKlassPtr(ptr, k, interfaces, offset, flat_in_array))->hashcons();\n@@ -5651,0 +6234,7 @@\n+bool TypeInstKlassPtr::empty() const {\n+  if (_flat_in_array == TopFlat) {\n+    return true;\n+  }\n+  return TypeKlassPtr::empty();\n+}\n+\n@@ -5653,2 +6243,2 @@\n-const TypePtr* TypeInstKlassPtr::add_offset( intptr_t offset ) const {\n-  return make( _ptr, klass(), _interfaces, xadd_offset(offset) );\n+const TypePtr *TypeInstKlassPtr::add_offset( intptr_t offset ) const {\n+  return make(_ptr, klass(), _interfaces, xadd_offset(offset), _flat_in_array);\n@@ -5658,1 +6248,1 @@\n-  return make(_ptr, klass(), _interfaces, offset);\n+  return make(_ptr, klass(), _interfaces, Offset(offset), _flat_in_array);\n@@ -5665,1 +6255,1 @@\n-  return make(ptr, _klass, _interfaces, _offset);\n+  return make(ptr, _klass, _interfaces, _offset, _flat_in_array);\n@@ -5681,1 +6271,2 @@\n-  return make(klass_is_exact ? Constant : NotNull, k, _interfaces, _offset);\n+  FlatInArray flat_in_array = compute_flat_in_array(k->as_instance_klass(), klass_is_exact);\n+  return make(klass_is_exact ? Constant : NotNull, k, _interfaces, _offset, flat_in_array);\n@@ -5697,0 +6288,1 @@\n+  ciInstanceKlass* ik = k->as_instance_klass();\n@@ -5699,1 +6291,0 @@\n-    ciInstanceKlass* ik = k->as_instance_klass();\n@@ -5713,1 +6304,3 @@\n-  return TypeInstPtr::make(TypePtr::BotPTR, k, interfaces, xk, nullptr, 0);\n+\n+  FlatInArray flat_in_array = compute_flat_in_array_if_unknown(ik, xk, _flat_in_array);\n+  return TypeInstPtr::make(TypePtr::BotPTR, k, interfaces, xk, nullptr, Offset(0), flat_in_array);\n@@ -5749,1 +6342,1 @@\n-    int offset = meet_offset(tp->offset());\n+    Offset offset = meet_offset(tp->offset());\n@@ -5757,1 +6350,1 @@\n-      return make( ptr, klass(), _interfaces, offset );\n+      return make(ptr, klass(), _interfaces, offset, _flat_in_array);\n@@ -5770,1 +6363,1 @@\n-    return TypePtr::BOTTOM;\n+      return TypePtr::BOTTOM;\n@@ -5790,1 +6383,1 @@\n-    int  off     = meet_offset(tkls->offset());\n+    Offset  off     = meet_offset(tkls->offset());\n@@ -5796,1 +6389,2 @@\n-    switch(meet_instptr(ptr, interfaces, this, tkls, res_klass, res_xk)) {\n+    const FlatInArray flat_in_array = meet_flat_in_array(_flat_in_array, tkls->flat_in_array());\n+    switch (meet_instptr(ptr, interfaces, this, tkls, res_klass, res_xk)) {\n@@ -5804,1 +6398,1 @@\n-        const Type* res = make(ptr, res_klass, interfaces, off);\n+        const Type* res = make(ptr, res_klass, interfaces, off, flat_in_array);\n@@ -5813,1 +6407,1 @@\n-    int offset = meet_offset(tp->offset());\n+    Offset offset = meet_offset(tp->offset());\n@@ -5825,2 +6419,5 @@\n-      if (klass()->equals(ciEnv::current()->Object_klass()) && tp_interfaces->contains(this_interfaces) && !klass_is_exact()) {\n-        return TypeAryKlassPtr::make(ptr, tp->elem(), tp->klass(), offset);\n+      \/\/\n+      \/\/ Flat in array: See explanation for meet with TypeInstPtr in TypeAryPtr::xmeet_helper().\n+      if (klass()->equals(ciEnv::current()->Object_klass()) && tp_interfaces->contains(this_interfaces) &&\n+          !klass_is_exact() && !is_not_flat_in_array()) {\n+        return TypeAryKlassPtr::make(ptr, tp->elem(), tp->klass(), offset, tp->is_not_flat(), tp->is_not_null_free(), tp->is_flat(), tp->is_null_free(), tp->is_atomic(), tp->is_refined_type());\n@@ -5831,1 +6428,2 @@\n-        return make(ptr, ciEnv::current()->Object_klass(), interfaces, offset);\n+        FlatInArray flat_in_array = meet_flat_in_array(_flat_in_array, NotFlat);\n+        return make(ptr, ciEnv::current()->Object_klass(), interfaces, offset, flat_in_array);\n@@ -5835,1 +6433,1 @@\n-    case BotPTR:                \/\/ Fall down to object klass\n+    case BotPTR: { \/\/ Fall down to object klass\n@@ -5843,1 +6441,4 @@\n-        if (klass()->equals(ciEnv::current()->Object_klass()) && tp_interfaces->contains(this_interfaces) && !klass_is_exact()) {\n+        \/\/\n+        \/\/ Flat in array: See explanation for meet with TypeInstPtr in TypeAryPtr::xmeet_helper().\n+        if (klass()->equals(ciEnv::current()->Object_klass()) && tp_interfaces->contains(this_interfaces) &&\n+            !klass_is_exact() && !is_not_flat_in_array()) {\n@@ -5845,2 +6446,1 @@\n-          return TypeAryKlassPtr::make(ptr,\n-                                       tp->elem(), tp->klass(), offset);\n+          return TypeAryKlassPtr::make(ptr, tp->elem(), tp->klass(), offset, tp->is_not_flat(), tp->is_not_null_free(), tp->is_flat(), tp->is_null_free(), tp->is_atomic(), tp->is_refined_type());\n@@ -5852,1 +6452,1 @@\n-         ptr = NotNull;\n+        ptr = NotNull;\n@@ -5854,1 +6454,3 @@\n-      return make(ptr, ciEnv::current()->Object_klass(), interfaces, offset);\n+      FlatInArray flat_in_array = meet_flat_in_array(_flat_in_array, NotFlat);\n+      return make(ptr, ciEnv::current()->Object_klass(), interfaces, offset, flat_in_array);\n+    }\n@@ -5865,2 +6467,2 @@\n-const Type    *TypeInstKlassPtr::xdual() const {\n-  return new TypeInstKlassPtr(dual_ptr(), klass(), _interfaces, dual_offset());\n+const Type* TypeInstKlassPtr::xdual() const {\n+  return new TypeInstKlassPtr(dual_ptr(), klass(), _interfaces, dual_offset(), dual_flat_in_array());\n@@ -5980,0 +6582,4 @@\n+bool TypeInstKlassPtr::can_be_inline_array() const {\n+  return _klass->equals(ciEnv::current()->Object_klass()) && TypeAryKlassPtr::_array_interfaces->contains(_interfaces);\n+}\n+\n@@ -5987,0 +6593,1 @@\n+  dump_flat_in_array(_flat_in_array, st);\n@@ -5990,2 +6597,6 @@\n-const TypeAryKlassPtr *TypeAryKlassPtr::make(PTR ptr, const Type* elem, ciKlass* k, int offset) {\n-  return (TypeAryKlassPtr*)(new TypeAryKlassPtr(ptr, elem, k, offset))->hashcons();\n+bool TypeAryKlassPtr::can_be_inline_array() const {\n+  return _elem->isa_instklassptr() && _elem->is_instklassptr()->_klass->can_be_inline_klass();\n+}\n+\n+bool TypeInstPtr::can_be_inline_array() const {\n+  return _klass->equals(ciEnv::current()->Object_klass()) && TypeAryPtr::_array_interfaces->contains(_interfaces);\n@@ -5994,1 +6605,9 @@\n-const TypeAryKlassPtr *TypeAryKlassPtr::make(PTR ptr, ciKlass* k, int offset, InterfaceHandling interface_handling) {\n+bool TypeAryPtr::can_be_inline_array() const {\n+  return elem()->make_ptr() && elem()->make_ptr()->isa_instptr() && elem()->make_ptr()->is_instptr()->_klass->can_be_inline_klass();\n+}\n+\n+const TypeAryKlassPtr *TypeAryKlassPtr::make(PTR ptr, const Type* elem, ciKlass* k, Offset offset, bool not_flat, bool not_null_free, bool flat, bool null_free, bool atomic, bool refined_type) {\n+  return (TypeAryKlassPtr*)(new TypeAryKlassPtr(ptr, elem, k, offset, not_flat, not_null_free, flat, null_free, atomic, refined_type))->hashcons();\n+}\n+\n+const TypeAryKlassPtr* TypeAryKlassPtr::make(PTR ptr, ciKlass* k, Offset offset, InterfaceHandling interface_handling, bool not_flat, bool not_null_free, bool flat, bool null_free, bool atomic, bool refined_type) {\n@@ -5998,2 +6617,2 @@\n-    const TypeKlassPtr *etype = TypeKlassPtr::make(eklass, interface_handling)->cast_to_exactness(false);\n-    return TypeAryKlassPtr::make(ptr, etype, nullptr, offset);\n+    const TypeKlassPtr* etype = TypeKlassPtr::make(eklass, interface_handling)->cast_to_exactness(false);\n+    return TypeAryKlassPtr::make(ptr, etype, nullptr, offset, not_flat, not_null_free, flat, null_free, atomic, refined_type);\n@@ -6003,1 +6622,5 @@\n-    return TypeAryKlassPtr::make(ptr, etype, k, offset);\n+    return TypeAryKlassPtr::make(ptr, etype, k, offset, not_flat, not_null_free, flat, null_free, atomic);\n+  } else if (k->is_flat_array_klass()) {\n+    ciKlass* eklass = k->as_flat_array_klass()->element_klass();\n+    const TypeKlassPtr* etype = TypeKlassPtr::make(eklass, interface_handling)->cast_to_exactness(false);\n+    return TypeAryKlassPtr::make(ptr, etype, k, offset, not_flat, not_null_free, flat, null_free, atomic, refined_type);\n@@ -6010,2 +6633,30 @@\n-const TypeAryKlassPtr* TypeAryKlassPtr::make(ciKlass* klass, InterfaceHandling interface_handling) {\n-  return TypeAryKlassPtr::make(Constant, klass, 0, interface_handling);\n+const TypeAryKlassPtr* TypeAryKlassPtr::make(PTR ptr, ciKlass* k, Offset offset, InterfaceHandling interface_handling, bool refined_type) {\n+  bool flat = k->is_flat_array_klass();\n+  bool null_free = k->as_array_klass()->is_elem_null_free();\n+  bool atomic = k->as_array_klass()->is_elem_atomic();\n+\n+  bool not_inline = k->is_type_array_klass() || !k->as_array_klass()->element_klass()->can_be_inline_klass(false);\n+  bool not_null_free = (ptr == Constant) ? !null_free : not_inline;\n+  bool not_flat = (ptr == Constant) ? !flat : (!UseArrayFlattening || not_inline ||\n+                   (k->as_array_klass()->element_klass() != nullptr &&\n+                    k->as_array_klass()->element_klass()->is_inlinetype() &&\n+                   !k->as_array_klass()->element_klass()->maybe_flat_in_array()));\n+\n+  return TypeAryKlassPtr::make(ptr, k, offset, interface_handling, not_flat, not_null_free, flat, null_free, atomic, refined_type);\n+}\n+\n+const TypeAryKlassPtr* TypeAryKlassPtr::make(ciKlass* klass, InterfaceHandling interface_handling, bool refined_type) {\n+  return TypeAryKlassPtr::make(Constant, klass, Offset(0), interface_handling, refined_type);\n+}\n+\n+\/\/ Get the (non-)refined array klass ptr\n+const TypeAryKlassPtr* TypeAryKlassPtr::cast_to_refined_array_klass_ptr(bool refined) const {\n+  if ((refined == is_refined_type()) || !klass_is_exact() || (!exact_klass()->is_obj_array_klass() && !exact_klass()->is_flat_array_klass())) {\n+    return this;\n+  }\n+  ciKlass* eklass = elem()->is_klassptr()->exact_klass_helper();\n+  if (elem()->isa_aryklassptr()) {\n+    eklass = exact_klass()->as_obj_array_klass()->element_klass();\n+  }\n+  ciKlass* array_klass = ciArrayKlass::make(eklass, eklass->is_inlinetype() ? is_null_free() : false, eklass->is_inlinetype() ? is_atomic() : true, refined);\n+  return make(_ptr, array_klass, Offset(0), trust_interfaces, refined);\n@@ -6020,0 +6671,6 @@\n+    _flat == p->_flat &&\n+    _not_flat == p->_not_flat &&\n+    _null_free == p->_null_free &&\n+    _not_null_free == p->_not_null_free &&\n+    _atomic == p->_atomic &&\n+    _refined_type == p->_refined_type &&\n@@ -6026,1 +6683,2 @@\n-  return (uint)(uintptr_t)_elem + TypeKlassPtr::hash();\n+  return (uint)(uintptr_t)_elem + TypeKlassPtr::hash() + (uint)(_not_flat ? 43 : 0) +\n+      (uint)(_not_null_free ? 44 : 0) + (uint)(_flat ? 45 : 0) + (uint)(_null_free ? 46 : 0)  + (uint)(_atomic ? 47 : 0) + (uint)(_refined_type ? 48 : 0);\n@@ -6043,1 +6701,1 @@\n-    \/\/ Leave k_ary at null.\n+    \/\/ Leave k_ary at nullptr.\n@@ -6045,1 +6703,1 @@\n-    \/\/ Leave k_ary at null.\n+    \/\/ Leave k_ary at nullptr.\n@@ -6093,1 +6751,1 @@\n-    k = ciObjArrayKlass::make(k);\n+    k = ciArrayKlass::make(k, is_null_free(), is_atomic(), is_flat() || is_null_free());\n@@ -6113,1 +6771,1 @@\n-  return make(_ptr, elem(), klass(), xadd_offset(offset));\n+  return make(_ptr, elem(), klass(), xadd_offset(offset), is_not_flat(), is_not_null_free(), _flat, _null_free, _atomic, _refined_type);\n@@ -6117,1 +6775,1 @@\n-  return make(_ptr, elem(), klass(), offset);\n+  return make(_ptr, elem(), klass(), Offset(offset), is_not_flat(), is_not_null_free(), _flat, _null_free, _atomic, _refined_type);\n@@ -6124,1 +6782,1 @@\n-  return make(ptr, elem(), _klass, _offset);\n+  return make(ptr, elem(), _klass, _offset, is_not_flat(), is_not_null_free(), _flat, _null_free, _atomic, _refined_type);\n@@ -6132,0 +6790,7 @@\n+  \/\/ Even though MyValue is final, [LMyValue is only exact if the array\n+  \/\/ is (not) null-free due to null-free [LMyValue <: null-able [LMyValue.\n+  \/\/ TODO 8350865 If we know that the array can't be null-free, it's allowed to be exact, right?\n+  \/\/ If so, we should add '&& !is_not_null_free()'\n+  if (tk->isa_instklassptr() && tk->klass()->is_inlinetype() && !is_null_free()) {\n+    return false;\n+  }\n@@ -6138,1 +6803,4 @@\n-  if (must_be_exact()) return this;  \/\/ cannot clear xk\n+  if (must_be_exact() && !klass_is_exact) return this;  \/\/ cannot clear xk\n+  if (klass_is_exact == this->klass_is_exact()) {\n+    return this;\n+  }\n@@ -6144,1 +6812,19 @@\n-  return make(klass_is_exact ? Constant : NotNull, elem, k, _offset);\n+  bool not_flat = is_not_flat();\n+  bool not_null_free = is_not_null_free();\n+  if (_elem->isa_klassptr()) {\n+    if (klass_is_exact || _elem->isa_aryklassptr()) {\n+      assert((!is_null_free() && !is_flat()) ||\n+             _elem->is_klassptr()->klass()->is_abstract() || _elem->is_klassptr()->klass()->is_java_lang_Object(),\n+             \"null-free (or flat) concrete inline type arrays should always be exact\");\n+      \/\/ An array can't be null-free (or flat) if the klass is exact\n+      not_null_free = true;\n+      not_flat = true;\n+    } else {\n+      \/\/ Klass is not exact (anymore), re-compute null-free\/flat properties\n+      const TypeOopPtr* exact_etype = TypeOopPtr::make_from_klass_unique(_elem->is_instklassptr()->instance_klass());\n+      bool not_inline = !exact_etype->can_be_inline_type();\n+      not_null_free = not_inline;\n+      not_flat = !UseArrayFlattening || not_inline || (exact_etype->is_inlinetypeptr() && !exact_etype->inline_klass()->maybe_flat_in_array());\n+    }\n+  }\n+  return make(klass_is_exact ? Constant : NotNull, elem, k, _offset, not_flat, not_null_free, _flat, _null_free, _atomic, _refined_type);\n@@ -6147,1 +6833,0 @@\n-\n@@ -6161,1 +6846,5 @@\n-  return TypeAryPtr::make(TypePtr::BotPTR, TypeAry::make(el, TypeInt::POS), k, xk, 0);\n+  bool null_free = _null_free;\n+  if (null_free && el->isa_ptr()) {\n+    el = el->is_ptr()->join_speculative(TypePtr::NOTNULL);\n+  }\n+  return TypeAryPtr::make(TypePtr::BotPTR, TypeAry::make(el, TypeInt::POS, false, is_flat(), is_not_flat(), is_not_null_free(), is_atomic()), k, xk, Offset(0));\n@@ -6198,1 +6887,1 @@\n-    int offset = meet_offset(tp->offset());\n+    Offset offset = meet_offset(tp->offset());\n@@ -6206,1 +6895,1 @@\n-      return make( ptr, _elem, klass(), offset );\n+      return make(ptr, _elem, klass(), offset, is_not_flat(), is_not_null_free(), is_flat(), is_null_free(), is_atomic(), is_refined_type());\n@@ -6239,1 +6928,1 @@\n-    int off = meet_offset(tap->offset());\n+    Offset off = meet_offset(tap->offset());\n@@ -6241,1 +6930,0 @@\n-\n@@ -6245,1 +6933,6 @@\n-    meet_aryptr(ptr, elem, this, tap, res_klass, res_xk);\n+    bool res_flat = false;\n+    bool res_not_flat = false;\n+    bool res_not_null_free = false;\n+    bool res_atomic = false;\n+    MeetResult res = meet_aryptr(ptr, elem, this, tap,\n+                                 res_klass, res_xk, res_flat, res_not_flat, res_not_null_free, res_atomic);\n@@ -6247,1 +6940,33 @@\n-    return make(ptr, elem, res_klass, off);\n+    bool flat = meet_flat(tap->_flat);\n+    bool null_free = meet_null_free(tap->_null_free);\n+    bool atomic = meet_atomic(tap->_atomic);\n+    bool refined_type = _refined_type && tap->_refined_type;\n+    if (res == NOT_SUBTYPE) {\n+      flat = false;\n+      null_free = false;\n+      atomic = false;\n+      refined_type = false;\n+    } else if (res == SUBTYPE) {\n+      if (above_centerline(tap->ptr()) && !above_centerline(this->ptr())) {\n+        flat = _flat;\n+        null_free = _null_free;\n+        atomic = _atomic;\n+        refined_type = _refined_type;\n+      } else if (above_centerline(this->ptr()) && !above_centerline(tap->ptr())) {\n+        flat = tap->_flat;\n+        null_free = tap->_null_free;\n+        atomic = tap->_atomic;\n+        refined_type = tap->_refined_type;\n+      } else if (above_centerline(this->ptr()) && above_centerline(tap->ptr())) {\n+        flat = _flat || tap->_flat;\n+        null_free = _null_free || tap->_null_free;\n+        atomic = _atomic || tap->_atomic;\n+        refined_type = _refined_type || tap->_refined_type;\n+      } else if (res_xk && _refined_type != tap->_refined_type) {\n+        \/\/ This can happen if the phi emitted by LibraryCallKit::load_default_refined_array_klass\/load_non_refined_array_klass\n+        \/\/ is processed before the typeArray guard is folded. Both inputs are constant but the input corresponding to the\n+        \/\/ typeArray will go away. Don't constant fold it yet but wait for the control input to collapse.\n+        ptr = PTR::NotNull;\n+      }\n+    }\n+    return make(ptr, elem, res_klass, off, res_not_flat, res_not_null_free, flat, null_free, atomic, refined_type);\n@@ -6251,1 +6976,1 @@\n-    int offset = meet_offset(tp->offset());\n+    Offset offset = meet_offset(tp->offset());\n@@ -6263,0 +6988,2 @@\n+      \/\/\n+      \/\/ Flat in array: See explanation for meet with TypeInstPtr in TypeAryPtr::xmeet_helper().\n@@ -6264,2 +6991,2 @@\n-          !tp->klass_is_exact()) {\n-        return TypeAryKlassPtr::make(ptr, _elem, _klass, offset);\n+          !tp->klass_is_exact() && !tp->is_not_flat_in_array()) {\n+        return TypeAryKlassPtr::make(ptr, _elem, _klass, offset, is_not_flat(), is_not_null_free(), is_flat(), is_null_free(), is_atomic(), is_refined_type());\n@@ -6270,1 +6997,2 @@\n-        return TypeInstKlassPtr::make(ptr, ciEnv::current()->Object_klass(), interfaces, offset);\n+        FlatInArray flat_in_array = meet_flat_in_array(NotFlat, tp->flat_in_array());\n+        return TypeInstKlassPtr::make(ptr, ciEnv::current()->Object_klass(), interfaces, offset, flat_in_array);\n@@ -6274,1 +7002,1 @@\n-    case BotPTR:                \/\/ Fall down to object klass\n+    case BotPTR: { \/\/ Fall down to object klass\n@@ -6282,0 +7010,2 @@\n+        \/\/\n+        \/\/ Flat in array: See explanation for meet with TypeInstPtr in TypeAryPtr::xmeet_helper().\n@@ -6283,1 +7013,1 @@\n-            !tp->klass_is_exact()) {\n+            !tp->klass_is_exact() && !tp->is_not_flat_in_array()) {\n@@ -6285,1 +7015,1 @@\n-          return make(ptr, _elem, _klass, offset);\n+          return make(ptr, _elem, _klass, offset, is_not_flat(), is_not_null_free(), is_flat(), is_null_free(), is_atomic(), is_refined_type());\n@@ -6291,1 +7021,1 @@\n-         ptr = NotNull;\n+        ptr = NotNull;\n@@ -6293,1 +7023,3 @@\n-      return TypeInstKlassPtr::make(ptr, ciEnv::current()->Object_klass(), interfaces, offset);\n+      FlatInArray flat_in_array = meet_flat_in_array(NotFlat, tp->flat_in_array());\n+      return TypeInstKlassPtr::make(ptr, ciEnv::current()->Object_klass(), interfaces, offset, tp->flat_in_array());\n+    }\n@@ -6331,0 +7063,3 @@\n+    if (other->is_null_free() && !this_one->is_null_free()) {\n+      return false; \/\/ A nullable array can't be a subtype of a null-free array\n+    }\n@@ -6423,1 +7158,1 @@\n-  return new TypeAryKlassPtr(dual_ptr(), elem()->dual(), klass(), dual_offset());\n+  return new TypeAryKlassPtr(dual_ptr(), elem()->dual(), klass(), dual_offset(), !is_not_flat(), !is_not_null_free(), dual_flat(), dual_null_free(), dual_atomic(), _refined_type);\n@@ -6433,1 +7168,1 @@\n-    k = ciObjArrayKlass::make(k);\n+    k = ciArrayKlass::make(k, k->is_inlinetype() ? is_null_free() : false, k->is_inlinetype() ? is_atomic() : true, _refined_type);\n@@ -6464,0 +7199,8 @@\n+  if (_flat) st->print(\":flat\");\n+  if (_null_free) st->print(\":null free\");\n+  if (_atomic) st->print(\":atomic\");\n+  if (_refined_type) st->print(\":refined_type\");\n+  if (Verbose) {\n+    if (_not_flat) st->print(\":not flat\");\n+    if (_not_null_free) st->print(\":nullable\");\n+  }\n@@ -6482,2 +7225,14 @@\n-const TypeFunc *TypeFunc::make( const TypeTuple *domain, const TypeTuple *range ) {\n-  return (TypeFunc*)(new TypeFunc(domain,range))->hashcons();\n+const TypeFunc *TypeFunc::make(const TypeTuple *domain_sig, const TypeTuple* domain_cc,\n+                               const TypeTuple *range_sig, const TypeTuple *range_cc) {\n+  return (TypeFunc*)(new TypeFunc(domain_sig, domain_cc, range_sig, range_cc))->hashcons();\n+}\n+\n+const TypeFunc *TypeFunc::make(const TypeTuple *domain, const TypeTuple *range) {\n+  return make(domain, domain, range, range);\n+}\n+\n+\/\/------------------------------osr_domain-----------------------------\n+const TypeTuple* osr_domain() {\n+  const Type **fields = TypeTuple::fields(2);\n+  fields[TypeFunc::Parms+0] = TypeRawPtr::BOTTOM;  \/\/ address of osr buffer\n+  return TypeTuple::make(TypeFunc::Parms+1, fields);\n@@ -6487,1 +7242,1 @@\n-const TypeFunc *TypeFunc::make(ciMethod* method) {\n+const TypeFunc* TypeFunc::make(ciMethod* method, bool is_osr_compilation) {\n@@ -6489,7 +7244,24 @@\n-  const TypeFunc* tf = C->last_tf(method); \/\/ check cache\n-  if (tf != nullptr)  return tf;  \/\/ The hit rate here is almost 50%.\n-  const TypeTuple *domain;\n-  if (method->is_static()) {\n-    domain = TypeTuple::make_domain(nullptr, method->signature(), ignore_interfaces);\n-  } else {\n-    domain = TypeTuple::make_domain(method->holder(), method->signature(), ignore_interfaces);\n+  const TypeFunc* tf = nullptr;\n+  if (!is_osr_compilation) {\n+    tf = C->last_tf(method); \/\/ check cache\n+    if (tf != nullptr)  return tf;  \/\/ The hit rate here is almost 50%.\n+  }\n+  \/\/ Inline types are not passed\/returned by reference, instead each field of\n+  \/\/ the inline type is passed\/returned as an argument. We maintain two views of\n+  \/\/ the argument\/return list here: one based on the signature (with an inline\n+  \/\/ type argument\/return as a single slot), one based on the actual calling\n+  \/\/ convention (with an inline type argument\/return as a list of its fields).\n+  bool has_scalar_args = method->has_scalarized_args() && !is_osr_compilation;\n+  \/\/ Fall back to the non-scalarized calling convention when compiling a call via a mismatching method\n+  if (method != C->method() && method->get_Method()->mismatch()) {\n+    has_scalar_args = false;\n+  }\n+  const TypeTuple* domain_sig = is_osr_compilation ? osr_domain() : TypeTuple::make_domain(method, ignore_interfaces, false);\n+  const TypeTuple* domain_cc = has_scalar_args ? TypeTuple::make_domain(method, ignore_interfaces, true) : domain_sig;\n+  ciSignature* sig = method->signature();\n+  bool has_scalar_ret = !method->is_native() && sig->return_type()->is_inlinetype() && sig->return_type()->as_inline_klass()->can_be_returned_as_fields();\n+  const TypeTuple* range_sig = TypeTuple::make_range(sig, ignore_interfaces, false);\n+  const TypeTuple* range_cc = has_scalar_ret ? TypeTuple::make_range(sig, ignore_interfaces, true) : range_sig;\n+  tf = TypeFunc::make(domain_sig, domain_cc, range_sig, range_cc);\n+  if (!is_osr_compilation) {\n+    C->set_last_tf(method, tf);  \/\/ fill cache\n@@ -6497,3 +7269,0 @@\n-  const TypeTuple *range  = TypeTuple::make_range(method->signature(), ignore_interfaces);\n-  tf = TypeFunc::make(domain, range);\n-  C->set_last_tf(method, tf);  \/\/ fill cache\n@@ -6534,2 +7303,4 @@\n-  return _domain == a->_domain &&\n-    _range == a->_range;\n+  return _domain_sig == a->_domain_sig &&\n+    _domain_cc == a->_domain_cc &&\n+    _range_sig == a->_range_sig &&\n+    _range_cc == a->_range_cc;\n@@ -6541,1 +7312,1 @@\n-  return (uint)(uintptr_t)_domain + (uint)(uintptr_t)_range;\n+  return (uint)(intptr_t)_domain_sig + (uint)(intptr_t)_domain_cc + (uint)(intptr_t)_range_sig + (uint)(intptr_t)_range_cc;\n@@ -6548,1 +7319,1 @@\n-  if( _range->cnt() <= Parms )\n+  if( _range_sig->cnt() <= Parms )\n@@ -6552,2 +7323,2 @@\n-    for (i = Parms; i < _range->cnt()-1; i++) {\n-      _range->field_at(i)->dump2(d,depth,st);\n+    for (i = Parms; i < _range_sig->cnt()-1; i++) {\n+      _range_sig->field_at(i)->dump2(d,depth,st);\n@@ -6556,1 +7327,1 @@\n-    _range->field_at(i)->dump2(d,depth,st);\n+    _range_sig->field_at(i)->dump2(d,depth,st);\n@@ -6565,3 +7336,3 @@\n-  if (Parms < _domain->cnt())\n-    _domain->field_at(Parms)->dump2(d,depth-1,st);\n-  for (uint i = Parms+1; i < _domain->cnt(); i++) {\n+  if (Parms < _domain_sig->cnt())\n+    _domain_sig->field_at(Parms)->dump2(d,depth-1,st);\n+  for (uint i = Parms+1; i < _domain_sig->cnt(); i++) {\n@@ -6569,1 +7340,1 @@\n-    _domain->field_at(i)->dump2(d,depth-1,st);\n+    _domain_sig->field_at(i)->dump2(d,depth-1,st);\n@@ -6589,1 +7360,1 @@\n-  if (range()->cnt() == TypeFunc::Parms) {\n+  if (range_sig()->cnt() == TypeFunc::Parms) {\n@@ -6592,1 +7363,1 @@\n-  return range()->field_at(TypeFunc::Parms)->basic_type();\n+  return range_sig()->field_at(TypeFunc::Parms)->basic_type();\n","filename":"src\/hotspot\/share\/opto\/type.cpp","additions":1117,"deletions":346,"binary":false,"changes":1463,"status":"modified"},{"patch":"@@ -28,0 +28,1 @@\n+#include \"ci\/ciInlineKlass.hpp\"\n@@ -31,1 +32,0 @@\n-#include \"runtime\/handles.hpp\"\n@@ -147,0 +147,24 @@\n+  class Offset {\n+  private:\n+    int _offset;\n+\n+  public:\n+    explicit Offset(int offset) : _offset(offset) {}\n+\n+    const Offset meet(const Offset other) const;\n+    const Offset dual() const;\n+    const Offset add(intptr_t offset) const;\n+    bool operator==(const Offset& other) const {\n+      return _offset == other._offset;\n+    }\n+    bool operator!=(const Offset& other) const {\n+      return _offset != other._offset;\n+    }\n+    int get() const { return _offset; }\n+\n+    void dump2(outputStream *st) const;\n+\n+    static const Offset top;\n+    static const Offset bottom;\n+  };\n+\n@@ -348,0 +372,3 @@\n+  bool is_inlinetypeptr() const;\n+  virtual ciInlineKlass* inline_klass() const;\n+\n@@ -955,2 +982,2 @@\n-  static const TypeTuple *make_range(ciSignature *sig, InterfaceHandling interface_handling = ignore_interfaces);\n-  static const TypeTuple *make_domain(ciInstanceKlass* recv, ciSignature *sig, InterfaceHandling interface_handling);\n+  static const TypeTuple *make_range(ciSignature* sig, InterfaceHandling interface_handling = ignore_interfaces, bool ret_vt_fields = false);\n+  static const TypeTuple *make_domain(ciMethod* method, InterfaceHandling interface_handling, bool vt_fields_as_args = false);\n@@ -985,2 +1012,2 @@\n-  TypeAry(const Type* elem, const TypeInt* size, bool stable) : Type(Array),\n-      _elem(elem), _size(size), _stable(stable) {}\n+  TypeAry(const Type* elem, const TypeInt* size, bool stable, bool flat, bool not_flat, bool not_null_free, bool atomic) : Type(Array),\n+      _elem(elem), _size(size), _stable(stable), _flat(flat), _not_flat(not_flat), _not_null_free(not_null_free), _atomic(atomic) {}\n@@ -997,0 +1024,7 @@\n+\n+  \/\/ Inline type array properties\n+  const bool _flat;             \/\/ Array is flat\n+  const bool _not_flat;         \/\/ Array is never flat\n+  const bool _not_null_free;    \/\/ Array is never null-free\n+  const bool _atomic;           \/\/ Array is atomic\n+\n@@ -1000,1 +1034,2 @@\n-  static const TypeAry* make(const Type* elem, const TypeInt* size, bool stable = false);\n+  static const TypeAry* make(const Type* elem, const TypeInt* size, bool stable = false,\n+                             bool flat = false, bool not_flat = false, bool not_null_free = false, bool atomic = false);\n@@ -1146,0 +1181,19 @@\n+\n+  \/\/ Only applies to TypeInstPtr and TypeInstKlassPtr. Since the common super class is TypePtr, it is defined here.\n+  \/\/\n+  \/\/ FlatInArray defines the following Boolean Lattice structure\n+  \/\/\n+  \/\/     TopFlat\n+  \/\/    \/      \\\n+  \/\/  Flat   NotFlat\n+  \/\/    \\      \/\n+  \/\/   MaybeFlat\n+  \/\/\n+  \/\/ with meet (see TypePtr::meet_flat_in_array()) and join (implemented over dual, see TypePtr::flat_in_array_dual)\n+  enum FlatInArray {\n+    TopFlat,        \/\/ Dedicated top element and dual of MaybeFlat. Result when joining Flat and NotFlat.\n+    Flat,           \/\/ An instance is always flat in an array.\n+    NotFlat,        \/\/ An instance is never flat in an array.\n+    MaybeFlat,      \/\/ We don't know whether an instance is flat in an array.\n+    Uninitialized   \/\/ Used when the flat in array property was not computed, yet - should never actually end up in a type.\n+  };\n@@ -1147,1 +1201,1 @@\n-  TypePtr(TYPES t, PTR ptr, int offset,\n+  TypePtr(TYPES t, PTR ptr, Offset offset,\n@@ -1156,0 +1210,3 @@\n+  static const FlatInArray flat_in_array_dual[Uninitialized];\n+  static const char* const flat_in_array_msg[Uninitialized];\n+\n@@ -1204,0 +1261,2 @@\n+ protected:\n+  static FlatInArray meet_flat_in_array(FlatInArray left, FlatInArray other);\n@@ -1206,1 +1265,1 @@\n-                                                  ciKlass*& res_klass, bool& res_xk);\n+                                                  ciKlass*& res_klass, bool& res_xk, bool &res_flat, bool &res_not_flat, bool &res_not_null_free, bool &res_atomic);\n@@ -1217,1 +1276,1 @@\n-  const int _offset;            \/\/ Offset into oop, with TOP & BOT\n+  const Offset _offset;         \/\/ Offset into oop, with TOP & BOT\n@@ -1220,1 +1279,1 @@\n-  int offset() const { return _offset; }\n+  int offset() const { return _offset.get(); }\n@@ -1223,1 +1282,1 @@\n-  static const TypePtr *make(TYPES t, PTR ptr, int offset,\n+  static const TypePtr* make(TYPES t, PTR ptr, Offset offset,\n@@ -1232,1 +1291,1 @@\n-  int xadd_offset( intptr_t offset ) const;\n+  Type::Offset xadd_offset(intptr_t offset) const;\n@@ -1235,0 +1294,1 @@\n+  virtual int flat_offset() const { return offset(); }\n@@ -1242,2 +1302,2 @@\n-  int meet_offset( int offset ) const;\n-  int dual_offset( ) const;\n+  Offset meet_offset(int offset) const;\n+  Offset dual_offset() const;\n@@ -1271,0 +1331,16 @@\n+  NOT_PRODUCT(static void dump_flat_in_array(FlatInArray flat_in_array, outputStream* st);)\n+\n+  static FlatInArray compute_flat_in_array(ciInstanceKlass* instance_klass, bool is_exact);\n+  FlatInArray compute_flat_in_array_if_unknown(ciInstanceKlass* instance_klass, bool is_exact,\n+                                               FlatInArray old_flat_in_array) const;\n+\n+  virtual bool can_be_inline_type() const { return false; }\n+  virtual bool is_flat_in_array()     const { return flat_in_array() == Flat; }\n+  virtual bool is_not_flat_in_array() const { return flat_in_array() == NotFlat; }\n+  virtual FlatInArray flat_in_array() const { return NotFlat; }\n+  virtual bool is_flat()            const { return false; }\n+  virtual bool is_not_flat()        const { return false; }\n+  virtual bool is_null_free()       const { return false; }\n+  virtual bool is_not_null_free()   const { return false; }\n+  virtual bool is_atomic()          const { return false; }\n+\n@@ -1288,1 +1364,1 @@\n-  TypeRawPtr( PTR ptr, address bits ) : TypePtr(RawPtr,ptr,0), _bits(bits){}\n+  TypeRawPtr(PTR ptr, address bits) : TypePtr(RawPtr,ptr,Offset(0)), _bits(bits){}\n@@ -1324,1 +1400,1 @@\n- TypeOopPtr(TYPES t, PTR ptr, ciKlass* k, const TypeInterfaces* interfaces, bool xk, ciObject* o, int offset, int instance_id,\n+ TypeOopPtr(TYPES t, PTR ptr, ciKlass* k, const TypeInterfaces* interfaces, bool xk, ciObject* o, Offset offset, Offset field_offset, int instance_id,\n@@ -1348,0 +1424,1 @@\n+  bool          _is_ptr_to_strict_final_field;\n@@ -1365,1 +1442,1 @@\n-  virtual ciKlass* klass() const { return _klass;     }\n+  virtual ciKlass* klass() const { return _klass; }\n@@ -1416,1 +1493,1 @@\n-  static const TypeOopPtr* make(PTR ptr, int offset, int instance_id,\n+  static const TypeOopPtr* make(PTR ptr, Offset offset, int instance_id,\n@@ -1433,0 +1510,1 @@\n+  bool is_ptr_to_strict_final_field() const { return _is_ptr_to_strict_final_field; }\n@@ -1435,1 +1513,4 @@\n-  bool is_known_instance_field() const { return is_known_instance() && _offset >= 0; }\n+  bool is_known_instance_field() const { return is_known_instance() && _offset.get() >= 0; }\n+\n+  virtual bool can_be_inline_type() const { return (_klass == nullptr || _klass->can_be_inline_klass(_klass_is_exact)); }\n+  virtual bool can_be_inline_array() const { ShouldNotReachHere(); return false; }\n@@ -1498,2 +1579,6 @@\n-  TypeInstPtr(PTR ptr, ciKlass* k, const TypeInterfaces* interfaces, bool xk, ciObject* o, int off, int instance_id,\n-              const TypePtr* speculative, int inline_depth);\n+  \/\/ Can this instance be in a flat array?\n+  FlatInArray _flat_in_array;\n+\n+  TypeInstPtr(PTR ptr, ciKlass* k, const TypeInterfaces* interfaces, bool xk, ciObject* o, Offset offset,\n+              FlatInArray flat_in_array, int instance_id, const TypePtr* speculative,\n+              int inline_depth);\n@@ -1502,1 +1587,0 @@\n-\n@@ -1521,1 +1605,1 @@\n-    return make(TypePtr::Constant, k, interfaces, true, o, 0, InstanceBot);\n+    return make(TypePtr::Constant, k, interfaces, true, o, Offset(0));\n@@ -1524,1 +1608,1 @@\n-  static const TypeInstPtr *make(ciObject* o, int offset) {\n+  static const TypeInstPtr *make(ciObject* o, Offset offset) {\n@@ -1527,1 +1611,1 @@\n-    return make(TypePtr::Constant, k, interfaces, true, o, offset, InstanceBot);\n+    return make(TypePtr::Constant, k, interfaces, true, o, offset);\n@@ -1533,1 +1617,1 @@\n-    return make(ptr, klass, interfaces, false, nullptr, 0, InstanceBot);\n+    return make(ptr, klass, interfaces, false, nullptr, Offset(0));\n@@ -1539,1 +1623,1 @@\n-    return make(ptr, klass, interfaces, true, nullptr, 0, InstanceBot);\n+    return make(ptr, klass, interfaces, true, nullptr, Offset(0));\n@@ -1543,1 +1627,1 @@\n-  static const TypeInstPtr *make(PTR ptr, ciKlass* klass, int offset) {\n+  static const TypeInstPtr *make(PTR ptr, ciKlass* klass, Offset offset) {\n@@ -1545,1 +1629,1 @@\n-    return make(ptr, klass, interfaces, false, nullptr, offset, InstanceBot);\n+    return make(ptr, klass, interfaces, false, nullptr, offset);\n@@ -1548,1 +1632,3 @@\n-  static const TypeInstPtr *make(PTR ptr, ciKlass* k, const TypeInterfaces* interfaces, bool xk, ciObject* o, int offset,\n+  \/\/ Make a pointer to an oop.\n+  static const TypeInstPtr* make(PTR ptr, ciKlass* k, const TypeInterfaces* interfaces, bool xk, ciObject* o, Offset offset,\n+                                 FlatInArray flat_in_array = Uninitialized,\n@@ -1553,1 +1639,2 @@\n-  static const TypeInstPtr *make(PTR ptr, ciKlass* k, bool xk, ciObject* o, int offset, int instance_id = InstanceBot) {\n+  static const TypeInstPtr *make(PTR ptr, ciKlass* k, bool xk, ciObject* o, Offset offset, int instance_id = InstanceBot,\n+                                 FlatInArray flat_in_array = Uninitialized) {\n@@ -1555,1 +1642,1 @@\n-    return make(ptr, k, interfaces, xk, o, offset, instance_id);\n+    return make(ptr, k, interfaces, xk, o, offset, flat_in_array, instance_id);\n@@ -1572,0 +1659,1 @@\n+  virtual bool empty() const;\n@@ -1581,0 +1669,8 @@\n+  virtual const TypeInstPtr* cast_to_flat_in_array() const;\n+  virtual const TypeInstPtr* cast_to_maybe_flat_in_array() const;\n+  virtual FlatInArray flat_in_array() const { return _flat_in_array; }\n+\n+  FlatInArray dual_flat_in_array() const {\n+    return flat_in_array_dual[_flat_in_array];\n+  }\n+\n@@ -1588,0 +1684,2 @@\n+  virtual bool can_be_inline_array() const;\n+\n@@ -1612,0 +1710,1 @@\n+  friend class TypeInstPtr;\n@@ -1614,4 +1713,4 @@\n-  TypeAryPtr( PTR ptr, ciObject* o, const TypeAry *ary, ciKlass* k, bool xk,\n-              int offset, int instance_id, bool is_autobox_cache,\n-              const TypePtr* speculative, int inline_depth)\n-    : TypeOopPtr(AryPtr,ptr,k,_array_interfaces,xk,o,offset, instance_id, speculative, inline_depth),\n+  TypeAryPtr(PTR ptr, ciObject* o, const TypeAry *ary, ciKlass* k, bool xk,\n+             Offset offset, Offset field_offset, int instance_id, bool is_autobox_cache,\n+             const TypePtr* speculative, int inline_depth)\n+    : TypeOopPtr(AryPtr, ptr, k, _array_interfaces, xk, o, offset, field_offset, instance_id, speculative, inline_depth),\n@@ -1619,1 +1718,2 @@\n-    _is_autobox_cache(is_autobox_cache)\n+    _is_autobox_cache(is_autobox_cache),\n+    _field_offset(field_offset)\n@@ -1625,2 +1725,2 @@\n-        _offset != 0 && _offset != arrayOopDesc::length_offset_in_bytes() &&\n-        _offset != arrayOopDesc::klass_offset_in_bytes()) {\n+        _offset.get() != 0 && _offset.get() != arrayOopDesc::length_offset_in_bytes() &&\n+        _offset.get() != arrayOopDesc::klass_offset_in_bytes()) {\n@@ -1635,0 +1735,6 @@\n+  \/\/ For flat inline type arrays, each field of the inline type in\n+  \/\/ the array has its own memory slice so we need to keep track of\n+  \/\/ which field is accessed\n+  const Offset _field_offset;\n+  Offset meet_field_offset(const Type::Offset offset) const;\n+  Offset dual_field_offset() const;\n@@ -1662,0 +1768,7 @@\n+  \/\/ Inline type array properties\n+  bool is_flat()          const { return _ary->_flat; }\n+  bool is_not_flat()      const { return _ary->_not_flat; }\n+  bool is_null_free()     const { return _ary->_elem->make_ptr() != nullptr && (_ary->_elem->make_ptr()->ptr() == NotNull || _ary->_elem->make_ptr()->ptr() == AnyNull); }\n+  bool is_not_null_free() const { return _ary->_not_null_free; }\n+  bool is_atomic()        const { return _ary->_atomic; }\n+\n@@ -1664,1 +1777,2 @@\n-  static const TypeAryPtr *make(PTR ptr, const TypeAry *ary, ciKlass* k, bool xk, int offset,\n+  static const TypeAryPtr* make(PTR ptr, const TypeAry *ary, ciKlass* k, bool xk, Offset offset,\n+                                Offset field_offset = Offset::bottom,\n@@ -1669,1 +1783,2 @@\n-  static const TypeAryPtr *make(PTR ptr, ciObject* o, const TypeAry *ary, ciKlass* k, bool xk, int offset,\n+  static const TypeAryPtr* make(PTR ptr, ciObject* o, const TypeAry *ary, ciKlass* k, bool xk, Offset offset,\n+                                Offset field_offset = Offset::bottom,\n@@ -1672,1 +1787,2 @@\n-                                int inline_depth = InlineDepthBottom, bool is_autobox_cache = false);\n+                                int inline_depth = InlineDepthBottom,\n+                                bool is_autobox_cache = false);\n@@ -1691,0 +1807,1 @@\n+  virtual const Type* cleanup_speculative() const;\n@@ -1698,0 +1815,8 @@\n+  \/\/ Inline type array properties\n+  const TypeAryPtr* cast_to_not_flat(bool not_flat = true) const;\n+  const TypeAryPtr* cast_to_not_null_free(bool not_null_free = true) const;\n+  const TypeAryPtr* update_properties(const TypeAryPtr* new_type) const;\n+  jint flat_layout_helper() const;\n+  int flat_elem_size() const;\n+  int flat_log_elem_size() const;\n+\n@@ -1703,1 +1828,8 @@\n-  static jint max_array_length(BasicType etype) ;\n+  static jint max_array_length(BasicType etype);\n+\n+  int flat_offset() const;\n+  const Offset field_offset() const { return _field_offset; }\n+  const TypeAryPtr* with_field_offset(int offset) const;\n+  const TypePtr* add_field_offset_and_offset(intptr_t offset) const;\n+\n+  virtual bool can_be_inline_type() const { return false; }\n@@ -1706,0 +1838,2 @@\n+  virtual bool can_be_inline_array() const;\n+\n@@ -1708,10 +1842,11 @@\n-  static const TypeAryPtr* RANGE;\n-  static const TypeAryPtr* OOPS;\n-  static const TypeAryPtr* NARROWOOPS;\n-  static const TypeAryPtr* BYTES;\n-  static const TypeAryPtr* SHORTS;\n-  static const TypeAryPtr* CHARS;\n-  static const TypeAryPtr* INTS;\n-  static const TypeAryPtr* LONGS;\n-  static const TypeAryPtr* FLOATS;\n-  static const TypeAryPtr* DOUBLES;\n+  static const TypeAryPtr *RANGE;\n+  static const TypeAryPtr *OOPS;\n+  static const TypeAryPtr *NARROWOOPS;\n+  static const TypeAryPtr *BYTES;\n+  static const TypeAryPtr *SHORTS;\n+  static const TypeAryPtr *CHARS;\n+  static const TypeAryPtr *INTS;\n+  static const TypeAryPtr *LONGS;\n+  static const TypeAryPtr *FLOATS;\n+  static const TypeAryPtr *DOUBLES;\n+  static const TypeAryPtr *INLINES;\n@@ -1736,1 +1871,1 @@\n-  TypeMetadataPtr(PTR ptr, ciMetadata* metadata, int offset);\n+  TypeMetadataPtr(PTR ptr, ciMetadata* metadata, Offset offset);\n@@ -1748,1 +1883,1 @@\n-  static const TypeMetadataPtr* make(PTR ptr, ciMetadata* m, int offset);\n+  static const TypeMetadataPtr* make(PTR ptr, ciMetadata* m, Offset offset);\n@@ -1779,1 +1914,1 @@\n-  TypeKlassPtr(TYPES t, PTR ptr, ciKlass* klass, const TypeInterfaces* interfaces, int offset);\n+  TypeKlassPtr(TYPES t, PTR ptr, ciKlass* klass, const TypeInterfaces* interfaces, Offset offset);\n@@ -1818,1 +1953,1 @@\n-  static const TypeKlassPtr *make(PTR ptr, ciKlass* klass, int offset, InterfaceHandling interface_handling = ignore_interfaces);\n+  static const TypeKlassPtr *make(PTR ptr, ciKlass* klass, Offset offset, InterfaceHandling interface_handling = ignore_interfaces);\n@@ -1837,0 +1972,2 @@\n+  virtual bool can_be_inline_array() const { ShouldNotReachHere(); return false; }\n+\n@@ -1867,0 +2004,2 @@\n+  \/\/ Can an instance of this class be in a flat array?\n+  const FlatInArray _flat_in_array;\n@@ -1868,2 +2007,3 @@\n-  TypeInstKlassPtr(PTR ptr, ciKlass* klass, const TypeInterfaces* interfaces, int offset)\n-    : TypeKlassPtr(InstKlassPtr, ptr, klass, interfaces, offset) {\n+  TypeInstKlassPtr(PTR ptr, ciKlass* klass, const TypeInterfaces* interfaces, Offset offset, FlatInArray flat_in_array)\n+    : TypeKlassPtr(InstKlassPtr, ptr, klass, interfaces, offset), _flat_in_array(flat_in_array) {\n+    assert(flat_in_array != Uninitialized, \"must be set now\");\n@@ -1888,0 +2028,2 @@\n+  virtual bool can_be_inline_type() const { return (_klass == nullptr || _klass->can_be_inline_klass(klass_is_exact())); }\n+\n@@ -1890,1 +2032,1 @@\n-    return make(TypePtr::Constant, k, interfaces, 0);\n+    return make(TypePtr::Constant, k, interfaces, Offset(0));\n@@ -1892,2 +2034,4 @@\n-  static const TypeInstKlassPtr* make(PTR ptr, ciKlass* k, const TypeInterfaces* interfaces, int offset);\n-  static const TypeInstKlassPtr* make(PTR ptr, ciKlass* k, int offset) {\n+  static const TypeInstKlassPtr* make(PTR ptr, ciKlass* k, const TypeInterfaces* interfaces, Offset offset,\n+                                      FlatInArray flat_in_array = Uninitialized);\n+\n+  static const TypeInstKlassPtr* make(PTR ptr, ciKlass* k, Offset offset, FlatInArray flat_in_array = Uninitialized) {\n@@ -1896,1 +2040,1 @@\n-    return make(ptr, k, interfaces, offset);\n+    return make(ptr, k, interfaces, offset, flat_in_array);\n@@ -1908,0 +2052,2 @@\n+\n+  virtual bool empty() const;\n@@ -1915,0 +2061,8 @@\n+  virtual FlatInArray flat_in_array() const { return _flat_in_array; }\n+\n+  FlatInArray dual_flat_in_array() const {\n+    return flat_in_array_dual[_flat_in_array];\n+  }\n+\n+  virtual bool can_be_inline_array() const;\n+\n@@ -1934,0 +2088,6 @@\n+  const bool _not_flat;      \/\/ Array is never flat\n+  const bool _not_null_free; \/\/ Array is never null-free\n+  const bool _flat;\n+  const bool _null_free;\n+  const bool _atomic;\n+  const bool _refined_type;\n@@ -1936,3 +2096,3 @@\n-  TypeAryKlassPtr(PTR ptr, const Type *elem, ciKlass* klass, int offset)\n-    : TypeKlassPtr(AryKlassPtr, ptr, klass, _array_interfaces, offset), _elem(elem) {\n-    assert(klass == nullptr || klass->is_type_array_klass() || !klass->as_obj_array_klass()->base_element_klass()->is_interface(), \"\");\n+  TypeAryKlassPtr(PTR ptr, const Type *elem, ciKlass* klass, Offset offset, bool not_flat, int not_null_free, bool flat, bool null_free, bool atomic, bool refined_type)\n+    : TypeKlassPtr(AryKlassPtr, ptr, klass, _array_interfaces, offset), _elem(elem), _not_flat(not_flat), _not_null_free(not_null_free), _flat(flat), _null_free(null_free), _atomic(atomic), _refined_type(refined_type) {\n+    assert(klass == nullptr || klass->is_type_array_klass() || klass->is_flat_array_klass() || !klass->as_obj_array_klass()->base_element_klass()->is_interface(), \"\");\n@@ -1947,0 +2107,24 @@\n+  bool dual_flat() const {\n+    return _flat;\n+  }\n+\n+  bool meet_flat(bool other) const {\n+    return _flat && other;\n+  }\n+\n+  bool dual_null_free() const {\n+    return _null_free;\n+  }\n+\n+  bool meet_null_free(bool other) const {\n+    return _null_free && other;\n+  }\n+\n+  bool dual_atomic() const {\n+    return _atomic;\n+  }\n+\n+  bool meet_atomic(bool other) const {\n+    return _atomic && other;\n+  }\n+\n@@ -1952,1 +2136,1 @@\n-  static const TypeAryKlassPtr *make(PTR ptr, ciKlass* k, int offset, InterfaceHandling interface_handling);\n+  static const TypeAryKlassPtr* make(PTR ptr, ciKlass* k, Offset offset, InterfaceHandling interface_handling, bool not_flat, bool not_null_free, bool flat, bool null_free, bool atomic, bool refined_type = false);\n@@ -1960,2 +2144,5 @@\n-  static const TypeAryKlassPtr *make(PTR ptr, const Type *elem, ciKlass* k, int offset);\n-  static const TypeAryKlassPtr* make(ciKlass* klass, InterfaceHandling interface_handling);\n+  static const TypeAryKlassPtr* make(PTR ptr, const Type* elem, ciKlass* k, Offset offset, bool not_flat, bool not_null_free, bool flat, bool null_free, bool atomic, bool refined_type = false);\n+  static const TypeAryKlassPtr* make(PTR ptr, ciKlass* k, Offset offset, InterfaceHandling interface_handling, bool refined_type = false);\n+  static const TypeAryKlassPtr* make(ciKlass* klass, InterfaceHandling interface_handling, bool refined_type = false);\n+\n+  const TypeAryKlassPtr* cast_to_refined_array_klass_ptr(bool refined = true) const;\n@@ -1985,0 +2172,8 @@\n+  bool is_flat()          const { return _flat; }\n+  bool is_not_flat()      const { return _not_flat; }\n+  bool is_null_free()     const { return _null_free; }\n+  bool is_not_null_free() const { return _not_null_free; }\n+  bool is_atomic()        const { return _atomic; }\n+  bool is_refined_type()  const { return _refined_type; }\n+  virtual bool can_be_inline_array() const;\n+\n@@ -2120,1 +2315,2 @@\n-  TypeFunc( const TypeTuple *domain, const TypeTuple *range ) : Type(Function),  _domain(domain), _range(range) {}\n+  TypeFunc(const TypeTuple *domain_sig, const TypeTuple *domain_cc, const TypeTuple *range_sig, const TypeTuple *range_cc)\n+    : Type(Function), _domain_sig(domain_sig), _domain_cc(domain_cc), _range_sig(range_sig), _range_cc(range_cc) {}\n@@ -2126,2 +2322,13 @@\n-  const TypeTuple* const _domain;     \/\/ Domain of inputs\n-  const TypeTuple* const _range;      \/\/ Range of results\n+  \/\/ Domains of inputs: inline type arguments are not passed by\n+  \/\/ reference, instead each field of the inline type is passed as an\n+  \/\/ argument. We maintain 2 views of the argument list here: one\n+  \/\/ based on the signature (with an inline type argument as a single\n+  \/\/ slot), one based on the actual calling convention (with a value\n+  \/\/ type argument as a list of its fields).\n+  const TypeTuple* const _domain_sig;\n+  const TypeTuple* const _domain_cc;\n+  \/\/ Range of results. Similar to domains: an inline type result can be\n+  \/\/ returned in registers in which case range_cc lists all fields and\n+  \/\/ is the actual calling convention.\n+  const TypeTuple* const _range_sig;\n+  const TypeTuple* const _range_cc;\n@@ -2141,5 +2348,8 @@\n-  const TypeTuple* domain() const { return _domain; }\n-  const TypeTuple* range()  const { return _range; }\n-\n-  static const TypeFunc *make(ciMethod* method);\n-  static const TypeFunc *make(ciSignature signature, const Type* extra);\n+  const TypeTuple* domain_sig() const { return _domain_sig; }\n+  const TypeTuple* domain_cc()  const { return _domain_cc; }\n+  const TypeTuple* range_sig()  const { return _range_sig; }\n+  const TypeTuple* range_cc()   const { return _range_cc; }\n+\n+  static const TypeFunc* make(ciMethod* method, bool is_osr_compilation = false);\n+  static const TypeFunc *make(const TypeTuple* domain_sig, const TypeTuple* domain_cc,\n+                              const TypeTuple* range_sig, const TypeTuple* range_cc);\n@@ -2153,0 +2363,2 @@\n+  bool returns_inline_type_as_fields() const { return range_sig() != range_cc(); }\n+\n@@ -2429,0 +2641,8 @@\n+inline bool Type::is_inlinetypeptr() const {\n+  return isa_instptr() != nullptr && is_instptr()->instance_klass()->is_inlinetype();\n+}\n+\n+inline ciInlineKlass* Type::inline_klass() const {\n+  return make_ptr()->is_instptr()->instance_klass()->as_inline_klass();\n+}\n+\n@@ -2474,0 +2694,1 @@\n+#define CmpUXNode    CmpULNode\n@@ -2492,0 +2713,1 @@\n+#define Op_StoreX    Op_StoreL\n@@ -2520,0 +2742,1 @@\n+#define CmpUXNode    CmpUNode\n@@ -2538,0 +2761,1 @@\n+#define Op_StoreX    Op_StoreI\n","filename":"src\/hotspot\/share\/opto\/type.hpp","additions":301,"deletions":77,"binary":false,"changes":378,"status":"modified"},{"patch":"@@ -26,0 +26,1 @@\n+#include \"classfile\/vmClasses.hpp\"\n@@ -468,0 +469,20 @@\n+\/\/ LoadableDescriptors {\n+\/\/   u2 attribute_name_index;\n+\/\/   u4 attribute_length;\n+\/\/   u2 number_of_descriptors;\n+\/\/   u2 descriptors[number_of_descriptors];\n+\/\/ }\n+void JvmtiClassFileReconstituter::write_loadable_descriptors_attribute() {\n+  Array<u2>* loadable_descriptors = ik()->loadable_descriptors();\n+  int number_of_descriptors = loadable_descriptors->length();\n+  int length = sizeof(u2) * (1 + number_of_descriptors); \/\/ '1 +' is for number_of_descriptors field\n+\n+  write_attribute_name_index(\"LoadableDescriptors\");\n+  write_u4(length);\n+  write_u2(checked_cast<u2>(number_of_descriptors));\n+  for (int i = 0; i < number_of_descriptors; i++) {\n+    u2 utf8_index = loadable_descriptors->at(i);\n+    write_u2(utf8_index);\n+  }\n+}\n+\n@@ -548,1 +569,6 @@\n-    write_u2(iter.inner_access_flags());\n+    u2 flags = iter.inner_access_flags();\n+    \/\/ ClassFileParser may add identity to inner class attributes, so remove it.\n+    if (!ik()->supports_inline_types()) {\n+      flags &= ~JVM_ACC_IDENTITY;;\n+    }\n+    write_u2(flags);\n@@ -807,0 +833,3 @@\n+  if (ik()->loadable_descriptors() != Universe::the_empty_short_array()) {\n+    ++attr_count;\n+  }\n@@ -837,0 +866,3 @@\n+  if (ik()->loadable_descriptors() != Universe::the_empty_short_array()) {\n+    write_loadable_descriptors_attribute();\n+  }\n@@ -1031,1 +1063,1 @@\n-      case Bytecodes::_putfield        :  {\n+      case Bytecodes::_putfield        : {\n","filename":"src\/hotspot\/share\/prims\/jvmtiClassFileReconstituter.cpp","additions":34,"deletions":2,"binary":false,"changes":36,"status":"modified"},{"patch":"@@ -620,2 +620,1 @@\n-    \/\/ At this stage JVM_CONSTANT_UnresolvedClassInError should not be\n-    \/\/ here\n+    \/\/ At this stage JVM_CONSTANT_UnresolvedClassInError should not be here\n@@ -1924,0 +1923,6 @@\n+  \/\/ rewrite constant pool references in the LoadableDescriptors attribute:\n+  if (!rewrite_cp_refs_in_loadable_descriptors_attribute(scratch_class)) {\n+    \/\/ propagate failure back to caller\n+    return false;\n+  }\n+\n@@ -2072,0 +2077,13 @@\n+\/\/ Rewrite constant pool references in the LoadableDescriptors attribute.\n+bool VM_RedefineClasses::rewrite_cp_refs_in_loadable_descriptors_attribute(\n+       InstanceKlass* scratch_class) {\n+\n+  Array<u2>* loadable_descriptors = scratch_class->loadable_descriptors();\n+  assert(loadable_descriptors != nullptr, \"unexpected null loadable_descriptors\");\n+  for (int i = 0; i < loadable_descriptors->length(); i++) {\n+    u2 cp_index = loadable_descriptors->at(i);\n+    loadable_descriptors->at_put(i, find_new_index(cp_index));\n+  }\n+  return true;\n+}\n+\n@@ -3259,0 +3277,8 @@\n+   if (frame_type == 246) {  \/\/ EARLY_LARVAL\n+     \/\/ rewrite_cp_refs in  unset fields and fall through.\n+     rewrite_cp_refs_in_early_larval_stackmaps(stackmap_p, stackmap_end, calc_number_of_entries, frame_type);\n+     \/\/ The larval frames point to the next frame, so advance to the next frame and fall through.\n+     frame_type = *stackmap_p;\n+     stackmap_p++;\n+   }\n+\n@@ -3468,0 +3494,23 @@\n+void VM_RedefineClasses::rewrite_cp_refs_in_early_larval_stackmaps(\n+       address& stackmap_p_ref, address stackmap_end, u2 frame_i,\n+       u1 frame_type) {\n+\n+    u2 num_early_larval_stackmaps = Bytes::get_Java_u2(stackmap_p_ref);\n+    stackmap_p_ref += 2;\n+\n+    for (u2 i = 0; i < num_early_larval_stackmaps; i++) {\n+\n+      u2 name_and_ref_index = Bytes::get_Java_u2(stackmap_p_ref);\n+      u2 new_cp_index = find_new_index(name_and_ref_index);\n+      if (new_cp_index != 0) {\n+        log_debug(redefine, class, stackmap)(\"mapped old name_and_ref_index=%d\", name_and_ref_index);\n+        Bytes::put_Java_u2(stackmap_p_ref, new_cp_index);\n+        name_and_ref_index = new_cp_index;\n+      }\n+      log_debug(redefine, class, stackmap)\n+        (\"frame_i=%u, frame_type=%u, name_and_ref_index=%d\", frame_i, frame_type, name_and_ref_index);\n+\n+      stackmap_p_ref += 2;\n+    }\n+} \/\/ rewrite_cp_refs_in_early_larval_stackmaps\n+\n","filename":"src\/hotspot\/share\/prims\/jvmtiRedefineClasses.cpp","additions":51,"deletions":2,"binary":false,"changes":53,"status":"modified"},{"patch":"@@ -480,0 +480,1 @@\n+  bool rewrite_cp_refs_in_loadable_descriptors_attribute(InstanceKlass* scratch_class);\n@@ -496,0 +497,4 @@\n+  void rewrite_cp_refs_in_early_larval_stackmaps(\n+         address& stackmap_p_ref, address stackmap_end, u2 frame_i,\n+         u1 frame_type);\n+\n","filename":"src\/hotspot\/share\/prims\/jvmtiRedefineClasses.hpp","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -79,0 +79,2 @@\n+#include <string.h>\n+\n@@ -371,0 +373,12 @@\n+bool Arguments::patching_migrated_classes(const char* property, const char* value) {\n+  if (strncmp(property, MODULE_PROPERTY_PREFIX, MODULE_PROPERTY_PREFIX_LEN) == 0) {\n+    const char* property_suffix = property + MODULE_PROPERTY_PREFIX_LEN;\n+    if (matches_property_suffix(property_suffix, PATCH, PATCH_LEN)) {\n+      if (strcmp(value, \"java.base-valueclasses.jar\")) {\n+        return true;\n+      }\n+    }\n+  }\n+  return false;\n+}\n+\n@@ -1817,1 +1831,0 @@\n-static unsigned int patch_mod_count = 0;\n@@ -1825,1 +1838,1 @@\n-  if (!CDSConfig::check_vm_args_consistency(patch_mod_javabase, mode_flag_cmd_line)) {\n+  if (!CDSConfig::check_vm_args_consistency(mode_flag_cmd_line)) {\n@@ -1992,0 +2005,4 @@\n+  if (UseAltSubstitutabilityMethod) {\n+    no_shared_spaces(\"Alternate substitutability method doesn't work with CDS yet\");\n+  }\n+\n@@ -2074,1 +2091,1 @@\n-      add_patch_mod_prefix(module_name, module_equal + 1);\n+      add_patch_mod_prefix(module_name, module_equal + 1, false \/* no append *\/, false \/* no cds *\/);\n@@ -2076,3 +2093,0 @@\n-      if (!create_numbered_module_property(\"jdk.module.patch\", patch_mod_tail, patch_mod_count++)) {\n-        return JNI_ENOMEM;\n-      }\n@@ -2086,0 +2100,82 @@\n+\/\/ Temporary system property to disable preview patching and enable the new preview mode\n+\/\/ feature for testing\/development. Once the preview mode feature is finished, the value\n+\/\/ will be always 'true' and this code, and all related dead-code can be removed.\n+#define DISABLE_PREVIEW_PATCHING_DEFAULT false\n+\n+bool Arguments::disable_preview_patching() {\n+  const char* prop = get_property(\"DISABLE_PREVIEW_PATCHING\");\n+  return (prop != nullptr)\n+      ? strncmp(prop, \"true\", strlen(\"true\")) == 0\n+      : DISABLE_PREVIEW_PATCHING_DEFAULT;\n+}\n+\n+\/\/ VALUECLASS_STR must match string used in the build\n+#define VALUECLASS_STR \"valueclasses\"\n+#define VALUECLASS_JAR \"-\" VALUECLASS_STR \".jar\"\n+\n+\/\/ Finalize --patch-module args and --enable-preview related to value class module patches.\n+\/\/ Create all numbered properties passing module patches.\n+int Arguments::finalize_patch_module() {\n+  \/\/ If --enable-preview and EnableValhalla is true, modules may have preview mode resources.\n+  bool enable_valhalla_preview = enable_preview() && EnableValhalla;\n+  \/\/ Whether to use module patching, or the new preview mode feature for preview resources.\n+  bool disable_patching = disable_preview_patching();\n+\n+  \/\/ This must be called, even with 'false', to enable resource lookup from JImage.\n+  ClassLoader::init_jimage(disable_patching && enable_valhalla_preview);\n+\n+  \/\/ For each <module>-valueclasses.jar in <JAVA_HOME>\/lib\/valueclasses\/\n+  \/\/ appends the equivalent of --patch-module <module>=<JAVA_HOME>\/lib\/valueclasses\/<module>-valueclasses.jar\n+  if (!disable_patching && enable_valhalla_preview) {\n+    char * valueclasses_dir = AllocateHeap(JVM_MAXPATHLEN, mtArguments);\n+    const char * fileSep = os::file_separator();\n+\n+    jio_snprintf(valueclasses_dir, JVM_MAXPATHLEN, \"%s%slib%s\" VALUECLASS_STR \"%s\",\n+                 Arguments::get_java_home(), fileSep, fileSep, fileSep);\n+    DIR* dir = os::opendir(valueclasses_dir);\n+    if (dir != nullptr) {\n+      char * module_name = AllocateHeap(JVM_MAXPATHLEN, mtArguments);\n+      char * path = AllocateHeap(JVM_MAXPATHLEN, mtArguments);\n+\n+      for (dirent * entry = os::readdir(dir); entry != nullptr; entry = os::readdir(dir)) {\n+        \/\/ Test if file ends-with \"-valueclasses.jar\"\n+        int len = (int)strlen(entry->d_name) - (sizeof(VALUECLASS_JAR) - 1);\n+        if (len <= 0 || strcmp(&entry->d_name[len], VALUECLASS_JAR) != 0) {\n+          continue;         \/\/ too short or not the expected suffix\n+        }\n+\n+        strcpy(module_name, entry->d_name);\n+        module_name[len] = '\\0';     \/\/ truncate to just module-name\n+\n+        jio_snprintf(path, JVM_MAXPATHLEN, \"%s%s\", valueclasses_dir, &entry->d_name);\n+        add_patch_mod_prefix(module_name, path, true \/* append *\/, true \/* cds OK*\/);\n+        log_info(class)(\"--enable-preview appending value classes for module %s: %s\", module_name, entry->d_name);\n+      }\n+      FreeHeap(module_name);\n+      FreeHeap(path);\n+      os::closedir(dir);\n+    }\n+    FreeHeap(valueclasses_dir);\n+  }\n+\n+  \/\/ Create numbered properties for each module that has been patched either\n+  \/\/ by --patch-module (or --enable-preview if disable_patching is false).\n+  \/\/ Format is \"jdk.module.patch.<n>=<module_name>=<path>\"\n+  if (_patch_mod_prefix != nullptr) {\n+    char * prop_value = AllocateHeap(JVM_MAXPATHLEN + JVM_MAXPATHLEN + 1, mtArguments);\n+    unsigned int patch_mod_count = 0;\n+\n+    for (GrowableArrayIterator<ModulePatchPath *> it = _patch_mod_prefix->begin();\n+            it != _patch_mod_prefix->end(); ++it) {\n+      jio_snprintf(prop_value, JVM_MAXPATHLEN + JVM_MAXPATHLEN + 1, \"%s=%s\",\n+                   (*it)->module_name(), (*it)->path_string());\n+      if (!create_numbered_module_property(\"jdk.module.patch\", prop_value, patch_mod_count++)) {\n+        FreeHeap(prop_value);\n+        return JNI_ENOMEM;\n+      }\n+    }\n+    FreeHeap(prop_value);\n+  }\n+  return JNI_OK;\n+}\n+\n@@ -2367,0 +2463,4 @@\n+      \/\/ --enable-preview enables Valhalla, EnableValhalla VM option will eventually be removed before integration\n+      if (FLAG_SET_CMDLINE(EnableValhalla, true) != JVMFlag::SUCCESS) {\n+        return JNI_EINVAL;\n+      }\n@@ -2873,10 +2973,5 @@\n-void Arguments::add_patch_mod_prefix(const char* module_name, const char* path) {\n-  \/\/ For java.base check for duplicate --patch-module options being specified on the command line.\n-  \/\/ This check is only required for java.base, all other duplicate module specifications\n-  \/\/ will be checked during module system initialization.  The module system initialization\n-  \/\/ will throw an ExceptionInInitializerError if this situation occurs.\n-  if (strcmp(module_name, JAVA_BASE_NAME) == 0) {\n-    if (patch_mod_javabase) {\n-      vm_exit_during_initialization(\"Cannot specify \" JAVA_BASE_NAME \" more than once to --patch-module\");\n-    } else {\n-      patch_mod_javabase = true;\n+void Arguments::add_patch_mod_prefix(const char* module_name, const char* path, bool allow_append, bool allow_cds) {\n+  if (!allow_cds) {\n+    CDSConfig::set_module_patching_disables_cds();\n+    if (strcmp(module_name, JAVA_BASE_NAME) == 0) {\n+      CDSConfig::set_java_base_module_patching_disables_cds();\n@@ -2891,1 +2986,18 @@\n-  _patch_mod_prefix->push(new ModulePatchPath(module_name, path));\n+  \/\/ Scan patches for matching module\n+  int i = _patch_mod_prefix->find_if([&](ModulePatchPath* patch) {\n+    return (strcmp(module_name, patch->module_name()) == 0);\n+  });\n+  if (i == -1) {\n+    _patch_mod_prefix->push(new ModulePatchPath(module_name, path));\n+  } else {\n+    if (allow_append) {\n+      \/\/ append path to existing module entry\n+      _patch_mod_prefix->at(i)->append_path(path);\n+    } else {\n+      if (strcmp(module_name, JAVA_BASE_NAME) == 0) {\n+        vm_exit_during_initialization(\"Cannot specify \" JAVA_BASE_NAME \" more than once to --patch-module\");\n+      } else {\n+        vm_exit_during_initialization(\"Cannot specify a module more than once to --patch-module\", module_name);\n+      }\n+    }\n+  }\n@@ -3004,1 +3116,2 @@\n-  if (!check_vm_args_consistency()) {\n+  \/\/ finalize --module-patch and related --enable-preview\n+  if (finalize_patch_module() != JNI_OK) {\n@@ -3008,0 +3121,3 @@\n+  if (!check_vm_args_consistency()) {\n+    return JNI_ERR;\n+  }\n@@ -3893,0 +4009,12 @@\n+  if (!EnableValhalla || (is_interpreter_only() && !CDSConfig::is_dumping_archive() && !UseSharedSpaces)) {\n+    \/\/ Disable calling convention optimizations if inline types are not supported.\n+    \/\/ Also these aren't useful in -Xint. However, don't disable them when dumping or using\n+    \/\/ the CDS archive, as the values must match between dumptime and runtime.\n+    FLAG_SET_DEFAULT(InlineTypePassFieldsAsArgs, false);\n+    FLAG_SET_DEFAULT(InlineTypeReturnedAsFields, false);\n+  }\n+  if (!UseNonAtomicValueFlattening && !UseNullableValueFlattening && !UseAtomicValueFlattening) {\n+    \/\/ Flattening is disabled\n+    FLAG_SET_DEFAULT(UseArrayFlattening, false);\n+    FLAG_SET_DEFAULT(UseFieldFlattening, false);\n+  }\n","filename":"src\/hotspot\/share\/runtime\/arguments.cpp","additions":146,"deletions":18,"binary":false,"changes":164,"status":"modified"},{"patch":"@@ -53,0 +53,3 @@\n+#include \"oops\/flatArrayKlass.hpp\"\n+#include \"oops\/flatArrayOop.hpp\"\n+#include \"oops\/inlineKlass.inline.hpp\"\n@@ -300,0 +303,18 @@\n+\n+static Klass* get_refined_array_klass(Klass* k, frame* fr, RegisterMap* map, ObjectValue* sv, TRAPS) {\n+  \/\/ If it's an array, get the properties\n+  if (k->is_array_klass() && !k->is_typeArray_klass()) {\n+    assert(!k->is_refArray_klass() && !k->is_flatArray_klass(), \"Unexpected refined klass\");\n+    nmethod* nm = fr->cb()->as_nmethod_or_null();\n+    if (nm->is_compiled_by_c2()) {\n+      assert(sv->has_properties(), \"Property information is missing\");\n+      ArrayKlass::ArrayProperties props = static_cast<ArrayKlass::ArrayProperties>(StackValue::create_stack_value(fr, map, sv->properties())->get_jint());\n+      k = ObjArrayKlass::cast(k)->klass_with_properties(props, THREAD);\n+    } else {\n+      \/\/ TODO Graal needs to be fixed. Just go with the default properties for now\n+      k = ObjArrayKlass::cast(k)->klass_with_properties(ArrayKlass::ArrayProperties::DEFAULT, THREAD);\n+    }\n+  }\n+  return k;\n+}\n+\n@@ -301,2 +322,2 @@\n-static void print_objects(JavaThread* deoptee_thread,\n-                          GrowableArray<ScopeValue*>* objects, bool realloc_failures) {\n+static void print_objects(JavaThread* deoptee_thread, frame* deoptee, RegisterMap* map,\n+                          GrowableArray<ScopeValue*>* objects, bool realloc_failures, TRAPS) {\n@@ -318,0 +339,1 @@\n+    k = get_refined_array_klass(k, deoptee, map, sv, THREAD);\n@@ -351,2 +373,13 @@\n-  bool save_oop_result = chunk->at(0)->scope()->return_oop() && !thread->popframe_forcing_deopt_reexecution() && (exec_mode == Deoptimization::Unpack_deopt);\n-  Handle return_value;\n+  ScopeDesc* scope = chunk->at(0)->scope();\n+  bool save_oop_result = scope->return_oop() && !thread->popframe_forcing_deopt_reexecution() && (exec_mode == Deoptimization::Unpack_deopt);\n+  \/\/ In case of the return of multiple values, we must take care\n+  \/\/ of all oop return values.\n+  GrowableArray<Handle> return_oops;\n+  InlineKlass* vk = nullptr;\n+  if (save_oop_result && scope->return_scalarized()) {\n+    vk = InlineKlass::returned_inline_klass(map);\n+    if (vk != nullptr) {\n+      vk->save_oop_fields(map, return_oops);\n+      save_oop_result = false;\n+    }\n+  }\n@@ -358,1 +391,1 @@\n-    return_value = Handle(thread, result);\n+    return_oops.push(Handle(thread, result));\n@@ -365,1 +398,1 @@\n-  if (objects != nullptr) {\n+  if (objects != nullptr || vk != nullptr) {\n@@ -370,1 +403,9 @@\n-      realloc_failures = Deoptimization::realloc_objects(thread, &deoptee, &map, objects, CHECK_AND_CLEAR_(true));\n+      if (vk != nullptr) {\n+        realloc_failures = Deoptimization::realloc_inline_type_result(vk, map, return_oops, CHECK_AND_CLEAR_(true));\n+      }\n+      if (objects != nullptr) {\n+        realloc_failures = realloc_failures || Deoptimization::realloc_objects(thread, &deoptee, &map, objects, CHECK_AND_CLEAR_(true));\n+        guarantee(compiled_method != nullptr, \"deopt must be associated with an nmethod\");\n+        bool is_jvmci = compiled_method->is_compiled_by_jvmci();\n+        Deoptimization::reassign_fields(&deoptee, &map, objects, realloc_failures, is_jvmci, CHECK_AND_CLEAR_(true));\n+      }\n@@ -375,1 +416,9 @@\n-      realloc_failures = Deoptimization::realloc_objects(thread, &deoptee, &map, objects, THREAD);\n+      if (vk != nullptr) {\n+        realloc_failures = Deoptimization::realloc_inline_type_result(vk, map, return_oops, THREAD);\n+      }\n+      if (objects != nullptr) {\n+        realloc_failures = realloc_failures || Deoptimization::realloc_objects(thread, &deoptee, &map, objects, THREAD);\n+        guarantee(compiled_method != nullptr, \"deopt must be associated with an nmethod\");\n+        bool is_jvmci = compiled_method->is_compiled_by_jvmci();\n+        Deoptimization::reassign_fields(&deoptee, &map, objects, realloc_failures, is_jvmci, THREAD);\n+      }\n@@ -378,5 +427,2 @@\n-    guarantee(compiled_method != nullptr, \"deopt must be associated with an nmethod\");\n-    bool is_jvmci = compiled_method->is_compiled_by_jvmci();\n-    Deoptimization::reassign_fields(&deoptee, &map, objects, realloc_failures, is_jvmci);\n-    if (TraceDeoptimization) {\n-      print_objects(deoptee_thread, objects, realloc_failures);\n+    if (TraceDeoptimization && objects != nullptr) {\n+      print_objects(deoptee_thread, &deoptee, &map, objects, realloc_failures, thread);\n@@ -385,1 +431,1 @@\n-  if (save_oop_result) {\n+  if (save_oop_result || vk != nullptr) {\n@@ -387,1 +433,2 @@\n-    deoptee.set_saved_oop_result(&map, return_value());\n+    assert(return_oops.length() == 1, \"no inline type\");\n+    deoptee.set_saved_oop_result(&map, return_oops.pop()());\n@@ -721,1 +768,1 @@\n-  \/\/ If the sender is deoptimized the we must retrieve the address of the handler\n+  \/\/ If the sender is deoptimized we must retrieve the address of the handler\n@@ -1238,2 +1285,10 @@\n-\n-    oop obj = nullptr;\n+    k = get_refined_array_klass(k, fr, reg_map, sv, THREAD);\n+\n+    \/\/ Check if the object may be null and has an additional null_marker input that needs\n+    \/\/ to be checked before using the field values. Skip re-allocation if it is null.\n+    if (k->is_inline_klass() && sv->has_properties()) {\n+      jint null_marker = StackValue::create_stack_value(fr, reg_map, sv->properties())->get_jint();\n+      if (null_marker == 0) {\n+        continue;\n+      }\n+    }\n@@ -1242,0 +1297,1 @@\n+    oop obj = nullptr;\n@@ -1269,0 +1325,4 @@\n+    } else if (k->is_flatArray_klass()) {\n+      FlatArrayKlass* ak = FlatArrayKlass::cast(k);\n+      \/\/ Inline type array must be zeroed because not all memory is reassigned\n+      obj = ak->allocate_instance(sv->field_size(), ak->properties(), THREAD);\n@@ -1275,2 +1335,2 @@\n-    } else if (k->is_objArray_klass()) {\n-      ObjArrayKlass* ak = ObjArrayKlass::cast(k);\n+    } else if (k->is_refArray_klass()) {\n+      RefArrayKlass* ak = RefArrayKlass::cast(k);\n@@ -1278,1 +1338,1 @@\n-      obj = ak->allocate_instance(sv->field_size(), THREAD);\n+      obj = ak->allocate_instance(sv->field_size(), ak->properties(), THREAD);\n@@ -1300,0 +1360,15 @@\n+\/\/ We're deoptimizing at the return of a call, inline type fields are\n+\/\/ in registers. When we go back to the interpreter, it will expect a\n+\/\/ reference to an inline type instance. Allocate and initialize it from\n+\/\/ the register values here.\n+bool Deoptimization::realloc_inline_type_result(InlineKlass* vk, const RegisterMap& map, GrowableArray<Handle>& return_oops, TRAPS) {\n+  oop new_vt = vk->realloc_result(map, return_oops, THREAD);\n+  if (new_vt == nullptr) {\n+    CLEAR_PENDING_EXCEPTION;\n+    THROW_OOP_(Universe::out_of_memory_error_realloc_objects(), true);\n+  }\n+  return_oops.clear();\n+  return_oops.push(Handle(THREAD, new_vt));\n+  return false;\n+}\n+\n@@ -1471,0 +1546,3 @@\n+  InstanceKlass* _klass;\n+  bool _is_flat;\n+  bool _is_null_free;\n@@ -1472,4 +1550,1 @@\n-  ReassignedField() {\n-    _offset = 0;\n-    _type = T_ILLEGAL;\n-  }\n+  ReassignedField() : _offset(0), _type(T_ILLEGAL), _klass(nullptr), _is_flat(false), _is_null_free(false) { }\n@@ -1489,0 +1564,6 @@\n+      if (fs.is_flat()) {\n+        field._is_flat = true;\n+        field._is_null_free = fs.is_null_free_inline_type();\n+        \/\/ Resolve klass of flat inline type field\n+        field._klass = InlineKlass::cast(klass->get_inline_type_field_klass(fs.index()));\n+      }\n@@ -1495,2 +1576,3 @@\n-\/\/ Restore fields of an eliminated instance object employing the same field order used by the compiler.\n-static int reassign_fields_by_klass(InstanceKlass* klass, frame* fr, RegisterMap* reg_map, ObjectValue* sv, int svIndex, oop obj, bool is_jvmci) {\n+\/\/ Restore fields of an eliminated instance object employing the same field order used by the\n+\/\/ compiler when it scalarizes an object at safepoints.\n+static int reassign_fields_by_klass(InstanceKlass* klass, frame* fr, RegisterMap* reg_map, ObjectValue* sv, int svIndex, oop obj, bool is_jvmci, int base_offset, TRAPS) {\n@@ -1499,0 +1581,19 @@\n+    BasicType type = fields->at(i)._type;\n+    int offset = base_offset + fields->at(i)._offset;\n+    \/\/ Check for flat inline type field before accessing the ScopeValue because it might not have any fields\n+    if (fields->at(i)._is_flat) {\n+      \/\/ Recursively re-assign flat inline type fields\n+      InstanceKlass* vk = fields->at(i)._klass;\n+      assert(vk != nullptr, \"must be resolved\");\n+      offset -= InlineKlass::cast(vk)->payload_offset(); \/\/ Adjust offset to omit oop header\n+      svIndex = reassign_fields_by_klass(vk, fr, reg_map, sv, svIndex, obj, is_jvmci, offset, CHECK_0);\n+      if (!fields->at(i)._is_null_free) {\n+        ScopeValue* scope_field = sv->field_at(svIndex);\n+        StackValue* value = StackValue::create_stack_value(fr, reg_map, scope_field);\n+        int nm_offset = offset + InlineKlass::cast(vk)->null_marker_offset();\n+        obj->bool_field_put(nm_offset, value->get_jint() & 1);\n+        svIndex++;\n+      }\n+      continue; \/\/ Continue because we don't need to increment svIndex\n+    }\n+\n@@ -1501,3 +1602,2 @@\n-    int offset = fields->at(i)._offset;\n-    BasicType type = fields->at(i)._type;\n-      case T_OBJECT: case T_ARRAY:\n+      case T_OBJECT:\n+      case T_ARRAY:\n@@ -1575,0 +1675,1 @@\n+\n@@ -1578,0 +1679,23 @@\n+\/\/ restore fields of an eliminated inline type array\n+void Deoptimization::reassign_flat_array_elements(frame* fr, RegisterMap* reg_map, ObjectValue* sv, flatArrayOop obj, FlatArrayKlass* vak, bool is_jvmci, TRAPS) {\n+  InlineKlass* vk = vak->element_klass();\n+  assert(vk->maybe_flat_in_array(), \"should only be used for flat inline type arrays\");\n+  \/\/ Adjust offset to omit oop header\n+  int base_offset = arrayOopDesc::base_offset_in_bytes(T_FLAT_ELEMENT) - vk->payload_offset();\n+  \/\/ Initialize all elements of the flat inline type array\n+  for (int i = 0; i < sv->field_size(); i++) {\n+    ObjectValue* val = sv->field_at(i)->as_ObjectValue();\n+    int offset = base_offset + (i << Klass::layout_helper_log2_element_size(vak->layout_helper()));\n+    reassign_fields_by_klass(vk, fr, reg_map, val, 0, (oop)obj, is_jvmci, offset, CHECK);\n+    if (!obj->is_null_free_array()) {\n+      jboolean null_marker_value;\n+      if (val->has_properties()) {\n+        null_marker_value = StackValue::create_stack_value(fr, reg_map, val->properties())->get_jint() & 1;\n+      } else {\n+        null_marker_value = 1;\n+      }\n+      obj->bool_field_put(offset + vk->null_marker_offset(), null_marker_value);\n+    }\n+  }\n+}\n+\n@@ -1579,1 +1703,1 @@\n-void Deoptimization::reassign_fields(frame* fr, RegisterMap* reg_map, GrowableArray<ScopeValue*>* objects, bool realloc_failures, bool is_jvmci) {\n+void Deoptimization::reassign_fields(frame* fr, RegisterMap* reg_map, GrowableArray<ScopeValue*>* objects, bool realloc_failures, bool is_jvmci, TRAPS) {\n@@ -1584,0 +1708,2 @@\n+    k = get_refined_array_klass(k, fr, reg_map, sv, THREAD);\n+\n@@ -1585,1 +1711,1 @@\n-    assert(obj.not_null() || realloc_failures, \"reallocation was missed\");\n+    assert(obj.not_null() || realloc_failures || sv->has_properties(), \"reallocation was missed\");\n@@ -1623,1 +1749,4 @@\n-      reassign_fields_by_klass(ik, fr, reg_map, sv, 0, obj(), is_jvmci);\n+      reassign_fields_by_klass(ik, fr, reg_map, sv, 0, obj(), is_jvmci, 0, CHECK);\n+    } else if (k->is_flatArray_klass()) {\n+      FlatArrayKlass* vak = FlatArrayKlass::cast(k);\n+      reassign_flat_array_elements(fr, reg_map, sv, (flatArrayOop) obj(), vak, is_jvmci, CHECK);\n@@ -1627,1 +1756,1 @@\n-    } else if (k->is_objArray_klass()) {\n+    } else if (k->is_refArray_klass()) {\n@@ -1805,1 +1934,1 @@\n-  \/\/ Deoptimize only if the frame comes from compile code.\n+  \/\/ Deoptimize only if the frame comes from compiled code.\n","filename":"src\/hotspot\/share\/runtime\/deoptimization.cpp","additions":164,"deletions":35,"binary":false,"changes":199,"status":"modified"},{"patch":"@@ -38,0 +38,1 @@\n+#include \"oops\/inlineKlass.hpp\"\n@@ -62,0 +63,3 @@\n+#ifdef COMPILER1\n+#include \"c1\/c1_Runtime1.hpp\"\n+#endif\n@@ -364,0 +368,19 @@\n+\n+#ifdef COMPILER1\n+  if (nm->is_compiled_by_c1() && nm->method()->has_scalarized_args() &&\n+      pc() < nm->verified_inline_entry_point()) {\n+    \/\/ The VEP and VIEP(RO) of C1-compiled methods call into the runtime to buffer scalarized value\n+    \/\/ type args. We can't deoptimize at that point because the buffers have not yet been initialized.\n+    \/\/ Also, if the method is synchronized, we first need to acquire the lock.\n+    \/\/ Don't patch the return pc to delay deoptimization until we enter the method body (the check\n+    \/\/ added in LIRGenerator::do_Base will detect the pending deoptimization by checking the original_pc).\n+#if defined ASSERT && !defined AARCH64   \/\/ Stub call site does not look like NativeCall on AArch64\n+    NativeCall* call = nativeCall_before(this->pc());\n+    address dest = call->destination();\n+    assert(dest == Runtime1::entry_for(StubId::c1_buffer_inline_args_no_receiver_id) ||\n+           dest == Runtime1::entry_for(StubId::c1_buffer_inline_args_id), \"unexpected safepoint in entry point\");\n+#endif\n+    return;\n+  }\n+#endif\n+\n@@ -774,1 +797,3 @@\n-      _f->do_oop(addr);\n+      if (_f != nullptr) {\n+        _f->do_oop(addr);\n+      }\n@@ -786,1 +811,3 @@\n-        _f->do_oop(addr);\n+        if (_f != nullptr) {\n+          _f->do_oop(addr);\n+        }\n@@ -1015,0 +1042,1 @@\n+    assert(_offset < _arg_size, \"out of bounds\");\n@@ -1041,5 +1069,1 @@\n-    _arg_size  = ArgumentSizeComputer(signature).size() + (has_receiver ? 1 : 0) + (has_appendix ? 1 : 0);\n-\n-    int arg_size;\n-    _regs = SharedRuntime::find_callee_arguments(signature, has_receiver, has_appendix, &arg_size);\n-    assert(arg_size == _arg_size, \"wrong arg size\");\n+    _regs = SharedRuntime::find_callee_arguments(signature, has_receiver, has_appendix, &_arg_size);\n@@ -1425,2 +1449,2 @@\n-                    FormatBuffer<1024>(\"#%d nmethod \" INTPTR_FORMAT \" for method J %s%s\", frame_no,\n-                                       p2i(nm),\n+                    FormatBuffer<1024>(\"#%d nmethod (%s %d) \" INTPTR_FORMAT \" for method J %s%s\", frame_no,\n+                                       nm->is_compiled_by_c1() ? \"c1\" : \"c2\", nm->frame_size(), p2i(nm),\n@@ -1436,0 +1460,5 @@\n+      CompiledEntrySignature ces(m);\n+      ces.compute_calling_conventions(false);\n+      const GrowableArray<SigEntry>* sig_cc = nm->is_compiled_by_c2() ? ces.sig_cc() : ces.sig();\n+      const VMRegPair* regs = nm->is_compiled_by_c2() ? ces.regs_cc() : ces.regs();\n+\n@@ -1437,21 +1466,0 @@\n-      int sizeargs = m->size_of_parameters();\n-\n-      BasicType* sig_bt = NEW_RESOURCE_ARRAY(BasicType, sizeargs);\n-      VMRegPair* regs   = NEW_RESOURCE_ARRAY(VMRegPair, sizeargs);\n-      {\n-        int sig_index = 0;\n-        if (!m->is_static()) {\n-          sig_bt[sig_index++] = T_OBJECT; \/\/ 'this'\n-        }\n-        for (SignatureStream ss(m->signature()); !ss.at_return_type(); ss.next()) {\n-          BasicType t = ss.type();\n-          assert(type2size[t] == 1 || type2size[t] == 2, \"size is 1 or 2\");\n-          sig_bt[sig_index++] = t;\n-          if (type2size[t] == 2) {\n-            sig_bt[sig_index++] = T_VOID;\n-          }\n-        }\n-        assert(sig_index == sizeargs, \"\");\n-      }\n-      int stack_arg_slots = SharedRuntime::java_calling_convention(sig_bt, regs, sizeargs);\n-      assert(stack_arg_slots ==  nm->as_nmethod()->num_stack_arg_slots(false \/* rounded *\/) || nm->is_osr_method(), \"\");\n@@ -1461,1 +1469,1 @@\n-      for (SignatureStream ss(m->signature()); !ss.at_return_type(); ) {\n+      for (ExtendedSignature sig = ExtendedSignature(sig_cc, SigEntryFilter()); !sig.at_end(); ++sig) {\n@@ -1463,3 +1471,1 @@\n-        bool at_old_sp = false;\n-        BasicType t = (at_this ? T_OBJECT : ss.type());\n-        assert(t == sig_bt[sig_index], \"sigs in sync\");\n+        BasicType t = (*sig)._bt;\n@@ -1479,3 +1485,0 @@\n-        if (!at_this) {\n-          ss.next();\n-        }\n","filename":"src\/hotspot\/share\/runtime\/frame.cpp","additions":40,"deletions":37,"binary":false,"changes":77,"status":"modified"},{"patch":"@@ -48,0 +48,1 @@\n+#include \"memory\/oopFactory.hpp\"\n@@ -51,0 +52,3 @@\n+#include \"oops\/access.hpp\"\n+#include \"oops\/fieldStreams.inline.hpp\"\n+#include \"oops\/inlineKlass.inline.hpp\"\n@@ -54,0 +58,1 @@\n+#include \"oops\/objArrayOop.inline.hpp\"\n@@ -73,0 +78,1 @@\n+#include \"runtime\/signature.hpp\"\n@@ -1254,0 +1260,16 @@\n+  \/\/ Substitutability test implementation piggy backs on static call resolution\n+  Bytecodes::Code code = caller->java_code_at(bci);\n+  if (code == Bytecodes::_if_acmpeq || code == Bytecodes::_if_acmpne) {\n+    bc = Bytecodes::_invokestatic;\n+    methodHandle attached_method(THREAD, extract_attached_method(vfst));\n+    assert(attached_method.not_null(), \"must have attached method\");\n+    vmClasses::ValueObjectMethods_klass()->initialize(CHECK_NH);\n+    LinkResolver::resolve_invoke(callinfo, receiver, attached_method, bc, false, CHECK_NH);\n+#ifdef ASSERT\n+    Symbol* subst_method_name = UseAltSubstitutabilityMethod ? vmSymbols::isSubstitutableAlt_name() : vmSymbols::isSubstitutable_name();\n+    Method* is_subst = vmClasses::ValueObjectMethods_klass()->find_method(subst_method_name, vmSymbols::object_object_boolean_signature());\n+    assert(callinfo.selected_method() == is_subst, \"must be isSubstitutable method\");\n+#endif\n+    return receiver;\n+  }\n+\n@@ -1289,0 +1311,6 @@\n+    } else {\n+      assert(attached_method->has_scalarized_args(), \"invalid use of attached method\");\n+      if (!attached_method->method_holder()->is_inline_klass()) {\n+        \/\/ Ignore the attached method in this case to not confuse below code\n+        attached_method = methodHandle(current, nullptr);\n+      }\n@@ -1297,0 +1325,1 @@\n+  bool check_null_and_abstract = true;\n@@ -1310,2 +1339,3 @@\n-    if (attached_method.is_null()) {\n-      Method* callee = bytecode.static_target(CHECK_NH);\n+    Method* callee = attached_method();\n+    if (callee == nullptr) {\n+      callee = bytecode.static_target(CHECK_NH);\n@@ -1316,7 +1346,17 @@\n-\n-    \/\/ Retrieve from a compiled argument list\n-    receiver = Handle(current, callerFrame.retrieve_receiver(&reg_map2));\n-    assert(oopDesc::is_oop_or_null(receiver()), \"\");\n-\n-    if (receiver.is_null()) {\n-      THROW_(vmSymbols::java_lang_NullPointerException(), nullHandle);\n+    bool caller_is_c1 = callerFrame.is_compiled_frame() && callerFrame.cb()->as_nmethod()->is_compiled_by_c1();\n+    if (!caller_is_c1 && callee->is_scalarized_arg(0)) {\n+      \/\/ If the receiver is an inline type that is passed as fields, no oop is available\n+      \/\/ Resolve the call without receiver null checking.\n+      assert(!callee->mismatch(), \"calls with inline type receivers should never mismatch\");\n+      assert(attached_method.not_null() && !attached_method->is_abstract(), \"must have non-abstract attached method\");\n+      if (bc == Bytecodes::_invokeinterface) {\n+        bc = Bytecodes::_invokevirtual; \/\/ C2 optimistically replaces interface calls by virtual calls\n+      }\n+      check_null_and_abstract = false;\n+    } else {\n+      \/\/ Retrieve from a compiled argument list\n+      receiver = Handle(current, callerFrame.retrieve_receiver(&reg_map2));\n+      assert(oopDesc::is_oop_or_null(receiver()), \"\");\n+      if (receiver.is_null()) {\n+        THROW_(vmSymbols::java_lang_NullPointerException(), nullHandle);\n+      }\n@@ -1329,1 +1369,1 @@\n-    LinkResolver::resolve_invoke(callinfo, receiver, attached_method, bc, CHECK_NH);\n+    LinkResolver::resolve_invoke(callinfo, receiver, attached_method, bc, check_null_and_abstract, CHECK_NH);\n@@ -1338,1 +1378,1 @@\n-  if (has_receiver) {\n+  if (has_receiver && check_null_and_abstract) {\n@@ -1366,1 +1406,1 @@\n-methodHandle SharedRuntime::find_callee_method(TRAPS) {\n+methodHandle SharedRuntime::find_callee_method(bool& caller_does_not_scalarize, TRAPS) {\n@@ -1392,0 +1432,4 @@\n+    \/\/ Calls via mismatching methods are always non-scalarized\n+    if (callinfo.resolved_method()->mismatch()) {\n+      caller_does_not_scalarize = true;\n+    }\n@@ -1399,1 +1443,1 @@\n-methodHandle SharedRuntime::resolve_helper(bool is_virtual, bool is_optimized, TRAPS) {\n+methodHandle SharedRuntime::resolve_helper(bool is_virtual, bool is_optimized, bool& caller_does_not_scalarize, TRAPS) {\n@@ -1422,0 +1466,4 @@\n+  \/\/ Calls via mismatching methods are always non-scalarized\n+  if (caller_nm->is_compiled_by_c1() || call_info.resolved_method()->mismatch()) {\n+    caller_does_not_scalarize = true;\n+  }\n@@ -1440,1 +1488,1 @@\n-    tty->print(\"resolving %s%s (%s) call to\",\n+    tty->print(\"resolving %s%s (%s) %s call to\",\n@@ -1442,1 +1490,1 @@\n-               Bytecodes::name(invoke_code));\n+               Bytecodes::name(invoke_code), (caller_does_not_scalarize) ? \"non-scalar\" : \"\");\n@@ -1481,1 +1529,1 @@\n-    inline_cache->update(&call_info, receiver->klass());\n+    inline_cache->update(&call_info, receiver->klass(), caller_does_not_scalarize);\n@@ -1485,1 +1533,1 @@\n-    callsite->set(callee_method);\n+    callsite->set(callee_method, caller_does_not_scalarize);\n@@ -1505,0 +1553,1 @@\n+  bool caller_does_not_scalarize = false;\n@@ -1506,1 +1555,1 @@\n-    callee_method = SharedRuntime::handle_ic_miss_helper(CHECK_NULL);\n+    callee_method = SharedRuntime::handle_ic_miss_helper(caller_does_not_scalarize, CHECK_NULL);\n@@ -1511,1 +1560,1 @@\n-  return get_resolved_entry(current, callee_method);\n+  return get_resolved_entry(current, callee_method, false, false, caller_does_not_scalarize);\n@@ -1552,1 +1601,5 @@\n-      return callee->get_c2i_entry();\n+      if (caller_frame.is_interpreted_frame()) {\n+        return callee->get_c2i_inline_entry();\n+      } else {\n+        return callee->get_c2i_entry();\n+      }\n@@ -1558,0 +1611,3 @@\n+  bool is_static_call = false;\n+  bool is_optimized = false;\n+  bool caller_does_not_scalarize = false;\n@@ -1560,1 +1616,1 @@\n-    callee_method = SharedRuntime::reresolve_call_site(CHECK_NULL);\n+    callee_method = SharedRuntime::reresolve_call_site(is_optimized, caller_does_not_scalarize, CHECK_NULL);\n@@ -1564,1 +1620,1 @@\n-  return get_resolved_entry(current, callee_method);\n+  return get_resolved_entry(current, callee_method, callee_method->is_static(), is_optimized, caller_does_not_scalarize);\n@@ -1603,1 +1659,2 @@\n-address SharedRuntime::get_resolved_entry(JavaThread* current, methodHandle callee_method) {\n+address SharedRuntime::get_resolved_entry(JavaThread* current, methodHandle callee_method,\n+                                          bool is_static_call, bool is_optimized, bool caller_does_not_scalarize) {\n@@ -1609,2 +1666,11 @@\n-  assert(callee_method->verified_code_entry() != nullptr, \" Jump to zero!\");\n-  return callee_method->verified_code_entry();\n+\n+  if (caller_does_not_scalarize) {\n+    assert(callee_method->verified_inline_code_entry() != nullptr, \"Jump to zero!\");\n+    return callee_method->verified_inline_code_entry();\n+  } else if (is_static_call || is_optimized) {\n+    assert(callee_method->verified_code_entry() != nullptr, \"Jump to zero!\");\n+    return callee_method->verified_code_entry();\n+  } else {\n+    assert(callee_method->verified_inline_ro_code_entry() != nullptr, \"Jump to zero!\");\n+    return callee_method->verified_inline_ro_code_entry();\n+  }\n@@ -1616,0 +1682,1 @@\n+  bool caller_does_not_scalarize = false;\n@@ -1618,1 +1685,1 @@\n-    callee_method = SharedRuntime::resolve_helper(false, false, CHECK_NULL);\n+    callee_method = SharedRuntime::resolve_helper(false, false, caller_does_not_scalarize, CHECK_NULL);\n@@ -1622,1 +1689,1 @@\n-  return get_resolved_entry(current, callee_method);\n+  return get_resolved_entry(current, callee_method, true, false, caller_does_not_scalarize);\n@@ -1628,0 +1695,1 @@\n+  bool caller_does_not_scalarize = false;\n@@ -1629,1 +1697,1 @@\n-    callee_method = SharedRuntime::resolve_helper(true, false, CHECK_NULL);\n+    callee_method = SharedRuntime::resolve_helper(true, false, caller_does_not_scalarize, CHECK_NULL);\n@@ -1633,1 +1701,1 @@\n-  return get_resolved_entry(current, callee_method);\n+  return get_resolved_entry(current, callee_method, false, false, caller_does_not_scalarize);\n@@ -1641,0 +1709,1 @@\n+  bool caller_does_not_scalarize = false;\n@@ -1642,1 +1711,1 @@\n-    callee_method = SharedRuntime::resolve_helper(true, true, CHECK_NULL);\n+    callee_method = SharedRuntime::resolve_helper(true, true, caller_does_not_scalarize, CHECK_NULL);\n@@ -1646,1 +1715,1 @@\n-  return get_resolved_entry(current, callee_method);\n+  return get_resolved_entry(current, callee_method, false, true, caller_does_not_scalarize);\n@@ -1649,1 +1718,3 @@\n-methodHandle SharedRuntime::handle_ic_miss_helper(TRAPS) {\n+\n+\n+methodHandle SharedRuntime::handle_ic_miss_helper(bool& caller_does_not_scalarize, TRAPS) {\n@@ -1667,1 +1738,1 @@\n-    tty->print(\"IC miss (%s) call to\", Bytecodes::name(bc));\n+    tty->print(\"IC miss (%s) %s call to\", Bytecodes::name(bc), (caller_does_not_scalarize) ? \"non-scalar\" : \"\");\n@@ -1699,0 +1770,4 @@\n+  \/\/ Calls via mismatching methods are always non-scalarized\n+  if (caller_nm->is_compiled_by_c1() || call_info.resolved_method()->mismatch()) {\n+    caller_does_not_scalarize = true;\n+  }\n@@ -1702,1 +1777,1 @@\n-  inline_cache->update(&call_info, receiver()->klass());\n+  inline_cache->update(&call_info, receiver()->klass(), caller_does_not_scalarize);\n@@ -1713,1 +1788,1 @@\n-methodHandle SharedRuntime::reresolve_call_site(TRAPS) {\n+methodHandle SharedRuntime::reresolve_call_site(bool& is_optimized, bool& caller_does_not_scalarize, TRAPS) {\n@@ -1723,8 +1798,21 @@\n-\n-  \/\/ Do nothing if the frame isn't a live compiled frame.\n-  \/\/ nmethod could be deoptimized by the time we get here\n-  \/\/ so no update to the caller is needed.\n-\n-  if ((caller.is_compiled_frame() && !caller.is_deoptimized_frame()) ||\n-      (caller.is_native_frame() && caller.cb()->as_nmethod()->method()->is_continuation_enter_intrinsic())) {\n-\n+  if (caller.is_compiled_frame()) {\n+    caller_does_not_scalarize = caller.cb()->as_nmethod()->is_compiled_by_c1();\n+  }\n+  assert(!caller.is_interpreted_frame(), \"must be compiled\");\n+\n+  \/\/ If the frame isn't a live compiled frame (i.e. deoptimized by the time we get here), no IC clearing must be done\n+  \/\/ for the caller. However, when the caller is C2 compiled and the callee a C1 or C2 compiled method, then we still\n+  \/\/ need to figure out whether it was an optimized virtual call with an inline type receiver. Otherwise, we end up\n+  \/\/ using the wrong method entry point and accidentally skip the buffering of the receiver.\n+  methodHandle callee_method = find_callee_method(caller_does_not_scalarize, CHECK_(methodHandle()));\n+  const bool caller_is_compiled_and_not_deoptimized = caller.is_compiled_frame() && !caller.is_deoptimized_frame();\n+  const bool caller_is_continuation_enter_intrinsic =\n+    caller.is_native_frame() && caller.cb()->as_nmethod()->method()->is_continuation_enter_intrinsic();\n+  const bool do_IC_clearing = caller_is_compiled_and_not_deoptimized || caller_is_continuation_enter_intrinsic;\n+\n+  const bool callee_compiled_with_scalarized_receiver = callee_method->has_compiled_code() &&\n+                                                        !callee_method()->is_static() &&\n+                                                        callee_method()->is_scalarized_arg(0);\n+  const bool compute_is_optimized = !caller_does_not_scalarize && callee_compiled_with_scalarized_receiver;\n+\n+  if (do_IC_clearing || compute_is_optimized) {\n@@ -1763,0 +1851,1 @@\n+        is_optimized = false;\n@@ -1765,0 +1854,1 @@\n+            assert(callee_method->is_static(), \"must be\");\n@@ -1766,2 +1856,5 @@\n-            CompiledDirectCall* cdc = CompiledDirectCall::at(call_addr);\n-            cdc->set_to_clean();\n+            is_optimized = (iter.type() == relocInfo::opt_virtual_call_type);\n+            if (do_IC_clearing) {\n+              CompiledDirectCall* cdc = CompiledDirectCall::at(call_addr);\n+              cdc->set_to_clean();\n+            }\n@@ -1770,4 +1863,5 @@\n-\n-            \/\/ compiled, dispatched call (which used to call an interpreted method)\n-            CompiledIC* inline_cache = CompiledIC_at(caller_nm, call_addr);\n-            inline_cache->set_to_clean();\n+            if (do_IC_clearing) {\n+              \/\/ compiled, dispatched call (which used to call an interpreted method)\n+              CompiledIC* inline_cache = CompiledIC_at(caller_nm, call_addr);\n+              inline_cache->set_to_clean();\n+            }\n@@ -1784,3 +1878,0 @@\n-  methodHandle callee_method = find_callee_method(CHECK_(methodHandle()));\n-\n-\n@@ -1792,1 +1883,1 @@\n-    tty->print(\"handle_wrong_method reresolving call to\");\n+    tty->print(\"handle_wrong_method reresolving %s call to\", (caller_does_not_scalarize) ? \"non-scalar\" : \"\");\n@@ -1998,0 +2089,15 @@\n+char* SharedRuntime::generate_identity_exception_message(JavaThread* current, Klass* klass) {\n+  assert(klass->is_inline_klass(), \"Must be a concrete value class\");\n+  const char* desc = \"Cannot synchronize on an instance of value class \";\n+  const char* className = klass->external_name();\n+  size_t msglen = strlen(desc) + strlen(className) + 1;\n+  char* message = NEW_RESOURCE_ARRAY(char, msglen);\n+  if (nullptr == message) {\n+    \/\/ Out of memory: can't create detailed error message\n+    message = const_cast<char*>(klass->external_name());\n+  } else {\n+    jio_snprintf(message, msglen, \"%s%s\", desc, className);\n+  }\n+  return message;\n+}\n+\n@@ -2231,5 +2337,30 @@\n- private:\n-  enum {\n-    _basic_type_bits = 4,\n-    _basic_type_mask = right_n_bits(_basic_type_bits),\n-    _basic_types_per_int = BitsPerInt \/ _basic_type_bits,\n+public:\n+  class Element {\n+  private:\n+    \/\/ The highest byte is the type of the argument. The remaining bytes contain the offset of the\n+    \/\/ field if it is flattened in the calling convention, -1 otherwise.\n+    juint _payload;\n+\n+    static constexpr int offset_bit_width = 24;\n+    static constexpr juint offset_bit_mask = (1 << offset_bit_width) - 1;\n+  public:\n+    Element(BasicType bt, int offset) : _payload((static_cast<juint>(bt) << offset_bit_width) | (juint(offset) & offset_bit_mask)) {\n+      assert(offset >= -1 && offset < jint(offset_bit_mask), \"invalid offset %d\", offset);\n+    }\n+\n+    BasicType bt() const {\n+      return static_cast<BasicType>(_payload >> offset_bit_width);\n+    }\n+\n+    int offset() const {\n+      juint res = _payload & offset_bit_mask;\n+      return res == offset_bit_mask ? -1 : res;\n+    }\n+\n+    juint hash() const {\n+      return _payload;\n+    }\n+\n+    bool operator!=(const Element& other) const {\n+      return _payload != other._payload;\n+    }\n@@ -2237,3 +2368,3 @@\n-  \/\/ TO DO:  Consider integrating this with a more global scheme for compressing signatures.\n-  \/\/ For now, 4 bits per components (plus T_VOID gaps after double\/long) is not excessive.\n-  int _length;\n+private:\n+  const bool _has_ro_adapter;\n+  const int _length;\n@@ -2243,2 +2374,8 @@\n-  int* data_pointer() {\n-    return (int*)((address)this + data_offset());\n+  Element* data_pointer() {\n+    return reinterpret_cast<Element*>(reinterpret_cast<address>(this) + data_offset());\n+  }\n+\n+  const Element& element_at(int index) {\n+    assert(index < length(), \"index %d out of bounds for length %d\", index, length());\n+    Element* data = data_pointer();\n+    return data[index];\n@@ -2248,6 +2385,5 @@\n-  AdapterFingerPrint(int total_args_passed, BasicType* sig_bt, int len) {\n-    int* data = data_pointer();\n-    \/\/ Pack the BasicTypes with 8 per int\n-    assert(len == length(total_args_passed), \"sanity\");\n-    _length = len;\n-    int sig_index = 0;\n+  AdapterFingerPrint(const GrowableArray<SigEntry>* sig, bool has_ro_adapter)\n+    : _has_ro_adapter(has_ro_adapter), _length(total_args_passed_in_sig(sig)) {\n+    Element* data = data_pointer();\n+    BasicType prev_bt = T_ILLEGAL;\n+    int vt_count = 0;\n@@ -2255,5 +2391,15 @@\n-      int value = 0;\n-      for (int byte = 0; sig_index < total_args_passed && byte < _basic_types_per_int; byte++) {\n-        int bt = adapter_encoding(sig_bt[sig_index++]);\n-        assert((bt & _basic_type_mask) == bt, \"must fit in 4 bits\");\n-        value = (value << _basic_type_bits) | bt;\n+      const SigEntry& sig_entry = sig->at(index);\n+      BasicType bt = sig_entry._bt;\n+      if (bt == T_METADATA) {\n+        \/\/ Found start of inline type in signature\n+        assert(InlineTypePassFieldsAsArgs, \"unexpected start of inline type\");\n+        vt_count++;\n+      } else if (bt == T_VOID && prev_bt != T_LONG && prev_bt != T_DOUBLE) {\n+        \/\/ Found end of inline type in signature\n+        assert(InlineTypePassFieldsAsArgs, \"unexpected end of inline type\");\n+        vt_count--;\n+        assert(vt_count >= 0, \"invalid vt_count\");\n+      } else if (vt_count == 0) {\n+        \/\/ Widen fields that are not part of a scalarized inline type argument\n+        assert(sig_entry._offset == -1, \"invalid offset for argument that is not a flattened field %d\", sig_entry._offset);\n+        bt = adapter_encoding(bt);\n@@ -2261,1 +2407,3 @@\n-      data[index] = value;\n+\n+      ::new(&data[index]) Element(bt, sig_entry._offset);\n+      prev_bt = bt;\n@@ -2263,0 +2411,1 @@\n+    assert(vt_count == 0, \"invalid vt_count\");\n@@ -2270,2 +2419,2 @@\n-  static int length(int total_args) {\n-    return (total_args + (_basic_types_per_int-1)) \/ _basic_types_per_int;\n+  static int total_args_passed_in_sig(const GrowableArray<SigEntry>* sig) {\n+    return (sig != nullptr) ? sig->length() : 0;\n@@ -2275,1 +2424,1 @@\n-    return (int)heap_word_size(sizeof(AdapterFingerPrint) + (len * sizeof(int)));\n+    return (int)heap_word_size(sizeof(AdapterFingerPrint) + (len * sizeof(Element)));\n@@ -2281,1 +2430,1 @@\n-  static int adapter_encoding(BasicType in) {\n+  static BasicType adapter_encoding(BasicType in) {\n@@ -2287,1 +2436,1 @@\n-        \/\/ There are all promoted to T_INT in the calling convention\n+        \/\/ They are all promoted to T_INT in the calling convention\n@@ -2320,0 +2469,1 @@\n+public:\n@@ -2323,10 +2473,1 @@\n-      unsigned val = (unsigned)value(i);\n-      \/\/ args are packed so that first\/lower arguments are in the highest\n-      \/\/ bits of each int value, so iterate from highest to the lowest\n-      for (int j = 32 - _basic_type_bits; j >= 0; j -= _basic_type_bits) {\n-        unsigned v = (val >> j) & _basic_type_mask;\n-        if (v == 0) {\n-          continue;\n-        }\n-        function(v);\n-      }\n+      function(element_at(i));\n@@ -2336,3 +2477,2 @@\n- public:\n-  static AdapterFingerPrint* allocate(int total_args_passed, BasicType* sig_bt) {\n-    int len = length(total_args_passed);\n+  static AdapterFingerPrint* allocate(const GrowableArray<SigEntry>* sig, bool has_ro_adapter = false) {\n+    int len = total_args_passed_in_sig(sig);\n@@ -2340,1 +2480,1 @@\n-    AdapterFingerPrint* afp = new (size_in_bytes) AdapterFingerPrint(total_args_passed, sig_bt, len);\n+    AdapterFingerPrint* afp = new (size_in_bytes) AdapterFingerPrint(sig, has_ro_adapter);\n@@ -2349,3 +2489,2 @@\n-  int value(int index) {\n-    int* data = data_pointer();\n-    return data[index];\n+  bool has_ro_adapter() const {\n+    return _has_ro_adapter;\n@@ -2354,1 +2493,1 @@\n-  int length() {\n+  int length() const {\n@@ -2361,1 +2500,1 @@\n-      int v = value(i);\n+      const Element& v = element_at(i);\n@@ -2363,1 +2502,1 @@\n-      hash = ((hash << 8) ^ v ^ (hash >> 5)) + 3;\n+      hash = ((hash << 8) ^ v.hash() ^ (hash >> 5)) + 3;\n@@ -2370,1 +2509,6 @@\n-    st.print(\"0x\");\n+    st.print(\"{\");\n+    if (_has_ro_adapter) {\n+      st.print(\"has_ro_adapter\");\n+    } else {\n+      st.print(\"no_ro_adapter\");\n+    }\n@@ -2372,1 +2516,3 @@\n-      st.print(\"%x\", value(i));\n+      st.print(\", \");\n+      const Element& elem = element_at(i);\n+      st.print(\"{%s, %d}\", type2name(elem.bt()), elem.offset());\n@@ -2374,0 +2520,1 @@\n+    st.print(\"}\");\n@@ -2380,1 +2527,1 @@\n-    iterate_args([&] (int arg) {\n+    iterate_args([&] (const Element& arg) {\n@@ -2383,1 +2530,1 @@\n-        if (arg == T_VOID) {\n+        if (arg.bt() == T_VOID) {\n@@ -2389,7 +2536,4 @@\n-      switch (arg) {\n-        case T_INT:    st.print(\"I\");    break;\n-        case T_LONG:   long_prev = true; break;\n-        case T_FLOAT:  st.print(\"F\");    break;\n-        case T_DOUBLE: st.print(\"D\");    break;\n-        case T_VOID:   break;\n-        default: ShouldNotReachHere();\n+      if (arg.bt() == T_LONG) {\n+        long_prev = true;\n+      } else if (arg.bt() != T_VOID) {\n+        st.print(\"%c\", type2char(arg.bt()));\n@@ -2404,52 +2548,3 @@\n-  BasicType* as_basic_type(int& nargs) {\n-    nargs = 0;\n-    GrowableArray<BasicType> btarray;\n-    bool long_prev = false;\n-\n-    iterate_args([&] (int arg) {\n-      if (long_prev) {\n-        long_prev = false;\n-        if (arg == T_VOID) {\n-          btarray.append(T_LONG);\n-        } else {\n-          btarray.append(T_OBJECT); \/\/ it could be T_ARRAY; it shouldn't matter\n-        }\n-      }\n-      switch (arg) {\n-        case T_INT: \/\/ fallthrough\n-        case T_FLOAT: \/\/ fallthrough\n-        case T_DOUBLE:\n-        case T_VOID:\n-          btarray.append((BasicType)arg);\n-          break;\n-        case T_LONG:\n-          long_prev = true;\n-          break;\n-        default: ShouldNotReachHere();\n-      }\n-    });\n-\n-    if (long_prev) {\n-      btarray.append(T_OBJECT);\n-    }\n-\n-    nargs = btarray.length();\n-    BasicType* sig_bt = NEW_RESOURCE_ARRAY(BasicType, nargs);\n-    int index = 0;\n-    GrowableArrayIterator<BasicType> iter = btarray.begin();\n-    while (iter != btarray.end()) {\n-      sig_bt[index++] = *iter;\n-      ++iter;\n-    }\n-    assert(index == btarray.length(), \"sanity check\");\n-#ifdef ASSERT\n-    {\n-      AdapterFingerPrint* compare_fp = AdapterFingerPrint::allocate(nargs, sig_bt);\n-      assert(this->equals(compare_fp), \"sanity check\");\n-      AdapterFingerPrint::deallocate(compare_fp);\n-    }\n-#endif\n-    return sig_bt;\n-  }\n-\n-    if (other->_length != _length) {\n+    if (other->_has_ro_adapter != _has_ro_adapter) {\n+      return false;\n+    } else if (other->_length != _length) {\n@@ -2460,1 +2555,1 @@\n-        if (value(i) != other->value(i)) {\n+        if (element_at(i) != other->element_at(i)) {\n@@ -2503,1 +2598,1 @@\n-AdapterHandlerEntry* AdapterHandlerLibrary::lookup(int total_args_passed, BasicType* sig_bt) {\n+AdapterHandlerEntry* AdapterHandlerLibrary::lookup(const GrowableArray<SigEntry>* sig, bool has_ro_adapter) {\n@@ -2506,1 +2601,1 @@\n-  AdapterFingerPrint* fp = AdapterFingerPrint::allocate(total_args_passed, sig_bt);\n+  AdapterFingerPrint* fp = AdapterFingerPrint::allocate(sig, has_ro_adapter);\n@@ -2563,1 +2658,1 @@\n-static const int AdapterHandlerLibrary_size = 16*K;\n+static const int AdapterHandlerLibrary_size = 48*K;\n@@ -2611,1 +2706,3 @@\n-    _no_arg_handler = create_adapter(0, nullptr);\n+    CompiledEntrySignature no_args;\n+    no_args.compute_calling_conventions();\n+    _no_arg_handler = create_adapter(no_args, true);\n@@ -2613,2 +2710,4 @@\n-    BasicType obj_args[] = { T_OBJECT };\n-    _obj_arg_handler = create_adapter(1, obj_args);\n+    CompiledEntrySignature obj_args;\n+    SigEntry::add_entry(obj_args.sig(), T_OBJECT);\n+    obj_args.compute_calling_conventions();\n+    _obj_arg_handler = create_adapter(obj_args, true);\n@@ -2616,2 +2715,4 @@\n-    BasicType int_args[] = { T_INT };\n-    _int_arg_handler = create_adapter(1, int_args);\n+    CompiledEntrySignature int_args;\n+    SigEntry::add_entry(int_args.sig(), T_INT);\n+    int_args.compute_calling_conventions();\n+    _int_arg_handler = create_adapter(int_args, true);\n@@ -2619,2 +2720,5 @@\n-    BasicType obj_int_args[] = { T_OBJECT, T_INT };\n-    _obj_int_arg_handler = create_adapter(2, obj_int_args);\n+    CompiledEntrySignature obj_int_args;\n+    SigEntry::add_entry(obj_int_args.sig(), T_OBJECT);\n+    SigEntry::add_entry(obj_int_args.sig(), T_INT);\n+    obj_int_args.compute_calling_conventions();\n+    _obj_int_arg_handler = create_adapter(obj_int_args, true);\n@@ -2622,2 +2726,5 @@\n-    BasicType obj_obj_args[] = { T_OBJECT, T_OBJECT };\n-    _obj_obj_arg_handler = create_adapter(2, obj_obj_args);\n+    CompiledEntrySignature obj_obj_args;\n+    SigEntry::add_entry(obj_obj_args.sig(), T_OBJECT);\n+    SigEntry::add_entry(obj_obj_args.sig(), T_OBJECT);\n+    obj_obj_args.compute_calling_conventions();\n+    _obj_obj_arg_handler = create_adapter(obj_obj_args, true);\n@@ -2657,0 +2764,3 @@\n+      if (InlineTypePassFieldsAsArgs && method->method_holder()->is_inline_klass()) {\n+        return nullptr;\n+      }\n@@ -2660,1 +2770,10 @@\n-      case JVM_SIGNATURE_CLASS:\n+      case JVM_SIGNATURE_CLASS: {\n+        if (InlineTypePassFieldsAsArgs) {\n+          SignatureStream ss(method->signature());\n+          InlineKlass* vk = ss.as_inline_klass(method->method_holder());\n+          if (vk != nullptr) {\n+            return nullptr;\n+          }\n+        }\n+        return _obj_arg_handler;\n+      }\n@@ -2671,1 +2790,1 @@\n-             !method->is_static()) {\n+             !method->is_static() && (!InlineTypePassFieldsAsArgs || !method->method_holder()->is_inline_klass())) {\n@@ -2673,1 +2792,10 @@\n-      case JVM_SIGNATURE_CLASS:\n+      case JVM_SIGNATURE_CLASS: {\n+        if (InlineTypePassFieldsAsArgs) {\n+          SignatureStream ss(method->signature());\n+          InlineKlass* vk = ss.as_inline_klass(method->method_holder());\n+          if (vk != nullptr) {\n+            return nullptr;\n+          }\n+        }\n+        return _obj_obj_arg_handler;\n+      }\n@@ -2687,5 +2815,9 @@\n-class AdapterSignatureIterator : public SignatureIterator {\n- private:\n-  BasicType stack_sig_bt[16];\n-  BasicType* sig_bt;\n-  int index;\n+CompiledEntrySignature::CompiledEntrySignature(Method* method) :\n+  _method(method), _num_inline_args(0), _has_inline_recv(false),\n+  _regs(nullptr), _regs_cc(nullptr), _regs_cc_ro(nullptr),\n+  _args_on_stack(0), _args_on_stack_cc(0), _args_on_stack_cc_ro(0),\n+  _c1_needs_stack_repair(false), _c2_needs_stack_repair(false), _supers(nullptr) {\n+  _sig = new GrowableArray<SigEntry>((method != nullptr) ? method->size_of_parameters() : 1);\n+  _sig_cc = new GrowableArray<SigEntry>((method != nullptr) ? method->size_of_parameters() : 1);\n+  _sig_cc_ro = new GrowableArray<SigEntry>((method != nullptr) ? method->size_of_parameters() : 1);\n+}\n@@ -2693,11 +2825,24 @@\n- public:\n-  AdapterSignatureIterator(Symbol* signature,\n-                           fingerprint_t fingerprint,\n-                           bool is_static,\n-                           int total_args_passed) :\n-    SignatureIterator(signature, fingerprint),\n-    index(0)\n-  {\n-    sig_bt = (total_args_passed <= 16) ? stack_sig_bt : NEW_RESOURCE_ARRAY(BasicType, total_args_passed);\n-    if (!is_static) { \/\/ Pass in receiver first\n-      sig_bt[index++] = T_OBJECT;\n+\/\/ See if we can save space by sharing the same entry for VIEP and VIEP(RO),\n+\/\/ or the same entry for VEP and VIEP(RO).\n+CodeOffsets::Entries CompiledEntrySignature::c1_inline_ro_entry_type() const {\n+  if (!has_scalarized_args()) {\n+    \/\/ VEP\/VIEP\/VIEP(RO) all share the same entry. There's no packing.\n+    return CodeOffsets::Verified_Entry;\n+  }\n+  if (_method->is_static()) {\n+    \/\/ Static methods don't need VIEP(RO)\n+    return CodeOffsets::Verified_Entry;\n+  }\n+\n+  if (has_inline_recv()) {\n+    if (num_inline_args() == 1) {\n+      \/\/ Share same entry for VIEP and VIEP(RO).\n+      \/\/ This is quite common: we have an instance method in an InlineKlass that has\n+      \/\/ no inline type args other than <this>.\n+      return CodeOffsets::Verified_Inline_Entry;\n+    } else {\n+      assert(num_inline_args() > 1, \"must be\");\n+      \/\/ No sharing:\n+      \/\/   VIEP(RO) -- <this> is passed as object\n+      \/\/   VEP      -- <this> is passed as fields\n+      return CodeOffsets::Verified_Inline_Entry_RO;\n@@ -2705,1 +2850,0 @@\n-    do_parameters_on(this);\n@@ -2708,2 +2852,9 @@\n-  BasicType* basic_types() {\n-    return sig_bt;\n+  \/\/ Either a static method, or <this> is not an inline type\n+  if (args_on_stack_cc() != args_on_stack_cc_ro()) {\n+    \/\/ No sharing:\n+    \/\/ Some arguments are passed on the stack, and we have inserted reserved entries\n+    \/\/ into the VEP, but we never insert reserved entries into the VIEP(RO).\n+    return CodeOffsets::Verified_Inline_Entry_RO;\n+  } else {\n+    \/\/ Share same entry for VEP and VIEP(RO).\n+    return CodeOffsets::Verified_Entry;\n@@ -2711,0 +2862,1 @@\n+}\n@@ -2712,3 +2864,39 @@\n-#ifdef ASSERT\n-  int slots() {\n-    return index;\n+\/\/ Returns all super methods (transitive) in classes and interfaces that are overridden by the current method.\n+GrowableArray<Method*>* CompiledEntrySignature::get_supers() {\n+  if (_supers != nullptr) {\n+    return _supers;\n+  }\n+  _supers = new GrowableArray<Method*>();\n+  \/\/ Skip private, static, and <init> methods\n+  if (_method->is_private() || _method->is_static() || _method->is_object_constructor()) {\n+    return _supers;\n+  }\n+  Symbol* name = _method->name();\n+  Symbol* signature = _method->signature();\n+  const Klass* holder = _method->method_holder()->super();\n+  Symbol* holder_name = holder->name();\n+  ThreadInVMfromUnknown tiv;\n+  JavaThread* current = JavaThread::current();\n+  HandleMark hm(current);\n+  Handle loader(current, _method->method_holder()->class_loader());\n+\n+  \/\/ Walk up the class hierarchy and search for super methods\n+  while (holder != nullptr) {\n+    Method* super_method = holder->lookup_method(name, signature);\n+    if (super_method == nullptr) {\n+      break;\n+    }\n+    if (!super_method->is_static() && !super_method->is_private() &&\n+        (!super_method->is_package_private() ||\n+         super_method->method_holder()->is_same_class_package(loader(), holder_name))) {\n+      _supers->push(super_method);\n+    }\n+    holder = super_method->method_holder()->super();\n+  }\n+  \/\/ Search interfaces for super methods\n+  Array<InstanceKlass*>* interfaces = _method->method_holder()->transitive_interfaces();\n+  for (int i = 0; i < interfaces->length(); ++i) {\n+    Method* m = interfaces->at(i)->lookup_method(name, signature);\n+    if (m != nullptr && !m->is_static() && m->is_public()) {\n+      _supers->push(m);\n+    }\n@@ -2716,0 +2904,50 @@\n+  return _supers;\n+}\n+\n+\/\/ Iterate over arguments and compute scalarized and non-scalarized signatures\n+void CompiledEntrySignature::compute_calling_conventions(bool init) {\n+  bool has_scalarized = false;\n+  if (_method != nullptr) {\n+    InstanceKlass* holder = _method->method_holder();\n+    int arg_num = 0;\n+    if (!_method->is_static()) {\n+      \/\/ We shouldn't scalarize 'this' in a value class constructor\n+      if (holder->is_inline_klass() && InlineKlass::cast(holder)->can_be_passed_as_fields() && !_method->is_object_constructor() &&\n+          (init || _method->is_scalarized_arg(arg_num))) {\n+        _sig_cc->appendAll(InlineKlass::cast(holder)->extended_sig());\n+        has_scalarized = true;\n+        _has_inline_recv = true;\n+        _num_inline_args++;\n+      } else {\n+        SigEntry::add_entry(_sig_cc, T_OBJECT, holder->name());\n+      }\n+      SigEntry::add_entry(_sig, T_OBJECT, holder->name());\n+      SigEntry::add_entry(_sig_cc_ro, T_OBJECT, holder->name());\n+      arg_num++;\n+    }\n+    for (SignatureStream ss(_method->signature()); !ss.at_return_type(); ss.next()) {\n+      BasicType bt = ss.type();\n+      if (bt == T_OBJECT) {\n+        InlineKlass* vk = ss.as_inline_klass(holder);\n+        if (vk != nullptr && vk->can_be_passed_as_fields() && (init || _method->is_scalarized_arg(arg_num))) {\n+          \/\/ Check for a calling convention mismatch with super method(s)\n+          bool scalar_super = false;\n+          bool non_scalar_super = false;\n+          GrowableArray<Method*>* supers = get_supers();\n+          for (int i = 0; i < supers->length(); ++i) {\n+            Method* super_method = supers->at(i);\n+            if (super_method->is_scalarized_arg(arg_num)) {\n+              scalar_super = true;\n+            } else {\n+              non_scalar_super = true;\n+            }\n+          }\n+#ifdef ASSERT\n+          \/\/ Randomly enable below code paths for stress testing\n+          bool stress = init && StressCallingConvention;\n+          if (stress && (os::random() & 1) == 1) {\n+            non_scalar_super = true;\n+            if ((os::random() & 1) == 1) {\n+              scalar_super = true;\n+            }\n+          }\n@@ -2717,0 +2955,50 @@\n+          if (non_scalar_super) {\n+            \/\/ Found a super method with a non-scalarized argument. Fall back to the non-scalarized calling convention.\n+            if (scalar_super) {\n+              \/\/ Found non-scalar *and* scalar super methods. We can't handle both.\n+              \/\/ Mark the scalar method as mismatch and re-compile call sites to use non-scalarized calling convention.\n+              for (int i = 0; i < supers->length(); ++i) {\n+                Method* super_method = supers->at(i);\n+                if (super_method->is_scalarized_arg(arg_num) DEBUG_ONLY(|| (stress && (os::random() & 1) == 1))) {\n+                  super_method->set_mismatch();\n+                  MutexLocker ml(Compile_lock, Mutex::_safepoint_check_flag);\n+                  JavaThread* thread = JavaThread::current();\n+                  HandleMark hm(thread);\n+                  methodHandle mh(thread, super_method);\n+                  DeoptimizationScope deopt_scope;\n+                  CodeCache::mark_for_deoptimization(&deopt_scope, mh());\n+                  deopt_scope.deoptimize_marked();\n+                }\n+              }\n+            }\n+            \/\/ Fall back to non-scalarized calling convention\n+            SigEntry::add_entry(_sig_cc, T_OBJECT, ss.as_symbol());\n+            SigEntry::add_entry(_sig_cc_ro, T_OBJECT, ss.as_symbol());\n+          } else {\n+            _num_inline_args++;\n+            has_scalarized = true;\n+            int last = _sig_cc->length();\n+            int last_ro = _sig_cc_ro->length();\n+            _sig_cc->appendAll(vk->extended_sig());\n+            _sig_cc_ro->appendAll(vk->extended_sig());\n+            if (bt == T_OBJECT) {\n+              \/\/ Nullable inline type argument, insert InlineTypeNode::NullMarker field right after T_METADATA delimiter\n+              _sig_cc->insert_before(last+1, SigEntry(T_BOOLEAN, -1, nullptr, true));\n+              _sig_cc_ro->insert_before(last_ro+1, SigEntry(T_BOOLEAN, -1, nullptr, true));\n+            }\n+          }\n+        } else {\n+          SigEntry::add_entry(_sig_cc, T_OBJECT, ss.as_symbol());\n+          SigEntry::add_entry(_sig_cc_ro, T_OBJECT, ss.as_symbol());\n+        }\n+        bt = T_OBJECT;\n+      } else {\n+        SigEntry::add_entry(_sig_cc, ss.type(), ss.as_symbol());\n+        SigEntry::add_entry(_sig_cc_ro, ss.type(), ss.as_symbol());\n+      }\n+      SigEntry::add_entry(_sig, bt, ss.as_symbol());\n+      if (bt != T_VOID) {\n+        arg_num++;\n+      }\n+    }\n+  }\n@@ -2718,1 +3006,3 @@\n- private:\n+  \/\/ Compute the non-scalarized calling convention\n+  _regs = NEW_RESOURCE_ARRAY(VMRegPair, _sig->length());\n+  _args_on_stack = SharedRuntime::java_calling_convention(_sig, _regs);\n@@ -2720,5 +3010,16 @@\n-  friend class SignatureIterator;  \/\/ so do_parameters_on can call do_type\n-  void do_type(BasicType type) {\n-    sig_bt[index++] = type;\n-    if (type == T_LONG || type == T_DOUBLE) {\n-      sig_bt[index++] = T_VOID; \/\/ Longs & doubles take 2 Java slots\n+  \/\/ Compute the scalarized calling conventions if there are scalarized inline types in the signature\n+  if (has_scalarized && !_method->is_native()) {\n+    _regs_cc = NEW_RESOURCE_ARRAY(VMRegPair, _sig_cc->length());\n+    _args_on_stack_cc = SharedRuntime::java_calling_convention(_sig_cc, _regs_cc);\n+\n+    _regs_cc_ro = NEW_RESOURCE_ARRAY(VMRegPair, _sig_cc_ro->length());\n+    _args_on_stack_cc_ro = SharedRuntime::java_calling_convention(_sig_cc_ro, _regs_cc_ro);\n+\n+    _c1_needs_stack_repair = (_args_on_stack_cc < _args_on_stack) || (_args_on_stack_cc_ro < _args_on_stack);\n+    _c2_needs_stack_repair = (_args_on_stack_cc > _args_on_stack) || (_args_on_stack_cc > _args_on_stack_cc_ro);\n+\n+    \/\/ Upper bound on stack arguments to avoid hitting the argument limit and\n+    \/\/ bailing out of compilation (\"unsupported incoming calling sequence\").\n+    \/\/ TODO we need a reasonable limit (flag?) here\n+    if (MAX2(_args_on_stack_cc, _args_on_stack_cc_ro) <= 60) {\n+      return; \/\/ Success\n@@ -2727,1 +3028,130 @@\n-};\n+  \/\/ No scalarized args\n+  _sig_cc = _sig;\n+  _regs_cc = _regs;\n+  _args_on_stack_cc = _args_on_stack;\n+\n+  _sig_cc_ro = _sig;\n+  _regs_cc_ro = _regs;\n+  _args_on_stack_cc_ro = _args_on_stack;\n+}\n+\n+void CompiledEntrySignature::initialize_from_fingerprint(AdapterFingerPrint* fingerprint) {\n+  _has_inline_recv = fingerprint->has_ro_adapter();\n+\n+  int value_object_count = 0;\n+  BasicType prev_bt = T_ILLEGAL;\n+  bool has_scalarized_arguments = false;\n+  bool long_prev = false;\n+  int long_prev_offset = -1;\n+\n+  fingerprint->iterate_args([&] (const AdapterFingerPrint::Element& arg) {\n+    BasicType bt = arg.bt();\n+    int offset = arg.offset();\n+\n+    if (long_prev) {\n+      long_prev = false;\n+      BasicType bt_to_add;\n+      if (bt == T_VOID) {\n+        bt_to_add = T_LONG;\n+      } else {\n+        bt_to_add = T_OBJECT;\n+      }\n+      if (value_object_count == 0) {\n+        SigEntry::add_entry(_sig, bt_to_add);\n+      }\n+      SigEntry::add_entry(_sig_cc, bt_to_add, nullptr, long_prev_offset);\n+      SigEntry::add_entry(_sig_cc_ro, bt_to_add, nullptr, long_prev_offset);\n+    }\n+\n+    switch (bt) {\n+      case T_VOID:\n+        if (prev_bt != T_LONG && prev_bt != T_DOUBLE) {\n+          assert(InlineTypePassFieldsAsArgs, \"unexpected end of inline type\");\n+          value_object_count--;\n+          SigEntry::add_entry(_sig_cc, T_VOID, nullptr, offset);\n+          SigEntry::add_entry(_sig_cc_ro, T_VOID, nullptr, offset);\n+          assert(value_object_count >= 0, \"invalid value object count\");\n+        } else {\n+          \/\/ Nothing to add for _sig: We already added an addition T_VOID in add_entry() when adding T_LONG or T_DOUBLE.\n+        }\n+        break;\n+      case T_INT:\n+      case T_FLOAT:\n+      case T_DOUBLE:\n+        if (value_object_count == 0) {\n+          SigEntry::add_entry(_sig, bt);\n+        }\n+        SigEntry::add_entry(_sig_cc, bt, nullptr, offset);\n+        SigEntry::add_entry(_sig_cc_ro, bt, nullptr, offset);\n+        break;\n+      case T_LONG:\n+        long_prev = true;\n+        long_prev_offset = offset;\n+        break;\n+      case T_BOOLEAN:\n+      case T_CHAR:\n+      case T_BYTE:\n+      case T_SHORT:\n+      case T_OBJECT:\n+      case T_ARRAY:\n+        assert(value_object_count > 0, \"must be value object field\");\n+        SigEntry::add_entry(_sig_cc, bt, nullptr, offset);\n+        SigEntry::add_entry(_sig_cc_ro, bt, nullptr, offset);\n+        break;\n+      case T_METADATA:\n+        assert(InlineTypePassFieldsAsArgs, \"unexpected start of inline type\");\n+        if (value_object_count == 0) {\n+          SigEntry::add_entry(_sig, T_OBJECT);\n+        }\n+        SigEntry::add_entry(_sig_cc, T_METADATA, nullptr, offset);\n+        SigEntry::add_entry(_sig_cc_ro, T_METADATA, nullptr, offset);\n+        value_object_count++;\n+        has_scalarized_arguments = true;\n+        break;\n+      default: {\n+        fatal(\"Unexpected BasicType: %s\", basictype_to_str(bt));\n+      }\n+    }\n+    prev_bt = bt;\n+  });\n+\n+  if (long_prev) {\n+    \/\/ If previous bt was T_LONG and we reached the end of the signature, we know that it must be a T_OBJECT.\n+    SigEntry::add_entry(_sig, T_OBJECT);\n+    SigEntry::add_entry(_sig_cc, T_OBJECT);\n+    SigEntry::add_entry(_sig_cc_ro, T_OBJECT);\n+  }\n+  assert(value_object_count == 0, \"invalid value object count\");\n+\n+  _regs = NEW_RESOURCE_ARRAY(VMRegPair, _sig->length());\n+  _args_on_stack = SharedRuntime::java_calling_convention(_sig, _regs);\n+\n+  \/\/ Compute the scalarized calling conventions if there are scalarized inline types in the signature\n+  if (has_scalarized_arguments) {\n+    _regs_cc = NEW_RESOURCE_ARRAY(VMRegPair, _sig_cc->length());\n+    _args_on_stack_cc = SharedRuntime::java_calling_convention(_sig_cc, _regs_cc);\n+\n+    _regs_cc_ro = NEW_RESOURCE_ARRAY(VMRegPair, _sig_cc_ro->length());\n+    _args_on_stack_cc_ro = SharedRuntime::java_calling_convention(_sig_cc_ro, _regs_cc_ro);\n+\n+    _c1_needs_stack_repair = (_args_on_stack_cc < _args_on_stack) || (_args_on_stack_cc_ro < _args_on_stack);\n+    _c2_needs_stack_repair = (_args_on_stack_cc > _args_on_stack) || (_args_on_stack_cc > _args_on_stack_cc_ro);\n+  } else {\n+    \/\/ No scalarized args\n+    _sig_cc = _sig;\n+    _regs_cc = _regs;\n+    _args_on_stack_cc = _args_on_stack;\n+\n+    _sig_cc_ro = _sig;\n+    _regs_cc_ro = _regs;\n+    _args_on_stack_cc_ro = _args_on_stack;\n+  }\n+\n+#ifdef ASSERT\n+  {\n+    AdapterFingerPrint* compare_fp = AdapterFingerPrint::allocate(_sig_cc, _has_inline_recv);\n+    assert(fingerprint->equals(compare_fp), \"%s - %s\", fingerprint->as_string(), compare_fp->as_string());\n+    AdapterFingerPrint::deallocate(compare_fp);\n+  }\n+#endif\n+}\n@@ -2735,1 +3165,1 @@\n-void AdapterHandlerLibrary::verify_adapter_sharing(int total_args_passed, BasicType* sig_bt, AdapterHandlerEntry* cached_entry) {\n+void AdapterHandlerLibrary::verify_adapter_sharing(CompiledEntrySignature& ces, AdapterHandlerEntry* cached_entry) {\n@@ -2738,1 +3168,1 @@\n-  AdapterHandlerEntry* comparison_entry = create_adapter(total_args_passed, sig_bt, true);\n+  AdapterHandlerEntry* comparison_entry = create_adapter(ces, false, true);\n@@ -2763,2 +3193,13 @@\n-  \/\/ Fill in the signature array, for the calling-convention call.\n-  int total_args_passed = method->size_of_parameters(); \/\/ All args on stack\n+  CompiledEntrySignature ces(method());\n+  ces.compute_calling_conventions();\n+  if (ces.has_scalarized_args()) {\n+    if (!method->has_scalarized_args()) {\n+      method->set_has_scalarized_args();\n+    }\n+    if (ces.c1_needs_stack_repair()) {\n+      method->set_c1_needs_stack_repair();\n+    }\n+    if (ces.c2_needs_stack_repair() && !method->c2_needs_stack_repair()) {\n+      method->set_c2_needs_stack_repair();\n+    }\n+  }\n@@ -2766,4 +3207,0 @@\n-  AdapterSignatureIterator si(method->signature(), method->constMethod()->fingerprint(),\n-                              method->is_static(), total_args_passed);\n-  assert(si.slots() == total_args_passed, \"\");\n-  BasicType* sig_bt = si.basic_types();\n@@ -2774,1 +3211,1 @@\n-    entry = lookup(total_args_passed, sig_bt);\n+    entry = lookup(ces.sig_cc(), ces.has_inline_recv());\n@@ -2782,1 +3219,1 @@\n-        verify_adapter_sharing(total_args_passed, sig_bt, entry);\n+        verify_adapter_sharing(ces, entry);\n@@ -2786,1 +3223,1 @@\n-      entry = create_adapter(total_args_passed, sig_bt);\n+      entry = create_adapter(ces, \/* allocate_code_blob *\/ true);\n@@ -2841,0 +3278,2 @@\n+  entry_offset[AdapterBlob::C2I_Inline] = entry_address[AdapterBlob::C2I_Inline] - entry_address[AdapterBlob::I2C];\n+  entry_offset[AdapterBlob::C2I_Inline_RO] = entry_address[AdapterBlob::C2I_Inline_RO] - entry_address[AdapterBlob::I2C];\n@@ -2842,0 +3281,1 @@\n+  entry_offset[AdapterBlob::C2I_Unverified_Inline] = entry_address[AdapterBlob::C2I_Unverified_Inline] - entry_address[AdapterBlob::I2C];\n@@ -2850,2 +3290,2 @@\n-                                                  int total_args_passed,\n-                                                  BasicType* sig_bt,\n+                                                  CompiledEntrySignature& ces,\n+                                                  bool allocate_code_blob,\n@@ -2858,0 +3298,1 @@\n+  AdapterBlob* adapter_blob = nullptr;\n@@ -2864,2 +3305,1 @@\n-  VMRegPair stack_regs[16];\n-  VMRegPair* regs = (total_args_passed <= 16) ? stack_regs : NEW_RESOURCE_ARRAY(VMRegPair, total_args_passed);\n+  address entry_address[AdapterBlob::ENTRY_COUNT];\n@@ -2868,7 +3308,17 @@\n-  int comp_args_on_stack = SharedRuntime::java_calling_convention(sig_bt, regs, total_args_passed);\n-  address entry_address[AdapterBlob::ENTRY_COUNT];\n-                                         total_args_passed,\n-                                         comp_args_on_stack,\n-                                         sig_bt,\n-                                         regs,\n-                                         entry_address);\n+                                         ces.args_on_stack(),\n+                                         ces.sig(),\n+                                         ces.regs(),\n+                                         ces.sig_cc(),\n+                                         ces.regs_cc(),\n+                                         ces.sig_cc_ro(),\n+                                         ces.regs_cc_ro(),\n+                                         entry_address,\n+                                         adapter_blob,\n+                                         allocate_code_blob);\n+\n+  if (ces.has_scalarized_args()) {\n+    \/\/ Save a C heap allocated version of the scalarized signature and store it in the adapter\n+    GrowableArray<SigEntry>* heap_sig = new (mtInternal) GrowableArray<SigEntry>(ces.sig_cc()->length(), mtInternal);\n+    heap_sig->appendAll(ces.sig_cc());\n+    handler->set_sig_cc(heap_sig);\n+  }\n@@ -2888,1 +3338,0 @@\n-  AdapterBlob* adapter_blob = AdapterBlob::create(&buffer, entry_offset);\n@@ -2915,2 +3364,2 @@\n-AdapterHandlerEntry* AdapterHandlerLibrary::create_adapter(int total_args_passed,\n-                                                           BasicType* sig_bt,\n+AdapterHandlerEntry* AdapterHandlerLibrary::create_adapter(CompiledEntrySignature& ces,\n+                                                           bool allocate_code_blob,\n@@ -2918,1 +3367,6 @@\n-  AdapterFingerPrint* fp = AdapterFingerPrint::allocate(total_args_passed, sig_bt);\n+  AdapterFingerPrint* fp = AdapterFingerPrint::allocate(ces.sig_cc(), ces.has_inline_recv());\n+#ifdef ASSERT\n+  \/\/ Verify that we can successfully restore the compiled entry signature object.\n+  CompiledEntrySignature ces_verify;\n+  ces_verify.initialize_from_fingerprint(fp);\n+#endif\n@@ -2920,1 +3374,1 @@\n-  if (!generate_adapter_code(handler, total_args_passed, sig_bt, is_transient)) {\n+  if (!generate_adapter_code(handler, ces, allocate_code_blob, is_transient)) {\n@@ -3023,3 +3477,3 @@\n-    int nargs;\n-    BasicType* bt = _fingerprint->as_basic_type(nargs);\n-    if (!AdapterHandlerLibrary::generate_adapter_code(this, nargs, bt, \/* is_transient *\/ false)) {\n+    CompiledEntrySignature ces;\n+    ces.initialize_from_fingerprint(_fingerprint);\n+    if (!AdapterHandlerLibrary::generate_adapter_code(this, ces, true, false)) {\n@@ -3063,13 +3517,26 @@\n-  _no_arg_handler = lookup(0, nullptr);\n-\n-  BasicType obj_args[] = { T_OBJECT };\n-  _obj_arg_handler = lookup(1, obj_args);\n-\n-  BasicType int_args[] = { T_INT };\n-  _int_arg_handler = lookup(1, int_args);\n-\n-  BasicType obj_int_args[] = { T_OBJECT, T_INT };\n-  _obj_int_arg_handler = lookup(2, obj_int_args);\n-\n-  BasicType obj_obj_args[] = { T_OBJECT, T_OBJECT };\n-  _obj_obj_arg_handler = lookup(2, obj_obj_args);\n+  ResourceMark rm;\n+  CompiledEntrySignature no_args;\n+  no_args.compute_calling_conventions();\n+  _no_arg_handler = lookup(no_args.sig_cc(), no_args.has_inline_recv());\n+\n+  CompiledEntrySignature obj_args;\n+  SigEntry::add_entry(obj_args.sig(), T_OBJECT);\n+  obj_args.compute_calling_conventions();\n+  _obj_arg_handler = lookup(obj_args.sig_cc(), obj_args.has_inline_recv());\n+\n+  CompiledEntrySignature int_args;\n+  SigEntry::add_entry(int_args.sig(), T_INT);\n+  int_args.compute_calling_conventions();\n+  _int_arg_handler = lookup(int_args.sig_cc(), int_args.has_inline_recv());\n+\n+  CompiledEntrySignature obj_int_args;\n+  SigEntry::add_entry(obj_int_args.sig(), T_OBJECT);\n+  SigEntry::add_entry(obj_int_args.sig(), T_INT);\n+  obj_int_args.compute_calling_conventions();\n+  _obj_int_arg_handler = lookup(obj_int_args.sig_cc(), obj_int_args.has_inline_recv());\n+\n+  CompiledEntrySignature obj_obj_args;\n+  SigEntry::add_entry(obj_obj_args.sig(), T_OBJECT);\n+  SigEntry::add_entry(obj_obj_args.sig(), T_OBJECT);\n+  obj_obj_args.compute_calling_conventions();\n+  _obj_obj_arg_handler = lookup(obj_obj_args.sig_cc(), obj_obj_args.has_inline_recv());\n@@ -3104,0 +3571,3 @@\n+  if (_sig_cc != nullptr) {\n+    delete _sig_cc;\n+  }\n@@ -3191,0 +3661,1 @@\n+      BasicType stack_sig_bt[16];\n@@ -3192,0 +3663,1 @@\n+      BasicType* sig_bt = (total_args_passed <= 16) ? stack_sig_bt : NEW_RESOURCE_ARRAY(BasicType, total_args_passed);\n@@ -3194,5 +3666,13 @@\n-      AdapterSignatureIterator si(method->signature(), method->constMethod()->fingerprint(),\n-                              method->is_static(), total_args_passed);\n-      BasicType* sig_bt = si.basic_types();\n-      assert(si.slots() == total_args_passed, \"\");\n-      BasicType ret_type = si.return_type();\n+      int i = 0;\n+      if (!method->is_static()) {  \/\/ Pass in receiver first\n+        sig_bt[i++] = T_OBJECT;\n+      }\n+      SignatureStream ss(method->signature());\n+      for (; !ss.at_return_type(); ss.next()) {\n+        sig_bt[i++] = ss.type();  \/\/ Collect remaining bits of signature\n+        if (ss.type() == T_LONG || ss.type() == T_DOUBLE) {\n+          sig_bt[i++] = T_VOID;   \/\/ Longs & doubles take 2 Java slots\n+        }\n+      }\n+      assert(i == total_args_passed, \"\");\n+      BasicType ret_type = ss.type();\n@@ -3446,1 +3926,4 @@\n-    st->print(\" c2iUV: \" INTPTR_FORMAT, p2i(get_c2i_unverified_entry()));\n+    st->print(\" c2iVE: \" INTPTR_FORMAT, p2i(get_c2i_inline_entry()));\n+    st->print(\" c2iVROE: \" INTPTR_FORMAT, p2i(get_c2i_inline_ro_entry()));\n+    st->print(\" c2iUE: \" INTPTR_FORMAT, p2i(get_c2i_unverified_entry()));\n+    st->print(\" c2iUVE: \" INTPTR_FORMAT, p2i(get_c2i_unverified_inline_entry()));\n@@ -3533,0 +4016,195 @@\n+\n+\/\/ We are at a compiled code to interpreter call. We need backing\n+\/\/ buffers for all inline type arguments. Allocate an object array to\n+\/\/ hold them (convenient because once we're done with it we don't have\n+\/\/ to worry about freeing it).\n+oop SharedRuntime::allocate_inline_types_impl(JavaThread* current, methodHandle callee, bool allocate_receiver, TRAPS) {\n+  assert(InlineTypePassFieldsAsArgs, \"no reason to call this\");\n+  ResourceMark rm;\n+\n+  int nb_slots = 0;\n+  InstanceKlass* holder = callee->method_holder();\n+  allocate_receiver &= !callee->is_static() && holder->is_inline_klass() && callee->is_scalarized_arg(0);\n+  if (allocate_receiver) {\n+    nb_slots++;\n+  }\n+  int arg_num = callee->is_static() ? 0 : 1;\n+  for (SignatureStream ss(callee->signature()); !ss.at_return_type(); ss.next()) {\n+    BasicType bt = ss.type();\n+    if (bt == T_OBJECT && callee->is_scalarized_arg(arg_num)) {\n+      nb_slots++;\n+    }\n+    if (bt != T_VOID) {\n+      arg_num++;\n+    }\n+  }\n+  objArrayOop array_oop = oopFactory::new_objectArray(nb_slots, CHECK_NULL);\n+  objArrayHandle array(THREAD, array_oop);\n+  arg_num = callee->is_static() ? 0 : 1;\n+  int i = 0;\n+  if (allocate_receiver) {\n+    InlineKlass* vk = InlineKlass::cast(holder);\n+    oop res = vk->allocate_instance(CHECK_NULL);\n+    array->obj_at_put(i++, res);\n+  }\n+  for (SignatureStream ss(callee->signature()); !ss.at_return_type(); ss.next()) {\n+    BasicType bt = ss.type();\n+    if (bt == T_OBJECT && callee->is_scalarized_arg(arg_num)) {\n+      InlineKlass* vk = ss.as_inline_klass(holder);\n+      assert(vk != nullptr, \"Unexpected klass\");\n+      oop res = vk->allocate_instance(CHECK_NULL);\n+      array->obj_at_put(i++, res);\n+    }\n+    if (bt != T_VOID) {\n+      arg_num++;\n+    }\n+  }\n+  return array();\n+}\n+\n+JRT_ENTRY(void, SharedRuntime::allocate_inline_types(JavaThread* current, Method* callee_method, bool allocate_receiver))\n+  methodHandle callee(current, callee_method);\n+  oop array = SharedRuntime::allocate_inline_types_impl(current, callee, allocate_receiver, CHECK);\n+  current->set_vm_result_oop(array);\n+  current->set_vm_result_metadata(callee()); \/\/ TODO: required to keep callee live?\n+JRT_END\n+\n+\/\/ We're returning from an interpreted method: load each field into a\n+\/\/ register following the calling convention\n+JRT_LEAF(void, SharedRuntime::load_inline_type_fields_in_regs(JavaThread* current, oopDesc* res))\n+{\n+  assert(res->klass()->is_inline_klass(), \"only inline types here\");\n+  ResourceMark rm;\n+  RegisterMap reg_map(current,\n+                      RegisterMap::UpdateMap::include,\n+                      RegisterMap::ProcessFrames::include,\n+                      RegisterMap::WalkContinuation::skip);\n+  frame stubFrame = current->last_frame();\n+  frame callerFrame = stubFrame.sender(&reg_map);\n+  assert(callerFrame.is_interpreted_frame(), \"should be coming from interpreter\");\n+\n+  InlineKlass* vk = InlineKlass::cast(res->klass());\n+\n+  const Array<SigEntry>* sig_vk = vk->extended_sig();\n+  const Array<VMRegPair>* regs = vk->return_regs();\n+\n+  if (regs == nullptr) {\n+    \/\/ The fields of the inline klass don't fit in registers, bail out\n+    return;\n+  }\n+\n+  int j = 1;\n+  for (int i = 0; i < sig_vk->length(); i++) {\n+    BasicType bt = sig_vk->at(i)._bt;\n+    if (bt == T_METADATA) {\n+      continue;\n+    }\n+    if (bt == T_VOID) {\n+      if (sig_vk->at(i-1)._bt == T_LONG ||\n+          sig_vk->at(i-1)._bt == T_DOUBLE) {\n+        j++;\n+      }\n+      continue;\n+    }\n+    int off = sig_vk->at(i)._offset;\n+    assert(off > 0, \"offset in object should be positive\");\n+    VMRegPair pair = regs->at(j);\n+    address loc = reg_map.location(pair.first(), nullptr);\n+    switch(bt) {\n+    case T_BOOLEAN:\n+      *(jboolean*)loc = res->bool_field(off);\n+      break;\n+    case T_CHAR:\n+      *(jchar*)loc = res->char_field(off);\n+      break;\n+    case T_BYTE:\n+      *(jbyte*)loc = res->byte_field(off);\n+      break;\n+    case T_SHORT:\n+      *(jshort*)loc = res->short_field(off);\n+      break;\n+    case T_INT: {\n+      *(jint*)loc = res->int_field(off);\n+      break;\n+    }\n+    case T_LONG:\n+#ifdef _LP64\n+      *(intptr_t*)loc = res->long_field(off);\n+#else\n+      Unimplemented();\n+#endif\n+      break;\n+    case T_OBJECT:\n+    case T_ARRAY: {\n+      *(oop*)loc = res->obj_field(off);\n+      break;\n+    }\n+    case T_FLOAT:\n+      *(jfloat*)loc = res->float_field(off);\n+      break;\n+    case T_DOUBLE:\n+      *(jdouble*)loc = res->double_field(off);\n+      break;\n+    default:\n+      ShouldNotReachHere();\n+    }\n+    j++;\n+  }\n+  assert(j == regs->length(), \"missed a field?\");\n+\n+#ifdef ASSERT\n+  VMRegPair pair = regs->at(0);\n+  address loc = reg_map.location(pair.first(), nullptr);\n+  assert(*(oopDesc**)loc == res, \"overwritten object\");\n+#endif\n+\n+  current->set_vm_result_oop(res);\n+}\n+JRT_END\n+\n+\/\/ We've returned to an interpreted method, the interpreter needs a\n+\/\/ reference to an inline type instance. Allocate it and initialize it\n+\/\/ from field's values in registers.\n+JRT_BLOCK_ENTRY(void, SharedRuntime::store_inline_type_fields_to_buf(JavaThread* current, intptr_t res))\n+{\n+  ResourceMark rm;\n+  RegisterMap reg_map(current,\n+                      RegisterMap::UpdateMap::include,\n+                      RegisterMap::ProcessFrames::include,\n+                      RegisterMap::WalkContinuation::skip);\n+  frame stubFrame = current->last_frame();\n+  frame callerFrame = stubFrame.sender(&reg_map);\n+\n+#ifdef ASSERT\n+  InlineKlass* verif_vk = InlineKlass::returned_inline_klass(reg_map);\n+#endif\n+\n+  if (!is_set_nth_bit(res, 0)) {\n+    \/\/ We're not returning with inline type fields in registers (the\n+    \/\/ calling convention didn't allow it for this inline klass)\n+    assert(!Metaspace::contains((void*)res), \"should be oop or pointer in buffer area\");\n+    current->set_vm_result_oop((oopDesc*)res);\n+    assert(verif_vk == nullptr, \"broken calling convention\");\n+    return;\n+  }\n+\n+  clear_nth_bit(res, 0);\n+  InlineKlass* vk = (InlineKlass*)res;\n+  assert(verif_vk == vk, \"broken calling convention\");\n+  assert(Metaspace::contains((void*)res), \"should be klass\");\n+\n+  \/\/ Allocate handles for every oop field so they are safe in case of\n+  \/\/ a safepoint when allocating\n+  GrowableArray<Handle> handles;\n+  vk->save_oop_fields(reg_map, handles);\n+\n+  \/\/ It's unsafe to safepoint until we are here\n+  JRT_BLOCK;\n+  {\n+    JavaThread* THREAD = current;\n+    oop vt = vk->realloc_result(reg_map, handles, CHECK);\n+    current->set_vm_result_oop(vt);\n+  }\n+  JRT_BLOCK_END;\n+}\n+JRT_END\n","filename":"src\/hotspot\/share\/runtime\/sharedRuntime.cpp","additions":935,"deletions":257,"binary":false,"changes":1192,"status":"modified"},{"patch":"@@ -62,0 +62,2 @@\n+#include \"oops\/flatArrayKlass.hpp\"\n+#include \"oops\/inlineKlass.hpp\"\n@@ -945,1 +947,3 @@\n-           declare_type(ObjArrayKlass, ArrayKlass)                        \\\n+           declare_type(ObjArrayKlass, ArrayKlass)                        \\\n+             declare_type(FlatArrayKlass, ArrayKlass)                     \\\n+             declare_type(RefArrayKlass, ArrayKlass)                      \\\n@@ -948,0 +952,1 @@\n+        declare_type(InlineKlass, InstanceKlass)                          \\\n@@ -1421,1 +1426,1 @@\n-  declare_constant(Klass::_lh_array_tag_obj_value)                        \\\n+  declare_constant(Klass::_lh_array_tag_ref_value)                        \\\n","filename":"src\/hotspot\/share\/runtime\/vmStructs.cpp","additions":7,"deletions":2,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -123,0 +123,1 @@\n+  DCmdFactory::register_DCmdFactory(new DCmdFactoryImpl<PrintClassLayoutDCmd>(full_export, true, false));\n@@ -954,1 +955,25 @@\n-#endif\n+\n+PrintClassLayoutDCmd::PrintClassLayoutDCmd(outputStream* output, bool heap) :\n+                                       DCmdWithParser(output, heap),\n+  _classname(\"classname\", \"Name of class whose layout should be printed. \",\n+             \"STRING\", true) {\n+  _dcmdparser.add_dcmd_argument(&_classname);\n+}\n+\n+void PrintClassLayoutDCmd::execute(DCmdSource source, TRAPS) {\n+  VM_PrintClassLayout printClassLayoutOp(output(), _classname.value());\n+  VMThread::execute(&printClassLayoutOp);\n+}\n+\n+int PrintClassLayoutDCmd::num_arguments() {\n+  ResourceMark rm;\n+  PrintClassLayoutDCmd* dcmd = new PrintClassLayoutDCmd(nullptr, false);\n+  if (dcmd != nullptr) {\n+    DCmdMark mark(dcmd);\n+    return dcmd->_dcmdparser.num_arguments();\n+  } else {\n+    return 0;\n+  }\n+}\n+\n+#endif \/\/ INCLUDE_SERVICES\n","filename":"src\/hotspot\/share\/services\/diagnosticCommand.cpp","additions":26,"deletions":1,"binary":false,"changes":27,"status":"modified"},{"patch":"@@ -328,0 +328,20 @@\n+class PrintClassLayoutDCmd : public DCmdWithParser {\n+protected:\n+  DCmdArgument<char*> _classname; \/\/ lass name whose layout should be printed.\n+public:\n+  PrintClassLayoutDCmd(outputStream* output, bool heap);\n+  static const char* name() {\n+    return \"VM.class_print_layout\";\n+  }\n+  static const char* description() {\n+    return \"Print the layout of an instance of a class, including flat fields. \"\n+           \"The name of each class is followed by the ClassLoaderData* of its ClassLoader, \"\n+           \"or \\\"null\\\" if loaded by the bootstrap class loader.\";\n+  }\n+  static const char* impact() {\n+      return \"Medium: Depends on number of loaded classes.\";\n+  }\n+  static int num_arguments();\n+  virtual void execute(DCmdSource source, TRAPS);\n+};\n+\n","filename":"src\/hotspot\/share\/services\/diagnosticCommand.hpp","additions":20,"deletions":0,"binary":false,"changes":20,"status":"modified"},{"patch":"@@ -29,0 +29,2 @@\n+import jdk.internal.misc.PreviewFeatures;\n+import jdk.internal.value.DeserializeConstructor;\n@@ -171,4 +173,12 @@\n- * class; programmers should treat instances that are\n- * {@linkplain #equals(Object) equal} as interchangeable and should not\n- * use instances for synchronization, or unpredictable behavior may\n- * occur. For example, in a future release, synchronization may fail.\n+ * class; programmers should treat instances that are {@linkplain #equals(Object) equal}\n+ * as interchangeable and should not use instances for synchronization, mutexes, or\n+ * with {@linkplain java.lang.ref.Reference object references}.\n+ *\n+ * <div class=\"preview-block\">\n+ *      <div class=\"preview-comment\">\n+ *          When preview features are enabled, {@code Character} is a {@linkplain Class#isValue value class}.\n+ *          Use of value class instances for synchronization, mutexes, or with\n+ *          {@linkplain java.lang.ref.Reference object references} result in\n+ *          {@link IdentityException}.\n+ *      <\/div>\n+ * <\/div>\n@@ -184,0 +194,1 @@\n+@jdk.internal.MigratedValueClass\n@@ -185,2 +196,1 @@\n-public final\n-class Character implements java.io.Serializable, Comparable<Character>, Constable {\n+public final class Character implements java.io.Serializable, Comparable<Character>, Constable {\n@@ -9409,9 +9419,20 @@\n-     * If a new {@code Character} instance is not required, this method\n-     * should generally be used in preference to the constructor\n-     * {@link #Character(char)}, as this method is likely to yield\n-     * significantly better space and time performance by caching\n-     * frequently requested values.\n-     *\n-     * This method will always cache values in the range {@code\n-     * '\\u005Cu0000'} to {@code '\\u005Cu007F'}, inclusive, and may\n-     * cache other values outside of this range.\n+     * <div class=\"preview-block\">\n+     *      <div class=\"preview-comment\">\n+     *          <p>\n+     *              - When preview features are NOT enabled, {@code Character} is an identity class.\n+     *              If a new {@code Character} instance is not required, this method\n+     *              should generally be used in preference to the constructor\n+     *              {@link #Character(char)}, as this method is likely to yield\n+     *              significantly better space and time performance by caching\n+     *              frequently requested values.\n+     *              This method will always cache values in the range {@code\n+     *              '\\u005Cu0000'} to {@code '\\u005Cu007F'}, inclusive, and may\n+     *              cache other values outside of this range.\n+     *          <\/p>\n+     *          <p>\n+     *             - When preview features are enabled, {@code Character} is a {@linkplain Class#isValue value class}.\n+     *              The {@code valueOf} behavior is the same as invoking the constructor,\n+     *              whether cached or not.\n+     *          <\/p>\n+     *      <\/div>\n+     * <\/div>\n@@ -9424,0 +9445,1 @@\n+    @DeserializeConstructor\n","filename":"src\/java.base\/share\/classes\/java\/lang\/Character.java","additions":37,"deletions":15,"binary":false,"changes":52,"status":"modified"},{"patch":"@@ -41,0 +41,1 @@\n+import java.lang.reflect.ClassFileFormatVersion;\n@@ -63,0 +64,1 @@\n+import java.util.HashSet;\n@@ -73,0 +75,1 @@\n+import jdk.internal.javac.PreviewFeature;\n@@ -75,0 +78,1 @@\n+import jdk.internal.misc.PreviewFeatures;\n@@ -227,3 +231,3 @@\n-    private static final int ANNOTATION= 0x00002000;\n-    private static final int ENUM      = 0x00004000;\n-    private static final int SYNTHETIC = 0x00001000;\n+    private static final int ANNOTATION = 0x00002000;\n+    private static final int ENUM       = 0x00004000;\n+    private static final int SYNTHETIC  = 0x00001000;\n@@ -248,1 +252,1 @@\n-    private Class(ClassLoader loader, Class<?> arrayComponentType, char mods, ProtectionDomain pd, boolean isPrim, char flags) {\n+    private Class(ClassLoader loader, Class<?> arrayComponentType, char mods, ProtectionDomain pd, boolean isPrim, boolean isIdentity, char flags) {\n@@ -257,0 +261,1 @@\n+        identity = isIdentity;\n@@ -328,0 +333,2 @@\n+                \/\/ Modifier.toString() below mis-interprets SYNCHRONIZED, STRICT, and VOLATILE bits\n+                modifiers &= ~(Modifier.SYNCHRONIZED | Modifier.STRICT | Modifier.VOLATILE);\n@@ -346,4 +353,9 @@\n-                    else if (isRecord())\n-                        sb.append(\"record\");\n-                    else\n-                        sb.append(\"class\");\n+                    else {\n+                        if (isValue()) {\n+                            sb.append(\"value \");\n+                        }\n+                        if (isRecord())\n+                            sb.append(\"record\");\n+                        else\n+                            sb.append(\"class\");\n+                    }\n@@ -618,0 +630,48 @@\n+    \/**\n+     * {@return {@code true} if this {@code Class} object represents an identity class,\n+     * otherwise {@code false}}\n+     *\n+     * <ul>\n+     *      <li>\n+     *          If this {@code Class} object represents an array type this method returns {@code true}.\n+     *      <li>\n+     *          If this {@code Class} object represents an interface, a primitive type,\n+     *          or {@code void} this method returns {@code false}.\n+     *      <li>\n+     *          For all other {@code Class} objects, this method returns {@code true} if either\n+     *          preview features are disabled or {@linkplain Modifier#IDENTITY} is set in the\n+     *          {@linkplain #getModifiers() class modifiers}.\n+     * <\/ul>\n+     * @see AccessFlag#IDENTITY\n+     * @since Valhalla\n+     *\/\n+    @PreviewFeature(feature = PreviewFeature.Feature.VALUE_OBJECTS, reflective=true)\n+    public boolean isIdentity() {\n+        return identity;\n+    }\n+\n+    \/**\n+     * {@return {@code true} if this {@code Class} object represents a value class,\n+     * otherwise {@code false}}\n+     * <ul>\n+     *      <li>\n+     *          If this {@code Class} object represents an array type this method returns {@code false}.\n+     *      <li>\n+     *          If this {@code Class} object represents an interface, a primitive type,\n+     *          or {@code void} this method returns {@code true} only if preview features are enabled.\n+     *      <li>\n+     *          For all other {@code Class} objects, this method returns {@code true} only if\n+     *          preview features are enabled and {@linkplain Modifier#IDENTITY} is not set in the\n+     *          {@linkplain #getModifiers() class modifiers}.\n+     * <\/ul>\n+     * @see AccessFlag#IDENTITY\n+     * @since Valhalla\n+     *\/\n+    @PreviewFeature(feature = PreviewFeature.Feature.VALUE_OBJECTS, reflective=true)\n+    public boolean isValue() {\n+        if (!PreviewFeatures.isEnabled()) {\n+            return false;\n+        }\n+        return !isIdentity();\n+    }\n+\n@@ -1021,1 +1081,2 @@\n-    private final transient boolean primitive;  \/\/ Set by the VM if the Class is a primitive type.\n+    private final transient boolean primitive;  \/\/ Set by the VM if the Class is a primitive type\n+    private final transient boolean identity;   \/\/ Set by the VM if the Class is an identity class\n@@ -1341,0 +1402,1 @@\n+     * <li> its {@code identity} modifier is always true\n@@ -1365,1 +1427,1 @@\n-    \/**\n+   \/**\n@@ -1368,0 +1430,1 @@\n+     * The {@code AccessFlags} may depend on the class file format version of the class.\n@@ -1376,0 +1439,1 @@\n+    * <li> its {@code identity} modifier is always true\n@@ -1393,0 +1457,1 @@\n+        \/\/ Arrays need to use PRIVATE\/PROTECTED from its component modifiers.\n@@ -1397,2 +1462,10 @@\n-        return getReflectionFactory().parseAccessFlags((location == AccessFlag.Location.CLASS) ?\n-                        getClassFileAccessFlags() : getModifiers(), location, this);\n+        int accessFlags = location == AccessFlag.Location.CLASS ? getClassFileAccessFlags() : getModifiers();\n+        var reflectionFactory = getReflectionFactory();\n+        var ans = reflectionFactory.parseAccessFlags(accessFlags, location, this);\n+        if (PreviewFeatures.isEnabled() && reflectionFactory.classFileFormatVersion(this) != ClassFileFormatVersion.CURRENT_PREVIEW_FEATURES\n+                && isIdentity()) {\n+            var set = new HashSet<>(ans);\n+            set.add(AccessFlag.IDENTITY);\n+            return Set.copyOf(set);\n+        }\n+        return ans;\n@@ -1401,1 +1474,1 @@\n-    \/**\n+   \/**\n@@ -1409,0 +1482,1 @@\n+\n@@ -3782,1 +3856,1 @@\n-         return TypeAnnotationParser.buildAnnotatedInterfaces(getRawTypeAnnotations(), getConstantPool(), this);\n+        return TypeAnnotationParser.buildAnnotatedInterfaces(getRawTypeAnnotations(), getConstantPool(), this);\n@@ -4123,0 +4197,1 @@\n+    \/* package-private *\/\n","filename":"src\/java.base\/share\/classes\/java\/lang\/Class.java","additions":89,"deletions":14,"binary":false,"changes":103,"status":"modified"},{"patch":"@@ -516,0 +516,1 @@\n+ *     | LoadableDescriptorsAttribute?(List<Utf8Entry> loadableDescriptors)\n","filename":"src\/java.base\/share\/classes\/java\/lang\/classfile\/package-info.java","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -28,0 +28,2 @@\n+import java.lang.classfile.ClassFile;\n+\n@@ -386,1 +388,2 @@\n-    ; \/\/ Reduce code churn when appending new constants\n+\n+    \/\/ Reduce code churn when appending new constants\n@@ -391,0 +394,4 @@\n+    \/\/\/ The preview features of Valhalla.\n+    \/\/\/ @since 26\n+    CURRENT_PREVIEW_FEATURES(ClassFile.latestMajorVersion());\n+\n","filename":"src\/java.base\/share\/classes\/java\/lang\/reflect\/ClassFileFormatVersion.java","additions":8,"deletions":1,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -448,1 +448,1 @@\n-                if ((accessFlags & ~Modifier.PUBLIC) != 0) {\n+                if ((accessFlags & ~(Modifier.PUBLIC | Modifier.IDENTITY)) != 0) {\n@@ -470,1 +470,1 @@\n-                                                                      context.accessFlags() | Modifier.FINAL);\n+                                                                      context.accessFlags() | Modifier.FINAL | Modifier.IDENTITY);\n","filename":"src\/java.base\/share\/classes\/java\/lang\/reflect\/Proxy.java","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -120,5 +120,12 @@\n- * class; programmers should treat instances that are\n- * {@linkplain #equals(Object) equal} as interchangeable and should not\n- * use instances for synchronization, or unpredictable behavior may\n- * occur. For example, in a future release, synchronization may fail.\n- * The {@code equals} method should be used for comparisons.\n+ * class; programmers should treat instances that are {@linkplain #equals(Object) equal}\n+ * as interchangeable and should not use instances for synchronization, mutexes, or\n+ * with {@linkplain java.lang.ref.Reference object references}.\n+ *\n+ * <div class=\"preview-block\">\n+ *      <div class=\"preview-comment\">\n+ *          When preview features are enabled, {@code MonthDay} is a {@linkplain Class#isValue value class}.\n+ *          Use of value class instances for synchronization, mutexes, or with\n+ *          {@linkplain java.lang.ref.Reference object references} result in\n+ *          {@link IdentityException}.\n+ *      <\/div>\n+ * <\/div>\n@@ -132,0 +139,1 @@\n+@jdk.internal.MigratedValueClass\n","filename":"src\/java.base\/share\/classes\/java\/time\/MonthDay.java","additions":13,"deletions":5,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -124,5 +124,12 @@\n- * class; programmers should treat instances that are\n- * {@linkplain #equals(Object) equal} as interchangeable and should not\n- * use instances for synchronization, or unpredictable behavior may\n- * occur. For example, in a future release, synchronization may fail.\n- * The {@code equals} method should be used for comparisons.\n+ * class; programmers should treat instances that are {@linkplain #equals(Object) equal}\n+ * as interchangeable and should not use instances for synchronization, mutexes, or\n+ * with {@linkplain java.lang.ref.Reference object references}.\n+ *\n+ * <div class=\"preview-block\">\n+ *      <div class=\"preview-comment\">\n+ *          When preview features are enabled, {@code YearMonth} is a {@linkplain Class#isValue value class}.\n+ *          Use of value class instances for synchronization, mutexes, or with\n+ *          {@linkplain java.lang.ref.Reference object references} result in\n+ *          {@link IdentityException}.\n+ *      <\/div>\n+ * <\/div>\n@@ -136,0 +143,1 @@\n+@jdk.internal.MigratedValueClass\n","filename":"src\/java.base\/share\/classes\/java\/time\/YearMonth.java","additions":13,"deletions":5,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -145,0 +145,1 @@\n+@jdk.internal.MigratedValueClass\n","filename":"src\/java.base\/share\/classes\/java\/time\/chrono\/ChronoLocalDateImpl.java","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -51,0 +51,1 @@\n+import java.util.Objects;\n@@ -671,2 +672,2 @@\n-                    (k == key || k.equals(key)) &&\n-                    (v == (u = val) || v.equals(u)));\n+                    (Objects.equals(k, key)) &&\n+                    v.equals(val));\n@@ -684,1 +685,1 @@\n-                        ((ek = e.key) == k || (ek != null && k.equals(ek))))\n+                        (ek = e.key) != null && Objects.equals(k, ek))\n@@ -955,1 +956,1 @@\n-                if ((ek = e.key) == key || (ek != null && key.equals(ek)))\n+                if ((ek = e.key) != null && Objects.equals(key, ek))\n@@ -962,1 +963,1 @@\n-                    ((ek = e.key) == key || (ek != null && key.equals(ek))))\n+                    ((ek = e.key) != null && Objects.equals(key, ek)))\n@@ -1000,1 +1001,1 @@\n-                if ((v = p.val) == value || (v != null && value.equals(v)))\n+                if ((v = p.val) != null && Objects.equals(value, v))\n@@ -1041,1 +1042,1 @@\n-                     && ((fk = f.key) == key || (fk != null && key.equals(fk)))\n+                     && (fk = f.key) != null && Objects.equals(key, fk)\n@@ -1053,2 +1054,1 @@\n-                                    ((ek = e.key) == key ||\n-                                     (ek != null && key.equals(ek)))) {\n+                                    (ek = e.key) != null && Objects.equals(key, ek)) {\n@@ -1146,2 +1146,1 @@\n-                                    ((ek = e.key) == key ||\n-                                     (ek != null && key.equals(ek)))) {\n+                                    ((ek = e.key) != null && Objects.equals(key, ek))) {\n@@ -1149,2 +1148,2 @@\n-                                    if (cv == null || cv == ev ||\n-                                        (ev != null && cv.equals(ev))) {\n+                                    if (cv == null ||\n+                                        (ev != null && Objects.equals(cv, ev))) {\n@@ -1173,2 +1172,2 @@\n-                                if (cv == null || cv == pv ||\n-                                    (pv != null && cv.equals(pv))) {\n+                                if (cv == null ||\n+                                    (pv != null && Objects.equals(cv, pv))) {\n@@ -1380,1 +1379,1 @@\n-                    if (v == null || (v != val && !v.equals(val)))\n+                    if (!Objects.equals(val, v))\n@@ -1394,1 +1393,1 @@\n-                    (mv != v && !mv.equals(v)))\n+                    !Objects.equals(mv, v))\n@@ -1518,2 +1517,1 @@\n-                                ((qk = q.key) == k ||\n-                                 (qk != null && k.equals(qk)))) {\n+                                ((qk = q.key) != null && Objects.equals(k, qk))) {\n@@ -1748,1 +1746,1 @@\n-                     && ((fk = f.key) == key || (fk != null && key.equals(fk)))\n+                     && ((fk = f.key) != null && Objects.equals(key, fk))\n@@ -1853,2 +1851,1 @@\n-                                    ((ek = e.key) == key ||\n-                                     (ek != null && key.equals(ek)))) {\n+                                    ((ek = e.key) != null && Objects.equals(key, ek))) {\n@@ -1965,2 +1962,1 @@\n-                                    ((ek = e.key) == key ||\n-                                     (ek != null && key.equals(ek)))) {\n+                                    ((ek = e.key) != null && Objects.equals(key, ek))) {\n@@ -2081,2 +2077,1 @@\n-                                    ((ek = e.key) == key ||\n-                                     (ek != null && key.equals(ek)))) {\n+                                    ((ek = e.key) != null && Objects.equals(key, ek))) {\n@@ -2275,1 +2270,1 @@\n-                        ((ek = e.key) == k || (ek != null && k.equals(ek))))\n+                        ((ek = e.key) != null && Objects.equals(k, ek)))\n@@ -2770,1 +2765,1 @@\n-                    else if ((pk = p.key) == k || (pk != null && k.equals(pk)))\n+                    else if ((pk = p.key) != null && Objects.equals(k, pk))\n@@ -2921,1 +2916,1 @@\n-                            ((ek = e.key) == k || (ek != null && k.equals(ek))))\n+                            ((ek = e.key) != null && Objects.equals(k, ek)))\n@@ -2961,1 +2956,1 @@\n-                else if ((pk = p.key) == k || (pk != null && k.equals(pk)))\n+                else if ((pk = p.key) != null && Objects.equals(k, pk))\n@@ -3560,2 +3555,2 @@\n-                    (k == key || k.equals(key)) &&\n-                    (v == val || v.equals(val)));\n+                    Objects.equals(k, key) &&\n+                    Objects.equals(v, val));\n","filename":"src\/java.base\/share\/classes\/java\/util\/concurrent\/ConcurrentHashMap.java","additions":27,"deletions":32,"binary":false,"changes":59,"status":"modified"},{"patch":"@@ -29,0 +29,1 @@\n+import java.lang.classfile.ClassModel;\n@@ -35,0 +36,1 @@\n+import java.util.HashSet;\n@@ -40,0 +42,1 @@\n+import static java.lang.classfile.ClassFile.*;\n@@ -50,0 +53,3 @@\n+    private WritableField.UnsetField[] strictInstanceFields; \/\/ do not modify array contents\n+    private ClassModel lastStrictCheckClass; \/\/ buf writer has short life, so do not need weak here\n+    private boolean lastStrictCheckResult;\n@@ -72,0 +78,27 @@\n+    public boolean strictFieldsMatch(ClassModel cm) {\n+        \/\/ We have a cache because this check will be called multiple times\n+        \/\/ if a MethodModel is sent wholesale\n+        if (lastStrictCheckClass == cm) {\n+            return lastStrictCheckResult;\n+        }\n+\n+        var result = doStrictFieldsMatchCheck(cm);\n+        lastStrictCheckClass = cm;\n+        lastStrictCheckResult = result;\n+        return result;\n+    }\n+\n+    private boolean doStrictFieldsMatchCheck(ClassModel cm) {\n+        \/\/ TODO only check for preview class files?\n+        \/\/ UTF8 Entry can be used as equality objects\n+        var checks = new HashSet<>(Arrays.asList(getStrictInstanceFields()));\n+        for (var f : cm.fields()) {\n+            if ((f.flags().flagsMask() & (ACC_STATIC | ACC_STRICT_INIT)) == ACC_STRICT_INIT) {\n+                if (!checks.remove(new WritableField.UnsetField(f.fieldName(), f.fieldType()))) {\n+                    return false; \/\/ Field mismatch!\n+                }\n+            }\n+        }\n+        return checks.isEmpty();\n+    }\n+\n@@ -86,0 +119,10 @@\n+    public WritableField.UnsetField[] getStrictInstanceFields() {\n+        assert strictInstanceFields != null : \"should access only after setter call in DirectClassBuilder\";\n+        return strictInstanceFields;\n+    }\n+\n+    public void setStrictInstanceFields(WritableField.UnsetField[] strictInstanceFields) {\n+        this.strictInstanceFields = strictInstanceFields;\n+    }\n+\n+\n","filename":"src\/java.base\/share\/classes\/jdk\/internal\/classfile\/impl\/BufWriterImpl.java","additions":43,"deletions":0,"binary":false,"changes":43,"status":"modified"},{"patch":"@@ -94,1 +94,0 @@\n- * The version number should be \"9.0\" and is not used in locating the resource.\n@@ -101,1 +100,1 @@\n- *                                 \"java.base\", \"9.0\", \"java\/lang\/String.class\", &size);\n+ *           \"java.base\", \"java\/lang\/String.class\", is_preview_mode, &size);\n@@ -105,1 +104,1 @@\n-        const char* module_name, const char* version, const char* name,\n+        const char* module_name, const char* name, bool is_preview_mode,\n@@ -107,0 +106,21 @@\n+    static const char str_modules[] = \"modules\";\n+    static const char str_packages[] = \"packages\";\n+    static const char preview_infix[] = \"\/META-INF\/preview\";\n+\n+    size_t module_name_len = strlen(module_name);\n+    size_t name_len = strlen(name);\n+    size_t preview_infix_len = strlen(preview_infix);\n+    assert(module_name_len > 0 && \"module name must be non-empty\");\n+    assert(name_len > 0 && \"name must non-empty\");\n+\n+    \/\/ Do not attempt to lookup anything of the form \/modules\/... or \/packages\/...\n+    if (strncmp(module_name, str_modules, sizeof(str_modules)) == 0\n+            || strncmp(module_name, str_packages, sizeof(str_packages)) == 0) {\n+        return 0L;\n+    }\n+    \/\/ If the preview mode version of the path string is too long for the buffer,\n+    \/\/ return not found (even when not in preview mode).\n+    if (1 + module_name_len + preview_infix_len + 1 + name_len + 1 > IMAGE_MAX_PATH) {\n+        return 0L;\n+    }\n+\n@@ -108,4 +128,15 @@\n-    char fullpath[IMAGE_MAX_PATH];\n-    size_t moduleNameLen = strlen(module_name);\n-    size_t nameLen = strlen(name);\n-    size_t index;\n+    char name_buffer[IMAGE_MAX_PATH];\n+    char* path;\n+    {   \/\/ Write the buffer with room to prepend the preview mode infix\n+        \/\/ at the start (saves copying the trailing name part twice).\n+        size_t index = preview_infix_len;\n+        name_buffer[index++] = '\/';\n+        memcpy(&name_buffer[index], module_name, module_name_len);\n+        index += module_name_len;\n+        name_buffer[index++] = '\/';\n+        memcpy(&name_buffer[index], name, name_len);\n+        index += name_len;\n+        name_buffer[index++] = '\\0';\n+        \/\/ Path begins at the leading '\/' (not the start of the buffer).\n+        path = &name_buffer[preview_infix_len];\n+    }\n@@ -113,2 +144,6 @@\n-    assert(moduleNameLen > 0 && \"module name must be non-empty\");\n-    assert(nameLen > 0 && \"name must non-empty\");\n+    \/\/ find_location_index() returns the data \"offset\", not an index.\n+    const ImageFileReader* image_file = (ImageFileReader*) image;\n+    u4 locOffset = image_file->find_location_index(path, (u8*) size);\n+    if (locOffset != 0) {\n+        ImageLocation loc;\n+        loc.set_data(image_file->get_location_offset_data(locOffset));\n@@ -116,2 +151,17 @@\n-    \/\/ If the concatenated string is too long for the buffer, return not found\n-    if (1 + moduleNameLen + 1 + nameLen + 1 > IMAGE_MAX_PATH) {\n+        u4 flags = loc.get_preview_flags();\n+        \/\/ No preview flags means \"a normal resource, without a preview version\".\n+        \/\/ This is the overwhelmingly common case, with or without preview mode.\n+        if (flags == 0) {\n+            return locOffset;\n+        }\n+        \/\/ Regardless of preview mode, don't return resources requested directly\n+        \/\/ via their preview path.\n+        if ((flags & ImageLocation::FLAGS_IS_PREVIEW_VERSION) != 0) {\n+            return 0L;\n+        }\n+        \/\/ Even if there is a preview version, we might not want to return it.\n+        if (!is_preview_mode || (flags & ImageLocation::FLAGS_HAS_PREVIEW_VERSION) == 0) {\n+            return locOffset;\n+        }\n+    } else if (!is_preview_mode) {\n+        \/\/ No normal resource found, and not in preview mode.\n@@ -121,12 +171,23 @@\n-    index = 0;\n-    fullpath[index++] = '\/';\n-    memcpy(&fullpath[index], module_name, moduleNameLen);\n-    index += moduleNameLen;\n-    fullpath[index++] = '\/';\n-    memcpy(&fullpath[index], name, nameLen);\n-    index += nameLen;\n-    fullpath[index++] = '\\0';\n-\n-    JImageLocationRef loc =\n-            (JImageLocationRef) ((ImageFileReader*) image)->find_location_index(fullpath, (u8*) size);\n-    return loc;\n+    \/\/ We are in preview mode, and the preview version of the resource is needed.\n+    \/\/ This is either because:\n+    \/\/ 1. The normal resource was flagged as having a preview version (rare)\n+    \/\/ 2. This is a preview-only resource (there was no normal resource, very rare)\n+    \/\/ 3. The requested resource doesn't exist (this should typically not happen)\n+    \/\/\n+    \/\/ Since we only expect requests for resources which exist in jimage files, we\n+    \/\/ expect this 2nd lookup to succeed (this is contrary to the expectations for\n+    \/\/ the JRT file system, where non-existent resource lookups are common).\n+\n+    {   \/\/ Rewrite the front of the name buffer to make it a preview path.\n+        size_t index = 0;\n+        name_buffer[index++] = '\/';\n+        memcpy(&name_buffer[index], module_name, module_name_len);\n+        index += module_name_len;\n+        memcpy(&name_buffer[index], preview_infix, preview_infix_len);\n+        index += preview_infix_len;\n+        \/\/ Check we copied up to the expected '\/' separator.\n+        assert(name_buffer[index] == '\/' && \"bad string concatenation\");\n+        \/\/ The preview path now begins at the start of the buffer.\n+        path = &name_buffer[0];\n+    }\n+    return image_file->find_location_index(path, (u8*) size);\n","filename":"src\/java.base\/share\/native\/libjimage\/jimage.cpp","additions":84,"deletions":23,"binary":false,"changes":107,"status":"modified"},{"patch":"@@ -127,0 +127,1 @@\n+    final LocalProxyVarsGen localProxyVarsGen;\n@@ -167,0 +168,1 @@\n+        localProxyVarsGen = LocalProxyVarsGen.instance(context);\n@@ -189,0 +191,2 @@\n+        allowValueClasses = (!preview.isPreview(Feature.VALUE_CLASSES) || preview.isEnabled()) &&\n+                Feature.VALUE_CLASSES.allowedInSource(source);\n@@ -207,0 +211,4 @@\n+    \/** Are value classes allowed\n+     *\/\n+    private final boolean allowValueClasses;\n+\n@@ -300,3 +308,1 @@\n-            return;\n-        }\n-        if ((v.flags() & FINAL) != 0 &&\n+        } else if ((v.flags() & FINAL) != 0 &&\n@@ -313,17 +319,0 @@\n-            return;\n-        }\n-\n-        \/\/ Check instance field assignments that appear in constructor prologues\n-        if (rs.isEarlyReference(env, base, v)) {\n-\n-            \/\/ Field may not be inherited from a superclass\n-            if (v.owner != env.enclClass.sym) {\n-                log.error(pos, Errors.CantRefBeforeCtorCalled(v));\n-                return;\n-            }\n-\n-            \/\/ Field may not have an initializer\n-            if ((v.flags() & HASINIT) != 0) {\n-                log.error(pos, Errors.CantAssignInitializedBeforeCtorCalled(v));\n-                return;\n-            }\n@@ -1124,1 +1113,2 @@\n-                            if (TreeInfo.hasAnyConstructorCall(tree)) {\n+                            if ((!allowValueClasses || TreeInfo.isCompactConstructor(tree)) &&\n+                                    TreeInfo.hasAnyConstructorCall(tree)) {\n@@ -1202,0 +1192,1 @@\n+                boolean addedSuperInIdentityClass = false;\n@@ -1206,1 +1197,6 @@\n-                        tree.body.stats = tree.body.stats.prepend(supCall);\n+                        if (allowValueClasses && (owner.isValueClass() || owner.hasStrict() || ((owner.flags_field & RECORD) != 0))) {\n+                            tree.body.stats = tree.body.stats.append(supCall);\n+                        } else {\n+                            tree.body.stats = tree.body.stats.prepend(supCall);\n+                            addedSuperInIdentityClass = true;\n+                        }\n@@ -1244,0 +1240,29 @@\n+                if (localEnv.info.ctorPrologue) {\n+                    boolean thisInvocation = false;\n+                    ListBuffer<JCTree> prologueCode = new ListBuffer<>();\n+                    for (JCTree stat : tree.body.stats) {\n+                        prologueCode.add(stat);\n+                        \/* gather all the stats in the body until a `super` or `this` constructor invocation is found,\n+                         * including the constructor invocation, that way we don't need to worry in the visitor below if\n+                         * if we are dealing or not with prologue code\n+                         *\/\n+                        if (stat instanceof JCExpressionStatement expStmt &&\n+                                expStmt.expr instanceof JCMethodInvocation mi &&\n+                                TreeInfo.isConstructorCall(mi)) {\n+                            thisInvocation = TreeInfo.name(mi.meth) == names._this;\n+                            if (!addedSuperInIdentityClass || !allowValueClasses) {\n+                                break;\n+                            }\n+                        }\n+                    }\n+                    if (!prologueCode.isEmpty()) {\n+                        CtorPrologueVisitor ctorPrologueVisitor = new CtorPrologueVisitor(localEnv,\n+                                addedSuperInIdentityClass && allowValueClasses ?\n+                                        PrologueVisitorMode.WARNINGS_ONLY :\n+                                        thisInvocation ?\n+                                                PrologueVisitorMode.THIS_CONSTRUCTOR :\n+                                                PrologueVisitorMode.SUPER_CONSTRUCTOR,\n+                                false);\n+                        ctorPrologueVisitor.scan(prologueCode.toList());\n+                    }\n+                }\n@@ -1255,0 +1280,338 @@\n+    enum PrologueVisitorMode {\n+        WARNINGS_ONLY,\n+        SUPER_CONSTRUCTOR,\n+        THIS_CONSTRUCTOR\n+    }\n+\n+    class CtorPrologueVisitor extends TreeScanner {\n+        Env<AttrContext> localEnv;\n+        PrologueVisitorMode mode;\n+        boolean isInitializer;\n+\n+        CtorPrologueVisitor(Env<AttrContext> localEnv, PrologueVisitorMode mode, boolean isInitializer) {\n+            this.localEnv = localEnv;\n+            currentClassSym = localEnv.enclClass.sym;\n+            this.mode = mode;\n+            this.isInitializer = isInitializer;\n+        }\n+\n+        boolean insideLambdaOrClassDef = false;\n+\n+        @Override\n+        public void visitLambda(JCLambda lambda) {\n+            boolean previousInsideLambdaOrClassDef = insideLambdaOrClassDef;\n+            try {\n+                insideLambdaOrClassDef = true;\n+                super.visitLambda(lambda);\n+            } finally {\n+                insideLambdaOrClassDef = previousInsideLambdaOrClassDef;\n+            }\n+        }\n+\n+        ClassSymbol currentClassSym;\n+\n+        @Override\n+        public void visitClassDef(JCClassDecl classDecl) {\n+            boolean previousInsideLambdaOrClassDef = insideLambdaOrClassDef;\n+            ClassSymbol previousClassSym = currentClassSym;\n+            try {\n+                insideLambdaOrClassDef = true;\n+                currentClassSym = classDecl.sym;\n+                super.visitClassDef(classDecl);\n+            } finally {\n+                insideLambdaOrClassDef = previousInsideLambdaOrClassDef;\n+                currentClassSym = previousClassSym;\n+            }\n+        }\n+\n+        private void reportPrologueError(JCTree tree, Symbol sym) {\n+            reportPrologueError(tree, sym, false);\n+        }\n+\n+        private void reportPrologueError(JCTree tree, Symbol sym, boolean hasInit) {\n+            preview.checkSourceLevel(tree, Feature.FLEXIBLE_CONSTRUCTORS);\n+            if (mode != PrologueVisitorMode.WARNINGS_ONLY) {\n+                if (hasInit) {\n+                    log.error(tree, Errors.CantAssignInitializedBeforeCtorCalled(sym));\n+                } else {\n+                    log.error(tree, Errors.CantRefBeforeCtorCalled(sym));\n+                }\n+            } else if (allowValueClasses) {\n+                \/\/ issue lint warning\n+                log.warning(tree, LintWarnings.WouldNotBeAllowedInPrologue(sym));\n+            }\n+        }\n+\n+        @Override\n+        public void visitApply(JCMethodInvocation tree) {\n+            super.visitApply(tree);\n+            Name name = TreeInfo.name(tree.meth);\n+            boolean isConstructorCall = name == names._this || name == names._super;\n+            Symbol msym = TreeInfo.symbolFor(tree.meth);\n+            \/\/ is this an instance method call or an illegal constructor invocation like: `this.super()`?\n+            if (msym != null && \/\/ for erroneous invocations msym can be null, ignore those\n+                (!isConstructorCall ||\n+                isConstructorCall && tree.meth.hasTag(SELECT))) {\n+                if (isEarlyReference(localEnv, tree.meth, msym))\n+                    reportPrologueError(tree.meth, msym);\n+            }\n+        }\n+\n+        @Override\n+        public void visitIdent(JCIdent tree) {\n+            analyzeSymbol(tree);\n+        }\n+\n+        boolean isIndexed = false;\n+\n+        @Override\n+        public void visitIndexed(JCArrayAccess tree) {\n+            boolean previousIsIndexed = isIndexed;\n+            try {\n+                isIndexed = true;\n+                scan(tree.indexed);\n+            } finally {\n+                isIndexed = previousIsIndexed;\n+            }\n+            scan(tree.index);\n+            if (mode == PrologueVisitorMode.SUPER_CONSTRUCTOR && isInstanceField(tree.indexed)) {\n+                localProxyVarsGen.addFieldReadInPrologue(localEnv.enclMethod, TreeInfo.symbolFor(tree.indexed));\n+            }\n+        }\n+\n+        @Override\n+        public void visitSelect(JCFieldAccess tree) {\n+            SelectScanner ss = new SelectScanner();\n+            ss.scan(tree);\n+            if (ss.scanLater == null) {\n+                Symbol sym = TreeInfo.symbolFor(tree);\n+                \/\/ if this is a field access\n+                if (sym.kind == VAR && sym.owner.kind == TYP) {\n+                    \/\/ Type.super.field or super.field expressions are forbidden in early construction contexts\n+                    for (JCTree subtree : ss.selectorTrees) {\n+                        if (TreeInfo.isSuperOrSelectorDotSuper(subtree)) {\n+                            reportPrologueError(tree, sym);\n+                            return;\n+                        }\n+                    }\n+                }\n+                analyzeSymbol(tree);\n+            } else {\n+                boolean prevLhs = isInLHS;\n+                try {\n+                    isInLHS = false;\n+                    scan(ss.scanLater);\n+                } finally {\n+                    isInLHS = prevLhs;\n+                }\n+            }\n+            if (mode == PrologueVisitorMode.SUPER_CONSTRUCTOR) {\n+                for (JCTree subtree : ss.selectorTrees) {\n+                    if (isInstanceField(subtree)) {\n+                        \/\/ we need to add a proxy for this one\n+                        localProxyVarsGen.addFieldReadInPrologue(localEnv.enclMethod, TreeInfo.symbolFor(subtree));\n+                    }\n+                }\n+            }\n+        }\n+\n+        boolean isInstanceField(JCTree tree) {\n+            Symbol sym = TreeInfo.symbolFor(tree);\n+            return (sym != null &&\n+                    !sym.isStatic() &&\n+                    sym.kind == VAR &&\n+                    sym.owner.kind == TYP &&\n+                    sym.name != names._this &&\n+                    sym.name != names._super &&\n+                    isEarlyReference(localEnv, tree, sym));\n+        }\n+\n+        @Override\n+        public void visitNewClass(JCNewClass tree) {\n+            super.visitNewClass(tree);\n+            checkNewClassAndMethRefs(tree, tree.type);\n+        }\n+\n+        @Override\n+        public void visitReference(JCMemberReference tree) {\n+            super.visitReference(tree);\n+            if (tree.getMode() == JCMemberReference.ReferenceMode.NEW) {\n+                checkNewClassAndMethRefs(tree, tree.expr.type);\n+            }\n+        }\n+\n+        void checkNewClassAndMethRefs(JCTree tree, Type t) {\n+            if (t.tsym.isEnclosedBy(localEnv.enclClass.sym) &&\n+                    !t.tsym.isStatic() &&\n+                    !t.tsym.isDirectlyOrIndirectlyLocal()) {\n+                reportPrologueError(tree, t.getEnclosingType().tsym);\n+            }\n+        }\n+\n+        \/* if a symbol is in the LHS of an assignment expression we won't consider it as a candidate\n+         * for a proxy local variable later on\n+         *\/\n+        boolean isInLHS = false;\n+\n+        @Override\n+        public void visitAssign(JCAssign tree) {\n+            boolean previousIsInLHS = isInLHS;\n+            try {\n+                isInLHS = true;\n+                scan(tree.lhs);\n+            } finally {\n+                isInLHS = previousIsInLHS;\n+            }\n+            scan(tree.rhs);\n+        }\n+\n+        @Override\n+        public void visitMethodDef(JCMethodDecl tree) {\n+            \/\/ ignore any declarative part, mainly to avoid scanning receiver parameters\n+            scan(tree.body);\n+        }\n+\n+        void analyzeSymbol(JCTree tree) {\n+            Symbol sym = TreeInfo.symbolFor(tree);\n+            \/\/ make sure that there is a symbol and it is not static\n+            if (sym == null || sym.isStatic()) {\n+                return;\n+            }\n+            if (isInLHS && !insideLambdaOrClassDef) {\n+                \/\/ Check instance field assignments that appear in constructor prologues\n+                if (isEarlyReference(localEnv, tree, sym)) {\n+                    \/\/ Field may not be inherited from a superclass\n+                    if (sym.owner != localEnv.enclClass.sym) {\n+                        reportPrologueError(tree, sym);\n+                        return;\n+                    }\n+                    \/\/ Field may not have an initializer\n+                    if ((sym.flags() & HASINIT) != 0) {\n+                        if (!localEnv.enclClass.sym.isValueClass() || !sym.type.hasTag(ARRAY) || !isIndexed) {\n+                            reportPrologueError(tree, sym, true);\n+                        }\n+                        return;\n+                    }\n+                    \/\/ cant reference an instance field before a this constructor\n+                    if (allowValueClasses && mode == PrologueVisitorMode.THIS_CONSTRUCTOR) {\n+                        reportPrologueError(tree, sym);\n+                        return;\n+                    }\n+                }\n+                return;\n+            }\n+            tree = TreeInfo.skipParens(tree);\n+            if (sym.kind == VAR && sym.owner.kind == TYP) {\n+                if (sym.name == names._this || sym.name == names._super) {\n+                    \/\/ are we seeing something like `this` or `CurrentClass.this` or `SuperClass.super::foo`?\n+                    if (TreeInfo.isExplicitThisReference(\n+                            types,\n+                            (ClassType)localEnv.enclClass.sym.type,\n+                            tree)) {\n+                        reportPrologueError(tree, sym);\n+                    }\n+                } else if (sym.kind == VAR && sym.owner.kind == TYP) { \/\/ now fields only\n+                    if (sym.owner != localEnv.enclClass.sym) {\n+                        if (localEnv.enclClass.sym.isSubClass(sym.owner, types) &&\n+                                sym.isInheritedIn(localEnv.enclClass.sym, types)) {\n+                            \/* if we are dealing with a field that doesn't belong to the current class, but the\n+                             * field is inherited, this is an error. Unless, the super class is also an outer\n+                             * class and the field's qualifier refers to the outer class\n+                             *\/\n+                            if (tree.hasTag(IDENT) ||\n+                                TreeInfo.isExplicitThisReference(\n+                                        types,\n+                                        (ClassType)localEnv.enclClass.sym.type,\n+                                        ((JCFieldAccess)tree).selected)) {\n+                                reportPrologueError(tree, sym);\n+                            }\n+                        }\n+                    } else if (isEarlyReference(localEnv, tree, sym)) {\n+                        \/* now this is a `proper` instance field of the current class\n+                         * references to fields of identity classes which happen to have initializers are\n+                         * not allowed in the prologue.\n+                         * But it is OK for a field with initializer to refer to another field with initializer,\n+                         * so no warning or error if we are analyzing a field initializer.\n+                         *\/\n+                        if (insideLambdaOrClassDef ||\n+                            (!localEnv.enclClass.sym.isValueClass() &&\n+                             (sym.flags_field & HASINIT) != 0 &&\n+                             !isInitializer))\n+                            reportPrologueError(tree, sym);\n+                        \/\/ we will need to generate a proxy for this field later on\n+                        if (!isInLHS) {\n+                            if (!allowValueClasses) {\n+                                reportPrologueError(tree, sym);\n+                            } else {\n+                                if (mode == PrologueVisitorMode.THIS_CONSTRUCTOR) {\n+                                    reportPrologueError(tree, sym);\n+                                } else if (mode == PrologueVisitorMode.SUPER_CONSTRUCTOR) {\n+                                    localProxyVarsGen.addFieldReadInPrologue(localEnv.enclMethod, sym);\n+                                }\n+                                \/* we do nothing in warnings only mode, as in that mode we are simulating what\n+                                 * the compiler would do in case the constructor code would be in the prologue\n+                                 * phase\n+                                 *\/\n+                            }\n+                        }\n+                    }\n+                }\n+            }\n+        }\n+\n+        \/**\n+         * Determine if the symbol appearance constitutes an early reference to the current class.\n+         *\n+         * <p>\n+         * This means the symbol is an instance field, or method, of the current class and it appears\n+         * in an early initialization context of it (i.e., one of its constructor prologues).\n+         *\n+         * @param env    The current environment\n+         * @param tree   the AST referencing the variable\n+         * @param sym    The symbol\n+         *\/\n+        private boolean isEarlyReference(Env<AttrContext> env, JCTree tree, Symbol sym) {\n+            if ((sym.flags() & STATIC) == 0 &&\n+                    (sym.kind == VAR || sym.kind == MTH) &&\n+                    sym.isMemberOf(env.enclClass.sym, types)) {\n+                \/\/ Allow \"Foo.this.x\" when \"Foo\" is (also) an outer class, as this refers to the outer instance\n+                if (tree instanceof JCFieldAccess fa) {\n+                    return TreeInfo.isExplicitThisReference(types, (ClassType)env.enclClass.type, fa.selected);\n+                } else if (currentClassSym != env.enclClass.sym) {\n+                    \/* so we are inside a class, CI, in the prologue of an outer class, CO, and the symbol being\n+                     * analyzed has no qualifier. So if the symbol is a member of CI the reference is allowed,\n+                     * otherwise it is not.\n+                     * It could be that the reference to CI's member happens inside CI's own prologue, but that\n+                     * will be checked separately, when CI's prologue is analyzed.\n+                     *\/\n+                    return !sym.isMemberOf(currentClassSym, types);\n+                }\n+                return true;\n+            }\n+            return false;\n+        }\n+\n+        \/* scanner for a select expression, anything that is not a select or identifier\n+         * will be stored for further analysis\n+         *\/\n+        class SelectScanner extends DeferredAttr.FilterScanner {\n+            JCTree scanLater;\n+            java.util.List<JCTree> selectorTrees = new ArrayList<>();\n+\n+            SelectScanner() {\n+                super(Set.of(IDENT, SELECT, PARENS));\n+            }\n+\n+            @Override\n+            public void visitSelect(JCFieldAccess tree) {\n+                super.visitSelect(tree);\n+                selectorTrees.add(tree.selected);\n+            }\n+\n+            @Override\n+            void skip(JCTree tree) {\n+                scanLater = tree;\n+            }\n+        }\n+    }\n+\n@@ -1303,0 +1666,1 @@\n+                Env<AttrContext> initEnv = memberEnter.initEnv(tree, env);\n@@ -1309,1 +1673,0 @@\n-                    Env<AttrContext> initEnv = memberEnter.initEnv(tree, env);\n@@ -1315,4 +1678,13 @@\n-                    attribExpr(tree.init, initEnv, v.type);\n-                    if (tree.isImplicitlyTyped()) {\n-                        \/\/fixup local variable type\n-                        v.type = chk.checkLocalVarType(tree, tree.init.type, tree.name);\n+                    boolean previousCtorPrologue = initEnv.info.ctorPrologue;\n+                    try {\n+                        if (v.owner.kind == TYP && !v.isStatic() && v.isStrict()) {\n+                            \/\/ strict instance initializer in a value class\n+                            initEnv.info.ctorPrologue = true;\n+                        }\n+                        attribExpr(tree.init, initEnv, v.type);\n+                        if (tree.isImplicitlyTyped()) {\n+                            \/\/fixup local variable type\n+                            v.type = chk.checkLocalVarType(tree, tree.init.type, tree.name);\n+                        }\n+                    } finally {\n+                        initEnv.info.ctorPrologue = previousCtorPrologue;\n@@ -1321,0 +1693,7 @@\n+                if (allowValueClasses && v.owner.kind == TYP && !v.isStatic()) {\n+                    \/\/ strict field initializers are inlined in constructor's prologues\n+                    CtorPrologueVisitor ctorPrologueVisitor = new CtorPrologueVisitor(initEnv,\n+                            !v.isStrict() ? PrologueVisitorMode.WARNINGS_ONLY : PrologueVisitorMode.SUPER_CONSTRUCTOR,\n+                            true);\n+                    ctorPrologueVisitor.scan(tree.init);\n+                }\n@@ -1439,1 +1818,5 @@\n-            if ((tree.flags & STATIC) != 0) localEnv.info.staticLevel++;\n+            if ((tree.flags & STATIC) != 0) {\n+                localEnv.info.staticLevel++;\n+            } else {\n+                localEnv.info.instanceInitializerBlock = true;\n+            }\n@@ -1952,2 +2335,2 @@\n-        chk.checkRefType(tree.pos(), attribExpr(tree.lock, env));\n-        if (tree.lock.type != null && tree.lock.type.isValueBased()) {\n+        boolean identityType = chk.checkIdentityType(tree.pos(), attribExpr(tree.lock, env));\n+        if (identityType && tree.lock.type != null && tree.lock.type.isValueBased()) {\n@@ -4409,0 +4792,1 @@\n+        Assert.check(site == tree.selected.type);\n@@ -5520,1 +5904,1 @@\n-                } else {\n+                } else if ((c.flags_field & Flags.COMPOUND) == 0) {\n@@ -5557,0 +5941,5 @@\n+                if (c.isValueClass()) {\n+                    Assert.check(env.tree.hasTag(CLASSDEF));\n+                    chk.checkConstraintsOfValueClass((JCClassDecl) env.tree, c);\n+                }\n+\n@@ -5699,1 +6088,1 @@\n-            chk.checkSerialStructure(tree, c);\n+            chk.checkSerialStructure(env, tree, c);\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/comp\/Attr.java","additions":421,"deletions":32,"binary":false,"changes":453,"status":"modified"},{"patch":"@@ -118,0 +118,1 @@\n+    private final LocalProxyVarsGen localProxyVarsGen;\n@@ -157,0 +158,1 @@\n+        localProxyVarsGen = LocalProxyVarsGen.instance(context);\n@@ -424,39 +426,44 @@\n-        switch ((short)(sym.flags() & AccessFlags)) {\n-        case PRIVATE:\n-            return\n-                (env.enclClass.sym == sym.owner \/\/ fast special case\n-                 ||\n-                 env.enclClass.sym.outermostClass() ==\n-                 sym.owner.outermostClass()\n-                 ||\n-                 privateMemberInPermitsClauseIfAllowed(env, sym))\n-                &&\n-                sym.isInheritedIn(site.tsym, types);\n-        case 0:\n-            return\n-                (env.toplevel.packge == sym.owner.owner \/\/ fast special case\n-                 ||\n-                 env.toplevel.packge == sym.packge())\n-                &&\n-                isAccessible(env, site, checkInner)\n-                &&\n-                sym.isInheritedIn(site.tsym, types)\n-                &&\n-                notOverriddenIn(site, sym);\n-        case PROTECTED:\n-            return\n-                (env.toplevel.packge == sym.owner.owner \/\/ fast special case\n-                 ||\n-                 env.toplevel.packge == sym.packge()\n-                 ||\n-                 isProtectedAccessible(sym, env.enclClass.sym, site)\n-                 ||\n-                 \/\/ OK to select instance method or field from 'super' or type name\n-                 \/\/ (but type names should be disallowed elsewhere!)\n-                 env.info.selectSuper && (sym.flags() & STATIC) == 0 && sym.kind != TYP)\n-                &&\n-                isAccessible(env, site, checkInner)\n-                &&\n-                notOverriddenIn(site, sym);\n-        default: \/\/ this case includes erroneous combinations as well\n-            return isAccessible(env, site, checkInner) && notOverriddenIn(site, sym);\n+        ClassSymbol enclosingCsym = env.enclClass.sym;\n+        try {\n+            switch ((short)(sym.flags() & AccessFlags)) {\n+                case PRIVATE:\n+                    return\n+                            (env.enclClass.sym == sym.owner \/\/ fast special case\n+                                    ||\n+                                    env.enclClass.sym.outermostClass() ==\n+                                    sym.owner.outermostClass()\n+                                    ||\n+                                    privateMemberInPermitsClauseIfAllowed(env, sym))\n+                                &&\n+                                    sym.isInheritedIn(site.tsym, types);\n+                case 0:\n+                    return\n+                            (env.toplevel.packge == sym.owner.owner \/\/ fast special case\n+                                    ||\n+                                    env.toplevel.packge == sym.packge())\n+                                    &&\n+                                    isAccessible(env, site, checkInner)\n+                                    &&\n+                                    sym.isInheritedIn(site.tsym, types)\n+                                    &&\n+                                    notOverriddenIn(site, sym);\n+                case PROTECTED:\n+                    return\n+                            (env.toplevel.packge == sym.owner.owner \/\/ fast special case\n+                                    ||\n+                                    env.toplevel.packge == sym.packge()\n+                                    ||\n+                                    isProtectedAccessible(sym, env.enclClass.sym, site)\n+                                    ||\n+                                    \/\/ OK to select instance method or field from 'super' or type name\n+                                    \/\/ (but type names should be disallowed elsewhere!)\n+                                    env.info.selectSuper && (sym.flags() & STATIC) == 0 && sym.kind != TYP)\n+                                    &&\n+                                    isAccessible(env, site, checkInner)\n+                                    &&\n+                                    notOverriddenIn(site, sym);\n+                default: \/\/ this case includes erroneous combinations as well\n+                    return isAccessible(env, site, checkInner) && notOverriddenIn(site, sym);\n+            }\n+        } finally {\n+            env.enclClass.sym = enclosingCsym;\n@@ -1533,2 +1540,0 @@\n-                    if (env1.info.ctorPrologue && !isAllowedEarlyReference(pos, env1, (VarSymbol)sym))\n-                        return new RefBeforeCtorCalledError(sym);\n@@ -2042,2 +2047,0 @@\n-                        if (env1.info.ctorPrologue && env1 == env)\n-                            return new RefBeforeCtorCalledError(sym);\n@@ -3819,3 +3822,0 @@\n-                    } else if (env1.info.ctorPrologue && !isAllowedEarlyReference(pos, env1, (VarSymbol)sym)) {\n-                        \/\/ early construction context, stop search\n-                        return new RefBeforeCtorCalledError(sym);\n@@ -3881,4 +3881,0 @@\n-                    else if (env1.info.ctorPrologue &&\n-                            !isReceiverParameter(env, tree) &&\n-                            !isAllowedEarlyReference(pos, env1, (VarSymbol)sym))\n-                        sym = new RefBeforeCtorCalledError(sym);\n@@ -3898,2 +3894,0 @@\n-                    if (env.info.ctorPrologue)\n-                        log.error(pos, Errors.CantRefBeforeCtorCalled(name));\n@@ -3935,95 +3929,0 @@\n-    private boolean isReceiverParameter(Env<AttrContext> env, JCFieldAccess tree) {\n-        if (env.tree.getTag() != METHODDEF)\n-            return false;\n-        JCMethodDecl method = (JCMethodDecl)env.tree;\n-        return method.recvparam != null && tree == method.recvparam.nameexpr;\n-    }\n-\n-    \/**\n-     * Determine if an early instance field reference may appear in a constructor prologue.\n-     *\n-     * <p>\n-     * This is only allowed when:\n-     *  - The field is being assigned a value (i.e., written but not read)\n-     *  - The field is not inherited from a superclass\n-     *  - The assignment is not within a lambda, because that would require\n-     *    capturing 'this' which is not allowed prior to super().\n-     *\n-     * <p>\n-     * Note, this method doesn't catch all such scenarios, because this method\n-     * is invoked for symbol \"x\" only for \"x = 42\" but not for \"this.x = 42\".\n-     * We also don't verify that the field has no initializer, which is required.\n-     * To catch those cases, we rely on similar logic in Attr.checkAssignable().\n-     *\/\n-    private boolean isAllowedEarlyReference(DiagnosticPosition pos, Env<AttrContext> env, VarSymbol v) {\n-\n-        \/\/ Check assumptions\n-        Assert.check(env.info.ctorPrologue);\n-        Assert.check((v.flags_field & STATIC) == 0);\n-\n-        \/\/ The symbol must appear in the LHS of an assignment statement\n-        if (!(env.tree instanceof JCAssign assign))\n-            return false;\n-\n-        \/\/ The assignment statement must not be within a lambda\n-        if (env.info.isLambda)\n-            return false;\n-\n-        \/\/ Get the symbol's qualifier, if any\n-        JCExpression lhs = TreeInfo.skipParens(assign.lhs);\n-        JCExpression base;\n-        switch (lhs.getTag()) {\n-        case IDENT:\n-            base = null;\n-            break;\n-        case SELECT:\n-            JCFieldAccess select = (JCFieldAccess)lhs;\n-            base = select.selected;\n-            if (!TreeInfo.isExplicitThisReference(types, (ClassType)env.enclClass.type, base))\n-                return false;\n-            break;\n-        default:\n-            return false;\n-        }\n-\n-        \/\/ If an early reference, the field must not be declared in a superclass\n-        if (isEarlyReference(env, base, v) && v.owner != env.enclClass.sym)\n-            return false;\n-\n-        \/\/ The flexible constructors feature must be enabled\n-        preview.checkSourceLevel(pos, Feature.FLEXIBLE_CONSTRUCTORS);\n-\n-        \/\/ OK\n-        return true;\n-    }\n-\n-    \/**\n-     * Determine if the variable appearance constitutes an early reference to the current class.\n-     *\n-     * <p>\n-     * This means the variable is an instance field of the current class and it appears\n-     * in an early initialization context of it (i.e., one of its constructor prologues).\n-     *\n-     * <p>\n-     * Such a reference is only allowed for assignments to non-initialized fields that are\n-     * not inherited from a superclass, though that is not enforced by this method.\n-     *\n-     * @param env    The current environment\n-     * @param base   Variable qualifier, if any, otherwise null\n-     * @param v      The variable\n-     *\/\n-    public boolean isEarlyReference(Env<AttrContext> env, JCTree base, VarSymbol v) {\n-        if (env.info.ctorPrologue &&\n-                (v.flags() & STATIC) == 0 &&\n-                v.isMemberOf(env.enclClass.sym, types)) {\n-\n-            \/\/ Allow \"Foo.this.x\" when \"Foo\" is (also) an outer class, as this refers to the outer instance\n-            if (base != null) {\n-                return TreeInfo.isExplicitThisReference(types, (ClassType)env.enclClass.type, base);\n-            }\n-\n-            \/\/ It's an early reference to an instance field member of the current instance\n-            return true;\n-        }\n-        return false;\n-    }\n@@ -4339,1 +4238,0 @@\n-                              kindName(ws.owner),\n@@ -5240,4 +5138,0 @@\n-\n-        boolean internal() {\n-            return internalResolution;\n-        }\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/comp\/Resolve.java","additions":46,"deletions":152,"binary":false,"changes":198,"status":"modified"},{"patch":"@@ -28,2 +28,0 @@\n-\n-\n@@ -34,1 +32,0 @@\n-import com.sun.tools.javac.comp.AttrContext;\n@@ -185,0 +182,15 @@\n+    \/** Is this tree `super`, or `Ident.super`?\n+     *\/\n+    public static boolean isSuperOrSelectorDotSuper(JCTree tree) {\n+        switch (tree.getTag()) {\n+            case PARENS:\n+                return isSuperOrSelectorDotSuper(skipParens(tree));\n+            case IDENT:\n+                return ((JCIdent)tree).name == ((JCIdent)tree).name.table.names._super;\n+            case SELECT:\n+                return ((JCFieldAccess)tree).name == ((JCFieldAccess)tree).name.table.names._super;\n+            default:\n+                return false;\n+        }\n+    }\n+\n@@ -193,0 +205,1 @@\n+        Symbol.ClassSymbol currentClassSym = (Symbol.ClassSymbol) types.erasure(currentClass).tsym;\n@@ -199,1 +212,4 @@\n-                return ident.name == names._this || ident.name == names._super;\n+                return ident.name == names._this && tree.type.tsym == currentClass.tsym ||\n+                       ident.name == names._super &&\n+                               (tree.type.tsym == currentClass.tsym ||\n+                                currentClassSym.isSubClass(tree.type.tsym, types));\n@@ -206,2 +222,1 @@\n-                Symbol.ClassSymbol currentClassSym = (Symbol.ClassSymbol)((Type.ClassType)types.erasure(currentClass)).tsym;\n-                Symbol.ClassSymbol selectedClassSym = (Symbol.ClassSymbol)((Type.ClassType)selectedType).tsym;\n+                Symbol.ClassSymbol selectedClassSym = (Symbol.ClassSymbol)(selectedType).tsym;\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/tree\/TreeInfo.java","additions":21,"deletions":6,"binary":false,"changes":27,"status":"modified"},{"patch":"@@ -80,0 +80,3 @@\n+compiler\/jvmci\/jdk.vm.ci.hotspot.test\/src\/jdk\/vm\/ci\/hotspot\/test\/MemoryAccessProviderTest.java 8350208 generic-all\n+compiler\/jvmci\/jdk.vm.ci.hotspot.test\/src\/jdk\/vm\/ci\/hotspot\/test\/TestHotSpotResolvedJavaField.java 8350208 generic-all\n+\n@@ -82,0 +85,24 @@\n+# Valhalla\n+compiler\/whitebox\/DeoptimizeRelocatedNMethod.java 8370571 generic-all\n+compiler\/whitebox\/RelocateNMethodMultiplePaths.java#C1 8370571 generic-all\n+compiler\/whitebox\/RelocateNMethodMultiplePaths.java#C2 8370571 generic-all\n+compiler\/whitebox\/RelocateNMethodMultiplePaths.java#G1C1 8370571 generic-all\n+compiler\/whitebox\/RelocateNMethodMultiplePaths.java#G1C2 8370571 generic-all\n+compiler\/whitebox\/RelocateNMethodMultiplePaths.java#ParallelC1 8370571 generic-all\n+compiler\/whitebox\/RelocateNMethodMultiplePaths.java#ParallelC2 8370571 generic-all\n+compiler\/whitebox\/RelocateNMethodMultiplePaths.java#SerialC1 8370571 generic-all\n+compiler\/whitebox\/RelocateNMethodMultiplePaths.java#SerialC2 8370571 generic-all\n+compiler\/whitebox\/RelocateNMethodMultiplePaths.java#ZGCC1 8370571 generic-all\n+compiler\/whitebox\/RelocateNMethodMultiplePaths.java#ZGCC2 8370571 generic-all\n+compiler\/whitebox\/StressNMethodRelocation.java 8370571 generic-all\n+compiler\/valhalla\/inlinetypes\/TestC1.java             8372341 generic-all\n+compiler\/valhalla\/inlinetypes\/TestCastMismatch.java   8372341 generic-all\n+compiler\/valhalla\/inlinetypes\/TestGetfieldChains.java 8372341 generic-all\n+compiler\/codegen\/TestRedundantLea.java#GetAndSet          8361089 generic-all\n+compiler\/codegen\/TestRedundantLea.java#StringEquals       8361089 generic-all\n+compiler\/codegen\/TestRedundantLea.java#StringInflate      8361089 generic-all\n+compiler\/codegen\/TestRedundantLea.java#RegexFind          8361089 generic-all\n+compiler\/codegen\/TestRedundantLea.java#StoreNSerial       8361089 generic-all\n+compiler\/codegen\/TestRedundantLea.java#StoreNParallel     8361089 generic-all\n+compiler\/codegen\/TestRedundantLea.java#Spill              8361089 generic-all\n+\n@@ -103,0 +130,1 @@\n+runtime\/cds\/appcds\/redefineClass\/RedefineRunningMethods_Shared.java  8304168 generic-all\n@@ -122,0 +150,16 @@\n+\n+# Valhalla\n+runtime\/valhalla\/inlinetypes\/verifier\/StrictInstanceFieldsTest.java CODETOOLS-7904031 generic-all\n+runtime\/valhalla\/inlinetypes\/verifier\/StrictStaticFieldsTest.java CODETOOLS-7904031 generic-all\n+\n+runtime\/cds\/TestDefaultArchiveLoading.java#coops_nocoh            8366774           generic-all\n+runtime\/cds\/TestDefaultArchiveLoading.java#nocoops_nocoh          8366774           generic-all\n+\n+# Valhalla + AOT\n+runtime\/cds\/appcds\/aotCache\/HelloAOTCache.java                                  8369043 generic-aarch64\n+runtime\/cds\/appcds\/aotCode\/AOTCodeFlags.java                                    8369043 generic-aarch64\n+runtime\/cds\/appcds\/methodHandles\/MethodHandlesGeneralTest.java#aot              8367408 generic-all\n+runtime\/cds\/appcds\/resolvedConstants\/ResolvedConstants.java#aot                 8371456 generic-all\n+runtime\/cds\/appcds\/resolvedConstants\/ResolvedConstants.java#static              8371456 generic-all\n+runtime\/cds\/appcds\/dynamicArchive\/DynamicArchiveRelocationTest.java             8372265 generic-all\n+runtime\/cds\/appcds\/dynamicArchive\/HelloDynamicInlineClass.java                  8372265 generic-all\n@@ -148,0 +192,57 @@\n+\n+# Valhalla TODO:\n+serviceability\/sa\/ClhsdbCDSCore.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbCDSJstackPrintAll.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbFindPC.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbInspect.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbLongConstant.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbJdis.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbJstack.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbPrintAll.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbPrintAs.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbPrintStatics.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbSource.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbSymbol.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbWhere.java 8190936 generic-all\n+serviceability\/sa\/JhsdbThreadInfoTest.java 8190936 generic-all\n+serviceability\/sa\/TestClassDump.java 8190936 generic-all\n+serviceability\/sa\/TestClhsdbJstackLock.java 8190936 generic-all\n+serviceability\/sa\/TestCpoolForInvokeDynamic.java 8190936 generic-all\n+serviceability\/sa\/TestHeapDumpForInvokeDynamic.java 8190936 generic-all\n+serviceability\/sa\/TestHeapDumpForLargeArray.java 8190936 generic-all\n+serviceability\/sa\/TestIntConstant.java 8190936 generic-all\n+serviceability\/sa\/TestJhsdbJstackLock.java 8190936 generic-all\n+serviceability\/sa\/TestJmapCore.java 8190936 generic-all\n+serviceability\/sa\/TestJmapCoreMetaspace.java 8190936 generic-all\n+serviceability\/sa\/TestPrintMdo.java 8190936 generic-all\n+serviceability\/sa\/jmap-hprof\/JMapHProfLargeHeapTest.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbDumpclass.java 8190936 generic-all\n+\n+# Array Changes TODO\n+serviceability\/sa\/CDSJMapClstats.java 8365722 generic-all\n+serviceability\/sa\/ClhsdbClasses.java 8365722 generic-all\n+serviceability\/sa\/sadebugd\/DisableRegistryTest.java 8365722 generic-all\n+serviceability\/sa\/ClhsdbDumpheap.java 8365722 generic-all\n+serviceability\/sa\/sadebugd\/ClhsdbTestConnectArgument.java 8365722 generic-all\n+serviceability\/sa\/sadebugd\/DebugdConnectTest.java 8365722 generic-all\n+serviceability\/sa\/ClhsdbJhisto.java 8365722 generic-all\n+serviceability\/sa\/ClhsdbJstack.java#id1 8365722 generic-all\n+serviceability\/sa\/ClhsdbJstackWithConcurrentLock.java 8365722 generic-all\n+serviceability\/sa\/TestJhsdbJstackWithVirtualThread.java 8365722 generic-all\n+serviceability\/sa\/ClhsdbJstackXcompStress.java 8365722 generic-all\n+serviceability\/sa\/ClhsdbPstack.java#process 8365722 generic-all\n+serviceability\/sa\/ClhsdbPstack.java#core 8365722 generic-all\n+serviceability\/sa\/ClhsdbScanOops.java#id0 8365722 generic-all\n+serviceability\/sa\/ClhsdbScanOops.java#id1 8365722 generic-all\n+serviceability\/sa\/ClhsdbScanOops.java#serial 8365722 generic-all\n+serviceability\/sa\/ClhsdbScanOops.java#parallel 8365722 generic-all\n+serviceability\/sa\/DeadlockDetectionTest.java 8365722 generic-all\n+serviceability\/sa\/ClhsdbJstack.java#id0 8365722 generic-all\n+serviceability\/sa\/TestInstanceKlassSize.java 8365722 generic-all\n+serviceability\/sa\/TestSysProps.java 8365722 generic-all\n+serviceability\/sa\/sadebugd\/ClhsdbAttachToDebugServer.java 8365722 generic-all\n+resourcehogs\/serviceability\/sa\/TestHeapDumpForLargeArray.java 8365722 generic-all\n+\n+resourcehogs\/serviceability\/sa\/ClhsdbRegionDetailsScanOopsForG1.java 8190936 generic-all\n+vmTestbase\/nsk\/jvmti\/scenarios\/events\/EM04\/em04t001\/TestDescription.java 8367590 generic-all\n+\n@@ -187,0 +288,2 @@\n+vmTestbase\/vm\/mlvm\/hiddenloader\/stress\/byteMutation\/Test.java 8317172 generic-all\n+\n","filename":"test\/hotspot\/jtreg\/ProblemList.txt","additions":103,"deletions":0,"binary":false,"changes":103,"status":"modified"},{"patch":"@@ -31,0 +31,1 @@\n+import compiler.valhalla.inlinetypes.InlineTypeIRNode;\n@@ -90,1 +91,1 @@\n-    private static final String PREFIX = \"_#\";\n+    public static final String PREFIX = \"_#\";\n@@ -153,0 +154,6 @@\n+    \/\/ Valhalla: Make sure that all Valhalla specific IR nodes are also properly initialized. Doing it here also\n+    \/\/           ensures that the Flag VM is able to pick up the correct compile phases.\n+    static {\n+        InlineTypeIRNode.forceStaticInitialization();\n+    }\n+\n@@ -384,2 +391,6 @@\n-        String regex = START + \"Allocate\\\\b\" + MID + \"allocationKlass:.*\\\\b\" + IS_REPLACED + \"\\\\s.*\" + END;\n-        macroNodes(ALLOC_OF, regex);\n+        allocateOfNodes(ALLOC_OF, IS_REPLACED);\n+    }\n+\n+    public static void allocateOfNodes(String irNodePlaceholder, String allocatee) {\n+        String regex = START + \"Allocate\\\\b\" + MID + \"allocationKlass:.*\\\\b\" + allocatee + \"\\\\s.*\" + END;\n+        macroNodes(irNodePlaceholder, regex);\n@@ -396,0 +407,4 @@\n+        allocateArrayOfNodes(ALLOC_ARRAY_OF, IS_REPLACED);\n+    }\n+\n+    public static void allocateArrayOfNodes(String irNodePlaceholder, String allocatee) {\n@@ -414,1 +429,1 @@\n-        String name_part = \"\\\\[+.(\" + partial_name_prefix + \")?\" + IS_REPLACED + \";\";\n+        String name_part = \"\\\\[+.(\" + partial_name_prefix + \")?\" + allocatee + \";\";\n@@ -416,1 +431,1 @@\n-        macroNodes(ALLOC_ARRAY_OF, regex);\n+        macroNodes(irNodePlaceholder, regex);\n@@ -481,1 +496,1 @@\n-        callOfNodes(CALL_OF, \"Call.*\");\n+        callOfNodes(CALL_OF, \"Call.*\", IS_REPLACED + \" \" );\n@@ -486,1 +501,6 @@\n-        callOfNodes(CALL_OF_METHOD, \"Call.*Java\");\n+        callOfNodes(CALL_OF_METHOD, \"Call.*Java\", IS_REPLACED + \" \");\n+    }\n+\n+    public static final String STATIC_CALL = PREFIX + \"STATIC_CALL\" + POSTFIX;\n+    static {\n+        beforeMatchingNameRegex(STATIC_CALL, \"CallStaticJava\");\n@@ -491,1 +511,19 @@\n-        callOfNodes(STATIC_CALL_OF_METHOD, \"CallStaticJava\");\n+        staticCallOfMethodNodes(STATIC_CALL_OF_METHOD, IS_REPLACED + \" \");\n+    }\n+\n+    public static void staticCallOfMethodNodes(String irNodePlaceholder, String calleeRegex) {\n+        callOfNodes(irNodePlaceholder, \"CallStaticJava\", calleeRegex);\n+    }\n+\n+    public static final String CALL_LEAF_NO_FP = PREFIX + \"CALL_LEAF_NO_FP\" + POSTFIX;\n+    static {\n+        beforeMatchingNameRegex(CALL_LEAF_NO_FP, \"CallLeafNoFP\");\n+    }\n+\n+    public static final String CALL_LEAF_NO_FP_OF_METHOD = COMPOSITE_PREFIX + \"CALL_LEAF_NO_FP_OF_METHOD\" + POSTFIX;\n+    static {\n+        callLeafNoFpOfMethodNodes(CALL_LEAF_NO_FP_OF_METHOD, IS_REPLACED);\n+    }\n+\n+    public static void callLeafNoFpOfMethodNodes(String irNodePlaceholder, String calleeRegex) {\n+        callOfNodes(irNodePlaceholder, \"CallLeafNoFP\", calleeRegex);\n@@ -588,0 +626,5 @@\n+    public static final String CMP_N = PREFIX + \"CMP_N\" + POSTFIX;\n+    static {\n+        beforeMatchingNameRegex(CMP_N, \"CmpN\");\n+    }\n+\n@@ -742,1 +785,1 @@\n-        callOfNodes(DYNAMIC_CALL_OF_METHOD, \"CallDynamicJava\");\n+        callOfNodes(DYNAMIC_CALL_OF_METHOD, \"CallDynamicJava\", IS_REPLACED);\n@@ -878,0 +921,5 @@\n+    public static final String INLINE_TYPE = PREFIX + \"INLINE_TYPE\" + POSTFIX;\n+    static {\n+        beforeMatchingNameRegex(INLINE_TYPE, \"InlineType\");\n+    }\n+\n@@ -924,1 +972,5 @@\n-        loadOfNodes(LOAD_OF_CLASS, \"Load(B|UB|S|US|I|L|F|D|P|N)\");\n+        anyLoadOfNodes(LOAD_OF_CLASS, IS_REPLACED);\n+    }\n+\n+    public static void anyLoadOfNodes(String irNodePlaceholder, String fieldHolder) {\n+        loadOfNodes(irNodePlaceholder, \"Load(B|UB|S|US|I|L|F|D|P|N)\", fieldHolder);\n@@ -934,1 +986,1 @@\n-        loadOfNodes(LOAD_B_OF_CLASS, \"LoadB\");\n+        loadOfNodes(LOAD_B_OF_CLASS, \"LoadB\", IS_REPLACED);\n@@ -944,1 +996,1 @@\n-        loadOfNodes(LOAD_D_OF_CLASS, \"LoadD\");\n+        loadOfNodes(LOAD_D_OF_CLASS, \"LoadD\", IS_REPLACED);\n@@ -954,1 +1006,1 @@\n-        loadOfNodes(LOAD_F_OF_CLASS, \"LoadF\");\n+        loadOfNodes(LOAD_F_OF_CLASS, \"LoadF\", IS_REPLACED);\n@@ -964,1 +1016,1 @@\n-        loadOfNodes(LOAD_I_OF_CLASS, \"LoadI\");\n+        loadOfNodes(LOAD_I_OF_CLASS, \"LoadI\", IS_REPLACED);\n@@ -989,1 +1041,1 @@\n-        loadOfNodes(LOAD_L_OF_CLASS, \"LoadL\");\n+        loadOfNodes(LOAD_L_OF_CLASS, \"LoadL\", IS_REPLACED);\n@@ -999,1 +1051,1 @@\n-        loadOfNodes(LOAD_N_OF_CLASS, \"LoadN\");\n+        loadOfNodes(LOAD_N_OF_CLASS, \"LoadN\", IS_REPLACED);\n@@ -1015,1 +1067,1 @@\n-        loadOfNodes(LOAD_P_OF_CLASS, \"LoadP\");\n+        loadOfNodes(LOAD_P_OF_CLASS, \"LoadP\", IS_REPLACED);\n@@ -1025,1 +1077,1 @@\n-        loadOfNodes(LOAD_S_OF_CLASS, \"LoadS\");\n+        loadOfNodes(LOAD_S_OF_CLASS, \"LoadS\", IS_REPLACED);\n@@ -1035,1 +1087,1 @@\n-        loadOfNodes(LOAD_UB_OF_CLASS, \"LoadUB\");\n+        loadOfNodes(LOAD_UB_OF_CLASS, \"LoadUB\", IS_REPLACED);\n@@ -1045,1 +1097,1 @@\n-        loadOfNodes(LOAD_US_OF_CLASS, \"LoadUS\");\n+        loadOfNodes(LOAD_US_OF_CLASS, \"LoadUS\", IS_REPLACED);\n@@ -1959,1 +2011,1 @@\n-        storeOfNodes(STORE_B_OF_CLASS, \"StoreB\");\n+        storeOfNodes(STORE_B_OF_CLASS, \"StoreB\", IS_REPLACED);\n@@ -1969,1 +2021,1 @@\n-        storeOfNodes(STORE_C_OF_CLASS, \"StoreC\");\n+        storeOfNodes(STORE_C_OF_CLASS, \"StoreC\", IS_REPLACED);\n@@ -1979,1 +2031,1 @@\n-        storeOfNodes(STORE_D_OF_CLASS, \"StoreD\");\n+        storeOfNodes(STORE_D_OF_CLASS, \"StoreD\", IS_REPLACED);\n@@ -1989,1 +2041,1 @@\n-        storeOfNodes(STORE_F_OF_CLASS, \"StoreF\");\n+        storeOfNodes(STORE_F_OF_CLASS, \"StoreF\", IS_REPLACED);\n@@ -1999,1 +2051,1 @@\n-        storeOfNodes(STORE_I_OF_CLASS, \"StoreI\");\n+        storeOfNodes(STORE_I_OF_CLASS, \"StoreI\", IS_REPLACED);\n@@ -2009,1 +2061,1 @@\n-        storeOfNodes(STORE_L_OF_CLASS, \"StoreL\");\n+        storeOfNodes(STORE_L_OF_CLASS, \"StoreL\", IS_REPLACED);\n@@ -2019,1 +2071,1 @@\n-        storeOfNodes(STORE_N_OF_CLASS, \"StoreN\");\n+        storeOfNodes(STORE_N_OF_CLASS, \"StoreN\", IS_REPLACED);\n@@ -2024,1 +2076,5 @@\n-        storeOfNodes(STORE_OF_CLASS, \"Store(B|C|S|I|L|F|D|P|N)\");\n+        anyStoreOfNodes(STORE_OF_CLASS, IS_REPLACED);\n+    }\n+\n+    public static void anyStoreOfNodes(String irNodePlaceholder, String fieldHolder) {\n+        storeOfNodes(irNodePlaceholder, \"Store(B|C|S|I|L|F|D|P|N)\", fieldHolder);\n@@ -2040,1 +2096,1 @@\n-        storeOfNodes(STORE_P_OF_CLASS, \"StoreP\");\n+        storeOfNodes(STORE_P_OF_CLASS, \"StoreP\", IS_REPLACED);\n@@ -2140,1 +2196,2 @@\n-        beforeMatchingNameRegex(SUBTYPE_CHECK, \"SubTypeCheck\");\n+        String regex = START + \"SubTypeCheck\" + MID + END;\n+        macroNodes(SUBTYPE_CHECK, regex);\n@@ -3092,1 +3149,1 @@\n-    private static void beforeMatching(String irNodePlaceholder, String regex) {\n+    public static void beforeMatching(String irNodePlaceholder, String regex) {\n@@ -3128,2 +3185,2 @@\n-    private static void callOfNodes(String irNodePlaceholder, String callRegex) {\n-        String regex = START + callRegex + MID + IS_REPLACED + \" \" +  END;\n+    private static void callOfNodes(String irNodePlaceholder, String callRegex, String calleeRegex) {\n+        String regex = START + callRegex + MID + calleeRegex + END;\n@@ -3137,1 +3194,1 @@\n-    private static void optoOnly(String irNodePlaceholder, String regex) {\n+    public static void optoOnly(String irNodePlaceholder, String regex) {\n@@ -3232,2 +3289,2 @@\n-    private static void loadOfNodes(String irNodePlaceholder, String irNodeRegex) {\n-        String regex = START + irNodeRegex + MID + LOAD_STORE_PREFIX + IS_REPLACED + LOAD_STORE_SUFFIX + END;\n+    private static void loadOfNodes(String irNodePlaceholder, String irNodeRegex, String loadee) {\n+        String regex = START + irNodeRegex + MID + LOAD_STORE_PREFIX + loadee + LOAD_STORE_SUFFIX + END;\n@@ -3237,2 +3294,2 @@\n-    private static void storeOfNodes(String irNodePlaceholder, String irNodeRegex) {\n-        String regex = START + irNodeRegex + MID + LOAD_STORE_PREFIX + IS_REPLACED + LOAD_STORE_SUFFIX + END;\n+    private static void storeOfNodes(String irNodePlaceholder, String irNodeRegex, String storee) {\n+        String regex = START + irNodeRegex + MID + LOAD_STORE_PREFIX + storee + LOAD_STORE_SUFFIX + END;\n","filename":"test\/hotspot\/jtreg\/compiler\/lib\/ir_framework\/IRNode.java","additions":95,"deletions":38,"binary":false,"changes":133,"status":"modified"},{"patch":"@@ -28,0 +28,1 @@\n+ * @enablePreview\n","filename":"test\/jdk\/jdk\/classfile\/TransformTests.java","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"}]}