{"files":[{"patch":"@@ -96,1 +96,1 @@\n-JAVADOC_OPTIONS := -use -keywords -notimestamp \\\n+JAVADOC_OPTIONS := -XDignore.symbol.file=true -use -keywords -notimestamp \\\n@@ -99,0 +99,2 @@\n+    -XDenableValueTypes \\\n+    --enable-preview -source $(JDK_SOURCE_TARGET_VERSION) \\\n","filename":"make\/Docs.gmk","additions":3,"deletions":1,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -141,0 +141,1 @@\n+# Param3 - _valhalla, or empty\n@@ -148,2 +149,2 @@\n-  $1_$2_DUMP_EXTRA_ARG := $$($1_$2_COOPS_OPTION) $$($1_$2_COH_OPTION)\n-  $1_$2_DUMP_TYPE      := $(if $(findstring _nocoops, $2),-NOCOOPS,)$(if $(findstring _coh, $2),-COH,)\n+  $1_$2_$3_DUMP_EXTRA_ARG := $$($1_$2_COOPS_OPTION) $$($1_$2_COH_OPTION) $(if $(findstring _valhalla, $3), --enable-preview,)\n+  $1_$2_$3_DUMP_TYPE      := $(if $(findstring _nocoops, $2),-NOCOOPS,)$(if $(findstring _coh, $2),-COH,)$(if $(findstring _valhalla, $3), -VALHALLA,)\n@@ -151,1 +152,1 @@\n-  $1_$2_CDS_DUMP_FLAGS := $(CDS_DUMP_FLAGS) $(if $(filter g1gc, $(JVM_FEATURES_$1)), -XX:+UseG1GC)\n+  $1_$2_$3_CDS_DUMP_FLAGS := $(CDS_DUMP_FLAGS) $(if $(filter g1gc, $(JVM_FEATURES_$1)), -XX:+UseG1GC)\n@@ -154,1 +155,1 @@\n-    $1_$2_CDS_ARCHIVE := bin\/$1\/classes$2.jsa\n+    $1_$2_$3_CDS_ARCHIVE := bin\/$1\/classes$2$3.jsa\n@@ -156,1 +157,1 @@\n-    $1_$2_CDS_ARCHIVE := lib\/$1\/classes$2.jsa\n+    $1_$2_$3_CDS_ARCHIVE := lib\/$1\/classes$2$3.jsa\n@@ -167,3 +168,3 @@\n-  $$(eval $$(call SetupExecute, $1_$2_gen_cds_archive_jdk, \\\n-      WARN := Creating CDS$$($1_$2_DUMP_TYPE) archive for jdk image for $1, \\\n-      INFO := Using CDS flags for $1: $$($1_$2_CDS_DUMP_FLAGS), \\\n+  $$(eval $$(call SetupExecute, $1_$2_$3_gen_cds_archive_jdk, \\\n+      WARN := Creating CDS$$($1_$2_$3_DUMP_TYPE) archive for jdk image for $1, \\\n+      INFO := Using CDS flags for $1: $$($1_$2_$3_CDS_DUMP_FLAGS), \\\n@@ -171,1 +172,1 @@\n-      OUTPUT_FILE := $$(JDK_IMAGE_DIR)\/$$($1_$2_CDS_ARCHIVE), \\\n+      OUTPUT_FILE := $$(JDK_IMAGE_DIR)\/$$($1_$2_$3_CDS_ARCHIVE), \\\n@@ -174,2 +175,2 @@\n-          -XX:SharedArchiveFile=$$(JDK_IMAGE_DIR)\/$$($1_$2_CDS_ARCHIVE) \\\n-          -$1 $$($1_$2_DUMP_EXTRA_ARG) $$($1_$2_CDS_DUMP_FLAGS) $$(LOG_INFO), \\\n+          -XX:SharedArchiveFile=$$(JDK_IMAGE_DIR)\/$$($1_$2_$3_CDS_ARCHIVE) \\\n+          -$1 $$($1_$2_$3_DUMP_EXTRA_ARG) $$($1_$2_$3_CDS_DUMP_FLAGS) $$(LOG_INFO), \\\n@@ -178,1 +179,1 @@\n-  JDK_TARGETS += $$($1_$2_gen_cds_archive_jdk)\n+  JDK_TARGETS += $$($1_$2_$3_gen_cds_archive_jdk)\n@@ -180,3 +181,3 @@\n-  $$(eval $$(call SetupExecute, $1_$2_gen_cds_archive_jre, \\\n-      WARN := Creating CDS$$($1_$2_DUMP_TYPE) archive for jre image for $1, \\\n-      INFO := Using CDS flags for $1: $$($1_$2_CDS_DUMP_FLAGS), \\\n+  $$(eval $$(call SetupExecute, $1_$2_$3_gen_cds_archive_jre, \\\n+      WARN := Creating CDS$$($1_$2_$3_DUMP_TYPE) archive for jre image for $1, \\\n+      INFO := Using CDS flags for $1: $$($1_$2_$3_CDS_DUMP_FLAGS), \\\n@@ -184,1 +185,1 @@\n-      OUTPUT_FILE := $$(JRE_IMAGE_DIR)\/$$($1_$2_CDS_ARCHIVE), \\\n+      OUTPUT_FILE := $$(JRE_IMAGE_DIR)\/$$($1_$2_$3_CDS_ARCHIVE), \\\n@@ -187,2 +188,2 @@\n-          -XX:SharedArchiveFile=$$(JRE_IMAGE_DIR)\/$$($1_$2_CDS_ARCHIVE) \\\n-          -$1 $$($1_$2_DUMP_EXTRA_ARG) $$($1_$2_CDS_DUMP_FLAGS) $$(LOG_INFO), \\\n+          -XX:SharedArchiveFile=$$(JRE_IMAGE_DIR)\/$$($1_$2_$3_CDS_ARCHIVE) \\\n+          -$1 $$($1_$2_$3_DUMP_EXTRA_ARG) $$($1_$2_$3_CDS_DUMP_FLAGS) $$(LOG_INFO), \\\n@@ -191,1 +192,1 @@\n-  JRE_TARGETS += $$($1_$2_gen_cds_archive_jre)\n+  JRE_TARGETS += $$($1_$2_$3_gen_cds_archive_jre)\n@@ -196,1 +197,2 @@\n-    $(eval $(call CreateCDSArchive,$v,)) \\\n+    $(eval $(call CreateCDSArchive,$v,,)) \\\n+    $(eval $(call CreateCDSArchive,$v,,_valhalla)) \\\n@@ -201,1 +203,2 @@\n-      $(eval $(call CreateCDSArchive,$v,_nocoops)) \\\n+      $(eval $(call CreateCDSArchive,$v,_nocoops,)) \\\n+      $(eval $(call CreateCDSArchive,$v,_nocoops,_valhalla)) \\\n","filename":"make\/Images.gmk","additions":24,"deletions":21,"binary":false,"changes":45,"status":"modified"},{"patch":"@@ -1697,0 +1697,3 @@\n+  } else if (_entry_point == nullptr) {\n+    \/\/ See CallLeafNoFPIndirect\n+    return 1 * NativeInstruction::instruction_size;\n@@ -1805,12 +1808,1 @@\n-  \/\/ n.b. frame size includes space for return pc and rfp\n-  const int framesize = C->output()->frame_size_in_bytes();\n-\n-  if (C->clinit_barrier_on_entry()) {\n-    assert(!C->method()->holder()->is_not_initialized(), \"initialization should have been started\");\n-\n-    Label L_skip_barrier;\n-    __ mov_metadata(rscratch2, C->method()->holder()->constant_encoding());\n-    __ clinit_barrier(rscratch2, rscratch1, &L_skip_barrier);\n-    __ far_jump(RuntimeAddress(SharedRuntime::get_handle_wrong_method_stub()));\n-    __ bind(L_skip_barrier);\n-  }\n+  __ verified_entry(C, 0);\n@@ -1819,2 +1811,2 @@\n-  if (C->max_vector_size() > 0) {\n-    __ reinitialize_ptrue();\n+  if (C->stub_function() == nullptr) {\n+    __ entry_barrier();\n@@ -1823,25 +1815,2 @@\n-  int bangsize = C->output()->bang_size_in_bytes();\n-  if (C->output()->need_stack_bang(bangsize))\n-    __ generate_stack_overflow_check(bangsize);\n-\n-  __ build_frame(framesize);\n-\n-  if (C->stub_function() == nullptr) {\n-    BarrierSetAssembler* bs = BarrierSet::barrier_set()->barrier_set_assembler();\n-    \/\/ Dummy labels for just measuring the code size\n-    Label dummy_slow_path;\n-    Label dummy_continuation;\n-    Label dummy_guard;\n-    Label* slow_path = &dummy_slow_path;\n-    Label* continuation = &dummy_continuation;\n-    Label* guard = &dummy_guard;\n-    if (!Compile::current()->output()->in_scratch_emit_size()) {\n-      \/\/ Use real labels from actual stub when not emitting code for the purpose of measuring its size\n-      C2EntryBarrierStub* stub = new (Compile::current()->comp_arena()) C2EntryBarrierStub();\n-      Compile::current()->output()->add_stub(stub);\n-      slow_path = &stub->entry();\n-      continuation = &stub->continuation();\n-      guard = &stub->guard();\n-    }\n-    \/\/ In the C2 code, we move the non-hot part of nmethod entry barriers out-of-line to a stub.\n-    bs->nmethod_entry_barrier(masm, slow_path, continuation, guard);\n+  if (!Compile::current()->output()->in_scratch_emit_size()) {\n+    __ bind(*_verified_entry);\n@@ -1864,6 +1833,0 @@\n-uint MachPrologNode::size(PhaseRegAlloc* ra_) const\n-{\n-  return MachNode::size(ra_); \/\/ too many variables; just compute it\n-                              \/\/ the hard way\n-}\n-\n@@ -1912,1 +1875,1 @@\n-  __ remove_frame(framesize);\n+  __ remove_frame(framesize, C->needs_stack_repair());\n@@ -1931,5 +1894,0 @@\n-uint MachEpilogNode::size(PhaseRegAlloc *ra_) const {\n-  \/\/ Variable size. Determine dynamically.\n-  return MachNode::size(ra_);\n-}\n-\n@@ -2231,1 +2189,12 @@\n-\/\/=============================================================================\n+\/\/\/=============================================================================\n+#ifndef PRODUCT\n+void MachVEPNode::format(PhaseRegAlloc* ra_, outputStream* st) const\n+{\n+  st->print_cr(\"# MachVEPNode\");\n+  if (!_verified) {\n+    st->print_cr(\"\\t load_class\");\n+  } else {\n+    st->print_cr(\"\\t unpack_inline_arg\");\n+  }\n+}\n+#endif\n@@ -2233,0 +2202,27 @@\n+void MachVEPNode::emit(C2_MacroAssembler *masm, PhaseRegAlloc* ra_) const\n+{\n+  if (!_verified) {\n+    __ ic_check(1);\n+  } else {\n+    \/\/ TODO 8284443 Avoid creation of temporary frame\n+    if (ra_->C->stub_function() == nullptr) {\n+      __ verified_entry(ra_->C, 0);\n+      __ entry_barrier();\n+      int framesize = ra_->C->output()->frame_slots() << LogBytesPerInt;\n+      __ remove_frame(framesize, false);\n+    }\n+    \/\/ Unpack inline type args passed as oop and then jump to\n+    \/\/ the verified entry point (skipping the unverified entry).\n+    int sp_inc = __ unpack_inline_args(ra_->C, _receiver_only);\n+    \/\/ Emit code for verified entry and save increment for stack repair on return\n+    __ verified_entry(ra_->C, sp_inc);\n+    if (Compile::current()->output()->in_scratch_emit_size()) {\n+      Label dummy_verified_entry;\n+      __ b(dummy_verified_entry);\n+    } else {\n+      __ b(*_verified_entry);\n+    }\n+  }\n+}\n+\n+\/\/=============================================================================\n@@ -2255,5 +2251,0 @@\n-uint MachUEPNode::size(PhaseRegAlloc* ra_) const\n-{\n-  return MachNode::size(ra_);\n-}\n-\n@@ -3761,0 +3752,31 @@\n+    if (tf()->returns_inline_type_as_fields() && !_method->is_method_handle_intrinsic() && _method->return_type()->is_loaded()) {\n+      \/\/ The last return value is not set by the callee but used to pass the null marker to compiled code.\n+      \/\/ Search for the corresponding projection, get the register and emit code that initialized it.\n+      uint con = (tf()->range_cc()->cnt() - 1);\n+      for (DUIterator_Fast imax, i = fast_outs(imax); i < imax; i++) {\n+        ProjNode* proj = fast_out(i)->as_Proj();\n+        if (proj->_con == con) {\n+          \/\/ Set null marker if r0 is non-null (a non-null value is returned buffered or scalarized)\n+          OptoReg::Name optoReg = ra_->get_reg_first(proj);\n+          VMReg reg = OptoReg::as_VMReg(optoReg, ra_->_framesize, OptoReg::reg2stack(ra_->_matcher._new_SP));\n+          Register toReg = reg->is_reg() ? reg->as_Register() : rscratch1;\n+          __ cmp(r0, zr);\n+          __ cset(toReg, Assembler::NE);\n+          if (reg->is_stack()) {\n+            int st_off = reg->reg2stack() * VMRegImpl::stack_slot_size;\n+            __ str(toReg, Address(sp, st_off));\n+          }\n+          break;\n+        }\n+      }\n+      if (return_value_is_used()) {\n+        \/\/ An inline type is returned as fields in multiple registers.\n+        \/\/ R0 either contains an oop if the inline type is buffered or a pointer\n+        \/\/ to the corresponding InlineKlass with the lowest bit set to 1. Zero r0\n+        \/\/ if the lowest bit is set to allow C2 to use the oop after null checking.\n+        \/\/ r0 &= (r0 & 1) - 1\n+        __ andr(rscratch1, r0, 0x1);\n+        __ sub(rscratch1, rscratch1, 0x1);\n+        __ andr(r0, r0, rscratch1);\n+      }\n+    }\n@@ -6942,1 +6964,1 @@\n-    \"mov  $dst, $con\\t# ptr\\n\\t\"\n+    \"mov  $dst, $con\\t# ptr\"\n@@ -8135,0 +8157,30 @@\n+instruct castI2N(iRegNNoSp dst, iRegI src) %{\n+  match(Set dst (CastI2N src));\n+\n+  ins_cost(INSN_COST);\n+  format %{ \"mov $dst, $src\\t# int -> narrow ptr\" %}\n+\n+  ins_encode %{\n+    if ($dst$$reg != $src$$reg) {\n+      __ mov(as_Register($dst$$reg), as_Register($src$$reg));\n+    }\n+  %}\n+\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct castN2X(iRegLNoSp dst, iRegN src) %{\n+  match(Set dst (CastP2X src));\n+\n+  ins_cost(INSN_COST);\n+  format %{ \"mov $dst, $src\\t# ptr -> long\" %}\n+\n+  ins_encode %{\n+    if ($dst$$reg != $src$$reg) {\n+      __ mov(as_Register($dst$$reg), as_Register($src$$reg));\n+    }\n+  %}\n+\n+  ins_pipe(ialu_reg);\n+%}\n+\n@@ -15088,1 +15140,1 @@\n-instruct clearArray_reg_reg(iRegL_R11 cnt, iRegP_R10 base, Universe dummy, rFlagsReg cr)\n+instruct clearArray_reg_reg_immL0(iRegL_R11 cnt, iRegP_R10 base, immL0 zero, Universe dummy, rFlagsReg cr)\n@@ -15090,1 +15142,1 @@\n-  match(Set dummy (ClearArray cnt base));\n+  match(Set dummy (ClearArray (Binary cnt base) zero));\n@@ -15107,0 +15159,16 @@\n+instruct clearArray_reg_reg(iRegL_R11 cnt, iRegP_R10 base, iRegL val, Universe dummy, rFlagsReg cr)\n+%{\n+  predicate(((ClearArrayNode*)n)->word_copy_only());\n+  match(Set dummy (ClearArray (Binary cnt base) val));\n+  effect(USE_KILL cnt, USE_KILL base, KILL cr);\n+\n+  ins_cost(4 * INSN_COST);\n+  format %{ \"ClearArray $cnt, $base, $val\" %}\n+\n+  ins_encode %{\n+    __ fill_words($base$$Register, $cnt$$Register, $val$$Register);\n+  %}\n+\n+  ins_pipe(pipe_class_memory);\n+%}\n+\n@@ -15110,1 +15178,2 @@\n-            < (uint64_t)(BlockZeroingLowLimit >> LogBytesPerWord));\n+            < (uint64_t)(BlockZeroingLowLimit >> LogBytesPerWord)\n+            && !((ClearArrayNode*)n)->word_copy_only());\n@@ -16427,0 +16496,18 @@\n+\/\/ entry point is null, target holds the address to call\n+instruct CallLeafNoFPIndirect(iRegP target)\n+%{\n+  predicate(n->as_Call()->entry_point() == nullptr);\n+\n+  match(CallLeafNoFP target);\n+\n+  ins_cost(CALL_COST);\n+\n+  format %{ \"CALL, runtime leaf nofp indirect $target\" %}\n+\n+  ins_encode %{\n+    __ blr($target$$Register);\n+  %}\n+\n+  ins_pipe(pipe_class_call);\n+%}\n+\n@@ -16429,0 +16516,2 @@\n+  predicate(n->as_Call()->entry_point() != nullptr);\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/aarch64.ad","additions":150,"deletions":61,"binary":false,"changes":211,"status":"modified"},{"patch":"@@ -35,0 +35,1 @@\n+#include \"ci\/ciInlineKlass.hpp\"\n@@ -36,0 +37,1 @@\n+#include \"ci\/ciObjArrayKlass.hpp\"\n@@ -41,0 +43,1 @@\n+#include \"oops\/oop.inline.hpp\"\n@@ -429,1 +432,1 @@\n-  __ remove_frame(initial_frame_size_in_bytes());\n+  __ remove_frame(initial_frame_size_in_bytes(), needs_stack_repair());\n@@ -473,0 +476,42 @@\n+  if (InlineTypeReturnedAsFields) {\n+    \/\/ Check if we are returning an non-null inline type and load its fields into registers\n+    ciType* return_type = compilation()->method()->return_type();\n+    if (return_type->is_inlinetype()) {\n+      ciInlineKlass* vk = return_type->as_inline_klass();\n+      if (vk->can_be_returned_as_fields()) {\n+        address unpack_handler = vk->unpack_handler();\n+        assert(unpack_handler != nullptr, \"must be\");\n+        __ far_call(RuntimeAddress(unpack_handler));\n+      }\n+    } else if (return_type->is_instance_klass() && (!return_type->is_loaded() || StressCallingConvention)) {\n+      Label skip;\n+      Label not_null;\n+      __ cbnz(r0, not_null);\n+      \/\/ Returned value is null, zero all return registers because they may belong to oop fields\n+      __ mov(j_rarg1, zr);\n+      __ mov(j_rarg2, zr);\n+      __ mov(j_rarg3, zr);\n+      __ mov(j_rarg4, zr);\n+      __ mov(j_rarg5, zr);\n+      __ mov(j_rarg6, zr);\n+      __ mov(j_rarg7, zr);\n+      __ b(skip);\n+      __ bind(not_null);\n+\n+      \/\/ Check if we are returning an non-null inline type and load its fields into registers\n+      __ test_oop_is_not_inline_type(r0, rscratch2, skip, \/* can_be_null= *\/ false);\n+\n+      \/\/ Load fields from a buffered value with an inline class specific handler\n+      __ load_klass(rscratch1 \/*dst*\/, r0 \/*src*\/);\n+      __ ldr(rscratch1, Address(rscratch1, InstanceKlass::adr_inlineklass_fixed_block_offset()));\n+      __ ldr(rscratch1, Address(rscratch1, InlineKlass::unpack_handler_offset()));\n+      \/\/ Unpack handler can be null if inline type is not scalarizable in returns\n+      __ cbz(rscratch1, skip);\n+      __ blr(rscratch1);\n+\n+      __ bind(skip);\n+    }\n+    \/\/ At this point, r0 points to the value object (for interpreter or C1 caller).\n+    \/\/ The fields of the object are copied into registers (for C2 caller).\n+  }\n+\n@@ -474,1 +519,1 @@\n-  __ remove_frame(initial_frame_size_in_bytes());\n+  __ remove_frame(initial_frame_size_in_bytes(), needs_stack_repair());\n@@ -486,0 +531,4 @@\n+int LIR_Assembler::store_inline_type_fields_to_buf(ciInlineKlass* vk) {\n+  return (__ store_inline_type_fields_to_buf(vk, false));\n+}\n+\n@@ -532,3 +581,1 @@\n-        if (patch_code == lir_patch_none) {\n-          jobject2reg(c->as_jobject(), dest->as_register());\n-        } else {\n+        if (patch_code != lir_patch_none) {\n@@ -536,0 +583,2 @@\n+        } else {\n+          jobject2reg(c->as_jobject(), dest->as_register());\n@@ -645,0 +694,2 @@\n+    \/\/ Non-null case is not handled on aarch64 but handled on x86\n+    \/\/ FIXME: do we need to add it here?\n@@ -992,0 +1043,14 @@\n+void LIR_Assembler::move(LIR_Opr src, LIR_Opr dst) {\n+  assert(dst->is_cpu_register(), \"must be\");\n+  assert(dst->type() == src->type(), \"must be\");\n+\n+  if (src->is_cpu_register()) {\n+    reg2reg(src, dst);\n+  } else if (src->is_stack()) {\n+    stack2reg(src, dst, dst->type());\n+  } else if (src->is_constant()) {\n+    const2reg(src, dst, lir_patch_none, nullptr);\n+  } else {\n+    ShouldNotReachHere();\n+  }\n+}\n@@ -1183,1 +1248,1 @@\n-  if (UseSlowPath ||\n+  if (UseSlowPath || op->always_slow_path() ||\n@@ -1295,22 +1360,16 @@\n-  if (should_profile) {\n-    Register mdo  = klass_RInfo;\n-    __ mov_metadata(mdo, md->constant_encoding());\n-    Label not_null;\n-    __ cbnz(obj, not_null);\n-    \/\/ Object is null; update MDO and exit\n-    Address data_addr\n-      = __ form_address(rscratch2, mdo,\n-                        md->byte_offset_of_slot(data, DataLayout::flags_offset()),\n-                        0);\n-    __ ldrb(rscratch1, data_addr);\n-    __ orr(rscratch1, rscratch1, BitData::null_seen_byte_constant());\n-    __ strb(rscratch1, data_addr);\n-    __ b(*obj_is_null);\n-    __ bind(not_null);\n-\n-    Label update_done;\n-    Register recv = k_RInfo;\n-    __ load_klass(recv, obj);\n-    type_profile_helper(mdo, md, data, recv, &update_done);\n-    Address counter_addr(mdo, md->byte_offset_of_slot(data, CounterData::count_offset()));\n-    __ addptr(counter_addr, DataLayout::counter_increment);\n+  if (op->need_null_check()) {\n+    if (should_profile) {\n+      Register mdo  = klass_RInfo;\n+      __ mov_metadata(mdo, md->constant_encoding());\n+      Label not_null;\n+      __ cbnz(obj, not_null);\n+      \/\/ Object is null; update MDO and exit\n+      Address data_addr\n+        = __ form_address(rscratch2, mdo,\n+                          md->byte_offset_of_slot(data, DataLayout::flags_offset()),\n+                          0);\n+      __ ldrb(rscratch1, data_addr);\n+      __ orr(rscratch1, rscratch1, BitData::null_seen_byte_constant());\n+      __ strb(rscratch1, data_addr);\n+      __ b(*obj_is_null);\n+      __ bind(not_null);\n@@ -1318,3 +1377,11 @@\n-    __ bind(update_done);\n-  } else {\n-    __ cbz(obj, *obj_is_null);\n+      Label update_done;\n+      Register recv = k_RInfo;\n+      __ load_klass(recv, obj);\n+      type_profile_helper(mdo, md, data, recv, &update_done);\n+      Address counter_addr(mdo, md->byte_offset_of_slot(data, CounterData::count_offset()));\n+      __ addptr(counter_addr, DataLayout::counter_increment);\n+\n+      __ bind(update_done);\n+    } else {\n+      __ cbz(obj, *obj_is_null);\n+    }\n@@ -1331,0 +1398,1 @@\n+    assert(!k->is_loaded() || !k->is_obj_array_klass(), \"Use refined array for a direct pointer comparison\");\n@@ -1353,1 +1421,8 @@\n-        __ cmp(klass_RInfo, k_RInfo);\n+        if (k->is_loaded() && k->is_obj_array_klass()) {\n+          \/\/ For a direct pointer comparison, we need the refined array klass pointer\n+          ciKlass* k_refined = ciObjArrayKlass::make(k->as_obj_array_klass()->element_klass());\n+          __ mov_metadata(rscratch1, k_refined->constant_encoding());\n+          __ cmp(klass_RInfo, rscratch1);\n+        } else {\n+          __ cmp(klass_RInfo, k_RInfo);\n+        }\n@@ -1478,0 +1553,106 @@\n+void LIR_Assembler::emit_opFlattenedArrayCheck(LIR_OpFlattenedArrayCheck* op) {\n+  \/\/ We are loading\/storing from\/to an array that *may* be a flat array (the\n+  \/\/ declared type is Object[], abstract[], interface[] or VT.ref[]).\n+  \/\/ If this array is a flat array, take the slow path.\n+  __ test_flat_array_oop(op->array()->as_register(), op->tmp()->as_register(), *op->stub()->entry());\n+  if (!op->value()->is_illegal()) {\n+    \/\/ The array is not a flat array, but it might be null-free. If we are storing\n+    \/\/ a null into a null-free array, take the slow path (which will throw NPE).\n+    Label skip;\n+    __ cbnz(op->value()->as_register(), skip);\n+    __ test_null_free_array_oop(op->array()->as_register(), op->tmp()->as_register(), *op->stub()->entry());\n+    __ bind(skip);\n+  }\n+}\n+\n+void LIR_Assembler::emit_opNullFreeArrayCheck(LIR_OpNullFreeArrayCheck* op) {\n+  \/\/ We are storing into an array that *may* be null-free (the declared type is\n+  \/\/ Object[], abstract[], interface[] or VT.ref[]).\n+  Label test_mark_word;\n+  Register tmp = op->tmp()->as_register();\n+  __ ldr(tmp, Address(op->array()->as_register(), oopDesc::mark_offset_in_bytes()));\n+  __ tst(tmp, markWord::unlocked_value);\n+  __ br(Assembler::NE, test_mark_word);\n+  __ load_prototype_header(tmp, op->array()->as_register());\n+  __ bind(test_mark_word);\n+  __ tst(tmp, markWord::null_free_array_bit_in_place);\n+}\n+\n+void LIR_Assembler::emit_opSubstitutabilityCheck(LIR_OpSubstitutabilityCheck* op) {\n+  Label L_oops_equal;\n+  Label L_oops_not_equal;\n+  Label L_end;\n+\n+  Register left  = op->left()->as_register();\n+  Register right = op->right()->as_register();\n+\n+  __ cmp(left, right);\n+  __ br(Assembler::EQ, L_oops_equal);\n+\n+  \/\/ (1) Null check -- if one of the operands is null, the other must not be null (because\n+  \/\/     the two references are not equal), so they are not substitutable,\n+  \/\/     FIXME: do null check only if the operand is nullable\n+  {\n+    __ cbz(left, L_oops_not_equal);\n+    __ cbz(right, L_oops_not_equal);\n+  }\n+\n+  ciKlass* left_klass = op->left_klass();\n+  ciKlass* right_klass = op->right_klass();\n+\n+  \/\/ (2) Inline type check -- if either of the operands is not a inline type,\n+  \/\/     they are not substitutable. We do this only if we are not sure that the\n+  \/\/     operands are inline type\n+  if ((left_klass == nullptr || right_klass == nullptr) ||\/\/ The klass is still unloaded, or came from a Phi node.\n+      !left_klass->is_inlinetype() || !right_klass->is_inlinetype()) {\n+    Register tmp1  = op->tmp1()->as_register();\n+    __ mov(tmp1, markWord::inline_type_pattern);\n+    __ ldr(rscratch1, Address(left, oopDesc::mark_offset_in_bytes()));\n+    __ andr(tmp1, tmp1, rscratch1);\n+    __ ldr(rscratch1, Address(right, oopDesc::mark_offset_in_bytes()));\n+    __ andr(tmp1, tmp1, rscratch1);\n+    __ cmp(tmp1, (u1)markWord::inline_type_pattern);\n+    __ br(Assembler::NE, L_oops_not_equal);\n+  }\n+\n+  \/\/ (3) Same klass check: if the operands are of different klasses, they are not substitutable.\n+  if (left_klass != nullptr && left_klass->is_inlinetype() && left_klass == right_klass) {\n+    \/\/ No need to load klass -- the operands are statically known to be the same inline klass.\n+    __ b(*op->stub()->entry());\n+  } else {\n+    Register left_klass_op = op->left_klass_op()->as_register();\n+    Register right_klass_op = op->right_klass_op()->as_register();\n+\n+    if (UseCompressedClassPointers) {\n+      __ ldrw(left_klass_op,  Address(left,  oopDesc::klass_offset_in_bytes()));\n+      __ ldrw(right_klass_op, Address(right, oopDesc::klass_offset_in_bytes()));\n+      __ cmpw(left_klass_op, right_klass_op);\n+    } else {\n+      __ ldr(left_klass_op,  Address(left,  oopDesc::klass_offset_in_bytes()));\n+      __ ldr(right_klass_op, Address(right, oopDesc::klass_offset_in_bytes()));\n+      __ cmp(left_klass_op, right_klass_op);\n+    }\n+\n+    __ br(Assembler::EQ, *op->stub()->entry()); \/\/ same klass -> do slow check\n+    \/\/ fall through to L_oops_not_equal\n+  }\n+\n+  __ bind(L_oops_not_equal);\n+  move(op->not_equal_result(), op->result_opr());\n+  __ b(L_end);\n+\n+  __ bind(L_oops_equal);\n+  move(op->equal_result(), op->result_opr());\n+  __ b(L_end);\n+\n+  \/\/ We've returned from the stub. R0 contains 0x0 IFF the two\n+  \/\/ operands are not substitutable. (Don't compare against 0x1 in case the\n+  \/\/ C compiler is naughty)\n+  __ bind(*op->stub()->continuation());\n+  __ cbz(r0, L_oops_not_equal); \/\/ (call_stub() == 0x0) -> not_equal\n+  move(op->equal_result(), op->result_opr()); \/\/ (call_stub() != 0x0) -> equal\n+  \/\/ fall-through\n+  __ bind(L_end);\n+}\n+\n+\n@@ -1991,1 +2172,1 @@\n-  add_call_info(code_offset(), op->info());\n+  add_call_info(code_offset(), op->info(), op->maybe_return_as_fields());\n@@ -2002,1 +2183,1 @@\n-  add_call_info(code_offset(), op->info());\n+  add_call_info(code_offset(), op->info(), op->maybe_return_as_fields());\n@@ -2165,0 +2346,12 @@\n+void LIR_Assembler::arraycopy_inlinetype_check(Register obj, Register tmp, CodeStub* slow_path, bool is_dest, bool null_check) {\n+  if (null_check) {\n+    __ cbz(obj, *slow_path->entry());\n+  }\n+  if (is_dest) {\n+    __ test_null_free_array_oop(obj, tmp, *slow_path->entry());\n+    \/\/ TODO 8350865 Flat no longer implies null-free, so we need to check for flat dest. Can we do better here?\n+    __ test_flat_array_oop(obj, tmp, *slow_path->entry());\n+  } else {\n+    __ test_flat_array_oop(obj, tmp, *slow_path->entry());\n+  }\n+}\n@@ -2183,0 +2376,6 @@\n+  if (flags & LIR_OpArrayCopy::always_slow_path) {\n+    __ b(*stub->entry());\n+    __ bind(*stub->continuation());\n+    return;\n+  }\n+\n@@ -2236,0 +2435,8 @@\n+  \/\/ Handle inline type arrays\n+  if (flags & LIR_OpArrayCopy::src_inlinetype_check) {\n+    arraycopy_inlinetype_check(src, tmp, stub, false, (flags & LIR_OpArrayCopy::src_null_check));\n+  }\n+  if (flags & LIR_OpArrayCopy::dst_inlinetype_check) {\n+    arraycopy_inlinetype_check(dst, tmp, stub, true, (flags & LIR_OpArrayCopy::dst_null_check));\n+  }\n+\n@@ -2777,0 +2984,20 @@\n+void LIR_Assembler::emit_profile_inline_type(LIR_OpProfileInlineType* op) {\n+  Register obj = op->obj()->as_register();\n+  Register tmp = op->tmp()->as_pointer_register();\n+  bool not_null = op->not_null();\n+  int flag = op->flag();\n+\n+  Label not_inline_type;\n+  if (!not_null) {\n+    __ cbz(obj, not_inline_type);\n+  }\n+\n+  __ test_oop_is_not_inline_type(obj, tmp, not_inline_type);\n+\n+  Address mdo_addr = as_Address(op->mdp()->as_address_ptr(), rscratch2);\n+  __ ldrb(rscratch1, mdo_addr);\n+  __ orr(rscratch1, rscratch1, flag);\n+  __ strb(rscratch1, mdo_addr);\n+\n+  __ bind(not_inline_type);\n+}\n@@ -2916,0 +3143,4 @@\n+void LIR_Assembler::check_orig_pc() {\n+  __ ldr(rscratch2, frame_map()->address_for_orig_pc_addr());\n+  __ cmp(rscratch2, (u1)NULL_WORD);\n+}\n","filename":"src\/hotspot\/cpu\/aarch64\/c1_LIRAssembler_aarch64.cpp","additions":265,"deletions":34,"binary":false,"changes":299,"status":"modified"},{"patch":"@@ -77,0 +77,3 @@\n+  void arraycopy_inlinetype_check(Register obj, Register tmp, CodeStub* slow_path, bool is_dest, bool null_check);\n+  void move(LIR_Opr src, LIR_Opr dst);\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/c1_LIRAssembler_aarch64.hpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -30,0 +30,2 @@\n+#include \"gc\/shared\/barrierSet.hpp\"\n+#include \"gc\/shared\/barrierSetAssembler.hpp\"\n@@ -102,1 +104,4 @@\n-  if (UseCompactObjectHeaders) {\n+  if (UseCompactObjectHeaders || EnableValhalla) {\n+    \/\/ COH: Markword contains class pointer which is only known at runtime.\n+    \/\/ Valhalla: Could have value class which has a different prototype header to a normal object.\n+    \/\/ In both cases, we need to fetch dynamically.\n@@ -106,0 +111,1 @@\n+    \/\/ Otherwise: Can use the statically computed prototype header which is the same for every object.\n@@ -108,0 +114,5 @@\n+  }\n+\n+  if (!UseCompactObjectHeaders) {\n+    \/\/ COH: Markword already contains class pointer. Nothing else to do.\n+    \/\/ Otherwise: Fetch klass pointer following the markword\n@@ -244,2 +255,13 @@\n-void C1_MacroAssembler::build_frame(int framesize, int bang_size_in_bytes) {\n-  assert(bang_size_in_bytes >= framesize, \"stack bang size incorrect\");\n+void C1_MacroAssembler::build_frame_helper(int frame_size_in_bytes, int sp_offset_for_orig_pc, int sp_inc, bool reset_orig_pc, bool needs_stack_repair) {\n+  MacroAssembler::build_frame(frame_size_in_bytes);\n+\n+  if (needs_stack_repair) {\n+    save_stack_increment(sp_inc, frame_size_in_bytes);\n+  }\n+  if (reset_orig_pc) {\n+    \/\/ Zero orig_pc to detect deoptimization during buffering in the entry points\n+    str(zr, Address(sp, sp_offset_for_orig_pc));\n+  }\n+}\n+\n+void C1_MacroAssembler::build_frame(int frame_size_in_bytes, int bang_size_in_bytes, int sp_offset_for_orig_pc, bool needs_stack_repair, bool has_scalarized_args, Label* verified_inline_entry_label) {\n@@ -248,0 +270,1 @@\n+  assert(bang_size_in_bytes >= frame_size_in_bytes, \"stack bang size incorrect\");\n@@ -249,1 +272,2 @@\n-  MacroAssembler::build_frame(framesize);\n+\n+  build_frame_helper(frame_size_in_bytes, sp_offset_for_orig_pc, 0, has_scalarized_args, needs_stack_repair);\n@@ -254,3 +278,4 @@\n-}\n-void C1_MacroAssembler::remove_frame(int framesize) {\n-  MacroAssembler::remove_frame(framesize);\n+  if (verified_inline_entry_label != nullptr) {\n+    \/\/ Jump here from the scalarized entry points that already created the frame.\n+    bind(*verified_inline_entry_label);\n+  }\n@@ -260,1 +285,0 @@\n-\n@@ -267,0 +291,1 @@\n+  if (C1Breakpoint) brk(1);\n@@ -269,0 +294,62 @@\n+int C1_MacroAssembler::scalarized_entry(const CompiledEntrySignature* ces, int frame_size_in_bytes, int bang_size_in_bytes, int sp_offset_for_orig_pc, Label& verified_inline_entry_label, bool is_inline_ro_entry) {\n+  assert(InlineTypePassFieldsAsArgs, \"sanity\");\n+  \/\/ Make sure there is enough stack space for this method's activation.\n+  assert(bang_size_in_bytes >= frame_size_in_bytes, \"stack bang size incorrect\");\n+  generate_stack_overflow_check(bang_size_in_bytes);\n+\n+  GrowableArray<SigEntry>* sig    = ces->sig();\n+  GrowableArray<SigEntry>* sig_cc = is_inline_ro_entry ? ces->sig_cc_ro() : ces->sig_cc();\n+  VMRegPair* regs      = ces->regs();\n+  VMRegPair* regs_cc   = is_inline_ro_entry ? ces->regs_cc_ro() : ces->regs_cc();\n+  int args_on_stack    = ces->args_on_stack();\n+  int args_on_stack_cc = is_inline_ro_entry ? ces->args_on_stack_cc_ro() : ces->args_on_stack_cc();\n+\n+  assert(sig->length() <= sig_cc->length(), \"Zero-sized inline class not allowed!\");\n+  BasicType* sig_bt = NEW_RESOURCE_ARRAY(BasicType, sig_cc->length());\n+  int args_passed = sig->length();\n+  int args_passed_cc = SigEntry::fill_sig_bt(sig_cc, sig_bt);\n+\n+  \/\/ Create a temp frame so we can call into the runtime. It must be properly set up to accommodate GC.\n+  build_frame_helper(frame_size_in_bytes, sp_offset_for_orig_pc, 0, true, ces->c1_needs_stack_repair());\n+\n+  \/\/ The runtime call might safepoint, make sure nmethod entry barrier is executed\n+  BarrierSetAssembler* bs = BarrierSet::barrier_set()->barrier_set_assembler();\n+  \/\/ C1 code is not hot enough to micro optimize the nmethod entry barrier with an out-of-line stub\n+  bs->nmethod_entry_barrier(this, nullptr \/* slow_path *\/, nullptr \/* continuation *\/, nullptr \/* guard *\/);\n+\n+  \/\/ FIXME -- call runtime only if we cannot in-line allocate all the incoming inline type args.\n+  mov(r19, (intptr_t) ces->method());\n+  if (is_inline_ro_entry) {\n+    far_call(RuntimeAddress(Runtime1::entry_for(StubId::c1_buffer_inline_args_no_receiver_id)));\n+  } else {\n+    far_call(RuntimeAddress(Runtime1::entry_for(StubId::c1_buffer_inline_args_id)));\n+  }\n+  int rt_call_offset = offset();\n+\n+  \/\/ The runtime call returns the new array in r20 instead of the usual r0\n+  \/\/ because r0 is also j_rarg7 which may be holding a live argument here.\n+  Register val_array = r20;\n+\n+  \/\/ Remove the temp frame\n+  MacroAssembler::remove_frame(frame_size_in_bytes);\n+\n+  \/\/ Check if we need to extend the stack for packing\n+  int sp_inc = 0;\n+  if (args_on_stack > args_on_stack_cc) {\n+    sp_inc = extend_stack_for_inline_args(args_on_stack);\n+  }\n+\n+  shuffle_inline_args(true, is_inline_ro_entry, sig_cc,\n+                      args_passed_cc, args_on_stack_cc, regs_cc, \/\/ from\n+                      args_passed, args_on_stack, regs,          \/\/ to\n+                      sp_inc, val_array);\n+\n+  \/\/ Create the real frame. Below jump will then skip over the stack banging and frame\n+  \/\/ setup code in the verified_inline_entry (which has a different real_frame_size).\n+  build_frame_helper(frame_size_in_bytes, sp_offset_for_orig_pc, sp_inc, false, ces->c1_needs_stack_repair());\n+\n+  b(verified_inline_entry_label);\n+  return rt_call_offset;\n+}\n+\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/c1_MacroAssembler_aarch64.cpp","additions":95,"deletions":8,"binary":false,"changes":103,"status":"modified"},{"patch":"@@ -49,0 +49,21 @@\n+void C2_MacroAssembler::entry_barrier() {\n+  BarrierSetAssembler* bs = BarrierSet::barrier_set()->barrier_set_assembler();\n+  \/\/ Dummy labels for just measuring the code size\n+  Label dummy_slow_path;\n+  Label dummy_continuation;\n+  Label dummy_guard;\n+  Label* slow_path = &dummy_slow_path;\n+  Label* continuation = &dummy_continuation;\n+  Label* guard = &dummy_guard;\n+  if (!Compile::current()->output()->in_scratch_emit_size()) {\n+    \/\/ Use real labels from actual stub when not emitting code for the purpose of measuring its size\n+    C2EntryBarrierStub* stub = new (Compile::current()->comp_arena()) C2EntryBarrierStub();\n+    Compile::current()->output()->add_stub(stub);\n+    slow_path = &stub->entry();\n+    continuation = &stub->continuation();\n+    guard = &stub->guard();\n+  }\n+  \/\/ In the C2 code, we move the non-hot part of nmethod entry barriers out-of-line to a stub.\n+  bs->nmethod_entry_barrier(this, slow_path, continuation, guard);\n+}\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/c2_MacroAssembler_aarch64.cpp","additions":21,"deletions":0,"binary":false,"changes":21,"status":"modified"},{"patch":"@@ -47,0 +47,2 @@\n+  void entry_barrier();\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/c2_MacroAssembler_aarch64.hpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -134,0 +134,1 @@\n+  assert_different_registers(value, temp1, temp2);\n@@ -207,1 +208,13 @@\n-  __ push_call_clobbered_registers();\n+  \/\/ save the live input values\n+  RegSet saved = RegSet::of(pre_val);\n+  FloatRegSet fsaved;\n+\n+  \/\/ Barriers might be emitted when converting between (scalarized) calling\n+  \/\/ conventions for inline types. Save all argument registers before calling\n+  \/\/ into the runtime.\n+\n+  \/\/ TODO 8366717 This came with 8284161: Implementation of Virtual Threads (Preview) later in May 2022\n+  \/\/ Check if it's sufficient\n+  \/\/__ push_call_clobbered_registers();\n+  assert_different_registers(rscratch1, pre_val); \/\/ push_CPU_state trashes rscratch1\n+  __ push_CPU_state(true);\n@@ -228,1 +241,1 @@\n-  __ pop_call_clobbered_registers();\n+  __ pop_CPU_state(true);\n@@ -369,0 +382,10 @@\n+\n+  bool in_heap = (decorators & IN_HEAP) != 0;\n+  bool as_normal = (decorators & AS_NORMAL) != 0;\n+  bool dest_uninitialized = (decorators & IS_DEST_UNINITIALIZED) != 0;\n+\n+  bool needs_pre_barrier = as_normal && !dest_uninitialized;\n+  bool needs_post_barrier = (val != noreg && in_heap);\n+\n+  assert_different_registers(val, tmp1, tmp2, tmp3);\n+\n@@ -378,8 +401,10 @@\n-  g1_write_barrier_pre(masm,\n-                       tmp3 \/* obj *\/,\n-                       tmp2 \/* pre_val *\/,\n-                       rthread \/* thread *\/,\n-                       tmp1  \/* tmp1 *\/,\n-                       rscratch2  \/* tmp2 *\/,\n-                       val != noreg \/* tosca_live *\/,\n-                       false \/* expand_call *\/);\n+  if (needs_pre_barrier) {\n+    g1_write_barrier_pre(masm,\n+                         tmp3 \/* obj *\/,\n+                         tmp2 \/* pre_val *\/,\n+                         rthread \/* thread *\/,\n+                         tmp1  \/* tmp1 *\/,\n+                         rscratch2  \/* tmp2 *\/,\n+                         val != noreg \/* tosca_live *\/,\n+                         false \/* expand_call *\/);\n+  }\n@@ -392,3 +417,5 @@\n-    if (UseCompressedOops) {\n-      new_val = rscratch2;\n-      __ mov(new_val, val);\n+    if (needs_post_barrier) {\n+      if (UseCompressedOops) {\n+        new_val = rscratch2;\n+        __ mov(new_val, val);\n+      }\n@@ -396,0 +423,1 @@\n+\n@@ -397,6 +425,8 @@\n-    g1_write_barrier_post(masm,\n-                          tmp3 \/* store_adr *\/,\n-                          new_val \/* new_val *\/,\n-                          rthread \/* thread *\/,\n-                          tmp1 \/* tmp1 *\/,\n-                          tmp2 \/* tmp2 *\/);\n+    if (needs_post_barrier) {\n+      g1_write_barrier_post(masm,\n+                            tmp3 \/* store_adr *\/,\n+                            new_val \/* new_val *\/,\n+                            rthread \/* thread *\/,\n+                            tmp1 \/* tmp1 *\/,\n+                            tmp2 \/* tmp2 *\/);\n+    }\n","filename":"src\/hotspot\/cpu\/aarch64\/gc\/g1\/g1BarrierSetAssembler_aarch64.cpp","additions":49,"deletions":19,"binary":false,"changes":68,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2018, 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2018, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -102,0 +102,3 @@\n+  virtual void flat_field_copy(MacroAssembler* masm, DecoratorSet decorators,\n+                          Register src, Register dst, Register inline_layout_info);\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/gc\/shared\/barrierSetAssembler_aarch64.hpp","additions":4,"deletions":1,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -111,0 +111,1 @@\n+\n@@ -115,1 +116,9 @@\n-      store_check(masm, dst.base(), dst);\n+      if (tmp3 != noreg) {\n+        \/\/ TODO 8366717 This change is from before the 'tmp3' arg was added to mainline, check if it's still needed. Same on x64. Also, this should be a __ lea\n+        \/\/ Called by MacroAssembler::pack_inline_helper. We cannot corrupt the dst.base() register\n+        __ mov(tmp3, dst.base());\n+        store_check(masm, tmp3, dst);\n+      } else {\n+        \/\/ It's OK to corrupt the dst.base() register.\n+        store_check(masm, dst.base(), dst);\n+      }\n","filename":"src\/hotspot\/cpu\/aarch64\/gc\/shared\/cardTableBarrierSetAssembler_aarch64.cpp","additions":10,"deletions":1,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -35,0 +35,1 @@\n+#include \"oops\/constMethodFlags.hpp\"\n@@ -38,0 +39,1 @@\n+#include \"oops\/inlineKlass.hpp\"\n@@ -211,0 +213,86 @@\n+void InterpreterMacroAssembler::allocate_instance(Register klass, Register new_obj,\n+                                                  Register t1, Register t2,\n+                                                  bool clear_fields, Label& alloc_failed) {\n+  MacroAssembler::allocate_instance(klass, new_obj, t1, t2, clear_fields, alloc_failed);\n+  if (DTraceAllocProbes) {\n+    \/\/ Trigger dtrace event for fastpath\n+    push(atos);\n+    call_VM_leaf(CAST_FROM_FN_PTR(address, static_cast<int (*)(oopDesc*)>(SharedRuntime::dtrace_object_alloc)), new_obj);\n+    pop(atos);\n+  }\n+}\n+\n+void InterpreterMacroAssembler::read_flat_field(Register entry,\n+                                                Register field_index, Register field_offset,\n+                                                Register temp, Register obj) {\n+  Label failed_alloc, slow_path, done;\n+  const Register src = field_offset;\n+  const Register alloc_temp = r10;\n+  const Register dst_temp   = field_index;\n+  const Register layout_info = temp;\n+  assert_different_registers(obj, entry, field_index, field_offset, temp, alloc_temp, rscratch1);\n+\n+  load_unsigned_byte(temp, Address(entry, in_bytes(ResolvedFieldEntry::flags_offset())));\n+  \/\/ If the field is nullable, jump to slow path\n+  tbz(temp, ResolvedFieldEntry::is_null_free_inline_type_shift, slow_path);\n+\n+  \/\/ Grab the inline field klass\n+  ldr(rscratch1, Address(entry, in_bytes(ResolvedFieldEntry::field_holder_offset())));\n+  inline_layout_info(rscratch1, field_index, layout_info);\n+\n+  const Register field_klass = dst_temp;\n+  ldr(field_klass, Address(layout_info, in_bytes(InlineLayoutInfo::klass_offset())));\n+\n+  \/\/ allocate buffer\n+  push(obj); \/\/ save holder\n+  allocate_instance(field_klass, obj, alloc_temp, rscratch2, false, failed_alloc);\n+\n+  \/\/ Have an oop instance buffer, copy into it\n+  payload_address(obj, dst_temp, field_klass);  \/\/ danger, uses rscratch1\n+  pop(alloc_temp);             \/\/ restore holder\n+  lea(src, Address(alloc_temp, field_offset));\n+  \/\/ call_VM_leaf, clobbers a few regs, save restore new obj\n+  push(obj);\n+  flat_field_copy(IS_DEST_UNINITIALIZED, src, dst_temp, layout_info);\n+  pop(obj);\n+  b(done);\n+\n+  bind(failed_alloc);\n+  pop(obj);\n+  bind(slow_path);\n+  call_VM(obj, CAST_FROM_FN_PTR(address, InterpreterRuntime::read_flat_field),\n+          obj, entry);\n+\n+  bind(done);\n+  membar(Assembler::StoreStore);\n+}\n+\n+void InterpreterMacroAssembler::write_flat_field(Register entry, Register field_offset,\n+                                                 Register tmp1, Register tmp2,\n+                                                 Register obj) {\n+  assert_different_registers(entry, field_offset, tmp1, tmp2, obj);\n+  Label slow_path, done;\n+\n+  load_unsigned_byte(tmp1, Address(entry, in_bytes(ResolvedFieldEntry::flags_offset())));\n+  test_field_is_not_null_free_inline_type(tmp1, noreg \/* temp *\/, slow_path);\n+\n+  null_check(r0); \/\/ FIXME JDK-8341120\n+\n+  add(obj, obj, field_offset);\n+\n+  load_klass(tmp1, r0);\n+  payload_address(r0, r0, tmp1);\n+\n+  Register layout_info = field_offset;\n+  load_unsigned_short(tmp1, Address(entry, in_bytes(ResolvedFieldEntry::field_index_offset())));\n+  ldr(tmp2, Address(entry, in_bytes(ResolvedFieldEntry::field_holder_offset())));\n+  inline_layout_info(tmp2, tmp1, layout_info);\n+\n+  flat_field_copy(IN_HEAP, r0, obj, layout_info);\n+  b(done);\n+\n+  bind(slow_path);\n+  call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::write_flat_field), obj, r0, entry);\n+  bind(done);\n+}\n+\n@@ -245,1 +333,2 @@\n-                                                  Label& ok_is_subtype) {\n+                                                  Label& ok_is_subtype,\n+                                                  bool profile) {\n@@ -251,1 +340,3 @@\n-  profile_typecheck(r2, Rsub_klass, r5); \/\/ blows r2, reloads r5\n+  if (profile) {\n+    profile_typecheck(r2, Rsub_klass, r5); \/\/ blows r2, reloads r5\n+  }\n@@ -635,0 +726,1 @@\n+\n@@ -662,0 +754,44 @@\n+  if (state == atos && InlineTypeReturnedAsFields) {\n+    Label skip;\n+    Label not_null;\n+    cbnz(r0, not_null);\n+    \/\/ Returned value is null, zero all return registers because they may belong to oop fields\n+    mov(j_rarg1, zr);\n+    mov(j_rarg2, zr);\n+    mov(j_rarg3, zr);\n+    mov(j_rarg4, zr);\n+    mov(j_rarg5, zr);\n+    mov(j_rarg6, zr);\n+    mov(j_rarg7, zr);\n+    b(skip);\n+    bind(not_null);\n+\n+    \/\/ Check if we are returning an non-null inline type and load its fields into registers\n+    test_oop_is_not_inline_type(r0, rscratch2, skip, \/* can_be_null= *\/ false);\n+\n+    \/\/ Load fields from a buffered value with an inline class specific handler\n+    load_klass(rscratch1 \/*dst*\/, r0 \/*src*\/);\n+    ldr(rscratch1, Address(rscratch1, InstanceKlass::adr_inlineklass_fixed_block_offset()));\n+    ldr(rscratch1, Address(rscratch1, InlineKlass::unpack_handler_offset()));\n+    \/\/ Unpack handler can be null if inline type is not scalarizable in returns\n+    cbz(rscratch1, skip);\n+\n+    blr(rscratch1);\n+#ifdef ASSERT\n+    \/\/ TODO 8284443 Enable\n+    if (StressCallingConvention && false) {\n+      Label skip_stress;\n+      ldr(rscratch1, Address(rfp, frame::interpreter_frame_method_offset * wordSize));\n+      ldrw(rscratch1, Address(rscratch1, Method::flags_offset()));\n+      tstw(rscratch1, MethodFlags::has_scalarized_return_flag());\n+      br(Assembler::EQ, skip_stress);\n+      load_klass(r0, r0);\n+      orr(r0, r0, 1);\n+      bind(skip_stress);\n+    }\n+#endif\n+    bind(skip);\n+    \/\/ Check above kills sender esp in rscratch2. Reload it.\n+    ldr(rscratch2, Address(rfp, frame::interpreter_frame_sender_sp_offset * wordSize));\n+  }\n+\n@@ -939,1 +1075,1 @@\n-void InterpreterMacroAssembler::profile_not_taken_branch(Register mdp) {\n+void InterpreterMacroAssembler::profile_not_taken_branch(Register mdp, bool acmp) {\n@@ -951,1 +1087,1 @@\n-    update_mdp_by_constant(mdp, in_bytes(BranchData::branch_data_size()));\n+    update_mdp_by_constant(mdp, acmp ? in_bytes(ACmpData::acmp_data_size()) : in_bytes(BranchData::branch_data_size()));\n@@ -1274,0 +1410,114 @@\n+template <class ArrayData> void InterpreterMacroAssembler::profile_array_type(Register mdp,\n+                                                                              Register array,\n+                                                                              Register tmp) {\n+  if (ProfileInterpreter) {\n+    Label profile_continue;\n+\n+    \/\/ If no method data exists, go to profile_continue.\n+    test_method_data_pointer(mdp, profile_continue);\n+\n+    mov(tmp, array);\n+    profile_obj_type(tmp, Address(mdp, in_bytes(ArrayData::array_offset())));\n+\n+    Label not_flat;\n+    test_non_flat_array_oop(array, tmp, not_flat);\n+\n+    set_mdp_flag_at(mdp, ArrayData::flat_array_byte_constant());\n+\n+    bind(not_flat);\n+\n+    Label not_null_free;\n+    test_non_null_free_array_oop(array, tmp, not_null_free);\n+\n+    set_mdp_flag_at(mdp, ArrayData::null_free_array_byte_constant());\n+\n+    bind(not_null_free);\n+\n+    bind(profile_continue);\n+  }\n+}\n+\n+template void InterpreterMacroAssembler::profile_array_type<ArrayLoadData>(Register mdp,\n+                                                                           Register array,\n+                                                                           Register tmp);\n+template void InterpreterMacroAssembler::profile_array_type<ArrayStoreData>(Register mdp,\n+                                                                            Register array,\n+                                                                            Register tmp);\n+\n+void InterpreterMacroAssembler::profile_multiple_element_types(Register mdp, Register element, Register tmp, const Register tmp2) {\n+  if (ProfileInterpreter) {\n+    Label profile_continue;\n+\n+    \/\/ If no method data exists, go to profile_continue.\n+    test_method_data_pointer(mdp, profile_continue);\n+\n+    Label done, update;\n+    cbnz(element, update);\n+    set_mdp_flag_at(mdp, BitData::null_seen_byte_constant());\n+    b(done);\n+\n+    bind(update);\n+    load_klass(tmp, element);\n+\n+    \/\/ Record the object type.\n+    record_klass_in_profile(tmp, mdp, tmp2);\n+\n+    bind(done);\n+\n+    \/\/ The method data pointer needs to be updated.\n+    update_mdp_by_constant(mdp, in_bytes(ArrayStoreData::array_store_data_size()));\n+\n+    bind(profile_continue);\n+  }\n+}\n+\n+\n+void InterpreterMacroAssembler::profile_element_type(Register mdp,\n+                                                     Register element,\n+                                                     Register tmp) {\n+  if (ProfileInterpreter) {\n+    Label profile_continue;\n+\n+    \/\/ If no method data exists, go to profile_continue.\n+    test_method_data_pointer(mdp, profile_continue);\n+\n+    mov(tmp, element);\n+    profile_obj_type(tmp, Address(mdp, in_bytes(ArrayLoadData::element_offset())));\n+\n+    \/\/ The method data pointer needs to be updated.\n+    update_mdp_by_constant(mdp, in_bytes(ArrayLoadData::array_load_data_size()));\n+\n+    bind(profile_continue);\n+  }\n+}\n+\n+void InterpreterMacroAssembler::profile_acmp(Register mdp,\n+                                             Register left,\n+                                             Register right,\n+                                             Register tmp) {\n+  if (ProfileInterpreter) {\n+    Label profile_continue;\n+\n+    \/\/ If no method data exists, go to profile_continue.\n+    test_method_data_pointer(mdp, profile_continue);\n+\n+    mov(tmp, left);\n+    profile_obj_type(tmp, Address(mdp, in_bytes(ACmpData::left_offset())));\n+\n+    Label left_not_inline_type;\n+    test_oop_is_not_inline_type(left, tmp, left_not_inline_type);\n+    set_mdp_flag_at(mdp, ACmpData::left_inline_type_byte_constant());\n+    bind(left_not_inline_type);\n+\n+    mov(tmp, right);\n+    profile_obj_type(tmp, Address(mdp, in_bytes(ACmpData::right_offset())));\n+\n+    Label right_not_inline_type;\n+    test_oop_is_not_inline_type(right, tmp, right_not_inline_type);\n+    set_mdp_flag_at(mdp, ACmpData::right_inline_type_byte_constant());\n+    bind(right_not_inline_type);\n+\n+    bind(profile_continue);\n+  }\n+}\n+\n@@ -1637,1 +1887,1 @@\n-        assert(ReturnTypeEntry::static_cell_count() < TypeStackSlotEntries::per_arg_count(), \"can't move past ret type\");\n+        assert(SingleTypeEntry::static_cell_count() < TypeStackSlotEntries::per_arg_count(), \"can't move past ret type\");\n@@ -1683,1 +1933,1 @@\n-    Address mdo_ret_addr(mdp, -in_bytes(ReturnTypeEntry::size()));\n+    Address mdo_ret_addr(mdp, -in_bytes(SingleTypeEntry::size()));\n","filename":"src\/hotspot\/cpu\/aarch64\/interp_masm_aarch64.cpp","additions":256,"deletions":6,"binary":false,"changes":262,"status":"modified"},{"patch":"@@ -29,0 +29,1 @@\n+#include \"ci\/ciInlineKlass.hpp\"\n@@ -50,0 +51,1 @@\n+#include \"oops\/resolvedFieldEntry.hpp\"\n@@ -51,0 +53,1 @@\n+#include \"runtime\/globals.hpp\"\n@@ -56,0 +59,1 @@\n+#include \"runtime\/signature_cc.hpp\"\n@@ -59,0 +63,1 @@\n+#include \"vmreg_aarch64.inline.hpp\"\n@@ -2031,1 +2036,5 @@\n-  if (!VerifyOops) return;\n+  if (!VerifyOops || VerifyAdapterSharing) {\n+    \/\/ Below address of the code string confuses VerifyAdapterSharing\n+    \/\/ because it may differ between otherwise equivalent adapters.\n+    return;\n+  }\n@@ -2064,1 +2073,5 @@\n-  if (!VerifyOops) return;\n+  if (!VerifyOops || VerifyAdapterSharing) {\n+    \/\/ Below address of the code string confuses VerifyAdapterSharing\n+    \/\/ because it may differ between otherwise equivalent adapters.\n+    return;\n+  }\n@@ -2162,0 +2175,4 @@\n+void MacroAssembler::super_call_VM_leaf(address entry_point) {\n+  MacroAssembler::call_VM_leaf_base(entry_point, 1);\n+}\n+\n@@ -2207,0 +2224,88 @@\n+void MacroAssembler::test_markword_is_inline_type(Register markword, Label& is_inline_type) {\n+  assert_different_registers(markword, rscratch2);\n+  mov(rscratch2, markWord::inline_type_mask_in_place);\n+  andr(markword, markword, rscratch2);\n+  mov(rscratch2, markWord::inline_type_pattern);\n+  cmp(markword, rscratch2);\n+  br(Assembler::EQ, is_inline_type);\n+}\n+\n+void MacroAssembler::test_oop_is_not_inline_type(Register object, Register tmp, Label& not_inline_type, bool can_be_null) {\n+  assert_different_registers(tmp, rscratch1);\n+  if (can_be_null) {\n+    cbz(object, not_inline_type);\n+  }\n+  const int is_inline_type_mask = markWord::inline_type_pattern;\n+  ldr(tmp, Address(object, oopDesc::mark_offset_in_bytes()));\n+  mov(rscratch1, is_inline_type_mask);\n+  andr(tmp, tmp, rscratch1);\n+  cmp(tmp, rscratch1);\n+  br(Assembler::NE, not_inline_type);\n+}\n+\n+void MacroAssembler::test_field_is_null_free_inline_type(Register flags, Register temp_reg, Label& is_null_free_inline_type) {\n+  assert(temp_reg == noreg, \"not needed\"); \/\/ keep signature uniform with x86\n+  tbnz(flags, ResolvedFieldEntry::is_null_free_inline_type_shift, is_null_free_inline_type);\n+}\n+\n+void MacroAssembler::test_field_is_not_null_free_inline_type(Register flags, Register temp_reg, Label& not_null_free_inline_type) {\n+  assert(temp_reg == noreg, \"not needed\"); \/\/ keep signature uniform with x86\n+  tbz(flags, ResolvedFieldEntry::is_null_free_inline_type_shift, not_null_free_inline_type);\n+}\n+\n+void MacroAssembler::test_field_is_flat(Register flags, Register temp_reg, Label& is_flat) {\n+  assert(temp_reg == noreg, \"not needed\"); \/\/ keep signature uniform with x86\n+  tbnz(flags, ResolvedFieldEntry::is_flat_shift, is_flat);\n+}\n+\n+void MacroAssembler::test_field_has_null_marker(Register flags, Register temp_reg, Label& has_null_marker) {\n+  assert(temp_reg == noreg, \"not needed\"); \/\/ keep signature uniform with x86\n+  tbnz(flags, ResolvedFieldEntry::has_null_marker_shift, has_null_marker);\n+}\n+\n+void MacroAssembler::test_oop_prototype_bit(Register oop, Register temp_reg, int32_t test_bit, bool jmp_set, Label& jmp_label) {\n+  Label test_mark_word;\n+  \/\/ load mark word\n+  ldr(temp_reg, Address(oop, oopDesc::mark_offset_in_bytes()));\n+  \/\/ check displaced\n+  tst(temp_reg, markWord::unlocked_value);\n+  br(Assembler::NE, test_mark_word);\n+  \/\/ slow path use klass prototype\n+  load_prototype_header(temp_reg, oop);\n+\n+  bind(test_mark_word);\n+  andr(temp_reg, temp_reg, test_bit);\n+  if (jmp_set) {\n+    cbnz(temp_reg, jmp_label);\n+  } else {\n+    cbz(temp_reg, jmp_label);\n+  }\n+}\n+\n+void MacroAssembler::test_flat_array_oop(Register oop, Register temp_reg, Label& is_flat_array) {\n+  test_oop_prototype_bit(oop, temp_reg, markWord::flat_array_bit_in_place, true, is_flat_array);\n+}\n+\n+void MacroAssembler::test_non_flat_array_oop(Register oop, Register temp_reg,\n+                                                  Label&is_non_flat_array) {\n+  test_oop_prototype_bit(oop, temp_reg, markWord::flat_array_bit_in_place, false, is_non_flat_array);\n+}\n+\n+void MacroAssembler::test_null_free_array_oop(Register oop, Register temp_reg, Label& is_null_free_array) {\n+  test_oop_prototype_bit(oop, temp_reg, markWord::null_free_array_bit_in_place, true, is_null_free_array);\n+}\n+\n+void MacroAssembler::test_non_null_free_array_oop(Register oop, Register temp_reg, Label&is_non_null_free_array) {\n+  test_oop_prototype_bit(oop, temp_reg, markWord::null_free_array_bit_in_place, false, is_non_null_free_array);\n+}\n+\n+void MacroAssembler::test_flat_array_layout(Register lh, Label& is_flat_array) {\n+  tst(lh, Klass::_lh_array_tag_flat_value_bit_inplace);\n+  br(Assembler::NE, is_flat_array);\n+}\n+\n+void MacroAssembler::test_non_flat_array_layout(Register lh, Label& is_non_flat_array) {\n+  tst(lh, Klass::_lh_array_tag_flat_value_bit_inplace);\n+  br(Assembler::EQ, is_non_flat_array);\n+}\n+\n@@ -4907,0 +5012,10 @@\n+void MacroAssembler::load_metadata(Register dst, Register src) {\n+  if (UseCompactObjectHeaders) {\n+    load_narrow_klass_compact(dst, src);\n+  } else if (UseCompressedClassPointers) {\n+    ldrw(dst, Address(src, oopDesc::klass_offset_in_bytes()));\n+  } else {\n+    ldr(dst, Address(src, oopDesc::klass_offset_in_bytes()));\n+  }\n+}\n+\n@@ -5017,0 +5132,5 @@\n+void MacroAssembler::load_prototype_header(Register dst, Register src) {\n+  load_klass(dst, src);\n+  ldr(dst, Address(dst, Klass::prototype_header_offset()));\n+}\n+\n@@ -5418,0 +5538,40 @@\n+void MacroAssembler::flat_field_copy(DecoratorSet decorators, Register src, Register dst,\n+                                     Register inline_layout_info) {\n+  BarrierSetAssembler* bs = BarrierSet::barrier_set()->barrier_set_assembler();\n+  bs->flat_field_copy(this, decorators, src, dst, inline_layout_info);\n+}\n+\n+void MacroAssembler::payload_offset(Register inline_klass, Register offset) {\n+  ldr(offset, Address(inline_klass, InstanceKlass::adr_inlineklass_fixed_block_offset()));\n+  ldrw(offset, Address(offset, InlineKlass::payload_offset_offset()));\n+}\n+\n+void MacroAssembler::payload_address(Register oop, Register data, Register inline_klass) {\n+  \/\/ ((address) (void*) o) + vk->payload_offset();\n+  Register offset = (data == oop) ? rscratch1 : data;\n+  payload_offset(inline_klass, offset);\n+  if (data == oop) {\n+    add(data, data, offset);\n+  } else {\n+    lea(data, Address(oop, offset));\n+  }\n+}\n+\n+void MacroAssembler::data_for_value_array_index(Register array, Register array_klass,\n+                                                Register index, Register data) {\n+  assert_different_registers(array, array_klass, index);\n+  assert_different_registers(rscratch1, array, index);\n+\n+  \/\/ array->base() + (index << Klass::layout_helper_log2_element_size(lh));\n+  ldrw(rscratch1, Address(array_klass, Klass::layout_helper_offset()));\n+\n+  \/\/ Klass::layout_helper_log2_element_size(lh)\n+  \/\/ (lh >> _lh_log2_element_size_shift) & _lh_log2_element_size_mask;\n+  lsr(rscratch1, rscratch1, Klass::_lh_log2_element_size_shift);\n+  andr(rscratch1, rscratch1, Klass::_lh_log2_element_size_mask);\n+  lslv(index, index, rscratch1);\n+\n+  add(data, array, index);\n+  add(data, data, arrayOopDesc::base_offset_in_bytes(T_FLAT_ELEMENT));\n+}\n+\n@@ -5493,0 +5653,102 @@\n+\/\/ Object \/ value buffer allocation...\n+void MacroAssembler::allocate_instance(Register klass, Register new_obj,\n+                                       Register t1, Register t2,\n+                                       bool clear_fields, Label& alloc_failed)\n+{\n+  Label done, initialize_header, initialize_object, slow_case, slow_case_no_pop;\n+  Register layout_size = t1;\n+  assert(new_obj == r0, \"needs to be r0\");\n+  assert_different_registers(klass, new_obj, t1, t2);\n+\n+  \/\/ get instance_size in InstanceKlass (scaled to a count of bytes)\n+  ldrw(layout_size, Address(klass, Klass::layout_helper_offset()));\n+  \/\/ test to see if it is malformed in some way\n+  tst(layout_size, Klass::_lh_instance_slow_path_bit);\n+  br(Assembler::NE, slow_case_no_pop);\n+\n+  \/\/ Allocate the instance:\n+  \/\/  If TLAB is enabled:\n+  \/\/    Try to allocate in the TLAB.\n+  \/\/    If fails, go to the slow path.\n+  \/\/    Initialize the allocation.\n+  \/\/    Exit.\n+  \/\/\n+  \/\/  Go to slow path.\n+\n+  if (UseTLAB) {\n+    push(klass);\n+    tlab_allocate(new_obj, layout_size, 0, klass, t2, slow_case);\n+    if (ZeroTLAB || (!clear_fields)) {\n+      \/\/ the fields have been already cleared\n+      b(initialize_header);\n+    } else {\n+      \/\/ initialize both the header and fields\n+      b(initialize_object);\n+    }\n+\n+    if (clear_fields) {\n+      \/\/ The object is initialized before the header.  If the object size is\n+      \/\/ zero, go directly to the header initialization.\n+      bind(initialize_object);\n+      int header_size = oopDesc::header_size() * HeapWordSize;\n+      assert(is_aligned(header_size, BytesPerLong), \"oop header size must be 8-byte-aligned\");\n+      subs(layout_size, layout_size, header_size);\n+      br(Assembler::EQ, initialize_header);\n+\n+      \/\/ Initialize topmost object field, divide size by 8, check if odd and\n+      \/\/ test if zero.\n+\n+  #ifdef ASSERT\n+      \/\/ make sure instance_size was multiple of 8\n+      Label L;\n+      tst(layout_size, 7);\n+      br(Assembler::EQ, L);\n+      stop(\"object size is not multiple of 8 - adjust this code\");\n+      bind(L);\n+      \/\/ must be > 0, no extra check needed here\n+  #endif\n+\n+      lsr(layout_size, layout_size, LogBytesPerLong);\n+\n+      \/\/ initialize remaining object fields: instance_size was a multiple of 8\n+      {\n+        Label loop;\n+        Register base = t2;\n+\n+        bind(loop);\n+        add(rscratch1, new_obj, layout_size, Assembler::LSL, LogBytesPerLong);\n+        str(zr, Address(rscratch1, header_size - 1*oopSize));\n+        subs(layout_size, layout_size, 1);\n+        br(Assembler::NE, loop);\n+      }\n+    } \/\/ clear_fields\n+\n+    \/\/ initialize object header only.\n+    bind(initialize_header);\n+    pop(klass);\n+    Register mark_word = t2;\n+    if (UseCompactObjectHeaders || EnableValhalla) {\n+      ldr(mark_word, Address(klass, Klass::prototype_header_offset()));\n+      str(mark_word, Address(new_obj, oopDesc::mark_offset_in_bytes()));\n+    } else {\n+      mov(mark_word, (intptr_t)markWord::prototype().value());\n+      str(mark_word, Address(new_obj, oopDesc::mark_offset_in_bytes()));\n+    }\n+    if (!UseCompactObjectHeaders) {\n+      store_klass_gap(new_obj, zr);  \/\/ zero klass gap for compressed oops\n+      mov(t2, klass);                \/\/ preserve klass\n+      store_klass(new_obj, t2);      \/\/ src klass reg is potentially compressed\n+    }\n+    b(done);\n+  }\n+\n+  if (UseTLAB) {\n+    bind(slow_case);\n+    pop(klass);\n+  }\n+  bind(slow_case_no_pop);\n+  b(alloc_failed);\n+\n+  bind(done);\n+}\n+\n@@ -5532,0 +5794,20 @@\n+void MacroAssembler::get_inline_type_field_klass(Register holder_klass, Register index, Register inline_klass) {\n+  inline_layout_info(holder_klass, index, inline_klass);\n+  ldr(inline_klass, Address(inline_klass, InlineLayoutInfo::klass_offset()));\n+}\n+\n+void MacroAssembler::inline_layout_info(Register holder_klass, Register index, Register layout_info) {\n+  assert_different_registers(holder_klass, index, layout_info);\n+  InlineLayoutInfo array[2];\n+  int size = (char*)&array[1] - (char*)&array[0]; \/\/ computing size of array elements\n+  if (is_power_of_2(size)) {\n+    lsl(index, index, log2i_exact(size)); \/\/ Scale index by power of 2\n+  } else {\n+    mov(layout_info, size);\n+    mul(index, index, layout_info); \/\/ Scale the index to be the entry index * array_element_size\n+  }\n+  ldr(layout_info, Address(holder_klass, InstanceKlass::inline_layout_info_array_offset()));\n+  add(layout_info, layout_info, Array<InlineLayoutInfo>::base_offset_in_bytes());\n+  lea(layout_info, Address(layout_info, index));\n+}\n+\n@@ -5618,0 +5900,1 @@\n+#ifdef ASSERT\n@@ -5619,0 +5902,5 @@\n+  build_frame(framesize, false);\n+}\n+#endif\n+\n+void MacroAssembler::build_frame(int framesize DEBUG_ONLY(COMMA bool zap_rfp_lr_spills)) {\n@@ -5624,1 +5912,6 @@\n-    stp(rfp, lr, Address(sp, framesize - 2 * wordSize));\n+    if (DEBUG_ONLY(zap_rfp_lr_spills ||) false) {\n+      mov_immediate64(rscratch1, ((uint64_t)badRegWordVal) << 32 | (uint64_t)badRegWordVal);\n+      stp(rscratch1, rscratch1, Address(sp, framesize - 2 * wordSize));\n+    } else {\n+      stp(rfp, lr, Address(sp, framesize - 2 * wordSize));\n+    }\n@@ -5627,1 +5920,6 @@\n-    stp(rfp, lr, Address(pre(sp, -2 * wordSize)));\n+    if (DEBUG_ONLY(zap_rfp_lr_spills ||) false) {\n+      mov_immediate64(rscratch1, ((uint64_t)badRegWordVal) << 32 | (uint64_t)badRegWordVal);\n+      stp(rscratch1, rscratch1, Address(pre(sp, -2 * wordSize)));\n+    } else {\n+      stp(rfp, lr, Address(pre(sp, -2 * wordSize)));\n+    }\n@@ -5657,0 +5955,66 @@\n+void MacroAssembler::remove_frame(int initial_framesize, bool needs_stack_repair) {\n+  if (needs_stack_repair) {\n+    \/\/ Remove the extension of the caller's frame used for inline type unpacking\n+    \/\/\n+    \/\/ Right now the stack looks like this:\n+    \/\/\n+    \/\/ | Arguments from caller     |\n+    \/\/ |---------------------------|  <-- caller's SP\n+    \/\/ | Saved LR #1               |\n+    \/\/ | Saved FP #1               |\n+    \/\/ |---------------------------|\n+    \/\/ | Extension space for       |\n+    \/\/ |   inline arg (un)packing  |\n+    \/\/ |---------------------------|  <-- start of this method's frame\n+    \/\/ | Saved LR #2               |\n+    \/\/ | Saved FP #2               |\n+    \/\/ |---------------------------|  <-- FP\n+    \/\/ | sp_inc                    |\n+    \/\/ | method locals             |\n+    \/\/ |---------------------------|  <-- SP\n+    \/\/\n+    \/\/ There are two copies of FP and LR on the stack. They will be identical at\n+    \/\/ first, but that can change.\n+    \/\/ If the caller has been deoptimized, LR #1 will be patched to point at the\n+    \/\/ deopt blob, and LR #2 will still point into the old method.\n+    \/\/ If the saved FP (x29) was not used as the frame pointer, but to store an\n+    \/\/ oop, the GC will be aware only of FP #1 as the spilled location of x29 and\n+    \/\/ will fix only this one. Overall, FP\/LR #2 are not reliable and are simply\n+    \/\/ needed to add space between the extension space and the locals, as there\n+    \/\/ would be between the real arguments and the locals if we don't need to\n+    \/\/ do unpacking.\n+    \/\/\n+    \/\/ When restoring, one must then load FP #1 into x29, and LR #1 into x30,\n+    \/\/ while keeping in mind that from the scalarized entry point, there will be\n+    \/\/ only one copy of each.\n+    \/\/\n+    \/\/ The sp_inc stack slot holds the total size of the frame including the\n+    \/\/ extension space minus two words for the saved FP and LR. That is how to\n+    \/\/ find FP\/LR #1. This size is expressed in bytes. Be careful when using it\n+    \/\/ from C++ in pointer arithmetic; you might need to divide it by wordSize.\n+    \/\/\n+    \/\/ TODO 8371993 store fake values instead of LR\/FP#2\n+\n+    int sp_inc_offset = initial_framesize - 3 * wordSize;  \/\/ Immediately below saved LR and FP\n+\n+    ldr(rscratch1, Address(sp, sp_inc_offset));\n+    add(sp, sp, rscratch1);\n+    ldp(rfp, lr, Address(post(sp, 2 * wordSize)));\n+  } else {\n+    remove_frame(initial_framesize);\n+  }\n+}\n+\n+void MacroAssembler::save_stack_increment(int sp_inc, int frame_size) {\n+  int real_frame_size = frame_size + sp_inc;\n+  assert(sp_inc == 0 || sp_inc > 2*wordSize, \"invalid sp_inc value\");\n+  assert(real_frame_size >= 2*wordSize, \"frame size must include FP\/LR space\");\n+  assert((real_frame_size & (StackAlignmentInBytes-1)) == 0, \"frame size not aligned\");\n+\n+  int sp_inc_offset = frame_size - 3 * wordSize;  \/\/ Immediately below saved LR and FP\n+\n+  \/\/ Subtract two words for the saved FP and LR as these will be popped\n+  \/\/ separately. See remove_frame above.\n+  mov(rscratch1, real_frame_size - 2*wordSize);\n+  str(rscratch1, Address(sp, sp_inc_offset));\n+}\n@@ -6574,0 +6938,448 @@\n+#ifdef COMPILER2\n+\/\/ C2 compiled method's prolog code\n+\/\/ Moved here from aarch64.ad to support Valhalla code belows\n+void MacroAssembler::verified_entry(Compile* C, int sp_inc) {\n+  if (C->clinit_barrier_on_entry()) {\n+    assert(!C->method()->holder()->is_not_initialized(), \"initialization should have been started\");\n+\n+    Label L_skip_barrier;\n+\n+    mov_metadata(rscratch2, C->method()->holder()->constant_encoding());\n+    clinit_barrier(rscratch2, rscratch1, &L_skip_barrier);\n+    far_jump(RuntimeAddress(SharedRuntime::get_handle_wrong_method_stub()));\n+    bind(L_skip_barrier);\n+  }\n+\n+  if (C->max_vector_size() > 0) {\n+    reinitialize_ptrue();\n+  }\n+\n+  int bangsize = C->output()->bang_size_in_bytes();\n+  if (C->output()->need_stack_bang(bangsize))\n+    generate_stack_overflow_check(bangsize);\n+\n+  \/\/ n.b. frame size includes space for return pc and rfp\n+  const long framesize = C->output()->frame_size_in_bytes();\n+  build_frame(framesize DEBUG_ONLY(COMMA sp_inc != 0));\n+\n+  if (C->needs_stack_repair()) {\n+    save_stack_increment(sp_inc, framesize);\n+  }\n+\n+  if (VerifyStackAtCalls) {\n+    Unimplemented();\n+  }\n+}\n+#endif \/\/ COMPILER2\n+\n+int MacroAssembler::store_inline_type_fields_to_buf(ciInlineKlass* vk, bool from_interpreter) {\n+  assert(InlineTypeReturnedAsFields, \"Inline types should never be returned as fields\");\n+  \/\/ An inline type might be returned. If fields are in registers we\n+  \/\/ need to allocate an inline type instance and initialize it with\n+  \/\/ the value of the fields.\n+  Label skip;\n+  \/\/ We only need a new buffered inline type if a new one is not returned\n+  tbz(r0, 0, skip);\n+  int call_offset = -1;\n+\n+  \/\/ Be careful not to clobber r1-7 which hold returned fields\n+  \/\/ Also do not use callee-saved registers as these may be live in the interpreter\n+  Register tmp1 = r13, tmp2 = r14, klass = r15, r0_preserved = r12;\n+\n+  \/\/ The following code is similar to allocate_instance but has some slight differences,\n+  \/\/ e.g. object size is always not zero, sometimes it's constant; storing klass ptr after\n+  \/\/ allocating is not necessary if vk != nullptr, etc. allocate_instance is not aware of these.\n+  Label slow_case;\n+  \/\/ 1. Try to allocate a new buffered inline instance either from TLAB or eden space\n+  mov(r0_preserved, r0); \/\/ save r0 for slow_case since *_allocate may corrupt it when allocation failed\n+\n+  if (vk != nullptr) {\n+    \/\/ Called from C1, where the return type is statically known.\n+    movptr(klass, (intptr_t)vk->get_InlineKlass());\n+    jint lh = vk->layout_helper();\n+    assert(lh != Klass::_lh_neutral_value, \"inline class in return type must have been resolved\");\n+    if (UseTLAB && !Klass::layout_helper_needs_slow_path(lh)) {\n+      tlab_allocate(r0, noreg, lh, tmp1, tmp2, slow_case);\n+    } else {\n+      b(slow_case);\n+    }\n+  } else {\n+    \/\/ Call from interpreter. R0 contains ((the InlineKlass* of the return type) | 0x01)\n+    andr(klass, r0, -2);\n+    if (UseTLAB) {\n+      ldrw(tmp2, Address(klass, Klass::layout_helper_offset()));\n+      tst(tmp2, Klass::_lh_instance_slow_path_bit);\n+      br(Assembler::NE, slow_case);\n+      tlab_allocate(r0, tmp2, 0, tmp1, tmp2, slow_case);\n+    } else {\n+      b(slow_case);\n+    }\n+  }\n+  if (UseTLAB) {\n+    \/\/ 2. Initialize buffered inline instance header\n+    Register buffer_obj = r0;\n+    if (UseCompactObjectHeaders) {\n+      ldr(rscratch1, Address(klass, Klass::prototype_header_offset()));\n+      str(rscratch1, Address(buffer_obj, oopDesc::mark_offset_in_bytes()));\n+    } else {\n+      mov(rscratch1, (intptr_t)markWord::inline_type_prototype().value());\n+      str(rscratch1, Address(buffer_obj, oopDesc::mark_offset_in_bytes()));\n+      store_klass_gap(buffer_obj, zr);\n+      if (vk == nullptr) {\n+        \/\/ store_klass corrupts klass, so save it for later use (interpreter case only).\n+        mov(tmp1, klass);\n+      }\n+      store_klass(buffer_obj, klass);\n+      klass = tmp1;\n+    }\n+    \/\/ 3. Initialize its fields with an inline class specific handler\n+    if (vk != nullptr) {\n+      far_call(RuntimeAddress(vk->pack_handler())); \/\/ no need for call info as this will not safepoint.\n+    } else {\n+      ldr(tmp1, Address(klass, InstanceKlass::adr_inlineklass_fixed_block_offset()));\n+      ldr(tmp1, Address(tmp1, InlineKlass::pack_handler_offset()));\n+      blr(tmp1);\n+    }\n+\n+    membar(Assembler::StoreStore);\n+    b(skip);\n+  } else {\n+    \/\/ Must have already branched to slow_case above.\n+    DEBUG_ONLY(should_not_reach_here());\n+  }\n+  bind(slow_case);\n+  \/\/ We failed to allocate a new inline type, fall back to a runtime\n+  \/\/ call. Some oop field may be live in some registers but we can't\n+  \/\/ tell. That runtime call will take care of preserving them\n+  \/\/ across a GC if there's one.\n+  mov(r0, r0_preserved);\n+\n+  if (from_interpreter) {\n+    super_call_VM_leaf(StubRoutines::store_inline_type_fields_to_buf());\n+  } else {\n+    far_call(RuntimeAddress(StubRoutines::store_inline_type_fields_to_buf()));\n+    call_offset = offset();\n+  }\n+  membar(Assembler::StoreStore);\n+\n+  bind(skip);\n+  return call_offset;\n+}\n+\n+\/\/ Move a value between registers\/stack slots and update the reg_state\n+bool MacroAssembler::move_helper(VMReg from, VMReg to, BasicType bt, RegState reg_state[]) {\n+  assert(from->is_valid() && to->is_valid(), \"source and destination must be valid\");\n+  if (reg_state[to->value()] == reg_written) {\n+    return true; \/\/ Already written\n+  }\n+\n+  if (from != to && bt != T_VOID) {\n+    if (reg_state[to->value()] == reg_readonly) {\n+      return false; \/\/ Not yet writable\n+    }\n+    if (from->is_reg()) {\n+      if (to->is_reg()) {\n+        if (from->is_Register() && to->is_Register()) {\n+          mov(to->as_Register(), from->as_Register());\n+        } else if (from->is_FloatRegister() && to->is_FloatRegister()) {\n+          fmovd(to->as_FloatRegister(), from->as_FloatRegister());\n+        } else {\n+          ShouldNotReachHere();\n+        }\n+      } else {\n+        int st_off = to->reg2stack() * VMRegImpl::stack_slot_size;\n+        Address to_addr = Address(sp, st_off);\n+        if (from->is_FloatRegister()) {\n+          if (bt == T_DOUBLE) {\n+             strd(from->as_FloatRegister(), to_addr);\n+          } else {\n+             assert(bt == T_FLOAT, \"must be float\");\n+             strs(from->as_FloatRegister(), to_addr);\n+          }\n+        } else {\n+          str(from->as_Register(), to_addr);\n+        }\n+      }\n+    } else {\n+      Address from_addr = Address(sp, from->reg2stack() * VMRegImpl::stack_slot_size);\n+      if (to->is_reg()) {\n+        if (to->is_FloatRegister()) {\n+          if (bt == T_DOUBLE) {\n+            ldrd(to->as_FloatRegister(), from_addr);\n+          } else {\n+            assert(bt == T_FLOAT, \"must be float\");\n+            ldrs(to->as_FloatRegister(), from_addr);\n+          }\n+        } else {\n+          ldr(to->as_Register(), from_addr);\n+        }\n+      } else {\n+        int st_off = to->reg2stack() * VMRegImpl::stack_slot_size;\n+        ldr(rscratch1, from_addr);\n+        str(rscratch1, Address(sp, st_off));\n+      }\n+    }\n+  }\n+\n+  \/\/ Update register states\n+  reg_state[from->value()] = reg_writable;\n+  reg_state[to->value()] = reg_written;\n+  return true;\n+}\n+\n+\/\/ Calculate the extra stack space required for packing or unpacking inline\n+\/\/ args and adjust the stack pointer\n+int MacroAssembler::extend_stack_for_inline_args(int args_on_stack) {\n+  int sp_inc = args_on_stack * VMRegImpl::stack_slot_size;\n+  sp_inc = align_up(sp_inc, StackAlignmentInBytes);\n+  assert(sp_inc > 0, \"sanity\");\n+\n+  \/\/ Save a copy of the FP and LR here for deoptimization patching and frame walking\n+  stp(rfp, lr, Address(pre(sp, -2 * wordSize)));\n+\n+  \/\/ Adjust the stack pointer. This will be repaired on return by MacroAssembler::remove_frame\n+  if (sp_inc < (1 << 9)) {\n+    sub(sp, sp, sp_inc);   \/\/ Fits in an immediate\n+  } else {\n+    mov(rscratch1, sp_inc);\n+    sub(sp, sp, rscratch1);\n+  }\n+\n+  return sp_inc + 2 * wordSize;  \/\/ Account for the FP\/LR space\n+}\n+\n+\/\/ Read all fields from an inline type oop and store the values in registers\/stack slots\n+bool MacroAssembler::unpack_inline_helper(const GrowableArray<SigEntry>* sig, int& sig_index,\n+                                          VMReg from, int& from_index, VMRegPair* to, int to_count, int& to_index,\n+                                          RegState reg_state[]) {\n+  assert(sig->at(sig_index)._bt == T_VOID, \"should be at end delimiter\");\n+  assert(from->is_valid(), \"source must be valid\");\n+  bool progress = false;\n+#ifdef ASSERT\n+  const int start_offset = offset();\n+#endif\n+\n+  Label L_null, L_notNull;\n+  \/\/ Don't use r14 as tmp because it's used for spilling (see MacroAssembler::spill_reg_for)\n+  \/\/ TODO 8366717 We need to make sure that r14 (and potentially other long-life regs) are kept live in slowpath runtime calls in GC barriers\n+  Register tmp1 = r10;\n+  Register tmp2 = r11;\n+  Register fromReg = noreg;\n+  ScalarizedInlineArgsStream stream(sig, sig_index, to, to_count, to_index, -1);\n+  bool done = true;\n+  bool mark_done = true;\n+  VMReg toReg;\n+  BasicType bt;\n+  \/\/ Check if argument requires a null check\n+  bool null_check = false;\n+  VMReg nullCheckReg;\n+  while (stream.next(nullCheckReg, bt)) {\n+    if (sig->at(stream.sig_index())._offset == -1) {\n+      null_check = true;\n+      break;\n+    }\n+  }\n+  stream.reset(sig_index, to_index);\n+  while (stream.next(toReg, bt)) {\n+    assert(toReg->is_valid(), \"destination must be valid\");\n+    int idx = (int)toReg->value();\n+    if (reg_state[idx] == reg_readonly) {\n+      if (idx != from->value()) {\n+        mark_done = false;\n+      }\n+      done = false;\n+      continue;\n+    } else if (reg_state[idx] == reg_written) {\n+      continue;\n+    }\n+    assert(reg_state[idx] == reg_writable, \"must be writable\");\n+    reg_state[idx] = reg_written;\n+    progress = true;\n+\n+    if (fromReg == noreg) {\n+      if (from->is_reg()) {\n+        fromReg = from->as_Register();\n+      } else {\n+        int st_off = from->reg2stack() * VMRegImpl::stack_slot_size;\n+        ldr(tmp1, Address(sp, st_off));\n+        fromReg = tmp1;\n+      }\n+      if (null_check) {\n+        \/\/ Nullable inline type argument, emit null check\n+        cbz(fromReg, L_null);\n+      }\n+    }\n+    int off = sig->at(stream.sig_index())._offset;\n+    if (off == -1) {\n+      assert(null_check, \"Missing null check at\");\n+      if (toReg->is_stack()) {\n+        int st_off = toReg->reg2stack() * VMRegImpl::stack_slot_size;\n+        mov(tmp2, 1);\n+        str(tmp2, Address(sp, st_off));\n+      } else {\n+        mov(toReg->as_Register(), 1);\n+      }\n+      continue;\n+    }\n+    assert(off > 0, \"offset in object should be positive\");\n+    Address fromAddr = Address(fromReg, off);\n+    if (!toReg->is_FloatRegister()) {\n+      Register dst = toReg->is_stack() ? tmp2 : toReg->as_Register();\n+      if (is_reference_type(bt)) {\n+        load_heap_oop(dst, fromAddr, rscratch1, rscratch2);\n+      } else {\n+        bool is_signed = (bt != T_CHAR) && (bt != T_BOOLEAN);\n+        load_sized_value(dst, fromAddr, type2aelembytes(bt), is_signed);\n+      }\n+      if (toReg->is_stack()) {\n+        int st_off = toReg->reg2stack() * VMRegImpl::stack_slot_size;\n+        str(dst, Address(sp, st_off));\n+      }\n+    } else if (bt == T_DOUBLE) {\n+      ldrd(toReg->as_FloatRegister(), fromAddr);\n+    } else {\n+      assert(bt == T_FLOAT, \"must be float\");\n+      ldrs(toReg->as_FloatRegister(), fromAddr);\n+    }\n+  }\n+  if (progress && null_check) {\n+    if (done) {\n+      b(L_notNull);\n+      bind(L_null);\n+      \/\/ Set null marker to zero to signal that the argument is null.\n+      \/\/ Also set all oop fields to zero to make the GC happy.\n+      stream.reset(sig_index, to_index);\n+      while (stream.next(toReg, bt)) {\n+        if (sig->at(stream.sig_index())._offset == -1 ||\n+            bt == T_OBJECT || bt == T_ARRAY) {\n+          if (toReg->is_stack()) {\n+            int st_off = toReg->reg2stack() * VMRegImpl::stack_slot_size;\n+            str(zr, Address(sp, st_off));\n+          } else {\n+            mov(toReg->as_Register(), zr);\n+          }\n+        }\n+      }\n+      bind(L_notNull);\n+    } else {\n+      bind(L_null);\n+    }\n+  }\n+\n+  \/\/ TODO 8366717 This is probably okay but looks fishy because stream is reset in the \"Set null marker to zero\" case just above. Same on x64.\n+  sig_index = stream.sig_index();\n+  to_index = stream.regs_index();\n+\n+  if (mark_done && reg_state[from->value()] != reg_written) {\n+    \/\/ This is okay because no one else will write to that slot\n+    reg_state[from->value()] = reg_writable;\n+  }\n+  from_index--;\n+  assert(progress || (start_offset == offset()), \"should not emit code\");\n+  return done;\n+}\n+\n+\/\/ Pack fields back into an inline type oop\n+bool MacroAssembler::pack_inline_helper(const GrowableArray<SigEntry>* sig, int& sig_index, int vtarg_index,\n+                                        VMRegPair* from, int from_count, int& from_index, VMReg to,\n+                                        RegState reg_state[], Register val_array) {\n+  assert(sig->at(sig_index)._bt == T_METADATA, \"should be at delimiter\");\n+  assert(to->is_valid(), \"destination must be valid\");\n+\n+  if (reg_state[to->value()] == reg_written) {\n+    skip_unpacked_fields(sig, sig_index, from, from_count, from_index);\n+    return true; \/\/ Already written\n+  }\n+\n+  \/\/ The GC barrier expanded by store_heap_oop below may call into the\n+  \/\/ runtime so use callee-saved registers for any values that need to be\n+  \/\/ preserved. The GC barrier assembler should take care of saving the\n+  \/\/ Java argument registers.\n+  \/\/ TODO 8284443 Isn't it an issue if below code uses r14 as tmp when it contains a spilled value?\n+  \/\/ Be careful with r14 because it's used for spilling (see MacroAssembler::spill_reg_for).\n+  Register val_obj_tmp = r21;\n+  Register from_reg_tmp = r22;\n+  Register tmp1 = r14;\n+  Register tmp2 = r13;\n+  Register tmp3 = r12;\n+  Register val_obj = to->is_stack() ? val_obj_tmp : to->as_Register();\n+\n+  assert_different_registers(val_obj_tmp, from_reg_tmp, tmp1, tmp2, tmp3, val_array);\n+\n+  if (reg_state[to->value()] == reg_readonly) {\n+    if (!is_reg_in_unpacked_fields(sig, sig_index, to, from, from_count, from_index)) {\n+      skip_unpacked_fields(sig, sig_index, from, from_count, from_index);\n+      return false; \/\/ Not yet writable\n+    }\n+    val_obj = val_obj_tmp;\n+  }\n+\n+  int index = arrayOopDesc::base_offset_in_bytes(T_OBJECT) + vtarg_index * type2aelembytes(T_OBJECT);\n+  load_heap_oop(val_obj, Address(val_array, index), tmp1, tmp2);\n+\n+  ScalarizedInlineArgsStream stream(sig, sig_index, from, from_count, from_index);\n+  VMReg fromReg;\n+  BasicType bt;\n+  Label L_null;\n+  while (stream.next(fromReg, bt)) {\n+    assert(fromReg->is_valid(), \"source must be valid\");\n+    reg_state[fromReg->value()] = reg_writable;\n+\n+    int off = sig->at(stream.sig_index())._offset;\n+    if (off == -1) {\n+      \/\/ Nullable inline type argument, emit null check\n+      Label L_notNull;\n+      if (fromReg->is_stack()) {\n+        int ld_off = fromReg->reg2stack() * VMRegImpl::stack_slot_size;\n+        ldrb(tmp2, Address(sp, ld_off));\n+        cbnz(tmp2, L_notNull);\n+      } else {\n+        cbnz(fromReg->as_Register(), L_notNull);\n+      }\n+      mov(val_obj, 0);\n+      b(L_null);\n+      bind(L_notNull);\n+      continue;\n+    }\n+\n+    assert(off > 0, \"offset in object should be positive\");\n+    size_t size_in_bytes = is_java_primitive(bt) ? type2aelembytes(bt) : wordSize;\n+\n+    \/\/ Pack the scalarized field into the value object.\n+    Address dst(val_obj, off);\n+    if (!fromReg->is_FloatRegister()) {\n+      Register src;\n+      if (fromReg->is_stack()) {\n+        src = from_reg_tmp;\n+        int ld_off = fromReg->reg2stack() * VMRegImpl::stack_slot_size;\n+        load_sized_value(src, Address(sp, ld_off), size_in_bytes, \/* is_signed *\/ false);\n+      } else {\n+        src = fromReg->as_Register();\n+      }\n+      assert_different_registers(dst.base(), src, tmp1, tmp2, tmp3, val_array);\n+      if (is_reference_type(bt)) {\n+        store_heap_oop(dst, src, tmp1, tmp2, tmp3, IN_HEAP | ACCESS_WRITE | IS_DEST_UNINITIALIZED);\n+      } else {\n+        store_sized_value(dst, src, size_in_bytes);\n+      }\n+    } else if (bt == T_DOUBLE) {\n+      strd(fromReg->as_FloatRegister(), dst);\n+    } else {\n+      assert(bt == T_FLOAT, \"must be float\");\n+      strs(fromReg->as_FloatRegister(), dst);\n+    }\n+  }\n+  bind(L_null);\n+  sig_index = stream.sig_index();\n+  from_index = stream.regs_index();\n+\n+  assert(reg_state[to->value()] == reg_writable, \"must have already been read\");\n+  bool success = move_helper(val_obj->as_VMReg(), to, T_OBJECT, reg_state);\n+  assert(success, \"to register must be writeable\");\n+  return true;\n+}\n+\n+VMReg MacroAssembler::spill_reg_for(VMReg reg) {\n+  return (reg->is_FloatRegister()) ? v8->as_VMReg() : r14->as_VMReg();\n+}\n+\n@@ -6984,0 +7796,3 @@\n+  \/\/ Mask inline_type bit such that we go to the slow path if object is an inline type\n+  andr(mark, mark, ~((int) markWord::inline_type_bit_in_place));\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/macroAssembler_aarch64.cpp","additions":819,"deletions":4,"binary":false,"changes":823,"status":"modified"},{"patch":"@@ -36,0 +36,1 @@\n+#include \"utilities\/macros.hpp\"\n@@ -37,0 +38,4 @@\n+#include \"runtime\/signature.hpp\"\n+\n+\n+class ciInlineKlass;\n@@ -187,1 +192,2 @@\n-  void build_frame(int framesize);\n+  DEBUG_ONLY(void build_frame(int framesize);)\n+  void build_frame(int framesize DEBUG_ONLY(COMMA bool zap_rfp_lr_spills));\n@@ -680,0 +686,22 @@\n+  \/\/ markWord tests, kills markWord reg\n+  void test_markword_is_inline_type(Register markword, Label& is_inline_type);\n+\n+  \/\/ inlineKlass queries, kills temp_reg\n+  void test_oop_is_not_inline_type(Register object, Register tmp, Label& not_inline_type, bool can_be_null = true);\n+\n+  void test_field_is_null_free_inline_type(Register flags, Register temp_reg, Label& is_null_free);\n+  void test_field_is_not_null_free_inline_type(Register flags, Register temp_reg, Label& not_null_free);\n+  void test_field_is_flat(Register flags, Register temp_reg, Label& is_flat);\n+  void test_field_has_null_marker(Register flags, Register temp_reg, Label& has_null_marker);\n+\n+  \/\/ Check oops for special arrays, i.e. flat arrays and\/or null-free arrays\n+  void test_oop_prototype_bit(Register oop, Register temp_reg, int32_t test_bit, bool jmp_set, Label& jmp_label);\n+  void test_flat_array_oop(Register klass, Register temp_reg, Label& is_flat_array);\n+  void test_non_flat_array_oop(Register oop, Register temp_reg, Label&is_non_flat_array);\n+  void test_null_free_array_oop(Register oop, Register temp_reg, Label& is_null_free_array);\n+  void test_non_null_free_array_oop(Register oop, Register temp_reg, Label&is_non_null_free_array);\n+\n+  \/\/ Check array klass layout helper for flat or null-free arrays...\n+  void test_flat_array_layout(Register lh, Label& is_flat_array);\n+  void test_non_flat_array_layout(Register lh, Label& is_non_flat_array);\n+\n@@ -909,0 +937,2 @@\n+  void load_metadata(Register dst, Register src);\n+\n@@ -925,0 +955,9 @@\n+  void flat_field_copy(DecoratorSet decorators, Register src, Register dst, Register inline_layout_info);\n+\n+  \/\/ inline type data payload offsets...\n+  void payload_offset(Register inline_klass, Register offset);\n+  void payload_address(Register oop, Register data, Register inline_klass);\n+  \/\/ get data payload ptr a flat value array at index, kills rcx and index\n+  void data_for_value_array_index(Register array, Register array_klass,\n+                                  Register index, Register data);\n+\n@@ -938,0 +977,2 @@\n+  void load_prototype_header(Register dst, Register src);\n+\n@@ -987,0 +1028,9 @@\n+\n+  \/\/ Object \/ value buffer allocation...\n+  \/\/ Allocate instance of klass, assumes klass initialized by caller\n+  \/\/ new_obj prefers to be rax\n+  \/\/ Kills t1 and t2, perserves klass, return allocation in new_obj (rsi on LP64)\n+  void allocate_instance(Register klass, Register new_obj,\n+                         Register t1, Register t2,\n+                         bool clear_fields, Label& alloc_failed);\n+\n@@ -997,0 +1047,5 @@\n+  \/\/ For field \"index\" within \"klass\", return inline_klass ...\n+  void get_inline_type_field_klass(Register klass, Register index, Register inline_klass);\n+  void inline_layout_info(Register holder_klass, Register index, Register layout_info);\n+\n+\n@@ -1452,0 +1507,18 @@\n+  void verified_entry(Compile* C, int sp_inc);\n+\n+  \/\/ Inline type specific methods\n+  #include \"asm\/macroAssembler_common.hpp\"\n+\n+  int store_inline_type_fields_to_buf(ciInlineKlass* vk, bool from_interpreter = true);\n+  bool move_helper(VMReg from, VMReg to, BasicType bt, RegState reg_state[]);\n+  bool unpack_inline_helper(const GrowableArray<SigEntry>* sig, int& sig_index,\n+                            VMReg from, int& from_index, VMRegPair* to, int to_count, int& to_index,\n+                            RegState reg_state[]);\n+  bool pack_inline_helper(const GrowableArray<SigEntry>* sig, int& sig_index, int vtarg_index,\n+                          VMRegPair* from, int from_count, int& from_index, VMReg to,\n+                          RegState reg_state[], Register val_array);\n+  int extend_stack_for_inline_args(int args_on_stack);\n+  void remove_frame(int initial_framesize, bool needs_stack_repair);\n+  VMReg spill_reg_for(VMReg reg);\n+  void save_stack_increment(int sp_inc, int frame_size);\n+\n@@ -1523,0 +1596,2 @@\n+  void fill_words(Register base, uint64_t cnt, Register value);\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/macroAssembler_aarch64.hpp","additions":76,"deletions":1,"binary":false,"changes":77,"status":"modified"},{"patch":"@@ -29,0 +29,1 @@\n+#include \"classfile\/symbolTable.hpp\"\n@@ -362,0 +363,79 @@\n+\n+const uint SharedRuntime::java_return_convention_max_int = Argument::n_int_register_parameters_j;\n+const uint SharedRuntime::java_return_convention_max_float = Argument::n_float_register_parameters_j;\n+\n+int SharedRuntime::java_return_convention(const BasicType *sig_bt, VMRegPair *regs, int total_args_passed) {\n+\n+  \/\/ Create the mapping between argument positions and registers.\n+\n+  static const Register INT_ArgReg[java_return_convention_max_int] = {\n+    r0 \/* j_rarg7 *\/, j_rarg6, j_rarg5, j_rarg4, j_rarg3, j_rarg2, j_rarg1, j_rarg0\n+  };\n+\n+  static const FloatRegister FP_ArgReg[java_return_convention_max_float] = {\n+    j_farg0, j_farg1, j_farg2, j_farg3, j_farg4, j_farg5, j_farg6, j_farg7\n+  };\n+\n+  uint int_args = 0;\n+  uint fp_args = 0;\n+\n+  for (int i = 0; i < total_args_passed; i++) {\n+    switch (sig_bt[i]) {\n+    case T_BOOLEAN:\n+    case T_CHAR:\n+    case T_BYTE:\n+    case T_SHORT:\n+    case T_INT:\n+      if (int_args < SharedRuntime::java_return_convention_max_int) {\n+        regs[i].set1(INT_ArgReg[int_args]->as_VMReg());\n+        int_args ++;\n+      } else {\n+        return -1;\n+      }\n+      break;\n+    case T_VOID:\n+      \/\/ halves of T_LONG or T_DOUBLE\n+      assert(i != 0 && (sig_bt[i - 1] == T_LONG || sig_bt[i - 1] == T_DOUBLE), \"expecting half\");\n+      regs[i].set_bad();\n+      break;\n+    case T_LONG:\n+      assert((i + 1) < total_args_passed && sig_bt[i + 1] == T_VOID, \"expecting half\");\n+      \/\/ fall through\n+    case T_OBJECT:\n+    case T_ARRAY:\n+    case T_ADDRESS:\n+      \/\/ Should T_METADATA be added to java_calling_convention as well ?\n+    case T_METADATA:\n+      if (int_args < SharedRuntime::java_return_convention_max_int) {\n+        regs[i].set2(INT_ArgReg[int_args]->as_VMReg());\n+        int_args ++;\n+      } else {\n+        return -1;\n+      }\n+      break;\n+    case T_FLOAT:\n+      if (fp_args < SharedRuntime::java_return_convention_max_float) {\n+        regs[i].set1(FP_ArgReg[fp_args]->as_VMReg());\n+        fp_args ++;\n+      } else {\n+        return -1;\n+      }\n+      break;\n+    case T_DOUBLE:\n+      assert((i + 1) < total_args_passed && sig_bt[i + 1] == T_VOID, \"expecting half\");\n+      if (fp_args < SharedRuntime::java_return_convention_max_float) {\n+        regs[i].set2(FP_ArgReg[fp_args]->as_VMReg());\n+        fp_args ++;\n+      } else {\n+        return -1;\n+      }\n+      break;\n+    default:\n+      ShouldNotReachHere();\n+      break;\n+    }\n+  }\n+\n+  return int_args + fp_args;\n+}\n+\n@@ -396,0 +476,107 @@\n+\/\/ For each inline type argument, sig includes the list of fields of\n+\/\/ the inline type. This utility function computes the number of\n+\/\/ arguments for the call if inline types are passed by reference (the\n+\/\/ calling convention the interpreter expects).\n+static int compute_total_args_passed_int(const GrowableArray<SigEntry>* sig_extended) {\n+  int total_args_passed = 0;\n+  if (InlineTypePassFieldsAsArgs) {\n+    for (int i = 0; i < sig_extended->length(); i++) {\n+      BasicType bt = sig_extended->at(i)._bt;\n+      if (bt == T_METADATA) {\n+        \/\/ In sig_extended, an inline type argument starts with:\n+        \/\/ T_METADATA, followed by the types of the fields of the\n+        \/\/ inline type and T_VOID to mark the end of the value\n+        \/\/ type. Inline types are flattened so, for instance, in the\n+        \/\/ case of an inline type with an int field and an inline type\n+        \/\/ field that itself has 2 fields, an int and a long:\n+        \/\/ T_METADATA T_INT T_METADATA T_INT T_LONG T_VOID (second\n+        \/\/ slot for the T_LONG) T_VOID (inner inline type) T_VOID\n+        \/\/ (outer inline type)\n+        total_args_passed++;\n+        int vt = 1;\n+        do {\n+          i++;\n+          BasicType bt = sig_extended->at(i)._bt;\n+          BasicType prev_bt = sig_extended->at(i-1)._bt;\n+          if (bt == T_METADATA) {\n+            vt++;\n+          } else if (bt == T_VOID &&\n+                     prev_bt != T_LONG &&\n+                     prev_bt != T_DOUBLE) {\n+            vt--;\n+          }\n+        } while (vt != 0);\n+      } else {\n+        total_args_passed++;\n+      }\n+    }\n+  } else {\n+    total_args_passed = sig_extended->length();\n+  }\n+  return total_args_passed;\n+}\n+\n+\n+static void gen_c2i_adapter_helper(MacroAssembler* masm,\n+                                   BasicType bt,\n+                                   BasicType prev_bt,\n+                                   size_t size_in_bytes,\n+                                   const VMRegPair& reg_pair,\n+                                   const Address& to,\n+                                   Register tmp1,\n+                                   Register tmp2,\n+                                   Register tmp3,\n+                                   int extraspace,\n+                                   bool is_oop) {\n+  if (bt == T_VOID) {\n+    assert(prev_bt == T_LONG || prev_bt == T_DOUBLE, \"missing half\");\n+    return;\n+  }\n+\n+  \/\/ Say 4 args:\n+  \/\/ i   st_off\n+  \/\/ 0   32 T_LONG\n+  \/\/ 1   24 T_VOID\n+  \/\/ 2   16 T_OBJECT\n+  \/\/ 3    8 T_BOOL\n+  \/\/ -    0 return address\n+  \/\/\n+  \/\/ However to make thing extra confusing. Because we can fit a Java long\/double in\n+  \/\/ a single slot on a 64 bt vm and it would be silly to break them up, the interpreter\n+  \/\/ leaves one slot empty and only stores to a single slot. In this case the\n+  \/\/ slot that is occupied is the T_VOID slot. See I said it was confusing.\n+\n+  bool wide = (size_in_bytes == wordSize);\n+  VMReg r_1 = reg_pair.first();\n+  VMReg r_2 = reg_pair.second();\n+  assert(r_2->is_valid() == wide, \"invalid size\");\n+  if (!r_1->is_valid()) {\n+    assert(!r_2->is_valid(), \"\");\n+    return;\n+  }\n+\n+  if (!r_1->is_FloatRegister()) {\n+    Register val = r25;\n+    if (r_1->is_stack()) {\n+      \/\/ memory to memory use r25 (scratch registers is used by store_heap_oop)\n+      int ld_off = r_1->reg2stack() * VMRegImpl::stack_slot_size + extraspace;\n+      __ load_sized_value(val, Address(sp, ld_off), size_in_bytes, \/* is_signed *\/ false);\n+    } else {\n+      val = r_1->as_Register();\n+    }\n+    assert_different_registers(to.base(), val, tmp1, tmp2, tmp3);\n+    if (is_oop) {\n+      __ store_heap_oop(to, val, tmp1, tmp2, tmp3, IN_HEAP | ACCESS_WRITE | IS_DEST_UNINITIALIZED);\n+    } else {\n+      __ store_sized_value(to, val, size_in_bytes);\n+    }\n+  } else {\n+    if (wide) {\n+      __ strd(r_1->as_FloatRegister(), to);\n+    } else {\n+      \/\/ only a float use just part of the slot\n+      __ strs(r_1->as_FloatRegister(), to);\n+    }\n+  }\n+}\n+\n@@ -397,3 +584,1 @@\n-                            int total_args_passed,\n-                            int comp_args_on_stack,\n-                            const BasicType *sig_bt,\n+                            const GrowableArray<SigEntry>* sig_extended,\n@@ -401,1 +586,28 @@\n-                            Label& skip_fixup) {\n+                            bool requires_clinit_barrier,\n+                            address& c2i_no_clinit_check_entry,\n+                            Label& skip_fixup,\n+                            address start,\n+                            OopMapSet* oop_maps,\n+                            int& frame_complete,\n+                            int& frame_size_in_words,\n+                            bool alloc_inline_receiver) {\n+  if (requires_clinit_barrier && VM_Version::supports_fast_class_init_checks()) {\n+    Label L_skip_barrier;\n+\n+    { \/\/ Bypass the barrier for non-static methods\n+      __ ldrh(rscratch1, Address(rmethod, Method::access_flags_offset()));\n+      __ andsw(zr, rscratch1, JVM_ACC_STATIC);\n+      __ br(Assembler::EQ, L_skip_barrier); \/\/ non-static\n+    }\n+\n+    __ load_method_holder(rscratch2, rmethod);\n+    __ clinit_barrier(rscratch2, rscratch1, &L_skip_barrier);\n+    __ far_jump(RuntimeAddress(SharedRuntime::get_handle_wrong_method_stub()));\n+\n+    __ bind(L_skip_barrier);\n+    c2i_no_clinit_check_entry = __ pc();\n+  }\n+\n+  BarrierSetAssembler* bs = BarrierSet::barrier_set()->barrier_set_assembler();\n+  bs->c2i_entry_barrier(masm);\n+\n@@ -411,1 +623,25 @@\n-  int words_pushed = 0;\n+  \/\/ TODO 8366717 Is the comment about r13 correct? Isn't that r19_sender_sp?\n+  \/\/ Name some registers to be used in the following code. We can use\n+  \/\/ anything except r0-r7 which are arguments in the Java calling\n+  \/\/ convention, rmethod (r12), and r13 which holds the outgoing sender\n+  \/\/ SP for the interpreter.\n+  \/\/ TODO 8366717 We need to make sure that buf_array, buf_oop (and potentially other long-life regs) are kept live in slowpath runtime calls in GC barriers\n+  Register buf_array = r10;   \/\/ Array of buffered inline types\n+  Register buf_oop = r11;     \/\/ Buffered inline type oop\n+  Register tmp1 = r15;\n+  Register tmp2 = r16;\n+  Register tmp3 = r17;\n+\n+  if (InlineTypePassFieldsAsArgs) {\n+    \/\/ Is there an inline type argument?\n+    bool has_inline_argument = false;\n+    for (int i = 0; i < sig_extended->length() && !has_inline_argument; i++) {\n+      has_inline_argument = (sig_extended->at(i)._bt == T_METADATA);\n+    }\n+    if (has_inline_argument) {\n+      \/\/ There is at least an inline type argument: we're coming from\n+      \/\/ compiled code so we have no buffers to back the inline types\n+      \/\/ Allocate the buffers here with a runtime call.\n+      \/\/ TODO 8366717 Do we need to save vectors here? They could be used as arg registers, right? Same on x64.\n+      RegisterSaver reg_save(true \/* save_vectors *\/);\n+      OopMap* map = reg_save.save_live_registers(masm, 0, &frame_size_in_words);\n@@ -413,2 +649,2 @@\n-  \/\/ Since all args are passed on the stack, total_args_passed *\n-  \/\/ Interpreter::stackElementSize is the space we need.\n+      frame_complete = __ offset();\n+      address the_pc = __ pc();\n@@ -416,1 +652,2 @@\n-  int extraspace = total_args_passed * Interpreter::stackElementSize;\n+      Label retaddr;\n+      __ set_last_Java_frame(sp, noreg, retaddr, rscratch1);\n@@ -418,1 +655,3 @@\n-  __ mov(r19_sender_sp, sp);\n+      __ mov(c_rarg0, rthread);\n+      __ mov(c_rarg1, rmethod);\n+      __ mov(c_rarg2, (int64_t)alloc_inline_receiver);\n@@ -420,2 +659,3 @@\n-  \/\/ stack is aligned, keep it that way\n-  extraspace = align_up(extraspace, 2*wordSize);\n+      __ lea(rscratch1, RuntimeAddress(CAST_FROM_FN_PTR(address, SharedRuntime::allocate_inline_types)));\n+      __ blr(rscratch1);\n+      __ bind(retaddr);\n@@ -423,2 +663,2 @@\n-  if (extraspace)\n-    __ sub(sp, sp, extraspace);\n+      oop_maps->add_gc_map(__ pc() - start, map);\n+      __ reset_last_Java_frame(false);\n@@ -426,6 +666,1 @@\n-  \/\/ Now write the args into the outgoing interpreter space\n-  for (int i = 0; i < total_args_passed; i++) {\n-    if (sig_bt[i] == T_VOID) {\n-      assert(i > 0 && (sig_bt[i-1] == T_LONG || sig_bt[i-1] == T_DOUBLE), \"missing half\");\n-      continue;\n-    }\n+      reg_save.restore_live_registers(masm);\n@@ -433,16 +668,3 @@\n-    \/\/ offset to start parameters\n-    int st_off   = (total_args_passed - i - 1) * Interpreter::stackElementSize;\n-    int next_off = st_off - Interpreter::stackElementSize;\n-\n-    \/\/ Say 4 args:\n-    \/\/ i   st_off\n-    \/\/ 0   32 T_LONG\n-    \/\/ 1   24 T_VOID\n-    \/\/ 2   16 T_OBJECT\n-    \/\/ 3    8 T_BOOL\n-    \/\/ -    0 return address\n-    \/\/\n-    \/\/ However to make thing extra confusing. Because we can fit a Java long\/double in\n-    \/\/ a single slot on a 64 bt vm and it would be silly to break them up, the interpreter\n-    \/\/ leaves one slot empty and only stores to a single slot. In this case the\n-    \/\/ slot that is occupied is the T_VOID slot. See I said it was confusing.\n+      Label no_exception;\n+      __ ldr(rscratch1, Address(rthread, Thread::pending_exception_offset()));\n+      __ cbz(rscratch1, no_exception);\n@@ -450,5 +672,9 @@\n-    VMReg r_1 = regs[i].first();\n-    VMReg r_2 = regs[i].second();\n-    if (!r_1->is_valid()) {\n-      assert(!r_2->is_valid(), \"\");\n-      continue;\n+      __ str(zr, Address(rthread, JavaThread::vm_result_oop_offset()));\n+      __ ldr(r0, Address(rthread, Thread::pending_exception_offset()));\n+      __ far_jump(RuntimeAddress(StubRoutines::forward_exception_entry()));\n+\n+      __ bind(no_exception);\n+\n+      \/\/ We get an array of objects from the runtime call\n+      __ get_vm_result_oop(buf_array, rthread);\n+      __ get_vm_result_metadata(rmethod, rthread); \/\/ TODO: required to keep the callee Method live?\n@@ -456,9 +682,1 @@\n-    if (r_1->is_stack()) {\n-      \/\/ memory to memory use rscratch1\n-      int ld_off = (r_1->reg2stack() * VMRegImpl::stack_slot_size\n-                    + extraspace\n-                    + words_pushed * wordSize);\n-      if (!r_2->is_valid()) {\n-        \/\/ sign extend??\n-        __ ldrw(rscratch1, Address(sp, ld_off));\n-        __ str(rscratch1, Address(sp, st_off));\n+  }\n@@ -466,1 +684,11 @@\n-      } else {\n+  \/\/ Since all args are passed on the stack, total_args_passed *\n+  \/\/ Interpreter::stackElementSize is the space we need.\n+\n+  int total_args_passed = compute_total_args_passed_int(sig_extended);\n+  int extraspace = total_args_passed * Interpreter::stackElementSize;\n+\n+  \/\/ stack is aligned, keep it that way\n+  extraspace = align_up(extraspace, StackAlignmentInBytes);\n+\n+  \/\/ set senderSP value\n+  __ mov(r19_sender_sp, sp);\n@@ -468,1 +696,1 @@\n-        __ ldr(rscratch1, Address(sp, ld_off));\n+  __ sub(sp, sp, extraspace);\n@@ -470,6 +698,26 @@\n-        \/\/ Two VMREgs|OptoRegs can be T_OBJECT, T_ADDRESS, T_DOUBLE, T_LONG\n-        \/\/ T_DOUBLE and T_LONG use two slots in the interpreter\n-        if ( sig_bt[i] == T_LONG || sig_bt[i] == T_DOUBLE) {\n-          \/\/ ld_off == LSW, ld_off+wordSize == MSW\n-          \/\/ st_off == MSW, next_off == LSW\n-          __ str(rscratch1, Address(sp, next_off));\n+  \/\/ Now write the args into the outgoing interpreter space\n+\n+  \/\/ next_arg_comp is the next argument from the compiler point of\n+  \/\/ view (inline type fields are passed in registers\/on the stack). In\n+  \/\/ sig_extended, an inline type argument starts with: T_METADATA,\n+  \/\/ followed by the types of the fields of the inline type and T_VOID\n+  \/\/ to mark the end of the inline type. ignored counts the number of\n+  \/\/ T_METADATA\/T_VOID. next_vt_arg is the next inline type argument:\n+  \/\/ used to get the buffer for that argument from the pool of buffers\n+  \/\/ we allocated above and want to pass to the\n+  \/\/ interpreter. next_arg_int is the next argument from the\n+  \/\/ interpreter point of view (inline types are passed by reference).\n+  for (int next_arg_comp = 0, ignored = 0, next_vt_arg = 0, next_arg_int = 0;\n+       next_arg_comp < sig_extended->length(); next_arg_comp++) {\n+    assert(ignored <= next_arg_comp, \"shouldn't skip over more slots than there are arguments\");\n+    assert(next_arg_int <= total_args_passed, \"more arguments for the interpreter than expected?\");\n+    BasicType bt = sig_extended->at(next_arg_comp)._bt;\n+    int st_off = (total_args_passed - next_arg_int - 1) * Interpreter::stackElementSize;\n+    if (!InlineTypePassFieldsAsArgs || bt != T_METADATA) {\n+      int next_off = st_off - Interpreter::stackElementSize;\n+      const int offset = (bt == T_LONG || bt == T_DOUBLE) ? next_off : st_off;\n+      const VMRegPair reg_pair = regs[next_arg_comp-ignored];\n+      size_t size_in_bytes = reg_pair.second()->is_valid() ? 8 : 4;\n+      gen_c2i_adapter_helper(masm, bt, next_arg_comp > 0 ? sig_extended->at(next_arg_comp-1)._bt : T_ILLEGAL,\n+                             size_in_bytes, reg_pair, Address(sp, offset), tmp1, tmp2, tmp3, extraspace, false);\n+      next_arg_int++;\n@@ -477,7 +725,4 @@\n-          \/\/ Overwrite the unused slot with known junk\n-          __ mov(rscratch1, (uint64_t)0xdeadffffdeadaaaaull);\n-          __ str(rscratch1, Address(sp, st_off));\n-#endif \/* ASSERT *\/\n-        } else {\n-          __ str(rscratch1, Address(sp, st_off));\n-        }\n+      if (bt == T_LONG || bt == T_DOUBLE) {\n+        \/\/ Overwrite the unused slot with known junk\n+        __ mov(rscratch1, CONST64(0xdeadffffdeadaaaa));\n+        __ str(rscratch1, Address(sp, st_off));\n@@ -485,16 +730,24 @@\n-    } else if (r_1->is_Register()) {\n-      Register r = r_1->as_Register();\n-      if (!r_2->is_valid()) {\n-        \/\/ must be only an int (or less ) so move only 32bits to slot\n-        \/\/ why not sign extend??\n-        __ str(r, Address(sp, st_off));\n-      } else {\n-        \/\/ Two VMREgs|OptoRegs can be T_OBJECT, T_ADDRESS, T_DOUBLE, T_LONG\n-        \/\/ T_DOUBLE and T_LONG use two slots in the interpreter\n-        if ( sig_bt[i] == T_LONG || sig_bt[i] == T_DOUBLE) {\n-          \/\/ jlong\/double in gpr\n-#ifdef ASSERT\n-          \/\/ Overwrite the unused slot with known junk\n-          __ mov(rscratch1, (uint64_t)0xdeadffffdeadaaabull);\n-          __ str(rscratch1, Address(sp, st_off));\n-          __ str(r, Address(sp, next_off));\n+    } else {\n+      ignored++;\n+      \/\/ get the buffer from the just allocated pool of buffers\n+      int index = arrayOopDesc::base_offset_in_bytes(T_OBJECT) + next_vt_arg * type2aelembytes(T_OBJECT);\n+      __ load_heap_oop(buf_oop, Address(buf_array, index), tmp1, tmp2);\n+      next_vt_arg++; next_arg_int++;\n+      int vt = 1;\n+      \/\/ write fields we get from compiled code in registers\/stack\n+      \/\/ slots to the buffer: we know we are done with that inline type\n+      \/\/ argument when we hit the T_VOID that acts as an end of inline\n+      \/\/ type delimiter for this inline type. Inline types are flattened\n+      \/\/ so we might encounter embedded inline types. Each entry in\n+      \/\/ sig_extended contains a field offset in the buffer.\n+      Label L_null;\n+      do {\n+        next_arg_comp++;\n+        BasicType bt = sig_extended->at(next_arg_comp)._bt;\n+        BasicType prev_bt = sig_extended->at(next_arg_comp - 1)._bt;\n+        if (bt == T_METADATA) {\n+          vt++;\n+          ignored++;\n+        } else if (bt == T_VOID && prev_bt != T_LONG && prev_bt != T_DOUBLE) {\n+          vt--;\n+          ignored++;\n@@ -503,1 +756,22 @@\n-          __ str(r, Address(sp, st_off));\n+          int off = sig_extended->at(next_arg_comp)._offset;\n+          if (off == -1) {\n+            \/\/ Nullable inline type argument, emit null check\n+            VMReg reg = regs[next_arg_comp-ignored].first();\n+            Label L_notNull;\n+            if (reg->is_stack()) {\n+              int ld_off = reg->reg2stack() * VMRegImpl::stack_slot_size + extraspace;\n+              __ ldrb(tmp1, Address(sp, ld_off));\n+              __ cbnz(tmp1, L_notNull);\n+            } else {\n+              __ cbnz(reg->as_Register(), L_notNull);\n+            }\n+            __ str(zr, Address(sp, st_off));\n+            __ b(L_null);\n+            __ bind(L_notNull);\n+            continue;\n+          }\n+          assert(off > 0, \"offset in object should be positive\");\n+          size_t size_in_bytes = is_java_primitive(bt) ? type2aelembytes(bt) : wordSize;\n+          bool is_oop = is_reference_type(bt);\n+          gen_c2i_adapter_helper(masm, bt, next_arg_comp > 0 ? sig_extended->at(next_arg_comp-1)._bt : T_ILLEGAL,\n+                                 size_in_bytes, regs[next_arg_comp-ignored], Address(buf_oop, off), tmp1, tmp2, tmp3, extraspace, is_oop);\n@@ -505,14 +779,4 @@\n-      }\n-    } else {\n-      assert(r_1->is_FloatRegister(), \"\");\n-      if (!r_2->is_valid()) {\n-        \/\/ only a float use just part of the slot\n-        __ strs(r_1->as_FloatRegister(), Address(sp, st_off));\n-      } else {\n-#ifdef ASSERT\n-        \/\/ Overwrite the unused slot with known junk\n-        __ mov(rscratch1, (uint64_t)0xdeadffffdeadaaacull);\n-        __ str(rscratch1, Address(sp, st_off));\n-#endif \/* ASSERT *\/\n-        __ strd(r_1->as_FloatRegister(), Address(sp, next_off));\n-      }\n+      } while (vt != 0);\n+      \/\/ pass the buffer to the interpreter\n+      __ str(buf_oop, Address(sp, st_off));\n+      __ bind(L_null);\n@@ -528,0 +792,1 @@\n+void SharedRuntime::gen_i2c_adapter(MacroAssembler *masm, int comp_args_on_stack, const GrowableArray<SigEntry>* sig, const VMRegPair *regs) {\n@@ -529,5 +794,0 @@\n-void SharedRuntime::gen_i2c_adapter(MacroAssembler *masm,\n-                                    int total_args_passed,\n-                                    int comp_args_on_stack,\n-                                    const BasicType *sig_bt,\n-                                    const VMRegPair *regs) {\n@@ -562,1 +822,1 @@\n-  int comp_words_on_stack = align_up(comp_args_on_stack*VMRegImpl::stack_slot_size, wordSize)>>LogBytesPerWord;\n+  int comp_words_on_stack = 0;\n@@ -564,2 +824,3 @@\n-    __ sub(rscratch1, sp, comp_words_on_stack * wordSize);\n-    __ andr(sp, rscratch1, -16);\n+     comp_words_on_stack = align_up(comp_args_on_stack * VMRegImpl::stack_slot_size, wordSize) >> LogBytesPerWord;\n+     __ sub(rscratch1, sp, comp_words_on_stack * wordSize);\n+     __ andr(sp, rscratch1, -16);\n@@ -570,1 +831,1 @@\n-  __ ldr(rscratch1, Address(rmethod, in_bytes(Method::from_compiled_offset())));\n+  __ ldr(rscratch1, Address(rmethod, in_bytes(Method::from_compiled_inline_offset())));\n@@ -584,0 +845,2 @@\n+  int total_args_passed = sig->length();\n+\n@@ -586,2 +849,3 @@\n-    if (sig_bt[i] == T_VOID) {\n-      assert(i > 0 && (sig_bt[i-1] == T_LONG || sig_bt[i-1] == T_DOUBLE), \"missing half\");\n+    BasicType bt = sig->at(i)._bt;\n+    if (bt == T_VOID) {\n+      assert(i > 0 && (sig->at(i - 1)._bt == T_LONG || sig->at(i - 1)._bt == T_DOUBLE), \"missing half\");\n@@ -592,0 +856,1 @@\n+    assert(!regs[i].second()->is_valid() || regs[i].first()->next() == regs[i].second(), \"scrambled load targets?\");\n@@ -593,3 +858,1 @@\n-    assert(!regs[i].second()->is_valid() || regs[i].first()->next() == regs[i].second(),\n-            \"scrambled load targets?\");\n-    int ld_off = (total_args_passed - i - 1)*Interpreter::stackElementSize;\n+    int ld_off = (total_args_passed - i - 1) * Interpreter::stackElementSize;\n@@ -610,1 +873,1 @@\n-      int st_off = regs[i].first()->reg2stack()*VMRegImpl::stack_slot_size;\n+      int st_off = regs[i].first()->reg2stack() * VMRegImpl::stack_slot_size;\n@@ -627,2 +890,1 @@\n-        const int offset = (sig_bt[i]==T_LONG||sig_bt[i]==T_DOUBLE)?\n-                           next_off : ld_off;\n+        const int offset = (bt == T_LONG || bt == T_DOUBLE) ? next_off : ld_off;\n@@ -631,11 +893,28 @@\n-        __ str(rscratch2, Address(sp, st_off));\n-      }\n-    } else if (r_1->is_Register()) {  \/\/ Register argument\n-      Register r = r_1->as_Register();\n-      if (r_2->is_valid()) {\n-        \/\/\n-        \/\/ We are using two VMRegs. This can be either T_OBJECT,\n-        \/\/ T_ADDRESS, T_LONG, or T_DOUBLE the interpreter allocates\n-        \/\/ two slots but only uses one for thr T_LONG or T_DOUBLE case\n-        \/\/ So we must adjust where to pick up the data to match the\n-        \/\/ interpreter.\n+         __ str(rscratch2, Address(sp, st_off));\n+       }\n+     } else if (r_1->is_Register()) {  \/\/ Register argument\n+       Register r = r_1->as_Register();\n+       if (r_2->is_valid()) {\n+         \/\/\n+         \/\/ We are using two VMRegs. This can be either T_OBJECT,\n+         \/\/ T_ADDRESS, T_LONG, or T_DOUBLE the interpreter allocates\n+         \/\/ two slots but only uses one for thr T_LONG or T_DOUBLE case\n+         \/\/ So we must adjust where to pick up the data to match the\n+         \/\/ interpreter.\n+\n+        const int offset = (bt == T_LONG || bt == T_DOUBLE) ? next_off : ld_off;\n+\n+         \/\/ this can be a misaligned move\n+         __ ldr(r, Address(esp, offset));\n+       } else {\n+         \/\/ sign extend and use a full word?\n+         __ ldrw(r, Address(esp, ld_off));\n+       }\n+     } else {\n+       if (!r_2->is_valid()) {\n+         __ ldrs(r_1->as_FloatRegister(), Address(esp, ld_off));\n+       } else {\n+         __ ldrd(r_1->as_FloatRegister(), Address(esp, next_off));\n+       }\n+     }\n+   }\n@@ -643,17 +922,0 @@\n-        const int offset = (sig_bt[i]==T_LONG||sig_bt[i]==T_DOUBLE)?\n-                           next_off : ld_off;\n-\n-        \/\/ this can be a misaligned move\n-        __ ldr(r, Address(esp, offset));\n-      } else {\n-        \/\/ sign extend and use a full word?\n-        __ ldrw(r, Address(esp, ld_off));\n-      }\n-    } else {\n-      if (!r_2->is_valid()) {\n-        __ ldrs(r_1->as_FloatRegister(), Address(esp, ld_off));\n-      } else {\n-        __ ldrd(r_1->as_FloatRegister(), Address(esp, next_off));\n-      }\n-    }\n-  }\n@@ -676,1 +938,0 @@\n-\n@@ -680,8 +941,4 @@\n-\/\/ ---------------------------------------------------------------\n-void SharedRuntime::generate_i2c2i_adapters(MacroAssembler *masm,\n-                                            int total_args_passed,\n-                                            int comp_args_on_stack,\n-                                            const BasicType *sig_bt,\n-                                            const VMRegPair *regs,\n-                                            address entry_address[AdapterBlob::ENTRY_COUNT]) {\n-  entry_address[AdapterBlob::I2C] = __ pc();\n+static void gen_inline_cache_check(MacroAssembler *masm, Label& skip_fixup) {\n+  Register data = rscratch2;\n+  __ ic_check(1 \/* end_alignment *\/);\n+  __ ldr(rmethod, Address(data, CompiledICData::speculated_method_offset()));\n@@ -689,1 +946,7 @@\n-  gen_i2c_adapter(masm, total_args_passed, comp_args_on_stack, sig_bt, regs);\n+  \/\/ Method might have been compiled since the call site was patched to\n+  \/\/ interpreted; if that is the case treat it as a miss so we can get\n+  \/\/ the call site corrected.\n+  __ ldr(rscratch1, Address(rmethod, in_bytes(Method::code_offset())));\n+  __ cbz(rscratch1, skip_fixup);\n+  __ far_jump(RuntimeAddress(SharedRuntime::get_ic_miss_stub()));\n+}\n@@ -691,2 +954,12 @@\n-  entry_address[AdapterBlob::C2I_Unverified] = __ pc();\n-  Label skip_fixup;\n+\/\/ ---------------------------------------------------------------\n+void SharedRuntime::generate_i2c2i_adapters(MacroAssembler* masm,\n+                                            int comp_args_on_stack,\n+                                            const GrowableArray<SigEntry>* sig,\n+                                            const VMRegPair* regs,\n+                                            const GrowableArray<SigEntry>* sig_cc,\n+                                            const VMRegPair* regs_cc,\n+                                            const GrowableArray<SigEntry>* sig_cc_ro,\n+                                            const VMRegPair* regs_cc_ro,\n+                                            address entry_address[AdapterBlob::ENTRY_COUNT],\n+                                            AdapterBlob*& new_adapter,\n+                                            bool allocate_code_blob) {\n@@ -694,3 +967,2 @@\n-  Register data = rscratch2;\n-  Register receiver = j_rarg0;\n-  Register tmp = r10;  \/\/ A call-clobbered register not used for arg passing\n+  entry_address[AdapterBlob::I2C] = __ pc();\n+  gen_i2c_adapter(masm, comp_args_on_stack, sig, regs);\n@@ -707,7 +979,3 @@\n-  {\n-    __ block_comment(\"c2i_unverified_entry {\");\n-    \/\/ Method might have been compiled since the call site was patched to\n-    \/\/ interpreted; if that is the case treat it as a miss so we can get\n-    \/\/ the call site corrected.\n-    __ ic_check(1 \/* end_alignment *\/);\n-    __ ldr(rmethod, Address(data, CompiledICData::speculated_method_offset()));\n+  entry_address[AdapterBlob::C2I_Unverified] = __ pc();\n+  entry_address[AdapterBlob::C2I_Unverified_Inline] = __ pc();\n+  Label skip_fixup;\n@@ -715,5 +983,1 @@\n-    __ ldr(rscratch1, Address(rmethod, in_bytes(Method::code_offset())));\n-    __ cbz(rscratch1, skip_fixup);\n-    __ far_jump(RuntimeAddress(SharedRuntime::get_ic_miss_stub()));\n-    __ block_comment(\"} c2i_unverified_entry\");\n-  }\n+  gen_inline_cache_check(masm, skip_fixup);\n@@ -721,1 +985,3 @@\n-  entry_address[AdapterBlob::C2I] = __ pc();\n+  OopMapSet* oop_maps = new OopMapSet();\n+  int frame_complete = CodeOffsets::frame_never_safe;\n+  int frame_size_in_words = 0;\n@@ -723,1 +989,1 @@\n-  \/\/ Class initialization barrier for static methods\n+  \/\/ Scalarized c2i adapter with non-scalarized receiver (i.e., don't pack receiver)\n@@ -725,15 +991,33 @@\n-  if (VM_Version::supports_fast_class_init_checks()) {\n-    Label L_skip_barrier;\n-\n-    { \/\/ Bypass the barrier for non-static methods\n-      __ ldrh(rscratch1, Address(rmethod, Method::access_flags_offset()));\n-      __ andsw(zr, rscratch1, JVM_ACC_STATIC);\n-      __ br(Assembler::EQ, L_skip_barrier); \/\/ non-static\n-    }\n-\n-    __ load_method_holder(rscratch2, rmethod);\n-    __ clinit_barrier(rscratch2, rscratch1, &L_skip_barrier);\n-    __ far_jump(RuntimeAddress(SharedRuntime::get_handle_wrong_method_stub()));\n-\n-    __ bind(L_skip_barrier);\n-    entry_address[AdapterBlob::C2I_No_Clinit_Check] = __ pc();\n+  entry_address[AdapterBlob::C2I_Inline_RO] = __ pc();\n+  if (regs_cc != regs_cc_ro) {\n+    \/\/ No class init barrier needed because method is guaranteed to be non-static\n+    gen_c2i_adapter(masm, sig_cc_ro, regs_cc_ro, \/* requires_clinit_barrier = *\/ false, entry_address[AdapterBlob::C2I_No_Clinit_Check],\n+                    skip_fixup, entry_address[AdapterBlob::I2C], oop_maps, frame_complete, frame_size_in_words, \/* alloc_inline_receiver = *\/ false);\n+    skip_fixup.reset();\n+  }\n+\n+  \/\/ Scalarized c2i adapter\n+  entry_address[AdapterBlob::C2I]        = __ pc();\n+  entry_address[AdapterBlob::C2I_Inline] = __ pc();\n+  gen_c2i_adapter(masm, sig_cc, regs_cc, \/* requires_clinit_barrier = *\/ true, entry_address[AdapterBlob::C2I_No_Clinit_Check],\n+                  skip_fixup, entry_address[AdapterBlob::I2C], oop_maps, frame_complete, frame_size_in_words, \/* alloc_inline_receiver = *\/ true);\n+\n+  \/\/ Non-scalarized c2i adapter\n+  if (regs != regs_cc) {\n+    entry_address[AdapterBlob::C2I_Unverified_Inline] = __ pc();\n+    Label inline_entry_skip_fixup;\n+    gen_inline_cache_check(masm, inline_entry_skip_fixup);\n+\n+    entry_address[AdapterBlob::C2I_Inline] = __ pc();\n+    gen_c2i_adapter(masm, sig, regs, \/* requires_clinit_barrier = *\/ true, entry_address[AdapterBlob::C2I_No_Clinit_Check],\n+                    inline_entry_skip_fixup, entry_address[AdapterBlob::I2C], oop_maps, frame_complete, frame_size_in_words, \/* alloc_inline_receiver = *\/ false);\n+  }\n+\n+  \/\/ The c2i adapters might safepoint and trigger a GC. The caller must make sure that\n+  \/\/ the GC knows about the location of oop argument locations passed to the c2i adapter.\n+  if (allocate_code_blob) {\n+    bool caller_must_gc_arguments = (regs != regs_cc);\n+    int entry_offset[AdapterHandlerEntry::ENTRIES_COUNT];\n+    assert(AdapterHandlerEntry::ENTRIES_COUNT == 7, \"sanity\");\n+    AdapterHandlerLibrary::address_to_offset(entry_address, entry_offset);\n+    new_adapter = AdapterBlob::create(masm->code(), entry_offset, frame_complete, frame_size_in_words, oop_maps, caller_must_gc_arguments);\n@@ -741,6 +1025,0 @@\n-\n-  BarrierSetAssembler* bs = BarrierSet::barrier_set()->barrier_set_assembler();\n-  bs->c2i_entry_barrier(masm);\n-\n-  gen_c2i_adapter(masm, total_args_passed, comp_args_on_stack, sig_bt, regs, skip_fixup);\n-  return;\n@@ -2707,0 +2985,143 @@\n+BufferedInlineTypeBlob* SharedRuntime::generate_buffered_inline_type_adapter(const InlineKlass* vk) {\n+  BufferBlob* buf = BufferBlob::create(\"inline types pack\/unpack\", 16 * K);\n+  if (buf == nullptr) {\n+    return nullptr;\n+  }\n+  CodeBuffer buffer(buf);\n+  short buffer_locs[20];\n+  buffer.insts()->initialize_shared_locs((relocInfo*)buffer_locs,\n+                                         sizeof(buffer_locs)\/sizeof(relocInfo));\n+\n+  MacroAssembler _masm(&buffer);\n+  MacroAssembler* masm = &_masm;\n+\n+  const Array<SigEntry>* sig_vk = vk->extended_sig();\n+  const Array<VMRegPair>* regs = vk->return_regs();\n+\n+  int pack_fields_jobject_off = __ offset();\n+  \/\/ Resolve pre-allocated buffer from JNI handle.\n+  \/\/ We cannot do this in generate_call_stub() because it requires GC code to be initialized.\n+  Register Rresult = r14;  \/\/ See StubGenerator::generate_call_stub().\n+  __ ldr(r0, Address(Rresult));\n+  __ resolve_jobject(r0 \/* value *\/,\n+                     rthread \/* thread *\/,\n+                     r12 \/* tmp *\/);\n+  __ str(r0, Address(Rresult));\n+\n+  int pack_fields_off = __ offset();\n+\n+  int j = 1;\n+  for (int i = 0; i < sig_vk->length(); i++) {\n+    BasicType bt = sig_vk->at(i)._bt;\n+    if (bt == T_METADATA) {\n+      continue;\n+    }\n+    if (bt == T_VOID) {\n+      if (sig_vk->at(i-1)._bt == T_LONG ||\n+          sig_vk->at(i-1)._bt == T_DOUBLE) {\n+        j++;\n+      }\n+      continue;\n+    }\n+    int off = sig_vk->at(i)._offset;\n+    VMRegPair pair = regs->at(j);\n+    VMReg r_1 = pair.first();\n+    VMReg r_2 = pair.second();\n+    Address to(r0, off);\n+    if (bt == T_FLOAT) {\n+      __ strs(r_1->as_FloatRegister(), to);\n+    } else if (bt == T_DOUBLE) {\n+      __ strd(r_1->as_FloatRegister(), to);\n+    } else {\n+      Register val = r_1->as_Register();\n+      assert_different_registers(to.base(), val, r15, r16, r17);\n+      if (is_reference_type(bt)) {\n+        __ store_heap_oop(to, val, r15, r16, r17, IN_HEAP | ACCESS_WRITE | IS_DEST_UNINITIALIZED);\n+      } else {\n+        __ store_sized_value(to, r_1->as_Register(), type2aelembytes(bt));\n+      }\n+    }\n+    j++;\n+  }\n+  assert(j == regs->length(), \"missed a field?\");\n+  if (vk->has_nullable_atomic_layout()) {\n+    \/\/ Zero the null marker (setting it to 1 would be better but would require an additional register)\n+    __ strb(zr, Address(r0, vk->null_marker_offset()));\n+  }\n+  __ ret(lr);\n+\n+  int unpack_fields_off = __ offset();\n+\n+  Label skip;\n+  Label not_null;\n+  __ cbnz(r0, not_null);\n+\n+  \/\/ Return value is null. Zero oop registers to make the GC happy.\n+  j = 1;\n+  for (int i = 0; i < sig_vk->length(); i++) {\n+    BasicType bt = sig_vk->at(i)._bt;\n+    if (bt == T_METADATA) {\n+      continue;\n+    }\n+    if (bt == T_VOID) {\n+      if (sig_vk->at(i-1)._bt == T_LONG ||\n+          sig_vk->at(i-1)._bt == T_DOUBLE) {\n+        j++;\n+      }\n+      continue;\n+    }\n+    if (bt == T_OBJECT || bt == T_ARRAY) {\n+      VMRegPair pair = regs->at(j);\n+      VMReg r_1 = pair.first();\n+      __ mov(r_1->as_Register(), zr);\n+    }\n+    j++;\n+  }\n+  __ b(skip);\n+  __ bind(not_null);\n+\n+  j = 1;\n+  for (int i = 0; i < sig_vk->length(); i++) {\n+    BasicType bt = sig_vk->at(i)._bt;\n+    if (bt == T_METADATA) {\n+      continue;\n+    }\n+    if (bt == T_VOID) {\n+      if (sig_vk->at(i-1)._bt == T_LONG ||\n+          sig_vk->at(i-1)._bt == T_DOUBLE) {\n+        j++;\n+      }\n+      continue;\n+    }\n+    int off = sig_vk->at(i)._offset;\n+    assert(off > 0, \"offset in object should be positive\");\n+    VMRegPair pair = regs->at(j);\n+    VMReg r_1 = pair.first();\n+    VMReg r_2 = pair.second();\n+    Address from(r0, off);\n+    if (bt == T_FLOAT) {\n+      __ ldrs(r_1->as_FloatRegister(), from);\n+    } else if (bt == T_DOUBLE) {\n+      __ ldrd(r_1->as_FloatRegister(), from);\n+    } else if (bt == T_OBJECT || bt == T_ARRAY) {\n+      assert_different_registers(r0, r_1->as_Register());\n+      __ load_heap_oop(r_1->as_Register(), from, rscratch1, rscratch2);\n+    } else {\n+      assert(is_java_primitive(bt), \"unexpected basic type\");\n+      assert_different_registers(r0, r_1->as_Register());\n+      size_t size_in_bytes = type2aelembytes(bt);\n+      __ load_sized_value(r_1->as_Register(), from, size_in_bytes, bt != T_CHAR && bt != T_BOOLEAN);\n+    }\n+    j++;\n+  }\n+  assert(j == regs->length(), \"missed a field?\");\n+\n+  __ bind(skip);\n+\n+  __ ret(lr);\n+\n+  __ flush();\n+\n+  return BufferedInlineTypeBlob::create(&buffer, pack_fields_off, pack_fields_jobject_off, unpack_fields_off);\n+}\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/sharedRuntime_aarch64.cpp","additions":611,"deletions":190,"binary":false,"changes":801,"status":"modified"},{"patch":"@@ -331,4 +331,11 @@\n-    __ ldr(j_rarg2, result);\n-    Label is_long, is_float, is_double, exit;\n-    __ ldr(j_rarg1, result_type);\n-    __ cmp(j_rarg1, (u1)T_OBJECT);\n+    \/\/ All of j_rargN may be used to return inline type fields so be careful\n+    \/\/ not to clobber those.\n+    \/\/ SharedRuntime::generate_buffered_inline_type_adapter() knows the register\n+    \/\/ assignment of Rresult below.\n+    Register Rresult = r14, Rresult_type = r15;\n+    __ ldr(Rresult, result);\n+    Label is_long, is_float, is_double, check_prim, exit;\n+    __ ldr(Rresult_type, result_type);\n+    __ cmp(Rresult_type, (u1)T_OBJECT);\n+    __ br(Assembler::EQ, check_prim);\n+    __ cmp(Rresult_type, (u1)T_LONG);\n@@ -336,3 +343,1 @@\n-    __ cmp(j_rarg1, (u1)T_LONG);\n-    __ br(Assembler::EQ, is_long);\n-    __ cmp(j_rarg1, (u1)T_FLOAT);\n+    __ cmp(Rresult_type, (u1)T_FLOAT);\n@@ -340,1 +345,1 @@\n-    __ cmp(j_rarg1, (u1)T_DOUBLE);\n+    __ cmp(Rresult_type, (u1)T_DOUBLE);\n@@ -344,1 +349,1 @@\n-    __ strw(r0, Address(j_rarg2));\n+    __ strw(r0, Address(Rresult));\n@@ -396,0 +401,11 @@\n+    __ BIND(check_prim);\n+    if (InlineTypeReturnedAsFields) {\n+      \/\/ Check for scalarized return value\n+      __ tbz(r0, 0, is_long);\n+      \/\/ Load pack handler address\n+      __ andr(rscratch1, r0, -2);\n+      __ ldr(rscratch1, Address(rscratch1, InstanceKlass::adr_inlineklass_fixed_block_offset()));\n+      __ ldr(rscratch1, Address(rscratch1, InlineKlass::pack_handler_jobject_offset()));\n+      __ blr(rscratch1);\n+      __ b(exit);\n+    }\n@@ -398,1 +414,1 @@\n-    __ str(r0, Address(j_rarg2, 0));\n+    __ str(r0, Address(Rresult, 0));\n@@ -402,1 +418,1 @@\n-    __ strs(j_farg0, Address(j_rarg2, 0));\n+    __ strs(j_farg0, Address(Rresult, 0));\n@@ -406,1 +422,1 @@\n-    __ strd(j_farg0, Address(j_rarg2, 0));\n+    __ strd(j_farg0, Address(Rresult, 0));\n@@ -2227,0 +2243,6 @@\n+    \/\/ Check for flat inline type array -> return -1\n+    __ test_flat_array_oop(src, rscratch2, L_failed);\n+\n+    \/\/ Check for null-free (non-flat) inline type array -> handle as object array\n+    __ test_null_free_array_oop(src, rscratch2, L_objArray);\n+\n@@ -10525,0 +10547,24 @@\n+  static void save_return_registers(MacroAssembler* masm) {\n+    if (InlineTypeReturnedAsFields) {\n+      masm->push(RegSet::range(r0, r7), sp);\n+      masm->sub(sp, sp, 4 * wordSize);\n+      masm->st1(v0, v1, v2, v3, masm->T1D, Address(sp));\n+      masm->sub(sp, sp, 4 * wordSize);\n+      masm->st1(v4, v5, v6, v7, masm->T1D, Address(sp));\n+    } else {\n+      masm->fmovd(rscratch1, v0);\n+      masm->stp(rscratch1, r0, Address(masm->pre(sp, -2 * wordSize)));\n+    }\n+  }\n+\n+  static void restore_return_registers(MacroAssembler* masm) {\n+    if (InlineTypeReturnedAsFields) {\n+      masm->ld1(v4, v5, v6, v7, masm->T1D, Address(masm->post(sp, 4 * wordSize)));\n+      masm->ld1(v0, v1, v2, v3, masm->T1D, Address(masm->post(sp, 4 * wordSize)));\n+      masm->pop(RegSet::range(r0, r7), sp);\n+    } else {\n+      masm->ldp(rscratch1, r0, Address(masm->post(sp, 2 * wordSize)));\n+      masm->fmovd(v0, rscratch1);\n+    }\n+  }\n+\n@@ -10539,2 +10585,1 @@\n-      __ fmovd(rscratch1, v0);\n-      __ stp(rscratch1, r0, Address(__ pre(sp, -2 * wordSize)));\n+      save_return_registers(_masm);\n@@ -10549,2 +10594,1 @@\n-      __ ldp(rscratch1, r0, Address(__ post(sp, 2 * wordSize)));\n-      __ fmovd(v0, rscratch1);\n+      restore_return_registers(_masm);\n@@ -10569,2 +10613,1 @@\n-      __ fmovd(rscratch1, v0);\n-      __ stp(rscratch1, r0, Address(__ pre(sp, -2 * wordSize)));\n+      save_return_registers(_masm);\n@@ -10581,2 +10624,1 @@\n-      __ ldp(rscratch1, r0, Address(__ post(sp, 2 * wordSize)));\n-      __ fmovd(v0, rscratch1);\n+      restore_return_registers(_masm);\n@@ -11719,0 +11761,128 @@\n+  \/\/ Call here from the interpreter or compiled code to either load\n+  \/\/ multiple returned values from the inline type instance being\n+  \/\/ returned to registers or to store returned values to a newly\n+  \/\/ allocated inline type instance.\n+  address generate_return_value_stub(address destination, const char* name, bool has_res) {\n+    \/\/ We need to save all registers the calling convention may use so\n+    \/\/ the runtime calls read or update those registers. This needs to\n+    \/\/ be in sync with SharedRuntime::java_return_convention().\n+    \/\/ n.b. aarch64 asserts that frame::arg_reg_save_area_bytes == 0\n+    enum layout {\n+      j_rarg7_off = 0, j_rarg7_2,    \/\/ j_rarg7 is r0\n+      j_rarg6_off, j_rarg6_2,\n+      j_rarg5_off, j_rarg5_2,\n+      j_rarg4_off, j_rarg4_2,\n+      j_rarg3_off, j_rarg3_2,\n+      j_rarg2_off, j_rarg2_2,\n+      j_rarg1_off, j_rarg1_2,\n+      j_rarg0_off, j_rarg0_2,\n+\n+      j_farg7_off, j_farg7_2,\n+      j_farg6_off, j_farg6_2,\n+      j_farg5_off, j_farg5_2,\n+      j_farg4_off, j_farg4_2,\n+      j_farg3_off, j_farg3_2,\n+      j_farg2_off, j_farg2_2,\n+      j_farg1_off, j_farg1_2,\n+      j_farg0_off, j_farg0_2,\n+\n+      rfp_off, rfp_off2,\n+      return_off, return_off2,\n+\n+      framesize \/\/ inclusive of return address\n+    };\n+\n+    CodeBuffer code(name, 512, 64);\n+    MacroAssembler* masm = new MacroAssembler(&code);\n+\n+    int frame_size_in_bytes = align_up(framesize*BytesPerInt, 16);\n+    assert(frame_size_in_bytes == framesize*BytesPerInt, \"misaligned\");\n+    int frame_size_in_slots = frame_size_in_bytes \/ BytesPerInt;\n+    int frame_size_in_words = frame_size_in_bytes \/ wordSize;\n+\n+    OopMapSet* oop_maps = new OopMapSet();\n+    OopMap* map = new OopMap(frame_size_in_slots, 0);\n+\n+    map->set_callee_saved(VMRegImpl::stack2reg(j_rarg7_off), j_rarg7->as_VMReg());\n+    map->set_callee_saved(VMRegImpl::stack2reg(j_rarg6_off), j_rarg6->as_VMReg());\n+    map->set_callee_saved(VMRegImpl::stack2reg(j_rarg5_off), j_rarg5->as_VMReg());\n+    map->set_callee_saved(VMRegImpl::stack2reg(j_rarg4_off), j_rarg4->as_VMReg());\n+    map->set_callee_saved(VMRegImpl::stack2reg(j_rarg3_off), j_rarg3->as_VMReg());\n+    map->set_callee_saved(VMRegImpl::stack2reg(j_rarg2_off), j_rarg2->as_VMReg());\n+    map->set_callee_saved(VMRegImpl::stack2reg(j_rarg1_off), j_rarg1->as_VMReg());\n+    map->set_callee_saved(VMRegImpl::stack2reg(j_rarg0_off), j_rarg0->as_VMReg());\n+\n+    map->set_callee_saved(VMRegImpl::stack2reg(j_farg0_off), j_farg0->as_VMReg());\n+    map->set_callee_saved(VMRegImpl::stack2reg(j_farg1_off), j_farg1->as_VMReg());\n+    map->set_callee_saved(VMRegImpl::stack2reg(j_farg2_off), j_farg2->as_VMReg());\n+    map->set_callee_saved(VMRegImpl::stack2reg(j_farg3_off), j_farg3->as_VMReg());\n+    map->set_callee_saved(VMRegImpl::stack2reg(j_farg4_off), j_farg4->as_VMReg());\n+    map->set_callee_saved(VMRegImpl::stack2reg(j_farg5_off), j_farg5->as_VMReg());\n+    map->set_callee_saved(VMRegImpl::stack2reg(j_farg6_off), j_farg6->as_VMReg());\n+    map->set_callee_saved(VMRegImpl::stack2reg(j_farg7_off), j_farg7->as_VMReg());\n+\n+    address start = __ pc();\n+\n+    __ enter(); \/\/ Save FP and LR before call\n+\n+    __ stpd(j_farg1, j_farg0, Address(__ pre(sp, -2 * wordSize)));\n+    __ stpd(j_farg3, j_farg2, Address(__ pre(sp, -2 * wordSize)));\n+    __ stpd(j_farg5, j_farg4, Address(__ pre(sp, -2 * wordSize)));\n+    __ stpd(j_farg7, j_farg6, Address(__ pre(sp, -2 * wordSize)));\n+\n+    __ stp(j_rarg1, j_rarg0, Address(__ pre(sp, -2 * wordSize)));\n+    __ stp(j_rarg3, j_rarg2, Address(__ pre(sp, -2 * wordSize)));\n+    __ stp(j_rarg5, j_rarg4, Address(__ pre(sp, -2 * wordSize)));\n+    __ stp(j_rarg7, j_rarg6, Address(__ pre(sp, -2 * wordSize)));\n+\n+    int frame_complete = __ offset();\n+\n+    \/\/ Set up last_Java_sp and last_Java_fp\n+    address the_pc = __ pc();\n+    __ set_last_Java_frame(sp, noreg, the_pc, rscratch1);\n+\n+    \/\/ Call runtime\n+    __ mov(c_rarg1, r0);\n+    __ mov(c_rarg0, rthread);\n+\n+    __ mov(rscratch1, destination);\n+    __ blr(rscratch1);\n+\n+    oop_maps->add_gc_map(the_pc - start, map);\n+\n+    __ reset_last_Java_frame(false);\n+\n+    __ ldp(j_rarg7, j_rarg6, Address(__ post(sp, 2 * wordSize)));\n+    __ ldp(j_rarg5, j_rarg4, Address(__ post(sp, 2 * wordSize)));\n+    __ ldp(j_rarg3, j_rarg2, Address(__ post(sp, 2 * wordSize)));\n+    __ ldp(j_rarg1, j_rarg0, Address(__ post(sp, 2 * wordSize)));\n+\n+    __ ldpd(j_farg7, j_farg6, Address(__ post(sp, 2 * wordSize)));\n+    __ ldpd(j_farg5, j_farg4, Address(__ post(sp, 2 * wordSize)));\n+    __ ldpd(j_farg3, j_farg2, Address(__ post(sp, 2 * wordSize)));\n+    __ ldpd(j_farg1, j_farg0, Address(__ post(sp, 2 * wordSize)));\n+\n+    __ leave();\n+\n+    \/\/ check for pending exceptions\n+    Label pending;\n+    __ ldr(rscratch1, Address(rthread, in_bytes(Thread::pending_exception_offset())));\n+    __ cbnz(rscratch1, pending);\n+\n+    if (has_res) {\n+      __ get_vm_result_oop(r0, rthread);\n+    }\n+\n+    __ ret(lr);\n+\n+    __ bind(pending);\n+    __ far_jump(RuntimeAddress(StubRoutines::forward_exception_entry()));\n+\n+    \/\/ -------------\n+    \/\/ make sure all code is generated\n+    masm->flush();\n+\n+    RuntimeStub* stub = RuntimeStub::new_runtime_stub(name, &code, frame_complete, frame_size_in_words, oop_maps, false);\n+    return stub->entry_point();\n+  }\n+\n@@ -11767,0 +11937,8 @@\n+\n+    if (InlineTypeReturnedAsFields) {\n+      StubRoutines::_load_inline_type_fields_in_regs =\n+         generate_return_value_stub(CAST_FROM_FN_PTR(address, SharedRuntime::load_inline_type_fields_in_regs), \"load_inline_type_fields_in_regs\", false);\n+      StubRoutines::_store_inline_type_fields_to_buf =\n+         generate_return_value_stub(CAST_FROM_FN_PTR(address, SharedRuntime::store_inline_type_fields_to_buf), \"store_inline_type_fields_to_buf\", true);\n+    }\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/stubGenerator_aarch64.cpp","additions":198,"deletions":20,"binary":false,"changes":218,"status":"modified"},{"patch":"@@ -2554,0 +2554,4 @@\n+void LIR_Assembler::emit_profile_inline_type(LIR_OpProfileInlineType* op) {\n+  Unimplemented();\n+}\n+\n","filename":"src\/hotspot\/cpu\/arm\/c1_LIRAssembler_arm.cpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -3126,0 +3126,3 @@\n+void LIR_Assembler::emit_profile_inline_type(LIR_OpProfileInlineType* op) {\n+  Unimplemented();\n+}\n","filename":"src\/hotspot\/cpu\/ppc\/c1_LIRAssembler_ppc.cpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -1695,1 +1695,1 @@\n-        assert(ReturnTypeEntry::static_cell_count() < TypeStackSlotEntries::per_arg_count(),\n+        assert(SingleTypeEntry::static_cell_count() < TypeStackSlotEntries::per_arg_count(),\n@@ -1736,1 +1736,1 @@\n-    profile_obj_type(ret, R28_mdx, -in_bytes(ReturnTypeEntry::size()), tmp1, tmp2);\n+    profile_obj_type(ret, R28_mdx, -in_bytes(SingleTypeEntry::size()), tmp1, tmp2);\n","filename":"src\/hotspot\/cpu\/ppc\/interp_masm_ppc_64.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -3056,0 +3056,4 @@\n+void LIR_Assembler::emit_profile_inline_type(LIR_OpProfileInlineType* op) {\n+  Unimplemented();\n+}\n+\n","filename":"src\/hotspot\/cpu\/s390\/c1_LIRAssembler_s390.cpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -1653,1 +1653,1 @@\n-        assert(ReturnTypeEntry::static_cell_count() < TypeStackSlotEntries::per_arg_count(), \"can't move past ret type\");\n+        assert(SingleTypeEntry::static_cell_count() < TypeStackSlotEntries::per_arg_count(), \"can't move past ret type\");\n@@ -1702,1 +1702,1 @@\n-    Address mdo_ret_addr(mdp, -in_bytes(ReturnTypeEntry::size()));\n+    Address mdo_ret_addr(mdp, -in_bytes(SingleTypeEntry::size()));\n","filename":"src\/hotspot\/cpu\/s390\/interp_masm_s390.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -34,0 +34,1 @@\n+#include \"ci\/ciInlineKlass.hpp\"\n@@ -35,0 +36,1 @@\n+#include \"ci\/ciObjArrayKlass.hpp\"\n@@ -39,0 +41,1 @@\n+#include \"oops\/oop.inline.hpp\"\n@@ -431,1 +434,1 @@\n-  __ remove_frame(initial_frame_size_in_bytes());\n+  __ remove_frame(initial_frame_size_in_bytes(), needs_stack_repair());\n@@ -468,0 +471,44 @@\n+  if (InlineTypeReturnedAsFields) {\n+  #ifndef _LP64\n+     Unimplemented();\n+  #endif\n+    \/\/ Check if we are returning an non-null inline type and load its fields into registers\n+    ciType* return_type = compilation()->method()->return_type();\n+    if (return_type->is_inlinetype()) {\n+      ciInlineKlass* vk = return_type->as_inline_klass();\n+      if (vk->can_be_returned_as_fields()) {\n+        address unpack_handler = vk->unpack_handler();\n+        assert(unpack_handler != nullptr, \"must be\");\n+        __ call(RuntimeAddress(unpack_handler));\n+      }\n+    } else if (return_type->is_instance_klass() && (!return_type->is_loaded() || StressCallingConvention)) {\n+      Label skip;\n+      Label not_null;\n+      __ testptr(rax, rax);\n+      __ jcc(Assembler::notZero, not_null);\n+      \/\/ Returned value is null, zero all return registers because they may belong to oop fields\n+      __ xorq(j_rarg1, j_rarg1);\n+      __ xorq(j_rarg2, j_rarg2);\n+      __ xorq(j_rarg3, j_rarg3);\n+      __ xorq(j_rarg4, j_rarg4);\n+      __ xorq(j_rarg5, j_rarg5);\n+      __ jmp(skip);\n+      __ bind(not_null);\n+\n+      \/\/ Check if we are returning an non-null inline type and load its fields into registers\n+      __ test_oop_is_not_inline_type(rax, rscratch1, skip, \/* can_be_null= *\/ false);\n+\n+      \/\/ Load fields from a buffered value with an inline class specific handler\n+      __ load_klass(rdi, rax, rscratch1);\n+      __ movptr(rdi, Address(rdi, InstanceKlass::adr_inlineklass_fixed_block_offset()));\n+      __ movptr(rdi, Address(rdi, InlineKlass::unpack_handler_offset()));\n+      \/\/ Unpack handler can be null if inline type is not scalarizable in returns\n+      __ testptr(rdi, rdi);\n+      __ jcc(Assembler::zero, skip);\n+      __ call(rdi);\n+\n+      __ bind(skip);\n+    }\n+    \/\/ At this point, rax points to the value object (for interpreter or C1 caller).\n+    \/\/ The fields of the object are copied into registers (for C2 caller).\n+  }\n@@ -470,1 +517,1 @@\n-  __ remove_frame(initial_frame_size_in_bytes());\n+  __ remove_frame(initial_frame_size_in_bytes(), needs_stack_repair());\n@@ -486,0 +533,4 @@\n+int LIR_Assembler::store_inline_type_fields_to_buf(ciInlineKlass* vk) {\n+  return (__ store_inline_type_fields_to_buf(vk, false));\n+}\n+\n@@ -1224,1 +1275,1 @@\n-  if (UseSlowPath ||\n+  if (UseSlowPath || op->always_slow_path() ||\n@@ -1323,24 +1374,26 @@\n-  __ testptr(obj, obj);\n-  if (op->should_profile()) {\n-    Label not_null;\n-    Register mdo  = klass_RInfo;\n-    __ mov_metadata(mdo, md->constant_encoding());\n-    __ jccb(Assembler::notEqual, not_null);\n-    \/\/ Object is null; update MDO and exit\n-    Address data_addr(mdo, md->byte_offset_of_slot(data, DataLayout::flags_offset()));\n-    int header_bits = BitData::null_seen_byte_constant();\n-    __ orb(data_addr, header_bits);\n-    __ jmp(*obj_is_null);\n-    __ bind(not_null);\n-\n-    Label update_done;\n-    Register recv = k_RInfo;\n-    __ load_klass(recv, obj, tmp_load_klass);\n-    type_profile_helper(mdo, md, data, recv, &update_done);\n-\n-    Address nonprofiled_receiver_count_addr(mdo, md->byte_offset_of_slot(data, CounterData::count_offset()));\n-    __ addptr(nonprofiled_receiver_count_addr, DataLayout::counter_increment);\n-\n-    __ bind(update_done);\n-  } else {\n-    __ jcc(Assembler::equal, *obj_is_null);\n+  if (op->need_null_check()) {\n+    __ testptr(obj, obj);\n+    if (op->should_profile()) {\n+      Label not_null;\n+      Register mdo  = klass_RInfo;\n+      __ mov_metadata(mdo, md->constant_encoding());\n+      __ jccb(Assembler::notEqual, not_null);\n+      \/\/ Object is null; update MDO and exit\n+      Address data_addr(mdo, md->byte_offset_of_slot(data, DataLayout::flags_offset()));\n+      int header_bits = BitData::null_seen_byte_constant();\n+      __ orb(data_addr, header_bits);\n+      __ jmp(*obj_is_null);\n+      __ bind(not_null);\n+\n+      Label update_done;\n+      Register recv = k_RInfo;\n+      __ load_klass(recv, obj, tmp_load_klass);\n+      type_profile_helper(mdo, md, data, recv, &update_done);\n+\n+      Address nonprofiled_receiver_count_addr(mdo, md->byte_offset_of_slot(data, CounterData::count_offset()));\n+      __ addptr(nonprofiled_receiver_count_addr, DataLayout::counter_increment);\n+\n+      __ bind(update_done);\n+    } else {\n+      __ jcc(Assembler::equal, *obj_is_null);\n+    }\n@@ -1357,0 +1410,1 @@\n+    assert(!k->is_loaded() || !k->is_obj_array_klass(), \"Use refined array for a direct pointer comparison\");\n@@ -1381,1 +1435,8 @@\n-        __ cmpptr(klass_RInfo, k_RInfo);\n+        if (k->is_loaded() && k->is_obj_array_klass()) {\n+          \/\/ For a direct pointer comparison, we need the refined array klass pointer\n+          ciKlass* k_refined = ciObjArrayKlass::make(k->as_obj_array_klass()->element_klass());\n+          __ mov_metadata(tmp_load_klass, k_refined->constant_encoding());\n+          __ cmpptr(klass_RInfo, tmp_load_klass);\n+        } else {\n+          __ cmpptr(klass_RInfo, k_RInfo);\n+        }\n@@ -1516,0 +1577,103 @@\n+void LIR_Assembler::emit_opFlattenedArrayCheck(LIR_OpFlattenedArrayCheck* op) {\n+  \/\/ We are loading\/storing from\/to an array that *may* be a flat array (the\n+  \/\/ declared type is Object[], abstract[], interface[] or VT.ref[]).\n+  \/\/ If this array is a flat array, take the slow path.\n+  __ test_flat_array_oop(op->array()->as_register(), op->tmp()->as_register(), *op->stub()->entry());\n+  if (!op->value()->is_illegal()) {\n+    \/\/ TODO 8350865 This is also used for profiling code, right? And in that case we don't care about null but just want to know if the array is flat or not.\n+    \/\/ The array is not a flat array, but it might be null-free. If we are storing\n+    \/\/ a null into a null-free array, take the slow path (which will throw NPE).\n+    Label skip;\n+    __ cmpptr(op->value()->as_register(), NULL_WORD);\n+    __ jcc(Assembler::notEqual, skip);\n+    __ test_null_free_array_oop(op->array()->as_register(), op->tmp()->as_register(), *op->stub()->entry());\n+    __ bind(skip);\n+  }\n+}\n+\n+void LIR_Assembler::emit_opNullFreeArrayCheck(LIR_OpNullFreeArrayCheck* op) {\n+  \/\/ We are storing into an array that *may* be null-free (the declared type is\n+  \/\/ Object[], abstract[], interface[] or VT.ref[]).\n+  Label test_mark_word;\n+  Register tmp = op->tmp()->as_register();\n+  __ movptr(tmp, Address(op->array()->as_register(), oopDesc::mark_offset_in_bytes()));\n+  __ testl(tmp, markWord::unlocked_value);\n+  __ jccb(Assembler::notZero, test_mark_word);\n+  __ load_prototype_header(tmp, op->array()->as_register(), rscratch1);\n+  __ bind(test_mark_word);\n+  __ testl(tmp, markWord::null_free_array_bit_in_place);\n+}\n+\n+void LIR_Assembler::emit_opSubstitutabilityCheck(LIR_OpSubstitutabilityCheck* op) {\n+  Label L_oops_equal;\n+  Label L_oops_not_equal;\n+  Label L_end;\n+\n+  Register left  = op->left()->as_register();\n+  Register right = op->right()->as_register();\n+\n+  __ cmpptr(left, right);\n+  __ jcc(Assembler::equal, L_oops_equal);\n+\n+  \/\/ (1) Null check -- if one of the operands is null, the other must not be null (because\n+  \/\/     the two references are not equal), so they are not substitutable,\n+  \/\/     FIXME: do null check only if the operand is nullable\n+  __ testptr(left, right);\n+  __ jcc(Assembler::zero, L_oops_not_equal);\n+\n+  ciKlass* left_klass = op->left_klass();\n+  ciKlass* right_klass = op->right_klass();\n+\n+  \/\/ (2) Inline type check -- if either of the operands is not a inline type,\n+  \/\/     they are not substitutable. We do this only if we are not sure that the\n+  \/\/     operands are inline type\n+  if ((left_klass == nullptr || right_klass == nullptr) ||\/\/ The klass is still unloaded, or came from a Phi node.\n+      !left_klass->is_inlinetype() || !right_klass->is_inlinetype()) {\n+    Register tmp1  = op->tmp1()->as_register();\n+    __ movptr(tmp1, (intptr_t)markWord::inline_type_pattern);\n+    __ andptr(tmp1, Address(left, oopDesc::mark_offset_in_bytes()));\n+    __ andptr(tmp1, Address(right, oopDesc::mark_offset_in_bytes()));\n+    __ cmpptr(tmp1, (intptr_t)markWord::inline_type_pattern);\n+    __ jcc(Assembler::notEqual, L_oops_not_equal);\n+  }\n+\n+  \/\/ (3) Same klass check: if the operands are of different klasses, they are not substitutable.\n+  if (left_klass != nullptr && left_klass->is_inlinetype() && left_klass == right_klass) {\n+    \/\/ No need to load klass -- the operands are statically known to be the same inline klass.\n+    __ jmp(*op->stub()->entry());\n+  } else {\n+    Register left_klass_op = op->left_klass_op()->as_register();\n+    Register right_klass_op = op->right_klass_op()->as_register();\n+\n+    if (UseCompressedClassPointers) {\n+      __ movl(left_klass_op,  Address(left,  oopDesc::klass_offset_in_bytes()));\n+      __ movl(right_klass_op, Address(right, oopDesc::klass_offset_in_bytes()));\n+      __ cmpl(left_klass_op, right_klass_op);\n+    } else {\n+      __ movptr(left_klass_op,  Address(left,  oopDesc::klass_offset_in_bytes()));\n+      __ movptr(right_klass_op, Address(right, oopDesc::klass_offset_in_bytes()));\n+      __ cmpptr(left_klass_op, right_klass_op);\n+    }\n+\n+    __ jcc(Assembler::equal, *op->stub()->entry()); \/\/ same klass -> do slow check\n+    \/\/ fall through to L_oops_not_equal\n+  }\n+\n+  __ bind(L_oops_not_equal);\n+  move(op->not_equal_result(), op->result_opr());\n+  __ jmp(L_end);\n+\n+  __ bind(L_oops_equal);\n+  move(op->equal_result(), op->result_opr());\n+  __ jmp(L_end);\n+\n+  \/\/ We've returned from the stub. RAX contains 0x0 IFF the two\n+  \/\/ operands are not substitutable. (Don't compare against 0x1 in case the\n+  \/\/ C compiler is naughty)\n+  __ bind(*op->stub()->continuation());\n+  __ cmpl(rax, 0);\n+  __ jcc(Assembler::equal, L_oops_not_equal); \/\/ (call_stub() == 0x0) -> not_equal\n+  move(op->equal_result(), op->result_opr()); \/\/ (call_stub() != 0x0) -> equal\n+  \/\/ fall-through\n+  __ bind(L_end);\n+}\n@@ -1561,0 +1725,15 @@\n+void LIR_Assembler::move(LIR_Opr src, LIR_Opr dst) {\n+  assert(dst->is_cpu_register(), \"must be\");\n+  assert(dst->type() == src->type(), \"must be\");\n+\n+  if (src->is_cpu_register()) {\n+    reg2reg(src, dst);\n+  } else if (src->is_stack()) {\n+    stack2reg(src, dst, dst->type());\n+  } else if (src->is_constant()) {\n+    const2reg(src, dst, lir_patch_none, nullptr);\n+  } else {\n+    ShouldNotReachHere();\n+  }\n+}\n+\n@@ -2171,1 +2350,1 @@\n-  add_call_info(code_offset(), op->info());\n+  add_call_info(code_offset(), op->info(), op->maybe_return_as_fields());\n@@ -2178,1 +2357,1 @@\n-  add_call_info(code_offset(), op->info());\n+  add_call_info(code_offset(), op->info(), op->maybe_return_as_fields());\n@@ -2345,0 +2524,15 @@\n+void LIR_Assembler::arraycopy_inlinetype_check(Register obj, Register tmp, CodeStub* slow_path, bool is_dest, bool null_check) {\n+  if (null_check) {\n+    __ testptr(obj, obj);\n+    __ jcc(Assembler::zero, *slow_path->entry());\n+  }\n+  if (is_dest) {\n+    __ test_null_free_array_oop(obj, tmp, *slow_path->entry());\n+    \/\/ TODO 8350865 Flat no longer implies null-free, so we need to check for flat dest. Can we do better here?\n+    __ test_flat_array_oop(obj, tmp, *slow_path->entry());\n+  } else {\n+    __ test_flat_array_oop(obj, tmp, *slow_path->entry());\n+  }\n+}\n+\n+\n@@ -2364,0 +2558,6 @@\n+  if (flags & LIR_OpArrayCopy::always_slow_path) {\n+    __ jmp(*stub->entry());\n+    __ bind(*stub->continuation());\n+    return;\n+  }\n+\n@@ -2440,0 +2640,8 @@\n+  \/\/ Handle inline type arrays\n+  if (flags & LIR_OpArrayCopy::src_inlinetype_check) {\n+    arraycopy_inlinetype_check(src, tmp, stub, false, (flags & LIR_OpArrayCopy::src_null_check));\n+  }\n+  if (flags & LIR_OpArrayCopy::dst_inlinetype_check) {\n+    arraycopy_inlinetype_check(dst, tmp, stub, true, (flags & LIR_OpArrayCopy::dst_null_check));\n+  }\n+\n@@ -2666,0 +2874,1 @@\n+\n@@ -3002,0 +3211,21 @@\n+void LIR_Assembler::emit_profile_inline_type(LIR_OpProfileInlineType* op) {\n+  Register obj = op->obj()->as_register();\n+  Register tmp = op->tmp()->as_pointer_register();\n+  Address mdo_addr = as_Address(op->mdp()->as_address_ptr());\n+  bool not_null = op->not_null();\n+  int flag = op->flag();\n+\n+  Label not_inline_type;\n+  if (!not_null) {\n+    __ testptr(obj, obj);\n+    __ jccb(Assembler::zero, not_inline_type);\n+  }\n+\n+  __ test_oop_is_not_inline_type(obj, tmp, not_inline_type);\n+\n+  __ orb(mdo_addr, flag);\n+\n+  __ bind(not_inline_type);\n+}\n+\n+\n@@ -3187,0 +3417,3 @@\n+void LIR_Assembler::check_orig_pc() {\n+  __ cmpptr(frame_map()->address_for_orig_pc_addr(), NULL_WORD);\n+}\n","filename":"src\/hotspot\/cpu\/x86\/c1_LIRAssembler_x86.cpp","additions":263,"deletions":30,"binary":false,"changes":293,"status":"modified"},{"patch":"@@ -54,0 +54,3 @@\n+  void arraycopy_inlinetype_check(Register obj, Register tmp, CodeStub* slow_path, bool is_dest, bool null_check);\n+  void move(LIR_Opr src, LIR_Opr dst);\n+\n","filename":"src\/hotspot\/cpu\/x86\/c1_LIRAssembler_x86.hpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -37,0 +37,1 @@\n+#include \"runtime\/frame.inline.hpp\"\n@@ -85,1 +86,4 @@\n-  if (UseCompactObjectHeaders) {\n+  if (UseCompactObjectHeaders || EnableValhalla) {\n+    \/\/ COH: Markword contains class pointer which is only known at runtime.\n+    \/\/ Valhalla: Could have value class which has a different prototype header to a normal object.\n+    \/\/ In both cases, we need to fetch dynamically.\n@@ -88,5 +92,1 @@\n-  } else if (UseCompressedClassPointers) { \/\/ Take care not to kill klass\n-    movptr(Address(obj, oopDesc::mark_offset_in_bytes()), checked_cast<int32_t>(markWord::prototype().value()));\n-    movptr(t1, klass);\n-    encode_klass_not_null(t1, rscratch1);\n-    movl(Address(obj, oopDesc::klass_offset_in_bytes()), t1);\n+    \/\/ Otherwise: Can use the statically computed prototype header which is the same for every object.\n@@ -95,1 +95,11 @@\n-    movptr(Address(obj, oopDesc::klass_offset_in_bytes()), klass);\n+  }\n+  if (!UseCompactObjectHeaders) {\n+    \/\/ COH: Markword already contains class pointer. Nothing else to do.\n+    \/\/ Otherwise: Fetch klass pointer following the markword\n+    if (UseCompressedClassPointers) { \/\/ Take care not to kill klass\n+      movptr(t1, klass);\n+      encode_klass_not_null(t1, rscratch1);\n+      movl(Address(obj, oopDesc::klass_offset_in_bytes()), t1);\n+    } else {\n+      movptr(Address(obj, oopDesc::klass_offset_in_bytes()), klass);\n+    }\n@@ -225,2 +235,20 @@\n-void C1_MacroAssembler::build_frame(int frame_size_in_bytes, int bang_size_in_bytes) {\n-  assert(bang_size_in_bytes >= frame_size_in_bytes, \"stack bang size incorrect\");\n+void C1_MacroAssembler::build_frame_helper(int frame_size_in_bytes, int sp_offset_for_orig_pc, int sp_inc, bool reset_orig_pc, bool needs_stack_repair) {\n+  push(rbp);\n+  if (PreserveFramePointer) {\n+    mov(rbp, rsp);\n+  }\n+  decrement(rsp, frame_size_in_bytes);\n+\n+  if (needs_stack_repair) {\n+    \/\/ Save stack increment (also account for fixed framesize and rbp)\n+    assert((sp_inc & (StackAlignmentInBytes-1)) == 0, \"stack increment not aligned\");\n+    int real_frame_size = sp_inc + frame_size_in_bytes + wordSize;\n+    movptr(Address(rsp, frame_size_in_bytes - wordSize), real_frame_size);\n+  }\n+  if (reset_orig_pc) {\n+    \/\/ Zero orig_pc to detect deoptimization during buffering in the entry points\n+    movptr(Address(rsp, sp_offset_for_orig_pc), 0);\n+  }\n+}\n+\n+void C1_MacroAssembler::build_frame(int frame_size_in_bytes, int bang_size_in_bytes, int sp_offset_for_orig_pc, bool needs_stack_repair, bool has_scalarized_args, Label* verified_inline_entry_label) {\n@@ -232,0 +260,1 @@\n+  assert(bang_size_in_bytes >= frame_size_in_bytes, \"stack bang size incorrect\");\n@@ -234,5 +263,1 @@\n-  push(rbp);\n-  if (PreserveFramePointer) {\n-    mov(rbp, rsp);\n-  }\n-  decrement(rsp, frame_size_in_bytes); \/\/ does not emit code for frame_size == 0\n+  build_frame_helper(frame_size_in_bytes, sp_offset_for_orig_pc, 0, has_scalarized_args, needs_stack_repair);\n@@ -243,5 +268,4 @@\n-}\n-\n-void C1_MacroAssembler::remove_frame(int frame_size_in_bytes) {\n-  increment(rsp, frame_size_in_bytes);  \/\/ Does not emit code for frame_size == 0\n-  pop(rbp);\n+  if (verified_inline_entry_label != nullptr) {\n+    \/\/ Jump here from the scalarized entry points that already created the frame.\n+    bind(*verified_inline_entry_label);\n+  }\n@@ -251,1 +275,0 @@\n-\n@@ -257,0 +280,58 @@\n+int C1_MacroAssembler::scalarized_entry(const CompiledEntrySignature* ces, int frame_size_in_bytes, int bang_size_in_bytes, int sp_offset_for_orig_pc, Label& verified_inline_entry_label, bool is_inline_ro_entry) {\n+  assert(InlineTypePassFieldsAsArgs, \"sanity\");\n+  \/\/ Make sure there is enough stack space for this method's activation.\n+  assert(bang_size_in_bytes >= frame_size_in_bytes, \"stack bang size incorrect\");\n+  generate_stack_overflow_check(bang_size_in_bytes);\n+\n+  GrowableArray<SigEntry>* sig    = ces->sig();\n+  GrowableArray<SigEntry>* sig_cc = is_inline_ro_entry ? ces->sig_cc_ro() : ces->sig_cc();\n+  VMRegPair* regs      = ces->regs();\n+  VMRegPair* regs_cc   = is_inline_ro_entry ? ces->regs_cc_ro() : ces->regs_cc();\n+  int args_on_stack    = ces->args_on_stack();\n+  int args_on_stack_cc = is_inline_ro_entry ? ces->args_on_stack_cc_ro() : ces->args_on_stack_cc();\n+\n+  assert(sig->length() <= sig_cc->length(), \"Zero-sized inline class not allowed!\");\n+  BasicType* sig_bt = NEW_RESOURCE_ARRAY(BasicType, sig_cc->length());\n+  int args_passed = sig->length();\n+  int args_passed_cc = SigEntry::fill_sig_bt(sig_cc, sig_bt);\n+\n+  \/\/ Create a temp frame so we can call into the runtime. It must be properly set up to accommodate GC.\n+  build_frame_helper(frame_size_in_bytes, sp_offset_for_orig_pc, 0, true, ces->c1_needs_stack_repair());\n+\n+  \/\/ The runtime call might safepoint, make sure nmethod entry barrier is executed\n+  BarrierSetAssembler* bs = BarrierSet::barrier_set()->barrier_set_assembler();\n+  \/\/ C1 code is not hot enough to micro optimize the nmethod entry barrier with an out-of-line stub\n+  bs->nmethod_entry_barrier(this, nullptr \/* slow_path *\/, nullptr \/* continuation *\/);\n+\n+  \/\/ FIXME -- call runtime only if we cannot in-line allocate all the incoming inline type args.\n+  movptr(rbx, (intptr_t)(ces->method()));\n+  if (is_inline_ro_entry) {\n+    call(RuntimeAddress(Runtime1::entry_for(StubId::c1_buffer_inline_args_no_receiver_id)));\n+  } else {\n+    call(RuntimeAddress(Runtime1::entry_for(StubId::c1_buffer_inline_args_id)));\n+  }\n+  int rt_call_offset = offset();\n+\n+  \/\/ Remove the temp frame\n+  addptr(rsp, frame_size_in_bytes);\n+  pop(rbp);\n+\n+  \/\/ Check if we need to extend the stack for packing\n+  int sp_inc = 0;\n+  if (args_on_stack > args_on_stack_cc) {\n+    sp_inc = extend_stack_for_inline_args(args_on_stack);\n+  }\n+\n+  shuffle_inline_args(true, is_inline_ro_entry, sig_cc,\n+                      args_passed_cc, args_on_stack_cc, regs_cc, \/\/ from\n+                      args_passed, args_on_stack, regs,          \/\/ to\n+                      sp_inc, rax);\n+\n+  \/\/ Create the real frame. Below jump will then skip over the stack banging and frame\n+  \/\/ setup code in the verified_inline_entry (which has a different real_frame_size).\n+  build_frame_helper(frame_size_in_bytes, sp_offset_for_orig_pc, sp_inc, false, ces->c1_needs_stack_repair());\n+\n+  jmp(verified_inline_entry_label);\n+  return rt_call_offset;\n+}\n+\n","filename":"src\/hotspot\/cpu\/x86\/c1_MacroAssembler_x86.cpp","additions":101,"deletions":20,"binary":false,"changes":121,"status":"modified"},{"patch":"@@ -52,1 +52,21 @@\n-void C2_MacroAssembler::verified_entry(int framesize, int stack_bang_size, bool fp_mode_24b, bool is_stub) {\n+void C2_MacroAssembler::verified_entry(Compile* C, int sp_inc) {\n+  if (C->clinit_barrier_on_entry()) {\n+    assert(VM_Version::supports_fast_class_init_checks(), \"sanity\");\n+    assert(!C->method()->holder()->is_not_initialized(), \"initialization should have been started\");\n+\n+    Label L_skip_barrier;\n+    Register klass = rscratch1;\n+\n+    mov_metadata(klass, C->method()->holder()->constant_encoding());\n+    clinit_barrier(klass, &L_skip_barrier \/*L_fast_path*\/);\n+\n+    jump(RuntimeAddress(SharedRuntime::get_handle_wrong_method_stub())); \/\/ slow path\n+\n+    bind(L_skip_barrier);\n+  }\n+\n+  int framesize = C->output()->frame_size_in_bytes();\n+  int bangsize = C->output()->bang_size_in_bytes();\n+  bool fp_mode_24b = false;\n+  int stack_bang_size = C->output()->need_stack_bang(bangsize) ? bangsize : 0;\n+\n@@ -97,0 +117,6 @@\n+  if (C->needs_stack_repair()) {\n+    \/\/ Save stack increment just below the saved rbp (also account for fixed framesize and rbp)\n+    assert((sp_inc & (StackAlignmentInBytes-1)) == 0, \"stack increment not aligned\");\n+    movptr(Address(rsp, framesize - wordSize), sp_inc + framesize + wordSize);\n+  }\n+\n@@ -115,0 +141,1 @@\n+}\n@@ -116,15 +143,13 @@\n-  if (!is_stub) {\n-    BarrierSetAssembler* bs = BarrierSet::barrier_set()->barrier_set_assembler();\n-    \/\/ We put the non-hot code of the nmethod entry barrier out-of-line in a stub.\n-    Label dummy_slow_path;\n-    Label dummy_continuation;\n-    Label* slow_path = &dummy_slow_path;\n-    Label* continuation = &dummy_continuation;\n-    if (!Compile::current()->output()->in_scratch_emit_size()) {\n-      \/\/ Use real labels from actual stub when not emitting code for the purpose of measuring its size\n-      C2EntryBarrierStub* stub = new (Compile::current()->comp_arena()) C2EntryBarrierStub();\n-      Compile::current()->output()->add_stub(stub);\n-      slow_path = &stub->entry();\n-      continuation = &stub->continuation();\n-    }\n-    bs->nmethod_entry_barrier(this, slow_path, continuation);\n+void C2_MacroAssembler::entry_barrier() {\n+  BarrierSetAssembler* bs = BarrierSet::barrier_set()->barrier_set_assembler();\n+  \/\/ We put the non-hot code of the nmethod entry barrier out-of-line in a stub.\n+  Label dummy_slow_path;\n+  Label dummy_continuation;\n+  Label* slow_path = &dummy_slow_path;\n+  Label* continuation = &dummy_continuation;\n+  if (!Compile::current()->output()->in_scratch_emit_size()) {\n+    \/\/ Use real labels from actual stub when not emitting code for the purpose of measuring its size\n+    C2EntryBarrierStub* stub = new (Compile::current()->comp_arena()) C2EntryBarrierStub();\n+    Compile::current()->output()->add_stub(stub);\n+    slow_path = &stub->entry();\n+    continuation = &stub->continuation();\n@@ -132,0 +157,1 @@\n+  bs->nmethod_entry_barrier(this, slow_path, continuation);\n","filename":"src\/hotspot\/cpu\/x86\/c2_MacroAssembler_x86.cpp","additions":42,"deletions":16,"binary":false,"changes":58,"status":"modified"},{"patch":"@@ -32,1 +32,1 @@\n-  void verified_entry(int framesize, int stack_bang_size, bool fp_mode_24b, bool is_stub);\n+  void verified_entry(Compile* C, int sp_inc = 0);\n@@ -34,0 +34,1 @@\n+  void entry_barrier();\n","filename":"src\/hotspot\/cpu\/x86\/c2_MacroAssembler_x86.hpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -35,0 +35,3 @@\n+#ifdef COMPILER1\n+#include \"c1\/c1_Runtime1.hpp\"\n+#endif\n@@ -432,2 +435,3 @@\n-  \/\/ On Intel the return_address is always the word on the stack\n-  address sender_pc = (address) *(sender_sp-1);\n+#ifdef ASSERT\n+  address sender_pc_copy = (address) *(sender_sp-1);\n+#endif\n@@ -440,0 +444,16 @@\n+  \/\/ Repair the sender sp if the frame has been extended\n+  sender_sp = repair_sender_sp(sender_sp, saved_fp_addr);\n+\n+  \/\/ On Intel the return_address is always the word on the stack\n+  address sender_pc = (address) *(sender_sp-1);\n+\n+#ifdef ASSERT\n+  if (sender_pc != sender_pc_copy) {\n+    \/\/ When extending the stack in the callee method entry to make room for unpacking of value\n+    \/\/ type args, we keep a copy of the sender pc at the expected location in the callee frame.\n+    \/\/ If the sender pc is patched due to deoptimization, the copy is not consistent anymore.\n+    nmethod* nm = CodeCache::find_blob(sender_pc)->as_nmethod();\n+    assert(sender_pc == nm->deopt_handler_begin(), \"unexpected sender pc\");\n+  }\n+#endif\n+\n@@ -444,2 +464,21 @@\n-    if (!_cb->is_nmethod()) { \/\/ compiled frames do not use callee-saved registers\n-      map->set_include_argument_oops(_cb->caller_must_gc_arguments(map->thread()));\n+    bool c1_buffering = false;\n+#ifdef COMPILER1\n+    nmethod* nm = _cb->as_nmethod_or_null();\n+    if (nm != nullptr && nm->is_compiled_by_c1() && nm->method()->has_scalarized_args() &&\n+        pc() < nm->verified_inline_entry_point()) {\n+      \/\/ TODO 8284443 Can't we do that by not passing 'dont_gc_arguments' in case 'StubId::c1_buffer_inline_args_id' in 'Runtime1::generate_code_for'?\n+      \/\/ The VEP and VIEP(RO) of C1-compiled methods call buffer_inline_args_xxx\n+      \/\/ before doing any argument shuffling, so we need to scan the oops\n+      \/\/ as the caller passes them.\n+      c1_buffering = true;\n+#ifdef ASSERT\n+      NativeCall* call = nativeCall_before(pc());\n+      address dest = call->destination();\n+      assert(dest == Runtime1::entry_for(StubId::c1_buffer_inline_args_no_receiver_id) ||\n+             dest == Runtime1::entry_for(StubId::c1_buffer_inline_args_id), \"unexpected safepoint in entry point\");\n+#endif\n+    }\n+#endif\n+    if (!_cb->is_nmethod() || c1_buffering) { \/\/ compiled frames do not use callee-saved registers\n+      bool caller_args = _cb->caller_must_gc_arguments(map->thread()) || c1_buffering;\n+      map->set_include_argument_oops(caller_args);\n","filename":"src\/hotspot\/cpu\/x86\/frame_x86.inline.hpp","additions":43,"deletions":4,"binary":false,"changes":47,"status":"modified"},{"patch":"@@ -31,0 +31,1 @@\n+#include \"oops\/constMethodFlags.hpp\"\n@@ -34,0 +35,1 @@\n+#include \"oops\/inlineKlass.hpp\"\n@@ -168,1 +170,1 @@\n-        assert(ReturnTypeEntry::static_cell_count() < TypeStackSlotEntries::per_arg_count(), \"can't move past ret type\");\n+        assert(SingleTypeEntry::static_cell_count() < TypeStackSlotEntries::per_arg_count(), \"can't move past ret type\");\n@@ -213,1 +215,1 @@\n-    Address mdo_ret_addr(mdp, -in_bytes(ReturnTypeEntry::size()));\n+    Address mdo_ret_addr(mdp, -in_bytes(SingleTypeEntry::size()));\n@@ -585,1 +587,2 @@\n-                                                  Label& ok_is_subtype) {\n+                                                  Label& ok_is_subtype,\n+                                                  bool profile) {\n@@ -593,1 +596,3 @@\n-  profile_typecheck(rcx, Rsub_klass, rdi); \/\/ blows rcx, reloads rdi\n+  if (profile) {\n+    profile_typecheck(rcx, Rsub_klass, rdi); \/\/ blows rcx, reloads rdi\n+  }\n@@ -887,1 +892,1 @@\n- \/\/ get method access flags\n+  \/\/ get method access flags\n@@ -1026,4 +1031,2 @@\n-  \/\/ remove activation\n-  \/\/ get sender sp\n-  movptr(rbx,\n-         Address(rbp, frame::interpreter_frame_sender_sp_offset * wordSize));\n+    movptr(rbx,\n+               Address(rbp, frame::interpreter_frame_sender_sp_offset * wordSize));\n@@ -1054,0 +1057,52 @@\n+  \/\/ remove activation\n+  \/\/ get sender sp\n+  movptr(rbx,\n+         Address(rbp, frame::interpreter_frame_sender_sp_offset * wordSize));\n+\n+  if (state == atos && InlineTypeReturnedAsFields) {\n+    Label skip;\n+    Label not_null;\n+    testptr(rax, rax);\n+    jcc(Assembler::notZero, not_null);\n+    \/\/ Returned value is null, zero all return registers because they may belong to oop fields\n+    xorq(j_rarg1, j_rarg1);\n+    xorq(j_rarg2, j_rarg2);\n+    xorq(j_rarg3, j_rarg3);\n+    xorq(j_rarg4, j_rarg4);\n+    xorq(j_rarg5, j_rarg5);\n+    jmp(skip);\n+    bind(not_null);\n+\n+    \/\/ Check if we are returning an non-null inline type and load its fields into registers\n+    test_oop_is_not_inline_type(rax, rscratch1, skip, \/* can_be_null= *\/ false);\n+\n+#ifndef _LP64\n+    super_call_VM_leaf(StubRoutines::load_inline_type_fields_in_regs());\n+#else\n+    \/\/ Load fields from a buffered value with an inline class specific handler\n+    load_klass(rdi, rax, rscratch1);\n+    movptr(rdi, Address(rdi, InstanceKlass::adr_inlineklass_fixed_block_offset()));\n+    movptr(rdi, Address(rdi, InlineKlass::unpack_handler_offset()));\n+    \/\/ Unpack handler can be null if inline type is not scalarizable in returns\n+    testptr(rdi, rdi);\n+    jcc(Assembler::zero, skip);\n+    call(rdi);\n+#endif\n+#ifdef ASSERT\n+    \/\/ TODO 8284443 Enable\n+    if (StressCallingConvention && false) {\n+      Label skip_stress;\n+      movptr(rscratch1, Address(rbp, frame::interpreter_frame_method_offset * wordSize));\n+      movl(rscratch1, Address(rscratch1, Method::flags_offset()));\n+      testl(rcx, MethodFlags::has_scalarized_return_flag());\n+      jcc(Assembler::zero, skip_stress);\n+      load_klass(rax, rax, rscratch1);\n+      orptr(rax, 1);\n+      bind(skip_stress);\n+    }\n+#endif\n+    \/\/ call above kills the value in rbx. Reload it.\n+    movptr(rbx, Address(rbp, frame::interpreter_frame_sender_sp_offset * wordSize));\n+    bind(skip);\n+  }\n+\n@@ -1090,0 +1145,89 @@\n+void InterpreterMacroAssembler::allocate_instance(Register klass, Register new_obj,\n+                                                  Register t1, Register t2,\n+                                                  bool clear_fields, Label& alloc_failed) {\n+  MacroAssembler::allocate_instance(klass, new_obj, t1, t2, clear_fields, alloc_failed);\n+  if (DTraceAllocProbes) {\n+    \/\/ Trigger dtrace event for fastpath\n+    push(atos);\n+    call_VM_leaf(CAST_FROM_FN_PTR(address, static_cast<int (*)(oopDesc*)>(SharedRuntime::dtrace_object_alloc)), new_obj);\n+    pop(atos);\n+  }\n+}\n+\n+void InterpreterMacroAssembler::read_flat_field(Register entry, Register tmp1, Register tmp2, Register obj) {\n+  Label alloc_failed, slow_path, done;\n+  const Register alloc_temp = LP64_ONLY(rscratch1) NOT_LP64(rsi);\n+  const Register dst_temp   = LP64_ONLY(rscratch2) NOT_LP64(rdi);\n+  assert_different_registers(obj, entry, tmp1, tmp2, dst_temp, r8, r9);\n+\n+  \/\/ If the field is nullable, jump to slow path\n+  load_unsigned_byte(tmp1, Address(entry, in_bytes(ResolvedFieldEntry::flags_offset())));\n+  testl(tmp1, 1 << ResolvedFieldEntry::is_null_free_inline_type_shift);\n+  jcc(Assembler::equal, slow_path);\n+\n+  \/\/ Grap the inline field klass\n+  const Register field_klass = tmp1;\n+  load_unsigned_short(tmp2, Address(entry, in_bytes(ResolvedFieldEntry::field_index_offset())));\n+\n+  movptr(tmp1, Address(entry, ResolvedFieldEntry::field_holder_offset()));\n+  get_inline_type_field_klass(tmp1, tmp2, field_klass);\n+\n+  \/\/ allocate buffer\n+  push(obj);  \/\/ push object being read from\n+  allocate_instance(field_klass, obj, alloc_temp, dst_temp, false, alloc_failed);\n+\n+  \/\/ Have an oop instance buffer, copy into it\n+  load_unsigned_short(r9, Address(entry, in_bytes(ResolvedFieldEntry::field_index_offset())));\n+  movptr(r8, Address(entry, in_bytes(ResolvedFieldEntry::field_holder_offset())));\n+  inline_layout_info(r8, r9, r8); \/\/ holder, index, info => InlineLayoutInfo into r8\n+\n+  payload_addr(obj, dst_temp, field_klass);\n+  pop(alloc_temp);             \/\/ restore object being read from\n+  load_sized_value(tmp2, Address(entry, in_bytes(ResolvedFieldEntry::field_offset_offset())), sizeof(int), true \/*is_signed*\/);\n+  lea(tmp2, Address(alloc_temp, tmp2));\n+  \/\/ call_VM_leaf, clobbers a few regs, save restore new obj\n+  push(obj);\n+  flat_field_copy(IS_DEST_UNINITIALIZED, tmp2, dst_temp, r8);\n+  pop(obj);\n+  jmp(done);\n+\n+  bind(alloc_failed);\n+  pop(obj);\n+  bind(slow_path);\n+  call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::read_flat_field),\n+          obj, entry);\n+  get_vm_result_oop(obj);\n+  bind(done);\n+}\n+\n+void InterpreterMacroAssembler::write_flat_field(Register entry, Register tmp1, Register tmp2,\n+                                                 Register obj, Register off, Register value) {\n+  assert_different_registers(entry, tmp1, tmp2, obj, off, value);\n+\n+  Label slow_path, done;\n+\n+  load_unsigned_byte(tmp2, Address(entry, in_bytes(ResolvedFieldEntry::flags_offset())));\n+  test_field_is_not_null_free_inline_type(tmp2, tmp1, slow_path);\n+\n+  null_check(value); \/\/ FIXME JDK-8341120\n+\n+  lea(obj, Address(obj, off, Address::times_1));\n+\n+  load_klass(tmp2, value, tmp1);\n+  payload_addr(value, value, tmp2);\n+\n+  Register idx = tmp1;\n+  load_unsigned_short(idx, Address(entry, in_bytes(ResolvedFieldEntry::field_index_offset())));\n+  movptr(tmp2, Address(entry, in_bytes(ResolvedFieldEntry::field_holder_offset())));\n+\n+  Register layout_info = off;\n+  inline_layout_info(tmp2, idx, layout_info);\n+\n+  flat_field_copy(IN_HEAP, value, obj, layout_info);\n+  jmp(done);\n+\n+  bind(slow_path);\n+  call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::write_flat_field), obj, value, entry);\n+\n+  bind(done);\n+}\n@@ -1342,1 +1486,1 @@\n-void InterpreterMacroAssembler::profile_not_taken_branch(Register mdp) {\n+void InterpreterMacroAssembler::profile_not_taken_branch(Register mdp, bool acmp) {\n@@ -1354,1 +1498,1 @@\n-    update_mdp_by_constant(mdp, in_bytes(BranchData::branch_data_size()));\n+    update_mdp_by_constant(mdp, acmp ? in_bytes(ACmpData::acmp_data_size()): in_bytes(BranchData::branch_data_size()));\n@@ -1417,1 +1561,1 @@\n-    record_klass_in_profile(receiver, mdp, reg2, true);\n+    record_klass_in_profile(receiver, mdp, reg2);\n@@ -1437,4 +1581,3 @@\n-void InterpreterMacroAssembler::record_klass_in_profile_helper(\n-                                        Register receiver, Register mdp,\n-                                        Register reg2, int start_row,\n-                                        Label& done, bool is_virtual_call) {\n+void InterpreterMacroAssembler::record_klass_in_profile_helper(Register receiver, Register mdp,\n+                                                               Register reg2, int start_row,\n+                                                               Label& done) {\n@@ -1544,3 +1687,1 @@\n-void InterpreterMacroAssembler::record_klass_in_profile(Register receiver,\n-                                                        Register mdp, Register reg2,\n-                                                        bool is_virtual_call) {\n+void InterpreterMacroAssembler::record_klass_in_profile(Register receiver, Register mdp, Register reg2) {\n@@ -1550,1 +1691,1 @@\n-  record_klass_in_profile_helper(receiver, mdp, reg2, 0, done, is_virtual_call);\n+  record_klass_in_profile_helper(receiver, mdp, reg2, 0, done);\n@@ -1627,1 +1768,1 @@\n-      record_klass_in_profile(klass, mdp, reg2, false);\n+      record_klass_in_profile(klass, mdp, reg2);\n@@ -1687,0 +1828,114 @@\n+template <class ArrayData> void InterpreterMacroAssembler::profile_array_type(Register mdp,\n+                                                                              Register array,\n+                                                                              Register tmp) {\n+  if (ProfileInterpreter) {\n+    Label profile_continue;\n+\n+    \/\/ If no method data exists, go to profile_continue.\n+    test_method_data_pointer(mdp, profile_continue);\n+\n+    mov(tmp, array);\n+    profile_obj_type(tmp, Address(mdp, in_bytes(ArrayData::array_offset())));\n+\n+    Label not_flat;\n+    test_non_flat_array_oop(array, tmp, not_flat);\n+\n+    set_mdp_flag_at(mdp, ArrayData::flat_array_byte_constant());\n+\n+    bind(not_flat);\n+\n+    Label not_null_free;\n+    test_non_null_free_array_oop(array, tmp, not_null_free);\n+\n+    set_mdp_flag_at(mdp, ArrayData::null_free_array_byte_constant());\n+\n+    bind(not_null_free);\n+\n+    bind(profile_continue);\n+  }\n+}\n+\n+template void InterpreterMacroAssembler::profile_array_type<ArrayLoadData>(Register mdp,\n+                                                                           Register array,\n+                                                                           Register tmp);\n+template void InterpreterMacroAssembler::profile_array_type<ArrayStoreData>(Register mdp,\n+                                                                            Register array,\n+                                                                            Register tmp);\n+\n+\n+void InterpreterMacroAssembler::profile_multiple_element_types(Register mdp, Register element, Register tmp, const Register tmp2) {\n+  if (ProfileInterpreter) {\n+    Label profile_continue;\n+\n+    \/\/ If no method data exists, go to profile_continue.\n+    test_method_data_pointer(mdp, profile_continue);\n+\n+    Label done, update;\n+    testptr(element, element);\n+    jccb(Assembler::notZero, update);\n+    set_mdp_flag_at(mdp, BitData::null_seen_byte_constant());\n+    jmp(done);\n+\n+    bind(update);\n+    load_klass(tmp, element, rscratch1);\n+\n+    \/\/ Record the object type.\n+    record_klass_in_profile(tmp, mdp, tmp2);\n+\n+    bind(done);\n+\n+    \/\/ The method data pointer needs to be updated.\n+    update_mdp_by_constant(mdp, in_bytes(ArrayStoreData::array_store_data_size()));\n+\n+    bind(profile_continue);\n+  }\n+}\n+\n+void InterpreterMacroAssembler::profile_element_type(Register mdp,\n+                                                     Register element,\n+                                                     Register tmp) {\n+  if (ProfileInterpreter) {\n+    Label profile_continue;\n+\n+    \/\/ If no method data exists, go to profile_continue.\n+    test_method_data_pointer(mdp, profile_continue);\n+\n+    mov(tmp, element);\n+    profile_obj_type(tmp, Address(mdp, in_bytes(ArrayLoadData::element_offset())));\n+\n+    \/\/ The method data pointer needs to be updated.\n+    update_mdp_by_constant(mdp, in_bytes(ArrayLoadData::array_load_data_size()));\n+\n+    bind(profile_continue);\n+  }\n+}\n+\n+void InterpreterMacroAssembler::profile_acmp(Register mdp,\n+                                             Register left,\n+                                             Register right,\n+                                             Register tmp) {\n+  if (ProfileInterpreter) {\n+    Label profile_continue;\n+\n+    \/\/ If no method data exists, go to profile_continue.\n+    test_method_data_pointer(mdp, profile_continue);\n+\n+    mov(tmp, left);\n+    profile_obj_type(tmp, Address(mdp, in_bytes(ACmpData::left_offset())));\n+\n+    Label left_not_inline_type;\n+    test_oop_is_not_inline_type(left, tmp, left_not_inline_type);\n+    set_mdp_flag_at(mdp, ACmpData::left_inline_type_byte_constant());\n+    bind(left_not_inline_type);\n+\n+    mov(tmp, right);\n+    profile_obj_type(tmp, Address(mdp, in_bytes(ACmpData::right_offset())));\n+\n+    Label right_not_inline_type;\n+    test_oop_is_not_inline_type(right, tmp, right_not_inline_type);\n+    set_mdp_flag_at(mdp, ACmpData::right_inline_type_byte_constant());\n+    bind(right_not_inline_type);\n+\n+    bind(profile_continue);\n+  }\n+}\n","filename":"src\/hotspot\/cpu\/x86\/interp_masm_x86.cpp","additions":276,"deletions":21,"binary":false,"changes":297,"status":"modified"},{"patch":"@@ -31,0 +31,1 @@\n+#include \"ci\/ciInlineKlass.hpp\"\n@@ -46,0 +47,1 @@\n+#include \"oops\/resolvedFieldEntry.hpp\"\n@@ -56,0 +58,1 @@\n+#include \"runtime\/signature_cc.hpp\"\n@@ -59,0 +62,4 @@\n+#include \"vmreg_x86.inline.hpp\"\n+#ifdef COMPILER2\n+#include \"opto\/output.hpp\"\n+#endif\n@@ -1306,0 +1313,4 @@\n+void MacroAssembler::super_call_VM_leaf(address entry_point) {\n+  MacroAssembler::call_VM_leaf_base(entry_point, 1);\n+}\n+\n@@ -2359,0 +2370,107 @@\n+void MacroAssembler::test_markword_is_inline_type(Register markword, Label& is_inline_type) {\n+  andptr(markword, markWord::inline_type_mask_in_place);\n+  cmpptr(markword, markWord::inline_type_pattern);\n+  jcc(Assembler::equal, is_inline_type);\n+}\n+\n+void MacroAssembler::test_oop_is_not_inline_type(Register object, Register tmp, Label& not_inline_type, bool can_be_null) {\n+  if (can_be_null) {\n+    testptr(object, object);\n+    jcc(Assembler::zero, not_inline_type);\n+  }\n+  const int is_inline_type_mask = markWord::inline_type_pattern;\n+  movptr(tmp, Address(object, oopDesc::mark_offset_in_bytes()));\n+  andptr(tmp, is_inline_type_mask);\n+  cmpptr(tmp, is_inline_type_mask);\n+  jcc(Assembler::notEqual, not_inline_type);\n+}\n+\n+void MacroAssembler::test_field_is_null_free_inline_type(Register flags, Register temp_reg, Label& is_null_free_inline_type) {\n+  movl(temp_reg, flags);\n+  testl(temp_reg, 1 << ResolvedFieldEntry::is_null_free_inline_type_shift);\n+  jcc(Assembler::notEqual, is_null_free_inline_type);\n+}\n+\n+void MacroAssembler::test_field_is_not_null_free_inline_type(Register flags, Register temp_reg, Label& not_null_free_inline_type) {\n+  movl(temp_reg, flags);\n+  testl(temp_reg, 1 << ResolvedFieldEntry::is_null_free_inline_type_shift);\n+  jcc(Assembler::equal, not_null_free_inline_type);\n+}\n+\n+void MacroAssembler::test_field_is_flat(Register flags, Register temp_reg, Label& is_flat) {\n+  movl(temp_reg, flags);\n+  testl(temp_reg, 1 << ResolvedFieldEntry::is_flat_shift);\n+  jcc(Assembler::notEqual, is_flat);\n+}\n+\n+void MacroAssembler::test_field_has_null_marker(Register flags, Register temp_reg, Label& has_null_marker) {\n+  movl(temp_reg, flags);\n+  testl(temp_reg, 1 << ResolvedFieldEntry::has_null_marker_shift);\n+  jcc(Assembler::notEqual, has_null_marker);\n+}\n+\n+void MacroAssembler::test_oop_prototype_bit(Register oop, Register temp_reg, int32_t test_bit, bool jmp_set, Label& jmp_label) {\n+  Label test_mark_word;\n+  \/\/ load mark word\n+  movptr(temp_reg, Address(oop, oopDesc::mark_offset_in_bytes()));\n+  \/\/ check displaced\n+  testl(temp_reg, markWord::unlocked_value);\n+  jccb(Assembler::notZero, test_mark_word);\n+  \/\/ slow path use klass prototype\n+  push(rscratch1);\n+  load_prototype_header(temp_reg, oop, rscratch1);\n+  pop(rscratch1);\n+\n+  bind(test_mark_word);\n+  testl(temp_reg, test_bit);\n+  jcc((jmp_set) ? Assembler::notZero : Assembler::zero, jmp_label);\n+}\n+\n+void MacroAssembler::test_flat_array_oop(Register oop, Register temp_reg,\n+                                         Label& is_flat_array) {\n+#ifdef _LP64\n+  test_oop_prototype_bit(oop, temp_reg, markWord::flat_array_bit_in_place, true, is_flat_array);\n+#else\n+  load_klass(temp_reg, oop, noreg);\n+  movl(temp_reg, Address(temp_reg, Klass::layout_helper_offset()));\n+  test_flat_array_layout(temp_reg, is_flat_array);\n+#endif\n+}\n+\n+void MacroAssembler::test_non_flat_array_oop(Register oop, Register temp_reg,\n+                                             Label& is_non_flat_array) {\n+#ifdef _LP64\n+  test_oop_prototype_bit(oop, temp_reg, markWord::flat_array_bit_in_place, false, is_non_flat_array);\n+#else\n+  load_klass(temp_reg, oop, noreg);\n+  movl(temp_reg, Address(temp_reg, Klass::layout_helper_offset()));\n+  test_non_flat_array_layout(temp_reg, is_non_flat_array);\n+#endif\n+}\n+\n+void MacroAssembler::test_null_free_array_oop(Register oop, Register temp_reg, Label&is_null_free_array) {\n+#ifdef _LP64\n+  test_oop_prototype_bit(oop, temp_reg, markWord::null_free_array_bit_in_place, true, is_null_free_array);\n+#else\n+  Unimplemented();\n+#endif\n+}\n+\n+void MacroAssembler::test_non_null_free_array_oop(Register oop, Register temp_reg, Label&is_non_null_free_array) {\n+#ifdef _LP64\n+  test_oop_prototype_bit(oop, temp_reg, markWord::null_free_array_bit_in_place, false, is_non_null_free_array);\n+#else\n+  Unimplemented();\n+#endif\n+}\n+\n+void MacroAssembler::test_flat_array_layout(Register lh, Label& is_flat_array) {\n+  testl(lh, Klass::_lh_array_tag_flat_value_bit_inplace);\n+  jcc(Assembler::notZero, is_flat_array);\n+}\n+\n+void MacroAssembler::test_non_flat_array_layout(Register lh, Label& is_non_flat_array) {\n+  testl(lh, Klass::_lh_array_tag_flat_value_bit_inplace);\n+  jcc(Assembler::zero, is_non_flat_array);\n+}\n+\n@@ -3441,0 +3559,118 @@\n+\/\/ Object \/ value buffer allocation...\n+\/\/\n+\/\/ Kills klass and rsi on LP64\n+void MacroAssembler::allocate_instance(Register klass, Register new_obj,\n+                                       Register t1, Register t2,\n+                                       bool clear_fields, Label& alloc_failed)\n+{\n+  Label done, initialize_header, initialize_object, slow_case, slow_case_no_pop;\n+  Register layout_size = t1;\n+  assert(new_obj == rax, \"needs to be rax\");\n+  assert_different_registers(klass, new_obj, t1, t2);\n+\n+  \/\/ get instance_size in InstanceKlass (scaled to a count of bytes)\n+  movl(layout_size, Address(klass, Klass::layout_helper_offset()));\n+  \/\/ test to see if it is malformed in some way\n+  testl(layout_size, Klass::_lh_instance_slow_path_bit);\n+  jcc(Assembler::notZero, slow_case_no_pop);\n+\n+  \/\/ Allocate the instance:\n+  \/\/  If TLAB is enabled:\n+  \/\/    Try to allocate in the TLAB.\n+  \/\/    If fails, go to the slow path.\n+  \/\/  Else If inline contiguous allocations are enabled:\n+  \/\/    Try to allocate in eden.\n+  \/\/    If fails due to heap end, go to slow path.\n+  \/\/\n+  \/\/  If TLAB is enabled OR inline contiguous is enabled:\n+  \/\/    Initialize the allocation.\n+  \/\/    Exit.\n+  \/\/\n+  \/\/  Go to slow path.\n+\n+  push(klass);\n+  if (UseTLAB) {\n+    tlab_allocate(new_obj, layout_size, 0, klass, t2, slow_case);\n+    if (ZeroTLAB || (!clear_fields)) {\n+      \/\/ the fields have been already cleared\n+      jmp(initialize_header);\n+    } else {\n+      \/\/ initialize both the header and fields\n+      jmp(initialize_object);\n+    }\n+  } else {\n+    jmp(slow_case);\n+  }\n+\n+  \/\/ If UseTLAB is true, the object is created above and there is an initialize need.\n+  \/\/ Otherwise, skip and go to the slow path.\n+  if (UseTLAB) {\n+    if (clear_fields) {\n+      \/\/ The object is initialized before the header.  If the object size is\n+      \/\/ zero, go directly to the header initialization.\n+      bind(initialize_object);\n+      if (UseCompactObjectHeaders) {\n+        assert(is_aligned(oopDesc::base_offset_in_bytes(), BytesPerLong), \"oop base offset must be 8-byte-aligned\");\n+        decrement(layout_size, oopDesc::base_offset_in_bytes());\n+      } else {\n+        decrement(layout_size, sizeof(oopDesc));\n+      }\n+      jcc(Assembler::zero, initialize_header);\n+\n+      \/\/ Initialize topmost object field, divide size by 8, check if odd and\n+      \/\/ test if zero.\n+      Register zero = klass;\n+      xorl(zero, zero);    \/\/ use zero reg to clear memory (shorter code)\n+      shrl(layout_size, LogBytesPerLong); \/\/ divide by 2*oopSize and set carry flag if odd\n+\n+  #ifdef ASSERT\n+      \/\/ make sure instance_size was multiple of 8\n+      Label L;\n+      \/\/ Ignore partial flag stall after shrl() since it is debug VM\n+      jcc(Assembler::carryClear, L);\n+      stop(\"object size is not multiple of 2 - adjust this code\");\n+      bind(L);\n+      \/\/ must be > 0, no extra check needed here\n+  #endif\n+\n+      \/\/ initialize remaining object fields: instance_size was a multiple of 8\n+      {\n+        Label loop;\n+        bind(loop);\n+        int header_size_bytes = oopDesc::header_size() * HeapWordSize;\n+        assert(is_aligned(header_size_bytes, BytesPerLong), \"oop header size must be 8-byte-aligned\");\n+        movptr(Address(new_obj, layout_size, Address::times_8, header_size_bytes - 1*oopSize), zero);\n+        decrement(layout_size);\n+        jcc(Assembler::notZero, loop);\n+      }\n+    } \/\/ clear_fields\n+\n+    \/\/ initialize object header only.\n+    bind(initialize_header);\n+    if (UseCompactObjectHeaders || EnableValhalla) {\n+      pop(klass);\n+      Register mark_word = t2;\n+      movptr(mark_word, Address(klass, Klass::prototype_header_offset()));\n+      movptr(Address(new_obj, oopDesc::mark_offset_in_bytes ()), mark_word);\n+    } else {\n+     movptr(Address(new_obj, oopDesc::mark_offset_in_bytes()),\n+            (intptr_t)markWord::prototype().value()); \/\/ header\n+     pop(klass);   \/\/ get saved klass back in the register.\n+    }\n+    if (!UseCompactObjectHeaders) {\n+      xorl(rsi, rsi);                 \/\/ use zero reg to clear memory (shorter code)\n+      store_klass_gap(new_obj, rsi);  \/\/ zero klass gap for compressed oops\n+      movptr(t2, klass);         \/\/ preserve klass\n+      store_klass(new_obj, t2, rscratch1);  \/\/ src klass reg is potentially compressed\n+    }\n+    jmp(done);\n+  }\n+\n+  bind(slow_case);\n+  pop(klass);\n+  bind(slow_case_no_pop);\n+  jmp(alloc_failed);\n+\n+  bind(done);\n+}\n+\n@@ -3644,0 +3880,27 @@\n+void MacroAssembler::get_inline_type_field_klass(Register holder_klass, Register index, Register inline_klass) {\n+  inline_layout_info(holder_klass, index, inline_klass);\n+  movptr(inline_klass, Address(inline_klass, InlineLayoutInfo::klass_offset()));\n+}\n+\n+void MacroAssembler::inline_layout_info(Register holder_klass, Register index, Register layout_info) {\n+  movptr(layout_info, Address(holder_klass, InstanceKlass::inline_layout_info_array_offset()));\n+#ifdef ASSERT\n+  {\n+    Label done;\n+    cmpptr(layout_info, 0);\n+    jcc(Assembler::notEqual, done);\n+    stop(\"inline_layout_info_array is null\");\n+    bind(done);\n+  }\n+#endif\n+\n+  InlineLayoutInfo array[2];\n+  int size = (char*)&array[1] - (char*)&array[0]; \/\/ computing size of array elements\n+  if (is_power_of_2(size)) {\n+    shll(index, log2i_exact(size)); \/\/ Scale index by power of 2\n+  } else {\n+    imull(index, index, size); \/\/ Scale the index to be the entry index * array_element_size\n+  }\n+  lea(layout_info, Address(layout_info, index, Address::times_1, Array<InlineLayoutInfo>::base_offset_in_bytes()));\n+}\n+\n@@ -4694,1 +4957,5 @@\n-  if (!VerifyOops) return;\n+  if (!VerifyOops || VerifyAdapterSharing) {\n+    \/\/ Below address of the code string confuses VerifyAdapterSharing\n+    \/\/ because it may differ between otherwise equivalent adapters.\n+    return;\n+  }\n@@ -4753,1 +5020,5 @@\n-  if (!VerifyOops) return;\n+  if (!VerifyOops || VerifyAdapterSharing) {\n+    \/\/ Below address of the code string confuses VerifyAdapterSharing\n+    \/\/ because it may differ between otherwise equivalent adapters.\n+    return;\n+  }\n@@ -5147,0 +5418,10 @@\n+void MacroAssembler::load_metadata(Register dst, Register src) {\n+  if (UseCompactObjectHeaders) {\n+    load_narrow_klass_compact(dst, src);\n+  } else if (UseCompressedClassPointers) {\n+    movl(dst, Address(src, oopDesc::klass_offset_in_bytes()));\n+  } else {\n+    movptr(dst, Address(src, oopDesc::klass_offset_in_bytes()));\n+  }\n+}\n+\n@@ -5168,0 +5449,5 @@\n+void MacroAssembler::load_prototype_header(Register dst, Register src, Register tmp) {\n+  load_klass(dst, src, tmp);\n+  movptr(dst, Address(dst, Klass::prototype_header_offset()));\n+}\n+\n@@ -5233,0 +5519,40 @@\n+void MacroAssembler::flat_field_copy(DecoratorSet decorators, Register src, Register dst,\n+                                     Register inline_layout_info) {\n+  BarrierSetAssembler* bs = BarrierSet::barrier_set()->barrier_set_assembler();\n+  bs->flat_field_copy(this, decorators, src, dst, inline_layout_info);\n+}\n+\n+void MacroAssembler::payload_offset(Register inline_klass, Register offset) {\n+  movptr(offset, Address(inline_klass, InstanceKlass::adr_inlineklass_fixed_block_offset()));\n+  movl(offset, Address(offset, InlineKlass::payload_offset_offset()));\n+}\n+\n+void MacroAssembler::payload_addr(Register oop, Register data, Register inline_klass) {\n+  \/\/ ((address) (void*) o) + vk->payload_offset();\n+  Register offset = (data == oop) ? rscratch1 : data;\n+  payload_offset(inline_klass, offset);\n+  if (data == oop) {\n+    addptr(data, offset);\n+  } else {\n+    lea(data, Address(oop, offset));\n+  }\n+}\n+\n+void MacroAssembler::data_for_value_array_index(Register array, Register array_klass,\n+                                                Register index, Register data) {\n+  assert(index != rcx, \"index needs to shift by rcx\");\n+  assert_different_registers(array, array_klass, index);\n+  assert_different_registers(rcx, array, index);\n+\n+  \/\/ array->base() + (index << Klass::layout_helper_log2_element_size(lh));\n+  movl(rcx, Address(array_klass, Klass::layout_helper_offset()));\n+\n+  \/\/ Klass::layout_helper_log2_element_size(lh)\n+  \/\/ (lh >> _lh_log2_element_size_shift) & _lh_log2_element_size_mask;\n+  shrl(rcx, Klass::_lh_log2_element_size_shift);\n+  andl(rcx, Klass::_lh_log2_element_size_mask);\n+  shlptr(index); \/\/ index << rcx\n+\n+  lea(data, Address(array, index, Address::times_1, arrayOopDesc::base_offset_in_bytes(T_FLAT_ELEMENT)));\n+}\n+\n@@ -5592,1 +5918,1 @@\n-void MacroAssembler::xmm_clear_mem(Register base, Register cnt, Register rtmp, XMMRegister xtmp, KRegister mask) {\n+void MacroAssembler::xmm_clear_mem(Register base, Register cnt, Register val, XMMRegister xtmp, KRegister mask) {\n@@ -5598,1 +5924,1 @@\n-    vpxor(xtmp, xtmp, xtmp, AVX_512bit);\n+    evpbroadcastq(xtmp, val, AVX_512bit);\n@@ -5600,1 +5926,3 @@\n-    vpxor(xtmp, xtmp, xtmp, AVX_256bit);\n+    movdq(xtmp, val);\n+    punpcklqdq(xtmp, xtmp);\n+    vinserti128_high(xtmp, xtmp);\n@@ -5602,1 +5930,2 @@\n-    pxor(xtmp, xtmp);\n+    movdq(xtmp, val);\n+    punpcklqdq(xtmp, xtmp);\n@@ -5625,1 +5954,1 @@\n-    fill64_masked(3, base, 0, xtmp, mask, cnt, rtmp, true);\n+    fill64_masked(3, base, 0, xtmp, mask, cnt, val, true);\n@@ -5644,1 +5973,1 @@\n-    fill32_masked(3, base, 0, xtmp, mask, cnt, rtmp);\n+    fill32_masked(3, base, 0, xtmp, mask, cnt, val);\n@@ -5657,0 +5986,410 @@\n+int MacroAssembler::store_inline_type_fields_to_buf(ciInlineKlass* vk, bool from_interpreter) {\n+  assert(InlineTypeReturnedAsFields, \"Inline types should never be returned as fields\");\n+  \/\/ An inline type might be returned. If fields are in registers we\n+  \/\/ need to allocate an inline type instance and initialize it with\n+  \/\/ the value of the fields.\n+  Label skip;\n+  \/\/ We only need a new buffered inline type if a new one is not returned\n+  testptr(rax, 1);\n+  jcc(Assembler::zero, skip);\n+  int call_offset = -1;\n+\n+#ifdef _LP64\n+  \/\/ The following code is similar to allocate_instance but has some slight differences,\n+  \/\/ e.g. object size is always not zero, sometimes it's constant; storing klass ptr after\n+  \/\/ allocating is not necessary if vk != nullptr, etc. allocate_instance is not aware of these.\n+  Label slow_case;\n+  \/\/ 1. Try to allocate a new buffered inline instance either from TLAB or eden space\n+  mov(rscratch1, rax); \/\/ save rax for slow_case since *_allocate may corrupt it when allocation failed\n+  if (vk != nullptr) {\n+    \/\/ Called from C1, where the return type is statically known.\n+    movptr(rbx, (intptr_t)vk->get_InlineKlass());\n+    jint lh = vk->layout_helper();\n+    assert(lh != Klass::_lh_neutral_value, \"inline class in return type must have been resolved\");\n+    if (UseTLAB && !Klass::layout_helper_needs_slow_path(lh)) {\n+      tlab_allocate(rax, noreg, lh, r13, r14, slow_case);\n+    } else {\n+      jmp(slow_case);\n+    }\n+  } else {\n+    \/\/ Call from interpreter. RAX contains ((the InlineKlass* of the return type) | 0x01)\n+    mov(rbx, rax);\n+    andptr(rbx, -2);\n+    if (UseTLAB) {\n+      movl(r14, Address(rbx, Klass::layout_helper_offset()));\n+      testl(r14, Klass::_lh_instance_slow_path_bit);\n+      jcc(Assembler::notZero, slow_case);\n+      tlab_allocate(rax, r14, 0, r13, r14, slow_case);\n+    } else {\n+      jmp(slow_case);\n+    }\n+  }\n+  if (UseTLAB) {\n+    \/\/ 2. Initialize buffered inline instance header\n+    Register buffer_obj = rax;\n+    Register klass = rbx;\n+    if (UseCompactObjectHeaders) {\n+      Register mark_word = r13;\n+      movptr(mark_word, Address(klass, Klass::prototype_header_offset()));\n+      movptr(Address(buffer_obj, oopDesc::mark_offset_in_bytes()), mark_word);\n+    } else {\n+      movptr(Address(buffer_obj, oopDesc::mark_offset_in_bytes()), (intptr_t)markWord::inline_type_prototype().value());\n+      xorl(r13, r13);\n+      store_klass_gap(buffer_obj, r13);\n+      if (vk == nullptr) {\n+        \/\/ store_klass corrupts rbx(klass), so save it in r13 for later use (interpreter case only).\n+        mov(r13, klass);\n+      }\n+      store_klass(buffer_obj, klass, rscratch1);\n+      klass = r13;\n+    }\n+    \/\/ 3. Initialize its fields with an inline class specific handler\n+    if (vk != nullptr) {\n+      call(RuntimeAddress(vk->pack_handler())); \/\/ no need for call info as this will not safepoint.\n+    } else {\n+      movptr(rbx, Address(klass, InstanceKlass::adr_inlineklass_fixed_block_offset()));\n+      movptr(rbx, Address(rbx, InlineKlass::pack_handler_offset()));\n+      call(rbx);\n+    }\n+    jmp(skip);\n+  }\n+  bind(slow_case);\n+  \/\/ We failed to allocate a new inline type, fall back to a runtime\n+  \/\/ call. Some oop field may be live in some registers but we can't\n+  \/\/ tell. That runtime call will take care of preserving them\n+  \/\/ across a GC if there's one.\n+  mov(rax, rscratch1);\n+#endif\n+\n+  if (from_interpreter) {\n+    super_call_VM_leaf(StubRoutines::store_inline_type_fields_to_buf());\n+  } else {\n+    call(RuntimeAddress(StubRoutines::store_inline_type_fields_to_buf()));\n+    call_offset = offset();\n+  }\n+\n+  bind(skip);\n+  return call_offset;\n+}\n+\n+\/\/ Move a value between registers\/stack slots and update the reg_state\n+bool MacroAssembler::move_helper(VMReg from, VMReg to, BasicType bt, RegState reg_state[]) {\n+  assert(from->is_valid() && to->is_valid(), \"source and destination must be valid\");\n+  if (reg_state[to->value()] == reg_written) {\n+    return true; \/\/ Already written\n+  }\n+  if (from != to && bt != T_VOID) {\n+    if (reg_state[to->value()] == reg_readonly) {\n+      return false; \/\/ Not yet writable\n+    }\n+    if (from->is_reg()) {\n+      if (to->is_reg()) {\n+        if (from->is_XMMRegister()) {\n+          if (bt == T_DOUBLE) {\n+            movdbl(to->as_XMMRegister(), from->as_XMMRegister());\n+          } else {\n+            assert(bt == T_FLOAT, \"must be float\");\n+            movflt(to->as_XMMRegister(), from->as_XMMRegister());\n+          }\n+        } else {\n+          movq(to->as_Register(), from->as_Register());\n+        }\n+      } else {\n+        int st_off = to->reg2stack() * VMRegImpl::stack_slot_size + wordSize;\n+        Address to_addr = Address(rsp, st_off);\n+        if (from->is_XMMRegister()) {\n+          if (bt == T_DOUBLE) {\n+            movdbl(to_addr, from->as_XMMRegister());\n+          } else {\n+            assert(bt == T_FLOAT, \"must be float\");\n+            movflt(to_addr, from->as_XMMRegister());\n+          }\n+        } else {\n+          movq(to_addr, from->as_Register());\n+        }\n+      }\n+    } else {\n+      Address from_addr = Address(rsp, from->reg2stack() * VMRegImpl::stack_slot_size + wordSize);\n+      if (to->is_reg()) {\n+        if (to->is_XMMRegister()) {\n+          if (bt == T_DOUBLE) {\n+            movdbl(to->as_XMMRegister(), from_addr);\n+          } else {\n+            assert(bt == T_FLOAT, \"must be float\");\n+            movflt(to->as_XMMRegister(), from_addr);\n+          }\n+        } else {\n+          movq(to->as_Register(), from_addr);\n+        }\n+      } else {\n+        int st_off = to->reg2stack() * VMRegImpl::stack_slot_size + wordSize;\n+        movq(r13, from_addr);\n+        movq(Address(rsp, st_off), r13);\n+      }\n+    }\n+  }\n+  \/\/ Update register states\n+  reg_state[from->value()] = reg_writable;\n+  reg_state[to->value()] = reg_written;\n+  return true;\n+}\n+\n+\/\/ Calculate the extra stack space required for packing or unpacking inline\n+\/\/ args and adjust the stack pointer\n+int MacroAssembler::extend_stack_for_inline_args(int args_on_stack) {\n+  \/\/ Two additional slots to account for return address\n+  int sp_inc = (args_on_stack + 2) * VMRegImpl::stack_slot_size;\n+  sp_inc = align_up(sp_inc, StackAlignmentInBytes);\n+  \/\/ Save the return address, adjust the stack (make sure it is properly\n+  \/\/ 16-byte aligned) and copy the return address to the new top of the stack.\n+  \/\/ The stack will be repaired on return (see MacroAssembler::remove_frame).\n+  assert(sp_inc > 0, \"sanity\");\n+  pop(r13);\n+  subptr(rsp, sp_inc);\n+  push(r13);\n+  return sp_inc;\n+}\n+\n+\/\/ Read all fields from an inline type buffer and store the field values in registers\/stack slots.\n+bool MacroAssembler::unpack_inline_helper(const GrowableArray<SigEntry>* sig, int& sig_index,\n+                                          VMReg from, int& from_index, VMRegPair* to, int to_count, int& to_index,\n+                                          RegState reg_state[]) {\n+  assert(sig->at(sig_index)._bt == T_VOID, \"should be at end delimiter\");\n+  assert(from->is_valid(), \"source must be valid\");\n+  bool progress = false;\n+#ifdef ASSERT\n+  const int start_offset = offset();\n+#endif\n+\n+  Label L_null, L_notNull;\n+  \/\/ Don't use r14 as tmp because it's used for spilling (see MacroAssembler::spill_reg_for)\n+  Register tmp1 = r10;\n+  Register tmp2 = r13;\n+  Register fromReg = noreg;\n+  ScalarizedInlineArgsStream stream(sig, sig_index, to, to_count, to_index, -1);\n+  bool done = true;\n+  bool mark_done = true;\n+  VMReg toReg;\n+  BasicType bt;\n+  \/\/ Check if argument requires a null check\n+  bool null_check = false;\n+  VMReg nullCheckReg;\n+  while (stream.next(nullCheckReg, bt)) {\n+    if (sig->at(stream.sig_index())._offset == -1) {\n+      null_check = true;\n+      break;\n+    }\n+  }\n+  stream.reset(sig_index, to_index);\n+  while (stream.next(toReg, bt)) {\n+    assert(toReg->is_valid(), \"destination must be valid\");\n+    int idx = (int)toReg->value();\n+    if (reg_state[idx] == reg_readonly) {\n+      if (idx != from->value()) {\n+        mark_done = false;\n+      }\n+      done = false;\n+      continue;\n+    } else if (reg_state[idx] == reg_written) {\n+      continue;\n+    }\n+    assert(reg_state[idx] == reg_writable, \"must be writable\");\n+    reg_state[idx] = reg_written;\n+    progress = true;\n+\n+    if (fromReg == noreg) {\n+      if (from->is_reg()) {\n+        fromReg = from->as_Register();\n+      } else {\n+        int st_off = from->reg2stack() * VMRegImpl::stack_slot_size + wordSize;\n+        movq(tmp1, Address(rsp, st_off));\n+        fromReg = tmp1;\n+      }\n+      if (null_check) {\n+        \/\/ Nullable inline type argument, emit null check\n+        testptr(fromReg, fromReg);\n+        jcc(Assembler::zero, L_null);\n+      }\n+    }\n+    int off = sig->at(stream.sig_index())._offset;\n+    if (off == -1) {\n+      assert(null_check, \"Missing null check at\");\n+      if (toReg->is_stack()) {\n+        int st_off = toReg->reg2stack() * VMRegImpl::stack_slot_size + wordSize;\n+        movq(Address(rsp, st_off), 1);\n+      } else {\n+        movq(toReg->as_Register(), 1);\n+      }\n+      continue;\n+    }\n+    assert(off > 0, \"offset in object should be positive\");\n+    Address fromAddr = Address(fromReg, off);\n+    if (!toReg->is_XMMRegister()) {\n+      Register dst = toReg->is_stack() ? tmp2 : toReg->as_Register();\n+      if (is_reference_type(bt)) {\n+        load_heap_oop(dst, fromAddr);\n+      } else {\n+        bool is_signed = (bt != T_CHAR) && (bt != T_BOOLEAN);\n+        load_sized_value(dst, fromAddr, type2aelembytes(bt), is_signed);\n+      }\n+      if (toReg->is_stack()) {\n+        int st_off = toReg->reg2stack() * VMRegImpl::stack_slot_size + wordSize;\n+        movq(Address(rsp, st_off), dst);\n+      }\n+    } else if (bt == T_DOUBLE) {\n+      movdbl(toReg->as_XMMRegister(), fromAddr);\n+    } else {\n+      assert(bt == T_FLOAT, \"must be float\");\n+      movflt(toReg->as_XMMRegister(), fromAddr);\n+    }\n+  }\n+  if (progress && null_check) {\n+    if (done) {\n+      jmp(L_notNull);\n+      bind(L_null);\n+      \/\/ Set null marker to zero to signal that the argument is null.\n+      \/\/ Also set all oop fields to zero to make the GC happy.\n+      stream.reset(sig_index, to_index);\n+      while (stream.next(toReg, bt)) {\n+        if (sig->at(stream.sig_index())._offset == -1 ||\n+            bt == T_OBJECT || bt == T_ARRAY) {\n+          if (toReg->is_stack()) {\n+            int st_off = toReg->reg2stack() * VMRegImpl::stack_slot_size + wordSize;\n+            movq(Address(rsp, st_off), 0);\n+          } else {\n+            xorq(toReg->as_Register(), toReg->as_Register());\n+          }\n+        }\n+      }\n+      bind(L_notNull);\n+    } else {\n+      bind(L_null);\n+    }\n+  }\n+\n+  sig_index = stream.sig_index();\n+  to_index = stream.regs_index();\n+\n+  if (mark_done && reg_state[from->value()] != reg_written) {\n+    \/\/ This is okay because no one else will write to that slot\n+    reg_state[from->value()] = reg_writable;\n+  }\n+  from_index--;\n+  assert(progress || (start_offset == offset()), \"should not emit code\");\n+  return done;\n+}\n+\n+bool MacroAssembler::pack_inline_helper(const GrowableArray<SigEntry>* sig, int& sig_index, int vtarg_index,\n+                                        VMRegPair* from, int from_count, int& from_index, VMReg to,\n+                                        RegState reg_state[], Register val_array) {\n+  assert(sig->at(sig_index)._bt == T_METADATA, \"should be at delimiter\");\n+  assert(to->is_valid(), \"destination must be valid\");\n+\n+  if (reg_state[to->value()] == reg_written) {\n+    skip_unpacked_fields(sig, sig_index, from, from_count, from_index);\n+    return true; \/\/ Already written\n+  }\n+\n+  \/\/ TODO 8284443 Isn't it an issue if below code uses r14 as tmp when it contains a spilled value?\n+  \/\/ Be careful with r14 because it's used for spilling (see MacroAssembler::spill_reg_for).\n+  Register val_obj_tmp = r11;\n+  Register from_reg_tmp = r14;\n+  Register tmp1 = r10;\n+  Register tmp2 = r13;\n+  Register tmp3 = rbx;\n+  Register val_obj = to->is_stack() ? val_obj_tmp : to->as_Register();\n+\n+  assert_different_registers(val_obj_tmp, from_reg_tmp, tmp1, tmp2, tmp3, val_array);\n+\n+  if (reg_state[to->value()] == reg_readonly) {\n+    if (!is_reg_in_unpacked_fields(sig, sig_index, to, from, from_count, from_index)) {\n+      skip_unpacked_fields(sig, sig_index, from, from_count, from_index);\n+      return false; \/\/ Not yet writable\n+    }\n+    val_obj = val_obj_tmp;\n+  }\n+\n+  int index = arrayOopDesc::base_offset_in_bytes(T_OBJECT) + vtarg_index * type2aelembytes(T_OBJECT);\n+  load_heap_oop(val_obj, Address(val_array, index));\n+\n+  ScalarizedInlineArgsStream stream(sig, sig_index, from, from_count, from_index);\n+  VMReg fromReg;\n+  BasicType bt;\n+  Label L_null;\n+  while (stream.next(fromReg, bt)) {\n+    assert(fromReg->is_valid(), \"source must be valid\");\n+    reg_state[fromReg->value()] = reg_writable;\n+\n+    int off = sig->at(stream.sig_index())._offset;\n+    if (off == -1) {\n+      \/\/ Nullable inline type argument, emit null check\n+      Label L_notNull;\n+      if (fromReg->is_stack()) {\n+        int ld_off = fromReg->reg2stack() * VMRegImpl::stack_slot_size + wordSize;\n+        testb(Address(rsp, ld_off), 1);\n+      } else {\n+        testb(fromReg->as_Register(), 1);\n+      }\n+      jcc(Assembler::notZero, L_notNull);\n+      movptr(val_obj, 0);\n+      jmp(L_null);\n+      bind(L_notNull);\n+      continue;\n+    }\n+\n+    assert(off > 0, \"offset in object should be positive\");\n+    size_t size_in_bytes = is_java_primitive(bt) ? type2aelembytes(bt) : wordSize;\n+\n+    \/\/ Pack the scalarized field into the value object.\n+    Address dst(val_obj, off);\n+    if (!fromReg->is_XMMRegister()) {\n+      Register src;\n+      if (fromReg->is_stack()) {\n+        src = from_reg_tmp;\n+        int ld_off = fromReg->reg2stack() * VMRegImpl::stack_slot_size + wordSize;\n+        load_sized_value(src, Address(rsp, ld_off), size_in_bytes, \/* is_signed *\/ false);\n+      } else {\n+        src = fromReg->as_Register();\n+      }\n+      assert_different_registers(dst.base(), src, tmp1, tmp2, tmp3, val_array);\n+      if (is_reference_type(bt)) {\n+        store_heap_oop(dst, src, tmp1, tmp2, tmp3, IN_HEAP | ACCESS_WRITE | IS_DEST_UNINITIALIZED);\n+      } else {\n+        store_sized_value(dst, src, size_in_bytes);\n+      }\n+    } else if (bt == T_DOUBLE) {\n+      movdbl(dst, fromReg->as_XMMRegister());\n+    } else {\n+      assert(bt == T_FLOAT, \"must be float\");\n+      movflt(dst, fromReg->as_XMMRegister());\n+    }\n+  }\n+  bind(L_null);\n+  sig_index = stream.sig_index();\n+  from_index = stream.regs_index();\n+\n+  assert(reg_state[to->value()] == reg_writable, \"must have already been read\");\n+  bool success = move_helper(val_obj->as_VMReg(), to, T_OBJECT, reg_state);\n+  assert(success, \"to register must be writeable\");\n+  return true;\n+}\n+\n+VMReg MacroAssembler::spill_reg_for(VMReg reg) {\n+  return reg->is_XMMRegister() ? xmm8->as_VMReg() : r14->as_VMReg();\n+}\n+\n+void MacroAssembler::remove_frame(int initial_framesize, bool needs_stack_repair) {\n+  assert((initial_framesize & (StackAlignmentInBytes-1)) == 0, \"frame size not aligned\");\n+  if (needs_stack_repair) {\n+    \/\/ TODO 8284443 Add a comment drawing the frame like in Aarch64's version of MacroAssembler::remove_frame\n+    movq(rbp, Address(rsp, initial_framesize));\n+    \/\/ The stack increment resides just below the saved rbp\n+    addq(rsp, Address(rsp, initial_framesize - wordSize));\n+  } else {\n+    if (initial_framesize > 0) {\n+      addq(rsp, initial_framesize);\n+    }\n+    pop(rbp);\n+  }\n+}\n+\n@@ -5746,2 +6485,2 @@\n-void MacroAssembler::clear_mem(Register base, Register cnt, Register tmp, XMMRegister xtmp,\n-                               bool is_large, KRegister mask) {\n+void MacroAssembler::clear_mem(Register base, Register cnt, Register val, XMMRegister xtmp,\n+                               bool is_large, bool word_copy_only, KRegister mask) {\n@@ -5752,1 +6491,1 @@\n-  assert(tmp==rax,   \"tmp register must be eax for rep stos\");\n+  assert(val==rax,   \"val register must be eax for rep stos\");\n@@ -5758,3 +6497,0 @@\n-  if (!is_large || !UseXMMForObjInit) {\n-    xorptr(tmp, tmp);\n-  }\n@@ -5772,1 +6508,1 @@\n-    movptr(Address(base, cnt, Address::times_ptr), tmp);\n+    movptr(Address(base, cnt, Address::times_ptr), val);\n@@ -5781,1 +6517,1 @@\n-  if (UseFastStosb) {\n+  if (UseFastStosb && !word_copy_only) {\n@@ -5785,1 +6521,1 @@\n-    xmm_clear_mem(base, cnt, tmp, xtmp, mask);\n+    xmm_clear_mem(base, cnt, val, xtmp, mask);\n@@ -9705,0 +10441,3 @@\n+  \/\/ Mask inline_type bit such that we go to the slow path if object is an inline type\n+  andptr(reg_rax, ~((int) markWord::inline_type_bit_in_place));\n+\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.cpp","additions":756,"deletions":17,"binary":false,"changes":773,"status":"modified"},{"patch":"@@ -33,0 +33,1 @@\n+#include \"runtime\/signature.hpp\"\n@@ -36,0 +37,2 @@\n+class ciInlineKlass;\n+\n@@ -97,0 +100,22 @@\n+  \/\/ markWord tests, kills markWord reg\n+  void test_markword_is_inline_type(Register markword, Label& is_inline_type);\n+\n+  \/\/ inlineKlass queries, kills temp_reg\n+  void test_oop_is_not_inline_type(Register object, Register tmp, Label& not_inline_type, bool can_be_null = true);\n+\n+  void test_field_is_null_free_inline_type(Register flags, Register temp_reg, Label& is_null_free);\n+  void test_field_is_not_null_free_inline_type(Register flags, Register temp_reg, Label& not_null_free);\n+  void test_field_is_flat(Register flags, Register temp_reg, Label& is_flat);\n+  void test_field_has_null_marker(Register flags, Register temp_reg, Label& has_null_marker);\n+\n+  \/\/ Check oops for special arrays, i.e. flat arrays and\/or null-free arrays\n+  void test_oop_prototype_bit(Register oop, Register temp_reg, int32_t test_bit, bool jmp_set, Label& jmp_label);\n+  void test_flat_array_oop(Register oop, Register temp_reg, Label& is_flat_array);\n+  void test_non_flat_array_oop(Register oop, Register temp_reg, Label& is_non_flat_array);\n+  void test_null_free_array_oop(Register oop, Register temp_reg, Label& is_null_free_array);\n+  void test_non_null_free_array_oop(Register oop, Register temp_reg, Label& is_non_null_free_array);\n+\n+  \/\/ Check array klass layout helper for flat or null-free arrays...\n+  void test_flat_array_layout(Register lh, Label& is_flat_array);\n+  void test_non_flat_array_layout(Register lh, Label& is_non_flat_array);\n+\n@@ -350,0 +375,3 @@\n+\n+  \/\/ Load oopDesc._metadata without decode (useful for direct Klass* compare from oops)\n+  void load_metadata(Register dst, Register src);\n@@ -367,0 +395,9 @@\n+  void flat_field_copy(DecoratorSet decorators, Register src, Register dst, Register inline_layout_info);\n+\n+  \/\/ inline type data payload offsets...\n+  void payload_offset(Register inline_klass, Register offset);\n+  void payload_addr(Register oop, Register data, Register inline_klass);\n+  \/\/ get data payload ptr a flat value array at index, kills rcx and index\n+  void data_for_value_array_index(Register array, Register array_klass,\n+                                  Register index, Register data);\n+\n@@ -376,0 +413,2 @@\n+  void load_prototype_header(Register dst, Register src, Register tmp);\n+\n@@ -510,0 +549,9 @@\n+\n+  \/\/ Object \/ value buffer allocation...\n+  \/\/ Allocate instance of klass, assumes klass initialized by caller\n+  \/\/ new_obj prefers to be rax\n+  \/\/ Kills t1 and t2, perserves klass, return allocation in new_obj (rsi on LP64)\n+  void allocate_instance(Register klass, Register new_obj,\n+                         Register t1, Register t2,\n+                         bool clear_fields, Label& alloc_failed);\n+\n@@ -520,0 +568,5 @@\n+  \/\/ For field \"index\" within \"klass\", return inline_klass ...\n+  void get_inline_type_field_klass(Register klass, Register index, Register inline_klass);\n+\n+  void inline_layout_info(Register klass, Register index, Register layout_info);\n+\n@@ -766,0 +819,1 @@\n+  void andptr(Register dst, Address src) { andq(dst, src); }\n@@ -1904,0 +1958,15 @@\n+  \/\/ Inline type specific methods\n+  #include \"asm\/macroAssembler_common.hpp\"\n+\n+  int store_inline_type_fields_to_buf(ciInlineKlass* vk, bool from_interpreter = true);\n+  bool move_helper(VMReg from, VMReg to, BasicType bt, RegState reg_state[]);\n+  bool unpack_inline_helper(const GrowableArray<SigEntry>* sig, int& sig_index,\n+                            VMReg from, int& from_index, VMRegPair* to, int to_count, int& to_index,\n+                            RegState reg_state[]);\n+  bool pack_inline_helper(const GrowableArray<SigEntry>* sig, int& sig_index, int vtarg_index,\n+                          VMRegPair* from, int from_count, int& from_index, VMReg to,\n+                          RegState reg_state[], Register val_array);\n+  int extend_stack_for_inline_args(int args_on_stack);\n+  void remove_frame(int initial_framesize, bool needs_stack_repair);\n+  VMReg spill_reg_for(VMReg reg);\n+\n@@ -1906,1 +1975,1 @@\n-  void clear_mem(Register base, Register cnt, Register rtmp, XMMRegister xtmp, bool is_large, KRegister mask=knoreg);\n+  void clear_mem(Register base, Register cnt, Register val, XMMRegister xtmp, bool is_large, bool word_copy_only, KRegister mask=knoreg);\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.hpp","additions":70,"deletions":1,"binary":false,"changes":71,"status":"modified"},{"patch":"@@ -30,0 +30,1 @@\n+#include \"classfile\/symbolTable.hpp\"\n@@ -637,0 +638,81 @@\n+\/\/ Same as java_calling_convention() but for multiple return\n+\/\/ values. There's no way to store them on the stack so if we don't\n+\/\/ have enough registers, multiple values can't be returned.\n+const uint SharedRuntime::java_return_convention_max_int = Argument::n_int_register_parameters_j+1;\n+const uint SharedRuntime::java_return_convention_max_float = Argument::n_float_register_parameters_j;\n+int SharedRuntime::java_return_convention(const BasicType *sig_bt,\n+                                          VMRegPair *regs,\n+                                          int total_args_passed) {\n+  \/\/ Create the mapping between argument positions and\n+  \/\/ registers.\n+  static const Register INT_ArgReg[java_return_convention_max_int] = {\n+    rax, j_rarg5, j_rarg4, j_rarg3, j_rarg2, j_rarg1, j_rarg0\n+  };\n+  static const XMMRegister FP_ArgReg[java_return_convention_max_float] = {\n+    j_farg0, j_farg1, j_farg2, j_farg3,\n+    j_farg4, j_farg5, j_farg6, j_farg7\n+  };\n+\n+\n+  uint int_args = 0;\n+  uint fp_args = 0;\n+\n+  for (int i = 0; i < total_args_passed; i++) {\n+    switch (sig_bt[i]) {\n+    case T_BOOLEAN:\n+    case T_CHAR:\n+    case T_BYTE:\n+    case T_SHORT:\n+    case T_INT:\n+      if (int_args < Argument::n_int_register_parameters_j+1) {\n+        regs[i].set1(INT_ArgReg[int_args]->as_VMReg());\n+        int_args++;\n+      } else {\n+        return -1;\n+      }\n+      break;\n+    case T_VOID:\n+      \/\/ halves of T_LONG or T_DOUBLE\n+      assert(i != 0 && (sig_bt[i - 1] == T_LONG || sig_bt[i - 1] == T_DOUBLE), \"expecting half\");\n+      regs[i].set_bad();\n+      break;\n+    case T_LONG:\n+      assert(sig_bt[i + 1] == T_VOID, \"expecting half\");\n+      \/\/ fall through\n+    case T_OBJECT:\n+    case T_ARRAY:\n+    case T_ADDRESS:\n+    case T_METADATA:\n+      if (int_args < Argument::n_int_register_parameters_j+1) {\n+        regs[i].set2(INT_ArgReg[int_args]->as_VMReg());\n+        int_args++;\n+      } else {\n+        return -1;\n+      }\n+      break;\n+    case T_FLOAT:\n+      if (fp_args < Argument::n_float_register_parameters_j) {\n+        regs[i].set1(FP_ArgReg[fp_args]->as_VMReg());\n+        fp_args++;\n+      } else {\n+        return -1;\n+      }\n+      break;\n+    case T_DOUBLE:\n+      assert(sig_bt[i + 1] == T_VOID, \"expecting half\");\n+      if (fp_args < Argument::n_float_register_parameters_j) {\n+        regs[i].set2(FP_ArgReg[fp_args]->as_VMReg());\n+        fp_args++;\n+      } else {\n+        return -1;\n+      }\n+      break;\n+    default:\n+      ShouldNotReachHere();\n+      break;\n+    }\n+  }\n+\n+  return int_args + fp_args;\n+}\n+\n@@ -679,0 +761,106 @@\n+\/\/ For each inline type argument, sig includes the list of fields of\n+\/\/ the inline type. This utility function computes the number of\n+\/\/ arguments for the call if inline types are passed by reference (the\n+\/\/ calling convention the interpreter expects).\n+static int compute_total_args_passed_int(const GrowableArray<SigEntry>* sig_extended) {\n+  int total_args_passed = 0;\n+  if (InlineTypePassFieldsAsArgs) {\n+    for (int i = 0; i < sig_extended->length(); i++) {\n+      BasicType bt = sig_extended->at(i)._bt;\n+      if (bt == T_METADATA) {\n+        \/\/ In sig_extended, an inline type argument starts with:\n+        \/\/ T_METADATA, followed by the types of the fields of the\n+        \/\/ inline type and T_VOID to mark the end of the value\n+        \/\/ type. Inline types are flattened so, for instance, in the\n+        \/\/ case of an inline type with an int field and an inline type\n+        \/\/ field that itself has 2 fields, an int and a long:\n+        \/\/ T_METADATA T_INT T_METADATA T_INT T_LONG T_VOID (second\n+        \/\/ slot for the T_LONG) T_VOID (inner inline type) T_VOID\n+        \/\/ (outer inline type)\n+        total_args_passed++;\n+        int vt = 1;\n+        do {\n+          i++;\n+          BasicType bt = sig_extended->at(i)._bt;\n+          BasicType prev_bt = sig_extended->at(i-1)._bt;\n+          if (bt == T_METADATA) {\n+            vt++;\n+          } else if (bt == T_VOID &&\n+                     prev_bt != T_LONG &&\n+                     prev_bt != T_DOUBLE) {\n+            vt--;\n+          }\n+        } while (vt != 0);\n+      } else {\n+        total_args_passed++;\n+      }\n+    }\n+  } else {\n+    total_args_passed = sig_extended->length();\n+  }\n+  return total_args_passed;\n+}\n+\n+\n+static void gen_c2i_adapter_helper(MacroAssembler* masm,\n+                                   BasicType bt,\n+                                   BasicType prev_bt,\n+                                   size_t size_in_bytes,\n+                                   const VMRegPair& reg_pair,\n+                                   const Address& to,\n+                                   int extraspace,\n+                                   bool is_oop) {\n+  if (bt == T_VOID) {\n+    assert(prev_bt == T_LONG || prev_bt == T_DOUBLE, \"missing half\");\n+    return;\n+  }\n+\n+  \/\/ Say 4 args:\n+  \/\/ i   st_off\n+  \/\/ 0   32 T_LONG\n+  \/\/ 1   24 T_VOID\n+  \/\/ 2   16 T_OBJECT\n+  \/\/ 3    8 T_BOOL\n+  \/\/ -    0 return address\n+  \/\/\n+  \/\/ However to make thing extra confusing. Because we can fit a long\/double in\n+  \/\/ a single slot on a 64 bt vm and it would be silly to break them up, the interpreter\n+  \/\/ leaves one slot empty and only stores to a single slot. In this case the\n+  \/\/ slot that is occupied is the T_VOID slot. See I said it was confusing.\n+\n+  bool wide = (size_in_bytes == wordSize);\n+  VMReg r_1 = reg_pair.first();\n+  VMReg r_2 = reg_pair.second();\n+  assert(r_2->is_valid() == wide, \"invalid size\");\n+  if (!r_1->is_valid()) {\n+    assert(!r_2->is_valid(), \"must be invalid\");\n+    return;\n+  }\n+\n+  if (!r_1->is_XMMRegister()) {\n+    Register val = rax;\n+    if (r_1->is_stack()) {\n+      int ld_off = r_1->reg2stack() * VMRegImpl::stack_slot_size + extraspace;\n+      __ load_sized_value(val, Address(rsp, ld_off), size_in_bytes, \/* is_signed *\/ false);\n+    } else {\n+      val = r_1->as_Register();\n+    }\n+    assert_different_registers(to.base(), val, rscratch1);\n+    if (is_oop) {\n+      __ push(r13);\n+      __ push(rbx);\n+      __ store_heap_oop(to, val, rscratch1, r13, rbx, IN_HEAP | ACCESS_WRITE | IS_DEST_UNINITIALIZED);\n+      __ pop(rbx);\n+      __ pop(r13);\n+    } else {\n+      __ store_sized_value(to, val, size_in_bytes);\n+    }\n+  } else {\n+    if (wide) {\n+      __ movdbl(to, r_1->as_XMMRegister());\n+    } else {\n+      __ movflt(to, r_1->as_XMMRegister());\n+    }\n+  }\n+}\n+\n@@ -680,3 +868,1 @@\n-                            int total_args_passed,\n-                            int comp_args_on_stack,\n-                            const BasicType *sig_bt,\n+                            const GrowableArray<SigEntry>* sig_extended,\n@@ -684,1 +870,32 @@\n-                            Label& skip_fixup) {\n+                            bool requires_clinit_barrier,\n+                            address& c2i_no_clinit_check_entry,\n+                            Label& skip_fixup,\n+                            address start,\n+                            OopMapSet* oop_maps,\n+                            int& frame_complete,\n+                            int& frame_size_in_words,\n+                            bool alloc_inline_receiver) {\n+  if (requires_clinit_barrier && VM_Version::supports_fast_class_init_checks()) {\n+    Label L_skip_barrier;\n+    Register method = rbx;\n+\n+    { \/\/ Bypass the barrier for non-static methods\n+      Register flags = rscratch1;\n+      __ load_unsigned_short(flags, Address(method, Method::access_flags_offset()));\n+      __ testl(flags, JVM_ACC_STATIC);\n+      __ jcc(Assembler::zero, L_skip_barrier); \/\/ non-static\n+    }\n+\n+    Register klass = rscratch1;\n+    __ load_method_holder(klass, method);\n+    __ clinit_barrier(klass, &L_skip_barrier \/*L_fast_path*\/);\n+\n+    __ jump(RuntimeAddress(SharedRuntime::get_handle_wrong_method_stub())); \/\/ slow path\n+\n+    __ bind(L_skip_barrier);\n+    c2i_no_clinit_check_entry = __ pc();\n+  }\n+\n+  BarrierSetAssembler* bs = BarrierSet::barrier_set()->barrier_set_assembler();\n+  bs->c2i_entry_barrier(masm);\n+\n@@ -694,0 +911,42 @@\n+  if (InlineTypePassFieldsAsArgs) {\n+    \/\/ Is there an inline type argument?\n+    bool has_inline_argument = false;\n+    for (int i = 0; i < sig_extended->length() && !has_inline_argument; i++) {\n+      has_inline_argument = (sig_extended->at(i)._bt == T_METADATA);\n+    }\n+    if (has_inline_argument) {\n+      \/\/ There is at least an inline type argument: we're coming from\n+      \/\/ compiled code so we have no buffers to back the inline types.\n+      \/\/ Allocate the buffers here with a runtime call.\n+      OopMap* map = RegisterSaver::save_live_registers(masm, 0, &frame_size_in_words, \/*save_vectors*\/ false);\n+\n+      frame_complete = __ offset();\n+\n+      __ set_last_Java_frame(noreg, noreg, nullptr, rscratch1);\n+\n+      __ mov(c_rarg0, r15_thread);\n+      __ mov(c_rarg1, rbx);\n+      __ mov64(c_rarg2, (int64_t)alloc_inline_receiver);\n+      __ call(RuntimeAddress(CAST_FROM_FN_PTR(address, SharedRuntime::allocate_inline_types)));\n+\n+      oop_maps->add_gc_map((int)(__ pc() - start), map);\n+      __ reset_last_Java_frame(false);\n+\n+      RegisterSaver::restore_live_registers(masm);\n+\n+      Label no_exception;\n+      __ cmpptr(Address(r15_thread, Thread::pending_exception_offset()), NULL_WORD);\n+      __ jcc(Assembler::equal, no_exception);\n+\n+      __ movptr(Address(r15_thread, JavaThread::vm_result_oop_offset()), NULL_WORD);\n+      __ movptr(rax, Address(r15_thread, Thread::pending_exception_offset()));\n+      __ jump(RuntimeAddress(StubRoutines::forward_exception_entry()));\n+\n+      __ bind(no_exception);\n+\n+      \/\/ We get an array of objects from the runtime call\n+      __ get_vm_result_oop(rscratch2); \/\/ Use rscratch2 (r11) as temporary because rscratch1 (r10) is trashed by movptr()\n+      __ get_vm_result_metadata(rbx); \/\/ TODO: required to keep the callee Method live?\n+    }\n+  }\n+\n@@ -696,1 +955,1 @@\n-\n+  int total_args_passed = compute_total_args_passed_int(sig_extended);\n@@ -731,46 +990,24 @@\n-  for (int i = 0; i < total_args_passed; i++) {\n-    if (sig_bt[i] == T_VOID) {\n-      assert(i > 0 && (sig_bt[i-1] == T_LONG || sig_bt[i-1] == T_DOUBLE), \"missing half\");\n-      continue;\n-    }\n-\n-    \/\/ offset to start parameters\n-    int st_off   = (total_args_passed - i) * Interpreter::stackElementSize;\n-    int next_off = st_off - Interpreter::stackElementSize;\n-\n-    \/\/ Say 4 args:\n-    \/\/ i   st_off\n-    \/\/ 0   32 T_LONG\n-    \/\/ 1   24 T_VOID\n-    \/\/ 2   16 T_OBJECT\n-    \/\/ 3    8 T_BOOL\n-    \/\/ -    0 return address\n-    \/\/\n-    \/\/ However to make thing extra confusing. Because we can fit a long\/double in\n-    \/\/ a single slot on a 64 bt vm and it would be silly to break them up, the interpreter\n-    \/\/ leaves one slot empty and only stores to a single slot. In this case the\n-    \/\/ slot that is occupied is the T_VOID slot. See I said it was confusing.\n-\n-    VMReg r_1 = regs[i].first();\n-    VMReg r_2 = regs[i].second();\n-    if (!r_1->is_valid()) {\n-      assert(!r_2->is_valid(), \"\");\n-      continue;\n-    }\n-    if (r_1->is_stack()) {\n-      \/\/ memory to memory use rax\n-      int ld_off = r_1->reg2stack() * VMRegImpl::stack_slot_size + extraspace;\n-      if (!r_2->is_valid()) {\n-        \/\/ sign extend??\n-        __ movl(rax, Address(rsp, ld_off));\n-        __ movptr(Address(rsp, st_off), rax);\n-      } else {\n-\n-        __ movq(rax, Address(rsp, ld_off));\n-\n-        \/\/ Two VMREgs|OptoRegs can be T_OBJECT, T_ADDRESS, T_DOUBLE, T_LONG\n-        \/\/ T_DOUBLE and T_LONG use two slots in the interpreter\n-        if ( sig_bt[i] == T_LONG || sig_bt[i] == T_DOUBLE) {\n-          \/\/ ld_off == LSW, ld_off+wordSize == MSW\n-          \/\/ st_off == MSW, next_off == LSW\n-          __ movq(Address(rsp, next_off), rax);\n+  \/\/ next_arg_comp is the next argument from the compiler point of\n+  \/\/ view (inline type fields are passed in registers\/on the stack). In\n+  \/\/ sig_extended, an inline type argument starts with: T_METADATA,\n+  \/\/ followed by the types of the fields of the inline type and T_VOID\n+  \/\/ to mark the end of the inline type. ignored counts the number of\n+  \/\/ T_METADATA\/T_VOID. next_vt_arg is the next inline type argument:\n+  \/\/ used to get the buffer for that argument from the pool of buffers\n+  \/\/ we allocated above and want to pass to the\n+  \/\/ interpreter. next_arg_int is the next argument from the\n+  \/\/ interpreter point of view (inline types are passed by reference).\n+  for (int next_arg_comp = 0, ignored = 0, next_vt_arg = 0, next_arg_int = 0;\n+       next_arg_comp < sig_extended->length(); next_arg_comp++) {\n+    assert(ignored <= next_arg_comp, \"shouldn't skip over more slots than there are arguments\");\n+    assert(next_arg_int <= total_args_passed, \"more arguments for the interpreter than expected?\");\n+    BasicType bt = sig_extended->at(next_arg_comp)._bt;\n+    int st_off = (total_args_passed - next_arg_int) * Interpreter::stackElementSize;\n+    if (!InlineTypePassFieldsAsArgs || bt != T_METADATA) {\n+      int next_off = st_off - Interpreter::stackElementSize;\n+      const int offset = (bt == T_LONG || bt == T_DOUBLE) ? next_off : st_off;\n+      const VMRegPair reg_pair = regs[next_arg_comp-ignored];\n+      size_t size_in_bytes = reg_pair.second()->is_valid() ? 8 : 4;\n+      gen_c2i_adapter_helper(masm, bt, next_arg_comp > 0 ? sig_extended->at(next_arg_comp-1)._bt : T_ILLEGAL,\n+                             size_in_bytes, reg_pair, Address(rsp, offset), extraspace, false);\n+      next_arg_int++;\n@@ -779,7 +1016,4 @@\n-          \/\/ Overwrite the unused slot with known junk\n-          __ mov64(rax, CONST64(0xdeadffffdeadaaaa));\n-          __ movptr(Address(rsp, st_off), rax);\n-#endif \/* ASSERT *\/\n-        } else {\n-          __ movq(Address(rsp, st_off), rax);\n-        }\n+      if (bt == T_LONG || bt == T_DOUBLE) {\n+        \/\/ Overwrite the unused slot with known junk\n+        __ mov64(rax, CONST64(0xdeadffffdeadaaaa));\n+        __ movptr(Address(rsp, st_off), rax);\n@@ -787,16 +1021,26 @@\n-    } else if (r_1->is_Register()) {\n-      Register r = r_1->as_Register();\n-      if (!r_2->is_valid()) {\n-        \/\/ must be only an int (or less ) so move only 32bits to slot\n-        \/\/ why not sign extend??\n-        __ movl(Address(rsp, st_off), r);\n-      } else {\n-        \/\/ Two VMREgs|OptoRegs can be T_OBJECT, T_ADDRESS, T_DOUBLE, T_LONG\n-        \/\/ T_DOUBLE and T_LONG use two slots in the interpreter\n-        if ( sig_bt[i] == T_LONG || sig_bt[i] == T_DOUBLE) {\n-          \/\/ long\/double in gpr\n-#ifdef ASSERT\n-          \/\/ Overwrite the unused slot with known junk\n-          __ mov64(rax, CONST64(0xdeadffffdeadaaab));\n-          __ movptr(Address(rsp, st_off), rax);\n-          __ movq(Address(rsp, next_off), r);\n+    } else {\n+      ignored++;\n+      \/\/ get the buffer from the just allocated pool of buffers\n+      int index = arrayOopDesc::base_offset_in_bytes(T_OBJECT) + next_vt_arg * type2aelembytes(T_OBJECT);\n+      __ load_heap_oop(r14, Address(rscratch2, index));\n+      next_vt_arg++; next_arg_int++;\n+      int vt = 1;\n+      \/\/ write fields we get from compiled code in registers\/stack\n+      \/\/ slots to the buffer: we know we are done with that inline type\n+      \/\/ argument when we hit the T_VOID that acts as an end of inline\n+      \/\/ type delimiter for this inline type. Inline types are flattened\n+      \/\/ so we might encounter embedded inline types. Each entry in\n+      \/\/ sig_extended contains a field offset in the buffer.\n+      Label L_null;\n+      do {\n+        next_arg_comp++;\n+        BasicType bt = sig_extended->at(next_arg_comp)._bt;\n+        BasicType prev_bt = sig_extended->at(next_arg_comp-1)._bt;\n+        if (bt == T_METADATA) {\n+          vt++;\n+          ignored++;\n+        } else if (bt == T_VOID &&\n+                   prev_bt != T_LONG &&\n+                   prev_bt != T_DOUBLE) {\n+          vt--;\n+          ignored++;\n@@ -805,1 +1049,22 @@\n-          __ movptr(Address(rsp, st_off), r);\n+          int off = sig_extended->at(next_arg_comp)._offset;\n+          if (off == -1) {\n+            \/\/ Nullable inline type argument, emit null check\n+            VMReg reg = regs[next_arg_comp-ignored].first();\n+            Label L_notNull;\n+            if (reg->is_stack()) {\n+              int ld_off = reg->reg2stack() * VMRegImpl::stack_slot_size + extraspace;\n+              __ testb(Address(rsp, ld_off), 1);\n+            } else {\n+              __ testb(reg->as_Register(), 1);\n+            }\n+            __ jcc(Assembler::notZero, L_notNull);\n+            __ movptr(Address(rsp, st_off), 0);\n+            __ jmp(L_null);\n+            __ bind(L_notNull);\n+            continue;\n+          }\n+          assert(off > 0, \"offset in object should be positive\");\n+          size_t size_in_bytes = is_java_primitive(bt) ? type2aelembytes(bt) : wordSize;\n+          bool is_oop = is_reference_type(bt);\n+          gen_c2i_adapter_helper(masm, bt, next_arg_comp > 0 ? sig_extended->at(next_arg_comp-1)._bt : T_ILLEGAL,\n+                                 size_in_bytes, regs[next_arg_comp-ignored], Address(r14, off), extraspace, is_oop);\n@@ -807,14 +1072,4 @@\n-      }\n-    } else {\n-      assert(r_1->is_XMMRegister(), \"\");\n-      if (!r_2->is_valid()) {\n-        \/\/ only a float use just part of the slot\n-        __ movflt(Address(rsp, st_off), r_1->as_XMMRegister());\n-      } else {\n-#ifdef ASSERT\n-        \/\/ Overwrite the unused slot with known junk\n-        __ mov64(rax, CONST64(0xdeadffffdeadaaac));\n-        __ movptr(Address(rsp, st_off), rax);\n-#endif \/* ASSERT *\/\n-        __ movdbl(Address(rsp, next_off), r_1->as_XMMRegister());\n-      }\n+      } while (vt != 0);\n+      \/\/ pass the buffer to the interpreter\n+      __ movptr(Address(rsp, st_off), r14);\n+      __ bind(L_null);\n@@ -830,2 +1085,1 @@\n-                                    int total_args_passed,\n-                                    const BasicType *sig_bt,\n+                                    const GrowableArray<SigEntry>* sig,\n@@ -888,1 +1142,1 @@\n-  __ movptr(r11, Address(rbx, in_bytes(Method::from_compiled_offset())));\n+  __ movptr(r11, Address(rbx, in_bytes(Method::from_compiled_inline_offset())));\n@@ -902,0 +1156,2 @@\n+  int total_args_passed = sig->length();\n+\n@@ -905,1 +1161,2 @@\n-    if (sig_bt[i] == T_VOID) {\n+    BasicType bt = sig->at(i)._bt;\n+    if (bt == T_VOID) {\n@@ -908,1 +1165,2 @@\n-      assert(i > 0 && (sig_bt[i-1] == T_LONG || sig_bt[i-1] == T_DOUBLE), \"missing half\");\n+      BasicType prev_bt = (i > 0) ? sig->at(i-1)._bt : T_ILLEGAL;\n+      assert(i > 0 && (prev_bt == T_LONG || prev_bt == T_DOUBLE), \"missing half\");\n@@ -950,1 +1208,1 @@\n-        const int offset = (sig_bt[i]==T_LONG||sig_bt[i]==T_DOUBLE)?\n+        const int offset = (bt==T_LONG||bt==T_DOUBLE)?\n@@ -965,1 +1223,1 @@\n-        const int offset = (sig_bt[i]==T_LONG||sig_bt[i]==T_DOUBLE)?\n+        const int offset = (bt==T_LONG||bt==T_DOUBLE)?\n@@ -998,1 +1256,1 @@\n-  \/\/ only needed because eof c2 resolve stubs return Method* as a result in\n+  \/\/ only needed because of c2 resolve stubs return Method* as a result in\n@@ -1004,0 +1262,13 @@\n+static void gen_inline_cache_check(MacroAssembler *masm, Label& skip_fixup) {\n+  Register data = rax;\n+  __ ic_check(1 \/* end_alignment *\/);\n+  __ movptr(rbx, Address(data, CompiledICData::speculated_method_offset()));\n+\n+  \/\/ Method might have been compiled since the call site was patched to\n+  \/\/ interpreted if that is the case treat it as a miss so we can get\n+  \/\/ the call site corrected.\n+  __ cmpptr(Address(rbx, in_bytes(Method::code_offset())), NULL_WORD);\n+  __ jcc(Assembler::equal, skip_fixup);\n+  __ jump(RuntimeAddress(SharedRuntime::get_ic_miss_stub()));\n+}\n+\n@@ -1005,2 +1276,1 @@\n-void SharedRuntime::generate_i2c2i_adapters(MacroAssembler *masm,\n-                                            int total_args_passed,\n+void SharedRuntime::generate_i2c2i_adapters(MacroAssembler* masm,\n@@ -1008,3 +1278,9 @@\n-                                            const BasicType *sig_bt,\n-                                            const VMRegPair *regs,\n-                                            address entry_address[AdapterBlob::ENTRY_COUNT]) {\n+                                            const GrowableArray<SigEntry>* sig,\n+                                            const VMRegPair* regs,\n+                                            const GrowableArray<SigEntry>* sig_cc,\n+                                            const VMRegPair* regs_cc,\n+                                            const GrowableArray<SigEntry>* sig_cc_ro,\n+                                            const VMRegPair* regs_cc_ro,\n+                                            address entry_address[AdapterBlob::ENTRY_COUNT],\n+                                            AdapterBlob*& new_adapter,\n+                                            bool allocate_code_blob) {\n@@ -1012,2 +1288,1 @@\n-\n-  gen_i2c_adapter(masm, total_args_passed, comp_args_on_stack, sig_bt, regs);\n+  gen_i2c_adapter(masm, comp_args_on_stack, sig, regs);\n@@ -1025,0 +1300,1 @@\n+  entry_address[AdapterBlob::C2I_Unverified_Inline] = __ pc();\n@@ -1027,3 +1303,1 @@\n-  Register data = rax;\n-  Register receiver = j_rarg0;\n-  Register temp = rbx;\n+  gen_inline_cache_check(masm, skip_fixup);\n@@ -1031,12 +1305,3 @@\n-  {\n-    __ ic_check(1 \/* end_alignment *\/);\n-    __ movptr(rbx, Address(data, CompiledICData::speculated_method_offset()));\n-    \/\/ Method might have been compiled since the call site was patched to\n-    \/\/ interpreted if that is the case treat it as a miss so we can get\n-    \/\/ the call site corrected.\n-    __ cmpptr(Address(rbx, in_bytes(Method::code_offset())), NULL_WORD);\n-    __ jcc(Assembler::equal, skip_fixup);\n-    __ jump(RuntimeAddress(SharedRuntime::get_ic_miss_stub()));\n-  }\n-\n-  entry_address[AdapterBlob::C2I] = __ pc();\n+  OopMapSet* oop_maps = new OopMapSet();\n+  int frame_complete = CodeOffsets::frame_never_safe;\n+  int frame_size_in_words = 0;\n@@ -1044,1 +1309,1 @@\n-  \/\/ Class initialization barrier for static methods\n+  \/\/ Scalarized c2i adapter with non-scalarized receiver (i.e., don't pack receiver)\n@@ -1046,19 +1311,33 @@\n-  if (VM_Version::supports_fast_class_init_checks()) {\n-    Label L_skip_barrier;\n-    Register method = rbx;\n-\n-    { \/\/ Bypass the barrier for non-static methods\n-      Register flags = rscratch1;\n-      __ load_unsigned_short(flags, Address(method, Method::access_flags_offset()));\n-      __ testl(flags, JVM_ACC_STATIC);\n-      __ jcc(Assembler::zero, L_skip_barrier); \/\/ non-static\n-    }\n-\n-    Register klass = rscratch1;\n-    __ load_method_holder(klass, method);\n-    __ clinit_barrier(klass, &L_skip_barrier \/*L_fast_path*\/);\n-\n-    __ jump(RuntimeAddress(SharedRuntime::get_handle_wrong_method_stub())); \/\/ slow path\n-\n-    __ bind(L_skip_barrier);\n-    entry_address[AdapterBlob::C2I_No_Clinit_Check] = __ pc();\n+  entry_address[AdapterBlob::C2I_Inline_RO] = __ pc();\n+  if (regs_cc != regs_cc_ro) {\n+    \/\/ No class init barrier needed because method is guaranteed to be non-static\n+    gen_c2i_adapter(masm, sig_cc_ro, regs_cc_ro, \/* requires_clinit_barrier = *\/ false, entry_address[AdapterBlob::C2I_No_Clinit_Check],\n+                    skip_fixup, entry_address[AdapterBlob::I2C], oop_maps, frame_complete, frame_size_in_words, \/* alloc_inline_receiver = *\/ false);\n+    skip_fixup.reset();\n+  }\n+\n+  \/\/ Scalarized c2i adapter\n+  entry_address[AdapterBlob::C2I]        = __ pc();\n+  entry_address[AdapterBlob::C2I_Inline] = __ pc();\n+  gen_c2i_adapter(masm, sig_cc, regs_cc, \/* requires_clinit_barrier = *\/ true, entry_address[AdapterBlob::C2I_No_Clinit_Check],\n+                  skip_fixup, entry_address[AdapterBlob::I2C], oop_maps, frame_complete, frame_size_in_words, \/* alloc_inline_receiver = *\/ true);\n+\n+  \/\/ Non-scalarized c2i adapter\n+  if (regs != regs_cc) {\n+    entry_address[AdapterBlob::C2I_Unverified_Inline] = __ pc();\n+    Label inline_entry_skip_fixup;\n+    gen_inline_cache_check(masm, inline_entry_skip_fixup);\n+\n+    entry_address[AdapterBlob::C2I_Inline] = __ pc();\n+    gen_c2i_adapter(masm, sig, regs, \/* requires_clinit_barrier = *\/ true, entry_address[AdapterBlob::C2I_No_Clinit_Check],\n+                    inline_entry_skip_fixup, entry_address[AdapterBlob::I2C], oop_maps, frame_complete, frame_size_in_words, \/* alloc_inline_receiver = *\/ false);\n+  }\n+\n+  \/\/ The c2i adapters might safepoint and trigger a GC. The caller must make sure that\n+  \/\/ the GC knows about the location of oop argument locations passed to the c2i adapter.\n+  if (allocate_code_blob) {\n+    bool caller_must_gc_arguments = (regs != regs_cc);\n+    int entry_offset[AdapterHandlerEntry::ENTRIES_COUNT];\n+    assert(AdapterHandlerEntry::ENTRIES_COUNT == 7, \"sanity\");\n+    AdapterHandlerLibrary::address_to_offset(entry_address, entry_offset);\n+    new_adapter = AdapterBlob::create(masm->code(), entry_offset, frame_complete, frame_size_in_words, oop_maps, caller_must_gc_arguments);\n@@ -1066,6 +1345,0 @@\n-\n-  BarrierSetAssembler* bs = BarrierSet::barrier_set()->barrier_set_assembler();\n-  bs->c2i_entry_barrier(masm);\n-\n-  gen_c2i_adapter(masm, total_args_passed, comp_args_on_stack, sig_bt, regs, skip_fixup);\n-  return;\n@@ -3513,0 +3786,141 @@\n+BufferedInlineTypeBlob* SharedRuntime::generate_buffered_inline_type_adapter(const InlineKlass* vk) {\n+  BufferBlob* buf = BufferBlob::create(\"inline types pack\/unpack\", 16 * K);\n+  if (buf == nullptr) {\n+    return nullptr;\n+  }\n+  CodeBuffer buffer(buf);\n+  short buffer_locs[20];\n+  buffer.insts()->initialize_shared_locs((relocInfo*)buffer_locs,\n+                                         sizeof(buffer_locs)\/sizeof(relocInfo));\n+\n+  MacroAssembler* masm = new MacroAssembler(&buffer);\n+\n+  const Array<SigEntry>* sig_vk = vk->extended_sig();\n+  const Array<VMRegPair>* regs = vk->return_regs();\n+\n+  int pack_fields_jobject_off = __ offset();\n+  \/\/ Resolve pre-allocated buffer from JNI handle.\n+  \/\/ We cannot do this in generate_call_stub() because it requires GC code to be initialized.\n+  __ movptr(rax, Address(r13, 0));\n+  __ resolve_jobject(rax \/* value *\/,\n+                     r12 \/* tmp *\/);\n+  __ movptr(Address(r13, 0), rax);\n+\n+  int pack_fields_off = __ offset();\n+\n+  int j = 1;\n+  for (int i = 0; i < sig_vk->length(); i++) {\n+    BasicType bt = sig_vk->at(i)._bt;\n+    if (bt == T_METADATA) {\n+      continue;\n+    }\n+    if (bt == T_VOID) {\n+      if (sig_vk->at(i-1)._bt == T_LONG ||\n+          sig_vk->at(i-1)._bt == T_DOUBLE) {\n+        j++;\n+      }\n+      continue;\n+    }\n+    int off = sig_vk->at(i)._offset;\n+    assert(off > 0, \"offset in object should be positive\");\n+    VMRegPair pair = regs->at(j);\n+    VMReg r_1 = pair.first();\n+    VMReg r_2 = pair.second();\n+    Address to(rax, off);\n+    if (bt == T_FLOAT) {\n+      __ movflt(to, r_1->as_XMMRegister());\n+    } else if (bt == T_DOUBLE) {\n+      __ movdbl(to, r_1->as_XMMRegister());\n+    } else {\n+      Register val = r_1->as_Register();\n+      assert_different_registers(to.base(), val, r14, r13, rbx, rscratch1);\n+      if (is_reference_type(bt)) {\n+        __ store_heap_oop(to, val, r14, r13, rbx, IN_HEAP | ACCESS_WRITE | IS_DEST_UNINITIALIZED);\n+      } else {\n+        __ store_sized_value(to, r_1->as_Register(), type2aelembytes(bt));\n+      }\n+    }\n+    j++;\n+  }\n+  assert(j == regs->length(), \"missed a field?\");\n+  if (vk->has_nullable_atomic_layout()) {\n+    \/\/ Set the null marker\n+    __ movb(Address(rax, vk->null_marker_offset()), 1);\n+  }\n+  __ ret(0);\n+\n+  int unpack_fields_off = __ offset();\n+\n+  Label skip;\n+  Label not_null;\n+  __ testptr(rax, rax);\n+  __ jcc(Assembler::notZero, not_null);\n+\n+  \/\/ Return value is null. Zero oop registers to make the GC happy.\n+  j = 1;\n+  for (int i = 0; i < sig_vk->length(); i++) {\n+    BasicType bt = sig_vk->at(i)._bt;\n+    if (bt == T_METADATA) {\n+      continue;\n+    }\n+    if (bt == T_VOID) {\n+      if (sig_vk->at(i-1)._bt == T_LONG ||\n+          sig_vk->at(i-1)._bt == T_DOUBLE) {\n+        j++;\n+      }\n+      continue;\n+    }\n+    if (bt == T_OBJECT || bt == T_ARRAY) {\n+      VMRegPair pair = regs->at(j);\n+      VMReg r_1 = pair.first();\n+      __ xorq(r_1->as_Register(), r_1->as_Register());\n+    }\n+    j++;\n+  }\n+  __ jmp(skip);\n+  __ bind(not_null);\n+\n+  j = 1;\n+  for (int i = 0; i < sig_vk->length(); i++) {\n+    BasicType bt = sig_vk->at(i)._bt;\n+    if (bt == T_METADATA) {\n+      continue;\n+    }\n+    if (bt == T_VOID) {\n+      if (sig_vk->at(i-1)._bt == T_LONG ||\n+          sig_vk->at(i-1)._bt == T_DOUBLE) {\n+        j++;\n+      }\n+      continue;\n+    }\n+    int off = sig_vk->at(i)._offset;\n+    assert(off > 0, \"offset in object should be positive\");\n+    VMRegPair pair = regs->at(j);\n+    VMReg r_1 = pair.first();\n+    VMReg r_2 = pair.second();\n+    Address from(rax, off);\n+    if (bt == T_FLOAT) {\n+      __ movflt(r_1->as_XMMRegister(), from);\n+    } else if (bt == T_DOUBLE) {\n+      __ movdbl(r_1->as_XMMRegister(), from);\n+    } else if (bt == T_OBJECT || bt == T_ARRAY) {\n+      assert_different_registers(rax, r_1->as_Register());\n+      __ load_heap_oop(r_1->as_Register(), from);\n+    } else {\n+      assert(is_java_primitive(bt), \"unexpected basic type\");\n+      assert_different_registers(rax, r_1->as_Register());\n+      size_t size_in_bytes = type2aelembytes(bt);\n+      __ load_sized_value(r_1->as_Register(), from, size_in_bytes, bt != T_CHAR && bt != T_BOOLEAN);\n+    }\n+    j++;\n+  }\n+  assert(j == regs->length(), \"missed a field?\");\n+\n+  __ bind(skip);\n+  __ ret(0);\n+\n+  __ flush();\n+\n+  return BufferedInlineTypeBlob::create(&buffer, pack_fields_off, pack_fields_jobject_off, unpack_fields_off);\n+}\n+\n@@ -3605,1 +4019,0 @@\n-\n","filename":"src\/hotspot\/cpu\/x86\/sharedRuntime_x86_64.cpp","additions":559,"deletions":146,"binary":false,"changes":705,"status":"modified"},{"patch":"@@ -1652,0 +1652,4 @@\n+  if (_entry_point == nullptr) {\n+    \/\/ CallLeafNoFPInDirect\n+    return 3; \/\/ callq (register)\n+  }\n@@ -1658,0 +1662,1 @@\n+\n@@ -1883,6 +1888,1 @@\n-  int framesize = C->output()->frame_size_in_bytes();\n-  int bangsize = C->output()->bang_size_in_bytes();\n-\n-  if (C->clinit_barrier_on_entry()) {\n-    assert(VM_Version::supports_fast_class_init_checks(), \"sanity\");\n-    assert(!C->method()->holder()->is_not_initialized(), \"initialization should have been started\");\n+  __ verified_entry(C);\n@@ -1890,9 +1890,2 @@\n-    Label L_skip_barrier;\n-    Register klass = rscratch1;\n-\n-    __ mov_metadata(klass, C->method()->holder()->constant_encoding());\n-    __ clinit_barrier(klass, &L_skip_barrier \/*L_fast_path*\/);\n-\n-    __ jump(RuntimeAddress(SharedRuntime::get_handle_wrong_method_stub())); \/\/ slow path\n-\n-    __ bind(L_skip_barrier);\n+  if (ra_->C->stub_function() == nullptr) {\n+    __ entry_barrier();\n@@ -1901,1 +1894,3 @@\n-  __ verified_entry(framesize, C->output()->need_stack_bang(bangsize)?bangsize:0, false, C->stub_function() != nullptr);\n+  if (!Compile::current()->output()->in_scratch_emit_size()) {\n+    __ bind(*_verified_entry);\n+  }\n@@ -1913,5 +1908,0 @@\n-uint MachPrologNode::size(PhaseRegAlloc* ra_) const\n-{\n-  return MachNode::size(ra_); \/\/ too many variables; just compute it\n-                              \/\/ the hard way\n-}\n@@ -1965,13 +1955,3 @@\n-  int framesize = C->output()->frame_size_in_bytes();\n-  assert((framesize & (StackAlignmentInBytes-1)) == 0, \"frame size not aligned\");\n-  \/\/ Remove word for return adr already pushed\n-  \/\/ and RBP\n-  framesize -= 2*wordSize;\n-\n-  \/\/ Note that VerifyStackAtCalls' Majik cookie does not change the frame size popped here\n-\n-  if (framesize) {\n-    __ addq(rsp, framesize);\n-  }\n-\n-  __ popq(rbp);\n+  \/\/ Subtract two words to account for return address and rbp\n+  int initial_framesize = C->output()->frame_size_in_bytes() - 2*wordSize;\n+  __ remove_frame(initial_framesize, C->needs_stack_repair());\n@@ -1996,6 +1976,0 @@\n-uint MachEpilogNode::size(PhaseRegAlloc* ra_) const\n-{\n-  return MachNode::size(ra_); \/\/ too many variables; just compute it\n-                              \/\/ the hard way\n-}\n-\n@@ -2603,0 +2577,43 @@\n+\/\/=============================================================================\n+#ifndef PRODUCT\n+void MachVEPNode::format(PhaseRegAlloc* ra_, outputStream* st) const\n+{\n+  st->print_cr(\"MachVEPNode\");\n+}\n+#endif\n+\n+void MachVEPNode::emit(C2_MacroAssembler* masm, PhaseRegAlloc* ra_) const\n+{\n+  CodeBuffer* cbuf = masm->code();\n+  uint insts_size = cbuf->insts_size();\n+  if (!_verified) {\n+    __ ic_check(1);\n+  } else {\n+    \/\/ TODO 8284443 Avoid creation of temporary frame\n+    if (ra_->C->stub_function() == nullptr) {\n+      __ verified_entry(ra_->C, 0);\n+      __ entry_barrier();\n+      int initial_framesize = ra_->C->output()->frame_size_in_bytes() - 2*wordSize;\n+      __ remove_frame(initial_framesize, false);\n+    }\n+    \/\/ Unpack inline type args passed as oop and then jump to\n+    \/\/ the verified entry point (skipping the unverified entry).\n+    int sp_inc = __ unpack_inline_args(ra_->C, _receiver_only);\n+    \/\/ Emit code for verified entry and save increment for stack repair on return\n+    __ verified_entry(ra_->C, sp_inc);\n+    if (Compile::current()->output()->in_scratch_emit_size()) {\n+      Label dummy_verified_entry;\n+      __ jmp(dummy_verified_entry);\n+    } else {\n+      __ jmp(*_verified_entry);\n+    }\n+  }\n+  \/* WARNING these NOPs are critical so that verified entry point is properly\n+     4 bytes aligned for patching by NativeJump::patch_verified_entry() *\/\n+  int nops_cnt = 4 - ((cbuf->insts_size() - insts_size) & 0x3);\n+  nops_cnt &= 0x3; \/\/ Do not add nops if code is aligned.\n+  if (nops_cnt > 0) {\n+    __ nop(nops_cnt);\n+  }\n+}\n+\n@@ -2623,6 +2640,0 @@\n-uint MachUEPNode::size(PhaseRegAlloc* ra_) const\n-{\n-  return MachNode::size(ra_); \/\/ too many variables; just compute it\n-                              \/\/ the hard way\n-}\n-\n@@ -4561,0 +4572,33 @@\n+    if (tf()->returns_inline_type_as_fields() && !_method->is_method_handle_intrinsic() && _method->return_type()->is_loaded()) {\n+      \/\/ The last return value is not set by the callee but used to pass the null marker to compiled code.\n+      \/\/ Search for the corresponding projection, get the register and emit code that initialized it.\n+      uint con = (tf()->range_cc()->cnt() - 1);\n+      for (DUIterator_Fast imax, i = fast_outs(imax); i < imax; i++) {\n+        ProjNode* proj = fast_out(i)->as_Proj();\n+        if (proj->_con == con) {\n+          \/\/ Set null marker if rax is non-null (a non-null value is returned buffered or scalarized)\n+          OptoReg::Name optoReg = ra_->get_reg_first(proj);\n+          VMReg reg = OptoReg::as_VMReg(optoReg, ra_->_framesize, OptoReg::reg2stack(ra_->_matcher._new_SP));\n+          Register toReg = reg->is_reg() ? reg->as_Register() : rscratch1;\n+          __ testq(rax, rax);\n+          __ setb(Assembler::notZero, toReg);\n+          __ movzbl(toReg, toReg);\n+          if (reg->is_stack()) {\n+            int st_off = reg->reg2stack() * VMRegImpl::stack_slot_size;\n+            __ movq(Address(rsp, st_off), toReg);\n+          }\n+          break;\n+        }\n+      }\n+      if (return_value_is_used()) {\n+        \/\/ An inline type is returned as fields in multiple registers.\n+        \/\/ Rax either contains an oop if the inline type is buffered or a pointer\n+        \/\/ to the corresponding InlineKlass with the lowest bit set to 1. Zero rax\n+        \/\/ if the lowest bit is set to allow C2 to use the oop after null checking.\n+        \/\/ rax &= (rax & 1) - 1\n+        __ movptr(rscratch1, rax);\n+        __ andptr(rscratch1, 0x1);\n+        __ subptr(rscratch1, 0x1);\n+        __ andptr(rax, rscratch1);\n+      }\n+    }\n@@ -5723,0 +5767,16 @@\n+\/\/ Indirect Narrow Oop Operand\n+operand indCompressedOop(rRegN reg) %{\n+  predicate(UseCompressedOops && (CompressedOops::shift() == Address::times_8));\n+  constraint(ALLOC_IN_RC(ptr_reg));\n+  match(DecodeN reg);\n+\n+  op_cost(10);\n+  format %{\"[R12 + $reg << 3] (compressed oop addressing)\" %}\n+  interface(MEMORY_INTER) %{\n+    base(0xc); \/\/ R12\n+    index($reg);\n+    scale(0x3);\n+    disp(0x0);\n+  %}\n+%}\n+\n@@ -6199,1 +6259,1 @@\n-               indCompressedOopOffset,\n+               indCompressedOop, indCompressedOopOffset,\n@@ -8797,0 +8857,26 @@\n+instruct castI2N(rRegN dst, rRegI src)\n+%{\n+  match(Set dst (CastI2N src));\n+\n+  format %{ \"movq    $dst, $src\\t# int -> narrow ptr\" %}\n+  ins_encode %{\n+    if ($dst$$reg != $src$$reg) {\n+      __ movl($dst$$Register, $src$$Register);\n+    }\n+  %}\n+  ins_pipe(ialu_reg_reg); \/\/ XXX\n+%}\n+\n+instruct castN2X(rRegL dst, rRegN src)\n+%{\n+  match(Set dst (CastP2X src));\n+\n+  format %{ \"movq    $dst, $src\\t# ptr -> long\" %}\n+  ins_encode %{\n+    if ($dst$$reg != $src$$reg) {\n+      __ movptr($dst$$Register, $src$$Register);\n+    }\n+  %}\n+  ins_pipe(ialu_reg_reg); \/\/ XXX\n+%}\n+\n@@ -15020,0 +15106,1 @@\n+\n@@ -15022,1 +15109,1 @@\n-instruct rep_stos(rcx_RegL cnt, rdi_RegP base, regD tmp, rax_RegI zero,\n+instruct rep_stos(rcx_RegL cnt, rdi_RegP base, regD tmp, rax_RegL val,\n@@ -15025,3 +15112,120 @@\n-  predicate(!((ClearArrayNode*)n)->is_large() && (UseAVX <= 2));\n-  match(Set dummy (ClearArray cnt base));\n-  effect(USE_KILL cnt, USE_KILL base, TEMP tmp, KILL zero, KILL cr);\n+  predicate(!((ClearArrayNode*)n)->is_large() && !((ClearArrayNode*)n)->word_copy_only() && (UseAVX <= 2));\n+  match(Set dummy (ClearArray (Binary cnt base) val));\n+  effect(USE_KILL cnt, USE_KILL base, TEMP tmp, USE_KILL val, KILL cr);\n+\n+  format %{ $$template\n+    $$emit$$\"cmp     InitArrayShortSize,rcx\\n\\t\"\n+    $$emit$$\"jg      LARGE\\n\\t\"\n+    $$emit$$\"dec     rcx\\n\\t\"\n+    $$emit$$\"js      DONE\\t# Zero length\\n\\t\"\n+    $$emit$$\"mov     rax,(rdi,rcx,8)\\t# LOOP\\n\\t\"\n+    $$emit$$\"dec     rcx\\n\\t\"\n+    $$emit$$\"jge     LOOP\\n\\t\"\n+    $$emit$$\"jmp     DONE\\n\\t\"\n+    $$emit$$\"# LARGE:\\n\\t\"\n+    if (UseFastStosb) {\n+       $$emit$$\"shlq    rcx,3\\t# Convert doublewords to bytes\\n\\t\"\n+       $$emit$$\"rep     stosb\\t# Store rax to *rdi++ while rcx--\\n\\t\"\n+    } else if (UseXMMForObjInit) {\n+       $$emit$$\"movdq   $tmp, $val\\n\\t\"\n+       $$emit$$\"punpcklqdq $tmp, $tmp\\n\\t\"\n+       $$emit$$\"vinserti128_high $tmp, $tmp\\n\\t\"\n+       $$emit$$\"jmpq    L_zero_64_bytes\\n\\t\"\n+       $$emit$$\"# L_loop:\\t# 64-byte LOOP\\n\\t\"\n+       $$emit$$\"vmovdqu $tmp,(rax)\\n\\t\"\n+       $$emit$$\"vmovdqu $tmp,0x20(rax)\\n\\t\"\n+       $$emit$$\"add     0x40,rax\\n\\t\"\n+       $$emit$$\"# L_zero_64_bytes:\\n\\t\"\n+       $$emit$$\"sub     0x8,rcx\\n\\t\"\n+       $$emit$$\"jge     L_loop\\n\\t\"\n+       $$emit$$\"add     0x4,rcx\\n\\t\"\n+       $$emit$$\"jl      L_tail\\n\\t\"\n+       $$emit$$\"vmovdqu $tmp,(rax)\\n\\t\"\n+       $$emit$$\"add     0x20,rax\\n\\t\"\n+       $$emit$$\"sub     0x4,rcx\\n\\t\"\n+       $$emit$$\"# L_tail:\\t# Clearing tail bytes\\n\\t\"\n+       $$emit$$\"add     0x4,rcx\\n\\t\"\n+       $$emit$$\"jle     L_end\\n\\t\"\n+       $$emit$$\"dec     rcx\\n\\t\"\n+       $$emit$$\"# L_sloop:\\t# 8-byte short loop\\n\\t\"\n+       $$emit$$\"vmovq   xmm0,(rax)\\n\\t\"\n+       $$emit$$\"add     0x8,rax\\n\\t\"\n+       $$emit$$\"dec     rcx\\n\\t\"\n+       $$emit$$\"jge     L_sloop\\n\\t\"\n+       $$emit$$\"# L_end:\\n\\t\"\n+    } else {\n+       $$emit$$\"rep     stosq\\t# Store rax to *rdi++ while rcx--\\n\\t\"\n+    }\n+    $$emit$$\"# DONE\"\n+  %}\n+  ins_encode %{\n+    __ clear_mem($base$$Register, $cnt$$Register, $val$$Register,\n+                 $tmp$$XMMRegister, false, false);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct rep_stos_word_copy(rcx_RegL cnt, rdi_RegP base, regD tmp, rax_RegL val,\n+                            Universe dummy, rFlagsReg cr)\n+%{\n+  predicate(!((ClearArrayNode*)n)->is_large() && ((ClearArrayNode*)n)->word_copy_only() && (UseAVX <= 2));\n+  match(Set dummy (ClearArray (Binary cnt base) val));\n+  effect(USE_KILL cnt, USE_KILL base, TEMP tmp, USE_KILL val, KILL cr);\n+\n+  format %{ $$template\n+    $$emit$$\"cmp     InitArrayShortSize,rcx\\n\\t\"\n+    $$emit$$\"jg      LARGE\\n\\t\"\n+    $$emit$$\"dec     rcx\\n\\t\"\n+    $$emit$$\"js      DONE\\t# Zero length\\n\\t\"\n+    $$emit$$\"mov     rax,(rdi,rcx,8)\\t# LOOP\\n\\t\"\n+    $$emit$$\"dec     rcx\\n\\t\"\n+    $$emit$$\"jge     LOOP\\n\\t\"\n+    $$emit$$\"jmp     DONE\\n\\t\"\n+    $$emit$$\"# LARGE:\\n\\t\"\n+    if (UseXMMForObjInit) {\n+       $$emit$$\"movdq   $tmp, $val\\n\\t\"\n+       $$emit$$\"punpcklqdq $tmp, $tmp\\n\\t\"\n+       $$emit$$\"vinserti128_high $tmp, $tmp\\n\\t\"\n+       $$emit$$\"jmpq    L_zero_64_bytes\\n\\t\"\n+       $$emit$$\"# L_loop:\\t# 64-byte LOOP\\n\\t\"\n+       $$emit$$\"vmovdqu $tmp,(rax)\\n\\t\"\n+       $$emit$$\"vmovdqu $tmp,0x20(rax)\\n\\t\"\n+       $$emit$$\"add     0x40,rax\\n\\t\"\n+       $$emit$$\"# L_zero_64_bytes:\\n\\t\"\n+       $$emit$$\"sub     0x8,rcx\\n\\t\"\n+       $$emit$$\"jge     L_loop\\n\\t\"\n+       $$emit$$\"add     0x4,rcx\\n\\t\"\n+       $$emit$$\"jl      L_tail\\n\\t\"\n+       $$emit$$\"vmovdqu $tmp,(rax)\\n\\t\"\n+       $$emit$$\"add     0x20,rax\\n\\t\"\n+       $$emit$$\"sub     0x4,rcx\\n\\t\"\n+       $$emit$$\"# L_tail:\\t# Clearing tail bytes\\n\\t\"\n+       $$emit$$\"add     0x4,rcx\\n\\t\"\n+       $$emit$$\"jle     L_end\\n\\t\"\n+       $$emit$$\"dec     rcx\\n\\t\"\n+       $$emit$$\"# L_sloop:\\t# 8-byte short loop\\n\\t\"\n+       $$emit$$\"vmovq   xmm0,(rax)\\n\\t\"\n+       $$emit$$\"add     0x8,rax\\n\\t\"\n+       $$emit$$\"dec     rcx\\n\\t\"\n+       $$emit$$\"jge     L_sloop\\n\\t\"\n+       $$emit$$\"# L_end:\\n\\t\"\n+    } else {\n+       $$emit$$\"rep     stosq\\t# Store rax to *rdi++ while rcx--\\n\\t\"\n+    }\n+    $$emit$$\"# DONE\"\n+  %}\n+  ins_encode %{\n+    __ clear_mem($base$$Register, $cnt$$Register, $val$$Register,\n+                 $tmp$$XMMRegister, false, true);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ Small non-constant length ClearArray for AVX512 targets.\n+instruct rep_stos_evex(rcx_RegL cnt, rdi_RegP base, legRegD tmp, kReg ktmp, rax_RegL val,\n+                       Universe dummy, rFlagsReg cr)\n+%{\n+  predicate(!((ClearArrayNode*)n)->is_large() && !((ClearArrayNode*)n)->word_copy_only() && (UseAVX > 2));\n+  match(Set dummy (ClearArray (Binary cnt base) val));\n+  ins_cost(125);\n+  effect(USE_KILL cnt, USE_KILL base, TEMP tmp, TEMP ktmp, USE_KILL val, KILL cr);\n@@ -15075,2 +15279,2 @@\n-    __ clear_mem($base$$Register, $cnt$$Register, $zero$$Register,\n-                 $tmp$$XMMRegister, false, knoreg);\n+    __ clear_mem($base$$Register, $cnt$$Register, $val$$Register,\n+                 $tmp$$XMMRegister, false, false, $ktmp$$KRegister);\n@@ -15081,3 +15285,2 @@\n-\/\/ Small non-constant length ClearArray for AVX512 targets.\n-instruct rep_stos_evex(rcx_RegL cnt, rdi_RegP base, legRegD tmp, kReg ktmp, rax_RegI zero,\n-                       Universe dummy, rFlagsReg cr)\n+instruct rep_stos_evex_word_copy(rcx_RegL cnt, rdi_RegP base, legRegD tmp, kReg ktmp, rax_RegL val,\n+                                 Universe dummy, rFlagsReg cr)\n@@ -15085,2 +15288,2 @@\n-  predicate(!((ClearArrayNode*)n)->is_large() && (UseAVX > 2));\n-  match(Set dummy (ClearArray cnt base));\n+  predicate(!((ClearArrayNode*)n)->is_large() && ((ClearArrayNode*)n)->word_copy_only() && (UseAVX > 2));\n+  match(Set dummy (ClearArray (Binary cnt base) val));\n@@ -15088,1 +15291,1 @@\n-  effect(USE_KILL cnt, USE_KILL base, TEMP tmp, TEMP ktmp, KILL zero, KILL cr);\n+  effect(USE_KILL cnt, USE_KILL base, TEMP tmp, TEMP ktmp, USE_KILL val, KILL cr);\n@@ -15136,2 +15339,2 @@\n-    __ clear_mem($base$$Register, $cnt$$Register, $zero$$Register,\n-                 $tmp$$XMMRegister, false, $ktmp$$KRegister);\n+    __ clear_mem($base$$Register, $cnt$$Register, $val$$Register,\n+                 $tmp$$XMMRegister, false, true, $ktmp$$KRegister);\n@@ -15143,1 +15346,1 @@\n-instruct rep_stos_large(rcx_RegL cnt, rdi_RegP base, regD tmp, rax_RegI zero,\n+instruct rep_stos_large(rcx_RegL cnt, rdi_RegP base, regD tmp, rax_RegL val,\n@@ -15146,3 +15349,99 @@\n-  predicate((UseAVX <=2) && ((ClearArrayNode*)n)->is_large());\n-  match(Set dummy (ClearArray cnt base));\n-  effect(USE_KILL cnt, USE_KILL base, TEMP tmp, KILL zero, KILL cr);\n+  predicate(((ClearArrayNode*)n)->is_large() && !((ClearArrayNode*)n)->word_copy_only() && (UseAVX <= 2));\n+  match(Set dummy (ClearArray (Binary cnt base) val));\n+  effect(USE_KILL cnt, USE_KILL base, TEMP tmp, USE_KILL val, KILL cr);\n+\n+  format %{ $$template\n+    if (UseFastStosb) {\n+       $$emit$$\"shlq    rcx,3\\t# Convert doublewords to bytes\\n\\t\"\n+       $$emit$$\"rep     stosb\\t# Store rax to *rdi++ while rcx--\"\n+    } else if (UseXMMForObjInit) {\n+       $$emit$$\"movdq   $tmp, $val\\n\\t\"\n+       $$emit$$\"punpcklqdq $tmp, $tmp\\n\\t\"\n+       $$emit$$\"vinserti128_high $tmp, $tmp\\n\\t\"\n+       $$emit$$\"jmpq    L_zero_64_bytes\\n\\t\"\n+       $$emit$$\"# L_loop:\\t# 64-byte LOOP\\n\\t\"\n+       $$emit$$\"vmovdqu $tmp,(rax)\\n\\t\"\n+       $$emit$$\"vmovdqu $tmp,0x20(rax)\\n\\t\"\n+       $$emit$$\"add     0x40,rax\\n\\t\"\n+       $$emit$$\"# L_zero_64_bytes:\\n\\t\"\n+       $$emit$$\"sub     0x8,rcx\\n\\t\"\n+       $$emit$$\"jge     L_loop\\n\\t\"\n+       $$emit$$\"add     0x4,rcx\\n\\t\"\n+       $$emit$$\"jl      L_tail\\n\\t\"\n+       $$emit$$\"vmovdqu $tmp,(rax)\\n\\t\"\n+       $$emit$$\"add     0x20,rax\\n\\t\"\n+       $$emit$$\"sub     0x4,rcx\\n\\t\"\n+       $$emit$$\"# L_tail:\\t# Clearing tail bytes\\n\\t\"\n+       $$emit$$\"add     0x4,rcx\\n\\t\"\n+       $$emit$$\"jle     L_end\\n\\t\"\n+       $$emit$$\"dec     rcx\\n\\t\"\n+       $$emit$$\"# L_sloop:\\t# 8-byte short loop\\n\\t\"\n+       $$emit$$\"vmovq   xmm0,(rax)\\n\\t\"\n+       $$emit$$\"add     0x8,rax\\n\\t\"\n+       $$emit$$\"dec     rcx\\n\\t\"\n+       $$emit$$\"jge     L_sloop\\n\\t\"\n+       $$emit$$\"# L_end:\\n\\t\"\n+    } else {\n+       $$emit$$\"rep     stosq\\t# Store rax to *rdi++ while rcx--\"\n+    }\n+  %}\n+  ins_encode %{\n+    __ clear_mem($base$$Register, $cnt$$Register, $val$$Register,\n+                 $tmp$$XMMRegister, true, false);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct rep_stos_large_word_copy(rcx_RegL cnt, rdi_RegP base, regD tmp, rax_RegL val,\n+                                  Universe dummy, rFlagsReg cr)\n+%{\n+  predicate(((ClearArrayNode*)n)->is_large() && ((ClearArrayNode*)n)->word_copy_only() && (UseAVX <= 2));\n+  match(Set dummy (ClearArray (Binary cnt base) val));\n+  effect(USE_KILL cnt, USE_KILL base, TEMP tmp, USE_KILL val, KILL cr);\n+\n+  format %{ $$template\n+    if (UseXMMForObjInit) {\n+       $$emit$$\"movdq   $tmp, $val\\n\\t\"\n+       $$emit$$\"punpcklqdq $tmp, $tmp\\n\\t\"\n+       $$emit$$\"vinserti128_high $tmp, $tmp\\n\\t\"\n+       $$emit$$\"jmpq    L_zero_64_bytes\\n\\t\"\n+       $$emit$$\"# L_loop:\\t# 64-byte LOOP\\n\\t\"\n+       $$emit$$\"vmovdqu $tmp,(rax)\\n\\t\"\n+       $$emit$$\"vmovdqu $tmp,0x20(rax)\\n\\t\"\n+       $$emit$$\"add     0x40,rax\\n\\t\"\n+       $$emit$$\"# L_zero_64_bytes:\\n\\t\"\n+       $$emit$$\"sub     0x8,rcx\\n\\t\"\n+       $$emit$$\"jge     L_loop\\n\\t\"\n+       $$emit$$\"add     0x4,rcx\\n\\t\"\n+       $$emit$$\"jl      L_tail\\n\\t\"\n+       $$emit$$\"vmovdqu $tmp,(rax)\\n\\t\"\n+       $$emit$$\"add     0x20,rax\\n\\t\"\n+       $$emit$$\"sub     0x4,rcx\\n\\t\"\n+       $$emit$$\"# L_tail:\\t# Clearing tail bytes\\n\\t\"\n+       $$emit$$\"add     0x4,rcx\\n\\t\"\n+       $$emit$$\"jle     L_end\\n\\t\"\n+       $$emit$$\"dec     rcx\\n\\t\"\n+       $$emit$$\"# L_sloop:\\t# 8-byte short loop\\n\\t\"\n+       $$emit$$\"vmovq   xmm0,(rax)\\n\\t\"\n+       $$emit$$\"add     0x8,rax\\n\\t\"\n+       $$emit$$\"dec     rcx\\n\\t\"\n+       $$emit$$\"jge     L_sloop\\n\\t\"\n+       $$emit$$\"# L_end:\\n\\t\"\n+    } else {\n+       $$emit$$\"rep     stosq\\t# Store rax to *rdi++ while rcx--\"\n+    }\n+  %}\n+  ins_encode %{\n+    __ clear_mem($base$$Register, $cnt$$Register, $val$$Register,\n+                 $tmp$$XMMRegister, true, true);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ Large non-constant length ClearArray for AVX512 targets.\n+instruct rep_stos_large_evex(rcx_RegL cnt, rdi_RegP base, legRegD tmp, kReg ktmp, rax_RegL val,\n+                             Universe dummy, rFlagsReg cr)\n+%{\n+  predicate(((ClearArrayNode*)n)->is_large() && !((ClearArrayNode*)n)->word_copy_only() && (UseAVX > 2));\n+  match(Set dummy (ClearArray (Binary cnt base) val));\n+  effect(USE_KILL cnt, USE_KILL base, TEMP tmp, TEMP ktmp, USE_KILL val, KILL cr);\n@@ -15187,2 +15486,2 @@\n-    __ clear_mem($base$$Register, $cnt$$Register, $zero$$Register,\n-                 $tmp$$XMMRegister, true, knoreg);\n+    __ clear_mem($base$$Register, $cnt$$Register, $val$$Register,\n+                 $tmp$$XMMRegister, true, false, $ktmp$$KRegister);\n@@ -15193,3 +15492,2 @@\n-\/\/ Large non-constant length ClearArray for AVX512 targets.\n-instruct rep_stos_large_evex(rcx_RegL cnt, rdi_RegP base, legRegD tmp, kReg ktmp, rax_RegI zero,\n-                             Universe dummy, rFlagsReg cr)\n+instruct rep_stos_large_evex_word_copy(rcx_RegL cnt, rdi_RegP base, legRegD tmp, kReg ktmp, rax_RegL val,\n+                                       Universe dummy, rFlagsReg cr)\n@@ -15197,3 +15495,3 @@\n-  predicate((UseAVX > 2) && ((ClearArrayNode*)n)->is_large());\n-  match(Set dummy (ClearArray cnt base));\n-  effect(USE_KILL cnt, USE_KILL base, TEMP tmp, TEMP ktmp, KILL zero, KILL cr);\n+  predicate(((ClearArrayNode*)n)->is_large() && ((ClearArrayNode*)n)->word_copy_only() && (UseAVX > 2));\n+  match(Set dummy (ClearArray (Binary cnt base) val));\n+  effect(USE_KILL cnt, USE_KILL base, TEMP tmp, TEMP ktmp, USE_KILL val, KILL cr);\n@@ -15238,2 +15536,2 @@\n-    __ clear_mem($base$$Register, $cnt$$Register, $zero$$Register,\n-                 $tmp$$XMMRegister, true, $ktmp$$KRegister);\n+    __ clear_mem($base$$Register, $cnt$$Register, $val$$Register,\n+                 $tmp$$XMMRegister, true, true, $ktmp$$KRegister);\n@@ -15245,1 +15543,1 @@\n-instruct rep_stos_im(immL cnt, rRegP base, regD tmp, rRegI zero, kReg ktmp, Universe dummy, rFlagsReg cr)\n+instruct rep_stos_im(immL cnt, rRegP base, regD tmp, rax_RegL val, kReg ktmp, Universe dummy, rFlagsReg cr)\n@@ -15247,2 +15545,3 @@\n-  predicate(!((ClearArrayNode*)n)->is_large() && (MaxVectorSize >= 32) && VM_Version::supports_avx512vl());\n-  match(Set dummy (ClearArray cnt base));\n+  predicate(!((ClearArrayNode*)n)->is_large() && !((ClearArrayNode*)n)->word_copy_only() &&\n+            ((MaxVectorSize >= 32) && VM_Version::supports_avx512vl()));\n+  match(Set dummy (ClearArray (Binary cnt base) val));\n@@ -15250,1 +15549,1 @@\n-  effect(TEMP tmp, TEMP zero, TEMP ktmp, KILL cr);\n+  effect(TEMP tmp, USE_KILL val, TEMP ktmp, KILL cr);\n@@ -15253,1 +15552,1 @@\n-   __ clear_mem($base$$Register, $cnt$$constant, $zero$$Register, $tmp$$XMMRegister, $ktmp$$KRegister);\n+    __ clear_mem($base$$Register, $cnt$$constant, $val$$Register, $tmp$$XMMRegister, $ktmp$$KRegister);\n@@ -17098,0 +17397,16 @@\n+\/\/ Call runtime without safepoint\n+\/\/ entry point is null, target holds the address to call\n+instruct CallLeafNoFPInDirect(rRegP target)\n+%{\n+  predicate(n->as_Call()->entry_point() == nullptr);\n+  match(CallLeafNoFP target);\n+\n+  ins_cost(300);\n+  format %{ \"call_leaf_nofp,runtime indirect \" %}\n+  ins_encode %{\n+     __ call($target$$Register);\n+  %}\n+\n+  ins_pipe(pipe_slow);\n+%}\n+\n@@ -17101,0 +17416,1 @@\n+  predicate(n->as_Call()->entry_point() != nullptr);\n","filename":"src\/hotspot\/cpu\/x86\/x86.ad","additions":396,"deletions":80,"binary":false,"changes":476,"status":"modified"},{"patch":"@@ -231,0 +231,77 @@\n+class LoadFlattenedArrayStub: public CodeStub {\n+ private:\n+  LIR_Opr          _array;\n+  LIR_Opr          _index;\n+  LIR_Opr          _result;\n+  LIR_Opr          _scratch_reg;\n+  CodeEmitInfo*    _info;\n+\n+ public:\n+  LoadFlattenedArrayStub(LIR_Opr array, LIR_Opr index, LIR_Opr result, CodeEmitInfo* info);\n+  virtual void emit_code(LIR_Assembler* e);\n+  virtual CodeEmitInfo* info() const             { return _info; }\n+  virtual void visit(LIR_OpVisitState* visitor) {\n+    visitor->do_slow_case(_info);\n+    visitor->do_input(_array);\n+    visitor->do_input(_index);\n+    visitor->do_output(_result);\n+    if (_scratch_reg != LIR_OprFact::illegalOpr) {\n+      visitor->do_temp(_scratch_reg);\n+    }\n+  }\n+\n+#ifndef PRODUCT\n+  virtual void print_name(outputStream* out) const { out->print(\"LoadFlattenedArrayStub\"); }\n+#endif \/\/ PRODUCT\n+};\n+\n+\n+class StoreFlattenedArrayStub: public CodeStub {\n+ private:\n+  LIR_Opr          _array;\n+  LIR_Opr          _index;\n+  LIR_Opr          _value;\n+  LIR_Opr          _scratch_reg;\n+  CodeEmitInfo*    _info;\n+\n+ public:\n+  StoreFlattenedArrayStub(LIR_Opr array, LIR_Opr index, LIR_Opr value, CodeEmitInfo* info);\n+  virtual void emit_code(LIR_Assembler* e);\n+  virtual CodeEmitInfo* info() const             { return _info; }\n+  virtual void visit(LIR_OpVisitState* visitor) {\n+    visitor->do_slow_case(_info);\n+    visitor->do_input(_array);\n+    visitor->do_input(_index);\n+    visitor->do_input(_value);\n+    if (_scratch_reg != LIR_OprFact::illegalOpr) {\n+      visitor->do_temp(_scratch_reg);\n+    }\n+  }\n+#ifndef PRODUCT\n+  virtual void print_name(outputStream* out) const { out->print(\"StoreFlattenedArrayStub\"); }\n+#endif \/\/ PRODUCT\n+};\n+\n+class SubstitutabilityCheckStub: public CodeStub {\n+ private:\n+  LIR_Opr          _left;\n+  LIR_Opr          _right;\n+  LIR_Opr          _scratch_reg;\n+  CodeEmitInfo*    _info;\n+ public:\n+  SubstitutabilityCheckStub(LIR_Opr left, LIR_Opr right, CodeEmitInfo* info);\n+  virtual void emit_code(LIR_Assembler* e);\n+  virtual CodeEmitInfo* info() const             { return _info; }\n+  virtual void visit(LIR_OpVisitState* visitor) {\n+    visitor->do_slow_case(_info);\n+    visitor->do_input(_left);\n+    visitor->do_input(_right);\n+    if (_scratch_reg != LIR_OprFact::illegalOpr) {\n+      visitor->do_temp(_scratch_reg);\n+    }\n+  }\n+#ifndef PRODUCT\n+  virtual void print_name(outputStream* out) const { out->print(\"SubstitutabilityCheckStub\"); }\n+#endif \/\/ PRODUCT\n+};\n+\n@@ -283,1 +360,1 @@\n-\n+  bool           _is_null_free;\n@@ -285,1 +362,1 @@\n-  NewObjectArrayStub(LIR_Opr klass_reg, LIR_Opr length, LIR_Opr result, CodeEmitInfo* info);\n+  NewObjectArrayStub(LIR_Opr klass_reg, LIR_Opr length, LIR_Opr result, CodeEmitInfo* info, bool is_null_free);\n@@ -320,0 +397,2 @@\n+  CodeStub* _throw_ie_stub;\n+  LIR_Opr _scratch_reg;\n@@ -322,1 +401,2 @@\n-  MonitorEnterStub(LIR_Opr obj_reg, LIR_Opr lock_reg, CodeEmitInfo* info)\n+  MonitorEnterStub(LIR_Opr obj_reg, LIR_Opr lock_reg, CodeEmitInfo* info,\n+                   CodeStub* throw_ie_stub = nullptr, LIR_Opr scratch_reg = LIR_OprFact::illegalOpr)\n@@ -325,0 +405,5 @@\n+    _scratch_reg = scratch_reg;\n+    _throw_ie_stub = throw_ie_stub;\n+    if (_throw_ie_stub != nullptr) {\n+      assert(_scratch_reg != LIR_OprFact::illegalOpr, \"must be\");\n+    }\n@@ -334,0 +419,3 @@\n+    if (_scratch_reg != LIR_OprFact::illegalOpr) {\n+      visitor->do_temp(_scratch_reg);\n+    }\n","filename":"src\/hotspot\/share\/c1\/c1_CodeStubs.hpp","additions":91,"deletions":3,"binary":false,"changes":94,"status":"modified"},{"patch":"@@ -30,0 +30,1 @@\n+#include \"ci\/ciInlineKlass.hpp\"\n@@ -289,1 +290,1 @@\n-                                 CodeStub* stub)\n+                                 CodeStub* stub, bool need_null_check)\n@@ -305,0 +306,1 @@\n+  , _need_null_check(need_null_check)\n@@ -332,0 +334,1 @@\n+  , _need_null_check(true)\n@@ -341,0 +344,31 @@\n+LIR_OpFlattenedArrayCheck::LIR_OpFlattenedArrayCheck(LIR_Opr array, LIR_Opr value, LIR_Opr tmp, CodeStub* stub)\n+  : LIR_Op(lir_flat_array_check, LIR_OprFact::illegalOpr, nullptr)\n+  , _array(array)\n+  , _value(value)\n+  , _tmp(tmp)\n+  , _stub(stub) {}\n+\n+\n+LIR_OpNullFreeArrayCheck::LIR_OpNullFreeArrayCheck(LIR_Opr array, LIR_Opr tmp)\n+  : LIR_Op(lir_null_free_array_check, LIR_OprFact::illegalOpr, nullptr)\n+  , _array(array)\n+  , _tmp(tmp) {}\n+\n+\n+LIR_OpSubstitutabilityCheck::LIR_OpSubstitutabilityCheck(LIR_Opr result, LIR_Opr left, LIR_Opr right, LIR_Opr equal_result, LIR_Opr not_equal_result,\n+                                                         LIR_Opr tmp1, LIR_Opr tmp2,\n+                                                         ciKlass* left_klass, ciKlass* right_klass, LIR_Opr left_klass_op, LIR_Opr right_klass_op,\n+                                                         CodeEmitInfo* info, CodeStub* stub)\n+  : LIR_Op(lir_substitutability_check, result, info)\n+  , _left(left)\n+  , _right(right)\n+  , _equal_result(equal_result)\n+  , _not_equal_result(not_equal_result)\n+  , _tmp1(tmp1)\n+  , _tmp2(tmp2)\n+  , _left_klass(left_klass)\n+  , _right_klass(right_klass)\n+  , _left_klass_op(left_klass_op)\n+  , _right_klass_op(right_klass_op)\n+  , _stub(stub) {}\n+\n@@ -415,0 +449,1 @@\n+    case lir_check_orig_pc:            \/\/ result and info always invalid\n@@ -792,0 +827,1 @@\n+      do_stub(opLock->_throw_ie_stub);\n@@ -819,0 +855,47 @@\n+\/\/ LIR_OpFlattenedArrayCheck\n+    case lir_flat_array_check: {\n+      assert(op->as_OpFlattenedArrayCheck() != nullptr, \"must be\");\n+      LIR_OpFlattenedArrayCheck* opFlattenedArrayCheck = (LIR_OpFlattenedArrayCheck*)op;\n+\n+      if (opFlattenedArrayCheck->_array->is_valid()) do_input(opFlattenedArrayCheck->_array);\n+      if (opFlattenedArrayCheck->_value->is_valid()) do_input(opFlattenedArrayCheck->_value);\n+      if (opFlattenedArrayCheck->_tmp->is_valid())   do_temp(opFlattenedArrayCheck->_tmp);\n+\n+      do_stub(opFlattenedArrayCheck->_stub);\n+\n+      break;\n+    }\n+\n+\/\/ LIR_OpNullFreeArrayCheck\n+    case lir_null_free_array_check: {\n+      assert(op->as_OpNullFreeArrayCheck() != nullptr, \"must be\");\n+      LIR_OpNullFreeArrayCheck* opNullFreeArrayCheck = (LIR_OpNullFreeArrayCheck*)op;\n+\n+      if (opNullFreeArrayCheck->_array->is_valid()) do_input(opNullFreeArrayCheck->_array);\n+      if (opNullFreeArrayCheck->_tmp->is_valid())   do_temp(opNullFreeArrayCheck->_tmp);\n+      break;\n+    }\n+\n+\/\/ LIR_OpSubstitutabilityCheck\n+    case lir_substitutability_check: {\n+      assert(op->as_OpSubstitutabilityCheck() != nullptr, \"must be\");\n+      LIR_OpSubstitutabilityCheck* opSubstitutabilityCheck = (LIR_OpSubstitutabilityCheck*)op;\n+                                                                do_input(opSubstitutabilityCheck->_left);\n+                                                                do_temp (opSubstitutabilityCheck->_left);\n+                                                                do_input(opSubstitutabilityCheck->_right);\n+                                                                do_temp (opSubstitutabilityCheck->_right);\n+                                                                do_input(opSubstitutabilityCheck->_equal_result);\n+                                                                do_temp (opSubstitutabilityCheck->_equal_result);\n+                                                                do_input(opSubstitutabilityCheck->_not_equal_result);\n+                                                                do_temp (opSubstitutabilityCheck->_not_equal_result);\n+      if (opSubstitutabilityCheck->_tmp1->is_valid())           do_temp(opSubstitutabilityCheck->_tmp1);\n+      if (opSubstitutabilityCheck->_tmp2->is_valid())           do_temp(opSubstitutabilityCheck->_tmp2);\n+      if (opSubstitutabilityCheck->_left_klass_op->is_valid())  do_temp(opSubstitutabilityCheck->_left_klass_op);\n+      if (opSubstitutabilityCheck->_right_klass_op->is_valid()) do_temp(opSubstitutabilityCheck->_right_klass_op);\n+      if (opSubstitutabilityCheck->_result->is_valid())         do_output(opSubstitutabilityCheck->_result);\n+\n+      do_info(opSubstitutabilityCheck->_info);\n+      do_stub(opSubstitutabilityCheck->_stub);\n+      break;\n+    }\n+\n@@ -896,1 +979,12 @@\n-  default:\n+\n+    \/\/ LIR_OpProfileInlineType:\n+    case lir_profile_inline_type: {\n+      assert(op->as_OpProfileInlineType() != nullptr, \"must be\");\n+      LIR_OpProfileInlineType* opProfileInlineType = (LIR_OpProfileInlineType*)op;\n+\n+      do_input(opProfileInlineType->_mdp); do_temp(opProfileInlineType->_mdp);\n+      do_input(opProfileInlineType->_obj);\n+      do_temp(opProfileInlineType->_tmp);\n+      break;\n+    }\n+default:\n@@ -969,0 +1063,28 @@\n+bool LIR_OpJavaCall::maybe_return_as_fields(ciInlineKlass** vk_ret) const {\n+  ciType* return_type = method()->return_type();\n+  if (InlineTypeReturnedAsFields) {\n+    if (return_type->is_inlinetype()) {\n+      ciInlineKlass* vk = return_type->as_inline_klass();\n+      if (vk->can_be_returned_as_fields()) {\n+        if (vk_ret != nullptr) {\n+          *vk_ret = vk;\n+        }\n+        return true;\n+      }\n+    } else if (return_type->is_instance_klass() &&\n+               (method()->is_method_handle_intrinsic() || !return_type->is_loaded() ||\n+                StressCallingConvention)) {\n+      \/\/ An inline type might be returned from the call but we don't know its type.\n+      \/\/ This can happen with method handle intrinsics or when the return type is\n+      \/\/ not loaded (method holder is not loaded or preload attribute is missing).\n+      \/\/ If an inline type is returned, we either get an oop to a buffer and nothing\n+      \/\/ needs to be done or one of the values being returned is the klass of the\n+      \/\/ inline type (RAX on x64, with LSB set to 1) and we need to allocate an inline\n+      \/\/ type instance of that type and initialize it with the fields values being\n+      \/\/ returned in other registers.\n+      return true;\n+    }\n+  }\n+  return false;\n+}\n+\n@@ -1029,0 +1151,18 @@\n+void LIR_OpFlattenedArrayCheck::emit_code(LIR_Assembler* masm) {\n+  masm->emit_opFlattenedArrayCheck(this);\n+  if (stub() != nullptr) {\n+    masm->append_code_stub(stub());\n+  }\n+}\n+\n+void LIR_OpNullFreeArrayCheck::emit_code(LIR_Assembler* masm) {\n+  masm->emit_opNullFreeArrayCheck(this);\n+}\n+\n+void LIR_OpSubstitutabilityCheck::emit_code(LIR_Assembler* masm) {\n+  masm->emit_opSubstitutabilityCheck(this);\n+  if (stub() != nullptr) {\n+    masm->append_code_stub(stub());\n+  }\n+}\n+\n@@ -1046,0 +1186,3 @@\n+  if (throw_ie_stub()) {\n+    masm->append_code_stub(throw_ie_stub());\n+  }\n@@ -1066,0 +1209,4 @@\n+void LIR_OpProfileInlineType::emit_code(LIR_Assembler* masm) {\n+  masm->emit_profile_inline_type(this);\n+}\n+\n@@ -1341,1 +1488,1 @@\n-void LIR_List::allocate_array(LIR_Opr dst, LIR_Opr len, LIR_Opr t1,LIR_Opr t2, LIR_Opr t3,LIR_Opr t4, BasicType type, LIR_Opr klass, CodeStub* stub, bool zero_array) {\n+void LIR_List::allocate_array(LIR_Opr dst, LIR_Opr len, LIR_Opr t1,LIR_Opr t2, LIR_Opr t3,LIR_Opr t4, BasicType type, LIR_Opr klass, CodeStub* stub, bool zero_array, bool always_slow_path) {\n@@ -1352,1 +1499,2 @@\n-                           zero_array));\n+                           zero_array,\n+                           always_slow_path));\n@@ -1390,1 +1538,1 @@\n-void LIR_List::lock_object(LIR_Opr hdr, LIR_Opr obj, LIR_Opr lock, LIR_Opr scratch, CodeStub* stub, CodeEmitInfo* info) {\n+void LIR_List::lock_object(LIR_Opr hdr, LIR_Opr obj, LIR_Opr lock, LIR_Opr scratch, CodeStub* stub, CodeEmitInfo* info, CodeStub* throw_ie_stub) {\n@@ -1398,1 +1546,2 @@\n-                    info));\n+                    info,\n+                    throw_ie_stub));\n@@ -1423,1 +1572,4 @@\n-                          ciMethod* profiled_method, int profiled_bci) {\n+                          ciMethod* profiled_method, int profiled_bci, bool is_null_free) {\n+  \/\/ If klass is non-nullable,  LIRGenerator::do_CheckCast has already performed null-check\n+  \/\/ on the object.\n+  bool need_null_check = !is_null_free;\n@@ -1425,1 +1577,2 @@\n-                                           tmp1, tmp2, tmp3, fast_check, info_for_exception, info_for_patch, stub);\n+                                           tmp1, tmp2, tmp3, fast_check, info_for_exception, info_for_patch, stub,\n+                                           need_null_check);\n@@ -1447,0 +1600,1 @@\n+  \/\/ FIXME -- if the types of the array and\/or the object are known statically, we can avoid loading the klass\n@@ -1468,0 +1622,21 @@\n+void LIR_List::check_flat_array(LIR_Opr array, LIR_Opr value, LIR_Opr tmp, CodeStub* stub) {\n+  LIR_OpFlattenedArrayCheck* c = new LIR_OpFlattenedArrayCheck(array, value, tmp, stub);\n+  append(c);\n+}\n+\n+void LIR_List::check_null_free_array(LIR_Opr array, LIR_Opr tmp) {\n+  LIR_OpNullFreeArrayCheck* c = new LIR_OpNullFreeArrayCheck(array, tmp);\n+  append(c);\n+}\n+\n+void LIR_List::substitutability_check(LIR_Opr result, LIR_Opr left, LIR_Opr right, LIR_Opr equal_result, LIR_Opr not_equal_result,\n+                                      LIR_Opr tmp1, LIR_Opr tmp2,\n+                                      ciKlass* left_klass, ciKlass* right_klass, LIR_Opr left_klass_op, LIR_Opr right_klass_op,\n+                                      CodeEmitInfo* info, CodeStub* stub) {\n+  LIR_OpSubstitutabilityCheck* c = new LIR_OpSubstitutabilityCheck(result, left, right, equal_result, not_equal_result,\n+                                                                   tmp1, tmp2,\n+                                                                   left_klass, right_klass, left_klass_op, right_klass_op,\n+                                                                   info, stub);\n+  append(c);\n+}\n+\n@@ -1683,0 +1858,1 @@\n+     case lir_check_orig_pc:         s = \"check_orig_pc\"; break;\n@@ -1746,0 +1922,6 @@\n+     \/\/ LIR_OpFlattenedArrayCheck\n+     case lir_flat_array_check:      s = \"flat_array_check\"; break;\n+     \/\/ LIR_OpNullFreeArrayCheck\n+     case lir_null_free_array_check: s = \"null_free_array_check\"; break;\n+     \/\/ LIR_OpSubstitutabilityCheck\n+     case lir_substitutability_check: s = \"substitutability_check\"; break;\n@@ -1754,0 +1936,2 @@\n+     \/\/ LIR_OpProfileInlineType\n+     case lir_profile_inline_type:   s = \"profile_inline_type\"; break;\n@@ -1979,0 +2163,38 @@\n+void LIR_OpFlattenedArrayCheck::print_instr(outputStream* out) const {\n+  array()->print(out);                   out->print(\" \");\n+  value()->print(out);                   out->print(\" \");\n+  tmp()->print(out);                     out->print(\" \");\n+  if (stub() != nullptr) {\n+    out->print(\"[label:\" INTPTR_FORMAT \"]\", p2i(stub()->entry()));\n+  }\n+}\n+\n+void LIR_OpNullFreeArrayCheck::print_instr(outputStream* out) const {\n+  array()->print(out);                   out->print(\" \");\n+  tmp()->print(out);                     out->print(\" \");\n+}\n+\n+void LIR_OpSubstitutabilityCheck::print_instr(outputStream* out) const {\n+  result_opr()->print(out);              out->print(\" \");\n+  left()->print(out);                    out->print(\" \");\n+  right()->print(out);                   out->print(\" \");\n+  equal_result()->print(out);            out->print(\" \");\n+  not_equal_result()->print(out);        out->print(\" \");\n+  tmp1()->print(out);                    out->print(\" \");\n+  tmp2()->print(out);                    out->print(\" \");\n+  if (left_klass() == nullptr) {\n+    out->print(\"unknown \");\n+  } else {\n+    left_klass()->print(out);            out->print(\" \");\n+  }\n+  if (right_klass() == nullptr) {\n+    out->print(\"unknown \");\n+  } else {\n+    right_klass()->print(out);           out->print(\" \");\n+  }\n+  left_klass_op()->print(out);           out->print(\" \");\n+  right_klass_op()->print(out);          out->print(\" \");\n+  if (stub() != nullptr) {\n+    out->print(\"[label:\" INTPTR_FORMAT \"]\", p2i(stub()->entry()));\n+  }\n+}\n@@ -2049,0 +2271,8 @@\n+\/\/ LIR_OpProfileInlineType\n+void LIR_OpProfileInlineType::print_instr(outputStream* out) const {\n+  out->print(\" flag = %x \", flag());\n+  mdp()->print(out);          out->print(\" \");\n+  obj()->print(out);          out->print(\" \");\n+  tmp()->print(out);          out->print(\" \");\n+}\n+\n","filename":"src\/hotspot\/share\/c1\/c1_LIR.cpp","additions":238,"deletions":8,"binary":false,"changes":246,"status":"modified"},{"patch":"@@ -892,0 +892,3 @@\n+class    LIR_OpFlattenedArrayCheck;\n+class    LIR_OpNullFreeArrayCheck;\n+class    LIR_OpSubstitutabilityCheck;\n@@ -896,0 +899,1 @@\n+class    LIR_OpProfileInlineType;\n@@ -919,0 +923,1 @@\n+      , lir_check_orig_pc\n@@ -992,0 +997,9 @@\n+  , begin_opFlattenedArrayCheck\n+    , lir_flat_array_check\n+  , end_opFlattenedArrayCheck\n+  , begin_opNullFreeArrayCheck\n+    , lir_null_free_array_check\n+  , end_opNullFreeArrayCheck\n+  , begin_opSubstitutabilityCheck\n+    , lir_substitutability_check\n+  , end_opSubstitutabilityCheck\n@@ -1000,0 +1014,1 @@\n+    , lir_profile_inline_type\n@@ -1138,0 +1153,3 @@\n+  virtual LIR_OpFlattenedArrayCheck* as_OpFlattenedArrayCheck() { return nullptr; }\n+  virtual LIR_OpNullFreeArrayCheck* as_OpNullFreeArrayCheck() { return nullptr; }\n+  virtual LIR_OpSubstitutabilityCheck* as_OpSubstitutabilityCheck() { return nullptr; }\n@@ -1142,0 +1160,1 @@\n+  virtual LIR_OpProfileInlineType* as_OpProfileInlineType() { return nullptr; }\n@@ -1211,0 +1230,2 @@\n+\n+  bool maybe_return_as_fields(ciInlineKlass** vk = nullptr) const;\n@@ -1262,1 +1283,4 @@\n-    all_flags              = (1 << 12) - 1\n+    always_slow_path       = 1 << 12,\n+    src_inlinetype_check   = 1 << 13,\n+    dst_inlinetype_check   = 1 << 14,\n+    all_flags              = (1 << 15) - 1\n@@ -1517,0 +1541,1 @@\n+  bool          _need_null_check;\n@@ -1521,1 +1546,1 @@\n-                  CodeEmitInfo* info_for_exception, CodeEmitInfo* info_for_patch, CodeStub* stub);\n+                  CodeEmitInfo* info_for_exception, CodeEmitInfo* info_for_patch, CodeStub* stub, bool need_null_check = true);\n@@ -1543,1 +1568,1 @@\n-\n+  bool      need_null_check() const              { return _need_null_check;   }\n@@ -1550,0 +1575,76 @@\n+\/\/ LIR_OpFlattenedArrayCheck\n+class LIR_OpFlattenedArrayCheck: public LIR_Op {\n+ friend class LIR_OpVisitState;\n+\n+ private:\n+  LIR_Opr       _array;\n+  LIR_Opr       _value;\n+  LIR_Opr       _tmp;\n+  CodeStub*     _stub;\n+public:\n+  LIR_OpFlattenedArrayCheck(LIR_Opr array, LIR_Opr value, LIR_Opr tmp, CodeStub* stub);\n+  LIR_Opr array() const                          { return _array;         }\n+  LIR_Opr value() const                          { return _value;         }\n+  LIR_Opr tmp() const                            { return _tmp;           }\n+  CodeStub* stub() const                         { return _stub;          }\n+\n+  virtual void emit_code(LIR_Assembler* masm);\n+  virtual LIR_OpFlattenedArrayCheck* as_OpFlattenedArrayCheck() { return this; }\n+  virtual void print_instr(outputStream* out) const PRODUCT_RETURN;\n+};\n+\n+\/\/ LIR_OpNullFreeArrayCheck\n+class LIR_OpNullFreeArrayCheck: public LIR_Op {\n+ friend class LIR_OpVisitState;\n+\n+ private:\n+  LIR_Opr       _array;\n+  LIR_Opr       _tmp;\n+public:\n+  LIR_OpNullFreeArrayCheck(LIR_Opr array, LIR_Opr tmp);\n+  LIR_Opr array() const                          { return _array;         }\n+  LIR_Opr tmp() const                            { return _tmp;           }\n+\n+  virtual void emit_code(LIR_Assembler* masm);\n+  virtual LIR_OpNullFreeArrayCheck* as_OpNullFreeArrayCheck() { return this; }\n+  virtual void print_instr(outputStream* out) const PRODUCT_RETURN;\n+};\n+\n+class LIR_OpSubstitutabilityCheck: public LIR_Op {\n+ friend class LIR_OpVisitState;\n+\n+ private:\n+  LIR_Opr       _left;\n+  LIR_Opr       _right;\n+  LIR_Opr       _equal_result;\n+  LIR_Opr       _not_equal_result;\n+  LIR_Opr       _tmp1;\n+  LIR_Opr       _tmp2;\n+  ciKlass*      _left_klass;\n+  ciKlass*      _right_klass;\n+  LIR_Opr       _left_klass_op;\n+  LIR_Opr       _right_klass_op;\n+  CodeStub*     _stub;\n+public:\n+  LIR_OpSubstitutabilityCheck(LIR_Opr result, LIR_Opr left, LIR_Opr right, LIR_Opr equal_result, LIR_Opr not_equal_result,\n+                              LIR_Opr tmp1, LIR_Opr tmp2,\n+                              ciKlass* left_klass, ciKlass* right_klass, LIR_Opr left_klass_op, LIR_Opr right_klass_op,\n+                              CodeEmitInfo* info, CodeStub* stub);\n+\n+  LIR_Opr left() const             { return _left; }\n+  LIR_Opr right() const            { return _right; }\n+  LIR_Opr equal_result() const     { return _equal_result; }\n+  LIR_Opr not_equal_result() const { return _not_equal_result; }\n+  LIR_Opr tmp1() const             { return _tmp1; }\n+  LIR_Opr tmp2() const             { return _tmp2; }\n+  ciKlass* left_klass() const      { return _left_klass; }\n+  ciKlass* right_klass() const     { return _right_klass; }\n+  LIR_Opr left_klass_op() const    { return _left_klass_op; }\n+  LIR_Opr right_klass_op() const   { return _right_klass_op; }\n+  CodeStub* stub() const           { return _stub; }\n+\n+  virtual void emit_code(LIR_Assembler* masm);\n+  virtual LIR_OpSubstitutabilityCheck* as_OpSubstitutabilityCheck() { return this; }\n+  virtual void print_instr(outputStream* out) const PRODUCT_RETURN;\n+};\n+\n@@ -1708,0 +1809,1 @@\n+  bool      _always_slow_path;\n@@ -1710,1 +1812,1 @@\n-  LIR_OpAllocArray(LIR_Opr klass, LIR_Opr len, LIR_Opr result, LIR_Opr t1, LIR_Opr t2, LIR_Opr t3, LIR_Opr t4, BasicType type, CodeStub* stub, bool zero_array)\n+  LIR_OpAllocArray(LIR_Opr klass, LIR_Opr len, LIR_Opr result, LIR_Opr t1, LIR_Opr t2, LIR_Opr t3, LIR_Opr t4, BasicType type, CodeStub* stub, bool zero_array, bool always_slow_path)\n@@ -1720,1 +1822,2 @@\n-    , _zero_array(zero_array) {}\n+    , _zero_array(zero_array)\n+    , _always_slow_path(always_slow_path) {}\n@@ -1731,1 +1834,2 @@\n-  bool zero_array()   const                      { return _zero_array;  }\n+  bool      zero_array() const                   { return _zero_array;  }\n+  bool      always_slow_path() const             { return _always_slow_path; }\n@@ -1838,0 +1942,1 @@\n+  CodeStub* _throw_ie_stub;\n@@ -1839,1 +1944,1 @@\n-  LIR_OpLock(LIR_Code code, LIR_Opr hdr, LIR_Opr obj, LIR_Opr lock, LIR_Opr scratch, CodeStub* stub, CodeEmitInfo* info)\n+  LIR_OpLock(LIR_Code code, LIR_Opr hdr, LIR_Opr obj, LIR_Opr lock, LIR_Opr scratch, CodeStub* stub, CodeEmitInfo* info, CodeStub* throw_ie_stub=nullptr)\n@@ -1845,1 +1950,2 @@\n-    , _stub(stub)                      {}\n+    , _stub(stub)\n+    , _throw_ie_stub(throw_ie_stub)                    {}\n@@ -1852,0 +1958,1 @@\n+  CodeStub* throw_ie_stub() const                { return _throw_ie_stub; }\n@@ -2017,0 +2124,32 @@\n+\/\/ LIR_OpProfileInlineType\n+class LIR_OpProfileInlineType : public LIR_Op {\n+ friend class LIR_OpVisitState;\n+\n+ private:\n+  LIR_Opr      _mdp;\n+  LIR_Opr      _obj;\n+  int          _flag;\n+  LIR_Opr      _tmp;\n+  bool         _not_null;      \/\/ true if we know statically that _obj cannot be null\n+\n+ public:\n+  \/\/ Destroys recv\n+  LIR_OpProfileInlineType(LIR_Opr mdp, LIR_Opr obj, int flag, LIR_Opr tmp, bool not_null)\n+    : LIR_Op(lir_profile_inline_type, LIR_OprFact::illegalOpr, nullptr)  \/\/ no result, no info\n+    , _mdp(mdp)\n+    , _obj(obj)\n+    , _flag(flag)\n+    , _tmp(tmp)\n+    , _not_null(not_null) { }\n+\n+  LIR_Opr      mdp()              const             { return _mdp;              }\n+  LIR_Opr      obj()              const             { return _obj;              }\n+  int          flag()             const             { return _flag;             }\n+  LIR_Opr      tmp()              const             { return _tmp;              }\n+  bool         not_null()         const             { return _not_null;         }\n+\n+  virtual void emit_code(LIR_Assembler* masm);\n+  virtual LIR_OpProfileInlineType* as_OpProfileInlineType() { return this; }\n+  virtual void print_instr(outputStream* out) const PRODUCT_RETURN;\n+};\n+\n@@ -2241,1 +2380,1 @@\n-  void allocate_array(LIR_Opr dst, LIR_Opr len, LIR_Opr t1,LIR_Opr t2, LIR_Opr t3,LIR_Opr t4, BasicType type, LIR_Opr klass, CodeStub* stub, bool zero_array = true);\n+  void allocate_array(LIR_Opr dst, LIR_Opr len, LIR_Opr t1,LIR_Opr t2, LIR_Opr t3,LIR_Opr t4, BasicType type, LIR_Opr klass, CodeStub* stub, bool zero_array = true, bool always_slow_path = false);\n@@ -2288,1 +2427,1 @@\n-  void lock_object(LIR_Opr hdr, LIR_Opr obj, LIR_Opr lock, LIR_Opr scratch, CodeStub* stub, CodeEmitInfo* info);\n+  void lock_object(LIR_Opr hdr, LIR_Opr obj, LIR_Opr lock, LIR_Opr scratch, CodeStub* stub, CodeEmitInfo* info, CodeStub* throw_ie_stub=nullptr);\n@@ -2298,0 +2437,6 @@\n+  void check_flat_array(LIR_Opr array, LIR_Opr value, LIR_Opr tmp, CodeStub* stub);\n+  void check_null_free_array(LIR_Opr array, LIR_Opr tmp);\n+  void substitutability_check(LIR_Opr result, LIR_Opr left, LIR_Opr right, LIR_Opr equal_result, LIR_Opr not_equal_result,\n+                              LIR_Opr tmp1, LIR_Opr tmp2,\n+                              ciKlass* left_klass, ciKlass* right_klass, LIR_Opr left_klass_op, LIR_Opr right_klass_op,\n+                              CodeEmitInfo* info, CodeStub* stub);\n@@ -2302,1 +2447,1 @@\n-                  ciMethod* profiled_method, int profiled_bci);\n+                  ciMethod* profiled_method, int profiled_bci, bool is_null_free);\n@@ -2310,0 +2455,3 @@\n+  void profile_inline_type(LIR_Address* mdp, LIR_Opr obj, int flag, LIR_Opr tmp, bool not_null) {\n+    append(new LIR_OpProfileInlineType(LIR_OprFact::address(mdp), obj, flag, tmp, not_null));\n+  }\n","filename":"src\/hotspot\/share\/c1\/c1_LIR.hpp","additions":159,"deletions":11,"binary":false,"changes":170,"status":"modified"},{"patch":"@@ -258,1 +258,1 @@\n-  memset(mem, 0, objArrayOopDesc::object_size(element_count));\n+  memset(mem, 0, refArrayOopDesc::object_size(element_count));\n@@ -264,0 +264,1 @@\n+    assert(!EnableValhalla || Universe::objectArrayKlass()->prototype_header() == markWord::prototype(), \"should be the same\");\n@@ -294,1 +295,1 @@\n-  assert(objArrayOopDesc::object_size(max_elem_count)*HeapWordSize == MIN_GC_REGION_ALIGNMENT,\n+  assert(refArrayOopDesc::object_size(max_elem_count)*HeapWordSize == MIN_GC_REGION_ALIGNMENT,\n@@ -409,1 +410,1 @@\n-  size_t byte_size = objArrayOopDesc::object_size(length) * HeapWordSize;\n+  size_t byte_size = refArrayOopDesc::object_size(length) * HeapWordSize;\n@@ -654,1 +655,1 @@\n-  if (!src_obj->fast_no_hash_check()) {\n+  if (!src_obj->fast_no_hash_check() && (!(EnableValhalla && src_obj->mark().is_inline_type()))) {\n@@ -658,0 +659,2 @@\n+    } else if (EnableValhalla) {\n+      fake_oop->set_mark(src_klass->prototype_header().copy_set_hash(src_hash));\n","filename":"src\/hotspot\/share\/cds\/aotMappedHeapWriter.cpp","additions":7,"deletions":4,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -80,0 +80,2 @@\n+#include \"oops\/flatArrayKlass.hpp\"\n+#include \"oops\/inlineKlass.hpp\"\n@@ -485,1 +487,1 @@\n-  soc->do_tag(objArrayOopDesc::base_offset_in_bytes());\n+  soc->do_tag(refArrayOopDesc::base_offset_in_bytes());\n@@ -1354,0 +1356,5 @@\n+  if (CDSConfig::is_valhalla_preview()) {\n+    log_info(cds)(\"Archived java heap is not yet supported with Valhalla preview\");\n+    return;\n+  }\n+\n","filename":"src\/hotspot\/share\/cds\/aotMetaspace.cpp","additions":8,"deletions":1,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -853,1 +853,1 @@\n-      k->set_prototype_header(markWord::prototype().set_narrow_klass(nk));\n+      k->set_prototype_header_klass(nk);\n@@ -856,1 +856,7 @@\n-    if (k->is_objArray_klass()) {\n+    if (k->is_flatArray_klass()) {\n+      num_obj_array_klasses ++;\n+      type = \"flat array\";\n+    } else if (k->is_refArray_klass()) {\n+        num_obj_array_klasses ++;\n+        type = \"ref array\";\n+    } else if (k->is_objArray_klass()) {\n@@ -860,1 +866,1 @@\n-      type = \"array\";\n+      type = \"obj array\";\n","filename":"src\/hotspot\/share\/cds\/archiveBuilder.cpp","additions":9,"deletions":3,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -393,1 +393,1 @@\n-  return objArrayOopDesc::object_size(size_in_elems(seg_idx)) * HeapWordSize;\n+  return refArrayOopDesc::object_size(size_in_elems(seg_idx)) * HeapWordSize;\n","filename":"src\/hotspot\/share\/cds\/archiveUtils.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -39,0 +39,1 @@\n+#include \"runtime\/globals.hpp\"\n@@ -60,0 +61,3 @@\n+bool CDSConfig::_module_patching_disables_cds = false;\n+bool CDSConfig::_java_base_module_patching_disables_cds = false;\n+\n@@ -146,0 +150,3 @@\n+    if (is_valhalla_preview()) {\n+      tmp.print_raw(\"_valhalla\");\n+    }\n@@ -301,1 +308,1 @@\n-  if (Arguments::is_incompatible_cds_internal_module_property(key)) {\n+  if (Arguments::is_incompatible_cds_internal_module_property(key) && !Arguments::patching_migrated_classes(key, value)) {\n@@ -334,2 +341,1 @@\n-    \"jdk.module.upgrade.path\",\n-    \"jdk.module.patch.0\"\n+    \"jdk.module.upgrade.path\"\n@@ -339,2 +345,1 @@\n-    \"--upgrade-module-path\",\n-    \"--patch-module\"\n+    \"--upgrade-module-path\"\n@@ -363,0 +368,6 @@\n+\n+  if (module_patching_disables_cds()) {\n+    vm_exit_during_initialization(\n+            \"Cannot use the following option when dumping the shared archive\", \"--patch-module\");\n+  }\n+\n@@ -391,0 +402,10 @@\n+\n+  if (module_patching_disables_cds()) {\n+    if (RequireSharedSpaces) {\n+      warning(\"CDS is disabled when the %s option is specified.\", \"--patch-module\");\n+    } else {\n+      log_info(cds)(\"CDS is disabled when the %s option is specified.\", \"--patch-module\");\n+    }\n+    return true;\n+  }\n+\n@@ -625,1 +646,1 @@\n-bool CDSConfig::check_vm_args_consistency(bool patch_mod_javabase, bool mode_flag_cmd_line) {\n+bool CDSConfig::check_vm_args_consistency(bool mode_flag_cmd_line) {\n@@ -700,1 +721,1 @@\n-  if (is_using_archive() && patch_mod_javabase) {\n+  if (is_using_archive() && java_base_module_patching_disables_cds() && module_patching_disables_cds()) {\n@@ -957,0 +978,4 @@\n+  if (is_valhalla_preview()) {\n+    \/\/ Not working yet -- e.g., HeapShared::oop_hash() needs to be implemented for value oops\n+    return false;\n+  }\n","filename":"src\/hotspot\/share\/cds\/cdsConfig.cpp","additions":32,"deletions":7,"binary":false,"changes":39,"status":"modified"},{"patch":"@@ -94,0 +94,3 @@\n+  \/\/ There should be no oops for ObjArrayKlass but InstanceKlass::array_klasses holds a list of ObjArrayKlass,\n+  \/\/ therefore we need the super of the refined array klass.\n+  Klass* oop_field_klass = oop_field->is_refined_objArray() ? oop_field->klass()->super() : oop_field->klass();\n@@ -97,1 +100,1 @@\n-  } else if (oop_field->klass() != ik && oop_field->klass() != ik->array_klass_or_null()) {\n+  } else if (oop_field_klass != ik && oop_field_klass != ik->array_klass_or_null()) {\n","filename":"src\/hotspot\/share\/cds\/cdsEnumKlass.cpp","additions":4,"deletions":1,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -373,0 +373,3 @@\n+      if (oak->is_refined_objArray_klass()) {\n+        oak = ObjArrayKlass::cast(oak->super());\n+      }\n@@ -442,8 +445,10 @@\n-\n-      if (elm->is_instance_klass()) {\n-        assert(InstanceKlass::cast(elm)->array_klasses() == nullptr, \"must be\");\n-        InstanceKlass::cast(elm)->set_array_klasses(oak);\n-      } else {\n-        assert(elm->is_array_klass(), \"sanity\");\n-        assert(ArrayKlass::cast(elm)->higher_dimension() == nullptr, \"must be\");\n-        ArrayKlass::cast(elm)->set_higher_dimension(oak);\n+      \/\/ Higher dimension may have been set when doing setup on ObjArrayKlass\n+      if (!oak->is_refined_objArray_klass()) {\n+        if (elm->is_instance_klass()) {\n+          assert(InstanceKlass::cast(elm)->array_klasses() == nullptr, \"must be\");\n+          InstanceKlass::cast(elm)->set_array_klasses(oak);\n+        } else {\n+          assert(elm->is_array_klass(), \"sanity\");\n+          assert(ArrayKlass::cast(elm)->higher_dimension() == nullptr, \"must be\");\n+          ArrayKlass::cast(elm)->set_higher_dimension(oak);\n+        }\n","filename":"src\/hotspot\/share\/cds\/dynamicArchive.cpp","additions":13,"deletions":8,"binary":false,"changes":21,"status":"modified"},{"patch":"@@ -90,0 +90,65 @@\n+inline void CDSMustMatchFlags::do_print(outputStream* st, bool v) {\n+  st->print(\"%s\", v ? \"true\" : \"false\");\n+}\n+\n+inline void CDSMustMatchFlags::do_print(outputStream* st, intx v) {\n+  st->print(\"%zd\", v);\n+}\n+\n+inline void CDSMustMatchFlags::do_print(outputStream* st, uintx v) {\n+  st->print(\"%zu\", v);\n+}\n+\n+inline void CDSMustMatchFlags::do_print(outputStream* st, double v) {\n+  st->print(\"%f\", v);\n+}\n+\n+void CDSMustMatchFlags::init() {\n+  assert(CDSConfig::is_dumping_archive(), \"sanity\");\n+  _max_name_width = 0;\n+\n+#define INIT_CDS_MUST_MATCH_FLAG(n) \\\n+  _v_##n = n; \\\n+  _max_name_width = MAX2(_max_name_width,strlen(#n));\n+  CDS_MUST_MATCH_FLAGS_DO(INIT_CDS_MUST_MATCH_FLAG);\n+#undef INIT_CDS_MUST_MATCH_FLAG\n+}\n+\n+bool CDSMustMatchFlags::runtime_check() const {\n+#define CHECK_CDS_MUST_MATCH_FLAG(n) \\\n+  if (_v_##n != n) { \\\n+    ResourceMark rm; \\\n+    stringStream ss; \\\n+    ss.print(\"VM option %s is different between dumptime (\", #n);  \\\n+    do_print(&ss, _v_ ## n); \\\n+    ss.print(\") and runtime (\"); \\\n+    do_print(&ss, n); \\\n+    ss.print(\")\"); \\\n+    log_info(cds)(\"%s\", ss.as_string()); \\\n+    return false; \\\n+  }\n+  CDS_MUST_MATCH_FLAGS_DO(CHECK_CDS_MUST_MATCH_FLAG);\n+#undef CHECK_CDS_MUST_MATCH_FLAG\n+\n+  return true;\n+}\n+\n+void CDSMustMatchFlags::print_info() const {\n+  LogTarget(Info, cds) lt;\n+  if (lt.is_enabled()) {\n+    LogStream ls(lt);\n+    ls.print_cr(\"Recorded VM flags during dumptime:\");\n+    print(&ls);\n+  }\n+}\n+\n+void CDSMustMatchFlags::print(outputStream* st) const {\n+#define PRINT_CDS_MUST_MATCH_FLAG(n) \\\n+  st->print(\"- %-s \", #n);                   \\\n+  st->sp(int(_max_name_width - strlen(#n))); \\\n+  do_print(st, _v_##n);                      \\\n+  st->cr();\n+  CDS_MUST_MATCH_FLAGS_DO(PRINT_CDS_MUST_MATCH_FLAG);\n+#undef PRINT_CDS_MUST_MATCH_FLAG\n+}\n+\n@@ -249,0 +314,1 @@\n+  _has_valhalla_patched_classes = CDSConfig::is_valhalla_preview();\n@@ -262,0 +328,1 @@\n+  _must_match.init();\n@@ -331,0 +398,2 @@\n+  st->print_cr(\"- has_valhalla_patched_classes              %d\", _has_valhalla_patched_classes);\n+  _must_match.print(st);\n@@ -714,0 +783,4 @@\n+  if (!header()->check_must_match_flags()) {\n+    return false;\n+  }\n+\n@@ -1917,0 +1990,18 @@\n+  if (is_static()) {\n+    const char* err = nullptr;\n+    if (CDSConfig::is_valhalla_preview()) {\n+      if (!_has_valhalla_patched_classes) {\n+        err = \"not created\";\n+      }\n+    } else {\n+      if (_has_valhalla_patched_classes) {\n+        err = \"created\";\n+      }\n+    }\n+    if (err != nullptr) {\n+      log_warning(cds)(\"This archive was %s with --enable-preview -XX:+EnableValhalla. It is \"\n+                         \"incompatible with the current JVM setting\", err);\n+      return false;\n+    }\n+  }\n+\n","filename":"src\/hotspot\/share\/cds\/filemap.cpp","additions":91,"deletions":0,"binary":false,"changes":91,"status":"modified"},{"patch":"@@ -36,0 +36,1 @@\n+#include \"runtime\/globals.hpp\"\n@@ -102,0 +103,31 @@\n+#define CDS_MUST_MATCH_FLAGS_DO(f) \\\n+  f(EnableValhalla) \\\n+  f(UseArrayFlattening) \\\n+  f(UseFieldFlattening) \\\n+  f(InlineTypePassFieldsAsArgs) \\\n+  f(InlineTypeReturnedAsFields) \\\n+  f(UseNonAtomicValueFlattening) \\\n+  f(UseAtomicValueFlattening) \\\n+  f(UseNullableValueFlattening)\n+\n+\n+class CDSMustMatchFlags {\n+private:\n+  size_t _max_name_width;\n+#define DECLARE_CDS_MUST_MATCH_FLAG(n) \\\n+  decltype(n) _v_##n;\n+  CDS_MUST_MATCH_FLAGS_DO(DECLARE_CDS_MUST_MATCH_FLAG);\n+#undef DECLARE_CDS_MUST_MATCH_FLAG\n+\n+  inline static void do_print(outputStream* st, bool v);\n+  inline static void do_print(outputStream* st, intx v);\n+  inline static void do_print(outputStream* st, uintx v);\n+  inline static void do_print(outputStream* st, double v);\n+  void print_info() const;\n+\n+public:\n+  void init();\n+  bool runtime_check() const;\n+  void print(outputStream* st) const;\n+};\n+\n@@ -144,0 +176,2 @@\n+  bool   _has_valhalla_patched_classes; \/\/ Is this archived dumped with --enable-preview -XX:+EnableValhalla?\n+  CDSMustMatchFlags _must_match;        \/\/ These flags must be the same between dumptime and runtime\n@@ -255,0 +289,4 @@\n+  bool check_must_match_flags() const {\n+    return _must_match.runtime_check();\n+  }\n+\n","filename":"src\/hotspot\/share\/cds\/filemap.hpp","additions":38,"deletions":0,"binary":false,"changes":38,"status":"modified"},{"patch":"@@ -805,0 +805,1 @@\n+      \/\/ For valhalla, the prototype header is the same as markWord::prototype();\n@@ -1506,1 +1507,5 @@\n-      assert(resolved_k == k, \"classes used by archived heap must not be replaced by JVMTI ClassFileLoadHook\");\n+      if (resolved_k->is_array_klass()) {\n+        assert(resolved_k == k || resolved_k == k->super(), \"classes used by archived heap must not be replaced by JVMTI ClassFileLoadHook\");\n+      } else {\n+        assert(resolved_k == k, \"classes used by archived heap must not be replaced by JVMTI ClassFileLoadHook\");\n+      }\n@@ -2258,0 +2263,7 @@\n+\n+    if (CDSConfig::is_valhalla_preview() && strcmp(klass_name, \"jdk\/internal\/module\/ArchivedModuleGraph\") == 0) {\n+      \/\/ FIXME -- ArchivedModuleGraph doesn't work when java.base is patched with valhalla classes.\n+      i++;\n+      continue;\n+    }\n+\n","filename":"src\/hotspot\/share\/cds\/heapShared.cpp","additions":13,"deletions":1,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -28,0 +28,1 @@\n+#include \"ci\/ciInlineKlass.hpp\"\n@@ -479,1 +480,2 @@\n-      (sym->char_at(1) == JVM_SIGNATURE_ARRAY || sym->char_at(1) == JVM_SIGNATURE_CLASS)) {\n+      (sym->char_at(1) == JVM_SIGNATURE_ARRAY ||\n+       sym->char_at(1) == JVM_SIGNATURE_CLASS )) {\n@@ -492,1 +494,1 @@\n-      return ciObjArrayKlass::make_impl(elem_klass);\n+      return ciArrayKlass::make(elem_klass);\n@@ -518,0 +520,4 @@\n+  int i = 0;\n+  while (sym->char_at(i) == JVM_SIGNATURE_ARRAY) {\n+    i++;\n+  }\n","filename":"src\/hotspot\/share\/ci\/ciEnv.cpp","additions":8,"deletions":2,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -26,0 +26,1 @@\n+#include \"ci\/ciInlineKlass.hpp\"\n@@ -30,0 +31,1 @@\n+#include \"classfile\/systemDictionary.hpp\"\n@@ -35,0 +37,1 @@\n+#include \"oops\/inlineKlass.inline.hpp\"\n@@ -68,1 +71,2 @@\n-  _nonstatic_fields = nullptr; \/\/ initialized lazily by compute_nonstatic_fields:\n+  _declared_nonstatic_fields = nullptr; \/\/ initialized lazily by compute_nonstatic_fields\n+  _nonstatic_fields = nullptr;          \/\/ initialized lazily by compute_nonstatic_fields\n@@ -70,1 +74,1 @@\n-  _implementor = nullptr; \/\/ we will fill these lazily\n+  _implementor = nullptr;               \/\/ we will fill these lazily\n@@ -117,2 +121,3 @@\n-                                 jobject loader)\n-  : ciKlass(name, T_OBJECT)\n+                                 jobject loader,\n+                                 BasicType bt)\n+  : ciKlass(name, bt)\n@@ -123,1 +128,2 @@\n-  _nonstatic_fields = nullptr;\n+  _declared_nonstatic_fields = nullptr; \/\/ initialized lazily by compute_nonstatic_fields\n+  _nonstatic_fields = nullptr;          \/\/ initialized lazily by compute_nonstatic_fields\n@@ -321,1 +327,1 @@\n-    _flags.print_klass_flags();\n+    _flags.print_klass_flags(st);\n@@ -325,1 +331,1 @@\n-      _super->print_name();\n+      _super->print_name_on(st);\n@@ -396,1 +402,1 @@\n-    ciField* field = _nonstatic_fields->at(i);\n+    ciField* field = nonstatic_field_at(i);\n@@ -398,1 +404,1 @@\n-    if (field_off == field_offset)\n+    if (field_off == field_offset) {\n@@ -400,0 +406,1 @@\n+    }\n@@ -410,0 +417,1 @@\n+\n@@ -420,0 +428,34 @@\n+ciField* ciInstanceKlass::get_non_flat_field_by_offset(int field_offset) {\n+  for (int i = 0, len = nof_declared_nonstatic_fields(); i < len; i++) {\n+    ciField* field = declared_nonstatic_field_at(i);\n+    int field_off = field->offset_in_bytes();\n+    if (field_off == field_offset) {\n+      return field;\n+    }\n+  }\n+  return nullptr;\n+}\n+\n+int ciInstanceKlass::field_index_by_offset(int offset) {\n+  assert(contains_field_offset(offset), \"invalid field offset\");\n+  int best_offset = 0;\n+  int best_index = -1;\n+  \/\/ Search the field with the given offset\n+  for (int i = 0; i < nof_declared_nonstatic_fields(); ++i) {\n+    int field_offset = declared_nonstatic_field_at(i)->offset_in_bytes();\n+    if (field_offset == offset) {\n+      \/\/ Exact match\n+      return i;\n+    } else if (field_offset < offset && field_offset > best_offset) {\n+      \/\/ No exact match. Save the index of the field with the closest offset that\n+      \/\/ is smaller than the given field offset. This index corresponds to the\n+      \/\/ flat field that holds the field we are looking for.\n+      best_offset = field_offset;\n+      best_index = i;\n+    }\n+  }\n+  assert(best_index >= 0, \"field not found\");\n+  assert(best_offset == offset || declared_nonstatic_field_at(best_index)->type()->is_inlinetype(), \"offset should match for non-inline types\");\n+  return best_index;\n+}\n+\n@@ -434,0 +476,2 @@\n+const GrowableArray<ciField*> empty_field_array(0, MemTag::mtCompiler);\n+\n@@ -459,3 +503,1 @@\n-\/\/ ------------------------------------------------------------------\n-\/\/ ciInstanceKlass::compute_nonstatic_fields\n-int ciInstanceKlass::compute_nonstatic_fields() {\n+void ciInstanceKlass::compute_nonstatic_fields() {\n@@ -464,2 +506,4 @@\n-  if (_nonstatic_fields != nullptr)\n-    return _nonstatic_fields->length();\n+  if (_nonstatic_fields != nullptr) {\n+    assert(_declared_nonstatic_fields != nullptr, \"must be initialized at the same time, class %s\", name()->as_utf8());\n+    return;\n+  }\n@@ -468,3 +512,3 @@\n-    Arena* arena = CURRENT_ENV->arena();\n-    _nonstatic_fields = new (arena) GrowableArray<ciField*>(arena, 0, 0, nullptr);\n-    return 0;\n+    _declared_nonstatic_fields = &empty_field_array;\n+    _nonstatic_fields = &empty_field_array;\n+    return;\n@@ -475,6 +519,5 @@\n-  GrowableArray<ciField*>* super_fields = nullptr;\n-  if (super != nullptr && super->has_nonstatic_fields()) {\n-    int super_flen   = super->nof_nonstatic_fields();\n-    super_fields = super->_nonstatic_fields;\n-    assert(super_flen == 0 || super_fields != nullptr, \"first get nof_fields\");\n-  }\n+  assert(super != nullptr, \"must have a super class, current class: %s\", name()->as_utf8());\n+  super->compute_nonstatic_fields();\n+  const GrowableArray<ciField*>* super_declared_fields = super->_declared_nonstatic_fields;;\n+  const GrowableArray<ciField*>* super_fields = super->_nonstatic_fields;\n+  assert(super_declared_fields != nullptr && super_fields != nullptr, \"must have been initialized, current class: %s, super class: %s\", name()->as_utf8(), super->name()->as_utf8());\n@@ -482,18 +525,2 @@\n-  GrowableArray<ciField*>* fields = nullptr;\n-      fields = compute_nonstatic_fields_impl(super_fields);\n-    });\n-\n-  if (fields == nullptr) {\n-    \/\/ This can happen if this class (java.lang.Class) has invisible fields.\n-    if (super_fields != nullptr) {\n-      _nonstatic_fields = super_fields;\n-      return super_fields->length();\n-    } else {\n-      return 0;\n-    }\n-  }\n-\n-  int flen = fields->length();\n-\n-  _nonstatic_fields = fields;\n-  return flen;\n+    compute_nonstatic_fields_impl(super_declared_fields, super_fields);\n+  });\n@@ -503,3 +530,2 @@\n-GrowableArray<ciField*>*\n-ciInstanceKlass::compute_nonstatic_fields_impl(GrowableArray<ciField*>*\n-                                               super_fields) {\n+void ciInstanceKlass::compute_nonstatic_fields_impl(const GrowableArray<ciField*>* super_declared_fields, const GrowableArray<ciField*>* super_fields) {\n+  assert(_declared_nonstatic_fields == nullptr && _nonstatic_fields == nullptr, \"initialized already\");\n@@ -508,10 +534,19 @@\n-  int flen = 0;\n-  GrowableArray<ciField*>* fields = nullptr;\n-  InstanceKlass* k = get_instanceKlass();\n-  for (JavaFieldStream fs(k); !fs.done(); fs.next()) {\n-    if (fs.access_flags().is_static())  continue;\n-    flen += 1;\n-  }\n-  \/\/ allocate the array:\n-  if (flen == 0) {\n-    return nullptr;  \/\/ return nothing if none are locally declared\n+  InstanceKlass* this_klass = get_instanceKlass();\n+  int declared_field_num = 0;\n+  int field_num = 0;\n+  for (JavaFieldStream fs(this_klass); !fs.done(); fs.next()) {\n+    if (fs.access_flags().is_static()) {\n+      continue;\n+    }\n+\n+    declared_field_num++;\n+\n+    fieldDescriptor& fd = fs.field_descriptor();\n+    if (fd.is_flat()) {\n+      InlineKlass* k = this_klass->get_inline_type_field_klass(fd.index());\n+      ciInlineKlass* vk = CURRENT_ENV->get_klass(k)->as_inline_klass();\n+      field_num += vk->nof_nonstatic_fields();\n+      field_num += fd.has_null_marker() ? 1 : 0;\n+    } else {\n+      field_num++;\n+    }\n@@ -520,2 +555,5 @@\n-  if (super_fields != nullptr) {\n-    flen += super_fields->length();\n+\n+  GrowableArray<ciField*>* tmp_declared_fields = nullptr;\n+  if (declared_field_num != 0) {\n+    tmp_declared_fields = new (arena) GrowableArray<ciField*>(arena, declared_field_num + super_declared_fields->length(), 0, nullptr);\n+    tmp_declared_fields->appendAll(super_declared_fields);\n@@ -523,3 +561,5 @@\n-  fields = new (arena) GrowableArray<ciField*>(arena, flen, 0, nullptr);\n-  if (super_fields != nullptr) {\n-    fields->appendAll(super_fields);\n+\n+  GrowableArray<ciField*>* tmp_fields = nullptr;\n+  if (field_num != 0) {\n+    tmp_fields = new (arena) GrowableArray<ciField*>(arena, field_num + super_fields->length(), 0, nullptr);\n+    tmp_fields->appendAll(super_fields);\n@@ -528,2 +568,9 @@\n-  for (JavaFieldStream fs(k); !fs.done(); fs.next()) {\n-    if (fs.access_flags().is_static())  continue;\n+  \/\/ For later assertion\n+  declared_field_num += super_declared_fields->length();\n+  field_num += super_fields->length();\n+\n+  for (JavaFieldStream fs(this_klass); !fs.done(); fs.next()) {\n+    if (fs.access_flags().is_static()) {\n+      continue;\n+    }\n+\n@@ -531,2 +578,38 @@\n-    ciField* field = new (arena) ciField(&fd);\n-    fields->append(field);\n+    ciField* declared_field = new (arena) ciField(&fd);\n+    assert(tmp_declared_fields != nullptr, \"should be initialized\");\n+    tmp_declared_fields->append(declared_field);\n+\n+    if (fd.is_flat()) {\n+      \/\/ Flat fields are embedded\n+      Klass* k = get_instanceKlass()->get_inline_type_field_klass(fd.index());\n+      ciInlineKlass* vk = CURRENT_ENV->get_klass(k)->as_inline_klass();\n+      \/\/ Iterate over fields of the flat inline type and copy them to 'this'\n+      for (int i = 0; i < vk->nof_nonstatic_fields(); ++i) {\n+        assert(tmp_fields != nullptr, \"should be initialized\");\n+        tmp_fields->append(new (arena) ciField(declared_field, vk->nonstatic_field_at(i)));\n+      }\n+      if (fd.has_null_marker()) {\n+        assert(tmp_fields != nullptr, \"should be initialized\");\n+        tmp_fields->append(new (arena) ciField(declared_field));\n+      }\n+    } else {\n+      assert(tmp_fields != nullptr, \"should be initialized\");\n+      tmp_fields->append(declared_field);\n+    }\n+  }\n+\n+  \/\/ Now sort them by offset, ascending. In principle, they could mix with superclass fields.\n+  if (tmp_declared_fields != nullptr) {\n+    assert(tmp_declared_fields->length() == declared_field_num, \"sanity check failed for class: %s, number of declared fields: %d, expected: %d\",\n+           name()->as_utf8(), tmp_declared_fields->length(), declared_field_num);\n+    _declared_nonstatic_fields = tmp_declared_fields;\n+  } else {\n+    _declared_nonstatic_fields = super_declared_fields;\n+  }\n+\n+  if (tmp_fields != nullptr) {\n+    assert(tmp_fields->length() == field_num, \"sanity check failed for class: %s, number of fields: %d, expected: %d\",\n+           name()->as_utf8(), tmp_fields->length(), field_num);\n+    _nonstatic_fields = tmp_fields;\n+  } else {\n+    _nonstatic_fields = super_fields;\n@@ -534,2 +617,0 @@\n-  assert(fields->length() == flen, \"sanity\");\n-  return fields;\n@@ -646,0 +727,16 @@\n+bool ciInstanceKlass::can_be_inline_klass(bool is_exact) {\n+  if (!EnableValhalla) {\n+    return false;\n+  }\n+  if (!is_loaded() || is_inlinetype()) {\n+    \/\/ Not loaded or known to be an inline klass\n+    return true;\n+  }\n+  if (!is_exact) {\n+    \/\/ Not exact, check if this is a valid super for an inline klass\n+    VM_ENTRY_MARK;\n+    return !get_instanceKlass()->access_flags().is_identity_class() || is_java_lang_Object() ;\n+  }\n+  return false;\n+}\n+\n@@ -654,1 +751,2 @@\n-class StaticFinalFieldPrinter : public FieldClosure {\n+class StaticFieldPrinter : public FieldClosure {\n+protected:\n@@ -656,0 +754,8 @@\n+public:\n+  StaticFieldPrinter(outputStream* out) :\n+    _out(out) {\n+  }\n+  void do_field_helper(fieldDescriptor* fd, oop obj, bool is_flat);\n+};\n+\n+class StaticFinalFieldPrinter : public StaticFieldPrinter {\n@@ -659,2 +765,1 @@\n-    _out(out),\n-    _holder(holder) {\n+    StaticFieldPrinter(out), _holder(holder) {\n@@ -665,46 +770,59 @@\n-      oop mirror = fd->field_holder()->java_mirror();\n-      _out->print(\"staticfield %s %s %s \", _holder, fd->name()->as_quoted_ascii(), fd->signature()->as_quoted_ascii());\n-      BasicType field_type = fd->field_type();\n-      switch (field_type) {\n-        case T_BYTE:    _out->print_cr(\"%d\", mirror->byte_field(fd->offset()));   break;\n-        case T_BOOLEAN: _out->print_cr(\"%d\", mirror->bool_field(fd->offset()));   break;\n-        case T_SHORT:   _out->print_cr(\"%d\", mirror->short_field(fd->offset()));  break;\n-        case T_CHAR:    _out->print_cr(\"%d\", mirror->char_field(fd->offset()));   break;\n-        case T_INT:     _out->print_cr(\"%d\", mirror->int_field(fd->offset()));    break;\n-        case T_LONG:    _out->print_cr(INT64_FORMAT, (int64_t)(mirror->long_field(fd->offset())));   break;\n-        case T_FLOAT: {\n-          float f = mirror->float_field(fd->offset());\n-          _out->print_cr(\"%d\", *(int*)&f);\n-          break;\n-        }\n-        case T_DOUBLE: {\n-          double d = mirror->double_field(fd->offset());\n-          _out->print_cr(INT64_FORMAT, *(int64_t*)&d);\n-          break;\n-        }\n-        case T_ARRAY:  \/\/ fall-through\n-        case T_OBJECT: {\n-          oop value =  mirror->obj_field_acquire(fd->offset());\n-          if (value == nullptr) {\n-            if (field_type == T_ARRAY) {\n-              _out->print(\"%d\", -1);\n-            }\n-            _out->cr();\n-          } else if (value->is_instance()) {\n-            assert(field_type == T_OBJECT, \"\");\n-            if (value->is_a(vmClasses::String_klass())) {\n-              const char* ascii_value = java_lang_String::as_quoted_ascii(value);\n-              _out->print_cr(\"\\\"%s\\\"\", (ascii_value != nullptr) ? ascii_value : \"\");\n-            } else {\n-              const char* klass_name  = value->klass()->name()->as_quoted_ascii();\n-              _out->print_cr(\"%s\", klass_name);\n-            }\n-          } else if (value->is_array()) {\n-            typeArrayOop ta = (typeArrayOop)value;\n-            _out->print(\"%d\", ta->length());\n-            if (value->is_objArray()) {\n-              objArrayOop oa = (objArrayOop)value;\n-              const char* klass_name  = value->klass()->name()->as_quoted_ascii();\n-              _out->print(\" %s\", klass_name);\n-            }\n-            _out->cr();\n+      InstanceKlass* holder = fd->field_holder();\n+      oop mirror = holder->java_mirror();\n+      _out->print(\"staticfield %s %s \", _holder, fd->name()->as_quoted_ascii());\n+      BasicType bt = fd->field_type();\n+      if (bt != T_OBJECT && bt != T_ARRAY) {\n+        _out->print(\"%s \", fd->signature()->as_quoted_ascii());\n+      }\n+      do_field_helper(fd, mirror, false);\n+      _out->cr();\n+    }\n+  }\n+};\n+\n+class InlineTypeFieldPrinter : public StaticFieldPrinter {\n+  oop _obj;\n+public:\n+  InlineTypeFieldPrinter(outputStream* out, oop obj) :\n+    StaticFieldPrinter(out), _obj(obj) {\n+  }\n+  void do_field(fieldDescriptor* fd) {\n+    do_field_helper(fd, _obj, true);\n+    _out->print(\" \");\n+  }\n+};\n+\n+void StaticFieldPrinter::do_field_helper(fieldDescriptor* fd, oop mirror, bool is_flat) {\n+  BasicType field_type = fd->field_type();\n+  switch (field_type) {\n+    case T_BYTE:    _out->print(\"%d\", mirror->byte_field(fd->offset()));   break;\n+    case T_BOOLEAN: _out->print(\"%d\", mirror->bool_field(fd->offset()));   break;\n+    case T_SHORT:   _out->print(\"%d\", mirror->short_field(fd->offset()));  break;\n+    case T_CHAR:    _out->print(\"%d\", mirror->char_field(fd->offset()));   break;\n+    case T_INT:     _out->print(\"%d\", mirror->int_field(fd->offset()));    break;\n+    case T_LONG:    _out->print(INT64_FORMAT, (int64_t)(mirror->long_field(fd->offset())));   break;\n+    case T_FLOAT: {\n+      float f = mirror->float_field(fd->offset());\n+      _out->print(\"%d\", *(int*)&f);\n+      break;\n+    }\n+    case T_DOUBLE: {\n+      double d = mirror->double_field(fd->offset());\n+      _out->print(INT64_FORMAT, *(int64_t*)&d);\n+      break;\n+    }\n+    case T_ARRAY:  \/\/ fall-through\n+    case T_OBJECT:\n+      if (!fd->is_null_free_inline_type()) {\n+        _out->print(\"%s \", fd->signature()->as_quoted_ascii());\n+        oop value =  mirror->obj_field_acquire(fd->offset());\n+        if (value == nullptr) {\n+          if (field_type == T_ARRAY) {\n+            _out->print(\"%d\", -1);\n+          }\n+          _out->cr();\n+        } else if (value->is_instance()) {\n+          assert(field_type == T_OBJECT, \"\");\n+          if (value->is_a(vmClasses::String_klass())) {\n+            const char* ascii_value = java_lang_String::as_quoted_ascii(value);\n+            _out->print(\"\\\"%s\\\"\", (ascii_value != nullptr) ? ascii_value : \"\");\n@@ -712,1 +830,2 @@\n-            ShouldNotReachHere();\n+            const char* klass_name  = value->klass()->name()->as_quoted_ascii();\n+            _out->print(\"%s\", klass_name);\n@@ -714,3 +833,9 @@\n-          break;\n-        }\n-        default:\n+        } else if (value->is_array()) {\n+          typeArrayOop ta = (typeArrayOop)value;\n+          _out->print(\"%d\", ta->length());\n+          if (value->is_objArray() || value->is_flatArray()) {\n+            objArrayOop oa = (objArrayOop)value;\n+            const char* klass_name  = value->klass()->name()->as_quoted_ascii();\n+            _out->print(\" %s\", klass_name);\n+          }\n+        } else {\n@@ -719,1 +844,26 @@\n-    }\n+        break;\n+      } else {\n+        \/\/ handling of null free inline type\n+        ResetNoHandleMark rnhm;\n+        Thread* THREAD = Thread::current();\n+        SignatureStream ss(fd->signature(), false);\n+        Symbol* name = ss.as_symbol();\n+        assert(!HAS_PENDING_EXCEPTION, \"can resolve klass?\");\n+        InstanceKlass* holder = fd->field_holder();\n+        InstanceKlass* k = SystemDictionary::find_instance_klass(THREAD, name,\n+                                                                 Handle(THREAD, holder->class_loader()));\n+        assert(k != nullptr && !HAS_PENDING_EXCEPTION, \"can resolve klass?\");\n+        InlineKlass* vk = InlineKlass::cast(k);\n+        oop obj;\n+        if (is_flat) {\n+          int field_offset = fd->offset() - vk->payload_offset();\n+          obj = cast_to_oop(cast_from_oop<address>(mirror) + field_offset);\n+        } else {\n+          obj = mirror->obj_field_acquire(fd->offset());\n+        }\n+        InlineTypeFieldPrinter print_field(_out, obj);\n+        vk->do_nonstatic_fields(&print_field);\n+        break;\n+      }\n+    default:\n+      ShouldNotReachHere();\n@@ -721,1 +871,1 @@\n-};\n+}\n","filename":"src\/hotspot\/share\/ci\/ciInstanceKlass.cpp","additions":270,"deletions":120,"binary":false,"changes":390,"status":"modified"},{"patch":"@@ -71,1 +71,14 @@\n-  GrowableArray<ciField*>* _nonstatic_fields;  \/\/ ordered by JavaFieldStream\n+\n+  \/\/ Fields declared in the bytecode (without nested fields in flat fields),\n+  \/\/ ordered in JavaFieldStream order, with superclasses first (i.e. from lang.java.Object\n+  \/\/ to most derived class).\n+  const GrowableArray<ciField*>* _declared_nonstatic_fields;\n+\n+  \/\/ Fields laid out in memory (flat fields are expanded into their components). The ciField object\n+  \/\/ for each primitive component has the holder being this ciInstanceKlass or one of its\n+  \/\/ superclasses.\n+  \/\/ Fields are in the same order as in _declared_nonstatic_fields, but flat fields are replaced by\n+  \/\/ the list of their own fields, ordered the same way (hierarchy traversed top-down, in\n+  \/\/ JavaFieldStream order).\n+  const GrowableArray<ciField*>* _nonstatic_fields;\n+\n@@ -89,1 +102,1 @@\n-  ciInstanceKlass(ciSymbol* name, jobject loader);\n+  ciInstanceKlass(ciSymbol* name, jobject loader, BasicType bt = T_OBJECT); \/\/ for unloaded klasses\n@@ -110,2 +123,2 @@\n-  int  compute_nonstatic_fields();\n-  GrowableArray<ciField*>* compute_nonstatic_fields_impl(GrowableArray<ciField*>* super_fields);\n+  void compute_nonstatic_fields();\n+  void compute_nonstatic_fields_impl(const GrowableArray<ciField*>* super_declared_fields, const GrowableArray<ciField*>* super_fields);\n@@ -209,0 +222,12 @@\n+  \/\/ Get field descriptor at field_offset ignoring flattening\n+  ciField* get_non_flat_field_by_offset(int field_offset);\n+  \/\/ Get the index of the declared field that contains this offset\n+  int field_index_by_offset(int offset);\n+\n+  \/\/ Total number of nonstatic fields (including inherited)\n+  int nof_declared_nonstatic_fields() {\n+    if (_declared_nonstatic_fields == nullptr) {\n+      compute_nonstatic_fields();\n+    }\n+    return _declared_nonstatic_fields->length();\n+  }\n@@ -211,5 +236,4 @@\n-  \/\/ total number of nonstatic fields (including inherited):\n-    if (_nonstatic_fields == nullptr)\n-      return compute_nonstatic_fields();\n-    else\n-      return _nonstatic_fields->length();\n+    if (_nonstatic_fields == nullptr) {\n+      compute_nonstatic_fields();\n+    }\n+    return _nonstatic_fields->length();\n@@ -228,1 +252,5 @@\n-  \/\/ nth nonstatic field (presented by ascending address)\n+  ciField* declared_nonstatic_field_at(int i) {\n+    assert(_declared_nonstatic_fields != nullptr, \"should be initialized\");\n+    return _declared_nonstatic_fields->at(i);\n+  }\n+\n@@ -249,1 +277,0 @@\n-  bool is_super       () { return flags().is_super(); }\n@@ -252,0 +279,1 @@\n+  bool is_abstract_value_klass() { return is_abstract() && !flags().is_identity(); }\n@@ -266,0 +294,2 @@\n+  virtual bool can_be_inline_klass(bool is_exact = false);\n+\n","filename":"src\/hotspot\/share\/ci\/ciInstanceKlass.hpp","additions":41,"deletions":11,"binary":false,"changes":52,"status":"modified"},{"patch":"@@ -55,0 +55,2 @@\n+#include \"oops\/flatArrayKlass.hpp\"\n+#include \"oops\/inlineKlass.inline.hpp\"\n@@ -56,1 +58,1 @@\n-#include \"oops\/instanceMirrorKlass.hpp\"\n+#include \"oops\/instanceMirrorKlass.inline.hpp\"\n@@ -871,0 +873,1 @@\n+int java_lang_Class::_is_identity_offset;\n@@ -1098,0 +1101,1 @@\n+  set_is_identity(mirror(), k->is_array_klass() || k->is_identity_class());\n@@ -1106,1 +1110,9 @@\n-    if (k->is_typeArray_klass()) {\n+    if (k->is_flatArray_klass()) {\n+      Klass* element_klass = (Klass*) FlatArrayKlass::cast(k)->element_klass();\n+      assert(element_klass->is_inline_klass(), \"Must be inline type component\");\n+      if (is_scratch) {\n+        comp_mirror = Handle(THREAD, HeapShared::scratch_java_mirror(element_klass));\n+      } else {\n+        comp_mirror = Handle(THREAD, element_klass->java_mirror());\n+      }\n+    } else if (k->is_typeArray_klass()) {\n@@ -1115,0 +1127,1 @@\n+      assert(!k->is_refArray_klass() || !k->is_flatArray_klass(), \"Must not have mirror\");\n@@ -1117,0 +1130,1 @@\n+      oop comp_oop = element_klass->java_mirror();\n@@ -1120,1 +1134,1 @@\n-        comp_mirror = Handle(THREAD, element_klass->java_mirror());\n+        comp_mirror = Handle(THREAD, comp_oop);\n@@ -1149,1 +1163,0 @@\n-\n@@ -1153,0 +1166,9 @@\n+\n+    if (k->is_refined_objArray_klass()) {\n+      Klass* super_klass = k->super();\n+      assert(super_klass != nullptr, \"Must be\");\n+      Handle mirror(THREAD, super_klass->java_mirror());\n+      k->set_java_mirror(mirror);\n+      return;\n+    }\n+\n@@ -1175,0 +1197,1 @@\n+\n@@ -1198,3 +1221,3 @@\n-  if (k->class_loader() != nullptr &&\n-      k->class_loader() != SystemDictionary::java_platform_loader() &&\n-      k->class_loader() != SystemDictionary::java_system_loader()) {\n+  if ((k->class_loader() != nullptr &&\n+       k->class_loader() != SystemDictionary::java_platform_loader() &&\n+       k->class_loader() != SystemDictionary::java_system_loader())) {\n@@ -1239,1 +1262,0 @@\n-  assert(as_Klass(m) == k, \"must be\");\n@@ -1243,0 +1265,1 @@\n+    assert(as_Klass(m) == k, \"must be\");\n@@ -1252,0 +1275,4 @@\n+  } else {\n+    ObjArrayKlass* objarray_k = (ObjArrayKlass*)as_Klass(m);\n+    \/\/ Mirror should be restored for an ObjArrayKlass or one of its refined array klasses\n+    assert(objarray_k == k || objarray_k->next_refined_array_klass() == k, \"must be\");\n@@ -1389,0 +1416,4 @@\n+void java_lang_Class::set_is_identity(oop java_class, bool value) {\n+  assert(_is_identity_offset != 0, \"must be set\");\n+  java_class->bool_field_put(_is_identity_offset, value);\n+}\n@@ -1430,1 +1461,3 @@\n-  if (is_instance)  st->print(\"L\");\n+  if (is_instance)  {\n+    st->print(\"L\");\n+  }\n@@ -1489,0 +1522,1 @@\n+  assert(!klass->is_refined_objArray_klass(), \"should not be ref or flat array klass\");\n@@ -1548,1 +1582,2 @@\n-  macro(_is_primitive_offset,        k, \"primitive\",           bool_signature,         false);\n+  macro(_is_primitive_offset,        k, \"primitive\",           bool_signature,         false); \\\n+  macro(_is_identity_offset,         k, \"identity\",            bool_signature,         false);\n@@ -2873,1 +2908,1 @@\n-      if (method->name() == vmSymbols::object_initializer_name() &&\n+      if (method->is_object_constructor() &&\n@@ -3232,2 +3267,2 @@\n-  if (m->is_object_initializer()) {\n-    flags |= java_lang_invoke_MemberName::MN_IS_CONSTRUCTOR;\n+  if (m->is_object_constructor()) {\n+    flags |= java_lang_invoke_MemberName::MN_IS_OBJECT_CONSTRUCTOR;\n@@ -3624,1 +3659,1 @@\n-int java_lang_reflect_Field::_trusted_final_offset;\n+int java_lang_reflect_Field::_flags_offset;\n@@ -3634,1 +3669,1 @@\n-  macro(_trusted_final_offset,    k, vmSymbols::trusted_final_name(),    bool_signature,       false); \\\n+  macro(_flags_offset,     k, vmSymbols::flags_name(),     int_signature,    false); \\\n@@ -3699,2 +3734,2 @@\n-void java_lang_reflect_Field::set_trusted_final(oop field) {\n-  field->bool_field_put(_trusted_final_offset, true);\n+void java_lang_reflect_Field::set_flags(oop field, int value) {\n+  field->int_field_put(_flags_offset, value);\n@@ -4000,2 +4035,1 @@\n-int java_lang_boxing_object::_value_offset;\n-int java_lang_boxing_object::_long_value_offset;\n+int* java_lang_boxing_object::_offsets;\n@@ -4003,3 +4037,9 @@\n-#define BOXING_FIELDS_DO(macro) \\\n-  macro(_value_offset,      integerKlass, \"value\", int_signature, false); \\\n-  macro(_long_value_offset, longKlass, \"value\", long_signature, false);\n+#define BOXING_FIELDS_DO(macro)                                                                                                    \\\n+  macro(java_lang_boxing_object::_offsets[T_BOOLEAN - T_BOOLEAN], vmClasses::Boolean_klass(),   \"value\", bool_signature,   false); \\\n+  macro(java_lang_boxing_object::_offsets[T_CHAR - T_BOOLEAN],    vmClasses::Character_klass(), \"value\", char_signature,   false); \\\n+  macro(java_lang_boxing_object::_offsets[T_FLOAT - T_BOOLEAN],   vmClasses::Float_klass(),     \"value\", float_signature,  false); \\\n+  macro(java_lang_boxing_object::_offsets[T_DOUBLE - T_BOOLEAN],  vmClasses::Double_klass(),    \"value\", double_signature, false); \\\n+  macro(java_lang_boxing_object::_offsets[T_BYTE - T_BOOLEAN],    vmClasses::Byte_klass(),      \"value\", byte_signature,   false); \\\n+  macro(java_lang_boxing_object::_offsets[T_SHORT - T_BOOLEAN],   vmClasses::Short_klass(),     \"value\", short_signature,  false); \\\n+  macro(java_lang_boxing_object::_offsets[T_INT - T_BOOLEAN],     vmClasses::Integer_klass(),   \"value\", int_signature,    false); \\\n+  macro(java_lang_boxing_object::_offsets[T_LONG - T_BOOLEAN],    vmClasses::Long_klass(),      \"value\", long_signature,   false);\n@@ -4008,2 +4048,2 @@\n-  InstanceKlass* integerKlass = vmClasses::Integer_klass();\n-  InstanceKlass* longKlass = vmClasses::Long_klass();\n+  assert(T_LONG - T_BOOLEAN == 7, \"Sanity check\");\n+  java_lang_boxing_object::_offsets = NEW_C_HEAP_ARRAY(int, 8, mtInternal);\n@@ -4015,0 +4055,4 @@\n+  if (f->reading()) {\n+    assert(T_LONG - T_BOOLEAN == 7, \"Sanity check\");\n+    java_lang_boxing_object::_offsets = NEW_C_HEAP_ARRAY(int, 8, mtInternal);\n+  }\n@@ -4035,1 +4079,1 @@\n-      box->bool_field_put(_value_offset, value->z);\n+      box->bool_field_put(value_offset(type), value->z);\n@@ -4038,1 +4082,1 @@\n-      box->char_field_put(_value_offset, value->c);\n+      box->char_field_put(value_offset(type), value->c);\n@@ -4041,1 +4085,1 @@\n-      box->float_field_put(_value_offset, value->f);\n+      box->float_field_put(value_offset(type), value->f);\n@@ -4044,1 +4088,1 @@\n-      box->double_field_put(_long_value_offset, value->d);\n+      box->double_field_put(value_offset(type), value->d);\n@@ -4047,1 +4091,1 @@\n-      box->byte_field_put(_value_offset, value->b);\n+      box->byte_field_put(value_offset(type), value->b);\n@@ -4050,1 +4094,1 @@\n-      box->short_field_put(_value_offset, value->s);\n+      box->short_field_put(value_offset(type), value->s);\n@@ -4053,1 +4097,1 @@\n-      box->int_field_put(_value_offset, value->i);\n+      box->int_field_put(value_offset(type), value->i);\n@@ -4056,1 +4100,1 @@\n-      box->long_field_put(_long_value_offset, value->j);\n+      box->long_field_put(value_offset(type), value->j);\n@@ -4078,1 +4122,1 @@\n-    value->z = box->bool_field(_value_offset);\n+    value->z = box->bool_field(value_offset(type));\n@@ -4081,1 +4125,1 @@\n-    value->c = box->char_field(_value_offset);\n+    value->c = box->char_field(value_offset(type));\n@@ -4084,1 +4128,1 @@\n-    value->f = box->float_field(_value_offset);\n+    value->f = box->float_field(value_offset(type));\n@@ -4087,1 +4131,1 @@\n-    value->d = box->double_field(_long_value_offset);\n+    value->d = box->double_field(value_offset(type));\n@@ -4090,1 +4134,1 @@\n-    value->b = box->byte_field(_value_offset);\n+    value->b = box->byte_field(value_offset(type));\n@@ -4093,1 +4137,1 @@\n-    value->s = box->short_field(_value_offset);\n+    value->s = box->short_field(value_offset(type));\n@@ -4096,1 +4140,1 @@\n-    value->i = box->int_field(_value_offset);\n+    value->i = box->int_field(value_offset(type));\n@@ -4099,1 +4143,1 @@\n-    value->j = box->long_field(_long_value_offset);\n+    value->j = box->long_field(value_offset(type));\n@@ -4112,1 +4156,1 @@\n-    box->bool_field_put(_value_offset, value->z);\n+    box->bool_field_put(value_offset(type), value->z);\n@@ -4115,1 +4159,1 @@\n-    box->char_field_put(_value_offset, value->c);\n+    box->char_field_put(value_offset(type), value->c);\n@@ -4118,1 +4162,1 @@\n-    box->float_field_put(_value_offset, value->f);\n+    box->float_field_put(value_offset(type), value->f);\n@@ -4121,1 +4165,1 @@\n-    box->double_field_put(_long_value_offset, value->d);\n+    box->double_field_put(value_offset(type), value->d);\n@@ -4124,1 +4168,1 @@\n-    box->byte_field_put(_value_offset, value->b);\n+    box->byte_field_put(value_offset(type), value->b);\n@@ -4127,1 +4171,1 @@\n-    box->short_field_put(_value_offset, value->s);\n+    box->short_field_put(value_offset(type), value->s);\n@@ -4130,1 +4174,1 @@\n-    box->int_field_put(_value_offset, value->i);\n+    box->int_field_put(value_offset(type), value->i);\n@@ -4133,1 +4177,1 @@\n-    box->long_field_put(_long_value_offset, value->j);\n+    box->long_field_put(value_offset(type), value->j);\n@@ -4529,1 +4573,0 @@\n-\n@@ -4539,1 +4582,1 @@\n-  return (flags(mname) & (MN_IS_METHOD | MN_IS_CONSTRUCTOR)) > 0;\n+  return (flags(mname) & (MN_IS_METHOD | MN_IS_OBJECT_CONSTRUCTOR)) > 0;\n@@ -5532,16 +5575,11 @@\n-#define CHECK_OFFSET(klass_name, cpp_klass_name, field_name, field_sig) \\\n-  valid &= check_offset(klass_name, cpp_klass_name :: _##field_name ## _offset, #field_name, field_sig)\n-\n-#define CHECK_LONG_OFFSET(klass_name, cpp_klass_name, field_name, field_sig) \\\n-  valid &= check_offset(klass_name, cpp_klass_name :: _##long_ ## field_name ## _offset, #field_name, field_sig)\n-\n-  \/\/ Boxed primitive objects (java_lang_boxing_object)\n-\n-  CHECK_OFFSET(\"java\/lang\/Boolean\",   java_lang_boxing_object, value, \"Z\");\n-  CHECK_OFFSET(\"java\/lang\/Character\", java_lang_boxing_object, value, \"C\");\n-  CHECK_OFFSET(\"java\/lang\/Float\",     java_lang_boxing_object, value, \"F\");\n-  CHECK_LONG_OFFSET(\"java\/lang\/Double\", java_lang_boxing_object, value, \"D\");\n-  CHECK_OFFSET(\"java\/lang\/Byte\",      java_lang_boxing_object, value, \"B\");\n-  CHECK_OFFSET(\"java\/lang\/Short\",     java_lang_boxing_object, value, \"S\");\n-  CHECK_OFFSET(\"java\/lang\/Integer\",   java_lang_boxing_object, value, \"I\");\n-  CHECK_LONG_OFFSET(\"java\/lang\/Long\", java_lang_boxing_object, value, \"J\");\n+#define CHECK_OFFSET(klass_name, type, field_sig) \\\n+  valid &= check_offset(klass_name, java_lang_boxing_object::value_offset(type), \"value\", field_sig)\n+\n+  CHECK_OFFSET(\"java\/lang\/Boolean\",   T_BOOLEAN, \"Z\");\n+  CHECK_OFFSET(\"java\/lang\/Character\", T_CHAR,    \"C\");\n+  CHECK_OFFSET(\"java\/lang\/Float\",     T_FLOAT,   \"F\");\n+  CHECK_OFFSET(\"java\/lang\/Double\",    T_DOUBLE,  \"D\");\n+  CHECK_OFFSET(\"java\/lang\/Byte\",      T_BYTE,    \"B\");\n+  CHECK_OFFSET(\"java\/lang\/Short\",     T_SHORT,   \"S\");\n+  CHECK_OFFSET(\"java\/lang\/Integer\",   T_INT,     \"I\");\n+  CHECK_OFFSET(\"java\/lang\/Long\",      T_LONG,    \"J\");\n","filename":"src\/hotspot\/share\/classfile\/javaClasses.cpp","additions":104,"deletions":66,"binary":false,"changes":170,"status":"modified"},{"patch":"@@ -984,1 +984,1 @@\n-  size_t single_array_size = objArrayOopDesc::object_size(total);\n+  size_t single_array_size = refArrayOopDesc::object_size(total);\n@@ -996,2 +996,2 @@\n-    size_t primary_array_size = objArrayOopDesc::object_size(primary_array_length);\n-    size_t secondary_array_size = objArrayOopDesc::object_size(_secondary_array_max_length);\n+    size_t primary_array_size = refArrayOopDesc::object_size(primary_array_length);\n+    size_t secondary_array_size = refArrayOopDesc::object_size(_secondary_array_max_length);\n@@ -1038,1 +1038,1 @@\n-    size_t next_size = objArrayOopDesc::object_size(1 << (max + 1));\n+    size_t next_size = refArrayOopDesc::object_size(1 << (max + 1));\n","filename":"src\/hotspot\/share\/classfile\/stringTable.cpp","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -57,0 +57,2 @@\n+#include \"oops\/fieldStreams.inline.hpp\"\n+#include \"oops\/inlineKlass.inline.hpp\"\n@@ -74,0 +76,1 @@\n+#include \"runtime\/os.hpp\"\n@@ -188,0 +191,15 @@\n+\/\/ These migrated value classes are loaded by the bootstrap class loader but are added to the initiating\n+\/\/ loaders automatically so that fields of these types can be found and potentially flattened during\n+\/\/ field layout.\n+static void add_migrated_value_classes(ClassLoaderData* cld) {\n+  JavaThread* current = JavaThread::current();\n+  auto add_klass = [&] (Symbol* classname) {\n+    InstanceKlass* ik = SystemDictionary::find_instance_klass(current, classname, Handle(current, nullptr));\n+    assert(ik != nullptr, \"Must exist\");\n+    SystemDictionary::add_to_initiating_loader(current, ik, cld);\n+  };\n+\n+  MonitorLocker mu1(SystemDictionary_lock);\n+  vmSymbols::migrated_class_names_do(add_klass);\n+}\n+\n@@ -193,2 +211,10 @@\n-    return (class_loader() == nullptr) ? ClassLoaderData::the_null_class_loader_data() :\n-                                      ClassLoaderDataGraph::find_or_create(class_loader);\n+    if (class_loader() == nullptr) {\n+      return ClassLoaderData::the_null_class_loader_data();\n+    } else {\n+      bool created = false;\n+      ClassLoaderData* cld = ClassLoaderDataGraph::find_or_create(class_loader, created);\n+      if (created && Arguments::enable_preview()) {\n+        add_migrated_value_classes(cld);\n+      }\n+      return cld;\n+    }\n@@ -402,1 +428,2 @@\n-\/\/ during class definition to allow class circularity checking\n+\/\/ during class definition, or may be called for inline field layout processing\n+\/\/ to detect class circularity errors.\n@@ -415,0 +442,2 @@\n+\/\/ inline field layout callers:\n+\/\/    The field's class must be loaded to determine layout.\n@@ -418,1 +447,1 @@\n-\/\/ placeholder for the same thread, class, classloader is found.\n+\/\/ placeholder for the same thread, class, and classloader is found.\n@@ -453,1 +482,1 @@\n-    \/\/ Must check ClassCircularity before resolving next_name (superclass or interface).\n+    \/\/ Must check ClassCircularity before resolving next_name (superclass, interface, field types or speculatively preloaded argument types).\n@@ -477,1 +506,1 @@\n-  \/\/ Resolve the superclass or superinterface, check results on return\n+  \/\/ Resolve the superclass, superinterface, field type or speculatively preloaded argument types and check results on return.\n@@ -923,2 +952,1 @@\n-  assert(!ModuleEntryTable::javabase_moduleEntry()->is_patched(),\n-         \"Cannot use sharing if java.base is patched\");\n+  assert(!CDSConfig::module_patching_disables_cds(), \"Cannot use CDS\");\n@@ -1000,1 +1028,1 @@\n-        assert(!mod_entry->is_patched(), \"cannot load archived classes for patched module\");\n+        assert(!CDSConfig::module_patching_disables_cds(), \"Cannot use CDS\");\n@@ -1078,0 +1106,69 @@\n+\/\/ Pre-load class referred to in non-static null-free instance field. These fields trigger MANDATORY loading.\n+\/\/ Some pre-loading does not fail fatally\n+bool SystemDictionary::preload_from_null_free_field(InstanceKlass* ik, Handle class_loader, Symbol* sig, int field_index, TRAPS) {\n+  TempNewSymbol name = Signature::strip_envelope(sig);\n+  log_info(class, preload)(\"Preloading of class %s during loading of shared class %s. \"\n+                           \"Cause: a null-free non-static field is declared with this type\",\n+                           name->as_C_string(), ik->name()->as_C_string());\n+  InstanceKlass* real_k = SystemDictionary::resolve_with_circularity_detection(ik->name(), name,\n+                                                                               class_loader, false, CHECK_false);\n+  if (HAS_PENDING_EXCEPTION) {\n+    log_info(class, preload)(\"Preloading of class %s during loading of class %s \"\n+                                \"(cause: null-free non-static field) failed: %s\",\n+                                name->as_C_string(), ik->name()->as_C_string(),\n+                                PENDING_EXCEPTION->klass()->name()->as_C_string());\n+    return false; \/\/ Exception is still pending\n+  }\n+\n+  InstanceKlass* k = ik->get_inline_type_field_klass_or_null(field_index);\n+  if (real_k != k) {\n+    \/\/ oops, the app has substituted a different version of k! Does not fail fatally\n+    log_info(class, preload)(\"Preloading of class %s during loading of shared class %s \"\n+                                \"(cause: null-free non-static field) failed : \"\n+                                \"app substituted a different version of %s\",\n+                                name->as_C_string(), ik->name()->as_C_string(),\n+                                name->as_C_string());\n+    return false;\n+  }\n+  log_info(class, preload)(\"Preloading of class %s during loading of shared class %s \"\n+                           \"(cause: null-free non-static field) succeeded\",\n+                           name->as_C_string(), ik->name()->as_C_string());\n+\n+  assert(real_k != nullptr, \"Sanity check\");\n+  InstanceKlass::check_can_be_annotated_with_NullRestricted(real_k, ik->name(), CHECK_false);\n+\n+  return true;\n+}\n+\n+\/\/ Tries to pre-load classes referred to in non-static nullable instance fields if they are found in the\n+\/\/ loadable descriptors attribute. If loading fails, we can fail silently.\n+void SystemDictionary::try_preload_from_loadable_descriptors(InstanceKlass* ik, Handle class_loader, Symbol* sig, int field_index, TRAPS) {\n+  TempNewSymbol name = Signature::strip_envelope(sig);\n+  if (name != ik->name() && ik->is_class_in_loadable_descriptors_attribute(sig)) {\n+    log_info(class, preload)(\"Preloading of class %s during loading of shared class %s. \"\n+                             \"Cause: field type in LoadableDescriptors attribute\",\n+                             name->as_C_string(), ik->name()->as_C_string());\n+    InstanceKlass* real_k = SystemDictionary::resolve_with_circularity_detection(ik->name(), name,\n+                                                                                 class_loader, false, THREAD);\n+    if (HAS_PENDING_EXCEPTION) {\n+      CLEAR_PENDING_EXCEPTION;\n+    }\n+\n+    InstanceKlass* k = ik->get_inline_type_field_klass_or_null(field_index);\n+    if (real_k != k) {\n+      \/\/ oops, the app has substituted a different version of k!\n+      log_info(class, preload)(\"Preloading of class %s during loading of shared class %s \"\n+                                  \"(cause: field type in LoadableDescriptors attribute) failed : \"\n+                                  \"app substituted a different version of %s\",\n+                                  name->as_C_string(), ik->name()->as_C_string(),\n+                                  k->name()->as_C_string());\n+      return;\n+    } else if (real_k != nullptr) {\n+      log_info(class, preload)(\"Preloading of class %s during loading of shared class %s \"\n+                               \"(cause: field type in LoadableDescriptors attribute) succeeded\",\n+                                name->as_C_string(), ik->name()->as_C_string());\n+    }\n+  }\n+}\n+\n+\n@@ -1101,0 +1198,21 @@\n+  if (ik->has_inline_type_fields()) {\n+    for (AllFieldStream fs(ik); !fs.done(); fs.next()) {\n+      if (fs.access_flags().is_static()) continue;\n+\n+      Symbol* sig = fs.signature();\n+      int field_index = fs.index();\n+\n+      if (fs.is_null_free_inline_type()) {\n+        \/\/ A false return means that the class didn't load for other reasons than an exception.\n+        bool check = preload_from_null_free_field(ik, class_loader, sig, field_index, CHECK_NULL);\n+        if (!check) {\n+          ik->set_shared_loading_failed();\n+          return nullptr;\n+        }\n+      } else if (Signature::has_envelope(sig)) {\n+          \/\/ Pending exceptions are cleared so we can fail silently\n+          try_preload_from_loadable_descriptors(ik, class_loader, sig, field_index, CHECK_NULL);\n+      }\n+    }\n+  }\n+\n@@ -1136,0 +1254,1 @@\n+\n@@ -1710,1 +1829,0 @@\n-#if INCLUDE_CDS\n@@ -1713,1 +1831,3 @@\n-\/\/ This API should be used only by AOTLinkedClassBulkLoader\n+\/\/ This API is used by AOTLinkedClassBulkLoader and to register boxing\n+\/\/ classes from java.lang in all class loaders to enable more value\n+\/\/ classes optimizations\n@@ -1717,1 +1837,0 @@\n-  assert(CDSConfig::is_using_aot_linked_classes(), \"must be\");\n@@ -1723,2 +1842,3 @@\n-  assert(dictionary->find_class(current, name) == nullptr, \"sanity\");\n-  dictionary->add_klass(current, name, k);\n+  if (dictionary->find_class(current, name) == nullptr) {\n+    dictionary->add_klass(current, name, k);\n+  }\n@@ -1726,1 +1846,0 @@\n-#endif\n","filename":"src\/hotspot\/share\/classfile\/systemDictionary.cpp","additions":134,"deletions":15,"binary":false,"changes":149,"status":"modified"},{"patch":"@@ -65,0 +65,1 @@\n+static_assert(!std::is_polymorphic<BufferedInlineTypeBlob>::value,   \"no virtual methods are allowed in code blobs\");\n@@ -93,0 +94,1 @@\n+      &BufferedInlineTypeBlob::_vpntr,\n@@ -428,1 +430,1 @@\n-    blob = new (size) BufferBlob(name, CodeBlobKind::Buffer, cb, size);\n+    blob = new (size) BufferBlob(name, CodeBlobKind::Buffer, cb, size, sizeof(BufferBlob));\n@@ -444,0 +446,4 @@\n+BufferBlob::BufferBlob(const char* name, CodeBlobKind kind, CodeBuffer* cb, int size, uint16_t header_size, int frame_complete, int frame_size, OopMapSet* oop_maps, bool caller_must_gc_arguments)\n+  : RuntimeBlob(name, kind, cb, size, header_size, frame_complete, frame_size, oop_maps, caller_must_gc_arguments)\n+{}\n+\n@@ -448,3 +454,2 @@\n-AdapterBlob::AdapterBlob(int size, CodeBuffer* cb, int entry_offset[AdapterBlob::ENTRY_COUNT]) :\n-  BufferBlob(\"I2C\/C2I adapters\", CodeBlobKind::Adapter, cb, size, sizeof(AdapterBlob)) {\n-  assert(entry_offset[I2C] == 0, \"sanity check\");\n+AdapterBlob::AdapterBlob(int size, CodeBuffer* cb, int entry_offset[AdapterBlob::ENTRY_COUNT], int frame_complete, int frame_size, OopMapSet* oop_maps, bool caller_must_gc_arguments) :\n+  BufferBlob(\"I2C\/C2I adapters\", CodeBlobKind::Adapter, cb, size, sizeof(AdapterBlob), frame_complete, frame_size, oop_maps, caller_must_gc_arguments) {\n@@ -452,0 +457,1 @@\n+  assert(entry_offset[I2C] == 0, \"sanity check\");\n@@ -461,0 +467,2 @@\n+  _c2i_inline_offset = entry_offset[C2I_Inline];\n+  _c2i_inline_ro_offset = entry_offset[C2I_Inline_RO];\n@@ -462,0 +470,1 @@\n+  _c2i_unverified_inline_offset = entry_offset[C2I_Unverified_Inline];\n@@ -466,1 +475,1 @@\n-AdapterBlob* AdapterBlob::create(CodeBuffer* cb, int entry_offset[AdapterBlob::ENTRY_COUNT]) {\n+AdapterBlob* AdapterBlob::create(CodeBuffer* cb, int entry_offset[AdapterBlob::ENTRY_COUNT], int frame_complete, int frame_size, OopMapSet* oop_maps, bool caller_must_gc_arguments) {\n@@ -475,1 +484,1 @@\n-    blob = new (size) AdapterBlob(size, cb, entry_offset);\n+    blob = new (size) AdapterBlob(size, cb, entry_offset, frame_complete, frame_size, oop_maps, caller_must_gc_arguments);\n@@ -556,0 +565,25 @@\n+\/\/----------------------------------------------------------------------------------------------------\n+\/\/ Implementation of BufferedInlineTypeBlob\n+BufferedInlineTypeBlob::BufferedInlineTypeBlob(int size, CodeBuffer* cb, int pack_fields_off, int pack_fields_jobject_off, int unpack_fields_off) :\n+  BufferBlob(\"buffered inline type\", CodeBlobKind::BufferedInlineType, cb, size, sizeof(BufferedInlineTypeBlob)),\n+  _pack_fields_off(pack_fields_off),\n+  _pack_fields_jobject_off(pack_fields_jobject_off),\n+  _unpack_fields_off(unpack_fields_off) {\n+  CodeCache::commit(this);\n+}\n+\n+BufferedInlineTypeBlob* BufferedInlineTypeBlob::create(CodeBuffer* cb, int pack_fields_off, int pack_fields_jobject_off, int unpack_fields_off) {\n+  ThreadInVMfromUnknown __tiv;  \/\/ get to VM state in case we block on CodeCache_lock\n+\n+  BufferedInlineTypeBlob* blob = nullptr;\n+  unsigned int size = CodeBlob::allocation_size(cb, sizeof(BufferedInlineTypeBlob));\n+  {\n+    MutexLocker mu(CodeCache_lock, Mutex::_no_safepoint_check_flag);\n+    blob = new (size) BufferedInlineTypeBlob(size, cb, pack_fields_off, pack_fields_jobject_off, unpack_fields_off);\n+  }\n+  \/\/ Track memory usage statistic after releasing CodeCache_lock\n+  MemoryService::track_code_cache_memory_usage();\n+\n+  return blob;\n+}\n+\n","filename":"src\/hotspot\/share\/code\/codeBlob.cpp","additions":40,"deletions":6,"binary":false,"changes":46,"status":"modified"},{"patch":"@@ -716,0 +716,11 @@\n+\n+      \/\/ If inline types are passed as fields, use the extended signature\n+      \/\/ which contains the types of all (oop) fields of the inline type.\n+      if (is_compiled_by_c2() && callee->has_scalarized_args()) {\n+        const GrowableArray<SigEntry>* sig = callee->adapter()->get_sig_cc();\n+        assert(sig != nullptr, \"sig should never be null\");\n+        TempNewSymbol tmp_sig = SigEntry::create_symbol(sig);\n+        has_receiver = false; \/\/ The extended signature contains the receiver type\n+        fr.oops_compiled_arguments_do(tmp_sig, has_receiver, has_appendix, reg_map, f);\n+        return;\n+      }\n@@ -1246,0 +1257,4 @@\n+  _inline_entry_point             = entry_point();\n+  _verified_inline_entry_point    = verified_entry_point();\n+  _verified_inline_ro_entry_point = verified_entry_point();\n+\n@@ -1285,1 +1300,1 @@\n-\n+    assert(!method->has_scalarized_args(), \"scalarized native wrappers not supported yet\");\n@@ -1736,0 +1751,4 @@\n+    _inline_entry_point             = code_begin() + offsets->value(CodeOffsets::Inline_Entry);\n+    _verified_inline_entry_point    = code_begin() + offsets->value(CodeOffsets::Verified_Inline_Entry);\n+    _verified_inline_ro_entry_point = code_begin() + offsets->value(CodeOffsets::Verified_Inline_Entry_RO);\n+\n@@ -3181,4 +3200,4 @@\n-    if (deps.type() != Dependencies::evol_method)\n-      continue;\n-    Method* method = deps.method_argument(0);\n-    if (method == dependee) return true;\n+    if (Dependencies::has_method_dep(deps.type())) {\n+      Method* method = deps.method_argument(0);\n+      if (method == dependee) return true;\n+    }\n@@ -4022,0 +4041,1 @@\n+  if (pos == inline_entry_point())                                      label = \"[Inline Entry Point]\";\n@@ -4023,0 +4043,2 @@\n+  if (pos == verified_inline_entry_point())                             label = \"[Verified Inline Entry Point]\";\n+  if (pos == verified_inline_ro_entry_point())                          label = \"[Verified Inline Entry Point (RO)]\";\n@@ -4031,0 +4053,10 @@\n+static int maybe_print_entry_label(outputStream* stream, address pos, address entry, const char* label) {\n+  if (pos == entry) {\n+    stream->bol();\n+    stream->print_cr(\"%s\", label);\n+    return 1;\n+  } else {\n+    return 0;\n+  }\n+}\n+\n@@ -4033,4 +4065,13 @@\n-    const char* label = nmethod_section_label(block_begin);\n-    if (label != nullptr) {\n-      stream->bol();\n-      stream->print_cr(\"%s\", label);\n+    int n = 0;\n+    \/\/ Multiple entry points may be at the same position. Print them all.\n+    n += maybe_print_entry_label(stream, block_begin, entry_point(),                    \"[Entry Point]\");\n+    n += maybe_print_entry_label(stream, block_begin, inline_entry_point(),             \"[Inline Entry Point]\");\n+    n += maybe_print_entry_label(stream, block_begin, verified_entry_point(),           \"[Verified Entry Point]\");\n+    n += maybe_print_entry_label(stream, block_begin, verified_inline_entry_point(),    \"[Verified Inline Entry Point]\");\n+    n += maybe_print_entry_label(stream, block_begin, verified_inline_ro_entry_point(), \"[Verified Inline Entry Point (RO)]\");\n+    if (n == 0) {\n+      const char* label = nmethod_section_label(block_begin);\n+      if (label != nullptr) {\n+        stream->bol();\n+        stream->print_cr(\"%s\", label);\n+      }\n@@ -4040,6 +4081,62 @@\n-  if (block_begin == entry_point()) {\n-    Method* m = method();\n-    if (m != nullptr) {\n-      stream->print(\"  # \");\n-      m->print_value_on(stream);\n-      stream->cr();\n+  Method* m = method();\n+  if (m == nullptr || is_osr_method()) {\n+    return;\n+  }\n+\n+  \/\/ Print the name of the method (only once)\n+  address low = MIN3(entry_point(),\n+                     verified_entry_point(),\n+                     inline_entry_point());\n+  \/\/ The verified inline entry point and verified inline RO entry point are not always\n+  \/\/ used. When they are unused. CodeOffsets::Verified_Inline_Entry(_RO) is -1. Hence,\n+  \/\/ the calculated entry point is smaller than the block they are offsetting into.\n+  if (verified_inline_entry_point() >= block_begin) {\n+    low = MIN2(low, verified_inline_entry_point());\n+  }\n+  if (verified_inline_ro_entry_point() >= block_begin) {\n+    low = MIN2(low, verified_inline_ro_entry_point());\n+  }\n+  assert(low != 0, \"sanity\");\n+  if (block_begin == low) {\n+    stream->print(\"  # \");\n+    m->print_value_on(stream);\n+    stream->cr();\n+  }\n+\n+  \/\/ Print the arguments for the 3 types of verified entry points\n+  CompiledEntrySignature ces(m);\n+  ces.compute_calling_conventions(false);\n+  const GrowableArray<SigEntry>* sig_cc;\n+  const VMRegPair* regs;\n+  if (block_begin == verified_entry_point()) {\n+    sig_cc = ces.sig_cc();\n+    regs = ces.regs_cc();\n+  } else if (block_begin == verified_inline_entry_point()) {\n+    sig_cc = ces.sig();\n+    regs = ces.regs();\n+  } else if (block_begin == verified_inline_ro_entry_point()) {\n+    sig_cc = ces.sig_cc_ro();\n+    regs = ces.regs_cc_ro();\n+  } else {\n+    return;\n+  }\n+\n+  bool has_this = !m->is_static();\n+  if (ces.has_inline_recv() && block_begin == verified_entry_point()) {\n+    \/\/ <this> argument is scalarized for verified_entry_point()\n+    has_this = false;\n+  }\n+  const char* spname = \"sp\"; \/\/ make arch-specific?\n+  int stack_slot_offset = this->frame_size() * wordSize;\n+  int tab1 = 14, tab2 = 24;\n+  int sig_index = 0;\n+  int arg_index = has_this ? -1 : 0;\n+  bool did_old_sp = false;\n+  for (ExtendedSignature sig = ExtendedSignature(sig_cc, SigEntryFilter()); !sig.at_end(); ++sig) {\n+    bool at_this = (arg_index == -1);\n+    bool at_old_sp = false;\n+    BasicType t = (*sig)._bt;\n+    if (at_this) {\n+      stream->print(\"  # this: \");\n+    } else {\n+      stream->print(\"  # parm%d: \", arg_index);\n@@ -4047,19 +4144,7 @@\n-    if (m != nullptr && !is_osr_method()) {\n-      ResourceMark rm;\n-      int sizeargs = m->size_of_parameters();\n-      BasicType* sig_bt = NEW_RESOURCE_ARRAY(BasicType, sizeargs);\n-      VMRegPair* regs   = NEW_RESOURCE_ARRAY(VMRegPair, sizeargs);\n-      {\n-        int sig_index = 0;\n-        if (!m->is_static())\n-          sig_bt[sig_index++] = T_OBJECT; \/\/ 'this'\n-        for (SignatureStream ss(m->signature()); !ss.at_return_type(); ss.next()) {\n-          BasicType t = ss.type();\n-          sig_bt[sig_index++] = t;\n-          if (type2size[t] == 2) {\n-            sig_bt[sig_index++] = T_VOID;\n-          } else {\n-            assert(type2size[t] == 1, \"size is 1 or 2\");\n-          }\n-        }\n-        assert(sig_index == sizeargs, \"\");\n+    stream->move_to(tab1);\n+    VMReg fst = regs[sig_index].first();\n+    VMReg snd = regs[sig_index].second();\n+    if (fst->is_reg()) {\n+      stream->print(\"%s\", fst->name());\n+      if (snd->is_valid())  {\n+        stream->print(\":%s\", snd->name());\n@@ -4067,54 +4152,18 @@\n-      const char* spname = \"sp\"; \/\/ make arch-specific?\n-      SharedRuntime::java_calling_convention(sig_bt, regs, sizeargs);\n-      int stack_slot_offset = this->frame_size() * wordSize;\n-      int tab1 = 14, tab2 = 24;\n-      int sig_index = 0;\n-      int arg_index = (m->is_static() ? 0 : -1);\n-      bool did_old_sp = false;\n-      for (SignatureStream ss(m->signature()); !ss.at_return_type(); ) {\n-        bool at_this = (arg_index == -1);\n-        bool at_old_sp = false;\n-        BasicType t = (at_this ? T_OBJECT : ss.type());\n-        assert(t == sig_bt[sig_index], \"sigs in sync\");\n-        if (at_this)\n-          stream->print(\"  # this: \");\n-        else\n-          stream->print(\"  # parm%d: \", arg_index);\n-        stream->move_to(tab1);\n-        VMReg fst = regs[sig_index].first();\n-        VMReg snd = regs[sig_index].second();\n-        if (fst->is_reg()) {\n-          stream->print(\"%s\", fst->name());\n-          if (snd->is_valid())  {\n-            stream->print(\":%s\", snd->name());\n-          }\n-        } else if (fst->is_stack()) {\n-          int offset = fst->reg2stack() * VMRegImpl::stack_slot_size + stack_slot_offset;\n-          if (offset == stack_slot_offset)  at_old_sp = true;\n-          stream->print(\"[%s+0x%x]\", spname, offset);\n-        } else {\n-          stream->print(\"reg%d:%d??\", (int)(intptr_t)fst, (int)(intptr_t)snd);\n-        }\n-        stream->print(\" \");\n-        stream->move_to(tab2);\n-        stream->print(\"= \");\n-        if (at_this) {\n-          m->method_holder()->print_value_on(stream);\n-        } else {\n-          bool did_name = false;\n-          if (!at_this && ss.is_reference()) {\n-            Symbol* name = ss.as_symbol();\n-            name->print_value_on(stream);\n-            did_name = true;\n-          }\n-          if (!did_name)\n-            stream->print(\"%s\", type2name(t));\n-        }\n-        if (at_old_sp) {\n-          stream->print(\"  (%s of caller)\", spname);\n-          did_old_sp = true;\n-        }\n-        stream->cr();\n-        sig_index += type2size[t];\n-        arg_index += 1;\n-        if (!at_this)  ss.next();\n+    } else if (fst->is_stack()) {\n+      int offset = fst->reg2stack() * VMRegImpl::stack_slot_size + stack_slot_offset;\n+      if (offset == stack_slot_offset)  at_old_sp = true;\n+      stream->print(\"[%s+0x%x]\", spname, offset);\n+    } else {\n+      stream->print(\"reg%d:%d??\", (int)(intptr_t)fst, (int)(intptr_t)snd);\n+    }\n+    stream->print(\" \");\n+    stream->move_to(tab2);\n+    stream->print(\"= \");\n+    if (at_this) {\n+      m->method_holder()->print_value_on(stream);\n+    } else {\n+      bool did_name = false;\n+      if (is_reference_type(t)) {\n+        Symbol* name = (*sig)._name;\n+        name->print_value_on(stream);\n+        did_name = true;\n@@ -4122,6 +4171,4 @@\n-      if (!did_old_sp) {\n-        stream->print(\"  # \");\n-        stream->move_to(tab1);\n-        stream->print(\"[%s+0x%x]\", spname, stack_slot_offset);\n-        stream->print(\"  (%s of caller)\", spname);\n-        stream->cr();\n+      if (!did_name)\n+        stream->print(\"%s\", type2name(t));\n+      if ((*sig)._null_marker) {\n+        stream->print(\" (null marker)\");\n@@ -4130,0 +4177,14 @@\n+    if (at_old_sp) {\n+      stream->print(\"  (%s of caller)\", spname);\n+      did_old_sp = true;\n+    }\n+    stream->cr();\n+    sig_index += type2size[t];\n+    arg_index += 1;\n+  }\n+  if (!did_old_sp) {\n+    stream->print(\"  # \");\n+    stream->move_to(tab1);\n+    stream->print(\"[%s+0x%x]\", spname, stack_slot_offset);\n+    stream->print(\"  (%s of caller)\", spname);\n+    stream->cr();\n@@ -4253,1 +4314,1 @@\n-      st->print(\" {reexecute=%d rethrow=%d return_oop=%d}\", sd->should_reexecute(), sd->rethrow_exception(), sd->return_oop());\n+      st->print(\" {reexecute=%d rethrow=%d return_oop=%d return_scalarized=%d}\", sd->should_reexecute(), sd->rethrow_exception(), sd->return_oop(), sd->return_scalarized());\n","filename":"src\/hotspot\/share\/code\/nmethod.cpp","additions":156,"deletions":95,"binary":false,"changes":251,"status":"modified"},{"patch":"@@ -30,0 +30,1 @@\n+#include \"compiler\/compilerDefinitions.hpp\"\n@@ -219,0 +220,4 @@\n+  \/\/ TODO: can these be uint16_t, seem rely on -1 CodeOffset, can change later...\n+  address _inline_entry_point;              \/\/ inline type entry point (unpack all inline type args) with class check\n+  address _verified_inline_entry_point;     \/\/ inline type entry point (unpack all inline type args) without class check\n+  address _verified_inline_ro_entry_point;  \/\/ inline type entry point (unpack receiver only) without class check\n@@ -697,0 +702,3 @@\n+  address inline_entry_point() const              { return _inline_entry_point; }             \/\/ inline type entry point (unpack all inline type args)\n+  address verified_inline_entry_point() const     { return _verified_inline_entry_point; }    \/\/ inline type entry point (unpack all inline type args) without class check\n+  address verified_inline_ro_entry_point() const  { return _verified_inline_ro_entry_point; } \/\/ inline type entry point (only unpack receiver) without class check\n@@ -763,0 +771,10 @@\n+  bool  needs_stack_repair() const {\n+    if (is_compiled_by_c1()) {\n+      return method()->c1_needs_stack_repair();\n+    } else if (is_compiled_by_c2()) {\n+      return method()->c2_needs_stack_repair();\n+    } else {\n+      return false;\n+    }\n+  }\n+\n@@ -1075,3 +1093,4 @@\n-  \/\/ Fast breakpoint support. Tells if this compiled method is\n-  \/\/ dependent on the given method. Returns true if this nmethod\n-  \/\/ corresponds to the given method as well.\n+  \/\/ Tells if this compiled method is dependent on the given method.\n+  \/\/ Returns true if this nmethod corresponds to the given method as well.\n+  \/\/ It is used for fast breakpoint support and updating the calling convention\n+  \/\/ in case of mismatch.\n","filename":"src\/hotspot\/share\/code\/nmethod.hpp","additions":22,"deletions":3,"binary":false,"changes":25,"status":"modified"},{"patch":"@@ -295,0 +295,1 @@\n+  oop obj_buffer_allocate(Klass* klass, size_t size, TRAPS); \/\/ doesn't clear memory\n","filename":"src\/hotspot\/share\/gc\/shared\/collectedHeap.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -200,1 +200,1 @@\n-      if (p < left && !obj->is_objArray()) {\n+      if (p < left && !obj->is_refArray()) {\n@@ -274,1 +274,1 @@\n-        if (!last_obj->is_objArray()) {\n+        if (!last_obj->is_refArray()) {\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahScanRemembered.inline.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -27,0 +27,1 @@\n+#include \"classfile\/systemDictionary.hpp\"\n@@ -47,0 +48,3 @@\n+#include \"oops\/flatArrayKlass.hpp\"\n+#include \"oops\/flatArrayOop.inline.hpp\"\n+#include \"oops\/inlineKlass.inline.hpp\"\n@@ -78,0 +82,1 @@\n+#include \"utilities\/globalDefinitions.hpp\"\n@@ -228,0 +233,32 @@\n+JRT_ENTRY(void, InterpreterRuntime::read_flat_field(JavaThread* current, oopDesc* obj, ResolvedFieldEntry* entry))\n+  assert(oopDesc::is_oop(obj), \"Sanity check\");\n+  Handle obj_h(THREAD, obj);\n+\n+  InstanceKlass* holder = InstanceKlass::cast(entry->field_holder());\n+  assert(entry->field_holder()->field_is_flat(entry->field_index()), \"Sanity check\");\n+\n+  InlineLayoutInfo* layout_info = holder->inline_layout_info_adr(entry->field_index());\n+  InlineKlass* field_vklass = layout_info->klass();\n+\n+#ifdef ASSERT\n+  fieldDescriptor fd;\n+  bool found = holder->find_field_from_offset(entry->field_offset(), false, &fd);\n+  assert(found, \"Field not found\");\n+  assert(fd.is_flat(), \"Field must be flat\");\n+#endif \/\/ ASSERT\n+\n+  oop res = field_vklass->read_payload_from_addr(obj_h(), entry->field_offset(), layout_info->kind(), CHECK);\n+  current->set_vm_result_oop(res);\n+JRT_END\n+\n+JRT_ENTRY(void, InterpreterRuntime::write_flat_field(JavaThread* current, oopDesc* obj, oopDesc* value, ResolvedFieldEntry* entry))\n+  assert(oopDesc::is_oop(obj), \"Sanity check\");\n+  Handle obj_h(THREAD, obj);\n+  assert(value == nullptr || oopDesc::is_oop(value), \"Sanity check\");\n+  Handle val_h(THREAD, value);\n+\n+  InstanceKlass* holder = entry->field_holder();\n+  InlineLayoutInfo* li = holder->inline_layout_info_adr(entry->field_index());\n+  InlineKlass* vk = li->klass();\n+  vk->write_value_to_addr(val_h(), ((char*)(oopDesc*)obj_h()) + entry->field_offset(), li->kind(), true, CHECK);\n+JRT_END\n@@ -237,1 +274,1 @@\n-  objArrayOop obj = oopFactory::new_objArray(klass, size, CHECK);\n+  arrayOop obj = oopFactory::new_objArray(klass, size, CHECK);\n@@ -241,0 +278,12 @@\n+JRT_ENTRY(void, InterpreterRuntime::flat_array_load(JavaThread* current, arrayOopDesc* array, int index))\n+  assert(array->is_flatArray(), \"Must be\");\n+  flatArrayOop farray = (flatArrayOop)array;\n+  oop res = farray->obj_at(index, CHECK);\n+  current->set_vm_result_oop(res);\n+JRT_END\n+\n+JRT_ENTRY(void, InterpreterRuntime::flat_array_store(JavaThread* current, oopDesc* val, arrayOopDesc* array, int index))\n+  assert(array->is_flatArray(), \"Must be\");\n+  flatArrayOop farray = (flatArrayOop)array;\n+  farray->obj_at_put(index, val, CHECK);\n+JRT_END\n@@ -246,2 +295,2 @@\n-  int          i = last_frame.get_index_u2(Bytecodes::_multianewarray);\n-  Klass* klass   = constants->klass_at(i, CHECK);\n+  int i = last_frame.get_index_u2(Bytecodes::_multianewarray);\n+  Klass* klass = constants->klass_at(i, CHECK);\n@@ -276,0 +325,24 @@\n+JRT_ENTRY(jboolean, InterpreterRuntime::is_substitutable(JavaThread* current, oopDesc* aobj, oopDesc* bobj))\n+  assert(oopDesc::is_oop(aobj) && oopDesc::is_oop(bobj), \"must be valid oops\");\n+\n+  Handle ha(THREAD, aobj);\n+  Handle hb(THREAD, bobj);\n+  JavaValue result(T_BOOLEAN);\n+  JavaCallArguments args;\n+  args.push_oop(ha);\n+  args.push_oop(hb);\n+  methodHandle method(current, UseAltSubstitutabilityMethod ?  Universe::is_substitutableAlt_method() : Universe::is_substitutable_method());\n+  method->method_holder()->initialize(CHECK_false); \/\/ Ensure class ValueObjectMethods is initialized\n+  JavaCalls::call(&result, method, &args, THREAD);\n+  if (HAS_PENDING_EXCEPTION) {\n+    \/\/ Something really bad happened because isSubstitutable() should not throw exceptions\n+    \/\/ If it is an error, just let it propagate\n+    \/\/ If it is an exception, wrap it into an InternalError\n+    if (!PENDING_EXCEPTION->is_a(vmClasses::Error_klass())) {\n+      Handle e(THREAD, PENDING_EXCEPTION);\n+      CLEAR_PENDING_EXCEPTION;\n+      THROW_MSG_CAUSE_(vmSymbols::java_lang_InternalError(), \"Internal error in substitutability test\", e, false);\n+    }\n+  }\n+  return result.get_jboolean();\n+JRT_END\n@@ -623,0 +696,4 @@\n+JRT_ENTRY(void, InterpreterRuntime::throw_InstantiationError(JavaThread* current))\n+  THROW(vmSymbols::java_lang_InstantiationError());\n+JRT_END\n+\n@@ -702,0 +779,1 @@\n+  bool strict_static_final = info.is_strict() && info.is_static() && info.is_final();\n@@ -706,1 +784,14 @@\n-  if (!uninitialized_static || VM_Version::supports_fast_class_init_checks()) {\n+  if (uninitialized_static && (info.is_strict_static_unset() || strict_static_final)) {\n+    \/\/ During <clinit>, closely track the state of strict statics.\n+    \/\/ 1. if we are reading an uninitialized strict static, throw\n+    \/\/ 2. if we are writing one, clear the \"unset\" flag\n+    \/\/\n+    \/\/ Note: If we were handling an attempted write of a null to a\n+    \/\/ null-restricted strict static, we would NOT clear the \"unset\"\n+    \/\/ flag.\n+    assert(klass->is_being_initialized(), \"else should have thrown\");\n+    assert(klass->is_reentrant_initialization(THREAD),\n+      \"<clinit> must be running in current thread\");\n+    klass->notify_strict_static_access(info.index(), is_put, CHECK);\n+    assert(!info.is_strict_static_unset(), \"after initialization, no unset flags\");\n+  } else if (!uninitialized_static || VM_Version::supports_fast_class_init_checks()) {\n@@ -714,1 +805,4 @@\n-  entry->set_flags(info.access_flags().is_final(), info.access_flags().is_volatile());\n+  entry->set_flags(info.access_flags().is_final(), info.access_flags().is_volatile(),\n+                   info.is_flat(), info.is_null_free_inline_type(),\n+                   info.has_null_marker());\n+\n@@ -761,1 +855,0 @@\n-\n@@ -766,1 +859,0 @@\n-\n@@ -781,0 +873,15 @@\n+JRT_ENTRY(void, InterpreterRuntime::throw_identity_exception(JavaThread* current, oopDesc* obj))\n+  Klass* klass = cast_to_oop(obj)->klass();\n+  ResourceMark rm(THREAD);\n+  const char* desc = \"Cannot synchronize on an instance of value class \";\n+  const char* className = klass->external_name();\n+  size_t msglen = strlen(desc) + strlen(className) + 1;\n+  char* message = NEW_RESOURCE_ARRAY(char, msglen);\n+  if (nullptr == message) {\n+    \/\/ Out of memory: can't create detailed error message\n+    THROW_MSG(vmSymbols::java_lang_IdentityException(), className);\n+  } else {\n+    jio_snprintf(message, msglen, \"%s%s\", desc, className);\n+    THROW_MSG(vmSymbols::java_lang_IdentityException(), message);\n+  }\n+JRT_END\n@@ -1194,0 +1301,1 @@\n+  assert(entry->is_valid(), \"Invalid ResolvedFieldEntry\");\n@@ -1201,0 +1309,1 @@\n+  bool is_flat = entry->is_flat();\n@@ -1209,1 +1318,1 @@\n-  jfieldID fid = jfieldIDWorkaround::to_jfieldID(field_holder, entry->field_offset(), is_static);\n+  jfieldID fid = jfieldIDWorkaround::to_jfieldID(field_holder, entry->field_offset(), is_static, is_flat);\n@@ -1217,0 +1326,1 @@\n+  assert(entry->is_valid(), \"Invalid ResolvedFieldEntry\");\n@@ -1238,0 +1348,1 @@\n+\n@@ -1239,0 +1350,1 @@\n+  bool is_flat = entry->is_flat();\n@@ -1241,1 +1353,1 @@\n-  jfieldID fid = jfieldIDWorkaround::to_jfieldID(ik, entry->field_offset(), is_static);\n+  jfieldID fid = jfieldIDWorkaround::to_jfieldID(ik, entry->field_offset(), is_static, is_flat);\n","filename":"src\/hotspot\/share\/interpreter\/interpreterRuntime.cpp","additions":121,"deletions":9,"binary":false,"changes":130,"status":"modified"},{"patch":"@@ -69,0 +69,1 @@\n+#include \"oops\/refArrayKlass.hpp\"\n@@ -116,0 +117,3 @@\n+static LatestMethodCache _is_substitutable_cache;           \/\/ ValueObjectMethods.isSubstitutable()\n+static LatestMethodCache _value_object_hash_code_cache;     \/\/ ValueObjectMethods.valueObjectHashCode()\n+static LatestMethodCache _is_substitutable_alt_cache;       \/\/ ValueObjectMethods.isSubstitutableAlt()\n@@ -466,0 +470,1 @@\n+\n@@ -513,2 +518,6 @@\n-    Klass* oak = vmClasses::Object_klass()->array_klass(CHECK);\n-    _objectArrayKlass = ObjArrayKlass::cast(oak);\n+    ArrayKlass* oak = vmClasses::Object_klass()->array_klass(CHECK);\n+    oak->append_to_sibling_list();\n+\n+    \/\/ Create a RefArrayKlass (which is the default) and initialize.\n+    ObjArrayKlass* rak = ObjArrayKlass::cast(oak)->klass_with_properties(ArrayKlass::ArrayProperties::DEFAULT, THREAD);\n+    _objectArrayKlass = rak;\n@@ -516,7 +525,0 @@\n-  \/\/ OLD\n-  \/\/ Add the class to the class hierarchy manually to make sure that\n-  \/\/ its vtable is initialized after core bootstrapping is completed.\n-  \/\/ ---\n-  \/\/ New\n-  \/\/ Have already been initialized.\n-  _objectArrayKlass->append_to_sibling_list();\n@@ -657,0 +659,3 @@\n+\n+  \/\/ This isn't added to the subclass list, so need to reinitialize vtables directly.\n+  Universe::objectArrayKlass()->vtable().initialize_vtable();\n@@ -902,1 +907,0 @@\n-\n@@ -1086,0 +1090,3 @@\n+Method* Universe::is_substitutable_method()       { return _is_substitutable_cache.get_method(); }\n+Method* Universe::value_object_hash_code_method() { return _value_object_hash_code_cache.get_method(); }\n+Method* Universe::is_substitutableAlt_method()    { return _is_substitutable_alt_cache.get_method(); }\n@@ -1115,0 +1122,15 @@\n+\n+  \/\/ Set up substitutability testing\n+  ResourceMark rm(current);\n+  _is_substitutable_cache.init(current,\n+                          vmClasses::ValueObjectMethods_klass(),\n+                          vmSymbols::isSubstitutable_name()->as_C_string(),\n+                          vmSymbols::object_object_boolean_signature(), true);\n+  _value_object_hash_code_cache.init(current,\n+                          vmClasses::ValueObjectMethods_klass(),\n+                          vmSymbols::valueObjectHashCode_name()->as_C_string(),\n+                          vmSymbols::object_int_signature(), true);\n+  _is_substitutable_alt_cache.init(current,\n+                          vmClasses::ValueObjectMethods_klass(),\n+                          vmSymbols::isSubstitutableAlt_name()->as_C_string(),\n+                          vmSymbols::object_object_boolean_signature(), true);\n","filename":"src\/hotspot\/share\/memory\/universe.cpp","additions":32,"deletions":10,"binary":false,"changes":42,"status":"modified"},{"patch":"@@ -54,0 +54,1 @@\n+#include \"oops\/flatArrayKlass.hpp\"\n@@ -59,0 +60,1 @@\n+#include \"oops\/refArrayOop.hpp\"\n@@ -192,1 +194,1 @@\n-  return resolved_references()->replace_if_null(index, new_result);\n+  return refArrayOopDesc::cast(resolved_references())->replace_if_null(index, new_result);\n@@ -263,1 +265,1 @@\n-      \/\/ All of these should have been reverted back to ClassIndex before calling\n+      \/\/ All of these should have been reverted back to Unresolved before calling\n@@ -479,0 +481,1 @@\n+    assert(src_k->is_instance_klass() || src_k->is_typeArray_klass(), \"Sanity check\");\n@@ -625,0 +628,6 @@\n+void check_is_inline_type(Klass* k, TRAPS) {\n+  if (!k->is_inline_klass()) {\n+    THROW(vmSymbols::java_lang_IncompatibleClassChangeError());\n+  }\n+}\n+\n@@ -662,0 +671,1 @@\n+  bool inline_type_signature = false;\n@@ -670,0 +680,3 @@\n+  if (inline_type_signature) {\n+    name->decrement_refcount();\n+  }\n@@ -678,0 +691,16 @@\n+  if (!HAS_PENDING_EXCEPTION && inline_type_signature) {\n+    check_is_inline_type(k, THREAD);\n+  }\n+\n+  if (!HAS_PENDING_EXCEPTION) {\n+    Klass* bottom_klass = nullptr;\n+    if (k->is_objArray_klass()) {\n+      bottom_klass = ObjArrayKlass::cast(k)->bottom_klass();\n+      assert(bottom_klass != nullptr, \"Should be set\");\n+      assert(bottom_klass->is_instance_klass() || bottom_klass->is_typeArray_klass(), \"Sanity check\");\n+    } else if (k->is_flatArray_klass()) {\n+      bottom_klass = FlatArrayKlass::cast(k)->element_klass();\n+      assert(bottom_klass != nullptr, \"Should be set\");\n+    }\n+  }\n+\n","filename":"src\/hotspot\/share\/oops\/constantPool.cpp","additions":31,"deletions":2,"binary":false,"changes":33,"status":"modified"},{"patch":"@@ -280,14 +280,0 @@\n-static markWord make_prototype(const Klass* kls) {\n-  markWord prototype = markWord::prototype();\n-#ifdef _LP64\n-  if (UseCompactObjectHeaders) {\n-    \/\/ With compact object headers, the narrow Klass ID is part of the mark word.\n-    \/\/ We therefore seed the mark word with the narrow Klass ID.\n-    precond(CompressedKlassPointers::is_encodable(kls));\n-    const narrowKlass nk = CompressedKlassPointers::encode(const_cast<Klass*>(kls));\n-    prototype = prototype.set_narrow_klass(nk);\n-  }\n-#endif\n-  return prototype;\n-}\n-\n@@ -306,2 +292,1 @@\n-Klass::Klass(KlassKind kind) : _kind(kind),\n-                               _prototype_header(make_prototype(this)),\n+Klass::Klass(KlassKind kind, markWord prototype_header) : _kind(kind),\n@@ -309,0 +294,1 @@\n+  set_prototype_header(make_prototype_header(this, prototype_header));\n@@ -321,2 +307,2 @@\n-  int  tag   =  isobj ? _lh_array_tag_obj_value : _lh_array_tag_type_value;\n-  int lh = array_layout_helper(tag, hsize, etype, exact_log2(esize));\n+  int  tag   =  isobj ? _lh_array_tag_ref_value : _lh_array_tag_type_value;\n+  int lh = array_layout_helper(tag, false, hsize, etype, exact_log2(esize));\n@@ -326,1 +312,1 @@\n-  assert(layout_helper_is_objArray(lh) == isobj, \"correct kind\");\n+  assert(layout_helper_is_refArray(lh) == isobj, \"correct kind\");\n@@ -1037,4 +1023,2 @@\n-     if (UseCompactObjectHeaders) {\n-       st->print(BULLET\"prototype_header: \" INTPTR_FORMAT, _prototype_header.value());\n-       st->cr();\n-     }\n+     st->print(BULLET\"prototype_header: \" INTPTR_FORMAT, _prototype_header.value());\n+     st->cr();\n","filename":"src\/hotspot\/share\/oops\/klass.cpp","additions":7,"deletions":23,"binary":false,"changes":30,"status":"modified"},{"patch":"@@ -31,0 +31,1 @@\n+#include \"oops\/layoutKind.hpp\"\n@@ -33,0 +34,1 @@\n+#include \"utilities\/vmEnums.hpp\"\n@@ -44,1 +46,1 @@\n-\/\/  unused:22 hash:31 -->| unused_gap:4  age:4  self-fwd:1  lock:2 (normal object)\n+\/\/  unused:22 hash:31 -->| valhalla:4  age:4  self-fwd:1  lock:2 (normal object)\n@@ -48,1 +50,1 @@\n-\/\/  klass:22  hash:31 -->| unused_gap:4  age:4  self-fwd:1  lock:2 (normal object)\n+\/\/  klass:22  hash:31 -->| valhalla:4  age:4  self-fwd:1  lock:2 (normal object)\n@@ -67,0 +69,1 @@\n+\/\/\n@@ -70,0 +73,18 @@\n+\/\/\n+\/\/  VALHALLA EXTENSIONS:\n+\/\/\n+\/\/  N.B.: 32 bit mode is not supported, this section assumes 64 bit systems.\n+\/\/\n+\/\/  Project Valhalla uses markWord bits to denote the following oops (listed least to most significant):\n+\/\/  * inline types: have alternative bytecode behavior, e.g. can not be locked\n+\/\/  * flat arrays: load\/decode of klass layout helper is expensive for aaload\n+\/\/  * \"null free\" arrays: load\/decode of klass layout helper again for aaload\n+\/\/  * inline type: \"larval state\": mutable state, but only during object init, observable\n+\/\/      by only by a single thread (generally do not mutate markWord)\n+\/\/\n+\/\/  Inline types cannot be locked, monitored or inflating.\n+\/\/\n+\/\/  Note the position of 'self-fwd' is not by accident. When forwarding an\n+\/\/  object to a new heap position, HeapWord alignment guarantees the lower\n+\/\/  bits, including 'self-fwd' are 0. \"is_self_forwarded()\" will be correctly\n+\/\/  set to false. Otherwise encode_pointer_as_mark() may have 'self-fwd' set.\n@@ -108,2 +129,1 @@\n-  \/\/ Constants\n-  static const int age_bits                       = 4;\n+  \/\/ Constants, in least significant bit order\n@@ -112,1 +132,8 @@\n-  static const int max_hash_bits                  = BitsPerWord - age_bits - lock_bits - self_fwd_bits;\n+  \/\/ instance state\n+  static const int age_bits                       = 4;\n+  \/\/ EnableValhalla: static prototype header bits (fast path instead of klass layout_helper)\n+  static const int inline_type_bits               = 1;\n+  static const int null_free_array_bits           = LP64_ONLY(1) NOT_LP64(0);\n+  static const int flat_array_bits                = LP64_ONLY(1) NOT_LP64(0);\n+  static const int larval_bits                    = 1;\n+  static const int max_hash_bits                  = BitsPerWord - age_bits - lock_bits - inline_type_bits - larval_bits - flat_array_bits - null_free_array_bits - self_fwd_bits;\n@@ -114,1 +141,0 @@\n-  static const int unused_gap_bits                = LP64_ONLY(4) NOT_LP64(0); \/\/ Reserved for Valhalla.\n@@ -119,1 +145,5 @@\n-  static const int hash_shift                     = age_shift + age_bits + unused_gap_bits;\n+  static const int inline_type_shift              = age_shift + age_bits;\n+  static const int null_free_array_shift          = inline_type_shift + inline_type_bits;\n+  static const int flat_array_shift               = null_free_array_shift + null_free_array_bits;\n+  static const int larval_shift                   = flat_array_shift + flat_array_bits;\n+  static const int hash_shift                     = larval_shift + larval_bits;\n@@ -125,0 +155,8 @@\n+  static const uintptr_t inline_type_bit_in_place = right_n_bits(inline_type_bits) << inline_type_shift;\n+  static const uintptr_t inline_type_mask_in_place = inline_type_bit_in_place + lock_mask;\n+  static const uintptr_t null_free_array_mask     = right_n_bits(null_free_array_bits);\n+  static const uintptr_t null_free_array_mask_in_place = (null_free_array_mask << null_free_array_shift) | lock_mask_in_place;\n+  static const uintptr_t null_free_array_bit_in_place  = (right_n_bits(null_free_array_bits) << null_free_array_shift);\n+  static const uintptr_t flat_array_mask          = right_n_bits(flat_array_bits);\n+  static const uintptr_t flat_array_mask_in_place = (flat_array_mask << flat_array_shift) | null_free_array_mask_in_place | lock_mask_in_place;\n+  static const uintptr_t flat_array_bit_in_place  = right_n_bits(flat_array_bits) << flat_array_shift;\n@@ -127,0 +165,5 @@\n+\n+  static const uintptr_t larval_mask              = right_n_bits(larval_bits);\n+  static const uintptr_t larval_mask_in_place     = (larval_mask << larval_shift) | inline_type_mask_in_place;\n+  static const uintptr_t larval_bit_in_place      = right_n_bits(larval_bits) << larval_shift;\n+\n@@ -149,0 +192,7 @@\n+  static const uintptr_t inline_type_pattern      = inline_type_bit_in_place | unlocked_value;\n+  static const uintptr_t null_free_array_pattern  = null_free_array_bit_in_place | unlocked_value;\n+  static const uintptr_t null_free_flat_array_pattern = flat_array_bit_in_place | null_free_array_pattern;\n+  static const uintptr_t nullable_flat_array_pattern = flat_array_bit_in_place | unlocked_value;\n+\n+  static const uintptr_t larval_pattern           = larval_bit_in_place | inline_type_pattern;\n+\n@@ -158,0 +208,4 @@\n+  bool is_inline_type() const {\n+    return (mask_bits(value(), inline_type_mask_in_place) == inline_type_pattern);\n+  }\n+\n@@ -168,0 +222,7 @@\n+\n+  \/\/ is unlocked and not an inline type (which cannot be involved in locking, displacement or inflation)\n+  \/\/ i.e. test both lock bits and the inline type bit together\n+  bool is_neutral()  const {  \/\/ Not locked, or marked - a \"clean\" neutral state\n+    return (mask_bits(value(), inline_type_mask_in_place) == unlocked_value);\n+  }\n+\n@@ -172,3 +233,0 @@\n-  bool is_neutral()  const {  \/\/ Not locked, or marked - a \"clean\" neutral state\n-    return (mask_bits(value(), lock_mask_in_place) == unlocked_value);\n-  }\n@@ -191,1 +249,2 @@\n-    return (!is_unlocked() || !has_no_hash());\n+    return (!is_unlocked() || !has_no_hash() ||\n+      (EnableValhalla && (is_larval_state() || is_inline_type() || is_flat_array() || is_null_free_array())));\n@@ -272,0 +331,28 @@\n+  \/\/ private buffered value operations\n+  markWord enter_larval_state() const {\n+    return markWord(value() | larval_bit_in_place);\n+  }\n+  markWord exit_larval_state() const {\n+    return markWord(value() & ~larval_bit_in_place);\n+  }\n+  bool is_larval_state() const {\n+    return (mask_bits(value(), larval_mask_in_place) == larval_pattern);\n+  }\n+\n+  bool is_flat_array() const {\n+#ifdef _LP64 \/\/ 64 bit encodings only\n+    return (mask_bits(value(), flat_array_mask_in_place) == null_free_flat_array_pattern)\n+           || (mask_bits(value(), flat_array_mask_in_place) == nullable_flat_array_pattern);\n+#else\n+    return false;\n+#endif\n+  }\n+\n+  bool is_null_free_array() const {\n+#ifdef _LP64 \/\/ 64 bit encodings only\n+    return (mask_bits(value(), null_free_array_mask_in_place) == null_free_array_pattern);\n+#else\n+    return false;\n+#endif\n+  }\n+\n@@ -283,0 +370,12 @@\n+  static markWord inline_type_prototype() {\n+    return markWord(inline_type_pattern);\n+  }\n+\n+#ifdef _LP64 \/\/ 64 bit encodings only\n+  static markWord flat_array_prototype(LayoutKind lk);\n+\n+  static markWord null_free_array_prototype() {\n+    return markWord(null_free_array_pattern);\n+  }\n+#endif\n+\n@@ -289,2 +388,3 @@\n-  \/\/ Recover address of oop from encoded form used in mark\n-  inline void* decode_pointer() const { return (void*)clear_lock_bits().value(); }\n+  inline void* decode_pointer() const {\n+    return (void*) (clear_lock_bits().value());\n+  }\n","filename":"src\/hotspot\/share\/oops\/markWord.hpp","additions":113,"deletions":13,"binary":false,"changes":126,"status":"modified"},{"patch":"@@ -25,0 +25,1 @@\n+#include \"cds\/cdsConfig.hpp\"\n@@ -34,0 +35,1 @@\n+#include \"memory\/oopFactory.hpp\"\n@@ -37,0 +39,1 @@\n+#include \"oops\/flatArrayKlass.hpp\"\n@@ -39,0 +42,1 @@\n+#include \"oops\/markWord.hpp\"\n@@ -42,0 +46,1 @@\n+#include \"oops\/refArrayKlass.hpp\"\n@@ -43,0 +48,1 @@\n+#include \"runtime\/arguments.hpp\"\n@@ -47,1 +53,3 @@\n-ObjArrayKlass* ObjArrayKlass::allocate_klass(ClassLoaderData* loader_data, int n, Klass* k, Symbol* name, TRAPS) {\n+ObjArrayKlass* ObjArrayKlass::allocate_klass(ClassLoaderData* loader_data, int n,\n+                                       Klass* k, Symbol* name, ArrayKlass::ArrayProperties props,\n+                                       TRAPS) {\n@@ -53,1 +61,1 @@\n-  return new (loader_data, size, THREAD) ObjArrayKlass(n, k, name);\n+  return new (loader_data, size, THREAD) ObjArrayKlass(n, k, name, Kind, props, ArrayKlass::is_null_restricted(props) ? markWord::null_free_array_prototype() : markWord::prototype());\n@@ -77,1 +85,1 @@\n-                                                      int n, Klass* element_klass, TRAPS) {\n+                                                      int n, Klass* element_klass,  TRAPS) {\n@@ -105,1 +113,1 @@\n-  ObjArrayKlass* oak = ObjArrayKlass::allocate_klass(loader_data, n, element_klass, name, CHECK_NULL);\n+  ObjArrayKlass* oak = ObjArrayKlass::allocate_klass(loader_data, n, element_klass, name, ArrayProperties::INVALID, CHECK_NULL);\n@@ -123,1 +131,2 @@\n-ObjArrayKlass::ObjArrayKlass(int n, Klass* element_klass, Symbol* name) : ArrayKlass(name, Kind) {\n+ObjArrayKlass::ObjArrayKlass(int n, Klass* element_klass, Symbol* name, KlassKind kind, ArrayKlass::ArrayProperties props, markWord mk) :\n+ArrayKlass(name, kind, props, mk) {\n@@ -126,0 +135,2 @@\n+  set_next_refined_klass_klass(nullptr);\n+  set_properties(props);\n@@ -131,0 +142,1 @@\n+    assert(!element_klass->is_refArray_klass(), \"Sanity\");\n@@ -141,1 +153,9 @@\n-  set_layout_helper(array_layout_helper(T_OBJECT));\n+  int lh = array_layout_helper(T_OBJECT);\n+  if (ArrayKlass::is_null_restricted(props)) {\n+    assert(n == 1, \"Bytecode does not support null-free multi-dim\");\n+    lh = layout_helper_set_null_free(lh);\n+#ifdef _LP64\n+    assert(prototype_header().is_null_free_array(), \"sanity\");\n+#endif\n+  }\n+  set_layout_helper(lh);\n@@ -151,1 +171,57 @@\n-  return objArrayOop(obj)->object_size();\n+  \/\/ return objArrayOop(obj)->object_size();\n+  return obj->is_flatArray() ? flatArrayOop(obj)->object_size(layout_helper()) : refArrayOop(obj)->object_size();\n+}\n+\n+ArrayDescription ObjArrayKlass::array_layout_selection(Klass* element, ArrayProperties properties) {\n+  \/\/ TODO FIXME: the layout selection should take the array size in consideration\n+  \/\/ to avoid creation of arrays too big to be handled by the VM. See JDK-8233189\n+  if (!UseArrayFlattening || element->is_array_klass() || element->is_identity_class() || element->is_abstract()) {\n+    return ArrayDescription(RefArrayKlassKind, properties, LayoutKind::REFERENCE);\n+  }\n+  assert(element->is_final(), \"Flat layouts below require monomorphic elements\");\n+  InlineKlass* vk = InlineKlass::cast(element);\n+  if (is_null_restricted(properties)) {\n+    if (is_non_atomic(properties)) {\n+      \/\/ Null-restricted + non-atomic\n+      if (vk->maybe_flat_in_array() && vk->has_non_atomic_layout()) {\n+        return ArrayDescription(FlatArrayKlassKind, properties, LayoutKind::NON_ATOMIC_FLAT);\n+      } else {\n+        return ArrayDescription(RefArrayKlassKind, properties, LayoutKind::REFERENCE);\n+      }\n+    } else {\n+      \/\/ Null-restricted + atomic\n+      if (vk->maybe_flat_in_array() && vk->is_naturally_atomic() && vk->has_non_atomic_layout()) {\n+        return ArrayDescription(FlatArrayKlassKind, properties, LayoutKind::NON_ATOMIC_FLAT);\n+      } else if (vk->maybe_flat_in_array() && vk->has_atomic_layout()) {\n+        return ArrayDescription(FlatArrayKlassKind, properties, LayoutKind::ATOMIC_FLAT);\n+      } else {\n+        return ArrayDescription(RefArrayKlassKind, properties, LayoutKind::REFERENCE);\n+      }\n+    }\n+  } else {\n+    \/\/ nullable implies atomic, so the non-atomic property is ignored\n+    if (vk->maybe_flat_in_array() && vk->has_nullable_atomic_layout()) {\n+      return ArrayDescription(FlatArrayKlassKind, properties, LayoutKind::NULLABLE_ATOMIC_FLAT);\n+    } else {\n+      return ArrayDescription(RefArrayKlassKind, properties, LayoutKind::REFERENCE);\n+    }\n+  }\n+}\n+\n+ObjArrayKlass* ObjArrayKlass::allocate_klass_with_properties(ArrayKlass::ArrayProperties props, TRAPS) {\n+  ObjArrayKlass* ak = nullptr;\n+  ArrayDescription ad = ObjArrayKlass::array_layout_selection(element_klass(), props);\n+  switch (ad._kind) {\n+    case Klass::RefArrayKlassKind: {\n+      ak = RefArrayKlass::allocate_refArray_klass(class_loader_data(), dimension(), element_klass(), props, CHECK_NULL);\n+      break;\n+    }\n+    case Klass::FlatArrayKlassKind: {\n+      assert(dimension() == 1, \"Flat arrays can only be dimension 1 arrays\");\n+      ak = FlatArrayKlass::allocate_klass(element_klass(), props, ad._layout_kind, CHECK_NULL);\n+      break;\n+    }\n+    default:\n+      ShouldNotReachHere();\n+  }\n+  return ak;\n@@ -154,1 +230,1 @@\n-objArrayOop ObjArrayKlass::allocate_instance(int length, TRAPS) {\n+objArrayOop ObjArrayKlass::allocate_instance(int length, ArrayProperties props, TRAPS) {\n@@ -156,3 +232,18 @@\n-  size_t size = objArrayOopDesc::object_size(length);\n-  return (objArrayOop)Universe::heap()->array_allocate(this, size, length,\n-                                                       \/* do_zero *\/ true, THREAD);\n+  ObjArrayKlass* ak = klass_with_properties(props, THREAD);\n+  size_t size = 0;\n+  switch(ak->kind()) {\n+    case Klass::RefArrayKlassKind:\n+      size = refArrayOopDesc::object_size(length);\n+      break;\n+    case Klass::FlatArrayKlassKind:\n+      size = flatArrayOopDesc::object_size(ak->layout_helper(), length);\n+      break;\n+    default:\n+      ShouldNotReachHere();\n+  }\n+  assert(size != 0, \"Sanity check\");\n+  objArrayOop array = (objArrayOop)Universe::heap()->array_allocate(\n+    ak, size, length,\n+    \/* do_zero *\/ true, CHECK_NULL);\n+  assert(array->is_refArray() || array->is_flatArray(), \"Must be\");\n+  return array;\n@@ -165,1 +256,3 @@\n-  objArrayOop array = allocate_instance(length, CHECK_NULL);\n+  ObjArrayKlass* oak = klass_with_properties(ArrayProperties::DEFAULT, CHECK_NULL);\n+  assert(oak->is_refArray_klass() || oak->is_flatArray_klass(), \"Must be\");\n+  objArrayOop array = oak->allocate_instance(length, ArrayProperties::DEFAULT, CHECK_NULL);\n@@ -170,1 +263,1 @@\n-        oop sub_array = ld_klass->multi_allocate(rank - 1, &sizes[1], CHECK_NULL);\n+        oop sub_array = ld_klass->multi_allocate(rank-1, &sizes[1], CHECK_NULL);\n@@ -188,36 +281,0 @@\n-\/\/ Either oop or narrowOop depending on UseCompressedOops.\n-void ObjArrayKlass::do_copy(arrayOop s, size_t src_offset,\n-                            arrayOop d, size_t dst_offset, int length, TRAPS) {\n-  if (s == d) {\n-    \/\/ since source and destination are equal we do not need conversion checks.\n-    assert(length > 0, \"sanity check\");\n-    ArrayAccess<>::oop_arraycopy(s, src_offset, d, dst_offset, length);\n-  } else {\n-    \/\/ We have to make sure all elements conform to the destination array\n-    Klass* bound = ObjArrayKlass::cast(d->klass())->element_klass();\n-    Klass* stype = ObjArrayKlass::cast(s->klass())->element_klass();\n-    if (stype == bound || stype->is_subtype_of(bound)) {\n-      \/\/ elements are guaranteed to be subtypes, so no check necessary\n-      ArrayAccess<ARRAYCOPY_DISJOINT>::oop_arraycopy(s, src_offset, d, dst_offset, length);\n-    } else {\n-      \/\/ slow case: need individual subtype checks\n-      \/\/ note: don't use obj_at_put below because it includes a redundant store check\n-      if (!ArrayAccess<ARRAYCOPY_DISJOINT | ARRAYCOPY_CHECKCAST>::oop_arraycopy(s, src_offset, d, dst_offset, length)) {\n-        ResourceMark rm(THREAD);\n-        stringStream ss;\n-        if (!bound->is_subtype_of(stype)) {\n-          ss.print(\"arraycopy: type mismatch: can not copy %s[] into %s[]\",\n-                   stype->external_name(), bound->external_name());\n-        } else {\n-          \/\/ oop_arraycopy should return the index in the source array that\n-          \/\/ contains the problematic oop.\n-          ss.print(\"arraycopy: element type mismatch: can not cast one of the elements\"\n-                   \" of %s[] to the type of the destination array, %s\",\n-                   stype->external_name(), bound->external_name());\n-        }\n-        THROW_MSG(vmSymbols::java_lang_ArrayStoreException(), ss.as_string());\n-      }\n-    }\n-  }\n-}\n-\n@@ -228,25 +285,4 @@\n-  if (!d->is_objArray()) {\n-    ResourceMark rm(THREAD);\n-    stringStream ss;\n-    if (d->is_typeArray()) {\n-      ss.print(\"arraycopy: type mismatch: can not copy object array[] into %s[]\",\n-               type2name_tab[ArrayKlass::cast(d->klass())->element_type()]);\n-    } else {\n-      ss.print(\"arraycopy: destination type %s is not an array\", d->klass()->external_name());\n-    }\n-    THROW_MSG(vmSymbols::java_lang_ArrayStoreException(), ss.as_string());\n-  }\n-\n-  \/\/ Check is all offsets and lengths are non negative\n-  if (src_pos < 0 || dst_pos < 0 || length < 0) {\n-    \/\/ Pass specific exception reason.\n-    ResourceMark rm(THREAD);\n-    stringStream ss;\n-    if (src_pos < 0) {\n-      ss.print(\"arraycopy: source index %d out of bounds for object array[%d]\",\n-               src_pos, s->length());\n-    } else if (dst_pos < 0) {\n-      ss.print(\"arraycopy: destination index %d out of bounds for object array[%d]\",\n-               dst_pos, d->length());\n-    } else {\n-      ss.print(\"arraycopy: length %d is negative\", length);\n+  if (UseArrayFlattening) {\n+    if (d->is_flatArray()) {\n+      FlatArrayKlass::cast(d->klass())->copy_array(s, src_pos, d, dst_pos, length, THREAD);\n+      return;\n@@ -254,14 +290,3 @@\n-    THROW_MSG(vmSymbols::java_lang_ArrayIndexOutOfBoundsException(), ss.as_string());\n-  }\n-  \/\/ Check if the ranges are valid\n-  if ((((unsigned int) length + (unsigned int) src_pos) > (unsigned int) s->length()) ||\n-      (((unsigned int) length + (unsigned int) dst_pos) > (unsigned int) d->length())) {\n-    \/\/ Pass specific exception reason.\n-    ResourceMark rm(THREAD);\n-    stringStream ss;\n-    if (((unsigned int) length + (unsigned int) src_pos) > (unsigned int) s->length()) {\n-      ss.print(\"arraycopy: last source index %u out of bounds for object array[%d]\",\n-               (unsigned int) length + (unsigned int) src_pos, s->length());\n-    } else {\n-      ss.print(\"arraycopy: last destination index %u out of bounds for object array[%d]\",\n-               (unsigned int) length + (unsigned int) dst_pos, d->length());\n+    if (s->is_flatArray()) {\n+      FlatArrayKlass::cast(s->klass())->copy_array(s, src_pos, d, dst_pos, length, THREAD);\n+      return;\n@@ -269,1 +294,0 @@\n-    THROW_MSG(vmSymbols::java_lang_ArrayIndexOutOfBoundsException(), ss.as_string());\n@@ -272,24 +296,2 @@\n-  \/\/ Special case. Boundary cases must be checked first\n-  \/\/ This allows the following call: copy_array(s, s.length(), d.length(), 0).\n-  \/\/ This is correct, since the position is supposed to be an 'in between point', i.e., s.length(),\n-  \/\/ points to the right of the last element.\n-  if (length==0) {\n-    return;\n-  }\n-  if (UseCompressedOops) {\n-    size_t src_offset = (size_t) objArrayOopDesc::obj_at_offset<narrowOop>(src_pos);\n-    size_t dst_offset = (size_t) objArrayOopDesc::obj_at_offset<narrowOop>(dst_pos);\n-    assert(arrayOopDesc::obj_offset_to_raw<narrowOop>(s, src_offset, nullptr) ==\n-           objArrayOop(s)->obj_at_addr<narrowOop>(src_pos), \"sanity\");\n-    assert(arrayOopDesc::obj_offset_to_raw<narrowOop>(d, dst_offset, nullptr) ==\n-           objArrayOop(d)->obj_at_addr<narrowOop>(dst_pos), \"sanity\");\n-    do_copy(s, src_offset, d, dst_offset, length, CHECK);\n-  } else {\n-    size_t src_offset = (size_t) objArrayOopDesc::obj_at_offset<oop>(src_pos);\n-    size_t dst_offset = (size_t) objArrayOopDesc::obj_at_offset<oop>(dst_pos);\n-    assert(arrayOopDesc::obj_offset_to_raw<oop>(s, src_offset, nullptr) ==\n-           objArrayOop(s)->obj_at_addr<oop>(src_pos), \"sanity\");\n-    assert(arrayOopDesc::obj_offset_to_raw<oop>(d, dst_offset, nullptr) ==\n-           objArrayOop(d)->obj_at_addr<oop>(dst_pos), \"sanity\");\n-    do_copy(s, src_offset, d, dst_offset, length, CHECK);\n-  }\n+  assert(s->is_refArray() && d->is_refArray(), \"Must be\");\n+  RefArrayKlass::cast(s->klass())->copy_array(s, src_pos, d, dst_pos, length, THREAD);\n@@ -340,0 +342,3 @@\n+  if (_next_refined_array_klass != nullptr && !CDSConfig::is_dumping_dynamic_archive()) {\n+    it->push(&_next_refined_array_klass);\n+  }\n@@ -342,0 +347,25 @@\n+#if INCLUDE_CDS\n+void ObjArrayKlass::restore_unshareable_info(ClassLoaderData* loader_data, Handle protection_domain, TRAPS) {\n+  ArrayKlass::restore_unshareable_info(loader_data, protection_domain, CHECK);\n+  if (_next_refined_array_klass != nullptr) {\n+    _next_refined_array_klass->restore_unshareable_info(loader_data, protection_domain, CHECK);\n+  }\n+}\n+\n+void ObjArrayKlass::remove_unshareable_info() {\n+  ArrayKlass::remove_unshareable_info();\n+  if (_next_refined_array_klass != nullptr && !CDSConfig::is_dumping_dynamic_archive()) {\n+    _next_refined_array_klass->remove_unshareable_info();\n+  } else {\n+    _next_refined_array_klass = nullptr;\n+  }\n+}\n+\n+void ObjArrayKlass::remove_java_mirror() {\n+  ArrayKlass::remove_java_mirror();\n+  if (_next_refined_array_klass != nullptr && !CDSConfig::is_dumping_dynamic_archive()) {\n+    _next_refined_array_klass->remove_java_mirror();\n+  }\n+}\n+#endif \/\/ INCLUDE_CDS\n+\n@@ -349,0 +379,2 @@\n+  int identity_flag = (Arguments::enable_preview()) ? JVM_ACC_IDENTITY : 0;\n+\n@@ -350,1 +382,1 @@\n-                        | (JVM_ACC_ABSTRACT | JVM_ACC_FINAL);\n+                        | (identity_flag | JVM_ACC_ABSTRACT | JVM_ACC_FINAL);\n@@ -364,0 +396,33 @@\n+ObjArrayKlass* ObjArrayKlass::klass_with_properties(ArrayKlass::ArrayProperties props, TRAPS) {\n+  assert(props != ArrayProperties::INVALID, \"Sanity check\");\n+\n+  if (properties() == props) {\n+    assert(is_refArray_klass() || is_flatArray_klass(), \"Must be a concrete array klass\");\n+    return this;\n+  }\n+\n+  ObjArrayKlass* ak = next_refined_array_klass_acquire();\n+  if (ak == nullptr) {\n+    \/\/ Ensure atomic creation of refined array klasses\n+    RecursiveLocker rl(MultiArray_lock, THREAD);\n+\n+    if (next_refined_array_klass() == nullptr) {\n+      ObjArrayKlass* first = this;\n+      if (!is_refArray_klass() && !is_flatArray_klass() && props != ArrayKlass::ArrayProperties::DEFAULT) {\n+        \/\/ Make sure that the first entry in the linked list is always the default refined klass because\n+        \/\/ C2 relies on this for a fast lookup (see LibraryCallKit::load_default_refined_array_klass).\n+        first = allocate_klass_with_properties(ArrayKlass::ArrayProperties::DEFAULT, THREAD);\n+        release_set_next_refined_klass(first);\n+      }\n+      ak = allocate_klass_with_properties(props, THREAD);\n+      first->release_set_next_refined_klass(ak);\n+    }\n+  }\n+\n+  ak = next_refined_array_klass();\n+  assert(ak != nullptr, \"should be set\");\n+  THREAD->check_possible_safepoint();\n+  return ak->klass_with_properties(props, THREAD); \/\/ why not CHECK_NULL ?\n+}\n+\n+\n@@ -369,1 +434,1 @@\n-  st->print(\" - instance klass: \");\n+  st->print(\" - element klass: \");\n@@ -385,17 +450,1 @@\n-  ArrayKlass::oop_print_on(obj, st);\n-  assert(obj->is_objArray(), \"must be objArray\");\n-  objArrayOop oa = objArrayOop(obj);\n-  int print_len = MIN2(oa->length(), MaxElementPrintSize);\n-  for(int index = 0; index < print_len; index++) {\n-    st->print(\" - %3d : \", index);\n-    if (oa->obj_at(index) != nullptr) {\n-      oa->obj_at(index)->print_value_on(st);\n-      st->cr();\n-    } else {\n-      st->print_cr(\"null\");\n-    }\n-  }\n-  int remaining = oa->length() - print_len;\n-  if (remaining > 0) {\n-    st->print_cr(\" - <%d more elements, increase MaxElementPrintSize to print>\", remaining);\n-  }\n+  ShouldNotReachHere();\n@@ -407,10 +456,1 @@\n-  assert(obj->is_objArray(), \"must be objArray\");\n-  st->print(\"a \");\n-  element_klass()->print_value_on(st);\n-  int len = objArrayOop(obj)->length();\n-  st->print(\"[%d] \", len);\n-  if (obj != nullptr) {\n-    obj->print_address_on(st);\n-  } else {\n-    st->print_cr(\"null\");\n-  }\n+  ShouldNotReachHere();\n@@ -431,1 +471,2 @@\n-  guarantee(bk->is_instance_klass() || bk->is_typeArray_klass(),  \"invalid bottom klass\");\n+  guarantee(bk->is_instance_klass() || bk->is_typeArray_klass() || bk->is_flatArray_klass(),\n+            \"invalid bottom klass\");\n@@ -437,0 +478,1 @@\n+  guarantee(obj->is_null_free_array() || (!is_null_free_array_klass()), \"null-free klass but not object\");\n","filename":"src\/hotspot\/share\/oops\/objArrayKlass.cpp","additions":185,"deletions":143,"binary":false,"changes":328,"status":"modified"},{"patch":"@@ -61,0 +61,2 @@\n+  inline static objArrayOop cast(oop o);\n+\n@@ -66,0 +68,1 @@\n+  oop obj_at(int index, TRAPS) const;\n@@ -68,14 +71,1 @@\n-\n-  oop replace_if_null(int index, oop exchange_value);\n-\n-  \/\/ Sizing\n-  size_t object_size()        { return object_size(length()); }\n-\n-  static size_t object_size(int length) {\n-    \/\/ This returns the object size in HeapWords.\n-    size_t asz = (size_t)length * heapOopSize;\n-    size_t size_words = heap_word_size(base_offset_in_bytes() + asz);\n-    size_t osz = align_object_size(size_words);\n-    assert(osz < max_jint, \"no overflow\");\n-    return osz;\n-  }\n+  void obj_at_put(int index, oop value, TRAPS);\n","filename":"src\/hotspot\/share\/oops\/objArrayOop.hpp","additions":4,"deletions":14,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -145,6 +145,9 @@\n-bool oopDesc::is_instance_noinline()    const { return is_instance();    }\n-bool oopDesc::is_instanceRef_noinline() const { return is_instanceRef(); }\n-bool oopDesc::is_stackChunk_noinline()  const { return is_stackChunk();  }\n-bool oopDesc::is_array_noinline()       const { return is_array();       }\n-bool oopDesc::is_objArray_noinline()    const { return is_objArray();    }\n-bool oopDesc::is_typeArray_noinline()   const { return is_typeArray();   }\n+bool oopDesc::is_instance_noinline()        const { return is_instance();         }\n+bool oopDesc::is_instanceRef_noinline()     const { return is_instanceRef();      }\n+bool oopDesc::is_stackChunk_noinline()      const { return is_stackChunk();       }\n+bool oopDesc::is_array_noinline()           const { return is_array();            }\n+bool oopDesc::is_objArray_noinline()        const { return is_objArray();         }\n+bool oopDesc::is_refArray_noinline()        const { return is_refArray();         }\n+bool oopDesc::is_typeArray_noinline()       const { return is_typeArray();        }\n+bool oopDesc::is_flatArray_noinline()       const { return is_flatArray();        }\n+bool oopDesc::is_null_free_array_noinline() const { return is_null_free_array();  }\n","filename":"src\/hotspot\/share\/oops\/oop.cpp","additions":9,"deletions":6,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -0,0 +1,375 @@\n+\/*\n+ * Copyright (c) 2025, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"classfile\/moduleEntry.hpp\"\n+#include \"classfile\/packageEntry.hpp\"\n+#include \"classfile\/symbolTable.hpp\"\n+#include \"classfile\/vmClasses.hpp\"\n+#include \"classfile\/vmSymbols.hpp\"\n+#include \"gc\/shared\/collectedHeap.inline.hpp\"\n+#include \"memory\/iterator.inline.hpp\"\n+#include \"memory\/metadataFactory.hpp\"\n+#include \"memory\/metaspaceClosure.hpp\"\n+#include \"memory\/oopFactory.hpp\"\n+#include \"memory\/resourceArea.hpp\"\n+#include \"memory\/universe.hpp\"\n+#include \"oops\/arrayKlass.hpp\"\n+#include \"oops\/instanceKlass.hpp\"\n+#include \"oops\/klass.inline.hpp\"\n+#include \"oops\/markWord.hpp\"\n+#include \"oops\/oop.inline.hpp\"\n+#include \"oops\/refArrayKlass.inline.hpp\"\n+#include \"oops\/refArrayOop.inline.hpp\"\n+#include \"oops\/symbol.hpp\"\n+#include \"runtime\/handles.inline.hpp\"\n+#include \"runtime\/mutexLocker.hpp\"\n+#include \"utilities\/macros.hpp\"\n+\n+RefArrayKlass *RefArrayKlass::allocate_klass(ClassLoaderData* loader_data, int n,\n+                                       Klass* k, Symbol *name, ArrayKlass::ArrayProperties props,\n+                                       TRAPS) {\n+  assert(RefArrayKlass::header_size() <= InstanceKlass::header_size(),\n+         \"array klasses must be same size as InstanceKlass\");\n+\n+  int size = ArrayKlass::static_size(RefArrayKlass::header_size());\n+\n+  return new (loader_data, size, THREAD) RefArrayKlass(n, k, name, props);\n+}\n+\n+RefArrayKlass* RefArrayKlass::allocate_refArray_klass(ClassLoaderData* loader_data, int n,\n+                                       Klass* element_klass, ArrayKlass::ArrayProperties props,\n+                                       TRAPS) {\n+  assert(!ArrayKlass::is_null_restricted(props) || (n == 1 && element_klass->is_inline_klass()),\n+         \"null-free unsupported\");\n+\n+  \/\/ Eagerly allocate the direct array supertype.\n+  Klass* super_klass = nullptr;\n+  if (!Universe::is_bootstrapping() || vmClasses::Object_klass_is_loaded()) {\n+    assert(MultiArray_lock->holds_lock(THREAD),\n+           \"must hold lock after bootstrapping\");\n+    Klass* element_super = element_klass->super();\n+    super_klass = element_klass->array_klass(CHECK_NULL);\n+  }\n+\n+  \/\/ Create type name for klass.\n+  Symbol* name = ArrayKlass::create_element_klass_array_name(element_klass, CHECK_NULL);\n+\n+  \/\/ Initialize instance variables\n+  RefArrayKlass* oak = RefArrayKlass::allocate_klass(loader_data, n, element_klass,\n+                                               name, props, CHECK_NULL);\n+\n+  ModuleEntry* module = oak->module();\n+  assert(module != nullptr, \"No module entry for array\");\n+\n+  \/\/ Call complete_create_array_klass after all instance variables has been\n+  \/\/ initialized.\n+  ArrayKlass::complete_create_array_klass(oak, super_klass, module, CHECK_NULL);\n+\n+  \/\/ Add all classes to our internal class loader list here,\n+  \/\/ including classes in the bootstrap (null) class loader.\n+  \/\/ Do this step after creating the mirror so that if the\n+  \/\/ mirror creation fails, loaded_classes_do() doesn't find\n+  \/\/ an array class without a mirror.\n+  loader_data->add_class(oak);\n+\n+  return oak;\n+}\n+\n+RefArrayKlass::RefArrayKlass(int n, Klass* element_klass, Symbol* name,\n+                             ArrayKlass::ArrayProperties props)\n+    : ObjArrayKlass(n, element_klass, name, Kind, props,\n+                    ArrayKlass::is_null_restricted(props) ? markWord::null_free_array_prototype() : markWord::prototype()) {\n+  set_dimension(n);\n+  set_element_klass(element_klass);\n+\n+  Klass* bk;\n+  if (element_klass->is_objArray_klass()) {\n+    bk = ObjArrayKlass::cast(element_klass)->bottom_klass();\n+  } else {\n+    bk = element_klass;\n+  }\n+  assert(bk != nullptr && (bk->is_instance_klass() || bk->is_typeArray_klass()),\n+         \"invalid bottom klass\");\n+  set_bottom_klass(bk);\n+  set_class_loader_data(bk->class_loader_data());\n+\n+  if (element_klass->is_array_klass()) {\n+    set_lower_dimension(ArrayKlass::cast(element_klass));\n+  }\n+\n+  int lh = array_layout_helper(T_OBJECT);\n+  if (ArrayKlass::is_null_restricted(props)) {\n+    assert(n == 1, \"Bytecode does not support null-free multi-dim\");\n+    lh = layout_helper_set_null_free(lh);\n+#ifdef _LP64\n+    assert(prototype_header().is_null_free_array(), \"sanity\");\n+#endif\n+  }\n+  set_layout_helper(lh);\n+  assert(is_array_klass(), \"sanity\");\n+  assert(is_refArray_klass(), \"sanity\");\n+}\n+\n+size_t RefArrayKlass::oop_size(oop obj) const {\n+  \/\/ In this assert, we cannot safely access the Klass* with compact headers,\n+  \/\/ because size_given_klass() calls oop_size() on objects that might be\n+  \/\/ concurrently forwarded, which would overwrite the Klass*.\n+  assert(UseCompactObjectHeaders || obj->is_refArray(), \"must be a reference array\");\n+  return refArrayOop(obj)->object_size();\n+}\n+\n+objArrayOop RefArrayKlass::allocate_instance(int length, ArrayProperties props, TRAPS) {\n+  check_array_allocation_length(\n+      length, arrayOopDesc::max_array_length(T_OBJECT), CHECK_NULL);\n+  size_t size = refArrayOopDesc::object_size(length);\n+  objArrayOop array = (objArrayOop)Universe::heap()->array_allocate(\n+      this, size, length,\n+      \/* do_zero *\/ true, CHECK_NULL);\n+  assert(array->is_refArray(), \"Must be\");\n+  return array;\n+}\n+\n+\n+\/\/ Either oop or narrowOop depending on UseCompressedOops.\n+void RefArrayKlass::do_copy(arrayOop s, size_t src_offset, arrayOop d,\n+                            size_t dst_offset, int length, TRAPS) {\n+  if (s == d) {\n+    \/\/ since source and destination are equal we do not need conversion checks.\n+    assert(length > 0, \"sanity check\");\n+    ArrayAccess<>::oop_arraycopy(s, src_offset, d, dst_offset, length);\n+  } else {\n+    \/\/ We have to make sure all elements conform to the destination array\n+    Klass *bound = RefArrayKlass::cast(d->klass())->element_klass();\n+    Klass *stype = RefArrayKlass::cast(s->klass())->element_klass();\n+    \/\/ Perform null check if dst is null-free but src has no such guarantee\n+    bool null_check = ((!s->klass()->is_null_free_array_klass()) &&\n+                       d->klass()->is_null_free_array_klass());\n+    if (stype == bound || stype->is_subtype_of(bound)) {\n+      if (null_check) {\n+        ArrayAccess<ARRAYCOPY_DISJOINT | ARRAYCOPY_NOTNULL>::oop_arraycopy(\n+            s, src_offset, d, dst_offset, length);\n+      } else {\n+        ArrayAccess<ARRAYCOPY_DISJOINT>::oop_arraycopy(s, src_offset, d,\n+                                                       dst_offset, length);\n+      }\n+    } else {\n+      if (null_check) {\n+        ArrayAccess<ARRAYCOPY_DISJOINT | ARRAYCOPY_CHECKCAST |\n+                    ARRAYCOPY_NOTNULL>::oop_arraycopy(s, src_offset, d,\n+                                                      dst_offset, length);\n+      } else {\n+        ArrayAccess<ARRAYCOPY_DISJOINT | ARRAYCOPY_CHECKCAST>::oop_arraycopy(\n+            s, src_offset, d, dst_offset, length);\n+      }\n+    }\n+  }\n+}\n+\n+void RefArrayKlass::copy_array(arrayOop s, int src_pos, arrayOop d, int dst_pos,\n+                               int length, TRAPS) {\n+  assert(s->is_refArray(), \"must be a reference array\");\n+\n+  if (UseArrayFlattening) {\n+    if (d->is_flatArray()) {\n+      FlatArrayKlass::cast(d->klass())->copy_array(s, src_pos, d, dst_pos, length, THREAD);\n+      return;\n+    }\n+    if (s->is_flatArray()) {\n+      FlatArrayKlass::cast(s->klass())->copy_array(s, src_pos, d, dst_pos, length, THREAD);\n+      return;\n+    }\n+  }\n+\n+  if (!d->is_refArray()) {\n+    ResourceMark rm(THREAD);\n+    stringStream ss;\n+    if (d->is_typeArray()) {\n+      ss.print(\n+          \"arraycopy: type mismatch: can not copy object array[] into %s[]\",\n+          type2name_tab[ArrayKlass::cast(d->klass())->element_type()]);\n+    } else {\n+      ss.print(\"arraycopy: destination type %s is not an array\",\n+               d->klass()->external_name());\n+    }\n+    THROW_MSG(vmSymbols::java_lang_ArrayStoreException(), ss.as_string());\n+  }\n+\n+  \/\/ Check is all offsets and lengths are non negative\n+  if (src_pos < 0 || dst_pos < 0 || length < 0) {\n+    \/\/ Pass specific exception reason.\n+    ResourceMark rm(THREAD);\n+    stringStream ss;\n+    if (src_pos < 0) {\n+      ss.print(\"arraycopy: source index %d out of bounds for object array[%d]\",\n+               src_pos, s->length());\n+    } else if (dst_pos < 0) {\n+      ss.print(\n+          \"arraycopy: destination index %d out of bounds for object array[%d]\",\n+          dst_pos, d->length());\n+    } else {\n+      ss.print(\"arraycopy: length %d is negative\", length);\n+    }\n+    THROW_MSG(vmSymbols::java_lang_ArrayIndexOutOfBoundsException(),\n+              ss.as_string());\n+  }\n+  \/\/ Check if the ranges are valid\n+  if ((((unsigned int)length + (unsigned int)src_pos) >\n+       (unsigned int)s->length()) ||\n+      (((unsigned int)length + (unsigned int)dst_pos) >\n+       (unsigned int)d->length())) {\n+    \/\/ Pass specific exception reason.\n+    ResourceMark rm(THREAD);\n+    stringStream ss;\n+    if (((unsigned int)length + (unsigned int)src_pos) >\n+        (unsigned int)s->length()) {\n+      ss.print(\n+          \"arraycopy: last source index %u out of bounds for object array[%d]\",\n+          (unsigned int)length + (unsigned int)src_pos, s->length());\n+    } else {\n+      ss.print(\"arraycopy: last destination index %u out of bounds for object \"\n+               \"array[%d]\",\n+               (unsigned int)length + (unsigned int)dst_pos, d->length());\n+    }\n+    THROW_MSG(vmSymbols::java_lang_ArrayIndexOutOfBoundsException(),\n+              ss.as_string());\n+  }\n+\n+  \/\/ Special case. Boundary cases must be checked first\n+  \/\/ This allows the following call: copy_array(s, s.length(), d.length(), 0).\n+  \/\/ This is correct, since the position is supposed to be an 'in between\n+  \/\/ point', i.e., s.length(), points to the right of the last element.\n+  if (length == 0) {\n+    return;\n+  }\n+  if (UseCompressedOops) {\n+    size_t src_offset =\n+        (size_t)refArrayOopDesc::obj_at_offset<narrowOop>(src_pos);\n+    size_t dst_offset =\n+        (size_t)refArrayOopDesc::obj_at_offset<narrowOop>(dst_pos);\n+    assert(arrayOopDesc::obj_offset_to_raw<narrowOop>(s, src_offset, nullptr) ==\n+               refArrayOop(s)->obj_at_addr<narrowOop>(src_pos),\n+           \"sanity\");\n+    assert(arrayOopDesc::obj_offset_to_raw<narrowOop>(d, dst_offset, nullptr) ==\n+               refArrayOop(d)->obj_at_addr<narrowOop>(dst_pos),\n+           \"sanity\");\n+    do_copy(s, src_offset, d, dst_offset, length, CHECK);\n+  } else {\n+    size_t src_offset = (size_t)refArrayOopDesc::obj_at_offset<oop>(src_pos);\n+    size_t dst_offset = (size_t)refArrayOopDesc::obj_at_offset<oop>(dst_pos);\n+    assert(arrayOopDesc::obj_offset_to_raw<oop>(s, src_offset, nullptr) ==\n+               refArrayOop(s)->obj_at_addr<oop>(src_pos),\n+           \"sanity\");\n+    assert(arrayOopDesc::obj_offset_to_raw<oop>(d, dst_offset, nullptr) ==\n+               refArrayOop(d)->obj_at_addr<oop>(dst_pos),\n+           \"sanity\");\n+    do_copy(s, src_offset, d, dst_offset, length, CHECK);\n+  }\n+}\n+\n+void RefArrayKlass::initialize(TRAPS) {\n+  bottom_klass()->initialize(THREAD); \/\/ dispatches to either InstanceKlass or TypeArrayKlass\n+}\n+\n+void RefArrayKlass::metaspace_pointers_do(MetaspaceClosure *it) {\n+  ObjArrayKlass::metaspace_pointers_do(it);\n+}\n+\n+\/\/ Printing\n+\n+void RefArrayKlass::print_on(outputStream* st) const {\n+#ifndef PRODUCT\n+  Klass::print_on(st);\n+  st->print(\" - element klass: \");\n+  element_klass()->print_value_on(st);\n+  st->cr();\n+#endif \/\/ PRODUCT\n+}\n+\n+void RefArrayKlass::print_value_on(outputStream* st) const {\n+  assert(is_klass(), \"must be klass\");\n+\n+  element_klass()->print_value_on(st);\n+  st->print(\"[]\");\n+}\n+\n+#ifndef PRODUCT\n+\n+void RefArrayKlass::oop_print_on(oop obj, outputStream* st) {\n+  ArrayKlass::oop_print_on(obj, st);\n+  assert(obj->is_refArray(), \"must be refArray\");\n+  refArrayOop oa = refArrayOop(obj);\n+  int print_len = MIN2(oa->length(), MaxElementPrintSize);\n+  for (int index = 0; index < print_len; index++) {\n+    st->print(\" - %3d : \", index);\n+    if (oa->obj_at(index) != nullptr) {\n+      oa->obj_at(index)->print_value_on(st);\n+      st->cr();\n+    } else {\n+      st->print_cr(\"null\");\n+    }\n+  }\n+  int remaining = oa->length() - print_len;\n+  if (remaining > 0) {\n+    st->print_cr(\" - <%d more elements, increase MaxElementPrintSize to print>\",\n+                 remaining);\n+  }\n+}\n+\n+#endif \/\/ PRODUCT\n+\n+void RefArrayKlass::oop_print_value_on(oop obj, outputStream* st) {\n+  assert(obj->is_refArray(), \"must be refArray\");\n+  st->print(\"a \");\n+  element_klass()->print_value_on(st);\n+  int len = refArrayOop(obj)->length();\n+  st->print(\"[%d] \", len);\n+  if (obj != nullptr) {\n+    obj->print_address_on(st);\n+  } else {\n+    st->print_cr(\"null\");\n+  }\n+}\n+\n+\/\/ Verification\n+\n+void RefArrayKlass::verify_on(outputStream* st) {\n+  ArrayKlass::verify_on(st);\n+  guarantee(element_klass()->is_klass(), \"should be klass\");\n+  guarantee(bottom_klass()->is_klass(), \"should be klass\");\n+  Klass *bk = bottom_klass();\n+  guarantee(bk->is_instance_klass() || bk->is_typeArray_klass() ||\n+                bk->is_flatArray_klass(),\n+            \"invalid bottom klass\");\n+}\n+\n+void RefArrayKlass::oop_verify_on(oop obj, outputStream* st) {\n+  ArrayKlass::oop_verify_on(obj, st);\n+  guarantee(obj->is_refArray(), \"must be refArray\");\n+  guarantee(obj->is_null_free_array() || (!is_null_free_array_klass()),\n+            \"null-free klass but not object\");\n+  refArrayOop oa = refArrayOop(obj);\n+  for (int index = 0; index < oa->length(); index++) {\n+    guarantee(oopDesc::is_oop_or_null(oa->obj_at(index)), \"should be oop\");\n+  }\n+}\n","filename":"src\/hotspot\/share\/oops\/refArrayKlass.cpp","additions":375,"deletions":0,"binary":false,"changes":375,"status":"added"},{"patch":"@@ -268,0 +268,2 @@\n+  RegMask* return_values_mask(const TypeFunc* tf);\n+\n@@ -411,1 +413,1 @@\n-  RegMask                     _return_value_mask;\n+  RegMask*            _return_values_mask;\n","filename":"src\/hotspot\/share\/opto\/matcher.hpp","additions":3,"deletions":1,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -36,0 +36,1 @@\n+#include \"gc\/shared\/gc_globals.hpp\"\n@@ -232,1 +233,9 @@\n-    _orig_pc_slot = C->fixed_slots() - (sizeof(address) \/ VMRegImpl::stack_slot_size);\n+    int fixed_slots = C->fixed_slots();\n+    if (C->needs_stack_repair()) {\n+      fixed_slots -= 2;\n+    }\n+    \/\/ TODO 8284443 Only reserve extra slot if needed\n+    if (InlineTypeReturnedAsFields) {\n+      fixed_slots -= 2;\n+    }\n+    _orig_pc_slot = fixed_slots - (sizeof(address) \/ VMRegImpl::stack_slot_size);\n@@ -273,1 +282,2 @@\n-  MachPrologNode *prolog = new MachPrologNode();\n+  Label verified_entry;\n+  MachPrologNode* prolog = new MachPrologNode(&verified_entry);\n@@ -279,3 +289,2 @@\n-\n-  if( C->is_osr_compilation() ) {\n-    if( PoisonOSREntry ) {\n+  if (C->is_osr_compilation()) {\n+    if (PoisonOSREntry) {\n@@ -286,3 +295,14 @@\n-    if( C->method() && !C->method()->flags().is_static() ) {\n-      \/\/ Insert unvalidated entry point\n-      C->cfg()->insert( broot, 0, new MachUEPNode() );\n+    if (C->method()) {\n+      if (C->method()->has_scalarized_args()) {\n+        \/\/ Add entry point to unpack all inline type arguments\n+        C->cfg()->insert(broot, 0, new MachVEPNode(&verified_entry, \/* verified *\/ true, \/* receiver_only *\/ false));\n+        if (!C->method()->is_static()) {\n+          \/\/ Add verified\/unverified entry points to only unpack inline type receiver at interface calls\n+          C->cfg()->insert(broot, 0, new MachVEPNode(&verified_entry, \/* verified *\/ false, \/* receiver_only *\/ false));\n+          C->cfg()->insert(broot, 0, new MachVEPNode(&verified_entry, \/* verified *\/ true,  \/* receiver_only *\/ true));\n+          C->cfg()->insert(broot, 0, new MachVEPNode(&verified_entry, \/* verified *\/ false, \/* receiver_only *\/ true));\n+        }\n+      } else if (!C->method()->is_static()) {\n+        \/\/ Insert unvalidated entry point\n+        C->cfg()->insert(broot, 0, new MachUEPNode());\n+      }\n@@ -290,1 +310,0 @@\n-\n@@ -330,0 +349,25 @@\n+  if (!C->is_osr_compilation() && C->has_scalarized_args()) {\n+    \/\/ Compute the offsets of the entry points required by the inline type calling convention\n+    if (!C->method()->is_static()) {\n+      \/\/ We have entries at the beginning of the method, implemented by the first 4 nodes.\n+      \/\/ Entry                     (unverified) @ offset 0\n+      \/\/ Verified_Inline_Entry_RO\n+      \/\/ Inline_Entry              (unverified)\n+      \/\/ Verified_Inline_Entry\n+      uint offset = 0;\n+      _code_offsets.set_value(CodeOffsets::Entry, offset);\n+\n+      offset += ((MachVEPNode*)broot->get_node(0))->size(C->regalloc());\n+      _code_offsets.set_value(CodeOffsets::Verified_Inline_Entry_RO, offset);\n+\n+      offset += ((MachVEPNode*)broot->get_node(1))->size(C->regalloc());\n+      _code_offsets.set_value(CodeOffsets::Inline_Entry, offset);\n+\n+      offset += ((MachVEPNode*)broot->get_node(2))->size(C->regalloc());\n+      _code_offsets.set_value(CodeOffsets::Verified_Inline_Entry, offset);\n+    } else {\n+      _code_offsets.set_value(CodeOffsets::Entry, -1); \/\/ will be patched later\n+      _code_offsets.set_value(CodeOffsets::Verified_Inline_Entry, 0);\n+    }\n+  }\n+\n@@ -491,1 +535,3 @@\n-          mcall->method_set((intptr_t)mcall->entry_point());\n+          if (mcall->entry_point() != nullptr) {\n+            mcall->method_set((intptr_t)mcall->entry_point());\n+          }\n@@ -746,0 +792,29 @@\n+      uint first_ind = spobj->first_index(sfpt->jvms());\n+      \/\/ Nullable, scalarized inline types have a null_marker input\n+      \/\/ that needs to be checked before using the field values.\n+      ScopeValue* properties = nullptr;\n+      if (cik->is_inlinetype()) {\n+        Node* null_marker_node = sfpt->in(first_ind++);\n+        assert(null_marker_node != nullptr, \"null_marker node not found\");\n+        if (!null_marker_node->is_top()) {\n+          const TypeInt* null_marker_type = null_marker_node->bottom_type()->is_int();\n+          if (null_marker_node->is_Con()) {\n+            properties = new ConstantIntValue(null_marker_type->get_con());\n+          } else {\n+            OptoReg::Name null_marker_reg = C->regalloc()->get_reg_first(null_marker_node);\n+            properties = new_loc_value(C->regalloc(), null_marker_reg, Location::normal);\n+          }\n+        }\n+      }\n+      if (cik->is_array_klass() && !cik->is_type_array_klass()) {\n+        jint props = ArrayKlass::ArrayProperties::DEFAULT;\n+        if (cik->as_array_klass()->element_klass()->is_inlinetype()) {\n+          if (cik->as_array_klass()->is_elem_null_free()) {\n+            props |= ArrayKlass::ArrayProperties::NULL_RESTRICTED;\n+          }\n+          if (!cik->as_array_klass()->is_elem_atomic()) {\n+            props |= ArrayKlass::ArrayProperties::NON_ATOMIC;\n+          }\n+        }\n+        properties = new ConstantIntValue(props);\n+      }\n@@ -747,1 +822,1 @@\n-                           new ConstantOopWriteValue(cik->java_mirror()->constant_encoding()));\n+                           new ConstantOopWriteValue(cik->java_mirror()->constant_encoding()), true, properties);\n@@ -750,1 +825,0 @@\n-      uint first_ind = spobj->first_index(sfpt->jvms());\n@@ -996,0 +1070,1 @@\n+  bool return_scalarized = false;\n@@ -1011,1 +1086,1 @@\n-    if (mcall->returns_pointer()) {\n+    if (mcall->returns_pointer() || mcall->returns_scalarized()) {\n@@ -1014,0 +1089,3 @@\n+    if (mcall->returns_scalarized()) {\n+      return_scalarized = true;\n+    }\n@@ -1082,0 +1160,14 @@\n+          assert(!cik->is_inlinetype(), \"Synchronization on value object?\");\n+          ScopeValue* properties = nullptr;\n+          if (cik->is_array_klass() && !cik->is_type_array_klass()) {\n+            jint props = ArrayKlass::ArrayProperties::DEFAULT;\n+            if (cik->as_array_klass()->element_klass()->is_inlinetype()) {\n+              if (cik->as_array_klass()->is_elem_null_free()) {\n+                props |= ArrayKlass::ArrayProperties::NULL_RESTRICTED;\n+              }\n+              if (!cik->as_array_klass()->is_elem_atomic()) {\n+                props |= ArrayKlass::ArrayProperties::NON_ATOMIC;\n+              }\n+            }\n+            properties = new ConstantIntValue(props);\n+          }\n@@ -1083,1 +1175,1 @@\n-                                            new ConstantOopWriteValue(cik->java_mirror()->constant_encoding()));\n+                                            new ConstantOopWriteValue(cik->java_mirror()->constant_encoding()), true, properties);\n@@ -1190,0 +1282,1 @@\n+      return_scalarized,\n@@ -1545,2 +1638,4 @@\n-          \/\/ This destination address is NOT PC-relative\n-          mcall->method_set((intptr_t)mcall->entry_point());\n+          if (mcall->entry_point() != nullptr) {\n+            \/\/ This destination address is NOT PC-relative\n+            mcall->method_set((intptr_t)mcall->entry_point());\n+          }\n@@ -1684,1 +1779,0 @@\n-\n@@ -2941,0 +3035,13 @@\n+\n+      \/\/ Do not allow a CheckCastPP node whose input is a raw pointer to\n+      \/\/ float past a safepoint.  This can occur when a buffered inline\n+      \/\/ type is allocated in a loop and the CheckCastPP from that\n+      \/\/ allocation is reused outside the loop.  If the use inside the\n+      \/\/ loop is scalarized the CheckCastPP will no longer be connected\n+      \/\/ to the loop safepoint.  See JDK-8264340.\n+      if (m->is_Mach() && m->as_Mach()->ideal_Opcode() == Op_CheckCastPP) {\n+        Node *def = m->in(1);\n+        if (def != nullptr && def->bottom_type()->base() == Type::RawPtr) {\n+          last_safept_node->add_prec(m);\n+        }\n+      }\n@@ -3089,0 +3196,19 @@\n+    if (C->has_scalarized_args()) {\n+      \/\/ Inline type entry points (MachVEPNodes) require lots of space for GC barriers and oop verification\n+      \/\/ when loading object fields from the buffered argument. Increase scratch buffer size accordingly.\n+      ciMethod* method = C->method();\n+      int barrier_size = UseZGC ? 200 : (7 DEBUG_ONLY(+ 37));\n+      int arg_num = 0;\n+      if (!method->is_static()) {\n+        if (method->is_scalarized_arg(arg_num)) {\n+          size += method->holder()->as_inline_klass()->oop_count() * barrier_size;\n+        }\n+        arg_num++;\n+      }\n+      for (ciSignatureStream str(method->signature()); !str.at_return_type(); str.next()) {\n+        if (method->is_scalarized_arg(arg_num)) {\n+          size += str.type()->as_inline_klass()->oop_count() * barrier_size;\n+        }\n+        arg_num++;\n+      }\n+    }\n@@ -3159,1 +3285,2 @@\n-  if (is_branch) \/\/ Restore label.\n+  \/\/ Restore label.\n+  if (is_branch) {\n@@ -3161,0 +3288,1 @@\n+  }\n@@ -3201,6 +3329,9 @@\n-      if (!target->is_static()) {\n-        \/\/ The UEP of an nmethod ensures that the VEP is padded. However, the padding of the UEP is placed\n-        \/\/ before the inline cache check, so we don't have to execute any nop instructions when dispatching\n-        \/\/ through the UEP, yet we can ensure that the VEP is aligned appropriately.\n-        _code_offsets.set_value(CodeOffsets::Entry, _first_block_size - MacroAssembler::ic_check_size());\n-      }\n+      if (_code_offsets.value(CodeOffsets::Verified_Inline_Entry) == -1) {\n+        _code_offsets.set_value(CodeOffsets::Verified_Inline_Entry, _first_block_size);\n+      }\n+      if (_code_offsets.value(CodeOffsets::Verified_Inline_Entry_RO) == -1) {\n+        _code_offsets.set_value(CodeOffsets::Verified_Inline_Entry_RO, _first_block_size);\n+      }\n+      if (_code_offsets.value(CodeOffsets::Entry) == -1) {\n+        _code_offsets.set_value(CodeOffsets::Entry, _first_block_size);\n+      }\n@@ -3212,14 +3343,14 @@\n-                                     entry_bci,\n-                                     &_code_offsets,\n-                                     _orig_pc_slot_offset_in_bytes,\n-                                     code_buffer(),\n-                                     frame_size_in_words(),\n-                                     oop_map_set(),\n-                                     &_handler_table,\n-                                     inc_table(),\n-                                     compiler,\n-                                     has_unsafe_access,\n-                                     SharedRuntime::is_wide_vector(C->max_vector_size()),\n-                                     C->has_monitors(),\n-                                     C->has_scoped_access(),\n-                                     0);\n+                              entry_bci,\n+                              &_code_offsets,\n+                              _orig_pc_slot_offset_in_bytes,\n+                              code_buffer(),\n+                              frame_size_in_words(),\n+                              _oop_map_set,\n+                              &_handler_table,\n+                              inc_table(),\n+                              compiler,\n+                              has_unsafe_access,\n+                              SharedRuntime::is_wide_vector(C->max_vector_size()),\n+                              C->has_monitors(),\n+                              C->has_scoped_access(),\n+                              0);\n","filename":"src\/hotspot\/share\/opto\/output.cpp","additions":169,"deletions":38,"binary":false,"changes":207,"status":"modified"},{"patch":"@@ -1184,1 +1184,1 @@\n-  n->dump_bfs(1, nullptr, \"\", &ss);\n+  n->dump_bfs(3, nullptr, \"\", &ss);\n@@ -2077,6 +2077,0 @@\n-  if (_delay_transform) {\n-    \/\/ Register the node but don't optimize for now\n-    register_new_node_with_optimizer(n);\n-    return n;\n-  }\n-\n@@ -2089,0 +2083,6 @@\n+  if (_delay_transform) {\n+    \/\/ Add the node to the worklist but don't optimize for now\n+    _worklist.push(n);\n+    return n;\n+  }\n+\n@@ -2361,0 +2361,13 @@\n+void PhaseIterGVN::replace_in_uses(Node* n, Node* m) {\n+  assert(n != nullptr, \"sanity\");\n+  for (DUIterator_Fast imax, i = n->fast_outs(imax); i < imax; i++) {\n+    Node* u = n->fast_out(i);\n+    if (u != n) {\n+      rehash_node_delayed(u);\n+      int nb = u->replace_edge(n, m);\n+      --i, imax -= nb;\n+    }\n+  }\n+  assert(n->outcnt() == 0, \"all uses must be deleted\");\n+}\n+\n@@ -2416,0 +2429,10 @@\n+  \/\/ AndLNode::Ideal folds GraphKit::mark_word_test patterns. Give it a chance to run.\n+  if (n->is_Load() && use->is_Phi()) {\n+    for (DUIterator_Fast imax, i = use->fast_outs(imax); i < imax; i++) {\n+      Node* u = use->fast_out(i);\n+      if (u->Opcode() == Op_AndL) {\n+        worklist.push(u);\n+      }\n+    }\n+  }\n+\n@@ -2513,0 +2536,9 @@\n+  \/\/ Inline type nodes can have other inline types as users. If an input gets\n+  \/\/ updated, make sure that inline type users get a chance for optimization.\n+  if (use->is_InlineType()) {\n+    for (DUIterator_Fast i2max, i2 = use->fast_outs(i2max); i2 < i2max; i2++) {\n+      Node* u = use->fast_out(i2);\n+      if (u->is_InlineType())\n+        worklist.push(u);\n+    }\n+  }\n@@ -2629,0 +2661,8 @@\n+  if (use_op == Op_CastP2X) {\n+    for (DUIterator_Fast i2max, i2 = use->fast_outs(i2max); i2 < i2max; i2++) {\n+      Node* u = use->fast_out(i2);\n+      if (u->Opcode() == Op_AndX) {\n+        worklist.push(u);\n+      }\n+    }\n+  }\n@@ -2647,0 +2687,10 @@\n+  \/\/ Give CallStaticJavaNode::remove_useless_allocation a chance to run\n+  if (use->is_Region()) {\n+    Node* c = use;\n+    do {\n+      c = c->unique_ctrl_out_or_null();\n+    } while (c != nullptr && c->is_Region());\n+    if (c != nullptr && c->is_CallStaticJava() && c->as_CallStaticJava()->uncommon_trap_request() != 0) {\n+      worklist.push(c);\n+    }\n+  }\n@@ -2766,1 +2816,1 @@\n-    n->dump(1);\n+    n->dump(3);\n@@ -2888,0 +2938,1 @@\n+  push_cast(worklist, use);\n@@ -2999,0 +3050,12 @@\n+  }\n+}\n+\n+void PhaseCCP::push_cast(Unique_Node_List& worklist, const Node* use) {\n+  uint use_op = use->Opcode();\n+  if (use_op == Op_CastP2X) {\n+    for (DUIterator_Fast i2max, i2 = use->fast_outs(i2max); i2 < i2max; i2++) {\n+      Node* u = use->fast_out(i2);\n+      if (u->Opcode() == Op_AndX) {\n+        worklist.push(u);\n+      }\n+    }\n","filename":"src\/hotspot\/share\/opto\/phaseX.cpp","additions":71,"deletions":8,"binary":false,"changes":79,"status":"modified"},{"patch":"@@ -482,1 +482,1 @@\n-  virtual void record_for_igvn(Node *n) { }\n+  virtual void record_for_igvn(Node *n) { _worklist.push(n); }\n@@ -557,0 +557,2 @@\n+  void replace_in_uses(Node* n, Node* m);\n+\n@@ -648,0 +650,1 @@\n+  static void push_cast(Unique_Node_List& worklist, const Node* use);\n","filename":"src\/hotspot\/share\/opto\/phaseX.hpp","additions":4,"deletions":1,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -25,0 +25,3 @@\n+#include \"ci\/ciField.hpp\"\n+#include \"ci\/ciFlatArrayKlass.hpp\"\n+#include \"ci\/ciInlineKlass.hpp\"\n@@ -48,0 +51,1 @@\n+#include \"utilities\/globalDefinitions.hpp\"\n@@ -57,0 +61,45 @@\n+const Type::Offset Type::Offset::top(Type::OffsetTop);\n+const Type::Offset Type::Offset::bottom(Type::OffsetBot);\n+\n+const Type::Offset Type::Offset::meet(const Type::Offset other) const {\n+  \/\/ Either is 'TOP' offset?  Return the other offset!\n+  if (_offset == OffsetTop) return other;\n+  if (other._offset == OffsetTop) return *this;\n+  \/\/ If either is different, return 'BOTTOM' offset\n+  if (_offset != other._offset) return bottom;\n+  return Offset(_offset);\n+}\n+\n+const Type::Offset Type::Offset::dual() const {\n+  if (_offset == OffsetTop) return bottom;\/\/ Map 'TOP' into 'BOTTOM'\n+  if (_offset == OffsetBot) return top;\/\/ Map 'BOTTOM' into 'TOP'\n+  return Offset(_offset);               \/\/ Map everything else into self\n+}\n+\n+const Type::Offset Type::Offset::add(intptr_t offset) const {\n+  \/\/ Adding to 'TOP' offset?  Return 'TOP'!\n+  if (_offset == OffsetTop || offset == OffsetTop) return top;\n+  \/\/ Adding to 'BOTTOM' offset?  Return 'BOTTOM'!\n+  if (_offset == OffsetBot || offset == OffsetBot) return bottom;\n+  \/\/ Addition overflows or \"accidentally\" equals to OffsetTop? Return 'BOTTOM'!\n+  offset += (intptr_t)_offset;\n+  if (offset != (int)offset || offset == OffsetTop) return bottom;\n+\n+  \/\/ assert( _offset >= 0 && _offset+offset >= 0, \"\" );\n+  \/\/ It is possible to construct a negative offset during PhaseCCP\n+\n+  return Offset((int)offset);        \/\/ Sum valid offsets\n+}\n+\n+void Type::Offset::dump2(outputStream *st) const {\n+  if (_offset == 0) {\n+    return;\n+  } else if (_offset == OffsetTop) {\n+    st->print(\"+top\");\n+  }\n+  else if (_offset == OffsetBot) {\n+    st->print(\"+bot\");\n+  } else if (_offset) {\n+    st->print(\"+%d\", _offset);\n+  }\n+}\n@@ -235,0 +284,3 @@\n+  case T_OBJECT:\n+    return Type::get_const_type(type->unwrap())->join_speculative(type->is_null_free() ? TypePtr::NOTNULL : TypePtr::BOTTOM);\n+\n@@ -557,3 +609,3 @@\n-  TypePtr::NULL_PTR= TypePtr::make(AnyPtr, TypePtr::Null, 0);\n-  TypePtr::NOTNULL = TypePtr::make(AnyPtr, TypePtr::NotNull, OffsetBot);\n-  TypePtr::BOTTOM  = TypePtr::make(AnyPtr, TypePtr::BotPTR, OffsetBot);\n+  TypePtr::NULL_PTR= TypePtr::make(AnyPtr, TypePtr::Null, Offset(0));\n+  TypePtr::NOTNULL = TypePtr::make(AnyPtr, TypePtr::NotNull, Offset::bottom);\n+  TypePtr::BOTTOM  = TypePtr::make(AnyPtr, TypePtr::BotPTR, Offset::bottom);\n@@ -576,1 +628,1 @@\n-                                           false, nullptr, oopDesc::mark_offset_in_bytes());\n+                                           false, nullptr, Offset(oopDesc::mark_offset_in_bytes()));\n@@ -578,2 +630,2 @@\n-                                           false, nullptr, oopDesc::klass_offset_in_bytes());\n-  TypeOopPtr::BOTTOM  = TypeOopPtr::make(TypePtr::BotPTR, OffsetBot, TypeOopPtr::InstanceBot);\n+                                           false, nullptr, Offset(oopDesc::klass_offset_in_bytes()));\n+  TypeOopPtr::BOTTOM  = TypeOopPtr::make(TypePtr::BotPTR, Offset::bottom, TypeOopPtr::InstanceBot);\n@@ -581,1 +633,1 @@\n-  TypeMetadataPtr::BOTTOM = TypeMetadataPtr::make(TypePtr::BotPTR, nullptr, OffsetBot);\n+  TypeMetadataPtr::BOTTOM = TypeMetadataPtr::make(TypePtr::BotPTR, nullptr, Offset::bottom);\n@@ -604,2 +656,2 @@\n-  TypeAryPtr::BOTTOM = TypeAryPtr::make(TypePtr::BotPTR, TypeAry::make(Type::BOTTOM, TypeInt::POS), nullptr, false, Type::OffsetBot);\n-  TypeAryPtr::RANGE   = TypeAryPtr::make( TypePtr::BotPTR, TypeAry::make(Type::BOTTOM,TypeInt::POS), nullptr \/* current->env()->Object_klass() *\/, false, arrayOopDesc::length_offset_in_bytes());\n+  TypeAryPtr::BOTTOM = TypeAryPtr::make(TypePtr::BotPTR, TypeAry::make(Type::BOTTOM, TypeInt::POS), nullptr, false, Offset::bottom);\n+  TypeAryPtr::RANGE   = TypeAryPtr::make(TypePtr::BotPTR, TypeAry::make(Type::BOTTOM,TypeInt::POS), nullptr \/* current->env()->Object_klass() *\/, false, Offset(arrayOopDesc::length_offset_in_bytes()));\n@@ -607,1 +659,1 @@\n-  TypeAryPtr::NARROWOOPS = TypeAryPtr::make(TypePtr::BotPTR, TypeAry::make(TypeNarrowOop::BOTTOM, TypeInt::POS), nullptr \/*ciArrayKlass::make(o)*\/,  false,  Type::OffsetBot);\n+  TypeAryPtr::NARROWOOPS = TypeAryPtr::make(TypePtr::BotPTR, TypeAry::make(TypeNarrowOop::BOTTOM, TypeInt::POS), nullptr \/*ciArrayKlass::make(o)*\/,  false,  Offset::bottom);\n@@ -617,1 +669,1 @@\n-    TypeAryPtr::OOPS  = TypeAryPtr::make(TypePtr::BotPTR, TypeAry::make(TypeInstPtr::BOTTOM,TypeInt::POS), nullptr \/*ciArrayKlass::make(o)*\/,  false,  Type::OffsetBot);\n+    TypeAryPtr::OOPS  = TypeAryPtr::make(TypePtr::BotPTR, TypeAry::make(TypeInstPtr::BOTTOM,TypeInt::POS), nullptr \/*ciArrayKlass::make(o)*\/,  false,  Offset::bottom);\n@@ -619,7 +671,8 @@\n-  TypeAryPtr::BYTES   = TypeAryPtr::make(TypePtr::BotPTR, TypeAry::make(TypeInt::BYTE      ,TypeInt::POS), ciTypeArrayKlass::make(T_BYTE),   true,  Type::OffsetBot);\n-  TypeAryPtr::SHORTS  = TypeAryPtr::make(TypePtr::BotPTR, TypeAry::make(TypeInt::SHORT     ,TypeInt::POS), ciTypeArrayKlass::make(T_SHORT),  true,  Type::OffsetBot);\n-  TypeAryPtr::CHARS   = TypeAryPtr::make(TypePtr::BotPTR, TypeAry::make(TypeInt::CHAR      ,TypeInt::POS), ciTypeArrayKlass::make(T_CHAR),   true,  Type::OffsetBot);\n-  TypeAryPtr::INTS    = TypeAryPtr::make(TypePtr::BotPTR, TypeAry::make(TypeInt::INT       ,TypeInt::POS), ciTypeArrayKlass::make(T_INT),    true,  Type::OffsetBot);\n-  TypeAryPtr::LONGS   = TypeAryPtr::make(TypePtr::BotPTR, TypeAry::make(TypeLong::LONG     ,TypeInt::POS), ciTypeArrayKlass::make(T_LONG),   true,  Type::OffsetBot);\n-  TypeAryPtr::FLOATS  = TypeAryPtr::make(TypePtr::BotPTR, TypeAry::make(Type::FLOAT        ,TypeInt::POS), ciTypeArrayKlass::make(T_FLOAT),  true,  Type::OffsetBot);\n-  TypeAryPtr::DOUBLES = TypeAryPtr::make(TypePtr::BotPTR, TypeAry::make(Type::DOUBLE       ,TypeInt::POS), ciTypeArrayKlass::make(T_DOUBLE), true,  Type::OffsetBot);\n+  TypeAryPtr::BYTES   = TypeAryPtr::make(TypePtr::BotPTR, TypeAry::make(TypeInt::BYTE      ,TypeInt::POS), ciTypeArrayKlass::make(T_BYTE),   true,  Offset::bottom);\n+  TypeAryPtr::SHORTS  = TypeAryPtr::make(TypePtr::BotPTR, TypeAry::make(TypeInt::SHORT     ,TypeInt::POS), ciTypeArrayKlass::make(T_SHORT),  true,  Offset::bottom);\n+  TypeAryPtr::CHARS   = TypeAryPtr::make(TypePtr::BotPTR, TypeAry::make(TypeInt::CHAR      ,TypeInt::POS), ciTypeArrayKlass::make(T_CHAR),   true,  Offset::bottom);\n+  TypeAryPtr::INTS    = TypeAryPtr::make(TypePtr::BotPTR, TypeAry::make(TypeInt::INT       ,TypeInt::POS), ciTypeArrayKlass::make(T_INT),    true,  Offset::bottom);\n+  TypeAryPtr::LONGS   = TypeAryPtr::make(TypePtr::BotPTR, TypeAry::make(TypeLong::LONG     ,TypeInt::POS), ciTypeArrayKlass::make(T_LONG),   true,  Offset::bottom);\n+  TypeAryPtr::FLOATS  = TypeAryPtr::make(TypePtr::BotPTR, TypeAry::make(Type::FLOAT        ,TypeInt::POS), ciTypeArrayKlass::make(T_FLOAT),  true,  Offset::bottom);\n+  TypeAryPtr::DOUBLES = TypeAryPtr::make(TypePtr::BotPTR, TypeAry::make(Type::DOUBLE       ,TypeInt::POS), ciTypeArrayKlass::make(T_DOUBLE), true,  Offset::bottom);\n+  TypeAryPtr::INLINES = TypeAryPtr::make(TypePtr::BotPTR, TypeAry::make(TypeInstPtr::BOTTOM,TypeInt::POS, \/* stable= *\/ false, \/* flat= *\/ true), nullptr, false, Offset::bottom);\n@@ -630,0 +683,1 @@\n+  TypeAryPtr::_array_body_type[T_FLAT_ELEMENT] = TypeAryPtr::OOPS;\n@@ -640,2 +694,2 @@\n-  TypeInstKlassPtr::OBJECT = TypeInstKlassPtr::make(TypePtr::NotNull, current->env()->Object_klass(), 0);\n-  TypeInstKlassPtr::OBJECT_OR_NULL = TypeInstKlassPtr::make(TypePtr::BotPTR, current->env()->Object_klass(), 0);\n+  TypeInstKlassPtr::OBJECT = TypeInstKlassPtr::make(TypePtr::NotNull, current->env()->Object_klass(), Offset(0));\n+  TypeInstKlassPtr::OBJECT_OR_NULL = TypeInstKlassPtr::make(TypePtr::BotPTR, current->env()->Object_klass(), Offset(0));\n@@ -680,0 +734,1 @@\n+  _const_basic_type[T_FLAT_ELEMENT] = TypeInstPtr::BOTTOM;\n@@ -696,0 +751,1 @@\n+  _zero_type[T_FLAT_ELEMENT] = TypePtr::NULL_PTR;\n@@ -970,0 +1026,3 @@\n+\n+  \/\/ Verify that:\n+  \/\/      this meet t == t meet this\n@@ -986,0 +1045,14 @@\n+  \/\/ Verify that:\n+  \/\/ 1)     mt_dual meet t_dual    == t_dual\n+  \/\/    which corresponds to\n+  \/\/       !(t meet this)  meet !t ==\n+  \/\/       (!t join !this) meet !t == !t\n+  \/\/ 2)    mt_dual meet this_dual     == this_dual\n+  \/\/    which corresponds to\n+  \/\/       !(t meet this)  meet !this ==\n+  \/\/       (!t join !this) meet !this == !this\n+  \/\/ TODO 8332406 Fix this\n+  if ((   isa_instptr() != nullptr &&    is_instptr()->flat_in_array()) ||\n+      (t->isa_instptr() != nullptr && t->is_instptr()->flat_in_array())) {\n+    return;\n+  }\n@@ -996,0 +1069,1 @@\n+    \/\/ 1)\n@@ -997,0 +1071,1 @@\n+    \/\/ 2)\n@@ -1034,0 +1109,3 @@\n+  \/\/ TODO 8350865 This currently triggers a verification failure, the code around \"\/\/ Even though MyValue is final\" needs adjustments\n+  if ((this_t->isa_ptr() && this_t->is_ptr()->is_not_flat()) ||\n+      (this_t->_dual->isa_ptr() && this_t->_dual->is_ptr()->is_not_flat())) return mt;\n@@ -2050,0 +2128,21 @@\n+static void collect_inline_fields(ciInlineKlass* vk, const Type** field_array, uint& pos) {\n+  for (int i = 0; i < vk->nof_declared_nonstatic_fields(); i++) {\n+    ciField* field = vk->declared_nonstatic_field_at(i);\n+    if (field->is_flat()) {\n+      collect_inline_fields(field->type()->as_inline_klass(), field_array, pos);\n+      if (!field->is_null_free()) {\n+        \/\/ Use T_INT instead of T_BOOLEAN here because the upper bits can contain garbage if the holder\n+        \/\/ is null and C2 will only zero them for T_INT assuming that T_BOOLEAN is already canonicalized.\n+        field_array[pos++] = Type::get_const_basic_type(T_INT);\n+      }\n+    } else {\n+      BasicType bt = field->type()->basic_type();\n+      const Type* ft = Type::get_const_type(field->type());\n+      field_array[pos++] = ft;\n+      if (type2size[bt] == 2) {\n+        field_array[pos++] = Type::HALF;\n+      }\n+    }\n+  }\n+}\n+\n@@ -2052,1 +2151,1 @@\n-const TypeTuple *TypeTuple::make_range(ciSignature* sig, InterfaceHandling interface_handling) {\n+const TypeTuple *TypeTuple::make_range(ciSignature* sig, InterfaceHandling interface_handling, bool ret_vt_fields) {\n@@ -2055,0 +2154,5 @@\n+  if (ret_vt_fields) {\n+    arg_cnt = return_type->as_inline_klass()->inline_arg_slots() + 1;\n+    \/\/ InlineTypeNode::NullMarker field used for null checking\n+    arg_cnt++;\n+  }\n@@ -2066,0 +2170,12 @@\n+    if (return_type->is_inlinetype() && ret_vt_fields) {\n+      uint pos = TypeFunc::Parms;\n+      field_array[pos++] = get_const_type(return_type); \/\/ Oop might be null when returning as fields\n+      collect_inline_fields(return_type->as_inline_klass(), field_array, pos);\n+      \/\/ InlineTypeNode::NullMarker field used for null checking\n+      field_array[pos++] = get_const_basic_type(T_BOOLEAN);\n+      assert(pos == (TypeFunc::Parms + arg_cnt), \"out of bounds\");\n+      break;\n+    } else {\n+      field_array[TypeFunc::Parms] = get_const_type(return_type, interface_handling)->join_speculative(TypePtr::BOTTOM);\n+    }\n+    break;\n@@ -2084,2 +2200,10 @@\n-const TypeTuple *TypeTuple::make_domain(ciInstanceKlass* recv, ciSignature* sig, InterfaceHandling interface_handling) {\n-  uint arg_cnt = sig->size();\n+const TypeTuple *TypeTuple::make_domain(ciMethod* method, InterfaceHandling interface_handling, bool vt_fields_as_args) {\n+  ciSignature* sig = method->signature();\n+  uint arg_cnt = sig->size() + (method->is_static() ? 0 : 1);\n+  if (vt_fields_as_args) {\n+    arg_cnt = 0;\n+    assert(method->get_sig_cc() != nullptr, \"Should have scalarized signature\");\n+    for (ExtendedSignature sig_cc = ExtendedSignature(method->get_sig_cc(), SigEntryFilter()); !sig_cc.at_end(); ++sig_cc) {\n+      arg_cnt += type2size[(*sig_cc)._bt];\n+    }\n+  }\n@@ -2088,8 +2212,8 @@\n-  const Type **field_array;\n-  if (recv != nullptr) {\n-    arg_cnt++;\n-    field_array = fields(arg_cnt);\n-    \/\/ Use get_const_type here because it respects UseUniqueSubclasses:\n-    field_array[pos++] = get_const_type(recv, interface_handling)->join_speculative(TypePtr::NOTNULL);\n-  } else {\n-    field_array = fields(arg_cnt);\n+  const Type** field_array = fields(arg_cnt);\n+  if (!method->is_static()) {\n+    ciInstanceKlass* recv = method->holder();\n+    if (vt_fields_as_args && recv->is_inlinetype() && recv->as_inline_klass()->can_be_passed_as_fields() && method->is_scalarized_arg(0)) {\n+      collect_inline_fields(recv->as_inline_klass(), field_array, pos);\n+    } else {\n+      field_array[pos++] = get_const_type(recv, interface_handling)->join_speculative(TypePtr::NOTNULL);\n+    }\n@@ -2101,0 +2225,1 @@\n+    BasicType bt = type->basic_type();\n@@ -2102,1 +2227,1 @@\n-    switch (type->basic_type()) {\n+    switch (bt) {\n@@ -2112,0 +2237,8 @@\n+      if (type->is_inlinetype() && vt_fields_as_args && method->is_scalarized_arg(i + (method->is_static() ? 0 : 1))) {\n+        \/\/ InlineTypeNode::NullMarker field used for null checking\n+        field_array[pos++] = get_const_basic_type(T_BOOLEAN);\n+        collect_inline_fields(type->as_inline_klass(), field_array, pos);\n+      } else {\n+        field_array[pos++] = get_const_type(type, interface_handling);\n+      }\n+      break;\n@@ -2128,0 +2261,1 @@\n+  assert(pos == TypeFunc::Parms + arg_cnt, \"wrong number of arguments\");\n@@ -2262,1 +2396,2 @@\n-const TypeAry* TypeAry::make(const Type* elem, const TypeInt* size, bool stable) {\n+const TypeAry* TypeAry::make(const Type* elem, const TypeInt* size, bool stable,\n+                             bool flat, bool not_flat, bool not_null_free, bool atomic) {\n@@ -2267,1 +2402,1 @@\n-  return (TypeAry*)(new TypeAry(elem,size,stable))->hashcons();\n+  return (TypeAry*)(new TypeAry(elem, size, stable, flat, not_flat, not_null_free, atomic))->hashcons();\n@@ -2294,1 +2429,5 @@\n-                         isize, _stable && a->_stable);\n+                         isize, _stable && a->_stable,\n+                         _flat && a->_flat,\n+                         _not_flat && a->_not_flat,\n+                         _not_null_free && a->_not_null_free,\n+                         _atomic && a->_atomic);\n@@ -2307,1 +2446,1 @@\n-  return new TypeAry(_elem->dual(), size_dual, !_stable);\n+  return new TypeAry(_elem->dual(), size_dual, !_stable, !_flat, !_not_flat, !_not_null_free, !_atomic);\n@@ -2316,1 +2455,6 @@\n-    _size == a->_size;\n+    _size == a->_size &&\n+    _flat == a->_flat &&\n+    _not_flat == a->_not_flat &&\n+    _not_null_free == a->_not_null_free &&\n+    _atomic == a->_atomic;\n+\n@@ -2322,1 +2466,2 @@\n-  return (uint)(uintptr_t)_elem + (uint)(uintptr_t)_size + (uint)(_stable ? 43 : 0);\n+  return (uint)(uintptr_t)_elem + (uint)(uintptr_t)_size + (uint)(_stable ? 43 : 0) +\n+      (uint)(_flat ? 44 : 0) + (uint)(_not_flat ? 45 : 0) + (uint)(_not_null_free ? 46 : 0) + (uint)(_atomic ? 47 : 0);\n@@ -2329,1 +2474,1 @@\n-  return make(_elem->remove_speculative(), _size, _stable);\n+  return make(_elem->remove_speculative(), _size, _stable, _flat, _not_flat, _not_null_free, _atomic);\n@@ -2336,1 +2481,1 @@\n-  return make(_elem->cleanup_speculative(), _size, _stable);\n+  return make(_elem->cleanup_speculative(), _size, _stable, _flat, _not_flat, _not_null_free, _atomic);\n@@ -2355,0 +2500,6 @@\n+  if (_flat) st->print(\"flat:\");\n+  if (Verbose) {\n+    if (_not_flat) st->print(\"not flat:\");\n+    if (_not_null_free) st->print(\"not null free:\");\n+  }\n+  if (_atomic) st->print(\"atomic:\");\n@@ -2394,2 +2545,13 @@\n-  if (tinst)\n-    return tinst->instance_klass()->is_final();\n+  if (tinst) {\n+    if (tinst->instance_klass()->is_final()) {\n+      \/\/ Even though MyValue is final, [LMyValue is only exact if the array\n+      \/\/ is (not) null-free due to null-free [LMyValue <: null-able [LMyValue.\n+      \/\/ TODO 8350865 If we know that the array can't be null-free, it's allowed to be exact, right?\n+      \/\/ If so, we should add '&& !_not_null_free'\n+      if (tinst->is_inlinetypeptr() && (tinst->ptr() != TypePtr::NotNull)) {\n+        return false;\n+      }\n+      return true;\n+    }\n+    return false;\n+  }\n@@ -2570,1 +2732,1 @@\n-const TypePtr *TypePtr::make(TYPES t, enum PTR ptr, int offset, const TypePtr* speculative, int inline_depth) {\n+const TypePtr* TypePtr::make(TYPES t, enum PTR ptr, Offset offset, const TypePtr* speculative, int inline_depth) {\n@@ -2584,1 +2746,1 @@\n-  return _offset;\n+  return offset();\n@@ -2658,7 +2820,2 @@\n-int TypePtr::meet_offset( int offset ) const {\n-  \/\/ Either is 'TOP' offset?  Return the other offset!\n-  if( _offset == OffsetTop ) return offset;\n-  if( offset == OffsetTop ) return _offset;\n-  \/\/ If either is different, return 'BOTTOM' offset\n-  if( _offset != offset ) return OffsetBot;\n-  return _offset;\n+Type::Offset TypePtr::meet_offset(int offset) const {\n+  return _offset.meet(Offset(offset));\n@@ -2668,4 +2825,2 @@\n-int TypePtr::dual_offset( ) const {\n-  if( _offset == OffsetTop ) return OffsetBot;\/\/ Map 'TOP' into 'BOTTOM'\n-  if( _offset == OffsetBot ) return OffsetTop;\/\/ Map 'BOTTOM' into 'TOP'\n-  return _offset;               \/\/ Map everything else into self\n+Type::Offset TypePtr::dual_offset() const {\n+  return _offset.dual();\n@@ -2684,13 +2839,2 @@\n-int TypePtr::xadd_offset( intptr_t offset ) const {\n-  \/\/ Adding to 'TOP' offset?  Return 'TOP'!\n-  if( _offset == OffsetTop || offset == OffsetTop ) return OffsetTop;\n-  \/\/ Adding to 'BOTTOM' offset?  Return 'BOTTOM'!\n-  if( _offset == OffsetBot || offset == OffsetBot ) return OffsetBot;\n-  \/\/ Addition overflows or \"accidentally\" equals to OffsetTop? Return 'BOTTOM'!\n-  offset += (intptr_t)_offset;\n-  if (offset != (int)offset || offset == OffsetTop) return OffsetBot;\n-\n-  \/\/ assert( _offset >= 0 && _offset+offset >= 0, \"\" );\n-  \/\/ It is possible to construct a negative offset during PhaseCCP\n-\n-  return (int)offset;        \/\/ Sum valid offsets\n+Type::Offset TypePtr::xadd_offset(intptr_t offset) const {\n+  return _offset.add(offset);\n@@ -2705,1 +2849,1 @@\n-  return make(AnyPtr, _ptr, offset, _speculative, _inline_depth);\n+  return make(AnyPtr, _ptr, Offset(offset), _speculative, _inline_depth);\n@@ -2712,1 +2856,1 @@\n-  return _ptr == a->ptr() && _offset == a->offset() && eq_speculative(a) && _inline_depth == a->_inline_depth;\n+  return _ptr == a->ptr() && _offset == a->_offset && eq_speculative(a) && _inline_depth == a->_inline_depth;\n@@ -2718,1 +2862,1 @@\n-  return (uint)_ptr + (uint)_offset + (uint)hash_speculative() + (uint)_inline_depth;\n+  return (uint)_ptr + (uint)offset() + (uint)hash_speculative() + (uint)_inline_depth;\n@@ -2984,3 +3128,1 @@\n-  if( _offset == OffsetTop ) st->print(\"+top\");\n-  else if( _offset == OffsetBot ) st->print(\"+bot\");\n-  else if( _offset ) st->print(\"+%d\", _offset);\n+  _offset.dump2(st);\n@@ -3021,1 +3163,1 @@\n-  return (_offset != OffsetBot) && !below_centerline(_ptr);\n+  return (_offset != Offset::bottom) && !below_centerline(_ptr);\n@@ -3025,1 +3167,1 @@\n-  return (_offset == OffsetTop) || above_centerline(_ptr);\n+  return (_offset == Offset::top) || above_centerline(_ptr);\n@@ -3432,1 +3574,1 @@\n-TypeOopPtr::TypeOopPtr(TYPES t, PTR ptr, ciKlass* k, const TypeInterfaces* interfaces, bool xk, ciObject* o, int offset,\n+TypeOopPtr::TypeOopPtr(TYPES t, PTR ptr, ciKlass* k, const TypeInterfaces* interfaces, bool xk, ciObject* o, Offset offset, Offset field_offset,\n@@ -3441,0 +3583,1 @@\n+    _is_ptr_to_strict_final_field(false),\n@@ -3448,2 +3591,11 @@\n-      (offset > 0) && xk && (k != nullptr) && k->is_instance_klass()) {\n-    _is_ptr_to_boxed_value = k->as_instance_klass()->is_boxed_value_offset(offset);\n+      (offset.get() > 0) && xk && (k != nullptr) && k->is_instance_klass()) {\n+    _is_ptr_to_boxed_value = k->as_instance_klass()->is_boxed_value_offset(offset.get());\n+    _is_ptr_to_strict_final_field = _is_ptr_to_boxed_value;\n+  }\n+\n+  if (klass() != nullptr && klass()->is_instance_klass() && klass()->is_loaded() &&\n+      this->offset() != Type::OffsetBot && this->offset() != Type::OffsetTop) {\n+    ciField* field = klass()->as_instance_klass()->get_field_by_offset(this->offset(), false);\n+    if (field != nullptr && field->is_strict() && field->is_final()) {\n+      _is_ptr_to_strict_final_field = true;\n+    }\n@@ -3451,0 +3603,1 @@\n+\n@@ -3452,2 +3605,2 @@\n-  if (_offset > 0 || _offset == Type::OffsetTop || _offset == Type::OffsetBot) {\n-    if (_offset == oopDesc::klass_offset_in_bytes()) {\n+  if (this->offset() > 0 || this->offset() == Type::OffsetTop || this->offset() == Type::OffsetBot) {\n+    if (this->offset() == oopDesc::klass_offset_in_bytes()) {\n@@ -3459,3 +3612,17 @@\n-    } else if (this->isa_aryptr()) {\n-      _is_ptr_to_narrowoop = (UseCompressedOops && klass()->is_obj_array_klass() &&\n-                             _offset != arrayOopDesc::length_offset_in_bytes());\n+    } else if (UseCompressedOops && this->isa_aryptr() && this->offset() != arrayOopDesc::length_offset_in_bytes()) {\n+      if (klass()->is_obj_array_klass()) {\n+        _is_ptr_to_narrowoop = true;\n+      } else if (klass()->is_flat_array_klass() && field_offset != Offset::top && field_offset != Offset::bottom) {\n+        \/\/ Check if the field of the inline type array element contains oops\n+        ciInlineKlass* vk = klass()->as_flat_array_klass()->element_klass()->as_inline_klass();\n+        int foffset = field_offset.get() + vk->payload_offset();\n+        BasicType field_bt;\n+        ciField* field = vk->get_field_by_offset(foffset, false);\n+        if (field != nullptr) {\n+          field_bt = field->layout_type();\n+        } else {\n+          assert(field_offset.get() == vk->null_marker_offset_in_payload(), \"no field or null marker of %s at offset %d\", vk->name()->as_utf8(), foffset);\n+          field_bt = T_BOOLEAN;\n+        }\n+        _is_ptr_to_narrowoop = UseCompressedOops && ::is_reference_type(field_bt);\n+      }\n@@ -3463,1 +3630,0 @@\n-      ciInstanceKlass* ik = klass()->as_instance_klass();\n@@ -3466,1 +3632,1 @@\n-      } else if (_offset == OffsetBot || _offset == OffsetTop) {\n+      } else if (_offset == Offset::bottom || _offset == Offset::top) {\n@@ -3471,3 +3637,2 @@\n-\n-            (_offset == java_lang_Class::klass_offset() ||\n-             _offset == java_lang_Class::array_klass_offset())) {\n+            (this->offset() == java_lang_Class::klass_offset() ||\n+             this->offset() == java_lang_Class::array_klass_offset())) {\n@@ -3479,1 +3644,1 @@\n-                   _offset >= InstanceMirrorKlass::offset_of_static_fields()) {\n+                   this->offset() >= InstanceMirrorKlass::offset_of_static_fields()) {\n@@ -3484,1 +3649,1 @@\n-            basic_elem_type = k->get_field_type_by_offset(_offset, true);\n+            basic_elem_type = k->get_field_type_by_offset(this->offset(), true);\n@@ -3494,1 +3659,2 @@\n-          BasicType basic_elem_type = ik->get_field_type_by_offset(_offset, false);\n+          ciInstanceKlass* ik = klass()->as_instance_klass();\n+          BasicType basic_elem_type = ik->get_field_type_by_offset(this->offset(), false);\n@@ -3509,1 +3675,1 @@\n-#endif\n+#endif \/\/ _LP64\n@@ -3513,2 +3679,2 @@\n-const TypeOopPtr *TypeOopPtr::make(PTR ptr, int offset, int instance_id,\n-                                     const TypePtr* speculative, int inline_depth) {\n+const TypeOopPtr *TypeOopPtr::make(PTR ptr, Offset offset, int instance_id,\n+                                   const TypePtr* speculative, int inline_depth) {\n@@ -3520,1 +3686,1 @@\n-  return (TypeOopPtr*)(new TypeOopPtr(OopPtr, ptr, k, interfaces, xk, o, offset, instance_id, speculative, inline_depth))->hashcons();\n+  return (TypeOopPtr*)(new TypeOopPtr(OopPtr, ptr, k, interfaces, xk, o, offset, Offset::bottom, instance_id, speculative, inline_depth))->hashcons();\n@@ -3545,1 +3711,0 @@\n-\n@@ -3594,1 +3759,1 @@\n-    int offset = meet_offset(tp->offset());\n+    Offset offset = meet_offset(tp->offset());\n@@ -3636,1 +3801,1 @@\n-  return new TypeOopPtr(_base, dual_ptr(), klass(), _interfaces, klass_is_exact(), const_oop(), dual_offset(), dual_instance_id(), dual_speculative(), dual_inline_depth());\n+  return new TypeOopPtr(_base, dual_ptr(), klass(), _interfaces, klass_is_exact(), const_oop(), dual_offset(), Offset::bottom, dual_instance_id(), dual_speculative(), dual_inline_depth());\n@@ -3641,2 +3806,2 @@\n-const TypeOopPtr* TypeOopPtr::make_from_klass_common(ciKlass* klass, bool klass_change, bool try_for_exact, InterfaceHandling interface_handling) {\n-  if (klass->is_instance_klass()) {\n+const TypeOopPtr* TypeOopPtr::make_from_klass_common(ciKlass *klass, bool klass_change, bool try_for_exact, InterfaceHandling interface_handling) {\n+  if (klass->is_instance_klass() || klass->is_inlinetype()) {\n@@ -3669,1 +3834,1 @@\n-    return TypeInstPtr::make(TypePtr::BotPTR, klass, interfaces, klass_is_exact, nullptr, 0);\n+    return TypeInstPtr::make(TypePtr::BotPTR, klass, interfaces, klass_is_exact, nullptr, Offset(0));\n@@ -3671,5 +3836,19 @@\n-    \/\/ Element is an object array. Recursively call ourself.\n-    ciKlass* eklass = klass->as_obj_array_klass()->element_klass();\n-    const TypeOopPtr *etype = TypeOopPtr::make_from_klass_common(eklass, false, try_for_exact, interface_handling);\n-    bool xk = etype->klass_is_exact();\n-    const TypeAry* arr0 = TypeAry::make(etype, TypeInt::POS);\n+    \/\/ Element is an object or inline type array. Recursively call ourself.\n+    const TypeOopPtr* etype = TypeOopPtr::make_from_klass_common(klass->as_array_klass()->element_klass(), \/* klass_change= *\/ false, try_for_exact, interface_handling);\n+    \/\/ Determine null-free\/flat properties\n+    const bool is_null_free = klass->as_array_klass()->is_elem_null_free();\n+    if (is_null_free) {\n+      etype = etype->join_speculative(NOTNULL)->is_oopptr();\n+    }\n+    const TypeOopPtr* exact_etype = etype;\n+    if (etype->can_be_inline_type()) {\n+      \/\/ Use exact type if element can be an inline type\n+      exact_etype = TypeOopPtr::make_from_klass_common(klass->as_array_klass()->element_klass(), \/* klass_change= *\/ true, \/* try_for_exact= *\/ true, interface_handling);\n+    }\n+    bool not_inline = !exact_etype->can_be_inline_type();\n+    bool not_null_free = not_inline;\n+    bool not_flat = !UseArrayFlattening || not_inline || (exact_etype->is_inlinetypeptr() && !exact_etype->inline_klass()->maybe_flat_in_array());\n+    bool atomic = klass->as_array_klass()->is_elem_atomic();\n+    \/\/ Even though MyValue is final, [LMyValue is not exact because null-free [LMyValue is a subtype.\n+    bool xk = etype->klass_is_exact() && !etype->is_inlinetypeptr();\n+    const TypeAry* arr0 = TypeAry::make(etype, TypeInt::POS, \/* stable= *\/ false, \/* flat= *\/ false, not_flat, not_null_free, atomic);\n@@ -3678,2 +3857,2 @@\n-    \/\/ slam nulls down in the subarrays.\n-    const TypeAryPtr* arr = TypeAryPtr::make(TypePtr::BotPTR, arr0, nullptr, xk, 0);\n+    \/\/ slam nullptrs down in the subarrays.\n+    const TypeAryPtr* arr = TypeAryPtr::make(TypePtr::BotPTR, arr0, nullptr, xk, Offset(0));\n@@ -3684,1 +3863,2 @@\n-    const TypeAry* arr0 = TypeAry::make(etype, TypeInt::POS);\n+    const TypeAry* arr0 = TypeAry::make(etype, TypeInt::POS,\n+                                        \/* stable= *\/ false, \/* flat= *\/ false, \/* not_flat= *\/ true, \/* not_null_free= *\/ true);\n@@ -3687,1 +3867,12 @@\n-    const TypeAryPtr* arr = TypeAryPtr::make(TypePtr::BotPTR, arr0, klass, true, 0);\n+    const TypeAryPtr* arr = TypeAryPtr::make(TypePtr::BotPTR, arr0, klass, true, Offset(0));\n+    return arr;\n+  } else if (klass->is_flat_array_klass()) {\n+    const TypeOopPtr* etype = TypeOopPtr::make_from_klass_raw(klass->as_array_klass()->element_klass(), trust_interfaces);\n+    const bool is_null_free = klass->as_array_klass()->is_elem_null_free();\n+    if (is_null_free) {\n+      etype = etype->join_speculative(NOTNULL)->is_oopptr();\n+    }\n+    bool atomic = klass->as_array_klass()->is_elem_atomic();\n+    const TypeAry* arr0 = TypeAry::make(etype, TypeInt::POS, \/* stable= *\/ false, \/* flat= *\/ true, \/* not_flat= *\/ false, \/* not_null_free= *\/ false, atomic);\n+    const bool exact = is_null_free; \/\/ Only exact if null-free because \"null-free [LMyValue <: null-able [LMyValue\".\n+    const TypeAryPtr* arr = TypeAryPtr::make(TypePtr::BotPTR, arr0, klass, exact, Offset(0));\n@@ -3703,2 +3894,2 @@\n-  if (klass->is_instance_klass()) {\n-    \/\/ Element is an instance\n+  if (klass->is_instance_klass() || klass->is_inlinetype()) {\n+    \/\/ Element is an instance or inline type\n@@ -3708,1 +3899,1 @@\n-      return TypeInstPtr::make(TypePtr::NotNull, klass, true, nullptr, 0);\n+      return TypeInstPtr::make(TypePtr::NotNull, klass, true, nullptr, Offset(0));\n@@ -3712,3 +3903,9 @@\n-    const TypeOopPtr *etype =\n-      TypeOopPtr::make_from_klass_raw(klass->as_obj_array_klass()->element_klass(), trust_interfaces);\n-    const TypeAry* arr0 = TypeAry::make(etype, TypeInt::make(o->as_array()->length()));\n+    const TypeOopPtr* etype = TypeOopPtr::make_from_klass_raw(klass->as_array_klass()->element_klass(), trust_interfaces);\n+    bool is_flat = o->as_array()->is_flat();\n+    bool is_null_free = o->as_array()->is_null_free();\n+    if (is_null_free) {\n+      etype = etype->join_speculative(TypePtr::NOTNULL)->is_oopptr();\n+    }\n+    bool is_atomic = o->as_array()->is_atomic();\n+    const TypeAry* arr0 = TypeAry::make(etype, TypeInt::make(o->as_array()->length()), \/* stable= *\/ false, \/* flat= *\/ false,\n+                                        \/* not_flat= *\/ !is_flat, \/* not_null_free= *\/ !is_null_free, \/* atomic= *\/ is_atomic);\n@@ -3719,1 +3916,1 @@\n-      return TypeAryPtr::make(TypePtr::Constant, o, arr0, klass, true, 0);\n+      return TypeAryPtr::make(TypePtr::Constant, o, arr0, klass, true, Offset(0));\n@@ -3721,1 +3918,1 @@\n-      return TypeAryPtr::make(TypePtr::NotNull, arr0, klass, true, 0);\n+      return TypeAryPtr::make(TypePtr::NotNull, arr0, klass, true, Offset(0));\n@@ -3725,3 +3922,3 @@\n-    const Type* etype =\n-      (Type*)get_const_basic_type(klass->as_type_array_klass()->element_type());\n-    const TypeAry* arr0 = TypeAry::make(etype, TypeInt::make(o->as_array()->length()));\n+    const Type* etype = (Type*)get_const_basic_type(klass->as_type_array_klass()->element_type());\n+    const TypeAry* arr0 = TypeAry::make(etype, TypeInt::make(o->as_array()->length()), \/* stable= *\/ false, \/* flat= *\/ false,\n+                                        \/* not_flat= *\/ true, \/* not_null_free= *\/ true);\n@@ -3731,1 +3928,1 @@\n-      return TypeAryPtr::make(TypePtr::Constant, o, arr0, klass, true, 0);\n+      return TypeAryPtr::make(TypePtr::Constant, o, arr0, klass, true, Offset(0));\n@@ -3733,1 +3930,18 @@\n-      return TypeAryPtr::make(TypePtr::NotNull, arr0, klass, true, 0);\n+      return TypeAryPtr::make(TypePtr::NotNull, arr0, klass, true, Offset(0));\n+    }\n+  } else if (klass->is_flat_array_klass()) {\n+    const TypeOopPtr* etype = TypeOopPtr::make_from_klass_raw(klass->as_array_klass()->element_klass(), trust_interfaces);\n+    bool is_null_free = o->as_array()->is_null_free();\n+    if (is_null_free) {\n+      etype = etype->join_speculative(TypePtr::NOTNULL)->is_oopptr();\n+    }\n+    bool is_atomic = o->as_array()->is_atomic();\n+    const TypeAry* arr0 = TypeAry::make(etype, TypeInt::make(o->as_array()->length()), \/* stable= *\/ false, \/* flat= *\/ true,\n+                                        \/* not_flat= *\/ false, \/* not_null_free= *\/ !is_null_free, \/* atomic= *\/ is_atomic);\n+    \/\/ We used to pass NotNull in here, asserting that the sub-arrays\n+    \/\/ are all not-null.  This is not true in generally, as code can\n+    \/\/ slam nullptrs down in the subarrays.\n+    if (make_constant) {\n+      return TypeAryPtr::make(TypePtr::Constant, o, arr0, klass, true, Offset(0));\n+    } else {\n+      return TypeAryPtr::make(TypePtr::NotNull, arr0, klass, true, Offset(0));\n@@ -3744,1 +3958,1 @@\n-  assert( _offset >= 0, \"\" );\n+  assert(offset() >= 0, \"\");\n@@ -3746,1 +3960,1 @@\n-  if (_offset != 0) {\n+  if (offset() != 0) {\n@@ -3805,6 +4019,1 @@\n-  switch( _offset ) {\n-  case OffsetTop: st->print(\"+top\"); break;\n-  case OffsetBot: st->print(\"+any\"); break;\n-  case         0: break;\n-  default:        st->print(\"+%d\",_offset); break;\n-  }\n+  _offset.dump2(st);\n@@ -3827,1 +4036,1 @@\n-  return (_offset == 0) && !below_centerline(_ptr);\n+  return (offset() == 0) && !below_centerline(_ptr);\n@@ -3836,1 +4045,1 @@\n-  return make(_ptr, offset, _instance_id, with_offset_speculative(offset), _inline_depth);\n+  return make(_ptr, Offset(offset), _instance_id, with_offset_speculative(offset), _inline_depth);\n@@ -3949,3 +4158,4 @@\n-TypeInstPtr::TypeInstPtr(PTR ptr, ciKlass* k, const TypeInterfaces* interfaces, bool xk, ciObject* o, int off,\n-                         int instance_id, const TypePtr* speculative, int inline_depth)\n-  : TypeOopPtr(InstPtr, ptr, k, interfaces, xk, o, off, instance_id, speculative, inline_depth) {\n+TypeInstPtr::TypeInstPtr(PTR ptr, ciKlass* k, const TypeInterfaces* interfaces, bool xk, ciObject* o, Offset off,\n+                         bool flat_in_array, int instance_id, const TypePtr* speculative, int inline_depth)\n+  : TypeOopPtr(InstPtr, ptr, k, interfaces, xk, o, off, Offset::bottom, instance_id, speculative, inline_depth),\n+    _flat_in_array(flat_in_array) {\n@@ -3956,0 +4166,2 @@\n+  assert(!klass()->maybe_flat_in_array() || flat_in_array, \"Should be flat in array\");\n+  assert(!flat_in_array || can_be_inline_type(), \"Only inline types can be flat in array\");\n@@ -3964,1 +4176,2 @@\n-                                     int offset,\n+                                     Offset offset,\n+                                     bool flat_in_array,\n@@ -3986,0 +4199,3 @@\n+  \/\/ Check if this type is known to be flat in arrays\n+  flat_in_array = flat_in_array || k->maybe_flat_in_array();\n+\n@@ -3988,1 +4204,1 @@\n-    (TypeInstPtr*)(new TypeInstPtr(ptr, k, interfaces, xk, o ,offset, instance_id, speculative, inline_depth))->hashcons();\n+    (TypeInstPtr*)(new TypeInstPtr(ptr, k, interfaces, xk, o, offset, flat_in_array, instance_id, speculative, inline_depth))->hashcons();\n@@ -4054,1 +4270,1 @@\n-  return make(ptr, klass(), _interfaces, klass_is_exact(), ptr == Constant ? const_oop() : nullptr, _offset, _instance_id, _speculative, _inline_depth);\n+  return make(ptr, klass(), _interfaces, klass_is_exact(), ptr == Constant ? const_oop() : nullptr, _offset, _flat_in_array, _instance_id, _speculative, _inline_depth);\n@@ -4065,1 +4281,1 @@\n-  return make(ptr(), klass(), _interfaces, klass_is_exact, const_oop(), _offset, _instance_id, _speculative, _inline_depth);\n+  return make(ptr(), klass(), _interfaces, klass_is_exact, const_oop(), _offset, _flat_in_array, _instance_id, _speculative, _inline_depth);\n@@ -4071,1 +4287,1 @@\n-  return make(_ptr, klass(),  _interfaces, _klass_is_exact, const_oop(), _offset, instance_id, _speculative, _inline_depth);\n+  return make(_ptr, klass(), _interfaces, _klass_is_exact, const_oop(), _offset, _flat_in_array, instance_id, _speculative, _inline_depth);\n@@ -4078,1 +4294,1 @@\n-  int off = meet_offset(tinst->offset());\n+  Offset off = meet_offset(tinst->offset());\n@@ -4103,1 +4319,1 @@\n-    else if (loaded->ptr() == TypePtr::AnyNull)  { return make(ptr, unloaded->klass(), interfaces, false, nullptr, off, instance_id, speculative, depth); }\n+    else if (loaded->ptr() == TypePtr::AnyNull)  { return make(ptr, unloaded->klass(), interfaces, false, nullptr, off, false, instance_id, speculative, depth); }\n@@ -4167,1 +4383,1 @@\n-    int offset = meet_offset(tp->offset());\n+    Offset offset = meet_offset(tp->offset());\n@@ -4176,1 +4392,1 @@\n-                  (ptr == Constant ? const_oop() : nullptr), offset, instance_id, speculative, depth);\n+                  (ptr == Constant ? const_oop() : nullptr), offset, flat_in_array(), instance_id, speculative, depth);\n@@ -4192,1 +4408,1 @@\n-    int offset = meet_offset(tp->offset());\n+    Offset offset = meet_offset(tp->offset());\n@@ -4204,1 +4420,1 @@\n-                  (ptr == Constant ? const_oop() : nullptr), offset, instance_id, speculative, depth);\n+                  (ptr == Constant ? const_oop() : nullptr), offset, flat_in_array(), instance_id, speculative, depth);\n@@ -4232,1 +4448,1 @@\n-    int off = meet_offset(tinst->offset());\n+    Offset off = meet_offset(tinst->offset());\n@@ -4244,0 +4460,1 @@\n+    bool res_flat_in_array = false;\n@@ -4245,1 +4462,1 @@\n-    MeetResult kind = meet_instptr(ptr, interfaces, this, tinst, res_klass, res_xk);\n+    MeetResult kind = meet_instptr(ptr, interfaces, this, tinst, res_klass, res_xk, res_flat_in_array);\n@@ -4286,1 +4503,1 @@\n-      res = make(ptr, res_klass, interfaces, res_xk, o, off, instance_id, speculative, depth);\n+      res = make(ptr, res_klass, interfaces, res_xk, o, off, res_flat_in_array, instance_id, speculative, depth);\n@@ -4298,1 +4515,1 @@\n-                                                            ciKlass*& res_klass, bool& res_xk) {\n+                                                            ciKlass*& res_klass, bool& res_xk, bool& res_flat_in_array) {\n@@ -4301,0 +4518,5 @@\n+  const bool this_flat_in_array = this_type->flat_in_array();\n+  const bool other_flat_in_array = other_type->flat_in_array();\n+  const bool this_not_flat_in_array = this_type->not_flat_in_array();\n+  const bool other_not_flat_in_array = other_type->not_flat_in_array();\n+\n@@ -4311,1 +4533,1 @@\n-  if (ptr != Constant && this_klass->equals(other_klass) && this_xk == other_xk) {\n+  if (ptr != Constant && this_klass->equals(other_klass) && this_xk == other_xk && this_flat_in_array == other_flat_in_array) {\n@@ -4314,0 +4536,1 @@\n+    res_flat_in_array = this_flat_in_array;\n@@ -4346,1 +4569,57 @@\n-  \/\/ Check for subtyping:\n+  \/\/ Flat in Array property _flat_in_array.\n+  \/\/ For simplicity, _flat_in_array is a boolean but we actually have a tri state:\n+  \/\/ - Flat in array       -> flat_in_array()\n+  \/\/ - Not flat in array   -> not_flat_in_array()\n+  \/\/ - Maybe flat in array -> !not_flat_in_array()\n+  \/\/\n+  \/\/ Maybe we should convert _flat_in_array to a proper lattice with four elements at some point:\n+  \/\/\n+  \/\/                  Top\n+  \/\/    Flat in Array     Not Flat in Array\n+  \/\/          Maybe Flat in Array\n+  \/\/\n+  \/\/ where\n+  \/\/     Top = dual(maybe Flat In Array) = \"Flat in Array AND Not Flat in Array\"\n+  \/\/\n+  \/\/ But for now we stick with the current model with _flat_in_array as a boolean.\n+  \/\/\n+  \/\/ When meeting two InstPtr types, we want to have the following behavior:\n+  \/\/\n+  \/\/ (FiA-M) Meet(this, other):\n+  \/\/     'this' and 'other' are either the same klass OR sub klasses:\n+  \/\/\n+  \/\/                yes maybe no\n+  \/\/           yes   y    m    m                      y = Flat in Array\n+  \/\/         maybe   m    m    m                      n = Not Flat in Array\n+  \/\/            no   m    m    n                      m = Maybe Flat in Array\n+  \/\/\n+  \/\/  Join(this, other):\n+  \/\/     (FiA-J-Same) 'this' and 'other' are the SAME klass:\n+  \/\/\n+  \/\/                yes maybe no                      E = Empty set\n+  \/\/           yes   y    y    E                      y = Flat in Array\n+  \/\/         maybe   y    m    m                      n = Not Flat in Array\n+  \/\/            no   E    m    n                      m = Maybe Flat in Array\n+  \/\/\n+  \/\/     (FiA-J-Sub) 'this' and 'other' are SUB klasses:\n+  \/\/\n+  \/\/               yes maybe no   -> Super Klass      E = Empty set\n+  \/\/          yes   y    y    y                       y = Flat in Array\n+  \/\/        maybe   y    m    m                       n = Not Flat in Array\n+  \/\/           no   E    m    n                       m = Maybe Flat in Array\n+  \/\/           |\n+  \/\/           v\n+  \/\/       Sub Klass\n+  \/\/\n+  \/\/     Note the difference when joining a super klass that is not flat in array with a sub klass that is compared to\n+  \/\/     the same klass case. We will take over the flat in array property of the sub klass. This can be done because\n+  \/\/     the super klass could be Object (i.e. not an inline type and thus not flat in array) while the sub klass is a\n+  \/\/     value class which can be flat in array.\n+  \/\/\n+  \/\/     The empty set is only a possible result when matching 'ptr' above the center line (i.e. joining). In this case,\n+  \/\/     we can \"fall hard\" by setting 'ptr' to NotNull such that when we take the dual of that meet above the center\n+  \/\/     line, we get an empty set again.\n+  \/\/\n+  \/\/     Note: When changing to a separate lattice with _flat_in_array we may want to add TypeInst(Klass)Ptr::empty()\n+  \/\/           that returns true when the meet result is FlatInArray::Top (i.e. dual(maybe flat in array)).\n+\n@@ -4349,0 +4628,2 @@\n+  bool flat_in_array = false;\n+  bool is_empty = false;\n@@ -4350,0 +4631,1 @@\n+    \/\/ Same klass\n@@ -4352,1 +4634,6 @@\n-  } else if (!other_xk && this_type->is_meet_subtype_of(other_type)) {\n+    if (above_centerline(ptr)) {\n+      \/\/ Case (FiA-J-Same)\n+      \/\/ One is flat in array and the other not? Result is empty\/\"fall hard\".\n+      is_empty = (this_flat_in_array && other_not_flat_in_array) || (this_not_flat_in_array && other_flat_in_array);\n+    }\n+  } else if (!other_xk && is_meet_subtype_of(this_type, other_type)) {\n@@ -4355,1 +4642,9 @@\n-  } else if(!this_xk && other_type->is_meet_subtype_of(this_type)) {\n+    if (above_centerline(ptr)) {\n+      \/\/ Case (FiA-J-Sub)\n+      is_empty = this_not_flat_in_array && other_flat_in_array;\n+      if (!is_empty) {\n+        bool other_flat_this_maybe_flat = other_flat_in_array && (!this_flat_in_array && !this_not_flat_in_array);\n+        flat_in_array = this_flat_in_array || other_flat_this_maybe_flat;\n+      }\n+    }\n+  } else if (!this_xk && is_meet_subtype_of(other_type, this_type)) {\n@@ -4358,0 +4653,8 @@\n+    if (above_centerline(ptr)) {\n+      \/\/ Case (FiA-J-Sub)\n+      is_empty = this_flat_in_array && other_not_flat_in_array;\n+      if (!is_empty) {\n+        bool this_flat_other_maybe_flat = this_flat_in_array && (!other_flat_in_array && !other_not_flat_in_array);\n+        flat_in_array = other_flat_in_array || this_flat_other_maybe_flat;\n+      }\n+    }\n@@ -4360,2 +4663,4 @@\n-  if (subtype) {\n-    if (above_centerline(ptr)) { \/\/ both are up?\n+\n+  if (subtype && !is_empty) {\n+    if (above_centerline(ptr)) {\n+      \/\/ Both types are empty.\n@@ -4364,0 +4669,5 @@\n+      \/\/ Case (FiA-J-Sub)\n+      bool other_flat_this_maybe_flat = other_flat_in_array && (!this_flat_in_array && !this_not_flat_in_array);\n+      flat_in_array = this_flat_in_array || other_flat_this_maybe_flat;\n+      \/\/ One is flat in array and the other not? Result is empty\/\"fall hard\".\n+      is_empty = (this_flat_in_array && other_not_flat_in_array) || (this_not_flat_in_array && other_flat_in_array);\n@@ -4365,1 +4675,2 @@\n-      this_type = other_type; \/\/ tinst is down; keep down man\n+      \/\/ this_type is empty while other_type is not. Take other_type.\n+      this_type = other_type;\n@@ -4367,0 +4678,1 @@\n+      flat_in_array = other_flat_in_array;\n@@ -4368,0 +4680,1 @@\n+      \/\/ other_type is empty while this_type is not. Take this_type.\n@@ -4369,1 +4682,1 @@\n-      other_xk = this_xk;\n+      flat_in_array = this_flat_in_array;\n@@ -4371,0 +4684,1 @@\n+      \/\/ this_type and other_type are both non-empty.\n@@ -4372,0 +4686,3 @@\n+      \/\/ Case (FiA-M)\n+      \/\/ Meeting two types below the center line: Only flat in array if both are.\n+      flat_in_array = this_flat_in_array && other_flat_in_array;\n@@ -4376,1 +4693,1 @@\n-  if (this_type->is_same_java_type_as(other_type)) {\n+  if (this_type->is_same_java_type_as(other_type) && !is_empty) {\n@@ -4382,0 +4699,1 @@\n+    res_flat_in_array = flat_in_array;\n@@ -4398,0 +4716,1 @@\n+  res_flat_in_array = this_flat_in_array && other_flat_in_array;\n@@ -4402,0 +4721,4 @@\n+template<class T> bool TypePtr::is_meet_subtype_of(const T* sub_type, const T* super_type) {\n+  return sub_type->is_meet_subtype_of(super_type) && !(super_type->flat_in_array() && sub_type->not_flat_in_array());\n+}\n+\n@@ -4409,1 +4732,0 @@\n-\n@@ -4418,1 +4740,1 @@\n-  return new TypeInstPtr(dual_ptr(), klass(), _interfaces, klass_is_exact(), const_oop(), dual_offset(), dual_instance_id(), dual_speculative(), dual_inline_depth());\n+  return new TypeInstPtr(dual_ptr(), klass(), _interfaces, klass_is_exact(), const_oop(), dual_offset(), flat_in_array(), dual_instance_id(), dual_speculative(), dual_inline_depth());\n@@ -4427,0 +4749,1 @@\n+    flat_in_array() == p->flat_in_array() &&\n@@ -4434,1 +4757,1 @@\n-  return klass()->hash() + TypeOopPtr::hash() + _interfaces->hash();\n+  return klass()->hash() + TypeOopPtr::hash() + _interfaces->hash() + (uint)flat_in_array();\n@@ -4488,5 +4811,1 @@\n-  if( _offset ) {               \/\/ Dump offset, if any\n-    if( _offset == OffsetBot )      st->print(\"+any\");\n-    else if( _offset == OffsetTop ) st->print(\"+unknown\");\n-    else st->print(\"+%d\", _offset);\n-  }\n+  _offset.dump2(st);\n@@ -4495,0 +4814,5 @@\n+\n+  if (flat_in_array() && !klass()->is_inlinetype()) {\n+    st->print(\" (flat in array)\");\n+  }\n+\n@@ -4507,1 +4831,1 @@\n-  return make(_ptr, klass(), _interfaces, klass_is_exact(), const_oop(), xadd_offset(offset),\n+  return make(_ptr, klass(), _interfaces, klass_is_exact(), const_oop(), xadd_offset(offset), flat_in_array(),\n@@ -4512,1 +4836,1 @@\n-  return make(_ptr, klass(), _interfaces, klass_is_exact(), const_oop(), offset,\n+  return make(_ptr, klass(), _interfaces, klass_is_exact(), const_oop(), Offset(offset), flat_in_array(),\n@@ -4521,1 +4845,1 @@\n-  return make(_ptr, klass(), _interfaces, klass_is_exact(), const_oop(), _offset,\n+  return make(_ptr, klass(), _interfaces, klass_is_exact(), const_oop(), _offset, flat_in_array(),\n@@ -4526,1 +4850,1 @@\n-  return make(_ptr, klass(), _interfaces, klass_is_exact(), const_oop(), _offset, _instance_id, speculative, _inline_depth);\n+  return make(_ptr, klass(), _interfaces, klass_is_exact(), const_oop(), _offset, flat_in_array(), _instance_id, speculative, _inline_depth);\n@@ -4533,1 +4857,1 @@\n-  return make(_ptr, klass(), _interfaces, klass_is_exact(), const_oop(), _offset, _instance_id, _speculative, depth);\n+  return make(_ptr, klass(), _interfaces, klass_is_exact(), const_oop(), _offset, flat_in_array(), _instance_id, _speculative, depth);\n@@ -4538,1 +4862,5 @@\n-  return make(_ptr, klass(), _interfaces, klass_is_exact(), const_oop(), _offset, instance_id, _speculative, _inline_depth);\n+  return make(_ptr, klass(), _interfaces, klass_is_exact(), const_oop(), _offset, flat_in_array(), instance_id, _speculative, _inline_depth);\n+}\n+\n+const TypeInstPtr *TypeInstPtr::cast_to_flat_in_array() const {\n+  return make(_ptr, klass(), _interfaces, klass_is_exact(), const_oop(), _offset, true, _instance_id, _speculative, _inline_depth);\n@@ -4552,1 +4880,1 @@\n-  return TypeInstKlassPtr::make(xk ? TypePtr::Constant : TypePtr::NotNull, klass(), _interfaces, 0);\n+  return TypeInstKlassPtr::make(xk ? TypePtr::Constant : TypePtr::NotNull, klass(), _interfaces, Offset(0), flat_in_array());\n@@ -4597,1 +4925,0 @@\n-\n@@ -4620,10 +4947,11 @@\n-const TypeAryPtr* TypeAryPtr::RANGE;\n-const TypeAryPtr* TypeAryPtr::OOPS;\n-const TypeAryPtr* TypeAryPtr::NARROWOOPS;\n-const TypeAryPtr* TypeAryPtr::BYTES;\n-const TypeAryPtr* TypeAryPtr::SHORTS;\n-const TypeAryPtr* TypeAryPtr::CHARS;\n-const TypeAryPtr* TypeAryPtr::INTS;\n-const TypeAryPtr* TypeAryPtr::LONGS;\n-const TypeAryPtr* TypeAryPtr::FLOATS;\n-const TypeAryPtr* TypeAryPtr::DOUBLES;\n+const TypeAryPtr *TypeAryPtr::RANGE;\n+const TypeAryPtr *TypeAryPtr::OOPS;\n+const TypeAryPtr *TypeAryPtr::NARROWOOPS;\n+const TypeAryPtr *TypeAryPtr::BYTES;\n+const TypeAryPtr *TypeAryPtr::SHORTS;\n+const TypeAryPtr *TypeAryPtr::CHARS;\n+const TypeAryPtr *TypeAryPtr::INTS;\n+const TypeAryPtr *TypeAryPtr::LONGS;\n+const TypeAryPtr *TypeAryPtr::FLOATS;\n+const TypeAryPtr *TypeAryPtr::DOUBLES;\n+const TypeAryPtr *TypeAryPtr::INLINES;\n@@ -4632,1 +4960,1 @@\n-const TypeAryPtr *TypeAryPtr::make(PTR ptr, const TypeAry *ary, ciKlass* k, bool xk, int offset,\n+const TypeAryPtr* TypeAryPtr::make(PTR ptr, const TypeAry *ary, ciKlass* k, bool xk, Offset offset, Offset field_offset,\n@@ -4642,1 +4970,1 @@\n-  return (TypeAryPtr*)(new TypeAryPtr(ptr, nullptr, ary, k, xk, offset, instance_id, false, speculative, inline_depth))->hashcons();\n+  return (TypeAryPtr*)(new TypeAryPtr(ptr, nullptr, ary, k, xk, offset, field_offset, instance_id, false, speculative, inline_depth))->hashcons();\n@@ -4646,1 +4974,1 @@\n-const TypeAryPtr *TypeAryPtr::make(PTR ptr, ciObject* o, const TypeAry *ary, ciKlass* k, bool xk, int offset,\n+const TypeAryPtr* TypeAryPtr::make(PTR ptr, ciObject* o, const TypeAry *ary, ciKlass* k, bool xk, Offset offset, Offset field_offset,\n@@ -4658,1 +4986,1 @@\n-  return (TypeAryPtr*)(new TypeAryPtr(ptr, o, ary, k, xk, offset, instance_id, is_autobox_cache, speculative, inline_depth))->hashcons();\n+  return (TypeAryPtr*)(new TypeAryPtr(ptr, o, ary, k, xk, offset, field_offset, instance_id, is_autobox_cache, speculative, inline_depth))->hashcons();\n@@ -4664,1 +4992,1 @@\n-  return make(ptr, ptr == Constant ? const_oop() : nullptr, _ary, klass(), klass_is_exact(), _offset, _instance_id, _speculative, _inline_depth);\n+  return make(ptr, ptr == Constant ? const_oop() : nullptr, _ary, klass(), klass_is_exact(), _offset, _field_offset, _instance_id, _speculative, _inline_depth, _is_autobox_cache);\n@@ -4672,1 +5000,1 @@\n-  return make(ptr(), const_oop(), _ary, klass(), klass_is_exact, _offset, _instance_id, _speculative, _inline_depth);\n+  return make(ptr(), const_oop(), _ary, klass(), klass_is_exact, _offset, _field_offset, _instance_id, _speculative, _inline_depth, _is_autobox_cache);\n@@ -4678,1 +5006,1 @@\n-  return make(_ptr, const_oop(), _ary, klass(), _klass_is_exact, _offset, instance_id, _speculative, _inline_depth);\n+  return make(_ptr, const_oop(), _ary, klass(), _klass_is_exact, _offset, _field_offset, instance_id, _speculative, _inline_depth, _is_autobox_cache);\n@@ -4736,2 +5064,65 @@\n-  const TypeAry* new_ary = TypeAry::make(elem(), new_size, is_stable());\n-  return make(ptr(), const_oop(), new_ary, klass(), klass_is_exact(), _offset, _instance_id, _speculative, _inline_depth);\n+  const TypeAry* new_ary = TypeAry::make(elem(), new_size, is_stable(), is_flat(), is_not_flat(), is_not_null_free(), is_atomic());\n+  return make(ptr(), const_oop(), new_ary, klass(), klass_is_exact(), _offset, _field_offset, _instance_id, _speculative, _inline_depth, _is_autobox_cache);\n+}\n+\n+\/\/-------------------------------cast_to_not_flat------------------------------\n+const TypeAryPtr* TypeAryPtr::cast_to_not_flat(bool not_flat) const {\n+  if (not_flat == is_not_flat()) {\n+    return this;\n+  }\n+  assert(!not_flat || !is_flat(), \"inconsistency\");\n+  const TypeAry* new_ary = TypeAry::make(elem(), size(), is_stable(), is_flat(), not_flat, is_not_null_free(), is_atomic());\n+  const TypeAryPtr* res = make(ptr(), const_oop(), new_ary, klass(), klass_is_exact(), _offset, _field_offset, _instance_id, _speculative, _inline_depth, _is_autobox_cache);\n+  \/\/ We keep the speculative part if it contains information about flat-\/nullability.\n+  \/\/ Make sure it's removed if it's not better than the non-speculative type anymore.\n+  if (res->speculative() == res->remove_speculative()) {\n+    return res->remove_speculative();\n+  }\n+  return res;\n+}\n+\n+\/\/-------------------------------cast_to_not_null_free-------------------------\n+const TypeAryPtr* TypeAryPtr::cast_to_not_null_free(bool not_null_free) const {\n+  if (not_null_free == is_not_null_free()) {\n+    return this;\n+  }\n+  assert(!not_null_free || !is_null_free(), \"inconsistency\");\n+  const TypeAry* new_ary = TypeAry::make(elem(), size(), is_stable(), is_flat(), is_not_flat(), not_null_free, is_atomic());\n+  const TypeAryPtr* res = make(ptr(), const_oop(), new_ary, klass(), klass_is_exact(), _offset, _field_offset,\n+                               _instance_id, _speculative, _inline_depth, _is_autobox_cache);\n+  \/\/ We keep the speculative part if it contains information about flat-\/nullability.\n+  \/\/ Make sure it's removed if it's not better than the non-speculative type anymore.\n+  if (res->speculative() == res->remove_speculative()) {\n+    return res->remove_speculative();\n+  }\n+  return res;\n+}\n+\n+\/\/---------------------------------update_properties---------------------------\n+const TypeAryPtr* TypeAryPtr::update_properties(const TypeAryPtr* from) const {\n+  if ((from->is_flat()          && is_not_flat()) ||\n+      (from->is_not_flat()      && is_flat()) ||\n+      (from->is_null_free()     && is_not_null_free()) ||\n+      (from->is_not_null_free() && is_null_free())) {\n+    return nullptr; \/\/ Inconsistent properties\n+  }\n+  const TypeAryPtr* res = this;\n+  if (from->is_not_null_free()) {\n+    res = res->cast_to_not_null_free();\n+  }\n+  if (from->is_not_flat()) {\n+    res = res->cast_to_not_flat();\n+  }\n+  return res;\n+}\n+\n+jint TypeAryPtr::flat_layout_helper() const {\n+  return exact_klass()->as_flat_array_klass()->layout_helper();\n+}\n+\n+int TypeAryPtr::flat_elem_size() const {\n+  return exact_klass()->as_flat_array_klass()->element_byte_size();\n+}\n+\n+int TypeAryPtr::flat_log_elem_size() const {\n+  return exact_klass()->as_flat_array_klass()->log2_element_size();\n@@ -4753,1 +5144,1 @@\n-  const TypeAry* new_ary = TypeAry::make(elem, size(), stable);\n+  const TypeAry* new_ary = TypeAry::make(elem, size(), stable, is_flat(), is_not_flat(), is_not_null_free(), is_atomic());\n@@ -4755,1 +5146,1 @@\n-  return make(ptr(), const_oop(), new_ary, klass(), klass_is_exact(), _offset, _instance_id, _speculative, _inline_depth);\n+  return make(ptr(), const_oop(), new_ary, klass(), klass_is_exact(), _offset, _field_offset, _instance_id, _speculative, _inline_depth, _is_autobox_cache);\n@@ -4775,2 +5166,2 @@\n-  const TypeAry* new_ary = TypeAry::make(etype, size(), is_stable());\n-  return make(ptr(), const_oop(), new_ary, klass(), klass_is_exact(), _offset, _instance_id, _speculative, _inline_depth, \/*is_autobox_cache=*\/true);\n+  const TypeAry* new_ary = TypeAry::make(etype, size(), is_stable(), is_flat(), is_not_flat(), is_not_null_free(), is_atomic());\n+  return make(ptr(), const_oop(), new_ary, klass(), klass_is_exact(), _offset, _field_offset, _instance_id, _speculative, _inline_depth, \/*is_autobox_cache=*\/true);\n@@ -4785,1 +5176,2 @@\n-    TypeOopPtr::eq(p);  \/\/ Check sub-parts\n+    TypeOopPtr::eq(p) &&\/\/ Check sub-parts\n+    _field_offset == p->_field_offset;\n@@ -4791,1 +5183,1 @@\n-  return (uint)(uintptr_t)_ary + TypeOopPtr::hash();\n+  return (uint)(uintptr_t)_ary + TypeOopPtr::hash() + _field_offset.get();\n@@ -4838,1 +5230,1 @@\n-    int offset = meet_offset(tp->offset());\n+    Offset offset = meet_offset(tp->offset());\n@@ -4847,1 +5239,1 @@\n-                  _ary, _klass, _klass_is_exact, offset, instance_id, speculative, depth);\n+                  _ary, _klass, _klass_is_exact, offset, _field_offset, instance_id, speculative, depth);\n@@ -4861,1 +5253,1 @@\n-    int offset = meet_offset(tp->offset());\n+    Offset offset = meet_offset(tp->offset());\n@@ -4877,1 +5269,1 @@\n-                  _ary, _klass, _klass_is_exact, offset, instance_id, speculative, depth);\n+                  _ary, _klass, _klass_is_exact, offset, _field_offset, instance_id, speculative, depth);\n@@ -4891,1 +5283,2 @@\n-    int off = meet_offset(tap->offset());\n+    Offset off = meet_offset(tap->offset());\n+    Offset field_off = meet_field_offset(tap->field_offset());\n@@ -4905,0 +5298,4 @@\n+    bool res_flat = false;\n+    bool res_not_flat = false;\n+    bool res_not_null_free = false;\n+    bool res_atomic = false;\n@@ -4906,1 +5303,1 @@\n-    if (meet_aryptr(ptr, elem, this, tap, res_klass, res_xk) == NOT_SUBTYPE) {\n+    if (meet_aryptr(ptr, elem, this, tap, res_klass, res_xk, res_flat, res_not_flat, res_not_null_free, res_atomic) == NOT_SUBTYPE) {\n@@ -4908,0 +5305,14 @@\n+    } else if (this->is_flat() != tap->is_flat()) {\n+      \/\/ Meeting flat inline type array with non-flat array. Adjust (field) offset accordingly.\n+      if (tary->_flat) {\n+        \/\/ Result is in a flat representation\n+        off = Offset(is_flat() ? offset() : tap->offset());\n+        field_off = is_flat() ? field_offset() : tap->field_offset();\n+      } else if (below_centerline(ptr)) {\n+        \/\/ Result is in a non-flat representation\n+        off = Offset(flat_offset()).meet(Offset(tap->flat_offset()));\n+        field_off = (field_off == Offset::top) ? Offset::top : Offset::bottom;\n+      } else if (flat_offset() == tap->flat_offset()) {\n+        off = Offset(!is_flat() ? offset() : tap->offset());\n+        field_off = !is_flat() ? field_offset() : tap->field_offset();\n+      }\n@@ -4925,1 +5336,1 @@\n-    return make(ptr, o, TypeAry::make(elem, tary->_size, tary->_stable), res_klass, res_xk, off, instance_id, speculative, depth);\n+    return make(ptr, o, TypeAry::make(elem, tary->_size, tary->_stable, res_flat, res_not_flat, res_not_null_free, res_atomic), res_klass, res_xk, off, field_off, instance_id, speculative, depth);\n@@ -4931,1 +5342,1 @@\n-    int offset = meet_offset(tp->offset());\n+    Offset offset = meet_offset(tp->offset());\n@@ -4946,2 +5357,2 @@\n-      if (tp->klass()->equals(ciEnv::current()->Object_klass()) && this_interfaces->contains(tp_interfaces) && !tp->klass_is_exact()) {\n-        return TypeAryPtr::make(ptr, _ary, _klass, _klass_is_exact, offset, instance_id, speculative, depth);\n+      if (tp->klass()->equals(ciEnv::current()->Object_klass()) && this_interfaces->contains(tp_interfaces) && !tp->klass_is_exact() && !tp->flat_in_array()) {\n+        return TypeAryPtr::make(ptr, _ary, _klass, _klass_is_exact, offset, _field_offset, instance_id, speculative, depth);\n@@ -4953,1 +5364,1 @@\n-        return TypeInstPtr::make(ptr, ciEnv::current()->Object_klass(), interfaces, false, nullptr,offset, instance_id, speculative, depth);\n+        return TypeInstPtr::make(ptr, ciEnv::current()->Object_klass(), interfaces, false, nullptr, offset, false, instance_id, speculative, depth);\n@@ -4965,1 +5376,1 @@\n-        if (tp->klass()->equals(ciEnv::current()->Object_klass()) && this_interfaces->contains(tp_interfaces) && !tp->klass_is_exact()) {\n+        if (tp->klass()->equals(ciEnv::current()->Object_klass()) && this_interfaces->contains(tp_interfaces) && !tp->klass_is_exact() && !tp->flat_in_array()) {\n@@ -4968,1 +5379,1 @@\n-                      _ary, _klass, _klass_is_exact, offset, instance_id, speculative, depth);\n+                      _ary, _klass, _klass_is_exact, offset, _field_offset, instance_id, speculative, depth);\n@@ -4980,1 +5391,1 @@\n-      return TypeInstPtr::make(ptr, ciEnv::current()->Object_klass(), interfaces, false, nullptr, offset, instance_id, speculative, depth);\n+      return TypeInstPtr::make(ptr, ciEnv::current()->Object_klass(), interfaces, false, nullptr, offset, false, instance_id, speculative, depth);\n@@ -4989,2 +5400,2 @@\n-template<class T> TypePtr::MeetResult TypePtr::meet_aryptr(PTR& ptr, const Type*& elem, const T* this_ary,\n-                                                           const T* other_ary, ciKlass*& res_klass, bool& res_xk) {\n+template<class T> TypePtr::MeetResult TypePtr::meet_aryptr(PTR& ptr, const Type*& elem, const T* this_ary, const T* other_ary,\n+                                                           ciKlass*& res_klass, bool& res_xk, bool &res_flat, bool& res_not_flat, bool& res_not_null_free, bool &res_atomic) {\n@@ -5000,0 +5411,9 @@\n+  bool this_flat = this_ary->is_flat();\n+  bool this_not_flat = this_ary->is_not_flat();\n+  bool other_flat = other_ary->is_flat();\n+  bool other_not_flat = other_ary->is_not_flat();\n+  bool this_not_null_free = this_ary->is_not_null_free();\n+  bool other_not_null_free = other_ary->is_not_null_free();\n+  bool this_atomic = this_ary->is_atomic();\n+  bool other_atomic = other_ary->is_atomic();\n+  const bool same_nullness = this_ary->is_null_free() == other_ary->is_null_free();\n@@ -5002,0 +5422,6 @@\n+  res_flat = this_flat && other_flat;\n+  bool res_null_free = this_ary->is_null_free() && other_ary->is_null_free();\n+  res_not_flat = this_not_flat && other_not_flat;\n+  res_not_null_free = this_not_null_free && other_not_null_free;\n+  res_atomic = this_atomic && other_atomic;\n+\n@@ -5005,3 +5431,3 @@\n-    if (this_top_or_bottom)\n-      res_klass = other_klass;\n-    else if (other_top_or_bottom || other_klass == this_klass) {\n+      if (this_top_or_bottom) {\n+        res_klass = other_klass;\n+      } else if (other_top_or_bottom || other_klass == this_klass) {\n@@ -5049,0 +5475,3 @@\n+        if (this_ary->is_flat()) {\n+          elem = this_ary->elem();\n+        }\n@@ -5052,1 +5481,1 @@\n-      return result;\n+      break;\n@@ -5054,1 +5483,3 @@\n-      if (this_ptr == Constant) {\n+      if (this_ptr == Constant && same_nullness) {\n+        \/\/ Only exact if same nullness since:\n+        \/\/     null-free [LMyValue <: nullable [LMyValue.\n@@ -5056,1 +5487,1 @@\n-      } else if(above_centerline(this_ptr)) {\n+      } else if (above_centerline(this_ptr)) {\n@@ -5061,0 +5492,6 @@\n+        \/\/ Even though MyValue is final, [LMyValue is only exact if the array\n+        \/\/ is (not) null-free due to null-free [LMyValue <: null-able [LMyValue.\n+        if (res_xk && !res_null_free && !res_not_null_free) {\n+          ptr = NotNull;\n+          res_xk = false;\n+        }\n@@ -5062,1 +5499,1 @@\n-      return result;\n+      break;\n@@ -5069,0 +5506,3 @@\n+        if (other_ary->is_flat()) {\n+          elem = other_ary->elem();\n+        }\n@@ -5072,0 +5512,6 @@\n+        \/\/ Even though MyValue is final, [LMyValue is only exact if the array\n+        \/\/ is (not) null-free due to null-free [LMyValue <: null-able [LMyValue.\n+        if (res_xk && !res_null_free && !res_not_null_free) {\n+          ptr = NotNull;\n+          res_xk = false;\n+        }\n@@ -5073,1 +5519,1 @@\n-      return result;\n+      break;\n@@ -5086,1 +5532,11 @@\n-  return new TypeAryPtr(dual_ptr(), _const_oop, _ary->dual()->is_ary(),_klass, _klass_is_exact, dual_offset(), dual_instance_id(), is_autobox_cache(), dual_speculative(), dual_inline_depth());\n+  bool xk = _klass_is_exact;\n+  return new TypeAryPtr(dual_ptr(), _const_oop, _ary->dual()->is_ary(), _klass, xk, dual_offset(), dual_field_offset(), dual_instance_id(), is_autobox_cache(), dual_speculative(), dual_inline_depth());\n+}\n+\n+Type::Offset TypeAryPtr::meet_field_offset(const Type::Offset offset) const {\n+  return _field_offset.meet(offset);\n+}\n+\n+\/\/------------------------------dual_offset------------------------------------\n+Type::Offset TypeAryPtr::dual_field_offset() const {\n+  return _field_offset.dual();\n@@ -5114,1 +5570,23 @@\n-  if( _offset != 0 ) {\n+  if (is_flat()) {\n+    st->print(\":flat\");\n+    st->print(\"(\");\n+    _field_offset.dump2(st);\n+    st->print(\")\");\n+  } else if (is_not_flat()) {\n+    st->print(\":not_flat\");\n+  }\n+  if (is_null_free()) {\n+    st->print(\":null free\");\n+  }\n+  if (is_atomic()) {\n+    st->print(\":atomic\");\n+  }\n+  if (Verbose) {\n+    if (is_not_flat()) {\n+      st->print(\":not flat\");\n+    }\n+    if (is_not_null_free()) {\n+      st->print(\":nullable\");\n+    }\n+  }\n+  if (offset() != 0) {\n@@ -5117,3 +5595,3 @@\n-    if( _offset == OffsetTop )       st->print(\"+undefined\");\n-    else if( _offset == OffsetBot )  st->print(\"+any\");\n-    else if( _offset < header_size ) st->print(\"+%d\", _offset);\n+    if( _offset == Offset::top )       st->print(\"+undefined\");\n+    else if( _offset == Offset::bottom )  st->print(\"+any\");\n+    else if( offset() < header_size ) st->print(\"+%d\", offset());\n@@ -5125,1 +5603,1 @@\n-        st->print(\"[%d]\", (_offset - header_size)\/elem_size);\n+        st->print(\"[%d]\", (offset() - header_size)\/elem_size);\n@@ -5142,0 +5620,4 @@\n+  \/\/ FIXME: Does this belong here? Or in the meet code itself?\n+  if (is_flat() && is_not_flat()) {\n+    return true;\n+  }\n@@ -5147,1 +5629,1 @@\n-  return make(_ptr, _const_oop, _ary, _klass, _klass_is_exact, xadd_offset(offset), _instance_id, add_offset_speculative(offset), _inline_depth);\n+  return make(_ptr, _const_oop, _ary, _klass, _klass_is_exact, xadd_offset(offset), _field_offset, _instance_id, add_offset_speculative(offset), _inline_depth, _is_autobox_cache);\n@@ -5151,1 +5633,1 @@\n-  return make(_ptr, _const_oop, _ary, _klass, _klass_is_exact, offset, _instance_id, with_offset_speculative(offset), _inline_depth);\n+  return make(_ptr, _const_oop, _ary, _klass, _klass_is_exact, Offset(offset), _field_offset, _instance_id, with_offset_speculative(offset), _inline_depth, _is_autobox_cache);\n@@ -5155,1 +5637,1 @@\n-  return make(_ptr, _const_oop, ary, _klass, _klass_is_exact, _offset, _instance_id, _speculative, _inline_depth);\n+  return make(_ptr, _const_oop, ary, _klass, _klass_is_exact, _offset, _field_offset, _instance_id, _speculative, _inline_depth, _is_autobox_cache);\n@@ -5163,1 +5645,14 @@\n-  return make(_ptr, _const_oop, _ary->remove_speculative()->is_ary(), _klass, _klass_is_exact, _offset, _instance_id, nullptr, _inline_depth);\n+  return make(_ptr, _const_oop, _ary->remove_speculative()->is_ary(), _klass, _klass_is_exact, _offset, _field_offset, _instance_id, nullptr, _inline_depth, _is_autobox_cache);\n+}\n+\n+const Type* TypeAryPtr::cleanup_speculative() const {\n+  if (speculative() == nullptr) {\n+    return this;\n+  }\n+  \/\/ Keep speculative part if it contains information about flat-\/nullability\n+  const TypeAryPtr* spec_aryptr = speculative()->isa_aryptr();\n+  if (spec_aryptr != nullptr && !above_centerline(spec_aryptr->ptr()) &&\n+      (spec_aryptr->is_not_flat() || spec_aryptr->is_not_null_free())) {\n+    return this;\n+  }\n+  return TypeOopPtr::cleanup_speculative();\n@@ -5170,1 +5665,44 @@\n-  return make(_ptr, _const_oop, _ary->remove_speculative()->is_ary(), _klass, _klass_is_exact, _offset, _instance_id, _speculative, depth);\n+  return make(_ptr, _const_oop, _ary->remove_speculative()->is_ary(), _klass, _klass_is_exact, _offset, _field_offset, _instance_id, _speculative, depth, _is_autobox_cache);\n+}\n+\n+const TypeAryPtr* TypeAryPtr::with_field_offset(int offset) const {\n+  return make(_ptr, _const_oop, _ary->remove_speculative()->is_ary(), _klass, _klass_is_exact, _offset, Offset(offset), _instance_id, _speculative, _inline_depth, _is_autobox_cache);\n+}\n+\n+const TypePtr* TypeAryPtr::add_field_offset_and_offset(intptr_t offset) const {\n+  int adj = 0;\n+  if (is_flat() && klass_is_exact() && offset != Type::OffsetBot && offset != Type::OffsetTop) {\n+    if (_offset.get() != OffsetBot && _offset.get() != OffsetTop) {\n+      adj = _offset.get();\n+      offset += _offset.get();\n+    }\n+    uint header = arrayOopDesc::base_offset_in_bytes(T_FLAT_ELEMENT);\n+    if (_field_offset.get() != OffsetBot && _field_offset.get() != OffsetTop) {\n+      offset += _field_offset.get();\n+      if (_offset.get() == OffsetBot || _offset.get() == OffsetTop) {\n+        offset += header;\n+      }\n+    }\n+    if (elem()->make_oopptr()->is_inlinetypeptr() && (offset >= (intptr_t)header || offset < 0)) {\n+      \/\/ Try to get the field of the inline type array element we are pointing to\n+      ciInlineKlass* vk = elem()->inline_klass();\n+      int shift = flat_log_elem_size();\n+      int mask = (1 << shift) - 1;\n+      intptr_t field_offset = ((offset - header) & mask);\n+      ciField* field = vk->get_field_by_offset(field_offset + vk->payload_offset(), false);\n+      if (field != nullptr || field_offset == vk->null_marker_offset_in_payload()) {\n+        return with_field_offset(field_offset)->add_offset(offset - field_offset - adj);\n+      }\n+    }\n+  }\n+  return add_offset(offset - adj);\n+}\n+\n+\/\/ Return offset incremented by field_offset for flat inline type arrays\n+int TypeAryPtr::flat_offset() const {\n+  int offset = _offset.get();\n+  if (offset != Type::OffsetBot && offset != Type::OffsetTop &&\n+      _field_offset != Offset::bottom && _field_offset != Offset::top) {\n+    offset += _field_offset.get();\n+  }\n+  return offset;\n@@ -5175,1 +5713,1 @@\n-  return make(_ptr, _const_oop, _ary->remove_speculative()->is_ary(), _klass, _klass_is_exact, _offset, instance_id, _speculative, _inline_depth);\n+  return make(_ptr, _const_oop, _ary->remove_speculative()->is_ary(), _klass, _klass_is_exact, _offset, _field_offset, instance_id, _speculative, _inline_depth);\n@@ -5180,0 +5718,1 @@\n+\n@@ -5273,1 +5812,0 @@\n-\n@@ -5357,1 +5895,1 @@\n-  return (_offset == 0) && !below_centerline(_ptr);\n+  return (offset() == 0) && !below_centerline(_ptr);\n@@ -5377,1 +5915,1 @@\n-  assert( _offset >= 0, \"\" );\n+  assert(offset() >= 0, \"\");\n@@ -5379,1 +5917,1 @@\n-  if (_offset != 0) {\n+  if (offset() != 0) {\n@@ -5433,1 +5971,1 @@\n-    int offset = meet_offset(tp->offset());\n+    Offset offset = meet_offset(tp->offset());\n@@ -5461,1 +5999,1 @@\n-    int offset = meet_offset(tp->offset());\n+    Offset offset = meet_offset(tp->offset());\n@@ -5494,1 +6032,1 @@\n-  switch( _offset ) {\n+  switch (offset()) {\n@@ -5498,1 +6036,1 @@\n-  default:        st->print(\"+%d\",_offset); break;\n+  default:        st->print(\"+%d\",offset()); break;\n@@ -5508,1 +6046,1 @@\n-TypeMetadataPtr::TypeMetadataPtr(PTR ptr, ciMetadata* metadata, int offset):\n+TypeMetadataPtr::TypeMetadataPtr(PTR ptr, ciMetadata* metadata, Offset offset):\n@@ -5513,1 +6051,1 @@\n-  return make(Constant, m, 0);\n+  return make(Constant, m, Offset(0));\n@@ -5516,1 +6054,1 @@\n-  return make(Constant, m, 0);\n+  return make(Constant, m, Offset(0));\n@@ -5521,1 +6059,1 @@\n-const TypeMetadataPtr *TypeMetadataPtr::make(PTR ptr, ciMetadata* m, int offset) {\n+const TypeMetadataPtr* TypeMetadataPtr::make(PTR ptr, ciMetadata* m, Offset offset) {\n@@ -5532,1 +6070,6 @@\n-    if (elem->is_klassptr()->klass_is_exact()) {\n+    if (elem->is_klassptr()->klass_is_exact() &&\n+        \/\/ Even though MyValue is final, [LMyValue is only exact if the array\n+        \/\/ is (not) null-free due to null-free [LMyValue <: null-able [LMyValue.\n+        \/\/ TODO 8350865 If we know that the array can't be null-free, it's allowed to be exact, right?\n+        \/\/ If so, we should add '|| is_not_null_free()'\n+        (is_null_free() || !_ary->_elem->make_oopptr()->is_inlinetypeptr())) {\n@@ -5536,1 +6079,1 @@\n-  return TypeAryKlassPtr::make(xk ? TypePtr::Constant : TypePtr::NotNull, elem, klass(), 0);\n+  return TypeAryKlassPtr::make(xk ? TypePtr::Constant : TypePtr::NotNull, elem, klass(), Offset(0), is_not_flat(), is_not_null_free(), is_flat(), is_null_free(), is_atomic(), is_flat() || is_null_free());\n@@ -5539,1 +6082,1 @@\n-const TypeKlassPtr* TypeKlassPtr::make(ciKlass *klass, InterfaceHandling interface_handling) {\n+const TypeKlassPtr* TypeKlassPtr::make(ciKlass* klass, InterfaceHandling interface_handling) {\n@@ -5546,1 +6089,1 @@\n-const TypeKlassPtr* TypeKlassPtr::make(PTR ptr, ciKlass* klass, int offset, InterfaceHandling interface_handling) {\n+const TypeKlassPtr* TypeKlassPtr::make(PTR ptr, ciKlass* klass, Offset offset, InterfaceHandling interface_handling) {\n@@ -5554,3 +6097,1 @@\n-\n-\/\/------------------------------TypeKlassPtr-----------------------------------\n-TypeKlassPtr::TypeKlassPtr(TYPES t, PTR ptr, ciKlass* klass, const TypeInterfaces* interfaces, int offset)\n+TypeKlassPtr::TypeKlassPtr(TYPES t, PTR ptr, ciKlass* klass, const TypeInterfaces* interfaces, Offset offset)\n@@ -5559,1 +6100,1 @@\n-         klass->is_type_array_klass() || !klass->as_obj_array_klass()->base_element_klass()->is_interface(), \"no interface here\");\n+         klass->is_type_array_klass() || klass->is_flat_array_klass() || !klass->as_obj_array_klass()->base_element_klass()->is_interface(), \"no interface here\");\n@@ -5598,1 +6139,1 @@\n-  return (_offset == 0) && !below_centerline(_ptr);\n+  return (offset() == 0) && !below_centerline(_ptr);\n@@ -5628,1 +6169,1 @@\n-  assert( _offset >= 0, \"\" );\n+  assert( offset() >= 0, \"\" );\n@@ -5630,1 +6171,1 @@\n-  if (_offset != 0) {\n+  if (offset() != 0) {\n@@ -5674,5 +6215,2 @@\n-\n-  if (_offset) {               \/\/ Dump offset, if any\n-    if (_offset == OffsetBot)      { st->print(\"+any\"); }\n-    else if (_offset == OffsetTop) { st->print(\"+unknown\"); }\n-    else                            { st->print(\"+%d\", _offset); }\n+  if (Verbose) {\n+    if (isa_instklassptr() && is_instklassptr()->flat_in_array()) st->print(\":flat in array\");\n@@ -5680,1 +6218,1 @@\n-\n+  _offset.dump2(st);\n@@ -5682,0 +6220,4 @@\n+\n+  if (flat_in_array() && !klass()->is_inlinetype()) {\n+    st->print(\" (flat in array)\");\n+  }\n@@ -5696,0 +6238,1 @@\n+    flat_in_array() == p->flat_in_array() &&\n@@ -5700,1 +6243,1 @@\n-  return klass()->hash() + TypeKlassPtr::hash();\n+  return klass()->hash() + TypeKlassPtr::hash() + (uint)flat_in_array();\n@@ -5703,1 +6246,3 @@\n-const TypeInstKlassPtr *TypeInstKlassPtr::make(PTR ptr, ciKlass* k, const TypeInterfaces* interfaces, int offset) {\n+const TypeInstKlassPtr *TypeInstKlassPtr::make(PTR ptr, ciKlass* k, const TypeInterfaces* interfaces, Offset offset, bool flat_in_array) {\n+  flat_in_array = flat_in_array || k->maybe_flat_in_array();\n+\n@@ -5705,1 +6250,1 @@\n-    (TypeInstKlassPtr*)(new TypeInstKlassPtr(ptr, k, interfaces, offset))->hashcons();\n+    (TypeInstKlassPtr*)(new TypeInstKlassPtr(ptr, k, interfaces, offset, flat_in_array))->hashcons();\n@@ -5712,2 +6257,2 @@\n-const TypePtr* TypeInstKlassPtr::add_offset( intptr_t offset ) const {\n-  return make( _ptr, klass(), _interfaces, xadd_offset(offset) );\n+const TypePtr *TypeInstKlassPtr::add_offset( intptr_t offset ) const {\n+  return make(_ptr, klass(), _interfaces, xadd_offset(offset), flat_in_array());\n@@ -5717,1 +6262,1 @@\n-  return make(_ptr, klass(), _interfaces, offset);\n+  return make(_ptr, klass(), _interfaces, Offset(offset), flat_in_array());\n@@ -5724,1 +6269,1 @@\n-  return make(ptr, _klass, _interfaces, _offset);\n+  return make(ptr, _klass, _interfaces, _offset, flat_in_array());\n@@ -5740,1 +6285,1 @@\n-  return make(klass_is_exact ? Constant : NotNull, k, _interfaces, _offset);\n+  return make(klass_is_exact ? Constant : NotNull, k, _interfaces, _offset, flat_in_array());\n@@ -5772,1 +6317,1 @@\n-  return TypeInstPtr::make(TypePtr::BotPTR, k, interfaces, xk, nullptr, 0);\n+  return TypeInstPtr::make(TypePtr::BotPTR, k, interfaces, xk, nullptr, Offset(0), flat_in_array() && !klass()->is_inlinetype());\n@@ -5808,1 +6353,1 @@\n-    int offset = meet_offset(tp->offset());\n+    Offset offset = meet_offset(tp->offset());\n@@ -5816,1 +6361,1 @@\n-      return make( ptr, klass(), _interfaces, offset );\n+      return make(ptr, klass(), _interfaces, offset, flat_in_array());\n@@ -5829,1 +6374,1 @@\n-    return TypePtr::BOTTOM;\n+      return TypePtr::BOTTOM;\n@@ -5849,1 +6394,1 @@\n-    int  off     = meet_offset(tkls->offset());\n+    Offset  off     = meet_offset(tkls->offset());\n@@ -5855,1 +6400,2 @@\n-    switch(meet_instptr(ptr, interfaces, this, tkls, res_klass, res_xk)) {\n+    bool res_flat_in_array = false;\n+    switch(meet_instptr(ptr, interfaces, this, tkls, res_klass, res_xk, res_flat_in_array)) {\n@@ -5863,1 +6409,1 @@\n-        const Type* res = make(ptr, res_klass, interfaces, off);\n+        const Type* res = make(ptr, res_klass, interfaces, off, res_flat_in_array);\n@@ -5872,1 +6418,1 @@\n-    int offset = meet_offset(tp->offset());\n+    Offset offset = meet_offset(tp->offset());\n@@ -5885,1 +6431,1 @@\n-        return TypeAryKlassPtr::make(ptr, tp->elem(), tp->klass(), offset);\n+        return TypeAryKlassPtr::make(ptr, tp->elem(), tp->klass(), offset, tp->is_not_flat(), tp->is_not_null_free(), tp->is_flat(), tp->is_null_free(), tp->is_atomic(), tp->is_refined_type());\n@@ -5890,1 +6436,1 @@\n-        return make(ptr, ciEnv::current()->Object_klass(), interfaces, offset);\n+        return make(ptr, ciEnv::current()->Object_klass(), interfaces, offset, false);\n@@ -5904,2 +6450,1 @@\n-          return TypeAryKlassPtr::make(ptr,\n-                                       tp->elem(), tp->klass(), offset);\n+          return TypeAryKlassPtr::make(ptr, tp->elem(), tp->klass(), offset, tp->is_not_flat(), tp->is_not_null_free(), tp->is_flat(), tp->is_null_free(), tp->is_atomic(), tp->is_refined_type());\n@@ -5913,1 +6458,1 @@\n-      return make(ptr, ciEnv::current()->Object_klass(), interfaces, offset);\n+      return make(ptr, ciEnv::current()->Object_klass(), interfaces, offset, false);\n@@ -5925,1 +6470,1 @@\n-  return new TypeInstKlassPtr(dual_ptr(), klass(), _interfaces, dual_offset());\n+  return new TypeInstKlassPtr(dual_ptr(), klass(), _interfaces, dual_offset(), flat_in_array());\n@@ -6039,0 +6584,3 @@\n+bool TypeInstKlassPtr::can_be_inline_array() const {\n+  return _klass->equals(ciEnv::current()->Object_klass()) && TypeAryKlassPtr::_array_interfaces->contains(_interfaces);\n+}\n@@ -6040,2 +6588,2 @@\n-const TypeAryKlassPtr *TypeAryKlassPtr::make(PTR ptr, const Type* elem, ciKlass* k, int offset) {\n-  return (TypeAryKlassPtr*)(new TypeAryKlassPtr(ptr, elem, k, offset))->hashcons();\n+bool TypeAryKlassPtr::can_be_inline_array() const {\n+  return _elem->isa_instklassptr() && _elem->is_instklassptr()->_klass->can_be_inline_klass();\n@@ -6044,1 +6592,13 @@\n-const TypeAryKlassPtr *TypeAryKlassPtr::make(PTR ptr, ciKlass* k, int offset, InterfaceHandling interface_handling) {\n+bool TypeInstPtr::can_be_inline_array() const {\n+  return _klass->equals(ciEnv::current()->Object_klass()) && TypeAryPtr::_array_interfaces->contains(_interfaces);\n+}\n+\n+bool TypeAryPtr::can_be_inline_array() const {\n+  return elem()->make_ptr() && elem()->make_ptr()->isa_instptr() && elem()->make_ptr()->is_instptr()->_klass->can_be_inline_klass();\n+}\n+\n+const TypeAryKlassPtr *TypeAryKlassPtr::make(PTR ptr, const Type* elem, ciKlass* k, Offset offset, bool not_flat, bool not_null_free, bool flat, bool null_free, bool atomic, bool refined_type) {\n+  return (TypeAryKlassPtr*)(new TypeAryKlassPtr(ptr, elem, k, offset, not_flat, not_null_free, flat, null_free, atomic, refined_type))->hashcons();\n+}\n+\n+const TypeAryKlassPtr* TypeAryKlassPtr::make(PTR ptr, ciKlass* k, Offset offset, InterfaceHandling interface_handling, bool not_flat, bool not_null_free, bool flat, bool null_free, bool atomic, bool refined_type) {\n@@ -6048,2 +6608,2 @@\n-    const TypeKlassPtr *etype = TypeKlassPtr::make(eklass, interface_handling)->cast_to_exactness(false);\n-    return TypeAryKlassPtr::make(ptr, etype, nullptr, offset);\n+    const TypeKlassPtr* etype = TypeKlassPtr::make(eklass, interface_handling)->cast_to_exactness(false);\n+    return TypeAryKlassPtr::make(ptr, etype, nullptr, offset, not_flat, not_null_free, flat, null_free, atomic, refined_type);\n@@ -6053,1 +6613,5 @@\n-    return TypeAryKlassPtr::make(ptr, etype, k, offset);\n+    return TypeAryKlassPtr::make(ptr, etype, k, offset, not_flat, not_null_free, flat, null_free, atomic);\n+  } else if (k->is_flat_array_klass()) {\n+    ciKlass* eklass = k->as_flat_array_klass()->element_klass();\n+    const TypeKlassPtr* etype = TypeKlassPtr::make(eklass, interface_handling)->cast_to_exactness(false);\n+    return TypeAryKlassPtr::make(ptr, etype, k, offset, not_flat, not_null_free, flat, null_free, atomic, refined_type);\n@@ -6060,2 +6624,30 @@\n-const TypeAryKlassPtr* TypeAryKlassPtr::make(ciKlass* klass, InterfaceHandling interface_handling) {\n-  return TypeAryKlassPtr::make(Constant, klass, 0, interface_handling);\n+const TypeAryKlassPtr* TypeAryKlassPtr::make(PTR ptr, ciKlass* k, Offset offset, InterfaceHandling interface_handling, bool refined_type) {\n+  bool flat = k->is_flat_array_klass();\n+  bool null_free = k->as_array_klass()->is_elem_null_free();\n+  bool atomic = k->as_array_klass()->is_elem_atomic();\n+\n+  bool not_inline = k->is_type_array_klass() || !k->as_array_klass()->element_klass()->can_be_inline_klass(false);\n+  bool not_null_free = (ptr == Constant) ? !null_free : not_inline;\n+  bool not_flat = (ptr == Constant) ? !flat : (!UseArrayFlattening || not_inline ||\n+                   (k->as_array_klass()->element_klass() != nullptr &&\n+                    k->as_array_klass()->element_klass()->is_inlinetype() &&\n+                   !k->as_array_klass()->element_klass()->maybe_flat_in_array()));\n+\n+  return TypeAryKlassPtr::make(ptr, k, offset, interface_handling, not_flat, not_null_free, flat, null_free, atomic, refined_type);\n+}\n+\n+const TypeAryKlassPtr* TypeAryKlassPtr::make(ciKlass* klass, InterfaceHandling interface_handling, bool refined_type) {\n+  return TypeAryKlassPtr::make(Constant, klass, Offset(0), interface_handling, refined_type);\n+}\n+\n+\/\/ Get the (non-)refined array klass ptr\n+const TypeAryKlassPtr* TypeAryKlassPtr::cast_to_refined_array_klass_ptr(bool refined) const {\n+  if ((refined == is_refined_type()) || !klass_is_exact() || (!exact_klass()->is_obj_array_klass() && !exact_klass()->is_flat_array_klass())) {\n+    return this;\n+  }\n+  ciKlass* eklass = elem()->is_klassptr()->exact_klass_helper();\n+  if (elem()->isa_aryklassptr()) {\n+    eklass = exact_klass()->as_obj_array_klass()->element_klass();\n+  }\n+  ciKlass* array_klass = ciArrayKlass::make(eklass, eklass->is_inlinetype() ? is_null_free() : false, eklass->is_inlinetype() ? is_atomic() : true, refined);\n+  return make(_ptr, array_klass, Offset(0), trust_interfaces, refined);\n@@ -6070,0 +6662,6 @@\n+    _flat == p->_flat &&\n+    _not_flat == p->_not_flat &&\n+    _null_free == p->_null_free &&\n+    _not_null_free == p->_not_null_free &&\n+    _atomic == p->_atomic &&\n+    _refined_type == p->_refined_type &&\n@@ -6076,1 +6674,2 @@\n-  return (uint)(uintptr_t)_elem + TypeKlassPtr::hash();\n+  return (uint)(uintptr_t)_elem + TypeKlassPtr::hash() + (uint)(_not_flat ? 43 : 0) +\n+      (uint)(_not_null_free ? 44 : 0) + (uint)(_flat ? 45 : 0) + (uint)(_null_free ? 46 : 0)  + (uint)(_atomic ? 47 : 0) + (uint)(_refined_type ? 48 : 0);\n@@ -6093,1 +6692,1 @@\n-    \/\/ Leave k_ary at null.\n+    \/\/ Leave k_ary at nullptr.\n@@ -6095,1 +6694,1 @@\n-    \/\/ Leave k_ary at null.\n+    \/\/ Leave k_ary at nullptr.\n@@ -6143,1 +6742,1 @@\n-    k = ciObjArrayKlass::make(k);\n+    k = ciArrayKlass::make(k, is_null_free(), is_atomic(), is_flat() || is_null_free());\n@@ -6163,1 +6762,1 @@\n-  return make(_ptr, elem(), klass(), xadd_offset(offset));\n+  return make(_ptr, elem(), klass(), xadd_offset(offset), is_not_flat(), is_not_null_free(), _flat, _null_free, _atomic, _refined_type);\n@@ -6167,1 +6766,1 @@\n-  return make(_ptr, elem(), klass(), offset);\n+  return make(_ptr, elem(), klass(), Offset(offset), is_not_flat(), is_not_null_free(), _flat, _null_free, _atomic, _refined_type);\n@@ -6174,1 +6773,1 @@\n-  return make(ptr, elem(), _klass, _offset);\n+  return make(ptr, elem(), _klass, _offset, is_not_flat(), is_not_null_free(), _flat, _null_free, _atomic, _refined_type);\n@@ -6182,0 +6781,7 @@\n+  \/\/ Even though MyValue is final, [LMyValue is only exact if the array\n+  \/\/ is (not) null-free due to null-free [LMyValue <: null-able [LMyValue.\n+  \/\/ TODO 8350865 If we know that the array can't be null-free, it's allowed to be exact, right?\n+  \/\/ If so, we should add '&& !is_not_null_free()'\n+  if (tk->isa_instklassptr() && tk->klass()->is_inlinetype() && !is_null_free()) {\n+    return false;\n+  }\n@@ -6188,1 +6794,4 @@\n-  if (must_be_exact()) return this;  \/\/ cannot clear xk\n+  if (must_be_exact() && !klass_is_exact) return this;  \/\/ cannot clear xk\n+  if (klass_is_exact == this->klass_is_exact()) {\n+    return this;\n+  }\n@@ -6194,1 +6803,19 @@\n-  return make(klass_is_exact ? Constant : NotNull, elem, k, _offset);\n+  bool not_flat = is_not_flat();\n+  bool not_null_free = is_not_null_free();\n+  if (_elem->isa_klassptr()) {\n+    if (klass_is_exact || _elem->isa_aryklassptr()) {\n+      assert((!is_null_free() && !is_flat()) ||\n+             _elem->is_klassptr()->klass()->is_abstract() || _elem->is_klassptr()->klass()->is_java_lang_Object(),\n+             \"null-free (or flat) concrete inline type arrays should always be exact\");\n+      \/\/ An array can't be null-free (or flat) if the klass is exact\n+      not_null_free = true;\n+      not_flat = true;\n+    } else {\n+      \/\/ Klass is not exact (anymore), re-compute null-free\/flat properties\n+      const TypeOopPtr* exact_etype = TypeOopPtr::make_from_klass_unique(_elem->is_instklassptr()->instance_klass());\n+      bool not_inline = !exact_etype->can_be_inline_type();\n+      not_null_free = not_inline;\n+      not_flat = !UseArrayFlattening || not_inline || (exact_etype->is_inlinetypeptr() && !exact_etype->inline_klass()->maybe_flat_in_array());\n+    }\n+  }\n+  return make(klass_is_exact ? Constant : NotNull, elem, k, _offset, not_flat, not_null_free, _flat, _null_free, _atomic, _refined_type);\n@@ -6197,1 +6824,0 @@\n-\n@@ -6211,1 +6837,5 @@\n-  return TypeAryPtr::make(TypePtr::BotPTR, TypeAry::make(el, TypeInt::POS), k, xk, 0);\n+  bool null_free = _null_free;\n+  if (null_free && el->isa_ptr()) {\n+    el = el->is_ptr()->join_speculative(TypePtr::NOTNULL);\n+  }\n+  return TypeAryPtr::make(TypePtr::BotPTR, TypeAry::make(el, TypeInt::POS, false, is_flat(), is_not_flat(), is_not_null_free(), is_atomic()), k, xk, Offset(0));\n@@ -6248,1 +6878,1 @@\n-    int offset = meet_offset(tp->offset());\n+    Offset offset = meet_offset(tp->offset());\n@@ -6256,1 +6886,1 @@\n-      return make( ptr, _elem, klass(), offset );\n+      return make(ptr, _elem, klass(), offset, is_not_flat(), is_not_null_free(), is_flat(), is_null_free(), is_atomic(), is_refined_type());\n@@ -6289,1 +6919,1 @@\n-    int off = meet_offset(tap->offset());\n+    Offset off = meet_offset(tap->offset());\n@@ -6291,1 +6921,0 @@\n-\n@@ -6295,1 +6924,6 @@\n-    meet_aryptr(ptr, elem, this, tap, res_klass, res_xk);\n+    bool res_flat = false;\n+    bool res_not_flat = false;\n+    bool res_not_null_free = false;\n+    bool res_atomic = false;\n+    MeetResult res = meet_aryptr(ptr, elem, this, tap,\n+                                 res_klass, res_xk, res_flat, res_not_flat, res_not_null_free, res_atomic);\n@@ -6297,1 +6931,33 @@\n-    return make(ptr, elem, res_klass, off);\n+    bool flat = meet_flat(tap->_flat);\n+    bool null_free = meet_null_free(tap->_null_free);\n+    bool atomic = meet_atomic(tap->_atomic);\n+    bool refined_type = _refined_type && tap->_refined_type;\n+    if (res == NOT_SUBTYPE) {\n+      flat = false;\n+      null_free = false;\n+      atomic = false;\n+      refined_type = false;\n+    } else if (res == SUBTYPE) {\n+      if (above_centerline(tap->ptr()) && !above_centerline(this->ptr())) {\n+        flat = _flat;\n+        null_free = _null_free;\n+        atomic = _atomic;\n+        refined_type = _refined_type;\n+      } else if (above_centerline(this->ptr()) && !above_centerline(tap->ptr())) {\n+        flat = tap->_flat;\n+        null_free = tap->_null_free;\n+        atomic = tap->_atomic;\n+        refined_type = tap->_refined_type;\n+      } else if (above_centerline(this->ptr()) && above_centerline(tap->ptr())) {\n+        flat = _flat || tap->_flat;\n+        null_free = _null_free || tap->_null_free;\n+        atomic = _atomic || tap->_atomic;\n+        refined_type = _refined_type || tap->_refined_type;\n+      } else if (res_xk && _refined_type != tap->_refined_type) {\n+        \/\/ This can happen if the phi emitted by LibraryCallKit::load_default_refined_array_klass\/load_non_refined_array_klass\n+        \/\/ is processed before the typeArray guard is folded. Both inputs are constant but the input corresponding to the\n+        \/\/ typeArray will go away. Don't constant fold it yet but wait for the control input to collapse.\n+        ptr = PTR::NotNull;\n+      }\n+    }\n+    return make(ptr, elem, res_klass, off, res_not_flat, res_not_null_free, flat, null_free, atomic, refined_type);\n@@ -6301,1 +6967,1 @@\n-    int offset = meet_offset(tp->offset());\n+    Offset offset = meet_offset(tp->offset());\n@@ -6315,1 +6981,1 @@\n-        return TypeAryKlassPtr::make(ptr, _elem, _klass, offset);\n+        return TypeAryKlassPtr::make(ptr, _elem, _klass, offset, is_not_flat(), is_not_null_free(), is_flat(), is_null_free(), is_atomic(), is_refined_type());\n@@ -6320,1 +6986,1 @@\n-        return TypeInstKlassPtr::make(ptr, ciEnv::current()->Object_klass(), interfaces, offset);\n+        return TypeInstKlassPtr::make(ptr, ciEnv::current()->Object_klass(), interfaces, offset, false);\n@@ -6335,1 +7001,1 @@\n-          return make(ptr, _elem, _klass, offset);\n+          return make(ptr, _elem, _klass, offset, is_not_flat(), is_not_null_free(), is_flat(), is_null_free(), is_atomic(), is_refined_type());\n@@ -6343,1 +7009,1 @@\n-      return TypeInstKlassPtr::make(ptr, ciEnv::current()->Object_klass(), interfaces, offset);\n+      return TypeInstKlassPtr::make(ptr, ciEnv::current()->Object_klass(), interfaces, offset, false);\n@@ -6381,0 +7047,3 @@\n+    if (other->is_null_free() && !this_one->is_null_free()) {\n+      return false; \/\/ A nullable array can't be a subtype of a null-free array\n+    }\n@@ -6473,1 +7142,1 @@\n-  return new TypeAryKlassPtr(dual_ptr(), elem()->dual(), klass(), dual_offset());\n+  return new TypeAryKlassPtr(dual_ptr(), elem()->dual(), klass(), dual_offset(), !is_not_flat(), !is_not_null_free(), dual_flat(), dual_null_free(), dual_atomic(), _refined_type);\n@@ -6483,1 +7152,1 @@\n-    k = ciObjArrayKlass::make(k);\n+    k = ciArrayKlass::make(k, k->is_inlinetype() ? is_null_free() : false, k->is_inlinetype() ? is_atomic() : true, _refined_type);\n@@ -6530,5 +7199,7 @@\n-\n-  if( _offset ) {               \/\/ Dump offset, if any\n-    if( _offset == OffsetBot )      { st->print(\"+any\"); }\n-    else if( _offset == OffsetTop ) { st->print(\"+unknown\"); }\n-    else                            { st->print(\"+%d\", _offset); }\n+  if (_flat) st->print(\":flat\");\n+  if (_null_free) st->print(\":null free\");\n+  if (_atomic) st->print(\":atomic\");\n+  if (_refined_type) st->print(\":refined_type\");\n+  if (Verbose) {\n+    if (_not_flat) st->print(\":not flat\");\n+    if (_not_null_free) st->print(\":nullable\");\n@@ -6537,0 +7208,2 @@\n+  _offset.dump2(st);\n+\n@@ -6555,2 +7228,14 @@\n-const TypeFunc *TypeFunc::make( const TypeTuple *domain, const TypeTuple *range ) {\n-  return (TypeFunc*)(new TypeFunc(domain,range))->hashcons();\n+const TypeFunc *TypeFunc::make(const TypeTuple *domain_sig, const TypeTuple* domain_cc,\n+                               const TypeTuple *range_sig, const TypeTuple *range_cc) {\n+  return (TypeFunc*)(new TypeFunc(domain_sig, domain_cc, range_sig, range_cc))->hashcons();\n+}\n+\n+const TypeFunc *TypeFunc::make(const TypeTuple *domain, const TypeTuple *range) {\n+  return make(domain, domain, range, range);\n+}\n+\n+\/\/------------------------------osr_domain-----------------------------\n+const TypeTuple* osr_domain() {\n+  const Type **fields = TypeTuple::fields(2);\n+  fields[TypeFunc::Parms+0] = TypeRawPtr::BOTTOM;  \/\/ address of osr buffer\n+  return TypeTuple::make(TypeFunc::Parms+1, fields);\n@@ -6560,1 +7245,1 @@\n-const TypeFunc *TypeFunc::make(ciMethod* method) {\n+const TypeFunc* TypeFunc::make(ciMethod* method, bool is_osr_compilation) {\n@@ -6562,7 +7247,24 @@\n-  const TypeFunc* tf = C->last_tf(method); \/\/ check cache\n-  if (tf != nullptr)  return tf;  \/\/ The hit rate here is almost 50%.\n-  const TypeTuple *domain;\n-  if (method->is_static()) {\n-    domain = TypeTuple::make_domain(nullptr, method->signature(), ignore_interfaces);\n-  } else {\n-    domain = TypeTuple::make_domain(method->holder(), method->signature(), ignore_interfaces);\n+  const TypeFunc* tf = nullptr;\n+  if (!is_osr_compilation) {\n+    tf = C->last_tf(method); \/\/ check cache\n+    if (tf != nullptr)  return tf;  \/\/ The hit rate here is almost 50%.\n+  }\n+  \/\/ Inline types are not passed\/returned by reference, instead each field of\n+  \/\/ the inline type is passed\/returned as an argument. We maintain two views of\n+  \/\/ the argument\/return list here: one based on the signature (with an inline\n+  \/\/ type argument\/return as a single slot), one based on the actual calling\n+  \/\/ convention (with an inline type argument\/return as a list of its fields).\n+  bool has_scalar_args = method->has_scalarized_args() && !is_osr_compilation;\n+  \/\/ Fall back to the non-scalarized calling convention when compiling a call via a mismatching method\n+  if (method != C->method() && method->get_Method()->mismatch()) {\n+    has_scalar_args = false;\n+  }\n+  const TypeTuple* domain_sig = is_osr_compilation ? osr_domain() : TypeTuple::make_domain(method, ignore_interfaces, false);\n+  const TypeTuple* domain_cc = has_scalar_args ? TypeTuple::make_domain(method, ignore_interfaces, true) : domain_sig;\n+  ciSignature* sig = method->signature();\n+  bool has_scalar_ret = !method->is_native() && sig->return_type()->is_inlinetype() && sig->return_type()->as_inline_klass()->can_be_returned_as_fields();\n+  const TypeTuple* range_sig = TypeTuple::make_range(sig, ignore_interfaces, false);\n+  const TypeTuple* range_cc = has_scalar_ret ? TypeTuple::make_range(sig, ignore_interfaces, true) : range_sig;\n+  tf = TypeFunc::make(domain_sig, domain_cc, range_sig, range_cc);\n+  if (!is_osr_compilation) {\n+    C->set_last_tf(method, tf);  \/\/ fill cache\n@@ -6570,3 +7272,0 @@\n-  const TypeTuple *range  = TypeTuple::make_range(method->signature(), ignore_interfaces);\n-  tf = TypeFunc::make(domain, range);\n-  C->set_last_tf(method, tf);  \/\/ fill cache\n@@ -6607,2 +7306,4 @@\n-  return _domain == a->_domain &&\n-    _range == a->_range;\n+  return _domain_sig == a->_domain_sig &&\n+    _domain_cc == a->_domain_cc &&\n+    _range_sig == a->_range_sig &&\n+    _range_cc == a->_range_cc;\n@@ -6614,1 +7315,1 @@\n-  return (uint)(uintptr_t)_domain + (uint)(uintptr_t)_range;\n+  return (uint)(intptr_t)_domain_sig + (uint)(intptr_t)_domain_cc + (uint)(intptr_t)_range_sig + (uint)(intptr_t)_range_cc;\n@@ -6621,1 +7322,1 @@\n-  if( _range->cnt() <= Parms )\n+  if( _range_sig->cnt() <= Parms )\n@@ -6625,2 +7326,2 @@\n-    for (i = Parms; i < _range->cnt()-1; i++) {\n-      _range->field_at(i)->dump2(d,depth,st);\n+    for (i = Parms; i < _range_sig->cnt()-1; i++) {\n+      _range_sig->field_at(i)->dump2(d,depth,st);\n@@ -6629,1 +7330,1 @@\n-    _range->field_at(i)->dump2(d,depth,st);\n+    _range_sig->field_at(i)->dump2(d,depth,st);\n@@ -6638,3 +7339,3 @@\n-  if (Parms < _domain->cnt())\n-    _domain->field_at(Parms)->dump2(d,depth-1,st);\n-  for (uint i = Parms+1; i < _domain->cnt(); i++) {\n+  if (Parms < _domain_sig->cnt())\n+    _domain_sig->field_at(Parms)->dump2(d,depth-1,st);\n+  for (uint i = Parms+1; i < _domain_sig->cnt(); i++) {\n@@ -6642,1 +7343,1 @@\n-    _domain->field_at(i)->dump2(d,depth-1,st);\n+    _domain_sig->field_at(i)->dump2(d,depth-1,st);\n@@ -6662,1 +7363,1 @@\n-  if (range()->cnt() == TypeFunc::Parms) {\n+  if (range_sig()->cnt() == TypeFunc::Parms) {\n@@ -6665,1 +7366,1 @@\n-  return range()->field_at(TypeFunc::Parms)->basic_type();\n+  return range_sig()->field_at(TypeFunc::Parms)->basic_type();\n","filename":"src\/hotspot\/share\/opto\/type.cpp","additions":1053,"deletions":352,"binary":false,"changes":1405,"status":"modified"},{"patch":"@@ -54,0 +54,2 @@\n+#include \"oops\/flatArrayOop.inline.hpp\"\n+#include \"oops\/inlineKlass.inline.hpp\"\n@@ -421,0 +423,1 @@\n+  bool is_flat = InstanceKlass::cast(k1)->field_is_flat(slot);\n@@ -422,1 +425,1 @@\n-  ret = jfieldIDWorkaround::to_instance_jfieldID(k1, offset);\n+  ret = jfieldIDWorkaround::to_instance_jfieldID(k1, offset, is_flat);\n@@ -439,1 +442,1 @@\n-  if (m->is_object_initializer()) {\n+  if (m->is_object_constructor()) {\n@@ -798,1 +801,1 @@\n-    case T_OBJECT:      push_object(va_arg(_ap, jobject)); break;\n+    case T_OBJECT: push_object(va_arg(_ap, jobject)); break;\n@@ -965,1 +968,7 @@\n-  instanceOop i = InstanceKlass::allocate_instance(JNIHandles::resolve_non_null(clazz), CHECK_NULL);\n+  oop clazzoop = JNIHandles::resolve_non_null(clazz);\n+  Klass* k = java_lang_Class::as_Klass(clazzoop);\n+  if (k == nullptr || k->is_inline_klass()) {\n+    ResourceMark rm(THREAD);\n+    THROW_(vmSymbols::java_lang_InstantiationException(), nullptr);\n+  }\n+  instanceOop i = InstanceKlass::allocate_instance(clazzoop, CHECK_NULL);\n@@ -979,1 +988,8 @@\n-  instanceOop i = InstanceKlass::allocate_instance(JNIHandles::resolve_non_null(clazz), CHECK_NULL);\n+  oop clazzoop = JNIHandles::resolve_non_null(clazz);\n+  Klass* k = java_lang_Class::as_Klass(clazzoop);\n+  if (k == nullptr) {\n+    ResourceMark rm(THREAD);\n+    THROW_(vmSymbols::java_lang_InstantiationException(), nullptr);\n+  }\n+\n+  instanceOop i = InstanceKlass::allocate_instance(clazzoop, CHECK_NULL);\n@@ -984,0 +1000,1 @@\n+\n@@ -985,1 +1002,1 @@\n-JNI_END\n+  JNI_END\n@@ -997,1 +1014,8 @@\n-  instanceOop i = InstanceKlass::allocate_instance(JNIHandles::resolve_non_null(clazz), CHECK_NULL);\n+  oop clazzoop = JNIHandles::resolve_non_null(clazz);\n+  Klass* k = java_lang_Class::as_Klass(clazzoop);\n+  if (k == nullptr) {\n+    ResourceMark rm(THREAD);\n+    THROW_(vmSymbols::java_lang_InstantiationException(), nullptr);\n+  }\n+\n+  instanceOop i = InstanceKlass::allocate_instance(clazzoop, CHECK_NULL);\n@@ -1002,0 +1026,1 @@\n+\n@@ -1015,1 +1040,8 @@\n-  instanceOop i = InstanceKlass::allocate_instance(JNIHandles::resolve_non_null(clazz), CHECK_NULL);\n+  oop clazzoop = JNIHandles::resolve_non_null(clazz);\n+  Klass* k = java_lang_Class::as_Klass(clazzoop);\n+  if (k == nullptr) {\n+    ResourceMark rm(THREAD);\n+    THROW_(vmSymbols::java_lang_InstantiationException(), nullptr);\n+  }\n+\n+  instanceOop i = InstanceKlass::allocate_instance(clazzoop, CHECK_NULL);\n@@ -1023,0 +1055,1 @@\n+\n@@ -1773,1 +1806,1 @@\n-  ret = jfieldIDWorkaround::to_instance_jfieldID(k, fd.offset());\n+  ret = jfieldIDWorkaround::to_instance_jfieldID(k, fd.offset(), fd.is_flat());\n@@ -1783,0 +1816,1 @@\n+  oop res = nullptr;\n@@ -1788,2 +1822,15 @@\n-  oop loaded_obj = HeapAccess<ON_UNKNOWN_OOP_REF>::oop_load_at(o, offset);\n-  jobject ret = JNIHandles::make_local(THREAD, loaded_obj);\n+  if (!jfieldIDWorkaround::is_flat_jfieldID(fieldID)) {\n+    res = HeapAccess<ON_UNKNOWN_OOP_REF>::oop_load_at(o, offset);\n+  } else {\n+    assert(k->is_instance_klass(), \"Only instance can have flat fields\");\n+    InstanceKlass* ik = InstanceKlass::cast(k);\n+    fieldDescriptor fd;\n+    bool found = ik->find_field_from_offset(offset, false, &fd);  \/\/ performance bottleneck\n+    assert(found, \"Field not found\");\n+    InstanceKlass* holder = fd.field_holder();\n+    assert(holder->field_is_flat(fd.index()), \"Must be\");\n+    InlineLayoutInfo* li = holder->inline_layout_info_adr(fd.index());\n+    InlineKlass* field_vklass = li->klass();\n+    res = field_vklass->read_payload_from_addr(o, ik->field_offset(fd.index()), li->kind(), CHECK_NULL);\n+  }\n+  jobject ret = JNIHandles::make_local(THREAD, res);\n@@ -1794,2 +1841,0 @@\n-\n-\n@@ -1881,1 +1926,22 @@\n-  HeapAccess<ON_UNKNOWN_OOP_REF>::oop_store_at(o, offset, JNIHandles::resolve(value));\n+  if (!jfieldIDWorkaround::is_flat_jfieldID(fieldID)) {\n+    oop v = JNIHandles::resolve(value);\n+    if (v == nullptr) {\n+      InstanceKlass *ik = InstanceKlass::cast(k);\n+      fieldDescriptor fd;\n+      ik->find_field_from_offset(offset, false, &fd);\n+      if (fd.is_null_free_inline_type()) {\n+        THROW_MSG(vmSymbols::java_lang_NullPointerException(), \"Cannot store null in a null-restricted field\");\n+      }\n+    }\n+    HeapAccess<ON_UNKNOWN_OOP_REF>::oop_store_at(o, offset, v);\n+  } else {\n+    assert(k->is_instance_klass(), \"Only instances can have flat fields\");\n+    InstanceKlass* ik = InstanceKlass::cast(k);\n+    fieldDescriptor fd;\n+    ik->find_field_from_offset(offset, false, &fd);\n+    InstanceKlass* holder = fd.field_holder();\n+    InlineLayoutInfo* li = holder->inline_layout_info_adr(fd.index());\n+    InlineKlass* vklass = li->klass();\n+    oop v = JNIHandles::resolve(value);\n+    vklass->write_value_to_addr(v, ((char*)(oopDesc*)o) + offset, li->kind(), true, CHECK);\n+  }\n@@ -2310,1 +2376,3 @@\n-    ret = JNIHandles::make_local(THREAD, a->obj_at(index));\n+    oop res = a->obj_at(index, CHECK_NULL);\n+    assert(res != nullptr || !a->is_null_free_array(), \"Invalid value\");\n+    ret = JNIHandles::make_local(THREAD, res);\n@@ -2327,5 +2395,6 @@\n-  objArrayOop a = objArrayOop(JNIHandles::resolve_non_null(array));\n-  oop v = JNIHandles::resolve(value);\n-  if (a->is_within_bounds(index)) {\n-    if (v == nullptr || v->is_a(ObjArrayKlass::cast(a->klass())->element_klass())) {\n-      a->obj_at_put(index, v);\n+   objArrayOop a = objArrayOop(JNIHandles::resolve_non_null(array));\n+   oop v = JNIHandles::resolve(value);\n+   if (a->is_within_bounds(index)) {\n+    Klass* ek = a->is_flatArray() ? FlatArrayKlass::cast(a->klass())->element_klass() : RefArrayKlass::cast(a->klass())->element_klass();\n+    if (v == nullptr || v->is_a(ek)) {\n+      a->obj_at_put(index, v, CHECK);\n@@ -2345,6 +2414,6 @@\n-  } else {\n-    ResourceMark rm(THREAD);\n-    stringStream ss;\n-    ss.print(\"Index %d out of bounds for length %d\", index, a->length());\n-    THROW_MSG(vmSymbols::java_lang_ArrayIndexOutOfBoundsException(), ss.as_string());\n-  }\n+   } else {\n+     ResourceMark rm(THREAD);\n+     stringStream ss;\n+     ss.print(\"Index %d out of bounds for length %d\", index, a->length());\n+     THROW_MSG(vmSymbols::java_lang_ArrayIndexOutOfBoundsException(), ss.as_string());\n+   }\n@@ -2726,1 +2795,1 @@\n-  ObjectSynchronizer::jni_enter(obj, thread);\n+  ObjectSynchronizer::jni_enter(obj, CHECK_(JNI_ERR));\n","filename":"src\/hotspot\/share\/prims\/jni.cpp","additions":96,"deletions":27,"binary":false,"changes":123,"status":"modified"},{"patch":"@@ -55,0 +55,1 @@\n+#include \"memory\/iterator.inline.hpp\"\n@@ -63,0 +64,1 @@\n+#include \"oops\/access.hpp\"\n@@ -65,0 +67,1 @@\n+#include \"oops\/compressedOops.inline.hpp\"\n@@ -90,0 +93,1 @@\n+#include \"runtime\/keepStackGCProcessed.hpp\"\n@@ -2007,0 +2011,103 @@\n+WB_ENTRY(jobjectArray, WB_getObjectsViaKlassOopMaps(JNIEnv* env, jobject wb, jobject thing))\n+  oop aoop = JNIHandles::resolve(thing);\n+  if (!aoop->is_instance()) {\n+    return nullptr;\n+  }\n+  instanceHandle ih(THREAD, (instanceOop) aoop);\n+  InstanceKlass* klass = InstanceKlass::cast(ih->klass());\n+  if (klass->nonstatic_oop_map_count() == 0) {\n+    return nullptr;\n+  }\n+  const OopMapBlock* map = klass->start_of_nonstatic_oop_maps();\n+  const OopMapBlock* const end = map + klass->nonstatic_oop_map_count();\n+  int oop_count = 0;\n+  while (map < end) {\n+    oop_count += map->count();\n+    map++;\n+  }\n+\n+  objArrayHandle result_array =\n+      oopFactory::new_objArray_handle(vmClasses::Object_klass(), oop_count, CHECK_NULL);\n+  map = klass->start_of_nonstatic_oop_maps();\n+  int index = 0;\n+  while (map < end) {\n+    int offset = map->offset();\n+    for (unsigned int j = 0; j < map->count(); j++) {\n+      result_array->obj_at_put(index++, ih->obj_field(offset));\n+      offset += heapOopSize;\n+    }\n+    map++;\n+  }\n+  return (jobjectArray)JNIHandles::make_local(THREAD, result_array());\n+WB_END\n+\n+\/\/ Collect Object oops but not value objects...loaded from heap\n+class CollectObjectOops : public BasicOopIterateClosure {\n+  public:\n+  GrowableArray<Handle>* _array;\n+\n+  CollectObjectOops() {\n+      _array = new GrowableArray<Handle>(128);\n+  }\n+\n+  void add_oop(oop o) {\n+    Handle oh = Handle(Thread::current(), o);\n+    if (oh != nullptr && oh->is_inline_type()) {\n+      oh->oop_iterate(this);\n+    } else {\n+      _array->append(oh);\n+    }\n+  }\n+\n+  template <class T> inline void add_oop(T* p) { add_oop(HeapAccess<>::oop_load(p)); }\n+  void do_oop(oop* o) { add_oop(o); }\n+  void do_oop(narrowOop* v) { add_oop(v); }\n+\n+  jobjectArray create_jni_result(JNIEnv* env, TRAPS) {\n+    objArrayHandle result_array =\n+        oopFactory::new_objArray_handle(vmClasses::Object_klass(), _array->length(), CHECK_NULL);\n+    for (int i = 0 ; i < _array->length(); i++) {\n+      result_array->obj_at_put(i, _array->at(i)());\n+    }\n+    return (jobjectArray)JNIHandles::make_local(THREAD, result_array());\n+  }\n+};\n+\n+\/\/ Collect Object oops but not value objects...loaded from frames\n+class CollectFrameObjectOops : public BasicOopIterateClosure {\n+ public:\n+  CollectObjectOops _collect;\n+\n+  template <class T> inline void add_oop(T* p) { _collect.add_oop(RawAccess<>::oop_load(p)); }\n+  void do_oop(oop* o) { add_oop(o); }\n+  void do_oop(narrowOop* v) { add_oop(v); }\n+\n+  jobjectArray create_jni_result(JNIEnv* env, TRAPS) {\n+    return _collect.create_jni_result(env, THREAD);\n+  }\n+};\n+\n+\/\/ Collect Object oops for the given oop, iterate through value objects\n+WB_ENTRY(jobjectArray, WB_getObjectsViaOopIterator(JNIEnv* env, jobject wb, jobject thing))\n+  ResourceMark rm(thread);\n+  Handle objh(thread, JNIHandles::resolve(thing));\n+  CollectObjectOops collectOops;\n+  objh->oop_iterate(&collectOops);\n+  return collectOops.create_jni_result(env, THREAD);\n+WB_END\n+\n+\/\/ Collect Object oops for the given frame deep, iterate through value objects\n+WB_ENTRY(jobjectArray, WB_getObjectsViaFrameOopIterator(JNIEnv* env, jobject wb, jint depth))\n+  KeepStackGCProcessedMark ksgcpm(THREAD);\n+  ResourceMark rm(THREAD);\n+  CollectFrameObjectOops collectOops;\n+  StackFrameStream sfs(thread, true \/* update *\/, true \/* process_frames *\/);\n+  while (depth > 0) { \/\/ Skip the native WB API frame\n+    sfs.next();\n+    frame* f = sfs.current();\n+    f->oops_do(&collectOops, nullptr, sfs.register_map());\n+    depth--;\n+  }\n+  return collectOops.create_jni_result(env, THREAD);\n+WB_END\n+\n@@ -3017,0 +3124,6 @@\n+  {CC\"getObjectsViaKlassOopMaps0\",\n+      CC\"(Ljava\/lang\/Object;)[Ljava\/lang\/Object;\",    (void*)&WB_getObjectsViaKlassOopMaps},\n+  {CC\"getObjectsViaOopIterator0\",\n+          CC\"(Ljava\/lang\/Object;)[Ljava\/lang\/Object;\",(void*)&WB_getObjectsViaOopIterator},\n+  {CC\"getObjectsViaFrameOopIterator\",\n+      CC\"(I)[Ljava\/lang\/Object;\",                     (void*)&WB_getObjectsViaFrameOopIterator},\n","filename":"src\/hotspot\/share\/prims\/whitebox.cpp","additions":113,"deletions":0,"binary":false,"changes":113,"status":"modified"},{"patch":"@@ -79,0 +79,2 @@\n+#include <string.h>\n+\n@@ -365,0 +367,12 @@\n+bool Arguments::patching_migrated_classes(const char* property, const char* value) {\n+  if (strncmp(property, MODULE_PROPERTY_PREFIX, MODULE_PROPERTY_PREFIX_LEN) == 0) {\n+    const char* property_suffix = property + MODULE_PROPERTY_PREFIX_LEN;\n+    if (matches_property_suffix(property_suffix, PATCH, PATCH_LEN)) {\n+      if (strcmp(value, \"java.base-valueclasses.jar\")) {\n+        return true;\n+      }\n+    }\n+  }\n+  return false;\n+}\n+\n@@ -1810,1 +1824,0 @@\n-static unsigned int patch_mod_count = 0;\n@@ -1817,1 +1830,1 @@\n-  if (!CDSConfig::check_vm_args_consistency(patch_mod_javabase, mode_flag_cmd_line)) {\n+  if (!CDSConfig::check_vm_args_consistency(mode_flag_cmd_line)) {\n@@ -1984,0 +1997,4 @@\n+  if (UseAltSubstitutabilityMethod) {\n+    no_shared_spaces(\"Alternate substitutability method doesn't work with CDS yet\");\n+  }\n+\n@@ -2066,1 +2083,1 @@\n-      add_patch_mod_prefix(module_name, module_equal + 1);\n+      add_patch_mod_prefix(module_name, module_equal + 1, false \/* no append *\/, false \/* no cds *\/);\n@@ -2068,3 +2085,0 @@\n-      if (!create_numbered_module_property(\"jdk.module.patch\", patch_mod_tail, patch_mod_count++)) {\n-        return JNI_ENOMEM;\n-      }\n@@ -2078,0 +2092,82 @@\n+\/\/ Temporary system property to disable preview patching and enable the new preview mode\n+\/\/ feature for testing\/development. Once the preview mode feature is finished, the value\n+\/\/ will be always 'true' and this code, and all related dead-code can be removed.\n+#define DISABLE_PREVIEW_PATCHING_DEFAULT false\n+\n+bool Arguments::disable_preview_patching() {\n+  const char* prop = get_property(\"DISABLE_PREVIEW_PATCHING\");\n+  return (prop != nullptr)\n+      ? strncmp(prop, \"true\", strlen(\"true\")) == 0\n+      : DISABLE_PREVIEW_PATCHING_DEFAULT;\n+}\n+\n+\/\/ VALUECLASS_STR must match string used in the build\n+#define VALUECLASS_STR \"valueclasses\"\n+#define VALUECLASS_JAR \"-\" VALUECLASS_STR \".jar\"\n+\n+\/\/ Finalize --patch-module args and --enable-preview related to value class module patches.\n+\/\/ Create all numbered properties passing module patches.\n+int Arguments::finalize_patch_module() {\n+  \/\/ If --enable-preview and EnableValhalla is true, modules may have preview mode resources.\n+  bool enable_valhalla_preview = enable_preview() && EnableValhalla;\n+  \/\/ Whether to use module patching, or the new preview mode feature for preview resources.\n+  bool disable_patching = disable_preview_patching();\n+\n+  \/\/ This must be called, even with 'false', to enable resource lookup from JImage.\n+  ClassLoader::init_jimage(disable_patching && enable_valhalla_preview);\n+\n+  \/\/ For each <module>-valueclasses.jar in <JAVA_HOME>\/lib\/valueclasses\/\n+  \/\/ appends the equivalent of --patch-module <module>=<JAVA_HOME>\/lib\/valueclasses\/<module>-valueclasses.jar\n+  if (!disable_patching && enable_valhalla_preview) {\n+    char * valueclasses_dir = AllocateHeap(JVM_MAXPATHLEN, mtArguments);\n+    const char * fileSep = os::file_separator();\n+\n+    jio_snprintf(valueclasses_dir, JVM_MAXPATHLEN, \"%s%slib%s\" VALUECLASS_STR \"%s\",\n+                 Arguments::get_java_home(), fileSep, fileSep, fileSep);\n+    DIR* dir = os::opendir(valueclasses_dir);\n+    if (dir != nullptr) {\n+      char * module_name = AllocateHeap(JVM_MAXPATHLEN, mtArguments);\n+      char * path = AllocateHeap(JVM_MAXPATHLEN, mtArguments);\n+\n+      for (dirent * entry = os::readdir(dir); entry != nullptr; entry = os::readdir(dir)) {\n+        \/\/ Test if file ends-with \"-valueclasses.jar\"\n+        int len = (int)strlen(entry->d_name) - (sizeof(VALUECLASS_JAR) - 1);\n+        if (len <= 0 || strcmp(&entry->d_name[len], VALUECLASS_JAR) != 0) {\n+          continue;         \/\/ too short or not the expected suffix\n+        }\n+\n+        strcpy(module_name, entry->d_name);\n+        module_name[len] = '\\0';     \/\/ truncate to just module-name\n+\n+        jio_snprintf(path, JVM_MAXPATHLEN, \"%s%s\", valueclasses_dir, &entry->d_name);\n+        add_patch_mod_prefix(module_name, path, true \/* append *\/, true \/* cds OK*\/);\n+        log_info(class)(\"--enable-preview appending value classes for module %s: %s\", module_name, entry->d_name);\n+      }\n+      FreeHeap(module_name);\n+      FreeHeap(path);\n+      os::closedir(dir);\n+    }\n+    FreeHeap(valueclasses_dir);\n+  }\n+\n+  \/\/ Create numbered properties for each module that has been patched either\n+  \/\/ by --patch-module (or --enable-preview if disable_patching is false).\n+  \/\/ Format is \"jdk.module.patch.<n>=<module_name>=<path>\"\n+  if (_patch_mod_prefix != nullptr) {\n+    char * prop_value = AllocateHeap(JVM_MAXPATHLEN + JVM_MAXPATHLEN + 1, mtArguments);\n+    unsigned int patch_mod_count = 0;\n+\n+    for (GrowableArrayIterator<ModulePatchPath *> it = _patch_mod_prefix->begin();\n+            it != _patch_mod_prefix->end(); ++it) {\n+      jio_snprintf(prop_value, JVM_MAXPATHLEN + JVM_MAXPATHLEN + 1, \"%s=%s\",\n+                   (*it)->module_name(), (*it)->path_string());\n+      if (!create_numbered_module_property(\"jdk.module.patch\", prop_value, patch_mod_count++)) {\n+        FreeHeap(prop_value);\n+        return JNI_ENOMEM;\n+      }\n+    }\n+    FreeHeap(prop_value);\n+  }\n+  return JNI_OK;\n+}\n+\n@@ -2346,0 +2442,4 @@\n+      \/\/ --enable-preview enables Valhalla, EnableValhalla VM option will eventually be removed before integration\n+      if (FLAG_SET_CMDLINE(EnableValhalla, true) != JVMFlag::SUCCESS) {\n+        return JNI_EINVAL;\n+      }\n@@ -2849,10 +2949,5 @@\n-void Arguments::add_patch_mod_prefix(const char* module_name, const char* path) {\n-  \/\/ For java.base check for duplicate --patch-module options being specified on the command line.\n-  \/\/ This check is only required for java.base, all other duplicate module specifications\n-  \/\/ will be checked during module system initialization.  The module system initialization\n-  \/\/ will throw an ExceptionInInitializerError if this situation occurs.\n-  if (strcmp(module_name, JAVA_BASE_NAME) == 0) {\n-    if (patch_mod_javabase) {\n-      vm_exit_during_initialization(\"Cannot specify \" JAVA_BASE_NAME \" more than once to --patch-module\");\n-    } else {\n-      patch_mod_javabase = true;\n+void Arguments::add_patch_mod_prefix(const char* module_name, const char* path, bool allow_append, bool allow_cds) {\n+  if (!allow_cds) {\n+    CDSConfig::set_module_patching_disables_cds();\n+    if (strcmp(module_name, JAVA_BASE_NAME) == 0) {\n+      CDSConfig::set_java_base_module_patching_disables_cds();\n@@ -2867,1 +2962,18 @@\n-  _patch_mod_prefix->push(new ModulePatchPath(module_name, path));\n+  \/\/ Scan patches for matching module\n+  int i = _patch_mod_prefix->find_if([&](ModulePatchPath* patch) {\n+    return (strcmp(module_name, patch->module_name()) == 0);\n+  });\n+  if (i == -1) {\n+    _patch_mod_prefix->push(new ModulePatchPath(module_name, path));\n+  } else {\n+    if (allow_append) {\n+      \/\/ append path to existing module entry\n+      _patch_mod_prefix->at(i)->append_path(path);\n+    } else {\n+      if (strcmp(module_name, JAVA_BASE_NAME) == 0) {\n+        vm_exit_during_initialization(\"Cannot specify \" JAVA_BASE_NAME \" more than once to --patch-module\");\n+      } else {\n+        vm_exit_during_initialization(\"Cannot specify a module more than once to --patch-module\", module_name);\n+      }\n+    }\n+  }\n@@ -2980,1 +3092,2 @@\n-  if (!check_vm_args_consistency()) {\n+  \/\/ finalize --module-patch and related --enable-preview\n+  if (finalize_patch_module() != JNI_OK) {\n@@ -2984,0 +3097,3 @@\n+  if (!check_vm_args_consistency()) {\n+    return JNI_ERR;\n+  }\n@@ -3869,0 +3985,12 @@\n+  if (!EnableValhalla || (is_interpreter_only() && !CDSConfig::is_dumping_archive() && !UseSharedSpaces)) {\n+    \/\/ Disable calling convention optimizations if inline types are not supported.\n+    \/\/ Also these aren't useful in -Xint. However, don't disable them when dumping or using\n+    \/\/ the CDS archive, as the values must match between dumptime and runtime.\n+    FLAG_SET_DEFAULT(InlineTypePassFieldsAsArgs, false);\n+    FLAG_SET_DEFAULT(InlineTypeReturnedAsFields, false);\n+  }\n+  if (!UseNonAtomicValueFlattening && !UseNullableValueFlattening && !UseAtomicValueFlattening) {\n+    \/\/ Flattening is disabled\n+    FLAG_SET_DEFAULT(UseArrayFlattening, false);\n+    FLAG_SET_DEFAULT(UseFieldFlattening, false);\n+  }\n","filename":"src\/hotspot\/share\/runtime\/arguments.cpp","additions":146,"deletions":18,"binary":false,"changes":164,"status":"modified"},{"patch":"@@ -53,0 +53,3 @@\n+#include \"oops\/flatArrayKlass.hpp\"\n+#include \"oops\/flatArrayOop.hpp\"\n+#include \"oops\/inlineKlass.inline.hpp\"\n@@ -300,0 +303,18 @@\n+\n+static Klass* get_refined_array_klass(Klass* k, frame* fr, RegisterMap* map, ObjectValue* sv, TRAPS) {\n+  \/\/ If it's an array, get the properties\n+  if (k->is_array_klass() && !k->is_typeArray_klass()) {\n+    assert(!k->is_refArray_klass() && !k->is_flatArray_klass(), \"Unexpected refined klass\");\n+    nmethod* nm = fr->cb()->as_nmethod_or_null();\n+    if (nm->is_compiled_by_c2()) {\n+      assert(sv->has_properties(), \"Property information is missing\");\n+      ArrayKlass::ArrayProperties props = static_cast<ArrayKlass::ArrayProperties>(StackValue::create_stack_value(fr, map, sv->properties())->get_jint());\n+      k = ObjArrayKlass::cast(k)->klass_with_properties(props, THREAD);\n+    } else {\n+      \/\/ TODO Graal needs to be fixed. Just go with the default properties for now\n+      k = ObjArrayKlass::cast(k)->klass_with_properties(ArrayKlass::ArrayProperties::DEFAULT, THREAD);\n+    }\n+  }\n+  return k;\n+}\n+\n@@ -301,2 +322,2 @@\n-static void print_objects(JavaThread* deoptee_thread,\n-                          GrowableArray<ScopeValue*>* objects, bool realloc_failures) {\n+static void print_objects(JavaThread* deoptee_thread, frame* deoptee, RegisterMap* map,\n+                          GrowableArray<ScopeValue*>* objects, bool realloc_failures, TRAPS) {\n@@ -318,0 +339,1 @@\n+    k = get_refined_array_klass(k, deoptee, map, sv, THREAD);\n@@ -351,2 +373,13 @@\n-  bool save_oop_result = chunk->at(0)->scope()->return_oop() && !thread->popframe_forcing_deopt_reexecution() && (exec_mode == Deoptimization::Unpack_deopt);\n-  Handle return_value;\n+  ScopeDesc* scope = chunk->at(0)->scope();\n+  bool save_oop_result = scope->return_oop() && !thread->popframe_forcing_deopt_reexecution() && (exec_mode == Deoptimization::Unpack_deopt);\n+  \/\/ In case of the return of multiple values, we must take care\n+  \/\/ of all oop return values.\n+  GrowableArray<Handle> return_oops;\n+  InlineKlass* vk = nullptr;\n+  if (save_oop_result && scope->return_scalarized()) {\n+    vk = InlineKlass::returned_inline_klass(map);\n+    if (vk != nullptr) {\n+      vk->save_oop_fields(map, return_oops);\n+      save_oop_result = false;\n+    }\n+  }\n@@ -358,1 +391,1 @@\n-    return_value = Handle(thread, result);\n+    return_oops.push(Handle(thread, result));\n@@ -365,1 +398,1 @@\n-  if (objects != nullptr) {\n+  if (objects != nullptr || vk != nullptr) {\n@@ -370,1 +403,9 @@\n-      realloc_failures = Deoptimization::realloc_objects(thread, &deoptee, &map, objects, CHECK_AND_CLEAR_(true));\n+      if (vk != nullptr) {\n+        realloc_failures = Deoptimization::realloc_inline_type_result(vk, map, return_oops, CHECK_AND_CLEAR_(true));\n+      }\n+      if (objects != nullptr) {\n+        realloc_failures = realloc_failures || Deoptimization::realloc_objects(thread, &deoptee, &map, objects, CHECK_AND_CLEAR_(true));\n+        guarantee(compiled_method != nullptr, \"deopt must be associated with an nmethod\");\n+        bool is_jvmci = compiled_method->is_compiled_by_jvmci();\n+        Deoptimization::reassign_fields(&deoptee, &map, objects, realloc_failures, is_jvmci, CHECK_AND_CLEAR_(true));\n+      }\n@@ -375,1 +416,9 @@\n-      realloc_failures = Deoptimization::realloc_objects(thread, &deoptee, &map, objects, THREAD);\n+      if (vk != nullptr) {\n+        realloc_failures = Deoptimization::realloc_inline_type_result(vk, map, return_oops, THREAD);\n+      }\n+      if (objects != nullptr) {\n+        realloc_failures = realloc_failures || Deoptimization::realloc_objects(thread, &deoptee, &map, objects, THREAD);\n+        guarantee(compiled_method != nullptr, \"deopt must be associated with an nmethod\");\n+        bool is_jvmci = compiled_method->is_compiled_by_jvmci();\n+        Deoptimization::reassign_fields(&deoptee, &map, objects, realloc_failures, is_jvmci, THREAD);\n+      }\n@@ -378,5 +427,2 @@\n-    guarantee(compiled_method != nullptr, \"deopt must be associated with an nmethod\");\n-    bool is_jvmci = compiled_method->is_compiled_by_jvmci();\n-    Deoptimization::reassign_fields(&deoptee, &map, objects, realloc_failures, is_jvmci);\n-    if (TraceDeoptimization) {\n-      print_objects(deoptee_thread, objects, realloc_failures);\n+    if (TraceDeoptimization && objects != nullptr) {\n+      print_objects(deoptee_thread, &deoptee, &map, objects, realloc_failures, thread);\n@@ -385,1 +431,1 @@\n-  if (save_oop_result) {\n+  if (save_oop_result || vk != nullptr) {\n@@ -387,1 +433,2 @@\n-    deoptee.set_saved_oop_result(&map, return_value());\n+    assert(return_oops.length() == 1, \"no inline type\");\n+    deoptee.set_saved_oop_result(&map, return_oops.pop()());\n@@ -718,1 +765,1 @@\n-  \/\/ If the sender is deoptimized the we must retrieve the address of the handler\n+  \/\/ If the sender is deoptimized we must retrieve the address of the handler\n@@ -1235,2 +1282,10 @@\n-\n-    oop obj = nullptr;\n+    k = get_refined_array_klass(k, fr, reg_map, sv, THREAD);\n+\n+    \/\/ Check if the object may be null and has an additional null_marker input that needs\n+    \/\/ to be checked before using the field values. Skip re-allocation if it is null.\n+    if (k->is_inline_klass() && sv->has_properties()) {\n+      jint null_marker = StackValue::create_stack_value(fr, reg_map, sv->properties())->get_jint();\n+      if (null_marker == 0) {\n+        continue;\n+      }\n+    }\n@@ -1239,0 +1294,1 @@\n+    oop obj = nullptr;\n@@ -1266,0 +1322,4 @@\n+    } else if (k->is_flatArray_klass()) {\n+      FlatArrayKlass* ak = FlatArrayKlass::cast(k);\n+      \/\/ Inline type array must be zeroed because not all memory is reassigned\n+      obj = ak->allocate_instance(sv->field_size(), ak->properties(), THREAD);\n@@ -1272,2 +1332,2 @@\n-    } else if (k->is_objArray_klass()) {\n-      ObjArrayKlass* ak = ObjArrayKlass::cast(k);\n+    } else if (k->is_refArray_klass()) {\n+      RefArrayKlass* ak = RefArrayKlass::cast(k);\n@@ -1275,1 +1335,1 @@\n-      obj = ak->allocate_instance(sv->field_size(), THREAD);\n+      obj = ak->allocate_instance(sv->field_size(), ak->properties(), THREAD);\n@@ -1297,0 +1357,15 @@\n+\/\/ We're deoptimizing at the return of a call, inline type fields are\n+\/\/ in registers. When we go back to the interpreter, it will expect a\n+\/\/ reference to an inline type instance. Allocate and initialize it from\n+\/\/ the register values here.\n+bool Deoptimization::realloc_inline_type_result(InlineKlass* vk, const RegisterMap& map, GrowableArray<Handle>& return_oops, TRAPS) {\n+  oop new_vt = vk->realloc_result(map, return_oops, THREAD);\n+  if (new_vt == nullptr) {\n+    CLEAR_PENDING_EXCEPTION;\n+    THROW_OOP_(Universe::out_of_memory_error_realloc_objects(), true);\n+  }\n+  return_oops.clear();\n+  return_oops.push(Handle(THREAD, new_vt));\n+  return false;\n+}\n+\n@@ -1468,0 +1543,3 @@\n+  InstanceKlass* _klass;\n+  bool _is_flat;\n+  bool _is_null_free;\n@@ -1469,4 +1547,1 @@\n-  ReassignedField() {\n-    _offset = 0;\n-    _type = T_ILLEGAL;\n-  }\n+  ReassignedField() : _offset(0), _type(T_ILLEGAL), _klass(nullptr), _is_flat(false), _is_null_free(false) { }\n@@ -1486,0 +1561,6 @@\n+      if (fs.is_flat()) {\n+        field._is_flat = true;\n+        field._is_null_free = fs.is_null_free_inline_type();\n+        \/\/ Resolve klass of flat inline type field\n+        field._klass = InlineKlass::cast(klass->get_inline_type_field_klass(fs.index()));\n+      }\n@@ -1492,2 +1573,3 @@\n-\/\/ Restore fields of an eliminated instance object employing the same field order used by the compiler.\n-static int reassign_fields_by_klass(InstanceKlass* klass, frame* fr, RegisterMap* reg_map, ObjectValue* sv, int svIndex, oop obj, bool is_jvmci) {\n+\/\/ Restore fields of an eliminated instance object employing the same field order used by the\n+\/\/ compiler when it scalarizes an object at safepoints.\n+static int reassign_fields_by_klass(InstanceKlass* klass, frame* fr, RegisterMap* reg_map, ObjectValue* sv, int svIndex, oop obj, bool is_jvmci, int base_offset, TRAPS) {\n@@ -1496,0 +1578,19 @@\n+    BasicType type = fields->at(i)._type;\n+    int offset = base_offset + fields->at(i)._offset;\n+    \/\/ Check for flat inline type field before accessing the ScopeValue because it might not have any fields\n+    if (fields->at(i)._is_flat) {\n+      \/\/ Recursively re-assign flat inline type fields\n+      InstanceKlass* vk = fields->at(i)._klass;\n+      assert(vk != nullptr, \"must be resolved\");\n+      offset -= InlineKlass::cast(vk)->payload_offset(); \/\/ Adjust offset to omit oop header\n+      svIndex = reassign_fields_by_klass(vk, fr, reg_map, sv, svIndex, obj, is_jvmci, offset, CHECK_0);\n+      if (!fields->at(i)._is_null_free) {\n+        ScopeValue* scope_field = sv->field_at(svIndex);\n+        StackValue* value = StackValue::create_stack_value(fr, reg_map, scope_field);\n+        int nm_offset = offset + InlineKlass::cast(vk)->null_marker_offset();\n+        obj->bool_field_put(nm_offset, value->get_jint() & 1);\n+        svIndex++;\n+      }\n+      continue; \/\/ Continue because we don't need to increment svIndex\n+    }\n+\n@@ -1498,3 +1599,2 @@\n-    int offset = fields->at(i)._offset;\n-    BasicType type = fields->at(i)._type;\n-      case T_OBJECT: case T_ARRAY:\n+      case T_OBJECT:\n+      case T_ARRAY:\n@@ -1572,0 +1672,1 @@\n+\n@@ -1575,0 +1676,23 @@\n+\/\/ restore fields of an eliminated inline type array\n+void Deoptimization::reassign_flat_array_elements(frame* fr, RegisterMap* reg_map, ObjectValue* sv, flatArrayOop obj, FlatArrayKlass* vak, bool is_jvmci, TRAPS) {\n+  InlineKlass* vk = vak->element_klass();\n+  assert(vk->maybe_flat_in_array(), \"should only be used for flat inline type arrays\");\n+  \/\/ Adjust offset to omit oop header\n+  int base_offset = arrayOopDesc::base_offset_in_bytes(T_FLAT_ELEMENT) - vk->payload_offset();\n+  \/\/ Initialize all elements of the flat inline type array\n+  for (int i = 0; i < sv->field_size(); i++) {\n+    ObjectValue* val = sv->field_at(i)->as_ObjectValue();\n+    int offset = base_offset + (i << Klass::layout_helper_log2_element_size(vak->layout_helper()));\n+    reassign_fields_by_klass(vk, fr, reg_map, val, 0, (oop)obj, is_jvmci, offset, CHECK);\n+    if (!obj->is_null_free_array()) {\n+      jboolean null_marker_value;\n+      if (val->has_properties()) {\n+        null_marker_value = StackValue::create_stack_value(fr, reg_map, val->properties())->get_jint() & 1;\n+      } else {\n+        null_marker_value = 1;\n+      }\n+      obj->bool_field_put(offset + vk->null_marker_offset(), null_marker_value);\n+    }\n+  }\n+}\n+\n@@ -1576,1 +1700,1 @@\n-void Deoptimization::reassign_fields(frame* fr, RegisterMap* reg_map, GrowableArray<ScopeValue*>* objects, bool realloc_failures, bool is_jvmci) {\n+void Deoptimization::reassign_fields(frame* fr, RegisterMap* reg_map, GrowableArray<ScopeValue*>* objects, bool realloc_failures, bool is_jvmci, TRAPS) {\n@@ -1581,0 +1705,2 @@\n+    k = get_refined_array_klass(k, fr, reg_map, sv, THREAD);\n+\n@@ -1582,1 +1708,1 @@\n-    assert(obj.not_null() || realloc_failures, \"reallocation was missed\");\n+    assert(obj.not_null() || realloc_failures || sv->has_properties(), \"reallocation was missed\");\n@@ -1620,1 +1746,4 @@\n-      reassign_fields_by_klass(ik, fr, reg_map, sv, 0, obj(), is_jvmci);\n+      reassign_fields_by_klass(ik, fr, reg_map, sv, 0, obj(), is_jvmci, 0, CHECK);\n+    } else if (k->is_flatArray_klass()) {\n+      FlatArrayKlass* vak = FlatArrayKlass::cast(k);\n+      reassign_flat_array_elements(fr, reg_map, sv, (flatArrayOop) obj(), vak, is_jvmci, CHECK);\n@@ -1624,1 +1753,1 @@\n-    } else if (k->is_objArray_klass()) {\n+    } else if (k->is_refArray_klass()) {\n@@ -1802,1 +1931,1 @@\n-  \/\/ Deoptimize only if the frame comes from compile code.\n+  \/\/ Deoptimize only if the frame comes from compiled code.\n","filename":"src\/hotspot\/share\/runtime\/deoptimization.cpp","additions":164,"deletions":35,"binary":false,"changes":199,"status":"modified"},{"patch":"@@ -38,0 +38,1 @@\n+#include \"oops\/inlineKlass.hpp\"\n@@ -62,0 +63,3 @@\n+#ifdef COMPILER1\n+#include \"c1\/c1_Runtime1.hpp\"\n+#endif\n@@ -364,0 +368,19 @@\n+\n+#ifdef COMPILER1\n+  if (nm->is_compiled_by_c1() && nm->method()->has_scalarized_args() &&\n+      pc() < nm->verified_inline_entry_point()) {\n+    \/\/ The VEP and VIEP(RO) of C1-compiled methods call into the runtime to buffer scalarized value\n+    \/\/ type args. We can't deoptimize at that point because the buffers have not yet been initialized.\n+    \/\/ Also, if the method is synchronized, we first need to acquire the lock.\n+    \/\/ Don't patch the return pc to delay deoptimization until we enter the method body (the check\n+    \/\/ added in LIRGenerator::do_Base will detect the pending deoptimization by checking the original_pc).\n+#if defined ASSERT && !defined AARCH64   \/\/ Stub call site does not look like NativeCall on AArch64\n+    NativeCall* call = nativeCall_before(this->pc());\n+    address dest = call->destination();\n+    assert(dest == Runtime1::entry_for(StubId::c1_buffer_inline_args_no_receiver_id) ||\n+           dest == Runtime1::entry_for(StubId::c1_buffer_inline_args_id), \"unexpected safepoint in entry point\");\n+#endif\n+    return;\n+  }\n+#endif\n+\n@@ -762,1 +785,1 @@\n-                          OopClosure* f) {\n+                          OopClosure* f, BufferedValueClosure* bvt_f) {\n@@ -774,1 +797,3 @@\n-      _f->do_oop(addr);\n+      if (_f != nullptr) {\n+        _f->do_oop(addr);\n+      }\n@@ -786,1 +811,3 @@\n-        _f->do_oop(addr);\n+        if (_f != nullptr) {\n+          _f->do_oop(addr);\n+        }\n@@ -948,1 +975,1 @@\n-  InterpreterFrameClosure blk(this, max_locals, m->max_stack(), f);\n+  InterpreterFrameClosure blk(this, max_locals, m->max_stack(), f, nullptr);\n@@ -960,0 +987,18 @@\n+void frame::buffered_values_interpreted_do(BufferedValueClosure* f) {\n+  assert(is_interpreted_frame(), \"Not an interpreted frame\");\n+  Thread *thread = Thread::current();\n+  methodHandle m (thread, interpreter_frame_method());\n+  jint      bci = interpreter_frame_bci();\n+\n+  assert(m->is_method(), \"checking frame value\");\n+  assert(!m->is_native() && bci >= 0 && bci < m->code_size(),\n+         \"invalid bci value\");\n+\n+  InterpreterFrameClosure blk(this, m->max_locals(), m->max_stack(), nullptr, f);\n+\n+  \/\/ process locals & expression stack\n+  InterpreterOopMap mask;\n+  m->mask_for(bci, &mask);\n+  mask.iterate_oop(&blk);\n+}\n+\n@@ -1015,0 +1060,1 @@\n+    assert(_offset < _arg_size, \"out of bounds\");\n@@ -1041,5 +1087,1 @@\n-    _arg_size  = ArgumentSizeComputer(signature).size() + (has_receiver ? 1 : 0) + (has_appendix ? 1 : 0);\n-\n-    int arg_size;\n-    _regs = SharedRuntime::find_callee_arguments(signature, has_receiver, has_appendix, &arg_size);\n-    assert(arg_size == _arg_size, \"wrong arg size\");\n+    _regs = SharedRuntime::find_callee_arguments(signature, has_receiver, has_appendix, &_arg_size);\n@@ -1425,2 +1467,2 @@\n-                    FormatBuffer<1024>(\"#%d nmethod \" INTPTR_FORMAT \" for method J %s%s\", frame_no,\n-                                       p2i(nm),\n+                    FormatBuffer<1024>(\"#%d nmethod (%s %d) \" INTPTR_FORMAT \" for method J %s%s\", frame_no,\n+                                       nm->is_compiled_by_c1() ? \"c1\" : \"c2\", nm->frame_size(), p2i(nm),\n@@ -1436,0 +1478,5 @@\n+      CompiledEntrySignature ces(m);\n+      ces.compute_calling_conventions(false);\n+      const GrowableArray<SigEntry>* sig_cc = nm->is_compiled_by_c2() ? ces.sig_cc() : ces.sig();\n+      const VMRegPair* regs = nm->is_compiled_by_c2() ? ces.regs_cc() : ces.regs();\n+\n@@ -1437,21 +1484,0 @@\n-      int sizeargs = m->size_of_parameters();\n-\n-      BasicType* sig_bt = NEW_RESOURCE_ARRAY(BasicType, sizeargs);\n-      VMRegPair* regs   = NEW_RESOURCE_ARRAY(VMRegPair, sizeargs);\n-      {\n-        int sig_index = 0;\n-        if (!m->is_static()) {\n-          sig_bt[sig_index++] = T_OBJECT; \/\/ 'this'\n-        }\n-        for (SignatureStream ss(m->signature()); !ss.at_return_type(); ss.next()) {\n-          BasicType t = ss.type();\n-          assert(type2size[t] == 1 || type2size[t] == 2, \"size is 1 or 2\");\n-          sig_bt[sig_index++] = t;\n-          if (type2size[t] == 2) {\n-            sig_bt[sig_index++] = T_VOID;\n-          }\n-        }\n-        assert(sig_index == sizeargs, \"\");\n-      }\n-      int stack_arg_slots = SharedRuntime::java_calling_convention(sig_bt, regs, sizeargs);\n-      assert(stack_arg_slots ==  nm->as_nmethod()->num_stack_arg_slots(false \/* rounded *\/) || nm->is_osr_method(), \"\");\n@@ -1461,1 +1487,1 @@\n-      for (SignatureStream ss(m->signature()); !ss.at_return_type(); ) {\n+      for (ExtendedSignature sig = ExtendedSignature(sig_cc, SigEntryFilter()); !sig.at_end(); ++sig) {\n@@ -1463,3 +1489,1 @@\n-        bool at_old_sp = false;\n-        BasicType t = (at_this ? T_OBJECT : ss.type());\n-        assert(t == sig_bt[sig_index], \"sigs in sync\");\n+        BasicType t = (*sig)._bt;\n@@ -1479,3 +1503,0 @@\n-        if (!at_this) {\n-          ss.next();\n-        }\n","filename":"src\/hotspot\/share\/runtime\/frame.cpp","additions":60,"deletions":39,"binary":false,"changes":99,"status":"modified"},{"patch":"@@ -816,1 +816,1 @@\n-  develop(bool, PrintFieldLayout, false,                                    \\\n+  product(bool, PrintFieldLayout, false, DIAGNOSTIC,                        \\\n@@ -819,0 +819,27 @@\n+  product(bool, PrintInlineLayout, false, DIAGNOSTIC,                       \\\n+          \"Print field layout for each inline type or class with inline fields\") \\\n+                                                                            \\\n+  product(bool, PrintFlatArrayLayout, false, DIAGNOSTIC,                    \\\n+          \"Print array layout for each inline type array\")                  \\\n+                                                                            \\\n+  product(bool, UseArrayFlattening, true,                                   \\\n+          \"Allow the VM to flatten arrays\")                                 \\\n+                                                                            \\\n+  product(bool, UseFieldFlattening, true,                                   \\\n+          \"Allow the VM to flatten value fields\")                           \\\n+                                                                            \\\n+  product(bool, UseNonAtomicValueFlattening, true,                          \\\n+          \"Allow the JVM to flatten some non-atomic null-free values\")      \\\n+                                                                            \\\n+  product(bool, UseNullableValueFlattening, true,                           \\\n+          \"Allow the JVM to flatten some nullable values\")                  \\\n+                                                                            \\\n+  product(bool, UseAtomicValueFlattening, true,                             \\\n+          \"Allow the JVM to flatten some atomic values\")                    \\\n+                                                                            \\\n+  product(intx, FlatArrayElementMaxOops, 4,                                 \\\n+          \"Max nof embedded object references in an inline type to flatten, <0 no limit\")  \\\n+                                                                            \\\n+  develop(ccstrlist, PrintInlineKlassFields, \"\",                            \\\n+          \"Print fields collected by InlineKlass::collect_fields\")          \\\n+                                                                            \\\n@@ -1777,0 +1804,3 @@\n+  product(bool, IgnoreAssertUnsetFields, false, DIAGNOSTIC,                           \\\n+          \"Ignore assert_unset_fields\")                                     \\\n+                                                                            \\\n@@ -1948,0 +1978,20 @@\n+  product(bool, EnableValhalla, true,                                       \\\n+          \"Enable experimental Valhalla features\")                          \\\n+                                                                            \\\n+  product_pd(bool, InlineTypePassFieldsAsArgs,                              \\\n+          \"Pass each inline type field as an argument at calls\")            \\\n+                                                                            \\\n+  product_pd(bool, InlineTypeReturnedAsFields,                              \\\n+          \"Return fields instead of an inline type reference\")              \\\n+                                                                            \\\n+  develop(bool, StressCallingConvention, false,                             \\\n+          \"Stress the scalarized calling convention.\")                      \\\n+                                                                            \\\n+  develop(bool, PreloadClasses, true,                                       \\\n+          \"Preloading all classes from the LoadableDescriptors attribute\")  \\\n+                                                                            \\\n+  product(ccstrlist, ForceNonTearable, \"\", DIAGNOSTIC,                      \\\n+          \"List of inline classes which are forced to be atomic \"           \\\n+          \"(whitespace and commas separate names, \"                         \\\n+          \"and leading and trailing stars '*' are wildcards)\")              \\\n+                                                                            \\\n@@ -2006,0 +2056,3 @@\n+  product(bool, UseAltSubstitutabilityMethod, false,                        \\\n+          \"Use alternate version of the isSubstitutable method to \"         \\\n+          \"compare value class instances\")                                  \\\n","filename":"src\/hotspot\/share\/runtime\/globals.hpp","additions":54,"deletions":1,"binary":false,"changes":55,"status":"modified"},{"patch":"@@ -50,0 +50,1 @@\n+#include \"oops\/inlineKlass.hpp\"\n","filename":"src\/hotspot\/share\/runtime\/javaThread.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -48,0 +48,1 @@\n+#include \"memory\/oopFactory.hpp\"\n@@ -51,0 +52,3 @@\n+#include \"oops\/access.hpp\"\n+#include \"oops\/fieldStreams.inline.hpp\"\n+#include \"oops\/inlineKlass.inline.hpp\"\n@@ -54,0 +58,1 @@\n+#include \"oops\/objArrayOop.inline.hpp\"\n@@ -73,0 +78,1 @@\n+#include \"runtime\/signature.hpp\"\n@@ -1246,0 +1252,16 @@\n+  \/\/ Substitutability test implementation piggy backs on static call resolution\n+  Bytecodes::Code code = caller->java_code_at(bci);\n+  if (code == Bytecodes::_if_acmpeq || code == Bytecodes::_if_acmpne) {\n+    bc = Bytecodes::_invokestatic;\n+    methodHandle attached_method(THREAD, extract_attached_method(vfst));\n+    assert(attached_method.not_null(), \"must have attached method\");\n+    vmClasses::ValueObjectMethods_klass()->initialize(CHECK_NH);\n+    LinkResolver::resolve_invoke(callinfo, receiver, attached_method, bc, false, CHECK_NH);\n+#ifdef ASSERT\n+    Symbol* subst_method_name = UseAltSubstitutabilityMethod ? vmSymbols::isSubstitutableAlt_name() : vmSymbols::isSubstitutable_name();\n+    Method* is_subst = vmClasses::ValueObjectMethods_klass()->find_method(subst_method_name, vmSymbols::object_object_boolean_signature());\n+    assert(callinfo.selected_method() == is_subst, \"must be isSubstitutable method\");\n+#endif\n+    return receiver;\n+  }\n+\n@@ -1281,0 +1303,6 @@\n+    } else {\n+      assert(attached_method->has_scalarized_args(), \"invalid use of attached method\");\n+      if (!attached_method->method_holder()->is_inline_klass()) {\n+        \/\/ Ignore the attached method in this case to not confuse below code\n+        attached_method = methodHandle(current, nullptr);\n+      }\n@@ -1289,0 +1317,1 @@\n+  bool check_null_and_abstract = true;\n@@ -1302,2 +1331,3 @@\n-    if (attached_method.is_null()) {\n-      Method* callee = bytecode.static_target(CHECK_NH);\n+    Method* callee = attached_method();\n+    if (callee == nullptr) {\n+      callee = bytecode.static_target(CHECK_NH);\n@@ -1308,7 +1338,17 @@\n-\n-    \/\/ Retrieve from a compiled argument list\n-    receiver = Handle(current, callerFrame.retrieve_receiver(&reg_map2));\n-    assert(oopDesc::is_oop_or_null(receiver()), \"\");\n-\n-    if (receiver.is_null()) {\n-      THROW_(vmSymbols::java_lang_NullPointerException(), nullHandle);\n+    bool caller_is_c1 = callerFrame.is_compiled_frame() && callerFrame.cb()->as_nmethod()->is_compiled_by_c1();\n+    if (!caller_is_c1 && callee->is_scalarized_arg(0)) {\n+      \/\/ If the receiver is an inline type that is passed as fields, no oop is available\n+      \/\/ Resolve the call without receiver null checking.\n+      assert(!callee->mismatch(), \"calls with inline type receivers should never mismatch\");\n+      assert(attached_method.not_null() && !attached_method->is_abstract(), \"must have non-abstract attached method\");\n+      if (bc == Bytecodes::_invokeinterface) {\n+        bc = Bytecodes::_invokevirtual; \/\/ C2 optimistically replaces interface calls by virtual calls\n+      }\n+      check_null_and_abstract = false;\n+    } else {\n+      \/\/ Retrieve from a compiled argument list\n+      receiver = Handle(current, callerFrame.retrieve_receiver(&reg_map2));\n+      assert(oopDesc::is_oop_or_null(receiver()), \"\");\n+      if (receiver.is_null()) {\n+        THROW_(vmSymbols::java_lang_NullPointerException(), nullHandle);\n+      }\n@@ -1321,1 +1361,1 @@\n-    LinkResolver::resolve_invoke(callinfo, receiver, attached_method, bc, CHECK_NH);\n+    LinkResolver::resolve_invoke(callinfo, receiver, attached_method, bc, check_null_and_abstract, CHECK_NH);\n@@ -1330,1 +1370,1 @@\n-  if (has_receiver) {\n+  if (has_receiver && check_null_and_abstract) {\n@@ -1358,1 +1398,1 @@\n-methodHandle SharedRuntime::find_callee_method(TRAPS) {\n+methodHandle SharedRuntime::find_callee_method(bool& caller_does_not_scalarize, TRAPS) {\n@@ -1384,0 +1424,4 @@\n+    \/\/ Calls via mismatching methods are always non-scalarized\n+    if (callinfo.resolved_method()->mismatch()) {\n+      caller_does_not_scalarize = true;\n+    }\n@@ -1391,1 +1435,1 @@\n-methodHandle SharedRuntime::resolve_helper(bool is_virtual, bool is_optimized, TRAPS) {\n+methodHandle SharedRuntime::resolve_helper(bool is_virtual, bool is_optimized, bool& caller_does_not_scalarize, TRAPS) {\n@@ -1414,0 +1458,4 @@\n+  \/\/ Calls via mismatching methods are always non-scalarized\n+  if (caller_nm->is_compiled_by_c1() || call_info.resolved_method()->mismatch()) {\n+    caller_does_not_scalarize = true;\n+  }\n@@ -1432,1 +1480,1 @@\n-    tty->print(\"resolving %s%s (%s) call to\",\n+    tty->print(\"resolving %s%s (%s) %s call to\",\n@@ -1434,1 +1482,1 @@\n-               Bytecodes::name(invoke_code));\n+               Bytecodes::name(invoke_code), (caller_does_not_scalarize) ? \"non-scalar\" : \"\");\n@@ -1473,1 +1521,1 @@\n-    inline_cache->update(&call_info, receiver->klass());\n+    inline_cache->update(&call_info, receiver->klass(), caller_does_not_scalarize);\n@@ -1477,1 +1525,1 @@\n-    callsite->set(callee_method);\n+    callsite->set(callee_method, caller_does_not_scalarize);\n@@ -1497,0 +1545,1 @@\n+  bool caller_does_not_scalarize = false;\n@@ -1498,1 +1547,1 @@\n-    callee_method = SharedRuntime::handle_ic_miss_helper(CHECK_NULL);\n+    callee_method = SharedRuntime::handle_ic_miss_helper(caller_does_not_scalarize, CHECK_NULL);\n@@ -1503,1 +1552,1 @@\n-  return get_resolved_entry(current, callee_method);\n+  return get_resolved_entry(current, callee_method, false, false, caller_does_not_scalarize);\n@@ -1544,1 +1593,5 @@\n-      return callee->get_c2i_entry();\n+      if (caller_frame.is_interpreted_frame()) {\n+        return callee->get_c2i_inline_entry();\n+      } else {\n+        return callee->get_c2i_entry();\n+      }\n@@ -1550,0 +1603,3 @@\n+  bool is_static_call = false;\n+  bool is_optimized = false;\n+  bool caller_does_not_scalarize = false;\n@@ -1552,1 +1608,1 @@\n-    callee_method = SharedRuntime::reresolve_call_site(CHECK_NULL);\n+    callee_method = SharedRuntime::reresolve_call_site(is_optimized, caller_does_not_scalarize, CHECK_NULL);\n@@ -1556,1 +1612,1 @@\n-  return get_resolved_entry(current, callee_method);\n+  return get_resolved_entry(current, callee_method, callee_method->is_static(), is_optimized, caller_does_not_scalarize);\n@@ -1595,1 +1651,2 @@\n-address SharedRuntime::get_resolved_entry(JavaThread* current, methodHandle callee_method) {\n+address SharedRuntime::get_resolved_entry(JavaThread* current, methodHandle callee_method,\n+                                          bool is_static_call, bool is_optimized, bool caller_does_not_scalarize) {\n@@ -1601,2 +1658,11 @@\n-  assert(callee_method->verified_code_entry() != nullptr, \" Jump to zero!\");\n-  return callee_method->verified_code_entry();\n+\n+  if (caller_does_not_scalarize) {\n+    assert(callee_method->verified_inline_code_entry() != nullptr, \"Jump to zero!\");\n+    return callee_method->verified_inline_code_entry();\n+  } else if (is_static_call || is_optimized) {\n+    assert(callee_method->verified_code_entry() != nullptr, \"Jump to zero!\");\n+    return callee_method->verified_code_entry();\n+  } else {\n+    assert(callee_method->verified_inline_ro_code_entry() != nullptr, \"Jump to zero!\");\n+    return callee_method->verified_inline_ro_code_entry();\n+  }\n@@ -1608,0 +1674,1 @@\n+  bool caller_does_not_scalarize = false;\n@@ -1610,1 +1677,1 @@\n-    callee_method = SharedRuntime::resolve_helper(false, false, CHECK_NULL);\n+    callee_method = SharedRuntime::resolve_helper(false, false, caller_does_not_scalarize, CHECK_NULL);\n@@ -1614,1 +1681,1 @@\n-  return get_resolved_entry(current, callee_method);\n+  return get_resolved_entry(current, callee_method, true, false, caller_does_not_scalarize);\n@@ -1620,0 +1687,1 @@\n+  bool caller_does_not_scalarize = false;\n@@ -1621,1 +1689,1 @@\n-    callee_method = SharedRuntime::resolve_helper(true, false, CHECK_NULL);\n+    callee_method = SharedRuntime::resolve_helper(true, false, caller_does_not_scalarize, CHECK_NULL);\n@@ -1625,1 +1693,1 @@\n-  return get_resolved_entry(current, callee_method);\n+  return get_resolved_entry(current, callee_method, false, false, caller_does_not_scalarize);\n@@ -1633,0 +1701,1 @@\n+  bool caller_does_not_scalarize = false;\n@@ -1634,1 +1703,1 @@\n-    callee_method = SharedRuntime::resolve_helper(true, true, CHECK_NULL);\n+    callee_method = SharedRuntime::resolve_helper(true, true, caller_does_not_scalarize, CHECK_NULL);\n@@ -1638,1 +1707,1 @@\n-  return get_resolved_entry(current, callee_method);\n+  return get_resolved_entry(current, callee_method, false, true, caller_does_not_scalarize);\n@@ -1641,1 +1710,3 @@\n-methodHandle SharedRuntime::handle_ic_miss_helper(TRAPS) {\n+\n+\n+methodHandle SharedRuntime::handle_ic_miss_helper(bool& caller_does_not_scalarize, TRAPS) {\n@@ -1659,1 +1730,1 @@\n-    tty->print(\"IC miss (%s) call to\", Bytecodes::name(bc));\n+    tty->print(\"IC miss (%s) %s call to\", Bytecodes::name(bc), (caller_does_not_scalarize) ? \"non-scalar\" : \"\");\n@@ -1691,0 +1762,4 @@\n+  \/\/ Calls via mismatching methods are always non-scalarized\n+  if (caller_nm->is_compiled_by_c1() || call_info.resolved_method()->mismatch()) {\n+    caller_does_not_scalarize = true;\n+  }\n@@ -1694,1 +1769,1 @@\n-  inline_cache->update(&call_info, receiver()->klass());\n+  inline_cache->update(&call_info, receiver()->klass(), caller_does_not_scalarize);\n@@ -1705,1 +1780,1 @@\n-methodHandle SharedRuntime::reresolve_call_site(TRAPS) {\n+methodHandle SharedRuntime::reresolve_call_site(bool& is_optimized, bool& caller_does_not_scalarize, TRAPS) {\n@@ -1715,8 +1790,21 @@\n-\n-  \/\/ Do nothing if the frame isn't a live compiled frame.\n-  \/\/ nmethod could be deoptimized by the time we get here\n-  \/\/ so no update to the caller is needed.\n-\n-  if ((caller.is_compiled_frame() && !caller.is_deoptimized_frame()) ||\n-      (caller.is_native_frame() && caller.cb()->as_nmethod()->method()->is_continuation_enter_intrinsic())) {\n-\n+  if (caller.is_compiled_frame()) {\n+    caller_does_not_scalarize = caller.cb()->as_nmethod()->is_compiled_by_c1();\n+  }\n+  assert(!caller.is_interpreted_frame(), \"must be compiled\");\n+\n+  \/\/ If the frame isn't a live compiled frame (i.e. deoptimized by the time we get here), no IC clearing must be done\n+  \/\/ for the caller. However, when the caller is C2 compiled and the callee a C1 or C2 compiled method, then we still\n+  \/\/ need to figure out whether it was an optimized virtual call with an inline type receiver. Otherwise, we end up\n+  \/\/ using the wrong method entry point and accidentally skip the buffering of the receiver.\n+  methodHandle callee_method = find_callee_method(caller_does_not_scalarize, CHECK_(methodHandle()));\n+  const bool caller_is_compiled_and_not_deoptimized = caller.is_compiled_frame() && !caller.is_deoptimized_frame();\n+  const bool caller_is_continuation_enter_intrinsic =\n+    caller.is_native_frame() && caller.cb()->as_nmethod()->method()->is_continuation_enter_intrinsic();\n+  const bool do_IC_clearing = caller_is_compiled_and_not_deoptimized || caller_is_continuation_enter_intrinsic;\n+\n+  const bool callee_compiled_with_scalarized_receiver = callee_method->has_compiled_code() &&\n+                                                        !callee_method()->is_static() &&\n+                                                        callee_method()->is_scalarized_arg(0);\n+  const bool compute_is_optimized = !caller_does_not_scalarize && callee_compiled_with_scalarized_receiver;\n+\n+  if (do_IC_clearing || compute_is_optimized) {\n@@ -1755,0 +1843,1 @@\n+        is_optimized = false;\n@@ -1757,0 +1846,1 @@\n+            assert(callee_method->is_static(), \"must be\");\n@@ -1758,2 +1848,5 @@\n-            CompiledDirectCall* cdc = CompiledDirectCall::at(call_addr);\n-            cdc->set_to_clean();\n+            is_optimized = (iter.type() == relocInfo::opt_virtual_call_type);\n+            if (do_IC_clearing) {\n+              CompiledDirectCall* cdc = CompiledDirectCall::at(call_addr);\n+              cdc->set_to_clean();\n+            }\n@@ -1762,4 +1855,5 @@\n-\n-            \/\/ compiled, dispatched call (which used to call an interpreted method)\n-            CompiledIC* inline_cache = CompiledIC_at(caller_nm, call_addr);\n-            inline_cache->set_to_clean();\n+            if (do_IC_clearing) {\n+              \/\/ compiled, dispatched call (which used to call an interpreted method)\n+              CompiledIC* inline_cache = CompiledIC_at(caller_nm, call_addr);\n+              inline_cache->set_to_clean();\n+            }\n@@ -1776,3 +1870,0 @@\n-  methodHandle callee_method = find_callee_method(CHECK_(methodHandle()));\n-\n-\n@@ -1784,1 +1875,1 @@\n-    tty->print(\"handle_wrong_method reresolving call to\");\n+    tty->print(\"handle_wrong_method reresolving %s call to\", (caller_does_not_scalarize) ? \"non-scalar\" : \"\");\n@@ -1990,0 +2081,15 @@\n+char* SharedRuntime::generate_identity_exception_message(JavaThread* current, Klass* klass) {\n+  assert(klass->is_inline_klass(), \"Must be a concrete value class\");\n+  const char* desc = \"Cannot synchronize on an instance of value class \";\n+  const char* className = klass->external_name();\n+  size_t msglen = strlen(desc) + strlen(className) + 1;\n+  char* message = NEW_RESOURCE_ARRAY(char, msglen);\n+  if (nullptr == message) {\n+    \/\/ Out of memory: can't create detailed error message\n+    message = const_cast<char*>(klass->external_name());\n+  } else {\n+    jio_snprintf(message, msglen, \"%s%s\", desc, className);\n+  }\n+  return message;\n+}\n+\n@@ -2223,5 +2329,30 @@\n- private:\n-  enum {\n-    _basic_type_bits = 4,\n-    _basic_type_mask = right_n_bits(_basic_type_bits),\n-    _basic_types_per_int = BitsPerInt \/ _basic_type_bits,\n+public:\n+  class Element {\n+  private:\n+    \/\/ The highest byte is the type of the argument. The remaining bytes contain the offset of the\n+    \/\/ field if it is flattened in the calling convention, -1 otherwise.\n+    juint _payload;\n+\n+    static constexpr int offset_bit_width = 24;\n+    static constexpr juint offset_bit_mask = (1 << offset_bit_width) - 1;\n+  public:\n+    Element(BasicType bt, int offset) : _payload((static_cast<juint>(bt) << offset_bit_width) | (juint(offset) & offset_bit_mask)) {\n+      assert(offset >= -1 && offset < jint(offset_bit_mask), \"invalid offset %d\", offset);\n+    }\n+\n+    BasicType bt() const {\n+      return static_cast<BasicType>(_payload >> offset_bit_width);\n+    }\n+\n+    int offset() const {\n+      juint res = _payload & offset_bit_mask;\n+      return res == offset_bit_mask ? -1 : res;\n+    }\n+\n+    juint hash() const {\n+      return _payload;\n+    }\n+\n+    bool operator!=(const Element& other) const {\n+      return _payload != other._payload;\n+    }\n@@ -2229,3 +2360,3 @@\n-  \/\/ TO DO:  Consider integrating this with a more global scheme for compressing signatures.\n-  \/\/ For now, 4 bits per components (plus T_VOID gaps after double\/long) is not excessive.\n-  int _length;\n+private:\n+  const bool _has_ro_adapter;\n+  const int _length;\n@@ -2235,2 +2366,8 @@\n-  int* data_pointer() {\n-    return (int*)((address)this + data_offset());\n+  Element* data_pointer() {\n+    return reinterpret_cast<Element*>(reinterpret_cast<address>(this) + data_offset());\n+  }\n+\n+  const Element& element_at(int index) {\n+    assert(index < length(), \"index %d out of bounds for length %d\", index, length());\n+    Element* data = data_pointer();\n+    return data[index];\n@@ -2240,6 +2377,5 @@\n-  AdapterFingerPrint(int total_args_passed, BasicType* sig_bt, int len) {\n-    int* data = data_pointer();\n-    \/\/ Pack the BasicTypes with 8 per int\n-    assert(len == length(total_args_passed), \"sanity\");\n-    _length = len;\n-    int sig_index = 0;\n+  AdapterFingerPrint(const GrowableArray<SigEntry>* sig, bool has_ro_adapter)\n+    : _has_ro_adapter(has_ro_adapter), _length(total_args_passed_in_sig(sig)) {\n+    Element* data = data_pointer();\n+    BasicType prev_bt = T_ILLEGAL;\n+    int vt_count = 0;\n@@ -2247,5 +2383,15 @@\n-      int value = 0;\n-      for (int byte = 0; sig_index < total_args_passed && byte < _basic_types_per_int; byte++) {\n-        int bt = adapter_encoding(sig_bt[sig_index++]);\n-        assert((bt & _basic_type_mask) == bt, \"must fit in 4 bits\");\n-        value = (value << _basic_type_bits) | bt;\n+      const SigEntry& sig_entry = sig->at(index);\n+      BasicType bt = sig_entry._bt;\n+      if (bt == T_METADATA) {\n+        \/\/ Found start of inline type in signature\n+        assert(InlineTypePassFieldsAsArgs, \"unexpected start of inline type\");\n+        vt_count++;\n+      } else if (bt == T_VOID && prev_bt != T_LONG && prev_bt != T_DOUBLE) {\n+        \/\/ Found end of inline type in signature\n+        assert(InlineTypePassFieldsAsArgs, \"unexpected end of inline type\");\n+        vt_count--;\n+        assert(vt_count >= 0, \"invalid vt_count\");\n+      } else if (vt_count == 0) {\n+        \/\/ Widen fields that are not part of a scalarized inline type argument\n+        assert(sig_entry._offset == -1, \"invalid offset for argument that is not a flattened field %d\", sig_entry._offset);\n+        bt = adapter_encoding(bt);\n@@ -2253,1 +2399,3 @@\n-      data[index] = value;\n+\n+      ::new(&data[index]) Element(bt, sig_entry._offset);\n+      prev_bt = bt;\n@@ -2255,0 +2403,1 @@\n+    assert(vt_count == 0, \"invalid vt_count\");\n@@ -2262,2 +2411,2 @@\n-  static int length(int total_args) {\n-    return (total_args + (_basic_types_per_int-1)) \/ _basic_types_per_int;\n+  static int total_args_passed_in_sig(const GrowableArray<SigEntry>* sig) {\n+    return (sig != nullptr) ? sig->length() : 0;\n@@ -2267,1 +2416,1 @@\n-    return (int)heap_word_size(sizeof(AdapterFingerPrint) + (len * sizeof(int)));\n+    return (int)heap_word_size(sizeof(AdapterFingerPrint) + (len * sizeof(Element)));\n@@ -2273,1 +2422,1 @@\n-  static int adapter_encoding(BasicType in) {\n+  static BasicType adapter_encoding(BasicType in) {\n@@ -2279,1 +2428,1 @@\n-        \/\/ There are all promoted to T_INT in the calling convention\n+        \/\/ They are all promoted to T_INT in the calling convention\n@@ -2312,0 +2461,1 @@\n+public:\n@@ -2315,10 +2465,1 @@\n-      unsigned val = (unsigned)value(i);\n-      \/\/ args are packed so that first\/lower arguments are in the highest\n-      \/\/ bits of each int value, so iterate from highest to the lowest\n-      for (int j = 32 - _basic_type_bits; j >= 0; j -= _basic_type_bits) {\n-        unsigned v = (val >> j) & _basic_type_mask;\n-        if (v == 0) {\n-          continue;\n-        }\n-        function(v);\n-      }\n+      function(element_at(i));\n@@ -2328,3 +2469,2 @@\n- public:\n-  static AdapterFingerPrint* allocate(int total_args_passed, BasicType* sig_bt) {\n-    int len = length(total_args_passed);\n+  static AdapterFingerPrint* allocate(const GrowableArray<SigEntry>* sig, bool has_ro_adapter = false) {\n+    int len = total_args_passed_in_sig(sig);\n@@ -2332,1 +2472,1 @@\n-    AdapterFingerPrint* afp = new (size_in_bytes) AdapterFingerPrint(total_args_passed, sig_bt, len);\n+    AdapterFingerPrint* afp = new (size_in_bytes) AdapterFingerPrint(sig, has_ro_adapter);\n@@ -2341,3 +2481,2 @@\n-  int value(int index) {\n-    int* data = data_pointer();\n-    return data[index];\n+  bool has_ro_adapter() const {\n+    return _has_ro_adapter;\n@@ -2346,1 +2485,1 @@\n-  int length() {\n+  int length() const {\n@@ -2353,1 +2492,1 @@\n-      int v = value(i);\n+      const Element& v = element_at(i);\n@@ -2355,1 +2494,1 @@\n-      hash = ((hash << 8) ^ v ^ (hash >> 5)) + 3;\n+      hash = ((hash << 8) ^ v.hash() ^ (hash >> 5)) + 3;\n@@ -2362,1 +2501,6 @@\n-    st.print(\"0x\");\n+    st.print(\"{\");\n+    if (_has_ro_adapter) {\n+      st.print(\"has_ro_adapter\");\n+    } else {\n+      st.print(\"no_ro_adapter\");\n+    }\n@@ -2364,1 +2508,3 @@\n-      st.print(\"%x\", value(i));\n+      st.print(\", \");\n+      const Element& elem = element_at(i);\n+      st.print(\"{%s, %d}\", type2name(elem.bt()), elem.offset());\n@@ -2366,0 +2512,1 @@\n+    st.print(\"}\");\n@@ -2372,1 +2519,1 @@\n-    iterate_args([&] (int arg) {\n+    iterate_args([&] (const Element& arg) {\n@@ -2375,1 +2522,1 @@\n-        if (arg == T_VOID) {\n+        if (arg.bt() == T_VOID) {\n@@ -2381,7 +2528,4 @@\n-      switch (arg) {\n-        case T_INT:    st.print(\"I\");    break;\n-        case T_LONG:   long_prev = true; break;\n-        case T_FLOAT:  st.print(\"F\");    break;\n-        case T_DOUBLE: st.print(\"D\");    break;\n-        case T_VOID:   break;\n-        default: ShouldNotReachHere();\n+      if (arg.bt() == T_LONG) {\n+        long_prev = true;\n+      } else if (arg.bt() != T_VOID) {\n+        st.print(\"%c\", type2char(arg.bt()));\n@@ -2396,52 +2540,3 @@\n-  BasicType* as_basic_type(int& nargs) {\n-    nargs = 0;\n-    GrowableArray<BasicType> btarray;\n-    bool long_prev = false;\n-\n-    iterate_args([&] (int arg) {\n-      if (long_prev) {\n-        long_prev = false;\n-        if (arg == T_VOID) {\n-          btarray.append(T_LONG);\n-        } else {\n-          btarray.append(T_OBJECT); \/\/ it could be T_ARRAY; it shouldn't matter\n-        }\n-      }\n-      switch (arg) {\n-        case T_INT: \/\/ fallthrough\n-        case T_FLOAT: \/\/ fallthrough\n-        case T_DOUBLE:\n-        case T_VOID:\n-          btarray.append((BasicType)arg);\n-          break;\n-        case T_LONG:\n-          long_prev = true;\n-          break;\n-        default: ShouldNotReachHere();\n-      }\n-    });\n-\n-    if (long_prev) {\n-      btarray.append(T_OBJECT);\n-    }\n-\n-    nargs = btarray.length();\n-    BasicType* sig_bt = NEW_RESOURCE_ARRAY(BasicType, nargs);\n-    int index = 0;\n-    GrowableArrayIterator<BasicType> iter = btarray.begin();\n-    while (iter != btarray.end()) {\n-      sig_bt[index++] = *iter;\n-      ++iter;\n-    }\n-    assert(index == btarray.length(), \"sanity check\");\n-#ifdef ASSERT\n-    {\n-      AdapterFingerPrint* compare_fp = AdapterFingerPrint::allocate(nargs, sig_bt);\n-      assert(this->equals(compare_fp), \"sanity check\");\n-      AdapterFingerPrint::deallocate(compare_fp);\n-    }\n-#endif\n-    return sig_bt;\n-  }\n-\n-    if (other->_length != _length) {\n+    if (other->_has_ro_adapter != _has_ro_adapter) {\n+      return false;\n+    } else if (other->_length != _length) {\n@@ -2452,1 +2547,1 @@\n-        if (value(i) != other->value(i)) {\n+        if (element_at(i) != other->element_at(i)) {\n@@ -2495,1 +2590,1 @@\n-AdapterHandlerEntry* AdapterHandlerLibrary::lookup(int total_args_passed, BasicType* sig_bt) {\n+AdapterHandlerEntry* AdapterHandlerLibrary::lookup(const GrowableArray<SigEntry>* sig, bool has_ro_adapter) {\n@@ -2498,1 +2593,1 @@\n-  AdapterFingerPrint* fp = AdapterFingerPrint::allocate(total_args_passed, sig_bt);\n+  AdapterFingerPrint* fp = AdapterFingerPrint::allocate(sig, has_ro_adapter);\n@@ -2555,1 +2650,1 @@\n-static const int AdapterHandlerLibrary_size = 16*K;\n+static const int AdapterHandlerLibrary_size = 48*K;\n@@ -2603,1 +2698,3 @@\n-    _no_arg_handler = create_adapter(0, nullptr);\n+    CompiledEntrySignature no_args;\n+    no_args.compute_calling_conventions();\n+    _no_arg_handler = create_adapter(no_args, true);\n@@ -2605,2 +2702,4 @@\n-    BasicType obj_args[] = { T_OBJECT };\n-    _obj_arg_handler = create_adapter(1, obj_args);\n+    CompiledEntrySignature obj_args;\n+    SigEntry::add_entry(obj_args.sig(), T_OBJECT);\n+    obj_args.compute_calling_conventions();\n+    _obj_arg_handler = create_adapter(obj_args, true);\n@@ -2608,2 +2707,4 @@\n-    BasicType int_args[] = { T_INT };\n-    _int_arg_handler = create_adapter(1, int_args);\n+    CompiledEntrySignature int_args;\n+    SigEntry::add_entry(int_args.sig(), T_INT);\n+    int_args.compute_calling_conventions();\n+    _int_arg_handler = create_adapter(int_args, true);\n@@ -2611,2 +2712,5 @@\n-    BasicType obj_int_args[] = { T_OBJECT, T_INT };\n-    _obj_int_arg_handler = create_adapter(2, obj_int_args);\n+    CompiledEntrySignature obj_int_args;\n+    SigEntry::add_entry(obj_int_args.sig(), T_OBJECT);\n+    SigEntry::add_entry(obj_int_args.sig(), T_INT);\n+    obj_int_args.compute_calling_conventions();\n+    _obj_int_arg_handler = create_adapter(obj_int_args, true);\n@@ -2614,2 +2718,5 @@\n-    BasicType obj_obj_args[] = { T_OBJECT, T_OBJECT };\n-    _obj_obj_arg_handler = create_adapter(2, obj_obj_args);\n+    CompiledEntrySignature obj_obj_args;\n+    SigEntry::add_entry(obj_obj_args.sig(), T_OBJECT);\n+    SigEntry::add_entry(obj_obj_args.sig(), T_OBJECT);\n+    obj_obj_args.compute_calling_conventions();\n+    _obj_obj_arg_handler = create_adapter(obj_obj_args, true);\n@@ -2649,0 +2756,3 @@\n+      if (InlineTypePassFieldsAsArgs && method->method_holder()->is_inline_klass()) {\n+        return nullptr;\n+      }\n@@ -2652,1 +2762,10 @@\n-      case JVM_SIGNATURE_CLASS:\n+      case JVM_SIGNATURE_CLASS: {\n+        if (InlineTypePassFieldsAsArgs) {\n+          SignatureStream ss(method->signature());\n+          InlineKlass* vk = ss.as_inline_klass(method->method_holder());\n+          if (vk != nullptr) {\n+            return nullptr;\n+          }\n+        }\n+        return _obj_arg_handler;\n+      }\n@@ -2663,1 +2782,1 @@\n-             !method->is_static()) {\n+             !method->is_static() && (!InlineTypePassFieldsAsArgs || !method->method_holder()->is_inline_klass())) {\n@@ -2665,1 +2784,10 @@\n-      case JVM_SIGNATURE_CLASS:\n+      case JVM_SIGNATURE_CLASS: {\n+        if (InlineTypePassFieldsAsArgs) {\n+          SignatureStream ss(method->signature());\n+          InlineKlass* vk = ss.as_inline_klass(method->method_holder());\n+          if (vk != nullptr) {\n+            return nullptr;\n+          }\n+        }\n+        return _obj_obj_arg_handler;\n+      }\n@@ -2679,5 +2807,9 @@\n-class AdapterSignatureIterator : public SignatureIterator {\n- private:\n-  BasicType stack_sig_bt[16];\n-  BasicType* sig_bt;\n-  int index;\n+CompiledEntrySignature::CompiledEntrySignature(Method* method) :\n+  _method(method), _num_inline_args(0), _has_inline_recv(false),\n+  _regs(nullptr), _regs_cc(nullptr), _regs_cc_ro(nullptr),\n+  _args_on_stack(0), _args_on_stack_cc(0), _args_on_stack_cc_ro(0),\n+  _c1_needs_stack_repair(false), _c2_needs_stack_repair(false), _supers(nullptr) {\n+  _sig = new GrowableArray<SigEntry>((method != nullptr) ? method->size_of_parameters() : 1);\n+  _sig_cc = new GrowableArray<SigEntry>((method != nullptr) ? method->size_of_parameters() : 1);\n+  _sig_cc_ro = new GrowableArray<SigEntry>((method != nullptr) ? method->size_of_parameters() : 1);\n+}\n@@ -2685,11 +2817,24 @@\n- public:\n-  AdapterSignatureIterator(Symbol* signature,\n-                           fingerprint_t fingerprint,\n-                           bool is_static,\n-                           int total_args_passed) :\n-    SignatureIterator(signature, fingerprint),\n-    index(0)\n-  {\n-    sig_bt = (total_args_passed <= 16) ? stack_sig_bt : NEW_RESOURCE_ARRAY(BasicType, total_args_passed);\n-    if (!is_static) { \/\/ Pass in receiver first\n-      sig_bt[index++] = T_OBJECT;\n+\/\/ See if we can save space by sharing the same entry for VIEP and VIEP(RO),\n+\/\/ or the same entry for VEP and VIEP(RO).\n+CodeOffsets::Entries CompiledEntrySignature::c1_inline_ro_entry_type() const {\n+  if (!has_scalarized_args()) {\n+    \/\/ VEP\/VIEP\/VIEP(RO) all share the same entry. There's no packing.\n+    return CodeOffsets::Verified_Entry;\n+  }\n+  if (_method->is_static()) {\n+    \/\/ Static methods don't need VIEP(RO)\n+    return CodeOffsets::Verified_Entry;\n+  }\n+\n+  if (has_inline_recv()) {\n+    if (num_inline_args() == 1) {\n+      \/\/ Share same entry for VIEP and VIEP(RO).\n+      \/\/ This is quite common: we have an instance method in an InlineKlass that has\n+      \/\/ no inline type args other than <this>.\n+      return CodeOffsets::Verified_Inline_Entry;\n+    } else {\n+      assert(num_inline_args() > 1, \"must be\");\n+      \/\/ No sharing:\n+      \/\/   VIEP(RO) -- <this> is passed as object\n+      \/\/   VEP      -- <this> is passed as fields\n+      return CodeOffsets::Verified_Inline_Entry_RO;\n@@ -2697,1 +2842,0 @@\n-    do_parameters_on(this);\n@@ -2700,2 +2844,9 @@\n-  BasicType* basic_types() {\n-    return sig_bt;\n+  \/\/ Either a static method, or <this> is not an inline type\n+  if (args_on_stack_cc() != args_on_stack_cc_ro()) {\n+    \/\/ No sharing:\n+    \/\/ Some arguments are passed on the stack, and we have inserted reserved entries\n+    \/\/ into the VEP, but we never insert reserved entries into the VIEP(RO).\n+    return CodeOffsets::Verified_Inline_Entry_RO;\n+  } else {\n+    \/\/ Share same entry for VEP and VIEP(RO).\n+    return CodeOffsets::Verified_Entry;\n@@ -2703,0 +2854,1 @@\n+}\n@@ -2704,3 +2856,39 @@\n-#ifdef ASSERT\n-  int slots() {\n-    return index;\n+\/\/ Returns all super methods (transitive) in classes and interfaces that are overridden by the current method.\n+GrowableArray<Method*>* CompiledEntrySignature::get_supers() {\n+  if (_supers != nullptr) {\n+    return _supers;\n+  }\n+  _supers = new GrowableArray<Method*>();\n+  \/\/ Skip private, static, and <init> methods\n+  if (_method->is_private() || _method->is_static() || _method->is_object_constructor()) {\n+    return _supers;\n+  }\n+  Symbol* name = _method->name();\n+  Symbol* signature = _method->signature();\n+  const Klass* holder = _method->method_holder()->super();\n+  Symbol* holder_name = holder->name();\n+  ThreadInVMfromUnknown tiv;\n+  JavaThread* current = JavaThread::current();\n+  HandleMark hm(current);\n+  Handle loader(current, _method->method_holder()->class_loader());\n+\n+  \/\/ Walk up the class hierarchy and search for super methods\n+  while (holder != nullptr) {\n+    Method* super_method = holder->lookup_method(name, signature);\n+    if (super_method == nullptr) {\n+      break;\n+    }\n+    if (!super_method->is_static() && !super_method->is_private() &&\n+        (!super_method->is_package_private() ||\n+         super_method->method_holder()->is_same_class_package(loader(), holder_name))) {\n+      _supers->push(super_method);\n+    }\n+    holder = super_method->method_holder()->super();\n+  }\n+  \/\/ Search interfaces for super methods\n+  Array<InstanceKlass*>* interfaces = _method->method_holder()->transitive_interfaces();\n+  for (int i = 0; i < interfaces->length(); ++i) {\n+    Method* m = interfaces->at(i)->lookup_method(name, signature);\n+    if (m != nullptr && !m->is_static() && m->is_public()) {\n+      _supers->push(m);\n+    }\n@@ -2708,0 +2896,50 @@\n+  return _supers;\n+}\n+\n+\/\/ Iterate over arguments and compute scalarized and non-scalarized signatures\n+void CompiledEntrySignature::compute_calling_conventions(bool init) {\n+  bool has_scalarized = false;\n+  if (_method != nullptr) {\n+    InstanceKlass* holder = _method->method_holder();\n+    int arg_num = 0;\n+    if (!_method->is_static()) {\n+      \/\/ We shouldn't scalarize 'this' in a value class constructor\n+      if (holder->is_inline_klass() && InlineKlass::cast(holder)->can_be_passed_as_fields() && !_method->is_object_constructor() &&\n+          (init || _method->is_scalarized_arg(arg_num))) {\n+        _sig_cc->appendAll(InlineKlass::cast(holder)->extended_sig());\n+        has_scalarized = true;\n+        _has_inline_recv = true;\n+        _num_inline_args++;\n+      } else {\n+        SigEntry::add_entry(_sig_cc, T_OBJECT, holder->name());\n+      }\n+      SigEntry::add_entry(_sig, T_OBJECT, holder->name());\n+      SigEntry::add_entry(_sig_cc_ro, T_OBJECT, holder->name());\n+      arg_num++;\n+    }\n+    for (SignatureStream ss(_method->signature()); !ss.at_return_type(); ss.next()) {\n+      BasicType bt = ss.type();\n+      if (bt == T_OBJECT) {\n+        InlineKlass* vk = ss.as_inline_klass(holder);\n+        if (vk != nullptr && vk->can_be_passed_as_fields() && (init || _method->is_scalarized_arg(arg_num))) {\n+          \/\/ Check for a calling convention mismatch with super method(s)\n+          bool scalar_super = false;\n+          bool non_scalar_super = false;\n+          GrowableArray<Method*>* supers = get_supers();\n+          for (int i = 0; i < supers->length(); ++i) {\n+            Method* super_method = supers->at(i);\n+            if (super_method->is_scalarized_arg(arg_num)) {\n+              scalar_super = true;\n+            } else {\n+              non_scalar_super = true;\n+            }\n+          }\n+#ifdef ASSERT\n+          \/\/ Randomly enable below code paths for stress testing\n+          bool stress = init && StressCallingConvention;\n+          if (stress && (os::random() & 1) == 1) {\n+            non_scalar_super = true;\n+            if ((os::random() & 1) == 1) {\n+              scalar_super = true;\n+            }\n+          }\n@@ -2709,0 +2947,50 @@\n+          if (non_scalar_super) {\n+            \/\/ Found a super method with a non-scalarized argument. Fall back to the non-scalarized calling convention.\n+            if (scalar_super) {\n+              \/\/ Found non-scalar *and* scalar super methods. We can't handle both.\n+              \/\/ Mark the scalar method as mismatch and re-compile call sites to use non-scalarized calling convention.\n+              for (int i = 0; i < supers->length(); ++i) {\n+                Method* super_method = supers->at(i);\n+                if (super_method->is_scalarized_arg(arg_num) DEBUG_ONLY(|| (stress && (os::random() & 1) == 1))) {\n+                  super_method->set_mismatch();\n+                  MutexLocker ml(Compile_lock, Mutex::_safepoint_check_flag);\n+                  JavaThread* thread = JavaThread::current();\n+                  HandleMark hm(thread);\n+                  methodHandle mh(thread, super_method);\n+                  DeoptimizationScope deopt_scope;\n+                  CodeCache::mark_for_deoptimization(&deopt_scope, mh());\n+                  deopt_scope.deoptimize_marked();\n+                }\n+              }\n+            }\n+            \/\/ Fall back to non-scalarized calling convention\n+            SigEntry::add_entry(_sig_cc, T_OBJECT, ss.as_symbol());\n+            SigEntry::add_entry(_sig_cc_ro, T_OBJECT, ss.as_symbol());\n+          } else {\n+            _num_inline_args++;\n+            has_scalarized = true;\n+            int last = _sig_cc->length();\n+            int last_ro = _sig_cc_ro->length();\n+            _sig_cc->appendAll(vk->extended_sig());\n+            _sig_cc_ro->appendAll(vk->extended_sig());\n+            if (bt == T_OBJECT) {\n+              \/\/ Nullable inline type argument, insert InlineTypeNode::NullMarker field right after T_METADATA delimiter\n+              _sig_cc->insert_before(last+1, SigEntry(T_BOOLEAN, -1, nullptr, true));\n+              _sig_cc_ro->insert_before(last_ro+1, SigEntry(T_BOOLEAN, -1, nullptr, true));\n+            }\n+          }\n+        } else {\n+          SigEntry::add_entry(_sig_cc, T_OBJECT, ss.as_symbol());\n+          SigEntry::add_entry(_sig_cc_ro, T_OBJECT, ss.as_symbol());\n+        }\n+        bt = T_OBJECT;\n+      } else {\n+        SigEntry::add_entry(_sig_cc, ss.type(), ss.as_symbol());\n+        SigEntry::add_entry(_sig_cc_ro, ss.type(), ss.as_symbol());\n+      }\n+      SigEntry::add_entry(_sig, bt, ss.as_symbol());\n+      if (bt != T_VOID) {\n+        arg_num++;\n+      }\n+    }\n+  }\n@@ -2710,1 +2998,3 @@\n- private:\n+  \/\/ Compute the non-scalarized calling convention\n+  _regs = NEW_RESOURCE_ARRAY(VMRegPair, _sig->length());\n+  _args_on_stack = SharedRuntime::java_calling_convention(_sig, _regs);\n@@ -2712,5 +3002,16 @@\n-  friend class SignatureIterator;  \/\/ so do_parameters_on can call do_type\n-  void do_type(BasicType type) {\n-    sig_bt[index++] = type;\n-    if (type == T_LONG || type == T_DOUBLE) {\n-      sig_bt[index++] = T_VOID; \/\/ Longs & doubles take 2 Java slots\n+  \/\/ Compute the scalarized calling conventions if there are scalarized inline types in the signature\n+  if (has_scalarized && !_method->is_native()) {\n+    _regs_cc = NEW_RESOURCE_ARRAY(VMRegPair, _sig_cc->length());\n+    _args_on_stack_cc = SharedRuntime::java_calling_convention(_sig_cc, _regs_cc);\n+\n+    _regs_cc_ro = NEW_RESOURCE_ARRAY(VMRegPair, _sig_cc_ro->length());\n+    _args_on_stack_cc_ro = SharedRuntime::java_calling_convention(_sig_cc_ro, _regs_cc_ro);\n+\n+    _c1_needs_stack_repair = (_args_on_stack_cc < _args_on_stack) || (_args_on_stack_cc_ro < _args_on_stack);\n+    _c2_needs_stack_repair = (_args_on_stack_cc > _args_on_stack) || (_args_on_stack_cc > _args_on_stack_cc_ro);\n+\n+    \/\/ Upper bound on stack arguments to avoid hitting the argument limit and\n+    \/\/ bailing out of compilation (\"unsupported incoming calling sequence\").\n+    \/\/ TODO we need a reasonable limit (flag?) here\n+    if (MAX2(_args_on_stack_cc, _args_on_stack_cc_ro) <= 60) {\n+      return; \/\/ Success\n@@ -2719,1 +3020,130 @@\n-};\n+  \/\/ No scalarized args\n+  _sig_cc = _sig;\n+  _regs_cc = _regs;\n+  _args_on_stack_cc = _args_on_stack;\n+\n+  _sig_cc_ro = _sig;\n+  _regs_cc_ro = _regs;\n+  _args_on_stack_cc_ro = _args_on_stack;\n+}\n+\n+void CompiledEntrySignature::initialize_from_fingerprint(AdapterFingerPrint* fingerprint) {\n+  _has_inline_recv = fingerprint->has_ro_adapter();\n+\n+  int value_object_count = 0;\n+  BasicType prev_bt = T_ILLEGAL;\n+  bool has_scalarized_arguments = false;\n+  bool long_prev = false;\n+  int long_prev_offset = -1;\n+\n+  fingerprint->iterate_args([&] (const AdapterFingerPrint::Element& arg) {\n+    BasicType bt = arg.bt();\n+    int offset = arg.offset();\n+\n+    if (long_prev) {\n+      long_prev = false;\n+      BasicType bt_to_add;\n+      if (bt == T_VOID) {\n+        bt_to_add = T_LONG;\n+      } else {\n+        bt_to_add = T_OBJECT;\n+      }\n+      if (value_object_count == 0) {\n+        SigEntry::add_entry(_sig, bt_to_add);\n+      }\n+      SigEntry::add_entry(_sig_cc, bt_to_add, nullptr, long_prev_offset);\n+      SigEntry::add_entry(_sig_cc_ro, bt_to_add, nullptr, long_prev_offset);\n+    }\n+\n+    switch (bt) {\n+      case T_VOID:\n+        if (prev_bt != T_LONG && prev_bt != T_DOUBLE) {\n+          assert(InlineTypePassFieldsAsArgs, \"unexpected end of inline type\");\n+          value_object_count--;\n+          SigEntry::add_entry(_sig_cc, T_VOID, nullptr, offset);\n+          SigEntry::add_entry(_sig_cc_ro, T_VOID, nullptr, offset);\n+          assert(value_object_count >= 0, \"invalid value object count\");\n+        } else {\n+          \/\/ Nothing to add for _sig: We already added an addition T_VOID in add_entry() when adding T_LONG or T_DOUBLE.\n+        }\n+        break;\n+      case T_INT:\n+      case T_FLOAT:\n+      case T_DOUBLE:\n+        if (value_object_count == 0) {\n+          SigEntry::add_entry(_sig, bt);\n+        }\n+        SigEntry::add_entry(_sig_cc, bt, nullptr, offset);\n+        SigEntry::add_entry(_sig_cc_ro, bt, nullptr, offset);\n+        break;\n+      case T_LONG:\n+        long_prev = true;\n+        long_prev_offset = offset;\n+        break;\n+      case T_BOOLEAN:\n+      case T_CHAR:\n+      case T_BYTE:\n+      case T_SHORT:\n+      case T_OBJECT:\n+      case T_ARRAY:\n+        assert(value_object_count > 0, \"must be value object field\");\n+        SigEntry::add_entry(_sig_cc, bt, nullptr, offset);\n+        SigEntry::add_entry(_sig_cc_ro, bt, nullptr, offset);\n+        break;\n+      case T_METADATA:\n+        assert(InlineTypePassFieldsAsArgs, \"unexpected start of inline type\");\n+        if (value_object_count == 0) {\n+          SigEntry::add_entry(_sig, T_OBJECT);\n+        }\n+        SigEntry::add_entry(_sig_cc, T_METADATA, nullptr, offset);\n+        SigEntry::add_entry(_sig_cc_ro, T_METADATA, nullptr, offset);\n+        value_object_count++;\n+        has_scalarized_arguments = true;\n+        break;\n+      default: {\n+        fatal(\"Unexpected BasicType: %s\", basictype_to_str(bt));\n+      }\n+    }\n+    prev_bt = bt;\n+  });\n+\n+  if (long_prev) {\n+    \/\/ If previous bt was T_LONG and we reached the end of the signature, we know that it must be a T_OBJECT.\n+    SigEntry::add_entry(_sig, T_OBJECT);\n+    SigEntry::add_entry(_sig_cc, T_OBJECT);\n+    SigEntry::add_entry(_sig_cc_ro, T_OBJECT);\n+  }\n+  assert(value_object_count == 0, \"invalid value object count\");\n+\n+  _regs = NEW_RESOURCE_ARRAY(VMRegPair, _sig->length());\n+  _args_on_stack = SharedRuntime::java_calling_convention(_sig, _regs);\n+\n+  \/\/ Compute the scalarized calling conventions if there are scalarized inline types in the signature\n+  if (has_scalarized_arguments) {\n+    _regs_cc = NEW_RESOURCE_ARRAY(VMRegPair, _sig_cc->length());\n+    _args_on_stack_cc = SharedRuntime::java_calling_convention(_sig_cc, _regs_cc);\n+\n+    _regs_cc_ro = NEW_RESOURCE_ARRAY(VMRegPair, _sig_cc_ro->length());\n+    _args_on_stack_cc_ro = SharedRuntime::java_calling_convention(_sig_cc_ro, _regs_cc_ro);\n+\n+    _c1_needs_stack_repair = (_args_on_stack_cc < _args_on_stack) || (_args_on_stack_cc_ro < _args_on_stack);\n+    _c2_needs_stack_repair = (_args_on_stack_cc > _args_on_stack) || (_args_on_stack_cc > _args_on_stack_cc_ro);\n+  } else {\n+    \/\/ No scalarized args\n+    _sig_cc = _sig;\n+    _regs_cc = _regs;\n+    _args_on_stack_cc = _args_on_stack;\n+\n+    _sig_cc_ro = _sig;\n+    _regs_cc_ro = _regs;\n+    _args_on_stack_cc_ro = _args_on_stack;\n+  }\n+\n+#ifdef ASSERT\n+  {\n+    AdapterFingerPrint* compare_fp = AdapterFingerPrint::allocate(_sig_cc, _has_inline_recv);\n+    assert(fingerprint->equals(compare_fp), \"%s - %s\", fingerprint->as_string(), compare_fp->as_string());\n+    AdapterFingerPrint::deallocate(compare_fp);\n+  }\n+#endif\n+}\n@@ -2727,1 +3157,1 @@\n-void AdapterHandlerLibrary::verify_adapter_sharing(int total_args_passed, BasicType* sig_bt, AdapterHandlerEntry* cached_entry) {\n+void AdapterHandlerLibrary::verify_adapter_sharing(CompiledEntrySignature& ces, AdapterHandlerEntry* cached_entry) {\n@@ -2730,1 +3160,1 @@\n-  AdapterHandlerEntry* comparison_entry = create_adapter(total_args_passed, sig_bt, true);\n+  AdapterHandlerEntry* comparison_entry = create_adapter(ces, false, true);\n@@ -2755,2 +3185,13 @@\n-  \/\/ Fill in the signature array, for the calling-convention call.\n-  int total_args_passed = method->size_of_parameters(); \/\/ All args on stack\n+  CompiledEntrySignature ces(method());\n+  ces.compute_calling_conventions();\n+  if (ces.has_scalarized_args()) {\n+    if (!method->has_scalarized_args()) {\n+      method->set_has_scalarized_args();\n+    }\n+    if (ces.c1_needs_stack_repair()) {\n+      method->set_c1_needs_stack_repair();\n+    }\n+    if (ces.c2_needs_stack_repair() && !method->c2_needs_stack_repair()) {\n+      method->set_c2_needs_stack_repair();\n+    }\n+  }\n@@ -2758,4 +3199,0 @@\n-  AdapterSignatureIterator si(method->signature(), method->constMethod()->fingerprint(),\n-                              method->is_static(), total_args_passed);\n-  assert(si.slots() == total_args_passed, \"\");\n-  BasicType* sig_bt = si.basic_types();\n@@ -2766,1 +3203,1 @@\n-    entry = lookup(total_args_passed, sig_bt);\n+    entry = lookup(ces.sig_cc(), ces.has_inline_recv());\n@@ -2774,1 +3211,1 @@\n-        verify_adapter_sharing(total_args_passed, sig_bt, entry);\n+        verify_adapter_sharing(ces, entry);\n@@ -2778,1 +3215,1 @@\n-      entry = create_adapter(total_args_passed, sig_bt);\n+      entry = create_adapter(ces, \/* allocate_code_blob *\/ true);\n@@ -2833,0 +3270,2 @@\n+  entry_offset[AdapterBlob::C2I_Inline] = entry_address[AdapterBlob::C2I_Inline] - entry_address[AdapterBlob::I2C];\n+  entry_offset[AdapterBlob::C2I_Inline_RO] = entry_address[AdapterBlob::C2I_Inline_RO] - entry_address[AdapterBlob::I2C];\n@@ -2834,0 +3273,1 @@\n+  entry_offset[AdapterBlob::C2I_Unverified_Inline] = entry_address[AdapterBlob::C2I_Unverified_Inline] - entry_address[AdapterBlob::I2C];\n@@ -2842,2 +3282,2 @@\n-                                                  int total_args_passed,\n-                                                  BasicType* sig_bt,\n+                                                  CompiledEntrySignature& ces,\n+                                                  bool allocate_code_blob,\n@@ -2850,0 +3290,1 @@\n+  AdapterBlob* adapter_blob = nullptr;\n@@ -2856,2 +3297,1 @@\n-  VMRegPair stack_regs[16];\n-  VMRegPair* regs = (total_args_passed <= 16) ? stack_regs : NEW_RESOURCE_ARRAY(VMRegPair, total_args_passed);\n+  address entry_address[AdapterBlob::ENTRY_COUNT];\n@@ -2860,7 +3300,17 @@\n-  int comp_args_on_stack = SharedRuntime::java_calling_convention(sig_bt, regs, total_args_passed);\n-  address entry_address[AdapterBlob::ENTRY_COUNT];\n-                                         total_args_passed,\n-                                         comp_args_on_stack,\n-                                         sig_bt,\n-                                         regs,\n-                                         entry_address);\n+                                         ces.args_on_stack(),\n+                                         ces.sig(),\n+                                         ces.regs(),\n+                                         ces.sig_cc(),\n+                                         ces.regs_cc(),\n+                                         ces.sig_cc_ro(),\n+                                         ces.regs_cc_ro(),\n+                                         entry_address,\n+                                         adapter_blob,\n+                                         allocate_code_blob);\n+\n+  if (ces.has_scalarized_args()) {\n+    \/\/ Save a C heap allocated version of the scalarized signature and store it in the adapter\n+    GrowableArray<SigEntry>* heap_sig = new (mtInternal) GrowableArray<SigEntry>(ces.sig_cc()->length(), mtInternal);\n+    heap_sig->appendAll(ces.sig_cc());\n+    handler->set_sig_cc(heap_sig);\n+  }\n@@ -2880,1 +3330,0 @@\n-  AdapterBlob* adapter_blob = AdapterBlob::create(&buffer, entry_offset);\n@@ -2907,2 +3356,2 @@\n-AdapterHandlerEntry* AdapterHandlerLibrary::create_adapter(int total_args_passed,\n-                                                           BasicType* sig_bt,\n+AdapterHandlerEntry* AdapterHandlerLibrary::create_adapter(CompiledEntrySignature& ces,\n+                                                           bool allocate_code_blob,\n@@ -2910,1 +3359,6 @@\n-  AdapterFingerPrint* fp = AdapterFingerPrint::allocate(total_args_passed, sig_bt);\n+  AdapterFingerPrint* fp = AdapterFingerPrint::allocate(ces.sig_cc(), ces.has_inline_recv());\n+#ifdef ASSERT\n+  \/\/ Verify that we can successfully restore the compiled entry signature object.\n+  CompiledEntrySignature ces_verify;\n+  ces_verify.initialize_from_fingerprint(fp);\n+#endif\n@@ -2912,1 +3366,1 @@\n-  if (!generate_adapter_code(handler, total_args_passed, sig_bt, is_transient)) {\n+  if (!generate_adapter_code(handler, ces, allocate_code_blob, is_transient)) {\n@@ -3015,3 +3469,3 @@\n-    int nargs;\n-    BasicType* bt = _fingerprint->as_basic_type(nargs);\n-    if (!AdapterHandlerLibrary::generate_adapter_code(this, nargs, bt, \/* is_transient *\/ false)) {\n+    CompiledEntrySignature ces;\n+    ces.initialize_from_fingerprint(_fingerprint);\n+    if (!AdapterHandlerLibrary::generate_adapter_code(this, ces, true, false)) {\n@@ -3055,13 +3509,26 @@\n-  _no_arg_handler = lookup(0, nullptr);\n-\n-  BasicType obj_args[] = { T_OBJECT };\n-  _obj_arg_handler = lookup(1, obj_args);\n-\n-  BasicType int_args[] = { T_INT };\n-  _int_arg_handler = lookup(1, int_args);\n-\n-  BasicType obj_int_args[] = { T_OBJECT, T_INT };\n-  _obj_int_arg_handler = lookup(2, obj_int_args);\n-\n-  BasicType obj_obj_args[] = { T_OBJECT, T_OBJECT };\n-  _obj_obj_arg_handler = lookup(2, obj_obj_args);\n+  ResourceMark rm;\n+  CompiledEntrySignature no_args;\n+  no_args.compute_calling_conventions();\n+  _no_arg_handler = lookup(no_args.sig_cc(), no_args.has_inline_recv());\n+\n+  CompiledEntrySignature obj_args;\n+  SigEntry::add_entry(obj_args.sig(), T_OBJECT);\n+  obj_args.compute_calling_conventions();\n+  _obj_arg_handler = lookup(obj_args.sig_cc(), obj_args.has_inline_recv());\n+\n+  CompiledEntrySignature int_args;\n+  SigEntry::add_entry(int_args.sig(), T_INT);\n+  int_args.compute_calling_conventions();\n+  _int_arg_handler = lookup(int_args.sig_cc(), int_args.has_inline_recv());\n+\n+  CompiledEntrySignature obj_int_args;\n+  SigEntry::add_entry(obj_int_args.sig(), T_OBJECT);\n+  SigEntry::add_entry(obj_int_args.sig(), T_INT);\n+  obj_int_args.compute_calling_conventions();\n+  _obj_int_arg_handler = lookup(obj_int_args.sig_cc(), obj_int_args.has_inline_recv());\n+\n+  CompiledEntrySignature obj_obj_args;\n+  SigEntry::add_entry(obj_obj_args.sig(), T_OBJECT);\n+  SigEntry::add_entry(obj_obj_args.sig(), T_OBJECT);\n+  obj_obj_args.compute_calling_conventions();\n+  _obj_obj_arg_handler = lookup(obj_obj_args.sig_cc(), obj_obj_args.has_inline_recv());\n@@ -3096,0 +3563,3 @@\n+  if (_sig_cc != nullptr) {\n+    delete _sig_cc;\n+  }\n@@ -3183,0 +3653,1 @@\n+      BasicType stack_sig_bt[16];\n@@ -3184,0 +3655,1 @@\n+      BasicType* sig_bt = (total_args_passed <= 16) ? stack_sig_bt : NEW_RESOURCE_ARRAY(BasicType, total_args_passed);\n@@ -3186,5 +3658,13 @@\n-      AdapterSignatureIterator si(method->signature(), method->constMethod()->fingerprint(),\n-                              method->is_static(), total_args_passed);\n-      BasicType* sig_bt = si.basic_types();\n-      assert(si.slots() == total_args_passed, \"\");\n-      BasicType ret_type = si.return_type();\n+      int i = 0;\n+      if (!method->is_static()) {  \/\/ Pass in receiver first\n+        sig_bt[i++] = T_OBJECT;\n+      }\n+      SignatureStream ss(method->signature());\n+      for (; !ss.at_return_type(); ss.next()) {\n+        sig_bt[i++] = ss.type();  \/\/ Collect remaining bits of signature\n+        if (ss.type() == T_LONG || ss.type() == T_DOUBLE) {\n+          sig_bt[i++] = T_VOID;   \/\/ Longs & doubles take 2 Java slots\n+        }\n+      }\n+      assert(i == total_args_passed, \"\");\n+      BasicType ret_type = ss.type();\n@@ -3438,1 +3918,4 @@\n-    st->print(\" c2iUV: \" INTPTR_FORMAT, p2i(get_c2i_unverified_entry()));\n+    st->print(\" c2iVE: \" INTPTR_FORMAT, p2i(get_c2i_inline_entry()));\n+    st->print(\" c2iVROE: \" INTPTR_FORMAT, p2i(get_c2i_inline_ro_entry()));\n+    st->print(\" c2iUE: \" INTPTR_FORMAT, p2i(get_c2i_unverified_entry()));\n+    st->print(\" c2iUVE: \" INTPTR_FORMAT, p2i(get_c2i_unverified_inline_entry()));\n@@ -3525,0 +4008,195 @@\n+\n+\/\/ We are at a compiled code to interpreter call. We need backing\n+\/\/ buffers for all inline type arguments. Allocate an object array to\n+\/\/ hold them (convenient because once we're done with it we don't have\n+\/\/ to worry about freeing it).\n+oop SharedRuntime::allocate_inline_types_impl(JavaThread* current, methodHandle callee, bool allocate_receiver, TRAPS) {\n+  assert(InlineTypePassFieldsAsArgs, \"no reason to call this\");\n+  ResourceMark rm;\n+\n+  int nb_slots = 0;\n+  InstanceKlass* holder = callee->method_holder();\n+  allocate_receiver &= !callee->is_static() && holder->is_inline_klass() && callee->is_scalarized_arg(0);\n+  if (allocate_receiver) {\n+    nb_slots++;\n+  }\n+  int arg_num = callee->is_static() ? 0 : 1;\n+  for (SignatureStream ss(callee->signature()); !ss.at_return_type(); ss.next()) {\n+    BasicType bt = ss.type();\n+    if (bt == T_OBJECT && callee->is_scalarized_arg(arg_num)) {\n+      nb_slots++;\n+    }\n+    if (bt != T_VOID) {\n+      arg_num++;\n+    }\n+  }\n+  objArrayOop array_oop = oopFactory::new_objectArray(nb_slots, CHECK_NULL);\n+  objArrayHandle array(THREAD, array_oop);\n+  arg_num = callee->is_static() ? 0 : 1;\n+  int i = 0;\n+  if (allocate_receiver) {\n+    InlineKlass* vk = InlineKlass::cast(holder);\n+    oop res = vk->allocate_instance(CHECK_NULL);\n+    array->obj_at_put(i++, res);\n+  }\n+  for (SignatureStream ss(callee->signature()); !ss.at_return_type(); ss.next()) {\n+    BasicType bt = ss.type();\n+    if (bt == T_OBJECT && callee->is_scalarized_arg(arg_num)) {\n+      InlineKlass* vk = ss.as_inline_klass(holder);\n+      assert(vk != nullptr, \"Unexpected klass\");\n+      oop res = vk->allocate_instance(CHECK_NULL);\n+      array->obj_at_put(i++, res);\n+    }\n+    if (bt != T_VOID) {\n+      arg_num++;\n+    }\n+  }\n+  return array();\n+}\n+\n+JRT_ENTRY(void, SharedRuntime::allocate_inline_types(JavaThread* current, Method* callee_method, bool allocate_receiver))\n+  methodHandle callee(current, callee_method);\n+  oop array = SharedRuntime::allocate_inline_types_impl(current, callee, allocate_receiver, CHECK);\n+  current->set_vm_result_oop(array);\n+  current->set_vm_result_metadata(callee()); \/\/ TODO: required to keep callee live?\n+JRT_END\n+\n+\/\/ We're returning from an interpreted method: load each field into a\n+\/\/ register following the calling convention\n+JRT_LEAF(void, SharedRuntime::load_inline_type_fields_in_regs(JavaThread* current, oopDesc* res))\n+{\n+  assert(res->klass()->is_inline_klass(), \"only inline types here\");\n+  ResourceMark rm;\n+  RegisterMap reg_map(current,\n+                      RegisterMap::UpdateMap::include,\n+                      RegisterMap::ProcessFrames::include,\n+                      RegisterMap::WalkContinuation::skip);\n+  frame stubFrame = current->last_frame();\n+  frame callerFrame = stubFrame.sender(&reg_map);\n+  assert(callerFrame.is_interpreted_frame(), \"should be coming from interpreter\");\n+\n+  InlineKlass* vk = InlineKlass::cast(res->klass());\n+\n+  const Array<SigEntry>* sig_vk = vk->extended_sig();\n+  const Array<VMRegPair>* regs = vk->return_regs();\n+\n+  if (regs == nullptr) {\n+    \/\/ The fields of the inline klass don't fit in registers, bail out\n+    return;\n+  }\n+\n+  int j = 1;\n+  for (int i = 0; i < sig_vk->length(); i++) {\n+    BasicType bt = sig_vk->at(i)._bt;\n+    if (bt == T_METADATA) {\n+      continue;\n+    }\n+    if (bt == T_VOID) {\n+      if (sig_vk->at(i-1)._bt == T_LONG ||\n+          sig_vk->at(i-1)._bt == T_DOUBLE) {\n+        j++;\n+      }\n+      continue;\n+    }\n+    int off = sig_vk->at(i)._offset;\n+    assert(off > 0, \"offset in object should be positive\");\n+    VMRegPair pair = regs->at(j);\n+    address loc = reg_map.location(pair.first(), nullptr);\n+    switch(bt) {\n+    case T_BOOLEAN:\n+      *(jboolean*)loc = res->bool_field(off);\n+      break;\n+    case T_CHAR:\n+      *(jchar*)loc = res->char_field(off);\n+      break;\n+    case T_BYTE:\n+      *(jbyte*)loc = res->byte_field(off);\n+      break;\n+    case T_SHORT:\n+      *(jshort*)loc = res->short_field(off);\n+      break;\n+    case T_INT: {\n+      *(jint*)loc = res->int_field(off);\n+      break;\n+    }\n+    case T_LONG:\n+#ifdef _LP64\n+      *(intptr_t*)loc = res->long_field(off);\n+#else\n+      Unimplemented();\n+#endif\n+      break;\n+    case T_OBJECT:\n+    case T_ARRAY: {\n+      *(oop*)loc = res->obj_field(off);\n+      break;\n+    }\n+    case T_FLOAT:\n+      *(jfloat*)loc = res->float_field(off);\n+      break;\n+    case T_DOUBLE:\n+      *(jdouble*)loc = res->double_field(off);\n+      break;\n+    default:\n+      ShouldNotReachHere();\n+    }\n+    j++;\n+  }\n+  assert(j == regs->length(), \"missed a field?\");\n+\n+#ifdef ASSERT\n+  VMRegPair pair = regs->at(0);\n+  address loc = reg_map.location(pair.first(), nullptr);\n+  assert(*(oopDesc**)loc == res, \"overwritten object\");\n+#endif\n+\n+  current->set_vm_result_oop(res);\n+}\n+JRT_END\n+\n+\/\/ We've returned to an interpreted method, the interpreter needs a\n+\/\/ reference to an inline type instance. Allocate it and initialize it\n+\/\/ from field's values in registers.\n+JRT_BLOCK_ENTRY(void, SharedRuntime::store_inline_type_fields_to_buf(JavaThread* current, intptr_t res))\n+{\n+  ResourceMark rm;\n+  RegisterMap reg_map(current,\n+                      RegisterMap::UpdateMap::include,\n+                      RegisterMap::ProcessFrames::include,\n+                      RegisterMap::WalkContinuation::skip);\n+  frame stubFrame = current->last_frame();\n+  frame callerFrame = stubFrame.sender(&reg_map);\n+\n+#ifdef ASSERT\n+  InlineKlass* verif_vk = InlineKlass::returned_inline_klass(reg_map);\n+#endif\n+\n+  if (!is_set_nth_bit(res, 0)) {\n+    \/\/ We're not returning with inline type fields in registers (the\n+    \/\/ calling convention didn't allow it for this inline klass)\n+    assert(!Metaspace::contains((void*)res), \"should be oop or pointer in buffer area\");\n+    current->set_vm_result_oop((oopDesc*)res);\n+    assert(verif_vk == nullptr, \"broken calling convention\");\n+    return;\n+  }\n+\n+  clear_nth_bit(res, 0);\n+  InlineKlass* vk = (InlineKlass*)res;\n+  assert(verif_vk == vk, \"broken calling convention\");\n+  assert(Metaspace::contains((void*)res), \"should be klass\");\n+\n+  \/\/ Allocate handles for every oop field so they are safe in case of\n+  \/\/ a safepoint when allocating\n+  GrowableArray<Handle> handles;\n+  vk->save_oop_fields(reg_map, handles);\n+\n+  \/\/ It's unsafe to safepoint until we are here\n+  JRT_BLOCK;\n+  {\n+    JavaThread* THREAD = current;\n+    oop vt = vk->realloc_result(reg_map, handles, CHECK);\n+    current->set_vm_result_oop(vt);\n+  }\n+  JRT_BLOCK_END;\n+}\n+JRT_END\n","filename":"src\/hotspot\/share\/runtime\/sharedRuntime.cpp","additions":935,"deletions":257,"binary":false,"changes":1192,"status":"modified"},{"patch":"@@ -28,0 +28,1 @@\n+#include \"asm\/codeBuffer.hpp\"\n@@ -36,0 +37,1 @@\n+#include \"runtime\/signature.hpp\"\n@@ -42,0 +44,1 @@\n+class SigEntry;\n@@ -379,0 +382,2 @@\n+  static char* generate_identity_exception_message(JavaThread* thr, Klass* klass);\n+\n@@ -381,1 +386,1 @@\n-  static methodHandle resolve_helper(bool is_virtual, bool is_optimized, TRAPS);\n+  static methodHandle resolve_helper(bool is_virtual, bool is_optimized, bool& caller_does_not_scalarize, TRAPS);\n@@ -391,1 +396,1 @@\n-  static methodHandle reresolve_call_site(TRAPS);\n+  static methodHandle reresolve_call_site(bool& is_optimized, bool& caller_does_not_scalarize, TRAPS);\n@@ -395,1 +400,1 @@\n-  static methodHandle handle_ic_miss_helper(TRAPS);\n+  static methodHandle handle_ic_miss_helper(bool& caller_does_not_scalarize, TRAPS);\n@@ -398,1 +403,1 @@\n-  static methodHandle find_callee_method(TRAPS);\n+  static methodHandle find_callee_method(bool& caller_does_not_scalarize, TRAPS);\n@@ -426,0 +431,8 @@\n+  static int java_calling_convention(const GrowableArray<SigEntry>* sig, VMRegPair* regs) {\n+    BasicType* sig_bt = NEW_RESOURCE_ARRAY(BasicType, sig->length());\n+    int total_args_passed = SigEntry::fill_sig_bt(sig, sig_bt);\n+    return java_calling_convention(sig_bt, regs, total_args_passed);\n+  }\n+  static int java_return_convention(const BasicType* sig_bt, VMRegPair* regs, int total_args_passed);\n+  static const uint java_return_convention_max_int;\n+  static const uint java_return_convention_max_float;\n@@ -472,1 +485,1 @@\n-  static void generate_i2c2i_adapters(MacroAssembler *_masm,\n+  static void generate_i2c2i_adapters(MacroAssembler* _masm,\n@@ -474,4 +487,9 @@\n-                                      int max_arg,\n-                                      const BasicType *sig_bt,\n-                                      const VMRegPair *regs,\n-                                      address entry_address[AdapterBlob::ENTRY_COUNT]);\n+                                      const GrowableArray<SigEntry>* sig,\n+                                      const VMRegPair* regs,\n+                                      const GrowableArray<SigEntry>* sig_cc,\n+                                      const VMRegPair* regs_cc,\n+                                      const GrowableArray<SigEntry>* sig_cc_ro,\n+                                      const VMRegPair* regs_cc_ro,\n+                                      address entry_address[AdapterBlob::ENTRY_COUNT],\n+                                      AdapterBlob*& new_adapter,\n+                                      bool allocate_code_blob);\n@@ -480,2 +498,1 @@\n-                              int total_args_passed,\n-                              const BasicType *sig_bt,\n+                              const GrowableArray<SigEntry>* sig,\n@@ -554,1 +571,2 @@\n-  static address get_resolved_entry        (JavaThread* current, methodHandle callee_method);\n+  static address get_resolved_entry        (JavaThread* current, methodHandle callee_method,\n+                                            bool is_static_call, bool is_optimized, bool caller_does_not_scalarize);\n@@ -559,0 +577,3 @@\n+  static void load_inline_type_fields_in_regs(JavaThread* current, oopDesc* res);\n+  static void store_inline_type_fields_to_buf(JavaThread* current, intptr_t res);\n+\n@@ -569,0 +590,2 @@\n+  static void allocate_inline_types(JavaThread* current, Method* callee, bool allocate_receiver);\n+  static oop allocate_inline_types_impl(JavaThread* current, methodHandle callee, bool allocate_receiver, TRAPS);\n@@ -572,0 +595,1 @@\n+  static BufferedInlineTypeBlob* generate_buffered_inline_type_adapter(const InlineKlass* vk);\n@@ -710,1 +734,1 @@\n-  static const int ENTRIES_COUNT = 4;\n+  static const int ENTRIES_COUNT = 7;\n@@ -720,0 +744,3 @@\n+  \/\/ Support for scalarized inline type calling convention\n+  const GrowableArray<SigEntry>* _sig_cc;\n+\n@@ -731,1 +758,2 @@\n-    _linked(false)\n+    _linked(false),\n+    _sig_cc(nullptr)\n@@ -782,0 +810,18 @@\n+  address get_c2i_inline_entry() const {\n+#ifndef ZERO\n+    assert(_adapter_blob != nullptr, \"must be\");\n+    return _adapter_blob->c2i_inline_entry();\n+#else\n+    return nullptr;\n+#endif \/\/ ZERO\n+  }\n+\n+  address get_c2i_inline_ro_entry() const {\n+#ifndef ZERO\n+    assert(_adapter_blob != nullptr, \"must be\");\n+    return _adapter_blob->c2i_inline_ro_entry();\n+#else\n+    return nullptr;\n+#endif \/\/ ZERO\n+  }\n+\n@@ -791,0 +837,9 @@\n+  address get_c2i_unverified_inline_entry() const {\n+#ifndef ZERO\n+    assert(_adapter_blob != nullptr, \"must be\");\n+    return _adapter_blob->c2i_unverified_inline_entry();\n+#else\n+    return nullptr;\n+#endif \/\/ ZERO\n+  }\n+\n@@ -803,0 +858,4 @@\n+  \/\/ Support for scalarized inline type calling convention\n+  void set_sig_cc(const GrowableArray<SigEntry>* sig)  { _sig_cc = sig; }\n+  const GrowableArray<SigEntry>* get_sig_cc()    const { return _sig_cc; }\n+\n@@ -827,0 +886,2 @@\n+class CompiledEntrySignature;\n+\n@@ -845,2 +906,2 @@\n-  static AdapterHandlerEntry* create_adapter(int total_args_passed,\n-                                             BasicType* sig_bt,\n+  static AdapterHandlerEntry* create_adapter(CompiledEntrySignature& ces,\n+                                             bool allocate_code_blob,\n@@ -857,1 +918,1 @@\n-  static AdapterHandlerEntry* lookup(int total_args_passed, BasicType* sig_bt);\n+  static AdapterHandlerEntry* lookup(const GrowableArray<SigEntry>* sig, bool has_ro_adapter = false);\n@@ -859,2 +920,2 @@\n-                                    int total_args_passed,\n-                                    BasicType* sig_bt,\n+                                    CompiledEntrySignature& ces,\n+                                    bool allocate_code_blob,\n@@ -864,1 +925,1 @@\n-  static void verify_adapter_sharing(int total_args_passed, BasicType* sig_bt, AdapterHandlerEntry* cached);\n+  static void verify_adapter_sharing(CompiledEntrySignature& ces, AdapterHandlerEntry* cached_entry);\n@@ -882,0 +943,60 @@\n+\/\/ Utility class for computing the calling convention of the 3 types\n+\/\/ of compiled method entries:\n+\/\/     Method::_from_compiled_entry               - sig_cc\n+\/\/     Method::_from_compiled_inline_ro_entry     - sig_cc_ro\n+\/\/     Method::_from_compiled_inline_entry        - sig\n+class CompiledEntrySignature : public StackObj {\n+  Method* _method;\n+  int  _num_inline_args;\n+  bool _has_inline_recv;\n+  GrowableArray<SigEntry>* _sig;\n+  GrowableArray<SigEntry>* _sig_cc;\n+  GrowableArray<SigEntry>* _sig_cc_ro;\n+  VMRegPair* _regs;\n+  VMRegPair* _regs_cc;\n+  VMRegPair* _regs_cc_ro;\n+\n+  int _args_on_stack;\n+  int _args_on_stack_cc;\n+  int _args_on_stack_cc_ro;\n+\n+  bool _c1_needs_stack_repair;\n+  bool _c2_needs_stack_repair;\n+\n+  GrowableArray<Method*>* _supers;\n+\n+public:\n+  Method* method()                     const { return _method; }\n+\n+  \/\/ Used by Method::_from_compiled_inline_entry\n+  GrowableArray<SigEntry>* sig()       const { return _sig; }\n+\n+  \/\/ Used by Method::_from_compiled_entry\n+  GrowableArray<SigEntry>* sig_cc()    const { return _sig_cc; }\n+\n+  \/\/ Used by Method::_from_compiled_inline_ro_entry\n+  GrowableArray<SigEntry>* sig_cc_ro() const { return _sig_cc_ro; }\n+\n+  VMRegPair* regs()                    const { return _regs; }\n+  VMRegPair* regs_cc()                 const { return _regs_cc; }\n+  VMRegPair* regs_cc_ro()              const { return _regs_cc_ro; }\n+\n+  int args_on_stack()                  const { return _args_on_stack; }\n+  int args_on_stack_cc()               const { return _args_on_stack_cc; }\n+  int args_on_stack_cc_ro()            const { return _args_on_stack_cc_ro; }\n+\n+  int  num_inline_args()               const { return _num_inline_args; }\n+  bool has_inline_recv()               const { return _has_inline_recv; }\n+\n+  bool has_scalarized_args()           const { return _sig != _sig_cc; }\n+  bool c1_needs_stack_repair()         const { return _c1_needs_stack_repair; }\n+  bool c2_needs_stack_repair()         const { return _c2_needs_stack_repair; }\n+  CodeOffsets::Entries c1_inline_ro_entry_type() const;\n+\n+  GrowableArray<Method*>* get_supers();\n+\n+  CompiledEntrySignature(Method* method = nullptr);\n+  void compute_calling_conventions(bool init = true);\n+  void initialize_from_fingerprint(AdapterFingerPrint* fingerprint);\n+};\n+\n","filename":"src\/hotspot\/share\/runtime\/sharedRuntime.hpp","additions":141,"deletions":20,"binary":false,"changes":161,"status":"modified"},{"patch":"@@ -317,0 +317,16 @@\n+\/\/ These checks are required for wait, notify and exit to avoid inflating the monitor to\n+\/\/ find out this inline type object cannot be locked.\n+#define CHECK_THROW_NOSYNC_IMSE(obj)  \\\n+  if ((obj)->mark().is_inline_type()) {  \\\n+    JavaThread* THREAD = current;           \\\n+    ResourceMark rm(THREAD);                \\\n+    THROW_MSG(vmSymbols::java_lang_IllegalMonitorStateException(), obj->klass()->external_name()); \\\n+  }\n+\n+#define CHECK_THROW_NOSYNC_IMSE_0(obj)  \\\n+  if ((obj)->mark().is_inline_type()) {  \\\n+    JavaThread* THREAD = current;             \\\n+    ResourceMark rm(THREAD);                  \\\n+    THROW_MSG_0(vmSymbols::java_lang_IllegalMonitorStateException(), obj->klass()->external_name()); \\\n+  }\n+\n@@ -343,0 +359,1 @@\n+  assert(!obj->klass()->is_inline_klass(), \"monitor op on inline type\");\n@@ -430,0 +447,1 @@\n+  JavaThread* THREAD = current;\n@@ -438,0 +456,10 @@\n+  if (obj->klass()->is_inline_klass()) {\n+    ResourceMark rm(THREAD);\n+    const char* desc = \"Cannot synchronize on an instance of value class \";\n+    const char* className = obj->klass()->external_name();\n+    size_t msglen = strlen(desc) + strlen(className) + 1;\n+    char* message = NEW_RESOURCE_ARRAY(char, msglen);\n+    assert(message != nullptr, \"NEW_RESOURCE_ARRAY should have called vm_exit_out_of_memory and not return nullptr\");\n+    THROW_MSG(vmSymbols::java_lang_IdentityException(), className);\n+  }\n+\n@@ -455,0 +483,1 @@\n+  CHECK_THROW_NOSYNC_IMSE(obj);\n@@ -515,0 +544,1 @@\n+  CHECK_THROW_NOSYNC_IMSE_0(obj);\n@@ -544,0 +574,1 @@\n+  CHECK_THROW_NOSYNC_IMSE(obj);\n@@ -557,0 +588,1 @@\n+  CHECK_THROW_NOSYNC_IMSE(obj);\n@@ -641,0 +673,3 @@\n+  \/\/ VM should be calling bootstrap method.\n+  assert(!obj->klass()->is_inline_klass(), \"FastHashCode should not be called for inline classes\");\n+\n@@ -740,0 +775,3 @@\n+  if (h_obj->mark().is_inline_type()) {\n+    return false;\n+  }\n","filename":"src\/hotspot\/share\/runtime\/synchronizer.cpp","additions":38,"deletions":0,"binary":false,"changes":38,"status":"modified"},{"patch":"@@ -420,0 +420,1 @@\n+  initialize_class(vmSymbols::java_lang_IdentityException(), CHECK);\n","filename":"src\/hotspot\/share\/runtime\/threads.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -61,0 +61,2 @@\n+#include \"oops\/flatArrayKlass.hpp\"\n+#include \"oops\/inlineKlass.hpp\"\n@@ -942,1 +944,3 @@\n-           declare_type(ObjArrayKlass, ArrayKlass)                        \\\n+           declare_type(ObjArrayKlass, ArrayKlass)                        \\\n+             declare_type(FlatArrayKlass, ArrayKlass)                     \\\n+             declare_type(RefArrayKlass, ArrayKlass)                      \\\n@@ -945,0 +949,1 @@\n+        declare_type(InlineKlass, InstanceKlass)                          \\\n@@ -1417,1 +1422,1 @@\n-  declare_constant(Klass::_lh_array_tag_obj_value)                        \\\n+  declare_constant(Klass::_lh_array_tag_ref_value)                        \\\n","filename":"src\/hotspot\/share\/runtime\/vmStructs.cpp","additions":7,"deletions":2,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -67,0 +67,3 @@\n+        @JEP(number=401, title=\"Value Classes and Objects\", status = \"Preview\")\n+        VALUE_OBJECTS,\n+\n@@ -84,1 +87,0 @@\n-\n","filename":"src\/java.base\/share\/classes\/jdk\/internal\/javac\/PreviewFeature.java","additions":3,"deletions":1,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -211,0 +211,13 @@\n+    public static boolean isSupported(Feature feature, int majorVersion) {\n+        Source source = null;\n+        for (Target target : Target.values()) {\n+            if (majorVersion == target.majorVersion) {\n+                source = lookup(target.name);\n+            }\n+        }\n+        if (source != null) {\n+            return feature.allowedInSource(source);\n+        }\n+        return false;\n+    }\n+\n@@ -275,0 +288,1 @@\n+        VALUE_CLASSES(JDK22, Fragments.FeatureValueClasses, DiagKind.PLURAL),\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/code\/Source.java","additions":14,"deletions":0,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -127,0 +127,1 @@\n+    final LocalProxyVarsGen localProxyVarsGen;\n@@ -167,0 +168,1 @@\n+        localProxyVarsGen = LocalProxyVarsGen.instance(context);\n@@ -189,0 +191,2 @@\n+        allowValueClasses = (!preview.isPreview(Feature.VALUE_CLASSES) || preview.isEnabled()) &&\n+                Feature.VALUE_CLASSES.allowedInSource(source);\n@@ -207,0 +211,4 @@\n+    \/** Are value classes allowed\n+     *\/\n+    private final boolean allowValueClasses;\n+\n@@ -300,3 +308,1 @@\n-            return;\n-        }\n-        if ((v.flags() & FINAL) != 0 &&\n+        } else if ((v.flags() & FINAL) != 0 &&\n@@ -313,17 +319,0 @@\n-            return;\n-        }\n-\n-        \/\/ Check instance field assignments that appear in constructor prologues\n-        if (rs.isEarlyReference(env, base, v)) {\n-\n-            \/\/ Field may not be inherited from a superclass\n-            if (v.owner != env.enclClass.sym) {\n-                log.error(pos, Errors.CantRefBeforeCtorCalled(v));\n-                return;\n-            }\n-\n-            \/\/ Field may not have an initializer\n-            if ((v.flags() & HASINIT) != 0) {\n-                log.error(pos, Errors.CantAssignInitializedBeforeCtorCalled(v));\n-                return;\n-            }\n@@ -1124,1 +1113,1 @@\n-                            if (TreeInfo.hasAnyConstructorCall(tree)) {\n+                            if (!allowValueClasses && TreeInfo.hasAnyConstructorCall(tree)) {\n@@ -1202,0 +1191,1 @@\n+                boolean addedSuperInIdentityClass = false;\n@@ -1206,1 +1196,6 @@\n-                        tree.body.stats = tree.body.stats.prepend(supCall);\n+                        if (allowValueClasses && (owner.isValueClass() || owner.hasStrict() || ((owner.flags_field & RECORD) != 0))) {\n+                            tree.body.stats = tree.body.stats.append(supCall);\n+                        } else {\n+                            tree.body.stats = tree.body.stats.prepend(supCall);\n+                            addedSuperInIdentityClass = true;\n+                        }\n@@ -1244,0 +1239,29 @@\n+                if (localEnv.info.ctorPrologue) {\n+                    boolean thisInvocation = false;\n+                    ListBuffer<JCTree> prologueCode = new ListBuffer<>();\n+                    for (JCTree stat : tree.body.stats) {\n+                        prologueCode.add(stat);\n+                        \/* gather all the stats in the body until a `super` or `this` constructor invocation is found,\n+                         * including the constructor invocation, that way we don't need to worry in the visitor below if\n+                         * if we are dealing or not with prologue code\n+                         *\/\n+                        if (stat instanceof JCExpressionStatement expStmt &&\n+                                expStmt.expr instanceof JCMethodInvocation mi &&\n+                                TreeInfo.isConstructorCall(mi)) {\n+                            thisInvocation = TreeInfo.name(mi.meth) == names._this;\n+                            if (!addedSuperInIdentityClass || !allowValueClasses) {\n+                                break;\n+                            }\n+                        }\n+                    }\n+                    if (!prologueCode.isEmpty()) {\n+                        CtorPrologueVisitor ctorPrologueVisitor = new CtorPrologueVisitor(localEnv,\n+                                addedSuperInIdentityClass && allowValueClasses ?\n+                                        PrologueVisitorMode.WARNINGS_ONLY :\n+                                        thisInvocation ?\n+                                                PrologueVisitorMode.THIS_CONSTRUCTOR :\n+                                                PrologueVisitorMode.SUPER_CONSTRUCTOR,\n+                                false);\n+                        ctorPrologueVisitor.scan(prologueCode.toList());\n+                    }\n+                }\n@@ -1255,0 +1279,338 @@\n+    enum PrologueVisitorMode {\n+        WARNINGS_ONLY,\n+        SUPER_CONSTRUCTOR,\n+        THIS_CONSTRUCTOR\n+    }\n+\n+    class CtorPrologueVisitor extends TreeScanner {\n+        Env<AttrContext> localEnv;\n+        PrologueVisitorMode mode;\n+        boolean isInitializer;\n+\n+        CtorPrologueVisitor(Env<AttrContext> localEnv, PrologueVisitorMode mode, boolean isInitializer) {\n+            this.localEnv = localEnv;\n+            currentClassSym = localEnv.enclClass.sym;\n+            this.mode = mode;\n+            this.isInitializer = isInitializer;\n+        }\n+\n+        boolean insideLambdaOrClassDef = false;\n+\n+        @Override\n+        public void visitLambda(JCLambda lambda) {\n+            boolean previousInsideLambdaOrClassDef = insideLambdaOrClassDef;\n+            try {\n+                insideLambdaOrClassDef = true;\n+                super.visitLambda(lambda);\n+            } finally {\n+                insideLambdaOrClassDef = previousInsideLambdaOrClassDef;\n+            }\n+        }\n+\n+        ClassSymbol currentClassSym;\n+\n+        @Override\n+        public void visitClassDef(JCClassDecl classDecl) {\n+            boolean previousInsideLambdaOrClassDef = insideLambdaOrClassDef;\n+            ClassSymbol previousClassSym = currentClassSym;\n+            try {\n+                insideLambdaOrClassDef = true;\n+                currentClassSym = classDecl.sym;\n+                super.visitClassDef(classDecl);\n+            } finally {\n+                insideLambdaOrClassDef = previousInsideLambdaOrClassDef;\n+                currentClassSym = previousClassSym;\n+            }\n+        }\n+\n+        private void reportPrologueError(JCTree tree, Symbol sym) {\n+            reportPrologueError(tree, sym, false);\n+        }\n+\n+        private void reportPrologueError(JCTree tree, Symbol sym, boolean hasInit) {\n+            preview.checkSourceLevel(tree, Feature.FLEXIBLE_CONSTRUCTORS);\n+            if (mode != PrologueVisitorMode.WARNINGS_ONLY) {\n+                if (hasInit) {\n+                    log.error(tree, Errors.CantAssignInitializedBeforeCtorCalled(sym));\n+                } else {\n+                    log.error(tree, Errors.CantRefBeforeCtorCalled(sym));\n+                }\n+            } else if (allowValueClasses) {\n+                \/\/ issue lint warning\n+                log.warning(tree, LintWarnings.WouldNotBeAllowedInPrologue(sym));\n+            }\n+        }\n+\n+        @Override\n+        public void visitApply(JCMethodInvocation tree) {\n+            super.visitApply(tree);\n+            Name name = TreeInfo.name(tree.meth);\n+            boolean isConstructorCall = name == names._this || name == names._super;\n+            Symbol msym = TreeInfo.symbolFor(tree.meth);\n+            \/\/ is this an instance method call or an illegal constructor invocation like: `this.super()`?\n+            if (msym != null && \/\/ for erroneous invocations msym can be null, ignore those\n+                (!isConstructorCall ||\n+                isConstructorCall && tree.meth.hasTag(SELECT))) {\n+                if (isEarlyReference(localEnv, tree.meth, msym))\n+                    reportPrologueError(tree.meth, msym);\n+            }\n+        }\n+\n+        @Override\n+        public void visitIdent(JCIdent tree) {\n+            analyzeSymbol(tree);\n+        }\n+\n+        boolean isIndexed = false;\n+\n+        @Override\n+        public void visitIndexed(JCArrayAccess tree) {\n+            boolean previousIsIndexed = isIndexed;\n+            try {\n+                isIndexed = true;\n+                scan(tree.indexed);\n+            } finally {\n+                isIndexed = previousIsIndexed;\n+            }\n+            scan(tree.index);\n+            if (mode == PrologueVisitorMode.SUPER_CONSTRUCTOR && isInstanceField(tree.indexed)) {\n+                localProxyVarsGen.addFieldReadInPrologue(localEnv.enclMethod, TreeInfo.symbolFor(tree.indexed));\n+            }\n+        }\n+\n+        @Override\n+        public void visitSelect(JCFieldAccess tree) {\n+            SelectScanner ss = new SelectScanner();\n+            ss.scan(tree);\n+            if (ss.scanLater == null) {\n+                Symbol sym = TreeInfo.symbolFor(tree);\n+                \/\/ if this is a field access\n+                if (sym.kind == VAR && sym.owner.kind == TYP) {\n+                    \/\/ Type.super.field or super.field expressions are forbidden in early construction contexts\n+                    for (JCTree subtree : ss.selectorTrees) {\n+                        if (TreeInfo.isSuperOrSelectorDotSuper(subtree)) {\n+                            reportPrologueError(tree, sym);\n+                            return;\n+                        }\n+                    }\n+                }\n+                analyzeSymbol(tree);\n+            } else {\n+                boolean prevLhs = isInLHS;\n+                try {\n+                    isInLHS = false;\n+                    scan(ss.scanLater);\n+                } finally {\n+                    isInLHS = prevLhs;\n+                }\n+            }\n+            if (mode == PrologueVisitorMode.SUPER_CONSTRUCTOR) {\n+                for (JCTree subtree : ss.selectorTrees) {\n+                    if (isInstanceField(subtree)) {\n+                        \/\/ we need to add a proxy for this one\n+                        localProxyVarsGen.addFieldReadInPrologue(localEnv.enclMethod, TreeInfo.symbolFor(subtree));\n+                    }\n+                }\n+            }\n+        }\n+\n+        boolean isInstanceField(JCTree tree) {\n+            Symbol sym = TreeInfo.symbolFor(tree);\n+            return (sym != null &&\n+                    !sym.isStatic() &&\n+                    sym.kind == VAR &&\n+                    sym.owner.kind == TYP &&\n+                    sym.name != names._this &&\n+                    sym.name != names._super &&\n+                    isEarlyReference(localEnv, tree, sym));\n+        }\n+\n+        @Override\n+        public void visitNewClass(JCNewClass tree) {\n+            super.visitNewClass(tree);\n+            checkNewClassAndMethRefs(tree, tree.type);\n+        }\n+\n+        @Override\n+        public void visitReference(JCMemberReference tree) {\n+            super.visitReference(tree);\n+            if (tree.getMode() == JCMemberReference.ReferenceMode.NEW) {\n+                checkNewClassAndMethRefs(tree, tree.expr.type);\n+            }\n+        }\n+\n+        void checkNewClassAndMethRefs(JCTree tree, Type t) {\n+            if (t.tsym.isEnclosedBy(localEnv.enclClass.sym) &&\n+                    !t.tsym.isStatic() &&\n+                    !t.tsym.isDirectlyOrIndirectlyLocal()) {\n+                reportPrologueError(tree, t.getEnclosingType().tsym);\n+            }\n+        }\n+\n+        \/* if a symbol is in the LHS of an assignment expression we won't consider it as a candidate\n+         * for a proxy local variable later on\n+         *\/\n+        boolean isInLHS = false;\n+\n+        @Override\n+        public void visitAssign(JCAssign tree) {\n+            boolean previousIsInLHS = isInLHS;\n+            try {\n+                isInLHS = true;\n+                scan(tree.lhs);\n+            } finally {\n+                isInLHS = previousIsInLHS;\n+            }\n+            scan(tree.rhs);\n+        }\n+\n+        @Override\n+        public void visitMethodDef(JCMethodDecl tree) {\n+            \/\/ ignore any declarative part, mainly to avoid scanning receiver parameters\n+            scan(tree.body);\n+        }\n+\n+        void analyzeSymbol(JCTree tree) {\n+            Symbol sym = TreeInfo.symbolFor(tree);\n+            \/\/ make sure that there is a symbol and it is not static\n+            if (sym == null || sym.isStatic()) {\n+                return;\n+            }\n+            if (isInLHS && !insideLambdaOrClassDef) {\n+                \/\/ Check instance field assignments that appear in constructor prologues\n+                if (isEarlyReference(localEnv, tree, sym)) {\n+                    \/\/ Field may not be inherited from a superclass\n+                    if (sym.owner != localEnv.enclClass.sym) {\n+                        reportPrologueError(tree, sym);\n+                        return;\n+                    }\n+                    \/\/ Field may not have an initializer\n+                    if ((sym.flags() & HASINIT) != 0) {\n+                        if (!localEnv.enclClass.sym.isValueClass() || !sym.type.hasTag(ARRAY) || !isIndexed) {\n+                            reportPrologueError(tree, sym, true);\n+                        }\n+                        return;\n+                    }\n+                    \/\/ cant reference an instance field before a this constructor\n+                    if (allowValueClasses && mode == PrologueVisitorMode.THIS_CONSTRUCTOR) {\n+                        reportPrologueError(tree, sym);\n+                        return;\n+                    }\n+                }\n+                return;\n+            }\n+            tree = TreeInfo.skipParens(tree);\n+            if (sym.kind == VAR && sym.owner.kind == TYP) {\n+                if (sym.name == names._this || sym.name == names._super) {\n+                    \/\/ are we seeing something like `this` or `CurrentClass.this` or `SuperClass.super::foo`?\n+                    if (TreeInfo.isExplicitThisReference(\n+                            types,\n+                            (ClassType)localEnv.enclClass.sym.type,\n+                            tree)) {\n+                        reportPrologueError(tree, sym);\n+                    }\n+                } else if (sym.kind == VAR && sym.owner.kind == TYP) { \/\/ now fields only\n+                    if (sym.owner != localEnv.enclClass.sym) {\n+                        if (localEnv.enclClass.sym.isSubClass(sym.owner, types) &&\n+                                sym.isInheritedIn(localEnv.enclClass.sym, types)) {\n+                            \/* if we are dealing with a field that doesn't belong to the current class, but the\n+                             * field is inherited, this is an error. Unless, the super class is also an outer\n+                             * class and the field's qualifier refers to the outer class\n+                             *\/\n+                            if (tree.hasTag(IDENT) ||\n+                                TreeInfo.isExplicitThisReference(\n+                                        types,\n+                                        (ClassType)localEnv.enclClass.sym.type,\n+                                        ((JCFieldAccess)tree).selected)) {\n+                                reportPrologueError(tree, sym);\n+                            }\n+                        }\n+                    } else if (isEarlyReference(localEnv, tree, sym)) {\n+                        \/* now this is a `proper` instance field of the current class\n+                         * references to fields of identity classes which happen to have initializers are\n+                         * not allowed in the prologue.\n+                         * But it is OK for a field with initializer to refer to another field with initializer,\n+                         * so no warning or error if we are analyzing a field initializer.\n+                         *\/\n+                        if (insideLambdaOrClassDef ||\n+                            (!localEnv.enclClass.sym.isValueClass() &&\n+                             (sym.flags_field & HASINIT) != 0 &&\n+                             !isInitializer))\n+                            reportPrologueError(tree, sym);\n+                        \/\/ we will need to generate a proxy for this field later on\n+                        if (!isInLHS) {\n+                            if (!allowValueClasses) {\n+                                reportPrologueError(tree, sym);\n+                            } else {\n+                                if (mode == PrologueVisitorMode.THIS_CONSTRUCTOR) {\n+                                    reportPrologueError(tree, sym);\n+                                } else if (mode == PrologueVisitorMode.SUPER_CONSTRUCTOR) {\n+                                    localProxyVarsGen.addFieldReadInPrologue(localEnv.enclMethod, sym);\n+                                }\n+                                \/* we do nothing in warnings only mode, as in that mode we are simulating what\n+                                 * the compiler would do in case the constructor code would be in the prologue\n+                                 * phase\n+                                 *\/\n+                            }\n+                        }\n+                    }\n+                }\n+            }\n+        }\n+\n+        \/**\n+         * Determine if the symbol appearance constitutes an early reference to the current class.\n+         *\n+         * <p>\n+         * This means the symbol is an instance field, or method, of the current class and it appears\n+         * in an early initialization context of it (i.e., one of its constructor prologues).\n+         *\n+         * @param env    The current environment\n+         * @param tree   the AST referencing the variable\n+         * @param sym    The symbol\n+         *\/\n+        private boolean isEarlyReference(Env<AttrContext> env, JCTree tree, Symbol sym) {\n+            if ((sym.flags() & STATIC) == 0 &&\n+                    (sym.kind == VAR || sym.kind == MTH) &&\n+                    sym.isMemberOf(env.enclClass.sym, types)) {\n+                \/\/ Allow \"Foo.this.x\" when \"Foo\" is (also) an outer class, as this refers to the outer instance\n+                if (tree instanceof JCFieldAccess fa) {\n+                    return TreeInfo.isExplicitThisReference(types, (ClassType)env.enclClass.type, fa.selected);\n+                } else if (currentClassSym != env.enclClass.sym) {\n+                    \/* so we are inside a class, CI, in the prologue of an outer class, CO, and the symbol being\n+                     * analyzed has no qualifier. So if the symbol is a member of CI the reference is allowed,\n+                     * otherwise it is not.\n+                     * It could be that the reference to CI's member happens inside CI's own prologue, but that\n+                     * will be checked separately, when CI's prologue is analyzed.\n+                     *\/\n+                    return !sym.isMemberOf(currentClassSym, types);\n+                }\n+                return true;\n+            }\n+            return false;\n+        }\n+\n+        \/* scanner for a select expression, anything that is not a select or identifier\n+         * will be stored for further analysis\n+         *\/\n+        class SelectScanner extends DeferredAttr.FilterScanner {\n+            JCTree scanLater;\n+            java.util.List<JCTree> selectorTrees = new ArrayList<>();\n+\n+            SelectScanner() {\n+                super(Set.of(IDENT, SELECT, PARENS));\n+            }\n+\n+            @Override\n+            public void visitSelect(JCFieldAccess tree) {\n+                super.visitSelect(tree);\n+                selectorTrees.add(tree.selected);\n+            }\n+\n+            @Override\n+            void skip(JCTree tree) {\n+                scanLater = tree;\n+            }\n+        }\n+    }\n+\n@@ -1303,0 +1665,1 @@\n+                Env<AttrContext> initEnv = memberEnter.initEnv(tree, env);\n@@ -1309,1 +1672,0 @@\n-                    Env<AttrContext> initEnv = memberEnter.initEnv(tree, env);\n@@ -1315,4 +1677,13 @@\n-                    attribExpr(tree.init, initEnv, v.type);\n-                    if (tree.isImplicitlyTyped()) {\n-                        \/\/fixup local variable type\n-                        v.type = chk.checkLocalVarType(tree, tree.init.type, tree.name);\n+                    boolean previousCtorPrologue = initEnv.info.ctorPrologue;\n+                    try {\n+                        if (v.owner.kind == TYP && !v.isStatic() && v.isStrict()) {\n+                            \/\/ strict instance initializer in a value class\n+                            initEnv.info.ctorPrologue = true;\n+                        }\n+                        attribExpr(tree.init, initEnv, v.type);\n+                        if (tree.isImplicitlyTyped()) {\n+                            \/\/fixup local variable type\n+                            v.type = chk.checkLocalVarType(tree, tree.init.type, tree.name);\n+                        }\n+                    } finally {\n+                        initEnv.info.ctorPrologue = previousCtorPrologue;\n@@ -1321,0 +1692,7 @@\n+                if (allowValueClasses && v.owner.kind == TYP && !v.isStatic()) {\n+                    \/\/ strict field initializers are inlined in constructor's prologues\n+                    CtorPrologueVisitor ctorPrologueVisitor = new CtorPrologueVisitor(initEnv,\n+                            !v.isStrict() ? PrologueVisitorMode.WARNINGS_ONLY : PrologueVisitorMode.SUPER_CONSTRUCTOR,\n+                            true);\n+                    ctorPrologueVisitor.scan(tree.init);\n+                }\n@@ -1439,1 +1817,5 @@\n-            if ((tree.flags & STATIC) != 0) localEnv.info.staticLevel++;\n+            if ((tree.flags & STATIC) != 0) {\n+                localEnv.info.staticLevel++;\n+            } else {\n+                localEnv.info.instanceInitializerBlock = true;\n+            }\n@@ -1952,2 +2334,2 @@\n-        chk.checkRefType(tree.pos(), attribExpr(tree.lock, env));\n-        if (tree.lock.type != null && tree.lock.type.isValueBased()) {\n+        boolean identityType = chk.checkIdentityType(tree.pos(), attribExpr(tree.lock, env));\n+        if (identityType && tree.lock.type != null && tree.lock.type.isValueBased()) {\n@@ -4407,0 +4789,1 @@\n+        Assert.check(site == tree.selected.type);\n@@ -5518,1 +5901,1 @@\n-                } else {\n+                } else if ((c.flags_field & Flags.COMPOUND) == 0) {\n@@ -5555,0 +5938,5 @@\n+                if (c.isValueClass()) {\n+                    Assert.check(env.tree.hasTag(CLASSDEF));\n+                    chk.checkConstraintsOfValueClass((JCClassDecl) env.tree, c);\n+                }\n+\n@@ -5697,1 +6085,1 @@\n-            chk.checkSerialStructure(tree, c);\n+            chk.checkSerialStructure(env, tree, c);\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/comp\/Attr.java","additions":420,"deletions":32,"binary":false,"changes":452,"status":"modified"},{"patch":"@@ -64,0 +64,1 @@\n+import com.sun.tools.javac.resources.CompilerProperties;\n@@ -68,0 +69,1 @@\n+import com.sun.tools.javac.tree.JCTree;\n@@ -78,0 +80,1 @@\n+import static com.sun.tools.javac.code.Scope.LookupKind.NON_RECURSIVE;\n@@ -113,0 +116,4 @@\n+    \/** Switch: allow value classes.\n+     *\/\n+    boolean allowValueClasses;\n+\n@@ -292,0 +299,2 @@\n+        allowValueClasses = (!preview.isPreview(Feature.VALUE_CLASSES) || preview.isEnabled()) &&\n+                Feature.VALUE_CLASSES.allowedInSource(source);\n@@ -584,3 +593,5 @@\n-                    return (outer == Type.noType) ?\n-                            t.erasure(types) :\n-                        new ClassType(outer, List.nil(), t);\n+                    if (outer == Type.noType) {\n+                        ClassType et = (ClassType) t.erasure(types);\n+                        return new ClassType(et.getEnclosingType(), List.nil(), et.tsym, et.getMetadata());\n+                    }\n+                    return new ClassType(outer, List.nil(), t, List.nil());\n@@ -608,1 +619,1 @@\n-                outer = new ClassType(outer, actuals, t) {\n+                outer = new ClassType(outer, actuals, t, List.nil()) {\n@@ -692,1 +703,1 @@\n-                    outer = new ClassType(outer, List.nil(), t);\n+                    outer = new ClassType(outer, List.nil(), t, List.nil());\n@@ -1549,0 +1560,7 @@\n+            } else if (proxy.type.tsym.flatName() == syms.migratedValueClassInternalType.tsym.flatName()) {\n+                Assert.check(sym.kind == TYP);\n+                sym.flags_field |= MIGRATED_VALUE_CLASS;\n+                if (needsValueFlag(sym, sym.flags_field)) {\n+                    sym.flags_field |= VALUE_CLASS;\n+                    sym.flags_field &= ~IDENTITY_TYPE;\n+                }\n@@ -1568,0 +1586,6 @@\n+                }  else if (proxy.type.tsym == syms.migratedValueClassType.tsym && sym.kind == TYP) {\n+                    sym.flags_field |= MIGRATED_VALUE_CLASS;\n+                    if (needsValueFlag(sym, sym.flags_field)) {\n+                        sym.flags_field |= VALUE_CLASS;\n+                        sym.flags_field &= ~IDENTITY_TYPE;\n+                    }\n@@ -3058,1 +3082,1 @@\n-        long flags = adjustClassFlags(f);\n+        long flags = adjustClassFlags(c, f);\n@@ -3150,1 +3174,1 @@\n-            long flags = adjustClassFlags(nextChar());\n+            long flags = adjustClassFlags(c, nextChar());\n@@ -3296,0 +3320,5 @@\n+        boolean previewClassFile = minorVersion == ClassFile.PREVIEW_MINOR_VERSION;\n+        if (allowValueClasses && previewClassFile && (flags & ACC_STRICT) != 0) {\n+            flags &= ~ACC_STRICT;\n+            flags |= STRICT;\n+        }\n@@ -3311,1 +3340,1 @@\n-    long adjustClassFlags(long flags) {\n+    long adjustClassFlags(ClassSymbol c, long flags) {\n@@ -3316,1 +3345,24 @@\n-        return flags & ~ACC_SUPER; \/\/ SUPER and SYNCHRONIZED bits overloaded\n+        if (((flags & ACC_IDENTITY) != 0 && !isMigratedValueClass(flags))\n+                || (majorVersion <= Version.MAX().major && minorVersion != PREVIEW_MINOR_VERSION && (flags & INTERFACE) == 0)) {\n+            flags |= IDENTITY_TYPE;\n+        } else if (needsValueFlag(c, flags)) {\n+            flags |= VALUE_CLASS;\n+            flags &= ~IDENTITY_TYPE;\n+        }\n+        flags &= ~ACC_IDENTITY; \/\/ ACC_IDENTITY and SYNCHRONIZED bits overloaded\n+        return flags;\n+    }\n+\n+    private boolean needsValueFlag(Symbol c, long flags) {\n+        boolean previewClassFile = minorVersion == ClassFile.PREVIEW_MINOR_VERSION;\n+        if (allowValueClasses) {\n+            if (previewClassFile && majorVersion >= V67.major && (flags & INTERFACE) == 0 ||\n+                    majorVersion >= V67.major && isMigratedValueClass(flags)) {\n+                return true;\n+            }\n+        }\n+        return false;\n+    }\n+\n+    private boolean isMigratedValueClass(long flags) {\n+        return allowValueClasses && ((flags & MIGRATED_VALUE_CLASS) != 0);\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/jvm\/ClassReader.java","additions":61,"deletions":9,"binary":false,"changes":70,"status":"modified"},{"patch":"@@ -786,1 +786,1 @@\n-    improperly formed type, some parameters are missing\n+    improperly formed type, some parameters are missing or misplaced\n@@ -2139,0 +2139,8 @@\n+# lint: serial\n+compiler.warn.serializable.value.class.without.write.replace.1=\\\n+    serializable value class does not declare, or inherits, a writeReplace method\n+\n+# lint: serial\n+compiler.warn.serializable.value.class.without.write.replace.2=\\\n+    serializable class does not declare, or inherits, a writeReplace method\n+\n@@ -2881,0 +2889,3 @@\n+compiler.misc.type.req.identity=\\\n+    a type with identity\n+\n@@ -3941,0 +3952,3 @@\n+compiler.misc.bad.access.flags=\\\n+    bad access flags combination: {0}\n+\n@@ -4282,0 +4296,22 @@\n+compiler.misc.feature.value.classes=\\\n+    value classes\n+\n+# 0: type, 1: type\n+compiler.err.value.type.has.identity.super.type=\\\n+    The identity type {1} cannot be a supertype of the value type {0}\n+\n+# 0: symbol, 1: type\n+compiler.err.concrete.supertype.for.value.class=\\\n+    The concrete class {1} is not allowed to be a super class of the value class {0} either directly or indirectly\n+\n+# 0: symbol, 1: symbol, 2: type\n+compiler.err.super.class.method.cannot.be.synchronized=\\\n+    The method {0} in the super class {2} of the value class {1} is synchronized. This is disallowed\n+\n+compiler.err.non.abstract.value.class.cant.be.sealed.or.non.sealed=\\\n+    ''sealed'' or ''non-sealed'' modifiers are only applicable to abstract value classes\n+\n+# 0: symbol\n+compiler.err.strict.field.not.have.been.initialized.before.super=\\\n+    strict field {0} is not initialized before the supertype constructor has been called\n+\n@@ -4309,0 +4345,5 @@\n+# 0: symbol or name\n+# lint: initialization\n+compiler.warn.would.not.be.allowed.in.prologue=\\\n+    reference to {0} would not be allowed in the prologue phase\n+\n@@ -4313,0 +4354,3 @@\n+\n+compiler.warn.value.finalize=\\\n+    value classes should not have finalize methods, they are not invoked\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/resources\/compiler.properties","additions":45,"deletions":1,"binary":false,"changes":46,"status":"modified"},{"patch":"@@ -613,0 +613,3 @@\n+    -   `initialization`: Warns about code in identity classes that wouldn't be\n+        allowed in early construction due to a `this` dependency.\n+\n","filename":"src\/jdk.compiler\/share\/man\/javac.md","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -82,0 +82,3 @@\n+compiler\/jvmci\/jdk.vm.ci.hotspot.test\/src\/jdk\/vm\/ci\/hotspot\/test\/MemoryAccessProviderTest.java 8350208 generic-all\n+compiler\/jvmci\/jdk.vm.ci.hotspot.test\/src\/jdk\/vm\/ci\/hotspot\/test\/TestHotSpotResolvedJavaField.java 8350208 generic-all\n+\n@@ -84,0 +87,24 @@\n+# Valhalla\n+compiler\/whitebox\/DeoptimizeRelocatedNMethod.java 8370571 generic-all\n+compiler\/whitebox\/RelocateNMethodMultiplePaths.java#C1 8370571 generic-all\n+compiler\/whitebox\/RelocateNMethodMultiplePaths.java#C2 8370571 generic-all\n+compiler\/whitebox\/RelocateNMethodMultiplePaths.java#G1C1 8370571 generic-all\n+compiler\/whitebox\/RelocateNMethodMultiplePaths.java#G1C2 8370571 generic-all\n+compiler\/whitebox\/RelocateNMethodMultiplePaths.java#ParallelC1 8370571 generic-all\n+compiler\/whitebox\/RelocateNMethodMultiplePaths.java#ParallelC2 8370571 generic-all\n+compiler\/whitebox\/RelocateNMethodMultiplePaths.java#SerialC1 8370571 generic-all\n+compiler\/whitebox\/RelocateNMethodMultiplePaths.java#SerialC2 8370571 generic-all\n+compiler\/whitebox\/RelocateNMethodMultiplePaths.java#ZGCC1 8370571 generic-all\n+compiler\/whitebox\/RelocateNMethodMultiplePaths.java#ZGCC2 8370571 generic-all\n+compiler\/whitebox\/StressNMethodRelocation.java 8370571 generic-all\n+compiler\/valhalla\/inlinetypes\/TestC1.java             8372341 generic-all\n+compiler\/valhalla\/inlinetypes\/TestCastMismatch.java   8372341 generic-all\n+compiler\/valhalla\/inlinetypes\/TestGetfieldChains.java 8372341 generic-all\n+compiler\/codegen\/TestRedundantLea.java#GetAndSet          8361089 generic-all\n+compiler\/codegen\/TestRedundantLea.java#StringEquals       8361089 generic-all\n+compiler\/codegen\/TestRedundantLea.java#StringInflate      8361089 generic-all\n+compiler\/codegen\/TestRedundantLea.java#RegexFind          8361089 generic-all\n+compiler\/codegen\/TestRedundantLea.java#StoreNSerial       8361089 generic-all\n+compiler\/codegen\/TestRedundantLea.java#StoreNParallel     8361089 generic-all\n+compiler\/codegen\/TestRedundantLea.java#Spill              8361089 generic-all\n+\n@@ -105,0 +132,1 @@\n+runtime\/cds\/appcds\/redefineClass\/RedefineRunningMethods_Shared.java  8304168 generic-all\n@@ -125,0 +153,16 @@\n+\n+# Valhalla\n+runtime\/valhalla\/inlinetypes\/verifier\/StrictInstanceFieldsTest.java CODETOOLS-7904031 generic-all\n+runtime\/valhalla\/inlinetypes\/verifier\/StrictStaticFieldsTest.java CODETOOLS-7904031 generic-all\n+\n+runtime\/cds\/TestDefaultArchiveLoading.java#coops_nocoh            8366774           generic-all\n+runtime\/cds\/TestDefaultArchiveLoading.java#nocoops_nocoh          8366774           generic-all\n+\n+# Valhalla + AOT\n+runtime\/cds\/appcds\/aotCache\/HelloAOTCache.java                                  8369043 generic-aarch64\n+runtime\/cds\/appcds\/aotCode\/AOTCodeFlags.java                                    8369043 generic-aarch64\n+runtime\/cds\/appcds\/methodHandles\/MethodHandlesGeneralTest.java#aot              8367408 generic-all\n+runtime\/cds\/appcds\/resolvedConstants\/ResolvedConstants.java#aot                 8371456 generic-all\n+runtime\/cds\/appcds\/resolvedConstants\/ResolvedConstants.java#static              8371456 generic-all\n+runtime\/cds\/appcds\/dynamicArchive\/DynamicArchiveRelocationTest.java             8372265 generic-all\n+runtime\/cds\/appcds\/dynamicArchive\/HelloDynamicInlineClass.java                  8372265 generic-all\n@@ -151,0 +195,57 @@\n+\n+# Valhalla TODO:\n+serviceability\/sa\/ClhsdbCDSCore.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbCDSJstackPrintAll.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbFindPC.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbInspect.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbLongConstant.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbJdis.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbJstack.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbPrintAll.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbPrintAs.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbPrintStatics.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbSource.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbSymbol.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbWhere.java 8190936 generic-all\n+serviceability\/sa\/JhsdbThreadInfoTest.java 8190936 generic-all\n+serviceability\/sa\/TestClassDump.java 8190936 generic-all\n+serviceability\/sa\/TestClhsdbJstackLock.java 8190936 generic-all\n+serviceability\/sa\/TestCpoolForInvokeDynamic.java 8190936 generic-all\n+serviceability\/sa\/TestHeapDumpForInvokeDynamic.java 8190936 generic-all\n+serviceability\/sa\/TestHeapDumpForLargeArray.java 8190936 generic-all\n+serviceability\/sa\/TestIntConstant.java 8190936 generic-all\n+serviceability\/sa\/TestJhsdbJstackLock.java 8190936 generic-all\n+serviceability\/sa\/TestJmapCore.java 8190936 generic-all\n+serviceability\/sa\/TestJmapCoreMetaspace.java 8190936 generic-all\n+serviceability\/sa\/TestPrintMdo.java 8190936 generic-all\n+serviceability\/sa\/jmap-hprof\/JMapHProfLargeHeapTest.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbDumpclass.java 8190936 generic-all\n+\n+# Array Changes TODO\n+serviceability\/sa\/CDSJMapClstats.java 8365722 generic-all\n+serviceability\/sa\/ClhsdbClasses.java 8365722 generic-all\n+serviceability\/sa\/sadebugd\/DisableRegistryTest.java 8365722 generic-all\n+serviceability\/sa\/ClhsdbDumpheap.java 8365722 generic-all\n+serviceability\/sa\/sadebugd\/ClhsdbTestConnectArgument.java 8365722 generic-all\n+serviceability\/sa\/sadebugd\/DebugdConnectTest.java 8365722 generic-all\n+serviceability\/sa\/ClhsdbJhisto.java 8365722 generic-all\n+serviceability\/sa\/ClhsdbJstack.java#id1 8365722 generic-all\n+serviceability\/sa\/ClhsdbJstackWithConcurrentLock.java 8365722 generic-all\n+serviceability\/sa\/TestJhsdbJstackWithVirtualThread.java 8365722 generic-all\n+serviceability\/sa\/ClhsdbJstackXcompStress.java 8365722 generic-all\n+serviceability\/sa\/ClhsdbPstack.java#process 8365722 generic-all\n+serviceability\/sa\/ClhsdbPstack.java#core 8365722 generic-all\n+serviceability\/sa\/ClhsdbScanOops.java#id0 8365722 generic-all\n+serviceability\/sa\/ClhsdbScanOops.java#id1 8365722 generic-all\n+serviceability\/sa\/ClhsdbScanOops.java#serial 8365722 generic-all\n+serviceability\/sa\/ClhsdbScanOops.java#parallel 8365722 generic-all\n+serviceability\/sa\/DeadlockDetectionTest.java 8365722 generic-all\n+serviceability\/sa\/ClhsdbJstack.java#id0 8365722 generic-all\n+serviceability\/sa\/TestInstanceKlassSize.java 8365722 generic-all\n+serviceability\/sa\/TestSysProps.java 8365722 generic-all\n+serviceability\/sa\/sadebugd\/ClhsdbAttachToDebugServer.java 8365722 generic-all\n+resourcehogs\/serviceability\/sa\/TestHeapDumpForLargeArray.java 8365722 generic-all\n+\n+resourcehogs\/serviceability\/sa\/ClhsdbRegionDetailsScanOopsForG1.java 8190936 generic-all\n+vmTestbase\/nsk\/jvmti\/scenarios\/events\/EM04\/em04t001\/TestDescription.java 8367590 generic-all\n+\n@@ -189,0 +290,2 @@\n+vmTestbase\/vm\/mlvm\/hiddenloader\/stress\/byteMutation\/Test.java 8317172 generic-all\n+\n","filename":"test\/hotspot\/jtreg\/ProblemList.txt","additions":103,"deletions":0,"binary":false,"changes":103,"status":"modified"},{"patch":"@@ -49,1 +49,1 @@\n-  runtime\n+  runtime \\\n@@ -65,0 +65,8 @@\n+hotspot_valhalla = \\\n+  runtime\/valhalla \\\n+  compiler\/valhalla \\\n+  serviceability\/jvmti\/valhalla\n+\n+hotspot_valhalla_runtime = \\\n+  runtime\/valhalla\n+\n@@ -214,0 +222,1 @@\n+  compiler\/valhalla\/ \\\n@@ -255,0 +264,7 @@\n+\n+tier1_compiler_no_valhalla = \\\n+  :tier1_compiler_1 \\\n+  :tier1_compiler_2 \\\n+  :tier1_compiler_3 \\\n+  -compiler\/valhalla\n+\n@@ -408,0 +424,4 @@\n+tier1_runtime_no_valhalla = \\\n+  :tier1_runtime \\\n+  -runtime\/valhalla\n+\n","filename":"test\/hotspot\/jtreg\/TEST.groups","additions":21,"deletions":1,"binary":false,"changes":22,"status":"modified"},{"patch":"@@ -31,0 +31,1 @@\n+import compiler.valhalla.inlinetypes.InlineTypeIRNode;\n@@ -90,1 +91,1 @@\n-    private static final String PREFIX = \"_#\";\n+    public static final String PREFIX = \"_#\";\n@@ -153,0 +154,6 @@\n+    \/\/ Valhalla: Make sure that all Valhalla specific IR nodes are also properly initialized. Doing it here also\n+    \/\/           ensures that the Flag VM is able to pick up the correct compile phases.\n+    static {\n+        InlineTypeIRNode.forceStaticInitialization();\n+    }\n+\n@@ -384,2 +391,6 @@\n-        String regex = START + \"Allocate\\\\b\" + MID + \"allocationKlass:.*\\\\b\" + IS_REPLACED + \"\\\\s.*\" + END;\n-        macroNodes(ALLOC_OF, regex);\n+        allocateOfNodes(ALLOC_OF, IS_REPLACED);\n+    }\n+\n+    public static void allocateOfNodes(String irNodePlaceholder, String allocatee) {\n+        String regex = START + \"Allocate\\\\b\" + MID + \"allocationKlass:.*\\\\b\" + allocatee + \"\\\\s.*\" + END;\n+        macroNodes(irNodePlaceholder, regex);\n@@ -396,0 +407,4 @@\n+        allocateArrayOfNodes(ALLOC_ARRAY_OF, IS_REPLACED);\n+    }\n+\n+    public static void allocateArrayOfNodes(String irNodePlaceholder, String allocatee) {\n@@ -414,1 +429,1 @@\n-        String name_part = \"\\\\[+.(\" + partial_name_prefix + \")?\" + IS_REPLACED + \";\";\n+        String name_part = \"\\\\[+.(\" + partial_name_prefix + \")?\" + allocatee + \";\";\n@@ -416,1 +431,1 @@\n-        macroNodes(ALLOC_ARRAY_OF, regex);\n+        macroNodes(irNodePlaceholder, regex);\n@@ -481,1 +496,1 @@\n-        callOfNodes(CALL_OF, \"Call.*\");\n+        callOfNodes(CALL_OF, \"Call.*\", IS_REPLACED + \" \" );\n@@ -486,1 +501,6 @@\n-        callOfNodes(CALL_OF_METHOD, \"Call.*Java\");\n+        callOfNodes(CALL_OF_METHOD, \"Call.*Java\", IS_REPLACED + \" \");\n+    }\n+\n+    public static final String STATIC_CALL = PREFIX + \"STATIC_CALL\" + POSTFIX;\n+    static {\n+        beforeMatchingNameRegex(STATIC_CALL, \"CallStaticJava\");\n@@ -491,1 +511,19 @@\n-        callOfNodes(STATIC_CALL_OF_METHOD, \"CallStaticJava\");\n+        staticCallOfMethodNodes(STATIC_CALL_OF_METHOD, IS_REPLACED + \" \");\n+    }\n+\n+    public static void staticCallOfMethodNodes(String irNodePlaceholder, String calleeRegex) {\n+        callOfNodes(irNodePlaceholder, \"CallStaticJava\", calleeRegex);\n+    }\n+\n+    public static final String CALL_LEAF_NO_FP = PREFIX + \"CALL_LEAF_NO_FP\" + POSTFIX;\n+    static {\n+        beforeMatchingNameRegex(CALL_LEAF_NO_FP, \"CallLeafNoFP\");\n+    }\n+\n+    public static final String CALL_LEAF_NO_FP_OF_METHOD = COMPOSITE_PREFIX + \"CALL_LEAF_NO_FP_OF_METHOD\" + POSTFIX;\n+    static {\n+        callLeafNoFpOfMethodNodes(CALL_LEAF_NO_FP_OF_METHOD, IS_REPLACED);\n+    }\n+\n+    public static void callLeafNoFpOfMethodNodes(String irNodePlaceholder, String calleeRegex) {\n+        callOfNodes(irNodePlaceholder, \"CallLeafNoFP\", calleeRegex);\n@@ -588,0 +626,5 @@\n+    public static final String CMP_N = PREFIX + \"CMP_N\" + POSTFIX;\n+    static {\n+        beforeMatchingNameRegex(CMP_N, \"CmpN\");\n+    }\n+\n@@ -742,1 +785,1 @@\n-        callOfNodes(DYNAMIC_CALL_OF_METHOD, \"CallDynamicJava\");\n+        callOfNodes(DYNAMIC_CALL_OF_METHOD, \"CallDynamicJava\", IS_REPLACED);\n@@ -878,0 +921,5 @@\n+    public static final String INLINE_TYPE = PREFIX + \"INLINE_TYPE\" + POSTFIX;\n+    static {\n+        beforeMatchingNameRegex(INLINE_TYPE, \"InlineType\");\n+    }\n+\n@@ -924,1 +972,5 @@\n-        loadOfNodes(LOAD_OF_CLASS, \"Load(B|UB|S|US|I|L|F|D|P|N)\");\n+        anyLoadOfNodes(LOAD_OF_CLASS, IS_REPLACED);\n+    }\n+\n+    public static void anyLoadOfNodes(String irNodePlaceholder, String fieldHolder) {\n+        loadOfNodes(irNodePlaceholder, \"Load(B|UB|S|US|I|L|F|D|P|N)\", fieldHolder);\n@@ -934,1 +986,1 @@\n-        loadOfNodes(LOAD_B_OF_CLASS, \"LoadB\");\n+        loadOfNodes(LOAD_B_OF_CLASS, \"LoadB\", IS_REPLACED);\n@@ -944,1 +996,1 @@\n-        loadOfNodes(LOAD_D_OF_CLASS, \"LoadD\");\n+        loadOfNodes(LOAD_D_OF_CLASS, \"LoadD\", IS_REPLACED);\n@@ -954,1 +1006,1 @@\n-        loadOfNodes(LOAD_F_OF_CLASS, \"LoadF\");\n+        loadOfNodes(LOAD_F_OF_CLASS, \"LoadF\", IS_REPLACED);\n@@ -964,1 +1016,1 @@\n-        loadOfNodes(LOAD_I_OF_CLASS, \"LoadI\");\n+        loadOfNodes(LOAD_I_OF_CLASS, \"LoadI\", IS_REPLACED);\n@@ -989,1 +1041,1 @@\n-        loadOfNodes(LOAD_L_OF_CLASS, \"LoadL\");\n+        loadOfNodes(LOAD_L_OF_CLASS, \"LoadL\", IS_REPLACED);\n@@ -999,1 +1051,1 @@\n-        loadOfNodes(LOAD_N_OF_CLASS, \"LoadN\");\n+        loadOfNodes(LOAD_N_OF_CLASS, \"LoadN\", IS_REPLACED);\n@@ -1015,1 +1067,1 @@\n-        loadOfNodes(LOAD_P_OF_CLASS, \"LoadP\");\n+        loadOfNodes(LOAD_P_OF_CLASS, \"LoadP\", IS_REPLACED);\n@@ -1025,1 +1077,1 @@\n-        loadOfNodes(LOAD_S_OF_CLASS, \"LoadS\");\n+        loadOfNodes(LOAD_S_OF_CLASS, \"LoadS\", IS_REPLACED);\n@@ -1035,1 +1087,1 @@\n-        loadOfNodes(LOAD_UB_OF_CLASS, \"LoadUB\");\n+        loadOfNodes(LOAD_UB_OF_CLASS, \"LoadUB\", IS_REPLACED);\n@@ -1045,1 +1097,1 @@\n-        loadOfNodes(LOAD_US_OF_CLASS, \"LoadUS\");\n+        loadOfNodes(LOAD_US_OF_CLASS, \"LoadUS\", IS_REPLACED);\n@@ -1959,1 +2011,1 @@\n-        storeOfNodes(STORE_B_OF_CLASS, \"StoreB\");\n+        storeOfNodes(STORE_B_OF_CLASS, \"StoreB\", IS_REPLACED);\n@@ -1969,1 +2021,1 @@\n-        storeOfNodes(STORE_C_OF_CLASS, \"StoreC\");\n+        storeOfNodes(STORE_C_OF_CLASS, \"StoreC\", IS_REPLACED);\n@@ -1979,1 +2031,1 @@\n-        storeOfNodes(STORE_D_OF_CLASS, \"StoreD\");\n+        storeOfNodes(STORE_D_OF_CLASS, \"StoreD\", IS_REPLACED);\n@@ -1989,1 +2041,1 @@\n-        storeOfNodes(STORE_F_OF_CLASS, \"StoreF\");\n+        storeOfNodes(STORE_F_OF_CLASS, \"StoreF\", IS_REPLACED);\n@@ -1999,1 +2051,1 @@\n-        storeOfNodes(STORE_I_OF_CLASS, \"StoreI\");\n+        storeOfNodes(STORE_I_OF_CLASS, \"StoreI\", IS_REPLACED);\n@@ -2009,1 +2061,1 @@\n-        storeOfNodes(STORE_L_OF_CLASS, \"StoreL\");\n+        storeOfNodes(STORE_L_OF_CLASS, \"StoreL\", IS_REPLACED);\n@@ -2019,1 +2071,1 @@\n-        storeOfNodes(STORE_N_OF_CLASS, \"StoreN\");\n+        storeOfNodes(STORE_N_OF_CLASS, \"StoreN\", IS_REPLACED);\n@@ -2024,1 +2076,5 @@\n-        storeOfNodes(STORE_OF_CLASS, \"Store(B|C|S|I|L|F|D|P|N)\");\n+        anyStoreOfNodes(STORE_OF_CLASS, IS_REPLACED);\n+    }\n+\n+    public static void anyStoreOfNodes(String irNodePlaceholder, String fieldHolder) {\n+        storeOfNodes(irNodePlaceholder, \"Store(B|C|S|I|L|F|D|P|N)\", fieldHolder);\n@@ -2040,1 +2096,1 @@\n-        storeOfNodes(STORE_P_OF_CLASS, \"StoreP\");\n+        storeOfNodes(STORE_P_OF_CLASS, \"StoreP\", IS_REPLACED);\n@@ -2140,1 +2196,2 @@\n-        beforeMatchingNameRegex(SUBTYPE_CHECK, \"SubTypeCheck\");\n+        String regex = START + \"SubTypeCheck\" + MID + END;\n+        macroNodes(SUBTYPE_CHECK, regex);\n@@ -3092,1 +3149,1 @@\n-    private static void beforeMatching(String irNodePlaceholder, String regex) {\n+    public static void beforeMatching(String irNodePlaceholder, String regex) {\n@@ -3128,2 +3185,2 @@\n-    private static void callOfNodes(String irNodePlaceholder, String callRegex) {\n-        String regex = START + callRegex + MID + IS_REPLACED + \" \" +  END;\n+    private static void callOfNodes(String irNodePlaceholder, String callRegex, String calleeRegex) {\n+        String regex = START + callRegex + MID + calleeRegex + END;\n@@ -3137,1 +3194,1 @@\n-    private static void optoOnly(String irNodePlaceholder, String regex) {\n+    public static void optoOnly(String irNodePlaceholder, String regex) {\n@@ -3222,2 +3279,2 @@\n-    private static void loadOfNodes(String irNodePlaceholder, String irNodeRegex) {\n-        String regex = START + irNodeRegex + MID + LOAD_STORE_PREFIX + IS_REPLACED + LOAD_STORE_SUFFIX + END;\n+    private static void loadOfNodes(String irNodePlaceholder, String irNodeRegex, String loadee) {\n+        String regex = START + irNodeRegex + MID + LOAD_STORE_PREFIX + loadee + LOAD_STORE_SUFFIX + END;\n@@ -3227,2 +3284,2 @@\n-    private static void storeOfNodes(String irNodePlaceholder, String irNodeRegex) {\n-        String regex = START + irNodeRegex + MID + LOAD_STORE_PREFIX + IS_REPLACED + LOAD_STORE_SUFFIX + END;\n+    private static void storeOfNodes(String irNodePlaceholder, String irNodeRegex, String storee) {\n+        String regex = START + irNodeRegex + MID + LOAD_STORE_PREFIX + storee + LOAD_STORE_SUFFIX + END;\n","filename":"test\/hotspot\/jtreg\/compiler\/lib\/ir_framework\/IRNode.java","additions":95,"deletions":38,"binary":false,"changes":133,"status":"modified"},{"patch":"@@ -479,0 +479,3 @@\n+        if (flag.equals(\"enable-valhalla\")) {\n+            return checkBooleanFlag(flag, value, Integer.class.isValue());\n+        }\n","filename":"test\/hotspot\/jtreg\/compiler\/lib\/ir_framework\/test\/IREncodingPrinter.java","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -532,0 +532,5 @@\n+java\/lang\/ModuleLayer\/LayerControllerTest.java                  8337048 generic-all\n+java\/lang\/ModuleLayer\/BasicLayerTest.java                       8337048 generic-all\n+\n+java\/lang\/Thread\/virtual\/stress\/Skynet.java#default             8342977 generic-all\n+\n@@ -549,0 +554,1 @@\n+com\/sun\/management\/HotSpotDiagnosticMXBean\/DumpThreadsWithEliminatedLock.java 8360599 generic-all\n@@ -570,0 +576,2 @@\n+java\/net\/CookieHandler\/B6644726.java                            8365811 generic-all\n+\n@@ -720,0 +728,4 @@\n+com\/sun\/jdi\/cds\/CDSBreakpointTest.java                          8304168 generic-all\n+com\/sun\/jdi\/cds\/CDSDeleteAllBkptsTest.java                      8304168 generic-all\n+com\/sun\/jdi\/cds\/CDSFieldWatchpoints.java                        8304168 generic-all\n+\n@@ -814,0 +826,1 @@\n+\n@@ -819,0 +832,16 @@\n+\n+############################################################################\n+\n+# valhalla\n+java\/lang\/invoke\/VarHandles\/VarHandleTestMethodHandleAccessValue.java 8367346 generic-all\n+\n+jdk\/classfile\/AccessFlagsTest.java 8366270 generic-all\n+jdk\/jfr\/event\/runtime\/TestClassLoaderStatsEvent.java 8366820 generic-all\n+\n+jdk\/internal\/vm\/Continuation\/Fuzz.java#default 8370177 generic-aarch64\n+jdk\/internal\/vm\/Continuation\/Fuzz.java#preserve-fp 8370177 generic-aarch64\n+\n+sun\/tools\/jhsdb\/BasicLauncherTest.java 8366806 generic-all\n+sun\/tools\/jhsdb\/HeapDumpTest.java 8366806 generic-all\n+sun\/tools\/jhsdb\/HeapDumpTestWithActiveProcess.java 8366806 generic-all\n+sun\/tools\/jhsdb\/JShellHeapDumpTest.java 8366806 generic-all\n","filename":"test\/jdk\/ProblemList.txt","additions":29,"deletions":0,"binary":false,"changes":29,"status":"modified"},{"patch":"@@ -387,0 +387,1 @@\n+        vmOptFinalFlag(map, \"TieredCompilation\");\n","filename":"test\/jtreg-ext\/requires\/VMProps.java","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -58,0 +58,1 @@\n+    @SuppressWarnings(\"initialization\")\n","filename":"test\/lib\/jdk\/test\/lib\/containers\/docker\/DockerRunOptions.java","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -156,0 +156,14 @@\n+  private native Object[] getObjectsViaKlassOopMaps0(Object thing);\n+  public Object[] getObjectsViaKlassOopMaps(Object thing) {\n+    Objects.requireNonNull(thing);\n+    return getObjectsViaKlassOopMaps0(thing);\n+  }\n+\n+  private native Object[] getObjectsViaOopIterator0(Object thing);\n+  public Object[] getObjectsViaOopIterator(Object thing) {\n+    Objects.requireNonNull(thing);\n+    return getObjectsViaOopIterator0(thing);\n+  }\n+\n+  public native Object[] getObjectsViaFrameOopIterator(int depth);\n+\n@@ -734,4 +748,7 @@\n-  private final List<Function<String,Object>> flagsGetters = Arrays.asList(\n-    this::getBooleanVMFlag, this::getIntVMFlag, this::getUintVMFlag,\n-    this::getIntxVMFlag, this::getUintxVMFlag, this::getUint64VMFlag,\n-    this::getSizeTVMFlag, this::getStringVMFlag, this::getDoubleVMFlag);\n+  private final List<Function<String,Object>> flagsGetters;\n+  {\n+      flagsGetters = Arrays.asList(\n+          this::getBooleanVMFlag, this::getIntVMFlag, this::getUintVMFlag,\n+          this::getIntxVMFlag, this::getUintxVMFlag, this::getUint64VMFlag,\n+          this::getSizeTVMFlag, this::getStringVMFlag, this::getDoubleVMFlag);\n+  }\n@@ -776,0 +793,1 @@\n+  @SuppressWarnings(\"initialization\")\n","filename":"test\/lib\/jdk\/test\/whitebox\/WhiteBox.java","additions":22,"deletions":4,"binary":false,"changes":26,"status":"modified"}]}