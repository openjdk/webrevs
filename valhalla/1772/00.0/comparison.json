{"files":[{"patch":"@@ -296,2 +296,1 @@\n-  $1_JAVA_ARGS := -Dextlink.spec.version=$$(VERSION_SPECIFICATION) \\\n-\t-Djspec.version=$$(VERSION_SPECIFICATION)\n+  $1_JAVA_ARGS := -Dextlink.spec.version=$$(VERSION_SPECIFICATION)\n","filename":"make\/Docs.gmk","additions":1,"deletions":2,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -152,1 +152,0 @@\n-  # Only G1 supports dumping the shared heap, so explicitly use G1 if the JVM supports it.\n","filename":"make\/Images.gmk","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -4,1 +4,0 @@\n-\/\/ Copyright 2025 Arm Limited and\/or its affiliates.\n@@ -1198,0 +1197,1 @@\n+  static int emit_exception_handler(C2_MacroAssembler *masm);\n@@ -1200,0 +1200,4 @@\n+  static uint size_exception_handler() {\n+    return MacroAssembler::far_codestub_branch_size();\n+  }\n+\n@@ -1201,1 +1205,1 @@\n-    \/\/ count one branch instruction and one far call instruction sequence\n+    \/\/ count one adr and one far branch instruction\n@@ -2251,0 +2255,19 @@\n+\/\/ Emit exception handler code.\n+int HandlerImpl::emit_exception_handler(C2_MacroAssembler* masm)\n+{\n+  \/\/ mov rscratch1 #exception_blob_entry_point\n+  \/\/ br rscratch1\n+  \/\/ Note that the code buffer's insts_mark is always relative to insts.\n+  \/\/ That's why we must use the macroassembler to generate a handler.\n+  address base = __ start_a_stub(size_exception_handler());\n+  if (base == nullptr) {\n+    ciEnv::current()->record_failure(\"CodeCache is full\");\n+    return 0;  \/\/ CodeBuffer::expand failed\n+  }\n+  int offset = __ offset();\n+  __ far_jump(RuntimeAddress(OptoRuntime::exception_blob()->entry_point()));\n+  assert(__ offset() - offset <= (int) size_exception_handler(), \"overflow\");\n+  __ end_a_stub();\n+  return offset;\n+}\n+\n@@ -2261,6 +2284,2 @@\n-\n-  Label start;\n-  __ bind(start);\n-  __ far_call(RuntimeAddress(SharedRuntime::deopt_blob()->unpack()));\n-  int entry_offset = __ offset();\n-  __ b(start);\n+  __ adr(lr, __ pc());\n+  __ far_jump(RuntimeAddress(SharedRuntime::deopt_blob()->unpack()));\n@@ -2272,1 +2291,1 @@\n-  return entry_offset;\n+  return offset;\n@@ -16334,1 +16353,1 @@\n-instruct cmpFastLockLightweight(rFlagsReg cr, iRegP object, iRegP box, iRegPNoSp tmp, iRegPNoSp tmp2, iRegPNoSp tmp3)\n+instruct cmpFastLock(rFlagsReg cr, iRegP object, iRegP box, iRegPNoSp tmp, iRegPNoSp tmp2, iRegPNoSp tmp3)\n@@ -16343,1 +16362,1 @@\n-    __ fast_lock_lightweight($object$$Register, $box$$Register, $tmp$$Register, $tmp2$$Register, $tmp3$$Register);\n+    __ fast_lock($object$$Register, $box$$Register, $tmp$$Register, $tmp2$$Register, $tmp3$$Register);\n@@ -16349,1 +16368,1 @@\n-instruct cmpFastUnlockLightweight(rFlagsReg cr, iRegP object, iRegP box, iRegPNoSp tmp, iRegPNoSp tmp2, iRegPNoSp tmp3)\n+instruct cmpFastUnlock(rFlagsReg cr, iRegP object, iRegP box, iRegPNoSp tmp, iRegPNoSp tmp2, iRegPNoSp tmp3)\n@@ -16358,1 +16377,1 @@\n-    __ fast_unlock_lightweight($object$$Register, $box$$Register, $tmp$$Register, $tmp2$$Register, $tmp3$$Register);\n+    __ fast_unlock($object$$Register, $box$$Register, $tmp$$Register, $tmp2$$Register, $tmp3$$Register);\n","filename":"src\/hotspot\/cpu\/aarch64\/aarch64.ad","additions":32,"deletions":13,"binary":false,"changes":45,"status":"modified"},{"patch":"@@ -455,8 +455,2 @@\n-  Label start;\n-  __ bind(start);\n-\n-  __ far_call(RuntimeAddress(SharedRuntime::deopt_blob()->unpack()));\n-\n-  int entry_offset = __ offset();\n-  __ b(start);\n-\n+  __ adr(lr, pc());\n+  __ far_jump(RuntimeAddress(SharedRuntime::deopt_blob()->unpack()));\n@@ -466,1 +460,1 @@\n-  return entry_offset;\n+  return offset;\n","filename":"src\/hotspot\/cpu\/aarch64\/c1_LIRAssembler_aarch64.cpp","additions":3,"deletions":9,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -74,1 +74,1 @@\n-    _deopt_handler_size = 4 * NativeInstruction::instruction_size\n+    _deopt_handler_size = 7 * NativeInstruction::instruction_size\n","filename":"src\/hotspot\/cpu\/aarch64\/c1_LIRAssembler_aarch64.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -75,1 +75,1 @@\n-  lightweight_lock(basic_lock, obj, hdr, temp, rscratch2, slow_case);\n+  fast_lock(basic_lock, obj, hdr, temp, rscratch2, slow_case);\n@@ -88,1 +88,1 @@\n-  lightweight_unlock(obj, hdr, temp, rscratch2, slow_case);\n+  fast_unlock(obj, hdr, temp, rscratch2, slow_case);\n","filename":"src\/hotspot\/cpu\/aarch64\/c1_MacroAssembler_aarch64.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -171,2 +171,2 @@\n-void C2_MacroAssembler::fast_lock_lightweight(Register obj, Register box, Register t1,\n-                                              Register t2, Register t3) {\n+void C2_MacroAssembler::fast_lock(Register obj, Register box, Register t1,\n+                                  Register t2, Register t3) {\n@@ -197,1 +197,1 @@\n-  { \/\/ Lightweight locking\n+  { \/\/ Fast locking\n@@ -327,2 +327,2 @@\n-void C2_MacroAssembler::fast_unlock_lightweight(Register obj, Register box, Register t1,\n-                                                Register t2, Register t3) {\n+void C2_MacroAssembler::fast_unlock(Register obj, Register box, Register t1,\n+                                    Register t2, Register t3) {\n@@ -342,1 +342,1 @@\n-  { \/\/ Lightweight unlock\n+  { \/\/ Fast unlock\n@@ -1423,5 +1423,11 @@\n-\/\/ Pack the lowest-numbered bit of each mask element in src into a long value\n-\/\/ in dst, at most the first 64 lane elements.\n-\/\/ Clobbers: rscratch1, if UseSVE=1 or the hardware doesn't support FEAT_BITPERM.\n-void C2_MacroAssembler::sve_vmask_tolong(Register dst, PRegister src, BasicType bt, int lane_cnt,\n-                                         FloatRegister vtmp1, FloatRegister vtmp2) {\n+\/\/ Pack the value of each mask element in \"src\" into a long value in \"dst\", at most\n+\/\/ the first 64 lane elements. The input \"src\" is a vector of boolean represented as\n+\/\/ bytes with 0x00\/0x01 as element values. Each lane value from \"src\" is packed into\n+\/\/ one bit in \"dst\".\n+\/\/\n+\/\/ Example:   src = 0x0001010000010001 0100000001010001, lane_cnt = 16\n+\/\/ Expected:  dst = 0x658D\n+\/\/\n+\/\/ Clobbers: rscratch1\n+void C2_MacroAssembler::sve_vmask_tolong(Register dst, FloatRegister src,\n+                                         FloatRegister vtmp, int lane_cnt) {\n@@ -1430,11 +1436,14 @@\n-  assert_different_registers(vtmp1, vtmp2);\n-\n-  Assembler::SIMD_RegVariant size = elemType_to_regVariant(bt);\n-  \/\/ Example:   src = 0b01100101 10001101, bt = T_BYTE, lane_cnt = 16\n-  \/\/ Expected:  dst = 0x658D\n-\n-  \/\/ Convert the mask into vector with sequential bytes.\n-  \/\/ vtmp1 = 0x00010100 0x00010001 0x01000000 0x01010001\n-  sve_cpy(vtmp1, size, src, 1, false);\n-  if (bt != T_BYTE) {\n-    sve_vector_narrow(vtmp1, B, vtmp1, size, vtmp2);\n+  assert_different_registers(src, vtmp);\n+  assert(UseSVE > 0, \"must be\");\n+\n+  \/\/ Compress the lowest 8 bytes.\n+  fmovd(dst, src);\n+  bytemask_compress(dst);\n+  if (lane_cnt <= 8) return;\n+\n+  \/\/ Repeat on higher bytes and join the results.\n+  \/\/ Compress 8 bytes in each iteration.\n+  for (int idx = 1; idx < (lane_cnt \/ 8); idx++) {\n+    sve_extract_integral(rscratch1, T_LONG, src, idx, vtmp);\n+    bytemask_compress(rscratch1);\n+    orr(dst, dst, rscratch1, Assembler::LSL, idx << 3);\n@@ -1442,0 +1451,1 @@\n+}\n@@ -1443,47 +1453,36 @@\n-  if (UseSVE > 1 && VM_Version::supports_svebitperm()) {\n-    \/\/ Given a vector with the value 0x00 or 0x01 in each byte, the basic idea\n-    \/\/ is to compress each significant bit of the byte in a cross-lane way. Due\n-    \/\/ to the lack of a cross-lane bit-compress instruction, we use BEXT\n-    \/\/ (bit-compress in each lane) with the biggest lane size (T = D) then\n-    \/\/ concatenate the results.\n-\n-    \/\/ The second source input of BEXT, initialized with 0x01 in each byte.\n-    \/\/ vtmp2 = 0x01010101 0x01010101 0x01010101 0x01010101\n-    sve_dup(vtmp2, B, 1);\n-\n-    \/\/ BEXT vtmp1.D, vtmp1.D, vtmp2.D\n-    \/\/ vtmp1 = 0x0001010000010001 | 0x0100000001010001\n-    \/\/ vtmp2 = 0x0101010101010101 | 0x0101010101010101\n-    \/\/         ---------------------------------------\n-    \/\/ vtmp1 = 0x0000000000000065 | 0x000000000000008D\n-    sve_bext(vtmp1, D, vtmp1, vtmp2);\n-\n-    \/\/ Concatenate the lowest significant 8 bits in each 8 bytes, and extract the\n-    \/\/ result to dst.\n-    \/\/ vtmp1 = 0x0000000000000000 | 0x000000000000658D\n-    \/\/ dst   = 0x658D\n-    if (lane_cnt <= 8) {\n-      \/\/ No need to concatenate.\n-      umov(dst, vtmp1, B, 0);\n-    } else if (lane_cnt <= 16) {\n-      ins(vtmp1, B, vtmp1, 1, 8);\n-      umov(dst, vtmp1, H, 0);\n-    } else {\n-      \/\/ As the lane count is 64 at most, the final expected value must be in\n-      \/\/ the lowest 64 bits after narrowing vtmp1 from D to B.\n-      sve_vector_narrow(vtmp1, B, vtmp1, D, vtmp2);\n-      umov(dst, vtmp1, D, 0);\n-    }\n-  } else if (UseSVE > 0) {\n-    \/\/ Compress the lowest 8 bytes.\n-    fmovd(dst, vtmp1);\n-    bytemask_compress(dst);\n-    if (lane_cnt <= 8) return;\n-\n-    \/\/ Repeat on higher bytes and join the results.\n-    \/\/ Compress 8 bytes in each iteration.\n-    for (int idx = 1; idx < (lane_cnt \/ 8); idx++) {\n-      sve_extract_integral(rscratch1, T_LONG, vtmp1, idx, vtmp2);\n-      bytemask_compress(rscratch1);\n-      orr(dst, dst, rscratch1, Assembler::LSL, idx << 3);\n-    }\n+\/\/ The function is same as above \"sve_vmask_tolong\", but it uses SVE2's BEXT\n+\/\/ instruction which requires the FEAT_BITPERM feature.\n+void C2_MacroAssembler::sve2_vmask_tolong(Register dst, FloatRegister src,\n+                                          FloatRegister vtmp1, FloatRegister vtmp2,\n+                                          int lane_cnt) {\n+  assert(lane_cnt <= 64 && is_power_of_2(lane_cnt), \"Unsupported lane count\");\n+  assert_different_registers(src, vtmp1, vtmp2);\n+  assert(UseSVE > 1 && VM_Version::supports_svebitperm(), \"must be\");\n+\n+  \/\/ Given a vector with the value 0x00 or 0x01 in each byte, the basic idea\n+  \/\/ is to compress each significant bit of the byte in a cross-lane way. Due\n+  \/\/ to the lack of a cross-lane bit-compress instruction, we use BEXT\n+  \/\/ (bit-compress in each lane) with the biggest lane size (T = D) then\n+  \/\/ concatenate the results.\n+\n+  \/\/ The second source input of BEXT, initialized with 0x01 in each byte.\n+  \/\/ vtmp2 = 0x01010101 0x01010101 0x01010101 0x01010101\n+  sve_dup(vtmp2, B, 1);\n+\n+  \/\/ BEXT vtmp1.D, src.D, vtmp2.D\n+  \/\/ src   = 0x0001010000010001 | 0x0100000001010001\n+  \/\/ vtmp2 = 0x0101010101010101 | 0x0101010101010101\n+  \/\/         ---------------------------------------\n+  \/\/ vtmp1 = 0x0000000000000065 | 0x000000000000008D\n+  sve_bext(vtmp1, D, src, vtmp2);\n+\n+  \/\/ Concatenate the lowest significant 8 bits in each 8 bytes, and extract the\n+  \/\/ result to dst.\n+  \/\/ vtmp1 = 0x0000000000000000 | 0x000000000000658D\n+  \/\/ dst   = 0x658D\n+  if (lane_cnt <= 8) {\n+    \/\/ No need to concatenate.\n+    umov(dst, vtmp1, B, 0);\n+  } else if (lane_cnt <= 16) {\n+    ins(vtmp1, B, vtmp1, 1, 8);\n+    umov(dst, vtmp1, H, 0);\n@@ -1491,2 +1490,4 @@\n-    assert(false, \"unsupported\");\n-    ShouldNotReachHere();\n+    \/\/ As the lane count is 64 at most, the final expected value must be in\n+    \/\/ the lowest 64 bits after narrowing vtmp1 from D to B.\n+    sve_vector_narrow(vtmp1, B, vtmp1, D, vtmp2);\n+    umov(dst, vtmp1, D, 0);\n@@ -1496,8 +1497,4 @@\n-\/\/ Unpack the mask, a long value in src, into predicate register dst based on the\n-\/\/ corresponding data type. Note that dst can support at most 64 lanes.\n-\/\/ Below example gives the expected dst predicate register in different types, with\n-\/\/ a valid src(0x658D) on a 1024-bit vector size machine.\n-\/\/ BYTE:  dst = 0x00 00 00 00 00 00 00 00 00 00 00 00 00 00 65 8D\n-\/\/ SHORT: dst = 0x00 00 00 00 00 00 00 00 00 00 00 00 14 11 40 51\n-\/\/ INT:   dst = 0x00 00 00 00 00 00 00 00 01 10 01 01 10 00 11 01\n-\/\/ LONG:  dst = 0x00 01 01 00 00 01 00 01 01 00 00 00 01 01 00 01\n+\/\/ Unpack the mask, a long value in \"src\", into a vector register of boolean\n+\/\/ represented as bytes with 0x00\/0x01 as element values in \"dst\".  Each bit in\n+\/\/ \"src\" is unpacked into one byte lane in \"dst\". Note that \"dst\" can support at\n+\/\/ most 64 lanes.\n@@ -1505,5 +1502,6 @@\n-\/\/ The number of significant bits of src must be equal to lane_cnt. E.g., 0xFF658D which\n-\/\/ has 24 significant bits would be an invalid input if dst predicate register refers to\n-\/\/ a LONG type 1024-bit vector, which has at most 16 lanes.\n-void C2_MacroAssembler::sve_vmask_fromlong(PRegister dst, Register src, BasicType bt, int lane_cnt,\n-                                           FloatRegister vtmp1, FloatRegister vtmp2) {\n+\/\/ Below example gives the expected dst vector register, with a valid src(0x658D)\n+\/\/ on a 128-bit vector size machine.\n+\/\/ dst = 0x00 01 01 00 00 01 00 01 01 00 00 00 01 01 00 01\n+void C2_MacroAssembler::sve_vmask_fromlong(FloatRegister dst, Register src,\n+                                           FloatRegister vtmp, int lane_cnt) {\n+  assert_different_registers(dst, vtmp);\n@@ -1512,3 +1510,3 @@\n-  Assembler::SIMD_RegVariant size = elemType_to_regVariant(bt);\n-  \/\/ Example:   src = 0x658D, bt = T_BYTE, size = B, lane_cnt = 16\n-  \/\/ Expected:  dst = 0b01101001 10001101\n+\n+  \/\/ Example:   src = 0x658D, lane_cnt = 16\n+  \/\/ Expected:  dst = 0x00 01 01 00 00 01 00 01 01 00 00 00 01 01 00 01\n@@ -1517,3 +1515,3 @@\n-  \/\/ vtmp1 = 0x0000000000000000 | 0x000000000000658D\n-  sve_dup(vtmp1, B, 0);\n-  mov(vtmp1, D, 0, src);\n+  \/\/ vtmp = 0x0000000000000000 | 0x000000000000658D\n+  sve_dup(vtmp, B, 0);\n+  mov(vtmp, D, 0, src);\n@@ -1521,3 +1519,2 @@\n-  \/\/ As sve_cmp generates mask value with the minimum unit in byte, we should\n-  \/\/ transform the value in the first lane which is mask in bit now to the\n-  \/\/ mask in byte, which can be done by SVE2's BDEP instruction.\n+  \/\/ Transform the value in the first lane which is mask in bit now to the mask in\n+  \/\/ byte, which can be done by SVE2's BDEP instruction.\n@@ -1526,1 +1523,1 @@\n-  \/\/ vtmp1 = 0x0000000000000065 | 0x000000000000008D\n+  \/\/ vtmp = 0x0000000000000065 | 0x000000000000008D\n@@ -1530,2 +1527,1 @@\n-    ins(vtmp1, B, vtmp1, 8, 1);\n-    mov(vtmp1, B, 1, zr);\n+    ins(vtmp, B, vtmp, 8, 1);\n@@ -1533,1 +1529,1 @@\n-    sve_vector_extend(vtmp1, D, vtmp1, B);\n+    sve_vector_extend(vtmp, D, vtmp, B);\n@@ -1537,17 +1533,9 @@\n-  \/\/ vtmp2 = 0x01010101 0x01010101 0x01010101 0x01010101\n-  sve_dup(vtmp2, B, 1);\n-\n-  \/\/ BDEP vtmp1.D, vtmp1.D, vtmp2.D\n-  \/\/ vtmp1 = 0x0000000000000065 | 0x000000000000008D\n-  \/\/ vtmp2 = 0x0101010101010101 | 0x0101010101010101\n-  \/\/         ---------------------------------------\n-  \/\/ vtmp1 = 0x0001010000010001 | 0x0100000001010001\n-  sve_bdep(vtmp1, D, vtmp1, vtmp2);\n-\n-  if (bt != T_BYTE) {\n-    sve_vector_extend(vtmp1, size, vtmp1, B);\n-  }\n-  \/\/ Generate mask according to the given vector, in which the elements have been\n-  \/\/ extended to expected type.\n-  \/\/ dst = 0b01101001 10001101\n-  sve_cmp(Assembler::NE, dst, size, ptrue, vtmp1, 0);\n+  \/\/ dst = 0x01010101 0x01010101 0x01010101 0x01010101\n+  sve_dup(dst, B, 1);\n+\n+  \/\/ BDEP dst.D, vtmp.D, dst.D\n+  \/\/ vtmp = 0x0000000000000065 | 0x000000000000008D\n+  \/\/ dst  = 0x0101010101010101 | 0x0101010101010101\n+  \/\/        ---------------------------------------\n+  \/\/ dst  = 0x0001010000010001 | 0x0100000001010001\n+  sve_bdep(dst, D, vtmp, dst);\n@@ -2883,1 +2871,1 @@\n-}\n+}\n","filename":"src\/hotspot\/cpu\/aarch64\/c2_MacroAssembler_aarch64.cpp","additions":103,"deletions":115,"binary":false,"changes":218,"status":"modified"},{"patch":"@@ -56,3 +56,3 @@\n-  \/\/ Code used by cmpFastLockLightweight and cmpFastUnlockLightweight mach instructions in .ad file.\n-  void fast_lock_lightweight(Register object, Register box, Register t1, Register t2, Register t3);\n-  void fast_unlock_lightweight(Register object, Register box, Register t1, Register t2, Register t3);\n+  \/\/ Code used by cmpFastLock and cmpFastUnlock mach instructions in .ad file.\n+  void fast_lock(Register object, Register box, Register t1, Register t2, Register t3);\n+  void fast_unlock(Register object, Register box, Register t1, Register t2, Register t3);\n@@ -90,4 +90,5 @@\n-  \/\/ Pack the lowest-numbered bit of each mask element in src into a long value\n-  \/\/ in dst, at most the first 64 lane elements.\n-  void sve_vmask_tolong(Register dst, PRegister src, BasicType bt, int lane_cnt,\n-                        FloatRegister vtmp1, FloatRegister vtmp2);\n+  \/\/ Pack the value of each mask element in \"src\" into a long value in \"dst\", at most the\n+  \/\/ first 64 lane elements. The input \"src\" is a vector of boolean represented as bytes\n+  \/\/ with 0x00\/0x01 as element values. Each lane value from \"src\" is packed into one bit in\n+  \/\/ \"dst\".\n+  void sve_vmask_tolong(Register dst, FloatRegister src, FloatRegister vtmp, int lane_cnt);\n@@ -95,4 +96,7 @@\n-  \/\/ Unpack the mask, a long value in src, into predicate register dst based on the\n-  \/\/ corresponding data type. Note that dst can support at most 64 lanes.\n-  void sve_vmask_fromlong(PRegister dst, Register src, BasicType bt, int lane_cnt,\n-                          FloatRegister vtmp1, FloatRegister vtmp2);\n+  void sve2_vmask_tolong(Register dst, FloatRegister src, FloatRegister vtmp1,\n+                         FloatRegister vtmp2, int lane_cnt);\n+\n+  \/\/ Unpack the mask, a long value in \"src\", into vector register \"dst\" with boolean type.\n+  \/\/ Each bit in \"src\" is unpacked into one byte lane in \"dst\". Note that \"dst\" can support\n+  \/\/ at most 64 lanes.\n+  void sve_vmask_fromlong(FloatRegister dst, Register src, FloatRegister vtmp, int lane_cnt);\n","filename":"src\/hotspot\/cpu\/aarch64\/c2_MacroAssembler_aarch64.hpp","additions":15,"deletions":11,"binary":false,"changes":26,"status":"modified"},{"patch":"@@ -93,2 +93,1 @@\n-                                                             Register scratch,\n-                                                             RegSet saved_regs) {\n+                                                             Register scratch) {\n","filename":"src\/hotspot\/cpu\/aarch64\/gc\/g1\/g1BarrierSetAssembler_aarch64.cpp","additions":1,"deletions":2,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -50,1 +50,1 @@\n-                                  Register start, Register count, Register tmp, RegSet saved_regs) {}\n+                                  Register start, Register count, Register tmp) {}\n","filename":"src\/hotspot\/cpu\/aarch64\/gc\/shared\/barrierSetAssembler_aarch64.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -44,2 +44,1 @@\n-                                                      Register start, Register count, Register tmp,\n-                                                      RegSet saved_regs) {\n+                                                      Register start, Register count, Register tmp) {\n@@ -47,1 +46,1 @@\n-    gen_write_ref_array_post_barrier(masm, decorators, start, count, tmp, saved_regs);\n+    gen_write_ref_array_post_barrier(masm, decorators, start, count, tmp);\n@@ -83,1 +82,1 @@\n-                                                                    Register start, Register count, Register scratch, RegSet saved_regs) {\n+                                                                    Register start, Register count, Register scratch) {\n","filename":"src\/hotspot\/cpu\/aarch64\/gc\/shared\/cardTableBarrierSetAssembler_aarch64.cpp","additions":3,"deletions":4,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -848,1 +848,1 @@\n-  lightweight_lock(lock_reg, obj_reg, tmp, tmp2, tmp3, slow_case);\n+  fast_lock(lock_reg, obj_reg, tmp, tmp2, tmp3, slow_case);\n@@ -880,1 +880,1 @@\n-  const Register tmp_reg    = c_rarg4;  \/\/ Temporary used by lightweight_unlock\n+  const Register tmp_reg    = c_rarg4;  \/\/ Temporary used by fast_unlock\n@@ -891,1 +891,1 @@\n-  lightweight_unlock(obj_reg, header_reg, swap_reg, tmp_reg, slow_case);\n+  fast_unlock(obj_reg, header_reg, swap_reg, tmp_reg, slow_case);\n","filename":"src\/hotspot\/cpu\/aarch64\/interp_masm_aarch64.cpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -7749,1 +7749,1 @@\n-\/\/ Implements lightweight-locking.\n+\/\/ Implements fast-locking.\n@@ -7754,1 +7754,1 @@\n-void MacroAssembler::lightweight_lock(Register basic_lock, Register obj, Register t1, Register t2, Register t3, Label& slow) {\n+void MacroAssembler::fast_lock(Register basic_lock, Register obj, Register t1, Register t2, Register t3, Label& slow) {\n@@ -7811,1 +7811,1 @@\n-\/\/ Implements lightweight-unlocking.\n+\/\/ Implements fast-unlocking.\n@@ -7816,1 +7816,1 @@\n-void MacroAssembler::lightweight_unlock(Register obj, Register t1, Register t2, Register t3, Label& slow) {\n+void MacroAssembler::fast_unlock(Register obj, Register t1, Register t2, Register t3, Label& slow) {\n@@ -7862,1 +7862,1 @@\n-  stop(\"lightweight_unlock already unlocked\");\n+  stop(\"fast_unlock already unlocked\");\n","filename":"src\/hotspot\/cpu\/aarch64\/macroAssembler_aarch64.cpp","additions":5,"deletions":5,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -1799,2 +1799,2 @@\n-  void lightweight_lock(Register basic_lock, Register obj, Register t1, Register t2, Register t3, Label& slow);\n-  void lightweight_unlock(Register obj, Register t1, Register t2, Register t3, Label& slow);\n+  void fast_lock(Register basic_lock, Register obj, Register t1, Register t2, Register t3, Label& slow);\n+  void fast_unlock(Register obj, Register t1, Register t2, Register t3, Label& slow);\n","filename":"src\/hotspot\/cpu\/aarch64\/macroAssembler_aarch64.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -1988,1 +1988,1 @@\n-  const Register lock_tmp = r14;  \/\/ Temporary used by lightweight_lock\/unlock\n+  const Register lock_tmp = r14;  \/\/ Temporary used by fast_lock\/unlock\n@@ -2005,1 +2005,1 @@\n-    __ lightweight_lock(lock_reg, obj_reg, swap_reg, tmp, lock_tmp, slow_path_lock);\n+    __ fast_lock(lock_reg, obj_reg, swap_reg, tmp, lock_tmp, slow_path_lock);\n@@ -2114,1 +2114,1 @@\n-    __ lightweight_unlock(obj_reg, old_hdr, swap_reg, lock_tmp, slow_path_unlock);\n+    __ fast_unlock(obj_reg, old_hdr, swap_reg, lock_tmp, slow_path_unlock);\n","filename":"src\/hotspot\/cpu\/aarch64\/sharedRuntime_aarch64.cpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -1666,1 +1666,1 @@\n-    bs->arraycopy_epilogue(_masm, decorators, is_oop, d, count, rscratch1, RegSet());\n+    bs->arraycopy_epilogue(_masm, decorators, is_oop, d, count, rscratch1);\n@@ -1826,1 +1826,1 @@\n-    bs->arraycopy_epilogue(_masm, decorators, is_oop, d, count, rscratch1, RegSet());\n+    bs->arraycopy_epilogue(_masm, decorators, is_oop, d, count, rscratch1);\n@@ -1893,1 +1893,0 @@\n-    RegSet wb_post_saved_regs = RegSet::of(count);\n@@ -2019,1 +2018,1 @@\n-    bs->arraycopy_epilogue(_masm, decorators, is_oop, start_to, count_save, rscratch1, wb_post_saved_regs);\n+    bs->arraycopy_epilogue(_masm, decorators, is_oop, start_to, count_save, rscratch1);\n","filename":"src\/hotspot\/cpu\/aarch64\/stubGenerator_aarch64.cpp","additions":3,"deletions":4,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -275,6 +275,0 @@\n-  Label start;\n-  __ bind(start);\n-\n-  __ jump(SharedRuntime::deopt_blob()->unpack(), relocInfo::runtime_call_type, noreg);\n-\n-  int entry_offset = __ offset();\n@@ -283,1 +277,1 @@\n-  __ b(start);\n+  __ jump(SharedRuntime::deopt_blob()->unpack(), relocInfo::runtime_call_type, noreg);\n@@ -288,1 +282,1 @@\n-  return entry_offset;\n+  return offset;\n","filename":"src\/hotspot\/cpu\/arm\/c1_LIRAssembler_arm.cpp","additions":2,"deletions":8,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -267,5 +267,0 @@\n-  Label start;\n-\n-  __ bind(start);\n-  int entry_offset = __ offset();\n-  __ b(start);\n@@ -277,1 +272,1 @@\n-  return entry_offset;\n+  return offset;\n","filename":"src\/hotspot\/cpu\/ppc\/c1_LIRAssembler_ppc.cpp","additions":1,"deletions":6,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -961,1 +961,1 @@\n-  lightweight_lock(monitor, object, header, tmp, slow_case);\n+  fast_lock(monitor, object, header, tmp, slow_case);\n@@ -990,1 +990,1 @@\n-  lightweight_unlock(object, header, slow_case);\n+  fast_unlock(object, header, slow_case);\n","filename":"src\/hotspot\/cpu\/ppc\/interp_masm_ppc_64.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -275,7 +275,1 @@\n-  }\n-\n-  int offset = code_offset();\n-\n-  Label start;\n-  __ bind(start);\n-\n+  }  int offset = code_offset();\n@@ -285,5 +279,0 @@\n-\n-  int entry_offset = __ offset();\n-\n-  __ z_bru(start);\n-\n@@ -293,1 +282,1 @@\n-  return entry_offset;\n+  return offset;\n","filename":"src\/hotspot\/cpu\/s390\/c1_LIRAssembler_s390.cpp","additions":2,"deletions":13,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -1022,1 +1022,1 @@\n-  lightweight_lock(monitor, object, header, tmp, slow_case);\n+  fast_lock(monitor, object, header, tmp, slow_case);\n@@ -1057,1 +1057,1 @@\n-  lightweight_unlock(object, header, current_header, slow_case);\n+  fast_unlock(object, header, current_header, slow_case);\n","filename":"src\/hotspot\/cpu\/s390\/interp_masm_s390.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -456,0 +456,1 @@\n+  InternalAddress here(__ pc());\n@@ -457,9 +458,2 @@\n-  Label start;\n-  __ bind(start);\n-\n-  __ call(RuntimeAddress(SharedRuntime::deopt_blob()->unpack()));\n-\n-  int entry_offset = __ offset();\n-\n-  __ jmp(start);\n-\n+  __ pushptr(here.addr(), rscratch1);\n+  __ jump(RuntimeAddress(SharedRuntime::deopt_blob()->unpack()));\n@@ -469,1 +463,1 @@\n-  return entry_offset;\n+  return offset;\n","filename":"src\/hotspot\/cpu\/x86\/c1_LIRAssembler_x86.cpp","additions":4,"deletions":10,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -51,1 +51,1 @@\n-    _deopt_handler_size = 10\n+    _deopt_handler_size = 17\n","filename":"src\/hotspot\/cpu\/x86\/c1_LIRAssembler_x86.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -57,1 +57,1 @@\n-  lightweight_lock(basic_lock, obj, hdr, tmp, slow_case);\n+  fast_lock(basic_lock, obj, hdr, tmp, slow_case);\n@@ -70,1 +70,1 @@\n-  lightweight_unlock(obj, rax, hdr, slow_case);\n+  fast_unlock(obj, rax, hdr, slow_case);\n","filename":"src\/hotspot\/cpu\/x86\/c1_MacroAssembler_x86.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -251,2 +251,2 @@\n-void C2_MacroAssembler::fast_lock_lightweight(Register obj, Register box, Register rax_reg,\n-                                              Register t, Register thread) {\n+void C2_MacroAssembler::fast_lock(Register obj, Register box, Register rax_reg,\n+                                  Register t, Register thread) {\n@@ -276,1 +276,1 @@\n-  { \/\/ Lightweight Lock\n+  { \/\/ Fast Lock\n@@ -444,1 +444,1 @@\n-void C2_MacroAssembler::fast_unlock_lightweight(Register obj, Register reg_rax, Register t, Register thread) {\n+void C2_MacroAssembler::fast_unlock(Register obj, Register reg_rax, Register t, Register thread) {\n@@ -459,1 +459,1 @@\n-  C2FastUnlockLightweightStub* stub = nullptr;\n+  C2FastUnlockStub* stub = nullptr;\n@@ -462,1 +462,1 @@\n-    stub = new (Compile::current()->comp_arena()) C2FastUnlockLightweightStub(obj, mark, reg_rax, thread);\n+    stub = new (Compile::current()->comp_arena()) C2FastUnlockStub(obj, mark, reg_rax, thread);\n@@ -468,1 +468,1 @@\n-  { \/\/ Lightweight Unlock\n+  { \/\/ Fast Unlock\n@@ -6148,9 +6148,3 @@\n-  \/\/ Since IEEE 754 floating point format represents mantissa in 1.0 format\n-  \/\/ hence biased exponent can be used to compute leading zero count as per\n-  \/\/ following formula:-\n-  \/\/ LZCNT = 31 - (biased_exp - 127)\n-  \/\/ Special handling has been introduced for Zero, Max_Int and -ve source values.\n-\n-  \/\/ Broadcast 0xFF\n-  vpcmpeqd(xtmp1, xtmp1, xtmp1, vec_enc);\n-  vpsrld(xtmp1, xtmp1, 24, vec_enc);\n+  \/\/ By converting the integer to a float, we can obtain the number of leading zeros based on the exponent of the float.\n+  \/\/ As the float exponent contains a bias of 127 for nonzero values, the bias must be removed before interpreting the\n+  \/\/ exponent as the leading zero count.\n@@ -6161,2 +6155,2 @@\n-  vpsrld(xtmp2, src, 1, vec_enc);\n-  vpandn(xtmp3, xtmp2, src, vec_enc);\n+  vpsrld(dst, src, 1, vec_enc);\n+  vpandn(dst, dst, src, vec_enc);\n@@ -6164,4 +6158,1 @@\n-  \/\/ Extract biased exponent.\n-  vcvtdq2ps(dst, xtmp3, vec_enc);\n-  vpsrld(dst, dst, 23, vec_enc);\n-  vpand(dst, dst, xtmp1, vec_enc);\n+  vcvtdq2ps(dst, dst, vec_enc);\n@@ -6169,4 +6160,2 @@\n-  \/\/ Broadcast 127.\n-  vpsrld(xtmp1, xtmp1, 1, vec_enc);\n-  \/\/ Exponent = biased_exp - 127\n-  vpsubd(dst, dst, xtmp1, vec_enc);\n+  \/\/ By comparing the register to itself, all the bits in the destination are set.\n+  vpcmpeqd(xtmp1, xtmp1, xtmp1, vec_enc);\n@@ -6174,3 +6163,4 @@\n-  \/\/ Exponent_plus_one = Exponent + 1\n-  vpsrld(xtmp3, xtmp1, 6, vec_enc);\n-  vpaddd(dst, dst, xtmp3, vec_enc);\n+  \/\/ Move the biased exponent to the low end of the lane and mask with 0xFF to discard the sign bit.\n+  vpsrld(xtmp2, xtmp1, 24, vec_enc);\n+  vpsrld(dst, dst, 23, vec_enc);\n+  vpand(dst, xtmp2, dst, vec_enc);\n@@ -6178,4 +6168,3 @@\n-  \/\/ Replace -ve exponent with zero, exponent is -ve when src\n-  \/\/ lane contains a zero value.\n-  vpxor(xtmp2, xtmp2, xtmp2, vec_enc);\n-  vblendvps(dst, dst, xtmp2, dst, vec_enc);\n+  \/\/ Subtract 127 from the exponent, which removes the bias from the exponent.\n+  vpsrld(xtmp2, xtmp1, 25, vec_enc);\n+  vpsubd(dst, dst, xtmp2, vec_enc);\n@@ -6183,6 +6172,1 @@\n-  \/\/ Rematerialize broadcast 32.\n-  vpslld(xtmp1, xtmp3, 5, vec_enc);\n-  \/\/ Exponent is 32 if corresponding source lane contains max_int value.\n-  vpcmpeqd(xtmp2, dst, xtmp1, vec_enc);\n-  \/\/ LZCNT = 32 - exponent_plus_one\n-  vpsubd(dst, xtmp1, dst, vec_enc);\n+  vpsrld(xtmp2, xtmp1, 27, vec_enc);\n@@ -6190,3 +6174,3 @@\n-  \/\/ Replace LZCNT with a value 1 if corresponding source lane\n-  \/\/ contains max_int value.\n-  vpblendvb(dst, dst, xtmp3, xtmp2, vec_enc);\n+  \/\/ If the original value is 0 the exponent would not have bias, so the subtraction creates a negative number. If this\n+  \/\/ is found in any of the lanes, replace the lane with -1 from xtmp1.\n+  vblendvps(dst, dst, xtmp1, dst, vec_enc, true, xtmp3);\n@@ -6194,3 +6178,6 @@\n-  \/\/ Replace biased_exp with 0 if source lane value is less than zero.\n-  vpxor(xtmp2, xtmp2, xtmp2, vec_enc);\n-  vblendvps(dst, dst, xtmp2, src, vec_enc);\n+  \/\/ If the original value is negative, replace the lane with 31.\n+  vblendvps(dst, dst, xtmp2, src, vec_enc, true, xtmp3);\n+\n+  \/\/ Subtract the exponent from 31, giving the final result. For 0, the result is 32 as the exponent was replaced with -1,\n+  \/\/ and for negative numbers the result is 0 as the exponent was replaced with 31.\n+  vpsubd(dst, xtmp2, dst, vec_enc);\n@@ -6201,18 +6188,18 @@\n-  vector_count_leading_zeros_short_avx(dst, src, xtmp1, xtmp2, xtmp3, rtmp, vec_enc);\n-  \/\/ Add zero counts of lower word and upper word of a double word if\n-  \/\/ upper word holds a zero value.\n-  vpsrld(xtmp3, src, 16, vec_enc);\n-  \/\/ xtmp1 is set to all zeros by vector_count_leading_zeros_byte_avx.\n-  vpcmpeqd(xtmp3, xtmp1, xtmp3, vec_enc);\n-  vpslld(xtmp2, dst, 16, vec_enc);\n-  vpaddd(xtmp2, xtmp2, dst, vec_enc);\n-  vpblendvb(dst, dst, xtmp2, xtmp3, vec_enc);\n-  vpsrld(dst, dst, 16, vec_enc);\n-  \/\/ Add zero counts of lower doubleword and upper doubleword of a\n-  \/\/ quadword if upper doubleword holds a zero value.\n-  vpsrlq(xtmp3, src, 32, vec_enc);\n-  vpcmpeqq(xtmp3, xtmp1, xtmp3, vec_enc);\n-  vpsllq(xtmp2, dst, 32, vec_enc);\n-  vpaddq(xtmp2, xtmp2, dst, vec_enc);\n-  vpblendvb(dst, dst, xtmp2, xtmp3, vec_enc);\n-  vpsrlq(dst, dst, 32, vec_enc);\n+  \/\/ Find the leading zeros of the top and bottom halves of the long individually.\n+  vector_count_leading_zeros_int_avx(dst, src, xtmp1, xtmp2, xtmp3, vec_enc);\n+\n+  \/\/ Move the top half result to the bottom half of xtmp1, setting the top half to 0.\n+  vpsrlq(xtmp1, dst, 32, vec_enc);\n+  \/\/ By moving the top half result to the right by 6 bits, if the top half was empty (i.e. 32 is returned) the result bit will\n+  \/\/ be in the most significant position of the bottom half.\n+  vpsrlq(xtmp2, dst, 6, vec_enc);\n+\n+  \/\/ In the bottom half, add the top half and bottom half results.\n+  vpaddq(dst, xtmp1, dst, vec_enc);\n+\n+  \/\/ For the bottom half, choose between the values using the most significant bit of xtmp2.\n+  \/\/ If the MSB is set, then bottom+top in dst is the resulting value. If the top half is less than 32 xtmp1 is chosen,\n+  \/\/ which contains only the top half result.\n+  \/\/ In the top half the MSB is always zero, so the value in xtmp1 is always chosen. This value is always 0, which clears\n+  \/\/ the lane as required.\n+  vblendvps(dst, xtmp1, dst, xtmp2, vec_enc, true, xtmp3);\n","filename":"src\/hotspot\/cpu\/x86\/c2_MacroAssembler_x86.cpp","additions":50,"deletions":63,"binary":false,"changes":113,"status":"modified"},{"patch":"@@ -39,3 +39,3 @@\n-  void fast_lock_lightweight(Register obj, Register box, Register rax_reg,\n-                             Register t, Register thread);\n-  void fast_unlock_lightweight(Register obj, Register reg_rax, Register t, Register thread);\n+  void fast_lock(Register obj, Register box, Register rax_reg,\n+                 Register t, Register thread);\n+  void fast_unlock(Register obj, Register reg_rax, Register t, Register thread);\n","filename":"src\/hotspot\/cpu\/x86\/c2_MacroAssembler_x86.hpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -456,1 +456,1 @@\n-    assert(sender_pc == nm->deopt_handler_entry(), \"unexpected sender pc\");\n+    assert(sender_pc == nm->deopt_handler_begin(), \"unexpected sender pc\");\n","filename":"src\/hotspot\/cpu\/x86\/frame_x86.inline.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -1254,1 +1254,1 @@\n-  lightweight_lock(lock_reg, obj_reg, swap_reg, tmp_reg, slow_case);\n+  fast_lock(lock_reg, obj_reg, swap_reg, tmp_reg, slow_case);\n@@ -1296,1 +1296,1 @@\n-  lightweight_unlock(obj_reg, swap_reg, header_reg, slow_case);\n+  fast_unlock(obj_reg, swap_reg, header_reg, slow_case);\n","filename":"src\/hotspot\/cpu\/x86\/interp_masm_x86.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -10392,1 +10392,1 @@\n-\/\/ Implements lightweight-locking.\n+\/\/ Implements fast-locking.\n@@ -10398,1 +10398,1 @@\n-void MacroAssembler::lightweight_lock(Register basic_lock, Register obj, Register reg_rax, Register tmp, Label& slow) {\n+void MacroAssembler::fast_lock(Register basic_lock, Register obj, Register reg_rax, Register tmp, Label& slow) {\n@@ -10457,1 +10457,1 @@\n-\/\/ Implements lightweight-unlocking.\n+\/\/ Implements fast-unlocking.\n@@ -10463,1 +10463,1 @@\n-void MacroAssembler::lightweight_unlock(Register obj, Register reg_rax, Register tmp, Label& slow) {\n+void MacroAssembler::fast_unlock(Register obj, Register reg_rax, Register tmp, Label& slow) {\n@@ -10495,1 +10495,1 @@\n-  stop(\"lightweight_unlock already unlocked\");\n+  stop(\"fast_unlock already unlocked\");\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.cpp","additions":5,"deletions":5,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -2126,2 +2126,2 @@\n-  void lightweight_lock(Register basic_lock, Register obj, Register reg_rax, Register tmp, Label& slow);\n-  void lightweight_unlock(Register obj, Register reg_rax, Register tmp, Label& slow);\n+  void fast_lock(Register basic_lock, Register obj, Register reg_rax, Register tmp, Label& slow);\n+  void fast_unlock(Register obj, Register reg_rax, Register tmp, Label& slow);\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2417,1 +2417,1 @@\n-    __ lightweight_lock(lock_reg, obj_reg, swap_reg, rscratch1, slow_path_lock);\n+    __ fast_lock(lock_reg, obj_reg, swap_reg, rscratch1, slow_path_lock);\n@@ -2542,1 +2542,1 @@\n-    __ lightweight_unlock(obj_reg, swap_reg, lock_reg, slow_path_unlock);\n+    __ fast_unlock(obj_reg, swap_reg, lock_reg, slow_path_unlock);\n","filename":"src\/hotspot\/cpu\/x86\/sharedRuntime_x86_64.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2781,0 +2781,1 @@\n+  static int emit_exception_handler(C2_MacroAssembler *masm);\n@@ -2783,0 +2784,9 @@\n+  static uint size_exception_handler() {\n+    \/\/ NativeCall instruction size is the same as NativeJump.\n+    \/\/ exception handler starts out as jump and can be patched to\n+    \/\/ a call be deoptimization.  (4932387)\n+    \/\/ Note that this value is also credited (in output.cpp) to\n+    \/\/ the size of the code section.\n+    return NativeJump::instruction_size;\n+  }\n+\n@@ -2784,2 +2794,2 @@\n-    \/\/ one call and one jmp.\n-    return 10;\n+    \/\/ three 5 byte instructions plus one move for unreachable address.\n+    return 15+3;\n@@ -2877,0 +2887,18 @@\n+\/\/ Emit exception handler code.\n+\/\/ Stuff framesize into a register and call a VM stub routine.\n+int HandlerImpl::emit_exception_handler(C2_MacroAssembler* masm) {\n+\n+  \/\/ Note that the code buffer's insts_mark is always relative to insts.\n+  \/\/ That's why we must use the macroassembler to generate a handler.\n+  address base = __ start_a_stub(size_exception_handler());\n+  if (base == nullptr) {\n+    ciEnv::current()->record_failure(\"CodeCache is full\");\n+    return 0;  \/\/ CodeBuffer::expand failed\n+  }\n+  int offset = __ offset();\n+  __ jump(RuntimeAddress(OptoRuntime::exception_blob()->entry_point()));\n+  assert(__ offset() - offset <= (int) size_exception_handler(), \"overflow\");\n+  __ end_a_stub();\n+  return offset;\n+}\n+\n@@ -2889,4 +2917,4 @@\n-  Label start;\n-  __ bind(start);\n-\n-  __ call(RuntimeAddress(SharedRuntime::deopt_blob()->unpack()));\n+  address the_pc = (address) __ pc();\n+  Label next;\n+  \/\/ push a \"the_pc\" on the stack without destroying any registers\n+  \/\/ as they all may be live.\n@@ -2894,3 +2922,5 @@\n-  int entry_offset = __ offset();\n-\n-  __ jmp(start);\n+  \/\/ push address of \"next\"\n+  __ call(next, relocInfo::none); \/\/ reloc none is fine since it is a disp32\n+  __ bind(next);\n+  \/\/ adjust it so it matches \"the_pc\"\n+  __ subptr(Address(rsp, 0), __ offset() - offset);\n@@ -2898,0 +2928,1 @@\n+  __ jump(RuntimeAddress(SharedRuntime::deopt_blob()->unpack()));\n@@ -2900,1 +2931,1 @@\n-  return entry_offset;\n+  return offset;\n@@ -3719,0 +3750,5 @@\n+bool Matcher::mask_op_prefers_predicate(int opcode, const TypeVect* vt) {\n+  \/\/ Prefer predicate if the mask type is \"TypeVectMask\".\n+  return vt->isa_vectmask() != nullptr;\n+}\n+\n@@ -17227,1 +17263,1 @@\n-instruct cmpFastLockLightweight(rFlagsReg cr, rRegP object, rbx_RegP box, rax_RegI rax_reg, rRegP tmp) %{\n+instruct cmpFastLock(rFlagsReg cr, rRegP object, rbx_RegP box, rax_RegI rax_reg, rRegP tmp) %{\n@@ -17233,1 +17269,1 @@\n-    __ fast_lock_lightweight($object$$Register, $box$$Register, $rax_reg$$Register, $tmp$$Register, r15_thread);\n+    __ fast_lock($object$$Register, $box$$Register, $rax_reg$$Register, $tmp$$Register, r15_thread);\n@@ -17238,1 +17274,1 @@\n-instruct cmpFastUnlockLightweight(rFlagsReg cr, rRegP object, rax_RegP rax_reg, rRegP tmp) %{\n+instruct cmpFastUnlock(rFlagsReg cr, rRegP object, rax_RegP rax_reg, rRegP tmp) %{\n@@ -17244,1 +17280,1 @@\n-    __ fast_unlock_lightweight($object$$Register, $rax_reg$$Register, $tmp$$Register, r15_thread);\n+    __ fast_unlock($object$$Register, $rax_reg$$Register, $tmp$$Register, r15_thread);\n","filename":"src\/hotspot\/cpu\/x86\/x86.ad","additions":50,"deletions":14,"binary":false,"changes":64,"status":"modified"},{"patch":"@@ -130,30 +130,0 @@\n-class ConversionStub: public CodeStub {\n- private:\n-  Bytecodes::Code _bytecode;\n-  LIR_Opr         _input;\n-  LIR_Opr         _result;\n-\n-  static float float_zero;\n-  static double double_zero;\n- public:\n-  ConversionStub(Bytecodes::Code bytecode, LIR_Opr input, LIR_Opr result)\n-    : _bytecode(bytecode), _input(input), _result(result) {\n-    ShouldNotReachHere();\n-  }\n-\n-  Bytecodes::Code bytecode() { return _bytecode; }\n-  LIR_Opr         input()    { return _input; }\n-  LIR_Opr         result()   { return _result; }\n-\n-  virtual void emit_code(LIR_Assembler* e);\n-  virtual void visit(LIR_OpVisitState* visitor) {\n-    visitor->do_slow_case();\n-    visitor->do_input(_input);\n-    visitor->do_output(_result);\n-  }\n-#ifndef PRODUCT\n-  virtual void print_name(outputStream* out) const { out->print(\"ConversionStub\"); }\n-#endif \/\/ PRODUCT\n-};\n-\n-\n","filename":"src\/hotspot\/share\/c1\/c1_CodeStubs.hpp","additions":0,"deletions":30,"binary":false,"changes":30,"status":"modified"},{"patch":"@@ -539,1 +539,0 @@\n-      do_stub(opConvert->_stub);\n@@ -1134,3 +1133,0 @@\n-  if (stub() != nullptr) {\n-    masm->append_code_stub(stub());\n-  }\n","filename":"src\/hotspot\/share\/c1\/c1_LIR.cpp","additions":0,"deletions":4,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -1457,2 +1457,0 @@\n-class ConversionStub;\n-\n@@ -1464,1 +1462,0 @@\n-   ConversionStub* _stub;\n@@ -1467,1 +1464,1 @@\n-   LIR_OpConvert(Bytecodes::Code code, LIR_Opr opr, LIR_Opr result, ConversionStub* stub)\n+   LIR_OpConvert(Bytecodes::Code code, LIR_Opr opr, LIR_Opr result)\n@@ -1469,2 +1466,1 @@\n-     , _bytecode(code)\n-     , _stub(stub)                               {}\n+     , _bytecode(code)                           {}\n@@ -1473,1 +1469,0 @@\n-  ConversionStub* stub() const                   { return _stub; }\n@@ -2313,1 +2308,1 @@\n-  void convert(Bytecodes::Code code, LIR_Opr left, LIR_Opr dst, ConversionStub* stub = nullptr\/*, bool is_32bit = false*\/) { append(new LIR_OpConvert(code, left, dst, stub)); }\n+  void convert(Bytecodes::Code code, LIR_Opr left, LIR_Opr dst) { append(new LIR_OpConvert(code, left, dst)); }\n","filename":"src\/hotspot\/share\/c1\/c1_LIR.hpp","additions":3,"deletions":8,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -0,0 +1,942 @@\n+\/*\n+ * Copyright (c) 2023, 2025, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"cds\/aotMappedHeapLoader.hpp\"\n+#include \"cds\/aotMappedHeapWriter.hpp\"\n+#include \"cds\/aotReferenceObjSupport.hpp\"\n+#include \"cds\/cdsConfig.hpp\"\n+#include \"cds\/filemap.hpp\"\n+#include \"cds\/heapShared.inline.hpp\"\n+#include \"cds\/regeneratedClasses.hpp\"\n+#include \"classfile\/javaClasses.hpp\"\n+#include \"classfile\/modules.hpp\"\n+#include \"classfile\/systemDictionary.hpp\"\n+#include \"gc\/shared\/collectedHeap.hpp\"\n+#include \"memory\/allocation.inline.hpp\"\n+#include \"memory\/iterator.inline.hpp\"\n+#include \"memory\/oopFactory.hpp\"\n+#include \"memory\/universe.hpp\"\n+#include \"oops\/compressedOops.hpp\"\n+#include \"oops\/objArrayOop.inline.hpp\"\n+#include \"oops\/oop.inline.hpp\"\n+#include \"oops\/oopHandle.inline.hpp\"\n+#include \"oops\/typeArrayKlass.hpp\"\n+#include \"oops\/typeArrayOop.hpp\"\n+#include \"runtime\/java.hpp\"\n+#include \"runtime\/mutexLocker.hpp\"\n+#include \"utilities\/bitMap.inline.hpp\"\n+#if INCLUDE_G1GC\n+#include \"gc\/g1\/g1CollectedHeap.hpp\"\n+#include \"gc\/g1\/g1HeapRegion.hpp\"\n+#endif\n+\n+#if INCLUDE_CDS_JAVA_HEAP\n+\n+GrowableArrayCHeap<u1, mtClassShared>* AOTMappedHeapWriter::_buffer = nullptr;\n+\n+\/\/ The following are offsets from buffer_bottom()\n+size_t AOTMappedHeapWriter::_buffer_used;\n+\n+\/\/ Heap root segments\n+HeapRootSegments AOTMappedHeapWriter::_heap_root_segments;\n+\n+address AOTMappedHeapWriter::_requested_bottom;\n+address AOTMappedHeapWriter::_requested_top;\n+\n+GrowableArrayCHeap<AOTMappedHeapWriter::NativePointerInfo, mtClassShared>* AOTMappedHeapWriter::_native_pointers;\n+GrowableArrayCHeap<oop, mtClassShared>* AOTMappedHeapWriter::_source_objs;\n+GrowableArrayCHeap<AOTMappedHeapWriter::HeapObjOrder, mtClassShared>* AOTMappedHeapWriter::_source_objs_order;\n+\n+AOTMappedHeapWriter::BufferOffsetToSourceObjectTable*\n+AOTMappedHeapWriter::_buffer_offset_to_source_obj_table = nullptr;\n+\n+DumpedInternedStrings *AOTMappedHeapWriter::_dumped_interned_strings = nullptr;\n+\n+typedef HashTable<\n+      size_t,    \/\/ offset of a filler from ArchiveHeapWriter::buffer_bottom()\n+      size_t,    \/\/ size of this filler (in bytes)\n+      127,       \/\/ prime number\n+      AnyObj::C_HEAP,\n+      mtClassShared> FillersTable;\n+static FillersTable* _fillers;\n+static int _num_native_ptrs = 0;\n+\n+void AOTMappedHeapWriter::init() {\n+  if (CDSConfig::is_dumping_heap()) {\n+    Universe::heap()->collect(GCCause::_java_lang_system_gc);\n+\n+    _buffer_offset_to_source_obj_table = new BufferOffsetToSourceObjectTable(\/*size (prime)*\/36137, \/*max size*\/1 * M);\n+    _dumped_interned_strings = new (mtClass)DumpedInternedStrings(INITIAL_TABLE_SIZE, MAX_TABLE_SIZE);\n+    _fillers = new FillersTable();\n+    _requested_bottom = nullptr;\n+    _requested_top = nullptr;\n+\n+    _native_pointers = new GrowableArrayCHeap<NativePointerInfo, mtClassShared>(2048);\n+    _source_objs = new GrowableArrayCHeap<oop, mtClassShared>(10000);\n+\n+    guarantee(MIN_GC_REGION_ALIGNMENT <= G1HeapRegion::min_region_size_in_words() * HeapWordSize, \"must be\");\n+  }\n+}\n+\n+void AOTMappedHeapWriter::delete_tables_with_raw_oops() {\n+  delete _source_objs;\n+  _source_objs = nullptr;\n+\n+  delete _dumped_interned_strings;\n+  _dumped_interned_strings = nullptr;\n+}\n+\n+void AOTMappedHeapWriter::add_source_obj(oop src_obj) {\n+  _source_objs->append(src_obj);\n+}\n+\n+void AOTMappedHeapWriter::write(GrowableArrayCHeap<oop, mtClassShared>* roots,\n+                                ArchiveMappedHeapInfo* heap_info) {\n+  assert(CDSConfig::is_dumping_heap(), \"sanity\");\n+  allocate_buffer();\n+  copy_source_objs_to_buffer(roots);\n+  set_requested_address(heap_info);\n+  relocate_embedded_oops(roots, heap_info);\n+}\n+\n+bool AOTMappedHeapWriter::is_too_large_to_archive(oop o) {\n+  return is_too_large_to_archive(o->size());\n+}\n+\n+bool AOTMappedHeapWriter::is_string_too_large_to_archive(oop string) {\n+  typeArrayOop value = java_lang_String::value_no_keepalive(string);\n+  return is_too_large_to_archive(value);\n+}\n+\n+bool AOTMappedHeapWriter::is_too_large_to_archive(size_t size) {\n+  assert(size > 0, \"no zero-size object\");\n+  assert(size * HeapWordSize > size, \"no overflow\");\n+  static_assert(MIN_GC_REGION_ALIGNMENT > 0, \"must be positive\");\n+\n+  size_t byte_size = size * HeapWordSize;\n+  if (byte_size > size_t(MIN_GC_REGION_ALIGNMENT)) {\n+    return true;\n+  } else {\n+    return false;\n+  }\n+}\n+\n+\/\/ Keep track of the contents of the archived interned string table. This table\n+\/\/ is used only by CDSHeapVerifier.\n+void AOTMappedHeapWriter::add_to_dumped_interned_strings(oop string) {\n+  assert_at_safepoint(); \/\/ DumpedInternedStrings uses raw oops\n+  assert(!is_string_too_large_to_archive(string), \"must be\");\n+  bool created;\n+  _dumped_interned_strings->put_if_absent(string, true, &created);\n+  if (created) {\n+    \/\/ Prevent string deduplication from changing the value field to\n+    \/\/ something not in the archive.\n+    java_lang_String::set_deduplication_forbidden(string);\n+    _dumped_interned_strings->maybe_grow();\n+  }\n+}\n+\n+bool AOTMappedHeapWriter::is_dumped_interned_string(oop o) {\n+  return _dumped_interned_strings->get(o) != nullptr;\n+}\n+\n+\/\/ Various lookup functions between source_obj, buffered_obj and requested_obj\n+bool AOTMappedHeapWriter::is_in_requested_range(oop o) {\n+  assert(_requested_bottom != nullptr, \"do not call before _requested_bottom is initialized\");\n+  address a = cast_from_oop<address>(o);\n+  return (_requested_bottom <= a && a < _requested_top);\n+}\n+\n+oop AOTMappedHeapWriter::requested_obj_from_buffer_offset(size_t offset) {\n+  oop req_obj = cast_to_oop(_requested_bottom + offset);\n+  assert(is_in_requested_range(req_obj), \"must be\");\n+  return req_obj;\n+}\n+\n+oop AOTMappedHeapWriter::source_obj_to_requested_obj(oop src_obj) {\n+  assert(CDSConfig::is_dumping_heap(), \"dump-time only\");\n+  HeapShared::CachedOopInfo* p = HeapShared::get_cached_oop_info(src_obj);\n+  if (p != nullptr) {\n+    return requested_obj_from_buffer_offset(p->buffer_offset());\n+  } else {\n+    return nullptr;\n+  }\n+}\n+\n+oop AOTMappedHeapWriter::buffered_addr_to_source_obj(address buffered_addr) {\n+  OopHandle* oh = _buffer_offset_to_source_obj_table->get(buffered_address_to_offset(buffered_addr));\n+  if (oh != nullptr) {\n+    return oh->resolve();\n+  } else {\n+    return nullptr;\n+  }\n+}\n+\n+Klass* AOTMappedHeapWriter::real_klass_of_buffered_oop(address buffered_addr) {\n+  oop p = buffered_addr_to_source_obj(buffered_addr);\n+  if (p != nullptr) {\n+    return p->klass();\n+  } else if (get_filler_size_at(buffered_addr) > 0) {\n+    return Universe::fillerArrayKlass();\n+  } else {\n+    \/\/ This is one of the root segments\n+    return Universe::objectArrayKlass();\n+  }\n+}\n+\n+size_t AOTMappedHeapWriter::size_of_buffered_oop(address buffered_addr) {\n+  oop p = buffered_addr_to_source_obj(buffered_addr);\n+  if (p != nullptr) {\n+    return p->size();\n+  }\n+\n+  size_t nbytes = get_filler_size_at(buffered_addr);\n+  if (nbytes > 0) {\n+    assert((nbytes % BytesPerWord) == 0, \"should be aligned\");\n+    return nbytes \/ BytesPerWord;\n+  }\n+\n+  address hrs = buffer_bottom();\n+  for (size_t seg_idx = 0; seg_idx < _heap_root_segments.count(); seg_idx++) {\n+    nbytes = _heap_root_segments.size_in_bytes(seg_idx);\n+    if (hrs == buffered_addr) {\n+      assert((nbytes % BytesPerWord) == 0, \"should be aligned\");\n+      return nbytes \/ BytesPerWord;\n+    }\n+    hrs += nbytes;\n+  }\n+\n+  ShouldNotReachHere();\n+  return 0;\n+}\n+\n+address AOTMappedHeapWriter::buffered_addr_to_requested_addr(address buffered_addr) {\n+  return _requested_bottom + buffered_address_to_offset(buffered_addr);\n+}\n+\n+address AOTMappedHeapWriter::requested_address() {\n+  assert(_buffer != nullptr, \"must be initialized\");\n+  return _requested_bottom;\n+}\n+\n+void AOTMappedHeapWriter::allocate_buffer() {\n+  int initial_buffer_size = 100000;\n+  _buffer = new GrowableArrayCHeap<u1, mtClassShared>(initial_buffer_size);\n+  _buffer_used = 0;\n+  ensure_buffer_space(1); \/\/ so that buffer_bottom() works\n+}\n+\n+void AOTMappedHeapWriter::ensure_buffer_space(size_t min_bytes) {\n+  \/\/ We usually have very small heaps. If we get a huge one it's probably caused by a bug.\n+  guarantee(min_bytes <= max_jint, \"we dont support archiving more than 2G of objects\");\n+  _buffer->at_grow(to_array_index(min_bytes));\n+}\n+\n+objArrayOop AOTMappedHeapWriter::allocate_root_segment(size_t offset, int element_count) {\n+  HeapWord* mem = offset_to_buffered_address<HeapWord *>(offset);\n+  memset(mem, 0, refArrayOopDesc::object_size(element_count));\n+\n+  \/\/ The initialization code is copied from MemAllocator::finish and ObjArrayAllocator::initialize.\n+  if (UseCompactObjectHeaders) {\n+    oopDesc::release_set_mark(mem, Universe::objectArrayKlass()->prototype_header());\n+  } else {\n+    assert(!EnableValhalla || Universe::objectArrayKlass()->prototype_header() == markWord::prototype(), \"should be the same\");\n+    oopDesc::set_mark(mem, markWord::prototype());\n+    oopDesc::release_set_klass(mem, Universe::objectArrayKlass());\n+  }\n+  arrayOopDesc::set_length(mem, element_count);\n+  return objArrayOop(cast_to_oop(mem));\n+}\n+\n+void AOTMappedHeapWriter::root_segment_at_put(objArrayOop segment, int index, oop root) {\n+  \/\/ Do not use arrayOop->obj_at_put(i, o) as arrayOop is outside the real heap!\n+  if (UseCompressedOops) {\n+    *segment->obj_at_addr<narrowOop>(index) = CompressedOops::encode(root);\n+  } else {\n+    *segment->obj_at_addr<oop>(index) = root;\n+  }\n+}\n+\n+void AOTMappedHeapWriter::copy_roots_to_buffer(GrowableArrayCHeap<oop, mtClassShared>* roots) {\n+  \/\/ Depending on the number of classes we are archiving, a single roots array may be\n+  \/\/ larger than MIN_GC_REGION_ALIGNMENT. Roots are allocated first in the buffer, which\n+  \/\/ allows us to chop the large array into a series of \"segments\". Current layout\n+  \/\/ starts with zero or more segments exactly fitting MIN_GC_REGION_ALIGNMENT, and end\n+  \/\/ with a single segment that may be smaller than MIN_GC_REGION_ALIGNMENT.\n+  \/\/ This is simple and efficient. We do not need filler objects anywhere between the segments,\n+  \/\/ or immediately after the last segment. This allows starting the object dump immediately\n+  \/\/ after the roots.\n+\n+  assert((_buffer_used % MIN_GC_REGION_ALIGNMENT) == 0,\n+         \"Pre-condition: Roots start at aligned boundary: %zu\", _buffer_used);\n+\n+  int max_elem_count = ((MIN_GC_REGION_ALIGNMENT - arrayOopDesc::header_size_in_bytes()) \/ heapOopSize);\n+  assert(refArrayOopDesc::object_size(max_elem_count)*HeapWordSize == MIN_GC_REGION_ALIGNMENT,\n+         \"Should match exactly\");\n+\n+  HeapRootSegments segments(_buffer_used,\n+                            roots->length(),\n+                            MIN_GC_REGION_ALIGNMENT,\n+                            max_elem_count);\n+\n+  int root_index = 0;\n+  for (size_t seg_idx = 0; seg_idx < segments.count(); seg_idx++) {\n+    int size_elems = segments.size_in_elems(seg_idx);\n+    size_t size_bytes = segments.size_in_bytes(seg_idx);\n+\n+    size_t oop_offset = _buffer_used;\n+    _buffer_used = oop_offset + size_bytes;\n+    ensure_buffer_space(_buffer_used);\n+\n+    assert((oop_offset % MIN_GC_REGION_ALIGNMENT) == 0,\n+           \"Roots segment %zu start is not aligned: %zu\",\n+           segments.count(), oop_offset);\n+\n+    objArrayOop seg_oop = allocate_root_segment(oop_offset, size_elems);\n+    for (int i = 0; i < size_elems; i++) {\n+      root_segment_at_put(seg_oop, i, roots->at(root_index++));\n+    }\n+\n+    log_info(aot, heap)(\"archived obj root segment [%d] = %zu bytes, obj = \" PTR_FORMAT,\n+                        size_elems, size_bytes, p2i(seg_oop));\n+  }\n+\n+  assert(root_index == roots->length(), \"Post-condition: All roots are handled\");\n+\n+  _heap_root_segments = segments;\n+}\n+\n+\/\/ The goal is to sort the objects in increasing order of:\n+\/\/ - objects that have only oop pointers\n+\/\/ - objects that have both native and oop pointers\n+\/\/ - objects that have only native pointers\n+\/\/ - objects that have no pointers\n+static int oop_sorting_rank(oop o) {\n+  bool has_oop_ptr, has_native_ptr;\n+  HeapShared::get_pointer_info(o, has_oop_ptr, has_native_ptr);\n+\n+  if (has_oop_ptr) {\n+    if (!has_native_ptr) {\n+      return 0;\n+    } else {\n+      return 1;\n+    }\n+  } else {\n+    if (has_native_ptr) {\n+      return 2;\n+    } else {\n+      return 3;\n+    }\n+  }\n+}\n+\n+int AOTMappedHeapWriter::compare_objs_by_oop_fields(HeapObjOrder* a, HeapObjOrder* b) {\n+  int rank_a = a->_rank;\n+  int rank_b = b->_rank;\n+\n+  if (rank_a != rank_b) {\n+    return rank_a - rank_b;\n+  } else {\n+    \/\/ If they are the same rank, sort them by their position in the _source_objs array\n+    return a->_index - b->_index;\n+  }\n+}\n+\n+void AOTMappedHeapWriter::sort_source_objs() {\n+  log_info(aot)(\"sorting heap objects\");\n+  int len = _source_objs->length();\n+  _source_objs_order = new GrowableArrayCHeap<HeapObjOrder, mtClassShared>(len);\n+\n+  for (int i = 0; i < len; i++) {\n+    oop o = _source_objs->at(i);\n+    int rank = oop_sorting_rank(o);\n+    HeapObjOrder os = {i, rank};\n+    _source_objs_order->append(os);\n+  }\n+  log_info(aot)(\"computed ranks\");\n+  _source_objs_order->sort(compare_objs_by_oop_fields);\n+  log_info(aot)(\"sorting heap objects done\");\n+}\n+\n+void AOTMappedHeapWriter::copy_source_objs_to_buffer(GrowableArrayCHeap<oop, mtClassShared>* roots) {\n+  \/\/ There could be multiple root segments, which we want to be aligned by region.\n+  \/\/ Putting them ahead of objects makes sure we waste no space.\n+  copy_roots_to_buffer(roots);\n+\n+  sort_source_objs();\n+  for (int i = 0; i < _source_objs_order->length(); i++) {\n+    int src_obj_index = _source_objs_order->at(i)._index;\n+    oop src_obj = _source_objs->at(src_obj_index);\n+    HeapShared::CachedOopInfo* info = HeapShared::get_cached_oop_info(src_obj);\n+    assert(info != nullptr, \"must be\");\n+    size_t buffer_offset = copy_one_source_obj_to_buffer(src_obj);\n+    info->set_buffer_offset(buffer_offset);\n+\n+    OopHandle handle(Universe::vm_global(), src_obj);\n+    _buffer_offset_to_source_obj_table->put_when_absent(buffer_offset, handle);\n+    _buffer_offset_to_source_obj_table->maybe_grow();\n+\n+    if (java_lang_Module::is_instance(src_obj)) {\n+      Modules::check_archived_module_oop(src_obj);\n+    }\n+  }\n+\n+  log_info(aot)(\"Size of heap region = %zu bytes, %d objects, %d roots, %d native ptrs\",\n+                _buffer_used, _source_objs->length() + 1, roots->length(), _num_native_ptrs);\n+}\n+\n+size_t AOTMappedHeapWriter::filler_array_byte_size(int length) {\n+  size_t byte_size = refArrayOopDesc::object_size(length) * HeapWordSize;\n+  return byte_size;\n+}\n+\n+int AOTMappedHeapWriter::filler_array_length(size_t fill_bytes) {\n+  assert(is_object_aligned(fill_bytes), \"must be\");\n+  size_t elemSize = (UseCompressedOops ? sizeof(narrowOop) : sizeof(oop));\n+\n+  int initial_length = to_array_length(fill_bytes \/ elemSize);\n+  for (int length = initial_length; length >= 0; length --) {\n+    size_t array_byte_size = filler_array_byte_size(length);\n+    if (array_byte_size == fill_bytes) {\n+      return length;\n+    }\n+  }\n+\n+  ShouldNotReachHere();\n+  return -1;\n+}\n+\n+HeapWord* AOTMappedHeapWriter::init_filler_array_at_buffer_top(int array_length, size_t fill_bytes) {\n+  assert(UseCompressedClassPointers, \"Archived heap only supported for compressed klasses\");\n+  Klass* oak = Universe::objectArrayKlass(); \/\/ already relocated to point to archived klass\n+  HeapWord* mem = offset_to_buffered_address<HeapWord*>(_buffer_used);\n+  memset(mem, 0, fill_bytes);\n+  narrowKlass nk = ArchiveBuilder::current()->get_requested_narrow_klass(oak);\n+  if (UseCompactObjectHeaders) {\n+    oopDesc::release_set_mark(mem, markWord::prototype().set_narrow_klass(nk));\n+  } else {\n+    oopDesc::set_mark(mem, markWord::prototype());\n+    cast_to_oop(mem)->set_narrow_klass(nk);\n+  }\n+  arrayOopDesc::set_length(mem, array_length);\n+  return mem;\n+}\n+\n+void AOTMappedHeapWriter::maybe_fill_gc_region_gap(size_t required_byte_size) {\n+  \/\/ We fill only with arrays (so we don't need to use a single HeapWord filler if the\n+  \/\/ leftover space is smaller than a zero-sized array object). Therefore, we need to\n+  \/\/ make sure there's enough space of min_filler_byte_size in the current region after\n+  \/\/ required_byte_size has been allocated. If not, fill the remainder of the current\n+  \/\/ region.\n+  size_t min_filler_byte_size = filler_array_byte_size(0);\n+  size_t new_used = _buffer_used + required_byte_size + min_filler_byte_size;\n+\n+  const size_t cur_min_region_bottom = align_down(_buffer_used, MIN_GC_REGION_ALIGNMENT);\n+  const size_t next_min_region_bottom = align_down(new_used, MIN_GC_REGION_ALIGNMENT);\n+\n+  if (cur_min_region_bottom != next_min_region_bottom) {\n+    \/\/ Make sure that no objects span across MIN_GC_REGION_ALIGNMENT. This way\n+    \/\/ we can map the region in any region-based collector.\n+    assert(next_min_region_bottom > cur_min_region_bottom, \"must be\");\n+    assert(next_min_region_bottom - cur_min_region_bottom == MIN_GC_REGION_ALIGNMENT,\n+           \"no buffered object can be larger than %d bytes\",  MIN_GC_REGION_ALIGNMENT);\n+\n+    const size_t filler_end = next_min_region_bottom;\n+    const size_t fill_bytes = filler_end - _buffer_used;\n+    assert(fill_bytes > 0, \"must be\");\n+    ensure_buffer_space(filler_end);\n+\n+    int array_length = filler_array_length(fill_bytes);\n+    log_info(aot, heap)(\"Inserting filler obj array of %d elements (%zu bytes total) @ buffer offset %zu\",\n+                        array_length, fill_bytes, _buffer_used);\n+    HeapWord* filler = init_filler_array_at_buffer_top(array_length, fill_bytes);\n+    _buffer_used = filler_end;\n+    _fillers->put(buffered_address_to_offset((address)filler), fill_bytes);\n+  }\n+}\n+\n+size_t AOTMappedHeapWriter::get_filler_size_at(address buffered_addr) {\n+  size_t* p = _fillers->get(buffered_address_to_offset(buffered_addr));\n+  if (p != nullptr) {\n+    assert(*p > 0, \"filler must be larger than zero bytes\");\n+    return *p;\n+  } else {\n+    return 0; \/\/ buffered_addr is not a filler\n+  }\n+}\n+\n+template <typename T>\n+void update_buffered_object_field(address buffered_obj, int field_offset, T value) {\n+  T* field_addr = cast_to_oop(buffered_obj)->field_addr<T>(field_offset);\n+  *field_addr = value;\n+}\n+\n+size_t AOTMappedHeapWriter::copy_one_source_obj_to_buffer(oop src_obj) {\n+  assert(!is_too_large_to_archive(src_obj), \"already checked\");\n+  size_t byte_size = src_obj->size() * HeapWordSize;\n+  assert(byte_size > 0, \"no zero-size objects\");\n+\n+  \/\/ For region-based collectors such as G1, the archive heap may be mapped into\n+  \/\/ multiple regions. We need to make sure that we don't have an object that can possible\n+  \/\/ span across two regions.\n+  maybe_fill_gc_region_gap(byte_size);\n+\n+  size_t new_used = _buffer_used + byte_size;\n+  assert(new_used > _buffer_used, \"no wrap around\");\n+\n+  size_t cur_min_region_bottom = align_down(_buffer_used, MIN_GC_REGION_ALIGNMENT);\n+  size_t next_min_region_bottom = align_down(new_used, MIN_GC_REGION_ALIGNMENT);\n+  assert(cur_min_region_bottom == next_min_region_bottom, \"no object should cross minimal GC region boundaries\");\n+\n+  ensure_buffer_space(new_used);\n+\n+  address from = cast_from_oop<address>(src_obj);\n+  address to = offset_to_buffered_address<address>(_buffer_used);\n+  assert(is_object_aligned(_buffer_used), \"sanity\");\n+  assert(is_object_aligned(byte_size), \"sanity\");\n+  memcpy(to, from, byte_size);\n+\n+  \/\/ These native pointers will be restored explicitly at run time.\n+  if (java_lang_Module::is_instance(src_obj)) {\n+    update_buffered_object_field<ModuleEntry*>(to, java_lang_Module::module_entry_offset(), nullptr);\n+  } else if (java_lang_ClassLoader::is_instance(src_obj)) {\n+#ifdef ASSERT\n+    \/\/ We only archive these loaders\n+    if (src_obj != SystemDictionary::java_platform_loader() &&\n+        src_obj != SystemDictionary::java_system_loader()) {\n+      assert(src_obj->klass()->name()->equals(\"jdk\/internal\/loader\/ClassLoaders$BootClassLoader\"), \"must be\");\n+    }\n+#endif\n+    update_buffered_object_field<ClassLoaderData*>(to, java_lang_ClassLoader::loader_data_offset(), nullptr);\n+  }\n+\n+  size_t buffered_obj_offset = _buffer_used;\n+  _buffer_used = new_used;\n+\n+  return buffered_obj_offset;\n+}\n+\n+void AOTMappedHeapWriter::set_requested_address(ArchiveMappedHeapInfo* info) {\n+  assert(!info->is_used(), \"only set once\");\n+\n+  size_t heap_region_byte_size = _buffer_used;\n+  assert(heap_region_byte_size > 0, \"must archived at least one object!\");\n+\n+  if (UseCompressedOops) {\n+    if (UseG1GC) {\n+      address heap_end = (address)G1CollectedHeap::heap()->reserved().end();\n+      log_info(aot, heap)(\"Heap end = %p\", heap_end);\n+      _requested_bottom = align_down(heap_end - heap_region_byte_size, G1HeapRegion::GrainBytes);\n+      _requested_bottom = align_down(_requested_bottom, MIN_GC_REGION_ALIGNMENT);\n+      assert(is_aligned(_requested_bottom, G1HeapRegion::GrainBytes), \"sanity\");\n+    } else {\n+      _requested_bottom = align_up(CompressedOops::begin(), MIN_GC_REGION_ALIGNMENT);\n+    }\n+  } else {\n+    \/\/ We always write the objects as if the heap started at this address. This\n+    \/\/ makes the contents of the archive heap deterministic.\n+    \/\/\n+    \/\/ Note that at runtime, the heap address is selected by the OS, so the archive\n+    \/\/ heap will not be mapped at 0x10000000, and the contents need to be patched.\n+    _requested_bottom = align_up((address)NOCOOPS_REQUESTED_BASE, MIN_GC_REGION_ALIGNMENT);\n+  }\n+\n+  assert(is_aligned(_requested_bottom, MIN_GC_REGION_ALIGNMENT), \"sanity\");\n+\n+  _requested_top = _requested_bottom + _buffer_used;\n+\n+  info->set_buffer_region(MemRegion(offset_to_buffered_address<HeapWord*>(0),\n+                                    offset_to_buffered_address<HeapWord*>(_buffer_used)));\n+  info->set_root_segments(_heap_root_segments);\n+}\n+\n+\/\/ Oop relocation\n+\n+template <typename T> T* AOTMappedHeapWriter::requested_addr_to_buffered_addr(T* p) {\n+  assert(is_in_requested_range(cast_to_oop(p)), \"must be\");\n+\n+  address addr = address(p);\n+  assert(addr >= _requested_bottom, \"must be\");\n+  size_t offset = addr - _requested_bottom;\n+  return offset_to_buffered_address<T*>(offset);\n+}\n+\n+template <typename T> oop AOTMappedHeapWriter::load_source_oop_from_buffer(T* buffered_addr) {\n+  oop o = load_oop_from_buffer(buffered_addr);\n+  assert(!in_buffer(cast_from_oop<address>(o)), \"must point to source oop\");\n+  return o;\n+}\n+\n+template <typename T> void AOTMappedHeapWriter::store_requested_oop_in_buffer(T* buffered_addr,\n+                                                                                   oop request_oop) {\n+  assert(request_oop == nullptr || is_in_requested_range(request_oop), \"must be\");\n+  store_oop_in_buffer(buffered_addr, request_oop);\n+}\n+\n+inline void AOTMappedHeapWriter::store_oop_in_buffer(oop* buffered_addr, oop requested_obj) {\n+  *buffered_addr = requested_obj;\n+}\n+\n+inline void AOTMappedHeapWriter::store_oop_in_buffer(narrowOop* buffered_addr, oop requested_obj) {\n+  narrowOop val = CompressedOops::encode(requested_obj);\n+  *buffered_addr = val;\n+}\n+\n+oop AOTMappedHeapWriter::load_oop_from_buffer(oop* buffered_addr) {\n+  return *buffered_addr;\n+}\n+\n+oop AOTMappedHeapWriter::load_oop_from_buffer(narrowOop* buffered_addr) {\n+  return CompressedOops::decode(*buffered_addr);\n+}\n+\n+template <typename T> void AOTMappedHeapWriter::relocate_field_in_buffer(T* field_addr_in_buffer, oop source_referent, CHeapBitMap* oopmap) {\n+  oop request_referent = source_obj_to_requested_obj(source_referent);\n+  store_requested_oop_in_buffer<T>(field_addr_in_buffer, request_referent);\n+  if (request_referent != nullptr) {\n+    mark_oop_pointer<T>(field_addr_in_buffer, oopmap);\n+  }\n+}\n+\n+template <typename T> void AOTMappedHeapWriter::mark_oop_pointer(T* buffered_addr, CHeapBitMap* oopmap) {\n+  T* request_p = (T*)(buffered_addr_to_requested_addr((address)buffered_addr));\n+  address requested_region_bottom;\n+\n+  assert(request_p >= (T*)_requested_bottom, \"sanity\");\n+  assert(request_p <  (T*)_requested_top, \"sanity\");\n+  requested_region_bottom = _requested_bottom;\n+\n+  \/\/ Mark the pointer in the oopmap\n+  T* region_bottom = (T*)requested_region_bottom;\n+  assert(request_p >= region_bottom, \"must be\");\n+  BitMap::idx_t idx = request_p - region_bottom;\n+  assert(idx < oopmap->size(), \"overflow\");\n+  oopmap->set_bit(idx);\n+}\n+\n+void AOTMappedHeapWriter::update_header_for_requested_obj(oop requested_obj, oop src_obj,  Klass* src_klass) {\n+  assert(UseCompressedClassPointers, \"Archived heap only supported for compressed klasses\");\n+  narrowKlass nk = ArchiveBuilder::current()->get_requested_narrow_klass(src_klass);\n+  address buffered_addr = requested_addr_to_buffered_addr(cast_from_oop<address>(requested_obj));\n+\n+  oop fake_oop = cast_to_oop(buffered_addr);\n+  if (UseCompactObjectHeaders) {\n+    fake_oop->set_mark(markWord::prototype().set_narrow_klass(nk));\n+  } else {\n+    fake_oop->set_narrow_klass(nk);\n+  }\n+\n+  if (src_obj == nullptr) {\n+    return;\n+  }\n+  \/\/ We need to retain the identity_hash, because it may have been used by some hashtables\n+  \/\/ in the shared heap.\n+  if (!src_obj->fast_no_hash_check() && (!(EnableValhalla && src_obj->mark().is_inline_type()))) {\n+    intptr_t src_hash = src_obj->identity_hash();\n+    if (UseCompactObjectHeaders) {\n+      fake_oop->set_mark(markWord::prototype().set_narrow_klass(nk).copy_set_hash(src_hash));\n+    } else if (EnableValhalla) {\n+      fake_oop->set_mark(src_klass->prototype_header().copy_set_hash(src_hash));\n+    } else {\n+      fake_oop->set_mark(markWord::prototype().copy_set_hash(src_hash));\n+    }\n+    assert(fake_oop->mark().is_unlocked(), \"sanity\");\n+\n+    DEBUG_ONLY(intptr_t archived_hash = fake_oop->identity_hash());\n+    assert(src_hash == archived_hash, \"Different hash codes: original \" INTPTR_FORMAT \", archived \" INTPTR_FORMAT, src_hash, archived_hash);\n+  }\n+  \/\/ Strip age bits.\n+  fake_oop->set_mark(fake_oop->mark().set_age(0));\n+}\n+\n+class AOTMappedHeapWriter::EmbeddedOopRelocator: public BasicOopIterateClosure {\n+  oop _src_obj;\n+  address _buffered_obj;\n+  CHeapBitMap* _oopmap;\n+  bool _is_java_lang_ref;\n+public:\n+  EmbeddedOopRelocator(oop src_obj, address buffered_obj, CHeapBitMap* oopmap) :\n+    _src_obj(src_obj), _buffered_obj(buffered_obj), _oopmap(oopmap)\n+  {\n+    _is_java_lang_ref = AOTReferenceObjSupport::check_if_ref_obj(src_obj);\n+  }\n+\n+  void do_oop(narrowOop *p) { EmbeddedOopRelocator::do_oop_work(p); }\n+  void do_oop(      oop *p) { EmbeddedOopRelocator::do_oop_work(p); }\n+\n+private:\n+  template <class T> void do_oop_work(T *p) {\n+    int field_offset = pointer_delta_as_int((char*)p, cast_from_oop<char*>(_src_obj));\n+    T* field_addr = (T*)(_buffered_obj + field_offset);\n+    oop referent = load_source_oop_from_buffer<T>(field_addr);\n+    referent = HeapShared::maybe_remap_referent(_is_java_lang_ref, field_offset, referent);\n+    AOTMappedHeapWriter::relocate_field_in_buffer<T>(field_addr, referent, _oopmap);\n+  }\n+};\n+\n+static void log_bitmap_usage(const char* which, BitMap* bitmap, size_t total_bits) {\n+  \/\/ The whole heap is covered by total_bits, but there are only non-zero bits within [start ... end).\n+  size_t start = bitmap->find_first_set_bit(0);\n+  size_t end = bitmap->size();\n+  log_info(aot)(\"%s = %7zu ... %7zu (%3zu%% ... %3zu%% = %3zu%%)\", which,\n+                start, end,\n+                start * 100 \/ total_bits,\n+                end * 100 \/ total_bits,\n+                (end - start) * 100 \/ total_bits);\n+}\n+\n+\/\/ Update all oop fields embedded in the buffered objects\n+void AOTMappedHeapWriter::relocate_embedded_oops(GrowableArrayCHeap<oop, mtClassShared>* roots,\n+                                                      ArchiveMappedHeapInfo* heap_info) {\n+  size_t oopmap_unit = (UseCompressedOops ? sizeof(narrowOop) : sizeof(oop));\n+  size_t heap_region_byte_size = _buffer_used;\n+  heap_info->oopmap()->resize(heap_region_byte_size   \/ oopmap_unit);\n+\n+  for (int i = 0; i < _source_objs_order->length(); i++) {\n+    int src_obj_index = _source_objs_order->at(i)._index;\n+    oop src_obj = _source_objs->at(src_obj_index);\n+    HeapShared::CachedOopInfo* info = HeapShared::get_cached_oop_info(src_obj);\n+    assert(info != nullptr, \"must be\");\n+    oop requested_obj = requested_obj_from_buffer_offset(info->buffer_offset());\n+    update_header_for_requested_obj(requested_obj, src_obj, src_obj->klass());\n+    address buffered_obj = offset_to_buffered_address<address>(info->buffer_offset());\n+    EmbeddedOopRelocator relocator(src_obj, buffered_obj, heap_info->oopmap());\n+    src_obj->oop_iterate(&relocator);\n+    mark_native_pointers(src_obj);\n+  };\n+\n+  \/\/ Relocate HeapShared::roots(), which is created in copy_roots_to_buffer() and\n+  \/\/ doesn't have a corresponding src_obj, so we can't use EmbeddedOopRelocator on it.\n+  for (size_t seg_idx = 0; seg_idx < _heap_root_segments.count(); seg_idx++) {\n+    size_t seg_offset = _heap_root_segments.segment_offset(seg_idx);\n+\n+    objArrayOop requested_obj = (objArrayOop)requested_obj_from_buffer_offset(seg_offset);\n+    update_header_for_requested_obj(requested_obj, nullptr, Universe::objectArrayKlass());\n+    address buffered_obj = offset_to_buffered_address<address>(seg_offset);\n+    int length = _heap_root_segments.size_in_elems(seg_idx);\n+\n+    size_t elem_size = UseCompressedOops ? sizeof(narrowOop) : sizeof(oop);\n+\n+    for (int i = 0; i < length; i++) {\n+      \/\/ There is no source object; these are native oops - load, translate and\n+      \/\/ write back\n+      size_t elem_offset = objArrayOopDesc::base_offset_in_bytes() + elem_size * i;\n+      HeapWord* elem_addr = (HeapWord*)(buffered_obj + elem_offset);\n+      oop obj = NativeAccess<>::oop_load(elem_addr);\n+      obj = HeapShared::maybe_remap_referent(false \/* is_reference_field *\/, elem_offset, obj);\n+      if (UseCompressedOops) {\n+        relocate_field_in_buffer<narrowOop>((narrowOop*)elem_addr, obj, heap_info->oopmap());\n+      } else {\n+        relocate_field_in_buffer<oop>((oop*)elem_addr, obj, heap_info->oopmap());\n+      }\n+    }\n+  }\n+\n+  compute_ptrmap(heap_info);\n+\n+  size_t total_bytes = (size_t)_buffer->length();\n+  log_bitmap_usage(\"oopmap\", heap_info->oopmap(), total_bytes \/ (UseCompressedOops ? sizeof(narrowOop) : sizeof(oop)));\n+  log_bitmap_usage(\"ptrmap\", heap_info->ptrmap(), total_bytes \/ sizeof(address));\n+}\n+\n+void AOTMappedHeapWriter::mark_native_pointer(oop src_obj, int field_offset) {\n+  Metadata* ptr = src_obj->metadata_field_acquire(field_offset);\n+  if (ptr != nullptr) {\n+    NativePointerInfo info;\n+    info._src_obj = src_obj;\n+    info._field_offset = field_offset;\n+    _native_pointers->append(info);\n+    HeapShared::set_has_native_pointers(src_obj);\n+    _num_native_ptrs ++;\n+  }\n+}\n+\n+void AOTMappedHeapWriter::mark_native_pointers(oop orig_obj) {\n+  HeapShared::do_metadata_offsets(orig_obj, [&](int offset) {\n+    mark_native_pointer(orig_obj, offset);\n+  });\n+}\n+\n+void AOTMappedHeapWriter::compute_ptrmap(ArchiveMappedHeapInfo* heap_info) {\n+  int num_non_null_ptrs = 0;\n+  Metadata** bottom = (Metadata**) _requested_bottom;\n+  Metadata** top = (Metadata**) _requested_top; \/\/ exclusive\n+  heap_info->ptrmap()->resize(top - bottom);\n+\n+  BitMap::idx_t max_idx = 32; \/\/ paranoid - don't make it too small\n+  for (int i = 0; i < _native_pointers->length(); i++) {\n+    NativePointerInfo info = _native_pointers->at(i);\n+    oop src_obj = info._src_obj;\n+    int field_offset = info._field_offset;\n+    HeapShared::CachedOopInfo* p = HeapShared::get_cached_oop_info(src_obj);\n+    \/\/ requested_field_addr = the address of this field in the requested space\n+    oop requested_obj = requested_obj_from_buffer_offset(p->buffer_offset());\n+    Metadata** requested_field_addr = (Metadata**)(cast_from_oop<address>(requested_obj) + field_offset);\n+    assert(bottom <= requested_field_addr && requested_field_addr < top, \"range check\");\n+\n+    \/\/ Mark this field in the bitmap\n+    BitMap::idx_t idx = requested_field_addr - bottom;\n+    heap_info->ptrmap()->set_bit(idx);\n+    num_non_null_ptrs ++;\n+    max_idx = MAX2(max_idx, idx);\n+\n+    \/\/ Set the native pointer to the requested address of the metadata (at runtime, the metadata will have\n+    \/\/ this address if the RO\/RW regions are mapped at the default location).\n+\n+    Metadata** buffered_field_addr = requested_addr_to_buffered_addr(requested_field_addr);\n+    Metadata* native_ptr = *buffered_field_addr;\n+    guarantee(native_ptr != nullptr, \"sanity\");\n+\n+    if (RegeneratedClasses::has_been_regenerated(native_ptr)) {\n+      native_ptr = RegeneratedClasses::get_regenerated_object(native_ptr);\n+    }\n+\n+    guarantee(ArchiveBuilder::current()->has_been_archived((address)native_ptr),\n+              \"Metadata %p should have been archived\", native_ptr);\n+\n+    address buffered_native_ptr = ArchiveBuilder::current()->get_buffered_addr((address)native_ptr);\n+    address requested_native_ptr = ArchiveBuilder::current()->to_requested(buffered_native_ptr);\n+    *buffered_field_addr = (Metadata*)requested_native_ptr;\n+  }\n+\n+  heap_info->ptrmap()->resize(max_idx + 1);\n+  log_info(aot, heap)(\"calculate_ptrmap: marked %d non-null native pointers for heap region (%zu bits)\",\n+                      num_non_null_ptrs, size_t(heap_info->ptrmap()->size()));\n+}\n+\n+AOTMapLogger::OopDataIterator* AOTMappedHeapWriter::oop_iterator(ArchiveMappedHeapInfo* heap_info) {\n+  class MappedWriterOopIterator : public AOTMapLogger::OopDataIterator {\n+  private:\n+    address _current;\n+    address _next;\n+\n+    address _buffer_start;\n+    address _buffer_end;\n+    uint64_t _buffer_start_narrow_oop;\n+    intptr_t _buffer_to_requested_delta;\n+    int _requested_shift;\n+\n+    size_t _num_root_segments;\n+    size_t _num_obj_arrays_logged;\n+\n+  public:\n+    MappedWriterOopIterator(address buffer_start,\n+                            address buffer_end,\n+                            uint64_t buffer_start_narrow_oop,\n+                            intptr_t buffer_to_requested_delta,\n+                            int requested_shift,\n+                            size_t num_root_segments)\n+      : _current(nullptr),\n+        _next(buffer_start),\n+        _buffer_start(buffer_start),\n+        _buffer_end(buffer_end),\n+        _buffer_start_narrow_oop(buffer_start_narrow_oop),\n+        _buffer_to_requested_delta(buffer_to_requested_delta),\n+        _requested_shift(requested_shift),\n+        _num_root_segments(num_root_segments),\n+        _num_obj_arrays_logged(0) {\n+    }\n+\n+    AOTMapLogger::OopData capture(address buffered_addr) {\n+      oopDesc* raw_oop = (oopDesc*)buffered_addr;\n+      size_t size = size_of_buffered_oop(buffered_addr);\n+      address requested_addr = buffered_addr_to_requested_addr(buffered_addr);\n+      intptr_t target_location = (intptr_t)requested_addr;\n+      uint64_t pd = (uint64_t)(pointer_delta(buffered_addr, _buffer_start, 1));\n+      uint32_t narrow_location = checked_cast<uint32_t>(_buffer_start_narrow_oop + (pd >> _requested_shift));\n+      Klass* klass = real_klass_of_buffered_oop(buffered_addr);\n+\n+      return { buffered_addr,\n+               requested_addr,\n+               target_location,\n+               narrow_location,\n+               raw_oop,\n+               klass,\n+               size,\n+               false };\n+    }\n+\n+    bool has_next() override {\n+      return _next < _buffer_end;\n+    }\n+\n+    AOTMapLogger::OopData next() override {\n+      _current = _next;\n+      AOTMapLogger::OopData result = capture(_current);\n+      if (result._klass->is_objArray_klass()) {\n+        result._is_root_segment = _num_obj_arrays_logged++ < _num_root_segments;\n+      }\n+      _next = _current + result._size * BytesPerWord;\n+      return result;\n+    }\n+\n+    AOTMapLogger::OopData obj_at(narrowOop* addr) override {\n+      uint64_t n = (uint64_t)(*addr);\n+      if (n == 0) {\n+        return null_data();\n+      } else {\n+        precond(n >= _buffer_start_narrow_oop);\n+        address buffer_addr = _buffer_start + ((n - _buffer_start_narrow_oop) << _requested_shift);\n+        return capture(buffer_addr);\n+      }\n+    }\n+\n+    AOTMapLogger::OopData obj_at(oop* addr) override {\n+      address requested_value = cast_from_oop<address>(*addr);\n+      if (requested_value == nullptr) {\n+        return null_data();\n+      } else {\n+        address buffer_addr = requested_value - _buffer_to_requested_delta;\n+        return capture(buffer_addr);\n+      }\n+    }\n+\n+    GrowableArrayCHeap<AOTMapLogger::OopData, mtClass>* roots() override {\n+      return new GrowableArrayCHeap<AOTMapLogger::OopData, mtClass>();\n+    }\n+  };\n+\n+  MemRegion r = heap_info->buffer_region();\n+  address buffer_start = address(r.start());\n+  address buffer_end = address(r.end());\n+\n+  address requested_base = UseCompressedOops ? (address)CompressedOops::base() : (address)AOTMappedHeapWriter::NOCOOPS_REQUESTED_BASE;\n+  address requested_start = UseCompressedOops ? buffered_addr_to_requested_addr(buffer_start) : requested_base;\n+  int requested_shift =  CompressedOops::shift();\n+  intptr_t buffer_to_requested_delta = requested_start - buffer_start;\n+  uint64_t buffer_start_narrow_oop = 0xdeadbeed;\n+  if (UseCompressedOops) {\n+    buffer_start_narrow_oop = (uint64_t)(pointer_delta(requested_start, requested_base, 1)) >> requested_shift;\n+    assert(buffer_start_narrow_oop < 0xffffffff, \"sanity\");\n+  }\n+\n+  return new MappedWriterOopIterator(buffer_start,\n+                                     buffer_end,\n+                                     buffer_start_narrow_oop,\n+                                     buffer_to_requested_delta,\n+                                     requested_shift,\n+                                     heap_info->root_segments().count());\n+}\n+\n+#endif \/\/ INCLUDE_CDS_JAVA_HEAP\n","filename":"src\/hotspot\/share\/cds\/aotMappedHeapWriter.cpp","additions":942,"deletions":0,"binary":false,"changes":942,"status":"added"},{"patch":"@@ -33,0 +33,1 @@\n+#include \"cds\/aotMappedHeapLoader.hpp\"\n@@ -36,2 +37,0 @@\n-#include \"cds\/archiveHeapLoader.hpp\"\n-#include \"cds\/archiveHeapWriter.hpp\"\n@@ -48,1 +47,1 @@\n-#include \"cds\/heapShared.hpp\"\n+#include \"cds\/heapShared.inline.hpp\"\n@@ -344,1 +343,4 @@\n-    static_mapinfo->unmap_region(AOTMetaspace::bm);\n+\n+    if (HeapShared::is_loading() && HeapShared::is_loading_mapping_mode()) {\n+      static_mapinfo->unmap_region(AOTMetaspace::bm);\n+    }\n@@ -400,1 +402,1 @@\n-        if (ArchiveHeapWriter::is_string_too_large_to_archive(str)) {\n+        if (HeapShared::is_string_too_large_to_archive(str)) {\n@@ -643,1 +645,2 @@\n-  ArchiveHeapInfo _heap_info;\n+  ArchiveMappedHeapInfo _mapped_heap_info;\n+  ArchiveStreamedHeapInfo _streamed_heap_info;\n@@ -658,1 +661,1 @@\n-    VM_Operation(), _heap_info(), _map_info(nullptr), _builder(b) {}\n+    VM_Operation(), _mapped_heap_info(), _streamed_heap_info(), _map_info(nullptr), _builder(b) {}\n@@ -663,1 +666,2 @@\n-  ArchiveHeapInfo* heap_info()  { return &_heap_info; }\n+  ArchiveMappedHeapInfo* mapped_heap_info()  { return &_mapped_heap_info; }\n+  ArchiveStreamedHeapInfo* streamed_heap_info()  { return &_streamed_heap_info; }\n@@ -1105,2 +1109,1 @@\n-    ArchiveHeapWriter::init();\n-\n+    HeapShared::init_heap_writer();\n@@ -1129,3 +1132,5 @@\n-    \/\/ Do this at the very end, when no Java code will be executed. Otherwise\n-    \/\/ some new strings may be added to the intern table.\n-    StringTable::allocate_shared_strings_array(CHECK);\n+    if (HeapShared::is_writing_mapping_mode()) {\n+      \/\/ Do this at the very end, when no Java code will be executed. Otherwise\n+      \/\/ some new strings may be added to the intern table.\n+      StringTable::allocate_shared_strings_array(CHECK);\n+    }\n@@ -1152,1 +1157,1 @@\n-  bool status = write_static_archive(&builder, op.map_info(), op.heap_info());\n+  bool status = write_static_archive(&builder, op.map_info(), op.mapped_heap_info(), op.streamed_heap_info());\n@@ -1166,1 +1171,4 @@\n-bool AOTMetaspace::write_static_archive(ArchiveBuilder* builder, FileMapInfo* map_info, ArchiveHeapInfo* heap_info) {\n+bool AOTMetaspace::write_static_archive(ArchiveBuilder* builder,\n+                                        FileMapInfo* map_info,\n+                                        ArchiveMappedHeapInfo* mapped_heap_info,\n+                                        ArchiveStreamedHeapInfo* streamed_heap_info) {\n@@ -1175,1 +1183,1 @@\n-  builder->write_archive(map_info, heap_info);\n+  builder->write_archive(map_info, mapped_heap_info, streamed_heap_info);\n@@ -1354,1 +1362,1 @@\n-    HeapShared::write_heap(&_heap_info);\n+    HeapShared::write_heap(&_mapped_heap_info, &_streamed_heap_info);\n@@ -1756,3 +1764,23 @@\n-      \/\/ map_or_load_heap_region() compares the current narrow oop and klass encodings\n-      \/\/ with the archived ones, so it must be done after all encodings are determined.\n-      static_mapinfo->map_or_load_heap_region();\n+      if (static_mapinfo->can_use_heap_region()) {\n+        if (static_mapinfo->object_streaming_mode()) {\n+          HeapShared::initialize_loading_mode(HeapArchiveMode::_streaming);\n+        } else {\n+          \/\/ map_or_load_heap_region() compares the current narrow oop and klass encodings\n+          \/\/ with the archived ones, so it must be done after all encodings are determined.\n+          static_mapinfo->map_or_load_heap_region();\n+          HeapShared::initialize_loading_mode(HeapArchiveMode::_mapping);\n+        }\n+      } else {\n+        FileMapRegion* r = static_mapinfo->region_at(AOTMetaspace::hp);\n+        if (r->used() > 0) {\n+          if (static_mapinfo->object_streaming_mode()) {\n+            AOTMetaspace::report_loading_error(\"Cannot use CDS heap data.\");\n+          } else {\n+            if (!UseCompressedOops && !AOTMappedHeapLoader::can_map()) {\n+              AOTMetaspace::report_loading_error(\"Cannot use CDS heap data. Selected GC not compatible -XX:-UseCompressedOops\");\n+            } else {\n+              AOTMetaspace::report_loading_error(\"Cannot use CDS heap data. UseEpsilonGC, UseG1GC, UseSerialGC, UseParallelGC, or UseShenandoahGC are required.\");\n+            }\n+          }\n+        }\n+      }\n@@ -2067,1 +2095,1 @@\n-   int _count;\n+   size_t _count;\n@@ -2073,1 +2101,1 @@\n-  int total() { return _count; }\n+  size_t total() { return _count; }\n@@ -2091,4 +2119,3 @@\n-  \/\/ Finish up archived heap initialization. These must be\n-  \/\/ done after ReadClosure.\n-  static_mapinfo->patch_heap_embedded_pointers();\n-  ArchiveHeapLoader::finish_initialization();\n+  \/\/ Finish initializing the heap dump mode used in the archive\n+  \/\/ Heap initialization can be done only after vtables are initialized by ReadClosure.\n+  HeapShared::finalize_initialization(static_mapinfo);\n@@ -2096,0 +2123,1 @@\n+\n@@ -2147,2 +2175,4 @@\n-    tty->print_cr(\"Number of shared symbols: %d\", cl.total());\n-    tty->print_cr(\"Number of shared strings: %zu\", StringTable::shared_entry_count());\n+    tty->print_cr(\"Number of shared symbols: %zu\", cl.total());\n+    if (HeapShared::is_loading_mapping_mode()) {\n+      tty->print_cr(\"Number of shared strings: %zu\", StringTable::shared_entry_count());\n+    }\n","filename":"src\/hotspot\/share\/cds\/aotMetaspace.cpp","additions":58,"deletions":28,"binary":false,"changes":86,"status":"modified"},{"patch":"@@ -31,1 +31,0 @@\n-#include \"cds\/archiveHeapWriter.hpp\"\n@@ -1184,1 +1183,1 @@\n-void ArchiveBuilder::write_archive(FileMapInfo* mapinfo, ArchiveHeapInfo* heap_info) {\n+void ArchiveBuilder::write_archive(FileMapInfo* mapinfo, ArchiveMappedHeapInfo* mapped_heap_info, ArchiveStreamedHeapInfo* streamed_heap_info) {\n@@ -1189,0 +1188,2 @@\n+  ResourceMark rm;\n+\n@@ -1197,1 +1198,4 @@\n-  char* bitmap = mapinfo->write_bitmap_region(ArchivePtrMarker::rw_ptrmap(), ArchivePtrMarker::ro_ptrmap(), heap_info,\n+  char* bitmap = mapinfo->write_bitmap_region(ArchivePtrMarker::rw_ptrmap(),\n+                                              ArchivePtrMarker::ro_ptrmap(),\n+                                              mapped_heap_info,\n+                                              streamed_heap_info,\n@@ -1200,2 +1204,4 @@\n-  if (heap_info->is_used()) {\n-    _total_heap_region_size = mapinfo->write_heap_region(heap_info);\n+  if (mapped_heap_info != nullptr && mapped_heap_info->is_used()) {\n+    _total_heap_region_size = mapinfo->write_mapped_heap_region(mapped_heap_info);\n+  } else if (streamed_heap_info != nullptr && streamed_heap_info->is_used()) {\n+    _total_heap_region_size = mapinfo->write_streamed_heap_region(streamed_heap_info);\n@@ -1204,1 +1210,1 @@\n-  print_region_stats(mapinfo, heap_info);\n+  print_region_stats(mapinfo, mapped_heap_info, streamed_heap_info);\n@@ -1219,1 +1225,1 @@\n-    AOTMapLogger::dumptime_log(this, mapinfo, heap_info, bitmap, bitmap_size_in_bytes);\n+    AOTMapLogger::dumptime_log(this, mapinfo, mapped_heap_info, streamed_heap_info, bitmap, bitmap_size_in_bytes);\n@@ -1235,1 +1241,3 @@\n-void ArchiveBuilder::print_region_stats(FileMapInfo *mapinfo, ArchiveHeapInfo* heap_info) {\n+void ArchiveBuilder::print_region_stats(FileMapInfo *mapinfo,\n+                                        ArchiveMappedHeapInfo* mapped_heap_info,\n+                                        ArchiveStreamedHeapInfo* streamed_heap_info) {\n@@ -1253,2 +1261,4 @@\n-  if (heap_info->is_used()) {\n-    print_heap_region_stats(heap_info, total_reserved);\n+  if (mapped_heap_info != nullptr && mapped_heap_info->is_used()) {\n+    print_heap_region_stats(mapped_heap_info->buffer_start(), mapped_heap_info->buffer_byte_size(), total_reserved);\n+  } else if (streamed_heap_info != nullptr && streamed_heap_info->is_used()) {\n+    print_heap_region_stats(streamed_heap_info->buffer_start(), streamed_heap_info->buffer_byte_size(), total_reserved);\n@@ -1258,1 +1268,1 @@\n-                 total_bytes, total_reserved, total_u_perc);\n+                     total_bytes, total_reserved, total_u_perc);\n@@ -1263,1 +1273,1 @@\n-                 size, size\/double(total_size)*100.0, size);\n+                     size, size\/double(total_size)*100.0, size);\n@@ -1266,3 +1276,1 @@\n-void ArchiveBuilder::print_heap_region_stats(ArchiveHeapInfo *info, size_t total_size) {\n-  char* start = info->buffer_start();\n-  size_t size = info->buffer_byte_size();\n+void ArchiveBuilder::print_heap_region_stats(char* start, size_t size, size_t total_size) {\n","filename":"src\/hotspot\/share\/cds\/archiveBuilder.cpp","additions":23,"deletions":15,"binary":false,"changes":38,"status":"modified"},{"patch":"@@ -28,1 +28,0 @@\n-#include \"cds\/archiveHeapLoader.inline.hpp\"\n","filename":"src\/hotspot\/share\/cds\/archiveUtils.cpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -27,1 +27,0 @@\n-#include \"cds\/archiveHeapLoader.hpp\"\n@@ -31,1 +30,1 @@\n-#include \"cds\/heapShared.hpp\"\n+#include \"cds\/heapShared.inline.hpp\"\n@@ -917,5 +916,0 @@\n-  \/\/ Almost all GCs support heap region dump, except ZGC (so far).\n-  if (UseZGC) {\n-    return \"ZGC is not supported\";\n-  }\n-\n@@ -997,1 +991,1 @@\n-  return ArchiveHeapLoader::is_in_use();\n+  return HeapShared::is_archived_heap_in_use();\n@@ -1009,1 +1003,1 @@\n-  if (is_using_archive() && ArchiveHeapLoader::can_use()) {\n+  if (is_using_archive() && HeapShared::can_use_archived_heap()) {\n","filename":"src\/hotspot\/share\/cds\/cdsConfig.cpp","additions":3,"deletions":9,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -25,2 +25,1 @@\n-#include \"cds\/archiveHeapLoader.hpp\"\n-#include \"cds\/heapShared.hpp\"\n+#include \"cds\/heapShared.inline.hpp\"\n@@ -115,1 +114,1 @@\n-  if (!ArchiveHeapLoader::is_in_use()) {\n+  if (!HeapShared::is_archived_heap_in_use()) {\n@@ -127,1 +126,0 @@\n-  oop mirror = k->java_mirror();\n@@ -134,1 +132,2 @@\n-      mirror->obj_field_put(fd.offset(), HeapShared::get_root(root_index, \/*clear=*\/true));\n+      oop root_object = HeapShared::get_root(root_index, \/*clear=*\/true);\n+      k->java_mirror()->obj_field_put(fd.offset(), root_object);\n","filename":"src\/hotspot\/share\/cds\/cdsEnumKlass.cpp","additions":4,"deletions":5,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -31,1 +31,0 @@\n-#include \"cds\/archiveHeapWriter.hpp\"\n@@ -36,0 +35,1 @@\n+#include \"cds\/heapShared.hpp\"\n@@ -356,2 +356,1 @@\n-  ArchiveHeapInfo no_heap_for_dynamic_dump;\n-  ArchiveBuilder::write_archive(dynamic_info, &no_heap_for_dynamic_dump);\n+  ArchiveBuilder::write_archive(dynamic_info, nullptr, nullptr);\n","filename":"src\/hotspot\/share\/cds\/dynamicArchive.cpp","additions":2,"deletions":3,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -27,0 +27,2 @@\n+#include \"cds\/aotMappedHeapLoader.hpp\"\n+#include \"cds\/aotMappedHeapWriter.hpp\"\n@@ -29,2 +31,0 @@\n-#include \"cds\/archiveHeapLoader.inline.hpp\"\n-#include \"cds\/archiveHeapWriter.hpp\"\n@@ -36,1 +36,1 @@\n-#include \"cds\/heapShared.hpp\"\n+#include \"cds\/heapShared.inline.hpp\"\n@@ -285,0 +285,1 @@\n+    _object_streaming_mode = HeapShared::is_writing_streaming_mode();\n@@ -353,34 +354,45 @@\n-  st->print_cr(\"- core_region_alignment:          %zu\", _core_region_alignment);\n-  st->print_cr(\"- obj_alignment:                  %d\", _obj_alignment);\n-  st->print_cr(\"- narrow_oop_base:                \" INTPTR_FORMAT, p2i(_narrow_oop_base));\n-  st->print_cr(\"- narrow_oop_shift                %d\", _narrow_oop_shift);\n-  st->print_cr(\"- compact_strings:                %d\", _compact_strings);\n-  st->print_cr(\"- compact_headers:                %d\", _compact_headers);\n-  st->print_cr(\"- max_heap_size:                  %zu\", _max_heap_size);\n-  st->print_cr(\"- narrow_oop_mode:                %d\", _narrow_oop_mode);\n-  st->print_cr(\"- compressed_oops:                %d\", _compressed_oops);\n-  st->print_cr(\"- compressed_class_ptrs:          %d\", _compressed_class_ptrs);\n-  st->print_cr(\"- narrow_klass_pointer_bits:      %d\", _narrow_klass_pointer_bits);\n-  st->print_cr(\"- narrow_klass_shift:             %d\", _narrow_klass_shift);\n-  st->print_cr(\"- cloned_vtables_offset:          0x%zx\", _cloned_vtables_offset);\n-  st->print_cr(\"- early_serialized_data_offset:   0x%zx\", _early_serialized_data_offset);\n-  st->print_cr(\"- serialized_data_offset:         0x%zx\", _serialized_data_offset);\n-  st->print_cr(\"- jvm_ident:                      %s\", _jvm_ident);\n-  st->print_cr(\"- class_location_config_offset:   0x%zx\", _class_location_config_offset);\n-  st->print_cr(\"- verify_local:                   %d\", _verify_local);\n-  st->print_cr(\"- verify_remote:                  %d\", _verify_remote);\n-  st->print_cr(\"- has_platform_or_app_classes:    %d\", _has_platform_or_app_classes);\n-  st->print_cr(\"- requested_base_address:         \" INTPTR_FORMAT, p2i(_requested_base_address));\n-  st->print_cr(\"- mapped_base_address:            \" INTPTR_FORMAT, p2i(_mapped_base_address));\n-  st->print_cr(\"- heap_root_segments.roots_count: %d\" , _heap_root_segments.roots_count());\n-  st->print_cr(\"- heap_root_segments.base_offset: 0x%zx\", _heap_root_segments.base_offset());\n-  st->print_cr(\"- heap_root_segments.count:       %zu\", _heap_root_segments.count());\n-  st->print_cr(\"- heap_root_segments.max_size_elems: %d\", _heap_root_segments.max_size_in_elems());\n-  st->print_cr(\"- heap_root_segments.max_size_bytes: %d\", _heap_root_segments.max_size_in_bytes());\n-  st->print_cr(\"- _heap_oopmap_start_pos:         %zu\", _heap_oopmap_start_pos);\n-  st->print_cr(\"- _heap_ptrmap_start_pos:         %zu\", _heap_ptrmap_start_pos);\n-  st->print_cr(\"- _rw_ptrmap_start_pos:           %zu\", _rw_ptrmap_start_pos);\n-  st->print_cr(\"- _ro_ptrmap_start_pos:           %zu\", _ro_ptrmap_start_pos);\n-  st->print_cr(\"- use_optimized_module_handling:  %d\", _use_optimized_module_handling);\n-  st->print_cr(\"- has_full_module_graph           %d\", _has_full_module_graph);\n-  st->print_cr(\"- has_valhalla_patched_classes    %d\", _has_valhalla_patched_classes);\n+  st->print_cr(\"- core_region_alignment:                    %zu\", _core_region_alignment);\n+  st->print_cr(\"- obj_alignment:                            %d\", _obj_alignment);\n+  st->print_cr(\"- narrow_oop_base:                          \" INTPTR_FORMAT, p2i(_narrow_oop_base));\n+  st->print_cr(\"- narrow_oop_shift                          %d\", _narrow_oop_shift);\n+  st->print_cr(\"- compact_strings:                          %d\", _compact_strings);\n+  st->print_cr(\"- compact_headers:                          %d\", _compact_headers);\n+  st->print_cr(\"- max_heap_size:                            %zu\", _max_heap_size);\n+  st->print_cr(\"- narrow_oop_mode:                          %d\", _narrow_oop_mode);\n+  st->print_cr(\"- compressed_oops:                          %d\", _compressed_oops);\n+  st->print_cr(\"- compressed_class_ptrs:                    %d\", _compressed_class_ptrs);\n+  st->print_cr(\"- narrow_klass_pointer_bits:                %d\", _narrow_klass_pointer_bits);\n+  st->print_cr(\"- narrow_klass_shift:                       %d\", _narrow_klass_shift);\n+  st->print_cr(\"- cloned_vtables_offset:                    0x%zx\", _cloned_vtables_offset);\n+  st->print_cr(\"- early_serialized_data_offset:             0x%zx\", _early_serialized_data_offset);\n+  st->print_cr(\"- serialized_data_offset:                   0x%zx\", _serialized_data_offset);\n+  st->print_cr(\"- jvm_ident:                                %s\", _jvm_ident);\n+  st->print_cr(\"- class_location_config_offset:             0x%zx\", _class_location_config_offset);\n+  st->print_cr(\"- verify_local:                             %d\", _verify_local);\n+  st->print_cr(\"- verify_remote:                            %d\", _verify_remote);\n+  st->print_cr(\"- has_platform_or_app_classes:              %d\", _has_platform_or_app_classes);\n+  st->print_cr(\"- requested_base_address:                   \" INTPTR_FORMAT, p2i(_requested_base_address));\n+  st->print_cr(\"- mapped_base_address:                      \" INTPTR_FORMAT, p2i(_mapped_base_address));\n+\n+  st->print_cr(\"- object_streaming_mode:                    %d\", _object_streaming_mode);\n+  st->print_cr(\"- mapped_heap_header\");\n+  st->print_cr(\"  - root_segments\");\n+  st->print_cr(\"    - roots_count:                          %d\", _mapped_heap_header.root_segments().roots_count());\n+  st->print_cr(\"    - base_offset:                          0x%zx\", _mapped_heap_header.root_segments().base_offset());\n+  st->print_cr(\"    - count:                                %zu\", _mapped_heap_header.root_segments().count());\n+  st->print_cr(\"    - max_size_elems:                       %d\", _mapped_heap_header.root_segments().max_size_in_elems());\n+  st->print_cr(\"    - max_size_bytes:                       %zu\", _mapped_heap_header.root_segments().max_size_in_bytes());\n+  st->print_cr(\"  - oopmap_start_pos:                       %zu\", _mapped_heap_header.oopmap_start_pos());\n+  st->print_cr(\"  - oopmap_ptrmap_pos:                      %zu\", _mapped_heap_header.ptrmap_start_pos());\n+  st->print_cr(\"- streamed_heap_header\");\n+  st->print_cr(\"  - forwarding_offset:                      %zu\", _streamed_heap_header.forwarding_offset());\n+  st->print_cr(\"  - roots_offset:                           %zu\", _streamed_heap_header.roots_offset());\n+  st->print_cr(\"  - num_roots:                              %zu\", _streamed_heap_header.num_roots());\n+  st->print_cr(\"  - root_highest_object_index_table_offset: %zu\", _streamed_heap_header.root_highest_object_index_table_offset());\n+  st->print_cr(\"  - num_archived_objects:                   %zu\", _streamed_heap_header.num_archived_objects());\n+\n+  st->print_cr(\"- _rw_ptrmap_start_pos:                     %zu\", _rw_ptrmap_start_pos);\n+  st->print_cr(\"- _ro_ptrmap_start_pos:                     %zu\", _ro_ptrmap_start_pos);\n+  st->print_cr(\"- use_optimized_module_handling:            %d\", _use_optimized_module_handling);\n+  st->print_cr(\"- has_full_module_graph                     %d\", _has_full_module_graph);\n+  st->print_cr(\"- has_valhalla_patched_classes              %d\", _has_valhalla_patched_classes);\n@@ -388,1 +400,1 @@\n-  st->print_cr(\"- has_aot_linked_classes          %d\", _has_aot_linked_classes);\n+  st->print_cr(\"- has_aot_linked_classes                    %d\", _has_aot_linked_classes);\n@@ -972,4 +984,6 @@\n-    requested_base = (char*)ArchiveHeapWriter::requested_address();\n-    if (UseCompressedOops) {\n-      mapping_offset = (size_t)((address)requested_base - CompressedOops::base());\n-      assert((mapping_offset >> CompressedOops::shift()) << CompressedOops::shift() == mapping_offset, \"must be\");\n+    if (HeapShared::is_writing_mapping_mode()) {\n+      requested_base = (char*)AOTMappedHeapWriter::requested_address();\n+      if (UseCompressedOops) {\n+        mapping_offset = (size_t)((address)requested_base - CompressedOops::base());\n+        assert((mapping_offset >> CompressedOops::shift()) << CompressedOops::shift() == mapping_offset, \"must be\");\n+      }\n@@ -977,1 +991,1 @@\n-      mapping_offset = 0; \/\/ not used with !UseCompressedOops\n+      requested_base = nullptr;\n@@ -1030,1 +1044,4 @@\n-char* FileMapInfo::write_bitmap_region(CHeapBitMap* rw_ptrmap, CHeapBitMap* ro_ptrmap, ArchiveHeapInfo* heap_info,\n+char* FileMapInfo::write_bitmap_region(CHeapBitMap* rw_ptrmap,\n+                                       CHeapBitMap* ro_ptrmap,\n+                                       ArchiveMappedHeapInfo* mapped_heap_info,\n+                                       ArchiveStreamedHeapInfo* streamed_heap_info,\n@@ -1038,1 +1055,1 @@\n-  if (heap_info->is_used()) {\n+  if (mapped_heap_info != nullptr && mapped_heap_info->is_used()) {\n@@ -1040,4 +1057,10 @@\n-    size_t removed_oop_leading_zeros = remove_bitmap_zeros(heap_info->oopmap());\n-    size_t removed_ptr_leading_zeros = remove_bitmap_zeros(heap_info->ptrmap());\n-    header()->set_heap_oopmap_start_pos(removed_oop_leading_zeros);\n-    header()->set_heap_ptrmap_start_pos(removed_ptr_leading_zeros);\n+    assert(HeapShared::is_writing_mapping_mode(), \"unexpected dumping mode\");\n+    size_t removed_oop_leading_zeros = remove_bitmap_zeros(mapped_heap_info->oopmap());\n+    size_t removed_ptr_leading_zeros = remove_bitmap_zeros(mapped_heap_info->ptrmap());\n+    mapped_heap_info->set_oopmap_start_pos(removed_oop_leading_zeros);\n+    mapped_heap_info->set_ptrmap_start_pos(removed_ptr_leading_zeros);\n+\n+    size_in_bytes += mapped_heap_info->oopmap()->size_in_bytes();\n+    size_in_bytes += mapped_heap_info->ptrmap()->size_in_bytes();\n+  } else if (streamed_heap_info != nullptr && streamed_heap_info->is_used()) {\n+    assert(HeapShared::is_writing_streaming_mode(), \"unexpected dumping mode\");\n@@ -1045,2 +1068,1 @@\n-    size_in_bytes += heap_info->oopmap()->size_in_bytes();\n-    size_in_bytes += heap_info->ptrmap()->size_in_bytes();\n+    size_in_bytes += streamed_heap_info->oopmap()->size_in_bytes();\n@@ -1050,4 +1072,4 @@\n-  \/\/ rw_ptrmap:           metaspace pointers inside the read-write region\n-  \/\/ ro_ptrmap:           metaspace pointers inside the read-only region\n-  \/\/ heap_info->oopmap(): Java oop pointers in the heap region\n-  \/\/ heap_info->ptrmap(): metaspace pointers in the heap region\n+  \/\/ rw_ptrmap:                  metaspace pointers inside the read-write region\n+  \/\/ ro_ptrmap:                  metaspace pointers inside the read-only region\n+  \/\/ *_heap_info->oopmap():      Java oop pointers in the heap region\n+  \/\/ mapped_heap_info->ptrmap(): metaspace pointers in the heap region\n@@ -1063,1 +1085,2 @@\n-  if (heap_info->is_used()) {\n+  if (mapped_heap_info != nullptr && mapped_heap_info->is_used()) {\n+    assert(HeapShared::is_writing_mapping_mode(), \"unexpected dumping mode\");\n@@ -1066,2 +1089,8 @@\n-    r->init_oopmap(written, heap_info->oopmap()->size());\n-    written = write_bitmap(heap_info->oopmap(), buffer, written);\n+    r->init_oopmap(written, mapped_heap_info->oopmap()->size());\n+    written = write_bitmap(mapped_heap_info->oopmap(), buffer, written);\n+\n+    r->init_ptrmap(written, mapped_heap_info->ptrmap()->size());\n+    written = write_bitmap(mapped_heap_info->ptrmap(), buffer, written);\n+  } else if (streamed_heap_info != nullptr && streamed_heap_info->is_used()) {\n+    assert(HeapShared::is_writing_streaming_mode(), \"unexpected dumping mode\");\n+    FileMapRegion* r = region_at(AOTMetaspace::hp);\n@@ -1069,2 +1098,2 @@\n-    r->init_ptrmap(written, heap_info->ptrmap()->size());\n-    written = write_bitmap(heap_info->ptrmap(), buffer, written);\n+    r->init_oopmap(written, streamed_heap_info->oopmap()->size());\n+    written = write_bitmap(streamed_heap_info->oopmap(), buffer, written);\n@@ -1077,1 +1106,2 @@\n-size_t FileMapInfo::write_heap_region(ArchiveHeapInfo* heap_info) {\n+#if INCLUDE_CDS_JAVA_HEAP\n+size_t FileMapInfo::write_mapped_heap_region(ArchiveMappedHeapInfo* heap_info) {\n@@ -1081,1 +1111,9 @@\n-  header()->set_heap_root_segments(heap_info->heap_root_segments());\n+  header()->set_mapped_heap_header(heap_info->create_header());\n+  return buffer_size;\n+}\n+\n+size_t FileMapInfo::write_streamed_heap_region(ArchiveStreamedHeapInfo* heap_info) {\n+  char* buffer_start = heap_info->buffer_start();\n+  size_t buffer_size = heap_info->buffer_byte_size();\n+  write_region(AOTMetaspace::hp, buffer_start, buffer_size, true, false);\n+  header()->set_streamed_heap_header(heap_info->create_header());\n@@ -1084,0 +1122,1 @@\n+#endif \/\/ INCLUDE_CDS_JAVA_HEAP\n@@ -1152,1 +1191,1 @@\n-                        char *addr, size_t bytes, bool read_only,\n+                        char* addr, size_t bytes, bool read_only,\n@@ -1163,0 +1202,11 @@\n+char* FileMapInfo::map_heap_region(FileMapRegion* r, char* addr, size_t bytes) {\n+  return ::map_memory(_fd,\n+                    _full_path,\n+                    r->file_offset(),\n+                    addr,\n+                    bytes,\n+                    r->read_only(),\n+                    r->allow_exec(),\n+                    mtJavaHeap);\n+}\n+\n@@ -1330,2 +1380,2 @@\n-char* FileMapInfo::map_bitmap_region() {\n-  FileMapRegion* r = region_at(AOTMetaspace::bm);\n+char* FileMapInfo::map_auxiliary_region(int region_index, bool read_only) {\n+  FileMapRegion* r = region_at(region_index);\n@@ -1335,1 +1385,2 @@\n-  bool read_only = true, allow_exec = false;\n+  const char* region_name = shared_region_name[region_index];\n+  bool allow_exec = false;\n@@ -1337,1 +1388,1 @@\n-  char* bitmap_base = map_memory(_fd, _full_path, r->file_offset(),\n+  char* mapped_base = map_memory(_fd, _full_path, r->file_offset(),\n@@ -1339,2 +1390,2 @@\n-  if (bitmap_base == nullptr) {\n-    AOTMetaspace::report_loading_error(\"failed to map relocation bitmap\");\n+  if (mapped_base == nullptr) {\n+    AOTMetaspace::report_loading_error(\"failed to map %d region\", region_index);\n@@ -1344,4 +1395,4 @@\n-  if (VerifySharedSpaces && !r->check_region_crc(bitmap_base)) {\n-    aot_log_error(aot)(\"relocation bitmap CRC error\");\n-    if (!os::unmap_memory(bitmap_base, r->used_aligned())) {\n-      fatal(\"os::unmap_memory of relocation bitmap failed\");\n+  if (VerifySharedSpaces && !r->check_region_crc(mapped_base)) {\n+    aot_log_error(aot)(\"region %d CRC error\", region_index);\n+    if (!os::unmap_memory(mapped_base, r->used_aligned())) {\n+      fatal(\"os::unmap_memory of region %d failed\", region_index);\n@@ -1353,2 +1404,2 @@\n-  r->set_mapped_base(bitmap_base);\n-  aot_log_info(aot)(\"Mapped %s region #%d at base \" INTPTR_FORMAT \" top \" INTPTR_FORMAT \" (%s)\",\n+  r->set_mapped_base(mapped_base);\n+  aot_log_info(aot)(\"Mapped %s region #%d at base %zu top %zu (%s)\",\n@@ -1356,3 +1407,7 @@\n-                AOTMetaspace::bm, p2i(r->mapped_base()), p2i(r->mapped_end()),\n-                shared_region_name[AOTMetaspace::bm]);\n-  return bitmap_base;\n+                region_index, p2i(r->mapped_base()), p2i(r->mapped_end()),\n+                region_name);\n+  return mapped_base;\n+}\n+\n+char* FileMapInfo::map_bitmap_region() {\n+  return map_auxiliary_region(AOTMetaspace::bm, false);\n@@ -1505,1 +1560,0 @@\n-MemRegion FileMapInfo::_mapped_heap_memregion;\n@@ -1511,8 +1565,14 @@\n-\/\/ Returns the address range of the archived heap region computed using the\n-\/\/ current oop encoding mode. This range may be different than the one seen at\n-\/\/ dump time due to encoding mode differences. The result is used in determining\n-\/\/ if\/how these regions should be relocated at run time.\n-MemRegion FileMapInfo::get_heap_region_requested_range() {\n-  FileMapRegion* r = region_at(AOTMetaspace::hp);\n-  size_t size = r->used();\n-  assert(size > 0, \"must have non-empty heap region\");\n+static void on_heap_region_loading_error() {\n+  if (CDSConfig::is_using_aot_linked_classes()) {\n+    \/\/ It's too late to recover -- we have already committed to use the archived metaspace objects, but\n+    \/\/ the archived heap objects cannot be loaded, so we don't have the archived FMG to guarantee that\n+    \/\/ all AOT-linked classes are visible.\n+    \/\/\n+    \/\/ We get here because the heap is too small. The app will fail anyway. So let's quit.\n+    aot_log_error(aot)(\"%s has aot-linked classes but the archived \"\n+                       \"heap objects cannot be loaded. Try increasing your heap size.\",\n+                       CDSConfig::type_of_archive_being_loaded());\n+    AOTMetaspace::unrecoverable_loading_error();\n+  }\n+  CDSConfig::stop_using_full_module_graph();\n+}\n@@ -1520,4 +1580,2 @@\n-  address start = heap_region_requested_address();\n-  address end = start + size;\n-  aot_log_info(aot)(\"Requested heap region [\" INTPTR_FORMAT \" - \" INTPTR_FORMAT \"] = %8zu bytes\",\n-                p2i(start), p2i(end), size);\n+void FileMapInfo::stream_heap_region() {\n+  assert(object_streaming_mode(), \"This should only be done for the streaming approach\");\n@@ -1525,1 +1583,5 @@\n-  return MemRegion((HeapWord*)start, (HeapWord*)end);\n+  if (map_auxiliary_region(AOTMetaspace::hp, \/*readonly=*\/true) != nullptr) {\n+    HeapShared::initialize_streaming();\n+  } else {\n+    on_heap_region_loading_error();\n+  }\n@@ -1529,0 +1591,1 @@\n+  assert(!object_streaming_mode(), \"This should only be done for the mapping approach\");\n@@ -1531,12 +1594,4 @@\n-  if (can_use_heap_region()) {\n-    if (ArchiveHeapLoader::can_map()) {\n-      success = map_heap_region();\n-    } else if (ArchiveHeapLoader::can_load()) {\n-      success = ArchiveHeapLoader::load_heap_region(this);\n-    } else {\n-      if (!UseCompressedOops && !ArchiveHeapLoader::can_map()) {\n-        AOTMetaspace::report_loading_error(\"Cannot use CDS heap data. Selected GC not compatible -XX:-UseCompressedOops\");\n-      } else {\n-        AOTMetaspace::report_loading_error(\"Cannot use CDS heap data. UseEpsilonGC, UseG1GC, UseSerialGC, UseParallelGC, or UseShenandoahGC are required.\");\n-      }\n-    }\n+  if (AOTMappedHeapLoader::can_map()) {\n+    success = AOTMappedHeapLoader::map_heap_region(this);\n+  } else if (AOTMappedHeapLoader::can_load()) {\n+    success = AOTMappedHeapLoader::load_heap_region(this);\n@@ -1546,12 +1601,1 @@\n-    if (CDSConfig::is_using_aot_linked_classes()) {\n-      \/\/ It's too late to recover -- we have already committed to use the archived metaspace objects, but\n-      \/\/ the archived heap objects cannot be loaded, so we don't have the archived FMG to guarantee that\n-      \/\/ all AOT-linked classes are visible.\n-      \/\/\n-      \/\/ We get here because the heap is too small. The app will fail anyway. So let's quit.\n-      aot_log_error(aot)(\"%s has aot-linked classes but the archived \"\n-                     \"heap objects cannot be loaded. Try increasing your heap size.\",\n-                     CDSConfig::type_of_archive_being_loaded());\n-      AOTMetaspace::unrecoverable_loading_error();\n-    }\n-    CDSConfig::stop_using_full_module_graph(\"archive heap loading failed\");\n+    on_heap_region_loading_error();\n@@ -1565,0 +1609,4 @@\n+  if (!object_streaming_mode() && !Universe::heap()->can_load_archived_objects() && !UseG1GC) {\n+    \/\/ Incompatible object format\n+    return false;\n+  }\n@@ -1579,1 +1627,1 @@\n-  \/\/ ArchiveBuilder::precomputed_narrow_klass_shift. We enforce this encoding at runtime (see\n+  \/\/ HeapShared::precomputed_narrow_klass_shift. We enforce this encoding at runtime (see\n@@ -1588,0 +1636,1 @@\n+\n@@ -1590,2 +1639,4 @@\n-  aot_log_info(aot)(\"    narrow_oop_mode = %d, narrow_oop_base = \" PTR_FORMAT \", narrow_oop_shift = %d\",\n-                narrow_oop_mode(), p2i(narrow_oop_base()), narrow_oop_shift());\n+  if (UseCompressedOops) {\n+    aot_log_info(aot)(\"    narrow_oop_mode = %d, narrow_oop_base = \" PTR_FORMAT \", narrow_oop_shift = %d\",\n+                      narrow_oop_mode(), p2i(narrow_oop_base()), narrow_oop_shift());\n+  }\n@@ -1596,7 +1647,11 @@\n-  aot_log_info(aot)(\"    narrow_oop_mode = %d, narrow_oop_base = \" PTR_FORMAT \", narrow_oop_shift = %d\",\n-                CompressedOops::mode(), p2i(CompressedOops::base()), CompressedOops::shift());\n-  aot_log_info(aot)(\"    heap range = [\" PTR_FORMAT \" - \"  PTR_FORMAT \"]\",\n-                UseCompressedOops ? p2i(CompressedOops::begin()) :\n-                                    UseG1GC ? p2i((address)G1CollectedHeap::heap()->reserved().start()) : 0L,\n-                UseCompressedOops ? p2i(CompressedOops::end()) :\n-                                    UseG1GC ? p2i((address)G1CollectedHeap::heap()->reserved().end()) : 0L);\n+  if (UseCompressedOops) {\n+    aot_log_info(aot)(\"    narrow_oop_mode = %d, narrow_oop_base = \" PTR_FORMAT \", narrow_oop_shift = %d\",\n+                      CompressedOops::mode(), p2i(CompressedOops::base()), CompressedOops::shift());\n+  }\n+  if (!object_streaming_mode()) {\n+    aot_log_info(aot)(\"    heap range = [\" PTR_FORMAT \" - \"  PTR_FORMAT \"]\",\n+                      UseCompressedOops ? p2i(CompressedOops::begin()) :\n+                      UseG1GC ? p2i((address)G1CollectedHeap::heap()->reserved().start()) : 0L,\n+                      UseCompressedOops ? p2i(CompressedOops::end()) :\n+                      UseG1GC ? p2i((address)G1CollectedHeap::heap()->reserved().end()) : 0L);\n+  }\n@@ -1646,196 +1701,0 @@\n-\/\/ The actual address of this region during dump time.\n-address FileMapInfo::heap_region_dumptime_address() {\n-  FileMapRegion* r = region_at(AOTMetaspace::hp);\n-  assert(CDSConfig::is_using_archive(), \"runtime only\");\n-  assert(is_aligned(r->mapping_offset(), sizeof(HeapWord)), \"must be\");\n-  if (UseCompressedOops) {\n-    return \/*dumptime*\/ (address)((uintptr_t)narrow_oop_base() + r->mapping_offset());\n-  } else {\n-    return heap_region_requested_address();\n-  }\n-}\n-\n-\/\/ The address where this region can be mapped into the runtime heap without\n-\/\/ patching any of the pointers that are embedded in this region.\n-address FileMapInfo::heap_region_requested_address() {\n-  assert(CDSConfig::is_using_archive(), \"runtime only\");\n-  FileMapRegion* r = region_at(AOTMetaspace::hp);\n-  assert(is_aligned(r->mapping_offset(), sizeof(HeapWord)), \"must be\");\n-  assert(ArchiveHeapLoader::can_use(), \"GC must support mapping or loading\");\n-  if (UseCompressedOops) {\n-    \/\/ We can avoid relocation if each region's offset from the runtime CompressedOops::base()\n-    \/\/ is the same as its offset from the CompressedOops::base() during dumptime.\n-    \/\/ Note that CompressedOops::base() may be different between dumptime and runtime.\n-    \/\/\n-    \/\/ Example:\n-    \/\/ Dumptime base = 0x1000 and shift is 0. We have a region at address 0x2000. There's a\n-    \/\/ narrowOop P stored in this region that points to an object at address 0x2200.\n-    \/\/ P's encoded value is 0x1200.\n-    \/\/\n-    \/\/ Runtime base = 0x4000 and shift is also 0. If we map this region at 0x5000, then\n-    \/\/ the value P can remain 0x1200. The decoded address = (0x4000 + (0x1200 << 0)) = 0x5200,\n-    \/\/ which is the runtime location of the referenced object.\n-    return \/*runtime*\/ (address)((uintptr_t)CompressedOops::base() + r->mapping_offset());\n-  } else {\n-    \/\/ This was the hard-coded requested base address used at dump time. With uncompressed oops,\n-    \/\/ the heap range is assigned by the OS so we will most likely have to relocate anyway, no matter\n-    \/\/ what base address was picked at duump time.\n-    return (address)ArchiveHeapWriter::NOCOOPS_REQUESTED_BASE;\n-  }\n-}\n-\n-bool FileMapInfo::map_heap_region() {\n-  if (map_heap_region_impl()) {\n-#ifdef ASSERT\n-    \/\/ The \"old\" regions must be parsable -- we cannot have any unused space\n-    \/\/ at the start of the lowest G1 region that contains archived objects.\n-    assert(is_aligned(_mapped_heap_memregion.start(), G1HeapRegion::GrainBytes), \"must be\");\n-\n-    \/\/ Make sure we map at the very top of the heap - see comments in\n-    \/\/ init_heap_region_relocation().\n-    MemRegion heap_range = G1CollectedHeap::heap()->reserved();\n-    assert(heap_range.contains(_mapped_heap_memregion), \"must be\");\n-\n-    address heap_end = (address)heap_range.end();\n-    address mapped_heap_region_end = (address)_mapped_heap_memregion.end();\n-    assert(heap_end >= mapped_heap_region_end, \"must be\");\n-    assert(heap_end - mapped_heap_region_end < (intx)(G1HeapRegion::GrainBytes),\n-           \"must be at the top of the heap to avoid fragmentation\");\n-#endif\n-\n-    ArchiveHeapLoader::set_mapped();\n-    return true;\n-  } else {\n-    return false;\n-  }\n-}\n-\n-bool FileMapInfo::map_heap_region_impl() {\n-  assert(UseG1GC, \"the following code assumes G1\");\n-\n-  FileMapRegion* r = region_at(AOTMetaspace::hp);\n-  size_t size = r->used();\n-  if (size == 0) {\n-    return false; \/\/ no archived java heap data\n-  }\n-\n-  size_t word_size = size \/ HeapWordSize;\n-  address requested_start = heap_region_requested_address();\n-\n-  aot_log_info(aot)(\"Preferred address to map heap data (to avoid relocation) is \" INTPTR_FORMAT, p2i(requested_start));\n-\n-  \/\/ allocate from java heap\n-  HeapWord* start = G1CollectedHeap::heap()->alloc_archive_region(word_size, (HeapWord*)requested_start);\n-  if (start == nullptr) {\n-    AOTMetaspace::report_loading_error(\"UseSharedSpaces: Unable to allocate java heap region for archive heap.\");\n-    return false;\n-  }\n-\n-  _mapped_heap_memregion = MemRegion(start, word_size);\n-\n-  \/\/ Map the archived heap data. No need to call MemTracker::record_virtual_memory_tag()\n-  \/\/ for mapped region as it is part of the reserved java heap, which is already recorded.\n-  char* addr = (char*)_mapped_heap_memregion.start();\n-  char* base;\n-\n-  if (AOTMetaspace::use_windows_memory_mapping() || UseLargePages) {\n-    \/\/ With UseLargePages, memory mapping may fail on some OSes if the size is not\n-    \/\/ large page aligned, so let's use read() instead. In this case, the memory region\n-    \/\/ is already commited by G1 so we don't need to commit it again.\n-    if (!read_region(AOTMetaspace::hp, addr,\n-                     align_up(_mapped_heap_memregion.byte_size(), os::vm_page_size()),\n-                     \/* do_commit = *\/ !UseLargePages)) {\n-      dealloc_heap_region();\n-      aot_log_error(aot)(\"Failed to read archived heap region into \" INTPTR_FORMAT, p2i(addr));\n-      return false;\n-    }\n-    \/\/ Checks for VerifySharedSpaces is already done inside read_region()\n-    base = addr;\n-  } else {\n-    base = map_memory(_fd, _full_path, r->file_offset(),\n-                      addr, _mapped_heap_memregion.byte_size(), r->read_only(),\n-                      r->allow_exec(), mtJavaHeap);\n-    if (base == nullptr || base != addr) {\n-      dealloc_heap_region();\n-      AOTMetaspace::report_loading_error(\"UseSharedSpaces: Unable to map at required address in java heap. \"\n-                                            INTPTR_FORMAT \", size = %zu bytes\",\n-                                            p2i(addr), _mapped_heap_memregion.byte_size());\n-      return false;\n-    }\n-\n-    if (VerifySharedSpaces && !r->check_region_crc(base)) {\n-      dealloc_heap_region();\n-      AOTMetaspace::report_loading_error(\"UseSharedSpaces: mapped heap region is corrupt\");\n-      return false;\n-    }\n-  }\n-\n-  r->set_mapped_base(base);\n-\n-  \/\/ If the requested range is different from the range allocated by GC, then\n-  \/\/ the pointers need to be patched.\n-  address mapped_start = (address) _mapped_heap_memregion.start();\n-  ptrdiff_t delta = mapped_start - requested_start;\n-  if (UseCompressedOops &&\n-      (narrow_oop_mode() != CompressedOops::mode() ||\n-       narrow_oop_shift() != CompressedOops::shift())) {\n-    _heap_pointers_need_patching = true;\n-  }\n-  if (delta != 0) {\n-    _heap_pointers_need_patching = true;\n-  }\n-  ArchiveHeapLoader::init_mapped_heap_info(mapped_start, delta, narrow_oop_shift());\n-\n-  if (_heap_pointers_need_patching) {\n-    char* bitmap_base = map_bitmap_region();\n-    if (bitmap_base == nullptr) {\n-      AOTMetaspace::report_loading_error(\"CDS heap cannot be used because bitmap region cannot be mapped\");\n-      dealloc_heap_region();\n-      _heap_pointers_need_patching = false;\n-      return false;\n-    }\n-  }\n-  aot_log_info(aot)(\"Heap data mapped at \" INTPTR_FORMAT \", size = %8zu bytes\",\n-                p2i(mapped_start), _mapped_heap_memregion.byte_size());\n-  aot_log_info(aot)(\"CDS heap data relocation delta = %zd bytes\", delta);\n-  return true;\n-}\n-\n-narrowOop FileMapInfo::encoded_heap_region_dumptime_address() {\n-  assert(CDSConfig::is_using_archive(), \"runtime only\");\n-  assert(UseCompressedOops, \"sanity\");\n-  FileMapRegion* r = region_at(AOTMetaspace::hp);\n-  return CompressedOops::narrow_oop_cast(r->mapping_offset() >> narrow_oop_shift());\n-}\n-\n-void FileMapInfo::patch_heap_embedded_pointers() {\n-  if (!ArchiveHeapLoader::is_mapped() || !_heap_pointers_need_patching) {\n-    return;\n-  }\n-\n-  char* bitmap_base = map_bitmap_region();\n-  assert(bitmap_base != nullptr, \"must have already been mapped\");\n-\n-  FileMapRegion* r = region_at(AOTMetaspace::hp);\n-  ArchiveHeapLoader::patch_embedded_pointers(\n-      this, _mapped_heap_memregion,\n-      (address)(region_at(AOTMetaspace::bm)->mapped_base()) + r->oopmap_offset(),\n-      r->oopmap_size_in_bits());\n-}\n-\n-void FileMapInfo::fixup_mapped_heap_region() {\n-  if (ArchiveHeapLoader::is_mapped()) {\n-    assert(!_mapped_heap_memregion.is_empty(), \"sanity\");\n-\n-    \/\/ Populate the archive regions' G1BlockOffsetTables. That ensures\n-    \/\/ fast G1BlockOffsetTable::block_start operations for any given address\n-    \/\/ within the archive regions when trying to find start of an object\n-    \/\/ (e.g. during card table scanning).\n-    G1CollectedHeap::heap()->populate_archive_regions_bot(_mapped_heap_memregion);\n-  }\n-}\n-\n-\/\/ dealloc the archive regions from java heap\n-void FileMapInfo::dealloc_heap_region() {\n-  G1CollectedHeap::heap()->dealloc_archive_regions(_mapped_heap_memregion);\n-}\n@@ -1844,0 +1703,2 @@\n+\/\/ Unmap a memory region in the address space.\n+\n@@ -1851,2 +1712,0 @@\n-\/\/ Unmap a memory region in the address space.\n-\n@@ -1884,1 +1743,0 @@\n-bool FileMapInfo::_heap_pointers_need_patching = false;\n","filename":"src\/hotspot\/share\/cds\/filemap.cpp","additions":183,"deletions":325,"binary":false,"changes":508,"status":"modified"},{"patch":"@@ -30,0 +30,1 @@\n+#include \"cds\/heapShared.hpp\"\n@@ -37,0 +38,1 @@\n+#include \"utilities\/bitMap.hpp\"\n@@ -47,1 +49,0 @@\n-class ArchiveHeapInfo;\n@@ -149,0 +150,1 @@\n+  bool    _object_streaming_mode;                 \/\/ dump was created for object streaming\n@@ -176,3 +178,0 @@\n-  HeapRootSegments _heap_root_segments; \/\/ Heap root segments info\n-  size_t _heap_oopmap_start_pos;        \/\/ The first bit in the oopmap corresponds to this position in the heap.\n-  size_t _heap_ptrmap_start_pos;        \/\/ The first bit in the ptrmap corresponds to this position in the heap.\n@@ -182,0 +181,3 @@\n+  ArchiveMappedHeapHeader _mapped_heap_header;\n+  ArchiveStreamedHeapHeader _streamed_heap_header;\n+\n@@ -229,0 +231,1 @@\n+  bool object_streaming_mode()             const { return _object_streaming_mode; }\n@@ -238,3 +241,0 @@\n-  HeapRootSegments heap_root_segments()    const { return _heap_root_segments; }\n-  size_t heap_oopmap_start_pos()           const { return _heap_oopmap_start_pos; }\n-  size_t heap_ptrmap_start_pos()           const { return _heap_ptrmap_start_pos; }\n@@ -245,0 +245,7 @@\n+  \/\/ Heap archiving\n+  const ArchiveMappedHeapHeader*   mapped_heap()   const { return &_mapped_heap_header; }\n+  const ArchiveStreamedHeapHeader* streamed_heap() const { return &_streamed_heap_header; }\n+\n+  void set_streamed_heap_header(ArchiveStreamedHeapHeader header) { _streamed_heap_header = header; }\n+  void set_mapped_heap_header(ArchiveMappedHeapHeader header) { _mapped_heap_header = header; }\n+\n@@ -250,3 +257,0 @@\n-  void set_heap_root_segments(HeapRootSegments segments) { _heap_root_segments = segments; }\n-  void set_heap_oopmap_start_pos(size_t n)       { _heap_oopmap_start_pos = n; }\n-  void set_heap_ptrmap_start_pos(size_t n)       { _heap_ptrmap_start_pos = n; }\n@@ -255,0 +259,1 @@\n+\n@@ -314,1 +319,0 @@\n-  static bool _heap_pointers_need_patching;\n@@ -344,3 +348,4 @@\n-  HeapRootSegments heap_root_segments() const { return header()->heap_root_segments(); }\n-  size_t  heap_oopmap_start_pos() const { return header()->heap_oopmap_start_pos(); }\n-  size_t  heap_ptrmap_start_pos() const { return header()->heap_ptrmap_start_pos(); }\n+  const ArchiveMappedHeapHeader*   mapped_heap()   const { return header()->mapped_heap(); }\n+  const ArchiveStreamedHeapHeader* streamed_heap() const { return header()->streamed_heap(); }\n+\n+  bool object_streaming_mode()                const { return header()->object_streaming_mode(); }\n@@ -365,0 +370,1 @@\n+  char* map_heap_region(FileMapRegion* r, char* addr, size_t bytes);\n@@ -404,1 +410,4 @@\n-  char* write_bitmap_region(CHeapBitMap* rw_ptrmap, CHeapBitMap* ro_ptrmap, ArchiveHeapInfo* heap_info,\n+  char* write_bitmap_region(CHeapBitMap* rw_ptrmap,\n+                            CHeapBitMap* ro_ptrmap,\n+                            ArchiveMappedHeapInfo* mapped_heap_info,\n+                            ArchiveStreamedHeapInfo* streamed_heap_info,\n@@ -406,1 +415,2 @@\n-  size_t write_heap_region(ArchiveHeapInfo* heap_info);\n+  size_t write_mapped_heap_region(ArchiveMappedHeapInfo* heap_info) NOT_CDS_JAVA_HEAP_RETURN_(0);\n+  size_t write_streamed_heap_region(ArchiveStreamedHeapInfo* heap_info) NOT_CDS_JAVA_HEAP_RETURN_(0);\n@@ -413,0 +423,3 @@\n+\n+  \/\/ Object loading support\n+  void  stream_heap_region() NOT_CDS_JAVA_HEAP_RETURN;\n@@ -414,2 +427,1 @@\n-  void  fixup_mapped_heap_region() NOT_CDS_JAVA_HEAP_RETURN;\n-  void  patch_heap_embedded_pointers() NOT_CDS_JAVA_HEAP_RETURN;\n+\n@@ -417,1 +429,0 @@\n-  MemRegion get_heap_region_requested_range() NOT_CDS_JAVA_HEAP_RETURN_(MemRegion());\n@@ -421,0 +432,1 @@\n+  char* map_forwarding_region();\n@@ -475,0 +487,1 @@\n+  bool  can_use_heap_region();\n@@ -479,6 +492,1 @@\n-  bool  map_heap_region_impl() NOT_CDS_JAVA_HEAP_RETURN_(false);\n-  void  dealloc_heap_region() NOT_CDS_JAVA_HEAP_RETURN;\n-  bool  can_use_heap_region();\n-  bool  load_heap_region() NOT_CDS_JAVA_HEAP_RETURN_(false);\n-  bool  map_heap_region() NOT_CDS_JAVA_HEAP_RETURN_(false);\n-  void  init_heap_region_relocation();\n+\n@@ -487,2 +495,1 @@\n-\n-  static MemRegion _mapped_heap_memregion;\n+  char* map_auxiliary_region(int region_index, bool read_only);\n@@ -491,3 +498,0 @@\n-  address heap_region_dumptime_address() NOT_CDS_JAVA_HEAP_RETURN_(nullptr);\n-  address heap_region_requested_address() NOT_CDS_JAVA_HEAP_RETURN_(nullptr);\n-  narrowOop encoded_heap_region_dumptime_address();\n","filename":"src\/hotspot\/share\/cds\/filemap.hpp","additions":34,"deletions":30,"binary":false,"changes":64,"status":"modified"},{"patch":"@@ -29,0 +29,2 @@\n+#include \"cds\/aotMappedHeapLoader.hpp\"\n+#include \"cds\/aotMappedHeapWriter.hpp\"\n@@ -32,0 +34,2 @@\n+#include \"cds\/aotStreamedHeapLoader.hpp\"\n+#include \"cds\/aotStreamedHeapWriter.hpp\"\n@@ -33,2 +37,1 @@\n-#include \"cds\/archiveHeapLoader.hpp\"\n-#include \"cds\/archiveHeapWriter.hpp\"\n+#include \"cds\/cds_globals.hpp\"\n@@ -39,1 +42,1 @@\n-#include \"cds\/heapShared.hpp\"\n+#include \"cds\/heapShared.inline.hpp\"\n@@ -67,0 +70,1 @@\n+#include \"runtime\/globals_extension.hpp\"\n@@ -94,1 +98,51 @@\n-DumpedInternedStrings *HeapShared::_dumped_interned_strings = nullptr;\n+\/\/ Anything that goes in the header must be thoroughly purged from uninitialized memory\n+\/\/ as it will be written to disk. Therefore, the constructors memset the memory to 0.\n+\/\/ This is not the prettiest thing, but we need to know every byte is initialized,\n+\/\/ including potential padding between fields.\n+\n+ArchiveMappedHeapHeader::ArchiveMappedHeapHeader(size_t ptrmap_start_pos,\n+                                                 size_t oopmap_start_pos,\n+                                                 HeapRootSegments root_segments) {\n+  memset((char*)this, 0, sizeof(*this));\n+  _ptrmap_start_pos = ptrmap_start_pos;\n+  _oopmap_start_pos = oopmap_start_pos;\n+  _root_segments = root_segments;\n+}\n+\n+ArchiveMappedHeapHeader::ArchiveMappedHeapHeader() {\n+  memset((char*)this, 0, sizeof(*this));\n+}\n+\n+ArchiveMappedHeapHeader ArchiveMappedHeapInfo::create_header() {\n+  return ArchiveMappedHeapHeader{_ptrmap_start_pos,\n+                                 _oopmap_start_pos,\n+                                 _root_segments};\n+}\n+\n+ArchiveStreamedHeapHeader::ArchiveStreamedHeapHeader(size_t forwarding_offset,\n+                                                     size_t roots_offset,\n+                                                     size_t num_roots,\n+                                                     size_t root_highest_object_index_table_offset,\n+                                                     size_t num_archived_objects) {\n+  memset((char*)this, 0, sizeof(*this));\n+  _forwarding_offset = forwarding_offset;\n+  _roots_offset = roots_offset;\n+  _num_roots = num_roots;\n+  _root_highest_object_index_table_offset = root_highest_object_index_table_offset;\n+  _num_archived_objects = num_archived_objects;\n+}\n+\n+ArchiveStreamedHeapHeader::ArchiveStreamedHeapHeader() {\n+  memset((char*)this, 0, sizeof(*this));\n+}\n+\n+ArchiveStreamedHeapHeader ArchiveStreamedHeapInfo::create_header() {\n+  return ArchiveStreamedHeapHeader{_forwarding_offset,\n+                                   _roots_offset,\n+                                   _num_roots,\n+                                   _root_highest_object_index_table_offset,\n+                                   _num_archived_objects};\n+}\n+\n+HeapArchiveMode HeapShared::_heap_load_mode = HeapArchiveMode::_uninitialized;\n+HeapArchiveMode HeapShared::_heap_write_mode = HeapArchiveMode::_uninitialized;\n@@ -145,2 +199,0 @@\n-GrowableArrayCHeap<OopHandle, mtClassShared>* HeapShared::_root_segments = nullptr;\n-int HeapShared::_root_segment_max_size_elems;\n@@ -242,4 +294,10 @@\n-bool HeapShared::has_been_archived(oop obj) {\n-  assert(CDSConfig::is_dumping_heap(), \"dump-time only\");\n-  OopHandle oh(&obj);\n-  return archived_object_cache()->get(oh) != nullptr;\n+bool HeapShared::is_archived_heap_in_use() {\n+  if (HeapShared::is_loading()) {\n+    if (HeapShared::is_loading_streaming_mode()) {\n+      return AOTStreamedHeapLoader::is_in_use();\n+    } else {\n+      return AOTMappedHeapLoader::is_in_use();\n+    }\n+  }\n+\n+  return false;\n@@ -248,4 +306,13 @@\n-int HeapShared::append_root(oop obj) {\n-  assert(CDSConfig::is_dumping_heap(), \"dump-time only\");\n-  if (obj != nullptr) {\n-    assert(has_been_archived(obj), \"must be\");\n+bool HeapShared::can_use_archived_heap() {\n+  FileMapInfo* static_mapinfo = FileMapInfo::current_info();\n+  if (static_mapinfo == nullptr) {\n+    return false;\n+  }\n+  if (!static_mapinfo->has_heap_region()) {\n+    return false;\n+  }\n+  if (!static_mapinfo->object_streaming_mode() &&\n+      !Universe::heap()->can_load_archived_objects() &&\n+      !UseG1GC) {\n+    \/\/ Incompatible object format\n+    return false;\n@@ -253,3 +320,1 @@\n-  \/\/ No GC should happen since we aren't scanning _pending_roots.\n-  assert(Thread::current() == (Thread*)VMThread::vm_thread(), \"should be in vm thread\");\n-  return _pending_roots->append(obj);\n+  return true;\n@@ -259,3 +324,11 @@\n-objArrayOop HeapShared::root_segment(int segment_idx) {\n-  if (CDSConfig::is_dumping_heap()) {\n-    assert(Thread::current() == (Thread*)VMThread::vm_thread(), \"should be in vm thread\");\n+bool HeapShared::is_too_large_to_archive(size_t size) {\n+  if (HeapShared::is_writing_streaming_mode()) {\n+    return false;\n+  } else {\n+    return AOTMappedHeapWriter::is_too_large_to_archive(size);\n+  }\n+}\n+\n+bool HeapShared::is_too_large_to_archive(oop obj) {\n+  if (HeapShared::is_writing_streaming_mode()) {\n+    return false;\n@@ -263,1 +336,1 @@\n-    assert(CDSConfig::is_using_archive(), \"must be\");\n+    return AOTMappedHeapWriter::is_too_large_to_archive(obj);\n@@ -265,0 +338,1 @@\n+}\n@@ -266,3 +340,3 @@\n-  objArrayOop segment = (objArrayOop)_root_segments->at(segment_idx).resolve();\n-  assert(segment != nullptr, \"should have been initialized\");\n-  return segment;\n+bool HeapShared::is_string_too_large_to_archive(oop string) {\n+  typeArrayOop value = java_lang_String::value_no_keepalive(string);\n+  return is_too_large_to_archive(value);\n@@ -271,2 +345,5 @@\n-void HeapShared::get_segment_indexes(int idx, int& seg_idx, int& int_idx) {\n-  assert(_root_segment_max_size_elems > 0, \"sanity\");\n+void HeapShared::initialize_loading_mode(HeapArchiveMode mode) {\n+  assert(_heap_load_mode == HeapArchiveMode::_uninitialized, \"already set?\");\n+  assert(mode != HeapArchiveMode::_uninitialized, \"sanity\");\n+  _heap_load_mode = mode;\n+};\n@@ -274,7 +351,90 @@\n-  \/\/ Try to avoid divisions for the common case.\n-  if (idx < _root_segment_max_size_elems) {\n-    seg_idx = 0;\n-    int_idx = idx;\n-  } else {\n-    seg_idx = idx \/ _root_segment_max_size_elems;\n-    int_idx = idx % _root_segment_max_size_elems;\n+void HeapShared::initialize_writing_mode() {\n+  assert(!FLAG_IS_ERGO(AOTStreamableObjects), \"Should not have been ergonomically set yet\");\n+\n+  if (!CDSConfig::is_dumping_archive()) {\n+    \/\/ We use FLAG_IS_CMDLINE below because we are specifically looking to warn\n+    \/\/ a user that explicitly sets the flag on the command line for a JVM that is\n+    \/\/ not dumping an archive.\n+    if (FLAG_IS_CMDLINE(AOTStreamableObjects)) {\n+      log_warning(cds)(\"-XX:%cAOTStreamableObjects was specified, \"\n+                       \"AOTStreamableObjects is only used for writing \"\n+                       \"the AOT cache.\",\n+                       AOTStreamableObjects ? '+' : '-');\n+    }\n+  }\n+\n+  \/\/ The below checks use !FLAG_IS_DEFAULT instead of FLAG_IS_CMDLINE\n+  \/\/ because the one step AOT cache creation transfers the AOTStreamableObjects\n+  \/\/ flag value from the training JVM to the assembly JVM using an environment\n+  \/\/ variable that sets the flag as ERGO in the assembly JVM.\n+  if (FLAG_IS_DEFAULT(AOTStreamableObjects)) {\n+    \/\/ By default, the value of AOTStreamableObjects should match !UseCompressedOops.\n+    FLAG_SET_DEFAULT(AOTStreamableObjects, !UseCompressedOops);\n+  } else if (!AOTStreamableObjects && UseZGC) {\n+    \/\/ Never write mapped heap with ZGC\n+    if (CDSConfig::is_dumping_archive()) {\n+      log_warning(cds)(\"Heap archiving without streaming not supported for -XX:+UseZGC\");\n+    }\n+    FLAG_SET_ERGO(AOTStreamableObjects, true);\n+  }\n+\n+  if (CDSConfig::is_dumping_archive()) {\n+    \/\/ Select default mode\n+    assert(_heap_write_mode == HeapArchiveMode::_uninitialized, \"already initialized?\");\n+    _heap_write_mode = AOTStreamableObjects ? HeapArchiveMode::_streaming : HeapArchiveMode::_mapping;\n+  }\n+}\n+\n+void HeapShared::initialize_streaming() {\n+  assert(is_loading_streaming_mode(), \"shouldn't call this\");\n+  if (can_use_archived_heap()) {\n+    AOTStreamedHeapLoader::initialize();\n+  }\n+}\n+\n+void HeapShared::enable_gc() {\n+  if (AOTStreamedHeapLoader::is_in_use()) {\n+    AOTStreamedHeapLoader::enable_gc();\n+  }\n+}\n+\n+void HeapShared::materialize_thread_object() {\n+  if (AOTStreamedHeapLoader::is_in_use()) {\n+    AOTStreamedHeapLoader::materialize_thread_object();\n+  }\n+}\n+\n+void HeapShared::add_to_dumped_interned_strings(oop string) {\n+  assert(HeapShared::is_writing_mapping_mode(), \"Only used by this mode\");\n+  AOTMappedHeapWriter::add_to_dumped_interned_strings(string);\n+}\n+\n+void HeapShared::finalize_initialization(FileMapInfo* static_mapinfo) {\n+  if (HeapShared::is_loading()) {\n+    if (HeapShared::is_loading_streaming_mode()) {\n+      \/\/ Heap initialization can be done only after vtables are initialized by ReadClosure.\n+      AOTStreamedHeapLoader::finish_initialization(static_mapinfo);\n+    } else {\n+      \/\/ Finish up archived heap initialization. These must be\n+      \/\/ done after ReadClosure.\n+      AOTMappedHeapLoader::finish_initialization(static_mapinfo);\n+    }\n+  }\n+}\n+\n+HeapShared::CachedOopInfo* HeapShared::get_cached_oop_info(oop obj) {\n+  OopHandle oh(Universe::vm_global(), obj);\n+  CachedOopInfo* result = _archived_object_cache->get(oh);\n+  oh.release(Universe::vm_global());\n+  return result;\n+}\n+\n+bool HeapShared::has_been_archived(oop obj) {\n+  assert(CDSConfig::is_dumping_heap(), \"dump-time only\");\n+  return get_cached_oop_info(obj) != nullptr;\n+}\n+\n+int HeapShared::append_root(oop obj) {\n+  assert(CDSConfig::is_dumping_heap(), \"dump-time only\");\n+  if (obj != nullptr) {\n+    assert(has_been_archived(obj), \"must be\");\n@@ -282,0 +442,2 @@\n+  \/\/ No GC should happen since we aren't scanning _pending_roots.\n+  assert(Thread::current() == (Thread*)VMThread::vm_thread(), \"should be in vm thread\");\n@@ -283,2 +445,1 @@\n-  assert(idx == seg_idx * _root_segment_max_size_elems + int_idx,\n-         \"sanity: %d index maps to %d segment and %d internal\", idx, seg_idx, int_idx);\n+  return _pending_roots->append(obj);\n@@ -287,1 +448,0 @@\n-\/\/ Returns an objArray that contains all the roots of the archived objects\n@@ -291,4 +451,10 @@\n-  assert(!_root_segments->is_empty(), \"must have loaded shared heap\");\n-  int seg_idx, int_idx;\n-  get_segment_indexes(index, seg_idx, int_idx);\n-  oop result = root_segment(seg_idx)->obj_at(int_idx);\n+  assert(is_archived_heap_in_use(), \"getting roots into heap that is not used\");\n+\n+  oop result;\n+  if (HeapShared::is_loading_streaming_mode()) {\n+    result = AOTStreamedHeapLoader::get_root(index);\n+  } else {\n+    assert(HeapShared::is_loading_mapping_mode(), \"must be\");\n+    result = AOTMappedHeapLoader::get_root(index);\n+  }\n+\n@@ -298,0 +464,1 @@\n+\n@@ -301,0 +468,6 @@\n+void HeapShared::finish_materialize_objects() {\n+  if (AOTStreamedHeapLoader::is_in_use()) {\n+    AOTStreamedHeapLoader::finish_materialize_objects();\n+  }\n+}\n+\n@@ -304,3 +477,1 @@\n-  if (ArchiveHeapLoader::is_in_use()) {\n-    int seg_idx, int_idx;\n-    get_segment_indexes(index, seg_idx, int_idx);\n+  if (is_archived_heap_in_use()) {\n@@ -308,2 +479,7 @@\n-      oop old = root_segment(seg_idx)->obj_at(int_idx);\n-      log_debug(aot, heap)(\"Clearing root %d: was \" PTR_FORMAT, index, p2i(old));\n+      log_debug(aot, heap)(\"Clearing root %d: was %zu\", index, p2i(get_root(index, false \/* clear *\/)));\n+    }\n+    if (HeapShared::is_loading_streaming_mode()) {\n+      AOTStreamedHeapLoader::clear_root(index);\n+    } else {\n+      assert(HeapShared::is_loading_mapping_mode(), \"must be\");\n+      AOTMappedHeapLoader::clear_root(index);\n@@ -311,1 +487,0 @@\n-    root_segment(seg_idx)->obj_at_put(int_idx, nullptr);\n@@ -323,1 +498,1 @@\n-  if (ArchiveHeapWriter::is_too_large_to_archive(obj->size())) {\n+  if (is_too_large_to_archive(obj)) {\n@@ -328,2 +503,1 @@\n-  } else {\n-    AOTOopChecker::check(obj); \/\/ Make sure contents of this oop are safe.\n+  }\n@@ -331,3 +505,2 @@\n-    count_allocation(obj->size());\n-    ArchiveHeapWriter::add_source_obj(obj);\n-    CachedOopInfo info = make_cached_oop_info(obj, referrer);\n+  AOTOopChecker::check(obj); \/\/ Make sure contents of this oop are safe.\n+  count_allocation(obj->size());\n@@ -335,4 +508,5 @@\n-    OopHandle oh(Universe::vm_global(), obj);\n-    archived_object_cache()->put_when_absent(oh, info);\n-    archived_object_cache()->maybe_grow();\n-    mark_native_pointers(obj);\n+  if (HeapShared::is_writing_streaming_mode()) {\n+    AOTStreamedHeapWriter::add_source_obj(obj);\n+  } else {\n+    AOTMappedHeapWriter::add_source_obj(obj);\n+  }\n@@ -340,21 +514,4 @@\n-    Klass* k = obj->klass();\n-    if (k->is_instance_klass()) {\n-      \/\/ Whenever we see a non-array Java object of type X, we mark X to be aot-initialized.\n-      \/\/ This ensures that during the production run, whenever Java code sees a cached object\n-      \/\/ of type X, we know that X is already initialized. (see TODO comment below ...)\n-\n-      if (InstanceKlass::cast(k)->is_enum_subclass()\n-          \/\/ We can't rerun <clinit> of enum classes (see cdsEnumKlass.cpp) so\n-          \/\/ we must store them as AOT-initialized.\n-          || (subgraph_info == _dump_time_special_subgraph))\n-          \/\/ TODO: we do this only for the special subgraph for now. Extending this to\n-          \/\/ other subgraphs would require more refactoring of the core library (such as\n-          \/\/ move some initialization logic into runtimeSetup()).\n-          \/\/\n-          \/\/ For the other subgraphs, we have a weaker mechanism to ensure that\n-          \/\/ all classes in a subgraph are initialized before the subgraph is programmatically\n-          \/\/ returned from jdk.internal.misc.CDS::initializeFromArchive().\n-          \/\/ See HeapShared::initialize_from_archived_subgraph().\n-      {\n-        AOTArtifactFinder::add_aot_inited_class(InstanceKlass::cast(k));\n-      }\n+  OopHandle oh(Universe::vm_global(), obj);\n+  CachedOopInfo info = make_cached_oop_info(obj, referrer);\n+  archived_object_cache()->put_when_absent(oh, info);\n+  archived_object_cache()->maybe_grow();\n@@ -362,13 +519,32 @@\n-      if (java_lang_Class::is_instance(obj)) {\n-        Klass* mirror_k = java_lang_Class::as_Klass(obj);\n-        if (mirror_k != nullptr) {\n-          AOTArtifactFinder::add_cached_class(mirror_k);\n-        }\n-      } else if (java_lang_invoke_ResolvedMethodName::is_instance(obj)) {\n-        Method* m = java_lang_invoke_ResolvedMethodName::vmtarget(obj);\n-        if (m != nullptr) {\n-          if (RegeneratedClasses::has_been_regenerated(m)) {\n-            m = RegeneratedClasses::get_regenerated_object(m);\n-          }\n-          InstanceKlass* method_holder = m->method_holder();\n-          AOTArtifactFinder::add_cached_class(method_holder);\n+  Klass* k = obj->klass();\n+  if (k->is_instance_klass()) {\n+    \/\/ Whenever we see a non-array Java object of type X, we mark X to be aot-initialized.\n+    \/\/ This ensures that during the production run, whenever Java code sees a cached object\n+    \/\/ of type X, we know that X is already initialized. (see TODO comment below ...)\n+\n+    if (InstanceKlass::cast(k)->is_enum_subclass()\n+        \/\/ We can't rerun <clinit> of enum classes (see cdsEnumKlass.cpp) so\n+        \/\/ we must store them as AOT-initialized.\n+        || (subgraph_info == _dump_time_special_subgraph))\n+        \/\/ TODO: we do this only for the special subgraph for now. Extending this to\n+        \/\/ other subgraphs would require more refactoring of the core library (such as\n+        \/\/ move some initialization logic into runtimeSetup()).\n+        \/\/\n+        \/\/ For the other subgraphs, we have a weaker mechanism to ensure that\n+        \/\/ all classes in a subgraph are initialized before the subgraph is programmatically\n+        \/\/ returned from jdk.internal.misc.CDS::initializeFromArchive().\n+        \/\/ See HeapShared::initialize_from_archived_subgraph().\n+    {\n+      AOTArtifactFinder::add_aot_inited_class(InstanceKlass::cast(k));\n+    }\n+\n+    if (java_lang_Class::is_instance(obj)) {\n+      Klass* mirror_k = java_lang_Class::as_Klass(obj);\n+      if (mirror_k != nullptr) {\n+        AOTArtifactFinder::add_cached_class(mirror_k);\n+      }\n+    } else if (java_lang_invoke_ResolvedMethodName::is_instance(obj)) {\n+      Method* m = java_lang_invoke_ResolvedMethodName::vmtarget(obj);\n+      if (m != nullptr) {\n+        if (RegeneratedClasses::has_been_regenerated(m)) {\n+          m = RegeneratedClasses::get_regenerated_object(m);\n@@ -376,0 +552,2 @@\n+        InstanceKlass* method_holder = m->method_holder();\n+        AOTArtifactFinder::add_cached_class(method_holder);\n@@ -378,0 +556,1 @@\n+  }\n@@ -379,13 +558,12 @@\n-    if (log_is_enabled(Debug, aot, heap)) {\n-      ResourceMark rm;\n-      LogTarget(Debug, aot, heap) log;\n-      LogStream out(log);\n-      out.print(\"Archived heap object \" PTR_FORMAT \" : %s \",\n-                p2i(obj), obj->klass()->external_name());\n-      if (java_lang_Class::is_instance(obj)) {\n-        Klass* k = java_lang_Class::as_Klass(obj);\n-        if (k != nullptr) {\n-          out.print(\"%s\", k->external_name());\n-        } else {\n-          out.print(\"primitive\");\n-        }\n+  if (log_is_enabled(Debug, aot, heap)) {\n+    ResourceMark rm;\n+    LogTarget(Debug, aot, heap) log;\n+    LogStream out(log);\n+    out.print(\"Archived heap object \" PTR_FORMAT \" : %s \",\n+              p2i(obj), obj->klass()->external_name());\n+    if (java_lang_Class::is_instance(obj)) {\n+      Klass* k = java_lang_Class::as_Klass(obj);\n+      if (k != nullptr) {\n+        out.print(\"%s\", k->external_name());\n+      } else {\n+        out.print(\"primitive\");\n@@ -393,3 +571,1 @@\n-      out.cr();\n-\n-    return true;\n+    out.cr();\n@@ -398,0 +574,2 @@\n+\n+  return true;\n@@ -440,3 +618,3 @@\n-void HeapShared::init_dumping() {\n-  _scratch_objects_table = new (mtClass)MetaspaceObjToOopHandleTable();\n-  _pending_roots = new GrowableArrayCHeap<oop, mtClassShared>(500);\n+ void HeapShared::init_dumping() {\n+   _scratch_objects_table = new (mtClass)MetaspaceObjToOopHandleTable();\n+   _pending_roots = new GrowableArrayCHeap<oop, mtClassShared>(500);\n@@ -645,1 +823,1 @@\n-    if (rr != nullptr && !ArchiveHeapWriter::is_too_large_to_archive(rr)) {\n+    if (rr != nullptr && !HeapShared::is_too_large_to_archive(rr)) {\n@@ -653,0 +831,1 @@\n+  assert(HeapShared::is_writing_mapping_mode(), \"should not reach here\");\n@@ -665,9 +844,0 @@\n-void HeapShared::mark_native_pointers(oop orig_obj) {\n-  if (java_lang_Class::is_instance(orig_obj)) {\n-    ArchiveHeapWriter::mark_native_pointer(orig_obj, java_lang_Class::klass_offset());\n-    ArchiveHeapWriter::mark_native_pointer(orig_obj, java_lang_Class::array_klass_offset());\n-  } else if (java_lang_invoke_ResolvedMethodName::is_instance(orig_obj)) {\n-    ArchiveHeapWriter::mark_native_pointer(orig_obj, java_lang_invoke_ResolvedMethodName::vmtarget_offset());\n-  }\n-}\n-\n@@ -702,1 +872,1 @@\n-    if (UseCompressedOops || UseG1GC) {\n+    if (HeapShared::is_writing_mapping_mode() && (UseG1GC || UseCompressedOops)) {\n@@ -718,1 +888,3 @@\n-  archive_strings();\n+  if (is_writing_mapping_mode()) {\n+    archive_strings();\n+  }\n@@ -722,1 +894,1 @@\n-void HeapShared::write_heap(ArchiveHeapInfo *heap_info) {\n+void HeapShared::write_heap(ArchiveMappedHeapInfo* mapped_heap_info, ArchiveStreamedHeapInfo* streamed_heap_info) {\n@@ -729,2 +901,7 @@\n-  StringTable::write_shared_table();\n-  ArchiveHeapWriter::write(_pending_roots, heap_info);\n+  if (HeapShared::is_writing_mapping_mode()) {\n+    StringTable::write_shared_table();\n+    AOTMappedHeapWriter::write(_pending_roots, mapped_heap_info);\n+  } else {\n+    assert(HeapShared::is_writing_streaming_mode(), \"are there more modes?\");\n+    AOTStreamedHeapWriter::write(_pending_roots, streamed_heap_info);\n+  }\n@@ -1053,1 +1230,1 @@\n-  CompactHashtableWriter writer(d_table->_count, &stats);\n+  CompactHashtableWriter writer(d_table->number_of_entries(), &stats);\n@@ -1071,13 +1248,0 @@\n-void HeapShared::add_root_segment(objArrayOop segment_oop) {\n-  assert(segment_oop != nullptr, \"must be\");\n-  assert(ArchiveHeapLoader::is_in_use(), \"must be\");\n-  if (_root_segments == nullptr) {\n-    _root_segments = new GrowableArrayCHeap<OopHandle, mtClassShared>(10);\n-  }\n-  _root_segments->push(OopHandle(Universe::vm_global(), segment_oop));\n-}\n-\n-void HeapShared::init_root_segment_sizes(int max_size_elems) {\n-  _root_segment_max_size_elems = max_size_elems;\n-}\n-\n@@ -1104,4 +1268,4 @@\n-    VM_Verify verify_op;\n-    VMThread::execute(&verify_op);\n-\n-    if (VerifyArchivedFields > 1 && is_init_completed()) {\n+    if (VerifyArchivedFields == 1) {\n+      VM_Verify verify_op;\n+      VMThread::execute(&verify_op);\n+    } else if (VerifyArchivedFields == 2 && is_init_completed()) {\n@@ -1133,1 +1297,1 @@\n-  if (!ArchiveHeapLoader::is_in_use()) {\n+  if (!is_archived_heap_in_use()) {\n@@ -1192,1 +1356,1 @@\n-  if (!ArchiveHeapLoader::is_in_use()) {\n+  if (!is_archived_heap_in_use()) {\n@@ -1224,1 +1388,1 @@\n-  if (!ArchiveHeapLoader::is_in_use()) {\n+  if (!is_archived_heap_in_use()) {\n@@ -1364,3 +1528,0 @@\n-  \/\/ Load the subgraph entry fields from the record and store them back to\n-  \/\/ the corresponding fields within the mirror.\n-  oop m = k->java_mirror();\n@@ -1374,0 +1535,2 @@\n+      \/\/ Load the subgraph entry fields from the record and store them back to\n+      \/\/ the corresponding fields within the mirror.\n@@ -1375,0 +1538,1 @@\n+      oop m = k->java_mirror();\n@@ -1453,1 +1617,1 @@\n-    if (!CompressedOops::is_null(obj)) {\n+    if (obj != nullptr) {\n@@ -1502,1 +1666,1 @@\n-  if (ArchiveHeapLoader::is_in_use()) {\n+  if (is_archived_heap_in_use()) {\n@@ -1747,2 +1911,2 @@\n-    oop obj = RawAccess<>::oop_load(p);\n-    if (!CompressedOops::is_null(obj)) {\n+    oop obj = HeapAccess<>::oop_load(p);\n+    if (obj != nullptr) {\n@@ -1828,3 +1992,3 @@\n-int HeapShared::_num_new_walked_objs;\n-int HeapShared::_num_new_archived_objs;\n-int HeapShared::_num_old_recorded_klasses;\n+size_t HeapShared::_num_new_walked_objs;\n+size_t HeapShared::_num_new_archived_objs;\n+size_t HeapShared::_num_old_recorded_klasses;\n@@ -1832,5 +1996,5 @@\n-int HeapShared::_num_total_subgraph_recordings = 0;\n-int HeapShared::_num_total_walked_objs = 0;\n-int HeapShared::_num_total_archived_objs = 0;\n-int HeapShared::_num_total_recorded_klasses = 0;\n-int HeapShared::_num_total_verifications = 0;\n+size_t HeapShared::_num_total_subgraph_recordings = 0;\n+size_t HeapShared::_num_total_walked_objs = 0;\n+size_t HeapShared::_num_total_archived_objs = 0;\n+size_t HeapShared::_num_total_recorded_klasses = 0;\n+size_t HeapShared::_num_total_verifications = 0;\n@@ -1859,1 +2023,1 @@\n-  int num_new_recorded_klasses = get_subgraph_info(k)->num_subgraph_object_klasses() -\n+  size_t num_new_recorded_klasses = get_subgraph_info(k)->num_subgraph_object_klasses() -\n@@ -1862,1 +2026,1 @@\n-                      \"walked %d objs, archived %d new objs, recorded %d classes\",\n+                      \"walked %zu objs, archived %zu new objs, recorded %zu classes\",\n@@ -2049,1 +2213,1 @@\n-  if (k != nullptr && ArchiveHeapLoader::is_in_use()) {\n+  if (k != nullptr && is_archived_heap_in_use()) {\n@@ -2070,1 +2234,0 @@\n-    _dumped_interned_strings = new (mtClass)DumpedInternedStrings(INITIAL_TABLE_SIZE, MAX_TABLE_SIZE);\n@@ -2075,0 +2238,8 @@\n+void HeapShared::init_heap_writer() {\n+  if (HeapShared::is_writing_streaming_mode()) {\n+    AOTStreamedHeapWriter::init();\n+  } else {\n+    AOTMappedHeapWriter::init();\n+  }\n+}\n+\n@@ -2117,1 +2288,1 @@\n-  log_info(aot, heap)(\"Archived subgraph records = %d\",\n+  log_info(aot, heap)(\"Archived subgraph records = %zu\",\n@@ -2119,3 +2290,3 @@\n-  log_info(aot, heap)(\"  Walked %d objects\", _num_total_walked_objs);\n-  log_info(aot, heap)(\"  Archived %d objects\", _num_total_archived_objs);\n-  log_info(aot, heap)(\"  Recorded %d klasses\", _num_total_recorded_klasses);\n+  log_info(aot, heap)(\"  Walked %zu objects\", _num_total_walked_objs);\n+  log_info(aot, heap)(\"  Archived %zu objects\", _num_total_archived_objs);\n+  log_info(aot, heap)(\"  Recorded %zu klasses\", _num_total_recorded_klasses);\n@@ -2128,1 +2299,1 @@\n-  log_info(aot, heap)(\"  Verified %d references\", _num_total_verifications);\n+  log_info(aot, heap)(\"  Verified %zu references\", _num_total_verifications);\n@@ -2132,16 +2303,5 @@\n-\/\/ Keep track of the contents of the archived interned string table. This table\n-\/\/ is used only by CDSHeapVerifier.\n-void HeapShared::add_to_dumped_interned_strings(oop string) {\n-  assert_at_safepoint(); \/\/ DumpedInternedStrings uses raw oops\n-  assert(!ArchiveHeapWriter::is_string_too_large_to_archive(string), \"must be\");\n-  bool created;\n-  _dumped_interned_strings->put_if_absent(string, true, &created);\n-  if (created) {\n-    \/\/ Prevent string deduplication from changing the value field to\n-    \/\/ something not in the archive.\n-    java_lang_String::set_deduplication_forbidden(string);\n-    _dumped_interned_strings->maybe_grow();\n-  }\n-}\n-\n-  return _dumped_interned_strings->get(o) != nullptr;\n+  if (is_writing_mapping_mode()) {\n+    return AOTMappedHeapWriter::is_dumped_interned_string(o);\n+  } else {\n+    return AOTStreamedHeapWriter::is_dumped_interned_string(o);\n+  }\n@@ -2157,4 +2317,6 @@\n-  delete _dumped_interned_strings;\n-  _dumped_interned_strings = nullptr;\n-\n-  ArchiveHeapWriter::delete_tables_with_raw_oops();\n+  if (is_writing_mapping_mode()) {\n+    AOTMappedHeapWriter::delete_tables_with_raw_oops();\n+  } else {\n+    assert(is_writing_streaming_mode(), \"what other mode?\");\n+    AOTStreamedHeapWriter::delete_tables_with_raw_oops();\n+  }\n@@ -2180,2 +2342,2 @@\n-  int _num_total_oops;\n-  int _num_null_oops;\n+  size_t _num_total_oops;\n+  size_t _num_null_oops;\n@@ -2207,2 +2369,2 @@\n-  int num_total_oops() const { return _num_total_oops; }\n-  int num_null_oops()  const { return _num_null_oops; }\n+  size_t num_total_oops() const { return _num_total_oops; }\n+  size_t num_null_oops()  const { return _num_null_oops; }\n@@ -2257,0 +2419,29 @@\n+bool HeapShared::is_metadata_field(oop src_obj, int offset) {\n+  bool result = false;\n+  do_metadata_offsets(src_obj, [&](int metadata_offset) {\n+    if (metadata_offset == offset) {\n+      result = true;\n+    }\n+  });\n+  return result;\n+}\n+\n+void HeapShared::remap_dumped_metadata(oop src_obj, address archived_object) {\n+  do_metadata_offsets(src_obj, [&](int offset) {\n+    Metadata** buffered_field_addr = (Metadata**)(archived_object + offset);\n+    Metadata* native_ptr = *buffered_field_addr;\n+\n+    if (native_ptr == nullptr) {\n+      return;\n+    }\n+\n+    if (RegeneratedClasses::has_been_regenerated(native_ptr)) {\n+      native_ptr = RegeneratedClasses::get_regenerated_object(native_ptr);\n+    }\n+\n+    address buffered_native_ptr = ArchiveBuilder::current()->get_buffered_addr((address)native_ptr);\n+    address requested_native_ptr = ArchiveBuilder::current()->to_requested(buffered_native_ptr);\n+    *buffered_field_addr = (Metadata*)requested_native_ptr;\n+  });\n+}\n+\n","filename":"src\/hotspot\/share\/cds\/heapShared.cpp","additions":384,"deletions":193,"binary":false,"changes":577,"status":"modified"},{"patch":"@@ -1066,3 +1066,1 @@\n-\n-    assert(compiler->type() == compiler_c2 ||\n-           offsets->value(CodeOffsets::Exceptions) != -1, \"must have exception entry\");\n+    assert(offsets->value(CodeOffsets::Exceptions) != -1, \"must have exception entry\");\n","filename":"src\/hotspot\/share\/ci\/ciEnv.cpp","additions":1,"deletions":3,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -400,0 +400,11 @@\n+ciField* ciInstanceKlass::get_non_static_field_by_offset(const int field_offset) {\n+  for (int i = 0, len = nof_nonstatic_fields(); i < len; i++) {\n+    ciField* field = nonstatic_field_at(i);\n+    int field_off = field->offset_in_bytes();\n+    if (field_off == field_offset) {\n+      return field;\n+    }\n+  }\n+  return nullptr;\n+}\n+\n@@ -404,8 +415,1 @@\n-    for (int i = 0, len = nof_nonstatic_fields(); i < len; i++) {\n-      ciField* field = nonstatic_field_at(i);\n-      int field_off = field->offset_in_bytes();\n-      if (field_off == field_offset) {\n-        return field;\n-      }\n-    }\n-    return nullptr;\n+    return get_non_static_field_by_offset(field_offset);\n@@ -474,0 +478,25 @@\n+\/\/ This is essentially a shortcut for:\n+\/\/   get_field_by_offset(field_offset, is_static)->layout_type()\n+\/\/ except this does not require allocating memory for a new ciField\n+BasicType ciInstanceKlass::get_field_type_by_offset(const int field_offset, const bool is_static) {\n+  if (!is_static) {\n+    ciField* field = get_non_static_field_by_offset(field_offset);\n+    return field != nullptr ? field->layout_type() : T_ILLEGAL;\n+  }\n+\n+  \/\/ Avoid allocating a new ciField by obtaining the field type directly\n+  VM_ENTRY_MARK;\n+  InstanceKlass* k = get_instanceKlass();\n+  fieldDescriptor fd;\n+  if (!k->find_field_from_offset(field_offset, is_static, &fd)) {\n+    return T_ILLEGAL;\n+  }\n+\n+  \/\/ Reproduce the behavior of ciField::layout_type\n+  BasicType field_type = fd.field_type();\n+  if (is_reference_type(field_type)) {\n+    return T_OBJECT;\n+  }\n+  return type2field[make(field_type)->basic_type()];\n+}\n+\n","filename":"src\/hotspot\/share\/ci\/ciInstanceKlass.cpp","additions":37,"deletions":8,"binary":false,"changes":45,"status":"modified"},{"patch":"@@ -98,0 +98,2 @@\n+  ciField* get_non_static_field_by_offset(int field_offset);\n+\n@@ -232,0 +234,1 @@\n+  BasicType get_field_type_by_offset(int field_offset, bool is_static);\n","filename":"src\/hotspot\/share\/ci\/ciInstanceKlass.hpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -28,2 +28,1 @@\n-#include \"cds\/archiveHeapLoader.hpp\"\n-#include \"cds\/heapShared.hpp\"\n+#include \"cds\/heapShared.inline.hpp\"\n@@ -984,1 +983,1 @@\n-    if (ArchiveHeapLoader::is_in_use()) {\n+    if (HeapShared::is_archived_heap_in_use()) {\n","filename":"src\/hotspot\/share\/classfile\/javaClasses.cpp","additions":2,"deletions":3,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -25,0 +25,1 @@\n+#include \"cds\/aotMappedHeapLoader.hpp\"\n@@ -26,3 +27,1 @@\n-#include \"cds\/archiveHeapLoader.inline.hpp\"\n-#include \"cds\/archiveHeapWriter.hpp\"\n-#include \"cds\/heapShared.hpp\"\n+#include \"cds\/heapShared.inline.hpp\"\n@@ -83,1 +82,1 @@\n-  assert(ArchiveHeapLoader::is_in_use(), \"sanity\");\n+  assert(AOTMappedHeapLoader::is_in_use(), \"sanity\");\n@@ -319,0 +318,1 @@\n+}\n@@ -321,4 +321,2 @@\n-  if (ArchiveHeapLoader::is_in_use()) {\n-    _shared_strings_array = OopHandle(Universe::vm_global(), HeapShared::get_root(_shared_strings_array_root_index));\n-  }\n-#endif\n+void StringTable::load_shared_strings_array() {\n+  _shared_strings_array = OopHandle(Universe::vm_global(), HeapShared::get_root(_shared_strings_array_root_index));\n@@ -326,0 +324,1 @@\n+#endif\n@@ -912,1 +911,1 @@\n-      _shared_table.iterate(&pss);\n+      _shared_table.iterate_all(&pss);\n@@ -935,0 +934,1 @@\n+  assert(HeapShared::is_loading_mapping_mode(), \"should not reach here\");\n@@ -939,0 +939,3 @@\n+  if (!AOTMappedHeapLoader::is_in_use()) {\n+    return nullptr;\n+  }\n@@ -946,0 +949,3 @@\n+  if (!AOTMappedHeapLoader::is_in_use()) {\n+    return nullptr;\n+  }\n@@ -958,0 +964,2 @@\n+  assert(HeapShared::is_writing_mapping_mode(), \"should not reach here\");\n+\n@@ -980,1 +988,1 @@\n-  if (!ArchiveHeapWriter::is_too_large_to_archive(single_array_size)) {\n+  if (!HeapShared::is_too_large_to_archive(single_array_size)) {\n@@ -991,1 +999,1 @@\n-    if (ArchiveHeapWriter::is_too_large_to_archive(secondary_array_size)) {\n+    if (HeapShared::is_too_large_to_archive(secondary_array_size)) {\n@@ -1017,1 +1025,1 @@\n-      assert(!ArchiveHeapWriter::is_too_large_to_archive(secondary), \"sanity\");\n+      assert(!HeapShared::is_too_large_to_archive(secondary), \"sanity\");\n@@ -1027,0 +1035,1 @@\n+  assert(HeapShared::is_writing_mapping_mode(), \"should not reach here\");\n@@ -1030,1 +1039,1 @@\n-    if (ArchiveHeapWriter::is_too_large_to_archive(next_size)) {\n+    if (HeapShared::is_too_large_to_archive(next_size)) {\n@@ -1053,0 +1062,1 @@\n+  assert(HeapShared::is_writing_mapping_mode(), \"should not reach here\");\n@@ -1060,1 +1070,1 @@\n-    if (string != nullptr && !ArchiveHeapWriter::is_string_too_large_to_archive(string)) {\n+    if (string != nullptr && !HeapShared::is_string_too_large_to_archive(string)) {\n@@ -1062,1 +1072,1 @@\n-      \/\/ - If there are no other refernences to it, it won't be stored into the archive,\n+      \/\/ - If there are no other references to it, it won't be stored into the archive,\n@@ -1064,1 +1074,1 @@\n-      \/\/ - If there's a referece to it, we will report an error inside HeapShared.cpp and\n+      \/\/ - If there's a reference to it, we will report an error inside HeapShared.cpp and\n@@ -1098,1 +1108,1 @@\n-    if (string != nullptr && !ArchiveHeapWriter::is_string_too_large_to_archive(string)) {\n+    if (string != nullptr && !HeapShared::is_string_too_large_to_archive(string)) {\n@@ -1112,0 +1122,1 @@\n+  assert(HeapShared::is_writing_mapping_mode(), \"should not reach here\");\n@@ -1121,1 +1132,1 @@\n-  } else if (!ArchiveHeapLoader::is_in_use()) {\n+  } else if (!AOTMappedHeapLoader::is_in_use()) {\n","filename":"src\/hotspot\/share\/classfile\/stringTable.cpp","additions":29,"deletions":18,"binary":false,"changes":47,"status":"modified"},{"patch":"@@ -1307,0 +1307,1 @@\n+  assert(java_lang_Class::module(java_mirror) != nullptr, \"must have been archived\");\n@@ -1324,1 +1325,0 @@\n-  assert(java_lang_Class::module(java_mirror) != nullptr, \"must have been archived\");\n","filename":"src\/hotspot\/share\/classfile\/systemDictionary.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -907,1 +907,1 @@\n-    if (AdapterHandlerLibrary::contains(this)) {\n+    if (is_adapter_blob()) {\n@@ -910,0 +910,1 @@\n+      return;\n","filename":"src\/hotspot\/share\/code\/codeBlob.cpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -1320,1 +1320,1 @@\n-    _deopt_handler_entry_offset    = 0;\n+    _deopt_handler_offset    = 0;\n@@ -1460,1 +1460,1 @@\n-  _deopt_handler_entry_offset   = nm._deopt_handler_entry_offset;\n+  _deopt_handler_offset         = nm._deopt_handler_offset;\n@@ -1722,1 +1722,1 @@\n-        _deopt_handler_entry_offset    = code_offset() + offsets->value(CodeOffsets::Deopt);\n+        _deopt_handler_offset    = code_offset() + offsets->value(CodeOffsets::Deopt);\n@@ -1724,1 +1724,1 @@\n-        _deopt_handler_entry_offset    = -1;\n+        _deopt_handler_offset    = -1;\n@@ -1730,0 +1730,1 @@\n+      assert(offsets->value(CodeOffsets::Exceptions) != -1, \"must be set\");\n@@ -1732,10 +1733,2 @@\n-      bool has_exception_handler = (offsets->value(CodeOffsets::Exceptions) != -1);\n-      assert(has_exception_handler == (compiler->type() != compiler_c2),\n-             \"C2 compiler doesn't provide exception handler stub code.\");\n-      if (has_exception_handler) {\n-        _exception_offset = _stub_offset + offsets->value(CodeOffsets::Exceptions);\n-      } else {\n-        _exception_offset = -1;\n-      }\n-\n-      _deopt_handler_entry_offset = _stub_offset + offsets->value(CodeOffsets::Deopt);\n+      _exception_offset          = _stub_offset + offsets->value(CodeOffsets::Exceptions);\n+      _deopt_handler_offset      = _stub_offset + offsets->value(CodeOffsets::Deopt);\n@@ -4056,1 +4049,1 @@\n-  if (JVMCI_ONLY(_deopt_handler_entry_offset != -1 &&) pos == deopt_handler_entry()) label = \"[Deopt Handler Entry Point]\";\n+  if (JVMCI_ONLY(_deopt_handler_offset != -1 &&) pos == deopt_handler_begin()) label = \"[Deopt Handler Code]\";\n","filename":"src\/hotspot\/share\/code\/nmethod.cpp","additions":8,"deletions":15,"binary":false,"changes":23,"status":"modified"},{"patch":"@@ -237,1 +237,1 @@\n-  int _deopt_handler_entry_offset;\n+  int _deopt_handler_offset;\n@@ -625,1 +625,1 @@\n-  address deopt_handler_entry   () const { return           header_begin() + _deopt_handler_entry_offset    ; }\n+  address deopt_handler_begin   () const { return           header_begin() + _deopt_handler_offset    ; }\n","filename":"src\/hotspot\/share\/code\/nmethod.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -504,0 +504,1 @@\n+  virtual size_t bootstrap_max_memory() const;\n","filename":"src\/hotspot\/share\/gc\/shared\/collectedHeap.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -53,1 +53,1 @@\n-                                                               ClosureType* cl, bool use_write_table, uint worker_id) {\n+                                                ClosureType* cl, bool use_write_table, uint worker_id) {\n@@ -105,1 +105,1 @@\n-  const HeapWord* tams = (ctx == nullptr ? region->bottom() : ctx->top_at_mark_start(region));\n+  HeapWord* const tams = (ctx == nullptr ? region->bottom() : ctx->top_at_mark_start(region));\n@@ -165,0 +165,4 @@\n+      if (end_addr <= left) {\n+        \/\/ The range of addresses to be scanned is empty\n+        continue;\n+      }\n@@ -170,5 +174,5 @@\n-      \/\/ NOTE: We'll not call block_start() repeatedly\n-      \/\/ on a very large object if its head card is dirty. If not,\n-      \/\/ (i.e. the head card is clean) we'll call it each time we\n-      \/\/ process a new dirty range on the object. This is always\n-      \/\/ the case for large object arrays, which are typically more\n+      \/\/ NOTE: We'll not call first_object_start() repeatedly\n+      \/\/ on a very large object, i.e. one spanning multiple cards,\n+      \/\/ if its head card is dirty. If not, (i.e. its head card is clean)\n+      \/\/ we'll call it each time we process a new dirty range on the object.\n+      \/\/ This is always the case for large object arrays, which are typically more\n@@ -176,1 +180,12 @@\n-      HeapWord* p = _scc->block_start(dirty_l);\n+      assert(ctx != nullptr || heap->old_generation()->is_parsable(), \"Error\");\n+      HeapWord* p = _scc->first_object_start(dirty_l, ctx, tams, right);\n+      assert((p == nullptr) || (p < right), \"No first object found is denoted by nullptr, p: \"\n+             PTR_FORMAT \", right: \" PTR_FORMAT \", end_addr: \" PTR_FORMAT \", next card addr: \" PTR_FORMAT,\n+             p2i(p), p2i(right), p2i(end_addr), p2i(_rs->addr_for_card_index(dirty_r + 1)));\n+      if (p == nullptr) {\n+        \/\/ There are no live objects to be scanned in this dirty range.  cur_index identifies first card in this\n+        \/\/ uninteresting dirty range.  At top of next loop iteration, we will either end the looop\n+        \/\/ (because cur_index < start_card_index) or we will begin the search for a range of clean cards.\n+        continue;\n+      }\n+\n@@ -178,0 +193,3 @@\n+      assert(oopDesc::is_oop(obj), \"Not an object at \" PTR_FORMAT \", left: \" PTR_FORMAT \", right: \" PTR_FORMAT,\n+             p2i(p), p2i(left), p2i(right));\n+      assert(ctx==nullptr || ctx->is_marked(obj), \"Error\");\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahScanRemembered.inline.hpp","additions":26,"deletions":8,"binary":false,"changes":34,"status":"modified"},{"patch":"@@ -77,1 +77,1 @@\n-#include \"runtime\/synchronizer.inline.hpp\"\n+#include \"runtime\/synchronizer.hpp\"\n","filename":"src\/hotspot\/share\/interpreter\/interpreterRuntime.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -26,1 +26,0 @@\n-#include \"cds\/archiveHeapLoader.hpp\"\n@@ -29,1 +28,1 @@\n-#include \"cds\/heapShared.hpp\"\n+#include \"cds\/heapShared.inline.hpp\"\n@@ -330,1 +329,1 @@\n-  if (ArchiveHeapLoader::is_in_use()) {\n+  if (HeapShared::is_archived_heap_in_use()) {\n@@ -564,1 +563,1 @@\n-        ArchiveHeapLoader::is_in_use() &&\n+        HeapShared::is_archived_heap_in_use() &&\n@@ -566,2 +565,0 @@\n-      assert(ArchiveHeapLoader::can_use(), \"Sanity\");\n-\n@@ -576,1 +573,1 @@\n-      \/\/ _basic_type_mirrors[T_INT], etc, are null if archived heap is not mapped.\n+      \/\/ _basic_type_mirrors[T_INT], etc, are null if not using an archived heap\n@@ -918,0 +915,15 @@\n+  \/\/ Add main_thread to threads list to finish barrier setup with\n+  \/\/ on_thread_attach.  Should be before starting to build Java objects in\n+  \/\/ the AOT heap loader, which invokes barriers.\n+  {\n+    JavaThread* main_thread = JavaThread::current();\n+    MutexLocker mu(Threads_lock);\n+    Threads::add(main_thread);\n+  }\n+\n+  HeapShared::initialize_writing_mode();\n+\n+  \/\/ Create the string table before the AOT object archive is loaded,\n+  \/\/ as it might need to access the string table.\n+  StringTable::create_table();\n+\n@@ -940,1 +952,0 @@\n-  StringTable::create_table();\n@@ -1203,0 +1214,1 @@\n+\n","filename":"src\/hotspot\/share\/memory\/universe.cpp","additions":20,"deletions":8,"binary":false,"changes":28,"status":"modified"},{"patch":"@@ -27,3 +27,1 @@\n-#include \"cds\/archiveHeapLoader.hpp\"\n-#include \"cds\/archiveHeapWriter.hpp\"\n-#include \"cds\/heapShared.hpp\"\n+#include \"cds\/heapShared.inline.hpp\"\n@@ -363,1 +361,1 @@\n-            if (!ArchiveHeapWriter::is_string_too_large_to_archive(obj)) {\n+            if (!HeapShared::is_string_too_large_to_archive(obj)) {\n@@ -403,1 +401,1 @@\n-    if (ArchiveHeapLoader::is_in_use() &&\n+    if (HeapShared::is_archived_heap_in_use() &&\n","filename":"src\/hotspot\/share\/oops\/constantPool.cpp","additions":3,"deletions":5,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -25,2 +25,1 @@\n-#include \"cds\/archiveHeapLoader.hpp\"\n-#include \"cds\/heapShared.hpp\"\n+#include \"cds\/heapShared.inline.hpp\"\n@@ -889,1 +888,1 @@\n-    if (ArchiveHeapLoader::is_in_use()) {\n+    if (HeapShared::is_archived_heap_in_use()) {\n","filename":"src\/hotspot\/share\/oops\/klass.cpp","additions":2,"deletions":3,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -273,1 +273,1 @@\n-    assert(!UseObjectMonitorTable, \"Lightweight locking with OM table does not use markWord for monitors\");\n+    assert(!UseObjectMonitorTable, \"Locking with OM table does not use markWord for monitors\");\n@@ -299,1 +299,1 @@\n-    assert(!UseObjectMonitorTable, \"Lightweight locking with OM table does not use markWord for monitors\");\n+    assert(!UseObjectMonitorTable, \"Locking with OM table does not use markWord for monitors\");\n","filename":"src\/hotspot\/share\/oops\/markWord.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -249,2 +249,1 @@\n-  objArrayHandle array_h(THREAD, array);\n-  return array_h();\n+  return array;\n","filename":"src\/hotspot\/share\/oops\/objArrayKlass.cpp","additions":1,"deletions":2,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -38,1 +38,1 @@\n-  friend class ArchiveHeapWriter;\n+  friend class AOTMappedHeapWriter;\n","filename":"src\/hotspot\/share\/oops\/objArrayOop.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -86,1 +86,4 @@\n-  if (java_lang_String::is_instance_without_asserts(obj)) {\n+  \/\/ We can't use java_lang_String::is_instance since that has klass assertions enabled.\n+  \/\/ If the klass is garbage we want to just fail the check and continue printing, as\n+  \/\/ opposed to aborting the VM entirely.\n+  if (obj != nullptr && obj->klass_without_asserts() == vmClasses::String_klass()) {\n","filename":"src\/hotspot\/share\/oops\/oop.cpp","additions":4,"deletions":1,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -150,2 +150,1 @@\n-  objArrayHandle array_h(THREAD, array);\n-  return array_h();\n+  return array;\n","filename":"src\/hotspot\/share\/oops\/refArrayKlass.cpp","additions":1,"deletions":2,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -338,0 +338,6 @@\n+  \/\/ Identify if a vector mask operation prefers the input\/output mask to be\n+  \/\/ saved with a predicate type or not.\n+  \/\/ - Return true if it prefers a predicate type (i.e. TypeVectMask).\n+  \/\/ - Return false if it prefers a general vector type (i.e. TypeVectA to TypeVectZ).\n+  static bool mask_op_prefers_predicate(int opcode, const TypeVect* vt);\n+\n","filename":"src\/hotspot\/share\/opto\/matcher.hpp","additions":6,"deletions":0,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -1443,0 +1443,1 @@\n+  int exception_handler_req = HandlerImpl::size_exception_handler() + MAX_stubs_size; \/\/ add marginal slop for handler\n@@ -1448,1 +1449,1 @@\n-    code_req = const_req = stub_req = deopt_handler_req = 0x10;  \/\/ force expansion\n+    code_req = const_req = stub_req = exception_handler_req = deopt_handler_req = 0x10;  \/\/ force expansion\n@@ -1455,0 +1456,1 @@\n+          exception_handler_req +\n@@ -1884,0 +1886,2 @@\n+    \/\/ Emit the exception handler code.\n+    _code_offsets.set_value(CodeOffsets::Exceptions, HandlerImpl::emit_exception_handler(masm));\n","filename":"src\/hotspot\/share\/opto\/output.cpp","additions":5,"deletions":1,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -2585,1 +2585,1 @@\n-  \/\/ If changed AndI\/AndL inputs, check RShift users for \"(x & mask) >> shift\" optimization opportunity\n+  \/\/ If changed AndI\/AndL inputs, check RShift\/URShift users for \"(x & mask) >> shift\" optimization opportunity\n@@ -2589,1 +2589,2 @@\n-      if (u->Opcode() == Op_RShiftI || u->Opcode() == Op_RShiftL) {\n+      if (u->Opcode() == Op_RShiftI || u->Opcode() == Op_RShiftL ||\n+          u->Opcode() == Op_URShiftI || u->Opcode() == Op_URShiftL) {\n@@ -2601,4 +2602,4 @@\n-  if ((n->Opcode() == Op_ConvD2L && use_op == Op_ConvL2D) ||\n-      (n->Opcode() == Op_ConvF2I && use_op == Op_ConvI2F) ||\n-      (n->Opcode() == Op_ConvF2L && use_op == Op_ConvL2F) ||\n-      (n->Opcode() == Op_ConvI2F && use_op == Op_ConvF2I)) {\n+  if (use_op == Op_ConvL2D ||\n+      use_op == Op_ConvI2F ||\n+      use_op == Op_ConvL2F ||\n+      use_op == Op_ConvF2I) {\n@@ -2607,1 +2608,4 @@\n-      if (u->Opcode() == n->Opcode()) {\n+      if ((use_op == Op_ConvL2D && u->Opcode() == Op_ConvD2L) ||\n+          (use_op == Op_ConvI2F && u->Opcode() == Op_ConvF2I) ||\n+          (use_op == Op_ConvL2F && u->Opcode() == Op_ConvF2L) ||\n+          (use_op == Op_ConvF2I && u->Opcode() == Op_ConvI2F)) {\n","filename":"src\/hotspot\/share\/opto\/phaseX.cpp","additions":11,"deletions":7,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -531,0 +531,8 @@\n+\n+  \/\/ Add one or more users of 'use' to the worklist if it appears that a\n+  \/\/ known optimization could be applied to those users.\n+  \/\/ Node 'n' is a node that was modified or is about to get replaced,\n+  \/\/ and 'use' is one use of 'n'.\n+  \/\/ Certain optimizations have dependencies that extend beyond a node's\n+  \/\/ direct inputs, so it is necessary to ensure the appropriate\n+  \/\/ notifications are made here.\n@@ -532,0 +540,8 @@\n+\n+  \/\/ Add users of 'n', and any other nodes that could be directly\n+  \/\/ affected by changes to 'n', to the worklist.\n+  \/\/ Node 'n' may be a node that is about to get replaced. In this\n+  \/\/ case, 'n' should not be considered part of the new graph.\n+  \/\/ Passing the old node (as 'n'), rather than the new node,\n+  \/\/ prevents unnecessary notifications when the new node already\n+  \/\/ has other users.\n","filename":"src\/hotspot\/share\/opto\/phaseX.hpp","additions":16,"deletions":0,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -3646,1 +3646,1 @@\n-          ciField* field = nullptr;\n+          BasicType basic_elem_type = T_ILLEGAL;\n@@ -3649,1 +3649,1 @@\n-            field = k->get_field_by_offset(this->offset(), true);\n+            basic_elem_type = k->get_field_type_by_offset(this->offset(), true);\n@@ -3651,2 +3651,1 @@\n-          if (field != nullptr) {\n-            BasicType basic_elem_type = field->layout_type();\n+          if (basic_elem_type != T_ILLEGAL) {\n@@ -3661,3 +3660,2 @@\n-          ciField* field = ik->get_field_by_offset(this->offset(), false);\n-          if (field != nullptr) {\n-            BasicType basic_elem_type = field->layout_type();\n+          BasicType basic_elem_type = ik->get_field_type_by_offset(this->offset(), false);\n+          if (basic_elem_type != T_ILLEGAL) {\n","filename":"src\/hotspot\/share\/opto\/type.cpp","additions":5,"deletions":7,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -3563,0 +3563,4 @@\n+bool is_vm_created() {\n+  return AtomicAccess::load(&vm_created) == COMPLETE;\n+}\n+\n","filename":"src\/hotspot\/share\/prims\/jni.cpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -26,0 +26,1 @@\n+#include \"cds\/aotMappedHeapLoader.hpp\"\n@@ -27,1 +28,0 @@\n-#include \"cds\/archiveHeapLoader.hpp\"\n@@ -30,1 +30,1 @@\n-#include \"cds\/heapShared.hpp\"\n+#include \"cds\/heapShared.inline.hpp\"\n@@ -94,1 +94,0 @@\n-#include \"runtime\/lightweightSynchronizer.hpp\"\n@@ -1981,2 +1980,2 @@\n-WB_ENTRY(jboolean, WB_supportsRecursiveLightweightLocking(JNIEnv* env))\n-  return (jboolean) VM_Version::supports_recursive_lightweight_locking();\n+WB_ENTRY(jboolean, WB_supportsRecursiveFastLocking(JNIEnv* env))\n+  return (jboolean) VM_Version::supports_recursive_fast_locking();\n@@ -2316,0 +2315,3 @@\n+  if (!HeapShared::is_loading_mapping_mode()) {\n+    return false;\n+  }\n@@ -2328,1 +2330,1 @@\n-  return ArchiveHeapLoader::is_mapped();\n+  return AOTMappedHeapLoader::is_mapped();\n@@ -2341,1 +2343,1 @@\n-  return ArchiveHeapLoader::is_mapped();\n+  return AOTMappedHeapLoader::is_mapped();\n@@ -2371,1 +2373,1 @@\n-WB_ENTRY(jboolean, WB_CanWriteJavaHeapArchive(JNIEnv* env))\n+static bool canWriteJavaHeapArchive() {\n@@ -2373,0 +2375,8 @@\n+}\n+\n+WB_ENTRY(jboolean, WB_CanWriteJavaHeapArchive(JNIEnv* env))\n+  return canWriteJavaHeapArchive();\n+WB_END\n+\n+WB_ENTRY(jboolean, WB_CanWriteMappedJavaHeapArchive(JNIEnv* env))\n+  return canWriteJavaHeapArchive() && !AOTStreamableObjects;\n@@ -2375,0 +2385,3 @@\n+WB_ENTRY(jboolean, WB_CanWriteStreamedJavaHeapArchive(JNIEnv* env))\n+  return canWriteJavaHeapArchive() && AOTStreamableObjects;\n+WB_END\n@@ -3106,1 +3119,1 @@\n-  {CC\"supportsRecursiveLightweightLocking\", CC\"()Z\",  (void*)&WB_supportsRecursiveLightweightLocking },\n+  {CC\"supportsRecursiveFastLocking\", CC\"()Z\",         (void*)&WB_supportsRecursiveFastLocking },\n@@ -3157,0 +3170,2 @@\n+  {CC\"canWriteMappedJavaHeapArchive\",     CC\"()Z\",    (void*)&WB_CanWriteMappedJavaHeapArchive },\n+  {CC\"canWriteStreamedJavaHeapArchive\",   CC\"()Z\",    (void*)&WB_CanWriteStreamedJavaHeapArchive },\n","filename":"src\/hotspot\/share\/prims\/whitebox.cpp","additions":24,"deletions":9,"binary":false,"changes":33,"status":"modified"},{"patch":"@@ -553,0 +553,3 @@\n+  { \"AggressiveHeap\",               JDK_Version::jdk(26),  JDK_Version::jdk(27), JDK_Version::jdk(28) },\n+  { \"NeverActAsServerClassMachine\", JDK_Version::jdk(26),  JDK_Version::jdk(27), JDK_Version::jdk(28) },\n+  { \"AlwaysActAsServerClassMachine\", JDK_Version::jdk(26),  JDK_Version::jdk(27), JDK_Version::jdk(28) },\n","filename":"src\/hotspot\/share\/runtime\/arguments.cpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -80,1 +80,0 @@\n-#include \"runtime\/lightweightSynchronizer.hpp\"\n@@ -91,1 +90,1 @@\n-#include \"runtime\/synchronizer.inline.hpp\"\n+#include \"runtime\/synchronizer.hpp\"\n@@ -549,3 +548,0 @@\n-  if (exec_mode == Unpack_deopt) {\n-    assert(deoptee.is_deoptimized_frame(), \"frame is not marked for deoptimization\");\n-  }\n@@ -1812,2 +1808,2 @@\n-            LightweightSynchronizer::inflate_fast_locked_object(obj(), ObjectSynchronizer::InflateCause::inflate_cause_vm_internal,\n-                                                              deoptee_thread, thread);\n+            ObjectSynchronizer::inflate_fast_locked_object(obj(), ObjectSynchronizer::InflateCause::inflate_cause_vm_internal,\n+                                                           deoptee_thread, thread);\n","filename":"src\/hotspot\/share\/runtime\/deoptimization.cpp","additions":3,"deletions":7,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -213,1 +213,1 @@\n-    return nm->deopt_handler_entry() - pc_return_offset;\n+    return nm->deopt_handler_begin() - pc_return_offset;\n@@ -362,1 +362,1 @@\n-  address deopt = nm->deopt_handler_entry();\n+  address deopt = nm->deopt_handler_begin();\n","filename":"src\/hotspot\/share\/runtime\/frame.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2007,8 +2007,8 @@\n-          \"With Lightweight Locking mode, use a table to record inflated \"  \\\n-          \"monitors rather than the first word of the object.\")             \\\n-                                                                            \\\n-  product(int, LightweightFastLockingSpins, 13, DIAGNOSTIC,                 \\\n-          \"Specifies the number of times lightweight fast locking will \"    \\\n-          \"attempt to CAS the markWord before inflating. Between each \"     \\\n-          \"CAS it will spin for exponentially more time, resulting in \"     \\\n-          \"a total number of spins on the order of O(2^value)\")             \\\n+          \"Use a table to record inflated monitors rather than the first \"  \\\n+          \"word of the object.\")                                            \\\n+                                                                            \\\n+  product(int, FastLockingSpins, 13, DIAGNOSTIC,                            \\\n+          \"Specifies the number of times fast locking will attempt to \"     \\\n+          \"CAS the markWord before inflating. Between each CAS it will \"    \\\n+          \"spin for exponentially more time, resulting in a total number \"  \\\n+          \"of spins on the order of O(2^value)\")                            \\\n","filename":"src\/hotspot\/share\/runtime\/globals.hpp","additions":8,"deletions":8,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -125,1 +125,1 @@\n-    {                                                                      \\\n+    if (!javathread->is_aot_thread()) {                                    \\\n@@ -767,1 +767,1 @@\n-  assert(_threadObj.peek() != nullptr, \"just checking\");\n+  assert(_threadObj.peek() != nullptr || is_aot_thread(), \"just checking\");\n@@ -1413,1 +1413,1 @@\n-  \/\/ Due to lightweight locking\n+  \/\/ Due to fast locking\n","filename":"src\/hotspot\/share\/runtime\/javaThread.cpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -81,1 +81,1 @@\n-#include \"runtime\/synchronizer.inline.hpp\"\n+#include \"runtime\/synchronizer.hpp\"\n@@ -96,3 +96,0 @@\n-#ifdef COMPILER2\n-#include \"opto\/runtime.hpp\"\n-#endif\n@@ -613,5 +610,0 @@\n-#ifdef COMPILER2\n-      if (nm->compiler_type() == compiler_c2) {\n-        return OptoRuntime::exception_blob()->entry_point();\n-      }\n-#endif \/\/ COMPILER2\n@@ -2138,1 +2130,1 @@\n-  \/\/ C2_MacroAssembler::fast_unlock_lightweight() unlocked an inflated\n+  \/\/ C2_MacroAssembler::fast_unlock() unlocked an inflated\n@@ -3501,1 +3493,1 @@\n-  _aot_adapter_handler_table.iterate([&](AdapterHandlerEntry* entry) {\n+  _aot_adapter_handler_table.iterate_all([&](AdapterHandlerEntry* entry) {\n@@ -3879,20 +3871,0 @@\n-bool AdapterHandlerLibrary::contains(const CodeBlob* b) {\n-  bool found = false;\n-#if INCLUDE_CDS\n-  if (AOTCodeCache::is_using_adapter()) {\n-    auto findblob_archived_table = [&] (AdapterHandlerEntry* handler) {\n-      return (found = (b == CodeCache::find_blob(handler->get_i2c_entry())));\n-    };\n-    _aot_adapter_handler_table.iterate(findblob_archived_table);\n-  }\n-#endif \/\/ INCLUDE_CDS\n-  if (!found) {\n-    auto findblob_runtime_table = [&] (AdapterFingerPrint* key, AdapterHandlerEntry* a) {\n-      return (found = (b == CodeCache::find_blob(a->get_i2c_entry())));\n-    };\n-    assert_locked_or_safepoint(AdapterHandlerLibrary_lock);\n-    _adapter_handler_table->iterate(findblob_runtime_table);\n-  }\n-  return found;\n-}\n-\n@@ -3912,1 +3884,1 @@\n-      if (b == CodeCache::find_blob(handler->get_i2c_entry())) {\n+      if (b == handler->adapter_blob()) {\n@@ -3916,1 +3888,1 @@\n-        return true;\n+        return false; \/\/ abort iteration\n@@ -3918,1 +3890,1 @@\n-        return false; \/\/ keep looking\n+        return true; \/\/ keep looking\n@@ -3925,2 +3897,2 @@\n-    auto findblob_runtime_table = [&] (AdapterFingerPrint* key, AdapterHandlerEntry* a) {\n-      if (b == CodeCache::find_blob(a->get_i2c_entry())) {\n+    auto findblob_runtime_table = [&] (AdapterFingerPrint* key, AdapterHandlerEntry* handler) {\n+      if (b == handler->adapter_blob()) {\n@@ -3929,2 +3901,2 @@\n-        a->print_adapter_on(st);\n-        return true;\n+        handler->print_adapter_on(st);\n+        return false; \/\/ abort iteration\n@@ -3932,1 +3904,1 @@\n-        return false; \/\/ keep looking\n+        return true; \/\/ keep looking\n","filename":"src\/hotspot\/share\/runtime\/sharedRuntime.cpp","additions":11,"deletions":39,"binary":false,"changes":50,"status":"modified"},{"patch":"@@ -930,1 +930,0 @@\n-  static bool contains(const CodeBlob* b);\n","filename":"src\/hotspot\/share\/runtime\/sharedRuntime.hpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -44,1 +44,0 @@\n-#include \"runtime\/lightweightSynchronizer.hpp\"\n@@ -54,1 +53,1 @@\n-#include \"runtime\/synchronizer.inline.hpp\"\n+#include \"runtime\/synchronizer.hpp\"\n@@ -57,0 +56,1 @@\n+#include \"runtime\/timerTrace.hpp\"\n@@ -61,0 +61,2 @@\n+#include \"utilities\/concurrentHashTable.inline.hpp\"\n+#include \"utilities\/concurrentHashTableTasks.inline.hpp\"\n@@ -284,1 +286,1 @@\n-  LightweightSynchronizer::initialize();\n+  ObjectSynchronizer::create_om_table();\n@@ -441,12 +443,0 @@\n-\/\/ -----------------------------------------------------------------------------\n-\/\/ Monitor Enter\/Exit\n-\n-void ObjectSynchronizer::enter_for(Handle obj, BasicLock* lock, JavaThread* locking_thread) {\n-  \/\/ When called with locking_thread != Thread::current() some mechanism must synchronize\n-  \/\/ the locking_thread with respect to the current thread. Currently only used when\n-  \/\/ deoptimizing and re-locking locks. See Deoptimization::relock_objects\n-  assert(locking_thread == Thread::current() || locking_thread->is_obj_deopt_suspend(), \"must be\");\n-  assert(!obj->klass()->is_inline_klass(), \"JITed code should never have locked an instance of a value class\");\n-  return LightweightSynchronizer::enter_for(obj, lock, locking_thread);\n-}\n-\n@@ -483,1 +473,1 @@\n-    if (LightweightSynchronizer::inflate_and_enter(obj(), &lock, inflate_cause_jni_enter, current, current) != nullptr) {\n+    if (ObjectSynchronizer::inflate_and_enter(obj(), &lock, inflate_cause_jni_enter, current, current) != nullptr) {\n@@ -496,1 +486,1 @@\n-  monitor = LightweightSynchronizer::inflate_locked_or_imse(obj, inflate_cause_jni_exit, CHECK);\n+  monitor = ObjectSynchronizer::inflate_locked_or_imse(obj, inflate_cause_jni_exit, CHECK);\n@@ -560,1 +550,1 @@\n-  monitor = LightweightSynchronizer::inflate_locked_or_imse(obj(), inflate_cause_wait, CHECK_0);\n+  monitor = ObjectSynchronizer::inflate_locked_or_imse(obj(), inflate_cause_wait, CHECK_0);\n@@ -577,1 +567,1 @@\n-  monitor = LightweightSynchronizer::inflate_locked_or_imse(obj(), inflate_cause_wait, CHECK);\n+  monitor = ObjectSynchronizer::inflate_locked_or_imse(obj(), inflate_cause_wait, CHECK);\n@@ -591,1 +581,1 @@\n-  ObjectMonitor* monitor = LightweightSynchronizer::inflate_locked_or_imse(obj(), inflate_cause_notify, CHECK);\n+  ObjectMonitor* monitor = ObjectSynchronizer::inflate_locked_or_imse(obj(), inflate_cause_notify, CHECK);\n@@ -606,1 +596,1 @@\n-  ObjectMonitor* monitor = LightweightSynchronizer::inflate_locked_or_imse(obj(), inflate_cause_notify, CHECK);\n+  ObjectMonitor* monitor = ObjectSynchronizer::inflate_locked_or_imse(obj(), inflate_cause_notify, CHECK);\n@@ -683,21 +673,0 @@\n-static intptr_t install_hash_code(Thread* current, oop obj) {\n-  assert(UseObjectMonitorTable, \"must be\");\n-\n-  markWord mark = obj->mark_acquire();\n-  for (;;) {\n-    intptr_t hash = mark.hash();\n-    if (hash != 0) {\n-      return hash;\n-    }\n-\n-    hash = get_next_hash(current, obj);\n-    const markWord old_mark = mark;\n-    const markWord new_mark = old_mark.copy_set_hash(hash);\n-\n-    mark = obj->cas_set_mark(new_mark, old_mark);\n-    if (old_mark == mark) {\n-      return hash;\n-    }\n-  }\n-}\n-\n@@ -707,6 +676,0 @@\n-  if (UseObjectMonitorTable) {\n-    \/\/ Since the monitor isn't in the object header, the hash can simply be\n-    \/\/ installed in the object header.\n-    return install_hash_code(current, obj);\n-  }\n-\n@@ -719,1 +682,3 @@\n-    if (mark.is_unlocked() || mark.is_fast_locked()) {\n+    \/\/ If UseObjectMonitorTable is set the hash can simply be installed in the\n+    \/\/ object header, since the monitor isn't in the object header.\n+    if (UseObjectMonitorTable || !mark.has_monitor()) {\n@@ -738,1 +703,2 @@\n-    } else if (mark.has_monitor()) {\n+    } else {\n+      assert(!mark.is_unlocked() && !mark.is_fast_locked(), \"invariant\");\n@@ -1272,1 +1238,1 @@\n-        assert(!LightweightSynchronizer::contains_monitor(current, monitor), \"Should have been removed\");\n+        assert(!ObjectSynchronizer::contains_monitor(current, monitor), \"Should have been removed\");\n@@ -1544,0 +1510,1216 @@\n+\n+\/\/ -----------------------------------------------------------------------------\n+\/\/ ConcurrentHashTable storing links from objects to ObjectMonitors\n+class ObjectMonitorTable : AllStatic {\n+  struct Config {\n+    using Value = ObjectMonitor*;\n+    static uintx get_hash(Value const& value, bool* is_dead) {\n+      return (uintx)value->hash();\n+    }\n+    static void* allocate_node(void* context, size_t size, Value const& value) {\n+      ObjectMonitorTable::inc_items_count();\n+      return AllocateHeap(size, mtObjectMonitor);\n+    };\n+    static void free_node(void* context, void* memory, Value const& value) {\n+      ObjectMonitorTable::dec_items_count();\n+      FreeHeap(memory);\n+    }\n+  };\n+  using ConcurrentTable = ConcurrentHashTable<Config, mtObjectMonitor>;\n+\n+  static ConcurrentTable* _table;\n+  static volatile size_t _items_count;\n+  static size_t _table_size;\n+  static volatile bool _resize;\n+\n+  class Lookup : public StackObj {\n+    oop _obj;\n+\n+   public:\n+    explicit Lookup(oop obj) : _obj(obj) {}\n+\n+    uintx get_hash() const {\n+      uintx hash = _obj->mark().hash();\n+      assert(hash != 0, \"should have a hash\");\n+      return hash;\n+    }\n+\n+    bool equals(ObjectMonitor** value) {\n+      assert(*value != nullptr, \"must be\");\n+      return (*value)->object_refers_to(_obj);\n+    }\n+\n+    bool is_dead(ObjectMonitor** value) {\n+      assert(*value != nullptr, \"must be\");\n+      return false;\n+    }\n+  };\n+\n+  class LookupMonitor : public StackObj {\n+    ObjectMonitor* _monitor;\n+\n+   public:\n+    explicit LookupMonitor(ObjectMonitor* monitor) : _monitor(monitor) {}\n+\n+    uintx get_hash() const {\n+      return _monitor->hash();\n+    }\n+\n+    bool equals(ObjectMonitor** value) {\n+      return (*value) == _monitor;\n+    }\n+\n+    bool is_dead(ObjectMonitor** value) {\n+      assert(*value != nullptr, \"must be\");\n+      return (*value)->object_is_dead();\n+    }\n+  };\n+\n+  static void inc_items_count() {\n+    AtomicAccess::inc(&_items_count, memory_order_relaxed);\n+  }\n+\n+  static void dec_items_count() {\n+    AtomicAccess::dec(&_items_count, memory_order_relaxed);\n+  }\n+\n+  static double get_load_factor() {\n+    size_t count = AtomicAccess::load(&_items_count);\n+    return (double)count \/ (double)_table_size;\n+  }\n+\n+  static size_t table_size(Thread* current = Thread::current()) {\n+    return ((size_t)1) << _table->get_size_log2(current);\n+  }\n+\n+  static size_t max_log_size() {\n+    \/\/ TODO[OMTable]: Evaluate the max size.\n+    \/\/ TODO[OMTable]: Need to fix init order to use Universe::heap()->max_capacity();\n+    \/\/                Using MaxHeapSize directly this early may be wrong, and there\n+    \/\/                are definitely rounding errors (alignment).\n+    const size_t max_capacity = MaxHeapSize;\n+    const size_t min_object_size = CollectedHeap::min_dummy_object_size() * HeapWordSize;\n+    const size_t max_objects = max_capacity \/ MAX2(MinObjAlignmentInBytes, checked_cast<int>(min_object_size));\n+    const size_t log_max_objects = log2i_graceful(max_objects);\n+\n+    return MAX2(MIN2<size_t>(SIZE_BIG_LOG2, log_max_objects), min_log_size());\n+  }\n+\n+  static size_t min_log_size() {\n+    \/\/ ~= log(AvgMonitorsPerThreadEstimate default)\n+    return 10;\n+  }\n+\n+  template<typename V>\n+  static size_t clamp_log_size(V log_size) {\n+    return MAX2(MIN2(log_size, checked_cast<V>(max_log_size())), checked_cast<V>(min_log_size()));\n+  }\n+\n+  static size_t initial_log_size() {\n+    const size_t estimate = log2i(MAX2(os::processor_count(), 1)) + log2i(MAX2(AvgMonitorsPerThreadEstimate, size_t(1)));\n+    return clamp_log_size(estimate);\n+  }\n+\n+  static size_t grow_hint () {\n+    return ConcurrentTable::DEFAULT_GROW_HINT;\n+  }\n+\n+ public:\n+  static void create() {\n+    _table = new ConcurrentTable(initial_log_size(), max_log_size(), grow_hint());\n+    _items_count = 0;\n+    _table_size = table_size();\n+    _resize = false;\n+  }\n+\n+  static void verify_monitor_get_result(oop obj, ObjectMonitor* monitor) {\n+#ifdef ASSERT\n+    if (SafepointSynchronize::is_at_safepoint()) {\n+      bool has_monitor = obj->mark().has_monitor();\n+      assert(has_monitor == (monitor != nullptr),\n+          \"Inconsistency between markWord and ObjectMonitorTable has_monitor: %s monitor: \" PTR_FORMAT,\n+          BOOL_TO_STR(has_monitor), p2i(monitor));\n+    }\n+#endif\n+  }\n+\n+  static ObjectMonitor* monitor_get(Thread* current, oop obj) {\n+    ObjectMonitor* result = nullptr;\n+    Lookup lookup_f(obj);\n+    auto found_f = [&](ObjectMonitor** found) {\n+      assert((*found)->object_peek() == obj, \"must be\");\n+      result = *found;\n+    };\n+    _table->get(current, lookup_f, found_f);\n+    verify_monitor_get_result(obj, result);\n+    return result;\n+  }\n+\n+  static void try_notify_grow() {\n+    if (!_table->is_max_size_reached() && !AtomicAccess::load(&_resize)) {\n+      AtomicAccess::store(&_resize, true);\n+      if (Service_lock->try_lock()) {\n+        Service_lock->notify();\n+        Service_lock->unlock();\n+      }\n+    }\n+  }\n+\n+  static bool should_shrink() {\n+    \/\/ Not implemented;\n+    return false;\n+  }\n+\n+  static constexpr double GROW_LOAD_FACTOR = 0.75;\n+\n+  static bool should_grow() {\n+    return get_load_factor() > GROW_LOAD_FACTOR && !_table->is_max_size_reached();\n+  }\n+\n+  static bool should_resize() {\n+    return should_grow() || should_shrink() || AtomicAccess::load(&_resize);\n+  }\n+\n+  template<typename Task, typename... Args>\n+  static bool run_task(JavaThread* current, Task& task, const char* task_name, Args&... args) {\n+    if (task.prepare(current)) {\n+      log_trace(monitortable)(\"Started to %s\", task_name);\n+      TraceTime timer(task_name, TRACETIME_LOG(Debug, monitortable, perf));\n+      while (task.do_task(current, args...)) {\n+        task.pause(current);\n+        {\n+          ThreadBlockInVM tbivm(current);\n+        }\n+        task.cont(current);\n+      }\n+      task.done(current);\n+      return true;\n+    }\n+    return false;\n+  }\n+\n+  static bool grow(JavaThread* current) {\n+    ConcurrentTable::GrowTask grow_task(_table);\n+    if (run_task(current, grow_task, \"Grow\")) {\n+      _table_size = table_size(current);\n+      log_info(monitortable)(\"Grown to size: %zu\", _table_size);\n+      return true;\n+    }\n+    return false;\n+  }\n+\n+  static bool clean(JavaThread* current) {\n+    ConcurrentTable::BulkDeleteTask clean_task(_table);\n+    auto is_dead = [&](ObjectMonitor** monitor) {\n+      return (*monitor)->object_is_dead();\n+    };\n+    auto do_nothing = [&](ObjectMonitor** monitor) {};\n+    NativeHeapTrimmer::SuspendMark sm(\"ObjectMonitorTable\");\n+    return run_task(current, clean_task, \"Clean\", is_dead, do_nothing);\n+  }\n+\n+  static bool resize(JavaThread* current) {\n+    LogTarget(Info, monitortable) lt;\n+    bool success = false;\n+\n+    if (should_grow()) {\n+      lt.print(\"Start growing with load factor %f\", get_load_factor());\n+      success = grow(current);\n+    } else {\n+      if (!_table->is_max_size_reached() && AtomicAccess::load(&_resize)) {\n+        lt.print(\"WARNING: Getting resize hints with load factor %f\", get_load_factor());\n+      }\n+      lt.print(\"Start cleaning with load factor %f\", get_load_factor());\n+      success = clean(current);\n+    }\n+\n+    AtomicAccess::store(&_resize, false);\n+\n+    return success;\n+  }\n+\n+  static ObjectMonitor* monitor_put_get(Thread* current, ObjectMonitor* monitor, oop obj) {\n+    \/\/ Enter the monitor into the concurrent hashtable.\n+    ObjectMonitor* result = monitor;\n+    Lookup lookup_f(obj);\n+    auto found_f = [&](ObjectMonitor** found) {\n+      assert((*found)->object_peek() == obj, \"must be\");\n+      result = *found;\n+    };\n+    bool grow;\n+    _table->insert_get(current, lookup_f, monitor, found_f, &grow);\n+    verify_monitor_get_result(obj, result);\n+    if (grow) {\n+      try_notify_grow();\n+    }\n+    return result;\n+  }\n+\n+  static bool remove_monitor_entry(Thread* current, ObjectMonitor* monitor) {\n+    LookupMonitor lookup_f(monitor);\n+    return _table->remove(current, lookup_f);\n+  }\n+\n+  static bool contains_monitor(Thread* current, ObjectMonitor* monitor) {\n+    LookupMonitor lookup_f(monitor);\n+    bool result = false;\n+    auto found_f = [&](ObjectMonitor** found) {\n+      result = true;\n+    };\n+    _table->get(current, lookup_f, found_f);\n+    return result;\n+  }\n+\n+  static void print_on(outputStream* st) {\n+    auto printer = [&] (ObjectMonitor** entry) {\n+       ObjectMonitor* om = *entry;\n+       oop obj = om->object_peek();\n+       st->print(\"monitor=\" PTR_FORMAT \", \", p2i(om));\n+       st->print(\"object=\" PTR_FORMAT, p2i(obj));\n+       assert(obj->mark().hash() == om->hash(), \"hash must match\");\n+       st->cr();\n+       return true;\n+    };\n+    if (SafepointSynchronize::is_at_safepoint()) {\n+      _table->do_safepoint_scan(printer);\n+    } else {\n+      _table->do_scan(Thread::current(), printer);\n+    }\n+  }\n+};\n+\n+ObjectMonitorTable::ConcurrentTable* ObjectMonitorTable::_table = nullptr;\n+volatile size_t ObjectMonitorTable::_items_count = 0;\n+size_t ObjectMonitorTable::_table_size = 0;\n+volatile bool ObjectMonitorTable::_resize = false;\n+\n+ObjectMonitor* ObjectSynchronizer::get_or_insert_monitor_from_table(oop object, JavaThread* current, bool* inserted) {\n+  ObjectMonitor* monitor = get_monitor_from_table(current, object);\n+  if (monitor != nullptr) {\n+    *inserted = false;\n+    return monitor;\n+  }\n+\n+  ObjectMonitor* alloced_monitor = new ObjectMonitor(object);\n+  alloced_monitor->set_anonymous_owner();\n+\n+  \/\/ Try insert monitor\n+  monitor = add_monitor(current, alloced_monitor, object);\n+\n+  *inserted = alloced_monitor == monitor;\n+  if (!*inserted) {\n+    delete alloced_monitor;\n+  }\n+\n+  return monitor;\n+}\n+\n+static void log_inflate(Thread* current, oop object, ObjectSynchronizer::InflateCause cause) {\n+  if (log_is_enabled(Trace, monitorinflation)) {\n+    ResourceMark rm(current);\n+    log_trace(monitorinflation)(\"inflate: object=\" INTPTR_FORMAT \", mark=\"\n+                                INTPTR_FORMAT \", type='%s' cause=%s\", p2i(object),\n+                                object->mark().value(), object->klass()->external_name(),\n+                                ObjectSynchronizer::inflate_cause_name(cause));\n+  }\n+}\n+\n+static void post_monitor_inflate_event(EventJavaMonitorInflate* event,\n+                                       const oop obj,\n+                                       ObjectSynchronizer::InflateCause cause) {\n+  assert(event != nullptr, \"invariant\");\n+  const Klass* monitor_klass = obj->klass();\n+  if (ObjectMonitor::is_jfr_excluded(monitor_klass)) {\n+    return;\n+  }\n+  event->set_monitorClass(monitor_klass);\n+  event->set_address((uintptr_t)(void*)obj);\n+  event->set_cause((u1)cause);\n+  event->commit();\n+}\n+\n+ObjectMonitor* ObjectSynchronizer::get_or_insert_monitor(oop object, JavaThread* current, ObjectSynchronizer::InflateCause cause) {\n+  assert(UseObjectMonitorTable, \"must be\");\n+\n+  EventJavaMonitorInflate event;\n+\n+  bool inserted;\n+  ObjectMonitor* monitor = get_or_insert_monitor_from_table(object, current, &inserted);\n+\n+  if (inserted) {\n+    log_inflate(current, object, cause);\n+    if (event.should_commit()) {\n+      post_monitor_inflate_event(&event, object, cause);\n+    }\n+\n+    \/\/ The monitor has an anonymous owner so it is safe from async deflation.\n+    ObjectSynchronizer::_in_use_list.add(monitor);\n+  }\n+\n+  return monitor;\n+}\n+\n+\/\/ Add the hashcode to the monitor to match the object and put it in the hashtable.\n+ObjectMonitor* ObjectSynchronizer::add_monitor(JavaThread* current, ObjectMonitor* monitor, oop obj) {\n+  assert(UseObjectMonitorTable, \"must be\");\n+  assert(obj == monitor->object(), \"must be\");\n+\n+  intptr_t hash = obj->mark().hash();\n+  assert(hash != 0, \"must be set when claiming the object monitor\");\n+  monitor->set_hash(hash);\n+\n+  return ObjectMonitorTable::monitor_put_get(current, monitor, obj);\n+}\n+\n+bool ObjectSynchronizer::remove_monitor(Thread* current, ObjectMonitor* monitor, oop obj) {\n+  assert(UseObjectMonitorTable, \"must be\");\n+  assert(monitor->object_peek() == obj, \"must be, cleared objects are removed by is_dead\");\n+\n+  return ObjectMonitorTable::remove_monitor_entry(current, monitor);\n+}\n+\n+void ObjectSynchronizer::deflate_mark_word(oop obj) {\n+  assert(UseObjectMonitorTable, \"must be\");\n+\n+  markWord mark = obj->mark_acquire();\n+  assert(!mark.has_no_hash(), \"obj with inflated monitor must have had a hash\");\n+\n+  while (mark.has_monitor()) {\n+    const markWord new_mark = mark.clear_lock_bits().set_unlocked();\n+    mark = obj->cas_set_mark(new_mark, mark);\n+  }\n+}\n+\n+void ObjectSynchronizer::create_om_table() {\n+  if (!UseObjectMonitorTable) {\n+    return;\n+  }\n+  ObjectMonitorTable::create();\n+}\n+\n+bool ObjectSynchronizer::needs_resize() {\n+  if (!UseObjectMonitorTable) {\n+    return false;\n+  }\n+  return ObjectMonitorTable::should_resize();\n+}\n+\n+bool ObjectSynchronizer::resize_table(JavaThread* current) {\n+  if (!UseObjectMonitorTable) {\n+    return true;\n+  }\n+  return ObjectMonitorTable::resize(current);\n+}\n+\n+class ObjectSynchronizer::LockStackInflateContendedLocks : private OopClosure {\n+ private:\n+  oop _contended_oops[LockStack::CAPACITY];\n+  int _length;\n+\n+  void do_oop(oop* o) final {\n+    oop obj = *o;\n+    if (obj->mark_acquire().has_monitor()) {\n+      if (_length > 0 && _contended_oops[_length - 1] == obj) {\n+        \/\/ Recursive\n+        return;\n+      }\n+      _contended_oops[_length++] = obj;\n+    }\n+  }\n+\n+  void do_oop(narrowOop* o) final {\n+    ShouldNotReachHere();\n+  }\n+\n+ public:\n+  LockStackInflateContendedLocks() :\n+    _contended_oops(),\n+    _length(0) {};\n+\n+  void inflate(JavaThread* current) {\n+    assert(current == JavaThread::current(), \"must be\");\n+    current->lock_stack().oops_do(this);\n+    for (int i = 0; i < _length; i++) {\n+      ObjectSynchronizer::\n+        inflate_fast_locked_object(_contended_oops[i], ObjectSynchronizer::inflate_cause_vm_internal, current, current);\n+    }\n+  }\n+};\n+\n+void ObjectSynchronizer::ensure_lock_stack_space(JavaThread* current) {\n+  assert(current == JavaThread::current(), \"must be\");\n+  LockStack& lock_stack = current->lock_stack();\n+\n+  \/\/ Make room on lock_stack\n+  if (lock_stack.is_full()) {\n+    \/\/ Inflate contended objects\n+    LockStackInflateContendedLocks().inflate(current);\n+    if (lock_stack.is_full()) {\n+      \/\/ Inflate the oldest object\n+      inflate_fast_locked_object(lock_stack.bottom(), ObjectSynchronizer::inflate_cause_vm_internal, current, current);\n+    }\n+  }\n+}\n+\n+class ObjectSynchronizer::CacheSetter : StackObj {\n+  JavaThread* const _thread;\n+  BasicLock* const _lock;\n+  ObjectMonitor* _monitor;\n+\n+  NONCOPYABLE(CacheSetter);\n+\n+ public:\n+  CacheSetter(JavaThread* thread, BasicLock* lock) :\n+    _thread(thread),\n+    _lock(lock),\n+    _monitor(nullptr) {}\n+\n+  ~CacheSetter() {\n+    \/\/ Only use the cache if using the table.\n+    if (UseObjectMonitorTable) {\n+      if (_monitor != nullptr) {\n+        \/\/ If the monitor is already in the BasicLock cache then it is most\n+        \/\/ likely in the thread cache, do not set it again to avoid reordering.\n+        if (_monitor != _lock->object_monitor_cache()) {\n+          _thread->om_set_monitor_cache(_monitor);\n+          _lock->set_object_monitor_cache(_monitor);\n+        }\n+      } else {\n+        _lock->clear_object_monitor_cache();\n+      }\n+    }\n+  }\n+\n+  void set_monitor(ObjectMonitor* monitor) {\n+    assert(_monitor == nullptr, \"only set once\");\n+    _monitor = monitor;\n+  }\n+\n+};\n+\n+\/\/ Reads first from the BasicLock cache then from the OMCache in the current thread.\n+\/\/ C2 fast-path may have put the monitor in the cache in the BasicLock.\n+inline static ObjectMonitor* read_caches(JavaThread* current, BasicLock* lock, oop object) {\n+  ObjectMonitor* monitor = lock->object_monitor_cache();\n+  if (monitor == nullptr) {\n+    monitor = current->om_get_from_monitor_cache(object);\n+  }\n+  return monitor;\n+}\n+\n+class ObjectSynchronizer::VerifyThreadState {\n+  bool _no_safepoint;\n+\n+ public:\n+  VerifyThreadState(JavaThread* locking_thread, JavaThread* current) : _no_safepoint(locking_thread != current) {\n+    assert(current == Thread::current(), \"must be\");\n+    assert(locking_thread == current || locking_thread->is_obj_deopt_suspend(), \"locking_thread may not run concurrently\");\n+    if (_no_safepoint) {\n+      DEBUG_ONLY(JavaThread::current()->inc_no_safepoint_count();)\n+    }\n+  }\n+  ~VerifyThreadState() {\n+    if (_no_safepoint){\n+      DEBUG_ONLY(JavaThread::current()->dec_no_safepoint_count();)\n+    }\n+  }\n+};\n+\n+inline bool ObjectSynchronizer::fast_lock_try_enter(oop obj, LockStack& lock_stack, JavaThread* current) {\n+  markWord mark = obj->mark();\n+  while (mark.is_unlocked()) {\n+    ensure_lock_stack_space(current);\n+    assert(!lock_stack.is_full(), \"must have made room on the lock stack\");\n+    assert(!lock_stack.contains(obj), \"thread must not already hold the lock\");\n+    \/\/ Try to swing into 'fast-locked' state.\n+    markWord locked_mark = mark.set_fast_locked();\n+    markWord old_mark = mark;\n+    mark = obj->cas_set_mark(locked_mark, old_mark);\n+    if (old_mark == mark) {\n+      \/\/ Successfully fast-locked, push object to lock-stack and return.\n+      lock_stack.push(obj);\n+      return true;\n+    }\n+  }\n+  return false;\n+}\n+\n+bool ObjectSynchronizer::fast_lock_spin_enter(oop obj, LockStack& lock_stack, JavaThread* current, bool observed_deflation) {\n+  assert(UseObjectMonitorTable, \"must be\");\n+  \/\/ Will spin with exponential backoff with an accumulative O(2^spin_limit) spins.\n+  const int log_spin_limit = os::is_MP() ? FastLockingSpins : 1;\n+  const int log_min_safepoint_check_interval = 10;\n+\n+  markWord mark = obj->mark();\n+  const auto should_spin = [&]() {\n+    if (!mark.has_monitor()) {\n+      \/\/ Spin while not inflated.\n+      return true;\n+    } else if (observed_deflation) {\n+      \/\/ Spin while monitor is being deflated.\n+      ObjectMonitor* monitor = ObjectSynchronizer::read_monitor(current, obj, mark);\n+      return monitor == nullptr || monitor->is_being_async_deflated();\n+    }\n+    \/\/ Else stop spinning.\n+    return false;\n+  };\n+  \/\/ Always attempt to lock once even when safepoint synchronizing.\n+  bool should_process = false;\n+  for (int i = 0; should_spin() && !should_process && i < log_spin_limit; i++) {\n+    \/\/ Spin with exponential backoff.\n+    const int total_spin_count = 1 << i;\n+    const int inner_spin_count = MIN2(1 << log_min_safepoint_check_interval, total_spin_count);\n+    const int outer_spin_count = total_spin_count \/ inner_spin_count;\n+    for (int outer = 0; outer < outer_spin_count; outer++) {\n+      should_process = SafepointMechanism::should_process(current);\n+      if (should_process) {\n+        \/\/ Stop spinning for safepoint.\n+        break;\n+      }\n+      for (int inner = 1; inner < inner_spin_count; inner++) {\n+        SpinPause();\n+      }\n+    }\n+\n+    if (fast_lock_try_enter(obj, lock_stack, current)) return true;\n+  }\n+  return false;\n+}\n+\n+void ObjectSynchronizer::enter_for(Handle obj, BasicLock* lock, JavaThread* locking_thread) {\n+  \/\/ When called with locking_thread != Thread::current() some mechanism must synchronize\n+  \/\/ the locking_thread with respect to the current thread. Currently only used when\n+  \/\/ deoptimizing and re-locking locks. See Deoptimization::relock_objects\n+  assert(locking_thread == Thread::current() || locking_thread->is_obj_deopt_suspend(), \"must be\");\n+\n+  assert(!UseObjectMonitorTable || lock->object_monitor_cache() == nullptr, \"must be cleared\");\n+  JavaThread* current = JavaThread::current();\n+  VerifyThreadState vts(locking_thread, current);\n+\n+  if (obj->klass()->is_value_based()) {\n+    ObjectSynchronizer::handle_sync_on_value_based_class(obj, locking_thread);\n+  }\n+\n+  LockStack& lock_stack = locking_thread->lock_stack();\n+\n+  ObjectMonitor* monitor = nullptr;\n+  if (lock_stack.contains(obj())) {\n+    monitor = inflate_fast_locked_object(obj(), ObjectSynchronizer::inflate_cause_monitor_enter, locking_thread, current);\n+    bool entered = monitor->enter_for(locking_thread);\n+    assert(entered, \"recursive ObjectMonitor::enter_for must succeed\");\n+  } else {\n+    do {\n+      \/\/ It is assumed that enter_for must enter on an object without contention.\n+      monitor = inflate_and_enter(obj(), lock, ObjectSynchronizer::inflate_cause_monitor_enter, locking_thread, current);\n+      \/\/ But there may still be a race with deflation.\n+    } while (monitor == nullptr);\n+  }\n+\n+  assert(monitor != nullptr, \"ObjectSynchronizer::enter_for must succeed\");\n+  assert(!UseObjectMonitorTable || lock->object_monitor_cache() == nullptr, \"unused. already cleared\");\n+}\n+\n+void ObjectSynchronizer::enter(Handle obj, BasicLock* lock, JavaThread* current) {\n+  assert(current == JavaThread::current(), \"must be\");\n+\n+  if (obj->klass()->is_value_based()) {\n+    ObjectSynchronizer::handle_sync_on_value_based_class(obj, current);\n+  }\n+\n+  CacheSetter cache_setter(current, lock);\n+\n+  \/\/ Used when deflation is observed. Progress here requires progress\n+  \/\/ from the deflator. After observing that the deflator is not\n+  \/\/ making progress (after two yields), switch to sleeping.\n+  SpinYield spin_yield(0, 2);\n+  bool observed_deflation = false;\n+\n+  LockStack& lock_stack = current->lock_stack();\n+\n+  if (!lock_stack.is_full() && lock_stack.try_recursive_enter(obj())) {\n+    \/\/ Recursively fast locked\n+    return;\n+  }\n+\n+  if (lock_stack.contains(obj())) {\n+    ObjectMonitor* monitor = inflate_fast_locked_object(obj(), ObjectSynchronizer::inflate_cause_monitor_enter, current, current);\n+    bool entered = monitor->enter(current);\n+    assert(entered, \"recursive ObjectMonitor::enter must succeed\");\n+    cache_setter.set_monitor(monitor);\n+    return;\n+  }\n+\n+  while (true) {\n+    \/\/ Fast-locking does not use the 'lock' argument.\n+    \/\/ Fast-lock spinning to avoid inflating for short critical sections.\n+    \/\/ The goal is to only inflate when the extra cost of using ObjectMonitors\n+    \/\/ is worth it.\n+    \/\/ If deflation has been observed we also spin while deflation is ongoing.\n+    if (fast_lock_try_enter(obj(), lock_stack, current)) {\n+      return;\n+    } else if (UseObjectMonitorTable && fast_lock_spin_enter(obj(), lock_stack, current, observed_deflation)) {\n+      return;\n+    }\n+\n+    if (observed_deflation) {\n+      spin_yield.wait();\n+    }\n+\n+    ObjectMonitor* monitor = inflate_and_enter(obj(), lock, ObjectSynchronizer::inflate_cause_monitor_enter, current, current);\n+    if (monitor != nullptr) {\n+      cache_setter.set_monitor(monitor);\n+      return;\n+    }\n+\n+    \/\/ If inflate_and_enter returns nullptr it is because a deflated monitor\n+    \/\/ was encountered. Fallback to fast locking. The deflater is responsible\n+    \/\/ for clearing out the monitor and transitioning the markWord back to\n+    \/\/ fast locking.\n+    observed_deflation = true;\n+  }\n+}\n+\n+void ObjectSynchronizer::exit(oop object, BasicLock* lock, JavaThread* current) {\n+  assert(current == Thread::current(), \"must be\");\n+\n+  markWord mark = object->mark();\n+  assert(!mark.is_unlocked(), \"must be\");\n+\n+  LockStack& lock_stack = current->lock_stack();\n+  if (mark.is_fast_locked()) {\n+    if (lock_stack.try_recursive_exit(object)) {\n+      \/\/ This is a recursive exit which succeeded\n+      return;\n+    }\n+    if (lock_stack.is_recursive(object)) {\n+      \/\/ Must inflate recursive locks if try_recursive_exit fails\n+      \/\/ This happens for un-structured unlocks, could potentially\n+      \/\/ fix try_recursive_exit to handle these.\n+      inflate_fast_locked_object(object, ObjectSynchronizer::inflate_cause_vm_internal, current, current);\n+    }\n+  }\n+\n+  while (mark.is_fast_locked()) {\n+    markWord unlocked_mark = mark.set_unlocked();\n+    markWord old_mark = mark;\n+    mark = object->cas_set_mark(unlocked_mark, old_mark);\n+    if (old_mark == mark) {\n+      \/\/ CAS successful, remove from lock_stack\n+      size_t recursion = lock_stack.remove(object) - 1;\n+      assert(recursion == 0, \"Should not have unlocked here\");\n+      return;\n+    }\n+  }\n+\n+  assert(mark.has_monitor(), \"must be\");\n+  \/\/ The monitor exists\n+  ObjectMonitor* monitor;\n+  if (UseObjectMonitorTable) {\n+    monitor = read_caches(current, lock, object);\n+    if (monitor == nullptr) {\n+      monitor = get_monitor_from_table(current, object);\n+    }\n+  } else {\n+    monitor = ObjectSynchronizer::read_monitor(mark);\n+  }\n+  if (monitor->has_anonymous_owner()) {\n+    assert(current->lock_stack().contains(object), \"current must have object on its lock stack\");\n+    monitor->set_owner_from_anonymous(current);\n+    monitor->set_recursions(current->lock_stack().remove(object) - 1);\n+  }\n+\n+  monitor->exit(current);\n+}\n+\n+\/\/ ObjectSynchronizer::inflate_locked_or_imse is used to get an\n+\/\/ inflated ObjectMonitor* from contexts which require that, such as\n+\/\/ notify\/wait and jni_exit. Fast locking keeps the invariant that it\n+\/\/ only inflates if it is already locked by the current thread or the current\n+\/\/ thread is in the process of entering. To maintain this invariant we need to\n+\/\/ throw a java.lang.IllegalMonitorStateException before inflating if the\n+\/\/ current thread is not the owner.\n+ObjectMonitor* ObjectSynchronizer::inflate_locked_or_imse(oop obj, ObjectSynchronizer::InflateCause cause, TRAPS) {\n+  JavaThread* current = THREAD;\n+\n+  for (;;) {\n+    markWord mark = obj->mark_acquire();\n+    if (mark.is_unlocked()) {\n+      \/\/ No lock, IMSE.\n+      THROW_MSG_(vmSymbols::java_lang_IllegalMonitorStateException(),\n+                 \"current thread is not owner\", nullptr);\n+    }\n+\n+    if (mark.is_fast_locked()) {\n+      if (!current->lock_stack().contains(obj)) {\n+        \/\/ Fast locked by other thread, IMSE.\n+        THROW_MSG_(vmSymbols::java_lang_IllegalMonitorStateException(),\n+                   \"current thread is not owner\", nullptr);\n+      } else {\n+        \/\/ Current thread owns the lock, must inflate\n+        return inflate_fast_locked_object(obj, cause, current, current);\n+      }\n+    }\n+\n+    assert(mark.has_monitor(), \"must be\");\n+    ObjectMonitor* monitor = ObjectSynchronizer::read_monitor(current, obj, mark);\n+    if (monitor != nullptr) {\n+      if (monitor->has_anonymous_owner()) {\n+        LockStack& lock_stack = current->lock_stack();\n+        if (lock_stack.contains(obj)) {\n+          \/\/ Current thread owns the lock but someone else inflated it.\n+          \/\/ Fix owner and pop lock stack.\n+          monitor->set_owner_from_anonymous(current);\n+          monitor->set_recursions(lock_stack.remove(obj) - 1);\n+        } else {\n+          \/\/ Fast locked (and inflated) by other thread, or deflation in progress, IMSE.\n+          THROW_MSG_(vmSymbols::java_lang_IllegalMonitorStateException(),\n+                     \"current thread is not owner\", nullptr);\n+        }\n+      }\n+      return monitor;\n+    }\n+  }\n+}\n+\n+ObjectMonitor* ObjectSynchronizer::inflate_into_object_header(oop object, ObjectSynchronizer::InflateCause cause, JavaThread* locking_thread, Thread* current) {\n+\n+  \/\/ The JavaThread* locking parameter requires that the locking_thread == JavaThread::current,\n+  \/\/ or is suspended throughout the call by some other mechanism.\n+  \/\/ Even with fast locking the thread might be nullptr when called from a non\n+  \/\/ JavaThread. (As may still be the case from FastHashCode). However it is only\n+  \/\/ important for the correctness of the fast locking algorithm that the thread\n+  \/\/ is set when called from ObjectSynchronizer::enter from the owning thread,\n+  \/\/ ObjectSynchronizer::enter_for from any thread, or ObjectSynchronizer::exit.\n+  EventJavaMonitorInflate event;\n+\n+  for (;;) {\n+    const markWord mark = object->mark_acquire();\n+\n+    \/\/ The mark can be in one of the following states:\n+    \/\/ *  inflated     - Just return if using stack-locking.\n+    \/\/                   If using fast-locking and the ObjectMonitor owner\n+    \/\/                   is anonymous and the locking_thread owns the\n+    \/\/                   object lock, then we make the locking_thread\n+    \/\/                   the ObjectMonitor owner and remove the lock from\n+    \/\/                   the locking_thread's lock stack.\n+    \/\/ *  fast-locked  - Coerce it to inflated from fast-locked.\n+    \/\/ *  unlocked     - Aggressively inflate the object.\n+\n+    \/\/ CASE: inflated\n+    if (mark.has_monitor()) {\n+      ObjectMonitor* inf = mark.monitor();\n+      markWord dmw = inf->header();\n+      assert(dmw.is_neutral(), \"invariant: header=\" INTPTR_FORMAT, dmw.value());\n+      if (inf->has_anonymous_owner() &&\n+          locking_thread != nullptr && locking_thread->lock_stack().contains(object)) {\n+        inf->set_owner_from_anonymous(locking_thread);\n+        size_t removed = locking_thread->lock_stack().remove(object);\n+        inf->set_recursions(removed - 1);\n+      }\n+      return inf;\n+    }\n+\n+    \/\/ CASE: fast-locked\n+    \/\/ Could be fast-locked either by the locking_thread or by some other thread.\n+    \/\/\n+    \/\/ Note that we allocate the ObjectMonitor speculatively, _before_\n+    \/\/ attempting to set the object's mark to the new ObjectMonitor. If\n+    \/\/ the locking_thread owns the monitor, then we set the ObjectMonitor's\n+    \/\/ owner to the locking_thread. Otherwise, we set the ObjectMonitor's owner\n+    \/\/ to anonymous. If we lose the race to set the object's mark to the\n+    \/\/ new ObjectMonitor, then we just delete it and loop around again.\n+    \/\/\n+    if (mark.is_fast_locked()) {\n+      ObjectMonitor* monitor = new ObjectMonitor(object);\n+      monitor->set_header(mark.set_unlocked());\n+      bool own = locking_thread != nullptr && locking_thread->lock_stack().contains(object);\n+      if (own) {\n+        \/\/ Owned by locking_thread.\n+        monitor->set_owner(locking_thread);\n+      } else {\n+        \/\/ Owned by somebody else.\n+        monitor->set_anonymous_owner();\n+      }\n+      markWord monitor_mark = markWord::encode(monitor);\n+      markWord old_mark = object->cas_set_mark(monitor_mark, mark);\n+      if (old_mark == mark) {\n+        \/\/ Success! Return inflated monitor.\n+        if (own) {\n+          size_t removed = locking_thread->lock_stack().remove(object);\n+          monitor->set_recursions(removed - 1);\n+        }\n+        \/\/ Once the ObjectMonitor is configured and object is associated\n+        \/\/ with the ObjectMonitor, it is safe to allow async deflation:\n+        ObjectSynchronizer::_in_use_list.add(monitor);\n+\n+        log_inflate(current, object, cause);\n+        if (event.should_commit()) {\n+          post_monitor_inflate_event(&event, object, cause);\n+        }\n+        return monitor;\n+      } else {\n+        delete monitor;\n+        continue;  \/\/ Interference -- just retry\n+      }\n+    }\n+\n+    \/\/ CASE: unlocked\n+    \/\/ TODO-FIXME: for entry we currently inflate and then try to CAS _owner.\n+    \/\/ If we know we're inflating for entry it's better to inflate by swinging a\n+    \/\/ pre-locked ObjectMonitor pointer into the object header.   A successful\n+    \/\/ CAS inflates the object *and* confers ownership to the inflating thread.\n+    \/\/ In the current implementation we use a 2-step mechanism where we CAS()\n+    \/\/ to inflate and then CAS() again to try to swing _owner from null to current.\n+    \/\/ An inflateTry() method that we could call from enter() would be useful.\n+\n+    assert(mark.is_unlocked(), \"invariant: header=\" INTPTR_FORMAT, mark.value());\n+    ObjectMonitor* m = new ObjectMonitor(object);\n+    \/\/ prepare m for installation - set monitor to initial state\n+    m->set_header(mark);\n+\n+    if (object->cas_set_mark(markWord::encode(m), mark) != mark) {\n+      delete m;\n+      m = nullptr;\n+      continue;\n+      \/\/ interference - the markword changed - just retry.\n+      \/\/ The state-transitions are one-way, so there's no chance of\n+      \/\/ live-lock -- \"Inflated\" is an absorbing state.\n+    }\n+\n+    \/\/ Once the ObjectMonitor is configured and object is associated\n+    \/\/ with the ObjectMonitor, it is safe to allow async deflation:\n+    ObjectSynchronizer::_in_use_list.add(m);\n+\n+    log_inflate(current, object, cause);\n+    if (event.should_commit()) {\n+      post_monitor_inflate_event(&event, object, cause);\n+    }\n+    return m;\n+  }\n+}\n+\n+ObjectMonitor* ObjectSynchronizer::inflate_fast_locked_object(oop object, ObjectSynchronizer::InflateCause cause, JavaThread* locking_thread, JavaThread* current) {\n+  VerifyThreadState vts(locking_thread, current);\n+  assert(locking_thread->lock_stack().contains(object), \"locking_thread must have object on its lock stack\");\n+\n+  ObjectMonitor* monitor;\n+\n+  if (!UseObjectMonitorTable) {\n+    return inflate_into_object_header(object, cause, locking_thread, current);\n+  }\n+\n+  \/\/ Inflating requires a hash code\n+  ObjectSynchronizer::FastHashCode(current, object);\n+\n+  markWord mark = object->mark_acquire();\n+  assert(!mark.is_unlocked(), \"Cannot be unlocked\");\n+\n+  for (;;) {\n+    \/\/ Fetch the monitor from the table\n+    monitor = get_or_insert_monitor(object, current, cause);\n+\n+    \/\/ ObjectMonitors are always inserted as anonymously owned, this thread is\n+    \/\/ the current holder of the monitor. So unless the entry is stale and\n+    \/\/ contains a deflating monitor it must be anonymously owned.\n+    if (monitor->has_anonymous_owner()) {\n+      \/\/ The monitor must be anonymously owned if it was added\n+      assert(monitor == get_monitor_from_table(current, object), \"The monitor must be found\");\n+      \/\/ New fresh monitor\n+      break;\n+    }\n+\n+    \/\/ If the monitor was not anonymously owned then we got a deflating monitor\n+    \/\/ from the table. We need to let the deflator make progress and remove this\n+    \/\/ entry before we are allowed to add a new one.\n+    os::naked_yield();\n+    assert(monitor->is_being_async_deflated(), \"Should be the reason\");\n+  }\n+\n+  \/\/ Set the mark word; loop to handle concurrent updates to other parts of the mark word\n+  while (mark.is_fast_locked()) {\n+    mark = object->cas_set_mark(mark.set_has_monitor(), mark);\n+  }\n+\n+  \/\/ Indicate that the monitor now has a known owner\n+  monitor->set_owner_from_anonymous(locking_thread);\n+\n+  \/\/ Remove the entry from the thread's lock stack\n+  monitor->set_recursions(locking_thread->lock_stack().remove(object) - 1);\n+\n+  if (locking_thread == current) {\n+    \/\/ Only change the thread local state of the current thread.\n+    locking_thread->om_set_monitor_cache(monitor);\n+  }\n+\n+  return monitor;\n+}\n+\n+ObjectMonitor* ObjectSynchronizer::inflate_and_enter(oop object, BasicLock* lock, ObjectSynchronizer::InflateCause cause, JavaThread* locking_thread, JavaThread* current) {\n+  VerifyThreadState vts(locking_thread, current);\n+\n+  \/\/ Note: In some paths (deoptimization) the 'current' thread inflates and\n+  \/\/ enters the lock on behalf of the 'locking_thread' thread.\n+\n+  ObjectMonitor* monitor = nullptr;\n+\n+  if (!UseObjectMonitorTable) {\n+    \/\/ Do the old inflate and enter.\n+    monitor = inflate_into_object_header(object, cause, locking_thread, current);\n+\n+    bool entered;\n+    if (locking_thread == current) {\n+      entered = monitor->enter(locking_thread);\n+    } else {\n+      entered = monitor->enter_for(locking_thread);\n+    }\n+\n+    \/\/ enter returns false for deflation found.\n+    return entered ? monitor : nullptr;\n+  }\n+\n+  NoSafepointVerifier nsv;\n+\n+  \/\/ Try to get the monitor from the thread-local cache.\n+  \/\/ There's no need to use the cache if we are locking\n+  \/\/ on behalf of another thread.\n+  if (current == locking_thread) {\n+    monitor = read_caches(current, lock, object);\n+  }\n+\n+  \/\/ Get or create the monitor\n+  if (monitor == nullptr) {\n+    \/\/ Lightweight monitors require that hash codes are installed first\n+    ObjectSynchronizer::FastHashCode(locking_thread, object);\n+    monitor = get_or_insert_monitor(object, current, cause);\n+  }\n+\n+  if (monitor->try_enter(locking_thread)) {\n+    return monitor;\n+  }\n+\n+  \/\/ Holds is_being_async_deflated() stable throughout this function.\n+  ObjectMonitorContentionMark contention_mark(monitor);\n+\n+  \/\/\/ First handle the case where the monitor from the table is deflated\n+  if (monitor->is_being_async_deflated()) {\n+    \/\/ The MonitorDeflation thread is deflating the monitor. The locking thread\n+    \/\/ must spin until further progress has been made.\n+\n+    \/\/ Clear the BasicLock cache as it may contain this monitor.\n+    lock->clear_object_monitor_cache();\n+\n+    const markWord mark = object->mark_acquire();\n+\n+    if (mark.has_monitor()) {\n+      \/\/ Waiting on the deflation thread to remove the deflated monitor from the table.\n+      os::naked_yield();\n+\n+    } else if (mark.is_fast_locked()) {\n+      \/\/ Some other thread managed to fast-lock the lock, or this is a\n+      \/\/ recursive lock from the same thread; yield for the deflation\n+      \/\/ thread to remove the deflated monitor from the table.\n+      os::naked_yield();\n+\n+    } else {\n+      assert(mark.is_unlocked(), \"Implied\");\n+      \/\/ Retry immediately\n+    }\n+\n+    \/\/ Retry\n+    return nullptr;\n+  }\n+\n+  for (;;) {\n+    const markWord mark = object->mark_acquire();\n+    \/\/ The mark can be in one of the following states:\n+    \/\/ *  inflated     - If the ObjectMonitor owner is anonymous\n+    \/\/                   and the locking_thread owns the object\n+    \/\/                   lock, then we make the locking_thread\n+    \/\/                   the ObjectMonitor owner and remove the\n+    \/\/                   lock from the locking_thread's lock stack.\n+    \/\/ *  fast-locked  - Coerce it to inflated from fast-locked.\n+    \/\/ *  neutral      - Inflate the object. Successful CAS is locked\n+\n+    \/\/ CASE: inflated\n+    if (mark.has_monitor()) {\n+      LockStack& lock_stack = locking_thread->lock_stack();\n+      if (monitor->has_anonymous_owner() && lock_stack.contains(object)) {\n+        \/\/ The lock is fast-locked by the locking thread,\n+        \/\/ convert it to a held monitor with a known owner.\n+        monitor->set_owner_from_anonymous(locking_thread);\n+        monitor->set_recursions(lock_stack.remove(object) - 1);\n+      }\n+\n+      break; \/\/ Success\n+    }\n+\n+    \/\/ CASE: fast-locked\n+    \/\/ Could be fast-locked either by locking_thread or by some other thread.\n+    \/\/\n+    if (mark.is_fast_locked()) {\n+      markWord old_mark = object->cas_set_mark(mark.set_has_monitor(), mark);\n+      if (old_mark != mark) {\n+        \/\/ CAS failed\n+        continue;\n+      }\n+\n+      \/\/ Success! Return inflated monitor.\n+      LockStack& lock_stack = locking_thread->lock_stack();\n+      if (lock_stack.contains(object)) {\n+        \/\/ The lock is fast-locked by the locking thread,\n+        \/\/ convert it to a held monitor with a known owner.\n+        monitor->set_owner_from_anonymous(locking_thread);\n+        monitor->set_recursions(lock_stack.remove(object) - 1);\n+      }\n+\n+      break; \/\/ Success\n+    }\n+\n+    \/\/ CASE: neutral (unlocked)\n+\n+    \/\/ Catch if the object's header is not neutral (not locked and\n+    \/\/ not marked is what we care about here).\n+    assert(mark.is_neutral(), \"invariant: header=\" INTPTR_FORMAT, mark.value());\n+    markWord old_mark = object->cas_set_mark(mark.set_has_monitor(), mark);\n+    if (old_mark != mark) {\n+      \/\/ CAS failed\n+      continue;\n+    }\n+\n+    \/\/ Transitioned from unlocked to monitor means locking_thread owns the lock.\n+    monitor->set_owner_from_anonymous(locking_thread);\n+\n+    return monitor;\n+  }\n+\n+  if (current == locking_thread) {\n+    \/\/ One round of spinning\n+    if (monitor->spin_enter(locking_thread)) {\n+      return monitor;\n+    }\n+\n+    \/\/ Monitor is contended, take the time before entering to fix the lock stack.\n+    LockStackInflateContendedLocks().inflate(current);\n+  }\n+\n+  \/\/ enter can block for safepoints; clear the unhandled object oop\n+  PauseNoSafepointVerifier pnsv(&nsv);\n+  object = nullptr;\n+\n+  if (current == locking_thread) {\n+    monitor->enter_with_contention_mark(locking_thread, contention_mark);\n+  } else {\n+    monitor->enter_for_with_contention_mark(locking_thread, contention_mark);\n+  }\n+\n+  return monitor;\n+}\n+\n+void ObjectSynchronizer::deflate_monitor(Thread* current, oop obj, ObjectMonitor* monitor) {\n+  if (obj != nullptr) {\n+    deflate_mark_word(obj);\n+  }\n+  bool removed = remove_monitor(current, monitor, obj);\n+  if (obj != nullptr) {\n+    assert(removed, \"Should have removed the entry if obj was alive\");\n+  }\n+}\n+\n+ObjectMonitor* ObjectSynchronizer::get_monitor_from_table(Thread* current, oop obj) {\n+  assert(UseObjectMonitorTable, \"must be\");\n+  return ObjectMonitorTable::monitor_get(current, obj);\n+}\n+\n+bool ObjectSynchronizer::contains_monitor(Thread* current, ObjectMonitor* monitor) {\n+  assert(UseObjectMonitorTable, \"must be\");\n+  return ObjectMonitorTable::contains_monitor(current, monitor);\n+}\n+\n+ObjectMonitor* ObjectSynchronizer::read_monitor(markWord mark) {\n+  return mark.monitor();\n+}\n+\n+ObjectMonitor* ObjectSynchronizer::read_monitor(Thread* current, oop obj) {\n+  return ObjectSynchronizer::read_monitor(current, obj, obj->mark());\n+}\n+\n+ObjectMonitor* ObjectSynchronizer::read_monitor(Thread* current, oop obj, markWord mark) {\n+  if (!UseObjectMonitorTable) {\n+    return read_monitor(mark);\n+  } else {\n+    return ObjectSynchronizer::get_monitor_from_table(current, obj);\n+  }\n+}\n+\n+bool ObjectSynchronizer::quick_enter_internal(oop obj, BasicLock* lock, JavaThread* current) {\n+  assert(current->thread_state() == _thread_in_Java, \"must be\");\n+  assert(obj != nullptr, \"must be\");\n+  NoSafepointVerifier nsv;\n+\n+  LockStack& lock_stack = current->lock_stack();\n+  if (lock_stack.is_full()) {\n+    \/\/ Always go into runtime if the lock stack is full.\n+    return false;\n+  }\n+\n+  const markWord mark = obj->mark();\n+\n+#ifndef _LP64\n+  \/\/ Only for 32bit which has limited support for fast locking outside the runtime.\n+  if (lock_stack.try_recursive_enter(obj)) {\n+    \/\/ Recursive lock successful.\n+    return true;\n+  }\n+\n+  if (mark.is_unlocked()) {\n+    markWord locked_mark = mark.set_fast_locked();\n+    if (obj->cas_set_mark(locked_mark, mark) == mark) {\n+      \/\/ Successfully fast-locked, push object to lock-stack and return.\n+      lock_stack.push(obj);\n+      return true;\n+    }\n+  }\n+#endif\n+\n+  if (mark.has_monitor()) {\n+    ObjectMonitor* monitor;\n+    if (UseObjectMonitorTable) {\n+      monitor = read_caches(current, lock, obj);\n+    } else {\n+      monitor = ObjectSynchronizer::read_monitor(mark);\n+    }\n+\n+    if (monitor == nullptr) {\n+      \/\/ Take the slow-path on a cache miss.\n+      return false;\n+    }\n+\n+    if (UseObjectMonitorTable) {\n+      \/\/ Set the monitor regardless of success.\n+      \/\/ Either we successfully lock on the monitor, or we retry with the\n+      \/\/ monitor in the slow path. If the monitor gets deflated, it will be\n+      \/\/ cleared, either by the CacheSetter if we fast lock in enter or in\n+      \/\/ inflate_and_enter when we see that the monitor is deflated.\n+      lock->set_object_monitor_cache(monitor);\n+    }\n+\n+    if (monitor->spin_enter(current)) {\n+      return true;\n+    }\n+  }\n+\n+  \/\/ Slow-path.\n+  return false;\n+}\n+\n+bool ObjectSynchronizer::quick_enter(oop obj, BasicLock* lock, JavaThread* current) {\n+  assert(current->thread_state() == _thread_in_Java, \"invariant\");\n+  NoSafepointVerifier nsv;\n+  if (obj == nullptr) return false;       \/\/ Need to throw NPE\n+\n+  if (obj->klass()->is_value_based()) {\n+    return false;\n+  }\n+\n+  return ObjectSynchronizer::quick_enter_internal(obj, lock, current);\n+}\n","filename":"src\/hotspot\/share\/runtime\/synchronizer.cpp","additions":1233,"deletions":51,"binary":false,"changes":1284,"status":"modified"},{"patch":"@@ -30,1 +30,1 @@\n-#include \"cds\/heapShared.hpp\"\n+#include \"cds\/heapShared.inline.hpp\"\n@@ -105,0 +105,1 @@\n+#include \"utilities\/debug.hpp\"\n@@ -386,0 +387,2 @@\n+  HeapShared::materialize_thread_object();\n+\n@@ -567,1 +570,2 @@\n-  \/\/ Attach the main thread to this os thread\n+  \/\/ Attach the main thread to this os thread. It is added to the threads list inside\n+  \/\/ universe_init(), within init_globals().\n@@ -581,1 +585,3 @@\n-  main_thread->set_monitor_owner_id(ThreadIdentifier::next());\n+  const int64_t main_thread_tid = ThreadIdentifier::next();\n+  guarantee(main_thread_tid == 3, \"Must equal the PRIMORDIAL_TID used in Threads.java\");\n+  main_thread->set_monitor_owner_id(main_thread_tid);\n@@ -617,8 +623,0 @@\n-  \/\/ Add main_thread to threads list to finish barrier setup with\n-  \/\/ on_thread_attach.  Should be before starting to build Java objects in\n-  \/\/ init_globals2, which invokes barriers.\n-  {\n-    MutexLocker mu(Threads_lock);\n-    Threads::add(main_thread);\n-  }\n-\n@@ -708,0 +706,3 @@\n+  \/\/ Prepare AOT heap loader for GC.\n+  HeapShared::enable_gc();\n+\n@@ -903,0 +904,3 @@\n+  \/\/ Finish materializing AOT objects\n+  HeapShared::finish_materialize_objects();\n+\n","filename":"src\/hotspot\/share\/runtime\/threads.cpp","additions":15,"deletions":11,"binary":false,"changes":26,"status":"modified"},{"patch":"@@ -540,1 +540,1 @@\n-  nonstatic_field(nmethod,                     _deopt_handler_entry_offset,                   int)                                   \\\n+  nonstatic_field(nmethod,                     _deopt_handler_offset,                         int)                                   \\\n","filename":"src\/hotspot\/share\/runtime\/vmStructs.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -87,1 +87,1 @@\n-        @JEP(number=505, title=\"Structured Concurrency\", status=\"Fifth Preview\")\n+        @JEP(number=525, title=\"Structured Concurrency\", status=\"Sixth Preview\")\n","filename":"src\/java.base\/share\/classes\/jdk\/internal\/javac\/PreviewFeature.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -294,0 +294,1 @@\n+        CAPTURE_MREF_RETURN_TYPE(JDK26),\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/code\/Source.java","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -128,0 +128,1 @@\n+    final boolean captureMRefReturnType;\n@@ -180,0 +181,1 @@\n+        captureMRefReturnType = Source.Feature.ERASE_POLY_SIG_RETURN_TYPE.allowedInSource(source);\n@@ -2367,1 +2369,1 @@\n-                        checkAutoCloseable(resource.pos(), localEnv, resource.type);\n+                        checkAutoCloseable(localEnv, resource, true);\n@@ -2416,1 +2418,3 @@\n-    void checkAutoCloseable(DiagnosticPosition pos, Env<AttrContext> env, Type resource) {\n+    void checkAutoCloseable(Env<AttrContext> env, JCTree tree, boolean useSite) {\n+        DiagnosticPosition pos = tree.pos();\n+        Type resource = tree.type;\n@@ -2434,1 +2438,2 @@\n-                    close.overrides(syms.autoCloseableClose, resource.tsym, types, true) &&\n+                    (useSite || close.owner != syms.autoCloseableType.tsym) &&\n+                    ((MethodSymbol)close).binaryOverrides(syms.autoCloseableClose, resource.tsym, types) &&\n@@ -2436,1 +2441,6 @@\n-                log.warning(pos, LintWarnings.TryResourceThrowsInterruptedExc(resource));\n+                if (!useSite && close.owner == resource.tsym) {\n+                    log.warning(TreeInfo.diagnosticPositionFor(close, tree),\n+                        LintWarnings.TryResourceCanThrowInterruptedExc(resource));\n+                } else {\n+                    log.warning(pos, LintWarnings.TryResourceThrowsInterruptedExc(resource));\n+                }\n@@ -4209,0 +4219,1 @@\n+            Type capturedResType = captureMRefReturnType ? types.capture(resType) : resType;\n@@ -4210,2 +4221,2 @@\n-                    new FunctionalReturnContext(checkContext).compatible(resType, returnType,\n-                            checkContext.checkWarner(tree, resType, returnType))) {\n+                    new FunctionalReturnContext(checkContext).compatible(capturedResType, returnType,\n+                            checkContext.checkWarner(tree, capturedResType, returnType))) {\n@@ -6040,1 +6051,1 @@\n-        checkAutoCloseable(tree.pos(), env, c.type);\n+        checkAutoCloseable(env, tree, false);\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/comp\/Attr.java","additions":18,"deletions":7,"binary":false,"changes":25,"status":"modified"},{"patch":"@@ -3179,8 +3179,12 @@\n-                if ((flags & STATIC) == 0) {\n-                    ((ClassType)member.type).setEnclosingType(outer.type);\n-                    if (member.erasure_field != null)\n-                        ((ClassType)member.erasure_field).setEnclosingType(types.erasure(outer.type));\n-                }\n-                if (c == outer && member.owner == c) {\n-                    member.flags_field = flags;\n-                    enterMember(c, member);\n+                if ((member.flags_field & FROM_SOURCE) == 0) {\n+                    if ((flags & STATIC) == 0) {\n+                        ((ClassType)member.type).setEnclosingType(outer.type);\n+                        if (member.erasure_field != null)\n+                            ((ClassType)member.erasure_field).setEnclosingType(types.erasure(outer.type));\n+                    }\n+                    if (c == outer && member.owner == c) {\n+                        member.flags_field = flags;\n+                        enterMember(c, member);\n+                    }\n+                } else if ((flags & STATIC) != (member.flags_field & STATIC)) {\n+                    log.warning(LintWarnings.InconsistentInnerClasses(member, currentClassFile));\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/jvm\/ClassReader.java","additions":12,"deletions":8,"binary":false,"changes":20,"status":"modified"},{"patch":"@@ -2269,0 +2269,6 @@\n+# 0: symbol, 1: file object\n+# lint: classfile\n+compiler.warn.inconsistent.inner.classes=\\\n+    InnerClasses attribute for {0} in {1} inconsistent with source code\\n\\\n+    ({1} may need to be recompiled with {0})\n+\n@@ -2379,0 +2385,5 @@\n+# 0: type\n+# lint: try\n+compiler.warn.try.resource.can.throw.interrupted.exc=\\\n+    close() method can throw InterruptedException in auto-closeable class {0}\n+\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/resources\/compiler.properties","additions":11,"deletions":0,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -1317,0 +1317,6 @@\n+When a JAR file in the user class path has a `Class-Path` manifest attribute,\n+and the specified JAR file(s) exist, they are automatically inserted into the\n+user class path after the JAR file. This rule also applies recursively to any\n+new JAR files found. Consult the [JAR File Specification](..\/jar\/jar.html#class-path-attribute)\n+for details.\n+\n","filename":"src\/jdk.compiler\/share\/man\/javac.md","additions":6,"deletions":0,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -275,1 +275,0 @@\n-vmTestbase\/nsk\/jvmti\/scenarios\/events\/EM02\/em02t006\/TestDescription.java 8371103 generic-all\n","filename":"test\/hotspot\/jtreg\/ProblemList.txt","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -332,1 +332,2 @@\n-  gc\/stress\/gcbasher\/TestGCBasherWithParallel.java\n+  gc\/stress\/gcbasher\/TestGCBasherWithParallel.java \\\n+  gc\/stress\/gcbasher\/TestGCBasherWithZ.java\n","filename":"test\/hotspot\/jtreg\/TEST.groups","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -2119,0 +2119,10 @@\n+    public static final String VECTOR_LOAD_MASK = PREFIX + \"VECTOR_LOAD_MASK\" + POSTFIX;\n+    static {\n+        beforeMatchingNameRegex(VECTOR_LOAD_MASK, \"VectorLoadMask\");\n+    }\n+\n+    public static final String VECTOR_STORE_MASK = PREFIX + \"VECTOR_STORE_MASK\" + POSTFIX;\n+    static {\n+        beforeMatchingNameRegex(VECTOR_STORE_MASK, \"VectorStoreMask\");\n+    }\n+\n","filename":"test\/hotspot\/jtreg\/compiler\/lib\/ir_framework\/IRNode.java","additions":10,"deletions":0,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -117,0 +117,1 @@\n+        \"svebitperm\",\n","filename":"test\/hotspot\/jtreg\/compiler\/lib\/ir_framework\/test\/IREncodingPrinter.java","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -182,1 +182,1 @@\n-java\/awt\/Mixing\/AWT_Mixing\/JTableInGlassPaneOverlapping.java 8158801 windows-all\n+java\/awt\/Mixing\/AWT_Mixing\/JTableInGlassPaneOverlapping.java 8158801,8357360 windows-all,linux-all\n@@ -763,0 +763,1 @@\n+jdk\/jfr\/event\/oldobject\/TestEmergencyDumpAtOOM.java             8371014 aix-ppc64,linux-ppc64le\n@@ -765,1 +766,0 @@\n-jdk\/jfr\/jvm\/TestWaste.java                                      8369949 generic-all\n","filename":"test\/jdk\/ProblemList.txt","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -129,0 +129,2 @@\n+        map.put(\"vm.cds.write.mapped.java.heap\", this::vmCDSCanWriteMappedArchivedJavaHeap);\n+        map.put(\"vm.cds.write.streamed.java.heap\", this::vmCDSCanWriteStreamedArchivedJavaHeap);\n@@ -489,1 +491,1 @@\n-     *         if -XX:-UseCompressedClassPointers is specified,\n+     *         if -XX:-UseCompressedClassPointers is specified.\n@@ -492,2 +494,19 @@\n-        return \"\" + (\"true\".equals(vmCDS()) && WB.canWriteJavaHeapArchive()\n-                     && isCDSRuntimeOptionsCompatible());\n+        return \"\" + (\"true\".equals(vmCDS()) && WB.canWriteJavaHeapArchive());\n+    }\n+\n+    \/**\n+     * @return true if it's possible for \"java -Xshare:dump\" to write Java heap objects\n+     *         with the current set of jtreg VM options. For example, false will be returned\n+     *         if -XX:-UseCompressedClassPointers is specified.\n+     *\/\n+    protected String vmCDSCanWriteMappedArchivedJavaHeap() {\n+        return \"\" + (\"true\".equals(vmCDS()) && WB.canWriteMappedJavaHeapArchive());\n+    }\n+\n+    \/**\n+     * @return true if it's possible for \"java -Xshare:dump\" to write Java heap objects\n+     *         with the current set of jtreg VM options. For example, false will be returned\n+     *         if -XX:-UseCompressedClassPointers is specified.\n+     *\/\n+    protected String vmCDSCanWriteStreamedArchivedJavaHeap() {\n+        return \"\" + (\"true\".equals(vmCDS()) && WB.canWriteStreamedJavaHeapArchive());\n@@ -518,25 +537,0 @@\n-    \/**\n-     * @return true if the VM options specified via the \"test.cds.runtime.options\"\n-     * property is compatible with writing Java heap objects into the CDS archive\n-     *\/\n-    protected boolean isCDSRuntimeOptionsCompatible() {\n-        String jtropts = System.getProperty(\"test.cds.runtime.options\");\n-        if (jtropts == null) {\n-            return true;\n-        }\n-        String CCP_DISABLED = \"-XX:-UseCompressedClassPointers\";\n-        String G1GC_ENABLED = \"-XX:+UseG1GC\";\n-        String PARALLELGC_ENABLED = \"-XX:+UseParallelGC\";\n-        String SERIALGC_ENABLED = \"-XX:+UseSerialGC\";\n-        for (String opt : jtropts.split(\",\")) {\n-            if (opt.equals(CCP_DISABLED)) {\n-                return false;\n-            }\n-            if (opt.startsWith(GC_PREFIX) && opt.endsWith(GC_SUFFIX) &&\n-                !opt.equals(G1GC_ENABLED) && !opt.equals(PARALLELGC_ENABLED) && !opt.equals(SERIALGC_ENABLED)) {\n-                return false;\n-            }\n-        }\n-        return true;\n-    }\n-\n","filename":"test\/jtreg-ext\/requires\/VMProps.java","additions":22,"deletions":28,"binary":false,"changes":50,"status":"modified"},{"patch":"@@ -34,0 +34,1 @@\n+    public ArrayList<String> engineOpts = new ArrayList<>();\n@@ -74,0 +75,5 @@\n+    public final DockerRunOptions addEngineOpts(String... opts) {\n+        Collections.addAll(engineOpts, opts);\n+        return this;\n+    }\n+\n","filename":"test\/lib\/jdk\/test\/lib\/containers\/docker\/DockerRunOptions.java","additions":6,"deletions":0,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -127,1 +127,1 @@\n-  public native boolean supportsRecursiveLightweightLocking();\n+  public native boolean supportsRecursiveFastLocking();\n@@ -820,0 +820,2 @@\n+  public native boolean canWriteMappedJavaHeapArchive();\n+  public native boolean canWriteStreamedJavaHeapArchive();\n","filename":"test\/lib\/jdk\/test\/whitebox\/WhiteBox.java","additions":3,"deletions":1,"binary":false,"changes":4,"status":"modified"}]}