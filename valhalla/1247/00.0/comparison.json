{"files":[{"patch":"@@ -106,0 +106,1 @@\n+        --add-exports java.base\/jdk.internal.util=ALL-UNNAMED \\\n","filename":"make\/test\/BuildMicrobenchmark.gmk","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -1129,0 +1129,4 @@\n+\/\/ Figure out which register class each belongs in: rc_int, rc_float or\n+\/\/ rc_stack.\n+enum RC { rc_bad, rc_int, rc_float, rc_predicate, rc_stack };\n+\n@@ -1858,4 +1862,0 @@\n-\/\/ Figure out which register class each belongs in: rc_int, rc_float or\n-\/\/ rc_stack.\n-enum RC { rc_bad, rc_int, rc_float, rc_predicate, rc_stack };\n-\n","filename":"src\/hotspot\/cpu\/aarch64\/aarch64.ad","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -195,5 +195,0 @@\n-    \/\/ Check if the secondary index definition is still ~x, otherwise\n-    \/\/ we have to change the following assembler code to calculate the\n-    \/\/ plain index.\n-    assert(ConstantPool::decode_invokedynamic_index(~123) == 123, \"else change next line\");\n-    eonw(index, index, zr);  \/\/ convert to plain index\n","filename":"src\/hotspot\/cpu\/aarch64\/interp_masm_aarch64.cpp","additions":0,"deletions":5,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -2800,1 +2800,1 @@\n-int MacroAssembler::push_fp(unsigned int bitset, Register stack) {\n+int MacroAssembler::push_fp(unsigned int bitset, Register stack, FpPushPopMode mode) {\n@@ -2823,2 +2823,23 @@\n-  \/\/ SVE\n-  if (use_sve && sve_vector_size_in_bytes > 16) {\n+  if (mode == PushPopFull) {\n+    if (use_sve && sve_vector_size_in_bytes > 16) {\n+      mode = PushPopSVE;\n+    } else {\n+      mode = PushPopNeon;\n+    }\n+  }\n+\n+#ifndef PRODUCT\n+  {\n+    char buffer[48];\n+    if (mode == PushPopSVE) {\n+      snprintf(buffer, sizeof(buffer), \"push_fp: %d SVE registers\", count);\n+    } else if (mode == PushPopNeon) {\n+      snprintf(buffer, sizeof(buffer), \"push_fp: %d Neon registers\", count);\n+    } else {\n+      snprintf(buffer, sizeof(buffer), \"push_fp: %d fp registers\", count);\n+    }\n+    block_comment(buffer);\n+  }\n+#endif\n+\n+  if (mode == PushPopSVE) {\n@@ -2832,4 +2853,25 @@\n-  \/\/ NEON\n-  if (count == 1) {\n-    strq(as_FloatRegister(regs[0]), Address(pre(stack, -wordSize * 2)));\n-    return 2;\n+  if (mode == PushPopNeon) {\n+    if (count == 1) {\n+      strq(as_FloatRegister(regs[0]), Address(pre(stack, -wordSize * 2)));\n+      return 2;\n+    }\n+\n+    bool odd = (count & 1) == 1;\n+    int push_slots = count + (odd ? 1 : 0);\n+\n+    \/\/ Always pushing full 128 bit registers.\n+    stpq(as_FloatRegister(regs[0]), as_FloatRegister(regs[1]), Address(pre(stack, -push_slots * wordSize * 2)));\n+    words_pushed += 2;\n+\n+    for (int i = 2; i + 1 < count; i += 2) {\n+      stpq(as_FloatRegister(regs[i]), as_FloatRegister(regs[i+1]), Address(stack, i * wordSize * 2));\n+      words_pushed += 2;\n+    }\n+\n+    if (odd) {\n+      strq(as_FloatRegister(regs[count - 1]), Address(stack, (count - 1) * wordSize * 2));\n+      words_pushed++;\n+    }\n+\n+    assert(words_pushed == count, \"oops, pushed(%d) != count(%d)\", words_pushed, count);\n+    return count * 2;\n@@ -2838,2 +2880,3 @@\n-  bool odd = (count & 1) == 1;\n-  int push_slots = count + (odd ? 1 : 0);\n+  if (mode == PushPopFp) {\n+    bool odd = (count & 1) == 1;\n+    int push_slots = count + (odd ? 1 : 0);\n@@ -2841,3 +2884,5 @@\n-  \/\/ Always pushing full 128 bit registers.\n-  stpq(as_FloatRegister(regs[0]), as_FloatRegister(regs[1]), Address(pre(stack, -push_slots * wordSize * 2)));\n-  words_pushed += 2;\n+    if (count == 1) {\n+      \/\/ Stack pointer must be 16 bytes aligned\n+      strd(as_FloatRegister(regs[0]), Address(pre(stack, -push_slots * wordSize)));\n+      return 1;\n+    }\n@@ -2845,2 +2890,1 @@\n-  for (int i = 2; i + 1 < count; i += 2) {\n-    stpq(as_FloatRegister(regs[i]), as_FloatRegister(regs[i+1]), Address(stack, i * wordSize * 2));\n+    stpd(as_FloatRegister(regs[0]), as_FloatRegister(regs[1]), Address(pre(stack, -push_slots * wordSize)));\n@@ -2848,4 +2892,14 @@\n-  }\n-  if (odd) {\n-    strq(as_FloatRegister(regs[count - 1]), Address(stack, (count - 1) * wordSize * 2));\n-    words_pushed++;\n+    for (int i = 2; i + 1 < count; i += 2) {\n+      stpd(as_FloatRegister(regs[i]), as_FloatRegister(regs[i+1]), Address(stack, i * wordSize));\n+      words_pushed += 2;\n+    }\n+\n+    if (odd) {\n+      \/\/ Stack pointer must be 16 bytes aligned\n+      strd(as_FloatRegister(regs[count - 1]), Address(stack, (count - 1) * wordSize));\n+      words_pushed++;\n+    }\n+\n+    assert(words_pushed == count, \"oops, pushed != count\");\n+\n+    return count;\n@@ -2855,2 +2909,1 @@\n-  assert(words_pushed == count, \"oops, pushed(%d) != count(%d)\", words_pushed, count);\n-  return count * 2;\n+  return 0;\n@@ -2860,1 +2913,1 @@\n-int MacroAssembler::pop_fp(unsigned int bitset, Register stack) {\n+int MacroAssembler::pop_fp(unsigned int bitset, Register stack, FpPushPopMode mode) {\n@@ -2882,2 +2935,23 @@\n-  \/\/ SVE\n-  if (use_sve && sve_vector_size_in_bytes > 16) {\n+  if (mode == PushPopFull) {\n+    if (use_sve && sve_vector_size_in_bytes > 16) {\n+      mode = PushPopSVE;\n+    } else {\n+      mode = PushPopNeon;\n+    }\n+  }\n+\n+#ifndef PRODUCT\n+  {\n+    char buffer[48];\n+    if (mode == PushPopSVE) {\n+      snprintf(buffer, sizeof(buffer), \"pop_fp: %d SVE registers\", count);\n+    } else if (mode == PushPopNeon) {\n+      snprintf(buffer, sizeof(buffer), \"pop_fp: %d Neon registers\", count);\n+    } else {\n+      snprintf(buffer, sizeof(buffer), \"pop_fp: %d fp registers\", count);\n+    }\n+    block_comment(buffer);\n+  }\n+#endif\n+\n+  if (mode == PushPopSVE) {\n@@ -2891,5 +2965,5 @@\n-  \/\/ NEON\n-  if (count == 1) {\n-    ldrq(as_FloatRegister(regs[0]), Address(post(stack, wordSize * 2)));\n-    return 2;\n-  }\n+  if (mode == PushPopNeon) {\n+    if (count == 1) {\n+      ldrq(as_FloatRegister(regs[0]), Address(post(stack, wordSize * 2)));\n+      return 2;\n+    }\n@@ -2897,2 +2971,2 @@\n-  bool odd = (count & 1) == 1;\n-  int push_slots = count + (odd ? 1 : 0);\n+    bool odd = (count & 1) == 1;\n+    int push_slots = count + (odd ? 1 : 0);\n@@ -2900,4 +2974,9 @@\n-  if (odd) {\n-    ldrq(as_FloatRegister(regs[count - 1]), Address(stack, (count - 1) * wordSize * 2));\n-    words_pushed++;\n-  }\n+    if (odd) {\n+      ldrq(as_FloatRegister(regs[count - 1]), Address(stack, (count - 1) * wordSize * 2));\n+      words_pushed++;\n+    }\n+\n+    for (int i = 2; i + 1 < count; i += 2) {\n+      ldpq(as_FloatRegister(regs[i]), as_FloatRegister(regs[i+1]), Address(stack, i * wordSize * 2));\n+      words_pushed += 2;\n+    }\n@@ -2905,2 +2984,1 @@\n-  for (int i = 2; i + 1 < count; i += 2) {\n-    ldpq(as_FloatRegister(regs[i]), as_FloatRegister(regs[i+1]), Address(stack, i * wordSize * 2));\n+    ldpq(as_FloatRegister(regs[0]), as_FloatRegister(regs[1]), Address(post(stack, push_slots * wordSize * 2)));\n@@ -2908,0 +2986,4 @@\n+\n+    assert(words_pushed == count, \"oops, pushed(%d) != count(%d)\", words_pushed, count);\n+\n+    return count * 2;\n@@ -2910,2 +2992,8 @@\n-  ldpq(as_FloatRegister(regs[0]), as_FloatRegister(regs[1]), Address(post(stack, push_slots * wordSize * 2)));\n-  words_pushed += 2;\n+  if (mode == PushPopFp) {\n+    bool odd = (count & 1) == 1;\n+    int push_slots = count + (odd ? 1 : 0);\n+\n+    if (count == 1) {\n+      ldrd(as_FloatRegister(regs[0]), Address(post(stack, push_slots * wordSize)));\n+      return 1;\n+    }\n@@ -2913,1 +3001,17 @@\n-  assert(words_pushed == count, \"oops, pushed(%d) != count(%d)\", words_pushed, count);\n+    if (odd) {\n+      ldrd(as_FloatRegister(regs[count - 1]), Address(stack, (count - 1) * wordSize));\n+      words_pushed++;\n+    }\n+\n+    for (int i = 2; i + 1 < count; i += 2) {\n+      ldpd(as_FloatRegister(regs[i]), as_FloatRegister(regs[i+1]), Address(stack, i * wordSize));\n+      words_pushed += 2;\n+    }\n+\n+    ldpd(as_FloatRegister(regs[0]), as_FloatRegister(regs[1]), Address(post(stack, push_slots * wordSize)));\n+    words_pushed += 2;\n+\n+    assert(words_pushed == count, \"oops, pushed != count\");\n+\n+    return count;\n+  }\n@@ -2915,1 +3019,1 @@\n-  return count * 2;\n+  return 0;\n@@ -5415,1 +5519,1 @@\n-  \/\/ test to see if it has a finalizer or is malformed in some way\n+  \/\/ test to see if it is malformed in some way\n","filename":"src\/hotspot\/cpu\/aarch64\/macroAssembler_aarch64.cpp","additions":145,"deletions":41,"binary":false,"changes":186,"status":"modified"},{"patch":"@@ -452,0 +452,9 @@\n+public:\n+\n+  enum FpPushPopMode {\n+    PushPopFull,\n+    PushPopSVE,\n+    PushPopNeon,\n+    PushPopFp\n+  };\n+\n@@ -461,2 +470,2 @@\n-  int push_fp(unsigned int bitset, Register stack);\n-  int pop_fp(unsigned int bitset, Register stack);\n+  int push_fp(unsigned int bitset, Register stack, FpPushPopMode mode);\n+  int pop_fp(unsigned int bitset, Register stack, FpPushPopMode mode);\n@@ -470,0 +479,1 @@\n+\n@@ -473,2 +483,2 @@\n-  void push_fp(FloatRegSet regs, Register stack) { if (regs.bits()) push_fp(regs.bits(), stack); }\n-  void pop_fp(FloatRegSet regs, Register stack) { if (regs.bits()) pop_fp(regs.bits(), stack); }\n+  void push_fp(FloatRegSet regs, Register stack, FpPushPopMode mode = PushPopFull) { if (regs.bits()) push_fp(regs.bits(), stack, mode); }\n+  void pop_fp(FloatRegSet regs, Register stack, FpPushPopMode mode = PushPopFull) { if (regs.bits()) pop_fp(regs.bits(), stack, mode); }\n","filename":"src\/hotspot\/cpu\/aarch64\/macroAssembler_aarch64.hpp","additions":14,"deletions":4,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -1538,1 +1538,1 @@\n-      \/\/ UnsafeCopyMemory page error: continue after ucm\n+      \/\/ UnsafeMemoryAccess page error: continue after unsafe access\n@@ -1540,1 +1540,1 @@\n-      UnsafeCopyMemoryMark ucmm(this, add_entry, true);\n+      UnsafeMemoryAccessMark umam(this, add_entry, true);\n@@ -1609,1 +1609,1 @@\n-      \/\/ UnsafeCopyMemory page error: continue after ucm\n+      \/\/ UnsafeMemoryAccess page error: continue after unsafe access\n@@ -1611,1 +1611,1 @@\n-      UnsafeCopyMemoryMark ucmm(this, add_entry, true);\n+      UnsafeMemoryAccessMark umam(this, add_entry, true);\n@@ -8531,2 +8531,2 @@\n-    if (UnsafeCopyMemory::_table == nullptr) {\n-      UnsafeCopyMemory::create_table(8);\n+    if (UnsafeMemoryAccess::_table == nullptr) {\n+      UnsafeMemoryAccess::create_table(8 + 4); \/\/ 8 for copyMemory; 4 for setMemory\n","filename":"src\/hotspot\/cpu\/aarch64\/stubGenerator_aarch64.cpp","additions":6,"deletions":6,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -447,2 +447,0 @@\n-    assert(ConstantPool::decode_invokedynamic_index(~123) == 123, \"else change next line\");\n-    nand(Rdst, Rdst, Rdst); \/\/ convert to plain index\n","filename":"src\/hotspot\/cpu\/ppc\/interp_masm_ppc_64.cpp","additions":0,"deletions":2,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -327,6 +327,0 @@\n-\n-    \/\/ Check if the secondary index definition is still ~x, otherwise\n-    \/\/ we have to change the following assembler code to calculate the\n-    \/\/ plain index.\n-    assert(ConstantPool::decode_invokedynamic_index(~123) == 123, \"else change next line\");\n-    not_(index);  \/\/ Convert to plain index.\n","filename":"src\/hotspot\/cpu\/s390\/interp_masm_s390.cpp","additions":0,"deletions":6,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -1828,0 +1828,124 @@\n+#ifdef _LP64\n+void C2_MacroAssembler::vgather8b_masked_offset(BasicType elem_bt,\n+                                                XMMRegister dst, Register base,\n+                                                Register idx_base,\n+                                                Register offset, Register mask,\n+                                                Register mask_idx, Register rtmp,\n+                                                int vlen_enc) {\n+  vpxor(dst, dst, dst, vlen_enc);\n+  if (elem_bt == T_SHORT) {\n+    for (int i = 0; i < 4; i++) {\n+      \/\/ dst[i] = mask[i] ? src[offset + idx_base[i]] : 0\n+      Label skip_load;\n+      btq(mask, mask_idx);\n+      jccb(Assembler::carryClear, skip_load);\n+      movl(rtmp, Address(idx_base, i * 4));\n+      if (offset != noreg) {\n+        addl(rtmp, offset);\n+      }\n+      pinsrw(dst, Address(base, rtmp, Address::times_2), i);\n+      bind(skip_load);\n+      incq(mask_idx);\n+    }\n+  } else {\n+    assert(elem_bt == T_BYTE, \"\");\n+    for (int i = 0; i < 8; i++) {\n+      \/\/ dst[i] = mask[i] ? src[offset + idx_base[i]] : 0\n+      Label skip_load;\n+      btq(mask, mask_idx);\n+      jccb(Assembler::carryClear, skip_load);\n+      movl(rtmp, Address(idx_base, i * 4));\n+      if (offset != noreg) {\n+        addl(rtmp, offset);\n+      }\n+      pinsrb(dst, Address(base, rtmp), i);\n+      bind(skip_load);\n+      incq(mask_idx);\n+    }\n+  }\n+}\n+#endif \/\/ _LP64\n+\n+void C2_MacroAssembler::vgather8b_offset(BasicType elem_bt, XMMRegister dst,\n+                                         Register base, Register idx_base,\n+                                         Register offset, Register rtmp,\n+                                         int vlen_enc) {\n+  vpxor(dst, dst, dst, vlen_enc);\n+  if (elem_bt == T_SHORT) {\n+    for (int i = 0; i < 4; i++) {\n+      \/\/ dst[i] = src[offset + idx_base[i]]\n+      movl(rtmp, Address(idx_base, i * 4));\n+      if (offset != noreg) {\n+        addl(rtmp, offset);\n+      }\n+      pinsrw(dst, Address(base, rtmp, Address::times_2), i);\n+    }\n+  } else {\n+    assert(elem_bt == T_BYTE, \"\");\n+    for (int i = 0; i < 8; i++) {\n+      \/\/ dst[i] = src[offset + idx_base[i]]\n+      movl(rtmp, Address(idx_base, i * 4));\n+      if (offset != noreg) {\n+        addl(rtmp, offset);\n+      }\n+      pinsrb(dst, Address(base, rtmp), i);\n+    }\n+  }\n+}\n+\n+\/*\n+ * Gather using hybrid algorithm, first partially unroll scalar loop\n+ * to accumulate values from gather indices into a quad-word(64bit) slice.\n+ * A slice may hold 8 bytes or 4 short values. This is followed by a vector\n+ * permutation to place the slice into appropriate vector lane\n+ * locations in destination vector. Following pseudo code describes the\n+ * algorithm in detail:\n+ *\n+ * DST_VEC = ZERO_VEC\n+ * PERM_INDEX = {0, 1, 2, 3, 4, 5, 6, 7, 8..}\n+ * TWO_VEC    = {2, 2, 2, 2, 2, 2, 2, 2, 2..}\n+ * FOREACH_ITER:\n+ *     TMP_VEC_64 = PICK_SUB_WORDS_FROM_GATHER_INDICES\n+ *     TEMP_PERM_VEC = PERMUTE TMP_VEC_64 PERM_INDEX\n+ *     DST_VEC = DST_VEC OR TEMP_PERM_VEC\n+ *     PERM_INDEX = PERM_INDEX - TWO_VEC\n+ *\n+ * With each iteration, doubleword permute indices (0,1) corresponding\n+ * to gathered quadword gets right shifted by two lane positions.\n+ *\n+ *\/\n+void C2_MacroAssembler::vgather_subword(BasicType elem_ty, XMMRegister dst,\n+                                        Register base, Register idx_base,\n+                                        Register offset, Register mask,\n+                                        XMMRegister xtmp1, XMMRegister xtmp2,\n+                                        XMMRegister temp_dst, Register rtmp,\n+                                        Register mask_idx, Register length,\n+                                        int vector_len, int vlen_enc) {\n+  Label GATHER8_LOOP;\n+  assert(is_subword_type(elem_ty), \"\");\n+  movl(length, vector_len);\n+  vpxor(xtmp1, xtmp1, xtmp1, vlen_enc); \/\/ xtmp1 = {0, ...}\n+  vpxor(dst, dst, dst, vlen_enc); \/\/ dst = {0, ...}\n+  vallones(xtmp2, vlen_enc);\n+  vpsubd(xtmp2, xtmp1, xtmp2, vlen_enc);\n+  vpslld(xtmp2, xtmp2, 1, vlen_enc); \/\/ xtmp2 = {2, 2, ...}\n+  load_iota_indices(xtmp1, vector_len * type2aelembytes(elem_ty), T_INT); \/\/ xtmp1 = {0, 1, 2, ...}\n+\n+  bind(GATHER8_LOOP);\n+    \/\/ TMP_VEC_64(temp_dst) = PICK_SUB_WORDS_FROM_GATHER_INDICES\n+    if (mask == noreg) {\n+      vgather8b_offset(elem_ty, temp_dst, base, idx_base, offset, rtmp, vlen_enc);\n+    } else {\n+      LP64_ONLY(vgather8b_masked_offset(elem_ty, temp_dst, base, idx_base, offset, mask, mask_idx, rtmp, vlen_enc));\n+    }\n+    \/\/ TEMP_PERM_VEC(temp_dst) = PERMUTE TMP_VEC_64(temp_dst) PERM_INDEX(xtmp1)\n+    vpermd(temp_dst, xtmp1, temp_dst, vlen_enc == Assembler::AVX_512bit ? vlen_enc : Assembler::AVX_256bit);\n+    \/\/ PERM_INDEX(xtmp1) = PERM_INDEX(xtmp1) - TWO_VEC(xtmp2)\n+    vpsubd(xtmp1, xtmp1, xtmp2, vlen_enc);\n+    \/\/ DST_VEC = DST_VEC OR TEMP_PERM_VEC\n+    vpor(dst, dst, temp_dst, vlen_enc);\n+    addptr(idx_base,  32 >> (type2aelembytes(elem_ty) - 1));\n+    subl(length, 8 >> (type2aelembytes(elem_ty) - 1));\n+    jcc(Assembler::notEqual, GATHER8_LOOP);\n+}\n+\n","filename":"src\/hotspot\/cpu\/x86\/c2_MacroAssembler_x86.cpp","additions":124,"deletions":0,"binary":false,"changes":124,"status":"modified"},{"patch":"@@ -504,0 +504,12 @@\n+\n+  void vgather_subword(BasicType elem_ty, XMMRegister dst,  Register base, Register idx_base, Register offset,\n+                       Register mask, XMMRegister xtmp1, XMMRegister xtmp2, XMMRegister xtmp3, Register rtmp,\n+                       Register midx, Register length, int vector_len, int vlen_enc);\n+\n+#ifdef _LP64\n+  void vgather8b_masked_offset(BasicType elem_bt, XMMRegister dst, Register base, Register idx_base,\n+                               Register offset, Register mask, Register midx, Register rtmp, int vlen_enc);\n+#endif\n+  void vgather8b_offset(BasicType elem_bt, XMMRegister dst, Register base, Register idx_base,\n+                              Register offset, Register rtmp, int vlen_enc);\n+\n","filename":"src\/hotspot\/cpu\/x86\/c2_MacroAssembler_x86.hpp","additions":12,"deletions":0,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -468,5 +468,0 @@\n-    \/\/ Check if the secondary index definition is still ~x, otherwise\n-    \/\/ we have to change the following assembler code to calculate the\n-    \/\/ plain index.\n-    assert(ConstantPool::decode_invokedynamic_index(~123) == 123, \"else change next line\");\n-    notl(index);  \/\/ convert to plain index\n","filename":"src\/hotspot\/cpu\/x86\/interp_masm_x86.cpp","additions":0,"deletions":5,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -3709,0 +3709,5 @@\n+void MacroAssembler::vpcmpeqb(XMMRegister dst, XMMRegister src1, Address src2, int vector_len) {\n+  assert(((dst->encoding() < 16 && src1->encoding() < 16) || VM_Version::supports_avx512vlbw()),\"XMM register should be 0-15\");\n+  Assembler::vpcmpeqb(dst, src1, src2, vector_len);\n+}\n+\n@@ -3714,0 +3719,5 @@\n+void MacroAssembler::vpcmpeqw(XMMRegister dst, XMMRegister nds, Address src, int vector_len) {\n+  assert(((dst->encoding() < 16 && nds->encoding() < 16) || VM_Version::supports_avx512vlbw()),\"XMM register should be 0-15\");\n+  Assembler::vpcmpeqw(dst, nds, src, vector_len);\n+}\n+\n@@ -4209,1 +4219,1 @@\n-  \/\/ test to see if it has a finalizer or is malformed in some way\n+  \/\/ test to see if it is malformed in some way\n@@ -7063,1 +7073,1 @@\n-  cmpl(count, 2<<shift); \/\/ Short arrays (< 8 bytes) fill by element\n+  cmpptr(count, 2<<shift); \/\/ Short arrays (< 8 bytes) fill by element\n@@ -7083,1 +7093,1 @@\n-    subl(count, 1<<(shift-1));\n+    subptr(count, 1<<(shift-1));\n@@ -7089,1 +7099,1 @@\n-    subl(count, 8 << shift);\n+    subptr(count, 8 << shift);\n@@ -7100,1 +7110,1 @@\n-    subl(count, 8 << shift);\n+    subptr(count, 8 << shift);\n@@ -7103,1 +7113,1 @@\n-    addl(count, 8 << shift);\n+    addptr(count, 8 << shift);\n@@ -7115,1 +7125,1 @@\n-    subl(count, 1 << (shift + 1));\n+    subptr(count, 1 << (shift + 1));\n@@ -7126,1 +7136,1 @@\n-      subl(count, 1<<shift);\n+      subptr(count, 1<<shift);\n@@ -7140,1 +7150,1 @@\n-          cmpl(count, VM_Version::avx3_threshold());\n+          cmpptr(count, VM_Version::avx3_threshold());\n@@ -7145,1 +7155,1 @@\n-          subl(count, 16 << shift);\n+          subptr(count, 16 << shift);\n@@ -7152,1 +7162,1 @@\n-          subl(count, 16 << shift);\n+          subptr(count, 16 << shift);\n@@ -7162,1 +7172,1 @@\n-        subl(count, 16 << shift);\n+        subptr(count, 16 << shift);\n@@ -7170,1 +7180,1 @@\n-        subl(count, 16 << shift);\n+        subptr(count, 16 << shift);\n@@ -7174,1 +7184,1 @@\n-        addl(count, 8 << shift);\n+        addptr(count, 8 << shift);\n@@ -7178,1 +7188,1 @@\n-        subl(count, 8 << shift);\n+        subptr(count, 8 << shift);\n@@ -7188,1 +7198,1 @@\n-        subl(count, 8 << shift);\n+        subptr(count, 8 << shift);\n@@ -7205,1 +7215,1 @@\n-        subl(count, 8 << shift);\n+        subptr(count, 8 << shift);\n@@ -7210,1 +7220,1 @@\n-      addl(count, 8 << shift);\n+      addptr(count, 8 << shift);\n@@ -7221,1 +7231,1 @@\n-      subl(count, 1 << (shift + 1));\n+      subptr(count, 1 << (shift + 1));\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.cpp","additions":29,"deletions":19,"binary":false,"changes":48,"status":"modified"},{"patch":"@@ -1016,0 +1016,68 @@\n+  \/\/ Adding more natural conditional jump instructions\n+  void ALWAYSINLINE jo(Label& L, bool maybe_short = true) { jcc(Assembler::overflow, L, maybe_short); }\n+  void ALWAYSINLINE jno(Label& L, bool maybe_short = true) { jcc(Assembler::noOverflow, L, maybe_short); }\n+  void ALWAYSINLINE js(Label& L, bool maybe_short = true) { jcc(Assembler::negative, L, maybe_short); }\n+  void ALWAYSINLINE jns(Label& L, bool maybe_short = true) { jcc(Assembler::positive, L, maybe_short); }\n+  void ALWAYSINLINE je(Label& L, bool maybe_short = true) { jcc(Assembler::equal, L, maybe_short); }\n+  void ALWAYSINLINE jz(Label& L, bool maybe_short = true) { jcc(Assembler::zero, L, maybe_short); }\n+  void ALWAYSINLINE jne(Label& L, bool maybe_short = true) { jcc(Assembler::notEqual, L, maybe_short); }\n+  void ALWAYSINLINE jnz(Label& L, bool maybe_short = true) { jcc(Assembler::notZero, L, maybe_short); }\n+  void ALWAYSINLINE jb(Label& L, bool maybe_short = true) { jcc(Assembler::below, L, maybe_short); }\n+  void ALWAYSINLINE jnae(Label& L, bool maybe_short = true) { jcc(Assembler::below, L, maybe_short); }\n+  void ALWAYSINLINE jc(Label& L, bool maybe_short = true) { jcc(Assembler::carrySet, L, maybe_short); }\n+  void ALWAYSINLINE jnb(Label& L, bool maybe_short = true) { jcc(Assembler::aboveEqual, L, maybe_short); }\n+  void ALWAYSINLINE jae(Label& L, bool maybe_short = true) { jcc(Assembler::aboveEqual, L, maybe_short); }\n+  void ALWAYSINLINE jnc(Label& L, bool maybe_short = true) { jcc(Assembler::carryClear, L, maybe_short); }\n+  void ALWAYSINLINE jbe(Label& L, bool maybe_short = true) { jcc(Assembler::belowEqual, L, maybe_short); }\n+  void ALWAYSINLINE jna(Label& L, bool maybe_short = true) { jcc(Assembler::belowEqual, L, maybe_short); }\n+  void ALWAYSINLINE ja(Label& L, bool maybe_short = true) { jcc(Assembler::above, L, maybe_short); }\n+  void ALWAYSINLINE jnbe(Label& L, bool maybe_short = true) { jcc(Assembler::above, L, maybe_short); }\n+  void ALWAYSINLINE jl(Label& L, bool maybe_short = true) { jcc(Assembler::less, L, maybe_short); }\n+  void ALWAYSINLINE jnge(Label& L, bool maybe_short = true) { jcc(Assembler::less, L, maybe_short); }\n+  void ALWAYSINLINE jge(Label& L, bool maybe_short = true) { jcc(Assembler::greaterEqual, L, maybe_short); }\n+  void ALWAYSINLINE jnl(Label& L, bool maybe_short = true) { jcc(Assembler::greaterEqual, L, maybe_short); }\n+  void ALWAYSINLINE jle(Label& L, bool maybe_short = true) { jcc(Assembler::lessEqual, L, maybe_short); }\n+  void ALWAYSINLINE jng(Label& L, bool maybe_short = true) { jcc(Assembler::lessEqual, L, maybe_short); }\n+  void ALWAYSINLINE jg(Label& L, bool maybe_short = true) { jcc(Assembler::greater, L, maybe_short); }\n+  void ALWAYSINLINE jnle(Label& L, bool maybe_short = true) { jcc(Assembler::greater, L, maybe_short); }\n+  void ALWAYSINLINE jp(Label& L, bool maybe_short = true) { jcc(Assembler::parity, L, maybe_short); }\n+  void ALWAYSINLINE jpe(Label& L, bool maybe_short = true) { jcc(Assembler::parity, L, maybe_short); }\n+  void ALWAYSINLINE jnp(Label& L, bool maybe_short = true) { jcc(Assembler::noParity, L, maybe_short); }\n+  void ALWAYSINLINE jpo(Label& L, bool maybe_short = true) { jcc(Assembler::noParity, L, maybe_short); }\n+  \/\/ * No condition for this *  void ALWAYSINLINE jcxz(Label& L, bool maybe_short = true) { jcc(Assembler::cxz, L, maybe_short); }\n+  \/\/ * No condition for this *  void ALWAYSINLINE jecxz(Label& L, bool maybe_short = true) { jcc(Assembler::cxz, L, maybe_short); }\n+\n+  \/\/ Short versions of the above\n+  void ALWAYSINLINE jo_b(Label& L) { jccb(Assembler::overflow, L); }\n+  void ALWAYSINLINE jno_b(Label& L) { jccb(Assembler::noOverflow, L); }\n+  void ALWAYSINLINE js_b(Label& L) { jccb(Assembler::negative, L); }\n+  void ALWAYSINLINE jns_b(Label& L) { jccb(Assembler::positive, L); }\n+  void ALWAYSINLINE je_b(Label& L) { jccb(Assembler::equal, L); }\n+  void ALWAYSINLINE jz_b(Label& L) { jccb(Assembler::zero, L); }\n+  void ALWAYSINLINE jne_b(Label& L) { jccb(Assembler::notEqual, L); }\n+  void ALWAYSINLINE jnz_b(Label& L) { jccb(Assembler::notZero, L); }\n+  void ALWAYSINLINE jb_b(Label& L) { jccb(Assembler::below, L); }\n+  void ALWAYSINLINE jnae_b(Label& L) { jccb(Assembler::below, L); }\n+  void ALWAYSINLINE jc_b(Label& L) { jccb(Assembler::carrySet, L); }\n+  void ALWAYSINLINE jnb_b(Label& L) { jccb(Assembler::aboveEqual, L); }\n+  void ALWAYSINLINE jae_b(Label& L) { jccb(Assembler::aboveEqual, L); }\n+  void ALWAYSINLINE jnc_b(Label& L) { jccb(Assembler::carryClear, L); }\n+  void ALWAYSINLINE jbe_b(Label& L) { jccb(Assembler::belowEqual, L); }\n+  void ALWAYSINLINE jna_b(Label& L) { jccb(Assembler::belowEqual, L); }\n+  void ALWAYSINLINE ja_b(Label& L) { jccb(Assembler::above, L); }\n+  void ALWAYSINLINE jnbe_b(Label& L) { jccb(Assembler::above, L); }\n+  void ALWAYSINLINE jl_b(Label& L) { jccb(Assembler::less, L); }\n+  void ALWAYSINLINE jnge_b(Label& L) { jccb(Assembler::less, L); }\n+  void ALWAYSINLINE jge_b(Label& L) { jccb(Assembler::greaterEqual, L); }\n+  void ALWAYSINLINE jnl_b(Label& L) { jccb(Assembler::greaterEqual, L); }\n+  void ALWAYSINLINE jle_b(Label& L) { jccb(Assembler::lessEqual, L); }\n+  void ALWAYSINLINE jng_b(Label& L) { jccb(Assembler::lessEqual, L); }\n+  void ALWAYSINLINE jg_b(Label& L) { jccb(Assembler::greater, L); }\n+  void ALWAYSINLINE jnle_b(Label& L) { jccb(Assembler::greater, L); }\n+  void ALWAYSINLINE jp_b(Label& L) { jccb(Assembler::parity, L); }\n+  void ALWAYSINLINE jpe_b(Label& L) { jccb(Assembler::parity, L); }\n+  void ALWAYSINLINE jnp_b(Label& L) { jccb(Assembler::noParity, L); }\n+  void ALWAYSINLINE jpo_b(Label& L) { jccb(Assembler::noParity, L); }\n+  \/\/ * No condition for this *  void ALWAYSINLINE jcxz_b(Label& L) { jccb(Assembler::cxz, L); }\n+  \/\/ * No condition for this *  void ALWAYSINLINE jecxz_b(Label& L) { jccb(Assembler::cxz, L); }\n+\n@@ -1450,0 +1518,1 @@\n+  void vpcmpeqb(XMMRegister dst, XMMRegister src1, Address src2, int vector_len);\n@@ -1451,0 +1520,1 @@\n+  void vpcmpeqw(XMMRegister dst, XMMRegister nds, Address src, int vector_len);\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.hpp","additions":70,"deletions":0,"binary":false,"changes":70,"status":"modified"},{"patch":"@@ -4076,2 +4076,2 @@\n-  if (UnsafeCopyMemory::_table == nullptr) {\n-    UnsafeCopyMemory::create_table(16);\n+  if (UnsafeMemoryAccess::_table == nullptr) {\n+    UnsafeMemoryAccess::create_table(16 + 4); \/\/ 16 for copyMemory; 4 for setMemory\n","filename":"src\/hotspot\/cpu\/x86\/stubGenerator_x86_64.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -271,0 +271,8 @@\n+  \/\/ Generate 'unsafe' set memory stub\n+  \/\/ Though just as safe as the other stubs, it takes an unscaled\n+  \/\/ size_t argument instead of an element count.\n+  \/\/\n+  \/\/ Examines the alignment of the operands and dispatches\n+  \/\/ to an int, short, or byte copy loop.\n+  address generate_unsafe_setmemory(const char *name, address byte_copy_entry);\n+\n","filename":"src\/hotspot\/cpu\/x86\/stubGenerator_x86_64.hpp","additions":8,"deletions":0,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2003, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2003, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -155,0 +155,2 @@\n+  StubRoutines::_unsafe_setmemory = generate_unsafe_setmemory(\"unsafe_setmemory\", StubRoutines::_jbyte_fill);\n+\n@@ -561,2 +563,2 @@\n-    \/\/ UnsafeCopyMemory page error: continue after ucm\n-    UnsafeCopyMemoryMark ucmm(this, !is_oop && !aligned, true);\n+    \/\/ UnsafeMemoryAccess page error: continue after unsafe access\n+    UnsafeMemoryAccessMark umam(this, !is_oop && !aligned, true);\n@@ -731,1 +733,1 @@\n-      UnsafeCopyMemoryMark ucmm(this, !is_oop && !aligned, false, ucme_exit_pc);\n+      UnsafeMemoryAccessMark umam(this, !is_oop && !aligned, false, ucme_exit_pc);\n@@ -860,2 +862,2 @@\n-    \/\/ UnsafeCopyMemory page error: continue after ucm\n-    UnsafeCopyMemoryMark ucmm(this, !is_oop && !aligned, true);\n+    \/\/ UnsafeMemoryAccess page error: continue after unsafe access\n+    UnsafeMemoryAccessMark umam(this, !is_oop && !aligned, true);\n@@ -1319,2 +1321,2 @@\n-    \/\/ UnsafeCopyMemory page error: continue after ucm\n-    UnsafeCopyMemoryMark ucmm(this, !aligned, true);\n+    \/\/ UnsafeMemoryAccess page error: continue after unsafe access\n+    UnsafeMemoryAccessMark umam(this, !aligned, true);\n@@ -1375,1 +1377,1 @@\n-    UnsafeCopyMemoryMark ucmm(this, !aligned, false, ucme_exit_pc);\n+    UnsafeMemoryAccessMark umam(this, !aligned, false, ucme_exit_pc);\n@@ -1433,2 +1435,2 @@\n-    \/\/ UnsafeCopyMemory page error: continue after ucm\n-    UnsafeCopyMemoryMark ucmm(this, !aligned, true);\n+    \/\/ UnsafeMemoryAccess page error: continue after unsafe access\n+    UnsafeMemoryAccessMark umam(this, !aligned, true);\n@@ -1478,2 +1480,2 @@\n-    \/\/ UnsafeCopyMemory page error: continue after ucm\n-    UnsafeCopyMemoryMark ucmm(this, !aligned, true);\n+    \/\/ UnsafeMemoryAccess page error: continue after unsafe access\n+    UnsafeMemoryAccessMark umam(this, !aligned, true);\n@@ -1550,2 +1552,2 @@\n-    \/\/ UnsafeCopyMemory page error: continue after ucm\n-    UnsafeCopyMemoryMark ucmm(this, !aligned, true);\n+    \/\/ UnsafeMemoryAccess page error: continue after unsafe access\n+    UnsafeMemoryAccessMark umam(this, !aligned, true);\n@@ -1599,1 +1601,1 @@\n-    UnsafeCopyMemoryMark ucmm(this, !aligned, false, ucme_exit_pc);\n+    UnsafeMemoryAccessMark umam(this, !aligned, false, ucme_exit_pc);\n@@ -1623,1 +1625,5 @@\n-  __ generate_fill(t, aligned, to, value, r11, rax, xmm0);\n+  {\n+    \/\/ Add set memory mark to protect against unsafe accesses faulting\n+    UnsafeMemoryAccessMark umam(this, ((t == T_BYTE) && !aligned), true);\n+    __ generate_fill(t, aligned, to, value, r11, rax, xmm0);\n+  }\n@@ -1682,2 +1688,2 @@\n-    \/\/ UnsafeCopyMemory page error: continue after ucm\n-    UnsafeCopyMemoryMark ucmm(this, !aligned, true);\n+    \/\/ UnsafeMemoryAccess page error: continue after unsafe access\n+    UnsafeMemoryAccessMark umam(this, !aligned, true);\n@@ -1719,2 +1725,2 @@\n-    \/\/ UnsafeCopyMemory page error: continue after ucm\n-    UnsafeCopyMemoryMark ucmm(this, !aligned, true);\n+    \/\/ UnsafeMemoryAccess page error: continue after unsafe access\n+    UnsafeMemoryAccessMark umam(this, !aligned, true);\n@@ -1803,2 +1809,2 @@\n-    \/\/ UnsafeCopyMemory page error: continue after ucm\n-    UnsafeCopyMemoryMark ucmm(this, !is_oop && !aligned, true);\n+    \/\/ UnsafeMemoryAccess page error: continue after unsafe access\n+    UnsafeMemoryAccessMark umam(this, !is_oop && !aligned, true);\n@@ -1840,1 +1846,1 @@\n-    UnsafeCopyMemoryMark ucmm(this, !is_oop && !aligned, false, ucme_exit_pc);\n+    UnsafeMemoryAccessMark umam(this, !is_oop && !aligned, false, ucme_exit_pc);\n@@ -1913,2 +1919,2 @@\n-    \/\/ UnsafeCopyMemory page error: continue after ucm\n-    UnsafeCopyMemoryMark ucmm(this, !is_oop && !aligned, true);\n+    \/\/ UnsafeMemoryAccess page error: continue after unsafe access\n+    UnsafeMemoryAccessMark umam(this, !is_oop && !aligned, true);\n@@ -1946,2 +1952,2 @@\n-    \/\/ UnsafeCopyMemory page error: continue after ucm\n-    UnsafeCopyMemoryMark ucmm(this, !is_oop && !aligned, true);\n+    \/\/ UnsafeMemoryAccess page error: continue after unsafe access\n+    UnsafeMemoryAccessMark umam(this, !is_oop && !aligned, true);\n@@ -2028,2 +2034,2 @@\n-    \/\/ UnsafeCopyMemory page error: continue after ucm\n-    UnsafeCopyMemoryMark ucmm(this, !is_oop && !aligned, true);\n+    \/\/ UnsafeMemoryAccess page error: continue after unsafe access\n+    UnsafeMemoryAccessMark umam(this, !is_oop && !aligned, true);\n@@ -2060,2 +2066,2 @@\n-    \/\/ UnsafeCopyMemory page error: continue after ucm\n-    UnsafeCopyMemoryMark ucmm(this, !is_oop && !aligned, true);\n+    \/\/ UnsafeMemoryAccess page error: continue after unsafe access\n+    UnsafeMemoryAccessMark umam(this, !is_oop && !aligned, true);\n@@ -2137,2 +2143,2 @@\n-    \/\/ UnsafeCopyMemory page error: continue after ucm\n-    UnsafeCopyMemoryMark ucmm(this, !is_oop && !aligned, true);\n+    \/\/ UnsafeMemoryAccess page error: continue after unsafe access\n+    UnsafeMemoryAccessMark umam(this, !is_oop && !aligned, true);\n@@ -2164,2 +2170,2 @@\n-    \/\/ UnsafeCopyMemory page error: continue after ucm\n-    UnsafeCopyMemoryMark ucmm(this, !is_oop && !aligned, true);\n+    \/\/ UnsafeMemoryAccess page error: continue after unsafe access\n+    UnsafeMemoryAccessMark umam(this, !is_oop && !aligned, true);\n@@ -2480,0 +2486,196 @@\n+\/\/ Static enum for helper\n+enum USM_TYPE {USM_SHORT, USM_DWORD, USM_QUADWORD};\n+\/\/ Helper for generate_unsafe_setmemory\n+\/\/\n+\/\/ Atomically fill an array of memory using 2-, 4-, or 8-byte chunks\n+static void do_setmemory_atomic_loop(USM_TYPE type, Register dest,\n+                                     Register size, Register wide_value,\n+                                     Register tmp, Label& L_exit,\n+                                     MacroAssembler *_masm) {\n+  Label L_Loop, L_Tail, L_TailLoop;\n+\n+  int shiftval = 0;\n+  int incr = 0;\n+\n+  switch (type) {\n+    case USM_SHORT:\n+      shiftval = 1;\n+      incr = 16;\n+      break;\n+    case USM_DWORD:\n+      shiftval = 2;\n+      incr = 32;\n+      break;\n+    case USM_QUADWORD:\n+      shiftval = 3;\n+      incr = 64;\n+      break;\n+  }\n+\n+  \/\/ At this point, we know the lower bits of size are zero\n+  __ shrq(size, shiftval);\n+  \/\/ size now has number of X-byte chunks (2, 4 or 8)\n+\n+  \/\/ Number of (8*X)-byte chunks into tmp\n+  __ movq(tmp, size);\n+  __ shrq(tmp, 3);\n+  __ jccb(Assembler::zero, L_Tail);\n+\n+  __ BIND(L_Loop);\n+\n+  \/\/ Unroll 8 stores\n+  for (int i = 0; i < 8; i++) {\n+    switch (type) {\n+      case USM_SHORT:\n+        __ movw(Address(dest, (2 * i)), wide_value);\n+        break;\n+      case USM_DWORD:\n+        __ movl(Address(dest, (4 * i)), wide_value);\n+        break;\n+      case USM_QUADWORD:\n+        __ movq(Address(dest, (8 * i)), wide_value);\n+        break;\n+    }\n+  }\n+  __ addq(dest, incr);\n+  __ decrementq(tmp);\n+  __ jccb(Assembler::notZero, L_Loop);\n+\n+  __ BIND(L_Tail);\n+\n+  \/\/ Find number of remaining X-byte chunks\n+  __ andq(size, 0x7);\n+\n+  \/\/ If zero, then we're done\n+  __ jccb(Assembler::zero, L_exit);\n+\n+  __ BIND(L_TailLoop);\n+\n+    switch (type) {\n+      case USM_SHORT:\n+        __ movw(Address(dest, 0), wide_value);\n+        break;\n+      case USM_DWORD:\n+        __ movl(Address(dest, 0), wide_value);\n+        break;\n+      case USM_QUADWORD:\n+        __ movq(Address(dest, 0), wide_value);\n+        break;\n+    }\n+  __ addq(dest, incr >> 3);\n+  __ decrementq(size);\n+  __ jccb(Assembler::notZero, L_TailLoop);\n+}\n+\n+\/\/  Generate 'unsafe' set memory stub\n+\/\/  Though just as safe as the other stubs, it takes an unscaled\n+\/\/  size_t (# bytes) argument instead of an element count.\n+\/\/\n+\/\/  Input:\n+\/\/    c_rarg0   - destination array address\n+\/\/    c_rarg1   - byte count (size_t)\n+\/\/    c_rarg2   - byte value\n+\/\/\n+\/\/ Examines the alignment of the operands and dispatches\n+\/\/ to an int, short, or byte fill loop.\n+\/\/\n+address StubGenerator::generate_unsafe_setmemory(const char *name,\n+                                                 address unsafe_byte_fill) {\n+  __ align(CodeEntryAlignment);\n+  StubCodeMark mark(this, \"StubRoutines\", name);\n+  address start = __ pc();\n+  __ enter();   \/\/ required for proper stackwalking of RuntimeStub frame\n+\n+  assert(unsafe_byte_fill != nullptr, \"Invalid call\");\n+\n+  \/\/ bump this on entry, not on exit:\n+  INC_COUNTER_NP(SharedRuntime::_unsafe_set_memory_ctr, rscratch1);\n+\n+  {\n+    Label L_exit, L_fillQuadwords, L_fillDwords, L_fillBytes;\n+\n+    const Register dest = c_rarg0;\n+    const Register size = c_rarg1;\n+    const Register byteVal = c_rarg2;\n+    const Register wide_value = rax;\n+    const Register rScratch1 = r10;\n+\n+    assert_different_registers(dest, size, byteVal, wide_value, rScratch1);\n+\n+    \/\/     fill_to_memory_atomic(unsigned char*, unsigned long, unsigned char)\n+\n+    __ testq(size, size);\n+    __ jcc(Assembler::zero, L_exit);\n+\n+    \/\/ Propagate byte to full Register\n+    __ movzbl(rScratch1, byteVal);\n+    __ mov64(wide_value, 0x0101010101010101ULL);\n+    __ imulq(wide_value, rScratch1);\n+\n+    \/\/ Check for pointer & size alignment\n+    __ movq(rScratch1, dest);\n+    __ orq(rScratch1, size);\n+\n+    __ testb(rScratch1, 7);\n+    __ jcc(Assembler::equal, L_fillQuadwords);\n+\n+    __ testb(rScratch1, 3);\n+    __ jcc(Assembler::equal, L_fillDwords);\n+\n+    __ testb(rScratch1, 1);\n+    __ jcc(Assembler::notEqual, L_fillBytes);\n+\n+    \/\/ Fill words\n+    {\n+      Label L_wordsTail, L_wordsLoop, L_wordsTailLoop;\n+      UnsafeMemoryAccessMark umam(this, true, true);\n+\n+      \/\/ At this point, we know the lower bit of size is zero and a\n+      \/\/ multiple of 2\n+      do_setmemory_atomic_loop(USM_SHORT, dest, size, wide_value, rScratch1,\n+                               L_exit, _masm);\n+    }\n+    __ jmpb(L_exit);\n+\n+    __ BIND(L_fillQuadwords);\n+\n+    \/\/ Fill QUADWORDs\n+    {\n+      Label L_qwordLoop, L_qwordsTail, L_qwordsTailLoop;\n+      UnsafeMemoryAccessMark umam(this, true, true);\n+\n+      \/\/ At this point, we know the lower 3 bits of size are zero and a\n+      \/\/ multiple of 8\n+      do_setmemory_atomic_loop(USM_QUADWORD, dest, size, wide_value, rScratch1,\n+                               L_exit, _masm);\n+    }\n+    __ BIND(L_exit);\n+\n+    __ leave();   \/\/ required for proper stackwalking of RuntimeStub frame\n+    __ ret(0);\n+\n+    __ BIND(L_fillDwords);\n+\n+    \/\/ Fill DWORDs\n+    {\n+      Label L_dwordLoop, L_dwordsTail, L_dwordsTailLoop;\n+      UnsafeMemoryAccessMark umam(this, true, true);\n+\n+      \/\/ At this point, we know the lower 2 bits of size are zero and a\n+      \/\/ multiple of 4\n+      do_setmemory_atomic_loop(USM_DWORD, dest, size, wide_value, rScratch1,\n+                               L_exit, _masm);\n+    }\n+    __ jmpb(L_exit);\n+\n+    __ BIND(L_fillBytes);\n+    \/\/ Set up for tail call to previously generated byte fill routine\n+    \/\/ Parameter order is (ptr, byteVal, size)\n+    __ xchgq(c_rarg1, c_rarg2);\n+    __ leave();    \/\/ Clear effect of enter()\n+    __ jump(RuntimeAddress(unsafe_byte_fill));\n+  }\n+\n+  return start;\n+}\n+\n","filename":"src\/hotspot\/cpu\/x86\/stubGenerator_x86_64_arraycopy.cpp","additions":238,"deletions":36,"binary":false,"changes":274,"status":"modified"},{"patch":"@@ -1572,0 +1572,1 @@\n+    case Op_LoadVectorGatherMasked:\n@@ -1909,0 +1910,11 @@\n+      if (!is_subword_type(bt) && size_in_bits < 512 && !VM_Version::supports_avx512vl()) {\n+        return false;\n+      }\n+      if (is_subword_type(bt) &&\n+         (!is_LP64                                                ||\n+         (size_in_bits > 256 && !VM_Version::supports_avx512bw()) ||\n+         (size_in_bits < 64)                                      ||\n+         (bt == T_SHORT && !VM_Version::supports_bmi2()))) {\n+        return false;\n+      }\n+      break;\n@@ -1918,1 +1930,4 @@\n-      if (size_in_bits == 64 ) {\n+      if (!is_subword_type(bt) && size_in_bits == 64) {\n+        return false;\n+      }\n+      if (is_subword_type(bt) && size_in_bits < 64) {\n@@ -4060,1 +4075,1 @@\n-\/\/ Gather INT, LONG, FLOAT, DOUBLE\n+\/\/ Gather BYTE, SHORT, INT, LONG, FLOAT, DOUBLE\n@@ -4063,1 +4078,2 @@\n-  predicate(!VM_Version::supports_avx512vl() && Matcher::vector_length_in_bytes(n) <= 32);\n+  predicate(!VM_Version::supports_avx512vl() && !is_subword_type(Matcher::vector_element_basic_type(n)) &&\n+            Matcher::vector_length_in_bytes(n) <= 32);\n@@ -4080,1 +4096,2 @@\n-  predicate(VM_Version::supports_avx512vl() || Matcher::vector_length_in_bytes(n) == 64);\n+  predicate((VM_Version::supports_avx512vl() || Matcher::vector_length_in_bytes(n) == 64) &&\n+            !is_subword_type(Matcher::vector_element_basic_type(n)));\n@@ -4095,1 +4112,2 @@\n-  predicate(VM_Version::supports_avx512vl() || Matcher::vector_length_in_bytes(n) == 64);\n+  predicate((VM_Version::supports_avx512vl() || Matcher::vector_length_in_bytes(n) == 64) &&\n+            !is_subword_type(Matcher::vector_element_basic_type(n)));\n@@ -4113,0 +4131,232 @@\n+\n+instruct vgather_subwordLE8B(vec dst, memory mem, rRegP idx_base, immI_0 offset, rRegP tmp, rRegI rtmp) %{\n+  predicate(is_subword_type(Matcher::vector_element_basic_type(n)) && Matcher::vector_length_in_bytes(n) <= 8);\n+  match(Set dst (LoadVectorGather mem (Binary idx_base offset)));\n+  effect(TEMP tmp, TEMP rtmp);\n+  format %{ \"vector_gatherLE8 $dst, $mem, $idx_base\\t! using $tmp and $rtmp as TEMP\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    BasicType elem_bt = Matcher::vector_element_basic_type(this);\n+    __ lea($tmp$$Register, $mem$$Address);\n+    __ vgather8b_offset(elem_bt, $dst$$XMMRegister, $tmp$$Register, $idx_base$$Register, noreg, $rtmp$$Register, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vgather_subwordGT8B(vec dst, memory mem, rRegP idx_base, immI_0 offset, rRegP tmp, rRegP idx_base_temp,\n+                             vec xtmp1, vec xtmp2, vec xtmp3, rRegI rtmp, rRegI length, rFlagsReg cr) %{\n+  predicate(is_subword_type(Matcher::vector_element_basic_type(n)) && Matcher::vector_length_in_bytes(n) > 8);\n+  match(Set dst (LoadVectorGather mem (Binary idx_base offset)));\n+  effect(TEMP_DEF dst, TEMP tmp, TEMP idx_base_temp, TEMP xtmp1, TEMP xtmp2, TEMP xtmp3, TEMP rtmp, TEMP length, KILL cr);\n+  format %{ \"vector_gatherGT8 $dst, $mem, $idx_base\\t! using $tmp, $idx_base_temp, $xtmp1, $xtmp2, $xtmp3, $rtmp and $length as TEMP\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    int vector_len = Matcher::vector_length(this);\n+    BasicType elem_bt = Matcher::vector_element_basic_type(this);\n+    __ lea($tmp$$Register, $mem$$Address);\n+    __ movptr($idx_base_temp$$Register, $idx_base$$Register);\n+    __ vgather_subword(elem_bt, $dst$$XMMRegister, $tmp$$Register, $idx_base_temp$$Register, noreg, noreg, $xtmp1$$XMMRegister,\n+                       $xtmp2$$XMMRegister, $xtmp3$$XMMRegister, $rtmp$$Register, noreg, $length$$Register, vector_len, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vgather_subwordLE8B_off(vec dst, memory mem, rRegP idx_base, rRegI offset, rRegP tmp, rRegI rtmp, rFlagsReg cr) %{\n+  predicate(is_subword_type(Matcher::vector_element_basic_type(n)) && Matcher::vector_length_in_bytes(n) <= 8);\n+  match(Set dst (LoadVectorGather mem (Binary idx_base offset)));\n+  effect(TEMP tmp, TEMP rtmp, KILL cr);\n+  format %{ \"vector_gatherLE8_off $dst, $mem, $idx_base, $offset\\t! using $tmp and $rtmp as TEMP\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    BasicType elem_bt = Matcher::vector_element_basic_type(this);\n+    __ lea($tmp$$Register, $mem$$Address);\n+    __ vgather8b_offset(elem_bt, $dst$$XMMRegister, $tmp$$Register, $idx_base$$Register, $offset$$Register, $rtmp$$Register, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+\n+instruct vgather_subwordGT8B_off(vec dst, memory mem, rRegP idx_base, rRegI offset, rRegP tmp, rRegP idx_base_temp,\n+                                 vec xtmp1, vec xtmp2, vec xtmp3, rRegI rtmp, rRegI length, rFlagsReg cr) %{\n+  predicate(is_subword_type(Matcher::vector_element_basic_type(n)) && Matcher::vector_length_in_bytes(n) > 8);\n+  match(Set dst (LoadVectorGather mem (Binary idx_base offset)));\n+  effect(TEMP_DEF dst, TEMP tmp, TEMP idx_base_temp, TEMP xtmp1, TEMP xtmp2, TEMP xtmp3, TEMP rtmp, TEMP length, KILL cr);\n+  format %{ \"vector_gatherGT8_off $dst, $mem, $idx_base, $offset\\t! using $tmp, $idx_base_temp, $xtmp1, $xtmp2, $xtmp3, $rtmp and $length as TEMP\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    int vector_len = Matcher::vector_length(this);\n+    BasicType elem_bt = Matcher::vector_element_basic_type(this);\n+    __ lea($tmp$$Register, $mem$$Address);\n+    __ movptr($idx_base_temp$$Register, $idx_base$$Register);\n+    __ vgather_subword(elem_bt, $dst$$XMMRegister, $tmp$$Register, $idx_base_temp$$Register, $offset$$Register, noreg, $xtmp1$$XMMRegister,\n+                       $xtmp2$$XMMRegister, $xtmp3$$XMMRegister, $rtmp$$Register, noreg, $length$$Register, vector_len, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+\n+#ifdef _LP64\n+instruct vgather_masked_subwordLE8B_avx3(vec dst, memory mem, rRegP idx_base, immI_0 offset, kReg mask, rRegL mask_idx, rRegP tmp, rRegI rtmp, rRegL rtmp2, rFlagsReg cr) %{\n+  predicate(VM_Version::supports_avx512bw() && is_subword_type(Matcher::vector_element_basic_type(n)) && Matcher::vector_length_in_bytes(n) <= 8);\n+  match(Set dst (LoadVectorGatherMasked mem (Binary idx_base (Binary mask offset))));\n+  effect(TEMP mask_idx, TEMP tmp, TEMP rtmp, TEMP rtmp2, KILL cr);\n+  format %{ \"vector_masked_gatherLE8 $dst, $mem, $idx_base, $mask\\t! using $mask_idx, $tmp, $rtmp and $rtmp2 as TEMP\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    BasicType elem_bt = Matcher::vector_element_basic_type(this);\n+    __ xorq($mask_idx$$Register, $mask_idx$$Register);\n+    __ lea($tmp$$Register, $mem$$Address);\n+    __ kmovql($rtmp2$$Register, $mask$$KRegister);\n+    __ vgather8b_masked_offset(elem_bt, $dst$$XMMRegister, $tmp$$Register, $idx_base$$Register, noreg, $rtmp2$$Register, $mask_idx$$Register, $rtmp$$Register, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vgather_masked_subwordGT8B_avx3(vec dst, memory mem, rRegP idx_base, immI_0 offset, kReg mask, rRegP tmp, rRegP idx_base_temp,\n+                                         vec xtmp1, vec xtmp2, vec xtmp3, rRegI rtmp, rRegL rtmp2, rRegL mask_idx, rRegI length, rFlagsReg cr) %{\n+  predicate(VM_Version::supports_avx512bw() && is_subword_type(Matcher::vector_element_basic_type(n)) && Matcher::vector_length_in_bytes(n) > 8);\n+  match(Set dst (LoadVectorGatherMasked mem (Binary idx_base (Binary mask offset))));\n+  effect(TEMP_DEF dst, TEMP tmp, TEMP idx_base_temp, TEMP xtmp1, TEMP xtmp2, TEMP xtmp3, TEMP rtmp, TEMP rtmp2, TEMP mask_idx, TEMP length, KILL cr);\n+  format %{ \"vector_gatherGT8_masked $dst, $mem, $idx_base, $mask\\t! using $tmp, $idx_base_temp, $xtmp1, $xtmp2, $xtmp3, $rtmp, $rtmp2, $mask_idx and $length as TEMP\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    int vector_len = Matcher::vector_length(this);\n+    BasicType elem_bt = Matcher::vector_element_basic_type(this);\n+    __ xorq($mask_idx$$Register, $mask_idx$$Register);\n+    __ lea($tmp$$Register, $mem$$Address);\n+    __ movptr($idx_base_temp$$Register, $idx_base$$Register);\n+    __ kmovql($rtmp2$$Register, $mask$$KRegister);\n+    __ vgather_subword(elem_bt, $dst$$XMMRegister, $tmp$$Register, $idx_base_temp$$Register, noreg, $rtmp2$$Register, $xtmp1$$XMMRegister,\n+                       $xtmp2$$XMMRegister, $xtmp3$$XMMRegister, $rtmp$$Register, $mask_idx$$Register, $length$$Register, vector_len, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vgather_masked_subwordLE8B_off_avx3(vec dst, memory mem, rRegP idx_base, rRegI offset, kReg mask, rRegL mask_idx, rRegP tmp, rRegI rtmp, rRegL rtmp2, rFlagsReg cr) %{\n+  predicate(VM_Version::supports_avx512bw() && is_subword_type(Matcher::vector_element_basic_type(n)) && Matcher::vector_length_in_bytes(n) <= 8);\n+  match(Set dst (LoadVectorGatherMasked mem (Binary idx_base (Binary mask offset))));\n+  effect(TEMP mask_idx, TEMP tmp, TEMP rtmp, TEMP rtmp2, KILL cr);\n+  format %{ \"vector_masked_gatherLE8_off $dst, $mem, $idx_base, $offset, $mask\\t! using $mask_idx, $tmp, $rtmp and $rtmp2 as TEMP\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    BasicType elem_bt = Matcher::vector_element_basic_type(this);\n+    __ xorq($mask_idx$$Register, $mask_idx$$Register);\n+    __ lea($tmp$$Register, $mem$$Address);\n+    __ kmovql($rtmp2$$Register, $mask$$KRegister);\n+    __ vgather8b_masked_offset(elem_bt, $dst$$XMMRegister, $tmp$$Register, $idx_base$$Register, $offset$$Register,\n+                                $rtmp2$$Register, $mask_idx$$Register, $rtmp$$Register, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vgather_masked_subwordGT8B_off_avx3(vec dst, memory mem, rRegP idx_base, rRegI offset, kReg mask, rRegP tmp, rRegP idx_base_temp,\n+                                             vec xtmp1, vec xtmp2, vec xtmp3, rRegI rtmp, rRegL rtmp2, rRegL mask_idx, rRegI length, rFlagsReg cr) %{\n+  predicate(VM_Version::supports_avx512bw() && is_subword_type(Matcher::vector_element_basic_type(n)) && Matcher::vector_length_in_bytes(n) > 8);\n+  match(Set dst (LoadVectorGatherMasked mem (Binary idx_base (Binary mask offset))));\n+  effect(TEMP_DEF dst, TEMP tmp, TEMP idx_base_temp, TEMP xtmp1, TEMP xtmp2, TEMP xtmp3, TEMP rtmp, TEMP rtmp2, TEMP mask_idx, TEMP length, KILL cr);\n+  format %{ \"vector_gatherGT8_masked_off $dst, $mem, $idx_base, $offset, $mask\\t! using $tmp, $idx_base_temp, $xtmp1, $xtmp2, $xtmp3, $rtmp, $rtmp2, $mask_idx and $length as TEMP\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    int vector_len = Matcher::vector_length(this);\n+    BasicType elem_bt = Matcher::vector_element_basic_type(this);\n+    __ xorq($mask_idx$$Register, $mask_idx$$Register);\n+    __ lea($tmp$$Register, $mem$$Address);\n+    __ movptr($idx_base_temp$$Register, $idx_base$$Register);\n+    __ kmovql($rtmp2$$Register, $mask$$KRegister);\n+    __ vgather_subword(elem_bt, $dst$$XMMRegister, $tmp$$Register, $idx_base_temp$$Register, $offset$$Register, $rtmp2$$Register, $xtmp1$$XMMRegister,\n+                       $xtmp2$$XMMRegister, $xtmp3$$XMMRegister, $rtmp$$Register, $mask_idx$$Register, $length$$Register, vector_len, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vgather_masked_subwordLE8B_avx2(vec dst, memory mem, rRegP idx_base, immI_0 offset, vec mask, rRegI mask_idx, rRegP tmp, rRegI rtmp, rRegI rtmp2, rFlagsReg cr) %{\n+  predicate(!VM_Version::supports_avx512vlbw() && is_subword_type(Matcher::vector_element_basic_type(n)) && Matcher::vector_length_in_bytes(n) <= 8);\n+  match(Set dst (LoadVectorGatherMasked mem (Binary idx_base (Binary mask offset))));\n+  effect(TEMP mask_idx, TEMP tmp, TEMP rtmp, TEMP rtmp2, KILL cr);\n+  format %{ \"vector_masked_gatherLE8 $dst, $mem, $idx_base, $mask\\t! using $mask_idx, $tmp, $rtmp and $rtmp2 as TEMP\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    BasicType elem_bt = Matcher::vector_element_basic_type(this);\n+    __ lea($tmp$$Register, $mem$$Address);\n+    __ vpmovmskb($rtmp2$$Register, $mask$$XMMRegister, vlen_enc);\n+    if (elem_bt == T_SHORT) {\n+      __ movl($mask_idx$$Register, 0x55555555);\n+      __ pextl($rtmp2$$Register, $rtmp2$$Register, $mask_idx$$Register);\n+    }\n+    __ xorl($mask_idx$$Register, $mask_idx$$Register);\n+    __ vgather8b_masked_offset(elem_bt, $dst$$XMMRegister, $tmp$$Register, $idx_base$$Register, noreg, $rtmp2$$Register, $mask_idx$$Register, $rtmp$$Register, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vgather_masked_subwordGT8B_avx2(vec dst, memory mem, rRegP idx_base, immI_0 offset, vec mask, rRegP tmp, rRegP idx_base_temp,\n+                                         vec xtmp1, vec xtmp2, vec xtmp3, rRegI rtmp, rRegI rtmp2, rRegI mask_idx, rRegI length, rFlagsReg cr) %{\n+  predicate(!VM_Version::supports_avx512vlbw() && is_subword_type(Matcher::vector_element_basic_type(n)) && Matcher::vector_length_in_bytes(n) > 8);\n+  match(Set dst (LoadVectorGatherMasked mem (Binary idx_base (Binary mask offset))));\n+  effect(TEMP_DEF dst, TEMP tmp, TEMP idx_base_temp, TEMP xtmp1, TEMP xtmp2, TEMP xtmp3, TEMP rtmp, TEMP rtmp2, TEMP mask_idx, TEMP length, KILL cr);\n+  format %{ \"vector_gatherGT8_masked $dst, $mem, $idx_base, $mask\\t! using $tmp, $idx_base_temp, $xtmp1, $xtmp2, $xtmp3, $rtmp, $rtmp2, $mask_idx and $length as TEMP\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    int vector_len = Matcher::vector_length(this);\n+    BasicType elem_bt = Matcher::vector_element_basic_type(this);\n+    __ lea($tmp$$Register, $mem$$Address);\n+    __ movptr($idx_base_temp$$Register, $idx_base$$Register);\n+    __ vpmovmskb($rtmp2$$Register, $mask$$XMMRegister, vlen_enc);\n+    if (elem_bt == T_SHORT) {\n+      __ movl($mask_idx$$Register, 0x55555555);\n+      __ pextl($rtmp2$$Register, $rtmp2$$Register, $mask_idx$$Register);\n+    }\n+    __ xorl($mask_idx$$Register, $mask_idx$$Register);\n+    __ vgather_subword(elem_bt, $dst$$XMMRegister, $tmp$$Register, $idx_base_temp$$Register, noreg, $rtmp2$$Register, $xtmp1$$XMMRegister,\n+                       $xtmp2$$XMMRegister, $xtmp3$$XMMRegister, $rtmp$$Register, $mask_idx$$Register, $length$$Register, vector_len, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vgather_masked_subwordLE8B_off_avx2(vec dst, memory mem, rRegP idx_base, rRegI offset, vec mask, rRegI mask_idx, rRegP tmp, rRegI rtmp, rRegI rtmp2, rFlagsReg cr) %{\n+  predicate(!VM_Version::supports_avx512vlbw() && is_subword_type(Matcher::vector_element_basic_type(n)) && Matcher::vector_length_in_bytes(n) <= 8);\n+  match(Set dst (LoadVectorGatherMasked mem (Binary idx_base (Binary mask offset))));\n+  effect(TEMP mask_idx, TEMP tmp, TEMP rtmp, TEMP rtmp2, KILL cr);\n+  format %{ \"vector_masked_gatherLE8_off $dst, $mem, $idx_base, $offset, $mask\\t! using $mask_idx, $tmp, $rtmp and $rtmp2 as TEMP\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    BasicType elem_bt = Matcher::vector_element_basic_type(this);\n+    __ lea($tmp$$Register, $mem$$Address);\n+    __ vpmovmskb($rtmp2$$Register, $mask$$XMMRegister, vlen_enc);\n+    if (elem_bt == T_SHORT) {\n+      __ movl($mask_idx$$Register, 0x55555555);\n+      __ pextl($rtmp2$$Register, $rtmp2$$Register, $mask_idx$$Register);\n+    }\n+    __ xorl($mask_idx$$Register, $mask_idx$$Register);\n+    __ vgather8b_masked_offset(elem_bt, $dst$$XMMRegister, $tmp$$Register, $idx_base$$Register, $offset$$Register,\n+                                $rtmp2$$Register, $mask_idx$$Register, $rtmp$$Register, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vgather_masked_subwordGT8B_off_avx2(vec dst, memory mem, rRegP idx_base, rRegI offset, vec mask, rRegP tmp, rRegP idx_base_temp,\n+                                             vec xtmp1, vec xtmp2, vec xtmp3, rRegI rtmp, rRegI rtmp2, rRegI mask_idx, rRegI length, rFlagsReg cr) %{\n+  predicate(!VM_Version::supports_avx512vlbw() && is_subword_type(Matcher::vector_element_basic_type(n)) && Matcher::vector_length_in_bytes(n) > 8);\n+  match(Set dst (LoadVectorGatherMasked mem (Binary idx_base (Binary mask offset))));\n+  effect(TEMP_DEF dst, TEMP tmp, TEMP idx_base_temp, TEMP xtmp1, TEMP xtmp2, TEMP xtmp3, TEMP rtmp, TEMP rtmp2, TEMP mask_idx, TEMP length, KILL cr);\n+  format %{ \"vector_gatherGT8_masked_off $dst, $mem, $idx_base, $offset, $mask\\t! using $tmp, $idx_base_temp, $xtmp1, $xtmp2, $xtmp3, $rtmp, $rtmp2, $mask_idx and $length as TEMP\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    int vector_len = Matcher::vector_length(this);\n+    BasicType elem_bt = Matcher::vector_element_basic_type(this);\n+    __ xorl($mask_idx$$Register, $mask_idx$$Register);\n+    __ lea($tmp$$Register, $mem$$Address);\n+    __ movptr($idx_base_temp$$Register, $idx_base$$Register);\n+    __ vpmovmskb($rtmp2$$Register, $mask$$XMMRegister, vlen_enc);\n+    if (elem_bt == T_SHORT) {\n+      __ movl($mask_idx$$Register, 0x55555555);\n+      __ pextl($rtmp2$$Register, $rtmp2$$Register, $mask_idx$$Register);\n+    }\n+    __ xorl($mask_idx$$Register, $mask_idx$$Register);\n+    __ vgather_subword(elem_bt, $dst$$XMMRegister, $tmp$$Register, $idx_base_temp$$Register, $offset$$Register, $rtmp2$$Register, $xtmp1$$XMMRegister,\n+                       $xtmp2$$XMMRegister, $xtmp3$$XMMRegister, $rtmp$$Register, $mask_idx$$Register, $length$$Register, vector_len, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+#endif\n+\n","filename":"src\/hotspot\/cpu\/x86\/x86.ad","additions":255,"deletions":5,"binary":false,"changes":260,"status":"modified"},{"patch":"@@ -1660,2 +1660,1 @@\n-  if (RegisterFinalizersAtInit &&\n-      method()->intrinsic_id() == vmIntrinsics::_Object_init) {\n+  if (method()->intrinsic_id() == vmIntrinsics::_Object_init) {\n","filename":"src\/hotspot\/share\/c1\/c1_GraphBuilder.cpp","additions":1,"deletions":2,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -40,0 +40,1 @@\n+#include \"compiler\/compilerOracle.hpp\"\n@@ -3748,1 +3749,1 @@\n-    if (_method->has_option_value(CompileCommand::CompileThresholdScaling, scale)) {\n+    if (_method->has_option_value(CompileCommandEnum::CompileThresholdScaling, scale)) {\n@@ -3789,1 +3790,1 @@\n-  if (_method->has_option_value(CompileCommand::CompileThresholdScaling, scale)) {\n+  if (_method->has_option_value(CompileCommandEnum::CompileThresholdScaling, scale)) {\n","filename":"src\/hotspot\/share\/c1\/c1_LIRGenerator.cpp","additions":3,"deletions":2,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -1219,2 +1219,1 @@\n-        int indy_index = pool->decode_invokedynamic_index(index);\n-        appendix = Handle(current, pool->cache()->set_dynamic_call(info, indy_index));\n+        appendix = Handle(current, pool->cache()->set_dynamic_call(info, index));\n","filename":"src\/hotspot\/share\/c1\/c1_Runtime1.cpp","additions":1,"deletions":2,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -598,1 +598,1 @@\n-                                       ConstantPool::encode_invokedynamic_index(indy_index),\n+                                       indy_index,\n","filename":"src\/hotspot\/share\/cds\/classListParser.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -362,0 +362,2 @@\n+  st->print_cr(\"- _rw_ptrmap_start_pos:           \" SIZE_FORMAT, _rw_ptrmap_start_pos);\n+  st->print_cr(\"- _ro_ptrmap_start_pos:           \" SIZE_FORMAT, _ro_ptrmap_start_pos);\n@@ -1655,1 +1657,1 @@\n-  size_t old_size = map->size_in_bytes();\n+  size_t old_size = map->size();\n@@ -1664,2 +1666,1 @@\n-\n-  assert(map->size_in_bytes() < old_size, \"Map size should have decreased\");\n+  assert(map->size() <= old_size, \"sanity\");\n@@ -1669,1 +1670,1 @@\n-char* FileMapInfo::write_bitmap_region(const CHeapBitMap* rw_ptrmap, const CHeapBitMap* ro_ptrmap, ArchiveHeapInfo* heap_info,\n+char* FileMapInfo::write_bitmap_region(CHeapBitMap* rw_ptrmap, CHeapBitMap* ro_ptrmap, ArchiveHeapInfo* heap_info,\n@@ -1671,0 +1672,4 @@\n+  size_t removed_rw_zeros = remove_bitmap_leading_zeros(rw_ptrmap);\n+  size_t removed_ro_zeros = remove_bitmap_leading_zeros(ro_ptrmap);\n+  header()->set_rw_ptrmap_start_pos(removed_rw_zeros);\n+  header()->set_ro_ptrmap_start_pos(removed_ro_zeros);\n@@ -2017,1 +2022,1 @@\n-    SharedDataRelocator rw_patcher((address*)rw_patch_base, (address*)rw_patch_end, valid_old_base, valid_old_end,\n+    SharedDataRelocator rw_patcher((address*)rw_patch_base + header()->rw_ptrmap_start_pos(), (address*)rw_patch_end, valid_old_base, valid_old_end,\n@@ -2019,1 +2024,1 @@\n-    SharedDataRelocator ro_patcher((address*)ro_patch_base, (address*)ro_patch_end, valid_old_base, valid_old_end,\n+    SharedDataRelocator ro_patcher((address*)ro_patch_base + header()->ro_ptrmap_start_pos(), (address*)ro_patch_end, valid_old_base, valid_old_end,\n","filename":"src\/hotspot\/share\/cds\/filemap.cpp","additions":11,"deletions":6,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -263,0 +263,2 @@\n+  size_t _rw_ptrmap_start_pos;          \/\/ The first bit in the ptrmap corresponds to this position in the rw region\n+  size_t _ro_ptrmap_start_pos;          \/\/ The first bit in the ptrmap corresponds to this position in the ro region\n@@ -303,2 +305,4 @@\n-  size_t heap_oopmap_start_pos()           const { return _heap_oopmap_start_pos;}\n-  size_t heap_ptrmap_start_pos()           const { return _heap_ptrmap_start_pos;}\n+  size_t heap_oopmap_start_pos()           const { return _heap_oopmap_start_pos; }\n+  size_t heap_ptrmap_start_pos()           const { return _heap_ptrmap_start_pos; }\n+  size_t rw_ptrmap_start_pos()             const { return _rw_ptrmap_start_pos; }\n+  size_t ro_ptrmap_start_pos()             const { return _ro_ptrmap_start_pos; }\n@@ -318,0 +322,2 @@\n+  void set_rw_ptrmap_start_pos(size_t n)         { _rw_ptrmap_start_pos = n; }\n+  void set_ro_ptrmap_start_pos(size_t n)         { _ro_ptrmap_start_pos = n; }\n@@ -478,1 +484,1 @@\n-  char* write_bitmap_region(const CHeapBitMap* rw_ptrmap, const CHeapBitMap* ro_ptrmap, ArchiveHeapInfo* heap_info,\n+  char* write_bitmap_region(CHeapBitMap* rw_ptrmap, CHeapBitMap* ro_ptrmap, ArchiveHeapInfo* heap_info,\n","filename":"src\/hotspot\/share\/cds\/filemap.hpp","additions":9,"deletions":3,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -1400,1 +1400,1 @@\n-              name == vmSymbols::java_lang_VirtualMachineError() ||\n+              name == vmSymbols::java_lang_InternalError() ||\n","filename":"src\/hotspot\/share\/cds\/heapShared.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -518,0 +518,1 @@\n+  builder.sort_metadata_objs();\n","filename":"src\/hotspot\/share\/cds\/metaspaceShared.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -878,4 +878,2 @@\n-    int indy_index = cpool->decode_invokedynamic_index(index);\n-    assert (indy_index >= 0, \"should be\");\n-    assert(indy_index < cpool->cache()->resolved_indy_entries_length(), \"impossible\");\n-    Method* adapter = cpool->resolved_indy_entry_at(indy_index)->method();\n+    assert(index < cpool->cache()->resolved_indy_entries_length(), \"impossible\");\n+    Method* adapter = cpool->resolved_indy_entry_at(index)->method();\n@@ -1508,2 +1506,1 @@\n-  int index = cp->decode_invokedynamic_index(indy_index);\n-  ResolvedIndyEntry* indy_info = cp->resolved_indy_entry_at(index);\n+  ResolvedIndyEntry* indy_info = cp->resolved_indy_entry_at(indy_index);\n@@ -1515,1 +1512,1 @@\n-    oop appendix = cp->resolved_reference_from_indy(index);\n+    oop appendix = cp->resolved_reference_from_indy(indy_index);\n@@ -1522,1 +1519,1 @@\n-    BootstrapInfo bootstrap_specifier(cp, pool_index, index);\n+    BootstrapInfo bootstrap_specifier(cp, pool_index, indy_index);\n","filename":"src\/hotspot\/share\/ci\/ciEnv.cpp","additions":5,"deletions":8,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -39,0 +39,1 @@\n+#include \"compiler\/compilerOracle.hpp\"\n@@ -1123,1 +1124,1 @@\n-bool ciMethod::has_option(enum CompileCommand option) {\n+bool ciMethod::has_option(CompileCommandEnum option) {\n@@ -1133,1 +1134,1 @@\n-bool ciMethod::has_option_value(enum CompileCommand option, double& value) {\n+bool ciMethod::has_option_value(CompileCommandEnum option, double& value) {\n","filename":"src\/hotspot\/share\/ci\/ciMethod.cpp","additions":3,"deletions":2,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1999, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1999, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -34,1 +34,0 @@\n-#include \"compiler\/compilerOracle.hpp\"\n@@ -38,0 +37,1 @@\n+#include \"utilities\/vmEnums.hpp\"\n@@ -310,2 +310,2 @@\n-  bool has_option(enum CompileCommand option);\n-  bool has_option_value(enum CompileCommand option, double& value);\n+  bool has_option(CompileCommandEnum option);\n+  bool has_option_value(CompileCommandEnum option, double& value);\n","filename":"src\/hotspot\/share\/ci\/ciMethod.hpp","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -420,1 +420,0 @@\n-        index = cp->decode_invokedynamic_index(index);\n","filename":"src\/hotspot\/share\/ci\/ciReplay.cpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -4376,2 +4376,1 @@\n-  if ((!RegisterFinalizersAtInit && ik->has_finalizer())\n-      || ik->is_abstract() || ik->is_interface()\n+  if (ik->is_abstract() || ik->is_interface()\n","filename":"src\/hotspot\/share\/classfile\/classFileParser.cpp","additions":1,"deletions":2,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -2047,1 +2047,1 @@\n-  \/\/ Throw VirtualMachineError or the pending exception in the JavaThread\n+  \/\/ Throw OOM or the pending exception in the JavaThread\n@@ -2049,1 +2049,1 @@\n-    THROW_MSG_NULL(vmSymbols::java_lang_VirtualMachineError(),\n+    THROW_MSG_NULL(vmSymbols::java_lang_OutOfMemoryError(),\n","filename":"src\/hotspot\/share\/classfile\/systemDictionary.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -259,1 +259,1 @@\n-        THROW_OOP_(Universe::virtual_machine_error_instance(), false);\n+        THROW_OOP_(Universe::internal_error_instance(), false);\n","filename":"src\/hotspot\/share\/classfile\/verifier.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2020, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2020, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -514,0 +514,3 @@\n+  case vmIntrinsics::_setMemory:\n+    if (!InlineUnsafeOps) return true;\n+    break;\n","filename":"src\/hotspot\/share\/classfile\/vmIntrinsics.cpp","additions":4,"deletions":1,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -628,0 +628,3 @@\n+  do_intrinsic(_setMemory,                jdk_internal_misc_Unsafe,     setMemory_name,  setMemory_signature,          F_RN)     \\\n+   do_name(     setMemory_name,                                         \"setMemory0\")                                            \\\n+   do_signature(setMemory_signature,                                    \"(Ljava\/lang\/Object;JJB)V\")                              \\\n","filename":"src\/hotspot\/share\/classfile\/vmIntrinsics.hpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -76,36 +76,2 @@\n-#ifdef ASSERT\n-void CodeBlob::verify_parameters() {\n-  assert(is_aligned(_size,            oopSize), \"unaligned size\");\n-  assert(is_aligned(_header_size,     oopSize), \"unaligned size\");\n-  assert(is_aligned(_relocation_size, oopSize), \"unaligned size\");\n-  assert(_data_offset <= size(), \"codeBlob is too small\");\n-  assert(code_end() == content_end(), \"must be the same - see code_end()\");\n-#ifdef COMPILER1\n-  \/\/ probably wrong for tiered\n-  assert(frame_size() >= -1, \"must use frame size or -1 for runtime stubs\");\n-#endif \/\/ COMPILER1\n-}\n-#endif\n-\n-CodeBlob::CodeBlob(const char* name, CodeBlobKind kind, int size, int header_size, int relocation_size,\n-                   int content_offset, int code_offset, int frame_complete_offset, int data_offset,\n-                   int frame_size, ImmutableOopMapSet* oop_maps, bool caller_must_gc_arguments) :\n-  _oop_maps(oop_maps),\n-  _name(name),\n-  _size(size),\n-  _header_size(header_size),\n-  _relocation_size(relocation_size),\n-  _content_offset(content_offset),\n-  _code_offset(code_offset),\n-  _frame_complete_offset(frame_complete_offset),\n-  _data_offset(data_offset),\n-  _frame_size(frame_size),\n-  S390_ONLY(_ctable_offset(0) COMMA)\n-  _kind(kind),\n-  _caller_must_gc_arguments(caller_must_gc_arguments)\n-{\n-  DEBUG_ONLY( verify_parameters(); )\n-}\n-\n-CodeBlob::CodeBlob(const char* name, CodeBlobKind kind, CodeBuffer* cb, int size, int header_size,\n-                   int frame_complete_offset, int frame_size, OopMapSet* oop_maps, bool caller_must_gc_arguments) :\n+CodeBlob::CodeBlob(const char* name, CodeBlobKind kind, CodeBuffer* cb, int size, uint16_t header_size,\n+                   int16_t frame_complete_offset, int frame_size, OopMapSet* oop_maps, bool caller_must_gc_arguments) :\n@@ -115,2 +81,1 @@\n-  _header_size(header_size),\n-  _content_offset(CodeBlob::align_code_offset(_header_size + _relocation_size)),\n+  _content_offset(CodeBlob::align_code_offset(header_size + _relocation_size)),\n@@ -119,1 +84,0 @@\n-  _frame_complete_offset(frame_complete_offset),\n@@ -123,0 +87,2 @@\n+  _header_size(header_size),\n+  _frame_complete_offset(frame_complete_offset),\n@@ -126,1 +92,9 @@\n-  DEBUG_ONLY( verify_parameters(); )\n+  assert(is_aligned(_size,            oopSize), \"unaligned size\");\n+  assert(is_aligned(header_size,      oopSize), \"unaligned size\");\n+  assert(is_aligned(_relocation_size, oopSize), \"unaligned size\");\n+  assert(_data_offset <= _size, \"codeBlob is too small: %d > %d\", _data_offset, _size);\n+  assert(code_end() == content_end(), \"must be the same - see code_end()\");\n+#ifdef COMPILER1\n+  \/\/ probably wrong for tiered\n+  assert(_frame_size >= -1, \"must use frame size or -1 for runtime stubs\");\n+#endif \/\/ COMPILER1\n@@ -132,1 +106,1 @@\n-CodeBlob::CodeBlob(const char* name, CodeBlobKind kind, int size, int header_size) :\n+CodeBlob::CodeBlob(const char* name, CodeBlobKind kind, int size, uint16_t header_size) :\n@@ -136,1 +110,0 @@\n-  _header_size(header_size),\n@@ -140,1 +113,0 @@\n-  _frame_complete_offset(CodeOffsets::frame_never_safe),\n@@ -144,0 +116,2 @@\n+  _header_size(header_size),\n+  _frame_complete_offset(CodeOffsets::frame_never_safe),\n@@ -151,1 +125,1 @@\n-void CodeBlob::purge(bool free_code_cache_data, bool unregister_nmethod) {\n+void CodeBlob::purge() {\n@@ -188,2 +162,2 @@\n-  int         header_size,\n-  int         frame_complete,\n+  uint16_t    header_size,\n+  int16_t     frame_complete,\n@@ -201,1 +175,1 @@\n-  blob->purge(true \/* free_code_cache_data *\/, true \/* unregister_nmethod *\/);\n+  blob->purge();\n@@ -440,1 +414,1 @@\n-  int         frame_complete,\n+  int16_t     frame_complete,\n@@ -452,1 +426,1 @@\n-                                           int frame_complete,\n+                                           int16_t frame_complete,\n@@ -700,4 +674,0 @@\n-void UpcallStub::preserve_callee_argument_oops(frame fr, const RegisterMap* reg_map, OopClosure* f) {\n-  ShouldNotReachHere(); \/\/ caller should never have to gc arguments\n-}\n-\n","filename":"src\/hotspot\/share\/code\/codeBlob.cpp","additions":23,"deletions":53,"binary":false,"changes":76,"status":"modified"},{"patch":"@@ -95,2 +95,2 @@\n-class UpcallStub; \/\/ for as_upcall_stub()\n-class RuntimeStub; \/\/ for as_runtime_stub()\n+class UpcallStub;      \/\/ for as_upcall_stub()\n+class RuntimeStub;     \/\/ for as_runtime_stub()\n@@ -106,1 +106,1 @@\n-  ImmutableOopMapSet* _oop_maps;                 \/\/ OopMap for this CodeBlob\n+  ImmutableOopMapSet* _oop_maps;   \/\/ OopMap for this CodeBlob\n@@ -109,11 +109,4 @@\n-  int        _size;                              \/\/ total size of CodeBlob in bytes\n-  int        _header_size;                       \/\/ size of header (depends on subclass)\n-  int        _relocation_size;                   \/\/ size of relocation\n-  int        _content_offset;                    \/\/ offset to where content region begins (this includes consts, insts, stubs)\n-  int        _code_offset;                       \/\/ offset to where instructions region begins (this includes insts, stubs)\n-  int        _frame_complete_offset;             \/\/ instruction offsets in [0.._frame_complete_offset) have\n-                                                 \/\/ not finished setting up their frame. Beware of pc's in\n-                                                 \/\/ that range. There is a similar range(s) on returns\n-                                                 \/\/ which we don't detect.\n-  int        _data_offset;                       \/\/ offset to where data region begins\n-  int        _frame_size;                        \/\/ size of stack frame in words (NOT slots. On x64 these are 64bit words)\n+  int      _size;                  \/\/ total size of CodeBlob in bytes\n+  int      _relocation_size;       \/\/ size of relocation (could be bigger than 64Kb)\n+  int      _content_offset;        \/\/ offset to where content region begins (this includes consts, insts, stubs)\n+  int      _code_offset;           \/\/ offset to where instructions region begins (this includes insts, stubs)\n@@ -121,1 +114,2 @@\n-  S390_ONLY(int       _ctable_offset;)\n+  int      _data_offset;           \/\/ offset to where data region begins\n+  int      _frame_size;            \/\/ size of stack frame in words (NOT slots. On x64 these are 64bit words)\n@@ -123,1 +117,1 @@\n-  CodeBlobKind        _kind;                     \/\/ Kind of this code blob\n+  S390_ONLY(int _ctable_offset;)\n@@ -125,1 +119,9 @@\n-  bool                _caller_must_gc_arguments;\n+  uint16_t _header_size;           \/\/ size of header (depends on subclass)\n+  int16_t  _frame_complete_offset; \/\/ instruction offsets in [0.._frame_complete_offset) have\n+                                   \/\/ not finished setting up their frame. Beware of pc's in\n+                                   \/\/ that range. There is a similar range(s) on returns\n+                                   \/\/ which we don't detect.\n+\n+  CodeBlobKind _kind;              \/\/ Kind of this code blob\n+\n+  bool _caller_must_gc_arguments;\n@@ -130,7 +132,1 @@\n-#endif \/\/ not PRODUCT\n-\n-  DEBUG_ONLY( void verify_parameters() );\n-\n-  CodeBlob(const char* name, CodeBlobKind kind, int size, int header_size, int relocation_size,\n-           int content_offset, int code_offset, int data_offset, int frame_complete_offset,\n-           int frame_size, ImmutableOopMapSet* oop_maps, bool caller_must_gc_arguments);\n+#endif\n@@ -138,2 +134,2 @@\n-  CodeBlob(const char* name, CodeBlobKind kind, CodeBuffer* cb, int size, int header_size,\n-           int frame_complete_offset, int frame_size, OopMapSet* oop_maps, bool caller_must_gc_arguments);\n+  CodeBlob(const char* name, CodeBlobKind kind, CodeBuffer* cb, int size, uint16_t header_size,\n+           int16_t frame_complete_offset, int frame_size, OopMapSet* oop_maps, bool caller_must_gc_arguments);\n@@ -142,1 +138,1 @@\n-  CodeBlob(const char* name, CodeBlobKind kind, int size, int header_size);\n+  CodeBlob(const char* name, CodeBlobKind kind, int size, uint16_t header_size);\n@@ -157,1 +153,1 @@\n-  virtual void purge(bool free_code_cache_data, bool unregister_nmethod);\n+  void purge();\n@@ -231,1 +227,0 @@\n-  virtual void preserve_callee_argument_oops(frame fr, const RegisterMap* reg_map, OopClosure* f) = 0;\n@@ -279,1 +274,1 @@\n-  RuntimeBlob(const char* name, CodeBlobKind kind, int size, int header_size)\n+  RuntimeBlob(const char* name, CodeBlobKind kind, int size, uint16_t header_size)\n@@ -291,2 +286,2 @@\n-    int         header_size,\n-    int         frame_complete,\n+    uint16_t    header_size,\n+    int16_t     frame_complete,\n@@ -332,3 +327,1 @@\n-  \/\/ GC\/Verification support\n-  void preserve_callee_argument_oops(frame fr, const RegisterMap* reg_map, OopClosure* f) override { \/* nothing to do *\/ }\n-\n+  \/\/ Verification support\n@@ -336,0 +329,1 @@\n+\n@@ -414,1 +408,1 @@\n-    int         frame_complete,\n+    int16_t     frame_complete,\n@@ -427,1 +421,1 @@\n-    int         frame_complete,\n+    int16_t     frame_complete,\n@@ -438,3 +432,1 @@\n-  \/\/ GC\/Verification support\n-  void preserve_callee_argument_oops(frame fr, const RegisterMap *reg_map, OopClosure* f) override { \/* nothing to do *\/ }\n-\n+  \/\/ Verification support\n@@ -442,0 +434,1 @@\n+\n@@ -462,1 +455,1 @@\n-     int          header_size,\n+     uint16_t     header_size,\n@@ -471,2 +464,1 @@\n-  \/\/ GC\/Verification support\n-  void preserve_callee_argument_oops(frame fr, const RegisterMap *reg_map, OopClosure* f) override { \/* nothing to do *\/ }\n+  \/\/ Verification support\n@@ -474,0 +466,1 @@\n+\n@@ -665,1 +658,0 @@\n-  void preserve_callee_argument_oops(frame fr, const RegisterMap* reg_map, OopClosure* f) override;\n","filename":"src\/hotspot\/share\/code\/codeBlob.hpp","additions":36,"deletions":44,"binary":false,"changes":80,"status":"modified"},{"patch":"@@ -247,1 +247,0 @@\n-  int chunks_reshared;\n@@ -251,4 +250,2 @@\n-    tty->print_cr(\"Debug Data Chunks: %d, shared %d+%d, non-SP's elided %d\",\n-                  chunks_queried,\n-                  chunks_shared, chunks_reshared,\n-                  chunks_elided);\n+    tty->print_cr(\"Debug Data Chunks: %d, shared %d, non-SP's elided %d\",\n+                  chunks_queried, chunks_shared, chunks_elided);\n","filename":"src\/hotspot\/share\/code\/debugInfoRec.cpp","additions":2,"deletions":5,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2005, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2005, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -391,3 +391,1 @@\n-  Copy::disjoint_words((HeapWord*) content_bytes(),\n-                       (HeapWord*) beg,\n-                       size_in_bytes() \/ sizeof(HeapWord));\n+  (void)memcpy(beg, content_bytes(), size_in_bytes());\n","filename":"src\/hotspot\/share\/code\/dependencies.cpp","additions":2,"deletions":4,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -39,0 +39,1 @@\n+#include \"compiler\/compilerOracle.hpp\"\n@@ -140,3 +141,0 @@\n-  uint size_gt_32k;\n-  int size_max;\n-\n@@ -161,3 +159,0 @@\n-    int short_pos_max = ((1<<15) - 1);\n-    if (nm->size() > short_pos_max) size_gt_32k++;\n-    if (nm->size() > size_max) size_max = nm->size();\n@@ -186,2 +181,0 @@\n-    if (size_gt_32k != 0)         tty->print_cr(\" size > 32k     = %u\", size_gt_32k);\n-    if (size_max != 0)            tty->print_cr(\" max size       = %d\", size_max);\n@@ -1018,21 +1011,0 @@\n-\/\/ Fill in default values for various flag fields\n-void nmethod::init_defaults() {\n-  \/\/ avoid uninitialized fields, even for short time periods\n-  _exception_cache            = nullptr;\n-\n-  _has_unsafe_access          = 0;\n-  _has_method_handle_invokes  = 0;\n-  _has_wide_vectors           = 0;\n-  _has_monitors               = 0;\n-\n-  _state                      = not_installed;\n-  _has_flushed_dependencies   = 0;\n-  _load_reported              = false; \/\/ jvmti state\n-\n-  _oops_do_mark_link          = nullptr;\n-  _osr_link                   = nullptr;\n-#if INCLUDE_RTM_OPT\n-  _rtm_state                  = NoRTM;\n-#endif\n-}\n-\n@@ -1213,0 +1185,50 @@\n+\/\/ Fill in default values for various fields\n+void nmethod::init_defaults(CodeBuffer *code_buffer, CodeOffsets* offsets) {\n+  \/\/ avoid uninitialized fields, even for short time periods\n+  _exception_cache            = nullptr;\n+  _gc_data                    = nullptr;\n+  _oops_do_mark_link          = nullptr;\n+  _compiled_ic_data           = nullptr;\n+\n+#if INCLUDE_RTM_OPT\n+  _rtm_state                  = NoRTM;\n+#endif\n+  _is_unloading_state         = 0;\n+  _state                      = not_installed;\n+\n+  _has_unsafe_access          = 0;\n+  _has_method_handle_invokes  = 0;\n+  _has_wide_vectors           = 0;\n+  _has_monitors               = 0;\n+  _has_flushed_dependencies   = 0;\n+  _is_unlinked                = 0;\n+  _load_reported              = 0; \/\/ jvmti state\n+\n+  _deoptimization_status      = not_marked;\n+\n+  \/\/ SECT_CONSTS is first in code buffer so the offset should be 0.\n+  int consts_offset = code_buffer->total_offset_of(code_buffer->consts());\n+  assert(consts_offset == 0, \"const_offset: %d\", consts_offset);\n+\n+  _entry_offset          = checked_cast<uint16_t>(offsets->value(CodeOffsets::Entry));\n+  _verified_entry_offset = checked_cast<uint16_t>(offsets->value(CodeOffsets::Verified_Entry));\n+  _inline_entry_point             = entry_point();\n+  _verified_inline_entry_point    = verified_entry_point();\n+  _verified_inline_ro_entry_point = verified_entry_point();\n+  _stub_offset           = content_offset() + code_buffer->total_offset_of(code_buffer->stubs());\n+\n+  _skipped_instructions_size = checked_cast<uint16_t>(code_buffer->total_skipped_instructions_size());\n+}\n+\n+\/\/ Post initialization\n+void nmethod::post_init() {\n+  clear_unloading_state();\n+\n+  finalize_relocations();\n+\n+  Universe::heap()->register_nmethod(this);\n+  debug_only(Universe::heap()->verify_nmethod(this));\n+\n+  CodeCache::commit(this);\n+}\n+\n@@ -1228,0 +1250,1 @@\n+  _gc_epoch(CodeCache::gc_epoch()),\n@@ -1229,6 +1252,1 @@\n-  _gc_data(nullptr),\n-  _compiled_ic_data(nullptr),\n-  _is_unlinked(false),\n-  _native_basic_lock_sp_offset(basic_lock_sp_offset),\n-  _is_unloading_state(0),\n-  _deoptimization_status(not_marked)\n+  _native_basic_lock_sp_offset(basic_lock_sp_offset)\n@@ -1240,0 +1258,2 @@\n+    assert(!method->has_scalarized_args(), \"scalarized native wrappers not supported yet\");\n+    init_defaults(code_buffer, offsets);\n@@ -1241,2 +1261,1 @@\n-    init_defaults();\n-    _comp_level              = CompLevel_none;\n+    _osr_entry_point         = nullptr;\n@@ -1244,4 +1263,3 @@\n-    _num_stack_arg_slots     = _method->constMethod()->num_stack_arg_slots();\n-    \/\/ We have no exception handler or deopt handler make the\n-    \/\/ values something that will never match a pc like the nmethod vtable entry\n-    _exception_offset        = 0;\n+    _compile_id              = compile_id;\n+    _comp_level              = CompLevel_none;\n+    _compiler_type           = type;\n@@ -1249,0 +1267,10 @@\n+    _num_stack_arg_slots     = _method->constMethod()->num_stack_arg_slots();\n+\n+    if (offsets->value(CodeOffsets::Exceptions) != -1) {\n+      \/\/ Continuation enter intrinsic\n+      _exception_offset      = code_offset() + offsets->value(CodeOffsets::Exceptions);\n+    } else {\n+      _exception_offset      = 0;\n+    }\n+    \/\/ Native wrappers do not have deopt handlers. Make the values\n+    \/\/ something that will never match a pc like the nmethod vtable entry\n@@ -1251,10 +1279,7 @@\n-    _gc_epoch                = CodeCache::gc_epoch();\n-\n-    _consts_offset           = content_offset()      + code_buffer->total_offset_of(code_buffer->consts());\n-    _stub_offset             = content_offset()      + code_buffer->total_offset_of(code_buffer->stubs());\n-    _oops_offset             = data_offset();\n-    _metadata_offset         = _oops_offset          + align_up(code_buffer->total_oop_size(), oopSize);\n-    _scopes_data_offset      = _metadata_offset      + align_up(code_buffer->total_metadata_size(), wordSize);\n-    _scopes_pcs_offset       = _scopes_data_offset;\n-    _dependencies_offset     = _scopes_pcs_offset;\n-    _handler_table_offset    = _dependencies_offset;\n+    _unwind_handler_offset   = 0;\n+\n+    _metadata_offset         = checked_cast<uint16_t>(align_up(code_buffer->total_oop_size(), oopSize));\n+    _dependencies_offset     = checked_cast<uint16_t>(_metadata_offset + align_up(code_buffer->total_metadata_size(), wordSize));\n+    _scopes_pcs_offset       = _dependencies_offset;\n+    _scopes_data_offset      = _scopes_pcs_offset;\n+    _handler_table_offset    = _scopes_data_offset;\n@@ -1262,1 +1287,0 @@\n-    _skipped_instructions_size = code_buffer->total_skipped_instructions_size();\n@@ -1266,1 +1290,1 @@\n-    _nmethod_end_offset      = _jvmci_data_offset;\n+    DEBUG_ONLY( int data_end_offset = _jvmci_data_offset; )\n@@ -1268,1 +1292,1 @@\n-    _nmethod_end_offset      = _nul_chk_table_offset;\n+    DEBUG_ONLY( int data_end_offset = _nul_chk_table_offset; )\n@@ -1270,9 +1294,1 @@\n-    _compile_id              = compile_id;\n-    _compiler_type           = type;\n-    _entry_point             = code_begin()          + offsets->value(CodeOffsets::Entry);\n-    _verified_entry_point    = code_begin()          + offsets->value(CodeOffsets::Verified_Entry);\n-\n-    assert(!method->has_scalarized_args(), \"scalarized native wrappers not supported yet\"); \/\/ for the next 3 fields\n-    _inline_entry_point       = _entry_point;\n-    _verified_inline_entry_point = _verified_entry_point;\n-    _verified_inline_ro_entry_point = _verified_entry_point;\n+    assert((data_offset() + data_end_offset) <= nmethod_size, \"wrong nmethod's size: %d < %d\", nmethod_size, (data_offset() + data_end_offset));\n@@ -1280,2 +1296,0 @@\n-    _osr_entry_point         = nullptr;\n-    _exception_cache         = nullptr;\n@@ -1284,2 +1298,0 @@\n-    _exception_offset        = code_offset()         + offsets->value(CodeOffsets::Exceptions);\n-\n@@ -1289,8 +1301,1 @@\n-    clear_unloading_state();\n-\n-    finalize_relocations();\n-\n-    Universe::heap()->register_nmethod(this);\n-    debug_only(Universe::heap()->verify_nmethod(this));\n-\n-    CodeCache::commit(this);\n+    post_init();\n@@ -1354,0 +1359,1 @@\n+\/\/ For normal JIT compiled code\n@@ -1380,0 +1386,1 @@\n+  _gc_epoch(CodeCache::gc_epoch()),\n@@ -1381,7 +1388,1 @@\n-  _gc_data(nullptr),\n-  _compiled_ic_data(nullptr),\n-  _is_unlinked(false),\n-  _native_receiver_sp_offset(in_ByteSize(-1)),\n-  _native_basic_lock_sp_offset(in_ByteSize(-1)),\n-  _is_unloading_state(0),\n-  _deoptimization_status(not_marked)\n+  _osr_link(nullptr)\n@@ -1394,8 +1395,10 @@\n-    init_defaults();\n-    _entry_bci               = entry_bci;\n-    _num_stack_arg_slots     = entry_bci != InvocationEntryBci ? 0 : _method->constMethod()->num_stack_arg_slots();\n-    _compile_id              = compile_id;\n-    _compiler_type           = type;\n-    _comp_level              = comp_level;\n-    _orig_pc_offset          = orig_pc_offset;\n-    _gc_epoch                = CodeCache::gc_epoch();\n+    init_defaults(code_buffer, offsets);\n+\n+    _osr_entry_point = code_begin() + offsets->value(CodeOffsets::OSR_Entry);\n+    _entry_bci       = entry_bci;\n+    _compile_id      = compile_id;\n+    _comp_level      = comp_level;\n+    _compiler_type   = type;\n+    _orig_pc_offset  = orig_pc_offset;\n+\n+    _num_stack_arg_slots = entry_bci != InvocationEntryBci ? 0 : _method->constMethod()->num_stack_arg_slots();\n@@ -1403,5 +1406,1 @@\n-    \/\/ Section offsets\n-    _consts_offset  = content_offset() + code_buffer->total_offset_of(code_buffer->consts());\n-    _stub_offset    = content_offset() + code_buffer->total_offset_of(code_buffer->stubs());\n-    set_ctable_begin(header_begin() + _consts_offset);\n-    _skipped_instructions_size = code_buffer->total_skipped_instructions_size();\n+    set_ctable_begin(header_begin() + content_offset());\n@@ -1447,9 +1446,6 @@\n-\n-    _oops_offset             = data_offset();\n-    _metadata_offset         = _oops_offset          + align_up(code_buffer->total_oop_size(), oopSize);\n-    _scopes_data_offset      = _metadata_offset      + align_up(code_buffer->total_metadata_size(), wordSize);\n-\n-    _scopes_pcs_offset       = _scopes_data_offset   + align_up(debug_info->data_size       (), oopSize);\n-    _dependencies_offset     = _scopes_pcs_offset    + adjust_pcs_size(debug_info->pcs_size());\n-    _handler_table_offset    = _dependencies_offset  + align_up((int)dependencies->size_in_bytes(), oopSize);\n-    _nul_chk_table_offset    = _handler_table_offset + align_up(handler_table->size_in_bytes(), oopSize);\n+    _metadata_offset      = checked_cast<uint16_t>(align_up(code_buffer->total_oop_size(), oopSize));\n+    _dependencies_offset  = checked_cast<uint16_t>(_metadata_offset     + align_up(code_buffer->total_metadata_size(), wordSize));\n+    _scopes_pcs_offset    = checked_cast<uint16_t>(_dependencies_offset + align_up((int)dependencies->size_in_bytes(), oopSize));\n+    _scopes_data_offset   = _scopes_pcs_offset    + adjust_pcs_size(debug_info->pcs_size());\n+    _handler_table_offset = _scopes_data_offset   + align_up(debug_info->data_size       (), oopSize);\n+    _nul_chk_table_offset = _handler_table_offset + align_up(handler_table->size_in_bytes(), oopSize);\n@@ -1457,4 +1453,4 @@\n-    _speculations_offset     = _nul_chk_table_offset + align_up(nul_chk_table->size_in_bytes(), oopSize);\n-    _jvmci_data_offset       = _speculations_offset  + align_up(speculations_len, oopSize);\n-    int jvmci_data_size      = compiler->is_jvmci() ? jvmci_data->size() : 0;\n-    _nmethod_end_offset      = _jvmci_data_offset    + align_up(jvmci_data_size, oopSize);\n+    _speculations_offset  = _nul_chk_table_offset + align_up(nul_chk_table->size_in_bytes(), oopSize);\n+    _jvmci_data_offset    = _speculations_offset  + align_up(speculations_len, oopSize);\n+    int jvmci_data_size   = compiler->is_jvmci() ? jvmci_data->size() : 0;\n+    DEBUG_ONLY( int data_end_offset = _jvmci_data_offset    + align_up(jvmci_data_size, oopSize); )\n@@ -1462,1 +1458,1 @@\n-    _nmethod_end_offset      = _nul_chk_table_offset + align_up(nul_chk_table->size_in_bytes(), oopSize);\n+    DEBUG_ONLY( int data_end_offset = _nul_chk_table_offset + align_up(nul_chk_table->size_in_bytes(), oopSize); )\n@@ -1464,7 +1460,4 @@\n-    _entry_point             = code_begin()          + offsets->value(CodeOffsets::Entry);\n-    _verified_entry_point    = code_begin()          + offsets->value(CodeOffsets::Verified_Entry);\n-    _inline_entry_point       = code_begin()         + offsets->value(CodeOffsets::Inline_Entry);\n-    _verified_inline_entry_point = code_begin()      + offsets->value(CodeOffsets::Verified_Inline_Entry);\n-    _verified_inline_ro_entry_point = code_begin()   + offsets->value(CodeOffsets::Verified_Inline_Entry_RO);\n-    _osr_entry_point         = code_begin()          + offsets->value(CodeOffsets::OSR_Entry);\n-    _exception_cache         = nullptr;\n+    _inline_entry_point             = code_begin() + offsets->value(CodeOffsets::Inline_Entry);\n+    _verified_inline_entry_point    = code_begin() + offsets->value(CodeOffsets::Verified_Inline_Entry);\n+    _verified_inline_ro_entry_point = code_begin() + offsets->value(CodeOffsets::Verified_Inline_Entry_RO);\n+    assert((data_offset() + data_end_offset) <= nmethod_size, \"wrong nmethod's size: %d < %d\", nmethod_size, (data_offset() + data_end_offset));\n@@ -1472,0 +1465,1 @@\n+    \/\/ after _scopes_pcs_offset is set\n@@ -1479,1 +1473,0 @@\n-    clear_unloading_state();\n@@ -1488,7 +1481,0 @@\n-    finalize_relocations();\n-\n-    Universe::heap()->register_nmethod(this);\n-    debug_only(Universe::heap()->verify_nmethod(this));\n-\n-    CodeCache::commit(this);\n-\n@@ -1506,0 +1492,2 @@\n+    post_init();\n+\n@@ -1509,1 +1497,1 @@\n-           _method->is_static() == (entry_point() == _verified_entry_point),\n+           _method->is_static() == (entry_point() == verified_entry_point()),\n@@ -1638,1 +1626,1 @@\n-    if (printmethod || PrintDebugInfo || CompilerOracle::has_option(mh, CompileCommand::PrintDebugInfo)) {\n+    if (printmethod || PrintDebugInfo || CompilerOracle::has_option(mh, CompileCommandEnum::PrintDebugInfo)) {\n@@ -1642,1 +1630,1 @@\n-    if (printmethod || PrintRelocations || CompilerOracle::has_option(mh, CompileCommand::PrintRelocations)) {\n+    if (printmethod || PrintRelocations || CompilerOracle::has_option(mh, CompileCommandEnum::PrintRelocations)) {\n@@ -1646,1 +1634,1 @@\n-    if (printmethod || PrintDependencies || CompilerOracle::has_option(mh, CompileCommand::PrintDependencies)) {\n+    if (printmethod || PrintDependencies || CompilerOracle::has_option(mh, CompileCommandEnum::PrintDependencies)) {\n@@ -2022,1 +2010,1 @@\n-  if (_is_unlinked) {\n+  if (is_unlinked()) {\n@@ -2056,2 +2044,1 @@\n-void nmethod::purge(bool free_code_cache_data, bool unregister_nmethod) {\n-  assert(!free_code_cache_data, \"must only call not freeing code cache data\");\n+void nmethod::purge(bool unregister_nmethod) {\n@@ -2085,1 +2072,1 @@\n-  CodeBlob::purge(free_code_cache_data, unregister_nmethod);\n+  CodeBlob::purge();\n","filename":"src\/hotspot\/share\/code\/nmethod.cpp","additions":119,"deletions":132,"binary":false,"changes":251,"status":"modified"},{"patch":"@@ -192,2 +192,17 @@\n-  \/\/ To support simple linked-list chaining of nmethods:\n-  nmethod*  _osr_link;         \/\/ from InstanceKlass::osr_nmethods_head\n+  \/\/ To reduce header size union fields which usages do not overlap.\n+  union {\n+    \/\/ To support simple linked-list chaining of nmethods:\n+    nmethod*  _osr_link; \/\/ from InstanceKlass::osr_nmethods_head\n+    struct {\n+      \/\/ These are used for compiled synchronized native methods to\n+      \/\/ locate the owner and stack slot for the BasicLock. They are\n+      \/\/ needed because there is no debug information for compiled native\n+      \/\/ wrappers and the oop maps are insufficient to allow\n+      \/\/ frame::retrieve_receiver() to work. Currently they are expected\n+      \/\/ to be byte offsets from the Java stack pointer for maximum code\n+      \/\/ sharing between platforms. JVMTI's GetLocalInstance() uses these\n+      \/\/ offsets to find the receiver for non-static native wrapper frames.\n+      ByteSize _native_receiver_sp_offset;\n+      ByteSize _native_basic_lock_sp_offset;\n+    };\n+  };\n@@ -204,0 +219,2 @@\n+  CompiledICData* _compiled_ic_data;\n+\n@@ -205,6 +222,8 @@\n-  address _entry_point;                      \/\/ entry point with class check\n-  address _verified_entry_point;             \/\/ entry point without class check\n-  address _inline_entry_point;               \/\/ inline type entry point (unpack all inline type args) with class check\n-  address _verified_inline_entry_point;      \/\/ inline type entry point (unpack all inline type args) without class check\n-  address _verified_inline_ro_entry_point;   \/\/ inline type entry point (unpack receiver only) without class check\n-  address _osr_entry_point;                  \/\/ entry point for on stack replacement\n+  address  _osr_entry_point;       \/\/ entry point for on stack replacement\n+  uint16_t _entry_offset;          \/\/ entry point with class check\n+  uint16_t _verified_entry_offset; \/\/ entry point without class check\n+  \/\/ TODO: can these be uint16_t, seem rely on -1 CodeOffset, can change later...\n+  address _inline_entry_point;              \/\/ inline type entry point (unpack all inline type args) with class check\n+  address _verified_inline_entry_point;     \/\/ inline type entry point (unpack all inline type args) without class check\n+  address _verified_inline_ro_entry_point;  \/\/ inline type entry point (unpack receiver only) without class check\n+  int      _entry_bci;             \/\/ != InvocationEntryBci if this nmethod is an on-stack replacement method\n@@ -212,1 +231,1 @@\n-  CompiledICData* _compiled_ic_data;\n+  \/\/ _consts_offset == _content_offset because SECT_CONSTS is first in code buffer\n@@ -214,2 +233,1 @@\n-  \/\/ Shared fields for all nmethod's\n-  int _entry_bci;      \/\/ != InvocationEntryBci if this nmethod is an on-stack replacement method\n+  int _stub_offset;\n@@ -217,2 +235,2 @@\n-  \/\/ Offsets for different nmethod parts\n-  int  _exception_offset;\n+  \/\/ Offsets for different stubs section parts\n+  int _exception_offset;\n@@ -228,9 +246,9 @@\n-  int _consts_offset;\n-  int _stub_offset;\n-  int _oops_offset;                       \/\/ offset to where embedded oop table begins (inside data)\n-  int _metadata_offset;                   \/\/ embedded meta data table\n-  int _scopes_data_offset;\n-  int _scopes_pcs_offset;\n-  int _dependencies_offset;\n-  int _handler_table_offset;\n-  int _nul_chk_table_offset;\n+  uint16_t _skipped_instructions_size;\n+\n+  \/\/ _oops_offset == _data_offset,  offset where embedded oop table begins (inside data)\n+  uint16_t _metadata_offset; \/\/ embedded meta data table\n+  uint16_t _dependencies_offset;\n+  uint16_t _scopes_pcs_offset;\n+  int      _scopes_data_offset;\n+  int      _handler_table_offset;\n+  int      _nul_chk_table_offset;\n@@ -238,2 +256,2 @@\n-  int _speculations_offset;\n-  int _jvmci_data_offset;\n+  int      _speculations_offset;\n+  int      _jvmci_data_offset;\n@@ -241,2 +259,0 @@\n-  int _nmethod_end_offset;\n-  int _skipped_instructions_size;\n@@ -248,5 +264,3 @@\n-  int _compile_id;                        \/\/ which compilation made this nmethod\n-\n-  int _num_stack_arg_slots;               \/\/ Number of arguments passed on the stack\n-\n-  CompilerType _compiler_type;            \/\/ which compiler made this nmethod (u1)\n+  int          _compile_id;            \/\/ which compilation made this nmethod\n+  CompLevel    _comp_level;            \/\/ compilation level (s1)\n+  CompilerType _compiler_type;         \/\/ which compiler made this nmethod (u1)\n@@ -254,1 +268,1 @@\n-  bool _is_unlinked;\n+  uint16_t     _num_stack_arg_slots;   \/\/ Number of arguments passed on the stack\n@@ -262,13 +276,0 @@\n-  \/\/ These are used for compiled synchronized native methods to\n-  \/\/ locate the owner and stack slot for the BasicLock. They are\n-  \/\/ needed because there is no debug information for compiled native\n-  \/\/ wrappers and the oop maps are insufficient to allow\n-  \/\/ frame::retrieve_receiver() to work. Currently they are expected\n-  \/\/ to be byte offsets from the Java stack pointer for maximum code\n-  \/\/ sharing between platforms. JVMTI's GetLocalInstance() uses these\n-  \/\/ offsets to find the receiver for non-static native wrapper frames.\n-  ByteSize _native_receiver_sp_offset;\n-  ByteSize _native_basic_lock_sp_offset;\n-\n-  CompLevel _comp_level;               \/\/ compilation level (s1)\n-\n@@ -278,3 +279,0 @@\n-  \/\/ used by jvmti to track if an event has been posted for this nmethod.\n-  bool _load_reported;\n-\n@@ -289,1 +287,3 @@\n-          _has_flushed_dependencies:1; \/\/ Used for maintenance of dependencies (under CodeCache_lock)\n+          _has_flushed_dependencies:1, \/\/ Used for maintenance of dependencies (under CodeCache_lock)\n+          _is_unlinked:1,              \/\/ mark during class unloading\n+          _load_reported:1;            \/\/ used by jvmti to track if an event has been posted for this nmethod\n@@ -304,0 +304,6 @@\n+  \/\/ Initialize fields to their default values\n+  void init_defaults(CodeBuffer *code_buffer, CodeOffsets* offsets);\n+\n+  \/\/ Post initialization\n+  void post_init();\n+\n@@ -316,1 +322,1 @@\n-  \/\/ Creation support\n+  \/\/ For normal JIT compiled code\n@@ -360,3 +366,0 @@\n-  \/\/ Initialize fields to their default values\n-  void init_defaults();\n-\n@@ -530,3 +533,3 @@\n-  address consts_begin          () const { return           header_begin() + _consts_offset           ; }\n-  address consts_end            () const { return           header_begin() +  code_offset()           ; }\n-  address insts_begin           () const { return           header_begin() +  code_offset()           ; }\n+  address consts_begin          () const { return           content_begin(); }\n+  address consts_end            () const { return           code_begin()   ; }\n+  address insts_begin           () const { return           code_begin()   ; }\n@@ -535,1 +538,1 @@\n-  address stub_end              () const { return           header_begin() + _oops_offset             ; }\n+  address stub_end              () const { return           data_begin()   ; }\n@@ -540,15 +543,16 @@\n-  oop*    oops_begin            () const { return (oop*)   (header_begin() + _oops_offset)            ; }\n-  oop*    oops_end              () const { return (oop*)   (header_begin() + _metadata_offset)        ; }\n-\n-  Metadata** metadata_begin     () const { return (Metadata**) (header_begin() + _metadata_offset)    ; }\n-  Metadata** metadata_end       () const { return (Metadata**) (header_begin() + _scopes_data_offset) ; }\n-\n-  address scopes_data_begin     () const { return           header_begin() + _scopes_data_offset      ; }\n-  address scopes_data_end       () const { return           header_begin() + _scopes_pcs_offset       ; }\n-  PcDesc* scopes_pcs_begin      () const { return (PcDesc*)(header_begin() + _scopes_pcs_offset)      ; }\n-  PcDesc* scopes_pcs_end        () const { return (PcDesc*)(header_begin() + _dependencies_offset)    ; }\n-  address dependencies_begin    () const { return           header_begin() + _dependencies_offset     ; }\n-  address dependencies_end      () const { return           header_begin() + _handler_table_offset    ; }\n-  address handler_table_begin   () const { return           header_begin() + _handler_table_offset    ; }\n-  address handler_table_end     () const { return           header_begin() + _nul_chk_table_offset    ; }\n-  address nul_chk_table_begin   () const { return           header_begin() + _nul_chk_table_offset    ; }\n+\n+  oop*    oops_begin            () const { return (oop*)    data_begin(); }\n+  oop*    oops_end              () const { return (oop*)   (data_begin() + _metadata_offset)          ; }\n+\n+  Metadata** metadata_begin     () const { return (Metadata**) (data_begin() + _metadata_offset)      ; }\n+  Metadata** metadata_end       () const { return (Metadata**) (data_begin() + _dependencies_offset)  ; }\n+\n+  address dependencies_begin    () const { return           data_begin() + _dependencies_offset       ; }\n+  address dependencies_end      () const { return           data_begin() + _scopes_pcs_offset         ; }\n+  PcDesc* scopes_pcs_begin      () const { return (PcDesc*)(data_begin() + _scopes_pcs_offset)        ; }\n+  PcDesc* scopes_pcs_end        () const { return (PcDesc*)(data_begin() + _scopes_data_offset)       ; }\n+  address scopes_data_begin     () const { return           data_begin() + _scopes_data_offset        ; }\n+  address scopes_data_end       () const { return           data_begin() + _handler_table_offset      ; }\n+  address handler_table_begin   () const { return           data_begin() + _handler_table_offset      ; }\n+  address handler_table_end     () const { return           data_begin() + _nul_chk_table_offset      ; }\n+  address nul_chk_table_begin   () const { return           data_begin() + _nul_chk_table_offset      ; }\n@@ -557,5 +561,5 @@\n-  address nul_chk_table_end     () const { return           header_begin() + _speculations_offset     ; }\n-  address speculations_begin    () const { return           header_begin() + _speculations_offset     ; }\n-  address speculations_end      () const { return           header_begin() + _jvmci_data_offset       ; }\n-  address jvmci_data_begin      () const { return           header_begin() + _jvmci_data_offset       ; }\n-  address jvmci_data_end        () const { return           header_begin() + _nmethod_end_offset      ; }\n+  address nul_chk_table_end     () const { return           data_begin() + _speculations_offset       ; }\n+  address speculations_begin    () const { return           data_begin() + _speculations_offset       ; }\n+  address speculations_end      () const { return           data_begin() + _jvmci_data_offset         ; }\n+  address jvmci_data_begin      () const { return           data_begin() + _jvmci_data_offset         ; }\n+  address jvmci_data_end        () const { return           data_end(); }\n@@ -563,1 +567,1 @@\n-  address nul_chk_table_end     () const { return           header_begin() + _nmethod_end_offset      ; }\n+  address nul_chk_table_end     () const { return           data_end(); }\n@@ -603,2 +607,2 @@\n-  address entry_point() const          { return _entry_point;          } \/\/ normal entry point\n-  address verified_entry_point() const { return _verified_entry_point; } \/\/ if klass is correct\n+  address entry_point() const          { return code_begin() + _entry_offset;          } \/\/ normal entry point\n+  address verified_entry_point() const { return code_begin() + _verified_entry_offset; } \/\/ if klass is correct\n@@ -627,3 +631,0 @@\n-  bool is_unlinked() const             { return _is_unlinked; }\n-  void set_is_unlinked()               { assert(!_is_unlinked, \"already unlinked\"); _is_unlinked = true; }\n-\n@@ -699,0 +700,6 @@\n+  bool  is_unlinked() const                       { return _is_unlinked; }\n+  void  set_is_unlinked()                         {\n+     assert(!_is_unlinked, \"already unlinked\");\n+      _is_unlinked = true;\n+  }\n+\n@@ -743,1 +750,0 @@\n-  void set_exception_cache(ExceptionCache *ec)    { _exception_cache = ec; }\n@@ -772,1 +778,1 @@\n-  void preserve_callee_argument_oops(frame fr, const RegisterMap *reg_map, OopClosure* f) override;\n+  void preserve_callee_argument_oops(frame fr, const RegisterMap *reg_map, OopClosure* f);\n@@ -808,5 +814,5 @@\n-  int   osr_entry_bci() const                     { assert(is_osr_method(), \"wrong kind of nmethod\"); return _entry_bci; }\n-  address  osr_entry() const                      { assert(is_osr_method(), \"wrong kind of nmethod\"); return _osr_entry_point; }\n-  void  invalidate_osr_method();\n-  nmethod* osr_link() const                       { return _osr_link; }\n-  void     set_osr_link(nmethod *n)               { _osr_link = n; }\n+  int      osr_entry_bci()    const { assert(is_osr_method(), \"wrong kind of nmethod\"); return _entry_bci; }\n+  address  osr_entry()        const { assert(is_osr_method(), \"wrong kind of nmethod\"); return _osr_entry_point; }\n+  nmethod* osr_link()         const { return _osr_link; }\n+  void     set_osr_link(nmethod *n) { _osr_link = n; }\n+  void     invalidate_osr_method();\n@@ -825,1 +831,1 @@\n-  void purge(bool free_code_cache_data, bool unregister_nmethod) override;\n+  void purge(bool unregister_nmethod);\n@@ -990,0 +996,1 @@\n+    assert(is_native_method(), \"sanity\");\n@@ -993,0 +1000,1 @@\n+    assert(is_native_method(), \"sanity\");\n@@ -997,3 +1005,2 @@\n-  static ByteSize verified_entry_point_offset() { return byte_offset_of(nmethod, _verified_entry_point); }\n-  static ByteSize osr_entry_point_offset()      { return byte_offset_of(nmethod, _osr_entry_point); }\n-  static ByteSize state_offset()                { return byte_offset_of(nmethod, _state); }\n+  static ByteSize osr_entry_point_offset() { return byte_offset_of(nmethod, _osr_entry_point); }\n+  static ByteSize state_offset()           { return byte_offset_of(nmethod, _state); }\n","filename":"src\/hotspot\/share\/code\/nmethod.hpp","additions":101,"deletions":94,"binary":false,"changes":195,"status":"modified"},{"patch":"@@ -1552,1 +1552,1 @@\n-  if (excluded || (CompilerOracle::has_option_value(method, CompileCommand::CompileThresholdScaling, scale) && scale == 0)) {\n+  if (excluded || (CompilerOracle::has_option_value(method, CompileCommandEnum::CompileThresholdScaling, scale) && scale == 0)) {\n@@ -1796,1 +1796,1 @@\n-    blob->purge(true \/* free_code_cache_data *\/, true \/* unregister_nmethod *\/);\n+    blob->purge();\n","filename":"src\/hotspot\/share\/compiler\/compileBroker.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -289,1 +289,1 @@\n-        CompileCommand::Unknown != CompilerOracle::parse_option_name(method_name)) &&\n+        CompileCommandEnum::Unknown != CompilerOracle::parse_option_name(method_name)) &&\n","filename":"src\/hotspot\/share\/compiler\/methodMatcher.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -214,2 +214,2 @@\n-  if (m.is_marked()) {\n-    obj = cast_to_oop(m.decode_pointer());\n+  if (m.is_forwarded()) {\n+    obj = m.forwardee();\n","filename":"src\/hotspot\/share\/gc\/g1\/g1ParScanThreadState.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -1277,4 +1277,0 @@\n-  if (ScavengeBeforeFullGC) {\n-    PSScavenge::invoke_no_policy();\n-  }\n-\n","filename":"src\/hotspot\/share\/gc\/parallel\/psParallelCompact.cpp","additions":0,"deletions":4,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -55,1 +55,1 @@\n-  virtual void invalidate(MemRegion mr) = 0;\n+  virtual void write_region(MemRegion mr) = 0;\n@@ -64,1 +64,1 @@\n-                                   bool dest_uninitialized = false) {}\n+                                   bool dest_uninitialized) {}\n@@ -66,1 +66,1 @@\n-                                   bool dest_uninitialized = false) {}\n+                                   bool dest_uninitialized) {}\n@@ -71,3 +71,0 @@\n- protected:\n-  virtual void write_ref_array_work(MemRegion mr) = 0;\n- public:\n","filename":"src\/hotspot\/share\/gc\/shared\/modRefBarrierSet.hpp","additions":3,"deletions":6,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -58,1 +58,1 @@\n-  write_ref_array_work(MemRegion(aligned_start, aligned_end));\n+  write_region(MemRegion(aligned_start, aligned_end));\n@@ -156,1 +156,1 @@\n-  bs->invalidate(MemRegion((HeapWord*)(void*)dst, size));\n+  bs->write_region(MemRegion((HeapWord*)(void*)dst, size));\n","filename":"src\/hotspot\/share\/gc\/shared\/modRefBarrierSet.inline.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -112,2 +112,0 @@\n-  static zaddress verify_old_object_live_slow_path(zaddress addr);\n-\n","filename":"src\/hotspot\/share\/gc\/z\/zBarrier.hpp","additions":0,"deletions":2,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -1149,1 +1149,1 @@\n-  pool->cache()->set_dynamic_call(info, pool->decode_invokedynamic_index(index));\n+  pool->cache()->set_dynamic_call(info, index);\n","filename":"src\/hotspot\/share\/interpreter\/interpreterRuntime.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -1786,2 +1786,1 @@\n-  int index = pool->decode_invokedynamic_index(indy_index);\n-  int pool_index = pool->resolved_indy_entry_at(index)->constant_pool_index();\n+  int pool_index = pool->resolved_indy_entry_at(indy_index)->constant_pool_index();\n@@ -1790,1 +1789,1 @@\n-  BootstrapInfo bootstrap_specifier(pool, pool_index, index);\n+  BootstrapInfo bootstrap_specifier(pool, pool_index, indy_index);\n","filename":"src\/hotspot\/share\/interpreter\/linkResolver.cpp","additions":2,"deletions":3,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -308,1 +308,1 @@\n-    Bytes::put_native_u4(p, ConstantPool::encode_invokedynamic_index(_invokedynamic_index));\n+    Bytes::put_native_u4(p, (u2)_invokedynamic_index);\n@@ -315,2 +315,1 @@\n-    int cache_index = ConstantPool::decode_invokedynamic_index(\n-                        Bytes::get_native_u4(p));\n+    int cache_index = Bytes::get_native_u4(p);\n@@ -534,1 +533,1 @@\n-  if (RegisterFinalizersAtInit && _klass->name() == vmSymbols::java_lang_Object()) {\n+  if (_klass->name() == vmSymbols::java_lang_Object()) {\n","filename":"src\/hotspot\/share\/interpreter\/rewriter.cpp","additions":3,"deletions":4,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -34,0 +34,1 @@\n+#include \"compiler\/compilerOracle.hpp\"\n@@ -912,1 +913,1 @@\n-C2V_VMENTRY_NULL(jobject, lookupAppendixInPool, (JNIEnv* env, jobject, ARGUMENT_PAIR(cp), jint which))\n+C2V_VMENTRY_NULL(jobject, lookupAppendixInPool, (JNIEnv* env, jobject, ARGUMENT_PAIR(cp), jint which, jint opcode))\n@@ -914,1 +915,1 @@\n-  oop appendix_oop = ConstantPool::appendix_at_if_loaded(cp, which);\n+  oop appendix_oop = ConstantPool::appendix_at_if_loaded(cp, which, Bytecodes::Code(opcode));\n@@ -1631,5 +1632,1 @@\n-C2V_VMENTRY_0(int, decodeIndyIndexToCPIndex, (JNIEnv* env, jobject, ARGUMENT_PAIR(cp), jint encoded_indy_index, jboolean resolve))\n-  if (!ConstantPool::is_invokedynamic_index(encoded_indy_index)) {\n-    JVMCI_THROW_MSG_0(IllegalStateException, err_msg(\"not an encoded indy index %d\", encoded_indy_index));\n-  }\n-\n+C2V_VMENTRY_0(int, decodeIndyIndexToCPIndex, (JNIEnv* env, jobject, ARGUMENT_PAIR(cp), jint indy_index, jboolean resolve))\n@@ -1638,2 +1635,1 @@\n-  int indy_index = cp->decode_invokedynamic_index(encoded_indy_index);\n-    LinkResolver::resolve_invoke(callInfo, Handle(), cp, encoded_indy_index, Bytecodes::_invokedynamic, CHECK_0);\n+    LinkResolver::resolve_invoke(callInfo, Handle(), cp, indy_index, Bytecodes::_invokedynamic, CHECK_0);\n@@ -1673,1 +1669,1 @@\n-C2V_VMENTRY_0(jint, isResolvedInvokeHandleInPool, (JNIEnv* env, jobject, ARGUMENT_PAIR(cp), jint index))\n+C2V_VMENTRY_0(jint, isResolvedInvokeHandleInPool, (JNIEnv* env, jobject, ARGUMENT_PAIR(cp), jint index, jint opcode))\n@@ -1707,2 +1703,2 @@\n-  if (cp->is_invokedynamic_index(index)) {\n-    if (cp->resolved_indy_entry_at(cp->decode_invokedynamic_index(index))->is_resolved()) {\n+  if ((Bytecodes::Code)opcode == Bytecodes::_invokedynamic) {\n+    if (cp->resolved_indy_entry_at(index)->is_resolved()) {\n@@ -3230,1 +3226,1 @@\n-  {CC \"lookupAppendixInPool\",                         CC \"(\" HS_CONSTANT_POOL2 \"I)\" OBJECTCONSTANT,                                         FN_PTR(lookupAppendixInPool)},\n+  {CC \"lookupAppendixInPool\",                         CC \"(\" HS_CONSTANT_POOL2 \"II)\" OBJECTCONSTANT,                                        FN_PTR(lookupAppendixInPool)},\n@@ -3242,1 +3238,1 @@\n-  {CC \"isResolvedInvokeHandleInPool\",                 CC \"(\" HS_CONSTANT_POOL2 \"I)I\",                                                       FN_PTR(isResolvedInvokeHandleInPool)},\n+  {CC \"isResolvedInvokeHandleInPool\",                 CC \"(\" HS_CONSTANT_POOL2 \"II)I\",                                                      FN_PTR(isResolvedInvokeHandleInPool)},\n","filename":"src\/hotspot\/share\/jvmci\/jvmciCompilerToVM.cpp","additions":10,"deletions":14,"binary":false,"changes":24,"status":"modified"},{"patch":"@@ -297,1 +297,1 @@\n-  nonstatic_field(nmethod,                     _verified_entry_point,                         address)                               \\\n+  nonstatic_field(nmethod,                     _verified_entry_offset,                        u2)                                    \\\n@@ -345,0 +345,1 @@\n+  static_field(StubRoutines,                _unsafe_setmemory,                                address)                               \\\n","filename":"src\/hotspot\/share\/jvmci\/vmStructs_jvmci.cpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -234,1 +234,1 @@\n-static BuiltinException _virtual_machine_error;\n+static BuiltinException _internal_error;\n@@ -251,1 +251,1 @@\n-oop Universe::virtual_machine_error_instance()    { return _virtual_machine_error.instance(); }\n+oop Universe::internal_error_instance()           { return _internal_error.instance(); }\n@@ -307,1 +307,1 @@\n-  _virtual_machine_error.store_in_cds();\n+  _internal_error.store_in_cds();\n@@ -323,1 +323,1 @@\n-    _virtual_machine_error.load_from_cds();\n+    _internal_error.load_from_cds();\n@@ -339,1 +339,1 @@\n-  _virtual_machine_error.serialize(f);\n+  _internal_error.serialize(f);\n@@ -1110,1 +1110,1 @@\n-  Klass* k = vmClasses::VirtualMachineError_klass();\n+  Klass* k = vmClasses::InternalError_klass();\n@@ -1113,1 +1113,1 @@\n-     tty->print_cr(\"Unable to link\/verify VirtualMachineError class\");\n+     tty->print_cr(\"Unable to link\/verify InternalError class\");\n@@ -1116,1 +1116,1 @@\n-  _virtual_machine_error.init_if_empty(vmSymbols::java_lang_VirtualMachineError(), CHECK_false);\n+  _internal_error.init_if_empty(vmSymbols::java_lang_InternalError(), CHECK_false);\n","filename":"src\/hotspot\/share\/memory\/universe.cpp","additions":8,"deletions":8,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -233,2 +233,2 @@\n-  static oop          virtual_machine_error_instance();\n-  static oop          vm_exception()                  { return virtual_machine_error_instance(); }\n+  static oop          internal_error_instance();\n+  static oop          vm_exception()                  { return internal_error_instance(); }\n","filename":"src\/hotspot\/share\/memory\/universe.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -682,1 +682,1 @@\n-bool ConstantPool::has_appendix_at_if_loaded(const constantPoolHandle& cpool, int which) {\n+bool ConstantPool::has_appendix_at_if_loaded(const constantPoolHandle& cpool, int which, Bytecodes::Code code) {\n@@ -684,3 +684,2 @@\n-  if (is_invokedynamic_index(which)) {\n-    int indy_index = decode_invokedynamic_index(which);\n-    return cpool->resolved_indy_entry_at(indy_index)->has_appendix();\n+  if (code == Bytecodes::_invokedynamic) {\n+    return cpool->resolved_indy_entry_at(which)->has_appendix();\n@@ -692,1 +691,1 @@\n-oop ConstantPool::appendix_at_if_loaded(const constantPoolHandle& cpool, int which) {\n+oop ConstantPool::appendix_at_if_loaded(const constantPoolHandle& cpool, int which, Bytecodes::Code code) {\n@@ -694,3 +693,2 @@\n-  if (is_invokedynamic_index(which)) {\n-    int indy_index = decode_invokedynamic_index(which);\n-    return cpool->resolved_reference_from_indy(indy_index);\n+  if (code == Bytecodes::_invokedynamic) {\n+    return cpool->resolved_reference_from_indy(which);\n@@ -703,1 +701,1 @@\n-bool ConstantPool::has_local_signature_at_if_loaded(const constantPoolHandle& cpool, int which) {\n+bool ConstantPool::has_local_signature_at_if_loaded(const constantPoolHandle& cpool, int which, Bytecodes::Code code) {\n@@ -705,3 +703,2 @@\n-  if (is_invokedynamic_index(which)) {\n-    int indy_index = decode_invokedynamic_index(which);\n-    return cpool->resolved_indy_entry_at(indy_index)->has_local_signature();\n+  if (code == Bytecodes::_invokedynamic) {\n+    return cpool->resolved_indy_entry_at(which)->has_local_signature();\n@@ -769,2 +766,0 @@\n-  guarantee(!ConstantPool::is_invokedynamic_index(index),\n-            \"an invokedynamic instruction does not have a klass\");\n","filename":"src\/hotspot\/share\/oops\/constantPool.cpp","additions":9,"deletions":14,"binary":false,"changes":23,"status":"modified"},{"patch":"@@ -251,8 +251,0 @@\n-  \/\/ Invokedynamic indexes.\n-  \/\/ They must look completely different from normal indexes.\n-  \/\/ The main reason is that byte swapping is sometimes done on normal indexes.\n-  \/\/ Finally, it is helpful for debugging to tell the two apart.\n-  static bool is_invokedynamic_index(int i) { return (i < 0); }\n-  static int  decode_invokedynamic_index(int i) { assert(is_invokedynamic_index(i),  \"\"); return ~i; }\n-  static int  encode_invokedynamic_index(int i) { assert(!is_invokedynamic_index(i), \"\"); return ~i; }\n-\n@@ -764,3 +756,3 @@\n-  static bool       has_appendix_at_if_loaded      (const constantPoolHandle& this_cp, int which);\n-  static oop            appendix_at_if_loaded      (const constantPoolHandle& this_cp, int which);\n-  static bool has_local_signature_at_if_loaded     (const constantPoolHandle& this_cp, int which);\n+  static bool       has_appendix_at_if_loaded      (const constantPoolHandle& this_cp, int which, Bytecodes::Code code);\n+  static oop            appendix_at_if_loaded      (const constantPoolHandle& this_cp, int which, Bytecodes::Code code);\n+  static bool has_local_signature_at_if_loaded     (const constantPoolHandle& this_cp, int which, Bytecodes::Code code);\n","filename":"src\/hotspot\/share\/oops\/constantPool.hpp","additions":3,"deletions":11,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -574,2 +574,1 @@\n-  int encoded_index = ResolutionErrorTable::encode_indy_index(\n-                          ConstantPool::encode_invokedynamic_index(index));\n+  int encoded_index = ResolutionErrorTable::encode_indy_index(index);\n@@ -594,2 +593,1 @@\n-    int encoded_index = ResolutionErrorTable::encode_indy_index(\n-                          ConstantPool::encode_invokedynamic_index(index));\n+    int encoded_index = ResolutionErrorTable::encode_indy_index(index);\n","filename":"src\/hotspot\/share\/oops\/cpCache.cpp","additions":2,"deletions":4,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -1736,1 +1736,1 @@\n-  bool has_finalizer_flag = has_finalizer(); \/\/ Query before possible GC\n+  assert(!is_abstract() && !is_interface(), \"Should not create this object\");\n@@ -1738,8 +1738,1 @@\n-\n-  instanceOop i;\n-\n-  i = (instanceOop)Universe::heap()->obj_allocate(this, size, CHECK_NULL);\n-  if (has_finalizer_flag && !RegisterFinalizersAtInit) {\n-    i = register_finalizer(i, CHECK_NULL);\n-  }\n-  return i;\n+  return (instanceOop)Universe::heap()->obj_allocate(this, size, CHECK_NULL);\n","filename":"src\/hotspot\/share\/oops\/instanceKlass.cpp","additions":2,"deletions":9,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -1042,1 +1042,0 @@\n-  \/\/  - the class has a finalizer (if !RegisterFinalizersAtInit)\n","filename":"src\/hotspot\/share\/oops\/instanceKlass.hpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -730,1 +730,0 @@\n-  it->push(&_secondary_super_cache);\n@@ -761,0 +760,5 @@\n+  \/\/ _secondary_super_cache may be updated by an is_subtype_of() call\n+  \/\/ while ArchiveBuilder is copying metaspace objects. Let's reset it to\n+  \/\/ null and let it be repopulated at runtime.\n+  set_secondary_super_cache(nullptr);\n+\n","filename":"src\/hotspot\/share\/oops\/klass.cpp","additions":5,"deletions":1,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -258,0 +258,4 @@\n+  bool is_forwarded()   const {\n+    return (mask_bits(value(), lock_mask_in_place) == marked_value);\n+  }\n+\n@@ -336,1 +340,1 @@\n-  markWord clear_lock_bits() { return markWord(value() & ~lock_mask_in_place); }\n+  markWord clear_lock_bits() const { return markWord(value() & ~lock_mask_in_place); }\n@@ -414,1 +418,1 @@\n-  inline void* decode_pointer() {\n+  inline void* decode_pointer() const {\n@@ -418,0 +422,4 @@\n+\n+  inline oop forwardee() const {\n+    return cast_to_oop(decode_pointer());\n+  }\n","filename":"src\/hotspot\/share\/oops\/markWord.hpp","additions":10,"deletions":2,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -1323,1 +1323,1 @@\n-      THROW_MSG_NULL(vmSymbols::java_lang_VirtualMachineError(), \"Out of space in CodeCache for adapters\");\n+      THROW_MSG_NULL(vmSymbols::java_lang_OutOfMemoryError(), \"Out of space in CodeCache for adapters\");\n","filename":"src\/hotspot\/share\/oops\/method.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -1389,1 +1389,1 @@\n-  CompilerOracle::has_option_value(mh, CompileCommand::CompileThresholdScaling, scale);\n+  CompilerOracle::has_option_value(mh, CompileCommandEnum::CompileThresholdScaling, scale);\n@@ -1406,2 +1406,2 @@\n-      !CompilerOracle::has_option(mh, CompileCommand::NoRTMLockEliding)) {\n-    if (CompilerOracle::has_option(mh, CompileCommand::UseRTMLockEliding) || !UseRTMDeopt) {\n+      !CompilerOracle::has_option(mh, CompileCommandEnum::NoRTMLockEliding)) {\n+    if (CompilerOracle::has_option(mh, CompileCommandEnum::UseRTMLockEliding) || !UseRTMDeopt) {\n","filename":"src\/hotspot\/share\/oops\/methodData.cpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -280,3 +280,1 @@\n-  \/\/ The extra heap check is needed since the obj might be locked, in which case the\n-  \/\/ mark would point to a stack location and have the sentinel bit cleared\n-  return mark().is_marked();\n+  return mark().is_forwarded();\n@@ -307,2 +305,1 @@\n-  assert(is_forwarded(), \"only decode when actually forwarded\");\n-  return cast_to_oop(mark().decode_pointer());\n+  return mark().forwardee();\n","filename":"src\/hotspot\/share\/oops\/oop.inline.hpp","additions":2,"deletions":5,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -739,1 +739,1 @@\n-int AddPNode::unpack_offsets(Node* elements[], int length) {\n+int AddPNode::unpack_offsets(Node* elements[], int length) const {\n@@ -741,1 +741,1 @@\n-  Node* addr = this;\n+  Node const* addr = this;\n@@ -1439,0 +1439,8 @@\n+Node* MaxNode::Identity(PhaseGVN* phase) {\n+  if (in(1) == in(2)) {\n+      return in(1);\n+  }\n+\n+  return AddNode::Identity(phase);\n+}\n+\n","filename":"src\/hotspot\/share\/opto\/addnode.cpp","additions":10,"deletions":2,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -356,0 +356,6 @@\n+  product(bool, MergeStores, true, DIAGNOSTIC,                              \\\n+          \"Optimize stores by combining values into larger store\")          \\\n+                                                                            \\\n+  develop(bool, TraceMergeStores, false,                                    \\\n+          \"Trace creation of merged stores\")                                \\\n+                                                                            \\\n@@ -797,0 +803,4 @@\n+                                                                            \\\n+  product(bool, UseStoreStoreForCtor, true, DIAGNOSTIC,                     \\\n+          \"Use StoreStore barrier instead of Release barrier at the end \"   \\\n+          \"of constructors\")                                                \\\n","filename":"src\/hotspot\/share\/opto\/c2_globals.hpp","additions":10,"deletions":0,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -261,0 +261,3 @@\n+  case vmIntrinsics::_setMemory:\n+    if (StubRoutines::unsafe_setmemory() == nullptr) return false;\n+    break;\n","filename":"src\/hotspot\/share\/opto\/c2compiler.cpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -36,0 +36,1 @@\n+#include \"compiler\/compilerOracle.hpp\"\n@@ -639,0 +640,1 @@\n+                  _allow_macro_nodes(true),\n@@ -940,0 +942,1 @@\n+    _allow_macro_nodes(true),\n@@ -957,0 +960,1 @@\n+    _for_post_loop_igvn(comp_arena(), 8, 0, nullptr),\n@@ -1113,1 +1117,1 @@\n-    if (method_has_option(CompileCommand::NoRTMLockEliding) || ((rtm_state & NoRTM) != 0)) {\n+    if (method_has_option(CompileCommandEnum::NoRTMLockEliding) || ((rtm_state & NoRTM) != 0)) {\n@@ -1116,1 +1120,1 @@\n-    } else if (method_has_option(CompileCommand::UseRTMLockEliding) || ((rtm_state & UseRTM) != 0) || !UseRTMDeopt) {\n+    } else if (method_has_option(CompileCommandEnum::UseRTMLockEliding) || ((rtm_state & UseRTM) != 0) || !UseRTMDeopt) {\n","filename":"src\/hotspot\/share\/opto\/compile.cpp","additions":6,"deletions":2,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -32,1 +32,0 @@\n-#include \"compiler\/compilerOracle.hpp\"\n@@ -49,0 +48,1 @@\n+#include \"utilities\/vmEnums.hpp\"\n@@ -323,0 +323,1 @@\n+  bool                  _allow_macro_nodes;     \/\/ True if we allow creation of macro nodes.\n@@ -700,1 +701,1 @@\n-  bool          method_has_option(enum CompileCommand option) {\n+  bool          method_has_option(CompileCommandEnum option) {\n@@ -809,0 +810,3 @@\n+  bool       allow_macro_nodes() { return _allow_macro_nodes;  }\n+  void reset_allow_macro_nodes() { _allow_macro_nodes = false;  }\n+\n","filename":"src\/hotspot\/share\/opto\/compile.hpp","additions":6,"deletions":2,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -213,1 +213,3 @@\n-        storestore_worklist.append(n->as_MemBarStoreStore());\n+        if (!UseStoreStoreForCtor || n->req() > MemBarNode::Precedent) {\n+          storestore_worklist.append(n->as_MemBarStoreStore());\n+        }\n@@ -504,0 +506,1 @@\n+  assert(cmp->Opcode() == Op_CmpP || cmp->Opcode() == Op_CmpN, \"not expected node: %s\", cmp->Name());\n@@ -507,2 +510,1 @@\n-  return (cmp->Opcode() == Op_CmpP || cmp->Opcode() == Op_CmpN) &&\n-         (left == n || right == n) &&\n+  return (left == n || right == n) &&\n@@ -572,2 +574,6 @@\n-        NOT_PRODUCT(use->dump();)\n-        NOT_PRODUCT(if (TraceReduceAllocationMerges) tty->print_cr(\"Can NOT reduce Phi %d on invocation %d. CastPP is not to an instance.\", n->_idx, _invocation);)\n+#ifndef PRODUCT\n+        if (TraceReduceAllocationMerges) {\n+          tty->print_cr(\"Can NOT reduce Phi %d on invocation %d. CastPP is not to an instance.\", n->_idx, _invocation);\n+          use->dump();\n+        }\n+#endif\n@@ -583,5 +589,12 @@\n-          Node* iff_cmp = iff->in(1)->in(1); \/\/ if->bool->cmp\n-          if (!can_reduce_cmp(n, iff_cmp)) {\n-            NOT_PRODUCT(if (TraceReduceAllocationMerges) tty->print_cr(\"Can NOT reduce Phi %d on invocation %d. CastPP %d doesn't have simple control.\", n->_idx, _invocation, use->_idx);)\n-            NOT_PRODUCT(n->dump(5);)\n-            return false;\n+          if (iff->Opcode() == Op_If && iff->in(1)->is_Bool() && iff->in(1)->in(1)->is_Cmp()) {\n+            Node* iff_cmp = iff->in(1)->in(1);\n+            int opc = iff_cmp->Opcode();\n+            if ((opc == Op_CmpP || opc == Op_CmpN) && !can_reduce_cmp(n, iff_cmp)) {\n+#ifndef PRODUCT\n+              if (TraceReduceAllocationMerges) {\n+                tty->print_cr(\"Can NOT reduce Phi %d on invocation %d. CastPP %d doesn't have simple control.\", n->_idx, _invocation, use->_idx);\n+                n->dump(5);\n+              }\n+#endif\n+              return false;\n+            }\n@@ -712,3 +725,2 @@\n-  Node* minus_one          = _igvn->transform(ConINode::make(-1));\n-  Node* boll               = _igvn->transform(new BoolNode(cmp, BoolTest::ne));\n-  IfNode* if_ne            = _igvn->transform(new IfNode(current_control, boll, PROB_MIN, COUNT_UNKNOWN))->as_If();\n+  Node* bol                = _igvn->transform(new BoolNode(cmp, BoolTest::ne));\n+  IfNode* if_ne            = _igvn->transform(new IfNode(current_control, bol, PROB_MIN, COUNT_UNKNOWN))->as_If();\n@@ -979,2 +991,2 @@\n-      Node* boll = _igvn->transform(new BoolNode(ncmp, mask));\n-      res_phi_input = boll->as_Bool()->as_int_value(_igvn);\n+      Node* bol = _igvn->transform(new BoolNode(ncmp, mask));\n+      res_phi_input = bol->as_Bool()->as_int_value(_igvn);\n","filename":"src\/hotspot\/share\/opto\/escape.cpp","additions":27,"deletions":15,"binary":false,"changes":42,"status":"modified"},{"patch":"@@ -516,0 +516,1 @@\n+  case vmIntrinsics::_setMemory:                return inline_unsafe_setMemory();\n@@ -5264,0 +5265,51 @@\n+\n+  return true;\n+}\n+\n+\/\/ unsafe_setmemory(void *base, ulong offset, size_t length, char fill_value);\n+\/\/ Fill 'length' bytes starting from 'base[offset]' with 'fill_value'\n+bool LibraryCallKit::inline_unsafe_setMemory() {\n+  if (callee()->is_static())  return false;  \/\/ caller must have the capability!\n+  null_check_receiver();  \/\/ null-check receiver\n+  if (stopped())  return true;\n+\n+  C->set_has_unsafe_access(true);  \/\/ Mark eventual nmethod as \"unsafe\".\n+\n+  Node* dst_base =         argument(1);  \/\/ type: oop\n+  Node* dst_off  = ConvL2X(argument(2)); \/\/ type: long\n+  Node* size     = ConvL2X(argument(4)); \/\/ type: long\n+  Node* byte     =         argument(6);  \/\/ type: byte\n+\n+  assert(Unsafe_field_offset_to_byte_offset(11) == 11,\n+         \"fieldOffset must be byte-scaled\");\n+\n+  Node* dst_addr = make_unsafe_address(dst_base, dst_off);\n+\n+  Node* thread = _gvn.transform(new ThreadLocalNode());\n+  Node* doing_unsafe_access_addr = basic_plus_adr(top(), thread, in_bytes(JavaThread::doing_unsafe_access_offset()));\n+  BasicType doing_unsafe_access_bt = T_BYTE;\n+  assert((sizeof(bool) * CHAR_BIT) == 8, \"not implemented\");\n+\n+  \/\/ update volatile field\n+  store_to_memory(control(), doing_unsafe_access_addr, intcon(1), doing_unsafe_access_bt, Compile::AliasIdxRaw, MemNode::unordered);\n+\n+  int flags = RC_LEAF | RC_NO_FP;\n+\n+  const TypePtr* dst_type = TypePtr::BOTTOM;\n+\n+  \/\/ Adjust memory effects of the runtime call based on input values.\n+  if (!has_wide_mem(_gvn, dst_addr, dst_base)) {\n+    dst_type = _gvn.type(dst_addr)->is_ptr(); \/\/ narrow out memory\n+\n+    flags |= RC_NARROW_MEM; \/\/ narrow in memory\n+  }\n+\n+  \/\/ Call it.  Note that the length argument is not scaled.\n+  make_runtime_call(flags,\n+                    OptoRuntime::make_setmemory_Type(),\n+                    StubRoutines::unsafe_setmemory(),\n+                    \"unsafe_setmemory\",\n+                    dst_type,\n+                    dst_addr, size XTOP, byte);\n+\n+  store_to_memory(control(), doing_unsafe_access_addr, intcon(0), doing_unsafe_access_bt, Compile::AliasIdxRaw, MemNode::unordered);\n","filename":"src\/hotspot\/share\/opto\/library_call.cpp","additions":52,"deletions":0,"binary":false,"changes":52,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2020, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2020, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -258,0 +258,1 @@\n+  bool inline_unsafe_setMemory();\n","filename":"src\/hotspot\/share\/opto\/library_call.hpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -959,1 +959,0 @@\n-  static bool subgraph_has_opaque(Node* n);\n@@ -1771,6 +1770,0 @@\n-  bool clone_cmp_down(Node* n, const Node* blk1, const Node* blk2);\n-\n-  void clone_loadklass_nodes_at_cmp_index(const Node* n, Node* cmp, int i);\n-\n-  bool clone_cmp_loadklass_down(Node* n, const Node* blk1, const Node* blk2);\n-\n@@ -1779,0 +1772,4 @@\n+  bool clone_cmp_loadklass_down(Node* n, const Node* blk1, const Node* blk2);\n+  void clone_loadklass_nodes_at_cmp_index(const Node* n, Node* cmp, int i);\n+  bool clone_cmp_down(Node* n, const Node* blk1, const Node* blk2);\n+  void clone_template_assertion_predicate_expression_down(Node* node);\n","filename":"src\/hotspot\/share\/opto\/loopnode.hpp","additions":4,"deletions":7,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -567,0 +567,5 @@\n+  if (barrier_data() != 0) {\n+    st->print(\" barrier(\");\n+    BarrierSet::barrier_set()->barrier_set_c2()->dump_barrier_data(this, st);\n+    st->print(\") \");\n+  }\n@@ -579,0 +584,1 @@\n+  MachNode::dump_spec(st);\n@@ -584,5 +590,0 @@\n-  if (barrier_data() != 0) {\n-    st->print(\" barrier(\");\n-    BarrierSet::barrier_set()->barrier_set_c2()->dump_barrier_data(this, st);\n-    st->print(\")\");\n-  }\n","filename":"src\/hotspot\/share\/opto\/machnode.cpp","additions":6,"deletions":5,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -717,1 +717,1 @@\n-      } else if (res_type->is_inlinetypeptr() && use->Opcode() == Op_MemBarRelease) {\n+      } else if (res_type->is_inlinetypeptr() && (use->Opcode() == Op_MemBarRelease || use->Opcode() == Op_MemBarStoreStore)) {\n@@ -719,1 +719,4 @@\n-      } else if (reduce_merge_precheck && (use->is_Phi() || use->is_EncodeP() || use->Opcode() == Op_MemBarRelease)) {\n+      } else if (reduce_merge_precheck &&\n+                 (use->is_Phi() || use->is_EncodeP() ||\n+                  use->Opcode() == Op_MemBarRelease ||\n+                  (UseStoreStoreForCtor && use->Opcode() == Op_MemBarStoreStore))) {\n@@ -1127,1 +1130,1 @@\n-      } else if (use->Opcode() == Op_MemBarRelease) {\n+      } else if (use->Opcode() == Op_MemBarRelease || use->Opcode() == Op_MemBarStoreStore) {\n@@ -2957,0 +2960,2 @@\n+  \/\/ Do not allow new macro nodes once we started to expand\n+  C->reset_allow_macro_nodes();\n","filename":"src\/hotspot\/share\/opto\/macro.cpp","additions":8,"deletions":3,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -2525,0 +2525,7 @@\n+    case Op_LoadVectorGather:\n+      if (is_subword_type(n->bottom_type()->is_vect()->element_basic_type())) {\n+        Node* pair = new BinaryNode(n->in(MemNode::ValueIn), n->in(MemNode::ValueIn+1));\n+        n->set_req(MemNode::ValueIn, pair);\n+        n->del_req(MemNode::ValueIn+1);\n+      }\n+      break;\n@@ -2526,0 +2533,8 @@\n+      if (is_subword_type(n->bottom_type()->is_vect()->element_basic_type())) {\n+        Node* pair2 = new BinaryNode(n->in(MemNode::ValueIn + 1), n->in(MemNode::ValueIn + 2));\n+        Node* pair1 = new BinaryNode(n->in(MemNode::ValueIn), pair2);\n+        n->set_req(MemNode::ValueIn, pair1);\n+        n->del_req(MemNode::ValueIn+2);\n+        n->del_req(MemNode::ValueIn+1);\n+        break;\n+      } \/\/ fall-through\n","filename":"src\/hotspot\/share\/opto\/matcher.cpp","additions":15,"deletions":0,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -2750,0 +2750,686 @@\n+\/\/ Class to parse array pointers, and determine if they are adjacent. We parse the form:\n+\/\/\n+\/\/   pointer =   base\n+\/\/             + constant_offset\n+\/\/             + LShiftL( ConvI2L(int_offset + int_con), int_offset_shift)\n+\/\/             + sum(other_offsets)\n+\/\/\n+\/\/\n+\/\/ Note: we accumulate all constant offsets into constant_offset, even the int constant behind\n+\/\/       the \"LShiftL(ConvI2L(...))\" pattern. We convert \"ConvI2L(int_offset + int_con)\" to\n+\/\/       \"ConvI2L(int_offset) + int_con\", which is only safe if we can assume that either all\n+\/\/       compared addresses have an overflow for \"int_offset + int_con\" or none.\n+\/\/       For loads and stores on arrays, we know that if one overflows and the other not, then\n+\/\/       the two addresses lay almost max_int indices apart, but the maximal array size is\n+\/\/       only about half of that. Therefore, the RangeCheck on at least one of them must have\n+\/\/       failed.\n+\/\/\n+\/\/   constant_offset += LShiftL( ConvI2L(int_con), int_offset_shift)\n+\/\/\n+\/\/   pointer =   base\n+\/\/             + constant_offset\n+\/\/             + LShiftL( ConvI2L(int_offset), int_offset_shift)\n+\/\/             + sum(other_offsets)\n+\/\/\n+class ArrayPointer {\n+private:\n+  const bool _is_valid;          \/\/ The parsing succeeded\n+  const Node* _pointer;          \/\/ The final pointer to the position in the array\n+  const Node* _base;             \/\/ Base address of the array\n+  const jlong _constant_offset;  \/\/ Sum of collected constant offsets\n+  const Node* _int_offset;       \/\/ (optional) Offset behind LShiftL and ConvI2L\n+  const jint  _int_offset_shift; \/\/ (optional) Shift value for int_offset\n+  const GrowableArray<Node*>* _other_offsets; \/\/ List of other AddP offsets\n+\n+  ArrayPointer(const bool is_valid,\n+               const Node* pointer,\n+               const Node* base,\n+               const jlong constant_offset,\n+               const Node* int_offset,\n+               const jint int_offset_shift,\n+               const GrowableArray<Node*>* other_offsets) :\n+      _is_valid(is_valid),\n+      _pointer(pointer),\n+      _base(base),\n+      _constant_offset(constant_offset),\n+      _int_offset(int_offset),\n+      _int_offset_shift(int_offset_shift),\n+      _other_offsets(other_offsets)\n+  {\n+    assert(_pointer != nullptr, \"must always have pointer\");\n+    assert(is_valid == (_base != nullptr), \"have base exactly if valid\");\n+    assert(is_valid == (_other_offsets != nullptr), \"have other_offsets exactly if valid\");\n+  }\n+\n+  static ArrayPointer make_invalid(const Node* pointer) {\n+    return ArrayPointer(false, pointer, nullptr, 0, nullptr, 0, nullptr);\n+  }\n+\n+  static bool parse_int_offset(Node* offset, Node*& int_offset, jint& int_offset_shift) {\n+    \/\/ offset = LShiftL( ConvI2L(int_offset), int_offset_shift)\n+    if (offset->Opcode() == Op_LShiftL &&\n+        offset->in(1)->Opcode() == Op_ConvI2L &&\n+        offset->in(2)->Opcode() == Op_ConI) {\n+      int_offset = offset->in(1)->in(1); \/\/ LShiftL -> ConvI2L -> int_offset\n+      int_offset_shift = offset->in(2)->get_int(); \/\/ LShiftL -> int_offset_shift\n+      return true;\n+    }\n+\n+    \/\/ offset = ConvI2L(int_offset) = LShiftL( ConvI2L(int_offset), 0)\n+    if (offset->Opcode() == Op_ConvI2L) {\n+      int_offset = offset->in(1);\n+      int_offset_shift = 0;\n+      return true;\n+    }\n+\n+    \/\/ parse failed\n+    return false;\n+  }\n+\n+public:\n+  \/\/ Parse the structure above the pointer\n+  static ArrayPointer make(PhaseGVN* phase, const Node* pointer) {\n+    assert(phase->type(pointer)->isa_aryptr() != nullptr, \"must be array pointer\");\n+    if (!pointer->is_AddP()) { return ArrayPointer::make_invalid(pointer); }\n+\n+    const Node* base = pointer->in(AddPNode::Base);\n+    if (base == nullptr) { return ArrayPointer::make_invalid(pointer); }\n+\n+    const int search_depth = 5;\n+    Node* offsets[search_depth];\n+    int count = pointer->as_AddP()->unpack_offsets(offsets, search_depth);\n+\n+    \/\/ We expect at least a constant each\n+    if (count <= 0) { return ArrayPointer::make_invalid(pointer); }\n+\n+    \/\/ We extract the form:\n+    \/\/\n+    \/\/   pointer =   base\n+    \/\/             + constant_offset\n+    \/\/             + LShiftL( ConvI2L(int_offset + int_con), int_offset_shift)\n+    \/\/             + sum(other_offsets)\n+    \/\/\n+    jlong constant_offset = 0;\n+    Node* int_offset = nullptr;\n+    jint int_offset_shift = 0;\n+    GrowableArray<Node*>* other_offsets = new GrowableArray<Node*>(count);\n+\n+    for (int i = 0; i < count; i++) {\n+      Node* offset = offsets[i];\n+      if (offset->Opcode() == Op_ConI) {\n+        \/\/ Constant int offset\n+        constant_offset += offset->get_int();\n+      } else if (offset->Opcode() == Op_ConL) {\n+        \/\/ Constant long offset\n+        constant_offset += offset->get_long();\n+      } else if(int_offset == nullptr && parse_int_offset(offset, int_offset, int_offset_shift)) {\n+        \/\/ LShiftL( ConvI2L(int_offset), int_offset_shift)\n+        int_offset = int_offset->uncast();\n+        if (int_offset->Opcode() == Op_AddI && int_offset->in(2)->Opcode() == Op_ConI) {\n+          \/\/ LShiftL( ConvI2L(int_offset + int_con), int_offset_shift)\n+          constant_offset += ((jlong)int_offset->in(2)->get_int()) << int_offset_shift;\n+          int_offset = int_offset->in(1);\n+        }\n+      } else {\n+        \/\/ All others\n+        other_offsets->append(offset);\n+      }\n+    }\n+\n+    return ArrayPointer(true, pointer, base, constant_offset, int_offset, int_offset_shift, other_offsets);\n+  }\n+\n+  bool is_adjacent_to_and_before(const ArrayPointer& other, const jlong data_size) const {\n+    if (!_is_valid || !other._is_valid) { return false; }\n+\n+    \/\/ Offset adjacent?\n+    if (this->_constant_offset + data_size != other._constant_offset) { return false; }\n+\n+    \/\/ All other components identical?\n+    if (this->_base != other._base ||\n+        this->_int_offset != other._int_offset ||\n+        this->_int_offset_shift != other._int_offset_shift ||\n+        this->_other_offsets->length() != other._other_offsets->length()) {\n+      return false;\n+    }\n+\n+    for (int i = 0; i < this->_other_offsets->length(); i++) {\n+      Node* o1 = this->_other_offsets->at(i);\n+      Node* o2 = other._other_offsets->at(i);\n+      if (o1 != o2) { return false; }\n+    }\n+\n+    return true;\n+  }\n+\n+#ifndef PRODUCT\n+  void dump() {\n+    if (!_is_valid) {\n+      tty->print(\"ArrayPointer[%d %s, invalid]\", _pointer->_idx, _pointer->Name());\n+      return;\n+    }\n+    tty->print(\"ArrayPointer[%d %s, base[%d %s] + %lld\",\n+               _pointer->_idx, _pointer->Name(),\n+               _base->_idx, _base->Name(),\n+               (long long)_constant_offset);\n+    if (_int_offset != 0) {\n+      tty->print(\" + I2L[%d %s] << %d\",\n+                 _int_offset->_idx, _int_offset->Name(), _int_offset_shift);\n+    }\n+    for (int i = 0; i < _other_offsets->length(); i++) {\n+      Node* n = _other_offsets->at(i);\n+      tty->print(\" + [%d %s]\", n->_idx, n->Name());\n+    }\n+    tty->print_cr(\"]\");\n+  }\n+#endif\n+};\n+\n+\/\/ Link together multiple stores (B\/S\/C\/I) into a longer one.\n+\/\/\n+\/\/ Example: _store = StoreB[i+3]\n+\/\/\n+\/\/   RangeCheck[i+0]           RangeCheck[i+0]\n+\/\/   StoreB[i+0]\n+\/\/   RangeCheck[i+1]           RangeCheck[i+1]\n+\/\/   StoreB[i+1]         -->   pass:             fail:\n+\/\/   StoreB[i+2]               StoreI[i+0]       StoreB[i+0]\n+\/\/   StoreB[i+3]\n+\/\/\n+\/\/ The 4 StoreB are merged into a single StoreI node. We have to be careful with RangeCheck[i+1]: before\n+\/\/ the optimization, if this RangeCheck[i+1] fails, then we execute only StoreB[i+0], and then trap. After\n+\/\/ the optimization, the new StoreI[i+0] is on the passing path of RangeCheck[i+1], and StoreB[i+0] on the\n+\/\/ failing path.\n+\/\/\n+\/\/ Note: For normal array stores, every store at first has a RangeCheck. But they can be removed with:\n+\/\/       - RCE (RangeCheck Elimination): the RangeChecks in the loop are hoisted out and before the loop,\n+\/\/                                       and possibly no RangeChecks remain between the stores.\n+\/\/       - RangeCheck smearing: the earlier RangeChecks are adjusted such that they cover later RangeChecks,\n+\/\/                              and those later RangeChecks can be removed. Example:\n+\/\/\n+\/\/                              RangeCheck[i+0]                         RangeCheck[i+0] <- before first store\n+\/\/                              StoreB[i+0]                             StoreB[i+0]     <- first store\n+\/\/                              RangeCheck[i+1]     --> smeared -->     RangeCheck[i+3] <- only RC between first and last store\n+\/\/                              StoreB[i+0]                             StoreB[i+1]     <- second store\n+\/\/                              RangeCheck[i+2]     --> removed\n+\/\/                              StoreB[i+0]                             StoreB[i+2]\n+\/\/                              RangeCheck[i+3]     --> removed\n+\/\/                              StoreB[i+0]                             StoreB[i+3]     <- last store\n+\/\/\n+\/\/                              Thus, it is a common pattern that between the first and last store in a chain\n+\/\/                              of adjacent stores there remains exactly one RangeCheck, located between the\n+\/\/                              first and the second store (e.g. RangeCheck[i+3]).\n+\/\/\n+class MergePrimitiveArrayStores : public StackObj {\n+private:\n+  PhaseGVN* _phase;\n+  StoreNode* _store;\n+\n+public:\n+  MergePrimitiveArrayStores(PhaseGVN* phase, StoreNode* store) : _phase(phase), _store(store) {}\n+\n+  StoreNode* run();\n+\n+private:\n+  bool is_compatible_store(const StoreNode* other_store) const;\n+  bool is_adjacent_pair(const StoreNode* use_store, const StoreNode* def_store) const;\n+  bool is_adjacent_input_pair(const Node* n1, const Node* n2, const int memory_size) const;\n+  static bool is_con_RShift(const Node* n, Node const*& base_out, jint& shift_out);\n+  enum CFGStatus { SuccessNoRangeCheck, SuccessWithRangeCheck, Failure };\n+  static CFGStatus cfg_status_for_pair(const StoreNode* use_store, const StoreNode* def_store);\n+\n+  class Status {\n+  private:\n+    StoreNode* _found_store;\n+    bool       _found_range_check;\n+\n+    Status(StoreNode* found_store, bool found_range_check)\n+      : _found_store(found_store), _found_range_check(found_range_check) {}\n+\n+  public:\n+    StoreNode* found_store() const { return _found_store; }\n+    bool found_range_check() const { return _found_range_check; }\n+    static Status make_failure() { return Status(nullptr, false); }\n+\n+    static Status make(StoreNode* found_store, const CFGStatus cfg_status) {\n+      if (cfg_status == CFGStatus::Failure) {\n+        return Status::make_failure();\n+      }\n+      return Status(found_store, cfg_status == CFGStatus::SuccessWithRangeCheck);\n+    }\n+  };\n+\n+  Status find_adjacent_use_store(const StoreNode* def_store) const;\n+  Status find_adjacent_def_store(const StoreNode* use_store) const;\n+  Status find_use_store(const StoreNode* def_store) const;\n+  Status find_def_store(const StoreNode* use_store) const;\n+  Status find_use_store_unidirectional(const StoreNode* def_store) const;\n+  Status find_def_store_unidirectional(const StoreNode* use_store) const;\n+\n+  void collect_merge_list(Node_List& merge_list) const;\n+  Node* make_merged_input_value(const Node_List& merge_list);\n+  StoreNode* make_merged_store(const Node_List& merge_list, Node* merged_input_value);\n+\n+  DEBUG_ONLY( void trace(const Node_List& merge_list, const Node* merged_input_value, const StoreNode* merged_store) const; )\n+};\n+\n+StoreNode* MergePrimitiveArrayStores::run() {\n+  \/\/ Check for B\/S\/C\/I\n+  int opc = _store->Opcode();\n+  if (opc != Op_StoreB && opc != Op_StoreC && opc != Op_StoreI) {\n+    return nullptr;\n+  }\n+\n+  \/\/ Only merge stores on arrays, and the stores must have the same size as the elements.\n+  const TypeAryPtr* aryptr_t = _store->adr_type()->isa_aryptr();\n+  if (aryptr_t == nullptr) {\n+    return nullptr;\n+  }\n+  BasicType bt = aryptr_t->elem()->array_element_basic_type();\n+  if (!is_java_primitive(bt) ||\n+      type2aelembytes(bt) != _store->memory_size()) {\n+    return nullptr;\n+  }\n+\n+  \/\/ The _store must be the \"last\" store in a chain. If we find a use we could merge with\n+  \/\/ then that use or a store further down is the \"last\" store.\n+  Status status_use = find_adjacent_use_store(_store);\n+  if (status_use.found_store() != nullptr) {\n+    return nullptr;\n+  }\n+\n+  \/\/ Check if we can merge with at least one def, so that we have at least 2 stores to merge.\n+  Status status_def = find_adjacent_def_store(_store);\n+  if (status_def.found_store() == nullptr) {\n+    return nullptr;\n+  }\n+\n+  ResourceMark rm;\n+  Node_List merge_list;\n+  collect_merge_list(merge_list);\n+\n+  Node* merged_input_value = make_merged_input_value(merge_list);\n+  if (merged_input_value == nullptr) { return nullptr; }\n+\n+  StoreNode* merged_store = make_merged_store(merge_list, merged_input_value);\n+\n+  DEBUG_ONLY( if(TraceMergeStores) { trace(merge_list, merged_input_value, merged_store); } )\n+\n+  return merged_store;\n+}\n+\n+\/\/ Check compatibility between _store and other_store.\n+bool MergePrimitiveArrayStores::is_compatible_store(const StoreNode* other_store) const {\n+  int opc = _store->Opcode();\n+  assert(opc == Op_StoreB || opc == Op_StoreC || opc == Op_StoreI, \"precondition\");\n+  assert(_store->adr_type()->isa_aryptr() != nullptr, \"must be array store\");\n+\n+  if (other_store == nullptr ||\n+      _store->Opcode() != other_store->Opcode() ||\n+      other_store->adr_type()->isa_aryptr() == nullptr) {\n+    return false;\n+  }\n+\n+  \/\/ Check that the size of the stores, and the array elements are all the same.\n+  const TypeAryPtr* aryptr_t1 = _store->adr_type()->is_aryptr();\n+  const TypeAryPtr* aryptr_t2 = other_store->adr_type()->is_aryptr();\n+  BasicType aryptr_bt1 = aryptr_t1->elem()->array_element_basic_type();\n+  BasicType aryptr_bt2 = aryptr_t2->elem()->array_element_basic_type();\n+  if (!is_java_primitive(aryptr_bt1) || !is_java_primitive(aryptr_bt2)) {\n+    return false;\n+  }\n+  int size1 = type2aelembytes(aryptr_bt1);\n+  int size2 = type2aelembytes(aryptr_bt2);\n+  if (size1 != size2 ||\n+      size1 != _store->memory_size() ||\n+      _store->memory_size() != other_store->memory_size()) {\n+    return false;\n+  }\n+  return true;\n+}\n+\n+bool MergePrimitiveArrayStores::is_adjacent_pair(const StoreNode* use_store, const StoreNode* def_store) const {\n+  if (!is_adjacent_input_pair(def_store->in(MemNode::ValueIn),\n+                              use_store->in(MemNode::ValueIn),\n+                              def_store->memory_size())) {\n+    return false;\n+  }\n+\n+  ResourceMark rm;\n+  ArrayPointer array_pointer_use = ArrayPointer::make(_phase, use_store->in(MemNode::Address));\n+  ArrayPointer array_pointer_def = ArrayPointer::make(_phase, def_store->in(MemNode::Address));\n+  if (!array_pointer_def.is_adjacent_to_and_before(array_pointer_use, use_store->memory_size())) {\n+    return false;\n+  }\n+\n+  return true;\n+}\n+\n+bool MergePrimitiveArrayStores::is_adjacent_input_pair(const Node* n1, const Node* n2, const int memory_size) const {\n+  \/\/ Pattern: [n1 = ConI, n2 = ConI]\n+  if (n1->Opcode() == Op_ConI) {\n+    return n2->Opcode() == Op_ConI;\n+  }\n+\n+  \/\/ Pattern: [n1 = base >> shift, n2 = base >> (shift + memory_size)]\n+  Node const* base_n2;\n+  jint shift_n2;\n+  if (!is_con_RShift(n2, base_n2, shift_n2)) {\n+    return false;\n+  }\n+  if (n1->Opcode() == Op_ConvL2I) {\n+    \/\/ look through\n+    n1 = n1->in(1);\n+  }\n+  Node const* base_n1;\n+  jint shift_n1;\n+  if (n1 == base_n2) {\n+    \/\/ n1 = base = base >> 0\n+    base_n1 = n1;\n+    shift_n1 = 0;\n+  } else if (!is_con_RShift(n1, base_n1, shift_n1)) {\n+    return false;\n+  }\n+  int bits_per_store = memory_size * 8;\n+  if (base_n1 != base_n2 ||\n+      shift_n1 + bits_per_store != shift_n2 ||\n+      shift_n1 % bits_per_store != 0) {\n+    return false;\n+  }\n+\n+  \/\/ both load from same value with correct shift\n+  return true;\n+}\n+\n+\/\/ Detect pattern: n = base_out >> shift_out\n+bool MergePrimitiveArrayStores::is_con_RShift(const Node* n, Node const*& base_out, jint& shift_out) {\n+  assert(n != nullptr, \"precondition\");\n+\n+  int opc = n->Opcode();\n+  if (opc == Op_ConvL2I) {\n+    n = n->in(1);\n+    opc = n->Opcode();\n+  }\n+\n+  if ((opc == Op_RShiftI ||\n+       opc == Op_RShiftL ||\n+       opc == Op_URShiftI ||\n+       opc == Op_URShiftL) &&\n+      n->in(2)->is_ConI()) {\n+    base_out = n->in(1);\n+    shift_out = n->in(2)->get_int();\n+    assert(shift_out >= 0, \"must be positive\");\n+    return true;\n+  }\n+  return false;\n+}\n+\n+\/\/ Check if there is nothing between the two stores, except optionally a RangeCheck leading to an uncommon trap.\n+MergePrimitiveArrayStores::CFGStatus MergePrimitiveArrayStores::cfg_status_for_pair(const StoreNode* use_store, const StoreNode* def_store) {\n+  assert(use_store->in(MemNode::Memory) == def_store, \"use-def relationship\");\n+\n+  Node* ctrl_use = use_store->in(MemNode::Control);\n+  Node* ctrl_def = def_store->in(MemNode::Control);\n+  if (ctrl_use == nullptr || ctrl_def == nullptr) {\n+    return CFGStatus::Failure;\n+  }\n+\n+  if (ctrl_use == ctrl_def) {\n+    \/\/ Same ctrl -> no RangeCheck in between.\n+    \/\/ Check: use_store must be the only use of def_store.\n+    if (def_store->outcnt() > 1) {\n+      return CFGStatus::Failure;\n+    }\n+    return CFGStatus::SuccessNoRangeCheck;\n+  }\n+\n+  \/\/ Different ctrl -> could have RangeCheck in between.\n+  \/\/ Check: 1. def_store only has these uses: use_store and MergeMem for uncommon trap, and\n+  \/\/        2. ctrl separated by RangeCheck.\n+  if (def_store->outcnt() != 2) {\n+    return CFGStatus::Failure; \/\/ Cannot have exactly these uses: use_store and MergeMem for uncommon trap.\n+  }\n+  int use_store_out_idx = def_store->raw_out(0) == use_store ? 0 : 1;\n+  Node* merge_mem = def_store->raw_out(1 - use_store_out_idx)->isa_MergeMem();\n+  if (merge_mem == nullptr ||\n+      merge_mem->outcnt() != 1) {\n+    return CFGStatus::Failure; \/\/ Does not have MergeMem for uncommon trap.\n+  }\n+  if (!ctrl_use->is_IfProj() ||\n+      !ctrl_use->in(0)->is_RangeCheck() ||\n+      ctrl_use->in(0)->outcnt() != 2) {\n+    return CFGStatus::Failure; \/\/ Not RangeCheck.\n+  }\n+  ProjNode* other_proj = ctrl_use->as_IfProj()->other_if_proj();\n+  Node* trap = other_proj->is_uncommon_trap_proj(Deoptimization::Reason_range_check);\n+  if (trap != merge_mem->unique_out() ||\n+      ctrl_use->in(0)->in(0) != ctrl_def) {\n+    return CFGStatus::Failure; \/\/ Not RangeCheck with merge_mem leading to uncommon trap.\n+  }\n+\n+  return CFGStatus::SuccessWithRangeCheck;\n+}\n+\n+MergePrimitiveArrayStores::Status MergePrimitiveArrayStores::find_adjacent_use_store(const StoreNode* def_store) const {\n+  Status status_use = find_use_store(def_store);\n+  StoreNode* use_store = status_use.found_store();\n+  if (use_store != nullptr && !is_adjacent_pair(use_store, def_store)) {\n+    return Status::make_failure();\n+  }\n+  return status_use;\n+}\n+\n+MergePrimitiveArrayStores::Status MergePrimitiveArrayStores::find_adjacent_def_store(const StoreNode* use_store) const {\n+  Status status_def = find_def_store(use_store);\n+  StoreNode* def_store = status_def.found_store();\n+  if (def_store != nullptr && !is_adjacent_pair(use_store, def_store)) {\n+    return Status::make_failure();\n+  }\n+  return status_def;\n+}\n+\n+MergePrimitiveArrayStores::Status MergePrimitiveArrayStores::find_use_store(const StoreNode* def_store) const {\n+  Status status_use = find_use_store_unidirectional(def_store);\n+\n+#ifdef ASSERT\n+  StoreNode* use_store = status_use.found_store();\n+  if (use_store != nullptr) {\n+    Status status_def = find_def_store_unidirectional(use_store);\n+    assert(status_def.found_store() == def_store &&\n+           status_def.found_range_check() == status_use.found_range_check(),\n+           \"find_use_store and find_def_store must be symmetric\");\n+  }\n+#endif\n+\n+  return status_use;\n+}\n+\n+MergePrimitiveArrayStores::Status MergePrimitiveArrayStores::find_def_store(const StoreNode* use_store) const {\n+  Status status_def = find_def_store_unidirectional(use_store);\n+\n+#ifdef ASSERT\n+  StoreNode* def_store = status_def.found_store();\n+  if (def_store != nullptr) {\n+    Status status_use = find_use_store_unidirectional(def_store);\n+    assert(status_use.found_store() == use_store &&\n+           status_use.found_range_check() == status_def.found_range_check(),\n+           \"find_use_store and find_def_store must be symmetric\");\n+  }\n+#endif\n+\n+  return status_def;\n+}\n+\n+MergePrimitiveArrayStores::Status MergePrimitiveArrayStores::find_use_store_unidirectional(const StoreNode* def_store) const {\n+  assert(is_compatible_store(def_store), \"precondition: must be compatible with _store\");\n+\n+  for (DUIterator_Fast imax, i = def_store->fast_outs(imax); i < imax; i++) {\n+    StoreNode* use_store = def_store->fast_out(i)->isa_Store();\n+    if (is_compatible_store(use_store)) {\n+      return Status::make(use_store, cfg_status_for_pair(use_store, def_store));\n+    }\n+  }\n+\n+  return Status::make_failure();\n+}\n+\n+MergePrimitiveArrayStores::Status MergePrimitiveArrayStores::find_def_store_unidirectional(const StoreNode* use_store) const {\n+  assert(is_compatible_store(use_store), \"precondition: must be compatible with _store\");\n+\n+  StoreNode* def_store = use_store->in(MemNode::Memory)->isa_Store();\n+  if (!is_compatible_store(def_store)) {\n+    return Status::make_failure();\n+  }\n+\n+  return Status::make(def_store, cfg_status_for_pair(use_store, def_store));\n+}\n+\n+void MergePrimitiveArrayStores::collect_merge_list(Node_List& merge_list) const {\n+  \/\/ The merged store can be at most 8 bytes.\n+  const uint merge_list_max_size = 8 \/ _store->memory_size();\n+  assert(merge_list_max_size >= 2 &&\n+         merge_list_max_size <= 8 &&\n+         is_power_of_2(merge_list_max_size),\n+         \"must be 2, 4 or 8\");\n+\n+  \/\/ Traverse up the chain of adjacent def stores.\n+  StoreNode* current = _store;\n+  merge_list.push(current);\n+  while (current != nullptr && merge_list.size() < merge_list_max_size) {\n+    Status status = find_adjacent_def_store(current);\n+    current = status.found_store();\n+    if (current != nullptr) {\n+      merge_list.push(current);\n+\n+      \/\/ We can have at most one RangeCheck.\n+      if (status.found_range_check()) {\n+        break;\n+      }\n+    }\n+  }\n+\n+  \/\/ Truncate the merge_list to a power of 2.\n+  const uint pow2size = round_down_power_of_2(merge_list.size());\n+  assert(pow2size >= 2, \"must be merging at least 2 stores\");\n+  while (merge_list.size() > pow2size) { merge_list.pop(); }\n+}\n+\n+\/\/ Merge the input values of the smaller stores to a single larger input value.\n+Node* MergePrimitiveArrayStores::make_merged_input_value(const Node_List& merge_list) {\n+  int new_memory_size = _store->memory_size() * merge_list.size();\n+  Node* first = merge_list.at(merge_list.size()-1);\n+  Node* merged_input_value = nullptr;\n+  if (_store->in(MemNode::ValueIn)->Opcode() == Op_ConI) {\n+    \/\/ Pattern: [ConI, ConI, ...] -> new constant\n+    jlong con = 0;\n+    jlong bits_per_store = _store->memory_size() * 8;\n+    jlong mask = (((jlong)1) << bits_per_store) - 1;\n+    for (uint i = 0; i < merge_list.size(); i++) {\n+      jlong con_i = merge_list.at(i)->in(MemNode::ValueIn)->get_int();\n+      con = con << bits_per_store;\n+      con = con | (mask & con_i);\n+    }\n+    merged_input_value = _phase->longcon(con);\n+  } else {\n+    \/\/ Pattern: [base >> 24, base >> 16, base >> 8, base] -> base\n+    \/\/             |                                  |\n+    \/\/           _store                             first\n+    \/\/\n+    merged_input_value = first->in(MemNode::ValueIn);\n+    Node const* base_last;\n+    jint shift_last;\n+    bool is_true = is_con_RShift(_store->in(MemNode::ValueIn), base_last, shift_last);\n+    assert(is_true, \"must detect con RShift\");\n+    if (merged_input_value != base_last && merged_input_value->Opcode() == Op_ConvL2I) {\n+      \/\/ look through\n+      merged_input_value = merged_input_value->in(1);\n+    }\n+    if (merged_input_value != base_last) {\n+      \/\/ merged_input_value is not the base\n+      return nullptr;\n+    }\n+  }\n+\n+  if (_phase->type(merged_input_value)->isa_long() != nullptr && new_memory_size <= 4) {\n+    \/\/ Example:\n+    \/\/\n+    \/\/   long base = ...;\n+    \/\/   a[0] = (byte)(base >> 0);\n+    \/\/   a[1] = (byte)(base >> 8);\n+    \/\/\n+    merged_input_value = _phase->transform(new ConvL2INode(merged_input_value));\n+  }\n+\n+  assert((_phase->type(merged_input_value)->isa_int() != nullptr && new_memory_size <= 4) ||\n+         (_phase->type(merged_input_value)->isa_long() != nullptr && new_memory_size == 8),\n+         \"merged_input_value is either int or long, and new_memory_size is small enough\");\n+\n+  return merged_input_value;\n+}\n+\n+\/\/                                                                                                          \/\/\n+\/\/ first_ctrl    first_mem   first_adr                first_ctrl    first_mem         first_adr             \/\/\n+\/\/  |                |           |                     |                |                 |                 \/\/\n+\/\/  |                |           |                     |                +---------------+ |                 \/\/\n+\/\/  |                |           |                     |                |               | |                 \/\/\n+\/\/  |                | +---------+                     |                | +---------------+                 \/\/\n+\/\/  |                | |                               |                | |             | |                 \/\/\n+\/\/  +--------------+ | |  v1                           +------------------------------+ | |  v1             \/\/\n+\/\/  |              | | |  |                            |                | |           | | |  |              \/\/\n+\/\/ RangeCheck     first_store                         RangeCheck        | |          first_store            \/\/\n+\/\/  |                |  |                              |                | |                |                \/\/\n+\/\/ last_ctrl         |  +----> unc_trap               last_ctrl         | |                +----> unc_trap  \/\/\n+\/\/  |                |                       ===>      |                | |                                 \/\/\n+\/\/  +--------------+ | a2 v2                           |                | |                                 \/\/\n+\/\/  |              | | |  |                            |                | |                                 \/\/\n+\/\/  |             second_store                         |                | |                                 \/\/\n+\/\/  |                |                                 |                | | [v1 v2   ...   vn]              \/\/\n+\/\/ ...              ...                                |                | |         |                       \/\/\n+\/\/  |                |                                 |                | |         v                       \/\/\n+\/\/  +--------------+ | an vn                           +--------------+ | | merged_input_value              \/\/\n+\/\/                 | | |  |                                           | | |  |                              \/\/\n+\/\/                last_store (= _store)                              merged_store                           \/\/\n+\/\/                                                                                                          \/\/\n+StoreNode* MergePrimitiveArrayStores::make_merged_store(const Node_List& merge_list, Node* merged_input_value) {\n+  Node* first_store = merge_list.at(merge_list.size()-1);\n+  Node* last_ctrl   = _store->in(MemNode::Control); \/\/ after (optional) RangeCheck\n+  Node* first_mem   = first_store->in(MemNode::Memory);\n+  Node* first_adr   = first_store->in(MemNode::Address);\n+\n+  const TypePtr* new_adr_type = _store->adr_type();\n+\n+  int new_memory_size = _store->memory_size() * merge_list.size();\n+  BasicType bt = T_ILLEGAL;\n+  switch (new_memory_size) {\n+    case 2: bt = T_SHORT; break;\n+    case 4: bt = T_INT;   break;\n+    case 8: bt = T_LONG;  break;\n+  }\n+\n+  StoreNode* merged_store = StoreNode::make(*_phase, last_ctrl, first_mem, first_adr,\n+                                            new_adr_type, merged_input_value, bt, MemNode::unordered);\n+\n+  \/\/ Marking the store mismatched is sufficient to prevent reordering, since array stores\n+  \/\/ are all on the same slice. Hence, we need no barriers.\n+  merged_store->set_mismatched_access();\n+\n+  \/\/ Constants above may now also be be packed -> put candidate on worklist\n+  _phase->is_IterGVN()->_worklist.push(first_mem);\n+\n+  return merged_store;\n+}\n+\n+#ifdef ASSERT\n+void MergePrimitiveArrayStores::trace(const Node_List& merge_list, const Node* merged_input_value, const StoreNode* merged_store) const {\n+  stringStream ss;\n+  ss.print_cr(\"[TraceMergeStores]: Replace\");\n+  for (int i = (int)merge_list.size() - 1; i >= 0; i--) {\n+    merge_list.at(i)->dump(\"\\n\", false, &ss);\n+  }\n+  ss.print_cr(\"[TraceMergeStores]: with\");\n+  merged_input_value->dump(\"\\n\", false, &ss);\n+  merged_store->dump(\"\\n\", false, &ss);\n+  tty->print(\"%s\", ss.as_string());\n+}\n+#endif\n+\n@@ -2836,0 +3522,12 @@\n+#ifdef VM_LITTLE_ENDIAN\n+  if (MergeStores && UseUnalignedAccesses) {\n+    if (phase->C->post_loop_opts_phase()) {\n+      MergePrimitiveArrayStores merge(phase, this);\n+      Node* progress = merge.run();\n+      if (progress != nullptr) { return progress; }\n+    } else {\n+      phase->C->record_for_post_loop_opts_igvn(this);\n+    }\n+  }\n+#endif\n+\n@@ -3514,1 +4212,1 @@\n-    } else if (opc == Op_MemBarRelease) {\n+    } else if (opc == Op_MemBarRelease || (UseStoreStoreForCtor && opc == Op_MemBarStoreStore)) {\n","filename":"src\/hotspot\/share\/opto\/memnode.cpp","additions":699,"deletions":1,"binary":false,"changes":700,"status":"modified"},{"patch":"@@ -1725,0 +1725,16 @@\n+  void push_non_cfg_inputs_of(const Node* node) {\n+    for (uint i = 1; i < node->req(); i++) {\n+      Node* input = node->in(i);\n+      if (input != nullptr && !input->is_CFG()) {\n+        push(input);\n+      }\n+    }\n+  }\n+\n+  void push_outputs_of(const Node* node) {\n+    for (DUIterator_Fast imax, i = node->fast_outs(imax); i < imax; i++) {\n+      Node* output = node->fast_out(i);\n+      push(output);\n+    }\n+  }\n+\n","filename":"src\/hotspot\/share\/opto\/node.hpp","additions":16,"deletions":0,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -1089,1 +1089,2 @@\n-    _exits.insert_mem_bar(Op_MemBarRelease, alloc_with_final());\n+    _exits.insert_mem_bar(UseStoreStoreForCtor ? Op_MemBarStoreStore : Op_MemBarRelease,\n+                          alloc_with_final());\n@@ -2397,2 +2398,1 @@\n-  if (RegisterFinalizersAtInit &&\n-      method()->intrinsic_id() == vmIntrinsics::_Object_init) {\n+  if (method()->intrinsic_id() == vmIntrinsics::_Object_init) {\n","filename":"src\/hotspot\/share\/opto\/parse1.cpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -2339,1 +2339,9 @@\n-  assert( igvn->hash_find(this) != this, \"Need to remove from hash before changing edges\" );\n+#ifdef ASSERT\n+  if (igvn->hash_find(this) == this) {\n+    tty->print_cr(\"Need to remove from hash before changing edges\");\n+    this->dump(1);\n+    tty->print_cr(\"Set at i = %d\", i);\n+    n->dump();\n+    assert(false, \"Need to remove from hash before changing edges\");\n+  }\n+#endif\n","filename":"src\/hotspot\/share\/opto\/phaseX.cpp","additions":9,"deletions":1,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -780,0 +780,23 @@\n+\/\/ Takes as parameters:\n+\/\/ void *dest\n+\/\/ long size\n+\/\/ uchar byte\n+const TypeFunc* OptoRuntime::make_setmemory_Type() {\n+  \/\/ create input type (domain)\n+  int argcnt = NOT_LP64(3) LP64_ONLY(4);\n+  const Type** fields = TypeTuple::fields(argcnt);\n+  int argp = TypeFunc::Parms;\n+  fields[argp++] = TypePtr::NOTNULL;        \/\/ dest\n+  fields[argp++] = TypeX_X;                 \/\/ size\n+  LP64_ONLY(fields[argp++] = Type::HALF);   \/\/ size\n+  fields[argp++] = TypeInt::UBYTE;          \/\/ bytevalue\n+  assert(argp == TypeFunc::Parms+argcnt, \"correct decoding\");\n+  const TypeTuple* domain = TypeTuple::make(TypeFunc::Parms+argcnt, fields);\n+\n+  \/\/ no result type needed\n+  fields = TypeTuple::fields(1);\n+  fields[TypeFunc::Parms+0] = nullptr; \/\/ void\n+  const TypeTuple* range = TypeTuple::make(TypeFunc::Parms, fields);\n+  return TypeFunc::make(domain, range);\n+}\n+\n","filename":"src\/hotspot\/share\/opto\/runtime.cpp","additions":23,"deletions":0,"binary":false,"changes":23,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1998, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1998, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -271,0 +271,2 @@\n+  static const TypeFunc* make_setmemory_Type();\n+\n","filename":"src\/hotspot\/share\/opto\/runtime.hpp","additions":3,"deletions":1,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -99,19 +99,1 @@\n-  if (subgraph_has_opaque(n)) {\n-    Unique_Node_List wq;\n-    wq.push(n);\n-    for (uint i = 0; i < wq.size(); i++) {\n-      Node* m = wq.at(i);\n-      if (m->is_If()) {\n-        assert(assertion_predicate_has_loop_opaque_node(m->as_If()), \"opaque node not reachable from if?\");\n-        TemplateAssertionPredicateExpression template_assertion_predicate_expression(m->in(1)->as_Opaque4());\n-        Opaque4Node* cloned_opaque4_node = template_assertion_predicate_expression.clone(m->in(0), this);\n-        _igvn.replace_input_of(m, 1, cloned_opaque4_node);\n-      } else {\n-        assert(!m->is_CFG(), \"not CFG expected\");\n-        for (DUIterator_Fast jmax, j = m->fast_outs(jmax); j < jmax; j++) {\n-          Node* u = m->fast_out(j);\n-          wq.push(u);\n-        }\n-      }\n-    }\n-  }\n+  clone_template_assertion_predicate_expression_down(n);\n@@ -431,0 +413,21 @@\n+\/\/ 'n' could be a node belonging to a Template Assertion Predicate Expression (i.e. any node between a Template\n+\/\/ Assertion Predicate and its OpaqueLoop* nodes (included)). We cannot simply split this node up since this would\n+\/\/ create a phi node inside the Template Assertion Predicate Expression - making it unrecognizable as such. Therefore,\n+\/\/ we completely clone the entire Template Assertion Predicate Expression \"down\". This ensures that we have an\n+\/\/ untouched copy that is still recognized by the Template Assertion Predicate matching code.\n+void PhaseIdealLoop::clone_template_assertion_predicate_expression_down(Node* node) {\n+  if (!TemplateAssertionPredicateExpressionNode::is_in_expression(node)) {\n+    return;\n+  }\n+\n+  TemplateAssertionPredicateExpressionNode template_assertion_predicate_expression_node(node);\n+  auto clone_expression = [&](IfNode* template_assertion_predicate) {\n+    Opaque4Node* opaque4_node = template_assertion_predicate->in(1)->as_Opaque4();\n+    TemplateAssertionPredicateExpression template_assertion_predicate_expression(opaque4_node);\n+    Node* new_ctrl = template_assertion_predicate->in(0);\n+    Opaque4Node* cloned_opaque4_node = template_assertion_predicate_expression.clone(new_ctrl, this);\n+    igvn().replace_input_of(template_assertion_predicate, 1, cloned_opaque4_node);\n+  };\n+  template_assertion_predicate_expression_node.for_each_template_assertion_predicate(clone_expression);\n+}\n+\n","filename":"src\/hotspot\/share\/opto\/split_if.cpp","additions":22,"deletions":19,"binary":false,"changes":41,"status":"modified"},{"patch":"@@ -2013,1 +2013,1 @@\n-    kit.insert_mem_bar(Op_MemBarRelease, result);\n+    kit.insert_mem_bar(UseStoreStoreForCtor ? Op_MemBarStoreStore : Op_MemBarRelease, result);\n","filename":"src\/hotspot\/share\/opto\/stringopts.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -1095,1 +1095,1 @@\n-          pool_index = mh->constants()->resolved_indy_entry_at(mh->constants()->decode_invokedynamic_index(cpci))->constant_pool_index();\n+          pool_index = mh->constants()->resolved_indy_entry_at(cpci)->constant_pool_index();\n","filename":"src\/hotspot\/share\/prims\/jvmtiClassFileReconstituter.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -543,1 +543,6 @@\n-    Copy::fill_to_memory_atomic(p, sz, value);\n+    if (StubRoutines::unsafe_setmemory() != nullptr) {\n+      MACOS_AARCH64_ONLY(ThreadWXEnable wx(WXExec, thread));\n+      StubRoutines::UnsafeSetMemory_stub()(p, sz, value);\n+    } else {\n+      Copy::fill_to_memory_atomic(p, sz, value);\n+    }\n","filename":"src\/hotspot\/share\/prims\/unsafe.cpp","additions":6,"deletions":1,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -44,0 +44,1 @@\n+#include \"compiler\/compilerOracle.hpp\"\n@@ -1889,4 +1890,0 @@\n-WB_ENTRY(jint, WB_ConstantPoolEncodeIndyIndex(JNIEnv* env, jobject wb, jint index))\n-  return ConstantPool::encode_invokedynamic_index(index);\n-WB_END\n-\n@@ -1953,1 +1950,0 @@\n-\n@@ -1979,1 +1975,0 @@\n-\n@@ -2081,1 +2076,1 @@\n-  enum CompileCommand option = CompilerOracle::string_to_option(flag_name);\n+  CompileCommandEnum option = CompilerOracle::string_to_option(flag_name);\n@@ -2083,1 +2078,1 @@\n-  if (option == CompileCommand::Unknown) {\n+  if (option == CompileCommandEnum::Unknown) {\n@@ -2937,2 +2932,0 @@\n-  {CC\"encodeConstantPoolIndyIndex0\",\n-      CC\"(I)I\",                      (void*)&WB_ConstantPoolEncodeIndyIndex},\n","filename":"src\/hotspot\/share\/prims\/whitebox.cpp","additions":3,"deletions":10,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -506,0 +506,2 @@\n+  { \"UseNotificationThread\",        JDK_Version::jdk(23), JDK_Version::jdk(24), JDK_Version::jdk(25) },\n+  { \"UseEmptySlotsInSupers\",        JDK_Version::jdk(23), JDK_Version::jdk(24), JDK_Version::jdk(25) },\n@@ -543,0 +545,1 @@\n+  { \"ScavengeBeforeFullGC\",         JDK_Version::undefined(), JDK_Version::jdk(23), JDK_Version::jdk(24) },\n@@ -1717,5 +1720,0 @@\n-  \/\/ This appears to improve mutator locality\n-  if (FLAG_SET_CMDLINE(ScavengeBeforeFullGC, false) != JVMFlag::SUCCESS) {\n-    return JNI_EINVAL;\n-  }\n-\n@@ -2809,4 +2807,0 @@\n-      \/\/ disable scavenge before parallel mark-compact\n-      if (FLAG_SET_CMDLINE(ScavengeBeforeFullGC, false) != JVMFlag::SUCCESS) {\n-        return JNI_EINVAL;\n-      }\n@@ -3739,0 +3733,5 @@\n+  \/\/ The VMThread needs to stop now and then to execute these debug options.\n+  if ((HandshakeALot || SafepointALot) && FLAG_IS_DEFAULT(GuaranteedSafepointInterval)) {\n+    FLAG_SET_DEFAULT(GuaranteedSafepointInterval, 1000);\n+  }\n+\n","filename":"src\/hotspot\/share\/runtime\/arguments.cpp","additions":8,"deletions":9,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -1022,2 +1022,3 @@\n-    if (reg_map->include_argument_oops()) {\n-      _cb->preserve_callee_argument_oops(*this, reg_map, f);\n+    if (reg_map->include_argument_oops() && _cb->is_nmethod()) {\n+      \/\/ Only nmethod preserves outgoing arguments at call.\n+      _cb->as_nmethod()->preserve_callee_argument_oops(*this, reg_map, f);\n","filename":"src\/hotspot\/share\/runtime\/frame.cpp","additions":3,"deletions":2,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -674,4 +674,0 @@\n-  product(bool, RegisterFinalizersAtInit, true,                             \\\n-          \"(Deprecated) Register finalizable objects at end of \"            \\\n-          \"Object.<init> or after allocation\")                              \\\n-                                                                            \\\n@@ -747,3 +743,2 @@\n-          \"off). The check is performed on GuaranteedSafepointInterval, \"   \\\n-          \"AsyncDeflationInterval or GuaranteedAsyncDeflationInterval, \"    \\\n-          \"whichever is lower.\")                                            \\\n+          \"off). The check is performed on AsyncDeflationInterval or \"      \\\n+          \"GuaranteedAsyncDeflationInterval, whichever is lower.\")          \\\n@@ -896,3 +891,0 @@\n-  develop(bool, TraceInvocationCounterOverflow, false,                      \\\n-          \"Trace method invocation counter overflow\")                       \\\n-                                                                            \\\n@@ -984,1 +976,1 @@\n-          \"Use Notification Thread\")                                        \\\n+          \"(Deprecated) Use Notification Thread\")                           \\\n@@ -1010,3 +1002,0 @@\n-  product(bool, PrintMethodFlushingStatistics, false, DIAGNOSTIC,           \\\n-          \"print statistics about method flushing\")                         \\\n-                                                                            \\\n@@ -1301,1 +1290,1 @@\n-  product(intx, GuaranteedSafepointInterval, 1000, DIAGNOSTIC,              \\\n+  product(intx, GuaranteedSafepointInterval, 0, DIAGNOSTIC,                 \\\n@@ -2004,1 +1993,2 @@\n-                \"Allow allocating fields in empty slots of super-classes\")  \\\n+          \"(Deprecated) Allow allocating fields in empty slots of \"         \\\n+          \"super-classes\")                                                  \\\n","filename":"src\/hotspot\/share\/runtime\/globals.hpp","additions":6,"deletions":16,"binary":false,"changes":22,"status":"modified"},{"patch":"@@ -182,0 +182,1 @@\n+uint SharedRuntime::_unsafe_set_memory_ctr=0;\n@@ -547,1 +548,0 @@\n-\n@@ -2078,0 +2078,1 @@\n+  if (_unsafe_set_memory_ctr) tty->print_cr(\"%5u unsafe set memorys\", _unsafe_set_memory_ctr);\n","filename":"src\/hotspot\/share\/runtime\/sharedRuntime.cpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -569,0 +569,2 @@\n+  static uint _unsafe_set_memory_ctr;      \/\/ Slow-path includes alignment checks\n+\n","filename":"src\/hotspot\/share\/runtime\/sharedRuntime.hpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -44,4 +44,4 @@\n-UnsafeCopyMemory* UnsafeCopyMemory::_table                      = nullptr;\n-int UnsafeCopyMemory::_table_length                             = 0;\n-int UnsafeCopyMemory::_table_max_length                         = 0;\n-address UnsafeCopyMemory::_common_exit_stub_pc                  = nullptr;\n+UnsafeMemoryAccess* UnsafeMemoryAccess::_table                  = nullptr;\n+int UnsafeMemoryAccess::_table_length                           = 0;\n+int UnsafeMemoryAccess::_table_max_length                       = 0;\n+address UnsafeMemoryAccess::_common_exit_stub_pc                = nullptr;\n@@ -113,0 +113,2 @@\n+address StubRoutines::_unsafe_setmemory                  = nullptr;\n+\n@@ -210,3 +212,3 @@\n-void UnsafeCopyMemory::create_table(int max_size) {\n-  UnsafeCopyMemory::_table = new UnsafeCopyMemory[max_size];\n-  UnsafeCopyMemory::_table_max_length = max_size;\n+void UnsafeMemoryAccess::create_table(int max_size) {\n+  UnsafeMemoryAccess::_table = new UnsafeMemoryAccess[max_size];\n+  UnsafeMemoryAccess::_table_max_length = max_size;\n@@ -215,3 +217,3 @@\n-bool UnsafeCopyMemory::contains_pc(address pc) {\n-  for (int i = 0; i < UnsafeCopyMemory::_table_length; i++) {\n-    UnsafeCopyMemory* entry = &UnsafeCopyMemory::_table[i];\n+bool UnsafeMemoryAccess::contains_pc(address pc) {\n+  for (int i = 0; i < UnsafeMemoryAccess::_table_length; i++) {\n+    UnsafeMemoryAccess* entry = &UnsafeMemoryAccess::_table[i];\n@@ -225,3 +227,3 @@\n-address UnsafeCopyMemory::page_error_continue_pc(address pc) {\n-  for (int i = 0; i < UnsafeCopyMemory::_table_length; i++) {\n-    UnsafeCopyMemory* entry = &UnsafeCopyMemory::_table[i];\n+address UnsafeMemoryAccess::page_error_continue_pc(address pc) {\n+  for (int i = 0; i < UnsafeMemoryAccess::_table_length; i++) {\n+    UnsafeMemoryAccess* entry = &UnsafeMemoryAccess::_table[i];\n@@ -527,1 +529,1 @@\n-UnsafeCopyMemoryMark::UnsafeCopyMemoryMark(StubCodeGenerator* cgen, bool add_entry, bool continue_at_scope_end, address error_exit_pc) {\n+UnsafeMemoryAccessMark::UnsafeMemoryAccessMark(StubCodeGenerator* cgen, bool add_entry, bool continue_at_scope_end, address error_exit_pc) {\n@@ -533,1 +535,1 @@\n-      err_exit_pc = error_exit_pc != nullptr ? error_exit_pc : UnsafeCopyMemory::common_exit_stub_pc();\n+      err_exit_pc = error_exit_pc != nullptr ? error_exit_pc : UnsafeMemoryAccess::common_exit_stub_pc();\n@@ -536,1 +538,1 @@\n-    _ucm_entry = UnsafeCopyMemory::add_to_table(_cgen->assembler()->pc(), nullptr, err_exit_pc);\n+    _ucm_entry = UnsafeMemoryAccess::add_to_table(_cgen->assembler()->pc(), nullptr, err_exit_pc);\n@@ -540,1 +542,1 @@\n-UnsafeCopyMemoryMark::~UnsafeCopyMemoryMark() {\n+UnsafeMemoryAccessMark::~UnsafeMemoryAccessMark() {\n","filename":"src\/hotspot\/share\/runtime\/stubRoutines.cpp","additions":19,"deletions":17,"binary":false,"changes":36,"status":"modified"},{"patch":"@@ -79,1 +79,1 @@\n-class UnsafeCopyMemory : public CHeapObj<mtCode> {\n+class UnsafeMemoryAccess : public CHeapObj<mtCode> {\n@@ -86,1 +86,1 @@\n-  static UnsafeCopyMemory* _table;\n+  static UnsafeMemoryAccess* _table;\n@@ -89,1 +89,1 @@\n-  UnsafeCopyMemory() : _start_pc(nullptr), _end_pc(nullptr), _error_exit_pc(nullptr) {}\n+  UnsafeMemoryAccess() : _start_pc(nullptr), _end_pc(nullptr), _error_exit_pc(nullptr) {}\n@@ -100,3 +100,3 @@\n-  static UnsafeCopyMemory* add_to_table(address start_pc, address end_pc, address error_exit_pc) {\n-    guarantee(_table_length < _table_max_length, \"Incorrect UnsafeCopyMemory::_table_max_length\");\n-    UnsafeCopyMemory* entry = &_table[_table_length];\n+  static UnsafeMemoryAccess* add_to_table(address start_pc, address end_pc, address error_exit_pc) {\n+    guarantee(_table_length < _table_max_length, \"Incorrect UnsafeMemoryAccess::_table_max_length\");\n+    UnsafeMemoryAccess* entry = &_table[_table_length];\n@@ -116,1 +116,1 @@\n-class UnsafeCopyMemoryMark : public StackObj {\n+class UnsafeMemoryAccessMark : public StackObj {\n@@ -118,1 +118,1 @@\n-  UnsafeCopyMemory*  _ucm_entry;\n+  UnsafeMemoryAccess*  _ucm_entry;\n@@ -121,2 +121,2 @@\n-  UnsafeCopyMemoryMark(StubCodeGenerator* cgen, bool add_entry, bool continue_at_scope_end, address error_exit_pc = nullptr);\n-  ~UnsafeCopyMemoryMark();\n+  UnsafeMemoryAccessMark(StubCodeGenerator* cgen, bool add_entry, bool continue_at_scope_end, address error_exit_pc = nullptr);\n+  ~UnsafeMemoryAccessMark();\n@@ -196,0 +196,2 @@\n+  static address _unsafe_setmemory;\n+\n@@ -390,0 +392,5 @@\n+  static address unsafe_setmemory()     { return _unsafe_setmemory; }\n+\n+  typedef void (*UnsafeSetMemoryStub)(const void* src, size_t count, char byte);\n+  static UnsafeSetMemoryStub UnsafeSetMemory_stub()         { return CAST_TO_FN_PTR(UnsafeSetMemoryStub,  _unsafe_setmemory); }\n+\n","filename":"src\/hotspot\/share\/runtime\/stubRoutines.hpp","additions":17,"deletions":10,"binary":false,"changes":27,"status":"modified"},{"patch":"@@ -837,4 +837,4 @@\n-\/\/ vm_exit() when the program calls System.exit() to return a value or when\n-\/\/ there is a serious error in VM. The two shutdown paths are not exactly\n-\/\/ the same, but they share Shutdown.shutdown() at Java level and before_exit()\n-\/\/ and VM_Exit op at VM level.\n+\/\/ vm_exit(), when the program calls System.exit() to return a value, or when\n+\/\/ there is a serious error in VM.\n+\/\/ These two separate shutdown paths are not exactly the same, but they share\n+\/\/ Shutdown.shutdown() at Java level and before_exit() and VM_Exit op at VM level.\n","filename":"src\/hotspot\/share\/runtime\/threads.cpp","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -512,1 +512,1 @@\n-  nonstatic_field(HeapBlock::Header,           _length,                                       size_t)                                \\\n+  nonstatic_field(HeapBlock::Header,           _length,                                       uint32_t)                              \\\n@@ -555,1 +555,1 @@\n-  nonstatic_field(CodeBlob,                    _header_size,                                  int)                                   \\\n+  nonstatic_field(CodeBlob,                    _header_size,                                  u2)                                    \\\n@@ -559,1 +559,1 @@\n-  nonstatic_field(CodeBlob,                    _frame_complete_offset,                        int)                                   \\\n+  nonstatic_field(CodeBlob,                    _frame_complete_offset,                        int16_t)                               \\\n@@ -580,3 +580,2 @@\n-  nonstatic_field(nmethod,                     _consts_offset,                                int)                                   \\\n-  nonstatic_field(nmethod,                     _oops_offset,                                  int)                                   \\\n-  nonstatic_field(nmethod,                     _metadata_offset,                              int)                                   \\\n+  nonstatic_field(nmethod,                     _metadata_offset,                              u2)                                    \\\n+  nonstatic_field(nmethod,                     _scopes_pcs_offset,                            u2)                                    \\\n@@ -584,2 +583,1 @@\n-  nonstatic_field(nmethod,                     _scopes_pcs_offset,                            int)                                   \\\n-  nonstatic_field(nmethod,                     _dependencies_offset,                          int)                                   \\\n+  nonstatic_field(nmethod,                     _dependencies_offset,                          u2)                                    \\\n@@ -588,3 +586,2 @@\n-  nonstatic_field(nmethod,                     _nmethod_end_offset,                           int)                                   \\\n-  nonstatic_field(nmethod,                     _entry_point,                                  address)                               \\\n-  nonstatic_field(nmethod,                     _verified_entry_point,                         address)                               \\\n+  nonstatic_field(nmethod,                     _entry_offset,                                 u2)                                    \\\n+  nonstatic_field(nmethod,                     _verified_entry_offset,                        u2)                                    \\\n@@ -1136,0 +1133,1 @@\n+  declare_integer_type(int16_t)                                           \\\n","filename":"src\/hotspot\/share\/runtime\/vmStructs.cpp","additions":9,"deletions":11,"binary":false,"changes":20,"status":"modified"},{"patch":"@@ -377,1 +377,7 @@\n-        long comp = Blocker.begin();\n+        if (!Thread.currentThread().isVirtual()) {\n+            wait0(timeoutMillis);\n+            return;\n+        }\n+\n+        \/\/ virtual thread waiting\n+        boolean attempted = Blocker.begin();\n@@ -381,3 +387,2 @@\n-            Thread thread = Thread.currentThread();\n-            if (thread.isVirtual())\n-                thread.getAndClearInterrupt();\n+            \/\/ virtual thread's interrupt status needs to be cleared\n+            Thread.currentThread().getAndClearInterrupt();\n@@ -386,1 +391,1 @@\n-            Blocker.end(comp);\n+            Blocker.end(attempted);\n","filename":"src\/java.base\/share\/classes\/java\/lang\/Object.java","additions":10,"deletions":5,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -75,0 +75,1 @@\n+import jdk.internal.misc.Blocker;\n@@ -2194,3 +2195,3 @@\n-        FileInputStream fdIn = new FileInputStream(FileDescriptor.in);\n-        FileOutputStream fdOut = new FileOutputStream(FileDescriptor.out);\n-        FileOutputStream fdErr = new FileOutputStream(FileDescriptor.err);\n+        FileInputStream fdIn = new In(FileDescriptor.in);\n+        FileOutputStream fdOut = new Out(FileDescriptor.out);\n+        FileOutputStream fdErr = new Out(FileDescriptor.err);\n@@ -2221,0 +2222,77 @@\n+    \/**\n+     * System.in.\n+     *\/\n+    private static class In extends FileInputStream {\n+        In(FileDescriptor fd) {\n+            super(fd);\n+        }\n+\n+        @Override\n+        public int read() throws IOException {\n+            boolean attempted = Blocker.begin();\n+            try {\n+                return super.read();\n+            } finally {\n+                Blocker.end(attempted);\n+            }\n+        }\n+\n+        @Override\n+        public int read(byte[] b) throws IOException {\n+            boolean attempted = Blocker.begin();\n+            try {\n+                return super.read(b);\n+            } finally {\n+                Blocker.end(attempted);\n+            }\n+        }\n+\n+        @Override\n+        public int read(byte[] b, int off, int len) throws IOException {\n+            boolean attempted = Blocker.begin();\n+            try {\n+                return super.read(b, off, len);\n+            } finally {\n+                Blocker.end(attempted);\n+            }\n+        }\n+    }\n+\n+    \/**\n+     * System.out\/System.err wrap this output stream.\n+     *\/\n+    private static class Out extends FileOutputStream {\n+        Out(FileDescriptor fd) {\n+            super(fd);\n+        }\n+\n+        public void write(int b) throws IOException {\n+            boolean attempted = Blocker.begin();\n+            try {\n+                super.write(b);\n+            } finally {\n+                Blocker.end(attempted);\n+            }\n+        }\n+\n+        @Override\n+        public void write(byte[] b) throws IOException {\n+            boolean attempted = Blocker.begin();\n+            try {\n+                super.write(b);\n+            } finally {\n+                Blocker.end(attempted);\n+            }\n+        }\n+\n+        @Override\n+        public void write(byte[] b, int off, int len) throws IOException {\n+            boolean attempted = Blocker.begin();\n+            try {\n+                super.write(b, off, len);\n+            } finally {\n+                Blocker.end(attempted);\n+            }\n+        }\n+    }\n+\n","filename":"src\/java.base\/share\/classes\/java\/lang\/System.java","additions":81,"deletions":3,"binary":false,"changes":84,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2022, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2022, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -1481,1 +1481,4 @@\n-    \/** 67 *\/\n+    \/**\n+     * The class major version of JAVA_23.\n+     * @since 23\n+     *\/\n","filename":"src\/java.base\/share\/classes\/java\/lang\/classfile\/ClassFile.java","additions":5,"deletions":2,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -578,1 +578,1 @@\n-    \/** Invoke this form on the given arguments. *\/\n+    \/\/ \/** Invoke this form on the given arguments. *\/\n@@ -930,0 +930,1 @@\n+    \/** Interpretively invoke this form on the given arguments. *\/\n@@ -932,1 +933,0 @@\n-    \/** Interpretively invoke this form on the given arguments. *\/\n@@ -947,0 +947,1 @@\n+    \/** Evaluate a single Name within this form, applying its function to its arguments. *\/\n@@ -949,1 +950,0 @@\n-    \/** Evaluate a single Name within this form, applying its function to its arguments. *\/\n","filename":"src\/java.base\/share\/classes\/java\/lang\/invoke\/LambdaForm.java","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -85,2 +85,0 @@\n-    private static final Map<TypePairs, String> typePairToName;\n-\n@@ -102,1 +100,0 @@\n-        typePairToName = TypePairs.initialize();\n@@ -510,1 +507,1 @@\n-                            String methodName = typePairToName.get(typePair);\n+                            String methodName = TypePairs.typePairToName.get(typePair);\n@@ -687,0 +684,3 @@\n+\n+        private static final Map<TypePairs, String> typePairToName = initialize();\n+\n@@ -694,0 +694,11 @@\n+        public int hashCode() {\n+            return 31 * from.hashCode() + to.hashCode();\n+        }\n+\n+        public boolean equals(Object other) {\n+            if (other instanceof TypePairs otherPair) {\n+                return otherPair.from == from && otherPair.to == to;\n+            }\n+            return false;\n+        }\n+\n","filename":"src\/java.base\/share\/classes\/java\/lang\/runtime\/SwitchBootstraps.java","additions":15,"deletions":4,"binary":false,"changes":19,"status":"modified"},{"patch":"@@ -4202,0 +4202,1 @@\n+    @IntrinsicCandidate\n","filename":"src\/java.base\/share\/classes\/jdk\/internal\/misc\/Unsafe.java","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -114,1 +114,1 @@\n-    \/*****************************************\n+    \/* ***************************************\n@@ -292,1 +292,1 @@\n-    \/**\n+    \/*\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/code\/Flags.java","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -50,1 +50,1 @@\n-    \/** 1.0 had no inner classes, and so could not pass the JCK. *\/\n+    \/* 1.0 had no inner classes, and so could not pass the JCK. *\/\n@@ -53,1 +53,1 @@\n-    \/** 1.1 did not have strictfp, and so could not pass the JCK. *\/\n+    \/* 1.1 did not have strictfp, and so could not pass the JCK. *\/\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/code\/Source.java","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -618,1 +618,1 @@\n-    \/** Navigation methods, these will work for classes, type variables,\n+    \/* Navigation methods, these will work for classes, type variables,\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/code\/Type.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -1031,1 +1031,1 @@\n-    \/********************\n+    \/* ******************\n@@ -1184,1 +1184,1 @@\n-    \/*********************\n+    \/* *******************\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/comp\/Annotate.java","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -159,1 +159,1 @@\n-\/**************************************************************************\n+\/* ************************************************************************\n@@ -657,1 +657,1 @@\n-\/**************************************************************************\n+\/* ************************************************************************\n@@ -794,1 +794,1 @@\n-\/**************************************************************************\n+\/* ************************************************************************\n@@ -949,1 +949,1 @@\n-\/**************************************************************************\n+\/* ************************************************************************\n@@ -1525,1 +1525,1 @@\n-\/**************************************************************************\n+\/* ************************************************************************\n@@ -1959,1 +1959,1 @@\n-\/**************************************************************************\n+\/* ************************************************************************\n@@ -2019,1 +2019,1 @@\n-\/**************************************************************************\n+\/* ************************************************************************\n@@ -2089,1 +2089,1 @@\n-\/**************************************************************************\n+\/* ************************************************************************\n@@ -2183,1 +2183,1 @@\n-\/**************************************************************************\n+\/* ************************************************************************\n@@ -4468,1 +4468,1 @@\n-\/**************************************************************************\n+\/* ************************************************************************\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/comp\/Lower.java","additions":10,"deletions":10,"binary":false,"changes":20,"status":"modified"},{"patch":"@@ -320,1 +320,1 @@\n-\/************************************************************************\n+\/* **********************************************************************\n@@ -350,1 +350,1 @@\n-\/************************************************************************\n+\/* **********************************************************************\n@@ -390,1 +390,1 @@\n-\/************************************************************************\n+\/* **********************************************************************\n@@ -449,1 +449,1 @@\n-\/************************************************************************\n+\/* **********************************************************************\n@@ -805,1 +805,1 @@\n-\/************************************************************************\n+\/* **********************************************************************\n@@ -1476,1 +1476,1 @@\n-\/************************************************************************\n+\/* **********************************************************************\n@@ -2606,1 +2606,1 @@\n-\/************************************************************************\n+\/* **********************************************************************\n@@ -3149,1 +3149,1 @@\n-\/************************************************************************\n+\/* **********************************************************************\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/jvm\/ClassReader.java","additions":8,"deletions":8,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -199,1 +199,1 @@\n-\/******************************************************************\n+\/* ****************************************************************\n@@ -241,1 +241,1 @@\n-\/******************************************************************\n+\/* ****************************************************************\n@@ -263,1 +263,1 @@\n-\/******************************************************************\n+\/* ****************************************************************\n@@ -281,1 +281,1 @@\n-\/******************************************************************\n+\/* ****************************************************************\n@@ -484,1 +484,1 @@\n-\/**********************************************************************\n+\/* ********************************************************************\n@@ -745,1 +745,1 @@\n-\/**********************************************************************\n+\/* ********************************************************************\n@@ -827,1 +827,1 @@\n-\/**********************************************************************\n+\/* ********************************************************************\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/jvm\/ClassWriter.java","additions":7,"deletions":7,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -1122,0 +1122,5 @@\n+        \/* this method is heavily invoked, as expected, for deeply nested blocks, if blocks doesn't happen to have\n+         * patterns there will be an unnecessary tax on memory consumption every time this method is executed, for this\n+         * reason we have created helper methods and here at a higher level we just discriminate depending on the\n+         * presence, or not, of patterns in a given block\n+         *\/\n@@ -1123,16 +1128,1 @@\n-            Set<JCMethodInvocation> prevInvocationsWithPatternMatchingCatch = invocationsWithPatternMatchingCatch;\n-            ListBuffer<int[]> prevRanges = patternMatchingInvocationRanges;\n-            State startState = code.state.dup();\n-            try {\n-                invocationsWithPatternMatchingCatch = tree.patternMatchingCatch.calls2Handle();\n-                patternMatchingInvocationRanges = new ListBuffer<>();\n-                doVisitBlock(tree);\n-            } finally {\n-                Chain skipCatch = code.branch(goto_);\n-                JCCatch handler = tree.patternMatchingCatch.handler();\n-                code.entryPoint(startState, handler.param.sym.type);\n-                genPatternMatchingCatch(handler, env, patternMatchingInvocationRanges.toList());\n-                code.resolve(skipCatch);\n-                invocationsWithPatternMatchingCatch = prevInvocationsWithPatternMatchingCatch;\n-                patternMatchingInvocationRanges = prevRanges;\n-            }\n+            visitBlockWithPatterns(tree);\n@@ -1140,1 +1130,20 @@\n-            doVisitBlock(tree);\n+            internalVisitBlock(tree);\n+        }\n+    }\n+\n+    private void visitBlockWithPatterns(JCBlock tree) {\n+        Set<JCMethodInvocation> prevInvocationsWithPatternMatchingCatch = invocationsWithPatternMatchingCatch;\n+        ListBuffer<int[]> prevRanges = patternMatchingInvocationRanges;\n+        State startState = code.state.dup();\n+        try {\n+            invocationsWithPatternMatchingCatch = tree.patternMatchingCatch.calls2Handle();\n+            patternMatchingInvocationRanges = new ListBuffer<>();\n+            internalVisitBlock(tree);\n+        } finally {\n+            Chain skipCatch = code.branch(goto_);\n+            JCCatch handler = tree.patternMatchingCatch.handler();\n+            code.entryPoint(startState, handler.param.sym.type);\n+            genPatternMatchingCatch(handler, env, patternMatchingInvocationRanges.toList());\n+            code.resolve(skipCatch);\n+            invocationsWithPatternMatchingCatch = prevInvocationsWithPatternMatchingCatch;\n+            patternMatchingInvocationRanges = prevRanges;\n@@ -1144,1 +1153,1 @@\n-    private void doVisitBlock(JCBlock tree) {\n+    private void internalVisitBlock(JCBlock tree) {\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/jvm\/Gen.java","additions":27,"deletions":18,"binary":false,"changes":45,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2022, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -59,0 +59,1 @@\n+import jdk.test.lib.process.ProcessTools;\n@@ -182,1 +183,1 @@\n-    static void run(String testCaseName, Consumer<OutputAnalyzer> processor) throws IOException {\n+    static void run(String testCaseName, Consumer<OutputAnalyzer> processor) throws Exception {\n@@ -196,1 +197,1 @@\n-        OutputAnalyzer analyzer = new OutputAnalyzer(pb.start());\n+        OutputAnalyzer analyzer = ProcessTools.executeProcess(pb);\n","filename":"test\/hotspot\/jtreg\/compiler\/c2\/unloaded\/TestInlineUnloaded.java","additions":4,"deletions":3,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -827,1 +827,6 @@\n-        beforeMatchingNameRegex(MAX, \"Max(I|L)\");\n+        beforeMatchingNameRegex(MAX, \"Max(I|L|F|D)\");\n+    }\n+\n+    public static final String MAX_D = PREFIX + \"MAX_D\" + POSTFIX;\n+    static {\n+        beforeMatchingNameRegex(MAX_D, \"MaxD\");\n@@ -840,0 +845,5 @@\n+    public static final String MAX_F = PREFIX + \"MAX_F\" + POSTFIX;\n+    static {\n+        beforeMatchingNameRegex(MAX_F, \"MaxF\");\n+    }\n+\n@@ -885,0 +895,10 @@\n+    public static final String MEMBAR_ACQUIRE = PREFIX + \"MEMBAR_ACQUIRE\" + POSTFIX;\n+    static {\n+        beforeMatchingNameRegex(MEMBAR_ACQUIRE, \"MemBarAcquire\");\n+    }\n+\n+    public static final String MEMBAR_RELEASE = PREFIX + \"MEMBAR_RELEASE\" + POSTFIX;\n+    static {\n+        beforeMatchingNameRegex(MEMBAR_RELEASE, \"MemBarRelease\");\n+    }\n+\n@@ -890,0 +910,5 @@\n+    public static final String MEMBAR_VOLATILE = PREFIX + \"MEMBAR_VOLATILE\" + POSTFIX;\n+    static {\n+        beforeMatchingNameRegex(MEMBAR_VOLATILE, \"MemBarVolatile\");\n+    }\n+\n@@ -895,0 +920,5 @@\n+    public static final String MIN_D = PREFIX + \"MIN_D\" + POSTFIX;\n+    static {\n+        beforeMatchingNameRegex(MIN_D, \"MinD\");\n+    }\n+\n@@ -905,0 +935,5 @@\n+    public static final String MIN_F = PREFIX + \"MIN_F\" + POSTFIX;\n+    static {\n+        beforeMatchingNameRegex(MIN_F, \"MinF\");\n+    }\n+\n@@ -2084,0 +2119,6 @@\n+    public static final String Z_COMPARE_AND_SWAP_P_WITH_BARRIER_FLAG = COMPOSITE_PREFIX + \"Z_COMPARE_AND_SWAP_P_WITH_BARRIER_FLAG\" + POSTFIX;\n+    static {\n+        String regex = START + \"zCompareAndSwapP\" + MID + \"barrier\\\\(\\\\s*\" + IS_REPLACED + \"\\\\s*\\\\)\" + END;\n+        machOnly(Z_COMPARE_AND_SWAP_P_WITH_BARRIER_FLAG, regex);\n+    }\n+\n","filename":"test\/hotspot\/jtreg\/compiler\/lib\/ir_framework\/IRNode.java","additions":42,"deletions":1,"binary":false,"changes":43,"status":"modified"}]}