{"files":[{"patch":"@@ -1088,1 +1088,1 @@\n-        linux_x64: \"gcc11.2.0-OL6.4+1.0\",\n+        linux_x64: \"gcc13.2.0-OL6.4+1.0\",\n@@ -1091,1 +1091,1 @@\n-        linux_aarch64: input.build_cpu == \"x64\" ? \"gcc11.2.0-OL7.6+1.1\" : \"gcc11.2.0-OL7.6+1.0\",\n+        linux_aarch64: \"gcc13.2.0-OL7.6+1.0\",\n","filename":"make\/conf\/jib-profiles.js","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2591,1 +2591,1 @@\n-  if (UseSVE == 0 || !VectorNode::is_invariant_vector(m)) {\n+  if (UseSVE == 0 || m->Opcode() != Op_Replicate) {\n","filename":"src\/hotspot\/cpu\/aarch64\/aarch64.ad","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -43,0 +43,1 @@\n+#include \"oops\/resolvedMethodEntry.hpp\"\n@@ -206,58 +207,0 @@\n-\/\/ Return\n-\/\/ Rindex: index into constant pool\n-\/\/ Rcache: address of cache entry - ConstantPoolCache::base_offset()\n-\/\/\n-\/\/ A caller must add ConstantPoolCache::base_offset() to Rcache to get\n-\/\/ the true address of the cache entry.\n-\/\/\n-void InterpreterMacroAssembler::get_cache_and_index_at_bcp(Register cache,\n-                                                           Register index,\n-                                                           int bcp_offset,\n-                                                           size_t index_size) {\n-  assert_different_registers(cache, index);\n-  assert_different_registers(cache, rcpool);\n-  get_cache_index_at_bcp(index, bcp_offset, index_size);\n-  assert(sizeof(ConstantPoolCacheEntry) == 4 * wordSize, \"adjust code below\");\n-  \/\/ convert from field index to ConstantPoolCacheEntry\n-  \/\/ aarch64 already has the cache in rcpool so there is no need to\n-  \/\/ install it in cache. instead we pre-add the indexed offset to\n-  \/\/ rcpool and return it in cache. All clients of this method need to\n-  \/\/ be modified accordingly.\n-  add(cache, rcpool, index, Assembler::LSL, 5);\n-}\n-\n-\n-void InterpreterMacroAssembler::get_cache_and_index_and_bytecode_at_bcp(Register cache,\n-                                                                        Register index,\n-                                                                        Register bytecode,\n-                                                                        int byte_no,\n-                                                                        int bcp_offset,\n-                                                                        size_t index_size) {\n-  get_cache_and_index_at_bcp(cache, index, bcp_offset, index_size);\n-  \/\/ We use a 32-bit load here since the layout of 64-bit words on\n-  \/\/ little-endian machines allow us that.\n-  \/\/ n.b. unlike x86 cache already includes the index offset\n-  lea(bytecode, Address(cache,\n-                         ConstantPoolCache::base_offset()\n-                         + ConstantPoolCacheEntry::indices_offset()));\n-  ldarw(bytecode, bytecode);\n-  const int shift_count = (1 + byte_no) * BitsPerByte;\n-  ubfx(bytecode, bytecode, shift_count, BitsPerByte);\n-}\n-\n-void InterpreterMacroAssembler::get_cache_entry_pointer_at_bcp(Register cache,\n-                                                               Register tmp,\n-                                                               int bcp_offset,\n-                                                               size_t index_size) {\n-  assert(cache != tmp, \"must use different register\");\n-  get_cache_index_at_bcp(tmp, bcp_offset, index_size);\n-  assert(sizeof(ConstantPoolCacheEntry) == 4 * wordSize, \"adjust code below\");\n-  \/\/ convert from field index to ConstantPoolCacheEntry index\n-  \/\/ and from word offset to byte offset\n-  assert(exact_log2(in_bytes(ConstantPoolCacheEntry::size_in_bytes())) == 2 + LogBytesPerWord, \"else change next line\");\n-  ldr(cache, Address(rfp, frame::interpreter_frame_cache_offset * wordSize));\n-  \/\/ skip past the header\n-  add(cache, cache, in_bytes(ConstantPoolCache::base_offset()));\n-  add(cache, cache, tmp, Assembler::LSL, 2 + LogBytesPerWord);  \/\/ construct pointer to cache entry\n-}\n-\n@@ -363,12 +306,0 @@\n-void InterpreterMacroAssembler::load_resolved_method_at_index(int byte_no,\n-                                                              Register method,\n-                                                              Register cache) {\n-  const int method_offset = in_bytes(\n-    ConstantPoolCache::base_offset() +\n-      ((byte_no == TemplateTable::f2_byte)\n-       ? ConstantPoolCacheEntry::f2_offset()\n-       : ConstantPoolCacheEntry::f1_offset()));\n-\n-  ldr(method, Address(cache, method_offset)); \/\/ get f1 Method*\n-}\n-\n@@ -2087,0 +2018,12 @@\n+\n+void InterpreterMacroAssembler::load_method_entry(Register cache, Register index, int bcp_offset) {\n+  \/\/ Get index out of bytecode pointer\n+  get_cache_index_at_bcp(index, bcp_offset, sizeof(u2));\n+  mov(cache, sizeof(ResolvedMethodEntry));\n+  mul(index, index, cache); \/\/ Scale the index to be the entry index * sizeof(ResolvedMethodEntry)\n+\n+  \/\/ Get address of field entries array\n+  ldr(cache, Address(rcpool, ConstantPoolCache::method_entries_offset()));\n+  add(cache, cache, Array<ResolvedMethodEntry>::base_offset_in_bytes());\n+  lea(cache, Address(cache, index));\n+}\n","filename":"src\/hotspot\/cpu\/aarch64\/interp_masm_aarch64.cpp","additions":13,"deletions":70,"binary":false,"changes":83,"status":"modified"},{"patch":"@@ -143,3 +143,0 @@\n-  void get_cache_and_index_at_bcp(Register cache, Register index, int bcp_offset, size_t index_size = sizeof(u2));\n-  void get_cache_and_index_and_bytecode_at_bcp(Register cache, Register index, Register bytecode, int byte_no, int bcp_offset, size_t index_size = sizeof(u2));\n-  void get_cache_entry_pointer_at_bcp(Register cache, Register tmp, int bcp_offset, size_t index_size = sizeof(u2));\n@@ -177,2 +174,0 @@\n-  void load_resolved_method_at_index(int byte_no, Register method, Register cache);\n-\n@@ -353,0 +348,1 @@\n+  void load_method_entry(Register cache, Register index, int bcp_offset = 1);\n","filename":"src\/hotspot\/cpu\/aarch64\/interp_masm_aarch64.hpp","additions":1,"deletions":5,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -46,0 +46,1 @@\n+#include \"oops\/resolvedMethodEntry.hpp\"\n@@ -502,4 +503,3 @@\n-    __ get_cache_and_index_at_bcp(cache, index, 1, index_size);\n-    __ ldr(cache, Address(cache, ConstantPoolCache::base_offset() + ConstantPoolCacheEntry::flags_offset()));\n-    __ andr(cache, cache, ConstantPoolCacheEntry::parameter_size_mask);\n-\n+    assert(index_size == sizeof(u2), \"Can only be u2\");\n+    __ load_method_entry(cache, index);\n+    __ load_unsigned_short(cache, Address(cache, in_bytes(ResolvedMethodEntry::num_parameters_offset())));\n","filename":"src\/hotspot\/cpu\/aarch64\/templateInterpreterGenerator_aarch64.cpp","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -43,0 +43,1 @@\n+#include \"oops\/resolvedMethodEntry.hpp\"\n@@ -493,1 +494,1 @@\n-  __ andw(off, off, ConstantPoolCacheEntry::field_index_mask);\n+  __ andw(off, off, ConstantPoolCache::field_index_mask);\n@@ -500,2 +501,2 @@\n-  __ ubfxw(flags, flags, ConstantPoolCacheEntry::tos_state_shift,\n-           ConstantPoolCacheEntry::tos_state_bits);\n+  __ ubfxw(flags, flags, ConstantPoolCache::tos_state_shift,\n+           ConstantPoolCache::tos_state_bits);\n@@ -2390,1 +2391,1 @@\n-void TemplateTable::resolve_cache_and_index(int byte_no,\n+void TemplateTable::resolve_cache_and_index_for_method(int byte_no,\n@@ -2392,2 +2393,1 @@\n-                                            Register index,\n-                                            size_t index_size) {\n+                                            Register index) {\n@@ -2396,0 +2396,1 @@\n+  assert(byte_no == f1_byte || byte_no == f2_byte, \"byte_no out of range\");\n@@ -2400,3 +2401,11 @@\n-\n-  assert(byte_no == f1_byte || byte_no == f2_byte, \"byte_no out of range\");\n-  __ get_cache_and_index_and_bytecode_at_bcp(Rcache, index, temp, byte_no, 1, index_size);\n+  __ load_method_entry(Rcache, index);\n+  switch(byte_no) {\n+    case f1_byte:\n+      __ lea(temp, Address(Rcache, in_bytes(ResolvedMethodEntry::bytecode1_offset())));\n+      break;\n+    case f2_byte:\n+      __ lea(temp, Address(Rcache, in_bytes(ResolvedMethodEntry::bytecode2_offset())));\n+      break;\n+  }\n+  \/\/ Load-acquire the bytecode to match store-release in InterpreterRuntime\n+  __ ldarb(temp, temp);\n@@ -2414,1 +2423,1 @@\n-  __ get_cache_and_index_at_bcp(Rcache, index, 1, index_size);\n+  __ load_method_entry(Rcache, index);\n@@ -2421,1 +2430,1 @@\n-    __ load_resolved_method_at_index(byte_no, temp, Rcache);\n+    __ ldr(temp, Address(Rcache, in_bytes(ResolvedMethodEntry::method_offset())));\n@@ -2490,9 +2499,3 @@\n-\/\/ The Rcache and index registers must be set before call\n-\/\/ n.b unlike x86 cache already includes the index offset\n-void TemplateTable::load_field_cp_cache_entry(Register obj,\n-                                              Register cache,\n-                                              Register index,\n-                                              Register off,\n-                                              Register flags,\n-                                              bool is_static = false) {\n-  assert_different_registers(cache, index, flags, off);\n+void TemplateTable::load_resolved_method_entry_special_or_static(Register cache,\n+                                                                 Register method,\n+                                                                 Register flags) {\n@@ -2500,7 +2503,3 @@\n-  ByteSize cp_base_offset = ConstantPoolCache::base_offset();\n-  \/\/ Field offset\n-  __ ldr(off, Address(cache, in_bytes(cp_base_offset +\n-                                      ConstantPoolCacheEntry::f2_offset())));\n-  \/\/ Flags\n-  __ ldrw(flags, Address(cache, in_bytes(cp_base_offset +\n-                                         ConstantPoolCacheEntry::flags_offset())));\n+  \/\/ setup registers\n+  const Register index = flags;\n+  assert_different_registers(method, cache, flags);\n@@ -2508,8 +2507,89 @@\n-  \/\/ klass overwrite register\n-  if (is_static) {\n-    __ ldr(obj, Address(cache, in_bytes(cp_base_offset +\n-                                        ConstantPoolCacheEntry::f1_offset())));\n-    const int mirror_offset = in_bytes(Klass::java_mirror_offset());\n-    __ ldr(obj, Address(obj, mirror_offset));\n-    __ resolve_oop_handle(obj, r5, rscratch2);\n-  }\n+  \/\/ determine constant pool cache field offsets\n+  resolve_cache_and_index_for_method(f1_byte, cache, index);\n+  __ load_unsigned_byte(flags, Address(cache, in_bytes(ResolvedMethodEntry::flags_offset())));\n+  __ ldr(method, Address(cache, in_bytes(ResolvedMethodEntry::method_offset())));\n+}\n+\n+void TemplateTable::load_resolved_method_entry_handle(Register cache,\n+                                                      Register method,\n+                                                      Register ref_index,\n+                                                      Register flags) {\n+  \/\/ setup registers\n+  const Register index = ref_index;\n+  assert_different_registers(method, flags);\n+  assert_different_registers(method, cache, index);\n+\n+  \/\/ determine constant pool cache field offsets\n+  resolve_cache_and_index_for_method(f1_byte, cache, index);\n+  __ load_unsigned_byte(flags, Address(cache, in_bytes(ResolvedMethodEntry::flags_offset())));\n+\n+  \/\/ maybe push appendix to arguments (just before return address)\n+  Label L_no_push;\n+  __ tbz(flags, ResolvedMethodEntry::has_appendix_shift, L_no_push);\n+  \/\/ invokehandle uses an index into the resolved references array\n+  __ load_unsigned_short(ref_index, Address(cache, in_bytes(ResolvedMethodEntry::resolved_references_index_offset())));\n+  \/\/ Push the appendix as a trailing parameter.\n+  \/\/ This must be done before we get the receiver,\n+  \/\/ since the parameter_size includes it.\n+  Register appendix = method;\n+  __ load_resolved_reference_at_index(appendix, ref_index);\n+  __ push(appendix);  \/\/ push appendix (MethodType, CallSite, etc.)\n+  __ bind(L_no_push);\n+\n+  __ ldr(method, Address(cache, in_bytes(ResolvedMethodEntry::method_offset())));\n+}\n+\n+void TemplateTable::load_resolved_method_entry_interface(Register cache,\n+                                                         Register klass,\n+                                                         Register method_or_table_index,\n+                                                         Register flags) {\n+  \/\/ setup registers\n+  const Register index = method_or_table_index;\n+  assert_different_registers(method_or_table_index, cache, flags);\n+\n+  \/\/ determine constant pool cache field offsets\n+  resolve_cache_and_index_for_method(f1_byte, cache, index);\n+  __ load_unsigned_byte(flags, Address(cache, in_bytes(ResolvedMethodEntry::flags_offset())));\n+\n+  \/\/ Invokeinterface can behave in different ways:\n+  \/\/ If calling a method from java.lang.Object, the forced virtual flag is true so the invocation will\n+  \/\/ behave like an invokevirtual call. The state of the virtual final flag will determine whether a method or\n+  \/\/ vtable index is placed in the register.\n+  \/\/ Otherwise, the registers will be populated with the klass and method.\n+\n+  Label NotVirtual; Label NotVFinal; Label Done;\n+  __ tbz(flags, ResolvedMethodEntry::is_forced_virtual_shift, NotVirtual);\n+  __ tbz(flags, ResolvedMethodEntry::is_vfinal_shift, NotVFinal);\n+  __ ldr(method_or_table_index, Address(cache, in_bytes(ResolvedMethodEntry::method_offset())));\n+  __ b(Done);\n+\n+  __ bind(NotVFinal);\n+  __ load_unsigned_short(method_or_table_index, Address(cache, in_bytes(ResolvedMethodEntry::table_index_offset())));\n+  __ b(Done);\n+\n+  __ bind(NotVirtual);\n+  __ ldr(method_or_table_index, Address(cache, in_bytes(ResolvedMethodEntry::method_offset())));\n+  __ ldr(klass, Address(cache, in_bytes(ResolvedMethodEntry::klass_offset())));\n+  __ bind(Done);\n+}\n+\n+void TemplateTable::load_resolved_method_entry_virtual(Register cache,\n+                                                       Register method_or_table_index,\n+                                                       Register flags) {\n+  \/\/ setup registers\n+  const Register index = flags;\n+  assert_different_registers(method_or_table_index, cache, flags);\n+\n+  \/\/ determine constant pool cache field offsets\n+  resolve_cache_and_index_for_method(f2_byte, cache, index);\n+  __ load_unsigned_byte(flags, Address(cache, in_bytes(ResolvedMethodEntry::flags_offset())));\n+\n+  \/\/ method_or_table_index can either be an itable index or a method depending on the virtual final flag\n+  Label NotVFinal; Label Done;\n+  __ tbz(flags, ResolvedMethodEntry::is_vfinal_shift, NotVFinal);\n+  __ ldr(method_or_table_index, Address(cache, in_bytes(ResolvedMethodEntry::method_offset())));\n+  __ b(Done);\n+\n+  __ bind(NotVFinal);\n+  __ load_unsigned_short(method_or_table_index, Address(cache, in_bytes(ResolvedMethodEntry::table_index_offset())));\n+  __ bind(Done);\n@@ -2587,38 +2667,0 @@\n-void TemplateTable::load_invoke_cp_cache_entry(int byte_no,\n-                                               Register method,\n-                                               Register itable_index,\n-                                               Register flags,\n-                                               bool is_invokevirtual,\n-                                               bool is_invokevfinal, \/*unused*\/\n-                                               bool is_invokedynamic \/*unused*\/) {\n-  \/\/ setup registers\n-  const Register cache = rscratch2;\n-  const Register index = r4;\n-  assert_different_registers(method, flags);\n-  assert_different_registers(method, cache, index);\n-  assert_different_registers(itable_index, flags);\n-  assert_different_registers(itable_index, cache, index);\n-  \/\/ determine constant pool cache field offsets\n-  assert(is_invokevirtual == (byte_no == f2_byte), \"is_invokevirtual flag redundant\");\n-  const int method_offset = in_bytes(\n-    ConstantPoolCache::base_offset() +\n-      (is_invokevirtual\n-       ? ConstantPoolCacheEntry::f2_offset()\n-       : ConstantPoolCacheEntry::f1_offset()));\n-  const int flags_offset = in_bytes(ConstantPoolCache::base_offset() +\n-                                    ConstantPoolCacheEntry::flags_offset());\n-  \/\/ access constant pool cache fields\n-  const int index_offset = in_bytes(ConstantPoolCache::base_offset() +\n-                                    ConstantPoolCacheEntry::f2_offset());\n-\n-  size_t index_size = sizeof(u2);\n-  resolve_cache_and_index(byte_no, cache, index, index_size);\n-  __ ldr(method, Address(cache, method_offset));\n-\n-  if (itable_index != noreg) {\n-    __ ldr(itable_index, Address(cache, index_offset));\n-  }\n-  __ ldrw(flags, Address(cache, flags_offset));\n-}\n-\n-\n@@ -3533,7 +3575,2 @@\n-void TemplateTable::prepare_invoke(int byte_no,\n-                                   Register method, \/\/ linked method (or i-klass)\n-                                   Register index,  \/\/ itable index, MethodType, etc.\n-                                   Register recv,   \/\/ if caller wants to see it\n-                                   Register flags   \/\/ if caller wants to test it\n-                                   ) {\n-  \/\/ determine flags\n+void TemplateTable::prepare_invoke(Register cache, Register recv) {\n+\n@@ -3541,16 +3578,1 @@\n-  const bool is_invokeinterface  = code == Bytecodes::_invokeinterface;\n-  const bool is_invokedynamic    = code == Bytecodes::_invokedynamic;\n-  const bool is_invokehandle     = code == Bytecodes::_invokehandle;\n-  const bool is_invokevirtual    = code == Bytecodes::_invokevirtual;\n-  const bool is_invokespecial    = code == Bytecodes::_invokespecial;\n-  const bool load_receiver       = (recv  != noreg);\n-  const bool save_flags          = (flags != noreg);\n-  assert(load_receiver == (code != Bytecodes::_invokestatic && code != Bytecodes::_invokedynamic), \"\");\n-  assert(save_flags    == (is_invokeinterface || is_invokevirtual), \"need flags for vfinal\");\n-  assert(flags == noreg || flags == r3, \"\");\n-  assert(recv  == noreg || recv  == r2, \"\");\n-\n-  \/\/ setup registers & access constant pool cache\n-  if (recv  == noreg)  recv  = r2;\n-  if (flags == noreg)  flags = r3;\n-  assert_different_registers(method, index, recv, flags);\n+  const bool load_receiver       = (code != Bytecodes::_invokestatic) && (code != Bytecodes::_invokedynamic);\n@@ -3561,16 +3583,2 @@\n-  load_invoke_cp_cache_entry(byte_no, method, index, flags, is_invokevirtual, false, is_invokedynamic);\n-\n-  \/\/ maybe push appendix to arguments (just before return address)\n-  if (is_invokehandle) {\n-    Label L_no_push;\n-    __ tbz(flags, ConstantPoolCacheEntry::has_appendix_shift, L_no_push);\n-    \/\/ Push the appendix as a trailing parameter.\n-    \/\/ This must be done before we get the receiver,\n-    \/\/ since the parameter_size includes it.\n-    __ push(r19);\n-    __ mov(r19, index);\n-    __ load_resolved_reference_at_index(index, r19);\n-    __ pop(r19);\n-    __ push(index);  \/\/ push appendix (MethodType, CallSite, etc.)\n-    __ bind(L_no_push);\n-  }\n+  \/\/ Load TOS state for later\n+  __ load_unsigned_byte(rscratch2, Address(cache, in_bytes(ResolvedMethodEntry::type_offset())));\n@@ -3580,7 +3588,2 @@\n-    __ andw(recv, flags, ConstantPoolCacheEntry::parameter_size_mask);\n-    \/\/ FIXME -- is this actually correct? looks like it should be 2\n-    \/\/ const int no_return_pc_pushed_yet = -1;  \/\/ argument slot correction before we push return address\n-    \/\/ const int receiver_is_at_end      = -1;  \/\/ back off one slot to get receiver\n-    \/\/ Address recv_addr = __ argument_address(recv, no_return_pc_pushed_yet + receiver_is_at_end);\n-    \/\/ __ movptr(recv, recv_addr);\n-    __ add(rscratch1, esp, recv, ext::uxtx, 3); \/\/ FIXME: uxtb here?\n+    __ load_unsigned_short(recv, Address(cache, in_bytes(ResolvedMethodEntry::num_parameters_offset())));\n+    __ add(rscratch1, esp, recv, ext::uxtx, 3);\n@@ -3591,4 +3594,0 @@\n-  \/\/ compute return type\n-  \/\/ x86 uses a shift and mask or wings it with a shift plus assert\n-  \/\/ the mask is not needed. aarch64 just uses bitfield extract\n-  __ ubfxw(rscratch2, flags, ConstantPoolCacheEntry::tos_state_shift,  ConstantPoolCacheEntry::tos_state_bits);\n@@ -3612,1 +3611,1 @@\n-  __ tbz(flags, ConstantPoolCacheEntry::is_vfinal_shift, notFinal);\n+  __ tbz(flags, ResolvedMethodEntry::is_vfinal_shift, notFinal);\n@@ -3651,1 +3650,4 @@\n-  prepare_invoke(byte_no, rmethod, noreg, r2, r3);\n+  load_resolved_method_entry_virtual(r2,      \/\/ ResolvedMethodEntry*\n+                                     rmethod, \/\/ Method* or itable index\n+                                     r3);     \/\/ flags\n+  prepare_invoke(r2, r2); \/\/ recv\n@@ -3665,2 +3667,4 @@\n-  prepare_invoke(byte_no, rmethod, noreg,  \/\/ get f1 Method*\n-                 r2);  \/\/ get receiver also for null check\n+  load_resolved_method_entry_special_or_static(r2,      \/\/ ResolvedMethodEntry*\n+                                               rmethod, \/\/ Method*\n+                                               r3);     \/\/ flags\n+  prepare_invoke(r2, r2);  \/\/ get receiver also for null check\n@@ -3680,1 +3684,5 @@\n-  prepare_invoke(byte_no, rmethod);  \/\/ get f1 Method*\n+  load_resolved_method_entry_special_or_static(r2,      \/\/ ResolvedMethodEntry*\n+                                               rmethod, \/\/ Method*\n+                                               r3);     \/\/ flags\n+  prepare_invoke(r2, r2);  \/\/ get receiver also for null check\n+\n@@ -3696,2 +3704,5 @@\n-  prepare_invoke(byte_no, r0, rmethod,  \/\/ get f1 Klass*, f2 Method*\n-                 r2, r3); \/\/ recv, flags\n+  load_resolved_method_entry_interface(r2,      \/\/ ResolvedMethodEntry*\n+                                       r0,      \/\/ Klass*\n+                                       rmethod, \/\/ Method* or itable\/vtable index\n+                                       r3);     \/\/ flags\n+  prepare_invoke(r2, r2); \/\/ receiver\n@@ -3710,1 +3721,1 @@\n-  __ tbz(r3, ConstantPoolCacheEntry::is_forced_virtual_shift, notObjectMethod);\n+  __ tbz(r3, ResolvedMethodEntry::is_forced_virtual_shift, notObjectMethod);\n@@ -3719,1 +3730,1 @@\n-  __ tbz(r3, ConstantPoolCacheEntry::is_vfinal_shift, notVFinal);\n+  __ tbz(r3, ResolvedMethodEntry::is_vfinal_shift, notVFinal);\n@@ -3816,1 +3827,6 @@\n-  prepare_invoke(byte_no, rmethod, r0, r2);\n+  load_resolved_method_entry_handle(r2,      \/\/ ResolvedMethodEntry*\n+                                    rmethod, \/\/ Method*\n+                                    r0,      \/\/ Resolved reference\n+                                    r3);     \/\/ flags\n+  prepare_invoke(r2, r2);\n+\n@@ -3838,1 +3854,1 @@\n-  \/\/ rmethod: MH.linkToCallSite method (from f2)\n+  \/\/ rmethod: MH.linkToCallSite method\n@@ -3840,1 +3856,1 @@\n-  \/\/ Note:  r0_callsite is already pushed by prepare_invoke\n+  \/\/ Note:  r0_callsite is already pushed\n","filename":"src\/hotspot\/cpu\/aarch64\/templateTable_aarch64.cpp","additions":151,"deletions":135,"binary":false,"changes":286,"status":"modified"},{"patch":"@@ -29,6 +29,1 @@\n-static void prepare_invoke(int byte_no,\n-                             Register method,         \/\/ linked method (or i-klass)\n-                             Register index = noreg,  \/\/ itable index, MethodType, etc.\n-                             Register recv  = noreg,  \/\/ if caller wants to see it\n-                             Register flags = noreg   \/\/ if caller wants to test it\n-                             );\n+  static void prepare_invoke(Register cache, Register recv);\n","filename":"src\/hotspot\/cpu\/aarch64\/templateTable_aarch64.hpp","additions":1,"deletions":6,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -37,0 +37,1 @@\n+#include \"oops\/resolvedMethodEntry.hpp\"\n@@ -456,7 +457,0 @@\n-void InterpreterMacroAssembler::get_cache_and_index_at_bcp(Register cache, int bcp_offset,\n-                                                           size_t index_size) {\n-  get_cache_index_at_bcp(cache, bcp_offset, index_size);\n-  sldi(cache, cache, exact_log2(in_words(ConstantPoolCacheEntry::size()) * BytesPerWord));\n-  add(cache, R27_constPoolCache, cache);\n-}\n-\n@@ -487,1 +481,1 @@\n-  \/\/ Get index out of bytecode pointer, get_cache_entry_pointer_at_bcp\n+  \/\/ Get index out of bytecode pointer\n@@ -494,0 +488,1 @@\n+  addi(cache, cache, Array<ResolvedIndyEntry>::base_offset_in_bytes());\n@@ -514,0 +509,12 @@\n+void InterpreterMacroAssembler::load_method_entry(Register cache, Register index, int bcp_offset) {\n+  \/\/ Get index out of bytecode pointer\n+  get_cache_index_at_bcp(index, bcp_offset, sizeof(u2));\n+  \/\/ Scale the index to be the entry index * sizeof(ResolvedMethodEntry)\n+  mulli(index, index, sizeof(ResolvedMethodEntry));\n+\n+  \/\/ Get address of field entries array\n+  ld_ptr(cache, ConstantPoolCache::method_entries_offset(), R27_constPoolCache);\n+  addi(cache, cache, Array<ResolvedMethodEntry>::base_offset_in_bytes());\n+  add(cache, cache, index); \/\/ method_entries + base_offset + scaled index\n+}\n+\n@@ -567,12 +574,0 @@\n-void InterpreterMacroAssembler::load_resolved_method_at_index(int byte_no,\n-                                                              Register cache,\n-                                                              Register method) {\n-  const int method_offset = in_bytes(\n-    ConstantPoolCache::base_offset() +\n-      ((byte_no == TemplateTable::f2_byte)\n-       ? ConstantPoolCacheEntry::f2_offset()\n-       : ConstantPoolCacheEntry::f1_offset()));\n-\n-  ld(method, method_offset, cache); \/\/ get f1 Method*\n-}\n-\n@@ -1981,1 +1976,1 @@\n-\/\/ Add a InterpMonitorElem to stack (see frame_sparc.hpp).\n+\/\/ Add a monitor (see frame_ppc.hpp).\n","filename":"src\/hotspot\/cpu\/ppc\/interp_masm_ppc_64.cpp","additions":16,"deletions":21,"binary":false,"changes":37,"status":"modified"},{"patch":"@@ -41,0 +41,1 @@\n+#include \"oops\/resolvedMethodEntry.hpp\"\n@@ -340,12 +341,0 @@\n-\n-void InterpreterMacroAssembler::get_cache_and_index_at_bcp(Register cache, Register cpe_offset,\n-                                                           int bcp_offset, size_t index_size) {\n-  BLOCK_COMMENT(\"get_cache_and_index_at_bcp {\");\n-  assert_different_registers(cache, cpe_offset);\n-  get_cache_index_at_bcp(cpe_offset, bcp_offset, index_size);\n-  z_lg(cache, Address(Z_fp, _z_ijava_state_neg(cpoolCache)));\n-  \/\/ Convert from field index to ConstantPoolCache offset in bytes.\n-  z_sllg(cpe_offset, cpe_offset, exact_log2(in_words(ConstantPoolCacheEntry::size()) * BytesPerWord));\n-  BLOCK_COMMENT(\"}\");\n-}\n-\n@@ -392,18 +381,3 @@\n-\/\/ Kills Z_R0_scratch.\n-void InterpreterMacroAssembler::get_cache_and_index_and_bytecode_at_bcp(Register cache,\n-                                                                        Register cpe_offset,\n-                                                                        Register bytecode,\n-                                                                        int byte_no,\n-                                                                        int bcp_offset,\n-                                                                        size_t index_size) {\n-  BLOCK_COMMENT(\"get_cache_and_index_and_bytecode_at_bcp {\");\n-  get_cache_and_index_at_bcp(cache, cpe_offset, bcp_offset, index_size);\n-\n-  \/\/ We want to load (from CP cache) the bytecode that corresponds to the passed-in byte_no.\n-  \/\/ It is located at (cache + cpe_offset + base_offset + indices_offset + (8-1) (last byte in DW) - (byte_no+1).\n-  \/\/ Instead of loading, shifting and masking a DW, we just load that one byte of interest with z_llgc (unsigned).\n-  const int base_ix_off = in_bytes(ConstantPoolCache::base_offset() + ConstantPoolCacheEntry::indices_offset());\n-  const int off_in_DW   = (8-1) - (1+byte_no);\n-  assert(ConstantPoolCacheEntry::bytecode_1_mask == ConstantPoolCacheEntry::bytecode_2_mask, \"common mask\");\n-  assert(ConstantPoolCacheEntry::bytecode_1_mask == 0xff, \"\");\n-  load_sized_value(bytecode, Address(cache, cpe_offset, base_ix_off+off_in_DW), 1, false \/*signed*\/);\n+void InterpreterMacroAssembler::load_method_entry(Register cache, Register index, int bcp_offset) {\n+  \/\/ Get field index out of bytecode pointer.\n+  get_cache_index_at_bcp(index, bcp_offset, sizeof(u2));\n@@ -411,1 +385,14 @@\n-  BLOCK_COMMENT(\"}\");\n+  \/\/ Get the address of the ResolvedMethodEntry array.\n+  get_constant_pool_cache(cache);\n+  z_lg(cache, Address(cache, in_bytes(ConstantPoolCache::method_entries_offset())));\n+\n+  \/\/ Scale the index to form a byte offset into the ResolvedMethodEntry array\n+  size_t entry_size = sizeof(ResolvedMethodEntry);\n+  if (is_power_of_2(entry_size)) {\n+    z_sllg(index, index, exact_log2(entry_size));\n+  } else {\n+    z_mghi(index, entry_size);\n+  }\n+\n+  \/\/ Calculate the final field address.\n+  z_la(cache, Array<ResolvedMethodEntry>::base_offset_in_bytes(), index, cache);\n@@ -451,23 +438,0 @@\n-void InterpreterMacroAssembler::get_cache_entry_pointer_at_bcp(Register cache,\n-                                                               Register tmp,\n-                                                               int bcp_offset,\n-                                                               size_t index_size) {\n-  BLOCK_COMMENT(\"get_cache_entry_pointer_at_bcp {\");\n-    get_cache_and_index_at_bcp(cache, tmp, bcp_offset, index_size);\n-    add2reg_with_index(cache, in_bytes(ConstantPoolCache::base_offset()), tmp, cache);\n-  BLOCK_COMMENT(\"}\");\n-}\n-\n-void InterpreterMacroAssembler::load_resolved_method_at_index(int byte_no,\n-                                                              Register cache,\n-                                                              Register cpe_offset,\n-                                                              Register method) {\n-  const int method_offset = in_bytes(\n-    ConstantPoolCache::base_offset() +\n-      ((byte_no == TemplateTable::f2_byte)\n-       ? ConstantPoolCacheEntry::f2_offset()\n-       : ConstantPoolCacheEntry::f1_offset()));\n-\n-  z_lg(method, Address(cache, cpe_offset, method_offset)); \/\/ get f1 Method*\n-}\n-\n","filename":"src\/hotspot\/cpu\/s390\/interp_masm_s390.cpp","additions":18,"deletions":54,"binary":false,"changes":72,"status":"modified"},{"patch":"@@ -220,0 +220,4 @@\n+  \/* Autodetected, see vm_version_x86.cpp *\/                                \\\n+  product(bool, EnableX86ECoreOpts, false, DIAGNOSTIC,                      \\\n+          \"Perform Ecore Optimization\")                                     \\\n+                                                                            \\\n","filename":"src\/hotspot\/cpu\/x86\/globals_x86.hpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -39,0 +39,1 @@\n+#include \"oops\/resolvedMethodEntry.hpp\"\n@@ -459,50 +460,0 @@\n-void InterpreterMacroAssembler::get_cache_and_index_at_bcp(Register cache,\n-                                                           Register index,\n-                                                           int bcp_offset,\n-                                                           size_t index_size) {\n-  assert_different_registers(cache, index);\n-  get_cache_index_at_bcp(index, bcp_offset, index_size);\n-  movptr(cache, Address(rbp, frame::interpreter_frame_cache_offset * wordSize));\n-  assert(sizeof(ConstantPoolCacheEntry) == 4 * wordSize, \"adjust code below\");\n-  \/\/ convert from field index to ConstantPoolCacheEntry index\n-  assert(exact_log2(in_words(ConstantPoolCacheEntry::size())) == 2, \"else change next line\");\n-  shll(index, 2);\n-}\n-\n-void InterpreterMacroAssembler::get_cache_and_index_and_bytecode_at_bcp(Register cache,\n-                                                                        Register index,\n-                                                                        Register bytecode,\n-                                                                        int byte_no,\n-                                                                        int bcp_offset,\n-                                                                        size_t index_size) {\n-  get_cache_and_index_at_bcp(cache, index, bcp_offset, index_size);\n-  \/\/ We use a 32-bit load here since the layout of 64-bit words on\n-  \/\/ little-endian machines allow us that.\n-  movl(bytecode, Address(cache, index, Address::times_ptr, ConstantPoolCache::base_offset() + ConstantPoolCacheEntry::indices_offset()));\n-  const int shift_count = (1 + byte_no) * BitsPerByte;\n-  assert((byte_no == TemplateTable::f1_byte && shift_count == ConstantPoolCacheEntry::bytecode_1_shift) ||\n-         (byte_no == TemplateTable::f2_byte && shift_count == ConstantPoolCacheEntry::bytecode_2_shift),\n-         \"correct shift count\");\n-  shrl(bytecode, shift_count);\n-  assert(ConstantPoolCacheEntry::bytecode_1_mask == ConstantPoolCacheEntry::bytecode_2_mask, \"common mask\");\n-  andl(bytecode, ConstantPoolCacheEntry::bytecode_1_mask);\n-}\n-\n-void InterpreterMacroAssembler::get_cache_entry_pointer_at_bcp(Register cache,\n-                                                               Register tmp,\n-                                                               int bcp_offset,\n-                                                               size_t index_size) {\n-  assert_different_registers(cache, tmp);\n-\n-  get_cache_index_at_bcp(tmp, bcp_offset, index_size);\n-  assert(sizeof(ConstantPoolCacheEntry) == 4 * wordSize, \"adjust code below\");\n-  \/\/ convert from field index to ConstantPoolCacheEntry index\n-  \/\/ and from word offset to byte offset\n-  assert(exact_log2(in_bytes(ConstantPoolCacheEntry::size_in_bytes())) == 2 + LogBytesPerWord, \"else change next line\");\n-  shll(tmp, 2 + LogBytesPerWord);\n-  movptr(cache, Address(rbp, frame::interpreter_frame_cache_offset * wordSize));\n-  \/\/ skip past the header\n-  addptr(cache, in_bytes(ConstantPoolCache::base_offset()));\n-  addptr(cache, tmp);  \/\/ construct pointer to cache entry\n-}\n-\n@@ -537,15 +488,0 @@\n-void InterpreterMacroAssembler::load_resolved_method_at_index(int byte_no,\n-                                                              Register method,\n-                                                              Register cache,\n-                                                              Register index) {\n-  assert_different_registers(cache, index);\n-\n-  const int method_offset = in_bytes(\n-    ConstantPoolCache::base_offset() +\n-      ((byte_no == TemplateTable::f2_byte)\n-       ? ConstantPoolCacheEntry::f2_offset()\n-       : ConstantPoolCacheEntry::f1_offset()));\n-\n-  movptr(method, Address(cache, index, Address::times_ptr, method_offset)); \/\/ get f1 Method*\n-}\n-\n@@ -2337,1 +2273,1 @@\n-  \/\/ Get index out of bytecode pointer, get_cache_entry_pointer_at_bcp\n+  \/\/ Get index out of bytecode pointer\n@@ -2364,0 +2300,10 @@\n+\n+void InterpreterMacroAssembler::load_method_entry(Register cache, Register index, int bcp_offset) {\n+  \/\/ Get index out of bytecode pointer\n+  movptr(cache, Address(rbp, frame::interpreter_frame_cache_offset * wordSize));\n+  get_cache_index_at_bcp(index, bcp_offset, sizeof(u2));\n+\n+  movptr(cache, Address(cache, ConstantPoolCache::method_entries_offset()));\n+  imull(index, index, sizeof(ResolvedMethodEntry)); \/\/ Scale the index to be the entry index * sizeof(ResolvedMethodEntry)\n+  lea(cache, Address(cache, index, Address::times_1, Array<ResolvedMethodEntry>::base_offset_in_bytes()));\n+}\n","filename":"src\/hotspot\/cpu\/x86\/interp_masm_x86.cpp","additions":12,"deletions":66,"binary":false,"changes":78,"status":"modified"},{"patch":"@@ -106,14 +106,1 @@\n-  void get_cache_and_index_at_bcp(Register cache,\n-                                  Register index,\n-                                  int bcp_offset,\n-                                  size_t index_size = sizeof(u2));\n-  void get_cache_and_index_and_bytecode_at_bcp(Register cache,\n-                                               Register index,\n-                                               Register bytecode,\n-                                               int byte_no,\n-                                               int bcp_offset,\n-                                               size_t index_size = sizeof(u2));\n-  void get_cache_entry_pointer_at_bcp(Register cache,\n-                                      Register tmp,\n-                                      int bcp_offset,\n-                                      size_t index_size = sizeof(u2));\n+\n@@ -132,5 +119,0 @@\n-  void load_resolved_method_at_index(int byte_no,\n-                                     Register method,\n-                                     Register cache,\n-                                     Register index);\n-\n@@ -336,0 +318,1 @@\n+  void load_method_entry(Register cache, Register index, int bcp_offset = 1);\n","filename":"src\/hotspot\/cpu\/x86\/interp_masm_x86.hpp","additions":2,"deletions":19,"binary":false,"changes":21,"status":"modified"},{"patch":"@@ -1349,0 +1349,4 @@\n+#ifdef _LP64\n+  \/\/ Needs full 64-bit immediate for later patching.\n+  mov64(rax, (intptr_t)Universe::non_oop_word());\n+#else\n@@ -1350,0 +1354,1 @@\n+#endif\n@@ -2576,1 +2581,9 @@\n-  LP64_ONLY(mov64(dst, src)) NOT_LP64(movl(dst, src));\n+#ifdef _LP64\n+  if (is_simm32(src)) {\n+    movq(dst, checked_cast<int32_t>(src));\n+  } else {\n+    mov64(dst, src);\n+  }\n+#else\n+  movl(dst, src);\n+#endif\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.cpp","additions":14,"deletions":1,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -3928,4 +3928,4 @@\n-  \/\/ Round to nearest, 64-bit mode, exceptions masked\n-  StubRoutines::x86::_mxcsr_std = 0x1F80;\n-  \/\/ Round to zero, 64-bit mode, exceptions masked\n-  StubRoutines::x86::_mxcsr_rz = 0x7F80;\n+  \/\/ Round to nearest, 64-bit mode, exceptions masked, flags specialized\n+  StubRoutines::x86::_mxcsr_std = EnableX86ECoreOpts ? 0x1FBF : 0x1F80;\n+  \/\/ Round to zero, 64-bit mode, exceptions masked, flags specialized\n+  StubRoutines::x86::_mxcsr_rz = EnableX86ECoreOpts ? 0x7FBF : 0x7F80;\n","filename":"src\/hotspot\/cpu\/x86\/stubGenerator_x86_64.cpp","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -44,0 +44,1 @@\n+#include \"oops\/resolvedMethodEntry.hpp\"\n@@ -238,5 +239,4 @@\n-    __ get_cache_and_index_at_bcp(cache, index, 1, index_size);\n-    Register flags = cache;\n-    __ movl(flags, Address(cache, index, Address::times_ptr, ConstantPoolCache::base_offset() + ConstantPoolCacheEntry::flags_offset()));\n-    __ andl(flags, ConstantPoolCacheEntry::parameter_size_mask);\n-    __ lea(rsp, Address(rsp, flags, Interpreter::stackElementScale()));\n+    assert(index_size == sizeof(u2), \"Can only be u2\");\n+    __ load_method_entry(cache, index);\n+    __ load_unsigned_short(cache, Address(cache, in_bytes(ResolvedMethodEntry::num_parameters_offset())));\n+    __ lea(rsp, Address(rsp, cache, Interpreter::stackElementScale()));\n","filename":"src\/hotspot\/cpu\/x86\/templateInterpreterGenerator_x86.cpp","additions":5,"deletions":5,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -43,0 +43,1 @@\n+#include \"oops\/resolvedMethodEntry.hpp\"\n@@ -530,1 +531,1 @@\n-  __ andl(off, ConstantPoolCacheEntry::field_index_mask);\n+  __ andl(off, ConstantPoolCache::field_index_mask);\n@@ -534,2 +535,2 @@\n-  __ shrl(flags, ConstantPoolCacheEntry::tos_state_shift);\n-  __ andl(flags, ConstantPoolCacheEntry::tos_state_mask);\n+  __ shrl(flags, ConstantPoolCache::tos_state_shift);\n+  __ andl(flags, ConstantPoolCache::tos_state_mask);\n@@ -2779,4 +2780,3 @@\n-void TemplateTable::resolve_cache_and_index(int byte_no,\n-                                            Register cache,\n-                                            Register index,\n-                                            size_t index_size) {\n+void TemplateTable::resolve_cache_and_index_for_method(int byte_no,\n+                                                       Register cache,\n+                                                       Register index) {\n@@ -2792,1 +2792,12 @@\n-  __ get_cache_and_index_and_bytecode_at_bcp(cache, index, temp, byte_no, 1, index_size);\n+\n+  __ load_method_entry(cache, index);\n+  switch(byte_no) {\n+    case f1_byte:\n+      __ load_unsigned_byte(temp, Address(cache, in_bytes(ResolvedMethodEntry::bytecode1_offset())));\n+      break;\n+    case f2_byte:\n+      __ load_unsigned_byte(temp, Address(cache, in_bytes(ResolvedMethodEntry::bytecode2_offset())));\n+      break;\n+    default:\n+      ShouldNotReachHere();\n+  }\n@@ -2803,1 +2814,1 @@\n-  __ get_cache_and_index_at_bcp(cache, index, 1, index_size);\n+  __ load_method_entry(cache, index);\n@@ -2814,1 +2825,1 @@\n-    __ load_resolved_method_at_index(byte_no, method, cache, index);\n+    __ movptr(method, Address(cache, in_bytes(ResolvedMethodEntry::method_offset())));\n@@ -2882,30 +2893,0 @@\n-\/\/ The cache and index registers must be set before call\n-void TemplateTable::load_field_cp_cache_entry(Register obj,\n-                                              Register cache,\n-                                              Register index,\n-                                              Register off,\n-                                              Register flags,\n-                                              bool is_static = false) {\n-  assert_different_registers(cache, index, flags, off);\n-\n-  ByteSize cp_base_offset = ConstantPoolCache::base_offset();\n-  \/\/ Field offset\n-  __ movptr(off, Address(cache, index, Address::times_ptr,\n-                         in_bytes(cp_base_offset +\n-                                  ConstantPoolCacheEntry::f2_offset())));\n-  \/\/ Flags\n-  __ movl(flags, Address(cache, index, Address::times_ptr,\n-                         in_bytes(cp_base_offset +\n-                                  ConstantPoolCacheEntry::flags_offset())));\n-\n-  \/\/ klass overwrite register\n-  if (is_static) {\n-    __ movptr(obj, Address(cache, index, Address::times_ptr,\n-                           in_bytes(cp_base_offset +\n-                                    ConstantPoolCacheEntry::f1_offset())));\n-    const int mirror_offset = in_bytes(Klass::java_mirror_offset());\n-    __ movptr(obj, Address(obj, mirror_offset));\n-    __ resolve_oop_handle(obj, rscratch2);\n-  }\n-}\n-\n@@ -2980,1 +2961,15 @@\n-void TemplateTable::load_invoke_cp_cache_entry(int byte_no,\n+void TemplateTable::load_resolved_method_entry_special_or_static(Register cache,\n+                                                                 Register method,\n+                                                                 Register flags) {\n+  \/\/ setup registers\n+  const Register index = rdx;\n+  assert_different_registers(cache, index);\n+  assert_different_registers(method, cache, flags);\n+\n+  \/\/ determine constant pool cache field offsets\n+  resolve_cache_and_index_for_method(f1_byte, cache, index);\n+  __ load_unsigned_byte(flags, Address(cache, in_bytes(ResolvedMethodEntry::flags_offset())));\n+  __ movptr(method, Address(cache, in_bytes(ResolvedMethodEntry::method_offset())));\n+}\n+\n+void TemplateTable::load_resolved_method_entry_handle(Register cache,\n@@ -2982,5 +2977,2 @@\n-                                               Register itable_index,\n-                                               Register flags,\n-                                               bool is_invokevirtual,\n-                                               bool is_invokevfinal, \/*unused*\/\n-                                               bool is_invokedynamic \/*unused*\/) {\n+                                               Register ref_index,\n+                                               Register flags) {\n@@ -2988,5 +2980,3 @@\n-  const Register cache = rcx;\n-  assert_different_registers(method, flags);\n-  assert_different_registers(method, cache, index);\n-  assert_different_registers(itable_index, flags);\n-  assert_different_registers(itable_index, cache, index);\n+  assert_different_registers(cache, index);\n+  assert_different_registers(cache, method, ref_index, flags);\n+\n@@ -2995,16 +2985,77 @@\n-  assert(is_invokevirtual == (byte_no == f2_byte), \"is_invokevirtual flag redundant\");\n-  const int flags_offset = in_bytes(ConstantPoolCache::base_offset() +\n-                                    ConstantPoolCacheEntry::flags_offset());\n-  \/\/ access constant pool cache fields\n-  const int index_offset = in_bytes(ConstantPoolCache::base_offset() +\n-                                    ConstantPoolCacheEntry::f2_offset());\n-\n-  size_t index_size = sizeof(u2);\n-  resolve_cache_and_index(byte_no, cache, index, index_size);\n-  __ load_resolved_method_at_index(byte_no, method, cache, index);\n-\n-  if (itable_index != noreg) {\n-    \/\/ pick up itable or appendix index from f2 also:\n-    __ movptr(itable_index, Address(cache, index, Address::times_ptr, index_offset));\n-  }\n-  __ movl(flags, Address(cache, index, Address::times_ptr, flags_offset));\n+  resolve_cache_and_index_for_method(f1_byte, cache, index);\n+  __ load_unsigned_byte(flags, Address(cache, in_bytes(ResolvedMethodEntry::flags_offset())));\n+\n+  \/\/ Maybe push appendix\n+  Label L_no_push;\n+  __ testl(flags, (1 << ResolvedMethodEntry::has_appendix_shift));\n+  __ jcc(Assembler::zero, L_no_push);\n+  \/\/ invokehandle uses an index into the resolved references array\n+  __ load_unsigned_short(ref_index, Address(cache, in_bytes(ResolvedMethodEntry::resolved_references_index_offset())));\n+  \/\/ Push the appendix as a trailing parameter.\n+  \/\/ This must be done before we get the receiver,\n+  \/\/ since the parameter_size includes it.\n+  Register appendix = method;\n+  __ load_resolved_reference_at_index(appendix, ref_index);\n+  __ push(appendix);  \/\/ push appendix (MethodType, CallSite, etc.)\n+  __ bind(L_no_push);\n+\n+  __ movptr(method, Address(cache, in_bytes(ResolvedMethodEntry::method_offset())));\n+}\n+\n+void TemplateTable::load_resolved_method_entry_interface(Register cache,\n+                                                         Register klass,\n+                                                         Register method_or_table_index,\n+                                                         Register flags) {\n+  \/\/ setup registers\n+  const Register index = rdx;\n+  assert_different_registers(cache, klass, method_or_table_index, flags);\n+\n+  \/\/ determine constant pool cache field offsets\n+  resolve_cache_and_index_for_method(f1_byte, cache, index);\n+  __ load_unsigned_byte(flags, Address(cache, in_bytes(ResolvedMethodEntry::flags_offset())));\n+\n+  \/\/ Invokeinterface can behave in different ways:\n+  \/\/ If calling a method from java.lang.Object, the forced virtual flag is true so the invocation will\n+  \/\/ behave like an invokevirtual call. The state of the virtual final flag will determine whether a method or\n+  \/\/ vtable index is placed in the register.\n+  \/\/ Otherwise, the registers will be populated with the klass and method.\n+\n+  Label NotVirtual; Label NotVFinal; Label Done;\n+  __ testl(flags, 1 << ResolvedMethodEntry::is_forced_virtual_shift);\n+  __ jcc(Assembler::zero, NotVirtual);\n+  __ testl(flags, (1 << ResolvedMethodEntry::is_vfinal_shift));\n+  __ jcc(Assembler::zero, NotVFinal);\n+  __ movptr(method_or_table_index, Address(cache, in_bytes(ResolvedMethodEntry::method_offset())));\n+  __ jmp(Done);\n+\n+  __ bind(NotVFinal);\n+  __ load_unsigned_short(method_or_table_index, Address(cache, in_bytes(ResolvedMethodEntry::table_index_offset())));\n+  __ jmp(Done);\n+\n+  __ bind(NotVirtual);\n+  __ movptr(method_or_table_index, Address(cache, in_bytes(ResolvedMethodEntry::method_offset())));\n+  __ movptr(klass, Address(cache, in_bytes(ResolvedMethodEntry::klass_offset())));\n+  __ bind(Done);\n+}\n+\n+void TemplateTable::load_resolved_method_entry_virtual(Register cache,\n+                                                       Register method_or_table_index,\n+                                                       Register flags) {\n+  \/\/ setup registers\n+  const Register index = rdx;\n+  assert_different_registers(index, cache);\n+  assert_different_registers(method_or_table_index, cache, flags);\n+\n+  \/\/ determine constant pool cache field offsets\n+  resolve_cache_and_index_for_method(f2_byte, cache, index);\n+  __ load_unsigned_byte(flags, Address(cache, in_bytes(ResolvedMethodEntry::flags_offset())));\n+\n+  \/\/ method_or_table_index can either be an itable index or a method depending on the virtual final flag\n+  Label isVFinal; Label Done;\n+  __ testl(flags, (1 << ResolvedMethodEntry::is_vfinal_shift));\n+  __ jcc(Assembler::notZero, isVFinal);\n+  __ load_unsigned_short(method_or_table_index, Address(cache, in_bytes(ResolvedMethodEntry::table_index_offset())));\n+  __ jmp(Done);\n+  __ bind(isVFinal);\n+  __ movptr(method_or_table_index, Address(cache, in_bytes(ResolvedMethodEntry::method_offset())));\n+  __ bind(Done);\n@@ -3946,6 +3997,1 @@\n-void TemplateTable::prepare_invoke(int byte_no,\n-                                   Register method,  \/\/ linked method (or i-klass)\n-                                   Register index,   \/\/ itable index, MethodType, etc.\n-                                   Register recv,    \/\/ if caller wants to see it\n-                                   Register flags    \/\/ if caller wants to test it\n-                                   ) {\n+void TemplateTable::prepare_invoke(Register cache, Register recv, Register flags) {\n@@ -3954,16 +4000,2 @@\n-  const bool is_invokeinterface  = code == Bytecodes::_invokeinterface;\n-  const bool is_invokedynamic    = code == Bytecodes::_invokedynamic;\n-  const bool is_invokehandle     = code == Bytecodes::_invokehandle;\n-  const bool is_invokevirtual    = code == Bytecodes::_invokevirtual;\n-  const bool is_invokespecial    = code == Bytecodes::_invokespecial;\n-  const bool load_receiver       = (recv  != noreg);\n-  const bool save_flags          = (flags != noreg);\n-  assert(load_receiver == (code != Bytecodes::_invokestatic && code != Bytecodes::_invokedynamic), \"\");\n-  assert(save_flags    == (is_invokeinterface || is_invokevirtual), \"need flags for vfinal\");\n-  assert(flags == noreg || flags == rdx, \"\");\n-  assert(recv  == noreg || recv  == rcx, \"\");\n-\n-  \/\/ setup registers & access constant pool cache\n-  if (recv  == noreg)  recv  = rcx;\n-  if (flags == noreg)  flags = rdx;\n-  assert_different_registers(method, index, recv, flags);\n+  const bool load_receiver       = (code != Bytecodes::_invokestatic) && (code != Bytecodes::_invokedynamic);\n+  assert_different_registers(recv, flags);\n@@ -3974,17 +4006,3 @@\n-  load_invoke_cp_cache_entry(byte_no, method, index, flags, is_invokevirtual, false, is_invokedynamic);\n-\n-  \/\/ maybe push appendix to arguments (just before return address)\n-  if (is_invokehandle) {\n-    Label L_no_push;\n-    __ testl(flags, (1 << ConstantPoolCacheEntry::has_appendix_shift));\n-    __ jcc(Assembler::zero, L_no_push);\n-    \/\/ Push the appendix as a trailing parameter.\n-    \/\/ This must be done before we get the receiver,\n-    \/\/ since the parameter_size includes it.\n-    __ push(rbx);\n-    __ mov(rbx, index);\n-    __ load_resolved_reference_at_index(index, rbx);\n-    __ pop(rbx);\n-    __ push(index);  \/\/ push appendix (MethodType, CallSite, etc.)\n-    __ bind(L_no_push);\n-  }\n+  \/\/ Save flags and load TOS\n+  __ movl(rbcp, flags);\n+  __ load_unsigned_byte(flags, Address(cache, in_bytes(ResolvedMethodEntry::type_offset())));\n@@ -3995,2 +4013,1 @@\n-    __ movl(recv, flags);\n-    __ andl(recv, ConstantPoolCacheEntry::parameter_size_mask);\n+    __ load_unsigned_short(recv, Address(cache, in_bytes(ResolvedMethodEntry::num_parameters_offset())));\n@@ -4004,8 +4021,0 @@\n-  if (save_flags) {\n-    __ movl(rbcp, flags);\n-  }\n-\n-  \/\/ compute return type\n-  __ shrl(flags, ConstantPoolCacheEntry::tos_state_shift);\n-  \/\/ Make sure we don't need to mask flags after the above shift\n-  ConstantPoolCacheEntry::verify_tos_state_shift();\n@@ -4027,1 +4036,1 @@\n-  \/\/ Restore flags value from the constant pool cache, and restore rsi\n+  \/\/ Restore flags value from the constant pool cache entry, and restore rsi\n@@ -4029,4 +4038,2 @@\n-  if (save_flags) {\n-    __ movl(flags, rbcp);\n-    __ restore_bcp();\n-  }\n+  __ movl(flags, rbcp);\n+  __ restore_bcp();\n@@ -4046,1 +4053,1 @@\n-  __ andl(rax, (1 << ConstantPoolCacheEntry::is_vfinal_shift));\n+  __ andl(rax, (1 << ResolvedMethodEntry::is_vfinal_shift));\n@@ -4082,4 +4089,7 @@\n-  prepare_invoke(byte_no,\n-                 rbx,    \/\/ method or vtable index\n-                 noreg,  \/\/ unused itable index\n-                 rcx, rdx); \/\/ recv, flags\n+\n+  load_resolved_method_entry_virtual(rcx,  \/\/ ResolvedMethodEntry*\n+                                     rbx,  \/\/ Method or itable index\n+                                     rdx); \/\/ Flags\n+  prepare_invoke(rcx,  \/\/ ResolvedMethodEntry*\n+                 rcx,  \/\/ Receiver\n+                 rdx); \/\/ flags\n@@ -4090,1 +4100,0 @@\n-\n@@ -4097,2 +4106,8 @@\n-  prepare_invoke(byte_no, rbx, noreg,  \/\/ get f1 Method*\n-                 rcx);  \/\/ get receiver also for null check\n+\n+  load_resolved_method_entry_special_or_static(rcx,  \/\/ ResolvedMethodEntry*\n+                                               rbx,  \/\/ Method*\n+                                               rdx); \/\/ flags\n+  prepare_invoke(rcx,\n+                 rcx,  \/\/ get receiver also for null check\n+                 rdx); \/\/ flags\n+\n@@ -4110,1 +4125,7 @@\n-  prepare_invoke(byte_no, rbx);  \/\/ get f1 Method*\n+\n+  load_resolved_method_entry_special_or_static(rcx, \/\/ ResolvedMethodEntry*\n+                                               rbx, \/\/ Method*\n+                                               rdx  \/\/ flags\n+                                               );\n+  prepare_invoke(rcx, rcx, rdx);  \/\/ cache and flags\n+\n@@ -4128,6 +4149,5 @@\n-  prepare_invoke(byte_no, rax, rbx,  \/\/ get f1 Klass*, f2 Method*\n-                 rcx, rdx); \/\/ recv, flags\n-  \/\/ rax: reference klass (from f1) if interface method\n-  \/\/ rbx: method (from f2)\n-  \/\/ rcx: receiver\n-  \/\/ rdx: flags\n+  load_resolved_method_entry_interface(rcx,  \/\/ ResolvedMethodEntry*\n+                                       rax,  \/\/ Klass*\n+                                       rbx,  \/\/ Method* or itable\/vtable index\n+                                       rdx); \/\/ flags\n+  prepare_invoke(rcx, rcx, rdx); \/\/ receiver, flags\n@@ -4143,1 +4163,1 @@\n-  __ andl(rlocals, (1 << ConstantPoolCacheEntry::is_forced_virtual_shift));\n+  __ andl(rlocals, (1 << ResolvedMethodEntry::is_forced_virtual_shift));\n@@ -4145,0 +4165,1 @@\n+\n@@ -4155,1 +4176,1 @@\n-  __ andl(rlocals, (1 << ConstantPoolCacheEntry::is_vfinal_shift));\n+  __ andl(rlocals, (1 << ResolvedMethodEntry::is_vfinal_shift));\n@@ -4276,1 +4297,3 @@\n-  prepare_invoke(byte_no, rbx_method, rax_mtype, rcx_recv);\n+  load_resolved_method_entry_handle(rcx, rbx_method, rax_mtype, rdx_flags);\n+  prepare_invoke(rcx, rcx_recv, rdx_flags);\n+\n@@ -4282,1 +4305,1 @@\n-  \/\/ rbx: MH.invokeExact_MT method (from f2)\n+  \/\/ rbx: MH.invokeExact_MT method\n@@ -4284,1 +4307,1 @@\n-  \/\/ Note:  rax_mtype is already pushed (if necessary) by prepare_invoke\n+  \/\/ Note:  rax_mtype is already pushed (if necessary)\n@@ -4301,2 +4324,2 @@\n-  \/\/ rax: CallSite object (from cpool->resolved_references[f1])\n-  \/\/ rbx: MH.linkToCallSite method (from f2)\n+  \/\/ rax: CallSite object (from cpool->resolved_references[])\n+  \/\/ rbx: MH.linkToCallSite method\n@@ -4304,1 +4327,1 @@\n-  \/\/ Note:  rax_callsite is already pushed by prepare_invoke\n+  \/\/ Note:  rax_callsite is already pushed\n","filename":"src\/hotspot\/cpu\/x86\/templateTable_x86.cpp","additions":167,"deletions":144,"binary":false,"changes":311,"status":"modified"},{"patch":"@@ -28,6 +28,3 @@\n-  static void prepare_invoke(int byte_no,\n-                             Register method,         \/\/ linked method (or i-klass)\n-                             Register index = noreg,  \/\/ itable index, MethodType, etc.\n-                             Register recv  = noreg,  \/\/ if caller wants to see it\n-                             Register flags = noreg   \/\/ if caller wants to test it\n-                             );\n+  static void prepare_invoke(Register cache,\n+                             Register recv,\n+                             Register flags);\n","filename":"src\/hotspot\/cpu\/x86\/templateTable_x86.hpp","additions":3,"deletions":6,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -860,0 +860,6 @@\n+  \/\/ Check if processor has Intel Ecore\n+  if (FLAG_IS_DEFAULT(EnableX86ECoreOpts) && is_intel() && cpu_family() == 6 &&\n+    (_model == 0x97 || _model == 0xAC || _model == 0xAF)) {\n+    FLAG_SET_DEFAULT(EnableX86ECoreOpts, true);\n+  }\n+\n","filename":"src\/hotspot\/cpu\/x86\/vm_version_x86.cpp","additions":6,"deletions":0,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -4197,2 +4197,2 @@\n-  predicate(UseAVX >= 2);\n-  match(Set dst (ReplicateB src));\n+  predicate(Matcher::vector_element_basic_type(n) == T_BYTE);\n+  match(Set dst (Replicate src));\n@@ -4202,4 +4202,9 @@\n-    int vlen_enc = vector_length_encoding(this);\n-    if (vlen == 64 || VM_Version::supports_avx512vlbw()) { \/\/ AVX512VL for <512bit operands\n-      assert(VM_Version::supports_avx512bw(), \"required\"); \/\/ 512-bit byte vectors assume AVX512BW\n-      __ evpbroadcastb($dst$$XMMRegister, $src$$Register, vlen_enc);\n+    if (UseAVX >= 2) {\n+      int vlen_enc = vector_length_encoding(this);\n+      if (vlen == 64 || VM_Version::supports_avx512vlbw()) { \/\/ AVX512VL for <512bit operands\n+        assert(VM_Version::supports_avx512bw(), \"required\"); \/\/ 512-bit byte vectors assume AVX512BW\n+        __ evpbroadcastb($dst$$XMMRegister, $src$$Register, vlen_enc);\n+      } else {\n+        __ movdl($dst$$XMMRegister, $src$$Register);\n+        __ vpbroadcastb($dst$$XMMRegister, $dst$$XMMRegister, vlen_enc);\n+      }\n@@ -4207,0 +4212,1 @@\n+       assert(UseAVX < 2, \"\");\n@@ -4208,18 +4214,6 @@\n-      __ vpbroadcastb($dst$$XMMRegister, $dst$$XMMRegister, vlen_enc);\n-    }\n-  %}\n-  ins_pipe( pipe_slow );\n-%}\n-\n-instruct ReplB_reg(vec dst, rRegI src) %{\n-  predicate(UseAVX < 2);\n-  match(Set dst (ReplicateB src));\n-  format %{ \"replicateB $dst,$src\" %}\n-  ins_encode %{\n-    uint vlen = Matcher::vector_length(this);\n-    __ movdl($dst$$XMMRegister, $src$$Register);\n-    __ punpcklbw($dst$$XMMRegister, $dst$$XMMRegister);\n-    __ pshuflw($dst$$XMMRegister, $dst$$XMMRegister, 0x00);\n-    if (vlen >= 16) {\n-      assert(vlen == 16, \"\");\n-      __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);\n+      __ punpcklbw($dst$$XMMRegister, $dst$$XMMRegister);\n+      __ pshuflw($dst$$XMMRegister, $dst$$XMMRegister, 0x00);\n+      if (vlen >= 16) {\n+        assert(vlen == 16, \"\");\n+        __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);\n+      }\n@@ -4232,2 +4226,2 @@\n-  predicate(UseAVX >= 2);\n-  match(Set dst (ReplicateB (LoadB mem)));\n+  predicate(UseAVX >= 2 && Matcher::vector_element_basic_type(n) == T_BYTE);\n+  match(Set dst (Replicate (LoadB mem)));\n@@ -4245,2 +4239,2 @@\n-  predicate(UseAVX >= 2);\n-  match(Set dst (ReplicateS src));\n+  predicate(Matcher::vector_element_basic_type(n) == T_SHORT);\n+  match(Set dst (Replicate src));\n@@ -4251,3 +4245,8 @@\n-    if (vlen == 32 || VM_Version::supports_avx512vlbw()) { \/\/ AVX512VL for <512bit operands\n-      assert(VM_Version::supports_avx512bw(), \"required\"); \/\/ 512-bit short vectors assume AVX512BW\n-      __ evpbroadcastw($dst$$XMMRegister, $src$$Register, vlen_enc);\n+    if (UseAVX >= 2) {\n+      if (vlen == 32 || VM_Version::supports_avx512vlbw()) { \/\/ AVX512VL for <512bit operands\n+        assert(VM_Version::supports_avx512bw(), \"required\"); \/\/ 512-bit short vectors assume AVX512BW\n+        __ evpbroadcastw($dst$$XMMRegister, $src$$Register, vlen_enc);\n+      } else {\n+        __ movdl($dst$$XMMRegister, $src$$Register);\n+        __ vpbroadcastw($dst$$XMMRegister, $dst$$XMMRegister, vlen_enc);\n+      }\n@@ -4255,0 +4254,1 @@\n+      assert(UseAVX < 2, \"\");\n@@ -4256,18 +4256,5 @@\n-      __ vpbroadcastw($dst$$XMMRegister, $dst$$XMMRegister, vlen_enc);\n-    }\n-  %}\n-  ins_pipe( pipe_slow );\n-%}\n-\n-instruct ReplS_reg(vec dst, rRegI src) %{\n-  predicate(UseAVX < 2);\n-  match(Set dst (ReplicateS src));\n-  format %{ \"replicateS $dst,$src\" %}\n-  ins_encode %{\n-    uint vlen = Matcher::vector_length(this);\n-    int vlen_enc = vector_length_encoding(this);\n-    __ movdl($dst$$XMMRegister, $src$$Register);\n-    __ pshuflw($dst$$XMMRegister, $dst$$XMMRegister, 0x00);\n-    if (vlen >= 8) {\n-      assert(vlen == 8, \"\");\n-      __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);\n+      __ pshuflw($dst$$XMMRegister, $dst$$XMMRegister, 0x00);\n+      if (vlen >= 8) {\n+        assert(vlen == 8, \"\");\n+        __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);\n+      }\n@@ -4280,2 +4267,2 @@\n-  predicate(UseAVX >= 2);\n-  match(Set dst (ReplicateS (LoadS mem)));\n+  predicate(UseAVX >= 2 && Matcher::vector_element_basic_type(n) == T_SHORT);\n+  match(Set dst (Replicate (LoadS mem)));\n@@ -4293,1 +4280,2 @@\n-  match(Set dst (ReplicateI src));\n+  predicate(Matcher::vector_element_basic_type(n) == T_INT);\n+  match(Set dst (Replicate src));\n@@ -4312,1 +4300,2 @@\n-  match(Set dst (ReplicateI (LoadI mem)));\n+  predicate(Matcher::vector_element_basic_type(n) == T_INT);\n+  match(Set dst (Replicate (LoadI mem)));\n@@ -4329,3 +4318,2 @@\n-  match(Set dst (ReplicateB con));\n-  match(Set dst (ReplicateS con));\n-  match(Set dst (ReplicateI con));\n+  predicate(Matcher::is_non_long_integral_vector(n));\n+  match(Set dst (Replicate con));\n@@ -4347,3 +4335,2 @@\n-  match(Set dst (ReplicateB zero));\n-  match(Set dst (ReplicateS zero));\n-  match(Set dst (ReplicateI zero));\n+  predicate(Matcher::is_non_long_integral_vector(n));\n+  match(Set dst (Replicate zero));\n@@ -4363,4 +4350,2 @@\n-  predicate(UseSSE >= 2);\n-  match(Set dst (ReplicateB con));\n-  match(Set dst (ReplicateS con));\n-  match(Set dst (ReplicateI con));\n+  predicate(UseSSE >= 2 && Matcher::is_non_long_integral_vector(n));\n+  match(Set dst (Replicate con));\n@@ -4380,1 +4365,2 @@\n-  match(Set dst (ReplicateL src));\n+  predicate(Matcher::vector_element_basic_type(n) == T_LONG);\n+  match(Set dst (Replicate src));\n@@ -4400,2 +4386,2 @@\n-  predicate(Matcher::vector_length(n) <= 4);\n-  match(Set dst (ReplicateL src));\n+  predicate(Matcher::vector_length(n) <= 4 && Matcher::vector_element_basic_type(n) == T_LONG);\n+  match(Set dst (Replicate src));\n@@ -4429,2 +4415,2 @@\n-  predicate(Matcher::vector_length(n) == 8);\n-  match(Set dst (ReplicateL src));\n+  predicate(Matcher::vector_length(n) == 8 && Matcher::vector_element_basic_type(n) == T_LONG);\n+  match(Set dst (Replicate src));\n@@ -4454,1 +4440,2 @@\n-  match(Set dst (ReplicateL (LoadL mem)));\n+  predicate(Matcher::vector_element_basic_type(n) == T_LONG);\n+  match(Set dst (Replicate (LoadL mem)));\n@@ -4472,1 +4459,2 @@\n-  match(Set dst (ReplicateL con));\n+  predicate(Matcher::vector_element_basic_type(n) == T_LONG);\n+  match(Set dst (Replicate con));\n@@ -4483,1 +4471,2 @@\n-  match(Set dst (ReplicateL zero));\n+  predicate(Matcher::vector_element_basic_type(n) == T_LONG);\n+  match(Set dst (Replicate zero));\n@@ -4497,2 +4486,2 @@\n-  predicate(UseSSE >= 2);\n-  match(Set dst (ReplicateL con));\n+  predicate(UseSSE >= 2 && Matcher::vector_element_basic_type(n) == T_LONG);\n+  match(Set dst (Replicate con));\n@@ -4510,2 +4499,2 @@\n-  predicate(UseAVX > 0);\n-  match(Set dst (ReplicateF src));\n+  predicate(UseAVX > 0 && Matcher::vector_element_basic_type(n) == T_FLOAT);\n+  match(Set dst (Replicate src));\n@@ -4530,2 +4519,2 @@\n-  predicate(UseAVX == 0);\n-  match(Set dst (ReplicateF src));\n+  predicate(UseAVX == 0 && Matcher::vector_element_basic_type(n) == T_FLOAT);\n+  match(Set dst (Replicate src));\n@@ -4540,2 +4529,2 @@\n-  predicate(UseAVX > 0);\n-  match(Set dst (ReplicateF (LoadF mem)));\n+  predicate(UseAVX > 0 && Matcher::vector_element_basic_type(n) == T_FLOAT);\n+  match(Set dst (Replicate (LoadF mem)));\n@@ -4552,1 +4541,2 @@\n-  match(Set dst (ReplicateF con));\n+  predicate(Matcher::vector_element_basic_type(n) == T_FLOAT);\n+  match(Set dst (Replicate con));\n@@ -4564,1 +4554,2 @@\n-  match(Set dst (ReplicateF zero));\n+  predicate(Matcher::vector_element_basic_type(n) == T_FLOAT);\n+  match(Set dst (Replicate zero));\n@@ -4581,2 +4572,2 @@\n-  predicate(UseSSE >= 3);\n-  match(Set dst (ReplicateD src));\n+  predicate(UseSSE >= 3 && Matcher::vector_element_basic_type(n) == T_DOUBLE);\n+  match(Set dst (Replicate src));\n@@ -4601,2 +4592,2 @@\n-  predicate(UseSSE < 3);\n-  match(Set dst (ReplicateD src));\n+  predicate(UseSSE < 3 && Matcher::vector_element_basic_type(n) == T_DOUBLE);\n+  match(Set dst (Replicate src));\n@@ -4611,2 +4602,2 @@\n-  predicate(UseSSE >= 3);\n-  match(Set dst (ReplicateD (LoadD mem)));\n+  predicate(UseSSE >= 3 && Matcher::vector_element_basic_type(n) == T_DOUBLE);\n+  match(Set dst (Replicate (LoadD mem)));\n@@ -4627,1 +4618,2 @@\n-  match(Set dst (ReplicateD con));\n+  predicate(Matcher::vector_element_basic_type(n) == T_DOUBLE);\n+  match(Set dst (Replicate con));\n@@ -4638,1 +4630,2 @@\n-  match(Set dst (ReplicateD zero));\n+  predicate(Matcher::vector_element_basic_type(n) == T_DOUBLE);\n+  match(Set dst (Replicate zero));\n@@ -7462,1 +7455,1 @@\n-    InternalAddress new_mxcsr = $constantaddress((jint)0x3F80);\n+    InternalAddress new_mxcsr = $constantaddress((jint)(EnableX86ECoreOpts ? 0x3FBF : 0x3F80));\n@@ -7479,1 +7472,1 @@\n-    InternalAddress new_mxcsr = $constantaddress((jint)0x3F80);\n+    InternalAddress new_mxcsr = $constantaddress((jint)(EnableX86ECoreOpts ? 0x3FBF : 0x3F80));\n@@ -7494,1 +7487,1 @@\n-    InternalAddress new_mxcsr = $constantaddress((jint)0x3F80);\n+    InternalAddress new_mxcsr = $constantaddress((jint)(EnableX86ECoreOpts ? 0x3FBF : 0x3F80));\n","filename":"src\/hotspot\/cpu\/x86\/x86.ad","additions":85,"deletions":92,"binary":false,"changes":177,"status":"modified"},{"patch":"@@ -4255,1 +4255,1 @@\n-    \"ReplicateB\",\"ReplicateS\",\"ReplicateI\",\"ReplicateL\",\"ReplicateF\",\"ReplicateD\",\"ReverseV\",\"ReverseBytesV\",\n+    \"Replicate\",\"ReverseV\",\"ReverseBytesV\",\n","filename":"src\/hotspot\/share\/adlc\/formssel.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -1215,5 +1215,2 @@\n-        int cache_index = ConstantPool::decode_cpcache_index(index, true);\n-        assert(cache_index >= 0 && cache_index < pool->cache()->length(), \"unexpected cache index\");\n-        ConstantPoolCacheEntry* cpce = pool->cache()->entry_at(cache_index);\n-        cpce->set_method_handle(pool, info);\n-        appendix = Handle(current, cpce->appendix_if_resolved(pool)); \/\/ just in case somebody already resolved the entry\n+        ResolvedMethodEntry* entry = pool->cache()->set_method_handle(index, info);\n+        appendix = Handle(current, pool->cache()->appendix_if_resolved(entry));\n","filename":"src\/hotspot\/share\/c1\/c1_Runtime1.cpp","additions":2,"deletions":5,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -262,1 +262,1 @@\n-  set_magic(DynamicDumpSharedSpaces ? CDS_DYNAMIC_ARCHIVE_MAGIC : CDS_ARCHIVE_MAGIC);\n+  set_magic(CDSConfig::is_dumping_dynamic_archive() ? CDS_DYNAMIC_ARCHIVE_MAGIC : CDS_ARCHIVE_MAGIC);\n@@ -281,1 +281,1 @@\n-  _use_full_module_graph = MetaspaceShared::use_full_module_graph();\n+  _has_full_module_graph = CDSConfig::is_dumping_full_module_graph();\n@@ -304,1 +304,1 @@\n-  if (!DynamicDumpSharedSpaces) {\n+  if (!CDSConfig::is_dumping_dynamic_archive()) {\n@@ -360,1 +360,1 @@\n-  st->print_cr(\"- use_full_module_graph           %d\", _use_full_module_graph);\n+  st->print_cr(\"- has_full_module_graph           %d\", _has_full_module_graph);\n@@ -1002,1 +1002,1 @@\n-  if (DynamicDumpSharedSpaces) {\n+  if (CDSConfig::is_dumping_dynamic_archive()) {\n@@ -1012,1 +1012,1 @@\n-      DynamicDumpSharedSpaces = false;\n+      CDSConfig::disable_dumping_dynamic_archive();\n@@ -1018,1 +1018,1 @@\n-        DynamicDumpSharedSpaces = false;\n+        CDSConfig::disable_dumping_dynamic_archive();\n@@ -1597,1 +1597,1 @@\n-    assert(!DynamicDumpSharedSpaces, \"must be\");\n+    assert(!CDSConfig::is_dumping_dynamic_archive(), \"must be\");\n@@ -2042,1 +2042,1 @@\n-    MetaspaceShared::disable_full_module_graph();\n+    CDSConfig::disable_loading_full_module_graph();\n@@ -2346,1 +2346,1 @@\n-        DynamicDumpSharedSpaces = true;\n+        CDSConfig::enable_dumping_dynamic_archive();\n@@ -2461,3 +2461,3 @@\n-  if (!_use_full_module_graph) {\n-    MetaspaceShared::disable_full_module_graph();\n-    log_info(cds)(\"full module graph: disabled because archive was created without full module graph\");\n+  if (is_static() && !_has_full_module_graph) {\n+    \/\/ Only the static archive can contain the full module graph.\n+    CDSConfig::disable_loading_full_module_graph(\"archive was created without full module graph\");\n","filename":"src\/hotspot\/share\/cds\/filemap.cpp","additions":13,"deletions":13,"binary":false,"changes":26,"status":"modified"},{"patch":"@@ -255,1 +255,1 @@\n-  bool   _use_full_module_graph;        \/\/ Can we use the full archived module graph?\n+  bool   _has_full_module_graph;        \/\/ Does this CDS archive contain the full archived module graph?\n@@ -282,0 +282,1 @@\n+  bool is_static()                         const { return magic() == CDS_ARCHIVE_MAGIC; }\n","filename":"src\/hotspot\/share\/cds\/filemap.hpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -102,1 +102,0 @@\n-bool MetaspaceShared::_use_full_module_graph = true;\n@@ -284,1 +283,1 @@\n-      if (!DynamicDumpSharedSpaces) {\n+      if (!CDSConfig::is_dumping_dynamic_archive()) {\n@@ -589,1 +588,1 @@\n-  if (DynamicDumpSharedSpaces && ik->is_shared_unregistered_class()) {\n+  if (CDSConfig::is_dumping_dynamic_archive() && ik->is_shared_unregistered_class()) {\n@@ -787,1 +786,1 @@\n-      disable_full_module_graph();\n+      CDSConfig::disable_dumping_full_module_graph();\n@@ -791,1 +790,1 @@\n-    if (use_full_module_graph()) {\n+    if (CDSConfig::is_dumping_full_module_graph()) {\n@@ -954,1 +953,1 @@\n-    if (DynamicDumpSharedSpaces) {\n+    if (CDSConfig::is_dumping_dynamic_archive()) {\n@@ -960,1 +959,1 @@\n-    DynamicDumpSharedSpaces = false;\n+    CDSConfig::disable_dumping_dynamic_archive();\n@@ -996,1 +995,1 @@\n-  if (DynamicDumpSharedSpaces) {\n+  if (CDSConfig::is_dumping_dynamic_archive()) {\n@@ -1176,1 +1175,1 @@\n-    log_info(cds)(\"initial full module graph: %s\", MetaspaceShared::use_full_module_graph() ? \"enabled\" : \"disabled\");\n+    log_info(cds)(\"initial full module graph: %s\", CDSConfig::is_loading_full_module_graph() ? \"enabled\" : \"disabled\");\n@@ -1491,1 +1490,1 @@\n-  if (DynamicDumpSharedSpaces) {\n+  if (CDSConfig::is_dumping_dynamic_archive()) {\n@@ -1550,23 +1549,0 @@\n-bool MetaspaceShared::use_full_module_graph() {\n-#if INCLUDE_CDS_JAVA_HEAP\n-  if (ClassLoaderDataShared::is_full_module_graph_loaded()) {\n-    return true;\n-  }\n-#endif\n-  bool result = _use_optimized_module_handling && _use_full_module_graph;\n-  if (DumpSharedSpaces) {\n-    result &= HeapShared::can_write();\n-  } else if (UseSharedSpaces) {\n-    result &= ArchiveHeapLoader::can_use();\n-  } else {\n-    result = false;\n-  }\n-\n-  if (result && UseSharedSpaces) {\n-    \/\/ Classes used by the archived full module graph are loaded in JVMTI early phase.\n-    assert(!(JvmtiExport::should_post_class_file_load_hook() && JvmtiExport::has_early_class_hook_env()),\n-           \"CDS should be disabled if early class hooks are enabled\");\n-  }\n-  return result;\n-}\n-\n","filename":"src\/hotspot\/share\/cds\/metaspaceShared.cpp","additions":9,"deletions":33,"binary":false,"changes":42,"status":"modified"},{"patch":"@@ -1569,2 +1569,2 @@\n-    ConstantPoolCacheEntry* cp_cache_entry = cp->cache()->entry_at(cp->decode_cpcache_index(index));\n-    if (cp_cache_entry->is_resolved(Bytecodes::_invokehandle)) {\n+    ResolvedMethodEntry* method_entry = cp->resolved_method_entry_at(index);\n+    if (method_entry->is_resolved(Bytecodes::_invokehandle)) {\n@@ -1572,2 +1572,2 @@\n-      Method* adapter = cp_cache_entry->f1_as_method();\n-      oop appendix = cp_cache_entry->appendix_if_resolved(cp);\n+      Method* adapter = method_entry->method();\n+      oop appendix = cp->cache()->appendix_if_resolved(method_entry);\n@@ -1625,1 +1625,1 @@\n-              int cp_cache_index = bcs.get_index_u2_cpcache();\n+              int cp_cache_index = bcs.get_index_u2();\n","filename":"src\/hotspot\/share\/ci\/ciEnv.cpp","additions":5,"deletions":5,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -411,1 +411,0 @@\n-      ConstantPoolCacheEntry* cp_cache_entry = nullptr;\n@@ -416,4 +415,0 @@\n-      \/\/ ResolvedIndyEntry and ConstantPoolCacheEntry must currently coexist.\n-      \/\/ To address this, the variables below contain the values that *might*\n-      \/\/ be used to avoid multiple blocks of similar code. When CPCE is obsoleted\n-      \/\/ these can be removed\n@@ -437,6 +432,4 @@\n-        cp_cache_entry = cp->cache()->entry_at(cp->decode_cpcache_index(index));\n-        cp_cache_entry->set_method_handle(cp, callInfo);\n-\n-        appendix = cp_cache_entry->appendix_if_resolved(cp);\n-        adapter_method = cp_cache_entry->f1_as_method();\n-        pool_index = cp_cache_entry->constant_pool_index();\n+        ResolvedMethodEntry* method_entry = cp->cache()->set_method_handle(index, callInfo);\n+        appendix = cp->cache()->appendix_if_resolved(method_entry);\n+        adapter_method = method_entry->method();\n+        pool_index = method_entry->constant_pool_index();\n","filename":"src\/hotspot\/share\/ci\/ciReplay.cpp","additions":4,"deletions":11,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -371,1 +371,1 @@\n-  return get_index_u2_cpcache();\n+  return get_index_u2();\n","filename":"src\/hotspot\/share\/ci\/ciStreams.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -164,5 +164,0 @@\n-  \/\/ Get 2-byte index in native byte order.  (Rewriter::rewrite makes these.)\n-  int get_index_u2_cpcache() const {\n-    return bytecode().get_index_u2_cpcache(cur_bc_raw());\n-  }\n-\n","filename":"src\/hotspot\/share\/ci\/ciStreams.hpp","additions":0,"deletions":5,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -1129,3 +1129,3 @@\n-    \/\/ DynamicDumpSharedSpaces requires UseSharedSpaces to be enabled. Since --patch-module\n-    \/\/ is not supported with UseSharedSpaces, it is not supported with DynamicDumpSharedSpaces.\n-    assert(!DynamicDumpSharedSpaces, \"sanity\");\n+    \/\/ Dynamic dumping requires UseSharedSpaces to be enabled. Since --patch-module\n+    \/\/ is not supported with UseSharedSpaces, we can never come here during dynamic dumping.\n+    assert(!CDSConfig::is_dumping_dynamic_archive(), \"sanity\");\n@@ -1494,1 +1494,1 @@\n-    assert(!DynamicDumpSharedSpaces, \"DynamicDumpSharedSpaces not supported with exploded module builds\");\n+    assert(!CDSConfig::is_dumping_dynamic_archive(), \"not supported with exploded module builds\");\n","filename":"src\/hotspot\/share\/classfile\/classLoader.cpp","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -146,1 +146,1 @@\n-    assert(MetaspaceShared::use_full_module_graph(), \"must be\");\n+    assert(CDSConfig::is_loading_full_module_graph(), \"must be\");\n@@ -159,1 +159,1 @@\n-    assert(MetaspaceShared::use_full_module_graph(), \"must be\");\n+    assert(CDSConfig::is_loading_full_module_graph(), \"must be\");\n","filename":"src\/hotspot\/share\/classfile\/systemDictionary.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -397,0 +397,1 @@\n+  MACOS_AARCH64_ONLY(ThreadWXEnable wx(WXWrite, Thread::current());)\n@@ -2718,3 +2719,0 @@\n-  \/\/ Decoding an nmethod can write to a PcDescCache (see PcDescCache::add_pc_desc)\n-  MACOS_AARCH64_ONLY(ThreadWXEnable wx(WXWrite, Thread::current());)\n-\n","filename":"src\/hotspot\/share\/code\/nmethod.cpp","additions":1,"deletions":3,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -334,0 +334,4 @@\n+  ++_total_added;\n+  if (_size > _peak_size) {\n+    _peak_size = _size;\n+  }\n@@ -490,0 +494,1 @@\n+  ++_total_removed;\n@@ -519,0 +524,8 @@\n+CompileQueue* CompileBroker::c1_compile_queue() {\n+  return _c1_compile_queue;\n+}\n+\n+CompileQueue* CompileBroker::c2_compile_queue() {\n+  return _c2_compile_queue;\n+}\n+\n","filename":"src\/hotspot\/share\/compiler\/compileBroker.cpp","additions":13,"deletions":0,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -29,0 +29,1 @@\n+#include \"gc\/shared\/bufferNode.hpp\"\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahBarrierSet.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -87,8 +87,0 @@\n-  int get_index_u1_cpcache(Bytecodes::Code bc) const {\n-    assert_same_format_as(bc); assert_index_size(1, bc);\n-    return *(u1*)addr_at(1) + ConstantPool::CPCACHE_INDEX_TAG;\n-  }\n-  int get_index_u2_cpcache(Bytecodes::Code bc) const {\n-    assert_same_format_as(bc); assert_index_size(2, bc); assert_native_index(bc);\n-    return Bytes::get_native_u2(addr_at(1)) + ConstantPool::CPCACHE_INDEX_TAG;\n-  }\n@@ -191,1 +183,1 @@\n-  ConstantPoolCacheEntry* cpcache_entry() const;\n+  ResolvedMethodEntry* resolved_method_entry() const;\n","filename":"src\/hotspot\/share\/interpreter\/bytecode.hpp","additions":1,"deletions":9,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -40,0 +40,1 @@\n+#include \"oops\/resolvedMethodEntry.hpp\"\n@@ -329,7 +330,0 @@\n-void BytecodePrinter::print_cpcache_entry(int cpc_index, outputStream* st) {\n-  ConstantPool* constants = method()->constants();\n-  ConstantPoolCacheEntry* cpce = constants->cache()->entry_at(cpc_index);\n-  st->print(\"  ConstantPoolCacheEntry: \");\n-  cpce->print(st, cpc_index, constants->cache());\n-}\n-\n@@ -520,3 +514,14 @@\n-        int cpcache_index;\n-          cpcache_index = get_native_index_u2();\n-          cp_index = cpcache()->entry_at(cpcache_index)->constant_pool_index();\n+          int method_index = get_native_index_u2();\n+          ResolvedMethodEntry* method_entry = cpcache()->resolved_method_entry_at(method_index);\n+          cp_index = method_entry->constant_pool_index();\n+          print_field_or_method(cp_index, st);\n+\n+          if (raw_code() == Bytecodes::_invokehandle &&\n+              ClassPrinter::has_mode(_flags, ClassPrinter::PRINT_METHOD_HANDLE)) {\n+            assert(is_linked(), \"invokehandle is only in rewritten methods\");\n+            method_entry->print_on(st);\n+            if (method_entry->has_appendix()) {\n+              st->print(\"  appendix: \");\n+              constants()->resolved_reference_from_method(method_index)->print_on(st);\n+            }\n+          }\n@@ -525,8 +530,1 @@\n-          cpcache_index = -1;\n-        }\n-        print_field_or_method(cp_index, st);\n-        if (raw_code() == Bytecodes::_invokehandle &&\n-            ClassPrinter::has_mode(_flags, ClassPrinter::PRINT_METHOD_HANDLE)) {\n-          assert(is_linked(), \"invokehandle is only in rewritten methods\");\n-          assert(cpcache_index >= 0, \"must be\");\n-          print_cpcache_entry(cpcache_index, st);\n+          print_field_or_method(cp_index, st);\n@@ -542,2 +540,2 @@\n-          int cpcache_index = get_native_index_u2();\n-          cp_index = cpcache()->entry_at(cpcache_index)->constant_pool_index();\n+          int method_index = get_native_index_u2();\n+          cp_index = cpcache()->resolved_method_entry_at(method_index)->constant_pool_index();\n","filename":"src\/hotspot\/share\/interpreter\/bytecodeTracer.cpp","additions":18,"deletions":20,"binary":false,"changes":38,"status":"modified"},{"patch":"@@ -1007,1 +1007,1 @@\n-        cp_index = Bytes::get_native_u2(code_base + pos) DEBUG_ONLY(+ ConstantPool::CPCACHE_INDEX_TAG);\n+        cp_index = Bytes::get_native_u2(code_base + pos);\n@@ -1149,1 +1149,1 @@\n-        int cp_index = Bytes::get_native_u2(code_base+ pos) DEBUG_ONLY(+ ConstantPool::CPCACHE_INDEX_TAG);\n+        int cp_index = Bytes::get_native_u2(code_base+ pos);\n@@ -1358,1 +1358,1 @@\n-      int cp_index = Bytes::get_native_u2(code_base + pos) DEBUG_ONLY(+ ConstantPool::CPCACHE_INDEX_TAG);\n+      int cp_index = Bytes::get_native_u2(code_base + pos);\n@@ -1441,1 +1441,1 @@\n-        int cp_index = Bytes::get_native_u2(code_base+ pos) DEBUG_ONLY(+ ConstantPool::CPCACHE_INDEX_TAG);\n+        int cp_index = Bytes::get_native_u2(code_base+ pos);\n","filename":"src\/hotspot\/share\/interpreter\/bytecodeUtils.cpp","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -112,2 +112,0 @@\n-  int get_index_u2_cpcache(Bytecodes::Code bc) const\n-                                                 { return bytecode().get_index_u2_cpcache(bc); }\n@@ -116,3 +114,0 @@\n-  ConstantPoolCacheEntry* cache_entry_at(int i) const\n-                                                 { return method()->constants()->cache()->entry_at(i); }\n-  ConstantPoolCacheEntry* cache_entry() const    { return cache_entry_at(Bytes::get_native_u2(bcp() + 1)); }\n@@ -217,2 +212,2 @@\n-    intptr_t flags = ((as_TosState(type) << ConstantPoolCacheEntry::tos_state_shift)\n-                      | (offset & ConstantPoolCacheEntry::field_index_mask));\n+    intptr_t flags = ((as_TosState(type) << ConstantPoolCache::tos_state_shift)\n+                      | (offset & ConstantPoolCache::field_index_mask));\n@@ -1109,0 +1104,1 @@\n+  ConstantPoolCache* cache = pool->cache();\n@@ -1112,0 +1108,1 @@\n+  int method_index = last_frame.get_index_u2(bytecode);\n@@ -1116,1 +1113,1 @@\n-                                 last_frame.get_index_u2_cpcache(bytecode), bytecode,\n+                                 method_index, bytecode,\n@@ -1137,2 +1134,1 @@\n-  ConstantPoolCacheEntry* cp_cache_entry = last_frame.cache_entry();\n-  if (cp_cache_entry->is_resolved(bytecode)) return;\n+  if (cache->resolved_method_entry_at(method_index)->is_resolved(bytecode)) return;\n@@ -1172,4 +1168,1 @@\n-    cp_cache_entry->set_direct_call(\n-      bytecode,\n-      resolved_method,\n-      sender->is_interface());\n+    cache->set_direct_call(bytecode, method_index, resolved_method, sender->is_interface());\n@@ -1178,4 +1171,1 @@\n-    cp_cache_entry->set_vtable_call(\n-      bytecode,\n-      resolved_method,\n-      info.vtable_index());\n+    cache->set_vtable_call(bytecode, method_index, resolved_method, info.vtable_index());\n@@ -1184,1 +1174,1 @@\n-    cp_cache_entry->set_itable_call(\n+    cache->set_itable_call(\n@@ -1186,0 +1176,1 @@\n+      method_index,\n@@ -1203,0 +1194,1 @@\n+  int method_index = last_frame.get_index_u2(bytecode);\n@@ -1207,1 +1199,1 @@\n-                                 last_frame.get_index_u2_cpcache(bytecode), bytecode,\n+                                 method_index, bytecode,\n@@ -1211,2 +1203,1 @@\n-  ConstantPoolCacheEntry* cp_cache_entry = last_frame.cache_entry();\n-  cp_cache_entry->set_method_handle(pool, info);\n+  pool->cache()->set_method_handle(method_index, info);\n@@ -1782,1 +1773,1 @@\n-  int cp_index = Bytes::get_native_u2(bcp + 1) + ConstantPool::CPCACHE_INDEX_TAG;\n+  int cp_index = Bytes::get_native_u2(bcp + 1);\n","filename":"src\/hotspot\/share\/interpreter\/interpreterRuntime.cpp","additions":14,"deletions":23,"binary":false,"changes":37,"status":"modified"},{"patch":"@@ -1733,3 +1733,2 @@\n-  int cache_index = ConstantPool::decode_cpcache_index(index, true);\n-  ConstantPoolCacheEntry* cpce = pool->cache()->entry_at(cache_index);\n-  if (!cpce->is_f1_null()) {\n+  ResolvedMethodEntry* method_entry = pool->cache()->resolved_method_entry_at(index);\n+  if (method_entry->method() != nullptr) {\n@@ -1737,2 +1736,2 @@\n-    methodHandle method(THREAD, cpce->f1_as_method());\n-    Handle     appendix(THREAD, cpce->appendix_if_resolved(pool));\n+    methodHandle method(THREAD, method_entry->method());\n+    Handle     appendix(THREAD, pool->cache()->appendix_if_resolved(method_entry));\n","filename":"src\/hotspot\/share\/interpreter\/linkResolver.cpp","additions":4,"deletions":5,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -38,0 +38,1 @@\n+#include \"oops\/resolvedMethodEntry.hpp\"\n@@ -61,1 +62,3 @@\n-        add_cp_cache_entry(i);\n+        _cp_map.at_put(i, _method_entry_index);\n+        _method_entry_index++;\n+        _initialized_method_entries.push(ResolvedMethodEntry((u2)i));\n@@ -84,2 +87,2 @@\n-  guarantee((int) _cp_cache_map.length() - 1 <= (int) ((u2)-1),\n-            \"all cp cache indexes fit in a u2\");\n+  guarantee(_initialized_field_entries.length() - 1 <= (int)((u2)-1), \"All resolved field indices fit in a u2\");\n+  guarantee(_initialized_method_entries.length() - 1 <= (int)((u2)-1), \"All resolved method indices fit in a u2\");\n@@ -107,0 +110,2 @@\n+  assert(_field_entry_index == _initialized_field_entries.length(), \"Field entry size mismatch\");\n+  assert(_method_entry_index == _initialized_method_entries.length(), \"Method entry size mismatch\");\n@@ -108,2 +113,3 @@\n-      ConstantPoolCache::allocate(loader_data, _cp_cache_map,\n-                                  _invokedynamic_references_map, _initialized_indy_entries, _initialized_field_entries, CHECK);\n+      ConstantPoolCache::allocate(loader_data, _invokedynamic_references_map,\n+                                  _initialized_indy_entries, _initialized_field_entries, _initialized_method_entries,\n+                                  CHECK);\n@@ -123,1 +129,1 @@\n-      assert(DynamicDumpSharedSpaces, \"must be\");\n+      assert(CDSConfig::is_dumping_dynamic_archive(), \"must be\");\n@@ -127,2 +133,0 @@\n-    } else {\n-      cache->save_for_archive(THREAD);\n@@ -197,2 +201,1 @@\n-\/\/ Rewrite a classfile-order CP index into a native-order CPC index.\n-void Rewriter::rewrite_member_reference(address bcp, int offset, bool reverse) {\n+void Rewriter::rewrite_method_reference(address bcp, int offset, bool reverse) {\n@@ -201,5 +204,6 @@\n-    int cp_index    = Bytes::get_Java_u2(p);\n-    int  cache_index = cp_entry_to_cp_cache(cp_index);\n-    Bytes::put_native_u2(p, (u2)cache_index);\n-    if (!_method_handle_invokers.is_empty())\n-      maybe_rewrite_invokehandle(p - 1, cp_index, cache_index, reverse);\n+    int  cp_index    = Bytes::get_Java_u2(p);\n+    int  method_entry_index = _cp_map.at(cp_index);\n+    Bytes::put_native_u2(p, (u2)method_entry_index);\n+    if (!_method_handle_invokers.is_empty()) {\n+      maybe_rewrite_invokehandle(p - 1, cp_index, method_entry_index, reverse);\n+    }\n@@ -207,2 +211,2 @@\n-    int cache_index = Bytes::get_native_u2(p);\n-    int pool_index = cp_cache_entry_pool_index(cache_index);\n+    int method_entry_index = Bytes::get_native_u2(p);\n+    int pool_index = _initialized_method_entries.at(method_entry_index).constant_pool_index();\n@@ -210,2 +214,3 @@\n-    if (!_method_handle_invokers.is_empty())\n-      maybe_rewrite_invokehandle(p - 1, pool_index, cache_index, reverse);\n+    if (!_method_handle_invokers.is_empty()) {\n+      maybe_rewrite_invokehandle(p - 1, pool_index, method_entry_index, reverse);\n+    }\n@@ -224,2 +229,4 @@\n-      int cache_index = add_invokespecial_cp_cache_entry(cp_index);\n-      if (cache_index != (int)(jushort) cache_index) {\n+      _initialized_method_entries.push(ResolvedMethodEntry((u2)cp_index));\n+      Bytes::put_native_u2(p, (u2)_method_entry_index);\n+      _method_entry_index++;\n+      if (_method_entry_index != (int)(u2)_method_entry_index) {\n@@ -228,2 +235,1 @@\n-      Bytes::put_native_u2(p, (u2)cache_index);\n-      rewrite_member_reference(bcp, offset, reverse);\n+      rewrite_method_reference(bcp, offset, reverse);\n@@ -233,1 +239,1 @@\n-    rewrite_member_reference(bcp, offset, reverse);\n+    rewrite_method_reference(bcp, offset, reverse);\n@@ -242,2 +248,2 @@\n-        (*opc) == (u1)Bytecodes::_invokespecial) {\n-      assert(_pool->tag_at(cp_index).is_method(), \"wrong index\");\n+        ((*opc) == (u1)Bytecodes::_invokespecial)) {\n+          assert(_pool->tag_at(cp_index).is_method(), \"wrong index\");\n@@ -253,1 +259,2 @@\n-          add_invokedynamic_resolved_references_entry(cp_index, cache_index);\n+          int resolved_index = add_invokedynamic_resolved_references_entry(cp_index, cache_index);\n+          _initialized_method_entries.at(cache_index).set_resolved_references_index((u2)resolved_index);\n@@ -259,1 +266,2 @@\n-          add_invokedynamic_resolved_references_entry(cp_index, cache_index);\n+          int resolved_index = add_invokedynamic_resolved_references_entry(cp_index, cache_index);\n+          _initialized_method_entries.at(cache_index).set_resolved_references_index((u2)resolved_index);\n@@ -474,1 +482,1 @@\n-        rewrite_member_reference(bcp, prefix_length+1, reverse);\n+        rewrite_method_reference(bcp, prefix_length+1, reverse);\n@@ -580,1 +588,0 @@\n-    _cp_cache_map(cpool->length() \/ 2),\n@@ -586,1 +593,2 @@\n-    _field_entry_index(0)\n+    _field_entry_index(0),\n+    _method_entry_index(0)\n","filename":"src\/hotspot\/share\/interpreter\/rewriter.cpp","additions":39,"deletions":31,"binary":false,"changes":70,"status":"modified"},{"patch":"@@ -265,4 +265,0 @@\n-  static void resolve_cache_and_index(int byte_no,       \/\/ one of 1,2,11\n-                                      Register cache,    \/\/ output for CP cache\n-                                      Register index,    \/\/ output for CP index\n-                                      size_t index_size); \/\/ one of 1,2,4\n@@ -272,0 +268,3 @@\n+  static void resolve_cache_and_index_for_method(int byte_no,\n+                                                 Register cache,\n+                                                 Register index);\n@@ -279,0 +278,14 @@\n+  static void load_resolved_method_entry_special_or_static(Register cache,\n+                                                           Register method,\n+                                                           Register flags);\n+  static void load_resolved_method_entry_handle(Register cache,\n+                                                Register method,\n+                                                Register ref_index,\n+                                                Register flags);\n+  static void load_resolved_method_entry_interface(Register cache,\n+                                                   Register klass,\n+                                                   Register method_or_table_index,\n+                                                   Register flags);\n+  static void load_resolved_method_entry_virtual(Register cache,\n+                                                 Register method_or_table_index,\n+                                                 Register flags);\n","filename":"src\/hotspot\/share\/interpreter\/templateTable.hpp","additions":17,"deletions":4,"binary":false,"changes":21,"status":"modified"},{"patch":"@@ -937,5 +937,0 @@\n-C2V_VMENTRY_0(jint, constantPoolRemapInstructionOperandFromCache, (JNIEnv* env, jobject, ARGUMENT_PAIR(cp), jint index))\n-  constantPoolHandle cp(THREAD, UNPACK_PAIR(ConstantPool, cp));\n-  return cp->remap_instruction_operand_from_cache(index);\n-C2V_END\n-\n@@ -1663,0 +1658,8 @@\n+C2V_VMENTRY_0(int, decodeMethodIndexToCPIndex, (JNIEnv* env, jobject, ARGUMENT_PAIR(cp), jint method_index))\n+  constantPoolHandle cp(THREAD, UNPACK_PAIR(ConstantPool, cp));\n+  if (method_index < 0 || method_index >= cp->resolved_method_entries_length()) {\n+    JVMCI_THROW_MSG_0(IllegalStateException, err_msg(\"invalid method index %d\", method_index));\n+  }\n+  return cp->resolved_method_entry_at(method_index)->constant_pool_index();\n+C2V_END\n+\n@@ -1670,2 +1673,1 @@\n-    ConstantPoolCacheEntry* cp_cache_entry = cp->cache()->entry_at(cp->decode_cpcache_index(index));\n-    cp_cache_entry->set_method_handle(cp, callInfo);\n+    cp->cache()->set_method_handle(index, callInfo);\n@@ -1677,2 +1679,2 @@\n-  ConstantPoolCacheEntry* cp_cache_entry = cp->cache()->entry_at(cp->decode_cpcache_index(index));\n-  if (cp_cache_entry->is_resolved(Bytecodes::_invokehandle)) {\n+  ResolvedMethodEntry* entry = cp->cache()->resolved_method_entry_at(index);\n+  if (entry->is_resolved(Bytecodes::_invokehandle)) {\n@@ -1691,1 +1693,1 @@\n-    methodHandle adapter_method(THREAD, cp_cache_entry->f1_as_method());\n+    methodHandle adapter_method(THREAD, entry->method());\n@@ -1700,1 +1702,1 @@\n-      vmassert(cp_cache_entry->appendix_if_resolved(cp) == nullptr, \"!\");\n+      vmassert(cp->cache()->appendix_if_resolved(entry) == nullptr, \"!\");\n@@ -1710,1 +1712,1 @@\n-    if (cp->resolved_indy_entry_at(cp->decode_cpcache_index(index))->is_resolved()) {\n+    if (cp->resolved_indy_entry_at(cp->decode_invokedynamic_index(index))->is_resolved()) {\n@@ -3223,1 +3225,0 @@\n-  {CC \"constantPoolRemapInstructionOperandFromCache\", CC \"(\" HS_CONSTANT_POOL2 \"I)I\",                                                       FN_PTR(constantPoolRemapInstructionOperandFromCache)},\n@@ -3230,0 +3231,1 @@\n+  {CC \"decodeMethodIndexToCPIndex\",                   CC \"(\" HS_CONSTANT_POOL2 \"I)I\",                                                       FN_PTR(decodeMethodIndexToCPIndex)},\n","filename":"src\/hotspot\/share\/jvmci\/jvmciCompilerToVM.cpp","additions":15,"deletions":13,"binary":false,"changes":28,"status":"modified"},{"patch":"@@ -615,1 +615,0 @@\n-  declare_constant(ConstantPool::CPCACHE_INDEX_TAG)                       \\\n","filename":"src\/hotspot\/share\/jvmci\/vmStructs_jvmci.cpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -686,2 +686,1 @@\n-  int cache_index = decode_cpcache_index(which, true);\n-  if (!(cache_index >= 0 && cache_index < cpool->cache()->length())) {\n+  if (!(which >= 0 && which < cpool->resolved_method_entries_length())) {\n@@ -692,2 +691,1 @@\n-  ConstantPoolCacheEntry* e = cpool->cache()->entry_at(cache_index);\n-  return e->method_if_resolved(cpool);\n+  return cpool->cache()->method_if_resolved(which);\n@@ -703,3 +701,1 @@\n-    int cache_index = decode_cpcache_index(which, true);\n-    ConstantPoolCacheEntry* e = cpool->cache()->entry_at(cache_index);\n-    return e->has_appendix();\n+    return cpool->resolved_method_entry_at(which)->has_appendix();\n@@ -715,3 +711,1 @@\n-    int cache_index = decode_cpcache_index(which, true);\n-    ConstantPoolCacheEntry* e = cpool->cache()->entry_at(cache_index);\n-    return e->appendix_if_resolved(cpool);\n+    return cpool->cache()->appendix_if_resolved(which);\n@@ -724,2 +718,2 @@\n-  int cache_index = decode_cpcache_index(which, true);\n-    return cpool->resolved_indy_entry_at(cache_index)->has_local_signature();\n+    int indy_index = decode_invokedynamic_index(which);\n+    return cpool->resolved_indy_entry_at(indy_index)->has_local_signature();\n@@ -728,2 +722,1 @@\n-    ConstantPoolCacheEntry* e = cpool->cache()->entry_at(cache_index);\n-    return e->has_local_signature();\n+    return cpool->resolved_method_entry_at(which)->has_local_signature();\n@@ -750,1 +743,2 @@\n-      \/\/ TODO: handle resolved method entries with new structure\n+    case Bytecodes::_fast_invokevfinal: \/\/ Bytecode interpreter uses this\n+      return resolved_method_entry_at(index)->constant_pool_index();\n@@ -752,2 +746,3 @@\n-      \/\/ change byte-ordering and go via cache\n-      return remap_instruction_operand_from_cache(index);\n+      tty->print_cr(\"Unexpected bytecode: %d\", code);\n+      ShouldNotReachHere(); \/\/ All cases should have been handled\n+      return -1;\n@@ -795,9 +790,0 @@\n-int ConstantPool::remap_instruction_operand_from_cache(int operand) {\n-  int cpc_index = operand;\n-  DEBUG_ONLY(cpc_index -= CPCACHE_INDEX_TAG);\n-  assert((int)(u2)cpc_index == cpc_index, \"clean u2\");\n-  int member_index = cache()->entry_at(cpc_index)->constant_pool_index();\n-  return member_index;\n-}\n-\n-\n","filename":"src\/hotspot\/share\/oops\/constantPool.cpp","additions":12,"deletions":26,"binary":false,"changes":38,"status":"modified"},{"patch":"@@ -655,2 +655,3 @@\n-  \/\/ actually rewritten constant pool cache indices.\n-  \/\/ The routine remap_instruction_operand_from_cache manages the adjustment\n+  \/\/ actually rewritten indices that point to entries in their respective structures\n+  \/\/ i.e. ResolvedMethodEntries or ResolvedFieldEntries.\n+  \/\/ The routine to_cp_index manages the adjustment\n@@ -661,5 +662,0 @@\n-  \/\/ FIXME: Consider renaming these with a prefix \"cached_\" to make the distinction clear.\n-  \/\/ In a few cases (the verifier) there are uses before a cpcache has been built,\n-  \/\/ which are handled by a dynamic check in remap_instruction_operand_from_cache.\n-  \/\/ FIXME: Remove the dynamic check, and adjust all callers to specify the correct mode.\n-\n@@ -681,2 +677,0 @@\n-  int remap_instruction_operand_from_cache(int operand);  \/\/ operand must be biased by CPCACHE_INDEX_TAG\n-\n@@ -805,13 +799,0 @@\n-#ifdef ASSERT\n-  enum { CPCACHE_INDEX_TAG = 0x10000 };  \/\/ helps keep CP cache indices distinct from CP indices\n-#else\n-  enum { CPCACHE_INDEX_TAG = 0 };        \/\/ in product mode, this zero value is a no-op\n-#endif \/\/ASSERT\n-\n-  static int decode_cpcache_index(int raw_index, bool invokedynamic_ok = false) {\n-    if (invokedynamic_ok && is_invokedynamic_index(raw_index))\n-      return decode_invokedynamic_index(raw_index);\n-    else\n-      return raw_index - CPCACHE_INDEX_TAG;\n-  }\n-\n@@ -935,0 +916,5 @@\n+  \/\/ ResolvedMethodEntry getters\n+  inline ResolvedMethodEntry* resolved_method_entry_at(int method_index);\n+  inline int resolved_method_entries_length() const;\n+  inline oop appendix_if_resolved(int method_index) const;\n+\n@@ -939,0 +925,1 @@\n+  inline oop resolved_reference_from_method(int index) const;\n","filename":"src\/hotspot\/share\/oops\/constantPool.hpp","additions":9,"deletions":22,"binary":false,"changes":31,"status":"modified"},{"patch":"@@ -52,0 +52,1 @@\n+#include \"oops\/resolvedMethodEntry.hpp\"\n@@ -60,85 +61,1 @@\n-\/\/ Implementation of ConstantPoolCacheEntry\n-\n-void ConstantPoolCacheEntry::initialize_entry(int index) {\n-  assert(0 < index && index < 0x10000, \"sanity check\");\n-  _indices = index;\n-  _f1 = nullptr;\n-  _f2 = _flags = 0;\n-  assert(constant_pool_index() == index, \"\");\n-}\n-\n-intx ConstantPoolCacheEntry::make_flags(TosState state,\n-                                       int option_bits,\n-                                       int field_index_or_method_params) {\n-  assert(state < number_of_states, \"Invalid state in make_flags\");\n-  intx f = ((int)state << tos_state_shift) | option_bits | field_index_or_method_params;\n-  \/\/ Preserve existing flag bit values\n-  \/\/ The low bits are a field offset, or else the method parameter size.\n-#ifdef ASSERT\n-  TosState old_state = flag_state();\n-  assert(old_state == (TosState)0 || old_state == state,\n-         \"inconsistent cpCache flags state\");\n-#endif\n-  return (_flags | f) ;\n-}\n-\n-void ConstantPoolCacheEntry::set_bytecode_1(Bytecodes::Code code) {\n-#ifdef ASSERT\n-  \/\/ Read once.\n-  volatile Bytecodes::Code c = bytecode_1();\n-  assert(c == 0 || c == code || code == 0, \"update must be consistent\");\n-#endif\n-  \/\/ Need to flush pending stores here before bytecode is written.\n-  Atomic::release_store(&_indices, _indices | ((u_char)code << bytecode_1_shift));\n-}\n-\n-void ConstantPoolCacheEntry::set_bytecode_2(Bytecodes::Code code) {\n-#ifdef ASSERT\n-  \/\/ Read once.\n-  volatile Bytecodes::Code c = bytecode_2();\n-  assert(c == 0 || c == code || code == 0, \"update must be consistent\");\n-#endif\n-  \/\/ Need to flush pending stores here before bytecode is written.\n-  Atomic::release_store(&_indices, _indices | ((u_char)code << bytecode_2_shift));\n-}\n-\n-\/\/ Sets f1, ordering with previous writes.\n-void ConstantPoolCacheEntry::release_set_f1(Metadata* f1) {\n-  assert(f1 != nullptr, \"\");\n-  Atomic::release_store(&_f1, f1);\n-}\n-\n-void ConstantPoolCacheEntry::set_indy_resolution_failed() {\n-  Atomic::release_store(&_flags, _flags | (1 << indy_resolution_failed_shift));\n-}\n-\n-\/\/ Note that concurrent update of both bytecodes can leave one of them\n-\/\/ reset to zero.  This is harmless; the interpreter will simply re-resolve\n-\/\/ the damaged entry.  More seriously, the memory synchronization is needed\n-\/\/ to flush other fields (f1, f2) completely to memory before the bytecodes\n-\/\/ are updated, lest other processors see a non-zero bytecode but zero f1\/f2.\n-void ConstantPoolCacheEntry::set_field(Bytecodes::Code get_code,\n-                                       Bytecodes::Code put_code,\n-                                       Klass* field_holder,\n-                                       int field_index,\n-                                       int field_offset,\n-                                       TosState field_type,\n-                                       bool is_final,\n-                                       bool is_volatile,\n-                                       bool is_flat,\n-                                       bool is_null_free_inline_type) {\n-  set_f1(field_holder);\n-  set_f2(field_offset);\n-  assert((field_index & field_index_mask) == field_index,\n-         \"field index does not fit in low flag bits\");\n-  assert(!is_flat || is_null_free_inline_type, \"Sanity check\");\n-  set_field_flags(field_type,\n-                  ((is_volatile ? 1 : 0) << is_volatile_shift) |\n-                  ((is_final    ? 1 : 0) << is_final_shift) |\n-                  ((is_flat     ? 1 : 0) << is_flat_shift) |\n-                  ((is_null_free_inline_type ? 1 : 0) << is_null_free_inline_type_shift),\n-                  field_index);\n-  set_bytecode_1(get_code);\n-  set_bytecode_2(put_code);\n-  NOT_PRODUCT(verify(tty));\n-}\n+\/\/ Implementation of ConstantPoolCache\n@@ -146,16 +63,11 @@\n-void ConstantPoolCacheEntry::set_parameter_size(int value) {\n-  \/\/ This routine is called only in corner cases where the CPCE is not yet initialized.\n-  \/\/ See AbstractInterpreter::deopt_continue_after_entry.\n-  assert(_flags == 0 || parameter_size() == 0 || parameter_size() == value,\n-         \"size must not change: parameter_size=%d, value=%d\", parameter_size(), value);\n-  \/\/ Setting the parameter size by itself is only safe if the\n-  \/\/ current value of _flags is 0, otherwise another thread may have\n-  \/\/ updated it and we don't want to overwrite that value.  Don't\n-  \/\/ bother trying to update it once it's nonzero but always make\n-  \/\/ sure that the final parameter size agrees with what was passed.\n-  if (_flags == 0) {\n-    intx newflags = (value & parameter_size_mask);\n-    Atomic::cmpxchg(&_flags, (intx)0, newflags);\n-  }\n-  guarantee(parameter_size() == value,\n-            \"size must not change: parameter_size=%d, value=%d\", parameter_size(), value);\n+template <class T>\n+static Array<T>* initialize_resolved_entries_array(ClassLoaderData* loader_data, GrowableArray<T> entries, TRAPS) {\n+  Array<T>* resolved_entries;\n+  if (entries.length() != 0) {\n+    resolved_entries = MetadataFactory::new_array<T>(loader_data, entries.length(), CHECK_NULL);\n+    for (int i = 0; i < entries.length(); i++) {\n+      resolved_entries->at_put(i, entries.at(i));\n+    }\n+    return resolved_entries;\n+  }\n+  return nullptr;\n@@ -164,1 +76,2 @@\n-void ConstantPoolCacheEntry::set_direct_or_vtable_call(Bytecodes::Code invoke_code,\n+void ConstantPoolCache::set_direct_or_vtable_call(Bytecodes::Code invoke_code,\n+                                                       int method_index,\n@@ -175,0 +88,1 @@\n+  ResolvedMethodEntry* method_entry = resolved_method_entry_at(method_index);\n@@ -182,6 +96,6 @@\n-        \/\/ set_f2_as_vfinal_method checks if is_vfinal flag is true.\n-        set_method_flags(as_TosState(method->result_type()),\n-                         (                             1      << is_vfinal_shift) |\n-                         ((method->is_final_method() ? 1 : 0) << is_final_shift),\n-                         method()->size_of_parameters());\n-        set_f2_as_vfinal_method(method());\n+\n+        method_entry->set_flags((                             1      << ResolvedMethodEntry::is_vfinal_shift) |\n+                                ((method->is_final_method() ? 1 : 0) << ResolvedMethodEntry::is_final_shift));\n+        method_entry->fill_in((u1)as_TosState(method->result_type()), (u2)method()->size_of_parameters());\n+        assert(method_entry->is_vfinal(), \"flags must be set\");\n+        method_entry->set_method(method());\n@@ -189,1 +103,1 @@\n-        set_f1(holder); \/\/ interface klass*\n+        method_entry->set_klass(holder);\n@@ -209,7 +123,6 @@\n-          \/\/ set_f2_as_vfinal_method checks if is_vfinal flag is true.\n-          set_method_flags(as_TosState(method->result_type()),\n-                           (                             1      << is_vfinal_shift) |\n-                           ((method->is_final_method() ? 1 : 0) << is_final_shift)  |\n-                           ((change_to_virtual         ? 1 : 0) << is_forced_virtual_shift),\n-                           method()->size_of_parameters());\n-          set_f2_as_vfinal_method(method());\n+          method_entry->set_flags((                             1      << ResolvedMethodEntry::is_vfinal_shift) |\n+                                  ((method->is_final_method() ? 1 : 0) << ResolvedMethodEntry::is_final_shift)  |\n+                                  ((change_to_virtual         ? 1 : 0) << ResolvedMethodEntry::is_forced_virtual_shift));\n+          method_entry->fill_in((u1)as_TosState(method->result_type()), (u2)method()->size_of_parameters());\n+          assert(method_entry->is_vfinal(), \"flags must be set\");\n+          method_entry->set_method(method());\n@@ -220,4 +133,4 @@\n-          set_method_flags(as_TosState(method->result_type()),\n-                           ((change_to_virtual ? 1 : 0) << is_forced_virtual_shift),\n-                           method()->size_of_parameters());\n-          set_f2(vtable_index);\n+          method_entry->set_flags((change_to_virtual ? 1 : 0) << ResolvedMethodEntry::is_forced_virtual_shift);\n+          method_entry->fill_in((u1)as_TosState(method->result_type()), (u2)method()->size_of_parameters());\n+          assert(!method_entry->is_vfinal(), \"flags must not be set\");\n+          method_entry->set_table_index(vtable_index);\n@@ -230,1 +143,1 @@\n-    case Bytecodes::_invokestatic:\n+    case Bytecodes::_invokestatic: {\n@@ -236,5 +149,5 @@\n-      set_method_flags(as_TosState(method->result_type()),\n-                       ((is_vfinal()               ? 1 : 0) << is_vfinal_shift) |\n-                       ((method->is_final_method() ? 1 : 0) << is_final_shift),\n-                       method()->size_of_parameters());\n-      set_f1(method());\n+      bool vfinal = method_entry->is_vfinal();\n+      method_entry->set_flags(((method->is_final_method() ? 1 : 0) << ResolvedMethodEntry::is_final_shift));\n+      assert(vfinal == method_entry->is_vfinal(), \"Vfinal flag must be preserved\");\n+      method_entry->fill_in((u1)as_TosState(method->result_type()), (u2)method()->size_of_parameters());\n+      method_entry->set_method(method());\n@@ -243,0 +156,1 @@\n+    }\n@@ -278,1 +192,1 @@\n-      set_bytecode_1(invoke_code);\n+      method_entry->set_bytecode1(invoke_code);\n@@ -309,1 +223,1 @@\n-        set_bytecode_1(invoke_code);\n+        method_entry->set_bytecode1(invoke_code);\n@@ -313,1 +227,1 @@\n-    set_bytecode_2(invoke_code);\n+    method_entry->set_bytecode2(invoke_code);\n@@ -317,1 +231,0 @@\n-  NOT_PRODUCT(verify(tty));\n@@ -320,2 +233,2 @@\n-void ConstantPoolCacheEntry::set_direct_call(Bytecodes::Code invoke_code, const methodHandle& method,\n-                                             bool sender_is_interface) {\n+void ConstantPoolCache::set_direct_call(Bytecodes::Code invoke_code, int method_index, const methodHandle& method,\n+                                        bool sender_is_interface) {\n@@ -324,1 +237,1 @@\n-  set_direct_or_vtable_call(invoke_code, method, index, sender_is_interface);\n+  set_direct_or_vtable_call(invoke_code, method_index, method, index, sender_is_interface);\n@@ -327,1 +240,1 @@\n-void ConstantPoolCacheEntry::set_vtable_call(Bytecodes::Code invoke_code, const methodHandle& method, int index) {\n+void ConstantPoolCache::set_vtable_call(Bytecodes::Code invoke_code, int method_index, const methodHandle& method, int index) {\n@@ -331,1 +244,1 @@\n-  set_direct_or_vtable_call(invoke_code, method, index, false);\n+  set_direct_or_vtable_call(invoke_code, method_index, method, index, false);\n@@ -334,3 +247,4 @@\n-void ConstantPoolCacheEntry::set_itable_call(Bytecodes::Code invoke_code,\n-                                             Klass* referenced_klass,\n-                                             const methodHandle& method, int index) {\n+void ConstantPoolCache::set_itable_call(Bytecodes::Code invoke_code,\n+                                        int method_index,\n+                                        Klass* referenced_klass,\n+                                        const methodHandle& method, int index) {\n@@ -342,20 +256,12 @@\n-  set_f1(referenced_klass);\n-  set_f2((intx)method());\n-  set_method_flags(as_TosState(method->result_type()),\n-                   0,  \/\/ no option bits\n-                   method()->size_of_parameters());\n-  set_bytecode_1(Bytecodes::_invokeinterface);\n-}\n-\n-\n-void ConstantPoolCacheEntry::set_method_handle(const constantPoolHandle& cpool, const CallInfo &call_info) {\n-  set_method_handle_common(cpool, Bytecodes::_invokehandle, call_info);\n-}\n-\n-void ConstantPoolCacheEntry::set_method_handle_common(const constantPoolHandle& cpool,\n-                                                      Bytecodes::Code invoke_code,\n-                                                      const CallInfo &call_info) {\n-  \/\/ NOTE: This CPCE can be the subject of data races.\n-  \/\/ There are three words to update: flags, refs[f2], f1 (in that order).\n-  \/\/ Writers must store all other values before f1.\n-  \/\/ Readers must test f1 first for non-null before reading other fields.\n+  ResolvedMethodEntry* method_entry = resolved_method_entry_at(method_index);\n+  method_entry->set_klass(static_cast<InstanceKlass*>(referenced_klass));\n+  method_entry->set_method(method());\n+  method_entry->fill_in((u1)as_TosState(method->result_type()), (u2)method()->size_of_parameters());\n+  method_entry->set_bytecode1(Bytecodes::_invokeinterface);\n+}\n+\n+ResolvedMethodEntry* ConstantPoolCache::set_method_handle(int method_index, const CallInfo &call_info) {\n+  \/\/ NOTE: This method entry can be the subject of data races.\n+  \/\/ There are three words to update: flags, refs[appendix_index], method (in that order).\n+  \/\/ Writers must store all other values before method.\n+  \/\/ Readers must test the method first for non-null before reading other fields.\n@@ -363,1 +269,1 @@\n-  \/\/ A losing writer waits on the lock until the winner writes f1 and leaves\n+  \/\/ A losing writer waits on the lock until the winner writes the method and leaves\n@@ -367,5 +273,3 @@\n-  MutexLocker ml(cpool->pool_holder()->init_monitor());\n-\n-  if (!is_f1_null()) {\n-    return;\n-  }\n+  Bytecodes::Code invoke_code = Bytecodes::_invokehandle;\n+  MutexLocker ml(constant_pool()->pool_holder()->init_monitor());\n+  ResolvedMethodEntry* method_entry = resolved_method_entry_at(method_index);\n@@ -373,17 +277,2 @@\n-  if (indy_resolution_failed()) {\n-    \/\/ Before we got here, another thread got a LinkageError exception during\n-    \/\/ resolution.  Ignore our success and throw their exception.\n-    ConstantPoolCache* cpCache = cpool->cache();\n-    int index = -1;\n-    for (int i = 0; i < cpCache->length(); i++) {\n-      if (cpCache->entry_at(i) == this) {\n-        index = i;\n-        break;\n-      }\n-    }\n-    guarantee(index >= 0, \"Didn't find cpCache entry!\");\n-    int encoded_index = ResolutionErrorTable::encode_cpcache_index(\n-                          ConstantPool::encode_invokedynamic_index(index));\n-    JavaThread* THREAD = JavaThread::current(); \/\/ For exception macros.\n-    ConstantPool::throw_resolution_error(cpool, encoded_index, THREAD);\n-    return;\n+  if (method_entry->is_resolved(invoke_code)) {\n+    return method_entry;\n@@ -397,23 +286,8 @@\n-  \/\/ MHs and indy are always sig-poly and have a local signature.\n-  set_method_flags(as_TosState(adapter->result_type()),\n-                   ((has_appendix    ? 1 : 0) << has_appendix_shift        ) |\n-                   (                   1      << has_local_signature_shift ) |\n-                   (                   1      << is_final_shift            ),\n-                   adapter->size_of_parameters());\n-\n-  LogStream* log_stream = nullptr;\n-  LogStreamHandle(Debug, methodhandles, indy) lsh_indy;\n-  if (lsh_indy.is_enabled()) {\n-    ResourceMark rm;\n-    log_stream = &lsh_indy;\n-    log_stream->print_cr(\"set_method_handle bc=%d appendix=\" PTR_FORMAT \"%s method=\" PTR_FORMAT \" (local signature) \",\n-                         invoke_code,\n-                         p2i(appendix()),\n-                         (has_appendix ? \"\" : \" (unused)\"),\n-                         p2i(adapter));\n-    adapter->print_on(log_stream);\n-    if (has_appendix)  appendix()->print_on(log_stream);\n-  }\n-\n-  \/\/ Method handle invokes and invokedynamic sites use both cp cache words.\n-  \/\/ refs[f2], if not null, contains a value passed as a trailing argument to the adapter.\n+  \/\/ MHs are always sig-poly and have a local signature.\n+  method_entry->fill_in((u1)as_TosState(adapter->result_type()), (u2)adapter->size_of_parameters());\n+  method_entry->set_flags(((has_appendix    ? 1 : 0) << ResolvedMethodEntry::has_appendix_shift        ) |\n+                          (                   1      << ResolvedMethodEntry::has_local_signature_shift ) |\n+                          (                   1      << ResolvedMethodEntry::is_final_shift            ));\n+\n+  \/\/ Method handle invokes use both a method and a resolved references index.\n+  \/\/ refs[appendix_index], if not null, contains a value passed as a trailing argument to the adapter.\n@@ -422,1 +296,1 @@\n-  \/\/ f1 contains the adapter method which manages the actual call.\n+  \/\/ method_entry->method() contains the adapter method which manages the actual call.\n@@ -426,1 +300,1 @@\n-  \/\/ JVM-level linking is via f1, as if for invokespecial, and signatures are erased.\n+  \/\/ JVM-level linking is via the method, as if for invokespecial, and signatures are erased.\n@@ -431,1 +305,1 @@\n-  \/\/ the f1 method has signature '(Ljl\/Object;Ljl\/invoke\/MethodType;)Ljl\/Object;',\n+  \/\/ the method has signature '(Ljl\/Object;Ljl\/invoke\/MethodType;)Ljl\/Object;',\n@@ -433,1 +307,1 @@\n-  \/\/ The fact that String and List are involved is encoded in the MethodType in refs[f2].\n+  \/\/ The fact that String and List are involved is encoded in the MethodType in refs[appendix_index].\n@@ -439,3 +313,5 @@\n-    const int appendix_index = f2_as_index();\n-    oop old_oop = cpool->set_resolved_reference_at(appendix_index, appendix());\n-    assert(old_oop == nullptr, \"init just once\");\n+    const int appendix_index = method_entry->resolved_references_index();\n+    objArrayOop resolved_references = constant_pool()->resolved_references();\n+    assert(appendix_index >= 0 && appendix_index < resolved_references->length(), \"oob\");\n+    assert(resolved_references->obj_at(appendix_index) == nullptr, \"init just once\");\n+    resolved_references->obj_at_put(appendix_index, appendix());\n@@ -444,1 +320,1 @@\n-  release_set_f1(adapter);  \/\/ This must be the last one to set (see NOTE above)!\n+  method_entry->set_method(adapter); \/\/ This must be the last one to set (see NOTE above)!\n@@ -448,2 +324,1 @@\n-  set_bytecode_1(invoke_code);\n-  NOT_PRODUCT(verify(tty));\n+  method_entry->set_bytecode1(invoke_code);\n@@ -451,6 +326,3 @@\n-  if (log_stream != nullptr) {\n-    this->print(log_stream, 0, cpool->cache());\n-  }\n-\n-  assert(has_appendix == this->has_appendix(), \"proper storage of appendix flag\");\n-  assert(this->has_local_signature(), \"proper storage of signature flag\");\n+  assert(has_appendix == method_entry->has_appendix(), \"proper storage of appendix flag\");\n+  assert(method_entry->has_local_signature(), \"proper storage of signature flag\");\n+  return method_entry;\n@@ -459,1 +331,1 @@\n-Method* ConstantPoolCacheEntry::method_if_resolved(const constantPoolHandle& cpool) const {\n+Method* ConstantPoolCache::method_if_resolved(int method_index) const {\n@@ -461,38 +333,13 @@\n-  Bytecodes::Code invoke_code = bytecode_1();\n-  if (invoke_code != (Bytecodes::Code)0) {\n-    Metadata* f1 = f1_ord();\n-    if (f1 != nullptr) {\n-      switch (invoke_code) {\n-      case Bytecodes::_invokeinterface:\n-        assert(f1->is_klass(), \"\");\n-        return f2_as_interface_method();\n-      case Bytecodes::_invokestatic:\n-      case Bytecodes::_invokespecial:\n-        assert(!has_appendix(), \"\");\n-      case Bytecodes::_invokehandle:\n-        assert(f1->is_method(), \"\");\n-        return (Method*)f1;\n-      case Bytecodes::_invokedynamic:\n-        ShouldNotReachHere();\n-      default:\n-        break;\n-      }\n-    }\n-  }\n-  invoke_code = bytecode_2();\n-  if (invoke_code != (Bytecodes::Code)0) {\n-    switch (invoke_code) {\n-    case Bytecodes::_invokevirtual:\n-      if (is_vfinal()) {\n-        \/\/ invokevirtual\n-        Method* m = f2_as_vfinal_method();\n-        assert(m->is_method(), \"\");\n-        return m;\n-      } else {\n-        int holder_index = cpool->uncached_klass_ref_index_at(constant_pool_index());\n-        if (cpool->tag_at(holder_index).is_klass()) {\n-          Klass* klass = cpool->resolved_klass_at(holder_index);\n-          return klass->method_at_vtable(f2_as_index());\n-        }\n-      }\n-      break;\n+  ResolvedMethodEntry* method_entry = resolved_method_entry_at(method_index);\n+\n+  Bytecodes::Code invoke_code = (Bytecodes::Code)method_entry->bytecode1();\n+  switch (invoke_code) {\n+    case Bytecodes::_invokeinterface:\n+    case Bytecodes::_invokestatic:\n+    case Bytecodes::_invokespecial:\n+      assert(!method_entry->has_appendix(), \"\");\n+      \/\/ fall through\n+    case Bytecodes::_invokehandle:\n+      return method_entry->method();\n+    case Bytecodes::_invokedynamic:\n+      ShouldNotReachHere();\n@@ -500,0 +347,1 @@\n+      assert(invoke_code == (Bytecodes::Code)0, \"unexpected bytecode\");\n@@ -501,84 +349,4 @@\n-    }\n-  }\n-  return nullptr;\n-}\n-\n-\n-oop ConstantPoolCacheEntry::appendix_if_resolved(const constantPoolHandle& cpool) const {\n-  if (!has_appendix())\n-    return nullptr;\n-  const int ref_index = f2_as_index();\n-  return cpool->resolved_reference_at(ref_index);\n-}\n-\n-\n-#if INCLUDE_JVMTI\n-\n-void log_adjust(const char* entry_type, Method* old_method, Method* new_method, bool* trace_name_printed) {\n-  ResourceMark rm;\n-\n-  if (!(*trace_name_printed)) {\n-    log_info(redefine, class, update)(\"adjust: name=%s\", old_method->method_holder()->external_name());\n-    *trace_name_printed = true;\n-  log_trace(redefine, class, update, constantpool)\n-    (\"cpc %s entry update: %s\", entry_type, new_method->external_name());\n-}\n-\/\/ RedefineClasses() API support:\n-\/\/ If this ConstantPoolCacheEntry refers to old_method then update it\n-\/\/ to refer to new_method.\n-void ConstantPoolCacheEntry::adjust_method_entry(Method* old_method,\n-       Method* new_method, bool * trace_name_printed) {\n-\n-  if (is_vfinal()) {\n-    \/\/ virtual and final so _f2 contains method ptr instead of vtable index\n-    if (f2_as_vfinal_method() == old_method) {\n-      \/\/ match old_method so need an update\n-      \/\/ NOTE: can't use set_f2_as_vfinal_method as it asserts on different values\n-      _f2 = (intptr_t)new_method;\n-      log_adjust(\"vfinal\", old_method, new_method, trace_name_printed);\n-    }\n-    return;\n-  }\n-\n-  assert (_f1 != nullptr, \"should not call with uninteresting entry\");\n-\n-  if (!(_f1->is_method())) {\n-    \/\/ _f1 is a Klass* for an interface, _f2 is the method\n-    if (f2_as_interface_method() == old_method) {\n-      _f2 = (intptr_t)new_method;\n-      log_adjust(\"interface\", old_method, new_method, trace_name_printed);\n-    }\n-  } else if (_f1 == old_method) {\n-    _f1 = new_method;\n-    log_adjust(\"special, static or dynamic\", old_method, new_method, trace_name_printed);\n-  }\n-}\n-\n-\/\/ a constant pool cache entry should never contain old or obsolete methods\n-bool ConstantPoolCacheEntry::check_no_old_or_obsolete_entries() {\n-  Method* m = get_interesting_method_entry();\n-  \/\/ return false if m refers to a non-deleted old or obsolete method\n-  if (m != nullptr) {\n-    assert(m->is_valid() && m->is_method(), \"m is a valid method\");\n-    return !m->is_old() && !m->is_obsolete(); \/\/ old is always set for old and obsolete\n-  } else {\n-    return true;\n-  }\n-}\n-\n-Method* ConstantPoolCacheEntry::get_interesting_method_entry() {\n-  if (!is_method_entry()) {\n-    \/\/ not a method entry so not interesting by default\n-    return nullptr;\n-  }\n-  Method* m = nullptr;\n-  if (is_vfinal()) {\n-    \/\/ virtual and final so _f2 contains method ptr instead of vtable index\n-    m = f2_as_vfinal_method();\n-  } else if (is_f1_null()) {\n-    \/\/ null _f1 means this is a virtual entry so also not interesting\n-    return nullptr;\n-  } else {\n-    if (!(_f1->is_method())) {\n-      \/\/ _f1 is a Klass* for an interface\n-      m = f2_as_interface_method();\n+  invoke_code = (Bytecodes::Code)method_entry->bytecode2();\n+  if (invoke_code == Bytecodes::_invokevirtual) {\n+    if (method_entry->is_vfinal()) {\n+      return method_entry->method();\n@@ -588,43 +356,4 @@\n-      m = f1_as_method();\n-    }\n-  }\n-  assert(m != nullptr && m->is_method(), \"sanity check\");\n-  if (m == nullptr || !m->is_method()) {\n-    return nullptr;\n-  }\n-  return m;\n-}\n-#endif \/\/ INCLUDE_JVMTI\n-\n-void ConstantPoolCacheEntry::print(outputStream* st, int index, const ConstantPoolCache* cache) const {\n-  \/\/ print separator\n-  if (index == 0) st->print_cr(\"                 -------------\");\n-  \/\/ print universal entry info\n-  st->print_cr(\"%3d\", index);\n-  st->print_cr(\" - this: \" PTR_FORMAT, p2i(this));\n-  st->print_cr(\" - bytecode 1: %s %02x\", Bytecodes::name(bytecode_1()), bytecode_1());\n-  st->print_cr(\" - bytecode 2: %s %02x\", Bytecodes::name(bytecode_2()), bytecode_2());\n-  st->print_cr(\" - cp index: %5d\", constant_pool_index());\n-  if (is_method_entry()) {\n-    ResourceMark rm;\n-    constantPoolHandle cph(Thread::current(), cache->constant_pool());\n-    Method* m = method_if_resolved(cph);\n-    st->print_cr(\" - F1:  [   \" PTR_FORMAT \"]\", (intptr_t)_f1);\n-    st->print_cr(\" - F2:  [   \" PTR_FORMAT \"]\", (intptr_t)_f2);\n-    st->print_cr(\" - method: \" INTPTR_FORMAT \" %s\", p2i(m), m != nullptr ? m->external_name() : nullptr);\n-    st->print_cr(\" - flag values: [%02x|0|0|%01x|%01x|%01x|%01x|0|%01x|%01x|00|00|%02x]\",\n-                 flag_state(), has_local_signature(), has_appendix(),\n-                 is_forced_virtual(), is_final(), is_vfinal(),\n-                 indy_resolution_failed(), parameter_size());\n-    st->print_cr(\" - tos: %s\\n - local signature: %01x\\n\"\n-                 \" - has appendix: %01x\\n - forced virtual: %01x\\n\"\n-                 \" - final: %01x\\n - virtual final: %01x\\n - resolution failed: %01x\\n\"\n-                 \" - num parameters: %02x\",\n-                 type2name(as_BasicType(flag_state())), has_local_signature(), has_appendix(),\n-                 is_forced_virtual(), is_final(), is_vfinal(),\n-                 indy_resolution_failed(), parameter_size());\n-    if ((bytecode_1() == Bytecodes::_invokehandle)) {\n-      oop appendix = appendix_if_resolved(cph);\n-      if (appendix != nullptr) {\n-        st->print(\"  appendix: \");\n-        appendix->print_on(st);\n+      int holder_index = constant_pool()->uncached_klass_ref_index_at(method_entry->constant_pool_index());\n+      if (constant_pool()->tag_at(holder_index).is_klass()) {\n+        Klass* klass = constant_pool()->resolved_klass_at(holder_index);\n+        return klass->method_at_vtable(method_entry->table_index());\n@@ -633,27 +362,0 @@\n-  } else {\n-    assert(is_field_entry(), \"must be a field entry\");\n-    st->print_cr(\" - F1:  [   \" PTR_FORMAT \"]\", (intptr_t)_f1);\n-    st->print_cr(\" - F2:  [   \" PTR_FORMAT \"]\", (intptr_t)_f2);\n-    st->print_cr(\" - flag values: [%02x|0|1|0|0|0|%01x|%01x|0|0|%04x]\",\n-                 flag_state(), is_final(), is_volatile(), field_index());\n-    st->print_cr(\" - tos: %s\\n - final: %d\\n - volatile: %d\\n - field index: %04x\",\n-                 type2name(as_BasicType(flag_state())), is_final(), is_volatile(), field_index());\n-  }\n-  st->print_cr(\"                 -------------\");\n-}\n-\n-void ConstantPoolCacheEntry::verify(outputStream* st) const {\n-  \/\/ not implemented yet\n-}\n-\n-\/\/ Implementation of ConstantPoolCache\n-\n-template <class T>\n-static Array<T>* initialize_resolved_entries_array(ClassLoaderData* loader_data, GrowableArray<T> entries, TRAPS) {\n-  Array<T>* resolved_entries;\n-  if (entries.length() != 0) {\n-    resolved_entries = MetadataFactory::new_array<T>(loader_data, entries.length(), CHECK_NULL);\n-    for (int i = 0; i < entries.length(); i++) {\n-      resolved_entries->at_put(i, entries.at(i));\n-    }\n-    return resolved_entries;\n@@ -665,1 +367,0 @@\n-                                     const intStack& index_map,\n@@ -669,0 +370,1 @@\n+                                     const GrowableArray<ResolvedMethodEntry> method_entries,\n@@ -671,2 +373,1 @@\n-  const int length = index_map.length();\n-  int size = ConstantPoolCache::size(length);\n+  int size = ConstantPoolCache::size();\n@@ -677,0 +378,1 @@\n+  Array<ResolvedMethodEntry>* resolved_method_entries = initialize_resolved_entries_array(loader_data, method_entries, CHECK_NULL);\n@@ -679,18 +381,1 @@\n-              ConstantPoolCache(length, index_map, invokedynamic_map, resolved_indy_entries, resolved_field_entries);\n-}\n-\n-void ConstantPoolCache::initialize(const intArray& inverse_index_map,\n-                                   const intArray& invokedynamic_references_map) {\n-  for (int i = 0; i < inverse_index_map.length(); i++) {\n-    ConstantPoolCacheEntry* e = entry_at(i);\n-    int original_index = inverse_index_map.at(i);\n-    e->initialize_entry(original_index);\n-    assert(entry_at(i) == e, \"sanity\");\n-  }\n-\n-  for (int ref = 0; ref < invokedynamic_references_map.length(); ref++) {\n-    const int cpci = invokedynamic_references_map.at(ref);\n-    if (cpci >= 0) {\n-      entry_at(cpci)->initialize_resolved_reference_index(ref);\n-    }\n-  }\n+              ConstantPoolCache(invokedynamic_map, resolved_indy_entries, resolved_field_entries, resolved_method_entries);\n@@ -705,8 +390,0 @@\n-void ConstantPoolCache::save_for_archive(TRAPS) {\n-  ClassLoaderData* loader_data = constant_pool()->pool_holder()->class_loader_data();\n-  _initial_entries = MetadataFactory::new_array<ConstantPoolCacheEntry>(loader_data, length(), CHECK);\n-  for (int i = 0; i < length(); i++) {\n-    _initial_entries->at_put(i, *entry_at(i));\n-  }\n-}\n-\n@@ -718,9 +395,0 @@\n-  assert(_initial_entries != nullptr, \"archived cpcache must have been initialized\");\n-  assert(!ArchiveBuilder::current()->is_in_buffer_space(_initial_entries), \"must be\");\n-  for (int i=0; i<length(); i++) {\n-    \/\/ Restore each entry to the initial state -- just after Rewriter::make_constant_pool_cache()\n-    \/\/ has finished.\n-    *entry_at(i) = _initial_entries->at(i);\n-  }\n-  _initial_entries = nullptr;\n-\n@@ -737,0 +405,5 @@\n+  if (_resolved_method_entries != nullptr) {\n+    for (int i = 0; i < _resolved_method_entries->length(); i++) {\n+      resolved_method_entry_at(i)->remove_unshareable_info();\n+    }\n+  }\n@@ -747,12 +420,11 @@\n-  if (_initial_entries != nullptr) {\n-    assert(CDSConfig::is_dumping_archive(), \"sanity\");\n-    MetadataFactory::free_array<ConstantPoolCacheEntry>(data, _initial_entries);\n-    if (_resolved_indy_entries) {\n-      MetadataFactory::free_array<ResolvedIndyEntry>(data, _resolved_indy_entries);\n-      _resolved_indy_entries = nullptr;\n-    }\n-    if (_resolved_field_entries) {\n-      MetadataFactory::free_array<ResolvedFieldEntry>(data, _resolved_field_entries);\n-      _resolved_field_entries = nullptr;\n-    }\n-    _initial_entries = nullptr;\n+  if (_resolved_indy_entries != nullptr) {\n+    MetadataFactory::free_array<ResolvedIndyEntry>(data, _resolved_indy_entries);\n+    _resolved_indy_entries = nullptr;\n+  }\n+  if (_resolved_field_entries != nullptr) {\n+    MetadataFactory::free_array<ResolvedFieldEntry>(data, _resolved_field_entries);\n+    _resolved_field_entries = nullptr;\n+  }\n+  if (_resolved_method_entries != nullptr) {\n+    MetadataFactory::free_array<ResolvedMethodEntry>(data, _resolved_method_entries);\n+    _resolved_method_entries = nullptr;\n@@ -785,0 +457,11 @@\n+void log_adjust(const char* entry_type, Method* old_method, Method* new_method, bool* trace_name_printed) {\n+  ResourceMark rm;\n+\n+  if (!(*trace_name_printed)) {\n+    log_info(redefine, class, update)(\"adjust: name=%s\", old_method->method_holder()->external_name());\n+    *trace_name_printed = true;\n+  }\n+  log_trace(redefine, class, update, constantpool)\n+    (\"cpc %s entry update: %s\", entry_type, new_method->external_name());\n+}\n+\n@@ -800,10 +483,16 @@\n-  for (int i = 0; i < length(); i++) {\n-    ConstantPoolCacheEntry* entry = entry_at(i);\n-    Method* old_method = entry->get_interesting_method_entry();\n-    if (old_method == nullptr || !old_method->is_old()) {\n-      continue; \/\/ skip uninteresting entries\n-    }\n-    if (old_method->is_deleted()) {\n-      \/\/ clean up entries with deleted methods\n-      entry->initialize_entry(entry->constant_pool_index());\n-      continue;\n+  if (_resolved_method_entries != nullptr) {\n+    for (int i = 0; i < _resolved_method_entries->length(); i++) {\n+      ResolvedMethodEntry* method_entry = resolved_method_entry_at(i);\n+      \/\/ get interesting method entry\n+      Method* old_method = method_entry->method();\n+      if (old_method == nullptr || !old_method->is_old()) {\n+        continue; \/\/ skip uninteresting entries\n+      }\n+      if (old_method->is_deleted()) {\n+        \/\/ clean up entries with deleted methods\n+        method_entry->reset_entry();\n+        continue;\n+      }\n+      Method* new_method = old_method->get_new_method();\n+      method_entry->adjust_method_entry(new_method);\n+      log_adjust(\"non-indy\", old_method, new_method, trace_name_printed);\n@@ -811,2 +500,0 @@\n-    Method* new_method = old_method->get_new_method();\n-    entry_at(i)->adjust_method_entry(old_method, new_method, trace_name_printed);\n@@ -819,1 +506,1 @@\n-  if (_resolved_indy_entries) {\n+  if (_resolved_indy_entries != nullptr) {\n@@ -830,8 +517,10 @@\n-\n-  for (int i = 1; i < length(); i++) {\n-    Method* m = entry_at(i)->get_interesting_method_entry();\n-    if (m != nullptr && !entry_at(i)->check_no_old_or_obsolete_entries()) {\n-      log_trace(redefine, class, update, constantpool)\n-        (\"cpcache check found old method entry: class: %s, old: %d, obsolete: %d, method: %s\",\n-         constant_pool()->pool_holder()->external_name(), m->is_old(), m->is_obsolete(), m->external_name());\n-      return false;\n+  if (_resolved_method_entries != nullptr) {\n+    for (int i = 0; i < _resolved_method_entries->length(); i++) {\n+      ResolvedMethodEntry* method_entry = resolved_method_entry_at(i);\n+      Method* m = method_entry->method();\n+      if (m != nullptr && !method_entry->check_no_old_or_obsolete_entry()) {\n+        log_trace(redefine, class, update, constantpool)\n+          (\"cpcache check found old method entry: class: %s, old: %d, obsolete: %d, method: %s\",\n+           constant_pool()->pool_holder()->external_name(), m->is_old(), m->is_obsolete(), m->external_name());\n+        return false;\n+      }\n@@ -844,5 +533,1 @@\n-  for (int i = 1; i < length(); i++) {\n-    if (entry_at(i)->get_interesting_method_entry() != nullptr) {\n-      entry_at(i)->print(tty, i, this);\n-    }\n-  }\n+  print_on(tty);\n@@ -862,0 +547,3 @@\n+  if (_resolved_method_entries != nullptr) {\n+    it->push(&_resolved_method_entries, MetaspaceClosure::_writable);\n+  }\n@@ -886,1 +574,1 @@\n-  int encoded_index = ResolutionErrorTable::encode_cpcache_index(\n+  int encoded_index = ResolutionErrorTable::encode_indy_index(\n@@ -906,1 +594,1 @@\n-    int encoded_index = ResolutionErrorTable::encode_cpcache_index(\n+    int encoded_index = ResolutionErrorTable::encode_indy_index(\n@@ -950,0 +638,12 @@\n+oop ConstantPoolCache::appendix_if_resolved(int method_index) const {\n+  ResolvedMethodEntry* method_entry = resolved_method_entry_at(method_index);\n+  return appendix_if_resolved(method_entry);\n+}\n+\n+oop ConstantPoolCache::appendix_if_resolved(ResolvedMethodEntry* method_entry) const {\n+  if (!method_entry->has_appendix())\n+    return nullptr;\n+  const int ref_index = method_entry->resolved_references_index();\n+  return constant_pool()->resolved_reference_at(ref_index);\n+}\n+\n@@ -955,1 +655,1 @@\n-  for (int i = 0; i < length(); i++) entry_at(i)->print(st, i, this);\n+  print_resolved_method_entries(st);\n@@ -960,8 +660,0 @@\n-void ConstantPoolCache::print_value_on(outputStream* st) const {\n-  st->print(\"cache [%d]\", length());\n-  print_address_on(st);\n-  st->print(\" for \");\n-  constant_pool()->print_value_on(st);\n-}\n-\n-\n@@ -974,0 +666,11 @@\n+void ConstantPoolCache::print_resolved_method_entries(outputStream* st) const {\n+  for (int method_index = 0; method_index < resolved_method_entries_length(); method_index++) {\n+    ResolvedMethodEntry* method_entry = resolved_method_entry_at(method_index);\n+    method_entry->print_on(st);\n+    if (method_entry->has_appendix()) {\n+      st->print(\"  appendix: \");\n+      constant_pool()->resolved_reference_from_method(method_index)->print_on(st);\n+    }\n+  }\n+}\n+\n@@ -984,7 +687,0 @@\n-\n-\/\/ Verification\n-\n-void ConstantPoolCache::verify_on(outputStream* st) {\n-  \/\/ print constant pool cache entries\n-  for (int i = 0; i < length(); i++) entry_at(i)->verify(st);\n-}\n","filename":"src\/hotspot\/share\/oops\/cpCache.cpp","additions":203,"deletions":507,"binary":false,"changes":710,"status":"modified"},{"patch":"@@ -1322,1 +1322,1 @@\n-      int idx = currentBC->has_index_u4() ? currentBC->get_index_u4() : currentBC->get_index_u2_cpcache();\n+      int idx = currentBC->has_index_u4() ? currentBC->get_index_u4() : currentBC->get_index_u2();\n@@ -1604,12 +1604,4 @@\n-    case Bytecodes::_getstatic:\n-      do_field(true,  true,  itr->get_index_u2(), itr->bci(), itr->code());\n-      break;\n-    case Bytecodes::_putstatic:\n-      do_field(false,  true,  itr->get_index_u2(), itr->bci(), itr->code());\n-      break;\n-    case Bytecodes::_getfield:\n-      do_field(true,  false,  itr->get_index_u2(), itr->bci(), itr->code());\n-      break;\n-    case Bytecodes::_putfield:\n-      do_field(false,  false,  itr->get_index_u2(), itr->bci(), itr->code());\n-      break;\n+    case Bytecodes::_getstatic:         do_field(true,   true,  itr->get_index_u2(), itr->bci(), itr->code()); break;\n+    case Bytecodes::_putstatic:         do_field(false,  true,  itr->get_index_u2(), itr->bci(), itr->code()); break;\n+    case Bytecodes::_getfield:          do_field(true,   false, itr->get_index_u2(), itr->bci(), itr->code()); break;\n+    case Bytecodes::_putfield:          do_field(false,  false, itr->get_index_u2(), itr->bci(), itr->code()); break;\n@@ -1619,3 +1611,3 @@\n-    case Bytecodes::_invokespecial:     do_method(false, itr->get_index_u2_cpcache(), itr->bci(), itr->code()); break;\n-    case Bytecodes::_invokestatic:      do_method(true , itr->get_index_u2_cpcache(), itr->bci(), itr->code()); break;\n-    case Bytecodes::_invokedynamic:     do_method(true , itr->get_index_u4(),         itr->bci(), itr->code()); break;\n+    case Bytecodes::_invokespecial:     do_method(false, itr->get_index_u2(), itr->bci(), itr->code()); break;\n+    case Bytecodes::_invokestatic:      do_method(true , itr->get_index_u2(), itr->bci(), itr->code()); break;\n+    case Bytecodes::_invokedynamic:     do_method(true , itr->get_index_u4(), itr->bci(), itr->code()); break;\n","filename":"src\/hotspot\/share\/oops\/generateOopMap.cpp","additions":8,"deletions":16,"binary":false,"changes":24,"status":"modified"},{"patch":"@@ -2105,1 +2105,1 @@\n-    assert(DynamicDumpSharedSpaces, \"must be\");\n+    assert(CDSConfig::is_dumping_dynamic_archive(), \"must be\");\n@@ -2940,0 +2940,1 @@\n+  assert(CDSConfig::is_dumping_archive(), \"must be\");\n@@ -2943,7 +2944,1 @@\n-  if (!MetaspaceShared::use_full_module_graph()) {\n-    _package_entry = nullptr;\n-  } else if (DynamicDumpSharedSpaces) {\n-    if (!MetaspaceShared::is_in_shared_metaspace(_package_entry)) {\n-      _package_entry = nullptr;\n-    }\n-  } else {\n+  if (CDSConfig::is_dumping_full_module_graph()) {\n@@ -2955,0 +2950,6 @@\n+  } else if (CDSConfig::is_dumping_dynamic_archive() &&\n+             CDSConfig::is_loading_full_module_graph() &&\n+             MetaspaceShared::is_in_shared_metaspace(_package_entry)) {\n+    \/\/ _package_entry is an archived package in the base archive. Leave it as is.\n+  } else {\n+    _package_entry = nullptr;\n@@ -3283,1 +3284,1 @@\n-    if (MetaspaceShared::use_full_module_graph() && _package_entry == pkg_entry) {\n+    if (CDSConfig::is_loading_full_module_graph() && _package_entry == pkg_entry) {\n","filename":"src\/hotspot\/share\/oops\/instanceKlass.cpp","additions":10,"deletions":9,"binary":false,"changes":19,"status":"modified"},{"patch":"@@ -40,1 +40,1 @@\n-\/\/ within the ConstantPoolCache and are accessed with indices added to the invokedynamic bytecode after\n+\/\/ within the ConstantPoolCache and are accessed with indices added to the bytecode after\n@@ -43,1 +43,1 @@\n-\/\/ Field bytecodes start with a constant pool index as their operate, which is then rewritten to\n+\/\/ Field bytecodes start with a constant pool index as their operand, which is then rewritten to\n","filename":"src\/hotspot\/share\/oops\/resolvedFieldEntry.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -511,1 +511,1 @@\n-      int index = iter.get_index_u2_cpcache();\n+      int index = iter.get_index_u2();\n","filename":"src\/hotspot\/share\/opto\/bytecodeInfo.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -471,1 +471,4 @@\n-          \"Trace decision for simplifying allocation merges.\")              \\\n+             \"Trace decision for simplifying allocation merges.\")           \\\n+                                                                            \\\n+  develop(bool, VerifyReduceAllocationMerges, true,                         \\\n+          \"Verify reduce allocation merges in escape analysis\")             \\\n","filename":"src\/hotspot\/share\/opto\/c2_globals.hpp","additions":4,"deletions":1,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -83,22 +83,0 @@\n-  \/\/ Visit all non-cast uses of the node, bypassing ConstraintCasts.\n-  \/\/ Pattern: this (-> ConstraintCast)* -> non_cast\n-  \/\/ In other words: find all non_cast nodes such that\n-  \/\/ non_cast->uncast() == this.\n-  template <typename Callback>\n-  static void visit_uncasted_uses(const Node* n, Callback callback) {\n-    ResourceMark rm;\n-    Unique_Node_List internals;\n-    internals.push((Node*)n); \/\/ start traversal\n-    for (uint j = 0; j < internals.size(); ++j) {\n-      Node* internal = internals.at(j); \/\/ for every internal\n-      for (DUIterator_Fast kmax, k = internal->fast_outs(kmax); k < kmax; k++) {\n-        Node* internal_use = internal->fast_out(k);\n-        if (internal_use->is_ConstraintCast()) {\n-          internals.push(internal_use); \/\/ traverse this cast also\n-        } else {\n-          callback(internal_use);\n-        }\n-      }\n-    }\n-  }\n-\n","filename":"src\/hotspot\/share\/opto\/castnode.hpp","additions":0,"deletions":22,"binary":false,"changes":22,"status":"modified"},{"patch":"@@ -471,6 +471,1 @@\n-macro(ReplicateB)\n-macro(ReplicateS)\n-macro(ReplicateI)\n-macro(ReplicateL)\n-macro(ReplicateF)\n-macro(ReplicateD)\n+macro(Replicate)\n","filename":"src\/hotspot\/share\/opto\/classes.hpp","additions":1,"deletions":6,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -828,2 +828,0 @@\n-    print_method(PHASE_BEFORE_REMOVEUSELESS, 3);\n-\n@@ -2486,1 +2484,0 @@\n-    PhaseGVN* gvn = initial_gvn();\n","filename":"src\/hotspot\/share\/opto\/compile.cpp","additions":0,"deletions":3,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -383,0 +383,12 @@\n+\n+  if (VerifyReduceAllocationMerges) {\n+    for (uint i = 0; i < reducible_merges.size(); i++ ) {\n+      Node* n = reducible_merges.at(i);\n+      if (!can_reduce_phi(n->as_Phi())) {\n+        TraceReduceAllocationMerges = true;\n+        n->dump(2);\n+        n->dump(-2);\n+        assert(can_reduce_phi(n->as_Phi()), \"Sanity: previous reducible Phi is no longer reducible before SUT.\");\n+      }\n+    }\n+  }\n@@ -544,0 +556,9 @@\n+#ifdef ASSERT\n+  if (VerifyReduceAllocationMerges && !can_reduce_phi(ophi)) {\n+    TraceReduceAllocationMerges = true;\n+    ophi->dump(2);\n+    ophi->dump(-2);\n+    assert(can_reduce_phi(ophi), \"Sanity: previous reducible Phi is no longer reducible inside reduce_phi_on_field_access.\");\n+  }\n+#endif\n+\n@@ -547,1 +568,0 @@\n-    uint num_edges = 1;\n@@ -551,1 +571,0 @@\n-      num_edges = previous_addp->in(AddPNode::Address) == previous_addp->in(AddPNode::Base) ? 2 : 1;\n@@ -561,0 +580,1 @@\n+          assert(data_phi->is_Phi(), \"Return of split_through_phi should be a Phi.\");\n@@ -566,13 +586,12 @@\n-          if (data_phi != nullptr && data_phi->is_Phi()) {\n-            for (uint i = 1; i < data_phi->req(); i++) {\n-              Node* new_load = data_phi->in(i);\n-              if (new_load->is_Load()) {\n-                Node* new_addp = new_load->in(MemNode::Address);\n-                Node* base = get_addp_base(new_addp);\n-\n-                \/\/ The base might not be something that we can create an unique\n-                \/\/ type for. If that's the case we are done with that input.\n-                PointsToNode* jobj_ptn = unique_java_object(base);\n-                if (jobj_ptn == nullptr || !jobj_ptn->scalar_replaceable()) {\n-                  continue;\n-                }\n+          for (uint i = 1; i < data_phi->req(); i++) {\n+            Node* new_load = data_phi->in(i);\n+            if (new_load->is_Load()) {\n+              Node* new_addp = new_load->in(MemNode::Address);\n+              Node* base = get_addp_base(new_addp);\n+\n+              \/\/ The base might not be something that we can create an unique\n+              \/\/ type for. If that's the case we are done with that input.\n+              PointsToNode* jobj_ptn = unique_java_object(base);\n+              if (jobj_ptn == nullptr || !jobj_ptn->scalar_replaceable()) {\n+                continue;\n+              }\n@@ -580,16 +599,15 @@\n-                \/\/ Push to alloc_worklist since the base has an unique_type\n-                alloc_worklist.append_if_missing(new_addp);\n-\n-                \/\/ Now let's add the node to the connection graph\n-                _nodes.at_grow(new_addp->_idx, nullptr);\n-                add_field(new_addp, fn->escape_state(), fn->offset());\n-                add_base(ptnode_adr(new_addp->_idx)->as_Field(), ptnode_adr(base->_idx));\n-\n-                \/\/ If the load doesn't load an object then it won't be\n-                \/\/ part of the connection graph\n-                PointsToNode* curr_load_ptn = ptnode_adr(previous_load->_idx);\n-                if (curr_load_ptn != nullptr) {\n-                  _nodes.at_grow(new_load->_idx, nullptr);\n-                  add_local_var(new_load, curr_load_ptn->escape_state());\n-                  add_edge(ptnode_adr(new_load->_idx), ptnode_adr(new_addp->_idx)->as_Field());\n-                }\n+              \/\/ Push to alloc_worklist since the base has an unique_type\n+              alloc_worklist.append_if_missing(new_addp);\n+\n+              \/\/ Now let's add the node to the connection graph\n+              _nodes.at_grow(new_addp->_idx, nullptr);\n+              add_field(new_addp, fn->escape_state(), fn->offset());\n+              add_base(ptnode_adr(new_addp->_idx)->as_Field(), ptnode_adr(base->_idx));\n+\n+              \/\/ If the load doesn't load an object then it won't be\n+              \/\/ part of the connection graph\n+              PointsToNode* curr_load_ptn = ptnode_adr(previous_load->_idx);\n+              if (curr_load_ptn != nullptr) {\n+                _nodes.at_grow(new_load->_idx, nullptr);\n+                add_local_var(new_load, curr_load_ptn->escape_state());\n+                add_edge(ptnode_adr(new_load->_idx), ptnode_adr(new_addp->_idx)->as_Field());\n@@ -600,2 +618,1 @@\n-        --k;\n-        k = MIN2(k, (int)previous_addp->outcnt()-1);\n+        k = MIN2(--k, (int)previous_addp->outcnt()-1);\n@@ -606,0 +623,14 @@\n+      _igvn->remove_globally_dead_node(previous_addp);\n+    }\n+    j = MIN2(--j, (int)ophi->outcnt()-1);\n+  }\n+\n+#ifdef ASSERT\n+  if (VerifyReduceAllocationMerges) {\n+    for (uint j = 0; j < ophi->outcnt(); j++) {\n+      Node* use = ophi->raw_out(j);\n+      if (!use->is_SafePoint()) {\n+        ophi->dump(2);\n+        ophi->dump(-2);\n+        assert(false, \"Should be a SafePoint.\");\n+      }\n@@ -607,2 +638,1 @@\n-    j -= num_edges;\n-    j = MIN2(j, (int)ophi->outcnt()-1);\n+#endif\n@@ -3684,0 +3714,1 @@\n+  DEBUG_ONLY(Unique_Node_List reduced_merges;)\n@@ -3860,0 +3891,5 @@\n+#ifdef ASSERT\n+        if (VerifyReduceAllocationMerges) {\n+          reduced_merges.push(n);\n+        }\n+#endif\n@@ -3982,8 +4018,18 @@\n-  \/\/ At this point reducible Phis shouldn't have AddP users anymore; only SafePoints.\n-  for (uint i = 0; i < reducible_merges.size(); i++) {\n-    Node* phi = reducible_merges.at(i);\n-    for (DUIterator_Fast jmax, j = phi->fast_outs(jmax); j < jmax; j++) {\n-      Node* use = phi->fast_out(j);\n-      if (!use->is_SafePoint()) {\n-        phi->dump(-3);\n-        assert(false, \"Unexpected user of reducible Phi -> %s\", use->Name());\n+  if (VerifyReduceAllocationMerges) {\n+    \/\/ At this point reducible Phis shouldn't have AddP users anymore; only SafePoints.\n+    for (uint i = 0; i < reducible_merges.size(); i++) {\n+      Node* phi = reducible_merges.at(i);\n+\n+      if (!reduced_merges.member(phi)) {\n+        phi->dump(2);\n+        phi->dump(-2);\n+        assert(false, \"This reducible merge wasn't reduced.\");\n+      }\n+\n+      for (DUIterator_Fast jmax, j = phi->fast_outs(jmax); j < jmax; j++) {\n+        Node* use = phi->fast_out(j);\n+        if (!use->is_SafePoint()) {\n+          phi->dump(2);\n+          phi->dump(-2);\n+          assert(false, \"Unexpected user of reducible Phi -> %d:%s:%d\", use->_idx, use->Name(), use->outcnt());\n+        }\n","filename":"src\/hotspot\/share\/opto\/escape.cpp","additions":89,"deletions":43,"binary":false,"changes":132,"status":"modified"},{"patch":"@@ -1743,0 +1743,2 @@\n+\n+  bool can_move_to_inner_loop(Node* n, LoopNode* n_loop, Node* x);\n","filename":"src\/hotspot\/share\/opto\/loopnode.hpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -162,2 +162,1 @@\n-    if (x != the_clone && the_clone != nullptr)\n-      _igvn.remove_dead_node(the_clone);\n+\n@@ -165,0 +164,14 @@\n+\n+    if (the_clone == nullptr) {\n+      continue;\n+    }\n+\n+    if (the_clone != x) {\n+      _igvn.remove_dead_node(the_clone);\n+    } else if (region->is_Loop() && i == LoopNode::LoopBackControl &&\n+               n->is_Load() && can_move_to_inner_loop(n, region->as_Loop(), x)) {\n+      \/\/ it is not a win if 'x' moved from an outer to an inner loop\n+      \/\/ this edge case can only happen for Load nodes\n+      wins = 0;\n+      break;\n+    }\n@@ -228,0 +241,10 @@\n+\/\/ Test whether node 'x' can move into an inner loop relative to node 'n'.\n+\/\/ Note: The test is not exact. Returns true if 'x' COULD end up in an inner loop,\n+\/\/ BUT it can also return true and 'x' is in the outer loop\n+bool PhaseIdealLoop::can_move_to_inner_loop(Node* n, LoopNode* n_loop, Node* x) {\n+  IdealLoopTree* n_loop_tree = get_loop(n_loop);\n+  IdealLoopTree* x_loop_tree = get_loop(get_early_ctrl(x));\n+  \/\/ x_loop_tree should be outer or same loop as n_loop_tree\n+  return !x_loop_tree->is_member(n_loop_tree);\n+}\n+\n","filename":"src\/hotspot\/share\/opto\/loopopts.cpp","additions":25,"deletions":2,"binary":false,"changes":27,"status":"modified"},{"patch":"@@ -659,1 +659,1 @@\n-          if (!n->is_Store() && n->Opcode() != Op_CastP2X && !bs->is_gc_pre_barrier_node(n)) {\n+          if (!n->is_Store() && n->Opcode() != Op_CastP2X && !bs->is_gc_pre_barrier_node(n) && !reduce_merge_precheck) {\n@@ -761,0 +761,5 @@\n+\n+  if (TraceReduceAllocationMerges && !can_eliminate && reduce_merge_precheck) {\n+    tty->print_cr(\"\\tCan't eliminate allocation because '%s': \", fail_eliminate != nullptr ? fail_eliminate : \"\");\n+    DEBUG_ONLY(if (disq_node != nullptr) disq_node->dump();)\n+  }\n","filename":"src\/hotspot\/share\/opto\/macro.cpp","additions":6,"deletions":1,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -2846,0 +2846,6 @@\n+bool Matcher::is_non_long_integral_vector(const Node* n) {\n+  BasicType bt = vector_element_basic_type(n);\n+  assert(bt != T_CHAR, \"char is not allowed in vector\");\n+  return is_subword_type(bt) || bt == T_INT;\n+}\n+\n","filename":"src\/hotspot\/share\/opto\/matcher.cpp","additions":6,"deletions":0,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -382,0 +382,3 @@\n+  \/\/ Vector element basic type is non double word integral type.\n+  static bool is_non_long_integral_vector(const Node* n);\n+\n","filename":"src\/hotspot\/share\/opto\/matcher.hpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -1145,0 +1145,7 @@\n+  \/\/ Visit boundary uses of the node and apply a callback function for each.\n+  \/\/ Recursively traverse uses, stopping and applying the callback when\n+  \/\/ reaching a boundary node, defined by is_boundary. Note: the function\n+  \/\/ definition appears after the complete type definition of Node_List.\n+  template <typename Callback, typename Check>\n+  void visit_uses(Callback callback, Check is_boundary) const;\n+\n@@ -1645,0 +1652,29 @@\n+\/\/ Definition must appear after complete type definition of Node_List\n+template <typename Callback, typename Check>\n+void Node::visit_uses(Callback callback, Check is_boundary) const {\n+  ResourceMark rm;\n+  VectorSet visited;\n+  Node_List worklist;\n+\n+  \/\/ The initial worklist consists of the direct uses\n+  for (DUIterator_Fast kmax, k = fast_outs(kmax); k < kmax; k++) {\n+    Node* out = fast_out(k);\n+    if (!visited.test_set(out->_idx)) { worklist.push(out); }\n+  }\n+\n+  while (worklist.size() > 0) {\n+    Node* use = worklist.pop();\n+    \/\/ Apply callback on boundary nodes\n+    if (is_boundary(use)) {\n+      callback(use);\n+    } else {\n+      \/\/ Not a boundary node, continue search\n+      for (DUIterator_Fast kmax, k = use->fast_outs(kmax); k < kmax; k++) {\n+        Node* out = use->fast_out(k);\n+        if (!visited.test_set(out->_idx)) { worklist.push(out); }\n+      }\n+    }\n+  }\n+}\n+\n+\n","filename":"src\/hotspot\/share\/opto\/node.hpp","additions":36,"deletions":0,"binary":false,"changes":36,"status":"modified"},{"patch":"@@ -367,0 +367,1 @@\n+  C->print_method(PHASE_BEFORE_REMOVEUSELESS, 3);\n@@ -1630,1 +1631,3 @@\n-      ConstraintCastNode::visit_uncasted_uses(use, push_the_uses_to_worklist);\n+      auto is_boundary = [](Node* n){ return !n->is_ConstraintCast(); };\n+      use->visit_uses(push_the_uses_to_worklist, is_boundary);\n+\n@@ -1875,1 +1878,1 @@\n-  assert(!failure, \"Missed optimization opportunity in PhaseCCP\");\n+  assert(!failure, \"PhaseCCP not at fixpoint: analysis result may be unsound.\");\n@@ -2030,1 +2033,1 @@\n-\/\/ Pattern: parent -> LShift (use) -> ConstraintCast* -> And\n+\/\/ Pattern: parent -> LShift (use) -> (ConstraintCast | ConvI2L)* -> And\n@@ -2041,1 +2044,4 @@\n-    ConstraintCastNode::visit_uncasted_uses(use, push_and_uses_to_worklist);\n+    auto is_boundary = [](Node* n) {\n+      return !(n->is_ConstraintCast() || n->Opcode() == Op_ConvI2L);\n+    };\n+    use->visit_uses(push_and_uses_to_worklist, is_boundary);\n","filename":"src\/hotspot\/share\/opto\/phaseX.cpp","additions":10,"deletions":4,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -156,0 +156,10 @@\n+static bool is_cloop_condition(BoolNode* bol) {\n+  for (DUIterator_Fast imax, i = bol->fast_outs(imax); i < imax; i++) {\n+    Node* out = bol->fast_out(i);\n+    if (out->is_CountedLoopEnd()) {\n+      return true;\n+    }\n+  }\n+  return false;\n+}\n+\n@@ -1659,2 +1669,2 @@\n-      !is_cloop_increment(cmp1) &&\n-      phase->type(cmp1->in(2)) == TypeInt::MIN) {\n+      phase->type(cmp1->in(2)) == TypeInt::MIN &&\n+      !is_cloop_condition(this)) {\n@@ -1666,2 +1676,2 @@\n-               !is_cloop_increment(cmp2) &&\n-               phase->type(cmp2->in(2)) == TypeInt::MIN) {\n+               phase->type(cmp2->in(2)) == TypeInt::MIN &&\n+               !is_cloop_condition(this)) {\n@@ -1677,2 +1687,2 @@\n-      !is_cloop_increment(cmp1) &&\n-      phase->type(cmp1->in(2)) == TypeLong::MIN) {\n+      phase->type(cmp1->in(2)) == TypeLong::MIN &&\n+      !is_cloop_condition(this)) {\n@@ -1684,2 +1694,2 @@\n-               !is_cloop_increment(cmp2) &&\n-               phase->type(cmp2->in(2)) == TypeLong::MIN) {\n+               phase->type(cmp2->in(2)) == TypeLong::MIN &&\n+               !is_cloop_condition(this)) {\n","filename":"src\/hotspot\/share\/opto\/subnode.cpp","additions":18,"deletions":8,"binary":false,"changes":26,"status":"modified"},{"patch":"@@ -113,0 +113,1 @@\n+  { Bad,             T_ARRAY,      \"interfaces:\",   false, Node::NotAMachineReg, relocInfo::none          },  \/\/ Interfaces\n@@ -172,2 +173,2 @@\n-const TypePtr::InterfaceSet* TypeAryPtr::_array_interfaces = nullptr;\n-const TypePtr::InterfaceSet* TypeAryKlassPtr::_array_interfaces = nullptr;\n+const TypeInterfaces* TypeAryPtr::_array_interfaces = nullptr;\n+const TypeInterfaces* TypeAryKlassPtr::_array_interfaces = nullptr;\n@@ -626,1 +627,1 @@\n-  TypeAryPtr::_array_interfaces = new TypePtr::InterfaceSet(&array_interfaces);\n+  TypeAryPtr::_array_interfaces = TypeInterfaces::make(&array_interfaces);\n@@ -3366,2 +3367,2 @@\n-TypePtr::InterfaceSet::InterfaceSet()\n-        : _list(Compile::current()->type_arena(), 0, 0, nullptr),\n+TypeInterfaces::TypeInterfaces()\n+        : Type(Interfaces), _list(Compile::current()->type_arena(), 0, 0, nullptr),\n@@ -3372,2 +3373,2 @@\n-TypePtr::InterfaceSet::InterfaceSet(GrowableArray<ciInstanceKlass*>* interfaces)\n-        : _list(Compile::current()->type_arena(), interfaces->length(), 0, nullptr),\n+TypeInterfaces::TypeInterfaces(GrowableArray<ciInstanceKlass*>* interfaces)\n+        : Type(Interfaces), _list(Compile::current()->type_arena(), interfaces->length(), 0, nullptr),\n@@ -3381,1 +3382,6 @@\n-void TypePtr::InterfaceSet::initialize() {\n+const TypeInterfaces* TypeInterfaces::make(GrowableArray<ciInstanceKlass*>* interfaces) {\n+  TypeInterfaces* result = (interfaces == nullptr) ? new TypeInterfaces() : new TypeInterfaces(interfaces);\n+  return (const TypeInterfaces*)result->hashcons();\n+}\n+\n+void TypeInterfaces::initialize() {\n@@ -3387,1 +3393,1 @@\n-int TypePtr::InterfaceSet::compare(ciKlass* const& k1, ciKlass* const& k2) {\n+int TypeInterfaces::compare(ciInstanceKlass* const& k1, ciInstanceKlass* const& k2) {\n@@ -3396,1 +3402,1 @@\n-void TypePtr::InterfaceSet::add(ciKlass* interface) {\n+void TypeInterfaces::add(ciInstanceKlass* interface) {\n@@ -3402,7 +3408,3 @@\n-void TypePtr::InterfaceSet::raw_add(ciKlass* interface) {\n-  assert(interface->is_interface(), \"for interfaces only\");\n-  _list.push(interface);\n-}\n-\n-bool TypePtr::InterfaceSet::eq(const InterfaceSet& other) const {\n-  if (_list.length() != other._list.length()) {\n+bool TypeInterfaces::eq(const Type* t) const {\n+  const TypeInterfaces* other = (const TypeInterfaces*)t;\n+  if (_list.length() != other->_list.length()) {\n@@ -3413,1 +3415,1 @@\n-    ciKlass* k2 = other._list.at(i);\n+    ciKlass* k2 = other->_list.at(i);\n@@ -3421,1 +3423,1 @@\n-bool TypePtr::InterfaceSet::eq(ciInstanceKlass* k) const {\n+bool TypeInterfaces::eq(ciInstanceKlass* k) const {\n@@ -3423,1 +3425,1 @@\n-  GrowableArray<ciInstanceKlass *>* interfaces = k->as_instance_klass()->transitive_interfaces();\n+  GrowableArray<ciInstanceKlass *>* interfaces = k->transitive_interfaces();\n@@ -3429,1 +3431,1 @@\n-    _list.find_sorted<ciKlass*, compare>(interfaces->at(i), found);\n+    _list.find_sorted<ciInstanceKlass*, compare>(interfaces->at(i), found);\n@@ -3438,1 +3440,1 @@\n-uint TypePtr::InterfaceSet::hash() const {\n+uint TypeInterfaces::hash() const {\n@@ -3443,1 +3445,5 @@\n-void TypePtr::InterfaceSet::compute_hash() {\n+const Type* TypeInterfaces::xdual() const {\n+  return this;\n+}\n+\n+void TypeInterfaces::compute_hash() {\n@@ -3452,1 +3458,1 @@\n-static int compare_interfaces(ciKlass** k1, ciKlass** k2) {\n+static int compare_interfaces(ciInstanceKlass** k1, ciInstanceKlass** k2) {\n@@ -3456,1 +3462,1 @@\n-void TypePtr::InterfaceSet::dump(outputStream* st) const {\n+void TypeInterfaces::dump(outputStream* st) const {\n@@ -3462,1 +3468,1 @@\n-  GrowableArray<ciKlass*> interfaces;\n+  GrowableArray<ciInstanceKlass*> interfaces;\n@@ -3477,1 +3483,1 @@\n-void TypePtr::InterfaceSet::verify() const {\n+void TypeInterfaces::verify() const {\n@@ -3479,2 +3485,2 @@\n-    ciKlass* k1 = _list.at(i-1);\n-    ciKlass* k2 = _list.at(i);\n+    ciInstanceKlass* k1 = _list.at(i-1);\n+    ciInstanceKlass* k2 = _list.at(i);\n@@ -3487,2 +3493,2 @@\n-TypePtr::InterfaceSet TypeOopPtr::InterfaceSet::union_with(const InterfaceSet& other) const {\n-  InterfaceSet result;\n+const TypeInterfaces* TypeInterfaces::union_with(const TypeInterfaces* other) const {\n+  GrowableArray<ciInstanceKlass*> result_list;\n@@ -3491,1 +3497,1 @@\n-  while (i < _list.length() || j < other._list.length()) {\n+  while (i < _list.length() || j < other->_list.length()) {\n@@ -3493,3 +3499,3 @@\n-           (j >= other._list.length() ||\n-            compare(_list.at(i), other._list.at(j)) < 0)) {\n-      result.raw_add(_list.at(i));\n+           (j >= other->_list.length() ||\n+            compare(_list.at(i), other->_list.at(j)) < 0)) {\n+      result_list.push(_list.at(i));\n@@ -3498,1 +3504,1 @@\n-    while (j < other._list.length() &&\n+    while (j < other->_list.length() &&\n@@ -3500,2 +3506,2 @@\n-            compare(other._list.at(j), _list.at(i)) < 0)) {\n-      result.raw_add(other._list.at(j));\n+            compare(other->_list.at(j), _list.at(i)) < 0)) {\n+      result_list.push(other->_list.at(j));\n@@ -3505,3 +3511,3 @@\n-        j < other._list.length() &&\n-        _list.at(i) == other._list.at(j)) {\n-      result.raw_add(_list.at(i));\n+        j < other->_list.length() &&\n+        _list.at(i) == other->_list.at(j)) {\n+      result_list.push(_list.at(i));\n@@ -3512,1 +3518,1 @@\n-  result.initialize();\n+  const TypeInterfaces* result = TypeInterfaces::make(&result_list);\n@@ -3514,1 +3520,1 @@\n-  result.verify();\n+  result->verify();\n@@ -3516,1 +3522,1 @@\n-    assert(result._list.contains(_list.at(i)), \"missing\");\n+    assert(result->_list.contains(_list.at(i)), \"missing\");\n@@ -3518,2 +3524,2 @@\n-  for (int i = 0; i < other._list.length(); i++) {\n-    assert(result._list.contains(other._list.at(i)), \"missing\");\n+  for (int i = 0; i < other->_list.length(); i++) {\n+    assert(result->_list.contains(other->_list.at(i)), \"missing\");\n@@ -3521,2 +3527,2 @@\n-  for (int i = 0; i < result._list.length(); i++) {\n-    assert(_list.contains(result._list.at(i)) || other._list.contains(result._list.at(i)), \"missing\");\n+  for (int i = 0; i < result->_list.length(); i++) {\n+    assert(_list.contains(result->_list.at(i)) || other->_list.contains(result->_list.at(i)), \"missing\");\n@@ -3528,2 +3534,2 @@\n-TypePtr::InterfaceSet TypeOopPtr::InterfaceSet::intersection_with(const InterfaceSet& other) const {\n-  InterfaceSet result;\n+const TypeInterfaces* TypeInterfaces::intersection_with(const TypeInterfaces* other) const {\n+  GrowableArray<ciInstanceKlass*> result_list;\n@@ -3532,1 +3538,1 @@\n-  while (i < _list.length() || j < other._list.length()) {\n+  while (i < _list.length() || j < other->_list.length()) {\n@@ -3534,2 +3540,2 @@\n-           (j >= other._list.length() ||\n-            compare(_list.at(i), other._list.at(j)) < 0)) {\n+           (j >= other->_list.length() ||\n+            compare(_list.at(i), other->_list.at(j)) < 0)) {\n@@ -3538,1 +3544,1 @@\n-    while (j < other._list.length() &&\n+    while (j < other->_list.length() &&\n@@ -3540,1 +3546,1 @@\n-            compare(other._list.at(j), _list.at(i)) < 0)) {\n+            compare(other->_list.at(j), _list.at(i)) < 0)) {\n@@ -3544,3 +3550,3 @@\n-        j < other._list.length() &&\n-        _list.at(i) == other._list.at(j)) {\n-      result.raw_add(_list.at(i));\n+        j < other->_list.length() &&\n+        _list.at(i) == other->_list.at(j)) {\n+      result_list.push(_list.at(i));\n@@ -3551,1 +3557,1 @@\n-  result.initialize();\n+  const TypeInterfaces* result = TypeInterfaces::make(&result_list);\n@@ -3553,1 +3559,1 @@\n-  result.verify();\n+  result->verify();\n@@ -3555,1 +3561,1 @@\n-    assert(!other._list.contains(_list.at(i)) || result._list.contains(_list.at(i)), \"missing\");\n+    assert(!other->_list.contains(_list.at(i)) || result->_list.contains(_list.at(i)), \"missing\");\n@@ -3557,2 +3563,2 @@\n-  for (int i = 0; i < other._list.length(); i++) {\n-    assert(!_list.contains(other._list.at(i)) || result._list.contains(other._list.at(i)), \"missing\");\n+  for (int i = 0; i < other->_list.length(); i++) {\n+    assert(!_list.contains(other->_list.at(i)) || result->_list.contains(other->_list.at(i)), \"missing\");\n@@ -3560,2 +3566,2 @@\n-  for (int i = 0; i < result._list.length(); i++) {\n-    assert(_list.contains(result._list.at(i)) && other._list.contains(result._list.at(i)), \"missing\");\n+  for (int i = 0; i < result->_list.length(); i++) {\n+    assert(_list.contains(result->_list.at(i)) && other->_list.contains(result->_list.at(i)), \"missing\");\n@@ -3568,1 +3574,1 @@\n-ciKlass* TypePtr::InterfaceSet::exact_klass() const {\n+ciInstanceKlass* TypeInterfaces::exact_klass() const {\n@@ -3573,1 +3579,1 @@\n-void TypePtr::InterfaceSet::compute_exact_klass() {\n+void TypeInterfaces::compute_exact_klass() {\n@@ -3578,1 +3584,1 @@\n-  ciKlass* res = nullptr;\n+  ciInstanceKlass* res = nullptr;\n@@ -3580,1 +3586,1 @@\n-    ciInstanceKlass* interface = _list.at(i)->as_instance_klass();\n+    ciInstanceKlass* interface = _list.at(i);\n@@ -3590,1 +3596,1 @@\n-void TypePtr::InterfaceSet::verify_is_loaded() const {\n+void TypeInterfaces::verify_is_loaded() const {\n@@ -3598,0 +3604,11 @@\n+\/\/ Can't be implemented because there's no way to know if the type is above or below the center line.\n+const Type* TypeInterfaces::xmeet(const Type* t) const {\n+  ShouldNotReachHere();\n+  return Type::xmeet(t);\n+}\n+\n+bool TypeInterfaces::singleton(void) const {\n+  ShouldNotReachHere();\n+  return Type::singleton();\n+}\n+\n@@ -3599,1 +3616,1 @@\n-TypeOopPtr::TypeOopPtr(TYPES t, PTR ptr, ciKlass* k, const InterfaceSet& interfaces, bool xk, ciObject* o, Offset offset, Offset field_offset,\n+TypeOopPtr::TypeOopPtr(TYPES t, PTR ptr, ciKlass* k, const TypeInterfaces* interfaces, bool xk, ciObject* o, Offset offset, Offset field_offset,\n@@ -3611,1 +3628,1 @@\n-    interfaces.verify_is_loaded();\n+    interfaces->verify_is_loaded();\n@@ -3702,1 +3719,2 @@\n-  return (TypeOopPtr*)(new TypeOopPtr(OopPtr, ptr, k, InterfaceSet(), xk, o, offset, Offset::bottom, instance_id, speculative, inline_depth))->hashcons();\n+  const TypeInterfaces* interfaces = TypeInterfaces::make();\n+  return (TypeOopPtr*)(new TypeOopPtr(OopPtr, ptr, k, interfaces, xk, o, offset, Offset::bottom, instance_id, speculative, inline_depth))->hashcons();\n@@ -3846,1 +3864,1 @@\n-    const TypePtr::InterfaceSet interfaces = TypePtr::interfaces(klass, true, true, false, interface_handling);\n+    const TypeInterfaces* interfaces = TypePtr::interfaces(klass, true, true, false, interface_handling);\n@@ -4111,1 +4129,1 @@\n-TypePtr::InterfaceSet TypeOopPtr::meet_interfaces(const TypeOopPtr* other) const {\n+const TypeInterfaces* TypeOopPtr::meet_interfaces(const TypeOopPtr* other) const {\n@@ -4113,1 +4131,1 @@\n-    return _interfaces.union_with(other->_interfaces);\n+    return _interfaces->union_with(other->_interfaces);\n@@ -4119,1 +4137,1 @@\n-  return _interfaces.intersection_with(other->_interfaces);\n+  return _interfaces->intersection_with(other->_interfaces);\n@@ -4148,1 +4166,1 @@\n-  if (_interfaces.empty()) {\n+  if (_interfaces->empty()) {\n@@ -4152,1 +4170,1 @@\n-    if (_interfaces.eq(_klass->as_instance_klass())) {\n+    if (_interfaces->eq(_klass->as_instance_klass())) {\n@@ -4157,1 +4175,1 @@\n-  return _interfaces.exact_klass();\n+  return _interfaces->exact_klass();\n@@ -4161,1 +4179,1 @@\n-TypeInstPtr::TypeInstPtr(PTR ptr, ciKlass* k, const InterfaceSet& interfaces, bool xk, ciObject* o, Offset off,\n+TypeInstPtr::TypeInstPtr(PTR ptr, ciKlass* k, const TypeInterfaces* interfaces, bool xk, ciObject* o, Offset off,\n@@ -4176,1 +4194,1 @@\n-                                     const InterfaceSet& interfaces,\n+                                     const TypeInterfaces* interfaces,\n@@ -4212,1 +4230,1 @@\n-TypePtr::InterfaceSet TypePtr::interfaces(ciKlass*& k, bool klass, bool interface, bool array, InterfaceHandling interface_handling) {\n+const TypeInterfaces* TypePtr::interfaces(ciKlass*& k, bool klass, bool interface, bool array, InterfaceHandling interface_handling) {\n@@ -4218,1 +4236,1 @@\n-        InterfaceSet interfaces;\n+        const TypeInterfaces* interfaces = TypeInterfaces::make();\n@@ -4222,1 +4240,1 @@\n-      InterfaceSet interfaces(k_interfaces);\n+      const TypeInterfaces* interfaces = TypeInterfaces::make(k_interfaces);\n@@ -4231,1 +4249,1 @@\n-    InterfaceSet interfaces;\n+    const TypeInterfaces* interfaces = TypeInterfaces::make();\n@@ -4242,1 +4260,1 @@\n-  return *TypeAryPtr::_array_interfaces;\n+  return TypeAryPtr::_array_interfaces;\n@@ -4296,1 +4314,1 @@\n-const TypeInstPtr *TypeInstPtr::xmeet_unloaded(const TypeInstPtr *tinst, const InterfaceSet& interfaces) const {\n+const TypeInstPtr *TypeInstPtr::xmeet_unloaded(const TypeInstPtr *tinst, const TypeInterfaces* interfaces) const {\n@@ -4453,1 +4471,1 @@\n-    InterfaceSet interfaces = meet_interfaces(tinst);\n+    const TypeInterfaces* interfaces = meet_interfaces(tinst);\n@@ -4514,1 +4532,1 @@\n-template<class T> TypePtr::MeetResult TypePtr::meet_instptr(PTR& ptr, InterfaceSet& interfaces, const T* this_type, const T* other_type,\n+template<class T> TypePtr::MeetResult TypePtr::meet_instptr(PTR& ptr, const TypeInterfaces*& interfaces, const T* this_type, const T* other_type,\n@@ -4526,2 +4544,2 @@\n-  InterfaceSet this_interfaces = this_type->interfaces();\n-  InterfaceSet other_interfaces = other_type->interfaces();\n+  const TypeInterfaces* this_interfaces = this_type->interfaces();\n+  const TypeInterfaces* other_interfaces = other_type->interfaces();\n@@ -4622,1 +4640,1 @@\n-  interfaces = this_interfaces.intersection_with(other_interfaces);\n+  interfaces = this_interfaces->intersection_with(other_interfaces);\n@@ -4659,1 +4677,1 @@\n-    _interfaces.eq(p->_interfaces) &&\n+    _interfaces->eq(p->_interfaces) &&\n@@ -4666,1 +4684,1 @@\n-  return klass()->hash() + TypeOopPtr::hash() + _interfaces.hash() + (uint)flat_in_array();\n+  return klass()->hash() + TypeOopPtr::hash() + _interfaces->hash() + (uint)flat_in_array();\n@@ -4689,1 +4707,1 @@\n-  _interfaces.dump(st);\n+  _interfaces->dump(st);\n@@ -4778,1 +4796,1 @@\n-    if (_interfaces.eq(ik)) {\n+    if (_interfaces->eq(ik)) {\n@@ -4795,1 +4813,1 @@\n-  if (other->klass() == ciEnv::current()->Object_klass() && other->_interfaces.empty()) {\n+  if (other->klass() == ciEnv::current()->Object_klass() && other->_interfaces->empty()) {\n@@ -4800,1 +4818,1 @@\n-         (!this_xk || this_one->_interfaces.contains(other->_interfaces));\n+         (!this_xk || this_one->_interfaces->contains(other->_interfaces));\n@@ -4810,1 +4828,1 @@\n-  if (other->klass() == ciEnv::current()->Object_klass() && other->_interfaces.empty()) {\n+  if (other->klass() == ciEnv::current()->Object_klass() && other->_interfaces->empty()) {\n@@ -4815,1 +4833,1 @@\n-    return other->klass() == ciEnv::current()->Object_klass() && this_one->_interfaces.contains(other->_interfaces);\n+    return other->klass() == ciEnv::current()->Object_klass() && this_one->_interfaces->contains(other->_interfaces);\n@@ -5243,3 +5261,3 @@\n-    InterfaceSet interfaces = meet_interfaces(tp);\n-    InterfaceSet tp_interfaces = tp->_interfaces;\n-    InterfaceSet this_interfaces = _interfaces;\n+    const TypeInterfaces* interfaces = meet_interfaces(tp);\n+    const TypeInterfaces* tp_interfaces = tp->_interfaces;\n+    const TypeInterfaces* this_interfaces = _interfaces;\n@@ -5253,1 +5271,1 @@\n-      if (tp->klass()->equals(ciEnv::current()->Object_klass()) && this_interfaces.contains(tp_interfaces) && !tp->klass_is_exact() && !tp->flat_in_array()) {\n+      if (tp->klass()->equals(ciEnv::current()->Object_klass()) && this_interfaces->contains(tp_interfaces) && !tp->klass_is_exact() && !tp->flat_in_array()) {\n@@ -5259,1 +5277,1 @@\n-        interfaces = this_interfaces.intersection_with(tp_interfaces);\n+        interfaces = this_interfaces->intersection_with(tp_interfaces);\n@@ -5272,1 +5290,1 @@\n-        if (tp->klass()->equals(ciEnv::current()->Object_klass()) && this_interfaces.contains(tp_interfaces) && !tp->klass_is_exact() && !tp->flat_in_array()) {\n+        if (tp->klass()->equals(ciEnv::current()->Object_klass()) && this_interfaces->contains(tp_interfaces) && !tp->klass_is_exact() && !tp->flat_in_array()) {\n@@ -5286,1 +5304,1 @@\n-      interfaces = this_interfaces.intersection_with(tp_interfaces);\n+      interfaces = this_interfaces->intersection_with(tp_interfaces);\n@@ -5433,1 +5451,1 @@\n-  _interfaces.dump(st);\n+  _interfaces->dump(st);\n@@ -5954,1 +5972,1 @@\n-    const InterfaceSet interfaces = TypePtr::interfaces(klass, true, true, false, interface_handling);\n+    const TypeInterfaces* interfaces = TypePtr::interfaces(klass, true, true, false, interface_handling);\n@@ -5960,1 +5978,1 @@\n-TypeKlassPtr::TypeKlassPtr(TYPES t, PTR ptr, ciKlass* klass, const InterfaceSet& interfaces, Offset offset)\n+TypeKlassPtr::TypeKlassPtr(TYPES t, PTR ptr, ciKlass* klass, const TypeInterfaces* interfaces, Offset offset)\n@@ -5969,1 +5987,1 @@\n-  if (_interfaces.empty()) {\n+  if (_interfaces->empty()) {\n@@ -5973,1 +5991,1 @@\n-    if (_interfaces.eq(_klass->as_instance_klass())) {\n+    if (_interfaces->eq(_klass->as_instance_klass())) {\n@@ -5978,1 +5996,1 @@\n-  return _interfaces.exact_klass();\n+  return _interfaces->exact_klass();\n@@ -5986,1 +6004,1 @@\n-    _interfaces.eq(p->_interfaces) &&\n+    _interfaces->eq(p->_interfaces) &&\n@@ -5993,1 +6011,1 @@\n-  return TypePtr::hash() + _interfaces.hash();\n+  return TypePtr::hash() + _interfaces->hash();\n@@ -6020,1 +6038,1 @@\n-TypePtr::InterfaceSet TypeKlassPtr::meet_interfaces(const TypeKlassPtr* other) const {\n+const TypeInterfaces* TypeKlassPtr::meet_interfaces(const TypeKlassPtr* other) const {\n@@ -6022,1 +6040,1 @@\n-    return _interfaces.union_with(other->_interfaces);\n+    return _interfaces->union_with(other->_interfaces);\n@@ -6028,1 +6046,1 @@\n-  return _interfaces.intersection_with(other->_interfaces);\n+  return _interfaces->intersection_with(other->_interfaces);\n@@ -6068,1 +6086,1 @@\n-      _interfaces.dump(st);\n+      _interfaces->dump(st);\n@@ -6107,1 +6125,1 @@\n-const TypeInstKlassPtr *TypeInstKlassPtr::make(PTR ptr, ciKlass* k, const InterfaceSet& interfaces, Offset offset, bool flat_in_array) {\n+const TypeInstKlassPtr *TypeInstKlassPtr::make(PTR ptr, ciKlass* k, const TypeInterfaces* interfaces, Offset offset, bool flat_in_array) {\n@@ -6161,1 +6179,1 @@\n-  TypePtr::InterfaceSet interfaces = _interfaces;\n+  const TypeInterfaces* interfaces = _interfaces;\n@@ -6170,1 +6188,1 @@\n-        if (_interfaces.eq(sub)) {\n+        if (_interfaces->eq(sub)) {\n@@ -6254,1 +6272,1 @@\n-    InterfaceSet interfaces = meet_interfaces(tkls);\n+    const TypeInterfaces* interfaces = meet_interfaces(tkls);\n@@ -6278,3 +6296,3 @@\n-    InterfaceSet interfaces = meet_interfaces(tp);\n-    InterfaceSet tp_interfaces = tp->_interfaces;\n-    InterfaceSet this_interfaces = _interfaces;\n+    const TypeInterfaces* interfaces = meet_interfaces(tp);\n+    const TypeInterfaces* tp_interfaces = tp->_interfaces;\n+    const TypeInterfaces* this_interfaces = _interfaces;\n@@ -6288,1 +6306,1 @@\n-      if (klass()->equals(ciEnv::current()->Object_klass()) && tp_interfaces.contains(this_interfaces) && !klass_is_exact()) {\n+      if (klass()->equals(ciEnv::current()->Object_klass()) && tp_interfaces->contains(this_interfaces) && !klass_is_exact()) {\n@@ -6293,1 +6311,1 @@\n-        interfaces = _interfaces.intersection_with(tp->_interfaces);\n+        interfaces = _interfaces->intersection_with(tp->_interfaces);\n@@ -6306,1 +6324,1 @@\n-        if (klass()->equals(ciEnv::current()->Object_klass()) && tp_interfaces.contains(this_interfaces) && !klass_is_exact()) {\n+        if (klass()->equals(ciEnv::current()->Object_klass()) && tp_interfaces->contains(this_interfaces) && !klass_is_exact()) {\n@@ -6315,1 +6333,1 @@\n-      interfaces = this_interfaces.intersection_with(tp_interfaces);\n+      interfaces = this_interfaces->intersection_with(tp_interfaces);\n@@ -6344,1 +6362,1 @@\n-  if (other->klass()->equals(ciEnv::current()->Object_klass()) && other->_interfaces.empty()) {\n+  if (other->klass()->equals(ciEnv::current()->Object_klass()) && other->_interfaces->empty()) {\n@@ -6348,1 +6366,1 @@\n-  return this_one->_klass->is_subtype_of(other->_klass) && this_one->_interfaces.contains(other->_interfaces);\n+  return this_one->_klass->is_subtype_of(other->_klass) && this_one->_interfaces->contains(other->_interfaces);\n@@ -6363,1 +6381,1 @@\n-  return this_one->_klass->equals(other->_klass) && this_one->_interfaces.eq(other->_interfaces);\n+  return this_one->_klass->equals(other->_klass) && this_one->_interfaces->eq(other->_interfaces);\n@@ -6377,1 +6395,1 @@\n-    return !this_exact && this_one->_klass->equals(ciEnv::current()->Object_klass())  && other->_interfaces.contains(this_one->_interfaces);\n+    return !this_exact && this_one->_klass->equals(ciEnv::current()->Object_klass())  && other->_interfaces->contains(this_one->_interfaces);\n@@ -6391,1 +6409,1 @@\n-    return this_one->_klass->is_subtype_of(other->_klass) && this_one->_interfaces.contains(other->_interfaces);\n+    return this_one->_klass->is_subtype_of(other->_klass) && this_one->_interfaces->contains(other->_interfaces);\n@@ -6409,1 +6427,1 @@\n-  TypePtr::InterfaceSet interfaces = _interfaces;\n+  const TypeInterfaces* interfaces = _interfaces;\n@@ -6417,1 +6435,1 @@\n-        if (_interfaces.eq(sub)) {\n+        if (_interfaces->eq(sub)) {\n@@ -6795,3 +6813,3 @@\n-    InterfaceSet interfaces = meet_interfaces(tp);\n-    InterfaceSet tp_interfaces = tp->_interfaces;\n-    InterfaceSet this_interfaces = _interfaces;\n+    const TypeInterfaces* interfaces = meet_interfaces(tp);\n+    const TypeInterfaces* tp_interfaces = tp->_interfaces;\n+    const TypeInterfaces* this_interfaces = _interfaces;\n@@ -6805,1 +6823,1 @@\n-      if (tp->klass()->equals(ciEnv::current()->Object_klass()) && this_interfaces.intersection_with(tp_interfaces).eq(tp_interfaces) && !tp->klass_is_exact()) {\n+      if (tp->klass()->equals(ciEnv::current()->Object_klass()) && this_interfaces->intersection_with(tp_interfaces)->eq(tp_interfaces) && !tp->klass_is_exact()) {\n@@ -6810,1 +6828,1 @@\n-        interfaces = this_interfaces.intersection_with(tp->_interfaces);\n+        interfaces = this_interfaces->intersection_with(tp->_interfaces);\n@@ -6823,1 +6841,1 @@\n-        if (tp->klass()->equals(ciEnv::current()->Object_klass()) && this_interfaces.intersection_with(tp_interfaces).eq(tp_interfaces) && !tp->klass_is_exact()) {\n+        if (tp->klass()->equals(ciEnv::current()->Object_klass()) && this_interfaces->intersection_with(tp_interfaces)->eq(tp_interfaces) && !tp->klass_is_exact()) {\n@@ -6832,1 +6850,1 @@\n-      interfaces = this_interfaces.intersection_with(tp_interfaces);\n+      interfaces = this_interfaces->intersection_with(tp_interfaces);\n@@ -6845,1 +6863,1 @@\n-  if (other->klass() == ciEnv::current()->Object_klass() && other->_interfaces.empty() && other_exact) {\n+  if (other->klass() == ciEnv::current()->Object_klass() && other->_interfaces->empty() && other_exact) {\n@@ -6857,1 +6875,1 @@\n-    return other->klass() == ciEnv::current()->Object_klass() && other->_interfaces.intersection_with(this_one->_interfaces).eq(other->_interfaces) && other_exact;\n+    return other->klass() == ciEnv::current()->Object_klass() && other->_interfaces->intersection_with(this_one->_interfaces)->eq(other->_interfaces) && other_exact;\n@@ -6920,1 +6938,1 @@\n-  if (other->klass() == ciEnv::current()->Object_klass() && other->_interfaces.empty() && other_exact) {\n+  if (other->klass() == ciEnv::current()->Object_klass() && other->_interfaces->empty() && other_exact) {\n@@ -6929,1 +6947,1 @@\n-    return other->_klass->equals(ciEnv::current()->Object_klass()) && other->_interfaces.intersection_with(this_one->_interfaces).eq(other->_interfaces);\n+    return other->_klass->equals(ciEnv::current()->Object_klass()) && other->_interfaces->intersection_with(this_one->_interfaces)->eq(other->_interfaces);\n@@ -7004,1 +7022,1 @@\n-      _interfaces.dump(st);\n+      _interfaces->dump(st);\n","filename":"src\/hotspot\/share\/opto\/type.cpp","additions":168,"deletions":150,"binary":false,"changes":318,"status":"modified"},{"patch":"@@ -99,0 +99,2 @@\n+    Interfaces,                 \/\/ Set of implemented interfaces for oop types\n+\n@@ -911,0 +913,42 @@\n+\/\/ Set of implemented interfaces. Referenced from TypeOopPtr and TypeKlassPtr.\n+class TypeInterfaces : public Type {\n+private:\n+  GrowableArray<ciInstanceKlass*> _list;\n+  uint _hash;\n+  ciInstanceKlass* _exact_klass;\n+  DEBUG_ONLY(bool _initialized;)\n+\n+  void initialize();\n+\n+  void add(ciInstanceKlass* interface);\n+  void verify() const NOT_DEBUG_RETURN;\n+  void compute_hash();\n+  void compute_exact_klass();\n+  TypeInterfaces();\n+  TypeInterfaces(GrowableArray<ciInstanceKlass*>* interfaces);\n+\n+  NONCOPYABLE(TypeInterfaces);\n+public:\n+  static const TypeInterfaces* make(GrowableArray<ciInstanceKlass*>* interfaces = nullptr);\n+  bool eq(const Type* other) const;\n+  bool eq(ciInstanceKlass* k) const;\n+  uint hash() const;\n+  const Type *xdual() const;\n+  void dump(outputStream* st) const;\n+  const TypeInterfaces* union_with(const TypeInterfaces* other) const;\n+  const TypeInterfaces* intersection_with(const TypeInterfaces* other) const;\n+  bool contains(const TypeInterfaces* other) const {\n+    return intersection_with(other)->eq(other);\n+  }\n+  bool empty() const { return _list.length() == 0; }\n+\n+  ciInstanceKlass* exact_klass() const;\n+  void verify_is_loaded() const NOT_DEBUG_RETURN;\n+\n+  static int compare(ciInstanceKlass* const& k1, ciInstanceKlass* const& k2);\n+\n+  const Type* xmeet(const Type* t) const;\n+\n+  bool singleton(void) const;\n+};\n+\n@@ -920,41 +964,1 @@\n-  class InterfaceSet {\n-  private:\n-    GrowableArray<ciKlass*> _list;\n-    uint _hash;\n-    ciKlass* _exact_klass;\n-    DEBUG_ONLY(bool _initialized;)\n-\n-    void initialize();\n-    void raw_add(ciKlass* interface);\n-    void add(ciKlass* interface);\n-    void verify() const NOT_DEBUG_RETURN;\n-    void compute_hash();\n-    void compute_exact_klass();\n-  public:\n-    InterfaceSet();\n-    InterfaceSet(GrowableArray<ciInstanceKlass*>* interfaces);\n-    bool eq(const InterfaceSet& other) const;\n-    bool eq(ciInstanceKlass* k) const;\n-    uint hash() const;\n-    void dump(outputStream* st) const;\n-    InterfaceSet union_with(const InterfaceSet& other) const;\n-    InterfaceSet intersection_with(const InterfaceSet& other) const;\n-    bool contains(const InterfaceSet& other) const {\n-      return intersection_with(other).eq(other);\n-    }\n-    bool empty() const { return _list.length() == 0; }\n-\n-    inline void* operator new(size_t x) throw() {\n-      Compile* compile = Compile::current();\n-      return compile->type_arena()->AmallocWords(x);\n-    }\n-    inline void operator delete(void* ptr) {\n-      ShouldNotReachHere();\n-    }\n-    ciKlass* exact_klass() const;\n-    void verify_is_loaded() const NOT_DEBUG_RETURN;\n-\n-    static int compare(ciKlass* const& k1, ciKlass* const& k2);\n-  };\n-\n-  static InterfaceSet interfaces(ciKlass*& k, bool klass, bool interface, bool array, InterfaceHandling interface_handling);\n+  static const TypeInterfaces* interfaces(ciKlass*& k, bool klass, bool interface, bool array, InterfaceHandling interface_handling);\n@@ -1020,2 +1024,2 @@\n-  template<class T> static TypePtr::MeetResult meet_instptr(PTR& ptr, InterfaceSet& interfaces, const T* this_type, const T* other_type,\n-                                                            ciKlass*& res_klass, bool& res_xk, bool& res_flat_array);\n+  template<class T> static TypePtr::MeetResult meet_instptr(PTR& ptr, const TypeInterfaces*& interfaces, const T* this_type,\n+                                                            const T* other_type, ciKlass*& res_klass, bool& res_xk, bool& res_flat_array);\n@@ -1151,2 +1155,2 @@\n-  TypeOopPtr(TYPES t, PTR ptr, ciKlass* k, const InterfaceSet& interfaces,bool xk, ciObject* o, Offset offset, Offset field_offset,\n-             int instance_id, const TypePtr* speculative, int inline_depth);\n+ TypeOopPtr(TYPES t, PTR ptr, ciKlass* k, const TypeInterfaces* interfaces, bool xk, ciObject* o, Offset offset, Offset field_offset, int instance_id,\n+            const TypePtr* speculative, int inline_depth);\n@@ -1168,1 +1172,1 @@\n-  const InterfaceSet _interfaces;\n+  const TypeInterfaces* _interfaces;\n@@ -1186,1 +1190,1 @@\n-  InterfaceSet meet_interfaces(const TypeOopPtr* other) const;\n+  const TypeInterfaces* meet_interfaces(const TypeOopPtr* other) const;\n@@ -1303,1 +1307,1 @@\n-  virtual const InterfaceSet interfaces() const {\n+  virtual const TypeInterfaces* interfaces() const {\n@@ -1324,1 +1328,1 @@\n-  TypeInstPtr(PTR ptr, ciKlass* k, const InterfaceSet& interfaces, bool xk, ciObject* o, Offset offset,\n+  TypeInstPtr(PTR ptr, ciKlass* k, const TypeInterfaces* interfaces, bool xk, ciObject* o, Offset offset,\n@@ -1347,1 +1351,1 @@\n-    const TypePtr::InterfaceSet interfaces = TypePtr::interfaces(k, true, false, false, ignore_interfaces);\n+    const TypeInterfaces* interfaces = TypePtr::interfaces(k, true, false, false, ignore_interfaces);\n@@ -1353,1 +1357,1 @@\n-    const TypePtr::InterfaceSet interfaces = TypePtr::interfaces(k, true, false, false, ignore_interfaces);\n+    const TypeInterfaces* interfaces = TypePtr::interfaces(k, true, false, false, ignore_interfaces);\n@@ -1359,1 +1363,1 @@\n-    const TypePtr::InterfaceSet interfaces = TypePtr::interfaces(klass, true, true, false, interface_handling);\n+    const TypeInterfaces* interfaces = TypePtr::interfaces(klass, true, true, false, interface_handling);\n@@ -1365,1 +1369,1 @@\n-    const TypePtr::InterfaceSet interfaces = TypePtr::interfaces(klass, true, false, false, ignore_interfaces);\n+    const TypeInterfaces* interfaces = TypePtr::interfaces(klass, true, false, false, ignore_interfaces);\n@@ -1371,1 +1375,1 @@\n-    const TypePtr::InterfaceSet interfaces = TypePtr::interfaces(klass, true, false, false, ignore_interfaces);\n+    const TypeInterfaces* interfaces = TypePtr::interfaces(klass, true, false, false, ignore_interfaces);\n@@ -1376,1 +1380,1 @@\n-  static const TypeInstPtr* make(PTR ptr, ciKlass* k, const InterfaceSet& interfaces, bool xk, ciObject* o, Offset offset,\n+  static const TypeInstPtr* make(PTR ptr, ciKlass* k, const TypeInterfaces* interfaces, bool xk, ciObject* o, Offset offset,\n@@ -1383,1 +1387,1 @@\n-    const TypePtr::InterfaceSet interfaces = TypePtr::interfaces(k, true, false, false, ignore_interfaces);\n+    const TypeInterfaces* interfaces = TypePtr::interfaces(k, true, false, false, ignore_interfaces);\n@@ -1415,1 +1419,1 @@\n-  virtual const TypeInstPtr *xmeet_unloaded(const TypeInstPtr *t, const InterfaceSet& interfaces) const;\n+  virtual const TypeInstPtr *xmeet_unloaded(const TypeInstPtr *tinst, const TypeInterfaces* interfaces) const;\n@@ -1436,1 +1440,1 @@\n-    return _klass->equals(other->is_instptr()->_klass) && _interfaces.eq(other->is_instptr()->_interfaces);\n+    return _klass->equals(other->is_instptr()->_klass) && _interfaces->eq(other->is_instptr()->_interfaces);\n@@ -1451,1 +1455,1 @@\n-    : TypeOopPtr(AryPtr, ptr, k, *_array_interfaces, xk, o, offset, field_offset, instance_id, speculative, inline_depth),\n+    : TypeOopPtr(AryPtr, ptr, k, _array_interfaces, xk, o, offset, field_offset, instance_id, speculative, inline_depth),\n@@ -1481,1 +1485,1 @@\n-  static const InterfaceSet* _array_interfaces;\n+  static const TypeInterfaces* _array_interfaces;\n@@ -1647,1 +1651,1 @@\n-  TypeKlassPtr(TYPES t, PTR ptr, ciKlass* klass, const InterfaceSet& interfaces, Offset offset);\n+  TypeKlassPtr(TYPES t, PTR ptr, ciKlass* klass, const TypeInterfaces* interfaces, Offset offset);\n@@ -1659,2 +1663,2 @@\n-  const InterfaceSet _interfaces;\n-  InterfaceSet meet_interfaces(const TypeKlassPtr* other) const;\n+  const TypeInterfaces* _interfaces;\n+  const TypeInterfaces* meet_interfaces(const TypeKlassPtr* other) const;\n@@ -1720,1 +1724,1 @@\n-  virtual const InterfaceSet interfaces() const {\n+  virtual const TypeInterfaces* interfaces() const {\n@@ -1740,1 +1744,1 @@\n-  TypeInstKlassPtr(PTR ptr, ciKlass* klass, const InterfaceSet& interfaces, Offset offset, bool flat_in_array)\n+  TypeInstKlassPtr(PTR ptr, ciKlass* klass, const TypeInterfaces* interfaces, Offset offset, bool flat_in_array)\n@@ -1763,1 +1767,1 @@\n-    InterfaceSet interfaces = TypePtr::interfaces(k, true, true, false, interface_handling);\n+    const TypeInterfaces* interfaces = TypePtr::interfaces(k, true, true, false, interface_handling);\n@@ -1766,1 +1770,1 @@\n-  static const TypeInstKlassPtr* make(PTR ptr, ciKlass* k, const InterfaceSet& interfaces, Offset offset, bool flat_in_array = false);\n+  static const TypeInstKlassPtr* make(PTR ptr, ciKlass* k, const TypeInterfaces* interfaces, Offset offset, bool flat_in_array = false);\n@@ -1769,1 +1773,1 @@\n-    const TypePtr::InterfaceSet interfaces = TypePtr::interfaces(k, true, false, false, ignore_interfaces);\n+    const TypeInterfaces* interfaces = TypePtr::interfaces(k, true, false, false, ignore_interfaces);\n@@ -1812,1 +1816,1 @@\n-  static const InterfaceSet* _array_interfaces;\n+  static const TypeInterfaces* _array_interfaces;\n@@ -1814,1 +1818,1 @@\n-    : TypeKlassPtr(AryKlassPtr, ptr, klass, *_array_interfaces, offset), _elem(elem), _not_flat(not_flat), _not_null_free(not_null_free), _null_free(null_free) {\n+    : TypeKlassPtr(AryKlassPtr, ptr, klass, _array_interfaces, offset), _elem(elem), _not_flat(not_flat), _not_null_free(not_null_free), _null_free(null_free) {\n","filename":"src\/hotspot\/share\/opto\/type.hpp","additions":74,"deletions":70,"binary":false,"changes":144,"status":"modified"},{"patch":"@@ -3836,1 +3836,1 @@\n-  return ClassListWriter::is_enabled() || DynamicDumpSharedSpaces;\n+  return ClassListWriter::is_enabled() || CDSConfig::is_dumping_dynamic_archive();\n@@ -3844,1 +3844,1 @@\n-  assert(ClassListWriter::is_enabled() || DynamicDumpSharedSpaces,  \"Should be set and open or do dynamic dump\");\n+  assert(ClassListWriter::is_enabled() || CDSConfig::is_dumping_dynamic_archive(),  \"Should be set and open or do dynamic dump\");\n@@ -3849,1 +3849,1 @@\n-    if (DynamicDumpSharedSpaces) {\n+    if (CDSConfig::is_dumping_dynamic_archive()) {\n","filename":"src\/hotspot\/share\/prims\/jvm.cpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -1050,1 +1050,0 @@\n-        ConstantPoolCacheEntry* entry;\n@@ -1056,3 +1055,2 @@\n-        \/\/ cache cannot be pre-fetched since some classes won't have it yet\n-          entry = mh->constants()->cache()->entry_at(cpci);\n-          pool_index = entry->constant_pool_index();\n+          \/\/ cache cannot be pre-fetched since some classes won't have it yet\n+          pool_index = mh->constants()->resolved_method_entry_at(cpci)->constant_pool_index();\n","filename":"src\/hotspot\/share\/prims\/jvmtiClassFileReconstituter.cpp","additions":2,"deletions":4,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -1874,13 +1874,0 @@\n-WB_ENTRY(jint, WB_GetConstantPoolCacheIndexTag(JNIEnv* env, jobject wb))\n-  return ConstantPool::CPCACHE_INDEX_TAG;\n-WB_END\n-\n-WB_ENTRY(jint, WB_GetConstantPoolCacheLength(JNIEnv* env, jobject wb, jclass klass))\n-  InstanceKlass* ik = InstanceKlass::cast(java_lang_Class::as_Klass(JNIHandles::resolve(klass)));\n-  ConstantPool* cp = ik->constants();\n-  if (cp->cache() == nullptr) {\n-      return -1;\n-  }\n-  return cp->cache()->length();\n-WB_END\n-\n@@ -1893,15 +1880,0 @@\n-WB_ENTRY(jint, WB_ConstantPoolRemapInstructionOperandFromCache(JNIEnv* env, jobject wb, jclass klass, jint index))\n-  InstanceKlass* ik = InstanceKlass::cast(java_lang_Class::as_Klass(JNIHandles::resolve(klass)));\n-  ConstantPool* cp = ik->constants();\n-  if (cp->cache() == nullptr) {\n-    THROW_MSG_0(vmSymbols::java_lang_IllegalStateException(), \"Constant pool does not have a cache\");\n-  }\n-  jint cpci = index;\n-  jint cpciTag = ConstantPool::CPCACHE_INDEX_TAG;\n-  if (cpciTag > cpci || cpci >= cp->cache()->length() + cpciTag) {\n-    THROW_MSG_0(vmSymbols::java_lang_IllegalArgumentException(), \"Constant pool cache index is out of range\");\n-  }\n-  jint cpi = cp->remap_instruction_operand_from_cache(cpci);\n-  return cpi;\n-WB_END\n-\n@@ -2017,0 +1989,18 @@\n+WB_ENTRY(jint, WB_getMethodEntriesLength(JNIEnv* env, jobject wb, jclass klass))\n+  InstanceKlass* ik = InstanceKlass::cast(java_lang_Class::as_Klass(JNIHandles::resolve(klass)));\n+  ConstantPool* cp = ik->constants();\n+  if (cp->cache() == nullptr) {\n+    return -1;\n+  }\n+  return cp->resolved_method_entries_length();\n+WB_END\n+\n+WB_ENTRY(jint, WB_getMethodCPIndex(JNIEnv* env, jobject wb, jclass klass, jint index))\n+  InstanceKlass* ik = InstanceKlass::cast(java_lang_Class::as_Klass(JNIHandles::resolve(klass)));\n+  ConstantPool* cp = ik->constants();\n+  if (cp->cache() == NULL) {\n+      return -1;\n+  }\n+  return cp->resolved_method_entry_at(index)->constant_pool_index();\n+WB_END\n+\n@@ -2905,4 +2895,0 @@\n-  {CC\"getConstantPoolCacheIndexTag0\", CC\"()I\",  (void*)&WB_GetConstantPoolCacheIndexTag},\n-  {CC\"getConstantPoolCacheLength0\", CC\"(Ljava\/lang\/Class;)I\",  (void*)&WB_GetConstantPoolCacheLength},\n-  {CC\"remapInstructionOperandFromCPCache0\",\n-      CC\"(Ljava\/lang\/Class;I)I\",                      (void*)&WB_ConstantPoolRemapInstructionOperandFromCache},\n@@ -2920,0 +2906,2 @@\n+  {CC\"getMethodEntriesLength0\", CC\"(Ljava\/lang\/Class;)I\",  (void*)&WB_getMethodEntriesLength},\n+  {CC\"getMethodCPIndex0\",    CC\"(Ljava\/lang\/Class;I)I\", (void*)&WB_getMethodCPIndex},\n","filename":"src\/hotspot\/share\/prims\/whitebox.cpp","additions":20,"deletions":32,"binary":false,"changes":52,"status":"modified"},{"patch":"@@ -1273,1 +1273,2 @@\n-    MetaspaceShared::disable_full_module_graph();\n+    CDSConfig::disable_loading_full_module_graph();\n+    CDSConfig::disable_dumping_full_module_graph();\n@@ -3161,1 +3162,1 @@\n-    DynamicDumpSharedSpaces = false;\n+    CDSConfig::disable_dumping_dynamic_archive();\n@@ -3163,1 +3164,1 @@\n-    DynamicDumpSharedSpaces = true;\n+    CDSConfig::enable_dumping_dynamic_archive();\n@@ -3184,1 +3185,1 @@\n-  if (DumpSharedSpaces || DynamicDumpSharedSpaces) {\n+  if (CDSConfig::is_dumping_archive()) {\n@@ -3591,1 +3592,1 @@\n-            DynamicDumpSharedSpaces = true;\n+            CDSConfig::enable_dumping_dynamic_archive();\n","filename":"src\/hotspot\/share\/runtime\/arguments.cpp","additions":6,"deletions":5,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -89,0 +89,1 @@\n+#include \"runtime\/synchronizer.hpp\"\n@@ -1733,3 +1734,13 @@\n-        BasicLock* lock = mon_info->lock();\n-        ObjectSynchronizer::enter(obj, lock, deoptee_thread);\n-        assert(mon_info->owner()->is_locked(), \"object must be locked now\");\n+        if (LockingMode == LM_LIGHTWEIGHT && exec_mode == Unpack_none) {\n+          \/\/ We have lost information about the correct state of the lock stack.\n+          \/\/ Inflate the locks instead. Enter then inflate to avoid races with\n+          \/\/ deflation.\n+          ObjectSynchronizer::enter(obj, nullptr, deoptee_thread);\n+          assert(mon_info->owner()->is_locked(), \"object must be locked now\");\n+          ObjectMonitor* mon = ObjectSynchronizer::inflate(deoptee_thread, obj(), ObjectSynchronizer::inflate_cause_vm_internal);\n+          assert(mon->owner() == deoptee_thread, \"must be\");\n+        } else {\n+          BasicLock* lock = mon_info->lock();\n+          ObjectSynchronizer::enter(obj, lock, deoptee_thread);\n+          assert(mon_info->owner()->is_locked(), \"object must be locked now\");\n+        }\n","filename":"src\/hotspot\/share\/runtime\/deoptimization.cpp","additions":14,"deletions":3,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -2022,1 +2022,1 @@\n-  product(int, LockingMode, LM_LIGHTWEIGHT,                                 \\\n+  product(int, LockingMode, LM_LEGACY,                                      \\\n@@ -2025,2 +2025,2 @@\n-          \"1: monitors & legacy stack-locking (LM_LEGACY), \"                \\\n-          \"2: monitors & new lightweight locking (LM_LIGHTWEIGHT, default)\") \\\n+          \"1: monitors & legacy stack-locking (LM_LEGACY, default), \"       \\\n+          \"2: monitors & new lightweight locking (LM_LIGHTWEIGHT)\")         \\\n","filename":"src\/hotspot\/share\/runtime\/globals.hpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -90,0 +90,1 @@\n+#include \"oops\/resolvedMethodEntry.hpp\"\n@@ -229,1 +230,0 @@\n-  nonstatic_field(ConstantPoolCache,           _length,                                       int)                                   \\\n@@ -233,0 +233,2 @@\n+  nonstatic_field(ConstantPoolCache,           _resolved_method_entries,                      Array<ResolvedMethodEntry>*)           \\\n+  nonstatic_field(ResolvedMethodEntry,         _cpool_index,                                  u2)                                    \\\n@@ -342,9 +344,0 @@\n-  \/***********************\/                                                                                                          \\\n-  \/* Constant Pool Cache *\/                                                                                                          \\\n-  \/***********************\/                                                                                                          \\\n-                                                                                                                                     \\\n-  volatile_nonstatic_field(ConstantPoolCacheEntry,      _indices,                             intx)                                  \\\n-  volatile_nonstatic_field(ConstantPoolCacheEntry,      _f1,                                  Metadata*)                             \\\n-  volatile_nonstatic_field(ConstantPoolCacheEntry,      _f2,                                  intx)                                  \\\n-  volatile_nonstatic_field(ConstantPoolCacheEntry,      _flags,                               intx)                                  \\\n-                                                                                                                                     \\\n@@ -493,0 +486,2 @@\n+  nonstatic_field(Array<ResolvedMethodEntry>,  _length,                                       int)                                   \\\n+  nonstatic_field(Array<ResolvedMethodEntry>,  _data[0],                                      ResolvedMethodEntry)                   \\\n@@ -980,0 +975,1 @@\n+  unchecked_nonstatic_field(Array<ResolvedMethodEntry>,_data,                                 sizeof(ResolvedMethodEntry))           \\\n@@ -1757,6 +1753,1 @@\n-  declare_c2_type(ReplicateBNode, VectorNode)                             \\\n-  declare_c2_type(ReplicateSNode, VectorNode)                             \\\n-  declare_c2_type(ReplicateINode, VectorNode)                             \\\n-  declare_c2_type(ReplicateLNode, VectorNode)                             \\\n-  declare_c2_type(ReplicateFNode, VectorNode)                             \\\n-  declare_c2_type(ReplicateDNode, VectorNode)                             \\\n+  declare_c2_type(ReplicateNode, VectorNode)                              \\\n@@ -1916,0 +1907,1 @@\n+            declare_type(Array<ResolvedMethodEntry>, MetaspaceObj)        \\\n@@ -1933,1 +1925,1 @@\n-  declare_toplevel_type(ConstantPoolCacheEntry)                           \\\n+  declare_toplevel_type(ResolvedMethodEntry)                              \\\n@@ -2211,12 +2203,0 @@\n-  declare_constant(ConstantPool::CPCACHE_INDEX_TAG)                       \\\n-                                                                          \\\n-  \/********************************\/                                      \\\n-  \/* ConstantPoolCacheEntry enums *\/                                      \\\n-  \/********************************\/                                      \\\n-                                                                          \\\n-  declare_constant(ConstantPoolCacheEntry::is_volatile_shift)             \\\n-  declare_constant(ConstantPoolCacheEntry::is_final_shift)                \\\n-  declare_constant(ConstantPoolCacheEntry::is_forced_virtual_shift)       \\\n-  declare_constant(ConstantPoolCacheEntry::is_vfinal_shift)               \\\n-  declare_constant(ConstantPoolCacheEntry::is_field_entry_shift)          \\\n-  declare_constant(ConstantPoolCacheEntry::tos_state_shift)               \\\n","filename":"src\/hotspot\/share\/runtime\/vmStructs.cpp","additions":9,"deletions":29,"binary":false,"changes":38,"status":"modified"},{"patch":"@@ -43,0 +43,2 @@\n+#include \"nmt\/memMapPrinter.hpp\"\n+#include \"nmt\/memTracker.hpp\"\n@@ -70,0 +72,1 @@\n+#include \"os_posix.hpp\"\n@@ -72,0 +75,1 @@\n+#include <errno.h>\n@@ -137,0 +141,2 @@\n+  DCmdFactory::register_DCmdFactory(new DCmdFactoryImpl<SystemMapDCmd>(full_export, true,false));\n+  DCmdFactory::register_DCmdFactory(new DCmdFactoryImpl<SystemDumpMapDCmd>(full_export, true,false));\n@@ -1178,0 +1184,42 @@\n+\n+#ifdef LINUX\n+\n+SystemMapDCmd::SystemMapDCmd(outputStream* output, bool heap) :\n+    DCmdWithParser(output, heap),\n+  _human_readable(\"-H\", \"Human readable format\", \"BOOLEAN\", false, \"false\") {\n+  _dcmdparser.add_dcmd_option(&_human_readable);\n+}\n+\n+void SystemMapDCmd::execute(DCmdSource source, TRAPS) {\n+  MemMapPrinter::print_all_mappings(output(), _human_readable.value());\n+}\n+\n+SystemDumpMapDCmd::SystemDumpMapDCmd(outputStream* output, bool heap) :\n+    DCmdWithParser(output, heap),\n+  _human_readable(\"-H\", \"Human readable format\", \"BOOLEAN\", false, \"false\"),\n+  _filename(\"-F\", \"file path (defaults: \\\"vm_memory_map_<pid>.txt\\\")\", \"STRING\", false) {\n+  _dcmdparser.add_dcmd_option(&_human_readable);\n+  _dcmdparser.add_dcmd_option(&_filename);\n+}\n+\n+void SystemDumpMapDCmd::execute(DCmdSource source, TRAPS) {\n+  stringStream default_name;\n+  default_name.print(\"vm_memory_map_%d.txt\", os::current_process_id());\n+  const char* name = _filename.is_set() ? _filename.value() : default_name.base();\n+  fileStream fs(name);\n+  if (fs.is_open()) {\n+    if (!MemTracker::enabled()) {\n+      output()->print_cr(\"(NMT is disabled, will not annotate mappings).\");\n+    }\n+    MemMapPrinter::print_all_mappings(&fs, _human_readable.value());\n+    \/\/ For the readers convenience, resolve path name.\n+    char tmp[JVM_MAXPATHLEN];\n+    const char* absname = os::Posix::realpath(name, tmp, sizeof(tmp));\n+    name = absname != nullptr ? absname : name;\n+    output()->print_cr(\"Memory map dumped to \\\"%s\\\".\", name);\n+  } else {\n+    output()->print_cr(\"Failed to open \\\"%s\\\" for writing (%s).\", name, os::strerror(errno));\n+  }\n+}\n+\n+#endif \/\/ LINUX\n","filename":"src\/hotspot\/share\/services\/diagnosticCommand.cpp","additions":48,"deletions":0,"binary":false,"changes":48,"status":"modified"},{"patch":"@@ -1006,0 +1006,41 @@\n+#ifdef LINUX\n+\n+class SystemMapDCmd : public DCmdWithParser {\n+  DCmdArgument<bool> _human_readable;\n+public:\n+  static int num_arguments() { return 1; }\n+  SystemMapDCmd(outputStream* output, bool heap);\n+  static const char* name() { return \"System.map\"; }\n+  static const char* description() {\n+    return \"Prints an annotated process memory map of the VM process (linux only).\";\n+  }\n+  static const char* impact() { return \"Low\"; }\n+  static const JavaPermission permission() {\n+    JavaPermission p = {\"java.lang.management.ManagementPermission\",\n+                        \"control\", nullptr};\n+    return p;\n+  }\n+  virtual void execute(DCmdSource source, TRAPS);\n+};\n+\n+class SystemDumpMapDCmd : public DCmdWithParser {\n+  DCmdArgument<bool> _human_readable;\n+  DCmdArgument<char*> _filename;\n+public:\n+  static int num_arguments() { return 2; }\n+  SystemDumpMapDCmd(outputStream* output, bool heap);\n+  static const char* name() { return \"System.dump_map\"; }\n+  static const char* description() {\n+    return \"Dumps an annotated process memory map to an output file (linux only).\";\n+  }\n+  static const char* impact() { return \"Low\"; }\n+  static const JavaPermission permission() {\n+    JavaPermission p = {\"java.lang.management.ManagementPermission\",\n+                        \"control\", nullptr};\n+    return p;\n+  }\n+  virtual void execute(DCmdSource source, TRAPS);\n+};\n+\n+#endif \/\/ LINUX\n+\n","filename":"src\/hotspot\/share\/services\/diagnosticCommand.hpp","additions":41,"deletions":0,"binary":false,"changes":41,"status":"modified"},{"patch":"@@ -822,1 +822,2 @@\n-\/\/ Support class with a collection of functions used when dumping the heap\n+class DumperClassCacheTable;\n+class DumperClassCacheTableEntry;\n@@ -824,0 +825,1 @@\n+\/\/ Support class with a collection of functions used when dumping the heap\n@@ -838,1 +840,1 @@\n-  static u4 instance_size(InstanceKlass* ik);\n+  static u4 instance_size(InstanceKlass* ik, DumperClassCacheTableEntry* class_cache_entry = nullptr);\n@@ -853,1 +855,1 @@\n-  static void dump_instance_fields(AbstractDumpWriter* writer, oop o, int offset, InstanceKlass* klass);\n+  static void dump_instance_fields(AbstractDumpWriter* writer, oop o, int offset, DumperClassCacheTable* class_cache, DumperClassCacheTableEntry* class_cache_entry);\n@@ -856,1 +858,1 @@\n-  static void dump_inlined_object_fields(AbstractDumpWriter* writer, oop o, int offset, InlineKlass* klass);\n+  static void dump_inlined_object_fields(AbstractDumpWriter* writer, oop o, int offset, DumperClassCacheTable* class_cache, DumperClassCacheTableEntry* class_cache_entry);\n@@ -863,1 +865,1 @@\n-  static void dump_instance(AbstractDumpWriter* writer, oop o);\n+  static void dump_instance(AbstractDumpWriter* writer, oop o, DumperClassCacheTable* class_cache);\n@@ -872,1 +874,1 @@\n-  static void dump_flat_array(AbstractDumpWriter* writer, flatArrayOop array);\n+  static void dump_flat_array(AbstractDumpWriter* writer, flatArrayOop array, DumperClassCacheTable* class_cache);\n@@ -908,0 +910,112 @@\n+\/\/ Hash table of klasses to the klass metadata. This should greatly improve the\n+\/\/ hash dumping performance. This hash table is supposed to be used by a single\n+\/\/ thread only.\n+\/\/\n+class DumperClassCacheTableEntry : public CHeapObj<mtServiceability> {\n+  friend class DumperClassCacheTable;\n+private:\n+  GrowableArray<char> _sigs_start;\n+  GrowableArray<int> _offsets;\n+  GrowableArray<InlineKlass*> _inline_klasses;\n+  u4 _instance_size;\n+  int _entries;\n+\n+public:\n+  DumperClassCacheTableEntry() : _instance_size(0), _entries(0) {};\n+\n+  int field_count()             { return _entries; }\n+  char sig_start(int field_idx) { return _sigs_start.at(field_idx); }\n+  void push_sig_start_inlined() { _sigs_start.push('Q'); }\n+  bool is_inlined(int field_idx){ return _sigs_start.at(field_idx) == 'Q'; }\n+  InlineKlass* inline_klass(int field_idx) { assert(is_inlined(field_idx), \"Not inlined\"); return _inline_klasses.at(field_idx); }\n+  int offset(int field_idx)     { return _offsets.at(field_idx); }\n+  u4 instance_size()            { return _instance_size; }\n+};\n+\n+class DumperClassCacheTable {\n+private:\n+  \/\/ ResourceHashtable SIZE is specified at compile time so we\n+  \/\/ use 1031 which is the first prime after 1024.\n+  static constexpr size_t TABLE_SIZE = 1031;\n+\n+  \/\/ Maintain the cache for N classes. This limits memory footprint\n+  \/\/ impact, regardless of how many classes we have in the dump.\n+  \/\/ This also improves look up performance by keeping the statically\n+  \/\/ sized table from overloading.\n+  static constexpr int CACHE_TOP = 256;\n+\n+  typedef ResourceHashtable<InstanceKlass*, DumperClassCacheTableEntry*,\n+                            TABLE_SIZE, AnyObj::C_HEAP, mtServiceability> PtrTable;\n+  PtrTable* _ptrs;\n+\n+  \/\/ Single-slot cache to handle the major case of objects of the same\n+  \/\/ class back-to-back, e.g. from T[].\n+  InstanceKlass* _last_ik;\n+  DumperClassCacheTableEntry* _last_entry;\n+\n+  void unlink_all(PtrTable* table) {\n+    class CleanupEntry: StackObj {\n+    public:\n+      bool do_entry(InstanceKlass*& key, DumperClassCacheTableEntry*& entry) {\n+        delete entry;\n+        return true;\n+      }\n+    } cleanup;\n+    table->unlink(&cleanup);\n+  }\n+\n+public:\n+  DumperClassCacheTableEntry* lookup_or_create(InstanceKlass* ik) {\n+    if (_last_ik == ik) {\n+      return _last_entry;\n+    }\n+\n+    DumperClassCacheTableEntry* entry;\n+    DumperClassCacheTableEntry** from_cache = _ptrs->get(ik);\n+    if (from_cache == nullptr) {\n+      entry = new DumperClassCacheTableEntry();\n+      for (HierarchicalFieldStream<JavaFieldStream> fld(ik); !fld.done(); fld.next()) {\n+        if (!fld.access_flags().is_static()) {\n+          InlineKlass* inlineKlass = nullptr;\n+          if (DumperSupport::is_inlined_field(fld.field_descriptor())) {\n+            inlineKlass = DumperSupport::get_inlined_field_klass(fld.field_descriptor());\n+            entry->push_sig_start_inlined();\n+            entry->_instance_size += DumperSupport::instance_size(inlineKlass);\n+          } else {\n+            Symbol* sig = fld.signature();\n+            entry->_sigs_start.push(sig->char_at(0));\n+            entry->_instance_size += DumperSupport::sig2size(sig);\n+          }\n+          entry->_inline_klasses.push(inlineKlass);\n+          entry->_offsets.push(fld.offset());\n+          entry->_entries++;\n+        }\n+      }\n+\n+      if (_ptrs->number_of_entries() >= CACHE_TOP) {\n+        \/\/ We do not track the individual hit rates for table entries.\n+        \/\/ Purge the entire table, and let the cache catch up with new\n+        \/\/ distribution.\n+        unlink_all(_ptrs);\n+      }\n+\n+      _ptrs->put(ik, entry);\n+    } else {\n+      entry = *from_cache;\n+    }\n+\n+    \/\/ Remember for single-slot cache.\n+    _last_ik = ik;\n+    _last_entry = entry;\n+\n+    return entry;\n+  }\n+\n+  DumperClassCacheTable() : _ptrs(new (mtServiceability) PtrTable), _last_ik(nullptr), _last_entry(nullptr) {}\n+\n+  ~DumperClassCacheTable() {\n+    unlink_all(_ptrs);\n+    delete _ptrs;\n+  }\n+};\n+\n@@ -1056,9 +1170,12 @@\n-u4 DumperSupport::instance_size(InstanceKlass *ik) {\n-  u4 size = 0;\n-\n-  for (HierarchicalFieldStream<JavaFieldStream> fld(ik); !fld.done(); fld.next()) {\n-    if (!fld.access_flags().is_static()) {\n-      if (is_inlined_field(fld.field_descriptor())) {\n-        size += instance_size(get_inlined_field_klass(fld.field_descriptor()));\n-      } else {\n-        size += sig2size(fld.signature());\n+u4 DumperSupport::instance_size(InstanceKlass* ik, DumperClassCacheTableEntry* class_cache_entry) {\n+  if (class_cache_entry != nullptr) {\n+    return class_cache_entry->instance_size();\n+  } else {\n+    u4 size = 0;\n+    for (HierarchicalFieldStream<JavaFieldStream> fld(ik); !fld.done(); fld.next()) {\n+      if (!fld.access_flags().is_static()) {\n+        if (is_inlined_field(fld.field_descriptor())) {\n+          size += instance_size(get_inlined_field_klass(fld.field_descriptor()));\n+        } else {\n+          size += sig2size(fld.signature());\n+        }\n@@ -1067,0 +1184,1 @@\n+    return size;\n@@ -1068,1 +1186,0 @@\n-  return size;\n@@ -1146,12 +1263,10 @@\n-void DumperSupport::dump_instance_fields(AbstractDumpWriter* writer, oop o, int offset, InstanceKlass *ik) {\n-  for (HierarchicalFieldStream<JavaFieldStream> fld(ik); !fld.done(); fld.next()) {\n-    if (!fld.access_flags().is_static()) {\n-      if (is_inlined_field(fld.field_descriptor())) {\n-        InlineKlass* field_klass = get_inlined_field_klass(fld.field_descriptor());\n-        \/\/ the field is inlined, so all its fields are stored without headers.\n-        int fields_offset = offset + fld.offset() - field_klass->first_field_offset();\n-        dump_inlined_object_fields(writer, o, offset + fld.offset(), field_klass);\n-      } else {\n-        Symbol* sig = fld.signature();\n-        dump_field_value(writer, sig->char_at(0), o, offset + fld.offset());\n-      }\n+void DumperSupport::dump_instance_fields(AbstractDumpWriter* writer, oop o, int offset, DumperClassCacheTable* class_cache, DumperClassCacheTableEntry* class_cache_entry) {\n+  assert(class_cache_entry != nullptr, \"Pre-condition: must be provided\");\n+  for (int idx = 0; idx < class_cache_entry->field_count(); idx++) {\n+    if (class_cache_entry->is_inlined(idx)) {\n+      InlineKlass* field_klass = class_cache_entry->inline_klass(idx);\n+      int fields_offset = offset + (class_cache_entry->offset(idx) - field_klass->first_field_offset());\n+      DumperClassCacheTableEntry* inline_class_cache_entry = class_cache->lookup_or_create(field_klass);\n+      dump_inlined_object_fields(writer, o, fields_offset, class_cache, inline_class_cache_entry);\n+    } else {\n+      dump_field_value(writer, class_cache_entry->sig_start(idx), o, class_cache_entry->offset(idx));\n@@ -1162,1 +1277,1 @@\n-void DumperSupport::dump_inlined_object_fields(AbstractDumpWriter* writer, oop o, int offset, InlineKlass* klass) {\n+void DumperSupport::dump_inlined_object_fields(AbstractDumpWriter* writer, oop o, int offset, DumperClassCacheTable* class_cache, DumperClassCacheTableEntry* class_cache_entry) {\n@@ -1164,1 +1279,1 @@\n-  dump_instance_fields(writer, o, offset - klass->first_field_offset(), klass);\n+  dump_instance_fields(writer, o, offset, class_cache, class_cache_entry);\n@@ -1230,1 +1345,1 @@\n-void DumperSupport::dump_instance(AbstractDumpWriter* writer, oop o) {\n+void DumperSupport::dump_instance(AbstractDumpWriter* writer, oop o, DumperClassCacheTable* class_cache) {\n@@ -1232,1 +1347,4 @@\n-  u4 is = instance_size(ik);\n+\n+  DumperClassCacheTableEntry* cache_entry = class_cache->lookup_or_create(ik);\n+\n+  u4 is = instance_size(ik, cache_entry);\n@@ -1246,1 +1364,1 @@\n-  dump_instance_fields(writer, o, 0, ik);\n+  dump_instance_fields(writer, o, 0, class_cache, cache_entry);\n@@ -1407,1 +1525,1 @@\n-void DumperSupport::dump_flat_array(AbstractDumpWriter* writer, flatArrayOop array) {\n+void DumperSupport::dump_flat_array(AbstractDumpWriter* writer, flatArrayOop array, DumperClassCacheTable* class_cache) {\n@@ -1436,1 +1554,2 @@\n-    dump_inlined_object_fields(writer, array, offset, element_klass);\n+    DumperClassCacheTableEntry* class_cache_entry = class_cache->lookup_or_create(element_klass);\n+    dump_inlined_object_fields(writer, array, offset, class_cache, class_cache_entry);\n@@ -2251,0 +2370,2 @@\n+  DumperClassCacheTable _class_cache;\n+\n@@ -2275,1 +2396,1 @@\n-    DumperSupport::dump_instance(writer(), o);\n+    DumperSupport::dump_instance(writer(), o, &_class_cache);\n@@ -2280,1 +2401,1 @@\n-    DumperSupport::dump_flat_array(writer(), flatArrayOop(o));\n+    DumperSupport::dump_flat_array(writer(), flatArrayOop(o), &_class_cache);\n@@ -2845,0 +2966,1 @@\n+    ResourceMark rm;\n","filename":"src\/hotspot\/share\/services\/heapDumper.cpp","additions":159,"deletions":37,"binary":false,"changes":196,"status":"modified"},{"patch":"@@ -44,1 +44,0 @@\n-bool DynamicDumpSharedSpaces;\n","filename":"src\/hotspot\/share\/utilities\/globalDefinitions.cpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -557,1 +557,0 @@\n-extern bool DynamicDumpSharedSpaces;\n","filename":"src\/hotspot\/share\/utilities\/globalDefinitions.hpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -2326,0 +2326,15 @@\n+        public Type implicitReceiverType() {\n+            ClassSymbol enclosingClass = enclClass();\n+            if (enclosingClass == null) {\n+                return null;\n+            }\n+            Type enclosingType = enclosingClass.type;\n+            if (isConstructor()) {\n+                return enclosingType.getEnclosingType();\n+            }\n+            if (!isStatic()) {\n+                return enclosingType;\n+            }\n+            return null;\n+        }\n+\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/code\/Symbol.java","additions":15,"deletions":0,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -178,0 +178,5 @@\n+    \/** The queue of classes that should have typeEnter completer installed after\n+     *  all the classes are discovered.\n+     *\/\n+    List<ClassSymbol> pendingCompleter = null;\n+\n@@ -541,2 +546,6 @@\n-        \/\/ install further completer for this type.\n-        c.completer = typeEnter;\n+        \/\/ schedule installation of further completer for this type.\n+        if (pendingCompleter != null) {\n+            pendingCompleter = pendingCompleter.prepend(c);\n+        } else {\n+            c.completer = typeEnter;\n+        }\n@@ -621,2 +630,12 @@\n-            \/\/ enter all classes, and construct uncompleted list\n-            classEnter(trees, null);\n+            List<ClassSymbol> prevPendingCompleter = pendingCompleter;\n+            try {\n+                pendingCompleter = List.nil();\n+                \/\/ enter all classes, and construct uncompleted list\n+                classEnter(trees, null);\n+                \/\/ install further completer for classes recognized by the above task:\n+                for (ClassSymbol sym : pendingCompleter) {\n+                    sym.completer = typeEnter;\n+                }\n+            } finally {\n+                pendingCompleter = prevPendingCompleter;\n+            }\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/comp\/Enter.java","additions":23,"deletions":4,"binary":false,"changes":27,"status":"modified"},{"patch":"@@ -2384,2 +2384,7 @@\n-            if (mt.recvtype != null) {\n-                mt.recvtype = addTypeAnnotations(mt.recvtype, TargetType.METHOD_RECEIVER);\n+\n+            Type recvtype = mt.recvtype != null ? mt.recvtype : s.implicitReceiverType();\n+            if (recvtype != null) {\n+                Type annotated = addTypeAnnotations(recvtype, TargetType.METHOD_RECEIVER);\n+                if (annotated != recvtype) {\n+                    mt.recvtype = annotated;\n+                }\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/jvm\/ClassReader.java","additions":7,"deletions":2,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -408,0 +408,1 @@\n+  public static boolean   isFieldCode  (int code) { return (_getstatic <= code && code <= _putfield); }\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/interpreter\/Bytecodes.java","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -222,1 +222,0 @@\n-    final int constantPoolCpCacheIndexTag = getConstant(\"ConstantPool::CPCACHE_INDEX_TAG\", Integer.class);\n","filename":"src\/jdk.internal.vm.ci\/share\/classes\/jdk\/vm\/ci\/hotspot\/HotSpotVMConfig.java","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -146,1 +146,0 @@\n-serviceability\/sa\/ClhsdbDumpclass.java 8316342 generic-all\n","filename":"test\/hotspot\/jtreg\/ProblemList.txt","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -935,0 +935,16 @@\n+    public static final String MUL_ADD_S2I = PREFIX + \"MUL_ADD_S2I\" + POSTFIX;\n+    static {\n+        beforeMatchingNameRegex(MUL_ADD_S2I, \"MulAddS2I\");\n+    }\n+\n+    public static final String MUL_ADD_VS2VI = VECTOR_PREFIX + \"MUL_ADD_VS2VI\" + POSTFIX;\n+    static {\n+        vectorNode(MUL_ADD_VS2VI, \"MulAddVS2VI\", TYPE_INT);\n+    }\n+\n+    \/\/ Can only be used if avx512_vnni is available.\n+    public static final String MUL_ADD_VS2VI_VNNI = PREFIX + \"MUL_ADD_VS2VI_VNNI\" + POSTFIX;\n+    static {\n+        machOnly(MUL_ADD_VS2VI_VNNI, \"vmuladdaddS2I_reg\");\n+    }\n+\n@@ -1129,1 +1145,1 @@\n-    public static final String REPLICATE_B = PREFIX + \"REPLICATE_B\" + POSTFIX;\n+    public static final String REPLICATE_B = VECTOR_PREFIX + \"REPLICATE_B\" + POSTFIX;\n@@ -1131,4 +1147,1 @@\n-        String regex = START + \"ReplicateB\" + MID + END;\n-        IR_NODE_MAPPINGS.put(REPLICATE_B, new SinglePhaseRangeEntry(CompilePhase.PRINT_IDEAL, regex,\n-                                                                    CompilePhase.AFTER_CLOOPS,\n-                                                                    CompilePhase.BEFORE_MATCHING));\n+        vectorNode(REPLICATE_B, \"Replicate\", TYPE_BYTE);\n@@ -1137,1 +1150,1 @@\n-    public static final String REPLICATE_S = PREFIX + \"REPLICATE_S\" + POSTFIX;\n+    public static final String REPLICATE_S = VECTOR_PREFIX + \"REPLICATE_S\" + POSTFIX;\n@@ -1139,4 +1152,1 @@\n-        String regex = START + \"ReplicateS\" + MID + END;\n-        IR_NODE_MAPPINGS.put(REPLICATE_S, new SinglePhaseRangeEntry(CompilePhase.PRINT_IDEAL, regex,\n-                                                                    CompilePhase.AFTER_CLOOPS,\n-                                                                    CompilePhase.BEFORE_MATCHING));\n+        vectorNode(REPLICATE_S, \"Replicate\", TYPE_SHORT);\n@@ -1145,1 +1155,1 @@\n-    public static final String REPLICATE_I = PREFIX + \"REPLICATE_I\" + POSTFIX;\n+    public static final String REPLICATE_I = VECTOR_PREFIX + \"REPLICATE_I\" + POSTFIX;\n@@ -1147,4 +1157,1 @@\n-        String regex = START + \"ReplicateI\" + MID + END;\n-        IR_NODE_MAPPINGS.put(REPLICATE_I, new SinglePhaseRangeEntry(CompilePhase.PRINT_IDEAL, regex,\n-                                                                    CompilePhase.AFTER_CLOOPS,\n-                                                                    CompilePhase.BEFORE_MATCHING));\n+        vectorNode(REPLICATE_I, \"Replicate\", TYPE_INT);\n@@ -1153,1 +1160,1 @@\n-    public static final String REPLICATE_L = PREFIX + \"REPLICATE_L\" + POSTFIX;\n+    public static final String REPLICATE_L = VECTOR_PREFIX + \"REPLICATE_L\" + POSTFIX;\n@@ -1155,4 +1162,1 @@\n-        String regex = START + \"ReplicateL\" + MID + END;\n-        IR_NODE_MAPPINGS.put(REPLICATE_L, new SinglePhaseRangeEntry(CompilePhase.PRINT_IDEAL, regex,\n-                                                                    CompilePhase.AFTER_CLOOPS,\n-                                                                    CompilePhase.BEFORE_MATCHING));\n+        vectorNode(REPLICATE_L, \"Replicate\", TYPE_LONG);\n@@ -1161,1 +1165,1 @@\n-    public static final String REPLICATE_F = PREFIX + \"REPLICATE_F\" + POSTFIX;\n+    public static final String REPLICATE_F = VECTOR_PREFIX + \"REPLICATE_F\" + POSTFIX;\n@@ -1163,4 +1167,1 @@\n-        String regex = START + \"ReplicateF\" + MID + END;\n-        IR_NODE_MAPPINGS.put(REPLICATE_F, new SinglePhaseRangeEntry(CompilePhase.PRINT_IDEAL, regex,\n-                                                                    CompilePhase.AFTER_CLOOPS,\n-                                                                    CompilePhase.BEFORE_MATCHING));\n+        vectorNode(REPLICATE_F, \"Replicate\", TYPE_FLOAT);\n@@ -1169,1 +1170,1 @@\n-    public static final String REPLICATE_D = PREFIX + \"REPLICATE_D\" + POSTFIX;\n+    public static final String REPLICATE_D = VECTOR_PREFIX + \"REPLICATE_D\" + POSTFIX;\n@@ -1171,4 +1172,1 @@\n-        String regex = START + \"ReplicateD\" + MID + END;\n-        IR_NODE_MAPPINGS.put(REPLICATE_D, new SinglePhaseRangeEntry(CompilePhase.PRINT_IDEAL, regex,\n-                                                                    CompilePhase.AFTER_CLOOPS,\n-                                                                    CompilePhase.BEFORE_MATCHING));\n+        vectorNode(REPLICATE_D, \"Replicate\", TYPE_DOUBLE);\n@@ -1944,20 +1942,0 @@\n-    public static final String VMASK_CMP_IMM_B_SVE = PREFIX + \"VMASK_CMP_IMM_B_SVE\" + POSTFIX;\n-    static {\n-        machOnlyNameRegex(VMASK_CMP_IMM_B_SVE, \"vmaskcmp_immB_sve\");\n-    }\n-\n-    public static final String VMASK_CMPU_IMM_B_SVE = PREFIX + \"VMASK_CMPU_IMM_B_SVE\" + POSTFIX;\n-    static {\n-        machOnlyNameRegex(VMASK_CMPU_IMM_B_SVE, \"vmaskcmpU_immB_sve\");\n-    }\n-\n-    public static final String VMASK_CMP_IMM_S_SVE = PREFIX + \"VMASK_CMP_IMM_S_SVE\" + POSTFIX;\n-    static {\n-        machOnlyNameRegex(VMASK_CMP_IMM_S_SVE, \"vmaskcmp_immS_sve\");\n-    }\n-\n-    public static final String VMASK_CMPU_IMM_S_SVE = PREFIX + \"VMASK_CMPU_IMM_S_SVE\" + POSTFIX;\n-    static {\n-        machOnlyNameRegex(VMASK_CMPU_IMM_S_SVE, \"vmaskcmpU_immS_sve\");\n-    }\n-\n","filename":"test\/hotspot\/jtreg\/compiler\/lib\/ir_framework\/IRNode.java","additions":28,"deletions":50,"binary":false,"changes":78,"status":"modified"},{"patch":"@@ -28,1 +28,1 @@\n-import test.java.lang.invoke.lib.InstructionHelper;\n+import test.java.lang.invoke.lib.OldInstructionHelper;\n@@ -54,1 +54,1 @@\n- * @build jdk.experimental.bytecode.BasicClassBuilder test.java.lang.invoke.lib.InstructionHelper\n+ * @build jdk.experimental.bytecode.BasicClassBuilder test.java.lang.invoke.lib.OldInstructionHelper\n@@ -1100,1 +1100,1 @@\n-    private static final MethodHandle setArrayElementNull = InstructionHelper.loadCode(MethodHandles.lookup(),\n+    private static final MethodHandle setArrayElementNull = OldInstructionHelper.loadCode(MethodHandles.lookup(),\n@@ -1440,1 +1440,1 @@\n-    private static final MethodHandle setArrayElementIncompatible = InstructionHelper.loadCode(MethodHandles.lookup(),\n+    private static final MethodHandle setArrayElementIncompatible = OldInstructionHelper.loadCode(MethodHandles.lookup(),\n@@ -4064,1 +4064,1 @@\n-    private static final MethodHandle setArrayElementIncompatibleRef = InstructionHelper.loadCode(MethodHandles.lookup(),\n+    private static final MethodHandle setArrayElementIncompatibleRef = OldInstructionHelper.loadCode(MethodHandles.lookup(),\n","filename":"test\/hotspot\/jtreg\/compiler\/valhalla\/inlinetypes\/TestLWorld.java","additions":5,"deletions":5,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -29,1 +29,1 @@\n-import test.java.lang.invoke.lib.InstructionHelper;\n+import test.java.lang.invoke.lib.OldInstructionHelper;\n@@ -53,1 +53,1 @@\n- * @build jdk.experimental.bytecode.BasicClassBuilder test.java.lang.invoke.lib.InstructionHelper\n+ * @build jdk.experimental.bytecode.BasicClassBuilder test.java.lang.invoke.lib.OldInstructionHelper\n@@ -2579,1 +2579,1 @@\n-    private static final MethodHandle refCheckCast = InstructionHelper.loadCode(MethodHandles.lookup(),\n+    private static final MethodHandle refCheckCast = OldInstructionHelper.loadCode(MethodHandles.lookup(),\n","filename":"test\/hotspot\/jtreg\/compiler\/valhalla\/inlinetypes\/TestNullableInlineTypes.java","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -29,1 +29,1 @@\n-import test.java.lang.invoke.lib.InstructionHelper;\n+import test.java.lang.invoke.lib.OldInstructionHelper;\n@@ -49,1 +49,1 @@\n- * @build jdk.experimental.bytecode.BasicClassBuilder test.java.lang.invoke.lib.InstructionHelper\n+ * @build jdk.experimental.bytecode.BasicClassBuilder test.java.lang.invoke.lib.OldInstructionHelper\n","filename":"test\/hotspot\/jtreg\/compiler\/valhalla\/inlinetypes\/TestValueClasses.java","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -37,1 +37,1 @@\n-import test.java.lang.invoke.lib.InstructionHelper;\n+import test.java.lang.invoke.lib.OldInstructionHelper;\n@@ -52,1 +52,1 @@\n- * @build jdk.experimental.bytecode.BasicClassBuilder test.java.lang.invoke.lib.InstructionHelper\n+ * @build jdk.experimental.bytecode.BasicClassBuilder test.java.lang.invoke.lib.OldInstructionHelper\n@@ -76,1 +76,1 @@\n-        MethodHandle testNewOnInlineClass = InstructionHelper.loadCode(\n+        MethodHandle testNewOnInlineClass = OldInstructionHelper.loadCode(\n@@ -98,1 +98,1 @@\n-        MethodHandle testAconstInitOnIdentityClass = InstructionHelper.loadCode(\n+        MethodHandle testAconstInitOnIdentityClass = OldInstructionHelper.loadCode(\n","filename":"test\/hotspot\/jtreg\/runtime\/valhalla\/inlinetypes\/CreationErrorTest.java","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -39,1 +39,1 @@\n-import test.java.lang.invoke.lib.InstructionHelper;\n+import test.java.lang.invoke.lib.OldInstructionHelper;\n@@ -48,1 +48,1 @@\n- * @build jdk.experimental.bytecode.BasicClassBuilder test.java.lang.invoke.lib.InstructionHelper\n+ * @build jdk.experimental.bytecode.BasicClassBuilder test.java.lang.invoke.lib.OldInstructionHelper\n@@ -65,1 +65,1 @@\n- * @build jdk.experimental.bytecode.BasicClassBuilder test.java.lang.invoke.lib.InstructionHelper\n+ * @build jdk.experimental.bytecode.BasicClassBuilder test.java.lang.invoke.lib.OldInstructionHelper\n@@ -82,1 +82,1 @@\n- * @build jdk.experimental.bytecode.BasicClassBuilder test.java.lang.invoke.lib.InstructionHelper\n+ * @build jdk.experimental.bytecode.BasicClassBuilder test.java.lang.invoke.lib.OldInstructionHelper\n@@ -99,1 +99,1 @@\n- * @build jdk.experimental.bytecode.BasicClassBuilder test.java.lang.invoke.lib.InstructionHelper\n+ * @build jdk.experimental.bytecode.BasicClassBuilder test.java.lang.invoke.lib.OldInstructionHelper\n@@ -117,1 +117,1 @@\n- * @build jdk.experimental.bytecode.BasicClassBuilder test.java.lang.invoke.lib.InstructionHelper\n+ * @build jdk.experimental.bytecode.BasicClassBuilder test.java.lang.invoke.lib.OldInstructionHelper\n@@ -376,1 +376,1 @@\n-            MethodHandle moveValueThroughStackAndLvt = InstructionHelper.loadCode(\n+            MethodHandle moveValueThroughStackAndLvt = OldInstructionHelper.loadCode(\n@@ -609,1 +609,1 @@\n-                InstructionHelper.loadCode(\n+                OldInstructionHelper.loadCode(\n@@ -650,1 +650,1 @@\n-                InstructionHelper.loadCode(LOOKUP, \"exerciseVBytecodeExprStackWithRefs\", mt,\n+                OldInstructionHelper.loadCode(LOOKUP, \"exerciseVBytecodeExprStackWithRefs\", mt,\n","filename":"test\/hotspot\/jtreg\/runtime\/valhalla\/inlinetypes\/InlineOops.java","additions":9,"deletions":9,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -50,1 +50,1 @@\n-import test.java.lang.invoke.lib.InstructionHelper;\n+import test.java.lang.invoke.lib.OldInstructionHelper;\n@@ -58,1 +58,1 @@\n- * @build jdk.experimental.bytecode.BasicClassBuilder test.java.lang.invoke.lib.InstructionHelper\n+ * @build jdk.experimental.bytecode.BasicClassBuilder test.java.lang.invoke.lib.OldInstructionHelper\n@@ -302,1 +302,1 @@\n-        MethodHandle fromExecStackToLocalVar = InstructionHelper.loadCode(\n+        MethodHandle fromExecStackToLocalVar = OldInstructionHelper.loadCode(\n@@ -346,1 +346,1 @@\n-        MethodHandle fromExecStackToFields = InstructionHelper.loadCode(\n+        MethodHandle fromExecStackToFields = OldInstructionHelper.loadCode(\n@@ -417,1 +417,1 @@\n-        MethodHandle fromExecStackToInlineArray = InstructionHelper.loadCode(\n+        MethodHandle fromExecStackToInlineArray = OldInstructionHelper.loadCode(\n","filename":"test\/hotspot\/jtreg\/runtime\/valhalla\/inlinetypes\/InlineTypesTest.java","additions":5,"deletions":5,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -27,1 +27,1 @@\n-import test.java.lang.invoke.lib.InstructionHelper;\n+import test.java.lang.invoke.lib.OldInstructionHelper;\n@@ -34,1 +34,1 @@\n- * @build jdk.experimental.bytecode.BasicClassBuilder test.java.lang.invoke.lib.InstructionHelper\n+ * @build jdk.experimental.bytecode.BasicClassBuilder test.java.lang.invoke.lib.OldInstructionHelper\n@@ -134,1 +134,1 @@\n-            InstructionHelper.loadCode(MethodHandles.lookup(),\n+            OldInstructionHelper.loadCode(MethodHandles.lookup(),\n","filename":"test\/hotspot\/jtreg\/runtime\/valhalla\/inlinetypes\/ObjectMethods.java","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -34,1 +34,1 @@\n-import test.java.lang.invoke.lib.InstructionHelper;\n+import test.java.lang.invoke.lib.OldInstructionHelper;\n@@ -42,1 +42,1 @@\n- * @build jdk.experimental.bytecode.BasicClassBuilder test.java.lang.invoke.lib.InstructionHelper\n+ * @build jdk.experimental.bytecode.BasicClassBuilder test.java.lang.invoke.lib.OldInstructionHelper\n@@ -66,1 +66,1 @@\n-        byte[] codeBytes = InstructionHelper.buildCode(LOOKUP, methodName, methodType,\n+        byte[] codeBytes = OldInstructionHelper.buildCode(LOOKUP, methodName, methodType,\n@@ -85,1 +85,1 @@\n-        InstructionHelper.loadCodeBytes(LOOKUP, methodName, methodType, codeBytes).invokeExact();\n+        OldInstructionHelper.loadCodeBytes(LOOKUP, methodName, methodType, codeBytes).invokeExact();\n@@ -95,1 +95,1 @@\n-        byte[] codeBytes = InstructionHelper.buildCode(LOOKUP, methodName, methodType,\n+        byte[] codeBytes = OldInstructionHelper.buildCode(LOOKUP, methodName, methodType,\n@@ -110,1 +110,1 @@\n-        InstructionHelper.loadCodeBytes(LOOKUP, methodName, methodType, codeBytes).invokeExact();\n+        OldInstructionHelper.loadCodeBytes(LOOKUP, methodName, methodType, codeBytes).invokeExact();\n@@ -122,1 +122,1 @@\n-        byte[] codeBytes = InstructionHelper.buildCode(LOOKUP, methodName, methodType,\n+        byte[] codeBytes = OldInstructionHelper.buildCode(LOOKUP, methodName, methodType,\n@@ -143,1 +143,1 @@\n-        InstructionHelper.loadCodeBytes(LOOKUP, methodName, methodType, codeBytes).invokeExact();\n+        OldInstructionHelper.loadCodeBytes(LOOKUP, methodName, methodType, codeBytes).invokeExact();\n","filename":"test\/hotspot\/jtreg\/runtime\/valhalla\/inlinetypes\/TestBytecodeLib.java","additions":8,"deletions":8,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -607,1 +607,2 @@\n-    com\/sun\/net\/httpserver\/simpleserver\/jwebserver\/CommandLinePortNotSpecifiedTest.java\n+    com\/sun\/net\/httpserver\/simpleserver\/jwebserver\/CommandLinePortNotSpecifiedTest.java \\\n+    javax\/xml\/jaxp\/datatype\/8033980\/GregorianCalAndDurSerDataUtil.java\n@@ -630,1 +631,2 @@\n-    sun\/security\/tools\/jarsigner\/compatibility\/Compatibility.java\n+    sun\/security\/tools\/jarsigner\/compatibility\/Compatibility.java \\\n+    sun\/security\/tools\/keytool\/ListKeyChainStore.java\n","filename":"test\/jdk\/TEST.groups","additions":4,"deletions":2,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -52,1 +52,4 @@\n-public class InstructionHelper {\n+\/**\n+ * 8308778: Temporarily keep the old bytecode API, but needs removing when Valhalla support is added to classfile API\n+ *\/\n+public class OldInstructionHelper {\n","filename":"test\/jdk\/java\/lang\/invoke\/common\/test\/java\/lang\/invoke\/lib\/OldInstructionHelper.java","additions":4,"deletions":1,"binary":false,"changes":5,"previous_filename":"test\/jdk\/java\/lang\/invoke\/common\/test\/java\/lang\/invoke\/lib\/InstructionHelper.java","status":"copied"},{"patch":"@@ -132,11 +132,0 @@\n-  private native int getConstantPoolCacheIndexTag0();\n-  public         int getConstantPoolCacheIndexTag() {\n-    return getConstantPoolCacheIndexTag0();\n-  }\n-\n-  private native int getConstantPoolCacheLength0(Class<?> aClass);\n-  public         int getConstantPoolCacheLength(Class<?> aClass) {\n-    Objects.requireNonNull(aClass);\n-    return getConstantPoolCacheLength0(aClass);\n-  }\n-\n@@ -186,0 +175,12 @@\n+  private native int getMethodEntriesLength0(Class<?> aClass);\n+  public         int getMethodEntriesLength(Class<?> aClass) {\n+    Objects.requireNonNull(aClass);\n+    return getMethodEntriesLength0(aClass);\n+  }\n+\n+  private native int getMethodCPIndex0(Class<?> aClass, int index);\n+  public         int getMethodCPIndex(Class<?> aClass, int index) {\n+    Objects.requireNonNull(aClass);\n+    return getMethodCPIndex0(aClass, index);\n+  }\n+\n","filename":"test\/lib\/jdk\/test\/whitebox\/WhiteBox.java","additions":12,"deletions":11,"binary":false,"changes":23,"status":"modified"}]}