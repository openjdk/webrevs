{"files":[{"patch":"@@ -2,1 +2,1 @@\n-\/\/ Copyright (c) 2003, 2024, Oracle and\/or its affiliates. All rights reserved.\n+\/\/ Copyright (c) 2003, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -2305,0 +2305,2 @@\n+    case Op_ConvI2HF:\n+    case Op_ConvL2HF:\n@@ -14550,0 +14552,12 @@\n+instruct convD2HF_reg_reg(iRegINoSp dst, vRegD src, vRegF tmp) %{\n+  match(Set dst (ConvD2HF src));\n+  format %{ \"fcvt $tmp, $src\\t# convert double to half precision\\n\\t\"\n+            \"smov $dst, $tmp\\t# move result from $tmp to $dst\"\n+  %}\n+  effect(TEMP tmp);\n+  ins_encode %{\n+    __ flt_to_flt16($dst$$Register, $src$$FloatRegister, $tmp$$FloatRegister, T_DOUBLE);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n@@ -14596,1 +14610,1 @@\n-      __ flt_to_flt16($dst$$Register, $src$$FloatRegister, $tmp$$FloatRegister);\n+    __ flt_to_flt16($dst$$Register, $src$$FloatRegister, $tmp$$FloatRegister, T_FLOAT);\n@@ -14660,0 +14674,13 @@\n+instruct convI2HF_reg_reg(iRegINoSp dst, iRegIorL2I src, vRegF tmp) %{\n+  match(Set dst (ConvI2HF src));\n+  effect(TEMP tmp);\n+  format %{ \"scvtf $tmp, $src\\t# convert integer to half precision\\n\\t\"\n+            \"smov $dst, $tmp\\t# move result from $tmp to $dst\"\n+  %}\n+  ins_encode %{\n+    __ scvtfwh($tmp$$FloatRegister, $src$$Register);\n+    __ smov($dst$$Register, $tmp$$FloatRegister, __ H, 0);\n+  %}\n+  ins_pipe(fp_i2f);\n+%}\n+\n@@ -14673,0 +14700,13 @@\n+instruct convL2HF_reg_reg(iRegINoSp dst, iRegL src, vRegF tmp) %{\n+  match(Set dst (ConvL2HF src));\n+  effect(TEMP tmp);\n+  format %{ \"scvtf $tmp, $src\\t# convert long to half precision\\n\\t\"\n+            \"smov $dst, $tmp\\t# move result from $tmp to $dst\"\n+  %}\n+  ins_encode %{\n+    __ scvtfxh($tmp$$FloatRegister, $src$$Register);\n+    __ smov($dst$$Register, $tmp$$FloatRegister, __ H, 0);\n+  %}\n+  ins_pipe(fp_l2f);\n+%}\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/aarch64.ad","additions":42,"deletions":2,"binary":false,"changes":44,"status":"modified"},{"patch":"@@ -2,2 +2,2 @@\n-\/\/ Copyright (c) 2020, 2024, Oracle and\/or its affiliates. All rights reserved.\n-\/\/ Copyright (c) 2020, 2024, Arm Limited. All rights reserved.\n+\/\/ Copyright (c) 2020, 2025, Oracle and\/or its affiliates. All rights reserved.\n+\/\/ Copyright (c) 2020, 2025, Arm Limited. All rights reserved.\n@@ -244,0 +244,1 @@\n+      case Op_VectorCastI2HF:\n@@ -4256,0 +4257,27 @@\n+\/\/ VectorCastI2HF\n+instruct vcvtItoHF_neon(vReg dst, vReg src) %{\n+  predicate(VM_Version::use_neon_for_vector(Matcher::vector_length_in_bytes(n->in(1))));\n+  match(Set dst (VectorCastI2HF src));\n+  effect(TEMP_DEF dst);\n+  format %{ \"vcvtItoHF_neon $dst, $src\" %}\n+  ins_encode %{\n+    \/\/ 4I to 4HF\n+    __ scvtfv(__ T4S, $dst$$FloatRegister, $src$$FloatRegister);\n+    __ fcvtn($dst$$FloatRegister, __ T4H, $dst$$FloatRegister, __ T4S);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vcvtItoHF_sve(vReg dst, vReg src, vReg tmp) %{\n+  predicate(!VM_Version::use_neon_for_vector(Matcher::vector_length_in_bytes(n->in(1))));\n+  match(Set dst (VectorCastI2HF src));\n+  effect(TEMP_DEF dst, TEMP tmp);\n+  format %{ \"vcvtItoHF_sve $dst, $src\" %}\n+  ins_encode %{\n+    \/\/ I to HF\n+    __ sve_scvtf($dst$$FloatRegister, __ H, ptrue, $src$$FloatRegister, __ S);\n+    __ sve_vector_narrow($dst$$FloatRegister, __ H, $dst$$FloatRegister, __ S, $tmp$$FloatRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n@@ -4272,0 +4300,14 @@\n+\/\/ VectorCastL2HF\n+instruct vcvtLtoHF(vReg dst, vReg src, vReg tmp) %{\n+  match(Set dst (VectorCastL2HF src));\n+  effect(TEMP_DEF dst, TEMP tmp);\n+  format %{ \"vcvtLtoHF $dst, $src\\t# KILL $tmp\" %}\n+  ins_encode %{\n+    assert(UseSVE > 0, \"must be sve\");\n+    __ sve_scvtf($dst$$FloatRegister, __ H, ptrue, $src$$FloatRegister, __ D);\n+    __ sve_vector_narrow($dst$$FloatRegister, __ H,\n+                         $dst$$FloatRegister, __ D, $tmp$$FloatRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n@@ -4492,0 +4534,14 @@\n+\/\/ VectorCastD2HF\n+instruct vcvtDtoHF(vReg dst, vReg src, vReg tmp) %{\n+  match(Set dst (VectorCastD2HF src));\n+  effect(TEMP_DEF dst, TEMP tmp);\n+  format %{ \"vcvtDtoHF $dst, $src\\t# vector > 128 bits. KILL $tmp\" %}\n+  ins_encode %{\n+    assert(UseSVE > 0, \"must be sve\");\n+    __ sve_fcvt($dst$$FloatRegister, __ H, ptrue, $src$$FloatRegister, __ D);\n+    __ sve_vector_narrow($dst$$FloatRegister, __ H,\n+                         $dst$$FloatRegister, __ D, $tmp$$FloatRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/aarch64_vector.ad","additions":58,"deletions":2,"binary":false,"changes":60,"status":"modified"},{"patch":"@@ -2,2 +2,2 @@\n-\/\/ Copyright (c) 2020, 2024, Oracle and\/or its affiliates. All rights reserved.\n-\/\/ Copyright (c) 2020, 2024, Arm Limited. All rights reserved.\n+\/\/ Copyright (c) 2020, 2025, Oracle and\/or its affiliates. All rights reserved.\n+\/\/ Copyright (c) 2020, 2025, Arm Limited. All rights reserved.\n@@ -234,0 +234,1 @@\n+      case Op_VectorCastI2HF:\n@@ -2562,0 +2563,27 @@\n+\/\/ VectorCastI2HF\n+instruct vcvtItoHF_neon(vReg dst, vReg src) %{\n+  predicate(VM_Version::use_neon_for_vector(Matcher::vector_length_in_bytes(n->in(1))));\n+  match(Set dst (VectorCastI2HF src));\n+  effect(TEMP_DEF dst);\n+  format %{ \"vcvtItoHF_neon $dst, $src\" %}\n+  ins_encode %{\n+    \/\/ 4I to 4HF\n+    __ scvtfv(__ T4S, $dst$$FloatRegister, $src$$FloatRegister);\n+    __ fcvtn($dst$$FloatRegister, __ T4H, $dst$$FloatRegister, __ T4S);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vcvtItoHF_sve(vReg dst, vReg src, vReg tmp) %{\n+  predicate(!VM_Version::use_neon_for_vector(Matcher::vector_length_in_bytes(n->in(1))));\n+  match(Set dst (VectorCastI2HF src));\n+  effect(TEMP_DEF dst, TEMP tmp);\n+  format %{ \"vcvtItoHF_sve $dst, $src\" %}\n+  ins_encode %{\n+    \/\/ I to HF\n+    __ sve_scvtf($dst$$FloatRegister, __ H, ptrue, $src$$FloatRegister, __ S);\n+    __ sve_vector_narrow($dst$$FloatRegister, __ H, $dst$$FloatRegister, __ S, $tmp$$FloatRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n@@ -2578,0 +2606,14 @@\n+\/\/ VectorCastL2HF\n+instruct vcvtLtoHF(vReg dst, vReg src, vReg tmp) %{\n+  match(Set dst (VectorCastL2HF src));\n+  effect(TEMP_DEF dst, TEMP tmp);\n+  format %{ \"vcvtLtoHF $dst, $src\\t# KILL $tmp\" %}\n+  ins_encode %{\n+    assert(UseSVE > 0, \"must be sve\");\n+    __ sve_scvtf($dst$$FloatRegister, __ H, ptrue, $src$$FloatRegister, __ D);\n+    __ sve_vector_narrow($dst$$FloatRegister, __ H,\n+                         $dst$$FloatRegister, __ D, $tmp$$FloatRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n@@ -2798,0 +2840,14 @@\n+\/\/ VectorCastD2HF\n+instruct vcvtDtoHF(vReg dst, vReg src, vReg tmp) %{\n+  match(Set dst (VectorCastD2HF src));\n+  effect(TEMP_DEF dst, TEMP tmp);\n+  format %{ \"vcvtDtoHF $dst, $src\\t# vector > 128 bits. KILL $tmp\" %}\n+  ins_encode %{\n+    assert(UseSVE > 0, \"must be sve\");\n+    __ sve_fcvt($dst$$FloatRegister, __ H, ptrue, $src$$FloatRegister, __ D);\n+    __ sve_vector_narrow($dst$$FloatRegister, __ H,\n+                         $dst$$FloatRegister, __ D, $tmp$$FloatRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/aarch64_vector_ad.m4","additions":58,"deletions":2,"binary":false,"changes":60,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -2017,0 +2017,1 @@\n+  INSN(fcvtdh, 0b01, 0b000111);   \/\/ Double-precision to half-precision\n@@ -2207,0 +2208,3 @@\n+  INSN(scvtfwh, 0b0, 0b11, 0b00, 0b010); \/\/ 32-bit to half-precision\n+  INSN(scvtfxh, 0b1, 0b11, 0b00, 0b010); \/\/ 64-bit to half-precision\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/assembler_aarch64.hpp","additions":5,"deletions":1,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2000, 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2000, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -1936,1 +1936,1 @@\n-  case lir_f2hf: __ flt_to_flt16(dest->as_register(), value->as_float_reg(), tmp->as_float_reg()); break;\n+  case lir_f2hf: __ flt_to_flt16(dest->as_register(), value->as_float_reg(), tmp->as_float_reg(), T_FLOAT); break;\n","filename":"src\/hotspot\/cpu\/aarch64\/c1_LIRAssembler_aarch64.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -532,3 +532,7 @@\n-  \/\/ Convert float to half-precision float\n-  void flt_to_flt16(Register dst, FloatRegister src, FloatRegister tmp) {\n-    fcvtsh(tmp, src);\n+  \/\/ Convert float\/double to half-precision float\n+  void flt_to_flt16(Register dst, FloatRegister src, FloatRegister tmp, BasicType bt) {\n+    switch (bt) {\n+      case T_FLOAT:  fcvtsh(tmp, src); break;\n+      case T_DOUBLE: fcvtdh(tmp, src); break;\n+      default: ShouldNotReachHere();\n+    }\n@@ -542,3 +546,3 @@\n-    case T_FLOAT:  fcvths(dst, tmp); break;\n-    case T_DOUBLE: fcvthd(dst, tmp); break;\n-    default: ShouldNotReachHere();\n+      case T_FLOAT:  fcvths(dst, tmp); break;\n+      case T_DOUBLE: fcvthd(dst, tmp); break;\n+      default: ShouldNotReachHere();\n@@ -552,3 +556,3 @@\n-    case T_INT:  fcvtzshw(dst, tmp); break;\n-    case T_LONG: fcvtzshx(dst, tmp); break;\n-    default: ShouldNotReachHere();\n+      case T_INT:  fcvtzshw(dst, tmp); break;\n+      case T_LONG: fcvtzshx(dst, tmp); break;\n+      default: ShouldNotReachHere();\n","filename":"src\/hotspot\/cpu\/aarch64\/macroAssembler_aarch64.hpp","additions":14,"deletions":10,"binary":false,"changes":24,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2003, 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2003, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -5537,1 +5537,1 @@\n-    __ flt_to_flt16(r0, v0, v1);\n+    __ flt_to_flt16(r0, v0, v1, T_FLOAT);\n","filename":"src\/hotspot\/cpu\/aarch64\/stubGenerator_aarch64.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2003, 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2003, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -334,1 +334,1 @@\n-  __ flt_to_flt16(c_rarg0, v0, v1);\n+  __ flt_to_flt16(c_rarg0, v0, v1, T_FLOAT);\n","filename":"src\/hotspot\/cpu\/aarch64\/templateInterpreterGenerator_aarch64.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1998, 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1998, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -4361,2 +4361,2 @@\n-    \"VectorCastB2X\", \"VectorCastS2X\", \"VectorCastI2X\",\n-    \"VectorCastL2X\", \"VectorCastF2X\", \"VectorCastD2X\", \"VectorCastF2HF\", \"VectorCastHF2X\",\n+    \"VectorCastB2X\", \"VectorCastS2X\", \"VectorCastI2X\", \"VectorCastI2HF\",\n+    \"VectorCastL2X\", \"VectorCastL2HF\", \"VectorCastF2X\", \"VectorCastD2X\", \"VectorCastD2HF\", \"VectorCastF2HF\", \"VectorCastHF2X\",\n","filename":"src\/hotspot\/share\/adlc\/formssel.cpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2020, 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2020, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -258,0 +258,6 @@\n+  do_intrinsic(_doubleToFloat16,        java_lang_Float16,      doubleToFloat16_name,     double_f16_signature, F_S)    \\\n+   do_name(    doubleToFloat16_name,                             \"doubleToFloat16\")                                     \\\n+   do_signature(double_f16_signature,                            \"(D)S\")                                                \\\n+  do_intrinsic(_longToFloat16,          java_lang_Float16,      longToFloat16_name,       long_f16_signature,   F_S)    \\\n+   do_name(    longToFloat16_name,                               \"longToFloat16\")                                       \\\n+   do_signature(long_f16_signature,                              \"(J)S\")                                                \\\n","filename":"src\/hotspot\/share\/classfile\/vmIntrinsics.hpp","additions":7,"deletions":1,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1999, 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1999, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -357,0 +357,6 @@\n+  case vmIntrinsics::_doubleToFloat16:\n+    if (!Matcher::match_rule_supported(Op_ConvD2HF)) return false;\n+    break;\n+  case vmIntrinsics::_longToFloat16:\n+    if (!Matcher::match_rule_supported(Op_ConvL2HF)) return false;\n+    break;\n","filename":"src\/hotspot\/share\/opto\/c2compiler.cpp","additions":7,"deletions":1,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -1898,0 +1898,24 @@\n+static bool is_HF2X_op(int opcode) {\n+  switch(opcode) {\n+    case Op_ConvHF2F:\n+    case Op_ConvHF2I:\n+    case Op_ConvHF2D:\n+    case Op_ConvHF2L:\n+      return true;\n+    default:\n+      return false;\n+    }\n+}\n+\n+static bool is_X2HF_op(int opcode) {\n+  switch(opcode) {\n+    case Op_ConvF2HF:\n+    case Op_ConvI2HF:\n+    case Op_ConvD2HF:\n+    case Op_ConvL2HF:\n+      return true;\n+    default:\n+      return false;\n+    }\n+}\n+\n@@ -1902,3 +1926,1 @@\n-    if (is_source && convert_op == Op_ConvHF2I) {\n-      return T_SHORT;\n-    } else if (convert_op == Op_ConvHF2F || convert_op == Op_ConvF2HF || convert_op == Op_ConvHF2D || convert_op == Op_ConvHF2L) {\n+    if ((is_source && is_HF2X_op(convert_op)) || (!is_source && is_X2HF_op(convert_op))) {\n","filename":"src\/hotspot\/share\/opto\/cfgnode.cpp","additions":26,"deletions":4,"binary":false,"changes":30,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -143,0 +143,1 @@\n+macro(ConvD2HF)\n@@ -151,0 +152,1 @@\n+macro(ConvI2HF)\n@@ -154,0 +156,1 @@\n+macro(ConvL2HF)\n@@ -532,0 +535,1 @@\n+macro(VectorCastI2HF)\n@@ -533,0 +537,1 @@\n+macro(VectorCastL2HF)\n@@ -535,0 +540,1 @@\n+macro(VectorCastD2HF)\n","filename":"src\/hotspot\/share\/opto\/classes.hpp","additions":7,"deletions":1,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2014, 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2014, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -111,0 +111,2 @@\n+    } else if (target == T_SHORT) {\n+      return new ConvI2HFNode(input);\n@@ -119,0 +121,2 @@\n+    } else if (target == T_SHORT) {\n+      return new ConvL2HFNode(input);\n@@ -137,0 +141,2 @@\n+    } else if (target == T_SHORT) {\n+      return new ConvD2HFNode(input);\n@@ -187,0 +193,17 @@\n+\/\/=============================================================================\n+\/\/------------------------------Ideal------------------------------------------\n+Node* ConvD2HFNode::Ideal(PhaseGVN *phase, bool can_reshape) {\n+  \/\/ Optimize pattern ConvI2D -> ConvD2HF ==> ConvI2HF\n+  if (in(1)->Opcode() == Op_ConvI2D && Matcher::match_rule_supported(Op_ConvI2HF)) {\n+    return new ConvI2HFNode(in(1)->in(1));\n+  }\n+  return nullptr;\n+}\n+\n+\/\/------------------------------Identity---------------------------------------\n+\/\/ Half-Float's can be converted to doubles with no loss of precision.  Hence\n+\/\/ converting a half float to a double and back to a half float is a NOP.\n+Node* ConvD2HFNode::Identity(PhaseGVN* phase) {\n+  return (in(1)->Opcode() == Op_ConvHF2D) ? in(1)->in(1) : this;\n+}\n+\n","filename":"src\/hotspot\/share\/opto\/convertnode.cpp","additions":24,"deletions":1,"binary":false,"changes":25,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2014, 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2014, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -73,0 +73,11 @@\n+\/\/------------------------------ConvD2HFNode-----------------------------------\n+\/\/ Convert double to half-precision float\n+class ConvD2HFNode : public ConvertNode {\n+  public:\n+  ConvD2HFNode(Node* in1) : ConvertNode(TypeInt::SHORT, in1) {}\n+  virtual int Opcode() const;\n+  virtual const Type* in_type() const { return Type::DOUBLE; }\n+  virtual Node* Identity(PhaseGVN* phase);\n+  virtual Node* Ideal(PhaseGVN* phase, bool can_reshape);\n+};\n+\n@@ -208,0 +219,9 @@\n+\/\/------------------------------ConvI2HFNode-----------------------------------\n+\/\/ Convert Integer to Half float\n+class ConvI2HFNode : public ConvertNode {\n+  public:\n+  ConvI2HFNode(Node* in1) : ConvertNode(TypeInt::SHORT, in1) {}\n+  virtual int Opcode() const;\n+  virtual const Type* in_type() const { return TypeInt::INT; }\n+};\n+\n@@ -263,0 +283,9 @@\n+\/\/------------------------------ConvL2HFNode-----------------------------------\n+\/\/ Convert Long to Half float\n+class ConvL2HFNode : public ConvertNode {\n+  public:\n+  ConvL2HFNode(Node* in1) : ConvertNode(TypeInt::SHORT, in1) {}\n+  virtual int Opcode() const;\n+  virtual const Type* in_type() const { return TypeLong::LONG; }\n+};\n+\n","filename":"src\/hotspot\/share\/opto\/convertnode.hpp","additions":30,"deletions":1,"binary":false,"changes":31,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1999, 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1999, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -548,1 +548,3 @@\n-  case vmIntrinsics::_float16ToFloat:           return inline_fp_conversions(intrinsic_id());\n+  case vmIntrinsics::_float16ToFloat:\n+  case vmIntrinsics::_doubleToFloat16:\n+  case vmIntrinsics::_longToFloat16:            return inline_fp_conversions(intrinsic_id());\n@@ -5152,0 +5154,2 @@\n+  case vmIntrinsics::_doubleToFloat16:      result = new ConvD2HFNode(arg); break;\n+  case vmIntrinsics::_longToFloat16:        result = new ConvL2HFNode(arg); break;\n","filename":"src\/hotspot\/share\/opto\/library_call.cpp","additions":6,"deletions":2,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2007, 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2007, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -576,0 +576,3 @@\n+    case Op_ConvI2HF:\n+    case Op_ConvD2HF:\n+    case Op_ConvL2HF:\n@@ -617,0 +620,3 @@\n+  case Op_ConvI2HF:\n+  case Op_ConvD2HF:\n+  case Op_ConvL2HF:\n@@ -1441,0 +1447,4 @@\n+    case Op_VectorCastI2HF: return new VectorCastI2HFNode(n1, vt);\n+    case Op_VectorCastD2HF: return new VectorCastD2HFNode(n1, vt);\n+    case Op_VectorCastL2HF: return new VectorCastL2HFNode(n1, vt);\n+\n@@ -1461,0 +1471,9 @@\n+    case Op_ConvI2HF:\n+      assert(bt == T_INT, \"\");\n+      return Op_VectorCastI2HF;\n+    case Op_ConvD2HF:\n+      assert(bt == T_DOUBLE, \"\");\n+      return Op_VectorCastD2HF;\n+    case Op_ConvL2HF:\n+      assert(bt == T_LONG, \"\");\n+      return Op_VectorCastL2HF;\n","filename":"src\/hotspot\/share\/opto\/vectornode.cpp","additions":20,"deletions":1,"binary":false,"changes":21,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2007, 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2007, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -1714,0 +1714,8 @@\n+class VectorCastI2HFNode : public VectorCastNode {\n+ public:\n+  VectorCastI2HFNode(Node* in, const TypeVect* vt) : VectorCastNode(in, vt) {\n+    assert(in->bottom_type()->is_vect()->element_basic_type() == T_INT, \"must be int\");\n+  }\n+  virtual int Opcode() const;\n+};\n+\n@@ -1722,0 +1730,8 @@\n+class VectorCastL2HFNode : public VectorCastNode {\n+ public:\n+  VectorCastL2HFNode(Node* in, const TypeVect* vt) : VectorCastNode(in, vt) {\n+    assert(in->bottom_type()->is_vect()->element_basic_type() == T_LONG, \"must be long\");\n+  }\n+  virtual int Opcode() const;\n+};\n+\n@@ -1736,0 +1752,8 @@\n+};\n+\n+class VectorCastD2HFNode : public VectorCastNode {\n+ public:\n+  VectorCastD2HFNode(Node* in, const TypeVect* vt) : VectorCastNode(in, vt) {\n+    assert(in->bottom_type()->is_vect()->element_basic_type() == T_DOUBLE, \"must be double\");\n+  }\n+  virtual int Opcode() const;\n","filename":"src\/hotspot\/share\/opto\/vectornode.hpp","additions":25,"deletions":1,"binary":false,"changes":26,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2000, 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2000, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -1491,0 +1491,1 @@\n+  declare_c2_type(ConvD2HFNode, Node)                                     \\\n@@ -1499,0 +1500,1 @@\n+  declare_c2_type(ConvI2HFNode, Node)                                     \\\n@@ -1502,0 +1504,1 @@\n+  declare_c2_type(ConvL2HFNode, Node)                                     \\\n@@ -1816,0 +1819,1 @@\n+  declare_c2_type(VectorCastI2HFNode, VectorNode)                         \\\n@@ -1817,0 +1821,1 @@\n+  declare_c2_type(VectorCastL2HFNode, VectorNode)                         \\\n@@ -1819,0 +1824,1 @@\n+  declare_c2_type(VectorCastD2HFNode, VectorNode)                         \\\n","filename":"src\/hotspot\/share\/runtime\/vmStructs.cpp","additions":7,"deletions":1,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2023, 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2023, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -387,0 +387,5 @@\n+        return new Float16(longToFloat16(value));\n+    }\n+\n+    @IntrinsicCandidate\n+    private static short longToFloat16(long value) {\n@@ -388,1 +393,1 @@\n-            return NEGATIVE_INFINITY;\n+            return float16ToRawShortBits(NEGATIVE_INFINITY);\n@@ -391,1 +396,1 @@\n-                return POSITIVE_INFINITY;\n+                return float16ToRawShortBits(POSITIVE_INFINITY);\n@@ -396,1 +401,1 @@\n-            return valueOf((float)value);\n+            return Float.floatToFloat16((float)value);\n@@ -414,1 +419,1 @@\n-   \/**\n+    \/**\n@@ -424,1 +429,6 @@\n-    public static Float16 valueOf(double d) {\n+        public static Float16 valueOf(double d) {\n+        return new Float16(doubleToFloat16(d));\n+    }\n+\n+    @IntrinsicCandidate\n+    private static short doubleToFloat16(double d) {\n@@ -432,1 +442,1 @@\n-            return valueOf((float)d);\n+            return Float.floatToFloat16((float)d);\n@@ -440,1 +450,1 @@\n-            return new Float16((short)(sign_bit | 0x7c00));\n+            return (short)(sign_bit | 0x7c00);\n@@ -446,1 +456,1 @@\n-            return new Float16(sign_bit); \/\/ Positive or negative zero\n+            return sign_bit; \/\/ Positive or negative zero\n@@ -497,1 +507,1 @@\n-        return new Float16((short)(sign_bit | ( ((exp + 15) << 10) + signif_bits ) ));\n+        return (short)(sign_bit | ( ((exp + 15) << 10) + signif_bits ) );\n","filename":"src\/java.base\/share\/classes\/java\/lang\/Float16.java","additions":20,"deletions":10,"binary":false,"changes":30,"status":"modified"},{"patch":"@@ -1048,1 +1048,1 @@\n-        if (self._name in [\"fcvtsh\", \"fcvths\", \"fcvthd\"]):\n+        if (self._name in [\"fcvtsh\", \"fcvths\", \"fcvthd\", \"fcvtdh\"]):\n@@ -1611,1 +1611,1 @@\n-          [\"fcvtd\", \"sd\"], [\"fabsh\", \"hh\"], [\"fnegh\", \"hh\"], [\"fsqrth\", \"hh\"],\n+          [\"fcvtd\", \"sd\"], [\"fcvtdh\", \"hd\"], [\"fabsh\", \"hh\"], [\"fnegh\", \"hh\"], [\"fsqrth\", \"hh\"],\n@@ -1619,0 +1619,1 @@\n+                          [\"scvtfwh\", \"scvtf\", \"hw\"], [\"scvtfxh\", \"scvtf\", \"hx\"],\n","filename":"test\/hotspot\/gtest\/aarch64\/aarch64-asmtest.py","additions":3,"deletions":2,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -534,3 +534,4 @@\n-    __ fabsh(v22, v17);                                \/\/       fabs    h22, h17\n-    __ fnegh(v13, v19);                                \/\/       fneg    h13, h19\n-    __ fsqrth(v19, v27);                               \/\/       fsqrt   h19, h27\n+    __ fcvtdh(v22, v17);                               \/\/       fcvt    h22, d17\n+    __ fabsh(v13, v19);                                \/\/       fabs    h13, h19\n+    __ fnegh(v19, v27);                                \/\/       fneg    h19, h27\n+    __ fsqrth(v17, v6);                                \/\/       fsqrt   h17, h6\n@@ -539,18 +540,20 @@\n-    __ fcvtzsw(r17, v6);                               \/\/       fcvtzs  w17, s6\n-    __ fcvtzs(r13, v7);                                \/\/       fcvtzs  x13, s7\n-    __ fcvtzdw(r28, v26);                              \/\/       fcvtzs  w28, d26\n-    __ fcvtzd(r17, v6);                                \/\/       fcvtzs  x17, d6\n-    __ fcvtzshw(r1, v4);                               \/\/       fcvtzs  w1, h4\n-    __ fcvtzshx(r13, v20);                             \/\/       fcvtzs  x13, h20\n-    __ scvtfws(v6, r21);                               \/\/       scvtf   s6, w21\n-    __ scvtfs(v26, r23);                               \/\/       scvtf   s26, x23\n-    __ scvtfwd(v13, r20);                              \/\/       scvtf   d13, w20\n-    __ scvtfd(v30, r27);                               \/\/       scvtf   d30, x27\n-    __ fcvtassw(r10, v21);                             \/\/       fcvtas  w10, s21\n-    __ fcvtasd(r5, v17);                               \/\/       fcvtas  x5, d17\n-    __ fcvtmssw(r11, v13);                             \/\/       fcvtms  w11, s13\n-    __ fcvtmsd(r13, v20);                              \/\/       fcvtms  x13, d20\n-    __ fmovs(r26, v14);                                \/\/       fmov    w26, s14\n-    __ fmovd(r4, v23);                                 \/\/       fmov    x4, d23\n-    __ fmovs(v23, r29);                                \/\/       fmov    s23, w29\n-    __ fmovd(v12, r14);                                \/\/       fmov    d12, x14\n+    __ fcvtzsw(r13, v7);                               \/\/       fcvtzs  w13, s7\n+    __ fcvtzs(r28, v26);                               \/\/       fcvtzs  x28, s26\n+    __ fcvtzdw(r17, v6);                               \/\/       fcvtzs  w17, d6\n+    __ fcvtzd(r1, v4);                                 \/\/       fcvtzs  x1, d4\n+    __ fcvtzshw(r13, v20);                             \/\/       fcvtzs  w13, h20\n+    __ fcvtzshx(r6, v21);                              \/\/       fcvtzs  x6, h21\n+    __ scvtfws(v26, r23);                              \/\/       scvtf   s26, w23\n+    __ scvtfs(v13, r20);                               \/\/       scvtf   s13, x20\n+    __ scvtfwd(v30, r27);                              \/\/       scvtf   d30, w27\n+    __ scvtfd(v10, r21);                               \/\/       scvtf   d10, x21\n+    __ scvtfwh(v5, r17);                               \/\/       scvtf   h5, w17\n+    __ scvtfxh(v11, r13);                              \/\/       scvtf   h11, x13\n+    __ fcvtassw(r13, v20);                             \/\/       fcvtas  w13, s20\n+    __ fcvtasd(r26, v14);                              \/\/       fcvtas  x26, d14\n+    __ fcvtmssw(r4, v23);                              \/\/       fcvtms  w4, s23\n+    __ fcvtmsd(r23, v29);                              \/\/       fcvtms  x23, d29\n+    __ fmovs(r12, v14);                                \/\/       fmov    w12, s14\n+    __ fmovd(r16, v27);                                \/\/       fmov    x16, d27\n+    __ fmovs(v21, r0);                                 \/\/       fmov    s21, w0\n+    __ fmovd(v6, r26);                                 \/\/       fmov    d6, x26\n@@ -559,4 +562,4 @@\n-    __ fcmps(v16, v27);                                \/\/       fcmp    s16, s27\n-    __ fcmpd(v21, v0);                                 \/\/       fcmp    d21, d0\n-    __ fcmps(v6, 0.0);                                 \/\/       fcmp    s6, #0.0\n-    __ fcmpd(v26, 0.0);                                \/\/       fcmp    d26, #0.0\n+    __ fcmps(v17, v27);                                \/\/       fcmp    s17, s27\n+    __ fcmpd(v12, v6);                                 \/\/       fcmp    d12, d6\n+    __ fcmps(v0, 0.0);                                 \/\/       fcmp    s0, #0.0\n+    __ fcmpd(v30, 0.0);                                \/\/       fcmp    d30, #0.0\n@@ -565,5 +568,5 @@\n-    __ stpw(r27, r12, Address(r6, -32));               \/\/       stp     w27, w12, [x6, #-32]\n-    __ ldpw(r14, r11, Address(r19, -256));             \/\/       ldp     w14, w11, [x19, #-256]\n-    __ ldpsw(r0, r12, Address(r15, -48));              \/\/       ldpsw   x0, x12, [x15, #-48]\n-    __ stp(r9, r23, Address(r23, 32));                 \/\/       stp     x9, x23, [x23, #32]\n-    __ ldp(r15, r4, Address(r26, -176));               \/\/       ldp     x15, x4, [x26, #-176]\n+    __ stpw(r19, r15, Address(r11, -208));             \/\/       stp     w19, w15, [x11, #-208]\n+    __ ldpw(r0, r12, Address(r15, -48));               \/\/       ldp     w0, w12, [x15, #-48]\n+    __ ldpsw(r7, r30, Address(r23, 32));               \/\/       ldpsw   x7, x30, [x23, #32]\n+    __ stp(r26, r15, Address(r4, -256));               \/\/       stp     x26, x15, [x4, #-256]\n+    __ ldp(r6, r11, Address(r25, -160));               \/\/       ldp     x6, x11, [x25, #-160]\n@@ -572,5 +575,5 @@\n-    __ stpw(r17, r8, Address(__ pre(r6, -160)));       \/\/       stp     w17, w8, [x6, #-160]!\n-    __ ldpw(r7, r2, Address(__ pre(r4, -112)));        \/\/       ldp     w7, w2, [x4, #-112]!\n-    __ ldpsw(r14, r9, Address(__ pre(r22, -16)));      \/\/       ldpsw   x14, x9, [x22, #-16]!\n-    __ stp(r13, r20, Address(__ pre(r24, -256)));      \/\/       stp     x13, x20, [x24, #-256]!\n-    __ ldp(r8, r11, Address(__ pre(r10, 96)));         \/\/       ldp     x8, x11, [x10, #96]!\n+    __ stpw(r10, r25, Address(__ pre(r2, -64)));       \/\/       stp     w10, w25, [x2, #-64]!\n+    __ ldpw(r30, r14, Address(__ pre(r9, -208)));      \/\/       ldp     w30, w14, [x9, #-208]!\n+    __ ldpsw(r29, r23, Address(__ pre(r27, 32)));      \/\/       ldpsw   x29, x23, [x27, #32]!\n+    __ stp(r13, r0, Address(__ pre(r11, -224)));       \/\/       stp     x13, x0, [x11, #-224]!\n+    __ ldp(r25, r16, Address(__ pre(r10, -128)));      \/\/       ldp     x25, x16, [x10, #-128]!\n@@ -579,5 +582,5 @@\n-    __ stpw(r24, r5, Address(__ post(r16, -112)));     \/\/       stp     w24, w5, [x16], #-112\n-    __ ldpw(r0, r26, Address(__ post(r9, -128)));      \/\/       ldp     w0, w26, [x9], #-128\n-    __ ldpsw(r24, r2, Address(__ post(r17, -128)));    \/\/       ldpsw   x24, x2, [x17], #-128\n-    __ stp(r26, r17, Address(__ post(r14, -48)));      \/\/       stp     x26, x17, [x14], #-48\n-    __ ldp(r30, r21, Address(__ post(r29, 48)));       \/\/       ldp     x30, x21, [x29], #48\n+    __ stpw(r5, r15, Address(__ post(r27, -48)));      \/\/       stp     w5, w15, [x27], #-48\n+    __ ldpw(r12, r24, Address(__ post(r2, -128)));     \/\/       ldp     w12, w24, [x2], #-128\n+    __ ldpsw(r14, r15, Address(__ post(r24, -32)));    \/\/       ldpsw   x14, x15, [x24], #-32\n+    __ stp(r17, r21, Address(__ post(r22, -128)));     \/\/       stp     x17, x21, [x22], #-128\n+    __ ldp(r10, r8, Address(__ post(r30, 128)));       \/\/       ldp     x10, x8, [x30], #128\n@@ -586,4 +589,4 @@\n-    __ stnpw(r17, r23, Address(r10, -112));            \/\/       stnp    w17, w23, [x10, #-112]\n-    __ ldnpw(r26, r6, Address(r30, -160));             \/\/       ldnp    w26, w6, [x30, #-160]\n-    __ stnp(r30, r8, Address(r20, 64));                \/\/       stnp    x30, x8, [x20, #64]\n-    __ ldnp(r22, r29, Address(r9, 48));                \/\/       ldnp    x22, x29, [x9, #48]\n+    __ stnpw(r30, r26, Address(r6, -128));             \/\/       stnp    w30, w26, [x6, #-128]\n+    __ ldnpw(r24, r2, Address(r20, 64));               \/\/       ldnp    w24, w2, [x20, #64]\n+    __ stnp(r9, r21, Address(r29, 64));                \/\/       stnp    x9, x21, [x29, #64]\n+    __ ldnp(r22, r30, Address(r3, -256));              \/\/       ldnp    x22, x30, [x3, #-256]\n@@ -592,22 +595,22 @@\n-    __ ld1(v12, __ T8B, Address(r1));                  \/\/       ld1     {v12.8B}, [x1]\n-    __ ld1(v0, v1, __ T16B, Address(__ post(r16, 32))); \/\/      ld1     {v0.16B, v1.16B}, [x16], 32\n-    __ ld1(v21, v22, v23, __ T1D, Address(__ post(r25, r19))); \/\/       ld1     {v21.1D, v22.1D, v23.1D}, [x25], x19\n-    __ ld1(v25, v26, v27, v28, __ T8H, Address(__ post(r26, 64))); \/\/   ld1     {v25.8H, v26.8H, v27.8H, v28.8H}, [x26], 64\n-    __ ld1r(v9, __ T8B, Address(r15));                 \/\/       ld1r    {v9.8B}, [x15]\n-    __ ld1r(v12, __ T4S, Address(__ post(r25, 4)));    \/\/       ld1r    {v12.4S}, [x25], 4\n-    __ ld1r(v4, __ T1D, Address(__ post(r14, r29)));   \/\/       ld1r    {v4.1D}, [x14], x29\n-    __ ld2(v17, v18, __ T2D, Address(r17));            \/\/       ld2     {v17.2D, v18.2D}, [x17]\n-    __ ld2(v22, v23, __ T4H, Address(__ post(r27, 16))); \/\/     ld2     {v22.4H, v23.4H}, [x27], 16\n-    __ ld2r(v2, v3, __ T16B, Address(r24));            \/\/       ld2r    {v2.16B, v3.16B}, [x24]\n-    __ ld2r(v10, v11, __ T2S, Address(__ post(r0, 8))); \/\/      ld2r    {v10.2S, v11.2S}, [x0], 8\n-    __ ld2r(v2, v3, __ T2D, Address(__ post(r12, r22))); \/\/     ld2r    {v2.2D, v3.2D}, [x12], x22\n-    __ ld3(v20, v21, v22, __ T4S, Address(__ post(r13, r13))); \/\/       ld3     {v20.4S, v21.4S, v22.4S}, [x13], x13\n-    __ ld3(v12, v13, v14, __ T2S, Address(r11));       \/\/       ld3     {v12.2S, v13.2S, v14.2S}, [x11]\n-    __ ld3r(v17, v18, v19, __ T8H, Address(r14));      \/\/       ld3r    {v17.8H, v18.8H, v19.8H}, [x14]\n-    __ ld3r(v25, v26, v27, __ T4S, Address(__ post(r21, 12))); \/\/       ld3r    {v25.4S, v26.4S, v27.4S}, [x21], 12\n-    __ ld3r(v20, v21, v22, __ T1D, Address(__ post(r16, r27))); \/\/      ld3r    {v20.1D, v21.1D, v22.1D}, [x16], x27\n-    __ ld4(v0, v1, v2, v3, __ T8H, Address(__ post(r1, 64))); \/\/        ld4     {v0.8H, v1.8H, v2.8H, v3.8H}, [x1], 64\n-    __ ld4(v0, v1, v2, v3, __ T8B, Address(__ post(r27, r29))); \/\/      ld4     {v0.8B, v1.8B, v2.8B, v3.8B}, [x27], x29\n-    __ ld4r(v17, v18, v19, v20, __ T8B, Address(r17)); \/\/       ld4r    {v17.8B, v18.8B, v19.8B, v20.8B}, [x17]\n-    __ ld4r(v5, v6, v7, v8, __ T4H, Address(__ post(r24, 8))); \/\/       ld4r    {v5.4H, v6.4H, v7.4H, v8.4H}, [x24], 8\n-    __ ld4r(v27, v28, v29, v30, __ T2S, Address(__ post(r5, r20))); \/\/  ld4r    {v27.2S, v28.2S, v29.2S, v30.2S}, [x5], x20\n+    __ ld1(v5, __ T8B, Address(r15));                  \/\/       ld1     {v5.8B}, [x15]\n+    __ ld1(v9, v10, __ T16B, Address(__ post(r7, 32))); \/\/      ld1     {v9.16B, v10.16B}, [x7], 32\n+    __ ld1(v12, v13, v14, __ T1D, Address(__ post(r16, r30))); \/\/       ld1     {v12.1D, v13.1D, v14.1D}, [x16], x30\n+    __ ld1(v7, v8, v9, v10, __ T8H, Address(__ post(r17, 64))); \/\/      ld1     {v7.8H, v8.8H, v9.8H, v10.8H}, [x17], 64\n+    __ ld1r(v16, __ T8B, Address(r25));                \/\/       ld1r    {v16.8B}, [x25]\n+    __ ld1r(v11, __ T4S, Address(__ post(r3, 4)));     \/\/       ld1r    {v11.4S}, [x3], 4\n+    __ ld1r(v12, __ T1D, Address(__ post(r7, r6)));    \/\/       ld1r    {v12.1D}, [x7], x6\n+    __ ld2(v9, v10, __ T2D, Address(r27));             \/\/       ld2     {v9.2D, v10.2D}, [x27]\n+    __ ld2(v6, v7, __ T4H, Address(__ post(r26, 16))); \/\/       ld2     {v6.4H, v7.4H}, [x26], 16\n+    __ ld2r(v23, v24, __ T16B, Address(r16));          \/\/       ld2r    {v23.16B, v24.16B}, [x16]\n+    __ ld2r(v6, v7, __ T2S, Address(__ post(r13, 8))); \/\/       ld2r    {v6.2S, v7.2S}, [x13], 8\n+    __ ld2r(v19, v20, __ T2D, Address(__ post(r1, r26))); \/\/    ld2r    {v19.2D, v20.2D}, [x1], x26\n+    __ ld3(v14, v15, v16, __ T4S, Address(__ post(r15, r0))); \/\/        ld3     {v14.4S, v15.4S, v16.4S}, [x15], x0\n+    __ ld3(v28, v29, v30, __ T2S, Address(r22));       \/\/       ld3     {v28.2S, v29.2S, v30.2S}, [x22]\n+    __ ld3r(v5, v6, v7, __ T8H, Address(r10));         \/\/       ld3r    {v5.8H, v6.8H, v7.8H}, [x10]\n+    __ ld3r(v14, v15, v16, __ T4S, Address(__ post(r6, 12))); \/\/        ld3r    {v14.4S, v15.4S, v16.4S}, [x6], 12\n+    __ ld3r(v6, v7, v8, __ T1D, Address(__ post(r10, r13))); \/\/ ld3r    {v6.1D, v7.1D, v8.1D}, [x10], x13\n+    __ ld4(v11, v12, v13, v14, __ T8H, Address(__ post(r3, 64))); \/\/    ld4     {v11.8H, v12.8H, v13.8H, v14.8H}, [x3], 64\n+    __ ld4(v12, v13, v14, v15, __ T8B, Address(__ post(r25, r0))); \/\/   ld4     {v12.8B, v13.8B, v14.8B, v15.8B}, [x25], x0\n+    __ ld4r(v10, v11, v12, v13, __ T8B, Address(r15)); \/\/       ld4r    {v10.8B, v11.8B, v12.8B, v13.8B}, [x15]\n+    __ ld4r(v29, v30, v31, v0, __ T4H, Address(__ post(r6, 8))); \/\/     ld4r    {v29.4H, v30.4H, v31.4H, v0.4H}, [x6], 8\n+    __ ld4r(v10, v11, v12, v13, __ T2S, Address(__ post(r26, r28))); \/\/ ld4r    {v10.2S, v11.2S, v12.2S, v13.2S}, [x26], x28\n@@ -616,21 +619,21 @@\n-    __ addv(v0, __ T8B, v1);                           \/\/       addv    b0, v1.8B\n-    __ addv(v20, __ T16B, v21);                        \/\/       addv    b20, v21.16B\n-    __ addv(v28, __ T4H, v29);                         \/\/       addv    h28, v29.4H\n-    __ addv(v15, __ T8H, v16);                         \/\/       addv    h15, v16.8H\n-    __ addv(v12, __ T4S, v13);                         \/\/       addv    s12, v13.4S\n-    __ smaxv(v10, __ T8B, v11);                        \/\/       smaxv   b10, v11.8B\n-    __ smaxv(v28, __ T16B, v29);                       \/\/       smaxv   b28, v29.16B\n-    __ smaxv(v28, __ T4H, v29);                        \/\/       smaxv   h28, v29.4H\n-    __ smaxv(v19, __ T8H, v20);                        \/\/       smaxv   h19, v20.8H\n-    __ smaxv(v22, __ T4S, v23);                        \/\/       smaxv   s22, v23.4S\n-    __ fmaxv(v10, __ T4S, v11);                        \/\/       fmaxv   s10, v11.4S\n-    __ sminv(v4, __ T8B, v5);                          \/\/       sminv   b4, v5.8B\n-    __ uminv(v30, __ T8B, v31);                        \/\/       uminv   b30, v31.8B\n-    __ sminv(v20, __ T16B, v21);                       \/\/       sminv   b20, v21.16B\n-    __ uminv(v8, __ T16B, v9);                         \/\/       uminv   b8, v9.16B\n-    __ sminv(v30, __ T4H, v31);                        \/\/       sminv   h30, v31.4H\n-    __ uminv(v17, __ T4H, v18);                        \/\/       uminv   h17, v18.4H\n-    __ sminv(v10, __ T8H, v11);                        \/\/       sminv   h10, v11.8H\n-    __ uminv(v27, __ T8H, v28);                        \/\/       uminv   h27, v28.8H\n-    __ sminv(v2, __ T4S, v3);                          \/\/       sminv   s2, v3.4S\n-    __ uminv(v24, __ T4S, v25);                        \/\/       uminv   s24, v25.4S\n+    __ addv(v28, __ T8B, v29);                         \/\/       addv    b28, v29.8B\n+    __ addv(v28, __ T16B, v29);                        \/\/       addv    b28, v29.16B\n+    __ addv(v19, __ T4H, v20);                         \/\/       addv    h19, v20.4H\n+    __ addv(v22, __ T8H, v23);                         \/\/       addv    h22, v23.8H\n+    __ addv(v10, __ T4S, v11);                         \/\/       addv    s10, v11.4S\n+    __ smaxv(v4, __ T8B, v5);                          \/\/       smaxv   b4, v5.8B\n+    __ smaxv(v30, __ T16B, v31);                       \/\/       smaxv   b30, v31.16B\n+    __ smaxv(v20, __ T4H, v21);                        \/\/       smaxv   h20, v21.4H\n+    __ smaxv(v8, __ T8H, v9);                          \/\/       smaxv   h8, v9.8H\n+    __ smaxv(v30, __ T4S, v31);                        \/\/       smaxv   s30, v31.4S\n+    __ fmaxv(v17, __ T4S, v18);                        \/\/       fmaxv   s17, v18.4S\n+    __ sminv(v10, __ T8B, v11);                        \/\/       sminv   b10, v11.8B\n+    __ uminv(v27, __ T8B, v28);                        \/\/       uminv   b27, v28.8B\n+    __ sminv(v2, __ T16B, v3);                         \/\/       sminv   b2, v3.16B\n+    __ uminv(v24, __ T16B, v25);                       \/\/       uminv   b24, v25.16B\n+    __ sminv(v4, __ T4H, v5);                          \/\/       sminv   h4, v5.4H\n+    __ uminv(v3, __ T4H, v4);                          \/\/       uminv   h3, v4.4H\n+    __ sminv(v8, __ T8H, v9);                          \/\/       sminv   h8, v9.8H\n+    __ uminv(v22, __ T8H, v23);                        \/\/       uminv   h22, v23.8H\n+    __ sminv(v17, __ T4S, v18);                        \/\/       sminv   s17, v18.4S\n+    __ uminv(v13, __ T4S, v14);                        \/\/       uminv   s13, v14.4S\n@@ -638,4 +641,4 @@\n-    __ fmaxp(v3, v4, __ S);                            \/\/       fmaxp   s3, v4.2S\n-    __ fmaxp(v8, v9, __ D);                            \/\/       fmaxp   d8, v9.2D\n-    __ fminp(v22, v23, __ S);                          \/\/       fminp   s22, v23.2S\n-    __ fminp(v17, v18, __ D);                          \/\/       fminp   d17, v18.2D\n+    __ fmaxp(v28, v29, __ S);                          \/\/       fmaxp   s28, v29.2S\n+    __ fmaxp(v23, v24, __ D);                          \/\/       fmaxp   d23, v24.2D\n+    __ fminp(v21, v22, __ S);                          \/\/       fminp   s21, v22.2S\n+    __ fminp(v25, v26, __ D);                          \/\/       fminp   d25, v26.2D\n@@ -644,7 +647,7 @@\n-    __ fcm(Assembler::GT, v13, __ T2S, v14);           \/\/       fcmgt   v13.2S, v14.2S, #0.0\n-    __ fcm(Assembler::GT, v4, __ T4S, v5);             \/\/       fcmgt   v4.4S, v5.4S, #0.0\n-    __ fcm(Assembler::GT, v28, __ T2D, v29);           \/\/       fcmgt   v28.2D, v29.2D, #0.0\n-    __ fcm(Assembler::GE, v23, __ T2S, v24);           \/\/       fcmge   v23.2S, v24.2S, #0.0\n-    __ fcm(Assembler::GE, v21, __ T4S, v22);           \/\/       fcmge   v21.4S, v22.4S, #0.0\n-    __ fcm(Assembler::GE, v25, __ T2D, v26);           \/\/       fcmge   v25.2D, v26.2D, #0.0\n-    __ fcm(Assembler::EQ, v24, __ T2S, v25);           \/\/       fcmeq   v24.2S, v25.2S, #0.0\n+    __ fcm(Assembler::GT, v24, __ T2S, v25);           \/\/       fcmgt   v24.2S, v25.2S, #0.0\n+    __ fcm(Assembler::GT, v3, __ T4S, v4);             \/\/       fcmgt   v3.4S, v4.4S, #0.0\n+    __ fcm(Assembler::GT, v23, __ T2D, v24);           \/\/       fcmgt   v23.2D, v24.2D, #0.0\n+    __ fcm(Assembler::GE, v26, __ T2S, v27);           \/\/       fcmge   v26.2S, v27.2S, #0.0\n+    __ fcm(Assembler::GE, v23, __ T4S, v24);           \/\/       fcmge   v23.4S, v24.4S, #0.0\n+    __ fcm(Assembler::GE, v14, __ T2D, v15);           \/\/       fcmge   v14.2D, v15.2D, #0.0\n+    __ fcm(Assembler::EQ, v21, __ T2S, v22);           \/\/       fcmeq   v21.2S, v22.2S, #0.0\n@@ -653,6 +656,6 @@\n-    __ fcm(Assembler::LT, v26, __ T2S, v27);           \/\/       fcmlt   v26.2S, v27.2S, #0.0\n-    __ fcm(Assembler::LT, v23, __ T4S, v24);           \/\/       fcmlt   v23.4S, v24.4S, #0.0\n-    __ fcm(Assembler::LT, v14, __ T2D, v15);           \/\/       fcmlt   v14.2D, v15.2D, #0.0\n-    __ fcm(Assembler::LE, v21, __ T2S, v22);           \/\/       fcmle   v21.2S, v22.2S, #0.0\n-    __ fcm(Assembler::LE, v3, __ T4S, v4);             \/\/       fcmle   v3.4S, v4.4S, #0.0\n-    __ fcm(Assembler::LE, v23, __ T2D, v24);           \/\/       fcmle   v23.2D, v24.2D, #0.0\n+    __ fcm(Assembler::LT, v8, __ T2S, v9);             \/\/       fcmlt   v8.2S, v9.2S, #0.0\n+    __ fcm(Assembler::LT, v24, __ T4S, v25);           \/\/       fcmlt   v24.4S, v25.4S, #0.0\n+    __ fcm(Assembler::LT, v19, __ T2D, v20);           \/\/       fcmlt   v19.2D, v20.2D, #0.0\n+    __ fcm(Assembler::LE, v15, __ T2S, v16);           \/\/       fcmle   v15.2S, v16.2S, #0.0\n+    __ fcm(Assembler::LE, v16, __ T4S, v17);           \/\/       fcmle   v16.4S, v17.4S, #0.0\n+    __ fcm(Assembler::LE, v2, __ T2D, v3);             \/\/       fcmle   v2.2D, v3.2D, #0.0\n@@ -661,18 +664,18 @@\n-    __ absr(v8, __ T8B, v9);                           \/\/       abs     v8.8B, v9.8B\n-    __ absr(v24, __ T16B, v25);                        \/\/       abs     v24.16B, v25.16B\n-    __ absr(v19, __ T4H, v20);                         \/\/       abs     v19.4H, v20.4H\n-    __ absr(v15, __ T8H, v16);                         \/\/       abs     v15.8H, v16.8H\n-    __ absr(v16, __ T2S, v17);                         \/\/       abs     v16.2S, v17.2S\n-    __ absr(v2, __ T4S, v3);                           \/\/       abs     v2.4S, v3.4S\n-    __ absr(v1, __ T2D, v2);                           \/\/       abs     v1.2D, v2.2D\n-    __ fabs(v0, __ T2S, v1);                           \/\/       fabs    v0.2S, v1.2S\n-    __ fabs(v24, __ T4S, v25);                         \/\/       fabs    v24.4S, v25.4S\n-    __ fabs(v4, __ T2D, v5);                           \/\/       fabs    v4.2D, v5.2D\n-    __ fneg(v3, __ T2S, v4);                           \/\/       fneg    v3.2S, v4.2S\n-    __ fneg(v11, __ T4S, v12);                         \/\/       fneg    v11.4S, v12.4S\n-    __ fneg(v30, __ T2D, v31);                         \/\/       fneg    v30.2D, v31.2D\n-    __ fsqrt(v27, __ T2S, v28);                        \/\/       fsqrt   v27.2S, v28.2S\n-    __ fsqrt(v9, __ T4S, v10);                         \/\/       fsqrt   v9.4S, v10.4S\n-    __ fsqrt(v25, __ T2D, v26);                        \/\/       fsqrt   v25.2D, v26.2D\n-    __ notr(v2, __ T8B, v3);                           \/\/       not     v2.8B, v3.8B\n-    __ notr(v12, __ T16B, v13);                        \/\/       not     v12.16B, v13.16B\n+    __ absr(v1, __ T8B, v2);                           \/\/       abs     v1.8B, v2.8B\n+    __ absr(v0, __ T16B, v1);                          \/\/       abs     v0.16B, v1.16B\n+    __ absr(v24, __ T4H, v25);                         \/\/       abs     v24.4H, v25.4H\n+    __ absr(v4, __ T8H, v5);                           \/\/       abs     v4.8H, v5.8H\n+    __ absr(v3, __ T2S, v4);                           \/\/       abs     v3.2S, v4.2S\n+    __ absr(v11, __ T4S, v12);                         \/\/       abs     v11.4S, v12.4S\n+    __ absr(v30, __ T2D, v31);                         \/\/       abs     v30.2D, v31.2D\n+    __ fabs(v27, __ T2S, v28);                         \/\/       fabs    v27.2S, v28.2S\n+    __ fabs(v9, __ T4S, v10);                          \/\/       fabs    v9.4S, v10.4S\n+    __ fabs(v25, __ T2D, v26);                         \/\/       fabs    v25.2D, v26.2D\n+    __ fneg(v2, __ T2S, v3);                           \/\/       fneg    v2.2S, v3.2S\n+    __ fneg(v12, __ T4S, v13);                         \/\/       fneg    v12.4S, v13.4S\n+    __ fneg(v17, __ T2D, v18);                         \/\/       fneg    v17.2D, v18.2D\n+    __ fsqrt(v30, __ T2S, v31);                        \/\/       fsqrt   v30.2S, v31.2S\n+    __ fsqrt(v1, __ T4S, v2);                          \/\/       fsqrt   v1.4S, v2.4S\n+    __ fsqrt(v12, __ T2D, v13);                        \/\/       fsqrt   v12.2D, v13.2D\n+    __ notr(v28, __ T8B, v29);                         \/\/       not     v28.8B, v29.8B\n+    __ notr(v0, __ T16B, v1);                          \/\/       not     v0.16B, v1.16B\n@@ -682,11 +685,11 @@\n-    __ andr(v30, __ T16B, v31, v0);                    \/\/       and     v30.16B, v31.16B, v0.16B\n-    __ orr(v1, __ T8B, v2, v3);                        \/\/       orr     v1.8B, v2.8B, v3.8B\n-    __ orr(v12, __ T16B, v13, v14);                    \/\/       orr     v12.16B, v13.16B, v14.16B\n-    __ eor(v28, __ T8B, v29, v30);                     \/\/       eor     v28.8B, v29.8B, v30.8B\n-    __ eor(v0, __ T16B, v1, v2);                       \/\/       eor     v0.16B, v1.16B, v2.16B\n-    __ addv(v17, __ T8B, v18, v19);                    \/\/       add     v17.8B, v18.8B, v19.8B\n-    __ addv(v12, __ T16B, v13, v14);                   \/\/       add     v12.16B, v13.16B, v14.16B\n-    __ addv(v17, __ T4H, v18, v19);                    \/\/       add     v17.4H, v18.4H, v19.4H\n-    __ addv(v21, __ T8H, v22, v23);                    \/\/       add     v21.8H, v22.8H, v23.8H\n-    __ addv(v12, __ T2S, v13, v14);                    \/\/       add     v12.2S, v13.2S, v14.2S\n-    __ addv(v27, __ T4S, v28, v29);                    \/\/       add     v27.4S, v28.4S, v29.4S\n+    __ andr(v12, __ T16B, v13, v14);                   \/\/       and     v12.16B, v13.16B, v14.16B\n+    __ orr(v17, __ T8B, v18, v19);                     \/\/       orr     v17.8B, v18.8B, v19.8B\n+    __ orr(v21, __ T16B, v22, v23);                    \/\/       orr     v21.16B, v22.16B, v23.16B\n+    __ eor(v12, __ T8B, v13, v14);                     \/\/       eor     v12.8B, v13.8B, v14.8B\n+    __ eor(v27, __ T16B, v28, v29);                    \/\/       eor     v27.16B, v28.16B, v29.16B\n+    __ addv(v29, __ T8B, v30, v31);                    \/\/       add     v29.8B, v30.8B, v31.8B\n+    __ addv(v30, __ T16B, v31, v0);                    \/\/       add     v30.16B, v31.16B, v0.16B\n+    __ addv(v1, __ T4H, v2, v3);                       \/\/       add     v1.4H, v2.4H, v3.4H\n+    __ addv(v25, __ T8H, v26, v27);                    \/\/       add     v25.8H, v26.8H, v27.8H\n+    __ addv(v27, __ T2S, v28, v29);                    \/\/       add     v27.2S, v28.2S, v29.2S\n+    __ addv(v4, __ T4S, v5, v6);                       \/\/       add     v4.4S, v5.4S, v6.4S\n@@ -694,28 +697,28 @@\n-    __ fadd(v30, __ T2S, v31, v0);                     \/\/       fadd    v30.2S, v31.2S, v0.2S\n-    __ fadd(v1, __ T4S, v2, v3);                       \/\/       fadd    v1.4S, v2.4S, v3.4S\n-    __ fadd(v25, __ T2D, v26, v27);                    \/\/       fadd    v25.2D, v26.2D, v27.2D\n-    __ subv(v27, __ T8B, v28, v29);                    \/\/       sub     v27.8B, v28.8B, v29.8B\n-    __ subv(v4, __ T16B, v5, v6);                      \/\/       sub     v4.16B, v5.16B, v6.16B\n-    __ subv(v29, __ T4H, v30, v31);                    \/\/       sub     v29.4H, v30.4H, v31.4H\n-    __ subv(v3, __ T8H, v4, v5);                       \/\/       sub     v3.8H, v4.8H, v5.8H\n-    __ subv(v6, __ T2S, v7, v8);                       \/\/       sub     v6.2S, v7.2S, v8.2S\n-    __ subv(v29, __ T4S, v30, v31);                    \/\/       sub     v29.4S, v30.4S, v31.4S\n-    __ subv(v25, __ T2D, v26, v27);                    \/\/       sub     v25.2D, v26.2D, v27.2D\n-    __ fsub(v17, __ T2S, v18, v19);                    \/\/       fsub    v17.2S, v18.2S, v19.2S\n-    __ fsub(v8, __ T4S, v9, v10);                      \/\/       fsub    v8.4S, v9.4S, v10.4S\n-    __ fsub(v7, __ T2D, v8, v9);                       \/\/       fsub    v7.2D, v8.2D, v9.2D\n-    __ mulv(v12, __ T8B, v13, v14);                    \/\/       mul     v12.8B, v13.8B, v14.8B\n-    __ mulv(v0, __ T16B, v1, v2);                      \/\/       mul     v0.16B, v1.16B, v2.16B\n-    __ mulv(v19, __ T4H, v20, v21);                    \/\/       mul     v19.4H, v20.4H, v21.4H\n-    __ mulv(v1, __ T8H, v2, v3);                       \/\/       mul     v1.8H, v2.8H, v3.8H\n-    __ mulv(v23, __ T2S, v24, v25);                    \/\/       mul     v23.2S, v24.2S, v25.2S\n-    __ mulv(v2, __ T4S, v3, v4);                       \/\/       mul     v2.4S, v3.4S, v4.4S\n-    __ fabd(v0, __ T2S, v1, v2);                       \/\/       fabd    v0.2S, v1.2S, v2.2S\n-    __ fabd(v8, __ T4S, v9, v10);                      \/\/       fabd    v8.4S, v9.4S, v10.4S\n-    __ fabd(v23, __ T2D, v24, v25);                    \/\/       fabd    v23.2D, v24.2D, v25.2D\n-    __ faddp(v25, __ T2S, v26, v27);                   \/\/       faddp   v25.2S, v26.2S, v27.2S\n-    __ faddp(v15, __ T4S, v16, v17);                   \/\/       faddp   v15.4S, v16.4S, v17.4S\n-    __ faddp(v29, __ T2D, v30, v31);                   \/\/       faddp   v29.2D, v30.2D, v31.2D\n-    __ fmul(v3, __ T2S, v4, v5);                       \/\/       fmul    v3.2S, v4.2S, v5.2S\n-    __ fmul(v10, __ T4S, v11, v12);                    \/\/       fmul    v10.4S, v11.4S, v12.4S\n-    __ fmul(v22, __ T2D, v23, v24);                    \/\/       fmul    v22.2D, v23.2D, v24.2D\n+    __ fadd(v3, __ T2S, v4, v5);                       \/\/       fadd    v3.2S, v4.2S, v5.2S\n+    __ fadd(v6, __ T4S, v7, v8);                       \/\/       fadd    v6.4S, v7.4S, v8.4S\n+    __ fadd(v29, __ T2D, v30, v31);                    \/\/       fadd    v29.2D, v30.2D, v31.2D\n+    __ subv(v25, __ T8B, v26, v27);                    \/\/       sub     v25.8B, v26.8B, v27.8B\n+    __ subv(v17, __ T16B, v18, v19);                   \/\/       sub     v17.16B, v18.16B, v19.16B\n+    __ subv(v8, __ T4H, v9, v10);                      \/\/       sub     v8.4H, v9.4H, v10.4H\n+    __ subv(v7, __ T8H, v8, v9);                       \/\/       sub     v7.8H, v8.8H, v9.8H\n+    __ subv(v12, __ T2S, v13, v14);                    \/\/       sub     v12.2S, v13.2S, v14.2S\n+    __ subv(v0, __ T4S, v1, v2);                       \/\/       sub     v0.4S, v1.4S, v2.4S\n+    __ subv(v19, __ T2D, v20, v21);                    \/\/       sub     v19.2D, v20.2D, v21.2D\n+    __ fsub(v1, __ T2S, v2, v3);                       \/\/       fsub    v1.2S, v2.2S, v3.2S\n+    __ fsub(v23, __ T4S, v24, v25);                    \/\/       fsub    v23.4S, v24.4S, v25.4S\n+    __ fsub(v2, __ T2D, v3, v4);                       \/\/       fsub    v2.2D, v3.2D, v4.2D\n+    __ mulv(v0, __ T8B, v1, v2);                       \/\/       mul     v0.8B, v1.8B, v2.8B\n+    __ mulv(v8, __ T16B, v9, v10);                     \/\/       mul     v8.16B, v9.16B, v10.16B\n+    __ mulv(v23, __ T4H, v24, v25);                    \/\/       mul     v23.4H, v24.4H, v25.4H\n+    __ mulv(v25, __ T8H, v26, v27);                    \/\/       mul     v25.8H, v26.8H, v27.8H\n+    __ mulv(v15, __ T2S, v16, v17);                    \/\/       mul     v15.2S, v16.2S, v17.2S\n+    __ mulv(v29, __ T4S, v30, v31);                    \/\/       mul     v29.4S, v30.4S, v31.4S\n+    __ fabd(v3, __ T2S, v4, v5);                       \/\/       fabd    v3.2S, v4.2S, v5.2S\n+    __ fabd(v10, __ T4S, v11, v12);                    \/\/       fabd    v10.4S, v11.4S, v12.4S\n+    __ fabd(v22, __ T2D, v23, v24);                    \/\/       fabd    v22.2D, v23.2D, v24.2D\n+    __ faddp(v10, __ T2S, v11, v12);                   \/\/       faddp   v10.2S, v11.2S, v12.2S\n+    __ faddp(v4, __ T4S, v5, v6);                      \/\/       faddp   v4.4S, v5.4S, v6.4S\n+    __ faddp(v17, __ T2D, v18, v19);                   \/\/       faddp   v17.2D, v18.2D, v19.2D\n+    __ fmul(v1, __ T2S, v2, v3);                       \/\/       fmul    v1.2S, v2.2S, v3.2S\n+    __ fmul(v11, __ T4S, v12, v13);                    \/\/       fmul    v11.4S, v12.4S, v13.4S\n+    __ fmul(v7, __ T2D, v8, v9);                       \/\/       fmul    v7.2D, v8.2D, v9.2D\n@@ -723,8 +726,8 @@\n-    __ mlav(v4, __ T8H, v5, v6);                       \/\/       mla     v4.8H, v5.8H, v6.8H\n-    __ mlav(v17, __ T2S, v18, v19);                    \/\/       mla     v17.2S, v18.2S, v19.2S\n-    __ mlav(v1, __ T4S, v2, v3);                       \/\/       mla     v1.4S, v2.4S, v3.4S\n-    __ fmla(v11, __ T2S, v12, v13);                    \/\/       fmla    v11.2S, v12.2S, v13.2S\n-    __ fmla(v7, __ T4S, v8, v9);                       \/\/       fmla    v7.4S, v8.4S, v9.4S\n-    __ fmla(v10, __ T2D, v11, v12);                    \/\/       fmla    v10.2D, v11.2D, v12.2D\n-    __ mlsv(v15, __ T4H, v16, v17);                    \/\/       mls     v15.4H, v16.4H, v17.4H\n-    __ mlsv(v16, __ T8H, v17, v18);                    \/\/       mls     v16.8H, v17.8H, v18.8H\n+    __ mlav(v15, __ T8H, v16, v17);                    \/\/       mla     v15.8H, v16.8H, v17.8H\n+    __ mlav(v16, __ T2S, v17, v18);                    \/\/       mla     v16.2S, v17.2S, v18.2S\n+    __ mlav(v2, __ T4S, v3, v4);                       \/\/       mla     v2.4S, v3.4S, v4.4S\n+    __ fmla(v9, __ T2S, v10, v11);                     \/\/       fmla    v9.2S, v10.2S, v11.2S\n+    __ fmla(v11, __ T4S, v12, v13);                    \/\/       fmla    v11.4S, v12.4S, v13.4S\n+    __ fmla(v12, __ T2D, v13, v14);                    \/\/       fmla    v12.2D, v13.2D, v14.2D\n+    __ mlsv(v14, __ T4H, v15, v16);                    \/\/       mls     v14.4H, v15.4H, v16.4H\n+    __ mlsv(v13, __ T8H, v14, v15);                    \/\/       mls     v13.8H, v14.8H, v15.8H\n@@ -732,5 +735,5 @@\n-    __ mlsv(v9, __ T4S, v10, v11);                     \/\/       mls     v9.4S, v10.4S, v11.4S\n-    __ fmls(v11, __ T2S, v12, v13);                    \/\/       fmls    v11.2S, v12.2S, v13.2S\n-    __ fmls(v12, __ T4S, v13, v14);                    \/\/       fmls    v12.4S, v13.4S, v14.4S\n-    __ fmls(v14, __ T2D, v15, v16);                    \/\/       fmls    v14.2D, v15.2D, v16.2D\n-    __ fdiv(v13, __ T2S, v14, v15);                    \/\/       fdiv    v13.2S, v14.2S, v15.2S\n+    __ mlsv(v6, __ T4S, v7, v8);                       \/\/       mls     v6.4S, v7.4S, v8.4S\n+    __ fmls(v19, __ T2S, v20, v21);                    \/\/       fmls    v19.2S, v20.2S, v21.2S\n+    __ fmls(v25, __ T4S, v26, v27);                    \/\/       fmls    v25.4S, v26.4S, v27.4S\n+    __ fmls(v15, __ T2D, v16, v17);                    \/\/       fmls    v15.2D, v16.2D, v17.2D\n+    __ fdiv(v4, __ T2S, v5, v6);                       \/\/       fdiv    v4.2S, v5.2S, v6.2S\n@@ -738,34 +741,34 @@\n-    __ fdiv(v6, __ T2D, v7, v8);                       \/\/       fdiv    v6.2D, v7.2D, v8.2D\n-    __ maxv(v19, __ T8B, v20, v21);                    \/\/       smax    v19.8B, v20.8B, v21.8B\n-    __ maxv(v25, __ T16B, v26, v27);                   \/\/       smax    v25.16B, v26.16B, v27.16B\n-    __ maxv(v15, __ T4H, v16, v17);                    \/\/       smax    v15.4H, v16.4H, v17.4H\n-    __ maxv(v4, __ T8H, v5, v6);                       \/\/       smax    v4.8H, v5.8H, v6.8H\n-    __ maxv(v2, __ T2S, v3, v4);                       \/\/       smax    v2.2S, v3.2S, v4.2S\n-    __ maxv(v4, __ T4S, v5, v6);                       \/\/       smax    v4.4S, v5.4S, v6.4S\n-    __ smaxp(v11, __ T8B, v12, v13);                   \/\/       smaxp   v11.8B, v12.8B, v13.8B\n-    __ smaxp(v17, __ T16B, v18, v19);                  \/\/       smaxp   v17.16B, v18.16B, v19.16B\n-    __ smaxp(v20, __ T4H, v21, v22);                   \/\/       smaxp   v20.4H, v21.4H, v22.4H\n-    __ smaxp(v16, __ T8H, v17, v18);                   \/\/       smaxp   v16.8H, v17.8H, v18.8H\n-    __ smaxp(v17, __ T2S, v18, v19);                   \/\/       smaxp   v17.2S, v18.2S, v19.2S\n-    __ smaxp(v10, __ T4S, v11, v12);                   \/\/       smaxp   v10.4S, v11.4S, v12.4S\n-    __ fmax(v20, __ T2S, v21, v22);                    \/\/       fmax    v20.2S, v21.2S, v22.2S\n-    __ fmax(v22, __ T4S, v23, v24);                    \/\/       fmax    v22.4S, v23.4S, v24.4S\n-    __ fmax(v12, __ T2D, v13, v14);                    \/\/       fmax    v12.2D, v13.2D, v14.2D\n-    __ minv(v25, __ T8B, v26, v27);                    \/\/       smin    v25.8B, v26.8B, v27.8B\n-    __ minv(v23, __ T16B, v24, v25);                   \/\/       smin    v23.16B, v24.16B, v25.16B\n-    __ minv(v28, __ T4H, v29, v30);                    \/\/       smin    v28.4H, v29.4H, v30.4H\n-    __ minv(v14, __ T8H, v15, v16);                    \/\/       smin    v14.8H, v15.8H, v16.8H\n-    __ minv(v10, __ T2S, v11, v12);                    \/\/       smin    v10.2S, v11.2S, v12.2S\n-    __ minv(v24, __ T4S, v25, v26);                    \/\/       smin    v24.4S, v25.4S, v26.4S\n-    __ sminp(v1, __ T8B, v2, v3);                      \/\/       sminp   v1.8B, v2.8B, v3.8B\n-    __ sminp(v11, __ T16B, v12, v13);                  \/\/       sminp   v11.16B, v12.16B, v13.16B\n-    __ sminp(v30, __ T4H, v31, v0);                    \/\/       sminp   v30.4H, v31.4H, v0.4H\n-    __ sminp(v10, __ T8H, v11, v12);                   \/\/       sminp   v10.8H, v11.8H, v12.8H\n-    __ sminp(v15, __ T2S, v16, v17);                   \/\/       sminp   v15.2S, v16.2S, v17.2S\n-    __ sminp(v7, __ T4S, v8, v9);                      \/\/       sminp   v7.4S, v8.4S, v9.4S\n-    __ fmin(v2, __ T2S, v3, v4);                       \/\/       fmin    v2.2S, v3.2S, v4.2S\n-    __ fmin(v3, __ T4S, v4, v5);                       \/\/       fmin    v3.4S, v4.4S, v5.4S\n-    __ fmin(v13, __ T2D, v14, v15);                    \/\/       fmin    v13.2D, v14.2D, v15.2D\n-    __ facgt(v19, __ T2S, v20, v21);                   \/\/       facgt   v19.2S, v20.2S, v21.2S\n-    __ facgt(v16, __ T4S, v17, v18);                   \/\/       facgt   v16.4S, v17.4S, v18.4S\n-    __ facgt(v16, __ T2D, v17, v18);                   \/\/       facgt   v16.2D, v17.2D, v18.2D\n+    __ fdiv(v4, __ T2D, v5, v6);                       \/\/       fdiv    v4.2D, v5.2D, v6.2D\n+    __ maxv(v11, __ T8B, v12, v13);                    \/\/       smax    v11.8B, v12.8B, v13.8B\n+    __ maxv(v17, __ T16B, v18, v19);                   \/\/       smax    v17.16B, v18.16B, v19.16B\n+    __ maxv(v20, __ T4H, v21, v22);                    \/\/       smax    v20.4H, v21.4H, v22.4H\n+    __ maxv(v16, __ T8H, v17, v18);                    \/\/       smax    v16.8H, v17.8H, v18.8H\n+    __ maxv(v17, __ T2S, v18, v19);                    \/\/       smax    v17.2S, v18.2S, v19.2S\n+    __ maxv(v10, __ T4S, v11, v12);                    \/\/       smax    v10.4S, v11.4S, v12.4S\n+    __ smaxp(v20, __ T8B, v21, v22);                   \/\/       smaxp   v20.8B, v21.8B, v22.8B\n+    __ smaxp(v22, __ T16B, v23, v24);                  \/\/       smaxp   v22.16B, v23.16B, v24.16B\n+    __ smaxp(v12, __ T4H, v13, v14);                   \/\/       smaxp   v12.4H, v13.4H, v14.4H\n+    __ smaxp(v25, __ T8H, v26, v27);                   \/\/       smaxp   v25.8H, v26.8H, v27.8H\n+    __ smaxp(v23, __ T2S, v24, v25);                   \/\/       smaxp   v23.2S, v24.2S, v25.2S\n+    __ smaxp(v28, __ T4S, v29, v30);                   \/\/       smaxp   v28.4S, v29.4S, v30.4S\n+    __ fmax(v14, __ T2S, v15, v16);                    \/\/       fmax    v14.2S, v15.2S, v16.2S\n+    __ fmax(v10, __ T4S, v11, v12);                    \/\/       fmax    v10.4S, v11.4S, v12.4S\n+    __ fmax(v24, __ T2D, v25, v26);                    \/\/       fmax    v24.2D, v25.2D, v26.2D\n+    __ minv(v1, __ T8B, v2, v3);                       \/\/       smin    v1.8B, v2.8B, v3.8B\n+    __ minv(v11, __ T16B, v12, v13);                   \/\/       smin    v11.16B, v12.16B, v13.16B\n+    __ minv(v30, __ T4H, v31, v0);                     \/\/       smin    v30.4H, v31.4H, v0.4H\n+    __ minv(v10, __ T8H, v11, v12);                    \/\/       smin    v10.8H, v11.8H, v12.8H\n+    __ minv(v15, __ T2S, v16, v17);                    \/\/       smin    v15.2S, v16.2S, v17.2S\n+    __ minv(v7, __ T4S, v8, v9);                       \/\/       smin    v7.4S, v8.4S, v9.4S\n+    __ sminp(v2, __ T8B, v3, v4);                      \/\/       sminp   v2.8B, v3.8B, v4.8B\n+    __ sminp(v3, __ T16B, v4, v5);                     \/\/       sminp   v3.16B, v4.16B, v5.16B\n+    __ sminp(v13, __ T4H, v14, v15);                   \/\/       sminp   v13.4H, v14.4H, v15.4H\n+    __ sminp(v19, __ T8H, v20, v21);                   \/\/       sminp   v19.8H, v20.8H, v21.8H\n+    __ sminp(v16, __ T2S, v17, v18);                   \/\/       sminp   v16.2S, v17.2S, v18.2S\n+    __ sminp(v16, __ T4S, v17, v18);                   \/\/       sminp   v16.4S, v17.4S, v18.4S\n+    __ fmin(v3, __ T2S, v4, v5);                       \/\/       fmin    v3.2S, v4.2S, v5.2S\n+    __ fmin(v1, __ T4S, v2, v3);                       \/\/       fmin    v1.4S, v2.4S, v3.4S\n+    __ fmin(v11, __ T2D, v12, v13);                    \/\/       fmin    v11.2D, v12.2D, v13.2D\n+    __ facgt(v29, __ T2S, v30, v31);                   \/\/       facgt   v29.2S, v30.2S, v31.2S\n+    __ facgt(v5, __ T4S, v6, v7);                      \/\/       facgt   v5.4S, v6.4S, v7.4S\n+    __ facgt(v8, __ T2D, v9, v10);                     \/\/       facgt   v8.2D, v9.2D, v10.2D\n@@ -774,30 +777,30 @@\n-    __ cm(Assembler::GT, v3, __ T8B, v4, v5);          \/\/       cmgt    v3.8B, v4.8B, v5.8B\n-    __ cm(Assembler::GT, v1, __ T16B, v2, v3);         \/\/       cmgt    v1.16B, v2.16B, v3.16B\n-    __ cm(Assembler::GT, v11, __ T4H, v12, v13);       \/\/       cmgt    v11.4H, v12.4H, v13.4H\n-    __ cm(Assembler::GT, v29, __ T8H, v30, v31);       \/\/       cmgt    v29.8H, v30.8H, v31.8H\n-    __ cm(Assembler::GT, v5, __ T2S, v6, v7);          \/\/       cmgt    v5.2S, v6.2S, v7.2S\n-    __ cm(Assembler::GT, v8, __ T4S, v9, v10);         \/\/       cmgt    v8.4S, v9.4S, v10.4S\n-    __ cm(Assembler::GT, v14, __ T2D, v15, v16);       \/\/       cmgt    v14.2D, v15.2D, v16.2D\n-    __ cm(Assembler::GE, v28, __ T8B, v29, v30);       \/\/       cmge    v28.8B, v29.8B, v30.8B\n-    __ cm(Assembler::GE, v29, __ T16B, v30, v31);      \/\/       cmge    v29.16B, v30.16B, v31.16B\n-    __ cm(Assembler::GE, v0, __ T4H, v1, v2);          \/\/       cmge    v0.4H, v1.4H, v2.4H\n-    __ cm(Assembler::GE, v20, __ T8H, v21, v22);       \/\/       cmge    v20.8H, v21.8H, v22.8H\n-    __ cm(Assembler::GE, v7, __ T2S, v8, v9);          \/\/       cmge    v7.2S, v8.2S, v9.2S\n-    __ cm(Assembler::GE, v20, __ T4S, v21, v22);       \/\/       cmge    v20.4S, v21.4S, v22.4S\n-    __ cm(Assembler::GE, v23, __ T2D, v24, v25);       \/\/       cmge    v23.2D, v24.2D, v25.2D\n-    __ cm(Assembler::EQ, v27, __ T8B, v28, v29);       \/\/       cmeq    v27.8B, v28.8B, v29.8B\n-    __ cm(Assembler::EQ, v21, __ T16B, v22, v23);      \/\/       cmeq    v21.16B, v22.16B, v23.16B\n-    __ cm(Assembler::EQ, v26, __ T4H, v27, v28);       \/\/       cmeq    v26.4H, v27.4H, v28.4H\n-    __ cm(Assembler::EQ, v24, __ T8H, v25, v26);       \/\/       cmeq    v24.8H, v25.8H, v26.8H\n-    __ cm(Assembler::EQ, v4, __ T2S, v5, v6);          \/\/       cmeq    v4.2S, v5.2S, v6.2S\n-    __ cm(Assembler::EQ, v1, __ T4S, v2, v3);          \/\/       cmeq    v1.4S, v2.4S, v3.4S\n-    __ cm(Assembler::EQ, v22, __ T2D, v23, v24);       \/\/       cmeq    v22.2D, v23.2D, v24.2D\n-    __ cm(Assembler::HI, v16, __ T8B, v17, v18);       \/\/       cmhi    v16.8B, v17.8B, v18.8B\n-    __ cm(Assembler::HI, v30, __ T16B, v31, v0);       \/\/       cmhi    v30.16B, v31.16B, v0.16B\n-    __ cm(Assembler::HI, v5, __ T4H, v6, v7);          \/\/       cmhi    v5.4H, v6.4H, v7.4H\n-    __ cm(Assembler::HI, v11, __ T8H, v12, v13);       \/\/       cmhi    v11.8H, v12.8H, v13.8H\n-    __ cm(Assembler::HI, v8, __ T2S, v9, v10);         \/\/       cmhi    v8.2S, v9.2S, v10.2S\n-    __ cm(Assembler::HI, v27, __ T4S, v28, v29);       \/\/       cmhi    v27.4S, v28.4S, v29.4S\n-    __ cm(Assembler::HI, v14, __ T2D, v15, v16);       \/\/       cmhi    v14.2D, v15.2D, v16.2D\n-    __ cm(Assembler::HS, v28, __ T8B, v29, v30);       \/\/       cmhs    v28.8B, v29.8B, v30.8B\n-    __ cm(Assembler::HS, v21, __ T16B, v22, v23);      \/\/       cmhs    v21.16B, v22.16B, v23.16B\n+    __ cm(Assembler::GT, v14, __ T8B, v15, v16);       \/\/       cmgt    v14.8B, v15.8B, v16.8B\n+    __ cm(Assembler::GT, v28, __ T16B, v29, v30);      \/\/       cmgt    v28.16B, v29.16B, v30.16B\n+    __ cm(Assembler::GT, v29, __ T4H, v30, v31);       \/\/       cmgt    v29.4H, v30.4H, v31.4H\n+    __ cm(Assembler::GT, v0, __ T8H, v1, v2);          \/\/       cmgt    v0.8H, v1.8H, v2.8H\n+    __ cm(Assembler::GT, v20, __ T2S, v21, v22);       \/\/       cmgt    v20.2S, v21.2S, v22.2S\n+    __ cm(Assembler::GT, v7, __ T4S, v8, v9);          \/\/       cmgt    v7.4S, v8.4S, v9.4S\n+    __ cm(Assembler::GT, v20, __ T2D, v21, v22);       \/\/       cmgt    v20.2D, v21.2D, v22.2D\n+    __ cm(Assembler::GE, v23, __ T8B, v24, v25);       \/\/       cmge    v23.8B, v24.8B, v25.8B\n+    __ cm(Assembler::GE, v27, __ T16B, v28, v29);      \/\/       cmge    v27.16B, v28.16B, v29.16B\n+    __ cm(Assembler::GE, v21, __ T4H, v22, v23);       \/\/       cmge    v21.4H, v22.4H, v23.4H\n+    __ cm(Assembler::GE, v26, __ T8H, v27, v28);       \/\/       cmge    v26.8H, v27.8H, v28.8H\n+    __ cm(Assembler::GE, v24, __ T2S, v25, v26);       \/\/       cmge    v24.2S, v25.2S, v26.2S\n+    __ cm(Assembler::GE, v4, __ T4S, v5, v6);          \/\/       cmge    v4.4S, v5.4S, v6.4S\n+    __ cm(Assembler::GE, v1, __ T2D, v2, v3);          \/\/       cmge    v1.2D, v2.2D, v3.2D\n+    __ cm(Assembler::EQ, v22, __ T8B, v23, v24);       \/\/       cmeq    v22.8B, v23.8B, v24.8B\n+    __ cm(Assembler::EQ, v16, __ T16B, v17, v18);      \/\/       cmeq    v16.16B, v17.16B, v18.16B\n+    __ cm(Assembler::EQ, v30, __ T4H, v31, v0);        \/\/       cmeq    v30.4H, v31.4H, v0.4H\n+    __ cm(Assembler::EQ, v5, __ T8H, v6, v7);          \/\/       cmeq    v5.8H, v6.8H, v7.8H\n+    __ cm(Assembler::EQ, v11, __ T2S, v12, v13);       \/\/       cmeq    v11.2S, v12.2S, v13.2S\n+    __ cm(Assembler::EQ, v8, __ T4S, v9, v10);         \/\/       cmeq    v8.4S, v9.4S, v10.4S\n+    __ cm(Assembler::EQ, v27, __ T2D, v28, v29);       \/\/       cmeq    v27.2D, v28.2D, v29.2D\n+    __ cm(Assembler::HI, v14, __ T8B, v15, v16);       \/\/       cmhi    v14.8B, v15.8B, v16.8B\n+    __ cm(Assembler::HI, v28, __ T16B, v29, v30);      \/\/       cmhi    v28.16B, v29.16B, v30.16B\n+    __ cm(Assembler::HI, v21, __ T4H, v22, v23);       \/\/       cmhi    v21.4H, v22.4H, v23.4H\n+    __ cm(Assembler::HI, v30, __ T8H, v31, v0);        \/\/       cmhi    v30.8H, v31.8H, v0.8H\n+    __ cm(Assembler::HI, v17, __ T2S, v18, v19);       \/\/       cmhi    v17.2S, v18.2S, v19.2S\n+    __ cm(Assembler::HI, v30, __ T4S, v31, v0);        \/\/       cmhi    v30.4S, v31.4S, v0.4S\n+    __ cm(Assembler::HI, v5, __ T2D, v6, v7);          \/\/       cmhi    v5.2D, v6.2D, v7.2D\n+    __ cm(Assembler::HS, v13, __ T8B, v14, v15);       \/\/       cmhs    v13.8B, v14.8B, v15.8B\n+    __ cm(Assembler::HS, v17, __ T16B, v18, v19);      \/\/       cmhs    v17.16B, v18.16B, v19.16B\n@@ -806,12 +809,12 @@\n-    __ cm(Assembler::HS, v30, __ T2S, v31, v0);        \/\/       cmhs    v30.2S, v31.2S, v0.2S\n-    __ cm(Assembler::HS, v5, __ T4S, v6, v7);          \/\/       cmhs    v5.4S, v6.4S, v7.4S\n-    __ cm(Assembler::HS, v13, __ T2D, v14, v15);       \/\/       cmhs    v13.2D, v14.2D, v15.2D\n-    __ fcm(Assembler::EQ, v17, __ T2S, v18, v19);      \/\/       fcmeq   v17.2S, v18.2S, v19.2S\n-    __ fcm(Assembler::EQ, v30, __ T4S, v31, v0);       \/\/       fcmeq   v30.4S, v31.4S, v0.4S\n-    __ fcm(Assembler::EQ, v17, __ T2D, v18, v19);      \/\/       fcmeq   v17.2D, v18.2D, v19.2D\n-    __ fcm(Assembler::GT, v26, __ T2S, v27, v28);      \/\/       fcmgt   v26.2S, v27.2S, v28.2S\n-    __ fcm(Assembler::GT, v19, __ T4S, v20, v21);      \/\/       fcmgt   v19.4S, v20.4S, v21.4S\n-    __ fcm(Assembler::GT, v15, __ T2D, v16, v17);      \/\/       fcmgt   v15.2D, v16.2D, v17.2D\n-    __ fcm(Assembler::GE, v12, __ T2S, v13, v14);      \/\/       fcmge   v12.2S, v13.2S, v14.2S\n-    __ fcm(Assembler::GE, v11, __ T4S, v12, v13);      \/\/       fcmge   v11.4S, v12.4S, v13.4S\n-    __ fcm(Assembler::GE, v9, __ T2D, v10, v11);       \/\/       fcmge   v9.2D, v10.2D, v11.2D\n+    __ cm(Assembler::HS, v26, __ T2S, v27, v28);       \/\/       cmhs    v26.2S, v27.2S, v28.2S\n+    __ cm(Assembler::HS, v19, __ T4S, v20, v21);       \/\/       cmhs    v19.4S, v20.4S, v21.4S\n+    __ cm(Assembler::HS, v15, __ T2D, v16, v17);       \/\/       cmhs    v15.2D, v16.2D, v17.2D\n+    __ fcm(Assembler::EQ, v12, __ T2S, v13, v14);      \/\/       fcmeq   v12.2S, v13.2S, v14.2S\n+    __ fcm(Assembler::EQ, v11, __ T4S, v12, v13);      \/\/       fcmeq   v11.4S, v12.4S, v13.4S\n+    __ fcm(Assembler::EQ, v9, __ T2D, v10, v11);       \/\/       fcmeq   v9.2D, v10.2D, v11.2D\n+    __ fcm(Assembler::GT, v6, __ T2S, v7, v8);         \/\/       fcmgt   v6.2S, v7.2S, v8.2S\n+    __ fcm(Assembler::GT, v29, __ T4S, v30, v31);      \/\/       fcmgt   v29.4S, v30.4S, v31.4S\n+    __ fcm(Assembler::GT, v16, __ T2D, v17, v18);      \/\/       fcmgt   v16.2D, v17.2D, v18.2D\n+    __ fcm(Assembler::GE, v26, __ T2S, v27, v28);      \/\/       fcmge   v26.2S, v27.2S, v28.2S\n+    __ fcm(Assembler::GE, v27, __ T4S, v28, v29);      \/\/       fcmge   v27.4S, v28.4S, v29.4S\n+    __ fcm(Assembler::GE, v29, __ T2D, v30, v31);      \/\/       fcmge   v29.2D, v30.2D, v31.2D\n@@ -820,6 +823,6 @@\n-    __ sve_fcm(Assembler::EQ, p3, __ D, p6, z29, 0.0); \/\/       fcmeq   p3.d, p6\/z, z29.d, #0.0\n-    __ sve_fcm(Assembler::GT, p14, __ S, p2, z29, 0.0); \/\/      fcmgt   p14.s, p2\/z, z29.s, #0.0\n-    __ sve_fcm(Assembler::GE, p10, __ S, p6, z9, 0.0); \/\/       fcmge   p10.s, p6\/z, z9.s, #0.0\n-    __ sve_fcm(Assembler::LT, p8, __ D, p0, z16, 0.0); \/\/       fcmlt   p8.d, p0\/z, z16.d, #0.0\n-    __ sve_fcm(Assembler::LE, p14, __ D, p4, z14, 0.0); \/\/      fcmle   p14.d, p4\/z, z14.d, #0.0\n-    __ sve_fcm(Assembler::NE, p9, __ S, p3, z22, 0.0); \/\/       fcmne   p9.s, p3\/z, z22.s, #0.0\n+    __ sve_fcm(Assembler::EQ, p3, __ D, p2, z10, 0.0); \/\/       fcmeq   p3.d, p2\/z, z10.d, #0.0\n+    __ sve_fcm(Assembler::GT, p2, __ D, p4, z23, 0.0); \/\/       fcmgt   p2.d, p4\/z, z23.d, #0.0\n+    __ sve_fcm(Assembler::GE, p11, __ D, p3, z3, 0.0); \/\/       fcmge   p11.d, p3\/z, z3.d, #0.0\n+    __ sve_fcm(Assembler::LT, p11, __ D, p5, z17, 0.0); \/\/      fcmlt   p11.d, p5\/z, z17.d, #0.0\n+    __ sve_fcm(Assembler::LE, p1, __ S, p3, z15, 0.0); \/\/       fcmle   p1.s, p3\/z, z15.s, #0.0\n+    __ sve_fcm(Assembler::NE, p8, __ S, p5, z10, 0.0); \/\/       fcmne   p8.s, p5\/z, z10.s, #0.0\n@@ -828,10 +831,10 @@\n-    __ sve_cmp(Assembler::EQ, p3, __ S, p2, z12, -3);  \/\/       cmpeq   p3.s, p2\/z, z12.s, #-3\n-    __ sve_cmp(Assembler::GT, p11, __ D, p4, z1, -11); \/\/       cmpgt   p11.d, p4\/z, z1.d, #-11\n-    __ sve_cmp(Assembler::GE, p8, __ S, p5, z2, -3);   \/\/       cmpge   p8.s, p5\/z, z2.s, #-3\n-    __ sve_cmp(Assembler::LT, p5, __ D, p6, z20, -4);  \/\/       cmplt   p5.d, p6\/z, z20.d, #-4\n-    __ sve_cmp(Assembler::LE, p13, __ B, p7, z3, 8);   \/\/       cmple   p13.b, p7\/z, z3.b, #8\n-    __ sve_cmp(Assembler::NE, p9, __ H, p7, z17, 11);  \/\/       cmpne   p9.h, p7\/z, z17.h, #11\n-    __ sve_cmp(Assembler::HS, p7, __ S, p5, z6, 127);  \/\/       cmphs   p7.s, p5\/z, z6.s, #127\n-    __ sve_cmp(Assembler::HI, p12, __ D, p6, z2, 74);  \/\/       cmphi   p12.d, p6\/z, z2.d, #74\n-    __ sve_cmp(Assembler::LS, p5, __ S, p0, z22, 72);  \/\/       cmpls   p5.s, p0\/z, z22.s, #72\n-    __ sve_cmp(Assembler::LO, p0, __ D, p5, z24, 11);  \/\/       cmplo   p0.d, p5\/z, z24.d, #11\n+    __ sve_cmp(Assembler::EQ, p0, __ S, p1, z29, 1);   \/\/       cmpeq   p0.s, p1\/z, z29.s, #1\n+    __ sve_cmp(Assembler::GT, p1, __ S, p3, z15, -6);  \/\/       cmpgt   p1.s, p3\/z, z15.s, #-6\n+    __ sve_cmp(Assembler::GE, p10, __ D, p3, z28, 11); \/\/       cmpge   p10.d, p3\/z, z28.d, #11\n+    __ sve_cmp(Assembler::LT, p1, __ D, p6, z1, 3);    \/\/       cmplt   p1.d, p6\/z, z1.d, #3\n+    __ sve_cmp(Assembler::LE, p8, __ D, p6, z9, -1);   \/\/       cmple   p8.d, p6\/z, z9.d, #-1\n+    __ sve_cmp(Assembler::NE, p3, __ S, p7, z20, 9);   \/\/       cmpne   p3.s, p7\/z, z20.s, #9\n+    __ sve_cmp(Assembler::HS, p1, __ D, p4, z30, 42);  \/\/       cmphs   p1.d, p4\/z, z30.d, #42\n+    __ sve_cmp(Assembler::HI, p11, __ B, p4, z17, 2);  \/\/       cmphi   p11.b, p4\/z, z17.b, #2\n+    __ sve_cmp(Assembler::LS, p12, __ S, p0, z25, 75); \/\/       cmpls   p12.s, p0\/z, z25.s, #75\n+    __ sve_cmp(Assembler::LO, p6, __ D, p7, z3, 118);  \/\/       cmplo   p6.d, p7\/z, z3.d, #118\n@@ -1092,9 +1095,9 @@\n-    __ swp(Assembler::xword, r16, r12, r4);            \/\/       swp     x16, x12, [x4]\n-    __ ldadd(Assembler::xword, r28, r30, r29);         \/\/       ldadd   x28, x30, [x29]\n-    __ ldbic(Assembler::xword, r16, r27, r6);          \/\/       ldclr   x16, x27, [x6]\n-    __ ldeor(Assembler::xword, r9, r29, r15);          \/\/       ldeor   x9, x29, [x15]\n-    __ ldorr(Assembler::xword, r7, r4, r7);            \/\/       ldset   x7, x4, [x7]\n-    __ ldsmin(Assembler::xword, r15, r9, r23);         \/\/       ldsmin  x15, x9, [x23]\n-    __ ldsmax(Assembler::xword, r8, r2, r28);          \/\/       ldsmax  x8, x2, [x28]\n-    __ ldumin(Assembler::xword, r21, zr, r5);          \/\/       ldumin  x21, xzr, [x5]\n-    __ ldumax(Assembler::xword, r27, r0, r17);         \/\/       ldumax  x27, x0, [x17]\n+    __ swp(Assembler::xword, r16, r27, r6);            \/\/       swp     x16, x27, [x6]\n+    __ ldadd(Assembler::xword, r9, r29, r15);          \/\/       ldadd   x9, x29, [x15]\n+    __ ldbic(Assembler::xword, r7, r4, r7);            \/\/       ldclr   x7, x4, [x7]\n+    __ ldeor(Assembler::xword, r15, r9, r23);          \/\/       ldeor   x15, x9, [x23]\n+    __ ldorr(Assembler::xword, r8, r2, r28);           \/\/       ldset   x8, x2, [x28]\n+    __ ldsmin(Assembler::xword, r21, zr, r5);          \/\/       ldsmin  x21, xzr, [x5]\n+    __ ldsmax(Assembler::xword, r27, r0, r17);         \/\/       ldsmax  x27, x0, [x17]\n+    __ ldumin(Assembler::xword, r15, r4, r26);         \/\/       ldumin  x15, x4, [x26]\n+    __ ldumax(Assembler::xword, r8, r28, r22);         \/\/       ldumax  x8, x28, [x22]\n@@ -1103,9 +1106,9 @@\n-    __ swpa(Assembler::xword, r15, r4, r26);           \/\/       swpa    x15, x4, [x26]\n-    __ ldadda(Assembler::xword, r8, r28, r22);         \/\/       ldadda  x8, x28, [x22]\n-    __ ldbica(Assembler::xword, r27, r27, r25);        \/\/       ldclra  x27, x27, [x25]\n-    __ ldeora(Assembler::xword, r23, r0, r4);          \/\/       ldeora  x23, x0, [x4]\n-    __ ldorra(Assembler::xword, r6, r16, r0);          \/\/       ldseta  x6, x16, [x0]\n-    __ ldsmina(Assembler::xword, r4, r15, r1);         \/\/       ldsmina x4, x15, [x1]\n-    __ ldsmaxa(Assembler::xword, r10, r7, r5);         \/\/       ldsmaxa x10, x7, [x5]\n-    __ ldumina(Assembler::xword, r10, r28, r7);        \/\/       ldumina x10, x28, [x7]\n-    __ ldumaxa(Assembler::xword, r20, r23, r21);       \/\/       ldumaxa x20, x23, [x21]\n+    __ swpa(Assembler::xword, r27, r27, r25);          \/\/       swpa    x27, x27, [x25]\n+    __ ldadda(Assembler::xword, r23, r0, r4);          \/\/       ldadda  x23, x0, [x4]\n+    __ ldbica(Assembler::xword, r6, r16, r0);          \/\/       ldclra  x6, x16, [x0]\n+    __ ldeora(Assembler::xword, r4, r15, r1);          \/\/       ldeora  x4, x15, [x1]\n+    __ ldorra(Assembler::xword, r10, r7, r5);          \/\/       ldseta  x10, x7, [x5]\n+    __ ldsmina(Assembler::xword, r10, r28, r7);        \/\/       ldsmina x10, x28, [x7]\n+    __ ldsmaxa(Assembler::xword, r20, r23, r21);       \/\/       ldsmaxa x20, x23, [x21]\n+    __ ldumina(Assembler::xword, r6, r11, r8);         \/\/       ldumina x6, x11, [x8]\n+    __ ldumaxa(Assembler::xword, r17, zr, r6);         \/\/       ldumaxa x17, xzr, [x6]\n@@ -1114,9 +1117,9 @@\n-    __ swpal(Assembler::xword, r6, r11, r8);           \/\/       swpal   x6, x11, [x8]\n-    __ ldaddal(Assembler::xword, r17, zr, r6);         \/\/       ldaddal x17, xzr, [x6]\n-    __ ldbical(Assembler::xword, r17, r2, r12);        \/\/       ldclral x17, x2, [x12]\n-    __ ldeoral(Assembler::xword, r30, r29, r3);        \/\/       ldeoral x30, x29, [x3]\n-    __ ldorral(Assembler::xword, r27, r22, r29);       \/\/       ldsetal x27, x22, [x29]\n-    __ ldsminal(Assembler::xword, r14, r13, r28);      \/\/       ldsminal        x14, x13, [x28]\n-    __ ldsmaxal(Assembler::xword, r17, r24, r5);       \/\/       ldsmaxal        x17, x24, [x5]\n-    __ lduminal(Assembler::xword, r2, r14, r10);       \/\/       lduminal        x2, x14, [x10]\n-    __ ldumaxal(Assembler::xword, r16, r11, r27);      \/\/       ldumaxal        x16, x11, [x27]\n+    __ swpal(Assembler::xword, r17, r2, r12);          \/\/       swpal   x17, x2, [x12]\n+    __ ldaddal(Assembler::xword, r30, r29, r3);        \/\/       ldaddal x30, x29, [x3]\n+    __ ldbical(Assembler::xword, r27, r22, r29);       \/\/       ldclral x27, x22, [x29]\n+    __ ldeoral(Assembler::xword, r14, r13, r28);       \/\/       ldeoral x14, x13, [x28]\n+    __ ldorral(Assembler::xword, r17, r24, r5);        \/\/       ldsetal x17, x24, [x5]\n+    __ ldsminal(Assembler::xword, r2, r14, r10);       \/\/       ldsminal        x2, x14, [x10]\n+    __ ldsmaxal(Assembler::xword, r16, r11, r27);      \/\/       ldsmaxal        x16, x11, [x27]\n+    __ lduminal(Assembler::xword, r23, r12, r4);       \/\/       lduminal        x23, x12, [x4]\n+    __ ldumaxal(Assembler::xword, r22, r17, r4);       \/\/       ldumaxal        x22, x17, [x4]\n@@ -1125,9 +1128,9 @@\n-    __ swpl(Assembler::xword, r23, r12, r4);           \/\/       swpl    x23, x12, [x4]\n-    __ ldaddl(Assembler::xword, r22, r17, r4);         \/\/       ldaddl  x22, x17, [x4]\n-    __ ldbicl(Assembler::xword, r1, r19, r16);         \/\/       ldclrl  x1, x19, [x16]\n-    __ ldeorl(Assembler::xword, r16, r13, r14);        \/\/       ldeorl  x16, x13, [x14]\n-    __ ldorrl(Assembler::xword, r12, r2, r17);         \/\/       ldsetl  x12, x2, [x17]\n-    __ ldsminl(Assembler::xword, r3, r21, r23);        \/\/       ldsminl x3, x21, [x23]\n-    __ ldsmaxl(Assembler::xword, r5, r6, r7);          \/\/       ldsmaxl x5, x6, [x7]\n-    __ lduminl(Assembler::xword, r19, r13, r28);       \/\/       lduminl x19, x13, [x28]\n-    __ ldumaxl(Assembler::xword, r17, r16, r6);        \/\/       ldumaxl x17, x16, [x6]\n+    __ swpl(Assembler::xword, r1, r19, r16);           \/\/       swpl    x1, x19, [x16]\n+    __ ldaddl(Assembler::xword, r16, r13, r14);        \/\/       ldaddl  x16, x13, [x14]\n+    __ ldbicl(Assembler::xword, r12, r2, r17);         \/\/       ldclrl  x12, x2, [x17]\n+    __ ldeorl(Assembler::xword, r3, r21, r23);         \/\/       ldeorl  x3, x21, [x23]\n+    __ ldorrl(Assembler::xword, r5, r6, r7);           \/\/       ldsetl  x5, x6, [x7]\n+    __ ldsminl(Assembler::xword, r19, r13, r28);       \/\/       ldsminl x19, x13, [x28]\n+    __ ldsmaxl(Assembler::xword, r17, r16, r6);        \/\/       ldsmaxl x17, x16, [x6]\n+    __ lduminl(Assembler::xword, r2, r29, r3);         \/\/       lduminl x2, x29, [x3]\n+    __ ldumaxl(Assembler::xword, r4, r6, r15);         \/\/       ldumaxl x4, x6, [x15]\n@@ -1136,9 +1139,9 @@\n-    __ swp(Assembler::word, r2, r29, r3);              \/\/       swp     w2, w29, [x3]\n-    __ ldadd(Assembler::word, r4, r6, r15);            \/\/       ldadd   w4, w6, [x15]\n-    __ ldbic(Assembler::word, r20, r13, r12);          \/\/       ldclr   w20, w13, [x12]\n-    __ ldeor(Assembler::word, r20, r8, r25);           \/\/       ldeor   w20, w8, [x25]\n-    __ ldorr(Assembler::word, r20, r19, r0);           \/\/       ldset   w20, w19, [x0]\n-    __ ldsmin(Assembler::word, r11, r24, r6);          \/\/       ldsmin  w11, w24, [x6]\n-    __ ldsmax(Assembler::word, r20, zr, r14);          \/\/       ldsmax  w20, wzr, [x14]\n-    __ ldumin(Assembler::word, r16, r6, r0);           \/\/       ldumin  w16, w6, [x0]\n-    __ ldumax(Assembler::word, r7, r15, r19);          \/\/       ldumax  w7, w15, [x19]\n+    __ swp(Assembler::word, r20, r13, r12);            \/\/       swp     w20, w13, [x12]\n+    __ ldadd(Assembler::word, r20, r8, r25);           \/\/       ldadd   w20, w8, [x25]\n+    __ ldbic(Assembler::word, r20, r19, r0);           \/\/       ldclr   w20, w19, [x0]\n+    __ ldeor(Assembler::word, r11, r24, r6);           \/\/       ldeor   w11, w24, [x6]\n+    __ ldorr(Assembler::word, r20, zr, r14);           \/\/       ldset   w20, wzr, [x14]\n+    __ ldsmin(Assembler::word, r16, r6, r0);           \/\/       ldsmin  w16, w6, [x0]\n+    __ ldsmax(Assembler::word, r7, r15, r19);          \/\/       ldsmax  w7, w15, [x19]\n+    __ ldumin(Assembler::word, r26, r9, r10);          \/\/       ldumin  w26, w9, [x10]\n+    __ ldumax(Assembler::word, r23, r21, r22);         \/\/       ldumax  w23, w21, [x22]\n@@ -1147,9 +1150,9 @@\n-    __ swpa(Assembler::word, r26, r9, r10);            \/\/       swpa    w26, w9, [x10]\n-    __ ldadda(Assembler::word, r23, r21, r22);         \/\/       ldadda  w23, w21, [x22]\n-    __ ldbica(Assembler::word, r28, r2, r3);           \/\/       ldclra  w28, w2, [x3]\n-    __ ldeora(Assembler::word, r15, r19, r20);         \/\/       ldeora  w15, w19, [x20]\n-    __ ldorra(Assembler::word, r7, r4, r29);           \/\/       ldseta  w7, w4, [x29]\n-    __ ldsmina(Assembler::word, r7, r0, r9);           \/\/       ldsmina w7, w0, [x9]\n-    __ ldsmaxa(Assembler::word, r16, r20, r23);        \/\/       ldsmaxa w16, w20, [x23]\n-    __ ldumina(Assembler::word, r4, r16, r10);         \/\/       ldumina w4, w16, [x10]\n-    __ ldumaxa(Assembler::word, r23, r11, r25);        \/\/       ldumaxa w23, w11, [x25]\n+    __ swpa(Assembler::word, r28, r2, r3);             \/\/       swpa    w28, w2, [x3]\n+    __ ldadda(Assembler::word, r15, r19, r20);         \/\/       ldadda  w15, w19, [x20]\n+    __ ldbica(Assembler::word, r7, r4, r29);           \/\/       ldclra  w7, w4, [x29]\n+    __ ldeora(Assembler::word, r7, r0, r9);            \/\/       ldeora  w7, w0, [x9]\n+    __ ldorra(Assembler::word, r16, r20, r23);         \/\/       ldseta  w16, w20, [x23]\n+    __ ldsmina(Assembler::word, r4, r16, r10);         \/\/       ldsmina w4, w16, [x10]\n+    __ ldsmaxa(Assembler::word, r23, r11, r25);        \/\/       ldsmaxa w23, w11, [x25]\n+    __ ldumina(Assembler::word, r6, zr, r16);          \/\/       ldumina w6, wzr, [x16]\n+    __ ldumaxa(Assembler::word, r13, r23, r12);        \/\/       ldumaxa w13, w23, [x12]\n@@ -1158,9 +1161,9 @@\n-    __ swpal(Assembler::word, r6, zr, r16);            \/\/       swpal   w6, wzr, [x16]\n-    __ ldaddal(Assembler::word, r13, r23, r12);        \/\/       ldaddal w13, w23, [x12]\n-    __ ldbical(Assembler::word, r1, r14, r9);          \/\/       ldclral w1, w14, [x9]\n-    __ ldeoral(Assembler::word, r21, r16, r26);        \/\/       ldeoral w21, w16, [x26]\n-    __ ldorral(Assembler::word, r15, r4, r4);          \/\/       ldsetal w15, w4, [x4]\n-    __ ldsminal(Assembler::word, r16, r8, r6);         \/\/       ldsminal        w16, w8, [x6]\n-    __ ldsmaxal(Assembler::word, r30, r4, r29);        \/\/       ldsmaxal        w30, w4, [x29]\n-    __ lduminal(Assembler::word, r17, r29, r26);       \/\/       lduminal        w17, w29, [x26]\n-    __ ldumaxal(Assembler::word, r9, r15, r2);         \/\/       ldumaxal        w9, w15, [x2]\n+    __ swpal(Assembler::word, r1, r14, r9);            \/\/       swpal   w1, w14, [x9]\n+    __ ldaddal(Assembler::word, r21, r16, r26);        \/\/       ldaddal w21, w16, [x26]\n+    __ ldbical(Assembler::word, r15, r4, r4);          \/\/       ldclral w15, w4, [x4]\n+    __ ldeoral(Assembler::word, r16, r8, r6);          \/\/       ldeoral w16, w8, [x6]\n+    __ ldorral(Assembler::word, r30, r4, r29);         \/\/       ldsetal w30, w4, [x29]\n+    __ ldsminal(Assembler::word, r17, r29, r26);       \/\/       ldsminal        w17, w29, [x26]\n+    __ ldsmaxal(Assembler::word, r9, r15, r2);         \/\/       ldsmaxal        w9, w15, [x2]\n+    __ lduminal(Assembler::word, r11, r29, r3);        \/\/       lduminal        w11, w29, [x3]\n+    __ ldumaxal(Assembler::word, r7, r1, r27);         \/\/       ldumaxal        w7, w1, [x27]\n@@ -1169,9 +1172,9 @@\n-    __ swpl(Assembler::word, r11, r29, r3);            \/\/       swpl    w11, w29, [x3]\n-    __ ldaddl(Assembler::word, r7, r1, r27);           \/\/       ldaddl  w7, w1, [x27]\n-    __ ldbicl(Assembler::word, r21, r16, r14);         \/\/       ldclrl  w21, w16, [x14]\n-    __ ldeorl(Assembler::word, r8, r16, r22);          \/\/       ldeorl  w8, w16, [x22]\n-    __ ldorrl(Assembler::word, r25, r5, r20);          \/\/       ldsetl  w25, w5, [x20]\n-    __ ldsminl(Assembler::word, r21, r16, r23);        \/\/       ldsminl w21, w16, [x23]\n-    __ ldsmaxl(Assembler::word, r16, r30, r20);        \/\/       ldsmaxl w16, w30, [x20]\n-    __ lduminl(Assembler::word, r20, r0, r4);          \/\/       lduminl w20, w0, [x4]\n-    __ ldumaxl(Assembler::word, r19, r24, r4);         \/\/       ldumaxl w19, w24, [x4]\n+    __ swpl(Assembler::word, r21, r16, r14);           \/\/       swpl    w21, w16, [x14]\n+    __ ldaddl(Assembler::word, r8, r16, r22);          \/\/       ldaddl  w8, w16, [x22]\n+    __ ldbicl(Assembler::word, r25, r5, r20);          \/\/       ldclrl  w25, w5, [x20]\n+    __ ldeorl(Assembler::word, r21, r16, r23);         \/\/       ldeorl  w21, w16, [x23]\n+    __ ldorrl(Assembler::word, r16, r30, r20);         \/\/       ldsetl  w16, w30, [x20]\n+    __ ldsminl(Assembler::word, r20, r0, r4);          \/\/       ldsminl w20, w0, [x4]\n+    __ ldsmaxl(Assembler::word, r19, r24, r4);         \/\/       ldsmaxl w19, w24, [x4]\n+    __ lduminl(Assembler::word, r20, r4, r24);         \/\/       lduminl w20, w4, [x24]\n+    __ ldumaxl(Assembler::word, r26, r19, r2);         \/\/       ldumaxl w26, w19, [x2]\n@@ -1180,4 +1183,4 @@\n-    __ bcax(v19, __ T16B, v4, v23, v25);               \/\/       bcax            v19.16B, v4.16B, v23.16B, v25.16B\n-    __ eor3(v19, __ T16B, v2, v8, v8);                 \/\/       eor3            v19.16B, v2.16B, v8.16B, v8.16B\n-    __ rax1(v14, __ T2D, v24, v17);                    \/\/       rax1            v14.2D, v24.2D, v17.2D\n-    __ xar(v30, __ T2D, v21, v4, 62);                  \/\/       xar             v30.2D, v21.2D, v4.2D, #62\n+    __ bcax(v8, __ T16B, v8, v14, v24);                \/\/       bcax            v8.16B, v8.16B, v14.16B, v24.16B\n+    __ eor3(v17, __ T16B, v30, v21, v4);               \/\/       eor3            v17.16B, v30.16B, v21.16B, v4.16B\n+    __ rax1(v30, __ T2D, v1, v10);                     \/\/       rax1            v30.2D, v1.2D, v10.2D\n+    __ xar(v19, __ T2D, v12, v0, 18);                  \/\/       xar             v19.2D, v12.2D, v0.2D, #18\n@@ -1186,4 +1189,4 @@\n-    __ sha512h(v1, __ T2D, v10, v19);                  \/\/       sha512h         q1, q10, v19.2D\n-    __ sha512h2(v12, __ T2D, v0, v9);                  \/\/       sha512h2                q12, q0, v9.2D\n-    __ sha512su0(v7, __ T2D, v24);                     \/\/       sha512su0               v7.2D, v24.2D\n-    __ sha512su1(v17, __ T2D, v4, v27);                \/\/       sha512su1               v17.2D, v4.2D, v27.2D\n+    __ sha512h(v7, __ T2D, v24, v17);                  \/\/       sha512h         q7, q24, v17.2D\n+    __ sha512h2(v4, __ T2D, v27, v6);                  \/\/       sha512h2                q4, q27, v6.2D\n+    __ sha512su0(v9, __ T2D, v27);                     \/\/       sha512su0               v9.2D, v27.2D\n+    __ sha512su1(v23, __ T2D, v13, v16);               \/\/       sha512su1               v23.2D, v13.2D, v16.2D\n@@ -1192,5 +1195,5 @@\n-    __ sve_add(z6, __ H, 223u);                        \/\/       add     z6.h, z6.h, #0xdf\n-    __ sve_sub(z23, __ H, 132u);                       \/\/       sub     z23.h, z23.h, #0x84\n-    __ sve_and(z30, __ S, 4160749823u);                \/\/       and     z30.s, z30.s, #0xf80000ff\n-    __ sve_eor(z30, __ D, 2017612633061982208u);       \/\/       eor     z30.d, z30.d, #0x1c00000000000000\n-    __ sve_orr(z19, __ B, 243u);                       \/\/       orr     z19.b, z19.b, #0xf3\n+    __ sve_add(z30, __ S, 183u);                       \/\/       add     z30.s, z30.s, #0xb7\n+    __ sve_sub(z20, __ D, 236u);                       \/\/       sub     z20.d, z20.d, #0xec\n+    __ sve_and(z9, __ H, 1008u);                       \/\/       and     z9.h, z9.h, #0x3f0\n+    __ sve_eor(z20, __ D, 72057594037895168u);         \/\/       eor     z20.d, z20.d, #0xffffffffff8000\n+    __ sve_orr(z13, __ H, 56u);                        \/\/       orr     z13.h, z13.h, #0x38\n@@ -1199,5 +1202,5 @@\n-    __ sve_add(z9, __ H, 115u);                        \/\/       add     z9.h, z9.h, #0x73\n-    __ sve_sub(z11, __ S, 13u);                        \/\/       sub     z11.s, z11.s, #0xd\n-    __ sve_and(z24, __ H, 32256u);                     \/\/       and     z24.h, z24.h, #0x7e00\n-    __ sve_eor(z17, __ S, 917504u);                    \/\/       eor     z17.s, z17.s, #0xe0000\n-    __ sve_orr(z0, __ B, 96u);                         \/\/       orr     z0.b, z0.b, #0x60\n+    __ sve_add(z24, __ H, 159u);                       \/\/       add     z24.h, z24.h, #0x9f\n+    __ sve_sub(z13, __ S, 145u);                       \/\/       sub     z13.s, z13.s, #0x91\n+    __ sve_and(z16, __ B, 12u);                        \/\/       and     z16.b, z16.b, #0xc\n+    __ sve_eor(z11, __ H, 32256u);                     \/\/       eor     z11.h, z11.h, #0x7e00\n+    __ sve_orr(z15, __ B, 254u);                       \/\/       orr     z15.b, z15.b, #0xfe\n@@ -1206,5 +1209,5 @@\n-    __ sve_add(z15, __ H, 131u);                       \/\/       add     z15.h, z15.h, #0x83\n-    __ sve_sub(z4, __ H, 243u);                        \/\/       sub     z4.h, z4.h, #0xf3\n-    __ sve_and(z5, __ B, 191u);                        \/\/       and     z5.b, z5.b, #0xbf\n-    __ sve_eor(z26, __ B, 96u);                        \/\/       eor     z26.b, z26.b, #0x60\n-    __ sve_orr(z19, __ D, 18446532967477018623u);      \/\/       orr     z19.d, z19.d, #0xffff3fffffffffff\n+    __ sve_add(z5, __ B, 86u);                         \/\/       add     z5.b, z5.b, #0x56\n+    __ sve_sub(z21, __ D, 27u);                        \/\/       sub     z21.d, z21.d, #0x1b\n+    __ sve_and(z0, __ H, 64519u);                      \/\/       and     z0.h, z0.h, #0xfc07\n+    __ sve_eor(z10, __ D, 18374686479671656447u);      \/\/       eor     z10.d, z10.d, #0xff00000000007fff\n+    __ sve_orr(z7, __ D, 2017612633061982208u);        \/\/       orr     z7.d, z7.d, #0x1c00000000000000\n@@ -1213,5 +1216,5 @@\n-    __ sve_add(z3, __ S, 63u);                         \/\/       add     z3.s, z3.s, #0x3f\n-    __ sve_sub(z23, __ D, 113u);                       \/\/       sub     z23.d, z23.d, #0x71\n-    __ sve_and(z21, __ H, 16368u);                     \/\/       and     z21.h, z21.h, #0x3ff0\n-    __ sve_eor(z17, __ D, 2017612633061982208u);       \/\/       eor     z17.d, z17.d, #0x1c00000000000000\n-    __ sve_orr(z2, __ D, 18437736874454811647u);       \/\/       orr     z2.d, z2.d, #0xffe00000000003ff\n+    __ sve_add(z21, __ H, 217u);                       \/\/       add     z21.h, z21.h, #0xd9\n+    __ sve_sub(z12, __ S, 252u);                       \/\/       sub     z12.s, z12.s, #0xfc\n+    __ sve_and(z17, __ H, 65283u);                     \/\/       and     z17.h, z17.h, #0xff03\n+    __ sve_eor(z16, __ S, 8388600u);                   \/\/       eor     z16.s, z16.s, #0x7ffff8\n+    __ sve_orr(z19, __ B, 243u);                       \/\/       orr     z19.b, z19.b, #0xf3\n@@ -1220,5 +1223,5 @@\n-    __ sve_add(z20, __ B, 163u);                       \/\/       add     z20.b, z20.b, #0xa3\n-    __ sve_sub(z2, __ B, 215u);                        \/\/       sub     z2.b, z2.b, #0xd7\n-    __ sve_and(z17, __ H, 33279u);                     \/\/       and     z17.h, z17.h, #0x81ff\n-    __ sve_eor(z21, __ B, 12u);                        \/\/       eor     z21.b, z21.b, #0xc\n-    __ sve_orr(z23, __ H, 8064u);                      \/\/       orr     z23.h, z23.h, #0x1f80\n+    __ sve_add(z17, __ H, 134u);                       \/\/       add     z17.h, z17.h, #0x86\n+    __ sve_sub(z17, __ S, 0u);                         \/\/       sub     z17.s, z17.s, #0x0\n+    __ sve_and(z4, __ B, 128u);                        \/\/       and     z4.b, z4.b, #0x80\n+    __ sve_eor(z6, __ H, 32256u);                      \/\/       eor     z6.h, z6.h, #0x7e00\n+    __ sve_orr(z16, __ D, 4160749568u);                \/\/       orr     z16.d, z16.d, #0xf8000000\n@@ -1227,5 +1230,5 @@\n-    __ sve_add(z20, __ H, 139u);                       \/\/       add     z20.h, z20.h, #0x8b\n-    __ sve_sub(z29, __ H, 26u);                        \/\/       sub     z29.h, z29.h, #0x1a\n-    __ sve_and(z3, __ S, 122880u);                     \/\/       and     z3.s, z3.s, #0x1e000\n-    __ sve_eor(z24, __ D, 18158513714670600195u);      \/\/       eor     z24.d, z24.d, #0xfc000003fc000003\n-    __ sve_orr(z22, __ B, 191u);                       \/\/       orr     z22.b, z22.b, #0xbf\n+    __ sve_add(z3, __ S, 79u);                         \/\/       add     z3.s, z3.s, #0x4f\n+    __ sve_sub(z3, __ D, 227u);                        \/\/       sub     z3.d, z3.d, #0xe3\n+    __ sve_and(z3, __ S, 16744448u);                   \/\/       and     z3.s, z3.s, #0xff8000\n+    __ sve_eor(z25, __ S, 917504u);                    \/\/       eor     z25.s, z25.s, #0xe0000\n+    __ sve_orr(z7, __ D, 18442240474082197503u);       \/\/       orr     z7.d, z7.d, #0xfff0000000003fff\n@@ -1234,56 +1237,56 @@\n-    __ sve_add(z13, __ D, z5, z7);                     \/\/       add     z13.d, z5.d, z7.d\n-    __ sve_sub(z5, __ S, z21, z17);                    \/\/       sub     z5.s, z21.s, z17.s\n-    __ sve_fadd(z0, __ D, z3, z9);                     \/\/       fadd    z0.d, z3.d, z9.d\n-    __ sve_fmul(z11, __ S, z7, z11);                   \/\/       fmul    z11.s, z7.s, z11.s\n-    __ sve_fsub(z17, __ S, z17, z11);                  \/\/       fsub    z17.s, z17.s, z11.s\n-    __ sve_abs(z24, __ S, p4, z30);                    \/\/       abs     z24.s, p4\/m, z30.s\n-    __ sve_add(z8, __ D, p4, z14);                     \/\/       add     z8.d, p4\/m, z8.d, z14.d\n-    __ sve_and(z22, __ H, p7, z22);                    \/\/       and     z22.h, p7\/m, z22.h, z22.h\n-    __ sve_asr(z8, __ D, p1, z27);                     \/\/       asr     z8.d, p1\/m, z8.d, z27.d\n-    __ sve_bic(z10, __ D, p0, z14);                    \/\/       bic     z10.d, p0\/m, z10.d, z14.d\n-    __ sve_clz(z21, __ B, p5, z0);                     \/\/       clz     z21.b, p5\/m, z0.b\n-    __ sve_cnt(z22, __ D, p6, z5);                     \/\/       cnt     z22.d, p6\/m, z5.d\n-    __ sve_eor(z29, __ B, p4, z17);                    \/\/       eor     z29.b, p4\/m, z29.b, z17.b\n-    __ sve_lsl(z12, __ H, p3, z29);                    \/\/       lsl     z12.h, p3\/m, z12.h, z29.h\n-    __ sve_lsr(z0, __ D, p4, z2);                      \/\/       lsr     z0.d, p4\/m, z0.d, z2.d\n-    __ sve_mul(z20, __ D, p5, z21);                    \/\/       mul     z20.d, p5\/m, z20.d, z21.d\n-    __ sve_neg(z12, __ B, p2, z2);                     \/\/       neg     z12.b, p2\/m, z2.b\n-    __ sve_not(z14, __ B, p5, z22);                    \/\/       not     z14.b, p5\/m, z22.b\n-    __ sve_orr(z19, __ D, p6, z26);                    \/\/       orr     z19.d, p6\/m, z19.d, z26.d\n-    __ sve_rbit(z12, __ B, p5, z21);                   \/\/       rbit    z12.b, p5\/m, z21.b\n-    __ sve_revb(z1, __ S, p2, z19);                    \/\/       revb    z1.s, p2\/m, z19.s\n-    __ sve_smax(z19, __ H, p6, z23);                   \/\/       smax    z19.h, p6\/m, z19.h, z23.h\n-    __ sve_smin(z30, __ S, p4, z19);                   \/\/       smin    z30.s, p4\/m, z30.s, z19.s\n-    __ sve_sub(z20, __ H, p1, z20);                    \/\/       sub     z20.h, p1\/m, z20.h, z20.h\n-    __ sve_fabs(z30, __ D, p5, z30);                   \/\/       fabs    z30.d, p5\/m, z30.d\n-    __ sve_fadd(z25, __ S, p4, z17);                   \/\/       fadd    z25.s, p4\/m, z25.s, z17.s\n-    __ sve_fdiv(z11, __ D, p3, z28);                   \/\/       fdiv    z11.d, p3\/m, z11.d, z28.d\n-    __ sve_fmax(z5, __ S, p0, z13);                    \/\/       fmax    z5.s, p0\/m, z5.s, z13.s\n-    __ sve_fmin(z2, __ S, p1, z10);                    \/\/       fmin    z2.s, p1\/m, z2.s, z10.s\n-    __ sve_fmul(z19, __ S, p1, z25);                   \/\/       fmul    z19.s, p1\/m, z19.s, z25.s\n-    __ sve_fneg(z2, __ S, p0, z29);                    \/\/       fneg    z2.s, p0\/m, z29.s\n-    __ sve_frintm(z20, __ D, p1, z20);                 \/\/       frintm  z20.d, p1\/m, z20.d\n-    __ sve_frintn(z28, __ S, p3, z13);                 \/\/       frintn  z28.s, p3\/m, z13.s\n-    __ sve_frintp(z13, __ S, p7, z1);                  \/\/       frintp  z13.s, p7\/m, z1.s\n-    __ sve_fsqrt(z27, __ D, p0, z3);                   \/\/       fsqrt   z27.d, p0\/m, z3.d\n-    __ sve_fsub(z8, __ S, p6, z9);                     \/\/       fsub    z8.s, p6\/m, z8.s, z9.s\n-    __ sve_fmad(z25, __ D, p2, z14, z1);               \/\/       fmad    z25.d, p2\/m, z14.d, z1.d\n-    __ sve_fmla(z25, __ D, p1, z28, z19);              \/\/       fmla    z25.d, p1\/m, z28.d, z19.d\n-    __ sve_fmls(z6, __ D, p7, z13, z1);                \/\/       fmls    z6.d, p7\/m, z13.d, z1.d\n-    __ sve_fmsb(z11, __ S, p2, z1, z1);                \/\/       fmsb    z11.s, p2\/m, z1.s, z1.s\n-    __ sve_fnmad(z27, __ S, p6, z14, z2);              \/\/       fnmad   z27.s, p6\/m, z14.s, z2.s\n-    __ sve_fnmsb(z29, __ S, p4, z24, z2);              \/\/       fnmsb   z29.s, p4\/m, z24.s, z2.s\n-    __ sve_fnmla(z24, __ S, p0, z25, z28);             \/\/       fnmla   z24.s, p0\/m, z25.s, z28.s\n-    __ sve_fnmls(z3, __ D, p5, z13, z15);              \/\/       fnmls   z3.d, p5\/m, z13.d, z15.d\n-    __ sve_mla(z16, __ S, p1, z11, z26);               \/\/       mla     z16.s, p1\/m, z11.s, z26.s\n-    __ sve_mls(z2, __ B, p4, z1, z27);                 \/\/       mls     z2.b, p4\/m, z1.b, z27.b\n-    __ sve_and(z22, z30, z27);                         \/\/       and     z22.d, z30.d, z27.d\n-    __ sve_eor(z10, z21, z16);                         \/\/       eor     z10.d, z21.d, z16.d\n-    __ sve_orr(z7, z21, z4);                           \/\/       orr     z7.d, z21.d, z4.d\n-    __ sve_bic(z24, z11, z8);                          \/\/       bic     z24.d, z11.d, z8.d\n-    __ sve_uzp1(z11, __ S, z0, z4);                    \/\/       uzp1    z11.s, z0.s, z4.s\n-    __ sve_uzp2(z21, __ B, z20, z4);                   \/\/       uzp2    z21.b, z20.b, z4.b\n-    __ sve_fabd(z15, __ D, p3, z3);                    \/\/       fabd    z15.d, p3\/m, z15.d, z3.d\n-    __ sve_bext(z25, __ S, z27, z5);                   \/\/       bext    z25.s, z27.s, z5.s\n-    __ sve_bdep(z25, __ B, z10, z30);                  \/\/       bdep    z25.b, z10.b, z30.b\n-    __ sve_eor3(z24, z4, z7);                          \/\/       eor3    z24.d, z24.d, z4.d, z7.d\n+    __ sve_add(z17, __ B, z17, z0);                    \/\/       add     z17.b, z17.b, z0.b\n+    __ sve_sub(z9, __ H, z19, z11);                    \/\/       sub     z9.h, z19.h, z11.h\n+    __ sve_fadd(z11, __ D, z14, z17);                  \/\/       fadd    z11.d, z14.d, z17.d\n+    __ sve_fmul(z11, __ D, z13, z24);                  \/\/       fmul    z11.d, z13.d, z24.d\n+    __ sve_fsub(z30, __ D, z17, z8);                   \/\/       fsub    z30.d, z17.d, z8.d\n+    __ sve_abs(z14, __ D, p6, z22);                    \/\/       abs     z14.d, p6\/m, z22.d\n+    __ sve_add(z22, __ B, p2, z8);                     \/\/       add     z22.b, p2\/m, z22.b, z8.b\n+    __ sve_and(z27, __ B, p7, z10);                    \/\/       and     z27.b, p7\/m, z27.b, z10.b\n+    __ sve_asr(z14, __ S, p6, z21);                    \/\/       asr     z14.s, p6\/m, z14.s, z21.s\n+    __ sve_bic(z0, __ D, p0, z22);                     \/\/       bic     z0.d, p0\/m, z0.d, z22.d\n+    __ sve_clz(z5, __ S, p6, z29);                     \/\/       clz     z5.s, p6\/m, z29.s\n+    __ sve_cnt(z17, __ H, p0, z12);                    \/\/       cnt     z17.h, p0\/m, z12.h\n+    __ sve_eor(z29, __ S, p3, z0);                     \/\/       eor     z29.s, p3\/m, z29.s, z0.s\n+    __ sve_lsl(z2, __ S, p7, z20);                     \/\/       lsl     z2.s, p7\/m, z2.s, z20.s\n+    __ sve_lsr(z21, __ H, p7, z12);                    \/\/       lsr     z21.h, p7\/m, z21.h, z12.h\n+    __ sve_mul(z2, __ S, p0, z14);                     \/\/       mul     z2.s, p0\/m, z2.s, z14.s\n+    __ sve_neg(z22, __ D, p0, z19);                    \/\/       neg     z22.d, p0\/m, z19.d\n+    __ sve_not(z26, __ S, p6, z12);                    \/\/       not     z26.s, p6\/m, z12.s\n+    __ sve_orr(z21, __ H, p0, z1);                     \/\/       orr     z21.h, p0\/m, z21.h, z1.h\n+    __ sve_rbit(z19, __ D, p3, z19);                   \/\/       rbit    z19.d, p3\/m, z19.d\n+    __ sve_revb(z23, __ S, p2, z30);                   \/\/       revb    z23.s, p2\/m, z30.s\n+    __ sve_smax(z19, __ B, p5, z20);                   \/\/       smax    z19.b, p5\/m, z19.b, z20.b\n+    __ sve_smin(z20, __ S, p3, z30);                   \/\/       smin    z20.s, p3\/m, z20.s, z30.s\n+    __ sve_sub(z30, __ S, p7, z25);                    \/\/       sub     z30.s, p7\/m, z30.s, z25.s\n+    __ sve_fabs(z17, __ S, p3, z11);                   \/\/       fabs    z17.s, p3\/m, z11.s\n+    __ sve_fadd(z28, __ S, p5, z5);                    \/\/       fadd    z28.s, p5\/m, z28.s, z5.s\n+    __ sve_fdiv(z13, __ S, p3, z2);                    \/\/       fdiv    z13.s, p3\/m, z13.s, z2.s\n+    __ sve_fmax(z10, __ S, p3, z19);                   \/\/       fmax    z10.s, p3\/m, z10.s, z19.s\n+    __ sve_fmin(z25, __ S, p3, z2);                    \/\/       fmin    z25.s, p3\/m, z25.s, z2.s\n+    __ sve_fmul(z29, __ S, p0, z20);                   \/\/       fmul    z29.s, p0\/m, z29.s, z20.s\n+    __ sve_fneg(z20, __ S, p7, z28);                   \/\/       fneg    z20.s, p7\/m, z28.s\n+    __ sve_frintm(z13, __ D, p2, z13);                 \/\/       frintm  z13.d, p2\/m, z13.d\n+    __ sve_frintn(z1, __ S, p3, z27);                  \/\/       frintn  z1.s, p3\/m, z27.s\n+    __ sve_frintp(z3, __ D, p6, z8);                   \/\/       frintp  z3.d, p6\/m, z8.d\n+    __ sve_fsqrt(z9, __ S, p0, z25);                   \/\/       fsqrt   z9.s, p0\/m, z25.s\n+    __ sve_fsub(z14, __ D, p0, z20);                   \/\/       fsub    z14.d, p0\/m, z14.d, z20.d\n+    __ sve_fmad(z6, __ S, p7, z19, z16);               \/\/       fmad    z6.s, p7\/m, z19.s, z16.s\n+    __ sve_fmla(z27, __ S, p3, z1, z28);               \/\/       fmla    z27.s, p3\/m, z1.s, z28.s\n+    __ sve_fmls(z9, __ D, p0, z1, z1);                 \/\/       fmls    z9.d, p0\/m, z1.d, z1.d\n+    __ sve_fmsb(z26, __ D, p3, z2, z4);                \/\/       fmsb    z26.d, p3\/m, z2.d, z4.d\n+    __ sve_fnmad(z17, __ D, p6, z2, z2);               \/\/       fnmad   z17.d, p6\/m, z2.d, z2.d\n+    __ sve_fnmsb(z3, __ S, p6, z28, z13);              \/\/       fnmsb   z3.s, p6\/m, z28.s, z13.s\n+    __ sve_fnmla(z22, __ D, p3, z15, z27);             \/\/       fnmla   z22.d, p3\/m, z15.d, z27.d\n+    __ sve_fnmls(z4, __ S, p2, z26, z15);              \/\/       fnmls   z4.s, p2\/m, z26.s, z15.s\n+    __ sve_mla(z15, __ S, p0, z27, z7);                \/\/       mla     z15.s, p0\/m, z27.s, z7.s\n+    __ sve_mls(z30, __ S, p7, z16, z10);               \/\/       mls     z30.s, p7\/m, z16.s, z10.s\n+    __ sve_and(z16, z28, z7);                          \/\/       and     z16.d, z28.d, z7.d\n+    __ sve_eor(z4, z12, z24);                          \/\/       eor     z4.d, z12.d, z24.d\n+    __ sve_orr(z8, z10, z11);                          \/\/       orr     z8.d, z10.d, z11.d\n+    __ sve_bic(z4, z22, z21);                          \/\/       bic     z4.d, z22.d, z21.d\n+    __ sve_uzp1(z4, __ H, z3, z15);                    \/\/       uzp1    z4.h, z3.h, z15.h\n+    __ sve_uzp2(z3, __ D, z29, z25);                   \/\/       uzp2    z3.d, z29.d, z25.d\n+    __ sve_fabd(z5, __ S, p5, z25);                    \/\/       fabd    z5.s, p5\/m, z5.s, z25.s\n+    __ sve_bext(z30, __ B, z2, z24);                   \/\/       bext    z30.b, z2.b, z24.b\n+    __ sve_bdep(z7, __ S, z5, z3);                     \/\/       bdep    z7.s, z5.s, z3.s\n+    __ sve_eor3(z7, z24, z23);                         \/\/       eor3    z7.d, z7.d, z24.d, z23.d\n@@ -1292,9 +1295,9 @@\n-    __ sve_andv(v3, __ D, p5, z7);                     \/\/       andv d3, p5, z7.d\n-    __ sve_orv(v23, __ D, p7, z24);                    \/\/       orv d23, p7, z24.d\n-    __ sve_eorv(v17, __ H, p0, z10);                   \/\/       eorv h17, p0, z10.h\n-    __ sve_smaxv(v29, __ D, p3, z8);                   \/\/       smaxv d29, p3, z8.d\n-    __ sve_sminv(v28, __ S, p0, z30);                  \/\/       sminv s28, p0, z30.s\n-    __ sve_fminv(v30, __ D, p5, z0);                   \/\/       fminv d30, p5, z0.d\n-    __ sve_fmaxv(v7, __ D, p7, z28);                   \/\/       fmaxv d7, p7, z28.d\n-    __ sve_fadda(v21, __ D, p2, z28);                  \/\/       fadda d21, p2, d21, z28.d\n-    __ sve_uaddv(v19, __ S, p1, z6);                   \/\/       uaddv d19, p1, z6.s\n+    __ sve_andv(v24, __ B, p7, z17);                   \/\/       andv b24, p7, z17.b\n+    __ sve_orv(v10, __ H, p3, z29);                    \/\/       orv h10, p3, z29.h\n+    __ sve_eorv(v8, __ B, p6, z28);                    \/\/       eorv b8, p6, z28.b\n+    __ sve_smaxv(v30, __ S, p5, z30);                  \/\/       smaxv s30, p5, z30.s\n+    __ sve_sminv(v0, __ D, p5, z7);                    \/\/       sminv d0, p5, z7.d\n+    __ sve_fminv(v28, __ S, p6, z21);                  \/\/       fminv s28, p6, z21.s\n+    __ sve_fmaxv(v28, __ S, p6, z19);                  \/\/       fmaxv s28, p6, z19.s\n+    __ sve_fadda(v6, __ S, p4, z17);                   \/\/       fadda s6, p4, s6, z17.s\n+    __ sve_uaddv(v25, __ B, p5, z7);                   \/\/       uaddv d25, p5, z7.b\n@@ -1319,7 +1322,7 @@\n-    0x14000000,     0x17ffffd7,     0x1400043a,     0x94000000,\n-    0x97ffffd4,     0x94000437,     0x3400000a,     0x34fffa2a,\n-    0x3400868a,     0x35000008,     0x35fff9c8,     0x35008628,\n-    0xb400000b,     0xb4fff96b,     0xb40085cb,     0xb500001d,\n-    0xb5fff91d,     0xb500857d,     0x10000013,     0x10fff8b3,\n-    0x10008513,     0x90000013,     0x36300016,     0x3637f836,\n-    0x36308496,     0x3758000c,     0x375ff7cc,     0x3758842c,\n+    0x14000000,     0x17ffffd7,     0x1400043d,     0x94000000,\n+    0x97ffffd4,     0x9400043a,     0x3400000a,     0x34fffa2a,\n+    0x340086ea,     0x35000008,     0x35fff9c8,     0x35008688,\n+    0xb400000b,     0xb4fff96b,     0xb400862b,     0xb500001d,\n+    0xb5fff91d,     0xb50085dd,     0x10000013,     0x10fff8b3,\n+    0x10008573,     0x90000013,     0x36300016,     0x3637f836,\n+    0x363084f6,     0x3758000c,     0x375ff7cc,     0x3758848c,\n@@ -1330,13 +1333,13 @@\n-    0x54008200,     0x54000001,     0x54fff541,     0x540081a1,\n-    0x54000002,     0x54fff4e2,     0x54008142,     0x54000002,\n-    0x54fff482,     0x540080e2,     0x54000003,     0x54fff423,\n-    0x54008083,     0x54000003,     0x54fff3c3,     0x54008023,\n-    0x54000004,     0x54fff364,     0x54007fc4,     0x54000005,\n-    0x54fff305,     0x54007f65,     0x54000006,     0x54fff2a6,\n-    0x54007f06,     0x54000007,     0x54fff247,     0x54007ea7,\n-    0x54000008,     0x54fff1e8,     0x54007e48,     0x54000009,\n-    0x54fff189,     0x54007de9,     0x5400000a,     0x54fff12a,\n-    0x54007d8a,     0x5400000b,     0x54fff0cb,     0x54007d2b,\n-    0x5400000c,     0x54fff06c,     0x54007ccc,     0x5400000d,\n-    0x54fff00d,     0x54007c6d,     0x5400000e,     0x54ffefae,\n-    0x54007c0e,     0x5400000f,     0x54ffef4f,     0x54007baf,\n+    0x54008260,     0x54000001,     0x54fff541,     0x54008201,\n+    0x54000002,     0x54fff4e2,     0x540081a2,     0x54000002,\n+    0x54fff482,     0x54008142,     0x54000003,     0x54fff423,\n+    0x540080e3,     0x54000003,     0x54fff3c3,     0x54008083,\n+    0x54000004,     0x54fff364,     0x54008024,     0x54000005,\n+    0x54fff305,     0x54007fc5,     0x54000006,     0x54fff2a6,\n+    0x54007f66,     0x54000007,     0x54fff247,     0x54007f07,\n+    0x54000008,     0x54fff1e8,     0x54007ea8,     0x54000009,\n+    0x54fff189,     0x54007e49,     0x5400000a,     0x54fff12a,\n+    0x54007dea,     0x5400000b,     0x54fff0cb,     0x54007d8b,\n+    0x5400000c,     0x54fff06c,     0x54007d2c,     0x5400000d,\n+    0x54fff00d,     0x54007ccd,     0x5400000e,     0x54ffefae,\n+    0x54007c6e,     0x5400000f,     0x54ffef4f,     0x54007c0f,\n@@ -1415,176 +1418,176 @@\n-    0x1e624385,     0x1ee0c236,     0x1ee1426d,     0x1ee1c373,\n-    0x1e3800d1,     0x9e3800ed,     0x1e78035c,     0x9e7800d1,\n-    0x1ef80081,     0x9ef8028d,     0x1e2202a6,     0x9e2202fa,\n-    0x1e62028d,     0x9e62037e,     0x1e2402aa,     0x9e640225,\n-    0x1e3001ab,     0x9e70028d,     0x1e2601da,     0x9e6602e4,\n-    0x1e2703b7,     0x9e6701cc,     0x1e3b2200,     0x1e6022a0,\n-    0x1e2020c8,     0x1e602348,     0x293c30db,     0x29602e6e,\n-    0x697a31e0,     0xa9025ee9,     0xa975134f,     0x29ac20d1,\n-    0x29f20887,     0x69fe26ce,     0xa9b0530d,     0xa9c62d48,\n-    0x28b21618,     0x28f06920,     0x68f00a38,     0xa8bd45da,\n-    0xa8c357be,     0x28325d51,     0x286c1bda,     0xa804229e,\n-    0xa8437536,     0x0c40702c,     0x4cdfa200,     0x0cd36f35,\n-    0x4cdf2759,     0x0d40c1e9,     0x4ddfcb2c,     0x0dddcdc4,\n-    0x4c408e31,     0x0cdf8776,     0x4d60c302,     0x0dffc80a,\n-    0x4df6cd82,     0x4ccd49b4,     0x0c40496c,     0x4d40e5d1,\n-    0x4ddfeab9,     0x0ddbee14,     0x4cdf0420,     0x0cdd0360,\n-    0x0d60e231,     0x0dffe705,     0x0df4e8bb,     0x0e31b820,\n-    0x4e31bab4,     0x0e71bbbc,     0x4e71ba0f,     0x4eb1b9ac,\n-    0x0e30a96a,     0x4e30abbc,     0x0e70abbc,     0x4e70aa93,\n-    0x4eb0aaf6,     0x6e30f96a,     0x0e31a8a4,     0x2e31abfe,\n-    0x4e31aab4,     0x6e31a928,     0x0e71abfe,     0x2e71aa51,\n-    0x4e71a96a,     0x6e71ab9b,     0x4eb1a862,     0x6eb1ab38,\n-    0x6eb0f8a4,     0x7e30f883,     0x7e70f928,     0x7eb0faf6,\n-    0x7ef0fa51,     0x0ea0c9cd,     0x4ea0c8a4,     0x4ee0cbbc,\n-    0x2ea0cb17,     0x6ea0cad5,     0x6ee0cb59,     0x0ea0db38,\n-    0x4ea0d883,     0x4ee0db17,     0x0ea0eb7a,     0x4ea0eb17,\n-    0x4ee0e9ee,     0x2ea0dad5,     0x6ea0d883,     0x6ee0db17,\n-    0x0e20b928,     0x4e20bb38,     0x0e60ba93,     0x4e60ba0f,\n-    0x0ea0ba30,     0x4ea0b862,     0x4ee0b841,     0x0ea0f820,\n-    0x4ea0fb38,     0x4ee0f8a4,     0x2ea0f883,     0x6ea0f98b,\n-    0x6ee0fbfe,     0x2ea1fb9b,     0x6ea1f949,     0x6ee1fb59,\n-    0x2e205862,     0x6e2059ac,     0x0e331e51,     0x4e201ffe,\n-    0x0ea31c41,     0x4eae1dac,     0x2e3e1fbc,     0x6e221c20,\n-    0x0e338651,     0x4e2e85ac,     0x0e738651,     0x4e7786d5,\n-    0x0eae85ac,     0x4ebd879b,     0x4eff87dd,     0x0e20d7fe,\n-    0x4e23d441,     0x4e7bd759,     0x2e3d879b,     0x6e2684a4,\n-    0x2e7f87dd,     0x6e658483,     0x2ea884e6,     0x6ebf87dd,\n-    0x6efb8759,     0x0eb3d651,     0x4eaad528,     0x4ee9d507,\n-    0x0e2e9dac,     0x4e229c20,     0x0e759e93,     0x4e639c41,\n-    0x0eb99f17,     0x4ea49c62,     0x2ea2d420,     0x6eaad528,\n-    0x6ef9d717,     0x2e3bd759,     0x6e31d60f,     0x6e7fd7dd,\n-    0x2e25dc83,     0x6e2cdd6a,     0x6e78def6,     0x0e6c956a,\n-    0x4e6694a4,     0x0eb39651,     0x4ea39441,     0x0e2dcd8b,\n-    0x4e29cd07,     0x4e6ccd6a,     0x2e71960f,     0x6e729630,\n-    0x2ea49462,     0x6eab9549,     0x0eadcd8b,     0x4eaecdac,\n-    0x4ef0cdee,     0x2e2ffdcd,     0x6e24fc62,     0x6e68fce6,\n-    0x0e356693,     0x4e3b6759,     0x0e71660f,     0x4e6664a4,\n-    0x0ea46462,     0x4ea664a4,     0x0e2da58b,     0x4e33a651,\n-    0x0e76a6b4,     0x4e72a630,     0x0eb3a651,     0x4eaca56a,\n-    0x0e36f6b4,     0x4e38f6f6,     0x4e6ef5ac,     0x0e3b6f59,\n-    0x4e396f17,     0x0e7e6fbc,     0x4e706dee,     0x0eac6d6a,\n-    0x4eba6f38,     0x0e23ac41,     0x4e2dad8b,     0x0e60affe,\n-    0x4e6cad6a,     0x0eb1ae0f,     0x4ea9ad07,     0x0ea4f462,\n-    0x4ea5f483,     0x4eeff5cd,     0x2eb5ee93,     0x6eb2ee30,\n-    0x6ef2ee30,     0x0e253483,     0x4e233441,     0x0e6d358b,\n-    0x4e7f37dd,     0x0ea734c5,     0x4eaa3528,     0x4ef035ee,\n-    0x0e3e3fbc,     0x4e3f3fdd,     0x0e623c20,     0x4e763eb4,\n-    0x0ea93d07,     0x4eb63eb4,     0x4ef93f17,     0x2e3d8f9b,\n-    0x6e378ed5,     0x2e7c8f7a,     0x6e7a8f38,     0x2ea68ca4,\n-    0x6ea38c41,     0x6ef88ef6,     0x2e323630,     0x6e2037fe,\n-    0x2e6734c5,     0x6e6d358b,     0x2eaa3528,     0x6ebd379b,\n-    0x6ef035ee,     0x2e3e3fbc,     0x6e373ed5,     0x2e603ffe,\n-    0x6e733e51,     0x2ea03ffe,     0x6ea73cc5,     0x6eef3dcd,\n-    0x0e33e651,     0x4e20e7fe,     0x4e73e651,     0x2ebce77a,\n-    0x6eb5e693,     0x6ef1e60f,     0x2e2ee5ac,     0x6e2de58b,\n-    0x6e6be549,     0x65d23ba3,     0x65902bbe,     0x6590392a,\n-    0x65d12208,     0x65d131de,     0x65932ec9,     0x259d8983,\n-    0x25d5103b,     0x259d1448,     0x25dc3a85,     0x25083c7d,\n-    0x254b9e39,     0x24bfd4c7,     0x24f2985c,     0x24b222d5,\n-    0x24e2f700,     0xba5fd3e3,     0x3a5f03e5,     0xfa411be4,\n-    0x7a42cbe2,     0x93df03ff,     0xc820ffff,     0x8822fc7f,\n-    0xc8247cbf,     0x88267fff,     0x4e010fe0,     0x5e040420,\n-    0x4e081fe1,     0x4e0c1fe1,     0x4e0a1fe1,     0x4e071fe1,\n-    0x4e042c20,     0x4e062c20,     0x4e052c20,     0x4e083c20,\n-    0x0e0c3c20,     0x0e0a3c20,     0x0e073c20,     0x9eae0020,\n-    0x0f03f409,     0x6f03f40e,     0x4cc0ac3f,     0x0ea1b820,\n-    0x4e21c862,     0x4e61b8a4,     0x05a08020,     0x05104fe0,\n-    0x05505001,     0x05906fe2,     0x05d03005,     0x05101fea,\n-    0x05901feb,     0x04b0e3e0,     0x0470e7e1,     0x042f9c20,\n-    0x043f9c35,     0x047f9c20,     0x04ff9c20,     0x04299420,\n-    0x04319160,     0x0461943e,     0x04a19020,     0x04038100,\n-    0x040381a0,     0x040387e1,     0x04438be2,     0x04c38fe3,\n-    0x040181e0,     0x04018100,     0x04018621,     0x04418b22,\n-    0x04418822,     0x04818c23,     0x040081e0,     0x04008120,\n-    0x04008761,     0x04008621,     0x04408822,     0x04808c23,\n-    0x042053ff,     0x047f5401,     0x25208028,     0x2538cfe0,\n-    0x2578d001,     0x25b8efe2,     0x25f8f007,     0x2538dfea,\n-    0x25b8dfeb,     0xa400a3e0,     0xa420a7e0,     0xa4484be0,\n-    0xa467afe0,     0xa4a8a7ea,     0xa547a814,     0xa4084ffe,\n-    0xa55c53e0,     0xa5e1540b,     0xe400fbf6,     0xe408ffff,\n-    0xe420e7e0,     0xe4484be0,     0xe460efe0,     0xe547e400,\n-    0xe4014be0,     0xe4a84fe0,     0xe5f15000,     0x858043e0,\n-    0x85a043ff,     0xe59f5d08,     0x0420e3e9,     0x0460e3ea,\n-    0x04a0e3eb,     0x04e0e3ec,     0x25104042,     0x25104871,\n-    0x25904861,     0x25904c92,     0x05344020,     0x05744041,\n-    0x05b44062,     0x05f44083,     0x252c8840,     0x253c1420,\n-    0x25681572,     0x25a21ce3,     0x25ea1e34,     0x253c0421,\n-    0x25680572,     0x25a20ce3,     0x25ea0e34,     0x0522c020,\n-    0x05e6c0a4,     0x2401a001,     0x2443a051,     0x24858881,\n-    0x24c78cd1,     0x24850891,     0x24c70cc1,     0x250f9001,\n-    0x25508051,     0x25802491,     0x25df28c1,     0x25850c81,\n-    0x251e10d1,     0x65816001,     0x65c36051,     0x65854891,\n-    0x65c74cc1,     0x05733820,     0x05b238a4,     0x05f138e6,\n-    0x0570396a,     0x65d0a001,     0x65d6a443,     0x65d4a826,\n-    0x6594ac26,     0x6554ac26,     0x6556ac26,     0x6552ac26,\n-    0x65cbac85,     0x65caac01,     0x6589ac85,     0x6588ac01,\n-    0x65c9ac85,     0x65c8ac01,     0x65dea833,     0x659ca509,\n-    0x65d8a801,     0x65dcac01,     0x655cb241,     0x0520a1e0,\n-    0x0521a601,     0x052281e0,     0x05238601,     0x04a14026,\n-    0x042244a6,     0x046344a6,     0x04a444a6,     0x04e544a7,\n-    0x0568aca7,     0x05b23230,     0x853040af,     0xc5b040af,\n-    0xe57080af,     0xe5b080af,     0x25034440,     0x254054c4,\n-    0x25034640,     0x25415a05,     0x25834440,     0x25c54489,\n-    0x250b5d3a,     0x2550dc20,     0x2518e3e1,     0x2518e021,\n-    0x2518e0a1,     0x2518e121,     0x2518e1a1,     0x2558e3e2,\n-    0x2558e042,     0x2558e0c2,     0x2558e142,     0x2598e3e3,\n-    0x2598e063,     0x2598e0e3,     0x2598e163,     0x25d8e3e4,\n-    0x25d8e084,     0x25d8e104,     0x25d8e184,     0x2518e407,\n-    0x05214800,     0x05614800,     0x05a14800,     0x05e14800,\n-    0x05214c00,     0x05614c00,     0x05a14c00,     0x05e14c00,\n-    0x05304001,     0x05314001,     0x05a18610,     0x05e18610,\n-    0x05271e11,     0x6545e891,     0x6585e891,     0x65c5e891,\n-    0x6545c891,     0x6585c891,     0x65c5c891,     0x45b0c210,\n-    0x45f1c231,     0x1e601000,     0x1e603000,     0x1e621000,\n-    0x1e623000,     0x1e641000,     0x1e643000,     0x1e661000,\n-    0x1e663000,     0x1e681000,     0x1e683000,     0x1e6a1000,\n-    0x1e6a3000,     0x1e6c1000,     0x1e6c3000,     0x1e6e1000,\n-    0x1e6e3000,     0x1e701000,     0x1e703000,     0x1e721000,\n-    0x1e723000,     0x1e741000,     0x1e743000,     0x1e761000,\n-    0x1e763000,     0x1e781000,     0x1e783000,     0x1e7a1000,\n-    0x1e7a3000,     0x1e7c1000,     0x1e7c3000,     0x1e7e1000,\n-    0x1e7e3000,     0xf830808c,     0xf83c03be,     0xf83010db,\n-    0xf82921fd,     0xf82730e4,     0xf82f52e9,     0xf8284382,\n-    0xf83570bf,     0xf83b6220,     0xf8af8344,     0xf8a802dc,\n-    0xf8bb133b,     0xf8b72080,     0xf8a63010,     0xf8a4502f,\n-    0xf8aa40a7,     0xf8aa70fc,     0xf8b462b7,     0xf8e6810b,\n-    0xf8f100df,     0xf8f11182,     0xf8fe207d,     0xf8fb33b6,\n-    0xf8ee538d,     0xf8f140b8,     0xf8e2714e,     0xf8f0636b,\n-    0xf877808c,     0xf8760091,     0xf8611213,     0xf87021cd,\n-    0xf86c3222,     0xf86352f5,     0xf86540e6,     0xf873738d,\n-    0xf87160d0,     0xb822807d,     0xb82401e6,     0xb834118d,\n-    0xb8342328,     0xb8343013,     0xb82b50d8,     0xb83441df,\n-    0xb8307006,     0xb827626f,     0xb8ba8149,     0xb8b702d5,\n-    0xb8bc1062,     0xb8af2293,     0xb8a733a4,     0xb8a75120,\n-    0xb8b042f4,     0xb8a47150,     0xb8b7632b,     0xb8e6821f,\n-    0xb8ed0197,     0xb8e1112e,     0xb8f52350,     0xb8ef3084,\n-    0xb8f050c8,     0xb8fe43a4,     0xb8f1735d,     0xb8e9604f,\n-    0xb86b807d,     0xb8670361,     0xb87511d0,     0xb86822d0,\n-    0xb8793285,     0xb87552f0,     0xb870429e,     0xb8747080,\n-    0xb8736098,     0xce376493,     0xce082053,     0xce718f0e,\n-    0xce84fabe,     0xce738141,     0xce69840c,     0xcec08307,\n-    0xce7b8891,     0x2560dbe6,     0x2561d097,     0x0580299e,\n-    0x0542305e,     0x050026b3,     0x2560ce69,     0x25a1c1ab,\n-    0x05803cb8,     0x05407851,     0x05001e20,     0x2560d06f,\n-    0x2561de64,     0x05800ec5,     0x05401e3a,     0x050287b3,\n-    0x25a0c7e3,     0x25e1ce37,     0x05806535,     0x05423051,\n-    0x05025a82,     0x2520d474,     0x2521dae2,     0x05800d31,\n-    0x05403635,     0x05004cb7,     0x2560d174,     0x2561c35d,\n-    0x05809863,     0x054030f8,     0x05000ed6,     0x04e700ad,\n-    0x04b106a5,     0x65c90060,     0x658b08eb,     0x658b0631,\n-    0x0496b3d8,     0x04c011c8,     0x045a1ed6,     0x04d08768,\n-    0x04db01ca,     0x0419b415,     0x04dab8b6,     0x0419123d,\n-    0x04538fac,     0x04d19040,     0x04d016b4,     0x0417a84c,\n-    0x041eb6ce,     0x04d81b53,     0x052796ac,     0x05a48a61,\n-    0x04481af3,     0x048a127e,     0x04410694,     0x04dcb7de,\n-    0x65809239,     0x65cd8f8b,     0x658681a5,     0x65878542,\n-    0x65828733,     0x049da3a2,     0x65c2a694,     0x6580adbc,\n-    0x6581bc2d,     0x65cda07b,     0x65819928,     0x65e189d9,\n-    0x65f30799,     0x65e13da6,     0x65a1a82b,     0x65a2d9db,\n-    0x65a2f31d,     0x65bc4338,     0x65ef75a3,     0x049a4570,\n-    0x041b7022,     0x043b33d6,     0x04b032aa,     0x046432a7,\n-    0x04e83178,     0x05a4680b,     0x05246e95,     0x65c88c6f,\n-    0x4585b379,     0x451eb559,     0x042438f8,     0x04da34e3,\n-    0x04d83f17,     0x04592151,     0x04c82d1d,     0x048a23dc,\n-    0x65c7341e,     0x65c63f87,     0x65d82b95,     0x048124d3,\n-\n+    0x1e624385,     0x1e63c236,     0x1ee0c26d,     0x1ee14373,\n+    0x1ee1c0d1,     0x1e3800ed,     0x9e38035c,     0x1e7800d1,\n+    0x9e780081,     0x1ef8028d,     0x9ef802a6,     0x1e2202fa,\n+    0x9e22028d,     0x1e62037e,     0x9e6202aa,     0x1ee20225,\n+    0x9ee201ab,     0x1e24028d,     0x9e6401da,     0x1e3002e4,\n+    0x9e7003b7,     0x1e2601cc,     0x9e660370,     0x1e270015,\n+    0x9e670346,     0x1e3b2220,     0x1e662180,     0x1e202008,\n+    0x1e6023c8,     0x29263d73,     0x297a31e0,     0x69447ae7,\n+    0xa9303c9a,     0xa9762f26,     0x29b8644a,     0x29e6393e,\n+    0x69c45f7d,     0xa9b2016d,     0xa9f84159,     0x28ba3f65,\n+    0x28f0604c,     0x68fc3f0e,     0xa8b856d1,     0xa8c823ca,\n+    0x283068de,     0x28480a98,     0xa80457a9,     0xa8707876,\n+    0x0c4071e5,     0x4cdfa0e9,     0x0cde6e0c,     0x4cdf2627,\n+    0x0d40c330,     0x4ddfc86b,     0x0dc6ccec,     0x4c408f69,\n+    0x0cdf8746,     0x4d60c217,     0x0dffc9a6,     0x4dfacc33,\n+    0x4cc049ee,     0x0c404adc,     0x4d40e545,     0x4ddfe8ce,\n+    0x0dcded46,     0x4cdf046b,     0x0cc0032c,     0x0d60e1ea,\n+    0x0dffe4dd,     0x0dfceb4a,     0x0e31bbbc,     0x4e31bbbc,\n+    0x0e71ba93,     0x4e71baf6,     0x4eb1b96a,     0x0e30a8a4,\n+    0x4e30abfe,     0x0e70aab4,     0x4e70a928,     0x4eb0abfe,\n+    0x6e30fa51,     0x0e31a96a,     0x2e31ab9b,     0x4e31a862,\n+    0x6e31ab38,     0x0e71a8a4,     0x2e71a883,     0x4e71a928,\n+    0x6e71aaf6,     0x4eb1aa51,     0x6eb1a9cd,     0x6eb0f8a4,\n+    0x7e30fbbc,     0x7e70fb17,     0x7eb0fad5,     0x7ef0fb59,\n+    0x0ea0cb38,     0x4ea0c883,     0x4ee0cb17,     0x2ea0cb7a,\n+    0x6ea0cb17,     0x6ee0c9ee,     0x0ea0dad5,     0x4ea0d883,\n+    0x4ee0db17,     0x0ea0e928,     0x4ea0eb38,     0x4ee0ea93,\n+    0x2ea0da0f,     0x6ea0da30,     0x6ee0d862,     0x0e20b841,\n+    0x4e20b820,     0x0e60bb38,     0x4e60b8a4,     0x0ea0b883,\n+    0x4ea0b98b,     0x4ee0bbfe,     0x0ea0fb9b,     0x4ea0f949,\n+    0x4ee0fb59,     0x2ea0f862,     0x6ea0f9ac,     0x6ee0fa51,\n+    0x2ea1fbfe,     0x6ea1f841,     0x6ee1f9ac,     0x2e205bbc,\n+    0x6e205820,     0x0e331e51,     0x4e2e1dac,     0x0eb31e51,\n+    0x4eb71ed5,     0x2e2e1dac,     0x6e3d1f9b,     0x0e3f87dd,\n+    0x4e2087fe,     0x0e638441,     0x4e7b8759,     0x0ebd879b,\n+    0x4ea684a4,     0x4eff87dd,     0x0e25d483,     0x4e28d4e6,\n+    0x4e7fd7dd,     0x2e3b8759,     0x6e338651,     0x2e6a8528,\n+    0x6e698507,     0x2eae85ac,     0x6ea28420,     0x6ef58693,\n+    0x0ea3d441,     0x4eb9d717,     0x4ee4d462,     0x0e229c20,\n+    0x4e2a9d28,     0x0e799f17,     0x4e7b9f59,     0x0eb19e0f,\n+    0x4ebf9fdd,     0x2ea5d483,     0x6eacd56a,     0x6ef8d6f6,\n+    0x2e2cd56a,     0x6e26d4a4,     0x6e73d651,     0x2e23dc41,\n+    0x6e2ddd8b,     0x6e69dd07,     0x0e6c956a,     0x4e71960f,\n+    0x0eb29630,     0x4ea49462,     0x0e2bcd49,     0x4e2dcd8b,\n+    0x4e6ecdac,     0x2e7095ee,     0x6e6f95cd,     0x2ea49462,\n+    0x6ea894e6,     0x0eb5ce93,     0x4ebbcf59,     0x4ef1ce0f,\n+    0x2e26fca4,     0x6e24fc62,     0x6e66fca4,     0x0e2d658b,\n+    0x4e336651,     0x0e7666b4,     0x4e726630,     0x0eb36651,\n+    0x4eac656a,     0x0e36a6b4,     0x4e38a6f6,     0x0e6ea5ac,\n+    0x4e7ba759,     0x0eb9a717,     0x4ebea7bc,     0x0e30f5ee,\n+    0x4e2cf56a,     0x4e7af738,     0x0e236c41,     0x4e2d6d8b,\n+    0x0e606ffe,     0x4e6c6d6a,     0x0eb16e0f,     0x4ea96d07,\n+    0x0e24ac62,     0x4e25ac83,     0x0e6fadcd,     0x4e75ae93,\n+    0x0eb2ae30,     0x4eb2ae30,     0x0ea5f483,     0x4ea3f441,\n+    0x4eedf58b,     0x2ebfefdd,     0x6ea7ecc5,     0x6eeaed28,\n+    0x0e3035ee,     0x4e3e37bc,     0x0e7f37dd,     0x4e623420,\n+    0x0eb636b4,     0x4ea93507,     0x4ef636b4,     0x0e393f17,\n+    0x4e3d3f9b,     0x0e773ed5,     0x4e7c3f7a,     0x0eba3f38,\n+    0x4ea63ca4,     0x4ee33c41,     0x2e388ef6,     0x6e328e30,\n+    0x2e608ffe,     0x6e678cc5,     0x2ead8d8b,     0x6eaa8d28,\n+    0x6efd8f9b,     0x2e3035ee,     0x6e3e37bc,     0x2e7736d5,\n+    0x6e6037fe,     0x2eb33651,     0x6ea037fe,     0x6ee734c5,\n+    0x2e2f3dcd,     0x6e333e51,     0x2e603ffe,     0x6e733e51,\n+    0x2ebc3f7a,     0x6eb53e93,     0x6ef13e0f,     0x0e2ee5ac,\n+    0x4e2de58b,     0x4e6be549,     0x2ea8e4e6,     0x6ebfe7dd,\n+    0x6ef2e630,     0x2e3ce77a,     0x6e3de79b,     0x6e7fe7dd,\n+    0x65d22943,     0x65d032f2,     0x65d02c6b,     0x65d1362b,\n+    0x65912df1,     0x65933548,     0x258187a0,     0x259a0df1,\n+    0x25cb0f8a,     0x25c33821,     0x25df3938,     0x25899e93,\n+    0x24ea93c1,     0x2420923b,     0x24b2e33c,     0x24fdbc66,\n+    0xba5fd3e3,     0x3a5f03e5,     0xfa411be4,     0x7a42cbe2,\n+    0x93df03ff,     0xc820ffff,     0x8822fc7f,     0xc8247cbf,\n+    0x88267fff,     0x4e010fe0,     0x5e040420,     0x4e081fe1,\n+    0x4e0c1fe1,     0x4e0a1fe1,     0x4e071fe1,     0x4e042c20,\n+    0x4e062c20,     0x4e052c20,     0x4e083c20,     0x0e0c3c20,\n+    0x0e0a3c20,     0x0e073c20,     0x9eae0020,     0x0f03f409,\n+    0x6f03f40e,     0x4cc0ac3f,     0x0ea1b820,     0x4e21c862,\n+    0x4e61b8a4,     0x05a08020,     0x05104fe0,     0x05505001,\n+    0x05906fe2,     0x05d03005,     0x05101fea,     0x05901feb,\n+    0x04b0e3e0,     0x0470e7e1,     0x042f9c20,     0x043f9c35,\n+    0x047f9c20,     0x04ff9c20,     0x04299420,     0x04319160,\n+    0x0461943e,     0x04a19020,     0x04038100,     0x040381a0,\n+    0x040387e1,     0x04438be2,     0x04c38fe3,     0x040181e0,\n+    0x04018100,     0x04018621,     0x04418b22,     0x04418822,\n+    0x04818c23,     0x040081e0,     0x04008120,     0x04008761,\n+    0x04008621,     0x04408822,     0x04808c23,     0x042053ff,\n+    0x047f5401,     0x25208028,     0x2538cfe0,     0x2578d001,\n+    0x25b8efe2,     0x25f8f007,     0x2538dfea,     0x25b8dfeb,\n+    0xa400a3e0,     0xa420a7e0,     0xa4484be0,     0xa467afe0,\n+    0xa4a8a7ea,     0xa547a814,     0xa4084ffe,     0xa55c53e0,\n+    0xa5e1540b,     0xe400fbf6,     0xe408ffff,     0xe420e7e0,\n+    0xe4484be0,     0xe460efe0,     0xe547e400,     0xe4014be0,\n+    0xe4a84fe0,     0xe5f15000,     0x858043e0,     0x85a043ff,\n+    0xe59f5d08,     0x0420e3e9,     0x0460e3ea,     0x04a0e3eb,\n+    0x04e0e3ec,     0x25104042,     0x25104871,     0x25904861,\n+    0x25904c92,     0x05344020,     0x05744041,     0x05b44062,\n+    0x05f44083,     0x252c8840,     0x253c1420,     0x25681572,\n+    0x25a21ce3,     0x25ea1e34,     0x253c0421,     0x25680572,\n+    0x25a20ce3,     0x25ea0e34,     0x0522c020,     0x05e6c0a4,\n+    0x2401a001,     0x2443a051,     0x24858881,     0x24c78cd1,\n+    0x24850891,     0x24c70cc1,     0x250f9001,     0x25508051,\n+    0x25802491,     0x25df28c1,     0x25850c81,     0x251e10d1,\n+    0x65816001,     0x65c36051,     0x65854891,     0x65c74cc1,\n+    0x05733820,     0x05b238a4,     0x05f138e6,     0x0570396a,\n+    0x65d0a001,     0x65d6a443,     0x65d4a826,     0x6594ac26,\n+    0x6554ac26,     0x6556ac26,     0x6552ac26,     0x65cbac85,\n+    0x65caac01,     0x6589ac85,     0x6588ac01,     0x65c9ac85,\n+    0x65c8ac01,     0x65dea833,     0x659ca509,     0x65d8a801,\n+    0x65dcac01,     0x655cb241,     0x0520a1e0,     0x0521a601,\n+    0x052281e0,     0x05238601,     0x04a14026,     0x042244a6,\n+    0x046344a6,     0x04a444a6,     0x04e544a7,     0x0568aca7,\n+    0x05b23230,     0x853040af,     0xc5b040af,     0xe57080af,\n+    0xe5b080af,     0x25034440,     0x254054c4,     0x25034640,\n+    0x25415a05,     0x25834440,     0x25c54489,     0x250b5d3a,\n+    0x2550dc20,     0x2518e3e1,     0x2518e021,     0x2518e0a1,\n+    0x2518e121,     0x2518e1a1,     0x2558e3e2,     0x2558e042,\n+    0x2558e0c2,     0x2558e142,     0x2598e3e3,     0x2598e063,\n+    0x2598e0e3,     0x2598e163,     0x25d8e3e4,     0x25d8e084,\n+    0x25d8e104,     0x25d8e184,     0x2518e407,     0x05214800,\n+    0x05614800,     0x05a14800,     0x05e14800,     0x05214c00,\n+    0x05614c00,     0x05a14c00,     0x05e14c00,     0x05304001,\n+    0x05314001,     0x05a18610,     0x05e18610,     0x05271e11,\n+    0x6545e891,     0x6585e891,     0x65c5e891,     0x6545c891,\n+    0x6585c891,     0x65c5c891,     0x45b0c210,     0x45f1c231,\n+    0x1e601000,     0x1e603000,     0x1e621000,     0x1e623000,\n+    0x1e641000,     0x1e643000,     0x1e661000,     0x1e663000,\n+    0x1e681000,     0x1e683000,     0x1e6a1000,     0x1e6a3000,\n+    0x1e6c1000,     0x1e6c3000,     0x1e6e1000,     0x1e6e3000,\n+    0x1e701000,     0x1e703000,     0x1e721000,     0x1e723000,\n+    0x1e741000,     0x1e743000,     0x1e761000,     0x1e763000,\n+    0x1e781000,     0x1e783000,     0x1e7a1000,     0x1e7a3000,\n+    0x1e7c1000,     0x1e7c3000,     0x1e7e1000,     0x1e7e3000,\n+    0xf83080db,     0xf82901fd,     0xf82710e4,     0xf82f22e9,\n+    0xf8283382,     0xf83550bf,     0xf83b4220,     0xf82f7344,\n+    0xf82862dc,     0xf8bb833b,     0xf8b70080,     0xf8a61010,\n+    0xf8a4202f,     0xf8aa30a7,     0xf8aa50fc,     0xf8b442b7,\n+    0xf8a6710b,     0xf8b160df,     0xf8f18182,     0xf8fe007d,\n+    0xf8fb13b6,     0xf8ee238d,     0xf8f130b8,     0xf8e2514e,\n+    0xf8f0436b,     0xf8f7708c,     0xf8f66091,     0xf8618213,\n+    0xf87001cd,     0xf86c1222,     0xf86322f5,     0xf86530e6,\n+    0xf873538d,     0xf87140d0,     0xf862707d,     0xf86461e6,\n+    0xb834818d,     0xb8340328,     0xb8341013,     0xb82b20d8,\n+    0xb83431df,     0xb8305006,     0xb827426f,     0xb83a7149,\n+    0xb83762d5,     0xb8bc8062,     0xb8af0293,     0xb8a713a4,\n+    0xb8a72120,     0xb8b032f4,     0xb8a45150,     0xb8b7432b,\n+    0xb8a6721f,     0xb8ad6197,     0xb8e1812e,     0xb8f50350,\n+    0xb8ef1084,     0xb8f020c8,     0xb8fe33a4,     0xb8f1535d,\n+    0xb8e9404f,     0xb8eb707d,     0xb8e76361,     0xb87581d0,\n+    0xb86802d0,     0xb8791285,     0xb87522f0,     0xb870329e,\n+    0xb8745080,     0xb8734098,     0xb8747304,     0xb87a6053,\n+    0xce2e6108,     0xce1513d1,     0xce6a8c3e,     0xce804993,\n+    0xce718307,     0xce668764,     0xcec08369,     0xce7089b7,\n+    0x25a0d6fe,     0x25e1dd94,     0x058064a9,     0x05438d14,\n+    0x05006c4d,     0x2560d3f8,     0x25a1d22d,     0x05803630,\n+    0x05403cab,     0x05003ecf,     0x2520cac5,     0x25e1c375,\n+    0x05803500,     0x054242ca,     0x05023047,     0x2560db35,\n+    0x25a1df8c,     0x05804531,     0x0540ea70,     0x050026b3,\n+    0x2560d0d1,     0x25a1c011,     0x05800e04,     0x05403ca6,\n+    0x05032890,     0x25a0c9e3,     0x25e1dc63,     0x05808903,\n+    0x05407859,     0x05026327,     0x04200231,     0x046b0669,\n+    0x65d101cb,     0x65d809ab,     0x65c8063e,     0x04d6bace,\n+    0x04000916,     0x041a1d5b,     0x04909aae,     0x04db02c0,\n+    0x0499bba5,     0x045aa191,     0x04990c1d,     0x04939e82,\n+    0x04519d95,     0x049001c2,     0x04d7a276,     0x049eb99a,\n+    0x04580035,     0x05e78e73,     0x05a48bd7,     0x04081693,\n+    0x048a0fd4,     0x04811f3e,     0x049cad71,     0x658094bc,\n+    0x658d8c4d,     0x65868e6a,     0x65878c59,     0x6582829d,\n+    0x049dbf94,     0x65c2a9ad,     0x6580af61,     0x65c1b903,\n+    0x658da329,     0x65c1828e,     0x65b09e66,     0x65bc0c3b,\n+    0x65e12029,     0x65e4ac5a,     0x65e2d851,     0x65adfb83,\n+    0x65fb4df6,     0x65af6b44,     0x0487436f,     0x048a7e1e,\n+    0x04273390,     0x04b83184,     0x046b3148,     0x04f532c4,\n+    0x056f6864,     0x05f96fa3,     0x65889725,     0x4518b05e,\n+    0x4583b4a7,     0x04383ae7,     0x041a3e38,     0x04582faa,\n+    0x04193b88,     0x048837de,     0x04ca34e0,     0x65873abc,\n+    0x65863a7c,     0x65983226,     0x040134f9,\n","filename":"test\/hotspot\/gtest\/aarch64\/asmtest.out.h","additions":640,"deletions":637,"binary":false,"changes":1277,"status":"modified"},{"patch":"@@ -0,0 +1,97 @@\n+\/*\n+ * Copyright (c) 2025, Arm Limited. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+package compiler.c2.irTests;\n+\n+import compiler.lib.ir_framework.*;\n+import static java.lang.Float16.*;\n+import jdk.test.lib.Asserts;\n+\n+\/*\n+ * @test\n+ * @bug 8345053\n+ * @summary Test that Ideal and identity transformations of ConvD2HF are performing as expected.\n+ * @library \/test\/lib \/\n+ * @run driver compiler.c2.irTests.ConvD2HFTransformationTests\n+ *\/\n+public class ConvD2HFTransformationTests {\n+    private int[] input;\n+    private Float16[] fin;\n+    private Float16[] fout;\n+\n+    private static final int SIZE = 65504; \/\/ Tests full range of FP16 values\n+    public ConvD2HFTransformationTests() {\n+        input = new int[SIZE];\n+        fin  = new Float16[SIZE];\n+        fout = new Float16[SIZE];\n+\n+        for (int i = 0; i < SIZE; i++) {\n+            input[i] = i;\n+            fin[i] = valueOf((float)i);\n+            fout[i] = valueOf(0);\n+        }\n+    }\n+    public static void main(String[] args) {\n+        TestFramework.runWithFlags(\"--enable-preview\");\n+    }\n+\n+    @Test\n+    @IR(counts = {IRNode.CONV_I2HF, \">=1\"},\n+        failOn = {IRNode.CONV_D2HF, IRNode.CONV_I2D},\n+        applyIfCPUFeatureAnd = {\"fphp\", \"true\", \"asimdhp\", \"true\"})\n+        \/\/ Test Ideal transformation of ConvD2HF node : pattern ConvI2D -> ConvD2HF is optimized to ConvI2HF\n+        public void testIdeal() {\n+        for (int i = 0; i < SIZE; i++) {\n+            fout[i] = valueOf(input[i]);\n+        }\n+    }\n+\n+    @Check(test=\"testIdeal\")\n+    public void checkTestIdeal() {\n+        for (int i = 0; i < SIZE; i++) {\n+            Float16 expected = valueOf((float) input[i]);\n+            if (expected != fout[i]) {\n+                throw new RuntimeException(\"Invalid result for testIdeal : fout[\" + i + \"] = \" + fout[i] + \" != \" + expected);\n+            }\n+        }\n+    }\n+\n+    @Test\n+    @IR(failOn = {IRNode.CONV_D2HF, IRNode.CONV_HF2D},\n+        applyIfCPUFeatureAnd = {\"fphp\", \"true\", \"asimdhp\", \"true\"})\n+        \/\/ Test Identity transformation of ConvD2HF node : pattern - ConvHF2D -> ConvD2HF is optimized away\n+        public void testIdentity() {\n+        for (int i = 0; i < SIZE; i++) {\n+            fout[i] = valueOf(fin[i].doubleValue());\n+        }\n+    }\n+\n+    @Check(test=\"testIdentity\")\n+    public void checkTestIdentity() {\n+        for (int i = 0; i < SIZE; i++) {\n+            Float16 expected = fin[i];\n+            if (expected != fout[i]) {\n+                throw new RuntimeException(\"Invalid result for testIdeal : fout[\" + i + \"] = \" + fout[i] + \" != \" + expected);\n+            }\n+        }\n+    }\n+}\n","filename":"test\/hotspot\/jtreg\/compiler\/c2\/irTests\/ConvD2HFTransformationTests.java","additions":97,"deletions":0,"binary":false,"changes":97,"status":"added"},{"patch":"@@ -2,2 +2,2 @@\n- * Copyright (c) 2023, 2024, Oracle and\/or its affiliates. All rights reserved.\n- * Copyright (c) 2024, Arm Limited. All rights reserved.\n+ * Copyright (c) 2023, 2025, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2024, 2025, Arm Limited. All rights reserved.\n@@ -44,0 +44,1 @@\n+    private Random rng;\n@@ -52,0 +53,2 @@\n+        rng = new Random(25);\n+\n@@ -230,0 +233,33 @@\n+\n+    @Test\n+    @IR(counts = {IRNode.CONV_I2HF, \"> 0\"},\n+        applyIfCPUFeatureAnd = {\"fphp\", \"true\", \"asimdhp\", \"true\"})\n+    public void testConvI2HF() {\n+        Float16 result = valueOf(0);\n+        for (int i = 0; i < count; i++) {\n+            result = valueOf(rng.nextInt());\n+            dst[i] = float16ToRawShortBits(result);\n+        }\n+    }\n+\n+    @Test\n+    @IR(counts = {IRNode.CONV_L2HF, \"> 0\"},\n+        applyIfCPUFeatureAnd = {\"fphp\", \"true\", \"asimdhp\", \"true\"})\n+    public void testConvL2HF() {\n+        Float16 result = valueOf(0);\n+        for (int i = 0; i < count; i++) {\n+            result = valueOf((long)i);\n+            dst[i] = float16ToRawShortBits(result);\n+        }\n+    }\n+\n+    @Test\n+    @IR(counts = {IRNode.CONV_D2HF, \"> 0\"},\n+        applyIfCPUFeatureAnd = {\"fphp\", \"true\", \"asimdhp\", \"true\"})\n+    public void testConvD2HF() {\n+        Float16 result = valueOf(0);\n+        for (int i = 0; i < count; i++) {\n+            result = Float16.valueOf(rng.nextDouble());\n+            dst[i] = float16ToRawShortBits(result);\n+        }\n+    }\n","filename":"test\/hotspot\/jtreg\/compiler\/intrinsics\/float16\/TestFP16ScalarOps.java","additions":38,"deletions":2,"binary":false,"changes":40,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2021, 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2021, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -475,0 +475,5 @@\n+    public static final String CONV_D2HF = PREFIX + \"CONV_D2HF\" + POSTFIX;\n+    static {\n+        beforeMatchingNameRegex(CONV_D2HF, \"ConvD2HF\");\n+    }\n+\n@@ -500,0 +505,10 @@\n+    public static final String CONV_I2HF = PREFIX + \"CONV_I2HF\" + POSTFIX;\n+    static {\n+        beforeMatchingNameRegex(CONV_I2HF, \"ConvI2HF\");\n+    }\n+\n+    public static final String CONV_I2D = PREFIX + \"CONV_I2D\" + POSTFIX;\n+    static {\n+        beforeMatchingNameRegex(CONV_I2D, \"ConvI2D\");\n+    }\n+\n@@ -505,0 +520,5 @@\n+    public static final String CONV_L2HF = PREFIX + \"CONV_L2HF\" + POSTFIX;\n+    static {\n+        beforeMatchingNameRegex(CONV_L2HF, \"ConvL2HF\");\n+    }\n+\n@@ -2017,0 +2037,15 @@\n+    public static final String VECTOR_CAST_I2HF = PREFIX + \"VECTOR_CAST_I2HF\" + POSTFIX;\n+    static {\n+        beforeMatchingNameRegex(VECTOR_CAST_I2HF, \"VectorCastI2HF\");\n+    }\n+\n+    public static final String VECTOR_CAST_D2HF = PREFIX + \"VECTOR_CAST_D2HF\" + POSTFIX;\n+    static {\n+        beforeMatchingNameRegex(VECTOR_CAST_D2HF, \"VectorCastD2HF\");\n+    }\n+\n+    public static final String VECTOR_CAST_L2HF = PREFIX + \"VECTOR_CAST_L2HF\" + POSTFIX;\n+    static {\n+        beforeMatchingNameRegex(VECTOR_CAST_L2HF, \"VectorCastL2HF\");\n+    }\n+\n","filename":"test\/hotspot\/jtreg\/compiler\/lib\/ir_framework\/IRNode.java","additions":36,"deletions":1,"binary":false,"changes":37,"status":"modified"},{"patch":"@@ -2,2 +2,2 @@\n- * Copyright (c) 2023, 2024, Oracle and\/or its affiliates. All rights reserved.\n- * Copyright (c) 2024, Arm Limited. All rights reserved.\n+ * Copyright (c) 2023, 2025, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2024, 2025, Arm Limited. All rights reserved.\n@@ -45,0 +45,4 @@\n+    private int[] iin;\n+    private double[] din;\n+    private long[] lin;\n+\n@@ -64,0 +68,5 @@\n+\n+        iin = new int[LEN];\n+        lin = new long[LEN];\n+        din = new double[LEN];\n+\n@@ -74,0 +83,4 @@\n+\n+            iin[i] = input1[i].intValue();\n+            din[i] = input1[i].doubleValue();\n+            lin[i] = input1[i].longValue();\n@@ -363,0 +376,64 @@\n+\n+    @Test\n+    @Warmup(10000)\n+    @IR(counts = {IRNode.VECTOR_CAST_I2HF, \">= 1\"},\n+        applyIfCPUFeature = {\"sve\", \"true\"})\n+    @IR(counts = {IRNode.VECTOR_CAST_I2HF, \">= 1\"},\n+        applyIfCPUFeatureAnd = {\"fphp\", \"true\", \"asimdhp\", \"true\"})\n+    public void vectorIntToFloat16() {\n+        for (int i = 0; i < LEN; ++i) {\n+            output[i] = valueOf(iin[i]);\n+        }\n+    }\n+    @Check(test=\"vectorIntToFloat16\")\n+    public void checkResulIntToFloat16() {\n+        Float16 expected;\n+        for (int i = 0; i < LEN; ++i) {\n+            expected = valueOf(iin[i]);\n+            if (float16ToRawShortBits(expected) != float16ToRawShortBits(output[i])) {\n+                throw new RuntimeException(\"Invalid result for int to Float16 conversion : output[\" + i + \"] = \" + float16ToRawShortBits(output[i]) + \" != \" + float16ToRawShortBits(expected));\n+            }\n+        }\n+    }\n+\n+    @Test\n+    @Warmup(10000)\n+    @IR(counts = {IRNode.VECTOR_CAST_D2HF, \">= 1\"},\n+        applyIf = {\"MaxVectorSize\", \"> 16\"},\n+        applyIfCPUFeature = {\"sve\", \"true\"})\n+        public void vectorDoubleToFloat16() {\n+        for (int i = 0; i < LEN; ++i) {\n+            output[i] = valueOf(din[i]);\n+        }\n+    }\n+    @Check(test=\"vectorDoubleToFloat16\")\n+    public void checkResulDoubleToFloat16() {\n+        Float16 expected;\n+        for (int i = 0; i < LEN; ++i) {\n+            expected = valueOf(din[i]);\n+            if (float16ToRawShortBits(expected) != float16ToRawShortBits(output[i])) {\n+                throw new RuntimeException(\"Invalid result for double to Float16 conversion : output[\" + i + \"] = \" + float16ToRawShortBits(output[i]) + \" != \" + float16ToRawShortBits(expected));\n+            }\n+        }\n+    }\n+\n+    @Test\n+    @Warmup(10000)\n+    @IR(counts = {IRNode.VECTOR_CAST_L2HF, \">= 1\"},\n+        applyIf = {\"MaxVectorSize\", \"> 16\"},\n+        applyIfCPUFeature = {\"sve\", \"true\"})\n+    public void vectorLongToFloat16() {\n+        for (int i = 0; i < LEN; ++i) {\n+            output[i] = valueOf(lin[i]);\n+        }\n+    }\n+    @Check(test=\"vectorLongToFloat16\")\n+    public void checkResulLongToFloat16() {\n+        Float16 expected;\n+        for (int i = 0; i < LEN; ++i) {\n+            expected = valueOf(lin[i]);\n+            if (float16ToRawShortBits(expected) != float16ToRawShortBits(output[i])) {\n+                throw new RuntimeException(\"Invalid result for long to Float16 conversion : output[\" + i + \"] = \" + float16ToRawShortBits(output[i]) + \" != \" + float16ToRawShortBits(expected));\n+            }\n+        }\n+    }\n","filename":"test\/hotspot\/jtreg\/compiler\/vectorization\/TestFloat16VectorOps.java","additions":79,"deletions":2,"binary":false,"changes":81,"status":"modified"},{"patch":"@@ -2,2 +2,2 @@\n- * Copyright (c) 2023, 2024, Oracle and\/or its affiliates. All rights reserved.\n- * Copyright (c) 2024, Arm Limited. All rights reserved.\n+ * Copyright (c) 2023, 2025, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2024, 2025, Arm Limited. All rights reserved.\n@@ -179,1 +179,1 @@\n-    public static void test_conversions(Float16 inp []) {\n+    public static void test_conversions_fromFP16(Float16 inp []) {\n@@ -187,0 +187,30 @@\n+    public static void test_conversions_toFP16(int inp []) {\n+        for (int i = 0; i < inp.length; i++) {\n+            Float16 actual = Float16.valueOf(inp[i]);\n+            Float16 expected = Float16.valueOf((float) inp[i]);\n+            if (expected != actual) {\n+                System.out.println(\"Incorrest result for int to FP16 conversion. Expected value:\" + expected + \" Actual value: \" + actual);\n+            }\n+        }\n+    }\n+\n+    public static void test_conversions_toFP16(double inp []) {\n+        for (int i = 0; i < inp.length; i++) {\n+            Float16 actual = Float16.valueOf(inp[i]);\n+            Float16 expected = Float16.valueOf((float) inp[i]);\n+            if (expected != actual) {\n+                System.out.println(\"Incorrest result for double to FP16 conversion. Expected value:\" + expected + \" Actual value: \" + actual);\n+            }\n+        }\n+    }\n+\n+    public static void test_conversions_toFP16(long inp []) {\n+        for (int i = 0; i < inp.length; i++) {\n+            Float16 actual = Float16.valueOf(inp[i]);\n+            Float16 expected = Float16.valueOf((float) inp[i]);\n+            if (expected != actual) {\n+                System.out.println(\"Incorrest result for int to FP16 conversion. Expected value:\" + expected + \" Actual value: \" + actual);\n+            }\n+        }\n+    }\n+\n@@ -192,0 +222,4 @@\n+        int [] int_inp       = new int[SIZE];\n+        long [] long_inp     = new long[SIZE];\n+        double [] double_inp = new double[SIZE];\n+\n@@ -197,0 +231,4 @@\n+        IntStream.range(0, int_inp.length).forEach(i -> {int_inp[i] = input1[i].intValue();});\n+        IntStream.range(0, long_inp.length).forEach(i -> {long_inp[i] = input1[i].longValue();});\n+        IntStream.range(0, double_inp.length).forEach(i -> {double_inp[i] = input1[i].doubleValue();});\n+\n@@ -222,2 +260,7 @@\n-            test_conversions(input1);\n-            test_conversions(special_values);\n+            test_conversions_fromFP16(input1);\n+            test_conversions_fromFP16(special_values);\n+\n+            \/\/ Test conversions from int\/long\/double to FP16\n+            test_conversions_toFP16(int_inp);\n+            test_conversions_toFP16(long_inp);\n+            test_conversions_toFP16(double_inp);\n","filename":"test\/jdk\/java\/lang\/Float16\/FP16ScalarOperations.java","additions":48,"deletions":5,"binary":false,"changes":53,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2024, Oracle and\/or its affiliates. All rights vectorReserved.\n+ * Copyright (c) 2025, Oracle and\/or its affiliates. All rights vectorReserved.\n@@ -46,0 +46,4 @@\n+    double  [] dinp;\n+    int     [] iinp;\n+    long    [] linp;\n+\n@@ -58,0 +62,4 @@\n+        dinp      = new double[vectorDim];\n+        iinp      = new int[vectorDim];\n+        linp      = new long[vectorDim];\n+\n@@ -62,0 +70,4 @@\n+        IntStream.range(0, vectorDim).forEach(i -> {dinp[i] = vector1[i].doubleValue();});\n+        IntStream.range(0, vectorDim).forEach(i -> {iinp[i] = vector1[i].intValue();});\n+        IntStream.range(0, vectorDim).forEach(i -> {linp[i] = vector1[i].longValue();});\n+\n@@ -264,0 +276,21 @@\n+\n+    @Benchmark\n+    public void IntToFP16() {\n+        for (int i = 0; i < vectorDim; i++) {\n+            vectorRes[i] = Float16.valueOf(iinp[i]);\n+        }\n+    }\n+\n+    @Benchmark\n+    public void LongToFP16() {\n+        for (int i = 0; i < vectorDim; i++) {\n+            vectorRes[i] = Float16.valueOf(linp[i]);\n+        }\n+    }\n+\n+    @Benchmark\n+    public void DoubleToFP16() {\n+        for (int i = 0; i < vectorDim; i++) {\n+            vectorRes[i] = Float16.valueOf(dinp[i]);\n+        }\n+    }\n","filename":"test\/micro\/org\/openjdk\/bench\/java\/lang\/Float16OpsBenchmark.java","additions":34,"deletions":1,"binary":false,"changes":35,"status":"modified"}]}