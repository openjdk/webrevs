{"files":[{"patch":"@@ -90,1 +90,1 @@\n-THIS_SNIPPET := modules\/$(MODULE)\/Java.gmk\n+THIS_SNIPPET := $(call GetModuleSnippetName, Java)\n@@ -119,0 +119,1 @@\n+    TARGET_RELEASE := $(TARGET_RELEASE), \\\n","filename":"make\/CompileJavaModules.gmk","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -276,0 +276,1 @@\n+# These variables are read by SetupCopyDebuginfo\n","filename":"make\/Images.gmk","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -398,1 +398,1 @@\n-  OLDEST_BOOT_JDK=`$ECHO $DEFAULT_ACCEPTABLE_BOOT_VERSIONS \\\n+  OLDEST_BOOT_JDK_VERSION=`$ECHO $DEFAULT_ACCEPTABLE_BOOT_VERSIONS \\\n@@ -400,3 +400,1 @@\n-  # -Xlint:-options is added to avoid \"warning: [options] system modules path not set in conjunction with -source\"\n-  BOOT_JDK_SOURCETARGET=\"-source $OLDEST_BOOT_JDK -target $OLDEST_BOOT_JDK -Xlint:-options\"\n-  AC_SUBST(BOOT_JDK_SOURCETARGET)\n+  AC_SUBST(OLDEST_BOOT_JDK_VERSION)\n","filename":"make\/autoconf\/boot-jdk.m4","additions":2,"deletions":4,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -36,1 +36,1 @@\n-    $(UPGRADEABLE_PLATFORM_MODULES)\n+    $(UPGRADEABLE_PLATFORM_MODULES) $(CUSTOM_UPGRADEABLE_PLATFORM_MODULES)\n@@ -235,1 +235,1 @@\n-  $(DEPS_$(strip $1))\n+  $(filter-out $(IMPORT_MODULES), $(DEPS_$(strip $1)))\n@@ -273,1 +273,2 @@\n-    $(sort $(filter-out $(MODULES_FILTER), $(UPGRADEABLE_PLATFORM_MODULES)))\n+    $(sort $(filter-out $(MODULES_FILTER), \\\n+    $(UPGRADEABLE_PLATFORM_MODULES) $(CUSTOM_UPGRADEABLE_PLATFORM_MODULES)))\n@@ -335,0 +336,13 @@\n+################################################################################\n+# Get a full snippet path for the current module and a given base name.\n+#\n+# Param 1 - The base name of the snippet file to include\n+GetModuleSnippetName = \\\n+  $(if $(CUSTOM_MODULE_MAKE_ROOT), \\\n+    $(if $(wildcard $(CUSTOM_MODULE_MAKE_ROOT)\/$(MODULE)\/$(strip $1).gmk), \\\n+      $(CUSTOM_MODULE_MAKE_ROOT)\/$(MODULE)\/$(strip $1).gmk, \\\n+      $(wildcard modules\/$(MODULE)\/$(strip $1).gmk) \\\n+    ), \\\n+    $(wildcard modules\/$(MODULE)\/$(strip $1).gmk) \\\n+  )\n+\n","filename":"make\/common\/Modules.gmk","additions":17,"deletions":3,"binary":false,"changes":20,"status":"modified"},{"patch":"@@ -16353,32 +16353,0 @@\n-instruct cmpFastLock(rFlagsReg cr, iRegP object, iRegP box, iRegPNoSp tmp, iRegPNoSp tmp2, iRegPNoSp tmp3)\n-%{\n-  predicate(LockingMode != LM_LIGHTWEIGHT);\n-  match(Set cr (FastLock object box));\n-  effect(TEMP tmp, TEMP tmp2, TEMP tmp3);\n-\n-  ins_cost(5 * INSN_COST);\n-  format %{ \"fastlock $object,$box\\t! kills $tmp,$tmp2,$tmp3\" %}\n-\n-  ins_encode %{\n-    __ fast_lock($object$$Register, $box$$Register, $tmp$$Register, $tmp2$$Register, $tmp3$$Register);\n-  %}\n-\n-  ins_pipe(pipe_serial);\n-%}\n-\n-instruct cmpFastUnlock(rFlagsReg cr, iRegP object, iRegP box, iRegPNoSp tmp, iRegPNoSp tmp2)\n-%{\n-  predicate(LockingMode != LM_LIGHTWEIGHT);\n-  match(Set cr (FastUnlock object box));\n-  effect(TEMP tmp, TEMP tmp2);\n-\n-  ins_cost(5 * INSN_COST);\n-  format %{ \"fastunlock $object,$box\\t! kills $tmp, $tmp2\" %}\n-\n-  ins_encode %{\n-    __ fast_unlock($object$$Register, $box$$Register, $tmp$$Register, $tmp2$$Register);\n-  %}\n-\n-  ins_pipe(pipe_serial);\n-%}\n-\n@@ -16387,1 +16355,0 @@\n-  predicate(LockingMode == LM_LIGHTWEIGHT);\n@@ -16403,1 +16370,0 @@\n-  predicate(LockingMode == LM_LIGHTWEIGHT);\n","filename":"src\/hotspot\/cpu\/aarch64\/aarch64.ad","additions":0,"deletions":34,"binary":false,"changes":34,"status":"modified"},{"patch":"@@ -415,5 +415,1 @@\n-    if (LockingMode == LM_MONITOR) {\n-      __ b(*stub->entry());\n-    } else {\n-      __ unlock_object(r5, r4, r0, r6, *stub->entry());\n-    }\n+    __ unlock_object(r5, r4, r0, r6, *stub->entry());\n@@ -2685,7 +2681,1 @@\n-  if (LockingMode == LM_MONITOR) {\n-    if (op->info() != nullptr) {\n-      add_debug_info_for_null_check_here(op->info());\n-      __ null_check(obj, -1);\n-    }\n-    __ b(*op->stub()->entry());\n-  } else if (op->code() == lir_lock) {\n+  if (op->code() == lir_lock) {\n@@ -3044,1 +3034,1 @@\n-  __ lea(dest->as_register_lo(), as_Address(addr->as_address_ptr()));\n+  __ lea(dest->as_pointer_register(), as_Address(addr->as_address_ptr()));\n","filename":"src\/hotspot\/cpu\/aarch64\/c1_LIRAssembler_aarch64.cpp","additions":3,"deletions":13,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -997,1 +997,1 @@\n-      LIR_Opr addr = new_pointer_register();\n+      LIR_Opr addr = new_register(T_ADDRESS);\n@@ -1074,1 +1074,1 @@\n-      LIR_Opr addr = new_pointer_register();\n+      LIR_Opr addr = new_register(T_ADDRESS);\n","filename":"src\/hotspot\/cpu\/aarch64\/c1_LIRGenerator_aarch64.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -65,2 +65,0 @@\n-  const int aligned_mask = BytesPerWord -1;\n-  const int hdr_offset = oopDesc::mark_offset_in_bytes();\n@@ -77,55 +75,2 @@\n-  if (LockingMode == LM_LIGHTWEIGHT) {\n-    lightweight_lock(disp_hdr, obj, hdr, temp, rscratch2, slow_case);\n-  } else if (LockingMode == LM_LEGACY) {\n-\n-    if (DiagnoseSyncOnValueBasedClasses != 0) {\n-      load_klass(hdr, obj);\n-      ldrb(hdr, Address(hdr, Klass::misc_flags_offset()));\n-      tst(hdr, KlassFlags::_misc_is_value_based_class);\n-      br(Assembler::NE, slow_case);\n-    }\n-\n-    Label done;\n-    \/\/ Load object header\n-    ldr(hdr, Address(obj, hdr_offset));\n-    \/\/ and mark it as unlocked\n-    orr(hdr, hdr, markWord::unlocked_value);\n-\n-    if (EnableValhalla) {\n-      \/\/ Mask always_locked bit such that we go to the slow path if object is an inline type\n-      andr(hdr, hdr, ~markWord::inline_type_bit_in_place);\n-    }\n-\n-    \/\/ save unlocked object header into the displaced header location on the stack\n-    str(hdr, Address(disp_hdr, 0));\n-    \/\/ test if object header is still the same (i.e. unlocked), and if so, store the\n-    \/\/ displaced header address in the object header - if it is not the same, get the\n-    \/\/ object header instead\n-    lea(rscratch2, Address(obj, hdr_offset));\n-    cmpxchgptr(hdr, disp_hdr, rscratch2, rscratch1, done, \/*fallthough*\/nullptr);\n-    \/\/ if the object header was the same, we're done\n-    \/\/ if the object header was not the same, it is now in the hdr register\n-    \/\/ => test if it is a stack pointer into the same stack (recursive locking), i.e.:\n-    \/\/\n-    \/\/ 1) (hdr & aligned_mask) == 0\n-    \/\/ 2) sp <= hdr\n-    \/\/ 3) hdr <= sp + page_size\n-    \/\/\n-    \/\/ these 3 tests can be done by evaluating the following expression:\n-    \/\/\n-    \/\/ (hdr - sp) & (aligned_mask - page_size)\n-    \/\/\n-    \/\/ assuming both the stack pointer and page_size have their least\n-    \/\/ significant 2 bits cleared and page_size is a power of 2\n-    mov(rscratch1, sp);\n-    sub(hdr, hdr, rscratch1);\n-    ands(hdr, hdr, aligned_mask - (int)os::vm_page_size());\n-    \/\/ for recursive locking, the result is zero => save it in the displaced header\n-    \/\/ location (null in the displaced hdr location indicates recursive locking)\n-    str(hdr, Address(disp_hdr, 0));\n-    \/\/ otherwise we don't care about the result and handle locking via runtime call\n-    cbnz(hdr, slow_case);\n-    \/\/ done\n-    bind(done);\n-    inc_held_monitor_count(rscratch1);\n-  }\n+  lightweight_lock(disp_hdr, obj, hdr, temp, rscratch2, slow_case);\n+\n@@ -137,11 +82,0 @@\n-  const int aligned_mask = BytesPerWord -1;\n-  const int hdr_offset = oopDesc::mark_offset_in_bytes();\n-  Label done;\n-\n-  if (LockingMode != LM_LIGHTWEIGHT) {\n-    \/\/ load displaced header\n-    ldr(hdr, Address(disp_hdr, 0));\n-    \/\/ if the loaded hdr is null we had recursive locking\n-    \/\/ if we had recursive locking, we are done\n-    cbz(hdr, done);\n-  }\n@@ -154,18 +88,1 @@\n-  if (LockingMode == LM_LIGHTWEIGHT) {\n-    lightweight_unlock(obj, hdr, temp, rscratch2, slow_case);\n-  } else if (LockingMode == LM_LEGACY) {\n-    \/\/ test if object header is pointing to the displaced header, and if so, restore\n-    \/\/ the displaced header in the object - if the object header is not pointing to\n-    \/\/ the displaced header, get the object header instead\n-    \/\/ if the object header was not pointing to the displaced header,\n-    \/\/ we do unlocking via runtime call\n-    if (hdr_offset) {\n-      lea(rscratch1, Address(obj, hdr_offset));\n-      cmpxchgptr(disp_hdr, hdr, rscratch1, rscratch2, done, &slow_case);\n-    } else {\n-      cmpxchgptr(disp_hdr, hdr, obj, rscratch2, done, &slow_case);\n-    }\n-    \/\/ done\n-    bind(done);\n-    dec_held_monitor_count(rscratch1);\n-  }\n+  lightweight_unlock(obj, hdr, temp, rscratch2, slow_case);\n","filename":"src\/hotspot\/cpu\/aarch64\/c1_MacroAssembler_aarch64.cpp","additions":3,"deletions":86,"binary":false,"changes":89,"status":"modified"},{"patch":"@@ -171,211 +171,0 @@\n-void C2_MacroAssembler::fast_lock(Register objectReg, Register boxReg, Register tmpReg,\n-                                  Register tmp2Reg, Register tmp3Reg) {\n-  Register oop = objectReg;\n-  Register box = boxReg;\n-  Register disp_hdr = tmpReg;\n-  Register tmp = tmp2Reg;\n-  Label cont;\n-  Label object_has_monitor;\n-  Label count, no_count;\n-\n-  assert(LockingMode != LM_LIGHTWEIGHT, \"lightweight locking should use fast_lock_lightweight\");\n-  assert_different_registers(oop, box, tmp, disp_hdr, rscratch2);\n-\n-  \/\/ Load markWord from object into displaced_header.\n-  ldr(disp_hdr, Address(oop, oopDesc::mark_offset_in_bytes()));\n-\n-  if (DiagnoseSyncOnValueBasedClasses != 0) {\n-    load_klass(tmp, oop);\n-    ldrb(tmp, Address(tmp, Klass::misc_flags_offset()));\n-    tst(tmp, KlassFlags::_misc_is_value_based_class);\n-    br(Assembler::NE, cont);\n-  }\n-\n-  \/\/ Check for existing monitor\n-  tbnz(disp_hdr, exact_log2(markWord::monitor_value), object_has_monitor);\n-\n-  if (LockingMode == LM_MONITOR) {\n-    tst(oop, oop); \/\/ Set NE to indicate 'failure' -> take slow-path. We know that oop != 0.\n-    b(cont);\n-  } else {\n-    assert(LockingMode == LM_LEGACY, \"must be\");\n-    \/\/ Set tmp to be (markWord of object | UNLOCK_VALUE).\n-    orr(tmp, disp_hdr, markWord::unlocked_value);\n-\n-    if (EnableValhalla) {\n-      \/\/ Mask inline_type bit such that we go to the slow path if object is an inline type\n-      andr(tmp, tmp, ~((int) markWord::inline_type_bit_in_place));\n-    }\n-\n-    \/\/ Initialize the box. (Must happen before we update the object mark!)\n-    str(tmp, Address(box, BasicLock::displaced_header_offset_in_bytes()));\n-\n-    \/\/ Compare object markWord with an unlocked value (tmp) and if\n-    \/\/ equal exchange the stack address of our box with object markWord.\n-    \/\/ On failure disp_hdr contains the possibly locked markWord.\n-    cmpxchg(oop, tmp, box, Assembler::xword, \/*acquire*\/ true,\n-            \/*release*\/ true, \/*weak*\/ false, disp_hdr);\n-    br(Assembler::EQ, cont);\n-\n-    assert(oopDesc::mark_offset_in_bytes() == 0, \"offset of _mark is not 0\");\n-\n-    \/\/ If the compare-and-exchange succeeded, then we found an unlocked\n-    \/\/ object, will have now locked it will continue at label cont\n-\n-    \/\/ Check if the owner is self by comparing the value in the\n-    \/\/ markWord of object (disp_hdr) with the stack pointer.\n-    mov(rscratch1, sp);\n-    sub(disp_hdr, disp_hdr, rscratch1);\n-    mov(tmp, (address) (~(os::vm_page_size()-1) | markWord::lock_mask_in_place));\n-    \/\/ If condition is true we are cont and hence we can store 0 as the\n-    \/\/ displaced header in the box, which indicates that it is a recursive lock.\n-    ands(tmp\/*==0?*\/, disp_hdr, tmp);   \/\/ Sets flags for result\n-    str(tmp\/*==0, perhaps*\/, Address(box, BasicLock::displaced_header_offset_in_bytes()));\n-    b(cont);\n-  }\n-\n-  \/\/ Handle existing monitor.\n-  bind(object_has_monitor);\n-\n-  \/\/ Try to CAS owner (no owner => current thread's _monitor_owner_id).\n-  ldr(rscratch2, Address(rthread, JavaThread::monitor_owner_id_offset()));\n-  add(tmp, disp_hdr, (in_bytes(ObjectMonitor::owner_offset())-markWord::monitor_value));\n-  cmpxchg(tmp, zr, rscratch2, Assembler::xword, \/*acquire*\/ true,\n-          \/*release*\/ true, \/*weak*\/ false, tmp3Reg); \/\/ Sets flags for result\n-\n-  \/\/ Store a non-null value into the box to avoid looking like a re-entrant\n-  \/\/ lock. The fast-path monitor unlock code checks for\n-  \/\/ markWord::monitor_value so use markWord::unused_mark which has the\n-  \/\/ relevant bit set, and also matches ObjectSynchronizer::enter.\n-  mov(tmp, (address)markWord::unused_mark().value());\n-  str(tmp, Address(box, BasicLock::displaced_header_offset_in_bytes()));\n-\n-  br(Assembler::EQ, cont); \/\/ CAS success means locking succeeded\n-\n-  cmp(tmp3Reg, rscratch2);\n-  br(Assembler::NE, cont); \/\/ Check for recursive locking\n-\n-  \/\/ Recursive lock case\n-  increment(Address(disp_hdr, in_bytes(ObjectMonitor::recursions_offset()) - markWord::monitor_value), 1);\n-  \/\/ flag == EQ still from the cmp above, checking if this is a reentrant lock\n-\n-  bind(cont);\n-  \/\/ flag == EQ indicates success\n-  \/\/ flag == NE indicates failure\n-  br(Assembler::NE, no_count);\n-\n-  bind(count);\n-  if (LockingMode == LM_LEGACY) {\n-    inc_held_monitor_count(rscratch1);\n-  }\n-\n-  bind(no_count);\n-}\n-\n-void C2_MacroAssembler::fast_unlock(Register objectReg, Register boxReg, Register tmpReg,\n-                                    Register tmp2Reg) {\n-  Register oop = objectReg;\n-  Register box = boxReg;\n-  Register disp_hdr = tmpReg;\n-  Register owner_addr = tmpReg;\n-  Register tmp = tmp2Reg;\n-  Label cont;\n-  Label object_has_monitor;\n-  Label count, no_count;\n-  Label unlocked;\n-\n-  assert(LockingMode != LM_LIGHTWEIGHT, \"lightweight locking should use fast_unlock_lightweight\");\n-  assert_different_registers(oop, box, tmp, disp_hdr);\n-\n-  if (LockingMode == LM_LEGACY) {\n-    \/\/ Find the lock address and load the displaced header from the stack.\n-    ldr(disp_hdr, Address(box, BasicLock::displaced_header_offset_in_bytes()));\n-\n-    \/\/ If the displaced header is 0, we have a recursive unlock.\n-    cmp(disp_hdr, zr);\n-    br(Assembler::EQ, cont);\n-  }\n-\n-  \/\/ Handle existing monitor.\n-  ldr(tmp, Address(oop, oopDesc::mark_offset_in_bytes()));\n-  tbnz(tmp, exact_log2(markWord::monitor_value), object_has_monitor);\n-\n-  if (LockingMode == LM_MONITOR) {\n-    tst(oop, oop); \/\/ Set NE to indicate 'failure' -> take slow-path. We know that oop != 0.\n-    b(cont);\n-  } else {\n-    assert(LockingMode == LM_LEGACY, \"must be\");\n-    \/\/ Check if it is still a light weight lock, this is is true if we\n-    \/\/ see the stack address of the basicLock in the markWord of the\n-    \/\/ object.\n-\n-    cmpxchg(oop, box, disp_hdr, Assembler::xword, \/*acquire*\/ false,\n-            \/*release*\/ true, \/*weak*\/ false, tmp);\n-    b(cont);\n-  }\n-\n-  assert(oopDesc::mark_offset_in_bytes() == 0, \"offset of _mark is not 0\");\n-\n-  \/\/ Handle existing monitor.\n-  bind(object_has_monitor);\n-  STATIC_ASSERT(markWord::monitor_value <= INT_MAX);\n-  add(tmp, tmp, -(int)markWord::monitor_value); \/\/ monitor\n-\n-  ldr(disp_hdr, Address(tmp, ObjectMonitor::recursions_offset()));\n-\n-  Label notRecursive;\n-  cbz(disp_hdr, notRecursive);\n-\n-  \/\/ Recursive lock\n-  sub(disp_hdr, disp_hdr, 1u);\n-  str(disp_hdr, Address(tmp, ObjectMonitor::recursions_offset()));\n-  cmp(disp_hdr, disp_hdr); \/\/ Sets flags for result\n-  b(cont);\n-\n-  bind(notRecursive);\n-\n-  \/\/ Compute owner address.\n-  lea(owner_addr, Address(tmp, ObjectMonitor::owner_offset()));\n-\n-  \/\/ Set owner to null.\n-  \/\/ Release to satisfy the JMM\n-  stlr(zr, owner_addr);\n-  \/\/ We need a full fence after clearing owner to avoid stranding.\n-  \/\/ StoreLoad achieves this.\n-  membar(StoreLoad);\n-\n-  \/\/ Check if the entry_list is empty.\n-  ldr(rscratch1, Address(tmp, ObjectMonitor::entry_list_offset()));\n-  cmp(rscratch1, zr);\n-  br(Assembler::EQ, cont);     \/\/ If so we are done.\n-\n-  \/\/ Check if there is a successor.\n-  ldr(rscratch1, Address(tmp, ObjectMonitor::succ_offset()));\n-  cmp(rscratch1, zr);\n-  br(Assembler::NE, unlocked); \/\/ If so we are done.\n-\n-  \/\/ Save the monitor pointer in the current thread, so we can try to\n-  \/\/ reacquire the lock in SharedRuntime::monitor_exit_helper().\n-  str(tmp, Address(rthread, JavaThread::unlocked_inflated_monitor_offset()));\n-\n-  cmp(zr, rthread); \/\/ Set Flag to NE => slow path\n-  b(cont);\n-\n-  bind(unlocked);\n-  cmp(zr, zr); \/\/ Set Flag to EQ => fast path\n-\n-  \/\/ Intentional fall-through\n-\n-  bind(cont);\n-  \/\/ flag == EQ indicates success\n-  \/\/ flag == NE indicates failure\n-  br(Assembler::NE, no_count);\n-\n-  bind(count);\n-  if (LockingMode == LM_LEGACY) {\n-    dec_held_monitor_count(rscratch1);\n-  }\n-\n-  bind(no_count);\n-}\n-\n@@ -384,1 +173,0 @@\n-  assert(LockingMode == LM_LIGHTWEIGHT, \"must be\");\n@@ -541,1 +329,0 @@\n-  assert(LockingMode == LM_LIGHTWEIGHT, \"must be\");\n","filename":"src\/hotspot\/cpu\/aarch64\/c2_MacroAssembler_aarch64.cpp","additions":0,"deletions":213,"binary":false,"changes":213,"status":"modified"},{"patch":"@@ -56,3 +56,0 @@\n-  \/\/ Code used by cmpFastLock and cmpFastUnlock mach instructions in .ad file.\n-  void fast_lock(Register object, Register box, Register tmp, Register tmp2, Register tmp3);\n-  void fast_unlock(Register object, Register box, Register tmp, Register tmp2);\n","filename":"src\/hotspot\/cpu\/aarch64\/c2_MacroAssembler_aarch64.hpp","additions":0,"deletions":3,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -830,41 +830,4 @@\n-  if (LockingMode == LM_MONITOR) {\n-    call_VM_preemptable(noreg,\n-            CAST_FROM_FN_PTR(address, InterpreterRuntime::monitorenter),\n-            lock_reg);\n-  } else {\n-    Label count, done;\n-\n-    const Register swap_reg = r0;\n-    const Register tmp = c_rarg2;\n-    const Register obj_reg = c_rarg3; \/\/ Will contain the oop\n-    const Register tmp2 = c_rarg4;\n-    const Register tmp3 = c_rarg5;\n-\n-    const int obj_offset = in_bytes(BasicObjectLock::obj_offset());\n-    const int lock_offset = in_bytes(BasicObjectLock::lock_offset());\n-    const int mark_offset = lock_offset +\n-                            BasicLock::displaced_header_offset_in_bytes();\n-\n-    Label slow_case;\n-\n-    \/\/ Load object pointer into obj_reg %c_rarg3\n-    ldr(obj_reg, Address(lock_reg, obj_offset));\n-\n-    if (LockingMode == LM_LIGHTWEIGHT) {\n-      lightweight_lock(lock_reg, obj_reg, tmp, tmp2, tmp3, slow_case);\n-      b(done);\n-    } else if (LockingMode == LM_LEGACY) {\n-\n-      if (DiagnoseSyncOnValueBasedClasses != 0) {\n-        load_klass(tmp, obj_reg);\n-        ldrb(tmp, Address(tmp, Klass::misc_flags_offset()));\n-        tst(tmp, KlassFlags::_misc_is_value_based_class);\n-        br(Assembler::NE, slow_case);\n-      }\n-      \/\/ Load (object->mark() | 1) into swap_reg\n-      ldr(rscratch1, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n-      orr(swap_reg, rscratch1, 1);\n-      if (EnableValhalla) {\n-        \/\/ Mask inline_type bit such that we go to the slow path if object is an inline type\n-        andr(swap_reg, swap_reg, ~((int) markWord::inline_type_bit_in_place));\n-      }\n+  const Register tmp = c_rarg2;\n+  const Register obj_reg = c_rarg3; \/\/ Will contain the oop\n+  const Register tmp2 = c_rarg4;\n+  const Register tmp3 = c_rarg5;\n@@ -873,51 +836,6 @@\n-      \/\/ Save (object->mark() | 1) into BasicLock's displaced header\n-      str(swap_reg, Address(lock_reg, mark_offset));\n-\n-      assert(lock_offset == 0,\n-             \"displached header must be first word in BasicObjectLock\");\n-\n-      Label fail;\n-      cmpxchg_obj_header(swap_reg, lock_reg, obj_reg, rscratch1, count, \/*fallthrough*\/nullptr);\n-\n-      \/\/ Fast check for recursive lock.\n-      \/\/\n-      \/\/ Can apply the optimization only if this is a stack lock\n-      \/\/ allocated in this thread. For efficiency, we can focus on\n-      \/\/ recently allocated stack locks (instead of reading the stack\n-      \/\/ base and checking whether 'mark' points inside the current\n-      \/\/ thread stack):\n-      \/\/  1) (mark & 7) == 0, and\n-      \/\/  2) sp <= mark < mark + os::pagesize()\n-      \/\/\n-      \/\/ Warning: sp + os::pagesize can overflow the stack base. We must\n-      \/\/ neither apply the optimization for an inflated lock allocated\n-      \/\/ just above the thread stack (this is why condition 1 matters)\n-      \/\/ nor apply the optimization if the stack lock is inside the stack\n-      \/\/ of another thread. The latter is avoided even in case of overflow\n-      \/\/ because we have guard pages at the end of all stacks. Hence, if\n-      \/\/ we go over the stack base and hit the stack of another thread,\n-      \/\/ this should not be in a writeable area that could contain a\n-      \/\/ stack lock allocated by that thread. As a consequence, a stack\n-      \/\/ lock less than page size away from sp is guaranteed to be\n-      \/\/ owned by the current thread.\n-      \/\/\n-      \/\/ These 3 tests can be done by evaluating the following\n-      \/\/ expression: ((mark - sp) & (7 - os::vm_page_size())),\n-      \/\/ assuming both stack pointer and pagesize have their\n-      \/\/ least significant 3 bits clear.\n-      \/\/ NOTE: the mark is in swap_reg %r0 as the result of cmpxchg\n-      \/\/ NOTE2: aarch64 does not like to subtract sp from rn so take a\n-      \/\/ copy\n-      mov(rscratch1, sp);\n-      sub(swap_reg, swap_reg, rscratch1);\n-      ands(swap_reg, swap_reg, (uint64_t)(7 - (int)os::vm_page_size()));\n-\n-      \/\/ Save the test result, for recursive case, the result is zero\n-      str(swap_reg, Address(lock_reg, mark_offset));\n-      br(Assembler::NE, slow_case);\n-\n-      bind(count);\n-      inc_held_monitor_count(rscratch1);\n-      b(done);\n-    }\n-    bind(slow_case);\n+  \/\/ Load object pointer into obj_reg %c_rarg3\n+  ldr(obj_reg, Address(lock_reg, BasicObjectLock::obj_offset()));\n+\n+  Label slow_case, done;\n+  lightweight_lock(lock_reg, obj_reg, tmp, tmp2, tmp3, slow_case);\n+  b(done);\n@@ -925,4 +843,1 @@\n-    \/\/ Call the runtime routine for slow case\n-    call_VM_preemptable(noreg,\n-            CAST_FROM_FN_PTR(address, InterpreterRuntime::monitorenter),\n-            lock_reg);\n+  bind(slow_case);\n@@ -930,2 +845,6 @@\n-    bind(done);\n-  }\n+  \/\/ Call the runtime routine for slow case\n+  call_VM_preemptable(noreg,\n+          CAST_FROM_FN_PTR(address, InterpreterRuntime::monitorenter),\n+          lock_reg);\n+\n+  bind(done);\n@@ -950,20 +869,4 @@\n-  if (LockingMode == LM_MONITOR) {\n-    call_VM_leaf(CAST_FROM_FN_PTR(address, InterpreterRuntime::monitorexit), lock_reg);\n-  } else {\n-    Label count, done;\n-\n-    const Register swap_reg   = r0;\n-    const Register header_reg = c_rarg2;  \/\/ Will contain the old oopMark\n-    const Register obj_reg    = c_rarg3;  \/\/ Will contain the oop\n-    const Register tmp_reg    = c_rarg4;  \/\/ Temporary used by lightweight_unlock\n-\n-    save_bcp(); \/\/ Save in case of exception\n-\n-    if (LockingMode != LM_LIGHTWEIGHT) {\n-      \/\/ Convert from BasicObjectLock structure to object and BasicLock\n-      \/\/ structure Store the BasicLock address into %r0\n-      lea(swap_reg, Address(lock_reg, BasicObjectLock::lock_offset()));\n-    }\n-\n-    \/\/ Load oop into obj_reg(%c_rarg3)\n-    ldr(obj_reg, Address(lock_reg, BasicObjectLock::obj_offset()));\n+  const Register swap_reg   = r0;\n+  const Register header_reg = c_rarg2;  \/\/ Will contain the old oopMark\n+  const Register obj_reg    = c_rarg3;  \/\/ Will contain the oop\n+  const Register tmp_reg    = c_rarg4;  \/\/ Temporary used by lightweight_unlock\n@@ -971,2 +874,1 @@\n-    \/\/ Free entry\n-    str(zr, Address(lock_reg, BasicObjectLock::obj_offset()));\n+  save_bcp(); \/\/ Save in case of exception\n@@ -974,8 +876,2 @@\n-    Label slow_case;\n-    if (LockingMode == LM_LIGHTWEIGHT) {\n-      lightweight_unlock(obj_reg, header_reg, swap_reg, tmp_reg, slow_case);\n-      b(done);\n-    } else if (LockingMode == LM_LEGACY) {\n-      \/\/ Load the old header from BasicLock structure\n-      ldr(header_reg, Address(swap_reg,\n-                              BasicLock::displaced_header_offset_in_bytes()));\n+  \/\/ Load oop into obj_reg(%c_rarg3)\n+  ldr(obj_reg, Address(lock_reg, BasicObjectLock::obj_offset()));\n@@ -983,2 +879,2 @@\n-      \/\/ Test for recursion\n-      cbz(header_reg, count);\n+  \/\/ Free entry\n+  str(zr, Address(lock_reg, BasicObjectLock::obj_offset()));\n@@ -986,7 +882,3 @@\n-      \/\/ Atomic swap back the old header\n-      cmpxchg_obj_header(swap_reg, header_reg, obj_reg, rscratch1, count, &slow_case);\n-\n-      bind(count);\n-      dec_held_monitor_count(rscratch1);\n-      b(done);\n-    }\n+  Label slow_case, done;\n+  lightweight_unlock(obj_reg, header_reg, swap_reg, tmp_reg, slow_case);\n+  b(done);\n@@ -994,7 +886,6 @@\n-    bind(slow_case);\n-    \/\/ Call the runtime routine for slow case.\n-    str(obj_reg, Address(lock_reg, BasicObjectLock::obj_offset())); \/\/ restore obj\n-    call_VM_leaf(CAST_FROM_FN_PTR(address, InterpreterRuntime::monitorexit), lock_reg);\n-    bind(done);\n-    restore_bcp();\n-  }\n+  bind(slow_case);\n+  \/\/ Call the runtime routine for slow case.\n+  str(obj_reg, Address(lock_reg, BasicObjectLock::obj_offset())); \/\/ restore obj\n+  call_VM_leaf(CAST_FROM_FN_PTR(address, InterpreterRuntime::monitorexit), lock_reg);\n+  bind(done);\n+  restore_bcp();\n","filename":"src\/hotspot\/cpu\/aarch64\/interp_masm_aarch64.cpp","additions":35,"deletions":144,"binary":false,"changes":179,"status":"modified"},{"patch":"@@ -7892,1 +7892,0 @@\n-  assert(LockingMode == LM_LIGHTWEIGHT, \"only used with new lightweight locking\");\n@@ -7956,1 +7955,0 @@\n-  assert(LockingMode == LM_LIGHTWEIGHT, \"only used with new lightweight locking\");\n","filename":"src\/hotspot\/cpu\/aarch64\/macroAssembler_aarch64.cpp","additions":0,"deletions":2,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -1019,1 +1019,10 @@\n-    new_adapter = AdapterBlob::create(masm->code(), frame_complete, frame_size_in_words, oop_maps, caller_must_gc_arguments);\n+    int entry_offset[AdapterHandlerEntry::ENTRIES_COUNT];\n+    assert(AdapterHandlerEntry::ENTRIES_COUNT == 7, \"sanity\");\n+    entry_offset[0] = 0; \/\/ i2c_entry offset\n+    entry_offset[1] = c2i_entry - i2c_entry;\n+    entry_offset[2] = c2i_inline_entry - i2c_entry;\n+    entry_offset[3] = c2i_inline_ro_entry - i2c_entry;\n+    entry_offset[4] = c2i_unverified_entry - i2c_entry;\n+    entry_offset[5] = c2i_unverified_inline_entry - i2c_entry;\n+    entry_offset[6] = c2i_no_clinit_check_entry - i2c_entry;\n+    new_adapter = AdapterBlob::create(masm->code(), entry_offset, frame_complete, frame_size_in_words, oop_maps, caller_must_gc_arguments);\n@@ -1999,1 +2008,1 @@\n-  if (LockingMode != LM_LEGACY && method->is_object_wait0()) {\n+  if (method->is_object_wait0()) {\n@@ -2054,42 +2063,1 @@\n-    if (LockingMode == LM_MONITOR) {\n-      __ b(slow_path_lock);\n-    } else if (LockingMode == LM_LEGACY) {\n-      \/\/ Load (object->mark() | 1) into swap_reg %r0\n-      __ ldr(rscratch1, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n-      __ orr(swap_reg, rscratch1, 1);\n-      if (EnableValhalla) {\n-        \/\/ Mask inline_type bit such that we go to the slow path if object is an inline type\n-        __ andr(swap_reg, swap_reg, ~((int) markWord::inline_type_bit_in_place));\n-      }\n-\n-      \/\/ Save (object->mark() | 1) into BasicLock's displaced header\n-      __ str(swap_reg, Address(lock_reg, mark_word_offset));\n-\n-      \/\/ src -> dest iff dest == r0 else r0 <- dest\n-      __ cmpxchg_obj_header(r0, lock_reg, obj_reg, rscratch1, count, \/*fallthrough*\/nullptr);\n-\n-      \/\/ Hmm should this move to the slow path code area???\n-\n-      \/\/ Test if the oopMark is an obvious stack pointer, i.e.,\n-      \/\/  1) (mark & 3) == 0, and\n-      \/\/  2) sp <= mark < mark + os::pagesize()\n-      \/\/ These 3 tests can be done by evaluating the following\n-      \/\/ expression: ((mark - sp) & (3 - os::vm_page_size())),\n-      \/\/ assuming both stack pointer and pagesize have their\n-      \/\/ least significant 2 bits clear.\n-      \/\/ NOTE: the oopMark is in swap_reg %r0 as the result of cmpxchg\n-\n-      __ sub(swap_reg, sp, swap_reg);\n-      __ neg(swap_reg, swap_reg);\n-      __ ands(swap_reg, swap_reg, 3 - (int)os::vm_page_size());\n-\n-      \/\/ Save the test result, for recursive case, the result is zero\n-      __ str(swap_reg, Address(lock_reg, mark_word_offset));\n-      __ br(Assembler::NE, slow_path_lock);\n-\n-      __ bind(count);\n-      __ inc_held_monitor_count(rscratch1);\n-    } else {\n-      assert(LockingMode == LM_LIGHTWEIGHT, \"must be\");\n-      __ lightweight_lock(lock_reg, obj_reg, swap_reg, tmp, lock_tmp, slow_path_lock);\n-    }\n+    __ lightweight_lock(lock_reg, obj_reg, swap_reg, tmp, lock_tmp, slow_path_lock);\n@@ -2170,1 +2138,1 @@\n-  if (LockingMode != LM_LEGACY && method->is_object_wait0()) {\n+  if (method->is_object_wait0()) {\n@@ -2199,12 +2167,0 @@\n-    Label done, not_recursive;\n-\n-    if (LockingMode == LM_LEGACY) {\n-      \/\/ Simple recursive lock?\n-      __ ldr(rscratch1, Address(sp, lock_slot_offset * VMRegImpl::stack_slot_size));\n-      __ cbnz(rscratch1, not_recursive);\n-      __ dec_held_monitor_count(rscratch1);\n-      __ b(done);\n-    }\n-\n-    __ bind(not_recursive);\n-\n@@ -2216,17 +2172,1 @@\n-    if (LockingMode == LM_MONITOR) {\n-      __ b(slow_path_unlock);\n-    } else if (LockingMode == LM_LEGACY) {\n-      \/\/ get address of the stack lock\n-      __ lea(r0, Address(sp, lock_slot_offset * VMRegImpl::stack_slot_size));\n-      \/\/  get old displaced header\n-      __ ldr(old_hdr, Address(r0, 0));\n-\n-      \/\/ Atomic swap old header if oop still contains the stack lock\n-      Label count;\n-      __ cmpxchg_obj_header(r0, old_hdr, obj_reg, rscratch1, count, &slow_path_unlock);\n-      __ bind(count);\n-      __ dec_held_monitor_count(rscratch1);\n-    } else {\n-      assert(LockingMode == LM_LIGHTWEIGHT, \"\");\n-      __ lightweight_unlock(obj_reg, old_hdr, swap_reg, lock_tmp, slow_path_unlock);\n-    }\n+    __ lightweight_unlock(obj_reg, old_hdr, swap_reg, lock_tmp, slow_path_unlock);\n@@ -2239,2 +2179,0 @@\n-\n-    __ bind(done);\n","filename":"src\/hotspot\/cpu\/aarch64\/sharedRuntime_aarch64.cpp","additions":14,"deletions":76,"binary":false,"changes":90,"status":"modified"},{"patch":"@@ -1487,16 +1487,11 @@\n-  if (LockingMode != LM_LEGACY) {\n-    \/\/ Check preemption for Object.wait()\n-    Label not_preempted;\n-    __ ldr(rscratch1, Address(rthread, JavaThread::preempt_alternate_return_offset()));\n-    __ cbz(rscratch1, not_preempted);\n-    __ str(zr, Address(rthread, JavaThread::preempt_alternate_return_offset()));\n-    __ br(rscratch1);\n-    __ bind(native_return);\n-    __ restore_after_resume(true \/* is_native *\/);\n-    \/\/ reload result_handler\n-    __ ldr(result_handler, Address(rfp, frame::interpreter_frame_result_handler_offset*wordSize));\n-    __ bind(not_preempted);\n-  } else {\n-    \/\/ any pc will do so just use this one for LM_LEGACY to keep code together.\n-    __ bind(native_return);\n-  }\n+  \/\/ Check preemption for Object.wait()\n+  Label not_preempted;\n+  __ ldr(rscratch1, Address(rthread, JavaThread::preempt_alternate_return_offset()));\n+  __ cbz(rscratch1, not_preempted);\n+  __ str(zr, Address(rthread, JavaThread::preempt_alternate_return_offset()));\n+  __ br(rscratch1);\n+  __ bind(native_return);\n+  __ restore_after_resume(true \/* is_native *\/);\n+  \/\/ reload result_handler\n+  __ ldr(result_handler, Address(rfp, frame::interpreter_frame_result_handler_offset*wordSize));\n+  __ bind(not_preempted);\n","filename":"src\/hotspot\/cpu\/aarch64\/templateInterpreterGenerator_aarch64.cpp","additions":11,"deletions":16,"binary":false,"changes":27,"status":"modified"},{"patch":"@@ -418,5 +418,1 @@\n-    if (LockingMode == LM_MONITOR) {\n-      __ jmp(*stub->entry());\n-    } else {\n-      __ unlock_object(rdi, rsi, rax, *stub->entry());\n-    }\n+    __ unlock_object(rdi, rsi, rax, *stub->entry());\n@@ -2937,7 +2933,1 @@\n-  if (LockingMode == LM_MONITOR) {\n-    if (op->info() != nullptr) {\n-      add_debug_info_for_null_check_here(op->info());\n-      __ null_check(obj);\n-    }\n-    __ jmp(*op->stub()->entry());\n-  } else if (op->code() == lir_lock) {\n+  if (op->code() == lir_lock) {\n@@ -2945,1 +2935,1 @@\n-    Register tmp = LockingMode == LM_LIGHTWEIGHT ? op->scratch_opr()->as_register() : noreg;\n+    Register tmp = op->scratch_opr()->as_register();\n","filename":"src\/hotspot\/cpu\/x86\/c1_LIRAssembler_x86.cpp","additions":3,"deletions":13,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -299,5 +299,1 @@\n-  LIR_Opr scratch = LIR_OprFact::illegalOpr;\n-  if ((LockingMode == LM_LIGHTWEIGHT) ||\n-      (EnableValhalla && x->maybe_inlinetype())) {\n-    scratch = new_register(T_ADDRESS);\n-  }\n+  LIR_Opr scratch = new_register(T_ADDRESS);\n@@ -989,1 +985,1 @@\n-      LIR_Opr addr = new_pointer_register();\n+      LIR_Opr addr = new_register(T_ADDRESS);\n@@ -1128,1 +1124,1 @@\n-  LIR_Opr ptr_addr_a = new_pointer_register();\n+  LIR_Opr ptr_addr_a = new_register(T_ADDRESS);\n@@ -1131,1 +1127,1 @@\n-  LIR_Opr ptr_addr_b = new_pointer_register();\n+  LIR_Opr ptr_addr_b = new_register(T_ADDRESS);\n","filename":"src\/hotspot\/cpu\/x86\/c1_LIRGenerator_x86.cpp","additions":4,"deletions":8,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -46,2 +46,0 @@\n-  const int aligned_mask = BytesPerWord -1;\n-  const int hdr_offset = oopDesc::mark_offset_in_bytes();\n@@ -59,52 +57,1 @@\n-  if (LockingMode == LM_LIGHTWEIGHT) {\n-    lightweight_lock(disp_hdr, obj, hdr, tmp, slow_case);\n-  } else  if (LockingMode == LM_LEGACY) {\n-    Label done;\n-\n-    if (DiagnoseSyncOnValueBasedClasses != 0) {\n-      load_klass(hdr, obj, rscratch1);\n-      testb(Address(hdr, Klass::misc_flags_offset()), KlassFlags::_misc_is_value_based_class);\n-      jcc(Assembler::notZero, slow_case);\n-    }\n-\n-    \/\/ Load object header\n-    movptr(hdr, Address(obj, hdr_offset));\n-    \/\/ and mark it as unlocked\n-    orptr(hdr, markWord::unlocked_value);\n-    if (EnableValhalla) {\n-      \/\/ Mask inline_type bit such that we go to the slow path if object is an inline type\n-      andptr(hdr, ~((int) markWord::inline_type_bit_in_place));\n-    }\n-    \/\/ save unlocked object header into the displaced header location on the stack\n-    movptr(Address(disp_hdr, 0), hdr);\n-    \/\/ test if object header is still the same (i.e. unlocked), and if so, store the\n-    \/\/ displaced header address in the object header - if it is not the same, get the\n-    \/\/ object header instead\n-    MacroAssembler::lock(); \/\/ must be immediately before cmpxchg!\n-    cmpxchgptr(disp_hdr, Address(obj, hdr_offset));\n-    \/\/ if the object header was the same, we're done\n-    jcc(Assembler::equal, done);\n-    \/\/ if the object header was not the same, it is now in the hdr register\n-    \/\/ => test if it is a stack pointer into the same stack (recursive locking), i.e.:\n-    \/\/\n-    \/\/ 1) (hdr & aligned_mask) == 0\n-    \/\/ 2) rsp <= hdr\n-    \/\/ 3) hdr <= rsp + page_size\n-    \/\/\n-    \/\/ these 3 tests can be done by evaluating the following expression:\n-    \/\/\n-    \/\/ (hdr - rsp) & (aligned_mask - page_size)\n-    \/\/\n-    \/\/ assuming both the stack pointer and page_size have their least\n-    \/\/ significant 2 bits cleared and page_size is a power of 2\n-    subptr(hdr, rsp);\n-    andptr(hdr, aligned_mask - (int)os::vm_page_size());\n-    \/\/ for recursive locking, the result is zero => save it in the displaced header\n-    \/\/ location (null in the displaced hdr location indicates recursive locking)\n-    movptr(Address(disp_hdr, 0), hdr);\n-    \/\/ otherwise we don't care about the result and handle locking via runtime call\n-    jcc(Assembler::notZero, slow_case);\n-    \/\/ done\n-    bind(done);\n-    inc_held_monitor_count();\n-  }\n+  lightweight_lock(disp_hdr, obj, hdr, tmp, slow_case);\n@@ -116,2 +63,0 @@\n-  const int aligned_mask = BytesPerWord -1;\n-  const int hdr_offset = oopDesc::mark_offset_in_bytes();\n@@ -120,10 +65,0 @@\n-  Label done;\n-\n-  if (LockingMode != LM_LIGHTWEIGHT) {\n-    \/\/ load displaced header\n-    movptr(hdr, Address(disp_hdr, 0));\n-    \/\/ if the loaded hdr is null we had recursive locking\n-    testptr(hdr, hdr);\n-    \/\/ if we had recursive locking, we are done\n-    jcc(Assembler::zero, done);\n-  }\n@@ -135,15 +70,1 @@\n-  if (LockingMode == LM_LIGHTWEIGHT) {\n-    lightweight_unlock(obj, disp_hdr, hdr, slow_case);\n-  } else if (LockingMode == LM_LEGACY) {\n-    \/\/ test if object header is pointing to the displaced header, and if so, restore\n-    \/\/ the displaced header in the object - if the object header is not pointing to\n-    \/\/ the displaced header, get the object header instead\n-    MacroAssembler::lock(); \/\/ must be immediately before cmpxchg!\n-    cmpxchgptr(hdr, Address(obj, hdr_offset));\n-    \/\/ if the object header was not pointing to the displaced header,\n-    \/\/ we do unlocking via runtime call\n-    jcc(Assembler::notEqual, slow_case);\n-    \/\/ done\n-    bind(done);\n-    dec_held_monitor_count();\n-  }\n+  lightweight_unlock(obj, disp_hdr, hdr, slow_case);\n","filename":"src\/hotspot\/cpu\/x86\/c1_MacroAssembler_x86.cpp","additions":2,"deletions":81,"binary":false,"changes":83,"status":"modified"},{"patch":"@@ -248,239 +248,3 @@\n-\/\/ box: on-stack box address (displaced header location) - KILLED\n-\/\/ rax,: tmp -- KILLED\n-\/\/ scr: tmp -- KILLED\n-void C2_MacroAssembler::fast_lock(Register objReg, Register boxReg, Register tmpReg,\n-                                 Register scrReg, Register cx1Reg, Register cx2Reg, Register thread,\n-                                 Metadata* method_data) {\n-  assert(LockingMode != LM_LIGHTWEIGHT, \"lightweight locking should use fast_lock_lightweight\");\n-  \/\/ Ensure the register assignments are disjoint\n-  assert(tmpReg == rax, \"\");\n-  assert(cx1Reg == noreg, \"\");\n-  assert(cx2Reg == noreg, \"\");\n-  assert_different_registers(objReg, boxReg, tmpReg, scrReg);\n-\n-  \/\/ Possible cases that we'll encounter in fast_lock\n-  \/\/ ------------------------------------------------\n-  \/\/ * Inflated\n-  \/\/    -- unlocked\n-  \/\/    -- Locked\n-  \/\/       = by self\n-  \/\/       = by other\n-  \/\/ * neutral\n-  \/\/ * stack-locked\n-  \/\/    -- by self\n-  \/\/       = sp-proximity test hits\n-  \/\/       = sp-proximity test generates false-negative\n-  \/\/    -- by other\n-  \/\/\n-\n-  Label IsInflated, DONE_LABEL, NO_COUNT, COUNT;\n-\n-  if (DiagnoseSyncOnValueBasedClasses != 0) {\n-    load_klass(tmpReg, objReg, scrReg);\n-    testb(Address(tmpReg, Klass::misc_flags_offset()), KlassFlags::_misc_is_value_based_class);\n-    jcc(Assembler::notZero, DONE_LABEL);\n-  }\n-\n-  movptr(tmpReg, Address(objReg, oopDesc::mark_offset_in_bytes()));          \/\/ [FETCH]\n-  testptr(tmpReg, markWord::monitor_value); \/\/ inflated vs stack-locked|neutral\n-  jcc(Assembler::notZero, IsInflated);\n-\n-  if (LockingMode == LM_MONITOR) {\n-    \/\/ Clear ZF so that we take the slow path at the DONE label. objReg is known to be not 0.\n-    testptr(objReg, objReg);\n-  } else {\n-    assert(LockingMode == LM_LEGACY, \"must be\");\n-    \/\/ Attempt stack-locking ...\n-    orptr (tmpReg, markWord::unlocked_value);\n-    if (EnableValhalla) {\n-      \/\/ Mask inline_type bit such that we go to the slow path if object is an inline type\n-      andptr(tmpReg, ~((int) markWord::inline_type_bit_in_place));\n-    }\n-    movptr(Address(boxReg, 0), tmpReg);          \/\/ Anticipate successful CAS\n-    lock();\n-    cmpxchgptr(boxReg, Address(objReg, oopDesc::mark_offset_in_bytes()));      \/\/ Updates tmpReg\n-    jcc(Assembler::equal, COUNT);           \/\/ Success\n-\n-    \/\/ Recursive locking.\n-    \/\/ The object is stack-locked: markword contains stack pointer to BasicLock.\n-    \/\/ Locked by current thread if difference with current SP is less than one page.\n-    subptr(tmpReg, rsp);\n-    \/\/ Next instruction set ZFlag == 1 (Success) if difference is less then one page.\n-    andptr(tmpReg, (int32_t) (7 - (int)os::vm_page_size()) );\n-    movptr(Address(boxReg, 0), tmpReg);\n-  }\n-  jmp(DONE_LABEL);\n-\n-  bind(IsInflated);\n-  \/\/ The object is inflated. tmpReg contains pointer to ObjectMonitor* + markWord::monitor_value\n-\n-  \/\/ Unconditionally set box->_displaced_header = markWord::unused_mark().\n-  \/\/ Without cast to int32_t this style of movptr will destroy r10 which is typically obj.\n-  movptr(Address(boxReg, 0), checked_cast<int32_t>(markWord::unused_mark().value()));\n-\n-  \/\/ It's inflated and we use scrReg for ObjectMonitor* in this section.\n-  movptr(boxReg, Address(r15_thread, JavaThread::monitor_owner_id_offset()));\n-  movq(scrReg, tmpReg);\n-  xorq(tmpReg, tmpReg);\n-  lock();\n-  cmpxchgptr(boxReg, Address(scrReg, OM_OFFSET_NO_MONITOR_VALUE_TAG(owner)));\n-\n-  \/\/ Propagate ICC.ZF from CAS above into DONE_LABEL.\n-  jccb(Assembler::equal, COUNT);    \/\/ CAS above succeeded; propagate ZF = 1 (success)\n-\n-  cmpptr(boxReg, rax);                \/\/ Check if we are already the owner (recursive lock)\n-  jccb(Assembler::notEqual, NO_COUNT);    \/\/ If not recursive, ZF = 0 at this point (fail)\n-  incq(Address(scrReg, OM_OFFSET_NO_MONITOR_VALUE_TAG(recursions)));\n-  xorq(rax, rax); \/\/ Set ZF = 1 (success) for recursive lock, denoting locking success\n-  bind(DONE_LABEL);\n-\n-  \/\/ ZFlag == 1 count in fast path\n-  \/\/ ZFlag == 0 count in slow path\n-  jccb(Assembler::notZero, NO_COUNT); \/\/ jump if ZFlag == 0\n-\n-  bind(COUNT);\n-  if (LockingMode == LM_LEGACY) {\n-    \/\/ Count monitors in fast path\n-    increment(Address(thread, JavaThread::held_monitor_count_offset()));\n-  }\n-  xorl(tmpReg, tmpReg); \/\/ Set ZF == 1\n-\n-  bind(NO_COUNT);\n-\n-  \/\/ At NO_COUNT the icc ZFlag is set as follows ...\n-  \/\/ fast_unlock uses the same protocol.\n-  \/\/ ZFlag == 1 -> Success\n-  \/\/ ZFlag == 0 -> Failure - force control through the slow path\n-}\n-\n-\/\/ obj: object to unlock\n-\/\/ box: box address (displaced header location), killed.  Must be EAX.\n-\/\/ tmp: killed, cannot be obj nor box.\n-\/\/\n-\/\/ Some commentary on balanced locking:\n-\/\/\n-\/\/ fast_lock and fast_unlock are emitted only for provably balanced lock sites.\n-\/\/ Methods that don't have provably balanced locking are forced to run in the\n-\/\/ interpreter - such methods won't be compiled to use fast_lock and fast_unlock.\n-\/\/ The interpreter provides two properties:\n-\/\/ I1:  At return-time the interpreter automatically and quietly unlocks any\n-\/\/      objects acquired the current activation (frame).  Recall that the\n-\/\/      interpreter maintains an on-stack list of locks currently held by\n-\/\/      a frame.\n-\/\/ I2:  If a method attempts to unlock an object that is not held by the\n-\/\/      the frame the interpreter throws IMSX.\n-\/\/\n-\/\/ Lets say A(), which has provably balanced locking, acquires O and then calls B().\n-\/\/ B() doesn't have provably balanced locking so it runs in the interpreter.\n-\/\/ Control returns to A() and A() unlocks O.  By I1 and I2, above, we know that O\n-\/\/ is still locked by A().\n-\/\/\n-\/\/ The only other source of unbalanced locking would be JNI.  The \"Java Native Interface:\n-\/\/ Programmer's Guide and Specification\" claims that an object locked by jni_monitorenter\n-\/\/ should not be unlocked by \"normal\" java-level locking and vice-versa.  The specification\n-\/\/ doesn't specify what will occur if a program engages in such mixed-mode locking, however.\n-\/\/ Arguably given that the spec legislates the JNI case as undefined our implementation\n-\/\/ could reasonably *avoid* checking owner in fast_unlock().\n-\/\/ In the interest of performance we elide m->Owner==Self check in unlock.\n-\/\/ A perfectly viable alternative is to elide the owner check except when\n-\/\/ Xcheck:jni is enabled.\n-\n-void C2_MacroAssembler::fast_unlock(Register objReg, Register boxReg, Register tmpReg) {\n-  assert(LockingMode != LM_LIGHTWEIGHT, \"lightweight locking should use fast_unlock_lightweight\");\n-  assert(boxReg == rax, \"\");\n-  assert_different_registers(objReg, boxReg, tmpReg);\n-\n-  Label DONE_LABEL, Stacked, COUNT, NO_COUNT;\n-\n-  if (LockingMode == LM_LEGACY) {\n-    cmpptr(Address(boxReg, 0), NULL_WORD);                            \/\/ Examine the displaced header\n-    jcc   (Assembler::zero, COUNT);                                   \/\/ 0 indicates recursive stack-lock\n-  }\n-  movptr(tmpReg, Address(objReg, oopDesc::mark_offset_in_bytes()));   \/\/ Examine the object's markword\n-  if (LockingMode != LM_MONITOR) {\n-    testptr(tmpReg, markWord::monitor_value);                         \/\/ Inflated?\n-    jcc(Assembler::zero, Stacked);\n-  }\n-\n-  \/\/ It's inflated.\n-\n-  \/\/ Despite our balanced locking property we still check that m->_owner == Self\n-  \/\/ as java routines or native JNI code called by this thread might\n-  \/\/ have released the lock.\n-  \/\/\n-  \/\/ If there's no contention try a 1-0 exit.  That is, exit without\n-  \/\/ a costly MEMBAR or CAS.  See synchronizer.cpp for details on how\n-  \/\/ we detect and recover from the race that the 1-0 exit admits.\n-  \/\/\n-  \/\/ Conceptually fast_unlock() must execute a STST|LDST \"release\" barrier\n-  \/\/ before it STs null into _owner, releasing the lock.  Updates\n-  \/\/ to data protected by the critical section must be visible before\n-  \/\/ we drop the lock (and thus before any other thread could acquire\n-  \/\/ the lock and observe the fields protected by the lock).\n-  \/\/ IA32's memory-model is SPO, so STs are ordered with respect to\n-  \/\/ each other and there's no need for an explicit barrier (fence).\n-  \/\/ See also http:\/\/gee.cs.oswego.edu\/dl\/jmm\/cookbook.html.\n-  Label LSuccess, LNotRecursive;\n-\n-  cmpptr(Address(tmpReg, OM_OFFSET_NO_MONITOR_VALUE_TAG(recursions)), 0);\n-  jccb(Assembler::equal, LNotRecursive);\n-\n-  \/\/ Recursive inflated unlock\n-  decrement(Address(tmpReg, OM_OFFSET_NO_MONITOR_VALUE_TAG(recursions)));\n-  jmpb(LSuccess);\n-\n-  bind(LNotRecursive);\n-\n-  \/\/ Set owner to null.\n-  \/\/ Release to satisfy the JMM\n-  movptr(Address(tmpReg, OM_OFFSET_NO_MONITOR_VALUE_TAG(owner)), NULL_WORD);\n-  \/\/ We need a full fence after clearing owner to avoid stranding.\n-  \/\/ StoreLoad achieves this.\n-  membar(StoreLoad);\n-\n-  \/\/ Check if the entry_list is empty.\n-  cmpptr(Address(tmpReg, OM_OFFSET_NO_MONITOR_VALUE_TAG(entry_list)), NULL_WORD);\n-  jccb(Assembler::zero, LSuccess);    \/\/ If so we are done.\n-\n-  \/\/ Check if there is a successor.\n-  cmpptr(Address(tmpReg, OM_OFFSET_NO_MONITOR_VALUE_TAG(succ)), NULL_WORD);\n-  jccb(Assembler::notZero, LSuccess); \/\/ If so we are done.\n-\n-  \/\/ Save the monitor pointer in the current thread, so we can try to\n-  \/\/ reacquire the lock in SharedRuntime::monitor_exit_helper().\n-  andptr(tmpReg, ~(int32_t)markWord::monitor_value);\n-  movptr(Address(r15_thread, JavaThread::unlocked_inflated_monitor_offset()), tmpReg);\n-\n-  orl   (boxReg, 1);                      \/\/ set ICC.ZF=0 to indicate failure\n-  jmpb  (DONE_LABEL);\n-\n-  bind  (LSuccess);\n-  testl (boxReg, 0);                      \/\/ set ICC.ZF=1 to indicate success\n-  jmpb  (DONE_LABEL);\n-\n-  if (LockingMode == LM_LEGACY) {\n-    bind  (Stacked);\n-    movptr(tmpReg, Address (boxReg, 0));      \/\/ re-fetch\n-    lock();\n-    cmpxchgptr(tmpReg, Address(objReg, oopDesc::mark_offset_in_bytes())); \/\/ Uses RAX which is box\n-    \/\/ Intentional fall-thru into DONE_LABEL\n-  }\n-\n-  bind(DONE_LABEL);\n-\n-  \/\/ ZFlag == 1 count in fast path\n-  \/\/ ZFlag == 0 count in slow path\n-  jccb(Assembler::notZero, NO_COUNT);\n-\n-  bind(COUNT);\n-\n-  if (LockingMode == LM_LEGACY) {\n-    \/\/ Count monitors in fast path\n-    decrementq(Address(r15_thread, JavaThread::held_monitor_count_offset()));\n-  }\n-\n-  xorl(tmpReg, tmpReg); \/\/ Set ZF == 1\n-\n-  bind(NO_COUNT);\n-}\n-\n+\/\/ box: on-stack box address -- KILLED\n+\/\/ rax: tmp -- KILLED\n+\/\/ t  : tmp -- KILLED\n@@ -489,1 +253,0 @@\n-  assert(LockingMode == LM_LIGHTWEIGHT, \"must be\");\n@@ -649,0 +412,32 @@\n+\/\/ obj: object to lock\n+\/\/ rax: tmp -- KILLED\n+\/\/ t  : tmp - cannot be obj nor rax -- KILLED\n+\/\/\n+\/\/ Some commentary on balanced locking:\n+\/\/\n+\/\/ fast_lock and fast_unlock are emitted only for provably balanced lock sites.\n+\/\/ Methods that don't have provably balanced locking are forced to run in the\n+\/\/ interpreter - such methods won't be compiled to use fast_lock and fast_unlock.\n+\/\/ The interpreter provides two properties:\n+\/\/ I1:  At return-time the interpreter automatically and quietly unlocks any\n+\/\/      objects acquired in the current activation (frame).  Recall that the\n+\/\/      interpreter maintains an on-stack list of locks currently held by\n+\/\/      a frame.\n+\/\/ I2:  If a method attempts to unlock an object that is not held by the\n+\/\/      frame the interpreter throws IMSX.\n+\/\/\n+\/\/ Lets say A(), which has provably balanced locking, acquires O and then calls B().\n+\/\/ B() doesn't have provably balanced locking so it runs in the interpreter.\n+\/\/ Control returns to A() and A() unlocks O.  By I1 and I2, above, we know that O\n+\/\/ is still locked by A().\n+\/\/\n+\/\/ The only other source of unbalanced locking would be JNI.  The \"Java Native Interface\n+\/\/ Specification\" states that an object locked by JNI's MonitorEnter should not be\n+\/\/ unlocked by \"normal\" java-level locking and vice-versa.  The specification doesn't\n+\/\/ specify what will occur if a program engages in such mixed-mode locking, however.\n+\/\/ Arguably given that the spec legislates the JNI case as undefined our implementation\n+\/\/ could reasonably *avoid* checking owner in fast_unlock().\n+\/\/ In the interest of performance we elide m->Owner==Self check in unlock.\n+\/\/ A perfectly viable alternative is to elide the owner check except when\n+\/\/ Xcheck:jni is enabled.\n+\n@@ -650,1 +445,0 @@\n-  assert(LockingMode == LM_LIGHTWEIGHT, \"must be\");\n","filename":"src\/hotspot\/cpu\/x86\/c2_MacroAssembler_x86.cpp","additions":35,"deletions":241,"binary":false,"changes":276,"status":"modified"},{"patch":"@@ -38,6 +38,1 @@\n-  \/\/ See full description in macroAssembler_x86.cpp.\n-  void fast_lock(Register obj, Register box, Register tmp,\n-                 Register scr, Register cx1, Register cx2, Register thread,\n-                 Metadata* method_data);\n-  void fast_unlock(Register obj, Register box, Register tmp);\n-\n+  \/\/ See full description in c2_MacroAssembler_x86.cpp.\n","filename":"src\/hotspot\/cpu\/x86\/c2_MacroAssembler_x86.hpp","additions":1,"deletions":6,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -1171,28 +1171,1 @@\n-  if (LockingMode == LM_MONITOR) {\n-    call_VM_preemptable(noreg,\n-            CAST_FROM_FN_PTR(address, InterpreterRuntime::monitorenter),\n-            lock_reg);\n-  } else {\n-    Label count_locking, done, slow_case;\n-\n-    const Register swap_reg = rax; \/\/ Must use rax for cmpxchg instruction\n-    const Register tmp_reg = rbx;\n-    const Register obj_reg = c_rarg3; \/\/ Will contain the oop\n-    const Register rklass_decode_tmp = rscratch1;\n-\n-    const int obj_offset = in_bytes(BasicObjectLock::obj_offset());\n-    const int lock_offset = in_bytes(BasicObjectLock::lock_offset());\n-    const int mark_offset = lock_offset +\n-                            BasicLock::displaced_header_offset_in_bytes();\n-\n-    \/\/ Load object pointer into obj_reg\n-    movptr(obj_reg, Address(lock_reg, obj_offset));\n-\n-    if (LockingMode == LM_LIGHTWEIGHT) {\n-      lightweight_lock(lock_reg, obj_reg, swap_reg, tmp_reg, slow_case);\n-    } else if (LockingMode == LM_LEGACY) {\n-      if (DiagnoseSyncOnValueBasedClasses != 0) {\n-        load_klass(tmp_reg, obj_reg, rklass_decode_tmp);\n-        testb(Address(tmp_reg, Klass::misc_flags_offset()), KlassFlags::_misc_is_value_based_class);\n-        jcc(Assembler::notZero, slow_case);\n-      }\n+  Label done, slow_case;\n@@ -1200,2 +1173,3 @@\n-      \/\/ Load immediate 1 into swap_reg %rax\n-      movl(swap_reg, 1);\n+  const Register swap_reg = rax; \/\/ Must use rax for cmpxchg instruction\n+  const Register tmp_reg = rbx;\n+  const Register obj_reg = c_rarg3; \/\/ Will contain the oop\n@@ -1203,6 +1177,2 @@\n-      \/\/ Load (object->mark() | 1) into swap_reg %rax\n-      orptr(swap_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n-      if (EnableValhalla) {\n-        \/\/ Mask inline_type bit such that we go to the slow path if object is an inline type\n-        andptr(swap_reg, ~((int) markWord::inline_type_bit_in_place));\n-      }\n+  \/\/ Load object pointer into obj_reg\n+  movptr(obj_reg, Address(lock_reg, BasicObjectLock::obj_offset()));\n@@ -1210,50 +1180,2 @@\n-      \/\/ Save (object->mark() | 1) into BasicLock's displaced header\n-      movptr(Address(lock_reg, mark_offset), swap_reg);\n-\n-      assert(lock_offset == 0,\n-             \"displaced header must be first word in BasicObjectLock\");\n-\n-      lock();\n-      cmpxchgptr(lock_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n-      jcc(Assembler::zero, count_locking);\n-\n-      const int zero_bits = 7;\n-\n-      \/\/ Fast check for recursive lock.\n-      \/\/\n-      \/\/ Can apply the optimization only if this is a stack lock\n-      \/\/ allocated in this thread. For efficiency, we can focus on\n-      \/\/ recently allocated stack locks (instead of reading the stack\n-      \/\/ base and checking whether 'mark' points inside the current\n-      \/\/ thread stack):\n-      \/\/  1) (mark & zero_bits) == 0, and\n-      \/\/  2) rsp <= mark < mark + os::pagesize()\n-      \/\/\n-      \/\/ Warning: rsp + os::pagesize can overflow the stack base. We must\n-      \/\/ neither apply the optimization for an inflated lock allocated\n-      \/\/ just above the thread stack (this is why condition 1 matters)\n-      \/\/ nor apply the optimization if the stack lock is inside the stack\n-      \/\/ of another thread. The latter is avoided even in case of overflow\n-      \/\/ because we have guard pages at the end of all stacks. Hence, if\n-      \/\/ we go over the stack base and hit the stack of another thread,\n-      \/\/ this should not be in a writeable area that could contain a\n-      \/\/ stack lock allocated by that thread. As a consequence, a stack\n-      \/\/ lock less than page size away from rsp is guaranteed to be\n-      \/\/ owned by the current thread.\n-      \/\/\n-      \/\/ These 3 tests can be done by evaluating the following\n-      \/\/ expression: ((mark - rsp) & (zero_bits - os::vm_page_size())),\n-      \/\/ assuming both stack pointer and pagesize have their\n-      \/\/ least significant bits clear.\n-      \/\/ NOTE: the mark is in swap_reg %rax as the result of cmpxchg\n-      subptr(swap_reg, rsp);\n-      andptr(swap_reg, zero_bits - (int)os::vm_page_size());\n-\n-      \/\/ Save the test result, for recursive case, the result is zero\n-      movptr(Address(lock_reg, mark_offset), swap_reg);\n-      jcc(Assembler::notZero, slow_case);\n-\n-      bind(count_locking);\n-      inc_held_monitor_count();\n-    }\n-    jmp(done);\n+  lightweight_lock(lock_reg, obj_reg, swap_reg, tmp_reg, slow_case);\n+  jmp(done);\n@@ -1261,1 +1183,1 @@\n-    bind(slow_case);\n+  bind(slow_case);\n@@ -1263,6 +1185,5 @@\n-    \/\/ Call the runtime routine for slow case\n-    call_VM_preemptable(noreg,\n-            CAST_FROM_FN_PTR(address, InterpreterRuntime::monitorenter),\n-            lock_reg);\n-    bind(done);\n-  }\n+  \/\/ Call the runtime routine for slow case\n+  call_VM_preemptable(noreg,\n+          CAST_FROM_FN_PTR(address, InterpreterRuntime::monitorenter),\n+          lock_reg);\n+  bind(done);\n@@ -1287,10 +1208,1 @@\n-  if (LockingMode == LM_MONITOR) {\n-    call_VM_leaf(CAST_FROM_FN_PTR(address, InterpreterRuntime::monitorexit), lock_reg);\n-  } else {\n-    Label count_locking, done, slow_case;\n-\n-    const Register swap_reg   = rax;  \/\/ Must use rax for cmpxchg instruction\n-    const Register header_reg = c_rarg2;  \/\/ Will contain the old oopMark\n-    const Register obj_reg    = c_rarg3;  \/\/ Will contain the oop\n-\n-    save_bcp(); \/\/ Save in case of exception\n+  Label done, slow_case;\n@@ -1298,21 +1210,3 @@\n-    if (LockingMode != LM_LIGHTWEIGHT) {\n-      \/\/ Convert from BasicObjectLock structure to object and BasicLock\n-      \/\/ structure Store the BasicLock address into %rax\n-      lea(swap_reg, Address(lock_reg, BasicObjectLock::lock_offset()));\n-    }\n-\n-    \/\/ Load oop into obj_reg(%c_rarg3)\n-    movptr(obj_reg, Address(lock_reg, BasicObjectLock::obj_offset()));\n-\n-    \/\/ Free entry\n-    movptr(Address(lock_reg, BasicObjectLock::obj_offset()), NULL_WORD);\n-\n-    if (LockingMode == LM_LIGHTWEIGHT) {\n-      lightweight_unlock(obj_reg, swap_reg, header_reg, slow_case);\n-    } else if (LockingMode == LM_LEGACY) {\n-      \/\/ Load the old header from BasicLock structure\n-      movptr(header_reg, Address(swap_reg,\n-                                 BasicLock::displaced_header_offset_in_bytes()));\n-\n-      \/\/ Test for recursion\n-      testptr(header_reg, header_reg);\n+  const Register swap_reg   = rax;  \/\/ Must use rax for cmpxchg instruction\n+  const Register header_reg = c_rarg2;  \/\/ Will contain the old oopMark\n+  const Register obj_reg    = c_rarg3;  \/\/ Will contain the oop\n@@ -1320,2 +1214,1 @@\n-      \/\/ zero for recursive case\n-      jcc(Assembler::zero, count_locking);\n+  save_bcp(); \/\/ Save in case of exception\n@@ -1323,3 +1216,2 @@\n-      \/\/ Atomic swap back the old header\n-      lock();\n-      cmpxchgptr(header_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+  \/\/ Load oop into obj_reg(%c_rarg3)\n+  movptr(obj_reg, Address(lock_reg, BasicObjectLock::obj_offset()));\n@@ -1327,2 +1219,2 @@\n-      \/\/ zero for simple unlock of a stack-lock case\n-      jcc(Assembler::notZero, slow_case);\n+  \/\/ Free entry\n+  movptr(Address(lock_reg, BasicObjectLock::obj_offset()), NULL_WORD);\n@@ -1330,4 +1222,2 @@\n-      bind(count_locking);\n-      dec_held_monitor_count();\n-    }\n-    jmp(done);\n+  lightweight_unlock(obj_reg, swap_reg, header_reg, slow_case);\n+  jmp(done);\n@@ -1335,4 +1225,4 @@\n-    bind(slow_case);\n-    \/\/ Call the runtime routine for slow case.\n-    movptr(Address(lock_reg, BasicObjectLock::obj_offset()), obj_reg); \/\/ restore obj\n-    call_VM_leaf(CAST_FROM_FN_PTR(address, InterpreterRuntime::monitorexit), lock_reg);\n+  bind(slow_case);\n+  \/\/ Call the runtime routine for slow case.\n+  movptr(Address(lock_reg, BasicObjectLock::obj_offset()), obj_reg); \/\/ restore obj\n+  call_VM_leaf(CAST_FROM_FN_PTR(address, InterpreterRuntime::monitorexit), lock_reg);\n@@ -1340,1 +1230,1 @@\n-    bind(done);\n+  bind(done);\n@@ -1342,2 +1232,1 @@\n-    restore_bcp();\n-  }\n+  restore_bcp();\n","filename":"src\/hotspot\/cpu\/x86\/interp_masm_x86.cpp","additions":31,"deletions":142,"binary":false,"changes":173,"status":"modified"},{"patch":"@@ -140,1 +140,1 @@\n-        __ jccb(Assembler::equal, L_ok);\n+        __ jcc(Assembler::equal, L_ok);\n@@ -157,1 +157,1 @@\n-        __ jccb(Assembler::notZero, L_ok);\n+        __ jcc(Assembler::notZero, L_ok);\n","filename":"src\/hotspot\/cpu\/x86\/methodHandles_x86.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -1340,1 +1340,11 @@\n-    new_adapter = AdapterBlob::create(masm->code(), frame_complete, frame_size_in_words, oop_maps, caller_must_gc_arguments);\n+    int entry_offset[AdapterHandlerEntry::ENTRIES_COUNT];\n+    assert(AdapterHandlerEntry::ENTRIES_COUNT == 7, \"sanity\");\n+    entry_offset[0] = 0; \/\/ i2c_entry offset\n+    entry_offset[1] = c2i_entry - i2c_entry;\n+    entry_offset[2] = c2i_inline_entry - i2c_entry;\n+    entry_offset[3] = c2i_inline_ro_entry - i2c_entry;\n+    entry_offset[4] = c2i_unverified_entry - i2c_entry;\n+    entry_offset[5] = c2i_unverified_inline_entry - i2c_entry;\n+    entry_offset[6] = c2i_no_clinit_check_entry - i2c_entry;\n+\n+    new_adapter = AdapterBlob::create(masm->code(), entry_offset, frame_complete, frame_size_in_words, oop_maps, caller_must_gc_arguments);\n@@ -2407,1 +2417,1 @@\n-  if (LockingMode != LM_LEGACY && method->is_object_wait0()) {\n+  if (method->is_object_wait0()) {\n@@ -2448,1 +2458,0 @@\n-  const Register old_hdr  = r13;  \/\/ value of old header at unlock time\n@@ -2454,4 +2463,0 @@\n-    Label count_mon;\n-\n-    const int mark_word_offset = BasicLock::displaced_header_offset_in_bytes();\n-\n@@ -2468,45 +2473,1 @@\n-    if (LockingMode == LM_MONITOR) {\n-      __ jmp(slow_path_lock);\n-    } else if (LockingMode == LM_LEGACY) {\n-      \/\/ Load immediate 1 into swap_reg %rax\n-      __ movl(swap_reg, 1);\n-\n-      \/\/ Load (object->mark() | 1) into swap_reg %rax\n-      __ orptr(swap_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n-      if (EnableValhalla) {\n-        \/\/ Mask inline_type bit such that we go to the slow path if object is an inline type\n-        __ andptr(swap_reg, ~((int) markWord::inline_type_bit_in_place));\n-      }\n-\n-      \/\/ Save (object->mark() | 1) into BasicLock's displaced header\n-      __ movptr(Address(lock_reg, mark_word_offset), swap_reg);\n-\n-      \/\/ src -> dest iff dest == rax else rax <- dest\n-      __ lock();\n-      __ cmpxchgptr(lock_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n-      __ jcc(Assembler::equal, count_mon);\n-\n-      \/\/ Hmm should this move to the slow path code area???\n-\n-      \/\/ Test if the oopMark is an obvious stack pointer, i.e.,\n-      \/\/  1) (mark & 3) == 0, and\n-      \/\/  2) rsp <= mark < mark + os::pagesize()\n-      \/\/ These 3 tests can be done by evaluating the following\n-      \/\/ expression: ((mark - rsp) & (3 - os::vm_page_size())),\n-      \/\/ assuming both stack pointer and pagesize have their\n-      \/\/ least significant 2 bits clear.\n-      \/\/ NOTE: the oopMark is in swap_reg %rax as the result of cmpxchg\n-\n-      __ subptr(swap_reg, rsp);\n-      __ andptr(swap_reg, 3 - (int)os::vm_page_size());\n-\n-      \/\/ Save the test result, for recursive case, the result is zero\n-      __ movptr(Address(lock_reg, mark_word_offset), swap_reg);\n-      __ jcc(Assembler::notEqual, slow_path_lock);\n-\n-      __ bind(count_mon);\n-      __ inc_held_monitor_count();\n-    } else {\n-      assert(LockingMode == LM_LIGHTWEIGHT, \"must be\");\n-      __ lightweight_lock(lock_reg, obj_reg, swap_reg, rscratch1, slow_path_lock);\n-    }\n+    __ lightweight_lock(lock_reg, obj_reg, swap_reg, rscratch1, slow_path_lock);\n@@ -2600,1 +2561,1 @@\n-  if (LockingMode != LM_LEGACY && method->is_object_wait0()) {\n+  if (method->is_object_wait0()) {\n@@ -2632,10 +2593,0 @@\n-    if (LockingMode == LM_LEGACY) {\n-      Label not_recur;\n-      \/\/ Simple recursive lock?\n-      __ cmpptr(Address(rsp, lock_slot_offset * VMRegImpl::stack_slot_size), NULL_WORD);\n-      __ jcc(Assembler::notEqual, not_recur);\n-      __ dec_held_monitor_count();\n-      __ jmpb(fast_done);\n-      __ bind(not_recur);\n-    }\n-\n@@ -2647,17 +2598,1 @@\n-    if (LockingMode == LM_MONITOR) {\n-      __ jmp(slow_path_unlock);\n-    } else if (LockingMode == LM_LEGACY) {\n-      \/\/ get address of the stack lock\n-      __ lea(rax, Address(rsp, lock_slot_offset * VMRegImpl::stack_slot_size));\n-      \/\/  get old displaced header\n-      __ movptr(old_hdr, Address(rax, 0));\n-\n-      \/\/ Atomic swap old header if oop still contains the stack lock\n-      __ lock();\n-      __ cmpxchgptr(old_hdr, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n-      __ jcc(Assembler::notEqual, slow_path_unlock);\n-      __ dec_held_monitor_count();\n-    } else {\n-      assert(LockingMode == LM_LIGHTWEIGHT, \"must be\");\n-      __ lightweight_unlock(obj_reg, swap_reg, lock_reg, slow_path_unlock);\n-    }\n+    __ lightweight_unlock(obj_reg, swap_reg, lock_reg, slow_path_unlock);\n","filename":"src\/hotspot\/cpu\/x86\/sharedRuntime_x86_64.cpp","additions":15,"deletions":80,"binary":false,"changes":95,"status":"modified"},{"patch":"@@ -1025,15 +1025,10 @@\n-  if (LockingMode != LM_LEGACY) {\n-    \/\/ Check preemption for Object.wait()\n-    Label not_preempted;\n-    __ movptr(rscratch1, Address(r15_thread, JavaThread::preempt_alternate_return_offset()));\n-    __ cmpptr(rscratch1, NULL_WORD);\n-    __ jccb(Assembler::equal, not_preempted);\n-    __ movptr(Address(r15_thread, JavaThread::preempt_alternate_return_offset()), NULL_WORD);\n-    __ jmp(rscratch1);\n-    __ bind(native_return);\n-    __ restore_after_resume(true \/* is_native *\/);\n-    __ bind(not_preempted);\n-  } else {\n-    \/\/ any pc will do so just use this one for LM_LEGACY to keep code together.\n-    __ bind(native_return);\n-  }\n+  \/\/ Check preemption for Object.wait()\n+  Label not_preempted;\n+  __ movptr(rscratch1, Address(r15_thread, JavaThread::preempt_alternate_return_offset()));\n+  __ cmpptr(rscratch1, NULL_WORD);\n+  __ jccb(Assembler::equal, not_preempted);\n+  __ movptr(Address(r15_thread, JavaThread::preempt_alternate_return_offset()), NULL_WORD);\n+  __ jmp(rscratch1);\n+  __ bind(native_return);\n+  __ restore_after_resume(true \/* is_native *\/);\n+  __ bind(not_preempted);\n","filename":"src\/hotspot\/cpu\/x86\/templateInterpreterGenerator_x86.cpp","additions":10,"deletions":15,"binary":false,"changes":25,"status":"modified"},{"patch":"@@ -41,0 +41,1 @@\n+#include \"utilities\/ostream.hpp\"\n@@ -1100,7 +1101,7 @@\n-  char buf[2048];\n-  size_t cpu_info_size = jio_snprintf(\n-              buf, sizeof(buf),\n-              \"(%u cores per cpu, %u threads per core) family %d model %d stepping %d microcode 0x%x\",\n-              cores_per_cpu(), threads_per_core(),\n-              cpu_family(), _model, _stepping, os::cpu_microcode_revision());\n-  assert(cpu_info_size > 0, \"not enough temporary space allocated\");\n+  stringStream ss(2048);\n+  ss.print(\"(%u cores per cpu, %u threads per core) family %d model %d stepping %d microcode 0x%x\",\n+           cores_per_cpu(), threads_per_core(),\n+           cpu_family(), _model, _stepping, os::cpu_microcode_revision());\n+  ss.print(\", \");\n+  int features_offset = (int)ss.size();\n+  insert_features_names(_features, ss);\n@@ -1108,7 +1109,2 @@\n-  insert_features_names(_features, buf + cpu_info_size, sizeof(buf) - cpu_info_size);\n-\n-  _cpu_info_string = os::strdup(buf);\n-\n-  _features_string = extract_features_string(_cpu_info_string,\n-                                             strnlen(_cpu_info_string, sizeof(buf)),\n-                                             cpu_info_size);\n+  _cpu_info_string = ss.as_string(true);\n+  _features_string = _cpu_info_string + features_offset;\n@@ -3269,7 +3265,8 @@\n-void VM_Version::insert_features_names(VM_Version::VM_Features features, char* buf, size_t buflen) {\n-  for (int i = 0; i < MAX_CPU_FEATURES; i++) {\n-    if (features.supports_feature((VM_Version::Feature_Flag)i)) {\n-      int res = jio_snprintf(buf, buflen, \", %s\", _features_names[i]);\n-      assert(res > 0, \"not enough temporary space allocated\");\n-      buf += res;\n-      buflen -= res;\n+void VM_Version::insert_features_names(VM_Version::VM_Features features, stringStream& ss) {\n+  int i = 0;\n+  ss.join([&]() {\n+    while (i < MAX_CPU_FEATURES) {\n+      if (_features.supports_feature((VM_Version::Feature_Flag)i)) {\n+        return _features_names[i++];\n+      }\n+      i += 1;\n@@ -3277,1 +3274,2 @@\n-  }\n+    return (const char*)nullptr;\n+  }, \", \");\n","filename":"src\/hotspot\/cpu\/x86\/vm_version_x86.cpp","additions":20,"deletions":22,"binary":false,"changes":42,"status":"modified"},{"patch":"@@ -14340,26 +14340,0 @@\n-instruct cmpFastLock(rFlagsReg cr, rRegP object, rbx_RegP box, rax_RegI tmp, rRegP scr) %{\n-  predicate(LockingMode != LM_LIGHTWEIGHT);\n-  match(Set cr (FastLock object box));\n-  effect(TEMP tmp, TEMP scr, USE_KILL box);\n-  ins_cost(300);\n-  format %{ \"fastlock $object,$box\\t! kills $box,$tmp,$scr\" %}\n-  ins_encode %{\n-    __ fast_lock($object$$Register, $box$$Register, $tmp$$Register,\n-                 $scr$$Register, noreg, noreg, r15_thread, nullptr);\n-  %}\n-  ins_pipe(pipe_slow);\n-%}\n-\n-instruct cmpFastUnlock(rFlagsReg cr, rRegP object, rax_RegP box, rRegP tmp) %{\n-  predicate(LockingMode != LM_LIGHTWEIGHT);\n-  match(Set cr (FastUnlock object box));\n-  effect(TEMP tmp, USE_KILL box);\n-  ins_cost(300);\n-  format %{ \"fastunlock $object,$box\\t! kills $box,$tmp\" %}\n-  ins_encode %{\n-    __ fast_unlock($object$$Register, $box$$Register, $tmp$$Register);\n-  %}\n-  ins_pipe(pipe_slow);\n-%}\n-\n-  predicate(LockingMode == LM_LIGHTWEIGHT);\n@@ -14378,1 +14352,0 @@\n-  predicate(LockingMode == LM_LIGHTWEIGHT);\n","filename":"src\/hotspot\/cpu\/x86\/x86_64.ad","additions":0,"deletions":27,"binary":false,"changes":27,"status":"modified"},{"patch":"@@ -76,1 +76,11 @@\n-    new_adapter = AdapterBlob::create(masm->code(), 0, 0, nullptr);\n+    int entry_offset[AdapterHandlerEntry::ENTRIES_COUNT];\n+    assert(AdapterHandlerEntry::ENTRIES_COUNT == 7, \"sanity\");\n+    entry_offset[0] = 0; \/\/ i2c_entry offset\n+    entry_offset[1] = -1;\n+    entry_offset[2] = -1;\n+    entry_offset[3] = -1;\n+    entry_offset[4] = -1;\n+    entry_offset[5] = -1;\n+    entry_offset[6] = -1;\n+\n+    new_adapter = AdapterBlob::create(masm->code(), entry_offset, 0, 0, nullptr);\n@@ -78,0 +88,1 @@\n+  \/\/ VM expects i2c entry to be always filled. The rest can be unset.\n@@ -86,1 +97,0 @@\n-  return;\n","filename":"src\/hotspot\/cpu\/zero\/sharedRuntime_zero.cpp","additions":12,"deletions":2,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -36,1 +36,1 @@\n-#include \"utilities\/resizeableResourceHash.hpp\"\n+#include \"utilities\/resizableHashTable.hpp\"\n@@ -551,1 +551,1 @@\n-  typedef ResizeableResourceHashtable<address, Offsets, AnyObj::C_HEAP, mtCompiler> SharedTrampolineRequests;\n+  typedef ResizeableHashTable<address, Offsets, AnyObj::C_HEAP, mtCompiler> SharedTrampolineRequests;\n","filename":"src\/hotspot\/share\/asm\/codeBuffer.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -73,1 +73,1 @@\n-typedef ResourceHashtable<\n+typedef HashTable<\n","filename":"src\/hotspot\/share\/cds\/archiveHeapWriter.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -370,1 +370,1 @@\n-class MetaspaceObjToOopHandleTable: public ResourceHashtable<MetaspaceObj*, OopHandle,\n+class MetaspaceObjToOopHandleTable: public HashTable<MetaspaceObj*, OopHandle,\n","filename":"src\/hotspot\/share\/cds\/heapShared.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -102,0 +102,1 @@\n+#include \"utilities\/hashTable.hpp\"\n@@ -104,1 +105,0 @@\n-#include \"utilities\/resourceHash.hpp\"\n@@ -183,1 +183,1 @@\n-  ResizeableResourceHashtable<InstanceKlass*, bool,\n+  ResizeableHashTable<InstanceKlass*, bool,\n","filename":"src\/hotspot\/share\/cds\/metaspaceShared.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -86,0 +86,1 @@\n+#include \"utilities\/hashTable.hpp\"\n@@ -88,1 +89,0 @@\n-#include \"utilities\/resourceHash.hpp\"\n@@ -802,1 +802,1 @@\n-using NameSigHashtable = ResourceHashtable<NameSigHash, int,\n+using NameSigHashtable = HashTable<NameSigHash, int,\n@@ -855,1 +855,1 @@\n-    ResourceHashtable<Symbol*, int>* interface_names = new ResourceHashtable<Symbol*, int>();\n+    HashTable<Symbol*, int>* interface_names = new HashTable<Symbol*, int>();\n@@ -2090,1 +2090,1 @@\n-  typedef ResourceHashtable<LocalVariableTableElement, LocalVariableTableElement*,\n+  typedef HashTable<LocalVariableTableElement, LocalVariableTableElement*,\n","filename":"src\/hotspot\/share\/classfile\/classFileParser.cpp","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -1930,14 +1930,10 @@\n-oop java_lang_Thread::async_get_stack_trace(oop java_thread, TRAPS) {\n-  ThreadsListHandle tlh(JavaThread::current());\n-  JavaThread* thread;\n-  bool is_virtual = java_lang_VirtualThread::is_instance(java_thread);\n-  if (is_virtual) {\n-    oop carrier_thread = java_lang_VirtualThread::carrier_thread(java_thread);\n-    if (carrier_thread == nullptr) {\n-      return nullptr;\n-    }\n-    thread = java_lang_Thread::thread(carrier_thread);\n-  } else {\n-    thread = java_lang_Thread::thread(java_thread);\n-  }\n-  if (thread == nullptr) {\n+\/\/ Obtain stack trace for platform or mounted virtual thread.\n+\/\/ If jthread is a virtual thread and it has been unmounted (or remounted to different carrier) the method returns null.\n+\/\/ The caller (java.lang.VirtualThread) handles returned nulls via retry.\n+oop java_lang_Thread::async_get_stack_trace(jobject jthread, TRAPS) {\n+  ThreadsListHandle tlh(THREAD);\n+  JavaThread* java_thread = nullptr;\n+  oop thread_oop;\n+\n+  bool has_java_thread = tlh.cv_internal_thread_to_JavaThread(jthread, &java_thread, &thread_oop);\n+  if (!has_java_thread) {\n@@ -1949,1 +1945,1 @@\n-    const Handle _java_thread;\n+    const Handle _thread_h;\n@@ -1955,2 +1951,2 @@\n-    GetStackTraceHandshakeClosure(Handle java_thread) :\n-        HandshakeClosure(\"GetStackTraceHandshakeClosure\"), _java_thread(java_thread), _depth(0), _retry_handshake(false),\n+    GetStackTraceHandshakeClosure(Handle thread_h) :\n+        HandshakeClosure(\"GetStackTraceHandshakeClosure\"), _thread_h(thread_h), _depth(0), _retry_handshake(false),\n@@ -1978,1 +1974,1 @@\n-      JavaThread* thread = JavaThread::cast(th);\n+      JavaThread* java_thread = JavaThread::cast(th);\n@@ -1980,1 +1976,1 @@\n-      if (!thread->has_last_Java_frame()) {\n+      if (!java_thread->has_last_Java_frame()) {\n@@ -1985,5 +1981,6 @@\n-      if (java_lang_VirtualThread::is_instance(_java_thread())) {\n-        \/\/ if (thread->vthread() != _java_thread()) \/\/ We might be inside a System.executeOnCarrierThread\n-        const ContinuationEntry* ce = thread->vthread_continuation();\n-        if (ce == nullptr || ce->cont_oop(thread) != java_lang_VirtualThread::continuation(_java_thread())) {\n-          return; \/\/ not mounted\n+      if (java_lang_VirtualThread::is_instance(_thread_h())) {\n+        \/\/ Ensure _thread_h is still mounted to java_thread.\n+        const ContinuationEntry* ce = java_thread->vthread_continuation();\n+        if (ce == nullptr || ce->cont_oop(java_thread) != java_lang_VirtualThread::continuation(_thread_h())) {\n+          \/\/ Target thread has been unmounted.\n+          return;\n@@ -1992,1 +1989,1 @@\n-        carrier = (thread->vthread_continuation() != nullptr);\n+        carrier = (java_thread->vthread_continuation() != nullptr);\n@@ -2004,1 +2001,1 @@\n-      for (vframeStream vfst(thread, false, false, carrier); \/\/ we don't process frames as we don't care about oops\n+      for (vframeStream vfst(java_thread, false, false, carrier); \/\/ we don't process frames as we don't care about oops\n@@ -2025,1 +2022,1 @@\n-  GetStackTraceHandshakeClosure gsthc(Handle(THREAD, java_thread));\n+  GetStackTraceHandshakeClosure gsthc(Handle(THREAD, thread_oop));\n@@ -2027,1 +2024,1 @@\n-   Handshake::execute(&gsthc, &tlh, thread);\n+   Handshake::execute(&gsthc, &tlh, java_thread);\n","filename":"src\/hotspot\/share\/classfile\/javaClasses.cpp","additions":25,"deletions":28,"binary":false,"changes":53,"status":"modified"},{"patch":"@@ -469,1 +469,1 @@\n-  static oop async_get_stack_trace(oop java_thread, TRAPS);\n+  static oop async_get_stack_trace(jobject jthread, TRAPS);\n","filename":"src\/hotspot\/share\/classfile\/javaClasses.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -34,1 +34,1 @@\n-#include \"utilities\/resourceHash.hpp\"\n+#include \"utilities\/hashTable.hpp\"\n@@ -52,1 +52,1 @@\n-using InternalPlaceholderTable = ResourceHashtable<PlaceholderKey, PlaceholderEntry, _placeholder_table_size, AnyObj::C_HEAP, mtClass,\n+using InternalPlaceholderTable = HashTable<PlaceholderKey, PlaceholderEntry, _placeholder_table_size, AnyObj::C_HEAP, mtClass,\n","filename":"src\/hotspot\/share\/classfile\/placeholders.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -56,1 +56,1 @@\n-  typedef ResourceHashtable<NameAndSig, bool, 17,\n+  typedef HashTable<NameAndSig, bool, 17,\n","filename":"src\/hotspot\/share\/classfile\/stackMapFrame.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -62,1 +62,1 @@\n-#include \"utilities\/resizeableResourceHash.hpp\"\n+#include \"utilities\/resizableHashTable.hpp\"\n@@ -813,1 +813,1 @@\n-  ResizeableResourceHashtable<oop, bool, AnyObj::C_HEAP, mtInternal,\n+  ResizeableHashTable<oop, bool, AnyObj::C_HEAP, mtInternal,\n","filename":"src\/hotspot\/share\/classfile\/stringTable.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -117,1 +117,1 @@\n-using InvokeMethodIntrinsicTable = ResourceHashtable<InvokeMethodKey, Method*, 139, AnyObj::C_HEAP, mtClass,\n+using InvokeMethodIntrinsicTable = HashTable<InvokeMethodKey, Method*, 139, AnyObj::C_HEAP, mtClass,\n@@ -120,1 +120,1 @@\n-using InvokeMethodTypeTable = ResourceHashtable<SymbolHandle, OopHandle, 139, AnyObj::C_HEAP, mtClass, SymbolHandle::compute_hash>;\n+using InvokeMethodTypeTable = HashTable<SymbolHandle, OopHandle, 139, AnyObj::C_HEAP, mtClass, SymbolHandle::compute_hash>;\n","filename":"src\/hotspot\/share\/classfile\/systemDictionary.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -34,1 +34,1 @@\n-#include \"utilities\/resourceHash.hpp\"\n+#include \"utilities\/hashTable.hpp\"\n@@ -296,1 +296,1 @@\n-typedef ResourceHashtable<int, sig_as_verification_types*, 1007>\n+typedef HashTable<int, sig_as_verification_types*, 1007>\n","filename":"src\/hotspot\/share\/classfile\/verifier.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -802,1 +802,1 @@\n-bool AOTCodeCache::store_code_blob(CodeBlob& blob, AOTCodeEntry::Kind entry_kind, uint id, const char* name, int entry_offset_count, int* entry_offsets) {\n+bool AOTCodeCache::store_code_blob(CodeBlob& blob, AOTCodeEntry::Kind entry_kind, uint id, const char* name) {\n@@ -886,12 +886,0 @@\n-  \/\/ Write entries offsets\n-  n = cache->write_bytes(&entry_offset_count, sizeof(int));\n-  if (n != sizeof(int)) {\n-    return false;\n-  }\n-  for (int i = 0; i < entry_offset_count; i++) {\n-    uint32_t off = (uint32_t)entry_offsets[i];\n-    n = cache->write_bytes(&off, sizeof(uint32_t));\n-    if (n != sizeof(uint32_t)) {\n-      return false;\n-    }\n-  }\n@@ -906,1 +894,1 @@\n-bool AOTCodeCache::store_code_blob(CodeBlob& blob, AOTCodeEntry::Kind entry_kind, BlobId id, int entry_offset_count, int* entry_offsets) {\n+bool AOTCodeCache::store_code_blob(CodeBlob& blob, AOTCodeEntry::Kind entry_kind, BlobId id) {\n@@ -909,1 +897,1 @@\n-  return store_code_blob(blob, entry_kind, (uint)id, StubInfo::name(id), entry_offset_count, entry_offsets);\n+  return store_code_blob(blob, entry_kind, (uint)id, StubInfo::name(id));\n@@ -912,1 +900,1 @@\n-CodeBlob* AOTCodeCache::load_code_blob(AOTCodeEntry::Kind entry_kind, uint id, const char* name, int entry_offset_count, int* entry_offsets) {\n+CodeBlob* AOTCodeCache::load_code_blob(AOTCodeEntry::Kind entry_kind, uint id, const char* name) {\n@@ -932,1 +920,1 @@\n-  CodeBlob* blob = reader.compile_code_blob(name, entry_offset_count, entry_offsets);\n+  CodeBlob* blob = reader.compile_code_blob(name);\n@@ -939,1 +927,1 @@\n-CodeBlob* AOTCodeCache::load_code_blob(AOTCodeEntry::Kind entry_kind, BlobId id, int entry_offset_count, int* entry_offsets) {\n+CodeBlob* AOTCodeCache::load_code_blob(AOTCodeEntry::Kind entry_kind, BlobId id) {\n@@ -942,1 +930,1 @@\n-  return load_code_blob(entry_kind, (uint)id, StubInfo::name(id), entry_offset_count, entry_offsets);\n+  return load_code_blob(entry_kind, (uint)id, StubInfo::name(id));\n@@ -945,1 +933,1 @@\n-CodeBlob* AOTCodeReader::compile_code_blob(const char* name, int entry_offset_count, int* entry_offsets) {\n+CodeBlob* AOTCodeReader::compile_code_blob(const char* name) {\n@@ -992,15 +980,0 @@\n-  \/\/ Read entries offsets\n-  offset = read_position();\n-  int stored_count = *(int*)addr(offset);\n-  assert(stored_count == entry_offset_count, \"entry offset count mismatch, count in AOT code cache=%d, expected=%d\", stored_count, entry_offset_count);\n-  offset += sizeof(int);\n-  set_read_position(offset);\n-  for (int i = 0; i < stored_count; i++) {\n-    uint32_t off = *(uint32_t*)addr(offset);\n-    offset += sizeof(uint32_t);\n-    const char* entry_name = (_entry->kind() == AOTCodeEntry::Adapter) ? AdapterHandlerEntry::entry_name(i) : \"\";\n-    log_trace(aot, codecache, stubs)(\"Reading adapter '%s:%s' (0x%x) offset: 0x%x from AOT Code Cache\",\n-                                      stored_name, entry_name, _entry->id(), off);\n-    entry_offsets[i] = off;\n-  }\n-\n","filename":"src\/hotspot\/share\/code\/aotCodeCache.cpp","additions":8,"deletions":35,"binary":false,"changes":43,"status":"modified"},{"patch":"@@ -394,2 +394,2 @@\n-BufferBlob::BufferBlob(const char* name, CodeBlobKind kind, int size)\n-: RuntimeBlob(name, kind, size, sizeof(BufferBlob))\n+BufferBlob::BufferBlob(const char* name, CodeBlobKind kind, int size, uint16_t header_size)\n+: RuntimeBlob(name, kind, size, header_size)\n@@ -418,1 +418,1 @@\n-BufferBlob::BufferBlob(const char* name, CodeBlobKind kind, CodeBuffer* cb, int size, int header_size)\n+BufferBlob::BufferBlob(const char* name, CodeBlobKind kind, CodeBuffer* cb, int size, uint16_t header_size)\n@@ -447,2 +447,2 @@\n-BufferBlob::BufferBlob(const char* name, CodeBlobKind kind, CodeBuffer* cb, int size, int frame_complete, int frame_size, OopMapSet* oop_maps, bool caller_must_gc_arguments)\n-  : RuntimeBlob(name, kind, cb, size, sizeof(BufferBlob), frame_complete, frame_size, oop_maps, caller_must_gc_arguments)\n+BufferBlob::BufferBlob(const char* name, CodeBlobKind kind, CodeBuffer* cb, int size, uint16_t header_size, int frame_complete, int frame_size, OopMapSet* oop_maps, bool caller_must_gc_arguments)\n+  : RuntimeBlob(name, kind, cb, size, header_size, frame_complete, frame_size, oop_maps, caller_must_gc_arguments)\n@@ -455,2 +455,15 @@\n-AdapterBlob::AdapterBlob(int size, CodeBuffer* cb, int frame_complete, int frame_size, OopMapSet* oop_maps, bool caller_must_gc_arguments) :\n-  BufferBlob(\"I2C\/C2I adapters\", CodeBlobKind::Adapter, cb, size, frame_complete, frame_size, oop_maps, caller_must_gc_arguments) {\n+AdapterBlob::AdapterBlob(int size, CodeBuffer* cb, int entry_offset[AdapterBlob::ENTRY_COUNT], int frame_complete, int frame_size, OopMapSet* oop_maps, bool caller_must_gc_arguments) :\n+  BufferBlob(\"I2C\/C2I adapters\", CodeBlobKind::Adapter, cb, size, sizeof(AdapterBlob), frame_complete, frame_size, oop_maps, caller_must_gc_arguments) {\n+  assert(entry_offset[0] == 0, \"sanity check\");\n+  for (int i = 1; i < AdapterBlob::ENTRY_COUNT; i++) {\n+    \/\/ The entry is within the adapter blob or unset.\n+    assert((entry_offset[i] > 0 && entry_offset[i] < cb->insts()->size()) ||\n+           (entry_offset[i] == -1),\n+           \"invalid entry offset[%d] = 0x%x\", i, entry_offset[i]);\n+  }\n+  _c2i_offset = entry_offset[1];\n+  _c2i_inline_offset = entry_offset[2];\n+  _c2i_inline_ro_offset = entry_offset[3];\n+  _c2i_unverified_offset = entry_offset[4];\n+  _c2i_unverified_inline_offset = entry_offset[5];\n+  _c2i_no_clinit_check_offset = entry_offset[6];\n@@ -460,1 +473,1 @@\n-AdapterBlob* AdapterBlob::create(CodeBuffer* cb, int frame_complete, int frame_size, OopMapSet* oop_maps, bool caller_must_gc_arguments) {\n+AdapterBlob* AdapterBlob::create(CodeBuffer* cb, int entry_offset[AdapterBlob::ENTRY_COUNT], int frame_complete, int frame_size, OopMapSet* oop_maps, bool caller_must_gc_arguments) {\n@@ -469,1 +482,1 @@\n-    blob = new (size) AdapterBlob(size, cb, frame_complete, frame_size, oop_maps, caller_must_gc_arguments);\n+    blob = new (size) AdapterBlob(size, cb, entry_offset, frame_complete, frame_size, oop_maps, caller_must_gc_arguments);\n@@ -477,0 +490,10 @@\n+void AdapterBlob::get_offsets(int entry_offset[ENTRY_COUNT]) {\n+  entry_offset[0] = 0;\n+  entry_offset[1] = _c2i_offset;\n+  entry_offset[2] = _c2i_inline_offset;\n+  entry_offset[3] = _c2i_inline_ro_offset;\n+  entry_offset[4] = _c2i_unverified_offset;\n+  entry_offset[5] = _c2i_unverified_inline_offset;\n+  entry_offset[6] = _c2i_no_clinit_check_offset;\n+}\n+\n","filename":"src\/hotspot\/share\/code\/codeBlob.cpp","additions":32,"deletions":9,"binary":false,"changes":41,"status":"modified"},{"patch":"@@ -379,3 +379,3 @@\n-  BufferBlob(const char* name, CodeBlobKind kind, int size);\n-  BufferBlob(const char* name, CodeBlobKind kind, CodeBuffer* cb, int size, int header_size);\n-  BufferBlob(const char* name, CodeBlobKind kind, CodeBuffer* cb, int size, int frame_complete, int frame_size, OopMapSet* oop_maps, bool caller_must_gc_arguments = false);\n+  BufferBlob(const char* name, CodeBlobKind kind, int size, uint16_t header_size = sizeof(BufferBlob));\n+  BufferBlob(const char* name, CodeBlobKind kind, CodeBuffer* cb, int size, uint16_t header_size = sizeof(BufferBlob));\n+  BufferBlob(const char* name, CodeBlobKind kind, CodeBuffer* cb, int size, uint16_t header_size, int frame_complete, int frame_size, OopMapSet* oop_maps, bool caller_must_gc_arguments = false);\n@@ -412,0 +412,2 @@\n+public:\n+  static const int ENTRY_COUNT = 7;\n@@ -413,2 +415,9 @@\n-  AdapterBlob(int size, CodeBuffer* cb, int frame_complete, int frame_size, OopMapSet* oop_maps, bool caller_must_gc_arguments = false);\n-\n+  AdapterBlob(int size, CodeBuffer* cb, int entry_offset[ENTRY_COUNT], int frame_complete, int frame_size, OopMapSet* oop_maps, bool caller_must_gc_arguments = false);\n+\n+  \/\/ _i2c_offset is always 0 so no need to store it\n+  int _c2i_offset;\n+  int _c2i_inline_offset;\n+  int _c2i_inline_ro_offset;\n+  int _c2i_unverified_offset;\n+  int _c2i_unverified_inline_offset;\n+  int _c2i_no_clinit_check_offset;\n@@ -418,0 +427,1 @@\n+                             int entry_offset[ENTRY_COUNT],\n@@ -424,0 +434,2 @@\n+  static AdapterBlob* create(CodeBuffer* cb, int entry_offset[ENTRY_COUNT]);\n+  void get_offsets(int entry_offset[ENTRY_COUNT]);\n","filename":"src\/hotspot\/share\/code\/codeBlob.hpp","additions":17,"deletions":5,"binary":false,"changes":22,"status":"modified"},{"patch":"@@ -82,1 +82,1 @@\n-#include \"utilities\/resourceHash.hpp\"\n+#include \"utilities\/hashTable.hpp\"\n","filename":"src\/hotspot\/share\/code\/nmethod.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -374,0 +374,1 @@\n+    CompileTask* next = current->next();\n@@ -382,1 +383,1 @@\n-    current = current->next();\n+    current = next;\n@@ -507,0 +508,2 @@\n+  task->set_next(nullptr);\n+  task->set_prev(nullptr);\n@@ -1731,6 +1734,9 @@\n-  if (!task->is_complete() && is_compilation_disabled_forever()) {\n-    \/\/ Task is not complete, and we are exiting for compilation shutdown.\n-    \/\/ The task can still be executed by some compiler thread, therefore\n-    \/\/ we cannot delete it. This will leave task allocated, which leaks it.\n-    \/\/ At this (degraded) point, it is less risky to abandon the task,\n-    \/\/ rather than attempting a more complicated deletion protocol.\n+  if (!task->is_complete()) {\n+    \/\/ Task is not complete, likely because we are exiting for compilation\n+    \/\/ shutdown. The task can still be reached through the queue, or executed\n+    \/\/ by some compiler thread. There is no coordination with either MCQ lock\n+    \/\/ holders or compilers, therefore we cannot delete the task.\n+    \/\/\n+    \/\/ This will leave task allocated, which leaks it. At this (degraded) point,\n+    \/\/ it is less risky to abandon the task, rather than attempting a more\n+    \/\/ complicated deletion protocol.\n@@ -1742,0 +1748,2 @@\n+    assert(task->next() == nullptr && task->prev() == nullptr,\n+           \"Completed task should not be in the queue\");\n","filename":"src\/hotspot\/share\/compiler\/compileBroker.cpp","additions":15,"deletions":7,"binary":false,"changes":22,"status":"modified"},{"patch":"@@ -3,1 +3,1 @@\n- * Copyright (C) 2022 THL A29 Limited, a Tencent company. All rights reserved.\n+ * Copyright (C) 2022, Tencent. All rights reserved.\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/c2\/shenandoahSupport.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -608,10 +608,0 @@\n-\n-\/* Differs from JVM_GetClassModifiers in treatment of inner classes.\n-   This returns the access flags for the class as specified in the\n-   class file rather than searching the InnerClasses attribute (if\n-   present) to find the source-level access flags. Only the values of\n-   the low 13 bits (i.e., a mask of 0x1FFF) are guaranteed to be\n-   valid. *\/\n-JNIEXPORT jint JNICALL\n-JVM_GetClassAccessFlags(JNIEnv *env, jclass cls);\n-\n","filename":"src\/hotspot\/share\/include\/jvm.h","additions":0,"deletions":10,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -40,0 +40,1 @@\n+#include \"utilities\/hashTable.hpp\"\n@@ -41,1 +42,0 @@\n-#include \"utilities\/resourceHash.hpp\"\n@@ -875,1 +875,1 @@\n-    ResourceHashtable<const Symbol*, u2, 256, AnyObj::C_HEAP, mtSymbol, Symbol::compute_hash> _table;\n+    HashTable<const Symbol*, u2, 256, AnyObj::C_HEAP, mtSymbol, Symbol::compute_hash> _table;\n","filename":"src\/hotspot\/share\/oops\/constantPool.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -1236,1 +1236,1 @@\n-using InitializationErrorTable = ResourceHashtable<const InstanceKlass*, OopHandle, 107, AnyObj::C_HEAP, mtClass>;\n+using InitializationErrorTable = HashTable<const InstanceKlass*, OopHandle, 107, AnyObj::C_HEAP, mtClass>;\n","filename":"src\/hotspot\/share\/oops\/instanceKlass.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -94,0 +94,6 @@\n+  \/\/ If there was an error generating the blob then UseCompiler will\n+  \/\/ have been unset and we need to skip the remaining initialization\n+  if (!UseCompiler) {\n+    return false;\n+  }\n+\n","filename":"src\/hotspot\/share\/opto\/c2compiler.cpp","additions":6,"deletions":0,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -92,0 +92,1 @@\n+#include \"utilities\/hashTable.hpp\"\n@@ -93,1 +94,0 @@\n-#include \"utilities\/resourceHash.hpp\"\n@@ -2620,1 +2620,1 @@\n-    igvn.reset_from_gvn(initial_gvn());\n+    igvn.reset();\n@@ -2785,2 +2785,1 @@\n-  \/\/ Initialize IterGVN with types and values from parse-time GVN\n-  PhaseIterGVN igvn(initial_gvn());\n+  PhaseIterGVN igvn;\n@@ -2855,1 +2854,1 @@\n-    igvn.reset_from_gvn(initial_gvn());\n+    igvn.reset();\n@@ -3300,1 +3299,1 @@\n-static uint eval_operand(Node* n, ResourceHashtable<Node*,uint>& eval_map) {\n+static uint eval_operand(Node* n, HashTable<Node*,uint>& eval_map) {\n@@ -3308,1 +3307,1 @@\n-                          ResourceHashtable<Node*,uint>& eval_map) {\n+                          HashTable<Node*,uint>& eval_map) {\n@@ -3332,1 +3331,1 @@\n-  ResourceHashtable<Node*,uint> eval_map;\n+  HashTable<Node*,uint> eval_map;\n","filename":"src\/hotspot\/share\/opto\/compile.cpp","additions":7,"deletions":8,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -1995,1 +1995,1 @@\n-        \/\/ Use 107 as best guess which is the first resize value in ResizeableResourceHashtable::large_table_sizes.\n+        \/\/ Use 107 as best guess which is the first resize value in ResizeableHashTable::large_table_sizes.\n","filename":"src\/hotspot\/share\/opto\/loopnode.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -229,1 +229,1 @@\n-typedef ResizeableResourceHashtable<Node*, Node*, AnyObj::RESOURCE_AREA, mtCompiler> OrigToNewHashtable;\n+typedef ResizeableHashTable<Node*, Node*, AnyObj::RESOURCE_AREA, mtCompiler> OrigToNewHashtable;\n","filename":"src\/hotspot\/share\/opto\/node.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -810,3 +810,3 @@\n-\/\/ Initialize with previous PhaseGVN info from Parser\n-PhaseIterGVN::PhaseIterGVN(PhaseGVN* gvn) : _delay_transform(false),\n-                                            _worklist(*C->igvn_worklist())\n+\/\/ Initialize from scratch\n+PhaseIterGVN::PhaseIterGVN() : _delay_transform(false),\n+                               _worklist(*C->igvn_worklist())\n","filename":"src\/hotspot\/share\/opto\/phaseX.cpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -462,1 +462,1 @@\n-  PhaseIterGVN(PhaseGVN* gvn); \/\/ Used after Parser\n+  PhaseIterGVN();\n@@ -464,8 +464,4 @@\n-  \/\/ Reset IGVN from GVN: call deconstructor, and placement new.\n-  \/\/ Achieves the same as the following (but without move constructors):\n-  \/\/ igvn = PhaseIterGVN(gvn);\n-  void reset_from_gvn(PhaseGVN* gvn) {\n-    if (this != gvn) {\n-      this->~PhaseIterGVN();\n-      ::new (static_cast<void*>(this)) PhaseIterGVN(gvn);\n-    }\n+  \/\/ Reset IGVN: call deconstructor, and placement new.\n+  void reset() {\n+    this->~PhaseIterGVN();\n+    ::new (static_cast<void*>(this)) PhaseIterGVN();\n","filename":"src\/hotspot\/share\/opto\/phaseX.hpp","additions":5,"deletions":9,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -74,1 +74,1 @@\n-    _igvn.reset_from_gvn(C->initial_gvn());\n+    _igvn.reset();\n","filename":"src\/hotspot\/share\/opto\/vector.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -3125,1 +3125,1 @@\n-  oop trace = java_lang_Thread::async_get_stack_trace(JNIHandles::resolve(jthread), THREAD);\n+  oop trace = java_lang_Thread::async_get_stack_trace(jthread, THREAD);\n","filename":"src\/hotspot\/share\/prims\/jvm.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -32,1 +32,1 @@\n-#include \"utilities\/resizeableResourceHash.hpp\"\n+#include \"utilities\/resizableHashTable.hpp\"\n@@ -82,1 +82,1 @@\n-\/\/ This class is the Key type for inserting in ResizeableResourceHashTable\n+\/\/ This class is the Key type for inserting in ResizeableHashTable\n@@ -120,4 +120,4 @@\n-ResizeableResourceHashtable <JvmtiTagMapKey, jlong,\n-                             AnyObj::C_HEAP, mtServiceability,\n-                             JvmtiTagMapKey::get_hash,\n-                             JvmtiTagMapKey::equals> ResizableResourceHT;\n+ResizeableHashTable <JvmtiTagMapKey, jlong,\n+                              AnyObj::C_HEAP, mtServiceability,\n+                              JvmtiTagMapKey::get_hash,\n+                              JvmtiTagMapKey::equals> ResizableHT;\n@@ -133,1 +133,1 @@\n-  ResizableResourceHT _table;\n+  ResizableHT _table;\n@@ -195,4 +195,4 @@\n-ResizeableResourceHashtable <JvmtiFlatTagMapKey, jlong,\n-                             AnyObj::C_HEAP, mtServiceability,\n-                             JvmtiFlatTagMapKey::get_hash,\n-                             JvmtiFlatTagMapKey::equals> FlatObjectHashtable;\n+ResizeableHashTable <JvmtiFlatTagMapKey, jlong,\n+                     AnyObj::C_HEAP, mtServiceability,\n+                     JvmtiFlatTagMapKey::get_hash,\n+                     JvmtiFlatTagMapKey::equals> FlatObjectHashtable;\n","filename":"src\/hotspot\/share\/prims\/jvmtiTagMapTable.hpp","additions":11,"deletions":11,"binary":false,"changes":22,"status":"modified"},{"patch":"@@ -748,1 +748,3 @@\n-static jlong find_field_offset(jclass clazz, jstring name, TRAPS) {\n+\/\/ Finds the object field offset of a field with the matching name, or an error code\n+\/\/ Error code -1 is not found, -2 is static field\n+static jlong find_known_instance_field_offset(jclass clazz, jstring name, TRAPS) {\n@@ -757,1 +759,1 @@\n-  jint offset = -1;\n+  jint offset = -1; \/\/ Not found\n@@ -761,1 +763,5 @@\n-      offset = fs.offset();\n+      if (!fs.access_flags().is_static()) {\n+        offset = fs.offset();\n+      } else {\n+        offset = -2; \/\/ A static field\n+      }\n@@ -766,1 +772,1 @@\n-    THROW_0(vmSymbols::java_lang_InternalError());\n+    return offset; \/\/ Error code\n@@ -795,2 +801,2 @@\n-UNSAFE_ENTRY(jlong, Unsafe_ObjectFieldOffset1(JNIEnv *env, jobject unsafe, jclass c, jstring name)) {\n-  return find_field_offset(c, name, THREAD);\n+UNSAFE_ENTRY(jlong, Unsafe_KnownObjectFieldOffset0(JNIEnv *env, jobject unsafe, jclass c, jstring name)) {\n+  return find_known_instance_field_offset(c, name, THREAD);\n@@ -1228,1 +1234,1 @@\n-    {CC \"objectFieldOffset1\", CC \"(\" CLS LANG \"String;)J\", FN_PTR(Unsafe_ObjectFieldOffset1)},\n+    {CC \"knownObjectFieldOffset0\", CC \"(\" CLS LANG \"String;)J\", FN_PTR(Unsafe_KnownObjectFieldOffset0)},\n","filename":"src\/hotspot\/share\/prims\/unsafe.cpp","additions":13,"deletions":7,"binary":false,"changes":20,"status":"modified"},{"patch":"@@ -1510,6 +1510,0 @@\n-#if INCLUDE_G1GC\n-  if (UseG1GC) {\n-    \/\/ Needs to be cleared explicitly for G1 GC.\n-    Universe::heap()->soft_ref_policy()->set_should_clear_all_soft_refs(false);\n-  }\n-#endif \/\/ INCLUDE_G1GC\n","filename":"src\/hotspot\/share\/prims\/whitebox.cpp","additions":0,"deletions":6,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -30,1 +30,1 @@\n-#include \"memory\/allStatic.hpp\"\n+#include \"memory\/allStatic.hpp\"\n","filename":"src\/hotspot\/share\/runtime\/arguments.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -88,1 +88,1 @@\n-#include \"utilities\/resourceHash.hpp\"\n+#include \"utilities\/hashTable.hpp\"\n@@ -2525,1 +2525,1 @@\n-using AdapterHandlerTable = ResourceHashtable<AdapterFingerPrint*, AdapterHandlerEntry*, 293,\n+using AdapterHandlerTable = HashTable<AdapterFingerPrint*, AdapterHandlerEntry*, 293,\n@@ -3202,1 +3202,1 @@\n-  int offsets[AdapterHandlerEntry::ENTRIES_COUNT];\n+  int offsets[AdapterBlob::ENTRY_COUNT];\n@@ -3205,1 +3205,1 @@\n-  CodeBlob* blob = AOTCodeCache::load_code_blob(AOTCodeEntry::Adapter, id, name, AdapterHandlerEntry::ENTRIES_COUNT, offsets);\n+  CodeBlob* blob = AOTCodeCache::load_code_blob(AOTCodeEntry::Adapter, id, name);\n@@ -3208,0 +3208,1 @@\n+    adapter_blob->get_offsets(offsets);\n@@ -3210,2 +3211,9 @@\n-    handler->set_entry_points(i2c_entry, i2c_entry + offsets[1], i2c_entry + offsets[2], i2c_entry + offsets[3],\n-                              i2c_entry + offsets[4], i2c_entry + offsets[5], i2c_entry + offsets[6]);\n+    handler->set_entry_points(\n+      i2c_entry,\n+      (offsets[1] != -1) ? (i2c_entry + offsets[1]) : nullptr,\n+      (offsets[2] != -1) ? (i2c_entry + offsets[2]) : nullptr,\n+      (offsets[3] != -1) ? (i2c_entry + offsets[3]) : nullptr,\n+      (offsets[4] != -1) ? (i2c_entry + offsets[4]) : nullptr,\n+      (offsets[5] != -1) ? (i2c_entry + offsets[5]) : nullptr,\n+      (offsets[6] != -1) ? (i2c_entry + offsets[6]) : nullptr\n+    );\n@@ -3290,11 +3298,1 @@\n-    int entry_offset[AdapterHandlerEntry::ENTRIES_COUNT];\n-    assert(AdapterHandlerEntry::ENTRIES_COUNT == 7, \"sanity\");\n-    address i2c_entry = handler->get_i2c_entry();\n-    entry_offset[0] = 0; \/\/ i2c_entry offset\n-    entry_offset[1] = handler->get_c2i_entry() - i2c_entry;\n-    entry_offset[2] = handler->get_c2i_inline_entry() - i2c_entry;\n-    entry_offset[3] = handler->get_c2i_inline_ro_entry() - i2c_entry;\n-    entry_offset[4] = handler->get_c2i_unverified_entry() - i2c_entry;\n-    entry_offset[5] = handler->get_c2i_unverified_inline_entry() - i2c_entry;\n-    entry_offset[6] = handler->get_c2i_no_clinit_check_entry() - i2c_entry;\n-    bool success = AOTCodeCache::store_code_blob(*adapter_blob, AOTCodeEntry::Adapter, id, name, AdapterHandlerEntry::ENTRIES_COUNT, entry_offset);\n+    bool success = AOTCodeCache::store_code_blob(*adapter_blob, AOTCodeEntry::Adapter, id, name);\n","filename":"src\/hotspot\/share\/runtime\/sharedRuntime.cpp","additions":15,"deletions":17,"binary":false,"changes":32,"status":"modified"},{"patch":"@@ -189,0 +189,11 @@\n+    \/\/ The compiler blob may be created late by a C2 compiler thread\n+    \/\/ rather than during normal initialization by the initial thread.\n+    \/\/ In that case we can tolerate an allocation failure because the\n+    \/\/ compiler will have been shut down and we have no need of the\n+    \/\/ blob.\n+    if (Thread::current()->is_Compiler_thread()) {\n+      assert(blob_id == BlobId::stubgen_compiler_id, \"sanity\");\n+      assert(DelayCompilerStubsGeneration, \"sanity\");\n+      log_warning(stubs)(\"%s\\t not generated:\\t no space left in CodeCache\", buffer_name);\n+      return nullptr;\n+    }\n","filename":"src\/hotspot\/share\/runtime\/stubRoutines.cpp","additions":11,"deletions":0,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -294,1 +294,1 @@\n-  \/\/ ResourceHashtable SIZE is specified at compile time so we\n+  \/\/ HashTable SIZE is specified at compile time so we\n@@ -296,1 +296,1 @@\n-  typedef ResourceHashtable<int64_t, ObjectMonitorLinkedList*, 1031, AnyObj::C_HEAP, mtThread,\n+  typedef HashTable<int64_t, ObjectMonitorLinkedList*, 1031, AnyObj::C_HEAP, mtThread,\n@@ -329,1 +329,1 @@\n-  \/\/ ResourceHashtable is passed to various functions and populated in\n+  \/\/ HashTable is passed to various functions and populated in\n","filename":"src\/hotspot\/share\/runtime\/vmOperations.cpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -956,1 +956,1 @@\n-  \/\/ ResourceHashtable SIZE is specified at compile time so we\n+  \/\/ HashTable SIZE is specified at compile time so we\n@@ -966,1 +966,1 @@\n-  typedef ResourceHashtable<InstanceKlass*, DumperClassCacheTableEntry*,\n+  typedef HashTable<InstanceKlass*, DumperClassCacheTableEntry*,\n","filename":"src\/hotspot\/share\/services\/heapDumper.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -1348,1 +1348,1 @@\n-\/\/ Default hash\/equals functions used by ResourceHashtable\n+\/\/ Default hash\/equals functions used by HashTable\n","filename":"src\/hotspot\/share\/utilities\/globalDefinitions.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -1273,0 +1273,3 @@\n+     *\n+     * @throws NullPointerException if the field is {@code null}\n+     * @throws IllegalArgumentException if the field is static\n@@ -1284,2 +1287,8 @@\n-     * Reports the location of the field with a given name in the storage\n-     * allocation of its class.\n+     * (For compile-time known instance fields in JDK code only) Reports the\n+     * location of the field with a given name in the storage allocation of its\n+     * class.\n+     * <p>\n+     * This API is used to avoid creating reflective Objects in Java code at\n+     * startup.  This should not be used to find fields in non-trusted code.\n+     * Use the {@link #objectFieldOffset(Field) Field}-accepting version for\n+     * arbitrary fields instead.\n@@ -1288,3 +1297,1 @@\n-     * @throws InternalError if there is no field named {@code name} declared\n-     *         in class {@code c}, i.e., if {@code c.getDeclaredField(name)}\n-     *         would throw {@code java.lang.NoSuchFieldException}.\n+     * @throws InternalError if the presumably known field couldn't be found\n@@ -1299,1 +1306,10 @@\n-        return objectFieldOffset1(c, name);\n+        long result = knownObjectFieldOffset0(c, name);\n+        if (result < 0) {\n+            String type = switch ((int) result) {\n+                case -2 -> \"a static field\";\n+                case -1 -> \"not found\";\n+                default -> \"unknown\";\n+            };\n+            throw new InternalError(\"Field %s.%s %s\".formatted(c.getTypeName(), name, type));\n+        }\n+        return result;\n@@ -1317,0 +1333,3 @@\n+     *\n+     * @throws NullPointerException if the field is {@code null}\n+     * @throws IllegalArgumentException if the field is not static\n@@ -1336,0 +1355,3 @@\n+     *\n+     * @throws NullPointerException if the field is {@code null}\n+     * @throws IllegalArgumentException if the field is not static\n@@ -4407,4 +4429,4 @@\n-    private native long objectFieldOffset0(Field f);\n-    private native long objectFieldOffset1(Class<?> c, String name);\n-    private native long staticFieldOffset0(Field f);\n-    private native Object staticFieldBase0(Field f);\n+    private native long objectFieldOffset0(Field f); \/\/ throws IAE\n+    private native long knownObjectFieldOffset0(Class<?> c, String name); \/\/ error code: -1 not found, -2 static\n+    private native long staticFieldOffset0(Field f); \/\/ throws IAE\n+    private native Object staticFieldBase0(Field f); \/\/ throws IAE\n","filename":"src\/java.base\/share\/classes\/jdk\/internal\/misc\/Unsafe.java","additions":32,"deletions":10,"binary":false,"changes":42,"status":"modified"},{"patch":"@@ -28,0 +28,2 @@\n+import java.lang.annotation.Retention;\n+import java.lang.annotation.RetentionPolicy;\n@@ -38,1 +40,0 @@\n-import com.sun.tools.javac.util.StringUtils;\n@@ -54,1 +55,1 @@\n-        for (Flag flag : asFlagSet(flags)) {\n+        for (FlagsEnum flag : asFlagSet(flags)) {\n@@ -62,4 +63,4 @@\n-    public static EnumSet<Flag> asFlagSet(long flags) {\n-        EnumSet<Flag> flagSet = EnumSet.noneOf(Flag.class);\n-        for (Flag flag : Flag.values()) {\n-            if ((flags & flag.value) != 0) {\n+    public static EnumSet<FlagsEnum> asFlagSet(long flags) {\n+        EnumSet<FlagsEnum> flagSet = EnumSet.noneOf(FlagsEnum.class);\n+        for (FlagsEnum flag : FlagsEnum.values()) {\n+            if ((flags & flag.value()) != 0) {\n@@ -67,1 +68,1 @@\n-                flags &= ~flag.value;\n+                flags &= ~flag.value();\n@@ -76,0 +77,1 @@\n+    @Use({FlagTarget.CLASS, FlagTarget.METHOD, FlagTarget.VARIABLE})\n@@ -77,0 +79,1 @@\n+    @Use({FlagTarget.CLASS, FlagTarget.METHOD, FlagTarget.VARIABLE})\n@@ -78,0 +81,1 @@\n+    @Use({FlagTarget.CLASS, FlagTarget.METHOD, FlagTarget.VARIABLE})\n@@ -79,0 +83,1 @@\n+    @Use({FlagTarget.BLOCK, FlagTarget.CLASS, FlagTarget.METHOD, FlagTarget.VARIABLE})\n@@ -80,0 +85,1 @@\n+    @Use({FlagTarget.CLASS, FlagTarget.METHOD, FlagTarget.VARIABLE})\n@@ -81,0 +87,1 @@\n+    @Use({FlagTarget.METHOD})\n@@ -82,0 +89,1 @@\n+    @Use({FlagTarget.VARIABLE})\n@@ -83,0 +91,1 @@\n+    @Use({FlagTarget.VARIABLE})\n@@ -84,0 +93,1 @@\n+    @Use({FlagTarget.METHOD})\n@@ -85,0 +95,1 @@\n+    @Use({FlagTarget.CLASS})\n@@ -86,0 +97,1 @@\n+    @Use({FlagTarget.CLASS, FlagTarget.METHOD})\n@@ -87,0 +99,1 @@\n+    @Use({FlagTarget.CLASS, FlagTarget.METHOD})\n@@ -90,0 +103,1 @@\n+    @Use({FlagTarget.CLASS, FlagTarget.METHOD, FlagTarget.VARIABLE})\n@@ -93,0 +107,1 @@\n+    @Use({FlagTarget.CLASS})\n@@ -97,0 +112,1 @@\n+    @Use({FlagTarget.CLASS, FlagTarget.VARIABLE})\n@@ -100,0 +116,1 @@\n+    @Use({FlagTarget.MODULE, FlagTarget.VARIABLE})\n@@ -102,0 +119,1 @@\n+    @NotFlag\n@@ -108,5 +126,15 @@\n-    public static final int ACC_IDENTITY = 0x0020;\n-    public static final int ACC_BRIDGE   = 0x0040;\n-    public static final int ACC_VARARGS  = 0x0080;\n-    public static final int ACC_STRICT   = 0x0800;\n-    public static final int ACC_MODULE   = 0x8000;\n+    @Use({FlagTarget.CLASS})\n+    @NoToStringValue\n+    public static final int ACC_IDENTITY = 1<<5;\n+    @Use({FlagTarget.METHOD})\n+    @NoToStringValue\n+    public static final int ACC_BRIDGE   = 1<<6;\n+    @Use({FlagTarget.METHOD})\n+    @NoToStringValue\n+    public static final int ACC_VARARGS  = 1<<7;\n+    @Use({FlagTarget.VARIABLE})\n+    @NoToStringValue\n+    public static final int ACC_STRICT   = 1<<11;\n+    @Use({FlagTarget.CLASS})\n+    @NoToStringValue\n+    public static final int ACC_MODULE   = 1<<15;\n@@ -120,0 +148,1 @@\n+    @Use({FlagTarget.CLASS, FlagTarget.METHOD, FlagTarget.MODULE, FlagTarget.PACKAGE, FlagTarget.TYPE_VAR, FlagTarget.VARIABLE})\n@@ -125,0 +154,1 @@\n+    @Use({FlagTarget.VARIABLE})\n@@ -132,1 +162,2 @@\n-    public static final int IDENTITY_TYPE            = 1<<19;\n+    @Use({FlagTarget.CLASS})\n+    public static final int IDENTITY_TYPE            = 1<<18;\n@@ -136,1 +167,2 @@\n-    public static final int IMPLICIT_CLASS    = 1<<23;\n+    @Use({FlagTarget.CLASS})\n+    public static final int IMPLICIT_CLASS    = 1<<19;\n@@ -141,0 +173,1 @@\n+    @Use({FlagTarget.METHOD})\n@@ -144,0 +177,1 @@\n+    @Use({FlagTarget.CLASS})\n@@ -148,1 +182,2 @@\n-    public static final int FROM_SOURCE      = 1<<21; \/\/ClassSymbols\n+    @Use({FlagTarget.CLASS})\n+    public static final int FROM_SOURCE      = 1<<21;\n@@ -157,0 +192,1 @@\n+    @Use({FlagTarget.CLASS, FlagTarget.VARIABLE})\n@@ -162,0 +198,1 @@\n+    @Use({FlagTarget.CLASS, FlagTarget.PACKAGE})\n@@ -167,0 +204,1 @@\n+    @Use({FlagTarget.CLASS})\n@@ -171,0 +209,1 @@\n+    @Use({FlagTarget.CLASS})\n@@ -176,0 +215,1 @@\n+    @Use({FlagTarget.CLASS})\n@@ -186,0 +226,1 @@\n+    @Use({FlagTarget.CLASS, FlagTarget.METHOD})\n@@ -191,0 +232,1 @@\n+    @Use({FlagTarget.CLASS})\n@@ -195,1 +237,2 @@\n-    public static final int ANONCONSTR   = 1<<29; \/\/non-class members\n+    @Use({FlagTarget.METHOD})\n+    public static final int ANONCONSTR   = 1<<29;\n@@ -200,1 +243,2 @@\n-    public static final int SUPER_OWNER_ATTRIBUTED = 1<<29; \/\/ClassSymbols\n+    @Use({FlagTarget.CLASS})\n+    public static final int SUPER_OWNER_ATTRIBUTED = 1<<29;\n@@ -205,0 +249,1 @@\n+    @Use({FlagTarget.CLASS, FlagTarget.METHOD, FlagTarget.TYPE_VAR})\n@@ -209,0 +254,1 @@\n+    @Use({FlagTarget.METHOD})\n@@ -213,0 +259,1 @@\n+    @Use({FlagTarget.VARIABLE})\n@@ -217,0 +264,1 @@\n+    @Use({FlagTarget.METHOD, FlagTarget.VARIABLE})\n@@ -222,0 +270,1 @@\n+    @Use({FlagTarget.CLASS})\n@@ -226,0 +275,1 @@\n+    @Use({FlagTarget.METHOD})\n@@ -232,0 +282,1 @@\n+    @Use({FlagTarget.METHOD})\n@@ -237,0 +288,1 @@\n+    @Use({FlagTarget.CLASS})\n@@ -242,0 +294,1 @@\n+    @Use({FlagTarget.VARIABLE})\n@@ -248,0 +301,1 @@\n+    @Use({FlagTarget.CLASS, FlagTarget.TYPE_VAR})\n@@ -253,0 +307,1 @@\n+    @Use({FlagTarget.VARIABLE})\n@@ -259,0 +314,1 @@\n+    @Use({FlagTarget.METHOD, FlagTarget.VARIABLE})\n@@ -264,0 +320,1 @@\n+    @Use({FlagTarget.CLASS, FlagTarget.METHOD})\n@@ -270,0 +327,1 @@\n+    @Use({FlagTarget.CLASS})\n@@ -275,0 +333,1 @@\n+    @Use({FlagTarget.CLASS})\n@@ -280,0 +339,1 @@\n+    @Use({FlagTarget.METHOD})\n@@ -285,0 +345,1 @@\n+    @Use({FlagTarget.METHOD})\n@@ -290,0 +351,1 @@\n+    @Use({FlagTarget.TYPE_VAR})\n@@ -295,0 +357,1 @@\n+    @Use({FlagTarget.CLASS})\n@@ -300,1 +363,2 @@\n-    public static final long LAMBDA_METHOD = 1L<<49; \/\/MethodSymbols only\n+    @Use({FlagTarget.METHOD})\n+    public static final long LAMBDA_METHOD = 1L<<49;\n@@ -305,1 +369,2 @@\n-    public static final long LOCAL_CAPTURE_FIELD = 1L<<49; \/\/VarSymbols only\n+    @Use({FlagTarget.VARIABLE})\n+    public static final long LOCAL_CAPTURE_FIELD = 1L<<49;\n@@ -310,0 +375,1 @@\n+    @Use({FlagTarget.CLASS})\n@@ -315,0 +381,1 @@\n+    @Use({FlagTarget.CLASS})\n@@ -320,1 +387,2 @@\n-    public static final long AUTOMATIC_MODULE = 1L<<52; \/\/ModuleSymbols only\n+    @Use({FlagTarget.MODULE})\n+    public static final long AUTOMATIC_MODULE = 1L<<52;\n@@ -325,1 +393,2 @@\n-    public static final long HAS_RESOURCE = 1L<<52; \/\/PackageSymbols only\n+    @Use({FlagTarget.PACKAGE})\n+    public static final long HAS_RESOURCE = 1L<<52;\n@@ -330,1 +399,2 @@\n-    public static final long NAME_FILLED = 1L<<52; \/\/ParamSymbols only\n+    @Use({FlagTarget.VARIABLE}) \/\/ParamSymbols only\n+    public static final long NAME_FILLED = 1L<<52;\n@@ -335,1 +405,2 @@\n-    public static final long SYSTEM_MODULE = 1L<<53; \/\/ModuleSymbols only\n+    @Use({FlagTarget.MODULE})\n+    public static final long SYSTEM_MODULE = 1L<<53;\n@@ -340,1 +411,2 @@\n-    public static final long VALUE_BASED = 1L<<53; \/\/ClassSymbols only\n+    @Use({FlagTarget.CLASS})\n+    public static final long VALUE_BASED = 1L<<53;\n@@ -345,0 +417,1 @@\n+    @Use({FlagTarget.CLASS})\n@@ -350,0 +423,1 @@\n+    @Use({FlagTarget.CLASS, FlagTarget.METHOD, FlagTarget.MODULE, FlagTarget.PACKAGE, FlagTarget.TYPE_VAR, FlagTarget.VARIABLE})\n@@ -355,0 +429,1 @@\n+    @Use({FlagTarget.CLASS, FlagTarget.METHOD, FlagTarget.MODULE, FlagTarget.PACKAGE, FlagTarget.TYPE_VAR, FlagTarget.VARIABLE})\n@@ -360,0 +435,1 @@\n+    @Use({FlagTarget.CLASS, FlagTarget.METHOD, FlagTarget.MODULE, FlagTarget.PACKAGE, FlagTarget.TYPE_VAR, FlagTarget.VARIABLE})\n@@ -365,0 +441,1 @@\n+    @Use({FlagTarget.METHOD})\n@@ -371,1 +448,2 @@\n-    public static final long BODY_ONLY_FINALIZE = 1L<<17; \/\/blocks only\n+    @Use({FlagTarget.BLOCK})\n+    public static final long BODY_ONLY_FINALIZE = 1L<<17;\n@@ -376,0 +454,1 @@\n+    @Use({FlagTarget.CLASS, FlagTarget.METHOD, FlagTarget.MODULE, FlagTarget.PACKAGE, FlagTarget.TYPE_VAR, FlagTarget.VARIABLE})\n@@ -381,0 +460,1 @@\n+    @Use({FlagTarget.VARIABLE})\n@@ -386,0 +466,1 @@\n+    @Use({FlagTarget.VARIABLE})\n@@ -392,1 +473,2 @@\n-    public static final long RECORD = 1L<<61; \/\/ ClassSymbols, MethodSymbols and VarSymbols\n+    @Use({FlagTarget.CLASS, FlagTarget.VARIABLE, FlagTarget.METHOD})\n+    public static final long RECORD = 1L<<61;\n@@ -397,1 +479,2 @@\n-    public static final long COMPACT_RECORD_CONSTRUCTOR = 1L<<51; \/\/ MethodSymbols only\n+    @Use({FlagTarget.METHOD})\n+    public static final long COMPACT_RECORD_CONSTRUCTOR = 1L<<51;\n@@ -402,1 +485,2 @@\n-    public static final long UNINITIALIZED_FIELD= 1L<<51; \/\/ VarSymbols only\n+    @Use({FlagTarget.VARIABLE})\n+    public static final long UNINITIALIZED_FIELD= 1L<<51;\n@@ -407,1 +491,2 @@\n-    public static final int GENERATED_MEMBER = 1<<24; \/\/ MethodSymbols and VarSymbols\n+    @Use({FlagTarget.METHOD, FlagTarget.VARIABLE})\n+    public static final int GENERATED_MEMBER = 1<<24;\n@@ -412,1 +497,2 @@\n-    public static final long RESTRICTED = 1L<<62; \/\/ MethodSymbols\n+    @Use({FlagTarget.METHOD})\n+    public static final long RESTRICTED = 1L<<62;\n@@ -417,1 +503,2 @@\n-    public static final long REQUIRES_IDENTITY = 1L<<62; \/\/ VarSymbols (parameters)\n+    @Use({FlagTarget.VARIABLE}) \/\/ParamSymbols only\n+    public static final long REQUIRES_IDENTITY = 1L<<62;\n@@ -422,1 +509,2 @@\n-    public static final long FIELD_INIT_TYPE_ANNOTATIONS_QUEUED = 1L<<53; \/\/ VarSymbols\n+    @Use({FlagTarget.VARIABLE})\n+    public static final long FIELD_INIT_TYPE_ANNOTATIONS_QUEUED = 1L<<53;\n@@ -427,0 +515,2 @@\n+    @Use({FlagTarget.CLASS})\n+    @CustomToStringValue(\"non-sealed\")\n@@ -432,0 +522,1 @@\n+    @Use({FlagTarget.CLASS})\n@@ -437,1 +528,2 @@\n-    public static final long STRICT = 1L<<53; \/\/ VarSymbols\n+    @Use({FlagTarget.VARIABLE})\n+    public static final long STRICT = 1L<<19; \/\/ VarSymbols\n@@ -451,0 +543,1 @@\n+    @NotFlag\n@@ -467,0 +560,1 @@\n+    @NotFlag\n@@ -524,79 +618,24 @@\n-    public enum Flag {\n-        PUBLIC(Flags.PUBLIC),\n-        PRIVATE(Flags.PRIVATE),\n-        PROTECTED(Flags.PROTECTED),\n-        STATIC(Flags.STATIC),\n-        FINAL(Flags.FINAL),\n-        SYNCHRONIZED(Flags.SYNCHRONIZED),\n-        VOLATILE(Flags.VOLATILE),\n-        TRANSIENT(Flags.TRANSIENT),\n-        NATIVE(Flags.NATIVE),\n-        INTERFACE(Flags.INTERFACE),\n-        ABSTRACT(Flags.ABSTRACT),\n-        DEFAULT(Flags.DEFAULT),\n-        STRICTFP(Flags.STRICTFP),\n-        BRIDGE(Flags.BRIDGE),\n-        SYNTHETIC(Flags.SYNTHETIC),\n-        ANNOTATION(Flags.ANNOTATION),\n-        DEPRECATED(Flags.DEPRECATED),\n-        HASINIT(Flags.HASINIT),\n-        IDENTITY_TYPE(Flags.IDENTITY_TYPE) {\n-            @Override\n-            public String toString() {\n-                return \"identity\";\n-            }\n-        },\n-        VALUE(Flags.VALUE_CLASS),\n-        IMPLICIT_CLASS(Flags.IMPLICIT_CLASS),\n-        BLOCK(Flags.BLOCK),\n-        FROM_SOURCE(Flags.FROM_SOURCE),\n-        ENUM(Flags.ENUM),\n-        MANDATED(Flags.MANDATED),\n-        NOOUTERTHIS(Flags.NOOUTERTHIS),\n-        EXISTS(Flags.EXISTS),\n-        COMPOUND(Flags.COMPOUND),\n-        CLASS_SEEN(Flags.CLASS_SEEN),\n-        SOURCE_SEEN(Flags.SOURCE_SEEN),\n-        LOCKED(Flags.LOCKED),\n-        UNATTRIBUTED(Flags.UNATTRIBUTED),\n-        ANONCONSTR(Flags.ANONCONSTR),\n-        ACYCLIC(Flags.ACYCLIC),\n-        PARAMETER(Flags.PARAMETER),\n-        VARARGS(Flags.VARARGS),\n-        ACYCLIC_ANN(Flags.ACYCLIC_ANN),\n-        GENERATEDCONSTR(Flags.GENERATEDCONSTR),\n-        HYPOTHETICAL(Flags.HYPOTHETICAL),\n-        PROPRIETARY(Flags.PROPRIETARY),\n-        UNION(Flags.UNION),\n-        EFFECTIVELY_FINAL(Flags.EFFECTIVELY_FINAL),\n-        CLASH(Flags.CLASH),\n-        AUXILIARY(Flags.AUXILIARY),\n-        NOT_IN_PROFILE(Flags.NOT_IN_PROFILE),\n-        BAD_OVERRIDE(Flags.BAD_OVERRIDE),\n-        SIGNATURE_POLYMORPHIC(Flags.SIGNATURE_POLYMORPHIC),\n-        THROWS(Flags.THROWS),\n-        LAMBDA_METHOD(Flags.LAMBDA_METHOD),\n-        TYPE_TRANSLATED(Flags.TYPE_TRANSLATED),\n-        MODULE(Flags.MODULE),\n-        AUTOMATIC_MODULE(Flags.AUTOMATIC_MODULE),\n-        SYSTEM_MODULE(Flags.SYSTEM_MODULE),\n-        DEPRECATED_ANNOTATION(Flags.DEPRECATED_ANNOTATION),\n-        DEPRECATED_REMOVAL(Flags.DEPRECATED_REMOVAL),\n-        HAS_RESOURCE(Flags.HAS_RESOURCE),\n-        SEALED(Flags.SEALED),\n-        ANONCONSTR_BASED(Flags.ANONCONSTR_BASED),\n-        NAME_FILLED(Flags.NAME_FILLED),\n-        PREVIEW_API(Flags.PREVIEW_API),\n-        PREVIEW_REFLECTIVE(Flags.PREVIEW_REFLECTIVE),\n-        MATCH_BINDING(Flags.MATCH_BINDING),\n-        MATCH_BINDING_TO_OUTER(Flags.MATCH_BINDING_TO_OUTER),\n-        RECORD(Flags.RECORD),\n-        RECOVERABLE(Flags.RECOVERABLE),\n-        RESTRICTED(Flags.RESTRICTED),\n-        NON_SEALED(Flags.NON_SEALED) {\n-            @Override\n-            public String toString() {\n-                return \"non-sealed\";\n-            }\n-        },\n-        STRICT(Flags.STRICT);\n+    public enum FlagTarget {\n+        \/** This flag can appear the JCBlock.\n+         *\/\n+        BLOCK,\n+        \/** This flag can appear on ClassSymbols.\n+         *\/\n+        CLASS,\n+        \/** This flag can appear on ModuleSymbols.\n+         *\/\n+        MODULE,\n+        \/** This flag can appear on PackageSymbols.\n+         *\/\n+        PACKAGE,\n+        \/** This flag can appear on TypeVarSymbols.\n+         *\/\n+        TYPE_VAR,\n+        \/** This flag can appear on MethodSymbols.\n+         *\/\n+        METHOD,\n+        \/** This flag can appear on VarSymbols, includes\n+         *  including ParamSymbol, and BindingSymbol.\n+         *\/\n+        VARIABLE;\n+    }\n@@ -604,4 +643,4 @@\n-        Flag(long flag) {\n-            this.value = flag;\n-            this.lowercaseName = StringUtils.toLowerCase(name());\n-        }\n+    @Retention(RetentionPolicy.RUNTIME)\n+    public @interface Use {\n+        public FlagTarget[] value();\n+    }\n@@ -609,4 +648,2 @@\n-        @Override\n-        public String toString() {\n-            return lowercaseName;\n-        }\n+    @Retention(RetentionPolicy.RUNTIME)\n+    public @interface NotFlag {}\n@@ -614,2 +651,3 @@\n-        final long value;\n-        final String lowercaseName;\n+    @Retention(RetentionPolicy.RUNTIME)\n+    public @interface CustomToStringValue {\n+        public String value();\n@@ -618,0 +656,3 @@\n+    @Retention(RetentionPolicy.RUNTIME)\n+    public @interface NoToStringValue {\n+    }\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/code\/Flags.java","additions":163,"deletions":122,"binary":false,"changes":285,"status":"modified"},{"patch":"@@ -536,8 +536,9 @@\n-        Attribute.Array values = (Attribute.Array)suppressWarnings.member(names.value);\n-        for (Attribute value : values.values) {\n-            Optional.of(value)\n-              .filter(val -> val instanceof Attribute.Constant)\n-              .map(val -> (String) ((Attribute.Constant) val).value)\n-              .flatMap(LintCategory::get)\n-              .filter(lc -> lc.annotationSuppression)\n-              .ifPresent(result::add);\n+        if (suppressWarnings.member(names.value) instanceof Attribute.Array values) {\n+            for (Attribute value : values.values) {\n+                Optional.of(value)\n+                  .filter(val -> val instanceof Attribute.Constant)\n+                  .map(val -> (String) ((Attribute.Constant) val).value)\n+                  .flatMap(LintCategory::get)\n+                  .filter(lc -> lc.annotationSuppression)\n+                  .ifPresent(result::add);\n+            }\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/code\/Lint.java","additions":9,"deletions":8,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -1847,2 +1847,7 @@\n-        if ((flags & IDENTITY_TYPE) != 0) {\n-            result |= ACC_IDENTITY;\n+        if (sym.kind == TYP) {\n+            \/* flags IDENTITY_TYPE and HAS_INIT share the same value, this is why we need first to double check that\n+             * we are dealing with a type\n+             *\/\n+            if ((flags & IDENTITY_TYPE) != 0) {\n+                result |= ACC_IDENTITY;\n+            }\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/jvm\/ClassWriter.java","additions":7,"deletions":2,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -40,1 +40,1 @@\n-import com.sun.tools.javac.code.Flags.Flag;\n+import com.sun.tools.javac.code.FlagsEnum;\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/parser\/JavacParser.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -78,3 +78,0 @@\n-compiler\/ciReplay\/TestInliningProtectionDomain.java 8349191 generic-all\n-compiler\/ciReplay\/TestIncrementalInlining.java 8349191 generic-all\n-\n@@ -89,0 +86,1 @@\n+compiler\/ciReplay\/TestInliningProtectionDomain.java 8368939 generic-all\n@@ -112,0 +110,1 @@\n+runtime\/cds\/DeterministicDump.java 8363986 macosx-x64,macosx-aarch64\n","filename":"test\/hotspot\/jtreg\/ProblemList.txt","additions":2,"deletions":3,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -626,2 +626,0 @@\n-java\/rmi\/transport\/checkLeaseInfoLeak\/CheckLeaseLeak.java       7191877 generic-all\n-\n@@ -861,2 +859,0 @@\n-jdk\/classfile\/AccessFlagsTest.java 8366270 generic-all\n-\n@@ -865,0 +861,2 @@\n+jdk\/classfile\/AccessFlagsTest.java 8366270 generic-all\n+jdk\/internal\/misc\/Unsafe\/AddressComputationContractTest.java 8368933 generic-all\n","filename":"test\/jdk\/ProblemList.txt","additions":2,"deletions":4,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -3,0 +3,1 @@\n+ * Copyright (c) 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -26,1 +27,1 @@\n- * @bug 8211138 8215246\n+ * @bug 8211138 8215246 8362885\n@@ -30,1 +31,2 @@\n- * @run main FlagsTest\n+ * @compile FlagsTest.java\n+ * @run main\/manual FlagsTest\n@@ -33,0 +35,3 @@\n+import com.sun.tools.javac.code.Flags.FlagTarget;\n+import com.sun.tools.javac.code.Flags.NotFlag;\n+import com.sun.tools.javac.code.Flags.Use;\n@@ -34,1 +39,4 @@\n-import java.lang.reflect.Modifier;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n@@ -37,1 +45,19 @@\n-    public static void main(String[] args) throws IllegalAccessException {\n+\n+    private static final int U2_SIZE = 16;\n+\n+    public static void main(String[] args) throws Throwable {\n+        findFreeFlags();\n+    }\n+\n+    private static void findFreeFlags() throws Throwable {\n+        Map<FlagTarget, Map<Long, List<Field>>> target2Flag2Fields = computeTarget2Flag2Fields();\n+\n+        for (FlagTarget target : FlagTarget.values()) {\n+            long freeFlags = ~collectFlags(target2Flag2Fields, target);\n+\n+            printFreeFlags(target.name(), freeFlags);\n+        }\n+    }\n+\n+    private static Map<FlagTarget, Map<Long, List<Field>>> computeTarget2Flag2Fields() throws Throwable {\n+        Map<FlagTarget, Map<Long, List<Field>>> target2Flag2Fields = new HashMap<>();\n@@ -39,1 +65,1 @@\n-            if (!Modifier.isStatic(f.getModifiers())) {\n+            if (f.isAnnotationPresent(NotFlag.class)) {\n@@ -42,5 +68,13 @@\n-            long flag = ((Number) f.get(null)).longValue();\n-            try {\n-                Flags.asFlagSet(flag);\n-            } catch (AssertionError e) {\n-                throw new AssertionError(\"missing Flags enum constant for: \" + f.getName(), e);\n+\n+            Use use = f.getAnnotation(Use.class);\n+\n+            if (use == null) {\n+                throw new AssertionError(\"No @Use and no @NotFlag for: \" + f.getName());\n+            }\n+\n+            long flagValue = ((Number) f.get(null)).longValue();\n+\n+            for (FlagTarget target : use.value()) {\n+                target2Flag2Fields.computeIfAbsent(target, _ -> new HashMap<>())\n+                        .computeIfAbsent(flagValue, _ -> new ArrayList<>())\n+                        .add(f);\n@@ -49,0 +83,23 @@\n+        return target2Flag2Fields;\n+    }\n+\n+    private static void printFreeFlags(String comment, long freeFlags) {\n+            System.err.print(\"free flags for \" + comment + \": \");\n+            for (int bit = U2_SIZE; bit < Long.SIZE; bit++) { \/\/lowest 16 bits are used in classfiles, never suggest adding anything there\n+                if ((freeFlags & (1L << bit)) != 0) {\n+                    System.err.print(\"1L<<\" + bit + \" \");\n+                }\n+            }\n+            System.err.println();\n+    }\n+\n+    private static long collectFlags(Map<FlagTarget, Map<Long, List<Field>>> target2Flag2Fields, FlagTarget... forTargets) {\n+        long flags = 0;\n+\n+        for (FlagTarget target : forTargets) {\n+            for (long used : target2Flag2Fields.get(target).keySet()) {\n+                flags |= used;\n+            }\n+        }\n+\n+        return flags;\n","filename":"test\/langtools\/tools\/javac\/flags\/FlagsTest.java","additions":67,"deletions":10,"binary":false,"changes":77,"status":"modified"}]}