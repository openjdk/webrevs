{"files":[{"patch":"@@ -34,0 +34,2 @@\n+include CopyFiles.gmk\n+\n@@ -93,1 +95,0 @@\n-\n@@ -120,0 +121,45 @@\n+################################################################################\n+# Setup compilation for value classes in the module\n+# TBD: When $(DOCLINT) was included there was an NPE in JavacTypes.getOverriddenMethods\n+\n+# Directory and file name suffix for jar file containing value classes\n+VALUECLASSES_STR := valueclasses\n+\n+ifneq ($(COMPILER), bootjdk)\n+  MODULE_VALUECLASS_SRC_DIRS := $(call FindModuleValueClassSrcDirs, $(MODULE))\n+  MODULE_VALUECLASS_SOURCEPATH := $(call GetModuleValueClassSrcPath)\n+\n+  ifneq ($(MODULE_VALUECLASS_SRC_DIRS),)\n+    $(eval $(call SetupJavaCompilation, $(MODULE)-$(VALUECLASSES_STR), \\\n+        SMALL_JAVA := false, \\\n+        MODULE := $(MODULE), \\\n+        SRC := $(wildcard $(MODULE_VALUECLASS_SRC_DIRS)), \\\n+        INCLUDES := $(JDK_USER_DEFINED_FILTER), \\\n+        FAIL_NO_SRC := $(FAIL_NO_SRC), \\\n+        BIN := $(SUPPORT_OUTPUTDIR)\/$(VALUECLASSES_STR)\/, \\\n+        JAR := $(JDK_OUTPUTDIR)\/lib\/$(VALUECLASSES_STR)\/$(MODULE)-$(VALUECLASSES_STR).jar, \\\n+        HEADERS := $(SUPPORT_OUTPUTDIR)\/headers, \\\n+        DISABLED_WARNINGS := $(DISABLED_WARNINGS_java) preview, \\\n+        EXCLUDES := $(EXCLUDES), \\\n+        EXCLUDE_FILES := $(EXCLUDE_FILES) \\\n+        KEEP_ALL_TRANSLATIONS := $(KEEP_ALL_TRANSLATIONS), \\\n+        DEPENDS := $($(MODULE)), \\\n+        JAVAC_FLAGS := \\\n+            $(JAVAC_FLAGS) \\\n+            --module-source-path $(MODULE_VALUECLASS_SOURCEPATH) \\\n+            --module-path $(JDK_OUTPUTDIR)\/modules \\\n+            --system none \\\n+            --enable-preview -source $(JDK_SOURCE_TARGET_VERSION), \\\n+    ))\n+\n+    TARGETS += $($(MODULE)-$(VALUECLASSES_STR))\n+\n+    $(eval $(call SetupCopyFiles, $(MODULE)-copy-valueclass-jar, \\\n+        FILES := $(JDK_OUTPUTDIR)\/lib\/$(VALUECLASSES_STR)\/$(MODULE)-$(VALUECLASSES_STR).jar, \\\n+        DEST := $(SUPPORT_OUTPUTDIR)\/modules_libs\/$(MODULE)\/$(VALUECLASSES_STR), \\\n+    ))\n+\n+    TARGETS += $($(MODULE)-copy-valueclass-jar)\n+  endif\n+endif\n+\n","filename":"make\/CompileJavaModules.gmk","additions":47,"deletions":1,"binary":false,"changes":48,"status":"modified"},{"patch":"@@ -107,1 +107,1 @@\n-JAVADOC_OPTIONS := -use -keywords -notimestamp \\\n+JAVADOC_OPTIONS := -XDignore.symbol.file=true -use -keywords -notimestamp \\\n@@ -110,0 +110,2 @@\n+    -XDenableValueTypes \\\n+    --enable-preview -source $(JDK_SOURCE_TARGET_VERSION) \\\n","filename":"make\/Docs.gmk","additions":3,"deletions":1,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -136,0 +136,1 @@\n+# Param3 - _valhalla, or empty\n@@ -137,2 +138,2 @@\n-  $1_$2_DUMP_EXTRA_ARG := $(if $(filter _nocoops, $2), -XX:-UseCompressedOops, )\n-  $1_$2_DUMP_TYPE      := $(if $(filter _nocoops, $2), -NOCOOPS, )\n+  $1_$2_$3_DUMP_EXTRA_ARG := $(if $(filter _nocoops, $2), -XX:-UseCompressedOops,) $(if $(filter _valhalla, $3), --enable-preview,)\n+  $1_$2_$3_DUMP_TYPE      := $(if $(filter _nocoops, $2), -NOCOOPS,)$(if $(filter _valhalla, $3), -VALHALLA,)\n@@ -141,1 +142,1 @@\n-  $1_$2_CDS_DUMP_FLAGS := $(CDS_DUMP_FLAGS) $(if $(filter g1gc, $(JVM_FEATURES_$1)), -XX:+UseG1GC)\n+  $1_$2_$3_CDS_DUMP_FLAGS := $(CDS_DUMP_FLAGS) $(if $(filter g1gc, $(JVM_FEATURES_$1)), -XX:+UseG1GC)\n@@ -144,1 +145,1 @@\n-    $1_$2_CDS_ARCHIVE := bin\/$1\/classes$2.jsa\n+    $1_$2_$3_CDS_ARCHIVE := bin\/$1\/classes$2$3.jsa\n@@ -146,1 +147,1 @@\n-    $1_$2_CDS_ARCHIVE := lib\/$1\/classes$2.jsa\n+    $1_$2_$3_CDS_ARCHIVE := lib\/$1\/classes$2$3.jsa\n@@ -157,3 +158,3 @@\n-  $$(eval $$(call SetupExecute, $1_$2_gen_cds_archive_jdk, \\\n-      WARN := Creating CDS$$($1_$2_DUMP_TYPE) archive for jdk image for $1, \\\n-      INFO := Using CDS flags for $1: $$($1_$2_CDS_DUMP_FLAGS), \\\n+  $$(eval $$(call SetupExecute, $1_$2_$3_gen_cds_archive_jdk, \\\n+      WARN := Creating CDS$$($1_$2_$3_DUMP_TYPE) archive for jdk image for $1, \\\n+      INFO := Using CDS flags for $1: $$($1_$2_$3_CDS_DUMP_FLAGS), \\\n@@ -161,1 +162,1 @@\n-      OUTPUT_FILE := $$(JDK_IMAGE_DIR)\/$$($1_$2_CDS_ARCHIVE), \\\n+      OUTPUT_FILE := $$(JDK_IMAGE_DIR)\/$$($1_$2_$3_CDS_ARCHIVE), \\\n@@ -164,2 +165,2 @@\n-          -XX:SharedArchiveFile=$$(JDK_IMAGE_DIR)\/$$($1_$2_CDS_ARCHIVE) \\\n-          -$1 $$($1_$2_DUMP_EXTRA_ARG) $$($1_$2_CDS_DUMP_FLAGS) $$(LOG_INFO), \\\n+          -XX:SharedArchiveFile=$$(JDK_IMAGE_DIR)\/$$($1_$2_$3_CDS_ARCHIVE) \\\n+          -$1 $$($1_$2_$3_DUMP_EXTRA_ARG) $$($1_$2_$3_CDS_DUMP_FLAGS) $$(LOG_INFO), \\\n@@ -168,1 +169,1 @@\n-  JDK_TARGETS += $$($1_$2_gen_cds_archive_jdk)\n+  JDK_TARGETS += $$($1_$2_$3_gen_cds_archive_jdk)\n@@ -170,3 +171,3 @@\n-  $$(eval $$(call SetupExecute, $1_$2_gen_cds_archive_jre, \\\n-      WARN := Creating CDS$$($1_$2_DUMP_TYPE) archive for jre image for $1, \\\n-      INFO := Using CDS flags for $1: $$($1_$2_CDS_DUMP_FLAGS), \\\n+  $$(eval $$(call SetupExecute, $1_$2_$3_gen_cds_archive_jre, \\\n+      WARN := Creating CDS$$($1_$2_$3_DUMP_TYPE) archive for jre image for $1, \\\n+      INFO := Using CDS flags for $1: $$($1_$2_$3_CDS_DUMP_FLAGS), \\\n@@ -174,1 +175,1 @@\n-      OUTPUT_FILE := $$(JRE_IMAGE_DIR)\/$$($1_$2_CDS_ARCHIVE), \\\n+      OUTPUT_FILE := $$(JRE_IMAGE_DIR)\/$$($1_$2_$3_CDS_ARCHIVE), \\\n@@ -177,2 +178,2 @@\n-          -XX:SharedArchiveFile=$$(JRE_IMAGE_DIR)\/$$($1_$2_CDS_ARCHIVE) \\\n-          -$1 $$($1_$2_DUMP_EXTRA_ARG) $$($1_$2_CDS_DUMP_FLAGS) $$(LOG_INFO), \\\n+          -XX:SharedArchiveFile=$$(JRE_IMAGE_DIR)\/$$($1_$2_$3_CDS_ARCHIVE) \\\n+          -$1 $$($1_$2_$3_DUMP_EXTRA_ARG) $$($1_$2_$3_CDS_DUMP_FLAGS) $$(LOG_INFO), \\\n@@ -181,1 +182,1 @@\n-  JRE_TARGETS += $$($1_$2_gen_cds_archive_jre)\n+  JRE_TARGETS += $$($1_$2_$3_gen_cds_archive_jre)\n@@ -186,1 +187,2 @@\n-    $(eval $(call CreateCDSArchive,$v,)) \\\n+    $(eval $(call CreateCDSArchive,$v,,)) \\\n+    $(eval $(call CreateCDSArchive,$v,,_valhalla)) \\\n@@ -191,1 +193,2 @@\n-      $(eval $(call CreateCDSArchive,$v,_nocoops)) \\\n+      $(eval $(call CreateCDSArchive,$v,_nocoops,)) \\\n+      $(eval $(call CreateCDSArchive,$v,_nocoops,_valhalla)) \\\n","filename":"make\/Images.gmk","additions":24,"deletions":21,"binary":false,"changes":45,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n-# Copyright (c) 2011, 2023, Oracle and\/or its affiliates. All rights reserved.\n+# Copyright (c) 2011, 2024, Oracle and\/or its affiliates. All rights reserved.\n","filename":"make\/Main.gmk","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -29,0 +29,3 @@\n+# Valhalla temporarily disabled\n+VALHALLA_TEMP=false\n+\n","filename":"make\/autoconf\/hotspot.m4","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -83,0 +83,4 @@\n+GENERATED_VALUE_CLASS_SUBDIRS += \\\n+    $(SUPPORT_OUTPUTDIR)\/gensrc-valueclasses \\\n+    #\n+\n@@ -149,0 +153,6 @@\n+# Find value class source dirs for a particular module  (only generated)\n+# $1 - Module to find source dirs for\n+FindModuleValueClassSrcDirs = \\\n+    $(strip $(wildcard \\\n+        $(addsuffix \/$(strip $1), $(GENERATED_VALUE_CLASS_SUBDIRS))))\n+\n@@ -166,0 +176,6 @@\n+\n+# Construct the complete module source path for value classes\n+GetModuleValueClassSrcPath = \\\n+    $(call PathList, \\\n+        $(addsuffix \/*, $(GENERATED_VALUE_CLASS_SUBDIRS) $(GENERATED_SRC_DIRS) $(IMPORT_MODULES_SRC)) \\\n+        $(foreach sub, $(SRC_SUBDIRS), $(addsuffix \/*\/$(sub), $(TOP_SRC_DIRS))))\n","filename":"make\/common\/Modules.gmk","additions":16,"deletions":0,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -50,4 +50,3 @@\n-  JVM_EXCLUDE_FILES += templateInterpreter.cpp \\\n-      templateInterpreterGenerator.cpp bcEscapeAnalyzer.cpp ciTypeFlow.cpp\n-  JVM_CFLAGS_FEATURES += -DZERO \\\n-      -DZERO_LIBARCH='\"$(OPENJDK_TARGET_CPU_LEGACY_LIB)\"' $(LIBFFI_CFLAGS)\n+  JVM_EXCLUDE_FILES += templateInterpreter.cpp templateInterpreterGenerator.cpp \\\n+                       bcEscapeAnalyzer.cpp ciTypeFlow.cpp macroAssembler_common.cpp\n+  JVM_CFLAGS_FEATURES += -DZERO -DZERO_LIBARCH='\"$(OPENJDK_TARGET_CPU_LEGACY_LIB)\"' $(LIBFFI_CFLAGS)\n","filename":"make\/hotspot\/lib\/JvmFeatures.gmk","additions":3,"deletions":4,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -99,1 +99,1 @@\n-        serial preview dangling-doc-comments, \\\n+        serial preview unchecked deprecation dangling-doc-comments, \\\n","filename":"make\/test\/BuildMicrobenchmark.gmk","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -766,0 +766,1 @@\n+BUILD_HOTSPOT_JTREG_LIBRARIES_CFLAGS_libobjmonusage007 := $(NSK_JVMTI_AGENT_INCLUDES)\n@@ -1483,0 +1484,1 @@\n+  BUILD_HOTSPOT_JTREG_LIBRARIES_LIBS_libobjmonusage007 += -lpthread\n","filename":"make\/test\/JtregNativeHotspot.gmk","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -1657,0 +1657,3 @@\n+  } else if (_entry_point == nullptr) {\n+    \/\/ See CallLeafNoFPIndirect\n+    return 1 * NativeInstruction::instruction_size;\n@@ -1765,3 +1768,0 @@\n-  \/\/ n.b. frame size includes space for return pc and rfp\n-  const int framesize = C->output()->frame_size_in_bytes();\n-\n@@ -1772,10 +1772,1 @@\n-  if (C->clinit_barrier_on_entry()) {\n-    assert(!C->method()->holder()->is_not_initialized(), \"initialization should have been started\");\n-\n-    Label L_skip_barrier;\n-\n-    __ mov_metadata(rscratch2, C->method()->holder()->constant_encoding());\n-    __ clinit_barrier(rscratch2, rscratch1, &L_skip_barrier);\n-    __ far_jump(RuntimeAddress(SharedRuntime::get_handle_wrong_method_stub()));\n-    __ bind(L_skip_barrier);\n-  }\n+  __ verified_entry(C, 0);\n@@ -1783,2 +1774,2 @@\n-  if (C->max_vector_size() > 0) {\n-    __ reinitialize_ptrue();\n+  if (C->stub_function() == nullptr) {\n+    __ entry_barrier();\n@@ -1787,27 +1778,2 @@\n-  int bangsize = C->output()->bang_size_in_bytes();\n-  if (C->output()->need_stack_bang(bangsize))\n-    __ generate_stack_overflow_check(bangsize);\n-\n-  __ build_frame(framesize);\n-\n-  if (C->stub_function() == nullptr) {\n-    BarrierSetAssembler* bs = BarrierSet::barrier_set()->barrier_set_assembler();\n-    if (BarrierSet::barrier_set()->barrier_set_nmethod() != nullptr) {\n-      \/\/ Dummy labels for just measuring the code size\n-      Label dummy_slow_path;\n-      Label dummy_continuation;\n-      Label dummy_guard;\n-      Label* slow_path = &dummy_slow_path;\n-      Label* continuation = &dummy_continuation;\n-      Label* guard = &dummy_guard;\n-      if (!Compile::current()->output()->in_scratch_emit_size()) {\n-        \/\/ Use real labels from actual stub when not emitting code for the purpose of measuring its size\n-        C2EntryBarrierStub* stub = new (Compile::current()->comp_arena()) C2EntryBarrierStub();\n-        Compile::current()->output()->add_stub(stub);\n-        slow_path = &stub->entry();\n-        continuation = &stub->continuation();\n-        guard = &stub->guard();\n-      }\n-      \/\/ In the C2 code, we move the non-hot part of nmethod entry barriers out-of-line to a stub.\n-      bs->nmethod_entry_barrier(masm, slow_path, continuation, guard);\n-    }\n+  if (!Compile::current()->output()->in_scratch_emit_size()) {\n+    __ bind(*_verified_entry);\n@@ -1830,6 +1796,0 @@\n-uint MachPrologNode::size(PhaseRegAlloc* ra_) const\n-{\n-  return MachNode::size(ra_); \/\/ too many variables; just compute it\n-                              \/\/ the hard way\n-}\n-\n@@ -1878,1 +1838,1 @@\n-  __ remove_frame(framesize);\n+  __ remove_frame(framesize, C->needs_stack_repair());\n@@ -1897,5 +1857,0 @@\n-uint MachEpilogNode::size(PhaseRegAlloc *ra_) const {\n-  \/\/ Variable size. Determine dynamically.\n-  return MachNode::size(ra_);\n-}\n-\n@@ -2197,1 +2152,42 @@\n-\/\/=============================================================================\n+\/\/\/=============================================================================\n+#ifndef PRODUCT\n+void MachVEPNode::format(PhaseRegAlloc* ra_, outputStream* st) const\n+{\n+  st->print_cr(\"# MachVEPNode\");\n+  if (!_verified) {\n+    st->print_cr(\"\\t load_class\");\n+  } else {\n+    st->print_cr(\"\\t unpack_inline_arg\");\n+  }\n+}\n+#endif\n+\n+void MachVEPNode::emit(C2_MacroAssembler *masm, PhaseRegAlloc* ra_) const\n+{\n+  if (!_verified) {\n+    __ ic_check(1);\n+  } else {\n+    \/\/ insert a nop at the start of the prolog so we can patch in a\n+    \/\/ branch if we need to invalidate the method later\n+    __ nop();\n+\n+    \/\/ TODO 8284443 Avoid creation of temporary frame\n+    if (ra_->C->stub_function() == nullptr) {\n+      __ verified_entry(ra_->C, 0);\n+      __ entry_barrier();\n+      int framesize = ra_->C->output()->frame_slots() << LogBytesPerInt;\n+      __ remove_frame(framesize, false);\n+    }\n+    \/\/ Unpack inline type args passed as oop and then jump to\n+    \/\/ the verified entry point (skipping the unverified entry).\n+    int sp_inc = __ unpack_inline_args(ra_->C, _receiver_only);\n+    \/\/ Emit code for verified entry and save increment for stack repair on return\n+    __ verified_entry(ra_->C, sp_inc);\n+    if (Compile::current()->output()->in_scratch_emit_size()) {\n+      Label dummy_verified_entry;\n+      __ b(dummy_verified_entry);\n+    } else {\n+      __ b(*_verified_entry);\n+    }\n+  }\n+}\n@@ -2199,0 +2195,1 @@\n+\/\/=============================================================================\n@@ -2221,5 +2218,0 @@\n-uint MachUEPNode::size(PhaseRegAlloc* ra_) const\n-{\n-  return MachNode::size(ra_);\n-}\n-\n@@ -3684,0 +3676,31 @@\n+    if (tf()->returns_inline_type_as_fields() && !_method->is_method_handle_intrinsic()) {\n+      \/\/ The last return value is not set by the callee but used to pass IsInit information to compiled code.\n+      \/\/ Search for the corresponding projection, get the register and emit code that initialized it.\n+      uint con = (tf()->range_cc()->cnt() - 1);\n+      for (DUIterator_Fast imax, i = fast_outs(imax); i < imax; i++) {\n+        ProjNode* proj = fast_out(i)->as_Proj();\n+        if (proj->_con == con) {\n+          \/\/ Set IsInit if r0 is non-null (a non-null value is returned buffered or scalarized)\n+          OptoReg::Name optoReg = ra_->get_reg_first(proj);\n+          VMReg reg = OptoReg::as_VMReg(optoReg, ra_->_framesize, OptoReg::reg2stack(ra_->_matcher._new_SP));\n+          Register toReg = reg->is_reg() ? reg->as_Register() : rscratch1;\n+          __ cmp(r0, zr);\n+          __ cset(toReg, Assembler::NE);\n+          if (reg->is_stack()) {\n+            int st_off = reg->reg2stack() * VMRegImpl::stack_slot_size;\n+            __ str(toReg, Address(sp, st_off));\n+          }\n+          break;\n+        }\n+      }\n+      if (return_value_is_used()) {\n+        \/\/ An inline type is returned as fields in multiple registers.\n+        \/\/ R0 either contains an oop if the inline type is buffered or a pointer\n+        \/\/ to the corresponding InlineKlass with the lowest bit set to 1. Zero r0\n+        \/\/ if the lowest bit is set to allow C2 to use the oop after null checking.\n+        \/\/ r0 &= (r0 & 1) - 1\n+        __ andr(rscratch1, r0, 0x1);\n+        __ sub(rscratch1, rscratch1, 0x1);\n+        __ andr(r0, r0, rscratch1);\n+      }\n+    }\n@@ -6514,1 +6537,1 @@\n-    \"mov  $dst, $con\\t# ptr\\n\\t\"\n+    \"mov  $dst, $con\\t# ptr\"\n@@ -7737,0 +7760,15 @@\n+instruct castN2X(iRegLNoSp dst, iRegN src) %{\n+  match(Set dst (CastP2X src));\n+\n+  ins_cost(INSN_COST);\n+  format %{ \"mov $dst, $src\\t# ptr -> long\" %}\n+\n+  ins_encode %{\n+    if ($dst$$reg != $src$$reg) {\n+      __ mov(as_Register($dst$$reg), as_Register($src$$reg));\n+    }\n+  %}\n+\n+  ins_pipe(ialu_reg);\n+%}\n+\n@@ -14558,1 +14596,1 @@\n-instruct clearArray_reg_reg(iRegL_R11 cnt, iRegP_R10 base, Universe dummy, rFlagsReg cr)\n+instruct clearArray_reg_reg_immL0(iRegL_R11 cnt, iRegP_R10 base, immL0 zero, Universe dummy, rFlagsReg cr)\n@@ -14560,1 +14598,1 @@\n-  match(Set dummy (ClearArray cnt base));\n+  match(Set dummy (ClearArray (Binary cnt base) zero));\n@@ -14577,0 +14615,16 @@\n+instruct clearArray_reg_reg(iRegL_R11 cnt, iRegP_R10 base, iRegL val, Universe dummy, rFlagsReg cr)\n+%{\n+  predicate(((ClearArrayNode*)n)->word_copy_only());\n+  match(Set dummy (ClearArray (Binary cnt base) val));\n+  effect(USE_KILL cnt, USE_KILL base, KILL cr);\n+\n+  ins_cost(4 * INSN_COST);\n+  format %{ \"ClearArray $cnt, $base, $val\" %}\n+\n+  ins_encode %{\n+    __ fill_words($base$$Register, $cnt$$Register, $val$$Register);\n+  %}\n+\n+  ins_pipe(pipe_class_memory);\n+%}\n+\n@@ -14580,1 +14634,2 @@\n-            < (uint64_t)(BlockZeroingLowLimit >> LogBytesPerWord));\n+            < (uint64_t)(BlockZeroingLowLimit >> LogBytesPerWord)\n+            && !((ClearArrayNode*)n)->word_copy_only());\n@@ -15915,0 +15970,18 @@\n+\/\/ entry point is null, target holds the address to call\n+instruct CallLeafNoFPIndirect(iRegP target)\n+%{\n+  predicate(n->as_Call()->entry_point() == nullptr);\n+\n+  match(CallLeafNoFP target);\n+\n+  ins_cost(CALL_COST);\n+\n+  format %{ \"CALL, runtime leaf nofp indirect $target\" %}\n+\n+  ins_encode %{\n+    __ blr($target$$Register);\n+  %}\n+\n+  ins_pipe(pipe_class_call);\n+%}\n+\n@@ -15917,0 +15990,2 @@\n+  predicate(n->as_Call()->entry_point() != nullptr);\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/aarch64.ad","additions":139,"deletions":64,"binary":false,"changes":203,"status":"modified"},{"patch":"@@ -79,0 +79,3 @@\n+  void arraycopy_inlinetype_check(Register obj, Register tmp, CodeStub* slow_path, bool is_dest, bool null_check);\n+  void move(LIR_Opr src, LIR_Opr dst);\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/c1_LIRAssembler_aarch64.hpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -31,0 +31,2 @@\n+#include \"gc\/shared\/barrierSet.hpp\"\n+#include \"gc\/shared\/barrierSetAssembler.hpp\"\n@@ -91,0 +93,6 @@\n+\n+    if (EnableValhalla) {\n+      \/\/ Mask always_locked bit such that we go to the slow path if object is an inline type\n+      andr(hdr, hdr, ~markWord::inline_type_bit_in_place);\n+    }\n+\n@@ -178,2 +186,8 @@\n-  \/\/ This assumes that all prototype bits fit in an int32_t\n-  mov(t1, (int32_t)(intptr_t)markWord::prototype().value());\n+  if (EnableValhalla) {\n+    \/\/ Need to copy markWord::prototype header for klass\n+    assert_different_registers(obj, klass, len, t1, t2);\n+    ldr(t1, Address(klass, Klass::prototype_header_offset()));\n+  } else {\n+    \/\/ This assumes that all prototype bits fit in an int32_t\n+    mov(t1, (int32_t)(intptr_t)markWord::prototype().value());\n+  }\n@@ -317,2 +331,13 @@\n-void C1_MacroAssembler::build_frame(int framesize, int bang_size_in_bytes) {\n-  assert(bang_size_in_bytes >= framesize, \"stack bang size incorrect\");\n+void C1_MacroAssembler::build_frame_helper(int frame_size_in_bytes, int sp_offset_for_orig_pc, int sp_inc, bool reset_orig_pc, bool needs_stack_repair) {\n+  MacroAssembler::build_frame(frame_size_in_bytes);\n+\n+  if (needs_stack_repair) {\n+    save_stack_increment(sp_inc, frame_size_in_bytes);\n+  }\n+  if (reset_orig_pc) {\n+    \/\/ Zero orig_pc to detect deoptimization during buffering in the entry points\n+    str(zr, Address(sp, sp_offset_for_orig_pc));\n+  }\n+}\n+\n+void C1_MacroAssembler::build_frame(int frame_size_in_bytes, int bang_size_in_bytes, int sp_offset_for_orig_pc, bool needs_stack_repair, bool has_scalarized_args, Label* verified_inline_entry_label) {\n@@ -321,0 +346,1 @@\n+  assert(bang_size_in_bytes >= frame_size_in_bytes, \"stack bang size incorrect\");\n@@ -322,1 +348,2 @@\n-  MacroAssembler::build_frame(framesize);\n+\n+  build_frame_helper(frame_size_in_bytes, sp_offset_for_orig_pc, 0, has_scalarized_args, needs_stack_repair);\n@@ -327,3 +354,4 @@\n-}\n-void C1_MacroAssembler::remove_frame(int framesize) {\n-  MacroAssembler::remove_frame(framesize);\n+  if (verified_inline_entry_label != nullptr) {\n+    \/\/ Jump here from the scalarized entry points that already created the frame.\n+    bind(*verified_inline_entry_label);\n+  }\n@@ -333,1 +361,0 @@\n-\n@@ -340,0 +367,62 @@\n+  if (C1Breakpoint) brk(1);\n+}\n+\n+int C1_MacroAssembler::scalarized_entry(const CompiledEntrySignature* ces, int frame_size_in_bytes, int bang_size_in_bytes, int sp_offset_for_orig_pc, Label& verified_inline_entry_label, bool is_inline_ro_entry) {\n+  assert(InlineTypePassFieldsAsArgs, \"sanity\");\n+  \/\/ Make sure there is enough stack space for this method's activation.\n+  assert(bang_size_in_bytes >= frame_size_in_bytes, \"stack bang size incorrect\");\n+  generate_stack_overflow_check(bang_size_in_bytes);\n+\n+  GrowableArray<SigEntry>* sig    = ces->sig();\n+  GrowableArray<SigEntry>* sig_cc = is_inline_ro_entry ? ces->sig_cc_ro() : ces->sig_cc();\n+  VMRegPair* regs      = ces->regs();\n+  VMRegPair* regs_cc   = is_inline_ro_entry ? ces->regs_cc_ro() : ces->regs_cc();\n+  int args_on_stack    = ces->args_on_stack();\n+  int args_on_stack_cc = is_inline_ro_entry ? ces->args_on_stack_cc_ro() : ces->args_on_stack_cc();\n+\n+  assert(sig->length() <= sig_cc->length(), \"Zero-sized inline class not allowed!\");\n+  BasicType* sig_bt = NEW_RESOURCE_ARRAY(BasicType, sig_cc->length());\n+  int args_passed = sig->length();\n+  int args_passed_cc = SigEntry::fill_sig_bt(sig_cc, sig_bt);\n+\n+  \/\/ Create a temp frame so we can call into the runtime. It must be properly set up to accommodate GC.\n+  build_frame_helper(frame_size_in_bytes, sp_offset_for_orig_pc, 0, true, ces->c1_needs_stack_repair());\n+\n+  \/\/ The runtime call might safepoint, make sure nmethod entry barrier is executed\n+  BarrierSetAssembler* bs = BarrierSet::barrier_set()->barrier_set_assembler();\n+  \/\/ C1 code is not hot enough to micro optimize the nmethod entry barrier with an out-of-line stub\n+  bs->nmethod_entry_barrier(this, nullptr \/* slow_path *\/, nullptr \/* continuation *\/, nullptr \/* guard *\/);\n+\n+  \/\/ FIXME -- call runtime only if we cannot in-line allocate all the incoming inline type args.\n+  mov(r19, (intptr_t) ces->method());\n+  if (is_inline_ro_entry) {\n+    far_call(RuntimeAddress(Runtime1::entry_for(Runtime1::buffer_inline_args_no_receiver_id)));\n+  } else {\n+    far_call(RuntimeAddress(Runtime1::entry_for(Runtime1::buffer_inline_args_id)));\n+  }\n+  int rt_call_offset = offset();\n+\n+  \/\/ The runtime call returns the new array in r20 instead of the usual r0\n+  \/\/ because r0 is also j_rarg7 which may be holding a live argument here.\n+  Register val_array = r20;\n+\n+  \/\/ Remove the temp frame\n+  MacroAssembler::remove_frame(frame_size_in_bytes);\n+\n+  \/\/ Check if we need to extend the stack for packing\n+  int sp_inc = 0;\n+  if (args_on_stack > args_on_stack_cc) {\n+    sp_inc = extend_stack_for_inline_args(args_on_stack);\n+  }\n+\n+  shuffle_inline_args(true, is_inline_ro_entry, sig_cc,\n+                      args_passed_cc, args_on_stack_cc, regs_cc, \/\/ from\n+                      args_passed, args_on_stack, regs,          \/\/ to\n+                      sp_inc, val_array);\n+\n+  \/\/ Create the real frame. Below jump will then skip over the stack banging and frame\n+  \/\/ setup code in the verified_inline_entry (which has a different real_frame_size).\n+  build_frame_helper(frame_size_in_bytes, sp_offset_for_orig_pc, sp_inc, false, ces->c1_needs_stack_repair());\n+\n+  b(verified_inline_entry_label);\n+  return rt_call_offset;\n@@ -342,0 +431,1 @@\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/c1_MacroAssembler_aarch64.cpp","additions":99,"deletions":9,"binary":false,"changes":108,"status":"modified"},{"patch":"@@ -669,1 +669,2 @@\n-        int call_offset = __ call_RT(obj, noreg, CAST_FROM_FN_PTR(address, new_instance), klass);\n+        int call_offset;\n+        call_offset = __ call_RT(obj, noreg, CAST_FROM_FN_PTR(address, new_instance), klass);\n@@ -702,0 +703,1 @@\n+    case new_null_free_array_id:\n@@ -709,1 +711,1 @@\n-        } else {\n+        } else if (id == new_object_array_id) {\n@@ -711,0 +713,2 @@\n+        } else {\n+          __ set_info(\"new_null_free_array\", dont_gc_arguments);\n@@ -720,7 +724,22 @@\n-          int tag = ((id == new_type_array_id)\n-                     ? Klass::_lh_array_tag_type_value\n-                     : Klass::_lh_array_tag_obj_value);\n-          __ mov(rscratch1, tag);\n-          __ cmpw(t0, rscratch1);\n-          __ br(Assembler::EQ, ok);\n-          __ stop(\"assert(is an array klass)\");\n+          switch (id) {\n+          case new_type_array_id:\n+            __ cmpw(t0, Klass::_lh_array_tag_type_value);\n+            __ br(Assembler::EQ, ok);\n+            __ stop(\"assert(is a type array klass)\");\n+            break;\n+          case new_object_array_id:\n+            __ cmpw(t0, Klass::_lh_array_tag_obj_value); \/\/ new \"[Ljava\/lang\/Object;\"\n+            __ br(Assembler::EQ, ok);\n+            __ cmpw(t0, Klass::_lh_array_tag_vt_value);  \/\/ new \"[LVT;\"\n+            __ br(Assembler::EQ, ok);\n+            __ stop(\"assert(is an object or inline type array klass)\");\n+            break;\n+          case new_null_free_array_id:\n+            __ cmpw(t0, Klass::_lh_array_tag_vt_value);  \/\/ the array can be a flat array.\n+            __ br(Assembler::EQ, ok);\n+            __ cmpw(t0, Klass::_lh_array_tag_obj_value); \/\/ the array cannot be a flat array (due to InlineArrayElementMaxFlatSize, etc)\n+            __ br(Assembler::EQ, ok);\n+            __ stop(\"assert(is an object or inline type array klass)\");\n+            break;\n+          default:  ShouldNotReachHere();\n+          }\n@@ -737,1 +756,1 @@\n-        } else {\n+        } else if (id == new_object_array_id) {\n@@ -739,0 +758,3 @@\n+        } else {\n+          assert(id == new_null_free_array_id, \"must be\");\n+          call_offset = __ call_RT(obj, noreg, CAST_FROM_FN_PTR(address, new_null_free_array), klass, length);\n@@ -773,0 +795,86 @@\n+    case buffer_inline_args_id:\n+    case buffer_inline_args_no_receiver_id:\n+      {\n+        const char* name = (id == buffer_inline_args_id) ?\n+          \"buffer_inline_args\" : \"buffer_inline_args_no_receiver\";\n+        StubFrame f(sasm, name, dont_gc_arguments);\n+        OopMap* map = save_live_registers(sasm);\n+        Register method = r19;   \/\/ Incoming\n+        address entry = (id == buffer_inline_args_id) ?\n+          CAST_FROM_FN_PTR(address, buffer_inline_args) :\n+          CAST_FROM_FN_PTR(address, buffer_inline_args_no_receiver);\n+        \/\/ This is called from a C1 method's scalarized entry point\n+        \/\/ where r0-r7 may be holding live argument values so we can't\n+        \/\/ return the result in r0 as the other stubs do. LR is used as\n+        \/\/ a temporay below to avoid the result being clobbered by\n+        \/\/ restore_live_registers.\n+        int call_offset = __ call_RT(lr, noreg, entry, method);\n+        oop_maps = new OopMapSet();\n+        oop_maps->add_gc_map(call_offset, map);\n+        restore_live_registers(sasm);\n+        __ mov(r20, lr);\n+        __ verify_oop(r20);  \/\/ r20: an array of buffered value objects\n+     }\n+     break;\n+\n+    case load_flat_array_id:\n+      {\n+        StubFrame f(sasm, \"load_flat_array\", dont_gc_arguments);\n+        OopMap* map = save_live_registers(sasm);\n+\n+        \/\/ Called with store_parameter and not C abi\n+\n+        f.load_argument(1, r0); \/\/ r0,: array\n+        f.load_argument(0, r1); \/\/ r1,: index\n+        int call_offset = __ call_RT(r0, noreg, CAST_FROM_FN_PTR(address, load_flat_array), r0, r1);\n+\n+        \/\/ Ensure the stores that initialize the buffer are visible\n+        \/\/ before any subsequent store that publishes this reference.\n+        __ membar(Assembler::StoreStore);\n+\n+        oop_maps = new OopMapSet();\n+        oop_maps->add_gc_map(call_offset, map);\n+        restore_live_registers_except_r0(sasm);\n+\n+        \/\/ r0: loaded element at array[index]\n+        __ verify_oop(r0);\n+      }\n+      break;\n+\n+    case store_flat_array_id:\n+      {\n+        StubFrame f(sasm, \"store_flat_array\", dont_gc_arguments);\n+        OopMap* map = save_live_registers(sasm, 4);\n+\n+        \/\/ Called with store_parameter and not C abi\n+\n+        f.load_argument(2, r0); \/\/ r0: array\n+        f.load_argument(1, r1); \/\/ r1: index\n+        f.load_argument(0, r2); \/\/ r2: value\n+        int call_offset = __ call_RT(noreg, noreg, CAST_FROM_FN_PTR(address, store_flat_array), r0, r1, r2);\n+\n+        oop_maps = new OopMapSet();\n+        oop_maps->add_gc_map(call_offset, map);\n+        restore_live_registers_except_r0(sasm);\n+      }\n+      break;\n+\n+    case substitutability_check_id:\n+      {\n+        StubFrame f(sasm, \"substitutability_check\", dont_gc_arguments);\n+        OopMap* map = save_live_registers(sasm);\n+\n+        \/\/ Called with store_parameter and not C abi\n+\n+        f.load_argument(1, r1); \/\/ r1,: left\n+        f.load_argument(0, r2); \/\/ r2,: right\n+        int call_offset = __ call_RT(noreg, noreg, CAST_FROM_FN_PTR(address, substitutability_check), r1, r2);\n+\n+        oop_maps = new OopMapSet();\n+        oop_maps->add_gc_map(call_offset, map);\n+        restore_live_registers_except_r0(sasm);\n+\n+        \/\/ r0,: are the two operands substitutable\n+      }\n+      break;\n+\n@@ -812,1 +920,1 @@\n-      { StubFrame f(sasm, \"throw_incompatible_class_cast_exception\", dont_gc_arguments, does_not_return);\n+      { StubFrame f(sasm, \"throw_incompatible_class_change_error\", dont_gc_arguments, does_not_return);\n@@ -817,0 +925,12 @@\n+    case throw_illegal_monitor_state_exception_id:\n+      { StubFrame f(sasm, \"throw_illegal_monitor_state_exception\", dont_gc_arguments);\n+        oop_maps = generate_exception_throw(sasm, CAST_FROM_FN_PTR(address, throw_illegal_monitor_state_exception), false);\n+      }\n+      break;\n+\n+    case throw_identity_exception_id:\n+      { StubFrame f(sasm, \"throw_identity_exception\", dont_gc_arguments);\n+        oop_maps = generate_exception_throw(sasm, CAST_FROM_FN_PTR(address, throw_identity_exception), true);\n+      }\n+      break;\n+\n@@ -1020,0 +1140,2 @@\n+      \/\/ FIXME: For unhandled trap_id this code fails with assert during vm intialization\n+      \/\/ rather than insert a call to unimplemented_entry\n@@ -1027,0 +1149,2 @@\n+\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/c1_Runtime1_aarch64.cpp","additions":135,"deletions":11,"binary":false,"changes":146,"status":"modified"},{"patch":"@@ -49,0 +49,23 @@\n+void C2_MacroAssembler::entry_barrier() {\n+  BarrierSetAssembler* bs = BarrierSet::barrier_set()->barrier_set_assembler();\n+  if (BarrierSet::barrier_set()->barrier_set_nmethod() != nullptr) {\n+    \/\/ Dummy labels for just measuring the code size\n+    Label dummy_slow_path;\n+    Label dummy_continuation;\n+    Label dummy_guard;\n+    Label* slow_path = &dummy_slow_path;\n+    Label* continuation = &dummy_continuation;\n+    Label* guard = &dummy_guard;\n+    if (!Compile::current()->output()->in_scratch_emit_size()) {\n+      \/\/ Use real labels from actual stub when not emitting code for the purpose of measuring its size\n+      C2EntryBarrierStub* stub = new (Compile::current()->comp_arena()) C2EntryBarrierStub();\n+      Compile::current()->output()->add_stub(stub);\n+      slow_path = &stub->entry();\n+      continuation = &stub->continuation();\n+      guard = &stub->guard();\n+    }\n+    \/\/ In the C2 code, we move the non-hot part of nmethod entry barriers out-of-line to a stub.\n+    bs->nmethod_entry_barrier(this, slow_path, continuation, guard);\n+  }\n+}\n+\n@@ -83,0 +106,5 @@\n+    if (EnableValhalla) {\n+      \/\/ Mask inline_type bit such that we go to the slow path if object is an inline type\n+      andr(tmp, tmp, ~((int) markWord::inline_type_bit_in_place));\n+    }\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/c2_MacroAssembler_aarch64.cpp","additions":28,"deletions":0,"binary":false,"changes":28,"status":"modified"},{"patch":"@@ -36,0 +36,1 @@\n+#include \"oops\/constMethodFlags.hpp\"\n@@ -39,0 +40,1 @@\n+#include \"oops\/inlineKlass.hpp\"\n@@ -212,0 +214,63 @@\n+void InterpreterMacroAssembler::allocate_instance(Register klass, Register new_obj,\n+                                                  Register t1, Register t2,\n+                                                  bool clear_fields, Label& alloc_failed) {\n+  MacroAssembler::allocate_instance(klass, new_obj, t1, t2, clear_fields, alloc_failed);\n+  {\n+    SkipIfEqual skip_if(this, &DTraceAllocProbes, 0);\n+    \/\/ Trigger dtrace event for fastpath\n+    push(atos);\n+    call_VM_leaf(CAST_FROM_FN_PTR(address, static_cast<int (*)(oopDesc*)>(SharedRuntime::dtrace_object_alloc)), new_obj);\n+    pop(atos);\n+  }\n+}\n+\n+void InterpreterMacroAssembler::read_flat_field(Register holder_klass,\n+                                                Register field_index, Register field_offset,\n+                                                Register temp, Register obj) {\n+  Label alloc_failed, empty_value, done;\n+  const Register src = field_offset;\n+  const Register alloc_temp = rscratch1;\n+  const Register dst_temp   = temp;\n+  assert_different_registers(obj, holder_klass, field_index, field_offset, dst_temp);\n+\n+  \/\/ Grab the inline field klass\n+  push(holder_klass);\n+  const Register field_klass = holder_klass;\n+  get_inline_type_field_klass(holder_klass, field_index, field_klass);\n+\n+  \/\/check for empty value klass\n+  test_klass_is_empty_inline_type(field_klass, dst_temp, empty_value);\n+\n+  \/\/ allocate buffer\n+  push(obj); \/\/ save holder\n+  allocate_instance(field_klass, obj, alloc_temp, dst_temp, false, alloc_failed);\n+\n+  \/\/ Have an oop instance buffer, copy into it\n+  data_for_oop(obj, dst_temp, field_klass);\n+  pop(alloc_temp);             \/\/ restore holder\n+  lea(src, Address(alloc_temp, field_offset));\n+  \/\/ call_VM_leaf, clobbers a few regs, save restore new obj\n+  push(obj);\n+  access_value_copy(IS_DEST_UNINITIALIZED, src, dst_temp, field_klass);\n+  pop(obj);\n+  pop(holder_klass);\n+  b(done);\n+\n+  bind(empty_value);\n+  get_empty_inline_type_oop(field_klass, dst_temp, obj);\n+  pop(holder_klass);\n+  b(done);\n+\n+  bind(alloc_failed);\n+  pop(obj);\n+  pop(holder_klass);\n+  call_VM(obj, CAST_FROM_FN_PTR(address, InterpreterRuntime::read_flat_field),\n+          obj, field_index, holder_klass);\n+\n+  bind(done);\n+\n+  \/\/ Ensure the stores to copy the inline field contents are visible\n+  \/\/ before any subsequent store that publishes this reference.\n+  membar(Assembler::StoreStore);\n+}\n+\n@@ -246,1 +311,2 @@\n-                                                  Label& ok_is_subtype) {\n+                                                  Label& ok_is_subtype,\n+                                                  bool profile) {\n@@ -252,1 +318,3 @@\n-  profile_typecheck(r2, Rsub_klass, r5); \/\/ blows r2, reloads r5\n+  if (profile) {\n+    profile_typecheck(r2, Rsub_klass, r5); \/\/ blows r2, reloads r5\n+  }\n@@ -620,0 +688,1 @@\n+\n@@ -645,0 +714,31 @@\n+  if (state == atos && InlineTypeReturnedAsFields) {\n+    \/\/ Check if we are returning an non-null inline type and load its fields into registers\n+    Label skip;\n+    test_oop_is_not_inline_type(r0, rscratch2, skip);\n+\n+    \/\/ Load fields from a buffered value with an inline class specific handler\n+    load_klass(rscratch1 \/*dst*\/, r0 \/*src*\/);\n+    ldr(rscratch1, Address(rscratch1, InstanceKlass::adr_inlineklass_fixed_block_offset()));\n+    ldr(rscratch1, Address(rscratch1, InlineKlass::unpack_handler_offset()));\n+    \/\/ Unpack handler can be null if inline type is not scalarizable in returns\n+    cbz(rscratch1, skip);\n+\n+    blr(rscratch1);\n+#ifdef ASSERT\n+    \/\/ TODO 8284443 Enable\n+    if (StressCallingConvention && false) {\n+      Label skip_stress;\n+      ldr(rscratch1, Address(rfp, frame::interpreter_frame_method_offset * wordSize));\n+      ldrw(rscratch1, Address(rscratch1, Method::flags_offset()));\n+      tstw(rscratch1, MethodFlags::has_scalarized_return_flag());\n+      br(Assembler::EQ, skip_stress);\n+      load_klass(r0, r0);\n+      orr(r0, r0, 1);\n+      bind(skip_stress);\n+    }\n+#endif\n+    bind(skip);\n+    \/\/ Check above kills sender esp in rscratch2. Reload it.\n+    ldr(rscratch2, Address(rfp, frame::interpreter_frame_sender_sp_offset * wordSize));\n+  }\n+\n@@ -705,0 +805,4 @@\n+      if (EnableValhalla) {\n+        \/\/ Mask inline_type bit such that we go to the slow path if object is an inline type\n+        andr(swap_reg, swap_reg, ~((int) markWord::inline_type_bit_in_place));\n+      }\n@@ -1058,1 +1162,1 @@\n-void InterpreterMacroAssembler::profile_not_taken_branch(Register mdp) {\n+void InterpreterMacroAssembler::profile_not_taken_branch(Register mdp, bool acmp) {\n@@ -1070,1 +1174,1 @@\n-    update_mdp_by_constant(mdp, in_bytes(BranchData::branch_data_size()));\n+    update_mdp_by_constant(mdp, acmp ? in_bytes(ACmpData::acmp_data_size()) : in_bytes(BranchData::branch_data_size()));\n@@ -1393,0 +1497,114 @@\n+template <class ArrayData> void InterpreterMacroAssembler::profile_array_type(Register mdp,\n+                                                                              Register array,\n+                                                                              Register tmp) {\n+  if (ProfileInterpreter) {\n+    Label profile_continue;\n+\n+    \/\/ If no method data exists, go to profile_continue.\n+    test_method_data_pointer(mdp, profile_continue);\n+\n+    mov(tmp, array);\n+    profile_obj_type(tmp, Address(mdp, in_bytes(ArrayData::array_offset())));\n+\n+    Label not_flat;\n+    test_non_flat_array_oop(array, tmp, not_flat);\n+\n+    set_mdp_flag_at(mdp, ArrayData::flat_array_byte_constant());\n+\n+    bind(not_flat);\n+\n+    Label not_null_free;\n+    test_non_null_free_array_oop(array, tmp, not_null_free);\n+\n+    set_mdp_flag_at(mdp, ArrayData::null_free_array_byte_constant());\n+\n+    bind(not_null_free);\n+\n+    bind(profile_continue);\n+  }\n+}\n+\n+template void InterpreterMacroAssembler::profile_array_type<ArrayLoadData>(Register mdp,\n+                                                                           Register array,\n+                                                                           Register tmp);\n+template void InterpreterMacroAssembler::profile_array_type<ArrayStoreData>(Register mdp,\n+                                                                            Register array,\n+                                                                            Register tmp);\n+\n+void InterpreterMacroAssembler::profile_multiple_element_types(Register mdp, Register element, Register tmp, const Register tmp2) {\n+  if (ProfileInterpreter) {\n+    Label profile_continue;\n+\n+    \/\/ If no method data exists, go to profile_continue.\n+    test_method_data_pointer(mdp, profile_continue);\n+\n+    Label done, update;\n+    cbnz(element, update);\n+    set_mdp_flag_at(mdp, BitData::null_seen_byte_constant());\n+    b(done);\n+\n+    bind(update);\n+    load_klass(tmp, element);\n+\n+    \/\/ Record the object type.\n+    record_klass_in_profile(tmp, mdp, tmp2);\n+\n+    bind(done);\n+\n+    \/\/ The method data pointer needs to be updated.\n+    update_mdp_by_constant(mdp, in_bytes(ArrayStoreData::array_store_data_size()));\n+\n+    bind(profile_continue);\n+  }\n+}\n+\n+\n+void InterpreterMacroAssembler::profile_element_type(Register mdp,\n+                                                     Register element,\n+                                                     Register tmp) {\n+  if (ProfileInterpreter) {\n+    Label profile_continue;\n+\n+    \/\/ If no method data exists, go to profile_continue.\n+    test_method_data_pointer(mdp, profile_continue);\n+\n+    mov(tmp, element);\n+    profile_obj_type(tmp, Address(mdp, in_bytes(ArrayLoadData::element_offset())));\n+\n+    \/\/ The method data pointer needs to be updated.\n+    update_mdp_by_constant(mdp, in_bytes(ArrayLoadData::array_load_data_size()));\n+\n+    bind(profile_continue);\n+  }\n+}\n+\n+void InterpreterMacroAssembler::profile_acmp(Register mdp,\n+                                             Register left,\n+                                             Register right,\n+                                             Register tmp) {\n+  if (ProfileInterpreter) {\n+    Label profile_continue;\n+\n+    \/\/ If no method data exists, go to profile_continue.\n+    test_method_data_pointer(mdp, profile_continue);\n+\n+    mov(tmp, left);\n+    profile_obj_type(tmp, Address(mdp, in_bytes(ACmpData::left_offset())));\n+\n+    Label left_not_inline_type;\n+    test_oop_is_not_inline_type(left, tmp, left_not_inline_type);\n+    set_mdp_flag_at(mdp, ACmpData::left_inline_type_byte_constant());\n+    bind(left_not_inline_type);\n+\n+    mov(tmp, right);\n+    profile_obj_type(tmp, Address(mdp, in_bytes(ACmpData::right_offset())));\n+\n+    Label right_not_inline_type;\n+    test_oop_is_not_inline_type(right, tmp, right_not_inline_type);\n+    set_mdp_flag_at(mdp, ACmpData::right_inline_type_byte_constant());\n+    bind(right_not_inline_type);\n+\n+    bind(profile_continue);\n+  }\n+}\n+\n@@ -1645,1 +1863,1 @@\n-        assert(ReturnTypeEntry::static_cell_count() < TypeStackSlotEntries::per_arg_count(), \"can't move past ret type\");\n+        assert(SingleTypeEntry::static_cell_count() < TypeStackSlotEntries::per_arg_count(), \"can't move past ret type\");\n@@ -1691,1 +1909,1 @@\n-    Address mdo_ret_addr(mdp, -in_bytes(ReturnTypeEntry::size()));\n+    Address mdo_ret_addr(mdp, -in_bytes(SingleTypeEntry::size()));\n","filename":"src\/hotspot\/cpu\/aarch64\/interp_masm_aarch64.cpp","additions":224,"deletions":6,"binary":false,"changes":230,"status":"modified"},{"patch":"@@ -30,0 +30,1 @@\n+#include \"ci\/ciInlineKlass.hpp\"\n@@ -50,0 +51,1 @@\n+#include \"oops\/resolvedFieldEntry.hpp\"\n@@ -56,0 +58,1 @@\n+#include \"runtime\/signature_cc.hpp\"\n@@ -59,0 +62,1 @@\n+#include \"vmreg_aarch64.inline.hpp\"\n@@ -1174,0 +1178,35 @@\n+void MacroAssembler::get_default_value_oop(Register inline_klass, Register temp_reg, Register obj) {\n+#ifdef ASSERT\n+  {\n+    Label done_check;\n+    test_klass_is_inline_type(inline_klass, temp_reg, done_check);\n+    stop(\"get_default_value_oop from non inline type klass\");\n+    bind(done_check);\n+  }\n+#endif\n+  Register offset = temp_reg;\n+  \/\/ Getting the offset of the pre-allocated default value\n+  ldr(offset, Address(inline_klass, in_bytes(InstanceKlass::adr_inlineklass_fixed_block_offset())));\n+  ldr(offset, Address(offset, in_bytes(InlineKlass::default_value_offset_offset())));\n+\n+  \/\/ Getting the mirror\n+  ldr(obj, Address(inline_klass, in_bytes(Klass::java_mirror_offset())));\n+  resolve_oop_handle(obj, inline_klass, temp_reg);\n+\n+  \/\/ Getting the pre-allocated default value from the mirror\n+  Address field(obj, offset);\n+  load_heap_oop(obj, field, inline_klass, rscratch2);\n+}\n+\n+void MacroAssembler::get_empty_inline_type_oop(Register inline_klass, Register temp_reg, Register obj) {\n+#ifdef ASSERT\n+  {\n+    Label done_check;\n+    test_klass_is_empty_inline_type(inline_klass, temp_reg, done_check);\n+    stop(\"get_empty_value from non-empty inline klass\");\n+    bind(done_check);\n+  }\n+#endif\n+  get_default_value_oop(inline_klass, temp_reg, obj);\n+}\n+\n@@ -1861,1 +1900,5 @@\n-  if (!VerifyOops) return;\n+  if (!VerifyOops || VerifyAdapterSharing) {\n+    \/\/ Below address of the code string confuses VerifyAdapterSharing\n+    \/\/ because it may differ between otherwise equivalent adapters.\n+    return;\n+  }\n@@ -1894,1 +1937,5 @@\n-  if (!VerifyOops) return;\n+  if (!VerifyOops || VerifyAdapterSharing) {\n+    \/\/ Below address of the code string confuses VerifyAdapterSharing\n+    \/\/ because it may differ between otherwise equivalent adapters.\n+    return;\n+  }\n@@ -1992,0 +2039,4 @@\n+void MacroAssembler::super_call_VM_leaf(address entry_point) {\n+  MacroAssembler::call_VM_leaf_base(entry_point, 1);\n+}\n+\n@@ -2037,0 +2088,105 @@\n+void MacroAssembler::test_markword_is_inline_type(Register markword, Label& is_inline_type) {\n+  assert_different_registers(markword, rscratch2);\n+  andr(markword, markword, markWord::inline_type_mask_in_place);\n+  mov(rscratch2, markWord::inline_type_pattern);\n+  cmp(markword, rscratch2);\n+  br(Assembler::EQ, is_inline_type);\n+}\n+\n+void MacroAssembler::test_klass_is_inline_type(Register klass, Register temp_reg, Label& is_inline_type) {\n+  ldrw(temp_reg, Address(klass, Klass::access_flags_offset()));\n+  andr(temp_reg, temp_reg, JVM_ACC_IDENTITY);\n+  cbz(temp_reg, is_inline_type);\n+}\n+\n+void MacroAssembler::test_oop_is_not_inline_type(Register object, Register tmp, Label& not_inline_type) {\n+  assert_different_registers(tmp, rscratch1);\n+  cbz(object, not_inline_type);\n+  const int is_inline_type_mask = markWord::inline_type_pattern;\n+  ldr(tmp, Address(object, oopDesc::mark_offset_in_bytes()));\n+  mov(rscratch1, is_inline_type_mask);\n+  andr(tmp, tmp, rscratch1);\n+  cmp(tmp, rscratch1);\n+  br(Assembler::NE, not_inline_type);\n+}\n+\n+void MacroAssembler::test_klass_is_empty_inline_type(Register klass, Register temp_reg, Label& is_empty_inline_type) {\n+#ifdef ASSERT\n+  {\n+    Label done_check;\n+    test_klass_is_inline_type(klass, temp_reg, done_check);\n+    stop(\"test_klass_is_empty_inline_type with non inline type klass\");\n+    bind(done_check);\n+  }\n+#endif\n+  ldrw(temp_reg, Address(klass, InstanceKlass::misc_flags_offset()));\n+  andr(temp_reg, temp_reg, InstanceKlassFlags::is_empty_inline_type_value());\n+  cbnz(temp_reg, is_empty_inline_type);\n+}\n+\n+void MacroAssembler::test_field_is_null_free_inline_type(Register flags, Register temp_reg, Label& is_null_free_inline_type) {\n+  assert(temp_reg == noreg, \"not needed\"); \/\/ keep signature uniform with x86\n+  tbnz(flags, ResolvedFieldEntry::is_null_free_inline_type_shift, is_null_free_inline_type);\n+}\n+\n+void MacroAssembler::test_field_is_not_null_free_inline_type(Register flags, Register temp_reg, Label& not_null_free_inline_type) {\n+  assert(temp_reg == noreg, \"not needed\"); \/\/ keep signature uniform with x86\n+  tbz(flags, ResolvedFieldEntry::is_null_free_inline_type_shift, not_null_free_inline_type);\n+}\n+\n+void MacroAssembler::test_field_is_flat(Register flags, Register temp_reg, Label& is_flat) {\n+  assert(temp_reg == noreg, \"not needed\"); \/\/ keep signature uniform with x86\n+  tbnz(flags, ResolvedFieldEntry::is_flat_shift, is_flat);\n+}\n+\n+void MacroAssembler::test_field_has_null_marker(Register flags, Register temp_reg, Label& has_null_marker) {\n+  assert(temp_reg == noreg, \"not needed\"); \/\/ keep signature uniform with x86\n+  tbnz(flags, ResolvedFieldEntry::has_null_marker_shift, has_null_marker);\n+}\n+\n+void MacroAssembler::test_oop_prototype_bit(Register oop, Register temp_reg, int32_t test_bit, bool jmp_set, Label& jmp_label) {\n+  Label test_mark_word;\n+  \/\/ load mark word\n+  ldr(temp_reg, Address(oop, oopDesc::mark_offset_in_bytes()));\n+  \/\/ check displaced\n+  tst(temp_reg, markWord::unlocked_value);\n+  br(Assembler::NE, test_mark_word);\n+  \/\/ slow path use klass prototype\n+  load_prototype_header(temp_reg, oop);\n+\n+  bind(test_mark_word);\n+  andr(temp_reg, temp_reg, test_bit);\n+  if (jmp_set) {\n+    cbnz(temp_reg, jmp_label);\n+  } else {\n+    cbz(temp_reg, jmp_label);\n+  }\n+}\n+\n+void MacroAssembler::test_flat_array_oop(Register oop, Register temp_reg, Label& is_flat_array) {\n+  test_oop_prototype_bit(oop, temp_reg, markWord::flat_array_bit_in_place, true, is_flat_array);\n+}\n+\n+void MacroAssembler::test_non_flat_array_oop(Register oop, Register temp_reg,\n+                                                  Label&is_non_flat_array) {\n+  test_oop_prototype_bit(oop, temp_reg, markWord::flat_array_bit_in_place, false, is_non_flat_array);\n+}\n+\n+void MacroAssembler::test_null_free_array_oop(Register oop, Register temp_reg, Label& is_null_free_array) {\n+  test_oop_prototype_bit(oop, temp_reg, markWord::null_free_array_bit_in_place, true, is_null_free_array);\n+}\n+\n+void MacroAssembler::test_non_null_free_array_oop(Register oop, Register temp_reg, Label&is_non_null_free_array) {\n+  test_oop_prototype_bit(oop, temp_reg, markWord::null_free_array_bit_in_place, false, is_non_null_free_array);\n+}\n+\n+void MacroAssembler::test_flat_array_layout(Register lh, Label& is_flat_array) {\n+  tst(lh, Klass::_lh_array_tag_flat_value_bit_inplace);\n+  br(Assembler::NE, is_flat_array);\n+}\n+\n+void MacroAssembler::test_non_flat_array_layout(Register lh, Label& is_non_flat_array) {\n+  tst(lh, Klass::_lh_array_tag_flat_value_bit_inplace);\n+  br(Assembler::EQ, is_non_flat_array);\n+}\n+\n@@ -4841,0 +4997,8 @@\n+void MacroAssembler::load_metadata(Register dst, Register src) {\n+  if (UseCompressedClassPointers) {\n+    ldrw(dst, Address(src, oopDesc::klass_offset_in_bytes()));\n+  } else {\n+    ldr(dst, Address(src, oopDesc::klass_offset_in_bytes()));\n+  }\n+}\n+\n@@ -4916,0 +5080,5 @@\n+void MacroAssembler::load_prototype_header(Register dst, Register src) {\n+  load_klass(dst, src);\n+  ldr(dst, Address(dst, Klass::prototype_header_offset()));\n+}\n+\n@@ -5240,0 +5409,40 @@\n+void MacroAssembler::access_value_copy(DecoratorSet decorators, Register src, Register dst,\n+                                       Register inline_klass) {\n+  BarrierSetAssembler* bs = BarrierSet::barrier_set()->barrier_set_assembler();\n+  bs->value_copy(this, decorators, src, dst, inline_klass);\n+}\n+\n+void MacroAssembler::first_field_offset(Register inline_klass, Register offset) {\n+  ldr(offset, Address(inline_klass, InstanceKlass::adr_inlineklass_fixed_block_offset()));\n+  ldrw(offset, Address(offset, InlineKlass::first_field_offset_offset()));\n+}\n+\n+void MacroAssembler::data_for_oop(Register oop, Register data, Register inline_klass) {\n+  \/\/ ((address) (void*) o) + vk->first_field_offset();\n+  Register offset = (data == oop) ? rscratch1 : data;\n+  first_field_offset(inline_klass, offset);\n+  if (data == oop) {\n+    add(data, data, offset);\n+  } else {\n+    lea(data, Address(oop, offset));\n+  }\n+}\n+\n+void MacroAssembler::data_for_value_array_index(Register array, Register array_klass,\n+                                                Register index, Register data) {\n+  assert_different_registers(array, array_klass, index);\n+  assert_different_registers(rscratch1, array, index);\n+\n+  \/\/ array->base() + (index << Klass::layout_helper_log2_element_size(lh));\n+  ldrw(rscratch1, Address(array_klass, Klass::layout_helper_offset()));\n+\n+  \/\/ Klass::layout_helper_log2_element_size(lh)\n+  \/\/ (lh >> _lh_log2_element_size_shift) & _lh_log2_element_size_mask;\n+  lsr(rscratch1, rscratch1, Klass::_lh_log2_element_size_shift);\n+  andr(rscratch1, rscratch1, Klass::_lh_log2_element_size_mask);\n+  lslv(index, index, rscratch1);\n+\n+  add(data, array, index);\n+  add(data, data, arrayOopDesc::base_offset_in_bytes(T_PRIMITIVE_OBJECT));\n+}\n+\n@@ -5316,0 +5525,96 @@\n+\/\/ Object \/ value buffer allocation...\n+void MacroAssembler::allocate_instance(Register klass, Register new_obj,\n+                                       Register t1, Register t2,\n+                                       bool clear_fields, Label& alloc_failed)\n+{\n+  Label done, initialize_header, initialize_object, slow_case, slow_case_no_pop;\n+  Register layout_size = t1;\n+  assert(new_obj == r0, \"needs to be r0\");\n+  assert_different_registers(klass, new_obj, t1, t2);\n+\n+  \/\/ get instance_size in InstanceKlass (scaled to a count of bytes)\n+  ldrw(layout_size, Address(klass, Klass::layout_helper_offset()));\n+  \/\/ test to see if it is malformed in some way\n+  tst(layout_size, Klass::_lh_instance_slow_path_bit);\n+  br(Assembler::NE, slow_case_no_pop);\n+\n+  \/\/ Allocate the instance:\n+  \/\/  If TLAB is enabled:\n+  \/\/    Try to allocate in the TLAB.\n+  \/\/    If fails, go to the slow path.\n+  \/\/    Initialize the allocation.\n+  \/\/    Exit.\n+  \/\/\n+  \/\/  Go to slow path.\n+\n+  if (UseTLAB) {\n+    push(klass);\n+    tlab_allocate(new_obj, layout_size, 0, klass, t2, slow_case);\n+    if (ZeroTLAB || (!clear_fields)) {\n+      \/\/ the fields have been already cleared\n+      b(initialize_header);\n+    } else {\n+      \/\/ initialize both the header and fields\n+      b(initialize_object);\n+    }\n+\n+    if (clear_fields) {\n+      \/\/ The object is initialized before the header.  If the object size is\n+      \/\/ zero, go directly to the header initialization.\n+      bind(initialize_object);\n+      subs(layout_size, layout_size, sizeof(oopDesc));\n+      br(Assembler::EQ, initialize_header);\n+\n+      \/\/ Initialize topmost object field, divide size by 8, check if odd and\n+      \/\/ test if zero.\n+\n+  #ifdef ASSERT\n+      \/\/ make sure instance_size was multiple of 8\n+      Label L;\n+      tst(layout_size, 7);\n+      br(Assembler::EQ, L);\n+      stop(\"object size is not multiple of 8 - adjust this code\");\n+      bind(L);\n+      \/\/ must be > 0, no extra check needed here\n+  #endif\n+\n+      lsr(layout_size, layout_size, LogBytesPerLong);\n+\n+      \/\/ initialize remaining object fields: instance_size was a multiple of 8\n+      {\n+        Label loop;\n+        Register base = t2;\n+\n+        bind(loop);\n+        add(rscratch1, new_obj, layout_size, Assembler::LSL, LogBytesPerLong);\n+        str(zr, Address(rscratch1, sizeof(oopDesc) - 1*oopSize));\n+        subs(layout_size, layout_size, 1);\n+        br(Assembler::NE, loop);\n+      }\n+    } \/\/ clear_fields\n+\n+    \/\/ initialize object header only.\n+    bind(initialize_header);\n+    pop(klass);\n+    Register mark_word = t2;\n+    ldr(mark_word, Address(klass, Klass::prototype_header_offset()));\n+    str(mark_word, Address(new_obj, oopDesc::mark_offset_in_bytes ()));\n+    store_klass_gap(new_obj, zr);  \/\/ zero klass gap for compressed oops\n+    mov(t2, klass);         \/\/ preserve klass\n+    store_klass(new_obj, t2);  \/\/ src klass reg is potentially compressed\n+\n+    \/\/ TODO: Valhalla removed SharedRuntime::dtrace_object_alloc from here ?\n+\n+    b(done);\n+  }\n+\n+  if (UseTLAB) {\n+    bind(slow_case);\n+    pop(klass);\n+  }\n+  bind(slow_case_no_pop);\n+  b(alloc_failed);\n+\n+  bind(done);\n+}\n+\n@@ -5355,0 +5660,14 @@\n+void MacroAssembler::get_inline_type_field_klass(Register klass, Register index, Register inline_klass) {\n+  ldr(inline_klass, Address(klass, InstanceKlass::inline_type_field_klasses_offset()));\n+#ifdef ASSERT\n+  {\n+    Label done;\n+    cbnz(inline_klass, done);\n+    stop(\"get_inline_type_field_klass contains no inline klass\");\n+    bind(done);\n+  }\n+#endif\n+  lea(inline_klass, Address(inline_klass, Array<InlineKlass*>::base_offset_in_bytes()));\n+  ldr(inline_klass, Address(inline_klass, index, Address::lsl(3)));\n+}\n+\n@@ -5480,0 +5799,51 @@\n+void MacroAssembler::remove_frame(int initial_framesize, bool needs_stack_repair) {\n+  if (needs_stack_repair) {\n+    \/\/ Remove the extension of the caller's frame used for inline type unpacking\n+    \/\/\n+    \/\/ Right now the stack looks like this:\n+    \/\/\n+    \/\/ | Arguments from caller     |\n+    \/\/ |---------------------------|  <-- caller's SP\n+    \/\/ | Saved LR #1               |\n+    \/\/ | Saved FP #1               |\n+    \/\/ |---------------------------|\n+    \/\/ | Extension space for       |\n+    \/\/ |   inline arg (un)packing  |\n+    \/\/ |---------------------------|  <-- start of this method's frame\n+    \/\/ | Saved LR #2               |\n+    \/\/ | Saved FP #2               |\n+    \/\/ |---------------------------|  <-- FP\n+    \/\/ | sp_inc                    |\n+    \/\/ | method locals             |\n+    \/\/ |---------------------------|  <-- SP\n+    \/\/\n+    \/\/ There are two copies of FP and LR on the stack. They will be identical\n+    \/\/ unless the caller has been deoptimized, in which case LR #1 will be patched\n+    \/\/ to point at the deopt blob, and LR #2 will still point into the old method.\n+    \/\/\n+    \/\/ The sp_inc stack slot holds the total size of the frame including the\n+    \/\/ extension space minus two words for the saved FP and LR.\n+\n+    int sp_inc_offset = initial_framesize - 3 * wordSize;  \/\/ Immediately below saved LR and FP\n+\n+    ldr(rscratch1, Address(sp, sp_inc_offset));\n+    add(sp, sp, rscratch1);\n+    ldp(rfp, lr, Address(post(sp, 2 * wordSize)));\n+  } else {\n+    remove_frame(initial_framesize);\n+  }\n+}\n+\n+void MacroAssembler::save_stack_increment(int sp_inc, int frame_size) {\n+  int real_frame_size = frame_size + sp_inc;\n+  assert(sp_inc == 0 || sp_inc > 2*wordSize, \"invalid sp_inc value\");\n+  assert(real_frame_size >= 2*wordSize, \"frame size must include FP\/LR space\");\n+  assert((real_frame_size & (StackAlignmentInBytes-1)) == 0, \"frame size not aligned\");\n+\n+  int sp_inc_offset = frame_size - 3 * wordSize;  \/\/ Immediately below saved LR and FP\n+\n+  \/\/ Subtract two words for the saved FP and LR as these will be popped\n+  \/\/ separately. See remove_frame above.\n+  mov(rscratch1, real_frame_size - 2*wordSize);\n+  str(rscratch1, Address(sp, sp_inc_offset));\n+}\n@@ -6393,0 +6763,443 @@\n+#ifdef COMPILER2\n+\/\/ C2 compiled method's prolog code\n+\/\/ Moved here from aarch64.ad to support Valhalla code belows\n+void MacroAssembler::verified_entry(Compile* C, int sp_inc) {\n+  if (C->clinit_barrier_on_entry()) {\n+    assert(!C->method()->holder()->is_not_initialized(), \"initialization should have been started\");\n+\n+    Label L_skip_barrier;\n+\n+    mov_metadata(rscratch2, C->method()->holder()->constant_encoding());\n+    clinit_barrier(rscratch2, rscratch1, &L_skip_barrier);\n+    far_jump(RuntimeAddress(SharedRuntime::get_handle_wrong_method_stub()));\n+    bind(L_skip_barrier);\n+  }\n+\n+  if (C->max_vector_size() > 0) {\n+    reinitialize_ptrue();\n+  }\n+\n+  int bangsize = C->output()->bang_size_in_bytes();\n+  if (C->output()->need_stack_bang(bangsize))\n+    generate_stack_overflow_check(bangsize);\n+\n+  \/\/ n.b. frame size includes space for return pc and rfp\n+  const long framesize = C->output()->frame_size_in_bytes();\n+  build_frame(framesize);\n+\n+  if (C->needs_stack_repair()) {\n+    save_stack_increment(sp_inc, framesize);\n+  }\n+\n+  if (VerifyStackAtCalls) {\n+    Unimplemented();\n+  }\n+}\n+#endif \/\/ COMPILER2\n+\n+int MacroAssembler::store_inline_type_fields_to_buf(ciInlineKlass* vk, bool from_interpreter) {\n+  assert(InlineTypeReturnedAsFields, \"Inline types should never be returned as fields\");\n+  \/\/ An inline type might be returned. If fields are in registers we\n+  \/\/ need to allocate an inline type instance and initialize it with\n+  \/\/ the value of the fields.\n+  Label skip;\n+  \/\/ We only need a new buffered inline type if a new one is not returned\n+  tbz(r0, 0, skip);\n+  int call_offset = -1;\n+\n+  \/\/ Be careful not to clobber r1-7 which hold returned fields\n+  \/\/ Also do not use callee-saved registers as these may be live in the interpreter\n+  Register tmp1 = r13, tmp2 = r14, klass = r15, r0_preserved = r12;\n+\n+  \/\/ The following code is similar to allocate_instance but has some slight differences,\n+  \/\/ e.g. object size is always not zero, sometimes it's constant; storing klass ptr after\n+  \/\/ allocating is not necessary if vk != nullptr, etc. allocate_instance is not aware of these.\n+  Label slow_case;\n+  \/\/ 1. Try to allocate a new buffered inline instance either from TLAB or eden space\n+  mov(r0_preserved, r0); \/\/ save r0 for slow_case since *_allocate may corrupt it when allocation failed\n+\n+  if (vk != nullptr) {\n+    \/\/ Called from C1, where the return type is statically known.\n+    movptr(klass, (intptr_t)vk->get_InlineKlass());\n+    jint lh = vk->layout_helper();\n+    assert(lh != Klass::_lh_neutral_value, \"inline class in return type must have been resolved\");\n+    if (UseTLAB && !Klass::layout_helper_needs_slow_path(lh)) {\n+      tlab_allocate(r0, noreg, lh, tmp1, tmp2, slow_case);\n+    } else {\n+      b(slow_case);\n+    }\n+  } else {\n+    \/\/ Call from interpreter. R0 contains ((the InlineKlass* of the return type) | 0x01)\n+    andr(klass, r0, -2);\n+    if (UseTLAB) {\n+      ldrw(tmp2, Address(klass, Klass::layout_helper_offset()));\n+      tst(tmp2, Klass::_lh_instance_slow_path_bit);\n+      br(Assembler::NE, slow_case);\n+      tlab_allocate(r0, tmp2, 0, tmp1, tmp2, slow_case);\n+    } else {\n+      b(slow_case);\n+    }\n+  }\n+  if (UseTLAB) {\n+    \/\/ 2. Initialize buffered inline instance header\n+    Register buffer_obj = r0;\n+    mov(rscratch1, (intptr_t)markWord::inline_type_prototype().value());\n+    str(rscratch1, Address(buffer_obj, oopDesc::mark_offset_in_bytes()));\n+    store_klass_gap(buffer_obj, zr);\n+    if (vk == nullptr) {\n+      \/\/ store_klass corrupts klass, so save it for later use (interpreter case only).\n+      mov(tmp1, klass);\n+    }\n+    store_klass(buffer_obj, klass);\n+    \/\/ 3. Initialize its fields with an inline class specific handler\n+    if (vk != nullptr) {\n+      far_call(RuntimeAddress(vk->pack_handler())); \/\/ no need for call info as this will not safepoint.\n+    } else {\n+      \/\/ tmp1 holds klass preserved above\n+      ldr(tmp1, Address(tmp1, InstanceKlass::adr_inlineklass_fixed_block_offset()));\n+      ldr(tmp1, Address(tmp1, InlineKlass::pack_handler_offset()));\n+      blr(tmp1);\n+    }\n+\n+    membar(Assembler::StoreStore);\n+    b(skip);\n+  } else {\n+    \/\/ Must have already branched to slow_case above.\n+    DEBUG_ONLY(should_not_reach_here());\n+  }\n+  bind(slow_case);\n+  \/\/ We failed to allocate a new inline type, fall back to a runtime\n+  \/\/ call. Some oop field may be live in some registers but we can't\n+  \/\/ tell. That runtime call will take care of preserving them\n+  \/\/ across a GC if there's one.\n+  mov(r0, r0_preserved);\n+\n+  if (from_interpreter) {\n+    super_call_VM_leaf(StubRoutines::store_inline_type_fields_to_buf());\n+  } else {\n+    far_call(RuntimeAddress(StubRoutines::store_inline_type_fields_to_buf()));\n+    call_offset = offset();\n+  }\n+  membar(Assembler::StoreStore);\n+\n+  bind(skip);\n+  return call_offset;\n+}\n+\n+\/\/ Move a value between registers\/stack slots and update the reg_state\n+bool MacroAssembler::move_helper(VMReg from, VMReg to, BasicType bt, RegState reg_state[]) {\n+  assert(from->is_valid() && to->is_valid(), \"source and destination must be valid\");\n+  if (reg_state[to->value()] == reg_written) {\n+    return true; \/\/ Already written\n+  }\n+\n+  if (from != to && bt != T_VOID) {\n+    if (reg_state[to->value()] == reg_readonly) {\n+      return false; \/\/ Not yet writable\n+    }\n+    if (from->is_reg()) {\n+      if (to->is_reg()) {\n+        if (from->is_Register() && to->is_Register()) {\n+          mov(to->as_Register(), from->as_Register());\n+        } else if (from->is_FloatRegister() && to->is_FloatRegister()) {\n+          fmovd(to->as_FloatRegister(), from->as_FloatRegister());\n+        } else {\n+          ShouldNotReachHere();\n+        }\n+      } else {\n+        int st_off = to->reg2stack() * VMRegImpl::stack_slot_size;\n+        Address to_addr = Address(sp, st_off);\n+        if (from->is_FloatRegister()) {\n+          if (bt == T_DOUBLE) {\n+             strd(from->as_FloatRegister(), to_addr);\n+          } else {\n+             assert(bt == T_FLOAT, \"must be float\");\n+             strs(from->as_FloatRegister(), to_addr);\n+          }\n+        } else {\n+          str(from->as_Register(), to_addr);\n+        }\n+      }\n+    } else {\n+      Address from_addr = Address(sp, from->reg2stack() * VMRegImpl::stack_slot_size);\n+      if (to->is_reg()) {\n+        if (to->is_FloatRegister()) {\n+          if (bt == T_DOUBLE) {\n+            ldrd(to->as_FloatRegister(), from_addr);\n+          } else {\n+            assert(bt == T_FLOAT, \"must be float\");\n+            ldrs(to->as_FloatRegister(), from_addr);\n+          }\n+        } else {\n+          ldr(to->as_Register(), from_addr);\n+        }\n+      } else {\n+        int st_off = to->reg2stack() * VMRegImpl::stack_slot_size;\n+        ldr(rscratch1, from_addr);\n+        str(rscratch1, Address(sp, st_off));\n+      }\n+    }\n+  }\n+\n+  \/\/ Update register states\n+  reg_state[from->value()] = reg_writable;\n+  reg_state[to->value()] = reg_written;\n+  return true;\n+}\n+\n+\/\/ Calculate the extra stack space required for packing or unpacking inline\n+\/\/ args and adjust the stack pointer\n+int MacroAssembler::extend_stack_for_inline_args(int args_on_stack) {\n+  int sp_inc = args_on_stack * VMRegImpl::stack_slot_size;\n+  sp_inc = align_up(sp_inc, StackAlignmentInBytes);\n+  assert(sp_inc > 0, \"sanity\");\n+\n+  \/\/ Save a copy of the FP and LR here for deoptimization patching and frame walking\n+  stp(rfp, lr, Address(pre(sp, -2 * wordSize)));\n+\n+  \/\/ Adjust the stack pointer. This will be repaired on return by MacroAssembler::remove_frame\n+  if (sp_inc < (1 << 9)) {\n+    sub(sp, sp, sp_inc);   \/\/ Fits in an immediate\n+  } else {\n+    mov(rscratch1, sp_inc);\n+    sub(sp, sp, rscratch1);\n+  }\n+\n+  return sp_inc + 2 * wordSize;  \/\/ Account for the FP\/LR space\n+}\n+\n+\/\/ Read all fields from an inline type oop and store the values in registers\/stack slots\n+bool MacroAssembler::unpack_inline_helper(const GrowableArray<SigEntry>* sig, int& sig_index,\n+                                          VMReg from, int& from_index, VMRegPair* to, int to_count, int& to_index,\n+                                          RegState reg_state[]) {\n+  assert(sig->at(sig_index)._bt == T_VOID, \"should be at end delimiter\");\n+  assert(from->is_valid(), \"source must be valid\");\n+  bool progress = false;\n+#ifdef ASSERT\n+  const int start_offset = offset();\n+#endif\n+\n+  Label L_null, L_notNull;\n+  \/\/ Don't use r14 as tmp because it's used for spilling (see MacroAssembler::spill_reg_for)\n+  Register tmp1 = r10;\n+  Register tmp2 = r11;\n+  Register fromReg = noreg;\n+  ScalarizedInlineArgsStream stream(sig, sig_index, to, to_count, to_index, -1);\n+  bool done = true;\n+  bool mark_done = true;\n+  VMReg toReg;\n+  BasicType bt;\n+  \/\/ Check if argument requires a null check\n+  bool null_check = false;\n+  VMReg nullCheckReg;\n+  while (stream.next(nullCheckReg, bt)) {\n+    if (sig->at(stream.sig_index())._offset == -1) {\n+      null_check = true;\n+      break;\n+    }\n+  }\n+  stream.reset(sig_index, to_index);\n+  while (stream.next(toReg, bt)) {\n+    assert(toReg->is_valid(), \"destination must be valid\");\n+    int idx = (int)toReg->value();\n+    if (reg_state[idx] == reg_readonly) {\n+      if (idx != from->value()) {\n+        mark_done = false;\n+      }\n+      done = false;\n+      continue;\n+    } else if (reg_state[idx] == reg_written) {\n+      continue;\n+    }\n+    assert(reg_state[idx] == reg_writable, \"must be writable\");\n+    reg_state[idx] = reg_written;\n+    progress = true;\n+\n+    if (fromReg == noreg) {\n+      if (from->is_reg()) {\n+        fromReg = from->as_Register();\n+      } else {\n+        int st_off = from->reg2stack() * VMRegImpl::stack_slot_size;\n+        ldr(tmp1, Address(sp, st_off));\n+        fromReg = tmp1;\n+      }\n+      if (null_check) {\n+        \/\/ Nullable inline type argument, emit null check\n+        cbz(fromReg, L_null);\n+      }\n+    }\n+    int off = sig->at(stream.sig_index())._offset;\n+    if (off == -1) {\n+      assert(null_check, \"Missing null check at\");\n+      if (toReg->is_stack()) {\n+        int st_off = toReg->reg2stack() * VMRegImpl::stack_slot_size;\n+        mov(tmp2, 1);\n+        str(tmp2, Address(sp, st_off));\n+      } else {\n+        mov(toReg->as_Register(), 1);\n+      }\n+      continue;\n+    }\n+    assert(off > 0, \"offset in object should be positive\");\n+    Address fromAddr = Address(fromReg, off);\n+    if (!toReg->is_FloatRegister()) {\n+      Register dst = toReg->is_stack() ? tmp2 : toReg->as_Register();\n+      if (is_reference_type(bt)) {\n+        load_heap_oop(dst, fromAddr, rscratch1, rscratch2);\n+      } else {\n+        bool is_signed = (bt != T_CHAR) && (bt != T_BOOLEAN);\n+        load_sized_value(dst, fromAddr, type2aelembytes(bt), is_signed);\n+      }\n+      if (toReg->is_stack()) {\n+        int st_off = toReg->reg2stack() * VMRegImpl::stack_slot_size;\n+        str(dst, Address(sp, st_off));\n+      }\n+    } else if (bt == T_DOUBLE) {\n+      ldrd(toReg->as_FloatRegister(), fromAddr);\n+    } else {\n+      assert(bt == T_FLOAT, \"must be float\");\n+      ldrs(toReg->as_FloatRegister(), fromAddr);\n+    }\n+  }\n+  if (progress && null_check) {\n+    if (done) {\n+      b(L_notNull);\n+      bind(L_null);\n+      \/\/ Set IsInit field to zero to signal that the argument is null.\n+      \/\/ Also set all oop fields to zero to make the GC happy.\n+      stream.reset(sig_index, to_index);\n+      while (stream.next(toReg, bt)) {\n+        if (sig->at(stream.sig_index())._offset == -1 ||\n+            bt == T_OBJECT || bt == T_ARRAY) {\n+          if (toReg->is_stack()) {\n+            int st_off = toReg->reg2stack() * VMRegImpl::stack_slot_size;\n+            str(zr, Address(sp, st_off));\n+          } else {\n+            mov(toReg->as_Register(), zr);\n+          }\n+        }\n+      }\n+      bind(L_notNull);\n+    } else {\n+      bind(L_null);\n+    }\n+  }\n+\n+  sig_index = stream.sig_index();\n+  to_index = stream.regs_index();\n+\n+  if (mark_done && reg_state[from->value()] != reg_written) {\n+    \/\/ This is okay because no one else will write to that slot\n+    reg_state[from->value()] = reg_writable;\n+  }\n+  from_index--;\n+  assert(progress || (start_offset == offset()), \"should not emit code\");\n+  return done;\n+}\n+\n+\/\/ Pack fields back into an inline type oop\n+bool MacroAssembler::pack_inline_helper(const GrowableArray<SigEntry>* sig, int& sig_index, int vtarg_index,\n+                                        VMRegPair* from, int from_count, int& from_index, VMReg to,\n+                                        RegState reg_state[], Register val_array) {\n+  assert(sig->at(sig_index)._bt == T_METADATA, \"should be at delimiter\");\n+  assert(to->is_valid(), \"destination must be valid\");\n+\n+  if (reg_state[to->value()] == reg_written) {\n+    skip_unpacked_fields(sig, sig_index, from, from_count, from_index);\n+    return true; \/\/ Already written\n+  }\n+\n+  \/\/ The GC barrier expanded by store_heap_oop below may call into the\n+  \/\/ runtime so use callee-saved registers for any values that need to be\n+  \/\/ preserved. The GC barrier assembler should take care of saving the\n+  \/\/ Java argument registers.\n+  \/\/ TODO 8284443 Isn't it an issue if below code uses r14 as tmp when it contains a spilled value?\n+  \/\/ Be careful with r14 because it's used for spilling (see MacroAssembler::spill_reg_for).\n+  Register val_obj_tmp = r21;\n+  Register from_reg_tmp = r22;\n+  Register tmp1 = r14;\n+  Register tmp2 = r13;\n+  Register tmp3 = r12;\n+  Register val_obj = to->is_stack() ? val_obj_tmp : to->as_Register();\n+\n+  assert_different_registers(val_obj_tmp, from_reg_tmp, tmp1, tmp2, tmp3, val_array);\n+\n+  if (reg_state[to->value()] == reg_readonly) {\n+    if (!is_reg_in_unpacked_fields(sig, sig_index, to, from, from_count, from_index)) {\n+      skip_unpacked_fields(sig, sig_index, from, from_count, from_index);\n+      return false; \/\/ Not yet writable\n+    }\n+    val_obj = val_obj_tmp;\n+  }\n+\n+  int index = arrayOopDesc::base_offset_in_bytes(T_OBJECT) + vtarg_index * type2aelembytes(T_OBJECT);\n+  load_heap_oop(val_obj, Address(val_array, index), tmp1, tmp2);\n+\n+  ScalarizedInlineArgsStream stream(sig, sig_index, from, from_count, from_index);\n+  VMReg fromReg;\n+  BasicType bt;\n+  Label L_null;\n+  while (stream.next(fromReg, bt)) {\n+    assert(fromReg->is_valid(), \"source must be valid\");\n+    reg_state[fromReg->value()] = reg_writable;\n+\n+    int off = sig->at(stream.sig_index())._offset;\n+    if (off == -1) {\n+      \/\/ Nullable inline type argument, emit null check\n+      Label L_notNull;\n+      if (fromReg->is_stack()) {\n+        int ld_off = fromReg->reg2stack() * VMRegImpl::stack_slot_size;\n+        ldrb(tmp2, Address(sp, ld_off));\n+        cbnz(tmp2, L_notNull);\n+      } else {\n+        cbnz(fromReg->as_Register(), L_notNull);\n+      }\n+      mov(val_obj, 0);\n+      b(L_null);\n+      bind(L_notNull);\n+      continue;\n+    }\n+\n+    assert(off > 0, \"offset in object should be positive\");\n+    size_t size_in_bytes = is_java_primitive(bt) ? type2aelembytes(bt) : wordSize;\n+\n+    \/\/ Pack the scalarized field into the value object.\n+    Address dst(val_obj, off);\n+\n+    if (!fromReg->is_FloatRegister()) {\n+      Register src;\n+      if (fromReg->is_stack()) {\n+        src = from_reg_tmp;\n+        int ld_off = fromReg->reg2stack() * VMRegImpl::stack_slot_size;\n+        load_sized_value(src, Address(sp, ld_off), size_in_bytes, \/* is_signed *\/ false);\n+      } else {\n+        src = fromReg->as_Register();\n+      }\n+      assert_different_registers(dst.base(), src, tmp1, tmp2, tmp3, val_array);\n+      if (is_reference_type(bt)) {\n+        store_heap_oop(dst, src, tmp1, tmp2, tmp3, IN_HEAP | ACCESS_WRITE | IS_DEST_UNINITIALIZED);\n+      } else {\n+        store_sized_value(dst, src, size_in_bytes);\n+      }\n+    } else if (bt == T_DOUBLE) {\n+      strd(fromReg->as_FloatRegister(), dst);\n+    } else {\n+      assert(bt == T_FLOAT, \"must be float\");\n+      strs(fromReg->as_FloatRegister(), dst);\n+    }\n+  }\n+  bind(L_null);\n+  sig_index = stream.sig_index();\n+  from_index = stream.regs_index();\n+\n+  assert(reg_state[to->value()] == reg_writable, \"must have already been read\");\n+  bool success = move_helper(val_obj->as_VMReg(), to, T_OBJECT, reg_state);\n+  assert(success, \"to register must be writeable\");\n+\n+  return true;\n+}\n+\n+VMReg MacroAssembler::spill_reg_for(VMReg reg) {\n+  return (reg->is_FloatRegister()) ? v8->as_VMReg() : r14->as_VMReg();\n+}\n+\n@@ -6791,0 +7604,4 @@\n+  if (EnableValhalla) {\n+    \/\/ Mask inline_type bit such that we go to the slow path if object is an inline type\n+    andr(mark, mark, ~((int) markWord::inline_type_bit_in_place));\n+  }\n","filename":"src\/hotspot\/cpu\/aarch64\/macroAssembler_aarch64.cpp","additions":819,"deletions":2,"binary":false,"changes":821,"status":"modified"},{"patch":"@@ -175,0 +175,1 @@\n+  case Bytecodes::_fast_vputfield:\n@@ -756,4 +757,4 @@\n-    \/\/ ??? convention: move array into r3 for exception message\n-  __ mov(r3, array);\n-  __ mov(rscratch1, Interpreter::_throw_ArrayIndexOutOfBoundsException_entry);\n-  __ br(rscratch1);\n+  \/\/ ??? convention: move array into r3 for exception message\n+   __ mov(r3, array);\n+   __ mov(rscratch1, Interpreter::_throw_ArrayIndexOutOfBoundsException_entry);\n+   __ br(rscratch1);\n@@ -819,5 +820,20 @@\n-  __ add(r1, r1, arrayOopDesc::base_offset_in_bytes(T_OBJECT) >> LogBytesPerHeapOop);\n-  do_oop_load(_masm,\n-              Address(r0, r1, Address::uxtw(LogBytesPerHeapOop)),\n-              r0,\n-              IS_ARRAY);\n+  __ profile_array_type<ArrayLoadData>(r2, r0, r4);\n+  if (UseFlatArray) {\n+    Label is_flat_array, done;\n+\n+    __ test_flat_array_oop(r0, r8 \/*temp*\/, is_flat_array);\n+    __ add(r1, r1, arrayOopDesc::base_offset_in_bytes(T_OBJECT) >> LogBytesPerHeapOop);\n+    do_oop_load(_masm, Address(r0, r1, Address::uxtw(LogBytesPerHeapOop)), r0, IS_ARRAY);\n+\n+    __ b(done);\n+    __ bind(is_flat_array);\n+    __ call_VM(r0, CAST_FROM_FN_PTR(address, InterpreterRuntime::value_array_load), r0, r1);\n+    \/\/ Ensure the stores to copy the inline field contents are visible\n+    \/\/ before any subsequent store that publishes this reference.\n+    __ membar(Assembler::StoreStore);\n+    __ bind(done);\n+  } else {\n+    __ add(r1, r1, arrayOopDesc::base_offset_in_bytes(T_OBJECT) >> LogBytesPerHeapOop);\n+    do_oop_load(_masm, Address(r0, r1, Address::uxtw(LogBytesPerHeapOop)), r0, IS_ARRAY);\n+  }\n+  __ profile_element_type(r2, r0, r4);\n@@ -1110,1 +1126,1 @@\n-  Label is_null, ok_is_subtype, done;\n+  Label is_null, is_flat_array, ok_is_subtype, done;\n@@ -1117,2 +1133,4 @@\n-  Address element_address(r3, r4, Address::uxtw(LogBytesPerHeapOop));\n-\n+\n+  __ profile_array_type<ArrayStoreData>(r4, r3, r5);\n+  __ profile_multiple_element_types(r4, r0, r5, r6);\n+\n@@ -1121,0 +1139,2 @@\n+  Address element_address(r3, r4, Address::uxtw(LogBytesPerHeapOop));\n+  \/\/ Be careful not to clobber r4 below\n@@ -1125,0 +1145,8 @@\n+  \/\/ Move array class to r5\n+  __ load_klass(r5, r3);\n+\n+  if (UseFlatArray) {\n+    __ ldrw(r6, Address(r5, Klass::layout_helper_offset()));\n+    __ test_flat_array_layout(r6, is_flat_array);\n+  }\n+\n@@ -1127,4 +1155,3 @@\n-  \/\/ Move superklass into r0\n-  __ load_klass(r0, r3);\n-  __ ldr(r0, Address(r0,\n-                     ObjArrayKlass::element_klass_offset()));\n+\n+  \/\/ Move array element superklass into r0\n+  __ ldr(r0, Address(r5, ObjArrayKlass::element_klass_offset()));\n@@ -1135,1 +1162,3 @@\n-  __ gen_subtype_check(r1, ok_is_subtype);\n+\n+  \/\/ is \"r1 <: r0\" ? (value subclass <: array element superclass)\n+  __ gen_subtype_check(r1, ok_is_subtype, false);\n@@ -1152,1 +1181,12 @@\n-  __ profile_null_seen(r2);\n+  if (EnableValhalla) {\n+    Label is_null_into_value_array_npe, store_null;\n+\n+    \/\/ No way to store null in flat null-free array\n+    __ test_null_free_array_oop(r3, r8, is_null_into_value_array_npe);\n+    __ b(store_null);\n+\n+    __ bind(is_null_into_value_array_npe);\n+    __ b(ExternalAddress(Interpreter::_throw_NullPointerException_entry));\n+\n+    __ bind(store_null);\n+  }\n@@ -1156,0 +1196,41 @@\n+  __ b(done);\n+\n+  if (UseFlatArray) {\n+     Label is_type_ok;\n+    __ bind(is_flat_array); \/\/ Store non-null value to flat\n+\n+    \/\/ Simplistic type check...\n+    \/\/ r0 - value, r2 - index, r3 - array.\n+\n+    \/\/ Profile the not-null value's klass.\n+    \/\/ Load value class\n+     __ load_klass(r1, r0);\n+\n+    \/\/ Move element klass into r7\n+     __ ldr(r7, Address(r5, ArrayKlass::element_klass_offset()));\n+\n+    \/\/ flat value array needs exact type match\n+    \/\/ is \"r1 == r7\" (value subclass == array element superclass)\n+\n+     __ cmp(r7, r1);\n+     __ br(Assembler::EQ, is_type_ok);\n+\n+     __ b(ExternalAddress(Interpreter::_throw_ArrayStoreException_entry));\n+\n+     __ bind(is_type_ok);\n+    \/\/ r1: value's klass\n+    \/\/ r3: array\n+    \/\/ r5: array klass\n+    __ test_klass_is_empty_inline_type(r1, r7, done);\n+\n+    \/\/ calc dst for copy\n+    __ ldrw(r7, at_tos_p1()); \/\/ index\n+    __ data_for_value_array_index(r3, r5, r7, r7);\n+\n+    \/\/ ...and src for copy\n+    __ ldr(r6, at_tos());  \/\/ value\n+    __ data_for_oop(r6, r6, r1);\n+\n+    __ mov(r4, r1);  \/\/ Shuffle arguments to avoid conflict with c_rarg1\n+    __ access_value_copy(IN_HEAP, r6, r7, r4);\n+  }\n@@ -1960,2 +2041,1 @@\n-void TemplateTable::if_acmp(Condition cc)\n-{\n+void TemplateTable::if_acmp(Condition cc) {\n@@ -1964,1 +2044,1 @@\n-  Label not_taken;\n+  Label taken, not_taken;\n@@ -1966,0 +2046,38 @@\n+\n+  __ profile_acmp(r2, r1, r0, r4);\n+\n+  Register is_inline_type_mask = rscratch1;\n+  __ mov(is_inline_type_mask, markWord::inline_type_pattern);\n+\n+  if (EnableValhalla) {\n+    __ cmp(r1, r0);\n+    __ br(Assembler::EQ, (cc == equal) ? taken : not_taken);\n+\n+    \/\/ might be substitutable, test if either r0 or r1 is null\n+    __ andr(r2, r0, r1);\n+    __ cbz(r2, (cc == equal) ? not_taken : taken);\n+\n+    \/\/ and both are values ?\n+    __ ldr(r2, Address(r1, oopDesc::mark_offset_in_bytes()));\n+    __ andr(r2, r2, is_inline_type_mask);\n+    __ ldr(r4, Address(r0, oopDesc::mark_offset_in_bytes()));\n+    __ andr(r4, r4, is_inline_type_mask);\n+    __ andr(r2, r2, r4);\n+    __ cmp(r2,  is_inline_type_mask);\n+    __ br(Assembler::NE, (cc == equal) ? not_taken : taken);\n+\n+    \/\/ same value klass ?\n+    __ load_metadata(r2, r1);\n+    __ load_metadata(r4, r0);\n+    __ cmp(r2, r4);\n+    __ br(Assembler::NE, (cc == equal) ? not_taken : taken);\n+\n+    \/\/ Know both are the same type, let's test for substitutability...\n+    if (cc == equal) {\n+      invoke_is_substitutable(r0, r1, taken, not_taken);\n+    } else {\n+      invoke_is_substitutable(r0, r1, not_taken, taken);\n+    }\n+    __ stop(\"Not reachable\");\n+  }\n+\n@@ -1968,0 +2086,1 @@\n+  __ bind(taken);\n@@ -1970,1 +2089,1 @@\n-  __ profile_not_taken_branch(r0);\n+  __ profile_not_taken_branch(r0, true);\n@@ -1973,0 +2092,10 @@\n+void TemplateTable::invoke_is_substitutable(Register aobj, Register bobj,\n+                                            Label& is_subst, Label& not_subst) {\n+\n+  __ call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::is_substitutable), aobj, bobj);\n+  \/\/ Restored... r0 answer, jmp to outcome...\n+  __ cbz(r0, not_subst);\n+  __ b(is_subst);\n+}\n+\n+\n@@ -2582,1 +2711,1 @@\n-  const Register cache     = r4;\n+  const Register cache     = r2;\n@@ -2584,0 +2713,3 @@\n+  const Register klass     = r5;\n+  const Register inline_klass = r7;\n+  const Register field_index = r23;\n@@ -2592,0 +2724,5 @@\n+\n+  \/\/ Valhalla extras\n+  __ load_unsigned_short(field_index, Address(cache, in_bytes(ResolvedFieldEntry::field_index_offset())));\n+  __ ldr(klass, Address(cache, ResolvedFieldEntry::field_holder_offset()));\n+\n@@ -2650,4 +2787,73 @@\n-  do_oop_load(_masm, field, r0, IN_HEAP);\n-  __ push(atos);\n-  if (rc == may_rewrite) {\n-    patch_bytecode(Bytecodes::_fast_agetfield, bc, r1);\n+  if (!EnableValhalla) {\n+    do_oop_load(_masm, field, r0, IN_HEAP);\n+    __ push(atos);\n+    if (rc == may_rewrite) {\n+      patch_bytecode(Bytecodes::_fast_agetfield, bc, r1);\n+    }\n+    __ b(Done);\n+  } else { \/\/ Valhalla\n+    if (is_static) {\n+      __ load_heap_oop(r0, field, rscratch1, rscratch2);\n+      Label is_null_free_inline_type, uninitialized;\n+      \/\/ Issue below if the static field has not been initialized yet\n+      __ test_field_is_null_free_inline_type(flags, noreg \/*temp*\/, is_null_free_inline_type);\n+        \/\/ field is not a null free inline type\n+        __ push(atos);\n+        __ b(Done);\n+      \/\/ field is a null free inline type, must not return null even if uninitialized\n+      __ bind(is_null_free_inline_type);\n+        __ cbz(r0, uninitialized);\n+          __ push(atos);\n+          __ b(Done);\n+        __ bind(uninitialized);\n+          Label slow_case, finish;\n+          __ ldrb(rscratch1, Address(klass, InstanceKlass::init_state_offset()));\n+          __ cmp(rscratch1, (u1)InstanceKlass::fully_initialized);\n+          __ br(Assembler::NE, slow_case);\n+          __ get_default_value_oop(klass, off \/* temp *\/, r0);\n+        __ b(finish);\n+        __ bind(slow_case);\n+          __ call_VM(r0, CAST_FROM_FN_PTR(address, InterpreterRuntime::uninitialized_static_inline_type_field), obj, cache);\n+          __ bind(finish);\n+          __ verify_oop(r0);\n+          __ push(atos);\n+          __ b(Done);\n+    } else {\n+      Label is_flat, nonnull, is_inline_type, has_null_marker, rewrite_inline;\n+      __ test_field_is_null_free_inline_type(flags, noreg \/*temp*\/, is_inline_type);\n+      __ test_field_has_null_marker(flags, noreg \/*temp*\/, has_null_marker);\n+        \/\/ Non-inline field case\n+        __ load_heap_oop(r0, field, rscratch1, rscratch2);\n+        __ push(atos);\n+        if (rc == may_rewrite) {\n+          patch_bytecode(Bytecodes::_fast_agetfield, bc, r1);\n+        }\n+        __ b(Done);\n+      __ bind(is_inline_type);\n+        __ test_field_is_flat(flags, noreg \/* temp *\/, is_flat);\n+         \/\/ field is not flat\n+          __ load_heap_oop(r0, field, rscratch1, rscratch2);\n+          __ cbnz(r0, nonnull);\n+            __ get_inline_type_field_klass(klass, field_index, inline_klass);\n+            __ get_default_value_oop(inline_klass, klass \/* temp *\/, r0);\n+          __ bind(nonnull);\n+          __ verify_oop(r0);\n+          __ push(atos);\n+          __ b(rewrite_inline);\n+        __ bind(is_flat);\n+        \/\/ field is flat\n+          __ mov(r0, obj);\n+          __ read_flat_field(klass, field_index, off, inline_klass \/* temp *\/, r0);\n+          __ verify_oop(r0);\n+          __ push(atos);\n+          __ b(rewrite_inline);\n+        __ bind(has_null_marker);\n+          call_VM(r0, CAST_FROM_FN_PTR(address, InterpreterRuntime::read_nullable_flat_field), obj, cache);\n+          __ verify_oop(r0);\n+          __ push(atos);\n+      __ bind(rewrite_inline);\n+      if (rc == may_rewrite) {\n+        patch_bytecode(Bytecodes::_fast_vgetfield, bc, r1);\n+      }\n+      __ b(Done);\n+    }\n@@ -2655,1 +2861,0 @@\n-  __ b(Done);\n@@ -2816,1 +3021,1 @@\n-  const Register flags     = r0;\n+  const Register flags     = r6;\n@@ -2818,0 +3023,1 @@\n+  const Register inline_klass = r5;\n@@ -2824,2 +3030,0 @@\n-  __ mov(r5, flags);\n-\n@@ -2828,1 +3032,1 @@\n-    __ tbz(r5, ResolvedFieldEntry::is_volatile_shift, notVolatile);\n+    __ tbz(flags, ResolvedFieldEntry::is_volatile_shift, notVolatile);\n@@ -2877,8 +3081,60 @@\n-    __ pop(atos);\n-    if (!is_static) pop_and_check_object(obj);\n-    \/\/ Store into the field\n-    do_oop_store(_masm, field, r0, IN_HEAP);\n-    if (rc == may_rewrite) {\n-      patch_bytecode(Bytecodes::_fast_aputfield, bc, r1, true, byte_no);\n-    }\n-    __ b(Done);\n+     if (!EnableValhalla) {\n+      __ pop(atos);\n+      if (!is_static) pop_and_check_object(obj);\n+      \/\/ Store into the field\n+      do_oop_store(_masm, field, r0, IN_HEAP);\n+      if (rc == may_rewrite) {\n+        patch_bytecode(Bytecodes::_fast_aputfield, bc, r1, true, byte_no);\n+      }\n+      __ b(Done);\n+     } else { \/\/ Valhalla\n+      __ pop(atos);\n+      if (is_static) {\n+        Label is_inline_type;\n+         __ test_field_is_not_null_free_inline_type(flags, noreg \/* temp *\/, is_inline_type);\n+         __ null_check(r0);\n+         __ bind(is_inline_type);\n+         do_oop_store(_masm, field, r0, IN_HEAP);\n+         __ b(Done);\n+      } else {\n+        Label is_inline_type, is_flat, has_null_marker, rewrite_not_inline, rewrite_inline;\n+        __ test_field_is_null_free_inline_type(flags, noreg \/*temp*\/, is_inline_type);\n+        __ test_field_has_null_marker(flags, noreg \/*temp*\/, has_null_marker);\n+        \/\/ Not an inline type\n+        pop_and_check_object(obj);\n+        \/\/ Store into the field\n+        do_oop_store(_masm, field, r0, IN_HEAP);\n+        __ bind(rewrite_not_inline);\n+        if (rc == may_rewrite) {\n+          patch_bytecode(Bytecodes::_fast_aputfield, bc, r19, true, byte_no);\n+        }\n+        __ b(Done);\n+        \/\/ Implementation of the inline type semantic\n+        __ bind(is_inline_type);\n+        __ null_check(r0);\n+        __ test_field_is_flat(flags, noreg \/*temp*\/, is_flat);\n+        \/\/ field is not flat\n+        pop_and_check_object(obj);\n+        \/\/ Store into the field\n+        do_oop_store(_masm, field, r0, IN_HEAP);\n+        __ b(rewrite_inline);\n+        __ bind(is_flat);\n+        \/\/ field is flat\n+        pop_and_check_object(obj);\n+        assert_different_registers(r0, inline_klass, obj, off);\n+        __ load_klass(inline_klass, r0);\n+        __ data_for_oop(r0, r0, inline_klass);\n+        __ add(obj, obj, off);\n+        __ access_value_copy(IN_HEAP, r0, obj, inline_klass);\n+        __ b(rewrite_inline);\n+        __ bind(has_null_marker);\n+        assert_different_registers(r0, cache, r19);\n+        pop_and_check_object(r19);\n+        __ call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::write_nullable_flat_field), r19, r0, cache);\n+        __ bind(rewrite_inline);\n+        if (rc == may_rewrite) {\n+          patch_bytecode(Bytecodes::_fast_vputfield, bc, r19, true, byte_no);\n+        }\n+        __ b(Done);\n+      }\n+     }  \/\/ Valhalla\n@@ -2989,1 +3245,1 @@\n-    __ tbz(r5, ResolvedFieldEntry::is_volatile_shift, notVolatile);\n+    __ tbz(flags, ResolvedFieldEntry::is_volatile_shift, notVolatile);\n@@ -3023,0 +3279,1 @@\n+    case Bytecodes::_fast_vputfield: \/\/fall through\n@@ -3049,0 +3306,1 @@\n+    case Bytecodes::_fast_vputfield: \/\/fall through\n@@ -3095,0 +3353,23 @@\n+  case Bytecodes::_fast_vputfield:\n+   {\n+      Label is_flat, has_null_marker, done;\n+      __ test_field_has_null_marker(r3, noreg \/* temp *\/, has_null_marker);\n+      __ null_check(r0);\n+      __ test_field_is_flat(r3, noreg \/* temp *\/, is_flat);\n+      \/\/ field is not flat\n+      do_oop_store(_masm, field, r0, IN_HEAP);\n+      __ b(done);\n+      __ bind(is_flat);\n+      \/\/ field is flat\n+      __ load_klass(r4, r0);\n+      __ data_for_oop(r0, r0, r4);\n+      __ lea(rscratch1, field);\n+      __ access_value_copy(IN_HEAP, r0, rscratch1, r4);\n+      __ b(done);\n+      __ bind(has_null_marker);\n+      __ load_field_entry(r4, r1);\n+      __ mov(r1, r2);\n+      __ call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::write_nullable_flat_field), r1, r0, r4);\n+      __ bind(done);\n+    }\n+    break;\n@@ -3187,0 +3468,29 @@\n+  case Bytecodes::_fast_vgetfield:\n+    {\n+      Register index = r4, klass = r5, inline_klass = r6, tmp = r7;\n+      Label is_flat, has_null_marker, nonnull, Done;\n+      __ test_field_has_null_marker(r3, noreg \/*temp*\/, has_null_marker);\n+      __ test_field_is_flat(r3, noreg \/* temp *\/, is_flat);\n+        \/\/ field is not flat\n+        __ load_heap_oop(r0, field, rscratch1, rscratch2);\n+        __ cbnz(r0, nonnull);\n+          __ load_unsigned_short(index, Address(r2, in_bytes(ResolvedFieldEntry::field_index_offset())));\n+          __ ldr(klass, Address(r2, in_bytes(ResolvedFieldEntry::field_holder_offset())));\n+          __ get_inline_type_field_klass(klass, index, inline_klass);\n+          __ get_default_value_oop(inline_klass, tmp \/* temp *\/, r0);\n+        __ bind(nonnull);\n+        __ verify_oop(r0);\n+        __ b(Done);\n+      __ bind(is_flat);\n+      \/\/ field is flat\n+        __ load_unsigned_short(index, Address(r2, in_bytes(ResolvedFieldEntry::field_index_offset())));\n+        __ ldr(klass, Address(r2, in_bytes(ResolvedFieldEntry::field_holder_offset())));\n+        __ read_flat_field(klass, index, r1, tmp \/* temp *\/, r0);\n+        __ verify_oop(r0);\n+        __ b(Done);\n+      __ bind(has_null_marker);\n+        call_VM(r0, CAST_FROM_FN_PTR(address, InterpreterRuntime::read_nullable_flat_field), r0, r2);\n+        __ verify_oop(r0);\n+      __ bind(Done);\n+    }\n+    break;\n@@ -3605,46 +3915,1 @@\n-  \/\/ get instance_size in InstanceKlass (scaled to a count of bytes)\n-  __ ldrw(r3,\n-          Address(r4,\n-                  Klass::layout_helper_offset()));\n-  \/\/ test to see if it is malformed in some way\n-  __ tbnz(r3, exact_log2(Klass::_lh_instance_slow_path_bit), slow_case);\n-\n-  \/\/ Allocate the instance:\n-  \/\/  If TLAB is enabled:\n-  \/\/    Try to allocate in the TLAB.\n-  \/\/    If fails, go to the slow path.\n-  \/\/    Initialize the allocation.\n-  \/\/    Exit.\n-  \/\/\n-  \/\/  Go to slow path.\n-\n-  if (UseTLAB) {\n-    __ tlab_allocate(r0, r3, 0, noreg, r1, slow_case);\n-\n-    if (ZeroTLAB) {\n-      \/\/ the fields have been already cleared\n-      __ b(initialize_header);\n-    }\n-\n-    \/\/ The object is initialized before the header.  If the object size is\n-    \/\/ zero, go directly to the header initialization.\n-    __ sub(r3, r3, sizeof(oopDesc));\n-    __ cbz(r3, initialize_header);\n-\n-    \/\/ Initialize object fields\n-    {\n-      __ add(r2, r0, sizeof(oopDesc));\n-      Label loop;\n-      __ bind(loop);\n-      __ str(zr, Address(__ post(r2, BytesPerLong)));\n-      __ sub(r3, r3, BytesPerLong);\n-      __ cbnz(r3, loop);\n-    }\n-\n-    \/\/ initialize object header only.\n-    __ bind(initialize_header);\n-    __ mov(rscratch1, (intptr_t)markWord::prototype().value());\n-    __ str(rscratch1, Address(r0, oopDesc::mark_offset_in_bytes()));\n-    __ store_klass_gap(r0, zr);  \/\/ zero klass gap for compressed oops\n-    __ store_klass(r0, r4);      \/\/ store klass last\n-\n+  __ allocate_instance(r4, r0, r3, r1, true, slow_case);\n@@ -3659,2 +3924,1 @@\n-    __ b(done);\n-  }\n+  __ b(done);\n@@ -3745,0 +4009,3 @@\n+  __ b(done);\n+  __ bind(is_null);\n+\n@@ -3747,4 +4014,1 @@\n-    __ b(done);\n-    __ bind(is_null);\n-  } else {\n-    __ bind(is_null);   \/\/ same as 'done'\n+\n@@ -3871,0 +4135,4 @@\n+  Label is_inline_type;\n+  __ ldr(rscratch1, Address(r0, oopDesc::mark_offset_in_bytes()));\n+  __ test_markword_is_inline_type(rscratch1, is_inline_type);\n+\n@@ -3972,0 +4240,5 @@\n+\n+  __ bind(is_inline_type);\n+  __ call_VM(noreg, CAST_FROM_FN_PTR(address,\n+                    InterpreterRuntime::throw_identity_exception), r0);\n+  __ should_not_reach_here();\n@@ -3982,0 +4255,12 @@\n+  const int is_inline_type_mask = markWord::inline_type_pattern;\n+  Label has_identity;\n+  __ ldr(rscratch1, Address(r0, oopDesc::mark_offset_in_bytes()));\n+  __ mov(rscratch2, is_inline_type_mask);\n+  __ andr(rscratch1, rscratch1, rscratch2);\n+  __ cmp(rscratch1, rscratch2);\n+  __ br(Assembler::NE, has_identity);\n+  __ call_VM(noreg, CAST_FROM_FN_PTR(address,\n+                     InterpreterRuntime::throw_illegal_monitor_state_exception));\n+  __ should_not_reach_here();\n+  __ bind(has_identity);\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/templateTable_aarch64.cpp","additions":378,"deletions":93,"binary":false,"changes":471,"status":"modified"},{"patch":"@@ -1840,1 +1840,1 @@\n-        assert(ReturnTypeEntry::static_cell_count() < TypeStackSlotEntries::per_arg_count(),\n+        assert(SingleTypeEntry::static_cell_count() < TypeStackSlotEntries::per_arg_count(),\n@@ -1881,1 +1881,1 @@\n-    profile_obj_type(ret, R28_mdx, -in_bytes(ReturnTypeEntry::size()), tmp1, tmp2);\n+    profile_obj_type(ret, R28_mdx, -in_bytes(SingleTypeEntry::size()), tmp1, tmp2);\n","filename":"src\/hotspot\/cpu\/ppc\/interp_masm_ppc_64.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -1766,1 +1766,1 @@\n-        assert(ReturnTypeEntry::static_cell_count() < TypeStackSlotEntries::per_arg_count(), \"can't move past ret type\");\n+        assert(SingleTypeEntry::static_cell_count() < TypeStackSlotEntries::per_arg_count(), \"can't move past ret type\");\n@@ -1815,1 +1815,1 @@\n-    Address mdo_ret_addr(mdp, -in_bytes(ReturnTypeEntry::size()));\n+    Address mdo_ret_addr(mdp, -in_bytes(SingleTypeEntry::size()));\n","filename":"src\/hotspot\/cpu\/s390\/interp_masm_s390.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -38,0 +38,1 @@\n+#include \"runtime\/frame.inline.hpp\"\n@@ -81,0 +82,4 @@\n+    if (EnableValhalla) {\n+      \/\/ Mask inline_type bit such that we go to the slow path if object is an inline type\n+      andptr(hdr, ~((int) markWord::inline_type_bit_in_place));\n+    }\n@@ -174,1 +179,9 @@\n-  movptr(Address(obj, oopDesc::mark_offset_in_bytes()), checked_cast<int32_t>(markWord::prototype().value()));\n+  if (EnableValhalla) {\n+    \/\/ Need to copy markWord::prototype header for klass\n+    assert_different_registers(obj, klass, len, t1, t2);\n+    movptr(t1, Address(klass, Klass::prototype_header_offset()));\n+    movptr(Address(obj, oopDesc::mark_offset_in_bytes()), t1);\n+  } else {\n+    \/\/ This assumes that all prototype bits fit in an int32_t\n+    movptr(Address(obj, oopDesc::mark_offset_in_bytes()), checked_cast<int32_t>(markWord::prototype().value()));\n+  }\n@@ -318,9 +331,1 @@\n-void C1_MacroAssembler::build_frame(int frame_size_in_bytes, int bang_size_in_bytes) {\n-  assert(bang_size_in_bytes >= frame_size_in_bytes, \"stack bang size incorrect\");\n-  \/\/ Make sure there is enough stack space for this method's activation.\n-  \/\/ Note that we do this before doing an enter(). This matches the\n-  \/\/ ordering of C2's stack overflow check \/ rsp decrement and allows\n-  \/\/ the SharedRuntime stack overflow handling to be consistent\n-  \/\/ between the two compilers.\n-  generate_stack_overflow_check(bang_size_in_bytes);\n-\n+void C1_MacroAssembler::build_frame_helper(int frame_size_in_bytes, int sp_offset_for_orig_pc, int sp_inc, bool reset_orig_pc, bool needs_stack_repair) {\n@@ -333,3 +338,3 @@\n-    \/\/ c2 leaves fpu stack dirty. Clean it on entry\n-    empty_FPU_stack();\n-  }\n+      \/\/ c2 leaves fpu stack dirty. Clean it on entry\n+      empty_FPU_stack();\n+    }\n@@ -337,1 +342,24 @@\n-  decrement(rsp, frame_size_in_bytes); \/\/ does not emit code for frame_size == 0\n+  decrement(rsp, frame_size_in_bytes);\n+\n+  if (needs_stack_repair) {\n+    \/\/ Save stack increment (also account for fixed framesize and rbp)\n+    assert((sp_inc & (StackAlignmentInBytes-1)) == 0, \"stack increment not aligned\");\n+    int real_frame_size = sp_inc + frame_size_in_bytes + wordSize;\n+    movptr(Address(rsp, frame_size_in_bytes - wordSize), real_frame_size);\n+  }\n+  if (reset_orig_pc) {\n+    \/\/ Zero orig_pc to detect deoptimization during buffering in the entry points\n+    movptr(Address(rsp, sp_offset_for_orig_pc), 0);\n+  }\n+}\n+\n+void C1_MacroAssembler::build_frame(int frame_size_in_bytes, int bang_size_in_bytes, int sp_offset_for_orig_pc, bool needs_stack_repair, bool has_scalarized_args, Label* verified_inline_entry_label) {\n+  \/\/ Make sure there is enough stack space for this method's activation.\n+  \/\/ Note that we do this before doing an enter(). This matches the\n+  \/\/ ordering of C2's stack overflow check \/ rsp decrement and allows\n+  \/\/ the SharedRuntime stack overflow handling to be consistent\n+  \/\/ between the two compilers.\n+  assert(bang_size_in_bytes >= frame_size_in_bytes, \"stack bang size incorrect\");\n+  generate_stack_overflow_check(bang_size_in_bytes);\n+\n+  build_frame_helper(frame_size_in_bytes, sp_offset_for_orig_pc, 0, has_scalarized_args, needs_stack_repair);\n@@ -342,5 +370,4 @@\n-}\n-\n-void C1_MacroAssembler::remove_frame(int frame_size_in_bytes) {\n-  increment(rsp, frame_size_in_bytes);  \/\/ Does not emit code for frame_size == 0\n-  pop(rbp);\n+  if (verified_inline_entry_label != nullptr) {\n+    \/\/ Jump here from the scalarized entry points that already created the frame.\n+    bind(*verified_inline_entry_label);\n+  }\n@@ -350,1 +377,0 @@\n-\n@@ -367,0 +393,58 @@\n+int C1_MacroAssembler::scalarized_entry(const CompiledEntrySignature* ces, int frame_size_in_bytes, int bang_size_in_bytes, int sp_offset_for_orig_pc, Label& verified_inline_entry_label, bool is_inline_ro_entry) {\n+  assert(InlineTypePassFieldsAsArgs, \"sanity\");\n+  \/\/ Make sure there is enough stack space for this method's activation.\n+  assert(bang_size_in_bytes >= frame_size_in_bytes, \"stack bang size incorrect\");\n+  generate_stack_overflow_check(bang_size_in_bytes);\n+\n+  GrowableArray<SigEntry>* sig    = ces->sig();\n+  GrowableArray<SigEntry>* sig_cc = is_inline_ro_entry ? ces->sig_cc_ro() : ces->sig_cc();\n+  VMRegPair* regs      = ces->regs();\n+  VMRegPair* regs_cc   = is_inline_ro_entry ? ces->regs_cc_ro() : ces->regs_cc();\n+  int args_on_stack    = ces->args_on_stack();\n+  int args_on_stack_cc = is_inline_ro_entry ? ces->args_on_stack_cc_ro() : ces->args_on_stack_cc();\n+\n+  assert(sig->length() <= sig_cc->length(), \"Zero-sized inline class not allowed!\");\n+  BasicType* sig_bt = NEW_RESOURCE_ARRAY(BasicType, sig_cc->length());\n+  int args_passed = sig->length();\n+  int args_passed_cc = SigEntry::fill_sig_bt(sig_cc, sig_bt);\n+\n+  \/\/ Create a temp frame so we can call into the runtime. It must be properly set up to accommodate GC.\n+  build_frame_helper(frame_size_in_bytes, sp_offset_for_orig_pc, 0, true, ces->c1_needs_stack_repair());\n+\n+  \/\/ The runtime call might safepoint, make sure nmethod entry barrier is executed\n+  BarrierSetAssembler* bs = BarrierSet::barrier_set()->barrier_set_assembler();\n+  \/\/ C1 code is not hot enough to micro optimize the nmethod entry barrier with an out-of-line stub\n+  bs->nmethod_entry_barrier(this, nullptr \/* slow_path *\/, nullptr \/* continuation *\/);\n+\n+  \/\/ FIXME -- call runtime only if we cannot in-line allocate all the incoming inline type args.\n+  movptr(rbx, (intptr_t)(ces->method()));\n+  if (is_inline_ro_entry) {\n+    call(RuntimeAddress(Runtime1::entry_for(Runtime1::buffer_inline_args_no_receiver_id)));\n+  } else {\n+    call(RuntimeAddress(Runtime1::entry_for(Runtime1::buffer_inline_args_id)));\n+  }\n+  int rt_call_offset = offset();\n+\n+  \/\/ Remove the temp frame\n+  addptr(rsp, frame_size_in_bytes);\n+  pop(rbp);\n+\n+  \/\/ Check if we need to extend the stack for packing\n+  int sp_inc = 0;\n+  if (args_on_stack > args_on_stack_cc) {\n+    sp_inc = extend_stack_for_inline_args(args_on_stack);\n+  }\n+\n+  shuffle_inline_args(true, is_inline_ro_entry, sig_cc,\n+                      args_passed_cc, args_on_stack_cc, regs_cc, \/\/ from\n+                      args_passed, args_on_stack, regs,          \/\/ to\n+                      sp_inc, rax);\n+\n+  \/\/ Create the real frame. Below jump will then skip over the stack banging and frame\n+  \/\/ setup code in the verified_inline_entry (which has a different real_frame_size).\n+  build_frame_helper(frame_size_in_bytes, sp_offset_for_orig_pc, sp_inc, false, ces->c1_needs_stack_repair());\n+\n+  jmp(verified_inline_entry_label);\n+  return rt_call_offset;\n+}\n+\n","filename":"src\/hotspot\/cpu\/x86\/c1_MacroAssembler_x86.cpp","additions":104,"deletions":20,"binary":false,"changes":124,"status":"modified"},{"patch":"@@ -1048,1 +1048,2 @@\n-        int call_offset = __ call_RT(obj, noreg, CAST_FROM_FN_PTR(address, new_instance), klass);\n+        int call_offset;\n+        call_offset = __ call_RT(obj, noreg, CAST_FROM_FN_PTR(address, new_instance), klass);\n@@ -1081,0 +1082,1 @@\n+    case new_null_free_array_id:\n@@ -1088,1 +1090,1 @@\n-        } else {\n+        } else if (id == new_object_array_id) {\n@@ -1090,0 +1092,2 @@\n+        } else {\n+          __ set_info(\"new_null_free_array\", dont_gc_arguments);\n@@ -1099,6 +1103,22 @@\n-          int tag = ((id == new_type_array_id)\n-                     ? Klass::_lh_array_tag_type_value\n-                     : Klass::_lh_array_tag_obj_value);\n-          __ cmpl(t0, tag);\n-          __ jcc(Assembler::equal, ok);\n-          __ stop(\"assert(is an array klass)\");\n+          switch (id) {\n+          case new_type_array_id:\n+            __ cmpl(t0, Klass::_lh_array_tag_type_value);\n+            __ jcc(Assembler::equal, ok);\n+            __ stop(\"assert(is a type array klass)\");\n+            break;\n+          case new_object_array_id:\n+            __ cmpl(t0, Klass::_lh_array_tag_obj_value); \/\/ new \"[Ljava\/lang\/Object;\"\n+            __ jcc(Assembler::equal, ok);\n+            __ cmpl(t0, Klass::_lh_array_tag_vt_value);  \/\/ new \"[LVT;\"\n+            __ jcc(Assembler::equal, ok);\n+            __ stop(\"assert(is an object or inline type array klass)\");\n+            break;\n+          case new_null_free_array_id:\n+            __ cmpl(t0, Klass::_lh_array_tag_vt_value);  \/\/ the array can be a flat array.\n+            __ jcc(Assembler::equal, ok);\n+            __ cmpl(t0, Klass::_lh_array_tag_obj_value); \/\/ the array cannot be a flat array (due to InlineArrayElementMaxFlatSize, etc)\n+            __ jcc(Assembler::equal, ok);\n+            __ stop(\"assert(is an object or inline type array klass)\");\n+            break;\n+          default:  ShouldNotReachHere();\n+          }\n@@ -1115,1 +1135,1 @@\n-        } else {\n+        } else if (id == new_object_array_id) {\n@@ -1117,0 +1137,3 @@\n+        } else {\n+          assert(id == new_null_free_array_id, \"must be\");\n+          call_offset = __ call_RT(obj, noreg, CAST_FROM_FN_PTR(address, new_null_free_array), klass, length);\n@@ -1148,0 +1171,77 @@\n+    case load_flat_array_id:\n+      {\n+        StubFrame f(sasm, \"load_flat_array\", dont_gc_arguments);\n+        OopMap* map = save_live_registers(sasm, 3);\n+\n+        \/\/ Called with store_parameter and not C abi\n+\n+        f.load_argument(1, rax); \/\/ rax,: array\n+        f.load_argument(0, rbx); \/\/ rbx,: index\n+        int call_offset = __ call_RT(rax, noreg, CAST_FROM_FN_PTR(address, load_flat_array), rax, rbx);\n+\n+        oop_maps = new OopMapSet();\n+        oop_maps->add_gc_map(call_offset, map);\n+        restore_live_registers_except_rax(sasm);\n+\n+        \/\/ rax,: loaded element at array[index]\n+        __ verify_oop(rax);\n+      }\n+      break;\n+\n+    case store_flat_array_id:\n+      {\n+        StubFrame f(sasm, \"store_flat_array\", dont_gc_arguments);\n+        OopMap* map = save_live_registers(sasm, 4);\n+\n+        \/\/ Called with store_parameter and not C abi\n+\n+        f.load_argument(2, rax); \/\/ rax,: array\n+        f.load_argument(1, rbx); \/\/ rbx,: index\n+        f.load_argument(0, rcx); \/\/ rcx,: value\n+        int call_offset = __ call_RT(noreg, noreg, CAST_FROM_FN_PTR(address, store_flat_array), rax, rbx, rcx);\n+\n+        oop_maps = new OopMapSet();\n+        oop_maps->add_gc_map(call_offset, map);\n+        restore_live_registers_except_rax(sasm);\n+      }\n+      break;\n+\n+    case substitutability_check_id:\n+      {\n+        StubFrame f(sasm, \"substitutability_check\", dont_gc_arguments);\n+        OopMap* map = save_live_registers(sasm, 3);\n+\n+        \/\/ Called with store_parameter and not C abi\n+\n+        f.load_argument(1, rax); \/\/ rax,: left\n+        f.load_argument(0, rbx); \/\/ rbx,: right\n+        int call_offset = __ call_RT(noreg, noreg, CAST_FROM_FN_PTR(address, substitutability_check), rax, rbx);\n+\n+        oop_maps = new OopMapSet();\n+        oop_maps->add_gc_map(call_offset, map);\n+        restore_live_registers_except_rax(sasm);\n+\n+        \/\/ rax,: are the two operands substitutable\n+      }\n+      break;\n+\n+\n+    case buffer_inline_args_id:\n+    case buffer_inline_args_no_receiver_id:\n+      {\n+        const char* name = (id == buffer_inline_args_id) ?\n+          \"buffer_inline_args\" : \"buffer_inline_args_no_receiver\";\n+        StubFrame f(sasm, name, dont_gc_arguments);\n+        OopMap* map = save_live_registers(sasm, 2);\n+        Register method = rbx;\n+        address entry = (id == buffer_inline_args_id) ?\n+          CAST_FROM_FN_PTR(address, buffer_inline_args) :\n+          CAST_FROM_FN_PTR(address, buffer_inline_args_no_receiver);\n+        int call_offset = __ call_RT(rax, noreg, entry, method);\n+        oop_maps = new OopMapSet();\n+        oop_maps->add_gc_map(call_offset, map);\n+        restore_live_registers_except_rax(sasm);\n+        __ verify_oop(rax);  \/\/ rax: an array of buffered value objects\n+      }\n+      break;\n+\n@@ -1248,1 +1348,1 @@\n-      { StubFrame f(sasm, \"throw_incompatible_class_cast_exception\", dont_gc_arguments);\n+      { StubFrame f(sasm, \"throw_incompatible_class_change_error\", dont_gc_arguments);\n@@ -1253,0 +1353,12 @@\n+    case throw_illegal_monitor_state_exception_id:\n+      { StubFrame f(sasm, \"throw_illegal_monitor_state_exception\", dont_gc_arguments);\n+        oop_maps = generate_exception_throw(sasm, CAST_FROM_FN_PTR(address, throw_illegal_monitor_state_exception), false);\n+      }\n+      break;\n+\n+    case throw_identity_exception_id:\n+      { StubFrame f(sasm, \"throw_identity_exception\", dont_gc_arguments);\n+        oop_maps = generate_exception_throw(sasm, CAST_FROM_FN_PTR(address, throw_identity_exception), true);\n+      }\n+      break;\n+\n","filename":"src\/hotspot\/cpu\/x86\/c1_Runtime1_x86.cpp","additions":122,"deletions":10,"binary":false,"changes":132,"status":"modified"},{"patch":"@@ -53,1 +53,20 @@\n-void C2_MacroAssembler::verified_entry(int framesize, int stack_bang_size, bool fp_mode_24b, bool is_stub) {\n+void C2_MacroAssembler::verified_entry(Compile* C, int sp_inc) {\n+  if (C->clinit_barrier_on_entry()) {\n+    assert(VM_Version::supports_fast_class_init_checks(), \"sanity\");\n+    assert(!C->method()->holder()->is_not_initialized(), \"initialization should have been started\");\n+\n+    Label L_skip_barrier;\n+    Register klass = rscratch1;\n+\n+    mov_metadata(klass, C->method()->holder()->constant_encoding());\n+    clinit_barrier(klass, r15_thread, &L_skip_barrier \/*L_fast_path*\/);\n+\n+    jump(RuntimeAddress(SharedRuntime::get_handle_wrong_method_stub())); \/\/ slow path\n+\n+    bind(L_skip_barrier);\n+  }\n+\n+  int framesize = C->output()->frame_size_in_bytes();\n+  int bangsize = C->output()->bang_size_in_bytes();\n+  bool fp_mode_24b = false;\n+  int stack_bang_size = C->output()->need_stack_bang(bangsize) ? bangsize : 0;\n@@ -106,0 +125,6 @@\n+  if (C->needs_stack_repair()) {\n+    \/\/ Save stack increment just below the saved rbp (also account for fixed framesize and rbp)\n+    assert((sp_inc & (StackAlignmentInBytes-1)) == 0, \"stack increment not aligned\");\n+    movptr(Address(rsp, framesize - wordSize), sp_inc + framesize + wordSize);\n+  }\n+\n@@ -134,0 +159,1 @@\n+}\n@@ -135,17 +161,15 @@\n-  if (!is_stub) {\n-    BarrierSetAssembler* bs = BarrierSet::barrier_set()->barrier_set_assembler();\n- #ifdef _LP64\n-    if (BarrierSet::barrier_set()->barrier_set_nmethod() != nullptr) {\n-      \/\/ We put the non-hot code of the nmethod entry barrier out-of-line in a stub.\n-      Label dummy_slow_path;\n-      Label dummy_continuation;\n-      Label* slow_path = &dummy_slow_path;\n-      Label* continuation = &dummy_continuation;\n-      if (!Compile::current()->output()->in_scratch_emit_size()) {\n-        \/\/ Use real labels from actual stub when not emitting code for the purpose of measuring its size\n-        C2EntryBarrierStub* stub = new (Compile::current()->comp_arena()) C2EntryBarrierStub();\n-        Compile::current()->output()->add_stub(stub);\n-        slow_path = &stub->entry();\n-        continuation = &stub->continuation();\n-      }\n-      bs->nmethod_entry_barrier(this, slow_path, continuation);\n+void C2_MacroAssembler::entry_barrier() {\n+  BarrierSetAssembler* bs = BarrierSet::barrier_set()->barrier_set_assembler();\n+#ifdef _LP64\n+  if (BarrierSet::barrier_set()->barrier_set_nmethod() != nullptr) {\n+    \/\/ We put the non-hot code of the nmethod entry barrier out-of-line in a stub.\n+    Label dummy_slow_path;\n+    Label dummy_continuation;\n+    Label* slow_path = &dummy_slow_path;\n+    Label* continuation = &dummy_continuation;\n+    if (!Compile::current()->output()->in_scratch_emit_size()) {\n+      \/\/ Use real labels from actual stub when not emitting code for the purpose of measuring its size\n+      C2EntryBarrierStub* stub = new (Compile::current()->comp_arena()) C2EntryBarrierStub();\n+      Compile::current()->output()->add_stub(stub);\n+      slow_path = &stub->entry();\n+      continuation = &stub->continuation();\n@@ -153,0 +177,2 @@\n+    bs->nmethod_entry_barrier(this, slow_path, continuation);\n+  }\n@@ -154,2 +180,2 @@\n-    \/\/ Don't bother with out-of-line nmethod entry barrier stub for x86_32.\n-    bs->nmethod_entry_barrier(this, nullptr \/* slow_path *\/, nullptr \/* continuation *\/);\n+  \/\/ Don't bother with out-of-line nmethod entry barrier stub for x86_32.\n+  bs->nmethod_entry_barrier(this, nullptr \/* slow_path *\/, nullptr \/* continuation *\/);\n@@ -157,1 +183,0 @@\n-  }\n@@ -295,0 +320,4 @@\n+    if (EnableValhalla) {\n+      \/\/ Mask inline_type bit such that we go to the slow path if object is an inline type\n+      andptr(tmpReg, ~((int) markWord::inline_type_bit_in_place));\n+    }\n","filename":"src\/hotspot\/cpu\/x86\/c2_MacroAssembler_x86.cpp","additions":50,"deletions":21,"binary":false,"changes":71,"status":"modified"},{"patch":"@@ -32,0 +32,1 @@\n+#include \"oops\/constMethodFlags.hpp\"\n@@ -35,0 +36,1 @@\n+#include \"oops\/inlineKlass.hpp\"\n@@ -176,1 +178,1 @@\n-        assert(ReturnTypeEntry::static_cell_count() < TypeStackSlotEntries::per_arg_count(), \"can't move past ret type\");\n+        assert(SingleTypeEntry::static_cell_count() < TypeStackSlotEntries::per_arg_count(), \"can't move past ret type\");\n@@ -221,1 +223,1 @@\n-    Address mdo_ret_addr(mdp, -in_bytes(ReturnTypeEntry::size()));\n+    Address mdo_ret_addr(mdp, -in_bytes(SingleTypeEntry::size()));\n@@ -511,1 +513,2 @@\n-                                                  Label& ok_is_subtype) {\n+                                                  Label& ok_is_subtype,\n+                                                  bool profile) {\n@@ -519,1 +522,3 @@\n-  profile_typecheck(rcx, Rsub_klass, rdi); \/\/ blows rcx, reloads rdi\n+  if (profile) {\n+    profile_typecheck(rcx, Rsub_klass, rdi); \/\/ blows rcx, reloads rdi\n+  }\n@@ -971,1 +976,1 @@\n- \/\/ get method access flags\n+  \/\/ get method access flags\n@@ -1097,4 +1102,2 @@\n-  \/\/ remove activation\n-  \/\/ get sender sp\n-  movptr(rbx,\n-         Address(rbp, frame::interpreter_frame_sender_sp_offset * wordSize));\n+    movptr(rbx,\n+               Address(rbp, frame::interpreter_frame_sender_sp_offset * wordSize));\n@@ -1124,0 +1127,40 @@\n+\n+  \/\/ remove activation\n+  \/\/ get sender sp\n+  movptr(rbx,\n+         Address(rbp, frame::interpreter_frame_sender_sp_offset * wordSize));\n+\n+  if (state == atos && InlineTypeReturnedAsFields) {\n+    \/\/ Check if we are returning an non-null inline type and load its fields into registers\n+    Label skip;\n+    test_oop_is_not_inline_type(rax, rscratch1, skip);\n+\n+#ifndef _LP64\n+    super_call_VM_leaf(StubRoutines::load_inline_type_fields_in_regs());\n+#else\n+    \/\/ Load fields from a buffered value with an inline class specific handler\n+    load_klass(rdi, rax, rscratch1);\n+    movptr(rdi, Address(rdi, InstanceKlass::adr_inlineklass_fixed_block_offset()));\n+    movptr(rdi, Address(rdi, InlineKlass::unpack_handler_offset()));\n+    \/\/ Unpack handler can be null if inline type is not scalarizable in returns\n+    testptr(rdi, rdi);\n+    jcc(Assembler::zero, skip);\n+    call(rdi);\n+#endif\n+#ifdef ASSERT\n+    \/\/ TODO 8284443 Enable\n+    if (StressCallingConvention && false) {\n+      Label skip_stress;\n+      movptr(rscratch1, Address(rbp, frame::interpreter_frame_method_offset * wordSize));\n+      movl(rscratch1, Address(rscratch1, Method::flags_offset()));\n+      testl(rcx, MethodFlags::has_scalarized_return_flag());\n+      jcc(Assembler::zero, skip_stress);\n+      load_klass(rax, rax, rscratch1);\n+      orptr(rax, 1);\n+      bind(skip_stress);\n+    }\n+#endif\n+    \/\/ call above kills the value in rbx. Reload it.\n+    movptr(rbx, Address(rbp, frame::interpreter_frame_sender_sp_offset * wordSize));\n+    bind(skip);\n+  }\n@@ -1144,0 +1187,106 @@\n+void InterpreterMacroAssembler::allocate_instance(Register klass, Register new_obj,\n+                                                  Register t1, Register t2,\n+                                                  bool clear_fields, Label& alloc_failed) {\n+  MacroAssembler::allocate_instance(klass, new_obj, t1, t2, clear_fields, alloc_failed);\n+  {\n+    SkipIfEqual skip_if(this, &DTraceAllocProbes, 0, rscratch1);\n+    \/\/ Trigger dtrace event for fastpath\n+    push(atos);\n+    call_VM_leaf(CAST_FROM_FN_PTR(address, static_cast<int (*)(oopDesc*)>(SharedRuntime::dtrace_object_alloc)), new_obj);\n+    pop(atos);\n+  }\n+}\n+\n+\n+void InterpreterMacroAssembler::read_flat_field(Register holder_klass,\n+                                                Register field_index, Register field_offset,\n+                                                Register obj) {\n+  Label alloc_failed, empty_value, done;\n+  const Register src = field_offset;\n+  const Register alloc_temp = LP64_ONLY(rscratch1) NOT_LP64(rsi);\n+  const Register dst_temp   = LP64_ONLY(rscratch2) NOT_LP64(rdi);\n+  assert_different_registers(obj, holder_klass, field_index, field_offset, dst_temp);\n+\n+  \/\/ Grap the inline field klass\n+  push(holder_klass);\n+  const Register field_klass = holder_klass;\n+  get_inline_type_field_klass(holder_klass, field_index, field_klass);\n+\n+  \/\/check for empty value klass\n+  test_klass_is_empty_inline_type(field_klass, dst_temp, empty_value);\n+\n+  \/\/ allocate buffer\n+  push(obj); \/\/ save holder\n+  allocate_instance(field_klass, obj, alloc_temp, dst_temp, false, alloc_failed);\n+\n+  \/\/ Have an oop instance buffer, copy into it\n+  data_for_oop(obj, dst_temp, field_klass);\n+  pop(alloc_temp);             \/\/ restore holder\n+  lea(src, Address(alloc_temp, field_offset));\n+  \/\/ call_VM_leaf, clobbers a few regs, save restore new obj\n+  push(obj);\n+  access_value_copy(IS_DEST_UNINITIALIZED, src, dst_temp, field_klass);\n+  pop(obj);\n+  pop(holder_klass);\n+  jmp(done);\n+\n+  bind(empty_value);\n+  get_empty_inline_type_oop(field_klass, dst_temp, obj);\n+  pop(holder_klass);\n+  jmp(done);\n+\n+  bind(alloc_failed);\n+  pop(obj);\n+  pop(holder_klass);\n+  call_VM(obj, CAST_FROM_FN_PTR(address, InterpreterRuntime::read_flat_field),\n+          obj, field_index, holder_klass);\n+\n+  bind(done);\n+}\n+\n+void InterpreterMacroAssembler::read_flat_element(Register array, Register index,\n+                                                  Register t1, Register t2,\n+                                                  Register obj) {\n+  assert_different_registers(array, index, t1, t2);\n+  Label alloc_failed, empty_value, done;\n+  const Register array_klass = t2;\n+  const Register elem_klass = t1;\n+  const Register alloc_temp = LP64_ONLY(rscratch1) NOT_LP64(rsi);\n+  const Register dst_temp   = LP64_ONLY(rscratch2) NOT_LP64(rdi);\n+\n+  \/\/ load in array->klass()->element_klass()\n+  Register tmp_load_klass = LP64_ONLY(rscratch1) NOT_LP64(noreg);\n+  load_klass(array_klass, array, tmp_load_klass);\n+  movptr(elem_klass, Address(array_klass, ArrayKlass::element_klass_offset()));\n+\n+  \/\/check for empty value klass\n+  test_klass_is_empty_inline_type(elem_klass, dst_temp, empty_value);\n+\n+  \/\/ calc source into \"array_klass\" and free up some regs\n+  const Register src = array_klass;\n+  push(index); \/\/ preserve index reg in case alloc_failed\n+  data_for_value_array_index(array, array_klass, index, src);\n+\n+  allocate_instance(elem_klass, obj, alloc_temp, dst_temp, false, alloc_failed);\n+  \/\/ Have an oop instance buffer, copy into it\n+  store_ptr(0, obj); \/\/ preserve obj (overwrite index, no longer needed)\n+  data_for_oop(obj, dst_temp, elem_klass);\n+  access_value_copy(IS_DEST_UNINITIALIZED, src, dst_temp, elem_klass);\n+  pop(obj);\n+  jmp(done);\n+\n+  bind(empty_value);\n+  get_empty_inline_type_oop(elem_klass, dst_temp, obj);\n+  jmp(done);\n+\n+  bind(alloc_failed);\n+  pop(index);\n+  if (array == c_rarg2) {\n+    mov(elem_klass, array);\n+    array = elem_klass;\n+  }\n+  call_VM(obj, CAST_FROM_FN_PTR(address, InterpreterRuntime::value_array_load), array, index);\n+\n+  bind(done);\n+}\n+\n@@ -1196,0 +1345,4 @@\n+      if (EnableValhalla) {\n+        \/\/ Mask inline_type bit such that we go to the slow path if object is an inline type\n+        andptr(swap_reg, ~((int) markWord::inline_type_bit_in_place));\n+      }\n@@ -1553,1 +1706,1 @@\n-void InterpreterMacroAssembler::profile_not_taken_branch(Register mdp) {\n+void InterpreterMacroAssembler::profile_not_taken_branch(Register mdp, bool acmp) {\n@@ -1565,1 +1718,1 @@\n-    update_mdp_by_constant(mdp, in_bytes(BranchData::branch_data_size()));\n+    update_mdp_by_constant(mdp, acmp ? in_bytes(ACmpData::acmp_data_size()): in_bytes(BranchData::branch_data_size()));\n@@ -1628,1 +1781,1 @@\n-    record_klass_in_profile(receiver, mdp, reg2, true);\n+    record_klass_in_profile(receiver, mdp, reg2);\n@@ -1648,4 +1801,3 @@\n-void InterpreterMacroAssembler::record_klass_in_profile_helper(\n-                                        Register receiver, Register mdp,\n-                                        Register reg2, int start_row,\n-                                        Label& done, bool is_virtual_call) {\n+void InterpreterMacroAssembler::record_klass_in_profile_helper(Register receiver, Register mdp,\n+                                                               Register reg2, int start_row,\n+                                                               Label& done) {\n@@ -1755,3 +1907,1 @@\n-void InterpreterMacroAssembler::record_klass_in_profile(Register receiver,\n-                                                        Register mdp, Register reg2,\n-                                                        bool is_virtual_call) {\n+void InterpreterMacroAssembler::record_klass_in_profile(Register receiver, Register mdp, Register reg2) {\n@@ -1761,1 +1911,1 @@\n-  record_klass_in_profile_helper(receiver, mdp, reg2, 0, done, is_virtual_call);\n+  record_klass_in_profile_helper(receiver, mdp, reg2, 0, done);\n@@ -1838,1 +1988,1 @@\n-      record_klass_in_profile(klass, mdp, reg2, false);\n+      record_klass_in_profile(klass, mdp, reg2);\n@@ -1900,0 +2050,114 @@\n+template <class ArrayData> void InterpreterMacroAssembler::profile_array_type(Register mdp,\n+                                                                              Register array,\n+                                                                              Register tmp) {\n+  if (ProfileInterpreter) {\n+    Label profile_continue;\n+\n+    \/\/ If no method data exists, go to profile_continue.\n+    test_method_data_pointer(mdp, profile_continue);\n+\n+    mov(tmp, array);\n+    profile_obj_type(tmp, Address(mdp, in_bytes(ArrayData::array_offset())));\n+\n+    Label not_flat;\n+    test_non_flat_array_oop(array, tmp, not_flat);\n+\n+    set_mdp_flag_at(mdp, ArrayData::flat_array_byte_constant());\n+\n+    bind(not_flat);\n+\n+    Label not_null_free;\n+    test_non_null_free_array_oop(array, tmp, not_null_free);\n+\n+    set_mdp_flag_at(mdp, ArrayData::null_free_array_byte_constant());\n+\n+    bind(not_null_free);\n+\n+    bind(profile_continue);\n+  }\n+}\n+\n+template void InterpreterMacroAssembler::profile_array_type<ArrayLoadData>(Register mdp,\n+                                                                           Register array,\n+                                                                           Register tmp);\n+template void InterpreterMacroAssembler::profile_array_type<ArrayStoreData>(Register mdp,\n+                                                                            Register array,\n+                                                                            Register tmp);\n+\n+\n+void InterpreterMacroAssembler::profile_multiple_element_types(Register mdp, Register element, Register tmp, const Register tmp2) {\n+  if (ProfileInterpreter) {\n+    Label profile_continue;\n+\n+    \/\/ If no method data exists, go to profile_continue.\n+    test_method_data_pointer(mdp, profile_continue);\n+\n+    Label done, update;\n+    testptr(element, element);\n+    jccb(Assembler::notZero, update);\n+    set_mdp_flag_at(mdp, BitData::null_seen_byte_constant());\n+    jmp(done);\n+\n+    bind(update);\n+    load_klass(tmp, element, rscratch1);\n+\n+    \/\/ Record the object type.\n+    record_klass_in_profile(tmp, mdp, tmp2);\n+\n+    bind(done);\n+\n+    \/\/ The method data pointer needs to be updated.\n+    update_mdp_by_constant(mdp, in_bytes(ArrayStoreData::array_store_data_size()));\n+\n+    bind(profile_continue);\n+  }\n+}\n+\n+void InterpreterMacroAssembler::profile_element_type(Register mdp,\n+                                                     Register element,\n+                                                     Register tmp) {\n+  if (ProfileInterpreter) {\n+    Label profile_continue;\n+\n+    \/\/ If no method data exists, go to profile_continue.\n+    test_method_data_pointer(mdp, profile_continue);\n+\n+    mov(tmp, element);\n+    profile_obj_type(tmp, Address(mdp, in_bytes(ArrayLoadData::element_offset())));\n+\n+    \/\/ The method data pointer needs to be updated.\n+    update_mdp_by_constant(mdp, in_bytes(ArrayLoadData::array_load_data_size()));\n+\n+    bind(profile_continue);\n+  }\n+}\n+\n+void InterpreterMacroAssembler::profile_acmp(Register mdp,\n+                                             Register left,\n+                                             Register right,\n+                                             Register tmp) {\n+  if (ProfileInterpreter) {\n+    Label profile_continue;\n+\n+    \/\/ If no method data exists, go to profile_continue.\n+    test_method_data_pointer(mdp, profile_continue);\n+\n+    mov(tmp, left);\n+    profile_obj_type(tmp, Address(mdp, in_bytes(ACmpData::left_offset())));\n+\n+    Label left_not_inline_type;\n+    test_oop_is_not_inline_type(left, tmp, left_not_inline_type);\n+    set_mdp_flag_at(mdp, ACmpData::left_inline_type_byte_constant());\n+    bind(left_not_inline_type);\n+\n+    mov(tmp, right);\n+    profile_obj_type(tmp, Address(mdp, in_bytes(ACmpData::right_offset())));\n+\n+    Label right_not_inline_type;\n+    test_oop_is_not_inline_type(right, tmp, right_not_inline_type);\n+    set_mdp_flag_at(mdp, ACmpData::right_inline_type_byte_constant());\n+    bind(right_not_inline_type);\n+\n+    bind(profile_continue);\n+  }\n+}\n","filename":"src\/hotspot\/cpu\/x86\/interp_masm_x86.cpp","additions":285,"deletions":21,"binary":false,"changes":306,"status":"modified"},{"patch":"@@ -40,0 +40,1 @@\n+#include \"oops\/inlineKlass.hpp\"\n@@ -186,0 +187,1 @@\n+  case Bytecodes::_fast_vputfield:\n@@ -833,9 +835,27 @@\n-  \/\/ rax: index\n-  \/\/ rdx: array\n-  index_check(rdx, rax); \/\/ kills rbx\n-  do_oop_load(_masm,\n-              Address(rdx, rax,\n-                      UseCompressedOops ? Address::times_4 : Address::times_ptr,\n-                      arrayOopDesc::base_offset_in_bytes(T_OBJECT)),\n-              rax,\n-              IS_ARRAY);\n+  Register array = rdx;\n+  Register index = rax;\n+\n+  index_check(array, index); \/\/ kills rbx\n+  __ profile_array_type<ArrayLoadData>(rbx, array, rcx);\n+  if (UseFlatArray) {\n+    Label is_flat_array, done;\n+    __ test_flat_array_oop(array, rbx, is_flat_array);\n+    do_oop_load(_masm,\n+                Address(array, index,\n+                        UseCompressedOops ? Address::times_4 : Address::times_ptr,\n+                        arrayOopDesc::base_offset_in_bytes(T_OBJECT)),\n+                rax,\n+                IS_ARRAY);\n+    __ jmp(done);\n+    __ bind(is_flat_array);\n+    __ read_flat_element(array, index, rbx, rcx, rax);\n+    __ bind(done);\n+  } else {\n+    do_oop_load(_masm,\n+                Address(array, index,\n+                        UseCompressedOops ? Address::times_4 : Address::times_ptr,\n+                        arrayOopDesc::base_offset_in_bytes(T_OBJECT)),\n+                rax,\n+                IS_ARRAY);\n+  }\n+  __ profile_element_type(rbx, rax, rcx);\n@@ -1127,1 +1147,1 @@\n-  Label is_null, ok_is_subtype, done;\n+  Label is_null, is_flat_array, ok_is_subtype, done;\n@@ -1139,0 +1159,4 @@\n+\n+  __ profile_array_type<ArrayStoreData>(rdi, rdx, rbx);\n+  __ profile_multiple_element_types(rdi, rax, rbx, rcx);\n+\n@@ -1142,0 +1166,7 @@\n+  \/\/ Move array class to rdi\n+  __ load_klass(rdi, rdx, rscratch1);\n+  if (UseFlatArray) {\n+    __ movl(rbx, Address(rdi, Klass::layout_helper_offset()));\n+    __ test_flat_array_layout(rbx, is_flat_array);\n+  }\n+\n@@ -1144,3 +1175,2 @@\n-  \/\/ Move superklass into rax\n-  __ load_klass(rax, rdx, rscratch1);\n-  __ movptr(rax, Address(rax,\n+  \/\/ Move array element superklass into rax\n+  __ movptr(rax, Address(rdi,\n@@ -1151,1 +1181,2 @@\n-  __ gen_subtype_check(rbx, ok_is_subtype);\n+  \/\/ is \"rbx <: rax\" ? (value subclass <: array element superclass)\n+  __ gen_subtype_check(rbx, ok_is_subtype, false);\n@@ -1169,1 +1200,2 @@\n-  __ profile_null_seen(rbx);\n+  if (EnableValhalla) {\n+    Label is_null_into_value_array_npe, store_null;\n@@ -1171,0 +1203,9 @@\n+    \/\/ No way to store null in null-free array\n+    __ test_null_free_array_oop(rdx, rbx, is_null_into_value_array_npe);\n+    __ jmp(store_null);\n+\n+    __ bind(is_null_into_value_array_npe);\n+    __ jump(RuntimeAddress(Interpreter::_throw_NullPointerException_entry));\n+\n+    __ bind(store_null);\n+  }\n@@ -1173,0 +1214,1 @@\n+  __ jmp(done);\n@@ -1174,0 +1216,33 @@\n+  if (UseFlatArray) {\n+    Label is_type_ok;\n+    __ bind(is_flat_array); \/\/ Store non-null value to flat\n+\n+    \/\/ Simplistic type check...\n+\n+    \/\/ Profile the not-null value's klass.\n+    __ load_klass(rbx, rax, rscratch1);\n+    \/\/ Move element klass into rax\n+    __ movptr(rax, Address(rdi, ArrayKlass::element_klass_offset()));\n+    \/\/ flat value array needs exact type match\n+    \/\/ is \"rax == rbx\" (value subclass == array element superclass)\n+    __ cmpptr(rax, rbx);\n+    __ jccb(Assembler::equal, is_type_ok);\n+\n+    __ jump(RuntimeAddress(Interpreter::_throw_ArrayStoreException_entry));\n+\n+    __ bind(is_type_ok);\n+    \/\/ rbx: value's klass\n+    \/\/ rdx: array\n+    \/\/ rdi: array klass\n+    __ test_klass_is_empty_inline_type(rbx, rax, done);\n+\n+    \/\/ calc dst for copy\n+    __ movl(rax, at_tos_p1()); \/\/ index\n+    __ data_for_value_array_index(rdx, rdi, rax, rax);\n+\n+    \/\/ ...and src for copy\n+    __ movptr(rcx, at_tos());  \/\/ value\n+    __ data_for_oop(rcx, rcx, rbx);\n+\n+    __ access_value_copy(IN_HEAP, rcx, rax, rbx);\n+  }\n@@ -2342,1 +2417,1 @@\n-  Label not_taken;\n+  Label taken, not_taken;\n@@ -2344,0 +2419,36 @@\n+\n+  __ profile_acmp(rbx, rdx, rax, rcx);\n+\n+  const int is_inline_type_mask = markWord::inline_type_pattern;\n+  if (EnableValhalla) {\n+    __ cmpoop(rdx, rax);\n+    __ jcc(Assembler::equal, (cc == equal) ? taken : not_taken);\n+\n+    \/\/ might be substitutable, test if either rax or rdx is null\n+    __ testptr(rax, rax);\n+    __ jcc(Assembler::zero, (cc == equal) ? not_taken : taken);\n+    __ testptr(rdx, rdx);\n+    __ jcc(Assembler::zero, (cc == equal) ? not_taken : taken);\n+\n+    \/\/ and both are values ?\n+    __ movptr(rbx, Address(rdx, oopDesc::mark_offset_in_bytes()));\n+    __ andptr(rbx, Address(rax, oopDesc::mark_offset_in_bytes()));\n+    __ andptr(rbx, is_inline_type_mask);\n+    __ cmpptr(rbx, is_inline_type_mask);\n+    __ jcc(Assembler::notEqual, (cc == equal) ? not_taken : taken);\n+\n+    \/\/ same value klass ?\n+    __ load_metadata(rbx, rdx);\n+    __ load_metadata(rcx, rax);\n+    __ cmpptr(rbx, rcx);\n+    __ jcc(Assembler::notEqual, (cc == equal) ? not_taken : taken);\n+\n+    \/\/ Know both are the same type, let's test for substitutability...\n+    if (cc == equal) {\n+      invoke_is_substitutable(rax, rdx, taken, not_taken);\n+    } else {\n+      invoke_is_substitutable(rax, rdx, not_taken, taken);\n+    }\n+    __ stop(\"Not reachable\");\n+  }\n+\n@@ -2346,0 +2457,1 @@\n+  __ bind(taken);\n@@ -2348,1 +2460,10 @@\n-  __ profile_not_taken_branch(rax);\n+  __ profile_not_taken_branch(rax, true);\n+}\n+\n+void TemplateTable::invoke_is_substitutable(Register aobj, Register bobj,\n+                                            Label& is_subst, Label& not_subst) {\n+  __ call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::is_substitutable), aobj, bobj);\n+  \/\/ Restored...rax answer, jmp to outcome...\n+  __ testl(rax, rax);\n+  __ jcc(Assembler::zero, not_subst);\n+  __ jmp(is_subst);\n@@ -2617,1 +2738,2 @@\n-  __ remove_activation(state, rbcp);\n+\n+  __ remove_activation(state, rbcp, true, true, true);\n@@ -2983,1 +3105,1 @@\n-  const Register obj   = LP64_ONLY(c_rarg3) NOT_LP64(rcx);\n+  const Register obj   = LP64_ONLY(r9) NOT_LP64(rcx);\n@@ -2995,2 +3117,0 @@\n-  if (!is_static) pop_and_check_object(obj);\n-\n@@ -2999,1 +3119,1 @@\n-  Label Done, notByte, notBool, notInt, notShort, notChar, notLong, notFloat, notObj;\n+  Label Done, notByte, notBool, notInt, notShort, notChar, notLong, notFloat, notObj, notInlineType;\n@@ -3007,0 +3127,1 @@\n+  if (!is_static) pop_and_check_object(obj);\n@@ -3018,1 +3139,1 @@\n-\n+   if (!is_static) pop_and_check_object(obj);\n@@ -3033,4 +3154,88 @@\n-  do_oop_load(_masm, field, rax);\n-  __ push(atos);\n-  if (!is_static && rc == may_rewrite) {\n-    patch_bytecode(Bytecodes::_fast_agetfield, bc, rbx);\n+  if (!EnableValhalla) {\n+    if (!is_static) pop_and_check_object(obj);\n+    do_oop_load(_masm, field, rax);\n+    __ push(atos);\n+    if (!is_static && rc == may_rewrite) {\n+      patch_bytecode(Bytecodes::_fast_agetfield, bc, rbx);\n+    }\n+    __ jmp(Done);\n+  } else {\n+    if (is_static) {\n+      __ load_heap_oop(rax, field);\n+      Label is_null_free_inline_type, uninitialized;\n+      \/\/ Issue below if the static field has not been initialized yet\n+      __ test_field_is_null_free_inline_type(flags, rscratch1, is_null_free_inline_type);\n+        \/\/ field is not a null free inline type\n+        __ push(atos);\n+        __ jmp(Done);\n+      \/\/ field is a null free inline type, must not return null even if uninitialized\n+      __ bind(is_null_free_inline_type);\n+          __ testptr(rax, rax);\n+        __ jcc(Assembler::zero, uninitialized);\n+          __ push(atos);\n+          __ jmp(Done);\n+        __ bind(uninitialized);\n+#ifdef _LP64\n+          Label slow_case, finish;\n+          __ movptr(rbx, Address(obj, java_lang_Class::klass_offset()));\n+          __ cmpb(Address(rbx, InstanceKlass::init_state_offset()), InstanceKlass::fully_initialized);\n+          __ jcc(Assembler::notEqual, slow_case);\n+        __ get_default_value_oop(rbx, rscratch1, rax);\n+        __ jmp(finish);\n+        __ bind(slow_case);\n+#endif \/\/ LP64\n+          __ call_VM(rax, CAST_FROM_FN_PTR(address, InterpreterRuntime::uninitialized_static_inline_type_field),\n+                obj, cache);\n+#ifdef _LP64\n+          __ bind(finish);\n+  #endif \/\/ _LP64\n+        __ verify_oop(rax);\n+        __ push(atos);\n+        __ jmp(Done);\n+    } else {\n+      Label is_flat, nonnull, is_inline_type, rewrite_inline, has_null_marker;\n+      __ test_field_is_null_free_inline_type(flags, rscratch1, is_inline_type);\n+      __ test_field_has_null_marker(flags, rscratch1, has_null_marker);\n+      \/\/ field is not a null free inline type\n+      pop_and_check_object(obj);\n+      __ load_heap_oop(rax, field);\n+      __ push(atos);\n+      if (rc == may_rewrite) {\n+        patch_bytecode(Bytecodes::_fast_agetfield, bc, rbx);\n+      }\n+      __ jmp(Done);\n+      __ bind(is_inline_type);\n+      __ test_field_is_flat(flags, rscratch1, is_flat);\n+          \/\/ field is not flat\n+          pop_and_check_object(obj);\n+          __ load_heap_oop(rax, field);\n+          __ testptr(rax, rax);\n+          __ jcc(Assembler::notZero, nonnull);\n+            __ load_unsigned_short(flags, Address(cache, in_bytes(ResolvedFieldEntry::field_index_offset())));\n+            __ movptr(rcx, Address(cache, ResolvedFieldEntry::field_holder_offset()));\n+            __ get_inline_type_field_klass(rcx, flags, rbx);\n+            __ get_default_value_oop(rbx, rcx, rax);\n+          __ bind(nonnull);\n+          __ verify_oop(rax);\n+          __ push(atos);\n+          __ jmp(rewrite_inline);\n+        __ bind(is_flat);\n+          pop_and_check_object(rax);\n+          __ load_unsigned_short(rdx, Address(cache, in_bytes(ResolvedFieldEntry::field_index_offset())));\n+          __ movptr(rcx, Address(cache, ResolvedFieldEntry::field_holder_offset()));\n+          __ read_flat_field(rcx, rdx, rbx, rax);\n+          __ verify_oop(rax);\n+          __ push(atos);\n+          __ jmp(rewrite_inline);\n+      __ bind(has_null_marker);\n+        pop_and_check_object(rax);\n+        __ load_field_entry(rcx, rbx);\n+        call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::read_nullable_flat_field), rax, rcx);\n+        __ get_vm_result(rax, r15_thread);\n+        __ push(atos);\n+      __ bind(rewrite_inline);\n+      if (rc == may_rewrite) {\n+        patch_bytecode(Bytecodes::_fast_vgetfield, bc, rbx);\n+      }\n+        __ jmp(Done);\n+    }\n@@ -3038,1 +3243,0 @@\n-  __ jmp(Done);\n@@ -3041,0 +3245,3 @@\n+\n+  if (!is_static) pop_and_check_object(obj);\n+\n@@ -3140,1 +3347,0 @@\n-\n@@ -3222,1 +3428,1 @@\n-  const Register flags = rax;\n+  const Register flags = r9;\n@@ -3235,2 +3441,3 @@\n-  __ andl(flags, (1 << ResolvedFieldEntry::is_volatile_shift));\n-  __ testl(flags, flags);\n+  __ movl(rscratch1, flags);\n+  __ andl(rscratch1, (1 << ResolvedFieldEntry::is_volatile_shift));\n+  __ testl(rscratch1, rscratch1);\n@@ -3239,1 +3446,1 @@\n-  putfield_or_static_helper(byte_no, is_static, rc, obj, off, tos_state);\n+  putfield_or_static_helper(byte_no, is_static, rc, obj, off, tos_state, flags);\n@@ -3245,1 +3452,1 @@\n-  putfield_or_static_helper(byte_no, is_static, rc, obj, off, tos_state);\n+  putfield_or_static_helper(byte_no, is_static, rc, obj, off, tos_state, flags);\n@@ -3251,1 +3458,1 @@\n-                                              Register obj, Register off, Register tos_state) {\n+                                              Register obj, Register off, Register tos_state, Register flags) {\n@@ -3258,1 +3465,1 @@\n-        notLong, notFloat, notObj;\n+        notLong, notFloat, notObj, notInlineType;\n@@ -3299,6 +3506,60 @@\n-    __ pop(atos);\n-    if (!is_static) pop_and_check_object(obj);\n-    \/\/ Store into the field\n-    do_oop_store(_masm, field, rax);\n-    if (!is_static && rc == may_rewrite) {\n-      patch_bytecode(Bytecodes::_fast_aputfield, bc, rbx, true, byte_no);\n+    if (!EnableValhalla) {\n+      __ pop(atos);\n+      if (!is_static) pop_and_check_object(obj);\n+      \/\/ Store into the field\n+      do_oop_store(_masm, field, rax);\n+      if (!is_static && rc == may_rewrite) {\n+        patch_bytecode(Bytecodes::_fast_aputfield, bc, rbx, true, byte_no);\n+      }\n+      __ jmp(Done);\n+    } else {\n+      __ pop(atos);\n+      if (is_static) {\n+        Label is_inline_type;\n+        __ test_field_is_not_null_free_inline_type(flags, rscratch1, is_inline_type);\n+        __ null_check(rax);\n+        __ bind(is_inline_type);\n+        do_oop_store(_masm, field, rax);\n+        __ jmp(Done);\n+      } else {\n+        Label is_null_free_inline_type, is_flat, has_null_marker,\n+              write_null, rewrite_not_inline, rewrite_inline;\n+        __ test_field_is_null_free_inline_type(flags, rscratch1, is_null_free_inline_type);\n+        __ test_field_has_null_marker(flags, rscratch1, has_null_marker);\n+          \/\/ Not an inline type\n+          pop_and_check_object(obj);\n+          \/\/ Store into the field\n+          do_oop_store(_masm, field, rax);\n+          __ bind(rewrite_not_inline);\n+          if (rc == may_rewrite) {\n+            patch_bytecode(Bytecodes::_fast_aputfield, bc, rbx, true, byte_no);\n+          }\n+          __ jmp(Done);\n+        \/\/ Implementation of the inline type semantic\n+        __ bind(is_null_free_inline_type);\n+          __ null_check(rax);\n+          __ test_field_is_flat(flags, rscratch1, is_flat);\n+            \/\/ field is not flat\n+            pop_and_check_object(obj);\n+            \/\/ Store into the field\n+            do_oop_store(_masm, field, rax);\n+          __ jmp(rewrite_inline);\n+          __ bind(is_flat);\n+            \/\/ field is flat\n+            pop_and_check_object(obj);\n+            assert_different_registers(rax, rdx, obj, off);\n+            __ load_klass(rdx, rax, rscratch1);\n+            __ data_for_oop(rax, rax, rdx);\n+            __ addptr(obj, off);\n+            __ access_value_copy(IN_HEAP, rax, obj, rdx);\n+            __ jmp(rewrite_inline);\n+        __ bind(has_null_marker); \/\/ has null marker means the field is flat with a null marker\n+          pop_and_check_object(rbx);\n+          __ load_field_entry(rcx, rdx);\n+          call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::write_nullable_flat_field), rbx, rax, rcx);\n+        __ bind(rewrite_inline);\n+        if (rc == may_rewrite) {\n+          patch_bytecode(Bytecodes::_fast_vputfield, bc, rbx, true, byte_no);\n+        }\n+        __ jmp(Done);\n+      }\n@@ -3306,1 +3567,0 @@\n-    __ jmp(Done);\n@@ -3445,0 +3705,1 @@\n+    case Bytecodes::_fast_vputfield: \/\/fall through\n@@ -3470,0 +3731,1 @@\n+    case Bytecodes::_fast_vputfield: \/\/ fall through\n@@ -3488,2 +3750,0 @@\n-  Register cache = rcx;\n-\n@@ -3496,3 +3756,1 @@\n-  load_resolved_field_entry(noreg, cache, rax, rbx, rdx);\n-  \/\/ RBX: field offset, RAX: TOS, RDX: flags\n-  __ andl(rdx, (1 << ResolvedFieldEntry::is_volatile_shift));\n+  load_resolved_field_entry(noreg, rcx, rax, rbx, rdx);\n@@ -3500,0 +3758,1 @@\n+  \/\/ RBX: field offset, RCX: RAX: TOS, RDX: flags\n@@ -3508,1 +3767,3 @@\n-  __ testl(rdx, rdx);\n+  __ movl(rscratch2, rdx);  \/\/ saving flags for is_flat test\n+  __ andl(rscratch2, (1 << ResolvedFieldEntry::is_volatile_shift));\n+  __ testl(rscratch2, rscratch2);\n@@ -3511,1 +3772,1 @@\n-  fast_storefield_helper(field, rax);\n+  fast_storefield_helper(field, rax, rdx);\n@@ -3517,1 +3778,1 @@\n-  fast_storefield_helper(field, rax);\n+  fast_storefield_helper(field, rax, rdx);\n@@ -3522,1 +3783,3 @@\n-void TemplateTable::fast_storefield_helper(Address field, Register rax) {\n+void TemplateTable::fast_storefield_helper(Address field, Register rax, Register flags) {\n+\n+  \/\/ DANGER: 'field' argument depends on rcx and rbx\n@@ -3526,0 +3789,24 @@\n+  case Bytecodes::_fast_vputfield:\n+    {\n+      Label is_flat, has_null_marker, write_null, done;\n+      __ test_field_has_null_marker(flags, rscratch1, has_null_marker);\n+      \/\/ Null free field cases: flat or not flat\n+      __ null_check(rax);\n+      __ test_field_is_flat(flags, rscratch1, is_flat);\n+        \/\/ field is not flat\n+        do_oop_store(_masm, field, rax);\n+        __ jmp(done);\n+      __ bind(is_flat);\n+        \/\/ field is flat\n+        __ load_klass(rdx, rax, rscratch1);\n+        __ data_for_oop(rax, rax, rdx);\n+        __ lea(rcx, field);\n+        __ access_value_copy(IN_HEAP, rax, rcx, rdx);\n+        __ jmp(done);\n+      __ bind(has_null_marker); \/\/ has null marker means the field is flat with a null marker\n+        __ movptr(rbx, rcx);\n+        __ load_field_entry(rcx, rdx);\n+        call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::write_nullable_flat_field), rbx, rax, rcx);\n+      __ bind(done);\n+    }\n+    break;\n@@ -3527,1 +3814,3 @@\n-    do_oop_store(_masm, field, rax);\n+    {\n+      do_oop_store(_masm, field, rax);\n+    }\n@@ -3589,1 +3878,1 @@\n-  __ load_sized_value(rbx, Address(rcx, in_bytes(ResolvedFieldEntry::field_offset_offset())), sizeof(int), true \/*is_signed*\/);\n+  __ load_sized_value(rdx, Address(rcx, in_bytes(ResolvedFieldEntry::field_offset_offset())), sizeof(int), true \/*is_signed*\/);\n@@ -3594,1 +3883,1 @@\n-  Address field(rax, rbx, Address::times_1);\n+  Address field(rax, rdx, Address::times_1);\n@@ -3598,0 +3887,33 @@\n+  case Bytecodes::_fast_vgetfield:\n+    {\n+      Label is_flat, nonnull, Done, has_null_marker;\n+      __ load_unsigned_byte(rscratch1, Address(rcx, in_bytes(ResolvedFieldEntry::flags_offset())));\n+      __ test_field_has_null_marker(rscratch1, rscratch2, has_null_marker);\n+      __ test_field_is_flat(rscratch1, rscratch2, is_flat);\n+        \/\/ field is not flat\n+        __ load_heap_oop(rax, field);\n+        __ testptr(rax, rax);\n+        __ jcc(Assembler::notZero, nonnull);\n+          __ load_unsigned_short(rdx, Address(rcx, in_bytes(ResolvedFieldEntry::field_index_offset())));\n+          __ movptr(rcx, Address(rcx, ResolvedFieldEntry::field_holder_offset()));\n+          __ get_inline_type_field_klass(rcx, rdx, rbx);\n+          __ get_default_value_oop(rbx, rcx, rax);\n+        __ bind(nonnull);\n+        __ verify_oop(rax);\n+        __ jmp(Done);\n+      __ bind(is_flat);\n+      \/\/ field is flat\n+        __ push(rdx); \/\/ save offset\n+        __ load_unsigned_short(rdx, Address(rcx, in_bytes(ResolvedFieldEntry::field_index_offset())));\n+        __ movptr(rcx, Address(rcx, ResolvedFieldEntry::field_holder_offset()));\n+        __ pop(rbx); \/\/ restore offset\n+        __ read_flat_field(rcx, rdx, rbx, rax);\n+        __ jmp(Done);\n+      __ bind(has_null_marker);\n+        \/\/ rax = instance, rcx = resolved entry\n+        call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::read_nullable_flat_field), rax, rcx);\n+        __ get_vm_result(rax, r15_thread);\n+      __ bind(Done);\n+      __ verify_oop(rax);\n+    }\n+    break;\n@@ -4033,2 +4355,0 @@\n-  Label slow_case_no_pop;\n-  Label initialize_header;\n@@ -4044,1 +4364,1 @@\n-  __ jcc(Assembler::notEqual, slow_case_no_pop);\n+  __ jcc(Assembler::notEqual, slow_case);\n@@ -4048,1 +4368,0 @@\n-  __ push(rcx);  \/\/ save the contexts of klass for initializing the header\n@@ -4059,66 +4378,1 @@\n-  \/\/ get instance_size in InstanceKlass (scaled to a count of bytes)\n-  __ movl(rdx, Address(rcx, Klass::layout_helper_offset()));\n-  \/\/ test to see if it is malformed in some way\n-  __ testl(rdx, Klass::_lh_instance_slow_path_bit);\n-  __ jcc(Assembler::notZero, slow_case);\n-\n-  \/\/ Allocate the instance:\n-  \/\/  If TLAB is enabled:\n-  \/\/    Try to allocate in the TLAB.\n-  \/\/    If fails, go to the slow path.\n-  \/\/    Initialize the allocation.\n-  \/\/    Exit.\n-  \/\/\n-  \/\/  Go to slow path.\n-\n-  const Register thread = LP64_ONLY(r15_thread) NOT_LP64(rcx);\n-\n-  if (UseTLAB) {\n-    NOT_LP64(__ get_thread(thread);)\n-    __ tlab_allocate(thread, rax, rdx, 0, rcx, rbx, slow_case);\n-    if (ZeroTLAB) {\n-      \/\/ the fields have been already cleared\n-      __ jmp(initialize_header);\n-    }\n-\n-    \/\/ The object is initialized before the header.  If the object size is\n-    \/\/ zero, go directly to the header initialization.\n-    __ decrement(rdx, sizeof(oopDesc));\n-    __ jcc(Assembler::zero, initialize_header);\n-\n-    \/\/ Initialize topmost object field, divide rdx by 8, check if odd and\n-    \/\/ test if zero.\n-    __ xorl(rcx, rcx);    \/\/ use zero reg to clear memory (shorter code)\n-    __ shrl(rdx, LogBytesPerLong); \/\/ divide by 2*oopSize and set carry flag if odd\n-\n-    \/\/ rdx must have been multiple of 8\n-#ifdef ASSERT\n-    \/\/ make sure rdx was multiple of 8\n-    Label L;\n-    \/\/ Ignore partial flag stall after shrl() since it is debug VM\n-    __ jcc(Assembler::carryClear, L);\n-    __ stop(\"object size is not multiple of 2 - adjust this code\");\n-    __ bind(L);\n-    \/\/ rdx must be > 0, no extra check needed here\n-#endif\n-\n-    \/\/ initialize remaining object fields: rdx was a multiple of 8\n-    { Label loop;\n-    __ bind(loop);\n-    __ movptr(Address(rax, rdx, Address::times_8, sizeof(oopDesc) - 1*oopSize), rcx);\n-    NOT_LP64(__ movptr(Address(rax, rdx, Address::times_8, sizeof(oopDesc) - 2*oopSize), rcx));\n-    __ decrement(rdx);\n-    __ jcc(Assembler::notZero, loop);\n-    }\n-\n-    \/\/ initialize object header only.\n-    __ bind(initialize_header);\n-    __ movptr(Address(rax, oopDesc::mark_offset_in_bytes()),\n-              (intptr_t)markWord::prototype().value()); \/\/ header\n-    __ pop(rcx);   \/\/ get saved klass back in the register.\n-#ifdef _LP64\n-    __ xorl(rsi, rsi); \/\/ use zero reg to clear memory (shorter code)\n-    __ store_klass_gap(rax, rsi);  \/\/ zero klass gap for compressed oops\n-#endif\n-    __ store_klass(rax, rcx, rscratch1);  \/\/ klass\n-\n+  __ allocate_instance(rcx, rax, rdx, rbx, true, slow_case);\n@@ -4132,3 +4386,1 @@\n-\n-    __ jmp(done);\n-  }\n+  __ jmp(done);\n@@ -4138,2 +4390,0 @@\n-  __ pop(rcx);   \/\/ restore stack pointer to what it was when we came in.\n-  __ bind(slow_case_no_pop);\n@@ -4188,4 +4438,4 @@\n-  __ cmpb(Address(rdx, rbx,\n-                  Address::times_1,\n-                  Array<u1>::base_offset_in_bytes()),\n-          JVM_CONSTANT_Class);\n+  __ movzbl(rdx, Address(rdx, rbx,\n+      Address::times_1,\n+      Array<u1>::base_offset_in_bytes()));\n+  __ cmpl(rdx, JVM_CONSTANT_Class);\n@@ -4229,0 +4479,3 @@\n+  __ jmp(done);\n+\n+  __ bind(is_null);\n@@ -4232,4 +4485,1 @@\n-    __ jmp(done);\n-    __ bind(is_null);\n-  } else {\n-    __ bind(is_null);   \/\/ same as 'done'\n+\n@@ -4251,4 +4501,4 @@\n-  __ cmpb(Address(rdx, rbx,\n-                  Address::times_1,\n-                  Array<u1>::base_offset_in_bytes()),\n-          JVM_CONSTANT_Class);\n+  __ movzbl(rdx, Address(rdx, rbx,\n+        Address::times_1,\n+        Array<u1>::base_offset_in_bytes()));\n+  __ cmpl(rdx, JVM_CONSTANT_Class);\n@@ -4306,1 +4556,0 @@\n-\n@@ -4368,0 +4617,4 @@\n+  Label is_inline_type;\n+  __ movptr(rbx, Address(rax, oopDesc::mark_offset_in_bytes()));\n+  __ test_markword_is_inline_type(rbx, is_inline_type);\n+\n@@ -4460,0 +4713,5 @@\n+\n+  __ bind(is_inline_type);\n+  __ call_VM(noreg, CAST_FROM_FN_PTR(address,\n+                    InterpreterRuntime::throw_identity_exception), rax);\n+  __ should_not_reach_here();\n@@ -4468,0 +4726,11 @@\n+  const int is_inline_type_mask = markWord::inline_type_pattern;\n+  Label has_identity;\n+  __ movptr(rbx, Address(rax, oopDesc::mark_offset_in_bytes()));\n+  __ andptr(rbx, is_inline_type_mask);\n+  __ cmpl(rbx, is_inline_type_mask);\n+  __ jcc(Assembler::notEqual, has_identity);\n+  __ call_VM(noreg, CAST_FROM_FN_PTR(address,\n+                     InterpreterRuntime::throw_illegal_monitor_state_exception));\n+  __ should_not_reach_here();\n+  __ bind(has_identity);\n+\n","filename":"src\/hotspot\/cpu\/x86\/templateTable_x86.cpp","additions":412,"deletions":143,"binary":false,"changes":555,"status":"modified"},{"patch":"@@ -606,0 +606,4 @@\n+  if (_entry_point == nullptr) {\n+    \/\/ CallLeafNoFPInDirect\n+    return 3; \/\/ callq (register)\n+  }\n@@ -612,0 +616,1 @@\n+\n@@ -838,14 +843,1 @@\n-  int framesize = C->output()->frame_size_in_bytes();\n-  int bangsize = C->output()->bang_size_in_bytes();\n-\n-  if (C->clinit_barrier_on_entry()) {\n-    assert(VM_Version::supports_fast_class_init_checks(), \"sanity\");\n-    assert(!C->method()->holder()->is_not_initialized(), \"initialization should have been started\");\n-\n-    Label L_skip_barrier;\n-    Register klass = rscratch1;\n-\n-    __ mov_metadata(klass, C->method()->holder()->constant_encoding());\n-    __ clinit_barrier(klass, r15_thread, &L_skip_barrier \/*L_fast_path*\/);\n-\n-    __ jump(RuntimeAddress(SharedRuntime::get_handle_wrong_method_stub())); \/\/ slow path\n+  __ verified_entry(C);\n@@ -853,1 +845,2 @@\n-    __ bind(L_skip_barrier);\n+  if (ra_->C->stub_function() == nullptr) {\n+    __ entry_barrier();\n@@ -856,1 +849,3 @@\n-  __ verified_entry(framesize, C->output()->need_stack_bang(bangsize)?bangsize:0, false, C->stub_function() != nullptr);\n+  if (!Compile::current()->output()->in_scratch_emit_size()) {\n+    __ bind(*_verified_entry);\n+  }\n@@ -868,6 +863,0 @@\n-uint MachPrologNode::size(PhaseRegAlloc* ra_) const\n-{\n-  return MachNode::size(ra_); \/\/ too many variables; just compute it\n-                              \/\/ the hard way\n-}\n-\n@@ -920,13 +909,3 @@\n-  int framesize = C->output()->frame_size_in_bytes();\n-  assert((framesize & (StackAlignmentInBytes-1)) == 0, \"frame size not aligned\");\n-  \/\/ Remove word for return adr already pushed\n-  \/\/ and RBP\n-  framesize -= 2*wordSize;\n-\n-  \/\/ Note that VerifyStackAtCalls' Majik cookie does not change the frame size popped here\n-\n-  if (framesize) {\n-    __ addq(rsp, framesize);\n-  }\n-\n-  __ popq(rbp);\n+  \/\/ Subtract two words to account for return address and rbp\n+  int initial_framesize = C->output()->frame_size_in_bytes() - 2*wordSize;\n+  __ remove_frame(initial_framesize, C->needs_stack_repair());\n@@ -951,6 +930,0 @@\n-uint MachEpilogNode::size(PhaseRegAlloc* ra_) const\n-{\n-  return MachNode::size(ra_); \/\/ too many variables; just compute it\n-                              \/\/ the hard way\n-}\n-\n@@ -1554,0 +1527,43 @@\n+\/\/=============================================================================\n+#ifndef PRODUCT\n+void MachVEPNode::format(PhaseRegAlloc* ra_, outputStream* st) const\n+{\n+  st->print_cr(\"MachVEPNode\");\n+}\n+#endif\n+\n+void MachVEPNode::emit(C2_MacroAssembler* masm, PhaseRegAlloc* ra_) const\n+{\n+  CodeBuffer* cbuf = masm->code();\n+  uint insts_size = cbuf->insts_size();\n+  if (!_verified) {\n+    __ ic_check(1);\n+  } else {\n+    \/\/ TODO 8284443 Avoid creation of temporary frame\n+    if (ra_->C->stub_function() == nullptr) {\n+      __ verified_entry(ra_->C, 0);\n+      __ entry_barrier();\n+      int initial_framesize = ra_->C->output()->frame_size_in_bytes() - 2*wordSize;\n+      __ remove_frame(initial_framesize, false);\n+    }\n+    \/\/ Unpack inline type args passed as oop and then jump to\n+    \/\/ the verified entry point (skipping the unverified entry).\n+    int sp_inc = __ unpack_inline_args(ra_->C, _receiver_only);\n+    \/\/ Emit code for verified entry and save increment for stack repair on return\n+    __ verified_entry(ra_->C, sp_inc);\n+    if (Compile::current()->output()->in_scratch_emit_size()) {\n+      Label dummy_verified_entry;\n+      __ jmp(dummy_verified_entry);\n+    } else {\n+      __ jmp(*_verified_entry);\n+    }\n+  }\n+  \/* WARNING these NOPs are critical so that verified entry point is properly\n+     4 bytes aligned for patching by NativeJump::patch_verified_entry() *\/\n+  int nops_cnt = 4 - ((cbuf->insts_size() - insts_size) & 0x3);\n+  nops_cnt &= 0x3; \/\/ Do not add nops if code is aligned.\n+  if (nops_cnt > 0) {\n+    __ nop(nops_cnt);\n+  }\n+}\n+\n@@ -1574,7 +1590,0 @@\n-uint MachUEPNode::size(PhaseRegAlloc* ra_) const\n-{\n-  return MachNode::size(ra_); \/\/ too many variables; just compute it\n-                              \/\/ the hard way\n-}\n-\n-\n@@ -3063,0 +3072,16 @@\n+\/\/ Indirect Narrow Oop Operand\n+operand indCompressedOop(rRegN reg) %{\n+  predicate(UseCompressedOops && (CompressedOops::shift() == Address::times_8));\n+  constraint(ALLOC_IN_RC(ptr_reg));\n+  match(DecodeN reg);\n+\n+  op_cost(10);\n+  format %{\"[R12 + $reg << 3] (compressed oop addressing)\" %}\n+  interface(MEMORY_INTER) %{\n+    base(0xc); \/\/ R12\n+    index($reg);\n+    scale(0x3);\n+    disp(0x0);\n+  %}\n+%}\n+\n@@ -3409,1 +3434,1 @@\n-               indCompressedOopOffset,\n+               indCompressedOop, indCompressedOopOffset,\n@@ -5940,0 +5965,13 @@\n+instruct castN2X(rRegL dst, rRegN src)\n+%{\n+  match(Set dst (CastP2X src));\n+\n+  format %{ \"movq    $dst, $src\\t# ptr -> long\" %}\n+  ins_encode %{\n+    if ($dst$$reg != $src$$reg) {\n+      __ movptr($dst$$Register, $src$$Register);\n+    }\n+  %}\n+  ins_pipe(ialu_reg_reg); \/\/ XXX\n+%}\n+\n@@ -10499,0 +10537,1 @@\n+\n@@ -10501,1 +10540,1 @@\n-instruct rep_stos(rcx_RegL cnt, rdi_RegP base, regD tmp, rax_RegI zero,\n+instruct rep_stos(rcx_RegL cnt, rdi_RegP base, regD tmp, rax_RegL val,\n@@ -10504,3 +10543,120 @@\n-  predicate(!((ClearArrayNode*)n)->is_large() && (UseAVX <= 2));\n-  match(Set dummy (ClearArray cnt base));\n-  effect(USE_KILL cnt, USE_KILL base, TEMP tmp, KILL zero, KILL cr);\n+  predicate(!((ClearArrayNode*)n)->is_large() && !((ClearArrayNode*)n)->word_copy_only() && (UseAVX <= 2));\n+  match(Set dummy (ClearArray (Binary cnt base) val));\n+  effect(USE_KILL cnt, USE_KILL base, TEMP tmp, USE_KILL val, KILL cr);\n+\n+  format %{ $$template\n+    $$emit$$\"cmp     InitArrayShortSize,rcx\\n\\t\"\n+    $$emit$$\"jg      LARGE\\n\\t\"\n+    $$emit$$\"dec     rcx\\n\\t\"\n+    $$emit$$\"js      DONE\\t# Zero length\\n\\t\"\n+    $$emit$$\"mov     rax,(rdi,rcx,8)\\t# LOOP\\n\\t\"\n+    $$emit$$\"dec     rcx\\n\\t\"\n+    $$emit$$\"jge     LOOP\\n\\t\"\n+    $$emit$$\"jmp     DONE\\n\\t\"\n+    $$emit$$\"# LARGE:\\n\\t\"\n+    if (UseFastStosb) {\n+       $$emit$$\"shlq    rcx,3\\t# Convert doublewords to bytes\\n\\t\"\n+       $$emit$$\"rep     stosb\\t# Store rax to *rdi++ while rcx--\\n\\t\"\n+    } else if (UseXMMForObjInit) {\n+       $$emit$$\"movdq   $tmp, $val\\n\\t\"\n+       $$emit$$\"punpcklqdq $tmp, $tmp\\n\\t\"\n+       $$emit$$\"vinserti128_high $tmp, $tmp\\n\\t\"\n+       $$emit$$\"jmpq    L_zero_64_bytes\\n\\t\"\n+       $$emit$$\"# L_loop:\\t# 64-byte LOOP\\n\\t\"\n+       $$emit$$\"vmovdqu $tmp,(rax)\\n\\t\"\n+       $$emit$$\"vmovdqu $tmp,0x20(rax)\\n\\t\"\n+       $$emit$$\"add     0x40,rax\\n\\t\"\n+       $$emit$$\"# L_zero_64_bytes:\\n\\t\"\n+       $$emit$$\"sub     0x8,rcx\\n\\t\"\n+       $$emit$$\"jge     L_loop\\n\\t\"\n+       $$emit$$\"add     0x4,rcx\\n\\t\"\n+       $$emit$$\"jl      L_tail\\n\\t\"\n+       $$emit$$\"vmovdqu $tmp,(rax)\\n\\t\"\n+       $$emit$$\"add     0x20,rax\\n\\t\"\n+       $$emit$$\"sub     0x4,rcx\\n\\t\"\n+       $$emit$$\"# L_tail:\\t# Clearing tail bytes\\n\\t\"\n+       $$emit$$\"add     0x4,rcx\\n\\t\"\n+       $$emit$$\"jle     L_end\\n\\t\"\n+       $$emit$$\"dec     rcx\\n\\t\"\n+       $$emit$$\"# L_sloop:\\t# 8-byte short loop\\n\\t\"\n+       $$emit$$\"vmovq   xmm0,(rax)\\n\\t\"\n+       $$emit$$\"add     0x8,rax\\n\\t\"\n+       $$emit$$\"dec     rcx\\n\\t\"\n+       $$emit$$\"jge     L_sloop\\n\\t\"\n+       $$emit$$\"# L_end:\\n\\t\"\n+    } else {\n+       $$emit$$\"rep     stosq\\t# Store rax to *rdi++ while rcx--\\n\\t\"\n+    }\n+    $$emit$$\"# DONE\"\n+  %}\n+  ins_encode %{\n+    __ clear_mem($base$$Register, $cnt$$Register, $val$$Register,\n+                 $tmp$$XMMRegister, false, false);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct rep_stos_word_copy(rcx_RegL cnt, rdi_RegP base, regD tmp, rax_RegL val,\n+                            Universe dummy, rFlagsReg cr)\n+%{\n+  predicate(!((ClearArrayNode*)n)->is_large() && ((ClearArrayNode*)n)->word_copy_only() && (UseAVX <= 2));\n+  match(Set dummy (ClearArray (Binary cnt base) val));\n+  effect(USE_KILL cnt, USE_KILL base, TEMP tmp, USE_KILL val, KILL cr);\n+\n+  format %{ $$template\n+    $$emit$$\"cmp     InitArrayShortSize,rcx\\n\\t\"\n+    $$emit$$\"jg      LARGE\\n\\t\"\n+    $$emit$$\"dec     rcx\\n\\t\"\n+    $$emit$$\"js      DONE\\t# Zero length\\n\\t\"\n+    $$emit$$\"mov     rax,(rdi,rcx,8)\\t# LOOP\\n\\t\"\n+    $$emit$$\"dec     rcx\\n\\t\"\n+    $$emit$$\"jge     LOOP\\n\\t\"\n+    $$emit$$\"jmp     DONE\\n\\t\"\n+    $$emit$$\"# LARGE:\\n\\t\"\n+    if (UseXMMForObjInit) {\n+       $$emit$$\"movdq   $tmp, $val\\n\\t\"\n+       $$emit$$\"punpcklqdq $tmp, $tmp\\n\\t\"\n+       $$emit$$\"vinserti128_high $tmp, $tmp\\n\\t\"\n+       $$emit$$\"jmpq    L_zero_64_bytes\\n\\t\"\n+       $$emit$$\"# L_loop:\\t# 64-byte LOOP\\n\\t\"\n+       $$emit$$\"vmovdqu $tmp,(rax)\\n\\t\"\n+       $$emit$$\"vmovdqu $tmp,0x20(rax)\\n\\t\"\n+       $$emit$$\"add     0x40,rax\\n\\t\"\n+       $$emit$$\"# L_zero_64_bytes:\\n\\t\"\n+       $$emit$$\"sub     0x8,rcx\\n\\t\"\n+       $$emit$$\"jge     L_loop\\n\\t\"\n+       $$emit$$\"add     0x4,rcx\\n\\t\"\n+       $$emit$$\"jl      L_tail\\n\\t\"\n+       $$emit$$\"vmovdqu $tmp,(rax)\\n\\t\"\n+       $$emit$$\"add     0x20,rax\\n\\t\"\n+       $$emit$$\"sub     0x4,rcx\\n\\t\"\n+       $$emit$$\"# L_tail:\\t# Clearing tail bytes\\n\\t\"\n+       $$emit$$\"add     0x4,rcx\\n\\t\"\n+       $$emit$$\"jle     L_end\\n\\t\"\n+       $$emit$$\"dec     rcx\\n\\t\"\n+       $$emit$$\"# L_sloop:\\t# 8-byte short loop\\n\\t\"\n+       $$emit$$\"vmovq   xmm0,(rax)\\n\\t\"\n+       $$emit$$\"add     0x8,rax\\n\\t\"\n+       $$emit$$\"dec     rcx\\n\\t\"\n+       $$emit$$\"jge     L_sloop\\n\\t\"\n+       $$emit$$\"# L_end:\\n\\t\"\n+    } else {\n+       $$emit$$\"rep     stosq\\t# Store rax to *rdi++ while rcx--\\n\\t\"\n+    }\n+    $$emit$$\"# DONE\"\n+  %}\n+  ins_encode %{\n+    __ clear_mem($base$$Register, $cnt$$Register, $val$$Register,\n+                 $tmp$$XMMRegister, false, true);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ Small non-constant length ClearArray for AVX512 targets.\n+instruct rep_stos_evex(rcx_RegL cnt, rdi_RegP base, legRegD tmp, kReg ktmp, rax_RegL val,\n+                       Universe dummy, rFlagsReg cr)\n+%{\n+  predicate(!((ClearArrayNode*)n)->is_large() && !((ClearArrayNode*)n)->word_copy_only() && (UseAVX > 2));\n+  match(Set dummy (ClearArray (Binary cnt base) val));\n+  ins_cost(125);\n+  effect(USE_KILL cnt, USE_KILL base, TEMP tmp, TEMP ktmp, USE_KILL val, KILL cr);\n@@ -10554,2 +10710,2 @@\n-    __ clear_mem($base$$Register, $cnt$$Register, $zero$$Register,\n-                 $tmp$$XMMRegister, false, knoreg);\n+    __ clear_mem($base$$Register, $cnt$$Register, $val$$Register,\n+                 $tmp$$XMMRegister, false, false, $ktmp$$KRegister);\n@@ -10560,3 +10716,2 @@\n-\/\/ Small non-constant length ClearArray for AVX512 targets.\n-instruct rep_stos_evex(rcx_RegL cnt, rdi_RegP base, legRegD tmp, kReg ktmp, rax_RegI zero,\n-                       Universe dummy, rFlagsReg cr)\n+instruct rep_stos_evex_word_copy(rcx_RegL cnt, rdi_RegP base, legRegD tmp, kReg ktmp, rax_RegL val,\n+                                 Universe dummy, rFlagsReg cr)\n@@ -10564,2 +10719,2 @@\n-  predicate(!((ClearArrayNode*)n)->is_large() && (UseAVX > 2));\n-  match(Set dummy (ClearArray cnt base));\n+  predicate(!((ClearArrayNode*)n)->is_large() && ((ClearArrayNode*)n)->word_copy_only() && (UseAVX > 2));\n+  match(Set dummy (ClearArray (Binary cnt base) val));\n@@ -10567,1 +10722,1 @@\n-  effect(USE_KILL cnt, USE_KILL base, TEMP tmp, TEMP ktmp, KILL zero, KILL cr);\n+  effect(USE_KILL cnt, USE_KILL base, TEMP tmp, TEMP ktmp, USE_KILL val, KILL cr);\n@@ -10615,2 +10770,2 @@\n-    __ clear_mem($base$$Register, $cnt$$Register, $zero$$Register,\n-                 $tmp$$XMMRegister, false, $ktmp$$KRegister);\n+    __ clear_mem($base$$Register, $cnt$$Register, $val$$Register,\n+                 $tmp$$XMMRegister, false, true, $ktmp$$KRegister);\n@@ -10622,1 +10777,1 @@\n-instruct rep_stos_large(rcx_RegL cnt, rdi_RegP base, regD tmp, rax_RegI zero,\n+instruct rep_stos_large(rcx_RegL cnt, rdi_RegP base, regD tmp, rax_RegL val,\n@@ -10625,3 +10780,99 @@\n-  predicate((UseAVX <=2) && ((ClearArrayNode*)n)->is_large());\n-  match(Set dummy (ClearArray cnt base));\n-  effect(USE_KILL cnt, USE_KILL base, TEMP tmp, KILL zero, KILL cr);\n+  predicate(((ClearArrayNode*)n)->is_large() && !((ClearArrayNode*)n)->word_copy_only() && (UseAVX <= 2));\n+  match(Set dummy (ClearArray (Binary cnt base) val));\n+  effect(USE_KILL cnt, USE_KILL base, TEMP tmp, USE_KILL val, KILL cr);\n+\n+  format %{ $$template\n+    if (UseFastStosb) {\n+       $$emit$$\"shlq    rcx,3\\t# Convert doublewords to bytes\\n\\t\"\n+       $$emit$$\"rep     stosb\\t# Store rax to *rdi++ while rcx--\"\n+    } else if (UseXMMForObjInit) {\n+       $$emit$$\"movdq   $tmp, $val\\n\\t\"\n+       $$emit$$\"punpcklqdq $tmp, $tmp\\n\\t\"\n+       $$emit$$\"vinserti128_high $tmp, $tmp\\n\\t\"\n+       $$emit$$\"jmpq    L_zero_64_bytes\\n\\t\"\n+       $$emit$$\"# L_loop:\\t# 64-byte LOOP\\n\\t\"\n+       $$emit$$\"vmovdqu $tmp,(rax)\\n\\t\"\n+       $$emit$$\"vmovdqu $tmp,0x20(rax)\\n\\t\"\n+       $$emit$$\"add     0x40,rax\\n\\t\"\n+       $$emit$$\"# L_zero_64_bytes:\\n\\t\"\n+       $$emit$$\"sub     0x8,rcx\\n\\t\"\n+       $$emit$$\"jge     L_loop\\n\\t\"\n+       $$emit$$\"add     0x4,rcx\\n\\t\"\n+       $$emit$$\"jl      L_tail\\n\\t\"\n+       $$emit$$\"vmovdqu $tmp,(rax)\\n\\t\"\n+       $$emit$$\"add     0x20,rax\\n\\t\"\n+       $$emit$$\"sub     0x4,rcx\\n\\t\"\n+       $$emit$$\"# L_tail:\\t# Clearing tail bytes\\n\\t\"\n+       $$emit$$\"add     0x4,rcx\\n\\t\"\n+       $$emit$$\"jle     L_end\\n\\t\"\n+       $$emit$$\"dec     rcx\\n\\t\"\n+       $$emit$$\"# L_sloop:\\t# 8-byte short loop\\n\\t\"\n+       $$emit$$\"vmovq   xmm0,(rax)\\n\\t\"\n+       $$emit$$\"add     0x8,rax\\n\\t\"\n+       $$emit$$\"dec     rcx\\n\\t\"\n+       $$emit$$\"jge     L_sloop\\n\\t\"\n+       $$emit$$\"# L_end:\\n\\t\"\n+    } else {\n+       $$emit$$\"rep     stosq\\t# Store rax to *rdi++ while rcx--\"\n+    }\n+  %}\n+  ins_encode %{\n+    __ clear_mem($base$$Register, $cnt$$Register, $val$$Register,\n+                 $tmp$$XMMRegister, true, false);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct rep_stos_large_word_copy(rcx_RegL cnt, rdi_RegP base, regD tmp, rax_RegL val,\n+                                  Universe dummy, rFlagsReg cr)\n+%{\n+  predicate(((ClearArrayNode*)n)->is_large() && ((ClearArrayNode*)n)->word_copy_only() && (UseAVX <= 2));\n+  match(Set dummy (ClearArray (Binary cnt base) val));\n+  effect(USE_KILL cnt, USE_KILL base, TEMP tmp, USE_KILL val, KILL cr);\n+\n+  format %{ $$template\n+    if (UseXMMForObjInit) {\n+       $$emit$$\"movdq   $tmp, $val\\n\\t\"\n+       $$emit$$\"punpcklqdq $tmp, $tmp\\n\\t\"\n+       $$emit$$\"vinserti128_high $tmp, $tmp\\n\\t\"\n+       $$emit$$\"jmpq    L_zero_64_bytes\\n\\t\"\n+       $$emit$$\"# L_loop:\\t# 64-byte LOOP\\n\\t\"\n+       $$emit$$\"vmovdqu $tmp,(rax)\\n\\t\"\n+       $$emit$$\"vmovdqu $tmp,0x20(rax)\\n\\t\"\n+       $$emit$$\"add     0x40,rax\\n\\t\"\n+       $$emit$$\"# L_zero_64_bytes:\\n\\t\"\n+       $$emit$$\"sub     0x8,rcx\\n\\t\"\n+       $$emit$$\"jge     L_loop\\n\\t\"\n+       $$emit$$\"add     0x4,rcx\\n\\t\"\n+       $$emit$$\"jl      L_tail\\n\\t\"\n+       $$emit$$\"vmovdqu $tmp,(rax)\\n\\t\"\n+       $$emit$$\"add     0x20,rax\\n\\t\"\n+       $$emit$$\"sub     0x4,rcx\\n\\t\"\n+       $$emit$$\"# L_tail:\\t# Clearing tail bytes\\n\\t\"\n+       $$emit$$\"add     0x4,rcx\\n\\t\"\n+       $$emit$$\"jle     L_end\\n\\t\"\n+       $$emit$$\"dec     rcx\\n\\t\"\n+       $$emit$$\"# L_sloop:\\t# 8-byte short loop\\n\\t\"\n+       $$emit$$\"vmovq   xmm0,(rax)\\n\\t\"\n+       $$emit$$\"add     0x8,rax\\n\\t\"\n+       $$emit$$\"dec     rcx\\n\\t\"\n+       $$emit$$\"jge     L_sloop\\n\\t\"\n+       $$emit$$\"# L_end:\\n\\t\"\n+    } else {\n+       $$emit$$\"rep     stosq\\t# Store rax to *rdi++ while rcx--\"\n+    }\n+  %}\n+  ins_encode %{\n+    __ clear_mem($base$$Register, $cnt$$Register, $val$$Register,\n+                 $tmp$$XMMRegister, true, true);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ Large non-constant length ClearArray for AVX512 targets.\n+instruct rep_stos_large_evex(rcx_RegL cnt, rdi_RegP base, legRegD tmp, kReg ktmp, rax_RegL val,\n+                             Universe dummy, rFlagsReg cr)\n+%{\n+  predicate(((ClearArrayNode*)n)->is_large() && !((ClearArrayNode*)n)->word_copy_only() && (UseAVX > 2));\n+  match(Set dummy (ClearArray (Binary cnt base) val));\n+  effect(USE_KILL cnt, USE_KILL base, TEMP tmp, TEMP ktmp, USE_KILL val, KILL cr);\n@@ -10666,2 +10917,2 @@\n-    __ clear_mem($base$$Register, $cnt$$Register, $zero$$Register,\n-                 $tmp$$XMMRegister, true, knoreg);\n+    __ clear_mem($base$$Register, $cnt$$Register, $val$$Register,\n+                 $tmp$$XMMRegister, true, false, $ktmp$$KRegister);\n@@ -10672,3 +10923,2 @@\n-\/\/ Large non-constant length ClearArray for AVX512 targets.\n-instruct rep_stos_large_evex(rcx_RegL cnt, rdi_RegP base, legRegD tmp, kReg ktmp, rax_RegI zero,\n-                             Universe dummy, rFlagsReg cr)\n+instruct rep_stos_large_evex_word_copy(rcx_RegL cnt, rdi_RegP base, legRegD tmp, kReg ktmp, rax_RegL val,\n+                                       Universe dummy, rFlagsReg cr)\n@@ -10676,3 +10926,3 @@\n-  predicate((UseAVX > 2) && ((ClearArrayNode*)n)->is_large());\n-  match(Set dummy (ClearArray cnt base));\n-  effect(USE_KILL cnt, USE_KILL base, TEMP tmp, TEMP ktmp, KILL zero, KILL cr);\n+  predicate(((ClearArrayNode*)n)->is_large() && ((ClearArrayNode*)n)->word_copy_only() && (UseAVX > 2));\n+  match(Set dummy (ClearArray (Binary cnt base) val));\n+  effect(USE_KILL cnt, USE_KILL base, TEMP tmp, TEMP ktmp, USE_KILL val, KILL cr);\n@@ -10717,2 +10967,2 @@\n-    __ clear_mem($base$$Register, $cnt$$Register, $zero$$Register,\n-                 $tmp$$XMMRegister, true, $ktmp$$KRegister);\n+    __ clear_mem($base$$Register, $cnt$$Register, $val$$Register,\n+                 $tmp$$XMMRegister, true, true, $ktmp$$KRegister);\n@@ -10724,1 +10974,1 @@\n-instruct rep_stos_im(immL cnt, rRegP base, regD tmp, rRegI zero, kReg ktmp, Universe dummy, rFlagsReg cr)\n+instruct rep_stos_im(immL cnt, rRegP base, regD tmp, rax_RegL val, kReg ktmp, Universe dummy, rFlagsReg cr)\n@@ -10726,2 +10976,3 @@\n-  predicate(!((ClearArrayNode*)n)->is_large() && (MaxVectorSize >= 32) && VM_Version::supports_avx512vl());\n-  match(Set dummy (ClearArray cnt base));\n+  predicate(!((ClearArrayNode*)n)->is_large() && !((ClearArrayNode*)n)->word_copy_only() &&\n+            ((MaxVectorSize >= 32) && VM_Version::supports_avx512vl()));\n+  match(Set dummy (ClearArray (Binary cnt base) val));\n@@ -10729,1 +10980,1 @@\n-  effect(TEMP tmp, TEMP zero, TEMP ktmp, KILL cr);\n+  effect(TEMP tmp, USE_KILL val, TEMP ktmp, KILL cr);\n@@ -10732,1 +10983,1 @@\n-   __ clear_mem($base$$Register, $cnt$$constant, $zero$$Register, $tmp$$XMMRegister, $ktmp$$KRegister);\n+    __ clear_mem($base$$Register, $cnt$$constant, $val$$Register, $tmp$$XMMRegister, $ktmp$$KRegister);\n@@ -12535,0 +12786,15 @@\n+\/\/ entry point is null, target holds the address to call\n+instruct CallLeafNoFPInDirect(rRegP target)\n+%{\n+  predicate(n->as_Call()->entry_point() == nullptr);\n+  match(CallLeafNoFP target);\n+\n+  ins_cost(300);\n+  format %{ \"call_leaf_nofp,runtime indirect \" %}\n+  ins_encode %{\n+     __ call($target$$Register);\n+  %}\n+\n+  ins_pipe(pipe_slow);\n+%}\n+\n@@ -12537,0 +12803,1 @@\n+  predicate(n->as_Call()->entry_point() != nullptr);\n","filename":"src\/hotspot\/cpu\/x86\/x86_64.ad","additions":349,"deletions":82,"binary":false,"changes":431,"status":"modified"},{"patch":"@@ -33,0 +33,2 @@\n+#include \"ci\/ciFlatArrayKlass.hpp\"\n+#include \"ci\/ciInlineKlass.hpp\"\n@@ -1066,1 +1068,9 @@\n-  ValueStack* state_before = copy_state_indexed_access();\n+  ValueStack* state_before = nullptr;\n+  int array_idx = state()->stack_size() - 2;\n+  if (type == T_OBJECT && state()->stack_at(array_idx)->maybe_flat_array()) {\n+    \/\/ Save the entire state and re-execute on deopt when accessing flat arrays\n+    state_before = copy_state_before();\n+    state_before->set_should_reexecute(true);\n+  } else {\n+    state_before = copy_state_indexed_access();\n+  }\n@@ -1078,1 +1088,61 @@\n-  push(as_ValueType(type), append(new LoadIndexed(array, index, length, type, state_before)));\n+\n+  bool need_membar = false;\n+  LoadIndexed* load_indexed = nullptr;\n+  Instruction* result = nullptr;\n+  if (array->is_loaded_flat_array()) {\n+    ciType* array_type = array->declared_type();\n+    ciInlineKlass* elem_klass = array_type->as_flat_array_klass()->element_klass()->as_inline_klass();\n+\n+    bool can_delay_access = false;\n+    ciBytecodeStream s(method());\n+    s.force_bci(bci());\n+    s.next();\n+    if (s.cur_bc() == Bytecodes::_getfield) {\n+      bool will_link;\n+      ciField* next_field = s.get_field(will_link);\n+      bool next_needs_patching = !next_field->holder()->is_initialized() ||\n+                                 !next_field->will_link(method(), Bytecodes::_getfield) ||\n+                                 PatchALot;\n+      can_delay_access = C1UseDelayedFlattenedFieldReads && !next_needs_patching;\n+    }\n+    if (can_delay_access) {\n+      \/\/ potentially optimizable array access, storing information for delayed decision\n+      LoadIndexed* li = new LoadIndexed(array, index, length, type, state_before);\n+      DelayedLoadIndexed* dli = new DelayedLoadIndexed(li, state_before);\n+      li->set_delayed(dli);\n+      set_pending_load_indexed(dli);\n+      return; \/\/ Nothing else to do for now\n+    } else {\n+      if (elem_klass->is_empty()) {\n+        \/\/ No need to create a new instance, the default instance will be used instead\n+        load_indexed = new LoadIndexed(array, index, length, type, state_before);\n+        apush(append(load_indexed));\n+      } else {\n+        NewInstance* new_instance = new NewInstance(elem_klass, state_before, false, true);\n+        _memory->new_instance(new_instance);\n+        apush(append_split(new_instance));\n+        load_indexed = new LoadIndexed(array, index, length, type, state_before);\n+        load_indexed->set_vt(new_instance);\n+        \/\/ The LoadIndexed node will initialise this instance by copying from\n+        \/\/ the flat field.  Ensure these stores are visible before any\n+        \/\/ subsequent store that publishes this reference.\n+        need_membar = true;\n+      }\n+    }\n+  } else {\n+    load_indexed = new LoadIndexed(array, index, length, type, state_before);\n+    if (profile_array_accesses() && is_reference_type(type)) {\n+      compilation()->set_would_profile(true);\n+      load_indexed->set_should_profile(true);\n+      load_indexed->set_profiled_method(method());\n+      load_indexed->set_profiled_bci(bci());\n+    }\n+  }\n+  result = append(load_indexed);\n+  if (need_membar) {\n+    append(new MemBar(lir_membar_storestore));\n+  }\n+  assert(!load_indexed->should_profile() || load_indexed == result, \"should not be optimized out\");\n+  if (!array->is_loaded_flat_array()) {\n+    push(as_ValueType(type), result);\n+  }\n@@ -1084,1 +1154,9 @@\n-  ValueStack* state_before = copy_state_indexed_access();\n+  ValueStack* state_before = nullptr;\n+  int array_idx = state()->stack_size() - 3;\n+  if (type == T_OBJECT && state()->stack_at(array_idx)->maybe_flat_array()) {\n+    \/\/ Save the entire state and re-execute on deopt when accessing flat arrays\n+    state_before = copy_state_before();\n+    state_before->set_should_reexecute(true);\n+  } else {\n+    state_before = copy_state_indexed_access();\n+  }\n@@ -1109,5 +1187,2 @@\n-  StoreIndexed* result = new StoreIndexed(array, index, length, type, value, state_before, check_boolean);\n-  append(result);\n-  _memory->store_value(value);\n-  if (type == T_OBJECT && is_profiling()) {\n-    \/\/ Note that we'd collect profile data in this method if we wanted it.\n+  StoreIndexed* store_indexed = new StoreIndexed(array, index, length, type, value, state_before, check_boolean);\n+  if (profile_array_accesses() && is_reference_type(type) && !array->is_loaded_flat_array()) {\n@@ -1116,6 +1191,3 @@\n-\n-    if (profile_checkcasts()) {\n-      result->set_profiled_method(method());\n-      result->set_profiled_bci(bci());\n-      result->set_should_profile(true);\n-    }\n+    store_indexed->set_should_profile(true);\n+    store_indexed->set_profiled_method(method());\n+    store_indexed->set_profiled_bci(bci());\n@@ -1123,0 +1195,3 @@\n+  Instruction* result = append(store_indexed);\n+  assert(!store_indexed->should_profile() || store_indexed == result, \"should not be optimized out\");\n+  _memory->store_value(value);\n@@ -1125,1 +1200,0 @@\n-\n@@ -1129,1 +1203,1 @@\n-      { state()->raw_pop();\n+      { Value w = state()->raw_pop();\n@@ -1133,2 +1207,2 @@\n-      { state()->raw_pop();\n-        state()->raw_pop();\n+      { Value w1 = state()->raw_pop();\n+        Value w2 = state()->raw_pop();\n@@ -1313,0 +1387,27 @@\n+\n+  bool subst_check = false;\n+  if (EnableValhalla && (stream()->cur_bc() == Bytecodes::_if_acmpeq || stream()->cur_bc() == Bytecodes::_if_acmpne)) {\n+    ValueType* left_vt = x->type();\n+    ValueType* right_vt = y->type();\n+    if (left_vt->is_object()) {\n+      assert(right_vt->is_object(), \"must be\");\n+      ciKlass* left_klass = x->as_loaded_klass_or_null();\n+      ciKlass* right_klass = y->as_loaded_klass_or_null();\n+\n+      if (left_klass == nullptr || right_klass == nullptr) {\n+        \/\/ The klass is still unloaded, or came from a Phi node. Go slow case;\n+        subst_check = true;\n+      } else if (left_klass->can_be_inline_klass() || right_klass->can_be_inline_klass()) {\n+        \/\/ Either operand may be a value object, but we're not sure. Go slow case;\n+        subst_check = true;\n+      } else {\n+        \/\/ No need to do substitutability check\n+      }\n+    }\n+  }\n+  if ((stream()->cur_bc() == Bytecodes::_if_acmpeq || stream()->cur_bc() == Bytecodes::_if_acmpne) &&\n+      is_profiling() && profile_branches()) {\n+    compilation()->set_would_profile(true);\n+    append(new ProfileACmpTypes(method(), bci(), x, y));\n+  }\n+\n@@ -1315,1 +1416,1 @@\n-  Instruction *i = append(new If(x, cond, false, y, tsux, fsux, (is_bb || compilation()->is_optimistic()) ? state_before : nullptr, is_bb));\n+  Instruction *i = append(new If(x, cond, false, y, tsux, fsux, (is_bb || compilation()->is_optimistic() || subst_check) ? state_before : nullptr, is_bb, subst_check));\n@@ -1570,1 +1671,1 @@\n-  if (method()->name() == ciSymbols::object_initializer_name() &&\n+  if (method()->is_object_constructor() &&\n@@ -1721,0 +1822,13 @@\n+void GraphBuilder::copy_inline_content(ciInlineKlass* vk, Value src, int src_off, Value dest, int dest_off, ValueStack* state_before, ciField* enclosing_field) {\n+  for (int i = 0; i < vk->nof_nonstatic_fields(); i++) {\n+    ciField* inner_field = vk->nonstatic_field_at(i);\n+    assert(!inner_field->is_flat(), \"the iteration over nested fields is handled by the loop itself\");\n+    int off = inner_field->offset_in_bytes() - vk->first_field_offset();\n+    LoadField* load = new LoadField(src, src_off + off, inner_field, false, state_before, false);\n+    Value replacement = append(load);\n+    StoreField* store = new StoreField(dest, dest_off + off, inner_field, replacement, false, state_before, false);\n+    store->set_enclosing_field(enclosing_field);\n+    append(store);\n+  }\n+}\n+\n@@ -1727,0 +1841,1 @@\n+\n@@ -1730,1 +1845,1 @@\n-                              PatchALot;\n+                              (!field->is_flat() && PatchALot);\n@@ -1762,1 +1877,1 @@\n-  const int offset = !needs_patching ? field->offset_in_bytes() : -1;\n+  int offset = !needs_patching ? field->offset_in_bytes() : -1;\n@@ -1772,0 +1887,4 @@\n+      } else if (field->is_null_free() && field->type()->as_instance_klass()->is_initialized() &&\n+                 field->type()->as_inline_klass()->is_empty()) {\n+        \/\/ Loading from a field of an empty inline type. Just return the default instance.\n+        constant = new Constant(new InstanceConstant(field->type()->as_inline_klass()->default_instance()));\n@@ -1779,2 +1898,3 @@\n-        push(type, append(new LoadField(append(obj), offset, field, true,\n-                                        state_before, needs_patching)));\n+        LoadField* load_field = new LoadField(append(obj), offset, field, true,\n+                                        state_before, needs_patching);\n+        push(type, append(load_field));\n@@ -1789,1 +1909,1 @@\n-      if (field->type()->basic_type() == T_BOOLEAN) {\n+      if (field_type == T_BOOLEAN) {\n@@ -1793,0 +1913,7 @@\n+      if (field->is_null_free()) {\n+        null_check(val);\n+      }\n+      if (field->is_null_free() && field->type()->is_loaded() && field->type()->as_inline_klass()->is_empty()) {\n+        \/\/ Storing to a field of an empty inline type. Ignore.\n+        break;\n+      }\n@@ -1799,14 +1926,31 @@\n-      obj = apop();\n-      ObjectType* obj_type = obj->type()->as_ObjectType();\n-      if (field->is_constant() && obj_type->is_constant() && !PatchALot) {\n-        ciObject* const_oop = obj_type->constant_value();\n-        if (!const_oop->is_null_object() && const_oop->is_loaded()) {\n-          ciConstant field_value = field->constant_value_of(const_oop);\n-          if (field_value.is_valid()) {\n-            constant = make_constant(field_value, field);\n-            \/\/ For CallSite objects add a dependency for invalidation of the optimization.\n-            if (field->is_call_site_target()) {\n-              ciCallSite* call_site = const_oop->as_call_site();\n-              if (!call_site->is_fully_initialized_constant_call_site()) {\n-                ciMethodHandle* target = field_value.as_object()->as_method_handle();\n-                dependency_recorder()->assert_call_site_target_value(call_site, target);\n+      if (state_before == nullptr && field->is_flat()) {\n+        \/\/ Save the entire state and re-execute on deopt when accessing flat fields\n+        assert(Interpreter::bytecode_should_reexecute(code), \"should reexecute\");\n+        state_before = copy_state_before();\n+      }\n+      if (!has_pending_field_access() && !has_pending_load_indexed()) {\n+        obj = apop();\n+        ObjectType* obj_type = obj->type()->as_ObjectType();\n+        if (field->is_null_free() && field->type()->as_instance_klass()->is_initialized()\n+            && field->type()->as_inline_klass()->is_empty()) {\n+          \/\/ Loading from a field of an empty inline type. Just return the default instance.\n+          null_check(obj);\n+          constant = new Constant(new InstanceConstant(field->type()->as_inline_klass()->default_instance()));\n+        } else if (field->is_constant() && !field->is_flat() && obj_type->is_constant() && !PatchALot) {\n+          ciObject* const_oop = obj_type->constant_value();\n+          if (!const_oop->is_null_object() && const_oop->is_loaded()) {\n+            ciConstant field_value = field->constant_value_of(const_oop);\n+            if (field_value.is_valid()) {\n+              if (field->is_null_free() && field_value.is_null_or_zero()) {\n+                \/\/ Non-flat inline type field. Replace null by the default value.\n+                constant = new Constant(new InstanceConstant(field->type()->as_inline_klass()->default_instance()));\n+              } else {\n+                constant = make_constant(field_value, field);\n+              }\n+              \/\/ For CallSite objects add a dependency for invalidation of the optimization.\n+              if (field->is_call_site_target()) {\n+                ciCallSite* call_site = const_oop->as_call_site();\n+                if (!call_site->is_fully_initialized_constant_call_site()) {\n+                  ciMethodHandle* target = field_value.as_object()->as_method_handle();\n+                  dependency_recorder()->assert_call_site_target_value(call_site, target);\n+                }\n@@ -1824,19 +1968,15 @@\n-        LoadField* load = new LoadField(obj, offset, field, false, state_before, needs_patching);\n-        Value replacement = !needs_patching ? _memory->load(load) : load;\n-        if (replacement != load) {\n-          assert(replacement->is_linked() || !replacement->can_be_linked(), \"should already by linked\");\n-          \/\/ Writing an (integer) value to a boolean, byte, char or short field includes an implicit narrowing\n-          \/\/ conversion. Emit an explicit conversion here to get the correct field value after the write.\n-          BasicType bt = field->type()->basic_type();\n-          switch (bt) {\n-          case T_BOOLEAN:\n-          case T_BYTE:\n-            replacement = append(new Convert(Bytecodes::_i2b, replacement, as_ValueType(bt)));\n-            break;\n-          case T_CHAR:\n-            replacement = append(new Convert(Bytecodes::_i2c, replacement, as_ValueType(bt)));\n-            break;\n-          case T_SHORT:\n-            replacement = append(new Convert(Bytecodes::_i2s, replacement, as_ValueType(bt)));\n-            break;\n-          default:\n+        if (!field->is_flat()) {\n+          if (has_pending_field_access()) {\n+            assert(!needs_patching, \"Can't patch delayed field access\");\n+            obj = pending_field_access()->obj();\n+            offset += pending_field_access()->offset() - field->holder()->as_inline_klass()->first_field_offset();\n+            field = pending_field_access()->holder()->get_field_by_offset(offset, false);\n+            assert(field != nullptr, \"field not found\");\n+            set_pending_field_access(nullptr);\n+          } else if (has_pending_load_indexed()) {\n+            assert(!needs_patching, \"Can't patch delayed field access\");\n+            pending_load_indexed()->update(field, offset - field->holder()->as_inline_klass()->first_field_offset());\n+            LoadIndexed* li = pending_load_indexed()->load_instr();\n+            li->set_type(type);\n+            push(type, append(li));\n+            set_pending_load_indexed(nullptr);\n@@ -1845,3 +1985,94 @@\n-          push(type, replacement);\n-        } else {\n-          push(type, append(load));\n+          LoadField* load = new LoadField(obj, offset, field, false, state_before, needs_patching);\n+          Value replacement = !needs_patching ? _memory->load(load) : load;\n+          if (replacement != load) {\n+            assert(replacement->is_linked() || !replacement->can_be_linked(), \"should already by linked\");\n+            \/\/ Writing an (integer) value to a boolean, byte, char or short field includes an implicit narrowing\n+            \/\/ conversion. Emit an explicit conversion here to get the correct field value after the write.\n+            switch (field_type) {\n+            case T_BOOLEAN:\n+            case T_BYTE:\n+              replacement = append(new Convert(Bytecodes::_i2b, replacement, type));\n+              break;\n+            case T_CHAR:\n+              replacement = append(new Convert(Bytecodes::_i2c, replacement, type));\n+              break;\n+            case T_SHORT:\n+              replacement = append(new Convert(Bytecodes::_i2s, replacement, type));\n+              break;\n+            default:\n+              break;\n+            }\n+            push(type, replacement);\n+          } else {\n+            push(type, append(load));\n+          }\n+        } else {  \/\/ field is flat\n+          \/\/ Look at the next bytecode to check if we can delay the field access\n+          bool can_delay_access = false;\n+          ciBytecodeStream s(method());\n+          s.force_bci(bci());\n+          s.next();\n+          if (s.cur_bc() == Bytecodes::_getfield && !needs_patching) {\n+            ciField* next_field = s.get_field(will_link);\n+            bool next_needs_patching = !next_field->holder()->is_loaded() ||\n+                                       !next_field->will_link(method(), Bytecodes::_getfield) ||\n+                                       PatchALot;\n+            can_delay_access = C1UseDelayedFlattenedFieldReads && !next_needs_patching;\n+          }\n+          if (can_delay_access) {\n+            if (has_pending_load_indexed()) {\n+              pending_load_indexed()->update(field, offset - field->holder()->as_inline_klass()->first_field_offset());\n+            } else if (has_pending_field_access()) {\n+              pending_field_access()->inc_offset(offset - field->holder()->as_inline_klass()->first_field_offset());\n+            } else {\n+              null_check(obj);\n+              DelayedFieldAccess* dfa = new DelayedFieldAccess(obj, field->holder(), field->offset_in_bytes(), state_before);\n+              set_pending_field_access(dfa);\n+            }\n+          } else {\n+            ciInlineKlass* inline_klass = field->type()->as_inline_klass();\n+            scope()->set_wrote_final();\n+            scope()->set_wrote_fields();\n+            bool need_membar = false;\n+            if (inline_klass->is_initialized() && inline_klass->is_empty()) {\n+              apush(append(new Constant(new InstanceConstant(inline_klass->default_instance()))));\n+              if (has_pending_field_access()) {\n+                set_pending_field_access(nullptr);\n+              } else if (has_pending_load_indexed()) {\n+                set_pending_load_indexed(nullptr);\n+              }\n+            } else if (has_pending_load_indexed()) {\n+              assert(!needs_patching, \"Can't patch delayed field access\");\n+              pending_load_indexed()->update(field, offset - field->holder()->as_inline_klass()->first_field_offset());\n+              NewInstance* vt = new NewInstance(inline_klass, pending_load_indexed()->state_before(), false, true);\n+              _memory->new_instance(vt);\n+              pending_load_indexed()->load_instr()->set_vt(vt);\n+              apush(append_split(vt));\n+              append(pending_load_indexed()->load_instr());\n+              set_pending_load_indexed(nullptr);\n+              need_membar = true;\n+            } else {\n+              if (has_pending_field_access()) {\n+                state_before = pending_field_access()->state_before();\n+              }\n+              NewInstance* new_instance = new NewInstance(inline_klass, state_before, false, true);\n+              _memory->new_instance(new_instance);\n+              apush(append_split(new_instance));\n+              assert(!needs_patching, \"Can't patch flat inline type field access\");\n+              if (has_pending_field_access()) {\n+                copy_inline_content(inline_klass, pending_field_access()->obj(),\n+                                    pending_field_access()->offset() + field->offset_in_bytes() - field->holder()->as_inline_klass()->first_field_offset(),\n+                                    new_instance, inline_klass->first_field_offset(), state_before);\n+                set_pending_field_access(nullptr);\n+              } else {\n+                copy_inline_content(inline_klass, obj, field->offset_in_bytes(), new_instance, inline_klass->first_field_offset(), state_before);\n+              }\n+              need_membar = true;\n+            }\n+            if (need_membar) {\n+              \/\/ If we allocated a new instance ensure the stores to copy the\n+              \/\/ field contents are visible before any subsequent store that\n+              \/\/ publishes this reference.\n+              append(new MemBar(lir_membar_storestore));\n+            }\n+          }\n@@ -1858,1 +2089,1 @@\n-      if (field->type()->basic_type() == T_BOOLEAN) {\n+      if (field_type == T_BOOLEAN) {\n@@ -1862,4 +2093,17 @@\n-      StoreField* store = new StoreField(obj, offset, field, val, false, state_before, needs_patching);\n-      if (!needs_patching) store = _memory->store(store);\n-      if (store != nullptr) {\n-        append(store);\n+      if (field->is_null_free() && field->type()->is_loaded() && field->type()->as_inline_klass()->is_empty()) {\n+        \/\/ Storing to a field of an empty inline type. Ignore.\n+        null_check(obj);\n+        null_check(val);\n+      } else if (!field->is_flat()) {\n+        if (field->is_null_free()) {\n+          null_check(val);\n+        }\n+        StoreField* store = new StoreField(obj, offset, field, val, false, state_before, needs_patching);\n+        if (!needs_patching) store = _memory->store(store);\n+        if (store != nullptr) {\n+          append(store);\n+        }\n+      } else {\n+        assert(!needs_patching, \"Can't patch flat inline type field access\");\n+        ciInlineKlass* inline_klass = field->type()->as_inline_klass();\n+        copy_inline_content(inline_klass, val, inline_klass->first_field_offset(), obj, offset, state_before, field);\n@@ -1875,1 +2119,0 @@\n-\n@@ -1992,1 +2235,1 @@\n-    } else if (bc_raw == Bytecodes::_invokespecial && !target->is_object_initializer() && calling_klass->is_interface()) {\n+    } else if (bc_raw == Bytecodes::_invokespecial && !target->is_object_constructor() && calling_klass->is_interface()) {\n@@ -2248,3 +2491,9 @@\n-  NewInstance* new_instance = new NewInstance(klass->as_instance_klass(), state_before, stream()->is_unresolved_klass());\n-  _memory->new_instance(new_instance);\n-  apush(append_split(new_instance));\n+  if (!stream()->is_unresolved_klass() && klass->is_inlinetype() &&\n+      klass->as_inline_klass()->is_initialized() && klass->as_inline_klass()->is_empty()) {\n+    ciInlineKlass* vk = klass->as_inline_klass();\n+    apush(append(new Constant(new InstanceConstant(vk->default_instance()))));\n+  } else {\n+    NewInstance* new_instance = new NewInstance(klass->as_instance_klass(), state_before, stream()->is_unresolved_klass(), false);\n+    _memory->new_instance(new_instance);\n+    apush(append_split(new_instance));\n+  }\n@@ -2253,1 +2502,0 @@\n-\n@@ -2326,0 +2574,19 @@\n+  bool maybe_inlinetype = false;\n+  if (bci == InvocationEntryBci) {\n+    \/\/ Called by GraphBuilder::inline_sync_entry.\n+#ifdef ASSERT\n+    ciType* obj_type = x->declared_type();\n+    assert(obj_type == nullptr || !obj_type->is_inlinetype(), \"inline types cannot have synchronized methods\");\n+#endif\n+  } else {\n+    \/\/ We are compiling a monitorenter bytecode\n+    if (EnableValhalla) {\n+      ciType* obj_type = x->declared_type();\n+      if (obj_type == nullptr || obj_type->as_klass()->can_be_inline_klass()) {\n+        \/\/ If we're (possibly) locking on an inline type, check for markWord::always_locked_pattern\n+        \/\/ and throw IMSE. (obj_type is null for Phi nodes, so let's just be conservative).\n+        maybe_inlinetype = true;\n+      }\n+    }\n+  }\n+\n@@ -2328,1 +2595,1 @@\n-  append_with_bci(new MonitorEnter(x, state()->lock(x), state_before), bci);\n+  append_with_bci(new MonitorEnter(x, state()->lock(x), state_before, maybe_inlinetype), bci);\n@@ -2476,0 +2743,1 @@\n+    if (value->is_null_free()) return;\n@@ -2501,1 +2769,3 @@\n-    assert(cur_bci == SynchronizationEntryBCI || cur_bci == cur_scope_data->stream()->cur_bci(), \"invalid bci\");\n+    assert(cur_bci == SynchronizationEntryBCI || cur_bci == cur_scope_data->stream()->cur_bci()\n+           || has_pending_field_access() || has_pending_load_indexed(), \"invalid bci\");\n+\n@@ -3283,1 +3553,2 @@\n-    state->store_local(idx, new Local(method()->holder(), objectType, idx, true));\n+    state->store_local(idx, new Local(method()->holder(), objectType, idx,\n+             \/*receiver*\/ true, \/*null_free*\/ method()->holder()->is_flat_array_klass()));\n@@ -3295,1 +3566,1 @@\n-    state->store_local(idx, new Local(type, vt, idx, false));\n+    state->store_local(idx, new Local(type, vt, idx, false, false));\n@@ -3315,0 +3586,2 @@\n+  , _pending_field_access(nullptr)\n+  , _pending_load_indexed(nullptr)\n","filename":"src\/hotspot\/share\/c1\/c1_GraphBuilder.cpp","additions":349,"deletions":76,"binary":false,"changes":425,"status":"modified"},{"patch":"@@ -512,1 +512,5 @@\n-  if (k->local_interfaces()->length() != _interfaces->length()) {\n+  const int actual_num_interfaces = k->local_interfaces()->length();\n+  const int specified_num_interfaces = _interfaces->length(); \/\/ specified in classlist\n+  int expected_num_interfaces = actual_num_interfaces;\n+\n+  if (specified_num_interfaces != expected_num_interfaces) {\n@@ -516,1 +520,1 @@\n-          _interfaces->length(), k->local_interfaces()->length());\n+          specified_num_interfaces, expected_num_interfaces);\n","filename":"src\/hotspot\/share\/cds\/classListParser.cpp","additions":6,"deletions":2,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -84,0 +84,65 @@\n+inline void CDSMustMatchFlags::do_print(outputStream* st, bool v) {\n+  st->print(\"%s\", v ? \"true\" : \"false\");\n+}\n+\n+inline void CDSMustMatchFlags::do_print(outputStream* st, intx v) {\n+  st->print(INTX_FORMAT, v);\n+}\n+\n+inline void CDSMustMatchFlags::do_print(outputStream* st, uintx v) {\n+  st->print(UINTX_FORMAT, v);\n+}\n+\n+inline void CDSMustMatchFlags::do_print(outputStream* st, double v) {\n+  st->print(\"%f\", v);\n+}\n+\n+void CDSMustMatchFlags::init() {\n+  assert(CDSConfig::is_dumping_archive(), \"sanity\");\n+  _max_name_width = 0;\n+\n+#define INIT_CDS_MUST_MATCH_FLAG(n) \\\n+  _v_##n = n; \\\n+  _max_name_width = MAX2(_max_name_width,strlen(#n));\n+  CDS_MUST_MATCH_FLAGS_DO(INIT_CDS_MUST_MATCH_FLAG);\n+#undef INIT_CDS_MUST_MATCH_FLAG\n+}\n+\n+bool CDSMustMatchFlags::runtime_check() const {\n+#define CHECK_CDS_MUST_MATCH_FLAG(n) \\\n+  if (_v_##n != n) { \\\n+    ResourceMark rm; \\\n+    stringStream ss; \\\n+    ss.print(\"VM option %s is different between dumptime (\", #n);  \\\n+    do_print(&ss, _v_ ## n); \\\n+    ss.print(\") and runtime (\"); \\\n+    do_print(&ss, n); \\\n+    ss.print(\")\"); \\\n+    log_info(cds)(\"%s\", ss.as_string()); \\\n+    return false; \\\n+  }\n+  CDS_MUST_MATCH_FLAGS_DO(CHECK_CDS_MUST_MATCH_FLAG);\n+#undef CHECK_CDS_MUST_MATCH_FLAG\n+\n+  return true;\n+}\n+\n+void CDSMustMatchFlags::print_info() const {\n+  LogTarget(Info, cds) lt;\n+  if (lt.is_enabled()) {\n+    LogStream ls(lt);\n+    ls.print_cr(\"Recorded VM flags during dumptime:\");\n+    print(&ls);\n+  }\n+}\n+\n+void CDSMustMatchFlags::print(outputStream* st) const {\n+#define PRINT_CDS_MUST_MATCH_FLAG(n) \\\n+  st->print(\"- %-s \", #n);                   \\\n+  st->sp(int(_max_name_width - strlen(#n))); \\\n+  do_print(st, _v_##n);                      \\\n+  st->cr();\n+  CDS_MUST_MATCH_FLAGS_DO(PRINT_CDS_MUST_MATCH_FLAG);\n+#undef PRINT_CDS_MUST_MATCH_FLAG\n+}\n+\n@@ -218,1 +283,1 @@\n-\n+  _has_valhalla_patched_classes = CDSConfig::is_valhalla_preview();\n@@ -238,0 +303,1 @@\n+  _must_match.init();\n@@ -300,0 +366,2 @@\n+  st->print_cr(\"- has_valhalla_patched_classes    %d\", _has_valhalla_patched_classes);\n+  _must_match.print(st);\n@@ -1354,0 +1422,4 @@\n+  if (!header()->check_must_match_flags()) {\n+    return false;\n+  }\n+\n@@ -2462,0 +2534,18 @@\n+  if (is_static()) {\n+    const char* err = nullptr;\n+    if (CDSConfig::is_valhalla_preview()) {\n+      if (!_has_valhalla_patched_classes) {\n+        err = \"not created\";\n+      }\n+    } else {\n+      if (_has_valhalla_patched_classes) {\n+        err = \"created\";\n+      }\n+    }\n+    if (err != nullptr) {\n+      log_warning(cds)(\"This archive was %s with --enable-preview -XX:+EnableValhalla. It is \"\n+                         \"incompatible with the current JVM setting\", err);\n+      return false;\n+    }\n+  }\n+\n","filename":"src\/hotspot\/share\/cds\/filemap.cpp","additions":91,"deletions":1,"binary":false,"changes":92,"status":"modified"},{"patch":"@@ -1560,0 +1560,7 @@\n+\n+    if (CDSConfig::is_valhalla_preview() && strcmp(klass_name, \"jdk\/internal\/module\/ArchivedModuleGraph\") == 0) {\n+      \/\/ FIXME -- ArchivedModuleGraph doesn't work when java.base is patched with valhalla classes.\n+      i++;\n+      continue;\n+    }\n+\n","filename":"src\/hotspot\/share\/cds\/heapShared.cpp","additions":7,"deletions":0,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -27,0 +27,1 @@\n+#include \"ci\/ciInlineKlass.hpp\"\n@@ -31,0 +32,1 @@\n+#include \"classfile\/systemDictionary.hpp\"\n@@ -39,0 +41,1 @@\n+#include \"oops\/inlineKlass.inline.hpp\"\n@@ -122,2 +125,3 @@\n-                                 jobject loader, jobject protection_domain)\n-  : ciKlass(name, T_OBJECT)\n+                                 jobject loader, jobject protection_domain,\n+                                 BasicType bt)\n+  : ciKlass(name, bt)\n@@ -128,1 +132,1 @@\n-  _nonstatic_fields = nullptr;\n+  _nonstatic_fields = nullptr;         \/\/ initialized lazily by compute_nonstatic_fields\n@@ -340,1 +344,1 @@\n-    _flags.print_klass_flags();\n+    _flags.print_klass_flags(st);\n@@ -344,1 +348,1 @@\n-      _super->print_name();\n+      _super->print_name_on(st);\n@@ -438,0 +442,23 @@\n+ciField* ciInstanceKlass::get_non_flat_field_by_offset(int field_offset) {\n+  if (super() != nullptr && super()->has_nonstatic_fields()) {\n+    ciField* f = super()->get_non_flat_field_by_offset(field_offset);\n+    if (f != nullptr) {\n+      return f;\n+    }\n+  }\n+\n+  VM_ENTRY_MARK;\n+  InstanceKlass* k = get_instanceKlass();\n+  Arena* arena = CURRENT_ENV->arena();\n+  for (JavaFieldStream fs(k); !fs.done(); fs.next()) {\n+    if (fs.access_flags().is_static())  continue;\n+    fieldDescriptor& fd = fs.field_descriptor();\n+    if (fd.offset() == field_offset) {\n+      ciField* f = new (arena) ciField(&fd);\n+      return f;\n+    }\n+  }\n+\n+  return nullptr;\n+}\n+\n@@ -496,6 +523,1 @@\n-  int flen = fields->length();\n-\n-  \/\/ Now sort them by offset, ascending.\n-  \/\/ (In principle, they could mix with superclass fields.)\n-  fields->sort(sort_field_by_offset);\n-  return flen;\n+  return fields->length();\n@@ -505,3 +527,1 @@\n-GrowableArray<ciField*>*\n-ciInstanceKlass::compute_nonstatic_fields_impl(GrowableArray<ciField*>*\n-                                               super_fields) {\n+GrowableArray<ciField*>* ciInstanceKlass::compute_nonstatic_fields_impl(GrowableArray<ciField*>* super_fields, bool is_flat) {\n@@ -533,2 +553,22 @@\n-    ciField* field = new (arena) ciField(&fd);\n-    fields->append(field);\n+    if (fd.is_flat() && is_flat) {\n+      \/\/ Inline type fields are embedded\n+      int field_offset = fd.offset();\n+      \/\/ Get InlineKlass and adjust number of fields\n+      Klass* k = get_instanceKlass()->get_inline_type_field_klass(fd.index());\n+      ciInlineKlass* vk = CURRENT_ENV->get_klass(k)->as_inline_klass();\n+      flen += vk->nof_nonstatic_fields() - 1;\n+      \/\/ Iterate over fields of the flat inline type and copy them to 'this'\n+      for (int i = 0; i < vk->nof_nonstatic_fields(); ++i) {\n+        ciField* flat_field = vk->nonstatic_field_at(i);\n+        \/\/ Adjust offset to account for missing oop header\n+        int offset = field_offset + (flat_field->offset_in_bytes() - vk->first_field_offset());\n+        \/\/ A flat field can be treated as final if the non-flat\n+        \/\/ field is declared final or the holder klass is an inline type itself.\n+        bool is_final = fd.is_final() || is_inlinetype();\n+        ciField* field = new (arena) ciField(flat_field, this, offset, is_final);\n+        fields->append(field);\n+      }\n+    } else {\n+      ciField* field = new (arena) ciField(&fd);\n+      fields->append(field);\n+    }\n@@ -537,0 +577,3 @@\n+  \/\/ Now sort them by offset, ascending.\n+  \/\/ (In principle, they could mix with superclass fields.)\n+  fields->sort(sort_field_by_offset);\n@@ -643,0 +686,16 @@\n+bool ciInstanceKlass::can_be_inline_klass(bool is_exact) {\n+  if (!EnableValhalla) {\n+    return false;\n+  }\n+  if (!is_loaded() || is_inlinetype()) {\n+    \/\/ Not loaded or known to be an inline klass\n+    return true;\n+  }\n+  if (!is_exact) {\n+    \/\/ Not exact, check if this is a valid super for an inline klass\n+    VM_ENTRY_MARK;\n+    return !get_instanceKlass()->access_flags().is_identity_class() || is_java_lang_Object() ;\n+  }\n+  return false;\n+}\n+\n@@ -651,1 +710,2 @@\n-class StaticFinalFieldPrinter : public FieldClosure {\n+class StaticFieldPrinter : public FieldClosure {\n+protected:\n@@ -653,0 +713,8 @@\n+public:\n+  StaticFieldPrinter(outputStream* out) :\n+    _out(out) {\n+  }\n+  void do_field_helper(fieldDescriptor* fd, oop obj, bool is_flat);\n+};\n+\n+class StaticFinalFieldPrinter : public StaticFieldPrinter {\n@@ -656,2 +724,1 @@\n-    _out(out),\n-    _holder(holder) {\n+    StaticFieldPrinter(out), _holder(holder) {\n@@ -662,46 +729,59 @@\n-      oop mirror = fd->field_holder()->java_mirror();\n-      _out->print(\"staticfield %s %s %s \", _holder, fd->name()->as_quoted_ascii(), fd->signature()->as_quoted_ascii());\n-      BasicType field_type = fd->field_type();\n-      switch (field_type) {\n-        case T_BYTE:    _out->print_cr(\"%d\", mirror->byte_field(fd->offset()));   break;\n-        case T_BOOLEAN: _out->print_cr(\"%d\", mirror->bool_field(fd->offset()));   break;\n-        case T_SHORT:   _out->print_cr(\"%d\", mirror->short_field(fd->offset()));  break;\n-        case T_CHAR:    _out->print_cr(\"%d\", mirror->char_field(fd->offset()));   break;\n-        case T_INT:     _out->print_cr(\"%d\", mirror->int_field(fd->offset()));    break;\n-        case T_LONG:    _out->print_cr(INT64_FORMAT, (int64_t)(mirror->long_field(fd->offset())));   break;\n-        case T_FLOAT: {\n-          float f = mirror->float_field(fd->offset());\n-          _out->print_cr(\"%d\", *(int*)&f);\n-          break;\n-        }\n-        case T_DOUBLE: {\n-          double d = mirror->double_field(fd->offset());\n-          _out->print_cr(INT64_FORMAT, *(int64_t*)&d);\n-          break;\n-        }\n-        case T_ARRAY:  \/\/ fall-through\n-        case T_OBJECT: {\n-          oop value =  mirror->obj_field_acquire(fd->offset());\n-          if (value == nullptr) {\n-            if (field_type == T_ARRAY) {\n-              _out->print(\"%d\", -1);\n-            }\n-            _out->cr();\n-          } else if (value->is_instance()) {\n-            assert(field_type == T_OBJECT, \"\");\n-            if (value->is_a(vmClasses::String_klass())) {\n-              const char* ascii_value = java_lang_String::as_quoted_ascii(value);\n-              _out->print_cr(\"\\\"%s\\\"\", (ascii_value != nullptr) ? ascii_value : \"\");\n-            } else {\n-              const char* klass_name  = value->klass()->name()->as_quoted_ascii();\n-              _out->print_cr(\"%s\", klass_name);\n-            }\n-          } else if (value->is_array()) {\n-            typeArrayOop ta = (typeArrayOop)value;\n-            _out->print(\"%d\", ta->length());\n-            if (value->is_objArray()) {\n-              objArrayOop oa = (objArrayOop)value;\n-              const char* klass_name  = value->klass()->name()->as_quoted_ascii();\n-              _out->print(\" %s\", klass_name);\n-            }\n-            _out->cr();\n+      InstanceKlass* holder = fd->field_holder();\n+      oop mirror = holder->java_mirror();\n+      _out->print(\"staticfield %s %s \", _holder, fd->name()->as_quoted_ascii());\n+      BasicType bt = fd->field_type();\n+      if (bt != T_OBJECT && bt != T_ARRAY) {\n+        _out->print(\"%s \", fd->signature()->as_quoted_ascii());\n+      }\n+      do_field_helper(fd, mirror, false);\n+      _out->cr();\n+    }\n+  }\n+};\n+\n+class InlineTypeFieldPrinter : public StaticFieldPrinter {\n+  oop _obj;\n+public:\n+  InlineTypeFieldPrinter(outputStream* out, oop obj) :\n+    StaticFieldPrinter(out), _obj(obj) {\n+  }\n+  void do_field(fieldDescriptor* fd) {\n+    do_field_helper(fd, _obj, true);\n+    _out->print(\" \");\n+  }\n+};\n+\n+void StaticFieldPrinter::do_field_helper(fieldDescriptor* fd, oop mirror, bool is_flat) {\n+  BasicType field_type = fd->field_type();\n+  switch (field_type) {\n+    case T_BYTE:    _out->print(\"%d\", mirror->byte_field(fd->offset()));   break;\n+    case T_BOOLEAN: _out->print(\"%d\", mirror->bool_field(fd->offset()));   break;\n+    case T_SHORT:   _out->print(\"%d\", mirror->short_field(fd->offset()));  break;\n+    case T_CHAR:    _out->print(\"%d\", mirror->char_field(fd->offset()));   break;\n+    case T_INT:     _out->print(\"%d\", mirror->int_field(fd->offset()));    break;\n+    case T_LONG:    _out->print(INT64_FORMAT, (int64_t)(mirror->long_field(fd->offset())));   break;\n+    case T_FLOAT: {\n+      float f = mirror->float_field(fd->offset());\n+      _out->print(\"%d\", *(int*)&f);\n+      break;\n+    }\n+    case T_DOUBLE: {\n+      double d = mirror->double_field(fd->offset());\n+      _out->print(INT64_FORMAT, *(int64_t*)&d);\n+      break;\n+    }\n+    case T_ARRAY:  \/\/ fall-through\n+    case T_OBJECT:\n+      if (!fd->is_null_free_inline_type()) {\n+        _out->print(\"%s \", fd->signature()->as_quoted_ascii());\n+        oop value =  mirror->obj_field_acquire(fd->offset());\n+        if (value == nullptr) {\n+          if (field_type == T_ARRAY) {\n+            _out->print(\"%d\", -1);\n+          }\n+          _out->cr();\n+        } else if (value->is_instance()) {\n+          assert(field_type == T_OBJECT, \"\");\n+          if (value->is_a(vmClasses::String_klass())) {\n+            const char* ascii_value = java_lang_String::as_quoted_ascii(value);\n+            _out->print(\"\\\"%s\\\"\", (ascii_value != nullptr) ? ascii_value : \"\");\n@@ -709,1 +789,2 @@\n-            ShouldNotReachHere();\n+            const char* klass_name  = value->klass()->name()->as_quoted_ascii();\n+            _out->print(\"%s\", klass_name);\n@@ -711,3 +792,9 @@\n-          break;\n-        }\n-        default:\n+        } else if (value->is_array()) {\n+          typeArrayOop ta = (typeArrayOop)value;\n+          _out->print(\"%d\", ta->length());\n+          if (value->is_objArray() || value->is_flatArray()) {\n+            objArrayOop oa = (objArrayOop)value;\n+            const char* klass_name  = value->klass()->name()->as_quoted_ascii();\n+            _out->print(\" %s\", klass_name);\n+          }\n+        } else {\n@@ -716,1 +803,27 @@\n-    }\n+        break;\n+      } else {\n+        \/\/ handling of null free inline type\n+        ResetNoHandleMark rnhm;\n+        Thread* THREAD = Thread::current();\n+        SignatureStream ss(fd->signature(), false);\n+        Symbol* name = ss.as_symbol();\n+        assert(!HAS_PENDING_EXCEPTION, \"can resolve klass?\");\n+        InstanceKlass* holder = fd->field_holder();\n+        InstanceKlass* k = SystemDictionary::find_instance_klass(THREAD, name,\n+                                                                 Handle(THREAD, holder->class_loader()),\n+                                                                 Handle(THREAD, holder->protection_domain()));\n+        assert(k != nullptr && !HAS_PENDING_EXCEPTION, \"can resolve klass?\");\n+        InlineKlass* vk = InlineKlass::cast(k);\n+        oop obj;\n+        if (is_flat) {\n+          int field_offset = fd->offset() - vk->first_field_offset();\n+          obj = cast_to_oop(cast_from_oop<address>(mirror) + field_offset);\n+        } else {\n+          obj = mirror->obj_field_acquire(fd->offset());\n+        }\n+        InlineTypeFieldPrinter print_field(_out, obj);\n+        vk->do_nonstatic_fields(&print_field);\n+        break;\n+      }\n+    default:\n+      ShouldNotReachHere();\n@@ -718,1 +831,1 @@\n-};\n+}\n","filename":"src\/hotspot\/share\/ci\/ciInstanceKlass.cpp","additions":184,"deletions":71,"binary":false,"changes":255,"status":"modified"},{"patch":"@@ -229,0 +229,9 @@\n+\/\/ ------------------------------------------------------------------\n+\/\/ ciKlass::prototype_header\n+markWord ciKlass::prototype_header() const {\n+  assert(is_loaded(), \"not loaded\");\n+  GUARDED_VM_ENTRY(\n+    return get_Klass()->prototype_header();\n+  )\n+}\n+\n","filename":"src\/hotspot\/share\/ci\/ciKlass.cpp","additions":9,"deletions":0,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -47,1 +47,3 @@\n-  friend class ciSignature;\n+  friend class ciSignature;\n+  friend class ciFlatArrayKlass;\n+  friend class ciArrayKlass;\n@@ -110,0 +112,8 @@\n+  virtual bool can_be_inline_klass(bool is_exact = false) {\n+    return false;\n+  }\n+\n+  virtual bool can_be_inline_array_klass() {\n+    return EnableValhalla && is_java_lang_Object();\n+  }\n+\n@@ -124,0 +134,2 @@\n+  markWord prototype_header() const;\n+\n","filename":"src\/hotspot\/share\/ci\/ciKlass.hpp","additions":13,"deletions":1,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -24,0 +24,2 @@\n+\n+#include \"oops\/inlineKlass.hpp\"\n@@ -54,0 +56,1 @@\n+#include \"oops\/inlineKlass.inline.hpp\"\n@@ -88,0 +91,1 @@\n+#include \"utilities\/stringUtils.hpp\"\n@@ -154,0 +158,2 @@\n+#define CONSTANT_CLASS_DESCRIPTORS        68\n+\n@@ -194,1 +200,1 @@\n-      case JVM_CONSTANT_Class : {\n+      case JVM_CONSTANT_Class: {\n@@ -515,0 +521,3 @@\n+\n+        Symbol* const name = cp->symbol_at(class_index);\n+        const unsigned int name_len = name->utf8_length();\n@@ -724,1 +733,1 @@\n-            } else if (!Signature::is_void_method(signature)) { \/\/ must have void signature.\n+            } else if (!Signature::is_void_method(signature)) {  \/\/ must have void signature.\n@@ -744,2 +753,3 @@\n-            if (ref_kind == JVM_REF_newInvokeSpecial) {\n-              if (name != vmSymbols::object_initializer_name()) {\n+\n+            if (name != vmSymbols::object_initializer_name()) { \/\/ !<init>\n+              if (ref_kind == JVM_REF_newInvokeSpecial) {\n@@ -751,2 +761,10 @@\n-            } else {\n-              if (name == vmSymbols::object_initializer_name()) {\n+            } else { \/\/ <init>\n+              \/\/ The allowed invocation mode of <init> depends on its signature.\n+              \/\/ This test corresponds to verify_invoke_instructions in the verifier.\n+              const int signature_ref_index =\n+                cp->signature_ref_index_at(name_and_type_ref_index);\n+              const Symbol* const signature = cp->symbol_at(signature_ref_index);\n+              if (signature->is_void_method_signature()\n+                  && ref_kind == JVM_REF_newInvokeSpecial) {\n+                \/\/ OK, could be a constructor call\n+              } else {\n@@ -804,4 +822,13 @@\n-\/\/ Side-effects: populates the _local_interfaces field\n-void ClassFileParser::parse_interfaces(const ClassFileStream* const stream,\n-                                       const int itfs_len,\n-                                       ConstantPool* const cp,\n+static void check_identity_and_value_modifiers(ClassFileParser* current, const InstanceKlass* super_type, TRAPS) {\n+  assert(super_type != nullptr,\"Method doesn't support null super type\");\n+  if (super_type->access_flags().is_identity_class() && !current->access_flags().is_identity_class()\n+      && super_type->name() != vmSymbols::java_lang_Object()) {\n+      THROW_MSG(vmSymbols::java_lang_IncompatibleClassChangeError(),\n+                err_msg(\"Value type %s has an identity type as supertype\",\n+                current->class_name()->as_klass_external_name()));\n+  }\n+}\n+\n+void ClassFileParser::parse_interfaces(const ClassFileStream* stream,\n+                                       int itfs_len,\n+                                       ConstantPool* cp,\n@@ -809,0 +836,6 @@\n+                                       \/\/ FIXME: lots of these functions\n+                                       \/\/ declare their parameters as const,\n+                                       \/\/ which adds only noise to the code.\n+                                       \/\/ Remove the spurious const modifiers.\n+                                       \/\/ Many are of the form \"const int x\"\n+                                       \/\/ or \"T* const x\".\n@@ -816,0 +849,1 @@\n+\n@@ -818,3 +852,2 @@\n-    _local_interfaces = MetadataFactory::new_array<InstanceKlass*>(_loader_data, itfs_len, nullptr, CHECK);\n-\n-    int index;\n+    _local_interface_indexes = new GrowableArray<u2>(itfs_len);\n+    int index = 0;\n@@ -823,1 +856,0 @@\n-      Klass* interf;\n@@ -828,30 +860,1 @@\n-      if (cp->tag_at(interface_index).is_klass()) {\n-        interf = cp->resolved_klass_at(interface_index);\n-      } else {\n-        Symbol* const unresolved_klass  = cp->klass_name_at(interface_index);\n-\n-        \/\/ Don't need to check legal name because it's checked when parsing constant pool.\n-        \/\/ But need to make sure it's not an array type.\n-        guarantee_property(unresolved_klass->char_at(0) != JVM_SIGNATURE_ARRAY,\n-                           \"Bad interface name in class file %s\", CHECK);\n-\n-        \/\/ Call resolve on the interface class name with class circularity checking\n-        interf = SystemDictionary::resolve_super_or_fail(_class_name,\n-                                                         unresolved_klass,\n-                                                         Handle(THREAD, _loader_data->class_loader()),\n-                                                         _protection_domain,\n-                                                         false, CHECK);\n-      }\n-\n-      if (!interf->is_interface()) {\n-        THROW_MSG(vmSymbols::java_lang_IncompatibleClassChangeError(),\n-                  err_msg(\"class %s can not implement %s, because it is not an interface (%s)\",\n-                          _class_name->as_klass_external_name(),\n-                          interf->external_name(),\n-                          interf->class_in_module_of_loader()));\n-      }\n-\n-      if (InstanceKlass::cast(interf)->has_nonstatic_concrete_methods()) {\n-        *has_nonstatic_concrete_methods = true;\n-      }\n-      _local_interfaces->at_put(index, InstanceKlass::cast(interf));\n+      _local_interface_indexes->at_put_grow(index, interface_index);\n@@ -869,2 +872,1 @@\n-      const InstanceKlass* const k = _local_interfaces->at(index);\n-      Symbol* interface_name = k->name();\n+      Symbol* interface_name = cp->klass_name_at(_local_interface_indexes->at(index));\n@@ -956,0 +958,3 @@\n+    _jdk_internal_ImplicitlyConstructible,\n+    _jdk_internal_LooselyConsistentValue,\n+    _jdk_internal_NullRestricted,\n@@ -1363,0 +1368,1 @@\n+  STATIC_INLINE,        \/\/ inline type field\n@@ -1368,0 +1374,1 @@\n+  NONSTATIC_INLINE,\n@@ -1387,6 +1394,7 @@\n-  BAD_ALLOCATION_TYPE, \/\/ T_VOID        = 14,\n-  BAD_ALLOCATION_TYPE, \/\/ T_ADDRESS     = 15,\n-  BAD_ALLOCATION_TYPE, \/\/ T_NARROWOOP   = 16,\n-  BAD_ALLOCATION_TYPE, \/\/ T_METADATA    = 17,\n-  BAD_ALLOCATION_TYPE, \/\/ T_NARROWKLASS = 18,\n-  BAD_ALLOCATION_TYPE, \/\/ T_CONFLICT    = 19,\n+  NONSTATIC_OOP,       \/\/ T_PRIMITIVE_OBJECT = 14,\n+  BAD_ALLOCATION_TYPE, \/\/ T_VOID        = 15,\n+  BAD_ALLOCATION_TYPE, \/\/ T_ADDRESS     = 16,\n+  BAD_ALLOCATION_TYPE, \/\/ T_NARROWOOP   = 17,\n+  BAD_ALLOCATION_TYPE, \/\/ T_METADATA    = 18,\n+  BAD_ALLOCATION_TYPE, \/\/ T_NARROWKLASS = 19,\n+  BAD_ALLOCATION_TYPE, \/\/ T_CONFLICT    = 20,\n@@ -1407,6 +1415,7 @@\n-  BAD_ALLOCATION_TYPE, \/\/ T_VOID        = 14,\n-  BAD_ALLOCATION_TYPE, \/\/ T_ADDRESS     = 15,\n-  BAD_ALLOCATION_TYPE, \/\/ T_NARROWOOP   = 16,\n-  BAD_ALLOCATION_TYPE, \/\/ T_METADATA    = 17,\n-  BAD_ALLOCATION_TYPE, \/\/ T_NARROWKLASS = 18,\n-  BAD_ALLOCATION_TYPE, \/\/ T_CONFLICT    = 19,\n+  STATIC_OOP,          \/\/ T_PRIMITIVE_OBJECT = 14,\n+  BAD_ALLOCATION_TYPE, \/\/ T_VOID        = 15,\n+  BAD_ALLOCATION_TYPE, \/\/ T_ADDRESS     = 16,\n+  BAD_ALLOCATION_TYPE, \/\/ T_NARROWOOP   = 17,\n+  BAD_ALLOCATION_TYPE, \/\/ T_METADATA    = 18,\n+  BAD_ALLOCATION_TYPE, \/\/ T_NARROWKLASS = 19,\n+  BAD_ALLOCATION_TYPE, \/\/ T_CONFLICT    = 20\n@@ -1415,1 +1424,1 @@\n-static FieldAllocationType basic_type_to_atype(bool is_static, BasicType type) {\n+static FieldAllocationType basic_type_to_atype(bool is_static, BasicType type, bool is_inline_type) {\n@@ -1419,0 +1428,3 @@\n+  if (is_inline_type) {\n+    result = is_static ? STATIC_INLINE : NONSTATIC_INLINE;\n+  }\n@@ -1432,2 +1444,2 @@\n-  void update(bool is_static, BasicType type) {\n-    FieldAllocationType atype = basic_type_to_atype(is_static, type);\n+  void update(bool is_static, BasicType type, bool is_inline_type) {\n+    FieldAllocationType atype = basic_type_to_atype(is_static, type, is_inline_type);\n@@ -1445,1 +1457,1 @@\n-                                   bool is_interface,\n+                                   AccessFlags class_access_flags,\n@@ -1460,0 +1472,1 @@\n+  bool is_inline_type = !class_access_flags.is_identity_class() && !class_access_flags.is_abstract();\n@@ -1467,1 +1480,5 @@\n-  const int total_fields = length + num_injected;\n+\n+  \/\/ two more slots are required for inline classes:\n+  \/\/ one for the static field with a reference to the pre-allocated default value\n+  \/\/ one for the field the JVM injects when detecting an empty inline class\n+  const int total_fields = length + num_injected + (is_inline_type ? 2 : 0);\n@@ -1473,0 +1490,1 @@\n+  int instance_fields_count = 0;\n@@ -1478,0 +1496,7 @@\n+    jint recognized_modifiers = JVM_RECOGNIZED_FIELD_MODIFIERS;\n+    if (!supports_inline_types()) {\n+      recognized_modifiers &= ~JVM_ACC_STRICT;\n+    }\n+\n+    const jint flags = cfs->get_u2_fast() & recognized_modifiers;\n+    verify_legal_field_modifiers(flags, class_access_flags, CHECK);\n@@ -1479,2 +1504,0 @@\n-    const jint flags = cfs->get_u2_fast() & JVM_RECOGNIZED_FIELD_MODIFIERS;\n-    verify_legal_field_modifiers(flags, is_interface, CHECK);\n@@ -1497,0 +1520,1 @@\n+    if (!access_flags.is_static()) instance_fields_count++;\n@@ -1504,0 +1528,2 @@\n+    bool is_null_restricted = false;\n+\n@@ -1523,0 +1549,10 @@\n+        if (parsed_annotations.has_annotation(AnnotationCollector::_jdk_internal_NullRestricted)) {\n+          if (!Signature::has_envelope(sig)) {\n+            Exceptions::fthrow(\n+              THREAD_AND_LOCATION,\n+              vmSymbols::java_lang_ClassFormatError(),\n+              \"Illegal use of @jdk.internal.vm.annotation.NullRestricted annotation on field %s with signature %s (primitive types can never be null)\",\n+              name->as_C_string(), sig->as_C_string());\n+          }\n+          is_null_restricted = true;\n+        }\n@@ -1548,1 +1584,4 @@\n-    fac->update(is_static, type);\n+    \/\/ This use of T_PRIMITIVE_OBJECT is not valid anymore => FIXME (relate to cleanup (removal?) of FiedAllocationCount\n+    fac->update(is_static, type, type == T_PRIMITIVE_OBJECT);\n+\n+    if (is_null_restricted) fieldFlags.update_null_free_inline_type(true);\n@@ -1596,1 +1635,1 @@\n-      fac->update(false, type);\n+      fac->update(false, type, false);\n@@ -1601,0 +1640,42 @@\n+  if (is_inline_type) {\n+    \/\/ Inject static \".default\" field\n+    FieldInfo::FieldFlags fflags(0);\n+    fflags.update_injected(true);\n+    AccessFlags aflags(JVM_ACC_STATIC);\n+    FieldInfo fi(aflags,\n+                 (u2)vmSymbols::as_int(VM_SYMBOL_ENUM_NAME(default_value_name)),\n+                 (u2)vmSymbols::as_int(VM_SYMBOL_ENUM_NAME(object_signature)),\n+                 0,\n+                 fflags);\n+      fi.set_index(index);\n+      _temp_field_info->append(fi);\n+\n+    const BasicType type = Signature::basic_type(vmSymbols::object_signature());\n+    fac->update(true, type, false);\n+    index++;\n+  }\n+\n+  if (is_inline_type && instance_fields_count == 0) {\n+    \/\/ Inject \".empty\" dummy field\n+    _is_empty_inline_type = true;\n+\n+    FieldInfo::FieldFlags fflags(0);\n+    fflags.update_injected(true);\n+    AccessFlags aflags;\n+    FieldInfo fi(aflags,\n+                 (u2)vmSymbols::as_int(VM_SYMBOL_ENUM_NAME(empty_marker_name)),\n+                 (u2)vmSymbols::as_int(VM_SYMBOL_ENUM_NAME(byte_signature)),\n+                 0,\n+                 fflags);\n+    fi.set_index(index);\n+    _temp_field_info->append(fi);\n+\n+    const BasicType type = Signature::basic_type(vmSymbols::byte_signature());\n+    fac->update(false, type, false);\n+    index++;\n+  }\n+\n+  if (instance_fields_count > 0) {\n+    _has_nonstatic_fields = true;\n+  }\n+\n@@ -1975,0 +2056,12 @@\n+    case VM_SYMBOL_ENUM_NAME(jdk_internal_vm_annotation_ImplicitlyConstructible_signature): {\n+      if (_location != _in_class)   break; \/\/ only allow for classes\n+      return _jdk_internal_ImplicitlyConstructible;\n+    }\n+    case VM_SYMBOL_ENUM_NAME(jdk_internal_vm_annotation_LooselyConsistentValue_signature): {\n+      if (_location != _in_class)   break; \/\/ only allow for classes\n+      return _jdk_internal_LooselyConsistentValue;\n+    }\n+    case VM_SYMBOL_ENUM_NAME(jdk_internal_vm_annotation_NullRestricted_signature): {\n+      if (_location != _in_field)   break; \/\/ only allow for fields\n+      return _jdk_internal_NullRestricted;\n+    }\n@@ -2196,0 +2289,2 @@\n+                                      bool is_value_class,\n+                                      bool is_abstract_class,\n@@ -2237,1 +2332,1 @@\n-    verify_legal_method_modifiers(flags, is_interface, name, CHECK_NULL);\n+    verify_legal_method_modifiers(flags, access_flags() , name, CHECK_NULL);\n@@ -2245,0 +2340,9 @@\n+  if (EnableValhalla) {\n+    if (((flags & JVM_ACC_SYNCHRONIZED) == JVM_ACC_SYNCHRONIZED)\n+        && ((flags & JVM_ACC_STATIC) == 0 )\n+        && !_access_flags.is_identity_class()) {\n+      classfile_parse_error(\"Invalid synchronized method in non-identity class %s\", THREAD);\n+        return nullptr;\n+    }\n+  }\n+\n@@ -2772,0 +2876,2 @@\n+                                    bool is_value_class,\n+                                    bool is_abstract_type,\n@@ -2796,0 +2902,2 @@\n+                                    is_value_class,\n+                                    is_abstract_type,\n@@ -3061,2 +3169,2 @@\n-    \/\/ Access flags\n-    jint flags;\n+\n+    jint recognized_modifiers = RECOGNIZED_INNER_CLASS_MODIFIERS;\n@@ -3065,3 +3173,1 @@\n-      flags = cfs->get_u2_fast() & (RECOGNIZED_INNER_CLASS_MODIFIERS | JVM_ACC_MODULE);\n-    } else {\n-      flags = cfs->get_u2_fast() & RECOGNIZED_INNER_CLASS_MODIFIERS;\n+      recognized_modifiers |= JVM_ACC_MODULE;\n@@ -3069,0 +3175,4 @@\n+\n+    \/\/ Access flags\n+    jint flags = cfs->get_u2_fast() & recognized_modifiers;\n+\n@@ -3073,1 +3183,11 @@\n-    verify_legal_class_modifiers(flags, CHECK_0);\n+\n+    if (!supports_inline_types()) {\n+      const bool is_module = (flags & JVM_ACC_MODULE) != 0;\n+      const bool is_interface = (flags & JVM_ACC_INTERFACE) != 0;\n+      if (!is_module && !is_interface) {\n+        flags |= JVM_ACC_IDENTITY;\n+      }\n+    }\n+\n+    const char* name = inner_name_index == 0 ? \"unnamed\" : cp->symbol_at(inner_name_index)->as_utf8();\n+    verify_legal_class_modifiers(flags, name, false, CHECK_0);\n@@ -3177,0 +3297,43 @@\n+u2 ClassFileParser::parse_classfile_loadable_descriptors_attribute(const ClassFileStream* const cfs,\n+                                                                   const u1* const loadable_descriptors_attribute_start,\n+                                                                   TRAPS) {\n+  const u1* const current_mark = cfs->current();\n+  u2 length = 0;\n+  if (loadable_descriptors_attribute_start != nullptr) {\n+    cfs->set_current(loadable_descriptors_attribute_start);\n+    cfs->guarantee_more(2, CHECK_0);  \/\/ length\n+    length = cfs->get_u2_fast();\n+  }\n+  const int size = length;\n+  Array<u2>* const loadable_descriptors = MetadataFactory::new_array<u2>(_loader_data, size, CHECK_0);\n+  _loadable_descriptors = loadable_descriptors;\n+  if (length > 0) {\n+    int index = 0;\n+    cfs->guarantee_more(2 * length, CHECK_0);\n+    for (int n = 0; n < length; n++) {\n+      const u2 descriptor_index = cfs->get_u2_fast();\n+      check_property(\n+        valid_symbol_at(descriptor_index),\n+        \"LoadableDescriptors descriptor_index %u has bad constant type in class file %s\",\n+        descriptor_index, CHECK_0);\n+      Symbol* descriptor = _cp->symbol_at(descriptor_index);\n+      bool valid = legal_field_signature(descriptor, CHECK_0);\n+      if(!valid) {\n+        ResourceMark rm(THREAD);\n+        Exceptions::fthrow(THREAD_AND_LOCATION,\n+          vmSymbols::java_lang_ClassFormatError(),\n+          \"Descriptor from LoadableDescriptors attribute at index \\\"%d\\\" in class %s has illegal signature \\\"%s\\\"\",\n+          descriptor_index, _class_name->as_C_string(), descriptor->as_C_string());\n+        return 0;\n+      }\n+      loadable_descriptors->at_put(index++, descriptor_index);\n+    }\n+    assert(index == size, \"wrong size\");\n+  }\n+\n+  \/\/ Restore buffer's current position.\n+  cfs->set_current(current_mark);\n+\n+  return length;\n+}\n+\n@@ -3442,0 +3605,2 @@\n+  \/\/ Set _loadable_descriptors attribute to default sentinel\n+  _loadable_descriptors = Universe::the_empty_short_array();\n@@ -3448,0 +3613,1 @@\n+  bool parsed_loadable_descriptors_attribute = false;\n@@ -3469,0 +3635,2 @@\n+  const u1* loadable_descriptors_attribute_start = nullptr;\n+  u4  loadable_descriptors_attribute_length = 0;\n@@ -3684,0 +3852,9 @@\n+            if (EnableValhalla && tag == vmSymbols::tag_loadable_descriptors()) {\n+              if (parsed_loadable_descriptors_attribute) {\n+                classfile_parse_error(\"Multiple LoadableDescriptors attributes in class file %s\", CHECK);\n+                return;\n+              }\n+              parsed_loadable_descriptors_attribute = true;\n+              loadable_descriptors_attribute_start = cfs->current();\n+              loadable_descriptors_attribute_length = attribute_length;\n+            }\n@@ -3760,0 +3937,12 @@\n+  if (parsed_loadable_descriptors_attribute) {\n+    const u2 num_classes = parse_classfile_loadable_descriptors_attribute(\n+                            cfs,\n+                            loadable_descriptors_attribute_start,\n+                            CHECK);\n+    if (_need_verify) {\n+      guarantee_property(\n+        loadable_descriptors_attribute_length == sizeof(num_classes) + sizeof(u2) * num_classes,\n+        \"Wrong LoadableDescriptors attribute length in class file %s\", CHECK);\n+    }\n+  }\n+\n@@ -3825,0 +4014,1 @@\n+  this_klass->set_loadable_descriptors(_loadable_descriptors);\n@@ -3828,0 +4018,2 @@\n+  this_klass->set_inline_type_field_klasses_array(_inline_type_field_klasses);\n+  this_klass->set_null_marker_offsets_array(_null_marker_offsets);\n@@ -3864,2 +4056,1 @@\n-                   \"Invalid superclass index %u in class file %s\",\n-                   super_class_index,\n+                   \"Invalid superclass index 0 in class file %s\",\n@@ -3874,1 +4065,0 @@\n-    bool is_array = false;\n@@ -3877,4 +4067,0 @@\n-      if (need_verify)\n-        is_array = super_klass->is_array_klass();\n-    } else if (need_verify) {\n-      is_array = (cp->klass_name_at(super_class_index)->char_at(0) == JVM_SIGNATURE_ARRAY);\n@@ -3883,0 +4069,1 @@\n+      bool is_array = (cp->klass_name_at(super_class_index)->char_at(0) == JVM_SIGNATURE_ARRAY);\n@@ -4056,0 +4243,6 @@\n+bool ClassFileParser::supports_inline_types() const {\n+  \/\/ Inline types are only supported by class file version 68.65535 and later\n+  return _major_version > JAVA_24_VERSION ||\n+         (_major_version == JAVA_24_VERSION && _minor_version == JAVA_PREVIEW_MINOR_VERSION);\n+}\n+\n@@ -4099,3 +4292,4 @@\n-  } else if (max_transitive_size == local_size) {\n-    \/\/ only local interfaces added, share local interface array\n-    return local_ifs;\n+    \/\/ The three lines below are commented to work around bug JDK-8245487\n+\/\/  } else if (max_transitive_size == local_size) {\n+\/\/    \/\/ only local interfaces added, share local interface array\n+\/\/    return local_ifs;\n@@ -4122,0 +4316,1 @@\n+\n@@ -4150,0 +4345,10 @@\n+    \/\/ The JVMS says that super classes for value types must not have the ACC_IDENTITY\n+    \/\/ flag set. But, java.lang.Object must still be allowed to be a direct super class\n+    \/\/ for a value classes.  So, it is treated as a special case for now.\n+    if (!this_klass->access_flags().is_identity_class() &&\n+        super_ik->name() != vmSymbols::java_lang_Object() &&\n+        super_ik->is_identity_class()) {\n+      classfile_icce_error(\"value class %s cannot inherit from class %s\", super_ik, THREAD);\n+      return;\n+    }\n+\n@@ -4335,1 +4540,1 @@\n-void ClassFileParser::verify_legal_class_modifiers(jint flags, TRAPS) const {\n+void ClassFileParser::verify_legal_class_modifiers(jint flags, const char* name, bool is_Object, TRAPS) const {\n@@ -4337,0 +4542,1 @@\n+  const bool is_inner_class = name != nullptr;\n@@ -4353,1 +4559,1 @@\n-  const bool is_super      = (flags & JVM_ACC_SUPER)      != 0;\n+  const bool is_identity   = (flags & JVM_ACC_IDENTITY)   != 0;\n@@ -4357,0 +4563,2 @@\n+  const bool valid_value_class = is_identity || is_interface ||\n+                                 (supports_inline_types() && (!is_identity && (is_abstract || is_final)));\n@@ -4360,2 +4568,3 @@\n-      (is_interface && major_gte_1_5 && (is_super || is_enum)) ||\n-      (!is_interface && major_gte_1_5 && is_annotation)) {\n+      (is_interface && major_gte_1_5 && (is_identity || is_enum)) ||   \/\/  ACC_SUPER (now ACC_IDENTITY) was illegal for interfaces\n+      (!is_interface && major_gte_1_5 && is_annotation) ||\n+      (!valid_value_class)) {\n@@ -4363,7 +4572,21 @@\n-    Exceptions::fthrow(\n-      THREAD_AND_LOCATION,\n-      vmSymbols::java_lang_ClassFormatError(),\n-      \"Illegal class modifiers in class %s: 0x%X\",\n-      _class_name->as_C_string(), flags\n-    );\n-    return;\n+    const char* class_note = \"\";\n+    if (!valid_value_class) {\n+      class_note = \" (a value class must be final or else abstract)\";\n+    }\n+    if (name == nullptr) { \/\/ Not an inner class\n+      Exceptions::fthrow(\n+        THREAD_AND_LOCATION,\n+        vmSymbols::java_lang_ClassFormatError(),\n+        \"Illegal class modifiers in class %s%s: 0x%X\",\n+        _class_name->as_C_string(), class_note, flags\n+      );\n+      return;\n+    } else {\n+      Exceptions::fthrow(\n+        THREAD_AND_LOCATION,\n+        vmSymbols::java_lang_ClassFormatError(),\n+        \"Illegal class modifiers in declaration of inner class %s%s of class %s: 0x%X\",\n+        name, class_note, _class_name->as_C_string(), flags\n+      );\n+      return;\n+    }\n@@ -4435,2 +4658,2 @@\n-void ClassFileParser::verify_legal_field_modifiers(jint flags,\n-                                                   bool is_interface,\n+void ClassFileParser:: verify_legal_field_modifiers(jint flags,\n+                                                   AccessFlags class_access_flags,\n@@ -4448,0 +4671,1 @@\n+  const bool is_strict    = (flags & JVM_ACC_STRICT)    != 0;\n@@ -4450,0 +4674,3 @@\n+  const bool is_interface = class_access_flags.is_interface();\n+  const bool is_identity_class = class_access_flags.is_identity_class();\n+\n@@ -4451,0 +4678,1 @@\n+  const char* error_msg = \"\";\n@@ -4452,4 +4680,8 @@\n-  if (is_interface) {\n-    if (!is_public || !is_static || !is_final || is_private ||\n-        is_protected || is_volatile || is_transient ||\n-        (major_gte_1_5 && is_enum)) {\n+  \/\/ There is some overlap in the checks that apply, for example interface fields\n+  \/\/ must be static, static fields can't be strict, and therefore interfaces can't\n+  \/\/ have strict fields. So we don't have to check every possible invalid combination\n+  \/\/ individually as long as all are covered. Once we have found an illegal combination\n+  \/\/ we can stop checking.\n+\n+  if (supports_inline_types()) {\n+    if (is_strict && is_static) {\n@@ -4457,0 +4689,1 @@\n+      error_msg = \"field cannot be strict and static\";\n@@ -4458,2 +4691,1 @@\n-  } else { \/\/ not interface\n-    if (has_illegal_visibility(flags) || (is_final && is_volatile)) {\n+    else if (is_strict && !is_final) {\n@@ -4461,0 +4693,25 @@\n+      error_msg = \"strict field must be final\";\n+    }\n+  }\n+\n+  if (!is_illegal) {\n+    if (is_interface) {\n+      if (!is_public || !is_static || !is_final || is_private ||\n+          is_protected || is_volatile || is_transient ||\n+          (major_gte_1_5 && is_enum)) {\n+        is_illegal = true;\n+        error_msg = \"interface fields must be public, static and final, and may be synthetic\";\n+      }\n+    } else { \/\/ not interface\n+      if (has_illegal_visibility(flags)) {\n+        is_illegal = true;\n+        error_msg = \"invalid visibility flags for class field\";\n+      } else if (is_final && is_volatile) {\n+        is_illegal = true;\n+        error_msg = \"fields cannot be final and volatile\";\n+      } else if (supports_inline_types()) {\n+        if (!is_identity_class && !is_static && !is_strict) {\n+          is_illegal = true;\n+          error_msg = \"value class fields must be either strict or static\";\n+        }\n+      }\n@@ -4469,2 +4726,2 @@\n-      \"Illegal field modifiers in class %s: 0x%X\",\n-      _class_name->as_C_string(), flags);\n+      \"Illegal field modifiers (%s) in class %s: 0x%X\",\n+      error_msg, _class_name->as_C_string(), flags);\n@@ -4476,1 +4733,1 @@\n-                                                    bool is_interface,\n+                                                    AccessFlags class_access_flags,\n@@ -4495,0 +4752,4 @@\n+  \/\/ LW401 CR required: removal of value factories support\n+  const bool is_interface    = class_access_flags.is_interface();\n+  const bool is_identity_class = class_access_flags.is_identity_class();\n+  const bool is_abstract_class = class_access_flags.is_abstract();\n@@ -4498,0 +4759,1 @@\n+  const char* class_note = \"\";\n@@ -4537,4 +4799,9 @@\n-        if (is_abstract) {\n-          if ((is_final || is_native || is_private || is_static ||\n-              (major_gte_1_5 && (is_synchronized || (!major_gte_17 && is_strict))))) {\n-            is_illegal = true;\n+        if (!is_identity_class && is_synchronized && !is_static) {\n+          is_illegal = true;\n+          class_note = \" (not an identity class)\";\n+        } else {\n+          if (is_abstract) {\n+            if ((is_final || is_native || is_private || is_static ||\n+                (major_gte_1_5 && (is_synchronized || (!major_gte_17 && is_strict))))) {\n+              is_illegal = true;\n+            }\n@@ -4552,2 +4819,3 @@\n-      \"Method %s in class %s has illegal modifiers: 0x%X\",\n-      name->as_C_string(), _class_name->as_C_string(), flags);\n+      \"Method %s in class %s%s has illegal modifiers: 0x%X\",\n+      name->as_C_string(), _class_name->as_C_string(),\n+      class_note, flags);\n@@ -4611,0 +4879,9 @@\n+bool ClassFileParser::is_class_in_loadable_descriptors_attribute(Symbol *klass) {\n+  if (_loadable_descriptors == nullptr) return false;\n+  for (int i = 0; i < _loadable_descriptors->length(); i++) {\n+        Symbol* class_name = _cp->symbol_at(_loadable_descriptors->at(i));\n+        if (class_name == klass) return true;\n+  }\n+  return false;\n+}\n+\n@@ -4712,1 +4989,2 @@\n-    case JVM_SIGNATURE_CLASS: {\n+    case JVM_SIGNATURE_CLASS:\n+    {\n@@ -4723,1 +5001,1 @@\n-        \/\/ Skip leading 'L' and ignore first appearance of ';'\n+        \/\/ Skip leading 'L' or 'Q' and ignore first appearance of ';'\n@@ -4779,0 +5057,4 @@\n+    } else if ((_major_version >= CONSTANT_CLASS_DESCRIPTORS || _class_name->starts_with(\"jdk\/internal\/reflect\/\"))\n+                   && bytes[length - 1] == ';' ) {\n+      \/\/ Support for L...; descriptors\n+      legal = verify_unqualified_name(bytes + 1, length - 2, LegalClass);\n@@ -4844,1 +5126,2 @@\n-      if (name == vmSymbols::object_initializer_name() || name == vmSymbols::class_initializer_name()) {\n+      if (name == vmSymbols::object_initializer_name() ||\n+          name == vmSymbols::class_initializer_name()) {\n@@ -4870,0 +5153,10 @@\n+bool ClassFileParser::legal_field_signature(const Symbol* signature, TRAPS) const {\n+  const char* const bytes = (const char*)signature->bytes();\n+  const unsigned int length = signature->utf8_length();\n+  const char* const p = skip_over_field_signature(bytes, false, length, CHECK_false);\n+\n+  if (p == nullptr || (p - bytes) != (int)length) {\n+    return false;\n+  }\n+  return true;\n+}\n@@ -4905,3 +5198,3 @@\n-      name->char_at(0) == JVM_SIGNATURE_SPECIAL &&\n-      sig_length > 0 &&\n-      signature->char_at(sig_length - 1) != JVM_SIGNATURE_VOID) {\n+    name->char_at(0) == JVM_SIGNATURE_SPECIAL &&\n+    sig_length > 0 &&\n+    signature->char_at(sig_length - 1) != JVM_SIGNATURE_VOID) {\n@@ -5084,1 +5377,0 @@\n-\n@@ -5120,0 +5412,4 @@\n+  if (_field_info->_is_naturally_atomic && ik->is_inline_klass()) {\n+    ik->set_is_naturally_atomic();\n+  }\n+\n@@ -5121,1 +5417,1 @@\n-  ik->set_static_oop_field_count(_fac->count[STATIC_OOP]);\n+  ik->set_static_oop_field_count(_fac->count[STATIC_OOP] + _fac->count[STATIC_INLINE]);\n@@ -5126,0 +5422,3 @@\n+  if (ik->is_inline_klass()) {\n+    InlineKlass::cast(ik)->init_fixed_block();\n+  }\n@@ -5139,0 +5438,1 @@\n+  assert(nullptr == _loadable_descriptors, \"invariant\");\n@@ -5142,0 +5442,1 @@\n+  assert(nullptr == _inline_type_field_klasses, \"invariant\");\n@@ -5177,0 +5478,6 @@\n+  if (_must_be_atomic) {\n+    ik->set_must_be_atomic();\n+  }\n+  if (_is_implicitly_constructible) {\n+    ik->set_is_implicitly_constructible();\n+  }\n@@ -5281,0 +5588,28 @@\n+  bool all_fields_empty = true;\n+  for (AllFieldStream fs(ik); !fs.done(); fs.next()) {\n+    if (!fs.access_flags().is_static()) {\n+      if (fs.field_descriptor().is_null_free_inline_type()) {\n+        Klass* k = ik->inline_type_field_klasses_array()->at(fs.index());\n+        assert(k->is_inline_klass(), \"must be\");\n+        if (!InlineKlass::cast(k)->is_empty_inline_type()) { all_fields_empty = false; }\n+      } else {\n+        all_fields_empty = false;\n+      }\n+    } else if (is_inline_type() && (fs.name() == vmSymbols::default_value_name())) {\n+      InlineKlass::cast(ik)->set_default_value_offset(ik->field_offset(fs.index()));\n+    }\n+  }\n+\n+  if (_is_empty_inline_type || (is_inline_type() && all_fields_empty)) {\n+    ik->set_is_empty_inline_type();\n+  }\n+\n+  if (is_inline_type()) {\n+    InlineKlass* vk = InlineKlass::cast(ik);\n+    vk->set_alignment(_alignment);\n+    vk->set_first_field_offset(_first_field_offset);\n+    vk->set_payload_size_in_bytes(_payload_size_in_bytes);\n+    vk->set_internal_null_marker_offset(_internal_null_marker_offset);\n+    InlineKlass::cast(ik)->initialize_calling_convention(CHECK);\n+  }\n+\n@@ -5372,0 +5707,1 @@\n+  _loadable_descriptors(nullptr),\n@@ -5374,0 +5710,1 @@\n+  _local_interface_indexes(nullptr),\n@@ -5385,0 +5722,2 @@\n+  _inline_type_field_klasses(nullptr),\n+  _null_marker_offsets(nullptr),\n@@ -5413,0 +5752,8 @@\n+  _has_inline_type_fields(false),\n+  _has_nonstatic_fields(false),\n+  _is_empty_inline_type(false),\n+  _is_naturally_atomic(false),\n+  _must_be_atomic(true),\n+  _is_implicitly_constructible(false),\n+  _has_loosely_consistent_annotation(false),\n+  _has_implicitly_constructible_annotation(false),\n@@ -5462,0 +5809,1 @@\n+  _loadable_descriptors = nullptr;\n@@ -5466,0 +5814,2 @@\n+  _inline_type_field_klasses = nullptr;\n+  _null_marker_offsets = nullptr;\n@@ -5484,0 +5834,8 @@\n+  if (_inline_type_field_klasses != nullptr) {\n+     MetadataFactory::free_array<InlineKlass*>(_loader_data, _inline_type_field_klasses);\n+  }\n+\n+  if (_null_marker_offsets != nullptr) {\n+     MetadataFactory::free_array<int>(_loader_data, _null_marker_offsets);\n+  }\n+\n@@ -5506,0 +5864,4 @@\n+  if (_loadable_descriptors != nullptr && _loadable_descriptors != Universe::the_empty_short_array()) {\n+    MetadataFactory::free_array<u2>(_loader_data, _loadable_descriptors);\n+  }\n+\n@@ -5590,2 +5952,1 @@\n-  \/\/ Access flags\n-  jint flags;\n+  jint recognized_modifiers = JVM_RECOGNIZED_CLASS_MODIFIERS;\n@@ -5594,3 +5955,1 @@\n-    flags = stream->get_u2_fast() & (JVM_RECOGNIZED_CLASS_MODIFIERS | JVM_ACC_MODULE);\n-  } else {\n-    flags = stream->get_u2_fast() & JVM_RECOGNIZED_CLASS_MODIFIERS;\n+    recognized_modifiers |= JVM_ACC_MODULE;\n@@ -5599,0 +5958,3 @@\n+  \/\/ Access flags\n+  jint flags = stream->get_u2_fast() & recognized_modifiers;\n+\n@@ -5604,8 +5966,7 @@\n-  verify_legal_class_modifiers(flags, CHECK);\n-\n-  short bad_constant = class_bad_constant_seen();\n-  if (bad_constant != 0) {\n-    \/\/ Do not throw CFE until after the access_flags are checked because if\n-    \/\/ ACC_MODULE is set in the access flags, then NCDFE must be thrown, not CFE.\n-    classfile_parse_error(\"Unknown constant tag %u in class file %s\", bad_constant, THREAD);\n-    return;\n+  \/\/ Fixing ACC_SUPER\/ACC_IDENTITY for old class files\n+  if (!supports_inline_types()) {\n+    const bool is_module = (flags & JVM_ACC_MODULE) != 0;\n+    const bool is_interface = (flags & JVM_ACC_INTERFACE) != 0;\n+    if (!is_module && !is_interface) {\n+      flags |= JVM_ACC_IDENTITY;\n+    }\n@@ -5614,1 +5975,0 @@\n-  _access_flags.set_flags(flags);\n@@ -5627,0 +5987,14 @@\n+  bool is_java_lang_Object = class_name_in_cp == vmSymbols::java_lang_Object();\n+\n+  verify_legal_class_modifiers(flags, nullptr, is_java_lang_Object, CHECK);\n+\n+  _access_flags.set_flags(flags);\n+\n+  short bad_constant = class_bad_constant_seen();\n+  if (bad_constant != 0) {\n+    \/\/ Do not throw CFE until after the access_flags are checked because if\n+    \/\/ ACC_MODULE is set in the access flags, then NCDFE must be thrown, not CFE.\n+    classfile_parse_error(\"Unknown constant tag %u in class file %s\", bad_constant, THREAD);\n+    return;\n+  }\n+\n@@ -5706,2 +6080,0 @@\n-  assert(_local_interfaces != nullptr, \"invariant\");\n-\n@@ -5711,1 +6083,1 @@\n-               _access_flags.is_interface(),\n+               _access_flags,\n@@ -5722,1 +6094,3 @@\n-                _access_flags.is_interface(),\n+                is_interface(),\n+                !is_identity_class(),\n+                is_abstract_class(),\n@@ -5802,2 +6176,2 @@\n-                   \"java.lang.Object cannot implement an interface in class file %s\",\n-                   CHECK);\n+        \"java.lang.Object cannot implement an interface in class file %s\",\n+        CHECK);\n@@ -5808,1 +6182,1 @@\n-    if (_access_flags.is_interface()) {\n+    if (is_interface()) {\n@@ -5820,1 +6194,1 @@\n-                       SystemDictionary::resolve_super_or_fail(_class_name,\n+                       SystemDictionary::resolve_with_circularity_detection_or_fail(_class_name,\n@@ -5830,0 +6204,14 @@\n+    if (_super_klass->is_interface()) {\n+      classfile_icce_error(\"class %s has interface %s as super class\", _super_klass, THREAD);\n+      return;\n+    }\n+\n+    if (_super_klass->is_final()) {\n+      classfile_icce_error(\"class %s cannot inherit from final class %s\", _super_klass, THREAD);\n+      return;\n+    }\n+\n+    if (EnableValhalla) {\n+      check_identity_and_value_modifiers(this, _super_klass, CHECK);\n+    }\n+\n@@ -5833,0 +6221,1 @@\n+  }\n@@ -5834,3 +6223,75 @@\n-    if (_super_klass->is_interface()) {\n-      classfile_icce_error(\"class %s has interface %s as super class\", _super_klass, THREAD);\n-      return;\n+  if (_parsed_annotations->has_annotation(AnnotationCollector::_jdk_internal_LooselyConsistentValue) && _access_flags.is_identity_class()) {\n+    THROW_MSG(vmSymbols::java_lang_ClassFormatError(),\n+          err_msg(\"class %s cannot have annotation jdk.internal.vm.annotation.LooselyConsistentValue, because it is not a value class\",\n+                  _class_name->as_klass_external_name()));\n+  }\n+  if (_parsed_annotations->has_annotation(AnnotationCollector::_jdk_internal_ImplicitlyConstructible) && _access_flags.is_identity_class()) {\n+    THROW_MSG(vmSymbols::java_lang_ClassFormatError(),\n+          err_msg(\"class %s cannot have annotation jdk.internal.vm.annotation.ImplicitlyConstructible, because it is not a value class\",\n+                  _class_name->as_klass_external_name()));\n+  }\n+\n+  \/\/ Determining is the class allows tearing or not (default is not)\n+  \/\/ Test might need extensions when field inheritance is added for value classes\n+  if (EnableValhalla && !_access_flags.is_identity_class()) {\n+    if (_parsed_annotations->has_annotation(ClassAnnotationCollector::_jdk_internal_LooselyConsistentValue)\n+        && (_super_klass == vmClasses::Object_klass() || !_super_klass->must_be_atomic())) {\n+      _must_be_atomic = false;\n+    }\n+    if (_parsed_annotations->has_annotation(ClassAnnotationCollector::_jdk_internal_ImplicitlyConstructible)\n+        && (_super_klass == vmClasses::Object_klass() || _super_klass->is_implicitly_constructible())) {\n+      _is_implicitly_constructible = true;\n+    }\n+    \/\/ Apply VM options override\n+    if (*ForceNonTearable != '\\0') {\n+      \/\/ Allow a command line switch to force the same atomicity property:\n+      const char* class_name_str = _class_name->as_C_string();\n+      if (StringUtils::class_list_match(ForceNonTearable, class_name_str)) {\n+        _must_be_atomic = true;\n+      }\n+    }\n+  }\n+\n+  int itfs_len = _local_interface_indexes == nullptr ? 0 : _local_interface_indexes->length();\n+  _local_interfaces = MetadataFactory::new_array<InstanceKlass*>(_loader_data, itfs_len, nullptr, CHECK);\n+  if (_local_interface_indexes != nullptr) {\n+    for (int i = 0; i < _local_interface_indexes->length(); i++) {\n+      u2 interface_index = _local_interface_indexes->at(i);\n+      Klass* interf;\n+      if (cp->tag_at(interface_index).is_klass()) {\n+        interf = cp->resolved_klass_at(interface_index);\n+      } else {\n+        Symbol* const unresolved_klass  = cp->klass_name_at(interface_index);\n+\n+        \/\/ Don't need to check legal name because it's checked when parsing constant pool.\n+        \/\/ But need to make sure it's not an array type.\n+        guarantee_property(unresolved_klass->char_at(0) != JVM_SIGNATURE_ARRAY,\n+                            \"Bad interface name in class file %s\", CHECK);\n+\n+        \/\/ Call resolve on the interface class name with class circularity checking\n+        interf = SystemDictionary::resolve_with_circularity_detection_or_fail(\n+                                                  _class_name,\n+                                                  unresolved_klass,\n+                                                  Handle(THREAD, _loader_data->class_loader()),\n+                                                  _protection_domain,\n+                                                  false,\n+                                                  CHECK);\n+      }\n+\n+      if (!interf->is_interface()) {\n+        THROW_MSG(vmSymbols::java_lang_IncompatibleClassChangeError(),\n+                  err_msg(\"class %s can not implement %s, because it is not an interface (%s)\",\n+                          _class_name->as_klass_external_name(),\n+                          interf->external_name(),\n+                          interf->class_in_module_of_loader()));\n+      }\n+\n+      if (EnableValhalla) {\n+        \/\/ Check modifiers and set carries_identity_modifier\/carries_value_modifier flags\n+        check_identity_and_value_modifiers(this, InstanceKlass::cast(interf), CHECK);\n+      }\n+\n+      if (InstanceKlass::cast(interf)->has_nonstatic_concrete_methods()) {\n+        _has_nonstatic_concrete_methods = true;\n+      }\n+      _local_interfaces->at_put(i, InstanceKlass::cast(interf));\n@@ -5839,0 +6300,1 @@\n+  assert(_local_interfaces != nullptr, \"invariant\");\n@@ -5867,1 +6329,1 @@\n-  _itable_size = _access_flags.is_interface() ? 0 :\n+  _itable_size = is_interface() ? 0 :\n@@ -5873,0 +6335,76 @@\n+  if (EnableValhalla) {\n+    _inline_type_field_klasses = MetadataFactory::new_array<InlineKlass*>(_loader_data,\n+                                                   java_fields_count(),\n+                                                   nullptr,\n+                                                   CHECK);\n+    for (GrowableArrayIterator<FieldInfo> it = _temp_field_info->begin(); it != _temp_field_info->end(); ++it) {\n+      FieldInfo fieldinfo = *it;\n+      if (fieldinfo.access_flags().is_static()) continue;  \/\/ Only non-static fields are processed at load time\n+      Symbol* sig = fieldinfo.signature(cp);\n+      if (fieldinfo.field_flags().is_null_free_inline_type()) {\n+        \/\/ Pre-load classes of null-free fields that are candidate for flattening\n+        TempNewSymbol s = Signature::strip_envelope(sig);\n+        if (s == _class_name) {\n+          THROW_MSG(vmSymbols::java_lang_ClassCircularityError(), err_msg(\"Class %s cannot have a null-free non-static field of its own type\", _class_name->as_C_string()));\n+        }\n+        log_info(class, preload)(\"Preloading class %s during loading of class %s. Cause: a null-free non-static field is declared with this type\", s->as_C_string(), _class_name->as_C_string());\n+        Klass* klass = SystemDictionary::resolve_with_circularity_detection_or_fail(_class_name, s, Handle(THREAD, _loader_data->class_loader()), _protection_domain, false, THREAD);\n+        if (HAS_PENDING_EXCEPTION) {\n+          log_warning(class, preload)(\"Preloading of class %s during loading of class %s (cause: null-free non-static field) failed: %s\",\n+                                      s->as_C_string(), _class_name->as_C_string(), PENDING_EXCEPTION->klass()->name()->as_C_string());\n+          return; \/\/ Exception is still pending\n+        }\n+        assert(klass != nullptr, \"Sanity check\");\n+        if (klass->access_flags().is_identity_class()) {\n+          assert(klass->is_instance_klass(), \"Sanity check\");\n+          ResourceMark rm(THREAD);\n+          THROW_MSG(vmSymbols::java_lang_IncompatibleClassChangeError(),\n+                    err_msg(\"Class %s expects class %s to be a value class, but it is an identity class\",\n+                    _class_name->as_C_string(),\n+                    InstanceKlass::cast(klass)->external_name()));\n+        }\n+        if (klass->is_abstract()) {\n+          assert(klass->is_instance_klass(), \"Sanity check\");\n+          ResourceMark rm(THREAD);\n+          THROW_MSG(vmSymbols::java_lang_IncompatibleClassChangeError(),\n+                    err_msg(\"Class %s expects class %s to be concrete value type, but it is an abstract class\",\n+                    _class_name->as_C_string(),\n+                    InstanceKlass::cast(klass)->external_name()));\n+        }\n+        InlineKlass* vk = InlineKlass::cast(klass);\n+        if (!vk->is_implicitly_constructible()) {\n+          THROW_MSG(vmSymbols::java_lang_IncompatibleClassChangeError(),\n+                    err_msg(\"class %s is not implicitly constructible and it is used in a null restricted non-static field (not supported)\",\n+                    klass->name()->as_C_string()));\n+        }\n+        _inline_type_field_klasses->at_put(fieldinfo.index(), vk);\n+        log_info(class, preload)(\"Preloading of class %s during loading of class %s (cause: null-free non-static field) succeeded\", s->as_C_string(), _class_name->as_C_string());\n+      } else if (Signature::has_envelope(sig)) {\n+        \/\/ Preloading classes for nullable fields that are listed in the LoadableDescriptors attribute\n+        \/\/ Those classes would be required later for the flattening of nullable inline type fields\n+        TempNewSymbol name = Signature::strip_envelope(sig);\n+        if (name != _class_name && is_class_in_loadable_descriptors_attribute(sig)) {\n+          log_info(class, preload)(\"Preloading class %s during loading of class %s. Cause: field type in LoadableDescriptors attribute\", name->as_C_string(), _class_name->as_C_string());\n+          oop loader = loader_data()->class_loader();\n+          Klass* klass = SystemDictionary::resolve_with_circularity_detection_or_fail(_class_name, name, Handle(THREAD, loader), _protection_domain, false, THREAD);\n+          if (klass != nullptr) {\n+            if (klass->is_inline_klass()) {\n+              _inline_type_field_klasses->at_put(fieldinfo.index(), InlineKlass::cast(klass));\n+              log_info(class, preload)(\"Preloading of class %s during loading of class %s (cause: field type in LoadableDescriptors attribute) succeeded\", name->as_C_string(), _class_name->as_C_string());\n+            } else {\n+              \/\/ Non value class are allowed by the current spec, but it could be an indication of an issue so let's log a warning\n+              log_warning(class, preload)(\"Preloading class %s during loading of class %s (cause: field type in LoadableDescriptors attribute) but loaded class is not a value class\", name->as_C_string(), _class_name->as_C_string());\n+            }\n+            } else {\n+            log_warning(class, preload)(\"Preloading of class %s during loading of class %s (cause: field type in LoadableDescriptors attribute) failed : %s\",\n+                                          name->as_C_string(), _class_name->as_C_string(), PENDING_EXCEPTION->klass()->name()->as_C_string());\n+          }\n+          \/\/ Loads triggered by the LoadableDescriptors attribute are speculative, failures must not impact loading of current class\n+          if (HAS_PENDING_EXCEPTION) {\n+            CLEAR_PENDING_EXCEPTION;\n+          }\n+        }\n+      }\n+    }\n+  }\n+\n@@ -5874,2 +6412,4 @@\n-  FieldLayoutBuilder lb(class_name(), super_klass(), _cp, \/*_fields*\/ _temp_field_info,\n-                        _parsed_annotations->is_contended(), _field_info);\n+  FieldLayoutBuilder lb(class_name(), loader_data(), super_klass(), _cp, \/*_fields*\/ _temp_field_info,\n+      _parsed_annotations->is_contended(), is_inline_type(),\n+      access_flags().is_abstract() && !access_flags().is_identity_class() && !access_flags().is_interface(),\n+      _field_info, _inline_type_field_klasses);\n@@ -5877,0 +6417,7 @@\n+  if (is_inline_type()) {\n+    _alignment = lb.get_alignment();\n+    _first_field_offset = lb.get_first_field_offset();\n+    _payload_size_in_bytes = lb.get_payload_size_in_byte();\n+    _internal_null_marker_offset = lb.get_internal_null_marker_offset();\n+  }\n+  _has_inline_type_fields = _field_info->_has_inline_fields;\n@@ -5882,0 +6429,1 @@\n+\n@@ -5885,0 +6433,11 @@\n+  if (_field_info->_has_null_marker_offsets) {\n+    int idx = 0;\n+    _null_marker_offsets = MetadataFactory::new_array<int>(_loader_data, _temp_field_info->length(), 0, CHECK);\n+    for (GrowableArrayIterator<FieldInfo> it = _temp_field_info->begin(); it != _temp_field_info->end(); ++it, ++idx) {\n+      FieldInfo fieldinfo = *it;\n+      if (fieldinfo.field_flags().has_null_marker()) {\n+        assert(fieldinfo.null_marker_offset() != 0, \"Invalid value\");\n+        _null_marker_offsets->at_put(idx, fieldinfo.null_marker_offset());\n+      }\n+    }\n+  }\n","filename":"src\/hotspot\/share\/classfile\/classFileParser.cpp","additions":707,"deletions":148,"binary":false,"changes":855,"status":"modified"},{"patch":"@@ -56,0 +56,2 @@\n+#include \"oops\/flatArrayKlass.hpp\"\n+#include \"oops\/inlineKlass.inline.hpp\"\n@@ -57,1 +59,1 @@\n-#include \"oops\/instanceMirrorKlass.hpp\"\n+#include \"oops\/instanceMirrorKlass.inline.hpp\"\n@@ -1045,1 +1047,9 @@\n-    if (k->is_typeArray_klass()) {\n+    if (k->is_flatArray_klass()) {\n+      Klass* element_klass = (Klass*) FlatArrayKlass::cast(k)->element_klass();\n+      assert(element_klass->is_inline_klass(), \"Must be inline type component\");\n+      if (is_scratch) {\n+        comp_mirror = Handle(THREAD, HeapShared::scratch_java_mirror(element_klass));\n+      } else {\n+        comp_mirror = Handle(THREAD, element_klass->java_mirror());\n+      }\n+    } else if (k->is_typeArray_klass()) {\n@@ -1056,0 +1066,1 @@\n+      oop comp_oop = element_klass->java_mirror();\n@@ -1059,1 +1070,1 @@\n-        comp_mirror = Handle(THREAD, element_klass->java_mirror());\n+        comp_mirror = Handle(THREAD, comp_oop);\n@@ -1121,0 +1132,1 @@\n+\n@@ -1143,3 +1155,3 @@\n-  if (k->class_loader() != nullptr &&\n-      k->class_loader() != SystemDictionary::java_platform_loader() &&\n-      k->class_loader() != SystemDictionary::java_system_loader()) {\n+  if ((k->class_loader() != nullptr &&\n+       k->class_loader() != SystemDictionary::java_platform_loader() &&\n+       k->class_loader() != SystemDictionary::java_system_loader())) {\n@@ -1358,1 +1370,3 @@\n-  if (is_instance)  st->print(\"L\");\n+  if (is_instance)  {\n+    st->print(\"L\");\n+  }\n@@ -1417,0 +1431,4 @@\n+  if (klass->is_flatArray_klass() || (klass->is_objArray_klass() && ObjArrayKlass::cast(klass)->is_null_free_array_klass())) {\n+    \/\/ TODO 8336006 Ignore flat \/ null-free arrays\n+    return;\n+  }\n@@ -2696,1 +2714,1 @@\n-      if (method->name() == vmSymbols::object_initializer_name() &&\n+      if (method->is_object_constructor() &&\n@@ -3055,2 +3073,2 @@\n-  if (m->is_initializer()) {\n-    flags |= java_lang_invoke_MemberName::MN_IS_CONSTRUCTOR;\n+  if (m->is_object_constructor()) {\n+    flags |= java_lang_invoke_MemberName::MN_IS_OBJECT_CONSTRUCTOR;\n@@ -3446,1 +3464,1 @@\n-int java_lang_reflect_Field::_trusted_final_offset;\n+int java_lang_reflect_Field::_flags_offset;\n@@ -3456,1 +3474,1 @@\n-  macro(_trusted_final_offset,    k, vmSymbols::trusted_final_name(),    bool_signature,       false); \\\n+  macro(_flags_offset,     k, vmSymbols::flags_name(),     int_signature,    false); \\\n@@ -3521,2 +3539,2 @@\n-void java_lang_reflect_Field::set_trusted_final(oop field) {\n-  field->bool_field_put(_trusted_final_offset, true);\n+void java_lang_reflect_Field::set_flags(oop field, int value) {\n+  field->int_field_put(_flags_offset, value);\n@@ -4364,1 +4382,1 @@\n-  return (flags(mname) & (MN_IS_METHOD | MN_IS_CONSTRUCTOR)) > 0;\n+  return (flags(mname) & (MN_IS_METHOD | MN_IS_OBJECT_CONSTRUCTOR)) > 0;\n","filename":"src\/hotspot\/share\/classfile\/javaClasses.cpp","additions":33,"deletions":15,"binary":false,"changes":48,"status":"modified"},{"patch":"@@ -238,0 +238,1 @@\n+\n@@ -252,0 +253,1 @@\n+\n@@ -295,0 +297,2 @@\n+  static int component_mirror_offset()     { CHECK_INIT(_component_mirror_offset); }\n+\n@@ -310,2 +314,0 @@\n-  static int component_mirror_offset() { return _component_mirror_offset; }\n-\n@@ -776,1 +778,1 @@\n-  static int _trusted_final_offset;\n+  static int _flags_offset;\n@@ -804,1 +806,1 @@\n-  static void set_trusted_final(oop field);\n+  static void set_flags(oop field, int value);\n@@ -1303,1 +1305,1 @@\n-    MN_IS_CONSTRUCTOR        = 0x00020000, \/\/ constructor\n+    MN_IS_OBJECT_CONSTRUCTOR = 0x00020000, \/\/ constructor\n@@ -1309,2 +1311,4 @@\n-    MN_REFERENCE_KIND_SHIFT  = 24, \/\/ refKind\n-    MN_REFERENCE_KIND_MASK   = 0x0F000000 >> MN_REFERENCE_KIND_SHIFT,\n+    MN_FLAT_FIELD            = 0x00800000, \/\/ flat field\n+    MN_NULL_RESTRICTED_FIELD = 0x01000000, \/\/ null-restricted field\n+    MN_REFERENCE_KIND_SHIFT  = 26, \/\/ refKind\n+    MN_REFERENCE_KIND_MASK   = 0x3C000000 >> MN_REFERENCE_KIND_SHIFT,\n@@ -1874,1 +1878,0 @@\n-\n","filename":"src\/hotspot\/share\/classfile\/javaClasses.hpp","additions":11,"deletions":8,"binary":false,"changes":19,"status":"modified"},{"patch":"@@ -59,0 +59,1 @@\n+#include \"oops\/fieldStreams.inline.hpp\"\n@@ -70,0 +71,1 @@\n+#include \"oops\/inlineKlass.inline.hpp\"\n@@ -78,0 +80,1 @@\n+#include \"runtime\/os.hpp\"\n@@ -352,1 +355,1 @@\n-      \/\/ Ignore wrapping L and ;.\n+      \/\/ Ignore wrapping L and ; (and Q and ; for value types).\n@@ -457,0 +460,1 @@\n+    \/\/ Otherwise, a LinkageError will be thrown later.\n@@ -956,2 +960,1 @@\n-  assert(!ModuleEntryTable::javabase_moduleEntry()->is_patched(),\n-         \"Cannot use sharing if java.base is patched\");\n+  assert(!CDSConfig::module_patching_disables_cds(), \"Cannot use CDS\");\n@@ -1033,1 +1036,1 @@\n-        assert(!mod_entry->is_patched(), \"cannot load archived classes for patched module\");\n+        assert(!CDSConfig::module_patching_disables_cds(), \"Cannot use CDS\");\n@@ -1159,0 +1162,32 @@\n+  if (ik->has_inline_type_fields()) {\n+    for (AllFieldStream fs(ik); !fs.done(); fs.next()) {\n+      if (fs.access_flags().is_static()) continue;\n+      Symbol* sig = fs.signature();\n+      if (fs.is_null_free_inline_type()) {\n+        \/\/ Pre-load inline class\n+        TempNewSymbol name = Signature::strip_envelope(sig);\n+        Klass* real_k = SystemDictionary::resolve_with_circularity_detection_or_fail(ik->name(), name,\n+          class_loader, protection_domain, false, CHECK_NULL);\n+        Klass* k = ik->get_inline_type_field_klass_or_null(fs.index());\n+        if (real_k != k) {\n+          \/\/ oops, the app has substituted a different version of k!\n+          return nullptr;\n+        }\n+      } else if (Signature::has_envelope(sig)) {\n+        TempNewSymbol name = Signature::strip_envelope(sig);\n+        if (name != ik->name() && ik->is_class_in_loadable_descriptors_attribute(name)) {\n+          Klass* real_k = SystemDictionary::resolve_with_circularity_detection_or_fail(ik->name(), name,\n+            class_loader, protection_domain, false, THREAD);\n+          if (HAS_PENDING_EXCEPTION) {\n+            CLEAR_PENDING_EXCEPTION;\n+          }\n+          Klass* k = ik->get_inline_type_field_klass_or_null(fs.index());\n+          if (real_k != k) {\n+            \/\/ oops, the app has substituted a different version of k!\n+            return nullptr;\n+          }\n+        }\n+      }\n+    }\n+  }\n+\n@@ -1194,0 +1229,1 @@\n+\n","filename":"src\/hotspot\/share\/classfile\/systemDictionary.cpp","additions":40,"deletions":4,"binary":false,"changes":44,"status":"modified"},{"patch":"@@ -1700,1 +1700,2 @@\n-      || m->number_of_breakpoints() > 0) {\n+      || m->number_of_breakpoints() > 0\n+      || m->mismatch()) {\n","filename":"src\/hotspot\/share\/code\/dependencies.cpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -627,4 +627,12 @@\n-    case Bytecodes::_return:\n-      if (instruction->method()->intrinsic_id() == vmIntrinsics::_Object_init) {\n-        \/\/ return from Object.init implicitly registers a finalizer\n-        \/\/ for the receiver if needed, so keep it alive.\n+    case Bytecodes::_return: {\n+      ciMethod* method = instruction->method();\n+      ciInstanceKlass* holder = method->holder();\n+      const bool abstract_klass = holder->is_abstract();\n+      const bool concrete_value_klass = !abstract_klass && holder->is_inlinetype();\n+      if (method->intrinsic_id() == vmIntrinsics::_Object_init ||\n+          (method->is_object_constructor() && (concrete_value_klass || abstract_klass))) {\n+        \/\/ Returning from Object.<init> implicitly registers a finalizer for the receiver if needed, to keep it alive.\n+        \/\/ Value class constructors update the scalarized receiver. We need to keep it live so that we can find it after\n+        \/\/ (chained) constructor calls and propagate updates to the caller. If the holder of the constructor is abstract,\n+        \/\/ we do not know if the constructor was called on a value class or not. We therefore keep the receiver of all\n+        \/\/ abstract constructors live.\n@@ -634,2 +642,1 @@\n-\n-\n+    }\n","filename":"src\/hotspot\/share\/compiler\/methodLiveness.cpp","additions":13,"deletions":6,"binary":false,"changes":19,"status":"modified"},{"patch":"@@ -47,0 +47,1 @@\n+#include \"oops\/flatArrayKlass.inline.hpp\"\n","filename":"src\/hotspot\/share\/gc\/parallel\/psPromotionManager.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -177,0 +177,3 @@\n+JNIEXPORT jboolean JNICALL\n+JVM_IsValhallaEnabled(void);\n+\n@@ -422,1 +425,1 @@\n- * Find a class from a boot class loader. Returns NULL if class not found.\n+ * Find a class from a boot class loader. Returns nullptr if class not found.\n@@ -567,0 +570,6 @@\n+JNIEXPORT jboolean JNICALL\n+JVM_IsIdentityClass(JNIEnv *env, jclass cls);\n+\n+JNIEXPORT jboolean JNICALL\n+JVM_IsImplicitlyConstructibleClass(JNIEnv *env, jclass cls);\n+\n@@ -1116,0 +1125,6 @@\n+JNIEXPORT jarray JNICALL\n+JVM_NewNullRestrictedArray(JNIEnv *env, jclass elmClass, jint len);\n+\n+JNIEXPORT jboolean JNICALL\n+JVM_IsNullRestrictedArray(JNIEnv *env, jobject obj);\n+\n","filename":"src\/hotspot\/share\/include\/jvm.h","additions":16,"deletions":1,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -210,1 +210,1 @@\n-  nonstatic_field(InstanceKlass,               _misc_flags._flags,                            u2)                                    \\\n+  nonstatic_field(InstanceKlass,               _misc_flags._flags,                            u4)                                    \\\n@@ -680,0 +680,3 @@\n+  declare_constant(DataLayout::array_store_data_tag)                      \\\n+  declare_constant(DataLayout::array_load_data_tag)                       \\\n+  declare_constant(DataLayout::acmp_data_tag)                             \\\n","filename":"src\/hotspot\/share\/jvmci\/vmStructs_jvmci.cpp","additions":4,"deletions":1,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -75,0 +75,1 @@\n+#include \"oops\/inlineKlass.hpp\"\n@@ -172,0 +173,13 @@\n+bool InstanceKlass::field_is_null_free_inline_type(int index) const {\n+  return field(index).field_flags().is_null_free_inline_type();\n+}\n+\n+bool InstanceKlass::is_class_in_loadable_descriptors_attribute(Symbol* name) const {\n+  if (_loadable_descriptors == nullptr) return false;\n+  for (int i = 0; i < _loadable_descriptors->length(); i++) {\n+        Symbol* class_name = _constants->klass_at_noresolve(_loadable_descriptors->at(i));\n+        if (class_name == name) return true;\n+  }\n+  return false;\n+}\n+\n@@ -444,1 +458,2 @@\n-                                       parser.is_interface());\n+                                       parser.is_interface(),\n+                                       parser.is_inline_type());\n@@ -466,0 +481,3 @@\n+  } else if (parser.is_inline_type()) {\n+    \/\/ inline type\n+    ik = new (loader_data, size, THREAD) InlineKlass(parser);\n@@ -477,0 +495,6 @@\n+#ifdef ASSERT\n+  ik->bounds_check((address) ik->start_of_vtable(), false, size);\n+  ik->bounds_check((address) ik->start_of_itable(), false, size);\n+  ik->bounds_check((address) ik->end_of_itable(), true, size);\n+  ik->bounds_check((address) ik->end_of_nonstatic_oop_maps(), true, size);\n+#endif \/\/ASSERT\n@@ -480,0 +504,23 @@\n+#ifndef PRODUCT\n+bool InstanceKlass::bounds_check(address addr, bool edge_ok, intptr_t size_in_bytes) const {\n+  const char* bad = nullptr;\n+  address end = nullptr;\n+  if (addr < (address)this) {\n+    bad = \"before\";\n+  } else if (addr == (address)this) {\n+    if (edge_ok)  return true;\n+    bad = \"just before\";\n+  } else if (addr == (end = (address)this + sizeof(intptr_t) * (size_in_bytes < 0 ? size() : size_in_bytes))) {\n+    if (edge_ok)  return true;\n+    bad = \"just after\";\n+  } else if (addr > end) {\n+    bad = \"after\";\n+  } else {\n+    return true;\n+  }\n+  tty->print_cr(\"%s object bounds: \" INTPTR_FORMAT \" [\" INTPTR_FORMAT \"..\" INTPTR_FORMAT \"]\",\n+      bad, (intptr_t)addr, (intptr_t)this, (intptr_t)end);\n+  Verbose = WizardMode = true; this->print(); \/\/@@\n+  return false;\n+}\n+#endif \/\/PRODUCT\n@@ -519,1 +566,5 @@\n-  _init_thread(nullptr)\n+  _init_thread(nullptr),\n+  _inline_type_field_klasses(nullptr),\n+  _null_marker_offsets(nullptr),\n+  _loadable_descriptors(nullptr),\n+  _adr_inlineklass_fixed_block(nullptr)\n@@ -526,0 +577,3 @@\n+  if (parser.has_inline_fields()) {\n+    set_has_inline_type_fields();\n+  }\n@@ -665,0 +719,10 @@\n+  if (inline_type_field_klasses_array() != nullptr) {\n+    MetadataFactory::free_array<InlineKlass*>(loader_data, inline_type_field_klasses_array());\n+    set_inline_type_field_klasses_array(nullptr);\n+  }\n+\n+  if (null_marker_offsets_array() != nullptr) {\n+    MetadataFactory::free_array<int>(loader_data, null_marker_offsets_array());\n+    set_null_marker_offsets_array(nullptr);\n+  }\n+\n@@ -699,0 +763,7 @@\n+  if (loadable_descriptors() != nullptr &&\n+      loadable_descriptors() != Universe::the_empty_short_array() &&\n+      !loadable_descriptors()->is_shared()) {\n+    MetadataFactory::free_array<jushort>(loader_data, loadable_descriptors());\n+  }\n+  set_loadable_descriptors(nullptr);\n+\n@@ -857,0 +928,102 @@\n+\n+  \/\/ If a class declares a method that uses an inline class as an argument\n+  \/\/ type or return inline type, this inline class must be loaded during the\n+  \/\/ linking of this class because size and properties of the inline class\n+  \/\/ must be known in order to be able to perform inline type optimizations.\n+  \/\/ The implementation below is an approximation of this rule, the code\n+  \/\/ iterates over all methods of the current class (including overridden\n+  \/\/ methods), not only the methods declared by this class. This\n+  \/\/ approximation makes the code simpler, and doesn't change the semantic\n+  \/\/ because classes declaring methods overridden by the current class are\n+  \/\/ linked (and have performed their own pre-loading) before the linking\n+  \/\/ of the current class.\n+\n+\n+  \/\/ Note:\n+  \/\/ Inline class types are loaded during\n+  \/\/ the loading phase (see ClassFileParser::post_process_parsed_stream()).\n+  \/\/ Inline class types used as element types for array creation\n+  \/\/ are not pre-loaded. Their loading is triggered by either anewarray\n+  \/\/ or multianewarray bytecodes.\n+\n+  \/\/ Could it be possible to do the following processing only if the\n+  \/\/ class uses inline types?\n+  if (EnableValhalla) {\n+    ResourceMark rm(THREAD);\n+    for (AllFieldStream fs(this); !fs.done(); fs.next()) {\n+      if (fs.is_null_free_inline_type() && fs.access_flags().is_static()) {\n+        Symbol* sig = fs.signature();\n+        TempNewSymbol s = Signature::strip_envelope(sig);\n+        if (s != name()) {\n+          log_info(class, preload)(\"Preloading class %s during linking of class %s. Cause: a null-free static field is declared with this type\", s->as_C_string(), name()->as_C_string());\n+          Klass* klass = SystemDictionary::resolve_or_fail(s,\n+                                                          Handle(THREAD, class_loader()), Handle(THREAD, protection_domain()), true,\n+                                                          CHECK_false);\n+          if (HAS_PENDING_EXCEPTION) {\n+            log_warning(class, preload)(\"Preloading of class %s during linking of class %s (cause: null-free static field) failed: %s\",\n+                                      s->as_C_string(), name()->as_C_string(), PENDING_EXCEPTION->klass()->name()->as_C_string());\n+            return false; \/\/ Exception is still pending\n+          }\n+          log_info(class, preload)(\"Preloading of class %s during linking of class %s (cause: null-free static field) succeeded\",\n+                                   s->as_C_string(), name()->as_C_string());\n+          assert(klass != nullptr, \"Sanity check\");\n+          if (!klass->is_inline_klass()) {\n+            THROW_MSG_(vmSymbols::java_lang_IncompatibleClassChangeError(),\n+                       err_msg(\"class %s expects class %s to be a value class but it is an identity class\",\n+                       name()->as_C_string(), klass->external_name()), false);\n+          }\n+          if (klass->is_abstract()) {\n+            THROW_MSG_(vmSymbols::java_lang_IncompatibleClassChangeError(),\n+                      err_msg(\"Class %s expects class %s to be concrete value class, but it is an abstract class\",\n+                      name()->as_C_string(),\n+                      InstanceKlass::cast(klass)->external_name()), false);\n+          }\n+          InstanceKlass* ik = InstanceKlass::cast(klass);\n+          if (!ik->is_implicitly_constructible()) {\n+             THROW_MSG_(vmSymbols::java_lang_IncompatibleClassChangeError(),\n+                        err_msg(\"class %s is not implicitly constructible and it is used in a null restricted static field (not supported)\",\n+                        klass->external_name()), false);\n+          }\n+          \/\/ the inline_type_field_klasses_array might have been loaded with CDS, so update only if not already set and check consistency\n+          if (inline_type_field_klasses_array()->at(fs.index()) == nullptr) {\n+            set_inline_type_field_klass(fs.index(), InlineKlass::cast(ik));\n+          }\n+          assert(get_inline_type_field_klass(fs.index()) == ik, \"Must match\");\n+        } else {\n+          if (inline_type_field_klasses_array()->at(fs.index()) == nullptr) {\n+            set_inline_type_field_klass(fs.index(), InlineKlass::cast(this));\n+          }\n+          assert(get_inline_type_field_klass(fs.index()) == this, \"Must match\");\n+        }\n+      }\n+    }\n+\n+    \/\/ Aggressively preloading all classes from the LoadableDescriptors attribute\n+    if (loadable_descriptors() != nullptr) {\n+      HandleMark hm(THREAD);\n+      for (int i = 0; i < loadable_descriptors()->length(); i++) {\n+        Symbol* sig = constants()->symbol_at(loadable_descriptors()->at(i));\n+        if (!Signature::has_envelope(sig)) continue;\n+        TempNewSymbol class_name = Signature::strip_envelope(sig);\n+        if (class_name == name()) continue;\n+        log_info(class, preload)(\"Preloading class %s during linking of class %s because of the class is listed in the LoadableDescriptors attribute\", sig->as_C_string(), name()->as_C_string());\n+        oop loader = class_loader();\n+        oop protection_domain = this->protection_domain();\n+        Klass* klass = SystemDictionary::resolve_or_null(class_name,\n+                                                         Handle(THREAD, loader), Handle(THREAD, protection_domain), THREAD);\n+        if (HAS_PENDING_EXCEPTION) {\n+          CLEAR_PENDING_EXCEPTION;\n+        }\n+        if (klass != nullptr) {\n+          log_info(class, preload)(\"Preloading of class %s during linking of class %s (cause: LoadableDescriptors attribute) succeeded\", class_name->as_C_string(), name()->as_C_string());\n+          if (!klass->is_inline_klass()) {\n+            \/\/ Non value class are allowed by the current spec, but it could be an indication of an issue so let's log a warning\n+              log_warning(class, preload)(\"Preloading class %s during linking of class %s (cause: LoadableDescriptors attribute) but loaded class is not a value class\", class_name->as_C_string(), name()->as_C_string());\n+          }\n+        } else {\n+          log_warning(class, preload)(\"Preloading of class %s during linking of class %s (cause: LoadableDescriptors attribute) failed\", class_name->as_C_string(), name()->as_C_string());\n+        }\n+      }\n+    }\n+  }\n+\n@@ -1161,0 +1334,19 @@\n+  \/\/ Pre-allocating an instance of the default value\n+  if (is_inline_klass()) {\n+      InlineKlass* vk = InlineKlass::cast(this);\n+      oop val = vk->allocate_instance(THREAD);\n+      if (HAS_PENDING_EXCEPTION) {\n+          Handle e(THREAD, PENDING_EXCEPTION);\n+          CLEAR_PENDING_EXCEPTION;\n+          {\n+              EXCEPTION_MARK;\n+              add_initialization_error(THREAD, e);\n+              \/\/ Locks object, set state, and notify all waiting threads\n+              set_initialization_state_and_notify(initialization_error, THREAD);\n+              CLEAR_PENDING_EXCEPTION;\n+          }\n+          THROW_OOP(e());\n+      }\n+      vk->set_default_value(val);\n+  }\n+\n@@ -1193,1 +1385,33 @@\n-\n+  \/\/ Initialize classes of inline fields\n+  if (EnableValhalla) {\n+    for (AllFieldStream fs(this); !fs.done(); fs.next()) {\n+      if (fs.is_null_free_inline_type()) {\n+\n+        \/\/ inline type field klass array entries must have alreadyt been filed at load time or link time\n+        Klass* klass = get_inline_type_field_klass(fs.index());\n+\n+        InstanceKlass::cast(klass)->initialize(THREAD);\n+        if (fs.access_flags().is_static()) {\n+          if (java_mirror()->obj_field(fs.offset()) == nullptr) {\n+            java_mirror()->obj_field_put(fs.offset(), InlineKlass::cast(klass)->default_value());\n+          }\n+        }\n+\n+        if (HAS_PENDING_EXCEPTION) {\n+          Handle e(THREAD, PENDING_EXCEPTION);\n+          CLEAR_PENDING_EXCEPTION;\n+          {\n+            EXCEPTION_MARK;\n+            add_initialization_error(THREAD, e);\n+            \/\/ Locks object, set state, and notify all waiting threads\n+            set_initialization_state_and_notify(initialization_error, THREAD);\n+            CLEAR_PENDING_EXCEPTION;\n+          }\n+          THROW_OOP(e());\n+        }\n+      }\n+    }\n+  }\n+\n+\n+  \/\/ Step 9\n@@ -1216,1 +1440,1 @@\n-  \/\/ Step 9\n+  \/\/ Step 10\n@@ -1222,1 +1446,1 @@\n-    \/\/ Step 10 and 11\n+    \/\/ Step 11 and 12\n@@ -1527,1 +1751,1 @@\n-      ObjArrayKlass* k = ObjArrayKlass::allocate_objArray_klass(class_loader_data(), 1, this, CHECK_NULL);\n+      ObjArrayKlass* k = ObjArrayKlass::allocate_objArray_klass(class_loader_data(), 1, this, false, CHECK_NULL);\n@@ -1534,1 +1758,1 @@\n-  ObjArrayKlass* ak = array_klasses();\n+  ArrayKlass* ak = array_klasses();\n@@ -1541,2 +1765,2 @@\n-  ObjArrayKlass* oak = array_klasses_acquire();\n-  if (oak == nullptr) {\n+  ArrayKlass* ak = array_klasses_acquire();\n+  if (ak == nullptr) {\n@@ -1545,1 +1769,1 @@\n-    return oak->array_klass_or_null(n);\n+    return ak->array_klass_or_null(n);\n@@ -1562,1 +1786,1 @@\n-  if (clinit != nullptr && clinit->has_valid_initializer_flags()) {\n+  if (clinit != nullptr && clinit->is_class_initializer()) {\n@@ -1626,4 +1850,0 @@\n-bool InstanceKlass::contains_field_offset(int offset) {\n-  fieldDescriptor fd;\n-  return find_field_from_offset(offset, false, &fd);\n-}\n@@ -1711,0 +1931,9 @@\n+bool InstanceKlass::contains_field_offset(int offset) {\n+  if (this->is_inline_klass()) {\n+    InlineKlass* vk = InlineKlass::cast(this);\n+    return offset >= vk->first_field_offset() && offset < (vk->first_field_offset() + vk->get_payload_size_in_bytes());\n+  } else {\n+    fieldDescriptor fd;\n+    return find_field_from_offset(offset, false, &fd);\n+  }\n+}\n@@ -2102,0 +2331,3 @@\n+    if (name == vmSymbols::object_initializer_name()) {\n+      break;  \/\/ <init> is never inherited\n+    }\n@@ -2498,0 +2730,1 @@\n+  it->push(&_loadable_descriptors);\n@@ -2499,0 +2732,3 @@\n+\n+  it->push(&_inline_type_field_klasses, MetaspaceClosure::_writable);\n+  it->push(&_null_marker_offsets);\n@@ -2544,1 +2780,1 @@\n-  \/\/ These are not allocated from metaspace. They are safe to set to null.\n+  \/\/ These are not allocated from metaspace. They are safe to set to nullptr.\n@@ -2628,0 +2864,4 @@\n+  if (is_inline_klass()) {\n+    InlineKlass::cast(this)->initialize_calling_convention(CHECK);\n+  }\n+\n@@ -2656,1 +2896,1 @@\n-    assert(this == array_klasses()->bottom_klass(), \"sanity\");\n+    assert(this == ObjArrayKlass::cast(array_klasses())->bottom_klass(), \"sanity\");\n@@ -2849,0 +3089,2 @@\n+  return signature_name_of_carrier(JVM_SIGNATURE_CLASS);\n+}\n@@ -2850,0 +3092,1 @@\n+const char* InstanceKlass::signature_name_of_carrier(char c) const {\n@@ -2856,1 +3099,1 @@\n-  \/\/ Add L as type indicator\n+  \/\/ Add L or Q as type indicator\n@@ -2858,1 +3101,1 @@\n-  dest[dest_index++] = JVM_SIGNATURE_CLASS;\n+  dest[dest_index++] = c;\n@@ -3200,2 +3443,1 @@\n-  \/\/ Remember to strip ACC_SUPER bit\n-  return (access & (~JVM_ACC_SUPER)) & JVM_ACC_WRITTEN_FLAGS;\n+  return (access & JVM_ACC_WRITTEN_FLAGS);\n@@ -3455,1 +3697,4 @@\n-static void print_vtable(intptr_t* start, int len, outputStream* st) {\n+static void print_vtable(address self, intptr_t* start, int len, outputStream* st) {\n+  ResourceMark rm;\n+  int* forward_refs = NEW_RESOURCE_ARRAY(int, len);\n+  for (int i = 0; i < len; i++)  forward_refs[i] = 0;\n@@ -3459,0 +3704,5 @@\n+    if (forward_refs[i] != 0) {\n+      int from = forward_refs[i];\n+      int off = (int) start[from];\n+      st->print(\" (offset %d <= [%d])\", off, from);\n+    }\n@@ -3462,0 +3712,6 @@\n+    } else if (self != nullptr && e > 0 && e < 0x10000) {\n+      address location = self + e;\n+      int index = (int)((intptr_t*)location - start);\n+      st->print(\" (offset %d => [%d])\", (int)e, index);\n+      if (index >= 0 && index < len)\n+        forward_refs[index] = i;\n@@ -3468,1 +3724,22 @@\n-  return print_vtable(reinterpret_cast<intptr_t*>(start), len, st);\n+  return print_vtable(nullptr, reinterpret_cast<intptr_t*>(start), len, st);\n+}\n+\n+template<typename T>\n+ static void print_array_on(outputStream* st, Array<T>* array) {\n+   if (array == nullptr) { st->print_cr(\"nullptr\"); return; }\n+   array->print_value_on(st); st->cr();\n+   if (Verbose || WizardMode) {\n+     for (int i = 0; i < array->length(); i++) {\n+       st->print(\"%d : \", i); array->at(i)->print_value_on(st); st->cr();\n+     }\n+   }\n+ }\n+\n+static void print_array_on(outputStream* st, Array<int>* array) {\n+  if (array == nullptr) { st->print_cr(\"nullptr\"); return; }\n+  array->print_value_on(st); st->cr();\n+  if (Verbose || WizardMode) {\n+    for (int i = 0; i < array->length(); i++) {\n+      st->print(\"%d : %d\", i, array->at(i)); st->cr();\n+    }\n+  }\n@@ -3509,8 +3786,2 @@\n-  st->print(BULLET\"methods:           \"); methods()->print_value_on(st);               st->cr();\n-  if (Verbose || WizardMode) {\n-    Array<Method*>* method_array = methods();\n-    for (int i = 0; i < method_array->length(); i++) {\n-      st->print(\"%d : \", i); method_array->at(i)->print_value(); st->cr();\n-    }\n-  }\n-  st->print(BULLET\"method ordering:   \"); method_ordering()->print_value_on(st);      st->cr();\n+  st->print(BULLET\"methods:           \"); print_array_on(st, methods());\n+  st->print(BULLET\"method ordering:   \"); print_array_on(st, method_ordering());\n@@ -3518,7 +3789,1 @@\n-    st->print(BULLET\"default_methods:   \"); default_methods()->print_value_on(st);    st->cr();\n-    if (Verbose) {\n-      Array<Method*>* method_array = default_methods();\n-      for (int i = 0; i < method_array->length(); i++) {\n-        st->print(\"%d : \", i); method_array->at(i)->print_value(); st->cr();\n-      }\n-    }\n+    st->print(BULLET\"default_methods:   \"); print_array_on(st, default_methods());\n@@ -3584,0 +3849,1 @@\n+  st->print(BULLET\"loadable descriptors:     \"); loadable_descriptors()->print_value_on(st); st->cr();\n@@ -3594,1 +3860,1 @@\n-  if (itable_length() > 0 && (Verbose || WizardMode))  print_vtable(start_of_itable(), itable_length(), st);\n+  if (itable_length() > 0 && (Verbose || WizardMode))  print_vtable(nullptr, start_of_itable(), itable_length(), st);\n","filename":"src\/hotspot\/share\/oops\/instanceKlass.cpp","additions":305,"deletions":39,"binary":false,"changes":344,"status":"modified"},{"patch":"@@ -263,1 +263,2 @@\n-                           _shared_class_path_index(-1) {\n+                               _prototype_header(markWord::prototype()),\n+                               _shared_class_path_index(-1) {\n@@ -277,1 +278,1 @@\n-  int lh = array_layout_helper(tag, hsize, etype, exact_log2(esize));\n+  int lh = array_layout_helper(tag, false, hsize, etype, exact_log2(esize));\n@@ -973,0 +974,2 @@\n+     st->print(BULLET\"prototype_header: \" INTPTR_FORMAT, _prototype_header.value());\n+     st->cr();\n","filename":"src\/hotspot\/share\/oops\/klass.cpp","additions":5,"deletions":2,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -74,0 +74,1 @@\n+    InlineKlassKind,\n@@ -79,0 +80,1 @@\n+    FlatArrayKlassKind,\n@@ -105,1 +107,1 @@\n-  \/\/    tag is 0x80 if the elements are oops, 0xC0 if non-oops\n+  \/\/    tag is 0x80 if the elements are oops, 0xC0 if non-oops, 0xA0 if value types\n@@ -174,0 +176,1 @@\n+  markWord _prototype_header;  \/\/ inline type and inline array mark patterns\n@@ -447,1 +450,1 @@\n-  static const int _lh_array_tag_bits          = 2;\n+  static const int _lh_array_tag_bits          = 3;\n@@ -449,2 +452,9 @@\n-  static const int _lh_array_tag_obj_value     = ~0x01;   \/\/ 0x80000000 >> 30\n-  static const unsigned int _lh_array_tag_type_value = 0Xffffffff; \/\/ ~0x00,  \/\/ 0xC0000000 >> 30\n+  static const unsigned int _lh_array_tag_type_value = 0Xfffffffc;\n+  static const unsigned int _lh_array_tag_vt_value   = 0Xfffffffd;\n+  static const unsigned int _lh_array_tag_obj_value  = 0Xfffffffe;\n+\n+  \/\/ null-free array flag bit under the array tag bits, shift one more to get array tag value\n+  static const int _lh_null_free_shift = _lh_array_tag_shift - 1;\n+  static const int _lh_null_free_mask  = 1;\n+\n+  static const jint _lh_array_tag_flat_value_bit_inplace = (jint) (1 << _lh_array_tag_shift);\n@@ -468,2 +478,1 @@\n-    \/\/ _lh_array_tag_type_value == (lh >> _lh_array_tag_shift);\n-    return (juint)lh >= (juint)(_lh_array_tag_type_value << _lh_array_tag_shift);\n+    return (juint) _lh_array_tag_type_value == (juint)(lh >> _lh_array_tag_shift);\n@@ -472,2 +481,13 @@\n-    \/\/ _lh_array_tag_obj_value == (lh >> _lh_array_tag_shift);\n-    return (jint)lh < (jint)(_lh_array_tag_type_value << _lh_array_tag_shift);\n+    return (juint)_lh_array_tag_obj_value == (juint)(lh >> _lh_array_tag_shift);\n+  }\n+  static bool layout_helper_is_flatArray(jint lh) {\n+    return (juint)_lh_array_tag_vt_value == (juint)(lh >> _lh_array_tag_shift);\n+  }\n+  static bool layout_helper_is_null_free(jint lh) {\n+    assert(layout_helper_is_flatArray(lh) || layout_helper_is_objArray(lh), \"must be array of inline types\");\n+    return ((lh >> _lh_null_free_shift) & _lh_null_free_mask);\n+  }\n+  static jint layout_helper_set_null_free(jint lh) {\n+    lh |= (_lh_null_free_mask << _lh_null_free_shift);\n+    assert(layout_helper_is_null_free(lh), \"Bad encoding\");\n+    return lh;\n@@ -484,1 +504,1 @@\n-    assert(btvalue >= T_BOOLEAN && btvalue <= T_OBJECT, \"sanity\");\n+    assert((btvalue >= T_BOOLEAN && btvalue <= T_OBJECT) || btvalue == T_PRIMITIVE_OBJECT, \"sanity\");\n@@ -505,1 +525,1 @@\n-    assert(l2esz <= LogBytesPerLong,\n+    assert(layout_helper_element_type(lh) == T_PRIMITIVE_OBJECT || l2esz <= LogBytesPerLong,\n@@ -509,1 +529,1 @@\n-  static jint array_layout_helper(jint tag, int hsize, BasicType etype, int log2_esize) {\n+  static jint array_layout_helper(jint tag, bool null_free, int hsize, BasicType etype, int log2_esize) {\n@@ -511,0 +531,1 @@\n+      |    ((null_free ? 1 : 0) <<  _lh_null_free_shift)\n@@ -663,0 +684,1 @@\n+  virtual bool is_flatArray_klass_slow()    const { return false; }\n@@ -664,0 +686,2 @@\n+  \/\/ current implementation uses this method even in non debug builds\n+  virtual bool is_inline_klass_slow()       const { return false; }\n@@ -679,2 +703,1 @@\n-  \/\/ Other is anything that is not one of the more specialized kinds of InstanceKlass.\n-  bool is_other_instance_klass()        const { return _kind == InstanceKlassKind; }\n+  bool is_inline_klass()                const { return assert_same_query(_kind == InlineKlassKind, is_inline_klass_slow()); }\n@@ -686,0 +709,1 @@\n+  bool is_flatArray_klass()             const { return assert_same_query( _kind == FlatArrayKlassKind, is_flatArray_klass_slow()); }\n@@ -690,0 +714,2 @@\n+  inline bool is_null_free_array_klass()      const { return layout_helper_is_null_free(layout_helper()); }\n+\n@@ -698,1 +724,1 @@\n-  bool is_super() const                 { return _access_flags.is_super(); }\n+  bool is_identity_class() const        { return _access_flags.is_identity_class(); }\n@@ -715,0 +741,11 @@\n+  \/\/ inline types and inline type array patterns\n+  markWord prototype_header() const {\n+    return _prototype_header;\n+  }\n+  static inline markWord default_prototype_header(Klass* k) {\n+    return (k == nullptr) ? markWord::prototype() : k->prototype_header();\n+  }\n+\n+  inline void set_prototype_header(markWord header);\n+  static ByteSize prototype_header_offset() { return in_ByteSize(offset_of(Klass, _prototype_header)); }\n+\n","filename":"src\/hotspot\/share\/oops\/klass.hpp","additions":51,"deletions":14,"binary":false,"changes":65,"status":"modified"},{"patch":"@@ -54,0 +54,18 @@\n+inline void Klass::set_prototype_header(markWord header) {\n+  assert(!is_inline_klass() || header.is_inline_type(), \"Unexpected prototype\");\n+  assert(_prototype_header.value() == 0 || _prototype_header == markWord::prototype(),\n+         \"Prototype already set\");\n+#ifdef _LP64\n+    assert(header == markWord::prototype() ||\n+           header.is_inline_type() ||\n+           header.is_flat_array() ||\n+           header.is_null_free_array(),\n+           \"unknown prototype header\");\n+#else\n+    assert(header == markWord::prototype() ||\n+           header.is_inline_type(),\n+           \"unknown prototype header\");\n+#endif\n+  _prototype_header = header;\n+}\n+\n","filename":"src\/hotspot\/share\/oops\/klass.inline.hpp","additions":18,"deletions":0,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -89,0 +89,40 @@\n+Symbol* Symbol::fundamental_name(TRAPS) {\n+  if (char_at(0) == JVM_SIGNATURE_CLASS && ends_with(JVM_SIGNATURE_ENDCLASS)) {\n+    return SymbolTable::new_symbol(this, 1, utf8_length() - 1);\n+  } else {\n+    \/\/ reference count is incremented to be consistent with the behavior with\n+    \/\/ the SymbolTable::new_symbol() call above\n+    this->increment_refcount();\n+    return this;\n+  }\n+}\n+\n+bool Symbol::is_same_fundamental_type(Symbol* s) const {\n+  if (this == s) return true;\n+  if (utf8_length() < 3) return false;\n+  int offset1, offset2, len;\n+  if (ends_with(JVM_SIGNATURE_ENDCLASS)) {\n+    if (char_at(0) != JVM_SIGNATURE_CLASS) return false;\n+    offset1 = 1;\n+    len = utf8_length() - 2;\n+  } else {\n+    offset1 = 0;\n+    len = utf8_length();\n+  }\n+  if (ends_with(JVM_SIGNATURE_ENDCLASS)) {\n+    if (s->char_at(0) != JVM_SIGNATURE_CLASS) return false;\n+    offset2 = 1;\n+  } else {\n+    offset2 = 0;\n+  }\n+  if ((offset2 + len) > s->utf8_length()) return false;\n+  if ((utf8_length() - offset1 * 2) != (s->utf8_length() - offset2 * 2))\n+    return false;\n+  int l = len;\n+  while (l-- > 0) {\n+    if (char_at(offset1 + l) != s->char_at(offset2 + l))\n+      return false;\n+  }\n+  return true;\n+}\n+\n","filename":"src\/hotspot\/share\/oops\/symbol.cpp","additions":40,"deletions":0,"binary":false,"changes":40,"status":"modified"},{"patch":"@@ -89,1 +89,1 @@\n-  if (callee_method->is_object_initializer()) {\n+  if (callee_method->is_object_constructor()) {\n@@ -92,1 +92,1 @@\n-  if (caller_method->is_object_initializer() &&\n+  if ((caller_method->is_object_constructor() || caller_method->is_class_initializer()) &&\n","filename":"src\/hotspot\/share\/opto\/bytecodeInfo.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -60,0 +60,1 @@\n+#include \"opto\/inlinetypenode.hpp\"\n@@ -405,0 +406,3 @@\n+  if (dead->is_InlineType()) {\n+    remove_inline_type(dead);\n+  }\n@@ -446,0 +450,3 @@\n+    if (n->outcnt() == 0) {\n+      worklist.push(n);\n+    }\n@@ -453,0 +460,6 @@\n+  remove_useless_nodes(_inline_type_nodes,  useful); \/\/ remove useless inline type nodes\n+#ifdef ASSERT\n+  if (_modified_nodes != nullptr) {\n+    _modified_nodes->remove_useless_nodes(useful.member_set());\n+  }\n+#endif\n@@ -632,0 +645,1 @@\n+                  _has_circular_inline_type(false),\n@@ -651,0 +665,1 @@\n+                  _inline_type_nodes (comp_arena(), 8, 0, nullptr),\n@@ -752,4 +767,2 @@\n-      const TypeTuple *domain = StartOSRNode::osr_domain();\n-      const TypeTuple *range = TypeTuple::make_range(method()->signature());\n-      init_tf(TypeFunc::make(domain, range));\n-      StartNode* s = new StartOSRNode(root(), domain);\n+      init_tf(TypeFunc::make(method(), \/* is_osr_compilation = *\/ true));\n+      StartNode* s = new StartOSRNode(root(), tf()->domain_sig());\n@@ -762,1 +775,1 @@\n-      StartNode* s = new StartNode(root(), tf()->domain());\n+      StartNode* s = new StartNode(root(), tf()->domain_cc());\n@@ -878,0 +891,10 @@\n+  if (needs_stack_repair()) {\n+    \/\/ One extra slot for the special stack increment value\n+    next_slot += 2;\n+  }\n+  \/\/ TODO 8284443 Only reserve extra slot if needed\n+  if (InlineTypeReturnedAsFields) {\n+    \/\/ One extra slot to hold the IsInit information for a nullable\n+    \/\/ inline type return if we run out of registers.\n+    next_slot += 2;\n+  }\n@@ -912,0 +935,1 @@\n+    _has_circular_inline_type(false),\n@@ -1057,0 +1081,4 @@\n+  _has_flat_accesses = false;\n+  _flat_accesses_share_alias = true;\n+  _scalarize_in_safepoints = false;\n+\n@@ -1329,1 +1357,2 @@\n-    assert(InlineUnsafeOps || StressReflectiveCode, \"indeterminate pointers come only from unsafe ops\");\n+    bool default_value_load = EnableValhalla && tj->is_instptr()->instance_klass() == ciEnv::current()->Class_klass();\n+    assert(InlineUnsafeOps || StressReflectiveCode || default_value_load, \"indeterminate pointers come only from unsafe ops\");\n@@ -1342,0 +1371,9 @@\n+  if (ta && ta->is_not_flat()) {\n+    \/\/ Erase not flat property for alias analysis.\n+    tj = ta = ta->cast_to_not_flat(false);\n+  }\n+  if (ta && ta->is_not_null_free()) {\n+    \/\/ Erase not null free property for alias analysis.\n+    tj = ta = ta->cast_to_not_null_free(false);\n+  }\n+\n@@ -1355,0 +1393,2 @@\n+    \/\/ For flat inline type array, each field has its own slice so\n+    \/\/ we must include the field offset.\n@@ -1395,1 +1435,1 @@\n-      tj = ta = TypeAryPtr::make(ptr,ta->const_oop(),tary,nullptr,false,offset);\n+      tj = ta = TypeAryPtr::make(ptr,ta->const_oop(),tary,nullptr,false,Type::Offset(offset), ta->field_offset());\n@@ -1399,1 +1439,6 @@\n-      tj = ta = TypeAryPtr::make(ptr,ta->const_oop(),tary,nullptr,false,offset);\n+      tj = ta = TypeAryPtr::make(ptr,ta->const_oop(),tary,nullptr,false,Type::Offset(offset), ta->field_offset());\n+    }\n+    \/\/ Initially all flattened array accesses share a single slice\n+    if (ta->is_flat() && ta->elem() != TypeInstPtr::BOTTOM && _flat_accesses_share_alias) {\n+      const TypeAry* tary = TypeAry::make(TypeInstPtr::BOTTOM, ta->size(), \/* stable= *\/ false, \/* flat= *\/ true);\n+      tj = ta = TypeAryPtr::make(ptr,ta->const_oop(),tary,nullptr,false,Type::Offset(offset), Type::Offset(Type::OffsetBot));\n@@ -1406,1 +1451,1 @@\n-      tj = ta = TypeAryPtr::make(ptr,ta->const_oop(),tary,aklass,false,offset);\n+      tj = ta = TypeAryPtr::make(ptr,ta->const_oop(),tary,aklass,false,Type::Offset(offset), ta->field_offset());\n@@ -1456,1 +1501,1 @@\n-        tj = to = TypeInstPtr::make(TypePtr::BotPTR, env()->Object_klass(), false, nullptr, offset);\n+        tj = to = TypeInstPtr::make(TypePtr::BotPTR, env()->Object_klass(), false, nullptr, Type::Offset(offset));\n@@ -1471,1 +1516,1 @@\n-          tj = to = TypeInstPtr::make(to->ptr(), canonical_holder, true, nullptr, offset, to->instance_id());\n+          tj = to = TypeInstPtr::make(to->ptr(), canonical_holder, true, nullptr, Type::Offset(offset), to->instance_id());\n@@ -1473,1 +1518,1 @@\n-          tj = to = TypeInstPtr::make(to->ptr(), canonical_holder, false, nullptr, offset);\n+          tj = to = TypeInstPtr::make(to->ptr(), canonical_holder, false, nullptr, Type::Offset(offset));\n@@ -1489,1 +1534,1 @@\n-                                       offset);\n+                                       Type::Offset(offset));\n@@ -1495,1 +1540,1 @@\n-        tj = tk = TypeInstKlassPtr::make(TypePtr::NotNull, env()->Object_klass(), offset);\n+        tj = tk = TypeInstKlassPtr::make(TypePtr::NotNull, env()->Object_klass(), Type::Offset(offset));\n@@ -1497,1 +1542,1 @@\n-        tj = tk = TypeAryKlassPtr::make(TypePtr::NotNull, tk->is_aryklassptr()->elem(), k, offset);\n+        tj = tk = TypeAryKlassPtr::make(TypePtr::NotNull, tk->is_aryklassptr()->elem(), k, Type::Offset(offset), tk->is_not_flat(), tk->is_not_null_free(), tk->is_null_free());\n@@ -1500,1 +1545,0 @@\n-\n@@ -1630,1 +1674,1 @@\n-Compile::AliasType* Compile::find_alias_type(const TypePtr* adr_type, bool no_create, ciField* original_field) {\n+Compile::AliasType* Compile::find_alias_type(const TypePtr* adr_type, bool no_create, ciField* original_field, bool uncached) {\n@@ -1635,3 +1679,6 @@\n-  AliasCacheEntry* ace = probe_alias_cache(adr_type);\n-  if (ace->_adr_type == adr_type) {\n-    return alias_type(ace->_index);\n+  AliasCacheEntry* ace = nullptr;\n+  if (!uncached) {\n+    ace = probe_alias_cache(adr_type);\n+    if (ace->_adr_type == adr_type) {\n+      return alias_type(ace->_index);\n+    }\n@@ -1687,0 +1734,1 @@\n+    ciField* field = nullptr;\n@@ -1693,0 +1741,1 @@\n+      const Type* elemtype = flat->is_aryptr()->elem();\n@@ -1694,1 +1743,8 @@\n-        alias_type(idx)->set_element(flat->is_aryptr()->elem());\n+        alias_type(idx)->set_element(elemtype);\n+      }\n+      int field_offset = flat->is_aryptr()->field_offset().get();\n+      if (flat->is_flat() &&\n+          field_offset != Type::OffsetBot) {\n+        ciInlineKlass* vk = elemtype->inline_klass();\n+        field_offset += vk->first_field_offset();\n+        field = vk->get_field_by_offset(field_offset, false);\n@@ -1708,0 +1764,2 @@\n+      if (flat->offset() == in_bytes(Klass::layout_helper_offset()))\n+        alias_type(idx)->set_rewritable(false);\n@@ -1718,1 +1776,0 @@\n-      ciField* field;\n@@ -1725,0 +1782,4 @@\n+      } else if (tinst->is_inlinetypeptr()) {\n+        \/\/ Inline type field\n+        ciInlineKlass* vk = tinst->inline_klass();\n+        field = vk->get_field_by_offset(tinst->offset(), false);\n@@ -1729,7 +1790,14 @@\n-      assert(field == nullptr ||\n-             original_field == nullptr ||\n-             (field->holder() == original_field->holder() &&\n-              field->offset_in_bytes() == original_field->offset_in_bytes() &&\n-              field->is_static() == original_field->is_static()), \"wrong field?\");\n-      \/\/ Set field() and is_rewritable() attributes.\n-      if (field != nullptr)  alias_type(idx)->set_field(field);\n+    }\n+    assert(field == nullptr ||\n+           original_field == nullptr ||\n+           (field->holder() == original_field->holder() &&\n+            field->offset_in_bytes() == original_field->offset_in_bytes() &&\n+            field->is_static() == original_field->is_static()), \"wrong field?\");\n+    \/\/ Set field() and is_rewritable() attributes.\n+    if (field != nullptr) {\n+      alias_type(idx)->set_field(field);\n+      if (flat->isa_aryptr()) {\n+        \/\/ Fields of flat arrays are rewritable although they are declared final\n+        assert(flat->is_flat(), \"must be a flat array\");\n+        alias_type(idx)->set_rewritable(true);\n+      }\n@@ -1740,3 +1808,4 @@\n-  ace->_adr_type = adr_type;\n-  ace->_index    = idx;\n-  assert(alias_type(adr_type) == alias_type(idx),  \"type must be installed\");\n+  if (!uncached) {\n+    ace->_adr_type = adr_type;\n+    ace->_index    = idx;\n+    assert(alias_type(adr_type) == alias_type(idx),  \"type must be installed\");\n@@ -1744,6 +1813,7 @@\n-  \/\/ Might as well try to fill the cache for the flattened version, too.\n-  AliasCacheEntry* face = probe_alias_cache(flat);\n-  if (face->_adr_type == nullptr) {\n-    face->_adr_type = flat;\n-    face->_index    = idx;\n-    assert(alias_type(flat) == alias_type(idx), \"flat type must work too\");\n+    \/\/ Might as well try to fill the cache for the flattened version, too.\n+    AliasCacheEntry* face = probe_alias_cache(flat);\n+    if (face->_adr_type == nullptr) {\n+      face->_adr_type = flat;\n+      face->_index    = idx;\n+      assert(alias_type(flat) == alias_type(idx), \"flat type must work too\");\n+    }\n@@ -1869,0 +1939,414 @@\n+void Compile::add_inline_type(Node* n) {\n+  assert(n->is_InlineType(), \"unexpected node\");\n+  _inline_type_nodes.push(n);\n+}\n+\n+void Compile::remove_inline_type(Node* n) {\n+  assert(n->is_InlineType(), \"unexpected node\");\n+  if (_inline_type_nodes.contains(n)) {\n+    _inline_type_nodes.remove(n);\n+  }\n+}\n+\n+\/\/ Does the return value keep otherwise useless inline type allocations alive?\n+static bool return_val_keeps_allocations_alive(Node* ret_val) {\n+  ResourceMark rm;\n+  Unique_Node_List wq;\n+  wq.push(ret_val);\n+  bool some_allocations = false;\n+  for (uint i = 0; i < wq.size(); i++) {\n+    Node* n = wq.at(i);\n+    if (n->outcnt() > 1) {\n+      \/\/ Some other use for the allocation\n+      return false;\n+    } else if (n->is_InlineType()) {\n+      wq.push(n->in(1));\n+    } else if (n->is_Phi()) {\n+      for (uint j = 1; j < n->req(); j++) {\n+        wq.push(n->in(j));\n+      }\n+    } else if (n->is_CheckCastPP() &&\n+               n->in(1)->is_Proj() &&\n+               n->in(1)->in(0)->is_Allocate()) {\n+      some_allocations = true;\n+    } else if (n->is_CheckCastPP()) {\n+      wq.push(n->in(1));\n+    }\n+  }\n+  return some_allocations;\n+}\n+\n+void Compile::process_inline_types(PhaseIterGVN &igvn, bool remove) {\n+  \/\/ Make sure that the return value does not keep an otherwise unused allocation alive\n+  if (tf()->returns_inline_type_as_fields()) {\n+    Node* ret = nullptr;\n+    for (uint i = 1; i < root()->req(); i++) {\n+      Node* in = root()->in(i);\n+      if (in->Opcode() == Op_Return) {\n+        assert(ret == nullptr, \"only one return\");\n+        ret = in;\n+      }\n+    }\n+    if (ret != nullptr) {\n+      Node* ret_val = ret->in(TypeFunc::Parms);\n+      if (igvn.type(ret_val)->isa_oopptr() &&\n+          return_val_keeps_allocations_alive(ret_val)) {\n+        igvn.replace_input_of(ret, TypeFunc::Parms, InlineTypeNode::tagged_klass(igvn.type(ret_val)->inline_klass(), igvn));\n+        assert(ret_val->outcnt() == 0, \"should be dead now\");\n+        igvn.remove_dead_node(ret_val);\n+      }\n+    }\n+  }\n+  if (_inline_type_nodes.length() == 0) {\n+    return;\n+  }\n+  \/\/ Scalarize inline types in safepoint debug info.\n+  \/\/ Delay this until all inlining is over to avoid getting inconsistent debug info.\n+  set_scalarize_in_safepoints(true);\n+  for (int i = _inline_type_nodes.length()-1; i >= 0; i--) {\n+    InlineTypeNode* vt = _inline_type_nodes.at(i)->as_InlineType();\n+    vt->make_scalar_in_safepoints(&igvn);\n+    igvn.record_for_igvn(vt);\n+  }\n+  if (remove) {\n+    \/\/ Remove inline type nodes by replacing them with their oop input\n+    while (_inline_type_nodes.length() > 0) {\n+      InlineTypeNode* vt = _inline_type_nodes.pop()->as_InlineType();\n+      if (vt->outcnt() == 0) {\n+        igvn.remove_dead_node(vt);\n+        continue;\n+      }\n+      for (DUIterator i = vt->outs(); vt->has_out(i); i++) {\n+        DEBUG_ONLY(bool must_be_buffered = false);\n+        Node* u = vt->out(i);\n+        \/\/ Check if any users are blackholes. If so, rewrite them to use either the\n+        \/\/ allocated buffer, or individual components, instead of the inline type node\n+        \/\/ that goes away.\n+        if (u->is_Blackhole()) {\n+          BlackholeNode* bh = u->as_Blackhole();\n+\n+          \/\/ Unlink the old input\n+          int idx = bh->find_edge(vt);\n+          assert(idx != -1, \"The edge should be there\");\n+          bh->del_req(idx);\n+          --i;\n+\n+          if (vt->is_allocated(&igvn)) {\n+            \/\/ Already has the allocated instance, blackhole that\n+            bh->add_req(vt->get_oop());\n+          } else {\n+            \/\/ Not allocated yet, blackhole the components\n+            for (uint c = 0; c < vt->field_count(); c++) {\n+              bh->add_req(vt->field_value(c));\n+            }\n+          }\n+\n+          \/\/ Node modified, record for IGVN\n+          igvn.record_for_igvn(bh);\n+        }\n+#ifdef ASSERT\n+        \/\/ Verify that inline type is buffered when replacing by oop\n+        else if (u->is_InlineType()) {\n+          \/\/ InlineType uses don't need buffering because they are about to be replaced as well\n+        } else if (u->is_Phi()) {\n+          \/\/ TODO 8302217 Remove this once InlineTypeNodes are reliably pushed through\n+        } else {\n+          must_be_buffered = true;\n+        }\n+        if (must_be_buffered && !vt->is_allocated(&igvn)) {\n+          vt->dump(0);\n+          u->dump(0);\n+          assert(false, \"Should have been buffered\");\n+        }\n+#endif\n+      }\n+      igvn.replace_node(vt, vt->get_oop());\n+    }\n+  }\n+  igvn.optimize();\n+}\n+\n+void Compile::adjust_flat_array_access_aliases(PhaseIterGVN& igvn) {\n+  if (!_has_flat_accesses) {\n+    return;\n+  }\n+  \/\/ Initially, all flat array accesses share the same slice to\n+  \/\/ keep dependencies with Object[] array accesses (that could be\n+  \/\/ to a flat array) correct. We're done with parsing so we\n+  \/\/ now know all flat array accesses in this compile\n+  \/\/ unit. Let's move flat array accesses to their own slice,\n+  \/\/ one per element field. This should help memory access\n+  \/\/ optimizations.\n+  ResourceMark rm;\n+  Unique_Node_List wq;\n+  wq.push(root());\n+\n+  Node_List mergememnodes;\n+  Node_List memnodes;\n+\n+  \/\/ Alias index currently shared by all flat memory accesses\n+  int index = get_alias_index(TypeAryPtr::INLINES);\n+\n+  \/\/ Find MergeMem nodes and flat array accesses\n+  for (uint i = 0; i < wq.size(); i++) {\n+    Node* n = wq.at(i);\n+    if (n->is_Mem()) {\n+      const TypePtr* adr_type = nullptr;\n+      if (n->Opcode() == Op_StoreCM) {\n+        adr_type = get_adr_type(get_alias_index(n->in(MemNode::OopStore)->adr_type()));\n+      } else {\n+        adr_type = get_adr_type(get_alias_index(n->adr_type()));\n+      }\n+      if (adr_type == TypeAryPtr::INLINES) {\n+        memnodes.push(n);\n+      }\n+    } else if (n->is_MergeMem()) {\n+      MergeMemNode* mm = n->as_MergeMem();\n+      if (mm->memory_at(index) != mm->base_memory()) {\n+        mergememnodes.push(n);\n+      }\n+    }\n+    for (uint j = 0; j < n->req(); j++) {\n+      Node* m = n->in(j);\n+      if (m != nullptr) {\n+        wq.push(m);\n+      }\n+    }\n+  }\n+\n+  if (memnodes.size() > 0) {\n+    _flat_accesses_share_alias = false;\n+\n+    \/\/ We are going to change the slice for the flat array\n+    \/\/ accesses so we need to clear the cache entries that refer to\n+    \/\/ them.\n+    for (uint i = 0; i < AliasCacheSize; i++) {\n+      AliasCacheEntry* ace = &_alias_cache[i];\n+      if (ace->_adr_type != nullptr &&\n+          ace->_adr_type->is_flat()) {\n+        ace->_adr_type = nullptr;\n+        ace->_index = (i != 0) ? 0 : AliasIdxTop; \/\/ Make sure the nullptr adr_type resolves to AliasIdxTop\n+      }\n+    }\n+\n+    \/\/ Find what aliases we are going to add\n+    int start_alias = num_alias_types()-1;\n+    int stop_alias = 0;\n+\n+    for (uint i = 0; i < memnodes.size(); i++) {\n+      Node* m = memnodes.at(i);\n+      const TypePtr* adr_type = nullptr;\n+      if (m->Opcode() == Op_StoreCM) {\n+        adr_type = m->in(MemNode::OopStore)->adr_type();\n+        if (adr_type != TypeAryPtr::INLINES) {\n+          \/\/ store was optimized out and we lost track of the adr_type\n+          Node* clone = new StoreCMNode(m->in(MemNode::Control), m->in(MemNode::Memory), m->in(MemNode::Address),\n+                                        m->adr_type(), m->in(MemNode::ValueIn), m->in(MemNode::OopStore),\n+                                        get_alias_index(adr_type));\n+          igvn.register_new_node_with_optimizer(clone);\n+          igvn.replace_node(m, clone);\n+        }\n+      } else {\n+        adr_type = m->adr_type();\n+#ifdef ASSERT\n+        m->as_Mem()->set_adr_type(adr_type);\n+#endif\n+      }\n+      int idx = get_alias_index(adr_type);\n+      start_alias = MIN2(start_alias, idx);\n+      stop_alias = MAX2(stop_alias, idx);\n+    }\n+\n+    assert(stop_alias >= start_alias, \"should have expanded aliases\");\n+\n+    Node_Stack stack(0);\n+#ifdef ASSERT\n+    VectorSet seen(Thread::current()->resource_area());\n+#endif\n+    \/\/ Now let's fix the memory graph so each flat array access\n+    \/\/ is moved to the right slice. Start from the MergeMem nodes.\n+    uint last = unique();\n+    for (uint i = 0; i < mergememnodes.size(); i++) {\n+      MergeMemNode* current = mergememnodes.at(i)->as_MergeMem();\n+      Node* n = current->memory_at(index);\n+      MergeMemNode* mm = nullptr;\n+      do {\n+        \/\/ Follow memory edges through memory accesses, phis and\n+        \/\/ narrow membars and push nodes on the stack. Once we hit\n+        \/\/ bottom memory, we pop element off the stack one at a\n+        \/\/ time, in reverse order, and move them to the right slice\n+        \/\/ by changing their memory edges.\n+        if ((n->is_Phi() && n->adr_type() != TypePtr::BOTTOM) || n->is_Mem() || n->adr_type() == TypeAryPtr::INLINES) {\n+          assert(!seen.test_set(n->_idx), \"\");\n+          \/\/ Uses (a load for instance) will need to be moved to the\n+          \/\/ right slice as well and will get a new memory state\n+          \/\/ that we don't know yet. The use could also be the\n+          \/\/ backedge of a loop. We put a place holder node between\n+          \/\/ the memory node and its uses. We replace that place\n+          \/\/ holder with the correct memory state once we know it,\n+          \/\/ i.e. when nodes are popped off the stack. Using the\n+          \/\/ place holder make the logic work in the presence of\n+          \/\/ loops.\n+          if (n->outcnt() > 1) {\n+            Node* place_holder = nullptr;\n+            assert(!n->has_out_with(Op_Node), \"\");\n+            for (DUIterator k = n->outs(); n->has_out(k); k++) {\n+              Node* u = n->out(k);\n+              if (u != current && u->_idx < last) {\n+                bool success = false;\n+                for (uint l = 0; l < u->req(); l++) {\n+                  if (!stack.is_empty() && u == stack.node() && l == stack.index()) {\n+                    continue;\n+                  }\n+                  Node* in = u->in(l);\n+                  if (in == n) {\n+                    if (place_holder == nullptr) {\n+                      place_holder = new Node(1);\n+                      place_holder->init_req(0, n);\n+                    }\n+                    igvn.replace_input_of(u, l, place_holder);\n+                    success = true;\n+                  }\n+                }\n+                if (success) {\n+                  --k;\n+                }\n+              }\n+            }\n+          }\n+          if (n->is_Phi()) {\n+            stack.push(n, 1);\n+            n = n->in(1);\n+          } else if (n->is_Mem()) {\n+            stack.push(n, n->req());\n+            n = n->in(MemNode::Memory);\n+          } else {\n+            assert(n->is_Proj() && n->in(0)->Opcode() == Op_MemBarCPUOrder, \"\");\n+            stack.push(n, n->req());\n+            n = n->in(0)->in(TypeFunc::Memory);\n+          }\n+        } else {\n+          assert(n->adr_type() == TypePtr::BOTTOM || (n->Opcode() == Op_Node && n->_idx >= last) || (n->is_Proj() && n->in(0)->is_Initialize()), \"\");\n+          \/\/ Build a new MergeMem node to carry the new memory state\n+          \/\/ as we build it. IGVN should fold extraneous MergeMem\n+          \/\/ nodes.\n+          mm = MergeMemNode::make(n);\n+          igvn.register_new_node_with_optimizer(mm);\n+          while (stack.size() > 0) {\n+            Node* m = stack.node();\n+            uint idx = stack.index();\n+            if (m->is_Mem()) {\n+              \/\/ Move memory node to its new slice\n+              const TypePtr* adr_type = m->adr_type();\n+              int alias = get_alias_index(adr_type);\n+              Node* prev = mm->memory_at(alias);\n+              igvn.replace_input_of(m, MemNode::Memory, prev);\n+              mm->set_memory_at(alias, m);\n+            } else if (m->is_Phi()) {\n+              \/\/ We need as many new phis as there are new aliases\n+              igvn.replace_input_of(m, idx, mm);\n+              if (idx == m->req()-1) {\n+                Node* r = m->in(0);\n+                for (uint j = (uint)start_alias; j <= (uint)stop_alias; j++) {\n+                  const TypePtr* adr_type = get_adr_type(j);\n+                  if (!adr_type->isa_aryptr() || !adr_type->is_flat() || j == (uint)index) {\n+                    continue;\n+                  }\n+                  Node* phi = new PhiNode(r, Type::MEMORY, get_adr_type(j));\n+                  igvn.register_new_node_with_optimizer(phi);\n+                  for (uint k = 1; k < m->req(); k++) {\n+                    phi->init_req(k, m->in(k)->as_MergeMem()->memory_at(j));\n+                  }\n+                  mm->set_memory_at(j, phi);\n+                }\n+                Node* base_phi = new PhiNode(r, Type::MEMORY, TypePtr::BOTTOM);\n+                igvn.register_new_node_with_optimizer(base_phi);\n+                for (uint k = 1; k < m->req(); k++) {\n+                  base_phi->init_req(k, m->in(k)->as_MergeMem()->base_memory());\n+                }\n+                mm->set_base_memory(base_phi);\n+              }\n+            } else {\n+              \/\/ This is a MemBarCPUOrder node from\n+              \/\/ Parse::array_load()\/Parse::array_store(), in the\n+              \/\/ branch that handles flat arrays hidden under\n+              \/\/ an Object[] array. We also need one new membar per\n+              \/\/ new alias to keep the unknown access that the\n+              \/\/ membars protect properly ordered with accesses to\n+              \/\/ known flat array.\n+              assert(m->is_Proj(), \"projection expected\");\n+              Node* ctrl = m->in(0)->in(TypeFunc::Control);\n+              igvn.replace_input_of(m->in(0), TypeFunc::Control, top());\n+              for (uint j = (uint)start_alias; j <= (uint)stop_alias; j++) {\n+                const TypePtr* adr_type = get_adr_type(j);\n+                if (!adr_type->isa_aryptr() || !adr_type->is_flat() || j == (uint)index) {\n+                  continue;\n+                }\n+                MemBarNode* mb = new MemBarCPUOrderNode(this, j, nullptr);\n+                igvn.register_new_node_with_optimizer(mb);\n+                Node* mem = mm->memory_at(j);\n+                mb->init_req(TypeFunc::Control, ctrl);\n+                mb->init_req(TypeFunc::Memory, mem);\n+                ctrl = new ProjNode(mb, TypeFunc::Control);\n+                igvn.register_new_node_with_optimizer(ctrl);\n+                mem = new ProjNode(mb, TypeFunc::Memory);\n+                igvn.register_new_node_with_optimizer(mem);\n+                mm->set_memory_at(j, mem);\n+              }\n+              igvn.replace_node(m->in(0)->as_Multi()->proj_out(TypeFunc::Control), ctrl);\n+            }\n+            if (idx < m->req()-1) {\n+              idx += 1;\n+              stack.set_index(idx);\n+              n = m->in(idx);\n+              break;\n+            }\n+            \/\/ Take care of place holder nodes\n+            if (m->has_out_with(Op_Node)) {\n+              Node* place_holder = m->find_out_with(Op_Node);\n+              if (place_holder != nullptr) {\n+                Node* mm_clone = mm->clone();\n+                igvn.register_new_node_with_optimizer(mm_clone);\n+                Node* hook = new Node(1);\n+                hook->init_req(0, mm);\n+                igvn.replace_node(place_holder, mm_clone);\n+                hook->destruct(&igvn);\n+              }\n+              assert(!m->has_out_with(Op_Node), \"place holder should be gone now\");\n+            }\n+            stack.pop();\n+          }\n+        }\n+      } while(stack.size() > 0);\n+      \/\/ Fix the memory state at the MergeMem we started from\n+      igvn.rehash_node_delayed(current);\n+      for (uint j = (uint)start_alias; j <= (uint)stop_alias; j++) {\n+        const TypePtr* adr_type = get_adr_type(j);\n+        if (!adr_type->isa_aryptr() || !adr_type->is_flat()) {\n+          continue;\n+        }\n+        current->set_memory_at(j, mm);\n+      }\n+      current->set_memory_at(index, current->base_memory());\n+    }\n+    igvn.optimize();\n+  }\n+  print_method(PHASE_SPLIT_INLINES_ARRAY, 2);\n+#ifdef ASSERT\n+  if (!_flat_accesses_share_alias) {\n+    wq.clear();\n+    wq.push(root());\n+    for (uint i = 0; i < wq.size(); i++) {\n+      Node* n = wq.at(i);\n+      assert(n->adr_type() != TypeAryPtr::INLINES, \"should have been removed from the graph\");\n+      for (uint j = 0; j < n->req(); j++) {\n+        Node* m = n->in(j);\n+        if (m != nullptr) {\n+          wq.push(m);\n+        }\n+      }\n+    }\n+  }\n+#endif\n+}\n+\n@@ -1944,1 +2428,1 @@\n-        if (!live_locals.at(i) && !local->is_top() && local != lhs && local!= rhs) {\n+        if (!live_locals.at(i) && !local->is_top() && local != lhs && local != rhs) {\n@@ -1959,1 +2443,1 @@\n-    \/\/ keep the mondified trap for late query\n+    \/\/ keep the modified trap for late query\n@@ -2154,1 +2638,4 @@\n-  assert(_modified_nodes == nullptr, \"not allowed\");\n+#ifdef ASSERT\n+  Unique_Node_List* modified_nodes = _modified_nodes;\n+  _modified_nodes = nullptr;\n+#endif\n@@ -2167,0 +2654,1 @@\n+  DEBUG_ONLY( _modified_nodes = modified_nodes; )\n@@ -2313,0 +2801,5 @@\n+  \/\/ Process inline type nodes now that all inlining is over\n+  process_inline_types(igvn);\n+\n+  adjust_flat_array_access_aliases(igvn);\n+\n@@ -2436,0 +2929,8 @@\n+  assert(_late_inlines.length() == 0 || IncrementalInlineMH || IncrementalInlineVirtual, \"not empty\");\n+\n+  if (_late_inlines.length() > 0) {\n+    \/\/ More opportunities to optimize virtual and MH calls.\n+    \/\/ Though it's maybe too late to perform inlining, strength-reducing them to direct calls is still an option.\n+    process_late_inline_calls_no_inline(igvn);\n+  }\n+\n@@ -2447,0 +2948,4 @@\n+  \/\/ Process inline type nodes again and remove them. From here\n+  \/\/ on we don't need to keep track of field values anymore.\n+  process_inline_types(igvn, \/* remove= *\/ true);\n+\n@@ -2463,0 +2968,1 @@\n+  DEBUG_ONLY( _late_inlines.clear(); )\n@@ -2465,9 +2971,0 @@\n-\n-  assert(_late_inlines.length() == 0 || IncrementalInlineMH || IncrementalInlineVirtual, \"not empty\");\n-\n-  if (_late_inlines.length() > 0) {\n-    \/\/ More opportunities to optimize virtual and MH calls.\n-    \/\/ Though it's maybe too late to perform inlining, strength-reducing them to direct calls is still an option.\n-    process_late_inline_calls_no_inline(igvn);\n-    if (failing())  return;\n-  }\n@@ -3101,0 +3598,1 @@\n+\n@@ -3252,1 +3750,16 @@\n-      n->add_prec(prec);\n+      if (prec->is_MergeMem()) {\n+        MergeMemNode* mm = prec->as_MergeMem();\n+        Node* base = mm->base_memory();\n+        for (int i = AliasIdxRaw + 1; i < num_alias_types(); i++) {\n+          const TypePtr* adr_type = get_adr_type(i);\n+          if (adr_type->is_flat()) {\n+            Node* m = mm->memory_at(i);\n+            n->add_prec(m);\n+          }\n+        }\n+        if (mm->outcnt() == 0) {\n+          mm->disconnect_inputs(this);\n+        }\n+      } else {\n+        n->add_prec(prec);\n+      }\n@@ -3873,0 +4386,7 @@\n+#ifdef ASSERT\n+  case Op_InlineType: {\n+    n->dump(-1);\n+    assert(false, \"inline type node was not removed\");\n+    break;\n+  }\n+#endif\n@@ -4252,2 +4772,2 @@\n-      if (accessing_method->is_static_initializer() ||\n-          accessing_method->is_object_initializer() ||\n+      if (accessing_method->is_class_initializer() ||\n+          accessing_method->is_object_constructor() ||\n@@ -4261,1 +4781,1 @@\n-      if (accessing_method->is_static_initializer()) {\n+      if (accessing_method->is_class_initializer()) {\n@@ -4317,0 +4837,1 @@\n+               (n->is_Allocate() && i >= AllocateNode::InlineType) ||\n@@ -4319,1 +4840,1 @@\n-              \"only region, phi, arraycopy, unlock or membar nodes have null data edges\");\n+              \"only region, phi, arraycopy, allocate, unlock or membar nodes have null data edges\");\n@@ -4449,0 +4970,7 @@\n+\n+    \/\/ Do not fold the subtype check to an array klass pointer comparison for null-able inline type arrays\n+    \/\/ because null-free [LMyValue <: null-able [LMyValue but the klasses are different. Perform a full test.\n+    if (!superk->is_aryklassptr()->is_null_free() && superk->is_aryklassptr()->elem()->isa_instklassptr() &&\n+        superk->is_aryklassptr()->elem()->is_instklassptr()->instance_klass()->is_inlinetype()) {\n+      return SSC_full_test;\n+    }\n@@ -5048,0 +5576,21 @@\n+Node* Compile::optimize_acmp(PhaseGVN* phase, Node* a, Node* b) {\n+  const TypeInstPtr* ta = phase->type(a)->isa_instptr();\n+  const TypeInstPtr* tb = phase->type(b)->isa_instptr();\n+  if (!EnableValhalla || ta == nullptr || tb == nullptr ||\n+      ta->is_zero_type() || tb->is_zero_type() ||\n+      !ta->can_be_inline_type() || !tb->can_be_inline_type()) {\n+    \/\/ Use old acmp if one operand is null or not an inline type\n+    return new CmpPNode(a, b);\n+  } else if (ta->is_inlinetypeptr() || tb->is_inlinetypeptr()) {\n+    \/\/ We know that one operand is an inline type. Therefore,\n+    \/\/ new acmp will only return true if both operands are nullptr.\n+    \/\/ Check if both operands are null by or'ing the oops.\n+    a = phase->transform(new CastP2XNode(nullptr, a));\n+    b = phase->transform(new CastP2XNode(nullptr, b));\n+    a = phase->transform(new OrXNode(a, b));\n+    return new CmpXNode(a, phase->MakeConX(0));\n+  }\n+  \/\/ Use new acmp\n+  return nullptr;\n+}\n+\n","filename":"src\/hotspot\/share\/opto\/compile.cpp","additions":604,"deletions":55,"binary":false,"changes":659,"status":"modified"},{"patch":"@@ -27,0 +27,1 @@\n+#include \"ci\/ciFlatArrayKlass.hpp\"\n@@ -327,0 +328,2 @@\n+  case vmIntrinsics::_makePrivateBuffer:        return inline_unsafe_make_private_buffer();\n+  case vmIntrinsics::_finishPrivateBuffer:      return inline_unsafe_finish_private_buffer();\n@@ -336,0 +339,1 @@\n+  case vmIntrinsics::_getValue:                 return inline_unsafe_access(!is_store, T_OBJECT,   Relaxed, false, true);\n@@ -346,0 +350,1 @@\n+  case vmIntrinsics::_putValue:                 return inline_unsafe_access( is_store, T_OBJECT,   Relaxed, false, true);\n@@ -513,0 +518,1 @@\n+  case vmIntrinsics::_isFlatArray:              return inline_unsafe_isFlatArray();\n@@ -525,0 +531,1 @@\n+  case vmIntrinsics::_newNullRestrictedArray:   return inline_newNullRestrictedArray();\n@@ -2245,0 +2252,1 @@\n+  bool null_free = false;\n@@ -2250,0 +2258,1 @@\n+      null_free = alias_type->field()->is_null_free();\n@@ -2258,0 +2267,1 @@\n+      null_free = adr_type->is_aryptr()->is_null_free();\n@@ -2270,0 +2280,3 @@\n+    if (null_free) {\n+      result = result->join_speculative(TypePtr::NOTNULL)->is_oopptr();\n+    }\n@@ -2300,1 +2313,1 @@\n-bool LibraryCallKit::inline_unsafe_access(bool is_store, const BasicType type, const AccessKind kind, const bool unaligned) {\n+bool LibraryCallKit::inline_unsafe_access(bool is_store, const BasicType type, const AccessKind kind, const bool unaligned, const bool is_flat) {\n@@ -2325,1 +2338,1 @@\n-      assert(sig->count() == 2, \"oop getter has 2 arguments\");\n+      assert(sig->count() == 2 || (is_flat && sig->count() == 3), \"oop getter has 2 or 3 arguments\");\n@@ -2331,1 +2344,1 @@\n-      assert(sig->count() == 3, \"oop putter has 3 arguments\");\n+      assert(sig->count() == 3 || (is_flat && sig->count() == 4), \"oop putter has 3 arguments\");\n@@ -2357,0 +2370,55 @@\n+\n+  ciInlineKlass* inline_klass = nullptr;\n+  if (is_flat) {\n+    const TypeInstPtr* cls = _gvn.type(argument(4))->isa_instptr();\n+    if (cls == nullptr || cls->const_oop() == nullptr) {\n+      return false;\n+    }\n+    ciType* mirror_type = cls->const_oop()->as_instance()->java_mirror_type();\n+    if (!mirror_type->is_inlinetype()) {\n+      return false;\n+    }\n+    inline_klass = mirror_type->as_inline_klass();\n+  }\n+\n+  if (base->is_InlineType()) {\n+    InlineTypeNode* vt = base->as_InlineType();\n+    if (is_store) {\n+      if (!vt->is_allocated(&_gvn)) {\n+        return false;\n+      }\n+      base = vt->get_oop();\n+    } else {\n+      if (offset->is_Con()) {\n+        long off = find_long_con(offset, 0);\n+        ciInlineKlass* vk = vt->type()->inline_klass();\n+        if ((long)(int)off != off || !vk->contains_field_offset(off)) {\n+          return false;\n+        }\n+\n+        ciField* field = vk->get_non_flat_field_by_offset(off);\n+        if (field != nullptr) {\n+          BasicType bt = type2field[field->type()->basic_type()];\n+          if (bt == T_ARRAY || bt == T_NARROWOOP) {\n+            bt = T_OBJECT;\n+          }\n+          if (bt == type && (!field->is_flat() || field->type() == inline_klass)) {\n+            Node* value = vt->field_value_by_offset(off, false);\n+            if (value->is_InlineType()) {\n+              value = value->as_InlineType()->adjust_scalarization_depth(this);\n+            }\n+            set_result(value);\n+            return true;\n+          }\n+        }\n+      }\n+      {\n+        \/\/ Re-execute the unsafe access if allocation triggers deoptimization.\n+        PreserveReexecuteState preexecs(this);\n+        jvms()->set_should_reexecute(true);\n+        vt = vt->buffer(this);\n+      }\n+      base = vt->get_oop();\n+    }\n+  }\n+\n@@ -2367,1 +2435,1 @@\n-    if (type != T_OBJECT) {\n+    if (type != T_OBJECT && (inline_klass == nullptr || !inline_klass->has_object_fields())) {\n@@ -2385,1 +2453,1 @@\n-  Node* val = is_store ? argument(4) : nullptr;\n+  Node* val = is_store ? argument(4 + (is_flat ? 1 : 0)) : nullptr;\n@@ -2406,1 +2474,22 @@\n-  BasicType bt = alias_type->basic_type();\n+  BasicType bt = T_ILLEGAL;\n+  ciField* field = nullptr;\n+  if (adr_type->isa_instptr()) {\n+    const TypeInstPtr* instptr = adr_type->is_instptr();\n+    ciInstanceKlass* k = instptr->instance_klass();\n+    int off = instptr->offset();\n+    if (instptr->const_oop() != nullptr &&\n+        k == ciEnv::current()->Class_klass() &&\n+        instptr->offset() >= (k->size_helper() * wordSize)) {\n+      k = instptr->const_oop()->as_instance()->java_lang_Class_klass()->as_instance_klass();\n+      field = k->get_field_by_offset(off, true);\n+    } else {\n+      field = k->get_non_flat_field_by_offset(off);\n+    }\n+    if (field != nullptr) {\n+      bt = type2field[field->type()->basic_type()];\n+    }\n+    assert(bt == alias_type->basic_type() || is_flat, \"should match\");\n+  } else {\n+    bt = alias_type->basic_type();\n+  }\n+\n@@ -2429,0 +2518,23 @@\n+  if (is_flat) {\n+    if (adr_type->isa_instptr()) {\n+      if (field == nullptr || field->type() != inline_klass) {\n+        mismatched = true;\n+      }\n+    } else if (adr_type->isa_aryptr()) {\n+      const Type* elem = adr_type->is_aryptr()->elem();\n+      if (!adr_type->is_flat() || elem->inline_klass() != inline_klass) {\n+        mismatched = true;\n+      }\n+    } else {\n+      mismatched = true;\n+    }\n+    if (is_store) {\n+      const Type* val_t = _gvn.type(val);\n+      if (!val_t->is_inlinetypeptr() || val_t->inline_klass() != inline_klass) {\n+        set_map(old_map);\n+        set_sp(old_sp);\n+        return false;\n+      }\n+    }\n+  }\n+\n@@ -2430,1 +2542,1 @@\n-  assert(!mismatched || alias_type->adr_type()->is_oopptr(), \"off-heap access can't be mismatched\");\n+  assert(!mismatched || is_flat || alias_type->adr_type()->is_oopptr(), \"off-heap access can't be mismatched\");\n@@ -2442,4 +2554,6 @@\n-  if (!is_store && type == T_OBJECT) {\n-    const TypeOopPtr* tjp = sharpen_unsafe_type(alias_type, adr_type);\n-    if (tjp != nullptr) {\n-      value_type = tjp;\n+  if (!is_store) {\n+    if (type == T_OBJECT && !is_flat) {\n+      const TypeOopPtr* tjp = sharpen_unsafe_type(alias_type, adr_type);\n+      if (tjp != nullptr) {\n+        value_type = tjp;\n+      }\n@@ -2461,2 +2575,2 @@\n-    ciField* field = alias_type->field();\n-    if (heap_base_oop != top() && field != nullptr && field->is_constant() && !mismatched) {\n+\n+    if (heap_base_oop != top() && field != nullptr && field->is_constant() && !field->is_flat() && !mismatched) {\n@@ -2468,1 +2582,16 @@\n-      p = access_load_at(heap_base_oop, adr, adr_type, value_type, type, decorators);\n+      if (is_flat) {\n+        if (adr_type->isa_instptr() && !mismatched) {\n+          ciInstanceKlass* holder = adr_type->is_instptr()->instance_klass();\n+          int offset = adr_type->is_instptr()->offset();\n+          p = InlineTypeNode::make_from_flat(this, inline_klass, base, base, holder, offset, decorators);\n+        } else {\n+          p = InlineTypeNode::make_from_flat(this, inline_klass, base, adr, nullptr, 0, decorators);\n+        }\n+      } else {\n+        p = access_load_at(heap_base_oop, adr, adr_type, value_type, type, decorators);\n+        const TypeOopPtr* ptr = value_type->make_oopptr();\n+        if (ptr != nullptr && ptr->is_inlinetypeptr()) {\n+          \/\/ Load a non-flattened inline type from memory\n+          p = InlineTypeNode::make_from_oop(this, p, ptr->inline_klass(), !ptr->maybe_null());\n+        }\n+      }\n@@ -2506,1 +2635,17 @@\n-    access_store_at(heap_base_oop, adr, adr_type, val, value_type, type, decorators);\n+    if (is_flat) {\n+      if (adr_type->isa_instptr() && !mismatched) {\n+        ciInstanceKlass* holder = adr_type->is_instptr()->instance_klass();\n+        int offset = adr_type->is_instptr()->offset();\n+        val->as_InlineType()->store_flat(this, base, base, holder, offset, decorators);\n+      } else {\n+        val->as_InlineType()->store_flat(this, base, adr, nullptr, 0, decorators);\n+      }\n+    } else {\n+      access_store_at(heap_base_oop, adr, adr_type, val, value_type, type, decorators);\n+    }\n+  }\n+\n+  if (argument(1)->is_InlineType() && is_store) {\n+    InlineTypeNode* value = InlineTypeNode::make_from_oop(this, base, _gvn.type(argument(1))->inline_klass());\n+    value = value->make_larval(this, false);\n+    replace_in_map(argument(1), value);\n@@ -2512,0 +2657,40 @@\n+bool LibraryCallKit::inline_unsafe_make_private_buffer() {\n+  Node* receiver = argument(0);\n+  Node* value = argument(1);\n+  if (!value->is_InlineType()) {\n+    return false;\n+  }\n+\n+  receiver = null_check(receiver);\n+  if (stopped()) {\n+    return true;\n+  }\n+\n+  set_result(value->as_InlineType()->make_larval(this, true));\n+  return true;\n+}\n+\n+bool LibraryCallKit::inline_unsafe_finish_private_buffer() {\n+  Node* receiver = argument(0);\n+  Node* buffer = argument(1);\n+  if (!buffer->is_InlineType()) {\n+    return false;\n+  }\n+  InlineTypeNode* vt = buffer->as_InlineType();\n+  if (!vt->is_allocated(&_gvn)) {\n+    return false;\n+  }\n+  \/\/ TODO 8239003 Why is this needed?\n+  if (AllocateNode::Ideal_allocation(vt->get_oop()) == nullptr) {\n+    return false;\n+  }\n+\n+  receiver = null_check(receiver);\n+  if (stopped()) {\n+    return true;\n+  }\n+\n+  set_result(vt->finish_larval(this));\n+  return true;\n+}\n+\n@@ -2717,0 +2902,13 @@\n+    if (oldval != nullptr && oldval->is_InlineType()) {\n+      \/\/ Re-execute the unsafe access if allocation triggers deoptimization.\n+      PreserveReexecuteState preexecs(this);\n+      jvms()->set_should_reexecute(true);\n+      oldval = oldval->as_InlineType()->buffer(this)->get_oop();\n+    }\n+    if (newval != nullptr && newval->is_InlineType()) {\n+      \/\/ Re-execute the unsafe access if allocation triggers deoptimization.\n+      PreserveReexecuteState preexecs(this);\n+      jvms()->set_should_reexecute(true);\n+      newval = newval->as_InlineType()->buffer(this)->get_oop();\n+    }\n+\n@@ -2903,2 +3101,7 @@\n-\n-  Node* obj = new_instance(kls, test);\n+  Node* obj = nullptr;\n+  const TypeInstKlassPtr* tkls = _gvn.type(kls)->isa_instklassptr();\n+  if (tkls != nullptr && tkls->instance_klass()->is_inlinetype()) {\n+    obj = InlineTypeNode::make_default(_gvn, tkls->instance_klass()->as_inline_klass())->buffer(this);\n+  } else {\n+    obj = new_instance(kls, test);\n+  }\n@@ -3698,1 +3901,1 @@\n-  const TypeAry* arr0 = TypeAry::make(etype, TypeInt::POS);\n+  const TypeAry* arr0 = TypeAry::make(etype, TypeInt::POS, \/* stable= *\/ false, \/* flat= *\/ false, \/* not_flat= *\/ true, \/* not_null_free= *\/ true);\n@@ -3703,1 +3906,1 @@\n-  const Type* objects_type = TypeAryPtr::make(TypePtr::BotPTR, arr0, objects_klass, xk, 0);\n+  const Type* objects_type = TypeAryPtr::make(TypePtr::BotPTR, arr0, objects_klass, xk, TypeAryPtr::Offset(0));\n@@ -3826,9 +4029,0 @@\n-\/\/---------------------------load_mirror_from_klass----------------------------\n-\/\/ Given a klass oop, load its java mirror (a java.lang.Class oop).\n-Node* LibraryCallKit::load_mirror_from_klass(Node* klass) {\n-  Node* p = basic_plus_adr(klass, in_bytes(Klass::java_mirror_offset()));\n-  Node* load = make_load(nullptr, p, TypeRawPtr::NOTNULL, T_ADDRESS, MemNode::unordered);\n-  \/\/ mirror = ((OopHandle)mirror)->resolve();\n-  return access_load(load, TypeInstPtr::MIRROR, T_OBJECT, IN_NATIVE);\n-}\n-\n@@ -3878,0 +4072,1 @@\n+\n@@ -4079,0 +4274,1 @@\n+\n@@ -4094,1 +4290,2 @@\n-  ciType* tm = mirror_con->java_mirror_type();\n+  bool is_null_free_array = false;\n+  ciType* tm = mirror_con->java_mirror_type(&is_null_free_array);\n@@ -4101,1 +4298,5 @@\n-      int static_res = C->static_subtype_check(TypeKlassPtr::make(tm->as_klass(), Type::trust_interfaces), tp->as_klass_type());\n+      const TypeKlassPtr* tklass = TypeKlassPtr::make(tm->as_klass(), Type::trust_interfaces);\n+      if (is_null_free_array) {\n+        tklass = tklass->is_aryklassptr()->cast_to_null_free();\n+      }\n+      int static_res = C->static_subtype_check(tklass, tp->as_klass_type());\n@@ -4130,2 +4331,2 @@\n-  \/\/ Not-subtype or the mirror's klass ptr is null (in case it is a primitive).\n-  enum { _bad_type_path = 1, _prim_path = 2, PATH_LIMIT };\n+  \/\/ Not-subtype or the mirror's klass ptr is nullptr (in case it is a primitive).\n+  enum { _bad_type_path = 1, _prim_path = 2, _npe_path = 3, PATH_LIMIT };\n@@ -4141,0 +4342,2 @@\n+  Node* io = i_o();\n+  Node* mem = merged_memory();\n@@ -4142,0 +4345,1 @@\n+\n@@ -4148,1 +4352,2 @@\n-      region->in(_bad_type_path) != top()) {\n+      region->in(_bad_type_path) != top() ||\n+      region->in(_npe_path) != top()) {\n@@ -4152,0 +4357,3 @@\n+    \/\/ Set IO and memory because gen_checkcast may override them when buffering inline types\n+    set_i_o(io);\n+    set_all_memory(mem);\n@@ -4185,0 +4393,1 @@\n+  RegionNode* prim_region = new RegionNode(2);\n@@ -4187,0 +4396,1 @@\n+  record_for_igvn(prim_region);\n@@ -4211,2 +4421,5 @@\n-    int prim_path = (which_arg == 0 ? _prim_0_path : _prim_1_path);\n-    region->init_req(prim_path, null_ctl);\n+    if (which_arg == 0) {\n+      prim_region->init_req(1, null_ctl);\n+    } else {\n+      region->init_req(_prim_1_path, null_ctl);\n+    }\n@@ -4222,1 +4435,0 @@\n-    \/\/ now we have a successful reference subtype check\n@@ -4229,1 +4441,2 @@\n-  set_control(region->in(_prim_0_path)); \/\/ go back to first null check\n+  \/\/ This path is also used if superc is a value mirror.\n+  set_control(_gvn.transform(prim_region));\n@@ -4234,1 +4447,1 @@\n-    generate_guard(bol_eq, region, PROB_FAIR);\n+    generate_fair_guard(bol_eq, region);\n@@ -4265,2 +4478,1 @@\n-Node* LibraryCallKit::generate_array_guard_common(Node* kls, RegionNode* region,\n-                                                  bool obj_array, bool not_array) {\n+Node* LibraryCallKit::generate_array_guard_common(Node* kls, RegionNode* region, ArrayKind kind) {\n@@ -4272,9 +4484,0 @@\n-  \/\/ If obj_array\/non_array==false\/false:\n-  \/\/ Branch around if the given klass is in fact an array (either obj or prim).\n-  \/\/ If obj_array\/non_array==false\/true:\n-  \/\/ Branch around if the given klass is not an array klass of any kind.\n-  \/\/ If obj_array\/non_array==true\/true:\n-  \/\/ Branch around if the kls is not an oop array (kls is int[], String, etc.)\n-  \/\/ If obj_array\/non_array==true\/false:\n-  \/\/ Branch around if the kls is an oop array (Object[] or subtype)\n-  \/\/\n@@ -4285,4 +4488,11 @@\n-    bool query = (obj_array\n-                  ? Klass::layout_helper_is_objArray(layout_con)\n-                  : Klass::layout_helper_is_array(layout_con));\n-    if (query == not_array) {\n+    bool query = 0;\n+    switch(kind) {\n+      case ObjectArray:    query = Klass::layout_helper_is_objArray(layout_con); break;\n+      case NonObjectArray: query = !Klass::layout_helper_is_objArray(layout_con); break;\n+      case TypeArray:      query = Klass::layout_helper_is_typeArray(layout_con); break;\n+      case AnyArray:       query = Klass::layout_helper_is_array(layout_con); break;\n+      case NonArray:       query = !Klass::layout_helper_is_array(layout_con); break;\n+      default:\n+        ShouldNotReachHere();\n+    }\n+    if (!query) {\n@@ -4298,0 +4508,21 @@\n+  unsigned int value = 0;\n+  BoolTest::mask btest = BoolTest::illegal;\n+  switch(kind) {\n+    case ObjectArray:\n+    case NonObjectArray: {\n+      value = Klass::_lh_array_tag_obj_value;\n+      layout_val = _gvn.transform(new RShiftINode(layout_val, intcon(Klass::_lh_array_tag_shift)));\n+      btest = (kind == ObjectArray) ? BoolTest::eq : BoolTest::ne;\n+      break;\n+    }\n+    case TypeArray: {\n+      value = Klass::_lh_array_tag_type_value;\n+      layout_val = _gvn.transform(new RShiftINode(layout_val, intcon(Klass::_lh_array_tag_shift)));\n+      btest = BoolTest::eq;\n+      break;\n+    }\n+    case AnyArray:    value = Klass::_lh_neutral_value; btest = BoolTest::lt; break;\n+    case NonArray:    value = Klass::_lh_neutral_value; btest = BoolTest::gt; break;\n+    default:\n+      ShouldNotReachHere();\n+  }\n@@ -4299,4 +4530,1 @@\n-  jint  nval = (obj_array\n-                ? (jint)(Klass::_lh_array_tag_type_value\n-                   <<    Klass::_lh_array_tag_shift)\n-                : Klass::_lh_neutral_value);\n+  jint nval = (jint)value;\n@@ -4304,3 +4532,0 @@\n-  BoolTest::mask btest = BoolTest::lt;  \/\/ correct for testing is_[obj]array\n-  \/\/ invert the test if we are looking for a non-array\n-  if (not_array)  btest = BoolTest(btest).negate();\n@@ -4311,0 +4536,26 @@\n+\/\/-----------------------inline_newNullRestrictedArray--------------------------\n+\/\/ public static native Object[] newNullRestrictedArray(Class<?> componentType, int length);\n+bool LibraryCallKit::inline_newNullRestrictedArray() {\n+  Node* componentType = argument(0);\n+  Node* length = argument(1);\n+\n+  const TypeInstPtr* tp = _gvn.type(componentType)->isa_instptr();\n+  if (tp != nullptr) {\n+    ciInstanceKlass* ik = tp->instance_klass();\n+    if (ik == C->env()->Class_klass()) {\n+      ciType* t = tp->java_mirror_type();\n+      if (t != nullptr && t->is_inlinetype()) {\n+        ciArrayKlass* array_klass = ciArrayKlass::make(t, true);\n+        if (array_klass->is_loaded() && array_klass->element_klass()->as_inline_klass()->is_initialized()) {\n+          const TypeAryKlassPtr* array_klass_type = TypeKlassPtr::make(array_klass, Type::trust_interfaces)->is_aryklassptr();\n+          array_klass_type = array_klass_type->cast_to_null_free();\n+          Node* obj = new_array(makecon(array_klass_type), length, 0, nullptr, false);  \/\/ no arguments to push\n+          set_result(obj);\n+          assert(gvn().type(obj)->is_aryptr()->is_null_free(), \"must be null-free\");\n+          return true;\n+        }\n+      }\n+    }\n+  }\n+  return false;\n+}\n@@ -4313,1 +4564,1 @@\n-\/\/ private static native Object java.lang.reflect.newArray(Class<?> componentType, int length);\n+\/\/ private static native Object java.lang.reflect.Array.newArray(Class<?> componentType, int length);\n@@ -4459,1 +4710,13 @@\n-    Node* not_objArray = generate_non_objArray_guard(klass_node, bailout);\n+    \/\/ Inline type array may have object field that would require a\n+    \/\/ write barrier. Conservatively, go to slow path.\n+    \/\/ TODO 8251971: Optimize for the case when flat src\/dst are later found\n+    \/\/ to not contain oops (i.e., move this check to the macro expansion phase).\n+    BarrierSetC2* bs = BarrierSet::barrier_set()->barrier_set_c2();\n+    const TypeAryPtr* orig_t = _gvn.type(original)->isa_aryptr();\n+    const TypeKlassPtr* tklass = _gvn.type(klass_node)->is_klassptr();\n+    bool exclude_flat = UseFlatArray && bs->array_copy_requires_gc_barriers(true, T_OBJECT, false, false, BarrierSetC2::Parsing) &&\n+                        \/\/ Can src array be flat and contain oops?\n+                        (orig_t == nullptr || (!orig_t->is_not_flat() && (!orig_t->is_flat() || orig_t->elem()->inline_klass()->contains_oops()))) &&\n+                        \/\/ Can dest array be flat and contain oops?\n+                        tklass->can_be_inline_array() && (!tklass->is_flat() || tklass->is_aryklassptr()->elem()->is_instklassptr()->instance_klass()->as_inline_klass()->contains_oops());\n+    Node* not_objArray = exclude_flat ? generate_non_objArray_guard(klass_node, bailout) : generate_typeArray_guard(klass_node, bailout);\n@@ -4463,1 +4726,1 @@\n-      const Type* akls = TypeKlassPtr::make(TypePtr::NotNull, ak, 0\/*offset*\/);\n+      const Type* akls = TypeKlassPtr::make(TypePtr::NotNull, ak, Type::Offset(0));\n@@ -4483,0 +4746,38 @@\n+    \/\/ Handle inline type arrays\n+    bool can_validate = !too_many_traps(Deoptimization::Reason_class_check);\n+    if (!stopped()) {\n+      \/\/ TODO JDK-8329224\n+      if (!orig_t->is_null_free()) {\n+        \/\/ Not statically known to be null free, add a check\n+        generate_fair_guard(null_free_array_test(original), bailout);\n+      }\n+      orig_t = _gvn.type(original)->isa_aryptr();\n+      if (orig_t != nullptr && orig_t->is_flat()) {\n+        \/\/ Src is flat, check that dest is flat as well\n+        if (exclude_flat) {\n+          \/\/ Dest can't be flat, bail out\n+          bailout->add_req(control());\n+          set_control(top());\n+        } else {\n+          generate_fair_guard(flat_array_test(klass_node, \/* flat = *\/ false), bailout);\n+        }\n+      } else if (UseFlatArray && (orig_t == nullptr || !orig_t->is_not_flat()) &&\n+                 \/\/ If dest is flat, src must be flat as well (guaranteed by src <: dest check if validated).\n+                 ((!tklass->is_flat() && tklass->can_be_inline_array()) || !can_validate)) {\n+        \/\/ Src might be flat and dest might not be flat. Go to the slow path if src is flat.\n+        \/\/ TODO 8251971: Optimize for the case when src\/dest are later found to be both flat.\n+        generate_fair_guard(flat_array_test(load_object_klass(original)), bailout);\n+        if (orig_t != nullptr) {\n+          orig_t = orig_t->cast_to_not_flat();\n+          original = _gvn.transform(new CheckCastPPNode(control(), original, orig_t));\n+        }\n+      }\n+      if (!can_validate) {\n+        \/\/ No validation. The subtype check emitted at macro expansion time will not go to the slow\n+        \/\/ path but call checkcast_arraycopy which can not handle flat\/null-free inline type arrays.\n+        \/\/ TODO 8251971: Optimize for the case when src\/dest are later found to be both flat\/null-free.\n+        generate_fair_guard(flat_array_test(klass_node), bailout);\n+        generate_fair_guard(null_free_array_test(original), bailout);\n+      }\n+    }\n+\n@@ -4528,1 +4829,1 @@\n-      if (!too_many_traps(Deoptimization::Reason_class_check)) {\n+      if (can_validate) {\n@@ -4614,1 +4915,1 @@\n-    const TypeTuple* range = tf->range();\n+    const TypeTuple* range = tf->range_cc();\n@@ -4618,1 +4919,1 @@\n-    tf = TypeFunc::make(tf->domain(), new_range);\n+    tf = TypeFunc::make(tf->domain_cc(), new_range);\n@@ -4675,1 +4976,8 @@\n-  Node* obj = nullptr;\n+  Node* obj = argument(0);\n+\n+  \/\/ Don't intrinsify hashcode on inline types for now.\n+  \/\/ The \"is locked\" runtime check below also serves as inline type check and goes to the slow path.\n+  if (gvn().type(obj)->is_inlinetypeptr()) {\n+    return false;\n+  }\n+\n@@ -4685,1 +4993,0 @@\n-    obj = argument(0);\n@@ -4726,1 +5033,2 @@\n-    Node *lock_mask      = _gvn.MakeConX(markWord::lock_mask_in_place);\n+  \/\/ This also serves as guard against inline types\n+    Node *lock_mask      = _gvn.MakeConX(markWord::inline_type_mask_in_place);\n@@ -4801,1 +5109,10 @@\n-  Node* obj = null_check_receiver();\n+  Node* obj = argument(0);\n+  if (obj->is_InlineType()) {\n+    const Type* t = _gvn.type(obj);\n+    if (t->maybe_null()) {\n+      null_check(obj);\n+    }\n+    set_result(makecon(TypeInstPtr::make(t->inline_klass()->java_mirror())));\n+    return true;\n+  }\n+  obj = null_check_receiver();\n@@ -5153,0 +5470,14 @@\n+\/\/----------------------inline_unsafe_isFlatArray------------------------\n+\/\/ public native boolean Unsafe.isFlatArray(Class<?> arrayClass);\n+\/\/ This intrinsic exploits assumptions made by the native implementation\n+\/\/ (arrayClass is neither null nor primitive) to avoid unnecessary null checks.\n+bool LibraryCallKit::inline_unsafe_isFlatArray() {\n+  Node* cls = argument(1);\n+  Node* p = basic_plus_adr(cls, java_lang_Class::klass_offset());\n+  Node* kls = _gvn.transform(LoadKlassNode::make(_gvn, nullptr, immutable_memory(), p,\n+                                                 TypeRawPtr::BOTTOM, TypeInstKlassPtr::OBJECT));\n+  Node* result = flat_array_test(kls);\n+  set_result(result);\n+  return true;\n+}\n+\n@@ -5223,1 +5554,2 @@\n-    Node* obj = null_check_receiver();\n+    Node* obj = argument(0);\n+    obj = null_check_receiver();\n@@ -5227,0 +5559,6 @@\n+    if (obj_type->is_inlinetypeptr()) {\n+      \/\/ If the object to clone is an inline type, we can simply return it (i.e. a nop) since inline types have\n+      \/\/ no identity.\n+      set_result(obj);\n+      return true;\n+    }\n@@ -5233,1 +5571,2 @@\n-        obj_type->speculative_type()->is_instance_klass()) {\n+        obj_type->speculative_type()->is_instance_klass() &&\n+        !obj_type->speculative_type()->is_inlinetype()) {\n@@ -5263,0 +5602,5 @@\n+    \/\/ We only go to the fast case code if we pass a number of guards.\n+    \/\/ The paths which do not pass are accumulated in the slow_region.\n+    RegionNode* slow_region = new RegionNode(1);\n+    record_for_igvn(slow_region);\n+\n@@ -5268,3 +5612,0 @@\n-      Node* obj_length = load_array_length(obj);\n-      Node* array_size = nullptr; \/\/ Size of the array without object alignment padding.\n-      Node* alloc_obj = new_array(obj_klass, obj_length, 0, &array_size, \/*deoptimize_on_exception=*\/true);\n@@ -5273,20 +5614,7 @@\n-      if (bs->array_copy_requires_gc_barriers(true, T_OBJECT, true, false, BarrierSetC2::Parsing)) {\n-        \/\/ If it is an oop array, it requires very special treatment,\n-        \/\/ because gc barriers are required when accessing the array.\n-        Node* is_obja = generate_objArray_guard(obj_klass, (RegionNode*)nullptr);\n-        if (is_obja != nullptr) {\n-          PreserveJVMState pjvms2(this);\n-          set_control(is_obja);\n-          \/\/ Generate a direct call to the right arraycopy function(s).\n-          \/\/ Clones are always tightly coupled.\n-          ArrayCopyNode* ac = ArrayCopyNode::make(this, true, obj, intcon(0), alloc_obj, intcon(0), obj_length, true, false);\n-          ac->set_clone_oop_array();\n-          Node* n = _gvn.transform(ac);\n-          assert(n == ac, \"cannot disappear\");\n-          ac->connect_outputs(this, \/*deoptimize_on_exception=*\/true);\n-\n-          result_reg->init_req(_objArray_path, control());\n-          result_val->init_req(_objArray_path, alloc_obj);\n-          result_i_o ->set_req(_objArray_path, i_o());\n-          result_mem ->set_req(_objArray_path, reset_memory());\n-        }\n+      const TypeAryPtr* ary_ptr = obj_type->isa_aryptr();\n+      if (UseFlatArray && bs->array_copy_requires_gc_barriers(true, T_OBJECT, true, false, BarrierSetC2::Expansion) &&\n+          obj_type->can_be_inline_array() &&\n+          (ary_ptr == nullptr || (!ary_ptr->is_not_flat() && (!ary_ptr->is_flat() || ary_ptr->elem()->inline_klass()->contains_oops())))) {\n+        \/\/ Flat inline type array may have object field that would require a\n+        \/\/ write barrier. Conservatively, go to slow path.\n+        generate_fair_guard(flat_array_test(obj_klass), slow_region);\n@@ -5294,7 +5622,0 @@\n-      \/\/ Otherwise, there are no barriers to worry about.\n-      \/\/ (We can dispense with card marks if we know the allocation\n-      \/\/  comes out of eden (TLAB)...  In fact, ReduceInitialCardMarks\n-      \/\/  causes the non-eden paths to take compensating steps to\n-      \/\/  simulate a fresh allocation, so that no further\n-      \/\/  card marks are required in compiled code to initialize\n-      \/\/  the object.)\n@@ -5303,7 +5624,43 @@\n-        copy_to_clone(obj, alloc_obj, array_size, true);\n-\n-        \/\/ Present the results of the copy.\n-        result_reg->init_req(_array_path, control());\n-        result_val->init_req(_array_path, alloc_obj);\n-        result_i_o ->set_req(_array_path, i_o());\n-        result_mem ->set_req(_array_path, reset_memory());\n+        Node* obj_length = load_array_length(obj);\n+        Node* array_size = nullptr; \/\/ Size of the array without object alignment padding.\n+        Node* alloc_obj = new_array(obj_klass, obj_length, 0, &array_size, \/*deoptimize_on_exception=*\/true);\n+\n+        BarrierSetC2* bs = BarrierSet::barrier_set()->barrier_set_c2();\n+        if (bs->array_copy_requires_gc_barriers(true, T_OBJECT, true, false, BarrierSetC2::Parsing)) {\n+          \/\/ If it is an oop array, it requires very special treatment,\n+          \/\/ because gc barriers are required when accessing the array.\n+          Node* is_obja = generate_objArray_guard(obj_klass, (RegionNode*)nullptr);\n+          if (is_obja != nullptr) {\n+            PreserveJVMState pjvms2(this);\n+            set_control(is_obja);\n+            \/\/ Generate a direct call to the right arraycopy function(s).\n+            \/\/ Clones are always tightly coupled.\n+            ArrayCopyNode* ac = ArrayCopyNode::make(this, true, obj, intcon(0), alloc_obj, intcon(0), obj_length, true, false);\n+            ac->set_clone_oop_array();\n+            Node* n = _gvn.transform(ac);\n+            assert(n == ac, \"cannot disappear\");\n+            ac->connect_outputs(this, \/*deoptimize_on_exception=*\/true);\n+\n+            result_reg->init_req(_objArray_path, control());\n+            result_val->init_req(_objArray_path, alloc_obj);\n+            result_i_o ->set_req(_objArray_path, i_o());\n+            result_mem ->set_req(_objArray_path, reset_memory());\n+          }\n+        }\n+        \/\/ Otherwise, there are no barriers to worry about.\n+        \/\/ (We can dispense with card marks if we know the allocation\n+        \/\/  comes out of eden (TLAB)...  In fact, ReduceInitialCardMarks\n+        \/\/  causes the non-eden paths to take compensating steps to\n+        \/\/  simulate a fresh allocation, so that no further\n+        \/\/  card marks are required in compiled code to initialize\n+        \/\/  the object.)\n+\n+        if (!stopped()) {\n+          copy_to_clone(obj, alloc_obj, array_size, true);\n+\n+          \/\/ Present the results of the copy.\n+          result_reg->init_req(_array_path, control());\n+          result_val->init_req(_array_path, alloc_obj);\n+          result_i_o ->set_req(_array_path, i_o());\n+          result_mem ->set_req(_array_path, reset_memory());\n+        }\n@@ -5313,4 +5670,0 @@\n-    \/\/ We only go to the instance fast case code if we pass a number of guards.\n-    \/\/ The paths which do not pass are accumulated in the slow_region.\n-    RegionNode* slow_region = new RegionNode(1);\n-    record_for_igvn(slow_region);\n@@ -5448,0 +5801,12 @@\n+  int adjustment = 1;\n+  const TypeAryKlassPtr* ary_klass_ptr = alloc->in(AllocateNode::KlassNode)->bottom_type()->is_aryklassptr();\n+  if (ary_klass_ptr->is_null_free()) {\n+    \/\/ A null-free, tightly coupled array allocation can only come from LibraryCallKit::inline_newNullRestrictedArray\n+    \/\/ which requires both the component type and the array length on stack for re-execution. Re-create and push\n+    \/\/ the component type.\n+    ciArrayKlass* klass = ary_klass_ptr->exact_klass()->as_array_klass();\n+    ciInstance* instance = klass->component_mirror_instance();\n+    const TypeInstPtr* t_instance = TypeInstPtr::make(instance);\n+    sfpt->ins_req(old_jvms->stkoff() + old_jvms->sp(), makecon(t_instance));\n+    adjustment++;\n+  }\n@@ -5449,5 +5814,5 @@\n-  sfpt->ins_req(old_jvms->stkoff() + old_jvms->sp(), alloc->in(AllocateNode::ALength));\n-  old_jvms->set_sp(old_jvms->sp()+1);\n-  old_jvms->set_monoff(old_jvms->monoff()+1);\n-  old_jvms->set_scloff(old_jvms->scloff()+1);\n-  old_jvms->set_endoff(old_jvms->endoff()+1);\n+  sfpt->ins_req(old_jvms->stkoff() + old_jvms->sp() + adjustment - 1, alloc->in(AllocateNode::ALength));\n+  old_jvms->set_sp(old_jvms->sp() + adjustment);\n+  old_jvms->set_monoff(old_jvms->monoff() + adjustment);\n+  old_jvms->set_scloff(old_jvms->scloff() + adjustment);\n+  old_jvms->set_endoff(old_jvms->endoff() + adjustment);\n@@ -5486,2 +5851,1 @@\n-    CallProjections callprojs;\n-    alloc->extract_projections(&callprojs, true);\n+    CallProjections* callprojs = alloc->extract_projections(true);\n@@ -5490,1 +5854,1 @@\n-    C->gvn_replace_by(callprojs.fallthrough_ioproj, alloc->in(TypeFunc::I_O));\n+    C->gvn_replace_by(callprojs->fallthrough_ioproj, alloc->in(TypeFunc::I_O));\n@@ -5532,1 +5896,1 @@\n-    set_i_o(callprojs.fallthrough_ioproj);\n+    set_i_o(callprojs->fallthrough_ioproj);\n@@ -5870,1 +6234,1 @@\n-    if (src_elem == dest_elem && src_elem == T_OBJECT) {\n+    if (src_elem == dest_elem && top_src->is_flat() == top_dest->is_flat() && src_elem == T_OBJECT) {\n@@ -5897,0 +6261,2 @@\n+          src_type = _gvn.type(src);\n+          top_src = src_type->isa_aryptr();\n@@ -5900,0 +6266,2 @@\n+          dest_type = _gvn.type(dest);\n+          top_dest = dest_type->isa_aryptr();\n@@ -5915,2 +6283,1 @@\n-      can_emit_guards &&\n-      !src->is_top() && !dest->is_top()) {\n+      can_emit_guards && !src->is_top() && !dest->is_top()) {\n@@ -5959,0 +6326,2 @@\n+      slow_region->add_req(not_subtype_ctrl);\n+    }\n@@ -5960,6 +6329,28 @@\n-      if (not_subtype_ctrl != top()) {\n-        PreserveJVMState pjvms(this);\n-        set_control(not_subtype_ctrl);\n-        uncommon_trap(Deoptimization::Reason_intrinsic,\n-                      Deoptimization::Action_make_not_entrant);\n-        assert(stopped(), \"Should be stopped\");\n+    const TypeKlassPtr* dest_klass_t = _gvn.type(dest_klass)->is_klassptr();\n+    const Type* toop = dest_klass_t->cast_to_exactness(false)->as_instance_type();\n+    src = _gvn.transform(new CheckCastPPNode(control(), src, toop));\n+    src_type = _gvn.type(src);\n+    top_src  = src_type->isa_aryptr();\n+\n+    \/\/ Handle flat inline type arrays (null-free arrays are handled by the subtype check above)\n+    if (!stopped() && UseFlatArray) {\n+      \/\/ If dest is flat, src must be flat as well (guaranteed by src <: dest check). Handle flat src here.\n+      assert(top_dest == nullptr || !top_dest->is_flat() || top_src->is_flat(), \"src array must be flat\");\n+      if (top_src != nullptr && top_src->is_flat()) {\n+        \/\/ Src is flat, check that dest is flat as well\n+        if (top_dest != nullptr && !top_dest->is_flat()) {\n+          generate_fair_guard(flat_array_test(dest_klass, \/* flat = *\/ false), slow_region);\n+          \/\/ Since dest is flat and src <: dest, dest must have the same type as src.\n+          top_dest = top_src->cast_to_exactness(false);\n+          assert(top_dest->is_flat(), \"dest must be flat\");\n+          dest = _gvn.transform(new CheckCastPPNode(control(), dest, top_dest));\n+        }\n+      } else if (top_src == nullptr || !top_src->is_not_flat()) {\n+        \/\/ Src might be flat and dest might not be flat. Go to the slow path if src is flat.\n+        \/\/ TODO 8251971: Optimize for the case when src\/dest are later found to be both flat.\n+        assert(top_dest == nullptr || !top_dest->is_flat(), \"dest array must not be flat\");\n+        generate_fair_guard(flat_array_test(src), slow_region);\n+        if (top_src != nullptr) {\n+          top_src = top_src->cast_to_not_flat();\n+          src = _gvn.transform(new CheckCastPPNode(control(), src, top_src));\n+        }\n@@ -5968,0 +6359,1 @@\n+\n@@ -5975,4 +6367,0 @@\n-\n-    const TypeKlassPtr* dest_klass_t = _gvn.type(dest_klass)->is_klassptr();\n-    const Type *toop = dest_klass_t->cast_to_exactness(false)->as_instance_type();\n-    src = _gvn.transform(new CheckCastPPNode(control(), src, toop));\n","filename":"src\/hotspot\/share\/opto\/library_call.cpp","additions":523,"deletions":135,"binary":false,"changes":658,"status":"modified"},{"patch":"@@ -31,0 +31,1 @@\n+#include \"opto\/inlinetypenode.hpp\"\n@@ -108,3 +109,11 @@\n-    if (!stopped() && result() != nullptr) {\n-      BasicType bt = result()->bottom_type()->basic_type();\n-      push_node(bt, result());\n+    Node* res = result();\n+    if (!stopped() && res != nullptr) {\n+      BasicType bt = res->bottom_type()->basic_type();\n+      if (C->inlining_incrementally() && res->is_InlineType()) {\n+        \/\/ The caller expects an oop when incrementally inlining an intrinsic that returns an\n+        \/\/ inline type. Make sure the call is re-executed if the allocation triggers a deoptimization.\n+        PreserveReexecuteState preexecs(this);\n+        jvms()->set_should_reexecute(true);\n+        res = res->as_InlineType()->buffer(this);\n+      }\n+      push_node(bt, res);\n@@ -141,1 +150,0 @@\n-  Node* load_mirror_from_klass(Node* klass);\n@@ -165,0 +173,9 @@\n+\n+  enum ArrayKind {\n+    AnyArray,\n+    NonArray,\n+    ObjectArray,\n+    NonObjectArray,\n+    TypeArray\n+  };\n+\n@@ -166,0 +183,1 @@\n+\n@@ -167,1 +185,1 @@\n-    return generate_array_guard_common(kls, region, false, false);\n+    return generate_array_guard_common(kls, region, AnyArray);\n@@ -170,1 +188,1 @@\n-    return generate_array_guard_common(kls, region, false, true);\n+    return generate_array_guard_common(kls, region, NonArray);\n@@ -173,1 +191,1 @@\n-    return generate_array_guard_common(kls, region, true, false);\n+    return generate_array_guard_common(kls, region, ObjectArray);\n@@ -176,1 +194,4 @@\n-    return generate_array_guard_common(kls, region, true, true);\n+    return generate_array_guard_common(kls, region, NonObjectArray);\n+  }\n+  Node* generate_typeArray_guard(Node* kls, RegionNode* region) {\n+    return generate_array_guard_common(kls, region, TypeArray);\n@@ -178,2 +199,1 @@\n-  Node* generate_array_guard_common(Node* kls, RegionNode* region,\n-                                    bool obj_array, bool not_array);\n+  Node* generate_array_guard_common(Node* kls, RegionNode* region, ArrayKind kind);\n@@ -229,1 +249,1 @@\n-  bool inline_unsafe_access(bool is_store, BasicType type, AccessKind kind, bool is_unaligned);\n+  bool inline_unsafe_access(bool is_store, BasicType type, AccessKind kind, bool is_unaligned, bool is_flat = false);\n@@ -233,0 +253,1 @@\n+  bool inline_newNullRestrictedArray();\n@@ -236,0 +257,3 @@\n+  bool inline_unsafe_isFlatArray();\n+  bool inline_unsafe_make_private_buffer();\n+  bool inline_unsafe_finish_private_buffer();\n@@ -262,0 +286,1 @@\n+  bool inline_primitive_Class_conversion(vmIntrinsics::ID id);\n","filename":"src\/hotspot\/share\/opto\/library_call.hpp","additions":36,"deletions":11,"binary":false,"changes":47,"status":"modified"},{"patch":"@@ -46,0 +46,1 @@\n+class UnswitchCandidate;\n@@ -82,1 +83,2 @@\n-         LoopNestLongOuterLoop = 1<<16 };\n+         LoopNestLongOuterLoop = 1<<16,\n+         FlatArrays            = 1<<17};\n@@ -105,0 +107,1 @@\n+  bool is_flat_arrays() const { return _loop_flags & FlatArrays; }\n@@ -118,0 +121,1 @@\n+  void mark_flat_arrays() { _loop_flags |= FlatArrays; }\n@@ -693,0 +697,1 @@\n+  bool no_unswitch_candidate() const;\n@@ -1429,1 +1434,2 @@\n-  IfNode* find_unswitch_candidate(const IdealLoopTree* loop) const;\n+  IfNode* find_unswitch_candidates(const IdealLoopTree* loop, Node_List& flat_array_checks) const;\n+  IfNode* find_unswitch_candidate_from_idoms(const IdealLoopTree* loop) const;\n@@ -1436,1 +1442,1 @@\n-                                   const UnswitchedLoopSelector& unswitched_loop_selector);\n+                                   const UnswitchCandidate& unswitch_candidate, const IfNode* loop_selector);\n@@ -1444,0 +1450,1 @@\n+                                            const UnswitchCandidate& unswitch_candidate,\n@@ -1578,0 +1585,1 @@\n+  void move_flat_array_check_out_of_loop(Node* n);\n@@ -1579,0 +1587,1 @@\n+  bool flat_array_element_type_check(Node *n);\n@@ -1787,0 +1796,2 @@\n+\n+  void collect_flat_array_checks(const IdealLoopTree* loop, Node_List& flat_array_checks) const;\n","filename":"src\/hotspot\/share\/opto\/loopnode.hpp","additions":14,"deletions":3,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -36,0 +36,1 @@\n+#include \"opto\/inlinetypenode.hpp\"\n@@ -67,0 +68,6 @@\n+  \/\/ Inline types should not be split through Phis because they cannot be merged\n+  \/\/ through Phi nodes but each value input needs to be merged individually.\n+  if (n->is_InlineType()) {\n+    return nullptr;\n+  }\n+\n@@ -763,0 +770,4 @@\n+      if (inp->isa_InlineType()) {\n+        \/\/ TODO 8302217 This prevents PhiNode::push_inline_types_through\n+        return nullptr;\n+      }\n@@ -1092,0 +1103,48 @@\n+\/\/ We can't use immutable memory for the flat array check because we are loading the mark word which is\n+\/\/ mutable. Although the bits we are interested in are immutable (we check for markWord::unlocked_value),\n+\/\/ we need to use raw memory to not break anti dependency analysis. Below code will attempt to still move\n+\/\/ flat array checks out of loops, mainly to enable loop unswitching.\n+void PhaseIdealLoop::move_flat_array_check_out_of_loop(Node* n) {\n+  \/\/ Skip checks for more than one array\n+  if (n->req() > 3) {\n+    return;\n+  }\n+  Node* mem = n->in(FlatArrayCheckNode::Memory);\n+  Node* array = n->in(FlatArrayCheckNode::ArrayOrKlass)->uncast();\n+  IdealLoopTree* check_loop = get_loop(get_ctrl(n));\n+  IdealLoopTree* ary_loop = get_loop(get_ctrl(array));\n+\n+  \/\/ Check if array is loop invariant\n+  if (!check_loop->is_member(ary_loop)) {\n+    \/\/ Walk up memory graph from the check until we leave the loop\n+    VectorSet wq;\n+    wq.set(mem->_idx);\n+    while (check_loop->is_member(get_loop(ctrl_or_self(mem)))) {\n+      if (mem->is_Phi()) {\n+        mem = mem->in(1);\n+      } else if (mem->is_MergeMem()) {\n+        mem = mem->as_MergeMem()->memory_at(Compile::AliasIdxRaw);\n+      } else if (mem->is_Proj()) {\n+        mem = mem->in(0);\n+      } else if (mem->is_MemBar() || mem->is_SafePoint()) {\n+        mem = mem->in(TypeFunc::Memory);\n+      } else if (mem->is_Store() || mem->is_LoadStore() || mem->is_ClearArray()) {\n+        mem = mem->in(MemNode::Memory);\n+      } else {\n+#ifdef ASSERT\n+        mem->dump();\n+#endif\n+        ShouldNotReachHere();\n+      }\n+      if (wq.test_set(mem->_idx)) {\n+        return;\n+      }\n+    }\n+    \/\/ Replace memory input and re-compute ctrl to move the check out of the loop\n+    _igvn.replace_input_of(n, 1, mem);\n+    set_ctrl_and_loop(n, get_early_ctrl(n));\n+    Node* bol = n->unique_out();\n+    set_ctrl_and_loop(bol, get_early_ctrl(bol));\n+  }\n+}\n+\n@@ -1123,0 +1182,6 @@\n+\n+  if (n->isa_FlatArrayCheck()) {\n+    move_flat_array_check_out_of_loop(n);\n+    return n;\n+  }\n+\n@@ -1402,0 +1467,98 @@\n+bool PhaseIdealLoop::flat_array_element_type_check(Node *n) {\n+  \/\/ If the CmpP is a subtype check for a value that has just been\n+  \/\/ loaded from an array, the subtype check guarantees the value\n+  \/\/ can't be stored in a flat array and the load of the value\n+  \/\/ happens with a flat array check then: push the type check\n+  \/\/ through the phi of the flat array check. This needs special\n+  \/\/ logic because the subtype check's input is not a phi but a\n+  \/\/ LoadKlass that must first be cloned through the phi.\n+  if (n->Opcode() != Op_CmpP) {\n+    return false;\n+  }\n+\n+  Node* klassptr = n->in(1);\n+  Node* klasscon = n->in(2);\n+\n+  if (klassptr->is_DecodeNarrowPtr()) {\n+    klassptr = klassptr->in(1);\n+  }\n+\n+  if (klassptr->Opcode() != Op_LoadKlass && klassptr->Opcode() != Op_LoadNKlass) {\n+    return false;\n+  }\n+\n+  if (!klasscon->is_Con()) {\n+    return false;\n+  }\n+\n+  Node* addr = klassptr->in(MemNode::Address);\n+\n+  if (!addr->is_AddP()) {\n+    return false;\n+  }\n+\n+  intptr_t offset;\n+  Node* obj = AddPNode::Ideal_base_and_offset(addr, &_igvn, offset);\n+\n+  if (obj == nullptr) {\n+    return false;\n+  }\n+\n+  assert(obj != nullptr && addr->in(AddPNode::Base) == addr->in(AddPNode::Address), \"malformed AddP?\");\n+  if (obj->Opcode() == Op_CastPP) {\n+    obj = obj->in(1);\n+  }\n+\n+  if (!obj->is_Phi()) {\n+    return false;\n+  }\n+\n+  Node* region = obj->in(0);\n+\n+  Node* phi = PhiNode::make_blank(region, n->in(1));\n+  for (uint i = 1; i < region->req(); i++) {\n+    Node* in = obj->in(i);\n+    Node* ctrl = region->in(i);\n+    if (addr->in(AddPNode::Base) != obj) {\n+      Node* cast = addr->in(AddPNode::Base);\n+      assert(cast->Opcode() == Op_CastPP && cast->in(0) != nullptr, \"inconsistent subgraph\");\n+      Node* cast_clone = cast->clone();\n+      cast_clone->set_req(0, ctrl);\n+      cast_clone->set_req(1, in);\n+      register_new_node(cast_clone, ctrl);\n+      const Type* tcast = cast_clone->Value(&_igvn);\n+      _igvn.set_type(cast_clone, tcast);\n+      cast_clone->as_Type()->set_type(tcast);\n+      in = cast_clone;\n+    }\n+    Node* addr_clone = addr->clone();\n+    addr_clone->set_req(AddPNode::Base, in);\n+    addr_clone->set_req(AddPNode::Address, in);\n+    register_new_node(addr_clone, ctrl);\n+    _igvn.set_type(addr_clone, addr_clone->Value(&_igvn));\n+    Node* klassptr_clone = klassptr->clone();\n+    klassptr_clone->set_req(2, addr_clone);\n+    register_new_node(klassptr_clone, ctrl);\n+    _igvn.set_type(klassptr_clone, klassptr_clone->Value(&_igvn));\n+    if (klassptr != n->in(1)) {\n+      Node* decode = n->in(1);\n+      assert(decode->is_DecodeNarrowPtr(), \"inconsistent subgraph\");\n+      Node* decode_clone = decode->clone();\n+      decode_clone->set_req(1, klassptr_clone);\n+      register_new_node(decode_clone, ctrl);\n+      _igvn.set_type(decode_clone, decode_clone->Value(&_igvn));\n+      klassptr_clone = decode_clone;\n+    }\n+    phi->set_req(i, klassptr_clone);\n+  }\n+  register_new_node(phi, region);\n+  Node* orig = n->in(1);\n+  _igvn.replace_input_of(n, 1, phi);\n+  split_if_with_blocks_post(n);\n+  if (n->outcnt() != 0) {\n+    _igvn.replace_input_of(n, 1, orig);\n+    _igvn.remove_dead_node(phi);\n+  }\n+  return true;\n+}\n+\n@@ -1408,0 +1571,4 @@\n+  if (flat_array_element_type_check(n)) {\n+    return;\n+  }\n+\n@@ -1555,0 +1722,5 @@\n+\n+  \/\/ Remove multiple allocations of the same inline type\n+  if (n->is_InlineType()) {\n+    n->as_InlineType()->remove_redundant_allocations(this);\n+  }\n@@ -2037,1 +2209,9 @@\n-  Node *sample_cmp = sample_bool->in(1);\n+  Node* sample_cmp = sample_bool->in(1);\n+  const Type* t = Type::TOP;\n+  const TypePtr* at = nullptr;\n+  if (sample_cmp->is_FlatArrayCheck()) {\n+    \/\/ Left input of a FlatArrayCheckNode is memory, set the (adr) type of the phi accordingly\n+    assert(sample_cmp->in(1)->bottom_type() == Type::MEMORY, \"unexpected input type\");\n+    t = Type::MEMORY;\n+    at = TypeRawPtr::BOTTOM;\n+  }\n@@ -2040,1 +2220,1 @@\n-  PhiNode *phi1 = new PhiNode(phi->in(0), Type::TOP);\n+  PhiNode *phi1 = new PhiNode(phi->in(0), t, at);\n","filename":"src\/hotspot\/share\/opto\/loopopts.cpp","additions":182,"deletions":2,"binary":false,"changes":184,"status":"modified"},{"patch":"@@ -27,0 +27,1 @@\n+#include \"ci\/ciFlatArrayKlass.hpp\"\n@@ -28,0 +29,1 @@\n+#include \"classfile\/systemDictionary.hpp\"\n@@ -42,0 +44,1 @@\n+#include \"opto\/inlinetypenode.hpp\"\n@@ -235,0 +238,2 @@\n+                     ->cast_to_not_flat(t_oop->is_aryptr()->is_not_flat())\n+                     ->cast_to_not_null_free(t_oop->is_aryptr()->is_not_null_free())\n@@ -261,1 +266,1 @@\n-               tp->isa_aryptr() &&        tp->offset() == Type::OffsetBot &&\n+        tp->isa_aryptr() &&        tp->offset() == Type::OffsetBot &&\n@@ -1014,1 +1019,1 @@\n-    return (eliminate_boxing && non_volatile) || is_stable_ary;\n+    return (eliminate_boxing && non_volatile) || is_stable_ary || tp->is_inlinetypeptr();\n@@ -1071,1 +1076,1 @@\n-      uint shift  = exact_log2(type2aelembytes(ary_elem));\n+      uint shift  = ary_t->is_flat() ? ary_t->flat_log_elem_size() : exact_log2(type2aelembytes(ary_elem));\n@@ -1190,1 +1195,1 @@\n-        const TypeVect* out_vt = as_LoadVector()->vect_type();\n+        const TypeVect* out_vt = is_Load() ? as_LoadVector()->vect_type() : as_StoreVector()->vect_type();\n@@ -1208,0 +1213,5 @@\n+      Node* default_value = ld_alloc->in(AllocateNode::DefaultValue);\n+      if (default_value != nullptr) {\n+        return default_value;\n+      }\n+      assert(ld_alloc->in(AllocateNode::RawDefaultValue) == nullptr, \"default value may not be null\");\n@@ -1275,0 +1285,17 @@\n+  \/\/ Loading from an InlineType? The InlineType has the values of\n+  \/\/ all fields as input. Look for the field with matching offset.\n+  Node* addr = in(Address);\n+  intptr_t offset;\n+  Node* base = AddPNode::Ideal_base_and_offset(addr, phase, offset);\n+  if (base != nullptr && base->is_InlineType() && offset > oopDesc::klass_offset_in_bytes()) {\n+    Node* value = base->as_InlineType()->field_value_by_offset((int)offset, true);\n+    if (value != nullptr) {\n+      if (Opcode() == Op_LoadN) {\n+        \/\/ Encode oop value if we are loading a narrow oop\n+        assert(!phase->type(value)->isa_narrowoop(), \"should already be decoded\");\n+        value = phase->transform(new EncodePNode(value, bottom_type()));\n+      }\n+      return value;\n+    }\n+  }\n+\n@@ -2059,0 +2086,1 @@\n+        && !ary->is_flat()\n@@ -2094,0 +2122,2 @@\n+            \/\/ Default value load\n+            tp->is_instptr()->instance_klass() == ciEnv::current()->Class_klass() ||\n@@ -2099,1 +2129,3 @@\n-    \/\/ Optimize loads from constant fields.\n+    BasicType bt = memory_type();\n+\n+    \/\/ Optimize loads from constant fields.\n@@ -2103,1 +2135,1 @@\n-      const Type* con_type = Type::make_constant_from_field(const_oop->as_instance(), off, is_unsigned(), memory_type());\n+      const Type* con_type = Type::make_constant_from_field(const_oop->as_instance(), off, is_unsigned(), bt);\n@@ -2238,1 +2270,0 @@\n-\n@@ -2241,1 +2272,10 @@\n-    return TypeX::make(markWord::prototype().value());\n+    if (EnableValhalla) {\n+      \/\/ The mark word may contain property bits (inline, flat, null-free)\n+      Node* klass_node = alloc->in(AllocateNode::KlassNode);\n+      const TypeKlassPtr* tkls = phase->type(klass_node)->isa_klassptr();\n+      if (tkls != nullptr && tkls->is_loaded() && tkls->klass_is_exact()) {\n+        return TypeX::make(tkls->exact_klass()->prototype_header().value());\n+      }\n+    } else {\n+      return TypeX::make(markWord::prototype().value());\n+    }\n@@ -2392,1 +2432,2 @@\n-Node* LoadKlassNode::make(PhaseGVN& gvn, Node* ctl, Node* mem, Node* adr, const TypePtr* at, const TypeKlassPtr* tk) {\n+Node* LoadKlassNode::make(PhaseGVN& gvn, Node* ctl, Node* mem, Node* adr, const TypePtr* at,\n+                          const TypeKlassPtr* tk) {\n@@ -2439,1 +2480,2 @@\n-      ciType* t = tinst->java_mirror_type();\n+      bool is_null_free_array = false;\n+      ciType* t = tinst->java_mirror_type(&is_null_free_array);\n@@ -2449,1 +2491,5 @@\n-          return TypeKlassPtr::make(ciArrayKlass::make(t), Type::trust_interfaces);\n+          const TypeKlassPtr* tklass = TypeKlassPtr::make(ciArrayKlass::make(t), Type::trust_interfaces);\n+          if (is_null_free_array) {\n+            tklass = tklass->is_aryklassptr()->cast_to_null_free();\n+          }\n+          return tklass;\n@@ -2456,1 +2502,5 @@\n-        return TypeKlassPtr::make(t->as_klass(), Type::trust_interfaces);\n+        const TypeKlassPtr* tklass = TypeKlassPtr::make(t->as_klass(), Type::trust_interfaces);\n+        if (is_null_free_array) {\n+          tklass = tklass->is_aryklassptr()->cast_to_null_free();\n+        }\n+        return tklass;\n@@ -2468,1 +2518,1 @@\n-  const TypeAryPtr *tary = tp->isa_aryptr();\n+  const TypeAryPtr* tary = tp->isa_aryptr();\n@@ -3468,1 +3518,1 @@\n-  {\n+  if (phase->C->get_adr_type(phase->C->get_alias_index(adr_type())) != TypeAryPtr::INLINES) {\n@@ -3488,0 +3538,1 @@\n+             (Opcode() == Op_StoreL && st->Opcode() == Op_StoreN) ||\n@@ -3614,2 +3665,1 @@\n-  if (result == this &&\n-      ReduceFieldZeroing && phase->type(val)->is_zero_type()) {\n+  if (result == this && ReduceFieldZeroing) {\n@@ -3617,1 +3667,2 @@\n-    if (mem->is_Proj() && mem->in(0)->is_Allocate()) {\n+    if (mem->is_Proj() && mem->in(0)->is_Allocate() &&\n+        (phase->type(val)->is_zero_type() || mem->in(0)->in(AllocateNode::DefaultValue) == val)) {\n@@ -3621,1 +3672,1 @@\n-    if (result == this) {\n+    if (result == this && phase->type(val)->is_zero_type()) {\n@@ -3806,3 +3857,7 @@\n-    Node* mem = my_store->as_MergeMem()->memory_at(oop_alias_idx());\n-    set_req_X(MemNode::OopStore, mem, phase);\n-    return this;\n+    if (oop_alias_idx() != phase->C->get_alias_index(TypeAryPtr::INLINES) ||\n+        phase->C->flat_accesses_share_alias()) {\n+      \/\/ The alias that was recorded is no longer accurate enough.\n+      Node* mem = my_store->as_MergeMem()->memory_at(oop_alias_idx());\n+      set_req_X(MemNode::OopStore, mem, phase);\n+      return this;\n+    }\n@@ -3979,1 +4034,1 @@\n-    return new ClearArrayNode(in(0), in(1), in(2), in(3), true);\n+    return new ClearArrayNode(in(0), in(1), in(2), in(3), in(4), true);\n@@ -3997,1 +4052,1 @@\n-  Node *zero = phase->makecon(TypeLong::ZERO);\n+  Node *val = in(4);\n@@ -3999,1 +4054,1 @@\n-  mem = new StoreLNode(in(0),mem,adr,atp,zero,MemNode::unordered,false);\n+  mem = new StoreLNode(in(0), mem, adr, atp, val, MemNode::unordered, false);\n@@ -4004,1 +4059,1 @@\n-    mem = new StoreLNode(in(0),mem,adr,atp,zero,MemNode::unordered,false);\n+    mem = new StoreLNode(in(0), mem, adr, atp, val, MemNode::unordered, false);\n@@ -4038,0 +4093,2 @@\n+                                   Node* val,\n+                                   Node* raw_val,\n@@ -4048,1 +4105,7 @@\n-    mem = StoreNode::make(*phase, ctl, mem, adr, atp, phase->zerocon(T_INT), T_INT, MemNode::unordered);\n+    if (val != nullptr) {\n+      assert(phase->type(val)->isa_narrowoop(), \"should be narrow oop\");\n+      mem = new StoreNNode(ctl, mem, adr, atp, val, MemNode::unordered);\n+    } else {\n+      assert(raw_val == nullptr, \"val may not be null\");\n+      mem = StoreNode::make(*phase, ctl, mem, adr, atp, phase->zerocon(T_INT), T_INT, MemNode::unordered);\n+    }\n@@ -4055,1 +4118,1 @@\n-  return clear_memory(ctl, mem, dest, phase->MakeConX(offset), end_offset, phase);\n+  return clear_memory(ctl, mem, dest, raw_val, phase->MakeConX(offset), end_offset, phase);\n@@ -4059,0 +4122,1 @@\n+                                   Node* raw_val,\n@@ -4081,1 +4145,4 @@\n-  mem = new ClearArrayNode(ctl, mem, zsize, adr, false);\n+  if (raw_val == nullptr) {\n+    raw_val = phase->MakeConX(0);\n+  }\n+  mem = new ClearArrayNode(ctl, mem, zsize, adr, raw_val, false);\n@@ -4086,0 +4153,2 @@\n+                                   Node* val,\n+                                   Node* raw_val,\n@@ -4100,1 +4169,1 @@\n-    mem = clear_memory(ctl, mem, dest,\n+    mem = clear_memory(ctl, mem, dest, val, raw_val,\n@@ -4107,1 +4176,7 @@\n-    mem = StoreNode::make(*phase, ctl, mem, adr, atp, phase->zerocon(T_INT), T_INT, MemNode::unordered);\n+    if (val != nullptr) {\n+      assert(phase->type(val)->isa_narrowoop(), \"should be narrow oop\");\n+      mem = new StoreNNode(ctl, mem, adr, atp, val, MemNode::unordered);\n+    } else {\n+      assert(raw_val == nullptr, \"val may not be null\");\n+      mem = StoreNode::make(*phase, ctl, mem, adr, atp, phase->zerocon(T_INT), T_INT, MemNode::unordered);\n+    }\n@@ -4253,1 +4328,1 @@\n-Node *MemBarNode::match( const ProjNode *proj, const Matcher *m ) {\n+Node *MemBarNode::match(const ProjNode *proj, const Matcher *m, const RegMask* mask) {\n@@ -4540,1 +4615,3 @@\n-  if (init == nullptr || init->is_complete())  return false;\n+  if (init == nullptr || init->is_complete()) {\n+    return false;\n+  }\n@@ -4719,0 +4796,6 @@\n+                if (base->is_Phi()) {\n+                  \/\/ In rare case, base may be a PhiNode and it may read\n+                  \/\/ the same memory slice between InitializeNode and store.\n+                  failed = true;\n+                  break;\n+                }\n@@ -5305,0 +5388,2 @@\n+                                              allocation()->in(AllocateNode::DefaultValue),\n+                                              allocation()->in(AllocateNode::RawDefaultValue),\n@@ -5364,0 +5449,2 @@\n+                                            allocation()->in(AllocateNode::DefaultValue),\n+                                            allocation()->in(AllocateNode::RawDefaultValue),\n","filename":"src\/hotspot\/share\/opto\/memnode.cpp","additions":119,"deletions":32,"binary":false,"changes":151,"status":"modified"},{"patch":"@@ -33,0 +33,1 @@\n+#include \"opto\/convertnode.hpp\"\n@@ -34,0 +35,1 @@\n+#include \"opto\/inlinetypenode.hpp\"\n@@ -105,4 +107,10 @@\n-Node *Parse::fetch_interpreter_state(int index,\n-                                     BasicType bt,\n-                                     Node *local_addrs,\n-                                     Node *local_addrs_base) {\n+Node* Parse::fetch_interpreter_state(int index,\n+                                     const Type* type,\n+                                     Node* local_addrs,\n+                                     Node* local_addrs_base) {\n+  BasicType bt = type->basic_type();\n+  if (type == TypePtr::NULL_PTR) {\n+    \/\/ Ptr types are mixed together with T_ADDRESS but nullptr is\n+    \/\/ really for T_OBJECT types so correct it.\n+    bt = T_OBJECT;\n+  }\n@@ -150,1 +158,0 @@\n-\n@@ -174,0 +181,6 @@\n+    if (tp->is_inlinetypeptr() && !tp->maybe_null()) {\n+      \/\/ Check inline types for null here to prevent checkcast from adding an\n+      \/\/ exception state before the bytecode entry (use 'bad_type_ctrl' instead).\n+      l = null_check_oop(l, &bad_type_ctrl);\n+      bad_type_exit->control()->add_req(bad_type_ctrl);\n+    }\n@@ -190,1 +203,0 @@\n-\n@@ -245,1 +257,1 @@\n-    Node *lock_object = fetch_interpreter_state(index*2, T_OBJECT, monitors_addr, osr_buf);\n+    Node* lock_object = fetch_interpreter_state(index*2, Type::get_const_basic_type(T_OBJECT), monitors_addr, osr_buf);\n@@ -247,2 +259,1 @@\n-    Node *displaced_hdr = fetch_interpreter_state((index*2) + 1, T_ADDRESS, monitors_addr, osr_buf);\n-\n+    Node* displaced_hdr = fetch_interpreter_state((index*2) + 1, Type::get_const_basic_type(T_ADDRESS), monitors_addr, osr_buf);\n@@ -316,7 +327,1 @@\n-    BasicType bt = type->basic_type();\n-    if (type == TypePtr::NULL_PTR) {\n-      \/\/ Ptr types are mixed together with T_ADDRESS but null is\n-      \/\/ really for T_OBJECT types so correct it.\n-      bt = T_OBJECT;\n-    }\n-    Node *value = fetch_interpreter_state(index, bt, locals_addr, osr_buf);\n+    Node* value = fetch_interpreter_state(index, type, locals_addr, osr_buf);\n@@ -522,1 +527,3 @@\n-    assert(false, \"type flow analysis failed during parsing\");\n+    \/\/ TODO Adding a trap due to an unloaded return type in ciTypeFlow::StateVector::do_invoke\n+    \/\/ can lead to this. Re-enable once 8284443 is fixed.\n+    \/\/assert(false, \"type flow analysis failed during parsing\");\n@@ -613,0 +620,22 @@\n+  \/\/ Handle inline type arguments\n+  int arg_size = method()->arg_size();\n+  for (int i = 0; i < arg_size; i++) {\n+    Node* parm = local(i);\n+    const Type* t = _gvn.type(parm);\n+    if (t->is_inlinetypeptr()) {\n+      \/\/ Create InlineTypeNode from the oop and replace the parameter\n+      bool is_larval = (i == 0) && method()->is_object_constructor() && !method()->holder()->is_java_lang_Object();\n+      Node* vt = InlineTypeNode::make_from_oop(this, parm, t->inline_klass(), !t->maybe_null(), is_larval);\n+      replace_in_map(parm, vt);\n+    } else if (UseTypeSpeculation && (i == (arg_size - 1)) && !is_osr_parse() && method()->has_vararg() &&\n+               t->isa_aryptr() != nullptr && !t->is_aryptr()->is_null_free() && !t->is_aryptr()->is_not_null_free()) {\n+      \/\/ Speculate on varargs Object array being not null-free (and therefore also not flat)\n+      const TypePtr* spec_type = t->speculative();\n+      spec_type = (spec_type != nullptr && spec_type->isa_aryptr() != nullptr) ? spec_type : t->is_aryptr();\n+      spec_type = spec_type->remove_speculative()->is_aryptr()->cast_to_not_null_free();\n+      spec_type = TypeOopPtr::make(TypePtr::BotPTR, Type::Offset::bottom, TypeOopPtr::InstanceBot, spec_type);\n+      Node* cast = _gvn.transform(new CheckCastPPNode(control(), parm, t->join_speculative(spec_type)));\n+      replace_in_map(parm, cast);\n+    }\n+  }\n+\n@@ -798,2 +827,2 @@\n-  if (tf()->range()->cnt() > TypeFunc::Parms) {\n-    const Type* ret_type = tf()->range()->field_at(TypeFunc::Parms);\n+  if (tf()->range_sig()->cnt() > TypeFunc::Parms) {\n+    const Type* ret_type = tf()->range_sig()->field_at(TypeFunc::Parms);\n@@ -821,1 +850,1 @@\n-    assert((int)(tf()->range()->cnt() - TypeFunc::Parms) == ret_size, \"good tf range\");\n+    assert((int)(tf()->range_sig()->cnt() - TypeFunc::Parms) == ret_size, \"good tf range\");\n@@ -828,1 +857,0 @@\n-\n@@ -833,2 +861,2 @@\n-  int        arg_size = tf->domain()->cnt();\n-  int        max_size = MAX2(arg_size, (int)tf->range()->cnt());\n+  int        arg_size = tf->domain_sig()->cnt();\n+  int        max_size = MAX2(arg_size, (int)tf->range_cc()->cnt());\n@@ -837,0 +865,1 @@\n+  jvms->set_map(map);\n@@ -848,3 +877,20 @@\n-  uint i;\n-  for (i = 0; i < (uint)arg_size; i++) {\n-    Node* parm = initial_gvn()->transform(new ParmNode(start, i));\n+  PhaseGVN& gvn = *initial_gvn();\n+  uint i = 0;\n+  int arg_num = 0;\n+  for (uint j = 0; i < (uint)arg_size; i++) {\n+    const Type* t = tf->domain_sig()->field_at(i);\n+    Node* parm = nullptr;\n+    if (t->is_inlinetypeptr() && method()->is_scalarized_arg(arg_num)) {\n+      \/\/ Inline type arguments are not passed by reference: we get an argument per\n+      \/\/ field of the inline type. Build InlineTypeNodes from the inline type arguments.\n+      GraphKit kit(jvms, &gvn);\n+      kit.set_control(map->control());\n+      Node* old_mem = map->memory();\n+      \/\/ Use immutable memory for inline type loads and restore it below\n+      kit.set_all_memory(C->immutable_memory());\n+      parm = InlineTypeNode::make_from_multi(&kit, start, t->inline_klass(), j, \/* in= *\/ true, \/* null_free= *\/ !t->maybe_null());\n+      map->set_control(kit.control());\n+      map->set_memory(old_mem);\n+    } else {\n+      parm = gvn.transform(new ParmNode(start, j++));\n+    }\n@@ -854,0 +900,3 @@\n+    if (i >= TypeFunc::Parms && t != Type::HALF) {\n+      arg_num++;\n+    }\n@@ -860,1 +909,0 @@\n-  jvms->set_map(map);\n@@ -887,1 +935,1 @@\n-  int ret_size = tf()->range()->cnt() - TypeFunc::Parms;\n+  int ret_size = tf()->range_sig()->cnt() - TypeFunc::Parms;\n@@ -891,2 +939,24 @@\n-    ret->add_req(kit.argument(0));\n-    \/\/ Note:  The second dummy edge is not needed by a ReturnNode.\n+    Node* res = kit.argument(0);\n+    if (tf()->returns_inline_type_as_fields()) {\n+      \/\/ Multiple return values (inline type fields): add as many edges\n+      \/\/ to the Return node as returned values.\n+      InlineTypeNode* vt = res->as_InlineType();\n+      ret->add_req_batch(nullptr, tf()->range_cc()->cnt() - TypeFunc::Parms);\n+      if (vt->is_allocated(&kit.gvn()) && !StressCallingConvention) {\n+        ret->init_req(TypeFunc::Parms, vt);\n+      } else {\n+        \/\/ Return the tagged klass pointer to signal scalarization to the caller\n+        Node* tagged_klass = vt->tagged_klass(kit.gvn());\n+        \/\/ Return null if the inline type is null (IsInit field is not set)\n+        Node* conv   = kit.gvn().transform(new ConvI2LNode(vt->get_is_init()));\n+        Node* shl    = kit.gvn().transform(new LShiftLNode(conv, kit.intcon(63)));\n+        Node* shr    = kit.gvn().transform(new RShiftLNode(shl, kit.intcon(63)));\n+        tagged_klass = kit.gvn().transform(new AndLNode(tagged_klass, shr));\n+        ret->init_req(TypeFunc::Parms, tagged_klass);\n+      }\n+      uint idx = TypeFunc::Parms + 1;\n+      vt->pass_fields(&kit, ret, idx, false, false);\n+    } else {\n+      ret->add_req(res);\n+      \/\/ Note:  The second dummy edge is not needed by a ReturnNode.\n+    }\n@@ -1016,1 +1086,1 @@\n-  if (method()->is_object_initializer() &&\n+  if ((method()->is_object_constructor() || method()->is_class_initializer()) &&\n@@ -1044,2 +1114,2 @@\n-  if (tf()->range()->cnt() > TypeFunc::Parms) {\n-    const Type* ret_type = tf()->range()->field_at(TypeFunc::Parms);\n+  if (tf()->range_sig()->cnt() > TypeFunc::Parms) {\n+    const Type* ret_type = tf()->range_sig()->field_at(TypeFunc::Parms);\n@@ -1139,1 +1209,2 @@\n-    kit.null_check_receiver_before_call(method());\n+    Node* receiver = kit.argument(0);\n+    Node* null_free = kit.null_check_receiver_before_call(method());\n@@ -1141,0 +1212,5 @@\n+    if (receiver->is_InlineType() && receiver->as_InlineType()->is_larval()) {\n+      \/\/ Replace the larval inline type receiver in the exit map as well to make sure that\n+      \/\/ we can find and update it in Parse::do_call when we are done with the initialization.\n+      _exits.map()->replace_edge(receiver, null_free);\n+    }\n@@ -1177,1 +1253,1 @@\n-  uint arg_size = tf()->domain()->cnt();\n+  uint arg_size = tf()->domain_sig()->cnt();\n@@ -1251,0 +1327,1 @@\n+      assert(!_gvn.type(lock_obj)->make_oopptr()->can_be_inline_type(), \"can't be an inline type\");\n@@ -1686,0 +1763,36 @@\n+  \/\/ Check for merge conflicts involving inline types\n+  JVMState* old_jvms = map()->jvms();\n+  int old_bci = bci();\n+  JVMState* tmp_jvms = old_jvms->clone_shallow(C);\n+  tmp_jvms->set_should_reexecute(true);\n+  tmp_jvms->bind_map(map());\n+  \/\/ Execution needs to restart a the next bytecode (entry of next\n+  \/\/ block)\n+  if (target->is_merged() ||\n+      pnum > PhiNode::Input ||\n+      target->is_handler() ||\n+      target->is_loop_head()) {\n+    set_parse_bci(target->start());\n+    for (uint j = TypeFunc::Parms; j < map()->req(); j++) {\n+      Node* n = map()->in(j);                 \/\/ Incoming change to target state.\n+      const Type* t = nullptr;\n+      if (tmp_jvms->is_loc(j)) {\n+        t = target->local_type_at(j - tmp_jvms->locoff());\n+      } else if (tmp_jvms->is_stk(j) && j < (uint)sp() + tmp_jvms->stkoff()) {\n+        t = target->stack_type_at(j - tmp_jvms->stkoff());\n+      }\n+      if (t != nullptr && t != Type::BOTTOM) {\n+        if (n->is_InlineType() && !t->is_inlinetypeptr()) {\n+          \/\/ Allocate inline type in src block to be able to merge it with oop in target block\n+          map()->set_req(j, n->as_InlineType()->buffer(this));\n+        } else if (!n->is_InlineType() && t->is_inlinetypeptr()) {\n+          \/\/ Scalarize null in src block to be able to merge it with inline type in target block\n+          assert(gvn().type(n)->is_zero_type(), \"Should have been scalarized\");\n+          map()->set_req(j, InlineTypeNode::make_null(gvn(), t->inline_klass()));\n+        }\n+      }\n+    }\n+  }\n+  old_jvms->bind_map(map());\n+  set_parse_bci(old_bci);\n+\n@@ -1740,0 +1853,1 @@\n+\n@@ -1775,0 +1889,1 @@\n+    bool last_merge = (pnum == PhiNode::Input);\n@@ -1779,1 +1894,1 @@\n-      if (m->is_Phi() && m->as_Phi()->region() == r)\n+      if (m->is_Phi() && m->as_Phi()->region() == r) {\n@@ -1781,1 +1896,3 @@\n-      else\n+      } else if (m->is_InlineType() && m->as_InlineType()->has_phi_inputs(r)) {\n+        phi = m->as_InlineType()->get_oop()->as_Phi();\n+      } else {\n@@ -1783,0 +1900,1 @@\n+      }\n@@ -1829,1 +1947,24 @@\n-      if (phi != nullptr) {\n+      \/\/ Merging two inline types?\n+      if (phi != nullptr && phi->bottom_type()->is_inlinetypeptr()) {\n+        \/\/ Reload current state because it may have been updated by ensure_phi\n+        m = map()->in(j);\n+        InlineTypeNode* vtm = m->as_InlineType(); \/\/ Current inline type\n+        InlineTypeNode* vtn = n->as_InlineType(); \/\/ Incoming inline type\n+        assert(vtm->get_oop() == phi, \"Inline type should have Phi input\");\n+        if (TraceOptoParse) {\n+#ifdef ASSERT\n+          tty->print_cr(\"\\nMerging inline types\");\n+          tty->print_cr(\"Current:\");\n+          vtm->dump(2);\n+          tty->print_cr(\"Incoming:\");\n+          vtn->dump(2);\n+          tty->cr();\n+#endif\n+        }\n+        \/\/ Do the merge\n+        vtm->merge_with(&_gvn, vtn, pnum, last_merge);\n+        if (last_merge) {\n+          map()->set_req(j, _gvn.transform(vtm));\n+          record_for_igvn(vtm);\n+        }\n+      } else if (phi != nullptr) {\n@@ -1833,1 +1974,1 @@\n-        if (pnum == PhiNode::Input) {\n+        if (last_merge) {\n@@ -1849,2 +1990,1 @@\n-    if (pnum == PhiNode::Input &&\n-        !r->in(0)) {         \/\/ The occasional useless Region\n+    if (last_merge && !r->in(0)) {         \/\/ The occasional useless Region\n@@ -2002,0 +2142,2 @@\n+      } else if (n->is_InlineType() && n->as_InlineType()->has_phi_inputs(r)) {\n+        n->as_InlineType()->add_new_path(r);\n@@ -2024,0 +2166,4 @@\n+  InlineTypeNode* vt = o->isa_InlineType();\n+  if (vt != nullptr && vt->has_phi_inputs(region)) {\n+    return vt->get_oop()->as_Phi();\n+  }\n@@ -2043,2 +2189,2 @@\n-  \/\/ is mixing ints and oops or some such.  Forcing it to top\n-  \/\/ makes it go dead.\n+  \/\/ is already dead or is mixing ints and oops or some such.\n+  \/\/ Forcing it to top makes it go dead.\n@@ -2057,5 +2203,14 @@\n-  PhiNode* phi = PhiNode::make(region, o, t);\n-  gvn().set_type(phi, t);\n-  if (C->do_escape_analysis()) record_for_igvn(phi);\n-  map->set_req(idx, phi);\n-  return phi;\n+  if (vt != nullptr && t->is_inlinetypeptr()) {\n+    \/\/ Inline types are merged by merging their field values.\n+    \/\/ Create a cloned InlineTypeNode with phi inputs that\n+    \/\/ represents the merged inline type and update the map.\n+    vt = vt->clone_with_phis(&_gvn, region);\n+    map->set_req(idx, vt);\n+    return vt->get_oop()->as_Phi();\n+  } else {\n+    PhiNode* phi = PhiNode::make(region, o, t);\n+    gvn().set_type(phi, t);\n+    if (C->do_escape_analysis()) record_for_igvn(phi);\n+    map->set_req(idx, phi);\n+    return phi;\n+  }\n@@ -2191,0 +2346,35 @@\n+  \/\/ frame pointer is always same, already captured\n+  if (value != nullptr) {\n+    Node* phi = _exits.argument(0);\n+    const Type* return_type = phi->bottom_type();\n+    const TypeInstPtr* tr = return_type->isa_instptr();\n+    assert(!value->is_InlineType() || !value->as_InlineType()->is_larval(), \"returning a larval\");\n+    if ((tf()->returns_inline_type_as_fields() || (_caller->has_method() && !Compile::current()->inlining_incrementally())) &&\n+        return_type->is_inlinetypeptr()) {\n+      \/\/ Inline type is returned as fields, make sure it is scalarized\n+      if (!value->is_InlineType()) {\n+        value = InlineTypeNode::make_from_oop(this, value, return_type->inline_klass(), false);\n+      }\n+      if (!_caller->has_method() || Compile::current()->inlining_incrementally()) {\n+        \/\/ Returning from root or an incrementally inlined method. Make sure all non-flat\n+        \/\/ fields are buffered and re-execute if allocation triggers deoptimization.\n+        PreserveReexecuteState preexecs(this);\n+        assert(tf()->returns_inline_type_as_fields(), \"must be returned as fields\");\n+        jvms()->set_should_reexecute(true);\n+        inc_sp(1);\n+        value = value->as_InlineType()->allocate_fields(this);\n+      }\n+    } else if (value->is_InlineType()) {\n+      \/\/ Inline type is returned as oop, make sure it is buffered and re-execute\n+      \/\/ if allocation triggers deoptimization.\n+      PreserveReexecuteState preexecs(this);\n+      jvms()->set_should_reexecute(true);\n+      inc_sp(1);\n+      value = value->as_InlineType()->buffer(this);\n+    }\n+    \/\/ ...else\n+    \/\/ If returning oops to an interface-return, there is a silent free\n+    \/\/ cast from oop to interface allowed by the Verifier. Make it explicit here.\n+    phi->add_req(value);\n+  }\n+\n@@ -2199,0 +2389,1 @@\n+\n@@ -2216,9 +2407,0 @@\n-  \/\/ frame pointer is always same, already captured\n-  if (value != nullptr) {\n-    \/\/ If returning oops to an interface-return, there is a silent free\n-    \/\/ cast from oop to interface allowed by the Verifier.  Make it explicit\n-    \/\/ here.\n-    Node* phi = _exits.argument(0);\n-    phi->add_req(value);\n-  }\n-\n","filename":"src\/hotspot\/share\/opto\/parse1.cpp","additions":239,"deletions":57,"binary":false,"changes":296,"status":"modified"},{"patch":"@@ -58,0 +58,2 @@\n+#include \"oops\/flatArrayOop.inline.hpp\"\n+#include \"oops\/inlineKlass.inline.hpp\"\n@@ -429,0 +431,1 @@\n+  bool is_flat = InstanceKlass::cast(k1)->field_is_flat(slot);\n@@ -430,1 +433,1 @@\n-  ret = jfieldIDWorkaround::to_instance_jfieldID(k1, offset);\n+  ret = jfieldIDWorkaround::to_instance_jfieldID(k1, offset, is_flat);\n@@ -447,1 +450,1 @@\n-  if (m->is_initializer()) {\n+  if (m->is_object_constructor()) {\n@@ -805,1 +808,1 @@\n-    case T_OBJECT:      push_object(va_arg(_ap, jobject)); break;\n+    case T_OBJECT: push_object(va_arg(_ap, jobject)); break;\n@@ -845,1 +848,2 @@\n-    case T_OBJECT:      push_object((_ap++)->l); break;\n+    case T_OBJECT:\n+    case T_PRIMITIVE_OBJECT: push_object((_ap++)->l); break;\n@@ -972,1 +976,7 @@\n-  instanceOop i = InstanceKlass::allocate_instance(JNIHandles::resolve_non_null(clazz), CHECK_NULL);\n+  oop clazzoop = JNIHandles::resolve_non_null(clazz);\n+  Klass* k = java_lang_Class::as_Klass(clazzoop);\n+  if (k == nullptr || k->is_inline_klass()) {\n+    ResourceMark rm(THREAD);\n+    THROW_(vmSymbols::java_lang_InstantiationException(), nullptr);\n+  }\n+  instanceOop i = InstanceKlass::allocate_instance(clazzoop, CHECK_NULL);\n@@ -986,1 +996,8 @@\n-  instanceOop i = InstanceKlass::allocate_instance(JNIHandles::resolve_non_null(clazz), CHECK_NULL);\n+  oop clazzoop = JNIHandles::resolve_non_null(clazz);\n+  Klass* k = java_lang_Class::as_Klass(clazzoop);\n+  if (k == nullptr) {\n+    ResourceMark rm(THREAD);\n+    THROW_(vmSymbols::java_lang_InstantiationException(), nullptr);\n+  }\n+\n+  instanceOop i = InstanceKlass::allocate_instance(clazzoop, CHECK_NULL);\n@@ -991,0 +1008,1 @@\n+\n@@ -992,1 +1010,1 @@\n-JNI_END\n+  JNI_END\n@@ -1004,1 +1022,8 @@\n-  instanceOop i = InstanceKlass::allocate_instance(JNIHandles::resolve_non_null(clazz), CHECK_NULL);\n+  oop clazzoop = JNIHandles::resolve_non_null(clazz);\n+  Klass* k = java_lang_Class::as_Klass(clazzoop);\n+  if (k == nullptr) {\n+    ResourceMark rm(THREAD);\n+    THROW_(vmSymbols::java_lang_InstantiationException(), nullptr);\n+  }\n+\n+  instanceOop i = InstanceKlass::allocate_instance(clazzoop, CHECK_NULL);\n@@ -1009,0 +1034,1 @@\n+\n@@ -1022,1 +1048,8 @@\n-  instanceOop i = InstanceKlass::allocate_instance(JNIHandles::resolve_non_null(clazz), CHECK_NULL);\n+  oop clazzoop = JNIHandles::resolve_non_null(clazz);\n+  Klass* k = java_lang_Class::as_Klass(clazzoop);\n+  if (k == nullptr) {\n+    ResourceMark rm(THREAD);\n+    THROW_(vmSymbols::java_lang_InstantiationException(), nullptr);\n+  }\n+\n+  instanceOop i = InstanceKlass::allocate_instance(clazzoop, CHECK_NULL);\n@@ -1030,0 +1063,1 @@\n+\n@@ -1780,1 +1814,1 @@\n-  ret = jfieldIDWorkaround::to_instance_jfieldID(k, fd.offset());\n+  ret = jfieldIDWorkaround::to_instance_jfieldID(k, fd.offset(), fd.is_flat());\n@@ -1790,0 +1824,1 @@\n+  oop res = nullptr;\n@@ -1795,2 +1830,12 @@\n-  oop loaded_obj = HeapAccess<ON_UNKNOWN_OOP_REF>::oop_load_at(o, offset);\n-  jobject ret = JNIHandles::make_local(THREAD, loaded_obj);\n+  if (!jfieldIDWorkaround::is_flat_jfieldID(fieldID)) {\n+    res = HeapAccess<ON_UNKNOWN_OOP_REF>::oop_load_at(o, offset);\n+  } else {\n+    assert(k->is_instance_klass(), \"Only instance can have flat fields\");\n+    InstanceKlass* ik = InstanceKlass::cast(k);\n+    fieldDescriptor fd;\n+    ik->find_field_from_offset(offset, false, &fd);  \/\/ performance bottleneck\n+    InstanceKlass* holder = fd.field_holder();\n+    InlineKlass* field_vklass = InlineKlass::cast(holder->get_inline_type_field_klass(fd.index()));\n+    res = field_vklass->read_flat_field(o, ik->field_offset(fd.index()), CHECK_NULL);\n+  }\n+  jobject ret = JNIHandles::make_local(THREAD, res);\n@@ -1888,1 +1933,12 @@\n-  HeapAccess<ON_UNKNOWN_OOP_REF>::oop_store_at(o, offset, JNIHandles::resolve(value));\n+  if (!jfieldIDWorkaround::is_flat_jfieldID(fieldID)) {\n+    HeapAccess<ON_UNKNOWN_OOP_REF>::oop_store_at(o, offset, JNIHandles::resolve(value));\n+  } else {\n+    assert(k->is_instance_klass(), \"Only instances can have flat fields\");\n+    InstanceKlass* ik = InstanceKlass::cast(k);\n+    fieldDescriptor fd;\n+    ik->find_field_from_offset(offset, false, &fd);\n+    InstanceKlass* holder = fd.field_holder();\n+    InlineKlass* vklass = InlineKlass::cast(holder->get_inline_type_field_klass(fd.index()));\n+    oop v = JNIHandles::resolve_non_null(value);\n+    vklass->write_flat_field(o, offset, v, CHECK);\n+  }\n@@ -2313,4 +2369,13 @@\n-  objArrayOop a = objArrayOop(JNIHandles::resolve_non_null(array));\n-  if (a->is_within_bounds(index)) {\n-    ret = JNIHandles::make_local(THREAD, a->obj_at(index));\n-    return ret;\n+  oop res = nullptr;\n+  arrayOop arr((arrayOop)JNIHandles::resolve_non_null(array));\n+  if (arr->is_within_bounds(index)) {\n+    if (arr->is_flatArray()) {\n+      flatArrayOop a = flatArrayOop(JNIHandles::resolve_non_null(array));\n+      flatArrayHandle vah(thread, a);\n+      res = flatArrayOopDesc::value_alloc_copy_from_index(vah, index, CHECK_NULL);\n+      assert(res != nullptr, \"Must be set in one of two paths above\");\n+    } else {\n+      assert(arr->is_objArray(), \"If not a valueArray. must be an objArray\");\n+      objArrayOop a = objArrayOop(JNIHandles::resolve_non_null(array));\n+      res = a->obj_at(index);\n+    }\n@@ -2320,1 +2385,1 @@\n-    ss.print(\"Index %d out of bounds for length %d\", index, a->length());\n+    ss.print(\"Index %d out of bounds for length %d\", index,arr->length());\n@@ -2323,0 +2388,2 @@\n+  ret = JNIHandles::make_local(THREAD, res);\n+  return ret;\n@@ -2332,24 +2399,51 @@\n-  objArrayOop a = objArrayOop(JNIHandles::resolve_non_null(array));\n-  oop v = JNIHandles::resolve(value);\n-  if (a->is_within_bounds(index)) {\n-    if (v == nullptr || v->is_a(ObjArrayKlass::cast(a->klass())->element_klass())) {\n-      a->obj_at_put(index, v);\n-    } else {\n-      ResourceMark rm(THREAD);\n-      stringStream ss;\n-      Klass *bottom_kl = ObjArrayKlass::cast(a->klass())->bottom_klass();\n-      ss.print(\"type mismatch: can not store %s to %s[%d]\",\n-               v->klass()->external_name(),\n-               bottom_kl->is_typeArray_klass() ? type2name_tab[ArrayKlass::cast(bottom_kl)->element_type()] : bottom_kl->external_name(),\n-               index);\n-      for (int dims = ArrayKlass::cast(a->klass())->dimension(); dims > 1; --dims) {\n-        ss.print(\"[]\");\n-      }\n-      THROW_MSG(vmSymbols::java_lang_ArrayStoreException(), ss.as_string());\n-    }\n-  } else {\n-    ResourceMark rm(THREAD);\n-    stringStream ss;\n-    ss.print(\"Index %d out of bounds for length %d\", index, a->length());\n-    THROW_MSG(vmSymbols::java_lang_ArrayIndexOutOfBoundsException(), ss.as_string());\n-  }\n+   bool oob = false;\n+   int length = -1;\n+   oop res = nullptr;\n+   arrayOop arr((arrayOop)JNIHandles::resolve_non_null(array));\n+   if (arr->is_within_bounds(index)) {\n+     if (arr->is_flatArray()) {\n+       flatArrayOop a = flatArrayOop(JNIHandles::resolve_non_null(array));\n+       oop v = JNIHandles::resolve(value);\n+       FlatArrayKlass* vaklass = FlatArrayKlass::cast(a->klass());\n+       InlineKlass* element_vklass = vaklass->element_klass();\n+       if (v != nullptr && v->is_a(element_vklass)) {\n+         a->value_copy_to_index(v, index);\n+       } else {\n+         ResourceMark rm(THREAD);\n+         stringStream ss;\n+         Klass *kl = FlatArrayKlass::cast(a->klass());\n+         ss.print(\"type mismatch: can not store %s to %s[%d]\",\n+             v->klass()->external_name(),\n+             kl->external_name(),\n+             index);\n+         for (int dims = ArrayKlass::cast(a->klass())->dimension(); dims > 1; --dims) {\n+           ss.print(\"[]\");\n+         }\n+         THROW_MSG(vmSymbols::java_lang_ArrayStoreException(), ss.as_string());\n+       }\n+     } else {\n+       assert(arr->is_objArray(), \"If not a valueArray. must be an objArray\");\n+       objArrayOop a = objArrayOop(JNIHandles::resolve_non_null(array));\n+       oop v = JNIHandles::resolve(value);\n+       if (v == nullptr || v->is_a(ObjArrayKlass::cast(a->klass())->element_klass())) {\n+         a->obj_at_put(index, v);\n+       } else {\n+         ResourceMark rm(THREAD);\n+         stringStream ss;\n+         Klass *bottom_kl = ObjArrayKlass::cast(a->klass())->bottom_klass();\n+         ss.print(\"type mismatch: can not store %s to %s[%d]\",\n+             v->klass()->external_name(),\n+             bottom_kl->is_typeArray_klass() ? type2name_tab[ArrayKlass::cast(bottom_kl)->element_type()] : bottom_kl->external_name(),\n+                 index);\n+         for (int dims = ArrayKlass::cast(a->klass())->dimension(); dims > 1; --dims) {\n+           ss.print(\"[]\");\n+         }\n+         THROW_MSG(vmSymbols::java_lang_ArrayStoreException(), ss.as_string());\n+       }\n+     }\n+   } else {\n+     ResourceMark rm(THREAD);\n+     stringStream ss;\n+     ss.print(\"Index %d out of bounds for length %d\", index, arr->length());\n+     THROW_MSG(vmSymbols::java_lang_ArrayIndexOutOfBoundsException(), ss.as_string());\n+   }\n","filename":"src\/hotspot\/share\/prims\/jni.cpp","additions":136,"deletions":42,"binary":false,"changes":178,"status":"modified"},{"patch":"@@ -259,1 +259,1 @@\n-      !(fd.field_type() == T_ARRAY && ftype == T_OBJECT)) {\n+      !(fd.field_type() == T_ARRAY && ftype == T_OBJECT))  {\n@@ -347,1 +347,1 @@\n-check_is_obj_array(JavaThread* thr, jarray jArray) {\n+check_is_obj_or_inline_array(JavaThread* thr, jarray jArray) {\n@@ -349,1 +349,1 @@\n-  if (!aOop->is_objArray()) {\n+  if (!aOop->is_objArray() && !aOop->is_flatArray()) {\n@@ -1630,1 +1630,1 @@\n-      check_is_obj_array(thr, array);\n+      check_is_obj_or_inline_array(thr, array);\n@@ -1644,1 +1644,1 @@\n-      check_is_obj_array(thr, array);\n+      check_is_obj_or_inline_array(thr, array);\n","filename":"src\/hotspot\/share\/prims\/jniCheck.cpp","additions":5,"deletions":5,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -60,0 +60,1 @@\n+#include \"oops\/flatArrayKlass.hpp\"\n@@ -430,0 +431,23 @@\n+JVM_ENTRY(jarray, JVM_NewNullRestrictedArray(JNIEnv *env, jclass elmClass, jint len))\n+  if (len < 0) {\n+    THROW_MSG_NULL(vmSymbols::java_lang_IllegalArgumentException(), \"Array length is negative\");\n+  }\n+  oop mirror = JNIHandles::resolve_non_null(elmClass);\n+  Klass* klass = java_lang_Class::as_Klass(mirror);\n+  klass->initialize(CHECK_NULL);\n+  if (klass->is_identity_class()) {\n+    THROW_MSG_NULL(vmSymbols::java_lang_IllegalArgumentException(), \"Element class is not a value class\");\n+  }\n+  InstanceKlass* ik = InstanceKlass::cast(klass);\n+  if (!ik->is_implicitly_constructible()) {\n+    THROW_MSG_NULL(vmSymbols::java_lang_IllegalArgumentException(), \"Element class is not implicitly constructible\");\n+  }\n+  oop array = oopFactory::new_valueArray(ik, len, CHECK_NULL);\n+  return (jarray) JNIHandles::make_local(THREAD, array);\n+JVM_END\n+\n+\n+JVM_ENTRY(jboolean, JVM_IsNullRestrictedArray(JNIEnv *env, jobject obj))\n+  arrayOop oop = arrayOop(JNIHandles::resolve_non_null(obj));\n+  return oop->is_null_free_array();\n+JVM_END\n@@ -638,2 +662,22 @@\n-  return handle == nullptr ? 0 :\n-         checked_cast<jint>(ObjectSynchronizer::FastHashCode (THREAD, JNIHandles::resolve_non_null(handle)));\n+  if (handle == nullptr) {\n+    return 0;\n+  }\n+  oop obj = JNIHandles::resolve_non_null(handle);\n+  if (EnableValhalla && obj->klass()->is_inline_klass()) {\n+      JavaValue result(T_INT);\n+      JavaCallArguments args;\n+      Handle ho(THREAD, obj);\n+      args.push_oop(ho);\n+      methodHandle method(THREAD, Universe::value_object_hash_code_method());\n+      JavaCalls::call(&result, method, &args, THREAD);\n+      if (HAS_PENDING_EXCEPTION) {\n+        if (!PENDING_EXCEPTION->is_a(vmClasses::Error_klass())) {\n+          Handle e(THREAD, PENDING_EXCEPTION);\n+          CLEAR_PENDING_EXCEPTION;\n+          THROW_MSG_CAUSE_(vmSymbols::java_lang_InternalError(), \"Internal error in hashCode\", e, false);\n+        }\n+      }\n+      return result.get_jint();\n+  } else {\n+    return checked_cast<jint>(ObjectSynchronizer::FastHashCode(THREAD, obj));\n+  }\n@@ -697,0 +741,6 @@\n+  if (klass->is_inline_klass()) {\n+    \/\/ Value instances have no identity, so return the current instance instead of allocating a new one\n+    \/\/ Value classes cannot have finalizers, so the method can return immediately\n+    return JNIHandles::make_local(THREAD, obj());\n+  }\n+\n@@ -1207,1 +1257,2 @@\n-    size = InstanceKlass::cast(klass)->local_interfaces()->length();\n+    InstanceKlass* ik = InstanceKlass::cast(klass);\n+    size = ik->local_interfaces()->length();\n@@ -1209,1 +1260,1 @@\n-    assert(klass->is_objArray_klass() || klass->is_typeArray_klass(), \"Illegal mirror klass\");\n+    assert(klass->is_objArray_klass() || klass->is_typeArray_klass() || klass->is_flatArray_klass(), \"Illegal mirror klass\");\n@@ -1220,1 +1271,2 @@\n-      Klass* k = InstanceKlass::cast(klass)->local_interfaces()->at(index);\n+      InstanceKlass* ik = InstanceKlass::cast(klass);\n+      Klass* k = ik->local_interfaces()->at(index);\n@@ -1255,0 +1307,19 @@\n+JVM_ENTRY(jboolean, JVM_IsIdentityClass(JNIEnv *env, jclass cls))\n+  oop mirror = JNIHandles::resolve_non_null(cls);\n+  if (java_lang_Class::is_primitive(mirror)) {\n+    return JNI_FALSE;\n+  }\n+  Klass* k = java_lang_Class::as_Klass(mirror);\n+  if (EnableValhalla) {\n+    return k->is_array_klass() || k->is_identity_class();\n+  } else {\n+    return k->is_interface() ? JNI_FALSE : JNI_TRUE;\n+  }\n+JVM_END\n+\n+JVM_ENTRY(jboolean, JVM_IsImplicitlyConstructibleClass(JNIEnv *env, jclass cls))\n+  oop mirror = JNIHandles::resolve_non_null(cls);\n+  InstanceKlass* ik = InstanceKlass::cast(java_lang_Class::as_Klass(mirror));\n+  return ik->is_implicitly_constructible();\n+JVM_END\n+\n@@ -1830,0 +1901,1 @@\n+  bool is_ctor = (method->is_object_constructor());\n@@ -1831,1 +1903,1 @@\n-    return (method->is_initializer() && !method->is_static());\n+    return is_ctor;\n@@ -1833,1 +1905,3 @@\n-    return  (!method->is_initializer() && !method->is_overpass());\n+    return (!is_ctor &&\n+            !method->is_class_initializer() &&\n+            !method->is_overpass());\n@@ -1896,0 +1970,1 @@\n+        assert(method->is_object_constructor(), \"must be\");\n@@ -2178,3 +2253,1 @@\n-  if (!m->is_initializer() || m->is_static()) {\n-    method = Reflection::new_method(m, true, CHECK_NULL);\n-  } else {\n+  if (m->is_object_constructor()) {\n@@ -2182,0 +2255,2 @@\n+  } else {\n+    method = Reflection::new_method(m, true, CHECK_NULL);\n@@ -2452,0 +2527,37 @@\n+\/\/ Arrays support \/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n+\n+JVM_ENTRY(jboolean, JVM_ArrayIsAccessAtomic(JNIEnv *env, jclass unused, jobject array))\n+  oop o = JNIHandles::resolve(array);\n+  Klass* k = o->klass();\n+  if ((o == nullptr) || (!k->is_array_klass())) {\n+    THROW_0(vmSymbols::java_lang_IllegalArgumentException());\n+  }\n+  return ArrayKlass::cast(k)->element_access_is_atomic();\n+JVM_END\n+\n+JVM_ENTRY(jobject, JVM_ArrayEnsureAccessAtomic(JNIEnv *env, jclass unused, jobject array))\n+  oop o = JNIHandles::resolve(array);\n+  Klass* k = o->klass();\n+  if ((o == nullptr) || (!k->is_array_klass())) {\n+    THROW_0(vmSymbols::java_lang_IllegalArgumentException());\n+  }\n+  if (k->is_flatArray_klass()) {\n+    FlatArrayKlass* vk = FlatArrayKlass::cast(k);\n+    if (!vk->element_access_is_atomic()) {\n+      \/**\n+       * Need to decide how to implement:\n+       *\n+       * 1) Change to objArrayOop layout, therefore oop->klass() differs so\n+       * then \"<atomic>[Qfoo;\" klass needs to subclass \"[Qfoo;\" to pass through\n+       * \"checkcast\" & \"instanceof\"\n+       *\n+       * 2) Use extra header in the flatArrayOop to flag atomicity required and\n+       * possibly per instance lock structure. Said info, could be placed in\n+       * \"trailer\" rather than disturb the current arrayOop\n+       *\/\n+      Unimplemented();\n+    }\n+  }\n+  return array;\n+JVM_END\n+\n@@ -2581,1 +2693,1 @@\n-  return InstanceKlass::cast(k)->field_access_flags(field_index) & JVM_RECOGNIZED_FIELD_MODIFIERS;\n+  return InstanceKlass::cast(k)->field_access_flags(field_index);\n@@ -2614,1 +2726,1 @@\n-  return method->name() == vmSymbols::object_initializer_name();\n+  return method->is_object_constructor();\n@@ -2771,1 +2883,1 @@\n-          return fs.access_flags().as_short() & JVM_RECOGNIZED_FIELD_MODIFIERS;\n+          return fs.access_flags().as_short();\n@@ -3443,0 +3555,4 @@\n+JVM_LEAF(jboolean, JVM_IsValhallaEnabled(void))\n+  return EnableValhalla ? JNI_TRUE : JNI_FALSE;\n+JVM_END\n+\n@@ -3528,1 +3644,1 @@\n-    objArrayHandle args(THREAD, objArrayOop(JNIHandles::resolve(args0)));\n+    objArrayHandle args = oopFactory::ensure_objArray(JNIHandles::resolve(args0), CHECK_NULL);\n@@ -3548,0 +3664,1 @@\n+  objArrayHandle args = oopFactory::ensure_objArray(JNIHandles::resolve(args0), CHECK_NULL);\n@@ -3549,1 +3666,0 @@\n-  objArrayHandle args(THREAD, objArrayOop(JNIHandles::resolve(args0)));\n","filename":"src\/hotspot\/share\/prims\/jvm.cpp","additions":131,"deletions":15,"binary":false,"changes":146,"status":"modified"},{"patch":"@@ -2698,3 +2698,3 @@\n-    if (k->is_super()) {\n-      result |= JVM_ACC_SUPER;\n-    }\n+    \/\/ if (k->is_super()) {\n+    \/\/   result |= JVM_ACC_SUPER;\n+    \/\/ }\n@@ -2831,1 +2831,2 @@\n-                                                     flds.access_flags().is_static());\n+                                                     flds.access_flags().is_static(),\n+                                                     flds.field_descriptor().is_flat());\n@@ -2869,2 +2870,3 @@\n-    Array<InstanceKlass*>* interface_list = InstanceKlass::cast(k)->local_interfaces();\n-    const int result_length = (interface_list == nullptr ? 0 : interface_list->length());\n+    InstanceKlass* ik = InstanceKlass::cast(k);\n+    Array<InstanceKlass*>* interface_list = ik->local_interfaces();\n+    int result_length = (interface_list == nullptr ? 0 : interface_list->length());\n","filename":"src\/hotspot\/share\/prims\/jvmtiEnv.cpp","additions":8,"deletions":6,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -398,0 +398,1 @@\n+  initialize_class(vmSymbols::java_lang_IdentityException(), CHECK);\n","filename":"src\/hotspot\/share\/runtime\/threads.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -71,0 +71,2 @@\n+#include \"oops\/flatArrayKlass.hpp\"\n+#include \"oops\/inlineKlass.hpp\"\n@@ -233,1 +235,1 @@\n-  volatile_nonstatic_field(InstanceKlass,      _array_klasses,                                ObjArrayKlass*)                        \\\n+  volatile_nonstatic_field(InstanceKlass,      _array_klasses,                                ArrayKlass*)                        \\\n@@ -1158,0 +1160,1 @@\n+           declare_type(FlatArrayKlass, ArrayKlass)                       \\\n@@ -1161,0 +1164,1 @@\n+        declare_type(InlineKlass, InstanceKlass)                          \\\n@@ -1539,0 +1543,1 @@\n+  declare_c2_type(MachVEPNode, MachIdealNode)                             \\\n","filename":"src\/hotspot\/share\/runtime\/vmStructs.cpp","additions":6,"deletions":1,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -62,0 +62,1 @@\n+  bool is_strict      () const         { return (_flags & JVM_ACC_STRICT      ) != 0; }\n@@ -63,1 +64,0 @@\n-  bool is_super       () const         { return (_flags & JVM_ACC_SUPER       ) != 0; }\n@@ -69,0 +69,1 @@\n+  bool is_identity_class  () const     { return (_flags & JVM_ACC_IDENTITY    ) != 0; }\n","filename":"src\/hotspot\/share\/utilities\/accessFlags.hpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -42,0 +42,1 @@\n+import java.lang.reflect.ClassFileFormatVersion;\n@@ -227,3 +228,3 @@\n-    private static final int ANNOTATION= 0x00002000;\n-    private static final int ENUM      = 0x00004000;\n-    private static final int SYNTHETIC = 0x00001000;\n+    private static final int ANNOTATION = 0x00002000;\n+    private static final int ENUM       = 0x00004000;\n+    private static final int SYNTHETIC  = 0x00001000;\n@@ -316,0 +317,2 @@\n+                \/\/ Modifier.toString() below mis-interprets SYNCHRONIZED, STRICT, and VOLATILE bits\n+                modifiers &= ~(Modifier.SYNCHRONIZED | Modifier.STRICT | Modifier.VOLATILE);\n@@ -329,0 +332,3 @@\n+                if (isValue()) {\n+                    sb.append(\"value \");\n+                }\n@@ -678,0 +684,33 @@\n+    \/**\n+     * {@return {@code true} if this {@code Class} object represents an identity\n+     * class or interface; otherwise {@code false}}\n+     *\n+     * If this {@code Class} object represents an array type, then this method\n+     * returns {@code true}.\n+     * If this {@code Class} object represents a primitive type, or {@code void},\n+     * then this method returns {@code false}.\n+     *\n+     * @since Valhalla\n+     *\/\n+    @PreviewFeature(feature = PreviewFeature.Feature.VALUE_OBJECTS, reflective=true)\n+    public native boolean isIdentity();\n+\n+    \/**\n+     * {@return {@code true} if this {@code Class} object represents a value\n+     * class; otherwise {@code false}}\n+     *\n+     * If this {@code Class} object represents an array type, an interface,\n+     * a primitive type, or {@code void}, then this method returns {@code false}.\n+     *\n+     * @since Valhalla\n+     *\/\n+    @PreviewFeature(feature = PreviewFeature.Feature.VALUE_OBJECTS, reflective=true)\n+    public boolean isValue() {\n+        if (!PreviewFeatures.isEnabled()) {\n+            return false;\n+        }\n+         if (isPrimitive() || isArray() || isInterface())\n+             return false;\n+        return ((getModifiers() & Modifier.IDENTITY) == 0);\n+    }\n+\n@@ -1444,0 +1483,1 @@\n+     * <li> its {@code identity} modifier is always true\n@@ -1469,1 +1509,1 @@\n-    \/**\n+   \/**\n@@ -1472,0 +1512,1 @@\n+     * The {@code AccessFlags} may depend on the class file format version of the class.\n@@ -1480,0 +1521,1 @@\n+    * <li> its {@code identity} modifier is always true\n@@ -1501,4 +1543,11 @@\n-        return AccessFlag.maskToAccessFlags((location == AccessFlag.Location.CLASS) ?\n-                                            getClassAccessFlagsRaw() :\n-                                            getModifiers(),\n-                                            location);\n+        int accessFlags = (location == AccessFlag.Location.CLASS) ?\n+                getClassAccessFlagsRaw() : getModifiers();\n+        if (isArray() && PreviewFeatures.isEnabled()) {\n+            accessFlags |= Modifier.IDENTITY;\n+        }\n+        var cffv = ClassFileFormatVersion.fromMajor(getClassFileVersion() & 0xffff);\n+        if (cffv.compareTo(ClassFileFormatVersion.latest()) >= 0) {\n+            \/\/ Ignore unspecified (0x0800) access flag for current version\n+            accessFlags &= ~0x0800;\n+        }\n+        return AccessFlag.maskToAccessFlags(accessFlags, location, cffv);\n@@ -1507,1 +1556,1 @@\n-    \/**\n+   \/**\n@@ -1515,0 +1564,1 @@\n+\n@@ -4441,1 +4491,1 @@\n-         return TypeAnnotationParser.buildAnnotatedInterfaces(getRawTypeAnnotations(), getConstantPool(), this);\n+        return TypeAnnotationParser.buildAnnotatedInterfaces(getRawTypeAnnotations(), getConstantPool(), this);\n@@ -4839,1 +4889,2 @@\n-    private int getClassFileVersion() {\n+    \/* package-private *\/\n+    int getClassFileVersion() {\n","filename":"src\/java.base\/share\/classes\/java\/lang\/Class.java","additions":62,"deletions":11,"binary":false,"changes":73,"status":"modified"},{"patch":"@@ -43,0 +43,1 @@\n+import java.lang.reflect.ClassFileFormatVersion;\n@@ -2767,0 +2768,4 @@\n+            public int classFileFormatVersion(Class<?> clazz) {\n+                return clazz.getClassFileVersion();\n+            }\n+\n","filename":"src\/java.base\/share\/classes\/java\/lang\/System.java","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -270,0 +270,4 @@\n+        GET_VALUE(\"getValue\"),\n+        PUT_VALUE(\"putValue\"),\n+        GET_VALUE_VOLATILE(\"getValueVolatile\"),\n+        PUT_VALUE_VOLATILE(\"putValueVolatile\"),\n","filename":"src\/java.base\/share\/classes\/java\/lang\/invoke\/LambdaForm.java","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -35,0 +35,1 @@\n+import java.lang.reflect.ClassFileFormatVersion;\n@@ -594,0 +595,6 @@\n+\n+    \/**\n+     * Returns the class file format version of the class.\n+     *\/\n+    int classFileFormatVersion(Class<?> klass);\n+\n","filename":"src\/java.base\/share\/classes\/jdk\/internal\/access\/JavaLangAccess.java","additions":7,"deletions":0,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -930,0 +930,3 @@\n+                case LoadableDescriptorsAttribute pa ->\n+                    nodes.add(list(\"loadable descriptors\", \"descriptor\", pa.loadableDescriptors().stream()\n+                            .map(e -> e.stringValue())));\n","filename":"src\/java.base\/share\/classes\/jdk\/internal\/classfile\/impl\/ClassPrinterImpl.java","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -55,0 +55,1 @@\n+import java.lang.classfile.attribute.LoadableDescriptorsAttribute;\n@@ -143,0 +144,2 @@\n+            case LoadableDescriptorsAttribute pa ->\n+                clb.with(LoadableDescriptorsAttribute.of(pa.loadableDescriptors()));\n","filename":"src\/java.base\/share\/classes\/jdk\/internal\/classfile\/impl\/ClassRemapperImpl.java","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -263,0 +263,2 @@\n+            case LoadableDescriptorsAttribute lda ->\n+                2 + 2 * lda.loadableDescriptors().size();\n","filename":"src\/java.base\/share\/classes\/jdk\/internal\/classfile\/impl\/verifier\/ParserVerifier.java","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -40,0 +40,1 @@\n+import com.sun.tools.javac.code.Flags.Flag;\n@@ -43,0 +44,1 @@\n+import com.sun.tools.javac.parser.Tokens.Comment.CommentStyle;\n@@ -54,0 +56,1 @@\n+import static com.sun.tools.javac.code.Flags.asFlagSet;\n@@ -62,0 +65,1 @@\n+import static com.sun.tools.javac.parser.Tokens.TokenKind.SYNCHRONIZED;\n@@ -202,0 +206,2 @@\n+        this.allowValueClasses = (!preview.isPreview(Feature.VALUE_CLASSES) || preview.isEnabled()) &&\n+                Feature.VALUE_CLASSES.allowedInSource(source);\n@@ -226,0 +232,2 @@\n+        this.allowValueClasses = (!preview.isPreview(Feature.VALUE_CLASSES) || preview.isEnabled()) &&\n+                Feature.VALUE_CLASSES.allowedInSource(source);\n@@ -263,0 +271,4 @@\n+    \/** Switch: are value classes allowed in this source level?\n+     *\/\n+    boolean allowValueClasses;\n+\n@@ -1652,2 +1664,2 @@\n-                        if (!isMode(TYPE) && isUnboundMemberRef()) {\n-                            \/\/this is an unbound method reference whose qualifier\n+                        if (!isMode(TYPE) && isParameterizedTypePrefix()) {\n+                            \/\/this is either an unbound method reference whose qualifier\n@@ -1895,1 +1907,1 @@\n-    boolean isUnboundMemberRef() {\n+    boolean isParameterizedTypePrefix() {\n@@ -2992,0 +3004,5 @@\n+        if ((isValueModifier()) && allowValueClasses) {\n+            checkSourceLevel(Feature.VALUE_CLASSES);\n+            dc = token.docComment();\n+            return List.of(classOrRecordOrInterfaceOrEnumDeclaration(modifiersOpt(), dc));\n+        }\n@@ -3597,0 +3614,5 @@\n+                if (isValueModifier()) {\n+                    checkSourceLevel(Feature.VALUE_CLASSES);\n+                    flag = Flags.VALUE_CLASS;\n+                    break;\n+                }\n@@ -3865,0 +3887,7 @@\n+        if (name == names.value) {\n+            if (allowValueClasses) {\n+                return Source.JDK23;\n+            } else if (shouldWarn) {\n+                log.warning(pos, Warnings.RestrictedTypeNotAllowedPreview(name, Source.JDK23));\n+            }\n+        }\n@@ -4989,0 +5018,26 @@\n+    protected boolean isValueModifier() {\n+        if (token.kind == IDENTIFIER && token.name() == names.value) {\n+            boolean isValueModifier = false;\n+            Token next = S.token(1);\n+            switch (next.kind) {\n+                case PRIVATE: case PROTECTED: case PUBLIC: case STATIC: case TRANSIENT:\n+                case FINAL: case ABSTRACT: case NATIVE: case VOLATILE: case SYNCHRONIZED:\n+                case STRICTFP: case MONKEYS_AT: case DEFAULT: case BYTE: case SHORT:\n+                case CHAR: case INT: case LONG: case FLOAT: case DOUBLE: case BOOLEAN: case VOID:\n+                case CLASS: case INTERFACE: case ENUM:\n+                    isValueModifier = true;\n+                    break;\n+                case IDENTIFIER: \/\/ value record R || value value || new value Comparable() {} ??\n+                    if (next.name() == names.record || next.name() == names.value\n+                            || (mode & EXPR) != 0)\n+                        isValueModifier = true;\n+                    break;\n+            }\n+            if (isValueModifier) {\n+                checkSourceLevel(Feature.VALUE_CLASSES);\n+                return true;\n+            }\n+        }\n+        return false;\n+    }\n+\n@@ -5016,1 +5071,3 @@\n-                case IDENTIFIER -> isNonSealedIdentifier(next, currentIsNonSealed ? 3 : 1) || next.name() == names.sealed;\n+                case IDENTIFIER -> isNonSealedIdentifier(next, currentIsNonSealed ? 3 : 1) ||\n+                        next.name() == names.sealed ||\n+                        allowValueClasses && next.name() == names.value;\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/parser\/JavacParser.java","additions":61,"deletions":4,"binary":false,"changes":65,"status":"modified"},{"patch":"@@ -779,1 +779,1 @@\n-    improperly formed type, some parameters are missing\n+    improperly formed type, some parameters are missing or misplaced\n@@ -2080,0 +2080,6 @@\n+compiler.warn.serializable.value.class.without.write.replace.1=\\\n+    serializable value class does not declare, or inherits, a writeReplace method\n+\n+compiler.warn.serializable.value.class.without.write.replace.2=\\\n+    serializable class does not declare, or inherits, a writeReplace method\n+\n@@ -2736,0 +2742,3 @@\n+compiler.misc.type.req.identity=\\\n+    a type with identity\n+\n@@ -3765,0 +3774,3 @@\n+compiler.misc.bad.access.flags=\\\n+    bad access flags combination: {0}\n+\n@@ -4098,0 +4110,19 @@\n+compiler.misc.feature.value.classes=\\\n+    value classes\n+\n+# 0: type, 1: type\n+compiler.err.value.type.has.identity.super.type=\\\n+    The identity type {1} cannot be a supertype of the value type {0}\n+\n+# 0: symbol, 1: type\n+compiler.err.concrete.supertype.for.value.class=\\\n+    The concrete class {1} is not allowed to be a super class of the value class {0} either directly or indirectly\n+\n+# 0: symbol, 1: symbol, 2: type\n+compiler.err.super.class.method.cannot.be.synchronized=\\\n+    The method {0} in the super class {2} of the value class {1} is synchronized. This is disallowed\n+\n+# 0: symbol or name\n+compiler.err.cant.ref.after.ctor.called=\\\n+    cannot assign to {0} after supertype constructor has been called\n+\n@@ -4122,0 +4153,3 @@\n+\n+compiler.warn.value.finalize=\\\n+    value classes should not have finalize methods, they are not invoked\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/resources\/compiler.properties","additions":35,"deletions":1,"binary":false,"changes":36,"status":"modified"},{"patch":"@@ -100,1 +100,1 @@\n-    final int instanceKlassMiscFlagsOffset = getFieldOffset(\"InstanceKlass::_misc_flags._flags\", Integer.class, \"u2\");\n+    final int instanceKlassMiscFlagsOffset = getFieldOffset(\"InstanceKlass::_misc_flags._flags\", Integer.class, \"u4\");\n@@ -306,0 +306,3 @@\n+    final int dataLayoutArrayLoadDataTag = getConstant(\"DataLayout::array_load_data_tag\", Integer.class);\n+    final int dataLayoutArrayStoreDataTag = getConstant(\"DataLayout::array_store_data_tag\", Integer.class);\n+    final int dataLayoutACmpDataTag = getConstant(\"DataLayout::acmp_data_tag\", Integer.class);\n","filename":"src\/jdk.internal.vm.ci\/share\/classes\/jdk\/vm\/ci\/hotspot\/HotSpotVMConfig.java","additions":4,"deletions":1,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -81,0 +81,7 @@\n+compiler\/valhalla\/inlinetypes\/TestMethodHandles.java 8341949 generic-all\n+compiler\/valhalla\/inlinetypes\/TestCallingConvention.java   8342064 generic-all\n+compiler\/valhalla\/inlinetypes\/TestIntrinsics.java          8342064 generic-all\n+compiler\/valhalla\/inlinetypes\/TestNullableInlineTypes.java 8342064 generic-all\n+compiler\/valhalla\/inlinetypes\/TestNullableArrays.java      8342064 generic-all\n+\n+compiler\/c2\/irTests\/scalarReplacement\/ScalarReplacementWithGCBarrierTests.java  8342488 generic-all\n@@ -107,0 +114,1 @@\n+runtime\/cds\/appcds\/redefineClass\/RedefineRunningMethods_Shared.java  8304168 generic-all\n@@ -128,0 +136,5 @@\n+\n+# Valhalla\n+runtime\/AccModule\/ConstModule.java 8294051 generic-all\n+runtime\/valhalla\/inlinetypes\/InlineOops.java#ZGen 8313607 linux-aarch64,macosx-aarch64\n+\n@@ -153,0 +166,31 @@\n+# Valhalla TODO:\n+serviceability\/jvmti\/valhalla\/HeapDump\/HeapDump.java 8317416 generic-all\n+\n+serviceability\/sa\/ClhsdbCDSCore.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbCDSJstackPrintAll.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbFindPC.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbInspect.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbLongConstant.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbJdis.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbJstack.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbPrintAll.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbPrintAs.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbPrintStatics.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbSource.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbSymbol.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbWhere.java 8190936 generic-all\n+serviceability\/sa\/JhsdbThreadInfoTest.java 8190936 generic-all\n+serviceability\/sa\/TestClassDump.java 8190936 generic-all\n+serviceability\/sa\/TestClhsdbJstackLock.java 8190936 generic-all\n+serviceability\/sa\/TestCpoolForInvokeDynamic.java 8190936 generic-all\n+serviceability\/sa\/TestHeapDumpForInvokeDynamic.java 8190936 generic-all\n+serviceability\/sa\/TestHeapDumpForLargeArray.java 8190936 generic-all\n+serviceability\/sa\/TestIntConstant.java 8190936 generic-all\n+serviceability\/sa\/TestJhsdbJstackLock.java 8190936 generic-all\n+serviceability\/sa\/TestJmapCore.java 8190936 generic-all\n+serviceability\/sa\/TestJmapCoreMetaspace.java 8190936 generic-all\n+serviceability\/sa\/TestPrintMdo.java 8190936 generic-all\n+serviceability\/sa\/jmap-hprof\/JMapHProfLargeHeapTest.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbDumpclass.java 8190936 generic-all\n+\n+\n@@ -191,0 +235,2 @@\n+vmTestbase\/vm\/mlvm\/hiddenloader\/stress\/byteMutation\/Test.java 8317172 generic-all\n+\n","filename":"test\/hotspot\/jtreg\/ProblemList.txt","additions":46,"deletions":0,"binary":false,"changes":46,"status":"modified"},{"patch":"@@ -279,1 +279,1 @@\n-        String optoRegex = \"(.*precise .*\\\\R((.*(?i:mov|mv|xorl|nop|spill).*|\\\\s*)\\\\R)*.*(?i:call,static).*wrapper for: _new_instance_Java\" + END;\n+        String optoRegex = \"(.*precise .*\\\\R((.*(?i:mov|mv|xorl|nop|spill|pushq|popq).*|\\\\s*)\\\\R)*.*(?i:call,static).*wrapper for: _new_instance_Java\" + END;\n@@ -285,1 +285,1 @@\n-        String regex = \"(.*precise .*\" + IS_REPLACED + \":.*\\\\R((.*(?i:mov|mv|xorl|nop|spill).*|\\\\s*)\\\\R)*.*(?i:call,static).*wrapper for: _new_instance_Java\" + END;\n+        String regex = \"(.*precise .*\" + IS_REPLACED + \":.*\\\\R((.*(?i:mov|mv|xorl|nop|spill|pushq|popq).*|\\\\s*)\\\\R)*.*(?i:call,static).*wrapper for: _new_instance_Java\" + END;\n@@ -291,1 +291,1 @@\n-        String optoRegex = \"(.*precise \\\\[.*\\\\R((.*(?i:mov|mv|xor|nop|spill).*|\\\\s*|.*(LGHI|LI).*)\\\\R)*.*(?i:call,static).*wrapper for: _new_array_Java\" + END;\n+        String optoRegex = \"(.*precise \\\\[.*\\\\R((.*(?i:mov|mv|xor|nop|spill|pushq|popq).*|\\\\s*|.*(LGHI|LI).*)\\\\R)*.*(?i:call,static).*wrapper for: _new_array_Java\" + END;\n@@ -297,1 +297,1 @@\n-        String regex = \"(.*precise \\\\[.*\" + IS_REPLACED + \":.*\\\\R((.*(?i:mov|mv|xorl|nop|spill).*|\\\\s*|.*(LGHI|LI).*)\\\\R)*.*(?i:call,static).*wrapper for: _new_array_Java\" + END;\n+        String regex = \"(.*precise \\\\[.*\" + IS_REPLACED + \":.*\\\\R((.*(?i:mov|mv|xorl|nop|spill|pushq|popq).*|\\\\s*|.*(LGHI|LI).*)\\\\R)*.*(?i:call,static).*wrapper for: _new_array_Java\" + END;\n@@ -574,0 +574,5 @@\n+    public static final String INLINE_TYPE = PREFIX + \"INLINE_TYPE\" + POSTFIX;\n+    static {\n+        beforeMatchingNameRegex(INLINE_TYPE, \"InlineType\");\n+    }\n+\n@@ -2259,1 +2264,1 @@\n-    private static void beforeMatching(String irNodePlaceholder, String regex) {\n+    public static void beforeMatching(String irNodePlaceholder, String regex) {\n@@ -2305,1 +2310,1 @@\n-    private static void optoOnly(String irNodePlaceholder, String regex) {\n+    public static void optoOnly(String irNodePlaceholder, String regex) {\n","filename":"test\/hotspot\/jtreg\/compiler\/lib\/ir_framework\/IRNode.java","additions":11,"deletions":6,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -0,0 +1,504 @@\n+\/*\n+ * Copyright (c) 2017, 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+package runtime.valhalla.inlinetypes;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.io.PrintWriter;\n+import java.lang.constant.ClassDesc;\n+import java.lang.constant.MethodTypeDesc;\n+import java.lang.invoke.*;\n+import java.lang.ref.*;\n+import java.nio.ByteBuffer;\n+import java.time.chrono.ThaiBuddhistChronology;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.concurrent.*;\n+\n+import static jdk.test.lib.Asserts.*;\n+\n+import java.lang.classfile.Label;\n+import java.lang.classfile.TypeKind;\n+import jdk.internal.vm.annotation.ImplicitlyConstructible;\n+import jdk.internal.vm.annotation.LooselyConsistentValue;\n+import jdk.internal.vm.annotation.NullRestricted;\n+import jdk.test.lib.Platform;\n+\n+import javax.tools.*;\n+\n+import test.java.lang.invoke.lib.InstructionHelper;\n+import static test.java.lang.invoke.lib.InstructionHelper.classDesc;\n+\n+\/**\n+ * @test InlineTypesTest\n+ * @summary Test data movement with inline types\n+ * @modules java.base\/jdk.internal.value\n+ * @library \/test\/lib \/test\/jdk\/java\/lang\/invoke\/common\n+ * @modules java.base\/jdk.internal.vm.annotation\n+ * @enablePreview\n+ * @compile InlineTypesTest.java\n+ * @run main\/othervm -Xmx128m -XX:+ExplicitGCInvokesConcurrent\n+ *                   -Xbootclasspath\/a:. -XX:+UnlockDiagnosticVMOptions\n+ *                   runtime.valhalla.inlinetypes.InlineTypesTest\n+ * @run main\/othervm -Xmx128m -XX:+ExplicitGCInvokesConcurrent\n+ *                   -Xbootclasspath\/a:. -XX:+UnlockDiagnosticVMOptions\n+ *                   -XX:ForceNonTearable=*\n+ *                   runtime.valhalla.inlinetypes.InlineTypesTest\n+ *\/\n+\n+ final class ContainerValue1 {\n+    static TestValue1 staticInlineField;\n+    @NullRestricted\n+    TestValue1 nonStaticInlineField;\n+    TestValue1[] valueArray;\n+}\n+\n+@ImplicitlyConstructible\n+@LooselyConsistentValue\n+value class TestValue1 {\n+\n+    static TestValue1 staticValue = getInstance();\n+\n+    final int i;\n+    final String name;\n+\n+    public TestValue1() {\n+        int now =  (int)System.nanoTime();\n+        i = now;\n+        name = Integer.valueOf(now).toString();\n+    }\n+\n+    public TestValue1(int i) {\n+        this.i = i;\n+        name = Integer.valueOf(i).toString();\n+    }\n+\n+    public static TestValue1 getInstance() {\n+        return new TestValue1();\n+    }\n+\n+    public static TestValue1 getNonBufferedInstance() {\n+        return (TestValue1) staticValue;\n+    }\n+\n+    public boolean verify() {\n+        if (name == null) return i == 0;\n+        return Integer.valueOf(i).toString().compareTo(name) == 0;\n+    }\n+}\n+\n+final class ContainerValue2 {\n+    static TestValue2 staticInlineField;\n+    @NullRestricted\n+    TestValue2 nonStaticInlineField;\n+    TestValue2[] valueArray;\n+}\n+\n+@ImplicitlyConstructible\n+@LooselyConsistentValue\n+value class TestValue2 {\n+    static TestValue2 staticValue = getInstance();\n+\n+    final long l;\n+    final double d;\n+    final String s;\n+\n+    public TestValue2() {\n+        long now = System.nanoTime();\n+        l = now;\n+        String stringNow = Long.valueOf(now).toString();\n+        s = stringNow;\n+        d = Double.parseDouble(stringNow);\n+    }\n+\n+    public TestValue2(long l) {\n+        this.l = l;\n+        String txt = Long.valueOf(l).toString();\n+        s = txt;\n+        d = Double.parseDouble(txt);\n+    }\n+\n+    public static TestValue2 getInstance() {\n+        return new TestValue2();\n+    }\n+\n+    public static TestValue2 getNonBufferedInstance() {\n+        return (TestValue2) staticValue;\n+    }\n+\n+    public boolean verify() {\n+        if (s == null) {\n+            return d == 0 && l == 0;\n+        }\n+        return Long.valueOf(l).toString().compareTo(s) == 0\n+                && Double.parseDouble(s) == d;\n+    }\n+}\n+\n+final class ContainerValue3 {\n+    static TestValue3 staticInlineField;\n+    @NullRestricted\n+    TestValue3 nonStaticInlineField;\n+    TestValue3[] valueArray;\n+}\n+\n+@ImplicitlyConstructible\n+@LooselyConsistentValue\n+value class TestValue3 {\n+\n+    static TestValue3 staticValue = getInstance();\n+\n+    final byte b;\n+\n+    public TestValue3() {\n+        b = 123;\n+    }\n+\n+    public TestValue3(byte b) {\n+        this.b = b;\n+    }\n+\n+    public static TestValue3 getInstance() {\n+        return new TestValue3();\n+    }\n+\n+    public static TestValue3 getNonBufferedInstance() {\n+        return (TestValue3) staticValue;\n+    }\n+\n+    public boolean verify() {\n+        return b == 0 || b == 123;\n+    }\n+}\n+\n+final class ContainerValue4 {\n+    static TestValue4 staticInlineField;\n+    @NullRestricted\n+    TestValue4 nonStaticInlineField;\n+    TestValue4[] valueArray;\n+}\n+\n+@ImplicitlyConstructible\n+@LooselyConsistentValue\n+value class TestValue4 {\n+\n+    static TestValue4 staticValue = getInstance();\n+\n+    final byte b1;\n+    final byte b2;\n+    final byte b3;\n+    final byte b4;\n+    final short s1;\n+    final short s2;\n+    final int i;\n+    final long l;\n+    final String val;\n+\n+    public TestValue4() {\n+        this((int) System.nanoTime());\n+    }\n+\n+    public TestValue4(int i) {\n+        this.i = i;\n+        val = Integer.valueOf(i).toString();\n+        ByteBuffer bf = ByteBuffer.allocate(8);\n+        bf.putInt(0, i);\n+        bf.putInt(4, i);\n+        l = bf.getLong(0);\n+        s1 = bf.getShort(2);\n+        s2 = bf.getShort(0);\n+        b1 = bf.get(3);\n+        b2 = bf.get(2);\n+        b3 = bf.get(1);\n+        b4 = bf.get(0);\n+    }\n+\n+    public static TestValue4 getInstance() {\n+        return new TestValue4();\n+    }\n+\n+    public static TestValue4 getNonBufferedInstance() {\n+        return (TestValue4) staticValue;\n+    }\n+\n+    public boolean verify() {\n+        if (val == null) {\n+            return i == 0 && l == 0 && b1 == 0 && b2 == 0 && b3 == 0 && b4 == 0\n+                    && s1 == 0 && s2 == 0;\n+        }\n+        ByteBuffer bf = ByteBuffer.allocate(8);\n+        bf.putInt(0, i);\n+        bf.putInt(4, i);\n+        long nl =  bf.getLong(0);\n+        bf.clear();\n+        bf.putShort(0, s2);\n+        bf.putShort(2, s1);\n+        int from_s = bf.getInt(0);\n+        bf.clear();\n+        bf.put(0, b4);\n+        bf.put(1, b3);\n+        bf.put(2, b2);\n+        bf.put(3, b1);\n+        int from_b = bf.getInt(0);\n+        return l == nl && Integer.valueOf(i).toString().compareTo(val) == 0\n+                && from_s == i && from_b == i;\n+    }\n+}\n+\n+public class InlineTypesTest {\n+\n+    public static void main(String[] args) {\n+        Class<?> inlineClass = runtime.valhalla.inlinetypes.TestValue1.class;\n+        Class<?> testClasses[] = {\n+                runtime.valhalla.inlinetypes.TestValue1.class,\n+                runtime.valhalla.inlinetypes.TestValue2.class,\n+                runtime.valhalla.inlinetypes.TestValue3.class,\n+                runtime.valhalla.inlinetypes.TestValue4.class\n+        };\n+        Class<?> containerClasses[] = {\n+                runtime.valhalla.inlinetypes.ContainerValue1.class,\n+                runtime.valhalla.inlinetypes.ContainerValue2.class,\n+                runtime.valhalla.inlinetypes.ContainerValue3.class,\n+                runtime.valhalla.inlinetypes.ContainerValue4.class\n+        };\n+\n+        for (int i = 0; i < testClasses.length; i++) {\n+            try {\n+                testExecutionStackToLocalVariable(testClasses[i]);\n+                testExecutionStackToFields(testClasses[i], containerClasses[i]);\n+                testExecutionStackToInlineArray(testClasses[i], containerClasses[i]);\n+            } catch (Throwable t) {\n+                t.printStackTrace();\n+                throw new RuntimeException(t);\n+            }\n+        }\n+    }\n+\n+    static MethodHandles.Lookup LOOKUP = MethodHandles.lookup();\n+\n+    static void testExecutionStackToLocalVariable(Class<?> inlineClass) throws Throwable {\n+        String sig = \"()L\" + inlineClass.getName().replace('.', '\/') + \";\";\n+        final MethodTypeDesc voidReturnClass = MethodTypeDesc.ofDescriptor(sig);\n+        final ClassDesc systemClassDesc = classDesc(System.class);\n+        final ClassDesc inlineClassDesc = classDesc(inlineClass);\n+        MethodHandle fromExecStackToLocalVar = InstructionHelper.buildMethodHandle(\n+                LOOKUP,\n+                \"execStackToLocalVar\",\n+                MethodType.methodType(boolean.class),\n+                CODE -> {\n+                    CODE.invokestatic(systemClassDesc, \"gc\", MethodTypeDesc.ofDescriptor(\"()V\"));\n+                    int n = -1;\n+                    while (n < 1024) {\n+                        n++;\n+                        CODE\n+                        .invokestatic(inlineClassDesc, \"getInstance\", voidReturnClass)\n+                        .astore(n);\n+                        n++;\n+                        CODE\n+                        .invokestatic(inlineClassDesc, \"getNonBufferedInstance\", voidReturnClass)\n+                        .astore(n);\n+                    }\n+                    CODE.invokestatic(systemClassDesc, \"gc\", MethodTypeDesc.ofDescriptor(\"()V\"));\n+                    Label endLabel = CODE.newLabel();\n+                    while (n > 0) {\n+                        CODE\n+                        .aload(n)\n+                        .invokevirtual(inlineClassDesc, \"verify\", MethodTypeDesc.ofDescriptor(\"()Z\"))\n+                        .iconst_1()\n+                        .if_icmpne(endLabel);\n+                        n--;\n+                    }\n+                    CODE\n+                    .iconst_1()\n+                    .return_(TypeKind.BOOLEAN)\n+                    .labelBinding(endLabel)\n+                    .iconst_0()\n+                    .return_(TypeKind.BOOLEAN);\n+                });\n+        boolean result = (boolean) fromExecStackToLocalVar.invokeExact();\n+        System.out.println(result);\n+        assertTrue(result, \"Invariant\");\n+    }\n+\n+    static void testExecutionStackToFields(Class<?> inlineClass, Class<?> containerClass) throws Throwable {\n+        final int ITERATIONS = Platform.isDebugBuild() ? 3 : 512;\n+        String sig = \"()L\" + inlineClass.getName().replace('.', '\/') + \";\";\n+        final MethodTypeDesc voidReturnClass = MethodTypeDesc.ofDescriptor(sig);\n+        final ClassDesc systemClassDesc = classDesc(System.class);\n+        final ClassDesc inlineClassDesc = classDesc(inlineClass);\n+        final ClassDesc containerClassDesc = classDesc(containerClass);\n+\n+        MethodHandle fromExecStackToFields = InstructionHelper.buildMethodHandle(\n+                LOOKUP,\n+                \"execStackToFields\",\n+                MethodType.methodType(boolean.class),\n+                CODE -> {\n+                    Label loop = CODE.newLabel();\n+                    Label end = CODE.newLabel();\n+                    Label failed = CODE.newLabel();\n+                    CODE\n+                    .invokestatic(systemClassDesc, \"gc\", MethodTypeDesc.ofDescriptor(\"()V\"), false)\n+                    .new_(containerClassDesc)\n+                    .dup()\n+                    .invokespecial(containerClassDesc, \"<init>\", MethodTypeDesc.ofDescriptor(\"()V\"))\n+                    .astore(1)\n+                    .iconst_m1()\n+                    .istore(2)\n+                    .labelBinding(loop)\n+                    .iload(2)\n+                    .ldc(ITERATIONS)\n+                    .if_icmpeq(end)\n+                    .aload(1)\n+                    .invokestatic(inlineClassDesc, \"getInstance\", voidReturnClass)\n+                    .putfield(containerClassDesc, \"nonStaticInlineField\", inlineClassDesc)\n+                    .invokestatic(systemClassDesc, \"gc\", MethodTypeDesc.ofDescriptor(\"()V\"))\n+                    .aload(1)\n+                    .getfield(containerClassDesc, \"nonStaticInlineField\", inlineClassDesc)\n+                    .invokevirtual(inlineClassDesc, \"verify\", MethodTypeDesc.ofDescriptor(\"()Z\"))\n+                    .iconst_1()\n+                    .if_icmpne(failed)\n+                    .aload(1)\n+                    .invokestatic(inlineClassDesc, \"getNonBufferedInstance\", voidReturnClass)\n+                    .putfield(containerClassDesc, \"nonStaticInlineField\", inlineClassDesc)\n+                    .invokestatic(systemClassDesc, \"gc\", MethodTypeDesc.ofDescriptor(\"()V\"))\n+                    .aload(1)\n+                    .getfield(containerClassDesc, \"nonStaticInlineField\", inlineClassDesc)\n+                    .invokevirtual(inlineClassDesc, \"verify\", MethodTypeDesc.ofDescriptor(\"()Z\"))\n+                    .iconst_1()\n+                    .if_icmpne(failed)\n+                    .invokestatic(inlineClassDesc, \"getInstance\", voidReturnClass)\n+                    .putstatic(containerClassDesc, \"staticInlineField\", inlineClassDesc)\n+                    .invokestatic(systemClassDesc, \"gc\", MethodTypeDesc.ofDescriptor(\"()V\"))\n+                    .getstatic(containerClassDesc, \"staticInlineField\", inlineClassDesc)\n+                    .checkcast(inlineClassDesc)\n+                    .invokevirtual(inlineClassDesc, \"verify\", MethodTypeDesc.ofDescriptor(\"()Z\"))\n+                    .iconst_1()\n+                    .if_icmpne(failed)\n+                    .invokestatic(inlineClassDesc, \"getNonBufferedInstance\", voidReturnClass)\n+                    .putstatic(containerClassDesc, \"staticInlineField\", inlineClassDesc)\n+                    .invokestatic(systemClassDesc, \"gc\", MethodTypeDesc.ofDescriptor(\"()V\"))\n+                    .getstatic(containerClassDesc, \"staticInlineField\", inlineClassDesc)\n+                    .checkcast(inlineClassDesc)\n+                    .invokevirtual(inlineClassDesc, \"verify\", MethodTypeDesc.ofDescriptor(\"()Z\"))\n+                    .iconst_1()\n+                    .if_icmpne(failed)\n+                    .iinc(2, 1)\n+                    .goto_(loop)\n+                    .labelBinding(end)\n+                    .iconst_1()\n+                    .return_(TypeKind.BOOLEAN)\n+                    .labelBinding(failed)\n+                    .iconst_0()\n+                    .return_(TypeKind.BOOLEAN);\n+                });\n+        boolean result = (boolean) fromExecStackToFields.invokeExact();\n+        System.out.println(result);\n+        assertTrue(result, \"Invariant\");\n+    }\n+\n+    static void testExecutionStackToInlineArray(Class<?> inlineClass, Class<?> containerClass) throws Throwable {\n+        final int ITERATIONS = Platform.isDebugBuild() ? 3 : 100;\n+        String sig = \"()L\" + inlineClass.getName().replace('.', '\/') + \";\";\n+        final MethodTypeDesc voidReturnClass = MethodTypeDesc.ofDescriptor(sig);\n+        final ClassDesc systemClassDesc = classDesc(System.class);\n+        final ClassDesc inlineClassDesc = classDesc(inlineClass);\n+        final ClassDesc containerClassDesc = classDesc(containerClass);\n+\n+        MethodHandle fromExecStackToInlineArray = InstructionHelper.buildMethodHandle(\n+                LOOKUP,\n+                \"execStackToInlineArray\",\n+                MethodType.methodType(boolean.class),\n+                CODE -> {\n+                    Label loop1 = CODE.newLabel();\n+                    Label loop2 = CODE.newLabel();\n+                    Label end1 = CODE.newLabel();\n+                    Label end2 = CODE.newLabel();\n+                    Label failed = CODE.newLabel();\n+                    CODE\n+                    .invokestatic(systemClassDesc, \"gc\", MethodTypeDesc.ofDescriptor(\"()V\"))\n+                    .new_(containerClassDesc)\n+                    .dup()\n+                    .invokespecial(containerClassDesc, \"<init>\", MethodTypeDesc.ofDescriptor(\"()V\"))\n+                    .astore(1)\n+                    .ldc(ITERATIONS * 3)\n+                    .anewarray(inlineClassDesc)\n+                    .astore(2)\n+                    .aload(2)\n+                    .aload(1)\n+                    .swap()\n+                    .putfield(containerClassDesc, \"valueArray\", inlineClassDesc.arrayType())\n+                    .iconst_0()\n+                    .istore(3)\n+                    .labelBinding(loop1)\n+                    .iload(3)\n+                    .ldc(ITERATIONS *3)\n+                    .if_icmpge(end1)\n+                    .aload(2)\n+                    .iload(3)\n+                    .invokestatic(inlineClassDesc, \"getInstance\", voidReturnClass)\n+                    .aastore()\n+                    .iinc(3, 1)\n+                    .aload(2)\n+                    .iload(3)\n+                    .invokestatic(inlineClassDesc, \"getNonBufferedInstance\", voidReturnClass)\n+                    .aastore()\n+                    .iinc(3, 1)\n+                    .aload(2)\n+                    .iload(3)\n+                    .new_(inlineClassDesc)\n+                    .dup()\n+                    .invokespecial(inlineClassDesc, \"<init>\", MethodTypeDesc.ofDescriptor(\"()V\"))\n+                    .aastore()\n+                    .iinc(3, 1)\n+                    .goto_(loop1)\n+                    .labelBinding(end1)\n+                    .invokestatic(systemClassDesc, \"gc\", MethodTypeDesc.ofDescriptor(\"()V\"))\n+                    .iconst_0()\n+                    .istore(3)\n+                    .labelBinding(loop2)\n+                    .iload(3)\n+                    .ldc(ITERATIONS * 3)\n+                    .if_icmpge(end2)\n+                    .aload(2)\n+                    .iload(3)\n+                    .aaload()\n+                    .invokevirtual(inlineClassDesc, \"verify\", MethodTypeDesc.ofDescriptor(\"()Z\"))\n+                    .iconst_1()\n+                    .if_icmpne(failed)\n+                    .iinc(3, 1)\n+                    .goto_(loop2)\n+                    .labelBinding(end2)\n+                    .iconst_1()\n+                    .return_(TypeKind.BOOLEAN)\n+                    .labelBinding(failed)\n+                    .iconst_0()\n+                    .return_(TypeKind.BOOLEAN);\n+                });\n+        boolean result = (boolean) fromExecStackToInlineArray.invokeExact();\n+        System.out.println(result);\n+        assertTrue(result, \"Invariant\");\n+    }\n+\n+}\n","filename":"test\/hotspot\/jtreg\/runtime\/valhalla\/inlinetypes\/InlineTypesTest.java","additions":504,"deletions":0,"binary":false,"changes":504,"status":"added"},{"patch":"@@ -32,0 +32,1 @@\n+static const char* IDENTITYOBJECT_IF = \"Ljava\/lang\/IdentityObject;\";\n@@ -199,1 +200,1 @@\n-  \/\/ check that hidden class implements just one interface\n+  \/\/ check that hidden class implements just one interface (or two if IdentityObject has been injected)\n@@ -202,2 +203,2 @@\n-  if (count != 1) {\n-    LOG1(\"check_hidden_class_impl_interf: FAIL: implemented interfaces count: %d, expected to be 1\\n\", count);\n+  if (count != 1 && count != 2) {\n+    LOG1(\"check_hidden_class_impl_interf: FAIL: implemented interfaces count: %d, expected to be in [1-2] range\\n\", count);\n@@ -207,3 +208,10 @@\n-  \/\/ get interface signature\n-  err = jvmti->GetClassSignature(interfaces[0], &sig, nullptr);\n-  CHECK_JVMTI_ERROR(jni, err, \"check_hidden_class_impl_interf: Error in JVMTI GetClassSignature for implemented interface\");\n+  bool found = false;\n+  for (int i = 0; i < count; i++) {\n+    \/\/ get interface signature\n+    err = jvmti->GetClassSignature(interfaces[i], &sig, nullptr);\n+    CHECK_JVMTI_ERROR(jni, err, \"check_hidden_class_impl_interf: Error in JVMTI GetClassSignature for implemented interface\");\n+    \/\/ check the interface signature is matching the expected\n+    if (strcmp(sig, EXP_INTERF_SIG) == 0) {\n+      found = true;\n+    }\n+  }\n@@ -211,2 +219,1 @@\n-  \/\/ check the interface signature is matching the expected\n-  if (strcmp(sig, EXP_INTERF_SIG) != 0) {\n+  if (!found) {\n","filename":"test\/hotspot\/jtreg\/serviceability\/jvmti\/HiddenClass\/libHiddenClassSigTest.cpp","additions":15,"deletions":8,"binary":false,"changes":23,"status":"modified"},{"patch":"@@ -496,0 +496,3 @@\n+java\/lang\/ModuleLayer\/LayerControllerTest.java                  8337048 generic-all\n+java\/lang\/ModuleLayer\/BasicLayerTest.java                       8337048 generic-all\n+\n@@ -709,0 +712,4 @@\n+com\/sun\/jdi\/cds\/CDSBreakpointTest.java                          8304168 generic-all\n+com\/sun\/jdi\/cds\/CDSDeleteAllBkptsTest.java                      8304168 generic-all\n+com\/sun\/jdi\/cds\/CDSFieldWatchpoints.java                        8304168 generic-all\n+\n@@ -751,0 +758,6 @@\n+jdk\/classfile\/SwapTest.java                                     8308778 generic-all\n+jdk\/classfile\/LowAdaptTest.java                                 8308778 generic-all\n+jdk\/classfile\/BuilderBlockTest.java                             8308778 generic-all\n+jdk\/classfile\/BuilderTryCatchTest.java                          8308778 generic-all\n+jdk\/classfile\/PrimitiveClassConstantTest.java                   8310649 generic-all\n+\n@@ -787,0 +800,3 @@\n+\n+# valhalla\n+jdk\/jfr\/event\/runtime\/TestSyncOnValueBasedClassEvent.java 8328777 generic-all\n","filename":"test\/jdk\/ProblemList.txt","additions":16,"deletions":0,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -108,0 +108,1 @@\n+                    case LoadableDescriptorsAttribute a -> clb.with(LoadableDescriptorsAttribute.of(a.loadableDescriptors()));\n","filename":"test\/jdk\/jdk\/classfile\/helpers\/RebuildingTransformation.java","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"}]}