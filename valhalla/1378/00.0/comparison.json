{"files":[{"patch":"@@ -519,0 +519,1 @@\n+  # GCC 13 also for array-bounds and stringop-overflow\n@@ -522,1 +523,1 @@\n-  UBSAN_CFLAGS=\"$UBSAN_CHECKS -Wno-stringop-truncation -Wno-format-overflow -fno-omit-frame-pointer -DUNDEFINED_BEHAVIOR_SANITIZER\"\n+  UBSAN_CFLAGS=\"$UBSAN_CHECKS -Wno-stringop-truncation -Wno-format-overflow -Wno-array-bounds -Wno-stringop-overflow -fno-omit-frame-pointer -DUNDEFINED_BEHAVIOR_SANITIZER\"\n","filename":"make\/autoconf\/jdk-options.m4","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -1189,3 +1189,3 @@\n-            version: \"7.5\",\n-            build_number: \"ci\/31\",\n-            file: \"bundles\/jtreg-7.5+1.zip\",\n+            version: \"7.5.1\",\n+            build_number: \"1\",\n+            file: \"bundles\/jtreg-7.5.1+1.zip\",\n","filename":"make\/conf\/jib-profiles.js","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -54,0 +54,4 @@\n+ifeq ($(call isTargetOs, linux), false)\n+    BUILD_TEST_LIB_JAR_EXCLUDES := jdk\/test\/lib\/containers\n+endif\n+\n@@ -57,1 +61,1 @@\n-    EXCLUDES := jdk\/test\/lib\/containers jdk\/test\/lib\/security org, \\\n+    EXCLUDES := $(BUILD_TEST_LIB_JAR_EXCLUDES) org, \\\n@@ -67,0 +71,5 @@\n+        --add-exports java.base\/jdk.internal.platform=ALL-UNNAMED \\\n+        --add-exports java.base\/sun.security.pkcs=ALL-UNNAMED \\\n+        --add-exports java.base\/sun.security.provider.certpath=ALL-UNNAMED \\\n+        --add-exports java.base\/sun.security.tools.keytool=ALL-UNNAMED \\\n+        --add-exports java.base\/sun.security.x509=ALL-UNNAMED \\\n","filename":"make\/test\/BuildTestLib.gmk","additions":10,"deletions":1,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -260,7 +260,6 @@\n-  for (int i = 0; i < FrameMap::nof_cpu_regs; i++) {\n-    Register r = as_Register(i);\n-    if (r == rthread || (i <= 18 && i != rscratch1->encoding() && i != rscratch2->encoding())) {\n-      int sp_offset = cpu_reg_save_offsets[i];\n-      oop_map->set_callee_saved(VMRegImpl::stack2reg(sp_offset),\n-                                r->as_VMReg());\n-    }\n+  for (int i = 0; i < FrameMap::nof_caller_save_cpu_regs(); i++) {\n+    LIR_Opr opr = FrameMap::caller_save_cpu_reg_at(i);\n+    Register r = opr->as_register();\n+    int reg_num = r->encoding();\n+    int sp_offset = cpu_reg_save_offsets[reg_num];\n+    oop_map->set_callee_saved(VMRegImpl::stack2reg(cpu_reg_save_offsets[reg_num]), r->as_VMReg());\n@@ -269,0 +268,4 @@\n+  Register r = rthread;\n+  int reg_num = r->encoding();\n+  oop_map->set_callee_saved(VMRegImpl::stack2reg(cpu_reg_save_offsets[reg_num]), r->as_VMReg());\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/c1_Runtime1_aarch64.cpp","additions":10,"deletions":7,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2000, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2000, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -35,0 +35,1 @@\n+#include \"oops\/compressedOops.hpp\"\n","filename":"src\/hotspot\/share\/c1\/c1_FrameMap.hpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -446,4 +446,3 @@\n-  jio_fprintf(defaultStream::error_stream(),\n-              \"An error has occurred while processing class list file %s %zu:%d.\\n\",\n-              _classlist_file, lineno(), (error_index + 1));\n-  jio_vfprintf(defaultStream::error_stream(), msg, ap);\n+  st->print(\"An error has occurred while processing class list file %s %zu:%d.\\n\",\n+            _classlist_file, lineno(), (error_index + 1));\n+  st->vprint(msg, ap);\n","filename":"src\/hotspot\/share\/cds\/classListParser.cpp","additions":3,"deletions":4,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -105,2 +105,0 @@\n-  _name = (ciSymbol*)ciEnv::current(THREAD)->get_symbol(name);\n-\n","filename":"src\/hotspot\/share\/ci\/ciField.cpp","additions":0,"deletions":2,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -2049,2 +2049,2 @@\n-\n-  if ((types & (1 << tag)) == 0) {\n+  \/\/ tags up to JVM_CONSTANT_ExternalMax are verifiable and valid for shift op\n+  if (tag > JVM_CONSTANT_ExternalMax || (types & (1 << tag)) == 0) {\n","filename":"src\/hotspot\/share\/classfile\/verifier.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -56,0 +56,47 @@\n+#include <type_traits>\n+\n+\/\/ Virtual methods are not allowed in code blobs to simplify caching compiled code.\n+\/\/ Check all \"leaf\" subclasses of CodeBlob class.\n+\n+static_assert(!std::is_polymorphic<nmethod>::value,            \"no virtual methods are allowed in nmethod\");\n+static_assert(!std::is_polymorphic<AdapterBlob>::value,        \"no virtual methods are allowed in code blobs\");\n+static_assert(!std::is_polymorphic<VtableBlob>::value,         \"no virtual methods are allowed in code blobs\");\n+static_assert(!std::is_polymorphic<MethodHandlesAdapterBlob>::value, \"no virtual methods are allowed in code blobs\");\n+static_assert(!std::is_polymorphic<RuntimeStub>::value,        \"no virtual methods are allowed in code blobs\");\n+static_assert(!std::is_polymorphic<DeoptimizationBlob>::value, \"no virtual methods are allowed in code blobs\");\n+static_assert(!std::is_polymorphic<SafepointBlob>::value,      \"no virtual methods are allowed in code blobs\");\n+static_assert(!std::is_polymorphic<UpcallStub>::value,         \"no virtual methods are allowed in code blobs\");\n+#ifdef COMPILER2\n+static_assert(!std::is_polymorphic<ExceptionBlob>::value,      \"no virtual methods are allowed in code blobs\");\n+static_assert(!std::is_polymorphic<UncommonTrapBlob>::value,   \"no virtual methods are allowed in code blobs\");\n+#endif\n+\n+\/\/ Add proxy vtables.\n+\/\/ We need only few for now - they are used only from prints.\n+const nmethod::Vptr                  nmethod::_vpntr;\n+const BufferBlob::Vptr               BufferBlob::_vpntr;\n+const RuntimeStub::Vptr              RuntimeStub::_vpntr;\n+const SingletonBlob::Vptr            SingletonBlob::_vpntr;\n+const DeoptimizationBlob::Vptr       DeoptimizationBlob::_vpntr;\n+const UpcallStub::Vptr               UpcallStub::_vpntr;\n+\n+const CodeBlob::Vptr* CodeBlob::vptr() const {\n+  constexpr const CodeBlob::Vptr* array[(size_t)CodeBlobKind::Number_Of_Kinds] = {\n+      nullptr\/* None *\/,\n+      &nmethod::_vpntr,\n+      &BufferBlob::_vpntr,\n+      &AdapterBlob::_vpntr,\n+      &VtableBlob::_vpntr,\n+      &MethodHandlesAdapterBlob::_vpntr,\n+      &RuntimeStub::_vpntr,\n+      &DeoptimizationBlob::_vpntr,\n+      &SafepointBlob::_vpntr,\n+#ifdef COMPILER2\n+      &ExceptionBlob::_vpntr,\n+      &UncommonTrapBlob::_vpntr,\n+#endif\n+      &UpcallStub::_vpntr\n+  };\n+\n+  return array[(size_t)_kind];\n+}\n@@ -418,1 +465,1 @@\n-: RuntimeBlob(name, CodeBlobKind::Runtime_Stub, cb, size, sizeof(RuntimeStub),\n+: RuntimeBlob(name, CodeBlobKind::RuntimeStub, cb, size, sizeof(RuntimeStub),\n@@ -514,0 +561,1 @@\n+#ifdef COMPILER2\n@@ -518,1 +566,0 @@\n-#ifdef COMPILER2\n@@ -525,1 +572,1 @@\n-: SingletonBlob(\"UncommonTrapBlob\", CodeBlobKind::Uncommon_Trap, cb,\n+: SingletonBlob(\"UncommonTrapBlob\", CodeBlobKind::UncommonTrap, cb,\n@@ -548,4 +595,0 @@\n-\n-#endif \/\/ COMPILER2\n-\n-\n@@ -555,1 +598,0 @@\n-#ifdef COMPILER2\n@@ -585,1 +627,0 @@\n-\n@@ -588,1 +629,0 @@\n-\n@@ -676,0 +716,6 @@\n+void CodeBlob::verify() {\n+  if (is_nmethod()) {\n+    as_nmethod()->verify();\n+  }\n+}\n+\n@@ -677,2 +723,1 @@\n-  st->print_cr(\"[CodeBlob (\" INTPTR_FORMAT \")]\", p2i(this));\n-  st->print_cr(\"Framesize: %d\", _frame_size);\n+  vptr()->print_on(this, st);\n@@ -684,0 +729,9 @@\n+  vptr()->print_value_on(this, st);\n+}\n+\n+void CodeBlob::print_on_impl(outputStream* st) const {\n+  st->print_cr(\"[CodeBlob (\" INTPTR_FORMAT \")]\", p2i(this));\n+  st->print_cr(\"Framesize: %d\", _frame_size);\n+}\n+\n+void CodeBlob::print_value_on_impl(outputStream* st) const {\n@@ -687,0 +741,14 @@\n+void CodeBlob::print_block_comment(outputStream* stream, address block_begin) const {\n+#if defined(SUPPORT_ASSEMBLY) || defined(SUPPORT_ABSTRACT_ASSEMBLY)\n+  if (is_nmethod()) {\n+    as_nmethod()->print_nmethod_labels(stream, block_begin);\n+  }\n+#endif\n+\n+#ifndef PRODUCT\n+  ptrdiff_t offset = block_begin - code_begin();\n+  assert(offset >= 0, \"Expecting non-negative offset!\");\n+  _asm_remarks.print(uint(offset), stream);\n+#endif\n+  }\n+\n@@ -688,1 +756,1 @@\n-  if (is_buffer_blob()) {\n+  if (is_buffer_blob() || is_adapter_blob() || is_vtable_blob() || is_method_handles_adapter_blob()) {\n@@ -740,1 +808,1 @@\n-      nm->print(st);\n+      nm->print_on(st);\n@@ -748,2 +816,3 @@\n-void BufferBlob::verify() {\n-  \/\/ unimplemented\n+void BufferBlob::print_on_impl(outputStream* st) const {\n+  RuntimeBlob::print_on_impl(st);\n+  print_value_on_impl(st);\n@@ -752,6 +821,1 @@\n-void BufferBlob::print_on(outputStream* st) const {\n-  RuntimeBlob::print_on(st);\n-  print_value_on(st);\n-}\n-\n-void BufferBlob::print_value_on(outputStream* st) const {\n+void BufferBlob::print_value_on_impl(outputStream* st) const {\n@@ -761,5 +825,1 @@\n-void RuntimeStub::verify() {\n-  \/\/ unimplemented\n-}\n-\n-void RuntimeStub::print_on(outputStream* st) const {\n+void RuntimeStub::print_on_impl(outputStream* st) const {\n@@ -767,1 +827,1 @@\n-  RuntimeBlob::print_on(st);\n+  RuntimeBlob::print_on_impl(st);\n@@ -773,1 +833,1 @@\n-void RuntimeStub::print_value_on(outputStream* st) const {\n+void RuntimeStub::print_value_on_impl(outputStream* st) const {\n@@ -777,5 +837,1 @@\n-void SingletonBlob::verify() {\n-  \/\/ unimplemented\n-}\n-\n-void SingletonBlob::print_on(outputStream* st) const {\n+void SingletonBlob::print_on_impl(outputStream* st) const {\n@@ -783,1 +839,1 @@\n-  RuntimeBlob::print_on(st);\n+  RuntimeBlob::print_on_impl(st);\n@@ -788,1 +844,1 @@\n-void SingletonBlob::print_value_on(outputStream* st) const {\n+void SingletonBlob::print_value_on_impl(outputStream* st) const {\n@@ -792,1 +848,1 @@\n-void DeoptimizationBlob::print_value_on(outputStream* st) const {\n+void DeoptimizationBlob::print_value_on_impl(outputStream* st) const {\n@@ -796,7 +852,3 @@\n-void UpcallStub::verify() {\n-  \/\/ unimplemented\n-}\n-\n-void UpcallStub::print_on(outputStream* st) const {\n-  RuntimeBlob::print_on(st);\n-  print_value_on(st);\n+void UpcallStub::print_on_impl(outputStream* st) const {\n+  RuntimeBlob::print_on_impl(st);\n+  print_value_on_impl(st);\n@@ -810,1 +862,1 @@\n-void UpcallStub::print_value_on(outputStream* st) const {\n+void UpcallStub::print_value_on_impl(outputStream* st) const {\n","filename":"src\/hotspot\/share\/code\/codeBlob.cpp","additions":97,"deletions":45,"binary":false,"changes":142,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1998, 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1998, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -65,1 +65,1 @@\n-\/\/    ExceptionBlob      : Used for stack unrolling\n+\/\/    ExceptionBlob      : Used for stack unrolling\n@@ -84,1 +84,1 @@\n-  MH_Adapter,\n+  MHAdapter,\n@@ -86,1 +86,1 @@\n-  Runtime_Stub,\n+  RuntimeStub,\n@@ -88,2 +88,4 @@\n-  Exception,\n-  Uncommon_Trap,\n+#ifdef COMPILER2\n+  Exception,\n+  UncommonTrap,\n+#endif\n@@ -133,0 +135,11 @@\n+  void print_on_impl(outputStream* st) const;\n+  void print_value_on_impl(outputStream* st) const;\n+\n+  class Vptr {\n+   public:\n+    virtual void print_on(const CodeBlob* instance, outputStream* st) const = 0;\n+    virtual void print_value_on(const CodeBlob* instance, outputStream* st) const = 0;\n+  };\n+\n+  const Vptr* vptr() const;\n+\n@@ -143,1 +156,1 @@\n-  virtual ~CodeBlob() {\n+  ~CodeBlob() {\n@@ -157,1 +170,1 @@\n-  bool is_runtime_stub() const                { return _kind == CodeBlobKind::Runtime_Stub; }\n+  bool is_runtime_stub() const                { return _kind == CodeBlobKind::RuntimeStub; }\n@@ -159,1 +172,2 @@\n-  bool is_uncommon_trap_stub() const          { return _kind == CodeBlobKind::Uncommon_Trap; }\n+#ifdef COMPILER2\n+  bool is_uncommon_trap_stub() const          { return _kind == CodeBlobKind::UncommonTrap; }\n@@ -161,0 +175,4 @@\n+#else\n+  bool is_uncommon_trap_stub() const          { return false; }\n+  bool is_exception_stub() const              { return false; }\n+#endif\n@@ -164,1 +182,1 @@\n-  bool is_method_handles_adapter_blob() const { return _kind == CodeBlobKind::MH_Adapter; }\n+  bool is_method_handles_adapter_blob() const { return _kind == CodeBlobKind::MHAdapter; }\n@@ -169,3 +187,3 @@\n-  nmethod* as_nmethod_or_null()               { return is_nmethod() ? (nmethod*) this : nullptr; }\n-  nmethod* as_nmethod()                       { assert(is_nmethod(), \"must be nmethod\"); return (nmethod*) this; }\n-  CodeBlob* as_codeblob_or_null() const       { return (CodeBlob*) this; }\n+  nmethod* as_nmethod_or_null() const         { return is_nmethod() ? (nmethod*) this : nullptr; }\n+  nmethod* as_nmethod() const                 { assert(is_nmethod(), \"must be nmethod\"); return (nmethod*) this; }\n+  CodeBlob* as_codeblob() const               { return (CodeBlob*) this; }\n@@ -239,4 +257,5 @@\n-  virtual void verify() = 0;\n-  virtual void print() const;\n-  virtual void print_on(outputStream* st) const;\n-  virtual void print_value_on(outputStream* st) const;\n+  void verify();\n+  void print() const;\n+  void print_on(outputStream* st) const;\n+  void print_value_on(outputStream* st) const;\n+\n@@ -247,7 +266,1 @@\n-  virtual void print_block_comment(outputStream* stream, address block_begin) const {\n-#ifndef PRODUCT\n-    ptrdiff_t offset = block_begin - code_begin();\n-    assert(offset >= 0, \"Expecting non-negative offset!\");\n-    _asm_remarks.print(uint(offset), stream);\n-#endif\n-  }\n+  void print_block_comment(outputStream* stream, address block_begin) const;\n@@ -296,0 +309,3 @@\n+\n+  class Vptr : public CodeBlob::Vptr {\n+  };\n@@ -326,2 +342,11 @@\n-  \/\/ Verification support\n-  void verify() override;\n+  void print_on_impl(outputStream* st) const;\n+  void print_value_on_impl(outputStream* st) const;\n+\n+  class Vptr : public RuntimeBlob::Vptr {\n+    void print_on(const CodeBlob* instance, outputStream* st) const override {\n+      ((const BufferBlob*)instance)->print_on_impl(st);\n+    }\n+    void print_value_on(const CodeBlob* instance, outputStream* st) const override {\n+      ((const BufferBlob*)instance)->print_value_on_impl(st);\n+    }\n+  };\n@@ -329,2 +354,1 @@\n-  void print_on(outputStream* st) const override;\n-  void print_value_on(outputStream* st) const override;\n+  static const Vptr _vpntr;\n@@ -369,1 +393,1 @@\n-  MethodHandlesAdapterBlob(int size): BufferBlob(\"MethodHandles adapters\", CodeBlobKind::MH_Adapter, size) {}\n+  MethodHandlesAdapterBlob(int size): BufferBlob(\"MethodHandles adapters\", CodeBlobKind::MHAdapter, size) {}\n@@ -429,1 +453,1 @@\n-  address entry_point() const                    { return code_begin(); }\n+  address entry_point() const         { return code_begin(); }\n@@ -431,2 +455,2 @@\n-  \/\/ Verification support\n-  void verify() override;\n+  void print_on_impl(outputStream* st) const;\n+  void print_value_on_impl(outputStream* st) const;\n@@ -434,2 +458,10 @@\n-  void print_on(outputStream* st) const override;\n-  void print_value_on(outputStream* st) const override;\n+  class Vptr : public RuntimeBlob::Vptr {\n+    void print_on(const CodeBlob* instance, outputStream* st) const override {\n+      instance->as_runtime_stub()->print_on_impl(st);\n+    }\n+    void print_value_on(const CodeBlob* instance, outputStream* st) const override {\n+      instance->as_runtime_stub()->print_value_on_impl(st);\n+    }\n+  };\n+\n+  static const Vptr _vpntr;\n@@ -463,2 +495,11 @@\n-  \/\/ Verification support\n-  void verify() override; \/\/ does nothing\n+  void print_on_impl(outputStream* st) const;\n+  void print_value_on_impl(outputStream* st) const;\n+\n+  class Vptr : public RuntimeBlob::Vptr {\n+    void print_on(const CodeBlob* instance, outputStream* st) const override {\n+      ((const SingletonBlob*)instance)->print_on_impl(st);\n+    }\n+    void print_value_on(const CodeBlob* instance, outputStream* st) const override {\n+      ((const SingletonBlob*)instance)->print_value_on_impl(st);\n+    }\n+  };\n@@ -466,2 +507,1 @@\n-  void print_on(outputStream* st) const override;\n-  void print_value_on(outputStream* st) const override;\n+  static const Vptr _vpntr;\n@@ -512,3 +552,0 @@\n-  \/\/ Printing\n-  void print_value_on(outputStream* st) const override;\n-\n@@ -544,0 +581,10 @@\n+\n+  void print_value_on_impl(outputStream* st) const;\n+\n+  class Vptr : public SingletonBlob::Vptr {\n+    void print_value_on(const CodeBlob* instance, outputStream* st) const override {\n+      ((const DeoptimizationBlob*)instance)->print_value_on_impl(st);\n+    }\n+  };\n+\n+  static const Vptr _vpntr;\n@@ -656,1 +703,1 @@\n-  \/\/ GC\/Verification support\n+  \/\/ GC support\n@@ -658,4 +705,13 @@\n-  void verify() override;\n-  \/\/ Misc.\n-  void print_on(outputStream* st) const override;\n-  void print_value_on(outputStream* st) const override;\n+  void print_on_impl(outputStream* st) const;\n+  void print_value_on_impl(outputStream* st) const;\n+\n+  class Vptr : public RuntimeBlob::Vptr {\n+    void print_on(const CodeBlob* instance, outputStream* st) const override {\n+      instance->as_upcall_stub()->print_on_impl(st);\n+    }\n+    void print_value_on(const CodeBlob* instance, outputStream* st) const override {\n+      instance->as_upcall_stub()->print_value_on_impl(st);\n+    }\n+  };\n+\n+  static const Vptr _vpntr;\n","filename":"src\/hotspot\/share\/code\/codeBlob.hpp","additions":102,"deletions":46,"binary":false,"changes":148,"status":"modified"},{"patch":"@@ -1640,1 +1640,1 @@\n-void nmethod::print_on(outputStream* st, const char* msg) const {\n+void nmethod::print_on_with_msg(outputStream* st, const char* msg) const {\n@@ -1992,1 +1992,1 @@\n-    print_on(tty, \"made not entrant\");\n+    print_on_with_msg(tty, \"made not entrant\");\n@@ -2164,1 +2164,3 @@\n-  return NMethodAccess<AS_NO_KEEPALIVE>::oop_load(oop_addr_at(index));\n+\n+  BarrierSetNMethod* bs_nm = BarrierSet::barrier_set()->barrier_set_nmethod();\n+  return bs_nm->oop_load_no_keepalive(this, index);\n@@ -2171,1 +2173,3 @@\n-  return NMethodAccess<ON_PHANTOM_OOP_REF>::oop_load(oop_addr_at(index));\n+\n+  BarrierSetNMethod* bs_nm = BarrierSet::barrier_set()->barrier_set_nmethod();\n+  return bs_nm->oop_load_phantom(this, index);\n@@ -3050,6 +3054,1 @@\n-void nmethod::print() const {\n-  ttyLocker ttyl;   \/\/ keep the following output all in one block\n-  print(tty);\n-}\n-\n-void nmethod::print(outputStream* st) const {\n+void nmethod::print_on_impl(outputStream* st) const {\n@@ -3070,1 +3069,1 @@\n-  print_on(st, nullptr);\n+  print_on_with_msg(st, nullptr);\n@@ -3421,1 +3420,1 @@\n-  this->print(st);\n+  this->print_on(st);\n@@ -4003,0 +4002,2 @@\n+void nmethod::print_value_on_impl(outputStream* st) const {\n+  st->print_cr(\"nmethod\");\n@@ -4004,4 +4005,1 @@\n-void nmethod::print_value_on(outputStream* st) const {\n-  st->print(\"nmethod\");\n-  print_on(st, nullptr);\n-}\n+  print_on_with_msg(st, nullptr);\n@@ -4009,0 +4007,1 @@\n+}\n","filename":"src\/hotspot\/share\/code\/nmethod.cpp","additions":15,"deletions":16,"binary":false,"changes":31,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -921,1 +921,1 @@\n-  void verify() override;\n+  void verify();\n@@ -933,2 +933,1 @@\n-  void print()                 const override;\n-  void print(outputStream* st) const;\n+  void print_on_impl(outputStream* st) const;\n@@ -936,0 +935,1 @@\n+  void print_value_on_impl(outputStream* st) const;\n@@ -943,1 +943,0 @@\n-  void print_value_on(outputStream* st) const override;\n@@ -962,3 +961,1 @@\n-  \/\/ need to re-define this from CodeBlob else the overload hides it\n-  void print_on(outputStream* st) const override { CodeBlob::print_on(st); }\n-  void print_on(outputStream* st, const char* msg) const;\n+  void print_on_with_msg(outputStream* st, const char* msg) const;\n@@ -972,7 +969,0 @@\n-  void print_block_comment(outputStream* stream, address block_begin) const override {\n-#if defined(SUPPORT_ASSEMBLY) || defined(SUPPORT_ABSTRACT_ASSEMBLY)\n-    print_nmethod_labels(stream, block_begin);\n-    CodeBlob::print_block_comment(stream, block_begin);\n-#endif\n-  }\n-\n@@ -1016,0 +1006,12 @@\n+\n+  class Vptr : public CodeBlob::Vptr {\n+    void print_on(const CodeBlob* instance, outputStream* st) const override {\n+      ttyLocker ttyl;\n+      instance->as_nmethod()->print_on_impl(st);\n+    }\n+    void print_value_on(const CodeBlob* instance, outputStream* st) const override {\n+      instance->as_nmethod()->print_value_on_impl(st);\n+    }\n+  };\n+\n+  static const Vptr _vpntr;\n","filename":"src\/hotspot\/share\/code\/nmethod.hpp","additions":17,"deletions":15,"binary":false,"changes":32,"status":"modified"},{"patch":"@@ -2077,0 +2077,13 @@\n+    \/\/ If we are executing a task during the request to block, report the task\n+    \/\/ before disappearing.\n+    CompilerThread* thread = CompilerThread::current();\n+    if (thread != nullptr) {\n+      CompileTask* task = thread->task();\n+      if (task != nullptr) {\n+        if (PrintCompilation) {\n+          task->print(tty, \"blocked\");\n+        }\n+        task->print_ul(\"blocked\");\n+      }\n+    }\n+    \/\/ Go to VM state and block for final VM shutdown safepoint.\n@@ -2078,0 +2091,1 @@\n+    assert(false, \"Should never unblock from TIVNM entry\");\n","filename":"src\/hotspot\/share\/compiler\/compileBroker.cpp","additions":14,"deletions":0,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -531,0 +531,56 @@\n+void G1BarrierSetC2::elide_dominated_barrier(MachNode* mach) const {\n+  uint8_t barrier_data = mach->barrier_data();\n+  barrier_data &= ~G1C2BarrierPre;\n+  if (CardTableBarrierSetC2::use_ReduceInitialCardMarks()) {\n+    barrier_data &= ~G1C2BarrierPost;\n+    barrier_data &= ~G1C2BarrierPostNotNull;\n+  }\n+  mach->set_barrier_data(barrier_data);\n+}\n+\n+void G1BarrierSetC2::analyze_dominating_barriers() const {\n+  ResourceMark rm;\n+  PhaseCFG* const cfg = Compile::current()->cfg();\n+\n+  \/\/ Find allocations and memory accesses (stores and atomic operations), and\n+  \/\/ track them in lists.\n+  Node_List accesses;\n+  Node_List allocations;\n+  for (uint i = 0; i < cfg->number_of_blocks(); ++i) {\n+    const Block* const block = cfg->get_block(i);\n+    for (uint j = 0; j < block->number_of_nodes(); ++j) {\n+      Node* const node = block->get_node(j);\n+      if (node->is_Phi()) {\n+        if (BarrierSetC2::is_allocation(node)) {\n+          allocations.push(node);\n+        }\n+        continue;\n+      } else if (!node->is_Mach()) {\n+        continue;\n+      }\n+\n+      MachNode* const mach = node->as_Mach();\n+      switch (mach->ideal_Opcode()) {\n+      case Op_StoreP:\n+      case Op_StoreN:\n+      case Op_CompareAndExchangeP:\n+      case Op_CompareAndSwapP:\n+      case Op_GetAndSetP:\n+      case Op_CompareAndExchangeN:\n+      case Op_CompareAndSwapN:\n+      case Op_GetAndSetN:\n+        if (mach->barrier_data() != 0) {\n+          accesses.push(mach);\n+        }\n+        break;\n+      default:\n+        break;\n+      }\n+    }\n+  }\n+\n+  \/\/ Find dominating allocations for each memory access (store or atomic\n+  \/\/ operation) and elide barriers if there is no safepoint poll in between.\n+  elide_dominated_barriers(accesses, allocations);\n+}\n+\n@@ -533,0 +589,1 @@\n+  analyze_dominating_barriers();\n","filename":"src\/hotspot\/share\/gc\/g1\/c2\/g1BarrierSetC2.cpp","additions":57,"deletions":0,"binary":false,"changes":57,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2018, 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2018, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -89,0 +89,3 @@\n+private:\n+  void analyze_dominating_barriers() const;\n+\n@@ -120,0 +123,1 @@\n+  virtual void elide_dominated_barrier(MachNode* mach) const;\n","filename":"src\/hotspot\/share\/gc\/g1\/c2\/g1BarrierSetC2.hpp","additions":5,"deletions":1,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -903,0 +903,256 @@\n+static bool block_has_safepoint(const Block* block, uint from, uint to) {\n+  for (uint i = from; i < to; i++) {\n+    if (block->get_node(i)->is_MachSafePoint()) {\n+      \/\/ Safepoint found\n+      return true;\n+    }\n+  }\n+\n+  \/\/ Safepoint not found\n+  return false;\n+}\n+\n+static bool block_has_safepoint(const Block* block) {\n+  return block_has_safepoint(block, 0, block->number_of_nodes());\n+}\n+\n+static uint block_index(const Block* block, const Node* node) {\n+  for (uint j = 0; j < block->number_of_nodes(); ++j) {\n+    if (block->get_node(j) == node) {\n+      return j;\n+    }\n+  }\n+  ShouldNotReachHere();\n+  return 0;\n+}\n+\n+\/\/ Look through various node aliases\n+static const Node* look_through_node(const Node* node) {\n+  while (node != nullptr) {\n+    const Node* new_node = node;\n+    if (node->is_Mach()) {\n+      const MachNode* const node_mach = node->as_Mach();\n+      if (node_mach->ideal_Opcode() == Op_CheckCastPP) {\n+        new_node = node->in(1);\n+      }\n+      if (node_mach->is_SpillCopy()) {\n+        new_node = node->in(1);\n+      }\n+    }\n+    if (new_node == node || new_node == nullptr) {\n+      break;\n+    } else {\n+      node = new_node;\n+    }\n+  }\n+\n+  return node;\n+}\n+\n+\/\/ Whether the given offset is undefined.\n+static bool is_undefined(intptr_t offset) {\n+  return offset == Type::OffsetTop;\n+}\n+\n+\/\/ Whether the given offset is unknown.\n+static bool is_unknown(intptr_t offset) {\n+  return offset == Type::OffsetBot;\n+}\n+\n+\/\/ Whether the given offset is concrete (defined and compile-time known).\n+static bool is_concrete(intptr_t offset) {\n+  return !is_undefined(offset) && !is_unknown(offset);\n+}\n+\n+\/\/ Compute base + offset components of the memory address accessed by mach.\n+\/\/ Return a node representing the base address, or null if the base cannot be\n+\/\/ found or the offset is undefined or a concrete negative value. If a non-null\n+\/\/ base is returned, the offset is a concrete, nonnegative value or unknown.\n+static const Node* get_base_and_offset(const MachNode* mach, intptr_t& offset) {\n+  const TypePtr* adr_type = nullptr;\n+  offset = 0;\n+  const Node* base = mach->get_base_and_disp(offset, adr_type);\n+\n+  if (base == nullptr || base == NodeSentinel) {\n+    return nullptr;\n+  }\n+\n+  if (offset == 0 && base->is_Mach() && base->as_Mach()->ideal_Opcode() == Op_AddP) {\n+    \/\/ The memory address is computed by 'base' and fed to 'mach' via an\n+    \/\/ indirect memory operand (indicated by offset == 0). The ultimate base and\n+    \/\/ offset can be fetched directly from the inputs and Ideal type of 'base'.\n+    const TypeOopPtr* oopptr = base->bottom_type()->isa_oopptr();\n+    if (oopptr == nullptr) return nullptr;\n+    offset = oopptr->offset();\n+    \/\/ Even if 'base' is not an Ideal AddP node anymore, Matcher::ReduceInst()\n+    \/\/ guarantees that the base address is still available at the same slot.\n+    base = base->in(AddPNode::Base);\n+    assert(base != nullptr, \"\");\n+  }\n+\n+  if (is_undefined(offset) || (is_concrete(offset) && offset < 0)) {\n+    return nullptr;\n+  }\n+\n+  return look_through_node(base);\n+}\n+\n+\/\/ Whether a phi node corresponds to an array allocation.\n+\/\/ This test is incomplete: in some edge cases, it might return false even\n+\/\/ though the node does correspond to an array allocation.\n+static bool is_array_allocation(const Node* phi) {\n+  precond(phi->is_Phi());\n+  \/\/ Check whether phi has a successor cast (CheckCastPP) to Java array pointer,\n+  \/\/ possibly below spill copies and other cast nodes. Limit the exploration to\n+  \/\/ a single path from the phi node consisting of these node types.\n+  const Node* current = phi;\n+  while (true) {\n+    const Node* next = nullptr;\n+    for (DUIterator_Fast imax, i = current->fast_outs(imax); i < imax; i++) {\n+      if (!current->fast_out(i)->isa_Mach()) {\n+        continue;\n+      }\n+      const MachNode* succ = current->fast_out(i)->as_Mach();\n+      if (succ->ideal_Opcode() == Op_CheckCastPP) {\n+        if (succ->get_ptr_type()->isa_aryptr()) {\n+          \/\/ Cast to Java array pointer: phi corresponds to an array allocation.\n+          return true;\n+        }\n+        \/\/ Other cast: record as candidate for further exploration.\n+        next = succ;\n+      } else if (succ->is_SpillCopy() && next == nullptr) {\n+        \/\/ Spill copy, and no better candidate found: record as candidate.\n+        next = succ;\n+      }\n+    }\n+    if (next == nullptr) {\n+      \/\/ No evidence found that phi corresponds to an array allocation, and no\n+      \/\/ candidates available to continue exploring.\n+      return false;\n+    }\n+    \/\/ Continue exploring from the best candidate found.\n+    current = next;\n+  }\n+  ShouldNotReachHere();\n+}\n+\n+bool BarrierSetC2::is_allocation(const Node* node) {\n+  assert(node->is_Phi(), \"expected phi node\");\n+  if (node->req() != 3) {\n+    return false;\n+  }\n+  const Node* const fast_node = node->in(2);\n+  if (!fast_node->is_Mach()) {\n+    return false;\n+  }\n+  const MachNode* const fast_mach = fast_node->as_Mach();\n+  if (fast_mach->ideal_Opcode() != Op_LoadP) {\n+    return false;\n+  }\n+  intptr_t offset;\n+  const Node* const base = get_base_and_offset(fast_mach, offset);\n+  if (base == nullptr || !base->is_Mach() || !is_concrete(offset)) {\n+    return false;\n+  }\n+  const MachNode* const base_mach = base->as_Mach();\n+  if (base_mach->ideal_Opcode() != Op_ThreadLocal) {\n+    return false;\n+  }\n+  return offset == in_bytes(Thread::tlab_top_offset());\n+}\n+\n+void BarrierSetC2::elide_dominated_barriers(Node_List& accesses, Node_List& access_dominators) const {\n+  Compile* const C = Compile::current();\n+  PhaseCFG* const cfg = C->cfg();\n+\n+  for (uint i = 0; i < accesses.size(); i++) {\n+    MachNode* const access = accesses.at(i)->as_Mach();\n+    intptr_t access_offset;\n+    const Node* const access_obj = get_base_and_offset(access, access_offset);\n+    Block* const access_block = cfg->get_block_for_node(access);\n+    const uint access_index = block_index(access_block, access);\n+\n+    if (access_obj == nullptr) {\n+      \/\/ No information available\n+      continue;\n+    }\n+\n+    for (uint j = 0; j < access_dominators.size(); j++) {\n+     const  Node* const mem = access_dominators.at(j);\n+      if (mem->is_Phi()) {\n+        assert(is_allocation(mem), \"expected allocation phi node\");\n+        if (mem != access_obj) {\n+          continue;\n+        }\n+        if (is_unknown(access_offset) && !is_array_allocation(mem)) {\n+          \/\/ The accessed address has an unknown offset, but the allocated\n+          \/\/ object cannot be determined to be an array. Avoid eliding in this\n+          \/\/ case, to be on the safe side.\n+          continue;\n+        }\n+        assert((is_concrete(access_offset) && access_offset >= 0) || (is_unknown(access_offset) && is_array_allocation(mem)),\n+               \"candidate allocation-dominated access offsets must be either concrete and nonnegative, or unknown (for array allocations only)\");\n+      } else {\n+        \/\/ Access node\n+        const MachNode* const mem_mach = mem->as_Mach();\n+        intptr_t mem_offset;\n+        const Node* const mem_obj = get_base_and_offset(mem_mach, mem_offset);\n+\n+        if (mem_obj == nullptr ||\n+            !is_concrete(access_offset) ||\n+            !is_concrete(mem_offset)) {\n+          \/\/ No information available\n+          continue;\n+        }\n+\n+        if (mem_obj != access_obj || mem_offset != access_offset) {\n+          \/\/ Not the same addresses, not a candidate\n+          continue;\n+        }\n+        assert(is_concrete(access_offset) && access_offset >= 0,\n+               \"candidate non-allocation-dominated access offsets must be concrete and nonnegative\");\n+      }\n+\n+      Block* mem_block = cfg->get_block_for_node(mem);\n+      const uint mem_index = block_index(mem_block, mem);\n+\n+      if (access_block == mem_block) {\n+        \/\/ Earlier accesses in the same block\n+        if (mem_index < access_index && !block_has_safepoint(mem_block, mem_index + 1, access_index)) {\n+          elide_dominated_barrier(access);\n+        }\n+      } else if (mem_block->dominates(access_block)) {\n+        \/\/ Dominating block? Look around for safepoints\n+        ResourceMark rm;\n+        Block_List stack;\n+        VectorSet visited;\n+        stack.push(access_block);\n+        bool safepoint_found = block_has_safepoint(access_block);\n+        while (!safepoint_found && stack.size() > 0) {\n+          const Block* const block = stack.pop();\n+          if (visited.test_set(block->_pre_order)) {\n+            continue;\n+          }\n+          if (block_has_safepoint(block)) {\n+            safepoint_found = true;\n+            break;\n+          }\n+          if (block == mem_block) {\n+            continue;\n+          }\n+\n+          \/\/ Push predecessor blocks\n+          for (uint p = 1; p < block->num_preds(); ++p) {\n+            Block* const pred = cfg->get_block_for_node(block->pred(p));\n+            stack.push(pred);\n+          }\n+        }\n+\n+        if (!safepoint_found) {\n+          elide_dominated_barrier(access);\n+        }\n+      }\n+    }\n+  }\n+}\n+\n","filename":"src\/hotspot\/share\/gc\/shared\/c2\/barrierSetC2.cpp","additions":256,"deletions":0,"binary":false,"changes":256,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2018, 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2018, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -374,0 +374,8 @@\n+  \/\/ Whether the given phi node joins OOPs from fast and slow allocation paths.\n+  static bool is_allocation(const Node* node);\n+  \/\/ Elide GC barriers from a Mach node according to elide_dominated_barriers().\n+  virtual void elide_dominated_barrier(MachNode* mach) const { }\n+  \/\/ Elide GC barriers from instructions in 'accesses' if they are dominated by\n+  \/\/ instructions in 'access_dominators' (according to elide_mach_barrier()) and\n+  \/\/ there is no safepoint poll in between.\n+  void elide_dominated_barriers(Node_List& accesses, Node_List& access_dominators) const;\n","filename":"src\/hotspot\/share\/gc\/shared\/c2\/barrierSetC2.hpp","additions":9,"deletions":1,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -123,1 +123,1 @@\n-bool CardTableBarrierSetC2::use_ReduceInitialCardMarks() const {\n+bool CardTableBarrierSetC2::use_ReduceInitialCardMarks() {\n","filename":"src\/hotspot\/share\/gc\/shared\/c2\/cardTableBarrierSetC2.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -44,1 +44,1 @@\n-  bool use_ReduceInitialCardMarks() const;\n+  static bool use_ReduceInitialCardMarks();\n","filename":"src\/hotspot\/share\/gc\/shared\/c2\/cardTableBarrierSetC2.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -31,1 +31,0 @@\n-#include \"opto\/addnode.hpp\"\n@@ -41,1 +40,0 @@\n-#include \"opto\/rootnode.hpp\"\n@@ -478,165 +476,1 @@\n-\/\/ == Dominating barrier elision ==\n-\n-static bool block_has_safepoint(const Block* block, uint from, uint to) {\n-  for (uint i = from; i < to; i++) {\n-    if (block->get_node(i)->is_MachSafePoint()) {\n-      \/\/ Safepoint found\n-      return true;\n-    }\n-  }\n-\n-  \/\/ Safepoint not found\n-  return false;\n-}\n-\n-static bool block_has_safepoint(const Block* block) {\n-  return block_has_safepoint(block, 0, block->number_of_nodes());\n-}\n-\n-static uint block_index(const Block* block, const Node* node) {\n-  for (uint j = 0; j < block->number_of_nodes(); ++j) {\n-    if (block->get_node(j) == node) {\n-      return j;\n-    }\n-  }\n-  ShouldNotReachHere();\n-  return 0;\n-}\n-\n-\/\/ Look through various node aliases\n-static const Node* look_through_node(const Node* node) {\n-  while (node != nullptr) {\n-    const Node* new_node = node;\n-    if (node->is_Mach()) {\n-      const MachNode* const node_mach = node->as_Mach();\n-      if (node_mach->ideal_Opcode() == Op_CheckCastPP) {\n-        new_node = node->in(1);\n-      }\n-      if (node_mach->is_SpillCopy()) {\n-        new_node = node->in(1);\n-      }\n-    }\n-    if (new_node == node || new_node == nullptr) {\n-      break;\n-    } else {\n-      node = new_node;\n-    }\n-  }\n-\n-  return node;\n-}\n-\n-\/\/ Whether the given offset is undefined.\n-static bool is_undefined(intptr_t offset) {\n-  return offset == Type::OffsetTop;\n-}\n-\n-\/\/ Whether the given offset is unknown.\n-static bool is_unknown(intptr_t offset) {\n-  return offset == Type::OffsetBot;\n-}\n-\n-\/\/ Whether the given offset is concrete (defined and compile-time known).\n-static bool is_concrete(intptr_t offset) {\n-  return !is_undefined(offset) && !is_unknown(offset);\n-}\n-\n-\/\/ Compute base + offset components of the memory address accessed by mach.\n-\/\/ Return a node representing the base address, or null if the base cannot be\n-\/\/ found or the offset is undefined or a concrete negative value. If a non-null\n-\/\/ base is returned, the offset is a concrete, nonnegative value or unknown.\n-static const Node* get_base_and_offset(const MachNode* mach, intptr_t& offset) {\n-  const TypePtr* adr_type = nullptr;\n-  offset = 0;\n-  const Node* base = mach->get_base_and_disp(offset, adr_type);\n-\n-  if (base == nullptr || base == NodeSentinel) {\n-    return nullptr;\n-  }\n-\n-  if (offset == 0 && base->is_Mach() && base->as_Mach()->ideal_Opcode() == Op_AddP) {\n-    \/\/ The memory address is computed by 'base' and fed to 'mach' via an\n-    \/\/ indirect memory operand (indicated by offset == 0). The ultimate base and\n-    \/\/ offset can be fetched directly from the inputs and Ideal type of 'base'.\n-    const TypeOopPtr* oopptr = base->bottom_type()->isa_oopptr();\n-    if (oopptr == nullptr) return nullptr;\n-    offset = oopptr->offset();\n-    \/\/ Even if 'base' is not an Ideal AddP node anymore, Matcher::ReduceInst()\n-    \/\/ guarantees that the base address is still available at the same slot.\n-    base = base->in(AddPNode::Base);\n-    assert(base != nullptr, \"\");\n-  }\n-\n-  if (is_undefined(offset) || (is_concrete(offset) && offset < 0)) {\n-    return nullptr;\n-  }\n-\n-  return look_through_node(base);\n-}\n-\n-\/\/ Whether a phi node corresponds to an array allocation.\n-\/\/ This test is incomplete: in some edge cases, it might return false even\n-\/\/ though the node does correspond to an array allocation.\n-static bool is_array_allocation(const Node* phi) {\n-  precond(phi->is_Phi());\n-  \/\/ Check whether phi has a successor cast (CheckCastPP) to Java array pointer,\n-  \/\/ possibly below spill copies and other cast nodes. Limit the exploration to\n-  \/\/ a single path from the phi node consisting of these node types.\n-  const Node* current = phi;\n-  while (true) {\n-    const Node* next = nullptr;\n-    for (DUIterator_Fast imax, i = current->fast_outs(imax); i < imax; i++) {\n-      if (!current->fast_out(i)->isa_Mach()) {\n-        continue;\n-      }\n-      const MachNode* succ = current->fast_out(i)->as_Mach();\n-      if (succ->ideal_Opcode() == Op_CheckCastPP) {\n-        if (succ->get_ptr_type()->isa_aryptr()) {\n-          \/\/ Cast to Java array pointer: phi corresponds to an array allocation.\n-          return true;\n-        }\n-        \/\/ Other cast: record as candidate for further exploration.\n-        next = succ;\n-      } else if (succ->is_SpillCopy() && next == nullptr) {\n-        \/\/ Spill copy, and no better candidate found: record as candidate.\n-        next = succ;\n-      }\n-    }\n-    if (next == nullptr) {\n-      \/\/ No evidence found that phi corresponds to an array allocation, and no\n-      \/\/ candidates available to continue exploring.\n-      return false;\n-    }\n-    \/\/ Continue exploring from the best candidate found.\n-    current = next;\n-  }\n-  ShouldNotReachHere();\n-}\n-\n-\/\/ Match the phi node that connects a TLAB allocation fast path with its slowpath\n-static bool is_allocation(const Node* node) {\n-  if (node->req() != 3) {\n-    return false;\n-  }\n-  const Node* const fast_node = node->in(2);\n-  if (!fast_node->is_Mach()) {\n-    return false;\n-  }\n-  const MachNode* const fast_mach = fast_node->as_Mach();\n-  if (fast_mach->ideal_Opcode() != Op_LoadP) {\n-    return false;\n-  }\n-  const TypePtr* const adr_type = nullptr;\n-  intptr_t offset;\n-  const Node* const base = get_base_and_offset(fast_mach, offset);\n-  if (base == nullptr || !base->is_Mach() || !is_concrete(offset)) {\n-    return false;\n-  }\n-  const MachNode* const base_mach = base->as_Mach();\n-  if (base_mach->ideal_Opcode() != Op_ThreadLocal) {\n-    return false;\n-  }\n-  return offset == in_bytes(Thread::tlab_top_offset());\n-}\n-\n-static void elide_mach_barrier(MachNode* mach) {\n+void ZBarrierSetC2::elide_dominated_barrier(MachNode* mach) const {\n@@ -646,95 +480,0 @@\n-void ZBarrierSetC2::analyze_dominating_barriers_impl(Node_List& accesses, Node_List& access_dominators) const {\n-  Compile* const C = Compile::current();\n-  PhaseCFG* const cfg = C->cfg();\n-\n-  for (uint i = 0; i < accesses.size(); i++) {\n-    MachNode* const access = accesses.at(i)->as_Mach();\n-    intptr_t access_offset;\n-    const Node* const access_obj = get_base_and_offset(access, access_offset);\n-    Block* const access_block = cfg->get_block_for_node(access);\n-    const uint access_index = block_index(access_block, access);\n-\n-    if (access_obj == nullptr) {\n-      \/\/ No information available\n-      continue;\n-    }\n-\n-    for (uint j = 0; j < access_dominators.size(); j++) {\n-     const  Node* const mem = access_dominators.at(j);\n-      if (mem->is_Phi()) {\n-        \/\/ Allocation node\n-        if (mem != access_obj) {\n-          continue;\n-        }\n-        if (is_unknown(access_offset) && !is_array_allocation(mem)) {\n-          \/\/ The accessed address has an unknown offset, but the allocated\n-          \/\/ object cannot be determined to be an array. Avoid eliding in this\n-          \/\/ case, to be on the safe side.\n-          continue;\n-        }\n-        assert((is_concrete(access_offset) && access_offset >= 0) || (is_unknown(access_offset) && is_array_allocation(mem)),\n-               \"candidate allocation-dominated access offsets must be either concrete and nonnegative, or unknown (for array allocations only)\");\n-      } else {\n-        \/\/ Access node\n-        const MachNode* const mem_mach = mem->as_Mach();\n-        intptr_t mem_offset;\n-        const Node* const mem_obj = get_base_and_offset(mem_mach, mem_offset);\n-\n-        if (mem_obj == nullptr ||\n-            !is_concrete(access_offset) ||\n-            !is_concrete(mem_offset)) {\n-          \/\/ No information available\n-          continue;\n-        }\n-\n-        if (mem_obj != access_obj || mem_offset != access_offset) {\n-          \/\/ Not the same addresses, not a candidate\n-          continue;\n-        }\n-        assert(is_concrete(access_offset) && access_offset >= 0,\n-               \"candidate non-allocation-dominated access offsets must be concrete and nonnegative\");\n-      }\n-\n-      Block* mem_block = cfg->get_block_for_node(mem);\n-      const uint mem_index = block_index(mem_block, mem);\n-\n-      if (access_block == mem_block) {\n-        \/\/ Earlier accesses in the same block\n-        if (mem_index < access_index && !block_has_safepoint(mem_block, mem_index + 1, access_index)) {\n-          elide_mach_barrier(access);\n-        }\n-      } else if (mem_block->dominates(access_block)) {\n-        \/\/ Dominating block? Look around for safepoints\n-        ResourceMark rm;\n-        Block_List stack;\n-        VectorSet visited;\n-        stack.push(access_block);\n-        bool safepoint_found = block_has_safepoint(access_block);\n-        while (!safepoint_found && stack.size() > 0) {\n-          const Block* const block = stack.pop();\n-          if (visited.test_set(block->_pre_order)) {\n-            continue;\n-          }\n-          if (block_has_safepoint(block)) {\n-            safepoint_found = true;\n-            break;\n-          }\n-          if (block == mem_block) {\n-            continue;\n-          }\n-\n-          \/\/ Push predecessor blocks\n-          for (uint p = 1; p < block->num_preds(); ++p) {\n-            Block* const pred = cfg->get_block_for_node(block->pred(p));\n-            stack.push(pred);\n-          }\n-        }\n-\n-        if (!safepoint_found) {\n-          elide_mach_barrier(access);\n-        }\n-      }\n-    }\n-  }\n-}\n-\n@@ -810,3 +549,3 @@\n-  analyze_dominating_barriers_impl(loads, load_dominators);\n-  analyze_dominating_barriers_impl(stores, store_dominators);\n-  analyze_dominating_barriers_impl(atomics, atomic_dominators);\n+  elide_dominated_barriers(loads, load_dominators);\n+  elide_dominated_barriers(stores, store_dominators);\n+  elide_dominated_barriers(atomics, atomic_dominators);\n","filename":"src\/hotspot\/share\/gc\/z\/c2\/zBarrierSetC2.cpp","additions":4,"deletions":265,"binary":false,"changes":269,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2015, 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2015, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -102,1 +102,0 @@\n-  void analyze_dominating_barriers_impl(Node_List& accesses, Node_List& access_dominators) const;\n@@ -131,0 +130,1 @@\n+  virtual void elide_dominated_barrier(MachNode* mach) const;\n","filename":"src\/hotspot\/share\/gc\/z\/c2\/zBarrierSetC2.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -548,5 +548,1 @@\n-  if (HasDecorator<decorators, IN_NMETHOD>::value) {\n-    return ZNMethod::load_oop(p, decorators);\n-  } else {\n-    return oop_load_not_in_heap((zpointer*)p);\n-  }\n+  return oop_load_not_in_heap((zpointer*)p);\n","filename":"src\/hotspot\/share\/gc\/z\/zBarrierSet.inline.hpp","additions":1,"deletions":5,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -2727,1 +2727,1 @@\n-      JVMCI_event_1(\"attached to JavaVM[%d] for JVMCI runtime %d\", runtime->get_shared_library_javavm_id(), runtime->id());\n+      JVMCI_event_1(\"attached to JavaVM[\" JLONG_FORMAT \"] for JVMCI runtime %d\", runtime->get_shared_library_javavm_id(), runtime->id());\n@@ -2760,1 +2760,1 @@\n-    JVMCI_event_1(\"detached from JavaVM[%d] for JVMCI runtime %d\",\n+    JVMCI_event_1(\"detached from JavaVM[\" JLONG_FORMAT \"] for JVMCI runtime %d\",\n","filename":"src\/hotspot\/share\/jvmci\/jvmciCompilerToVM.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -71,0 +71,2 @@\n+  DEBUG_ONLY(LOG_TAG(deathtest)) \/* Log Internal death test tag *\/ \\\n+  DEBUG_ONLY(LOG_TAG(deathtest2)) \/* Log Internal death test tag *\/ \\\n","filename":"src\/hotspot\/share\/logging\/logTag.hpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -303,5 +303,0 @@\n-\/\/ Helper for performing accesses in nmethods. These accesses\n-\/\/ may resolve an accessor on a GC barrier set.\n-template <DecoratorSet decorators = DECORATORS_NONE>\n-class NMethodAccess: public Access<IN_NMETHOD | decorators> {};\n-\n@@ -385,1 +380,0 @@\n-    (location_decorators ^ IN_NMETHOD) == 0 ||\n","filename":"src\/hotspot\/share\/oops\/access.hpp","additions":0,"deletions":6,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -176,1 +176,0 @@\n-\/\/ * IN_NMETHOD: The access is performed inside of an nmethod.\n@@ -179,2 +178,1 @@\n-const DecoratorSet IN_NMETHOD         = UCONST64(1) << 20;\n-const DecoratorSet IN_DECORATOR_MASK  = IN_HEAP | IN_NATIVE | IN_NMETHOD;\n+const DecoratorSet IN_DECORATOR_MASK  = IN_HEAP | IN_NATIVE;\n@@ -188,3 +186,3 @@\n-const DecoratorSet IS_ARRAY              = UCONST64(1) << 21;\n-const DecoratorSet IS_DEST_UNINITIALIZED = UCONST64(1) << 22;\n-const DecoratorSet IS_NOT_NULL           = UCONST64(1) << 23;\n+const DecoratorSet IS_ARRAY              = UCONST64(1) << 20;\n+const DecoratorSet IS_DEST_UNINITIALIZED = UCONST64(1) << 21;\n+const DecoratorSet IS_NOT_NULL           = UCONST64(1) << 22;\n@@ -204,6 +202,6 @@\n-const DecoratorSet ARRAYCOPY_CHECKCAST            = UCONST64(1) << 24;\n-const DecoratorSet ARRAYCOPY_NOTNULL              = UCONST64(1) << 25;\n-const DecoratorSet ARRAYCOPY_DISJOINT             = UCONST64(1) << 26;\n-const DecoratorSet ARRAYCOPY_ARRAYOF              = UCONST64(1) << 27;\n-const DecoratorSet ARRAYCOPY_ATOMIC               = UCONST64(1) << 28;\n-const DecoratorSet ARRAYCOPY_ALIGNED              = UCONST64(1) << 29;\n+const DecoratorSet ARRAYCOPY_CHECKCAST            = UCONST64(1) << 23;\n+const DecoratorSet ARRAYCOPY_NOTNULL              = UCONST64(1) << 24;\n+const DecoratorSet ARRAYCOPY_DISJOINT             = UCONST64(1) << 25;\n+const DecoratorSet ARRAYCOPY_ARRAYOF              = UCONST64(1) << 26;\n+const DecoratorSet ARRAYCOPY_ATOMIC               = UCONST64(1) << 27;\n+const DecoratorSet ARRAYCOPY_ALIGNED              = UCONST64(1) << 28;\n@@ -218,2 +216,2 @@\n-const DecoratorSet ACCESS_READ                    = UCONST64(1) << 30;\n-const DecoratorSet ACCESS_WRITE                   = UCONST64(1) << 31;\n+const DecoratorSet ACCESS_READ                    = UCONST64(1) << 29;\n+const DecoratorSet ACCESS_WRITE                   = UCONST64(1) << 30;\n@@ -222,1 +220,1 @@\n-const DecoratorSet DECORATOR_LAST = UCONST64(1) << 31;\n+const DecoratorSet DECORATOR_LAST = UCONST64(1) << 30;\n","filename":"src\/hotspot\/share\/oops\/accessDecorators.hpp","additions":13,"deletions":15,"binary":false,"changes":28,"status":"modified"},{"patch":"@@ -1985,33 +1985,0 @@\n-int Method::invocation_count() const {\n-  MethodCounters* mcs = method_counters();\n-  MethodData* mdo = method_data();\n-  if (((mcs != nullptr) ? mcs->invocation_counter()->carry() : false) ||\n-      ((mdo != nullptr) ? mdo->invocation_counter()->carry() : false)) {\n-    return InvocationCounter::count_limit;\n-  } else {\n-    return ((mcs != nullptr) ? mcs->invocation_counter()->count() : 0) +\n-           ((mdo != nullptr) ? mdo->invocation_counter()->count() : 0);\n-  }\n-}\n-\n-int Method::backedge_count() const {\n-  MethodCounters* mcs = method_counters();\n-  MethodData* mdo = method_data();\n-  if (((mcs != nullptr) ? mcs->backedge_counter()->carry() : false) ||\n-      ((mdo != nullptr) ? mdo->backedge_counter()->carry() : false)) {\n-    return InvocationCounter::count_limit;\n-  } else {\n-    return ((mcs != nullptr) ? mcs->backedge_counter()->count() : 0) +\n-           ((mdo != nullptr) ? mdo->backedge_counter()->count() : 0);\n-  }\n-}\n-\n-int Method::highest_comp_level() const {\n-  const MethodCounters* mcs = method_counters();\n-  if (mcs != nullptr) {\n-    return mcs->highest_comp_level();\n-  } else {\n-    return CompLevel_none;\n-  }\n-}\n-\n","filename":"src\/hotspot\/share\/oops\/method.cpp","additions":0,"deletions":33,"binary":false,"changes":33,"status":"modified"},{"patch":"@@ -262,1 +262,1 @@\n-  int highest_comp_level() const;\n+  inline int highest_comp_level() const;\n@@ -341,2 +341,2 @@\n-  int invocation_count() const;\n-  int backedge_count() const;\n+  inline int invocation_count() const;\n+  inline int backedge_count() const;\n@@ -351,1 +351,1 @@\n-  int interpreter_invocation_count()            { return invocation_count();          }\n+  inline int interpreter_invocation_count() const;\n","filename":"src\/hotspot\/share\/oops\/method.hpp","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -31,0 +31,1 @@\n+#include \"code\/nmethod.inline.hpp\"\n@@ -32,0 +33,1 @@\n+#include \"oops\/methodData.inline.hpp\"\n@@ -199,0 +201,37 @@\n+inline int Method::invocation_count() const {\n+  MethodCounters* mcs = method_counters();\n+  MethodData* mdo = method_data();\n+  if (((mcs != nullptr) ? mcs->invocation_counter()->carry() : false) ||\n+      ((mdo != nullptr) ? mdo->invocation_counter()->carry() : false)) {\n+    return InvocationCounter::count_limit;\n+  } else {\n+    return ((mcs != nullptr) ? mcs->invocation_counter()->count() : 0) +\n+           ((mdo != nullptr) ? mdo->invocation_counter()->count() : 0);\n+  }\n+}\n+\n+inline int Method::backedge_count() const {\n+  MethodCounters* mcs = method_counters();\n+  MethodData* mdo = method_data();\n+  if (((mcs != nullptr) ? mcs->backedge_counter()->carry() : false) ||\n+      ((mdo != nullptr) ? mdo->backedge_counter()->carry() : false)) {\n+    return InvocationCounter::count_limit;\n+  } else {\n+    return ((mcs != nullptr) ? mcs->backedge_counter()->count() : 0) +\n+           ((mdo != nullptr) ? mdo->backedge_counter()->count() : 0);\n+  }\n+}\n+\n+inline int Method::highest_comp_level() const {\n+  const MethodCounters* mcs = method_counters();\n+  if (mcs != nullptr) {\n+    return mcs->highest_comp_level();\n+  } else {\n+    return CompLevel_none;\n+  }\n+}\n+\n+inline int Method::interpreter_invocation_count() const {\n+  return invocation_count();\n+}\n+\n","filename":"src\/hotspot\/share\/oops\/method.inline.hpp","additions":39,"deletions":0,"binary":false,"changes":39,"status":"modified"},{"patch":"@@ -36,0 +36,1 @@\n+#include \"oops\/method.inline.hpp\"\n","filename":"src\/hotspot\/share\/oops\/methodData.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -707,0 +707,2 @@\n+static bool AndIL_is_zero_element_under_mask(const PhaseGVN* phase, const Node* expr, const Node* mask, BasicType bt);\n+\n@@ -708,2 +710,2 @@\n-  \/\/ patterns similar to (v << 2) & 3\n-  if (AndIL_shift_and_mask_is_always_zero(phase, in(1), in(2), T_INT, true)) {\n+  if (AndIL_is_zero_element_under_mask(phase, in(1), in(2), T_INT) ||\n+      AndIL_is_zero_element_under_mask(phase, in(2), in(1), T_INT)) {\n@@ -755,2 +757,2 @@\n-  \/\/ pattern similar to (v1 + (v2 << 2)) & 3 transformed to v1 & 3\n-  Node* progress = AndIL_add_shift_and_mask(phase, T_INT);\n+  \/\/ Simplify (v1 + v2) & mask to v1 & mask or v2 & mask when possible.\n+  Node* progress = AndIL_sum_and_mask(phase, T_INT);\n@@ -839,2 +841,2 @@\n-  \/\/ patterns similar to (v << 2) & 3\n-  if (AndIL_shift_and_mask_is_always_zero(phase, in(1), in(2), T_LONG, true)) {\n+  if (AndIL_is_zero_element_under_mask(phase, in(1), in(2), T_LONG) ||\n+      AndIL_is_zero_element_under_mask(phase, in(2), in(1), T_LONG)) {\n@@ -887,2 +889,2 @@\n-  \/\/ pattern similar to (v1 + (v2 << 2)) & 3 transformed to v1 & 3\n-  Node* progress = AndIL_add_shift_and_mask(phase, T_LONG);\n+  \/\/ Simplify (v1 + v2) & mask to v1 & mask or v2 & mask when possible.\n+  Node* progress = AndIL_sum_and_mask(phase, T_LONG);\n@@ -2152,24 +2154,8 @@\n-\/\/ Given an expression (AndX shift mask) or (AndX mask shift),\n-\/\/ determine if the AndX must always produce zero, because the\n-\/\/ the shift (x<<N) is bitwise disjoint from the mask #M.\n-\/\/ The X in AndX must be I or L, depending on bt.\n-\/\/ Specifically, the following cases fold to zero,\n-\/\/ when the shift value N is large enough to zero out\n-\/\/ all the set positions of the and-mask M.\n-\/\/   (AndI (LShiftI _ #N) #M) => #0\n-\/\/   (AndL (LShiftL _ #N) #M) => #0\n-\/\/   (AndL (ConvI2L (LShiftI _ #N)) #M) => #0\n-\/\/ The M and N values must satisfy ((-1 << N) & M) == 0.\n-\/\/ Because the optimization might work for a non-constant\n-\/\/ mask M, we check the AndX for both operand orders.\n-bool MulNode::AndIL_shift_and_mask_is_always_zero(PhaseGVN* phase, Node* shift, Node* mask, BasicType bt, bool check_reverse) {\n-  if (mask == nullptr || shift == nullptr) {\n-    return false;\n-  }\n-  const TypeInteger* mask_t = phase->type(mask)->isa_integer(bt);\n-  if (mask_t == nullptr || phase->type(shift)->isa_integer(bt) == nullptr) {\n-    return false;\n-  }\n-  shift = shift->uncast();\n-  if (shift == nullptr) {\n-    return false;\n+\/\/------------------------------ Sum & Mask ------------------------------\n+\n+\/\/ Returns a lower bound on the number of trailing zeros in expr.\n+static jint AndIL_min_trailing_zeros(const PhaseGVN* phase, const Node* expr, BasicType bt) {\n+  expr = expr->uncast();\n+  const TypeInteger* type = phase->type(expr)->isa_integer(bt);\n+  if (type == nullptr) {\n+    return 0;\n@@ -2177,2 +2163,4 @@\n-  if (phase->type(shift)->isa_integer(bt) == nullptr) {\n-    return false;\n+\n+  if (type->is_con()) {\n+    jlong con = type->get_con_as_long(bt);\n+    return con == 0L ? (type2aelembytes(bt) * BitsPerByte) : count_trailing_zeros(con);\n@@ -2180,2 +2168,3 @@\n-  BasicType shift_bt = bt;\n-  if (bt == T_LONG && shift->Opcode() == Op_ConvI2L) {\n+\n+  if (expr->Opcode() == Op_ConvI2L) {\n+    expr = expr->in(1)->uncast();\n@@ -2183,15 +2172,1 @@\n-    Node* val = shift->in(1);\n-    if (val == nullptr) {\n-      return false;\n-    }\n-    val = val->uncast();\n-    if (val == nullptr) {\n-      return false;\n-    }\n-    if (val->Opcode() == Op_LShiftI) {\n-      shift_bt = T_INT;\n-      shift = val;\n-      if (phase->type(shift)->isa_integer(bt) == nullptr) {\n-        return false;\n-      }\n-    }\n+    type = phase->type(expr)->isa_int();\n@@ -2199,6 +2174,6 @@\n-  if (shift->Opcode() != Op_LShift(shift_bt)) {\n-    if (check_reverse &&\n-        (mask->Opcode() == Op_LShift(bt) ||\n-         (bt == T_LONG && mask->Opcode() == Op_ConvI2L))) {\n-      \/\/ try it the other way around\n-      return AndIL_shift_and_mask_is_always_zero(phase, mask, shift, bt, false);\n+\n+  \/\/ Pattern: expr = (x << shift)\n+  if (expr->Opcode() == Op_LShift(bt)) {\n+    const TypeInt* shift_t = phase->type(expr->in(2))->isa_int();\n+    if (shift_t == nullptr || !shift_t->is_con()) {\n+      return 0;\n@@ -2206,5 +2181,6 @@\n-    return false;\n-  }\n-  Node* shift2 = shift->in(2);\n-  if (shift2 == nullptr) {\n-    return false;\n+    \/\/ We need to truncate the shift, as it may not have been canonicalized yet.\n+    \/\/ T_INT:  0..31 -> shift_mask = 4 * 8 - 1 = 31\n+    \/\/ T_LONG: 0..63 -> shift_mask = 8 * 8 - 1 = 63\n+    \/\/ (JLS: \"Shift Operators\")\n+    jint shift_mask = type2aelembytes(bt) * BitsPerByte - 1;\n+    return shift_t->get_con() & shift_mask;\n@@ -2212,2 +2188,48 @@\n-  const Type* shift2_t = phase->type(shift2);\n-  if (!shift2_t->isa_int() || !shift2_t->is_int()->is_con()) {\n+\n+  return 0;\n+}\n+\n+\/\/ Checks whether expr is neutral additive element (zero) under mask,\n+\/\/ i.e. whether an expression of the form:\n+\/\/   (AndX (AddX (expr addend) mask)\n+\/\/   (expr + addend) & mask\n+\/\/ is equivalent to\n+\/\/   (AndX addend mask)\n+\/\/   addend & mask\n+\/\/ for any addend.\n+\/\/ (The X in AndX must be I or L, depending on bt).\n+\/\/\n+\/\/ We check for the sufficient condition when the lowest set bit in expr is higher than\n+\/\/ the highest set bit in mask, i.e.:\n+\/\/ expr: eeeeee0000000000000\n+\/\/ mask: 000000mmmmmmmmmmmmm\n+\/\/             <--w bits--->\n+\/\/ We do not test for other cases.\n+\/\/\n+\/\/ Correctness:\n+\/\/   Given \"expr\" with at least \"w\" trailing zeros,\n+\/\/   let \"mod = 2^w\", \"suffix_mask = mod - 1\"\n+\/\/\n+\/\/   Since \"mask\" only has bits set where \"suffix_mask\" does, we have:\n+\/\/     mask = suffix_mask & mask     (SUFFIX_MASK)\n+\/\/\n+\/\/   And since expr only has bits set above w, and suffix_mask only below:\n+\/\/     expr & suffix_mask == 0     (NO_BIT_OVERLAP)\n+\/\/\n+\/\/   From unsigned modular arithmetic (with unsigned modulo %), and since mod is\n+\/\/   a power of 2, and we are computing in a ring of powers of 2, we know that\n+\/\/     (x + y) % mod         = (x % mod         + y) % mod\n+\/\/     (x + y) & suffix_mask = (x & suffix_mask + y) & suffix_mask       (MOD_ARITH)\n+\/\/\n+\/\/   We can now prove the equality:\n+\/\/     (expr               + addend)               & mask\n+\/\/   = (expr               + addend) & suffix_mask & mask    (SUFFIX_MASK)\n+\/\/   = (expr & suffix_mask + addend) & suffix_mask & mask    (MOD_ARITH)\n+\/\/   = (0                  + addend) & suffix_mask & mask    (NO_BIT_OVERLAP)\n+\/\/   =                       addend                & mask    (SUFFIX_MASK)\n+\/\/\n+\/\/ Hence, an expr with at least w trailing zeros is a neutral additive element under any mask with bit width w.\n+static bool AndIL_is_zero_element_under_mask(const PhaseGVN* phase, const Node* expr, const Node* mask, BasicType bt) {\n+  \/\/ When the mask is negative, it has the most significant bit set.\n+  const TypeInteger* mask_t = phase->type(mask)->isa_integer(bt);\n+  if (mask_t == nullptr || mask_t->lo_as_long() < 0) {\n@@ -2217,3 +2239,4 @@\n-  jint shift_con = shift2_t->is_int()->get_con() & ((shift_bt == T_INT ? BitsPerJavaInteger : BitsPerJavaLong) - 1);\n-  if ((((jlong)1) << shift_con) > mask_t->hi_as_long() && mask_t->lo_as_long() >= 0) {\n-    return true;\n+  \/\/ When the mask is constant zero, we defer to MulNode::Value to eliminate the entire AndX operation.\n+  if (mask_t->hi_as_long() == 0) {\n+    assert(mask_t->lo_as_long() == 0, \"checked earlier\");\n+    return false;\n@@ -2222,1 +2245,3 @@\n-  return false;\n+  jint mask_bit_width = BitsPerLong - count_leading_zeros(mask_t->hi_as_long());\n+  jint expr_trailing_zeros = AndIL_min_trailing_zeros(phase, expr, bt);\n+  return expr_trailing_zeros >= mask_bit_width;\n@@ -2225,15 +2250,5 @@\n-\/\/ Given an expression (AndX (AddX v1 (LShiftX v2 #N)) #M)\n-\/\/ determine if the AndX must always produce (AndX v1 #M),\n-\/\/ because the shift (v2<<N) is bitwise disjoint from the mask #M.\n-\/\/ The X in AndX will be I or L, depending on bt.\n-\/\/ Specifically, the following cases fold,\n-\/\/ when the shift value N is large enough to zero out\n-\/\/ all the set positions of the and-mask M.\n-\/\/   (AndI (AddI v1 (LShiftI _ #N)) #M) => (AndI v1 #M)\n-\/\/   (AndL (AddI v1 (LShiftL _ #N)) #M) => (AndL v1 #M)\n-\/\/   (AndL (AddL v1 (ConvI2L (LShiftI _ #N))) #M) => (AndL v1 #M)\n-\/\/ The M and N values must satisfy ((-1 << N) & M) == 0.\n-\/\/ Because the optimization might work for a non-constant\n-\/\/ mask M, and because the AddX operands can come in either\n-\/\/ order, we check for every operand order.\n-Node* MulNode::AndIL_add_shift_and_mask(PhaseGVN* phase, BasicType bt) {\n+\/\/ Reduces the pattern:\n+\/\/   (AndX (AddX add1 add2) mask)\n+\/\/ to\n+\/\/   (AndX add1 mask), if add2 is neutral wrt mask (see above), and vice versa.\n+Node* MulNode::AndIL_sum_and_mask(PhaseGVN* phase, BasicType bt) {\n@@ -2242,3 +2257,0 @@\n-  if (add == nullptr || mask == nullptr) {\n-    return nullptr;\n-  }\n@@ -2256,8 +2268,6 @@\n-    if (add1 != nullptr && add2 != nullptr) {\n-      if (AndIL_shift_and_mask_is_always_zero(phase, add1, mask, bt, false)) {\n-        set_req_X(addidx, add2, phase);\n-        return this;\n-      } else if (AndIL_shift_and_mask_is_always_zero(phase, add2, mask, bt, false)) {\n-        set_req_X(addidx, add1, phase);\n-        return this;\n-      }\n+    if (AndIL_is_zero_element_under_mask(phase, add1, mask, bt)) {\n+      set_req_X(addidx, add2, phase);\n+      return this;\n+    } else if (AndIL_is_zero_element_under_mask(phase, add2, mask, bt)) {\n+      set_req_X(addidx, add1, phase);\n+      return this;\n","filename":"src\/hotspot\/share\/opto\/mulnode.cpp","additions":104,"deletions":94,"binary":false,"changes":198,"status":"modified"},{"patch":"@@ -3,1 +3,1 @@\n- * Copyright (c) 2024, Alibaba Group Holding Limited. All rights reserved.\n+ * Copyright (c) 2024, 2025, Alibaba Group Holding Limited. All rights reserved.\n@@ -2435,1 +2435,1 @@\n-    uint max_width = static_cast<uint>(log10(static_cast<double>(C->unique()))) + 2;\n+    uint max_width = (C->unique() == 0 ? 0 : static_cast<uint>(log10(static_cast<double>(C->unique())))) + 2;\n@@ -2437,1 +2437,1 @@\n-    uint width = static_cast<uint>(log10(static_cast<double>(_idx))) + 1 + (is_new ? 0 : 1);\n+    uint width = (_idx == 0 ? 0 : static_cast<uint>(log10(static_cast<double>(_idx)))) + 1 + (is_new ? 0 : 1);\n","filename":"src\/hotspot\/share\/opto\/node.cpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -3,1 +3,1 @@\n- * Copyright (c) 2024, Alibaba Group Holding Limited. All rights reserved.\n+ * Copyright (c) 2024, 2025, Alibaba Group Holding Limited. All rights reserved.\n@@ -344,1 +344,1 @@\n-  \/\/ Each Node is assigned a unique small\/dense number.  This number is used\n+  \/\/ Each Node is assigned a unique small\/dense number. This number is used\n@@ -346,3 +346,1 @@\n-  \/\/ The field _idx is declared constant to defend against inadvertent assignments,\n-  \/\/ since it is used by clients as a naked field. However, the field's value can be\n-  \/\/ changed using the set_idx() method.\n+  \/\/ The value of _idx can be changed using the set_idx() method.\n","filename":"src\/hotspot\/share\/opto\/node.hpp","additions":3,"deletions":5,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -4188,2 +4188,0 @@\n-  const TypeInstPtr* ftip = ft->isa_instptr();\n-  const TypeInstPtr* ktip = kills->isa_instptr();\n@@ -6326,2 +6324,0 @@\n-  const TypeKlassPtr* ftkp = ft->isa_instklassptr();\n-  const TypeKlassPtr* ktkp = kills->isa_instklassptr();\n","filename":"src\/hotspot\/share\/opto\/type.cpp","additions":0,"deletions":4,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -503,1 +503,0 @@\n-  Monitor timer(Mutex::nosafepoint, \"VM_ExitTimer_lock\");\n@@ -512,4 +511,16 @@\n-  \/\/ wait for user threads too. Numbers are in 10 milliseconds.\n-  int wait_time_per_attempt = 10;               \/\/ in milliseconds\n-  int max_wait_attempts_user_thread = UserThreadWaitAttemptsAtExit;\n-  int max_wait_attempts_compiler_thread = 1000; \/\/ at least 10 seconds\n+  \/\/ wait for user threads too.\n+\n+  \/\/ Time per attempt. It is practical to start waiting with 10us delays\n+  \/\/ (around scheduling delay \/ timer slack), and exponentially ramp up\n+  \/\/ to 10ms if compiler threads are not responding.\n+  jlong max_wait_time = millis_to_nanos(10);\n+  jlong wait_time = 10000;\n+\n+  jlong start_time = os::javaTimeNanos();\n+\n+  \/\/ Deadline for user threads in native code.\n+  \/\/ User-settable flag counts \"attempts\" in 10ms units, to a maximum of 10s.\n+  jlong user_threads_deadline = start_time + (UserThreadWaitAttemptsAtExit * millis_to_nanos(10));\n+\n+  \/\/ Deadline for compiler threads: at least 10 seconds.\n+  jlong compiler_threads_deadline = start_time + millis_to_nanos(10000);\n@@ -517,1 +528,0 @@\n-  int attempts = 0;\n@@ -546,0 +556,2 @@\n+    jlong time = os::javaTimeNanos();\n+\n@@ -547,6 +559,7 @@\n-       return 0;\n-    } else if (attempts >= max_wait_attempts_compiler_thread) {\n-       return num_active;\n-    } else if (num_active_compiler_thread == 0 &&\n-               attempts >= max_wait_attempts_user_thread) {\n-       return num_active;\n+      return 0;\n+    }\n+    if (time >= compiler_threads_deadline) {\n+      return num_active;\n+    }\n+    if ((num_active_compiler_thread == 0) && (time >= user_threads_deadline)) {\n+      return num_active;\n@@ -555,4 +568,2 @@\n-    attempts++;\n-\n-    MonitorLocker ml(&timer, Mutex::_no_safepoint_check_flag);\n-    ml.wait(wait_time_per_attempt);\n+    os::naked_short_nanosleep(wait_time);\n+    wait_time = MIN2(max_wait_time, wait_time * 2);\n","filename":"src\/hotspot\/share\/runtime\/vmOperations.cpp","additions":27,"deletions":16,"binary":false,"changes":43,"status":"modified"},{"patch":"@@ -554,0 +554,1 @@\n+  nonstatic_field(CodeBlob,                    _kind,                                         CodeBlobKind)                          \\\n@@ -1924,0 +1925,1 @@\n+  declare_integer_type(CodeBlobKind)                                      \\\n@@ -2381,0 +2383,17 @@\n+  \/****************\/                                                      \\\n+  \/* CodeBlobKind *\/                                                      \\\n+  \/****************\/                                                      \\\n+                                                                          \\\n+  declare_constant(CodeBlobKind::Nmethod)                                 \\\n+  declare_constant(CodeBlobKind::Buffer)                                  \\\n+  declare_constant(CodeBlobKind::Adapter)                                 \\\n+  declare_constant(CodeBlobKind::Vtable)                                  \\\n+  declare_constant(CodeBlobKind::MHAdapter)                               \\\n+  declare_constant(CodeBlobKind::RuntimeStub)                             \\\n+  declare_constant(CodeBlobKind::Deoptimization)                          \\\n+  declare_constant(CodeBlobKind::Safepoint)                               \\\n+  COMPILER2_PRESENT(declare_constant(CodeBlobKind::Exception))            \\\n+  COMPILER2_PRESENT(declare_constant(CodeBlobKind::UncommonTrap))         \\\n+  declare_constant(CodeBlobKind::Upcall)                                  \\\n+  declare_constant(CodeBlobKind::Number_Of_Kinds)                         \\\n+                                                                          \\\n","filename":"src\/hotspot\/share\/runtime\/vmStructs.cpp","additions":19,"deletions":0,"binary":false,"changes":19,"status":"modified"},{"patch":"@@ -158,0 +158,1 @@\n+        jdk.crypto.cryptoki, \/\/ participates in preview features\n","filename":"src\/java.base\/share\/classes\/module-info.java","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -58,1 +58,1 @@\n-    String getHostArchitectureName() {\n+    static String getHostArchitectureName() {\n","filename":"src\/jdk.internal.vm.ci\/share\/classes\/jdk\/vm\/ci\/hotspot\/HotSpotVMConfig.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -52,2 +52,10 @@\n-\/\/ new, single extra label.\n-function mergeAndAppendTypeInfo(extra_label, bottom_type, phase_type) {\n+\/\/ new, single extra label. For memory nodes, add an extra label with the memory\n+\/\/ slice, extracted from the dump_spec field.\n+function mergeAndAppendTypeInfo(extra_label, bottom_type, phase_type, category, dump_spec) {\n+  new_extra_label = extra_label == null ? \"\" : (extra_label + \" \");\n+  if (category == \"memory\") {\n+    m = \/idx=([^\\s]+);\/.exec(dump_spec);\n+    if (m != null) {\n+      return new_extra_label + \"mem: \" + m[1];\n+    }\n+  }\n@@ -70,1 +78,0 @@\n-  new_extra_label = extra_label == null ? \"\" : (extra_label + \" \");\n@@ -76,2 +83,2 @@\n-             [\"extra_label\", \"bottom_type\", \"phase_type\"], \"extra_label\",\n-             function(propertyValues) {return mergeAndAppendTypeInfo(propertyValues[0], propertyValues[1], propertyValues[2]);});\n+             [\"extra_label\", \"bottom_type\", \"phase_type\", \"category\", \"dump_spec\"], \"extra_label\",\n+             function(propertyValues) {return mergeAndAppendTypeInfo(propertyValues[0], propertyValues[1], propertyValues[2], propertyValues[3], propertyValues[4]);});\n","filename":"src\/utils\/IdealGraphVisualizer\/ServerCompiler\/src\/main\/resources\/com\/sun\/hotspot\/igv\/servercompiler\/filters\/showTypes.filter","additions":12,"deletions":5,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -652,0 +652,3 @@\n+tier1_sources = \\\n+   sources\n+\n@@ -657,1 +660,2 @@\n-  :tier1_serviceability\n+  :tier1_serviceability \\\n+  :tier1_sources\n","filename":"test\/hotspot\/jtreg\/TEST.groups","additions":5,"deletions":1,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -703,4 +703,0 @@\n-# jdk_swing Ubuntu 23.04 specific\n-\n-javax\/swing\/JComboBox\/TestComboBoxComponentRendering.java 8309734 linux-all\n-\n","filename":"test\/jdk\/ProblemList.txt","additions":0,"deletions":4,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -32,0 +32,1 @@\n+ * @enablePreview\n","filename":"test\/langtools\/tools\/javac\/classfiles\/attributes\/innerclasses\/InnerAnnotationsInInnerClassTest.java","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -32,0 +32,1 @@\n+ * @enablePreview\n","filename":"test\/langtools\/tools\/javac\/classfiles\/attributes\/innerclasses\/InnerAnnotationsInInnerEnumTest.java","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -32,0 +32,1 @@\n+ * @enablePreview\n","filename":"test\/langtools\/tools\/javac\/classfiles\/attributes\/innerclasses\/InnerClassesInAnonymousClassTest.java","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -32,0 +32,1 @@\n+ * @enablePreview\n","filename":"test\/langtools\/tools\/javac\/classfiles\/attributes\/innerclasses\/InnerClassesInInnerAnnotationTest.java","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -32,0 +32,1 @@\n+ * @enablePreview\n","filename":"test\/langtools\/tools\/javac\/classfiles\/attributes\/innerclasses\/InnerClassesInInnerClassTest.java","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -32,0 +32,1 @@\n+ * @enablePreview\n","filename":"test\/langtools\/tools\/javac\/classfiles\/attributes\/innerclasses\/InnerClassesInInnerEnumTest.java","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -32,0 +32,1 @@\n+ * @enablePreview\n","filename":"test\/langtools\/tools\/javac\/classfiles\/attributes\/innerclasses\/InnerClassesInInnerInterfaceTest.java","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -32,0 +32,1 @@\n+ * @enablePreview\n","filename":"test\/langtools\/tools\/javac\/classfiles\/attributes\/innerclasses\/InnerClassesInLocalClassTest.java","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -34,0 +34,1 @@\n+ * @enablePreview\n","filename":"test\/langtools\/tools\/javac\/classfiles\/attributes\/innerclasses\/InnerClassesTest.java","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -32,0 +32,1 @@\n+ * @enablePreview\n","filename":"test\/langtools\/tools\/javac\/classfiles\/attributes\/innerclasses\/InnerEnumInInnerAnnotationTest.java","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -32,0 +32,1 @@\n+ * @enablePreview\n","filename":"test\/langtools\/tools\/javac\/classfiles\/attributes\/innerclasses\/InnerEnumInInnerEnumTest.java","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -32,0 +32,1 @@\n+ * @enablePreview\n","filename":"test\/langtools\/tools\/javac\/classfiles\/attributes\/innerclasses\/InnerEnumInInnerInterfaceTest.java","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -32,0 +32,1 @@\n+ * @enablePreview\n","filename":"test\/langtools\/tools\/javac\/classfiles\/attributes\/innerclasses\/InnerEnumsInInnerClassTest.java","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -32,0 +32,1 @@\n+ * @enablePreview\n","filename":"test\/langtools\/tools\/javac\/classfiles\/attributes\/innerclasses\/InnerInterfacesInInnerClassTest.java","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -32,0 +32,1 @@\n+ * @enablePreview\n","filename":"test\/langtools\/tools\/javac\/classfiles\/attributes\/innerclasses\/InnerInterfacesInInnerEnumTest.java","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"}]}