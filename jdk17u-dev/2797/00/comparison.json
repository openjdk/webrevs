{"files":[{"patch":"@@ -897,1 +897,5 @@\n-  ShenandoahHeapLocker locker(lock());\n+  \/\/ If we are dealing with mutator allocation, then we may need to block for safepoint.\n+  \/\/ We cannot block for safepoint for GC allocations, because there is a high chance\n+  \/\/ we are already running at safepoint or from stack watermark machinery, and we cannot\n+  \/\/ block again.\n+  ShenandoahHeapLocker locker(lock(), req.is_mutator_alloc());\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.cpp","additions":5,"deletions":1,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -31,0 +31,1 @@\n+#include \"runtime\/interfaceSupport.inline.hpp\"\n@@ -34,0 +35,37 @@\n+\/\/ These are inline variants of Thread::SpinAcquire with optional blocking in VM.\n+\n+class ShenandoahNoBlockOp : public StackObj {\n+public:\n+  ShenandoahNoBlockOp(JavaThread* java_thread) {\n+    assert(java_thread == NULL, \"Should not pass anything\");\n+  }\n+};\n+\n+void ShenandoahLock::contended_lock(bool allow_block_for_safepoint) {\n+  Thread* thread = Thread::current();\n+  if (allow_block_for_safepoint && thread->is_Java_thread()) {\n+    contended_lock_internal<ThreadBlockInVM>(static_cast<JavaThread*>(thread));\n+  } else {\n+    contended_lock_internal<ShenandoahNoBlockOp>(NULL);\n+  }\n+}\n+\n+template<typename BlockOp>\n+void ShenandoahLock::contended_lock_internal(JavaThread* java_thread) {\n+  int ctr = 0;\n+  int yields = 0;\n+  while (Atomic::cmpxchg(&_state, unlocked, locked) != unlocked) {\n+    if ((++ctr & 0xFFF) == 0) {\n+      BlockOp block(java_thread);\n+      if (yields > 5) {\n+        os::naked_short_sleep(1);\n+      } else {\n+        os::naked_yield();\n+        yields++;\n+      }\n+    } else {\n+      SpinPause();\n+    }\n+  }\n+}\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahLock.cpp","additions":38,"deletions":0,"binary":false,"changes":38,"status":"modified"},{"patch":"@@ -38,1 +38,1 @@\n-  volatile int _state;\n+  volatile LockState _state;\n@@ -43,0 +43,3 @@\n+  template<typename BlockOp>\n+  void contended_lock_internal(JavaThread* java_thread);\n+\n@@ -46,10 +49,11 @@\n-  void lock() {\n-#ifdef ASSERT\n-    assert(_owner != Thread::current(), \"reentrant locking attempt, would deadlock\");\n-#endif\n-    Thread::SpinAcquire(&_state, \"Shenandoah Heap Lock\");\n-#ifdef ASSERT\n-    assert(_state == locked, \"must be locked\");\n-    assert(_owner == NULL, \"must not be owned\");\n-    _owner = Thread::current();\n-#endif\n+  void lock(bool allow_block_for_safepoint) {\n+    assert(Atomic::load(&_owner) != Thread::current(), \"reentrant locking attempt, would deadlock\");\n+\n+    \/\/ Try to lock fast, or dive into contended lock handling.\n+    if (Atomic::cmpxchg(&_state, unlocked, locked) != unlocked) {\n+      contended_lock(allow_block_for_safepoint);\n+    }\n+\n+    assert(Atomic::load(&_state) == locked, \"must be locked\");\n+    assert(Atomic::load(&_owner) == NULL, \"must not be owned\");\n+    DEBUG_ONLY(Atomic::store(&_owner, Thread::current());)\n@@ -59,5 +63,4 @@\n-#ifdef ASSERT\n-    assert (_owner == Thread::current(), \"sanity\");\n-    _owner = NULL;\n-#endif\n-    Thread::SpinRelease(&_state);\n+    assert(Atomic::load(&_owner) == Thread::current(), \"sanity\");\n+    DEBUG_ONLY(Atomic::store(&_owner, (Thread*)NULL);)\n+    OrderAccess::fence();\n+    Atomic::store(&_state, unlocked);\n@@ -66,0 +69,2 @@\n+  void contended_lock(bool allow_block_for_safepoint);\n+\n@@ -80,1 +85,1 @@\n-  ShenandoahLocker(ShenandoahLock* lock) : _lock(lock) {\n+  ShenandoahLocker(ShenandoahLock* lock, bool allow_block_for_safepoint = false) : _lock(lock) {\n@@ -82,1 +87,1 @@\n-      _lock->lock();\n+      _lock->lock(allow_block_for_safepoint);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahLock.hpp","additions":23,"deletions":18,"binary":false,"changes":41,"status":"modified"}]}