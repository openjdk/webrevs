{"files":[{"patch":"@@ -1200,0 +1200,1 @@\n+  constexpr size_t batch_size = 128;\n@@ -1203,1 +1204,5 @@\n-  for (size_t bucket_it = 0; bucket_it < table->_size; bucket_it++) {\n+  size_t num_batches = table->_size \/ batch_size;\n+  for (size_t batch_start = 0; batch_start < _table->_size; batch_start += batch_size) {\n+    \/\/ We batch the use of ScopedCS here as it has been found to be quite expensive to\n+    \/\/ invoke it for every single bucket.\n+    size_t batch_end = MIN2(batch_start + batch_size, _table->_size);\n@@ -1205,10 +1210,13 @@\n-    size_t count = 0;\n-    Bucket* bucket = table->get_bucket(bucket_it);\n-    if (bucket->have_redirect() || bucket->is_locked()) {\n-      continue;\n-    }\n-    Node* current_node = bucket->first();\n-    while (current_node != NULL) {\n-      ++count;\n-      literal_bytes += vs_f(current_node->value());\n-      current_node = current_node->next();\n+    for (size_t bucket_it = batch_start; bucket_it < batch_end; bucket_it++) {\n+      size_t count = 0;\n+      Bucket* bucket = table->get_bucket(bucket_it);\n+      if (bucket->have_redirect() || bucket->is_locked()) {\n+        continue;\n+      }\n+      Node* current_node = bucket->first();\n+      while (current_node != nullptr) {\n+        ++count;\n+        literal_bytes += vs_f(current_node->value());\n+        current_node = current_node->next();\n+      }\n+      summary.add((double)count);\n@@ -1216,1 +1224,0 @@\n-    summary.add((double)count);\n","filename":"src\/hotspot\/share\/utilities\/concurrentHashTable.inline.hpp","additions":19,"deletions":12,"binary":false,"changes":31,"status":"modified"}]}