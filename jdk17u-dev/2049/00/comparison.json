{"files":[{"patch":"@@ -168,0 +168,76 @@\n+  \/\/ Bitwise logical operations (and, or, xor)\n+  \/\/\n+  \/\/ All operations apply the corresponding operation to the value in dest and\n+  \/\/ bits, storing the result in dest. They return either the old value\n+  \/\/ (fetch_then_BITOP) or the newly updated value (BITOP_then_fetch).\n+  \/\/\n+  \/\/ Requirements:\n+  \/\/ - T is an integral type\n+  \/\/ - sizeof(T) == sizeof(int) || sizeof(T) == sizeof(void*)\n+\n+  \/\/ Performs atomic bitwise-and of *dest and bits, storing the result in\n+  \/\/ *dest.  Returns the prior value of *dest.  That is, atomically performs\n+  \/\/ this sequence of operations:\n+  \/\/ { tmp = *dest; *dest &= bits; return tmp; }\n+  template<typename T>\n+  static T fetch_then_and(volatile T* dest, T bits,\n+                          atomic_memory_order order = memory_order_conservative) {\n+    static_assert(std::is_integral<T>::value, \"bitop with non-integral type\");\n+    return PlatformBitops<sizeof(T)>().fetch_then_and(dest, bits, order);\n+  }\n+\n+  \/\/ Performs atomic bitwise-or of *dest and bits, storing the result in\n+  \/\/ *dest.  Returns the prior value of *dest.  That is, atomically performs\n+  \/\/ this sequence of operations:\n+  \/\/ { tmp = *dest; *dest |= bits; return tmp; }\n+  template<typename T>\n+  static T fetch_then_or(volatile T* dest, T bits,\n+                         atomic_memory_order order = memory_order_conservative) {\n+    static_assert(std::is_integral<T>::value, \"bitop with non-integral type\");\n+    return PlatformBitops<sizeof(T)>().fetch_then_or(dest, bits, order);\n+  }\n+\n+  \/\/ Performs atomic bitwise-xor of *dest and bits, storing the result in\n+  \/\/ *dest.  Returns the prior value of *dest.  That is, atomically performs\n+  \/\/ this sequence of operations:\n+  \/\/ { tmp = *dest; *dest ^= bits; return tmp; }\n+  template<typename T>\n+  static T fetch_then_xor(volatile T* dest, T bits,\n+                          atomic_memory_order order = memory_order_conservative) {\n+    static_assert(std::is_integral<T>::value, \"bitop with non-integral type\");\n+    return PlatformBitops<sizeof(T)>().fetch_then_xor(dest, bits, order);\n+  }\n+\n+  \/\/ Performs atomic bitwise-and of *dest and bits, storing the result in\n+  \/\/ *dest.  Returns the new value of *dest.  That is, atomically performs\n+  \/\/ this operation:\n+  \/\/ { return *dest &= bits; }\n+  template<typename T>\n+  static T and_then_fetch(volatile T* dest, T bits,\n+                          atomic_memory_order order = memory_order_conservative) {\n+    static_assert(std::is_integral<T>::value, \"bitop with non-integral type\");\n+    return PlatformBitops<sizeof(T)>().and_then_fetch(dest, bits, order);\n+  }\n+\n+  \/\/ Performs atomic bitwise-or of *dest and bits, storing the result in\n+  \/\/ *dest.  Returns the new value of *dest.  That is, atomically performs\n+  \/\/ this operation:\n+  \/\/ { return *dest |= bits; }\n+  template<typename T>\n+  static T or_then_fetch(volatile T* dest, T bits,\n+                         atomic_memory_order order = memory_order_conservative) {\n+    static_assert(std::is_integral<T>::value, \"bitop with non-integral type\");\n+    return PlatformBitops<sizeof(T)>().or_then_fetch(dest, bits, order);\n+  }\n+\n+  \/\/ Performs atomic bitwise-xor of *dest and bits, storing the result in\n+  \/\/ *dest.  Returns the new value of *dest.  That is, atomically performs\n+  \/\/ this operation:\n+  \/\/ { return *dest ^= bits; }\n+  template<typename T>\n+  static T xor_then_fetch(volatile T* dest, T bits,\n+                          atomic_memory_order order = memory_order_conservative) {\n+    static_assert(std::is_integral<T>::value, \"bitop with non-integral type\");\n+    return PlatformBitops<sizeof(T)>().xor_then_fetch(dest, bits, order);\n+  }\n+\n@@ -376,0 +452,38 @@\n+\n+  \/\/ Platform-specific implementation of the bitops (and, or, xor).  Support\n+  \/\/ for sizes of 4 bytes and (if different) pointer size bytes are required.\n+  \/\/ The class is a function object that must be default constructable, with\n+  \/\/ these requirements:\n+  \/\/\n+  \/\/ - T is an integral type.\n+  \/\/ - dest is of type T*.\n+  \/\/ - bits is of type T.\n+  \/\/ - order is of type atomic_memory_order.\n+  \/\/ - platform_bitops is an object of type PlatformBitops<sizeof(T)>.\n+  \/\/\n+  \/\/ Then\n+  \/\/  platform_bitops.fetch_then_and(dest, bits, order)\n+  \/\/  platform_bitops.fetch_then_or(dest, bits, order)\n+  \/\/  platform_bitops.fetch_then_xor(dest, bits, order)\n+  \/\/  platform_bitops.and_then_fetch(dest, bits, order)\n+  \/\/  platform_bitops.or_then_fetch(dest, bits, order)\n+  \/\/  platform_bitops.xor_then_fetch(dest, bits, order)\n+  \/\/ must all be valid expressions, returning a result convertible to T.\n+  \/\/\n+  \/\/ A default definition is provided, which implements all of the operations\n+  \/\/ using cmpxchg.\n+  \/\/\n+  \/\/ For each required size, a platform must either use the default or\n+  \/\/ entirely specialize the class for that size by providing all of the\n+  \/\/ required operations.\n+  \/\/\n+  \/\/ The second (bool) template parameter allows platforms to provide a\n+  \/\/ partial specialization with a parameterized size, and is otherwise\n+  \/\/ unused.  The default value for that bool parameter means specializations\n+  \/\/ don't need to mention it.\n+  template<size_t size, bool = true> class PlatformBitops;\n+\n+  \/\/ Helper base classes that may be used to implement PlatformBitops.\n+  class PrefetchBitopsUsingCmpxchg;\n+  class PostfetchBitopsUsingCmpxchg;\n+  class PostfetchBitopsUsingPrefetch;\n@@ -585,0 +699,93 @@\n+\/\/ Implement fetch_then_bitop operations using a CAS loop.\n+class Atomic::PrefetchBitopsUsingCmpxchg {\n+  template<typename T, typename Op>\n+  T bitop(T volatile* dest, atomic_memory_order order, Op operation) const {\n+    T old_value;\n+    T new_value;\n+    T fetched_value = Atomic::load(dest);\n+    do {\n+      old_value = fetched_value;\n+      new_value = operation(old_value);\n+      fetched_value = Atomic::cmpxchg(dest, old_value, new_value, order);\n+    } while (old_value != fetched_value);\n+    return fetched_value;\n+  }\n+\n+public:\n+  template<typename T>\n+  T fetch_then_and(T volatile* dest, T bits, atomic_memory_order order) const {\n+    return bitop(dest, order, [&](T value) -> T { return value & bits; });\n+  }\n+\n+  template<typename T>\n+  T fetch_then_or(T volatile* dest, T bits, atomic_memory_order order) const {\n+    return bitop(dest, order, [&](T value) -> T { return value | bits; });\n+  }\n+\n+  template<typename T>\n+  T fetch_then_xor(T volatile* dest, T bits, atomic_memory_order order) const {\n+    return bitop(dest, order, [&](T value) -> T { return value ^ bits; });\n+  }\n+};\n+\n+\/\/ Implement bitop_then_fetch operations using a CAS loop.\n+class Atomic::PostfetchBitopsUsingCmpxchg {\n+  template<typename T, typename Op>\n+  T bitop(T volatile* dest, atomic_memory_order order, Op operation) const {\n+    T old_value;\n+    T new_value;\n+    T fetched_value = Atomic::load(dest);\n+    do {\n+      old_value = fetched_value;\n+      new_value = operation(old_value);\n+      fetched_value = Atomic::cmpxchg(dest, old_value, new_value, order);\n+    } while (old_value != fetched_value);\n+    return new_value;\n+  }\n+\n+public:\n+  template<typename T>\n+  T and_then_fetch(T volatile* dest, T bits, atomic_memory_order order) const {\n+    return bitop(dest, order, [&](T value) -> T { return value & bits; });\n+  }\n+\n+  template<typename T>\n+  T or_then_fetch(T volatile* dest, T bits, atomic_memory_order order) const {\n+    return bitop(dest, order, [&](T value) -> T { return value | bits; });\n+  }\n+\n+  template<typename T>\n+  T xor_then_fetch(T volatile* dest, T bits, atomic_memory_order order) const {\n+    return bitop(dest, order, [&](T value) -> T { return value ^ bits; });\n+  }\n+};\n+\n+\/\/ Implement bitop_then_fetch operations by calling fetch_then_bitop and\n+\/\/ applying the operation to the result and the bits argument.\n+class Atomic::PostfetchBitopsUsingPrefetch {\n+public:\n+  template<typename T>\n+  T and_then_fetch(T volatile* dest, T bits, atomic_memory_order order) const {\n+    return bits & Atomic::fetch_then_and(dest, bits, order);\n+  }\n+\n+  template<typename T>\n+  T or_then_fetch(T volatile* dest, T bits, atomic_memory_order order) const {\n+    return bits | Atomic::fetch_then_or(dest, bits, order);\n+  }\n+\n+  template<typename T>\n+  T xor_then_fetch(T volatile* dest, T bits, atomic_memory_order order) const {\n+    return bits ^ Atomic::fetch_then_xor(dest, bits, order);\n+  }\n+};\n+\n+\/\/ The default definition uses cmpxchg.  Platforms can override by defining a\n+\/\/ partial specialization providing size, either as a template parameter or as\n+\/\/ a specific value.\n+template<size_t size, bool>\n+class Atomic::PlatformBitops\n+  : public PrefetchBitopsUsingCmpxchg,\n+    public PostfetchBitopsUsingCmpxchg\n+{};\n+\n","filename":"src\/hotspot\/share\/runtime\/atomic.hpp","additions":207,"deletions":0,"binary":false,"changes":207,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2022, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -202,0 +202,96 @@\n+\n+template<typename T>\n+struct AtomicBitopsTestSupport {\n+  volatile T _test_value;\n+\n+  \/\/ At least one byte differs between _old_value and _old_value op _change_value.\n+  static const T _old_value =    static_cast<T>(UCONST64(0x7f5300007f530000));\n+  static const T _change_value = static_cast<T>(UCONST64(0x3800530038005300));\n+\n+  AtomicBitopsTestSupport() : _test_value(0) {}\n+\n+  void fetch_then_and() {\n+    Atomic::store(&_test_value, _old_value);\n+    T expected = _old_value & _change_value;\n+    T result = Atomic::fetch_then_and(&_test_value, _change_value);\n+    EXPECT_EQ(_old_value, result);\n+    EXPECT_EQ(expected, Atomic::load(&_test_value));\n+  }\n+\n+  void fetch_then_or() {\n+    Atomic::store(&_test_value, _old_value);\n+    T expected = _old_value | _change_value;\n+    T result = Atomic::fetch_then_or(&_test_value, _change_value);\n+    EXPECT_EQ(_old_value, result);\n+    EXPECT_EQ(expected, Atomic::load(&_test_value));\n+  }\n+\n+  void fetch_then_xor() {\n+    Atomic::store(&_test_value, _old_value);\n+    T expected = _old_value ^ _change_value;\n+    T result = Atomic::fetch_then_xor(&_test_value, _change_value);\n+    EXPECT_EQ(_old_value, result);\n+    EXPECT_EQ(expected, Atomic::load(&_test_value));\n+  }\n+\n+  void and_then_fetch() {\n+    Atomic::store(&_test_value, _old_value);\n+    T expected = _old_value & _change_value;\n+    T result = Atomic::and_then_fetch(&_test_value, _change_value);\n+    EXPECT_EQ(expected, result);\n+    EXPECT_EQ(expected, Atomic::load(&_test_value));\n+  }\n+\n+  void or_then_fetch() {\n+    Atomic::store(&_test_value, _old_value);\n+    T expected = _old_value | _change_value;\n+    T result = Atomic::or_then_fetch(&_test_value, _change_value);\n+    EXPECT_EQ(expected, result);\n+    EXPECT_EQ(expected, Atomic::load(&_test_value));\n+  }\n+\n+  void xor_then_fetch() {\n+    Atomic::store(&_test_value, _old_value);\n+    T expected = _old_value ^ _change_value;\n+    T result = Atomic::xor_then_fetch(&_test_value, _change_value);\n+    EXPECT_EQ(expected, result);\n+    EXPECT_EQ(expected, Atomic::load(&_test_value));\n+  }\n+\n+#define TEST_BITOP(name) { SCOPED_TRACE(XSTR(name)); name(); }\n+\n+  void operator()() {\n+    TEST_BITOP(fetch_then_and)\n+    TEST_BITOP(fetch_then_or)\n+    TEST_BITOP(fetch_then_xor)\n+    TEST_BITOP(and_then_fetch)\n+    TEST_BITOP(or_then_fetch)\n+    TEST_BITOP(xor_then_fetch)\n+  }\n+\n+#undef TEST_BITOP\n+};\n+\n+template<typename T>\n+const T AtomicBitopsTestSupport<T>::_old_value;\n+\n+template<typename T>\n+const T AtomicBitopsTestSupport<T>::_change_value;\n+\n+TEST(AtomicBitopsTest, int32) {\n+  AtomicBitopsTestSupport<int32_t>()();\n+}\n+\n+TEST(AtomicBitopsTest, uint32) {\n+  AtomicBitopsTestSupport<uint32_t>()();\n+}\n+\n+#ifdef _LP64\n+TEST(AtomicBitopsTest, int64) {\n+  AtomicBitopsTestSupport<int64_t>()();\n+}\n+\n+TEST(AtomicBitopsTest, uint64) {\n+  AtomicBitopsTestSupport<uint64_t>()();\n+}\n+#endif \/\/ _LP64\n","filename":"test\/hotspot\/gtest\/runtime\/test_atomic.cpp","additions":97,"deletions":1,"binary":false,"changes":98,"status":"modified"}]}