{"files":[{"patch":"@@ -243,1 +243,1 @@\n-  \/\/ - dest is of type D*, an integral or pointer type.\n+  \/\/ - dest is of type D*, where D is an integral or pointer type.\n@@ -247,0 +247,1 @@\n+  \/\/ - if D is a pointer type P*, sizeof(P) == 1.\n@@ -261,3 +262,11 @@\n-  \/\/ When D is a pointer type P*, both add_and_fetch and fetch_and_add\n-  \/\/ treat it as if it were an uintptr_t; they do not perform any\n-  \/\/ scaling of add_value, as that has already been done by the caller.\n+  \/\/ When the destination type D of the Atomic operation is a pointer type P*,\n+  \/\/ the addition must scale the add_value by sizeof(P) to add that many bytes\n+  \/\/ to the destination value.  Rather than requiring each platform deal with\n+  \/\/ this, the shared part of the implementation performs some adjustments\n+  \/\/ before and after calling the platform operation.  It ensures the pointee\n+  \/\/ type of the destination value passed to the platform operation has size\n+  \/\/ 1, casting if needed.  It also scales add_value by sizeof(P).  The result\n+  \/\/ of the platform operation is cast back to P*.  This means the platform\n+  \/\/ operation does not need to account for the scaling.  It also makes it\n+  \/\/ easy for the platform to implement one of add_and_fetch or fetch_and_add\n+  \/\/ in terms of the other (which is a common approach).\n@@ -693,3 +702,0 @@\n-  typedef typename Conditional<IsSigned<I>::value,\n-                               intptr_t,\n-                               uintptr_t>::type CI;\n@@ -697,2 +703,24 @@\n-  static CI scale_addend(CI add_value) {\n-    return add_value * sizeof(P);\n+  \/\/ Type of the scaled addend.  An integral type of the same size as a\n+  \/\/ pointer, and the same signedness as I.\n+  using SI = typename Conditional<IsSigned<I>::value, intptr_t, uintptr_t>::type;\n+\n+  \/\/ Type of the unscaled destination.  A pointer type with pointee size == 1.\n+  using UP = const char*;\n+\n+  \/\/ Scale add_value by the size of the pointee.\n+  static SI scale_addend(SI add_value) {\n+    return add_value * SI(sizeof(P));\n+  }\n+\n+  \/\/ Casting between P* and UP* here intentionally uses C-style casts,\n+  \/\/ because reinterpret_cast can't cast away cv qualifiers.  Using copy_cv\n+  \/\/ would be an alternative if it existed.\n+\n+  \/\/ Unscale dest to a char* pointee for consistency with scaled addend.\n+  static UP volatile* unscale_dest(P* volatile* dest) {\n+    return (UP volatile*) dest;\n+  }\n+\n+  \/\/ Convert the unscaled char* result to a P*.\n+  static P* scale_result(UP result) {\n+    return (P*) result;\n@@ -701,3 +729,4 @@\n-  static P* add_and_fetch(P* volatile* dest, I add_value, atomic_memory_order order) {\n-    CI addend = add_value;\n-    return PlatformAdd<sizeof(P*)>().add_and_fetch(dest, scale_addend(addend), order);\n+  static P* add_and_fetch(P* volatile* dest, I addend, atomic_memory_order order) {\n+    return scale_result(PlatformAdd<sizeof(P*)>().add_and_fetch(unscale_dest(dest),\n+                                                                scale_addend(addend),\n+                                                                order));\n@@ -705,3 +734,5 @@\n-  static P* fetch_and_add(P* volatile* dest, I add_value, atomic_memory_order order) {\n-    CI addend = add_value;\n-    return PlatformAdd<sizeof(P*)>().fetch_and_add(dest, scale_addend(addend), order);\n+\n+  static P* fetch_and_add(P* volatile* dest, I addend, atomic_memory_order order) {\n+    return scale_result(PlatformAdd<sizeof(P*)>().fetch_and_add(unscale_dest(dest),\n+                                                                scale_addend(addend),\n+                                                                order));\n","filename":"src\/hotspot\/share\/runtime\/atomic.hpp","additions":46,"deletions":15,"binary":false,"changes":61,"status":"modified"},{"patch":"@@ -31,0 +31,118 @@\n+template<typename T>\n+struct AtomicAddTestSupport {\n+  volatile T _test_value;\n+\n+  AtomicAddTestSupport() : _test_value{} {}\n+\n+  void test_add() {\n+    T zero = 0;\n+    T five = 5;\n+    Atomic::store(&_test_value, zero);\n+    T value = Atomic::add(&_test_value, five);\n+    EXPECT_EQ(five, value);\n+    EXPECT_EQ(five, Atomic::load(&_test_value));\n+  }\n+\n+  void test_fetch_add() {\n+    T zero = 0;\n+    T five = 5;\n+    Atomic::store(&_test_value, zero);\n+    T value = Atomic::fetch_and_add(&_test_value, five);\n+    EXPECT_EQ(zero, value);\n+    EXPECT_EQ(five, Atomic::load(&_test_value));\n+  }\n+};\n+\n+TEST(AtomicAddTest, int32) {\n+  using Support = AtomicAddTestSupport<int32_t>;\n+  Support().test_add();\n+  Support().test_fetch_add();\n+}\n+\n+\/\/ 64bit Atomic::add is only supported on 64bit platforms.\n+#ifdef _LP64\n+TEST(AtomicAddTest, int64) {\n+  using Support = AtomicAddTestSupport<int64_t>;\n+  Support().test_add();\n+  Support().test_fetch_add();\n+}\n+#endif \/\/ _LP64\n+\n+TEST(AtomicAddTest, ptr) {\n+  uint _test_values[10] = {};\n+  uint* volatile _test_value{};\n+\n+  uint* zero = &_test_values[0];\n+  uint* five = &_test_values[5];\n+  uint* six  = &_test_values[6];\n+\n+  Atomic::store(&_test_value, zero);\n+  uint* value = Atomic::add(&_test_value, 5);\n+  EXPECT_EQ(five, value);\n+  EXPECT_EQ(five, Atomic::load(&_test_value));\n+\n+  Atomic::store(&_test_value, zero);\n+  value = Atomic::fetch_and_add(&_test_value, 6);\n+  EXPECT_EQ(zero, value);\n+  EXPECT_EQ(six, Atomic::load(&_test_value));\n+};\n+\n+template<typename T>\n+struct AtomicXchgTestSupport {\n+  volatile T _test_value;\n+\n+  AtomicXchgTestSupport() : _test_value{} {}\n+\n+  void test() {\n+    T zero = 0;\n+    T five = 5;\n+    Atomic::store(&_test_value, zero);\n+    T res = Atomic::xchg(&_test_value, five);\n+    EXPECT_EQ(zero, res);\n+    EXPECT_EQ(five, Atomic::load(&_test_value));\n+  }\n+};\n+\n+TEST(AtomicXchgTest, int32) {\n+  using Support = AtomicXchgTestSupport<int32_t>;\n+  Support().test();\n+}\n+\n+\/\/ 64bit Atomic::xchg is only supported on 64bit platforms.\n+#ifdef _LP64\n+TEST(AtomicXchgTest, int64) {\n+  using Support = AtomicXchgTestSupport<int64_t>;\n+  Support().test();\n+}\n+#endif \/\/ _LP64\n+\n+template<typename T>\n+struct AtomicCmpxchgTestSupport {\n+  volatile T _test_value;\n+\n+  AtomicCmpxchgTestSupport() : _test_value{} {}\n+\n+  void test() {\n+    T zero = 0;\n+    T five = 5;\n+    T ten = 10;\n+    Atomic::store(&_test_value, zero);\n+    T res = Atomic::cmpxchg(&_test_value, five, ten);\n+    EXPECT_EQ(zero, res);\n+    EXPECT_EQ(zero, Atomic::load(&_test_value));\n+    res = Atomic::cmpxchg(&_test_value, zero, ten);\n+    EXPECT_EQ(zero, res);\n+    EXPECT_EQ(ten, Atomic::load(&_test_value));\n+  }\n+};\n+\n+TEST(AtomicCmpxchgTest, int32) {\n+  using Support = AtomicCmpxchgTestSupport<int32_t>;\n+  Support().test();\n+}\n+\n+TEST(AtomicCmpxchgTest, int64) {\n+  using Support = AtomicCmpxchgTestSupport<int64_t>;\n+  Support().test();\n+}\n+\n","filename":"test\/hotspot\/gtest\/runtime\/test_atomic.cpp","additions":118,"deletions":0,"binary":false,"changes":118,"status":"modified"}]}