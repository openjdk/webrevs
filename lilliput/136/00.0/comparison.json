{"files":[{"patch":"@@ -18,1 +18,1 @@\n-files=.*\\.cpp|.*\\.hpp|.*\\.c|.*\\.h|.*\\.java|.*\\.cc|.*\\.hh|.*\\.m|.*\\.mm|.*\\.md|.*\\.gmk|.*\\.m4|.*\\.ac|Makefile\n+files=.*\\.cpp|.*\\.hpp|.*\\.c|.*\\.h|.*\\.java|.*\\.cc|.*\\.hh|.*\\.m|.*\\.mm|.*\\.md|.*\\.properties|.*\\.gmk|.*\\.m4|.*\\.ac|Makefile\n","filename":".jcheck\/conf","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -1240,1 +1240,1 @@\n-    if (UseCompressedOops && (CompressedOops::ptrs_base() != NULL)) {\n+    if (UseCompressedOops && (CompressedOops::ptrs_base() != nullptr)) {\n@@ -1584,1 +1584,1 @@\n-  return st->trailing_membar() != NULL;\n+  return st->trailing_membar() != nullptr;\n@@ -1596,1 +1596,1 @@\n-    assert(ldst->trailing_membar() != NULL, \"expected trailing membar\");\n+    assert(ldst->trailing_membar() != nullptr, \"expected trailing membar\");\n@@ -1598,1 +1598,1 @@\n-    return ldst->trailing_membar() != NULL;\n+    return ldst->trailing_membar() != nullptr;\n@@ -1737,1 +1737,1 @@\n-  if (C->stub_function() == NULL && BarrierSet::barrier_set()->barrier_set_nmethod() != NULL) {\n+  if (C->stub_function() == nullptr && BarrierSet::barrier_set()->barrier_set_nmethod() != nullptr) {\n@@ -1786,1 +1786,1 @@\n-  if (C->stub_function() == NULL) {\n+  if (C->stub_function() == nullptr) {\n@@ -1788,1 +1788,1 @@\n-    if (BarrierSet::barrier_set()->barrier_set_nmethod() != NULL) {\n+    if (BarrierSet::barrier_set()->barrier_set_nmethod() != nullptr) {\n@@ -2156,1 +2156,1 @@\n-    implementation(NULL, ra_, false, st);\n+    implementation(nullptr, ra_, false, st);\n@@ -2161,1 +2161,1 @@\n-  implementation(&cbuf, ra_, false, NULL);\n+  implementation(&cbuf, ra_, false, nullptr);\n@@ -2208,4 +2208,3 @@\n-    st->print_cr(\"\\tldrw rscratch1, j_rarg0 + oopDesc::klass_offset_in_bytes()]\\t# compressed klass\");\n-    if (CompressedKlassPointers::shift() != 0) {\n-      st->print_cr(\"\\tdecode_klass_not_null rscratch1, rscratch1\");\n-    }\n+    st->print_cr(\"\\tldrw rscratch1, [j_rarg0 + oopDesc::klass_offset_in_bytes()]\\t# compressed klass\");\n+    st->print_cr(\"\\tldrw r10, [rscratch2 + CompiledICData::speculated_klass_offset()]\\t# compressed klass\");\n+    st->print_cr(\"\\tcmpw rscratch1, r10\");\n@@ -2213,1 +2212,3 @@\n-   st->print_cr(\"\\tldr rscratch1, j_rarg0 + oopDesc::klass_offset_in_bytes()]\\t# compressed klass\");\n+    st->print_cr(\"\\tldr rscratch1, [j_rarg0 + oopDesc::klass_offset_in_bytes()]\\t# compressed klass\");\n+    st->print_cr(\"\\tldr r10, [rscratch2 + CompiledICData::speculated_klass_offset()]\\t# compressed klass\");\n+    st->print_cr(\"\\tcmp rscratch1, r10\");\n@@ -2215,1 +2216,0 @@\n-  st->print_cr(\"\\tcmp r0, rscratch1\\t # Inline cache check\");\n@@ -2224,8 +2224,1 @@\n-\n-  __ cmp_klass(j_rarg0, rscratch2, rscratch1);\n-  Label skip;\n-  \/\/ TODO\n-  \/\/ can we avoid this skip and still use a reloc?\n-  __ br(Assembler::EQ, skip);\n-  __ far_jump(RuntimeAddress(SharedRuntime::get_ic_miss_stub()));\n-  __ bind(skip);\n+  __ ic_check(InteriorEntryAlignment);\n@@ -2252,1 +2245,1 @@\n-  if (base == NULL) {\n+  if (base == nullptr) {\n@@ -2270,1 +2263,1 @@\n-  if (base == NULL) {\n+  if (base == nullptr) {\n@@ -2413,1 +2406,1 @@\n-  return NULL;\n+  return nullptr;\n@@ -2585,2 +2578,2 @@\n-bool is_valid_sve_arith_imm_pattern(Node* n, Node* m) {\n-  if (n == NULL || m == NULL) {\n+static bool is_valid_sve_arith_imm_pattern(Node* n, Node* m) {\n+  if (n == nullptr || m == nullptr) {\n@@ -2626,2 +2619,2 @@\n-bool is_vector_bitwise_not_pattern(Node* n, Node* m) {\n-  if (n != NULL && m != NULL) {\n+static bool is_vector_bitwise_not_pattern(Node* n, Node* m) {\n+  if (n != nullptr && m != nullptr) {\n@@ -3433,1 +3426,1 @@\n-    if (con == NULL || con == (address)1) {\n+    if (con == nullptr || con == (address)1) {\n@@ -3476,1 +3469,1 @@\n-    if (con == NULL) {\n+    if (con == nullptr) {\n@@ -3495,1 +3488,1 @@\n-    if (con == NULL) {\n+    if (con == nullptr) {\n@@ -3678,1 +3671,1 @@\n-                                     NULL, &miss,\n+                                     nullptr, &miss,\n@@ -3694,1 +3687,1 @@\n-      if (call == NULL) {\n+      if (call == nullptr) {\n@@ -3708,1 +3701,1 @@\n-      if (call == NULL) {\n+      if (call == nullptr) {\n@@ -3718,2 +3711,2 @@\n-        address stub = CompiledStaticCall::emit_to_interp_stub(cbuf, call);\n-        if (stub == NULL) {\n+        address stub = CompiledDirectCall::emit_to_interp_stub(cbuf, call);\n+        if (stub == nullptr) {\n@@ -3738,1 +3731,1 @@\n-    if (call == NULL) {\n+    if (call == nullptr) {\n@@ -3767,1 +3760,1 @@\n-      if (call == NULL) {\n+      if (call == nullptr) {\n@@ -4666,1 +4659,1 @@\n-\/\/ NULL Pointer Immediate\n+\/\/ Null Pointer Immediate\n@@ -4798,1 +4791,1 @@\n-\/\/ Narrow NULL Pointer Immediate\n+\/\/ Narrow Null Pointer Immediate\n@@ -7236,1 +7229,1 @@\n-  format %{ \"mov  $dst, $con\\t# NULL ptr\" %}\n+  format %{ \"mov  $dst, $con\\t# null pointer\" %}\n@@ -7250,1 +7243,1 @@\n-  format %{ \"mov  $dst, $con\\t# NULL ptr\" %}\n+  format %{ \"mov  $dst, $con\\t# null pointer\" %}\n@@ -7292,1 +7285,1 @@\n-  format %{ \"mov  $dst, $con\\t# compressed NULL ptr\" %}\n+  format %{ \"mov  $dst, $con\\t# compressed null pointer\" %}\n@@ -8306,1 +8299,1 @@\n-            \"dmb ishld\" %}\n+            \"dmb ish\" %}\n@@ -8360,1 +8353,1 @@\n-            \"dmb ishst\\n\\tdmb ishld\" %}\n+            \"dmb ish\" %}\n@@ -8364,2 +8357,1 @@\n-    __ membar(Assembler::StoreStore);\n-    __ membar(Assembler::LoadStore);\n+    __ membar(Assembler::LoadStore|Assembler::StoreStore);\n@@ -15274,1 +15266,1 @@\n-    if (tpc == NULL) {\n+    if (tpc == nullptr) {\n@@ -15295,1 +15287,1 @@\n-    if (tpc == NULL) {\n+    if (tpc == nullptr) {\n@@ -16458,1 +16450,1 @@\n-  predicate(LockingMode != LM_PLACEHOLDER);\n+  predicate(LockingMode != LM_LIGHTWEIGHT && LockingMode != LM_PLACEHOLDER);\n@@ -16463,1 +16455,1 @@\n-  format %{ \"fastlock $object,$box\\t! kills $tmp,$tmp2,tmp3\" %}\n+  format %{ \"fastlock $object,$box\\t! kills $tmp,$tmp2,$tmp3\" %}\n@@ -16474,1 +16466,1 @@\n-  predicate(LockingMode != LM_PLACEHOLDER);\n+  predicate(LockingMode != LM_LIGHTWEIGHT && LockingMode != LM_PLACEHOLDER);\n@@ -16488,0 +16480,31 @@\n+instruct cmpFastLockLightweight(rFlagsReg cr, iRegP object, iRegP box, iRegPNoSp tmp, iRegPNoSp tmp2)\n+%{\n+  predicate(LockingMode == LM_LIGHTWEIGHT);\n+  match(Set cr (FastLock object box));\n+  effect(TEMP tmp, TEMP tmp2);\n+\n+  ins_cost(5 * INSN_COST);\n+  format %{ \"fastlock $object,$box\\t! kills $tmp,$tmp2\" %}\n+\n+  ins_encode %{\n+    __ fast_lock_lightweight($object$$Register, $box$$Register, $tmp$$Register, $tmp2$$Register);\n+  %}\n+\n+  ins_pipe(pipe_serial);\n+%}\n+\n+instruct cmpFastUnlockLightweight(rFlagsReg cr, iRegP object, iRegP box, iRegPNoSp tmp, iRegPNoSp tmp2)\n+%{\n+  predicate(LockingMode == LM_LIGHTWEIGHT);\n+  match(Set cr (FastUnlock object box));\n+  effect(TEMP tmp, TEMP tmp2);\n+\n+  ins_cost(5 * INSN_COST);\n+  format %{ \"fastunlock $object,$box\\t! kills $tmp, $tmp2\" %}\n+\n+  ins_encode %{\n+    __ fast_unlock_lightweight($object$$Register, $box$$Register, $tmp$$Register, $tmp2$$Register);\n+  %}\n+\n+  ins_pipe(pipe_serial);\n+%}\n@@ -17153,17 +17176,1 @@\n-                     $result$$Register, $cnt$$Register, 1);\n-  %}\n-  ins_pipe(pipe_class_memory);\n-%}\n-\n-instruct string_equalsU(iRegP_R1 str1, iRegP_R3 str2, iRegI_R4 cnt,\n-                        iRegI_R0 result, rFlagsReg cr)\n-%{\n-  predicate(((StrEqualsNode*)n)->encoding() == StrIntrinsicNode::UU);\n-  match(Set result (StrEquals (Binary str1 str2) cnt));\n-  effect(USE_KILL str1, USE_KILL str2, USE_KILL cnt, KILL cr);\n-\n-  format %{ \"String Equals $str1,$str2,$cnt -> $result\" %}\n-  ins_encode %{\n-    \/\/ Count is in 8-bit bytes; non-Compact chars are 16 bits.\n-    __ string_equals($str1$$Register, $str2$$Register,\n-                     $result$$Register, $cnt$$Register, 2);\n+                     $result$$Register, $cnt$$Register);\n@@ -17191,1 +17198,1 @@\n-    if (tpc == NULL) {\n+    if (tpc == nullptr) {\n@@ -17216,1 +17223,1 @@\n-    if (tpc == NULL) {\n+    if (tpc == nullptr) {\n@@ -17231,1 +17238,1 @@\n-    if (tpc == NULL) {\n+    if (tpc == nullptr) {\n@@ -17274,1 +17281,1 @@\n-    if (tpc == NULL) {\n+    if (tpc == nullptr) {\n","filename":"src\/hotspot\/cpu\/aarch64\/aarch64.ad","additions":82,"deletions":75,"binary":false,"changes":157,"status":"modified"},{"patch":"@@ -58,1 +58,0 @@\n-const Register IC_Klass    = rscratch2;   \/\/ where the IC klass is cached\n@@ -298,21 +297,1 @@\n-  Register receiver = FrameMap::receiver_opr->as_register();\n-  Register ic_klass = IC_Klass;\n-  int start_offset = __ offset();\n-  __ inline_cache_check(receiver, ic_klass);\n-\n-  \/\/ if icache check fails, then jump to runtime routine\n-  \/\/ Note: RECEIVER must still contain the receiver!\n-  Label dont;\n-  __ br(Assembler::EQ, dont);\n-  __ far_jump(RuntimeAddress(SharedRuntime::get_ic_miss_stub()));\n-\n-  \/\/ We align the verified entry point unless the method body\n-  \/\/ (including its inline cache check) will fit in a single 64-byte\n-  \/\/ icache line.\n-  if (! method()->is_accessor() || __ offset() - start_offset > 4 * 4) {\n-    \/\/ force alignment after the cache check.\n-    __ align(CodeEntryAlignment);\n-  }\n-\n-  __ bind(dont);\n-  return start_offset;\n+  return __ ic_check(CodeEntryAlignment);\n@@ -2047,1 +2026,1 @@\n-  assert(__ offset() - start + CompiledStaticCall::to_trampoline_stub_size()\n+  assert(__ offset() - start + CompiledDirectCall::to_trampoline_stub_size()\n","filename":"src\/hotspot\/cpu\/aarch64\/c1_LIRAssembler_aarch64.cpp","additions":2,"deletions":23,"binary":false,"changes":25,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1999, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1999, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -88,5 +88,0 @@\n-  if (LockingMode != LM_PLACEHOLDER) {\n-    \/\/ Load object header\n-    ldr(hdr, Address(obj, hdr_offset));\n-  }\n-\n@@ -100,0 +95,2 @@\n+    \/\/ Load object header\n+    ldr(hdr, Address(obj, hdr_offset));\n@@ -160,5 +157,0 @@\n-    ldr(hdr, Address(obj, oopDesc::mark_offset_in_bytes()));\n-    \/\/ We cannot use tbnz here, the target might be too far away and cannot\n-    \/\/ be encoded.\n-    tst(hdr, markWord::monitor_value);\n-    br(Assembler::NE, slow_case);\n@@ -340,15 +332,0 @@\n-\n-void C1_MacroAssembler::inline_cache_check(Register receiver, Register iCache) {\n-  verify_oop(receiver);\n-  \/\/ explicit null check not needed since load from [klass_offset] causes a trap\n-  \/\/ check against inline cache\n-  if (UseCompactObjectHeaders) {\n-    assert(!MacroAssembler::needs_explicit_null_check(oopDesc::mark_offset_in_bytes()), \"must add explicit null check\");\n-  } else {\n-    assert(!MacroAssembler::needs_explicit_null_check(oopDesc::klass_offset_in_bytes()), \"must add explicit null check\");\n-  }\n-\n-  cmp_klass(receiver, iCache, rscratch1);\n-}\n-\n-\n","filename":"src\/hotspot\/cpu\/aarch64\/c1_MacroAssembler_aarch64.cpp","additions":3,"deletions":26,"binary":false,"changes":29,"status":"modified"},{"patch":"@@ -60,0 +60,1 @@\n+  assert(LockingMode != LM_LIGHTWEIGHT, \"lightweight locking should use fast_lock_lightweight\");\n@@ -79,1 +80,2 @@\n-  } else if (LockingMode == LM_LEGACY) {\n+  } else {\n+    assert(LockingMode == LM_LEGACY, \"must be\");\n@@ -108,4 +110,0 @@\n-  } else {\n-    assert(LockingMode == LM_LIGHTWEIGHT, \"must be\");\n-    lightweight_lock(oop, disp_hdr, tmp, tmp3Reg, no_count);\n-    b(count);\n@@ -125,8 +123,7 @@\n-  if (LockingMode != LM_LIGHTWEIGHT) {\n-    \/\/ Store a non-null value into the box to avoid looking like a re-entrant\n-    \/\/ lock. The fast-path monitor unlock code checks for\n-    \/\/ markWord::monitor_value so use markWord::unused_mark which has the\n-    \/\/ relevant bit set, and also matches ObjectSynchronizer::enter.\n-    mov(tmp, (address)markWord::unused_mark().value());\n-    str(tmp, Address(box, BasicLock::displaced_header_offset_in_bytes()));\n-  }\n+  \/\/ Store a non-null value into the box to avoid looking like a re-entrant\n+  \/\/ lock. The fast-path monitor unlock code checks for\n+  \/\/ markWord::monitor_value so use markWord::unused_mark which has the\n+  \/\/ relevant bit set, and also matches ObjectSynchronizer::enter.\n+  mov(tmp, (address)markWord::unused_mark().value());\n+  str(tmp, Address(box, BasicLock::displaced_header_offset_in_bytes()));\n+\n@@ -163,0 +160,1 @@\n+  assert(LockingMode != LM_LIGHTWEIGHT, \"lightweight locking should use fast_unlock_lightweight\");\n@@ -181,1 +179,2 @@\n-  } else if (LockingMode == LM_LEGACY) {\n+  } else {\n+    assert(LockingMode == LM_LEGACY, \"must be\");\n@@ -189,4 +188,0 @@\n-  } else {\n-    assert(LockingMode == LM_LIGHTWEIGHT, \"must be\");\n-    lightweight_unlock(oop, tmp, box, disp_hdr, no_count);\n-    b(count);\n@@ -202,13 +197,0 @@\n-  if (LockingMode == LM_LIGHTWEIGHT) {\n-    \/\/ If the owner is anonymous, we need to fix it -- in an outline stub.\n-    Register tmp2 = disp_hdr;\n-    ldr(tmp2, Address(tmp, ObjectMonitor::owner_offset()));\n-    \/\/ We cannot use tbnz here, the target might be too far away and cannot\n-    \/\/ be encoded.\n-    tst(tmp2, (uint64_t)ObjectMonitor::ANONYMOUS_OWNER);\n-    C2HandleAnonOMOwnerStub* stub = new (Compile::current()->comp_arena()) C2HandleAnonOMOwnerStub(tmp, tmp2);\n-    Compile::current()->output()->add_stub(stub);\n-    br(Assembler::NE, stub->entry());\n-    bind(stub->continuation());\n-  }\n-\n@@ -247,0 +229,256 @@\n+void C2_MacroAssembler::fast_lock_lightweight(Register obj, Register t1,\n+                                              Register t2, Register t3) {\n+  assert(LockingMode == LM_LIGHTWEIGHT, \"must be\");\n+  assert_different_registers(obj, t1, t2, t3);\n+\n+  \/\/ Handle inflated monitor.\n+  Label inflated;\n+  \/\/ Finish fast lock successfully. MUST branch to with flag == EQ\n+  Label locked;\n+  \/\/ Finish fast lock unsuccessfully. MUST branch to with flag == NE\n+  Label slow_path;\n+\n+  if (DiagnoseSyncOnValueBasedClasses != 0) {\n+    load_klass(t1, obj);\n+    ldrw(t1, Address(t1, Klass::access_flags_offset()));\n+    tstw(t1, JVM_ACC_IS_VALUE_BASED_CLASS);\n+    br(Assembler::NE, slow_path);\n+  }\n+\n+  const Register t1_mark = t1;\n+\n+  { \/\/ Lightweight locking\n+\n+    \/\/ Push lock to the lock stack and finish successfully. MUST branch to with flag == EQ\n+    Label push;\n+\n+    const Register t2_top = t2;\n+    const Register t3_t = t3;\n+\n+    \/\/ Check if lock-stack is full.\n+    ldrw(t2_top, Address(rthread, JavaThread::lock_stack_top_offset()));\n+    cmpw(t2_top, (unsigned)LockStack::end_offset() - 1);\n+    br(Assembler::GT, slow_path);\n+\n+    \/\/ Check if recursive.\n+    subw(t3_t, t2_top, oopSize);\n+    ldr(t3_t, Address(rthread, t3_t));\n+    cmp(obj, t3_t);\n+    br(Assembler::EQ, push);\n+\n+    \/\/ Relaxed normal load to check for monitor. Optimization for monitor case.\n+    ldr(t1_mark, Address(obj, oopDesc::mark_offset_in_bytes()));\n+    tbnz(t1_mark, exact_log2(markWord::monitor_value), inflated);\n+\n+    \/\/ Not inflated\n+    assert(oopDesc::mark_offset_in_bytes() == 0, \"required to avoid a lea\");\n+\n+    \/\/ Try to lock. Transition lock-bits 0b01 => 0b00\n+    orr(t1_mark, t1_mark, markWord::unlocked_value);\n+    eor(t3_t, t1_mark, markWord::unlocked_value);\n+    cmpxchg(\/*addr*\/ obj, \/*expected*\/ t1_mark, \/*new*\/ t3_t, Assembler::xword,\n+            \/*acquire*\/ true, \/*release*\/ false, \/*weak*\/ false, noreg);\n+    br(Assembler::NE, slow_path);\n+\n+    bind(push);\n+    \/\/ After successful lock, push object on lock-stack.\n+    str(obj, Address(rthread, t2_top));\n+    addw(t2_top, t2_top, oopSize);\n+    strw(t2_top, Address(rthread, JavaThread::lock_stack_top_offset()));\n+    b(locked);\n+  }\n+\n+  { \/\/ Handle inflated monitor.\n+    bind(inflated);\n+\n+    \/\/ mark contains the tagged ObjectMonitor*.\n+    const Register t1_tagged_monitor = t1_mark;\n+    const uintptr_t monitor_tag = markWord::monitor_value;\n+    const Register t2_owner_addr = t2;\n+    const Register t3_owner = t3;\n+\n+    \/\/ Compute owner address.\n+    lea(t2_owner_addr, Address(t1_tagged_monitor, (in_bytes(ObjectMonitor::owner_offset()) - monitor_tag)));\n+\n+    \/\/ CAS owner (null => current thread).\n+    cmpxchg(t2_owner_addr, zr, rthread, Assembler::xword, \/*acquire*\/ true,\n+            \/*release*\/ false, \/*weak*\/ false, t3_owner);\n+    br(Assembler::EQ, locked);\n+\n+    \/\/ Check if recursive.\n+    cmp(t3_owner, rthread);\n+    br(Assembler::NE, slow_path);\n+\n+    \/\/ Recursive.\n+    increment(Address(t1_tagged_monitor, in_bytes(ObjectMonitor::recursions_offset()) - monitor_tag), 1);\n+  }\n+\n+  bind(locked);\n+  increment(Address(rthread, JavaThread::held_monitor_count_offset()));\n+\n+#ifdef ASSERT\n+  \/\/ Check that locked label is reached with Flags == EQ.\n+  Label flag_correct;\n+  br(Assembler::EQ, flag_correct);\n+  stop(\"Fast Lock Flag != EQ\");\n+#endif\n+\n+  bind(slow_path);\n+#ifdef ASSERT\n+  \/\/ Check that slow_path label is reached with Flags == NE.\n+  br(Assembler::NE, flag_correct);\n+  stop(\"Fast Lock Flag != NE\");\n+  bind(flag_correct);\n+#endif\n+  \/\/ C2 uses the value of Flags (NE vs EQ) to determine the continuation.\n+}\n+\n+void C2_MacroAssembler::fast_unlock_lightweight(Register obj, Register t1, Register t2,\n+                                                Register t3) {\n+  assert(LockingMode == LM_LIGHTWEIGHT, \"must be\");\n+  assert_different_registers(obj, t1, t2, t3);\n+\n+  \/\/ Handle inflated monitor.\n+  Label inflated, inflated_load_monitor;\n+  \/\/ Finish fast unlock successfully. MUST branch to with flag == EQ\n+  Label unlocked;\n+  \/\/ Finish fast unlock unsuccessfully. MUST branch to with flag == NE\n+  Label slow_path;\n+\n+  const Register t1_mark = t1;\n+  const Register t2_top = t2;\n+  const Register t3_t = t3;\n+\n+  { \/\/ Lightweight unlock\n+\n+    \/\/ Check if obj is top of lock-stack.\n+    ldrw(t2_top, Address(rthread, JavaThread::lock_stack_top_offset()));\n+    subw(t2_top, t2_top, oopSize);\n+    ldr(t3_t, Address(rthread, t2_top));\n+    cmp(obj, t3_t);\n+    \/\/ Top of lock stack was not obj. Must be monitor.\n+    br(Assembler::NE, inflated_load_monitor);\n+\n+    \/\/ Pop lock-stack.\n+    DEBUG_ONLY(str(zr, Address(rthread, t2_top));)\n+    strw(t2_top, Address(rthread, JavaThread::lock_stack_top_offset()));\n+\n+    \/\/ Check if recursive.\n+    subw(t3_t, t2_top, oopSize);\n+    ldr(t3_t, Address(rthread, t3_t));\n+    cmp(obj, t3_t);\n+    br(Assembler::EQ, unlocked);\n+\n+    \/\/ Not recursive.\n+    \/\/ Load Mark.\n+    ldr(t1_mark, Address(obj, oopDesc::mark_offset_in_bytes()));\n+\n+    \/\/ Check header for monitor (0b10).\n+    tbnz(t1_mark, exact_log2(markWord::monitor_value), inflated);\n+\n+    \/\/ Try to unlock. Transition lock bits 0b00 => 0b01\n+    assert(oopDesc::mark_offset_in_bytes() == 0, \"required to avoid lea\");\n+    orr(t3_t, t1_mark, markWord::unlocked_value);\n+    cmpxchg(\/*addr*\/ obj, \/*expected*\/ t1_mark, \/*new*\/ t3_t, Assembler::xword,\n+            \/*acquire*\/ false, \/*release*\/ true, \/*weak*\/ false, noreg);\n+    br(Assembler::EQ, unlocked);\n+\n+    \/\/ Compare and exchange failed.\n+    \/\/ Restore lock-stack and handle the unlock in runtime.\n+    DEBUG_ONLY(str(obj, Address(rthread, t2_top));)\n+    addw(t2_top, t2_top, oopSize);\n+    str(t2_top, Address(rthread, JavaThread::lock_stack_top_offset()));\n+    b(slow_path);\n+  }\n+\n+\n+  { \/\/ Handle inflated monitor.\n+    bind(inflated_load_monitor);\n+    ldr(t1_mark, Address(obj, oopDesc::mark_offset_in_bytes()));\n+#ifdef ASSERT\n+    tbnz(t1_mark, exact_log2(markWord::monitor_value), inflated);\n+    stop(\"Fast Unlock not monitor\");\n+#endif\n+\n+    bind(inflated);\n+\n+#ifdef ASSERT\n+    Label check_done;\n+    subw(t2_top, t2_top, oopSize);\n+    cmpw(t2_top, in_bytes(JavaThread::lock_stack_base_offset()));\n+    br(Assembler::LT, check_done);\n+    ldr(t3_t, Address(rthread, t2_top));\n+    cmp(obj, t3_t);\n+    br(Assembler::NE, inflated);\n+    stop(\"Fast Unlock lock on stack\");\n+    bind(check_done);\n+#endif\n+\n+    \/\/ mark contains the tagged ObjectMonitor*.\n+    const Register t1_monitor = t1_mark;\n+    const uintptr_t monitor_tag = markWord::monitor_value;\n+\n+    \/\/ Untag the monitor.\n+    sub(t1_monitor, t1_mark, monitor_tag);\n+\n+    const Register t2_recursions = t2;\n+    Label not_recursive;\n+\n+    \/\/ Check if recursive.\n+    ldr(t2_recursions, Address(t1_monitor, ObjectMonitor::recursions_offset()));\n+    cbz(t2_recursions, not_recursive);\n+\n+    \/\/ Recursive unlock.\n+    sub(t2_recursions, t2_recursions, 1u);\n+    str(t2_recursions, Address(t1_monitor, ObjectMonitor::recursions_offset()));\n+    \/\/ Set flag == EQ\n+    cmp(t2_recursions, t2_recursions);\n+    b(unlocked);\n+\n+    bind(not_recursive);\n+\n+    Label release;\n+    const Register t2_owner_addr = t2;\n+\n+    \/\/ Compute owner address.\n+    lea(t2_owner_addr, Address(t1_monitor, ObjectMonitor::owner_offset()));\n+\n+    \/\/ Check if the entry lists are empty.\n+    ldr(rscratch1, Address(t1_monitor, ObjectMonitor::EntryList_offset()));\n+    ldr(t3_t, Address(t1_monitor, ObjectMonitor::cxq_offset()));\n+    orr(rscratch1, rscratch1, t3_t);\n+    cmp(rscratch1, zr);\n+    br(Assembler::EQ, release);\n+\n+    \/\/ The owner may be anonymous and we removed the last obj entry in\n+    \/\/ the lock-stack. This loses the information about the owner.\n+    \/\/ Write the thread to the owner field so the runtime knows the owner.\n+    str(rthread, Address(t2_owner_addr));\n+    b(slow_path);\n+\n+    bind(release);\n+    \/\/ Set owner to null.\n+    \/\/ Release to satisfy the JMM\n+    stlr(zr, t2_owner_addr);\n+  }\n+\n+  bind(unlocked);\n+  decrement(Address(rthread, JavaThread::held_monitor_count_offset()));\n+\n+#ifdef ASSERT\n+  \/\/ Check that unlocked label is reached with Flags == EQ.\n+  Label flag_correct;\n+  br(Assembler::EQ, flag_correct);\n+  stop(\"Fast Unlock Flag != EQ\");\n+#endif\n+\n+  bind(slow_path);\n+#ifdef ASSERT\n+  \/\/ Check that slow_path label is reached with Flags == NE.\n+  br(Assembler::NE, flag_correct);\n+  stop(\"Fast Unlock Flag != NE\");\n+  bind(flag_correct);\n+#endif\n+  \/\/ C2 uses the value of Flags (NE vs EQ) to determine the continuation.\n+}\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/c2_MacroAssembler_aarch64.cpp","additions":269,"deletions":31,"binary":false,"changes":300,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2020, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2020, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -39,1 +39,0 @@\n-  \/\/ See full description in macroAssembler_aarch64.cpp.\n@@ -42,0 +41,3 @@\n+  \/\/ Code used by cmpFastLockLightweight and cmpFastUnlockLightweight mach instructions in .ad file.\n+  void fast_lock_lightweight(Register object, Register t1, Register t2, Register t3);\n+  void fast_unlock_lightweight(Register object, Register t1, Register t2, Register t3);\n","filename":"src\/hotspot\/cpu\/aarch64\/c2_MacroAssembler_aarch64.hpp","additions":4,"deletions":2,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -30,0 +30,1 @@\n+#include \"code\/compiledIC.hpp\"\n@@ -969,1 +970,1 @@\n-  \/\/ CompiledDirectStaticCall::set_to_interpreted knows the\n+  \/\/ CompiledDirectCall::set_to_interpreted knows the\n@@ -999,1 +1000,1 @@\n-  movptr(rscratch2, (uintptr_t)Universe::non_oop_word());\n+  movptr(rscratch2, (intptr_t)Universe::non_oop_word());\n@@ -1003,0 +1004,41 @@\n+int MacroAssembler::ic_check_size() {\n+  if (target_needs_far_branch(CAST_FROM_FN_PTR(address, SharedRuntime::get_ic_miss_stub()))) {\n+    return NativeInstruction::instruction_size * 7;\n+  } else {\n+    return NativeInstruction::instruction_size * 5;\n+  }\n+}\n+\n+int MacroAssembler::ic_check(int end_alignment) {\n+  Register receiver = j_rarg0;\n+  Register data = rscratch2;\n+  Register tmp1 = rscratch1;\n+  Register tmp2 = r10;\n+\n+  \/\/ The UEP of a code blob ensures that the VEP is padded. However, the padding of the UEP is placed\n+  \/\/ before the inline cache check, so we don't have to execute any nop instructions when dispatching\n+  \/\/ through the UEP, yet we can ensure that the VEP is aligned appropriately. That's why we align\n+  \/\/ before the inline cache check here, and not after\n+  align(end_alignment, offset() + ic_check_size());\n+\n+  int uep_offset = offset();\n+\n+  if (UseCompressedClassPointers) {\n+    ldrw(tmp1, Address(receiver, oopDesc::klass_offset_in_bytes()));\n+    ldrw(tmp2, Address(data, CompiledICData::speculated_klass_offset()));\n+    cmpw(tmp1, tmp2);\n+  } else {\n+    ldr(tmp1, Address(receiver, oopDesc::klass_offset_in_bytes()));\n+    ldr(tmp2, Address(data, CompiledICData::speculated_klass_offset()));\n+    cmp(tmp1, tmp2);\n+  }\n+\n+  Label dont;\n+  br(Assembler::EQ, dont);\n+  far_jump(RuntimeAddress(SharedRuntime::get_ic_miss_stub()));\n+  bind(dont);\n+  assert((offset() % end_alignment) == 0, \"Misaligned verified entry point\");\n+\n+  return uep_offset;\n+}\n+\n@@ -1104,1 +1146,8 @@\n-  while (offset() % modulus != 0) nop();\n+  align(modulus, offset());\n+}\n+\n+\/\/ Ensure that the code at target bytes offset from the current offset() is aligned\n+\/\/ according to modulus.\n+void MacroAssembler::align(int modulus, int target) {\n+  int delta = target - offset();\n+  while ((offset() + delta) % modulus != 0) nop();\n@@ -1201,1 +1250,1 @@\n-\/\/ - check recv_klass (actual object class) is a subtype of resolved_klass from CompiledICHolder\n+\/\/ - check recv_klass (actual object class) is a subtype of resolved_klass from CompiledICData\n@@ -2070,12 +2119,7 @@\n-    \/\/ Don't promote DMB ST|DMB LD to DMB (a full barrier) because\n-    \/\/ doing so would introduce a StoreLoad which the caller did not\n-    \/\/ intend\n-    if (AlwaysMergeDMB || bar->get_kind() == order_constraint\n-        || bar->get_kind() == AnyAny\n-        || order_constraint == AnyAny) {\n-      \/\/ We are merging two memory barrier instructions.  On AArch64 we\n-      \/\/ can do this simply by ORing them together.\n-      bar->set_kind(bar->get_kind() | order_constraint);\n-      BLOCK_COMMENT(\"merged membar\");\n-      return;\n-    }\n+    \/\/ We are merging two memory barrier instructions.  On AArch64 we\n+    \/\/ can do this simply by ORing them together.\n+    bar->set_kind(bar->get_kind() | order_constraint);\n+    BLOCK_COMMENT(\"merged membar\");\n+  } else {\n+    code()->set_last_insn(pc());\n+    dmb(Assembler::barrier(order_constraint));\n@@ -2083,2 +2127,0 @@\n-  code()->set_last_insn(pc());\n-  dmb(Assembler::barrier(order_constraint));\n@@ -4269,0 +4311,2 @@\n+    \/\/ Registers v0..v7 are used as data registers.\n+    \/\/ Registers v16..v31 are used as tmp registers.\n@@ -4270,14 +4314,15 @@\n-    ldrq(v1, Address(buf, 0x10));\n-    ldrq(v2, Address(buf, 0x20));\n-    ldrq(v3, Address(buf, 0x30));\n-    ldrq(v4, Address(buf, 0x40));\n-    ldrq(v5, Address(buf, 0x50));\n-    ldrq(v6, Address(buf, 0x60));\n-    ldrq(v7, Address(buf, 0x70));\n-    ldrq(v8, Address(pre(buf, 0x80)));\n-\n-    movi(v25, T4S, 0);\n-    mov(v25, S, 0, crc);\n-    eor(v1, T16B, v1, v25);\n-\n-    ldrq(v0, Address(table));\n+    ldrq(v0, Address(buf, 0x10));\n+    ldrq(v1, Address(buf, 0x20));\n+    ldrq(v2, Address(buf, 0x30));\n+    ldrq(v3, Address(buf, 0x40));\n+    ldrq(v4, Address(buf, 0x50));\n+    ldrq(v5, Address(buf, 0x60));\n+    ldrq(v6, Address(buf, 0x70));\n+    ldrq(v7, Address(pre(buf, 0x80)));\n+\n+    movi(v31, T4S, 0);\n+    mov(v31, S, 0, crc);\n+    eor(v0, T16B, v0, v31);\n+\n+    \/\/ Register v16 contains constants from the crc table.\n+    ldrq(v16, Address(table));\n@@ -4288,39 +4333,41 @@\n-    pmull (v9,  T1Q, v1, v0, T1D);\n-    pmull2(v10, T1Q, v1, v0, T2D);\n-    ldrq(v1, Address(buf, 0x10));\n-    eor3(v1, T16B, v9,  v10, v1);\n-\n-    pmull (v11, T1Q, v2, v0, T1D);\n-    pmull2(v12, T1Q, v2, v0, T2D);\n-    ldrq(v2, Address(buf, 0x20));\n-    eor3(v2, T16B, v11, v12, v2);\n-\n-    pmull (v13, T1Q, v3, v0, T1D);\n-    pmull2(v14, T1Q, v3, v0, T2D);\n-    ldrq(v3, Address(buf, 0x30));\n-    eor3(v3, T16B, v13, v14, v3);\n-\n-    pmull (v15, T1Q, v4, v0, T1D);\n-    pmull2(v16, T1Q, v4, v0, T2D);\n-    ldrq(v4, Address(buf, 0x40));\n-    eor3(v4, T16B, v15, v16, v4);\n-\n-    pmull (v17, T1Q, v5, v0, T1D);\n-    pmull2(v18, T1Q, v5, v0, T2D);\n-    ldrq(v5, Address(buf, 0x50));\n-    eor3(v5, T16B, v17, v18, v5);\n-\n-    pmull (v19, T1Q, v6, v0, T1D);\n-    pmull2(v20, T1Q, v6, v0, T2D);\n-    ldrq(v6, Address(buf, 0x60));\n-    eor3(v6, T16B, v19, v20, v6);\n-\n-    pmull (v21, T1Q, v7, v0, T1D);\n-    pmull2(v22, T1Q, v7, v0, T2D);\n-    ldrq(v7, Address(buf, 0x70));\n-    eor3(v7, T16B, v21, v22, v7);\n-\n-    pmull (v23, T1Q, v8, v0, T1D);\n-    pmull2(v24, T1Q, v8, v0, T2D);\n-    ldrq(v8, Address(pre(buf, 0x80)));\n-    eor3(v8, T16B, v23, v24, v8);\n+    pmull (v17,  T1Q, v0, v16, T1D);\n+    pmull2(v18, T1Q, v0, v16, T2D);\n+    ldrq(v0, Address(buf, 0x10));\n+    eor3(v0, T16B, v17,  v18, v0);\n+\n+    pmull (v19, T1Q, v1, v16, T1D);\n+    pmull2(v20, T1Q, v1, v16, T2D);\n+    ldrq(v1, Address(buf, 0x20));\n+    eor3(v1, T16B, v19, v20, v1);\n+\n+    pmull (v21, T1Q, v2, v16, T1D);\n+    pmull2(v22, T1Q, v2, v16, T2D);\n+    ldrq(v2, Address(buf, 0x30));\n+    eor3(v2, T16B, v21, v22, v2);\n+\n+    pmull (v23, T1Q, v3, v16, T1D);\n+    pmull2(v24, T1Q, v3, v16, T2D);\n+    ldrq(v3, Address(buf, 0x40));\n+    eor3(v3, T16B, v23, v24, v3);\n+\n+    pmull (v25, T1Q, v4, v16, T1D);\n+    pmull2(v26, T1Q, v4, v16, T2D);\n+    ldrq(v4, Address(buf, 0x50));\n+    eor3(v4, T16B, v25, v26, v4);\n+\n+    pmull (v27, T1Q, v5, v16, T1D);\n+    pmull2(v28, T1Q, v5, v16, T2D);\n+    ldrq(v5, Address(buf, 0x60));\n+    eor3(v5, T16B, v27, v28, v5);\n+\n+    pmull (v29, T1Q, v6, v16, T1D);\n+    pmull2(v30, T1Q, v6, v16, T2D);\n+    ldrq(v6, Address(buf, 0x70));\n+    eor3(v6, T16B, v29, v30, v6);\n+\n+    \/\/ Reuse registers v23, v24.\n+    \/\/ Using them won't block the first instruction of the next iteration.\n+    pmull (v23, T1Q, v7, v16, T1D);\n+    pmull2(v24, T1Q, v7, v16, T2D);\n+    ldrq(v7, Address(pre(buf, 0x80)));\n+    eor3(v7, T16B, v23, v24, v7);\n@@ -4332,1 +4379,2 @@\n-    ldrq(v0, Address(table, 0x10));\n+    \/\/ Use v31 for constants because v16 can be still in use.\n+    ldrq(v31, Address(table, 0x10));\n@@ -4334,3 +4382,3 @@\n-    pmull (v10,  T1Q, v1, v0, T1D);\n-    pmull2(v11, T1Q, v1, v0, T2D);\n-    eor3(v1, T16B, v10, v11, v5);\n+    pmull (v17,  T1Q, v0, v31, T1D);\n+    pmull2(v18, T1Q, v0, v31, T2D);\n+    eor3(v0, T16B, v17, v18, v4);\n@@ -4338,3 +4386,3 @@\n-    pmull (v12, T1Q, v2, v0, T1D);\n-    pmull2(v13, T1Q, v2, v0, T2D);\n-    eor3(v2, T16B, v12, v13, v6);\n+    pmull (v19, T1Q, v1, v31, T1D);\n+    pmull2(v20, T1Q, v1, v31, T2D);\n+    eor3(v1, T16B, v19, v20, v5);\n@@ -4342,3 +4390,3 @@\n-    pmull (v14, T1Q, v3, v0, T1D);\n-    pmull2(v15, T1Q, v3, v0, T2D);\n-    eor3(v3, T16B, v14, v15, v7);\n+    pmull (v21, T1Q, v2, v31, T1D);\n+    pmull2(v22, T1Q, v2, v31, T2D);\n+    eor3(v2, T16B, v21, v22, v6);\n@@ -4346,3 +4394,3 @@\n-    pmull (v16, T1Q, v4, v0, T1D);\n-    pmull2(v17, T1Q, v4, v0, T2D);\n-    eor3(v4, T16B, v16, v17, v8);\n+    pmull (v23, T1Q, v3, v31, T1D);\n+    pmull2(v24, T1Q, v3, v31, T2D);\n+    eor3(v3, T16B, v23, v24, v7);\n@@ -4351,14 +4399,17 @@\n-    ldrq(v5, Address(table, 0x20));\n-    pmull (v10, T1Q, v1, v5, T1D);\n-    pmull2(v11, T1Q, v1, v5, T2D);\n-    eor3(v4, T16B, v4, v10, v11);\n-\n-    ldrq(v6, Address(table, 0x30));\n-    pmull (v12, T1Q, v2, v6, T1D);\n-    pmull2(v13, T1Q, v2, v6, T2D);\n-    eor3(v4, T16B, v4, v12, v13);\n-\n-    ldrq(v7, Address(table, 0x40));\n-    pmull (v14, T1Q, v3, v7, T1D);\n-    pmull2(v15, T1Q, v3, v7, T2D);\n-    eor3(v1, T16B, v4, v14, v15);\n+    \/\/ Use v17 for constants because v31 can be still in use.\n+    ldrq(v17, Address(table, 0x20));\n+    pmull (v25, T1Q, v0, v17, T1D);\n+    pmull2(v26, T1Q, v0, v17, T2D);\n+    eor3(v3, T16B, v3, v25, v26);\n+\n+    \/\/ Use v18 for constants because v17 can be still in use.\n+    ldrq(v18, Address(table, 0x30));\n+    pmull (v27, T1Q, v1, v18, T1D);\n+    pmull2(v28, T1Q, v1, v18, T2D);\n+    eor3(v3, T16B, v3, v27, v28);\n+\n+    \/\/ Use v19 for constants because v18 can be still in use.\n+    ldrq(v19, Address(table, 0x40));\n+    pmull (v29, T1Q, v2, v19, T1D);\n+    pmull2(v30, T1Q, v2, v19, T2D);\n+    eor3(v0, T16B, v3, v29, v30);\n@@ -4369,2 +4420,2 @@\n-    mov(tmp0, v1, D, 0);\n-    mov(tmp1, v1, D, 1);\n+    mov(tmp0, v0, D, 0);\n+    mov(tmp1, v0, D, 1);\n@@ -5397,1 +5448,0 @@\n-\/\/ elem_size is the element size in bytes: either 1 or 2.\n@@ -5404,1 +5454,1 @@\n-                                   Register result, Register cnt1, int elem_size)\n+                                   Register result, Register cnt1)\n@@ -5411,1 +5461,0 @@\n-  assert(elem_size == 1 || elem_size == 2, \"must be 2 or 1 byte\");\n@@ -5416,2 +5465,1 @@\n-    const char kind = (elem_size == 2) ? 'U' : 'L';\n-    snprintf(comment, sizeof comment, \"{string_equals%c\", kind);\n+    snprintf(comment, sizeof comment, \"{string_equalsL\");\n@@ -5466,2 +5514,1 @@\n-  if (elem_size == 1) { \/\/ Only needed when comparing 1-byte elements\n-    tbz(cnt1, 0, SAME); \/\/ 0-1 bytes left.\n+  tbz(cnt1, 0, SAME); \/\/ 0-1 bytes left.\n@@ -5469,5 +5516,4 @@\n-      ldrb(tmp1, a1);\n-      ldrb(tmp2, a2);\n-      eorw(tmp1, tmp1, tmp2);\n-      cbnzw(tmp1, DONE);\n-    }\n+    ldrb(tmp1, a1);\n+    ldrb(tmp2, a2);\n+    eorw(tmp1, tmp1, tmp2);\n+    cbnzw(tmp1, DONE);\n@@ -6386,2 +6432,0 @@\n-\/\/ Branches to slow upon failure to lock the object, with ZF cleared.\n-\/\/ Falls through upon success with ZF set.\n@@ -6390,3 +6434,3 @@\n-\/\/  - hdr: the header, already loaded from obj, will be destroyed\n-\/\/  - t1, t2: temporary registers, will be destroyed\n-void MacroAssembler::lightweight_lock(Register obj, Register hdr, Register t1, Register t2, Label& slow) {\n+\/\/  - t1, t2, t3: temporary registers, will be destroyed\n+\/\/  - slow: branched to if locking fails, absolute offset may larger than 32KB (imm14 encoding).\n+void MacroAssembler::lightweight_lock(Register obj, Register t1, Register t2, Register t3, Label& slow) {\n@@ -6394,15 +6438,32 @@\n-  assert_different_registers(obj, hdr, t1, t2, rscratch1);\n-\n-  \/\/ Check if we would have space on lock-stack for the object.\n-  ldrw(t1, Address(rthread, JavaThread::lock_stack_top_offset()));\n-  cmpw(t1, (unsigned)LockStack::end_offset() - 1);\n-  br(Assembler::GT, slow);\n-\n-  \/\/ Load (object->mark() | 1) into hdr\n-  orr(hdr, hdr, markWord::unlocked_value);\n-  \/\/ Clear lock-bits, into t2\n-  eor(t2, hdr, markWord::unlocked_value);\n-  \/\/ Try to swing header from unlocked to locked\n-  \/\/ Clobbers rscratch1 when UseLSE is false\n-  cmpxchg(\/*addr*\/ obj, \/*expected*\/ hdr, \/*new*\/ t2, Assembler::xword,\n-          \/*acquire*\/ true, \/*release*\/ true, \/*weak*\/ false, t1);\n+  assert_different_registers(obj, t1, t2, t3, rscratch1);\n+\n+  Label push;\n+  const Register top = t1;\n+  const Register mark = t2;\n+  const Register t = t3;\n+\n+  \/\/ Preload the markWord. It is important that this is the first\n+  \/\/ instruction emitted as it is part of C1's null check semantics.\n+  ldr(mark, Address(obj, oopDesc::mark_offset_in_bytes()));\n+\n+  \/\/ Check if the lock-stack is full.\n+  ldrw(top, Address(rthread, JavaThread::lock_stack_top_offset()));\n+  cmpw(top, (unsigned)LockStack::end_offset());\n+  br(Assembler::GE, slow);\n+\n+  \/\/ Check for recursion.\n+  subw(t, top, oopSize);\n+  ldr(t, Address(rthread, t));\n+  cmp(obj, t);\n+  br(Assembler::EQ, push);\n+\n+  \/\/ Check header for monitor (0b10).\n+  tst(mark, markWord::monitor_value);\n+  br(Assembler::NE, slow);\n+\n+  \/\/ Try to lock. Transition lock bits 0b01 => 0b00\n+  assert(oopDesc::mark_offset_in_bytes() == 0, \"required to avoid lea\");\n+  orr(mark, mark, markWord::unlocked_value);\n+  eor(t, mark, markWord::unlocked_value);\n+  cmpxchg(\/*addr*\/ obj, \/*expected*\/ mark, \/*new*\/ t, Assembler::xword,\n+          \/*acquire*\/ true, \/*release*\/ false, \/*weak*\/ false, noreg);\n@@ -6411,5 +6472,5 @@\n-  \/\/ After successful lock, push object on lock-stack\n-  ldrw(t1, Address(rthread, JavaThread::lock_stack_top_offset()));\n-  str(obj, Address(rthread, t1));\n-  addw(t1, t1, oopSize);\n-  strw(t1, Address(rthread, JavaThread::lock_stack_top_offset()));\n+  bind(push);\n+  \/\/ After successful lock, push object on lock-stack.\n+  str(obj, Address(rthread, top));\n+  addw(top, top, oopSize);\n+  strw(top, Address(rthread, JavaThread::lock_stack_top_offset()));\n@@ -6419,2 +6480,0 @@\n-\/\/ Branches to slow upon failure, with ZF cleared.\n-\/\/ Falls through upon success, with ZF set.\n@@ -6423,3 +6482,3 @@\n-\/\/ - hdr: the (pre-loaded) header of the object\n-\/\/ - t1, t2: temporary registers\n-void MacroAssembler::lightweight_unlock(Register obj, Register hdr, Register t1, Register t2, Label& slow) {\n+\/\/ - t1, t2, t3: temporary registers\n+\/\/ - slow: branched to if unlocking fails, absolute offset may larger than 32KB (imm14 encoding).\n+void MacroAssembler::lightweight_unlock(Register obj, Register t1, Register t2, Register t3, Label& slow) {\n@@ -6427,1 +6486,2 @@\n-  assert_different_registers(obj, hdr, t1, t2, rscratch1);\n+  \/\/ cmpxchg clobbers rscratch1.\n+  assert_different_registers(obj, t1, t2, t3, rscratch1);\n@@ -6431,4 +6491,0 @@\n-    \/\/ The following checks rely on the fact that LockStack is only ever modified by\n-    \/\/ its owning thread, even if the lock got inflated concurrently; removal of LockStack\n-    \/\/ entries after inflation will happen delayed in that case.\n-\n@@ -6439,1 +6495,1 @@\n-    br(Assembler::GT, stack_ok);\n+    br(Assembler::GE, stack_ok);\n@@ -6443,18 +6499,0 @@\n-  {\n-    \/\/ Check if the top of the lock-stack matches the unlocked object.\n-    Label tos_ok;\n-    subw(t1, t1, oopSize);\n-    ldr(t1, Address(rthread, t1));\n-    cmpoop(t1, obj);\n-    br(Assembler::EQ, tos_ok);\n-    STOP(\"Top of lock-stack does not match the unlocked object\");\n-    bind(tos_ok);\n-  }\n-  {\n-    \/\/ Check that hdr is fast-locked.\n-    Label hdr_ok;\n-    tst(hdr, markWord::lock_mask_in_place);\n-    br(Assembler::EQ, hdr_ok);\n-    STOP(\"Header is not fast-locked\");\n-    bind(hdr_ok);\n-  }\n@@ -6463,2 +6501,4 @@\n-  \/\/ Load the new header (unlocked) into t1\n-  orr(t1, hdr, markWord::unlocked_value);\n+  Label unlocked, push_and_slow;\n+  const Register top = t1;\n+  const Register mark = t2;\n+  const Register t = t3;\n@@ -6466,4 +6506,5 @@\n-  \/\/ Try to swing header from locked to unlocked\n-  \/\/ Clobbers rscratch1 when UseLSE is false\n-  cmpxchg(obj, hdr, t1, Assembler::xword,\n-          \/*acquire*\/ true, \/*release*\/ true, \/*weak*\/ false, t2);\n+  \/\/ Check if obj is top of lock-stack.\n+  ldrw(top, Address(rthread, JavaThread::lock_stack_top_offset()));\n+  subw(top, top, oopSize);\n+  ldr(t, Address(rthread, top));\n+  cmp(obj, t);\n@@ -6472,3 +6513,14 @@\n-  \/\/ After successful unlock, pop object from lock-stack\n-  ldrw(t1, Address(rthread, JavaThread::lock_stack_top_offset()));\n-  subw(t1, t1, oopSize);\n+  \/\/ Pop lock-stack.\n+  DEBUG_ONLY(str(zr, Address(rthread, top));)\n+  strw(top, Address(rthread, JavaThread::lock_stack_top_offset()));\n+\n+  \/\/ Check if recursive.\n+  subw(t, top, oopSize);\n+  ldr(t, Address(rthread, t));\n+  cmp(obj, t);\n+  br(Assembler::EQ, unlocked);\n+\n+  \/\/ Not recursive. Check header for monitor (0b10).\n+  ldr(mark, Address(obj, oopDesc::mark_offset_in_bytes()));\n+  tbnz(mark, log2i_exact(markWord::monitor_value), push_and_slow);\n+\n@@ -6476,1 +6528,5 @@\n-  str(zr, Address(rthread, t1));\n+  \/\/ Check header not unlocked (0b01).\n+  Label not_unlocked;\n+  tbz(mark, log2i_exact(markWord::unlocked_value), not_unlocked);\n+  stop(\"lightweight_unlock already unlocked\");\n+  bind(not_unlocked);\n@@ -6478,1 +6534,16 @@\n-  strw(t1, Address(rthread, JavaThread::lock_stack_top_offset()));\n+\n+  \/\/ Try to unlock. Transition lock bits 0b00 => 0b01\n+  assert(oopDesc::mark_offset_in_bytes() == 0, \"required to avoid lea\");\n+  orr(t, mark, markWord::unlocked_value);\n+  cmpxchg(obj, mark, t, Assembler::xword,\n+          \/*acquire*\/ false, \/*release*\/ true, \/*weak*\/ false, noreg);\n+  br(Assembler::EQ, unlocked);\n+\n+  bind(push_and_slow);\n+  \/\/ Restore lock-stack and handle the unlock in runtime.\n+  DEBUG_ONLY(str(obj, Address(rthread, top));)\n+  addw(top, top, oopSize);\n+  strw(top, Address(rthread, JavaThread::lock_stack_top_offset()));\n+  b(slow);\n+\n+  bind(unlocked);\n","filename":"src\/hotspot\/cpu\/aarch64\/macroAssembler_aarch64.cpp","additions":249,"deletions":178,"binary":false,"changes":427,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -723,0 +723,1 @@\n+  void align(int modulus, int target);\n@@ -1252,0 +1253,2 @@\n+  static int ic_check_size();\n+  int ic_check(int end_alignment);\n@@ -1404,2 +1407,1 @@\n-  void string_equals(Register a1, Register a2, Register result, Register cnt1,\n-                     int elem_size);\n+  void string_equals(Register a1, Register a2, Register result, Register cnt1);\n@@ -1605,2 +1607,2 @@\n-  void lightweight_lock(Register obj, Register hdr, Register t1, Register t2, Label& slow);\n-  void lightweight_unlock(Register obj, Register hdr, Register t1, Register t2, Label& slow);\n+  void lightweight_lock(Register obj, Register t1, Register t2, Register t3, Label& slow);\n+  void lightweight_unlock(Register obj, Register t1, Register t2, Register t3, Label& slow);\n","filename":"src\/hotspot\/cpu\/aarch64\/macroAssembler_aarch64.hpp","additions":7,"deletions":5,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -164,4 +164,1 @@\n-  Register receiver = LIR_Assembler::receiverOpr()->as_register();\n-  int offset = __ offset();\n-  __ inline_cache_check(receiver, Ricklass);\n-  return offset;\n+  return __ ic_check(CodeEntryAlignment);\n@@ -1953,1 +1950,1 @@\n-  \/\/ (See CompiledStaticCall::set_to_interpreted())\n+  \/\/ (See CompiledDirectCall::set_to_interpreted())\n","filename":"src\/hotspot\/cpu\/arm\/c1_LIRAssembler_arm.cpp","additions":2,"deletions":5,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -46,10 +46,0 @@\n-void C1_MacroAssembler::inline_cache_check(Register receiver, Register iCache) {\n-  Label verified;\n-  load_klass(Rtemp, receiver);\n-  cmp(Rtemp, iCache);\n-  b(verified, eq); \/\/ jump over alignment no-ops\n-  jump(SharedRuntime::get_ic_miss_stub(), relocInfo::runtime_call_type);\n-  align(CodeEntryAlignment);\n-  bind(verified);\n-}\n-\n","filename":"src\/hotspot\/cpu\/arm\/c1_MacroAssembler_arm.cpp","additions":0,"deletions":10,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -77,1 +77,0 @@\n-const Register IC_Klass    = rax;   \/\/ where the IC klass is cached\n@@ -341,17 +340,1 @@\n-  Register receiver = FrameMap::receiver_opr->as_register();\n-  Register ic_klass = IC_Klass;\n-  const int ic_cmp_size = LP64_ONLY(10) NOT_LP64(9);\n-  const bool do_post_padding = VerifyOops || UseCompressedClassPointers;\n-  if (!do_post_padding) {\n-    \/\/ insert some nops so that the verified entry point is aligned on CodeEntryAlignment\n-    __ align(CodeEntryAlignment, __ offset() + ic_cmp_size);\n-  }\n-  int offset = __ offset();\n-  __ inline_cache_check(receiver, IC_Klass);\n-  assert(__ offset() % CodeEntryAlignment == 0 || do_post_padding, \"alignment must be correct\");\n-  if (do_post_padding) {\n-    \/\/ force alignment after the cache check.\n-    \/\/ It's been verified to be aligned if !VerifyOops\n-    __ align(CodeEntryAlignment);\n-  }\n-  return offset;\n+  return __ ic_check(CodeEntryAlignment);\n","filename":"src\/hotspot\/cpu\/x86\/c1_LIRAssembler_x86.cpp","additions":1,"deletions":18,"binary":false,"changes":19,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1999, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1999, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -28,0 +28,1 @@\n+#include \"code\/compiledIC.hpp\"\n@@ -68,5 +69,0 @@\n-  if (LockingMode != LM_PLACEHOLDER) {\n-    \/\/ Load object header\n-    movptr(hdr, Address(obj, hdr_offset));\n-  }\n-\n@@ -92,0 +88,2 @@\n+    \/\/ Load object header\n+    movptr(hdr, Address(obj, hdr_offset));\n@@ -163,3 +161,8 @@\n-    movptr(disp_hdr, Address(obj, hdr_offset));\n-    andptr(disp_hdr, ~(int32_t)markWord::lock_mask_in_place);\n-    lightweight_unlock(obj, disp_hdr, hdr, slow_case);\n+#ifdef _LP64\n+    lightweight_unlock(obj, disp_hdr, r15_thread, hdr, slow_case);\n+#else\n+    \/\/ This relies on the implementation of lightweight_unlock being able to handle\n+    \/\/ that the reg_rax and thread Register parameters may alias each other.\n+    get_thread(disp_hdr);\n+    lightweight_unlock(obj, disp_hdr, disp_hdr, hdr, slow_case);\n+#endif\n@@ -342,23 +345,0 @@\n-\n-\n-void C1_MacroAssembler::inline_cache_check(Register receiver, Register iCache) {\n-  verify_oop(receiver);\n-  \/\/ explicit null check not needed since load from [klass_offset] causes a trap\n-  \/\/ check against inline cache. This is checked in Universe::genesis().\n-  int start_offset = offset();\n-\n-  if (UseCompressedClassPointers) {\n-    load_klass(rscratch1, receiver, rscratch2);\n-    cmpptr(rscratch1, iCache);\n-  } else {\n-    cmpptr(iCache, Address(receiver, oopDesc::klass_offset_in_bytes()));\n-  }\n-  \/\/ if icache check fails, then jump to runtime routine\n-  \/\/ Note: RECEIVER must still contain the receiver!\n-  jump_cc(Assembler::notEqual,\n-          RuntimeAddress(SharedRuntime::get_ic_miss_stub()));\n-  const int ic_cmp_size = LP64_ONLY(10) NOT_LP64(9);\n-  assert(UseCompressedClassPointers || offset() - start_offset == ic_cmp_size, \"check alignment in emit_method_entry\");\n-}\n-\n-\n","filename":"src\/hotspot\/cpu\/x86\/c1_MacroAssembler_x86.cpp","additions":12,"deletions":32,"binary":false,"changes":44,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2020, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2020, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -80,6 +80,2 @@\n-#ifdef _LP64\n-int C2HandleAnonOMOwnerStub::max_size() const {\n-  \/\/ Max size of stub has been determined by testing with 0, in which case\n-  \/\/ C2CodeStubList::emit() will throw an assertion and report the actual size that\n-  \/\/ is needed.\n-  return DEBUG_ONLY(36) NOT_DEBUG(21);\n+int C2FastUnlockLightweightStub::max_size() const {\n+  return 128;\n@@ -88,6 +84,8 @@\n-void C2HandleAnonOMOwnerStub::emit(C2_MacroAssembler& masm) {\n-  __ bind(entry());\n-  Register mon = monitor();\n-  Register t = tmp();\n-  __ movptr(Address(mon, OM_OFFSET_NO_MONITOR_VALUE_TAG(owner)), r15_thread);\n-  __ subl(Address(r15_thread, JavaThread::lock_stack_top_offset()), oopSize);\n+void C2FastUnlockLightweightStub::emit(C2_MacroAssembler& masm) {\n+  assert(_t == rax, \"must be\");\n+\n+  Label restore_held_monitor_count_and_slow_path;\n+\n+  { \/\/ Restore lock-stack and handle the unlock in runtime.\n+\n+    __ bind(_push_and_slow_path);\n@@ -95,2 +93,3 @@\n-  __ movl(t, Address(r15_thread, JavaThread::lock_stack_top_offset()));\n-  __ movptr(Address(r15_thread, t), 0);\n+    \/\/ The obj was only cleared in debug.\n+    __ movl(_t, Address(_thread, JavaThread::lock_stack_top_offset()));\n+    __ movptr(Address(_thread, _t), _obj);\n@@ -98,1 +97,51 @@\n-  __ jmp(continuation());\n+    __ addl(Address(_thread, JavaThread::lock_stack_top_offset()), oopSize);\n+  }\n+\n+  { \/\/ Restore held monitor count and slow path.\n+\n+    __ bind(restore_held_monitor_count_and_slow_path);\n+    \/\/ Restore held monitor count.\n+    __ increment(Address(_thread, JavaThread::held_monitor_count_offset()));\n+    \/\/ increment will always result in ZF = 0 (no overflows).\n+    __ jmp(slow_path_continuation());\n+  }\n+\n+  { \/\/ Handle monitor medium path.\n+\n+    __ bind(_check_successor);\n+\n+    Label fix_zf_and_unlocked;\n+    const Register monitor = _mark;\n+\n+#ifndef _LP64\n+    __ jmpb(restore_held_monitor_count_and_slow_path);\n+#else \/\/ _LP64\n+    \/\/ successor null check.\n+    __ cmpptr(Address(monitor, OM_OFFSET_NO_MONITOR_VALUE_TAG(succ)), NULL_WORD);\n+    __ jccb(Assembler::equal, restore_held_monitor_count_and_slow_path);\n+\n+    \/\/ Release lock.\n+    __ movptr(Address(monitor, OM_OFFSET_NO_MONITOR_VALUE_TAG(owner)), NULL_WORD);\n+\n+    \/\/ Fence.\n+    \/\/ Instead of MFENCE we use a dummy locked add of 0 to the top-of-stack.\n+    __ lock(); __ addl(Address(rsp, 0), 0);\n+\n+    \/\/ Recheck successor.\n+    __ cmpptr(Address(monitor, OM_OFFSET_NO_MONITOR_VALUE_TAG(succ)), NULL_WORD);\n+    \/\/ Observed a successor after the release -> fence we have handed off the monitor\n+    __ jccb(Assembler::notEqual, fix_zf_and_unlocked);\n+\n+    \/\/ Try to relock, if it fails the monitor has been handed over\n+    \/\/ TODO: Caveat, this may fail due to deflation, which does\n+    \/\/       not handle the monitor handoff. Currently only works\n+    \/\/       due to the responsible thread.\n+    __ xorptr(rax, rax);\n+    __ lock(); __ cmpxchgptr(_thread, Address(monitor, OM_OFFSET_NO_MONITOR_VALUE_TAG(owner)));\n+    __ jccb  (Assembler::equal, restore_held_monitor_count_and_slow_path);\n+#endif\n+\n+    __ bind(fix_zf_and_unlocked);\n+    __ xorl(rax, rax);\n+    __ jmp(unlocked_continuation());\n+  }\n@@ -101,0 +150,1 @@\n+#ifdef _LP64\n","filename":"src\/hotspot\/cpu\/x86\/c2_CodeStubs_x86.cpp","additions":66,"deletions":16,"binary":false,"changes":82,"status":"modified"},{"patch":"@@ -566,0 +566,1 @@\n+  assert(LockingMode != LM_LIGHTWEIGHT, \"lightweight locking should use fast_lock_lightweight\");\n@@ -618,1 +619,2 @@\n-  } else if (LockingMode == LM_LEGACY) {\n+  } else {\n+    assert(LockingMode == LM_LEGACY, \"must be\");\n@@ -633,4 +635,0 @@\n-  } else {\n-    assert(LockingMode == LM_LIGHTWEIGHT, \"\");\n-    lightweight_lock(objReg, tmpReg, thread, scrReg, NO_COUNT);\n-    jmp(COUNT);\n@@ -767,0 +765,1 @@\n+  assert(LockingMode != LM_LIGHTWEIGHT, \"lightweight locking should use fast_unlock_lightweight\");\n@@ -798,17 +797,0 @@\n-  if (LockingMode == LM_LIGHTWEIGHT) {\n-    \/\/ If the owner is ANONYMOUS, we need to fix it -  in an outline stub.\n-    testb(Address(tmpReg, OM_OFFSET_NO_MONITOR_VALUE_TAG(owner)), (int32_t) ObjectMonitor::ANONYMOUS_OWNER);\n-#ifdef _LP64\n-    if (!Compile::current()->output()->in_scratch_emit_size()) {\n-      C2HandleAnonOMOwnerStub* stub = new (Compile::current()->comp_arena()) C2HandleAnonOMOwnerStub(tmpReg, boxReg);\n-      Compile::current()->output()->add_stub(stub);\n-      jcc(Assembler::notEqual, stub->entry());\n-      bind(stub->continuation());\n-    } else\n-#endif\n-    {\n-      \/\/ We can't easily implement this optimization on 32 bit because we don't have a thread register.\n-      \/\/ Call the slow-path instead.\n-      jcc(Assembler::notEqual, NO_COUNT);\n-    }\n-  }\n@@ -936,1 +918,1 @@\n-  if (LockingMode != LM_MONITOR) {\n+  if (LockingMode == LM_LEGACY) {\n@@ -938,9 +920,3 @@\n-    if (LockingMode == LM_LIGHTWEIGHT) {\n-      mov(boxReg, tmpReg);\n-      lightweight_unlock(objReg, boxReg, tmpReg, NO_COUNT);\n-      jmp(COUNT);\n-    } else if (LockingMode == LM_LEGACY) {\n-      movptr(tmpReg, Address (boxReg, 0));      \/\/ re-fetch\n-      lock();\n-      cmpxchgptr(tmpReg, Address(objReg, oopDesc::mark_offset_in_bytes())); \/\/ Uses RAX which is box\n-    }\n+    movptr(tmpReg, Address (boxReg, 0));      \/\/ re-fetch\n+    lock();\n+    cmpxchgptr(tmpReg, Address(objReg, oopDesc::mark_offset_in_bytes())); \/\/ Uses RAX which is box\n@@ -949,0 +925,1 @@\n+\n@@ -969,0 +946,241 @@\n+void C2_MacroAssembler::fast_lock_lightweight(Register obj, Register box, Register rax_reg,\n+                                              Register t, Register thread) {\n+  assert(LockingMode == LM_LIGHTWEIGHT, \"must be\");\n+  assert(rax_reg == rax, \"Used for CAS\");\n+  assert_different_registers(obj, box, rax_reg, t, thread);\n+\n+  \/\/ Handle inflated monitor.\n+  Label inflated;\n+  \/\/ Finish fast lock successfully. ZF value is irrelevant.\n+  Label locked;\n+  \/\/ Finish fast lock unsuccessfully. MUST jump with ZF == 0\n+  Label slow_path;\n+\n+  if (DiagnoseSyncOnValueBasedClasses != 0) {\n+    load_klass(rax_reg, obj, t);\n+    movl(rax_reg, Address(rax_reg, Klass::access_flags_offset()));\n+    testl(rax_reg, JVM_ACC_IS_VALUE_BASED_CLASS);\n+    jcc(Assembler::notZero, slow_path);\n+  }\n+\n+  const Register mark = t;\n+\n+  { \/\/ Lightweight Lock\n+\n+    Label push;\n+\n+    const Register top = box;\n+\n+    \/\/ Load the mark.\n+    movptr(mark, Address(obj, oopDesc::mark_offset_in_bytes()));\n+\n+    \/\/ Prefetch top.\n+    movl(top, Address(thread, JavaThread::lock_stack_top_offset()));\n+\n+    \/\/ Check for monitor (0b10).\n+    testptr(mark, markWord::monitor_value);\n+    jcc(Assembler::notZero, inflated);\n+\n+    \/\/ Check if lock-stack is full.\n+    cmpl(top, LockStack::end_offset() - 1);\n+    jcc(Assembler::greater, slow_path);\n+\n+    \/\/ Check if recursive.\n+    cmpptr(obj, Address(thread, top, Address::times_1, -oopSize));\n+    jccb(Assembler::equal, push);\n+\n+    \/\/ Try to lock. Transition lock bits 0b01 => 0b00\n+    movptr(rax_reg, mark);\n+    orptr(rax_reg, markWord::unlocked_value);\n+    andptr(mark, ~(int32_t)markWord::unlocked_value);\n+    lock(); cmpxchgptr(mark, Address(obj, oopDesc::mark_offset_in_bytes()));\n+    jcc(Assembler::notEqual, slow_path);\n+\n+    bind(push);\n+    \/\/ After successful lock, push object on lock-stack.\n+    movptr(Address(thread, top), obj);\n+    addl(Address(thread, JavaThread::lock_stack_top_offset()), oopSize);\n+    jmpb(locked);\n+  }\n+\n+  { \/\/ Handle inflated monitor.\n+    bind(inflated);\n+\n+    const Register tagged_monitor = mark;\n+\n+    \/\/ CAS owner (null => current thread).\n+    xorptr(rax_reg, rax_reg);\n+    lock(); cmpxchgptr(thread, Address(tagged_monitor, OM_OFFSET_NO_MONITOR_VALUE_TAG(owner)));\n+    jccb(Assembler::equal, locked);\n+\n+    \/\/ Check if recursive.\n+    cmpptr(thread, rax_reg);\n+    jccb(Assembler::notEqual, slow_path);\n+\n+    \/\/ Recursive.\n+    increment(Address(tagged_monitor, OM_OFFSET_NO_MONITOR_VALUE_TAG(recursions)));\n+  }\n+\n+  bind(locked);\n+  increment(Address(thread, JavaThread::held_monitor_count_offset()));\n+  \/\/ Set ZF = 1\n+  xorl(rax_reg, rax_reg);\n+\n+#ifdef ASSERT\n+  \/\/ Check that locked label is reached with ZF set.\n+  Label zf_correct;\n+  jccb(Assembler::zero, zf_correct);\n+  stop(\"Fast Lock ZF != 1\");\n+#endif\n+\n+  bind(slow_path);\n+#ifdef ASSERT\n+  \/\/ Check that slow_path label is reached with ZF not set.\n+  jccb(Assembler::notZero, zf_correct);\n+  stop(\"Fast Lock ZF != 0\");\n+  bind(zf_correct);\n+#endif\n+  \/\/ C2 uses the value of ZF to determine the continuation.\n+}\n+\n+void C2_MacroAssembler::fast_unlock_lightweight(Register obj, Register reg_rax, Register t, Register thread) {\n+  assert(LockingMode == LM_LIGHTWEIGHT, \"must be\");\n+  assert(reg_rax == rax, \"Used for CAS\");\n+  assert_different_registers(obj, reg_rax, t);\n+\n+  \/\/ Handle inflated monitor.\n+  Label inflated, inflated_check_lock_stack;\n+  \/\/ Finish fast unlock successfully.  MUST jump with ZF == 1\n+  Label unlocked;\n+\n+  \/\/ Assume success.\n+  decrement(Address(thread, JavaThread::held_monitor_count_offset()));\n+\n+  const Register mark = t;\n+  const Register top = reg_rax;\n+\n+  Label dummy;\n+  C2FastUnlockLightweightStub* stub = nullptr;\n+\n+  if (!Compile::current()->output()->in_scratch_emit_size()) {\n+    stub = new (Compile::current()->comp_arena()) C2FastUnlockLightweightStub(obj, mark, reg_rax, thread);\n+    Compile::current()->output()->add_stub(stub);\n+  }\n+\n+  Label& push_and_slow_path = stub == nullptr ? dummy : stub->push_and_slow_path();\n+  Label& check_successor = stub == nullptr ? dummy : stub->check_successor();\n+\n+  { \/\/ Lightweight Unlock\n+\n+    \/\/ Load top.\n+    movl(top, Address(thread, JavaThread::lock_stack_top_offset()));\n+\n+    \/\/ Prefetch mark.\n+    movptr(mark, Address(obj, oopDesc::mark_offset_in_bytes()));\n+\n+    \/\/ Check if obj is top of lock-stack.\n+    cmpptr(obj, Address(thread, top, Address::times_1, -oopSize));\n+    \/\/ Top of lock stack was not obj. Must be monitor.\n+    jcc(Assembler::notEqual, inflated_check_lock_stack);\n+\n+    \/\/ Pop lock-stack.\n+    DEBUG_ONLY(movptr(Address(thread, top, Address::times_1, -oopSize), 0);)\n+    subl(Address(thread, JavaThread::lock_stack_top_offset()), oopSize);\n+\n+    \/\/ Check if recursive.\n+    cmpptr(obj, Address(thread, top, Address::times_1, -2 * oopSize));\n+    jcc(Assembler::equal, unlocked);\n+\n+    \/\/ We elide the monitor check, let the CAS fail instead.\n+\n+    \/\/ Try to unlock. Transition lock bits 0b00 => 0b01\n+    movptr(reg_rax, mark);\n+    andptr(reg_rax, ~(int32_t)markWord::lock_mask);\n+    orptr(mark, markWord::unlocked_value);\n+    lock(); cmpxchgptr(mark, Address(obj, oopDesc::mark_offset_in_bytes()));\n+    jcc(Assembler::notEqual, push_and_slow_path);\n+    jmp(unlocked);\n+  }\n+\n+\n+  { \/\/ Handle inflated monitor.\n+    bind(inflated_check_lock_stack);\n+#ifdef ASSERT\n+    Label check_done;\n+    subl(top, oopSize);\n+    cmpl(top, in_bytes(JavaThread::lock_stack_base_offset()));\n+    jcc(Assembler::below, check_done);\n+    cmpptr(obj, Address(thread, top));\n+    jccb(Assembler::notEqual, inflated_check_lock_stack);\n+    stop(\"Fast Unlock lock on stack\");\n+    bind(check_done);\n+    testptr(mark, markWord::monitor_value);\n+    jccb(Assembler::notZero, inflated);\n+    stop(\"Fast Unlock not monitor\");\n+#endif\n+\n+    bind(inflated);\n+\n+    \/\/ mark contains the tagged ObjectMonitor*.\n+    const Register monitor = mark;\n+\n+#ifndef _LP64\n+    \/\/ Check if recursive.\n+    xorptr(reg_rax, reg_rax);\n+    orptr(reg_rax, Address(monitor, OM_OFFSET_NO_MONITOR_VALUE_TAG(recursions)));\n+    jcc(Assembler::notZero, check_successor);\n+\n+    \/\/ Check if the entry lists are empty.\n+    movptr(reg_rax, Address(monitor, OM_OFFSET_NO_MONITOR_VALUE_TAG(EntryList)));\n+    orptr(reg_rax, Address(monitor, OM_OFFSET_NO_MONITOR_VALUE_TAG(cxq)));\n+    jcc(Assembler::notZero, check_successor);\n+\n+    \/\/ Release lock.\n+    movptr(Address(monitor, OM_OFFSET_NO_MONITOR_VALUE_TAG(owner)), NULL_WORD);\n+#else \/\/ _LP64\n+    Label recursive;\n+\n+    \/\/ Check if recursive.\n+    cmpptr(Address(monitor, OM_OFFSET_NO_MONITOR_VALUE_TAG(recursions)), 0);\n+    jccb(Assembler::notEqual, recursive);\n+\n+    \/\/ Check if the entry lists are empty.\n+    movptr(reg_rax, Address(monitor, OM_OFFSET_NO_MONITOR_VALUE_TAG(cxq)));\n+    orptr(reg_rax, Address(monitor, OM_OFFSET_NO_MONITOR_VALUE_TAG(EntryList)));\n+    jcc(Assembler::notZero, check_successor);\n+\n+    \/\/ Release lock.\n+    movptr(Address(monitor, OM_OFFSET_NO_MONITOR_VALUE_TAG(owner)), NULL_WORD);\n+    jmpb(unlocked);\n+\n+    \/\/ Recursive unlock.\n+    bind(recursive);\n+    decrement(Address(monitor, OM_OFFSET_NO_MONITOR_VALUE_TAG(recursions)));\n+    xorl(t, t);\n+#endif\n+  }\n+\n+  bind(unlocked);\n+  if (stub != nullptr) {\n+    bind(stub->unlocked_continuation());\n+  }\n+\n+#ifdef ASSERT\n+  \/\/ Check that unlocked label is reached with ZF set.\n+  Label zf_correct;\n+  jccb(Assembler::zero, zf_correct);\n+  stop(\"Fast Unlock ZF != 1\");\n+#endif\n+\n+  if (stub != nullptr) {\n+    bind(stub->slow_path_continuation());\n+  }\n+#ifdef ASSERT\n+  \/\/ Check that stub->continuation() label is reached with ZF not set.\n+  jccb(Assembler::notZero, zf_correct);\n+  stop(\"Fast Unlock ZF != 0\");\n+  bind(zf_correct);\n+#endif\n+  \/\/ C2 uses the value of ZF to determine the continuation.\n+}\n+\n","filename":"src\/hotspot\/cpu\/x86\/c2_MacroAssembler_x86.cpp","additions":250,"deletions":32,"binary":false,"changes":282,"status":"modified"},{"patch":"@@ -46,0 +46,4 @@\n+  void fast_lock_lightweight(Register obj, Register box, Register rax_reg,\n+                             Register t, Register thread);\n+  void fast_unlock_lightweight(Register obj, Register reg_rax, Register t, Register thread);\n+\n","filename":"src\/hotspot\/cpu\/x86\/c2_MacroAssembler_x86.hpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -28,0 +28,1 @@\n+#include \"code\/compiledIC.hpp\"\n@@ -1344,1 +1345,1 @@\n-  mov64(rax, (intptr_t)Universe::non_oop_word());\n+  mov64(rax, (int64_t)Universe::non_oop_word());\n@@ -1351,0 +1352,32 @@\n+int MacroAssembler::ic_check_size() {\n+  return LP64_ONLY(14) NOT_LP64(12);\n+}\n+\n+int MacroAssembler::ic_check(int end_alignment) {\n+  Register receiver = LP64_ONLY(j_rarg0) NOT_LP64(rcx);\n+  Register data = rax;\n+  Register temp = LP64_ONLY(rscratch1) NOT_LP64(rbx);\n+\n+  \/\/ The UEP of a code blob ensures that the VEP is padded. However, the padding of the UEP is placed\n+  \/\/ before the inline cache check, so we don't have to execute any nop instructions when dispatching\n+  \/\/ through the UEP, yet we can ensure that the VEP is aligned appropriately. That's why we align\n+  \/\/ before the inline cache check here, and not after\n+  align(end_alignment, offset() + ic_check_size());\n+\n+  int uep_offset = offset();\n+\n+  if (UseCompressedClassPointers) {\n+    movl(temp, Address(receiver, oopDesc::klass_offset_in_bytes()));\n+    cmpl(temp, Address(data, CompiledICData::speculated_klass_offset()));\n+  } else {\n+    movptr(temp, Address(receiver, oopDesc::klass_offset_in_bytes()));\n+    cmpptr(temp, Address(data, CompiledICData::speculated_klass_offset()));\n+  }\n+\n+  \/\/ if inline cache check fails, then jump to runtime routine\n+  jump_cc(Assembler::notEqual, RuntimeAddress(SharedRuntime::get_ic_miss_stub()));\n+  assert((offset() % end_alignment) == 0, \"Misaligned verified entry point\");\n+\n+  return uep_offset;\n+}\n+\n@@ -4090,2 +4123,3 @@\n-int register_section_sizes(RegSet gp_registers, XMMRegSet xmm_registers, bool save_fpu,\n-                           int& gp_area_size, int& fp_area_size, int& xmm_area_size) {\n+static int register_section_sizes(RegSet gp_registers, XMMRegSet xmm_registers,\n+                                  bool save_fpu, int& gp_area_size,\n+                                  int& fp_area_size, int& xmm_area_size) {\n@@ -4357,1 +4391,1 @@\n-\/\/ - check recv_klass (actual object class) is a subtype of resolved_klass from CompiledICHolder\n+\/\/ - check recv_klass (actual object class) is a subtype of resolved_klass from CompiledICData\n@@ -9939,2 +9973,0 @@\n-\/\/ Branches to slow upon failure to lock the object, with ZF cleared.\n-\/\/ Falls through upon success with unspecified ZF.\n@@ -9943,1 +9975,1 @@\n-\/\/ hdr: the (pre-loaded) header of the object, must be rax\n+\/\/ reg_rax: rax\n@@ -9946,19 +9978,31 @@\n-void MacroAssembler::lightweight_lock(Register obj, Register hdr, Register thread, Register tmp, Label& slow) {\n-  assert(hdr == rax, \"header must be in rax for cmpxchg\");\n-  assert_different_registers(obj, hdr, thread, tmp);\n-\n-  \/\/ First we need to check if the lock-stack has room for pushing the object reference.\n-  \/\/ Note: we subtract 1 from the end-offset so that we can do a 'greater' comparison, instead\n-  \/\/ of 'greaterEqual' below, which readily clears the ZF. This makes C2 code a little simpler and\n-  \/\/ avoids one branch.\n-  cmpl(Address(thread, JavaThread::lock_stack_top_offset()), LockStack::end_offset() - 1);\n-  jcc(Assembler::greater, slow);\n-\n-  \/\/ Now we attempt to take the fast-lock.\n-  \/\/ Clear lock_mask bits (locked state).\n-  andptr(hdr, ~(int32_t)markWord::lock_mask_in_place);\n-  movptr(tmp, hdr);\n-  \/\/ Set unlocked_value bit.\n-  orptr(hdr, markWord::unlocked_value);\n-  lock();\n-  cmpxchgptr(tmp, Address(obj, oopDesc::mark_offset_in_bytes()));\n+void MacroAssembler::lightweight_lock(Register obj, Register reg_rax, Register thread, Register tmp, Label& slow) {\n+  assert(reg_rax == rax, \"\");\n+  assert_different_registers(obj, reg_rax, thread, tmp);\n+\n+  Label push;\n+  const Register top = tmp;\n+\n+  \/\/ Preload the markWord. It is important that this is the first\n+  \/\/ instruction emitted as it is part of C1's null check semantics.\n+  movptr(reg_rax, Address(obj, oopDesc::mark_offset_in_bytes()));\n+\n+  \/\/ Load top.\n+  movl(top, Address(thread, JavaThread::lock_stack_top_offset()));\n+\n+  \/\/ Check if the lock-stack is full.\n+  cmpl(top, LockStack::end_offset());\n+  jcc(Assembler::greaterEqual, slow);\n+\n+  \/\/ Check for recursion.\n+  cmpptr(obj, Address(thread, top, Address::times_1, -oopSize));\n+  jcc(Assembler::equal, push);\n+\n+  \/\/ Check header for monitor (0b10).\n+  testptr(reg_rax, markWord::monitor_value);\n+  jcc(Assembler::notZero, slow);\n+\n+  \/\/ Try to lock. Transition lock bits 0b01 => 0b00\n+  movptr(tmp, reg_rax);\n+  andptr(tmp, ~(int32_t)markWord::unlocked_value);\n+  orptr(reg_rax, markWord::unlocked_value);\n+  lock(); cmpxchgptr(tmp, Address(obj, oopDesc::mark_offset_in_bytes()));\n@@ -9967,5 +10011,8 @@\n-  \/\/ If successful, push object to lock-stack.\n-  movl(tmp, Address(thread, JavaThread::lock_stack_top_offset()));\n-  movptr(Address(thread, tmp), obj);\n-  incrementl(tmp, oopSize);\n-  movl(Address(thread, JavaThread::lock_stack_top_offset()), tmp);\n+  \/\/ Restore top, CAS clobbers register.\n+  movl(top, Address(thread, JavaThread::lock_stack_top_offset()));\n+\n+  bind(push);\n+  \/\/ After successful lock, push object on lock-stack.\n+  movptr(Address(thread, top), obj);\n+  incrementl(top, oopSize);\n+  movl(Address(thread, JavaThread::lock_stack_top_offset()), top);\n@@ -9975,2 +10022,0 @@\n-\/\/ Branches to slow upon failure, with ZF cleared.\n-\/\/ Falls through upon success, with unspecified ZF.\n@@ -9979,1 +10024,2 @@\n-\/\/ hdr: the (pre-loaded) header of the object, must be rax\n+\/\/ reg_rax: rax\n+\/\/ thread: the thread\n@@ -9981,3 +10027,7 @@\n-void MacroAssembler::lightweight_unlock(Register obj, Register hdr, Register tmp, Label& slow) {\n-  assert(hdr == rax, \"header must be in rax for cmpxchg\");\n-  assert_different_registers(obj, hdr, tmp);\n+\/\/\n+\/\/ x86_32 Note: reg_rax and thread may alias each other due to limited register\n+\/\/              availiability.\n+void MacroAssembler::lightweight_unlock(Register obj, Register reg_rax, Register thread, Register tmp, Label& slow) {\n+  assert(reg_rax == rax, \"\");\n+  assert_different_registers(obj, reg_rax, tmp);\n+  LP64_ONLY(assert_different_registers(obj, reg_rax, thread, tmp);)\n@@ -9985,5 +10035,6 @@\n-  \/\/ Mark-word must be lock_mask now, try to swing it back to unlocked_value.\n-  movptr(tmp, hdr); \/\/ The expected old value\n-  orptr(tmp, markWord::unlocked_value);\n-  lock();\n-  cmpxchgptr(tmp, Address(obj, oopDesc::mark_offset_in_bytes()));\n+  Label unlocked, push_and_slow;\n+  const Register top = tmp;\n+\n+  \/\/ Check if obj is top of lock-stack.\n+  movl(top, Address(thread, JavaThread::lock_stack_top_offset()));\n+  cmpptr(obj, Address(thread, top, Address::times_1, -oopSize));\n@@ -9991,7 +10042,3 @@\n-  \/\/ Pop the lock object from the lock-stack.\n-#ifdef _LP64\n-  const Register thread = r15_thread;\n-#else\n-  const Register thread = rax;\n-  get_thread(thread);\n-#endif\n+\n+  \/\/ Pop lock-stack.\n+  DEBUG_ONLY(movptr(Address(thread, top, Address::times_1, -oopSize), 0);)\n@@ -9999,0 +10046,31 @@\n+\n+  \/\/ Check if recursive.\n+  cmpptr(obj, Address(thread, top, Address::times_1, -2 * oopSize));\n+  jcc(Assembler::equal, unlocked);\n+\n+  \/\/ Not recursive. Check header for monitor (0b10).\n+  movptr(reg_rax, Address(obj, oopDesc::mark_offset_in_bytes()));\n+  testptr(reg_rax, markWord::monitor_value);\n+  jcc(Assembler::notZero, push_and_slow);\n+\n+#ifdef ASSERT\n+  \/\/ Check header not unlocked (0b01).\n+  Label not_unlocked;\n+  testptr(reg_rax, markWord::unlocked_value);\n+  jcc(Assembler::zero, not_unlocked);\n+  stop(\"lightweight_unlock already unlocked\");\n+  bind(not_unlocked);\n+#endif\n+\n+  \/\/ Try to unlock. Transition lock bits 0b00 => 0b01\n+  movptr(tmp, reg_rax);\n+  orptr(tmp, markWord::unlocked_value);\n+  lock(); cmpxchgptr(tmp, Address(obj, oopDesc::mark_offset_in_bytes()));\n+  jcc(Assembler::equal, unlocked);\n+\n+  bind(push_and_slow);\n+  \/\/ Restore lock-stack and handle the unlock in runtime.\n+  if (thread == reg_rax) {\n+    \/\/ On x86_32 we may lose the thread.\n+    get_thread(thread);\n+  }\n@@ -10000,2 +10078,2 @@\n-  movl(tmp, Address(thread, JavaThread::lock_stack_top_offset()));\n-  movptr(Address(thread, tmp), 0);\n+  movl(top, Address(thread, JavaThread::lock_stack_top_offset()));\n+  movptr(Address(thread, top), obj);\n@@ -10003,0 +10081,4 @@\n+  addl(Address(thread, JavaThread::lock_stack_top_offset()), oopSize);\n+  jmp(slow);\n+\n+  bind(unlocked);\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.cpp","additions":134,"deletions":52,"binary":false,"changes":186,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -910,0 +910,2 @@\n+  static int ic_check_size();\n+  int ic_check(int end_alignment);\n@@ -2045,2 +2047,2 @@\n-  void lightweight_lock(Register obj, Register hdr, Register thread, Register tmp, Label& slow);\n-  void lightweight_unlock(Register obj, Register hdr, Register tmp, Label& slow);\n+  void lightweight_lock(Register obj, Register reg_rax, Register thread, Register tmp, Label& slow);\n+  void lightweight_unlock(Register obj, Register reg_rax, Register thread, Register tmp, Label& slow);\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.hpp","additions":5,"deletions":3,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n-\/\/ Copyright (c) 2003, 2023, Oracle and\/or its affiliates. All rights reserved.\n+\/\/ Copyright (c) 2003, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -522,1 +522,1 @@\n-void emit_cmpfp_fixup(MacroAssembler& _masm) {\n+static void emit_cmpfp_fixup(MacroAssembler& _masm) {\n@@ -542,1 +542,1 @@\n-void emit_cmpfp3(MacroAssembler& _masm, Register dst) {\n+static void emit_cmpfp3(MacroAssembler& _masm, Register dst) {\n@@ -561,4 +561,4 @@\n-void emit_fp_min_max(MacroAssembler& _masm, XMMRegister dst,\n-                     XMMRegister a, XMMRegister b,\n-                     XMMRegister xmmt, Register rt,\n-                     bool min, bool single) {\n+static void emit_fp_min_max(MacroAssembler& _masm, XMMRegister dst,\n+                            XMMRegister a, XMMRegister b,\n+                            XMMRegister xmmt, Register rt,\n+                            bool min, bool single) {\n@@ -709,1 +709,1 @@\n-  if (C->stub_function() != NULL && BarrierSet::barrier_set()->barrier_set_nmethod() != NULL) {\n+  if (C->stub_function() != nullptr && BarrierSet::barrier_set()->barrier_set_nmethod() != nullptr) {\n@@ -744,1 +744,1 @@\n-  __ verified_entry(framesize, C->output()->need_stack_bang(bangsize)?bangsize:0, false, C->stub_function() != NULL);\n+  __ verified_entry(framesize, C->output()->need_stack_bang(bangsize)?bangsize:0, false, C->stub_function() != nullptr);\n@@ -973,1 +973,1 @@\n-  assert(cbuf != NULL || st  != NULL, \"sanity\");\n+  assert(cbuf != nullptr || st  != nullptr, \"sanity\");\n@@ -992,1 +992,1 @@\n-  if (bottom_type()->isa_vect() != NULL && bottom_type()->isa_vectmask() == NULL) {\n+  if (bottom_type()->isa_vect() != nullptr && bottom_type()->isa_vectmask() == nullptr) {\n@@ -1431,1 +1431,1 @@\n-  implementation(NULL, ra_, false, st);\n+  implementation(nullptr, ra_, false, st);\n@@ -1436,1 +1436,1 @@\n-  implementation(&cbuf, ra_, false, NULL);\n+  implementation(&cbuf, ra_, false, nullptr);\n@@ -1475,2 +1475,1 @@\n-    st->print_cr(\"\\tdecode_klass_not_null rscratch1, rscratch1\");\n-    st->print_cr(\"\\tcmpq    rax, rscratch1\\t # Inline cache check\");\n+    st->print_cr(\"\\tcmpl    rscratch1, [rax + CompiledICData::speculated_klass_offset()]\\t # Inline cache check\");\n@@ -1478,2 +1477,2 @@\n-    st->print_cr(\"\\tcmpq    rax, [j_rarg0 + oopDesc::klass_offset_in_bytes()]\\t\"\n-                 \"# Inline cache check\");\n+    st->print_cr(\"movq    rscratch1, [j_rarg0 + oopDesc::klass_offset_in_bytes()]\\t# compressed klass\");\n+    st->print_cr(\"\\tcmpq    rscratch1, [rax + CompiledICData::speculated_klass_offset()]\\t # Inline cache check\");\n@@ -1482,1 +1481,0 @@\n-  st->print_cr(\"\\tnop\\t# nops to align entry point\");\n@@ -1489,20 +1487,1 @@\n-  uint insts_size = cbuf.insts_size();\n-  if (UseCompressedClassPointers) {\n-    masm.load_klass(rscratch1, j_rarg0, rscratch2);\n-    masm.cmpptr(rax, rscratch1);\n-  } else {\n-    masm.cmpptr(rax, Address(j_rarg0, oopDesc::klass_offset_in_bytes()));\n-  }\n-\n-  masm.jump_cc(Assembler::notEqual, RuntimeAddress(SharedRuntime::get_ic_miss_stub()));\n-\n-  \/* WARNING these NOPs are critical so that verified entry point is properly\n-     4 bytes aligned for patching by NativeJump::patch_verified_entry() *\/\n-  int nops_cnt = 4 - ((cbuf.insts_size() - insts_size) & 0x3);\n-  if (OptoBreakpoint) {\n-    \/\/ Leave space for int3\n-    nops_cnt -= 1;\n-  }\n-  nops_cnt &= 0x3; \/\/ Do not add nops if code is aligned.\n-  if (nops_cnt > 0)\n-    masm.nop(nops_cnt);\n+  masm.ic_check(InteriorEntryAlignment);\n@@ -1787,1 +1766,1 @@\n-                                     NULL, &miss,\n+                                     nullptr, &miss,\n@@ -1843,2 +1822,2 @@\n-        address stub = CompiledStaticCall::emit_to_interp_stub(cbuf, mark);\n-        if (stub == NULL) {\n+        address stub = CompiledDirectCall::emit_to_interp_stub(cbuf, mark);\n+        if (stub == nullptr) {\n@@ -2182,1 +2161,1 @@\n-\/\/ NULL Pointer Immediate\n+\/\/ Null Pointer Immediate\n@@ -2210,1 +2189,1 @@\n-\/\/ NULL Pointer Immediate\n+\/\/ Null Pointer Immediate\n@@ -3124,1 +3103,1 @@\n-\/\/ we can't free r12 even with CompressedOops::base() == NULL.\n+\/\/ we can't free r12 even with CompressedOops::base() == nullptr.\n@@ -4921,1 +4900,1 @@\n-  format %{ \"xorq    $dst, $src\\t# compressed NULL ptr\" %}\n+  format %{ \"xorq    $dst, $src\\t# compressed null pointer\" %}\n@@ -4935,1 +4914,1 @@\n-    if (con == NULL) {\n+    if (con == nullptr) {\n@@ -4951,1 +4930,1 @@\n-    if (con == NULL) {\n+    if (con == nullptr) {\n@@ -5177,1 +5156,1 @@\n-  predicate(UseCompressedOops && (CompressedOops::base() == NULL) && n->as_Store()->barrier_data() == 0);\n+  predicate(UseCompressedOops && (CompressedOops::base() == nullptr) && n->as_Store()->barrier_data() == 0);\n@@ -5188,1 +5167,1 @@\n-\/\/ Store NULL Pointer, mark word, or other simple pointer constant.\n+\/\/ Store Null Pointer, mark word, or other simple pointer constant.\n@@ -5229,1 +5208,1 @@\n-  predicate(CompressedOops::base() == NULL);\n+  predicate(CompressedOops::base() == nullptr);\n@@ -5248,1 +5227,1 @@\n-    if (con == NULL) {\n+    if (con == nullptr) {\n@@ -5272,1 +5251,1 @@\n-  predicate(UseCompressedOops && (CompressedOops::base() == NULL));\n+  predicate(UseCompressedOops && (CompressedOops::base() == nullptr));\n@@ -5298,1 +5277,1 @@\n-  predicate(UseCompressedOops && (CompressedOops::base() == NULL));\n+  predicate(UseCompressedOops && (CompressedOops::base() == nullptr));\n@@ -5324,1 +5303,1 @@\n-  predicate(UseCompressedOops && (CompressedOops::base() == NULL));\n+  predicate(UseCompressedOops && (CompressedOops::base() == nullptr));\n@@ -5351,1 +5330,1 @@\n-  predicate(UseCompressedOops && (CompressedOops::base() == NULL));\n+  predicate(UseCompressedOops && (CompressedOops::base() == nullptr));\n@@ -5377,1 +5356,1 @@\n-  predicate(UseCompressedOops && (CompressedOops::base() == NULL));\n+  predicate(UseCompressedOops && (CompressedOops::base() == nullptr));\n@@ -5416,1 +5395,1 @@\n-  predicate(UseCompressedOops && (CompressedOops::base() == NULL));\n+  predicate(UseCompressedOops && (CompressedOops::base() == nullptr));\n@@ -5455,1 +5434,1 @@\n-  predicate(!UseCompressedOops || (CompressedOops::base() != NULL));\n+  predicate(!UseCompressedOops || (CompressedOops::base() != nullptr));\n@@ -5468,1 +5447,1 @@\n-  predicate(UseCompressedOops && (CompressedOops::base() == NULL));\n+  predicate(UseCompressedOops && (CompressedOops::base() == nullptr));\n@@ -11699,1 +11678,1 @@\n-  predicate((!UseCompressedOops || (CompressedOops::base() != NULL)) &&\n+  predicate((!UseCompressedOops || (CompressedOops::base() != nullptr)) &&\n@@ -11713,1 +11692,1 @@\n-  predicate(UseCompressedOops && (CompressedOops::base() == NULL) &&\n+  predicate(UseCompressedOops && (CompressedOops::base() == nullptr) &&\n@@ -11797,1 +11776,1 @@\n-  predicate(CompressedOops::base() != NULL);\n+  predicate(CompressedOops::base() != nullptr);\n@@ -11810,1 +11789,1 @@\n-  predicate(CompressedOops::base() == NULL);\n+  predicate(CompressedOops::base() == nullptr);\n@@ -12424,1 +12403,1 @@\n-  predicate(LockingMode != LM_PLACEHOLDER && !Compile::current()->use_rtm());\n+  predicate(LockingMode != LM_LIGHTWEIGHT && LockingMode != LM_PLACEHOLDER && !Compile::current()->use_rtm());\n@@ -12437,1 +12416,1 @@\n-  predicate(LockingMode != LM_PLACEHOLDER);\n+  predicate(LockingMode != LM_LIGHTWEIGHT && LockingMode != LM_PLACEHOLDER);\n@@ -12448,0 +12427,24 @@\n+instruct cmpFastLockLightweight(rFlagsReg cr, rRegP object, rbx_RegP box, rax_RegI rax_reg, rRegP tmp) %{\n+  predicate(LockingMode == LM_LIGHTWEIGHT);\n+  match(Set cr (FastLock object box));\n+  effect(TEMP rax_reg, TEMP tmp, USE_KILL box);\n+  ins_cost(300);\n+  format %{ \"fastlock $object,$box\\t! kills $box,$rax_reg,$tmp\" %}\n+  ins_encode %{\n+    __ fast_lock_lightweight($object$$Register, $box$$Register, $rax_reg$$Register, $tmp$$Register, r15_thread);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct cmpFastUnlockLightweight(rFlagsReg cr, rRegP object, rax_RegP rax_reg, rRegP tmp) %{\n+  predicate(LockingMode == LM_LIGHTWEIGHT);\n+  match(Set cr (FastUnlock object rax_reg));\n+  effect(TEMP tmp, USE_KILL rax_reg);\n+  ins_cost(300);\n+  format %{ \"fastunlock $object,$rax_reg\\t! kills $rax_reg,$tmp\" %}\n+  ins_encode %{\n+    __ fast_unlock_lightweight($object$$Register, $rax_reg$$Register, $tmp$$Register, r15_thread);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n","filename":"src\/hotspot\/cpu\/x86\/x86_64.ad","additions":68,"deletions":65,"binary":false,"changes":133,"status":"modified"},{"patch":"@@ -1669,3 +1669,3 @@\n-char* map_memory(int fd, const char* file_name, size_t file_offset,\n-                 char *addr, size_t bytes, bool read_only,\n-                 bool allow_exec, MEMFLAGS flags = mtNone) {\n+static char* map_memory(int fd, const char* file_name, size_t file_offset,\n+                        char *addr, size_t bytes, bool read_only,\n+                        bool allow_exec, MEMFLAGS flags = mtNone) {\n","filename":"src\/hotspot\/share\/cds\/filemap.cpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -286,7 +286,0 @@\n-bool VtableStubs::is_icholder_entry(address pc) {\n-  assert(contains(pc), \"must contain all vtable blobs\");\n-  VtableStub* stub = (VtableStub*)(pc - VtableStub::entry_offset());\n-  \/\/ itable stubs use CompiledICHolder.\n-  return stub->is_itable_stub();\n-}\n-\n","filename":"src\/hotspot\/share\/code\/vtableStubs.cpp","additions":0,"deletions":7,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -30,1 +30,0 @@\n-#include \"code\/icBuffer.hpp\"\n@@ -1141,1 +1140,0 @@\n-  _soft_ref_policy(),\n@@ -1532,4 +1530,0 @@\n-SoftRefPolicy* G1CollectedHeap::soft_ref_policy() {\n-  return &_soft_ref_policy;\n-}\n-\n@@ -2209,2 +2203,0 @@\n-  assert(InlineCacheBuffer::is_empty(), \"should have cleaned up ICBuffer\");\n-\n","filename":"src\/hotspot\/share\/gc\/g1\/g1CollectedHeap.cpp","additions":0,"deletions":8,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -123,0 +123,1 @@\n+      size_t pretouch_page_size = UseLargePages ? page_size : os::vm_page_size();\n@@ -124,1 +125,1 @@\n-                             page_size, pretouch_workers);\n+                             pretouch_page_size, pretouch_workers);\n@@ -127,1 +128,1 @@\n-                             page_size, pretouch_workers);\n+                             pretouch_page_size, pretouch_workers);\n","filename":"src\/hotspot\/share\/gc\/parallel\/mutableSpace.cpp","additions":3,"deletions":2,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -64,85 +64,0 @@\n-class ScavengeHelper {\n-  DefNewGeneration* _young_gen;\n-  HeapWord*         _young_gen_end;\n-public:\n-  ScavengeHelper(DefNewGeneration* young_gen) :\n-    _young_gen(young_gen),\n-    _young_gen_end(young_gen->reserved().end()) {}\n-\n-  bool is_in_young_gen(void* p) const {\n-    return p < _young_gen_end;\n-  }\n-\n-  template <typename T, typename Func>\n-  void try_scavenge(T* p, Func&& f) {\n-    T heap_oop = RawAccess<>::oop_load(p);\n-    \/\/ Should we copy the obj?\n-    if (!CompressedOops::is_null(heap_oop)) {\n-      oop obj = CompressedOops::decode_not_null(heap_oop);\n-      if (is_in_young_gen(obj)) {\n-        assert(!_young_gen->to()->is_in_reserved(obj), \"Scanning field twice?\");\n-        oop new_obj = obj->is_forwarded() ? obj->forwardee()\n-                                          : _young_gen->copy_to_survivor_space(obj);\n-        RawAccess<IS_NOT_NULL>::oop_store(p, new_obj);\n-\n-        \/\/ callback\n-        f(new_obj);\n-      }\n-    }\n-  }\n-};\n-\n-class InHeapScanClosure : public BasicOopIterateClosure {\n-  ScavengeHelper _helper;\n-protected:\n-  bool is_in_young_gen(void* p) const {\n-    return _helper.is_in_young_gen(p);\n-  }\n-\n-  template <typename T, typename Func>\n-  void try_scavenge(T* p, Func&& f) {\n-    _helper.try_scavenge(p, f);\n-  }\n-\n-  InHeapScanClosure(DefNewGeneration* young_gen) :\n-    BasicOopIterateClosure(young_gen->ref_processor()),\n-    _helper(young_gen) {}\n-};\n-\n-class OffHeapScanClosure : public OopClosure {\n-  ScavengeHelper _helper;\n-protected:\n-  bool is_in_young_gen(void* p) const {\n-    return _helper.is_in_young_gen(p);\n-  }\n-\n-  template <typename T, typename Func>\n-  void try_scavenge(T* p, Func&& f) {\n-    _helper.try_scavenge(p, f);\n-  }\n-\n-  OffHeapScanClosure(DefNewGeneration* young_gen) :  _helper(young_gen) {}\n-};\n-\n-class OldGenScanClosure : public InHeapScanClosure {\n-  CardTableRS* _rs;\n-\n-  template <typename T>\n-  void do_oop_work(T* p) {\n-    assert(!is_in_young_gen(p), \"precondition\");\n-\n-    try_scavenge(p, [&] (oop new_obj) {\n-      \/\/ If p points to a younger generation, mark the card.\n-      if (is_in_young_gen(new_obj)) {\n-        _rs->inline_write_ref_field_gc(p);\n-      }\n-    });\n-  }\n-public:\n-  OldGenScanClosure(DefNewGeneration* g) : InHeapScanClosure(g),\n-    _rs(SerialHeap::heap()->rem_set()) {}\n-\n-  void do_oop(oop* p)       { do_oop_work(p); }\n-  void do_oop(narrowOop* p) { do_oop_work(p); }\n-};\n-\n@@ -423,3 +338,3 @@\n-  assert(Space::is_aligned(eden_start), \"checking alignment\");\n-  assert(Space::is_aligned(from_start), \"checking alignment\");\n-  assert(Space::is_aligned(to_start),   \"checking alignment\");\n+  assert(is_aligned(eden_start, SpaceAlignment), \"checking alignment\");\n+  assert(is_aligned(from_start, SpaceAlignment), \"checking alignment\");\n+  assert(is_aligned(to_start, SpaceAlignment),   \"checking alignment\");\n@@ -676,6 +591,9 @@\n-\n-void DefNewGeneration::space_iterate(SpaceClosure* blk,\n-                                     bool usedOnly) {\n-  blk->do_space(eden());\n-  blk->do_space(from());\n-  blk->do_space(to());\n+HeapWord* DefNewGeneration::block_start(const void* p) const {\n+  if (eden()->is_in_reserved(p)) {\n+    return eden()->block_start_const(p);\n+  }\n+  if (from()->is_in_reserved(p)) {\n+    return from()->block_start_const(p);\n+  }\n+  assert(to()->is_in_reserved(p), \"inv\");\n+  return to()->block_start_const(p);\n@@ -785,1 +703,11 @@\n-    CLDScanClosure cld_scan_closure{this};\n+    CLDScanClosure cld_cl{this};\n+\n+    MarkingCodeBlobClosure code_cl(&root_cl,\n+                                   CodeBlobToOopClosure::FixRelocations,\n+                                   false \/* keepalive_nmethods *\/);\n+\n+    heap->process_roots(SerialHeap::SO_ScavengeCodeCache,\n+                        &root_cl,\n+                        &cld_cl,\n+                        &cld_cl,\n+                        &code_cl);\n@@ -787,3 +715,1 @@\n-    heap->young_process_roots(&root_cl,\n-                              &old_gen_cl,\n-                              &cld_scan_closure);\n+    _old_gen->scan_old_to_young_refs();\n@@ -939,0 +865,3 @@\n+\n+    ContinuationGCSupport::transform_stack_chunk(obj);\n+\n","filename":"src\/hotspot\/share\/gc\/serial\/defNewGeneration.cpp","additions":27,"deletions":98,"binary":false,"changes":125,"status":"modified"},{"patch":"@@ -33,1 +33,0 @@\n-#include \"code\/icBuffer.hpp\"\n@@ -474,4 +473,3 @@\n-  \/\/ Capture used regions for each generation that will be\n-  \/\/ subject to collection, so that card table adjustments can\n-  \/\/ be made intelligently (see clear \/ invalidate further below).\n-  gch->save_used_regions();\n+  \/\/ Capture used regions for old-gen to reestablish old-to-young invariant\n+  \/\/ after full-gc.\n+  gch->old_gen()->save_used_region();\n","filename":"src\/hotspot\/share\/gc\/serial\/genMarkSweep.cpp","additions":3,"deletions":5,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -31,1 +31,0 @@\n-#include \"code\/icBuffer.hpp\"\n@@ -98,1 +97,0 @@\n-    _soft_ref_policy(),\n@@ -151,11 +149,0 @@\n-void SerialHeap::young_process_roots(OopClosure* root_closure,\n-                                     OopIterateClosure* old_gen_closure,\n-                                     CLDClosure* cld_closure) {\n-  MarkingCodeBlobClosure mark_code_closure(root_closure, CodeBlobToOopClosure::FixRelocations, false \/* keepalive nmethods *\/);\n-\n-  process_roots(SO_ScavengeCodeCache, root_closure,\n-                cld_closure, cld_closure, &mark_code_closure);\n-\n-  old_gen()->younger_refs_iterate(old_gen_closure);\n-}\n-\n@@ -300,5 +287,0 @@\n-void SerialHeap::save_used_regions() {\n-  _old_gen->save_used_region();\n-  _young_gen->save_used_region();\n-}\n-\n@@ -987,1 +969,2 @@\n-  return _young_gen->is_maximal_no_gc() && _old_gen->is_maximal_no_gc();\n+  \/\/ We don't expand young-gen except at a GC.\n+  return _old_gen->is_maximal_no_gc();\n@@ -1062,2 +1045,0 @@\n-  assert(InlineCacheBuffer::is_empty(), \"should have cleaned up ICBuffer\");\n-\n","filename":"src\/hotspot\/share\/gc\/serial\/serialHeap.cpp","additions":2,"deletions":21,"binary":false,"changes":23,"status":"modified"},{"patch":"@@ -247,0 +247,1 @@\n+  _soft_ref_policy(),\n","filename":"src\/hotspot\/share\/gc\/shared\/collectedHeap.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -30,0 +30,1 @@\n+#include \"gc\/shared\/softRefPolicy.hpp\"\n@@ -59,1 +60,0 @@\n-class SoftRefPolicy;\n@@ -108,0 +108,2 @@\n+  SoftRefPolicy _soft_ref_policy;\n+\n@@ -406,1 +408,1 @@\n-  virtual SoftRefPolicy* soft_ref_policy() = 0;\n+  SoftRefPolicy* soft_ref_policy() { return &_soft_ref_policy; }\n","filename":"src\/hotspot\/share\/gc\/shared\/collectedHeap.hpp","additions":4,"deletions":2,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -523,1 +523,0 @@\n-  _soft_ref_policy(),\n@@ -839,1 +838,3 @@\n-  control_thread()->notify_heap_changed();\n+\n+  \/\/ This is called from allocation path, and thus should be fast.\n+  _heap_changed.try_set();\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.cpp","additions":3,"deletions":2,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -64,1 +64,1 @@\n-#include \"runtime\/reflectionUtils.hpp\"\n+#include \"runtime\/reflection.hpp\"\n@@ -182,9 +182,0 @@\n-static JavaThread* get_current_thread(bool allow_null=true) {\n-  Thread* thread = Thread::current_or_null_safe();\n-  if (thread == nullptr) {\n-    assert(allow_null, \"npe\");\n-    return nullptr;\n-  }\n-  return JavaThread::cast(thread);\n-}\n-\n@@ -195,1 +186,1 @@\n-  JavaThread* thread = get_current_thread();             \\\n+  JavaThread* thread = JavaThread::current_or_null();    \\\n@@ -206,1 +197,1 @@\n-  JavaThread* thread = get_current_thread();             \\\n+  JavaThread* thread = JavaThread::current_or_null();    \\\n@@ -222,1 +213,1 @@\n-  JavaThread* thread = get_current_thread();\n+  JavaThread* thread = JavaThread::current_or_null();\n@@ -1396,1 +1387,1 @@\n-GrowableArray<Method*>* init_resolved_methods(jobjectArray methods, JVMCIEnv* JVMCIENV) {\n+static GrowableArray<Method*>* init_resolved_methods(jobjectArray methods, JVMCIEnv* JVMCIENV) {\n@@ -1415,1 +1406,1 @@\n-bool matches(jobjectArray methods, Method* method, GrowableArray<Method*>** resolved_methods_ref, Handle* matched_jvmci_method_ref, Thread* THREAD, JVMCIEnv* JVMCIENV) {\n+static bool matches(jobjectArray methods, Method* method, GrowableArray<Method*>** resolved_methods_ref, Handle* matched_jvmci_method_ref, Thread* THREAD, JVMCIEnv* JVMCIENV) {\n@@ -1436,1 +1427,1 @@\n-methodHandle resolve_interface_call(Klass* spec_klass, Symbol* name, Symbol* signature, JavaCallArguments* args, TRAPS) {\n+static methodHandle resolve_interface_call(Klass* spec_klass, Symbol* name, Symbol* signature, JavaCallArguments* args, TRAPS) {\n@@ -1451,1 +1442,1 @@\n-void resync_vframestream_to_compiled_frame(vframeStream& vfst, intptr_t* stack_pointer, int vframe_id, JavaThread* thread, TRAPS) {\n+static void resync_vframestream_to_compiled_frame(vframeStream& vfst, intptr_t* stack_pointer, int vframe_id, JavaThread* thread, TRAPS) {\n@@ -1474,1 +1465,1 @@\n-GrowableArray<ScopeValue*>* get_unallocated_objects_or_null(GrowableArray<ScopeValue*>* scope_objects) {\n+static GrowableArray<ScopeValue*>* get_unallocated_objects_or_null(GrowableArray<ScopeValue*>* scope_objects) {\n@@ -2623,1 +2614,1 @@\n-  JavaThread* thread = get_current_thread(false);\n+  JavaThread* thread = JavaThread::thread_from_jni_environment(hotspotEnv);\n@@ -3081,1 +3072,1 @@\n-  EventCompilerPhase event;\n+  EventCompilerPhase event(UNTIMED);\n","filename":"src\/hotspot\/share\/jvmci\/jvmciCompilerToVM.cpp","additions":11,"deletions":20,"binary":false,"changes":31,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -301,1 +301,1 @@\n-void initialize_basic_type_klass(Klass* k, TRAPS) {\n+static void initialize_basic_type_klass(Klass* k, TRAPS) {\n@@ -932,5 +932,5 @@\n-void initialize_known_method(LatestMethodCache* method_cache,\n-                             InstanceKlass* ik,\n-                             const char* method,\n-                             Symbol* signature,\n-                             bool is_static, TRAPS)\n+static void initialize_known_method(LatestMethodCache* method_cache,\n+                                    InstanceKlass* ik,\n+                                    const char* method,\n+                                    Symbol* signature,\n+                                    bool is_static, TRAPS)\n","filename":"src\/hotspot\/share\/memory\/universe.cpp","additions":7,"deletions":7,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2022, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -100,0 +100,20 @@\n+class C2FastUnlockLightweightStub : public C2CodeStub {\n+private:\n+  Register _obj;\n+  Register _mark;\n+  Register _t;\n+  Register _thread;\n+  Label _push_and_slow_path;\n+  Label _check_successor;\n+  Label _unlocked_continuation;\n+public:\n+  C2FastUnlockLightweightStub(Register obj, Register mark, Register t, Register thread) : C2CodeStub(),\n+    _obj(obj), _mark(mark), _t(t), _thread(thread) {}\n+  int max_size() const;\n+  void emit(C2_MacroAssembler& masm);\n+  Label& push_and_slow_path() { return _push_and_slow_path; }\n+  Label& check_successor() { return _check_successor; }\n+  Label& unlocked_continuation() { return _unlocked_continuation; }\n+  Label& slow_path_continuation() { return continuation(); }\n+};\n+\n","filename":"src\/hotspot\/share\/opto\/c2_CodeStubs.hpp","additions":21,"deletions":1,"binary":false,"changes":22,"status":"modified"},{"patch":"@@ -1466,1 +1466,1 @@\n-SafePointScalarObjectNode::SafePointScalarObjectNode(const TypeOopPtr* tp, Node* alloc, uint first_index, uint n_fields) :\n+SafePointScalarObjectNode::SafePointScalarObjectNode(const TypeOopPtr* tp, Node* alloc, uint first_index, uint depth, uint n_fields) :\n@@ -1469,0 +1469,1 @@\n+  _depth(depth),\n","filename":"src\/hotspot\/share\/opto\/callnode.cpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -847,1 +847,2 @@\n-  if (StressLCM || StressGCM || StressIGVN || StressCCP || StressIncrementalInlining) {\n+  if (StressLCM || StressGCM || StressIGVN || StressCCP ||\n+      StressIncrementalInlining || StressMacroExpansion) {\n@@ -2458,0 +2459,1 @@\n+    print_method(PHASE_BEFORE_MACRO_EXPANSION, 3);\n@@ -2463,1 +2465,1 @@\n-    print_method(PHASE_MACRO_EXPANSION, 2);\n+    print_method(PHASE_AFTER_MACRO_EXPANSION, 2);\n@@ -5128,0 +5130,10 @@\n+void Compile::shuffle_macro_nodes() {\n+  if (_macro_nodes.length() < 2) {\n+    return;\n+  }\n+  for (uint i = _macro_nodes.length() - 1; i >= 1; i--) {\n+    uint j = C->random() % (i + 1);\n+    swap(_macro_nodes.at(i), _macro_nodes.at(j));\n+  }\n+}\n+\n@@ -5147,1 +5159,1 @@\n-  EventCompilerPhase event;\n+  EventCompilerPhase event(UNTIMED);\n@@ -5186,1 +5198,1 @@\n-  EventCompilerPhase event;\n+  EventCompilerPhase event(UNTIMED);\n","filename":"src\/hotspot\/share\/opto\/compile.cpp","additions":16,"deletions":4,"binary":false,"changes":20,"status":"modified"},{"patch":"@@ -314,1 +314,0 @@\n-  case vmIntrinsics::_equalsU:                  return inline_string_equals(StrIntrinsicNode::UU);\n","filename":"src\/hotspot\/share\/opto\/library_call.cpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2005, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2005, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -170,1 +170,2 @@\n-  Node *alloc_mem = alloc->in(TypeFunc::Memory);\n+  Node *alloc_mem = alloc->as_Allocate()->proj_out_or_null(TypeFunc::Memory, \/*io_use:*\/false);\n+  assert(alloc_mem != nullptr, \"Allocation without a memory projection.\");\n@@ -374,1 +375,2 @@\n-  Node *alloc_mem = alloc->in(TypeFunc::Memory);\n+  Node *alloc_mem = alloc->proj_out_or_null(TypeFunc::Memory, \/*io_use:*\/false);\n+  assert(alloc_mem != nullptr, \"Allocation without a memory projection.\");\n@@ -459,1 +461,2 @@\n-  Node *alloc_mem = alloc->in(TypeFunc::Memory);\n+  Node *alloc_mem = alloc->proj_out_or_null(TypeFunc::Memory, \/*io_use:*\/false);\n+  assert(alloc_mem != nullptr, \"Allocation without a memory projection.\");\n@@ -769,1 +772,1 @@\n-  SafePointScalarObjectNode* sobj = new SafePointScalarObjectNode(res_type, alloc, first_ind, nfields);\n+  SafePointScalarObjectNode* sobj = new SafePointScalarObjectNode(res_type, alloc, first_ind, sfpt->jvms()->depth(), nfields);\n@@ -2436,0 +2439,3 @@\n+  if (StressMacroExpansion) {\n+    C->shuffle_macro_nodes();\n+  }\n@@ -2517,0 +2523,3 @@\n+      if (success) {\n+        C->print_method(PHASE_AFTER_MACRO_EXPANSION_STEP, 5, n);\n+      }\n@@ -2577,0 +2586,1 @@\n+    C->print_method(PHASE_AFTER_MACRO_EXPANSION_STEP, 5, n);\n@@ -2619,0 +2629,1 @@\n+    C->print_method(PHASE_AFTER_MACRO_EXPANSION_STEP, 5, n);\n","filename":"src\/hotspot\/share\/opto\/macro.cpp","additions":16,"deletions":5,"binary":false,"changes":21,"status":"modified"},{"patch":"@@ -31,1 +31,0 @@\n-#include \"code\/icBuffer.hpp\"\n","filename":"src\/hotspot\/share\/opto\/runtime.cpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -507,1 +507,0 @@\n-  { \"TLABStats\",                    JDK_Version::jdk(12), JDK_Version::undefined(), JDK_Version::undefined() },\n@@ -529,0 +528,1 @@\n+  { \"TLABStats\",                    JDK_Version::jdk(12), JDK_Version::jdk(23), JDK_Version::jdk(24) },\n@@ -1122,12 +1122,1 @@\n-    if (strlen(locked_message_buf) == 0) {\n-      if (found_flag->is_bool() && !has_plus_minus) {\n-        jio_fprintf(defaultStream::error_stream(),\n-          \"Missing +\/- setting for VM option '%s'\\n\", argname);\n-      } else if (!found_flag->is_bool() && has_plus_minus) {\n-        jio_fprintf(defaultStream::error_stream(),\n-          \"Unexpected +\/- setting in VM option '%s'\\n\", argname);\n-      } else {\n-        jio_fprintf(defaultStream::error_stream(),\n-          \"Improperly specified VM option '%s'\\n\", argname);\n-      }\n-    } else {\n+    if (strlen(locked_message_buf) != 0) {\n@@ -1143,0 +1132,10 @@\n+    if (found_flag->is_bool() && !has_plus_minus) {\n+      jio_fprintf(defaultStream::error_stream(),\n+        \"Missing +\/- setting for VM option '%s'\\n\", argname);\n+    } else if (!found_flag->is_bool() && has_plus_minus) {\n+      jio_fprintf(defaultStream::error_stream(),\n+        \"Unexpected +\/- setting in VM option '%s'\\n\", argname);\n+    } else {\n+      jio_fprintf(defaultStream::error_stream(),\n+        \"Improperly specified VM option '%s'\\n\", argname);\n+    }\n@@ -1368,1 +1367,1 @@\n-void set_object_alignment() {\n+static void set_object_alignment() {\n@@ -2020,0 +2019,1 @@\n+#if !INCLUDE_JVMTI\n@@ -2024,1 +2024,1 @@\n-bool valid_jdwp_agent(char *name, bool is_path) {\n+static bool valid_jdwp_agent(char *name, bool is_path) {\n@@ -2064,0 +2064,1 @@\n+#endif\n","filename":"src\/hotspot\/share\/runtime\/arguments.cpp","additions":16,"deletions":15,"binary":false,"changes":31,"status":"modified"},{"patch":"@@ -887,3 +887,0 @@\n-  develop(bool, TraceInlineCacheClearing, false,                            \\\n-          \"Trace clearing of inline caches in nmethods\")                    \\\n-                                                                            \\\n@@ -908,6 +905,0 @@\n-  develop(bool, TraceICBuffer, false,                                       \\\n-          \"Trace usage of IC buffer\")                                       \\\n-                                                                            \\\n-  develop(bool, TraceCompiledIC, false,                                     \\\n-          \"Trace changes of compiled IC\")                                   \\\n-                                                                            \\\n","filename":"src\/hotspot\/share\/runtime\/globals.hpp","additions":0,"deletions":9,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1998, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1998, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -359,0 +359,1 @@\n+  bool      enter_for(JavaThread* locking_thread);\n","filename":"src\/hotspot\/share\/runtime\/objectMonitor.hpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1998, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1998, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -267,1 +267,1 @@\n-int dtrace_waited_probe(ObjectMonitor* monitor, Handle obj, JavaThread* thr) {\n+static int dtrace_waited_probe(ObjectMonitor* monitor, Handle obj, JavaThread* thr) {\n@@ -421,0 +421,13 @@\n+  if (LockingMode == LM_LIGHTWEIGHT) {\n+    LockStack& lock_stack = current->lock_stack();\n+    if (lock_stack.is_full()) {\n+      \/\/ Always go into runtime if the lock stack is full.\n+      return false;\n+    }\n+    if (lock_stack.try_recursive_enter(obj)) {\n+      \/\/ Recursive lock successful.\n+      current->inc_held_monitor_count();\n+      return true;\n+    }\n+  }\n+\n@@ -474,2 +487,3 @@\n-void ObjectSynchronizer::handle_sync_on_value_based_class(Handle obj, JavaThread* current) {\n-  frame last_frame = current->last_frame();\n+void ObjectSynchronizer::handle_sync_on_value_based_class(Handle obj, JavaThread* locking_thread) {\n+  assert(locking_thread == Thread::current() || locking_thread->is_obj_deopt_suspend(), \"must be\");\n+  frame last_frame = locking_thread->last_frame();\n@@ -488,1 +502,1 @@\n-    ResourceMark rm(current);\n+    ResourceMark rm;\n@@ -490,1 +504,1 @@\n-    current->print_active_stack_on(&ss);\n+    locking_thread->print_active_stack_on(&ss);\n@@ -499,1 +513,1 @@\n-    ResourceMark rm(current);\n+    ResourceMark rm;\n@@ -503,1 +517,1 @@\n-    if (current->has_last_Java_frame()) {\n+    if (locking_thread->has_last_Java_frame()) {\n@@ -505,1 +519,1 @@\n-      current->print_active_stack_on(&info_stream);\n+      locking_thread->print_active_stack_on(&info_stream);\n@@ -532,0 +546,49 @@\n+\n+void ObjectSynchronizer::enter_for(Handle obj, BasicLock* lock, JavaThread* locking_thread) {\n+  \/\/ When called with locking_thread != Thread::current() some mechanism must synchronize\n+  \/\/ the locking_thread with respect to the current thread. Currently only used when\n+  \/\/ deoptimizing and re-locking locks. See Deoptimization::relock_objects\n+  assert(locking_thread == Thread::current() || locking_thread->is_obj_deopt_suspend(), \"must be\");\n+\n+  if (LockingMode == LM_PLACEHOLDER) {\n+    return PlaceholderSynchronizer::enter_for(obj, lock, locking_thread);\n+  }\n+\n+  if (!enter_fast_impl(obj, lock, locking_thread)) {\n+    \/\/ Inflated ObjectMonitor::enter_for is required\n+\n+    \/\/ An async deflation can race after the inflate_for() call and before\n+    \/\/ enter_for() can make the ObjectMonitor busy. enter_for() returns false\n+    \/\/ if we have lost the race to async deflation and we simply try again.\n+    while (true) {\n+      ObjectMonitor* monitor = inflate_for(locking_thread, obj(), inflate_cause_monitor_enter);\n+      if (monitor->enter_for(locking_thread)) {\n+        return;\n+      }\n+      assert(monitor->is_being_async_deflated(), \"must be\");\n+    }\n+  }\n+}\n+\n+void ObjectSynchronizer::enter(Handle obj, BasicLock* lock, JavaThread* current) {\n+  assert(current == Thread::current(), \"must be\");\n+\n+  if (LockingMode == LM_PLACEHOLDER) {\n+    return PlaceholderSynchronizer::enter(obj, lock, current, current);\n+  }\n+\n+  if (!enter_fast_impl(obj, lock, current)) {\n+    \/\/ Inflated ObjectMonitor::enter is required\n+\n+    \/\/ An async deflation can race after the inflate() call and before\n+    \/\/ enter() can make the ObjectMonitor busy. enter() returns false if\n+    \/\/ we have lost the race to async deflation and we simply try again.\n+    while (true) {\n+      ObjectMonitor* monitor = inflate(current, obj(), inflate_cause_monitor_enter);\n+      if (monitor->enter(current)) {\n+        return;\n+      }\n+    }\n+  }\n+}\n+\n@@ -535,0 +598,1 @@\n+bool ObjectSynchronizer::enter_fast_impl(Handle obj, BasicLock* lock, JavaThread* locking_thread) {\n@@ -536,2 +600,0 @@\n-void ObjectSynchronizer::enter(Handle obj, BasicLock* lock, JavaThread* locking_thread,  JavaThread* current) {\n-    \/\/ ResourceMarks are wrong inside when locking_thread != current (relock)\n@@ -544,4 +606,0 @@\n-  if (LockingMode == LM_PLACEHOLDER) {\n-    return PlaceholderSynchronizer::enter(obj, lock, locking_thread, current);\n-  }\n-\n@@ -552,14 +610,40 @@\n-      if (lock_stack.can_push()) {\n-        markWord mark = obj()->mark_acquire();\n-        while (mark.is_neutral()) {\n-          \/\/ Retry until a lock state change has been observed.  cas_set_mark() may collide with non lock bits modifications.\n-          \/\/ Try to swing into 'fast-locked' state.\n-          assert(!lock_stack.contains(obj()), \"thread must not already hold the lock\");\n-          const markWord locked_mark = mark.set_fast_locked();\n-          const markWord old_mark = obj()->cas_set_mark(locked_mark, mark);\n-          if (old_mark == mark) {\n-            \/\/ Successfully fast-locked, push object to lock-stack and return.\n-            lock_stack.push(obj());\n-            return;\n-          }\n-          mark = old_mark;\n+      if (lock_stack.is_full()) {\n+        \/\/ We unconditionally make room on the lock stack by inflating\n+        \/\/ the least recently locked object on the lock stack.\n+\n+        \/\/ About the choice to inflate least recently locked object.\n+        \/\/ First we must chose to inflate a lock, either some lock on\n+        \/\/ the lock-stack or the lock that is currently being entered\n+        \/\/ (which may or may not be on the lock-stack).\n+        \/\/ Second the best lock to inflate is a lock which is entered\n+        \/\/ in a control flow where there are only a very few locks being\n+        \/\/ used, as the costly part of inflated locking is inflation,\n+        \/\/ not locking. But this property is entirely program dependent.\n+        \/\/ Third inflating the lock currently being entered on when it\n+        \/\/ is not present on the lock-stack will result in a still full\n+        \/\/ lock-stack. This creates a scenario where every deeper nested\n+        \/\/ monitorenter must call into the runtime.\n+        \/\/ The rational here is as follows:\n+        \/\/ Because we cannot (currently) figure out the second, and want\n+        \/\/ to avoid the third, we inflate a lock on the lock-stack.\n+        \/\/ The least recently locked lock is chosen as it is the lock\n+        \/\/ with the longest critical section.\n+\n+        log_info(monitorinflation)(\"LockStack capacity exceeded, inflating.\");\n+        ObjectMonitor* monitor = inflate_for(locking_thread, lock_stack.bottom(), inflate_cause_vm_internal);\n+        assert(monitor->owner() == Thread::current(), \"must be owner=\" PTR_FORMAT \" current=\" PTR_FORMAT \" mark=\" PTR_FORMAT,\n+               p2i(monitor->owner()), p2i(Thread::current()), monitor->object()->mark_acquire().value());\n+        assert(!lock_stack.is_full(), \"must have made room here\");\n+      }\n+\n+      markWord mark = obj()->mark_acquire();\n+      while (mark.is_neutral()) {\n+        \/\/ Retry until a lock state change has been observed. cas_set_mark() may collide with non lock bits modifications.\n+        \/\/ Try to swing into 'fast-locked' state.\n+        assert(!lock_stack.contains(obj()), \"thread must not already hold the lock\");\n+        const markWord locked_mark = mark.set_fast_locked();\n+        const markWord old_mark = obj()->cas_set_mark(locked_mark, mark);\n+        if (old_mark == mark) {\n+          \/\/ Successfully fast-locked, push object to lock-stack and return.\n+          lock_stack.push(obj());\n+          return true;\n@@ -567,0 +651,1 @@\n+        mark = old_mark;\n@@ -568,1 +653,8 @@\n-      \/\/ All other paths fall-through to inflate-enter.\n+\n+      if (mark.is_fast_locked() && lock_stack.try_recursive_enter(obj())) {\n+        \/\/ Recursive lock successful.\n+        return true;\n+      }\n+\n+      \/\/ Failed to fast lock.\n+      return false;\n@@ -576,1 +668,1 @@\n-          return;\n+          return true;\n@@ -578,1 +670,0 @@\n-        \/\/ Fall through to inflate() ...\n@@ -584,1 +675,1 @@\n-        return;\n+        return true;\n@@ -592,0 +683,3 @@\n+\n+      \/\/ Failed to fast lock.\n+      return false;\n@@ -597,9 +691,1 @@\n-  \/\/ An async deflation can race after the inflate() call and before\n-  \/\/ enter() can make the ObjectMonitor busy. enter() returns false if\n-  \/\/ we have lost the race to async deflation and we simply try again.\n-  while (true) {\n-    ObjectMonitor* monitor = inflate(locking_thread, obj(), inflate_cause_monitor_enter);\n-    if (monitor->enter(locking_thread)) {\n-      return;\n-    }\n-  }\n+  return false;\n@@ -619,7 +705,21 @@\n-      while (mark.is_fast_locked()) {\n-        \/\/ Retry until a lock state change has been observed.  cas_set_mark() may collide with non lock bits modifications.\n-        const markWord unlocked_mark = mark.set_unlocked();\n-        const markWord old_mark = object->cas_set_mark(unlocked_mark, mark);\n-        if (old_mark == mark) {\n-          current->lock_stack().remove(object);\n-          return;\n+      LockStack& lock_stack = current->lock_stack();\n+      if (mark.is_fast_locked() && lock_stack.try_recursive_exit(object)) {\n+        \/\/ Recursively unlocked.\n+        return;\n+      }\n+\n+      if (mark.is_fast_locked() && lock_stack.is_recursive(object)) {\n+        \/\/ This lock is recursive but is not at the top of the lock stack so we're\n+        \/\/ doing an unbalanced exit. We have to fall thru to inflation below and\n+        \/\/ let ObjectMonitor::exit() do the unlock.\n+      } else {\n+        while (mark.is_fast_locked()) {\n+          \/\/ Retry until a lock state change has been observed. cas_set_mark() may collide with non lock bits modifications.\n+          const markWord unlocked_mark = mark.set_unlocked();\n+          const markWord old_mark = object->cas_set_mark(unlocked_mark, mark);\n+          if (old_mark == mark) {\n+            size_t recursions = lock_stack.remove(object) - 1;\n+            assert(recursions == 0, \"must not be recursive here\");\n+            return;\n+          }\n+          mark = old_mark;\n@@ -627,1 +727,0 @@\n-        mark = old_mark;\n@@ -1400,5 +1499,6 @@\n-\/\/ Can be called from non JavaThreads (e.g., VMThread) for FastHashCode\n-\/\/ calculations as part of JVM\/TI tagging.\n-static bool is_lock_owned(Thread* thread, oop obj) {\n-  assert(LockingMode == LM_LIGHTWEIGHT, \"only call this with new lightweight locking enabled\");\n-  return thread->is_Java_thread() ? JavaThread::cast(thread)->lock_stack().contains(obj) : false;\n+ObjectMonitor* ObjectSynchronizer::inflate(Thread* current, oop obj, const InflateCause cause) {\n+  assert(current == Thread::current(), \"must be\");\n+  if (LockingMode == LM_LIGHTWEIGHT && current->is_Java_thread()) {\n+    return inflate_impl(JavaThread::cast(current), obj, cause);\n+  }\n+  return inflate_impl(nullptr, obj, cause);\n@@ -1407,3 +1507,4 @@\n-ObjectMonitor* ObjectSynchronizer::inflate(Thread* current, oop object,\n-                                           const InflateCause cause) {\n-  assert(LockingMode != LM_PLACEHOLDER, \"placeholder does not use inflate\");\n+ObjectMonitor* ObjectSynchronizer::inflate_for(JavaThread* thread, oop obj, const InflateCause cause) {\n+  assert(thread == Thread::current() || thread->is_obj_deopt_suspend(), \"must be\");\n+  return inflate_impl(thread, obj, cause);\n+}\n@@ -1411,0 +1512,10 @@\n+ObjectMonitor* ObjectSynchronizer::inflate_impl(JavaThread* inflating_thread, oop object, const InflateCause cause) {\n+  assert(LockingMode != LM_PLACEHOLDER, \"placeholder does not use inflate\");\n+  \/\/ The JavaThread* inflating_thread parameter is only used by LM_LIGHTWEIGHT and requires\n+  \/\/ that the inflating_thread == Thread::current() or is suspended throughout the call by\n+  \/\/ some other mechanism.\n+  \/\/ Even with LM_LIGHTWEIGHT the thread might be nullptr when called from a non\n+  \/\/ JavaThread. (As may still be the case from FastHashCode). However it is only\n+  \/\/ important for the correctness of the LM_LIGHTWEIGHT algorithm that the thread\n+  \/\/ is set when called from ObjectSynchronizer::enter from the owning thread,\n+  \/\/ ObjectSynchronizer::enter_for from any thread, or ObjectSynchronizer::exit.\n@@ -1419,4 +1530,4 @@\n-    \/\/                   is anonymous and the current thread owns the\n-    \/\/                   object lock, then we make the current thread the\n-    \/\/                   ObjectMonitor owner and remove the lock from the\n-    \/\/                   current thread's lock stack.\n+    \/\/                   is anonymous and the inflating_thread owns the\n+    \/\/                   object lock, then we make the inflating_thread\n+    \/\/                   the ObjectMonitor owner and remove the lock from\n+    \/\/                   the inflating_thread's lock stack.\n@@ -1434,3 +1545,5 @@\n-      if (LockingMode == LM_LIGHTWEIGHT && inf->is_owner_anonymous() && is_lock_owned(current, object)) {\n-        inf->set_owner_from_anonymous(current);\n-        JavaThread::cast(current)->lock_stack().remove(object);\n+      if (LockingMode == LM_LIGHTWEIGHT && inf->is_owner_anonymous() &&\n+          inflating_thread != nullptr && inflating_thread->lock_stack().contains(object)) {\n+        inf->set_owner_from_anonymous(inflating_thread);\n+        size_t removed = inflating_thread->lock_stack().remove(object);\n+        inf->set_recursions(removed - 1);\n@@ -1456,1 +1569,1 @@\n-    \/\/ Could be fast-locked either by current or by some other thread.\n+    \/\/ Could be fast-locked either by the inflating_thread or by some other thread.\n@@ -1460,2 +1573,2 @@\n-    \/\/ this thread owns the monitor, then we set the ObjectMonitor's\n-    \/\/ owner to this thread. Otherwise, we set the ObjectMonitor's owner\n+    \/\/ the inflating_thread owns the monitor, then we set the ObjectMonitor's\n+    \/\/ owner to the inflating_thread. Otherwise, we set the ObjectMonitor's owner\n@@ -1469,1 +1582,1 @@\n-      bool own = is_lock_owned(current, object);\n+      bool own = inflating_thread != nullptr && inflating_thread->lock_stack().contains(object);\n@@ -1471,2 +1584,2 @@\n-        \/\/ Owned by us.\n-        monitor->set_owner_from(nullptr, current);\n+        \/\/ Owned by inflating_thread.\n+        monitor->set_owner_from(nullptr, inflating_thread);\n@@ -1482,1 +1595,2 @@\n-          JavaThread::cast(current)->lock_stack().remove(object);\n+          size_t removed = inflating_thread->lock_stack().remove(object);\n+          monitor->set_recursions(removed - 1);\n@@ -1492,1 +1606,1 @@\n-          ResourceMark rm(current);\n+          ResourceMark rm;\n@@ -1591,1 +1705,1 @@\n-        ResourceMark rm(current);\n+        ResourceMark rm;\n@@ -1635,1 +1749,1 @@\n-      ResourceMark rm(current);\n+      ResourceMark rm;\n@@ -2059,3 +2173,1 @@\n-    \/\/ This should not happen, but if it does, it is not fatal.\n-    out->print_cr(\"WARNING: monitor=\" INTPTR_FORMAT \": in-use monitor is \"\n-                  \"deflated.\", p2i(n));\n+    \/\/ This could happen when monitor deflation blocks for a safepoint.\n@@ -2065,0 +2177,1 @@\n+\n","filename":"src\/hotspot\/share\/runtime\/synchronizer.cpp","additions":191,"deletions":78,"binary":false,"changes":269,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -762,0 +762,7 @@\n+  if (Continuations::enabled()) {\n+    \/\/ Initialize Continuation class now so that failure to create enterSpecial\/doYield\n+    \/\/ special nmethods due to limited CodeCache size can be treated as a fatal error at\n+    \/\/ startup with the proper message that CodeCache size is too small.\n+    initialize_class(vmSymbols::jdk_internal_vm_Continuation(), CHECK_JNI_ERR);\n+  }\n+\n@@ -1124,1 +1131,1 @@\n-void assert_thread_claimed(const char* kind, Thread* t, uintx expected) {\n+static void assert_thread_claimed(const char* kind, Thread* t, uintx expected) {\n","filename":"src\/hotspot\/share\/runtime\/threads.cpp","additions":9,"deletions":2,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -43,0 +43,1 @@\n+#include \"code\/compiledIC.hpp\"\n@@ -66,1 +67,0 @@\n-#include \"oops\/compiledICHolder.hpp\"\n@@ -224,2 +224,0 @@\n-  nonstatic_field(CompiledICHolder,            _holder_metadata,                              Metadata*)                             \\\n-  nonstatic_field(CompiledICHolder,            _holder_klass,                                 Klass*)                                \\\n@@ -1177,1 +1175,0 @@\n-  declare_toplevel_type(CompiledICHolder)                                 \\\n","filename":"src\/hotspot\/share\/runtime\/vmStructs.cpp","additions":1,"deletions":4,"binary":false,"changes":5,"status":"modified"}]}