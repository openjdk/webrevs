{"files":[{"patch":"@@ -1240,1 +1240,1 @@\n-    if (UseCompressedOops && (CompressedOops::ptrs_base() != NULL)) {\n+    if (UseCompressedOops && (CompressedOops::ptrs_base() != nullptr)) {\n@@ -1584,1 +1584,1 @@\n-  return st->trailing_membar() != NULL;\n+  return st->trailing_membar() != nullptr;\n@@ -1596,1 +1596,1 @@\n-    assert(ldst->trailing_membar() != NULL, \"expected trailing membar\");\n+    assert(ldst->trailing_membar() != nullptr, \"expected trailing membar\");\n@@ -1598,1 +1598,1 @@\n-    return ldst->trailing_membar() != NULL;\n+    return ldst->trailing_membar() != nullptr;\n@@ -1737,1 +1737,1 @@\n-  if (C->stub_function() == NULL && BarrierSet::barrier_set()->barrier_set_nmethod() != NULL) {\n+  if (C->stub_function() == nullptr && BarrierSet::barrier_set()->barrier_set_nmethod() != nullptr) {\n@@ -1786,1 +1786,1 @@\n-  if (C->stub_function() == NULL) {\n+  if (C->stub_function() == nullptr) {\n@@ -1788,1 +1788,1 @@\n-    if (BarrierSet::barrier_set()->barrier_set_nmethod() != NULL) {\n+    if (BarrierSet::barrier_set()->barrier_set_nmethod() != nullptr) {\n@@ -2156,1 +2156,1 @@\n-    implementation(NULL, ra_, false, st);\n+    implementation(nullptr, ra_, false, st);\n@@ -2161,1 +2161,1 @@\n-  implementation(&cbuf, ra_, false, NULL);\n+  implementation(&cbuf, ra_, false, nullptr);\n@@ -2208,4 +2208,3 @@\n-    st->print_cr(\"\\tldrw rscratch1, j_rarg0 + oopDesc::klass_offset_in_bytes()]\\t# compressed klass\");\n-    if (CompressedKlassPointers::shift() != 0) {\n-      st->print_cr(\"\\tdecode_klass_not_null rscratch1, rscratch1\");\n-    }\n+    st->print_cr(\"\\tldrw rscratch1, [j_rarg0 + oopDesc::klass_offset_in_bytes()]\\t# compressed klass\");\n+    st->print_cr(\"\\tldrw r10, [rscratch2 + CompiledICData::speculated_klass_offset()]\\t# compressed klass\");\n+    st->print_cr(\"\\tcmpw rscratch1, r10\");\n@@ -2213,1 +2212,3 @@\n-   st->print_cr(\"\\tldr rscratch1, j_rarg0 + oopDesc::klass_offset_in_bytes()]\\t# compressed klass\");\n+    st->print_cr(\"\\tldr rscratch1, [j_rarg0 + oopDesc::klass_offset_in_bytes()]\\t# compressed klass\");\n+    st->print_cr(\"\\tldr r10, [rscratch2 + CompiledICData::speculated_klass_offset()]\\t# compressed klass\");\n+    st->print_cr(\"\\tcmp rscratch1, r10\");\n@@ -2215,1 +2216,0 @@\n-  st->print_cr(\"\\tcmp r0, rscratch1\\t # Inline cache check\");\n@@ -2224,8 +2224,1 @@\n-\n-  __ cmp_klass(j_rarg0, rscratch2, rscratch1);\n-  Label skip;\n-  \/\/ TODO\n-  \/\/ can we avoid this skip and still use a reloc?\n-  __ br(Assembler::EQ, skip);\n-  __ far_jump(RuntimeAddress(SharedRuntime::get_ic_miss_stub()));\n-  __ bind(skip);\n+  __ ic_check(InteriorEntryAlignment);\n@@ -2252,1 +2245,1 @@\n-  if (base == NULL) {\n+  if (base == nullptr) {\n@@ -2270,1 +2263,1 @@\n-  if (base == NULL) {\n+  if (base == nullptr) {\n@@ -2413,1 +2406,1 @@\n-  return NULL;\n+  return nullptr;\n@@ -2585,2 +2578,2 @@\n-bool is_valid_sve_arith_imm_pattern(Node* n, Node* m) {\n-  if (n == NULL || m == NULL) {\n+static bool is_valid_sve_arith_imm_pattern(Node* n, Node* m) {\n+  if (n == nullptr || m == nullptr) {\n@@ -2626,2 +2619,2 @@\n-bool is_vector_bitwise_not_pattern(Node* n, Node* m) {\n-  if (n != NULL && m != NULL) {\n+static bool is_vector_bitwise_not_pattern(Node* n, Node* m) {\n+  if (n != nullptr && m != nullptr) {\n@@ -3433,1 +3426,1 @@\n-    if (con == NULL || con == (address)1) {\n+    if (con == nullptr || con == (address)1) {\n@@ -3476,1 +3469,1 @@\n-    if (con == NULL) {\n+    if (con == nullptr) {\n@@ -3495,1 +3488,1 @@\n-    if (con == NULL) {\n+    if (con == nullptr) {\n@@ -3678,1 +3671,1 @@\n-                                     NULL, &miss,\n+                                     nullptr, &miss,\n@@ -3694,1 +3687,1 @@\n-      if (call == NULL) {\n+      if (call == nullptr) {\n@@ -3708,1 +3701,1 @@\n-      if (call == NULL) {\n+      if (call == nullptr) {\n@@ -3718,2 +3711,2 @@\n-        address stub = CompiledStaticCall::emit_to_interp_stub(cbuf, call);\n-        if (stub == NULL) {\n+        address stub = CompiledDirectCall::emit_to_interp_stub(cbuf, call);\n+        if (stub == nullptr) {\n@@ -3738,1 +3731,1 @@\n-    if (call == NULL) {\n+    if (call == nullptr) {\n@@ -3767,1 +3760,1 @@\n-      if (call == NULL) {\n+      if (call == nullptr) {\n@@ -4666,1 +4659,1 @@\n-\/\/ NULL Pointer Immediate\n+\/\/ Null Pointer Immediate\n@@ -4798,1 +4791,1 @@\n-\/\/ Narrow NULL Pointer Immediate\n+\/\/ Narrow Null Pointer Immediate\n@@ -7222,1 +7215,1 @@\n-  format %{ \"mov  $dst, $con\\t# NULL ptr\" %}\n+  format %{ \"mov  $dst, $con\\t# null pointer\" %}\n@@ -7236,1 +7229,1 @@\n-  format %{ \"mov  $dst, $con\\t# NULL ptr\" %}\n+  format %{ \"mov  $dst, $con\\t# null pointer\" %}\n@@ -7278,1 +7271,1 @@\n-  format %{ \"mov  $dst, $con\\t# compressed NULL ptr\" %}\n+  format %{ \"mov  $dst, $con\\t# compressed null pointer\" %}\n@@ -8292,1 +8285,1 @@\n-            \"dmb ishld\" %}\n+            \"dmb ish\" %}\n@@ -8346,1 +8339,1 @@\n-            \"dmb ishst\\n\\tdmb ishld\" %}\n+            \"dmb ish\" %}\n@@ -8350,2 +8343,1 @@\n-    __ membar(Assembler::StoreStore);\n-    __ membar(Assembler::LoadStore);\n+    __ membar(Assembler::LoadStore|Assembler::StoreStore);\n@@ -15260,1 +15252,1 @@\n-    if (tpc == NULL) {\n+    if (tpc == nullptr) {\n@@ -15281,1 +15273,1 @@\n-    if (tpc == NULL) {\n+    if (tpc == nullptr) {\n@@ -16444,1 +16436,1 @@\n-  predicate(LockingMode != LM_PLACEHOLDER);\n+  predicate(LockingMode != LM_LIGHTWEIGHT && LockingMode != LM_PLACEHOLDER);\n@@ -16449,1 +16441,1 @@\n-  format %{ \"fastlock $object,$box\\t! kills $tmp,$tmp2,tmp3\" %}\n+  format %{ \"fastlock $object,$box\\t! kills $tmp,$tmp2,$tmp3\" %}\n@@ -16460,1 +16452,1 @@\n-  predicate(LockingMode != LM_PLACEHOLDER);\n+  predicate(LockingMode != LM_LIGHTWEIGHT && LockingMode != LM_PLACEHOLDER);\n@@ -16474,0 +16466,31 @@\n+instruct cmpFastLockLightweight(rFlagsReg cr, iRegP object, iRegP box, iRegPNoSp tmp, iRegPNoSp tmp2)\n+%{\n+  predicate(LockingMode == LM_LIGHTWEIGHT);\n+  match(Set cr (FastLock object box));\n+  effect(TEMP tmp, TEMP tmp2);\n+\n+  ins_cost(5 * INSN_COST);\n+  format %{ \"fastlock $object,$box\\t! kills $tmp,$tmp2\" %}\n+\n+  ins_encode %{\n+    __ fast_lock_lightweight($object$$Register, $box$$Register, $tmp$$Register, $tmp2$$Register);\n+  %}\n+\n+  ins_pipe(pipe_serial);\n+%}\n+\n+instruct cmpFastUnlockLightweight(rFlagsReg cr, iRegP object, iRegP box, iRegPNoSp tmp, iRegPNoSp tmp2)\n+%{\n+  predicate(LockingMode == LM_LIGHTWEIGHT);\n+  match(Set cr (FastUnlock object box));\n+  effect(TEMP tmp, TEMP tmp2);\n+\n+  ins_cost(5 * INSN_COST);\n+  format %{ \"fastunlock $object,$box\\t! kills $tmp, $tmp2\" %}\n+\n+  ins_encode %{\n+    __ fast_unlock_lightweight($object$$Register, $box$$Register, $tmp$$Register, $tmp2$$Register);\n+  %}\n+\n+  ins_pipe(pipe_serial);\n+%}\n@@ -17139,17 +17162,1 @@\n-                     $result$$Register, $cnt$$Register, 1);\n-  %}\n-  ins_pipe(pipe_class_memory);\n-%}\n-\n-instruct string_equalsU(iRegP_R1 str1, iRegP_R3 str2, iRegI_R4 cnt,\n-                        iRegI_R0 result, rFlagsReg cr)\n-%{\n-  predicate(((StrEqualsNode*)n)->encoding() == StrIntrinsicNode::UU);\n-  match(Set result (StrEquals (Binary str1 str2) cnt));\n-  effect(USE_KILL str1, USE_KILL str2, USE_KILL cnt, KILL cr);\n-\n-  format %{ \"String Equals $str1,$str2,$cnt -> $result\" %}\n-  ins_encode %{\n-    \/\/ Count is in 8-bit bytes; non-Compact chars are 16 bits.\n-    __ string_equals($str1$$Register, $str2$$Register,\n-                     $result$$Register, $cnt$$Register, 2);\n+                     $result$$Register, $cnt$$Register);\n@@ -17177,1 +17184,1 @@\n-    if (tpc == NULL) {\n+    if (tpc == nullptr) {\n@@ -17202,1 +17209,1 @@\n-    if (tpc == NULL) {\n+    if (tpc == nullptr) {\n@@ -17217,1 +17224,1 @@\n-    if (tpc == NULL) {\n+    if (tpc == nullptr) {\n@@ -17260,1 +17267,1 @@\n-    if (tpc == NULL) {\n+    if (tpc == nullptr) {\n","filename":"src\/hotspot\/cpu\/aarch64\/aarch64.ad","additions":82,"deletions":75,"binary":false,"changes":157,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1999, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1999, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -88,5 +88,0 @@\n-  if (LockingMode != LM_PLACEHOLDER) {\n-    \/\/ Load object header\n-    ldr(hdr, Address(obj, hdr_offset));\n-  }\n-\n@@ -100,0 +95,2 @@\n+    \/\/ Load object header\n+    ldr(hdr, Address(obj, hdr_offset));\n@@ -160,5 +157,0 @@\n-    ldr(hdr, Address(obj, oopDesc::mark_offset_in_bytes()));\n-    \/\/ We cannot use tbnz here, the target might be too far away and cannot\n-    \/\/ be encoded.\n-    tst(hdr, markWord::monitor_value);\n-    br(Assembler::NE, slow_case);\n@@ -324,11 +316,0 @@\n-\n-void C1_MacroAssembler::inline_cache_check(Register receiver, Register iCache) {\n-  verify_oop(receiver);\n-  \/\/ explicit null check not needed since load from [klass_offset] causes a trap\n-  \/\/ check against inline cache\n-  assert(!MacroAssembler::needs_explicit_null_check(oopDesc::klass_offset_in_bytes()), \"must add explicit null check\");\n-\n-  cmp_klass(receiver, iCache, rscratch1);\n-}\n-\n-\n","filename":"src\/hotspot\/cpu\/aarch64\/c1_MacroAssembler_aarch64.cpp","additions":3,"deletions":22,"binary":false,"changes":25,"status":"modified"},{"patch":"@@ -59,0 +59,1 @@\n+  assert(LockingMode != LM_LIGHTWEIGHT, \"lightweight locking should use fast_lock_lightweight\");\n@@ -78,1 +79,2 @@\n-  } else if (LockingMode == LM_LEGACY) {\n+  } else {\n+    assert(LockingMode == LM_LEGACY, \"must be\");\n@@ -107,4 +109,0 @@\n-  } else {\n-    assert(LockingMode == LM_LIGHTWEIGHT, \"must be\");\n-    lightweight_lock(oop, disp_hdr, tmp, tmp3Reg, no_count);\n-    b(count);\n@@ -124,8 +122,7 @@\n-  if (LockingMode != LM_LIGHTWEIGHT) {\n-    \/\/ Store a non-null value into the box to avoid looking like a re-entrant\n-    \/\/ lock. The fast-path monitor unlock code checks for\n-    \/\/ markWord::monitor_value so use markWord::unused_mark which has the\n-    \/\/ relevant bit set, and also matches ObjectSynchronizer::enter.\n-    mov(tmp, (address)markWord::unused_mark().value());\n-    str(tmp, Address(box, BasicLock::displaced_header_offset_in_bytes()));\n-  }\n+  \/\/ Store a non-null value into the box to avoid looking like a re-entrant\n+  \/\/ lock. The fast-path monitor unlock code checks for\n+  \/\/ markWord::monitor_value so use markWord::unused_mark which has the\n+  \/\/ relevant bit set, and also matches ObjectSynchronizer::enter.\n+  mov(tmp, (address)markWord::unused_mark().value());\n+  str(tmp, Address(box, BasicLock::displaced_header_offset_in_bytes()));\n+\n@@ -162,0 +159,1 @@\n+  assert(LockingMode != LM_LIGHTWEIGHT, \"lightweight locking should use fast_unlock_lightweight\");\n@@ -180,1 +178,2 @@\n-  } else if (LockingMode == LM_LEGACY) {\n+  } else {\n+    assert(LockingMode == LM_LEGACY, \"must be\");\n@@ -188,4 +187,0 @@\n-  } else {\n-    assert(LockingMode == LM_LIGHTWEIGHT, \"must be\");\n-    lightweight_unlock(oop, tmp, box, disp_hdr, no_count);\n-    b(count);\n@@ -201,13 +196,0 @@\n-  if (LockingMode == LM_LIGHTWEIGHT) {\n-    \/\/ If the owner is anonymous, we need to fix it -- in an outline stub.\n-    Register tmp2 = disp_hdr;\n-    ldr(tmp2, Address(tmp, ObjectMonitor::owner_offset()));\n-    \/\/ We cannot use tbnz here, the target might be too far away and cannot\n-    \/\/ be encoded.\n-    tst(tmp2, (uint64_t)ObjectMonitor::ANONYMOUS_OWNER);\n-    C2HandleAnonOMOwnerStub* stub = new (Compile::current()->comp_arena()) C2HandleAnonOMOwnerStub(tmp, tmp2);\n-    Compile::current()->output()->add_stub(stub);\n-    br(Assembler::NE, stub->entry());\n-    bind(stub->continuation());\n-  }\n-\n@@ -246,0 +228,256 @@\n+void C2_MacroAssembler::fast_lock_lightweight(Register obj, Register t1,\n+                                              Register t2, Register t3) {\n+  assert(LockingMode == LM_LIGHTWEIGHT, \"must be\");\n+  assert_different_registers(obj, t1, t2, t3);\n+\n+  \/\/ Handle inflated monitor.\n+  Label inflated;\n+  \/\/ Finish fast lock successfully. MUST branch to with flag == EQ\n+  Label locked;\n+  \/\/ Finish fast lock unsuccessfully. MUST branch to with flag == NE\n+  Label slow_path;\n+\n+  if (DiagnoseSyncOnValueBasedClasses != 0) {\n+    load_klass(t1, obj);\n+    ldrw(t1, Address(t1, Klass::access_flags_offset()));\n+    tstw(t1, JVM_ACC_IS_VALUE_BASED_CLASS);\n+    br(Assembler::NE, slow_path);\n+  }\n+\n+  const Register t1_mark = t1;\n+\n+  { \/\/ Lightweight locking\n+\n+    \/\/ Push lock to the lock stack and finish successfully. MUST branch to with flag == EQ\n+    Label push;\n+\n+    const Register t2_top = t2;\n+    const Register t3_t = t3;\n+\n+    \/\/ Check if lock-stack is full.\n+    ldrw(t2_top, Address(rthread, JavaThread::lock_stack_top_offset()));\n+    cmpw(t2_top, (unsigned)LockStack::end_offset() - 1);\n+    br(Assembler::GT, slow_path);\n+\n+    \/\/ Check if recursive.\n+    subw(t3_t, t2_top, oopSize);\n+    ldr(t3_t, Address(rthread, t3_t));\n+    cmp(obj, t3_t);\n+    br(Assembler::EQ, push);\n+\n+    \/\/ Relaxed normal load to check for monitor. Optimization for monitor case.\n+    ldr(t1_mark, Address(obj, oopDesc::mark_offset_in_bytes()));\n+    tbnz(t1_mark, exact_log2(markWord::monitor_value), inflated);\n+\n+    \/\/ Not inflated\n+    assert(oopDesc::mark_offset_in_bytes() == 0, \"required to avoid a lea\");\n+\n+    \/\/ Try to lock. Transition lock-bits 0b01 => 0b00\n+    orr(t1_mark, t1_mark, markWord::unlocked_value);\n+    eor(t3_t, t1_mark, markWord::unlocked_value);\n+    cmpxchg(\/*addr*\/ obj, \/*expected*\/ t1_mark, \/*new*\/ t3_t, Assembler::xword,\n+            \/*acquire*\/ true, \/*release*\/ false, \/*weak*\/ false, noreg);\n+    br(Assembler::NE, slow_path);\n+\n+    bind(push);\n+    \/\/ After successful lock, push object on lock-stack.\n+    str(obj, Address(rthread, t2_top));\n+    addw(t2_top, t2_top, oopSize);\n+    strw(t2_top, Address(rthread, JavaThread::lock_stack_top_offset()));\n+    b(locked);\n+  }\n+\n+  { \/\/ Handle inflated monitor.\n+    bind(inflated);\n+\n+    \/\/ mark contains the tagged ObjectMonitor*.\n+    const Register t1_tagged_monitor = t1_mark;\n+    const uintptr_t monitor_tag = markWord::monitor_value;\n+    const Register t2_owner_addr = t2;\n+    const Register t3_owner = t3;\n+\n+    \/\/ Compute owner address.\n+    lea(t2_owner_addr, Address(t1_tagged_monitor, (in_bytes(ObjectMonitor::owner_offset()) - monitor_tag)));\n+\n+    \/\/ CAS owner (null => current thread).\n+    cmpxchg(t2_owner_addr, zr, rthread, Assembler::xword, \/*acquire*\/ true,\n+            \/*release*\/ false, \/*weak*\/ false, t3_owner);\n+    br(Assembler::EQ, locked);\n+\n+    \/\/ Check if recursive.\n+    cmp(t3_owner, rthread);\n+    br(Assembler::NE, slow_path);\n+\n+    \/\/ Recursive.\n+    increment(Address(t1_tagged_monitor, in_bytes(ObjectMonitor::recursions_offset()) - monitor_tag), 1);\n+  }\n+\n+  bind(locked);\n+  increment(Address(rthread, JavaThread::held_monitor_count_offset()));\n+\n+#ifdef ASSERT\n+  \/\/ Check that locked label is reached with Flags == EQ.\n+  Label flag_correct;\n+  br(Assembler::EQ, flag_correct);\n+  stop(\"Fast Lock Flag != EQ\");\n+#endif\n+\n+  bind(slow_path);\n+#ifdef ASSERT\n+  \/\/ Check that slow_path label is reached with Flags == NE.\n+  br(Assembler::NE, flag_correct);\n+  stop(\"Fast Lock Flag != NE\");\n+  bind(flag_correct);\n+#endif\n+  \/\/ C2 uses the value of Flags (NE vs EQ) to determine the continuation.\n+}\n+\n+void C2_MacroAssembler::fast_unlock_lightweight(Register obj, Register t1, Register t2,\n+                                                Register t3) {\n+  assert(LockingMode == LM_LIGHTWEIGHT, \"must be\");\n+  assert_different_registers(obj, t1, t2, t3);\n+\n+  \/\/ Handle inflated monitor.\n+  Label inflated, inflated_load_monitor;\n+  \/\/ Finish fast unlock successfully. MUST branch to with flag == EQ\n+  Label unlocked;\n+  \/\/ Finish fast unlock unsuccessfully. MUST branch to with flag == NE\n+  Label slow_path;\n+\n+  const Register t1_mark = t1;\n+  const Register t2_top = t2;\n+  const Register t3_t = t3;\n+\n+  { \/\/ Lightweight unlock\n+\n+    \/\/ Check if obj is top of lock-stack.\n+    ldrw(t2_top, Address(rthread, JavaThread::lock_stack_top_offset()));\n+    subw(t2_top, t2_top, oopSize);\n+    ldr(t3_t, Address(rthread, t2_top));\n+    cmp(obj, t3_t);\n+    \/\/ Top of lock stack was not obj. Must be monitor.\n+    br(Assembler::NE, inflated_load_monitor);\n+\n+    \/\/ Pop lock-stack.\n+    DEBUG_ONLY(str(zr, Address(rthread, t2_top));)\n+    strw(t2_top, Address(rthread, JavaThread::lock_stack_top_offset()));\n+\n+    \/\/ Check if recursive.\n+    subw(t3_t, t2_top, oopSize);\n+    ldr(t3_t, Address(rthread, t3_t));\n+    cmp(obj, t3_t);\n+    br(Assembler::EQ, unlocked);\n+\n+    \/\/ Not recursive.\n+    \/\/ Load Mark.\n+    ldr(t1_mark, Address(obj, oopDesc::mark_offset_in_bytes()));\n+\n+    \/\/ Check header for monitor (0b10).\n+    tbnz(t1_mark, exact_log2(markWord::monitor_value), inflated);\n+\n+    \/\/ Try to unlock. Transition lock bits 0b00 => 0b01\n+    assert(oopDesc::mark_offset_in_bytes() == 0, \"required to avoid lea\");\n+    orr(t3_t, t1_mark, markWord::unlocked_value);\n+    cmpxchg(\/*addr*\/ obj, \/*expected*\/ t1_mark, \/*new*\/ t3_t, Assembler::xword,\n+            \/*acquire*\/ false, \/*release*\/ true, \/*weak*\/ false, noreg);\n+    br(Assembler::EQ, unlocked);\n+\n+    \/\/ Compare and exchange failed.\n+    \/\/ Restore lock-stack and handle the unlock in runtime.\n+    DEBUG_ONLY(str(obj, Address(rthread, t2_top));)\n+    addw(t2_top, t2_top, oopSize);\n+    str(t2_top, Address(rthread, JavaThread::lock_stack_top_offset()));\n+    b(slow_path);\n+  }\n+\n+\n+  { \/\/ Handle inflated monitor.\n+    bind(inflated_load_monitor);\n+    ldr(t1_mark, Address(obj, oopDesc::mark_offset_in_bytes()));\n+#ifdef ASSERT\n+    tbnz(t1_mark, exact_log2(markWord::monitor_value), inflated);\n+    stop(\"Fast Unlock not monitor\");\n+#endif\n+\n+    bind(inflated);\n+\n+#ifdef ASSERT\n+    Label check_done;\n+    subw(t2_top, t2_top, oopSize);\n+    cmpw(t2_top, in_bytes(JavaThread::lock_stack_base_offset()));\n+    br(Assembler::LT, check_done);\n+    ldr(t3_t, Address(rthread, t2_top));\n+    cmp(obj, t3_t);\n+    br(Assembler::NE, inflated);\n+    stop(\"Fast Unlock lock on stack\");\n+    bind(check_done);\n+#endif\n+\n+    \/\/ mark contains the tagged ObjectMonitor*.\n+    const Register t1_monitor = t1_mark;\n+    const uintptr_t monitor_tag = markWord::monitor_value;\n+\n+    \/\/ Untag the monitor.\n+    sub(t1_monitor, t1_mark, monitor_tag);\n+\n+    const Register t2_recursions = t2;\n+    Label not_recursive;\n+\n+    \/\/ Check if recursive.\n+    ldr(t2_recursions, Address(t1_monitor, ObjectMonitor::recursions_offset()));\n+    cbz(t2_recursions, not_recursive);\n+\n+    \/\/ Recursive unlock.\n+    sub(t2_recursions, t2_recursions, 1u);\n+    str(t2_recursions, Address(t1_monitor, ObjectMonitor::recursions_offset()));\n+    \/\/ Set flag == EQ\n+    cmp(t2_recursions, t2_recursions);\n+    b(unlocked);\n+\n+    bind(not_recursive);\n+\n+    Label release;\n+    const Register t2_owner_addr = t2;\n+\n+    \/\/ Compute owner address.\n+    lea(t2_owner_addr, Address(t1_monitor, ObjectMonitor::owner_offset()));\n+\n+    \/\/ Check if the entry lists are empty.\n+    ldr(rscratch1, Address(t1_monitor, ObjectMonitor::EntryList_offset()));\n+    ldr(t3_t, Address(t1_monitor, ObjectMonitor::cxq_offset()));\n+    orr(rscratch1, rscratch1, t3_t);\n+    cmp(rscratch1, zr);\n+    br(Assembler::EQ, release);\n+\n+    \/\/ The owner may be anonymous and we removed the last obj entry in\n+    \/\/ the lock-stack. This loses the information about the owner.\n+    \/\/ Write the thread to the owner field so the runtime knows the owner.\n+    str(rthread, Address(t2_owner_addr));\n+    b(slow_path);\n+\n+    bind(release);\n+    \/\/ Set owner to null.\n+    \/\/ Release to satisfy the JMM\n+    stlr(zr, t2_owner_addr);\n+  }\n+\n+  bind(unlocked);\n+  decrement(Address(rthread, JavaThread::held_monitor_count_offset()));\n+\n+#ifdef ASSERT\n+  \/\/ Check that unlocked label is reached with Flags == EQ.\n+  Label flag_correct;\n+  br(Assembler::EQ, flag_correct);\n+  stop(\"Fast Unlock Flag != EQ\");\n+#endif\n+\n+  bind(slow_path);\n+#ifdef ASSERT\n+  \/\/ Check that slow_path label is reached with Flags == NE.\n+  br(Assembler::NE, flag_correct);\n+  stop(\"Fast Unlock Flag != NE\");\n+  bind(flag_correct);\n+#endif\n+  \/\/ C2 uses the value of Flags (NE vs EQ) to determine the continuation.\n+}\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/c2_MacroAssembler_aarch64.cpp","additions":269,"deletions":31,"binary":false,"changes":300,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2020, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2020, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -39,1 +39,0 @@\n-  \/\/ See full description in macroAssembler_aarch64.cpp.\n@@ -42,0 +41,3 @@\n+  \/\/ Code used by cmpFastLockLightweight and cmpFastUnlockLightweight mach instructions in .ad file.\n+  void fast_lock_lightweight(Register object, Register t1, Register t2, Register t3);\n+  void fast_unlock_lightweight(Register object, Register t1, Register t2, Register t3);\n","filename":"src\/hotspot\/cpu\/aarch64\/c2_MacroAssembler_aarch64.hpp","additions":4,"deletions":2,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2003, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2003, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -709,1 +709,0 @@\n-      ldr(tmp, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n@@ -833,16 +832,0 @@\n-\n-      \/\/ Check for non-symmetric locking. This is allowed by the spec and the interpreter\n-      \/\/ must handle it.\n-      Register tmp = rscratch1;\n-      \/\/ First check for lock-stack underflow.\n-      ldrw(tmp, Address(rthread, JavaThread::lock_stack_top_offset()));\n-      cmpw(tmp, (unsigned)LockStack::start_offset());\n-      br(Assembler::LE, slow_case);\n-      \/\/ Then check if the top of the lock-stack matches the unlocked object.\n-      subw(tmp, tmp, oopSize);\n-      ldr(tmp, Address(rthread, tmp));\n-      cmpoop(tmp, obj_reg);\n-      br(Assembler::NE, slow_case);\n-\n-      ldr(header_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n-      tbnz(header_reg, exact_log2(markWord::monitor_value), slow_case);\n","filename":"src\/hotspot\/cpu\/aarch64\/interp_masm_aarch64.cpp","additions":1,"deletions":18,"binary":false,"changes":19,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -30,0 +30,1 @@\n+#include \"code\/compiledIC.hpp\"\n@@ -969,1 +970,1 @@\n-  \/\/ CompiledDirectStaticCall::set_to_interpreted knows the\n+  \/\/ CompiledDirectCall::set_to_interpreted knows the\n@@ -999,1 +1000,1 @@\n-  movptr(rscratch2, (uintptr_t)Universe::non_oop_word());\n+  movptr(rscratch2, (intptr_t)Universe::non_oop_word());\n@@ -1003,0 +1004,41 @@\n+int MacroAssembler::ic_check_size() {\n+  if (target_needs_far_branch(CAST_FROM_FN_PTR(address, SharedRuntime::get_ic_miss_stub()))) {\n+    return NativeInstruction::instruction_size * 7;\n+  } else {\n+    return NativeInstruction::instruction_size * 5;\n+  }\n+}\n+\n+int MacroAssembler::ic_check(int end_alignment) {\n+  Register receiver = j_rarg0;\n+  Register data = rscratch2;\n+  Register tmp1 = rscratch1;\n+  Register tmp2 = r10;\n+\n+  \/\/ The UEP of a code blob ensures that the VEP is padded. However, the padding of the UEP is placed\n+  \/\/ before the inline cache check, so we don't have to execute any nop instructions when dispatching\n+  \/\/ through the UEP, yet we can ensure that the VEP is aligned appropriately. That's why we align\n+  \/\/ before the inline cache check here, and not after\n+  align(end_alignment, offset() + ic_check_size());\n+\n+  int uep_offset = offset();\n+\n+  if (UseCompressedClassPointers) {\n+    ldrw(tmp1, Address(receiver, oopDesc::klass_offset_in_bytes()));\n+    ldrw(tmp2, Address(data, CompiledICData::speculated_klass_offset()));\n+    cmpw(tmp1, tmp2);\n+  } else {\n+    ldr(tmp1, Address(receiver, oopDesc::klass_offset_in_bytes()));\n+    ldr(tmp2, Address(data, CompiledICData::speculated_klass_offset()));\n+    cmp(tmp1, tmp2);\n+  }\n+\n+  Label dont;\n+  br(Assembler::EQ, dont);\n+  far_jump(RuntimeAddress(SharedRuntime::get_ic_miss_stub()));\n+  bind(dont);\n+  assert((offset() % end_alignment) == 0, \"Misaligned verified entry point\");\n+\n+  return uep_offset;\n+}\n+\n@@ -1104,1 +1146,8 @@\n-  while (offset() % modulus != 0) nop();\n+  align(modulus, offset());\n+}\n+\n+\/\/ Ensure that the code at target bytes offset from the current offset() is aligned\n+\/\/ according to modulus.\n+void MacroAssembler::align(int modulus, int target) {\n+  int delta = target - offset();\n+  while ((offset() + delta) % modulus != 0) nop();\n@@ -1201,1 +1250,1 @@\n-\/\/ - check recv_klass (actual object class) is a subtype of resolved_klass from CompiledICHolder\n+\/\/ - check recv_klass (actual object class) is a subtype of resolved_klass from CompiledICData\n@@ -2070,12 +2119,7 @@\n-    \/\/ Don't promote DMB ST|DMB LD to DMB (a full barrier) because\n-    \/\/ doing so would introduce a StoreLoad which the caller did not\n-    \/\/ intend\n-    if (AlwaysMergeDMB || bar->get_kind() == order_constraint\n-        || bar->get_kind() == AnyAny\n-        || order_constraint == AnyAny) {\n-      \/\/ We are merging two memory barrier instructions.  On AArch64 we\n-      \/\/ can do this simply by ORing them together.\n-      bar->set_kind(bar->get_kind() | order_constraint);\n-      BLOCK_COMMENT(\"merged membar\");\n-      return;\n-    }\n+    \/\/ We are merging two memory barrier instructions.  On AArch64 we\n+    \/\/ can do this simply by ORing them together.\n+    bar->set_kind(bar->get_kind() | order_constraint);\n+    BLOCK_COMMENT(\"merged membar\");\n+  } else {\n+    code()->set_last_insn(pc());\n+    dmb(Assembler::barrier(order_constraint));\n@@ -2083,2 +2127,0 @@\n-  code()->set_last_insn(pc());\n-  dmb(Assembler::barrier(order_constraint));\n@@ -4269,0 +4311,2 @@\n+    \/\/ Registers v0..v7 are used as data registers.\n+    \/\/ Registers v16..v31 are used as tmp registers.\n@@ -4270,14 +4314,15 @@\n-    ldrq(v1, Address(buf, 0x10));\n-    ldrq(v2, Address(buf, 0x20));\n-    ldrq(v3, Address(buf, 0x30));\n-    ldrq(v4, Address(buf, 0x40));\n-    ldrq(v5, Address(buf, 0x50));\n-    ldrq(v6, Address(buf, 0x60));\n-    ldrq(v7, Address(buf, 0x70));\n-    ldrq(v8, Address(pre(buf, 0x80)));\n-\n-    movi(v25, T4S, 0);\n-    mov(v25, S, 0, crc);\n-    eor(v1, T16B, v1, v25);\n-\n-    ldrq(v0, Address(table));\n+    ldrq(v0, Address(buf, 0x10));\n+    ldrq(v1, Address(buf, 0x20));\n+    ldrq(v2, Address(buf, 0x30));\n+    ldrq(v3, Address(buf, 0x40));\n+    ldrq(v4, Address(buf, 0x50));\n+    ldrq(v5, Address(buf, 0x60));\n+    ldrq(v6, Address(buf, 0x70));\n+    ldrq(v7, Address(pre(buf, 0x80)));\n+\n+    movi(v31, T4S, 0);\n+    mov(v31, S, 0, crc);\n+    eor(v0, T16B, v0, v31);\n+\n+    \/\/ Register v16 contains constants from the crc table.\n+    ldrq(v16, Address(table));\n@@ -4288,39 +4333,41 @@\n-    pmull (v9,  T1Q, v1, v0, T1D);\n-    pmull2(v10, T1Q, v1, v0, T2D);\n-    ldrq(v1, Address(buf, 0x10));\n-    eor3(v1, T16B, v9,  v10, v1);\n-\n-    pmull (v11, T1Q, v2, v0, T1D);\n-    pmull2(v12, T1Q, v2, v0, T2D);\n-    ldrq(v2, Address(buf, 0x20));\n-    eor3(v2, T16B, v11, v12, v2);\n-\n-    pmull (v13, T1Q, v3, v0, T1D);\n-    pmull2(v14, T1Q, v3, v0, T2D);\n-    ldrq(v3, Address(buf, 0x30));\n-    eor3(v3, T16B, v13, v14, v3);\n-\n-    pmull (v15, T1Q, v4, v0, T1D);\n-    pmull2(v16, T1Q, v4, v0, T2D);\n-    ldrq(v4, Address(buf, 0x40));\n-    eor3(v4, T16B, v15, v16, v4);\n-\n-    pmull (v17, T1Q, v5, v0, T1D);\n-    pmull2(v18, T1Q, v5, v0, T2D);\n-    ldrq(v5, Address(buf, 0x50));\n-    eor3(v5, T16B, v17, v18, v5);\n-\n-    pmull (v19, T1Q, v6, v0, T1D);\n-    pmull2(v20, T1Q, v6, v0, T2D);\n-    ldrq(v6, Address(buf, 0x60));\n-    eor3(v6, T16B, v19, v20, v6);\n-\n-    pmull (v21, T1Q, v7, v0, T1D);\n-    pmull2(v22, T1Q, v7, v0, T2D);\n-    ldrq(v7, Address(buf, 0x70));\n-    eor3(v7, T16B, v21, v22, v7);\n-\n-    pmull (v23, T1Q, v8, v0, T1D);\n-    pmull2(v24, T1Q, v8, v0, T2D);\n-    ldrq(v8, Address(pre(buf, 0x80)));\n-    eor3(v8, T16B, v23, v24, v8);\n+    pmull (v17,  T1Q, v0, v16, T1D);\n+    pmull2(v18, T1Q, v0, v16, T2D);\n+    ldrq(v0, Address(buf, 0x10));\n+    eor3(v0, T16B, v17,  v18, v0);\n+\n+    pmull (v19, T1Q, v1, v16, T1D);\n+    pmull2(v20, T1Q, v1, v16, T2D);\n+    ldrq(v1, Address(buf, 0x20));\n+    eor3(v1, T16B, v19, v20, v1);\n+\n+    pmull (v21, T1Q, v2, v16, T1D);\n+    pmull2(v22, T1Q, v2, v16, T2D);\n+    ldrq(v2, Address(buf, 0x30));\n+    eor3(v2, T16B, v21, v22, v2);\n+\n+    pmull (v23, T1Q, v3, v16, T1D);\n+    pmull2(v24, T1Q, v3, v16, T2D);\n+    ldrq(v3, Address(buf, 0x40));\n+    eor3(v3, T16B, v23, v24, v3);\n+\n+    pmull (v25, T1Q, v4, v16, T1D);\n+    pmull2(v26, T1Q, v4, v16, T2D);\n+    ldrq(v4, Address(buf, 0x50));\n+    eor3(v4, T16B, v25, v26, v4);\n+\n+    pmull (v27, T1Q, v5, v16, T1D);\n+    pmull2(v28, T1Q, v5, v16, T2D);\n+    ldrq(v5, Address(buf, 0x60));\n+    eor3(v5, T16B, v27, v28, v5);\n+\n+    pmull (v29, T1Q, v6, v16, T1D);\n+    pmull2(v30, T1Q, v6, v16, T2D);\n+    ldrq(v6, Address(buf, 0x70));\n+    eor3(v6, T16B, v29, v30, v6);\n+\n+    \/\/ Reuse registers v23, v24.\n+    \/\/ Using them won't block the first instruction of the next iteration.\n+    pmull (v23, T1Q, v7, v16, T1D);\n+    pmull2(v24, T1Q, v7, v16, T2D);\n+    ldrq(v7, Address(pre(buf, 0x80)));\n+    eor3(v7, T16B, v23, v24, v7);\n@@ -4332,1 +4379,2 @@\n-    ldrq(v0, Address(table, 0x10));\n+    \/\/ Use v31 for constants because v16 can be still in use.\n+    ldrq(v31, Address(table, 0x10));\n@@ -4334,3 +4382,3 @@\n-    pmull (v10,  T1Q, v1, v0, T1D);\n-    pmull2(v11, T1Q, v1, v0, T2D);\n-    eor3(v1, T16B, v10, v11, v5);\n+    pmull (v17,  T1Q, v0, v31, T1D);\n+    pmull2(v18, T1Q, v0, v31, T2D);\n+    eor3(v0, T16B, v17, v18, v4);\n@@ -4338,3 +4386,3 @@\n-    pmull (v12, T1Q, v2, v0, T1D);\n-    pmull2(v13, T1Q, v2, v0, T2D);\n-    eor3(v2, T16B, v12, v13, v6);\n+    pmull (v19, T1Q, v1, v31, T1D);\n+    pmull2(v20, T1Q, v1, v31, T2D);\n+    eor3(v1, T16B, v19, v20, v5);\n@@ -4342,3 +4390,3 @@\n-    pmull (v14, T1Q, v3, v0, T1D);\n-    pmull2(v15, T1Q, v3, v0, T2D);\n-    eor3(v3, T16B, v14, v15, v7);\n+    pmull (v21, T1Q, v2, v31, T1D);\n+    pmull2(v22, T1Q, v2, v31, T2D);\n+    eor3(v2, T16B, v21, v22, v6);\n@@ -4346,3 +4394,3 @@\n-    pmull (v16, T1Q, v4, v0, T1D);\n-    pmull2(v17, T1Q, v4, v0, T2D);\n-    eor3(v4, T16B, v16, v17, v8);\n+    pmull (v23, T1Q, v3, v31, T1D);\n+    pmull2(v24, T1Q, v3, v31, T2D);\n+    eor3(v3, T16B, v23, v24, v7);\n@@ -4351,14 +4399,17 @@\n-    ldrq(v5, Address(table, 0x20));\n-    pmull (v10, T1Q, v1, v5, T1D);\n-    pmull2(v11, T1Q, v1, v5, T2D);\n-    eor3(v4, T16B, v4, v10, v11);\n-\n-    ldrq(v6, Address(table, 0x30));\n-    pmull (v12, T1Q, v2, v6, T1D);\n-    pmull2(v13, T1Q, v2, v6, T2D);\n-    eor3(v4, T16B, v4, v12, v13);\n-\n-    ldrq(v7, Address(table, 0x40));\n-    pmull (v14, T1Q, v3, v7, T1D);\n-    pmull2(v15, T1Q, v3, v7, T2D);\n-    eor3(v1, T16B, v4, v14, v15);\n+    \/\/ Use v17 for constants because v31 can be still in use.\n+    ldrq(v17, Address(table, 0x20));\n+    pmull (v25, T1Q, v0, v17, T1D);\n+    pmull2(v26, T1Q, v0, v17, T2D);\n+    eor3(v3, T16B, v3, v25, v26);\n+\n+    \/\/ Use v18 for constants because v17 can be still in use.\n+    ldrq(v18, Address(table, 0x30));\n+    pmull (v27, T1Q, v1, v18, T1D);\n+    pmull2(v28, T1Q, v1, v18, T2D);\n+    eor3(v3, T16B, v3, v27, v28);\n+\n+    \/\/ Use v19 for constants because v18 can be still in use.\n+    ldrq(v19, Address(table, 0x40));\n+    pmull (v29, T1Q, v2, v19, T1D);\n+    pmull2(v30, T1Q, v2, v19, T2D);\n+    eor3(v0, T16B, v3, v29, v30);\n@@ -4369,2 +4420,2 @@\n-    mov(tmp0, v1, D, 0);\n-    mov(tmp1, v1, D, 1);\n+    mov(tmp0, v0, D, 0);\n+    mov(tmp1, v0, D, 1);\n@@ -5350,1 +5401,0 @@\n-\/\/ elem_size is the element size in bytes: either 1 or 2.\n@@ -5357,1 +5407,1 @@\n-                                   Register result, Register cnt1, int elem_size)\n+                                   Register result, Register cnt1)\n@@ -5364,1 +5414,0 @@\n-  assert(elem_size == 1 || elem_size == 2, \"must be 2 or 1 byte\");\n@@ -5369,2 +5418,1 @@\n-    const char kind = (elem_size == 2) ? 'U' : 'L';\n-    snprintf(comment, sizeof comment, \"{string_equals%c\", kind);\n+    snprintf(comment, sizeof comment, \"{string_equalsL\");\n@@ -5419,2 +5467,1 @@\n-  if (elem_size == 1) { \/\/ Only needed when comparing 1-byte elements\n-    tbz(cnt1, 0, SAME); \/\/ 0-1 bytes left.\n+  tbz(cnt1, 0, SAME); \/\/ 0-1 bytes left.\n@@ -5422,5 +5469,4 @@\n-      ldrb(tmp1, a1);\n-      ldrb(tmp2, a2);\n-      eorw(tmp1, tmp1, tmp2);\n-      cbnzw(tmp1, DONE);\n-    }\n+    ldrb(tmp1, a1);\n+    ldrb(tmp2, a2);\n+    eorw(tmp1, tmp1, tmp2);\n+    cbnzw(tmp1, DONE);\n@@ -6339,2 +6385,0 @@\n-\/\/ Branches to slow upon failure to lock the object, with ZF cleared.\n-\/\/ Falls through upon success with ZF set.\n@@ -6343,3 +6387,3 @@\n-\/\/  - hdr: the header, already loaded from obj, will be destroyed\n-\/\/  - t1, t2: temporary registers, will be destroyed\n-void MacroAssembler::lightweight_lock(Register obj, Register hdr, Register t1, Register t2, Label& slow) {\n+\/\/  - t1, t2, t3: temporary registers, will be destroyed\n+\/\/  - slow: branched to if locking fails, absolute offset may larger than 32KB (imm14 encoding).\n+void MacroAssembler::lightweight_lock(Register obj, Register t1, Register t2, Register t3, Label& slow) {\n@@ -6347,15 +6391,32 @@\n-  assert_different_registers(obj, hdr, t1, t2, rscratch1);\n-\n-  \/\/ Check if we would have space on lock-stack for the object.\n-  ldrw(t1, Address(rthread, JavaThread::lock_stack_top_offset()));\n-  cmpw(t1, (unsigned)LockStack::end_offset() - 1);\n-  br(Assembler::GT, slow);\n-\n-  \/\/ Load (object->mark() | 1) into hdr\n-  orr(hdr, hdr, markWord::unlocked_value);\n-  \/\/ Clear lock-bits, into t2\n-  eor(t2, hdr, markWord::unlocked_value);\n-  \/\/ Try to swing header from unlocked to locked\n-  \/\/ Clobbers rscratch1 when UseLSE is false\n-  cmpxchg(\/*addr*\/ obj, \/*expected*\/ hdr, \/*new*\/ t2, Assembler::xword,\n-          \/*acquire*\/ true, \/*release*\/ true, \/*weak*\/ false, t1);\n+  assert_different_registers(obj, t1, t2, t3, rscratch1);\n+\n+  Label push;\n+  const Register top = t1;\n+  const Register mark = t2;\n+  const Register t = t3;\n+\n+  \/\/ Preload the markWord. It is important that this is the first\n+  \/\/ instruction emitted as it is part of C1's null check semantics.\n+  ldr(mark, Address(obj, oopDesc::mark_offset_in_bytes()));\n+\n+  \/\/ Check if the lock-stack is full.\n+  ldrw(top, Address(rthread, JavaThread::lock_stack_top_offset()));\n+  cmpw(top, (unsigned)LockStack::end_offset());\n+  br(Assembler::GE, slow);\n+\n+  \/\/ Check for recursion.\n+  subw(t, top, oopSize);\n+  ldr(t, Address(rthread, t));\n+  cmp(obj, t);\n+  br(Assembler::EQ, push);\n+\n+  \/\/ Check header for monitor (0b10).\n+  tst(mark, markWord::monitor_value);\n+  br(Assembler::NE, slow);\n+\n+  \/\/ Try to lock. Transition lock bits 0b01 => 0b00\n+  assert(oopDesc::mark_offset_in_bytes() == 0, \"required to avoid lea\");\n+  orr(mark, mark, markWord::unlocked_value);\n+  eor(t, mark, markWord::unlocked_value);\n+  cmpxchg(\/*addr*\/ obj, \/*expected*\/ mark, \/*new*\/ t, Assembler::xword,\n+          \/*acquire*\/ true, \/*release*\/ false, \/*weak*\/ false, noreg);\n@@ -6364,5 +6425,5 @@\n-  \/\/ After successful lock, push object on lock-stack\n-  ldrw(t1, Address(rthread, JavaThread::lock_stack_top_offset()));\n-  str(obj, Address(rthread, t1));\n-  addw(t1, t1, oopSize);\n-  strw(t1, Address(rthread, JavaThread::lock_stack_top_offset()));\n+  bind(push);\n+  \/\/ After successful lock, push object on lock-stack.\n+  str(obj, Address(rthread, top));\n+  addw(top, top, oopSize);\n+  strw(top, Address(rthread, JavaThread::lock_stack_top_offset()));\n@@ -6372,2 +6433,0 @@\n-\/\/ Branches to slow upon failure, with ZF cleared.\n-\/\/ Falls through upon success, with ZF set.\n@@ -6376,3 +6435,3 @@\n-\/\/ - hdr: the (pre-loaded) header of the object\n-\/\/ - t1, t2: temporary registers\n-void MacroAssembler::lightweight_unlock(Register obj, Register hdr, Register t1, Register t2, Label& slow) {\n+\/\/ - t1, t2, t3: temporary registers\n+\/\/ - slow: branched to if unlocking fails, absolute offset may larger than 32KB (imm14 encoding).\n+void MacroAssembler::lightweight_unlock(Register obj, Register t1, Register t2, Register t3, Label& slow) {\n@@ -6380,1 +6439,2 @@\n-  assert_different_registers(obj, hdr, t1, t2, rscratch1);\n+  \/\/ cmpxchg clobbers rscratch1.\n+  assert_different_registers(obj, t1, t2, t3, rscratch1);\n@@ -6384,4 +6444,0 @@\n-    \/\/ The following checks rely on the fact that LockStack is only ever modified by\n-    \/\/ its owning thread, even if the lock got inflated concurrently; removal of LockStack\n-    \/\/ entries after inflation will happen delayed in that case.\n-\n@@ -6392,1 +6448,1 @@\n-    br(Assembler::GT, stack_ok);\n+    br(Assembler::GE, stack_ok);\n@@ -6396,18 +6452,0 @@\n-  {\n-    \/\/ Check if the top of the lock-stack matches the unlocked object.\n-    Label tos_ok;\n-    subw(t1, t1, oopSize);\n-    ldr(t1, Address(rthread, t1));\n-    cmpoop(t1, obj);\n-    br(Assembler::EQ, tos_ok);\n-    STOP(\"Top of lock-stack does not match the unlocked object\");\n-    bind(tos_ok);\n-  }\n-  {\n-    \/\/ Check that hdr is fast-locked.\n-    Label hdr_ok;\n-    tst(hdr, markWord::lock_mask_in_place);\n-    br(Assembler::EQ, hdr_ok);\n-    STOP(\"Header is not fast-locked\");\n-    bind(hdr_ok);\n-  }\n@@ -6416,2 +6454,4 @@\n-  \/\/ Load the new header (unlocked) into t1\n-  orr(t1, hdr, markWord::unlocked_value);\n+  Label unlocked, push_and_slow;\n+  const Register top = t1;\n+  const Register mark = t2;\n+  const Register t = t3;\n@@ -6419,4 +6459,5 @@\n-  \/\/ Try to swing header from locked to unlocked\n-  \/\/ Clobbers rscratch1 when UseLSE is false\n-  cmpxchg(obj, hdr, t1, Assembler::xword,\n-          \/*acquire*\/ true, \/*release*\/ true, \/*weak*\/ false, t2);\n+  \/\/ Check if obj is top of lock-stack.\n+  ldrw(top, Address(rthread, JavaThread::lock_stack_top_offset()));\n+  subw(top, top, oopSize);\n+  ldr(t, Address(rthread, top));\n+  cmp(obj, t);\n@@ -6425,3 +6466,14 @@\n-  \/\/ After successful unlock, pop object from lock-stack\n-  ldrw(t1, Address(rthread, JavaThread::lock_stack_top_offset()));\n-  subw(t1, t1, oopSize);\n+  \/\/ Pop lock-stack.\n+  DEBUG_ONLY(str(zr, Address(rthread, top));)\n+  strw(top, Address(rthread, JavaThread::lock_stack_top_offset()));\n+\n+  \/\/ Check if recursive.\n+  subw(t, top, oopSize);\n+  ldr(t, Address(rthread, t));\n+  cmp(obj, t);\n+  br(Assembler::EQ, unlocked);\n+\n+  \/\/ Not recursive. Check header for monitor (0b10).\n+  ldr(mark, Address(obj, oopDesc::mark_offset_in_bytes()));\n+  tbnz(mark, log2i_exact(markWord::monitor_value), push_and_slow);\n+\n@@ -6429,1 +6481,5 @@\n-  str(zr, Address(rthread, t1));\n+  \/\/ Check header not unlocked (0b01).\n+  Label not_unlocked;\n+  tbz(mark, log2i_exact(markWord::unlocked_value), not_unlocked);\n+  stop(\"lightweight_unlock already unlocked\");\n+  bind(not_unlocked);\n@@ -6431,1 +6487,16 @@\n-  strw(t1, Address(rthread, JavaThread::lock_stack_top_offset()));\n+\n+  \/\/ Try to unlock. Transition lock bits 0b00 => 0b01\n+  assert(oopDesc::mark_offset_in_bytes() == 0, \"required to avoid lea\");\n+  orr(t, mark, markWord::unlocked_value);\n+  cmpxchg(obj, mark, t, Assembler::xword,\n+          \/*acquire*\/ false, \/*release*\/ true, \/*weak*\/ false, noreg);\n+  br(Assembler::EQ, unlocked);\n+\n+  bind(push_and_slow);\n+  \/\/ Restore lock-stack and handle the unlock in runtime.\n+  DEBUG_ONLY(str(obj, Address(rthread, top));)\n+  addw(top, top, oopSize);\n+  strw(top, Address(rthread, JavaThread::lock_stack_top_offset()));\n+  b(slow);\n+\n+  bind(unlocked);\n","filename":"src\/hotspot\/cpu\/aarch64\/macroAssembler_aarch64.cpp","additions":249,"deletions":178,"binary":false,"changes":427,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -723,0 +723,1 @@\n+  void align(int modulus, int target);\n@@ -1250,0 +1251,2 @@\n+  static int ic_check_size();\n+  int ic_check(int end_alignment);\n@@ -1402,2 +1405,1 @@\n-  void string_equals(Register a1, Register a2, Register result, Register cnt1,\n-                     int elem_size);\n+  void string_equals(Register a1, Register a2, Register result, Register cnt1);\n@@ -1603,2 +1605,2 @@\n-  void lightweight_lock(Register obj, Register hdr, Register t1, Register t2, Label& slow);\n-  void lightweight_unlock(Register obj, Register hdr, Register t1, Register t2, Label& slow);\n+  void lightweight_lock(Register obj, Register t1, Register t2, Register t3, Label& slow);\n+  void lightweight_unlock(Register obj, Register t1, Register t2, Register t3, Label& slow);\n","filename":"src\/hotspot\/cpu\/aarch64\/macroAssembler_aarch64.hpp","additions":7,"deletions":5,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2003, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2003, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -33,1 +33,0 @@\n-#include \"code\/icBuffer.hpp\"\n@@ -42,1 +41,0 @@\n-#include \"oops\/compiledICHolder.hpp\"\n@@ -744,3 +742,1 @@\n-  Label ok;\n-\n-  Register holder = rscratch2;\n+  Register data = rscratch2;\n@@ -761,8 +757,0 @@\n-    __ load_klass(rscratch1, receiver);\n-    __ ldr(tmp, Address(holder, CompiledICHolder::holder_klass_offset()));\n-    __ cmp(rscratch1, tmp);\n-    __ ldr(rmethod, Address(holder, CompiledICHolder::holder_metadata_offset()));\n-    __ br(Assembler::EQ, ok);\n-    __ far_jump(RuntimeAddress(SharedRuntime::get_ic_miss_stub()));\n-\n-    __ bind(ok);\n@@ -772,0 +760,3 @@\n+    __ ic_check(1 \/* end_alignment *\/);\n+    __ ldr(rmethod, Address(data, CompiledICData::speculated_method_offset()));\n+\n@@ -1122,1 +1113,1 @@\n-    address stub = CompiledStaticCall::emit_to_interp_stub(*cbuf, tr_call);\n+    address stub = CompiledDirectCall::emit_to_interp_stub(*cbuf, tr_call);\n@@ -1187,1 +1178,1 @@\n-  address stub = CompiledStaticCall::emit_to_interp_stub(*cbuf, tr_call);\n+  address stub = CompiledDirectCall::emit_to_interp_stub(*cbuf, tr_call);\n@@ -1395,0 +1386,1 @@\n+    if (nm == nullptr) return nm;\n@@ -1542,2 +1534,0 @@\n-\n-  const Register ic_reg = rscratch2;\n@@ -1546,1 +1536,0 @@\n-  Label hit;\n@@ -1549,1 +1538,1 @@\n-  assert_different_registers(ic_reg, receiver, rscratch1);\n+  assert_different_registers(receiver, rscratch1);\n@@ -1551,4 +1540,1 @@\n-  __ cmp_klass(receiver, ic_reg, rscratch1);\n-  __ br(Assembler::EQ, hit);\n-\n-  __ far_jump(RuntimeAddress(SharedRuntime::get_ic_miss_stub()));\n+  __ ic_check(8 \/* end_alignment *\/);\n@@ -1557,4 +1543,0 @@\n-  __ align(8);\n-\n-  __ bind(hit);\n-\n@@ -1817,1 +1799,0 @@\n-      __ ldr(swap_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n@@ -1963,2 +1944,0 @@\n-      __ ldr(old_hdr, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n-      __ tbnz(old_hdr, exact_log2(markWord::monitor_value), slow_path_unlock);\n","filename":"src\/hotspot\/cpu\/aarch64\/sharedRuntime_aarch64.cpp","additions":10,"deletions":31,"binary":false,"changes":41,"status":"modified"},{"patch":"@@ -77,1 +77,0 @@\n-const Register IC_Klass    = rax;   \/\/ where the IC klass is cached\n@@ -341,17 +340,1 @@\n-  Register receiver = FrameMap::receiver_opr->as_register();\n-  Register ic_klass = IC_Klass;\n-  const int ic_cmp_size = LP64_ONLY(10) NOT_LP64(9);\n-  const bool do_post_padding = VerifyOops || UseCompressedClassPointers;\n-  if (!do_post_padding) {\n-    \/\/ insert some nops so that the verified entry point is aligned on CodeEntryAlignment\n-    __ align(CodeEntryAlignment, __ offset() + ic_cmp_size);\n-  }\n-  int offset = __ offset();\n-  __ inline_cache_check(receiver, IC_Klass);\n-  assert(__ offset() % CodeEntryAlignment == 0 || do_post_padding, \"alignment must be correct\");\n-  if (do_post_padding) {\n-    \/\/ force alignment after the cache check.\n-    \/\/ It's been verified to be aligned if !VerifyOops\n-    __ align(CodeEntryAlignment);\n-  }\n-  return offset;\n+  return __ ic_check(CodeEntryAlignment);\n","filename":"src\/hotspot\/cpu\/x86\/c1_LIRAssembler_x86.cpp","additions":1,"deletions":18,"binary":false,"changes":19,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2005, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2005, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -1210,0 +1210,1 @@\n+#ifndef _LP64\n@@ -1212,1 +1213,1 @@\n-LIR_Opr fixed_register_for(BasicType type) {\n+static LIR_Opr fixed_register_for(BasicType type) {\n@@ -1221,0 +1222,1 @@\n+#endif\n","filename":"src\/hotspot\/cpu\/x86\/c1_LIRGenerator_x86.cpp","additions":4,"deletions":2,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1999, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1999, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -28,0 +28,1 @@\n+#include \"code\/compiledIC.hpp\"\n@@ -68,5 +69,0 @@\n-  if (LockingMode != LM_PLACEHOLDER) {\n-    \/\/ Load object header\n-    movptr(hdr, Address(obj, hdr_offset));\n-  }\n-\n@@ -92,0 +88,2 @@\n+    \/\/ Load object header\n+    movptr(hdr, Address(obj, hdr_offset));\n@@ -163,3 +161,8 @@\n-    movptr(disp_hdr, Address(obj, hdr_offset));\n-    andptr(disp_hdr, ~(int32_t)markWord::lock_mask_in_place);\n-    lightweight_unlock(obj, disp_hdr, hdr, slow_case);\n+#ifdef _LP64\n+    lightweight_unlock(obj, disp_hdr, r15_thread, hdr, slow_case);\n+#else\n+    \/\/ This relies on the implementation of lightweight_unlock being able to handle\n+    \/\/ that the reg_rax and thread Register parameters may alias each other.\n+    get_thread(disp_hdr);\n+    lightweight_unlock(obj, disp_hdr, disp_hdr, hdr, slow_case);\n+#endif\n@@ -324,24 +327,0 @@\n-\n-\n-void C1_MacroAssembler::inline_cache_check(Register receiver, Register iCache) {\n-  verify_oop(receiver);\n-  \/\/ explicit null check not needed since load from [klass_offset] causes a trap\n-  \/\/ check against inline cache\n-  assert(!MacroAssembler::needs_explicit_null_check(oopDesc::klass_offset_in_bytes()), \"must add explicit null check\");\n-  int start_offset = offset();\n-\n-  if (UseCompressedClassPointers) {\n-    load_klass(rscratch1, receiver, rscratch2);\n-    cmpptr(rscratch1, iCache);\n-  } else {\n-    cmpptr(iCache, Address(receiver, oopDesc::klass_offset_in_bytes()));\n-  }\n-  \/\/ if icache check fails, then jump to runtime routine\n-  \/\/ Note: RECEIVER must still contain the receiver!\n-  jump_cc(Assembler::notEqual,\n-          RuntimeAddress(SharedRuntime::get_ic_miss_stub()));\n-  const int ic_cmp_size = LP64_ONLY(10) NOT_LP64(9);\n-  assert(UseCompressedClassPointers || offset() - start_offset == ic_cmp_size, \"check alignment in emit_method_entry\");\n-}\n-\n-\n","filename":"src\/hotspot\/cpu\/x86\/c1_MacroAssembler_x86.cpp","additions":12,"deletions":33,"binary":false,"changes":45,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2020, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2020, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -80,6 +80,2 @@\n-#ifdef _LP64\n-int C2HandleAnonOMOwnerStub::max_size() const {\n-  \/\/ Max size of stub has been determined by testing with 0, in which case\n-  \/\/ C2CodeStubList::emit() will throw an assertion and report the actual size that\n-  \/\/ is needed.\n-  return DEBUG_ONLY(36) NOT_DEBUG(21);\n+int C2FastUnlockLightweightStub::max_size() const {\n+  return 128;\n@@ -88,6 +84,8 @@\n-void C2HandleAnonOMOwnerStub::emit(C2_MacroAssembler& masm) {\n-  __ bind(entry());\n-  Register mon = monitor();\n-  Register t = tmp();\n-  __ movptr(Address(mon, OM_OFFSET_NO_MONITOR_VALUE_TAG(owner)), r15_thread);\n-  __ subl(Address(r15_thread, JavaThread::lock_stack_top_offset()), oopSize);\n+void C2FastUnlockLightweightStub::emit(C2_MacroAssembler& masm) {\n+  assert(_t == rax, \"must be\");\n+\n+  Label restore_held_monitor_count_and_slow_path;\n+\n+  { \/\/ Restore lock-stack and handle the unlock in runtime.\n+\n+    __ bind(_push_and_slow_path);\n@@ -95,2 +93,3 @@\n-  __ movl(t, Address(r15_thread, JavaThread::lock_stack_top_offset()));\n-  __ movptr(Address(r15_thread, t), 0);\n+    \/\/ The obj was only cleared in debug.\n+    __ movl(_t, Address(_thread, JavaThread::lock_stack_top_offset()));\n+    __ movptr(Address(_thread, _t), _obj);\n@@ -98,2 +97,45 @@\n-  __ jmp(continuation());\n-}\n+    __ addl(Address(_thread, JavaThread::lock_stack_top_offset()), oopSize);\n+  }\n+\n+  { \/\/ Restore held monitor count and slow path.\n+\n+    __ bind(restore_held_monitor_count_and_slow_path);\n+    \/\/ Restore held monitor count.\n+    __ increment(Address(_thread, JavaThread::held_monitor_count_offset()));\n+    \/\/ increment will always result in ZF = 0 (no overflows).\n+    __ jmp(slow_path_continuation());\n+  }\n+\n+  { \/\/ Handle monitor medium path.\n+\n+    __ bind(_check_successor);\n+\n+    Label fix_zf_and_unlocked;\n+    const Register monitor = _mark;\n+\n+#ifndef _LP64\n+    __ jmpb(restore_held_monitor_count_and_slow_path);\n+#else \/\/ _LP64\n+    \/\/ successor null check.\n+    __ cmpptr(Address(monitor, OM_OFFSET_NO_MONITOR_VALUE_TAG(succ)), NULL_WORD);\n+    __ jccb(Assembler::equal, restore_held_monitor_count_and_slow_path);\n+\n+    \/\/ Release lock.\n+    __ movptr(Address(monitor, OM_OFFSET_NO_MONITOR_VALUE_TAG(owner)), NULL_WORD);\n+\n+    \/\/ Fence.\n+    \/\/ Instead of MFENCE we use a dummy locked add of 0 to the top-of-stack.\n+    __ lock(); __ addl(Address(rsp, 0), 0);\n+\n+    \/\/ Recheck successor.\n+    __ cmpptr(Address(monitor, OM_OFFSET_NO_MONITOR_VALUE_TAG(succ)), NULL_WORD);\n+    \/\/ Observed a successor after the release -> fence we have handed off the monitor\n+    __ jccb(Assembler::notEqual, fix_zf_and_unlocked);\n+\n+    \/\/ Try to relock, if it fails the monitor has been handed over\n+    \/\/ TODO: Caveat, this may fail due to deflation, which does\n+    \/\/       not handle the monitor handoff. Currently only works\n+    \/\/       due to the responsible thread.\n+    __ xorptr(rax, rax);\n+    __ lock(); __ cmpxchgptr(_thread, Address(monitor, OM_OFFSET_NO_MONITOR_VALUE_TAG(owner)));\n+    __ jccb  (Assembler::equal, restore_held_monitor_count_and_slow_path);\n@@ -102,0 +144,6 @@\n+    __ bind(fix_zf_and_unlocked);\n+    __ xorl(rax, rax);\n+    __ jmp(unlocked_continuation());\n+  }\n+}\n+\n","filename":"src\/hotspot\/cpu\/x86\/c2_CodeStubs_x86.cpp","additions":65,"deletions":17,"binary":false,"changes":82,"status":"modified"},{"patch":"@@ -566,0 +566,1 @@\n+  assert(LockingMode != LM_LIGHTWEIGHT, \"lightweight locking should use fast_lock_lightweight\");\n@@ -618,1 +619,2 @@\n-  } else if (LockingMode == LM_LEGACY) {\n+  } else {\n+    assert(LockingMode == LM_LEGACY, \"must be\");\n@@ -633,4 +635,0 @@\n-  } else {\n-    assert(LockingMode == LM_LIGHTWEIGHT, \"\");\n-    lightweight_lock(objReg, tmpReg, thread, scrReg, NO_COUNT);\n-    jmp(COUNT);\n@@ -767,0 +765,1 @@\n+  assert(LockingMode != LM_LIGHTWEIGHT, \"lightweight locking should use fast_unlock_lightweight\");\n@@ -798,17 +797,0 @@\n-  if (LockingMode == LM_LIGHTWEIGHT) {\n-    \/\/ If the owner is ANONYMOUS, we need to fix it -  in an outline stub.\n-    testb(Address(tmpReg, OM_OFFSET_NO_MONITOR_VALUE_TAG(owner)), (int32_t) ObjectMonitor::ANONYMOUS_OWNER);\n-#ifdef _LP64\n-    if (!Compile::current()->output()->in_scratch_emit_size()) {\n-      C2HandleAnonOMOwnerStub* stub = new (Compile::current()->comp_arena()) C2HandleAnonOMOwnerStub(tmpReg, boxReg);\n-      Compile::current()->output()->add_stub(stub);\n-      jcc(Assembler::notEqual, stub->entry());\n-      bind(stub->continuation());\n-    } else\n-#endif\n-    {\n-      \/\/ We can't easily implement this optimization on 32 bit because we don't have a thread register.\n-      \/\/ Call the slow-path instead.\n-      jcc(Assembler::notEqual, NO_COUNT);\n-    }\n-  }\n@@ -936,1 +918,1 @@\n-  if (LockingMode != LM_MONITOR) {\n+  if (LockingMode == LM_LEGACY) {\n@@ -938,9 +920,3 @@\n-    if (LockingMode == LM_LIGHTWEIGHT) {\n-      mov(boxReg, tmpReg);\n-      lightweight_unlock(objReg, boxReg, tmpReg, NO_COUNT);\n-      jmp(COUNT);\n-    } else if (LockingMode == LM_LEGACY) {\n-      movptr(tmpReg, Address (boxReg, 0));      \/\/ re-fetch\n-      lock();\n-      cmpxchgptr(tmpReg, Address(objReg, oopDesc::mark_offset_in_bytes())); \/\/ Uses RAX which is box\n-    }\n+    movptr(tmpReg, Address (boxReg, 0));      \/\/ re-fetch\n+    lock();\n+    cmpxchgptr(tmpReg, Address(objReg, oopDesc::mark_offset_in_bytes())); \/\/ Uses RAX which is box\n@@ -949,0 +925,1 @@\n+\n@@ -969,0 +946,241 @@\n+void C2_MacroAssembler::fast_lock_lightweight(Register obj, Register box, Register rax_reg,\n+                                              Register t, Register thread) {\n+  assert(LockingMode == LM_LIGHTWEIGHT, \"must be\");\n+  assert(rax_reg == rax, \"Used for CAS\");\n+  assert_different_registers(obj, box, rax_reg, t, thread);\n+\n+  \/\/ Handle inflated monitor.\n+  Label inflated;\n+  \/\/ Finish fast lock successfully. ZF value is irrelevant.\n+  Label locked;\n+  \/\/ Finish fast lock unsuccessfully. MUST jump with ZF == 0\n+  Label slow_path;\n+\n+  if (DiagnoseSyncOnValueBasedClasses != 0) {\n+    load_klass(rax_reg, obj, t);\n+    movl(rax_reg, Address(rax_reg, Klass::access_flags_offset()));\n+    testl(rax_reg, JVM_ACC_IS_VALUE_BASED_CLASS);\n+    jcc(Assembler::notZero, slow_path);\n+  }\n+\n+  const Register mark = t;\n+\n+  { \/\/ Lightweight Lock\n+\n+    Label push;\n+\n+    const Register top = box;\n+\n+    \/\/ Load the mark.\n+    movptr(mark, Address(obj, oopDesc::mark_offset_in_bytes()));\n+\n+    \/\/ Prefetch top.\n+    movl(top, Address(thread, JavaThread::lock_stack_top_offset()));\n+\n+    \/\/ Check for monitor (0b10).\n+    testptr(mark, markWord::monitor_value);\n+    jcc(Assembler::notZero, inflated);\n+\n+    \/\/ Check if lock-stack is full.\n+    cmpl(top, LockStack::end_offset() - 1);\n+    jcc(Assembler::greater, slow_path);\n+\n+    \/\/ Check if recursive.\n+    cmpptr(obj, Address(thread, top, Address::times_1, -oopSize));\n+    jccb(Assembler::equal, push);\n+\n+    \/\/ Try to lock. Transition lock bits 0b01 => 0b00\n+    movptr(rax_reg, mark);\n+    orptr(rax_reg, markWord::unlocked_value);\n+    andptr(mark, ~(int32_t)markWord::unlocked_value);\n+    lock(); cmpxchgptr(mark, Address(obj, oopDesc::mark_offset_in_bytes()));\n+    jcc(Assembler::notEqual, slow_path);\n+\n+    bind(push);\n+    \/\/ After successful lock, push object on lock-stack.\n+    movptr(Address(thread, top), obj);\n+    addl(Address(thread, JavaThread::lock_stack_top_offset()), oopSize);\n+    jmpb(locked);\n+  }\n+\n+  { \/\/ Handle inflated monitor.\n+    bind(inflated);\n+\n+    const Register tagged_monitor = mark;\n+\n+    \/\/ CAS owner (null => current thread).\n+    xorptr(rax_reg, rax_reg);\n+    lock(); cmpxchgptr(thread, Address(tagged_monitor, OM_OFFSET_NO_MONITOR_VALUE_TAG(owner)));\n+    jccb(Assembler::equal, locked);\n+\n+    \/\/ Check if recursive.\n+    cmpptr(thread, rax_reg);\n+    jccb(Assembler::notEqual, slow_path);\n+\n+    \/\/ Recursive.\n+    increment(Address(tagged_monitor, OM_OFFSET_NO_MONITOR_VALUE_TAG(recursions)));\n+  }\n+\n+  bind(locked);\n+  increment(Address(thread, JavaThread::held_monitor_count_offset()));\n+  \/\/ Set ZF = 1\n+  xorl(rax_reg, rax_reg);\n+\n+#ifdef ASSERT\n+  \/\/ Check that locked label is reached with ZF set.\n+  Label zf_correct;\n+  jccb(Assembler::zero, zf_correct);\n+  stop(\"Fast Lock ZF != 1\");\n+#endif\n+\n+  bind(slow_path);\n+#ifdef ASSERT\n+  \/\/ Check that slow_path label is reached with ZF not set.\n+  jccb(Assembler::notZero, zf_correct);\n+  stop(\"Fast Lock ZF != 0\");\n+  bind(zf_correct);\n+#endif\n+  \/\/ C2 uses the value of ZF to determine the continuation.\n+}\n+\n+void C2_MacroAssembler::fast_unlock_lightweight(Register obj, Register reg_rax, Register t, Register thread) {\n+  assert(LockingMode == LM_LIGHTWEIGHT, \"must be\");\n+  assert(reg_rax == rax, \"Used for CAS\");\n+  assert_different_registers(obj, reg_rax, t);\n+\n+  \/\/ Handle inflated monitor.\n+  Label inflated, inflated_check_lock_stack;\n+  \/\/ Finish fast unlock successfully.  MUST jump with ZF == 1\n+  Label unlocked;\n+\n+  \/\/ Assume success.\n+  decrement(Address(thread, JavaThread::held_monitor_count_offset()));\n+\n+  const Register mark = t;\n+  const Register top = reg_rax;\n+\n+  Label dummy;\n+  C2FastUnlockLightweightStub* stub = nullptr;\n+\n+  if (!Compile::current()->output()->in_scratch_emit_size()) {\n+    stub = new (Compile::current()->comp_arena()) C2FastUnlockLightweightStub(obj, mark, reg_rax, thread);\n+    Compile::current()->output()->add_stub(stub);\n+  }\n+\n+  Label& push_and_slow_path = stub == nullptr ? dummy : stub->push_and_slow_path();\n+  Label& check_successor = stub == nullptr ? dummy : stub->check_successor();\n+\n+  { \/\/ Lightweight Unlock\n+\n+    \/\/ Load top.\n+    movl(top, Address(thread, JavaThread::lock_stack_top_offset()));\n+\n+    \/\/ Prefetch mark.\n+    movptr(mark, Address(obj, oopDesc::mark_offset_in_bytes()));\n+\n+    \/\/ Check if obj is top of lock-stack.\n+    cmpptr(obj, Address(thread, top, Address::times_1, -oopSize));\n+    \/\/ Top of lock stack was not obj. Must be monitor.\n+    jcc(Assembler::notEqual, inflated_check_lock_stack);\n+\n+    \/\/ Pop lock-stack.\n+    DEBUG_ONLY(movptr(Address(thread, top, Address::times_1, -oopSize), 0);)\n+    subl(Address(thread, JavaThread::lock_stack_top_offset()), oopSize);\n+\n+    \/\/ Check if recursive.\n+    cmpptr(obj, Address(thread, top, Address::times_1, -2 * oopSize));\n+    jcc(Assembler::equal, unlocked);\n+\n+    \/\/ We elide the monitor check, let the CAS fail instead.\n+\n+    \/\/ Try to unlock. Transition lock bits 0b00 => 0b01\n+    movptr(reg_rax, mark);\n+    andptr(reg_rax, ~(int32_t)markWord::lock_mask);\n+    orptr(mark, markWord::unlocked_value);\n+    lock(); cmpxchgptr(mark, Address(obj, oopDesc::mark_offset_in_bytes()));\n+    jcc(Assembler::notEqual, push_and_slow_path);\n+    jmp(unlocked);\n+  }\n+\n+\n+  { \/\/ Handle inflated monitor.\n+    bind(inflated_check_lock_stack);\n+#ifdef ASSERT\n+    Label check_done;\n+    subl(top, oopSize);\n+    cmpl(top, in_bytes(JavaThread::lock_stack_base_offset()));\n+    jcc(Assembler::below, check_done);\n+    cmpptr(obj, Address(thread, top));\n+    jccb(Assembler::notEqual, inflated_check_lock_stack);\n+    stop(\"Fast Unlock lock on stack\");\n+    bind(check_done);\n+    testptr(mark, markWord::monitor_value);\n+    jccb(Assembler::notZero, inflated);\n+    stop(\"Fast Unlock not monitor\");\n+#endif\n+\n+    bind(inflated);\n+\n+    \/\/ mark contains the tagged ObjectMonitor*.\n+    const Register monitor = mark;\n+\n+#ifndef _LP64\n+    \/\/ Check if recursive.\n+    xorptr(reg_rax, reg_rax);\n+    orptr(reg_rax, Address(monitor, OM_OFFSET_NO_MONITOR_VALUE_TAG(recursions)));\n+    jcc(Assembler::notZero, check_successor);\n+\n+    \/\/ Check if the entry lists are empty.\n+    movptr(reg_rax, Address(monitor, OM_OFFSET_NO_MONITOR_VALUE_TAG(EntryList)));\n+    orptr(reg_rax, Address(monitor, OM_OFFSET_NO_MONITOR_VALUE_TAG(cxq)));\n+    jcc(Assembler::notZero, check_successor);\n+\n+    \/\/ Release lock.\n+    movptr(Address(monitor, OM_OFFSET_NO_MONITOR_VALUE_TAG(owner)), NULL_WORD);\n+#else \/\/ _LP64\n+    Label recursive;\n+\n+    \/\/ Check if recursive.\n+    cmpptr(Address(monitor, OM_OFFSET_NO_MONITOR_VALUE_TAG(recursions)), 0);\n+    jccb(Assembler::notEqual, recursive);\n+\n+    \/\/ Check if the entry lists are empty.\n+    movptr(reg_rax, Address(monitor, OM_OFFSET_NO_MONITOR_VALUE_TAG(cxq)));\n+    orptr(reg_rax, Address(monitor, OM_OFFSET_NO_MONITOR_VALUE_TAG(EntryList)));\n+    jcc(Assembler::notZero, check_successor);\n+\n+    \/\/ Release lock.\n+    movptr(Address(monitor, OM_OFFSET_NO_MONITOR_VALUE_TAG(owner)), NULL_WORD);\n+    jmpb(unlocked);\n+\n+    \/\/ Recursive unlock.\n+    bind(recursive);\n+    decrement(Address(monitor, OM_OFFSET_NO_MONITOR_VALUE_TAG(recursions)));\n+    xorl(t, t);\n+#endif\n+  }\n+\n+  bind(unlocked);\n+  if (stub != nullptr) {\n+    bind(stub->unlocked_continuation());\n+  }\n+\n+#ifdef ASSERT\n+  \/\/ Check that unlocked label is reached with ZF set.\n+  Label zf_correct;\n+  jccb(Assembler::zero, zf_correct);\n+  stop(\"Fast Unlock ZF != 1\");\n+#endif\n+\n+  if (stub != nullptr) {\n+    bind(stub->slow_path_continuation());\n+  }\n+#ifdef ASSERT\n+  \/\/ Check that stub->continuation() label is reached with ZF not set.\n+  jccb(Assembler::notZero, zf_correct);\n+  stop(\"Fast Unlock ZF != 0\");\n+  bind(zf_correct);\n+#endif\n+  \/\/ C2 uses the value of ZF to determine the continuation.\n+}\n+\n","filename":"src\/hotspot\/cpu\/x86\/c2_MacroAssembler_x86.cpp","additions":250,"deletions":32,"binary":false,"changes":282,"status":"modified"},{"patch":"@@ -46,0 +46,4 @@\n+  void fast_lock_lightweight(Register obj, Register box, Register rax_reg,\n+                             Register t, Register thread);\n+  void fast_unlock_lightweight(Register obj, Register reg_rax, Register t, Register thread);\n+\n","filename":"src\/hotspot\/cpu\/x86\/c2_MacroAssembler_x86.hpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -1205,2 +1205,0 @@\n-      \/\/ Load object header, prepare for CAS from unlocked to locked.\n-      movptr(swap_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n@@ -1333,1 +1331,1 @@\n-      const Register thread = r15_thread;\n+      lightweight_unlock(obj_reg, swap_reg, r15_thread, header_reg, slow_case);\n@@ -1335,2 +1333,4 @@\n-      const Register thread = header_reg;\n-      get_thread(thread);\n+      \/\/ This relies on the implementation of lightweight_unlock being able to handle\n+      \/\/ that the reg_rax and thread Register parameters may alias each other.\n+      get_thread(swap_reg);\n+      lightweight_unlock(obj_reg, swap_reg, swap_reg, header_reg, slow_case);\n@@ -1338,9 +1338,0 @@\n-      \/\/ Handle unstructured locking.\n-      Register tmp = swap_reg;\n-      movl(tmp, Address(thread, JavaThread::lock_stack_top_offset()));\n-      cmpptr(obj_reg, Address(thread, tmp, Address::times_1,  -oopSize));\n-      jcc(Assembler::notEqual, slow_case);\n-      \/\/ Try to swing header from locked to unlocked.\n-      movptr(swap_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n-      andptr(swap_reg, ~(int32_t)markWord::lock_mask_in_place);\n-      lightweight_unlock(obj_reg, swap_reg, header_reg, slow_case);\n","filename":"src\/hotspot\/cpu\/x86\/interp_masm_x86.cpp","additions":6,"deletions":15,"binary":false,"changes":21,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -28,0 +28,1 @@\n+#include \"code\/compiledIC.hpp\"\n@@ -1344,1 +1345,1 @@\n-  mov64(rax, (intptr_t)Universe::non_oop_word());\n+  mov64(rax, (int64_t)Universe::non_oop_word());\n@@ -1351,0 +1352,32 @@\n+int MacroAssembler::ic_check_size() {\n+  return LP64_ONLY(14) NOT_LP64(12);\n+}\n+\n+int MacroAssembler::ic_check(int end_alignment) {\n+  Register receiver = LP64_ONLY(j_rarg0) NOT_LP64(rcx);\n+  Register data = rax;\n+  Register temp = LP64_ONLY(rscratch1) NOT_LP64(rbx);\n+\n+  \/\/ The UEP of a code blob ensures that the VEP is padded. However, the padding of the UEP is placed\n+  \/\/ before the inline cache check, so we don't have to execute any nop instructions when dispatching\n+  \/\/ through the UEP, yet we can ensure that the VEP is aligned appropriately. That's why we align\n+  \/\/ before the inline cache check here, and not after\n+  align(end_alignment, offset() + ic_check_size());\n+\n+  int uep_offset = offset();\n+\n+  if (UseCompressedClassPointers) {\n+    movl(temp, Address(receiver, oopDesc::klass_offset_in_bytes()));\n+    cmpl(temp, Address(data, CompiledICData::speculated_klass_offset()));\n+  } else {\n+    movptr(temp, Address(receiver, oopDesc::klass_offset_in_bytes()));\n+    cmpptr(temp, Address(data, CompiledICData::speculated_klass_offset()));\n+  }\n+\n+  \/\/ if inline cache check fails, then jump to runtime routine\n+  jump_cc(Assembler::notEqual, RuntimeAddress(SharedRuntime::get_ic_miss_stub()));\n+  assert((offset() % end_alignment) == 0, \"Misaligned verified entry point\");\n+\n+  return uep_offset;\n+}\n+\n@@ -4090,2 +4123,3 @@\n-int register_section_sizes(RegSet gp_registers, XMMRegSet xmm_registers, bool save_fpu,\n-                           int& gp_area_size, int& fp_area_size, int& xmm_area_size) {\n+static int register_section_sizes(RegSet gp_registers, XMMRegSet xmm_registers,\n+                                  bool save_fpu, int& gp_area_size,\n+                                  int& fp_area_size, int& xmm_area_size) {\n@@ -4357,1 +4391,1 @@\n-\/\/ - check recv_klass (actual object class) is a subtype of resolved_klass from CompiledICHolder\n+\/\/ - check recv_klass (actual object class) is a subtype of resolved_klass from CompiledICData\n@@ -9880,2 +9914,0 @@\n-\/\/ Branches to slow upon failure to lock the object, with ZF cleared.\n-\/\/ Falls through upon success with unspecified ZF.\n@@ -9884,1 +9916,1 @@\n-\/\/ hdr: the (pre-loaded) header of the object, must be rax\n+\/\/ reg_rax: rax\n@@ -9887,19 +9919,31 @@\n-void MacroAssembler::lightweight_lock(Register obj, Register hdr, Register thread, Register tmp, Label& slow) {\n-  assert(hdr == rax, \"header must be in rax for cmpxchg\");\n-  assert_different_registers(obj, hdr, thread, tmp);\n-\n-  \/\/ First we need to check if the lock-stack has room for pushing the object reference.\n-  \/\/ Note: we subtract 1 from the end-offset so that we can do a 'greater' comparison, instead\n-  \/\/ of 'greaterEqual' below, which readily clears the ZF. This makes C2 code a little simpler and\n-  \/\/ avoids one branch.\n-  cmpl(Address(thread, JavaThread::lock_stack_top_offset()), LockStack::end_offset() - 1);\n-  jcc(Assembler::greater, slow);\n-\n-  \/\/ Now we attempt to take the fast-lock.\n-  \/\/ Clear lock_mask bits (locked state).\n-  andptr(hdr, ~(int32_t)markWord::lock_mask_in_place);\n-  movptr(tmp, hdr);\n-  \/\/ Set unlocked_value bit.\n-  orptr(hdr, markWord::unlocked_value);\n-  lock();\n-  cmpxchgptr(tmp, Address(obj, oopDesc::mark_offset_in_bytes()));\n+void MacroAssembler::lightweight_lock(Register obj, Register reg_rax, Register thread, Register tmp, Label& slow) {\n+  assert(reg_rax == rax, \"\");\n+  assert_different_registers(obj, reg_rax, thread, tmp);\n+\n+  Label push;\n+  const Register top = tmp;\n+\n+  \/\/ Preload the markWord. It is important that this is the first\n+  \/\/ instruction emitted as it is part of C1's null check semantics.\n+  movptr(reg_rax, Address(obj, oopDesc::mark_offset_in_bytes()));\n+\n+  \/\/ Load top.\n+  movl(top, Address(thread, JavaThread::lock_stack_top_offset()));\n+\n+  \/\/ Check if the lock-stack is full.\n+  cmpl(top, LockStack::end_offset());\n+  jcc(Assembler::greaterEqual, slow);\n+\n+  \/\/ Check for recursion.\n+  cmpptr(obj, Address(thread, top, Address::times_1, -oopSize));\n+  jcc(Assembler::equal, push);\n+\n+  \/\/ Check header for monitor (0b10).\n+  testptr(reg_rax, markWord::monitor_value);\n+  jcc(Assembler::notZero, slow);\n+\n+  \/\/ Try to lock. Transition lock bits 0b01 => 0b00\n+  movptr(tmp, reg_rax);\n+  andptr(tmp, ~(int32_t)markWord::unlocked_value);\n+  orptr(reg_rax, markWord::unlocked_value);\n+  lock(); cmpxchgptr(tmp, Address(obj, oopDesc::mark_offset_in_bytes()));\n@@ -9908,5 +9952,8 @@\n-  \/\/ If successful, push object to lock-stack.\n-  movl(tmp, Address(thread, JavaThread::lock_stack_top_offset()));\n-  movptr(Address(thread, tmp), obj);\n-  incrementl(tmp, oopSize);\n-  movl(Address(thread, JavaThread::lock_stack_top_offset()), tmp);\n+  \/\/ Restore top, CAS clobbers register.\n+  movl(top, Address(thread, JavaThread::lock_stack_top_offset()));\n+\n+  bind(push);\n+  \/\/ After successful lock, push object on lock-stack.\n+  movptr(Address(thread, top), obj);\n+  incrementl(top, oopSize);\n+  movl(Address(thread, JavaThread::lock_stack_top_offset()), top);\n@@ -9916,2 +9963,0 @@\n-\/\/ Branches to slow upon failure, with ZF cleared.\n-\/\/ Falls through upon success, with unspecified ZF.\n@@ -9920,1 +9965,2 @@\n-\/\/ hdr: the (pre-loaded) header of the object, must be rax\n+\/\/ reg_rax: rax\n+\/\/ thread: the thread\n@@ -9922,3 +9968,7 @@\n-void MacroAssembler::lightweight_unlock(Register obj, Register hdr, Register tmp, Label& slow) {\n-  assert(hdr == rax, \"header must be in rax for cmpxchg\");\n-  assert_different_registers(obj, hdr, tmp);\n+\/\/\n+\/\/ x86_32 Note: reg_rax and thread may alias each other due to limited register\n+\/\/              availiability.\n+void MacroAssembler::lightweight_unlock(Register obj, Register reg_rax, Register thread, Register tmp, Label& slow) {\n+  assert(reg_rax == rax, \"\");\n+  assert_different_registers(obj, reg_rax, tmp);\n+  LP64_ONLY(assert_different_registers(obj, reg_rax, thread, tmp);)\n@@ -9926,5 +9976,6 @@\n-  \/\/ Mark-word must be lock_mask now, try to swing it back to unlocked_value.\n-  movptr(tmp, hdr); \/\/ The expected old value\n-  orptr(tmp, markWord::unlocked_value);\n-  lock();\n-  cmpxchgptr(tmp, Address(obj, oopDesc::mark_offset_in_bytes()));\n+  Label unlocked, push_and_slow;\n+  const Register top = tmp;\n+\n+  \/\/ Check if obj is top of lock-stack.\n+  movl(top, Address(thread, JavaThread::lock_stack_top_offset()));\n+  cmpptr(obj, Address(thread, top, Address::times_1, -oopSize));\n@@ -9932,7 +9983,3 @@\n-  \/\/ Pop the lock object from the lock-stack.\n-#ifdef _LP64\n-  const Register thread = r15_thread;\n-#else\n-  const Register thread = rax;\n-  get_thread(thread);\n-#endif\n+\n+  \/\/ Pop lock-stack.\n+  DEBUG_ONLY(movptr(Address(thread, top, Address::times_1, -oopSize), 0);)\n@@ -9940,0 +9987,31 @@\n+\n+  \/\/ Check if recursive.\n+  cmpptr(obj, Address(thread, top, Address::times_1, -2 * oopSize));\n+  jcc(Assembler::equal, unlocked);\n+\n+  \/\/ Not recursive. Check header for monitor (0b10).\n+  movptr(reg_rax, Address(obj, oopDesc::mark_offset_in_bytes()));\n+  testptr(reg_rax, markWord::monitor_value);\n+  jcc(Assembler::notZero, push_and_slow);\n+\n+#ifdef ASSERT\n+  \/\/ Check header not unlocked (0b01).\n+  Label not_unlocked;\n+  testptr(reg_rax, markWord::unlocked_value);\n+  jcc(Assembler::zero, not_unlocked);\n+  stop(\"lightweight_unlock already unlocked\");\n+  bind(not_unlocked);\n+#endif\n+\n+  \/\/ Try to unlock. Transition lock bits 0b00 => 0b01\n+  movptr(tmp, reg_rax);\n+  orptr(tmp, markWord::unlocked_value);\n+  lock(); cmpxchgptr(tmp, Address(obj, oopDesc::mark_offset_in_bytes()));\n+  jcc(Assembler::equal, unlocked);\n+\n+  bind(push_and_slow);\n+  \/\/ Restore lock-stack and handle the unlock in runtime.\n+  if (thread == reg_rax) {\n+    \/\/ On x86_32 we may lose the thread.\n+    get_thread(thread);\n+  }\n@@ -9941,2 +10019,2 @@\n-  movl(tmp, Address(thread, JavaThread::lock_stack_top_offset()));\n-  movptr(Address(thread, tmp), 0);\n+  movl(top, Address(thread, JavaThread::lock_stack_top_offset()));\n+  movptr(Address(thread, top), obj);\n@@ -9944,0 +10022,4 @@\n+  addl(Address(thread, JavaThread::lock_stack_top_offset()), oopSize);\n+  jmp(slow);\n+\n+  bind(unlocked);\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.cpp","additions":134,"deletions":52,"binary":false,"changes":186,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -899,0 +899,2 @@\n+  static int ic_check_size();\n+  int ic_check(int end_alignment);\n@@ -2034,2 +2036,2 @@\n-  void lightweight_lock(Register obj, Register hdr, Register thread, Register tmp, Label& slow);\n-  void lightweight_unlock(Register obj, Register hdr, Register tmp, Label& slow);\n+  void lightweight_lock(Register obj, Register reg_rax, Register thread, Register tmp, Label& slow);\n+  void lightweight_unlock(Register obj, Register reg_rax, Register thread, Register tmp, Label& slow);\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.hpp","additions":5,"deletions":3,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2003, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2003, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -28,0 +28,1 @@\n+#include \"code\/compiledIC.hpp\"\n@@ -29,1 +30,0 @@\n-#include \"code\/icBuffer.hpp\"\n@@ -39,1 +39,0 @@\n-#include \"oops\/compiledICHolder.hpp\"\n@@ -949,1 +948,1 @@\n-  Register holder = rax;\n+  Register data = rax;\n@@ -954,6 +953,2 @@\n-\n-    Label missed;\n-    __ movptr(temp, Address(receiver, oopDesc::klass_offset_in_bytes()));\n-    __ cmpptr(temp, Address(holder, CompiledICHolder::holder_klass_offset()));\n-    __ movptr(rbx, Address(holder, CompiledICHolder::holder_metadata_offset()));\n-    __ jcc(Assembler::notEqual, missed);\n+    __ ic_check(1 \/* end_alignment *\/);\n+    __ movptr(rbx, Address(data, CompiledICData::speculated_method_offset()));\n@@ -965,3 +960,0 @@\n-\n-    __ bind(missed);\n-    __ jump(RuntimeAddress(SharedRuntime::get_ic_miss_stub()));\n@@ -1454,2 +1446,0 @@\n-  const Register ic_reg = rax;\n-  Label hit;\n@@ -1460,10 +1450,1 @@\n-  __ cmpptr(ic_reg, Address(receiver, oopDesc::klass_offset_in_bytes()));\n-  __ jcc(Assembler::equal, hit);\n-\n-  __ jump(RuntimeAddress(SharedRuntime::get_ic_miss_stub()));\n-\n-  \/\/ and the first 5 bytes must be in the same cache line\n-  \/\/ if we align at 8 then we will be sure 5 bytes are in the same line\n-  __ align(8);\n-\n-  __ bind(hit);\n+  __ ic_check(8 \/* end_alignment *\/);\n@@ -1721,2 +1702,0 @@\n-      \/\/ Load object header\n-      __ movptr(swap_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n@@ -1883,3 +1862,1 @@\n-      __ movptr(swap_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n-      __ andptr(swap_reg, ~(int32_t)markWord::lock_mask_in_place);\n-      __ lightweight_unlock(obj_reg, swap_reg, lock_reg, slow_path_unlock);\n+      __ lightweight_unlock(obj_reg, swap_reg, thread, lock_reg, slow_path_unlock);\n","filename":"src\/hotspot\/cpu\/x86\/sharedRuntime_x86_32.cpp","additions":7,"deletions":30,"binary":false,"changes":37,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2003, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2003, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -34,1 +34,0 @@\n-#include \"code\/icBuffer.hpp\"\n@@ -46,1 +45,0 @@\n-#include \"oops\/compiledICHolder.hpp\"\n@@ -1004,2 +1002,1 @@\n-  Label ok;\n-  Register holder = rax;\n+  Register data = rax;\n@@ -1011,7 +1008,2 @@\n-    __ load_klass(temp, receiver, rscratch1);\n-    __ cmpptr(temp, Address(holder, CompiledICHolder::holder_klass_offset()));\n-    __ movptr(rbx, Address(holder, CompiledICHolder::holder_metadata_offset()));\n-    __ jcc(Assembler::equal, ok);\n-    __ jump(RuntimeAddress(SharedRuntime::get_ic_miss_stub()));\n-\n-    __ bind(ok);\n+    __ ic_check(1 \/* end_alignment *\/);\n+    __ movptr(rbx, Address(data, CompiledICData::speculated_method_offset()));\n@@ -1454,1 +1446,1 @@\n-    address stub = CompiledStaticCall::emit_to_interp_stub(*cbuf, __ pc());\n+    address stub = CompiledDirectCall::emit_to_interp_stub(*cbuf, __ pc());\n@@ -1491,1 +1483,1 @@\n-  address stub = CompiledStaticCall::emit_to_interp_stub(*cbuf, __ pc());\n+  address stub = CompiledDirectCall::emit_to_interp_stub(*cbuf, __ pc());\n@@ -1748,0 +1740,1 @@\n+    if (nm == nullptr) return nm;\n@@ -1886,2 +1879,0 @@\n-\n-  const Register ic_reg = rax;\n@@ -1890,1 +1881,0 @@\n-  Label hit;\n@@ -1893,1 +1883,1 @@\n-  assert_different_registers(ic_reg, receiver, rscratch1, rscratch2);\n+  assert_different_registers(receiver, rscratch1, rscratch2);\n@@ -1895,10 +1885,1 @@\n-  __ load_klass(rscratch1, receiver, rscratch2);\n-  __ cmpq(ic_reg, rscratch1);\n-  __ jcc(Assembler::equal, hit);\n-\n-  __ jump(RuntimeAddress(SharedRuntime::get_ic_miss_stub()));\n-\n-  \/\/ Verified entry point must be aligned\n-  __ align(8);\n-\n-  __ bind(hit);\n+  __ ic_check(8 \/* end_alignment *\/);\n@@ -2192,2 +2173,0 @@\n-      \/\/ Load object header\n-      __ movptr(swap_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n@@ -2339,4 +2318,1 @@\n-      assert(LockingMode == LM_LIGHTWEIGHT, \"must be\");\n-      __ movptr(swap_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n-      __ andptr(swap_reg, ~(int32_t)markWord::lock_mask_in_place);\n-      __ lightweight_unlock(obj_reg, swap_reg, lock_reg, slow_path_unlock);\n+      __ lightweight_unlock(obj_reg, swap_reg, r15_thread, lock_reg, slow_path_unlock);\n","filename":"src\/hotspot\/cpu\/x86\/sharedRuntime_x86_64.cpp","additions":10,"deletions":34,"binary":false,"changes":44,"status":"modified"},{"patch":"@@ -507,1 +507,1 @@\n-void emit_cmpfp3(MacroAssembler& _masm, Register dst) {\n+static void emit_cmpfp3(MacroAssembler& _masm, Register dst) {\n@@ -617,1 +617,1 @@\n-  __ verified_entry(framesize, C->output()->need_stack_bang(bangsize)?bangsize:0, C->in_24_bit_fp_mode(), C->stub_function() != NULL);\n+  __ verified_entry(framesize, C->output()->need_stack_bang(bangsize)?bangsize:0, C->in_24_bit_fp_mode(), C->stub_function() != nullptr);\n@@ -1055,1 +1055,1 @@\n-  if (bottom_type()->isa_vect() != NULL && bottom_type()->isa_vectmask() == NULL) {\n+  if (bottom_type()->isa_vect() != nullptr && bottom_type()->isa_vectmask() == nullptr) {\n@@ -1323,1 +1323,1 @@\n-  implementation( NULL, ra_, false, st );\n+  implementation( nullptr, ra_, false, st );\n@@ -1328,1 +1328,1 @@\n-  implementation( &cbuf, ra_, false, NULL );\n+  implementation( &cbuf, ra_, false, nullptr );\n@@ -1386,14 +1386,1 @@\n-#ifdef ASSERT\n-  uint insts_size = cbuf.insts_size();\n-#endif\n-  masm.cmpptr(rax, Address(rcx, oopDesc::klass_offset_in_bytes()));\n-  masm.jump_cc(Assembler::notEqual,\n-               RuntimeAddress(SharedRuntime::get_ic_miss_stub()));\n-  \/* WARNING these NOPs are critical so that verified entry point is properly\n-     aligned for patching by NativeJump::patch_verified_entry() *\/\n-  int nops_cnt = 2;\n-  if( !OptoBreakpoint ) \/\/ Leave space for int3\n-     nops_cnt += 1;\n-  masm.nop(nops_cnt);\n-\n-  assert(cbuf.insts_size() - insts_size == size(ra_), \"checking code size of inline cache node\");\n+  masm.ic_check(CodeEntryAlignment);\n@@ -1403,1 +1390,2 @@\n-  return OptoBreakpoint ? 11 : 12;\n+  return MachNode::size(ra_); \/\/ too many variables; just compute it\n+                              \/\/ the hard way\n@@ -1728,1 +1716,1 @@\n-                                     NULL, &miss,\n+                                     nullptr, &miss,\n@@ -1845,2 +1833,2 @@\n-        address stub = CompiledStaticCall::emit_to_interp_stub(cbuf, mark);\n-        if (stub == NULL) {\n+        address stub = CompiledDirectCall::emit_to_interp_stub(cbuf, mark);\n+        if (stub == nullptr) {\n@@ -3399,1 +3387,1 @@\n-\/\/ NULL Pointer Immediate\n+\/\/ Null Pointer Immediate\n@@ -13779,1 +13767,1 @@\n-  predicate(LockingMode != LM_PLACEHOLDER && !Compile::current()->use_rtm());\n+  predicate(LockingMode != LM_LIGHTWEIGHT && LockingMode != LM_PLACEHOLDER && !Compile::current()->use_rtm());\n@@ -13793,1 +13781,1 @@\n-  predicate(LockingMode != LM_PLACEHOLDER);\n+  predicate(LockingMode != LM_LIGHTWEIGHT && LockingMode != LM_PLACEHOLDER);\n@@ -13804,0 +13792,26 @@\n+instruct cmpFastLockLightweight(eFlagsReg cr, eRegP object, eBXRegP box, eAXRegI eax_reg, eRegP tmp, eRegP thread) %{\n+  predicate(LockingMode == LM_LIGHTWEIGHT);\n+  match(Set cr (FastLock object box));\n+  effect(TEMP eax_reg, TEMP tmp, USE_KILL box, TEMP thread);\n+  ins_cost(300);\n+  format %{ \"FASTLOCK $object,$box\\t! kills $box,$eax_reg,$tmp\" %}\n+  ins_encode %{\n+    __ get_thread($thread$$Register);\n+    __ fast_lock_lightweight($object$$Register, $box$$Register, $eax_reg$$Register, $tmp$$Register, $thread$$Register);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct cmpFastUnlockLightweight(eFlagsReg cr, eRegP object, eAXRegP eax_reg, eRegP tmp, eRegP thread) %{\n+  predicate(LockingMode == LM_LIGHTWEIGHT);\n+  match(Set cr (FastUnlock object eax_reg));\n+  effect(TEMP tmp, USE_KILL eax_reg, TEMP thread);\n+  ins_cost(300);\n+  format %{ \"FASTUNLOCK $object,$eax_reg\\t! kills $eax_reg,$tmp\" %}\n+  ins_encode %{\n+    __ get_thread($thread$$Register);\n+    __ fast_unlock_lightweight($object$$Register, $eax_reg$$Register, $tmp$$Register, $thread$$Register);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n","filename":"src\/hotspot\/cpu\/x86\/x86_32.ad","additions":40,"deletions":26,"binary":false,"changes":66,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n-\/\/ Copyright (c) 2003, 2023, Oracle and\/or its affiliates. All rights reserved.\n+\/\/ Copyright (c) 2003, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -522,1 +522,1 @@\n-void emit_cmpfp_fixup(MacroAssembler& _masm) {\n+static void emit_cmpfp_fixup(MacroAssembler& _masm) {\n@@ -542,1 +542,1 @@\n-void emit_cmpfp3(MacroAssembler& _masm, Register dst) {\n+static void emit_cmpfp3(MacroAssembler& _masm, Register dst) {\n@@ -561,4 +561,4 @@\n-void emit_fp_min_max(MacroAssembler& _masm, XMMRegister dst,\n-                     XMMRegister a, XMMRegister b,\n-                     XMMRegister xmmt, Register rt,\n-                     bool min, bool single) {\n+static void emit_fp_min_max(MacroAssembler& _masm, XMMRegister dst,\n+                            XMMRegister a, XMMRegister b,\n+                            XMMRegister xmmt, Register rt,\n+                            bool min, bool single) {\n@@ -709,1 +709,1 @@\n-  if (C->stub_function() != NULL && BarrierSet::barrier_set()->barrier_set_nmethod() != NULL) {\n+  if (C->stub_function() != nullptr && BarrierSet::barrier_set()->barrier_set_nmethod() != nullptr) {\n@@ -744,1 +744,1 @@\n-  __ verified_entry(framesize, C->output()->need_stack_bang(bangsize)?bangsize:0, false, C->stub_function() != NULL);\n+  __ verified_entry(framesize, C->output()->need_stack_bang(bangsize)?bangsize:0, false, C->stub_function() != nullptr);\n@@ -973,1 +973,1 @@\n-  assert(cbuf != NULL || st  != NULL, \"sanity\");\n+  assert(cbuf != nullptr || st  != nullptr, \"sanity\");\n@@ -992,1 +992,1 @@\n-  if (bottom_type()->isa_vect() != NULL && bottom_type()->isa_vectmask() == NULL) {\n+  if (bottom_type()->isa_vect() != nullptr && bottom_type()->isa_vectmask() == nullptr) {\n@@ -1431,1 +1431,1 @@\n-  implementation(NULL, ra_, false, st);\n+  implementation(nullptr, ra_, false, st);\n@@ -1436,1 +1436,1 @@\n-  implementation(&cbuf, ra_, false, NULL);\n+  implementation(&cbuf, ra_, false, nullptr);\n@@ -1475,2 +1475,1 @@\n-    st->print_cr(\"\\tdecode_klass_not_null rscratch1, rscratch1\");\n-    st->print_cr(\"\\tcmpq    rax, rscratch1\\t # Inline cache check\");\n+    st->print_cr(\"\\tcmpl    rscratch1, [rax + CompiledICData::speculated_klass_offset()]\\t # Inline cache check\");\n@@ -1478,2 +1477,2 @@\n-    st->print_cr(\"\\tcmpq    rax, [j_rarg0 + oopDesc::klass_offset_in_bytes()]\\t\"\n-                 \"# Inline cache check\");\n+    st->print_cr(\"movq    rscratch1, [j_rarg0 + oopDesc::klass_offset_in_bytes()]\\t# compressed klass\");\n+    st->print_cr(\"\\tcmpq    rscratch1, [rax + CompiledICData::speculated_klass_offset()]\\t # Inline cache check\");\n@@ -1482,1 +1481,0 @@\n-  st->print_cr(\"\\tnop\\t# nops to align entry point\");\n@@ -1489,20 +1487,1 @@\n-  uint insts_size = cbuf.insts_size();\n-  if (UseCompressedClassPointers) {\n-    masm.load_klass(rscratch1, j_rarg0, rscratch2);\n-    masm.cmpptr(rax, rscratch1);\n-  } else {\n-    masm.cmpptr(rax, Address(j_rarg0, oopDesc::klass_offset_in_bytes()));\n-  }\n-\n-  masm.jump_cc(Assembler::notEqual, RuntimeAddress(SharedRuntime::get_ic_miss_stub()));\n-\n-  \/* WARNING these NOPs are critical so that verified entry point is properly\n-     4 bytes aligned for patching by NativeJump::patch_verified_entry() *\/\n-  int nops_cnt = 4 - ((cbuf.insts_size() - insts_size) & 0x3);\n-  if (OptoBreakpoint) {\n-    \/\/ Leave space for int3\n-    nops_cnt -= 1;\n-  }\n-  nops_cnt &= 0x3; \/\/ Do not add nops if code is aligned.\n-  if (nops_cnt > 0)\n-    masm.nop(nops_cnt);\n+  masm.ic_check(InteriorEntryAlignment);\n@@ -1787,1 +1766,1 @@\n-                                     NULL, &miss,\n+                                     nullptr, &miss,\n@@ -1843,2 +1822,2 @@\n-        address stub = CompiledStaticCall::emit_to_interp_stub(cbuf, mark);\n-        if (stub == NULL) {\n+        address stub = CompiledDirectCall::emit_to_interp_stub(cbuf, mark);\n+        if (stub == nullptr) {\n@@ -2182,1 +2161,1 @@\n-\/\/ NULL Pointer Immediate\n+\/\/ Null Pointer Immediate\n@@ -2210,1 +2189,1 @@\n-\/\/ NULL Pointer Immediate\n+\/\/ Null Pointer Immediate\n@@ -3124,1 +3103,1 @@\n-\/\/ we can't free r12 even with CompressedOops::base() == NULL.\n+\/\/ we can't free r12 even with CompressedOops::base() == nullptr.\n@@ -4905,1 +4884,1 @@\n-  format %{ \"xorq    $dst, $src\\t# compressed NULL ptr\" %}\n+  format %{ \"xorq    $dst, $src\\t# compressed null pointer\" %}\n@@ -4919,1 +4898,1 @@\n-    if (con == NULL) {\n+    if (con == nullptr) {\n@@ -4935,1 +4914,1 @@\n-    if (con == NULL) {\n+    if (con == nullptr) {\n@@ -5161,1 +5140,1 @@\n-  predicate(UseCompressedOops && (CompressedOops::base() == NULL) && n->as_Store()->barrier_data() == 0);\n+  predicate(UseCompressedOops && (CompressedOops::base() == nullptr) && n->as_Store()->barrier_data() == 0);\n@@ -5172,1 +5151,1 @@\n-\/\/ Store NULL Pointer, mark word, or other simple pointer constant.\n+\/\/ Store Null Pointer, mark word, or other simple pointer constant.\n@@ -5213,1 +5192,1 @@\n-  predicate(CompressedOops::base() == NULL);\n+  predicate(CompressedOops::base() == nullptr);\n@@ -5232,1 +5211,1 @@\n-    if (con == NULL) {\n+    if (con == nullptr) {\n@@ -5256,1 +5235,1 @@\n-  predicate(UseCompressedOops && (CompressedOops::base() == NULL));\n+  predicate(UseCompressedOops && (CompressedOops::base() == nullptr));\n@@ -5282,1 +5261,1 @@\n-  predicate(UseCompressedOops && (CompressedOops::base() == NULL));\n+  predicate(UseCompressedOops && (CompressedOops::base() == nullptr));\n@@ -5308,1 +5287,1 @@\n-  predicate(UseCompressedOops && (CompressedOops::base() == NULL));\n+  predicate(UseCompressedOops && (CompressedOops::base() == nullptr));\n@@ -5335,1 +5314,1 @@\n-  predicate(UseCompressedOops && (CompressedOops::base() == NULL));\n+  predicate(UseCompressedOops && (CompressedOops::base() == nullptr));\n@@ -5361,1 +5340,1 @@\n-  predicate(UseCompressedOops && (CompressedOops::base() == NULL));\n+  predicate(UseCompressedOops && (CompressedOops::base() == nullptr));\n@@ -5400,1 +5379,1 @@\n-  predicate(UseCompressedOops && (CompressedOops::base() == NULL));\n+  predicate(UseCompressedOops && (CompressedOops::base() == nullptr));\n@@ -5439,1 +5418,1 @@\n-  predicate(!UseCompressedOops || (CompressedOops::base() != NULL));\n+  predicate(!UseCompressedOops || (CompressedOops::base() != nullptr));\n@@ -5452,1 +5431,1 @@\n-  predicate(UseCompressedOops && (CompressedOops::base() == NULL));\n+  predicate(UseCompressedOops && (CompressedOops::base() == nullptr));\n@@ -11683,1 +11662,1 @@\n-  predicate((!UseCompressedOops || (CompressedOops::base() != NULL)) &&\n+  predicate((!UseCompressedOops || (CompressedOops::base() != nullptr)) &&\n@@ -11697,1 +11676,1 @@\n-  predicate(UseCompressedOops && (CompressedOops::base() == NULL) &&\n+  predicate(UseCompressedOops && (CompressedOops::base() == nullptr) &&\n@@ -11780,1 +11759,1 @@\n-  predicate(CompressedOops::base() != NULL);\n+  predicate(CompressedOops::base() != nullptr);\n@@ -11793,1 +11772,1 @@\n-  predicate(CompressedOops::base() == NULL);\n+  predicate(CompressedOops::base() == nullptr);\n@@ -12407,1 +12386,1 @@\n-  predicate(LockingMode != LM_PLACEHOLDER && !Compile::current()->use_rtm());\n+  predicate(LockingMode != LM_LIGHTWEIGHT && LockingMode != LM_PLACEHOLDER && !Compile::current()->use_rtm());\n@@ -12420,1 +12399,1 @@\n-  predicate(LockingMode != LM_PLACEHOLDER);\n+  predicate(LockingMode != LM_LIGHTWEIGHT && LockingMode != LM_PLACEHOLDER);\n@@ -12431,0 +12410,24 @@\n+instruct cmpFastLockLightweight(rFlagsReg cr, rRegP object, rbx_RegP box, rax_RegI rax_reg, rRegP tmp) %{\n+  predicate(LockingMode == LM_LIGHTWEIGHT);\n+  match(Set cr (FastLock object box));\n+  effect(TEMP rax_reg, TEMP tmp, USE_KILL box);\n+  ins_cost(300);\n+  format %{ \"fastlock $object,$box\\t! kills $box,$rax_reg,$tmp\" %}\n+  ins_encode %{\n+    __ fast_lock_lightweight($object$$Register, $box$$Register, $rax_reg$$Register, $tmp$$Register, r15_thread);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct cmpFastUnlockLightweight(rFlagsReg cr, rRegP object, rax_RegP rax_reg, rRegP tmp) %{\n+  predicate(LockingMode == LM_LIGHTWEIGHT);\n+  match(Set cr (FastUnlock object rax_reg));\n+  effect(TEMP tmp, USE_KILL rax_reg);\n+  ins_cost(300);\n+  format %{ \"fastunlock $object,$rax_reg\\t! kills $rax_reg,$tmp\" %}\n+  ins_encode %{\n+    __ fast_unlock_lightweight($object$$Register, $rax_reg$$Register, $tmp$$Register, r15_thread);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n","filename":"src\/hotspot\/cpu\/x86\/x86_64.ad","additions":68,"deletions":65,"binary":false,"changes":133,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2022, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -100,0 +100,20 @@\n+class C2FastUnlockLightweightStub : public C2CodeStub {\n+private:\n+  Register _obj;\n+  Register _mark;\n+  Register _t;\n+  Register _thread;\n+  Label _push_and_slow_path;\n+  Label _check_successor;\n+  Label _unlocked_continuation;\n+public:\n+  C2FastUnlockLightweightStub(Register obj, Register mark, Register t, Register thread) : C2CodeStub(),\n+    _obj(obj), _mark(mark), _t(t), _thread(thread) {}\n+  int max_size() const;\n+  void emit(C2_MacroAssembler& masm);\n+  Label& push_and_slow_path() { return _push_and_slow_path; }\n+  Label& check_successor() { return _check_successor; }\n+  Label& unlocked_continuation() { return _unlocked_continuation; }\n+  Label& slow_path_continuation() { return continuation(); }\n+};\n+\n","filename":"src\/hotspot\/share\/opto\/c2_CodeStubs.hpp","additions":21,"deletions":1,"binary":false,"changes":22,"status":"modified"},{"patch":"@@ -314,1 +314,0 @@\n-  case vmIntrinsics::_equalsU:                  return inline_string_equals(StrIntrinsicNode::UU);\n","filename":"src\/hotspot\/share\/opto\/library_call.cpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -497,3 +497,0 @@\n-  { \"MaxRAMFraction\",               JDK_Version::jdk(10),  JDK_Version::undefined(), JDK_Version::undefined() },\n-  { \"MinRAMFraction\",               JDK_Version::jdk(10),  JDK_Version::undefined(), JDK_Version::undefined() },\n-  { \"InitialRAMFraction\",           JDK_Version::jdk(10),  JDK_Version::undefined(), JDK_Version::undefined() },\n@@ -509,2 +506,0 @@\n-  { \"DefaultMaxRAMFraction\",        JDK_Version::jdk(8),  JDK_Version::undefined(), JDK_Version::undefined() },\n-  { \"TLABStats\",                    JDK_Version::jdk(12), JDK_Version::undefined(), JDK_Version::undefined() },\n@@ -529,0 +524,5 @@\n+  { \"MaxRAMFraction\",               JDK_Version::jdk(10),  JDK_Version::jdk(23), JDK_Version::jdk(24) },\n+  { \"MinRAMFraction\",               JDK_Version::jdk(10),  JDK_Version::jdk(23), JDK_Version::jdk(24) },\n+  { \"InitialRAMFraction\",           JDK_Version::jdk(10),  JDK_Version::jdk(23), JDK_Version::jdk(24) },\n+  { \"DefaultMaxRAMFraction\",        JDK_Version::jdk(8),  JDK_Version::jdk(23), JDK_Version::jdk(24) },\n+  { \"TLABStats\",                    JDK_Version::jdk(12), JDK_Version::jdk(23), JDK_Version::jdk(24) },\n@@ -554,1 +554,0 @@\n-  { \"DefaultMaxRAMFraction\",    \"MaxRAMFraction\"    },\n@@ -1123,12 +1122,1 @@\n-    if (strlen(locked_message_buf) == 0) {\n-      if (found_flag->is_bool() && !has_plus_minus) {\n-        jio_fprintf(defaultStream::error_stream(),\n-          \"Missing +\/- setting for VM option '%s'\\n\", argname);\n-      } else if (!found_flag->is_bool() && has_plus_minus) {\n-        jio_fprintf(defaultStream::error_stream(),\n-          \"Unexpected +\/- setting in VM option '%s'\\n\", argname);\n-      } else {\n-        jio_fprintf(defaultStream::error_stream(),\n-          \"Improperly specified VM option '%s'\\n\", argname);\n-      }\n-    } else {\n+    if (strlen(locked_message_buf) != 0) {\n@@ -1144,0 +1132,10 @@\n+    if (found_flag->is_bool() && !has_plus_minus) {\n+      jio_fprintf(defaultStream::error_stream(),\n+        \"Missing +\/- setting for VM option '%s'\\n\", argname);\n+    } else if (!found_flag->is_bool() && has_plus_minus) {\n+      jio_fprintf(defaultStream::error_stream(),\n+        \"Unexpected +\/- setting in VM option '%s'\\n\", argname);\n+    } else {\n+      jio_fprintf(defaultStream::error_stream(),\n+        \"Improperly specified VM option '%s'\\n\", argname);\n+    }\n@@ -1369,1 +1367,1 @@\n-void set_object_alignment() {\n+static void set_object_alignment() {\n@@ -1482,3 +1480,0 @@\n-                           !FLAG_IS_DEFAULT(MaxRAMFraction) ||\n-                           !FLAG_IS_DEFAULT(MinRAMFraction) ||\n-                           !FLAG_IS_DEFAULT(InitialRAMFraction) ||\n@@ -1500,14 +1495,0 @@\n-\n-  \/\/ Convert deprecated flags\n-  if (FLAG_IS_DEFAULT(MaxRAMPercentage) &&\n-      !FLAG_IS_DEFAULT(MaxRAMFraction))\n-    MaxRAMPercentage = 100.0 \/ (double)MaxRAMFraction;\n-\n-  if (FLAG_IS_DEFAULT(MinRAMPercentage) &&\n-      !FLAG_IS_DEFAULT(MinRAMFraction))\n-    MinRAMPercentage = 100.0 \/ (double)MinRAMFraction;\n-\n-  if (FLAG_IS_DEFAULT(InitialRAMPercentage) &&\n-      !FLAG_IS_DEFAULT(InitialRAMFraction))\n-    InitialRAMPercentage = 100.0 \/ (double)InitialRAMFraction;\n-\n@@ -2038,0 +2019,1 @@\n+#if !INCLUDE_JVMTI\n@@ -2042,1 +2024,1 @@\n-bool valid_jdwp_agent(char *name, bool is_path) {\n+static bool valid_jdwp_agent(char *name, bool is_path) {\n@@ -2082,0 +2064,1 @@\n+#endif\n","filename":"src\/hotspot\/share\/runtime\/arguments.cpp","additions":21,"deletions":38,"binary":false,"changes":59,"status":"modified"},{"patch":"@@ -397,1 +397,2 @@\n-  for (int i = 0; i < chunk->length(); i++) {\n+  \/\/ Start locking from outermost\/oldest frame\n+  for (int i = (chunk->length() - 1); i >= 0; i--) {\n@@ -1453,1 +1454,1 @@\n-int compare(ReassignedField* left, ReassignedField* right) {\n+static int compare(ReassignedField* left, ReassignedField* right) {\n@@ -1652,1 +1653,1 @@\n-          ObjectSynchronizer::enter(obj, nullptr, deoptee_thread, thread);\n+          ObjectSynchronizer::enter_for(obj, nullptr, deoptee_thread);\n@@ -1654,1 +1655,1 @@\n-          ObjectMonitor* mon = ObjectSynchronizer::inflate(deoptee_thread, obj(), ObjectSynchronizer::inflate_cause_vm_internal);\n+          ObjectMonitor* mon = ObjectSynchronizer::inflate_for(deoptee_thread, obj(), ObjectSynchronizer::inflate_cause_vm_internal);\n@@ -1660,1 +1661,1 @@\n-          ObjectSynchronizer::enter(obj, lock, deoptee_thread, thread);\n+          ObjectSynchronizer::enter_for(obj, lock, deoptee_thread);\n@@ -1670,1 +1671,1 @@\n-          ObjectSynchronizer::enter(obj, lock, deoptee_thread, thread);\n+          ObjectSynchronizer::enter_for(obj, lock, deoptee_thread);\n@@ -1743,1 +1744,2 @@\n-      for (int j = 0; j < monitors->number_of_monitors(); j++) {\n+      \/\/ Unlock in reverse order starting from most nested monitor.\n+      for (int j = (monitors->number_of_monitors() - 1); j >= 0; j--) {\n","filename":"src\/hotspot\/share\/runtime\/deoptimization.cpp","additions":9,"deletions":7,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -883,3 +883,0 @@\n-  develop(bool, TraceInlineCacheClearing, false,                            \\\n-          \"Trace clearing of inline caches in nmethods\")                    \\\n-                                                                            \\\n@@ -904,6 +901,0 @@\n-  develop(bool, TraceICBuffer, false,                                       \\\n-          \"Trace usage of IC buffer\")                                       \\\n-                                                                            \\\n-  develop(bool, TraceCompiledIC, false,                                     \\\n-          \"Trace changes of compiled IC\")                                   \\\n-                                                                            \\\n","filename":"src\/hotspot\/share\/runtime\/globals.hpp","additions":0,"deletions":9,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -267,1 +267,1 @@\n-bool jvmci_counters_include(JavaThread* thread) {\n+static bool jvmci_counters_include(JavaThread* thread) {\n@@ -286,1 +286,1 @@\n-jlong* resize_counters_array(jlong* old_counters, int current_size, int new_size) {\n+static jlong* resize_counters_array(jlong* old_counters, int current_size, int new_size) {\n@@ -750,0 +750,1 @@\n+  assert(java_lang_Thread::thread(threadObj()) == thread, \"must be alive\");\n@@ -1462,1 +1463,1 @@\n-const char* _get_thread_state_name(JavaThreadState _thread_state) {\n+static const char* _get_thread_state_name(JavaThreadState _thread_state) {\n@@ -2162,0 +2163,2 @@\n+\n+  assert(java_lang_Thread::thread(thread_oop()) == nullptr, \"must not be alive\");\n","filename":"src\/hotspot\/share\/runtime\/javaThread.cpp","additions":6,"deletions":3,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -4,0 +4,1 @@\n+ * Copyright (c) 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -75,1 +76,1 @@\n-  assert((_top >= start_offset()), \"lockstack underflow: _top %d end_offset %d\", _top, start_offset());\n+  assert((_top >= start_offset()), \"lockstack underflow: _top %d start_offset %d\", _top, start_offset());\n@@ -80,1 +81,1 @@\n-      if (LockingMode == LM_PLACEHOLDER) {\n+      if (VM_Version::supports_recursive_lightweight_locking()) {\n","filename":"src\/hotspot\/share\/runtime\/lockStack.cpp","additions":3,"deletions":2,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -4,0 +4,1 @@\n+ * Copyright (c) 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -110,0 +111,1 @@\n+  \/\/ Precondition: This lock-stack must not be full.\n","filename":"src\/hotspot\/share\/runtime\/lockStack.hpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -4,0 +4,1 @@\n+ * Copyright (c) 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -46,0 +47,2 @@\n+  assert((offset <= end_offset()), \"lockstack overflow: offset %d end_offset %d\", offset, end_offset());\n+  assert((offset >= start_offset()), \"lockstack underflow: offset %d start_offset %d\", offset, start_offset());\n@@ -99,1 +102,1 @@\n-  if (LockingMode != LM_PLACEHOLDER) {\n+  if (!VM_Version::supports_recursive_lightweight_locking()) {\n@@ -102,0 +105,1 @@\n+  verify(\"pre-is_recursive\");\n@@ -103,1 +107,4 @@\n-  assert(contains(o), \"entries must exist\");\n+  \/\/ This will succeed iff there is a consecutive run of oops on the\n+  \/\/ lock-stack with a length of at least 2.\n+\n+  assert(contains(o), \"at least one entry must exist\");\n@@ -105,1 +112,5 @@\n-  for (int i = 1; i < end; i++) {\n+  \/\/ Start iterating from the top because the runtime code is more\n+  \/\/ interested in the balanced locking case when the top oop on the\n+  \/\/ lock-stack matches o. This will cause the for loop to break out\n+  \/\/ in the first loop iteration if it is non-recursive.\n+  for (int i = end - 1; i > 0; i--) {\n@@ -107,0 +118,1 @@\n+      verify(\"post-is_recursive\");\n@@ -109,0 +121,6 @@\n+    if (_base[i] == o) {\n+      \/\/ o can only occur in one consecutive run on the lock-stack.\n+      \/\/ Only one of the two oops checked matched o, so this run\n+      \/\/ must be of length 1 and thus not be recursive. Stop the search.\n+      break;\n+    }\n@@ -111,0 +129,1 @@\n+  verify(\"post-is_recursive\");\n@@ -115,1 +134,1 @@\n-  if (LockingMode != LM_PLACEHOLDER) {\n+  if (!VM_Version::supports_recursive_lightweight_locking()) {\n@@ -118,0 +137,6 @@\n+  verify(\"pre-try_recursive_enter\");\n+\n+  \/\/ This will succeed iff the top oop on the stack matches o.\n+  \/\/ When successful o will be pushed to the lock-stack creating\n+  \/\/ a consecutive run at least 2 oops that matches o on top of\n+  \/\/ the lock-stack.\n@@ -124,0 +149,1 @@\n+    verify(\"post-try_recursive_enter\");\n@@ -129,0 +155,1 @@\n+  verify(\"post-try_recursive_enter\");\n@@ -133,1 +160,1 @@\n-  if (LockingMode != LM_PLACEHOLDER) {\n+  if (!VM_Version::supports_recursive_lightweight_locking()) {\n@@ -136,0 +163,6 @@\n+  verify(\"pre-try_recursive_exit\");\n+\n+  \/\/ This will succeed iff the top two oops on the stack matches o.\n+  \/\/ When successful the top oop will be popped of the lock-stack.\n+  \/\/ When unsuccessful the lock may still be recursive, in which\n+  \/\/ case the locking is unbalanced. This case is handled externally.\n@@ -140,1 +173,1 @@\n-  if (end <= 1 || _base[end - 1] != o ||  _base[end - 2] != o) {\n+  if (end <= 1 || _base[end - 1] != o || _base[end - 2] != o) {\n@@ -142,0 +175,1 @@\n+    verify(\"post-try_recursive_exit\");\n@@ -147,0 +181,1 @@\n+  verify(\"post-try_recursive_exit\");\n","filename":"src\/hotspot\/share\/runtime\/lockStack.inline.hpp","additions":41,"deletions":6,"binary":false,"changes":47,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1998, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1998, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -336,0 +336,62 @@\n+bool ObjectMonitor::enter_for(JavaThread* locking_thread) {\n+  \/\/ Used by ObjectSynchronizer::enter_for to enter for another thread.\n+  \/\/ The monitor is private to or already owned by locking_thread which must be suspended.\n+  \/\/ So this code may only contend with deflation.\n+  assert(locking_thread == Thread::current() || locking_thread->is_obj_deopt_suspend(), \"must be\");\n+\n+  \/\/ Block out deflation as soon as possible.\n+  add_to_contentions(1);\n+\n+  bool success = false;\n+  if (!is_being_async_deflated()) {\n+    void* prev_owner = try_set_owner_from(nullptr, locking_thread);\n+\n+    if (prev_owner == nullptr) {\n+      assert(_recursions == 0, \"invariant\");\n+      success = true;\n+    } else if (prev_owner == locking_thread) {\n+      _recursions++;\n+      success = true;\n+    } else if (prev_owner == DEFLATER_MARKER) {\n+      \/\/ Racing with deflation.\n+      prev_owner = try_set_owner_from(DEFLATER_MARKER, locking_thread);\n+      if (prev_owner == DEFLATER_MARKER) {\n+        \/\/ Cancelled deflation. Increment contentions as part of the deflation protocol.\n+        add_to_contentions(1);\n+        success = true;\n+      } else if (prev_owner == nullptr) {\n+        \/\/ At this point we cannot race with deflation as we have both incremented\n+        \/\/ contentions, seen contention > 0 and seen a DEFLATER_MARKER.\n+        \/\/ success will only be false if this races with something other than\n+        \/\/ deflation.\n+        prev_owner = try_set_owner_from(nullptr, locking_thread);\n+        success = prev_owner == nullptr;\n+      }\n+    } else if (LockingMode == LM_LEGACY && locking_thread->is_lock_owned((address)prev_owner)) {\n+      assert(_recursions == 0, \"must be\");\n+      _recursions = 1;\n+      set_owner_from_BasicLock(prev_owner, locking_thread);\n+      success = true;\n+    }\n+    assert(success, \"Failed to enter_for: locking_thread=\" INTPTR_FORMAT\n+           \", this=\" INTPTR_FORMAT \"{owner=\" INTPTR_FORMAT \"}, observed owner: \" INTPTR_FORMAT,\n+           p2i(locking_thread), p2i(this), p2i(owner_raw()), p2i(prev_owner));\n+  } else {\n+    \/\/ Async deflation is in progress and our contentions increment\n+    \/\/ above lost the race to async deflation. Undo the work and\n+    \/\/ force the caller to retry.\n+    const oop l_object = object();\n+    if (l_object != nullptr) {\n+      \/\/ Attempt to restore the header\/dmw to the object's header so that\n+      \/\/ we only retry once if the deflater thread happens to be slow.\n+      install_displaced_markword_in_object(l_object);\n+    }\n+  }\n+\n+  add_to_contentions(-1);\n+\n+  assert(!success || owner_raw() == locking_thread, \"must be\");\n+\n+  return success;\n+}\n+\n@@ -337,0 +399,1 @@\n+  assert(current == JavaThread::current(), \"must be\");\n","filename":"src\/hotspot\/share\/runtime\/objectMonitor.cpp","additions":64,"deletions":1,"binary":false,"changes":65,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1998, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1998, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -358,0 +358,1 @@\n+  bool      enter_for(JavaThread* locking_thread);\n","filename":"src\/hotspot\/share\/runtime\/objectMonitor.hpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1998, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1998, 2024, Oracle and\/or its affiliates. All rights reserved.\n","filename":"src\/hotspot\/share\/runtime\/objectMonitor.inline.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -35,0 +35,1 @@\n+#include \"runtime\/javaThread.hpp\"\n@@ -524,0 +525,4 @@\n+void PlaceholderSynchronizer::enter_for(Handle obj, BasicLock* lock, JavaThread* locking_thread) {\n+  enter(obj, lock, locking_thread, JavaThread::current());\n+}\n+\n@@ -528,0 +533,6 @@\n+  if (obj->klass()->is_value_based()) {\n+    ObjectSynchronizer::handle_sync_on_value_based_class(obj, locking_thread);\n+  }\n+\n+  locking_thread->inc_held_monitor_count();\n+\n","filename":"src\/hotspot\/share\/runtime\/placeholderSynchronizer.cpp","additions":11,"deletions":0,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -56,0 +56,1 @@\n+  static void enter_for(Handle obj, BasicLock* lock, JavaThread* locking_thread);\n","filename":"src\/hotspot\/share\/runtime\/placeholderSynchronizer.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -30,1 +30,0 @@\n-#include \"code\/icBuffer.hpp\"\n@@ -516,1 +515,0 @@\n-  if (!InlineCacheBuffer::is_empty()) return true;\n@@ -561,4 +559,0 @@\n-    if (InlineCacheBuffer::needs_update_inline_caches()) {\n-      workers++;\n-    }\n-\n@@ -602,5 +596,0 @@\n-    if (_subtasks.try_claim_task(SafepointSynchronize::SAFEPOINT_CLEANUP_UPDATE_INLINE_CACHES)) {\n-      Tracer t(\"updating inline caches\");\n-      InlineCacheBuffer::update_inline_caches();\n-    }\n-\n@@ -636,2 +625,0 @@\n-  assert(InlineCacheBuffer::is_empty(), \"should have cleaned up ICBuffer\");\n-\n","filename":"src\/hotspot\/share\/runtime\/safepoint.cpp","additions":0,"deletions":13,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -32,1 +32,0 @@\n-#include \"code\/icBuffer.hpp\"\n@@ -50,1 +49,0 @@\n-#include \"oops\/compiledICHolder.inline.hpp\"\n@@ -1293,118 +1291,0 @@\n-  methodHandle callee_method;\n-  callee_method = resolve_sub_helper(is_virtual, is_optimized, THREAD);\n-  if (JvmtiExport::can_hotswap_or_post_breakpoint()) {\n-    int retry_count = 0;\n-    while (!HAS_PENDING_EXCEPTION && callee_method->is_old() &&\n-           callee_method->method_holder() != vmClasses::Object_klass()) {\n-      \/\/ If has a pending exception then there is no need to re-try to\n-      \/\/ resolve this method.\n-      \/\/ If the method has been redefined, we need to try again.\n-      \/\/ Hack: we have no way to update the vtables of arrays, so don't\n-      \/\/ require that java.lang.Object has been updated.\n-\n-      \/\/ It is very unlikely that method is redefined more than 100 times\n-      \/\/ in the middle of resolve. If it is looping here more than 100 times\n-      \/\/ means then there could be a bug here.\n-      guarantee((retry_count++ < 100),\n-                \"Could not resolve to latest version of redefined method\");\n-      \/\/ method is redefined in the middle of resolve so re-try.\n-      callee_method = resolve_sub_helper(is_virtual, is_optimized, THREAD);\n-    }\n-  }\n-  return callee_method;\n-}\n-\n-\/\/ This fails if resolution required refilling of IC stubs\n-bool SharedRuntime::resolve_sub_helper_internal(methodHandle callee_method, const frame& caller_frame,\n-                                                CompiledMethod* caller_nm, bool is_virtual, bool is_optimized,\n-                                                Handle receiver, CallInfo& call_info, Bytecodes::Code invoke_code, TRAPS) {\n-  StaticCallInfo static_call_info;\n-  CompiledICInfo virtual_call_info;\n-\n-  \/\/ Make sure the callee nmethod does not get deoptimized and removed before\n-  \/\/ we are done patching the code.\n-  CompiledMethod* callee = callee_method->code();\n-\n-  if (callee != nullptr) {\n-    assert(callee->is_compiled(), \"must be nmethod for patching\");\n-  }\n-\n-  if (callee != nullptr && !callee->is_in_use()) {\n-    \/\/ Patch call site to C2I adapter if callee nmethod is deoptimized or unloaded.\n-    callee = nullptr;\n-  }\n-#ifdef ASSERT\n-  address dest_entry_point = callee == nullptr ? 0 : callee->entry_point(); \/\/ used below\n-#endif\n-\n-  bool is_nmethod = caller_nm->is_nmethod();\n-\n-  if (is_virtual) {\n-    assert(receiver.not_null() || invoke_code == Bytecodes::_invokehandle, \"sanity check\");\n-    bool static_bound = call_info.resolved_method()->can_be_statically_bound();\n-    Klass* klass = invoke_code == Bytecodes::_invokehandle ? nullptr : receiver->klass();\n-    CompiledIC::compute_monomorphic_entry(callee_method, klass,\n-                     is_optimized, static_bound, is_nmethod, virtual_call_info,\n-                     CHECK_false);\n-  } else {\n-    \/\/ static call\n-    CompiledStaticCall::compute_entry(callee_method, is_nmethod, static_call_info);\n-  }\n-\n-  JFR_ONLY(bool patched_caller = false;)\n-  \/\/ grab lock, check for deoptimization and potentially patch caller\n-  {\n-    CompiledICLocker ml(caller_nm);\n-\n-    \/\/ Lock blocks for safepoint during which both nmethods can change state.\n-\n-    \/\/ Now that we are ready to patch if the Method* was redefined then\n-    \/\/ don't update call site and let the caller retry.\n-    \/\/ Don't update call site if callee nmethod was unloaded or deoptimized.\n-    \/\/ Don't update call site if callee nmethod was replaced by an other nmethod\n-    \/\/ which may happen when multiply alive nmethod (tiered compilation)\n-    \/\/ will be supported.\n-    if (!callee_method->is_old() &&\n-        (callee == nullptr || (callee->is_in_use() && callee_method->code() == callee))) {\n-      NoSafepointVerifier nsv;\n-#ifdef ASSERT\n-      \/\/ We must not try to patch to jump to an already unloaded method.\n-      if (dest_entry_point != 0) {\n-        CodeBlob* cb = CodeCache::find_blob(dest_entry_point);\n-        assert((cb != nullptr) && cb->is_compiled() && (((CompiledMethod*)cb) == callee),\n-               \"should not call unloaded nmethod\");\n-      }\n-#endif\n-      if (is_virtual) {\n-        CompiledIC* inline_cache = CompiledIC_before(caller_nm, caller_frame.pc());\n-        if (inline_cache->is_clean()) {\n-          if (!inline_cache->set_to_monomorphic(virtual_call_info)) {\n-            return false;\n-          }\n-          JFR_ONLY(patched_caller = true;)\n-        }\n-      } else {\n-        if (VM_Version::supports_fast_class_init_checks() &&\n-            invoke_code == Bytecodes::_invokestatic &&\n-            callee_method->needs_clinit_barrier() &&\n-            callee != nullptr && callee->is_compiled_by_jvmci()) {\n-          return true; \/\/ skip patching for JVMCI\n-        }\n-        CompiledStaticCall* ssc = caller_nm->compiledStaticCall_before(caller_frame.pc());\n-        if (is_nmethod && caller_nm->method()->is_continuation_enter_intrinsic()) {\n-          ssc->compute_entry_for_continuation_entry(callee_method, static_call_info);\n-        }\n-        if (ssc->is_clean()) {\n-          ssc->set(static_call_info);\n-          JFR_ONLY(patched_caller = true;)\n-        }\n-      }\n-    }\n-  } \/\/ unlock CompiledICLocker\n-  JFR_ONLY(if (patched_caller) Jfr::on_backpatching(callee_method(), THREAD);)\n-  return true;\n-}\n-\n-\/\/ Resolves a call.  The compilers generate code for calls that go here\n-\/\/ and are patched with the real destination of the call.\n-methodHandle SharedRuntime::resolve_sub_helper(bool is_virtual, bool is_optimized, TRAPS) {\n@@ -1421,1 +1301,1 @@\n-  CompiledMethod* caller_nm = caller_cb->as_compiled_method_or_null();\n+  CompiledMethod* caller_nm = caller_cb->as_compiled_method();\n@@ -1429,0 +1309,3 @@\n+\n+  NoSafepointVerifier nsv;\n+\n@@ -1473,0 +1356,1 @@\n+\n@@ -1477,24 +1361,16 @@\n-  \/\/ TODO detune for now but we might need it again\n-\/\/  assert(!callee_method->is_compiled_lambda_form() ||\n-\/\/         caller_nm->is_method_handle_return(caller_frame.pc()), \"must be MH call site\");\n-\n-  \/\/ Compute entry points. This might require generation of C2I converter\n-  \/\/ frames, so we cannot be holding any locks here. Furthermore, the\n-  \/\/ computation of the entry points is independent of patching the call.  We\n-  \/\/ always return the entry-point, but we only patch the stub if the call has\n-  \/\/ not been deoptimized.  Return values: For a virtual call this is an\n-  \/\/ (cached_oop, destination address) pair. For a static call\/optimized\n-  \/\/ virtual this is just a destination address.\n-\n-  \/\/ Patching IC caches may fail if we run out if transition stubs.\n-  \/\/ We refill the ic stubs then and try again.\n-  for (;;) {\n-    ICRefillVerifier ic_refill_verifier;\n-    bool successful = resolve_sub_helper_internal(callee_method, caller_frame, caller_nm,\n-                                                  is_virtual, is_optimized, receiver,\n-                                                  call_info, invoke_code, CHECK_(methodHandle()));\n-    if (successful) {\n-      return callee_method;\n-    } else {\n-      InlineCacheBuffer::refill_ic_stubs();\n-    }\n+\n+  \/\/ Compute entry points. The computation of the entry points is independent of\n+  \/\/ patching the call.\n+\n+  \/\/ Make sure the callee nmethod does not get deoptimized and removed before\n+  \/\/ we are done patching the code.\n+\n+\n+  CompiledICLocker ml(caller_nm);\n+  if (is_virtual && !is_optimized) {\n+    CompiledIC* inline_cache = CompiledIC_before(caller_nm, caller_frame.pc());\n+    inline_cache->update(&call_info, receiver->klass());\n+  } else {\n+    \/\/ Callsite is a direct call - set it to the destination method\n+    CompiledDirectCall* callsite = CompiledDirectCall::before(caller_frame.pc());\n+    callsite->set(callee_method);\n@@ -1503,0 +1379,1 @@\n+  return callee_method;\n@@ -1505,1 +1382,0 @@\n-\n@@ -1683,80 +1559,0 @@\n-\/\/ The handle_ic_miss_helper_internal function returns false if it failed due\n-\/\/ to either running out of vtable stubs or ic stubs due to IC transitions\n-\/\/ to transitional states. The needs_ic_stub_refill value will be set if\n-\/\/ the failure was due to running out of IC stubs, in which case handle_ic_miss_helper\n-\/\/ refills the IC stubs and tries again.\n-bool SharedRuntime::handle_ic_miss_helper_internal(Handle receiver, CompiledMethod* caller_nm,\n-                                                   const frame& caller_frame, methodHandle callee_method,\n-                                                   Bytecodes::Code bc, CallInfo& call_info,\n-                                                   bool& needs_ic_stub_refill, TRAPS) {\n-  CompiledICLocker ml(caller_nm);\n-  CompiledIC* inline_cache = CompiledIC_before(caller_nm, caller_frame.pc());\n-  bool should_be_mono = false;\n-  if (inline_cache->is_optimized()) {\n-    if (TraceCallFixup) {\n-      ResourceMark rm(THREAD);\n-      tty->print(\"OPTIMIZED IC miss (%s) call to\", Bytecodes::name(bc));\n-      callee_method->print_short_name(tty);\n-      tty->print_cr(\" code: \" INTPTR_FORMAT, p2i(callee_method->code()));\n-    }\n-    should_be_mono = true;\n-  } else if (inline_cache->is_icholder_call()) {\n-    CompiledICHolder* ic_oop = inline_cache->cached_icholder();\n-    if (ic_oop != nullptr) {\n-      if (!ic_oop->is_loader_alive()) {\n-        \/\/ Deferred IC cleaning due to concurrent class unloading\n-        if (!inline_cache->set_to_clean()) {\n-          needs_ic_stub_refill = true;\n-          return false;\n-        }\n-      } else if (receiver()->klass() == ic_oop->holder_klass()) {\n-        \/\/ This isn't a real miss. We must have seen that compiled code\n-        \/\/ is now available and we want the call site converted to a\n-        \/\/ monomorphic compiled call site.\n-        \/\/ We can't assert for callee_method->code() != nullptr because it\n-        \/\/ could have been deoptimized in the meantime\n-        if (TraceCallFixup) {\n-          ResourceMark rm(THREAD);\n-          tty->print(\"FALSE IC miss (%s) converting to compiled call to\", Bytecodes::name(bc));\n-          callee_method->print_short_name(tty);\n-          tty->print_cr(\" code: \" INTPTR_FORMAT, p2i(callee_method->code()));\n-        }\n-        should_be_mono = true;\n-      }\n-    }\n-  }\n-\n-  if (should_be_mono) {\n-    \/\/ We have a path that was monomorphic but was going interpreted\n-    \/\/ and now we have (or had) a compiled entry. We correct the IC\n-    \/\/ by using a new icBuffer.\n-    CompiledICInfo info;\n-    Klass* receiver_klass = receiver()->klass();\n-    inline_cache->compute_monomorphic_entry(callee_method,\n-                                            receiver_klass,\n-                                            inline_cache->is_optimized(),\n-                                            false, caller_nm->is_nmethod(),\n-                                            info, CHECK_false);\n-    if (!inline_cache->set_to_monomorphic(info)) {\n-      needs_ic_stub_refill = true;\n-      return false;\n-    }\n-  } else if (!inline_cache->is_megamorphic() && !inline_cache->is_clean()) {\n-    \/\/ Potential change to megamorphic\n-\n-    bool successful = inline_cache->set_to_megamorphic(&call_info, bc, needs_ic_stub_refill, CHECK_false);\n-    if (needs_ic_stub_refill) {\n-      return false;\n-    }\n-    if (!successful) {\n-      if (!inline_cache->set_to_clean()) {\n-        needs_ic_stub_refill = true;\n-        return false;\n-      }\n-    }\n-  } else {\n-    \/\/ Either clean or megamorphic\n-  }\n-  return true;\n-}\n-\n@@ -1772,26 +1568,0 @@\n-  \/\/ Compiler1 can produce virtual call sites that can actually be statically bound\n-  \/\/ If we fell thru to below we would think that the site was going megamorphic\n-  \/\/ when in fact the site can never miss. Worse because we'd think it was megamorphic\n-  \/\/ we'd try and do a vtable dispatch however methods that can be statically bound\n-  \/\/ don't have vtable entries (vtable_index < 0) and we'd blow up. So we force a\n-  \/\/ reresolution of the  call site (as if we did a handle_wrong_method and not an\n-  \/\/ plain ic_miss) and the site will be converted to an optimized virtual call site\n-  \/\/ never to miss again. I don't believe C2 will produce code like this but if it\n-  \/\/ did this would still be the correct thing to do for it too, hence no ifdef.\n-  \/\/\n-  if (call_info.resolved_method()->can_be_statically_bound()) {\n-    methodHandle callee_method = SharedRuntime::reresolve_call_site(CHECK_(methodHandle()));\n-    if (TraceCallFixup) {\n-      RegisterMap reg_map(current,\n-                          RegisterMap::UpdateMap::skip,\n-                          RegisterMap::ProcessFrames::include,\n-                          RegisterMap::WalkContinuation::skip);\n-      frame caller_frame = current->last_frame().sender(&reg_map);\n-      ResourceMark rm(current);\n-      tty->print(\"converting IC miss to reresolve (%s) call to\", Bytecodes::name(bc));\n-      callee_method->print_short_name(tty);\n-      tty->print_cr(\" from pc: \" INTPTR_FORMAT, p2i(caller_frame.pc()));\n-      tty->print_cr(\" code: \" INTPTR_FORMAT, p2i(callee_method->code()));\n-    }\n-    return callee_method;\n-  }\n@@ -1832,3 +1602,0 @@\n-  \/\/ Transitioning IC caches may require transition stubs. If we run out\n-  \/\/ of transition stubs, we have to drop locks and perform a safepoint\n-  \/\/ that refills them.\n@@ -1843,27 +1610,4 @@\n-  for (;;) {\n-    ICRefillVerifier ic_refill_verifier;\n-    bool needs_ic_stub_refill = false;\n-    bool successful = handle_ic_miss_helper_internal(receiver, caller_nm, caller_frame, callee_method,\n-                                                     bc, call_info, needs_ic_stub_refill, CHECK_(methodHandle()));\n-    if (successful || !needs_ic_stub_refill) {\n-      return callee_method;\n-    } else {\n-      InlineCacheBuffer::refill_ic_stubs();\n-    }\n-  }\n-}\n-\n-static bool clear_ic_at_addr(CompiledMethod* caller_nm, address call_addr, bool is_static_call) {\n-  if (is_static_call) {\n-    CompiledStaticCall* ssc = caller_nm->compiledStaticCall_at(call_addr);\n-    if (!ssc->is_clean()) {\n-      return ssc->set_to_clean();\n-    }\n-  } else {\n-    \/\/ compiled, dispatched call (which used to call an interpreted method)\n-    CompiledIC* inline_cache = CompiledIC_at(caller_nm, call_addr);\n-    if (!inline_cache->is_clean()) {\n-      return inline_cache->set_to_clean();\n-    }\n-  }\n-  return true;\n+  CompiledIC* inline_cache = CompiledIC_before(caller_nm, caller_frame.pc());\n+  inline_cache->update(&call_info, receiver()->klass());\n+\n+  return callee_method;\n@@ -1899,2 +1643,0 @@\n-    \/\/ Check for static or virtual call\n-    bool is_static_call = false;\n@@ -1921,8 +1663,2 @@\n-    address call_addr = nullptr;\n-    {\n-      \/\/ Get call instruction under lock because another thread may be\n-      \/\/ busy patching it.\n-      CompiledICLocker ml(caller_nm);\n-      \/\/ Location of call instruction\n-      call_addr = caller_nm->call_instruction_address(pc);\n-    }\n+    CompiledICLocker ml(caller_nm);\n+    address call_addr = caller_nm->call_instruction_address(pc);\n@@ -1930,2 +1666,0 @@\n-    \/\/ Check relocations for the matching call to 1) avoid false positives,\n-    \/\/ and 2) determine the type.\n@@ -1938,1 +1672,0 @@\n-        bool is_static_call = false;\n@@ -1941,19 +1674,10 @@\n-            is_static_call = true;\n-\n-          case relocInfo::virtual_call_type:\n-          case relocInfo::opt_virtual_call_type:\n-            \/\/ Cleaning the inline cache will force a new resolve. This is more robust\n-            \/\/ than directly setting it to the new destination, since resolving of calls\n-            \/\/ is always done through the same code path. (experience shows that it\n-            \/\/ leads to very hard to track down bugs, if an inline cache gets updated\n-            \/\/ to a wrong method). It should not be performance critical, since the\n-            \/\/ resolve is only done once.\n-            guarantee(iter.addr() == call_addr, \"must find call\");\n-            for (;;) {\n-              ICRefillVerifier ic_refill_verifier;\n-              if (!clear_ic_at_addr(caller_nm, call_addr, is_static_call)) {\n-                InlineCacheBuffer::refill_ic_stubs();\n-              } else {\n-                break;\n-              }\n-            }\n+          case relocInfo::opt_virtual_call_type: {\n+            CompiledDirectCall* cdc = CompiledDirectCall::at(call_addr);\n+            cdc->set_to_clean();\n+            break;\n+          }\n+\n+          case relocInfo::virtual_call_type: {\n+            \/\/ compiled, dispatched call (which used to call an interpreted method)\n+            CompiledIC* inline_cache = CompiledIC_at(caller_nm, call_addr);\n+            inline_cache->set_to_clean();\n@@ -1961,0 +1685,1 @@\n+          }\n@@ -2024,31 +1749,0 @@\n-bool SharedRuntime::should_fixup_call_destination(address destination, address entry_point, address caller_pc, Method* moop, CodeBlob* cb) {\n-  if (destination != entry_point) {\n-    CodeBlob* callee = CodeCache::find_blob(destination);\n-    \/\/ callee == cb seems weird. It means calling interpreter thru stub.\n-    if (callee != nullptr && (callee == cb || callee->is_adapter_blob())) {\n-      \/\/ static call or optimized virtual\n-      if (TraceCallFixup) {\n-        tty->print(\"fixup callsite           at \" INTPTR_FORMAT \" to compiled code for\", p2i(caller_pc));\n-        moop->print_short_name(tty);\n-        tty->print_cr(\" to \" INTPTR_FORMAT, p2i(entry_point));\n-      }\n-      return true;\n-    } else {\n-      if (TraceCallFixup) {\n-        tty->print(\"failed to fixup callsite at \" INTPTR_FORMAT \" to compiled code for\", p2i(caller_pc));\n-        moop->print_short_name(tty);\n-        tty->print_cr(\" to \" INTPTR_FORMAT, p2i(entry_point));\n-      }\n-      \/\/ assert is too strong could also be resolve destinations.\n-      \/\/ assert(InlineCacheBuffer::contains(destination) || VtableStubs::contains(destination), \"must be\");\n-    }\n-  } else {\n-    if (TraceCallFixup) {\n-      tty->print(\"already patched callsite at \" INTPTR_FORMAT \" to compiled code for\", p2i(caller_pc));\n-      moop->print_short_name(tty);\n-      tty->print_cr(\" to \" INTPTR_FORMAT, p2i(entry_point));\n-    }\n-  }\n-  return false;\n-}\n-\n@@ -2062,2 +1756,0 @@\n-  Method* moop(method);\n-\n@@ -2079,1 +1771,1 @@\n-  CompiledMethod* callee = moop->code();\n+  CompiledMethod* callee = method->code();\n@@ -2088,1 +1780,1 @@\n-  if (cb == nullptr || !cb->is_compiled() || callee->is_unloading()) {\n+  if (cb == nullptr || !cb->is_compiled() || !callee->is_in_use() || callee->is_unloading()) {\n@@ -2093,2 +1785,1 @@\n-  CompiledMethod* nm = cb->as_compiled_method_or_null();\n-  assert(nm, \"must be\");\n+  CompiledMethod* caller = cb->as_compiled_method();\n@@ -2099,47 +1790,15 @@\n-  \/\/ There is a benign race here. We could be attempting to patch to a compiled\n-  \/\/ entry point at the same time the callee is being deoptimized. If that is\n-  \/\/ the case then entry_point may in fact point to a c2i and we'd patch the\n-  \/\/ call site with the same old data. clear_code will set code() to null\n-  \/\/ at the end of it. If we happen to see that null then we can skip trying\n-  \/\/ to patch. If we hit the window where the callee has a c2i in the\n-  \/\/ from_compiled_entry and the null isn't present yet then we lose the race\n-  \/\/ and patch the code with the same old data. Asi es la vida.\n-\n-  if (moop->code() == nullptr) return;\n-\n-  if (nm->is_in_use()) {\n-    \/\/ Expect to find a native call there (unless it was no-inline cache vtable dispatch)\n-    CompiledICLocker ic_locker(nm);\n-    if (NativeCall::is_call_before(return_pc)) {\n-      ResourceMark mark;\n-      NativeCallWrapper* call = nm->call_wrapper_before(return_pc);\n-      \/\/\n-      \/\/ bug 6281185. We might get here after resolving a call site to a vanilla\n-      \/\/ virtual call. Because the resolvee uses the verified entry it may then\n-      \/\/ see compiled code and attempt to patch the site by calling us. This would\n-      \/\/ then incorrectly convert the call site to optimized and its downhill from\n-      \/\/ there. If you're lucky you'll get the assert in the bugid, if not you've\n-      \/\/ just made a call site that could be megamorphic into a monomorphic site\n-      \/\/ for the rest of its life! Just another racing bug in the life of\n-      \/\/ fixup_callers_callsite ...\n-      \/\/\n-      RelocIterator iter(nm, call->instruction_address(), call->next_instruction_address());\n-      iter.next();\n-      assert(iter.has_current(), \"must have a reloc at java call site\");\n-      relocInfo::relocType typ = iter.reloc()->type();\n-      if (typ != relocInfo::static_call_type &&\n-           typ != relocInfo::opt_virtual_call_type &&\n-           typ != relocInfo::static_stub_type) {\n-        return;\n-      }\n-      if (nm->method()->is_continuation_enter_intrinsic()) {\n-        if (ContinuationEntry::is_interpreted_call(call->instruction_address())) {\n-          return;\n-        }\n-      }\n-      address destination = call->destination();\n-      address entry_point = callee->verified_entry_point();\n-      if (should_fixup_call_destination(destination, entry_point, caller_pc, moop, cb)) {\n-        call->set_destination_mt_safe(entry_point);\n-      }\n-    }\n+  if (!caller->is_in_use() || !NativeCall::is_call_before(return_pc)) {\n+    return;\n+  }\n+\n+  \/\/ Expect to find a native call there (unless it was no-inline cache vtable dispatch)\n+  CompiledICLocker ic_locker(caller);\n+  ResourceMark rm;\n+\n+  \/\/ If we got here through a static call or opt_virtual call, then we know where the\n+  \/\/ call address would be; let's peek at it\n+  address callsite_addr = (address)nativeCall_before(return_pc);\n+  RelocIterator iter(caller, callsite_addr, callsite_addr + 1);\n+  if (!iter.next()) {\n+    \/\/ No reloc entry found; not a static or optimized virtual call\n+    return;\n@@ -2147,0 +1806,9 @@\n+\n+  relocInfo::relocType type = iter.reloc()->type();\n+  if (type != relocInfo::static_call_type &&\n+      type != relocInfo::opt_virtual_call_type) {\n+    return;\n+  }\n+\n+  CompiledDirectCall* callsite = CompiledDirectCall::before(return_pc);\n+  callsite->set_to_clean();\n@@ -3414,2 +3082,2 @@\n-      }\n-    }\n+          }\n+        }\n","filename":"src\/hotspot\/share\/runtime\/sharedRuntime.cpp","additions":68,"deletions":400,"binary":false,"changes":468,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1998, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1998, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -267,1 +267,1 @@\n-int dtrace_waited_probe(ObjectMonitor* monitor, Handle obj, JavaThread* thr) {\n+static int dtrace_waited_probe(ObjectMonitor* monitor, Handle obj, JavaThread* thr) {\n@@ -421,0 +421,13 @@\n+  if (LockingMode == LM_LIGHTWEIGHT) {\n+    LockStack& lock_stack = current->lock_stack();\n+    if (lock_stack.is_full()) {\n+      \/\/ Always go into runtime if the lock stack is full.\n+      return false;\n+    }\n+    if (lock_stack.try_recursive_enter(obj)) {\n+      \/\/ Recursive lock successful.\n+      current->inc_held_monitor_count();\n+      return true;\n+    }\n+  }\n+\n@@ -474,2 +487,3 @@\n-void ObjectSynchronizer::handle_sync_on_value_based_class(Handle obj, JavaThread* current) {\n-  frame last_frame = current->last_frame();\n+void ObjectSynchronizer::handle_sync_on_value_based_class(Handle obj, JavaThread* locking_thread) {\n+  assert(locking_thread == Thread::current() || locking_thread->is_obj_deopt_suspend(), \"must be\");\n+  frame last_frame = locking_thread->last_frame();\n@@ -488,1 +502,1 @@\n-    ResourceMark rm(current);\n+    ResourceMark rm;\n@@ -490,1 +504,1 @@\n-    current->print_active_stack_on(&ss);\n+    locking_thread->print_active_stack_on(&ss);\n@@ -499,1 +513,1 @@\n-    ResourceMark rm(current);\n+    ResourceMark rm;\n@@ -503,1 +517,1 @@\n-    if (current->has_last_Java_frame()) {\n+    if (locking_thread->has_last_Java_frame()) {\n@@ -505,1 +519,1 @@\n-      current->print_active_stack_on(&info_stream);\n+      locking_thread->print_active_stack_on(&info_stream);\n@@ -532,0 +546,49 @@\n+\n+void ObjectSynchronizer::enter_for(Handle obj, BasicLock* lock, JavaThread* locking_thread) {\n+  \/\/ When called with locking_thread != Thread::current() some mechanism must synchronize\n+  \/\/ the locking_thread with respect to the current thread. Currently only used when\n+  \/\/ deoptimizing and re-locking locks. See Deoptimization::relock_objects\n+  assert(locking_thread == Thread::current() || locking_thread->is_obj_deopt_suspend(), \"must be\");\n+\n+  if (LockingMode == LM_PLACEHOLDER) {\n+    return PlaceholderSynchronizer::enter_for(obj, lock, locking_thread);\n+  }\n+\n+  if (!enter_fast_impl(obj, lock, locking_thread)) {\n+    \/\/ Inflated ObjectMonitor::enter_for is required\n+\n+    \/\/ An async deflation can race after the inflate_for() call and before\n+    \/\/ enter_for() can make the ObjectMonitor busy. enter_for() returns false\n+    \/\/ if we have lost the race to async deflation and we simply try again.\n+    while (true) {\n+      ObjectMonitor* monitor = inflate_for(locking_thread, obj(), inflate_cause_monitor_enter);\n+      if (monitor->enter_for(locking_thread)) {\n+        return;\n+      }\n+      assert(monitor->is_being_async_deflated(), \"must be\");\n+    }\n+  }\n+}\n+\n+void ObjectSynchronizer::enter(Handle obj, BasicLock* lock, JavaThread* current) {\n+  assert(current == Thread::current(), \"must be\");\n+\n+  if (LockingMode == LM_PLACEHOLDER) {\n+    return PlaceholderSynchronizer::enter(obj, lock, current, current);\n+  }\n+\n+  if (!enter_fast_impl(obj, lock, current)) {\n+    \/\/ Inflated ObjectMonitor::enter is required\n+\n+    \/\/ An async deflation can race after the inflate() call and before\n+    \/\/ enter() can make the ObjectMonitor busy. enter() returns false if\n+    \/\/ we have lost the race to async deflation and we simply try again.\n+    while (true) {\n+      ObjectMonitor* monitor = inflate(current, obj(), inflate_cause_monitor_enter);\n+      if (monitor->enter(current)) {\n+        return;\n+      }\n+    }\n+  }\n+}\n+\n@@ -535,0 +598,1 @@\n+bool ObjectSynchronizer::enter_fast_impl(Handle obj, BasicLock* lock, JavaThread* locking_thread) {\n@@ -536,2 +600,0 @@\n-void ObjectSynchronizer::enter(Handle obj, BasicLock* lock, JavaThread* locking_thread,  JavaThread* current) {\n-    \/\/ ResourceMarks are wrong inside when locking_thread != current (relock)\n@@ -544,4 +606,0 @@\n-  if (LockingMode == LM_PLACEHOLDER) {\n-    return PlaceholderSynchronizer::enter(obj, lock, locking_thread, current);\n-  }\n-\n@@ -552,14 +610,40 @@\n-      if (lock_stack.can_push()) {\n-        markWord mark = obj()->mark_acquire();\n-        while (mark.is_neutral()) {\n-          \/\/ Retry until a lock state change has been observed.  cas_set_mark() may collide with non lock bits modifications.\n-          \/\/ Try to swing into 'fast-locked' state.\n-          assert(!lock_stack.contains(obj()), \"thread must not already hold the lock\");\n-          const markWord locked_mark = mark.set_fast_locked();\n-          const markWord old_mark = obj()->cas_set_mark(locked_mark, mark);\n-          if (old_mark == mark) {\n-            \/\/ Successfully fast-locked, push object to lock-stack and return.\n-            lock_stack.push(obj());\n-            return;\n-          }\n-          mark = old_mark;\n+      if (lock_stack.is_full()) {\n+        \/\/ We unconditionally make room on the lock stack by inflating\n+        \/\/ the least recently locked object on the lock stack.\n+\n+        \/\/ About the choice to inflate least recently locked object.\n+        \/\/ First we must chose to inflate a lock, either some lock on\n+        \/\/ the lock-stack or the lock that is currently being entered\n+        \/\/ (which may or may not be on the lock-stack).\n+        \/\/ Second the best lock to inflate is a lock which is entered\n+        \/\/ in a control flow where there are only a very few locks being\n+        \/\/ used, as the costly part of inflated locking is inflation,\n+        \/\/ not locking. But this property is entirely program dependent.\n+        \/\/ Third inflating the lock currently being entered on when it\n+        \/\/ is not present on the lock-stack will result in a still full\n+        \/\/ lock-stack. This creates a scenario where every deeper nested\n+        \/\/ monitorenter must call into the runtime.\n+        \/\/ The rational here is as follows:\n+        \/\/ Because we cannot (currently) figure out the second, and want\n+        \/\/ to avoid the third, we inflate a lock on the lock-stack.\n+        \/\/ The least recently locked lock is chosen as it is the lock\n+        \/\/ with the longest critical section.\n+\n+        log_info(monitorinflation)(\"LockStack capacity exceeded, inflating.\");\n+        ObjectMonitor* monitor = inflate_for(locking_thread, lock_stack.bottom(), inflate_cause_vm_internal);\n+        assert(monitor->owner() == Thread::current(), \"must be owner=\" PTR_FORMAT \" current=\" PTR_FORMAT \" mark=\" PTR_FORMAT,\n+               p2i(monitor->owner()), p2i(Thread::current()), monitor->object()->mark_acquire().value());\n+        assert(!lock_stack.is_full(), \"must have made room here\");\n+      }\n+\n+      markWord mark = obj()->mark_acquire();\n+      while (mark.is_neutral()) {\n+        \/\/ Retry until a lock state change has been observed. cas_set_mark() may collide with non lock bits modifications.\n+        \/\/ Try to swing into 'fast-locked' state.\n+        assert(!lock_stack.contains(obj()), \"thread must not already hold the lock\");\n+        const markWord locked_mark = mark.set_fast_locked();\n+        const markWord old_mark = obj()->cas_set_mark(locked_mark, mark);\n+        if (old_mark == mark) {\n+          \/\/ Successfully fast-locked, push object to lock-stack and return.\n+          lock_stack.push(obj());\n+          return true;\n@@ -567,0 +651,6 @@\n+        mark = old_mark;\n+      }\n+\n+      if (mark.is_fast_locked() && lock_stack.try_recursive_enter(obj())) {\n+        \/\/ Recursive lock successful.\n+        return true;\n@@ -568,1 +658,3 @@\n-      \/\/ All other paths fall-through to inflate-enter.\n+\n+      \/\/ Failed to fast lock.\n+      return false;\n@@ -576,1 +668,1 @@\n-          return;\n+          return true;\n@@ -578,1 +670,0 @@\n-        \/\/ Fall through to inflate() ...\n@@ -584,1 +675,1 @@\n-        return;\n+        return true;\n@@ -592,0 +683,3 @@\n+\n+      \/\/ Failed to fast lock.\n+      return false;\n@@ -597,9 +691,1 @@\n-  \/\/ An async deflation can race after the inflate() call and before\n-  \/\/ enter() can make the ObjectMonitor busy. enter() returns false if\n-  \/\/ we have lost the race to async deflation and we simply try again.\n-  while (true) {\n-    ObjectMonitor* monitor = inflate(locking_thread, obj(), inflate_cause_monitor_enter);\n-    if (monitor->enter(locking_thread)) {\n-      return;\n-    }\n-  }\n+  return false;\n@@ -619,7 +705,21 @@\n-      while (mark.is_fast_locked()) {\n-        \/\/ Retry until a lock state change has been observed.  cas_set_mark() may collide with non lock bits modifications.\n-        const markWord unlocked_mark = mark.set_unlocked();\n-        const markWord old_mark = object->cas_set_mark(unlocked_mark, mark);\n-        if (old_mark == mark) {\n-          current->lock_stack().remove(object);\n-          return;\n+      LockStack& lock_stack = current->lock_stack();\n+      if (mark.is_fast_locked() && lock_stack.try_recursive_exit(object)) {\n+        \/\/ Recursively unlocked.\n+        return;\n+      }\n+\n+      if (mark.is_fast_locked() && lock_stack.is_recursive(object)) {\n+        \/\/ This lock is recursive but is not at the top of the lock stack so we're\n+        \/\/ doing an unbalanced exit. We have to fall thru to inflation below and\n+        \/\/ let ObjectMonitor::exit() do the unlock.\n+      } else {\n+        while (mark.is_fast_locked()) {\n+          \/\/ Retry until a lock state change has been observed. cas_set_mark() may collide with non lock bits modifications.\n+          const markWord unlocked_mark = mark.set_unlocked();\n+          const markWord old_mark = object->cas_set_mark(unlocked_mark, mark);\n+          if (old_mark == mark) {\n+            size_t recursions = lock_stack.remove(object) - 1;\n+            assert(recursions == 0, \"must not be recursive here\");\n+            return;\n+          }\n+          mark = old_mark;\n@@ -627,1 +727,0 @@\n-        mark = old_mark;\n@@ -1400,5 +1499,6 @@\n-\/\/ Can be called from non JavaThreads (e.g., VMThread) for FastHashCode\n-\/\/ calculations as part of JVM\/TI tagging.\n-static bool is_lock_owned(Thread* thread, oop obj) {\n-  assert(LockingMode == LM_LIGHTWEIGHT, \"only call this with new lightweight locking enabled\");\n-  return thread->is_Java_thread() ? JavaThread::cast(thread)->lock_stack().contains(obj) : false;\n+ObjectMonitor* ObjectSynchronizer::inflate(Thread* current, oop obj, const InflateCause cause) {\n+  assert(current == Thread::current(), \"must be\");\n+  if (LockingMode == LM_LIGHTWEIGHT && current->is_Java_thread()) {\n+    return inflate_impl(JavaThread::cast(current), obj, cause);\n+  }\n+  return inflate_impl(nullptr, obj, cause);\n@@ -1407,3 +1507,4 @@\n-ObjectMonitor* ObjectSynchronizer::inflate(Thread* current, oop object,\n-                                           const InflateCause cause) {\n-  assert(LockingMode != LM_PLACEHOLDER, \"placeholder does not use inflate\");\n+ObjectMonitor* ObjectSynchronizer::inflate_for(JavaThread* thread, oop obj, const InflateCause cause) {\n+  assert(thread == Thread::current() || thread->is_obj_deopt_suspend(), \"must be\");\n+  return inflate_impl(thread, obj, cause);\n+}\n@@ -1411,0 +1512,9 @@\n+ObjectMonitor* ObjectSynchronizer::inflate_impl(JavaThread* inflating_thread, oop object, const InflateCause cause) {\n+  \/\/ The JavaThread* inflating_thread parameter is only used by LM_LIGHTWEIGHT and requires\n+  \/\/ that the inflating_thread == Thread::current() or is suspended throughout the call by\n+  \/\/ some other mechanism.\n+  \/\/ Even with LM_LIGHTWEIGHT the thread might be nullptr when called from a non\n+  \/\/ JavaThread. (As may still be the case from FastHashCode). However it is only\n+  \/\/ important for the correctness of the LM_LIGHTWEIGHT algorithm that the thread\n+  \/\/ is set when called from ObjectSynchronizer::enter from the owning thread,\n+  \/\/ ObjectSynchronizer::enter_for from any thread, or ObjectSynchronizer::exit.\n@@ -1419,4 +1529,4 @@\n-    \/\/                   is anonymous and the current thread owns the\n-    \/\/                   object lock, then we make the current thread the\n-    \/\/                   ObjectMonitor owner and remove the lock from the\n-    \/\/                   current thread's lock stack.\n+    \/\/                   is anonymous and the inflating_thread owns the\n+    \/\/                   object lock, then we make the inflating_thread\n+    \/\/                   the ObjectMonitor owner and remove the lock from\n+    \/\/                   the inflating_thread's lock stack.\n@@ -1434,3 +1544,5 @@\n-      if (LockingMode == LM_LIGHTWEIGHT && inf->is_owner_anonymous() && is_lock_owned(current, object)) {\n-        inf->set_owner_from_anonymous(current);\n-        JavaThread::cast(current)->lock_stack().remove(object);\n+      if (LockingMode == LM_LIGHTWEIGHT && inf->is_owner_anonymous() &&\n+          inflating_thread != nullptr && inflating_thread->lock_stack().contains(object)) {\n+        inf->set_owner_from_anonymous(inflating_thread);\n+        size_t removed = inflating_thread->lock_stack().remove(object);\n+        inf->set_recursions(removed - 1);\n@@ -1456,1 +1568,1 @@\n-    \/\/ Could be fast-locked either by current or by some other thread.\n+    \/\/ Could be fast-locked either by the inflating_thread or by some other thread.\n@@ -1460,2 +1572,2 @@\n-    \/\/ this thread owns the monitor, then we set the ObjectMonitor's\n-    \/\/ owner to this thread. Otherwise, we set the ObjectMonitor's owner\n+    \/\/ the inflating_thread owns the monitor, then we set the ObjectMonitor's\n+    \/\/ owner to the inflating_thread. Otherwise, we set the ObjectMonitor's owner\n@@ -1469,1 +1581,1 @@\n-      bool own = is_lock_owned(current, object);\n+      bool own = inflating_thread != nullptr && inflating_thread->lock_stack().contains(object);\n@@ -1471,2 +1583,2 @@\n-        \/\/ Owned by us.\n-        monitor->set_owner_from(nullptr, current);\n+        \/\/ Owned by inflating_thread.\n+        monitor->set_owner_from(nullptr, inflating_thread);\n@@ -1482,1 +1594,2 @@\n-          JavaThread::cast(current)->lock_stack().remove(object);\n+          size_t removed = inflating_thread->lock_stack().remove(object);\n+          monitor->set_recursions(removed - 1);\n@@ -1492,1 +1605,1 @@\n-          ResourceMark rm(current);\n+          ResourceMark rm;\n@@ -1591,1 +1704,1 @@\n-        ResourceMark rm(current);\n+        ResourceMark rm;\n@@ -1635,1 +1748,1 @@\n-      ResourceMark rm(current);\n+      ResourceMark rm;\n@@ -2059,3 +2172,1 @@\n-    \/\/ This should not happen, but if it does, it is not fatal.\n-    out->print_cr(\"WARNING: monitor=\" INTPTR_FORMAT \": in-use monitor is \"\n-                  \"deflated.\", p2i(n));\n+    \/\/ This could happen when monitor deflation blocks for a safepoint.\n@@ -2065,0 +2176,1 @@\n+\n","filename":"src\/hotspot\/share\/runtime\/synchronizer.cpp","additions":190,"deletions":78,"binary":false,"changes":268,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1998, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1998, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -96,2 +96,1 @@\n-  static void enter(Handle obj, BasicLock* lock, JavaThread* locking_thread, JavaThread* current);\n-  static void enter(Handle obj, BasicLock* lock, JavaThread* current) { enter(obj, lock, current, current); }\n+  static void enter(Handle obj, BasicLock* lock, JavaThread* current);\n@@ -99,0 +98,10 @@\n+  \/\/ Used to enter a monitor for another thread. This requires that the\n+  \/\/ locking_thread is suspended, and that entering on a potential\n+  \/\/ inflated monitor may only contend with deflation. That is the obj being\n+  \/\/ locked on is either already locked by the locking_thread or cannot\n+  \/\/ escape the locking_thread.\n+  static void enter_for(Handle obj, BasicLock* lock, JavaThread* locking_thread);\n+private:\n+  \/\/ Shared implementation for enter and enter_for. Performs all but\n+  \/\/ inflated monitor enter.\n+  static bool enter_fast_impl(Handle obj, BasicLock* lock, JavaThread* locking_thread);\n@@ -100,0 +109,1 @@\n+public:\n@@ -115,0 +125,8 @@\n+  \/\/ Used to inflate a monitor as if it was done from the thread JavaThread.\n+  static ObjectMonitor* inflate_for(JavaThread* thread, oop obj, const InflateCause cause);\n+\n+private:\n+  \/\/ Shared implementation between the different LockingMode.\n+  static ObjectMonitor* inflate_impl(JavaThread* thread, oop obj, const InflateCause cause);\n+\n+public:\n@@ -198,1 +216,1 @@\n-  static void handle_sync_on_value_based_class(Handle obj, JavaThread* current);\n+  static void handle_sync_on_value_based_class(Handle obj, JavaThread* locking_thread);\n","filename":"src\/hotspot\/share\/runtime\/synchronizer.hpp","additions":22,"deletions":4,"binary":false,"changes":26,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -752,0 +752,7 @@\n+  if (Continuations::enabled()) {\n+    \/\/ Initialize Continuation class now so that failure to create enterSpecial\/doYield\n+    \/\/ special nmethods due to limited CodeCache size can be treated as a fatal error at\n+    \/\/ startup with the proper message that CodeCache size is too small.\n+    initialize_class(vmSymbols::jdk_internal_vm_Continuation(), CHECK_JNI_ERR);\n+  }\n+\n@@ -1114,1 +1121,1 @@\n-void assert_thread_claimed(const char* kind, Thread* t, uintx expected) {\n+static void assert_thread_claimed(const char* kind, Thread* t, uintx expected) {\n","filename":"src\/hotspot\/share\/runtime\/threads.cpp","additions":9,"deletions":2,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -43,0 +43,1 @@\n+#include \"code\/compiledIC.hpp\"\n@@ -66,1 +67,0 @@\n-#include \"oops\/compiledICHolder.hpp\"\n@@ -214,2 +214,0 @@\n-  nonstatic_field(CompiledICHolder,            _holder_metadata,                              Metadata*)                             \\\n-  nonstatic_field(CompiledICHolder,            _holder_klass,                                 Klass*)                                \\\n@@ -1165,1 +1163,0 @@\n-  declare_toplevel_type(CompiledICHolder)                                 \\\n","filename":"src\/hotspot\/share\/runtime\/vmStructs.cpp","additions":1,"deletions":4,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2023, 2024, Oracle and\/or its affiliates. All rights reserved.\n","filename":"test\/hotspot\/gtest\/runtime\/test_lockStack.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2023, 2024, Oracle and\/or its affiliates. All rights reserved.\n","filename":"test\/hotspot\/jtreg\/gtest\/LockStackGtests.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"}]}