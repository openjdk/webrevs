{"files":[{"patch":"@@ -55,3 +55,0 @@\n-          - s390x\n-          - ppc64le\n-          - riscv64\n@@ -70,15 +67,0 @@\n-          - target-cpu: s390x\n-            gnu-arch: s390x\n-            debian-arch: s390x\n-            debian-repository: https:\/\/httpredir.debian.org\/debian\/\n-            debian-version: bullseye\n-          - target-cpu: ppc64le\n-            gnu-arch: powerpc64le\n-            debian-arch: ppc64el\n-            debian-repository: https:\/\/httpredir.debian.org\/debian\/\n-            debian-version: bullseye\n-          - target-cpu: riscv64\n-            gnu-arch: riscv64\n-            debian-arch: riscv64\n-            debian-repository: https:\/\/httpredir.debian.org\/debian\/\n-            debian-version: sid\n","filename":".github\/workflows\/build-cross-compile.yml","additions":0,"deletions":18,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n-project=jdk\n+project=lilliput\n@@ -7,1 +7,1 @@\n-error=author,committer,reviewers,merge,issues,executable,symlink,message,hg-tag,whitespace,problemlists\n+error=author,committer,reviewers,issues,executable,symlink,message,hg-tag,whitespace,problemlists\n@@ -21,3 +21,0 @@\n-[checks \"merge\"]\n-message=Merge\n-\n@@ -25,1 +22,1 @@\n-reviewers=1\n+committers=1\n","filename":".jcheck\/conf","additions":3,"deletions":6,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -840,0 +840,5 @@\n+  # Add more Lilliput-specific ProblemLists when UCOH is enabled\n+  ifneq ($$(findstring -XX:+UseCompactObjectHeaders, $$(TEST_OPTS)), )\n+    JTREG_EXTRA_PROBLEM_LISTS += $(TOPDIR)\/test\/hotspot\/jtreg\/ProblemList-lilliput.txt\n+  endif\n+\n","filename":"make\/RunTests.gmk","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -7127,1 +7127,1 @@\n-  predicate(!needs_acquiring_load(n));\n+  predicate(!needs_acquiring_load(n) && !UseCompactObjectHeaders);\n@@ -7137,0 +7137,14 @@\n+instruct loadNKlassCompactHeaders(iRegNNoSp dst, memory4 mem, rFlagsReg cr)\n+%{\n+  match(Set dst (LoadNKlass mem));\n+  effect(KILL cr);\n+  predicate(!needs_acquiring_load(n) && UseCompactObjectHeaders);\n+\n+  ins_cost(4 * INSN_COST);\n+  format %{ \"ldrw  $dst, $mem\\t# compressed class ptr\" %}\n+  ins_encode %{\n+    __ load_nklass_compact($dst$$Register, $mem$$base$$Register, $mem$$index$$Register, $mem$$scale, $mem$$disp);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n@@ -16436,1 +16450,1 @@\n-  predicate(LockingMode != LM_LIGHTWEIGHT);\n+  predicate(LockingMode != LM_LIGHTWEIGHT && LockingMode != LM_PLACEHOLDER);\n@@ -16452,1 +16466,1 @@\n-  predicate(LockingMode != LM_LIGHTWEIGHT);\n+  predicate(LockingMode != LM_LIGHTWEIGHT && LockingMode != LM_PLACEHOLDER);\n@@ -16497,0 +16511,31 @@\n+instruct cmpFastLockPlaceholder(rFlagsReg cr, iRegP object, iRegP box, iRegPNoSp tmp, iRegPNoSp tmp2, iRegPNoSp tmp3)\n+%{\n+  predicate(LockingMode == LM_PLACEHOLDER);\n+  match(Set cr (FastLock object box));\n+  effect(TEMP tmp, TEMP tmp2, TEMP tmp3);\n+\n+  ins_cost(5 * INSN_COST);\n+  format %{ \"fastlock $object,$box\\t! kills $tmp,$tmp2,$tmp3\" %}\n+\n+  ins_encode %{\n+    __ fast_lock_placeholder($object$$Register, $box$$Register, $tmp$$Register, $tmp2$$Register, $tmp3$$Register);\n+  %}\n+\n+  ins_pipe(pipe_serial);\n+%}\n+\n+instruct cmpFastUnlockPlaceholder(rFlagsReg cr, iRegP object, iRegP box, iRegPNoSp tmp, iRegPNoSp tmp2)\n+%{\n+  predicate(LockingMode == LM_PLACEHOLDER);\n+  match(Set cr (FastUnlock object box));\n+  effect(TEMP tmp, TEMP tmp2);\n+\n+  ins_cost(5 * INSN_COST);\n+  format %{ \"fastunlock $object,$box\\t! kills $tmp, $tmp2\" %}\n+\n+  ins_encode %{\n+    __ fast_unlock_placeholder($object$$Register, $box$$Register, $tmp$$Register, $tmp2$$Register);\n+  %}\n+\n+  ins_pipe(pipe_serial);\n+%}\n","filename":"src\/hotspot\/cpu\/aarch64\/aarch64.ad","additions":48,"deletions":3,"binary":false,"changes":51,"status":"modified"},{"patch":"@@ -35,0 +35,1 @@\n+#include \"runtime\/objectMonitor.hpp\"\n@@ -236,0 +237,7 @@\n+void LoadKlassStub::emit_code(LIR_Assembler* ce) {\n+  assert(UseCompactObjectHeaders, \"Only use with compact object headers\");\n+  __ bind(_entry);\n+  Register d = _result->as_register();\n+  __ ldr(d, Address(d, OM_OFFSET_NO_MONITOR_VALUE_TAG(header)));\n+  __ b(_continuation);\n+}\n","filename":"src\/hotspot\/cpu\/aarch64\/c1_CodeStubs_aarch64.cpp","additions":8,"deletions":0,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -43,0 +43,1 @@\n+#include \"runtime\/globals.hpp\"\n@@ -45,0 +46,1 @@\n+#include \"utilities\/globalDefinitions.hpp\"\n@@ -2248,2 +2250,0 @@\n-  Address src_klass_addr = Address(src, oopDesc::klass_offset_in_bytes());\n-  Address dst_klass_addr = Address(dst, oopDesc::klass_offset_in_bytes());\n@@ -2310,9 +2310,1 @@\n-      if (UseCompressedClassPointers) {\n-        __ ldrw(tmp, src_klass_addr);\n-        __ ldrw(rscratch1, dst_klass_addr);\n-        __ cmpw(tmp, rscratch1);\n-      } else {\n-        __ ldr(tmp, src_klass_addr);\n-        __ ldr(rscratch1, dst_klass_addr);\n-        __ cmp(tmp, rscratch1);\n-      }\n+      __ cmp_klass(src, dst, tmp, rscratch1);\n@@ -2440,3 +2432,0 @@\n-    if (UseCompressedClassPointers) {\n-      __ encode_klass_not_null(tmp);\n-    }\n@@ -2445,8 +2434,1 @@\n-\n-      if (UseCompressedClassPointers) {\n-        __ ldrw(rscratch1, dst_klass_addr);\n-        __ cmpw(tmp, rscratch1);\n-      } else {\n-        __ ldr(rscratch1, dst_klass_addr);\n-        __ cmp(tmp, rscratch1);\n-      }\n+      __ cmp_klass(dst, tmp, rscratch1);\n@@ -2454,7 +2436,1 @@\n-      if (UseCompressedClassPointers) {\n-        __ ldrw(rscratch1, src_klass_addr);\n-        __ cmpw(tmp, rscratch1);\n-      } else {\n-        __ ldr(rscratch1, src_klass_addr);\n-        __ cmp(tmp, rscratch1);\n-      }\n+      __ cmp_klass(src, tmp, rscratch1);\n@@ -2463,7 +2439,1 @@\n-      if (UseCompressedClassPointers) {\n-        __ ldrw(rscratch1, dst_klass_addr);\n-        __ cmpw(tmp, rscratch1);\n-      } else {\n-        __ ldr(rscratch1, dst_klass_addr);\n-        __ cmp(tmp, rscratch1);\n-      }\n+      __ cmp_klass(dst, tmp, rscratch1);\n@@ -2551,1 +2521,14 @@\n-    __ ldrw(result, Address (obj, oopDesc::klass_offset_in_bytes()));\n+    if (UseCompactObjectHeaders) {\n+      \/\/ Check if we can take the (common) fast path, if obj is unlocked.\n+      __ ldr(result, Address(obj, oopDesc::mark_offset_in_bytes()));\n+      if (LockingMode != LM_PLACEHOLDER) {\n+        __ tst(result, markWord::monitor_value);\n+        __ br(Assembler::NE, *op->stub()->entry());\n+        __ bind(*op->stub()->continuation());\n+      }\n+\n+      \/\/ Shift to get proper narrow Klass*.\n+      __ lsr(result, result, markWord::klass_shift);\n+    } else {\n+      __ ldrw(result, Address (obj, oopDesc::klass_offset_in_bytes()));\n+    }\n","filename":"src\/hotspot\/cpu\/aarch64\/c1_LIRAssembler_aarch64.cpp","additions":20,"deletions":37,"binary":false,"changes":57,"status":"modified"},{"patch":"@@ -36,0 +36,1 @@\n+#include \"runtime\/globals.hpp\"\n@@ -39,0 +40,1 @@\n+#include \"utilities\/globalDefinitions.hpp\"\n@@ -81,0 +83,3 @@\n+  } else if (LockingMode == LM_PLACEHOLDER) {\n+    \/\/ null check obj. load_klass performs load if DiagnoseSyncOnValueBasedClasses != 0.\n+    ldr(hdr, Address(obj));\n@@ -83,1 +88,4 @@\n-  if (LockingMode == LM_LIGHTWEIGHT) {\n+  if (LockingMode == LM_PLACEHOLDER) {\n+    str(zr, Address(disp_hdr, BasicObjectLock::lock_offset() + in_ByteSize((BasicLock::displaced_header_offset_in_bytes()))));\n+    placeholder_lock(obj, hdr, temp, rscratch2, slow_case);\n+  } else if (LockingMode == LM_LIGHTWEIGHT) {\n@@ -134,1 +142,1 @@\n-  if (LockingMode != LM_LIGHTWEIGHT) {\n+  if (LockingMode != LM_LIGHTWEIGHT && LockingMode != LM_PLACEHOLDER) {\n@@ -146,1 +154,3 @@\n-  if (LockingMode == LM_LIGHTWEIGHT) {\n+  if (LockingMode == LM_PLACEHOLDER) {\n+    placeholder_unlock(obj, hdr, temp, rscratch2, slow_case);\n+  } else if (LockingMode == LM_LIGHTWEIGHT) {\n@@ -178,7 +188,3 @@\n-  \/\/ This assumes that all prototype bits fit in an int32_t\n-  mov(t1, (int32_t)(intptr_t)markWord::prototype().value());\n-  str(t1, Address(obj, oopDesc::mark_offset_in_bytes()));\n-\n-  if (UseCompressedClassPointers) { \/\/ Take care not to kill klass\n-    encode_klass_not_null(t1, klass);\n-    strw(t1, Address(obj, oopDesc::klass_offset_in_bytes()));\n+  if (UseCompactObjectHeaders) {\n+    ldr(t1, Address(klass, Klass::prototype_header_offset()));\n+    str(t1, Address(obj, oopDesc::mark_offset_in_bytes()));\n@@ -186,1 +192,10 @@\n-    str(klass, Address(obj, oopDesc::klass_offset_in_bytes()));\n+    \/\/ This assumes that all prototype bits fit in an int32_t\n+    mov(t1, (int32_t)(intptr_t)markWord::prototype().value());\n+    str(t1, Address(obj, oopDesc::mark_offset_in_bytes()));\n+\n+    if (UseCompressedClassPointers) { \/\/ Take care not to kill klass\n+      encode_klass_not_null(t1, klass);\n+      strw(t1, Address(obj, oopDesc::klass_offset_in_bytes()));\n+    } else {\n+      str(klass, Address(obj, oopDesc::klass_offset_in_bytes()));\n+    }\n@@ -197,1 +212,1 @@\n-  } else if (UseCompressedClassPointers) {\n+  } else if (UseCompressedClassPointers && !UseCompactObjectHeaders) {\n","filename":"src\/hotspot\/cpu\/aarch64\/c1_MacroAssembler_aarch64.cpp","additions":27,"deletions":12,"binary":false,"changes":39,"status":"modified"},{"patch":"@@ -94,0 +94,11 @@\n+int C2LoadNKlassStub::max_size() const {\n+  return 8;\n+}\n+\n+void C2LoadNKlassStub::emit(C2_MacroAssembler& masm) {\n+  __ bind(entry());\n+  Register d = dst();\n+  __ ldr(d, Address(d, OM_OFFSET_NO_MONITOR_VALUE_TAG(header)));\n+  __ b(continuation());\n+}\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/c2_CodeStubs_aarch64.cpp","additions":11,"deletions":0,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -34,0 +34,1 @@\n+#include \"runtime\/globals.hpp\"\n@@ -60,0 +61,1 @@\n+  assert(LockingMode != LM_PLACEHOLDER, \"uses fast_lock_placeholder\");\n@@ -483,0 +485,334 @@\n+void C2_MacroAssembler::fast_lock_placeholder(Register obj, Register box, Register t1,\n+                                              Register t2, Register t3) {\n+  assert(LockingMode == LM_PLACEHOLDER, \"must be\");\n+  assert_different_registers(obj, box, t1, t2, t3);\n+\n+  \/\/ Handle inflated monitor.\n+  Label inflated;\n+  \/\/ Finish fast lock successfully. MUST reach to with flag == EQ\n+  Label locked;\n+  \/\/ Finish fast lock unsuccessfully. MUST branch to with flag == NE\n+  Label slow_path;\n+\n+  \/\/ Clear box. TODO: Is this neccesarry? May also defer this to not write twice.\n+  str(zr, Address(box, BasicLock::displaced_header_offset_in_bytes()));\n+\n+  if (DiagnoseSyncOnValueBasedClasses != 0) {\n+    load_klass(t1, obj);\n+    ldrw(t1, Address(t1, Klass::access_flags_offset()));\n+    tstw(t1, JVM_ACC_IS_VALUE_BASED_CLASS);\n+    br(Assembler::NE, slow_path);\n+  }\n+\n+  const Register mark = t1;\n+  const Register t = t3;\n+\n+  { \/\/ Lightweight locking\n+\n+    \/\/ Push lock to the lock stack and finish successfully. MUST reach to with flag == EQ\n+    Label push;\n+\n+    const Register top = t2;\n+\n+    \/\/ Check if lock-stack is full.\n+    ldrw(top, Address(rthread, JavaThread::lock_stack_top_offset()));\n+    cmpw(top, (unsigned)LockStack::end_offset() - 1);\n+    br(Assembler::GT, slow_path);\n+\n+    \/\/ Check if recursive.\n+    subw(t, top, oopSize);\n+    ldr(t, Address(rthread, t));\n+    cmp(obj, t);\n+    br(Assembler::EQ, push);\n+\n+    \/\/ Relaxed normal load to check for monitor. Optimization for monitor case.\n+    ldr(mark, Address(obj, oopDesc::mark_offset_in_bytes()));\n+    tbnz(mark, exact_log2(markWord::monitor_value), inflated);\n+\n+    \/\/ Not inflated\n+    assert(oopDesc::mark_offset_in_bytes() == 0, \"required to avoid a lea\");\n+    \/\/ Load-Acquire Exclusive Register to match Store Exclusive Register below.\n+    \/\/ Acquire to satisfy the JMM.\n+    ldaxr(mark, obj);\n+\n+    \/\/ Recheck for monitor (0b10).\n+    tbnz(mark, exact_log2(markWord::monitor_value), inflated);\n+\n+    \/\/ Check that obj is unlocked (0b01).\n+    orr(t, mark, markWord::unlocked_value);\n+    cmp(mark, t);\n+    br(Assembler::NE, slow_path);\n+\n+    \/\/ Clear unlock bit (0b01 => 0b00).\n+    andr(mark, mark, ~markWord::unlocked_value);\n+\n+    \/\/ Try to lock. Transition lock-bits 0b01 => 0b00\n+    stxr(t, mark, obj);\n+    cmpw(t, zr);\n+    br(Assembler::NE, slow_path);\n+\n+    bind(push);\n+    \/\/ After successful lock, push object on lock-stack.\n+    str(obj, Address(rthread, top));\n+    addw(top, top, oopSize);\n+    strw(top, Address(rthread, JavaThread::lock_stack_top_offset()));\n+    b(locked);\n+  }\n+\n+  { \/\/ Handle inflated monitor.\n+    bind(inflated);\n+\n+  if (!OMUseC2Cache) {\n+    \/\/ Set Flags == NE\n+    cmp(zr, obj);\n+    b(slow_path);\n+  } else {\n+\n+    if (OMCacheHitRate) increment(Address(rthread, JavaThread::lock_lookup_offset()));\n+\n+    Label monitor_found, loop;\n+    \/\/ Load cache address\n+    lea(t, Address(rthread, JavaThread::om_cache_oops_offset()));\n+\n+    \/\/ Search for obj in cache.\n+    bind(loop);\n+\n+    \/\/ Check for match.\n+    ldr(t1, Address(t));\n+    cmp(obj, t1);\n+    br(Assembler::EQ, monitor_found);\n+\n+    \/\/ Search until null encountered, guaranteed _null_sentinel at end.\n+    increment(t, oopSize);\n+    cbnz(t1, loop);\n+    \/\/ Cache Miss, NE set from cmp above, cbnz does not set flags\n+    b(slow_path);\n+\n+    bind(monitor_found);\n+    ldr(t1, Address(t, OMCache::oop_to_monitor_difference()));\n+    if (OMCacheHitRate) increment(Address(rthread, JavaThread::lock_hit_offset()));\n+\n+    \/\/ ObjectMonitor* is in t1\n+    const Register monitor = t1;\n+    const Register owner_addr = t2;\n+    const Register owner = t3;\n+\n+    Label recursive;\n+    Label monitor_locked;\n+\n+    \/\/ Compute owner address.\n+    lea(owner_addr, Address(monitor, ObjectMonitor::owner_offset()));\n+\n+    if (OMRecursiveFastPath) {\n+      ldr(owner, Address(owner_addr));\n+      cmp(owner, rthread);\n+      br(Assembler::EQ, recursive);\n+    }\n+\n+    \/\/ CAS owner (null => current thread).\n+    cmpxchg(owner_addr, zr, rthread, Assembler::xword, \/*acquire*\/ true,\n+            \/*release*\/ false, \/*weak*\/ false, owner);\n+    br(Assembler::EQ, monitor_locked);\n+\n+    if (OMRecursiveFastPath) {\n+      b(slow_path);\n+    } else {\n+      \/\/ Check if recursive.\n+      cmp(owner, rthread);\n+      br(Assembler::NE, slow_path);\n+    }\n+\n+    \/\/ Recursive.\n+    bind(recursive);\n+    increment(Address(monitor, ObjectMonitor::recursions_offset()), 1);\n+\n+    bind(monitor_locked);\n+    str(monitor, Address(box, BasicLock::displaced_header_offset_in_bytes()));\n+  }\n+\n+  }\n+\n+  bind(locked);\n+  increment(Address(rthread, JavaThread::held_monitor_count_offset()));\n+\n+#ifdef ASSERT\n+  \/\/ Check that locked label is reached with Flags == EQ.\n+  Label flag_correct;\n+  br(Assembler::EQ, flag_correct);\n+  stop(\"Fast Lock Flag != EQ\");\n+#endif\n+\n+  bind(slow_path);\n+#ifdef ASSERT\n+  \/\/ Check that slow_path label is reached with Flags == NE.\n+  br(Assembler::NE, flag_correct);\n+  stop(\"Fast Lock Flag != NE\");\n+  bind(flag_correct);\n+#endif\n+  \/\/ C2 uses the value of Flags (NE vs EQ) to determine the continuation.\n+}\n+\n+void C2_MacroAssembler::fast_unlock_placeholder(Register obj, Register box, Register t1,\n+                                                Register t2) {\n+  assert(LockingMode == LM_PLACEHOLDER, \"must be\");\n+  assert_different_registers(obj, box, t1, t2);\n+\n+  \/\/ Handle inflated monitor.\n+  Label inflated, inflated_check_stack;\n+  \/\/ Finish fast unlock successfully. MUST reach to with flag == EQ\n+  Label unlocked;\n+  \/\/ Finish fast unlock unsuccessfully. MUST branch to with flag == NE\n+  Label slow_path;\n+\n+  \/\/ TODO: Cleanup the registers, using rscratch2 for now because we need the box.\n+  const Register mark = rscratch2;\n+  const Register top = t1;\n+  const Register t = t2;\n+\n+  { \/\/ Lightweight unlock\n+\n+    Label push_and_slow_path;\n+\n+    \/\/ Check if obj is top of lock-stack.\n+    ldrw(top, Address(rthread, JavaThread::lock_stack_top_offset()));\n+    subw(top, top, oopSize);\n+    ldr(t, Address(rthread, top));\n+    cmp(obj, t);\n+    \/\/ Top of lock stack was not obj. Must be monitor.\n+    br(Assembler::NE, inflated_check_stack);\n+\n+    \/\/ Pop lock-stack.\n+    DEBUG_ONLY(str(zr, Address(rthread, top));)\n+    strw(top, Address(rthread, JavaThread::lock_stack_top_offset()));\n+\n+    \/\/ Check if recursive.\n+    subw(t, top, oopSize);\n+    ldr(t, Address(rthread, t));\n+    cmp(obj, t);\n+    br(Assembler::EQ, unlocked);\n+\n+    \/\/ Not recursive.\n+    assert(oopDesc::mark_offset_in_bytes() == 0, \"required to avoid a lea\");\n+    \/\/ Load Exclusive Register to match Store-Release Exclusive Register below.\n+    ldxr(mark, obj);\n+\n+    \/\/ Check header for monitor (0b10).\n+    \/\/ Because we got here by popping (meaning we pushed in locked)\n+    \/\/ there will be no monitor in the box. So we need to push back the obj\n+    \/\/ so that the runtime can fix any potential anonymous owner.\n+    tbnz(mark, exact_log2(markWord::monitor_value), push_and_slow_path);\n+\n+    \/\/ Try to unlock. Transition lock bits 0b00 => 0b01\n+    orr(t, mark, markWord::unlocked_value);\n+    \/\/ Release to satisfy the JMM.\n+    stlxr(rscratch1, t, obj);\n+    cmpw(rscratch1, 0u);\n+    br(Assembler::EQ, unlocked);\n+\n+    bind(push_and_slow_path);\n+    \/\/ Load link store conditional exclusive failed.\n+    \/\/ Restore lock-stack and handle the unlock in runtime.\n+    DEBUG_ONLY(str(obj, Address(rthread, top));)\n+    addw(top, top, oopSize);\n+    str(top, Address(rthread, JavaThread::lock_stack_top_offset()));\n+    b(slow_path);\n+  }\n+\n+\n+  { \/\/ Handle inflated monitor.\n+    bind(inflated_check_stack);\n+    ldr(mark, Address(obj, oopDesc::mark_offset_in_bytes()));\n+#ifdef ASSERT\n+    tbnz(mark, exact_log2(markWord::monitor_value), inflated);\n+    stop(\"Fast Unlock not monitor\");\n+#endif\n+\n+    bind(inflated);\n+\n+#ifdef ASSERT\n+    Label check_done;\n+    subw(top, top, oopSize);\n+    cmpw(top, in_bytes(JavaThread::lock_stack_base_offset()));\n+    br(Assembler::LT, check_done);\n+    ldr(t, Address(rthread, top));\n+    cmp(obj, t);\n+    br(Assembler::NE, inflated);\n+    stop(\"Fast Unlock lock on stack\");\n+    bind(check_done);\n+#endif\n+\n+    if (!OMUseC2Cache) {\n+      b(slow_path);\n+    } else {\n+      const Register monitor = box;\n+\n+      if (OMCacheHitRate) increment(Address(rthread, JavaThread::unlock_lookup_offset()));\n+      ldr(monitor, Address(box, BasicLock::displaced_header_offset_in_bytes()));\n+      \/\/ TODO: Cleanup these constants (with an enum and asserts)\n+      cmp(monitor, (uint8_t)2);\n+      \/\/ Non symmetrical, take slow path monitor == 0 or 1, 0 and 1 < 2, both LS and NE\n+      br(Assembler::LO, slow_path);\n+      if (OMCacheHitRate) increment(Address(rthread, JavaThread::unlock_hit_offset()));\n+\n+      const Register recursions = t1;\n+      Label not_recursive;\n+\n+      \/\/ Check if recursive.\n+      ldr(recursions, Address(monitor, ObjectMonitor::recursions_offset()));\n+      cbz(recursions, not_recursive);\n+\n+      \/\/ Recursive unlock.\n+      sub(recursions, recursions, 1u);\n+      str(recursions, Address(monitor, ObjectMonitor::recursions_offset()));\n+      \/\/ Set flag == EQ\n+      cmp(recursions, recursions);\n+      b(unlocked);\n+\n+      bind(not_recursive);\n+\n+      Label release;\n+      const Register owner_addr = t1;\n+\n+      \/\/ Compute owner address.\n+      lea(owner_addr, Address(monitor, ObjectMonitor::owner_offset()));\n+\n+      \/\/ Check if the entry lists are empty.\n+      ldr(rscratch1, Address(monitor, ObjectMonitor::EntryList_offset()));\n+      ldr(t, Address(monitor, ObjectMonitor::cxq_offset()));\n+      orr(rscratch1, rscratch1, t);\n+      cmp(rscratch1, zr);\n+      br(Assembler::EQ, release);\n+\n+      \/\/ The owner may be anonymous and we removed the last obj entry in\n+      \/\/ the lock-stack. This loses the information about the owner.\n+      \/\/ Write the thread to the owner field so the runtime knows the owner.\n+      str(rthread, Address(owner_addr));\n+      b(slow_path);\n+\n+      bind(release);\n+      \/\/ Set owner to null.\n+      \/\/ Release to satisfy the JMM\n+      stlr(zr, owner_addr);\n+    }\n+  }\n+\n+  bind(unlocked);\n+  decrement(Address(rthread, JavaThread::held_monitor_count_offset()));\n+\n+#ifdef ASSERT\n+  \/\/ Check that unlocked label is reached with Flags == EQ.\n+  Label flag_correct;\n+  br(Assembler::EQ, flag_correct);\n+  stop(\"Fast Unlock Flag != EQ\");\n+#endif\n+\n+  bind(slow_path);\n+#ifdef ASSERT\n+  \/\/ Check that slow_path label is reached with Flags == NE.\n+  br(Assembler::NE, flag_correct);\n+  stop(\"Fast Unlock Flag != NE\");\n+  bind(flag_correct);\n+#endif\n+  \/\/ C2 uses the value of Flags (NE vs EQ) to determine the continuation.\n+}\n+\n@@ -2497,0 +2833,31 @@\n+\n+void C2_MacroAssembler::load_nklass_compact(Register dst, Register obj, Register index, int scale, int disp) {\n+  \/\/ Note: Don't clobber obj anywhere in that method!\n+\n+  \/\/ The incoming address is pointing into obj-start + klass_offset_in_bytes. We need to extract\n+  \/\/ obj-start, so that we can load from the object's mark-word instead. Usually the address\n+  \/\/ comes as obj-start in obj and klass_offset_in_bytes in disp. However, sometimes C2\n+  \/\/ emits code that pre-computes obj-start + klass_offset_in_bytes into a register, and\n+  \/\/ then passes that register as obj and 0 in disp. The following code extracts the base\n+  \/\/ and offset to load the mark-word.\n+  int offset = oopDesc::mark_offset_in_bytes() + disp - oopDesc::klass_offset_in_bytes();\n+  if (index == noreg) {\n+    ldr(dst, Address(obj, offset));\n+  } else {\n+    lea(dst, Address(obj, index, Address::lsl(scale)));\n+    ldr(dst, Address(dst, offset));\n+  }\n+\n+  if (LockingMode != LM_PLACEHOLDER) {\n+    C2LoadNKlassStub* stub = new (Compile::current()->comp_arena()) C2LoadNKlassStub(dst);\n+    Compile::current()->output()->add_stub(stub);\n+\n+    \/\/ NOTE: We can't use tbnz here, because the target is sometimes too far away\n+    \/\/ and cannot be encoded.\n+    tst(dst, markWord::monitor_value);\n+    br(Assembler::NE, stub->entry());\n+    bind(stub->continuation());\n+  }\n+\n+  lsr(dst, dst, markWord::klass_shift);\n+}\n","filename":"src\/hotspot\/cpu\/aarch64\/c2_MacroAssembler_aarch64.cpp","additions":367,"deletions":0,"binary":false,"changes":367,"status":"modified"},{"patch":"@@ -44,0 +44,3 @@\n+  \/\/ Code used by cmpFastLockPlaceholder and cmpFastUnlockPlaceholder mach instructions in .ad file.\n+  void fast_lock_placeholder(Register object, Register box, Register t1, Register t2, Register t3);\n+  void fast_unlock_placeholder(Register object, Register box, Register t1, Register t2);\n@@ -182,0 +185,2 @@\n+  void load_nklass_compact(Register dst, Register obj, Register index, int scale, int disp);\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/c2_MacroAssembler_aarch64.hpp","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -49,0 +49,1 @@\n+#include \"utilities\/globalDefinitions.hpp\"\n@@ -703,1 +704,5 @@\n-    if (LockingMode == LM_LIGHTWEIGHT) {\n+    if (LockingMode == LM_PLACEHOLDER) {\n+      str(zr, Address(lock_reg, mark_offset));\n+      placeholder_lock(obj_reg, tmp, tmp2, tmp3, slow_case);\n+      b(count);\n+    } else if (LockingMode == LM_LIGHTWEIGHT) {\n@@ -760,1 +765,2 @@\n-    if (LockingMode == LM_LIGHTWEIGHT) {\n+    if (LockingMode == LM_LIGHTWEIGHT || LockingMode == LM_PLACEHOLDER) {\n+      \/\/ TODO: Clean this monitorenter_obj up. We still want to use the lock_reg for placeholder\n@@ -806,1 +812,2 @@\n-    if (LockingMode != LM_LIGHTWEIGHT) {\n+    if (LockingMode != LM_LIGHTWEIGHT && LockingMode != LM_PLACEHOLDER) {\n+      \/\/ TODO: Cleanup lock_reg usage for placeholder\n@@ -818,1 +825,6 @@\n-    if (LockingMode == LM_LIGHTWEIGHT) {\n+    if (LockingMode == LM_PLACEHOLDER) {\n+      Label slow_case;\n+      placeholder_unlock(obj_reg, header_reg, swap_reg, tmp_reg, slow_case);\n+      b(count);\n+      bind(slow_case);\n+    } else if (LockingMode == LM_LIGHTWEIGHT) {\n","filename":"src\/hotspot\/cpu\/aarch64\/interp_masm_aarch64.cpp","additions":16,"deletions":4,"binary":false,"changes":20,"status":"modified"},{"patch":"@@ -4481,0 +4481,21 @@\n+\/\/ Loads the obj's Klass* into dst.\n+\/\/ Preserves all registers (incl src, rscratch1 and rscratch2).\n+void MacroAssembler::load_nklass(Register dst, Register src) {\n+  assert(UseCompactObjectHeaders, \"expects UseCompactObjectHeaders\");\n+\n+\n+  \/\/ Check if we can take the (common) fast path, if obj is unlocked.\n+  ldr(dst, Address(src, oopDesc::mark_offset_in_bytes()));\n+  if (LockingMode != LM_PLACEHOLDER) {\n+    Label fast;\n+    tbz(dst, exact_log2(markWord::monitor_value), fast);\n+\n+    \/\/ Fetch displaced header\n+    ldr(dst, Address(dst, OM_OFFSET_NO_MONITOR_VALUE_TAG(header)));\n+\n+    \/\/ Fast-path: shift and decode Klass*.\n+    bind(fast);\n+  }\n+  lsr(dst, dst, markWord::klass_shift);\n+}\n+\n@@ -4482,1 +4503,4 @@\n-  if (UseCompressedClassPointers) {\n+  if (UseCompactObjectHeaders) {\n+    load_nklass(dst, src);\n+    decode_klass_not_null(dst);\n+  } else if (UseCompressedClassPointers) {\n@@ -4538,0 +4562,1 @@\n+  assert_different_registers(oop, trial_klass, tmp);\n@@ -4539,1 +4564,5 @@\n-    ldrw(tmp, Address(oop, oopDesc::klass_offset_in_bytes()));\n+    if (UseCompactObjectHeaders) {\n+      load_nklass(tmp, oop);\n+    } else {\n+      ldrw(tmp, Address(oop, oopDesc::klass_offset_in_bytes()));\n+    }\n@@ -4556,0 +4585,16 @@\n+void MacroAssembler::cmp_klass(Register src, Register dst, Register tmp1, Register tmp2) {\n+  if (UseCompactObjectHeaders) {\n+    load_nklass(tmp1, src);\n+    load_nklass(tmp2, dst);\n+    cmpw(tmp1, tmp2);\n+  } else if (UseCompressedClassPointers) {\n+    ldrw(tmp1, Address(src, oopDesc::klass_offset_in_bytes()));\n+    ldrw(tmp2, Address(dst, oopDesc::klass_offset_in_bytes()));\n+    cmpw(tmp1, tmp2);\n+  } else {\n+    ldr(tmp1, Address(src, oopDesc::klass_offset_in_bytes()));\n+    ldr(tmp2, Address(dst, oopDesc::klass_offset_in_bytes()));\n+    cmp(tmp1, tmp2);\n+  }\n+}\n+\n@@ -4559,0 +4604,1 @@\n+  assert(!UseCompactObjectHeaders, \"not with compact headers\");\n@@ -4568,0 +4614,1 @@\n+  assert(!UseCompactObjectHeaders, \"not with compact headers\");\n@@ -6442,0 +6489,115 @@\n+#ifdef ASSERT\n+  {\n+    \/\/ Check for lock-stack underflow.\n+    Label stack_ok;\n+    ldrw(t1, Address(rthread, JavaThread::lock_stack_top_offset()));\n+    cmpw(t1, (unsigned)LockStack::start_offset());\n+    br(Assembler::GE, stack_ok);\n+    STOP(\"Lock-stack underflow\");\n+    bind(stack_ok);\n+  }\n+#endif\n+\n+  Label unlocked, push_and_slow;\n+  const Register top = t1;\n+  const Register mark = t2;\n+  const Register t = t3;\n+\n+  \/\/ Check if obj is top of lock-stack.\n+  ldrw(top, Address(rthread, JavaThread::lock_stack_top_offset()));\n+  subw(top, top, oopSize);\n+  ldr(t, Address(rthread, top));\n+  cmp(obj, t);\n+  br(Assembler::NE, slow);\n+\n+  \/\/ Pop lock-stack.\n+  DEBUG_ONLY(str(zr, Address(rthread, top));)\n+  strw(top, Address(rthread, JavaThread::lock_stack_top_offset()));\n+\n+  \/\/ Check if recursive.\n+  subw(t, top, oopSize);\n+  ldr(t, Address(rthread, t));\n+  cmp(obj, t);\n+  br(Assembler::EQ, unlocked);\n+\n+  \/\/ Not recursive. Check header for monitor (0b10).\n+  ldr(mark, Address(obj, oopDesc::mark_offset_in_bytes()));\n+  tbnz(mark, log2i_exact(markWord::monitor_value), push_and_slow);\n+\n+#ifdef ASSERT\n+  \/\/ Check header not unlocked (0b01).\n+  Label not_unlocked;\n+  tbz(mark, log2i_exact(markWord::unlocked_value), not_unlocked);\n+  stop(\"lightweight_unlock already unlocked\");\n+  bind(not_unlocked);\n+#endif\n+\n+  \/\/ Try to unlock. Transition lock bits 0b00 => 0b01\n+  assert(oopDesc::mark_offset_in_bytes() == 0, \"required to avoid lea\");\n+  orr(t, mark, markWord::unlocked_value);\n+  cmpxchg(obj, mark, t, Assembler::xword,\n+          \/*acquire*\/ false, \/*release*\/ true, \/*weak*\/ false, noreg);\n+  br(Assembler::EQ, unlocked);\n+\n+  bind(push_and_slow);\n+  \/\/ Restore lock-stack and handle the unlock in runtime.\n+  DEBUG_ONLY(str(obj, Address(rthread, top));)\n+  addw(top, top, oopSize);\n+  strw(top, Address(rthread, JavaThread::lock_stack_top_offset()));\n+  b(slow);\n+\n+  bind(unlocked);\n+}\n+\n+\/\/ Implements placeholder-locking.\n+\/\/\n+\/\/  - obj: the object to be locked\n+\/\/  - t1, t2, t3: temporary registers, will be destroyed\n+void MacroAssembler::placeholder_lock(Register obj, Register t1, Register t2, Register t3, Label& slow) {\n+  assert(LockingMode == LM_PLACEHOLDER, \"only used with new placeholder locking\");\n+  assert_different_registers(obj, t1, t2, t3, rscratch1);\n+\n+  Label push;\n+  const Register top = t1;\n+  const Register mark = t2;\n+  const Register t = t3;\n+\n+  \/\/ Check if the lock-stack is full.\n+  ldrw(top, Address(rthread, JavaThread::lock_stack_top_offset()));\n+  cmpw(top, (unsigned)LockStack::end_offset());\n+  br(Assembler::GE, slow);\n+\n+  \/\/ Check for recursion.\n+  subw(t, top, oopSize);\n+  ldr(t, Address(rthread, t));\n+  cmp(obj, t);\n+  br(Assembler::EQ, push);\n+\n+  \/\/ Check header for monitor (0b10).\n+  ldr(mark, Address(obj, oopDesc::mark_offset_in_bytes()));\n+  tst(mark, markWord::monitor_value);\n+  br(Assembler::NE, slow);\n+\n+  \/\/ Try to lock. Transition lock bits 0b01 => 0b00\n+  assert(oopDesc::mark_offset_in_bytes() == 0, \"required to avoid lea\");\n+  orr(mark, mark, markWord::unlocked_value);\n+  eor(t, mark, markWord::unlocked_value);\n+  cmpxchg(\/*addr*\/ obj, \/*expected*\/ mark, \/*new*\/ t, Assembler::xword,\n+          \/*acquire*\/ true, \/*release*\/ false, \/*weak*\/ false, noreg);\n+  br(Assembler::NE, slow);\n+\n+  bind(push);\n+  \/\/ After successful lock, push object on lock-stack.\n+  str(obj, Address(rthread, top));\n+  addw(top, top, oopSize);\n+  strw(top, Address(rthread, JavaThread::lock_stack_top_offset()));\n+}\n+\n+\/\/ Implements placeholder-unlocking.\n+\/\/\n+\/\/ - obj: the object to be unlocked\n+\/\/ - t1, t2, t3: temporary registers\n+void MacroAssembler::placeholder_unlock(Register obj, Register t1, Register t2, Register t3, Label& slow) {\n+  assert(LockingMode == LM_PLACEHOLDER, \"only used with new placeholder locking\");\n+  assert_different_registers(obj, t1, t2, t3, rscratch1);\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/macroAssembler_aarch64.cpp","additions":164,"deletions":2,"binary":false,"changes":166,"status":"modified"},{"patch":"@@ -867,0 +867,1 @@\n+  void load_nklass(Register dst, Register src);\n@@ -870,0 +871,1 @@\n+  void cmp_klass(Register src, Register dst, Register tmp1, Register tmp2);\n@@ -1608,0 +1610,3 @@\n+  void placeholder_lock(Register obj, Register t1, Register t2, Register t3, Label& slow);\n+  void placeholder_unlock(Register obj, Register t1, Register t2, Register t3, Label& slow);\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/macroAssembler_aarch64.hpp","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -55,0 +55,1 @@\n+#include \"utilities\/globalDefinitions.hpp\"\n@@ -1797,2 +1798,1 @@\n-    } else {\n-      assert(LockingMode == LM_LIGHTWEIGHT, \"must be\");\n+    } else if (LockingMode == LM_LIGHTWEIGHT) {\n@@ -1800,0 +1800,4 @@\n+    } else {\n+      assert(LockingMode == LM_PLACEHOLDER, \"must be\");\n+      __ str(zr, Address(lock_reg, mark_word_offset));\n+      __ placeholder_lock(obj_reg, swap_reg, tmp, lock_tmp, slow_path_lock);\n@@ -1939,2 +1943,1 @@\n-    } else {\n-      assert(LockingMode == LM_LIGHTWEIGHT, \"\");\n+    } else if (LockingMode == LM_LIGHTWEIGHT) {\n@@ -1943,0 +1946,4 @@\n+    }  else {\n+      assert(LockingMode == LM_PLACEHOLDER, \"\");\n+      __ placeholder_unlock(obj_reg, old_hdr, swap_reg, lock_tmp, slow_path_unlock);\n+      __ decrement(Address(rthread, JavaThread::held_monitor_count_offset()));\n","filename":"src\/hotspot\/cpu\/aarch64\/sharedRuntime_aarch64.cpp","additions":11,"deletions":4,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -3636,1 +3636,6 @@\n-    __ sub(r3, r3, sizeof(oopDesc));\n+    if (UseCompactObjectHeaders) {\n+      assert(is_aligned(oopDesc::base_offset_in_bytes(), BytesPerLong), \"oop base offset must be 8-byte-aligned\");\n+      __ sub(r3, r3, oopDesc::base_offset_in_bytes());\n+    } else {\n+      __ sub(r3, r3, sizeof(oopDesc));\n+    }\n@@ -3641,1 +3646,6 @@\n-      __ add(r2, r0, sizeof(oopDesc));\n+      if (UseCompactObjectHeaders) {\n+        assert(is_aligned(oopDesc::base_offset_in_bytes(), BytesPerLong), \"oop base offset must be 8-byte-aligned\");\n+        __ add(r2, r0, oopDesc::base_offset_in_bytes());\n+      } else {\n+        __ add(r2, r0, sizeof(oopDesc));\n+      }\n@@ -3651,5 +3661,9 @@\n-    __ mov(rscratch1, (intptr_t)markWord::prototype().value());\n-    __ str(rscratch1, Address(r0, oopDesc::mark_offset_in_bytes()));\n-    __ store_klass_gap(r0, zr);  \/\/ zero klass gap for compressed oops\n-    __ store_klass(r0, r4);      \/\/ store klass last\n-\n+    if (UseCompactObjectHeaders) {\n+      __ ldr(rscratch1, Address(r4, Klass::prototype_header_offset()));\n+      __ str(rscratch1, Address(r0, oopDesc::mark_offset_in_bytes()));\n+    } else {\n+      __ mov(rscratch1, (intptr_t)markWord::prototype().value());\n+      __ str(rscratch1, Address(r0, oopDesc::mark_offset_in_bytes()));\n+      __ store_klass_gap(r0, zr);  \/\/ zero klass gap for compressed oops\n+      __ store_klass(r0, r4);      \/\/ store klass last\n+    }\n","filename":"src\/hotspot\/cpu\/aarch64\/templateTable_aarch64.cpp","additions":21,"deletions":7,"binary":false,"changes":28,"status":"modified"},{"patch":"@@ -220,0 +220,4 @@\n+void LoadKlassStub::emit_code(LIR_Assembler* ce) {\n+  \/\/ Currently not needed.\n+  Unimplemented();\n+}\n","filename":"src\/hotspot\/cpu\/arm\/c1_CodeStubs_arm.cpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -156,1 +156,1 @@\n-                                       int header_size, int element_size,\n+                                       int header_size_in_bytes, int element_size,\n@@ -159,1 +159,0 @@\n-  const int header_size_in_bytes = header_size * BytesPerWord;\n","filename":"src\/hotspot\/cpu\/arm\/c1_MacroAssembler_arm.cpp","additions":1,"deletions":2,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -55,1 +55,1 @@\n-                      int header_size, int element_size,\n+                      int header_size_in_bytes, int element_size,\n","filename":"src\/hotspot\/cpu\/arm\/c1_MacroAssembler_arm.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -301,0 +301,4 @@\n+void LoadKlassStub::emit_code(LIR_Assembler* ce) {\n+  \/\/ Currently not needed.\n+  Unimplemented();\n+}\n","filename":"src\/hotspot\/cpu\/ppc\/c1_CodeStubs_ppc.cpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -256,0 +256,5 @@\n+void LoadKlassStub::emit_code(LIR_Assembler* ce) {\n+  \/\/ Currently not needed.\n+  Unimplemented();\n+}\n+\n","filename":"src\/hotspot\/cpu\/s390\/c1_CodeStubs_s390.cpp","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -33,0 +33,1 @@\n+#include \"runtime\/objectMonitor.hpp\"\n@@ -281,0 +282,11 @@\n+void LoadKlassStub::emit_code(LIR_Assembler* ce) {\n+  assert(UseCompactObjectHeaders, \"only with compact headers\");\n+  __ bind(_entry);\n+#ifdef _LP64\n+  Register d = _result->as_register();\n+  __ movq(d, Address(d, OM_OFFSET_NO_MONITOR_VALUE_TAG(header)));\n+  __ jmp(_continuation);\n+#else\n+  __ should_not_reach_here();\n+#endif\n+}\n","filename":"src\/hotspot\/cpu\/x86\/c1_CodeStubs_x86.cpp","additions":12,"deletions":0,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -42,0 +42,1 @@\n+#include \"runtime\/globals.hpp\"\n@@ -45,0 +46,1 @@\n+#include \"utilities\/globalDefinitions.hpp\"\n@@ -3050,0 +3052,1 @@\n+  Register tmp2 = UseCompactObjectHeaders ? rscratch2 : noreg;\n@@ -3174,2 +3177,0 @@\n-  Address src_klass_addr = Address(src, oopDesc::klass_offset_in_bytes());\n-  Address dst_klass_addr = Address(dst, oopDesc::klass_offset_in_bytes());\n@@ -3241,7 +3242,1 @@\n-      if (UseCompressedClassPointers) {\n-        __ movl(tmp, src_klass_addr);\n-        __ cmpl(tmp, dst_klass_addr);\n-      } else {\n-        __ movptr(tmp, src_klass_addr);\n-        __ cmpptr(tmp, dst_klass_addr);\n-      }\n+      __ cmp_klass(src, dst, tmp, tmp2);\n@@ -3306,0 +3301,1 @@\n+        Address dst_klass_addr = Address(dst, oopDesc::klass_offset_in_bytes());\n@@ -3407,1 +3403,0 @@\n-\n@@ -3409,3 +3404,1 @@\n-\n-      if (UseCompressedClassPointers)          __ cmpl(tmp, dst_klass_addr);\n-      else                   __ cmpptr(tmp, dst_klass_addr);\n+      __ cmp_klass(tmp, dst, tmp2);\n@@ -3413,2 +3406,1 @@\n-      if (UseCompressedClassPointers)          __ cmpl(tmp, src_klass_addr);\n-      else                   __ cmpptr(tmp, src_klass_addr);\n+      __ cmp_klass(tmp, src, tmp2);\n@@ -3417,2 +3409,1 @@\n-      if (UseCompressedClassPointers)          __ cmpl(tmp, dst_klass_addr);\n-      else                   __ cmpptr(tmp, dst_klass_addr);\n+      __ cmp_klass(tmp, dst, tmp2);\n@@ -3488,1 +3479,1 @@\n-    Register tmp = LockingMode == LM_LIGHTWEIGHT ? op->scratch_opr()->as_register() : noreg;\n+    Register tmp = LockingMode == LM_LIGHTWEIGHT || LockingMode == LM_PLACEHOLDER ? op->scratch_opr()->as_register() : noreg;\n@@ -3514,1 +3505,16 @@\n-  if (UseCompressedClassPointers) {\n+  if (UseCompactObjectHeaders) {\n+    Register tmp = rscratch1;\n+    assert_different_registers(tmp, obj);\n+    assert_different_registers(tmp, result);\n+\n+    \/\/ Check if we can take the (common) fast path, if obj is unlocked.\n+    __ movq(result, Address(obj, oopDesc::mark_offset_in_bytes()));\n+    if (LockingMode != LM_PLACEHOLDER) {\n+      __ testb(result, markWord::monitor_value);\n+      __ jcc(Assembler::notZero, *op->stub()->entry());\n+      __ bind(*op->stub()->continuation());\n+    }\n+    \/\/ Fast-path: shift and decode Klass*.\n+    __ shrq(result, markWord::klass_shift);\n+    __ decode_klass_not_null(result, tmp);\n+  } else if (UseCompressedClassPointers) {\n@@ -3519,0 +3525,1 @@\n+  {\n@@ -3520,0 +3527,1 @@\n+  }\n","filename":"src\/hotspot\/cpu\/x86\/c1_LIRAssembler_x86.cpp","additions":27,"deletions":19,"binary":false,"changes":46,"status":"modified"},{"patch":"@@ -322,1 +322,1 @@\n-  LIR_Opr tmp = LockingMode == LM_LIGHTWEIGHT ? new_register(T_ADDRESS) : LIR_OprFact::illegalOpr;\n+  LIR_Opr tmp = LockingMode == LM_LIGHTWEIGHT || LockingMode == LM_PLACEHOLDER ? new_register(T_ADDRESS) : LIR_OprFact::illegalOpr;\n","filename":"src\/hotspot\/cpu\/x86\/c1_LIRGenerator_x86.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -64,0 +64,3 @@\n+  } else if (LockingMode == LM_PLACEHOLDER) {\n+    \/\/ null check obj. load_klass performs load if DiagnoseSyncOnValueBasedClasses != 0.\n+    testptr(hdr, Address(obj));\n@@ -66,1 +69,10 @@\n-  if (LockingMode == LM_LIGHTWEIGHT) {\n+  if (LockingMode == LM_PLACEHOLDER) {\n+    movptr(Address(disp_hdr), 0);\n+#ifdef _LP64\n+    const Register thread = r15_thread;\n+#else\n+    const Register thread = disp_hdr;\n+    get_thread(thread);\n+#endif\n+    placeholder_lock(obj, hdr, thread, tmp, slow_case);\n+  } else if (LockingMode == LM_LIGHTWEIGHT) {\n@@ -121,0 +133,1 @@\n+  assert(LockingMode != LM_MONITOR, \"not handled\");\n@@ -125,1 +138,1 @@\n-  if (LockingMode != LM_LIGHTWEIGHT) {\n+  if (LockingMode == LM_LEGACY) {\n@@ -138,1 +151,10 @@\n-  if (LockingMode == LM_LIGHTWEIGHT) {\n+  if (LockingMode == LM_PLACEHOLDER) {\n+#ifdef _LP64\n+    placeholder_unlock(obj, disp_hdr, r15_thread, hdr, slow_case);\n+#else\n+    \/\/ This relies on the implementation of paceholder_unlock knowing that it\n+    \/\/ will clobber its thread when using EAX.\n+    get_thread(disp_hdr);\n+    placeholder_unlock(obj, disp_hdr, disp_hdr, hdr, slow_case);\n+#endif\n+  } else if (LockingMode == LM_LIGHTWEIGHT) {\n@@ -174,2 +196,1 @@\n-  assert_different_registers(obj, klass, len);\n-  movptr(Address(obj, oopDesc::mark_offset_in_bytes()), checked_cast<int32_t>(markWord::prototype().value()));\n+  assert_different_registers(obj, klass, len, t1, t2);\n@@ -177,1 +198,5 @@\n-  if (UseCompressedClassPointers) { \/\/ Take care not to kill klass\n+  if (UseCompactObjectHeaders) {\n+    movptr(t1, Address(klass, Klass::prototype_header_offset()));\n+    movptr(Address(obj, oopDesc::mark_offset_in_bytes()), t1);\n+  } else if (UseCompressedClassPointers) { \/\/ Take care not to kill klass\n+    movptr(Address(obj, oopDesc::mark_offset_in_bytes()), checked_cast<int32_t>(markWord::prototype().value()));\n@@ -184,0 +209,1 @@\n+    movptr(Address(obj, oopDesc::mark_offset_in_bytes()), checked_cast<int32_t>(markWord::prototype().value()));\n@@ -186,1 +212,0 @@\n-\n@@ -200,1 +225,1 @@\n-  else if (UseCompressedClassPointers) {\n+  else if (UseCompressedClassPointers && !UseCompactObjectHeaders) {\n@@ -234,1 +259,3 @@\n-\n+  if (UseCompactObjectHeaders) {\n+    assert(hdr_size_in_bytes == 8, \"check object headers size\");\n+  }\n","filename":"src\/hotspot\/cpu\/x86\/c1_MacroAssembler_x86.cpp","additions":36,"deletions":9,"binary":false,"changes":45,"status":"modified"},{"patch":"@@ -26,1 +26,0 @@\n-#include \"opto\/c2_MacroAssembler.hpp\"\n@@ -28,0 +27,3 @@\n+#include \"opto\/c2_MacroAssembler.hpp\"\n+#include \"runtime\/javaThread.hpp\"\n+#include \"runtime\/lockStack.hpp\"\n@@ -31,0 +33,2 @@\n+#include \"utilities\/globalDefinitions.hpp\"\n+#include \"utilities\/macros.hpp\"\n@@ -146,0 +150,91 @@\n+#ifdef _LP64\n+int C2LoadNKlassStub::max_size() const {\n+  return 10;\n+}\n+\n+void C2LoadNKlassStub::emit(C2_MacroAssembler& masm) {\n+  __ bind(entry());\n+  Register d = dst();\n+  __ movq(d, Address(d, OM_OFFSET_NO_MONITOR_VALUE_TAG(header)));\n+  __ jmp(continuation());\n+}\n+#endif\n+\n+int C2FastUnlockPlaceholderStub::max_size() const {\n+  return 128;\n+}\n+\n+void C2FastUnlockPlaceholderStub::emit(C2_MacroAssembler& masm) {\n+  assert(_t == rax, \"must be\");\n+\n+  Label restore_held_monitor_count_and_slow_path;\n+\n+  { \/\/ Restore lock-stack and handle the unlock in runtime.\n+\n+    __ bind(_push_and_slow_path);\n+#ifdef ASSERT\n+    \/\/ The obj was only cleared in debug.\n+    __ movl(_t, Address(_thread, JavaThread::lock_stack_top_offset()));\n+    __ movptr(Address(_thread, _t), _obj);\n+#endif\n+    __ addl(Address(_thread, JavaThread::lock_stack_top_offset()), oopSize);\n+  }\n+\n+  { \/\/ Restore held monitor and slow path.\n+\n+    __ bind(restore_held_monitor_count_and_slow_path);\n+    __ bind(_slow_path);\n+    \/\/ Restore held monitor count.\n+    __ increment(Address(_thread, JavaThread::held_monitor_count_offset()));\n+    \/\/ increment will always result in ZF = 0 (no overflows).\n+    \/\/ continuation is the slow_path.\n+    __ jmp(continuation());\n+  }\n+\n+  { \/\/ Handle monitor medium path.\n+\n+    __ bind(_check_successor);\n+\n+    Label fix_zf_and_unlocked;\n+    const Register monitor = _monitor;\n+\n+#ifndef _LP64\n+    \/\/ The owner may be anonymous, see comment in x86_64 section.\n+    __ movptr(Address(monitor, OM_OFFSET_NO_MONITOR_VALUE_TAG(owner)), _thread);\n+    __ jmpb(restore_held_monitor_count_and_slow_path);\n+#else \/\/ _LP64\n+    \/\/ The owner may be anonymous and we removed the last obj entry in\n+    \/\/ the lock-stack. This loses the information about the owner.\n+    \/\/ Write the thread to the owner field so the runtime knows the owner.\n+    __ movptr(Address(monitor, OM_OFFSET_NO_MONITOR_VALUE_TAG(owner)), _thread);\n+\n+    \/\/ successor null check.\n+    __ cmpptr(Address(monitor, OM_OFFSET_NO_MONITOR_VALUE_TAG(succ)), NULL_WORD);\n+    __ jccb(Assembler::equal, restore_held_monitor_count_and_slow_path);\n+\n+    \/\/ Release lock.\n+    __ movptr(Address(monitor, OM_OFFSET_NO_MONITOR_VALUE_TAG(owner)), NULL_WORD);\n+\n+    \/\/ Fence.\n+    __ lock(); __ addl(Address(rsp, 0), 0);\n+\n+    \/\/ Recheck successor.\n+    __ cmpptr(Address(monitor, OM_OFFSET_NO_MONITOR_VALUE_TAG(succ)), NULL_WORD);\n+    \/\/ Seen a successor after the release -> fence we have handed of the monitor\n+    __ jccb(Assembler::notEqual, fix_zf_and_unlocked);\n+\n+    \/\/ Try to relock, if it fail the monitor has been handed over\n+    \/\/ TODO: Caveat, this may fail due to deflation, which does\n+    \/\/       not handle the monitor handoff. Currently only works\n+    \/\/       due to the responsible thread.\n+    __ xorptr(rax, rax);\n+    __ lock(); __ cmpxchgptr(_thread, Address(monitor, OM_OFFSET_NO_MONITOR_VALUE_TAG(owner)));\n+    __ jccb  (Assembler::equal, restore_held_monitor_count_and_slow_path);\n+#endif\n+\n+    __ bind(fix_zf_and_unlocked);\n+    __ xorl(rax, rax);\n+    __ jmp(unlocked());\n+  }\n+}\n+\n","filename":"src\/hotspot\/cpu\/x86\/c2_CodeStubs_x86.cpp","additions":96,"deletions":1,"binary":false,"changes":97,"status":"modified"},{"patch":"@@ -36,0 +36,1 @@\n+#include \"runtime\/basicLock.hpp\"\n@@ -37,0 +38,2 @@\n+#include \"runtime\/javaThread.inline.hpp\"\n+#include \"runtime\/lockStack.hpp\"\n@@ -39,0 +42,1 @@\n+#include \"runtime\/synchronizer.hpp\"\n@@ -43,0 +47,1 @@\n+#include \"utilities\/macros.hpp\"\n@@ -562,0 +567,1 @@\n+  assert(LockingMode != LM_PLACEHOLDER, \"uses fast_lock_placeholder\");\n@@ -760,0 +766,1 @@\n+  assert(LockingMode != LM_PLACEHOLDER, \"uses fast_unlock_placeholder\");\n@@ -1180,0 +1187,308 @@\n+void C2_MacroAssembler::fast_lock_placeholder(Register obj, Register box, Register rax_reg,\n+                                              Register t, Register thread) {\n+  assert(LockingMode == LM_PLACEHOLDER, \"must be\");\n+  assert(rax_reg == rax, \"Used for CAS\");\n+  assert_different_registers(obj, box, rax_reg, t, thread);\n+\n+  \/\/ Handle inflated monitor.\n+  Label inflated;\n+  \/\/ Finish fast lock successfully. ZF value is irrelevant.\n+  Label locked;\n+  \/\/ Finish fast lock unsuccessfully. MUST jump with ZF == 0\n+  Label slow_path;\n+\n+  \/\/ Clear box. TODO: Is this neccesarry? May also defer this to not write twice.\n+  movptr(Address(box, BasicLock::displaced_header_offset_in_bytes()), 0);\n+\n+  if (DiagnoseSyncOnValueBasedClasses != 0) {\n+    load_klass(rax_reg, obj, t);\n+    movl(rax_reg, Address(rax_reg, Klass::access_flags_offset()));\n+    testl(rax_reg, JVM_ACC_IS_VALUE_BASED_CLASS);\n+    jcc(Assembler::notZero, slow_path);\n+  }\n+\n+  const Register mark = t;\n+\n+  { \/\/ Placeholder Lock\n+\n+    Label push;\n+\n+    const Register top = box;\n+\n+    \/\/ Load the mark.\n+    movptr(mark, Address(obj, oopDesc::mark_offset_in_bytes()));\n+\n+    \/\/ Check for monitor (0b10).\n+    testptr(mark, markWord::monitor_value);\n+    jcc(Assembler::notZero, inflated);\n+\n+    \/\/ Prefetch top.\n+    movl(top, Address(thread, JavaThread::lock_stack_top_offset()));\n+\n+    \/\/ Check if lock-stack is full.\n+    cmpl(top, LockStack::end_offset() - 1);\n+    jcc(Assembler::greater, slow_path);\n+\n+    \/\/ Check if recursive.\n+    cmpptr(obj, Address(thread, top, Address::times_1, -oopSize));\n+    jccb(Assembler::equal, push);\n+\n+    \/\/ Try to lock. Transition lock bits 0b01 => 0b00\n+    movptr(rax_reg, mark);\n+    orptr(rax_reg, markWord::unlocked_value);\n+    andptr(mark, ~(int32_t)markWord::unlocked_value);\n+    lock(); cmpxchgptr(mark, Address(obj, oopDesc::mark_offset_in_bytes()));\n+    jcc(Assembler::notEqual, slow_path);\n+\n+    bind(push);\n+    \/\/ After successful lock, push object on lock-stack.\n+    movptr(Address(thread, top), obj);\n+    addl(Address(thread, JavaThread::lock_stack_top_offset()), oopSize);\n+    jmpb(locked);\n+  }\n+\n+  { \/\/ Handle inflated monitor.\n+    bind(inflated);\n+\n+    const Register monitor = t;\n+\n+    if (!OMUseC2Cache) {\n+      jmp(slow_path);\n+    } else {\n+      if (OMCacheHitRate) increment(Address(thread, JavaThread::lock_lookup_offset()));\n+\n+      \/\/ Fetch ObjectMonitor* from the cache or take the slow-path.\n+      Label monitor_found, loop;\n+      \/\/ Load cache address\n+      lea(t, Address(thread, JavaThread::om_cache_oops_offset()));\n+\n+      \/\/ Search for obj in cache.\n+      bind(loop);\n+\n+      \/\/ Check for match.\n+      cmpptr(obj, Address(t));\n+      jccb(Assembler::equal, monitor_found);\n+\n+      \/\/ Search until null encountered, guaranteed _null_sentinel at end.\n+      cmpptr(Address(t), 1);\n+      jcc(Assembler::below, slow_path); \/\/ 0 check, but with ZF=0 when *t == 0\n+      increment(t, oopSize);\n+      jmpb(loop);\n+\n+      \/\/ Cache hit.\n+      bind(monitor_found);\n+      movptr(monitor, Address(t, OMCache::oop_to_monitor_difference()));\n+      if (OMCacheHitRate) increment(Address(thread, JavaThread::lock_hit_offset()));\n+\n+      Label monitor_locked;\n+      \/\/ Lock the monitor.\n+      Label recursion;\n+      if (OMRecursiveFastPath) {\n+        \/\/ Check owner for recursion first.\n+        cmpptr(thread, Address(monitor, ObjectMonitor::owner_offset()));\n+        jccb(Assembler::equal, recursion);\n+      }\n+\n+      \/\/ CAS owner (null => current thread).\n+      xorptr(rax, rax);\n+      lock(); cmpxchgptr(thread, Address(monitor, ObjectMonitor::owner_offset()));\n+      jccb(Assembler::equal, monitor_locked);\n+\n+      if (OMRecursiveFastPath) {\n+        \/\/ Recursion already checked.\n+        jmpb(slow_path);\n+      } else {\n+        \/\/ Check if recursive.\n+        cmpptr(thread, rax);\n+        jccb(Assembler::notEqual, slow_path);\n+      }\n+\n+      \/\/ Recursive.\n+      bind(recursion);\n+      increment(Address(monitor, ObjectMonitor::recursions_offset()));\n+\n+      bind(monitor_locked);\n+      \/\/ Cache the monitor for unlock\n+      movptr(Address(box, BasicLock::displaced_header_offset_in_bytes()), monitor);\n+    }\n+  }\n+\n+  bind(locked);\n+  increment(Address(thread, JavaThread::held_monitor_count_offset()));\n+  \/\/ Set ZF = 1\n+  xorl(rax_reg, rax_reg);\n+\n+#ifdef ASSERT\n+  \/\/ Check that locked label is reached with ZF set.\n+  Label zf_correct;\n+  jccb(Assembler::zero, zf_correct);\n+  stop(\"Fast Lock ZF != 1\");\n+#endif\n+\n+  bind(slow_path);\n+#ifdef ASSERT\n+  \/\/ Check that slow_path label is reached with ZF not set.\n+  jccb(Assembler::notZero, zf_correct);\n+  stop(\"Fast Lock ZF != 0\");\n+  bind(zf_correct);\n+#endif\n+  \/\/ C2 uses the value of ZF to determine the continuation.\n+}\n+\n+void C2_MacroAssembler::fast_unlock_placeholder(Register obj, Register reg_rax, Register t, Register thread) {\n+  assert(LockingMode == LM_PLACEHOLDER, \"must be\");\n+  assert(reg_rax == rax, \"Used for CAS\");\n+  assert_different_registers(obj, reg_rax, t);\n+\n+  \/\/ Handle inflated monitor.\n+  Label inflated, inflated_check_lock_stack;\n+  \/\/ Finish fast unlock successfully.  MUST jump with ZF == 1\n+  Label unlocked;\n+\n+  \/\/ Assume success.\n+  decrement(Address(thread, JavaThread::held_monitor_count_offset()));\n+\n+  const Register mark = t;\n+  const Register monitor = t;\n+  const Register top = t;\n+  const Register box = reg_rax;\n+\n+  Label dummy;\n+  C2FastUnlockPlaceholderStub* stub = nullptr;\n+\n+  if (!Compile::current()->output()->in_scratch_emit_size()) {\n+    stub = new (Compile::current()->comp_arena()) C2FastUnlockPlaceholderStub(obj, monitor, reg_rax, thread);\n+    Compile::current()->output()->add_stub(stub);\n+  }\n+\n+  Label& push_and_slow_path = stub == nullptr ? dummy : stub->push_and_slow_path();\n+  Label& check_successor = stub == nullptr ? dummy : stub->check_successor();\n+  Label& slow_path = stub == nullptr ? dummy : stub->slow_path();\n+\n+  { \/\/ Placeholder Unlock\n+\n+    \/\/ Load top.\n+    movl(top, Address(thread, JavaThread::lock_stack_top_offset()));\n+\n+    \/\/ Check if obj is top of lock-stack.\n+    cmpptr(obj, Address(thread, top, Address::times_1, -oopSize));\n+    \/\/ Top of lock stack was not obj. Must be monitor.\n+    jcc(Assembler::notEqual, inflated_check_lock_stack);\n+\n+    \/\/ Pop lock-stack.\n+    DEBUG_ONLY(movptr(Address(thread, top, Address::times_1, -oopSize), 0);)\n+    subl(Address(thread, JavaThread::lock_stack_top_offset()), oopSize);\n+\n+    \/\/ Check if recursive.\n+    cmpptr(obj, Address(thread, top, Address::times_1, -2 * oopSize));\n+    jcc(Assembler::equal, unlocked);\n+\n+    \/\/ We elide the monitor check, let the CAS fail instead.\n+\n+    \/\/ Load mark.\n+    movptr(mark, Address(obj, oopDesc::mark_offset_in_bytes()));\n+\n+    \/\/ Try to unlock. Transition lock bits 0b00 => 0b01\n+    movptr(reg_rax, mark);\n+    andptr(reg_rax, ~(int32_t)markWord::lock_mask);\n+    orptr(mark, markWord::unlocked_value);\n+    lock(); cmpxchgptr(mark, Address(obj, oopDesc::mark_offset_in_bytes()));\n+    jcc(Assembler::notEqual, push_and_slow_path);\n+    jmp(unlocked);\n+  }\n+\n+\n+  { \/\/ Handle inflated monitor.\n+    bind(inflated_check_lock_stack);\n+#ifdef ASSERT\n+    Label check_done;\n+    subl(top, oopSize);\n+    cmpl(top, in_bytes(JavaThread::lock_stack_base_offset()));\n+    jcc(Assembler::below, check_done);\n+    cmpptr(obj, Address(thread, top));\n+    jccb(Assembler::notEqual, inflated_check_lock_stack);\n+    stop(\"Fast Unlock lock on stack\");\n+    bind(check_done);\n+    const Register mark = t;\n+    movptr(mark, Address(obj, oopDesc::mark_offset_in_bytes()));\n+    testptr(mark, markWord::monitor_value);\n+    jccb(Assembler::notZero, inflated);\n+    stop(\"Fast Unlock not monitor\");\n+#endif\n+\n+    bind(inflated);\n+\n+    if (!OMUseC2Cache) {\n+      jmp(slow_path);\n+    } else {\n+      if (OMCacheHitRate) increment(Address(thread, JavaThread::unlock_lookup_offset()));\n+      movptr(monitor, Address(box, BasicLock::displaced_header_offset_in_bytes()));\n+      \/\/ TODO: Figure out the correctness surrounding the owner field here. Obj is not on the lock stack\n+      \/\/       but this means this thread must have locked on the inflated monitor at some point. So it\n+      \/\/       should not be anonymous.\n+      cmpptr(monitor, 2);\n+      jcc(Assembler::below, slow_path);\n+\n+      if (OMCacheHitRate) increment(Address(thread, JavaThread::unlock_hit_offset()));\n+#ifndef _LP64\n+        \/\/ TODO: Unify 32 with 64. Should just be a straight up use 64 on 32. We have the registers here.\n+        \/\/ Check if recursive.\n+        xorptr(reg_rax, reg_rax);\n+        orptr(reg_rax, Address(monitor, ObjectMonitor::recursions_offset()));\n+        jcc(Assembler::notZero, check_successor);\n+\n+        \/\/ Check if the entry lists are empty.\n+        movptr(reg_rax, Address(monitor, ObjectMonitor::EntryList_offset()));\n+        orptr(reg_rax, Address(monitor, ObjectMonitor::cxq_offset()));\n+        jcc(Assembler::notZero, check_successor);\n+\n+        \/\/ Release lock.\n+        movptr(Address(monitor, ObjectMonitor::owner_offset()), NULL_WORD);\n+#else \/\/ _LP64\n+        Label recursive;\n+\n+        \/\/ Check if recursive.\n+        cmpptr(Address(monitor,ObjectMonitor::recursions_offset()),0);\n+        jccb(Assembler::notEqual, recursive);\n+\n+        \/\/ Check if the entry lists are empty.\n+        movptr(reg_rax, Address(monitor, ObjectMonitor::cxq_offset()));\n+        orptr(reg_rax, Address(monitor, ObjectMonitor::EntryList_offset()));\n+        jcc(Assembler::notZero, check_successor);\n+\n+        \/\/ Release lock.\n+        movptr(Address(monitor, ObjectMonitor::owner_offset()), NULL_WORD);\n+        jmpb(unlocked);\n+\n+        \/\/ Recursive unlock.\n+        bind(recursive);\n+        decrement(Address(monitor, ObjectMonitor::recursions_offset()));\n+        xorl(t, t);\n+#endif\n+    }\n+  }\n+\n+  bind(unlocked);\n+  if (stub != nullptr) {\n+    bind(stub->unlocked());\n+  }\n+\n+#ifdef ASSERT\n+  \/\/ Check that unlocked label is reached with ZF set.\n+  Label zf_correct;\n+  jccb(Assembler::zero, zf_correct);\n+  stop(\"Fast Unlock ZF != 1\");\n+#endif\n+\n+  if (stub != nullptr) {\n+    bind(stub->continuation());\n+  }\n+#ifdef ASSERT\n+  \/\/ Check that stub->continuation() label is reached with ZF not set.\n+  jccb(Assembler::notZero, zf_correct);\n+  stop(\"Fast Unlock ZF != 0\");\n+  bind(zf_correct);\n+#endif\n+  \/\/ C2 uses the value of ZF to determine the continuation.\n+}\n+\n@@ -6497,0 +6812,25 @@\n+\n+#ifdef _LP64\n+void C2_MacroAssembler::load_nklass_compact_c2(Register dst, Register obj, Register index, Address::ScaleFactor scale, int disp) {\n+  \/\/ Note: Don't clobber obj anywhere in that method!\n+\n+  \/\/ The incoming address is pointing into obj-start + klass_offset_in_bytes. We need to extract\n+  \/\/ obj-start, so that we can load from the object's mark-word instead. Usually the address\n+  \/\/ comes as obj-start in obj and klass_offset_in_bytes in disp. However, sometimes C2\n+  \/\/ emits code that pre-computes obj-start + klass_offset_in_bytes into a register, and\n+  \/\/ then passes that register as obj and 0 in disp. The following code extracts the base\n+  \/\/ and offset to load the mark-word.\n+  int offset = oopDesc::mark_offset_in_bytes() + disp - oopDesc::klass_offset_in_bytes();\n+  movq(dst, Address(obj, index, scale, offset));\n+\n+  if (LockingMode != LM_PLACEHOLDER) {\n+    C2LoadNKlassStub* stub = new (Compile::current()->comp_arena()) C2LoadNKlassStub(dst);\n+    Compile::current()->output()->add_stub(stub);\n+\n+    testb(dst, markWord::monitor_value);\n+    jcc(Assembler::notZero, stub->entry());\n+    bind(stub->continuation());\n+  }\n+  shrq(dst, markWord::klass_shift);\n+}\n+#endif\n","filename":"src\/hotspot\/cpu\/x86\/c2_MacroAssembler_x86.cpp","additions":340,"deletions":0,"binary":false,"changes":340,"status":"modified"},{"patch":"@@ -50,0 +50,4 @@\n+  void fast_lock_placeholder(Register obj, Register box, Register rax_reg,\n+                             Register t, Register thread);\n+  void fast_unlock_placeholder(Register obj, Register reg_rax, Register t, Register thread);\n+\n@@ -503,0 +507,2 @@\n+  void load_nklass_compact_c2(Register dst, Register obj, Register index, Address::ScaleFactor scale, int disp);\n+\n","filename":"src\/hotspot\/cpu\/x86\/c2_MacroAssembler_x86.hpp","additions":6,"deletions":0,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -45,0 +45,1 @@\n+#include \"utilities\/globalDefinitions.hpp\"\n@@ -1188,1 +1189,10 @@\n-    if (LockingMode == LM_LIGHTWEIGHT) {\n+    if (LockingMode == LM_PLACEHOLDER) {\n+      movptr(Address(lock_reg, mark_offset), 0);\n+#ifdef _LP64\n+      const Register thread = r15_thread;\n+#else\n+      const Register thread = lock_reg;\n+      get_thread(thread);\n+#endif\n+      placeholder_lock(obj_reg, swap_reg, thread, tmp_reg, slow_case);\n+    } else if (LockingMode == LM_LIGHTWEIGHT) {\n@@ -1257,1 +1267,1 @@\n-    if (LockingMode == LM_LIGHTWEIGHT) {\n+    if (LockingMode == LM_LIGHTWEIGHT || LockingMode == LM_PLACEHOLDER) {\n@@ -1310,1 +1320,10 @@\n-    if (LockingMode == LM_LIGHTWEIGHT) {\n+    if (LockingMode == LM_PLACEHOLDER) {\n+#ifdef _LP64\n+      placeholder_unlock(obj_reg, swap_reg, r15_thread, header_reg, slow_case);\n+#else\n+      \/\/ This relies on the implementation of placeholder_unlock knowing that it\n+      \/\/ will clobber its thread when using EAX.\n+      get_thread(swap_reg);\n+      placeholder_unlock(obj_reg, swap_reg, swap_reg, header_reg, slow_case);\n+#endif\n+    } else if (LockingMode == LM_LIGHTWEIGHT) {\n","filename":"src\/hotspot\/cpu\/x86\/interp_masm_x86.cpp","additions":22,"deletions":3,"binary":false,"changes":25,"status":"modified"},{"patch":"@@ -5328,0 +5328,19 @@\n+#ifdef _LP64\n+void MacroAssembler::load_nklass_compact(Register dst, Register src) {\n+  assert(UseCompactObjectHeaders, \"expect compact object headers\");\n+\n+  Label fast;\n+  movq(dst, Address(src, oopDesc::mark_offset_in_bytes()));\n+  if (LockingMode != LM_PLACEHOLDER) {\n+    testb(dst, markWord::monitor_value);\n+    jccb(Assembler::zero, fast);\n+\n+    \/\/ Fetch displaced header\n+    movq(dst, Address(dst, OM_OFFSET_NO_MONITOR_VALUE_TAG(header)));\n+\n+    bind(fast);\n+  }\n+  shrq(dst, markWord::klass_shift);\n+}\n+#endif\n+\n@@ -5332,1 +5351,4 @@\n-  if (UseCompressedClassPointers) {\n+  if (UseCompactObjectHeaders) {\n+    load_nklass_compact(dst, src);\n+    decode_klass_not_null(dst, tmp);\n+  } else if (UseCompressedClassPointers) {\n@@ -5337,0 +5359,1 @@\n+  {\n@@ -5338,0 +5361,1 @@\n+  }\n@@ -5341,0 +5365,1 @@\n+  assert(!UseCompactObjectHeaders, \"not with compact headers\");\n@@ -5352,0 +5377,33 @@\n+void MacroAssembler::cmp_klass(Register klass, Register obj, Register tmp) {\n+#ifdef _LP64\n+  if (UseCompactObjectHeaders) {\n+    load_nklass_compact(tmp, obj);\n+    cmpl(klass, tmp);\n+  } else if (UseCompressedClassPointers) {\n+    cmpl(klass, Address(obj, oopDesc::klass_offset_in_bytes()));\n+  } else\n+#endif\n+  {\n+    cmpptr(klass, Address(obj, oopDesc::klass_offset_in_bytes()));\n+  }\n+}\n+\n+void MacroAssembler::cmp_klass(Register src, Register dst, Register tmp1, Register tmp2) {\n+#ifdef _LP64\n+  if (UseCompactObjectHeaders) {\n+    assert(tmp2 != noreg, \"need tmp2\");\n+    assert_different_registers(src, dst, tmp1, tmp2);\n+    load_nklass_compact(tmp1, src);\n+    load_nklass_compact(tmp2, dst);\n+    cmpl(tmp1, tmp2);\n+  } else if (UseCompressedClassPointers) {\n+    movl(tmp1, Address(src, oopDesc::klass_offset_in_bytes()));\n+    cmpl(tmp1, Address(dst, oopDesc::klass_offset_in_bytes()));\n+  } else\n+#endif\n+  {\n+    movptr(tmp1, Address(src, oopDesc::klass_offset_in_bytes()));\n+    cmpptr(tmp1, Address(dst, oopDesc::klass_offset_in_bytes()));\n+  }\n+}\n+\n@@ -5399,0 +5457,1 @@\n+  assert(!UseCompactObjectHeaders, \"Don't use with compact headers\");\n@@ -9994,0 +10053,109 @@\n+  testptr(reg_rax, markWord::monitor_value);\n+  jcc(Assembler::notZero, push_and_slow);\n+\n+#ifdef ASSERT\n+  \/\/ Check header not unlocked (0b01).\n+  Label not_unlocked;\n+  testptr(reg_rax, markWord::unlocked_value);\n+  jcc(Assembler::zero, not_unlocked);\n+  stop(\"lightweight_unlock already unlocked\");\n+  bind(not_unlocked);\n+#endif\n+\n+  \/\/ Try to unlock. Transition lock bits 0b00 => 0b01\n+  movptr(tmp, reg_rax);\n+  orptr(tmp, markWord::unlocked_value);\n+  lock(); cmpxchgptr(tmp, Address(obj, oopDesc::mark_offset_in_bytes()));\n+  jcc(Assembler::equal, unlocked);\n+\n+  bind(push_and_slow);\n+  \/\/ Restore lock-stack and handle the unlock in runtime.\n+  if (thread == reg_rax) {\n+    \/\/ On x86_32 we may lose the thread.\n+    get_thread(thread);\n+  }\n+#ifdef ASSERT\n+  movl(top, Address(thread, JavaThread::lock_stack_top_offset()));\n+  movptr(Address(thread, top), obj);\n+#endif\n+  addl(Address(thread, JavaThread::lock_stack_top_offset()), oopSize);\n+  jmp(slow);\n+\n+  bind(unlocked);\n+}\n+\n+\/\/ Implements placeholder-locking.\n+\/\/\n+\/\/ obj: the object to be locked\n+\/\/ reg_rax: rax\n+\/\/ thread: the thread which attempts to lock obj\n+\/\/ tmp: a temporary register\n+void MacroAssembler::placeholder_lock(Register obj, Register reg_rax, Register thread, Register tmp, Label& slow) {\n+  assert(reg_rax == rax, \"\");\n+  assert_different_registers(obj, reg_rax, thread, tmp);\n+\n+  Label push;\n+  const Register top = tmp;\n+\n+  \/\/ Load top.\n+  movl(top, Address(thread, JavaThread::lock_stack_top_offset()));\n+\n+  \/\/ Check if the lock-stack is full.\n+  cmpl(top, LockStack::end_offset());\n+  jcc(Assembler::greaterEqual, slow);\n+\n+  \/\/ Check for recursion.\n+  cmpptr(obj, Address(thread, top, Address::times_1, -oopSize));\n+  jcc(Assembler::equal, push);\n+\n+  \/\/ Check header for monitor (0b10).\n+  movptr(reg_rax, Address(obj, oopDesc::mark_offset_in_bytes()));\n+  testptr(reg_rax, markWord::monitor_value);\n+  jcc(Assembler::notZero, slow);\n+\n+  \/\/ Try to lock. Transition lock bits 0b01 => 0b00\n+  movptr(tmp, reg_rax);\n+  andptr(tmp, ~(int32_t)markWord::unlocked_value);\n+  orptr(reg_rax, markWord::unlocked_value);\n+  lock(); cmpxchgptr(tmp, Address(obj, oopDesc::mark_offset_in_bytes()));\n+  jcc(Assembler::notEqual, slow);\n+\n+  \/\/ Restore top, CAS clobbers register.\n+  movl(top, Address(thread, JavaThread::lock_stack_top_offset()));\n+\n+  bind(push);\n+  \/\/ After successful lock, push object on lock-stack.\n+  movptr(Address(thread, top), obj);\n+  incrementl(top, oopSize);\n+  movl(Address(thread, JavaThread::lock_stack_top_offset()), top);\n+}\n+\n+\/\/ Implements placeholder-unlocking.\n+\/\/\n+\/\/ obj: the object to be unlocked\n+\/\/ reg_rax: rax\n+\/\/ thread: the thread, may be EAX on x86_32\n+\/\/ tmp: a temporary register\n+void MacroAssembler::placeholder_unlock(Register obj, Register reg_rax, Register thread, Register tmp, Label& slow) {\n+  assert(reg_rax == rax, \"\");\n+  assert_different_registers(obj, reg_rax, tmp);\n+  LP64_ONLY(assert_different_registers(obj, reg_rax, thread, tmp);)\n+\n+  Label unlocked, push_and_slow;\n+  const Register top = tmp;\n+\n+  \/\/ Check if obj is top of lock-stack.\n+  movl(top, Address(thread, JavaThread::lock_stack_top_offset()));\n+  cmpptr(obj, Address(thread, top, Address::times_1, -oopSize));\n+  jcc(Assembler::notEqual, slow);\n+\n+  \/\/ Pop lock-stack.\n+  DEBUG_ONLY(movptr(Address(thread, top, Address::times_1, -oopSize), 0);)\n+  subl(Address(thread, JavaThread::lock_stack_top_offset()), oopSize);\n+\n+  \/\/ Check if recursive.\n+  cmpptr(obj, Address(thread, top, Address::times_1, -2 * oopSize));\n+  jcc(Assembler::equal, unlocked);\n+\n+  \/\/ Not recursive. Check header for monitor (0b10).\n+  movptr(reg_rax, Address(obj, oopDesc::mark_offset_in_bytes()));\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.cpp","additions":169,"deletions":1,"binary":false,"changes":170,"status":"modified"},{"patch":"@@ -367,0 +367,3 @@\n+#ifdef _LP64\n+  void load_nklass_compact(Register dst, Register src);\n+#endif\n@@ -370,0 +373,8 @@\n+  \/\/ Compares the Klass pointer of an object to a given Klass (which might be narrow,\n+  \/\/ depending on UseCompressedClassPointers).\n+  void cmp_klass(Register klass, Register dst, Register tmp);\n+\n+  \/\/ Compares the Klass pointer of two objects o1 and o2. Result is in the condition flags.\n+  \/\/ Uses t1 and t2 as temporary registers.\n+  void cmp_klass(Register src, Register dst, Register tmp1, Register tmp2);\n+\n@@ -2038,0 +2049,3 @@\n+\n+  void placeholder_lock(Register obj, Register reg_rax, Register thread, Register tmp, Label& slow);\n+  void placeholder_unlock(Register obj, Register reg_rax, Register thread, Register tmp, Label& slow);\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.hpp","additions":14,"deletions":0,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -64,1 +64,1 @@\n-  if (LockingMode == LM_LIGHTWEIGHT) {\n+  if (LockingMode == LM_LIGHTWEIGHT || LockingMode == LM_PLACEHOLDER) {\n@@ -79,2 +79,7 @@\n-  __ shrptr(result, markWord::hash_shift);\n-  __ andptr(result, markWord::hash_mask);\n+  if (UseCompactObjectHeaders) {\n+    __ shrptr(result, markWord::hash_shift_compact);\n+    __ andptr(result, markWord::hash_mask_compact);\n+  } else {\n+    __ shrptr(result, markWord::hash_shift);\n+    __ andptr(result, markWord::hash_mask);\n+  }\n","filename":"src\/hotspot\/cpu\/x86\/sharedRuntime_x86.cpp","additions":8,"deletions":3,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -41,0 +41,1 @@\n+#include \"runtime\/globals.hpp\"\n@@ -49,0 +50,1 @@\n+#include \"utilities\/globalDefinitions.hpp\"\n@@ -1695,0 +1697,3 @@\n+    } else if (LockingMode == LM_PLACEHOLDER){\n+      __ movptr(Address(lock_reg, mark_word_offset), 0);\n+      __ placeholder_lock(obj_reg, swap_reg, thread, lock_reg, slow_path_lock);\n@@ -1852,0 +1857,3 @@\n+    } else if (LockingMode == LM_PLACEHOLDER) {\n+      __ placeholder_unlock(obj_reg, swap_reg, thread, lock_reg, slow_path_unlock);\n+      __ dec_held_monitor_count();\n@@ -1936,0 +1944,5 @@\n+    if (LockingMode == LM_PLACEHOLDER) {\n+      \/\/ Reload the lock addr. Clobbered by lightweight_lock.\n+      __ lea(lock_reg, Address(rbp, lock_slot_rbp_offset));\n+    }\n+\n","filename":"src\/hotspot\/cpu\/x86\/sharedRuntime_x86_32.cpp","additions":13,"deletions":0,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -26,0 +26,1 @@\n+#include \"utilities\/globalDefinitions.hpp\"\n@@ -2171,2 +2172,1 @@\n-    } else {\n-      assert(LockingMode == LM_LIGHTWEIGHT, \"must be\");\n+    } else if (LockingMode == LM_LIGHTWEIGHT) {\n@@ -2174,0 +2174,4 @@\n+    } else {\n+      assert(LockingMode == LM_PLACEHOLDER, \"must be\");\n+      __ movptr(Address(lock_reg, mark_word_offset), 0);\n+      __ placeholder_lock(obj_reg, swap_reg, r15_thread, rscratch1, slow_path_lock);\n@@ -2313,2 +2317,1 @@\n-    } else {\n-      assert(LockingMode == LM_LIGHTWEIGHT, \"must be\");\n+    } else if (LockingMode == LM_LIGHTWEIGHT) {\n@@ -2316,0 +2319,4 @@\n+      __ dec_held_monitor_count();\n+    } else {\n+      assert(LockingMode == LM_PLACEHOLDER, \"must be\");\n+      __ placeholder_unlock(obj_reg, swap_reg, r15_thread, lock_reg, slow_path_unlock);\n","filename":"src\/hotspot\/cpu\/x86\/sharedRuntime_x86_64.cpp","additions":11,"deletions":4,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -4088,1 +4088,6 @@\n-    __ decrement(rdx, sizeof(oopDesc));\n+    if (UseCompactObjectHeaders) {\n+      assert(is_aligned(oopDesc::base_offset_in_bytes(), BytesPerLong), \"oop base offset must be 8-byte-aligned\");\n+      __ decrement(rdx, oopDesc::base_offset_in_bytes());\n+    } else {\n+      __ decrement(rdx, sizeof(oopDesc));\n+    }\n@@ -4110,2 +4115,9 @@\n-    __ movptr(Address(rax, rdx, Address::times_8, sizeof(oopDesc) - 1*oopSize), rcx);\n-    NOT_LP64(__ movptr(Address(rax, rdx, Address::times_8, sizeof(oopDesc) - 2*oopSize), rcx));\n+    if (UseCompactObjectHeaders) {\n+      assert(is_aligned(oopDesc::base_offset_in_bytes(), BytesPerLong), \"oop base offset must be 8-byte-aligned\");\n+      int header_size = oopDesc::base_offset_in_bytes();\n+      __ movptr(Address(rax, rdx, Address::times_8, header_size - 1*oopSize), rcx);\n+      NOT_LP64(__ movptr(Address(rax, rdx, Address::times_8, header_size - 2*oopSize), rcx));\n+    } else {\n+      __ movptr(Address(rax, rdx, Address::times_8, sizeof(oopDesc) - 1*oopSize), rcx);\n+      NOT_LP64(__ movptr(Address(rax, rdx, Address::times_8, sizeof(oopDesc) - 2*oopSize), rcx));\n+    }\n@@ -4118,3 +4130,8 @@\n-    __ movptr(Address(rax, oopDesc::mark_offset_in_bytes()),\n-              (intptr_t)markWord::prototype().value()); \/\/ header\n-    __ pop(rcx);   \/\/ get saved klass back in the register.\n+    if (UseCompactObjectHeaders) {\n+      __ pop(rcx);   \/\/ get saved klass back in the register.\n+      __ movptr(rbx, Address(rcx, Klass::prototype_header_offset()));\n+      __ movptr(Address(rax, oopDesc::mark_offset_in_bytes ()), rbx);\n+    } else {\n+      __ movptr(Address(rax, oopDesc::mark_offset_in_bytes()),\n+                (intptr_t)markWord::prototype().value()); \/\/ header\n+      __ pop(rcx);   \/\/ get saved klass back in the register.\n@@ -4122,2 +4139,2 @@\n-    __ xorl(rsi, rsi); \/\/ use zero reg to clear memory (shorter code)\n-    __ store_klass_gap(rax, rsi);  \/\/ zero klass gap for compressed oops\n+      __ xorl(rsi, rsi); \/\/ use zero reg to clear memory (shorter code)\n+      __ store_klass_gap(rax, rsi);  \/\/ zero klass gap for compressed oops\n@@ -4125,1 +4142,2 @@\n-    __ store_klass(rax, rcx, rscratch1);  \/\/ klass\n+      __ store_klass(rax, rcx, rscratch1);  \/\/ klass\n+    }\n","filename":"src\/hotspot\/cpu\/x86\/templateTable_x86.cpp","additions":27,"deletions":9,"binary":false,"changes":36,"status":"modified"},{"patch":"@@ -13767,1 +13767,1 @@\n-  predicate(LockingMode != LM_LIGHTWEIGHT && !Compile::current()->use_rtm());\n+  predicate(LockingMode != LM_LIGHTWEIGHT && LockingMode != LM_PLACEHOLDER && !Compile::current()->use_rtm());\n@@ -13781,1 +13781,1 @@\n-  predicate(LockingMode != LM_LIGHTWEIGHT);\n+  predicate(LockingMode != LM_LIGHTWEIGHT && LockingMode != LM_PLACEHOLDER);\n@@ -13818,0 +13818,26 @@\n+instruct cmpFastLockPlaceholder(eFlagsReg cr, eRegP object, eBXRegP box, eAXRegI tmp, eRegP scr, eRegP thread) %{\n+  predicate(LockingMode == LM_PLACEHOLDER);\n+  match(Set cr (FastLock object box));\n+  effect(TEMP tmp, TEMP scr, USE_KILL box, TEMP thread);\n+  ins_cost(300);\n+  format %{ \"FASTLOCK $object,$box\\t! kills $box,$tmp,$scr\" %}\n+  ins_encode %{\n+    __ get_thread($thread$$Register);\n+    __ fast_lock_placeholder($object$$Register, $box$$Register, $tmp$$Register, $scr$$Register, $thread$$Register);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct cmpFastUnlockPlaceholder(eFlagsReg cr, eRegP object, eAXRegP box, eRegP tmp, eRegP thread) %{\n+  predicate(LockingMode == LM_PLACEHOLDER);\n+  match(Set cr (FastUnlock object box));\n+  effect(TEMP tmp, USE_KILL box, TEMP thread);\n+  ins_cost(300);\n+  format %{ \"FASTUNLOCK $object,$box\\t! kills $box,$tmp\" %}\n+  ins_encode %{\n+    __ get_thread($thread$$Register);\n+    __ fast_unlock_placeholder($object$$Register, $box$$Register, $tmp$$Register, $thread$$Register);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n","filename":"src\/hotspot\/cpu\/x86\/x86_32.ad","additions":28,"deletions":2,"binary":false,"changes":30,"status":"modified"},{"patch":"@@ -4410,0 +4410,1 @@\n+  predicate(!UseCompactObjectHeaders);\n@@ -4420,0 +4421,15 @@\n+instruct loadNKlassCompactHeaders(rRegN dst, memory mem, rFlagsReg cr)\n+%{\n+  predicate(UseCompactObjectHeaders);\n+  match(Set dst (LoadNKlass mem));\n+  effect(KILL cr);\n+  ins_cost(125); \/\/ XXX\n+  format %{ \"movl    $dst, $mem\\t# compressed klass ptr\" %}\n+  ins_encode %{\n+    Register index = $mem$$index != 4 ? $mem$$index$$Register : noreg;\n+    Address::ScaleFactor sf = (index != noreg) ? static_cast<Address::ScaleFactor>($mem$$scale) : Address::no_scale;\n+    __ load_nklass_compact_c2($dst$$Register, $mem$$base$$Register, index, sf, $mem$$disp);\n+  %}\n+  ins_pipe(pipe_slow); \/\/ XXX\n+%}\n+\n@@ -11740,0 +11756,1 @@\n+  predicate(!UseCompactObjectHeaders);\n@@ -12386,1 +12403,1 @@\n-  predicate(LockingMode != LM_LIGHTWEIGHT && !Compile::current()->use_rtm());\n+  predicate(LockingMode != LM_LIGHTWEIGHT && LockingMode != LM_PLACEHOLDER && !Compile::current()->use_rtm());\n@@ -12399,1 +12416,1 @@\n-  predicate(LockingMode != LM_LIGHTWEIGHT);\n+  predicate(LockingMode != LM_LIGHTWEIGHT && LockingMode != LM_PLACEHOLDER);\n@@ -12434,0 +12451,24 @@\n+instruct cmpFastLockPlaceholder(rFlagsReg cr, rRegP object, rbx_RegP box, rax_RegI tmp, rRegP scr) %{\n+  predicate(LockingMode == LM_PLACEHOLDER);\n+  match(Set cr (FastLock object box));\n+  effect(TEMP tmp, TEMP scr, USE_KILL box);\n+  ins_cost(300);\n+  format %{ \"fastlock $object,$box\\t! kills $box,$tmp,$scr\" %}\n+  ins_encode %{\n+    __ fast_lock_placeholder($object$$Register, $box$$Register, $tmp$$Register, $scr$$Register, r15_thread);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct cmpFastUnlockPlaceholder(rFlagsReg cr, rRegP object, rax_RegP box, rRegP tmp) %{\n+  predicate(LockingMode == LM_PLACEHOLDER);\n+  match(Set cr (FastUnlock object box));\n+  effect(TEMP tmp, USE_KILL box);\n+  ins_cost(300);\n+  format %{ \"fastunlock $object,$box\\t! kills $box,$tmp\" %}\n+  ins_encode %{\n+    __ fast_unlock_placeholder($object$$Register, $box$$Register, $tmp$$Register, r15_thread);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n","filename":"src\/hotspot\/cpu\/x86\/x86_64.ad","additions":43,"deletions":2,"binary":false,"changes":45,"status":"modified"},{"patch":"@@ -584,0 +584,18 @@\n+class LoadKlassStub: public CodeStub {\n+private:\n+  LIR_Opr          _result;\n+\n+public:\n+  LoadKlassStub(LIR_Opr result) :\n+    CodeStub(), _result(result) {};\n+\n+  virtual void emit_code(LIR_Assembler* e);\n+  virtual void visit(LIR_OpVisitState* visitor) {\n+    visitor->do_temp(_result);\n+    visitor->do_output(_result);\n+  }\n+#ifndef PRODUCT\n+virtual void print_name(outputStream* out) const { out->print(\"LoadKlassStub\"); }\n+#endif \/\/ PRODUCT\n+};\n+\n","filename":"src\/hotspot\/share\/c1\/c1_CodeStubs.hpp","additions":18,"deletions":0,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -893,0 +893,1 @@\n+      if (opLoadKlass->_stub) do_stub(opLoadKlass->_stub);\n@@ -1073,0 +1074,3 @@\n+  if (stub()) {\n+    masm->append_code_stub(stub());\n+  }\n@@ -2049,0 +2053,3 @@\n+  if (stub()) {\n+    out->print(\"[lbl:\" INTPTR_FORMAT \"]\", p2i(stub()->entry()));\n+  }\n","filename":"src\/hotspot\/share\/c1\/c1_LIR.cpp","additions":7,"deletions":0,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -1906,0 +1906,1 @@\n+  CodeStub* _stub;\n@@ -1907,1 +1908,1 @@\n-  LIR_OpLoadKlass(LIR_Opr obj, LIR_Opr result, CodeEmitInfo* info)\n+  LIR_OpLoadKlass(LIR_Opr obj, LIR_Opr result, CodeEmitInfo* info, CodeStub* stub)\n@@ -1910,0 +1911,1 @@\n+    , _stub(stub)\n@@ -1913,0 +1915,1 @@\n+  CodeStub* stub()     const { return _stub; }\n@@ -2378,1 +2381,1 @@\n-  void load_klass(LIR_Opr obj, LIR_Opr result, CodeEmitInfo* info) { append(new LIR_OpLoadKlass(obj, result, info)); }\n+  void load_klass(LIR_Opr obj, LIR_Opr result, CodeEmitInfo* info, CodeStub* stub) { append(new LIR_OpLoadKlass(obj, result, info, stub)); }\n","filename":"src\/hotspot\/share\/c1\/c1_LIR.hpp","additions":5,"deletions":2,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -42,0 +42,1 @@\n+#include \"runtime\/globals.hpp\"\n@@ -46,0 +47,1 @@\n+#include \"utilities\/globalDefinitions.hpp\"\n@@ -1259,1 +1261,3 @@\n-  __ load_klass(obj, klass, null_check_info);\n+  CodeStub* slow_path = UseCompactObjectHeaders && LockingMode != LM_PLACEHOLDER\n+                      ? new LoadKlassStub(klass) : nullptr;\n+  __ load_klass(obj, klass, null_check_info, slow_path);\n","filename":"src\/hotspot\/share\/c1\/c1_LIRGenerator.cpp","additions":5,"deletions":1,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -76,0 +76,2 @@\n+#include \"utilities\/globalDefinitions.hpp\"\n+#include \"utilities\/macros.hpp\"\n@@ -760,2 +762,2 @@\n-  assert(LockingMode == LM_LIGHTWEIGHT || obj == lock->obj(), \"must match\");\n-  SharedRuntime::monitor_enter_helper(obj, LockingMode == LM_LIGHTWEIGHT ? nullptr : lock->lock(), current);\n+  assert(LockingMode == LM_LIGHTWEIGHT NOT_LP64(|| LockingMode == LM_PLACEHOLDER) || obj == lock->obj(), \"must match\");\n+  SharedRuntime::monitor_enter_helper(obj, LockingMode == LM_LIGHTWEIGHT NOT_LP64(|| LockingMode == LM_PLACEHOLDER) ? nullptr : lock->lock(), current);\n","filename":"src\/hotspot\/share\/c1\/c1_Runtime1.cpp","additions":4,"deletions":2,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -730,0 +730,9 @@\n+#ifdef _LP64\n+    if (UseCompactObjectHeaders) {\n+      Klass* requested_k = to_requested(k);\n+      address narrow_klass_base = _requested_static_archive_bottom; \/\/ runtime encoding base == runtime mapping start\n+      const int narrow_klass_shift = precomputed_narrow_klass_shift;\n+      narrowKlass nk = CompressedKlassPointers::encode_not_null(requested_k, narrow_klass_base, narrow_klass_shift);\n+      k->set_prototype_header(markWord::prototype().set_narrow_klass(nk));\n+    }\n+#endif \/\/_LP64\n@@ -832,1 +841,1 @@\n-  const int narrow_klass_shift = ArchiveHeapWriter::precomputed_narrow_klass_shift;\n+  const int narrow_klass_shift = precomputed_narrow_klass_shift;\n","filename":"src\/hotspot\/share\/cds\/archiveBuilder.cpp","additions":10,"deletions":1,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -93,0 +93,14 @@\n+public:\n+  \/\/ The archive contains pre-computed narrow Klass IDs in two places:\n+  \/\/ - in the header of archived java objects (only if the archive contains java heap portions)\n+  \/\/ - within the prototype markword of archived Klass structures.\n+  \/\/ These narrow Klass ids have been computed at dump time with the following scheme:\n+  \/\/ 1) the encoding base must be the mapping start address.\n+  \/\/ 2) shift must be large enough to result in an encoding range that covers the runtime Klass range.\n+  \/\/    That Klass range is defined by CDS archive size and runtime class space size. Luckily, the maximum\n+  \/\/    size can be predicted: archive size is assumed to be <1G, class space size capped at 3G, and at\n+  \/\/    runtime we put both regions adjacent to each other. Therefore, runtime Klass range size < 4G.\n+  \/\/    Since nKlass itself is 32 bit, our encoding range len is 4G, and since we set the base directly\n+  \/\/    at mapping start, these 4G are enough. Therefore, we don't need to shift at all (shift=0).\n+  static constexpr int precomputed_narrow_klass_shift = 0;\n+\n","filename":"src\/hotspot\/share\/cds\/archiveBuilder.hpp","additions":14,"deletions":0,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -205,2 +205,7 @@\n-    oopDesc::set_mark(mem, markWord::prototype());\n-    oopDesc::release_set_klass(mem, k);\n+    if (UseCompactObjectHeaders) {\n+      narrowKlass nk = ArchiveBuilder::current()->get_requested_narrow_klass(k);\n+      oopDesc::release_set_mark(mem, markWord::prototype().set_narrow_klass(nk));\n+    } else {\n+      oopDesc::set_mark(mem, markWord::prototype());\n+      oopDesc::release_set_klass(mem, k);\n+    }\n@@ -272,1 +277,0 @@\n-  oopDesc::set_mark(mem, markWord::prototype());\n@@ -274,1 +278,6 @@\n-  cast_to_oop(mem)->set_narrow_klass(nk);\n+  if (UseCompactObjectHeaders) {\n+    oopDesc::release_set_mark(mem, markWord::prototype().set_narrow_klass(nk));\n+  } else {\n+    oopDesc::set_mark(mem, markWord::prototype());\n+    cast_to_oop(mem)->set_narrow_klass(nk);\n+  }\n@@ -474,1 +483,3 @@\n-  fake_oop->set_narrow_klass(nk);\n+  if (!UseCompactObjectHeaders) {\n+    fake_oop->set_narrow_klass(nk);\n+  }\n@@ -482,1 +493,5 @@\n-    fake_oop->set_mark(markWord::prototype().copy_set_hash(src_hash));\n+    if (UseCompactObjectHeaders) {\n+      fake_oop->set_mark(markWord::prototype().set_narrow_klass(nk).copy_set_hash(src_hash));\n+    } else {\n+      fake_oop->set_mark(markWord::prototype().copy_set_hash(src_hash));\n+    }\n","filename":"src\/hotspot\/share\/cds\/archiveHeapWriter.cpp","additions":21,"deletions":6,"binary":false,"changes":27,"status":"modified"},{"patch":"@@ -236,11 +236,0 @@\n-  \/\/ Archived heap object headers carry pre-computed narrow Klass ids calculated with the\n-  \/\/ following scheme:\n-  \/\/ 1) the encoding base must be the mapping start address.\n-  \/\/ 2) shift must be large enough to result in an encoding range that covers the runtime Klass range.\n-  \/\/    That Klass range is defined by CDS archive size and runtime class space size. Luckily, the maximum\n-  \/\/    size can be predicted: archive size is assumed to be <1G, class space size capped at 3G, and at\n-  \/\/    runtime we put both regions adjacent to each other. Therefore, runtime Klass range size < 4G.\n-  \/\/    Since nKlass itself is 32 bit, our encoding range len is 4G, and since we set the base directly\n-  \/\/    at mapping start, these 4G are enough. Therefore, we don't need to shift at all (shift=0).\n-  static constexpr int precomputed_narrow_klass_shift = 0;\n-\n","filename":"src\/hotspot\/share\/cds\/archiveHeapWriter.hpp","additions":0,"deletions":11,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -207,0 +207,1 @@\n+  _compact_headers = UseCompactObjectHeaders;\n@@ -273,0 +274,1 @@\n+  st->print_cr(\"- compact_headers:                %d\", _compact_headers);\n@@ -2002,1 +2004,1 @@\n-  \/\/ ArchiveHeapWriter::precomputed_narrow_klass_shift. We enforce this encoding at runtime (see\n+  \/\/ ArchiveBuilder::precomputed_narrow_klass_shift. We enforce this encoding at runtime (see\n@@ -2006,1 +2008,1 @@\n-  const int archive_narrow_klass_shift = ArchiveHeapWriter::precomputed_narrow_klass_shift;\n+  const int archive_narrow_klass_shift = ArchiveBuilder::precomputed_narrow_klass_shift;\n@@ -2395,0 +2397,8 @@\n+  if (compact_headers() != UseCompactObjectHeaders) {\n+    log_info(cds)(\"The shared archive file's UseCompactObjectHeaders setting (%s)\"\n+                  \" does not equal the current UseCompactObjectHeaders setting (%s).\",\n+                  _compact_headers          ? \"enabled\" : \"disabled\",\n+                  UseCompactObjectHeaders   ? \"enabled\" : \"disabled\");\n+    return false;\n+  }\n+\n","filename":"src\/hotspot\/share\/cds\/filemap.cpp","additions":12,"deletions":2,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -191,0 +191,1 @@\n+  bool   _compact_headers;                        \/\/ value of UseCompactObjectHeaders\n@@ -259,0 +260,1 @@\n+  bool compact_headers()                   const { return _compact_headers; }\n","filename":"src\/hotspot\/share\/cds\/filemap.hpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -1161,11 +1161,12 @@\n-#if INCLUDE_CDS_JAVA_HEAP\n-          \/\/ We archived objects with pre-computed narrow Klass id. Set up encoding such that these Ids stay valid.\n-          address precomputed_narrow_klass_base = cds_base;\n-          const int precomputed_narrow_klass_shift = ArchiveHeapWriter::precomputed_narrow_klass_shift;\n-          CompressedKlassPointers::initialize_for_given_encoding(\n-            cds_base, ccs_end - cds_base, \/\/ Klass range\n-            precomputed_narrow_klass_base, precomputed_narrow_klass_shift \/\/ precomputed encoding, see ArchiveHeapWriter\n-            );\n-#else\n-          CompressedKlassPointers::initialize (\n-            cds_base, ccs_end - cds_base \/\/ Klass range\n+          if (INCLUDE_CDS_JAVA_HEAP || UseCompactObjectHeaders) {\n+            \/\/ The CDS archive may contain narrow Klass IDs that were precomputed at archive generation time:\n+            \/\/ - every archived java object header (only if INCLUDE_CDS_JAVA_HEAP)\n+            \/\/ - every archived Klass' prototype   (only if +UseCompactObjectHeaders)\n+            \/\/\n+            \/\/ In order for those IDs to still be valid, we need to dictate base and shift: base should be the\n+            \/\/ mapping start, shift the shift used at archive generation time.\n+            address precomputed_narrow_klass_base = cds_base;\n+            const int precomputed_narrow_klass_shift = ArchiveBuilder::precomputed_narrow_klass_shift;\n+            CompressedKlassPointers::initialize_for_given_encoding(\n+              cds_base, ccs_end - cds_base, \/\/ Klass range\n+              precomputed_narrow_klass_base, precomputed_narrow_klass_shift \/\/ precomputed encoding, see ArchiveBuilder\n@@ -1173,1 +1174,6 @@\n-#endif \/\/ INCLUDE_CDS_JAVA_HEAP\n+          } else {\n+            \/\/ Let JVM freely chose encoding base and shift\n+            CompressedKlassPointers::initialize (\n+              cds_base, ccs_end - cds_base \/\/ Klass range\n+              );\n+          }\n","filename":"src\/hotspot\/share\/cds\/metaspaceShared.cpp","additions":18,"deletions":12,"binary":false,"changes":30,"status":"modified"},{"patch":"@@ -252,0 +252,20 @@\n+\n+\/\/ ------------------------------------------------------------------\n+\/\/ ciKlass::prototype_header_offset\n+juint ciKlass::prototype_header_offset() {\n+  assert(is_loaded(), \"must be loaded\");\n+\n+  VM_ENTRY_MARK;\n+  Klass* this_klass = get_Klass();\n+  return in_bytes(this_klass->prototype_header_offset());\n+}\n+\n+\/\/ ------------------------------------------------------------------\n+\/\/ ciKlass::prototype_header\n+uintptr_t ciKlass::prototype_header() {\n+  assert(is_loaded(), \"must be loaded\");\n+\n+  VM_ENTRY_MARK;\n+  Klass* this_klass = get_Klass();\n+  return (uintptr_t)this_klass->prototype_header().to_pointer();\n+}\n","filename":"src\/hotspot\/share\/ci\/ciKlass.cpp","additions":20,"deletions":0,"binary":false,"changes":20,"status":"modified"},{"patch":"@@ -132,0 +132,3 @@\n+\n+  juint prototype_header_offset();\n+  uintptr_t prototype_header();\n","filename":"src\/hotspot\/share\/ci\/ciKlass.hpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -104,1 +104,1 @@\n-  static const int first_vtableStub_size =  64;\n+  static const int first_vtableStub_size = 64;\n","filename":"src\/hotspot\/share\/code\/vtableStubs.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -89,0 +89,1 @@\n+#include \"gc\/shared\/slidingForwarding.hpp\"\n@@ -1443,0 +1444,2 @@\n+  SlidingForwarding::initialize(heap_rs.region(), HeapRegion::GrainWords);\n+\n","filename":"src\/hotspot\/share\/gc\/g1\/g1CollectedHeap.cpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -43,0 +43,1 @@\n+#include \"gc\/shared\/slidingForwarding.hpp\"\n@@ -215,0 +216,2 @@\n+  SlidingForwarding::begin();\n+\n@@ -227,0 +230,2 @@\n+  SlidingForwarding::end();\n+\n@@ -402,1 +407,2 @@\n-void G1FullCollector::phase2c_prepare_serial_compaction() {\n+template <bool ALT_FWD>\n+void G1FullCollector::phase2c_prepare_serial_compaction_impl() {\n@@ -427,1 +433,1 @@\n-  G1SerialRePrepareClosure re_prepare(serial_cp, dense_prefix_top);\n+  G1SerialRePrepareClosure<ALT_FWD> re_prepare(serial_cp, dense_prefix_top);\n@@ -440,1 +446,10 @@\n-void G1FullCollector::phase2d_prepare_humongous_compaction() {\n+void G1FullCollector::phase2c_prepare_serial_compaction() {\n+  if (UseAltGCForwarding) {\n+    phase2c_prepare_serial_compaction_impl<true>();\n+  } else {\n+    phase2c_prepare_serial_compaction_impl<false>();\n+  }\n+}\n+\n+template <bool ALT_FWD>\n+void G1FullCollector::phase2d_prepare_humongous_compaction_impl() {\n@@ -462,1 +477,1 @@\n-        humongous_cp->forward_humongous(hr);\n+        humongous_cp->forward_humongous<ALT_FWD>(hr);\n@@ -475,0 +490,8 @@\n+void G1FullCollector::phase2d_prepare_humongous_compaction() {\n+  if (UseAltGCForwarding) {\n+    phase2d_prepare_humongous_compaction_impl<true>();\n+  } else {\n+    phase2d_prepare_humongous_compaction_impl<false>();\n+  }\n+}\n+\n","filename":"src\/hotspot\/share\/gc\/g1\/g1FullCollector.cpp","additions":27,"deletions":4,"binary":false,"changes":31,"status":"modified"},{"patch":"@@ -161,0 +161,2 @@\n+  template <bool ALT_FWD>\n+  void phase2c_prepare_serial_compaction_impl();\n@@ -162,0 +164,2 @@\n+  template <bool ALT_FWD>\n+  void phase2d_prepare_humongous_compaction_impl();\n","filename":"src\/hotspot\/share\/gc\/g1\/g1FullCollector.hpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -43,0 +43,1 @@\n+template <bool ALT_FWD>\n@@ -44,1 +45,1 @@\n-  G1AdjustClosure* _adjust_closure;\n+  G1AdjustClosure<ALT_FWD>* _adjust_closure;\n@@ -46,1 +47,1 @@\n-  G1AdjustLiveClosure(G1AdjustClosure* cl) :\n+  G1AdjustLiveClosure(G1AdjustClosure<ALT_FWD>* cl) :\n@@ -65,1 +66,11 @@\n-    G1AdjustClosure cl(_collector);\n+    if (UseAltGCForwarding) {\n+      return do_heap_region_impl<true>(r);\n+    } else {\n+      return do_heap_region_impl<false>(r);\n+    }\n+  }\n+\n+private:\n+  template <bool ALT_FWD>\n+  bool do_heap_region_impl(HeapRegion* r) {\n+    G1AdjustClosure<ALT_FWD> cl(_collector);\n@@ -73,1 +84,1 @@\n-      G1AdjustLiveClosure adjust(&cl);\n+      G1AdjustLiveClosure<ALT_FWD> adjust(&cl);\n@@ -84,2 +95,1 @@\n-    _hrclaimer(collector->workers()),\n-    _adjust(collector) {\n+    _hrclaimer(collector->workers()) {\n@@ -89,1 +99,2 @@\n-void G1FullGCAdjustTask::work(uint worker_id) {\n+template <bool ALT_FWD>\n+void G1FullGCAdjustTask::work_impl(uint worker_id) {\n@@ -97,0 +108,1 @@\n+  G1AdjustClosure<ALT_FWD> adjust(collector());\n@@ -100,1 +112,1 @@\n-    _weak_proc_task.work(worker_id, &always_alive, &_adjust);\n+    _weak_proc_task.work(worker_id, &always_alive, &adjust);\n@@ -103,3 +115,3 @@\n-  CLDToOopClosure adjust_cld(&_adjust, ClassLoaderData::_claim_stw_fullgc_adjust);\n-  CodeBlobToOopClosure adjust_code(&_adjust, CodeBlobToOopClosure::FixRelocations);\n-  _root_processor.process_all_roots(&_adjust, &adjust_cld, &adjust_code);\n+  CLDToOopClosure adjust_cld(&adjust, ClassLoaderData::_claim_stw_fullgc_adjust);\n+  CodeBlobToOopClosure adjust_code(&adjust, CodeBlobToOopClosure::FixRelocations);\n+  _root_processor.process_all_roots(&adjust, &adjust_cld, &adjust_code);\n@@ -112,0 +124,8 @@\n+\n+void G1FullGCAdjustTask::work(uint worker_id) {\n+  if (UseAltGCForwarding) {\n+    work_impl<true>(worker_id);\n+  } else {\n+    work_impl<false>(worker_id);\n+  }\n+}\n","filename":"src\/hotspot\/share\/gc\/g1\/g1FullGCAdjustTask.cpp","additions":31,"deletions":11,"binary":false,"changes":42,"status":"modified"},{"patch":"@@ -41,1 +41,0 @@\n-  G1AdjustClosure          _adjust;\n@@ -43,0 +42,2 @@\n+  template <bool ALT_FWD>\n+  void work_impl(uint worker_id);\n","filename":"src\/hotspot\/share\/gc\/g1\/g1FullGCAdjustTask.hpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -33,0 +33,1 @@\n+#include \"gc\/shared\/slidingForwarding.inline.hpp\"\n@@ -37,1 +38,2 @@\n-void G1FullGCCompactTask::G1CompactRegionClosure::clear_in_bitmap(oop obj) {\n+template <bool ALT_FWD>\n+void G1FullGCCompactTask::G1CompactRegionClosure<ALT_FWD>::clear_in_bitmap(oop obj) {\n@@ -42,1 +44,2 @@\n-size_t G1FullGCCompactTask::G1CompactRegionClosure::apply(oop obj) {\n+template <bool ALT_FWD>\n+size_t G1FullGCCompactTask::G1CompactRegionClosure<ALT_FWD>::apply(oop obj) {\n@@ -44,2 +47,2 @@\n-  if (obj->is_forwarded()) {\n-    G1FullGCCompactTask::copy_object_to_new_location(obj);\n+  if (SlidingForwarding::is_forwarded(obj)) {\n+    G1FullGCCompactTask::copy_object_to_new_location<ALT_FWD>(obj);\n@@ -54,0 +57,1 @@\n+template <bool ALT_FWD>\n@@ -55,2 +59,2 @@\n-  assert(obj->is_forwarded(), \"Sanity!\");\n-  assert(obj->forwardee() != obj, \"Object must have a new location\");\n+  assert(SlidingForwarding::is_forwarded(obj), \"Sanity!\");\n+  assert(SlidingForwarding::forwardee<ALT_FWD>(obj) != obj, \"Object must have a new location\");\n@@ -61,1 +65,1 @@\n-  HeapWord* destination = cast_from_oop<HeapWord*>(obj->forwardee());\n+  HeapWord* destination = cast_from_oop<HeapWord*>(SlidingForwarding::forwardee<ALT_FWD>(obj));\n@@ -81,2 +85,7 @@\n-    G1CompactRegionClosure compact(collector()->mark_bitmap());\n-    hr->apply_to_marked_objects(collector()->mark_bitmap(), &compact);\n+    if (UseAltGCForwarding) {\n+      G1CompactRegionClosure<true> compact(collector()->mark_bitmap());\n+      hr->apply_to_marked_objects(collector()->mark_bitmap(), &compact);\n+    } else {\n+      G1CompactRegionClosure<false> compact(collector()->mark_bitmap());\n+      hr->apply_to_marked_objects(collector()->mark_bitmap(), &compact);\n+    }\n@@ -108,3 +117,2 @@\n-void G1FullGCCompactTask::humongous_compaction() {\n-  GCTraceTime(Debug, gc, phases) tm(\"Phase 4: Humonguous Compaction\", collector()->scope()->timer());\n-\n+template <bool ALT_FWD>\n+void G1FullGCCompactTask::humongous_compaction_impl() {\n@@ -113,1 +121,10 @@\n-    compact_humongous_obj(hr);\n+    compact_humongous_obj<ALT_FWD>(hr);\n+  }\n+}\n+\n+void G1FullGCCompactTask::humongous_compaction() {\n+  GCTraceTime(Debug, gc, phases) tm(\"Phase 4: Humonguous Compaction\", collector()->scope()->timer());\n+  if (UseAltGCForwarding) {\n+    humongous_compaction_impl<true>();\n+  } else {\n+    humongous_compaction_impl<false>();\n@@ -117,0 +134,1 @@\n+template <bool ALT_FWD>\n@@ -124,1 +142,1 @@\n-  HeapWord* destination = cast_from_oop<HeapWord*>(obj->forwardee());\n+  HeapWord* destination = cast_from_oop<HeapWord*>(SlidingForwarding::forwardee<ALT_FWD>(obj));\n@@ -129,1 +147,1 @@\n-  copy_object_to_new_location(obj);\n+  copy_object_to_new_location<ALT_FWD>(obj);\n","filename":"src\/hotspot\/share\/gc\/g1\/g1FullGCCompactTask.cpp","additions":33,"deletions":15,"binary":false,"changes":48,"status":"modified"},{"patch":"@@ -44,0 +44,1 @@\n+  template <bool ALT_FWD>\n@@ -47,0 +48,1 @@\n+  template <bool ALT_FWD>\n@@ -49,0 +51,3 @@\n+  template <bool ALT_FWD>\n+  void humongous_compaction_impl();\n+\n@@ -60,0 +65,1 @@\n+  template <bool ALT_FWD>\n","filename":"src\/hotspot\/share\/gc\/g1\/g1FullGCCompactTask.hpp","additions":6,"deletions":0,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -30,0 +30,1 @@\n+#include \"gc\/shared\/slidingForwarding.inline.hpp\"\n@@ -96,0 +97,1 @@\n+template <bool ALT_FWD>\n@@ -109,2 +111,2 @@\n-    object->forward_to(cast_to_oop(_compaction_top));\n-    assert(object->is_forwarded(), \"must be forwarded\");\n+    SlidingForwarding::forward_to<ALT_FWD>(object, cast_to_oop(_compaction_top));\n+    assert(SlidingForwarding::is_forwarded(object), \"must be forwarded\");\n@@ -112,1 +114,1 @@\n-    assert(!object->is_forwarded(), \"must not be forwarded\");\n+    assert(SlidingForwarding::is_not_forwarded(object), \"must not be forwarded\");\n@@ -120,0 +122,3 @@\n+template void G1FullGCCompactionPoint::forward<true>(oop object, size_t size);\n+template void G1FullGCCompactionPoint::forward<false>(oop object, size_t size);\n+\n@@ -152,0 +157,1 @@\n+template <bool ALT_FWD>\n@@ -175,2 +181,2 @@\n-  obj->forward_to(cast_to_oop(dest_hr->bottom()));\n-  assert(obj->is_forwarded(), \"Object must be forwarded!\");\n+  SlidingForwarding::forward_to<ALT_FWD>(obj, cast_to_oop(dest_hr->bottom()));\n+  assert(SlidingForwarding::is_forwarded(obj), \"Object must be forwarded!\");\n@@ -187,0 +193,3 @@\n+template void G1FullGCCompactionPoint::forward_humongous<true>(HeapRegion* hr);\n+template void G1FullGCCompactionPoint::forward_humongous<false>(HeapRegion* hr);\n+\n","filename":"src\/hotspot\/share\/gc\/g1\/g1FullGCCompactionPoint.cpp","additions":14,"deletions":5,"binary":false,"changes":19,"status":"modified"},{"patch":"@@ -59,0 +59,1 @@\n+  template <bool ALT_FWD>\n@@ -60,0 +61,1 @@\n+  template <bool ALT_FWD>\n","filename":"src\/hotspot\/share\/gc\/g1\/g1FullGCCompactionPoint.hpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -76,0 +76,1 @@\n+template <bool ALT_FWD>\n","filename":"src\/hotspot\/share\/gc\/g1\/g1FullGCOopClosures.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2017, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2017, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -34,0 +34,1 @@\n+#include \"gc\/shared\/slidingForwarding.inline.hpp\"\n@@ -54,1 +55,2 @@\n-template <class T> inline void G1AdjustClosure::adjust_pointer(T* p) {\n+template <bool ALT_FWD>\n+template <class T> inline void G1AdjustClosure<ALT_FWD>::adjust_pointer(T* p) {\n@@ -68,2 +70,2 @@\n-  if (obj->is_forwarded()) {\n-    oop forwardee = obj->forwardee();\n+  if (SlidingForwarding::is_forwarded(obj)) {\n+    oop forwardee = SlidingForwarding::forwardee<ALT_FWD>(obj);\n@@ -77,2 +79,4 @@\n-inline void G1AdjustClosure::do_oop(oop* p)       { do_oop_work(p); }\n-inline void G1AdjustClosure::do_oop(narrowOop* p) { do_oop_work(p); }\n+template <bool ALT_FWD>\n+inline void G1AdjustClosure<ALT_FWD>::do_oop(oop* p)       { do_oop_work(p); }\n+template <bool ALT_FWD>\n+inline void G1AdjustClosure<ALT_FWD>::do_oop(narrowOop* p) { do_oop_work(p); }\n","filename":"src\/hotspot\/share\/gc\/g1\/g1FullGCOopClosures.inline.hpp","additions":10,"deletions":6,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -107,1 +107,2 @@\n-G1FullGCPrepareTask::G1PrepareCompactLiveClosure::G1PrepareCompactLiveClosure(G1FullGCCompactionPoint* cp) :\n+template <bool ALT_FWD>\n+G1FullGCPrepareTask::G1PrepareCompactLiveClosure<ALT_FWD>::G1PrepareCompactLiveClosure(G1FullGCCompactionPoint* cp) :\n@@ -110,1 +111,2 @@\n-size_t G1FullGCPrepareTask::G1PrepareCompactLiveClosure::apply(oop object) {\n+template <bool ALT_FWD>\n+size_t G1FullGCPrepareTask::G1PrepareCompactLiveClosure<ALT_FWD>::apply(oop object) {\n@@ -112,1 +114,1 @@\n-  _cp->forward(object, size);\n+  _cp->forward<ALT_FWD>(object, size);\n@@ -118,2 +120,7 @@\n-    G1PrepareCompactLiveClosure prepare_compact(_cp);\n-    hr->apply_to_marked_objects(_bitmap, &prepare_compact);\n+    if (UseAltGCForwarding) {\n+      G1PrepareCompactLiveClosure<true> prepare_compact(_cp);\n+      hr->apply_to_marked_objects(_bitmap, &prepare_compact);\n+    } else {\n+      G1PrepareCompactLiveClosure<false> prepare_compact(_cp);\n+      hr->apply_to_marked_objects(_bitmap, &prepare_compact);\n+    }\n","filename":"src\/hotspot\/share\/gc\/g1\/g1FullGCPrepareTask.cpp","additions":12,"deletions":5,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -92,0 +92,1 @@\n+  template <bool ALT_FWD>\n@@ -103,0 +104,1 @@\n+template <bool ALT_FWD>\n","filename":"src\/hotspot\/share\/gc\/g1\/g1FullGCPrepareTask.hpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -34,0 +34,1 @@\n+#include \"gc\/shared\/slidingForwarding.inline.hpp\"\n@@ -116,2 +117,3 @@\n-inline size_t G1SerialRePrepareClosure::apply(oop obj) {\n-  if (obj->is_forwarded()) {\n+template <bool ALT_FWD>\n+inline size_t G1SerialRePrepareClosure<ALT_FWD>::apply(oop obj) {\n+  if (SlidingForwarding::is_forwarded(obj)) {\n@@ -120,1 +122,1 @@\n-    if (cast_from_oop<HeapWord*>(obj->forwardee()) < _dense_prefix_top) {\n+    if (cast_from_oop<HeapWord*>(SlidingForwarding::forwardee<ALT_FWD>(obj)) < _dense_prefix_top) {\n@@ -127,1 +129,1 @@\n-  _cp->forward(obj, size);\n+  _cp->forward<ALT_FWD>(obj, size);\n","filename":"src\/hotspot\/share\/gc\/g1\/g1FullGCPrepareTask.inline.hpp","additions":6,"deletions":4,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -231,1 +231,1 @@\n-      forwardee = cast_to_oop(m.decode_pointer());\n+      forwardee = obj->forwardee(m);\n","filename":"src\/hotspot\/share\/gc\/g1\/g1OopClosures.inline.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -215,1 +215,1 @@\n-    obj = cast_to_oop(m.decode_pointer());\n+    obj = obj->forwardee(m);\n@@ -229,1 +229,1 @@\n-  assert(from_obj->is_objArray(), \"must be obj array\");\n+  assert(from_obj->forward_safe_klass()->is_objArray_klass(), \"must be obj array\");\n@@ -259,1 +259,1 @@\n-  assert(from_obj->is_objArray(), \"precondition\");\n+  assert(from_obj->forward_safe_klass()->is_objArray_klass(), \"precondition\");\n@@ -386,1 +386,1 @@\n-                                                  oop const old, size_t word_sz, uint age,\n+                                                  Klass* klass, size_t word_sz, uint age,\n@@ -390,1 +390,1 @@\n-    _g1h->gc_tracer_stw()->report_promotion_in_new_plab_event(old->klass(), word_sz * HeapWordSize, age,\n+    _g1h->gc_tracer_stw()->report_promotion_in_new_plab_event(klass, word_sz * HeapWordSize, age,\n@@ -394,1 +394,1 @@\n-    _g1h->gc_tracer_stw()->report_promotion_outside_plab_event(old->klass(), word_sz * HeapWordSize, age,\n+    _g1h->gc_tracer_stw()->report_promotion_outside_plab_event(klass, word_sz * HeapWordSize, age,\n@@ -401,1 +401,1 @@\n-                                                   oop old,\n+                                                   Klass* klass,\n@@ -424,1 +424,1 @@\n-      report_promotion_event(*dest_attr, old, word_sz, age, obj_ptr, node_index);\n+      report_promotion_event(*dest_attr, klass, word_sz, age, obj_ptr, node_index);\n@@ -461,1 +461,7 @@\n-  Klass* klass = old->klass();\n+  \/\/ NOTE: With compact headers, it is not safe to load the Klass* from o, because\n+  \/\/ that would access the mark-word, and the mark-word might change at any time by\n+  \/\/ concurrent promotion. The promoted mark-word would point to the forwardee, which\n+  \/\/ may not yet have completed copying. Therefore we must load the Klass* from\n+  \/\/ the mark-word that we have already loaded. This is safe, because we have checked\n+  \/\/ that this is not yet forwarded in the caller.\n+  Klass* klass = old->forward_safe_klass(old_mark);\n@@ -479,1 +485,1 @@\n-    obj_ptr = allocate_copy_slow(&dest_attr, old, word_sz, age, node_index);\n+    obj_ptr = allocate_copy_slow(&dest_attr, klass, word_sz, age, node_index);\n@@ -635,1 +641,1 @@\n-  oop forward_ptr = old->forward_to_atomic(old, m, memory_order_relaxed);\n+  oop forward_ptr = old->forward_to_self_atomic(m, memory_order_relaxed);\n","filename":"src\/hotspot\/share\/gc\/g1\/g1ParScanThreadState.cpp","additions":17,"deletions":11,"binary":false,"changes":28,"status":"modified"},{"patch":"@@ -176,1 +176,1 @@\n-                               oop old,\n+                               Klass* klass,\n@@ -211,1 +211,1 @@\n-                              oop const old, size_t word_sz, uint age,\n+                              Klass* klass, size_t word_sz, uint age,\n","filename":"src\/hotspot\/share\/gc\/g1\/g1ParScanThreadState.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -236,1 +236,2 @@\n-void MutableSpace::object_iterate(ObjectClosure* cl) {\n+template<bool COMPACT_HEADERS>\n+void MutableSpace::object_iterate_impl(ObjectClosure* cl) {\n@@ -245,3 +246,2 @@\n-    }\n-#ifdef ASSERT\n-    else {\n+      p += obj->size();\n+    } else {\n@@ -249,0 +249,8 @@\n+      if (COMPACT_HEADERS) {\n+        \/\/ It is safe to use the forwardee here. Parallel GC only uses\n+        \/\/ header-based forwarding during promotion. Full GC doesn't\n+        \/\/ use the object header for forwarding at all.\n+        p += obj->forwardee()->size();\n+      } else {\n+        p += obj->size();\n+      }\n@@ -250,2 +258,8 @@\n-#endif\n-    p += obj->size();\n+  }\n+}\n+\n+void MutableSpace::object_iterate(ObjectClosure* cl) {\n+  if (UseCompactObjectHeaders) {\n+    object_iterate_impl<true>(cl);\n+  } else {\n+    object_iterate_impl<false>(cl);\n","filename":"src\/hotspot\/share\/gc\/parallel\/mutableSpace.cpp","additions":20,"deletions":6,"binary":false,"changes":26,"status":"modified"},{"patch":"@@ -70,0 +70,3 @@\n+  template<bool COMPACT_HEADERS>\n+  void object_iterate_impl(ObjectClosure* cl);\n+\n","filename":"src\/hotspot\/share\/gc\/parallel\/mutableSpace.hpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -394,1 +394,3 @@\n-    HeapWord* test_addr = cast_from_oop<HeapWord*>(obj) + 1;\n+    \/\/ With compact headers, the objects can be one-word sized.\n+    size_t int_off = UseCompactObjectHeaders ? MIN2((size_t)1, obj->size() - 1) : 1;\n+    HeapWord* test_addr = cast_from_oop<HeapWord*>(obj) + int_off;\n","filename":"src\/hotspot\/share\/gc\/parallel\/psOldGen.cpp","additions":3,"deletions":1,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -284,1 +284,1 @@\n-  assert(old->is_objArray(), \"invariant\");\n+  assert(old->forward_safe_klass()->is_objArray_klass(), \"invariant\");\n@@ -322,1 +322,1 @@\n-  if (obj->forward_to_atomic(obj, obj_mark) == nullptr) {\n+  if (obj->forward_to_self_atomic(obj_mark) == nullptr) {\n","filename":"src\/hotspot\/share\/gc\/parallel\/psPromotionManager.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -108,1 +108,1 @@\n-  inline void promotion_trace_event(oop new_obj, oop old_obj, size_t obj_size,\n+  inline void promotion_trace_event(oop new_obj, Klass* klass, size_t obj_size,\n","filename":"src\/hotspot\/share\/gc\/parallel\/psPromotionManager.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -69,1 +69,1 @@\n-inline void PSPromotionManager::promotion_trace_event(oop new_obj, oop old_obj,\n+inline void PSPromotionManager::promotion_trace_event(oop new_obj, Klass* klass,\n@@ -82,1 +82,1 @@\n-        gc_tracer->report_promotion_in_new_plab_event(old_obj->klass(), obj_bytes,\n+        gc_tracer->report_promotion_in_new_plab_event(klass, obj_bytes,\n@@ -89,1 +89,1 @@\n-        gc_tracer->report_promotion_outside_plab_event(old_obj->klass(), obj_bytes,\n+        gc_tracer->report_promotion_outside_plab_event(klass, obj_bytes,\n@@ -156,1 +156,1 @@\n-    return cast_to_oop(m.decode_pointer());\n+    return o->forwardee(m);\n@@ -172,1 +172,8 @@\n-  size_t new_obj_size = o->size();\n+  \/\/ NOTE: With compact headers, it is not safe to load the Klass* from o, because\n+  \/\/ that would access the mark-word, and the mark-word might change at any time by\n+  \/\/ concurrent promotion. The promoted mark-word would point to the forwardee, which\n+  \/\/ may not yet have completed copying. Therefore we must load the Klass* from\n+  \/\/ the mark-word that we have already loaded. This is safe, because we have checked\n+  \/\/ that this is not yet forwarded in the caller.\n+  Klass* klass = o->forward_safe_klass(test_mark);\n+  size_t new_obj_size = o->size_given_klass(klass);\n@@ -187,1 +194,1 @@\n-          promotion_trace_event(new_obj, o, new_obj_size, age, false, nullptr);\n+          promotion_trace_event(new_obj, klass, new_obj_size, age, false, nullptr);\n@@ -197,1 +204,1 @@\n-            promotion_trace_event(new_obj, o, new_obj_size, age, false, &_young_lab);\n+            promotion_trace_event(new_obj, klass, new_obj_size, age, false, &_young_lab);\n@@ -223,1 +230,1 @@\n-          promotion_trace_event(new_obj, o, new_obj_size, age, true, nullptr);\n+          promotion_trace_event(new_obj, klass, new_obj_size, age, true, nullptr);\n@@ -233,1 +240,1 @@\n-            promotion_trace_event(new_obj, o, new_obj_size, age, true, &_old_lab);\n+            promotion_trace_event(new_obj, klass, new_obj_size, age, true, &_old_lab);\n@@ -256,3 +263,15 @@\n-  \/\/ Parallel GC claims with a release - so other threads might access this object\n-  \/\/ after claiming and they should see the \"completed\" object.\n-  ContinuationGCSupport::transform_stack_chunk(new_obj);\n+  if (UseCompactObjectHeaders) {\n+    \/\/ The copy above is not atomic. Make sure we have seen the proper mark\n+    \/\/ and re-install it into the copy, so that Klass* is guaranteed to be correct.\n+    markWord mark = o->mark();\n+    if (!mark.is_marked()) {\n+      new_obj->set_mark(mark);\n+      ContinuationGCSupport::transform_stack_chunk(new_obj);\n+    } else {\n+      \/\/ If we copied a mark-word that indicates 'forwarded' state, the object\n+      \/\/ installation would not succeed. We cannot access Klass* anymore either.\n+      \/\/ Skip the transformation.\n+    }\n+  } else {\n+    ContinuationGCSupport::transform_stack_chunk(new_obj);\n+  }\n","filename":"src\/hotspot\/share\/gc\/parallel\/psPromotionManager.inline.hpp","additions":31,"deletions":12,"binary":false,"changes":43,"status":"modified"},{"patch":"@@ -810,1 +810,1 @@\n-        obj->init_mark();\n+        obj->forward_safe_init_mark();\n@@ -834,1 +834,1 @@\n-  old->forward_to(old);\n+  old->forward_to_self();\n","filename":"src\/hotspot\/share\/gc\/serial\/defNewGeneration.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -51,0 +51,1 @@\n+#include \"gc\/shared\/slidingForwarding.inline.hpp\"\n@@ -198,0 +199,1 @@\n+  template <bool ALT_FWD>\n@@ -201,1 +203,1 @@\n-      obj->forward_to(cast_to_oop(new_addr));\n+      SlidingForwarding::forward_to<ALT_FWD>(obj, cast_to_oop(new_addr));\n@@ -221,0 +223,1 @@\n+  template <bool ALT_FWD>\n@@ -226,1 +229,1 @@\n-    oop new_obj = obj->forwardee();\n+    oop new_obj = SlidingForwarding::forwardee<ALT_FWD>(obj);\n@@ -255,0 +258,1 @@\n+  template <bool ALT_FWD>\n@@ -270,1 +274,1 @@\n-          forward_obj(obj, new_addr);\n+          forward_obj<ALT_FWD>(obj, new_addr);\n@@ -295,0 +299,9 @@\n+  void phase2_calculate_new_addr() {\n+    if (UseAltGCForwarding) {\n+      phase2_calculate_new_addr<true>();\n+    } else {\n+      phase2_calculate_new_addr<false>();\n+    }\n+  }\n+\n+  template <bool ALT_FWD>\n@@ -305,1 +318,1 @@\n-          size_t size = MarkSweep::adjust_pointers(cast_to_oop(cur_addr));\n+          size_t size = MarkSweep::adjust_pointers<ALT_FWD>(cast_to_oop(cur_addr));\n@@ -315,0 +328,9 @@\n+  void phase3_adjust_pointers() {\n+    if (UseAltGCForwarding) {\n+      phase3_adjust_pointers<true>();\n+    } else {\n+      phase3_adjust_pointers<false>();\n+    }\n+  }\n+\n+  template <bool ALT_FWD>\n@@ -322,1 +344,1 @@\n-      if (!cast_to_oop(cur_addr)->is_forwarded()) {\n+      if (SlidingForwarding::is_not_forwarded(cast_to_oop(cur_addr))) {\n@@ -328,1 +350,1 @@\n-        if (!cast_to_oop(cur_addr)->is_forwarded()) {\n+        if (SlidingForwarding::is_not_forwarded(cast_to_oop(cur_addr))) {\n@@ -332,1 +354,1 @@\n-        cur_addr += relocate(cur_addr);\n+        cur_addr += relocate<ALT_FWD>(cur_addr);\n@@ -342,0 +364,8 @@\n+\n+  void phase4_compact() {\n+    if (UseAltGCForwarding) {\n+      phase4_compact<true>();\n+    } else {\n+      phase4_compact<false>();\n+    }\n+  }\n@@ -451,0 +481,2 @@\n+  SlidingForwarding::begin();\n+\n@@ -472,8 +504,23 @@\n-    CodeBlobToOopClosure code_closure(&adjust_pointer_closure, CodeBlobToOopClosure::FixRelocations);\n-    gch->process_roots(SerialHeap::SO_AllCodeCache,\n-                       &adjust_pointer_closure,\n-                       &adjust_cld_closure,\n-                       &adjust_cld_closure,\n-                       &code_closure);\n-\n-    WeakProcessor::oops_do(&adjust_pointer_closure);\n+    if (UseAltGCForwarding) {\n+      AdjustPointerClosure<true> adjust_pointer_closure;\n+      CLDToOopClosure adjust_cld_closure(&adjust_pointer_closure, ClassLoaderData::_claim_stw_fullgc_adjust);\n+      CodeBlobToOopClosure code_closure(&adjust_pointer_closure, CodeBlobToOopClosure::FixRelocations);\n+      gch->process_roots(SerialHeap::SO_AllCodeCache,\n+                         &adjust_pointer_closure,\n+                         &adjust_cld_closure,\n+                         &adjust_cld_closure,\n+                         &code_closure);\n+\n+      WeakProcessor::oops_do(&adjust_pointer_closure);\n+    } else {\n+      AdjustPointerClosure<false> adjust_pointer_closure;\n+      CLDToOopClosure adjust_cld_closure(&adjust_pointer_closure, ClassLoaderData::_claim_stw_fullgc_adjust);\n+      CodeBlobToOopClosure code_closure(&adjust_pointer_closure, CodeBlobToOopClosure::FixRelocations);\n+      gch->process_roots(SerialHeap::SO_AllCodeCache,\n+                         &adjust_pointer_closure,\n+                         &adjust_cld_closure,\n+                         &adjust_cld_closure,\n+                         &code_closure);\n+\n+      WeakProcessor::oops_do(&adjust_pointer_closure);\n+    }\n@@ -492,0 +539,2 @@\n+  SlidingForwarding::end();\n+\n","filename":"src\/hotspot\/share\/gc\/serial\/genMarkSweep.cpp","additions":64,"deletions":15,"binary":false,"changes":79,"status":"modified"},{"patch":"@@ -63,1 +63,0 @@\n-CLDToOopClosure    MarkSweep::adjust_cld_closure(&adjust_pointer_closure, ClassLoaderData::_claim_stw_fullgc_adjust);\n@@ -167,0 +166,4 @@\n+  \/\/ Do the transform while we still have the header intact,\n+  \/\/ which might include important class information.\n+  ContinuationGCSupport::transform_stack_chunk(obj);\n+\n@@ -170,3 +173,1 @@\n-  obj->set_mark(markWord::prototype().set_marked());\n-\n-  ContinuationGCSupport::transform_stack_chunk(obj);\n+  obj->set_mark(obj->prototype_mark().set_marked());\n@@ -195,3 +196,2 @@\n-AdjustPointerClosure MarkSweep::adjust_pointer_closure;\n-\n-void MarkSweep::adjust_marks() {\n+template <bool ALT_FWD>\n+void MarkSweep::adjust_marks_impl() {\n@@ -200,1 +200,1 @@\n-    PreservedMarks::adjust_preserved_mark(_preserved_marks + i);\n+    PreservedMarks::adjust_preserved_mark<ALT_FWD>(_preserved_marks + i);\n@@ -207,0 +207,8 @@\n+void MarkSweep::adjust_marks() {\n+  if (UseAltGCForwarding) {\n+    adjust_marks_impl<true>();\n+  } else {\n+    adjust_marks_impl<false>();\n+  }\n+}\n+\n","filename":"src\/hotspot\/share\/gc\/serial\/markSweep.cpp","additions":16,"deletions":8,"binary":false,"changes":24,"status":"modified"},{"patch":"@@ -54,1 +54,0 @@\n-class AdjustPointerClosure;\n@@ -88,1 +87,0 @@\n-  friend class AdjustPointerClosure;\n@@ -128,2 +126,0 @@\n-  static AdjustPointerClosure adjust_pointer_closure;\n-  static CLDToOopClosure      adjust_cld_closure;\n@@ -145,0 +141,1 @@\n+  template <bool ALT_FWD>\n@@ -149,1 +146,2 @@\n-  template <class T> static inline void adjust_pointer(T* p);\n+  template <bool ALT_FWD, class T>\n+  static void adjust_pointer(T* p);\n@@ -155,0 +153,3 @@\n+  template <bool ALT_FWD>\n+  static void adjust_marks_impl();\n+\n@@ -182,0 +183,1 @@\n+template <bool ALT_FWD>\n","filename":"src\/hotspot\/share\/gc\/serial\/markSweep.hpp","additions":7,"deletions":5,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2000, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2000, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -34,0 +34,1 @@\n+#include \"gc\/shared\/slidingForwarding.inline.hpp\"\n@@ -42,1 +43,2 @@\n-template <class T> inline void MarkSweep::adjust_pointer(T* p) {\n+template <bool ALT_FWD, class T>\n+inline void MarkSweep::adjust_pointer(T* p) {\n@@ -48,2 +50,2 @@\n-    if (obj->is_forwarded()) {\n-      oop new_obj = obj->forwardee();\n+    if (SlidingForwarding::is_forwarded(obj)) {\n+      oop new_obj = SlidingForwarding::forwardee<ALT_FWD>(obj);\n@@ -56,0 +58,1 @@\n+template <bool ALT_FWD>\n@@ -57,3 +60,3 @@\n-void AdjustPointerClosure::do_oop_work(T* p)           { MarkSweep::adjust_pointer(p); }\n-inline void AdjustPointerClosure::do_oop(oop* p)       { do_oop_work(p); }\n-inline void AdjustPointerClosure::do_oop(narrowOop* p) { do_oop_work(p); }\n+void AdjustPointerClosure<ALT_FWD>::do_oop_work(T* p)           { MarkSweep::adjust_pointer<ALT_FWD>(p); }\n+template <bool ALT_FWD>\n+inline void AdjustPointerClosure<ALT_FWD>::do_oop(oop* p)       { do_oop_work(p); }\n@@ -61,0 +64,4 @@\n+template <bool ALT_FWD>\n+inline void AdjustPointerClosure<ALT_FWD>::do_oop(narrowOop* p) { do_oop_work(p); }\n+\n+template <bool ALT_FWD>\n@@ -62,1 +69,2 @@\n-  return obj->oop_iterate_size(&MarkSweep::adjust_pointer_closure);\n+  AdjustPointerClosure<ALT_FWD> adjust_pointer_closure;\n+  return obj->oop_iterate_size(&adjust_pointer_closure);\n","filename":"src\/hotspot\/share\/gc\/serial\/markSweep.inline.hpp","additions":16,"deletions":8,"binary":false,"changes":24,"status":"modified"},{"patch":"@@ -58,0 +58,1 @@\n+#include \"gc\/shared\/slidingForwarding.hpp\"\n@@ -206,0 +207,2 @@\n+  SlidingForwarding::initialize(_reserved, SpaceAlignment \/ HeapWordSize);\n+\n","filename":"src\/hotspot\/share\/gc\/serial\/serialHeap.cpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -232,1 +232,3 @@\n-  if (!Metaspace::contains(object->klass_raw())) {\n+  \/\/ With compact headers, we can't safely access the class, due\n+  \/\/ to possibly forwarded objects.\n+  if (!UseCompactObjectHeaders && !Metaspace::contains(object->klass_raw())) {\n","filename":"src\/hotspot\/share\/gc\/shared\/collectedHeap.cpp","additions":3,"deletions":1,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -311,1 +311,1 @@\n-  static constexpr size_t min_dummy_object_size() {\n+  static size_t min_dummy_object_size() {\n","filename":"src\/hotspot\/share\/gc\/shared\/collectedHeap.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -660,2 +660,6 @@\n-          constraint(GCCardSizeInBytesConstraintFunc,AtParse)\n-  \/\/ end of GC_FLAGS\n+          constraint(GCCardSizeInBytesConstraintFunc,AtParse)               \\\n+                                                                            \\\n+  product(bool, UseAltGCForwarding, false, EXPERIMENTAL,                    \\\n+          \"Use alternative GC forwarding that preserves object headers\")    \\\n+\n+\/\/ end of GC_FLAGS\n","filename":"src\/hotspot\/share\/gc\/shared\/gc_globals.hpp","additions":6,"deletions":2,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -385,1 +385,3 @@\n-  oopDesc::set_klass_gap(mem, 0);\n+  if (!UseCompactObjectHeaders) {\n+    oopDesc::set_klass_gap(mem, 0);\n+  }\n@@ -391,2 +393,0 @@\n-  \/\/ May be bootstrapping\n-  oopDesc::set_mark(mem, markWord::prototype());\n@@ -396,1 +396,6 @@\n-  oopDesc::release_set_klass(mem, _klass);\n+  if (UseCompactObjectHeaders) {\n+    oopDesc::release_set_mark(mem, _klass->prototype_header());\n+  } else {\n+    oopDesc::set_mark(mem, markWord::prototype());\n+    oopDesc::release_set_klass(mem, _klass);\n+  }\n","filename":"src\/hotspot\/share\/gc\/shared\/memAllocator.cpp","additions":9,"deletions":4,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2016, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2016, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -27,0 +27,1 @@\n+#include \"gc\/shared\/slidingForwarding.inline.hpp\"\n@@ -43,4 +44,6 @@\n-void PreservedMarks::adjust_preserved_mark(PreservedMark* elem) {\n-  oop obj = elem->get_oop();\n-  if (obj->is_forwarded()) {\n-    elem->set_oop(obj->forwardee());\n+template <bool ALT_FWD>\n+void PreservedMarks::adjust_during_full_gc_impl() {\n+  StackIterator<PreservedMark, mtGC> iter(_stack);\n+  while (!iter.is_empty()) {\n+    PreservedMark* elem = iter.next_addr();\n+    adjust_preserved_mark<ALT_FWD>(elem);\n@@ -51,4 +54,4 @@\n-  StackIterator<PreservedMark, mtGC> iter(_stack);\n-  while (!iter.is_empty()) {\n-    PreservedMark* elem = iter.next_addr();\n-    adjust_preserved_mark(elem);\n+  if (UseAltGCForwarding) {\n+    adjust_during_full_gc_impl<true>();\n+  } else {\n+    adjust_during_full_gc_impl<false>();\n","filename":"src\/hotspot\/share\/gc\/shared\/preservedMarks.cpp","additions":12,"deletions":9,"binary":false,"changes":21,"status":"modified"},{"patch":"@@ -58,0 +58,3 @@\n+  template <bool ALT_FWD>\n+  void adjust_during_full_gc_impl();\n+\n@@ -68,1 +71,2 @@\n-  static void adjust_preserved_mark(PreservedMark* elem);\n+  template <bool ALT_FWD>\n+  static inline void adjust_preserved_mark(PreservedMark* elem);\n","filename":"src\/hotspot\/share\/gc\/shared\/preservedMarks.hpp","additions":5,"deletions":1,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -29,0 +29,1 @@\n+#include \"gc\/shared\/slidingForwarding.inline.hpp\"\n@@ -63,0 +64,8 @@\n+template <bool ALT_FWD>\n+inline void PreservedMarks::adjust_preserved_mark(PreservedMark* elem) {\n+  oop obj = elem->get_oop();\n+  if (obj->is_forwarded()) {\n+    elem->set_oop(SlidingForwarding::forwardee<ALT_FWD>(obj));\n+  }\n+}\n+\n","filename":"src\/hotspot\/share\/gc\/shared\/preservedMarks.inline.hpp","additions":9,"deletions":0,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -0,0 +1,123 @@\n+\/*\n+ * Copyright (c) 2021, Red Hat, Inc. All rights reserved.\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"gc\/shared\/gc_globals.hpp\"\n+#include \"gc\/shared\/slidingForwarding.hpp\"\n+#include \"utilities\/ostream.hpp\"\n+#include \"utilities\/powerOfTwo.hpp\"\n+\n+\/\/ We cannot use 0, because that may already be a valid base address in zero-based heaps.\n+\/\/ 0x1 is safe because heap base addresses must be aligned by much larger alignment\n+HeapWord* const SlidingForwarding::UNUSED_BASE = reinterpret_cast<HeapWord*>(0x1);\n+\n+HeapWord* SlidingForwarding::_heap_start = nullptr;\n+size_t SlidingForwarding::_region_size_words = 0;\n+size_t SlidingForwarding::_heap_start_region_bias = 0;\n+size_t SlidingForwarding::_num_regions = 0;\n+uint SlidingForwarding::_region_size_bytes_shift = 0;\n+uintptr_t SlidingForwarding::_region_mask = 0;\n+HeapWord** SlidingForwarding::_biased_bases[SlidingForwarding::NUM_TARGET_REGIONS] = { nullptr, nullptr };\n+HeapWord** SlidingForwarding::_bases_table = nullptr;\n+SlidingForwarding::FallbackTable* SlidingForwarding::_fallback_table = nullptr;\n+\n+void SlidingForwarding::initialize(MemRegion heap, size_t region_size_words) {\n+#ifdef _LP64\n+  if (UseAltGCForwarding) {\n+    _heap_start = heap.start();\n+\n+    \/\/ If the heap is small enough to fit directly into the available offset bits,\n+    \/\/ and we are running Serial GC, we can treat the whole heap as a single region\n+    \/\/ if it happens to be aligned to allow biasing.\n+    size_t rounded_heap_size = round_up_power_of_2(heap.byte_size());\n+\n+    if (UseSerialGC && (heap.word_size() <= (1 << NUM_OFFSET_BITS)) &&\n+        is_aligned((uintptr_t)_heap_start, rounded_heap_size)) {\n+      _num_regions = 1;\n+      _region_size_words = heap.word_size();\n+      _region_size_bytes_shift = log2i_exact(rounded_heap_size);\n+    } else {\n+      _num_regions = align_up(pointer_delta(heap.end(), heap.start()), region_size_words) \/ region_size_words;\n+      _region_size_words = region_size_words;\n+      _region_size_bytes_shift = log2i_exact(_region_size_words) + LogHeapWordSize;\n+    }\n+    _heap_start_region_bias = (uintptr_t)_heap_start >> _region_size_bytes_shift;\n+    _region_mask = ~((uintptr_t(1) << _region_size_bytes_shift) - 1);\n+\n+    guarantee((_heap_start_region_bias << _region_size_bytes_shift) == (uintptr_t)_heap_start, \"must be aligned: _heap_start_region_bias: \" SIZE_FORMAT \", _region_size_byte_shift: %u, _heap_start: \" PTR_FORMAT, _heap_start_region_bias, _region_size_bytes_shift, p2i(_heap_start));\n+\n+    assert(_region_size_words >= 1, \"regions must be at least a word large\");\n+    assert(_bases_table == nullptr, \"should not be initialized yet\");\n+    assert(_fallback_table == nullptr, \"should not be initialized yet\");\n+  }\n+#endif\n+}\n+\n+void SlidingForwarding::begin() {\n+#ifdef _LP64\n+  if (UseAltGCForwarding) {\n+    assert(_bases_table == nullptr, \"should not be initialized yet\");\n+    assert(_fallback_table == nullptr, \"should not be initialized yet\");\n+\n+    size_t max = _num_regions * NUM_TARGET_REGIONS;\n+    _bases_table = NEW_C_HEAP_ARRAY(HeapWord*, max, mtGC);\n+    HeapWord** biased_start = _bases_table - _heap_start_region_bias;\n+    _biased_bases[0] = biased_start;\n+    _biased_bases[1] = biased_start + _num_regions;\n+    for (size_t i = 0; i < max; i++) {\n+      _bases_table[i] = UNUSED_BASE;\n+    }\n+  }\n+#endif\n+}\n+\n+void SlidingForwarding::end() {\n+#ifdef _LP64\n+  if (UseAltGCForwarding) {\n+    assert(_bases_table != nullptr, \"should be initialized\");\n+    FREE_C_HEAP_ARRAY(HeapWord*, _bases_table);\n+    _bases_table = nullptr;\n+    delete _fallback_table;\n+    _fallback_table = nullptr;\n+  }\n+#endif\n+}\n+\n+void SlidingForwarding::fallback_forward_to(HeapWord* from, HeapWord* to) {\n+  if (_fallback_table == nullptr) {\n+    _fallback_table = new (mtGC) FallbackTable();\n+  }\n+  _fallback_table->put_when_absent(from, to);\n+}\n+\n+HeapWord* SlidingForwarding::fallback_forwardee(HeapWord* from) {\n+  assert(_fallback_table != nullptr, \"fallback table must be present\");\n+  HeapWord** found = _fallback_table->get(from);\n+  if (found != nullptr) {\n+    return *found;\n+  } else {\n+    return nullptr;\n+  }\n+}\n","filename":"src\/hotspot\/share\/gc\/shared\/slidingForwarding.cpp","additions":123,"deletions":0,"binary":false,"changes":123,"status":"added"},{"patch":"@@ -0,0 +1,181 @@\n+\/*\n+ * Copyright (c) 2021, Red Hat, Inc. All rights reserved.\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_GC_SHARED_SLIDINGFORWARDING_HPP\n+#define SHARE_GC_SHARED_SLIDINGFORWARDING_HPP\n+\n+#include \"memory\/allocation.hpp\"\n+#include \"memory\/memRegion.hpp\"\n+#include \"oops\/markWord.hpp\"\n+#include \"oops\/oopsHierarchy.hpp\"\n+#include \"utilities\/fastHash.hpp\"\n+#include \"utilities\/resourceHash.hpp\"\n+\n+\/**\n+ * SlidingForwarding is a method to store forwarding information in a compressed form into the object header,\n+ * that has been specifically designed for sliding compaction GCs and compact object headers. With compact object\n+ * headers, we store the compressed class pointer in the header, which would be overwritten by full forwarding\n+ * pointer, if we allow the legacy forwarding code to act. This would lose the class information for the object,\n+ * which is required later in GC cycle to iterate the reference fields and get the object size for copying.\n+ *\n+ * SlidingForwarding requires only small side tables and guarantees constant-time access and modification.\n+ *\n+ * The idea is to use a pointer compression scheme very similar to the one that is used for compressed oops.\n+ * We divide the heap into number of logical regions. Each region spans maximum of 2^NUM_OFFSET_BITS words.\n+ *\n+ * The key advantage of sliding compaction for encoding efficiency: it can forward objects from one region to a\n+ * maximum of two regions. This is an intuitive property: when we slide the compact region full of data, it can\n+ * only span two adjacent regions. This property allows us to use the off-side table to record the addresses of\n+ * two target regions. The table holds N*2 entries for N logical regions. For each region, it gives the base\n+ * address of the two target regions, or a special placeholder if not used. A single bit in forwarding would\n+ * indicate to which of the two \"to\" regions the object is forwarded into.\n+ *\n+ * This encoding efficiency allows to store the forwarding information in the object header _together_ with the\n+ * compressed class pointer.\n+ *\n+ * When recording the sliding forwarding, the mark word would look roughly like this:\n+ *\n+ *   64                              32                                0\n+ *    [................................OOOOOOOOOOOOOOOOOOOOOOOOOOOOAFTT]\n+ *                                                                    ^----- normal lock bits, would record \"object is forwarded\"\n+ *                                                                  ^------- fallback bit (explained below)\n+ *                                                                 ^-------- alternate region select\n+ *                                     ^------------------------------------ in-region offset\n+ *     ^-------------------------------------------------------------------- protected area, *not touched* by this code, useful for\n+ *                                                                           compressed class pointer with compact object headers\n+ *\n+ * Adding a forwarding then generally works as follows:\n+ *   1. Compute the \"to\" offset in the \"to\" region, this gives \"offset\".\n+ *   2. Check if the primary \"from\" offset at base table contains \"to\" region base, use it.\n+ *      If not usable, continue to next step. If usable, set \"alternate\" = \"false\" and jump to (4).\n+ *   3. Check if the alternate \"from\" offset at base table contains \"to\" region base, use it.\n+ *      This gives us \"alternate\" = \"true\". This should always complete for sliding forwarding.\n+ *   4. Compute the mark word from \"offset\" and \"alternate\", write it out\n+ *\n+ * Similarly, looking up the target address, given an original object address generally works as follows:\n+ *   1. Load the mark from object, and decode \"offset\" and \"alternate\" from there\n+ *   2. Compute the \"from\" base offset from the object\n+ *   3. Look up \"to\" region base from the base table either at primary or alternate indices, using \"alternate\" flag\n+ *   4. Compute the \"to\" address from \"to\" region base and \"offset\"\n+ *\n+ * This algorithm is broken by G1 last-ditch serial compaction: there, object from a single region can be\n+ * forwarded to multiple, more than two regions. To deal with that, we initialize a fallback-hashtable for\n+ * storing those extra forwardings, and set another bit in the header to indicate that the forwardee is not\n+ * encoded but should be looked-up in the hashtable. G1 serial compaction is not very common - it is the\n+ * last-last-ditch GC that is used when the JVM is scrambling to squeeze more space out of the heap, and at\n+ * that point, ultimate performance is no longer the main concern.\n+ *\/\n+class SlidingForwarding : public AllStatic {\n+private:\n+\n+  \/*\n+   * A simple hash-table that acts as fallback for the sliding forwarding.\n+   * This is used in the case of G1 serial compaction, which violates the\n+   * assumption of sliding forwarding that each object of any region is only\n+   * ever forwarded to one of two target regions. At this point, the GC is\n+   * scrambling to free up more Java heap memory, and therefore performance\n+   * is not the major concern.\n+   *\n+   * The implementation is a straightforward open hashtable.\n+   * It is a single-threaded (not thread-safe) implementation, and that\n+   * is sufficient because G1 serial compaction is single-threaded.\n+   *\/\n+  inline static unsigned hash(HeapWord* const& from) {\n+    uint64_t val = reinterpret_cast<uint64_t>(from);\n+    uint64_t hash = FastHash::get_hash64(val, UCONST64(0xAAAAAAAAAAAAAAAA));\n+    return checked_cast<unsigned>(hash >> 32);\n+  }\n+  inline static bool equals(HeapWord* const& lhs, HeapWord* const& rhs) {\n+    return lhs == rhs;\n+  }\n+  typedef ResourceHashtable<HeapWord* \/* key-type *\/, HeapWord* \/* value-type *\/,\n+                            1024 \/* size *\/, AnyObj::C_HEAP \/* alloc-type *\/, mtGC,\n+                            SlidingForwarding::hash, SlidingForwarding::equals> FallbackTable;\n+\n+  static const uintptr_t MARK_LOWER_HALF_MASK = right_n_bits(32);\n+\n+  \/\/ We need the lowest two bits to indicate a forwarded object.\n+  \/\/ The next bit indicates that the forwardee should be looked-up in a fallback-table.\n+  static const int FALLBACK_SHIFT = markWord::lock_bits;\n+  static const int FALLBACK_BITS = 1;\n+  static const int FALLBACK_MASK = right_n_bits(FALLBACK_BITS) << FALLBACK_SHIFT;\n+\n+  \/\/ Next bit selects the target region\n+  static const int ALT_REGION_SHIFT = FALLBACK_SHIFT + FALLBACK_BITS;\n+  static const int ALT_REGION_BITS = 1;\n+  \/\/ This will be \"2\" always, but expose it as named constant for clarity\n+  static const size_t NUM_TARGET_REGIONS = 1 << ALT_REGION_BITS;\n+\n+  \/\/ The offset bits start then\n+  static const int OFFSET_BITS_SHIFT = ALT_REGION_SHIFT + ALT_REGION_BITS;\n+\n+  \/\/ How many bits we use for the offset\n+  static const int NUM_OFFSET_BITS = 32 - OFFSET_BITS_SHIFT;\n+\n+  \/\/ Indicates an unused base address in the target base table.\n+  static HeapWord* const UNUSED_BASE;\n+\n+  static HeapWord*      _heap_start;\n+  static size_t         _region_size_words;\n+\n+  static size_t         _heap_start_region_bias;\n+  static size_t         _num_regions;\n+  static uint           _region_size_bytes_shift;\n+  static uintptr_t      _region_mask;\n+\n+  \/\/ The target base table memory.\n+  static HeapWord**     _bases_table;\n+  \/\/ Entries into the target base tables, biased to the start of the heap.\n+  static HeapWord**     _biased_bases[NUM_TARGET_REGIONS];\n+\n+  static FallbackTable* _fallback_table;\n+\n+  static inline size_t biased_region_index_containing(HeapWord* addr);\n+\n+  static inline uintptr_t encode_forwarding(HeapWord* from, HeapWord* to);\n+  static inline HeapWord* decode_forwarding(HeapWord* from, uintptr_t encoded);\n+\n+  static void fallback_forward_to(HeapWord* from, HeapWord* to);\n+  static HeapWord* fallback_forwardee(HeapWord* from);\n+\n+  static inline void forward_to_impl(oop from, oop to);\n+  static inline oop forwardee_impl(oop from);\n+\n+public:\n+  static void initialize(MemRegion heap, size_t region_size_words);\n+\n+  static void begin();\n+  static void end();\n+\n+  static inline bool is_forwarded(oop obj);\n+  static inline bool is_not_forwarded(oop obj);\n+\n+  template <bool ALT_FWD>\n+  static inline void forward_to(oop from, oop to);\n+  template <bool ALT_FWD>\n+  static inline oop forwardee(oop from);\n+};\n+\n+#endif \/\/ SHARE_GC_SHARED_SLIDINGFORWARDING_HPP\n","filename":"src\/hotspot\/share\/gc\/shared\/slidingForwarding.hpp","additions":181,"deletions":0,"binary":false,"changes":181,"status":"added"},{"patch":"@@ -0,0 +1,171 @@\n+\/*\n+ * Copyright (c) 2021, Red Hat, Inc. All rights reserved.\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifndef SHARE_GC_SHARED_SLIDINGFORWARDING_INLINE_HPP\n+#define SHARE_GC_SHARED_SLIDINGFORWARDING_INLINE_HPP\n+\n+#include \"gc\/shared\/gc_globals.hpp\"\n+#include \"gc\/shared\/slidingForwarding.hpp\"\n+#include \"oops\/markWord.hpp\"\n+#include \"oops\/oop.inline.hpp\"\n+#include \"utilities\/macros.hpp\"\n+\n+inline bool SlidingForwarding::is_forwarded(oop obj) {\n+  return obj->is_forwarded();\n+}\n+\n+inline bool SlidingForwarding::is_not_forwarded(oop obj) {\n+  return !obj->is_forwarded();\n+}\n+\n+size_t SlidingForwarding::biased_region_index_containing(HeapWord* addr) {\n+  return (uintptr_t)addr >> _region_size_bytes_shift;\n+}\n+\n+uintptr_t SlidingForwarding::encode_forwarding(HeapWord* from, HeapWord* to) {\n+  static_assert(NUM_TARGET_REGIONS == 2, \"Only implemented for this amount\");\n+\n+  size_t from_reg_idx = biased_region_index_containing(from);\n+  HeapWord* to_region_base = (HeapWord*)((uintptr_t)to & _region_mask);\n+\n+  HeapWord** base = &_biased_bases[0][from_reg_idx];\n+  uintptr_t alternate = 0;\n+  if (*base == to_region_base) {\n+    \/\/ Primary is good\n+  } else if (*base == UNUSED_BASE) {\n+    \/\/ Primary is free\n+    *base = to_region_base;\n+  } else {\n+    base = &_biased_bases[1][from_reg_idx];\n+    if (*base == to_region_base) {\n+      \/\/ Alternate is good\n+    } else if (*base == UNUSED_BASE) {\n+      \/\/ Alternate is free\n+      *base = to_region_base;\n+    } else {\n+      \/\/ Both primary and alternate are not fitting\n+      \/\/ This happens only in the following rare situations:\n+      \/\/ - In Serial GC, sometimes when compact-top switches spaces, because the\n+      \/\/   region boudaries are virtual and objects can cross regions\n+      \/\/ - In G1 serial compaction, because tails of various compaction chains\n+      \/\/   are distributed across the remainders of already compacted regions.\n+      return (1 << FALLBACK_SHIFT) | markWord::marked_value;\n+    }\n+    alternate = 1;\n+  }\n+\n+  size_t offset = pointer_delta(to, to_region_base);\n+  assert(offset < _region_size_words, \"Offset should be within the region. from: \" PTR_FORMAT\n+         \", to: \" PTR_FORMAT \", to_region_base: \" PTR_FORMAT \", offset: \" SIZE_FORMAT,\n+         p2i(from), p2i(to), p2i(to_region_base), offset);\n+\n+  uintptr_t encoded = (offset << OFFSET_BITS_SHIFT) |\n+                      (alternate << ALT_REGION_SHIFT) |\n+                      markWord::marked_value;\n+\n+  assert(to == decode_forwarding(from, encoded), \"must be reversible\");\n+  assert((encoded & ~MARK_LOWER_HALF_MASK) == 0, \"must encode to lowest 32 bits\");\n+  return encoded;\n+}\n+\n+HeapWord* SlidingForwarding::decode_forwarding(HeapWord* from, uintptr_t encoded) {\n+  assert((encoded & markWord::lock_mask_in_place) == markWord::marked_value, \"must be marked as forwarded\");\n+  assert((encoded & FALLBACK_MASK) == 0, \"must not be fallback-forwarded\");\n+  assert((encoded & ~MARK_LOWER_HALF_MASK) == 0, \"must decode from lowest 32 bits\");\n+  size_t alternate = (encoded >> ALT_REGION_SHIFT) & right_n_bits(ALT_REGION_BITS);\n+  assert(alternate < NUM_TARGET_REGIONS, \"Sanity\");\n+  uintptr_t offset = (encoded >> OFFSET_BITS_SHIFT);\n+\n+  size_t from_idx = biased_region_index_containing(from);\n+  HeapWord* base = _biased_bases[alternate][from_idx];\n+  assert(base != UNUSED_BASE, \"must not be unused base\");\n+  HeapWord* decoded = base + offset;\n+  assert(decoded >= _heap_start,\n+         \"Address must be above heap start. encoded: \" INTPTR_FORMAT \", alt_region: \" SIZE_FORMAT \", base: \" PTR_FORMAT,\n+         encoded, alternate, p2i(base));\n+\n+  return decoded;\n+}\n+\n+inline void SlidingForwarding::forward_to_impl(oop from, oop to) {\n+  assert(_bases_table != nullptr, \"call begin() before forwarding\");\n+\n+  markWord from_header = from->mark();\n+  if (from_header.has_displaced_mark_helper()) {\n+    from_header = from_header.displaced_mark_helper();\n+  }\n+\n+  HeapWord* from_hw = cast_from_oop<HeapWord*>(from);\n+  HeapWord* to_hw   = cast_from_oop<HeapWord*>(to);\n+  uintptr_t encoded = encode_forwarding(from_hw, to_hw);\n+  markWord new_header = markWord((from_header.value() & ~MARK_LOWER_HALF_MASK) | encoded);\n+  from->set_mark(new_header);\n+\n+  if ((encoded & FALLBACK_MASK) != 0) {\n+    fallback_forward_to(from_hw, to_hw);\n+  }\n+}\n+\n+template <bool ALT_FWD>\n+inline void SlidingForwarding::forward_to(oop obj, oop fwd) {\n+#ifdef _LP64\n+  if (ALT_FWD) {\n+    assert(_bases_table != nullptr, \"expect sliding forwarding initialized\");\n+    forward_to_impl(obj, fwd);\n+    assert(forwardee<ALT_FWD>(obj) == fwd, \"must be forwarded to correct forwardee\");\n+  } else\n+#endif\n+  {\n+    obj->forward_to(fwd);\n+  }\n+}\n+\n+inline oop SlidingForwarding::forwardee_impl(oop from) {\n+  assert(_bases_table != nullptr, \"call begin() before asking for forwarding\");\n+\n+  markWord header = from->mark();\n+  HeapWord* from_hw = cast_from_oop<HeapWord*>(from);\n+  if ((header.value() & FALLBACK_MASK) != 0) {\n+    HeapWord* to = fallback_forwardee(from_hw);\n+    return cast_to_oop(to);\n+  }\n+  uintptr_t encoded = header.value() & MARK_LOWER_HALF_MASK;\n+  HeapWord* to = decode_forwarding(from_hw, encoded);\n+  return cast_to_oop(to);\n+}\n+\n+template <bool ALT_FWD>\n+inline oop SlidingForwarding::forwardee(oop obj) {\n+#ifdef _LP64\n+  if (ALT_FWD) {\n+    assert(_bases_table != nullptr, \"expect sliding forwarding initialized\");\n+    return forwardee_impl(obj);\n+  } else\n+#endif\n+  {\n+    return obj->forwardee();\n+  }\n+}\n+\n+#endif \/\/ SHARE_GC_SHARED_SLIDINGFORWARDING_INLINE_HPP\n","filename":"src\/hotspot\/share\/gc\/shared\/slidingForwarding.inline.hpp","additions":171,"deletions":0,"binary":false,"changes":171,"status":"added"},{"patch":"@@ -200,1 +200,1 @@\n-  Klass* obj_klass = obj->klass_or_null();\n+  Klass* obj_klass = obj->forward_safe_klass();\n@@ -232,1 +232,1 @@\n-    if (obj_klass != fwd->klass()) {\n+    if (obj_klass != fwd->forward_safe_klass()) {\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahAsserts.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -31,0 +31,1 @@\n+#include \"gc\/shared\/slidingForwarding.inline.hpp\"\n@@ -231,0 +232,2 @@\n+    SlidingForwarding::begin();\n+\n@@ -240,0 +243,1 @@\n+    SlidingForwarding::end();\n@@ -303,0 +307,1 @@\n+template <bool ALT_FWD>\n@@ -371,1 +376,1 @@\n-    p->forward_to(cast_to_oop(_compact_point));\n+    SlidingForwarding::forward_to<ALT_FWD>(p, cast_to_oop(_compact_point));\n@@ -401,1 +406,8 @@\n-  void work(uint worker_id) override;\n+  void work(uint worker_id) override {\n+    if (UseAltGCForwarding) {\n+      work_impl<true>(worker_id);\n+    } else {\n+      work_impl<false>(worker_id);\n+    }\n+  }\n+\n@@ -403,0 +415,3 @@\n+  template<bool ALT_FWD>\n+  void work_impl(uint worker_id);\n+\n@@ -410,1 +425,2 @@\n-void ShenandoahPrepareForCompactionTask::work(uint worker_id) {\n+template<bool ALT_FWD>\n+void ShenandoahPrepareForCompactionTask::work_impl(uint worker_id) {\n@@ -426,1 +442,1 @@\n-  ShenandoahPrepareForCompactionObjectClosure cl(_preserved_marks->get(worker_id), empty_regions, from_region);\n+  ShenandoahPrepareForCompactionObjectClosure<ALT_FWD> cl(_preserved_marks->get(worker_id), empty_regions, from_region);\n@@ -457,1 +473,2 @@\n-void ShenandoahFullGC::calculate_target_humongous_objects() {\n+template<bool ALT_FWD>\n+void ShenandoahFullGC::calculate_target_humongous_objects_impl() {\n@@ -493,1 +510,1 @@\n-        old_obj->forward_to(cast_to_oop(heap->get_region(start)->bottom()));\n+        SlidingForwarding::forward_to<ALT_FWD>(old_obj, cast_to_oop(heap->get_region(start)->bottom()));\n@@ -505,0 +522,8 @@\n+void ShenandoahFullGC::calculate_target_humongous_objects() {\n+  if (UseAltGCForwarding) {\n+    calculate_target_humongous_objects_impl<true>();\n+  } else {\n+    calculate_target_humongous_objects_impl<false>();\n+  }\n+}\n+\n@@ -742,0 +767,1 @@\n+template <bool ALT_FWD>\n@@ -753,2 +779,2 @@\n-      if (obj->is_forwarded()) {\n-        oop forw = obj->forwardee();\n+      if (SlidingForwarding::is_forwarded(obj)) {\n+        oop forw = SlidingForwarding::forwardee<ALT_FWD>(obj);\n@@ -771,0 +797,1 @@\n+template <bool ALT_FWD>\n@@ -774,1 +801,1 @@\n-  ShenandoahAdjustPointersClosure _cl;\n+  ShenandoahAdjustPointersClosure<ALT_FWD> _cl;\n@@ -797,1 +824,3 @@\n-  void work(uint worker_id) {\n+private:\n+  template <bool ALT_FWD>\n+  void work_impl(uint worker_id) {\n@@ -799,1 +828,1 @@\n-    ShenandoahAdjustPointersObjectClosure obj_cl;\n+    ShenandoahAdjustPointersObjectClosure<ALT_FWD> obj_cl;\n@@ -808,0 +837,9 @@\n+\n+public:\n+  void work(uint worker_id) {\n+    if (UseAltGCForwarding) {\n+      work_impl<true>(worker_id);\n+    } else {\n+      work_impl<false>(worker_id);\n+    }\n+  }\n@@ -814,0 +852,1 @@\n+\n@@ -820,1 +859,3 @@\n-  void work(uint worker_id) {\n+private:\n+  template <bool ALT_FWD>\n+  void work_impl(uint worker_id) {\n@@ -822,1 +863,1 @@\n-    ShenandoahAdjustPointersClosure cl;\n+    ShenandoahAdjustPointersClosure<ALT_FWD> cl;\n@@ -826,0 +867,9 @@\n+\n+public:\n+  void work(uint worker_id) {\n+    if (UseAltGCForwarding) {\n+      work_impl<true>(worker_id);\n+    } else {\n+      work_impl<false>(worker_id);\n+    }\n+  }\n@@ -852,0 +902,1 @@\n+template <bool ALT_FWD>\n@@ -864,1 +915,1 @@\n-    if (p->is_forwarded()) {\n+    if (SlidingForwarding::is_forwarded(p)) {\n@@ -866,1 +917,1 @@\n-      HeapWord* compact_to = cast_from_oop<HeapWord*>(p->forwardee());\n+      HeapWord* compact_to = cast_from_oop<HeapWord*>(SlidingForwarding::forwardee<ALT_FWD>(p));\n@@ -888,1 +939,3 @@\n-  void work(uint worker_id) {\n+private:\n+  template <bool ALT_FWD>\n+  void work_impl(uint worker_id) {\n@@ -892,1 +945,1 @@\n-    ShenandoahCompactObjectsClosure cl(worker_id);\n+    ShenandoahCompactObjectsClosure<ALT_FWD> cl(worker_id);\n@@ -903,0 +956,9 @@\n+\n+public:\n+  void work(uint worker_id) {\n+    if (UseAltGCForwarding) {\n+      work_impl<true>(worker_id);\n+    } else {\n+      work_impl<false>(worker_id);\n+    }\n+  }\n@@ -958,1 +1020,2 @@\n-void ShenandoahFullGC::compact_humongous_objects() {\n+template <bool ALT_FWD>\n+void ShenandoahFullGC::compact_humongous_objects_impl() {\n@@ -971,1 +1034,1 @@\n-      if (!old_obj->is_forwarded()) {\n+      if (SlidingForwarding::is_not_forwarded(old_obj)) {\n@@ -980,1 +1043,1 @@\n-      size_t new_start = heap->heap_region_index_containing(old_obj->forwardee());\n+      size_t new_start = heap->heap_region_index_containing(SlidingForwarding::forwardee<ALT_FWD>(old_obj));\n@@ -1021,0 +1084,8 @@\n+void ShenandoahFullGC::compact_humongous_objects() {\n+  if (UseAltGCForwarding) {\n+    compact_humongous_objects_impl<true>();\n+  } else {\n+    compact_humongous_objects_impl<false>();\n+  }\n+}\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahFullGC.cpp","additions":91,"deletions":20,"binary":false,"changes":111,"status":"modified"},{"patch":"@@ -58,0 +58,1 @@\n+  template <bool ALT_FWD>\n@@ -86,0 +87,2 @@\n+  template <bool ALT_FWD>\n+  void calculate_target_humongous_objects_impl();\n@@ -88,0 +91,2 @@\n+  template <bool ALT_FWD>\n+  void compact_humongous_objects_impl();\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahFullGC.hpp","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -37,0 +37,1 @@\n+#include \"gc\/shared\/slidingForwarding.hpp\"\n@@ -441,0 +442,2 @@\n+  SlidingForwarding::initialize(_heap_region, ShenandoahHeapRegion::region_size_words());\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.cpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -299,1 +299,1 @@\n-  size_t size = p->size();\n+  size_t size = p->forward_safe_size();\n@@ -334,2 +334,0 @@\n-\n-  \/\/ Try to install the new forwarding pointer.\n@@ -337,1 +335,15 @@\n-  ContinuationGCSupport::relativize_stack_chunk(copy_val);\n+  if (UseCompactObjectHeaders) {\n+    \/\/ The copy above is not atomic. Make sure we have seen the proper mark\n+    \/\/ and re-install it into the copy, so that Klass* is guaranteed to be correct.\n+    markWord mark = copy_val->mark();\n+    if (!mark.is_marked()) {\n+      copy_val->set_mark(mark);\n+      ContinuationGCSupport::relativize_stack_chunk(copy_val);\n+    } else {\n+      \/\/ If we copied a mark-word that indicates 'forwarded' state, the object\n+      \/\/ installation would not succeed. We cannot access Klass* anymore either.\n+      \/\/ Skip the transformation.\n+    }\n+  } else {\n+    ContinuationGCSupport::relativize_stack_chunk(copy_val);\n+  }\n@@ -339,0 +351,1 @@\n+  \/\/ Try to install the new forwarding pointer.\n@@ -514,1 +527,1 @@\n-    size_t size = obj->size();\n+    size_t size = obj->forward_safe_size();\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.inline.hpp","additions":18,"deletions":5,"binary":false,"changes":23,"status":"modified"},{"patch":"@@ -176,1 +176,0 @@\n-\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahMarkBitMap.hpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -101,1 +101,1 @@\n-      if (is_instance_ref_klass(obj->klass())) {\n+      if (is_instance_ref_klass(obj->forward_safe_klass())) {\n@@ -128,1 +128,1 @@\n-    Klass* obj_klass = obj->klass_or_null();\n+    Klass* obj_klass = obj->forward_safe_klass();\n@@ -143,1 +143,1 @@\n-        check(ShenandoahAsserts::_safe_unknown, obj, (obj_addr + obj->size()) <= obj_reg->top(),\n+        check(ShenandoahAsserts::_safe_unknown, obj, (obj_addr + obj->forward_safe_size()) <= obj_reg->top(),\n@@ -147,1 +147,1 @@\n-        size_t humongous_end = humongous_start + (obj->size() >> ShenandoahHeapRegion::region_size_words_shift());\n+        size_t humongous_end = humongous_start + (obj->forward_safe_size() >> ShenandoahHeapRegion::region_size_words_shift());\n@@ -164,1 +164,1 @@\n-          Atomic::add(&_ld[obj_reg->index()], (uint) obj->size(), memory_order_relaxed);\n+          Atomic::add(&_ld[obj_reg->index()], (uint) obj->forward_safe_size(), memory_order_relaxed);\n@@ -205,1 +205,1 @@\n-      check(ShenandoahAsserts::_safe_oop, obj, (fwd_addr + fwd->size()) <= fwd_reg->top(),\n+      check(ShenandoahAsserts::_safe_oop, obj, (fwd_addr + fwd->forward_safe_size()) <= fwd_reg->top(),\n@@ -308,1 +308,2 @@\n-    obj->oop_iterate(this);\n+    Klass* klass = obj->forward_safe_klass();\n+    obj->oop_iterate_backwards(this, klass);\n@@ -588,1 +589,1 @@\n-    if (!is_instance_ref_klass(obj->klass())) {\n+    if (!is_instance_ref_klass(obj->forward_safe_klass())) {\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahVerifier.cpp","additions":9,"deletions":8,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -301,1 +301,1 @@\n-        assert(!UseCompressedClassPointers, \"should only happen without compressed class pointers\");\n+        assert(!UseCompressedClassPointers || UseCompactObjectHeaders, \"should only happen without compressed class pointers or with compact object headers\");\n","filename":"src\/hotspot\/share\/gc\/x\/c2\/xBarrierSetC2.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -76,2 +76,6 @@\n-  arrayOopDesc::set_mark(mem, markWord::prototype());\n-  arrayOopDesc::release_set_klass(mem, _klass);\n+  if (UseCompactObjectHeaders) {\n+    oopDesc::release_set_mark(mem, _klass->prototype_header());\n+  } else {\n+    arrayOopDesc::set_mark(mem, markWord::prototype());\n+    arrayOopDesc::release_set_klass(mem, _klass);\n+  }\n","filename":"src\/hotspot\/share\/gc\/x\/xObjArrayAllocator.cpp","additions":6,"deletions":2,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -500,1 +500,1 @@\n-        assert(!UseCompressedClassPointers, \"should only happen without compressed class pointers\");\n+        assert(!UseCompressedClassPointers || UseCompactObjectHeaders, \"should only happen without compressed class pointers or with compact object headers\");\n","filename":"src\/hotspot\/share\/gc\/z\/c2\/zBarrierSetC2.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -79,2 +79,6 @@\n-  arrayOopDesc::set_mark(mem, markWord::prototype().set_marked());\n-  arrayOopDesc::release_set_klass(mem, _klass);\n+  if (UseCompactObjectHeaders) {\n+    oopDesc::release_set_mark(mem, _klass->prototype_header().set_marked());\n+  } else {\n+    arrayOopDesc::set_mark(mem, markWord::prototype().set_marked());\n+    arrayOopDesc::release_set_klass(mem, _klass);\n+  }\n@@ -150,1 +154,5 @@\n-  oopDesc::release_set_mark(mem, markWord::prototype());\n+  if (UseCompactObjectHeaders) {\n+    oopDesc::release_set_mark(mem, _klass->prototype_header());\n+  } else {\n+    oopDesc::release_set_mark(mem, markWord::prototype());\n+  }\n","filename":"src\/hotspot\/share\/gc\/z\/zObjArrayAllocator.cpp","additions":11,"deletions":3,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -624,1 +624,0 @@\n-    const size_t size = ZUtils::object_size(from_addr);\n@@ -632,0 +631,1 @@\n+        const size_t size = ZUtils::object_size(to_addr);\n@@ -638,0 +638,1 @@\n+    const size_t size = ZUtils::object_size(from_addr);\n","filename":"src\/hotspot\/share\/gc\/z\/zRelocate.cpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -80,0 +80,1 @@\n+#include \"utilities\/globalDefinitions.hpp\"\n@@ -738,1 +739,1 @@\n-  assert(LockingMode != LM_LIGHTWEIGHT, \"Should call monitorenter_obj() when using the new lightweight locking\");\n+  assert(LockingMode != LM_LIGHTWEIGHT && LockingMode != LM_PLACEHOLDER, \"Should call monitorenter_obj() when using the new lightweight locking\");\n@@ -762,1 +763,1 @@\n-  assert(LockingMode == LM_LIGHTWEIGHT, \"Should call monitorenter() when not using the new lightweight locking\");\n+  assert(LockingMode == LM_LIGHTWEIGHT || LockingMode == LM_PLACEHOLDER, \"Should call monitorenter() when not using the new lightweight locking\");\n","filename":"src\/hotspot\/share\/interpreter\/interpreterRuntime.cpp","additions":3,"deletions":2,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -2004,4 +2004,7 @@\n-              oopDesc::set_mark(result, markWord::prototype());\n-              oopDesc::set_klass_gap(result, 0);\n-              oopDesc::release_set_klass(result, ik);\n-\n+              if (UseCompactObjectHeaders) {\n+                oopDesc::release_set_mark(result, ik->prototype_header());\n+              } else {\n+                oopDesc::set_mark(result, markWord::prototype());\n+                oopDesc::set_klass_gap(result, 0);\n+                oopDesc::release_set_klass(result, ik);\n+              }\n","filename":"src\/hotspot\/share\/interpreter\/zero\/bytecodeInterpreter.cpp","additions":7,"deletions":4,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -73,1 +73,1 @@\n-    obj->set_mark(markWord::prototype().set_marked());\n+    obj->set_mark(obj->prototype_mark().set_marked());\n","filename":"src\/hotspot\/share\/jfr\/leakprofiler\/chains\/objectSampleMarker.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -440,1 +440,5 @@\n-  if (offset == oopDesc::klass_offset_in_bytes()) {\n+\n+  \/\/ With compact object headers, we can test for the explicit offset within\n+  \/\/ the header to figure out if compiler code is accessing the class.\n+  int klass_offset = UseCompactObjectHeaders ? 4 : oopDesc::klass_offset_in_bytes();\n+  if (offset == klass_offset) {\n","filename":"src\/hotspot\/share\/jvmci\/jvmciCompilerToVM.cpp","additions":5,"deletions":1,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -325,2 +325,7 @@\n-  assert(oopDesc::klass_offset_in_bytes() < static_cast<intptr_t>(os::vm_page_size()),\n-         \"Klass offset is expected to be less than the page size\");\n+  if (UseCompactObjectHeaders) {\n+    assert(oopDesc::mark_offset_in_bytes() < static_cast<intptr_t>(os::vm_page_size()),\n+           \"Mark offset is expected to be less than the page size\");\n+  } else {\n+    assert(oopDesc::klass_offset_in_bytes() < static_cast<intptr_t>(os::vm_page_size()),\n+           \"Klass offset is expected to be less than the page size\");\n+  }\n","filename":"src\/hotspot\/share\/memory\/universe.cpp","additions":7,"deletions":2,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -85,2 +85,7 @@\n-    return UseCompressedClassPointers ? klass_gap_offset_in_bytes() :\n-                               sizeof(arrayOopDesc);\n+    if (UseCompactObjectHeaders) {\n+      return oopDesc::base_offset_in_bytes();\n+    } else if (UseCompressedClassPointers) {\n+      return klass_gap_offset_in_bytes();\n+    } else {\n+      return sizeof(arrayOopDesc);\n+    }\n","filename":"src\/hotspot\/share\/oops\/arrayOop.hpp","additions":7,"deletions":2,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -36,3 +36,0 @@\n-  \/\/ aligned header size.\n-  static int header_size() { return sizeof(instanceOopDesc)\/HeapWordSize; }\n-\n@@ -41,4 +38,7 @@\n-    return (UseCompressedClassPointers) ?\n-            klass_gap_offset_in_bytes() :\n-            sizeof(instanceOopDesc);\n-\n+    if (UseCompactObjectHeaders) {\n+      return oopDesc::base_offset_in_bytes();\n+    } else if (UseCompressedClassPointers) {\n+      return klass_gap_offset_in_bytes();\n+    } else {\n+      return sizeof(instanceOopDesc);\n+    }\n","filename":"src\/hotspot\/share\/oops\/instanceOop.hpp","additions":7,"deletions":7,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -194,0 +194,10 @@\n+static markWord make_prototype(Klass* kls) {\n+  markWord prototype = markWord::prototype();\n+#ifdef _LP64\n+  if (UseCompactObjectHeaders) {\n+    prototype = prototype.set_klass(kls);\n+  }\n+#endif\n+  return prototype;\n+}\n+\n@@ -203,0 +213,1 @@\n+                           _prototype_header(make_prototype(this)),\n@@ -749,0 +760,4 @@\n+     if (UseCompactObjectHeaders) {\n+       st->print(BULLET\"prototype_header: \" INTPTR_FORMAT, _prototype_header.value());\n+       st->cr();\n+     }\n","filename":"src\/hotspot\/share\/oops\/klass.cpp","additions":15,"deletions":0,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -170,0 +170,2 @@\n+  markWord _prototype_header;   \/\/ Used to initialize objects' header\n+\n@@ -678,0 +680,7 @@\n+  markWord prototype_header() const      {\n+    assert(UseCompactObjectHeaders, \"only use with compact object headers\");\n+    return _prototype_header;\n+  }\n+  inline void set_prototype_header(markWord header);\n+  static ByteSize prototype_header_offset() { return in_ByteSize(offset_of(Klass, _prototype_header)); }\n+\n","filename":"src\/hotspot\/share\/oops\/klass.hpp","additions":9,"deletions":0,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -55,0 +55,5 @@\n+inline void Klass::set_prototype_header(markWord header) {\n+  assert(UseCompactObjectHeaders, \"only with compact headers\");\n+  _prototype_header = header;\n+}\n+\n","filename":"src\/hotspot\/share\/oops\/klass.inline.hpp","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -29,0 +29,1 @@\n+#include \"utilities\/globalDefinitions.hpp\"\n@@ -70,1 +71,1 @@\n-    if (print_monitor_info) {\n+    if (print_monitor_info && LockingMode != LM_PLACEHOLDER) {\n","filename":"src\/hotspot\/share\/oops\/markWord.cpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -28,0 +28,1 @@\n+#include \"gc\/shared\/gc_globals.hpp\"\n@@ -29,0 +30,1 @@\n+#include \"oops\/compressedKlass.hpp\"\n@@ -31,0 +33,1 @@\n+#include \"utilities\/globalDefinitions.hpp\"\n@@ -46,0 +49,4 @@\n+\/\/  64 bits (with compact headers):\n+\/\/  -------------------------------\n+\/\/  nklass:32 hash:25 -->| unused_gap:1  age:4  self-fwded:1  lock:2 (normal object)\n+\/\/\n@@ -106,2 +113,2 @@\n-  static const int first_unused_gap_bits          = 1;\n-  static const int max_hash_bits                  = BitsPerWord - age_bits - lock_bits - first_unused_gap_bits;\n+  static const int self_forwarded_bits            = 1;\n+  static const int max_hash_bits                  = BitsPerWord - age_bits - lock_bits - self_forwarded_bits;\n@@ -109,1 +116,7 @@\n-  static const int second_unused_gap_bits         = LP64_ONLY(1) NOT_LP64(0);\n+  static const int hash_bits_compact              = max_hash_bits > 25 ? 25 : max_hash_bits;\n+  \/\/ Used only without compact headers.\n+  static const int unused_gap_bits                = LP64_ONLY(1) NOT_LP64(0);\n+#ifdef _LP64\n+  \/\/ Used only with compact headers.\n+  static const int klass_bits                     = 32;\n+#endif\n@@ -112,2 +125,8 @@\n-  static const int age_shift                      = lock_bits + first_unused_gap_bits;\n-  static const int hash_shift                     = age_shift + age_bits + second_unused_gap_bits;\n+  static const int self_forwarded_shift           = lock_shift + lock_bits;\n+  static const int age_shift                      = self_forwarded_shift + self_forwarded_bits;\n+  static const int hash_shift                     = age_shift + age_bits + unused_gap_bits;\n+  static const int hash_shift_compact             = age_shift + age_bits;\n+#ifdef _LP64\n+  \/\/ Used only with compact headers.\n+  static const int klass_shift                    = hash_shift_compact + hash_bits_compact;\n+#endif\n@@ -117,0 +136,2 @@\n+  static const uintptr_t self_forwarded_mask      = right_n_bits(self_forwarded_bits);\n+  static const uintptr_t self_forwarded_mask_in_place = self_forwarded_mask << self_forwarded_shift;\n@@ -121,0 +142,6 @@\n+  static const uintptr_t hash_mask_compact        = right_n_bits(hash_bits_compact);\n+  static const uintptr_t hash_mask_compact_in_place = hash_mask_compact << hash_shift_compact;\n+#ifdef _LP64\n+  static const uintptr_t klass_mask               = right_n_bits(klass_bits);\n+  static const uintptr_t klass_mask_in_place      = klass_mask << klass_shift;\n+#endif\n@@ -184,1 +211,1 @@\n-    assert(LockingMode == LM_LIGHTWEIGHT, \"should only be called with new lightweight locking\");\n+    assert(LockingMode == LM_LIGHTWEIGHT || LockingMode == LM_PLACEHOLDER, \"should only be called with new lightweight locking\");\n@@ -197,0 +224,1 @@\n+    assert(LockingMode != LM_PLACEHOLDER, \"Placeholder locking does not use markWord for monitors\");\n@@ -202,2 +230,3 @@\n-    return LockingMode == LM_LIGHTWEIGHT  ? lockbits == monitor_value   \/\/ monitor?\n-                                          : (lockbits & unlocked_value) == 0; \/\/ monitor | stack-locked?\n+    return LockingMode == LM_PLACEHOLDER ? false :                           \/\/ no displaced mark\n+           LockingMode == LM_LIGHTWEIGHT ? lockbits == monitor_value         \/\/ monitor?\n+                                         : (lockbits & unlocked_value) == 0; \/\/ monitor | stack-locked?\n@@ -208,3 +237,9 @@\n-    uintptr_t tmp = value() & (~hash_mask_in_place);\n-    tmp |= ((hash & hash_mask) << hash_shift);\n-    return markWord(tmp);\n+    if (UseCompactObjectHeaders) {\n+      uintptr_t tmp = value() & (~hash_mask_compact_in_place);\n+      tmp |= ((hash & hash_mask_compact) << hash_shift_compact);\n+      return markWord(tmp);\n+    } else {\n+      uintptr_t tmp = value() & (~hash_mask_in_place);\n+      tmp |= ((hash & hash_mask) << hash_shift);\n+      return markWord(tmp);\n+    }\n@@ -223,0 +258,1 @@\n+    assert(LockingMode != LM_PLACEHOLDER, \"Placeholder locking does not use markWord for monitors\");\n@@ -227,0 +263,4 @@\n+  markWord set_has_monitor() const {\n+    return markWord((value() & ~lock_mask_in_place) | monitor_value);\n+  }\n+\n@@ -228,1 +268,1 @@\n-  markWord clear_lock_bits() { return markWord(value() & ~lock_mask_in_place); }\n+  markWord clear_lock_bits() const { return markWord(value() & ~lock_mask_in_place); }\n@@ -243,1 +283,5 @@\n-    return mask_bits(value() >> hash_shift, hash_mask);\n+    if (UseCompactObjectHeaders) {\n+      return mask_bits(value() >> hash_shift_compact, hash_mask_compact);\n+    } else {\n+      return mask_bits(value() >> hash_shift, hash_mask);\n+    }\n@@ -250,0 +294,9 @@\n+#ifdef _LP64\n+  inline markWord actual_mark() const;\n+  inline Klass* klass() const;\n+  inline Klass* klass_or_null() const;\n+  inline narrowKlass narrow_klass() const;\n+  inline markWord set_narrow_klass(narrowKlass nklass) const;\n+  inline markWord set_klass(Klass* klass) const;\n+#endif\n+\n@@ -263,0 +316,13 @@\n+\n+#ifdef _LP64\n+  inline bool self_forwarded() const {\n+    bool self_fwd = mask_bits(value(), self_forwarded_mask_in_place) != 0;\n+    assert(!self_fwd || UseAltGCForwarding, \"Only set self-fwd bit when using alt GC forwarding\");\n+    return self_fwd;\n+  }\n+\n+  inline markWord set_self_forwarded() const {\n+    assert(UseAltGCForwarding, \"Only call this with alt GC forwarding\");\n+    return markWord(value() | self_forwarded_mask_in_place | marked_value);\n+  }\n+#endif\n","filename":"src\/hotspot\/share\/oops\/markWord.hpp","additions":79,"deletions":13,"binary":false,"changes":92,"status":"modified"},{"patch":"@@ -0,0 +1,70 @@\n+\/*\n+ * Copyright (c) 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_OOPS_MARKWORD_INLINE_HPP\n+#define SHARE_OOPS_MARKWORD_INLINE_HPP\n+\n+#include \"oops\/markWord.hpp\"\n+#include \"oops\/compressedOops.inline.hpp\"\n+\n+#ifdef _LP64\n+markWord markWord::actual_mark() const {\n+  assert(UseCompactObjectHeaders, \"only safe when using compact headers\");\n+  if (has_displaced_mark_helper()) {\n+    return displaced_mark_helper();\n+  } else {\n+    return *this;\n+  }\n+}\n+\n+Klass* markWord::klass() const {\n+  assert(UseCompactObjectHeaders, \"only used with compact object headers\");\n+  assert(!CompressedKlassPointers::is_null(narrow_klass()), \"narrow klass must not be null: \" INTPTR_FORMAT, value());\n+  return CompressedKlassPointers::decode_not_null(narrow_klass());\n+}\n+\n+Klass* markWord::klass_or_null() const {\n+  assert(UseCompactObjectHeaders, \"only used with compact object headers\");\n+  return CompressedKlassPointers::decode(narrow_klass());\n+}\n+\n+narrowKlass markWord::narrow_klass() const {\n+  assert(UseCompactObjectHeaders, \"only used with compact object headers\");\n+  return narrowKlass(value() >> klass_shift);\n+}\n+\n+markWord markWord::set_narrow_klass(narrowKlass nklass) const {\n+  assert(UseCompactObjectHeaders, \"only used with compact object headers\");\n+  return markWord((value() & ~klass_mask_in_place) | ((uintptr_t) nklass << klass_shift));\n+}\n+\n+markWord markWord::set_klass(Klass* klass) const {\n+  assert(UseCompactObjectHeaders, \"only used with compact object headers\");\n+  assert(UseCompressedClassPointers, \"expect compressed klass pointers\");\n+  narrowKlass nklass = CompressedKlassPointers::encode(const_cast<Klass*>(klass));\n+  return set_narrow_klass(nklass);\n+}\n+#endif \/\/ _LP64\n+\n+#endif \/\/ SHARE_OOPS_MARKWORD_INLINE_HPP\n","filename":"src\/hotspot\/share\/oops\/markWord.inline.hpp","additions":70,"deletions":0,"binary":false,"changes":70,"status":"added"},{"patch":"@@ -159,1 +159,2 @@\n-  assert(obj->is_objArray(), \"must be object array\");\n+  \/\/ In this assert, we cannot safely access the Klass* with compact headers.\n+  assert(UseCompactObjectHeaders || obj->is_objArray(), \"must be object array\");\n","filename":"src\/hotspot\/share\/oops\/objArrayKlass.cpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -73,1 +73,2 @@\n-  assert (obj->is_array(), \"obj must be array\");\n+  \/\/ In this assert, we cannot safely access the Klass* with compact headers.\n+  assert (UseCompactObjectHeaders || obj->is_array(), \"obj must be array\");\n","filename":"src\/hotspot\/share\/oops\/objArrayKlass.inline.hpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -129,1 +129,1 @@\n-  return LockingMode == LM_LIGHTWEIGHT || !SafepointSynchronize::is_at_safepoint();\n+  return LockingMode == LM_LIGHTWEIGHT || LockingMode == LM_PLACEHOLDER || !SafepointSynchronize::is_at_safepoint();\n@@ -157,1 +157,2 @@\n-  return UseCompressedClassPointers;\n+  \/\/ Except when using compact headers.\n+  return UseCompressedClassPointers && !UseCompactObjectHeaders;\n@@ -169,1 +170,3 @@\n-  if (UseCompressedClassPointers) {\n+  if (UseCompactObjectHeaders) {\n+    return obj->klass();\n+  } else if (UseCompressedClassPointers) {\n","filename":"src\/hotspot\/share\/oops\/oop.cpp","additions":6,"deletions":3,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -81,0 +81,5 @@\n+  inline markWord resolve_mark() const;\n+\n+  \/\/ Returns the prototype mark that should be used for this object.\n+  inline markWord prototype_mark() const;\n+\n@@ -99,1 +104,7 @@\n-  static constexpr int header_size() { return sizeof(oopDesc)\/HeapWordSize; }\n+  static int header_size() {\n+    if (UseCompactObjectHeaders) {\n+      return sizeof(markWord) \/ HeapWordSize;\n+    } else {\n+      return sizeof(oopDesc)\/HeapWordSize;\n+    }\n+  }\n@@ -111,0 +122,14 @@\n+  \/\/ The following set of methods is used to access the mark-word and related\n+  \/\/ properties when the object may be forwarded. Be careful where and when\n+  \/\/ using this method. It assumes that the forwardee is installed in\n+  \/\/ the header as a plain pointer (or self-forwarded). In particular,\n+  \/\/ those methods can not deal with the sliding-forwarding that is used\n+  \/\/ in Serial, G1 and Shenandoah full-GCs.\n+private:\n+  inline Klass*   forward_safe_klass_impl(markWord m) const;\n+public:\n+  inline Klass*   forward_safe_klass() const;\n+  inline Klass*   forward_safe_klass(markWord m) const;\n+  inline size_t   forward_safe_size();\n+  inline void     forward_safe_init_mark();\n+\n@@ -261,0 +286,1 @@\n+  inline void forward_to_self();\n@@ -267,0 +293,1 @@\n+  inline oop forward_to_self_atomic(markWord compare, atomic_memory_order order = memory_order_conservative);\n@@ -269,0 +296,1 @@\n+  inline oop forwardee(markWord header) const;\n@@ -312,1 +340,11 @@\n-  static int klass_offset_in_bytes()     { return (int)offset_of(oopDesc, _metadata._klass); }\n+  static int klass_offset_in_bytes()     {\n+#ifdef _LP64\n+    if (UseCompactObjectHeaders) {\n+      STATIC_ASSERT(markWord::klass_shift % 8 == 0);\n+      return mark_offset_in_bytes() + markWord::klass_shift \/ 8;\n+    } else\n+#endif\n+    {\n+      return (int)offset_of(oopDesc, _metadata._klass);\n+    }\n+  }\n@@ -315,0 +353,1 @@\n+    assert(!UseCompactObjectHeaders, \"don't use klass_offset_in_bytes() with compact headers\");\n@@ -318,0 +357,16 @@\n+  static int base_offset_in_bytes() {\n+#ifdef _LP64\n+    if (UseCompactObjectHeaders) {\n+      \/\/ With compact headers, the Klass* field is not used for the Klass*\n+      \/\/ and is used for the object fields instead.\n+      assert(sizeof(markWord) == 8, \"sanity\");\n+      return sizeof(markWord);\n+    } else if (UseCompressedClassPointers) {\n+      return sizeof(markWord) + sizeof(narrowKlass);\n+    } else\n+#endif\n+    {\n+      return sizeof(oopDesc);\n+    }\n+  }\n+\n","filename":"src\/hotspot\/share\/oops\/oop.hpp","additions":57,"deletions":2,"binary":false,"changes":59,"status":"modified"},{"patch":"@@ -37,1 +37,1 @@\n-#include \"oops\/markWord.hpp\"\n+#include \"oops\/markWord.inline.hpp\"\n@@ -41,0 +41,1 @@\n+#include \"runtime\/safepoint.hpp\"\n@@ -85,0 +86,17 @@\n+markWord oopDesc::resolve_mark() const {\n+  assert(LockingMode != LM_LEGACY, \"Not safe with legacy stack-locking\");\n+  markWord m = mark();\n+  if (m.has_displaced_mark_helper()) {\n+    m = m.displaced_mark_helper();\n+  }\n+  return m;\n+}\n+\n+markWord oopDesc::prototype_mark() const {\n+  if (UseCompactObjectHeaders) {\n+    return klass()->prototype_header();\n+  } else {\n+    return markWord::prototype();\n+  }\n+}\n+\n@@ -86,1 +104,5 @@\n-  set_mark(markWord::prototype());\n+  if (UseCompactObjectHeaders) {\n+    set_mark(prototype_mark());\n+  } else {\n+    set_mark(markWord::prototype());\n+  }\n@@ -90,1 +112,5 @@\n-  if (UseCompressedClassPointers) {\n+#ifdef _LP64\n+  if (UseCompactObjectHeaders) {\n+    markWord m = resolve_mark();\n+    return m.klass();\n+  } else if (UseCompressedClassPointers) {\n@@ -92,1 +118,3 @@\n-  } else {\n+  } else\n+#endif\n+  {\n@@ -98,1 +126,5 @@\n-  if (UseCompressedClassPointers) {\n+#ifdef _LP64\n+  if (UseCompactObjectHeaders) {\n+    markWord m = resolve_mark();\n+    return m.klass_or_null();\n+  } else if (UseCompressedClassPointers) {\n@@ -100,1 +132,3 @@\n-  } else {\n+  } else\n+#endif\n+  {\n@@ -106,4 +140,13 @@\n-  if (UseCompressedClassPointers) {\n-    narrowKlass nklass = Atomic::load_acquire(&_metadata._compressed_klass);\n-    return CompressedKlassPointers::decode(nklass);\n-  } else {\n+#ifdef _LP64\n+  if (UseCompactObjectHeaders) {\n+    markWord m = mark_acquire();\n+    if (m.has_displaced_mark_helper()) {\n+      m = m.displaced_mark_helper();\n+    }\n+    return m.klass_or_null();\n+  } else if (UseCompressedClassPointers) {\n+     narrowKlass nklass = Atomic::load_acquire(&_metadata._compressed_klass);\n+     return CompressedKlassPointers::decode(nklass);\n+  } else\n+#endif\n+  {\n@@ -115,1 +158,3 @@\n-  if (UseCompressedClassPointers) {\n+  if (UseCompactObjectHeaders) {\n+    return klass();\n+  } else if (UseCompressedClassPointers) {\n@@ -124,0 +169,1 @@\n+  assert(!UseCompactObjectHeaders, \"don't set Klass* with compact headers\");\n@@ -133,0 +179,1 @@\n+  assert(!UseCompactObjectHeaders, \"don't set Klass* with compact headers\");\n@@ -143,0 +190,1 @@\n+  assert(!UseCompactObjectHeaders, \"don't set Klass* gap with compact headers\");\n@@ -205,0 +253,47 @@\n+#ifdef _LP64\n+Klass* oopDesc::forward_safe_klass_impl(markWord m) const {\n+  assert(UseCompactObjectHeaders, \"Only get here with compact headers\");\n+  if (m.is_marked()) {\n+    oop fwd = forwardee(m);\n+    markWord m2 = fwd->mark();\n+    assert(!m2.is_marked() || m2.self_forwarded(), \"no double forwarding: this: \" PTR_FORMAT \" (\" INTPTR_FORMAT \"), fwd: \" PTR_FORMAT \" (\" INTPTR_FORMAT \")\", p2i(this), m.value(), p2i(fwd), m2.value());\n+    m = m2;\n+  }\n+  return m.actual_mark().klass();\n+}\n+#endif\n+\n+Klass* oopDesc::forward_safe_klass(markWord m) const {\n+#ifdef _LP64\n+  if (UseCompactObjectHeaders) {\n+    return forward_safe_klass_impl(m);\n+  } else\n+#endif\n+  {\n+    return klass();\n+  }\n+}\n+\n+Klass* oopDesc::forward_safe_klass() const {\n+#ifdef _LP64\n+  if (UseCompactObjectHeaders) {\n+    return forward_safe_klass_impl(mark());\n+  } else\n+#endif\n+  {\n+    return klass();\n+  }\n+}\n+\n+size_t oopDesc::forward_safe_size() {\n+  return size_given_klass(forward_safe_klass());\n+}\n+\n+void oopDesc::forward_safe_init_mark() {\n+  if (UseCompactObjectHeaders) {\n+    set_mark(forward_safe_klass()->prototype_header());\n+  } else {\n+    set_mark(markWord::prototype());\n+  }\n+}\n+\n@@ -273,0 +368,1 @@\n+  assert(p != cast_to_oop(this) || !UseAltGCForwarding, \"Must not be called with self-forwarding\");\n@@ -274,1 +370,1 @@\n-  assert(m.decode_pointer() == p, \"encoding must be reversible\");\n+  assert(forwardee(m) == p, \"encoding must be reversable\");\n@@ -278,0 +374,20 @@\n+void oopDesc::forward_to_self() {\n+#ifdef _LP64\n+  if (UseAltGCForwarding) {\n+    markWord m = mark();\n+    \/\/ If mark is displaced, we need to preserve the real header during GC.\n+    \/\/ It will be restored to the displaced header after GC.\n+    assert(SafepointSynchronize::is_at_safepoint(), \"we can only safely fetch the displaced header at safepoint\");\n+    if (m.has_displaced_mark_helper()) {\n+      m = m.displaced_mark_helper();\n+    }\n+    m = m.set_self_forwarded();\n+    assert(forwardee(m) == cast_to_oop(this), \"encoding must be reversible\");\n+    set_mark(m);\n+  } else\n+#endif\n+  {\n+    forward_to(oop(this));\n+  }\n+}\n+\n@@ -279,0 +395,1 @@\n+  assert(p != cast_to_oop(this) || !UseAltGCForwarding, \"Must not be called with self-forwarding\");\n@@ -285,1 +402,39 @@\n-    return cast_to_oop(old_mark.decode_pointer());\n+    return forwardee(old_mark);\n+  }\n+}\n+\n+oop oopDesc::forward_to_self_atomic(markWord compare, atomic_memory_order order) {\n+#ifdef _LP64\n+  if (UseAltGCForwarding) {\n+    markWord m = compare;\n+    \/\/ If mark is displaced, we need to preserve the real header during GC.\n+    \/\/ It will be restored to the displaced header after GC.\n+    assert(SafepointSynchronize::is_at_safepoint(), \"we can only safely fetch the displaced header at safepoint\");\n+    if (m.has_displaced_mark_helper()) {\n+      m = m.displaced_mark_helper();\n+    }\n+    m = m.set_self_forwarded();\n+    assert(forwardee(m) == cast_to_oop(this), \"encoding must be reversible\");\n+    markWord old_mark = cas_set_mark(m, compare, order);\n+    if (old_mark == compare) {\n+      return nullptr;\n+    } else {\n+      assert(old_mark.is_marked(), \"must be marked here\");\n+      return forwardee(old_mark);\n+    }\n+  } else\n+#endif\n+  {\n+    return forward_to_atomic(cast_to_oop(this), compare, order);\n+  }\n+}\n+\n+oop oopDesc::forwardee(markWord header) const {\n+  assert(header.is_marked(), \"only decode when actually forwarded\");\n+#ifdef _LP64\n+  if (header.self_forwarded()) {\n+    return cast_to_oop(this);\n+  } else\n+#endif\n+  {\n+    return cast_to_oop(header.decode_pointer());\n@@ -293,2 +448,1 @@\n-  assert(is_forwarded(), \"only decode when actually forwarded\");\n-  return cast_to_oop(mark().decode_pointer());\n+  return forwardee(mark());\n@@ -351,1 +505,2 @@\n-  assert(k == klass(), \"wrong klass\");\n+  \/\/ In this assert, we cannot safely access the Klass* with compact headers.\n+  assert(UseCompactObjectHeaders || k == klass(), \"wrong klass\");\n","filename":"src\/hotspot\/share\/oops\/oop.inline.hpp","additions":171,"deletions":16,"binary":false,"changes":187,"status":"modified"},{"patch":"@@ -174,1 +174,2 @@\n-  assert(obj->is_typeArray(),\"must be a type array\");\n+  \/\/ In this assert, we cannot safely access the Klass* with compact headers.\n+  assert(UseCompactObjectHeaders || obj->is_typeArray(),\"must be a type array\");\n","filename":"src\/hotspot\/share\/oops\/typeArrayKlass.cpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -38,1 +38,2 @@\n-  assert(obj->is_typeArray(),\"must be a type array\");\n+  \/\/ In this assert, we cannot safely access the Klass* with compact headers.\n+  assert(UseCompactObjectHeaders || obj->is_typeArray(),\"must be a type array\");\n","filename":"src\/hotspot\/share\/oops\/typeArrayKlass.inline.hpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -133,0 +133,10 @@\n+\n+class C2LoadNKlassStub : public C2CodeStub {\n+private:\n+  Register _dst;\n+public:\n+  C2LoadNKlassStub(Register dst) : C2CodeStub(), _dst(dst) {}\n+  Register dst() { return _dst; }\n+  int max_size() const;\n+  void emit(C2_MacroAssembler& masm);\n+};\n@@ -135,0 +145,21 @@\n+class C2FastUnlockPlaceholderStub : public C2CodeStub {\n+private:\n+  Register _obj;\n+  Register _monitor;\n+  Register _t;\n+  Register _thread;\n+  Label _slow_path;\n+  Label _push_and_slow_path;\n+  Label _check_successor;\n+  Label _unlocked;\n+public:\n+  C2FastUnlockPlaceholderStub(Register obj, Register monitor, Register t, Register thread) : C2CodeStub(),\n+    _obj(obj), _monitor(monitor), _t(t), _thread(thread) {}\n+  int max_size() const;\n+  void emit(C2_MacroAssembler& masm);\n+  Label& slow_path() { return _slow_path; }\n+  Label& push_and_slow_path() { return _push_and_slow_path; }\n+  Label& check_successor() { return _check_successor; }\n+  Label& unlocked() { return _unlocked; }\n+};\n+\n","filename":"src\/hotspot\/share\/opto\/c2_CodeStubs.hpp","additions":31,"deletions":0,"binary":false,"changes":31,"status":"modified"},{"patch":"@@ -1620,2 +1620,8 @@\n-  \/\/ For now only enable fast locking for non-array types\n-  mark_node = phase->MakeConX(markWord::prototype().value());\n+  if (UseCompactObjectHeaders) {\n+    Node* klass_node = in(AllocateNode::KlassNode);\n+    Node* proto_adr = phase->transform(new AddPNode(klass_node, klass_node, phase->MakeConX(in_bytes(Klass::prototype_header_offset()))));\n+    mark_node = LoadNode::make(*phase, control, mem, proto_adr, TypeRawPtr::BOTTOM, TypeX_X, TypeX_X->basic_type(), MemNode::unordered);\n+  } else {\n+    \/\/ For now only enable fast locking for non-array types\n+    mark_node = phase->MakeConX(markWord::prototype().value());\n+  }\n","filename":"src\/hotspot\/share\/opto\/callnode.cpp","additions":8,"deletions":2,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -1718,0 +1718,4 @@\n+      if (UseCompactObjectHeaders) {\n+        if (flat->offset() == in_bytes(Klass::prototype_header_offset()))\n+          alias_type(idx)->set_rewritable(false);\n+      }\n","filename":"src\/hotspot\/share\/opto\/compile.cpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -4577,1 +4577,1 @@\n-  if (LockingMode == LM_LIGHTWEIGHT) {\n+  if (LockingMode == LM_LIGHTWEIGHT || LockingMode == LM_PLACEHOLDER) {\n@@ -4595,2 +4595,2 @@\n-  Node *hash_mask      = _gvn.intcon(markWord::hash_mask);\n-  Node *hash_shift     = _gvn.intcon(markWord::hash_shift);\n+  Node *hash_mask      = _gvn.intcon(UseCompactObjectHeaders ? markWord::hash_mask_compact  : markWord::hash_mask);\n+  Node *hash_shift     = _gvn.intcon(UseCompactObjectHeaders ? markWord::hash_shift_compact : markWord::hash_shift);\n","filename":"src\/hotspot\/share\/opto\/library_call.cpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -1704,1 +1704,4 @@\n-  rawmem = make_store(control, rawmem, object, oopDesc::klass_offset_in_bytes(), klass_node, T_METADATA);\n+  if (!UseCompactObjectHeaders) {\n+    rawmem = make_store(control, rawmem, object, oopDesc::klass_offset_in_bytes(), klass_node, T_METADATA);\n+  }\n+\n","filename":"src\/hotspot\/share\/opto\/macro.cpp","additions":4,"deletions":1,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -1931,0 +1931,7 @@\n+  if (UseCompactObjectHeaders) {\n+    if (tkls->offset() == in_bytes(Klass::prototype_header_offset())) {\n+      \/\/ The field is Klass::_prototype_header.  Return its (constant) value.\n+      assert(this->Opcode() == Op_LoadX, \"must load a proper type from _prototype_header\");\n+      return TypeX::make(klass->prototype_header());\n+    }\n+  }\n@@ -2103,0 +2110,7 @@\n+      if (UseCompactObjectHeaders) {\n+        if (tkls->offset() == in_bytes(Klass::prototype_header_offset())) {\n+          \/\/ The field is Klass::_prototype_header. Return its (constant) value.\n+          assert(this->Opcode() == Op_LoadX, \"must load a proper type from _prototype_header\");\n+          return TypeX::make(klass->prototype_header());\n+        }\n+      }\n@@ -2193,1 +2207,1 @@\n-  if (alloc != nullptr) {\n+  if (!UseCompactObjectHeaders && alloc != nullptr) {\n","filename":"src\/hotspot\/share\/opto\/memnode.cpp","additions":15,"deletions":1,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -1460,1 +1460,0 @@\n-  ObjectMonitor *mon = nullptr;\n@@ -1482,3 +1481,6 @@\n-  if (mark.has_monitor()) {\n-    mon = mark.monitor();\n-    assert(mon != nullptr, \"must have monitor\");\n+\n+  ObjectMonitor* mon = mark.has_monitor()\n+      ? ObjectSynchronizer::read_monitor(current_thread, hobj(), mark)\n+      : nullptr;\n+\n+  if (mon != nullptr) {\n","filename":"src\/hotspot\/share\/prims\/jvmtiEnvBase.cpp","additions":6,"deletions":4,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -1829,0 +1829,7 @@\n+#if !defined(X86) && !defined(AARCH64)\n+  if (LockingMode == LM_PLACEHOLDER) {\n+    FLAG_SET_CMDLINE(LockingMode, LM_LEGACY);\n+    warning(\"New placeholder locking not supported on this platform\");\n+  }\n+#endif\n+\n@@ -2947,0 +2954,22 @@\n+#ifdef _LP64\n+  if (UseCompactObjectHeaders && UseZGC && !ZGenerational) {\n+    if (FLAG_IS_CMDLINE(UseCompactObjectHeaders)) {\n+      warning(\"Single-generational ZGC does not work with compact object headers, disabling UseCompactObjectHeaders\");\n+    }\n+    FLAG_SET_DEFAULT(UseCompactObjectHeaders, false);\n+  }\n+  if (UseCompactObjectHeaders && FLAG_IS_CMDLINE(UseCompressedClassPointers) && !UseCompressedClassPointers) {\n+    warning(\"Compact object headers require compressed class pointers. Disabling compact object headers.\");\n+    FLAG_SET_DEFAULT(UseCompactObjectHeaders, false);\n+  }\n+  if (UseCompactObjectHeaders && LockingMode == LM_LEGACY) {\n+    FLAG_SET_DEFAULT(LockingMode, LM_LIGHTWEIGHT);\n+  }\n+  if (UseCompactObjectHeaders && !UseAltGCForwarding) {\n+    FLAG_SET_DEFAULT(UseAltGCForwarding, true);\n+  }\n+  if (UseCompactObjectHeaders && !UseCompressedClassPointers) {\n+    FLAG_SET_DEFAULT(UseCompressedClassPointers, true);\n+  }\n+#endif\n+\n","filename":"src\/hotspot\/share\/runtime\/arguments.cpp","additions":29,"deletions":0,"binary":false,"changes":29,"status":"modified"},{"patch":"@@ -29,0 +29,1 @@\n+#include \"utilities\/globalDefinitions.hpp\"\n@@ -85,0 +86,7 @@\n+  } else if (LockingMode == LM_PLACEHOLDER) {\n+    \/\/ Placeholder locking uses the displace header to store a cache which\n+    \/\/ must contain either an ObjectMonitor* associated with this lock or null.\n+    \/\/ Preserve the ObjectMonitor*, the cache is cleared when a box is reused\n+    \/\/ and only read while the lock is held, so no stale ObjectMonitor* is\n+    \/\/ encountered.\n+    dest->set_displaced_header(displaced_header());\n","filename":"src\/hotspot\/share\/runtime\/basicLock.cpp","additions":8,"deletions":0,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -47,0 +47,8 @@\n+  void clear_displaced_header() {\n+    Atomic::store(&_displaced_header, markWord(0));\n+  }\n+\n+  void set_displaced_header(ObjectMonitor* mon) {\n+    Atomic::store(&_displaced_header, markWord::from_pointer(mon));\n+  }\n+\n","filename":"src\/hotspot\/share\/runtime\/basicLock.hpp","additions":8,"deletions":0,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -77,0 +77,1 @@\n+#include \"runtime\/lockStack.inline.hpp\"\n@@ -79,0 +80,1 @@\n+#include \"runtime\/placeholderSynchronizer.hpp\"\n@@ -95,0 +97,1 @@\n+#include \"utilities\/globalDefinitions.hpp\"\n@@ -1645,0 +1648,1 @@\n+        BasicLock* lock = mon_info->lock();\n@@ -1653,0 +1657,13 @@\n+        } else if (LockingMode == LM_PLACEHOLDER && exec_mode == Unpack_none) {\n+          \/\/ We have lost information about the correct state of the lock stack.\n+          \/\/ Entering may create an invalid lock stack. Inflate the lock if it\n+          \/\/ was fast_locked to restore the valid lock stack.\n+          ObjectSynchronizer::enter_for(obj, lock, deoptee_thread);\n+          if (obj->mark().is_fast_locked()) {\n+            PlaceholderSynchronizer::inflate_fast_locked_object(obj(), deoptee_thread, thread,\n+                                                                ObjectSynchronizer::InflateCause::inflate_cause_vm_internal);\n+          }\n+          assert(mon_info->owner()->is_locked(), \"object must be locked now\");\n+          assert(obj->mark().has_monitor(), \"must be\");\n+          assert(!deoptee_thread->lock_stack().contains(obj()), \"must be\");\n+          assert(PlaceholderSynchronizer::read_monitor(thread, obj())->owner() == deoptee_thread, \"must be\");\n@@ -1654,1 +1671,0 @@\n-          BasicLock* lock = mon_info->lock();\n","filename":"src\/hotspot\/share\/runtime\/deoptimization.cpp","additions":17,"deletions":1,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -133,0 +133,3 @@\n+  product(bool, UseCompactObjectHeaders, false, EXPERIMENTAL,               \\\n+          \"Use compact 64-bit object headers in 64-bit VM\")                 \\\n+                                                                            \\\n@@ -150,0 +153,1 @@\n+const bool UseCompactObjectHeaders = false;\n@@ -1983,1 +1987,1 @@\n-  product(int, LockingMode, LM_LEGACY,                                      \\\n+  product(int, LockingMode, LM_PLACEHOLDER,                                 \\\n@@ -1987,2 +1991,22 @@\n-          \"2: monitors & new lightweight locking (LM_LIGHTWEIGHT)\")         \\\n-          range(0, 2)                                                       \\\n+          \"2: monitors & new lightweight locking (LM_LIGHTWEIGHT), \"        \\\n+          \"3: placeholder (LM_PLACEHOLDER)\")                                \\\n+          range(0, 3)                                                       \\\n+                                                                            \\\n+  product(bool, OMUseC2Cache, true, \"\")                                     \\\n+                                                                            \\\n+  product(int, OMCacheSize, 8, \"\")                                          \\\n+          range(0, OMCache::CAPACITY)                                       \\\n+                                                                            \\\n+  product(bool, OMShrinkCHT, false, \"\")                                     \\\n+                                                                            \\\n+  product(int, OMSpins, 20, \"\")                                             \\\n+                                                                            \\\n+  product(int, OMYields, 5, \"\")                                             \\\n+                                                                            \\\n+  product(bool, OMDeflateAfterWait, false, \"\")                              \\\n+                                                                            \\\n+  product(bool, OMDeflateBeforeExit, false, \"\")                             \\\n+                                                                            \\\n+  product(bool, OMCacheHitRate, false, \"\")                                  \\\n+                                                                            \\\n+  product(bool, OMRecursiveFastPath, true, \"Inflated recursion check first\")\\\n","filename":"src\/hotspot\/share\/runtime\/globals.hpp","additions":27,"deletions":3,"binary":false,"changes":30,"status":"modified"},{"patch":"@@ -101,0 +101,1 @@\n+#include \"utilities\/globalDefinitions.hpp\"\n@@ -498,1 +499,2 @@\n-  _lock_stack(this) {\n+  _lock_stack(this),\n+  _om_cache(this) {\n@@ -770,0 +772,2 @@\n+  om_clear_monitor_cache();\n+\n@@ -1001,1 +1005,1 @@\n-  assert(LockingMode != LM_LIGHTWEIGHT, \"should not be called with new lightweight locking\");\n+  assert(LockingMode != LM_LIGHTWEIGHT && LockingMode != LM_PLACEHOLDER, \"should not be called with new lightweight locking\");\n@@ -1399,1 +1403,1 @@\n-  if (LockingMode == LM_LIGHTWEIGHT) {\n+  if (LockingMode == LM_LIGHTWEIGHT || LockingMode == LM_PLACEHOLDER) {\n","filename":"src\/hotspot\/share\/runtime\/javaThread.cpp","additions":7,"deletions":3,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -63,0 +63,1 @@\n+class ObjectMonitor;\n@@ -1155,0 +1156,1 @@\n+  OMCache _om_cache;\n@@ -1158,0 +1160,19 @@\n+  size_t _unlocked_inflation = 0;\n+  size_t _recursive_inflation = 0;\n+  size_t _contended_recursive_inflation = 0;\n+  size_t _contended_inflation = 0;\n+  size_t _wait_inflation = 0;\n+  size_t _lock_stack_inflation = 0;\n+\n+  size_t _wait_deflation = 0;\n+  size_t _exit_deflation = 0;\n+\n+  size_t _lock_lookup = 0;\n+  size_t _lock_hit = 0;\n+  size_t _unlock_lookup = 0;\n+  size_t _unlock_hit = 0;\n+\n+  static ByteSize lock_lookup_offset() { return byte_offset_of(JavaThread, _lock_lookup); }\n+  static ByteSize lock_hit_offset() { return byte_offset_of(JavaThread, _lock_hit); }\n+  static ByteSize unlock_lookup_offset() { return byte_offset_of(JavaThread, _unlock_lookup); }\n+  static ByteSize unlock_hit_offset() { return byte_offset_of(JavaThread, _unlock_hit); }\n@@ -1166,0 +1187,8 @@\n+\n+  static ByteSize om_cache_offset()        { return byte_offset_of(JavaThread, _om_cache); }\n+  static ByteSize om_cache_oops_offset()   { return om_cache_offset() + OMCache::oops_offset(); }\n+\n+  void om_set_monitor_cache(ObjectMonitor* monitor);\n+  void om_clear_monitor_cache();\n+  ObjectMonitor* om_get_from_monitor_cache(oop obj);\n+\n","filename":"src\/hotspot\/share\/runtime\/javaThread.hpp","additions":29,"deletions":0,"binary":false,"changes":29,"status":"modified"},{"patch":"@@ -33,0 +33,2 @@\n+#include \"logging\/log.hpp\"\n+#include \"memory\/resourceArea.hpp\"\n@@ -40,0 +42,1 @@\n+#include \"runtime\/objectMonitor.inline.hpp\"\n@@ -42,0 +45,3 @@\n+#include \"runtime\/synchronizer.hpp\"\n+#include \"utilities\/globalDefinitions.hpp\"\n+#include \"utilities\/sizes.hpp\"\n@@ -242,0 +248,75 @@\n+inline void JavaThread::om_set_monitor_cache(ObjectMonitor* monitor) {\n+  assert(LockingMode == LM_PLACEHOLDER, \"must be\");\n+  assert(monitor != nullptr, \"use om_clear_monitor_cache to clear\");\n+  assert(this == current(), \"only set own thread locals\");\n+\n+  _om_cache.set_monitor(monitor);\n+}\n+\n+inline void JavaThread::om_clear_monitor_cache() {\n+  if (LockingMode != LM_PLACEHOLDER) {\n+    return;\n+  }\n+\n+  _om_cache.clear();\n+\n+  LogTarget(Info, monitorinflation) lt;\n+  if (!lt.is_enabled()) {\n+    return;\n+  }\n+\n+  ResourceMark rm;\n+\n+  if (_unlocked_inflation != 0 ||\n+      _recursive_inflation != 0 ||\n+      _contended_recursive_inflation != 0 ||\n+      _contended_inflation != 0 ||\n+      _wait_inflation != 0 ||\n+      _lock_stack_inflation != 0) {\n+    log_info(monitorinflation)(\"Mon: %8zu Rec: %8zu CRec: %8zu Cont: %8zu Wait: %8zu Stack: %8zu Thread: %s\",\n+                              _unlocked_inflation,\n+                              _recursive_inflation,\n+                              _contended_recursive_inflation,\n+                              _contended_inflation,\n+                              _wait_inflation,\n+                              _lock_stack_inflation,\n+                              name());\n+  }\n+  _unlocked_inflation            = 0;\n+  _recursive_inflation           = 0;\n+  _contended_recursive_inflation = 0;\n+  _contended_inflation           = 0;\n+  _wait_inflation                = 0;\n+  _lock_stack_inflation          = 0;\n+\n+  if (_wait_deflation != 0 ||\n+      _exit_deflation != 0) {\n+    log_info(monitorinflation)(\"Wait: %8zu Exit: %8zu Thread: %s\",\n+                              _wait_deflation,\n+                              _exit_deflation,\n+                              name());\n+  }\n+  _wait_deflation = 0;\n+  _exit_deflation = 0;\n+\n+  if (_lock_lookup != 0 ||\n+      _unlock_lookup != 0) {\n+    const double lock_hit_rate = (double)_lock_hit \/ (double)_lock_lookup * 100;\n+    const double unlock_hit_rate = (double)_unlock_hit \/ (double)_unlock_lookup * 100;\n+    log_info(monitorinflation)(\"Lock: %3.2lf %% [%6zu \/ %6zu] Unlock: %3.2lf %% [%6zu \/ %6zu] Thread: %s\",\n+                              lock_hit_rate, _lock_hit, _lock_lookup,\n+                              unlock_hit_rate, _unlock_hit, _unlock_lookup,\n+                              name());\n+  }\n+  _lock_hit = 0;\n+  _lock_lookup = 0;\n+  _unlock_hit = 0;\n+  _unlock_lookup = 0;\n+}\n+\n+inline ObjectMonitor* JavaThread::om_get_from_monitor_cache(oop obj) {\n+  assert(obj != nullptr, \"do not look for null objects\");\n+  assert(this == current(), \"only get own thread locals\");\n+  return _om_cache.get_monitor(obj);\n+}\n+\n","filename":"src\/hotspot\/share\/runtime\/javaThread.inline.hpp","additions":81,"deletions":0,"binary":false,"changes":81,"status":"modified"},{"patch":"@@ -74,1 +74,1 @@\n-  assert(LockingMode == LM_LIGHTWEIGHT, \"never use lock-stack when light weight locking is disabled\");\n+  assert(LockingMode == LM_LIGHTWEIGHT || LockingMode == LM_PLACEHOLDER, \"never use lock-stack when light weight locking is disabled\");\n","filename":"src\/hotspot\/share\/runtime\/lockStack.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -35,0 +35,1 @@\n+class ObjectMonitor;\n@@ -37,0 +38,1 @@\n+class Thread;\n@@ -57,0 +59,1 @@\n+  bool _wait_was_inflated;\n@@ -84,0 +87,3 @@\n+  \/\/ Return true if we have room to push n oops onto this lock-stack, false otherwise.\n+  inline bool can_push(int n = 1) const;\n+\n@@ -97,0 +103,3 @@\n+  \/\/ Get the oldest oop in the stack\n+  inline oop top();\n+\n@@ -120,0 +129,4 @@\n+  bool wait_was_inflated() const { return _wait_was_inflated; };\n+  void set_wait_was_inflated() { _wait_was_inflated = true; };\n+  void clear_wait_was_inflated() { _wait_was_inflated = false; };\n+\n@@ -124,0 +137,23 @@\n+class OMCache {\n+  friend class VMStructs;\n+public:\n+  static constexpr int CAPACITY = 8;\n+\n+private:\n+  oop _oops[CAPACITY];\n+  const oop _null_sentinel;\n+  ObjectMonitor* _monitors[CAPACITY];\n+\n+public:\n+  static ByteSize oops_offset() { return byte_offset_of(OMCache, _oops); }\n+  static ByteSize monitors_offset() { return byte_offset_of(OMCache, _monitors); }\n+  static ByteSize oop_to_monitor_difference() { return monitors_offset() - oops_offset(); }\n+\n+  explicit OMCache(JavaThread* jt) : _oops(), _null_sentinel(nullptr), _monitors() {};\n+\n+  inline ObjectMonitor* get_monitor(oop o);\n+  inline void set_monitor(ObjectMonitor* monitor);\n+  inline void clear();\n+\n+};\n+\n","filename":"src\/hotspot\/share\/runtime\/lockStack.hpp","additions":36,"deletions":0,"binary":false,"changes":36,"status":"modified"},{"patch":"@@ -33,0 +33,2 @@\n+#include \"oops\/oop.inline.hpp\"\n+#include \"runtime\/globals.hpp\"\n@@ -34,0 +36,1 @@\n+#include \"runtime\/objectMonitor.inline.hpp\"\n@@ -39,0 +42,2 @@\n+#include \"runtime\/synchronizer.hpp\"\n+#include \"runtime\/placeholderSynchronizer.hpp\"\n@@ -56,0 +61,4 @@\n+inline bool LockStack::can_push(int n) const {\n+  return (CAPACITY - to_index(_top)) >= n;\n+}\n+\n@@ -83,0 +92,5 @@\n+inline oop LockStack::top() {\n+  assert(to_index(_top) > 0, \"may only call with at least one element in the stack\");\n+  return _base[to_index(_top) - 1];\n+}\n+\n@@ -200,0 +214,1 @@\n+  assert(o != nullptr, \"Catch me!\");\n@@ -225,0 +240,62 @@\n+inline void OMCache::set_monitor(ObjectMonitor *monitor) {\n+  const int end = OMCacheSize - 1;\n+  if (end < 0) {\n+    return;\n+  }\n+\n+  oop obj = monitor->object_peek();\n+  assert(obj != nullptr, \"must be alive\");\n+  assert(monitor == PlaceholderSynchronizer::read_monitor(JavaThread::current(), obj), \"must be exist in table\");\n+\n+  oop cmp_obj = obj;\n+  for (int i = 0; i < end; ++i) {\n+    if (_oops[i] == cmp_obj ||\n+        _monitors[i] == nullptr ||\n+        _monitors[i]->is_being_async_deflated()) {\n+      _oops[i] = obj;\n+      _monitors[i] = monitor;\n+      return;\n+    }\n+    \/\/ Remember Most Recent Values\n+    oop tmp_oop = obj;\n+    ObjectMonitor* tmp_mon = monitor;\n+    \/\/ Set next pair to the next most recent\n+    obj = _oops[i];\n+    monitor = _monitors[i];\n+    \/\/ Store most recent values\n+    _oops[i] = tmp_oop;\n+    _monitors[i] = tmp_mon;\n+  }\n+  _oops[end] = obj;\n+  _monitors[end] = monitor;\n+}\n+\n+inline ObjectMonitor* OMCache::get_monitor(oop o) {\n+  for (int i = 0; i < OMCacheSize; ++i) {\n+    if (_oops[i] == o) {\n+      assert(_monitors[i] != nullptr, \"monitor must exist\");\n+      if (_monitors[i]->is_being_async_deflated()) {\n+        \/\/ Bad monitor\n+        \/\/ Shift down rest\n+        for (; i < OMCacheSize - 1; ++i) {\n+          _oops[i] = _oops[i + 1];\n+          _monitors[i] =  _monitors[i + 1];\n+        }\n+        \/\/ i == CAPACITY - 1\n+        _oops[i] = nullptr;\n+        _monitors[i] = nullptr;\n+        return nullptr;\n+      }\n+      return _monitors[i];\n+    }\n+  }\n+  return nullptr;\n+}\n+\n+inline void OMCache::clear() {\n+  for (size_t i = 0 , r = 0; i < CAPACITY; ++i) {\n+    _oops[i] = nullptr;\n+    _monitors[i] = nullptr;\n+  }\n+}\n+\n","filename":"src\/hotspot\/share\/runtime\/lockStack.inline.hpp","additions":77,"deletions":0,"binary":false,"changes":77,"status":"modified"},{"patch":"@@ -36,0 +36,2 @@\n+#include \"runtime\/placeholderSynchronizer.hpp\"\n+#include \"utilities\/checkedCast.hpp\"\n@@ -63,1 +65,1 @@\n-  intx wait_time = max_intx;\n+  intx deflation_interval = max_intx;\n@@ -65,1 +67,1 @@\n-    wait_time = MIN2(wait_time, GuaranteedSafepointInterval);\n+    deflation_interval = MIN2(deflation_interval, GuaranteedSafepointInterval);\n@@ -68,1 +70,1 @@\n-    wait_time = MIN2(wait_time, AsyncDeflationInterval);\n+    deflation_interval = MIN2(deflation_interval, AsyncDeflationInterval);\n@@ -71,1 +73,1 @@\n-    wait_time = MIN2(wait_time, GuaranteedAsyncDeflationInterval);\n+    deflation_interval = MIN2(deflation_interval, GuaranteedAsyncDeflationInterval);\n@@ -77,1 +79,1 @@\n-  if (wait_time == max_intx) {\n+  if (deflation_interval == max_intx) {\n@@ -79,0 +81,1 @@\n+    PlaceholderSynchronizer::set_table_max(jt);\n@@ -82,0 +85,1 @@\n+    intx time_to_wait = deflation_interval;\n@@ -83,0 +87,1 @@\n+    bool resize = false;\n@@ -93,1 +98,28 @@\n-        ml.wait(wait_time);\n+        ml.wait(time_to_wait);\n+\n+        \/\/ Handle LightweightSynchronizer Hash Table Resizing\n+        if (PlaceholderSynchronizer::needs_resize(jt)) {\n+          resize = true;\n+          break;\n+        }\n+      }\n+    }\n+\n+    if (resize) {\n+      \/\/ TODO: Recheck this logic, especially !resize_successful and PlaceholderSynchronizer::needs_resize when is_max_size_reached == true\n+      const intx time_since_last_deflation = checked_cast<intx>(ObjectSynchronizer::time_since_last_async_deflation_ms());\n+      const bool resize_successful = PlaceholderSynchronizer::resize_table(jt);\n+      const bool deflation_interval_passed = time_since_last_deflation >= deflation_interval;\n+      const bool deflation_needed = deflation_interval_passed && ObjectSynchronizer::is_async_deflation_needed();\n+\n+      if (!resize_successful) {\n+        \/\/ Resize failed, try again in 250 ms\n+        time_to_wait = 250;\n+      } else if (deflation_interval_passed) {\n+        time_to_wait = deflation_interval;\n+      } else {\n+        time_to_wait = deflation_interval - time_since_last_deflation;\n+      }\n+\n+      if (!deflation_needed) {\n+        continue;\n@@ -95,0 +127,2 @@\n+    } else {\n+      time_to_wait = deflation_interval;\n@@ -97,0 +131,1 @@\n+\n","filename":"src\/hotspot\/share\/runtime\/monitorDeflationThread.cpp","additions":41,"deletions":6,"binary":false,"changes":47,"status":"modified"},{"patch":"@@ -52,0 +52,1 @@\n+#include \"runtime\/placeholderSynchronizer.hpp\"\n@@ -55,0 +56,1 @@\n+#include \"runtime\/synchronizer.hpp\"\n@@ -56,0 +58,1 @@\n+#include \"utilities\/debug.hpp\"\n@@ -289,4 +292,0 @@\n-oop ObjectMonitor::object_peek() const {\n-  return _object.peek();\n-}\n-\n@@ -314,0 +313,6 @@\n+#define assert_mark_word_concistency()                                                 \\\n+  assert(LockingMode == LM_PLACEHOLDER || object()->mark() == markWord::encode(this),  \\\n+         \"object mark must match encoded this: mark=\" INTPTR_FORMAT                    \\\n+         \", encoded this=\" INTPTR_FORMAT, object()->mark().value(),                    \\\n+         markWord::encode(this).value());\n+\n@@ -316,0 +321,14 @@\n+bool ObjectMonitor::try_enter(JavaThread* current) {\n+  void* cur = try_set_owner_from(nullptr, current);\n+  if (cur == nullptr) {\n+    assert(_recursions == 0, \"invariant\");\n+    return true;\n+  }\n+\n+  if (cur == current) {\n+    _recursions++;\n+    return true;\n+  }\n+\n+  return false;\n+}\n@@ -396,1 +415,1 @@\n-  if (LockingMode != LM_LIGHTWEIGHT && current->is_lock_owned((address)cur)) {\n+  if (LockingMode != LM_LIGHTWEIGHT && LockingMode != LM_PLACEHOLDER && current->is_lock_owned((address)cur)) {\n@@ -415,4 +434,1 @@\n-    assert(object()->mark() == markWord::encode(this),\n-           \"object mark must match encoded this: mark=\" INTPTR_FORMAT\n-           \", encoded this=\" INTPTR_FORMAT, object()->mark().value(),\n-           markWord::encode(this).value());\n+    assert_mark_word_concistency();\n@@ -435,1 +451,1 @@\n-    if (l_object != nullptr) {\n+    if (LockingMode != LM_PLACEHOLDER && l_object != nullptr) {\n@@ -509,1 +525,1 @@\n-  assert(object()->mark() == markWord::encode(this), \"invariant\");\n+  assert_mark_word_concistency();\n@@ -571,1 +587,1 @@\n-bool ObjectMonitor::deflate_monitor() {\n+bool ObjectMonitor::deflate_monitor(Thread* current) {\n@@ -577,0 +593,7 @@\n+  if (LockingMode == LM_PLACEHOLDER && is_being_async_deflated()) {\n+    \/\/ This happens when a locked monitor is deflated by a java thread\n+    \/\/ returning itself to fast_locked\n+    assert(is_owner_anonymous(), \"must stay anonymous when the java thread deflates\");\n+    return true;\n+  }\n+\n@@ -642,0 +665,1 @@\n+  }\n@@ -643,2 +667,7 @@\n-    \/\/ Install the old mark word if nobody else has already done it.\n-    install_displaced_markword_in_object(obj);\n+  if (LockingMode == LM_PLACEHOLDER) {\n+    PlaceholderSynchronizer::deflate_monitor(current, obj, this);\n+  } else {\n+    if (obj != nullptr) {\n+      \/\/ Install the old mark word if nobody else has already done it.\n+      install_displaced_markword_in_object(obj);\n+    }\n@@ -652,0 +681,71 @@\n+bool ObjectMonitor::deflate_anon_monitor(JavaThread* current) {\n+  assert(owner_raw() == current, \"must be\");\n+  assert(LockingMode == LM_PLACEHOLDER, \"must be\");\n+\n+  LockStack& lock_stack = current->lock_stack();\n+\n+  if (!lock_stack.can_push(1 + _recursions)) {\n+    \/\/ Will not be able to push the oop on the lock stack.\n+    return false;\n+  }\n+\n+  if (is_contended()) {\n+    \/\/ Easy checks are first - the ObjectMonitor is busy so no deflation.\n+    return false;\n+  }\n+\n+  \/\/ Make sure if a thread sees contentions() < 0 they also see owner == ANONYMOUS_OWNER\n+  set_owner_from(current, reinterpret_cast<void*>(ANONYMOUS_OWNER));\n+\n+    \/\/ Recheck after setting owner\n+  bool cleanup = is_contended();\n+\n+\n+  if (!cleanup) {\n+    \/\/ Make a zero contentions field negative to force any contending threads\n+    \/\/ to retry. Because this is only called while holding the lock, the owner\n+    \/\/ is anonymous and contentions is held over enter in inflate_and_enter\n+    \/\/ it means that if the cas succeeds then we can have no other thread\n+    \/\/ racily inserting themselves on the _waiters or _cxq lists, the\n+    \/\/ entry list is protected by the lock (_waiter technically too, only\n+    \/\/ removals are done outside the lock)\n+    \/\/ TODO: Double check _succ and _responsible invariants\n+    if (Atomic::cmpxchg(&_contentions, 0, INT_MIN) != 0) {\n+      \/\/ Contentions was no longer 0 so we lost the race.\n+      cleanup = true;\n+    }\n+  }\n+\n+  if (cleanup) {\n+    \/\/ Could not deflate\n+    set_owner_from_anonymous(current);\n+    return false;\n+  }\n+\n+  \/\/ Sanity checks for the races:\n+  guarantee(is_owner_anonymous(), \"must be\");\n+  guarantee(contentions() < 0, \"must be negative: contentions=%d\",\n+            contentions());\n+  guarantee(_waiters == 0, \"must be 0: waiters=%d\", _waiters);\n+  guarantee(_cxq == nullptr, \"must be no contending threads: cxq=\"\n+            INTPTR_FORMAT, p2i(_cxq));\n+  guarantee(_EntryList == nullptr,\n+            \"must be no entering threads: EntryList=\" INTPTR_FORMAT,\n+            p2i(_EntryList));\n+\n+  oop obj = object();\n+\n+  PlaceholderSynchronizer::deflate_anon_monitor(current, obj, this);\n+\n+  \/\/ We are deflated, restore the correct lock_stack\n+  lock_stack.push(obj);\n+  for (int i = 0; i < _recursions; i++) {\n+    bool entered = lock_stack.try_recursive_enter(obj);\n+    assert(entered, \"must have entered here\");\n+  }\n+\n+  \/\/ We leave owner == ANONYMOUS_OWNER and contentions < 0\n+  \/\/ to force any racing threads to retry.\n+  return true;  \/\/ Success, ObjectMonitor has been deflated.\n+}\n+\n@@ -658,0 +758,1 @@\n+  assert(LockingMode != LM_PLACEHOLDER, \"Placeholder has no dmw\");\n@@ -990,0 +1091,1 @@\n+  assert(current->thread_state() != _thread_blocked, \"invariant\");\n@@ -993,3 +1095,1 @@\n-  assert(object()->mark() == markWord::encode(this), \"invariant\");\n-\n-  assert(current->thread_state() != _thread_blocked, \"invariant\");\n+  assert_mark_word_concistency();\n@@ -1052,1 +1152,1 @@\n-  assert(object()->mark() == markWord::encode(this), \"invariant\");\n+  assert_mark_word_concistency();\n@@ -1186,1 +1286,1 @@\n-    if (LockingMode != LM_LIGHTWEIGHT && current->is_lock_owned((address)cur)) {\n+    if (LockingMode != LM_LIGHTWEIGHT && LockingMode != LM_PLACEHOLDER && current->is_lock_owned((address)cur)) {\n@@ -1401,1 +1501,1 @@\n-    if (LockingMode != LM_LIGHTWEIGHT && current->is_lock_owned((address)cur)) {\n+    if (LockingMode != LM_LIGHTWEIGHT && LockingMode != LM_PLACEHOLDER && current->is_lock_owned((address)cur)) {\n@@ -1440,1 +1540,1 @@\n-  if (LockingMode != LM_LIGHTWEIGHT && current->is_lock_owned((address)cur)) {\n+  if (LockingMode != LM_LIGHTWEIGHT && LockingMode != LM_PLACEHOLDER && current->is_lock_owned((address)cur)) {\n@@ -1680,0 +1780,9 @@\n+  bool deflated = false;\n+\n+  if (LockingMode == LM_PLACEHOLDER && OMDeflateAfterWait && current->lock_stack().wait_was_inflated()) {\n+    if (deflate_anon_monitor(current)) {\n+      current->_wait_deflation++;\n+      deflated = true;\n+    }\n+  }\n+\n@@ -1681,1 +1790,1 @@\n-  assert(owner_raw() == current, \"invariant\");\n+  assert(deflated || owner_raw() == current, \"invariant\");\n@@ -1683,1 +1792,1 @@\n-  assert(object()->mark() == markWord::encode(this), \"invariant\");\n+  assert_mark_word_concistency();\n@@ -2203,1 +2312,1 @@\n-  st->print_cr(\"  _header = \" INTPTR_FORMAT, header().value());\n+  st->print_cr(\"  _header = \" INTPTR_FORMAT, header_value());\n","filename":"src\/hotspot\/share\/runtime\/objectMonitor.cpp","additions":133,"deletions":24,"binary":false,"changes":157,"status":"modified"},{"patch":"@@ -124,0 +124,3 @@\n+#ifndef OM_CACHE_LINE_SIZE\n+\/\/ Use DEFAULT_CACHE_LINE_SIZE if not already specified for\n+\/\/ the current build platform.\n@@ -125,0 +128,1 @@\n+#endif\n@@ -152,1 +156,2 @@\n-  #define DEFLATER_MARKER reinterpret_cast<void*>(2)\n+  static const uintptr_t DEFLATER_MARKER_VALUE = 2;\n+  #define DEFLATER_MARKER reinterpret_cast<void*>(DEFLATER_MARKER_VALUE)\n@@ -156,0 +161,1 @@\n+  static const uintptr_t ANONYMOUS_OWNER_OR_DEFLATER_MARKER = ANONYMOUS_OWNER | DEFLATER_MARKER_VALUE;\n@@ -217,0 +223,1 @@\n+  static ByteSize header_offset()      { return byte_offset_of(ObjectMonitor, _header); }\n@@ -234,0 +241,3 @@\n+  \/\/ Placeholder locking fetches ObjectMonitor references from a cache\n+  \/\/ instead of the markWord and doesn't work with tagged values.\n+  \/\/\n@@ -235,1 +245,1 @@\n-    ((in_bytes(ObjectMonitor::f ## _offset())) - checked_cast<int>(markWord::monitor_value))\n+    ((in_bytes(ObjectMonitor::f ## _offset())) - (LockingMode == LM_PLACEHOLDER ? 0 : checked_cast<int>(markWord::monitor_value)))\n@@ -238,0 +248,1 @@\n+  uintptr_t          header_value() const;\n@@ -241,0 +252,3 @@\n+  intptr_t           hash_placeholder() const;\n+  void               set_hash_placeholder(intptr_t hash);\n+\n@@ -253,0 +267,8 @@\n+  bool is_contended() const {\n+    intptr_t ret_code = intptr_t(_waiters) | intptr_t(_cxq) | intptr_t(_EntryList);\n+    int cnts = contentions();\n+    if (cnts > 0) {\n+      ret_code |= intptr_t(cnts);\n+    }\n+    return ret_code != 0;\n+  }\n@@ -310,0 +332,3 @@\n+  bool      object_is_cleared() const;\n+  bool      object_is_dead() const;\n+  bool      object_refers_to(oop obj) const;\n@@ -333,0 +358,1 @@\n+  bool      try_enter(JavaThread* current);\n@@ -362,1 +388,4 @@\n-  bool      deflate_monitor();\n+  bool      deflate_monitor(Thread* current);\n+public:\n+  bool      deflate_anon_monitor(JavaThread* current);\n+private:\n@@ -366,0 +395,10 @@\n+\/\/ RAII object to ensure that ObjectMonitor::is_being_async_deflated() is\n+\/\/ stable within the context of this mark.\n+class ObjectMonitorContentionMark {\n+  ObjectMonitor* _monitor;\n+\n+public:\n+  ObjectMonitorContentionMark(ObjectMonitor* monitor);\n+  ~ObjectMonitorContentionMark();\n+};\n+\n","filename":"src\/hotspot\/share\/runtime\/objectMonitor.hpp","additions":42,"deletions":3,"binary":false,"changes":45,"status":"modified"},{"patch":"@@ -32,0 +32,1 @@\n+#include \"oops\/markWord.hpp\"\n@@ -33,0 +34,1 @@\n+#include \"runtime\/globals.hpp\"\n@@ -35,0 +37,2 @@\n+#include \"utilities\/checkedCast.hpp\"\n+#include \"utilities\/globalDefinitions.hpp\"\n@@ -37,1 +41,1 @@\n-  if (LockingMode == LM_LIGHTWEIGHT) {\n+  if (LockingMode == LM_LIGHTWEIGHT || LockingMode == LM_PLACEHOLDER) {\n@@ -53,0 +57,1 @@\n+  assert(LockingMode != LM_PLACEHOLDER, \"Placeholder locking does not use header\");\n@@ -56,0 +61,4 @@\n+inline uintptr_t ObjectMonitor::header_value() const {\n+  return Atomic::load(&_header).value();\n+}\n+\n@@ -61,0 +70,1 @@\n+  assert(LockingMode != LM_PLACEHOLDER, \"Placeholder locking does not use header\");\n@@ -64,0 +74,10 @@\n+inline intptr_t ObjectMonitor::hash_placeholder() const {\n+  assert(LockingMode == LM_PLACEHOLDER, \"Only used by placeholder locking\");\n+  return Atomic::load(&_header).hash();\n+}\n+\n+inline void ObjectMonitor::set_hash_placeholder(intptr_t hash) {\n+  assert(LockingMode == LM_PLACEHOLDER, \"Only used by placeholder locking\");\n+  Atomic::store(&_header, markWord::zero().copy_set_hash(hash));\n+}\n+\n@@ -183,0 +203,31 @@\n+inline ObjectMonitorContentionMark::ObjectMonitorContentionMark(ObjectMonitor* monitor)\n+  : _monitor(monitor) {\n+  _monitor->add_to_contentions(1);\n+}\n+\n+inline ObjectMonitorContentionMark::~ObjectMonitorContentionMark() {\n+  _monitor->add_to_contentions(-1);\n+}\n+\n+inline oop ObjectMonitor::object_peek() const {\n+  if (_object.is_null()) {\n+    return nullptr;\n+  }\n+  return _object.peek();\n+}\n+\n+inline bool ObjectMonitor::object_is_dead() const {\n+  return object_peek() == nullptr;\n+}\n+\n+inline bool ObjectMonitor::object_is_cleared() const {\n+  return _object.is_null();\n+}\n+\n+inline bool ObjectMonitor::object_refers_to(oop obj) const {\n+  if (_object.is_null()) {\n+    return false;\n+  }\n+  return _object.peek() == obj;\n+}\n+\n","filename":"src\/hotspot\/share\/runtime\/objectMonitor.inline.hpp","additions":52,"deletions":1,"binary":false,"changes":53,"status":"modified"},{"patch":"@@ -0,0 +1,1004 @@\n+\/*\n+ * Copyright (c) 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+\n+#include \"classfile\/vmSymbols.hpp\"\n+#include \"javaThread.inline.hpp\"\n+#include \"jfrfiles\/jfrEventClasses.hpp\"\n+#include \"logging\/log.hpp\"\n+#include \"logging\/logStream.hpp\"\n+#include \"memory\/resourceArea.hpp\"\n+#include \"oops\/oop.inline.hpp\"\n+#include \"runtime\/globals_extension.hpp\"\n+#include \"runtime\/javaThread.hpp\"\n+#include \"runtime\/lockStack.inline.hpp\"\n+#include \"runtime\/mutexLocker.hpp\"\n+#include \"runtime\/objectMonitor.inline.hpp\"\n+#include \"runtime\/perfData.inline.hpp\"\n+#include \"runtime\/placeholderSynchronizer.hpp\"\n+#include \"runtime\/safepointVerifiers.hpp\"\n+#include \"runtime\/synchronizer.hpp\"\n+#include \"utilities\/concurrentHashTable.inline.hpp\"\n+\n+\n+\/\/\n+\/\/ Placeholder synchronization.\n+\/\/\n+\/\/ When the lightweight synchronization needs to use a monitor the link\n+\/\/ between the object and the monitor is stored in a concurrent hash table\n+\/\/ instead of in the mark word. This has the benefit that it further decouples\n+\/\/ the mark word from the synchronization code.\n+\/\/\n+\n+\/\/ ConcurrentHashTable storing links from objects to ObjectMonitors\n+class ObjectMonitorWorld : public CHeapObj<mtThread> {\n+  struct Config {\n+    using Value = ObjectMonitor*;\n+    static uintx get_hash(Value const& value, bool* is_dead) {\n+      return (uintx)value->hash_placeholder();\n+    }\n+    static void* allocate_node(void* context, size_t size, Value const& value) {\n+      return AllocateHeap(size, mtThread);\n+    };\n+    static void free_node(void* context, void* memory, Value const& value) {\n+      FreeHeap(memory);\n+    }\n+  };\n+  using ConcurrentTable = ConcurrentHashTable<Config, mtThread>;\n+\n+  ConcurrentTable* _table;\n+  volatile bool _resize;\n+  uint32_t _shrink_count;\n+\n+  class Lookup : public StackObj {\n+    oop _obj;\n+\n+  public:\n+    Lookup(oop obj) : _obj(obj) {}\n+\n+    uintx get_hash() const {\n+      uintx hash = _obj->mark().hash();\n+      assert(hash != 0, \"should have a hash\");\n+      return hash;\n+    }\n+\n+    bool equals(ObjectMonitor** value) {\n+      \/\/ The entry is going to be removed soon.\n+      assert(*value != nullptr, \"must be\");\n+      return (*value)->object_refers_to(_obj);\n+    }\n+\n+    bool is_dead(ObjectMonitor** value) {\n+      assert(*value != nullptr, \"must be\");\n+      return (*value)->object_is_cleared();\n+    }\n+  };\n+\n+  class LookupMonitor : public StackObj {\n+    ObjectMonitor* _monitor;\n+\n+  public:\n+    LookupMonitor(ObjectMonitor* monitor) : _monitor(monitor) {}\n+\n+    uintx get_hash() const {\n+      return _monitor->hash_placeholder();\n+    }\n+\n+    bool equals(ObjectMonitor** value) {\n+      return (*value) == _monitor;\n+    }\n+\n+    bool is_dead(ObjectMonitor** value) {\n+      assert(*value != nullptr, \"must be\");\n+      return (*value)->object_is_dead();\n+    }\n+  };\n+\n+  static size_t max_log_size() {\n+    \/\/ TODO: Evaluate the max size, is 2 ** 21 to small.\n+    \/\/       Given the AvgMonitorsPerThreadEstimate default estimate\n+    \/\/\n+    return ConcurrentTable::DEFAULT_MAX_SIZE_LOG2;\n+  }\n+\n+  static size_t min_log_size() {\n+    \/\/ TODO: Evaluate the min size, currently ~= log(AvgMonitorsPerThreadEstimate default)\n+    return 10;\n+  }\n+\n+  template<typename V>\n+  static size_t clamp_log_size(V log_size) {\n+    return MAX2(MIN2(log_size, checked_cast<V>(max_log_size())), checked_cast<V>(min_log_size()));\n+  }\n+\n+  static size_t initial_log_size() {\n+    const size_t estimate = log2i(MAX2(os::processor_count(), 1)) + log2i(MAX2(AvgMonitorsPerThreadEstimate, size_t(1)));\n+    return clamp_log_size(estimate);\n+  }\n+\n+  static size_t grow_hint () {\n+    \/\/ TODO: Evaluate why 4 is a good grow hint.\n+    \/\/       Have seen grow hint hits when lower with a\n+    \/\/       load factor as low as 0.1. (Grow Hint = 3)\n+    \/\/ TODO: Evaluate the hash code used, are large buckets\n+    \/\/       expected even with a low load factor. Or is it\n+    \/\/       something with the hashing used.\n+    return ConcurrentTable::DEFAULT_GROW_HINT;\n+  }\n+\n+  static size_t log_shrink_difference() {\n+    \/\/ TODO: Evaluate shrink heuristics, currently disabled by\n+    \/\/       default, and only really shrinks if AvgMonitorsPerThreadEstimate\n+    \/\/       is also set to a none default value\n+    return 2;\n+  }\n+\n+public:\n+  ObjectMonitorWorld()\n+  : _table(new ConcurrentTable(initial_log_size(), max_log_size(), grow_hint())),\n+    _resize(false),\n+    _shrink_count(0) {}\n+\n+  void verify_monitor_get_result(oop obj, ObjectMonitor* monitor) {\n+#ifdef ASSERT\n+    if (SafepointSynchronize::is_at_safepoint()) {\n+      bool has_monitor = obj->mark().has_monitor();\n+      assert(has_monitor == (monitor != nullptr),\n+          \"Inconsistency between markWord and OMW table has_monitor: %s monitor: \" PTR_FORMAT,\n+          BOOL_TO_STR(has_monitor), p2i(monitor));\n+    }\n+#endif\n+  }\n+\n+  ObjectMonitor* monitor_get(Thread* current, oop obj) {\n+    ObjectMonitor* result = nullptr;\n+    Lookup lookup_f(obj);\n+    auto found_f = [&](ObjectMonitor** found) {\n+      assert((*found)->object_peek() == obj, \"must be\");\n+      result = *found;\n+    };\n+    _table->get(current, lookup_f, found_f);\n+    verify_monitor_get_result(obj, result);\n+    return result;\n+  }\n+\n+  void try_notify_grow() {\n+    if (!_table->is_max_size_reached() && !Atomic::load(&_resize)) {\n+      Atomic::store(&_resize, true);\n+      if (MonitorDeflation_lock->try_lock()) {\n+        MonitorDeflation_lock->notify();\n+        MonitorDeflation_lock->unlock();\n+      }\n+    }\n+  }\n+\n+  void set_table_max(JavaThread* current) {\n+    while (!_table->is_max_size_reached()) {\n+      _table->grow(current);\n+    }\n+  }\n+\n+  bool needs_shrink(size_t log_target, size_t log_size) {\n+    return OMShrinkCHT && log_target + log_shrink_difference() <= log_size;\n+  }\n+\n+  bool needs_grow(size_t log_target, size_t log_size) {\n+    return log_size < log_target;\n+  }\n+\n+  bool needs_resize(JavaThread* current, size_t ceiling, size_t count, size_t max) {\n+    const size_t log_size = _table->get_size_log2(current);\n+    const int log_ceiling = log2i_graceful(ceiling);\n+    const int log_max = log2i_graceful(max);\n+    const size_t log_count = log2i(MAX2(count, size_t(1)));\n+    const size_t log_target = clamp_log_size(MAX2(log_ceiling, log_max) + 2);\n+\n+    return needs_grow(log_target, log_size) || needs_shrink(log_target, log_size) || Atomic::load(&_resize);\n+  }\n+\n+  bool resize(JavaThread* current, size_t ceiling, size_t count, size_t max) {\n+    const size_t log_size = _table->get_size_log2(current);\n+    const int log_ceiling = log2i_graceful(ceiling);\n+    const int log_max = log2i_graceful(max);\n+    const size_t log_count = log2i(MAX2(count, size_t(1)));\n+    const size_t log_target = clamp_log_size(MAX2(log_ceiling, log_max) + 2);\n+    LogTarget(Info, monitorinflation) lt;\n+\n+    auto print_table_stats = [&]() {\n+      ResourceMark rm;\n+      LogStream ls(lt);\n+      auto vs_f = [](Config::Value* v) { return sizeof(Config::Value); };\n+      _table->statistics_to(current, vs_f, &ls, \"ObjectMonitorWorld\");\n+    };\n+\n+    bool success = true;\n+\n+    if (needs_grow(log_target, log_size)) {\n+      \/\/ Grow\n+      lt.print(\"Growing to %02zu->%02zu\", log_size, log_target);\n+      success = _table->grow(current, log_target);\n+      print_table_stats();\n+    } else if (!_table->is_max_size_reached() && Atomic::load(&_resize)) {\n+      lt.print(\"WARNING: Getting resize hints with Size: %02zu Ceiling: %2i Target: %02zu\", log_size, log_ceiling, log_target);\n+      print_table_stats();\n+      success = false;\n+    }\n+\n+    if (needs_shrink(log_target, log_size)) {\n+      _shrink_count++;\n+      \/\/ Shrink\n+      lt.print(\"Shrinking to %02zu->%02zu\", log_size, log_target);\n+      success = _table->shrink(current, log_target);\n+      print_table_stats();\n+    }\n+\n+    if (success) {\n+      Atomic::store(&_resize, _table->is_max_size_reached());\n+    }\n+\n+    return success;\n+  }\n+\n+  ObjectMonitor* monitor_put_get(Thread* current, ObjectMonitor* monitor, oop obj) {\n+    \/\/ Enter the monitor into the concurrent hashtable.\n+    ObjectMonitor* result = monitor;\n+    Lookup lookup_f(obj);\n+    auto found_f = [&](ObjectMonitor** found) {\n+      assert((*found)->object_peek() == obj, \"must be\");\n+      result = *found;\n+    };\n+    bool grow;\n+    _table->insert_get(current, lookup_f, monitor, found_f, &grow);\n+    verify_monitor_get_result(obj, result);\n+    if (grow) {\n+      try_notify_grow();\n+    }\n+    return result;\n+  }\n+\n+  bool remove_monitor_entry(Thread* current, ObjectMonitor* monitor) {\n+    LookupMonitor lookup_f(monitor);\n+    return _table->remove(current, lookup_f);\n+  }\n+\n+  bool contains_monitor(Thread* current, ObjectMonitor* monitor) {\n+    LookupMonitor lookup_f(monitor);\n+    bool result = false;\n+    auto found_f = [&](ObjectMonitor** found) {\n+      result = true;\n+    };\n+    _table->get(current, lookup_f, found_f);\n+    return result;\n+  }\n+\n+  void print_on(outputStream* st) {\n+    auto printer = [&] (ObjectMonitor** entry) {\n+       ObjectMonitor* om = *entry;\n+       oop obj = om->object_peek();\n+       st->print(\"monitor \" PTR_FORMAT \" \", p2i(om));\n+       st->print(\"object \" PTR_FORMAT, p2i(obj));\n+       assert(obj->mark().hash() == om->hash_placeholder(), \"hash must match\");\n+       st->cr();\n+       return true;\n+    };\n+    if (SafepointSynchronize::is_at_safepoint()) {\n+      _table->do_safepoint_scan(printer);\n+    } else {\n+      _table->do_scan(Thread::current(), printer);\n+    }\n+  }\n+};\n+\n+ObjectMonitorWorld* PlaceholderSynchronizer::_omworld = nullptr;\n+\n+ObjectMonitor* PlaceholderSynchronizer::get_or_insert_monitor_from_table(oop object, JavaThread* current, bool try_read, bool* inserted) {\n+  assert(LockingMode == LM_PLACEHOLDER, \"must be\");\n+\n+  if (try_read) {\n+    ObjectMonitor* monitor = read_monitor(current, object);\n+    if (monitor != nullptr) {\n+      *inserted = false;\n+      return monitor;\n+    }\n+  }\n+\n+  ObjectMonitor* alloced_monitor = new ObjectMonitor(object);\n+  alloced_monitor->set_owner_anonymous();\n+\n+  \/\/ Try insert monitor\n+  ObjectMonitor* monitor = add_monitor(current, alloced_monitor, object);\n+\n+  *inserted = alloced_monitor == monitor;\n+  if (!*inserted) {\n+    delete alloced_monitor;\n+  }\n+\n+  return monitor;\n+}\n+\n+static void log_inflate(Thread* current, oop object, const ObjectSynchronizer::InflateCause cause) {\n+  if (log_is_enabled(Trace, monitorinflation)) {\n+    ResourceMark rm(current);\n+    log_info(monitorinflation)(\"inflate(has_locker): object=\" INTPTR_FORMAT \", mark=\"\n+                               INTPTR_FORMAT \", type='%s' cause %s\", p2i(object),\n+                               object->mark().value(), object->klass()->external_name(),\n+                               ObjectSynchronizer::inflate_cause_name(cause));\n+  }\n+}\n+\n+static void post_monitor_inflate_event(EventJavaMonitorInflate* event,\n+                                       const oop obj,\n+                                       ObjectSynchronizer::InflateCause cause) {\n+  assert(event != nullptr, \"invariant\");\n+  event->set_monitorClass(obj->klass());\n+  event->set_address((uintptr_t)(void*)obj);\n+  event->set_cause((u1)cause);\n+  event->commit();\n+}\n+\n+ObjectMonitor* PlaceholderSynchronizer::get_or_insert_monitor(oop object, JavaThread* current, const ObjectSynchronizer::InflateCause cause, bool try_read) {\n+  assert(LockingMode == LM_PLACEHOLDER, \"must be\");\n+\n+  EventJavaMonitorInflate event;\n+\n+  bool inserted;\n+  ObjectMonitor* monitor = get_or_insert_monitor_from_table(object, current, try_read, &inserted);\n+\n+  if (inserted) {\n+    \/\/ Hopefully the performance counters are allocated on distinct\n+    \/\/ cache lines to avoid false sharing on MP systems ...\n+    OM_PERFDATA_OP(Inflations, inc());\n+    log_inflate(current, object, cause);\n+    if (event.should_commit()) {\n+      post_monitor_inflate_event(&event, object, cause);\n+    }\n+\n+    \/\/ The monitor has an anonymous owner so it is safe from async deflation.\n+    ObjectSynchronizer::_in_use_list.add(monitor);\n+  }\n+\n+  return monitor;\n+}\n+\n+\/\/ Add the hashcode to the monitor to match the object and put it in the hashtable.\n+ObjectMonitor* PlaceholderSynchronizer::add_monitor(JavaThread* current, ObjectMonitor* monitor, oop obj) {\n+  assert(LockingMode == LM_PLACEHOLDER, \"must be\");\n+  assert(obj == monitor->object(), \"must be\");\n+\n+  intptr_t hash = obj->mark().hash();\n+  assert(hash != 0, \"must be set when claiming the object monitor\");\n+  monitor->set_hash_placeholder(hash);\n+\n+  return _omworld->monitor_put_get(current, monitor, obj);\n+}\n+\n+bool PlaceholderSynchronizer::remove_monitor(Thread* current, oop obj, ObjectMonitor* monitor) {\n+  assert(LockingMode == LM_PLACEHOLDER, \"must be\");\n+  assert(monitor->object_peek() == obj, \"must be, cleared objects are removed by is_dead\");\n+\n+  return _omworld->remove_monitor_entry(current, monitor);\n+}\n+\n+void PlaceholderSynchronizer::deflate_mark_word(oop obj) {\n+  assert(LockingMode == LM_PLACEHOLDER, \"must use lightweight locking\");\n+\n+  markWord mark = obj->mark_acquire();\n+  assert(!mark.has_no_hash(), \"obj with inflated monitor must have had a hash\");\n+\n+  while (mark.has_monitor()) {\n+    const markWord new_mark = mark.clear_lock_bits().set_unlocked();\n+    mark = obj->cas_set_mark(new_mark, mark);\n+  }\n+}\n+\n+void PlaceholderSynchronizer::initialize() {\n+  _omworld = new ObjectMonitorWorld();\n+\n+  if (!FLAG_IS_CMDLINE(AvgMonitorsPerThreadEstimate)) {\n+    \/\/ This is updated after ceiling is set and ObjectMonitorWorld is created;\n+    \/\/ TODO: Clean this up and find a good initial ceiling,\n+    \/\/       and initial HashTable size\n+    FLAG_SET_ERGO(AvgMonitorsPerThreadEstimate, 0);\n+  }\n+}\n+\n+void PlaceholderSynchronizer::set_table_max(JavaThread* current) {\n+  if (LockingMode != LM_PLACEHOLDER) {\n+    return;\n+  }\n+  _omworld->set_table_max(current);\n+}\n+\n+bool PlaceholderSynchronizer::needs_resize(JavaThread *current) {\n+  if (LockingMode != LM_PLACEHOLDER) {\n+    return false;\n+  }\n+  return _omworld->needs_resize(current,\n+                                  ObjectSynchronizer::in_use_list_ceiling(),\n+                                  ObjectSynchronizer::_in_use_list.count(),\n+                                  ObjectSynchronizer::_in_use_list.max());\n+}\n+\n+bool PlaceholderSynchronizer::resize_table(JavaThread* current) {\n+  if (LockingMode != LM_PLACEHOLDER) {\n+    return true;\n+  }\n+  return _omworld->resize(current,\n+                          ObjectSynchronizer::in_use_list_ceiling(),\n+                          ObjectSynchronizer::_in_use_list.count(),\n+                          ObjectSynchronizer::_in_use_list.max());\n+}\n+\n+class LockStackInflateContendedLocks : private OopClosure {\n+ private:\n+  oop _contended_oops[LockStack::CAPACITY];\n+  int _length;\n+\n+  void do_oop(oop* o) final {\n+    oop obj = *o;\n+    if (obj->mark_acquire().has_monitor()) {\n+      if (_length > 0 && _contended_oops[_length-1] == obj) {\n+        \/\/ assert(VM_Version::supports_recursive_lightweight_locking(), \"must be\");\n+        \/\/ Recursive\n+        return;\n+      }\n+      _contended_oops[_length++] = obj;\n+    }\n+  }\n+\n+  void do_oop(narrowOop* o) final {\n+    ShouldNotReachHere();\n+  }\n+\n+ public:\n+  LockStackInflateContendedLocks() :\n+    _contended_oops(),\n+    _length(0) {};\n+\n+  void inflate(JavaThread* locking_thread, JavaThread* current) {\n+    locking_thread->lock_stack().oops_do(this);\n+    for (int i = 0; i < _length; i++) {\n+      PlaceholderSynchronizer::\n+        inflate_fast_locked_object(_contended_oops[i], locking_thread, current, ObjectSynchronizer::inflate_cause_vm_internal);\n+    }\n+  }\n+};\n+\n+void PlaceholderSynchronizer::ensure_lock_stack_space(JavaThread* locking_thread, JavaThread* current) {\n+  LockStack& lock_stack = locking_thread->lock_stack();\n+\n+  \/\/ Make room on lock_stack\n+  if (lock_stack.is_full()) {\n+    \/\/ Inflate contented objects\n+    LockStackInflateContendedLocks().inflate(locking_thread, current);\n+    if (lock_stack.is_full()) {\n+      \/\/ Inflate the oldest object\n+      inflate_fast_locked_object(lock_stack.bottom(), locking_thread, current, ObjectSynchronizer::inflate_cause_vm_internal);\n+    }\n+  }\n+}\n+\n+class VerifyThreadState {\n+  bool _no_safepoint;\n+  union {\n+    struct {} _dummy;\n+    NoSafepointVerifier _nsv;\n+  };\n+\n+public:\n+  VerifyThreadState(JavaThread* locking_thread, JavaThread* current) : _no_safepoint(locking_thread != current) {\n+    assert(current == Thread::current(), \"must be\");\n+    assert(locking_thread == current || locking_thread->is_obj_deopt_suspend(), \"locking_thread may not run concurrently\");\n+    if (_no_safepoint) {\n+      ::new (&_nsv) NoSafepointVerifier();\n+    }\n+  }\n+  ~VerifyThreadState() {\n+    if (_no_safepoint){\n+      _nsv.~NoSafepointVerifier();\n+    }\n+  }\n+};\n+\n+void PlaceholderSynchronizer::enter_for(Handle obj, BasicLock* lock, JavaThread* locking_thread) {\n+  enter(obj, lock, locking_thread, JavaThread::current());\n+}\n+\n+void PlaceholderSynchronizer::enter(Handle obj, BasicLock* lock, JavaThread* locking_thread, JavaThread* current) {\n+  assert(LockingMode == LM_PLACEHOLDER, \"must be\");\n+  VerifyThreadState vts(locking_thread, current);\n+\n+  if (obj->klass()->is_value_based()) {\n+    ObjectSynchronizer::handle_sync_on_value_based_class(obj, locking_thread);\n+  }\n+\n+  locking_thread->inc_held_monitor_count();\n+\n+  if (lock != nullptr) {\n+    \/\/ This is cleared in the interpreter\n+    \/\/ TODO: All paths should have cleared this, assert it is 0\n+    \/\/       instead of clearing it here. Should maybe only be for\n+    \/\/       c++ ObjectLocks and compiler re-lock (check this)\n+    \/\/       Also double check JNI interactions, JNI does not have\n+    \/\/       a slot, so no cache, but is there a problem if JNI first\n+    \/\/       followed by recursive monitor enter exit\n+    lock->clear_displaced_header();\n+  }\n+\n+  SpinYield spin_yield(0, 2);\n+  bool first_time = true;\n+\n+  LockStack& lock_stack = locking_thread->lock_stack();\n+\n+  if (locking_thread != current) {\n+    \/\/ Relock objects from compiler thread\n+    oop o = obj();\n+    \/\/ Would like to fast lock here, but cannot ensure lock order\n+    \/\/ Inflate the relocked lock.\n+    bool entered = inflate_and_enter(o, lock, locking_thread, current, ObjectSynchronizer::inflate_cause_monitor_enter);\n+    assert(entered, \"relock must lock the object, without races\");\n+    return;\n+  }\n+\n+  if (!lock_stack.is_full() && lock_stack.try_recursive_enter(obj())) {\n+    \/\/ TODO: Maybe guard this by the value in the markWord (only is fast locked)\n+    \/\/       Currently this is done when exiting. Doing it early could remove,\n+    \/\/       LockStack::CAPACITY - 1 slow paths in the best case. But need to fix\n+    \/\/       some of the inflation counters for this change.\n+\n+    \/\/ Recursively fast locked\n+    return;\n+  }\n+\n+  if (lock_stack.contains(obj())) {\n+    ObjectMonitor* mon = inflate_fast_locked_object(obj(), locking_thread, current, ObjectSynchronizer::inflate_cause_monitor_enter);\n+    bool entered = mon->enter(locking_thread);\n+    locking_thread->om_set_monitor_cache(mon);\n+    if (lock != nullptr) {\n+      lock->set_displaced_header(mon);\n+    }\n+    assert(entered, \"recursive ObjectMonitor::enter must succeed\");\n+    return;\n+  }\n+\n+  const int spins = OMSpins;\n+  const int yields = OMYields;\n+\n+  while (true) {\n+\n+    SpinYield fast_lock_spin_yield(spins, yields);\n+    \/\/ Fast-locking does not use the 'lock' argument.\n+    markWord mark = obj()->mark_acquire();\n+    const bool try_spin = !first_time || !mark.has_monitor();\n+    for (int attempts = spins + yields; try_spin && attempts > 0; attempts--) {\n+      while (mark.is_unlocked()) {\n+        ensure_lock_stack_space(locking_thread, current);\n+        assert(!lock_stack.is_full(), \"must have made room on the lock stack\");\n+        assert(!lock_stack.contains(obj()), \"thread must not already hold the lock\");\n+        \/\/ Try to swing into 'fast-locked' state.\n+        markWord locked_mark = mark.set_fast_locked();\n+        markWord old_mark = mark;\n+        mark = obj()->cas_set_mark(locked_mark, old_mark);\n+        if (old_mark == mark) {\n+          \/\/ Successfully fast-locked, push object to lock-stack and return.\n+          lock_stack.push(obj());\n+          return;\n+        }\n+      }\n+\n+      fast_lock_spin_yield.wait();\n+      mark = obj()->mark_acquire();\n+    }\n+\n+    if (!first_time) {\n+      spin_yield.wait();\n+    }\n+\n+    if (inflate_and_enter(obj(), lock, locking_thread, current, ObjectSynchronizer::inflate_cause_monitor_enter)) {\n+      return;\n+    }\n+\n+    first_time = false;\n+  }\n+}\n+\n+void PlaceholderSynchronizer::exit(oop object, JavaThread* current) {\n+  assert(LockingMode == LM_PLACEHOLDER, \"must be\");\n+  assert(current == Thread::current(), \"must be\");\n+\n+  bool first_try = true;\n+\n+  markWord mark = object->mark();\n+  assert(!mark.is_unlocked(), \"must be unlocked\");\n+\n+  LockStack& lock_stack = current->lock_stack();\n+  if (mark.is_fast_locked()) {\n+    if (lock_stack.try_recursive_exit(object)) {\n+      \/\/ This is a recursive exit which succeeded\n+      return;\n+    }\n+    if (lock_stack.is_recursive(object)) {\n+      \/\/ Must inflate recursive locks if try_recursive_exit fails\n+      \/\/ This happens for un-structured unlocks, could potentially\n+      \/\/ fix try_recursive_exit to handle these.\n+      inflate_fast_locked_object(object, current, current, ObjectSynchronizer::inflate_cause_vm_internal);\n+    }\n+  }\n+\n+retry:\n+  \/\/ Fast-locking does not use the 'lock' argument.\n+  while (mark.is_fast_locked()) {\n+    markWord unlocked_mark = mark.set_unlocked();\n+    markWord old_mark = mark;\n+    mark = object->cas_set_mark(unlocked_mark, old_mark);\n+    if (old_mark == mark) {\n+      \/\/ CAS successful, remove from lock_stack\n+      size_t recursion = lock_stack.remove(object) - 1;\n+      assert(recursion == 0, \"Should not have unlocked here\");\n+      return;\n+    }\n+  }\n+\n+  assert(mark.has_monitor(), \"must be\");\n+  \/\/ The monitor is\n+  ObjectMonitor* monitor = read_monitor(current, object);\n+  if (monitor->is_owner_anonymous()) {\n+    assert(current->lock_stack().contains(object), \"current must have object on its lock stack\");\n+    monitor->set_owner_from_anonymous(current);\n+    monitor->set_recursions(current->lock_stack().remove(object) - 1);\n+    current->_contended_inflation++;\n+  }\n+\n+  if (OMDeflateBeforeExit && first_try && monitor->recursions() == 0) {\n+    \/\/ Only deflate if recursions are 0 or the lock stack may become\n+    \/\/ imbalanced.\n+    first_try = false;\n+    if (monitor->deflate_anon_monitor(current)) {\n+      mark = object->mark();\n+      current->_exit_deflation++;\n+      goto retry;\n+    }\n+  }\n+\n+  monitor->exit(current);\n+}\n+\n+\/\/ TODO: Rename this. No idea what to call it, used by notify\/notifyall\/wait and jni exit\n+ObjectMonitor* PlaceholderSynchronizer::inflate_locked_or_imse(oop obj, const ObjectSynchronizer::InflateCause cause, TRAPS) {\n+  assert(LockingMode == LM_PLACEHOLDER, \"must be\");\n+  JavaThread* current = THREAD;\n+\n+  for(;;) {\n+    markWord mark = obj->mark_acquire();\n+    if (mark.is_unlocked()) {\n+      \/\/ No lock, IMSE.\n+      THROW_MSG_(vmSymbols::java_lang_IllegalMonitorStateException(),\n+                \"current thread is not owner\", nullptr);\n+    }\n+\n+    if (mark.is_fast_locked()) {\n+      if (!current->lock_stack().contains(obj)) {\n+        \/\/ Fast locked by other thread, IMSE.\n+        THROW_MSG_(vmSymbols::java_lang_IllegalMonitorStateException(),\n+                  \"current thread is not owner\", nullptr);\n+      } else {\n+        \/\/ Current thread owns the lock, must inflate\n+        return inflate_fast_locked_object(obj, current, current, cause);\n+      }\n+    }\n+\n+    assert(mark.has_monitor(), \"must be\");\n+    ObjectMonitor* monitor = read_monitor(current, obj);\n+    if (monitor != nullptr) {\n+      if (monitor->is_owner_anonymous()) {\n+        LockStack& lock_stack = current->lock_stack();\n+        if (lock_stack.contains(obj)) {\n+          \/\/ Current thread owns the lock but someone else inflated\n+          \/\/ fix owner and pop lock stack\n+          monitor->set_owner_from_anonymous(current);\n+          monitor->set_recursions(lock_stack.remove(obj) - 1);\n+          current->_contended_inflation++;\n+        } else {\n+          \/\/ Fast locked (and inflated) by other thread, or deflation in progress, IMSE.\n+          THROW_MSG_(vmSymbols::java_lang_IllegalMonitorStateException(),\n+                    \"current thread is not owner\", nullptr);\n+        }\n+      }\n+      return monitor;\n+    }\n+  }\n+}\n+\n+ObjectMonitor* PlaceholderSynchronizer::inflate_fast_locked_object(oop object, JavaThread* locking_thread, JavaThread* current, const ObjectSynchronizer::InflateCause cause) {\n+  assert(LockingMode == LM_PLACEHOLDER, \"only used for lightweight\");\n+  VerifyThreadState vts(locking_thread, current);\n+  assert(locking_thread->lock_stack().contains(object), \"locking_thread must have object on its lock stack\");\n+\n+  \/\/ Inflating requires a hash code\n+  FastHashCode(current, object);\n+\n+  markWord mark = object->mark_acquire();\n+  assert(!mark.is_unlocked(), \"Cannot be unlocked\");\n+\n+  ObjectMonitor* monitor;\n+\n+  for (;;) {\n+  \/\/ Fetch the monitor from the table\n+    monitor = get_or_insert_monitor(object, current, cause, true \/* try_read *\/);\n+\n+    if (monitor->is_owner_anonymous()) {\n+      assert(monitor == read_monitor(current, object), \"The monitor must be in the table\");\n+      \/\/ New fresh monitor\n+      break;\n+    }\n+\n+    os::naked_yield();\n+    assert(monitor->is_being_async_deflated(), \"Should be the reason\");\n+  }\n+\n+  \/\/ Set the mark word; loop to handle concurrent updates to other parts of the mark word\n+  while (mark.is_fast_locked()) {\n+    mark = object->cas_set_mark(mark.set_has_monitor(), mark);\n+  }\n+\n+  \/\/ Indicate that the monitor now has a known owner\n+  monitor->set_owner_from_anonymous(locking_thread);\n+\n+  \/\/ Remove the entry from the thread's lock stack\n+  monitor->set_recursions(locking_thread->lock_stack().remove(object) - 1);\n+\n+  locking_thread->om_set_monitor_cache(monitor);\n+\n+  if (cause == ObjectSynchronizer::inflate_cause_wait) {\n+    locking_thread->lock_stack().set_wait_was_inflated();\n+    locking_thread->_wait_inflation++;\n+  } else if (cause == ObjectSynchronizer::inflate_cause_monitor_enter) {\n+    locking_thread->_recursive_inflation++;\n+  } else if (cause == ObjectSynchronizer::inflate_cause_vm_internal) {\n+    locking_thread->_lock_stack_inflation++;\n+  }\n+\n+  return monitor;\n+}\n+\n+bool PlaceholderSynchronizer::inflate_and_enter(oop object, BasicLock* lock, JavaThread* locking_thread, JavaThread* current, const ObjectSynchronizer::InflateCause cause) {\n+  assert(LockingMode == LM_PLACEHOLDER, \"only used for lightweight\");\n+  VerifyThreadState vts(locking_thread, current);\n+  NoSafepointVerifier nsv;\n+\n+  \/\/ Note: In some paths (deoptimization) the 'current' thread inflates and\n+  \/\/ enters the lock on behalf of the 'locking_thread' thread.\n+\n+  \/\/ Placeholder monitors require that hash codes are installed first\n+  FastHashCode(locking_thread, object);\n+\n+  ObjectMonitor* monitor = nullptr;\n+\n+  \/\/ Try to get the monitor from the thread-local cache.\n+  \/\/ There's no need to use the cache if we are locking\n+  \/\/ on behalf of another thread.\n+  if (current == locking_thread) {\n+    monitor = current->om_get_from_monitor_cache(object);\n+  }\n+\n+  \/\/ Get or create the monitor\n+  if (monitor == nullptr) {\n+    monitor = get_or_insert_monitor(object, current, cause, true \/* try_read *\/);\n+  }\n+\n+  if (monitor->try_enter(locking_thread)) {\n+    current->om_set_monitor_cache(monitor);\n+    if (lock != nullptr) {\n+      lock->set_displaced_header(monitor);\n+    }\n+    return true;\n+  }\n+\n+  \/\/ Holds is_being_async_deflated() stable throughout this function.\n+  ObjectMonitorContentionMark mark(monitor);\n+\n+  \/\/\/ First handle the case where the monitor from the table is deflated\n+  if (monitor->is_being_async_deflated()) {\n+    \/\/ The MonitorDeflation thread is deflating the monitor. The locking thread\n+    \/\/ can either help transition the mark word or yield \/ spin until further\n+    \/\/ progress have been made.\n+\n+    const markWord mark = object->mark_acquire();\n+\n+    if (mark.has_monitor()) {\n+      if (monitor->owner_is_DEFLATER_MARKER()) {\n+        \/\/ Only help the monitor deflation thread transition to unlocked.\n+        \/\/ If owner is anonymous then a java thread deflated, and only they\n+        \/\/ may transition the mark word directly to fast_locked\n+\n+        \/\/ Let this thread help update the mark word to unlocked.\n+        const markWord new_mark = mark.clear_lock_bits().set_unlocked();\n+        (void)object->cas_set_mark(new_mark, mark);\n+        \/\/ Retry immediately\n+      }\n+\n+    } else if (mark.is_fast_locked()) {\n+      \/\/ Some other thread managed to fast-lock the lock, or this is a\n+      \/\/ recursive lock from the same thread; yield for the deflation\n+      \/\/ thread to remove the deflated monitor from the table.\n+      os::naked_yield();\n+\n+    } else {\n+      assert(mark.is_unlocked(), \"Implied\");\n+      \/\/ Retry immediately\n+    }\n+\n+    \/\/ Retry\n+    return false;\n+  }\n+\n+  for (;;) {\n+    const markWord mark = object->mark_acquire();\n+    \/\/ The mark can be in one of the following states:\n+    \/\/ *  inflated     - If the ObjectMonitor owner is anonymous\n+    \/\/                   and the locking_thread thread owns the object\n+    \/\/                   lock, then we make the locking_thread thread\n+    \/\/                   the ObjectMonitor owner and remove the\n+    \/\/                   lock from the locking_thread thread's lock stack.\n+    \/\/ *  fast-locked  - Coerce it to inflated from fast-locked.\n+    \/\/ *  neutral      - Inflate the object. Successful CAS is locked\n+\n+    \/\/ CASE: inflated\n+    if (mark.has_monitor()) {\n+      LockStack& lock_stack = locking_thread->lock_stack();\n+      if (monitor->is_owner_anonymous() && lock_stack.contains(object)) {\n+        \/\/ The lock is fast-locked by the locking thread,\n+        \/\/ convert it to a held monitor with a known owner.\n+        monitor->set_owner_from_anonymous(locking_thread);\n+        monitor->set_recursions(lock_stack.remove(object) - 1);\n+        locking_thread->_contended_recursive_inflation++;\n+      }\n+\n+      break; \/\/ Success\n+    }\n+\n+    \/\/ CASE: fast-locked\n+    \/\/ Could be fast-locked either by locking_thread or by some other thread.\n+    \/\/\n+    if (mark.is_fast_locked()) {\n+      markWord old_mark = object->cas_set_mark(mark.set_has_monitor(), mark);\n+      if (old_mark != mark) {\n+        \/\/ CAS failed\n+        continue;\n+      }\n+\n+      \/\/ Success! Return inflated monitor.\n+      LockStack& lock_stack = locking_thread->lock_stack();\n+      if (lock_stack.contains(object)) {\n+        \/\/ The lock is fast-locked by the locking thread,\n+        \/\/ convert it to a held monitor with a known owner.\n+        monitor->set_owner_from_anonymous(locking_thread);\n+        monitor->set_recursions(lock_stack.remove(object) - 1);\n+        locking_thread->_recursive_inflation++;\n+      }\n+\n+      break; \/\/ Success\n+    }\n+\n+    \/\/ CASE: neutral (unlocked)\n+\n+    \/\/ Catch if the object's header is not neutral (not locked and\n+    \/\/ not marked is what we care about here).\n+    assert(mark.is_neutral(), \"invariant: header=\" INTPTR_FORMAT, mark.value());\n+    markWord old_mark = object->cas_set_mark(mark.set_has_monitor(), mark);\n+    if (old_mark != mark) {\n+      \/\/ CAS failed\n+      continue;\n+    }\n+\n+    \/\/ Transitioned from unlocked to monitor means locking_thread owns the lock.\n+    monitor->set_owner_from_anonymous(locking_thread);\n+\n+    \/\/ Update the thread-local cache\n+    if (current == locking_thread) {\n+      current->om_set_monitor_cache(monitor);\n+      current->_unlocked_inflation++;\n+    }\n+\n+    return true;\n+  }\n+\n+  if (current == locking_thread && monitor->has_owner() && monitor->owner_raw() != locking_thread) {\n+    \/\/ Someone else owns the lock, take the time befor entering to fix the lock stack\n+    LockStackInflateContendedLocks().inflate(locking_thread, current);\n+  }\n+\n+  \/\/ enter can block for safepoints; clear the unhandled object oop\n+  PauseNoSafepointVerifier pnsv(&nsv);\n+  object = nullptr;\n+\n+  if (monitor->enter(locking_thread)) {\n+    \/\/ Update the thread-local cache\n+    if (current == locking_thread) {\n+      current->om_set_monitor_cache(monitor);\n+    }\n+    if (lock != nullptr) {\n+      lock->set_displaced_header(monitor);\n+    }\n+\n+    return true;\n+  }\n+\n+  return false;\n+}\n+\n+void PlaceholderSynchronizer::deflate_monitor(Thread* current, oop obj, ObjectMonitor* monitor) {\n+  if (obj != nullptr) {\n+    deflate_mark_word(obj);\n+  }\n+  bool removed = remove_monitor(current, obj, monitor);\n+  if (obj != nullptr) {\n+    assert(removed, \"Should have removed the entry if obj was alive\");\n+  }\n+}\n+\n+void PlaceholderSynchronizer::deflate_anon_monitor(Thread* current, oop obj, ObjectMonitor* monitor) {\n+  markWord mark = obj->mark_acquire();\n+  assert(!mark.has_no_hash(), \"obj with inflated monitor must have had a hash\");\n+\n+  while (mark.has_monitor()) {\n+    const markWord new_mark = mark.set_fast_locked();\n+    mark = obj->cas_set_mark(new_mark, mark);\n+  }\n+\n+  bool removed = remove_monitor(current, obj, monitor);\n+  assert(removed, \"Should have removed the entry\");\n+}\n+\n+ObjectMonitor* PlaceholderSynchronizer::read_monitor(Thread* current, oop obj) {\n+  assert(LockingMode == LM_PLACEHOLDER, \"must be\");\n+  return _omworld->monitor_get(current, obj);\n+}\n+\n+bool PlaceholderSynchronizer::contains_monitor(Thread* current, ObjectMonitor* monitor) {\n+  assert(LockingMode == LM_PLACEHOLDER, \"must be\");\n+  return _omworld->contains_monitor(current, monitor);\n+}\n+\n+intptr_t PlaceholderSynchronizer::FastHashCode(Thread* current, oop obj) {\n+  assert(LockingMode == LM_PLACEHOLDER, \"must be\");\n+\n+  markWord mark = obj->mark_acquire();\n+  for(;;) {\n+    intptr_t hash = mark.hash();\n+    if (hash != 0) {\n+      return hash;\n+    }\n+\n+    hash = ObjectSynchronizer::get_next_hash(current, obj);\n+    const markWord old_mark = mark;\n+    const markWord new_mark = old_mark.copy_set_hash(hash);\n+\n+    mark = obj->cas_set_mark(new_mark, old_mark);\n+    if (old_mark == mark) {\n+      return hash;\n+    }\n+  }\n+}\n","filename":"src\/hotspot\/share\/runtime\/placeholderSynchronizer.cpp","additions":1004,"deletions":0,"binary":false,"changes":1004,"status":"added"},{"patch":"@@ -0,0 +1,75 @@\n+\/*\n+ * Copyright (c) 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_RUNTIME_PLACEHOLDERSYNCHRONIZER_HPP\n+#define SHARE_RUNTIME_PLACEHOLDERSYNCHRONIZER_HPP\n+\n+#include \"memory\/allStatic.hpp\"\n+#include \"runtime\/objectMonitor.hpp\"\n+#include \"runtime\/synchronizer.hpp\"\n+#include \"runtime\/javaThread.hpp\"\n+\n+class ObjectMonitorWorld;\n+\n+class PlaceholderSynchronizer : AllStatic {\n+private:\n+  static ObjectMonitorWorld* _omworld;\n+\n+  static ObjectMonitor* get_or_insert_monitor_from_table(oop object, JavaThread* current, bool try_read, bool* inserted);\n+  static ObjectMonitor* get_or_insert_monitor(oop object, JavaThread* current, const ObjectSynchronizer::InflateCause cause, bool try_read);\n+\n+  static ObjectMonitor* add_monitor(JavaThread* current, ObjectMonitor* monitor, oop obj);\n+  static bool remove_monitor(Thread* current, oop obj, ObjectMonitor* monitor);\n+\n+  static void deflate_mark_word(oop object);\n+\n+  static void ensure_lock_stack_space(JavaThread* locking_thread, JavaThread* current);\n+\n+ public:\n+  static void initialize();\n+\n+  static bool needs_resize(JavaThread* current);\n+  static bool resize_table(JavaThread* current);\n+  static void set_table_max(JavaThread* current);\n+\n+  static void enter_for(Handle obj, BasicLock* lock, JavaThread* locking_thread);\n+  static void enter(Handle obj, BasicLock* lock,  JavaThread* locking_thread, JavaThread* current);\n+  static void exit(oop object, JavaThread* current);\n+\n+  static ObjectMonitor* inflate_locked_or_imse(oop object, const ObjectSynchronizer::InflateCause cause, TRAPS);\n+  static ObjectMonitor* inflate_fast_locked_object(oop object, JavaThread* locking_thread, JavaThread* current, const ObjectSynchronizer::InflateCause cause);\n+  static bool inflate_and_enter(oop object, BasicLock* lock, JavaThread* locking_thread, JavaThread* current, const ObjectSynchronizer::InflateCause cause);\n+\n+  static void deflate_monitor(Thread* current, oop obj, ObjectMonitor* monitor);\n+  static void deflate_anon_monitor(Thread* current, oop obj, ObjectMonitor* monitor);\n+\n+  static ObjectMonitor* read_monitor(Thread* current, oop obj);\n+\n+  static bool contains_monitor(Thread* current, ObjectMonitor* monitor);\n+\n+  \/\/ NOTE: May not cause monitor inflation\n+  static intptr_t FastHashCode(Thread* current, oop obj);\n+};\n+\n+#endif \/\/ SHARE_RUNTIME_PLACEHOLDERSYNCHRONIZER_HPP\n","filename":"src\/hotspot\/share\/runtime\/placeholderSynchronizer.hpp","additions":75,"deletions":0,"binary":false,"changes":75,"status":"added"},{"patch":"@@ -879,0 +879,5 @@\n+\n+  \/\/ The oops in the monitor cache is cleared to prevent stale cache entries\n+  \/\/ from keeping dead objects alive. Because these oops are always cleared\n+  \/\/ before safepoint operations they are not visited in JavaThread::oops_do.\n+  _thread->om_clear_monitor_cache();\n","filename":"src\/hotspot\/share\/runtime\/safepoint.cpp","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -60,0 +60,1 @@\n+#include \"runtime\/globals.hpp\"\n@@ -76,0 +77,1 @@\n+#include \"utilities\/globalDefinitions.hpp\"\n@@ -2956,0 +2958,2 @@\n+      } else if (LockingMode == LM_PLACEHOLDER) {\n+        buf[i] = (intptr_t)lock->displaced_header().value();\n","filename":"src\/hotspot\/share\/runtime\/sharedRuntime.cpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -51,0 +51,1 @@\n+#include \"runtime\/placeholderSynchronizer.hpp\"\n@@ -70,0 +71,12 @@\n+ObjectMonitor* ObjectSynchronizer::read_monitor(markWord mark) {\n+  assert(LockingMode != LM_PLACEHOLDER, \"placeholder locking uses table\");\n+  return mark.monitor();\n+}\n+\n+ObjectMonitor* ObjectSynchronizer::read_monitor(Thread* current, oop obj, markWord mark) {\n+  if (LockingMode != LM_PLACEHOLDER) {\n+    return read_monitor(mark);\n+  }\n+  return PlaceholderSynchronizer::read_monitor(current, obj);\n+}\n+\n@@ -279,0 +292,4 @@\n+\n+  if (LockingMode == LM_PLACEHOLDER) {\n+    PlaceholderSynchronizer::initialize();\n+  }\n@@ -337,1 +354,1 @@\n-  if (LockingMode == LM_LIGHTWEIGHT) {\n+  if (LockingMode == LM_LIGHTWEIGHT || LockingMode == LM_PLACEHOLDER) {\n@@ -352,1 +369,5 @@\n-    ObjectMonitor* const mon = mark.monitor();\n+    ObjectMonitor* const mon = read_monitor(current, obj, mark);\n+    if (LockingMode == LM_PLACEHOLDER && mon == nullptr) {\n+      \/\/ Racing with inflation\/deflation go slow path\n+      return false;\n+    }\n@@ -388,0 +409,4 @@\n+  if (LockingMode == LM_PLACEHOLDER) {\n+    \/\/ Not using quick_enter for now.\n+    return false;\n+  }\n@@ -527,0 +552,5 @@\n+\n+  if (LockingMode == LM_PLACEHOLDER) {\n+    return PlaceholderSynchronizer::enter_for(obj, lock, locking_thread);\n+  }\n+\n@@ -545,0 +575,5 @@\n+\n+  if (LockingMode == LM_PLACEHOLDER) {\n+    return PlaceholderSynchronizer::enter(obj, lock, current, current);\n+  }\n+\n@@ -662,0 +697,4 @@\n+  if (LockingMode == LM_PLACEHOLDER) {\n+    return PlaceholderSynchronizer::exit(object, current);\n+  }\n+\n@@ -711,1 +750,1 @@\n-            ObjectMonitor* m = mark.monitor();\n+            ObjectMonitor* m = read_monitor(mark);\n@@ -755,2 +794,10 @@\n-    ObjectMonitor* monitor = inflate(current, obj(), inflate_cause_jni_enter);\n-    if (monitor->enter(current)) {\n+    ObjectMonitor* monitor;\n+    bool entered;\n+    if (LockingMode == LM_PLACEHOLDER) {\n+      entered = PlaceholderSynchronizer::inflate_and_enter(obj(), nullptr, current, current, inflate_cause_jni_enter);\n+    } else {\n+      monitor = inflate(current, obj(), inflate_cause_jni_enter);\n+      entered = monitor->enter(current);\n+    }\n+\n+    if (entered) {\n@@ -768,3 +815,8 @@\n-  \/\/ The ObjectMonitor* can't be async deflated until ownership is\n-  \/\/ dropped inside exit() and the ObjectMonitor* must be !is_busy().\n-  ObjectMonitor* monitor = inflate(current, obj, inflate_cause_jni_exit);\n+  ObjectMonitor* monitor;\n+  if (LockingMode == LM_PLACEHOLDER) {\n+    monitor = PlaceholderSynchronizer::inflate_locked_or_imse(obj, inflate_cause_jni_exit, CHECK);\n+  } else {\n+    \/\/ The ObjectMonitor* can't be async deflated until ownership is\n+    \/\/ dropped inside exit() and the ObjectMonitor* must be !is_busy().\n+    monitor = inflate(current, obj, inflate_cause_jni_exit);\n+  }\n@@ -803,0 +855,1 @@\n+\n@@ -808,4 +861,11 @@\n-  \/\/ The ObjectMonitor* can't be async deflated because the _waiters\n-  \/\/ field is incremented before ownership is dropped and decremented\n-  \/\/ after ownership is regained.\n-  ObjectMonitor* monitor = inflate(current, obj(), inflate_cause_wait);\n+\n+  ObjectMonitor* monitor;\n+  if (LockingMode == LM_PLACEHOLDER) {\n+    current->lock_stack().clear_wait_was_inflated();\n+    monitor = PlaceholderSynchronizer::inflate_locked_or_imse(obj(), inflate_cause_wait, CHECK_0);\n+  } else {\n+    \/\/ The ObjectMonitor* can't be async deflated because the _waiters\n+    \/\/ field is incremented before ownership is dropped and decremented\n+    \/\/ after ownership is regained.\n+    monitor = inflate(current, obj(), inflate_cause_wait);\n+  }\n@@ -828,1 +888,1 @@\n-  if (LockingMode == LM_LIGHTWEIGHT) {\n+  if (LockingMode == LM_LIGHTWEIGHT || LockingMode == LM_PLACEHOLDER) {\n@@ -839,3 +899,9 @@\n-  \/\/ The ObjectMonitor* can't be async deflated until ownership is\n-  \/\/ dropped by the calling thread.\n-  ObjectMonitor* monitor = inflate(current, obj(), inflate_cause_notify);\n+\n+  ObjectMonitor* monitor;\n+  if (LockingMode == LM_PLACEHOLDER) {\n+    monitor = PlaceholderSynchronizer::inflate_locked_or_imse(obj(), inflate_cause_notify, CHECK);\n+  } else {\n+    \/\/ The ObjectMonitor* can't be async deflated until ownership is\n+    \/\/ dropped by the calling thread.\n+    monitor = inflate(current, obj(), inflate_cause_notify);\n+  }\n@@ -850,1 +916,1 @@\n-  if (LockingMode == LM_LIGHTWEIGHT) {\n+  if (LockingMode == LM_LIGHTWEIGHT || LockingMode == LM_PLACEHOLDER) {\n@@ -861,3 +927,9 @@\n-  \/\/ The ObjectMonitor* can't be async deflated until ownership is\n-  \/\/ dropped by the calling thread.\n-  ObjectMonitor* monitor = inflate(current, obj(), inflate_cause_notify);\n+\n+  ObjectMonitor* monitor;\n+  if (LockingMode == LM_PLACEHOLDER) {\n+    monitor = PlaceholderSynchronizer::inflate_locked_or_imse(obj(), inflate_cause_notify, CHECK);\n+  } else {\n+    \/\/ The ObjectMonitor* can't be async deflated until ownership is\n+    \/\/ dropped by the calling thread.\n+    monitor = inflate(current, obj(), inflate_cause_notify);\n+  }\n@@ -885,1 +957,1 @@\n-  if (!mark.is_being_inflated() || LockingMode == LM_LIGHTWEIGHT) {\n+  if (!mark.is_being_inflated() || LockingMode == LM_LIGHTWEIGHT || LockingMode == LM_PLACEHOLDER) {\n@@ -995,1 +1067,1 @@\n-  value &= markWord::hash_mask;\n+  value &= UseCompactObjectHeaders ? markWord::hash_mask_compact : markWord::hash_mask;\n@@ -1001,0 +1073,5 @@\n+intptr_t ObjectSynchronizer::get_next_hash(Thread* current, oop obj) {\n+  \/\/ CLEANUP[Axel]: hack for PlaceholderSynchronizer being in different translation unit\n+  return ::get_next_hash(current, obj);\n+}\n+\n@@ -1002,0 +1079,3 @@\n+  if (LockingMode == LM_PLACEHOLDER) {\n+    return PlaceholderSynchronizer::FastHashCode(current, obj);\n+  }\n@@ -1129,1 +1209,1 @@\n-  if (LockingMode == LM_LIGHTWEIGHT && mark.is_fast_locked()) {\n+  if ((LockingMode == LM_LIGHTWEIGHT || LockingMode == LM_PLACEHOLDER) && mark.is_fast_locked()) {\n@@ -1134,1 +1214,15 @@\n-  if (mark.has_monitor()) {\n+  while (LockingMode == LM_PLACEHOLDER && mark.has_monitor()) {\n+    ObjectMonitor* monitor = PlaceholderSynchronizer::read_monitor(current, obj);\n+    if (monitor != nullptr) {\n+      return monitor->is_entered(current) != 0;\n+    }\n+    \/\/ Racing with inflation\/deflation, retry\n+    mark = obj->mark_acquire();\n+\n+    if (mark.is_fast_locked()) {\n+      \/\/ Some other thread fast_locked, current could not have held the lock\n+      return false;\n+    }\n+  }\n+\n+  if (LockingMode != LM_PLACEHOLDER && mark.has_monitor()) {\n@@ -1138,1 +1232,1 @@\n-    ObjectMonitor* monitor = mark.monitor();\n+    ObjectMonitor* monitor = read_monitor(mark);\n@@ -1156,1 +1250,1 @@\n-  if (LockingMode == LM_LIGHTWEIGHT && mark.is_fast_locked()) {\n+  if ((LockingMode == LM_LIGHTWEIGHT || LockingMode == LM_PLACEHOLDER) && mark.is_fast_locked()) {\n@@ -1162,1 +1256,15 @@\n-  if (mark.has_monitor()) {\n+  while (LockingMode == LM_PLACEHOLDER && mark.has_monitor()) {\n+    ObjectMonitor* monitor = PlaceholderSynchronizer::read_monitor(Thread::current(), obj);\n+    if (monitor != nullptr) {\n+      return Threads::owning_thread_from_monitor(t_list, monitor);\n+    }\n+    \/\/ Racing with inflation\/deflation, retry\n+    mark = obj->mark_acquire();\n+\n+    if (mark.is_fast_locked()) {\n+      \/\/ Some other thread fast_locked\n+      return Threads::owning_thread_from_object(t_list, h_obj());\n+    }\n+  }\n+\n+  if (LockingMode != LM_PLACEHOLDER && mark.has_monitor()) {\n@@ -1166,1 +1274,1 @@\n-    ObjectMonitor* monitor = mark.monitor();\n+    ObjectMonitor* monitor = read_monitor(mark);\n@@ -1242,1 +1350,1 @@\n-    size_t new_ceiling = ceiling + (size_t)((double)ceiling * remainder) + 1;\n+    size_t new_ceiling = ceiling \/ remainder + 1;\n@@ -1378,0 +1486,3 @@\n+  if (LockingMode == LM_PLACEHOLDER) {\n+    return;\n+  }\n@@ -1380,1 +1491,1 @@\n-    ObjectMonitor* monitor = mark.monitor();\n+    ObjectMonitor* monitor = read_monitor(mark);\n@@ -1402,0 +1513,1 @@\n+  assert(LockingMode != LM_PLACEHOLDER, \"placeholder does not use inflate\");\n@@ -1655,0 +1767,1 @@\n+  Thread* current = Thread::current();\n@@ -1661,1 +1774,1 @@\n-    if (mid->deflate_monitor()) {\n+    if (mid->deflate_monitor(current)) {\n@@ -1679,0 +1792,5 @@\n+    if (thread->is_Java_thread()) {\n+      \/\/ Clear OM cache\n+      JavaThread* jt = JavaThread::cast(thread);\n+      jt->om_clear_monitor_cache();\n+    }\n@@ -1825,0 +1943,8 @@\n+#ifdef ASSERT\n+    if (LockingMode == LM_PLACEHOLDER) {\n+      for (ObjectMonitor* monitor : delete_list) {\n+        assert(!PlaceholderSynchronizer::contains_monitor(current, monitor), \"Should have been removed\");\n+      }\n+    }\n+#endif\n+\n@@ -2051,1 +2177,2 @@\n-  if (n->header().value() == 0) {\n+\n+  if (n->header_value() == 0) {\n@@ -2056,0 +2183,1 @@\n+\n@@ -2057,17 +2185,21 @@\n-  if (obj != nullptr) {\n-    const markWord mark = obj->mark();\n-    if (!mark.has_monitor()) {\n-      out->print_cr(\"ERROR: monitor=\" INTPTR_FORMAT \": in-use monitor's \"\n-                    \"object does not think it has a monitor: obj=\"\n-                    INTPTR_FORMAT \", mark=\" INTPTR_FORMAT, p2i(n),\n-                    p2i(obj), mark.value());\n-      *error_cnt_p = *error_cnt_p + 1;\n-    }\n-    ObjectMonitor* const obj_mon = mark.monitor();\n-    if (n != obj_mon) {\n-      out->print_cr(\"ERROR: monitor=\" INTPTR_FORMAT \": in-use monitor's \"\n-                    \"object does not refer to the same monitor: obj=\"\n-                    INTPTR_FORMAT \", mark=\" INTPTR_FORMAT \", obj_mon=\"\n-                    INTPTR_FORMAT, p2i(n), p2i(obj), mark.value(), p2i(obj_mon));\n-      *error_cnt_p = *error_cnt_p + 1;\n-    }\n+  if (obj == nullptr) {\n+    return;\n+  }\n+\n+  const markWord mark = obj->mark();\n+  if (!mark.has_monitor()) {\n+    out->print_cr(\"ERROR: monitor=\" INTPTR_FORMAT \": in-use monitor's \"\n+                  \"object does not think it has a monitor: obj=\"\n+                  INTPTR_FORMAT \", mark=\" INTPTR_FORMAT, p2i(n),\n+                  p2i(obj), mark.value());\n+    *error_cnt_p = *error_cnt_p + 1;\n+    return;\n+  }\n+\n+  ObjectMonitor* const obj_mon = read_monitor(Thread::current(), obj, mark);\n+  if (n != obj_mon) {\n+    out->print_cr(\"ERROR: monitor=\" INTPTR_FORMAT \": in-use monitor's \"\n+                  \"object does not refer to the same monitor: obj=\"\n+                  INTPTR_FORMAT \", mark=\" INTPTR_FORMAT \", obj_mon=\"\n+                  INTPTR_FORMAT, p2i(n), p2i(obj), mark.value(), p2i(obj_mon));\n+    *error_cnt_p = *error_cnt_p + 1;\n@@ -2096,1 +2228,1 @@\n-        const markWord mark = monitor->header();\n+        const intptr_t hash = LockingMode == LM_PLACEHOLDER ? monitor->hash_placeholder() : monitor->header().hash();\n@@ -2099,1 +2231,1 @@\n-                   monitor->is_busy(), mark.hash() != 0, monitor->owner() != nullptr,\n+                   monitor->is_busy(), hash != 0, monitor->owner() != nullptr,\n","filename":"src\/hotspot\/share\/runtime\/synchronizer.cpp","additions":183,"deletions":51,"binary":false,"changes":234,"status":"modified"},{"patch":"@@ -32,0 +32,1 @@\n+#include \"runtime\/javaThread.hpp\"\n@@ -136,0 +137,3 @@\n+  static ObjectMonitor* read_monitor(markWord mark);\n+  static ObjectMonitor* read_monitor(Thread* current, oop obj, markWord mark);\n+\n@@ -197,0 +201,3 @@\n+  friend class PlaceholderSynchronizer;\n+\n+  static intptr_t get_next_hash(Thread* current, oop obj);\n","filename":"src\/hotspot\/share\/runtime\/synchronizer.hpp","additions":7,"deletions":0,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -534,1 +534,1 @@\n-  assert(LockingMode != LM_LIGHTWEIGHT, \"should not be called with new lightweight locking\");\n+  assert(LockingMode != LM_LIGHTWEIGHT && LockingMode != LM_PLACEHOLDER, \"should not be called with new lightweight locking\");\n","filename":"src\/hotspot\/share\/runtime\/thread.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -117,0 +117,3 @@\n+#if INCLUDE_VM_STRUCTS\n+#include \"runtime\/vmStructs.hpp\"\n+#endif\n@@ -503,0 +506,7 @@\n+  \/\/ Should happen before any agent attaches and pokes into vmStructs\n+#if INCLUDE_VM_STRUCTS\n+  if (UseCompactObjectHeaders) {\n+    VMStructs::compact_headers_overrides();\n+  }\n+#endif\n+\n@@ -1209,1 +1219,1 @@\n-  assert(LockingMode != LM_LIGHTWEIGHT, \"Not with new lightweight locking\");\n+  assert(LockingMode != LM_LIGHTWEIGHT && LockingMode != LM_PLACEHOLDER, \"Not with new lightweight locking\");\n@@ -1240,1 +1250,1 @@\n-  assert(LockingMode == LM_LIGHTWEIGHT, \"Only with new lightweight locking\");\n+  assert(LockingMode == LM_LIGHTWEIGHT || LockingMode == LM_PLACEHOLDER, \"Only with new lightweight locking\");\n@@ -1256,1 +1266,1 @@\n-  if (LockingMode == LM_LIGHTWEIGHT) {\n+  if (LockingMode == LM_LIGHTWEIGHT || LockingMode == LM_PLACEHOLDER) {\n","filename":"src\/hotspot\/share\/runtime\/threads.cpp","additions":13,"deletions":3,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -249,3 +249,5 @@\n-          if (mark.has_monitor() &&\n-              ( \/\/ we have marked ourself as pending on this monitor\n-                mark.monitor() == thread()->current_pending_monitor() ||\n+          if (mark.has_monitor()) {\n+            \/\/ TODO: Fix\n+            ObjectMonitor* mon = ObjectSynchronizer::read_monitor(current, monitor->owner(), mark);\n+            if ( \/\/ we have marked ourself as pending on this monitor\n+                mon == thread()->current_pending_monitor() ||\n@@ -253,3 +255,3 @@\n-                !mark.monitor()->is_entered(thread())\n-              )) {\n-            lock_state = \"waiting to lock\";\n+                !mon->is_entered(thread())) {\n+              lock_state = \"waiting to lock\";\n+            }\n","filename":"src\/hotspot\/share\/runtime\/vframe.cpp","additions":8,"deletions":6,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -150,0 +150,10 @@\n+\/\/ Used by VMStructs when CompactObjectHeaders are enabled.\n+\/\/ Must match the relevant parts from the real oopDesc.\n+class fakeOopDesc {\n+private:\n+  union _metadata {\n+    Klass*      _klass;\n+    narrowKlass _compressed_klass;\n+  } _metadata;\n+};\n+\n@@ -1159,0 +1169,2 @@\n+  declare_toplevel_type(fakeOopDesc)                                      \\\n+                                                                          \\\n@@ -2351,0 +2363,1 @@\n+  declare_constant(LM_PLACEHOLDER)                                        \\\n@@ -2512,0 +2525,1 @@\n+  declare_constant(markWord::hash_bits_compact)                           \\\n@@ -2516,0 +2530,2 @@\n+  declare_constant(markWord::hash_shift_compact)                          \\\n+  LP64_ONLY(declare_constant(markWord::klass_shift))                      \\\n@@ -2523,0 +2539,2 @@\n+  declare_constant(markWord::hash_mask_compact)                           \\\n+  declare_constant(markWord::hash_mask_compact_in_place)                  \\\n@@ -3059,0 +3077,26 @@\n+\n+void VMStructs::compact_headers_overrides() {\n+  assert(UseCompactObjectHeaders, \"Should have been checked before\");\n+\n+  \/\/ We cannot allow SA and other facilities to poke into VM internal fields\n+  \/\/ expecting the class pointers there. This will crash in the best case,\n+  \/\/ or yield incorrect execution in the worst case. This code hides the\n+  \/\/ risky fields from external code by replacing their original container\n+  \/\/ type to a fake one. The fake type should exist for VMStructs verification\n+  \/\/ code to work.\n+\n+  size_t len = localHotSpotVMStructsLength();\n+  for (size_t off = 0; off < len; off++) {\n+    VMStructEntry* e = &localHotSpotVMStructs[off];\n+    if (e == nullptr) continue;\n+    if (e->typeName == nullptr) continue;\n+    if (e->fieldName == nullptr) continue;\n+\n+    if (strcmp(e->typeName, \"oopDesc\") == 0) {\n+      if ((strcmp(e->fieldName, \"_metadata._klass\") == 0) ||\n+          (strcmp(e->fieldName, \"_metadata._compressed_klass\") == 0)) {\n+        e->typeName = \"fakeOopDesc\";\n+      }\n+    }\n+  }\n+}\n","filename":"src\/hotspot\/share\/runtime\/vmStructs.cpp","additions":44,"deletions":0,"binary":false,"changes":44,"status":"modified"},{"patch":"@@ -149,0 +149,3 @@\n+\n+public:\n+  static void compact_headers_overrides() NOT_VM_STRUCTS_RETURN;\n","filename":"src\/hotspot\/share\/runtime\/vmStructs.hpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -0,0 +1,97 @@\n+\/*\n+ * Copyright (c) 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_UTILITIES_FASTHASH_HPP\n+#define SHARE_UTILITIES_FASTHASH_HPP\n+\n+#include \"memory\/allStatic.hpp\"\n+\n+class FastHash : public AllStatic {\n+private:\n+  static void fullmul64(uint64_t& hi, uint64_t& lo, uint64_t op1, uint64_t op2) {\n+#if defined(__SIZEOF_INT128__)\n+    __uint128_t prod = static_cast<__uint128_t>(op1) * static_cast<__uint128_t>(op2);\n+    hi = static_cast<uint64_t>(prod >> 64);\n+    lo = static_cast<uint64_t>(prod >>  0);\n+#else\n+    \/* First calculate all of the cross products. *\/\n+    uint64_t lo_lo = (op1 & 0xFFFFFFFF) * (op2 & 0xFFFFFFFF);\n+    uint64_t hi_lo = (op1 >> 32)        * (op2 & 0xFFFFFFFF);\n+    uint64_t lo_hi = (op1 & 0xFFFFFFFF) * (op2 >> 32);\n+    uint64_t hi_hi = (op1 >> 32)        * (op2 >> 32);\n+\n+    \/* Now add the products together. These will never overflow. *\/\n+    uint64_t cross = (lo_lo >> 32) + (hi_lo & 0xFFFFFFFF) + lo_hi;\n+    uint64_t upper = (hi_lo >> 32) + (cross >> 32)        + hi_hi;\n+    hi = upper;\n+    lo = (cross << 32) | (lo_lo & 0xFFFFFFFF);\n+#endif\n+  }\n+\n+  static void fullmul32(uint32_t& hi, uint32_t& lo, uint32_t op1, uint32_t op2) {\n+    uint64_t x64 = op1, y64 = op2, xy64 = x64 * y64;\n+    hi = (uint32_t)(xy64 >> 32);\n+    lo = (uint32_t)(xy64 >>  0);\n+  }\n+\n+  static uint64_t ror(uint64_t x, uint64_t distance) {\n+    distance = distance & 0x3F;\n+    return (x >> distance) | (x << (64 - distance));\n+  }\n+\n+public:\n+  static uint64_t get_hash64(uint64_t x, uint64_t y) {\n+    const uint64_t M  = 0x8ADAE89C337954D5;\n+    const uint64_t A  = 0xAAAAAAAAAAAAAAAA; \/\/ REPAA\n+    const uint64_t H0 = (x ^ y), L0 = (x ^ A);\n+\n+    uint64_t U0, V0; fullmul64(U0, V0, L0, M);\n+    const uint64_t Q0 = (H0 * M);\n+    const uint64_t L1 = (Q0 ^ U0);\n+\n+    uint64_t U1, V1; fullmul64(U1, V1, L1, M);\n+    const uint64_t P1 = (V0 ^ M);\n+    const uint64_t Q1 = ror(P1, L1);\n+    const uint64_t L2 = (Q1 ^ U1);\n+    return V1 ^ L2;\n+  }\n+\n+  static uint32_t get_hash32(uint32_t x, uint32_t y) {\n+    const uint32_t M  = 0x337954D5;\n+    const uint32_t A  = 0xAAAAAAAA; \/\/ REPAA\n+    const uint32_t H0 = (x ^ y), L0 = (x ^ A);\n+\n+    uint32_t U0, V0; fullmul32(U0, V0, L0, M);\n+    const uint32_t Q0 = (H0 * M);\n+    const uint32_t L1 = (Q0 ^ U0);\n+\n+    uint32_t U1, V1; fullmul32(U1, V1, L1, M);\n+    const uint32_t P1 = (V0 ^ M);\n+    const uint32_t Q1 = ror(P1, L1);\n+    const uint32_t L2 = (Q1 ^ U1);\n+    return V1 ^ L2;\n+  }\n+};\n+\n+#endif\/\/ SHARE_UTILITIES_FASTHASH_HPP\n","filename":"src\/hotspot\/share\/utilities\/fastHash.hpp","additions":97,"deletions":0,"binary":false,"changes":97,"status":"added"},{"patch":"@@ -1025,1 +1025,3 @@\n-  LM_LIGHTWEIGHT = 2\n+  LM_LIGHTWEIGHT = 2,\n+  \/\/ New PLACEHOLDER locking based on lightweigh, with monitors as 2nd tier using OMWorld\n+  LM_PLACEHOLDER = 3\n","filename":"src\/hotspot\/share\/utilities\/globalDefinitions.hpp","additions":3,"deletions":1,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -578,1 +578,1 @@\n-   st->print_cr(\"# Java VM: %s%s%s (%s%s, %s%s%s%s%s%s, %s, %s)\",\n+   st->print_cr(\"# Java VM: %s%s%s (%s%s, %s%s%s%s%s%s%s, %s, %s)\",\n@@ -593,0 +593,4 @@\n+                 LockingMode == LM_MONITOR ? \", lm_monitors\" :\n+                 LockingMode == LM_LEGACY ? \", lm_legacy\" :\n+                 LockingMode == LM_LIGHTWEIGHT ? \", lm_lightweight\" :\n+                 LockingMode == LM_PLACEHOLDER ? \", lm_placeholder\" : \"\",\n@@ -1119,1 +1123,1 @@\n-  STEP_IF(\"printing lock stack\", _verbose && _thread != nullptr && _thread->is_Java_thread() && LockingMode == LM_LIGHTWEIGHT);\n+  STEP_IF(\"printing lock stack\", _verbose && _thread != nullptr && _thread->is_Java_thread() && (LockingMode == LM_LIGHTWEIGHT || LockingMode == LM_PLACEHOLDER));\n","filename":"src\/hotspot\/share\/utilities\/vmError.cpp","additions":6,"deletions":2,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -27,0 +27,3 @@\n+import sun.jvm.hotspot.oops.Mark;\n+import sun.jvm.hotspot.runtime.VM;\n+\n@@ -397,1 +400,9 @@\n-    long value = readCInteger(address, getKlassPtrSize(), true);\n+    long value;\n+    if (VM.getVM().isCompactObjectHeadersEnabled()) {\n+      \/\/ On 64 bit systems, the compressed Klass* is currently read from the mark\n+      \/\/ word. We need to load the whole mark, and shift the upper parts.\n+      value = readCInteger(address, machDesc.getAddressSize(), true);\n+      value = value >>> Mark.getKlassShift();\n+    } else {\n+      value = readCInteger(address, getKlassPtrSize(), true);\n+    }\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/debugger\/DebuggerBase.java","additions":12,"deletions":1,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -84,1 +84,3 @@\n-    if (VM.getVM().isCompressedKlassPointersEnabled()) {\n+    if (VM.getVM().isCompactObjectHeadersEnabled()) {\n+      lengthOffsetInBytes = Oop.getHeaderSize();\n+    } else if (VM.getVM().isCompressedKlassPointersEnabled()) {\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/oops\/Array.java","additions":3,"deletions":1,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -58,1 +58,3 @@\n-    if (VM.getVM().isCompressedKlassPointersEnabled()) {\n+    if (VM.getVM().isCompactObjectHeadersEnabled()) {\n+      return Oop.getHeaderSize();\n+    } else if (VM.getVM().isCompressedKlassPointersEnabled()) {\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/oops\/Instance.java","additions":3,"deletions":1,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -54,0 +54,1 @@\n+    hashBitsCompact     = db.lookupLongConstant(\"markWord::hash_bits_compact\").longValue();\n@@ -57,0 +58,4 @@\n+    hashShiftCompact    = db.lookupLongConstant(\"markWord::hash_shift_compact\").longValue();\n+    if (VM.getVM().isLP64()) {\n+      klassShift          = db.lookupLongConstant(\"markWord::klass_shift\").longValue();\n+    }\n@@ -63,0 +68,2 @@\n+    hashMaskCompact     = db.lookupLongConstant(\"markWord::hash_mask_compact\").longValue();\n+    hashMaskCompactInPlace = db.lookupLongConstant(\"markWord::hash_mask_compact_in_place\").longValue();\n@@ -81,0 +88,1 @@\n+  private static long hashBitsCompact;\n@@ -85,0 +93,2 @@\n+  private static long hashShiftCompact;\n+  private static long klassShift;\n@@ -92,0 +102,2 @@\n+  private static long hashMaskCompact;\n+  private static long hashMaskCompactInPlace;\n@@ -105,0 +117,4 @@\n+  public static long getKlassShift() {\n+    return klassShift;\n+  }\n+\n@@ -159,0 +175,10 @@\n+    if (VM.getVM().getCommandLineFlag(\"LockingMode\").getInt() == LockingMode.getPlaceholder()) {\n+      Iterator it = ObjectSynchronizer.objectMonitorIterator();\n+      while (it != null && it.hasNext()) {\n+        ObjectMonitor mon = (ObjectMonitor)it.next();\n+        if (getAddress().equals(mon.object())) {\n+          return mon;\n+        }\n+      }\n+      return null;\n+    }\n@@ -177,1 +203,5 @@\n-    return Bits.maskBitsLong(value() >> hashShift, hashMask);\n+    if (VM.getVM().isCompactObjectHeadersEnabled()) {\n+      return Bits.maskBitsLong(value() >> hashShiftCompact, hashMaskCompact);\n+    } else {\n+      return Bits.maskBitsLong(value() >> hashShift, hashMask);\n+    }\n@@ -184,0 +214,6 @@\n+  public Klass getKlass() {\n+    assert(VM.getVM().isCompactObjectHeadersEnabled());\n+    assert(!hasMonitor());\n+    return (Klass)Metadata.instantiateWrapperFor(addr.getCompKlassAddressAt(0));\n+  }\n+\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/oops\/Mark.java","additions":37,"deletions":1,"binary":false,"changes":38,"status":"modified"},{"patch":"@@ -49,3 +49,8 @@\n-    klass      = new MetadataField(type.getAddressField(\"_metadata._klass\"), 0);\n-    compressedKlass  = new NarrowKlassField(type.getAddressField(\"_metadata._compressed_klass\"), 0);\n-    headerSize = type.getSize();\n+    if (VM.getVM().isCompactObjectHeadersEnabled()) {\n+      Type markType = db.lookupType(\"markWord\");\n+      headerSize = markType.getSize();\n+    } else {\n+      headerSize = type.getSize();\n+      klass      = new MetadataField(type.getAddressField(\"_metadata._klass\"), 0);\n+      compressedKlass  = new NarrowKlassField(type.getAddressField(\"_metadata._compressed_klass\"), 0);\n+    }\n@@ -78,0 +83,10 @@\n+\n+  private static Klass getKlass(Mark mark) {\n+    assert(VM.getVM().isCompactObjectHeadersEnabled());\n+    if (mark.hasMonitor() && VM.getVM().getCommandLineFlag(\"LockingMode\").getInt() != LockingMode.getPlaceholder()) {\n+      ObjectMonitor mon = mark.monitor();\n+      mark = mon.header();\n+    }\n+    return mark.getKlass();\n+  }\n+\n@@ -79,1 +94,4 @@\n-    if (VM.getVM().isCompressedKlassPointersEnabled()) {\n+    if (VM.getVM().isCompactObjectHeadersEnabled()) {\n+        assert(VM.getVM().isCompressedKlassPointersEnabled());\n+        return getKlass(getMark());\n+    } else if (VM.getVM().isCompressedKlassPointersEnabled()) {\n@@ -150,4 +168,6 @@\n-      if (VM.getVM().isCompressedKlassPointersEnabled()) {\n-        visitor.doMetadata(compressedKlass, true);\n-      } else {\n-        visitor.doMetadata(klass, true);\n+      if (!VM.getVM().isCompactObjectHeadersEnabled()) {\n+        if (VM.getVM().isCompressedKlassPointersEnabled()) {\n+          visitor.doMetadata(compressedKlass, true);\n+        } else {\n+          visitor.doMetadata(klass, true);\n+        }\n@@ -209,1 +229,4 @@\n-    if (VM.getVM().isCompressedKlassPointersEnabled()) {\n+    if (VM.getVM().isCompactObjectHeadersEnabled()) {\n+      Mark mark = new Mark(handle);\n+      return getKlass(mark);\n+    } else if (VM.getVM().isCompressedKlassPointersEnabled()) {\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/oops\/Oop.java","additions":32,"deletions":9,"binary":false,"changes":41,"status":"modified"},{"patch":"@@ -37,0 +37,1 @@\n+  private static int placeholder;\n@@ -47,0 +48,1 @@\n+    placeholder = db.lookupIntConstant(\"LM_PLACEHOLDER\").intValue();\n@@ -60,0 +62,4 @@\n+\n+  public static int getPlaceholder() {\n+    return placeholder;\n+  }\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/runtime\/LockingMode.java","additions":6,"deletions":0,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -58,0 +58,3 @@\n+      if (VM.getVM().getCommandLineFlag(\"LockingMode\").getInt() == LockingMode.getPlaceholder()) {\n+        return mark.hash();\n+      }\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/runtime\/ObjectSynchronizer.java","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -218,0 +218,1 @@\n+        assert(VM.getVM().getCommandLineFlag(\"LockingMode\").getInt() != LockingMode.getPlaceholder());\n@@ -235,1 +236,2 @@\n-        if (VM.getVM().getCommandLineFlag(\"LockingMode\").getInt() == LockingMode.getLightweight()) {\n+        if (VM.getVM().getCommandLineFlag(\"LockingMode\").getInt() == LockingMode.getLightweight() ||\n+            VM.getVM().getCommandLineFlag(\"LockingMode\").getInt() == LockingMode.getPlaceholder()) {\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/runtime\/Threads.java","additions":3,"deletions":1,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -151,0 +151,1 @@\n+  private Boolean compactObjectHeadersEnabled;\n@@ -963,0 +964,9 @@\n+  public boolean isCompactObjectHeadersEnabled() {\n+    if (compactObjectHeadersEnabled == null) {\n+        Flag flag = getCommandLineFlag(\"UseCompactObjectHeaders\");\n+        compactObjectHeadersEnabled = (flag == null) ? Boolean.FALSE:\n+             (flag.getBool()? Boolean.TRUE: Boolean.FALSE);\n+    }\n+    return compactObjectHeadersEnabled.booleanValue();\n+  }\n+\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/runtime\/VM.java","additions":10,"deletions":0,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -29,0 +29,1 @@\n+import sun.jvm.hotspot.oops.Oop;\n@@ -40,20 +41,0 @@\n-  private static AddressField klassField;\n-\n-  static {\n-    VM.registerVMInitializedObserver(new Observer() {\n-        public void update(Observable o, Object data) {\n-          initialize(VM.getVM().getTypeDataBase());\n-        }\n-      });\n-  }\n-\n-  private static void initialize(TypeDataBase db) {\n-    Type type = db.lookupType(\"oopDesc\");\n-\n-    if (VM.getVM().isCompressedKlassPointersEnabled()) {\n-      klassField = type.getAddressField(\"_metadata._compressed_klass\");\n-    } else {\n-      klassField = type.getAddressField(\"_metadata._klass\");\n-    }\n-  }\n-\n@@ -69,5 +50,1 @@\n-      if (VM.getVM().isCompressedKlassPointersEnabled()) {\n-        Metadata.instantiateWrapperFor(oop.getCompKlassAddressAt(klassField.getOffset()));\n-      } else {\n-        Metadata.instantiateWrapperFor(klassField.getValue(oop));\n-      }\n+      Oop.getKlassForOopHandle(oop);\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/utilities\/RobustOopDeterminator.java","additions":2,"deletions":25,"binary":false,"changes":27,"status":"modified"},{"patch":"@@ -25,0 +25,1 @@\n+#include \"gc\/shared\/gc_globals.hpp\"\n@@ -45,1 +46,1 @@\n-  static markWord originalMark() { return markWord(markWord::lock_mask_in_place); }\n+  static markWord originalMark() { return markWord(markWord::unlocked_value); }\n@@ -58,0 +59,2 @@\n+  FlagSetting fs(UseAltGCForwarding, false);\n+\n","filename":"test\/hotspot\/gtest\/gc\/shared\/test_preservedMarks.cpp","additions":4,"deletions":1,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -0,0 +1,124 @@\n+\/*\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"gc\/shared\/gc_globals.hpp\"\n+#include \"gc\/shared\/slidingForwarding.inline.hpp\"\n+#include \"oops\/markWord.hpp\"\n+#include \"oops\/oop.inline.hpp\"\n+#include \"utilities\/align.hpp\"\n+#include \"unittest.hpp\"\n+\n+#ifdef _LP64\n+#ifndef PRODUCT\n+\n+static uintptr_t make_mark(uintptr_t target_region, uintptr_t offset) {\n+  return (target_region) << 3 | (offset << 4) | 3 \/* forwarded *\/;\n+}\n+\n+static uintptr_t make_fallback() {\n+  return ((uintptr_t(1) << 2) \/* fallback *\/ | 3 \/* forwarded *\/);\n+}\n+\n+\/\/ Test simple forwarding within the same region.\n+TEST_VM(SlidingForwarding, simple) {\n+  FlagSetting fs(UseAltGCForwarding, true);\n+  HeapWord fakeheap[32] = { nullptr };\n+  HeapWord* heap = align_up(fakeheap, 8 * sizeof(HeapWord));\n+  oop obj1 = cast_to_oop(&heap[2]);\n+  oop obj2 = cast_to_oop(&heap[0]);\n+  SlidingForwarding::initialize(MemRegion(&heap[0], &heap[16]), 8);\n+  obj1->set_mark(markWord::prototype());\n+  SlidingForwarding::begin();\n+\n+  SlidingForwarding::forward_to<true>(obj1, obj2);\n+  ASSERT_EQ(obj1->mark().value(), make_mark(0 \/* target_region *\/, 0 \/* offset *\/));\n+  ASSERT_EQ(SlidingForwarding::forwardee<true>(obj1), obj2);\n+\n+  SlidingForwarding::end();\n+}\n+\n+\/\/ Test forwardings crossing 2 regions.\n+TEST_VM(SlidingForwarding, tworegions) {\n+  FlagSetting fs(UseAltGCForwarding, true);\n+  HeapWord fakeheap[32] = { nullptr };\n+  HeapWord* heap = align_up(fakeheap, 8 * sizeof(HeapWord));\n+  oop obj1 = cast_to_oop(&heap[14]);\n+  oop obj2 = cast_to_oop(&heap[2]);\n+  oop obj3 = cast_to_oop(&heap[10]);\n+  SlidingForwarding::initialize(MemRegion(&heap[0], &heap[16]), 8);\n+  obj1->set_mark(markWord::prototype());\n+  SlidingForwarding::begin();\n+\n+  SlidingForwarding::forward_to<true>(obj1, obj2);\n+  ASSERT_EQ(obj1->mark().value(), make_mark(0 \/* target_region *\/, 2 \/* offset *\/));\n+  ASSERT_EQ(SlidingForwarding::forwardee<true>(obj1), obj2);\n+\n+  SlidingForwarding::forward_to<true>(obj1, obj3);\n+  ASSERT_EQ(obj1->mark().value(), make_mark(1 \/* target_region *\/, 2 \/* offset *\/));\n+  ASSERT_EQ(SlidingForwarding::forwardee<true>(obj1), obj3);\n+\n+  SlidingForwarding::end();\n+}\n+\n+\/\/ Test fallback forwardings crossing 4 regions.\n+TEST_VM(SlidingForwarding, fallback) {\n+  FlagSetting fs(UseAltGCForwarding, true);\n+  HeapWord fakeheap[32] = { nullptr };\n+  HeapWord* heap = align_up(fakeheap, 8 * sizeof(HeapWord));\n+  oop s_obj1 = cast_to_oop(&heap[12]);\n+  oop s_obj2 = cast_to_oop(&heap[13]);\n+  oop s_obj3 = cast_to_oop(&heap[14]);\n+  oop s_obj4 = cast_to_oop(&heap[15]);\n+  oop t_obj1 = cast_to_oop(&heap[2]);\n+  oop t_obj2 = cast_to_oop(&heap[4]);\n+  oop t_obj3 = cast_to_oop(&heap[10]);\n+  oop t_obj4 = cast_to_oop(&heap[12]);\n+  SlidingForwarding::initialize(MemRegion(&heap[0], &heap[16]), 4);\n+  s_obj1->set_mark(markWord::prototype());\n+  s_obj2->set_mark(markWord::prototype());\n+  s_obj3->set_mark(markWord::prototype());\n+  s_obj4->set_mark(markWord::prototype());\n+  SlidingForwarding::begin();\n+\n+  SlidingForwarding::forward_to<true>(s_obj1, t_obj1);\n+  ASSERT_EQ(s_obj1->mark().value(), make_mark(0 \/* target_region *\/, 2 \/* offset *\/));\n+  ASSERT_EQ(SlidingForwarding::forwardee<true>(s_obj1), t_obj1);\n+\n+  SlidingForwarding::forward_to<true>(s_obj2, t_obj2);\n+  ASSERT_EQ(s_obj2->mark().value(), make_mark(1 \/* target_region *\/, 0 \/* offset *\/));\n+  ASSERT_EQ(SlidingForwarding::forwardee<true>(s_obj2), t_obj2);\n+\n+  SlidingForwarding::forward_to<true>(s_obj3, t_obj3);\n+  ASSERT_EQ(s_obj3->mark().value(), make_fallback());\n+  ASSERT_EQ(SlidingForwarding::forwardee<true>(s_obj3), t_obj3);\n+\n+  SlidingForwarding::forward_to<true>(s_obj4, t_obj4);\n+  ASSERT_EQ(s_obj4->mark().value(), make_fallback());\n+  ASSERT_EQ(SlidingForwarding::forwardee<true>(s_obj4), t_obj4);\n+\n+  SlidingForwarding::end();\n+}\n+\n+#endif \/\/ PRODUCT\n+#endif \/\/ _LP64\n","filename":"test\/hotspot\/gtest\/gc\/shared\/test_slidingForwarding.cpp","additions":124,"deletions":0,"binary":false,"changes":124,"status":"added"},{"patch":"@@ -85,1 +85,12 @@\n-  if (UseCompressedClassPointers) {\n+  if (UseCompactObjectHeaders) {\n+    EXPECT_EQ(arrayOopDesc::base_offset_in_bytes(T_BOOLEAN), 12);\n+    EXPECT_EQ(arrayOopDesc::base_offset_in_bytes(T_BYTE),    12);\n+    EXPECT_EQ(arrayOopDesc::base_offset_in_bytes(T_SHORT),   12);\n+    EXPECT_EQ(arrayOopDesc::base_offset_in_bytes(T_CHAR),    12);\n+    EXPECT_EQ(arrayOopDesc::base_offset_in_bytes(T_INT),     12);\n+    EXPECT_EQ(arrayOopDesc::base_offset_in_bytes(T_FLOAT),   12);\n+    EXPECT_EQ(arrayOopDesc::base_offset_in_bytes(T_LONG),    16);\n+    EXPECT_EQ(arrayOopDesc::base_offset_in_bytes(T_DOUBLE),  16);\n+    EXPECT_EQ(arrayOopDesc::base_offset_in_bytes(T_OBJECT),  12);\n+    EXPECT_EQ(arrayOopDesc::base_offset_in_bytes(T_ARRAY),   12);\n+  } else if (UseCompressedClassPointers) {\n","filename":"test\/hotspot\/gtest\/oops\/test_arrayOop.cpp","additions":12,"deletions":1,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -31,1 +31,1 @@\n-    int objal; bool ccp; bool coops; int result;\n+    int objal; bool ccp; bool coops; bool coh; int result;\n@@ -33,1 +33,1 @@\n-\/\/    ObjAligInB, UseCCP, UseCoops, object size in heap words\n+\/\/    ObjAligInB, UseCCP, UseCoops, UseCOH, object size in heap words\n@@ -35,12 +35,24 @@\n-    { 8,          false,  false,    4 },  \/\/ 20 byte header, 8 byte oops\n-    { 8,          false,  true,     3 },  \/\/ 20 byte header, 4 byte oops\n-    { 8,          true,   false,    3 },  \/\/ 16 byte header, 8 byte oops\n-    { 8,          true,   true,     3 },  \/\/ 16 byte header, 4 byte oops\n-    { 16,         false,  false,    4 },  \/\/ 20 byte header, 8 byte oops, 16-byte align\n-    { 16,         false,  true,     4 },  \/\/ 20 byte header, 4 byte oops, 16-byte align\n-    { 16,         true,   false,    4 },  \/\/ 16 byte header, 8 byte oops, 16-byte align\n-    { 16,         true,   true,     4 },  \/\/ 16 byte header, 4 byte oops, 16-byte align\n-    { 256,        false,  false,    32 }, \/\/ 20 byte header, 8 byte oops, 256-byte align\n-    { 256,        false,  true,     32 }, \/\/ 20 byte header, 4 byte oops, 256-byte align\n-    { 256,        true,   false,    32 }, \/\/ 16 byte header, 8 byte oops, 256-byte align\n-    { 256,        true,   true,     32 }, \/\/ 16 byte header, 4 byte oops, 256-byte align\n+    { 8,          false,  false,    false,  4 },  \/\/ 20 byte header, 8 byte oops\n+    { 8,          false,  true,     false,  3 },  \/\/ 20 byte header, 4 byte oops\n+    { 8,          true,   false,    false,  3 },  \/\/ 16 byte header, 8 byte oops\n+    { 8,          true,   true,     false,  3 },  \/\/ 16 byte header, 4 byte oops\n+    { 16,         false,  false,    false,  4 },  \/\/ 20 byte header, 8 byte oops, 16-byte align\n+    { 16,         false,  true,     false,  4 },  \/\/ 20 byte header, 4 byte oops, 16-byte align\n+    { 16,         true,   false,    false,  4 },  \/\/ 16 byte header, 8 byte oops, 16-byte align\n+    { 16,         true,   true,     false,  4 },  \/\/ 16 byte header, 4 byte oops, 16-byte align\n+    { 256,        false,  false,    false,  32 }, \/\/ 20 byte header, 8 byte oops, 256-byte align\n+    { 256,        false,  true,     false,  32 }, \/\/ 20 byte header, 4 byte oops, 256-byte align\n+    { 256,        true,   false,    false,  32 }, \/\/ 16 byte header, 8 byte oops, 256-byte align\n+    { 256,        true,   true,     false,  32 }, \/\/ 16 byte header, 4 byte oops, 256-byte align\n+    { 8,          false,  false,    true,   3 },  \/\/ 16 byte header, 8 byte oops\n+    { 8,          false,  true,     true,   2 },  \/\/ 12 byte header, 4 byte oops\n+    { 8,          true,   false,    true,   3 },  \/\/ 16 byte header, 8 byte oops\n+    { 8,          true,   true,     true,   2 },  \/\/ 12 byte header, 4 byte oops\n+    { 16,         false,  false,    true,   4 },  \/\/ 16 byte header, 8 byte oops, 16-byte align\n+    { 16,         false,  true,     true,   2 },  \/\/ 12 byte header, 4 byte oops, 16-byte align\n+    { 16,         true,   false,    true,   4 },  \/\/ 16 byte header, 8 byte oops, 16-byte align\n+    { 16,         true,   true,     true,   2 },  \/\/ 12 byte header, 4 byte oops, 16-byte align\n+    { 256,        false,  false,    true,   32 }, \/\/ 16 byte header, 8 byte oops, 256-byte align\n+    { 256,        false,  true,     true,   32 }, \/\/ 12 byte header, 4 byte oops, 256-byte align\n+    { 256,        true,   false,    true,   32 }, \/\/ 16 byte header, 8 byte oops, 256-byte align\n+    { 256,        true,   true,     true,   32 }, \/\/ 12 byte header, 4 byte oops, 256-byte align\n@@ -48,1 +60,1 @@\n-    { 8,          false,  false,    4 }, \/\/ 12 byte header, 4 byte oops, wordsize 4\n+    { 8,          false,  false,    false,  4 }, \/\/ 12 byte header, 4 byte oops, wordsize 4\n@@ -50,1 +62,1 @@\n-    { -1,         false,  false,   -1 }\n+    { -1,         false,  false,    false, -1 }\n@@ -53,1 +65,1 @@\n-    if (x[i].objal == (int)ObjectAlignmentInBytes && x[i].ccp == UseCompressedClassPointers && x[i].coops == UseCompressedOops) {\n+    if (x[i].objal == (int)ObjectAlignmentInBytes && x[i].ccp == UseCompressedClassPointers && x[i].coops == UseCompressedOops && x[i].coh == UseCompactObjectHeaders) {\n","filename":"test\/hotspot\/gtest\/oops\/test_objArrayOop.cpp","additions":29,"deletions":17,"binary":false,"changes":46,"status":"modified"},{"patch":"@@ -39,1 +39,5 @@\n-  o->set_klass(Universe::boolArrayKlassObj());\n+  if (UseCompactObjectHeaders) {\n+    o->set_mark(Universe::boolArrayKlassObj()->prototype_header());\n+  } else {\n+    o->set_klass(Universe::boolArrayKlassObj());\n+  }\n","filename":"test\/hotspot\/gtest\/oops\/test_typeArrayOop.cpp","additions":5,"deletions":1,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -67,1 +67,1 @@\n-  if (LockingMode != LM_LIGHTWEIGHT || !VM_Version::supports_recursive_lightweight_locking()) {\n+  if (LockingMode != LM_PLACEHOLDER) {\n@@ -134,1 +134,1 @@\n-  if (LockingMode != LM_LIGHTWEIGHT || !VM_Version::supports_recursive_lightweight_locking()) {\n+  if (LockingMode != LM_PLACEHOLDER) {\n@@ -201,1 +201,1 @@\n-  if (LockingMode != LM_LIGHTWEIGHT) {\n+  if (LockingMode != LM_LIGHTWEIGHT && LockingMode != LM_PLACEHOLDER) {\n@@ -205,1 +205,1 @@\n-  const bool test_recursive = VM_Version::supports_recursive_lightweight_locking();\n+  const bool test_recursive = LockingMode == LM_PLACEHOLDER;\n@@ -267,1 +267,1 @@\n-  if (LockingMode != LM_LIGHTWEIGHT) {\n+  if (LockingMode != LM_LIGHTWEIGHT && LockingMode != LM_PLACEHOLDER) {\n@@ -271,1 +271,1 @@\n-  const bool test_recursive = VM_Version::supports_recursive_lightweight_locking();\n+  const bool test_recursive = LockingMode == LM_PLACEHOLDER;\n","filename":"test\/hotspot\/gtest\/runtime\/test_lockStack.cpp","additions":6,"deletions":6,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -0,0 +1,29 @@\n+#\n+# Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n+# DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+#\n+# This code is free software; you can redistribute it and\/or modify it\n+# under the terms of the GNU General Public License version 2 only, as\n+# published by the Free Software Foundation.\n+#\n+# This code is distributed in the hope that it will be useful, but WITHOUT\n+# ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+# FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+# version 2 for more details (a copy is included in the LICENSE file that\n+# accompanied this code).\n+#\n+# You should have received a copy of the GNU General Public License version\n+# 2 along with this work; if not, write to the Free Software Foundation,\n+# Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+#\n+# Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+# or visit www.oracle.com if you need additional information or have any\n+# questions.\n+#\n+\n+#\n+# These tests are problematic when +UseCompactObjectHeaders is enabled.\n+# The test exclusions are for the cases when we are sure the tests would fail\n+# for the known and innocuous implementation reasons.\n+#\n+\n","filename":"test\/hotspot\/jtreg\/ProblemList-lilliput.txt","additions":29,"deletions":0,"binary":false,"changes":29,"status":"added"},{"patch":"@@ -150,1 +150,2 @@\n-    @IR(counts = { IRNode.LOAD_VECTOR_L, \">=1\", IRNode.STORE_VECTOR, \">=1\" })\n+    @IR(applyIf = {\"UseCompactObjectHeaders\", \"false\"},\n+        counts = { IRNode.LOAD_VECTOR_L, \">=1\", IRNode.STORE_VECTOR, \">=1\" })\n@@ -163,1 +164,2 @@\n-    @IR(counts = { IRNode.LOAD_VECTOR_L, \">=1\", IRNode.STORE_VECTOR, \">=1\" })\n+    @IR(applyIf = {\"UseCompactObjectHeaders\", \"false\"},\n+        counts = { IRNode.LOAD_VECTOR_L, \">=1\", IRNode.STORE_VECTOR, \">=1\" })\n@@ -176,1 +178,2 @@\n-    @IR(counts = { IRNode.LOAD_VECTOR_L, \">=1\", IRNode.STORE_VECTOR, \">=1\" })\n+    @IR(applyIf = {\"UseCompactObjectHeaders\", \"false\"},\n+        counts = { IRNode.LOAD_VECTOR_L, \">=1\", IRNode.STORE_VECTOR, \">=1\" })\n@@ -205,1 +208,2 @@\n-    @IR(counts = { IRNode.LOAD_VECTOR_L, \">=1\", IRNode.STORE_VECTOR, \">=1\" })\n+    @IR(applyIf = {\"UseCompactObjectHeaders\", \"false\"},\n+        counts = { IRNode.LOAD_VECTOR_L, \">=1\", IRNode.STORE_VECTOR, \">=1\" })\n@@ -219,1 +223,2 @@\n-    @IR(counts = { IRNode.LOAD_VECTOR_L, \">=1\", IRNode.STORE_VECTOR, \">=1\" })\n+    @IR(applyIf = {\"UseCompactObjectHeaders\", \"false\"},\n+        counts = { IRNode.LOAD_VECTOR_L, \">=1\", IRNode.STORE_VECTOR, \">=1\" })\n@@ -232,1 +237,2 @@\n-    @IR(counts = { IRNode.LOAD_VECTOR_L, \">=1\", IRNode.STORE_VECTOR, \">=1\" })\n+    @IR(applyIf = {\"UseCompactObjectHeaders\", \"false\"},\n+        counts = { IRNode.LOAD_VECTOR_L, \">=1\", IRNode.STORE_VECTOR, \">=1\" })\n","filename":"test\/hotspot\/jtreg\/compiler\/c2\/irTests\/TestVectorizationMismatchedAccess.java","additions":12,"deletions":6,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -54,1 +54,2 @@\n-    @IR(counts = { IRNode.LOAD_VECTOR_L, \">=1\", IRNode.STORE_VECTOR, \">=1\" })\n+    @IR(applyIf = {\"UseCompactObjectHeaders\", \"false\"},\n+        counts = { IRNode.LOAD_VECTOR_L, \">=1\", IRNode.STORE_VECTOR, \">=1\" })\n","filename":"test\/hotspot\/jtreg\/compiler\/c2\/irTests\/TestVectorizationNotRun.java","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -145,1 +145,2 @@\n-                    \"LogCompilation\"\n+                    \"LogCompilation\",\n+                    \"UseCompactObjectHeaders\"\n","filename":"test\/hotspot\/jtreg\/compiler\/lib\/ir_framework\/TestFramework.java","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -269,1 +269,2 @@\n-    @IR(counts = {IRNode.ADD_VI, \"> 0\", IRNode.MUL_VI, \"> 0\", IRNode.ADD_VF, \"> 0\"},\n+    @IR(applyIf = {\"UseCompactObjectHeaders\", \"false\"},\n+        counts = {IRNode.ADD_VI, \"> 0\", IRNode.MUL_VI, \"> 0\", IRNode.ADD_VF, \"> 0\"},\n","filename":"test\/hotspot\/jtreg\/compiler\/loopopts\/superword\/TestIndependentPacksWithCyclicDependency.java","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -35,1 +35,1 @@\n- * @run main\/timeout=240 gc.g1.plab.TestPLABPromotion\n+ * @run main\/othervm\/timeout=240 -Xbootclasspath\/a:. -XX:+UnlockDiagnosticVMOptions -XX:+WhiteBoxAPI gc.g1.plab.TestPLABPromotion\n@@ -51,0 +51,1 @@\n+import jdk.test.whitebox.WhiteBox;\n@@ -57,0 +58,2 @@\n+    private static final boolean COMPACT_HEADERS = Platform.is64bit() && WhiteBox.getWhiteBox().getBooleanVMFlag(\"UseCompactObjectHeaders\");\n+\n","filename":"test\/hotspot\/jtreg\/gc\/g1\/plab\/TestPLABPromotion.java","additions":4,"deletions":1,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -33,0 +33,9 @@\n+\n+\/* @test\n+ * @summary Run LockStack gtests with LockingMode=3\n+ * @library \/test\/lib\n+ * @modules java.base\/jdk.internal.misc\n+ *          java.xml\n+ * @requires vm.flagless\n+ * @run main\/native GTestWrapper --gtest_filter=LockStackTest* -XX:LockingMode=3\n+ *\/\n","filename":"test\/hotspot\/jtreg\/gtest\/LockStackGtests.java","additions":9,"deletions":0,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -25,2 +25,2 @@\n- * @test id=with-coops-no-ccp\n- * @library \/test\/lib\n+ * @test id=with-coops-no-ccp-no-ucoh\n+ * @library \/test\/lib \/\n@@ -29,1 +29,3 @@\n- * @run main\/othervm -XX:+UseCompressedOops -XX:-UseCompressedClassPointers ArrayBaseOffsets\n+ * @build jdk.test.whitebox.WhiteBox\n+ * @run driver jdk.test.lib.helpers.ClassFileInstaller jdk.test.whitebox.WhiteBox\n+ * @run main\/othervm -Xbootclasspath\/a:. -XX:+UnlockDiagnosticVMOptions -XX:+WhiteBoxAPI -XX:+UseCompressedOops -XX:-UseCompressedClassPointers -XX:+UnlockExperimentalVMOptions -XX:-UseCompactObjectHeaders ArrayBaseOffsets\n@@ -32,2 +34,2 @@\n- * @test id=with-coops-with-ccp\n- * @library \/test\/lib\n+ * @test id=with-coops-with-ccp-no-ucoh\n+ * @library \/test\/lib \/\n@@ -37,1 +39,3 @@\n- * @run main\/othervm -XX:+UseCompressedOops -XX:+UseCompressedClassPointers ArrayBaseOffsets\n+ * @build jdk.test.whitebox.WhiteBox\n+ * @run driver jdk.test.lib.helpers.ClassFileInstaller jdk.test.whitebox.WhiteBox\n+ * @run main\/othervm -Xbootclasspath\/a:. -XX:+UnlockDiagnosticVMOptions -XX:+WhiteBoxAPI -XX:+UseCompressedOops -XX:+UseCompressedClassPointers -XX:+UnlockExperimentalVMOptions -XX:-UseCompactObjectHeaders ArrayBaseOffsets\n@@ -40,2 +44,2 @@\n- * @test id=no-coops-no-ccp\n- * @library \/test\/lib\n+ * @test id=no-coops-no-ccp-no-ucoh\n+ * @library \/test\/lib \/\n@@ -44,1 +48,3 @@\n- * @run main\/othervm -XX:-UseCompressedOops -XX:-UseCompressedClassPointers ArrayBaseOffsets\n+ * @build jdk.test.whitebox.WhiteBox\n+ * @run driver jdk.test.lib.helpers.ClassFileInstaller jdk.test.whitebox.WhiteBox\n+ * @run main\/othervm -Xbootclasspath\/a:. -XX:+UnlockDiagnosticVMOptions -XX:+WhiteBoxAPI -XX:-UseCompressedOops -XX:-UseCompressedClassPointers -XX:+UnlockExperimentalVMOptions -XX:-UseCompactObjectHeaders ArrayBaseOffsets\n@@ -47,2 +53,2 @@\n- * @test id=no-coops-with-ccp\n- * @library \/test\/lib\n+ * @test id=no-coops-with-ccp-no-ucoh\n+ * @library \/test\/lib \/\n@@ -52,1 +58,41 @@\n- * @run main\/othervm -XX:-UseCompressedOops -XX:+UseCompressedClassPointers ArrayBaseOffsets\n+ * @build jdk.test.whitebox.WhiteBox\n+ * @run driver jdk.test.lib.helpers.ClassFileInstaller jdk.test.whitebox.WhiteBox\n+ * @run main\/othervm -Xbootclasspath\/a:. -XX:+UnlockDiagnosticVMOptions -XX:+WhiteBoxAPI -XX:-UseCompressedOops -XX:+UseCompressedClassPointers -XX:+UnlockExperimentalVMOptions -XX:-UseCompactObjectHeaders ArrayBaseOffsets\n+ *\/\n+\/*\n+ * @test id=with-coops-no-ccp-with-ucoh\n+ * @library \/test\/lib \/\n+ * @requires vm.bits == \"64\"\n+ * @modules java.base\/jdk.internal.misc\n+ * @build jdk.test.whitebox.WhiteBox\n+ * @run driver jdk.test.lib.helpers.ClassFileInstaller jdk.test.whitebox.WhiteBox\n+ * @run main\/othervm -Xbootclasspath\/a:. -XX:+UnlockDiagnosticVMOptions -XX:+WhiteBoxAPI -XX:+UseCompressedOops -XX:-UseCompressedClassPointers -XX:+UnlockExperimentalVMOptions -XX:+UseCompactObjectHeaders ArrayBaseOffsets\n+ *\/\n+\/*\n+ * @test id=with-coops-with-ccp-with-ucoh\n+ * @library \/test\/lib \/\n+ * @requires vm.bits == \"64\"\n+ * @requires vm.opt.UseCompressedClassPointers != false\n+ * @modules java.base\/jdk.internal.misc\n+ * @build jdk.test.whitebox.WhiteBox\n+ * @run driver jdk.test.lib.helpers.ClassFileInstaller jdk.test.whitebox.WhiteBox\n+ * @run main\/othervm -Xbootclasspath\/a:. -XX:+UnlockDiagnosticVMOptions -XX:+WhiteBoxAPI -XX:+UseCompressedOops -XX:+UseCompressedClassPointers -XX:+UnlockExperimentalVMOptions -XX:+UseCompactObjectHeaders ArrayBaseOffsets\n+ *\/\n+\/*\n+ * @test id=no-coops-no-ccp-with-ucoh\n+ * @library \/test\/lib \/\n+ * @requires vm.bits == \"64\"\n+ * @modules java.base\/jdk.internal.misc\n+ * @build jdk.test.whitebox.WhiteBox\n+ * @run driver jdk.test.lib.helpers.ClassFileInstaller jdk.test.whitebox.WhiteBox\n+ * @run main\/othervm -Xbootclasspath\/a:. -XX:+UnlockDiagnosticVMOptions -XX:+WhiteBoxAPI -XX:-UseCompressedOops -XX:-UseCompressedClassPointers -XX:+UnlockExperimentalVMOptions -XX:+UseCompactObjectHeaders ArrayBaseOffsets\n+ *\/\n+\/*\n+ * @test id=no-coops-with-ccp-with-ucoh\n+ * @library \/test\/lib \/\n+ * @requires vm.bits == \"64\"\n+ * @requires vm.opt.UseCompressedClassPointers != false\n+ * @modules java.base\/jdk.internal.misc\n+ * @build jdk.test.whitebox.WhiteBox\n+ * @run driver jdk.test.lib.helpers.ClassFileInstaller jdk.test.whitebox.WhiteBox\n+ * @run main\/othervm -Xbootclasspath\/a:. -XX:+UnlockDiagnosticVMOptions -XX:+WhiteBoxAPI -XX:-UseCompressedOops -XX:+UseCompressedClassPointers -XX:+UnlockExperimentalVMOptions -XX:+UseCompactObjectHeaders ArrayBaseOffsets\n@@ -56,1 +102,1 @@\n- * @library \/test\/lib\n+ * @library \/test\/lib \/\n@@ -59,1 +105,3 @@\n- * @run main\/othervm ArrayBaseOffsets\n+ * @build jdk.test.whitebox.WhiteBox\n+ * @run driver jdk.test.lib.helpers.ClassFileInstaller jdk.test.whitebox.WhiteBox\n+ * @run main\/othervm -Xbootclasspath\/a:. -XX:+UnlockDiagnosticVMOptions -XX:+WhiteBoxAPI ArrayBaseOffsets\n@@ -70,0 +118,1 @@\n+import jdk.test.whitebox.WhiteBox;\n@@ -73,3 +122,3 @@\n-    private static final boolean COOP;\n-    private static final boolean CCP;\n-\n+    static final long INT_OFFSET;\n+    static final int  INT_ARRAY_OFFSET;\n+    static final int  LONG_ARRAY_OFFSET;\n@@ -77,5 +126,9 @@\n-        if (Platform.is64bit()) {\n-            RuntimeMXBean runtime = ManagementFactory.getRuntimeMXBean();\n-            List<String> vmargs = runtime.getInputArguments();\n-            CCP = !vmargs.contains(\"-XX:-UseCompressedClassPointers\");\n-            COOP = System.getProperty(\"java.vm.compressedOopsMode\") != null;\n+        WhiteBox WB = WhiteBox.getWhiteBox();\n+        if (!Platform.is64bit() || WB.getBooleanVMFlag(\"UseCompactObjectHeaders\")) {\n+            INT_OFFSET = 8;\n+            INT_ARRAY_OFFSET = 12;\n+            LONG_ARRAY_OFFSET = 16;\n+        } else if (WB.getBooleanVMFlag(\"UseCompressedClassPointers\")) {\n+            INT_OFFSET = 12;\n+            INT_ARRAY_OFFSET = 16;\n+            LONG_ARRAY_OFFSET = 16;\n@@ -83,1 +136,3 @@\n-            COOP = CCP = false;\n+            INT_OFFSET = 16;\n+            INT_ARRAY_OFFSET = 20;\n+            LONG_ARRAY_OFFSET = 24;\n@@ -89,22 +144,11 @@\n-        int intOffset, longOffset;\n-        if (Platform.is64bit()) {\n-            if (CCP) {\n-                intOffset = 16;\n-                longOffset = 16;\n-            } else {\n-                intOffset = 20;\n-                longOffset = 24;\n-            }\n-        } else {\n-            intOffset = 12;\n-            longOffset = 16;\n-        }\n-        Asserts.assertEquals(unsafe.arrayBaseOffset(boolean[].class), intOffset,  \"Misplaced boolean array base\");\n-        Asserts.assertEquals(unsafe.arrayBaseOffset(byte[].class),    intOffset,  \"Misplaced byte    array base\");\n-        Asserts.assertEquals(unsafe.arrayBaseOffset(char[].class),    intOffset,  \"Misplaced char    array base\");\n-        Asserts.assertEquals(unsafe.arrayBaseOffset(short[].class),   intOffset,  \"Misplaced short   array base\");\n-        Asserts.assertEquals(unsafe.arrayBaseOffset(int[].class),     intOffset,  \"Misplaced int     array base\");\n-        Asserts.assertEquals(unsafe.arrayBaseOffset(long[].class),    longOffset, \"Misplaced long    array base\");\n-        Asserts.assertEquals(unsafe.arrayBaseOffset(float[].class),   intOffset,  \"Misplaced float   array base\");\n-        Asserts.assertEquals(unsafe.arrayBaseOffset(double[].class),  longOffset, \"Misplaced double  array base\");\n-        int expectedObjArrayOffset = (COOP || !Platform.is64bit()) ? intOffset : longOffset;\n+        Asserts.assertEquals(unsafe.arrayBaseOffset(boolean[].class), INT_ARRAY_OFFSET,  \"Misplaced boolean array base\");\n+        Asserts.assertEquals(unsafe.arrayBaseOffset(byte[].class),    INT_ARRAY_OFFSET,  \"Misplaced byte    array base\");\n+        Asserts.assertEquals(unsafe.arrayBaseOffset(char[].class),    INT_ARRAY_OFFSET,  \"Misplaced char    array base\");\n+        Asserts.assertEquals(unsafe.arrayBaseOffset(short[].class),   INT_ARRAY_OFFSET,  \"Misplaced short   array base\");\n+        Asserts.assertEquals(unsafe.arrayBaseOffset(int[].class),     INT_ARRAY_OFFSET,  \"Misplaced int     array base\");\n+        Asserts.assertEquals(unsafe.arrayBaseOffset(long[].class),    LONG_ARRAY_OFFSET, \"Misplaced long    array base\");\n+        Asserts.assertEquals(unsafe.arrayBaseOffset(float[].class),   INT_ARRAY_OFFSET,  \"Misplaced float   array base\");\n+        Asserts.assertEquals(unsafe.arrayBaseOffset(double[].class),  LONG_ARRAY_OFFSET, \"Misplaced double  array base\");\n+        boolean narrowOops = System.getProperty(\"java.vm.compressedOopsMode\") != null ||\n+                             !Platform.is64bit();\n+        int expectedObjArrayOffset = narrowOops ? INT_ARRAY_OFFSET : LONG_ARRAY_OFFSET;\n","filename":"test\/hotspot\/jtreg\/runtime\/FieldLayout\/ArrayBaseOffsets.java","additions":89,"deletions":45,"binary":false,"changes":134,"status":"modified"},{"patch":"@@ -0,0 +1,107 @@\n+\/*\n+ * Copyright (c) 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+\/*\n+ * @test id=default\n+ * @library \/test\/lib \/\n+ * @modules java.base\/jdk.internal.misc\n+ *          java.management\n+ * @build jdk.test.whitebox.WhiteBox\n+ * @run driver jdk.test.lib.helpers.ClassFileInstaller jdk.test.whitebox.WhiteBox\n+ * @run main\/othervm -Xbootclasspath\/a:. -XX:+UnlockDiagnosticVMOptions -XX:+WhiteBoxAPI BaseOffsets\n+ *\/\n+\/*\n+ * @test id=no-coops\n+ * @library \/test\/lib \/\n+ * @requires vm.bits == \"64\"\n+ * @modules java.base\/jdk.internal.misc\n+ *          java.management\n+ * @build jdk.test.whitebox.WhiteBox\n+ * @run driver jdk.test.lib.helpers.ClassFileInstaller jdk.test.whitebox.WhiteBox\n+ * @run main\/othervm -Xbootclasspath\/a:. -XX:+UnlockDiagnosticVMOptions -XX:+WhiteBoxAPI -XX:-UseCompressedOops BaseOffsets\n+ *\/\n+\n+import java.lang.reflect.Field;\n+import java.util.Arrays;\n+import java.util.Comparator;\n+import jdk.internal.misc.Unsafe;\n+\n+import jdk.test.lib.Asserts;\n+import jdk.test.lib.Platform;\n+import jdk.test.whitebox.WhiteBox;\n+\n+public class BaseOffsets {\n+\n+    static class LIClass {\n+        public int i;\n+    }\n+\n+    public static final WhiteBox WB = WhiteBox.getWhiteBox();\n+\n+  \/\/ @0:  8 byte header,  @8: int field\n+    static final long INT_OFFSET;\n+    static final int  INT_ARRAY_OFFSET;\n+    static final int  LONG_ARRAY_OFFSET;\n+    static {\n+        if (!Platform.is64bit() || WB.getBooleanVMFlag(\"UseCompactObjectHeaders\")) {\n+            INT_OFFSET = 8;\n+            INT_ARRAY_OFFSET = 12;\n+            LONG_ARRAY_OFFSET = 16;\n+        } else if (WB.getBooleanVMFlag(\"UseCompressedClassPointers\")) {\n+            INT_OFFSET = 12;\n+            INT_ARRAY_OFFSET = 16;\n+            LONG_ARRAY_OFFSET = 16;\n+        } else {\n+            INT_OFFSET = 16;\n+            INT_ARRAY_OFFSET = 20;\n+            LONG_ARRAY_OFFSET = 24;\n+        }\n+    }\n+\n+    static public void main(String[] args) {\n+        Unsafe unsafe = Unsafe.getUnsafe();\n+        Class c = LIClass.class;\n+        Field[] fields = c.getFields();\n+        for (int i = 0; i < fields.length; i++) {\n+            long offset = unsafe.objectFieldOffset(fields[i]);\n+            if (fields[i].getType() == int.class) {\n+                Asserts.assertEquals(offset, INT_OFFSET, \"Misplaced int field\");\n+            } else {\n+                Asserts.fail(\"Unexpected field type\");\n+            }\n+        }\n+\n+        Asserts.assertEquals(unsafe.arrayBaseOffset(boolean[].class), INT_ARRAY_OFFSET,  \"Misplaced boolean array base\");\n+        Asserts.assertEquals(unsafe.arrayBaseOffset(byte[].class),    INT_ARRAY_OFFSET,  \"Misplaced byte    array base\");\n+        Asserts.assertEquals(unsafe.arrayBaseOffset(char[].class),    INT_ARRAY_OFFSET,  \"Misplaced char    array base\");\n+        Asserts.assertEquals(unsafe.arrayBaseOffset(short[].class),   INT_ARRAY_OFFSET,  \"Misplaced short   array base\");\n+        Asserts.assertEquals(unsafe.arrayBaseOffset(int[].class),     INT_ARRAY_OFFSET,  \"Misplaced int     array base\");\n+        Asserts.assertEquals(unsafe.arrayBaseOffset(long[].class),    LONG_ARRAY_OFFSET, \"Misplaced long    array base\");\n+        Asserts.assertEquals(unsafe.arrayBaseOffset(float[].class),   INT_ARRAY_OFFSET,  \"Misplaced float   array base\");\n+        Asserts.assertEquals(unsafe.arrayBaseOffset(double[].class),  LONG_ARRAY_OFFSET, \"Misplaced double  array base\");\n+        boolean narrowOops = System.getProperty(\"java.vm.compressedOopsMode\") != null ||\n+                             !Platform.is64bit();\n+        int expected_objary_offset = narrowOops ? INT_ARRAY_OFFSET : LONG_ARRAY_OFFSET;\n+        Asserts.assertEquals(unsafe.arrayBaseOffset(Object[].class),  expected_objary_offset, \"Misplaced object  array base\");\n+    }\n+}\n","filename":"test\/hotspot\/jtreg\/runtime\/FieldLayout\/BaseOffsets.java","additions":107,"deletions":0,"binary":false,"changes":107,"status":"added"},{"patch":"@@ -28,1 +28,1 @@\n- * @library \/test\/lib\n+ * @library \/test\/lib \/\n@@ -32,1 +32,3 @@\n- * @run main\/othervm -XX:+UseCompressedClassPointers -XX:-UseEmptySlotsInSupers OldLayoutCheck\n+ * @build jdk.test.whitebox.WhiteBox\n+ * @run driver jdk.test.lib.helpers.ClassFileInstaller jdk.test.whitebox.WhiteBox\n+ * @run main\/othervm -Xbootclasspath\/a:. -XX:+UnlockDiagnosticVMOptions -XX:+WhiteBoxAPI -XX:+UseCompressedClassPointers -XX:-UseEmptySlotsInSupers OldLayoutCheck\n@@ -38,1 +40,1 @@\n- * @library \/test\/lib\n+ * @library \/test\/lib \/\n@@ -41,1 +43,3 @@\n- * @run main\/othervm -XX:-UseEmptySlotsInSupers OldLayoutCheck\n+ * @build jdk.test.whitebox.WhiteBox\n+ * @run driver jdk.test.lib.helpers.ClassFileInstaller jdk.test.whitebox.WhiteBox\n+ * @run main\/othervm -Xbootclasspath\/a:. -XX:+UnlockDiagnosticVMOptions -XX:+WhiteBoxAPI -XX:-UseEmptySlotsInSupers OldLayoutCheck\n@@ -51,0 +55,1 @@\n+import jdk.test.whitebox.WhiteBox;\n@@ -59,4 +64,15 @@\n-    \/\/ 32-bit VMs: @0:  8 byte header,  @8: long field, @16:  int field\n-    \/\/ 64-bit VMs: @0: 12 byte header, @12:  int field, @16: long field\n-    static final long INT_OFFSET  = Platform.is64bit() ? 12L : 16L;\n-    static final long LONG_OFFSET = Platform.is64bit() ? 16L :  8L;\n+    public static final WhiteBox WB = WhiteBox.getWhiteBox();\n+\n+    \/\/ 32-bit VMs\/compact headers: @0:  8 byte header,  @8: long field, @16:  int field\n+    \/\/ 64-bit VMs:                 @0: 12 byte header, @12:  int field, @16: long field\n+    static final long INT_OFFSET;\n+    static final long LONG_OFFSET;\n+    static {\n+      if (!Platform.is64bit() || WB.getBooleanVMFlag(\"UseCompactObjectHeaders\")) {\n+        INT_OFFSET = 16L;\n+        LONG_OFFSET = 8L;\n+      } else {\n+        INT_OFFSET = 12L;\n+        LONG_OFFSET = 16L;\n+      }\n+    }\n","filename":"test\/hotspot\/jtreg\/runtime\/FieldLayout\/OldLayoutCheck.java","additions":24,"deletions":8,"binary":false,"changes":32,"status":"modified"},{"patch":"@@ -0,0 +1,66 @@\n+\/*\n+ * Copyright (c) 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+\/**\n+ * @test CdsDifferentCompactObjectHeaders\n+ * @summary Testing CDS (class data sharing) using opposite compact object header settings.\n+ *          Using different compact bject headers setting for each dump\/load pair.\n+ *          This is a negative test; using compact header setting for loading that\n+ *          is different from compact headers for creating a CDS file\n+ *          should fail when loading.\n+ * @requires vm.cds\n+ * @requires vm.bits == 64\n+ * @library \/test\/lib\n+ * @run driver CdsDifferentCompactObjectHeaders\n+ *\/\n+\n+import jdk.test.lib.cds.CDSTestUtils;\n+import jdk.test.lib.process.OutputAnalyzer;\n+import jdk.test.lib.Platform;\n+\n+public class CdsDifferentCompactObjectHeaders {\n+\n+    public static void main(String[] args) throws Exception {\n+        createAndLoadSharedArchive(true, false);\n+        createAndLoadSharedArchive(false, true);\n+    }\n+\n+    \/\/ Parameters are object alignment expressed in bytes\n+    private static void\n+    createAndLoadSharedArchive(boolean createCompactHeaders, boolean loadCompactHeaders)\n+    throws Exception {\n+        String createCompactHeadersArg = \"-XX:\" + (createCompactHeaders ? \"+\" : \"-\") + \"UseCompactObjectHeaders\";\n+        String loadCompactHeadersArg   = \"-XX:\" + (loadCompactHeaders   ? \"+\" : \"-\") + \"UseCompactObjectHeaders\";\n+        String expectedErrorMsg =\n+            String.format(\n+            \"The shared archive file's UseCompactObjectHeaders setting (%s)\" +\n+            \" does not equal the current UseCompactObjectHeaders setting (%s)\",\n+            createCompactHeaders ? \"enabled\" : \"disabled\",\n+            loadCompactHeaders   ? \"enabled\" : \"disabled\");\n+\n+        CDSTestUtils.createArchiveAndCheck(\"-XX:+UnlockExperimentalVMOptions\", createCompactHeadersArg);\n+\n+        OutputAnalyzer out = CDSTestUtils.runWithArchive(\"-Xlog:cds\", \"-XX:+UnlockExperimentalVMOptions\", loadCompactHeadersArg);\n+        CDSTestUtils.checkExecExpectError(out, 1, expectedErrorMsg);\n+    }\n+}\n","filename":"test\/hotspot\/jtreg\/runtime\/cds\/CdsDifferentCompactObjectHeaders.java","additions":66,"deletions":0,"binary":false,"changes":66,"status":"added"},{"patch":"@@ -166,0 +166,1 @@\n+                      \"-XX:+UnlockExperimentalVMOptions\", \"-XX:-UseCompactObjectHeaders\",\n@@ -179,0 +180,1 @@\n+                                      \"-XX:+UnlockExperimentalVMOptions\", \"-XX:-UseCompactObjectHeaders\",\n","filename":"test\/hotspot\/jtreg\/runtime\/cds\/appcds\/TestCombinedCompressedFlags.java","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -59,0 +59,1 @@\n+         String compactHeaders = \"-XX:\" + (zGenerational.equals(\"-XX:+ZGenerational\") ? \"+\" : \"-\") + \"UseCompactObjectHeaders\";\n@@ -66,0 +67,2 @@\n+                                        \"-XX:+UnlockExperimentalVMOptions\",\n+                                        compactHeaders,\n@@ -75,0 +78,2 @@\n+                         \"-XX:+UnlockExperimentalVMOptions\",\n+                         compactHeaders,\n@@ -86,0 +91,2 @@\n+                         \"-XX:+UnlockExperimentalVMOptions\",\n+                         compactHeaders,\n@@ -98,0 +105,2 @@\n+                         \"-XX:+UnlockExperimentalVMOptions\",\n+                         compactHeaders,\n@@ -110,0 +119,2 @@\n+                         \"-XX:+UnlockExperimentalVMOptions\",\n+                         compactHeaders,\n@@ -121,0 +132,2 @@\n+                         \"-XX:+UnlockExperimentalVMOptions\",\n+                         compactHeaders,\n@@ -133,0 +146,2 @@\n+                         \"-XX:+UnlockExperimentalVMOptions\",\n+                         compactHeaders,\n@@ -146,0 +161,2 @@\n+                         \"-XX:+UnlockExperimentalVMOptions\",\n+                         compactHeaders,\n@@ -155,0 +172,2 @@\n+                         \"-XX:+UnlockExperimentalVMOptions\",\n+                         compactHeaders,\n","filename":"test\/hotspot\/jtreg\/runtime\/cds\/appcds\/TestZGCWithCDS.java","additions":19,"deletions":0,"binary":false,"changes":19,"status":"modified"},{"patch":"@@ -170,1 +170,1 @@\n-                                                \"-XX:+UseZGC\", zGenerational, \"-XX:ZCollectionInterval=0.01\",\n+                                                \"-XX:+UseZGC\", zGenerational, \"-XX:ZCollectionInterval=0.01\", \"-XX:+UnlockExperimentalVMOptions\", \"-XX:-UseCompactObjectHeaders\",\n","filename":"test\/hotspot\/jtreg\/runtime\/cds\/appcds\/loaderConstraints\/DynamicLoaderConstraintsTest.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -304,0 +304,1 @@\n+    private static final boolean COMPACT_HEADERS = Platform.is64bit() && WhiteBox.getWhiteBox().getBooleanVMFlag(\"UseCompactObjectHeaders\");\n@@ -377,0 +378,10 @@\n+    private static long expectedSmallObjSize() {\n+        long size;\n+        if (!Platform.is64bit() || COMPACT_HEADERS) {\n+            size = 8;\n+        } else {\n+            size = 16;\n+        }\n+        return roundUp(size, OBJ_ALIGN);\n+    }\n+\n@@ -378,1 +389,1 @@\n-        long expected = roundUp(Platform.is64bit() ? 16 : 8, OBJ_ALIGN);\n+        long expected = expectedSmallObjSize();\n@@ -385,1 +396,1 @@\n-        long expected = roundUp(Platform.is64bit() ? 16 : 8, OBJ_ALIGN);\n+        long expected = expectedSmallObjSize();\n@@ -395,1 +406,1 @@\n-        long expected = roundUp(Platform.is64bit() ? 16 : 8, OBJ_ALIGN);\n+        long expected = expectedSmallObjSize();\n","filename":"test\/jdk\/java\/lang\/instrument\/GetObjectSizeIntrinsicsTest.java","additions":14,"deletions":3,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -673,1 +673,5 @@\n-                \"CreateCoredumpOnCrash\"\n+                \"CreateCoredumpOnCrash\",\n+                \/\/ experimental features unlocking flag does not affect behavior\n+                \"UnlockExperimentalVMOptions\",\n+                \/\/ all compact headers settings should run flagless tests\n+                \"UseCompactObjectHeaders\"\n","filename":"test\/jtreg-ext\/requires\/VMProps.java","additions":5,"deletions":1,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -57,0 +57,1 @@\n+    public volatile Object lockObject3Inflated;\n@@ -65,0 +66,11 @@\n+        lockObject3Inflated = new Object();\n+\n+        \/\/ Inflate the lock to use an ObjectMonitor\n+        try {\n+          synchronized (lockObject3Inflated) {\n+            lockObject3Inflated.wait(1);\n+          }\n+        } catch (InterruptedException e) {\n+          throw new RuntimeException(e);\n+        }\n+\n@@ -71,1 +83,1 @@\n-    public void testSimpleLockUnlock() {\n+    public void testBasicSimpleLockUnlockLocal() {\n@@ -81,0 +93,34 @@\n+    \/** Perform a synchronized on an object within a loop. *\/\n+    @Benchmark\n+    public void testBasicSimpleLockUnlock() {\n+        for (int i = 0; i < innerCount; i++) {\n+            synchronized (lockObject1) {\n+                dummyInt1++;\n+                dummyInt2++;\n+            }\n+        }\n+    }\n+\n+    \/** Perform a synchronized on a local object within a loop. *\/\n+    @Benchmark\n+    public void testInflatedSimpleLockUnlockLocal() {\n+        Object localObject = lockObject3Inflated;\n+        for (int i = 0; i < innerCount; i++) {\n+            synchronized (localObject) {\n+                dummyInt1++;\n+                dummyInt2++;\n+            }\n+        }\n+    }\n+\n+    \/** Perform a synchronized on an object within a loop. *\/\n+    @Benchmark\n+    public void testInflatedSimpleLockUnlock() {\n+        for (int i = 0; i < innerCount; i++) {\n+            synchronized (lockObject3Inflated) {\n+                dummyInt1++;\n+                dummyInt2++;\n+            }\n+        }\n+    }\n+\n@@ -83,1 +129,1 @@\n-    public void testRecursiveLockUnlock() {\n+    public void testBasicRecursiveLockUnlockLocal() {\n@@ -95,0 +141,13 @@\n+    \/** Perform a recursive synchronized on an object within a loop. *\/\n+    @Benchmark\n+    public void testBasicRecursiveLockUnlock() {\n+        for (int i = 0; i < innerCount; i++) {\n+            synchronized (lockObject1) {\n+                synchronized (lockObject1) {\n+                    dummyInt1++;\n+                    dummyInt2++;\n+                }\n+            }\n+        }\n+    }\n+\n@@ -97,1 +156,1 @@\n-    public void testSerialLockUnlock() {\n+    public void testBasicSerialLockUnlockLocal() {\n@@ -109,0 +168,94 @@\n+  \/** Perform two synchronized after each other on the same object. *\/\n+    @Benchmark\n+    public void testBasicSerialLockUnlock() {\n+        for (int i = 0; i < innerCount; i++) {\n+            synchronized (lockObject1) {\n+                dummyInt1++;\n+            }\n+            synchronized (lockObject1) {\n+                dummyInt2++;\n+            }\n+        }\n+    }\n+\n+    \/** Perform two synchronized after each other on the same local object. *\/\n+    @Benchmark\n+    public void testInflatedSerialLockUnlockLocal() {\n+        Object localObject = lockObject3Inflated;\n+        for (int i = 0; i < innerCount; i++) {\n+            synchronized (localObject) {\n+                dummyInt1++;\n+            }\n+            synchronized (localObject) {\n+                dummyInt2++;\n+            }\n+        }\n+    }\n+\n+  \/** Perform two synchronized after each other on the same object. *\/\n+    @Benchmark\n+    public void testInflatedSerialLockUnlock() {\n+        for (int i = 0; i < innerCount; i++) {\n+            synchronized (lockObject3Inflated) {\n+                dummyInt1++;\n+            }\n+            synchronized (lockObject3Inflated) {\n+                dummyInt2++;\n+            }\n+        }\n+    }\n+\n+      \/** Perform a recursive-only synchronized on a local object within a loop. *\/\n+    @Benchmark\n+    public void testInflatedRecursiveOnlyLockUnlockLocal() {\n+        Object localObject = lockObject3Inflated;\n+        synchronized (localObject) {\n+            for (int i = 0; i < innerCount; i++) {\n+                synchronized (localObject) {\n+                    dummyInt1++;\n+                    dummyInt2++;\n+                }\n+            }\n+        }\n+    }\n+\n+    \/** Perform a recursive-only synchronized on an object within a loop. *\/\n+    @Benchmark\n+    public void testInflatedRecursiveOnlyLockUnlock() {\n+        synchronized (lockObject3Inflated) {\n+            for (int i = 0; i < innerCount; i++) {\n+                synchronized (lockObject3Inflated) {\n+                    dummyInt1++;\n+                    dummyInt2++;\n+                }\n+            }\n+        }\n+    }\n+\n+    \/** Perform a recursive-only synchronized on a local object within a loop. *\/\n+    @Benchmark\n+    public void testInflatedRecursiveLockUnlockLocal() {\n+        Object localObject = lockObject3Inflated;\n+        for (int i = 0; i < innerCount; i++) {\n+            synchronized (localObject) {\n+                synchronized (localObject) {\n+                    dummyInt1++;\n+                    dummyInt2++;\n+                }\n+            }\n+        }\n+    }\n+\n+    \/** Perform a recursive-only synchronized on an object within a loop. *\/\n+    @Benchmark\n+    public void testInflatedRecursiveLockUnlock() {\n+        for (int i = 0; i < innerCount; i++) {\n+            synchronized (lockObject3Inflated) {\n+                synchronized (lockObject3Inflated) {\n+                    dummyInt1++;\n+                    dummyInt2++;\n+                }\n+            }\n+        }\n+    }\n+\n","filename":"test\/micro\/org\/openjdk\/bench\/vm\/lang\/LockUnlock.java","additions":156,"deletions":3,"binary":false,"changes":159,"status":"modified"}]}