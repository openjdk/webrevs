{"files":[{"patch":"@@ -5802,0 +5802,3 @@\n+opclass memory_noindex(indirect,\n+                       indOffI1, indOffL1,indOffI2, indOffL2, indOffI4, indOffL4, indOffI8, indOffL8,\n+                       indirectN, indOffIN, indOffLN, indirectX2P, indOffX2P);\n@@ -6739,1 +6742,1 @@\n-instruct loadNKlassCompactHeaders(iRegNNoSp dst, memory4 mem)\n+instruct loadNKlassCompactHeaders(iRegNNoSp dst, memory_noindex mem)\n@@ -6747,1 +6750,1 @@\n-    \"lsrw  $dst, $dst, markWord::klass_shift_at_offset\"\n+    \"lsrw  $dst, $dst, markWord::klass_shift\"\n@@ -6750,4 +6753,5 @@\n-    \/\/ inlined aarch64_enc_ldrw\n-    loadStore(masm, &MacroAssembler::ldrw, $dst$$Register, $mem->opcode(),\n-              as_Register($mem$$base), $mem$$index, $mem$$scale, $mem$$disp, 4);\n-    __ lsrw($dst$$Register, $dst$$Register, markWord::klass_shift_at_offset);\n+    assert($mem$$index$$Register == noreg, \"must not have indexed address\");\n+    \/\/ The incoming address is pointing into obj-start + klass_offset_in_bytes. We need to extract\n+    \/\/ obj-start, so that we can load from the object's mark-word instead.\n+    __ ldrw($dst$$Register, Address($mem$$base$$Register, $mem$$disp - Type::klass_offset()));\n+    __ lsrw($dst$$Register, $dst$$Register, markWord::klass_shift);\n","filename":"src\/hotspot\/cpu\/aarch64\/aarch64.ad","additions":10,"deletions":6,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -5030,2 +5030,2 @@\n-  ldr(dst, Address(src, oopDesc::mark_offset_in_bytes()));\n-  lsr(dst, dst, markWord::klass_shift);\n+  ldrw(dst, Address(src, oopDesc::mark_offset_in_bytes()));\n+  lsrw(dst, dst, markWord::klass_shift);\n@@ -5824,0 +5824,1 @@\n+  Register cnt2 = tmp2;  \/\/ cnt2 only used in array length compare\n@@ -5826,1 +5827,0 @@\n-  int klass_offset  = arrayOopDesc::klass_offset_in_bytes();\n@@ -5830,10 +5830,0 @@\n-  \/\/ When the length offset is not aligned to 8 bytes,\n-  \/\/ then we align it down. This is valid because the new\n-  \/\/ offset will always be the klass which is the same\n-  \/\/ for type arrays.\n-  int start_offset = align_down(length_offset, BytesPerWord);\n-  int extra_length = base_offset - start_offset;\n-  assert(start_offset == length_offset || start_offset == klass_offset,\n-         \"start offset must be 8-byte-aligned or be the klass offset\");\n-  assert(base_offset != start_offset, \"must include the length field\");\n-  extra_length = extra_length \/ elem_size; \/\/ We count in elements, not bytes.\n@@ -5873,4 +5863,5 @@\n-    \/\/ Increase loop counter by diff between base- and actual start-offset.\n-    addw(cnt1, cnt1, extra_length);\n-    lea(a1, Address(a1, start_offset));\n-    lea(a2, Address(a2, start_offset));\n+    ldrw(cnt2, Address(a2, length_offset));\n+    eorw(tmp5, cnt1, cnt2);\n+    cbnzw(tmp5, DONE);\n+    lea(a1, Address(a1, base_offset));\n+    lea(a2, Address(a2, base_offset));\n@@ -5939,3 +5930,1 @@\n-    \/\/ Increase loop counter by diff between base- and actual start-offset.\n-    addw(cnt1, cnt1, extra_length);\n-\n+    ldrw(cnt2, Address(a2, length_offset));\n@@ -5946,1 +5935,1 @@\n-    ldr(tmp3, Address(pre(a1, start_offset)));\n+    ldr(tmp3, Address(pre(a1, base_offset)));\n@@ -5949,1 +5938,1 @@\n-    ldr(tmp4, Address(pre(a2, start_offset)));\n+    ldr(tmp4, Address(pre(a2, base_offset)));\n@@ -5951,0 +5940,2 @@\n+    cmp(cnt2, cnt1);\n+    br(NE, DONE);\n@@ -5982,1 +5973,3 @@\n-    ldr(tmp4, Address(pre(a2, start_offset)));\n+    ldr(tmp4, Address(pre(a2, base_offset)));\n+    cmp(cnt2, cnt1);\n+    br(NE, DONE);\n@@ -6003,0 +5996,3 @@\n+    cmp(cnt2, cnt1);\n+    br(NE, DONE);\n+    cbz(cnt1, SAME);\n@@ -6004,2 +6000,2 @@\n-    ldr(tmp3, Address(a1, start_offset));\n-    ldr(tmp4, Address(a2, start_offset));\n+    ldr(tmp3, Address(a1, base_offset));\n+    ldr(tmp4, Address(a2, base_offset));\n","filename":"src\/hotspot\/cpu\/aarch64\/macroAssembler_aarch64.cpp","additions":21,"deletions":25,"binary":false,"changes":46,"status":"modified"},{"patch":"@@ -3632,1 +3632,0 @@\n-    assert(is_aligned(header_size, BytesPerLong), \"oop header size must be 8-byte-aligned\");\n","filename":"src\/hotspot\/cpu\/aarch64\/templateTable_aarch64.cpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -4750,2 +4750,3 @@\n-    __ lwu(as_Register($dst$$reg), Address(as_Register($mem$$base), $mem$$disp));\n-    __ srli(as_Register($dst$$reg), as_Register($dst$$reg), (unsigned) markWord::klass_shift_at_offset);\n+    Unimplemented();\n+    \/\/ __ lwu(as_Register($dst$$reg), Address(as_Register($mem$$base), $mem$$disp));\n+    \/\/ __ srli(as_Register($dst$$reg), as_Register($dst$$reg), (unsigned) markWord::klass_shift_at_offset);\n","filename":"src\/hotspot\/cpu\/riscv\/riscv.ad","additions":3,"deletions":2,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -5139,2 +5139,2 @@\n-  movq(dst, Address(src, oopDesc::mark_offset_in_bytes()));\n-  shrq(dst, markWord::klass_shift);\n+  movl(dst, Address(src, oopDesc::mark_offset_in_bytes()));\n+  shrl(dst, markWord::klass_shift);\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -60,0 +60,5 @@\n+  if (UseCompactObjectHeaders) {\n+    \/\/ Don't generate anything else and always take the slow-path for now.\n+    return;\n+  }\n+\n","filename":"src\/hotspot\/cpu\/x86\/sharedRuntime_x86.cpp","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -3571,2 +3571,1 @@\n-      assert(is_aligned(oopDesc::base_offset_in_bytes(), BytesPerLong), \"oop base offset must be 8-byte-aligned\");\n-      __ decrement(rdx, oopDesc::base_offset_in_bytes());\n+      __ decrement(rdx, align_up(oopDesc::base_offset_in_bytes(), BytesPerLong));\n","filename":"src\/hotspot\/cpu\/x86\/templateTable_x86.cpp","additions":1,"deletions":2,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -4396,1 +4396,1 @@\n-    \"shrl    $dst, markWord::klass_shift_at_offset\"\n+    \"shrl    $dst, markWord::klass_shift\"\n@@ -4399,0 +4399,4 @@\n+    \/\/ The incoming address is pointing into obj-start + Type::klass_offset(). We need to extract\n+    \/\/ obj-start, so that we can load from the object's mark-word instead.\n+    Register d = $dst$$Register;\n+    Address  s = ($mem$$Address).plus_disp(-Type::klass_offset());\n@@ -4400,5 +4404,4 @@\n-      __ eshrl($dst$$Register, $mem$$Address, markWord::klass_shift_at_offset, false);\n-    }\n-    else {\n-      __ movl($dst$$Register, $mem$$Address);\n-      __ shrl($dst$$Register, markWord::klass_shift_at_offset);\n+      __ eshrl(d, s, markWord::klass_shift, false);\n+    } else {\n+      __ movl(d, s);\n+      __ shrl(d, markWord::klass_shift);\n","filename":"src\/hotspot\/cpu\/x86\/x86_64.ad","additions":9,"deletions":6,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -1255,1 +1255,3 @@\n-        byte_size = source_oop->size() * BytesPerWord;\n+        size_t old_size = source_oop->size();\n+        size_t new_size = source_oop->copy_size_cds(old_size, source_oop->mark());\n+        byte_size = new_size * BytesPerWord;\n","filename":"src\/hotspot\/share\/cds\/archiveBuilder.cpp","additions":3,"deletions":1,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -425,1 +425,3 @@\n-  size_t byte_size = src_obj->size() * HeapWordSize;\n+  size_t old_size = src_obj->size();\n+  size_t new_size = src_obj->copy_size_cds(old_size, src_obj->mark());\n+  size_t byte_size = new_size * HeapWordSize;\n@@ -446,1 +448,1 @@\n-  memcpy(to, from, byte_size);\n+  memcpy(to, from, old_size * HeapWordSize);\n@@ -582,0 +584,1 @@\n+    assert(fake_oop->mark().narrow_klass() != 0, \"must not be null\");\n@@ -594,1 +597,7 @@\n-      fake_oop->set_mark(markWord::prototype().set_narrow_klass(nk).copy_set_hash(src_hash));\n+      markWord m = markWord::prototype().set_narrow_klass(nk);\n+      m = m.copy_hashctrl_from(src_obj->mark());\n+      fake_oop->set_mark(m);\n+      if (m.is_hashed_not_expanded()) {\n+        fake_oop->initialize_hash_if_necessary(src_obj, src_klass, m);\n+      }\n+      assert(!fake_oop->mark().is_not_hashed_expanded() && !fake_oop->mark().is_hashed_not_expanded(), \"must not be not-hashed-moved and not be hashed-not-moved\");\n@@ -597,0 +606,2 @@\n+      DEBUG_ONLY(intptr_t archived_hash = fake_oop->identity_hash());\n+      assert(src_hash == archived_hash, \"Different hash codes: original \" INTPTR_FORMAT \", archived \" INTPTR_FORMAT, src_hash, archived_hash);\n@@ -599,3 +610,0 @@\n-\n-    DEBUG_ONLY(intptr_t archived_hash = fake_oop->identity_hash());\n-    assert(src_hash == archived_hash, \"Different hash codes: original \" INTPTR_FORMAT \", archived \" INTPTR_FORMAT, src_hash, archived_hash);\n","filename":"src\/hotspot\/share\/cds\/archiveHeapWriter.cpp","additions":14,"deletions":6,"binary":false,"changes":20,"status":"modified"},{"patch":"@@ -415,1 +415,1 @@\n-      oop m = java_lang_Class::create_basic_type_mirror(type2name(bt), bt, CHECK);\n+      oop m = java_lang_Class::create_basic_type_mirror(type2name(bt), bt, true, CHECK);\n@@ -583,0 +583,1 @@\n+  assert(!UseCompactObjectHeaders || scratch_m->mark().is_not_hashed_expanded(), \"scratch mirror must have not-hashed-expanded state\");\n@@ -584,0 +585,1 @@\n+    intptr_t orig_mark = orig_mirror->mark().value();\n@@ -586,2 +588,17 @@\n-      narrowKlass nk = CompressedKlassPointers::encode(orig_mirror->klass());\n-      scratch_m->set_mark(markWord::prototype().set_narrow_klass(nk).copy_set_hash(src_hash));\n+      \/\/ We leave the cases not_hashed\/not_hashed_expanded as they are.\n+      assert(orig_mirror->mark().is_hashed_not_expanded() || orig_mirror->mark().is_hashed_expanded(), \"must be hashed\");\n+      Klass* orig_klass = orig_mirror->klass();\n+      narrowKlass nk = CompressedKlassPointers::encode(orig_klass);\n+      markWord mark = markWord::prototype().set_narrow_klass(nk);\n+      mark = mark.copy_hashctrl_from(orig_mirror->mark());\n+      if (mark.is_hashed_not_expanded()) {\n+        scratch_m->initialize_hash_if_necessary(orig_mirror, orig_klass, mark);\n+      } else {\n+        assert(mark.is_hashed_expanded(), \"must be hashed & moved\");\n+        int offset = orig_klass->hash_offset_in_bytes(orig_mirror, mark);\n+        assert(offset >= 8, \"hash offset must not be in header\");\n+        scratch_m->int_field_put(offset, (jint) src_hash);\n+        scratch_m->set_mark(mark);\n+      }\n+      assert(scratch_m->mark().is_hashed_expanded(), \"must be hashed & moved\");\n+      assert(scratch_m->mark().is_not_hashed_expanded() || scratch_m->mark().is_hashed_expanded(), \"must be not hashed and expanded\");\n@@ -590,0 +607,2 @@\n+      DEBUG_ONLY(intptr_t archived_hash = scratch_m->identity_hash());\n+      assert(src_hash == archived_hash, \"Different hash codes: original \" INTPTR_FORMAT \", archived \" INTPTR_FORMAT, src_hash, archived_hash);\n@@ -592,3 +611,0 @@\n-\n-    DEBUG_ONLY(intptr_t archived_hash = scratch_m->identity_hash());\n-    assert(src_hash == archived_hash, \"Different hash codes: original \" INTPTR_FORMAT \", archived \" INTPTR_FORMAT, src_hash, archived_hash);\n","filename":"src\/hotspot\/share\/cds\/heapShared.cpp","additions":22,"deletions":6,"binary":false,"changes":28,"status":"modified"},{"patch":"@@ -4894,0 +4894,4 @@\n+int ClassFileParser::hash_offset() const {\n+  return _field_info->_hash_offset;\n+}\n+\n","filename":"src\/hotspot\/share\/classfile\/classFileParser.cpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -213,0 +213,17 @@\n+\/\/ Finds a slot for the identity hash-code.\n+\/\/ Same basic algorithm as above add() method, but simplified\n+\/\/ and does not actually insert the field.\n+int FieldLayout::find_hash_offset() {\n+  LayoutRawBlock* start = this->_start;\n+  LayoutRawBlock* last = last_block();\n+  LayoutRawBlock* cursor = start;\n+  while (cursor != last) {\n+    assert(cursor != nullptr, \"Sanity check\");\n+    if (cursor->kind() == LayoutRawBlock::EMPTY && cursor->fit(4, 1)) {\n+      break;\n+    }\n+    cursor = cursor->next_block();\n+  }\n+  return cursor->offset();\n+}\n+\n@@ -700,0 +717,3 @@\n+  if (UseCompactObjectHeaders) {\n+    _info->_hash_offset   = _layout->find_hash_offset();\n+  }\n","filename":"src\/hotspot\/share\/classfile\/fieldLayoutBuilder.cpp","additions":20,"deletions":0,"binary":false,"changes":20,"status":"modified"},{"patch":"@@ -188,0 +188,1 @@\n+  int find_hash_offset();\n","filename":"src\/hotspot\/share\/classfile\/fieldLayoutBuilder.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -1060,1 +1060,1 @@\n-  oop mirror_oop = InstanceMirrorKlass::cast(vmClasses::Class_klass())->allocate_instance(k, CHECK);\n+  oop mirror_oop = InstanceMirrorKlass::cast(vmClasses::Class_klass())->allocate_instance(k, is_scratch, CHECK);\n@@ -1361,1 +1361,1 @@\n-oop java_lang_Class::create_basic_type_mirror(const char* basic_type_name, BasicType type, TRAPS) {\n+oop java_lang_Class::create_basic_type_mirror(const char* basic_type_name, BasicType type, bool is_scratch, TRAPS) {\n@@ -1363,1 +1363,1 @@\n-  oop java_class = InstanceMirrorKlass::cast(vmClasses::Class_klass())->allocate_instance(nullptr, CHECK_NULL);\n+  oop java_class = InstanceMirrorKlass::cast(vmClasses::Class_klass())->allocate_instance(nullptr, is_scratch, CHECK_NULL);\n","filename":"src\/hotspot\/share\/classfile\/javaClasses.cpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -36,1 +36,0 @@\n-#include \"gc\/shared\/fullGCForwarding.hpp\"\n@@ -249,2 +248,0 @@\n-\n-  FullGCForwarding::initialize_flags(heap_reserved_size_bytes());\n","filename":"src\/hotspot\/share\/gc\/g1\/g1Arguments.cpp","additions":0,"deletions":3,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -214,0 +214,2 @@\n+  FullGCForwarding::begin();\n+\n@@ -226,0 +228,2 @@\n+  FullGCForwarding::end();\n+\n","filename":"src\/hotspot\/share\/gc\/g1\/g1FullCollector.cpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -478,1 +478,2 @@\n-  const size_t word_sz = old->size_given_klass(klass);\n+  const size_t old_size = old->size_given_mark_and_klass(old_mark, klass);\n+  const size_t word_sz = old->copy_size(old_size, old_mark);\n@@ -516,1 +517,1 @@\n-  Copy::aligned_disjoint_words(cast_from_oop<HeapWord*>(old), obj_ptr, word_sz);\n+  Copy::aligned_disjoint_words(cast_from_oop<HeapWord*>(old), obj_ptr, old_size);\n@@ -533,0 +534,2 @@\n+    obj->initialize_hash_if_necessary(old);\n+\n","filename":"src\/hotspot\/share\/gc\/g1\/g1ParScanThreadState.cpp","additions":5,"deletions":2,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -29,1 +29,0 @@\n-#include \"gc\/shared\/fullGCForwarding.hpp\"\n@@ -97,2 +96,0 @@\n-\n-  FullGCForwarding::initialize_flags(heap_reserved_size_bytes());\n","filename":"src\/hotspot\/share\/gc\/parallel\/parallelArguments.cpp","additions":0,"deletions":3,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -32,0 +32,1 @@\n+#include \"gc\/parallel\/psParallelCompactNew.inline.hpp\"\n@@ -123,2 +124,8 @@\n-  if (!PSParallelCompact::initialize_aux_data()) {\n-    return JNI_ENOMEM;\n+  if (UseCompactObjectHeaders) {\n+    if (!PSParallelCompactNew::initialize_aux_data()) {\n+      return JNI_ENOMEM;\n+    }\n+  } else {\n+    if (!PSParallelCompact::initialize_aux_data()) {\n+      return JNI_ENOMEM;\n+    }\n@@ -187,1 +194,5 @@\n-  PSParallelCompact::post_initialize();\n+  if (UseCompactObjectHeaders) {\n+    PSParallelCompactNew::post_initialize();\n+  } else {\n+    PSParallelCompact::post_initialize();\n+  }\n@@ -394,1 +405,5 @@\n-  PSParallelCompact::invoke(clear_all_soft_refs);\n+  if (UseCompactObjectHeaders) {\n+    PSParallelCompactNew::invoke(clear_all_soft_refs, false \/* serial *\/);\n+  } else {\n+    PSParallelCompact::invoke(clear_all_soft_refs);\n+  }\n@@ -432,1 +447,5 @@\n-    PSParallelCompact::invoke(clear_all_soft_refs);\n+    if (UseCompactObjectHeaders) {\n+      PSParallelCompactNew::invoke(clear_all_soft_refs, false \/* serial *\/);\n+    } else {\n+      PSParallelCompact::invoke(clear_all_soft_refs);\n+    }\n@@ -443,0 +462,9 @@\n+  if (UseCompactObjectHeaders) {\n+    PSParallelCompactNew::invoke(true \/* clear_soft_refs *\/, true \/* serial *\/);\n+  }\n+\n+  result = expand_heap_and_allocate(size, is_tlab);\n+  if (result != nullptr) {\n+    return result;\n+  }\n+\n@@ -538,1 +566,5 @@\n-  PSParallelCompact::invoke(clear_soft_refs);\n+  if (UseCompactObjectHeaders) {\n+    PSParallelCompactNew::invoke(clear_soft_refs, false \/* serial *\/);\n+  } else {\n+    PSParallelCompact::invoke(clear_soft_refs);\n+  }\n@@ -681,1 +713,5 @@\n-  PSParallelCompact::print_on(st);\n+  if (UseCompactObjectHeaders) {\n+    PSParallelCompactNew::print_on(st);\n+  } else {\n+    PSParallelCompact::print_on(st);\n+  }\n@@ -691,1 +727,5 @@\n-  log_debug(gc, heap, exit)(\"Accumulated old generation GC time %3.7f secs\", PSParallelCompact::accumulated_time()->seconds());\n+  if (UseCompactObjectHeaders) {\n+    log_debug(gc, heap, exit)(\"Accumulated old generation GC time %3.7f secs\", PSParallelCompactNew::accumulated_time()->seconds());\n+  } else {\n+    log_debug(gc, heap, exit)(\"Accumulated old generation GC time %3.7f secs\", PSParallelCompact::accumulated_time()->seconds());\n+  }\n","filename":"src\/hotspot\/share\/gc\/parallel\/parallelScavengeHeap.cpp","additions":48,"deletions":8,"binary":false,"changes":56,"status":"modified"},{"patch":"@@ -552,1 +552,1 @@\n-class PCAdjustPointerClosure: public BasicOopIterateClosure {\n+class PCAdjustPointerClosureNew: public BasicOopIterateClosure {\n@@ -563,1 +563,1 @@\n-static PCAdjustPointerClosure pc_adjust_pointer_closure;\n+static PCAdjustPointerClosureNew pc_adjust_pointer_closure;\n@@ -1060,0 +1060,2 @@\n+    FullGCForwarding::begin();\n+\n@@ -1066,0 +1068,2 @@\n+    FullGCForwarding::end();\n+\n","filename":"src\/hotspot\/share\/gc\/parallel\/psParallelCompact.cpp","additions":6,"deletions":2,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -0,0 +1,1147 @@\n+\/*\n+ * Copyright (c) 2005, 2025, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"classfile\/classLoaderDataGraph.hpp\"\n+#include \"classfile\/symbolTable.hpp\"\n+#include \"classfile\/systemDictionary.hpp\"\n+#include \"code\/codeCache.hpp\"\n+#include \"compiler\/oopMap.hpp\"\n+#include \"gc\/parallel\/objectStartArray.inline.hpp\"\n+#include \"gc\/parallel\/parallelArguments.hpp\"\n+#include \"gc\/parallel\/parallelScavengeHeap.inline.hpp\"\n+#include \"gc\/parallel\/parMarkBitMap.inline.hpp\"\n+#include \"gc\/parallel\/psAdaptiveSizePolicy.hpp\"\n+#include \"gc\/parallel\/psCompactionManagerNew.inline.hpp\"\n+#include \"gc\/parallel\/psOldGen.hpp\"\n+#include \"gc\/parallel\/psParallelCompactNew.inline.hpp\"\n+#include \"gc\/parallel\/psPromotionManager.inline.hpp\"\n+#include \"gc\/parallel\/psScavenge.hpp\"\n+#include \"gc\/parallel\/psYoungGen.hpp\"\n+#include \"gc\/shared\/classUnloadingContext.hpp\"\n+#include \"gc\/shared\/fullGCForwarding.inline.hpp\"\n+#include \"gc\/shared\/gcCause.hpp\"\n+#include \"gc\/shared\/gcHeapSummary.hpp\"\n+#include \"gc\/shared\/gcId.hpp\"\n+#include \"gc\/shared\/gcLocker.hpp\"\n+#include \"gc\/shared\/gcTimer.hpp\"\n+#include \"gc\/shared\/gcTrace.hpp\"\n+#include \"gc\/shared\/gcTraceTime.inline.hpp\"\n+#include \"gc\/shared\/gcVMOperations.hpp\"\n+#include \"gc\/shared\/isGCActiveMark.hpp\"\n+#include \"gc\/shared\/oopStorageSetParState.inline.hpp\"\n+#include \"gc\/shared\/preservedMarks.inline.hpp\"\n+#include \"gc\/shared\/referencePolicy.hpp\"\n+#include \"gc\/shared\/referenceProcessor.hpp\"\n+#include \"gc\/shared\/referenceProcessorPhaseTimes.hpp\"\n+#include \"gc\/shared\/spaceDecorator.hpp\"\n+#include \"gc\/shared\/strongRootsScope.hpp\"\n+#include \"gc\/shared\/taskTerminator.hpp\"\n+#include \"gc\/shared\/weakProcessor.inline.hpp\"\n+#include \"gc\/shared\/workerPolicy.hpp\"\n+#include \"gc\/shared\/workerThread.hpp\"\n+#include \"gc\/shared\/workerUtils.hpp\"\n+#include \"logging\/log.hpp\"\n+#include \"memory\/iterator.inline.hpp\"\n+#include \"memory\/memoryReserver.hpp\"\n+#include \"memory\/metaspaceUtils.hpp\"\n+#include \"memory\/resourceArea.hpp\"\n+#include \"memory\/universe.hpp\"\n+#include \"nmt\/memTracker.hpp\"\n+#include \"oops\/methodData.hpp\"\n+#include \"runtime\/java.hpp\"\n+#include \"runtime\/safepoint.hpp\"\n+#include \"runtime\/threads.hpp\"\n+#include \"runtime\/vmThread.hpp\"\n+#include \"services\/memoryService.hpp\"\n+#include \"utilities\/align.hpp\"\n+#include \"utilities\/debug.hpp\"\n+#include \"utilities\/events.hpp\"\n+#include \"utilities\/formatBuffer.hpp\"\n+#include \"utilities\/macros.hpp\"\n+#if INCLUDE_JVMCI\n+#include \"jvmci\/jvmci.hpp\"\n+#endif\n+\n+SpaceInfoNew PSParallelCompactNew::_space_info[PSParallelCompactNew::last_space_id];\n+\n+size_t PSParallelCompactNew::_num_regions;\n+PCRegionData* PSParallelCompactNew::_region_data_array;\n+size_t PSParallelCompactNew::_num_regions_serial;\n+PCRegionData* PSParallelCompactNew::_region_data_array_serial;\n+PCRegionData** PSParallelCompactNew::_per_worker_region_data;\n+bool PSParallelCompactNew::_serial = false;\n+\n+SpanSubjectToDiscoveryClosure PSParallelCompactNew::_span_based_discoverer;\n+ReferenceProcessor* PSParallelCompactNew::_ref_processor = nullptr;\n+\n+void PSParallelCompactNew::print_on(outputStream* st) {\n+  _mark_bitmap.print_on(st);\n+}\n+\n+STWGCTimer          PSParallelCompactNew::_gc_timer;\n+ParallelOldTracer   PSParallelCompactNew::_gc_tracer;\n+elapsedTimer        PSParallelCompactNew::_accumulated_time;\n+unsigned int        PSParallelCompactNew::_maximum_compaction_gc_num = 0;\n+CollectorCounters*  PSParallelCompactNew::_counters = nullptr;\n+ParMarkBitMap       PSParallelCompactNew::_mark_bitmap;\n+\n+PSParallelCompactNew::IsAliveClosure PSParallelCompactNew::_is_alive_closure;\n+\n+class PCAdjustPointerClosure: public BasicOopIterateClosure {\n+  template <typename T>\n+  void do_oop_work(T* p) { PSParallelCompactNew::adjust_pointer(p); }\n+\n+public:\n+  void do_oop(oop* p) final          { do_oop_work(p); }\n+  void do_oop(narrowOop* p) final    { do_oop_work(p); }\n+\n+  ReferenceIterationMode reference_iteration_mode() final { return DO_FIELDS; }\n+};\n+\n+static PCAdjustPointerClosure pc_adjust_pointer_closure;\n+\n+class IsAliveClosure: public BoolObjectClosure {\n+public:\n+  bool do_object_b(oop p) final;\n+};\n+\n+\n+bool PSParallelCompactNew::IsAliveClosure::do_object_b(oop p) { return mark_bitmap()->is_marked(p); }\n+\n+void PSParallelCompactNew::post_initialize() {\n+  ParallelScavengeHeap* heap = ParallelScavengeHeap::heap();\n+  _span_based_discoverer.set_span(heap->reserved_region());\n+  _ref_processor =\n+    new ReferenceProcessor(&_span_based_discoverer,\n+                           ParallelGCThreads,   \/\/ mt processing degree\n+                           ParallelGCThreads,   \/\/ mt discovery degree\n+                           false,               \/\/ concurrent_discovery\n+                           &_is_alive_closure); \/\/ non-header is alive closure\n+\n+  _counters = new CollectorCounters(\"Parallel full collection pauses\", 1);\n+\n+  \/\/ Initialize static fields in ParCompactionManager.\n+  ParCompactionManagerNew::initialize(mark_bitmap());\n+}\n+\n+bool PSParallelCompactNew::initialize_aux_data() {\n+  ParallelScavengeHeap* heap = ParallelScavengeHeap::heap();\n+  MemRegion mr = heap->reserved_region();\n+  assert(mr.byte_size() != 0, \"heap should be reserved\");\n+\n+  initialize_space_info();\n+\n+  if (!_mark_bitmap.initialize(mr)) {\n+    vm_shutdown_during_initialization(\n+      err_msg(\"Unable to allocate %zuKB bitmaps for parallel \"\n+      \"garbage collection for the requested %zuKB heap.\",\n+      _mark_bitmap.reserved_byte_size()\/K, mr.byte_size()\/K));\n+    return false;\n+  }\n+\n+  return true;\n+}\n+\n+void PSParallelCompactNew::initialize_space_info()\n+{\n+  memset(&_space_info, 0, sizeof(_space_info));\n+\n+  PSYoungGen* young_gen = ParallelScavengeHeap::young_gen();\n+\n+  _space_info[old_space_id].set_space(ParallelScavengeHeap::old_gen()->object_space());\n+  _space_info[eden_space_id].set_space(young_gen->eden_space());\n+  _space_info[from_space_id].set_space(young_gen->from_space());\n+  _space_info[to_space_id].set_space(young_gen->to_space());\n+\n+  _space_info[old_space_id].set_start_array(ParallelScavengeHeap::old_gen()->start_array());\n+}\n+\n+void\n+PSParallelCompactNew::clear_data_covering_space(SpaceId id)\n+{\n+  \/\/ At this point, top is the value before GC, new_top() is the value that will\n+  \/\/ be set at the end of GC.  The marking bitmap is cleared to top; nothing\n+  \/\/ should be marked above top.\n+  MutableSpace* const space = _space_info[id].space();\n+  HeapWord* const bot = space->bottom();\n+  HeapWord* const top = space->top();\n+\n+  _mark_bitmap.clear_range(bot, top);\n+}\n+\n+void PSParallelCompactNew::pre_compact()\n+{\n+  \/\/ Update the from & to space pointers in space_info, since they are swapped\n+  \/\/ at each young gen gc.  Do the update unconditionally (even though a\n+  \/\/ promotion failure does not swap spaces) because an unknown number of young\n+  \/\/ collections will have swapped the spaces an unknown number of times.\n+  GCTraceTime(Debug, gc, phases) tm(\"Pre Compact\", &_gc_timer);\n+  ParallelScavengeHeap* heap = ParallelScavengeHeap::heap();\n+  _space_info[from_space_id].set_space(ParallelScavengeHeap::young_gen()->from_space());\n+  _space_info[to_space_id].set_space(ParallelScavengeHeap::young_gen()->to_space());\n+\n+  \/\/ Increment the invocation count\n+  heap->increment_total_collections(true);\n+\n+  CodeCache::on_gc_marking_cycle_start();\n+\n+  heap->print_heap_before_gc();\n+  heap->trace_heap_before_gc(&_gc_tracer);\n+\n+  \/\/ Fill in TLABs\n+  heap->ensure_parsability(true);  \/\/ retire TLABs\n+\n+  if (VerifyBeforeGC && heap->total_collections() >= VerifyGCStartAt) {\n+    Universe::verify(\"Before GC\");\n+  }\n+\n+  DEBUG_ONLY(mark_bitmap()->verify_clear();)\n+}\n+\n+void PSParallelCompactNew::post_compact()\n+{\n+  GCTraceTime(Info, gc, phases) tm(\"Post Compact\", &_gc_timer);\n+\n+  CodeCache::on_gc_marking_cycle_finish();\n+  CodeCache::arm_all_nmethods();\n+\n+  for (unsigned int id = old_space_id; id < last_space_id; ++id) {\n+    \/\/ Clear the marking bitmap, summary data and split info.\n+    clear_data_covering_space(SpaceId(id));\n+  }\n+\n+  {\n+    PCRegionData* last_live[last_space_id];\n+    for (uint i = old_space_id; i < last_space_id; ++i) {\n+      last_live[i] = nullptr;\n+    }\n+\n+    \/\/ Figure out last region in each space that has live data.\n+    uint space_id = old_space_id;\n+    MutableSpace* space = _space_info[space_id].space();\n+    size_t num_regions = get_num_regions();\n+    PCRegionData* region_data_array = get_region_data_array();\n+    last_live[space_id] = &region_data_array[0];\n+    for (size_t idx = 0; idx < num_regions; idx++) {\n+      PCRegionData* rd = region_data_array + idx;\n+      if(!space->contains(rd->bottom())) {\n+        ++space_id;\n+        assert(space_id < last_space_id, \"invariant\");\n+        space = _space_info[space_id].space();\n+        log_develop_trace(gc, compaction)(\"Last live for space: %u: %zu\", space_id, idx);\n+        last_live[space_id] = rd;\n+      }\n+      assert(space->contains(rd->bottom()), \"next space should contain next region\");\n+      log_develop_trace(gc, compaction)(\"post-compact region: idx: %zu, bottom: \" PTR_FORMAT \", new_top: \" PTR_FORMAT \", end: \" PTR_FORMAT, rd->idx(), p2i(rd->bottom()), p2i(rd->new_top()), p2i(rd->end()));\n+      if (rd->new_top() > rd->bottom()) {\n+        last_live[space_id] = rd;\n+        log_develop_trace(gc, compaction)(\"Bump last live for space: %u\", space_id);\n+      }\n+    }\n+\n+    for (uint i = old_space_id; i < last_space_id; ++i) {\n+      PCRegionData* rd = last_live[i];\n+        log_develop_trace(gc, compaction)(\n+                \"Last live region in space: %u, compaction region, \" PTR_FORMAT \", #%zu: [\" PTR_FORMAT \", \" PTR_FORMAT \"), new_top: \" PTR_FORMAT,\n+                i, p2i(rd), rd->idx(),\n+                p2i(rd->bottom()), p2i(rd->end()), p2i(rd->new_top()));\n+    }\n+\n+    \/\/ Fill all gaps and update the space boundaries.\n+    space_id = old_space_id;\n+    space = _space_info[space_id].space();\n+    size_t total_live = 0;\n+    size_t total_waste = 0;\n+    for (size_t idx = 0; idx < num_regions; idx++) {\n+      PCRegionData* rd = &region_data_array[idx];\n+      PCRegionData* last_live_in_space = last_live[space_id];\n+      assert(last_live_in_space != nullptr, \"last live must not be null\");\n+      if (rd != last_live_in_space) {\n+        if (rd->new_top() < rd->end()) {\n+          ObjectStartArray* sa = start_array(SpaceId(space_id));\n+          if (sa != nullptr) {\n+            sa->update_for_block(rd->new_top(), rd->end());\n+          }\n+          ParallelScavengeHeap::heap()->fill_with_dummy_object(rd->new_top(), rd->end(), false);\n+        }\n+        size_t live = pointer_delta(rd->new_top(), rd->bottom());\n+        size_t waste = pointer_delta(rd->end(), rd->new_top());\n+        total_live += live;\n+        total_waste += waste;\n+        log_develop_trace(gc, compaction)(\n+                \"Live compaction region, #%zu: [\" PTR_FORMAT \", \" PTR_FORMAT \"), new_top: \" PTR_FORMAT \", live: %zu, waste: %zu\",\n+                rd->idx(),\n+                p2i(rd->bottom()), p2i(rd->end()), p2i(rd->new_top()), live, waste);\n+      } else {\n+        \/\/ Update top of space.\n+        space->set_top(rd->new_top());\n+        size_t live = pointer_delta(rd->new_top(), rd->bottom());\n+        total_live += live;\n+        log_develop_trace(gc, compaction)(\n+                \"Live compaction region, #%zu: [\" PTR_FORMAT \", \" PTR_FORMAT \"), new_top: \" PTR_FORMAT \", live: %zu, waste: %zu\",\n+                rd->idx(),\n+                p2i(rd->bottom()), p2i(rd->end()), p2i(rd->new_top()), live, size_t(0));\n+\n+        \/\/ Fast-Forward to next space.\n+        for (; idx < num_regions - 1; idx++) {\n+          rd = &region_data_array[idx + 1];\n+          if (!space->contains(rd->bottom())) {\n+            space_id++;\n+            assert(space_id < last_space_id, \"must be\");\n+            space = _space_info[space_id].space();\n+            assert(space->contains(rd->bottom()), \"space must contain region\");\n+            break;\n+          }\n+        }\n+      }\n+    }\n+    log_develop_debug(gc, compaction)(\"total live: %zu, total waste: %zu, ratio: %f\", total_live, total_waste, ((float)total_waste)\/((float)(total_live + total_waste)));\n+  }\n+  {\n+    FREE_C_HEAP_ARRAY(PCRegionData*, _per_worker_region_data);\n+    FREE_C_HEAP_ARRAY(PCRegionData, _region_data_array);\n+    FREE_C_HEAP_ARRAY(PCRegionData, _region_data_array_serial);\n+  }\n+#ifdef ASSERT\n+  {\n+    mark_bitmap()->verify_clear();\n+  }\n+#endif\n+\n+  ParCompactionManagerNew::flush_all_string_dedup_requests();\n+\n+  MutableSpace* const eden_space = _space_info[eden_space_id].space();\n+  MutableSpace* const from_space = _space_info[from_space_id].space();\n+  MutableSpace* const to_space   = _space_info[to_space_id].space();\n+\n+  ParallelScavengeHeap* heap = ParallelScavengeHeap::heap();\n+  bool eden_empty = eden_space->is_empty();\n+\n+  \/\/ Update heap occupancy information which is used as input to the soft ref\n+  \/\/ clearing policy at the next gc.\n+  Universe::heap()->update_capacity_and_used_at_gc();\n+\n+  bool young_gen_empty = eden_empty && from_space->is_empty() &&\n+    to_space->is_empty();\n+\n+  PSCardTable* ct = heap->card_table();\n+  MemRegion old_mr = ParallelScavengeHeap::old_gen()->committed();\n+  if (young_gen_empty) {\n+    ct->clear_MemRegion(old_mr);\n+  } else {\n+    ct->dirty_MemRegion(old_mr);\n+  }\n+\n+  {\n+    \/\/ Delete metaspaces for unloaded class loaders and clean up loader_data graph\n+    GCTraceTime(Debug, gc, phases) t(\"Purge Class Loader Data\", gc_timer());\n+    ClassLoaderDataGraph::purge(true \/* at_safepoint *\/);\n+    DEBUG_ONLY(MetaspaceUtils::verify();)\n+  }\n+\n+  \/\/ Need to clear claim bits for the next mark.\n+  ClassLoaderDataGraph::clear_claimed_marks();\n+\n+  heap->prune_scavengable_nmethods();\n+\n+#if COMPILER2_OR_JVMCI\n+  DerivedPointerTable::update_pointers();\n+#endif\n+\n+  \/\/ Signal that we have completed a visit to all live objects.\n+  Universe::heap()->record_whole_heap_examined_timestamp();\n+}\n+\n+void PSParallelCompactNew::setup_regions_parallel() {\n+  static const size_t REGION_SIZE_WORDS = (SpaceAlignment \/ HeapWordSize);\n+  size_t num_regions = 0;\n+  for (uint i = old_space_id; i < last_space_id; ++i) {\n+    MutableSpace* const space = _space_info[i].space();\n+    size_t const space_size_words = space->capacity_in_words();\n+    num_regions += align_up(space_size_words, REGION_SIZE_WORDS) \/ REGION_SIZE_WORDS;\n+  }\n+  _region_data_array = NEW_C_HEAP_ARRAY(PCRegionData, num_regions, mtGC);\n+\n+  size_t region_idx = 0;\n+  for (uint i = old_space_id; i < last_space_id; ++i) {\n+    const MutableSpace* space = _space_info[i].space();\n+    HeapWord* addr = space->bottom();\n+    HeapWord* sp_end = space->end();\n+    HeapWord* sp_top = space->top();\n+    while (addr < sp_end) {\n+      HeapWord* end = MIN2(align_up(addr + REGION_SIZE_WORDS, REGION_SIZE_WORDS), space->end());\n+      if (addr < sp_top) {\n+        HeapWord* prev_obj_start = _mark_bitmap.find_obj_beg_reverse(addr, end);\n+        if (prev_obj_start < end) {\n+          HeapWord* prev_obj_end = prev_obj_start + cast_to_oop(prev_obj_start)->size();\n+          if (end < prev_obj_end) {\n+            \/\/ Object crosses region boundary, adjust end to be after object's last word.\n+            end = prev_obj_end;\n+          }\n+        }\n+      }\n+      assert(region_idx < num_regions, \"must not exceed number of regions: region_idx: %zu, num_regions: %zu\", region_idx, num_regions);\n+      HeapWord* top;\n+      if (sp_top < addr) {\n+        top = addr;\n+      } else if (sp_top >= end) {\n+        top = end;\n+      } else {\n+        top = sp_top;\n+      }\n+      assert(ParallelScavengeHeap::heap()->is_in_reserved(addr), \"addr must be in heap: \" PTR_FORMAT, p2i(addr));\n+      new (_region_data_array + region_idx) PCRegionData(region_idx, addr, top, end);\n+      addr = end;\n+      region_idx++;\n+    }\n+  }\n+  _num_regions = region_idx;\n+  log_info(gc)(\"Number of regions: %zu\", _num_regions);\n+}\n+\n+void PSParallelCompactNew::setup_regions_serial() {\n+  _num_regions_serial = last_space_id;\n+  _region_data_array_serial = NEW_C_HEAP_ARRAY(PCRegionData, _num_regions_serial, mtGC);\n+  new (_region_data_array_serial + old_space_id)  PCRegionData(old_space_id, space(old_space_id)->bottom(), space(old_space_id)->top(), space(old_space_id)->end());\n+  new (_region_data_array_serial + eden_space_id) PCRegionData(eden_space_id, space(eden_space_id)->bottom(), space(eden_space_id)->top(), space(eden_space_id)->end());\n+  new (_region_data_array_serial + from_space_id) PCRegionData(from_space_id, space(from_space_id)->bottom(), space(from_space_id)->top(), space(from_space_id)->end());\n+  new (_region_data_array_serial + to_space_id)   PCRegionData(to_space_id, space(to_space_id)->bottom(), space(to_space_id)->top(), space(to_space_id)->end());\n+}\n+\n+bool PSParallelCompactNew::check_maximum_compaction() {\n+\n+  ParallelScavengeHeap* heap = ParallelScavengeHeap::heap();\n+\n+  \/\/ Check System.GC\n+  bool is_max_on_system_gc = UseMaximumCompactionOnSystemGC\n+                          && GCCause::is_user_requested_gc(heap->gc_cause());\n+\n+  \/\/ JVM flags\n+  const uint total_invocations = heap->total_full_collections();\n+  assert(total_invocations >= _maximum_compaction_gc_num, \"sanity\");\n+  const size_t gcs_since_max = total_invocations - _maximum_compaction_gc_num;\n+  const bool is_interval_ended = gcs_since_max > HeapMaximumCompactionInterval;\n+\n+  if (is_max_on_system_gc || is_interval_ended) {\n+    _maximum_compaction_gc_num = total_invocations;\n+    return true;\n+  }\n+\n+  return false;\n+}\n+\n+void PSParallelCompactNew::summary_phase() {\n+  GCTraceTime(Info, gc, phases) tm(\"Summary Phase\", &_gc_timer);\n+\n+  setup_regions_serial();\n+  setup_regions_parallel();\n+\n+#ifndef PRODUCT\n+  for (size_t idx = 0; idx < _num_regions; idx++) {\n+    PCRegionData* rd = &_region_data_array[idx];\n+    log_develop_trace(gc, compaction)(\"Compaction region #%zu: [\" PTR_FORMAT \", \" PTR_FORMAT \")\", rd->idx(), p2i(\n+            rd->bottom()), p2i(rd->end()));\n+  }\n+#endif\n+}\n+\n+\/\/ This method should contain all heap-specific policy for invoking a full\n+\/\/ collection.  invoke_no_policy() will only attempt to compact the heap; it\n+\/\/ will do nothing further.  If we need to bail out for policy reasons, scavenge\n+\/\/ before full gc, or any other specialized behavior, it needs to be added here.\n+\/\/\n+\/\/ Note that this method should only be called from the vm_thread while at a\n+\/\/ safepoint.\n+\/\/\n+\/\/ Note that the all_soft_refs_clear flag in the soft ref policy\n+\/\/ may be true because this method can be called without intervening\n+\/\/ activity.  For example when the heap space is tight and full measure\n+\/\/ are being taken to free space.\n+bool PSParallelCompactNew::invoke(bool clear_all_soft_refs, bool serial) {\n+  assert(SafepointSynchronize::is_at_safepoint(), \"should be at safepoint\");\n+  assert(Thread::current() == (Thread*)VMThread::vm_thread(),\n+         \"should be in vm thread\");\n+\n+  SvcGCMarker sgcm(SvcGCMarker::FULL);\n+  IsSTWGCActiveMark mark;\n+\n+  ParallelScavengeHeap* heap = ParallelScavengeHeap::heap();\n+  clear_all_soft_refs = clear_all_soft_refs\n+                     || heap->soft_ref_policy()->should_clear_all_soft_refs();\n+\n+  return PSParallelCompactNew::invoke_no_policy(clear_all_soft_refs, serial);\n+}\n+\n+\/\/ This method contains no policy. You should probably\n+\/\/ be calling invoke() instead.\n+bool PSParallelCompactNew::invoke_no_policy(bool clear_all_soft_refs, bool serial) {\n+  assert(SafepointSynchronize::is_at_safepoint(), \"must be at a safepoint\");\n+  assert(ref_processor() != nullptr, \"Sanity\");\n+\n+  ParallelScavengeHeap* heap = ParallelScavengeHeap::heap();\n+\n+  GCIdMark gc_id_mark;\n+  _gc_timer.register_gc_start();\n+  _gc_tracer.report_gc_start(heap->gc_cause(), _gc_timer.gc_start());\n+\n+  GCCause::Cause gc_cause = heap->gc_cause();\n+  PSYoungGen* young_gen = ParallelScavengeHeap::young_gen();\n+  PSOldGen* old_gen = ParallelScavengeHeap::old_gen();\n+  PSAdaptiveSizePolicy* size_policy = heap->size_policy();\n+\n+  \/\/ The scope of casr should end after code that can change\n+  \/\/ SoftRefPolicy::_should_clear_all_soft_refs.\n+  ClearedAllSoftRefs casr(clear_all_soft_refs,\n+                          heap->soft_ref_policy());\n+\n+  \/\/ Make sure data structures are sane, make the heap parsable, and do other\n+  \/\/ miscellaneous bookkeeping.\n+  pre_compact();\n+\n+  const PreGenGCValues pre_gc_values = heap->get_pre_gc_values();\n+\n+  {\n+    const uint active_workers =\n+      WorkerPolicy::calc_active_workers(ParallelScavengeHeap::heap()->workers().max_workers(),\n+                                        ParallelScavengeHeap::heap()->workers().active_workers(),\n+                                        Threads::number_of_non_daemon_threads());\n+    ParallelScavengeHeap::heap()->workers().set_active_workers(active_workers);\n+\n+    if (serial || check_maximum_compaction()) {\n+      \/\/ Serial compaction executes the forwarding and compaction phases serially,\n+      \/\/ thus achieving perfect compaction.\n+      \/\/ Marking and ajust-references would still be executed in parallel threads.\n+      _serial = true;\n+    } else {\n+      _serial = false;\n+    }\n+\n+    GCTraceCPUTime tcpu(&_gc_tracer);\n+    GCTraceTime(Info, gc) tm(\"Pause Full\", nullptr, gc_cause, true);\n+\n+    heap->pre_full_gc_dump(&_gc_timer);\n+\n+    TraceCollectorStats tcs(counters());\n+    TraceMemoryManagerStats tms(heap->old_gc_manager(), gc_cause, \"end of major GC\");\n+\n+    if (log_is_enabled(Debug, gc, heap, exit)) {\n+      accumulated_time()->start();\n+    }\n+\n+    \/\/ Let the size policy know we're starting\n+    size_policy->major_collection_begin();\n+\n+#if COMPILER2_OR_JVMCI\n+    DerivedPointerTable::clear();\n+#endif\n+\n+    ref_processor()->start_discovery(clear_all_soft_refs);\n+\n+    ClassUnloadingContext ctx(1 \/* num_nmethod_unlink_workers *\/,\n+                              false \/* unregister_nmethods_during_purge *\/,\n+                              false \/* lock_nmethod_free_separately *\/);\n+\n+    marking_phase(&_gc_tracer);\n+\n+    summary_phase();\n+\n+#if COMPILER2_OR_JVMCI\n+    assert(DerivedPointerTable::is_active(), \"Sanity\");\n+    DerivedPointerTable::set_active(false);\n+#endif\n+\n+    FullGCForwarding::begin();\n+\n+    forward_to_new_addr();\n+\n+    adjust_pointers();\n+\n+    compact();\n+\n+    FullGCForwarding::end();\n+\n+    ParCompactionManagerNew::_preserved_marks_set->restore(&ParallelScavengeHeap::heap()->workers());\n+\n+    \/\/ Reset the mark bitmap, summary data, and do other bookkeeping.  Must be\n+    \/\/ done before resizing.\n+    post_compact();\n+\n+    \/\/ Let the size policy know we're done\n+    size_policy->major_collection_end(old_gen->used_in_bytes(), gc_cause);\n+\n+    if (UseAdaptiveSizePolicy) {\n+      log_debug(gc, ergo)(\"AdaptiveSizeStart: collection: %d \", heap->total_collections());\n+      log_trace(gc, ergo)(\"old_gen_capacity: %zu young_gen_capacity: %zu\",\n+                          old_gen->capacity_in_bytes(), young_gen->capacity_in_bytes());\n+\n+      \/\/ Don't check if the size_policy is ready here.  Let\n+      \/\/ the size_policy check that internally.\n+      if (UseAdaptiveGenerationSizePolicyAtMajorCollection &&\n+          AdaptiveSizePolicy::should_update_promo_stats(gc_cause)) {\n+        \/\/ Swap the survivor spaces if from_space is empty. The\n+        \/\/ resize_young_gen() called below is normally used after\n+        \/\/ a successful young GC and swapping of survivor spaces;\n+        \/\/ otherwise, it will fail to resize the young gen with\n+        \/\/ the current implementation.\n+        if (young_gen->from_space()->is_empty()) {\n+          young_gen->from_space()->clear(SpaceDecorator::Mangle);\n+          young_gen->swap_spaces();\n+        }\n+\n+        \/\/ Calculate optimal free space amounts\n+        assert(young_gen->max_gen_size() >\n+          young_gen->from_space()->capacity_in_bytes() +\n+          young_gen->to_space()->capacity_in_bytes(),\n+          \"Sizes of space in young gen are out-of-bounds\");\n+\n+        size_t young_live = young_gen->used_in_bytes();\n+        size_t eden_live = young_gen->eden_space()->used_in_bytes();\n+        size_t old_live = old_gen->used_in_bytes();\n+        size_t cur_eden = young_gen->eden_space()->capacity_in_bytes();\n+        size_t max_old_gen_size = old_gen->max_gen_size();\n+        size_t max_eden_size = young_gen->max_gen_size() -\n+          young_gen->from_space()->capacity_in_bytes() -\n+          young_gen->to_space()->capacity_in_bytes();\n+\n+        \/\/ Used for diagnostics\n+        size_policy->clear_generation_free_space_flags();\n+\n+        size_policy->compute_generations_free_space(young_live,\n+                                                    eden_live,\n+                                                    old_live,\n+                                                    cur_eden,\n+                                                    max_old_gen_size,\n+                                                    max_eden_size,\n+                                                    true \/* full gc*\/);\n+\n+        size_policy->check_gc_overhead_limit(eden_live,\n+                                             max_old_gen_size,\n+                                             max_eden_size,\n+                                             true \/* full gc*\/,\n+                                             gc_cause,\n+                                             heap->soft_ref_policy());\n+\n+        size_policy->decay_supplemental_growth(true \/* full gc*\/);\n+\n+        heap->resize_old_gen(\n+          size_policy->calculated_old_free_size_in_bytes());\n+\n+        heap->resize_young_gen(size_policy->calculated_eden_size_in_bytes(),\n+                               size_policy->calculated_survivor_size_in_bytes());\n+      }\n+\n+      log_debug(gc, ergo)(\"AdaptiveSizeStop: collection: %d \", heap->total_collections());\n+    }\n+\n+    if (UsePerfData) {\n+      PSGCAdaptivePolicyCounters* const counters = ParallelScavengeHeap::gc_policy_counters();\n+      counters->update_counters();\n+      counters->update_old_capacity(old_gen->capacity_in_bytes());\n+      counters->update_young_capacity(young_gen->capacity_in_bytes());\n+    }\n+\n+    heap->resize_all_tlabs();\n+\n+    \/\/ Resize the metaspace capacity after a collection\n+    MetaspaceGC::compute_new_size();\n+\n+    if (log_is_enabled(Debug, gc, heap, exit)) {\n+      accumulated_time()->stop();\n+    }\n+\n+    heap->print_heap_change(pre_gc_values);\n+\n+    \/\/ Track memory usage and detect low memory\n+    MemoryService::track_memory_usage();\n+    heap->update_counters();\n+\n+    heap->post_full_gc_dump(&_gc_timer);\n+  }\n+\n+  if (VerifyAfterGC && heap->total_collections() >= VerifyGCStartAt) {\n+    Universe::verify(\"After GC\");\n+  }\n+\n+  heap->print_heap_after_gc();\n+  heap->trace_heap_after_gc(&_gc_tracer);\n+\n+  AdaptiveSizePolicyOutput::print(size_policy, heap->total_collections());\n+\n+  _gc_timer.register_gc_end();\n+\n+  _gc_tracer.report_gc_end(_gc_timer.gc_end(), _gc_timer.time_partitions());\n+\n+  return true;\n+}\n+\n+class PCAddThreadRootsMarkingTaskClosureNew : public ThreadClosure {\n+private:\n+  uint _worker_id;\n+\n+public:\n+  explicit PCAddThreadRootsMarkingTaskClosureNew(uint worker_id) : _worker_id(worker_id) { }\n+  void do_thread(Thread* thread) final {\n+    assert(ParallelScavengeHeap::heap()->is_stw_gc_active(), \"called outside gc\");\n+\n+    ResourceMark rm;\n+\n+    ParCompactionManagerNew* cm = ParCompactionManagerNew::gc_thread_compaction_manager(_worker_id);\n+\n+    MarkingNMethodClosure mark_and_push_in_blobs(&cm->_mark_and_push_closure,\n+                                                 !NMethodToOopClosure::FixRelocations,\n+                                                 true \/* keepalive nmethods *\/);\n+\n+    thread->oops_do(&cm->_mark_and_push_closure, &mark_and_push_in_blobs);\n+\n+    \/\/ Do the real work\n+    cm->follow_marking_stacks();\n+  }\n+};\n+\n+void steal_marking_work_new(TaskTerminator& terminator, uint worker_id) {\n+  assert(ParallelScavengeHeap::heap()->is_stw_gc_active(), \"called outside gc\");\n+\n+  ParCompactionManagerNew* cm =\n+    ParCompactionManagerNew::gc_thread_compaction_manager(worker_id);\n+\n+  do {\n+    ScannerTask task;\n+    if (ParCompactionManagerNew::steal(worker_id, task)) {\n+      cm->follow_contents(task, true);\n+    }\n+    cm->follow_marking_stacks();\n+  } while (!terminator.offer_termination());\n+}\n+\n+class MarkFromRootsTaskNew : public WorkerTask {\n+  StrongRootsScope _strong_roots_scope; \/\/ needed for Threads::possibly_parallel_threads_do\n+  OopStorageSetStrongParState<false \/* concurrent *\/, false \/* is_const *\/> _oop_storage_set_par_state;\n+  TaskTerminator _terminator;\n+  uint _active_workers;\n+\n+public:\n+  explicit MarkFromRootsTaskNew(uint active_workers) :\n+      WorkerTask(\"MarkFromRootsTaskNew\"),\n+      _strong_roots_scope(active_workers),\n+      _terminator(active_workers, ParCompactionManagerNew::marking_stacks()),\n+      _active_workers(active_workers) {}\n+\n+  void work(uint worker_id) final {\n+    ParCompactionManagerNew* cm = ParCompactionManagerNew::gc_thread_compaction_manager(worker_id);\n+    {\n+      CLDToOopClosure cld_closure(&cm->_mark_and_push_closure, ClassLoaderData::_claim_stw_fullgc_mark);\n+      ClassLoaderDataGraph::always_strong_cld_do(&cld_closure);\n+\n+      \/\/ Do the real work\n+      cm->follow_marking_stacks();\n+    }\n+\n+    {\n+      PCAddThreadRootsMarkingTaskClosureNew closure(worker_id);\n+      Threads::possibly_parallel_threads_do(_active_workers > 1 \/* is_par *\/, &closure);\n+    }\n+\n+    \/\/ Mark from OopStorages\n+    {\n+      _oop_storage_set_par_state.oops_do(&cm->_mark_and_push_closure);\n+      \/\/ Do the real work\n+      cm->follow_marking_stacks();\n+    }\n+\n+    if (_active_workers > 1) {\n+      steal_marking_work_new(_terminator, worker_id);\n+    }\n+  }\n+};\n+\n+class ParallelCompactRefProcProxyTaskNew : public RefProcProxyTask {\n+  TaskTerminator _terminator;\n+\n+public:\n+  explicit ParallelCompactRefProcProxyTaskNew(uint max_workers)\n+    : RefProcProxyTask(\"ParallelCompactRefProcProxyTaskNew\", max_workers),\n+      _terminator(_max_workers, ParCompactionManagerNew::marking_stacks()) {}\n+\n+  void work(uint worker_id) final {\n+    assert(worker_id < _max_workers, \"sanity\");\n+    ParCompactionManagerNew* cm = (_tm == RefProcThreadModel::Single) ? ParCompactionManagerNew::get_vmthread_cm() : ParCompactionManagerNew::gc_thread_compaction_manager(worker_id);\n+    BarrierEnqueueDiscoveredFieldClosure enqueue;\n+    ParCompactionManagerNew::FollowStackClosure complete_gc(cm, (_tm == RefProcThreadModel::Single) ? nullptr : &_terminator, worker_id);\n+    _rp_task->rp_work(worker_id, PSParallelCompactNew::is_alive_closure(), &cm->_mark_and_push_closure, &enqueue, &complete_gc);\n+  }\n+\n+  void prepare_run_task_hook() final {\n+    _terminator.reset_for_reuse(_queue_count);\n+  }\n+};\n+\n+void PSParallelCompactNew::marking_phase(ParallelOldTracer *gc_tracer) {\n+  \/\/ Recursively traverse all live objects and mark them\n+  GCTraceTime(Info, gc, phases) tm(\"Marking Phase\", &_gc_timer);\n+\n+  uint active_gc_threads = ParallelScavengeHeap::heap()->workers().active_workers();\n+\n+  ClassLoaderDataGraph::verify_claimed_marks_cleared(ClassLoaderData::_claim_stw_fullgc_mark);\n+  {\n+    GCTraceTime(Debug, gc, phases) pm_tm(\"Par Mark\", &_gc_timer);\n+\n+    MarkFromRootsTaskNew task(active_gc_threads);\n+    ParallelScavengeHeap::heap()->workers().run_task(&task);\n+  }\n+\n+  \/\/ Process reference objects found during marking\n+  {\n+    GCTraceTime(Debug, gc, phases) rp_tm(\"Reference Processing\", &_gc_timer);\n+\n+    ReferenceProcessorStats stats;\n+    ReferenceProcessorPhaseTimes pt(&_gc_timer, ref_processor()->max_num_queues());\n+\n+    ref_processor()->set_active_mt_degree(active_gc_threads);\n+    ParallelCompactRefProcProxyTaskNew task(ref_processor()->max_num_queues());\n+    stats = ref_processor()->process_discovered_references(task, pt);\n+\n+    gc_tracer->report_gc_reference_stats(stats);\n+    pt.print_all_references();\n+  }\n+\n+  \/\/ This is the point where the entire marking should have completed.\n+  ParCompactionManagerNew::verify_all_marking_stack_empty();\n+\n+  {\n+    GCTraceTime(Debug, gc, phases) wp_tm(\"Weak Processing\", &_gc_timer);\n+    WeakProcessor::weak_oops_do(&ParallelScavengeHeap::heap()->workers(),\n+                                is_alive_closure(),\n+                                &do_nothing_cl,\n+                                1);\n+  }\n+\n+  {\n+    GCTraceTime(Debug, gc, phases) tm_m(\"Class Unloading\", &_gc_timer);\n+\n+    ClassUnloadingContext* ctx = ClassUnloadingContext::context();\n+\n+    bool unloading_occurred;\n+    {\n+      CodeCache::UnlinkingScope scope(is_alive_closure());\n+\n+      \/\/ Follow system dictionary roots and unload classes.\n+      unloading_occurred = SystemDictionary::do_unloading(&_gc_timer);\n+\n+      \/\/ Unload nmethods.\n+      CodeCache::do_unloading(unloading_occurred);\n+    }\n+\n+    {\n+      GCTraceTime(Debug, gc, phases) t(\"Purge Unlinked NMethods\", gc_timer());\n+      \/\/ Release unloaded nmethod's memory.\n+      ctx->purge_nmethods();\n+    }\n+    {\n+      GCTraceTime(Debug, gc, phases) ur(\"Unregister NMethods\", &_gc_timer);\n+      ParallelScavengeHeap::heap()->prune_unlinked_nmethods();\n+    }\n+    {\n+      GCTraceTime(Debug, gc, phases) t(\"Free Code Blobs\", gc_timer());\n+      ctx->free_nmethods();\n+    }\n+\n+    \/\/ Prune dead klasses from subklass\/sibling\/implementor lists.\n+    Klass::clean_weak_klass_links(unloading_occurred);\n+\n+    \/\/ Clean JVMCI metadata handles.\n+    JVMCI_ONLY(JVMCI::do_unloading(unloading_occurred));\n+  }\n+\n+  {\n+    GCTraceTime(Debug, gc, phases) roc_tm(\"Report Object Count\", &_gc_timer);\n+    _gc_tracer.report_object_count_after_gc(is_alive_closure(), &ParallelScavengeHeap::heap()->workers());\n+  }\n+#if TASKQUEUE_STATS\n+  ParCompactionManagerNew::print_and_reset_taskqueue_stats();\n+#endif\n+}\n+\n+void PSParallelCompactNew::adjust_pointers_in_spaces(uint worker_id) {\n+  auto start_time = Ticks::now();\n+  for (size_t i = 0; i < _num_regions; i++) {\n+    PCRegionData* region = &_region_data_array[i];\n+    if (!region->claim()) {\n+      continue;\n+    }\n+    log_trace(gc, compaction)(\"Adjusting pointers in region: %zu (worker_id: %u)\", region->idx(), worker_id);\n+    HeapWord* end = region->top();\n+    HeapWord* current = _mark_bitmap.find_obj_beg(region->bottom(), end);\n+    while (current < end) {\n+      assert(_mark_bitmap.is_marked(current), \"must be marked\");\n+      oop obj = cast_to_oop(current);\n+      size_t size = obj->size();\n+      obj->oop_iterate(&pc_adjust_pointer_closure);\n+      current = _mark_bitmap.find_obj_beg(current + size, end);\n+    }\n+  }\n+  log_trace(gc, phases)(\"adjust_pointers_in_spaces worker %u: %.3f ms\", worker_id, (Ticks::now() - start_time).seconds() * 1000);\n+}\n+\n+class PSAdjustTaskNew final : public WorkerTask {\n+  SubTasksDone                               _sub_tasks;\n+  WeakProcessor::Task                        _weak_proc_task;\n+  OopStorageSetStrongParState<false, false>  _oop_storage_iter;\n+  uint                                       _nworkers;\n+\n+  enum PSAdjustSubTask {\n+    PSAdjustSubTask_code_cache,\n+\n+    PSAdjustSubTask_num_elements\n+  };\n+\n+public:\n+  explicit PSAdjustTaskNew(uint nworkers) :\n+    WorkerTask(\"PSAdjust task\"),\n+    _sub_tasks(PSAdjustSubTask_num_elements),\n+    _weak_proc_task(nworkers),\n+    _nworkers(nworkers) {\n+\n+    ClassLoaderDataGraph::verify_claimed_marks_cleared(ClassLoaderData::_claim_stw_fullgc_adjust);\n+    if (nworkers > 1) {\n+      Threads::change_thread_claim_token();\n+    }\n+  }\n+\n+  ~PSAdjustTaskNew() {\n+    Threads::assert_all_threads_claimed();\n+  }\n+\n+  void work(uint worker_id) final {\n+    ParCompactionManagerNew* cm = ParCompactionManagerNew::gc_thread_compaction_manager(worker_id);\n+    cm->preserved_marks()->adjust_during_full_gc();\n+    {\n+      \/\/ adjust pointers in all spaces\n+      PSParallelCompactNew::adjust_pointers_in_spaces(worker_id);\n+    }\n+    {\n+      ResourceMark rm;\n+      Threads::possibly_parallel_oops_do(_nworkers > 1, &pc_adjust_pointer_closure, nullptr);\n+    }\n+    _oop_storage_iter.oops_do(&pc_adjust_pointer_closure);\n+    {\n+      CLDToOopClosure cld_closure(&pc_adjust_pointer_closure, ClassLoaderData::_claim_stw_fullgc_adjust);\n+      ClassLoaderDataGraph::cld_do(&cld_closure);\n+    }\n+    {\n+      AlwaysTrueClosure always_alive;\n+      _weak_proc_task.work(worker_id, &always_alive, &pc_adjust_pointer_closure);\n+    }\n+    if (_sub_tasks.try_claim_task(PSAdjustSubTask_code_cache)) {\n+      NMethodToOopClosure adjust_code(&pc_adjust_pointer_closure, NMethodToOopClosure::FixRelocations);\n+      CodeCache::nmethods_do(&adjust_code);\n+    }\n+    _sub_tasks.all_tasks_claimed();\n+  }\n+};\n+\n+void PSParallelCompactNew::adjust_pointers() {\n+  \/\/ Adjust the pointers to reflect the new locations\n+  GCTraceTime(Info, gc, phases) tm(\"Adjust Pointers\", &_gc_timer);\n+  uint num_workers = ParallelScavengeHeap::heap()->workers().active_workers();\n+  PSAdjustTaskNew task(num_workers);\n+  ParallelScavengeHeap::heap()->workers().run_task(&task);\n+}\n+\n+void PSParallelCompactNew::forward_to_new_addr() {\n+  GCTraceTime(Info, gc, phases) tm(\"Forward\", &_gc_timer);\n+  uint num_workers = get_num_workers();\n+  _per_worker_region_data = NEW_C_HEAP_ARRAY(PCRegionData*, num_workers, mtGC);\n+  for (uint i = 0; i < num_workers; i++) {\n+    _per_worker_region_data[i] = nullptr;\n+  }\n+\n+  class ForwardState {\n+    uint _worker_id;\n+    PCRegionData* _compaction_region;\n+    HeapWord* _compaction_point;\n+\n+    void ensure_compaction_point() {\n+      if (_compaction_point == nullptr) {\n+        assert(_compaction_region == nullptr, \"invariant\");\n+        _compaction_region = _per_worker_region_data[_worker_id];\n+        assert(_compaction_region != nullptr, \"invariant\");\n+        _compaction_point = _compaction_region->bottom();\n+      }\n+    }\n+  public:\n+    explicit ForwardState(uint worker_id) :\n+            _worker_id(worker_id),\n+            _compaction_region(nullptr),\n+            _compaction_point(nullptr) {\n+    }\n+\n+    size_t available() const {\n+      return pointer_delta(_compaction_region->end(), _compaction_point);\n+    }\n+\n+    void forward_objs_in_region(ParCompactionManagerNew* cm, PCRegionData* region) {\n+      ensure_compaction_point();\n+      HeapWord* end = region->end();\n+      HeapWord* current = _mark_bitmap.find_obj_beg(region->bottom(), end);\n+      while (current < end) {\n+        assert(_mark_bitmap.is_marked(current), \"must be marked\");\n+        oop obj = cast_to_oop(current);\n+        assert(region->contains(obj), \"object must not cross region boundary: obj: \" PTR_FORMAT \", obj_end: \" PTR_FORMAT \", region start: \" PTR_FORMAT \", region end: \" PTR_FORMAT, p2i(obj), p2i(cast_from_oop<HeapWord*>(obj) + obj->size()), p2i(region->bottom()), p2i(region->end()));\n+        size_t old_size = obj->size();\n+        size_t new_size = obj->copy_size(old_size, obj->mark());\n+        size_t size = (current == _compaction_point) ? old_size : new_size;\n+        while (size > available()) {\n+          _compaction_region->set_new_top(_compaction_point);\n+          _compaction_region = _compaction_region->local_next();\n+          assert(_compaction_region != nullptr, \"must find a compaction region\");\n+          _compaction_point = _compaction_region->bottom();\n+          size = (current == _compaction_point) ? old_size : new_size;\n+        }\n+        \/\/log_develop_trace(gc, compaction)(\"Forwarding obj: \" PTR_FORMAT \", to: \" PTR_FORMAT, p2i(obj), p2i(_compaction_point));\n+        if (current != _compaction_point) {\n+          cm->preserved_marks()->push_if_necessary(obj, obj->mark());\n+          FullGCForwarding::forward_to(obj, cast_to_oop(_compaction_point));\n+        }\n+        _compaction_point += size;\n+        assert(_compaction_point <= _compaction_region->end(), \"object must fit in region\");\n+        current += old_size;\n+        assert(current <= end, \"object must not cross region boundary\");\n+        current = _mark_bitmap.find_obj_beg(current, end);\n+      }\n+    }\n+    void finish() {\n+      if (_compaction_region != nullptr) {\n+        _compaction_region->set_new_top(_compaction_point);\n+      }\n+    }\n+  };\n+\n+  struct ForwardTask final : public WorkerTask {\n+    ForwardTask() : WorkerTask(\"PSForward task\") {}\n+\n+    void work(uint worker_id) override {\n+      ParCompactionManagerNew* cm = ParCompactionManagerNew::gc_thread_compaction_manager(worker_id);\n+      ForwardState state(worker_id);\n+      PCRegionData** last_link = &_per_worker_region_data[worker_id];\n+      size_t idx = worker_id;\n+      uint num_workers = get_num_workers();\n+      size_t num_regions = get_num_regions();\n+      PCRegionData* region_data_array = get_region_data_array();\n+      while (idx < num_regions) {\n+        PCRegionData* region = region_data_array + idx;\n+        *last_link = region;\n+        last_link = region->local_next_addr();\n+        state.forward_objs_in_region(cm, region);\n+        idx += num_workers;\n+      }\n+      state.finish();\n+    }\n+  } task;\n+\n+  uint par_workers = ParallelScavengeHeap::heap()->workers().active_workers();\n+  ParallelScavengeHeap::heap()->workers().set_active_workers(num_workers);\n+  ParallelScavengeHeap::heap()->workers().run_task(&task);\n+  ParallelScavengeHeap::heap()->workers().set_active_workers(par_workers);\n+\n+#ifndef PRODUCT\n+  for (uint wid = 0; wid < num_workers; wid++) {\n+    for (PCRegionData* rd = _per_worker_region_data[wid]; rd != nullptr; rd = rd->local_next()) {\n+      log_develop_trace(gc, compaction)(\"Per worker compaction region, worker: %d, #%zu: [\" PTR_FORMAT \", \" PTR_FORMAT \"), new_top: \" PTR_FORMAT, wid, rd->idx(),\n+                                        p2i(rd->bottom()), p2i(rd->end()), p2i(rd->new_top()));\n+    }\n+  }\n+#endif\n+}\n+\n+void PSParallelCompactNew::compact() {\n+  GCTraceTime(Info, gc, phases) tm(\"Compaction Phase\", &_gc_timer);\n+  class CompactTask final : public WorkerTask {\n+    static void compact_region(PCRegionData* region) {\n+      HeapWord* bottom = region->bottom();\n+      HeapWord* end = region->top();\n+      if (bottom == end) {\n+        return;\n+      }\n+      HeapWord* current = _mark_bitmap.find_obj_beg(bottom, end);\n+      while (current < end) {\n+        oop obj = cast_to_oop(current);\n+        size_t size = obj->size();\n+        if (FullGCForwarding::is_forwarded(obj)) {\n+          oop fwd = FullGCForwarding::forwardee(obj);\n+          auto* dst = cast_from_oop<HeapWord*>(fwd);\n+          ObjectStartArray* sa = start_array(space_id(dst));\n+          if (sa != nullptr) {\n+            assert(dst != current, \"expect moving object\");\n+            size_t new_words = obj->copy_size(size, obj->mark());\n+            sa->update_for_block(dst, dst + new_words);\n+          }\n+\n+          Copy::aligned_conjoint_words(current, dst, size);\n+          fwd->init_mark();\n+          fwd->initialize_hash_if_necessary(obj);\n+        } else {\n+          \/\/ The start_array must be updated even if the object is not moving.\n+          ObjectStartArray* sa = start_array(space_id(current));\n+          if (sa != nullptr) {\n+            sa->update_for_block(current, current + size);\n+          }\n+        }\n+        current = _mark_bitmap.find_obj_beg(current + size, end);\n+      }\n+    }\n+  public:\n+    explicit CompactTask() : WorkerTask(\"PSCompact task\") {}\n+    void work(uint worker_id) override {\n+      PCRegionData* region = _per_worker_region_data[worker_id];\n+      while (region != nullptr) {\n+        log_trace(gc)(\"Compact worker: %u, compacting region: %zu\", worker_id, region->idx());\n+        compact_region(region);\n+        region = region->local_next();\n+      }\n+    }\n+  } task;\n+\n+  uint num_workers = get_num_workers();\n+  uint par_workers = ParallelScavengeHeap::heap()->workers().active_workers();\n+  ParallelScavengeHeap::heap()->workers().set_active_workers(num_workers);\n+  ParallelScavengeHeap::heap()->workers().run_task(&task);\n+  ParallelScavengeHeap::heap()->workers().set_active_workers(par_workers);\n+}\n+\n+\/\/ Return the SpaceId for the space containing addr.  If addr is not in the\n+\/\/ heap, last_space_id is returned.  In debug mode it expects the address to be\n+\/\/ in the heap and asserts such.\n+PSParallelCompactNew::SpaceId PSParallelCompactNew::space_id(HeapWord* addr) {\n+  assert(ParallelScavengeHeap::heap()->is_in_reserved(addr), \"addr not in the heap\");\n+\n+  for (unsigned int id = old_space_id; id < last_space_id; ++id) {\n+    if (_space_info[id].space()->contains(addr)) {\n+      return SpaceId(id);\n+    }\n+  }\n+\n+  assert(false, \"no space contains the addr\");\n+  return last_space_id;\n+}\n","filename":"src\/hotspot\/share\/gc\/parallel\/psParallelCompactNew.cpp","additions":1147,"deletions":0,"binary":false,"changes":1147,"status":"added"},{"patch":"@@ -0,0 +1,338 @@\n+\/*\n+ * Copyright (c) 2005, 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n+ * * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_GC_PARALLEL_PSPARALLELCOMPACTNEW_HPP\n+#define SHARE_GC_PARALLEL_PSPARALLELCOMPACTNEW_HPP\n+\n+#include \"gc\/parallel\/mutableSpace.hpp\"\n+#include \"gc\/parallel\/objectStartArray.hpp\"\n+#include \"gc\/parallel\/parallelScavengeHeap.hpp\"\n+#include \"gc\/parallel\/parMarkBitMap.hpp\"\n+#include \"gc\/shared\/collectedHeap.hpp\"\n+#include \"gc\/shared\/collectorCounters.hpp\"\n+#include \"gc\/shared\/referenceProcessor.hpp\"\n+#include \"gc\/shared\/taskTerminator.hpp\"\n+#include \"oops\/oop.hpp\"\n+#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/orderAccess.hpp\"\n+\n+class ParallelScavengeHeap;\n+class PSAdaptiveSizePolicy;\n+class PSYoungGen;\n+class PSOldGen;\n+class ParCompactionManagerNew;\n+class PSParallelCompactNew;\n+class ParallelOldTracer;\n+class STWGCTimer;\n+\n+class SpaceInfoNew\n+{\n+public:\n+  MutableSpace* space() const { return _space; }\n+\n+  \/\/ The start array for the (generation containing the) space, or null if there\n+  \/\/ is no start array.\n+  ObjectStartArray* start_array() const { return _start_array; }\n+\n+  void set_space(MutableSpace* s)           { _space = s; }\n+  void set_start_array(ObjectStartArray* s) { _start_array = s; }\n+\n+private:\n+  MutableSpace*     _space;\n+  ObjectStartArray* _start_array;\n+};\n+\n+\/\/ The Parallel compaction collector is a stop-the-world garbage collector that\n+\/\/ does parts of the collection using parallel threads.  The collection includes\n+\/\/ the tenured generation and the young generation.\n+\/\/\n+\/\/ A collection consists of the following phases.\n+\/\/\n+\/\/      - marking phase\n+\/\/      - summary phase (single-threaded)\n+\/\/      - forward (to new address) phase\n+\/\/      - adjust pointers phase\n+\/\/      - compacting phase\n+\/\/      - clean up phase\n+\/\/\n+\/\/ Roughly speaking these phases correspond, respectively, to\n+\/\/\n+\/\/      - mark all the live objects\n+\/\/      - set-up temporary regions to enable parallelism in following phases\n+\/\/      - calculate the destination of each object at the end of the collection\n+\/\/      - adjust pointers to reflect new destination of objects\n+\/\/      - move the objects to their destination\n+\/\/      - update some references and reinitialize some variables\n+\/\/\n+\/\/ A space that is being collected is divided into regions and with each region\n+\/\/ is associated an object of type PCRegionData. Regions are targeted to be of\n+\/\/ a mostly uniform size, but if an object would cross a region boundary, then\n+\/\/ the boundary is adjusted to be after the end of that object.\n+\/\/\n+\/\/ The marking phase does a complete marking of all live objects in the heap.\n+\/\/ The marking phase uses multiple GC threads and marking is done in a bit\n+\/\/ array of type ParMarkBitMap.  The marking of the bit map is done atomically.\n+\/\/\n+\/\/ The summary phase sets up the regions, such that region covers roughly\n+\/\/ uniform memory regions (currently same size as SpaceAlignment). However,\n+\/\/ if that would result in an object crossing a region boundary, then\n+\/\/ the upper bounds is adjusted such that the region ends after that object.\n+\/\/ This way we can ensure that a GC worker thread can fully 'own' a region\n+\/\/ during the forwarding, adjustment and compaction phases, without worrying\n+\/\/ about other threads messing with parts of the object. The summary phase\n+\/\/ also sets up an alternative set of regions, where each region covers\n+\/\/ a single space. This is used for a serial compaction mode which achieves\n+\/\/ maximum compaction at the expense of parallelism during the forwarding\n+\/\/ compaction phases.\n+\/\/\n+\/\/ The forwarding phase calculates the new address of each live\n+\/\/ object and records old-addr-to-new-addr association. It does this using\n+\/\/ multiple GC threads. Each thread 'claims' a source region and appends it to a\n+\/\/ local work-list. The region is also set as the current compaction region\n+\/\/ for that thread. All live objects in the region are then visited and its\n+\/\/ new location calculated by tracking the compaction point in the compaction\n+\/\/ region. Once the source region is exhausted, the next source region is\n+\/\/ claimed from the global pool and appended to the end of the local work-list.\n+\/\/ Once the compaction region is exhausted, the top of the old compaction region\n+\/\/ is recorded, and the next compaction region is fetched from the front of the\n+\/\/ local work-list (which is guaranteed to already have finished processing, or\n+\/\/ is the same as the source region). This way, each worker forms a local\n+\/\/ list of regions in which the worker can compact as if it were a serial\n+\/\/ compaction.\n+\/\/\n+\/\/ The adjust pointers phase remaps all pointers to reflect the new address of each\n+\/\/ object. Again, this uses multiple GC worker threads. Each thread claims\n+\/\/ a region, processes all references in all live objects of that region. Then\n+\/\/ the thread proceeds to claim the next region from the global pool, until\n+\/\/ all regions have been processed.\n+\/\/\n+\/\/ The compaction phase moves objects to their new location. Again, this uses\n+\/\/ multiple GC worker threads. Each worker processes the local work-list that\n+\/\/ has been set-up during the forwarding phase and processes it from bottom\n+\/\/ to top, copying each live object to its new location (which is guaranteed\n+\/\/ to be lower in that threads parts of the heap, and thus would never overwrite\n+\/\/ other objects).\n+\/\/\n+\/\/ This algorithm will usually leave gaps of non-fillable memory at the end\n+\/\/ of regions, and potentially whole empty regions at the end of compaction.\n+\/\/ The post-compaction phase fills those gaps with filler objects to ensure\n+\/\/ that the heap remains parsable.\n+\/\/\n+\/\/ In some situations, this inefficiency of leaving gaps can lead to a\n+\/\/ situation where after a full GC, it is still not possible to satisfy an\n+\/\/ allocation, even though there should be enough memory available. When\n+\/\/ that happens, the collector switches to a serial mode, where we only\n+\/\/ have 4 regions which correspond exaxtly to the 4 spaces, and the forwarding\n+\/\/ and compaction phases are executed using only a single thread. This\n+\/\/ achieves maximum compaction. This serial mode is also invoked when\n+\/\/ System.gc() is called *and* UseMaximumCompactionOnSystemGC is set to\n+\/\/ true (which is the default), or when the number of full GCs exceeds\n+\/\/ the HeapMaximumCompactionInterval.\n+\/\/\n+\/\/ Possible improvements to the algorithm include:\n+\/\/ - Identify and ignore a 'dense prefix'. This requires that we collect\n+\/\/   liveness data during marking, or that we scan the prefix object-by-object\n+\/\/   during the summary phase.\n+\/\/ - When an object does not fit into a remaining gap of a region, and the\n+\/\/   object is rather large, we could attempt to forward\/compact subsequent\n+\/\/   objects 'around' that large object in an attempt to minimize the\n+\/\/   resulting gap. This could be achieved by reconfiguring the regions\n+\/\/   to exclude the large object.\n+\/\/ - Instead of finding out *after* the whole compaction that an allocation\n+\/\/   can still not be satisfied, and then re-running the whole compaction\n+\/\/   serially, we could determine that after the forwarding phase, and\n+\/\/   re-do only forwarding serially, thus avoiding running marking,\n+\/\/   adjusting references and compaction twice.\n+class PCRegionData \/*: public CHeapObj<mtGC> *\/ {\n+  \/\/ A region index\n+  size_t const _idx;\n+\n+  \/\/ The start of the region\n+  HeapWord* const _bottom;\n+  \/\/ The top of the region. (first word after last live object in containing space)\n+  HeapWord* const _top;\n+  \/\/ The end of the region (first word after last word of the region)\n+  HeapWord* const _end;\n+\n+  \/\/ The next compaction address\n+  HeapWord* _new_top;\n+\n+  \/\/ Points to the next region in the GC-worker-local work-list\n+  PCRegionData* _local_next;\n+\n+  \/\/ Parallel workers claiming protocol, used during adjust-references phase.\n+  volatile bool _claimed;\n+\n+public:\n+\n+  PCRegionData(size_t idx, HeapWord* bottom, HeapWord* top, HeapWord* end) :\n+    _idx(idx), _bottom(bottom), _top(top), _end(end), _new_top(bottom),\n+          _local_next(nullptr), _claimed(false) {}\n+\n+  size_t idx() const { return _idx; };\n+\n+  HeapWord* bottom() const { return _bottom; }\n+  HeapWord* top() const { return _top; }\n+  HeapWord* end()   const { return _end;   }\n+\n+  PCRegionData*  local_next() const { return _local_next; }\n+  PCRegionData** local_next_addr() { return &_local_next; }\n+\n+  HeapWord* new_top() const {\n+    return _new_top;\n+  }\n+  void set_new_top(HeapWord* new_top) {\n+    _new_top = new_top;\n+  }\n+\n+  bool contains(oop obj) {\n+    auto* obj_start = cast_from_oop<HeapWord*>(obj);\n+    HeapWord* obj_end = obj_start + obj->size();\n+    return _bottom <= obj_start && obj_start < _end && _bottom < obj_end && obj_end <= _end;\n+  }\n+\n+  bool claim() {\n+    bool claimed =  _claimed;\n+    if (claimed) {\n+      return false;\n+    }\n+    return !Atomic::cmpxchg(&_claimed, false, true);\n+  }\n+};\n+\n+class PSParallelCompactNew : AllStatic {\n+public:\n+  typedef enum {\n+    old_space_id, eden_space_id,\n+    from_space_id, to_space_id, last_space_id\n+  } SpaceId;\n+\n+public:\n+  class IsAliveClosure: public BoolObjectClosure {\n+  public:\n+    bool do_object_b(oop p) final;\n+  };\n+\n+private:\n+  static STWGCTimer           _gc_timer;\n+  static ParallelOldTracer    _gc_tracer;\n+  static elapsedTimer         _accumulated_time;\n+  static unsigned int         _maximum_compaction_gc_num;\n+  static CollectorCounters*   _counters;\n+  static ParMarkBitMap        _mark_bitmap;\n+  static IsAliveClosure       _is_alive_closure;\n+  static SpaceInfoNew         _space_info[last_space_id];\n+\n+  \/\/ The head of the global region data list.\n+  static size_t               _num_regions;\n+  static PCRegionData*        _region_data_array;\n+  static PCRegionData**       _per_worker_region_data;\n+\n+  static size_t               _num_regions_serial;\n+  static PCRegionData*        _region_data_array_serial;\n+  static bool                 _serial;\n+\n+  \/\/ Reference processing (used in ...follow_contents)\n+  static SpanSubjectToDiscoveryClosure  _span_based_discoverer;\n+  static ReferenceProcessor*  _ref_processor;\n+\n+  static uint get_num_workers() { return _serial ? 1 : ParallelScavengeHeap::heap()->workers().active_workers(); }\n+  static size_t get_num_regions() { return _serial ? _num_regions_serial : _num_regions; }\n+  static PCRegionData* get_region_data_array() { return _serial ? _region_data_array_serial : _region_data_array; }\n+\n+public:\n+  static ParallelOldTracer* gc_tracer() { return &_gc_tracer; }\n+\n+private:\n+\n+  static void initialize_space_info();\n+\n+  \/\/ Clear the marking bitmap and summary data that cover the specified space.\n+  static void clear_data_covering_space(SpaceId id);\n+\n+  static void pre_compact();\n+\n+  static void post_compact();\n+\n+  static bool check_maximum_compaction();\n+\n+  \/\/ Mark live objects\n+  static void marking_phase(ParallelOldTracer *gc_tracer);\n+\n+  static void summary_phase();\n+  static void setup_regions_parallel();\n+  static void setup_regions_serial();\n+\n+  static void adjust_pointers();\n+  static void forward_to_new_addr();\n+\n+  \/\/ Move objects to new locations.\n+  static void compact();\n+\n+public:\n+  static bool invoke(bool maximum_heap_compaction, bool serial);\n+  static bool invoke_no_policy(bool maximum_heap_compaction, bool serial);\n+\n+  static void adjust_pointers_in_spaces(uint worker_id);\n+\n+  static void post_initialize();\n+  \/\/ Perform initialization for PSParallelCompactNew that requires\n+  \/\/ allocations.  This should be called during the VM initialization\n+  \/\/ at a pointer where it would be appropriate to return a JNI_ENOMEM\n+  \/\/ in the event of a failure.\n+  static bool initialize_aux_data();\n+\n+  \/\/ Closure accessors\n+  static BoolObjectClosure* is_alive_closure()     { return &_is_alive_closure; }\n+\n+  \/\/ Public accessors\n+  static elapsedTimer* accumulated_time() { return &_accumulated_time; }\n+\n+  static CollectorCounters* counters()    { return _counters; }\n+\n+  static inline bool is_marked(oop obj);\n+\n+  template <class T> static inline void adjust_pointer(T* p);\n+\n+  \/\/ Convenience wrappers for per-space data kept in _space_info.\n+  static inline MutableSpace*     space(SpaceId space_id);\n+  static inline ObjectStartArray* start_array(SpaceId space_id);\n+\n+  static ParMarkBitMap* mark_bitmap() { return &_mark_bitmap; }\n+\n+  \/\/ Reference Processing\n+  static ReferenceProcessor* ref_processor() { return _ref_processor; }\n+\n+  static STWGCTimer* gc_timer() { return &_gc_timer; }\n+\n+  \/\/ Return the SpaceId for the given address.\n+  static SpaceId space_id(HeapWord* addr);\n+\n+  static void print_on(outputStream* st);\n+};\n+\n+void steal_marking_work_new(TaskTerminator& terminator, uint worker_id);\n+\n+#endif \/\/ SHARE_GC_PARALLEL_PSPARALLELCOMPACTNEW_HPP\n","filename":"src\/hotspot\/share\/gc\/parallel\/psParallelCompactNew.hpp","additions":338,"deletions":0,"binary":false,"changes":338,"status":"added"},{"patch":"@@ -33,0 +33,1 @@\n+#include \"gc\/parallel\/psCompactionManagerNew.hpp\"\n@@ -207,1 +208,1 @@\n-      _terminator(max_workers, ParCompactionManager::marking_stacks()) {}\n+      _terminator(max_workers, UseCompactObjectHeaders ? ParCompactionManagerNew::marking_stacks() : ParCompactionManager::marking_stacks()) {}\n","filename":"src\/hotspot\/share\/gc\/parallel\/psScavenge.cpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -691,7 +691,1 @@\n-      if (obj->is_self_forwarded()) {\n-        obj->unset_self_forwarded();\n-      } else if (obj->is_forwarded()) {\n-        \/\/ To restore the klass-bits in the header.\n-        \/\/ Needed for object iteration to work properly.\n-        obj->set_mark(obj->forwardee()->prototype_mark());\n-      }\n+      obj->reset_forwarded();\n@@ -728,1 +722,3 @@\n-  size_t s = old->size();\n+  size_t old_size = old->size();\n+  size_t s = old->copy_size(old_size, old->mark());\n+\n@@ -753,1 +749,1 @@\n-  Copy::aligned_disjoint_words(cast_from_oop<HeapWord*>(old), cast_from_oop<HeapWord*>(obj), s);\n+  Copy::aligned_disjoint_words(cast_from_oop<HeapWord*>(old), cast_from_oop<HeapWord*>(obj), old_size);\n@@ -763,0 +759,2 @@\n+  obj->initialize_hash_if_necessary(old);\n+\n","filename":"src\/hotspot\/share\/gc\/serial\/defNewGeneration.cpp","additions":7,"deletions":9,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -30,5 +30,0 @@\n-void SerialArguments::initialize() {\n-  GCArguments::initialize();\n-  FullGCForwarding::initialize_flags(MaxHeapSize);\n-}\n-\n","filename":"src\/hotspot\/share\/gc\/serial\/serialArguments.cpp","additions":0,"deletions":5,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -193,1 +193,2 @@\n-  HeapWord* alloc(size_t words) {\n+  HeapWord* alloc(size_t old_size, size_t new_size, HeapWord* old_obj) {\n+    size_t words = (old_obj == _spaces[_index]._compaction_top) ? old_size : new_size;\n@@ -209,0 +210,1 @@\n+      words = (old_obj == _spaces[_index]._compaction_top) ? old_size : new_size;\n@@ -260,2 +262,0 @@\n-    assert(addr != new_addr, \"inv\");\n-    prefetch_write_copy(new_addr);\n@@ -264,1 +264,4 @@\n-    Copy::aligned_conjoint_words(addr, new_addr, obj_size);\n+    if (addr != new_addr) {\n+      prefetch_write_copy(new_addr);\n+      Copy::aligned_conjoint_words(addr, new_addr, obj_size);\n+    }\n@@ -266,0 +269,3 @@\n+    if (addr != new_addr) {\n+      new_obj->initialize_hash_if_necessary(obj);\n+    }\n@@ -301,0 +307,1 @@\n+        size_t new_size = obj->copy_size(obj_size, obj->mark());\n@@ -302,1 +309,1 @@\n-          HeapWord* new_addr = alloc(obj_size);\n+          HeapWord* new_addr = alloc(obj_size, new_size, cur_addr);\n@@ -304,0 +311,1 @@\n+          assert(obj->size() == obj_size, \"size must not change after forwarding\");\n@@ -310,1 +318,2 @@\n-            alloc(pointer_delta(next_live_addr, cur_addr));\n+            size_t size = pointer_delta(next_live_addr, cur_addr);\n+            alloc(size, size, cur_addr);\n@@ -596,1 +605,1 @@\n-  obj->set_mark(obj->prototype_mark().set_marked());\n+  obj->set_mark(mark.set_marked());\n@@ -699,0 +708,2 @@\n+  FullGCForwarding::begin();\n+\n@@ -742,0 +753,2 @@\n+  FullGCForwarding::end();\n+\n","filename":"src\/hotspot\/share\/gc\/serial\/serialFullGC.cpp","additions":20,"deletions":7,"binary":false,"changes":27,"status":"modified"},{"patch":"@@ -390,1 +390,1 @@\n-  assert(obj_size == obj->size(), \"bad obj_size passed in\");\n+  assert(obj_size == obj->size() || UseCompactObjectHeaders, \"bad obj_size passed in\");\n","filename":"src\/hotspot\/share\/gc\/serial\/tenuredGeneration.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -707,1 +707,2 @@\n-  \/\/ 8  - 32-bit VM or 64-bit VM, compact headers\n+  \/\/ 4  - compact headers\n+  \/\/ 8  - 32-bit VM\n@@ -712,1 +713,0 @@\n-    assert(!UseCompactObjectHeaders, \"\");\n@@ -717,2 +717,6 @@\n-      \/\/ Include klass to copy by 8 bytes words.\n-      base_off = instanceOopDesc::klass_offset_in_bytes();\n+      if (UseCompactObjectHeaders) {\n+        base_off = 0; \/* FIXME *\/\n+      } else {\n+        \/\/ Include klass to copy by 8 bytes words.\n+        base_off = instanceOopDesc::klass_offset_in_bytes();\n+      }\n@@ -720,1 +724,1 @@\n-    assert(base_off % BytesPerLong == 0, \"expect 8 bytes alignment\");\n+    assert(base_off % BytesPerLong == 0 || UseCompactObjectHeaders, \"expect 8 bytes alignment\");\n","filename":"src\/hotspot\/share\/gc\/shared\/c2\/barrierSetC2.cpp","additions":9,"deletions":5,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -285,1 +285,1 @@\n-  oop class_allocate(Klass* klass, size_t size, TRAPS);\n+  oop class_allocate(Klass* klass, size_t size, size_t base_size, TRAPS);\n","filename":"src\/hotspot\/share\/gc\/shared\/collectedHeap.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -442,1 +442,1 @@\n-  assert(_word_size > 0, \"oop_size must be positive.\");\n+  assert(_base_size > 0, \"oop_size must be positive.\");\n@@ -444,1 +444,1 @@\n-  java_lang_Class::set_oop_size(mem, _word_size);\n+  java_lang_Class::set_oop_size(mem, _base_size);\n","filename":"src\/hotspot\/share\/gc\/shared\/memAllocator.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -247,0 +247,2 @@\n+    FullGCForwarding::begin();\n+\n@@ -256,0 +258,2 @@\n+\n+    FullGCForwarding::end();\n@@ -353,1 +357,3 @@\n-    size_t obj_size = p->size();\n+    size_t old_size = p->size();\n+    size_t new_size = p->copy_size(old_size, p->mark());\n+    size_t obj_size = _compact_point == cast_from_oop<HeapWord*>(p) ? old_size : new_size;\n@@ -371,0 +377,1 @@\n+      obj_size = _compact_point == cast_from_oop<HeapWord*>(p) ? old_size : new_size;\n@@ -890,0 +897,1 @@\n+      new_obj->initialize_hash_if_necessary(p);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahFullGC.cpp","additions":9,"deletions":1,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -281,1 +281,2 @@\n-  size_t obj_size = p->size();\n+  size_t old_size = p->size();\n+  size_t new_size = p->copy_size(old_size, p->mark());\n@@ -288,1 +289,1 @@\n-    if ((_old_to_region != nullptr) && (_old_compact_point + obj_size > _old_to_region->end())) {\n+    if ((_old_to_region != nullptr) && (_old_compact_point + new_size > _old_to_region->end())) {\n@@ -310,0 +311,1 @@\n+    size_t obj_size = _old_compact_point == cast_from_oop<HeapWord*>(p) ? old_size : new_size;\n@@ -334,0 +336,1 @@\n+      obj_size = _old_compact_point == cast_from_oop<HeapWord*>(p) ? old_size : new_size;\n@@ -357,0 +360,1 @@\n+    size_t obj_size = _young_compact_point == cast_from_oop<HeapWord*>(p) ? old_size : new_size;\n@@ -380,0 +384,1 @@\n+      obj_size = _young_compact_point == cast_from_oop<HeapWord*>(p) ? old_size : new_size;\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahGenerationalFullGC.cpp","additions":7,"deletions":2,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -242,1 +242,8 @@\n-  size_t size = ShenandoahForwarding::size(p);\n+\n+  markWord mark = p->mark();\n+  if (ShenandoahForwarding::is_forwarded(mark)) {\n+    return ShenandoahForwarding::get_forwardee(p);\n+  }\n+  size_t old_size = ShenandoahForwarding::size(p);\n+  size_t size = p->copy_size(old_size, mark);\n+\n@@ -334,1 +341,1 @@\n-  Copy::aligned_disjoint_words(cast_from_oop<HeapWord*>(p), copy, size);\n+  Copy::aligned_disjoint_words(cast_from_oop<HeapWord*>(p), copy, old_size);\n@@ -352,0 +359,1 @@\n+    copy_val->initialize_hash_if_necessary(p);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahGenerationalHeap.cpp","additions":10,"deletions":2,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -1361,1 +1361,7 @@\n-  size_t size = ShenandoahForwarding::size(p);\n+\n+  markWord mark = p->mark();\n+  if (ShenandoahForwarding::is_forwarded(mark)) {\n+    return ShenandoahForwarding::get_forwardee(p);\n+  }\n+  size_t old_size = ShenandoahForwarding::size(p);\n+  size_t size = p->copy_size(old_size, mark);\n@@ -1391,1 +1397,1 @@\n-  Copy::aligned_disjoint_words(cast_from_oop<HeapWord*>(p), copy, size);\n+  Copy::aligned_disjoint_words(cast_from_oop<HeapWord*>(p), copy, old_size);\n@@ -1398,0 +1404,1 @@\n+    copy_val->initialize_hash_if_necessary(p);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.cpp","additions":9,"deletions":2,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -327,1 +327,2 @@\n-  const size_t size = ZUtils::object_size(from_addr);\n+  const size_t old_size = ZUtils::object_size(from_addr);\n+  const size_t size = ZUtils::copy_size(from_addr, old_size);\n@@ -337,0 +338,1 @@\n+  assert(to_addr != from_addr, \"addresses must be different\");\n@@ -339,1 +341,2 @@\n-  ZUtils::object_copy_disjoint(from_addr, to_addr, size);\n+  ZUtils::object_copy_disjoint(from_addr, to_addr, old_size);\n+  ZUtils::initialize_hash_if_necessary(to_addr, from_addr);\n@@ -593,1 +596,1 @@\n-  zaddress try_relocate_object_inner(zaddress from_addr) {\n+  zaddress try_relocate_object_inner(zaddress from_addr, size_t old_size) {\n@@ -595,2 +598,4 @@\n-\n-    const size_t size = ZUtils::object_size(from_addr);\n+    zoffset_end from_offset = to_zoffset_end(ZAddress::offset(from_addr));\n+    zoffset_end top = to_page != nullptr ? to_page->top() : to_zoffset_end(0);\n+    const size_t new_size = ZUtils::copy_size(from_addr, old_size);\n+    const size_t size = top == from_offset ? old_size : new_size;\n@@ -615,0 +620,4 @@\n+    if (old_size != new_size && ((top == from_offset) != (allocated_addr == from_addr))) {\n+      _allocator->undo_alloc_object(to_page, allocated_addr, size);\n+      return zaddress::null;\n+    }\n@@ -618,2 +627,2 @@\n-    if (_forwarding->in_place_relocation() && allocated_addr + size > from_addr) {\n-      ZUtils::object_copy_conjoint(from_addr, allocated_addr, size);\n+    if (_forwarding->in_place_relocation() && allocated_addr + old_size > from_addr) {\n+      ZUtils::object_copy_conjoint(from_addr, allocated_addr, old_size);\n@@ -621,1 +630,4 @@\n-      ZUtils::object_copy_disjoint(from_addr, allocated_addr, size);\n+      ZUtils::object_copy_disjoint(from_addr, allocated_addr, old_size);\n+    }\n+    if (from_addr != allocated_addr) {\n+      ZUtils::initialize_hash_if_necessary(allocated_addr, from_addr);\n@@ -635,1 +647,1 @@\n-  void update_remset_old_to_old(zaddress from_addr, zaddress to_addr) const {\n+  void update_remset_old_to_old(zaddress from_addr, zaddress to_addr, size_t size) const {\n@@ -657,4 +669,2 @@\n-\n-    \/\/ Read the size from the to-object, since the from-object\n-    \/\/ could have been overwritten during in-place relocation.\n-    const size_t size = ZUtils::object_size(to_addr);\n+    assert(size <= ZUtils::object_size(to_addr), \"old size must be <= new size\");\n+    assert(size > 0, \"size must be set\");\n@@ -785,1 +795,1 @@\n-  void update_remset_for_fields(zaddress from_addr, zaddress to_addr) const {\n+  void update_remset_for_fields(zaddress from_addr, zaddress to_addr, size_t size) const {\n@@ -793,1 +803,1 @@\n-      update_remset_old_to_old(from_addr, to_addr);\n+      update_remset_old_to_old(from_addr, to_addr, size);\n@@ -809,1 +819,2 @@\n-    const zaddress to_addr = try_relocate_object_inner(from_addr);\n+    size_t size = ZUtils::object_size(from_addr);\n+    const zaddress to_addr = try_relocate_object_inner(from_addr, size);\n@@ -815,1 +826,1 @@\n-    update_remset_for_fields(from_addr, to_addr);\n+    update_remset_for_fields(from_addr, to_addr, size);\n","filename":"src\/hotspot\/share\/gc\/z\/zRelocate.cpp","additions":28,"deletions":17,"binary":false,"changes":45,"status":"modified"},{"patch":"@@ -45,0 +45,3 @@\n+  static size_t copy_size(zaddress addr, size_t size);\n+  static void initialize_hash_if_necessary(zaddress to_addr, zaddress from_addr);\n+\n","filename":"src\/hotspot\/share\/gc\/z\/zUtils.hpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -62,0 +62,9 @@\n+inline size_t ZUtils::copy_size(zaddress addr, size_t old_size) {\n+  oop obj = to_oop(addr);\n+  return words_to_bytes(obj->copy_size(bytes_to_words(old_size), obj->mark()));\n+}\n+\n+inline void ZUtils::initialize_hash_if_necessary(zaddress to_addr, zaddress from_addr) {\n+  to_oop(to_addr)->initialize_hash_if_necessary(to_oop(from_addr));\n+}\n+\n","filename":"src\/hotspot\/share\/gc\/z\/zUtils.inline.hpp","additions":9,"deletions":0,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -453,1 +453,1 @@\n-  if (offset == oopDesc::klass_offset_in_bytes()) {\n+  if (offset == 1 \/*oopDesc::klass_offset_in_bytes()*\/) {\n","filename":"src\/hotspot\/share\/jvmci\/jvmciCompilerToVM.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -568,1 +568,1 @@\n-          oop m = java_lang_Class::create_basic_type_mirror(type2name(bt), bt, CHECK);\n+          oop m = java_lang_Class::create_basic_type_mirror(type2name(bt), bt, false, CHECK);\n","filename":"src\/hotspot\/share\/memory\/universe.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -548,1 +548,2 @@\n-  _init_thread(nullptr)\n+  _init_thread(nullptr),\n+  _hash_offset(parser.hash_offset())\n@@ -3800,1 +3801,1 @@\n-  st->print_cr(BULLET\"---- fields (total size %zu words):\", oop_size(obj));\n+  st->print_cr(BULLET\"---- fields (total size %zu words):\", oop_size(obj, obj->mark()));\n","filename":"src\/hotspot\/share\/oops\/instanceKlass.cpp","additions":3,"deletions":2,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -57,0 +57,1 @@\n+#include \"utilities\/numberSeq.hpp\"\n@@ -1351,0 +1352,13 @@\n+\n+static int expanded = 0;\n+static int not_expanded = 0;\n+static NumberSeq seq = NumberSeq();\n+\n+bool Klass::expand_for_hash(oop obj, markWord m) const {\n+  assert(UseCompactObjectHeaders, \"only with compact i-hash\");\n+  {\n+    ResourceMark rm;\n+    assert((size_t)hash_offset_in_bytes(obj,m ) <= (obj->base_size_given_klass(m, this) * HeapWordSize), \"hash offset must be eq or lt base size: hash offset: %d, base size: %zu, class-name: %s\", hash_offset_in_bytes(obj, m), obj->base_size_given_klass(m, this) * HeapWordSize, external_name());\n+  }\n+  return obj->base_size_given_klass(m, this) * HeapWordSize - hash_offset_in_bytes(obj, m) < (int)sizeof(uint32_t);\n+}\n","filename":"src\/hotspot\/share\/oops\/klass.cpp","additions":14,"deletions":0,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -73,0 +73,1 @@\n+  inline void set_mark_full(markWord m);\n@@ -114,0 +115,22 @@\n+  \/\/ Returns the size that a copy of this object requires, in machine words.\n+  \/\/ It can be 1 word larger than its current size to accomodate\n+  \/\/ an additional 4-byte-field for the identity hash-code.\n+  \/\/\n+  \/\/ size: the current size of this object, we're passing this here for performance\n+  \/\/       reasons, because all callers compute this anyway, and we want to avoid\n+  \/\/       recomputing it.\n+  \/\/ mark: the mark-word of this object. Some callers (e.g. G1ParScanThreadState::do_copy_to_survivor_space())\n+  \/\/       need to use a known markWord because of racing GC threads that can change\n+  \/\/       the markWord at any time.\n+  inline size_t copy_size(size_t size, markWord mark) const;\n+  \/\/ Special version to deal with scratch classes in CDS. There we allocate\n+  \/\/ temporary scratch classes (which are skeleton versions of InstanceMirrorKlass,\n+  \/\/ which represent java.lang.Class objects in the CDS archive). At that point, we\n+  \/\/ don't know whether or not the final archived version will be hashed or expanded,\n+  \/\/ and therefore we allocate them in the special state not-hashed-but-expanded.\n+  \/\/ When creating the final copy of those objects, we either populate the hidden hash\n+  \/\/ field and make the object 'expanded', or we turn it back to 'not-hashed'\n+  \/\/ and reduce the object's size. We do this by providing a separate method for CDS\n+  \/\/ so that we don't affect GC performance.\n+  inline size_t copy_size_cds(size_t size, markWord mark) const;\n+\n@@ -116,1 +139,2 @@\n-  inline size_t size_given_klass(Klass* klass);\n+  inline size_t base_size_given_klass(markWord m, const Klass* klass);\n+  inline size_t size_given_mark_and_klass(markWord mrk, const Klass* kls);\n@@ -284,0 +308,1 @@\n+  inline void reset_forwarded();\n@@ -314,0 +339,6 @@\n+  \/\/ Initialize identity hash code in hash word of object copy from original object.\n+  \/\/ Returns true if the object has been expanded, false otherwise.\n+  inline void initialize_hash_if_necessary(oop obj);\n+  \/\/ For CDS only.\n+  void initialize_hash_if_necessary(oop obj, Klass* k, markWord m);\n+\n@@ -330,10 +361,2 @@\n-#ifdef _LP64\n-    if (UseCompactObjectHeaders) {\n-      \/\/ NOTE: The only places where this is used with compact headers are the C2\n-      \/\/ compiler and JVMCI.\n-      return mark_offset_in_bytes() + markWord::klass_offset_in_bytes;\n-    } else\n-#endif\n-    {\n-      return (int)offset_of(oopDesc, _metadata._klass);\n-    }\n+    assert(!UseCompactObjectHeaders, \"don't use this with compact headers\");\n+    return (int)offset_of(oopDesc, _metadata._klass);\n@@ -343,1 +366,5 @@\n-    return klass_offset_in_bytes() + sizeof(narrowKlass);\n+    if (UseCompactObjectHeaders) {\n+      return base_offset_in_bytes();\n+    } else {\n+      return klass_offset_in_bytes() + sizeof(narrowKlass);\n+    }\n","filename":"src\/hotspot\/share\/oops\/oop.hpp","additions":39,"deletions":12,"binary":false,"changes":51,"status":"modified"},{"patch":"@@ -1370,1 +1370,1 @@\n-      } else if( offset == oopDesc::klass_offset_in_bytes() ) {\n+      } else if( offset == Type::klass_offset() ) {\n@@ -1543,1 +1543,1 @@\n-          (offset == oopDesc::klass_offset_in_bytes() && tj->base() == Type::AryPtr) ||\n+          (offset == Type::klass_offset() && tj->base() == Type::AryPtr) ||\n","filename":"src\/hotspot\/share\/opto\/compile.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -3445,1 +3445,1 @@\n-  } else if (offset != oopDesc::klass_offset_in_bytes()) {\n+  } else if (offset != Type::klass_offset()) {\n@@ -4444,1 +4444,1 @@\n-      _compile->get_alias_index(tinst->add_offset(oopDesc::klass_offset_in_bytes()));\n+      _compile->get_alias_index(tinst->add_offset(Type::klass_offset()));\n","filename":"src\/hotspot\/share\/opto\/escape.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -1217,1 +1217,1 @@\n-  Node* k_adr = basic_plus_adr(obj, oopDesc::klass_offset_in_bytes());\n+  Node* k_adr = basic_plus_adr(obj, Type::klass_offset());\n@@ -3636,1 +3636,1 @@\n-    set_memory(minit_out, C->get_alias_index(oop_type->add_offset(oopDesc::klass_offset_in_bytes())));\n+    set_memory(minit_out, C->get_alias_index(oop_type->add_offset(Type::klass_offset())));\n","filename":"src\/hotspot\/share\/opto\/graphKit.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -4689,1 +4689,1 @@\n-  enum { _slow_path = 1, _fast_path, _null_path, PATH_LIMIT };\n+  enum { _slow_path = 1, _null_path, _fast_path, _fast_path2, PATH_LIMIT };\n@@ -4737,6 +4737,34 @@\n-  \/\/ Get the header out of the object, use LoadMarkNode when available\n-  Node* header_addr = basic_plus_adr(obj, oopDesc::mark_offset_in_bytes());\n-  \/\/ The control of the load must be null. Otherwise, the load can move before\n-  \/\/ the null check after castPP removal.\n-  Node* no_ctrl = nullptr;\n-  Node* header = make_load(no_ctrl, header_addr, TypeX_X, TypeX_X->basic_type(), MemNode::unordered);\n+  if (UseCompactObjectHeaders) {\n+    \/\/ Get the header out of the object.\n+    Node* header_addr = basic_plus_adr(obj, oopDesc::mark_offset_in_bytes());\n+    \/\/ The control of the load must be null. Otherwise, the load can move before\n+    \/\/ the null check after castPP removal.\n+    Node* no_ctrl = nullptr;\n+    Node* header = make_load(no_ctrl, header_addr, TypeX_X, TypeX_X->basic_type(), MemNode::unordered);\n+\n+    \/\/ Test the header to see if the object is in hashed or copied state.\n+    Node* hashctrl_mask  = _gvn.MakeConX(markWord::hashctrl_mask_in_place);\n+    Node* masked_header  = _gvn.transform(new AndXNode(header, hashctrl_mask));\n+\n+    \/\/ Take slow-path when the object has not been hashed.\n+    Node* not_hashed_val = _gvn.MakeConX(0);\n+    Node* chk_hashed     = _gvn.transform(new CmpXNode(masked_header, not_hashed_val));\n+    Node* test_hashed    = _gvn.transform(new BoolNode(chk_hashed, BoolTest::eq));\n+\n+    generate_slow_guard(test_hashed, slow_region);\n+\n+    \/\/ Test whether the object is hashed or hashed&copied.\n+    Node* hashed_copied = _gvn.MakeConX(markWord::hashctrl_expanded_mask_in_place | markWord::hashctrl_hashed_mask_in_place);\n+    Node* chk_copied    = _gvn.transform(new CmpXNode(masked_header, hashed_copied));\n+    \/\/ If true, then object has been hashed&copied, otherwise it's only hashed.\n+    Node* test_copied   = _gvn.transform(new BoolNode(chk_copied, BoolTest::eq));\n+    IfNode* if_copied   = create_and_map_if(control(), test_copied, PROB_FAIR, COUNT_UNKNOWN);\n+    Node* if_true = _gvn.transform(new IfTrueNode(if_copied));\n+    Node* if_false = _gvn.transform(new IfFalseNode(if_copied));\n+\n+    \/\/ Hashed&Copied path: read hash-code out of the object.\n+    set_control(if_true);\n+    \/\/ result_val->del_req(_fast_path2);\n+    \/\/ result_reg->del_req(_fast_path2);\n+    \/\/ result_io->del_req(_fast_path2);\n+    \/\/ result_mem->del_req(_fast_path2);\n@@ -4744,8 +4772,19 @@\n-  if (!UseObjectMonitorTable) {\n-    \/\/ Test the header to see if it is safe to read w.r.t. locking.\n-    Node *lock_mask      = _gvn.MakeConX(markWord::lock_mask_in_place);\n-    Node *lmasked_header = _gvn.transform(new AndXNode(header, lock_mask));\n-    if (LockingMode == LM_LIGHTWEIGHT) {\n-      Node *monitor_val   = _gvn.MakeConX(markWord::monitor_value);\n-      Node *chk_monitor   = _gvn.transform(new CmpXNode(lmasked_header, monitor_val));\n-      Node *test_monitor  = _gvn.transform(new BoolNode(chk_monitor, BoolTest::eq));\n+    Node* obj_klass = load_object_klass(obj);\n+    Node* hash_addr;\n+    const TypeKlassPtr* klass_t = _gvn.type(obj_klass)->isa_klassptr();\n+    bool load_offset_runtime = true;\n+\n+    if (klass_t != nullptr) {\n+      if (klass_t->klass_is_exact()  && klass_t->isa_instklassptr()) {\n+        ciInstanceKlass* ciKlass = reinterpret_cast<ciInstanceKlass*>(klass_t->is_instklassptr()->exact_klass());\n+        if (!ciKlass->is_mirror_instance_klass() && !ciKlass->is_reference_instance_klass()) {\n+          \/\/ We know the InstanceKlass, load hash_offset from there at compile-time.\n+          int hash_offset = ciKlass->hash_offset_in_bytes();\n+          hash_addr = basic_plus_adr(obj, hash_offset);\n+          Node* loaded_hash = make_load(control(), hash_addr, TypeInt::INT, T_INT, MemNode::unordered);\n+          result_val->init_req(_fast_path2, loaded_hash);\n+          result_reg->init_req(_fast_path2, control());\n+          load_offset_runtime = false;\n+        }\n+      }\n+    }\n@@ -4753,5 +4792,22 @@\n-      generate_slow_guard(test_monitor, slow_region);\n-    } else {\n-      Node *unlocked_val      = _gvn.MakeConX(markWord::unlocked_value);\n-      Node *chk_unlocked      = _gvn.transform(new CmpXNode(lmasked_header, unlocked_val));\n-      Node *test_not_unlocked = _gvn.transform(new BoolNode(chk_unlocked, BoolTest::ne));\n+    \/\/tty->print_cr(\"Load hash-offset at runtime: %s\", BOOL_TO_STR(load_offset_runtime));\n+\n+    if (load_offset_runtime) {\n+      \/\/ We don't know if it is an array or an exact type, figure it out at run-time.\n+      \/\/ If not an ordinary instance, then we need to take slow-path.\n+      Node* kind_addr = basic_plus_adr(obj_klass, Klass::kind_offset_in_bytes());\n+      Node* kind = make_load(control(), kind_addr, TypeInt::INT, T_INT, MemNode::unordered);\n+      Node* instance_val = _gvn.intcon(Klass::InstanceKlassKind);\n+      Node* chk_inst     = _gvn.transform(new CmpINode(kind, instance_val));\n+      Node* test_inst    = _gvn.transform(new BoolNode(chk_inst, BoolTest::ne));\n+      generate_slow_guard(test_inst, slow_region);\n+\n+      \/\/ Otherwise it's an instance and we can read the hash_offset from the InstanceKlass.\n+      Node* hash_offset_addr = basic_plus_adr(obj_klass, InstanceKlass::hash_offset_offset_in_bytes());\n+      Node* hash_offset = make_load(control(), hash_offset_addr, TypeInt::INT, T_INT, MemNode::unordered);\n+      \/\/ hash_offset->dump();\n+      Node* hash_addr = basic_plus_adr(obj, ConvI2X(hash_offset));\n+      Compile::current()->set_has_unsafe_access(true);\n+      Node* loaded_hash = make_load(control(), hash_addr, TypeInt::INT, T_INT, MemNode::unordered);\n+      result_val->init_req(_fast_path2, loaded_hash);\n+      result_reg->init_req(_fast_path2, control());\n+    }\n@@ -4759,1 +4815,78 @@\n-      generate_slow_guard(test_not_unlocked, slow_region);\n+    \/\/ Hashed-only path: recompute hash-code from object address.\n+    set_control(if_false);\n+    \/\/ Our constants.\n+    Node* M = _gvn.intcon(0x337954D5);\n+    Node* A = _gvn.intcon(0xAAAAAAAA);\n+    \/\/ Split object address into lo and hi 32 bits.\n+    Node* obj_addr = _gvn.transform(new CastP2XNode(nullptr, obj));\n+    Node* x = _gvn.transform(new ConvL2INode(obj_addr));\n+    Node* upper_addr = _gvn.transform(new URShiftLNode(obj_addr, _gvn.intcon(32)));\n+    Node* y = _gvn.transform(new ConvL2INode(upper_addr));\n+\n+    Node* H0 = _gvn.transform(new XorINode(x, y));\n+    Node* L0 = _gvn.transform(new XorINode(x, A));\n+\n+    \/\/ Full multiplication of two 32 bit values L0 and M into a hi\/lo result in two 32 bit values V0 and U0.\n+    Node* L0_64 = _gvn.transform(new ConvI2LNode(L0));\n+    L0_64 = _gvn.transform(new AndLNode(L0_64, _gvn.longcon(0xFFFFFFFF)));\n+    Node* M_64 = _gvn.transform(new ConvI2LNode(M));\n+    \/\/ M_64 = _gvn.transform(new AndLNode(M_64, _gvn.longcon(0xFFFFFFFF)));\n+    Node* prod64 = _gvn.transform(new MulLNode(L0_64, M_64));\n+    Node* V0 = _gvn.transform(new ConvL2INode(prod64));\n+    Node* prod_upper = _gvn.transform(new URShiftLNode(prod64, _gvn.intcon(32)));\n+    Node* U0 = _gvn.transform(new ConvL2INode(prod_upper));\n+\n+    Node* Q0 = _gvn.transform(new MulINode(H0, M));\n+    Node* L1 = _gvn.transform(new XorINode(Q0, U0));\n+\n+    \/\/ Full multiplication of two 32 bit values L1 and M into a hi\/lo result in two 32 bit values V1 and U1.\n+    Node* L1_64 = _gvn.transform(new ConvI2LNode(L1));\n+    L1_64 = _gvn.transform(new AndLNode(L1_64, _gvn.longcon(0xFFFFFFFF)));\n+    prod64 = _gvn.transform(new MulLNode(L1_64, M_64));\n+    Node* V1 = _gvn.transform(new ConvL2INode(prod64));\n+    prod_upper = _gvn.transform(new URShiftLNode(prod64, _gvn.intcon(32)));\n+    Node* U1 = _gvn.transform(new ConvL2INode(prod_upper));\n+\n+    Node* P1 = _gvn.transform(new XorINode(V0, M));\n+\n+    \/\/ Right rotate P1 by distance L1.\n+    Node* distance = _gvn.transform(new AndINode(L1, _gvn.intcon(32 - 1)));\n+    Node* inverse_distance = _gvn.transform(new SubINode(_gvn.intcon(32), distance));\n+    Node* ror_part1 = _gvn.transform(new URShiftINode(P1, distance));\n+    Node* ror_part2 = _gvn.transform(new LShiftINode(P1, inverse_distance));\n+    Node* Q1 = _gvn.transform(new OrINode(ror_part1, ror_part2));\n+\n+    Node* L2 = _gvn.transform(new XorINode(Q1, U1));\n+    Node* hash = _gvn.transform(new XorINode(V1, L2));\n+    Node* hash_truncated = _gvn.transform(new AndINode(hash, _gvn.intcon(markWord::hash_mask)));\n+\n+    \/\/ TODO: We could generate a fast case here under the following conditions:\n+    \/\/ - The hashctrl is set to hash_is_copied (see markWord::hash_is_copied())\n+    \/\/ - The type of the object is known\n+    \/\/ Then we can load the identity hashcode from the int field at Klass::hash_offset_in_bytes() of the object.\n+    result_val->init_req(_fast_path, hash_truncated);\n+  } else {\n+    \/\/ Get the header out of the object, use LoadMarkNode when available\n+    Node* header_addr = basic_plus_adr(obj, oopDesc::mark_offset_in_bytes());\n+    \/\/ The control of the load must be null. Otherwise, the load can move before\n+    \/\/ the null check after castPP removal.\n+    Node* no_ctrl = nullptr;\n+    Node* header = make_load(no_ctrl, header_addr, TypeX_X, TypeX_X->basic_type(), MemNode::unordered);\n+\n+    if (!UseObjectMonitorTable) {\n+      \/\/ Test the header to see if it is safe to read w.r.t. locking.\n+      Node *lock_mask      = _gvn.MakeConX(markWord::lock_mask_in_place);\n+      Node *lmasked_header = _gvn.transform(new AndXNode(header, lock_mask));\n+      if (LockingMode == LM_LIGHTWEIGHT) {\n+        Node *monitor_val   = _gvn.MakeConX(markWord::monitor_value);\n+        Node *chk_monitor   = _gvn.transform(new CmpXNode(lmasked_header, monitor_val));\n+        Node *test_monitor  = _gvn.transform(new BoolNode(chk_monitor, BoolTest::eq));\n+\n+        generate_slow_guard(test_monitor, slow_region);\n+      } else {\n+        Node *unlocked_val      = _gvn.MakeConX(markWord::unlocked_value);\n+        Node *chk_unlocked      = _gvn.transform(new CmpXNode(lmasked_header, unlocked_val));\n+        Node *test_not_unlocked = _gvn.transform(new BoolNode(chk_unlocked, BoolTest::ne));\n+\n+        generate_slow_guard(test_not_unlocked, slow_region);\n+      }\n@@ -4761,14 +4894,13 @@\n-  }\n-  \/\/ Get the hash value and check to see that it has been properly assigned.\n-  \/\/ We depend on hash_mask being at most 32 bits and avoid the use of\n-  \/\/ hash_mask_in_place because it could be larger than 32 bits in a 64-bit\n-  \/\/ vm: see markWord.hpp.\n-  Node *hash_mask      = _gvn.intcon(markWord::hash_mask);\n-  Node *hash_shift     = _gvn.intcon(markWord::hash_shift);\n-  Node *hshifted_header= _gvn.transform(new URShiftXNode(header, hash_shift));\n-  \/\/ This hack lets the hash bits live anywhere in the mark object now, as long\n-  \/\/ as the shift drops the relevant bits into the low 32 bits.  Note that\n-  \/\/ Java spec says that HashCode is an int so there's no point in capturing\n-  \/\/ an 'X'-sized hashcode (32 in 32-bit build or 64 in 64-bit build).\n-  hshifted_header      = ConvX2I(hshifted_header);\n-  Node *hash_val       = _gvn.transform(new AndINode(hshifted_header, hash_mask));\n+    \/\/ Get the hash value and check to see that it has been properly assigned.\n+    \/\/ We depend on hash_mask being at most 32 bits and avoid the use of\n+    \/\/ hash_mask_in_place because it could be larger than 32 bits in a 64-bit\n+    \/\/ vm: see markWord.hpp.\n+    Node *hash_mask      = _gvn.intcon(markWord::hash_mask);\n+    Node *hash_shift     = _gvn.intcon(markWord::hash_shift);\n+    Node *hshifted_header= _gvn.transform(new URShiftXNode(header, hash_shift));\n+    \/\/ This hack lets the hash bits live anywhere in the mark object now, as long\n+    \/\/ as the shift drops the relevant bits into the low 32 bits.  Note that\n+    \/\/ Java spec says that HashCode is an int so there's no point in capturing\n+    \/\/ an 'X'-sized hashcode (32 in 32-bit build or 64 in 64-bit build).\n+    hshifted_header      = ConvX2I(hshifted_header);\n+    Node *hash_val       = _gvn.transform(new AndINode(hshifted_header, hash_mask));\n@@ -4777,3 +4909,3 @@\n-  Node *no_hash_val    = _gvn.intcon(markWord::no_hash);\n-  Node *chk_assigned   = _gvn.transform(new CmpINode( hash_val, no_hash_val));\n-  Node *test_assigned  = _gvn.transform(new BoolNode( chk_assigned, BoolTest::eq));\n+    Node *no_hash_val    = _gvn.intcon(markWord::no_hash);\n+    Node *chk_assigned   = _gvn.transform(new CmpINode( hash_val, no_hash_val));\n+    Node *test_assigned  = _gvn.transform(new BoolNode( chk_assigned, BoolTest::eq));\n@@ -4781,1 +4913,10 @@\n-  generate_slow_guard(test_assigned, slow_region);\n+    generate_slow_guard(test_assigned, slow_region);\n+\n+    result_val->init_req(_fast_path, hash_val);\n+\n+    \/\/ _fast_path2 is not used here.\n+    result_val->del_req(_fast_path2);\n+    result_reg->del_req(_fast_path2);\n+    result_io->del_req(_fast_path2);\n+    result_mem->del_req(_fast_path2);\n+  }\n@@ -4788,1 +4929,0 @@\n-  result_val->init_req(_fast_path, hash_val);\n@@ -4793,0 +4933,5 @@\n+  if (UseCompactObjectHeaders) {\n+    result_io->init_req(_fast_path2, i_o());\n+    result_mem->init_req(_fast_path2, init_mem);\n+  }\n+\n@@ -4794,0 +4939,1 @@\n+  assert(slow_region != nullptr, \"must have slow_region\");\n","filename":"src\/hotspot\/share\/opto\/library_call.cpp","additions":186,"deletions":40,"binary":false,"changes":226,"status":"modified"},{"patch":"@@ -1713,1 +1713,1 @@\n-    rawmem = make_store(control, rawmem, object, oopDesc::klass_offset_in_bytes(), klass_node, T_METADATA);\n+    rawmem = make_store(control, rawmem, object, Type::klass_offset(), klass_node, T_METADATA);\n@@ -2340,1 +2340,1 @@\n-      Node* k_adr = basic_plus_adr(obj_or_subklass, oopDesc::klass_offset_in_bytes());\n+      Node* k_adr = basic_plus_adr(obj_or_subklass, Type::klass_offset());\n","filename":"src\/hotspot\/share\/opto\/macro.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -265,1 +265,1 @@\n-          adr_check->offset() == oopDesc::klass_offset_in_bytes() ||\n+          adr_check->offset() == Type::klass_offset() ||\n@@ -953,1 +953,1 @@\n-           adr_type->offset() == oopDesc::klass_offset_in_bytes()),\n+           adr_type->offset() == Type::klass_offset()),\n@@ -2464,1 +2464,1 @@\n-    if (offset == oopDesc::klass_offset_in_bytes()) {\n+    if (offset == Type::klass_offset()) {\n@@ -2472,1 +2472,1 @@\n-      tary->offset() == oopDesc::klass_offset_in_bytes()) {\n+      tary->offset() == Type::klass_offset()) {\n@@ -2538,1 +2538,1 @@\n-  if (offset == oopDesc::klass_offset_in_bytes()) {\n+  if (offset == Type::klass_offset()) {\n","filename":"src\/hotspot\/share\/opto\/memnode.cpp","additions":5,"deletions":5,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -2122,1 +2122,1 @@\n-  Node* klass_addr = basic_plus_adr( receiver, receiver, oopDesc::klass_offset_in_bytes() );\n+  Node* klass_addr = basic_plus_adr( receiver, receiver, Type::klass_offset() );\n","filename":"src\/hotspot\/share\/opto\/parse1.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -435,1 +435,1 @@\n-    const size_t size = TypeArrayKlass::cast(array_type)->oop_size(result);\n+    const size_t size = TypeArrayKlass::cast(array_type)->oop_size(result, result->mark());\n","filename":"src\/hotspot\/share\/opto\/runtime.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -1186,1 +1186,1 @@\n-  if (con2 == oopDesc::klass_offset_in_bytes()) {\n+  if (con2 == Type::klass_offset()) {\n","filename":"src\/hotspot\/share\/opto\/subnode.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -575,1 +575,1 @@\n-                                           false, nullptr, oopDesc::klass_offset_in_bytes());\n+                                           false, nullptr, Type::klass_offset());\n@@ -3716,1 +3716,1 @@\n-    if (_offset == oopDesc::klass_offset_in_bytes()) {\n+    if (_offset == Type::klass_offset()) {\n","filename":"src\/hotspot\/share\/opto\/type.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -198,0 +198,11 @@\n+  \/\/ This is used as a marker to identify narrow Klass* loads, which\n+  \/\/ are really extracted from the mark-word, but we still want to\n+  \/\/ distinguish it.\n+  static int klass_offset() {\n+    if (UseCompactObjectHeaders) {\n+      return 1;\n+    } else {\n+      return oopDesc::klass_offset_in_bytes();\n+    }\n+  }\n+\n@@ -1438,1 +1449,1 @@\n-        _offset != arrayOopDesc::klass_offset_in_bytes()) {\n+        _offset != Type::klass_offset()) {\n","filename":"src\/hotspot\/share\/opto\/type.hpp","additions":12,"deletions":1,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -3689,0 +3689,3 @@\n+  if (UseCompactObjectHeaders && FLAG_IS_DEFAULT(hashCode)) {\n+    hashCode = 6;\n+  }\n","filename":"src\/hotspot\/share\/runtime\/arguments.cpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -1393,1 +1393,1 @@\n-  product(size_t, CompressedClassSpaceSize, 1*G,                            \\\n+  product(size_t, CompressedClassSpaceSize, 128*M,                          \\\n","filename":"src\/hotspot\/share\/runtime\/globals.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -51,0 +51,12 @@\n+static uintx objhash(oop obj) {\n+  if (UseCompactObjectHeaders) {\n+    uintx hash = LightweightSynchronizer::get_hash(obj->mark(), obj);\n+    assert(hash != 0, \"should have a hash\");\n+    return hash;\n+  } else {\n+    uintx hash = obj->mark().hash();\n+    assert(hash != 0, \"should have a hash\");\n+    return hash;\n+  }\n+}\n+\n@@ -81,3 +93,1 @@\n-      uintx hash = _obj->mark().hash();\n-      assert(hash != 0, \"should have a hash\");\n-      return hash;\n+      return objhash(_obj);\n@@ -286,0 +296,1 @@\n+      assert(objhash(obj) == (uintx)(*found)->hash(), \"hash must match\");\n@@ -318,1 +329,1 @@\n-       assert(obj->mark().hash() == om->hash(), \"hash must match\");\n+       assert(objhash(obj) == (uintx)om->hash(), \"hash must match\");\n@@ -408,1 +419,1 @@\n-  intptr_t hash = obj->mark().hash();\n+  intptr_t hash = objhash(obj);\n@@ -1244,0 +1255,18 @@\n+\n+uint32_t LightweightSynchronizer::get_hash(markWord mark, oop obj, Klass* klass) {\n+  assert(UseCompactObjectHeaders, \"Only with compact i-hash\");\n+  \/\/assert(mark.is_neutral() | mark.is_fast_locked(), \"only from neutral or fast-locked mark: \" INTPTR_FORMAT, mark.value());\n+  assert(mark.is_hashed(), \"only from hashed or copied object\");\n+  if (mark.is_hashed_expanded()) {\n+    return obj->int_field(klass->hash_offset_in_bytes(obj, mark));\n+  } else {\n+    assert(mark.is_hashed_not_expanded(), \"must be hashed\");\n+    assert(hashCode == 6 || hashCode == 2, \"must have idempotent hashCode\");\n+    \/\/ Already marked as hashed, but not yet copied. Recompute hash and return it.\n+    return ObjectSynchronizer::get_next_hash(nullptr, obj); \/\/ recompute hash\n+  }\n+}\n+\n+uint32_t LightweightSynchronizer::get_hash(markWord mark, oop obj) {\n+  return get_hash(mark, obj, mark.klass());\n+}\n","filename":"src\/hotspot\/share\/runtime\/lightweightSynchronizer.cpp","additions":34,"deletions":5,"binary":false,"changes":39,"status":"modified"},{"patch":"@@ -78,0 +78,5 @@\n+\n+  \/\/ NOTE: May not cause monitor inflation\n+  static uint32_t get_hash(markWord mark, oop obj);\n+  \/\/ For CDS path.\n+  static uint32_t get_hash(markWord mark, oop obj, Klass* klass);\n","filename":"src\/hotspot\/share\/runtime\/lightweightSynchronizer.hpp","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -66,0 +66,1 @@\n+#include \"utilities\/fastHash.hpp\"\n@@ -928,1 +929,1 @@\n-static intptr_t get_next_hash(Thread* current, oop obj) {\n+intptr_t ObjectSynchronizer::get_next_hash(Thread* current, oop obj) {\n@@ -947,1 +948,1 @@\n-  } else {\n+  } else if (hashCode == 5) {\n@@ -960,0 +961,10 @@\n+  } else {\n+    assert(UseCompactObjectHeaders, \"Only with compact i-hash\");\n+#ifdef _LP64\n+    uint64_t val = cast_from_oop<uint64_t>(obj);\n+    uint32_t hash = FastHash::get_hash32((uint32_t)val, (uint32_t)(val >> 32));\n+#else\n+    uint32_t val = cast_from_oop<uint32_t>(obj);\n+    uint32_t hash = FastHash::get_hash32(val, UCONST64(0xAAAAAAAA));\n+#endif\n+    value= static_cast<intptr_t>(hash);\n@@ -963,2 +974,2 @@\n-  if (value == 0) value = 0xBAD;\n-  assert(value != markWord::no_hash, \"invariant\");\n+  if (hashCode != 6 && value == 0) value = 0xBAD;\n+  assert(value != markWord::no_hash || hashCode == 6, \"invariant\");\n@@ -973,4 +984,23 @@\n-    intptr_t hash = mark.hash();\n-    if (hash != 0) {\n-      return hash;\n-    }\n+    if (UseCompactObjectHeaders) {\n+      if (mark.is_hashed()) {\n+        return LightweightSynchronizer::get_hash(mark, obj);\n+      }\n+      intptr_t hash = ObjectSynchronizer::get_next_hash(current, obj);  \/\/ get a new hash\n+      markWord new_mark;\n+      if (mark.is_not_hashed_expanded()) {\n+        new_mark = mark.set_hashed_expanded();\n+        int offset = mark.klass()->hash_offset_in_bytes(obj, mark);\n+        obj->int_field_put(offset, (jint) hash);\n+      } else {\n+        new_mark = mark.set_hashed_not_expanded();\n+      }\n+      markWord old_mark = obj->cas_set_mark(new_mark, mark);\n+      if (old_mark == mark) {\n+        return hash;\n+      }\n+      mark = old_mark;\n+    } else {\n+      intptr_t hash = mark.hash();\n+      if (hash != 0) {\n+        return hash;\n+      }\n@@ -978,3 +1008,3 @@\n-    hash = get_next_hash(current, obj);\n-    const markWord old_mark = mark;\n-    const markWord new_mark = old_mark.copy_set_hash(hash);\n+      hash = ObjectSynchronizer::get_next_hash(current, obj);\n+      const markWord old_mark = mark;\n+      const markWord new_mark = old_mark.copy_set_hash(hash);\n@@ -982,3 +1012,4 @@\n-    mark = obj->cas_set_mark(new_mark, old_mark);\n-    if (old_mark == mark) {\n-      return hash;\n+      mark = obj->cas_set_mark(new_mark, old_mark);\n+      if (old_mark == mark) {\n+        return hash;\n+      }\n","filename":"src\/hotspot\/share\/runtime\/synchronizer.cpp","additions":45,"deletions":14,"binary":false,"changes":59,"status":"modified"},{"patch":"@@ -210,0 +210,1 @@\n+  nonstatic_field(InstanceKlass,               _hash_offset,                                  int)                                    \\\n@@ -1818,0 +1819,1 @@\n+  declare_constant(markWord::hashctrl_bits)                               \\\n@@ -1822,0 +1824,1 @@\n+  declare_constant(markWord::hashctrl_shift)                              \\\n@@ -1830,0 +1833,4 @@\n+  declare_constant(markWord::hashctrl_mask)                               \\\n+  declare_constant(markWord::hashctrl_mask_in_place)                      \\\n+  declare_constant(markWord::hashctrl_hashed_mask_in_place)               \\\n+  declare_constant(markWord::hashctrl_expanded_mask_in_place)             \\\n","filename":"src\/hotspot\/share\/runtime\/vmStructs.cpp","additions":7,"deletions":0,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -140,1 +140,1 @@\n-            int expectedShift = 6;\n+            int expectedShift = 9;\n@@ -144,1 +144,1 @@\n-            expectedShift = 8;\n+            expectedShift = 10;\n@@ -148,1 +148,1 @@\n-            expectedShift = 9;\n+            expectedShift = 10;\n","filename":"test\/hotspot\/jtreg\/runtime\/CompressedOops\/CompressedClassPointersEncodingScheme.java","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"}]}