{"files":[{"patch":"@@ -2,1 +2,1 @@\n-\/\/ Copyright (c) 2003, 2024, Oracle and\/or its affiliates. All rights reserved.\n+\/\/ Copyright (c) 2003, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -2299,0 +2299,20 @@\n+    case Op_FmaHF:\n+      \/\/ UseFMA flag also needs to be checked along with FEAT_FP16\n+      if (!UseFMA || !is_feat_fp16_supported()) {\n+        return false;\n+      }\n+      break;\n+    case Op_AddHF:\n+    case Op_SubHF:\n+    case Op_MulHF:\n+    case Op_DivHF:\n+    case Op_MinHF:\n+    case Op_MaxHF:\n+    case Op_SqrtHF:\n+      \/\/ Half-precision floating point scalar operations require FEAT_FP16\n+      \/\/ to be available. FEAT_FP16 is enabled if both \"fphp\" and \"asimdhp\"\n+      \/\/ features are supported.\n+      if (!is_feat_fp16_supported()) {\n+        return false;\n+      }\n+      break;\n@@ -2309,1 +2329,1 @@\n-  return EnableVectorSupport && UseVectorStubs;\n+  return EnableVectorSupport;\n@@ -2313,1 +2333,1 @@\n-  assert(EnableVectorSupport && UseVectorStubs, \"sanity\");\n+  assert(EnableVectorSupport, \"sanity\");\n@@ -4602,0 +4622,9 @@\n+\/\/ Half Float (FP16) Immediate\n+operand immH()\n+%{\n+  match(ConH);\n+  op_cost(0);\n+  format %{ %}\n+  interface(CONST_INTER);\n+%}\n+\n@@ -6949,0 +6978,15 @@\n+\/\/ Load Half Float Constant\n+\/\/ The \"ldr\" instruction loads a 32-bit word from the constant pool into a\n+\/\/ 32-bit register but only the bottom half will be populated and the top\n+\/\/ 16 bits are zero.\n+instruct loadConH(vRegF dst, immH con) %{\n+  match(Set dst con);\n+  format %{\n+    \"ldrs $dst, [$constantaddress]\\t# load from constant table: half float=$con\\n\\t\"\n+  %}\n+  ins_encode %{\n+    __ ldrs(as_FloatRegister($dst$$reg), $constantaddress($con));\n+  %}\n+  ins_pipe(fp_load_constant_s);\n+%}\n+\n@@ -8151,0 +8195,1 @@\n+  predicate(VerifyConstraintCasts == 0);\n@@ -8160,0 +8205,13 @@\n+instruct castII_checked(iRegI dst, rFlagsReg cr)\n+%{\n+  predicate(VerifyConstraintCasts > 0);\n+  match(Set dst (CastII dst));\n+  effect(KILL cr);\n+\n+  format %{ \"# castII_checked of $dst\" %}\n+  ins_encode %{\n+    __ verify_int_in_range(_idx, bottom_type()->is_int(), $dst$$Register, rscratch1);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n@@ -8162,0 +8220,1 @@\n+  predicate(VerifyConstraintCasts == 0);\n@@ -8171,0 +8230,13 @@\n+instruct castLL_checked(iRegL dst, rFlagsReg cr)\n+%{\n+  predicate(VerifyConstraintCasts > 0);\n+  match(Set dst (CastLL dst));\n+  effect(KILL cr);\n+\n+  format %{ \"# castLL_checked of $dst\" %}\n+  ins_encode %{\n+    __ verify_long_in_range(_idx, bottom_type()->is_long(), $dst$$Register, rscratch1);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n@@ -13613,0 +13685,11 @@\n+instruct addHF_reg_reg(vRegF dst, vRegF src1, vRegF src2) %{\n+  match(Set dst (AddHF src1 src2));\n+  format %{ \"faddh $dst, $src1, $src2\" %}\n+  ins_encode %{\n+    __ faddh($dst$$FloatRegister,\n+             $src1$$FloatRegister,\n+             $src2$$FloatRegister);\n+  %}\n+  ins_pipe(fp_dop_reg_reg_s);\n+%}\n+\n@@ -13643,0 +13726,11 @@\n+instruct subHF_reg_reg(vRegF dst, vRegF src1, vRegF src2) %{\n+  match(Set dst (SubHF src1 src2));\n+  format %{ \"fsubh $dst, $src1, $src2\" %}\n+  ins_encode %{\n+    __ fsubh($dst$$FloatRegister,\n+             $src1$$FloatRegister,\n+             $src2$$FloatRegister);\n+  %}\n+  ins_pipe(fp_dop_reg_reg_s);\n+%}\n+\n@@ -13673,0 +13767,11 @@\n+instruct mulHF_reg_reg(vRegF dst, vRegF src1, vRegF src2) %{\n+  match(Set dst (MulHF src1 src2));\n+  format %{ \"fmulh $dst, $src1, $src2\" %}\n+  ins_encode %{\n+    __ fmulh($dst$$FloatRegister,\n+             $src1$$FloatRegister,\n+             $src2$$FloatRegister);\n+  %}\n+  ins_pipe(fp_dop_reg_reg_s);\n+%}\n+\n@@ -13703,0 +13808,14 @@\n+\/\/ src1 * src2 + src3 (half-precision float)\n+instruct maddHF_reg_reg(vRegF dst, vRegF src1, vRegF src2, vRegF src3) %{\n+  match(Set dst (FmaHF src3 (Binary src1 src2)));\n+  format %{ \"fmaddh $dst, $src1, $src2, $src3\" %}\n+  ins_encode %{\n+    assert(UseFMA, \"Needs FMA instructions support.\");\n+    __ fmaddh($dst$$FloatRegister,\n+              $src1$$FloatRegister,\n+              $src2$$FloatRegister,\n+              $src3$$FloatRegister);\n+  %}\n+  ins_pipe(pipe_class_default);\n+%}\n+\n@@ -13844,0 +13963,23 @@\n+\/\/ Math.max(HH)H (half-precision float)\n+instruct maxHF_reg_reg(vRegF dst, vRegF src1, vRegF src2) %{\n+  match(Set dst (MaxHF src1 src2));\n+  format %{ \"fmaxh $dst, $src1, $src2\" %}\n+  ins_encode %{\n+    __ fmaxh($dst$$FloatRegister,\n+             $src1$$FloatRegister,\n+             $src2$$FloatRegister);\n+  %}\n+  ins_pipe(fp_dop_reg_reg_s);\n+%}\n+\n+\/\/ Math.min(HH)H (half-precision float)\n+instruct minHF_reg_reg(vRegF dst, vRegF src1, vRegF src2) %{\n+  match(Set dst (MinHF src1 src2));\n+  format %{ \"fminh $dst, $src1, $src2\" %}\n+  ins_encode %{\n+    __ fminh($dst$$FloatRegister,\n+             $src1$$FloatRegister,\n+             $src2$$FloatRegister);\n+  %}\n+  ins_pipe(fp_dop_reg_reg_s);\n+%}\n@@ -13901,0 +14043,10 @@\n+instruct divHF_reg_reg(vRegF dst, vRegF src1, vRegF src2) %{\n+  match(Set dst (DivHF src1  src2));\n+  format %{ \"fdivh $dst, $src1, $src2\" %}\n+  ins_encode %{\n+    __ fdivh($dst$$FloatRegister,\n+             $src1$$FloatRegister,\n+             $src2$$FloatRegister);\n+  %}\n+  ins_pipe(fp_div_s);\n+%}\n@@ -14074,0 +14226,10 @@\n+instruct sqrtHF_reg(vRegF dst, vRegF src) %{\n+  match(Set dst (SqrtHF src));\n+  format %{ \"fsqrth $dst, $src\" %}\n+  ins_encode %{\n+    __ fsqrth($dst$$FloatRegister,\n+              $src$$FloatRegister);\n+  %}\n+  ins_pipe(fp_div_s);\n+%}\n+\n@@ -16304,1 +16466,2 @@\n-      __ stop(_halt_reason);\n+      const char* str = __ code_string(_halt_reason);\n+      __ stop(str);\n@@ -17122,0 +17285,58 @@\n+\/\/----------------------------- Reinterpret ----------------------------------\n+\/\/ Reinterpret a half-precision float value in a floating point register to a general purpose register\n+instruct reinterpretHF2S(iRegINoSp dst, vRegF src) %{\n+  match(Set dst (ReinterpretHF2S src));\n+  format %{ \"reinterpretHF2S $dst, $src\" %}\n+  ins_encode %{\n+    __ smov($dst$$Register, $src$$FloatRegister, __ H, 0);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ Reinterpret a half-precision float value in a general purpose register to a floating point register\n+instruct reinterpretS2HF(vRegF dst, iRegINoSp src) %{\n+  match(Set dst (ReinterpretS2HF src));\n+  format %{ \"reinterpretS2HF $dst, $src\" %}\n+  ins_encode %{\n+    __ mov($dst$$FloatRegister, __ H, 0, $src$$Register);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ Without this optimization, ReinterpretS2HF (ConvF2HF src) would result in the following\n+\/\/ instructions (the first two are for ConvF2HF and the last instruction is for ReinterpretS2HF) -\n+\/\/ fcvt $tmp1_fpr, $src_fpr    \/\/ Convert float to half-precision float\n+\/\/ mov  $tmp2_gpr, $tmp1_fpr   \/\/ Move half-precision float in FPR to a GPR\n+\/\/ mov  $dst_fpr,  $tmp2_gpr   \/\/ Move the result from a GPR to an FPR\n+\/\/ The move from FPR to GPR in ConvF2HF and the move from GPR to FPR in ReinterpretS2HF\n+\/\/ can be omitted in this pattern, resulting in -\n+\/\/ fcvt $dst, $src  \/\/ Convert float to half-precision float\n+instruct convF2HFAndS2HF(vRegF dst, vRegF src)\n+%{\n+  match(Set dst (ReinterpretS2HF (ConvF2HF src)));\n+  format %{ \"convF2HFAndS2HF $dst, $src\" %}\n+  ins_encode %{\n+    __ fcvtsh($dst$$FloatRegister, $src$$FloatRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ Without this optimization, ConvHF2F (ReinterpretHF2S src) would result in the following\n+\/\/ instructions (the first one is for ReinterpretHF2S and the last two are for ConvHF2F) -\n+\/\/ mov  $tmp1_gpr, $src_fpr  \/\/ Move the half-precision float from an FPR to a GPR\n+\/\/ mov  $tmp2_fpr, $tmp1_gpr \/\/ Move the same value from GPR to an FPR\n+\/\/ fcvt $dst_fpr,  $tmp2_fpr \/\/ Convert the half-precision float to 32-bit float\n+\/\/ The move from FPR to GPR in ReinterpretHF2S and the move from GPR to FPR in ConvHF2F\n+\/\/ can be omitted as the input (src) is already in an FPR required for the fcvths instruction\n+\/\/ resulting in -\n+\/\/ fcvt $dst, $src  \/\/ Convert half-precision float to a 32-bit float\n+instruct convHF2SAndHF2F(vRegF dst, vRegF src)\n+%{\n+  match(Set dst (ConvHF2F (ReinterpretHF2S src)));\n+  format %{ \"convHF2SAndHF2F $dst, $src\" %}\n+  ins_encode %{\n+    __ fcvths($dst$$FloatRegister, $src$$FloatRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/aarch64.ad","additions":225,"deletions":4,"binary":false,"changes":229,"status":"modified"},{"patch":"@@ -852,1 +852,1 @@\n-    get_vm_result(oop_result, java_thread);\n+    get_vm_result_oop(oop_result, java_thread);\n@@ -1006,3 +1006,0 @@\n-  \/\/ address const_ptr = long_constant((jlong)Universe::non_oop_word());\n-  \/\/ uintptr_t offset;\n-  \/\/ ldr_constant(rscratch2, const_ptr);\n@@ -1148,3 +1145,3 @@\n-void MacroAssembler::get_vm_result(Register oop_result, Register java_thread) {\n-  ldr(oop_result, Address(java_thread, JavaThread::vm_result_offset()));\n-  str(zr, Address(java_thread, JavaThread::vm_result_offset()));\n+void MacroAssembler::get_vm_result_oop(Register oop_result, Register java_thread) {\n+  ldr(oop_result, Address(java_thread, JavaThread::vm_result_oop_offset()));\n+  str(zr, Address(java_thread, JavaThread::vm_result_oop_offset()));\n@@ -1154,3 +1151,3 @@\n-void MacroAssembler::get_vm_result_2(Register metadata_result, Register java_thread) {\n-  ldr(metadata_result, Address(java_thread, JavaThread::vm_result_2_offset()));\n-  str(zr, Address(java_thread, JavaThread::vm_result_2_offset()));\n+void MacroAssembler::get_vm_result_metadata(Register metadata_result, Register java_thread) {\n+  ldr(metadata_result, Address(java_thread, JavaThread::vm_result_metadata_offset()));\n+  str(zr, Address(java_thread, JavaThread::vm_result_metadata_offset()));\n@@ -2044,1 +2041,1 @@\n-  subs(zr, scratch, InstanceKlass::fully_initialized);\n+  cmp(scratch, InstanceKlass::fully_initialized);\n@@ -5523,1 +5520,1 @@\n-    ldr_constant(dst, Address(dummy, rspec));\n+    ldr(dst, Address(dummy, rspec));\n@@ -5525,1 +5522,0 @@\n-\n@@ -7033,1 +7029,1 @@\n-    \/\/ Clear cache in case fast locking succeeds.\n+    \/\/ Clear cache in case fast locking succeeds or we need to take the slow-path.\n@@ -7037,0 +7033,7 @@\n+  if (DiagnoseSyncOnValueBasedClasses != 0) {\n+    load_klass(t1, obj);\n+    ldrb(t1, Address(t1, Klass::misc_flags_offset()));\n+    tst(t1, KlassFlags::_misc_is_value_based_class);\n+    br(Assembler::NE, slow);\n+  }\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/macroAssembler_aarch64.cpp","additions":17,"deletions":14,"binary":false,"changes":31,"status":"modified"},{"patch":"@@ -487,1 +487,1 @@\n-  __ get_vm_result_2(flags, rthread);\n+  __ get_vm_result_metadata(flags, rthread);\n@@ -3725,2 +3725,1 @@\n-  \/\/ vm_result_2 has metadata result\n-  __ get_vm_result_2(r0, rthread);\n+  __ get_vm_result_metadata(r0, rthread);\n@@ -3779,2 +3778,1 @@\n-  \/\/ vm_result_2 has metadata result\n-  __ get_vm_result_2(r0, rthread);\n+  __ get_vm_result_metadata(r0, rthread);\n","filename":"src\/hotspot\/cpu\/aarch64\/templateTable_aarch64.cpp","additions":3,"deletions":5,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -1599,1 +1599,2 @@\n-        __ vmv1r_v(as_VectorRegister(Matcher::_regEncode[dst_lo]), as_VectorRegister(Matcher::_regEncode[src_lo]));\n+        __ vsetvli_helper(T_BYTE, MaxVectorSize);\n+        __ vmv_v_v(as_VectorRegister(Matcher::_regEncode[dst_lo]), as_VectorRegister(Matcher::_regEncode[src_lo]));\n@@ -1617,1 +1618,2 @@\n-        __ vmv1r_v(as_VectorRegister(Matcher::_regEncode[dst_lo]), as_VectorRegister(Matcher::_regEncode[src_lo]));\n+        __ vsetvli_helper(T_BYTE, MaxVectorSize >> 3);\n+        __ vmv_v_v(as_VectorRegister(Matcher::_regEncode[dst_lo]), as_VectorRegister(Matcher::_regEncode[src_lo]));\n@@ -1936,0 +1938,6 @@\n+\n+    case Op_CMoveF:\n+    case Op_CMoveD:\n+    case Op_CMoveP:\n+    case Op_CMoveN:\n+      return false;\n@@ -1947,1 +1955,1 @@\n-  return EnableVectorSupport && UseVectorStubs;\n+  return EnableVectorSupport;\n@@ -1951,1 +1959,1 @@\n-  assert(EnableVectorSupport && UseVectorStubs, \"sanity\");\n+  assert(EnableVectorSupport, \"sanity\");\n@@ -4400,0 +4408,6 @@\n+\/\/ The real do-nothing guy\n+pipe_class real_empty()\n+%{\n+    instruction_count(0);\n+%}\n+\n@@ -6443,1 +6457,0 @@\n-    int32_t con = (int32_t)$src2$$constant;\n@@ -6505,1 +6518,0 @@\n-    \/\/ src2 is imm, so actually call the addi\n@@ -6827,1 +6839,1 @@\n-\/\/ In RV64I, only the low 5 bits of src2 are considered for the shift amount\n+\/\/ Only the low 5 bits of src2 are considered for the shift amount, all other bits are ignored.\n@@ -6860,1 +6872,1 @@\n-\/\/ In RV64I, only the low 5 bits of src2 are considered for the shift amount\n+\/\/ Only the low 5 bits of src2 are considered for the shift amount, all other bits are ignored.\n@@ -6893,1 +6905,1 @@\n-\/\/ In RV64I, only the low 5 bits of src2 are considered for the shift amount\n+\/\/ Only the low 5 bits of src2 are considered for the shift amount, all other bits are ignored.\n@@ -6928,1 +6940,1 @@\n-\/\/ In RV64I, only the low 6 bits of src2 are considered for the shift amount\n+\/\/ Only the low 6 bits of src2 are considered for the shift amount, all other bits are ignored.\n@@ -6963,1 +6975,1 @@\n-\/\/ In RV64I, only the low 6 bits of src2 are considered for the shift amount\n+\/\/ Only the low 6 bits of src2 are considered for the shift amount, all other bits are ignored.\n@@ -7016,1 +7028,1 @@\n-\/\/ In RV64I, only the low 6 bits of src2 are considered for the shift amount\n+\/\/ Only the low 6 bits of src2 are considered for the shift amount, all other bits are ignored.\n@@ -7906,1 +7918,4 @@\n-instruct load_fence() %{\n+\/\/ RVTSO\n+\n+instruct unnecessary_membar_rvtso() %{\n+  predicate(UseZtso);\n@@ -7908,1 +7923,9 @@\n-  ins_cost(ALU_COST);\n+  match(StoreFence);\n+  match(StoreStoreFence);\n+  match(MemBarAcquire);\n+  match(MemBarRelease);\n+  match(MemBarStoreStore);\n+  match(MemBarAcquireLock);\n+  match(MemBarReleaseLock);\n+\n+  ins_cost(0);\n@@ -7910,1 +7933,1 @@\n-  format %{ \"#@load_fence\" %}\n+  size(0);\n@@ -7912,0 +7935,1 @@\n+  format %{ \"#@unnecessary_membar_rvtso elided\/tso (empty encoding)\" %}\n@@ -7913,1 +7937,1 @@\n-    __ membar(MacroAssembler::LoadLoad | MacroAssembler::LoadStore);\n+    __ block_comment(\"unnecessary_membar_rvtso\");\n@@ -7915,1 +7939,1 @@\n-  ins_pipe(pipe_serial);\n+  ins_pipe(real_empty);\n@@ -7918,3 +7942,4 @@\n-instruct membar_acquire() %{\n-  match(MemBarAcquire);\n-  ins_cost(ALU_COST);\n+instruct membar_volatile_rvtso() %{\n+  predicate(UseZtso);\n+  match(MemBarVolatile);\n+  ins_cost(VOLATILE_REF_COST);\n@@ -7922,2 +7947,2 @@\n-  format %{ \"#@membar_acquire\\n\\t\"\n-            \"fence ir iorw\" %}\n+  format %{ \"#@membar_volatile_rvtso\\n\\t\"\n+            \"fence w, r\"%}\n@@ -7926,2 +7951,2 @@\n-    __ block_comment(\"membar_acquire\");\n-    __ membar(MacroAssembler::LoadLoad | MacroAssembler::LoadStore);\n+    __ block_comment(\"membar_volatile_rvtso\");\n+    __ membar(MacroAssembler::StoreLoad);\n@@ -7930,1 +7955,1 @@\n-  ins_pipe(pipe_serial);\n+  ins_pipe(pipe_slow);\n@@ -7933,2 +7958,3 @@\n-instruct membar_acquire_lock() %{\n-  match(MemBarAcquireLock);\n+instruct unnecessary_membar_volatile_rvtso() %{\n+  predicate(UseZtso && Matcher::post_store_load_barrier(n));\n+  match(MemBarVolatile);\n@@ -7937,2 +7963,3 @@\n-  format %{ \"#@membar_acquire_lock (elided)\" %}\n-\n+  size(0);\n+  \n+  format %{ \"#@unnecessary_membar_volatile_rvtso (unnecessary so empty encoding)\" %}\n@@ -7940,1 +7967,1 @@\n-    __ block_comment(\"membar_acquire_lock (elided)\");\n+    __ block_comment(\"unnecessary_membar_volatile_rvtso\");\n@@ -7942,2 +7969,1 @@\n-\n-  ins_pipe(pipe_serial);\n+  ins_pipe(real_empty);\n@@ -7946,3 +7972,7 @@\n-instruct store_fence() %{\n-  match(StoreFence);\n-  ins_cost(ALU_COST);\n+\/\/ RVWMO\n+\n+instruct membar_aqcuire_rvwmo() %{\n+  predicate(!UseZtso);\n+  match(LoadFence);\n+  match(MemBarAcquire);\n+  ins_cost(VOLATILE_REF_COST);\n@@ -7950,1 +7980,2 @@\n-  format %{ \"#@store_fence\" %}\n+  format %{ \"#@membar_aqcuire_rvwmo\\n\\t\"\n+            \"fence r, rw\" %}\n@@ -7953,1 +7984,2 @@\n-    __ membar(MacroAssembler::LoadStore | MacroAssembler::StoreStore);\n+    __ block_comment(\"membar_aqcuire_rvwmo\");\n+    __ membar(MacroAssembler::LoadLoad | MacroAssembler::LoadStore);\n@@ -7958,1 +7990,3 @@\n-instruct membar_release() %{\n+instruct membar_release_rvwmo() %{\n+  predicate(!UseZtso);\n+  match(StoreFence);\n@@ -7960,1 +7994,1 @@\n-  ins_cost(ALU_COST);\n+  ins_cost(VOLATILE_REF_COST);\n@@ -7962,2 +7996,2 @@\n-  format %{ \"#@membar_release\\n\\t\"\n-            \"fence iorw ow\" %}\n+  format %{ \"#@membar_release_rvwmo\\n\\t\"\n+            \"fence rw, w\" %}\n@@ -7966,1 +8000,1 @@\n-    __ block_comment(\"membar_release\");\n+    __ block_comment(\"membar_release_rvwmo\");\n@@ -7972,1 +8006,2 @@\n-instruct membar_storestore() %{\n+instruct membar_storestore_rvwmo() %{\n+  predicate(!UseZtso);\n@@ -7975,1 +8010,1 @@\n-  ins_cost(ALU_COST);\n+  ins_cost(VOLATILE_REF_COST);\n@@ -7977,1 +8012,2 @@\n-  format %{ \"MEMBAR-store-store\\t#@membar_storestore\" %}\n+  format %{ \"#@membar_storestore_rvwmo\\n\\t\"\n+            \"fence w, w\" %}\n@@ -7985,3 +8021,4 @@\n-instruct membar_release_lock() %{\n-  match(MemBarReleaseLock);\n-  ins_cost(0);\n+instruct membar_volatile_rvwmo() %{\n+  predicate(!UseZtso);\n+  match(MemBarVolatile);\n+  ins_cost(VOLATILE_REF_COST);\n@@ -7989,1 +8026,2 @@\n-  format %{ \"#@membar_release_lock (elided)\" %}\n+  format %{ \"#@membar_volatile_rvwmo\\n\\t\"\n+            \"fence w, r\"%}\n@@ -7992,1 +8030,2 @@\n-    __ block_comment(\"membar_release_lock (elided)\");\n+    __ block_comment(\"membar_volatile_rvwmo\");\n+    __ membar(MacroAssembler::StoreLoad);\n@@ -7998,3 +8037,5 @@\n-instruct membar_volatile() %{\n-  match(MemBarVolatile);\n-  ins_cost(ALU_COST);\n+instruct membar_lock_rvwmo() %{\n+  predicate(!UseZtso);\n+  match(MemBarAcquireLock);\n+  match(MemBarReleaseLock);\n+  ins_cost(0);\n@@ -8002,2 +8043,1 @@\n-  format %{ \"#@membar_volatile\\n\\t\"\n-             \"fence iorw iorw\"%}\n+  format %{ \"#@membar_lock_rvwmo (elided)\" %}\n@@ -8006,2 +8046,1 @@\n-    __ block_comment(\"membar_volatile\");\n-    __ membar(MacroAssembler::StoreLoad);\n+    __ block_comment(\"membar_lock_rvwmo (elided)\");\n@@ -8013,0 +8052,13 @@\n+instruct unnecessary_membar_volatile_rvwmo() %{\n+  predicate(!UseZtso && Matcher::post_store_load_barrier(n));\n+  match(MemBarVolatile);\n+  ins_cost(0);\n+\n+  size(0);\n+  format %{ \"#@unnecessary_membar_volatile_rvwmo (unnecessary so empty encoding)\" %}\n+  ins_encode %{\n+    __ block_comment(\"unnecessary_membar_volatile_rvwmo\");\n+  %}\n+  ins_pipe(real_empty);\n+%}\n+\n@@ -9898,0 +9950,3 @@\n+\n+\/\/ --------- CMoveI ---------\n+\n@@ -9903,1 +9958,1 @@\n-    \"CMove $dst, ($op1 $cop $op2), $dst, $src\\t#@cmovI_cmpI\\n\\t\"\n+    \"CMoveI $dst, ($op1 $cop $op2), $dst, $src\\t#@cmovI_cmpI\\n\\t\"\n@@ -9920,1 +9975,1 @@\n-    \"CMove $dst, ($op1 $cop $op2), $dst, $src\\t#@cmovI_cmpU\\n\\t\"\n+    \"CMoveI $dst, ($op1 $cop $op2), $dst, $src\\t#@cmovI_cmpU\\n\\t\"\n@@ -9937,1 +9992,1 @@\n-    \"CMove $dst, ($op1 $cop $op2), $dst, $src\\t#@cmovI_cmpL\\n\\t\"\n+    \"CMoveI $dst, ($op1 $cop $op2), $dst, $src\\t#@cmovI_cmpL\\n\\t\"\n@@ -9954,1 +10009,1 @@\n-    \"CMove $dst, ($op1 $cop $op2), $dst, $src\\t#@cmovI_cmpUL\\n\\t\"\n+    \"CMoveI $dst, ($op1 $cop $op2), $dst, $src\\t#@cmovI_cmpUL\\n\\t\"\n@@ -9966,0 +10021,34 @@\n+instruct cmovI_cmpF(iRegINoSp dst, iRegI src, fRegF op1, fRegF op2, cmpOp cop) %{\n+  match(Set dst (CMoveI (Binary cop (CmpF op1 op2)) (Binary dst src)));\n+  ins_cost(ALU_COST + BRANCH_COST);\n+\n+  format %{\n+    \"CMoveI $dst, ($op1 $cop $op2), $dst, $src\\t#@cmovI_cmpF\\n\\t\"\n+  %}\n+\n+  ins_encode %{\n+    __ enc_cmove_cmp_fp($cop$$cmpcode,\n+                        as_FloatRegister($op1$$reg), as_FloatRegister($op2$$reg),\n+                        as_Register($dst$$reg), as_Register($src$$reg), true \/* is_single *\/);\n+  %}\n+\n+  ins_pipe(pipe_class_compare);\n+%}\n+\n+instruct cmovI_cmpD(iRegINoSp dst, iRegI src, fRegD op1, fRegD op2, cmpOp cop) %{\n+  match(Set dst (CMoveI (Binary cop (CmpD op1 op2)) (Binary dst src)));\n+  ins_cost(ALU_COST + BRANCH_COST);\n+\n+  format %{\n+    \"CMoveI $dst, ($op1 $cop $op2), $dst, $src\\t#@cmovI_cmpD\\n\\t\"\n+  %}\n+\n+  ins_encode %{\n+    __ enc_cmove_cmp_fp($cop$$cmpcode | C2_MacroAssembler::double_branch_mask,\n+                        as_FloatRegister($op1$$reg), as_FloatRegister($op2$$reg),\n+                        as_Register($dst$$reg), as_Register($src$$reg), false \/* is_single *\/);\n+  %}\n+\n+  ins_pipe(pipe_class_compare);\n+%}\n+\n@@ -9971,1 +10060,1 @@\n-    \"CMove $dst, ($op1 $cop $op2), $dst, $src\\t#@cmovI_cmpN\\n\\t\"\n+    \"CMoveI $dst, ($op1 $cop $op2), $dst, $src\\t#@cmovI_cmpN\\n\\t\"\n@@ -9988,1 +10077,1 @@\n-    \"CMove $dst, ($op1 $cop $op2), $dst, $src\\t#@cmovI_cmpP\\n\\t\"\n+    \"CMoveI $dst, ($op1 $cop $op2), $dst, $src\\t#@cmovI_cmpP\\n\\t\"\n@@ -10000,0 +10089,2 @@\n+\/\/ --------- CMoveL ---------\n+\n@@ -10005,1 +10096,1 @@\n-    \"CMove $dst, ($op1 $cop $op2), $dst, $src\\t#@cmovL_cmpL\\n\\t\"\n+    \"CMoveL $dst, ($op1 $cop $op2), $dst, $src\\t#@cmovL_cmpL\\n\\t\"\n@@ -10022,1 +10113,1 @@\n-    \"CMove $dst, ($op1 $cop $op2), $dst, $src\\t#@cmovL_cmpUL\\n\\t\"\n+    \"CMoveL $dst, ($op1 $cop $op2), $dst, $src\\t#@cmovL_cmpUL\\n\\t\"\n@@ -10039,1 +10130,1 @@\n-    \"CMove $dst, ($op1 $cop $op2), $dst, $src\\t#@cmovL_cmpI\\n\\t\"\n+    \"CMoveL $dst, ($op1 $cop $op2), $dst, $src\\t#@cmovL_cmpI\\n\\t\"\n@@ -10056,1 +10147,1 @@\n-    \"CMove $dst, ($op1 $cop $op2), $dst, $src\\t#@cmovL_cmpU\\n\\t\"\n+    \"CMoveL $dst, ($op1 $cop $op2), $dst, $src\\t#@cmovL_cmpU\\n\\t\"\n@@ -10068,0 +10159,34 @@\n+instruct cmovL_cmpF(iRegLNoSp dst, iRegL src, fRegF op1, fRegF op2, cmpOp cop) %{\n+  match(Set dst (CMoveL (Binary cop (CmpF op1 op2)) (Binary dst src)));\n+  ins_cost(ALU_COST + BRANCH_COST);\n+\n+  format %{\n+    \"CMoveL $dst, ($op1 $cop $op2), $dst, $src\\t#@cmovL_cmpF\\n\\t\"\n+  %}\n+\n+  ins_encode %{\n+    __ enc_cmove_cmp_fp($cop$$cmpcode,\n+                        as_FloatRegister($op1$$reg), as_FloatRegister($op2$$reg),\n+                        as_Register($dst$$reg), as_Register($src$$reg), true \/* is_single *\/);\n+  %}\n+\n+  ins_pipe(pipe_class_compare);\n+%}\n+\n+instruct cmovL_cmpD(iRegLNoSp dst, iRegL src, fRegD op1, fRegD op2, cmpOp cop) %{\n+  match(Set dst (CMoveL (Binary cop (CmpD op1 op2)) (Binary dst src)));\n+  ins_cost(ALU_COST + BRANCH_COST);\n+\n+  format %{\n+    \"CMoveL $dst, ($op1 $cop $op2), $dst, $src\\t#@cmovL_cmpD\\n\\t\"\n+  %}\n+\n+  ins_encode %{\n+    __ enc_cmove_cmp_fp($cop$$cmpcode | C2_MacroAssembler::double_branch_mask,\n+                        as_FloatRegister($op1$$reg), as_FloatRegister($op2$$reg),\n+                        as_Register($dst$$reg), as_Register($src$$reg), false \/* is_single *\/);\n+  %}\n+\n+  ins_pipe(pipe_class_compare);\n+%}\n+\n@@ -10073,1 +10198,1 @@\n-    \"CMove $dst, ($op1 $cop $op2), $dst, $src\\t#@cmovL_cmpN\\n\\t\"\n+    \"CMoveL $dst, ($op1 $cop $op2), $dst, $src\\t#@cmovL_cmpN\\n\\t\"\n@@ -10090,1 +10215,1 @@\n-    \"CMove $dst, ($op1 $cop $op2), $dst, $src\\t#@cmovL_cmpP\\n\\t\"\n+    \"CMoveL $dst, ($op1 $cop $op2), $dst, $src\\t#@cmovL_cmpP\\n\\t\"\n@@ -10851,1 +10976,2 @@\n-      __ stop(_halt_reason);\n+      const char* str = __ code_string(_halt_reason);\n+      __ stop(str);\n","filename":"src\/hotspot\/cpu\/riscv\/riscv.ad","additions":196,"deletions":70,"binary":false,"changes":266,"status":"modified"},{"patch":"@@ -96,389 +96,0 @@\n-\/\/ First all the versions that have distinct versions depending on 32\/64 bit\n-\/\/ Unless the difference is trivial (1 line or so).\n-\n-#ifndef _LP64\n-\n-\/\/ 32bit versions\n-\n-Address MacroAssembler::as_Address(AddressLiteral adr) {\n-  return Address(adr.target(), adr.rspec());\n-}\n-\n-Address MacroAssembler::as_Address(ArrayAddress adr, Register rscratch) {\n-  assert(rscratch == noreg, \"\");\n-  return Address::make_array(adr);\n-}\n-\n-void MacroAssembler::call_VM_leaf_base(address entry_point,\n-                                       int number_of_arguments) {\n-  call(RuntimeAddress(entry_point));\n-  increment(rsp, number_of_arguments * wordSize);\n-}\n-\n-void MacroAssembler::cmpklass(Address src1, Metadata* obj) {\n-  cmp_literal32(src1, (int32_t)obj, metadata_Relocation::spec_for_immediate());\n-}\n-\n-\n-void MacroAssembler::cmpklass(Register src1, Metadata* obj) {\n-  cmp_literal32(src1, (int32_t)obj, metadata_Relocation::spec_for_immediate());\n-}\n-\n-void MacroAssembler::cmpoop(Address src1, jobject obj) {\n-  cmp_literal32(src1, (int32_t)obj, oop_Relocation::spec_for_immediate());\n-}\n-\n-void MacroAssembler::cmpoop(Register src1, jobject obj, Register rscratch) {\n-  assert(rscratch == noreg, \"redundant\");\n-  cmp_literal32(src1, (int32_t)obj, oop_Relocation::spec_for_immediate());\n-}\n-\n-void MacroAssembler::extend_sign(Register hi, Register lo) {\n-  \/\/ According to Intel Doc. AP-526, \"Integer Divide\", p.18.\n-  if (VM_Version::is_P6() && hi == rdx && lo == rax) {\n-    cdql();\n-  } else {\n-    movl(hi, lo);\n-    sarl(hi, 31);\n-  }\n-}\n-\n-void MacroAssembler::jC2(Register tmp, Label& L) {\n-  \/\/ set parity bit if FPU flag C2 is set (via rax)\n-  save_rax(tmp);\n-  fwait(); fnstsw_ax();\n-  sahf();\n-  restore_rax(tmp);\n-  \/\/ branch\n-  jcc(Assembler::parity, L);\n-}\n-\n-void MacroAssembler::jnC2(Register tmp, Label& L) {\n-  \/\/ set parity bit if FPU flag C2 is set (via rax)\n-  save_rax(tmp);\n-  fwait(); fnstsw_ax();\n-  sahf();\n-  restore_rax(tmp);\n-  \/\/ branch\n-  jcc(Assembler::noParity, L);\n-}\n-\n-\/\/ 32bit can do a case table jump in one instruction but we no longer allow the base\n-\/\/ to be installed in the Address class\n-void MacroAssembler::jump(ArrayAddress entry, Register rscratch) {\n-  assert(rscratch == noreg, \"not needed\");\n-  jmp(as_Address(entry, noreg));\n-}\n-\n-\/\/ Note: y_lo will be destroyed\n-void MacroAssembler::lcmp2int(Register x_hi, Register x_lo, Register y_hi, Register y_lo) {\n-  \/\/ Long compare for Java (semantics as described in JVM spec.)\n-  Label high, low, done;\n-\n-  cmpl(x_hi, y_hi);\n-  jcc(Assembler::less, low);\n-  jcc(Assembler::greater, high);\n-  \/\/ x_hi is the return register\n-  xorl(x_hi, x_hi);\n-  cmpl(x_lo, y_lo);\n-  jcc(Assembler::below, low);\n-  jcc(Assembler::equal, done);\n-\n-  bind(high);\n-  xorl(x_hi, x_hi);\n-  increment(x_hi);\n-  jmp(done);\n-\n-  bind(low);\n-  xorl(x_hi, x_hi);\n-  decrementl(x_hi);\n-\n-  bind(done);\n-}\n-\n-void MacroAssembler::lea(Register dst, AddressLiteral src) {\n-  mov_literal32(dst, (int32_t)src.target(), src.rspec());\n-}\n-\n-void MacroAssembler::lea(Address dst, AddressLiteral adr, Register rscratch) {\n-  assert(rscratch == noreg, \"not needed\");\n-\n-  \/\/ leal(dst, as_Address(adr));\n-  \/\/ see note in movl as to why we must use a move\n-  mov_literal32(dst, (int32_t)adr.target(), adr.rspec());\n-}\n-\n-void MacroAssembler::leave() {\n-  mov(rsp, rbp);\n-  pop(rbp);\n-}\n-\n-void MacroAssembler::lmul(int x_rsp_offset, int y_rsp_offset) {\n-  \/\/ Multiplication of two Java long values stored on the stack\n-  \/\/ as illustrated below. Result is in rdx:rax.\n-  \/\/\n-  \/\/ rsp ---> [  ??  ] \\               \\\n-  \/\/            ....    | y_rsp_offset  |\n-  \/\/          [ y_lo ] \/  (in bytes)    | x_rsp_offset\n-  \/\/          [ y_hi ]                  | (in bytes)\n-  \/\/            ....                    |\n-  \/\/          [ x_lo ]                 \/\n-  \/\/          [ x_hi ]\n-  \/\/            ....\n-  \/\/\n-  \/\/ Basic idea: lo(result) = lo(x_lo * y_lo)\n-  \/\/             hi(result) = hi(x_lo * y_lo) + lo(x_hi * y_lo) + lo(x_lo * y_hi)\n-  Address x_hi(rsp, x_rsp_offset + wordSize); Address x_lo(rsp, x_rsp_offset);\n-  Address y_hi(rsp, y_rsp_offset + wordSize); Address y_lo(rsp, y_rsp_offset);\n-  Label quick;\n-  \/\/ load x_hi, y_hi and check if quick\n-  \/\/ multiplication is possible\n-  movl(rbx, x_hi);\n-  movl(rcx, y_hi);\n-  movl(rax, rbx);\n-  orl(rbx, rcx);                                 \/\/ rbx, = 0 <=> x_hi = 0 and y_hi = 0\n-  jcc(Assembler::zero, quick);                   \/\/ if rbx, = 0 do quick multiply\n-  \/\/ do full multiplication\n-  \/\/ 1st step\n-  mull(y_lo);                                    \/\/ x_hi * y_lo\n-  movl(rbx, rax);                                \/\/ save lo(x_hi * y_lo) in rbx,\n-  \/\/ 2nd step\n-  movl(rax, x_lo);\n-  mull(rcx);                                     \/\/ x_lo * y_hi\n-  addl(rbx, rax);                                \/\/ add lo(x_lo * y_hi) to rbx,\n-  \/\/ 3rd step\n-  bind(quick);                                   \/\/ note: rbx, = 0 if quick multiply!\n-  movl(rax, x_lo);\n-  mull(y_lo);                                    \/\/ x_lo * y_lo\n-  addl(rdx, rbx);                                \/\/ correct hi(x_lo * y_lo)\n-}\n-\n-void MacroAssembler::lneg(Register hi, Register lo) {\n-  negl(lo);\n-  adcl(hi, 0);\n-  negl(hi);\n-}\n-\n-void MacroAssembler::lshl(Register hi, Register lo) {\n-  \/\/ Java shift left long support (semantics as described in JVM spec., p.305)\n-  \/\/ (basic idea for shift counts s >= n: x << s == (x << n) << (s - n))\n-  \/\/ shift value is in rcx !\n-  assert(hi != rcx, \"must not use rcx\");\n-  assert(lo != rcx, \"must not use rcx\");\n-  const Register s = rcx;                        \/\/ shift count\n-  const int      n = BitsPerWord;\n-  Label L;\n-  andl(s, 0x3f);                                 \/\/ s := s & 0x3f (s < 0x40)\n-  cmpl(s, n);                                    \/\/ if (s < n)\n-  jcc(Assembler::less, L);                       \/\/ else (s >= n)\n-  movl(hi, lo);                                  \/\/ x := x << n\n-  xorl(lo, lo);\n-  \/\/ Note: subl(s, n) is not needed since the Intel shift instructions work rcx mod n!\n-  bind(L);                                       \/\/ s (mod n) < n\n-  shldl(hi, lo);                                 \/\/ x := x << s\n-  shll(lo);\n-}\n-\n-\n-void MacroAssembler::lshr(Register hi, Register lo, bool sign_extension) {\n-  \/\/ Java shift right long support (semantics as described in JVM spec., p.306 & p.310)\n-  \/\/ (basic idea for shift counts s >= n: x >> s == (x >> n) >> (s - n))\n-  assert(hi != rcx, \"must not use rcx\");\n-  assert(lo != rcx, \"must not use rcx\");\n-  const Register s = rcx;                        \/\/ shift count\n-  const int      n = BitsPerWord;\n-  Label L;\n-  andl(s, 0x3f);                                 \/\/ s := s & 0x3f (s < 0x40)\n-  cmpl(s, n);                                    \/\/ if (s < n)\n-  jcc(Assembler::less, L);                       \/\/ else (s >= n)\n-  movl(lo, hi);                                  \/\/ x := x >> n\n-  if (sign_extension) sarl(hi, 31);\n-  else                xorl(hi, hi);\n-  \/\/ Note: subl(s, n) is not needed since the Intel shift instructions work rcx mod n!\n-  bind(L);                                       \/\/ s (mod n) < n\n-  shrdl(lo, hi);                                 \/\/ x := x >> s\n-  if (sign_extension) sarl(hi);\n-  else                shrl(hi);\n-}\n-\n-void MacroAssembler::movoop(Register dst, jobject obj) {\n-  mov_literal32(dst, (int32_t)obj, oop_Relocation::spec_for_immediate());\n-}\n-\n-void MacroAssembler::movoop(Address dst, jobject obj, Register rscratch) {\n-  assert(rscratch == noreg, \"redundant\");\n-  mov_literal32(dst, (int32_t)obj, oop_Relocation::spec_for_immediate());\n-}\n-\n-void MacroAssembler::mov_metadata(Register dst, Metadata* obj) {\n-  mov_literal32(dst, (int32_t)obj, metadata_Relocation::spec_for_immediate());\n-}\n-\n-void MacroAssembler::mov_metadata(Address dst, Metadata* obj, Register rscratch) {\n-  assert(rscratch == noreg, \"redundant\");\n-  mov_literal32(dst, (int32_t)obj, metadata_Relocation::spec_for_immediate());\n-}\n-\n-void MacroAssembler::movptr(Register dst, AddressLiteral src) {\n-  if (src.is_lval()) {\n-    mov_literal32(dst, (intptr_t)src.target(), src.rspec());\n-  } else {\n-    movl(dst, as_Address(src));\n-  }\n-}\n-\n-void MacroAssembler::movptr(ArrayAddress dst, Register src, Register rscratch) {\n-  assert(rscratch == noreg, \"redundant\");\n-  movl(as_Address(dst, noreg), src);\n-}\n-\n-void MacroAssembler::movptr(Register dst, ArrayAddress src) {\n-  movl(dst, as_Address(src, noreg));\n-}\n-\n-void MacroAssembler::movptr(Address dst, intptr_t src, Register rscratch) {\n-  assert(rscratch == noreg, \"redundant\");\n-  movl(dst, src);\n-}\n-\n-void MacroAssembler::pushoop(jobject obj, Register rscratch) {\n-  assert(rscratch == noreg, \"redundant\");\n-  push_literal32((int32_t)obj, oop_Relocation::spec_for_immediate());\n-}\n-\n-void MacroAssembler::pushklass(Metadata* obj, Register rscratch) {\n-  assert(rscratch == noreg, \"redundant\");\n-  push_literal32((int32_t)obj, metadata_Relocation::spec_for_immediate());\n-}\n-\n-void MacroAssembler::pushptr(AddressLiteral src, Register rscratch) {\n-  assert(rscratch == noreg, \"redundant\");\n-  if (src.is_lval()) {\n-    push_literal32((int32_t)src.target(), src.rspec());\n-  } else {\n-    pushl(as_Address(src));\n-  }\n-}\n-\n-static void pass_arg0(MacroAssembler* masm, Register arg) {\n-  masm->push(arg);\n-}\n-\n-static void pass_arg1(MacroAssembler* masm, Register arg) {\n-  masm->push(arg);\n-}\n-\n-static void pass_arg2(MacroAssembler* masm, Register arg) {\n-  masm->push(arg);\n-}\n-\n-static void pass_arg3(MacroAssembler* masm, Register arg) {\n-  masm->push(arg);\n-}\n-\n-#ifndef PRODUCT\n-extern \"C\" void findpc(intptr_t x);\n-#endif\n-\n-void MacroAssembler::debug32(int rdi, int rsi, int rbp, int rsp, int rbx, int rdx, int rcx, int rax, int eip, char* msg) {\n-  \/\/ In order to get locks to work, we need to fake a in_VM state\n-  JavaThread* thread = JavaThread::current();\n-  JavaThreadState saved_state = thread->thread_state();\n-  thread->set_thread_state(_thread_in_vm);\n-  if (ShowMessageBoxOnError) {\n-    JavaThread* thread = JavaThread::current();\n-    JavaThreadState saved_state = thread->thread_state();\n-    thread->set_thread_state(_thread_in_vm);\n-    if (CountBytecodes || TraceBytecodes || StopInterpreterAt) {\n-      ttyLocker ttyl;\n-      BytecodeCounter::print();\n-    }\n-    \/\/ To see where a verify_oop failed, get $ebx+40\/X for this frame.\n-    \/\/ This is the value of eip which points to where verify_oop will return.\n-    if (os::message_box(msg, \"Execution stopped, print registers?\")) {\n-      print_state32(rdi, rsi, rbp, rsp, rbx, rdx, rcx, rax, eip);\n-      BREAKPOINT;\n-    }\n-  }\n-  fatal(\"DEBUG MESSAGE: %s\", msg);\n-}\n-\n-void MacroAssembler::print_state32(int rdi, int rsi, int rbp, int rsp, int rbx, int rdx, int rcx, int rax, int eip) {\n-  ttyLocker ttyl;\n-  DebuggingContext debugging{};\n-  tty->print_cr(\"eip = 0x%08x\", eip);\n-#ifndef PRODUCT\n-  if ((WizardMode || Verbose) && PrintMiscellaneous) {\n-    tty->cr();\n-    findpc(eip);\n-    tty->cr();\n-  }\n-#endif\n-#define PRINT_REG(rax) \\\n-  { tty->print(\"%s = \", #rax); os::print_location(tty, rax); }\n-  PRINT_REG(rax);\n-  PRINT_REG(rbx);\n-  PRINT_REG(rcx);\n-  PRINT_REG(rdx);\n-  PRINT_REG(rdi);\n-  PRINT_REG(rsi);\n-  PRINT_REG(rbp);\n-  PRINT_REG(rsp);\n-#undef PRINT_REG\n-  \/\/ Print some words near top of staack.\n-  int* dump_sp = (int*) rsp;\n-  for (int col1 = 0; col1 < 8; col1++) {\n-    tty->print(\"(rsp+0x%03x) 0x%08x: \", (int)((intptr_t)dump_sp - (intptr_t)rsp), (intptr_t)dump_sp);\n-    os::print_location(tty, *dump_sp++);\n-  }\n-  for (int row = 0; row < 16; row++) {\n-    tty->print(\"(rsp+0x%03x) 0x%08x: \", (int)((intptr_t)dump_sp - (intptr_t)rsp), (intptr_t)dump_sp);\n-    for (int col = 0; col < 8; col++) {\n-      tty->print(\" 0x%08x\", *dump_sp++);\n-    }\n-    tty->cr();\n-  }\n-  \/\/ Print some instructions around pc:\n-  Disassembler::decode((address)eip-64, (address)eip);\n-  tty->print_cr(\"--------\");\n-  Disassembler::decode((address)eip, (address)eip+32);\n-}\n-\n-void MacroAssembler::stop(const char* msg) {\n-  \/\/ push address of message\n-  ExternalAddress message((address)msg);\n-  pushptr(message.addr(), noreg);\n-  { Label L; call(L, relocInfo::none); bind(L); }     \/\/ push eip\n-  pusha();                                            \/\/ push registers\n-  call(RuntimeAddress(CAST_FROM_FN_PTR(address, MacroAssembler::debug32)));\n-  hlt();\n-}\n-\n-void MacroAssembler::warn(const char* msg) {\n-  push_CPU_state();\n-\n-  \/\/ push address of message\n-  ExternalAddress message((address)msg);\n-  pushptr(message.addr(), noreg);\n-\n-  call(RuntimeAddress(CAST_FROM_FN_PTR(address, warning)));\n-  addl(rsp, wordSize);       \/\/ discard argument\n-  pop_CPU_state();\n-}\n-\n-void MacroAssembler::print_state() {\n-  { Label L; call(L, relocInfo::none); bind(L); }     \/\/ push eip\n-  pusha();                                            \/\/ push registers\n-\n-  push_CPU_state();\n-  call(RuntimeAddress(CAST_FROM_FN_PTR(address, MacroAssembler::print_state32)));\n-  pop_CPU_state();\n-\n-  popa();\n-  addl(rsp, wordSize);\n-}\n-\n-#else \/\/ _LP64\n-\n-\/\/ 64 bit versions\n-\n@@ -727,11 +338,0 @@\n-void MacroAssembler::reset_last_Java_frame(bool clear_fp) {\n-  reset_last_Java_frame(r15_thread, clear_fp);\n-}\n-\n-void MacroAssembler::set_last_Java_frame(Register last_java_sp,\n-                                         Register last_java_fp,\n-                                         address  last_java_pc,\n-                                         Register rscratch) {\n-  set_last_Java_frame(r15_thread, last_java_sp, last_java_fp, last_java_pc, rscratch);\n-}\n-\n@@ -1107,5 +707,1 @@\n-#endif \/\/ _LP64\n-\n-\/\/ Now versions that are common to 32\/64 bit\n-\n-  LP64_ONLY(addq(dst, imm32)) NOT_LP64(addl(dst, imm32));\n+  addq(dst, imm32);\n@@ -1116,1 +712,1 @@\n-  LP64_ONLY(addq(dst, src)) NOT_LP64(addl(dst, src));\n+  addq(dst, src);\n@@ -1120,1 +716,1 @@\n-  LP64_ONLY(addq(dst, src)) NOT_LP64(addl(dst, src));\n+  addq(dst, src);\n@@ -1230,1 +826,1 @@\n-  LP64_ONLY(andq(dst, imm32)) NOT_LP64(andl(dst, imm32));\n+  andq(dst, imm32);\n@@ -1233,1 +829,0 @@\n-#ifdef _LP64\n@@ -1244,1 +839,0 @@\n-#endif\n@@ -1262,1 +856,0 @@\n-#ifdef _LP64\n@@ -1278,1 +871,0 @@\n-#endif\n@@ -1310,3 +902,1 @@\n-  Register thread = NOT_LP64(rsi) LP64_ONLY(r15_thread);\n-  NOT_LP64(get_thread(rsi);)\n-  cmpptr(rsp, Address(thread, JavaThread::reserved_stack_activation_offset()));\n+  cmpptr(rsp, Address(r15_thread, JavaThread::reserved_stack_activation_offset()));\n@@ -1316,1 +906,1 @@\n-  call_VM_leaf(CAST_FROM_FN_PTR(address, SharedRuntime::enable_stack_reserved_zone), thread);\n+  call_VM_leaf(CAST_FROM_FN_PTR(address, SharedRuntime::enable_stack_reserved_zone), r15_thread);\n@@ -1354,1 +944,0 @@\n-#ifdef _LP64\n@@ -1357,3 +946,0 @@\n-#else\n-  movptr(rax, (intptr_t)Universe::non_oop_word());\n-#endif\n@@ -1364,2 +950,1 @@\n-  return\n-      LP64_ONLY(UseCompactObjectHeaders ? 17 : 14) NOT_LP64(12);\n+  return UseCompactObjectHeaders ? 17 : 14;\n@@ -1369,1 +954,1 @@\n-  Register receiver = LP64_ONLY(j_rarg0) NOT_LP64(rcx);\n+  Register receiver = j_rarg0;\n@@ -1371,1 +956,1 @@\n-  Register temp = LP64_ONLY(rscratch1) NOT_LP64(rbx);\n+  Register temp = rscratch1;\n@@ -1381,1 +966,0 @@\n-#ifdef _LP64\n@@ -1385,3 +969,1 @@\n-  } else\n-#endif\n-  if (UseCompressedClassPointers) {\n+  } else if (UseCompressedClassPointers) {\n@@ -1452,1 +1034,1 @@\n-  LP64_ONLY(assert_different_registers(arg_1, c_rarg2));\n+  assert_different_registers(arg_1, c_rarg2);\n@@ -1474,2 +1056,2 @@\n-  LP64_ONLY(assert_different_registers(arg_1, c_rarg2, c_rarg3));\n-  LP64_ONLY(assert_different_registers(arg_2, c_rarg3));\n+  assert_different_registers(arg_1, c_rarg2, c_rarg3);\n+  assert_different_registers(arg_2, c_rarg3);\n@@ -1490,2 +1072,1 @@\n-  Register thread = LP64_ONLY(r15_thread) NOT_LP64(noreg);\n-  call_VM_base(oop_result, thread, last_java_sp, entry_point, number_of_arguments, check_exceptions);\n+  call_VM_base(oop_result, last_java_sp, entry_point, number_of_arguments, check_exceptions);\n@@ -1510,1 +1091,1 @@\n-  LP64_ONLY(assert_different_registers(arg_1, c_rarg2));\n+  assert_different_registers(arg_1, c_rarg2);\n@@ -1523,2 +1104,2 @@\n-  LP64_ONLY(assert_different_registers(arg_1, c_rarg2, c_rarg3));\n-  LP64_ONLY(assert_different_registers(arg_2, c_rarg3));\n+  assert_different_registers(arg_1, c_rarg2, c_rarg3);\n+  assert_different_registers(arg_2, c_rarg3);\n@@ -1536,2 +1117,1 @@\n-  Register thread = LP64_ONLY(r15_thread) NOT_LP64(noreg);\n-  MacroAssembler::call_VM_base(oop_result, thread, last_java_sp, entry_point, number_of_arguments, check_exceptions);\n+  MacroAssembler::call_VM_base(oop_result, last_java_sp, entry_point, number_of_arguments, check_exceptions);\n@@ -1556,1 +1136,1 @@\n-  LP64_ONLY(assert_different_registers(arg_1, c_rarg2));\n+  assert_different_registers(arg_1, c_rarg2);\n@@ -1569,2 +1149,2 @@\n-  LP64_ONLY(assert_different_registers(arg_1, c_rarg2, c_rarg3));\n-  LP64_ONLY(assert_different_registers(arg_2, c_rarg3));\n+  assert_different_registers(arg_1, c_rarg2, c_rarg3);\n+  assert_different_registers(arg_2, c_rarg3);\n@@ -1578,1 +1158,0 @@\n-                                  Register java_thread,\n@@ -1583,9 +1162,2 @@\n-  \/\/ determine java_thread register\n-  if (!java_thread->is_valid()) {\n-#ifdef _LP64\n-    java_thread = r15_thread;\n-#else\n-    java_thread = rdi;\n-    get_thread(java_thread);\n-#endif \/\/ LP64\n-  }\n+  Register java_thread = r15_thread;\n+\n@@ -1598,1 +1170,0 @@\n-  LP64_ONLY(assert(java_thread == r15_thread, \"unexpected register\"));\n@@ -1602,1 +1173,1 @@\n-  LP64_ONLY(if (UseCompressedOops && !TraceBytecodes) verify_heapbase(\"call_VM_base: heap base corrupted?\");)\n+  if (UseCompressedOops && !TraceBytecodes) verify_heapbase(\"call_VM_base: heap base corrupted?\");\n@@ -1610,2 +1181,1 @@\n-  NOT_LP64(push(java_thread); number_of_arguments++);\n-  LP64_ONLY(mov(c_rarg0, r15_thread));\n+  mov(c_rarg0, r15_thread);\n@@ -1617,1 +1187,1 @@\n-  set_last_Java_frame(java_thread, last_java_sp, rbp, nullptr, rscratch1);\n+  set_last_Java_frame(last_java_sp, rbp, nullptr, rscratch1);\n@@ -1622,18 +1192,9 @@\n-  \/\/ restore the thread (cannot use the pushed argument since arguments\n-  \/\/ may be overwritten by C code generated by an optimizing compiler);\n-  \/\/ however can use the register value directly if it is callee saved.\n-  if (LP64_ONLY(true ||) java_thread == rdi || java_thread == rsi) {\n-    \/\/ rdi & rsi (also r15) are callee saved -> nothing to do\n-    guarantee(java_thread != rax, \"change this code\");\n-    push(rax);\n-    { Label L;\n-      get_thread(rax);\n-      cmpptr(java_thread, rax);\n-      jcc(Assembler::equal, L);\n-      STOP(\"MacroAssembler::call_VM_base: rdi not callee saved?\");\n-      bind(L);\n-    }\n-    pop(rax);\n-#endif\n-  } else {\n-    get_thread(java_thread);\n+  \/\/ Check that thread register is not clobbered.\n+  guarantee(java_thread != rax, \"change this code\");\n+  push(rax);\n+  { Label L;\n+    get_thread_slow(rax);\n+    cmpptr(java_thread, rax);\n+    jcc(Assembler::equal, L);\n+    STOP(\"MacroAssembler::call_VM_base: java_thread not callee saved?\");\n+    bind(L);\n@@ -1642,0 +1203,3 @@\n+  pop(rax);\n+#endif\n+\n@@ -1644,1 +1208,1 @@\n-  reset_last_Java_frame(java_thread, true);\n+  reset_last_Java_frame(true);\n@@ -1647,2 +1211,2 @@\n-  check_and_handle_popframe(java_thread);\n-  check_and_handle_earlyret(java_thread);\n+  check_and_handle_popframe();\n+  check_and_handle_earlyret();\n@@ -1652,5 +1216,1 @@\n-    cmpptr(Address(java_thread, Thread::pending_exception_offset()), NULL_WORD);\n-#ifndef _LP64\n-    jump_cc(Assembler::notEqual,\n-            RuntimeAddress(StubRoutines::forward_exception_entry()));\n-#else\n+    cmpptr(Address(r15_thread, Thread::pending_exception_offset()), NULL_WORD);\n@@ -1665,1 +1225,0 @@\n-#endif \/\/ LP64\n@@ -1670,1 +1229,1 @@\n-    get_vm_result(oop_result, java_thread);\n+    get_vm_result_oop(oop_result);\n@@ -1675,0 +1234,4 @@\n+  \/\/ Calculate the value for last_Java_sp somewhat subtle.\n+  \/\/ call_VM does an intermediate call which places a return address on\n+  \/\/ the stack just under the stack pointer as the user finished with it.\n+  \/\/ This allows use to retrieve last_Java_pc from last_Java_sp[-1].\n@@ -1676,11 +1239,0 @@\n-  \/\/ Calculate the value for last_Java_sp\n-  \/\/ somewhat subtle. call_VM does an intermediate call\n-  \/\/ which places a return address on the stack just under the\n-  \/\/ stack pointer as the user finished with it. This allows\n-  \/\/ use to retrieve last_Java_pc from last_Java_sp[-1].\n-  \/\/ On 32bit we then have to push additional args on the stack to accomplish\n-  \/\/ the actual requested call. On 64bit call_VM only can use register args\n-  \/\/ so the only extra space is the return address that call_VM created.\n-  \/\/ This hopefully explains the calculations here.\n-\n-#ifdef _LP64\n@@ -1689,5 +1241,1 @@\n-#else\n-  lea(rax, Address(rsp, (1 + number_of_arguments) * wordSize));\n-#endif \/\/ LP64\n-\n-  call_VM_base(oop_result, noreg, rax, entry_point, number_of_arguments, check_exceptions);\n+  call_VM_base(oop_result, rax, entry_point, number_of_arguments, check_exceptions);\n@@ -1713,1 +1261,1 @@\n-  LP64_ONLY(assert_different_registers(arg_0, c_rarg1));\n+  assert_different_registers(arg_0, c_rarg1);\n@@ -1720,2 +1268,2 @@\n-  LP64_ONLY(assert_different_registers(arg_0, c_rarg1, c_rarg2));\n-  LP64_ONLY(assert_different_registers(arg_1, c_rarg2));\n+  assert_different_registers(arg_0, c_rarg1, c_rarg2);\n+  assert_different_registers(arg_1, c_rarg2);\n@@ -1729,3 +1277,3 @@\n-  LP64_ONLY(assert_different_registers(arg_0, c_rarg1, c_rarg2, c_rarg3));\n-  LP64_ONLY(assert_different_registers(arg_1, c_rarg2, c_rarg3));\n-  LP64_ONLY(assert_different_registers(arg_2, c_rarg3));\n+  assert_different_registers(arg_0, c_rarg1, c_rarg2, c_rarg3);\n+  assert_different_registers(arg_1, c_rarg2, c_rarg3);\n+  assert_different_registers(arg_2, c_rarg3);\n@@ -1745,1 +1293,1 @@\n-  LP64_ONLY(assert_different_registers(arg_0, c_rarg1));\n+  assert_different_registers(arg_0, c_rarg1);\n@@ -1752,2 +1300,2 @@\n-  LP64_ONLY(assert_different_registers(arg_0, c_rarg1, c_rarg2));\n-  LP64_ONLY(assert_different_registers(arg_1, c_rarg2));\n+  assert_different_registers(arg_0, c_rarg1, c_rarg2);\n+  assert_different_registers(arg_1, c_rarg2);\n@@ -1761,3 +1309,3 @@\n-  LP64_ONLY(assert_different_registers(arg_0, c_rarg1, c_rarg2, c_rarg3));\n-  LP64_ONLY(assert_different_registers(arg_1, c_rarg2, c_rarg3));\n-  LP64_ONLY(assert_different_registers(arg_2, c_rarg3));\n+  assert_different_registers(arg_0, c_rarg1, c_rarg2, c_rarg3);\n+  assert_different_registers(arg_1, c_rarg2, c_rarg3);\n+  assert_different_registers(arg_2, c_rarg3);\n@@ -1771,3 +1319,3 @@\n-void MacroAssembler::get_vm_result(Register oop_result, Register java_thread) {\n-  movptr(oop_result, Address(java_thread, JavaThread::vm_result_offset()));\n-  movptr(Address(java_thread, JavaThread::vm_result_offset()), NULL_WORD);\n+void MacroAssembler::get_vm_result_oop(Register oop_result) {\n+  movptr(oop_result, Address(r15_thread, JavaThread::vm_result_oop_offset()));\n+  movptr(Address(r15_thread, JavaThread::vm_result_oop_offset()), NULL_WORD);\n@@ -1777,3 +1325,3 @@\n-void MacroAssembler::get_vm_result_2(Register metadata_result, Register java_thread) {\n-  movptr(metadata_result, Address(java_thread, JavaThread::vm_result_2_offset()));\n-  movptr(Address(java_thread, JavaThread::vm_result_2_offset()), NULL_WORD);\n+void MacroAssembler::get_vm_result_metadata(Register metadata_result) {\n+  movptr(metadata_result, Address(r15_thread, JavaThread::vm_result_metadata_offset()));\n+  movptr(Address(r15_thread, JavaThread::vm_result_metadata_offset()), NULL_WORD);\n@@ -1782,1 +1330,1 @@\n-void MacroAssembler::check_and_handle_earlyret(Register java_thread) {\n+void MacroAssembler::check_and_handle_earlyret() {\n@@ -1785,1 +1333,1 @@\n-void MacroAssembler::check_and_handle_popframe(Register java_thread) {\n+void MacroAssembler::check_and_handle_popframe() {\n@@ -1876,1 +1424,0 @@\n-#ifdef _LP64\n@@ -1888,8 +1435,0 @@\n-#else\n-  assert(rscratch == noreg, \"not needed\");\n-  if (src2.is_lval()) {\n-    cmp_literal32(src1, (int32_t)src2.target(), src2.rspec());\n-  } else {\n-    cmpl(src1, as_Address(src2));\n-  }\n-#endif \/\/ _LP64\n@@ -1900,1 +1439,0 @@\n-#ifdef _LP64\n@@ -1904,4 +1442,0 @@\n-#else\n-  assert(rscratch == noreg, \"not needed\");\n-  cmp_literal32(src1, (int32_t)src2.target(), src2.rspec());\n-#endif \/\/ _LP64\n@@ -1918,1 +1452,0 @@\n-#ifdef _LP64\n@@ -1923,1 +1456,0 @@\n-#endif\n@@ -1939,1 +1471,1 @@\n-  LP64_ONLY(cmpxchgq(reg, adr)) NOT_LP64(cmpxchgl(reg, adr));\n+  cmpxchgq(reg, adr);\n@@ -2102,109 +1634,0 @@\n-#ifndef _LP64\n-void MacroAssembler::fcmp(Register tmp) {\n-  fcmp(tmp, 1, true, true);\n-}\n-\n-void MacroAssembler::fcmp(Register tmp, int index, bool pop_left, bool pop_right) {\n-  assert(!pop_right || pop_left, \"usage error\");\n-  if (VM_Version::supports_cmov()) {\n-    assert(tmp == noreg, \"unneeded temp\");\n-    if (pop_left) {\n-      fucomip(index);\n-    } else {\n-      fucomi(index);\n-    }\n-    if (pop_right) {\n-      fpop();\n-    }\n-  } else {\n-    assert(tmp != noreg, \"need temp\");\n-    if (pop_left) {\n-      if (pop_right) {\n-        fcompp();\n-      } else {\n-        fcomp(index);\n-      }\n-    } else {\n-      fcom(index);\n-    }\n-    \/\/ convert FPU condition into eflags condition via rax,\n-    save_rax(tmp);\n-    fwait(); fnstsw_ax();\n-    sahf();\n-    restore_rax(tmp);\n-  }\n-  \/\/ condition codes set as follows:\n-  \/\/\n-  \/\/ CF (corresponds to C0) if x < y\n-  \/\/ PF (corresponds to C2) if unordered\n-  \/\/ ZF (corresponds to C3) if x = y\n-}\n-\n-void MacroAssembler::fcmp2int(Register dst, bool unordered_is_less) {\n-  fcmp2int(dst, unordered_is_less, 1, true, true);\n-}\n-\n-void MacroAssembler::fcmp2int(Register dst, bool unordered_is_less, int index, bool pop_left, bool pop_right) {\n-  fcmp(VM_Version::supports_cmov() ? noreg : dst, index, pop_left, pop_right);\n-  Label L;\n-  if (unordered_is_less) {\n-    movl(dst, -1);\n-    jcc(Assembler::parity, L);\n-    jcc(Assembler::below , L);\n-    movl(dst, 0);\n-    jcc(Assembler::equal , L);\n-    increment(dst);\n-  } else { \/\/ unordered is greater\n-    movl(dst, 1);\n-    jcc(Assembler::parity, L);\n-    jcc(Assembler::above , L);\n-    movl(dst, 0);\n-    jcc(Assembler::equal , L);\n-    decrementl(dst);\n-  }\n-  bind(L);\n-}\n-\n-void MacroAssembler::fld_d(AddressLiteral src) {\n-  fld_d(as_Address(src));\n-}\n-\n-void MacroAssembler::fld_s(AddressLiteral src) {\n-  fld_s(as_Address(src));\n-}\n-\n-void MacroAssembler::fldcw(AddressLiteral src) {\n-  fldcw(as_Address(src));\n-}\n-\n-void MacroAssembler::fpop() {\n-  ffree();\n-  fincstp();\n-}\n-\n-void MacroAssembler::fremr(Register tmp) {\n-  save_rax(tmp);\n-  { Label L;\n-    bind(L);\n-    fprem();\n-    fwait(); fnstsw_ax();\n-    sahf();\n-    jcc(Assembler::parity, L);\n-  }\n-  restore_rax(tmp);\n-  \/\/ Result is in ST0.\n-  \/\/ Note: fxch & fpop to get rid of ST1\n-  \/\/ (otherwise FPU stack could overflow eventually)\n-  fxch(1);\n-  fpop();\n-}\n-\n-void MacroAssembler::empty_FPU_stack() {\n-  if (VM_Version::supports_mmx()) {\n-    emms();\n-  } else {\n-    for (int i = 8; i-- > 0; ) ffree(i);\n-  }\n-}\n-#endif \/\/ !LP64\n-\n@@ -2221,48 +1644,0 @@\n-void MacroAssembler::load_float(Address src) {\n-#ifdef _LP64\n-  movflt(xmm0, src);\n-#else\n-  if (UseSSE >= 1) {\n-    movflt(xmm0, src);\n-  } else {\n-    fld_s(src);\n-  }\n-#endif \/\/ LP64\n-}\n-\n-void MacroAssembler::store_float(Address dst) {\n-#ifdef _LP64\n-  movflt(dst, xmm0);\n-#else\n-  if (UseSSE >= 1) {\n-    movflt(dst, xmm0);\n-  } else {\n-    fstp_s(dst);\n-  }\n-#endif \/\/ LP64\n-}\n-\n-void MacroAssembler::load_double(Address src) {\n-#ifdef _LP64\n-  movdbl(xmm0, src);\n-#else\n-  if (UseSSE >= 2) {\n-    movdbl(xmm0, src);\n-  } else {\n-    fld_d(src);\n-  }\n-#endif \/\/ LP64\n-}\n-\n-void MacroAssembler::store_double(Address dst) {\n-#ifdef _LP64\n-  movdbl(dst, xmm0);\n-#else\n-  if (UseSSE >= 2) {\n-    movdbl(dst, xmm0);\n-  } else {\n-    fstp_d(dst);\n-  }\n-#endif \/\/ LP64\n-}\n-\n@@ -2418,9 +1793,2 @@\n-  int off;\n-  if (LP64_ONLY(true ||) VM_Version::is_P6()) {\n-    off = offset();\n-    movsbl(dst, src); \/\/ movsxb\n-  } else {\n-    off = load_unsigned_byte(dst, src);\n-    shll(dst, 24);\n-    sarl(dst, 24);\n-  }\n+  int off = offset();\n+  movsbl(dst, src); \/\/ movsxb\n@@ -2435,12 +1803,5 @@\n-  int off;\n-  if (LP64_ONLY(true ||) VM_Version::is_P6()) {\n-    \/\/ This is dubious to me since it seems safe to do a signed 16 => 64 bit\n-    \/\/ version but this is what 64bit has always done. This seems to imply\n-    \/\/ that users are only using 32bits worth.\n-    off = offset();\n-    movswl(dst, src); \/\/ movsxw\n-  } else {\n-    off = load_unsigned_short(dst, src);\n-    shll(dst, 16);\n-    sarl(dst, 16);\n-  }\n+  \/\/ This is dubious to me since it seems safe to do a signed 16 => 64 bit\n+  \/\/ version but this is what 64bit has always done. This seems to imply\n+  \/\/ that users are only using 32bits worth.\n+  int off = offset();\n+  movswl(dst, src); \/\/ movsxw\n@@ -2453,9 +1814,2 @@\n-  int off;\n-  if (LP64_ONLY(true || ) VM_Version::is_P6() || src.uses(dst)) {\n-    off = offset();\n-    movzbl(dst, src); \/\/ movzxb\n-  } else {\n-    xorl(dst, dst);\n-    off = offset();\n-    movb(dst, src);\n-  }\n+  int off = offset();\n+  movzbl(dst, src); \/\/ movzxb\n@@ -2469,9 +1823,2 @@\n-  int off;\n-  if (LP64_ONLY(true ||) VM_Version::is_P6() || src.uses(dst)) {\n-    off = offset();\n-    movzwl(dst, src); \/\/ movzxw\n-  } else {\n-    xorl(dst, dst);\n-    off = offset();\n-    movw(dst, src);\n-  }\n+  int off = offset();\n+  movzwl(dst, src); \/\/ movzxw\n@@ -2483,8 +1830,0 @@\n-#ifndef _LP64\n-  case  8:\n-    assert(dst2 != noreg, \"second dest register required\");\n-    movl(dst,  src);\n-    movl(dst2, src.plus_disp(BytesPerInt));\n-    break;\n-#else\n-#endif\n@@ -2501,8 +1840,0 @@\n-#ifndef _LP64\n-  case  8:\n-    assert(src2 != noreg, \"second source register required\");\n-    movl(dst,                        src);\n-    movl(dst.plus_disp(BytesPerInt), src2);\n-    break;\n-#else\n-#endif\n@@ -2628,1 +1959,1 @@\n-  LP64_ONLY(movq(dst, src)) NOT_LP64(movl(dst, src));\n+  movq(dst, src);\n@@ -2632,1 +1963,1 @@\n-  LP64_ONLY(movq(dst, src)) NOT_LP64(movl(dst, src));\n+  movq(dst, src);\n@@ -2637,1 +1968,0 @@\n-#ifdef _LP64\n@@ -2645,3 +1975,0 @@\n-#else\n-  movl(dst, src);\n-#endif\n@@ -2651,1 +1978,1 @@\n-  LP64_ONLY(movq(dst, src)) NOT_LP64(movl(dst, src));\n+  movq(dst, src);\n@@ -2655,1 +1982,1 @@\n-  LP64_ONLY(movslq(dst, src)) NOT_LP64(movl(dst, src));\n+  movslq(dst, src);\n@@ -3033,2 +2360,0 @@\n-#ifdef _LP64\n-#endif\n@@ -3043,4 +2368,0 @@\n-#ifndef _LP64\n-  frstor(Address(rsp, 0));\n-#else\n-#endif\n@@ -3053,1 +2374,1 @@\n-  LP64_ONLY(addq(rsp, 8));\n+  addq(rsp, 8);\n@@ -3066,5 +2387,0 @@\n-#ifndef _LP64\n-  fnsave(Address(rsp, 0));\n-  fwait();\n-#else\n-#endif \/\/ LP64\n@@ -3078,1 +2394,1 @@\n-  LP64_ONLY(subq(rsp, 8));\n+  subq(rsp, 8);\n@@ -3085,27 +2401,5 @@\n-#ifndef _LP64\n-  Register rthread = rax;\n-  Register rrealsp = rbx;\n-  push(rthread);\n-  push(rrealsp);\n-\n-  get_thread(rthread);\n-\n-  \/\/ The code below wants the original RSP.\n-  \/\/ Move it back after the pushes above.\n-  movptr(rrealsp, rsp);\n-  addptr(rrealsp, 2*wordSize);\n-#else\n-  Register rthread = r15_thread;\n-  Register rrealsp = rsp;\n-#endif\n-\n-  Label done;\n-  cmpptr(rrealsp, Address(rthread, JavaThread::cont_fastpath_offset()));\n-  jccb(Assembler::belowEqual, done);\n-  movptr(Address(rthread, JavaThread::cont_fastpath_offset()), rrealsp);\n-  bind(done);\n-\n-#ifndef _LP64\n-  pop(rrealsp);\n-  pop(rthread);\n-#endif\n+  Label L_done;\n+  cmpptr(rsp, Address(r15_thread, JavaThread::cont_fastpath_offset()));\n+  jccb(Assembler::belowEqual, L_done);\n+  movptr(Address(r15_thread, JavaThread::cont_fastpath_offset()), rsp);\n+  bind(L_done);\n@@ -3117,27 +2411,5 @@\n-#ifndef _LP64\n-  Register rthread = rax;\n-  Register rrealsp = rbx;\n-  push(rthread);\n-  push(rrealsp);\n-\n-  get_thread(rthread);\n-\n-  \/\/ The code below wants the original RSP.\n-  \/\/ Move it back after the pushes above.\n-  movptr(rrealsp, rsp);\n-  addptr(rrealsp, 2*wordSize);\n-#else\n-  Register rthread = r15_thread;\n-  Register rrealsp = rsp;\n-#endif\n-\n-  Label done;\n-  cmpptr(rrealsp, Address(rthread, JavaThread::cont_fastpath_offset()));\n-  jccb(Assembler::below, done);\n-  movptr(Address(rthread, JavaThread::cont_fastpath_offset()), 0);\n-  bind(done);\n-\n-#ifndef _LP64\n-  pop(rrealsp);\n-  pop(rthread);\n-#endif\n+  Label L_done;\n+  cmpptr(rsp, Address(r15_thread, JavaThread::cont_fastpath_offset()));\n+  jccb(Assembler::below, L_done);\n+  movptr(Address(r15_thread, JavaThread::cont_fastpath_offset()), 0);\n+  bind(L_done);\n@@ -3147,2 +2419,0 @@\n-#ifdef _LP64\n-#endif\n@@ -3153,2 +2423,0 @@\n-#ifdef _LP64\n-#endif\n@@ -3160,1 +2428,0 @@\n-#ifdef _LP64\n@@ -3167,3 +2434,0 @@\n-#else\n-  Unimplemented();\n-#endif\n@@ -3173,5 +2437,1 @@\n-void MacroAssembler::reset_last_Java_frame(Register java_thread, bool clear_fp) { \/\/ determine java_thread register\n-  if (!java_thread->is_valid()) {\n-    java_thread = rdi;\n-    get_thread(java_thread);\n-  }\n+void MacroAssembler::reset_last_Java_frame(bool clear_fp) { \/\/ determine java_thread register\n@@ -3179,1 +2439,1 @@\n-  movptr(Address(java_thread, JavaThread::last_Java_sp_offset()), NULL_WORD);\n+  movptr(Address(r15_thread, JavaThread::last_Java_sp_offset()), NULL_WORD);\n@@ -3183,1 +2443,1 @@\n-    movptr(Address(java_thread, JavaThread::last_Java_fp_offset()), NULL_WORD);\n+    movptr(Address(r15_thread, JavaThread::last_Java_fp_offset()), NULL_WORD);\n@@ -3186,1 +2446,1 @@\n-  movptr(Address(java_thread, JavaThread::last_Java_pc_offset()), NULL_WORD);\n+  movptr(Address(r15_thread, JavaThread::last_Java_pc_offset()), NULL_WORD);\n@@ -3190,5 +2450,0 @@\n-void MacroAssembler::restore_rax(Register tmp) {\n-  if (tmp == noreg) pop(rax);\n-  else if (tmp != rax) mov(rax, tmp);\n-}\n-\n@@ -3200,6 +2455,1 @@\n-void MacroAssembler::save_rax(Register tmp) {\n-  if (tmp == noreg) push(rax);\n-  else if (tmp != rax) mov(tmp, rax);\n-}\n-\n-void MacroAssembler::safepoint_poll(Label& slow_path, Register thread_reg, bool at_return, bool in_nmethod) {\n+void MacroAssembler::safepoint_poll(Label& slow_path, bool at_return, bool in_nmethod) {\n@@ -3209,1 +2459,1 @@\n-    cmpptr(in_nmethod ? rsp : rbp, Address(thread_reg, JavaThread::polling_word_offset()));\n+    cmpptr(in_nmethod ? rsp : rbp, Address(r15_thread, JavaThread::polling_word_offset()));\n@@ -3213,1 +2463,1 @@\n-  testb(Address(thread_reg, JavaThread::polling_word_offset()), SafepointMechanism::poll_bit());\n+  testb(Address(r15_thread, JavaThread::polling_word_offset()), SafepointMechanism::poll_bit());\n@@ -3222,2 +2472,1 @@\n-void MacroAssembler::set_last_Java_frame(Register java_thread,\n-                                         Register last_java_sp,\n+void MacroAssembler::set_last_Java_frame(Register last_java_sp,\n@@ -3228,5 +2477,0 @@\n-  \/\/ determine java_thread register\n-  if (!java_thread->is_valid()) {\n-    java_thread = rdi;\n-    get_thread(java_thread);\n-  }\n@@ -3239,1 +2483,1 @@\n-    movptr(Address(java_thread, JavaThread::last_Java_fp_offset()), last_java_fp);\n+    movptr(Address(r15_thread, JavaThread::last_Java_fp_offset()), last_java_fp);\n@@ -3243,1 +2487,1 @@\n-    Address java_pc(java_thread,\n+    Address java_pc(r15_thread,\n@@ -3247,1 +2491,1 @@\n-  movptr(Address(java_thread, JavaThread::last_Java_sp_offset()), last_java_sp);\n+  movptr(Address(r15_thread, JavaThread::last_Java_sp_offset()), last_java_sp);\n@@ -3250,1 +2494,0 @@\n-#ifdef _LP64\n@@ -3257,1 +2500,1 @@\n-  set_last_Java_frame(r15_thread, last_java_sp, last_java_fp, nullptr, scratch);\n+  set_last_Java_frame(last_java_sp, last_java_fp, nullptr, scratch);\n@@ -3259,1 +2502,0 @@\n-#endif\n@@ -3262,1 +2504,1 @@\n-  LP64_ONLY(shlq(dst, imm8)) NOT_LP64(shll(dst, imm8));\n+  shlq(dst, imm8);\n@@ -3266,1 +2508,1 @@\n-  LP64_ONLY(shrq(dst, imm8)) NOT_LP64(shrl(dst, imm8));\n+  shrq(dst, imm8);\n@@ -3270,6 +2512,1 @@\n-  if (LP64_ONLY(true ||) (VM_Version::is_P6() && reg->has_byte_register())) {\n-    movsbl(reg, reg); \/\/ movsxb\n-  } else {\n-    shll(reg, 24);\n-    sarl(reg, 24);\n-  }\n+  movsbl(reg, reg); \/\/ movsxb\n@@ -3279,6 +2516,1 @@\n-  if (LP64_ONLY(true ||) VM_Version::is_P6()) {\n-    movswl(reg, reg); \/\/ movsxw\n-  } else {\n-    shll(reg, 16);\n-    sarl(reg, 16);\n-  }\n+  movswl(reg, reg); \/\/ movsxw\n@@ -3308,2 +2540,0 @@\n-#ifdef _LP64\n-\n@@ -3326,2 +2556,0 @@\n-#endif\n-\n@@ -4114,1 +3342,1 @@\n-                                     Register thread,\n+  Register thread = r15_thread;\n@@ -4124,1 +3352,1 @@\n-  access_load_at(T_OBJECT, IN_NATIVE | AS_RAW, value, Address(value, 0), tmp, thread);\n+  access_load_at(T_OBJECT, IN_NATIVE | AS_RAW, value, Address(value, 0), tmp);\n@@ -4133,1 +3361,1 @@\n-  access_load_at(T_OBJECT, IN_NATIVE, value, Address(value, -JNIHandles::TypeTag::global), tmp, thread);\n+  access_load_at(T_OBJECT, IN_NATIVE, value, Address(value, -JNIHandles::TypeTag::global), tmp);\n@@ -4140,1 +3368,1 @@\n-                 value, Address(value, -JNIHandles::TypeTag::weak_global), tmp, thread);\n+                 value, Address(value, -JNIHandles::TypeTag::weak_global), tmp);\n@@ -4147,1 +3375,1 @@\n-                                            Register thread,\n+  Register thread = r15_thread;\n@@ -4166,1 +3394,1 @@\n-  access_load_at(T_OBJECT, IN_NATIVE, value, Address(value, -JNIHandles::TypeTag::global), tmp, thread);\n+  access_load_at(T_OBJECT, IN_NATIVE, value, Address(value, -JNIHandles::TypeTag::global), tmp);\n@@ -4173,1 +3401,1 @@\n-  LP64_ONLY(subq(dst, imm32)) NOT_LP64(subl(dst, imm32));\n+  subq(dst, imm32);\n@@ -4178,1 +3406,1 @@\n-  LP64_ONLY(subq_imm32(dst, imm32)) NOT_LP64(subl_imm32(dst, imm32));\n+  subq_imm32(dst, imm32);\n@@ -4182,1 +3410,1 @@\n-  LP64_ONLY(subq(dst, src)) NOT_LP64(subl(dst, src));\n+  subq(dst, src);\n@@ -4200,1 +3428,1 @@\n-  LP64_ONLY(testq(dst, src)) NOT_LP64(testl(dst, src));\n+  testq(dst, src);\n@@ -4204,1 +3432,1 @@\n-void MacroAssembler::tlab_allocate(Register thread, Register obj,\n+void MacroAssembler::tlab_allocate(Register obj,\n@@ -4211,1 +3439,1 @@\n-  bs->tlab_allocate(this, thread, obj, var_size_in_bytes, con_size_in_bytes, t1, t2, slow_case);\n+  bs->tlab_allocate(this, obj, var_size_in_bytes, con_size_in_bytes, t1, t2, slow_case);\n@@ -4216,1 +3444,0 @@\n-#ifdef _LP64\n@@ -4222,4 +3449,0 @@\n-#else\n-  regs += RegSet::of(rax, rcx, rdx);\n-#endif\n-#ifdef _LP64\n@@ -4229,1 +3452,0 @@\n-#endif\n@@ -4246,8 +3468,1 @@\n-static int FPUSaveAreaSize = align_up(108, StackAlignmentInBytes); \/\/ 108 bytes needed for FPU state by fsave\/frstor\n-\n-#ifndef _LP64\n-static bool use_x87_registers() { return UseSSE < 2; }\n-#endif\n-static bool use_xmm_registers() { return UseSSE >= 1; }\n-\n-static int xmm_save_size() { return UseSSE >= 2 ? sizeof(double) : sizeof(float); }\n+static int xmm_save_size() { return sizeof(double); }\n@@ -4257,5 +3472,1 @@\n-  if (UseSSE == 1) {\n-    masm->movflt(Address(rsp, offset), reg);\n-  } else {\n-    masm->movdbl(Address(rsp, offset), reg);\n-  }\n+  masm->movdbl(Address(rsp, offset), reg);\n@@ -4265,5 +3476,1 @@\n-  if (UseSSE == 1) {\n-    masm->movflt(reg, Address(rsp, offset));\n-  } else {\n-    masm->movdbl(reg, Address(rsp, offset));\n-  }\n+  masm->movdbl(reg, Address(rsp, offset));\n@@ -4273,2 +3480,1 @@\n-                                  bool save_fpu, int& gp_area_size,\n-                                  int& fp_area_size, int& xmm_area_size) {\n+                                  bool save_fpu, int& gp_area_size, int& xmm_area_size) {\n@@ -4278,6 +3484,1 @@\n-#ifdef _LP64\n-  fp_area_size = 0;\n-#else\n-  fp_area_size = (save_fpu && use_x87_registers()) ? FPUSaveAreaSize : 0;\n-#endif\n-  xmm_area_size = (save_fpu && use_xmm_registers()) ? xmm_registers.size() * xmm_save_size() : 0;\n+  xmm_area_size = save_fpu ? xmm_registers.size() * xmm_save_size() : 0;\n@@ -4285,1 +3486,1 @@\n-  return gp_area_size + fp_area_size + xmm_area_size;\n+  return gp_area_size + xmm_area_size;\n@@ -4294,1 +3495,0 @@\n-  int fp_area_size;\n@@ -4297,1 +3497,1 @@\n-                                               gp_area_size, fp_area_size, xmm_area_size);\n+                                               gp_area_size, xmm_area_size);\n@@ -4302,8 +3502,2 @@\n-#ifndef _LP64\n-  if (save_fpu && use_x87_registers()) {\n-    fnsave(Address(rsp, gp_area_size));\n-    fwait();\n-  }\n-#endif\n-  if (save_fpu && use_xmm_registers()) {\n-    push_set(call_clobbered_xmm_registers(), gp_area_size + fp_area_size);\n+  if (save_fpu) {\n+    push_set(call_clobbered_xmm_registers(), gp_area_size);\n@@ -4321,1 +3515,0 @@\n-  int fp_area_size;\n@@ -4324,1 +3517,1 @@\n-                                               gp_area_size, fp_area_size, xmm_area_size);\n+                                               gp_area_size, xmm_area_size);\n@@ -4326,2 +3519,2 @@\n-  if (restore_fpu && use_xmm_registers()) {\n-    pop_set(call_clobbered_xmm_registers(), gp_area_size + fp_area_size);\n+  if (restore_fpu) {\n+    pop_set(call_clobbered_xmm_registers(), gp_area_size);\n@@ -4329,5 +3522,0 @@\n-#ifndef _LP64\n-  if (restore_fpu && use_x87_registers()) {\n-    frstor(Address(rsp, gp_area_size));\n-  }\n-#endif\n@@ -4433,15 +3621,1 @@\n-#ifndef _LP64\n-  \/\/ index could have not been a multiple of 8 (i.e., bit 2 was set)\n-  {\n-    Label even;\n-    \/\/ note: if index was a multiple of 8, then it cannot\n-    \/\/       be 0 now otherwise it must have been 0 before\n-    \/\/       => if it is even, we don't need to check for 0 again\n-    jcc(Assembler::carryClear, even);\n-    \/\/ clear topmost word (no jump would be needed if conditional assignment worked here)\n-    movptr(Address(address, index, Address::times_8, offset_in_bytes - 0*BytesPerWord), temp);\n-    \/\/ index could be 0 now, must check again\n-    jcc(Assembler::zero, done);\n-    bind(even);\n-  }\n-#endif \/\/ !_LP64\n+\n@@ -4453,1 +3627,0 @@\n-    NOT_LP64(movptr(Address(address, index, Address::times_8, offset_in_bytes - 2*BytesPerWord), temp);)\n@@ -4830,3 +4003,2 @@\n-  NOT_LP64(  incrementl(pst_counter_addr) );\n-  LP64_ONLY( lea(rcx, pst_counter_addr) );\n-  LP64_ONLY( incrementl(Address(rcx, 0)) );\n+  lea(rcx, pst_counter_addr);\n+  incrementl(Address(rcx, 0));\n@@ -4878,16 +4050,0 @@\n-#ifndef _LP64\n-\n-\/\/ 32-bit x86 only: always use the linear search.\n-void MacroAssembler::check_klass_subtype_slow_path(Register sub_klass,\n-                                                   Register super_klass,\n-                                                   Register temp_reg,\n-                                                   Register temp2_reg,\n-                                                   Label* L_success,\n-                                                   Label* L_failure,\n-                                                   bool set_cond_codes) {\n-  check_klass_subtype_slow_path_linear\n-    (sub_klass, super_klass, temp_reg, temp2_reg, L_success, L_failure, set_cond_codes);\n-}\n-\n-#else \/\/ _LP64\n-\n@@ -5477,3 +4633,1 @@\n-#endif \/\/ LP64\n-\n-void MacroAssembler::clinit_barrier(Register klass, Register thread, Label* L_fast_path, Label* L_slow_path) {\n+void MacroAssembler::clinit_barrier(Register klass, Label* L_fast_path, Label* L_slow_path) {\n@@ -5495,1 +4649,1 @@\n-  cmpptr(thread, Address(klass, InstanceKlass::init_thread_offset()));\n+  cmpptr(r15_thread, Address(klass, InstanceKlass::init_thread_offset()));\n@@ -5533,2 +4687,0 @@\n-#ifdef _LP64\n-#endif\n@@ -5593,2 +4745,0 @@\n-#ifdef _LP64\n-#endif\n@@ -5603,1 +4753,1 @@\n-    pushptr(Address(rax, LP64_ONLY(2 *) BytesPerWord));\n+    pushptr(Address(rax, 2 * BytesPerWord));\n@@ -5630,1 +4780,0 @@\n-    Register thread_reg = NOT_LP64(rbx) LP64_ONLY(r15_thread);\n@@ -5633,4 +4782,2 @@\n-    NOT_LP64(push(thread_reg));\n-    NOT_LP64(get_thread(thread_reg));\n-    movptr(t1, Address(thread_reg, in_bytes(JavaThread::tlab_top_offset())));\n-    cmpptr(t1, Address(thread_reg, in_bytes(JavaThread::tlab_start_offset())));\n+    movptr(t1, Address(r15_thread, in_bytes(JavaThread::tlab_top_offset())));\n+    cmpptr(t1, Address(r15_thread, in_bytes(JavaThread::tlab_start_offset())));\n@@ -5643,2 +4790,2 @@\n-    movptr(t1, Address(thread_reg, in_bytes(JavaThread::tlab_end_offset())));\n-    cmpptr(t1, Address(thread_reg, in_bytes(JavaThread::tlab_top_offset())));\n+    movptr(t1, Address(r15_thread, in_bytes(JavaThread::tlab_end_offset())));\n+    cmpptr(t1, Address(r15_thread, in_bytes(JavaThread::tlab_top_offset())));\n@@ -5650,1 +4797,0 @@\n-    NOT_LP64(pop(thread_reg));\n@@ -5930,79 +5076,0 @@\n-\n-#ifndef _LP64\n-static bool _verify_FPU(int stack_depth, char* s, CPU_State* state) {\n-  static int counter = 0;\n-  FPU_State* fs = &state->_fpu_state;\n-  counter++;\n-  \/\/ For leaf calls, only verify that the top few elements remain empty.\n-  \/\/ We only need 1 empty at the top for C2 code.\n-  if( stack_depth < 0 ) {\n-    if( fs->tag_for_st(7) != 3 ) {\n-      printf(\"FPR7 not empty\\n\");\n-      state->print();\n-      assert(false, \"error\");\n-      return false;\n-    }\n-    return true;                \/\/ All other stack states do not matter\n-  }\n-\n-  assert((fs->_control_word._value & 0xffff) == StubRoutines::x86::fpu_cntrl_wrd_std(),\n-         \"bad FPU control word\");\n-\n-  \/\/ compute stack depth\n-  int i = 0;\n-  while (i < FPU_State::number_of_registers && fs->tag_for_st(i)  < 3) i++;\n-  int d = i;\n-  while (i < FPU_State::number_of_registers && fs->tag_for_st(i) == 3) i++;\n-  \/\/ verify findings\n-  if (i != FPU_State::number_of_registers) {\n-    \/\/ stack not contiguous\n-    printf(\"%s: stack not contiguous at ST%d\\n\", s, i);\n-    state->print();\n-    assert(false, \"error\");\n-    return false;\n-  }\n-  \/\/ check if computed stack depth corresponds to expected stack depth\n-  if (stack_depth < 0) {\n-    \/\/ expected stack depth is -stack_depth or less\n-    if (d > -stack_depth) {\n-      \/\/ too many elements on the stack\n-      printf(\"%s: <= %d stack elements expected but found %d\\n\", s, -stack_depth, d);\n-      state->print();\n-      assert(false, \"error\");\n-      return false;\n-    }\n-  } else {\n-    \/\/ expected stack depth is stack_depth\n-    if (d != stack_depth) {\n-      \/\/ wrong stack depth\n-      printf(\"%s: %d stack elements expected but found %d\\n\", s, stack_depth, d);\n-      state->print();\n-      assert(false, \"error\");\n-      return false;\n-    }\n-  }\n-  \/\/ everything is cool\n-  return true;\n-}\n-\n-void MacroAssembler::verify_FPU(int stack_depth, const char* s) {\n-  if (!VerifyFPU) return;\n-  push_CPU_state();\n-  push(rsp);                \/\/ pass CPU state\n-  ExternalAddress msg((address) s);\n-  \/\/ pass message string s\n-  pushptr(msg.addr(), noreg);\n-  push(stack_depth);        \/\/ pass stack depth\n-  call(RuntimeAddress(CAST_FROM_FN_PTR(address, _verify_FPU)));\n-  addptr(rsp, 3 * wordSize);   \/\/ discard arguments\n-  \/\/ check for error\n-  { Label L;\n-    testl(rax, rax);\n-    jcc(Assembler::notZero, L);\n-    int3();                  \/\/ break if error condition\n-    bind(L);\n-  }\n-  pop_CPU_state();\n-}\n-#endif \/\/ _LP64\n-\n@@ -6021,8 +5088,0 @@\n-\n-#ifndef _LP64\n-  \/\/ Either restore the x87 floating pointer control word after returning\n-  \/\/ from the JNI call or verify that it wasn't changed.\n-  if (CheckJNICalls) {\n-    call(RuntimeAddress(StubRoutines::x86::verify_fpu_cntrl_wrd_entry()));\n-  }\n-#endif \/\/ _LP64\n@@ -6039,1 +5098,1 @@\n-                 result, Address(result, 0), tmp, \/*tmp_thread*\/noreg);\n+                 result, Address(result, 0), tmp);\n@@ -6055,1 +5114,1 @@\n-                 rresult, Address(rresult, 0), rtmp, \/*tmp_thread*\/noreg);\n+                 rresult, Address(rresult, 0), rtmp);\n@@ -6078,1 +5137,0 @@\n-#ifdef _LP64\n@@ -6084,1 +5142,0 @@\n-#endif\n@@ -6089,1 +5146,1 @@\n-#ifdef _LP64\n+\n@@ -6096,3 +5153,1 @@\n-  } else\n-#endif\n-  {\n+  } else {\n@@ -6107,1 +5162,0 @@\n-#ifdef _LP64\n@@ -6111,2 +5165,1 @@\n-  } else\n-#endif\n+  } else {\n@@ -6114,0 +5167,1 @@\n+  }\n@@ -6117,1 +5171,0 @@\n-#ifdef _LP64\n@@ -6125,3 +5178,1 @@\n-  } else\n-#endif\n-  {\n+  } else {\n@@ -6133,1 +5184,0 @@\n-#ifdef _LP64\n@@ -6143,3 +5193,1 @@\n-  } else\n-#endif\n-  {\n+  } else {\n@@ -6152,1 +5200,1 @@\n-                                    Register tmp1, Register thread_tmp) {\n+                                    Register tmp1) {\n@@ -6157,1 +5205,1 @@\n-    bs->BarrierSetAssembler::load_at(this, decorators, type, dst, src, tmp1, thread_tmp);\n+    bs->BarrierSetAssembler::load_at(this, decorators, type, dst, src, tmp1);\n@@ -6159,1 +5207,1 @@\n-    bs->load_at(this, decorators, type, dst, src, tmp1, thread_tmp);\n+    bs->load_at(this, decorators, type, dst, src, tmp1);\n@@ -6175,3 +5223,2 @@\n-void MacroAssembler::load_heap_oop(Register dst, Address src, Register tmp1,\n-                                   Register thread_tmp, DecoratorSet decorators) {\n-  access_load_at(T_OBJECT, IN_HEAP | decorators, dst, src, tmp1, thread_tmp);\n+void MacroAssembler::load_heap_oop(Register dst, Address src, Register tmp1, DecoratorSet decorators) {\n+  access_load_at(T_OBJECT, IN_HEAP | decorators, dst, src, tmp1);\n@@ -6181,3 +5228,2 @@\n-void MacroAssembler::load_heap_oop_not_null(Register dst, Address src, Register tmp1,\n-                                            Register thread_tmp, DecoratorSet decorators) {\n-  access_load_at(T_OBJECT, IN_HEAP | IS_NOT_NULL | decorators, dst, src, tmp1, thread_tmp);\n+void MacroAssembler::load_heap_oop_not_null(Register dst, Address src, Register tmp1, DecoratorSet decorators) {\n+  access_load_at(T_OBJECT, IN_HEAP | IS_NOT_NULL | decorators, dst, src, tmp1);\n@@ -6196,1 +5242,0 @@\n-#ifdef _LP64\n@@ -6518,2 +5563,0 @@\n-#endif \/\/ _LP64\n-\n@@ -6698,2 +5741,0 @@\n-    NOT_LP64(shlptr(cnt, 1);) \/\/ convert to number of 32-bit words for 32-bit VM\n-\n@@ -6720,1 +5761,0 @@\n-    NOT_LP64(shlptr(cnt, 1);) \/\/ convert to number of 32-bit words for 32-bit VM\n@@ -6738,1 +5778,1 @@\n-#if defined(COMPILER2) && defined(_LP64)\n+#if defined(COMPILER2)\n@@ -6799,33 +5839,1 @@\n-  if (UseSSE < 2) {\n-    Label L_fill_32_bytes_loop, L_check_fill_8_bytes, L_fill_8_bytes_loop, L_fill_8_bytes;\n-    \/\/ Fill 32-byte chunks\n-    subptr(count, 8 << shift);\n-    jcc(Assembler::less, L_check_fill_8_bytes);\n-    align(16);\n-\n-    BIND(L_fill_32_bytes_loop);\n-\n-    for (int i = 0; i < 32; i += 4) {\n-      movl(Address(to, i), value);\n-    }\n-\n-    addptr(to, 32);\n-    subptr(count, 8 << shift);\n-    jcc(Assembler::greaterEqual, L_fill_32_bytes_loop);\n-    BIND(L_check_fill_8_bytes);\n-    addptr(count, 8 << shift);\n-    jccb(Assembler::zero, L_exit);\n-    jmpb(L_fill_8_bytes);\n-\n-    \/\/\n-    \/\/ length is too short, just fill qwords\n-    \/\/\n-    BIND(L_fill_8_bytes_loop);\n-    movl(Address(to, 0), value);\n-    movl(Address(to, 4), value);\n-    addptr(to, 8);\n-    BIND(L_fill_8_bytes);\n-    subptr(count, 1 << (shift + 1));\n-    jcc(Assembler::greaterEqual, L_fill_8_bytes_loop);\n-    \/\/ fall through to fill 4 bytes\n-  } else {\n+  {\n@@ -6843,1 +5851,0 @@\n-      assert( UseSSE >= 2, \"supported cpu only\" );\n@@ -7151,1 +6158,0 @@\n-#ifdef _LP64\n@@ -8326,1 +7332,0 @@\n-#endif\n@@ -8549,1 +7554,0 @@\n-#ifdef _LP64\n@@ -9067,148 +8071,0 @@\n-#else\n-void MacroAssembler::crc32c_ipl_alg4(Register in_out, uint32_t n,\n-                                     Register tmp1, Register tmp2, Register tmp3,\n-                                     XMMRegister xtmp1, XMMRegister xtmp2) {\n-  lea(tmp3, ExternalAddress(StubRoutines::crc32c_table_addr()));\n-  if (n > 0) {\n-    addl(tmp3, n * 256 * 8);\n-  }\n-  \/\/    Q1 = TABLEExt[n][B & 0xFF];\n-  movl(tmp1, in_out);\n-  andl(tmp1, 0x000000FF);\n-  shll(tmp1, 3);\n-  addl(tmp1, tmp3);\n-  movq(xtmp1, Address(tmp1, 0));\n-\n-  \/\/    Q2 = TABLEExt[n][B >> 8 & 0xFF];\n-  movl(tmp2, in_out);\n-  shrl(tmp2, 8);\n-  andl(tmp2, 0x000000FF);\n-  shll(tmp2, 3);\n-  addl(tmp2, tmp3);\n-  movq(xtmp2, Address(tmp2, 0));\n-\n-  psllq(xtmp2, 8);\n-  pxor(xtmp1, xtmp2);\n-\n-  \/\/    Q3 = TABLEExt[n][B >> 16 & 0xFF];\n-  movl(tmp2, in_out);\n-  shrl(tmp2, 16);\n-  andl(tmp2, 0x000000FF);\n-  shll(tmp2, 3);\n-  addl(tmp2, tmp3);\n-  movq(xtmp2, Address(tmp2, 0));\n-\n-  psllq(xtmp2, 16);\n-  pxor(xtmp1, xtmp2);\n-\n-  \/\/    Q4 = TABLEExt[n][B >> 24 & 0xFF];\n-  shrl(in_out, 24);\n-  andl(in_out, 0x000000FF);\n-  shll(in_out, 3);\n-  addl(in_out, tmp3);\n-  movq(xtmp2, Address(in_out, 0));\n-\n-  psllq(xtmp2, 24);\n-  pxor(xtmp1, xtmp2); \/\/ Result in CXMM\n-  \/\/    return Q1 ^ Q2 << 8 ^ Q3 << 16 ^ Q4 << 24;\n-}\n-\n-void MacroAssembler::crc32c_pclmulqdq(XMMRegister w_xtmp1,\n-                                      Register in_out,\n-                                      uint32_t const_or_pre_comp_const_index, bool is_pclmulqdq_supported,\n-                                      XMMRegister w_xtmp2,\n-                                      Register tmp1,\n-                                      Register n_tmp2, Register n_tmp3) {\n-  if (is_pclmulqdq_supported) {\n-    movdl(w_xtmp1, in_out);\n-\n-    movl(tmp1, const_or_pre_comp_const_index);\n-    movdl(w_xtmp2, tmp1);\n-    pclmulqdq(w_xtmp1, w_xtmp2, 0);\n-    \/\/ Keep result in XMM since GPR is 32 bit in length\n-  } else {\n-    crc32c_ipl_alg4(in_out, const_or_pre_comp_const_index, tmp1, n_tmp2, n_tmp3, w_xtmp1, w_xtmp2);\n-  }\n-}\n-\n-void MacroAssembler::crc32c_rec_alt2(uint32_t const_or_pre_comp_const_index_u1, uint32_t const_or_pre_comp_const_index_u2, bool is_pclmulqdq_supported, Register in_out, Register in1, Register in2,\n-                                     XMMRegister w_xtmp1, XMMRegister w_xtmp2, XMMRegister w_xtmp3,\n-                                     Register tmp1, Register tmp2,\n-                                     Register n_tmp3) {\n-  crc32c_pclmulqdq(w_xtmp1, in_out, const_or_pre_comp_const_index_u1, is_pclmulqdq_supported, w_xtmp3, tmp1, tmp2, n_tmp3);\n-  crc32c_pclmulqdq(w_xtmp2, in1, const_or_pre_comp_const_index_u2, is_pclmulqdq_supported, w_xtmp3, tmp1, tmp2, n_tmp3);\n-\n-  psllq(w_xtmp1, 1);\n-  movdl(tmp1, w_xtmp1);\n-  psrlq(w_xtmp1, 32);\n-  movdl(in_out, w_xtmp1);\n-\n-  xorl(tmp2, tmp2);\n-  crc32(tmp2, tmp1, 4);\n-  xorl(in_out, tmp2);\n-\n-  psllq(w_xtmp2, 1);\n-  movdl(tmp1, w_xtmp2);\n-  psrlq(w_xtmp2, 32);\n-  movdl(in1, w_xtmp2);\n-\n-  xorl(tmp2, tmp2);\n-  crc32(tmp2, tmp1, 4);\n-  xorl(in1, tmp2);\n-  xorl(in_out, in1);\n-  xorl(in_out, in2);\n-}\n-\n-void MacroAssembler::crc32c_proc_chunk(uint32_t size, uint32_t const_or_pre_comp_const_index_u1, uint32_t const_or_pre_comp_const_index_u2, bool is_pclmulqdq_supported,\n-                                       Register in_out1, Register in_out2, Register in_out3,\n-                                       Register tmp1, Register tmp2, Register tmp3,\n-                                       XMMRegister w_xtmp1, XMMRegister w_xtmp2, XMMRegister w_xtmp3,\n-                                       Register tmp4, Register tmp5,\n-                                       Register n_tmp6) {\n-  Label L_processPartitions;\n-  Label L_processPartition;\n-  Label L_exit;\n-\n-  bind(L_processPartitions);\n-  cmpl(in_out1, 3 * size);\n-  jcc(Assembler::less, L_exit);\n-    xorl(tmp1, tmp1);\n-    xorl(tmp2, tmp2);\n-    movl(tmp3, in_out2);\n-    addl(tmp3, size);\n-\n-    bind(L_processPartition);\n-      crc32(in_out3, Address(in_out2, 0), 4);\n-      crc32(tmp1, Address(in_out2, size), 4);\n-      crc32(tmp2, Address(in_out2, size*2), 4);\n-      crc32(in_out3, Address(in_out2, 0+4), 4);\n-      crc32(tmp1, Address(in_out2, size+4), 4);\n-      crc32(tmp2, Address(in_out2, size*2+4), 4);\n-      addl(in_out2, 8);\n-      cmpl(in_out2, tmp3);\n-      jcc(Assembler::less, L_processPartition);\n-\n-        push(tmp3);\n-        push(in_out1);\n-        push(in_out2);\n-        tmp4 = tmp3;\n-        tmp5 = in_out1;\n-        n_tmp6 = in_out2;\n-\n-      crc32c_rec_alt2(const_or_pre_comp_const_index_u1, const_or_pre_comp_const_index_u2, is_pclmulqdq_supported, in_out3, tmp1, tmp2,\n-            w_xtmp1, w_xtmp2, w_xtmp3,\n-            tmp4, tmp5,\n-            n_tmp6);\n-\n-        pop(in_out2);\n-        pop(in_out1);\n-        pop(tmp3);\n-\n-    addl(in_out2, 2 * size);\n-    subl(in_out1, 3 * size);\n-    jmp(L_processPartitions);\n-\n-  bind(L_exit);\n-}\n-#endif \/\/LP64\n-#ifdef _LP64\n@@ -9307,78 +8163,0 @@\n-#else\n-void MacroAssembler::crc32c_ipl_alg2_alt2(Register in_out, Register in1, Register in2,\n-                                          Register tmp1, Register  tmp2, Register tmp3,\n-                                          Register tmp4, Register  tmp5, Register tmp6,\n-                                          XMMRegister w_xtmp1, XMMRegister w_xtmp2, XMMRegister w_xtmp3,\n-                                          bool is_pclmulqdq_supported) {\n-  uint32_t const_or_pre_comp_const_index[CRC32C_NUM_PRECOMPUTED_CONSTANTS];\n-  Label L_wordByWord;\n-  Label L_byteByByteProlog;\n-  Label L_byteByByte;\n-  Label L_exit;\n-\n-  if (is_pclmulqdq_supported) {\n-    const_or_pre_comp_const_index[1] = *(uint32_t *)StubRoutines::crc32c_table_addr();\n-    const_or_pre_comp_const_index[0] = *((uint32_t *)StubRoutines::crc32c_table_addr() + 1);\n-\n-    const_or_pre_comp_const_index[3] = *((uint32_t *)StubRoutines::crc32c_table_addr() + 2);\n-    const_or_pre_comp_const_index[2] = *((uint32_t *)StubRoutines::crc32c_table_addr() + 3);\n-\n-    const_or_pre_comp_const_index[5] = *((uint32_t *)StubRoutines::crc32c_table_addr() + 4);\n-    const_or_pre_comp_const_index[4] = *((uint32_t *)StubRoutines::crc32c_table_addr() + 5);\n-  } else {\n-    const_or_pre_comp_const_index[0] = 1;\n-    const_or_pre_comp_const_index[1] = 0;\n-\n-    const_or_pre_comp_const_index[2] = 3;\n-    const_or_pre_comp_const_index[3] = 2;\n-\n-    const_or_pre_comp_const_index[4] = 5;\n-    const_or_pre_comp_const_index[5] = 4;\n-  }\n-  crc32c_proc_chunk(CRC32C_HIGH, const_or_pre_comp_const_index[0], const_or_pre_comp_const_index[1], is_pclmulqdq_supported,\n-                    in2, in1, in_out,\n-                    tmp1, tmp2, tmp3,\n-                    w_xtmp1, w_xtmp2, w_xtmp3,\n-                    tmp4, tmp5,\n-                    tmp6);\n-  crc32c_proc_chunk(CRC32C_MIDDLE, const_or_pre_comp_const_index[2], const_or_pre_comp_const_index[3], is_pclmulqdq_supported,\n-                    in2, in1, in_out,\n-                    tmp1, tmp2, tmp3,\n-                    w_xtmp1, w_xtmp2, w_xtmp3,\n-                    tmp4, tmp5,\n-                    tmp6);\n-  crc32c_proc_chunk(CRC32C_LOW, const_or_pre_comp_const_index[4], const_or_pre_comp_const_index[5], is_pclmulqdq_supported,\n-                    in2, in1, in_out,\n-                    tmp1, tmp2, tmp3,\n-                    w_xtmp1, w_xtmp2, w_xtmp3,\n-                    tmp4, tmp5,\n-                    tmp6);\n-  movl(tmp1, in2);\n-  andl(tmp1, 0x00000007);\n-  negl(tmp1);\n-  addl(tmp1, in2);\n-  addl(tmp1, in1);\n-\n-  BIND(L_wordByWord);\n-  cmpl(in1, tmp1);\n-  jcc(Assembler::greaterEqual, L_byteByByteProlog);\n-    crc32(in_out, Address(in1,0), 4);\n-    addl(in1, 4);\n-    jmp(L_wordByWord);\n-\n-  BIND(L_byteByByteProlog);\n-  andl(in2, 0x00000007);\n-  movl(tmp2, 1);\n-\n-  BIND(L_byteByByte);\n-  cmpl(tmp2, in2);\n-  jccb(Assembler::greater, L_exit);\n-    movb(tmp1, Address(in1, 0));\n-    crc32(in_out, tmp1, 1);\n-    incl(in1);\n-    incl(tmp2);\n-    jmp(L_byteByByte);\n-\n-  BIND(L_exit);\n-}\n-#endif \/\/ LP64\n@@ -10378,1 +9156,0 @@\n-#ifdef _LP64\n@@ -10555,1 +9332,0 @@\n-#endif\n@@ -10559,1 +9335,0 @@\n-#ifdef _LP64\n@@ -10723,2 +9498,0 @@\n-#endif \/\/ _LP64\n-\n@@ -10749,1 +9522,1 @@\n-void MacroAssembler::get_thread(Register thread) {\n+void MacroAssembler::get_thread_slow(Register thread) {\n@@ -10753,2 +9526,2 @@\n-  LP64_ONLY(push(rdi);)\n-  LP64_ONLY(push(rsi);)\n+  push(rdi);\n+  push(rsi);\n@@ -10757,1 +9530,0 @@\n-#ifdef _LP64\n@@ -10762,1 +9534,0 @@\n-#endif\n@@ -10766,1 +9537,0 @@\n-#ifdef _LP64\n@@ -10771,1 +9541,0 @@\n-#endif\n@@ -10774,2 +9543,2 @@\n-  LP64_ONLY(pop(rsi);)\n-  LP64_ONLY(pop(rdi);)\n+  pop(rsi);\n+  pop(rdi);\n@@ -10804,1 +9573,3 @@\n-void MacroAssembler::lightweight_lock(Register basic_lock, Register obj, Register reg_rax, Register thread, Register tmp, Label& slow) {\n+void MacroAssembler::lightweight_lock(Register basic_lock, Register obj, Register reg_rax, Register tmp, Label& slow) {\n+  Register thread = r15_thread;\n+\n@@ -10816,1 +9587,1 @@\n-    \/\/ Clear cache in case fast locking succeeds.\n+    \/\/ Clear cache in case fast locking succeeds or we need to take the slow-path.\n@@ -10820,0 +9591,6 @@\n+  if (DiagnoseSyncOnValueBasedClasses != 0) {\n+    load_klass(tmp, obj, rscratch1);\n+    testb(Address(tmp, Klass::misc_flags_offset()), KlassFlags::_misc_is_value_based_class);\n+    jcc(Assembler::notZero, slow);\n+  }\n+\n@@ -10858,1 +9635,3 @@\n-void MacroAssembler::lightweight_unlock(Register obj, Register reg_rax, Register thread, Register tmp, Label& slow) {\n+void MacroAssembler::lightweight_unlock(Register obj, Register reg_rax, Register tmp, Label& slow) {\n+  Register thread = r15_thread;\n+\n@@ -10910,1 +9689,0 @@\n-#ifdef _LP64\n@@ -10959,1 +9737,0 @@\n-#endif\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.cpp","additions":181,"deletions":1404,"binary":false,"changes":1585,"status":"modified"},{"patch":"@@ -81,1 +81,0 @@\n-#ifdef _LP64\n@@ -87,3 +86,0 @@\n-#else\n-  __ andptr(result, markWord::hash_mask_in_place);\n-#endif \/\/_LP64\n@@ -92,4 +88,1 @@\n-  __ jcc(Assembler::zero, slowCase);\n-#ifndef _LP64\n-  __ shrptr(result, markWord::hash_shift);\n-#endif\n+  __ jccb(Assembler::zero, slowCase);\n","filename":"src\/hotspot\/cpu\/x86\/sharedRuntime_x86.cpp","additions":1,"deletions":8,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -154,1 +154,1 @@\n-  __ load_heap_oop(dst, src, rdx, rbx, decorators);\n+  __ load_heap_oop(dst, src, rdx, decorators);\n@@ -279,17 +279,12 @@\n-  if (UseSSE >= 1) {\n-    static float one = 1.0f, two = 2.0f;\n-    switch (value) {\n-    case 0:\n-      __ xorps(xmm0, xmm0);\n-      break;\n-    case 1:\n-      __ movflt(xmm0, ExternalAddress((address) &one), rscratch1);\n-      break;\n-    case 2:\n-      __ movflt(xmm0, ExternalAddress((address) &two), rscratch1);\n-      break;\n-    default:\n-      ShouldNotReachHere();\n-      break;\n-    }\n-  } else {\n+  static float one = 1.0f, two = 2.0f;\n+  switch (value) {\n+  case 0:\n+    __ xorps(xmm0, xmm0);\n+    break;\n+  case 1:\n+    __ movflt(xmm0, ExternalAddress((address) &one), rscratch1);\n+    break;\n+  case 2:\n+    __ movflt(xmm0, ExternalAddress((address) &two), rscratch1);\n+    break;\n+  default:\n@@ -297,0 +292,1 @@\n+    break;\n@@ -302,14 +298,9 @@\n-  if (UseSSE >= 2) {\n-    static double one = 1.0;\n-    switch (value) {\n-    case 0:\n-      __ xorpd(xmm0, xmm0);\n-      break;\n-    case 1:\n-      __ movdbl(xmm0, ExternalAddress((address) &one), rscratch1);\n-      break;\n-    default:\n-      ShouldNotReachHere();\n-      break;\n-    }\n-  } else {\n+  static double one = 1.0;\n+  switch (value) {\n+  case 0:\n+    __ xorpd(xmm0, xmm0);\n+    break;\n+  case 1:\n+    __ movdbl(xmm0, ExternalAddress((address) &one), rscratch1);\n+    break;\n+  default:\n@@ -317,0 +308,1 @@\n+    break;\n@@ -376,1 +368,1 @@\n-  __ load_float(Address(rcx, rbx, Address::times_ptr, base_offset));\n+  __ movflt(xmm0, Address(rcx, rbx, Address::times_ptr, base_offset));\n@@ -455,1 +447,1 @@\n-  __ load_double(Address(rcx, rbx, Address::times_ptr, base_offset));\n+  __ movdbl(xmm0, Address(rcx, rbx, Address::times_ptr, base_offset));\n@@ -481,1 +473,1 @@\n-  __ get_vm_result_2(flags, r15_thread);\n+  __ get_vm_result_metadata(flags);\n@@ -509,1 +501,1 @@\n-      __ load_float(field);\n+      __ movflt(xmm0, field);\n@@ -564,1 +556,1 @@\n-      __ load_double(field);\n+      __ movdbl(xmm0, field);\n@@ -658,1 +650,1 @@\n-  __ load_float(faddress(rbx));\n+  __ movflt(xmm0, faddress(rbx));\n@@ -664,1 +656,1 @@\n-  __ load_double(daddress(rbx));\n+  __ movdbl(xmm0, daddress(rbx));\n@@ -695,1 +687,1 @@\n-  __ load_float(faddress(rbx));\n+  __ movflt(xmm0, faddress(rbx));\n@@ -701,1 +693,1 @@\n-  __ load_double(daddress(rbx));\n+  __ movdbl(xmm0, daddress(rbx));\n@@ -743,1 +735,1 @@\n-                    noreg, noreg);\n+                    noreg);\n@@ -755,1 +747,1 @@\n-                    noreg, noreg);\n+                    noreg);\n@@ -769,1 +761,1 @@\n-                    noreg, noreg);\n+                    noreg);\n@@ -781,1 +773,1 @@\n-                    noreg, noreg);\n+                    noreg);\n@@ -804,1 +796,1 @@\n-                    noreg, noreg);\n+                    noreg);\n@@ -814,1 +806,1 @@\n-                    noreg, noreg);\n+                    noreg);\n@@ -829,1 +821,1 @@\n-                    noreg, noreg);\n+                    noreg);\n@@ -840,1 +832,1 @@\n-                    noreg, noreg);\n+                    noreg);\n@@ -855,1 +847,1 @@\n-  __ load_float(faddress(n));\n+  __ movflt(xmm0, faddress(n));\n@@ -860,1 +852,1 @@\n-  __ load_double(daddress(n));\n+  __ movdbl(xmm0, daddress(n));\n@@ -962,1 +954,1 @@\n-  __ store_float(faddress(rbx));\n+  __ movflt(faddress(rbx), xmm0);\n@@ -968,1 +960,1 @@\n-  __ store_double(daddress(rbx));\n+  __ movdbl(daddress(rbx), xmm0);\n@@ -1044,1 +1036,1 @@\n-  \/\/ value is in UseSSE >= 1 ? xmm0 : ST(0)\n+  \/\/ value is in xmm0\n@@ -1057,1 +1049,1 @@\n-  \/\/ value is in UseSSE >= 2 ? xmm0 : ST(0)\n+  \/\/ value is in xmm0\n@@ -1173,1 +1165,1 @@\n-  __ store_float(faddress(n));\n+  __ movflt(faddress(n), xmm0);\n@@ -1178,1 +1170,1 @@\n-  __ store_double(daddress(n));\n+  __ movdbl(daddress(n), xmm0);\n@@ -1400,35 +1392,30 @@\n-  if (UseSSE >= 1) {\n-    switch (op) {\n-    case add:\n-      __ addss(xmm0, at_rsp());\n-      __ addptr(rsp, Interpreter::stackElementSize);\n-      break;\n-    case sub:\n-      __ movflt(xmm1, xmm0);\n-      __ pop_f(xmm0);\n-      __ subss(xmm0, xmm1);\n-      break;\n-    case mul:\n-      __ mulss(xmm0, at_rsp());\n-      __ addptr(rsp, Interpreter::stackElementSize);\n-      break;\n-    case div:\n-      __ movflt(xmm1, xmm0);\n-      __ pop_f(xmm0);\n-      __ divss(xmm0, xmm1);\n-      break;\n-    case rem:\n-      \/\/ On x86_64 platforms the SharedRuntime::frem method is called to perform the\n-      \/\/ modulo operation. The frem method calls the function\n-      \/\/ double fmod(double x, double y) in math.h. The documentation of fmod states:\n-      \/\/ \"If x or y is a NaN, a NaN is returned.\" without specifying what type of NaN\n-      \/\/ (signalling or quiet) is returned.\n-      __ movflt(xmm1, xmm0);\n-      __ pop_f(xmm0);\n-      __ call_VM_leaf(CAST_FROM_FN_PTR(address, SharedRuntime::frem), 2);\n-      break;\n-    default:\n-      ShouldNotReachHere();\n-      break;\n-    }\n-  } else {\n+  switch (op) {\n+  case add:\n+    __ addss(xmm0, at_rsp());\n+    __ addptr(rsp, Interpreter::stackElementSize);\n+    break;\n+  case sub:\n+    __ movflt(xmm1, xmm0);\n+    __ pop_f(xmm0);\n+    __ subss(xmm0, xmm1);\n+    break;\n+  case mul:\n+    __ mulss(xmm0, at_rsp());\n+    __ addptr(rsp, Interpreter::stackElementSize);\n+    break;\n+  case div:\n+    __ movflt(xmm1, xmm0);\n+    __ pop_f(xmm0);\n+    __ divss(xmm0, xmm1);\n+    break;\n+  case rem:\n+    \/\/ On x86_64 platforms the SharedRuntime::frem method is called to perform the\n+    \/\/ modulo operation. The frem method calls the function\n+    \/\/ double fmod(double x, double y) in math.h. The documentation of fmod states:\n+    \/\/ \"If x or y is a NaN, a NaN is returned.\" without specifying what type of NaN\n+    \/\/ (signalling or quiet) is returned.\n+    __ movflt(xmm1, xmm0);\n+    __ pop_f(xmm0);\n+    __ call_VM_leaf(CAST_FROM_FN_PTR(address, SharedRuntime::frem), 2);\n+    break;\n+  default:\n@@ -1436,0 +1423,1 @@\n+    break;\n@@ -1441,33 +1429,28 @@\n-  if (UseSSE >= 2) {\n-    switch (op) {\n-    case add:\n-      __ addsd(xmm0, at_rsp());\n-      __ addptr(rsp, 2 * Interpreter::stackElementSize);\n-      break;\n-    case sub:\n-      __ movdbl(xmm1, xmm0);\n-      __ pop_d(xmm0);\n-      __ subsd(xmm0, xmm1);\n-      break;\n-    case mul:\n-      __ mulsd(xmm0, at_rsp());\n-      __ addptr(rsp, 2 * Interpreter::stackElementSize);\n-      break;\n-    case div:\n-      __ movdbl(xmm1, xmm0);\n-      __ pop_d(xmm0);\n-      __ divsd(xmm0, xmm1);\n-      break;\n-    case rem:\n-      \/\/ Similar to fop2(), the modulo operation is performed using the\n-      \/\/ SharedRuntime::drem method on x86_64 platforms for the same reasons\n-      \/\/ as mentioned in fop2().\n-      __ movdbl(xmm1, xmm0);\n-      __ pop_d(xmm0);\n-      __ call_VM_leaf(CAST_FROM_FN_PTR(address, SharedRuntime::drem), 2);\n-      break;\n-    default:\n-      ShouldNotReachHere();\n-      break;\n-    }\n-  } else {\n+  switch (op) {\n+  case add:\n+    __ addsd(xmm0, at_rsp());\n+    __ addptr(rsp, 2 * Interpreter::stackElementSize);\n+    break;\n+  case sub:\n+    __ movdbl(xmm1, xmm0);\n+    __ pop_d(xmm0);\n+    __ subsd(xmm0, xmm1);\n+    break;\n+  case mul:\n+    __ mulsd(xmm0, at_rsp());\n+    __ addptr(rsp, 2 * Interpreter::stackElementSize);\n+    break;\n+  case div:\n+    __ movdbl(xmm1, xmm0);\n+    __ pop_d(xmm0);\n+    __ divsd(xmm0, xmm1);\n+    break;\n+  case rem:\n+    \/\/ Similar to fop2(), the modulo operation is performed using the\n+    \/\/ SharedRuntime::drem method on x86_64 platforms for the same reasons\n+    \/\/ as mentioned in fop2().\n+    __ movdbl(xmm1, xmm0);\n+    __ pop_d(xmm0);\n+    __ call_VM_leaf(CAST_FROM_FN_PTR(address, SharedRuntime::drem), 2);\n+    break;\n+  default:\n@@ -1475,0 +1458,1 @@\n+    break;\n@@ -1505,6 +1489,2 @@\n-  if (UseSSE >= 1) {\n-    static jlong *float_signflip  = double_quadword(&float_signflip_pool[1],  CONST64(0x8000000080000000),  CONST64(0x8000000080000000));\n-    __ xorps(xmm0, ExternalAddress((address) float_signflip), rscratch1);\n-  } else {\n-    ShouldNotReachHere();\n-  }\n+  static jlong *float_signflip  = double_quadword(&float_signflip_pool[1],  CONST64(0x8000000080000000),  CONST64(0x8000000080000000));\n+  __ xorps(xmm0, ExternalAddress((address) float_signflip), rscratch1);\n@@ -1515,7 +1495,3 @@\n-  if (UseSSE >= 2) {\n-    static jlong *double_signflip =\n-      double_quadword(&double_signflip_pool[1], CONST64(0x8000000000000000), CONST64(0x8000000000000000));\n-    __ xorpd(xmm0, ExternalAddress((address) double_signflip), rscratch1);\n-  } else {\n-    ShouldNotReachHere();\n-  }\n+  static jlong *double_signflip =\n+    double_quadword(&double_signflip_pool[1], CONST64(0x8000000000000000), CONST64(0x8000000000000000));\n+  __ xorpd(xmm0, ExternalAddress((address) double_signflip), rscratch1);\n@@ -1685,27 +1661,5 @@\n-  if ((is_float && UseSSE >= 1) ||\n-      (!is_float && UseSSE >= 2)) {\n-    Label done;\n-    if (is_float) {\n-      \/\/ XXX get rid of pop here, use ... reg, mem32\n-      __ pop_f(xmm1);\n-      __ ucomiss(xmm1, xmm0);\n-    } else {\n-      \/\/ XXX get rid of pop here, use ... reg, mem64\n-      __ pop_d(xmm1);\n-      __ ucomisd(xmm1, xmm0);\n-    }\n-    if (unordered_result < 0) {\n-      __ movl(rax, -1);\n-      __ jccb(Assembler::parity, done);\n-      __ jccb(Assembler::below, done);\n-      __ setb(Assembler::notEqual, rdx);\n-      __ movzbl(rax, rdx);\n-    } else {\n-      __ movl(rax, 1);\n-      __ jccb(Assembler::parity, done);\n-      __ jccb(Assembler::above, done);\n-      __ movl(rax, 0);\n-      __ jccb(Assembler::equal, done);\n-      __ decrementl(rax);\n-    }\n-    __ bind(done);\n+  Label done;\n+  if (is_float) {\n+    \/\/ XXX get rid of pop here, use ... reg, mem32\n+    __ pop_f(xmm1);\n+    __ ucomiss(xmm1, xmm0);\n@@ -1713,1 +1667,17 @@\n-    ShouldNotReachHere();\n+    \/\/ XXX get rid of pop here, use ... reg, mem64\n+    __ pop_d(xmm1);\n+    __ ucomisd(xmm1, xmm0);\n+  }\n+  if (unordered_result < 0) {\n+    __ movl(rax, -1);\n+    __ jccb(Assembler::parity, done);\n+    __ jccb(Assembler::below, done);\n+    __ setb(Assembler::notEqual, rdx);\n+    __ movzbl(rax, rdx);\n+  } else {\n+    __ movl(rax, 1);\n+    __ jccb(Assembler::parity, done);\n+    __ jccb(Assembler::above, done);\n+    __ movl(rax, 0);\n+    __ jccb(Assembler::equal, done);\n+    __ decrementl(rax);\n@@ -1715,0 +1685,1 @@\n+  __ bind(done);\n@@ -2266,2 +2237,0 @@\n-    const Register thread = r15_thread;\n-    assert(thread != noreg, \"x86_32 not supported\");\n@@ -2271,1 +2240,1 @@\n-    __ clinit_barrier(klass, thread, nullptr \/*L_fast_path*\/, &L_clinit_barrier_slow);\n+    __ clinit_barrier(klass, nullptr \/*L_fast_path*\/, &L_clinit_barrier_slow);\n@@ -2571,1 +2540,1 @@\n-  __ access_load_at(T_BYTE, IN_HEAP, rax, field, noreg, noreg);\n+  __ access_load_at(T_BYTE, IN_HEAP, rax, field, noreg);\n@@ -2584,1 +2553,1 @@\n-  __ access_load_at(T_BOOLEAN, IN_HEAP, rax, field, noreg, noreg);\n+  __ access_load_at(T_BOOLEAN, IN_HEAP, rax, field, noreg);\n@@ -2608,1 +2577,1 @@\n-  __ access_load_at(T_INT, IN_HEAP, rax, field, noreg, noreg);\n+  __ access_load_at(T_INT, IN_HEAP, rax, field, noreg);\n@@ -2620,1 +2589,1 @@\n-  __ access_load_at(T_CHAR, IN_HEAP, rax, field, noreg, noreg);\n+  __ access_load_at(T_CHAR, IN_HEAP, rax, field, noreg);\n@@ -2632,1 +2601,1 @@\n-  __ access_load_at(T_SHORT, IN_HEAP, rax, field, noreg, noreg);\n+  __ access_load_at(T_SHORT, IN_HEAP, rax, field, noreg);\n@@ -2646,1 +2615,1 @@\n-  __ access_load_at(T_LONG, IN_HEAP | MO_RELAXED, noreg \/* ltos *\/, field, noreg, noreg);\n+  __ access_load_at(T_LONG, IN_HEAP | MO_RELAXED, noreg \/* ltos *\/, field, noreg);\n@@ -2657,1 +2626,1 @@\n-  __ access_load_at(T_FLOAT, IN_HEAP, noreg \/* ftos *\/, field, noreg, noreg);\n+  __ access_load_at(T_FLOAT, IN_HEAP, noreg \/* ftos *\/, field, noreg);\n@@ -2673,1 +2642,1 @@\n-  __ access_load_at(T_DOUBLE, IN_HEAP | MO_RELAXED, noreg \/* dtos *\/, field, noreg, noreg);\n+  __ access_load_at(T_DOUBLE, IN_HEAP | MO_RELAXED, noreg \/* dtos *\/, field, noreg);\n@@ -3136,1 +3105,1 @@\n-    __ access_load_at(T_LONG, IN_HEAP, noreg \/* ltos *\/, field, noreg, noreg);\n+    __ access_load_at(T_LONG, IN_HEAP, noreg \/* ltos *\/, field, noreg);\n@@ -3139,1 +3108,1 @@\n-    __ access_load_at(T_INT, IN_HEAP, rax, field, noreg, noreg);\n+    __ access_load_at(T_INT, IN_HEAP, rax, field, noreg);\n@@ -3142,1 +3111,1 @@\n-    __ access_load_at(T_BYTE, IN_HEAP, rax, field, noreg, noreg);\n+    __ access_load_at(T_BYTE, IN_HEAP, rax, field, noreg);\n@@ -3145,1 +3114,1 @@\n-    __ access_load_at(T_SHORT, IN_HEAP, rax, field, noreg, noreg);\n+    __ access_load_at(T_SHORT, IN_HEAP, rax, field, noreg);\n@@ -3148,1 +3117,1 @@\n-    __ access_load_at(T_CHAR, IN_HEAP, rax, field, noreg, noreg);\n+    __ access_load_at(T_CHAR, IN_HEAP, rax, field, noreg);\n@@ -3151,1 +3120,1 @@\n-    __ access_load_at(T_FLOAT, IN_HEAP, noreg \/* ftos *\/, field, noreg, noreg);\n+    __ access_load_at(T_FLOAT, IN_HEAP, noreg \/* ftos *\/, field, noreg);\n@@ -3154,1 +3123,1 @@\n-    __ access_load_at(T_DOUBLE, IN_HEAP, noreg \/* dtos *\/, field, noreg, noreg);\n+    __ access_load_at(T_DOUBLE, IN_HEAP, noreg \/* dtos *\/, field, noreg);\n@@ -3183,1 +3152,1 @@\n-    __ access_load_at(T_INT, IN_HEAP, rax, field, noreg, noreg);\n+    __ access_load_at(T_INT, IN_HEAP, rax, field, noreg);\n@@ -3190,1 +3159,1 @@\n-    __ access_load_at(T_FLOAT, IN_HEAP, noreg \/* ftos *\/, field, noreg, noreg);\n+    __ access_load_at(T_FLOAT, IN_HEAP, noreg \/* ftos *\/, field, noreg);\n@@ -3575,1 +3544,1 @@\n-  __ clinit_barrier(rcx, r15_thread, nullptr \/*L_fast_path*\/, &slow_case);\n+  __ clinit_barrier(rcx, nullptr \/*L_fast_path*\/, &slow_case);\n@@ -3593,1 +3562,1 @@\n-    __ tlab_allocate(r15_thread, rax, rdx, 0, rcx, rbx, slow_case);\n+    __ tlab_allocate(rax, rdx, 0, rcx, rbx, slow_case);\n@@ -3713,2 +3682,1 @@\n-  \/\/ vm_result_2 has metadata result\n-  __ get_vm_result_2(rax, r15_thread);\n+  __ get_vm_result_metadata(rax);\n@@ -3769,2 +3737,1 @@\n-  \/\/ vm_result_2 has metadata result\n-  __ get_vm_result_2(rax, r15_thread);\n+  __ get_vm_result_metadata(rax);\n","filename":"src\/hotspot\/cpu\/x86\/templateTable_x86.cpp","additions":159,"deletions":192,"binary":false,"changes":351,"status":"modified"},{"patch":"@@ -425,0 +425,12 @@\n+bool castLL_is_imm32(const Node* n);\n+\n+%}\n+\n+source %{\n+\n+bool castLL_is_imm32(const Node* n) {\n+  assert(n->is_CastLL(), \"must be a CastLL\");\n+  const TypeLong* t = n->bottom_type()->is_long();\n+  return (t->_lo == min_jlong || Assembler::is_simm32(t->_lo)) && (t->_hi == max_jlong || Assembler::is_simm32(t->_hi));\n+}\n+\n@@ -848,1 +860,1 @@\n-    __ clinit_barrier(klass, r15_thread, &L_skip_barrier \/*L_fast_path*\/);\n+    __ clinit_barrier(klass, &L_skip_barrier \/*L_fast_path*\/);\n@@ -946,1 +958,1 @@\n-    __ safepoint_poll(*code_stub, r15_thread, true \/* at_return *\/, true \/* in_nmethod *\/);\n+    __ safepoint_poll(*code_stub, true \/* at_return *\/, true \/* in_nmethod *\/);\n@@ -1587,4 +1599,1 @@\n-  if (EnableVectorSupport && UseVectorStubs) {\n-    return true;\n-  }\n-  return false;\n+  return EnableVectorSupport;\n@@ -1594,1 +1603,1 @@\n-  assert(EnableVectorSupport && UseVectorStubs, \"sanity\");\n+  assert(EnableVectorSupport, \"sanity\");\n@@ -1841,1 +1850,1 @@\n-    debug_only(int off0 = __ offset());\n+    DEBUG_ONLY(int off0 = __ offset());\n@@ -1848,1 +1857,1 @@\n-    debug_only(int off1 = __ offset());\n+    DEBUG_ONLY(int off1 = __ offset());\n@@ -7402,1 +7411,1 @@\n-instruct incL_rReg(rRegI dst, immL1 src, rFlagsReg cr)\n+instruct incL_rReg(rRegL dst, immL1 src, rFlagsReg cr)\n@@ -7415,1 +7424,1 @@\n-instruct incL_rReg_ndd(rRegI dst, rRegI src, immL1 val, rFlagsReg cr)\n+instruct incL_rReg_ndd(rRegL dst, rRegI src, immL1 val, rFlagsReg cr)\n@@ -7428,1 +7437,1 @@\n-instruct incL_rReg_mem_ndd(rRegI dst, memory src, immL1 val, rFlagsReg cr)\n+instruct incL_rReg_mem_ndd(rRegL dst, memory src, immL1 val, rFlagsReg cr)\n@@ -7611,0 +7620,1 @@\n+  predicate(VerifyConstraintCasts == 0);\n@@ -7620,0 +7630,13 @@\n+instruct castII_checked(rRegI dst, rFlagsReg cr)\n+%{\n+  predicate(VerifyConstraintCasts > 0);\n+  match(Set dst (CastII dst));\n+\n+  effect(KILL cr);\n+  format %{ \"# cast_checked_II $dst\" %}\n+  ins_encode %{\n+    __ verify_int_in_range(_idx, bottom_type()->is_int(), $dst$$Register);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n@@ -7622,0 +7645,1 @@\n+  predicate(VerifyConstraintCasts == 0);\n@@ -7631,0 +7655,26 @@\n+instruct castLL_checked_L32(rRegL dst, rFlagsReg cr)\n+%{\n+  predicate(VerifyConstraintCasts > 0 && castLL_is_imm32(n));\n+  match(Set dst (CastLL dst));\n+\n+  effect(KILL cr);\n+  format %{ \"# cast_checked_LL $dst\" %}\n+  ins_encode %{\n+    __ verify_long_in_range(_idx, bottom_type()->is_long(), $dst$$Register, noreg);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct castLL_checked(rRegL dst, rRegL tmp, rFlagsReg cr)\n+%{\n+  predicate(VerifyConstraintCasts > 0 && !castLL_is_imm32(n));\n+  match(Set dst (CastLL dst));\n+\n+  effect(KILL cr, TEMP tmp);\n+  format %{ \"# cast_checked_LL $dst\\tusing $tmp as TEMP\" %}\n+  ins_encode %{\n+    __ verify_long_in_range(_idx, bottom_type()->is_long(), $dst$$Register, $tmp$$Register);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n@@ -11351,1 +11401,1 @@\n-    __ exorq($dst$$Register, $src1$$Address, $src1$$Register, false);\n+    __ exorq($dst$$Register, $src1$$Address, $src2$$Register, false);\n","filename":"src\/hotspot\/cpu\/x86\/x86_64.ad","additions":63,"deletions":13,"binary":false,"changes":76,"status":"modified"},{"patch":"@@ -312,1 +312,2 @@\n-                                             os::vm_page_size());\n+                                             os::vm_page_size(),\n+                                             mtClassShared);\n@@ -1204,1 +1205,1 @@\n-  \/\/ runtime, this region will be mapped to requested_base. requested_base is 0 if this\n+  \/\/ runtime, this region will be mapped to requested_base. requested_base is nullptr if this\n@@ -1213,1 +1214,5 @@\n-    top = requested_base + size;\n+    if (requested_base == nullptr) {\n+      top = (address)size;\n+    } else {\n+      top = requested_base + size;\n+    }\n","filename":"src\/hotspot\/share\/cds\/archiveBuilder.cpp","additions":8,"deletions":3,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -25,0 +25,1 @@\n+#include \"cds\/aotReferenceObjSupport.hpp\"\n@@ -618,1 +619,1 @@\n-\n+  bool _is_java_lang_ref;\n@@ -621,1 +622,4 @@\n-    _src_obj(src_obj), _buffered_obj(buffered_obj), _oopmap(oopmap) {}\n+    _src_obj(src_obj), _buffered_obj(buffered_obj), _oopmap(oopmap)\n+  {\n+    _is_java_lang_ref = AOTReferenceObjSupport::check_if_ref_obj(src_obj);\n+  }\n@@ -628,2 +632,8 @@\n-    size_t field_offset = pointer_delta(p, _src_obj, sizeof(char));\n-    ArchiveHeapWriter::relocate_field_in_buffer<T>((T*)(_buffered_obj + field_offset), _oopmap);\n+    int field_offset = pointer_delta_as_int((char*)p, cast_from_oop<char*>(_src_obj));\n+    T* field_addr = (T*)(_buffered_obj + field_offset);\n+    if (_is_java_lang_ref && AOTReferenceObjSupport::skip_field(field_offset)) {\n+      \/\/ Do not copy these fields. Set them to null\n+      *field_addr = (T)0x0;\n+    } else {\n+      ArchiveHeapWriter::relocate_field_in_buffer<T>(field_addr, _oopmap);\n+    }\n","filename":"src\/hotspot\/share\/cds\/archiveHeapWriter.cpp","additions":14,"deletions":4,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -28,0 +28,1 @@\n+#include \"cds\/aotReferenceObjSupport.hpp\"\n@@ -1382,2 +1383,3 @@\n-\/\/ Push all oops that are referenced by _referencing_obj onto the _stack.\n-class HeapShared::ReferentPusher: public BasicOopIterateClosure {\n+\/\/ Push all oop fields (or oop array elemenets in case of an objArray) in\n+\/\/ _referencing_obj onto the _stack.\n+class HeapShared::OopFieldPusher: public BasicOopIterateClosure {\n@@ -1390,0 +1392,1 @@\n+  bool _is_java_lang_ref;\n@@ -1391,5 +1394,5 @@\n-  ReferentPusher(PendingOopStack* stack,\n-                           int level,\n-                           bool record_klasses_only,\n-                           KlassSubGraphInfo* subgraph_info,\n-                           oop orig) :\n+  OopFieldPusher(PendingOopStack* stack,\n+                 int level,\n+                 bool record_klasses_only,\n+                 KlassSubGraphInfo* subgraph_info,\n+                 oop orig) :\n@@ -1402,0 +1405,1 @@\n+    _is_java_lang_ref = AOTReferenceObjSupport::check_if_ref_obj(orig);\n@@ -1403,2 +1407,2 @@\n-  void do_oop(narrowOop *p) { ReferentPusher::do_oop_work(p); }\n-  void do_oop(      oop *p) { ReferentPusher::do_oop_work(p); }\n+  void do_oop(narrowOop *p) { OopFieldPusher::do_oop_work(p); }\n+  void do_oop(      oop *p) { OopFieldPusher::do_oop_work(p); }\n@@ -1406,1 +1410,1 @@\n-  ~ReferentPusher() {\n+  ~OopFieldPusher() {\n@@ -1409,1 +1413,1 @@\n-      \/\/ of ReferentPusher that recurses on the C stack -- a depth-first search,\n+      \/\/ of OopFieldPusher that recurses on the C stack -- a depth-first search,\n@@ -1418,1 +1422,2 @@\n-    oop obj = RawAccess<>::oop_load(p);\n+    int field_offset = pointer_delta_as_int((char*)p, cast_from_oop<char*>(_referencing_obj));\n+    oop obj = HeapAccess<ON_UNKNOWN_OOP_REF>::oop_load_at(_referencing_obj, field_offset);\n@@ -1420,1 +1425,4 @@\n-      size_t field_delta = pointer_delta(p, _referencing_obj, sizeof(char));\n+      if (_is_java_lang_ref && AOTReferenceObjSupport::skip_field(field_offset)) {\n+        \/\/ Do not follow these fields. They will be cleared to null.\n+        return;\n+      }\n@@ -1424,2 +1432,2 @@\n-        log_debug(cds, heap)(\"(%d) %s[%zu] ==> \" PTR_FORMAT \" size %zu %s\", _level,\n-                             _referencing_obj->klass()->external_name(), field_delta,\n+        log_debug(cds, heap)(\"(%d) %s[%d] ==> \" PTR_FORMAT \" size %zu %s\", _level,\n+                             _referencing_obj->klass()->external_name(), field_offset,\n@@ -1605,1 +1613,1 @@\n-    ReferentPusher pusher(stack, level, record_klasses_only, subgraph_info, orig_obj);\n+    OopFieldPusher pusher(stack, level, record_klasses_only, subgraph_info, orig_obj);\n@@ -1632,1 +1640,1 @@\n-\/\/ The Java heap object sub-graph archiving process (see ReferentPusher):\n+\/\/ The Java heap object sub-graph archiving process (see OopFieldPusher):\n","filename":"src\/hotspot\/share\/cds\/heapShared.cpp","additions":25,"deletions":17,"binary":false,"changes":42,"status":"modified"},{"patch":"@@ -179,1 +179,1 @@\n-  debug_only(const u1* const old_current = stream->current();)\n+  DEBUG_ONLY(const u1* const old_current = stream->current();)\n@@ -5250,1 +5250,1 @@\n-  debug_only(ik->verify();)\n+  DEBUG_ONLY(ik->verify();)\n@@ -5340,1 +5340,2 @@\n-  _need_verify = Verifier::should_verify_for(_loader_data->class_loader());\n+  \/\/ Always verify CFLH bytes from the user agents.\n+  _need_verify = stream->from_class_file_load_hook() ? true : Verifier::should_verify_for(_loader_data->class_loader());\n","filename":"src\/hotspot\/share\/classfile\/classFileParser.cpp","additions":4,"deletions":3,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -129,1 +129,1 @@\n-void FieldLayout::initialize_instance_layout(const InstanceKlass* super_klass) {\n+void FieldLayout::initialize_instance_layout(const InstanceKlass* super_klass, bool& super_ends_with_oop) {\n@@ -131,0 +131,1 @@\n+    super_ends_with_oop = false;\n@@ -137,1 +138,2 @@\n-    bool has_fields = reconstruct_layout(super_klass);\n+    bool super_has_instance_fields = false;\n+    reconstruct_layout(super_klass, super_has_instance_fields, super_ends_with_oop);\n@@ -139,1 +141,1 @@\n-    if (!super_klass->has_contended_annotations() || !has_fields) {\n+    if (!super_klass->has_contended_annotations() || !super_has_instance_fields) {\n@@ -313,2 +315,2 @@\n-bool FieldLayout::reconstruct_layout(const InstanceKlass* ik) {\n-  bool has_instance_fields = false;\n+void FieldLayout::reconstruct_layout(const InstanceKlass* ik, bool& has_instance_fields, bool& ends_with_oop) {\n+  has_instance_fields = ends_with_oop = false;\n@@ -316,0 +318,2 @@\n+  BasicType last_type;\n+  int last_offset = -1;\n@@ -322,0 +326,4 @@\n+      if (fs.offset() > last_offset) {\n+        last_offset = fs.offset();\n+        last_type = type;\n+      }\n@@ -330,0 +338,5 @@\n+  assert(last_offset == -1 || last_offset > 0, \"Sanity\");\n+  if (last_offset > 0 &&\n+      (last_type == BasicType::T_ARRAY || last_type == BasicType::T_OBJECT)) {\n+    ends_with_oop = true;\n+  }\n@@ -343,1 +356,0 @@\n-  return has_instance_fields;\n@@ -536,1 +548,1 @@\n-  _layout->initialize_instance_layout(super_klass);\n+  _layout->initialize_instance_layout(super_klass, _super_ends_with_oop);\n@@ -614,2 +626,8 @@\n-\/\/   - then oop fields are allocated, either in existing gaps or at the end of\n-\/\/     the layout\n+\/\/   - oop fields are allocated, either in existing gaps or at the end of\n+\/\/     the layout. We allocate oops in a single block to have a single oop map entry.\n+\/\/   - if the super class ended with an oop, we lead with oops. That will cause the\n+\/\/     trailing oop map entry of the super class and the oop map entry of this class\n+\/\/     to be folded into a single entry later. Correspondingly, if the super class\n+\/\/     ends with a primitive field, we gain nothing by leading with oops; therefore\n+\/\/     we let oop fields trail, thus giving future derived classes the chance to apply\n+\/\/     the same trick.\n@@ -628,2 +646,8 @@\n-  _layout->add(_root_group->primitive_fields());\n-  _layout->add(_root_group->oop_fields());\n+\n+  if (_super_ends_with_oop) {\n+    _layout->add(_root_group->oop_fields());\n+    _layout->add(_root_group->primitive_fields());\n+  } else {\n+    _layout->add(_root_group->primitive_fields());\n+    _layout->add(_root_group->oop_fields());\n+  }\n","filename":"src\/hotspot\/share\/classfile\/fieldLayoutBuilder.cpp","additions":35,"deletions":11,"binary":false,"changes":46,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2020, 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2020, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -172,1 +172,1 @@\n-  void initialize_instance_layout(const InstanceKlass* ik);\n+  void initialize_instance_layout(const InstanceKlass* ik, bool& super_ends_with_oop);\n@@ -192,1 +192,1 @@\n-  bool reconstruct_layout(const InstanceKlass* ik);\n+  void reconstruct_layout(const InstanceKlass* ik, bool& has_instance_fields, bool& ends_with_oop);\n@@ -241,0 +241,1 @@\n+  bool _super_ends_with_oop;\n","filename":"src\/hotspot\/share\/classfile\/fieldLayoutBuilder.hpp","additions":4,"deletions":3,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -25,0 +25,1 @@\n+#include \"cds\/aotReferenceObjSupport.hpp\"\n@@ -4824,1 +4825,1 @@\n-  debug_only(jint loop_count = 0);\n+  DEBUG_ONLY(jint loop_count = 0);\n@@ -5463,3 +5464,1 @@\n-  if (klass->is_subclass_of(vmClasses::Reference_klass())) {\n-    \/\/ It's problematic to archive Reference objects. One of the reasons is that\n-    \/\/ Reference::discovered may pull in unwanted objects (see JDK-8284336)\n+  if (!AOTReferenceObjSupport::is_enabled() && klass->is_subclass_of(vmClasses::Reference_klass())) {\n","filename":"src\/hotspot\/share\/classfile\/javaClasses.cpp","additions":3,"deletions":4,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -73,0 +73,3 @@\n+  if (FLAG_IS_DEFAULT(G1HeapRegionSize)) {\n+    return G1HeapRegion::max_ergonomics_size();\n+  }\n","filename":"src\/hotspot\/share\/gc\/g1\/g1Arguments.cpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -38,0 +38,1 @@\n+#include \"gc\/shared\/classUnloadingContext.hpp\"\n@@ -40,1 +41,0 @@\n-#include \"gc\/shared\/classUnloadingContext.hpp\"\n","filename":"src\/hotspot\/share\/gc\/g1\/g1FullCollector.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -241,2 +241,1 @@\n-void G1ParScanThreadState::start_partial_objarray(G1HeapRegionAttr dest_attr,\n-                                                  oop from_obj,\n+void G1ParScanThreadState::start_partial_objarray(oop from_obj,\n@@ -254,6 +253,1 @@\n-  \/\/ Skip the card enqueue iff the object (to_array) is in survivor region.\n-  \/\/ However, G1HeapRegion::is_survivor() is too expensive here.\n-  \/\/ Instead, we use dest_attr.is_young() because the two values are always\n-  \/\/ equal: successfully allocated young regions must be survivor regions.\n-  assert(dest_attr.is_young() == _g1h->heap_region_containing(to_array)->is_survivor(), \"must be\");\n-  G1SkipCardEnqueueSetter x(&_scanner, dest_attr.is_young());\n+  assert(_scanner.skip_card_enqueue_set(), \"must be\");\n@@ -425,0 +419,39 @@\n+ALWAYSINLINE\n+void G1ParScanThreadState::do_iterate_object(oop const obj,\n+                                             oop const old,\n+                                             Klass* const klass,\n+                                             G1HeapRegionAttr const region_attr,\n+                                             G1HeapRegionAttr const dest_attr,\n+                                             uint age) {\n+    \/\/ Most objects are not arrays, so do one array check rather than\n+    \/\/ checking for each array category for each object.\n+    if (klass->is_array_klass()) {\n+      assert(!klass->is_stack_chunk_instance_klass(), \"must be\");\n+\n+      if (klass->is_objArray_klass()) {\n+        start_partial_objarray(old, obj);\n+      } else {\n+        \/\/ Nothing needs to be done for typeArrays.  Body doesn't contain\n+        \/\/ any oops to scan, and the type in the klass will already be handled\n+        \/\/ by processing the built-in module.\n+        assert(klass->is_typeArray_klass(), \"invariant\");\n+      }\n+      return;\n+    }\n+\n+    ContinuationGCSupport::transform_stack_chunk(obj);\n+\n+    \/\/ Check for deduplicating young Strings.\n+    if (G1StringDedup::is_candidate_from_evacuation(klass,\n+                                                    region_attr,\n+                                                    dest_attr,\n+                                                    age)) {\n+      \/\/ Record old; request adds a new weak reference, which reference\n+      \/\/ processing expects to refer to a from-space object.\n+      _string_dedup_requests.add(old);\n+    }\n+\n+    assert(_scanner.skip_card_enqueue_set(), \"must be\");\n+    obj->oop_iterate_backwards(&_scanner, klass);\n+}\n+\n@@ -450,1 +483,1 @@\n-    return handle_evacuation_failure_par(old, old_mark, word_sz, true \/* cause_pinned *\/);\n+    return handle_evacuation_failure_par(old, old_mark, klass, region_attr, word_sz, true \/* cause_pinned *\/);\n@@ -467,1 +500,1 @@\n-      return handle_evacuation_failure_par(old, old_mark, word_sz, false \/* cause_pinned *\/);\n+      return handle_evacuation_failure_par(old, old_mark, klass, region_attr, word_sz, false \/* cause_pinned *\/);\n@@ -479,1 +512,1 @@\n-    return handle_evacuation_failure_par(old, old_mark, word_sz, false \/* cause_pinned *\/);\n+    return handle_evacuation_failure_par(old, old_mark, klass, region_attr, word_sz, false \/* cause_pinned *\/);\n@@ -513,24 +546,8 @@\n-    \/\/ Most objects are not arrays, so do one array check rather than\n-    \/\/ checking for each array category for each object.\n-    if (klass->is_array_klass()) {\n-      if (klass->is_objArray_klass()) {\n-        start_partial_objarray(dest_attr, old, obj);\n-      } else {\n-        \/\/ Nothing needs to be done for typeArrays.  Body doesn't contain\n-        \/\/ any oops to scan, and the type in the klass will already be handled\n-        \/\/ by processing the built-in module.\n-        assert(klass->is_typeArray_klass(), \"invariant\");\n-      }\n-      return obj;\n-    }\n-\n-    ContinuationGCSupport::transform_stack_chunk(obj);\n-\n-    \/\/ Check for deduplicating young Strings.\n-    if (G1StringDedup::is_candidate_from_evacuation(klass,\n-                                                    region_attr,\n-                                                    dest_attr,\n-                                                    age)) {\n-      \/\/ Record old; request adds a new weak reference, which reference\n-      \/\/ processing expects to refer to a from-space object.\n-      _string_dedup_requests.add(old);\n+    {\n+      \/\/ Skip the card enqueue iff the object (obj) is in survivor region.\n+      \/\/ However, G1HeapRegion::is_survivor() is too expensive here.\n+      \/\/ Instead, we use dest_attr.is_young() because the two values are always\n+      \/\/ equal: successfully allocated young regions must be survivor regions.\n+      assert(dest_attr.is_young() == _g1h->heap_region_containing(obj)->is_survivor(), \"must be\");\n+      G1SkipCardEnqueueSetter x(&_scanner, dest_attr.is_young());\n+      do_iterate_object(obj, old, klass, region_attr, dest_attr, age);\n@@ -539,7 +556,0 @@\n-    \/\/ Skip the card enqueue iff the object (obj) is in survivor region.\n-    \/\/ However, G1HeapRegion::is_survivor() is too expensive here.\n-    \/\/ Instead, we use dest_attr.is_young() because the two values are always\n-    \/\/ equal: successfully allocated young regions must be survivor regions.\n-    assert(dest_attr.is_young() == _g1h->heap_region_containing(obj)->is_survivor(), \"must be\");\n-    G1SkipCardEnqueueSetter x(&_scanner, dest_attr.is_young());\n-    obj->oop_iterate_backwards(&_scanner, klass);\n@@ -627,1 +637,1 @@\n-oop G1ParScanThreadState::handle_evacuation_failure_par(oop old, markWord m, size_t word_sz, bool cause_pinned) {\n+oop G1ParScanThreadState::handle_evacuation_failure_par(oop old, markWord m, Klass* klass, G1HeapRegionAttr attr, size_t word_sz, bool cause_pinned) {\n@@ -641,2 +651,0 @@\n-    ContinuationGCSupport::transform_stack_chunk(old);\n-\n@@ -645,6 +653,8 @@\n-    \/\/ For iterating objects that failed evacuation currently we can reuse the\n-    \/\/ existing closure to scan evacuated objects; since we are iterating from a\n-    \/\/ collection set region (i.e. never a Survivor region), we always need to\n-    \/\/ gather cards for this case.\n-    G1SkipCardEnqueueSetter x(&_scanner, false \/* skip_card_enqueue *\/);\n-    old->oop_iterate_backwards(&_scanner);\n+    {\n+      \/\/ For iterating objects that failed evacuation currently we can reuse the\n+      \/\/ existing closure to scan evacuated objects; since we are iterating from a\n+      \/\/ collection set region (i.e. never a Survivor region), we always need to\n+      \/\/ gather cards for this case.\n+      G1SkipCardEnqueueSetter x(&_scanner, false \/* skip_card_enqueue *\/);\n+      do_iterate_object(old, old, klass, attr, attr, m.age());\n+    }\n","filename":"src\/hotspot\/share\/gc\/g1\/g1ParScanThreadState.cpp","additions":61,"deletions":51,"binary":false,"changes":112,"status":"modified"},{"patch":"@@ -68,0 +68,12 @@\n+  if (InitialSurvivorRatio < MinSurvivorRatio) {\n+    if (FLAG_IS_CMDLINE(InitialSurvivorRatio)) {\n+      if (FLAG_IS_CMDLINE(MinSurvivorRatio)) {\n+        jio_fprintf(defaultStream::error_stream(),\n+          \"Inconsistent MinSurvivorRatio vs InitialSurvivorRatio: %d vs %d\\n\", MinSurvivorRatio, InitialSurvivorRatio);\n+      }\n+      FLAG_SET_DEFAULT(MinSurvivorRatio, InitialSurvivorRatio);\n+    } else {\n+      FLAG_SET_DEFAULT(InitialSurvivorRatio, MinSurvivorRatio);\n+    }\n+  }\n+\n@@ -101,11 +113,0 @@\n-\n-  \/\/ The survivor ratio's are calculated \"raw\", unlike the\n-  \/\/ default gc, which adds 2 to the ratio value. We need to\n-  \/\/ make sure the values are valid before using them.\n-  if (MinSurvivorRatio < 3) {\n-    FLAG_SET_ERGO(MinSurvivorRatio, 3);\n-  }\n-\n-  if (InitialSurvivorRatio < 3) {\n-    FLAG_SET_ERGO(InitialSurvivorRatio, 3);\n-  }\n","filename":"src\/hotspot\/share\/gc\/parallel\/parallelArguments.cpp","additions":12,"deletions":11,"binary":false,"changes":23,"status":"modified"},{"patch":"@@ -697,1 +697,1 @@\n-void ParallelScavengeHeap::print_on(outputStream* st) const {\n+void ParallelScavengeHeap::print_heap_on(outputStream* st) const {\n@@ -704,1 +704,0 @@\n-  MetaspaceUtils::print_on(st);\n@@ -707,3 +706,5 @@\n-void ParallelScavengeHeap::print_on_error(outputStream* st) const {\n-  this->CollectedHeap::print_on_error(st);\n-\n+void ParallelScavengeHeap::print_gc_on(outputStream* st) const {\n+  BarrierSet* bs = BarrierSet::barrier_set();\n+  if (bs != nullptr) {\n+    bs->print_on(st);\n+  }\n@@ -711,0 +712,1 @@\n+\n@@ -712,1 +714,1 @@\n-    PSParallelCompactNew::print_on_error(st);\n+    PSParallelCompactNew::print_on(st);\n@@ -714,1 +716,1 @@\n-    PSParallelCompact::print_on_error(st);\n+    PSParallelCompact::print_on(st);\n","filename":"src\/hotspot\/share\/gc\/parallel\/parallelScavengeHeap.cpp","additions":9,"deletions":7,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -213,2 +213,2 @@\n-void PSParallelCompact::print_on_error(outputStream* st) {\n-  _mark_bitmap.print_on_error(st);\n+void PSParallelCompact::print_on(outputStream* st) {\n+  _mark_bitmap.print_on(st);\n@@ -249,1 +249,2 @@\n-                                             page_sz);\n+                                             page_sz,\n+                                             mtGC);\n@@ -1636,1 +1637,1 @@\n-  debug_only(verify_forward();)\n+  DEBUG_ONLY(verify_forward();)\n","filename":"src\/hotspot\/share\/gc\/parallel\/psParallelCompact.cpp","additions":5,"deletions":4,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -99,2 +99,2 @@\n-void PSParallelCompactNew::print_on_error(outputStream* st) {\n-  _mark_bitmap.print_on_error(st);\n+void PSParallelCompactNew::print_on(outputStream* st) {\n+  _mark_bitmap.print_on(st);\n","filename":"src\/hotspot\/share\/gc\/parallel\/psParallelCompactNew.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -333,1 +333,1 @@\n-  static void print_on_error(outputStream* st);\n+  static void print_on(outputStream* st);\n","filename":"src\/hotspot\/share\/gc\/parallel\/psParallelCompactNew.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -48,1 +48,1 @@\n-#include \"gc\/shared\/oopStorageSetParState.inline.hpp\"\n+#include \"gc\/shared\/oopStorageSetParState.inline.hpp\"\n@@ -61,0 +61,1 @@\n+#include \"logging\/log.hpp\"\n@@ -64,1 +65,0 @@\n-#include \"logging\/log.hpp\"\n@@ -71,1 +71,1 @@\n-#include \"runtime\/vmThread.hpp\"\n+#include \"runtime\/vmThread.hpp\"\n","filename":"src\/hotspot\/share\/gc\/parallel\/psScavenge.cpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -826,15 +826,9 @@\n-  st->print(\" %-10s\", name());\n-\n-  st->print(\" total %zuK, used %zuK\",\n-            capacity()\/K, used()\/K);\n-  st->print_cr(\" [\" PTR_FORMAT \", \" PTR_FORMAT \", \" PTR_FORMAT \")\",\n-               p2i(_virtual_space.low_boundary()),\n-               p2i(_virtual_space.high()),\n-               p2i(_virtual_space.high_boundary()));\n-\n-  st->print(\"  eden\");\n-  eden()->print_on(st);\n-  st->print(\"  from\");\n-  from()->print_on(st);\n-  st->print(\"  to  \");\n-  to()->print_on(st);\n+  st->print(\"%-10s\", name());\n+\n+  st->print(\" total %zuK, used %zuK \", capacity() \/ K, used() \/ K);\n+  _virtual_space.print_space_boundaries_on(st);\n+\n+  StreamAutoIndentor indentor(st, 1);\n+  eden()->print_on(st, \"eden \");\n+  from()->print_on(st, \"from \");\n+  to()->print_on(st, \"to   \");\n","filename":"src\/hotspot\/share\/gc\/serial\/defNewGeneration.cpp","additions":9,"deletions":15,"binary":false,"changes":24,"status":"modified"},{"patch":"@@ -27,0 +27,2 @@\n+#include \"gc\/shared\/fullGCForwarding.hpp\"\n+#include \"gc\/shared\/gcArguments.hpp\"\n","filename":"src\/hotspot\/share\/gc\/serial\/serialArguments.cpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -25,1 +25,1 @@\n-#include \"classfile\/classLoaderDataGraph.hpp\"\n+#include \"classfile\/classLoaderDataGraph.hpp\"\n@@ -46,0 +46,1 @@\n+#include \"gc\/shared\/gc_globals.hpp\"\n@@ -50,1 +51,0 @@\n-#include \"gc\/shared\/gc_globals.hpp\"\n","filename":"src\/hotspot\/share\/gc\/serial\/serialFullGC.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -442,1 +442,1 @@\n-  st->print(\" %-10s\", name());\n+  st->print(\"%-10s\", name());\n@@ -444,1 +444,1 @@\n-  st->print(\" total %zuK, used %zuK\",\n+  st->print(\" total %zuK, used %zuK \",\n@@ -446,4 +446,1 @@\n-  st->print_cr(\" [\" PTR_FORMAT \", \" PTR_FORMAT \", \" PTR_FORMAT \")\",\n-               p2i(_virtual_space.low_boundary()),\n-               p2i(_virtual_space.high()),\n-               p2i(_virtual_space.high_boundary()));\n+  _virtual_space.print_space_boundaries_on(st);\n@@ -451,2 +448,2 @@\n-  st->print(\"   the\");\n-  _the_space->print_on(st);\n+  StreamAutoIndentor indentor(st, 1);\n+  _the_space->print_on(st, \"the  \");\n","filename":"src\/hotspot\/share\/gc\/serial\/tenuredGeneration.cpp","additions":5,"deletions":8,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -27,1 +27,1 @@\n-#include \"gc\/shared\/tlab_globals.hpp\"\n+#include \"gc\/shared\/tlab_globals.hpp\"\n","filename":"src\/hotspot\/share\/gc\/shared\/c2\/barrierSetC2.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2001, 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2001, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -182,1 +182,1 @@\n-  debug_only(static void check_for_valid_allocation_state();)\n+  DEBUG_ONLY(static void check_for_valid_allocation_state();)\n@@ -438,4 +438,2 @@\n-  \/\/ Print heap information on the given outputStream.\n-  virtual void print_on(outputStream* st) const = 0;\n-  \/\/ The default behavior is to call print_on() on tty.\n-  virtual void print() const;\n+  \/\/ Print heap information.\n+  virtual void print_heap_on(outputStream* st) const = 0;\n@@ -443,7 +441,2 @@\n-  \/\/ Print more detailed heap information on the given\n-  \/\/ outputStream. The default behavior is to call print_on(). It is\n-  \/\/ up to each subclass to override it and add any additional output\n-  \/\/ it needs.\n-  virtual void print_extended_on(outputStream* st) const {\n-    print_on(st);\n-  }\n+  \/\/ Print additional information about the GC that is not included in print_heap_on().\n+  virtual void print_gc_on(outputStream* st) const = 0;\n@@ -451,1 +444,2 @@\n-  virtual void print_on_error(outputStream* st) const;\n+  \/\/ The default behavior is to call print_heap_on() and print_gc_on() on tty.\n+  virtual void print() const;\n","filename":"src\/hotspot\/share\/gc\/shared\/collectedHeap.hpp","additions":8,"deletions":14,"binary":false,"changes":22,"status":"modified"},{"patch":"@@ -38,1 +38,1 @@\n-#include \"runtime\/sharedRuntime.hpp\"\n+#include \"runtime\/sharedRuntime.hpp\"\n@@ -129,0 +129,4 @@\n+#ifdef CHECK_UNHANDLED_OOPS\n+      \/\/ obj is null, no need to handle, but CheckUnhandledOops is not aware about null\n+      THREAD->allow_unhandled_oop(_obj_ptr);\n+#endif \/\/ CHECK_UNHANDLED_OOPS\n@@ -148,1 +152,1 @@\n-  debug_only(check_for_valid_allocation_state());\n+  DEBUG_ONLY(check_for_valid_allocation_state());\n@@ -330,1 +334,1 @@\n-  debug_only(allocation._thread->check_for_valid_safepoint_state());\n+  DEBUG_ONLY(allocation._thread->check_for_valid_safepoint_state());\n","filename":"src\/hotspot\/share\/gc\/shared\/memAllocator.cpp","additions":7,"deletions":3,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -37,2 +37,0 @@\n-#include \"gc\/shenandoah\/shenandoahCollectorPolicy.hpp\"\n-#include \"gc\/shenandoah\/shenandoahConcurrentGC.hpp\"\n@@ -41,0 +39,1 @@\n+#include \"gc\/shenandoah\/shenandoahConcurrentGC.hpp\"\n@@ -45,5 +44,0 @@\n-#include \"gc\/shenandoah\/shenandoahPhaseTimings.hpp\"\n-#include \"gc\/shenandoah\/shenandoahMark.inline.hpp\"\n-#include \"gc\/shenandoah\/shenandoahMonitoringSupport.hpp\"\n-#include \"gc\/shenandoah\/shenandoahHeapRegionClosures.hpp\"\n-#include \"gc\/shenandoah\/shenandoahHeapRegionSet.hpp\"\n@@ -52,0 +46,3 @@\n+#include \"gc\/shenandoah\/shenandoahHeapRegionClosures.hpp\"\n+#include \"gc\/shenandoah\/shenandoahHeapRegionSet.hpp\"\n+#include \"gc\/shenandoah\/shenandoahMark.inline.hpp\"\n@@ -54,0 +51,2 @@\n+#include \"gc\/shenandoah\/shenandoahMonitoringSupport.hpp\"\n+#include \"gc\/shenandoah\/shenandoahPhaseTimings.hpp\"\n@@ -355,2 +354,2 @@\n-    assert(_heap->complete_marking_context()->is_marked(p), \"must be marked\");\n-    assert(!_heap->complete_marking_context()->allocated_after_mark_start(p), \"must be truly marked\");\n+    assert(_heap->gc_generation()->complete_marking_context()->is_marked(p), \"must be marked\");\n+    assert(!_heap->gc_generation()->complete_marking_context()->allocated_after_mark_start(p), \"must be truly marked\");\n@@ -561,1 +560,1 @@\n-    _ctx(ShenandoahHeap::heap()->complete_marking_context()) {}\n+    _ctx(ShenandoahHeap::heap()->global_generation()->complete_marking_context()) {}\n@@ -785,1 +784,1 @@\n-    _ctx(ShenandoahHeap::heap()->complete_marking_context()) {}\n+    _ctx(ShenandoahHeap::heap()->gc_generation()->complete_marking_context()) {}\n@@ -803,1 +802,1 @@\n-    assert(_heap->complete_marking_context()->is_marked(p), \"must be marked\");\n+    assert(_heap->gc_generation()->complete_marking_context()->is_marked(p), \"must be marked\");\n@@ -887,1 +886,1 @@\n-    assert(_heap->complete_marking_context()->is_marked(p), \"must be marked\");\n+    assert(_heap->gc_generation()->complete_marking_context()->is_marked(p), \"must be marked\");\n@@ -961,1 +960,1 @@\n-      _heap->complete_marking_context()->reset_top_at_mark_start(r);\n+      _heap->gc_generation()->complete_marking_context()->reset_top_at_mark_start(r);\n@@ -1102,1 +1101,1 @@\n-    ShenandoahMarkingContext* const ctx = heap->complete_marking_context();\n+    ShenandoahMarkingContext* const ctx = heap->gc_generation()->complete_marking_context();\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahFullGC.cpp","additions":14,"deletions":15,"binary":false,"changes":29,"status":"modified"},{"patch":"@@ -28,0 +28,1 @@\n+#include \"gc\/shenandoah\/shenandoahGeneration.hpp\"\n@@ -30,1 +31,0 @@\n-#include \"gc\/shenandoah\/shenandoahGeneration.hpp\"\n@@ -33,1 +33,0 @@\n-#include \"gc\/shenandoah\/shenandoahYoungGeneration.hpp\"\n@@ -36,0 +35,1 @@\n+#include \"gc\/shenandoah\/shenandoahYoungGeneration.hpp\"\n@@ -278,2 +278,2 @@\n-  assert(_heap->complete_marking_context()->is_marked(p), \"must be marked\");\n-  assert(!_heap->complete_marking_context()->allocated_after_mark_start(p), \"must be truly marked\");\n+  assert(_heap->global_generation()->complete_marking_context()->is_marked(p), \"must be marked\");\n+  assert(!_heap->global_generation()->complete_marking_context()->allocated_after_mark_start(p), \"must be truly marked\");\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahGenerationalFullGC.cpp","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -43,0 +43,1 @@\n+#include \"gc\/shenandoah\/shenandoahUtils.hpp\"\n@@ -45,1 +46,0 @@\n-#include \"gc\/shenandoah\/shenandoahUtils.hpp\"\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahGenerationalHeap.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -30,1 +30,0 @@\n-\n@@ -40,1 +39,0 @@\n-\n@@ -43,0 +41,3 @@\n+#include \"gc\/shenandoah\/mode\/shenandoahGenerationalMode.hpp\"\n+#include \"gc\/shenandoah\/mode\/shenandoahPassiveMode.hpp\"\n+#include \"gc\/shenandoah\/mode\/shenandoahSATBMode.hpp\"\n@@ -45,0 +46,1 @@\n+#include \"gc\/shenandoah\/shenandoahClosures.inline.hpp\"\n@@ -50,1 +52,0 @@\n-#include \"gc\/shenandoah\/shenandoahClosures.inline.hpp\"\n@@ -56,1 +57,1 @@\n-#include \"gc\/shenandoah\/shenandoahHeapRegionClosures.hpp\"\n+#include \"gc\/shenandoah\/shenandoahHeapRegionClosures.hpp\"\n@@ -76,1 +77,1 @@\n-#include \"gc\/shenandoah\/shenandoahWorkGroup.hpp\"\n+#include \"gc\/shenandoah\/shenandoahWorkGroup.hpp\"\n@@ -79,9 +80,0 @@\n-#include \"gc\/shenandoah\/mode\/shenandoahGenerationalMode.hpp\"\n-#include \"gc\/shenandoah\/mode\/shenandoahPassiveMode.hpp\"\n-#include \"gc\/shenandoah\/mode\/shenandoahSATBMode.hpp\"\n-\n-#if INCLUDE_JFR\n-#include \"gc\/shenandoah\/shenandoahJfrSupport.hpp\"\n-#endif\n-\n-#include \"memory\/allocation.hpp\"\n@@ -106,1 +98,1 @@\n-#include \"utilities\/globalDefinitions.hpp\"\n+#include \"utilities\/globalDefinitions.hpp\"\n@@ -109,0 +101,3 @@\n+#if INCLUDE_JFR\n+#include \"gc\/shenandoah\/shenandoahJfrSupport.hpp\"\n+#endif\n@@ -169,1 +164,1 @@\n-  const ReservedSpace reserved = MemoryReserver::reserve(size, alignment, preferred_page_size);\n+  const ReservedSpace reserved = MemoryReserver::reserve(size, alignment, preferred_page_size, mtGC);\n@@ -383,1 +378,1 @@\n-      cset_rs = MemoryReserver::reserve(req_addr, cset_size, cset_align, cset_page_size);\n+      cset_rs = MemoryReserver::reserve(req_addr, cset_size, cset_align, cset_page_size, mtGC);\n@@ -392,1 +387,1 @@\n-      cset_rs = MemoryReserver::reserve(cset_size, cset_align, os::vm_page_size());\n+      cset_rs = MemoryReserver::reserve(cset_size, cset_align, os::vm_page_size(), mtGC);\n@@ -427,2 +422,0 @@\n-    \/\/ Initialize to complete\n-    _marking_context->mark_complete();\n@@ -593,1 +586,1 @@\n-void ShenandoahHeap::print_on(outputStream* st) const {\n+void ShenandoahHeap::print_heap_on(outputStream* st) const {\n@@ -644,1 +637,0 @@\n-  MetaspaceUtils::print_on(st);\n@@ -652,0 +644,4 @@\n+void ShenandoahHeap::print_gc_on(outputStream* st) const {\n+  print_heap_regions_on(st);\n+}\n+\n@@ -2592,6 +2588,0 @@\n-void ShenandoahHeap::print_extended_on(outputStream *st) const {\n-  print_on(st);\n-  st->cr();\n-  print_heap_regions_on(st);\n-}\n-\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.cpp","additions":18,"deletions":28,"binary":false,"changes":46,"status":"modified"},{"patch":"@@ -43,0 +43,1 @@\n+#include \"gc\/z\/zStringDedup.inline.hpp\"\n@@ -416,1 +417,1 @@\n-    ZHeap::heap()->free_page(page, true \/* allow_defragment *\/);\n+    ZHeap::heap()->free_page(page);\n@@ -565,6 +566,8 @@\n-  Allocator* const   _allocator;\n-  ZForwarding*       _forwarding;\n-  ZPage*             _target[ZAllocator::_relocation_allocators];\n-  ZGeneration* const _generation;\n-  size_t             _other_promoted;\n-  size_t             _other_compacted;\n+  Allocator* const    _allocator;\n+  ZForwarding*        _forwarding;\n+  ZPage*              _target[ZAllocator::_relocation_allocators];\n+  ZGeneration* const  _generation;\n+  size_t              _other_promoted;\n+  size_t              _other_compacted;\n+  ZStringDedupContext _string_dedup_context;\n+\n@@ -808,0 +811,7 @@\n+  void maybe_string_dedup(zaddress to_addr) {\n+    if (_forwarding->is_promotion()) {\n+      \/\/ Only deduplicate promoted objects, and let short-lived strings simply die instead.\n+      _string_dedup_context.request(to_oop(to_addr));\n+    }\n+  }\n+\n@@ -818,0 +828,2 @@\n+    maybe_string_dedup(to_addr);\n+\n@@ -855,1 +867,3 @@\n-    ZPage* const to_page = promotion ? from_page->clone_limited() : from_page;\n+    ZPage* const to_page = promotion\n+        ? from_page->clone_for_promotion()\n+        : from_page->reset(to_age);\n@@ -858,4 +872,0 @@\n-    to_page->reset(to_age);\n-    if (promotion) {\n-      to_page->remset_alloc();\n-    }\n@@ -1025,1 +1035,1 @@\n-      ZHeap::heap()->free_page(page, true \/* allow_defragment *\/);\n+      ZHeap::heap()->free_page(page);\n@@ -1192,0 +1202,1 @@\n+    ZStringDedupContext        string_dedup_context;\n@@ -1195,0 +1206,1 @@\n+        \/\/ Remap oops and add remset if needed\n@@ -1196,0 +1208,3 @@\n+\n+        \/\/ String dedup\n+        string_dedup_context.request(obj);\n@@ -1274,1 +1289,3 @@\n-      ZPage* const new_page = promotion ? prev_page->clone_limited() : prev_page;\n+      ZPage* const new_page = promotion\n+          ? prev_page->clone_for_promotion()\n+          : prev_page->reset(to_age);\n@@ -1277,4 +1294,0 @@\n-      new_page->reset(to_age);\n-      if (promotion) {\n-        new_page->remset_alloc();\n-      }\n","filename":"src\/hotspot\/share\/gc\/z\/zRelocate.cpp","additions":31,"deletions":18,"binary":false,"changes":49,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2015, 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2015, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -53,0 +53,10 @@\n+  template <typename T>\n+  static void copy_disjoint(T* dest, const T* src, size_t count);\n+  template <typename T>\n+  static void copy_disjoint(T* dest, const T* src, int count);\n+\n+  \/\/ Sort\n+  template <typename T, typename Comparator>\n+  static void sort(T* array, size_t count, Comparator comparator);\n+  template <typename T, typename Comparator>\n+  static void sort(T* array, int count, Comparator comparator);\n","filename":"src\/hotspot\/share\/gc\/z\/zUtils.hpp","additions":11,"deletions":1,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2015, 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2015, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -81,0 +81,30 @@\n+template <typename T>\n+inline void ZUtils::copy_disjoint(T* dest, const T* src, size_t count) {\n+  memcpy(dest, src, sizeof(T) * count);\n+}\n+\n+template <typename T>\n+inline void ZUtils::copy_disjoint(T* dest, const T* src, int count) {\n+  assert(count >= 0, \"must be positive %d\", count);\n+\n+  copy_disjoint(dest, src, static_cast<size_t>(count));\n+}\n+\n+template <typename T, typename Comparator>\n+inline void ZUtils::sort(T* array, size_t count, Comparator comparator) {\n+  using SortType = int(const void*, const void*);\n+  using ComparatorType = int(const T*, const T*);\n+\n+  ComparatorType* const comparator_fn_ptr = comparator;\n+\n+  \/\/ We rely on ABI compatibility between ComparatorType and SortType\n+  qsort(array, count, sizeof(T), reinterpret_cast<SortType*>(comparator_fn_ptr));\n+}\n+\n+template <typename T, typename Comparator>\n+inline void ZUtils::sort(T* array, int count, Comparator comparator) {\n+  assert(count >= 0, \"must be positive %d\", count);\n+\n+  sort(array, static_cast<size_t>(count), comparator);\n+}\n+\n","filename":"src\/hotspot\/share\/gc\/z\/zUtils.inline.hpp","additions":31,"deletions":1,"binary":false,"changes":32,"status":"modified"},{"patch":"@@ -176,1 +176,1 @@\n-  debug_only(VMNativeEntryWrapper __vew;)\n+  DEBUG_ONLY(VMNativeEntryWrapper __vew;)\n@@ -2256,0 +2256,10 @@\n+  if (obj->is_array()) {\n+    \/\/ Disallow reading after the last element of an array\n+    size_t array_length = arrayOop(obj())->length();\n+    int lh = obj->klass()->layout_helper();\n+    size_t size_in_bytes = array_length << Klass::layout_helper_log2_element_size(lh);\n+    size_in_bytes += Klass::layout_helper_header_size(lh);\n+    if ((size_t) displacement + basic_type_elemsize > size_in_bytes) {\n+      JVMCI_THROW_MSG_NULL(IllegalArgumentException, \"reading after last array element\");\n+    }\n+  }\n@@ -2261,3 +2271,0 @@\n-      if (displacement + heapOopSize > arrayOopDesc::base_offset_in_bytes(T_OBJECT) + arrayOop(obj())->length() * heapOopSize) {\n-        JVMCI_THROW_MSG_NULL(IllegalArgumentException, \"reading after last array element\");\n-      }\n","filename":"src\/hotspot\/share\/jvmci\/jvmciCompilerToVM.cpp","additions":11,"deletions":4,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -168,2 +168,2 @@\n-debug_only(OopHandle Universe::_fullgc_alot_dummy_array;)\n-debug_only(int Universe::_fullgc_alot_dummy_next = 0;)\n+DEBUG_ONLY(OopHandle Universe::_fullgc_alot_dummy_array;)\n+DEBUG_ONLY(int Universe::_fullgc_alot_dummy_next = 0;)\n@@ -900,1 +900,0 @@\n-  DynamicArchive::check_for_dynamic_dump();\n@@ -907,1 +906,1 @@\n-    MetaspaceShared::prepare_for_dumping();\n+    CDSConfig::prepare_for_dumping();\n@@ -1163,1 +1162,4 @@\n-  heap()->print_on(st);\n+\n+  StreamAutoIndentor indentor(st, 1);\n+  heap()->print_heap_on(st);\n+  MetaspaceUtils::print_on(st);\n","filename":"src\/hotspot\/share\/memory\/universe.cpp","additions":7,"deletions":5,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -100,1 +100,0 @@\n-#include \"utilities\/pair.hpp\"\n@@ -1326,1 +1325,1 @@\n-    debug_only(vtable().verify(tty, true);)\n+    DEBUG_ONLY(vtable().verify(tty, true);)\n@@ -1789,1 +1788,1 @@\n-      fd->reinitialize(const_cast<InstanceKlass*>(this), fs.index());\n+      fd->reinitialize(const_cast<InstanceKlass*>(this), fs.to_FieldInfo());\n@@ -1858,1 +1857,1 @@\n-      fd->reinitialize(const_cast<InstanceKlass*>(this), fs.index());\n+      fd->reinitialize(const_cast<InstanceKlass*>(this), fs.to_FieldInfo());\n@@ -1919,4 +1918,2 @@\n-  fieldDescriptor fd;\n-  int length = java_fields_count();\n-  for (int i = 0; i < length; i += 1) {\n-    fd.reinitialize(this, i);\n+  for (JavaFieldStream fs(this); !fs.done(); fs.next()) {\n+    fieldDescriptor& fd = fs.field_descriptor();\n@@ -1929,3 +1926,2 @@\n-\/\/ first in Pair is offset, second is index.\n-static int compare_fields_by_offset(Pair<int,int>* a, Pair<int,int>* b) {\n-  return a->first - b->first;\n+static int compare_fields_by_offset(FieldInfo* a, FieldInfo* b) {\n+  return a->offset() - b->offset();\n@@ -1940,3 +1936,1 @@\n-  fieldDescriptor fd;\n-  GrowableArray<Pair<int,int> > fields_sorted;\n-  int i = 0;\n+  GrowableArray<FieldInfo> fields_sorted;\n@@ -1946,4 +1940,1 @@\n-      fd = fs.field_descriptor();\n-      Pair<int,int> f(fs.offset(), fs.index());\n-      fields_sorted.push(f);\n-      i++;\n+      fields_sorted.push(fs.to_FieldInfo());\n@@ -1952,3 +1943,2 @@\n-  if (i > 0) {\n-    int length = i;\n-    assert(length == fields_sorted.length(), \"duh\");\n+  int length = fields_sorted.length();\n+  if (length > 0) {\n@@ -1956,0 +1946,1 @@\n+    fieldDescriptor fd;\n@@ -1957,2 +1948,2 @@\n-      fd.reinitialize(this, fields_sorted.at(i).second);\n-      assert(!fd.is_static() && fd.offset() == fields_sorted.at(i).first, \"only nonstatic fields\");\n+      fd.reinitialize(this, fields_sorted.at(i));\n+      assert(!fd.is_static() && fd.offset() == checked_cast<int>(fields_sorted.at(i).offset()), \"only nonstatic fields\");\n@@ -3767,1 +3758,1 @@\n-  st->print(BULLET\"non-static oop maps: \");\n+  st->print(BULLET\"non-static oop maps (%d entries): \", nonstatic_oop_map_count());\n@@ -4204,1 +4195,1 @@\n-  debug_only(_is_static_field_id = false;)\n+  DEBUG_ONLY(_is_static_field_id = false;)\n","filename":"src\/hotspot\/share\/oops\/instanceKlass.cpp","additions":16,"deletions":25,"binary":false,"changes":41,"status":"modified"},{"patch":"@@ -682,1 +682,1 @@\n-  debug_only(verify();)\n+  DEBUG_ONLY(verify();)\n@@ -707,1 +707,1 @@\n-  debug_only(verify();)\n+  DEBUG_ONLY(verify();)\n@@ -832,3 +832,15 @@\n-    oop scratch_mirror = HeapShared::scratch_java_mirror(orig_mirror);\n-    if (scratch_mirror != nullptr) {\n-      _archived_mirror_index = HeapShared::append_root(scratch_mirror);\n+    if (orig_mirror == nullptr) {\n+      assert(CDSConfig::is_dumping_final_static_archive(), \"sanity\");\n+      if (is_instance_klass()) {\n+        assert(InstanceKlass::cast(this)->is_shared_unregistered_class(), \"sanity\");\n+      } else {\n+        precond(is_objArray_klass());\n+        Klass *k = ObjArrayKlass::cast(this)->bottom_klass();\n+        precond(k->is_instance_klass());\n+        assert(InstanceKlass::cast(k)->is_shared_unregistered_class(), \"sanity\");\n+      }\n+    } else {\n+      oop scratch_mirror = HeapShared::scratch_java_mirror(orig_mirror);\n+      if (scratch_mirror != nullptr) {\n+        _archived_mirror_index = HeapShared::append_root(scratch_mirror);\n+      }\n","filename":"src\/hotspot\/share\/oops\/klass.cpp","additions":17,"deletions":5,"binary":false,"changes":22,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -47,5 +47,0 @@\n-\/\/ Forward declarations.\n-class OopClosure;\n-class PSPromotionManager;\n-class ParCompactionManager;\n-\n","filename":"src\/hotspot\/share\/oops\/oop.hpp","additions":1,"deletions":6,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -477,1 +477,1 @@\n-  debug_only(verify_graph_edges(true \/*check for no_dead_code*\/, root_and_safepoints);)\n+  DEBUG_ONLY(verify_graph_edges(true \/*check for no_dead_code*\/, root_and_safepoints);)\n@@ -1017,2 +1017,0 @@\n-  IA32_ONLY( set_24_bit_selection_and_mode(true, false); )\n-\n@@ -1860,0 +1858,3 @@\n+  if (has_loops() || _loop_opts_cnt > 0) {\n+    print_method(PHASE_AFTER_LOOP_OPTS, 2);\n+  }\n@@ -2088,0 +2089,1 @@\n+    bool is_scheduled_for_igvn_before = C->igvn_worklist()->member(cg->call_node());\n@@ -2098,0 +2100,10 @@\n+      } else {\n+        bool is_scheduled_for_igvn_after = C->igvn_worklist()->member(cg->call_node());\n+        if (!is_scheduled_for_igvn_before && is_scheduled_for_igvn_after) {\n+          \/\/ Avoid potential infinite loop if node already in the IGVN list\n+          assert(false, \"scheduled for IGVN during inlining attempt\");\n+        } else {\n+          \/\/ Ensure call node has not disappeared from IGVN worklist during a failed inlining attempt\n+          assert(!is_scheduled_for_igvn_before || is_scheduled_for_igvn_after, \"call node removed from IGVN list during inlining pass\");\n+          cg->call_node()->set_generator(cg);\n+        }\n@@ -2369,0 +2381,4 @@\n+  if (has_loops()) {\n+    print_method(PHASE_BEFORE_LOOP_OPTS, 2);\n+  }\n+\n@@ -4085,11 +4101,0 @@\n-#ifdef IA32\n-  \/\/ If original bytecodes contained a mixture of floats and doubles\n-  \/\/ check if the optimizer has made it homogeneous, item (3).\n-  if (UseSSE == 0 &&\n-      frc.get_float_count() > 32 &&\n-      frc.get_double_count() == 0 &&\n-      (10 * frc.get_call_count() < frc.get_float_count()) ) {\n-    set_24_bit_selection_and_mode(false, true);\n-  }\n-#endif \/\/ IA32\n-\n@@ -5263,1 +5268,1 @@\n-  igv_print_graph_to_network(phase_name, (Node*) C->root(), empty_list);\n+  igv_print_graph_to_network(phase_name, empty_list);\n@@ -5266,1 +5271,1 @@\n-void Compile::igv_print_graph_to_network(const char* name, Node* node, GrowableArray<const Node*>& visible_nodes) {\n+void Compile::igv_print_graph_to_network(const char* name, GrowableArray<const Node*>& visible_nodes) {\n","filename":"src\/hotspot\/share\/opto\/compile.cpp","additions":21,"deletions":16,"binary":false,"changes":37,"status":"modified"},{"patch":"@@ -2195,0 +2195,7 @@\n+                  strcmp(call->as_CallLeaf()->_name, \"kyberNtt\") == 0 ||\n+                  strcmp(call->as_CallLeaf()->_name, \"kyberInverseNtt\") == 0 ||\n+                  strcmp(call->as_CallLeaf()->_name, \"kyberNttMult\") == 0 ||\n+                  strcmp(call->as_CallLeaf()->_name, \"kyberAddPoly_2\") == 0 ||\n+                  strcmp(call->as_CallLeaf()->_name, \"kyberAddPoly_3\") == 0 ||\n+                  strcmp(call->as_CallLeaf()->_name, \"kyber12To16\") == 0 ||\n+                  strcmp(call->as_CallLeaf()->_name, \"kyberBarrettReduce\") == 0 ||\n@@ -2822,1 +2829,1 @@\n-          \/\/ StoreP::memory_type() == T_ADDRESS\n+          \/\/ StoreP::value_basic_type() == T_ADDRESS\n@@ -2829,1 +2836,1 @@\n-              store->as_Store()->memory_type() == ft) {\n+              store->as_Store()->value_basic_type() == ft) {\n@@ -4563,1 +4570,1 @@\n-      debug_only(n->dump();)\n+      DEBUG_ONLY(n->dump();)\n","filename":"src\/hotspot\/share\/opto\/escape.cpp","additions":10,"deletions":3,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -75,2 +75,2 @@\n-  debug_only(_sp = -99);\n-  debug_only(set_bci(-99));\n+  DEBUG_ONLY(_sp = -99);\n+  DEBUG_ONLY(set_bci(-99));\n@@ -199,1 +199,1 @@\n-  debug_only(verify_exception_state(ex_map));\n+  DEBUG_ONLY(verify_exception_state(ex_map));\n@@ -299,1 +299,1 @@\n-      debug_only(verify_map());\n+      DEBUG_ONLY(verify_map());\n@@ -530,26 +530,2 @@\n-  bool must_throw = true;\n-\n-  \/\/ If this particular condition has not yet happened at this\n-  \/\/ bytecode, then use the uncommon trap mechanism, and allow for\n-  \/\/ a future recompilation if several traps occur here.\n-  \/\/ If the throw is hot, try to use a more complicated inline mechanism\n-  \/\/ which keeps execution inside the compiled code.\n-  bool treat_throw_as_hot = false;\n-  ciMethodData* md = method()->method_data();\n-\n-  if (ProfileTraps) {\n-    if (too_many_traps(reason)) {\n-      treat_throw_as_hot = true;\n-    }\n-    \/\/ (If there is no MDO at all, assume it is early in\n-    \/\/ execution, and that any deopts are part of the\n-    \/\/ startup transient, and don't need to be remembered.)\n-\n-    \/\/ Also, if there is a local exception handler, treat all throws\n-    \/\/ as hot if there has been at least one in this method.\n-    if (C->trap_count(reason) != 0\n-        && method()->method_data()->trap_count(reason) != 0\n-        && has_exception_handler()) {\n-        treat_throw_as_hot = true;\n-    }\n-  }\n+  builtin_throw(reason, builtin_throw_exception(reason), \/*allow_too_many_traps*\/ true);\n+}\n@@ -557,0 +533,3 @@\n+void GraphKit::builtin_throw(Deoptimization::DeoptReason reason,\n+                             ciInstance* ex_obj,\n+                             bool allow_too_many_traps) {\n@@ -563,29 +542,8 @@\n-  if (treat_throw_as_hot && method()->can_omit_stack_trace()) {\n-    \/\/ If the throw is local, we use a pre-existing instance and\n-    \/\/ punt on the backtrace.  This would lead to a missing backtrace\n-    \/\/ (a repeat of 4292742) if the backtrace object is ever asked\n-    \/\/ for its backtrace.\n-    \/\/ Fixing this remaining case of 4292742 requires some flavor of\n-    \/\/ escape analysis.  Leave that for the future.\n-    ciInstance* ex_obj = nullptr;\n-    switch (reason) {\n-    case Deoptimization::Reason_null_check:\n-      ex_obj = env()->NullPointerException_instance();\n-      break;\n-    case Deoptimization::Reason_div0_check:\n-      ex_obj = env()->ArithmeticException_instance();\n-      break;\n-    case Deoptimization::Reason_range_check:\n-      ex_obj = env()->ArrayIndexOutOfBoundsException_instance();\n-      break;\n-    case Deoptimization::Reason_class_check:\n-      ex_obj = env()->ClassCastException_instance();\n-      break;\n-    case Deoptimization::Reason_array_check:\n-      ex_obj = env()->ArrayStoreException_instance();\n-      break;\n-    default:\n-      break;\n-    }\n-    \/\/ If we have a preconstructed exception object, use it.\n-    if (ex_obj != nullptr) {\n+  if (is_builtin_throw_hot(reason)) {\n+    if (method()->can_omit_stack_trace() && ex_obj != nullptr) {\n+      \/\/ If the throw is local, we use a pre-existing instance and\n+      \/\/ punt on the backtrace.  This would lead to a missing backtrace\n+      \/\/ (a repeat of 4292742) if the backtrace object is ever asked\n+      \/\/ for its backtrace.\n+      \/\/ Fixing this remaining case of 4292742 requires some flavor of\n+      \/\/ escape analysis.  Leave that for the future.\n@@ -594,1 +552,1 @@\n-        uncommon_trap_if_should_post_on_exceptions(reason, must_throw);\n+        uncommon_trap_if_should_post_on_exceptions(reason, true \/*must_throw*\/);\n@@ -625,0 +583,12 @@\n+    } else if (builtin_throw_too_many_traps(reason, ex_obj)) {\n+      \/\/ We cannot afford to take too many traps here. Suffer in the interpreter instead.\n+      assert(allow_too_many_traps, \"not allowed\");\n+      if (C->log() != nullptr) {\n+        C->log()->elem(\"hot_throw preallocated='0' reason='%s' mcount='%d'\",\n+                       Deoptimization::trap_reason_name(reason),\n+                       C->trap_count(reason));\n+      }\n+      uncommon_trap(reason, Deoptimization::Action_none,\n+                    (ciKlass*) nullptr, (char*) nullptr,\n+                    true \/*must_throw*\/);\n+      return;\n@@ -636,13 +606,0 @@\n-  ciMethod* m = Deoptimization::reason_is_speculate(reason) ? C->method() : nullptr;\n-  Deoptimization::DeoptAction action = Deoptimization::Action_maybe_recompile;\n-  if (treat_throw_as_hot\n-      && (method()->method_data()->trap_recompiled_at(bci(), m)\n-          || C->too_many_traps(reason))) {\n-    \/\/ We cannot afford to take more traps here.  Suffer in the interpreter.\n-    if (C->log() != nullptr)\n-      C->log()->elem(\"hot_throw preallocated='0' reason='%s' mcount='%d'\",\n-                     Deoptimization::trap_reason_name(reason),\n-                     C->trap_count(reason));\n-    action = Deoptimization::Action_none;\n-  }\n-\n@@ -653,0 +610,18 @@\n+  uncommon_trap(reason, Deoptimization::Action_maybe_recompile,\n+                (ciKlass*) nullptr, (char*) nullptr,\n+                true \/*must_throw*\/);\n+}\n+\n+bool GraphKit::is_builtin_throw_hot(Deoptimization::DeoptReason reason) {\n+  \/\/ If this particular condition has not yet happened at this\n+  \/\/ bytecode, then use the uncommon trap mechanism, and allow for\n+  \/\/ a future recompilation if several traps occur here.\n+  \/\/ If the throw is hot, try to use a more complicated inline mechanism\n+  \/\/ which keeps execution inside the compiled code.\n+  if (ProfileTraps) {\n+    if (too_many_traps(reason)) {\n+      return true;\n+    }\n+    \/\/ (If there is no MDO at all, assume it is early in\n+    \/\/ execution, and that any deopts are part of the\n+    \/\/ startup transient, and don't need to be remembered.)\n@@ -654,1 +629,9 @@\n-  uncommon_trap(reason, action, (ciKlass*)nullptr, (char*)nullptr, must_throw);\n+    \/\/ Also, if there is a local exception handler, treat all throws\n+    \/\/ as hot if there has been at least one in this method.\n+    if (C->trap_count(reason) != 0 &&\n+        method()->method_data()->trap_count(reason) != 0 &&\n+        has_exception_handler()) {\n+      return true;\n+    }\n+  }\n+  return false;\n@@ -657,0 +640,32 @@\n+bool GraphKit::builtin_throw_too_many_traps(Deoptimization::DeoptReason reason,\n+                                            ciInstance* ex_obj) {\n+  if (is_builtin_throw_hot(reason)) {\n+    if (method()->can_omit_stack_trace() && ex_obj != nullptr) {\n+      return false; \/\/ no traps; throws preallocated exception instead\n+    }\n+    ciMethod* m = Deoptimization::reason_is_speculate(reason) ? C->method() : nullptr;\n+    if (method()->method_data()->trap_recompiled_at(bci(), m) ||\n+        C->too_many_traps(reason)) {\n+      return true;\n+    }\n+  }\n+  return false;\n+}\n+\n+ciInstance* GraphKit::builtin_throw_exception(Deoptimization::DeoptReason reason) const {\n+  \/\/ Preallocated exception objects to use when we don't need the backtrace.\n+  switch (reason) {\n+  case Deoptimization::Reason_null_check:\n+    return env()->NullPointerException_instance();\n+  case Deoptimization::Reason_div0_check:\n+    return env()->ArithmeticException_instance();\n+  case Deoptimization::Reason_range_check:\n+    return env()->ArrayIndexOutOfBoundsException_instance();\n+  case Deoptimization::Reason_class_check:\n+    return env()->ClassCastException_instance();\n+  case Deoptimization::Reason_array_check:\n+    return env()->ArrayStoreException_instance();\n+  default:\n+    return nullptr;\n+  }\n+}\n@@ -660,1 +675,1 @@\n-  debug_only(kit->verify_map());\n+  DEBUG_ONLY(kit->verify_map());\n@@ -768,1 +783,1 @@\n-  debug_only(verify_map());\n+  DEBUG_ONLY(verify_map());\n@@ -1525,1 +1540,1 @@\n-  debug_only( map()->set_memory((Node*)nullptr) );\n+  DEBUG_ONLY( map()->set_memory((Node*)nullptr) );\n@@ -1562,1 +1577,1 @@\n-  debug_only(adr_type = C->get_adr_type(adr_idx));\n+  DEBUG_ONLY(adr_type = C->get_adr_type(adr_idx));\n@@ -1590,1 +1605,1 @@\n-  debug_only(adr_type = C->get_adr_type(adr_idx));\n+  DEBUG_ONLY(adr_type = C->get_adr_type(adr_idx));\n","filename":"src\/hotspot\/share\/opto\/graphKit.cpp","additions":94,"deletions":79,"binary":false,"changes":173,"status":"modified"},{"patch":"@@ -629,0 +629,14 @@\n+  case vmIntrinsics::_kyberNtt:\n+    return inline_kyberNtt();\n+  case vmIntrinsics::_kyberInverseNtt:\n+    return inline_kyberInverseNtt();\n+  case vmIntrinsics::_kyberNttMult:\n+    return inline_kyberNttMult();\n+  case vmIntrinsics::_kyberAddPoly_2:\n+    return inline_kyberAddPoly_2();\n+  case vmIntrinsics::_kyberAddPoly_3:\n+    return inline_kyberAddPoly_3();\n+  case vmIntrinsics::_kyber12To16:\n+    return inline_kyber12To16();\n+  case vmIntrinsics::_kyberBarrettReduce:\n+    return inline_kyberBarrettReduce();\n@@ -710,0 +724,4 @@\n+  case vmIntrinsics::_VectorUnaryLibOp:\n+    return inline_vector_call(1);\n+  case vmIntrinsics::_VectorBinaryLibOp:\n+    return inline_vector_call(2);\n@@ -2006,1 +2024,8 @@\n-void LibraryCallKit::inline_math_mathExact(Node* math, Node *test) {\n+bool LibraryCallKit::inline_math_mathExact(Node* math, Node* test) {\n+  if (builtin_throw_too_many_traps(Deoptimization::Reason_intrinsic,\n+                                   env()->ArithmeticException_instance())) {\n+    \/\/ It has been already too many times, but we cannot use builtin_throw (e.g. we care about backtraces),\n+    \/\/ so let's bail out intrinsic rather than risking deopting again.\n+    return false;\n+  }\n+\n@@ -2020,2 +2045,3 @@\n-    uncommon_trap(Deoptimization::Reason_intrinsic,\n-                  Deoptimization::Action_none);\n+    builtin_throw(Deoptimization::Reason_intrinsic,\n+                  env()->ArithmeticException_instance(),\n+                  \/*allow_too_many_traps*\/ false);\n@@ -2026,0 +2052,1 @@\n+  return true;\n@@ -2035,2 +2062,1 @@\n-  inline_math_mathExact(operation, ofcheck);\n-  return true;\n+  return inline_math_mathExact(operation, ofcheck);\n@@ -4300,6 +4326,1 @@\n-    Node* cast = _gvn.transform(new CastPPNode(is_array_ctrl, *obj, TypeAryPtr::BOTTOM));\n-    \/\/ Check for top because in rare cases, the type system can determine that\n-    \/\/ the object can't be an array but the layout helper check is not folded.\n-    if (!cast->is_top()) {\n-      *obj = cast;\n-    }\n+    *obj = _gvn.transform(new CastPPNode(is_array_ctrl, *obj, TypeAryPtr::BOTTOM));\n@@ -7786,0 +7807,239 @@\n+\/\/------------------------------inline_kyberNtt\n+bool LibraryCallKit::inline_kyberNtt() {\n+  address stubAddr;\n+  const char *stubName;\n+  assert(UseKyberIntrinsics, \"need Kyber intrinsics support\");\n+  assert(callee()->signature()->size() == 2, \"kyberNtt has 2 parameters\");\n+\n+  stubAddr = StubRoutines::kyberNtt();\n+  stubName = \"kyberNtt\";\n+  if (!stubAddr) return false;\n+\n+  Node* coeffs          = argument(0);\n+  Node* ntt_zetas        = argument(1);\n+\n+  coeffs = must_be_not_null(coeffs, true);\n+  ntt_zetas = must_be_not_null(ntt_zetas, true);\n+\n+  Node* coeffs_start  = array_element_address(coeffs, intcon(0), T_SHORT);\n+  assert(coeffs_start, \"coeffs is null\");\n+  Node* ntt_zetas_start  = array_element_address(ntt_zetas, intcon(0), T_SHORT);\n+  assert(ntt_zetas_start, \"ntt_zetas is null\");\n+  Node* kyberNtt = make_runtime_call(RC_LEAF|RC_NO_FP,\n+                                  OptoRuntime::kyberNtt_Type(),\n+                                  stubAddr, stubName, TypePtr::BOTTOM,\n+                                  coeffs_start, ntt_zetas_start);\n+  \/\/ return an int\n+  Node* retvalue = _gvn.transform(new ProjNode(kyberNtt, TypeFunc::Parms));\n+  set_result(retvalue);\n+  return true;\n+}\n+\n+\/\/------------------------------inline_kyberInverseNtt\n+bool LibraryCallKit::inline_kyberInverseNtt() {\n+  address stubAddr;\n+  const char *stubName;\n+  assert(UseKyberIntrinsics, \"need Kyber intrinsics support\");\n+  assert(callee()->signature()->size() == 2, \"kyberInverseNtt has 2 parameters\");\n+\n+  stubAddr = StubRoutines::kyberInverseNtt();\n+  stubName = \"kyberInverseNtt\";\n+  if (!stubAddr) return false;\n+\n+  Node* coeffs          = argument(0);\n+  Node* zetas           = argument(1);\n+\n+  coeffs = must_be_not_null(coeffs, true);\n+  zetas = must_be_not_null(zetas, true);\n+\n+  Node* coeffs_start  = array_element_address(coeffs, intcon(0), T_SHORT);\n+  assert(coeffs_start, \"coeffs is null\");\n+  Node* zetas_start  = array_element_address(zetas, intcon(0), T_SHORT);\n+  assert(zetas_start, \"inverseNtt_zetas is null\");\n+  Node* kyberInverseNtt = make_runtime_call(RC_LEAF|RC_NO_FP,\n+                                  OptoRuntime::kyberInverseNtt_Type(),\n+                                  stubAddr, stubName, TypePtr::BOTTOM,\n+                                  coeffs_start, zetas_start);\n+\n+  \/\/ return an int\n+  Node* retvalue = _gvn.transform(new ProjNode(kyberInverseNtt, TypeFunc::Parms));\n+  set_result(retvalue);\n+  return true;\n+}\n+\n+\/\/------------------------------inline_kyberNttMult\n+bool LibraryCallKit::inline_kyberNttMult() {\n+  address stubAddr;\n+  const char *stubName;\n+  assert(UseKyberIntrinsics, \"need Kyber intrinsics support\");\n+  assert(callee()->signature()->size() == 4, \"kyberNttMult has 4 parameters\");\n+\n+  stubAddr = StubRoutines::kyberNttMult();\n+  stubName = \"kyberNttMult\";\n+  if (!stubAddr) return false;\n+\n+  Node* result          = argument(0);\n+  Node* ntta            = argument(1);\n+  Node* nttb            = argument(2);\n+  Node* zetas           = argument(3);\n+\n+  result = must_be_not_null(result, true);\n+  ntta = must_be_not_null(ntta, true);\n+  nttb = must_be_not_null(nttb, true);\n+  zetas = must_be_not_null(zetas, true);\n+  Node* result_start  = array_element_address(result, intcon(0), T_SHORT);\n+  assert(result_start, \"result is null\");\n+  Node* ntta_start  = array_element_address(ntta, intcon(0), T_SHORT);\n+  assert(ntta_start, \"ntta is null\");\n+  Node* nttb_start  = array_element_address(nttb, intcon(0), T_SHORT);\n+  assert(nttb_start, \"nttb is null\");\n+  Node* zetas_start  = array_element_address(zetas, intcon(0), T_SHORT);\n+  assert(zetas_start, \"nttMult_zetas is null\");\n+  Node* kyberNttMult = make_runtime_call(RC_LEAF|RC_NO_FP,\n+                                  OptoRuntime::kyberNttMult_Type(),\n+                                  stubAddr, stubName, TypePtr::BOTTOM,\n+                                  result_start, ntta_start, nttb_start,\n+                                  zetas_start);\n+\n+  \/\/ return an int\n+  Node* retvalue = _gvn.transform(new ProjNode(kyberNttMult, TypeFunc::Parms));\n+  set_result(retvalue);\n+\n+  return true;\n+}\n+\n+\/\/------------------------------inline_kyberAddPoly_2\n+bool LibraryCallKit::inline_kyberAddPoly_2() {\n+  address stubAddr;\n+  const char *stubName;\n+  assert(UseKyberIntrinsics, \"need Kyber intrinsics support\");\n+  assert(callee()->signature()->size() == 3, \"kyberAddPoly_2 has 3 parameters\");\n+\n+  stubAddr = StubRoutines::kyberAddPoly_2();\n+  stubName = \"kyberAddPoly_2\";\n+  if (!stubAddr) return false;\n+\n+  Node* result          = argument(0);\n+  Node* a               = argument(1);\n+  Node* b               = argument(2);\n+\n+  result = must_be_not_null(result, true);\n+  a = must_be_not_null(a, true);\n+  b = must_be_not_null(b, true);\n+\n+  Node* result_start  = array_element_address(result, intcon(0), T_SHORT);\n+  assert(result_start, \"result is null\");\n+  Node* a_start  = array_element_address(a, intcon(0), T_SHORT);\n+  assert(a_start, \"a is null\");\n+  Node* b_start  = array_element_address(b, intcon(0), T_SHORT);\n+  assert(b_start, \"b is null\");\n+  Node* kyberAddPoly_2 = make_runtime_call(RC_LEAF|RC_NO_FP,\n+                                  OptoRuntime::kyberAddPoly_2_Type(),\n+                                  stubAddr, stubName, TypePtr::BOTTOM,\n+                                  result_start, a_start, b_start);\n+  \/\/ return an int\n+  Node* retvalue = _gvn.transform(new ProjNode(kyberAddPoly_2, TypeFunc::Parms));\n+  set_result(retvalue);\n+  return true;\n+}\n+\n+\/\/------------------------------inline_kyberAddPoly_3\n+bool LibraryCallKit::inline_kyberAddPoly_3() {\n+  address stubAddr;\n+  const char *stubName;\n+  assert(UseKyberIntrinsics, \"need Kyber intrinsics support\");\n+  assert(callee()->signature()->size() == 4, \"kyberAddPoly_3 has 4 parameters\");\n+\n+  stubAddr = StubRoutines::kyberAddPoly_3();\n+  stubName = \"kyberAddPoly_3\";\n+  if (!stubAddr) return false;\n+\n+  Node* result          = argument(0);\n+  Node* a               = argument(1);\n+  Node* b               = argument(2);\n+  Node* c               = argument(3);\n+\n+  result = must_be_not_null(result, true);\n+  a = must_be_not_null(a, true);\n+  b = must_be_not_null(b, true);\n+  c = must_be_not_null(c, true);\n+\n+  Node* result_start  = array_element_address(result, intcon(0), T_SHORT);\n+  assert(result_start, \"result is null\");\n+  Node* a_start  = array_element_address(a, intcon(0), T_SHORT);\n+  assert(a_start, \"a is null\");\n+  Node* b_start  = array_element_address(b, intcon(0), T_SHORT);\n+  assert(b_start, \"b is null\");\n+  Node* c_start  = array_element_address(c, intcon(0), T_SHORT);\n+  assert(c_start, \"c is null\");\n+  Node* kyberAddPoly_3 = make_runtime_call(RC_LEAF|RC_NO_FP,\n+                                  OptoRuntime::kyberAddPoly_3_Type(),\n+                                  stubAddr, stubName, TypePtr::BOTTOM,\n+                                  result_start, a_start, b_start, c_start);\n+  \/\/ return an int\n+  Node* retvalue = _gvn.transform(new ProjNode(kyberAddPoly_3, TypeFunc::Parms));\n+  set_result(retvalue);\n+  return true;\n+}\n+\n+\/\/------------------------------inline_kyber12To16\n+bool LibraryCallKit::inline_kyber12To16() {\n+  address stubAddr;\n+  const char *stubName;\n+  assert(UseKyberIntrinsics, \"need Kyber intrinsics support\");\n+  assert(callee()->signature()->size() == 4, \"kyber12To16 has 4 parameters\");\n+\n+  stubAddr = StubRoutines::kyber12To16();\n+  stubName = \"kyber12To16\";\n+  if (!stubAddr) return false;\n+\n+  Node* condensed       = argument(0);\n+  Node* condensedOffs   = argument(1);\n+  Node* parsed          = argument(2);\n+  Node* parsedLength    = argument(3);\n+\n+  condensed = must_be_not_null(condensed, true);\n+  parsed = must_be_not_null(parsed, true);\n+\n+  Node* condensed_start  = array_element_address(condensed, intcon(0), T_BYTE);\n+  assert(condensed_start, \"condensed is null\");\n+  Node* parsed_start  = array_element_address(parsed, intcon(0), T_SHORT);\n+  assert(parsed_start, \"parsed is null\");\n+  Node* kyber12To16 = make_runtime_call(RC_LEAF|RC_NO_FP,\n+                                  OptoRuntime::kyber12To16_Type(),\n+                                  stubAddr, stubName, TypePtr::BOTTOM,\n+                                  condensed_start, condensedOffs, parsed_start, parsedLength);\n+  \/\/ return an int\n+  Node* retvalue = _gvn.transform(new ProjNode(kyber12To16, TypeFunc::Parms));\n+  set_result(retvalue);\n+  return true;\n+\n+}\n+\n+\/\/------------------------------inline_kyberBarrettReduce\n+bool LibraryCallKit::inline_kyberBarrettReduce() {\n+  address stubAddr;\n+  const char *stubName;\n+  assert(UseKyberIntrinsics, \"need Kyber intrinsics support\");\n+  assert(callee()->signature()->size() == 1, \"kyberBarrettReduce has 1 parameters\");\n+\n+  stubAddr = StubRoutines::kyberBarrettReduce();\n+  stubName = \"kyberBarrettReduce\";\n+  if (!stubAddr) return false;\n+\n+  Node* coeffs          = argument(0);\n+\n+  coeffs = must_be_not_null(coeffs, true);\n+\n+  Node* coeffs_start  = array_element_address(coeffs, intcon(0), T_SHORT);\n+  assert(coeffs_start, \"coeffs is null\");\n+  Node* kyberBarrettReduce = make_runtime_call(RC_LEAF|RC_NO_FP,\n+                                  OptoRuntime::kyberBarrettReduce_Type(),\n+                                  stubAddr, stubName, TypePtr::BOTTOM,\n+                                  coeffs_start);\n+  \/\/ return an int\n+  Node* retvalue = _gvn.transform(new ProjNode(kyberBarrettReduce, TypeFunc::Parms));\n+  set_result(retvalue);\n+  return true;\n+}\n+\n@@ -7842,1 +8102,0 @@\n-\n@@ -7863,0 +8122,1 @@\n+  Node* zetas           = argument(3);\n@@ -7867,0 +8127,1 @@\n+  zetas = must_be_not_null(zetas, true);\n","filename":"src\/hotspot\/share\/opto\/library_call.cpp","additions":273,"deletions":12,"binary":false,"changes":285,"status":"modified"},{"patch":"@@ -228,1 +228,1 @@\n-        debug_only(intptr_t offset;)\n+        DEBUG_ONLY(intptr_t offset;)\n@@ -1331,1 +1331,1 @@\n-    debug_only(slow_region = NodeSentinel);\n+    DEBUG_ONLY(slow_region = NodeSentinel);\n","filename":"src\/hotspot\/share\/opto\/macro.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -292,4 +292,11 @@\n-    \/\/ compress paths and change unreachable cycles to TOP\n-    \/\/ If not, we can update the input infinitely along a MergeMem cycle\n-    \/\/ Equivalent code in PhiNode::Ideal\n-    Node* m  = phase->transform(mmem);\n+    \/\/ IGVN _delay_transform may be set to true and if that is the case and mmem\n+    \/\/ is already a registered node then the validation inside transform will\n+    \/\/ complain.\n+    Node* m = mmem;\n+    PhaseIterGVN* igvn = phase->is_IterGVN();\n+    if (igvn == nullptr || !igvn->delay_transform()) {\n+      \/\/ compress paths and change unreachable cycles to TOP\n+      \/\/ If not, we can update the input infinitely along a MergeMem cycle\n+      \/\/ Equivalent code in PhiNode::Ideal\n+      m = phase->transform(mmem);\n+    }\n@@ -1209,1 +1216,1 @@\n-      if (memory_type() != T_VOID) {\n+      if (value_basic_type() != T_VOID) {\n@@ -1214,1 +1221,1 @@\n-          return phase->zerocon(memory_type());\n+          return phase->zerocon(value_basic_type());\n@@ -2043,1 +2050,1 @@\n-                                                                      memory_type(), is_unsigned());\n+                                                                      value_basic_type(), is_unsigned());\n@@ -2111,1 +2118,1 @@\n-      const Type* con_type = Type::make_constant_from_field(const_oop->as_instance(), off, is_unsigned(), memory_type());\n+      const Type* con_type = Type::make_constant_from_field(const_oop->as_instance(), off, is_unsigned(), value_basic_type());\n@@ -2214,4 +2221,2 @@\n-        !tkls->is_instklassptr()->instance_klass()->is_java_lang_Object() \/\/ not the supertype of all T[] and specifically not Serializable & Cloneable\n-        ) {\n-      \/\/ Note:  When interfaces are reliable, we can narrow the interface\n-      \/\/ test to (klass != Serializable && klass != Cloneable).\n+        !tkls->is_instklassptr()->might_be_an_array() \/\/ not the supertype of all T[] (java.lang.Object) or has an interface that is not Serializable or Cloneable\n+    ) {\n","filename":"src\/hotspot\/share\/opto\/memnode.cpp","additions":17,"deletions":12,"binary":false,"changes":29,"status":"modified"},{"patch":"@@ -1838,1 +1838,1 @@\n-          debug_only(const Type* bt1 = phi->bottom_type());\n+          DEBUG_ONLY(const Type* bt1 = phi->bottom_type());\n@@ -1841,1 +1841,1 @@\n-          debug_only(const Type* bt2 = phi->bottom_type());\n+          DEBUG_ONLY(const Type* bt2 = phi->bottom_type());\n@@ -1939,1 +1939,1 @@\n-    debug_only(mms.set_memory());  \/\/ keep the iterator happy\n+    DEBUG_ONLY(mms.set_memory());  \/\/ keep the iterator happy\n","filename":"src\/hotspot\/share\/opto\/parse1.cpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -245,1 +245,7 @@\n-\n+const TypeFunc* OptoRuntime::_kyberNtt_Type                       = nullptr;\n+const TypeFunc* OptoRuntime::_kyberInverseNtt_Type                = nullptr;\n+const TypeFunc* OptoRuntime::_kyberNttMult_Type                   = nullptr;\n+const TypeFunc* OptoRuntime::_kyberAddPoly_2_Type                 = nullptr;\n+const TypeFunc* OptoRuntime::_kyberAddPoly_3_Type                 = nullptr;\n+const TypeFunc* OptoRuntime::_kyber12To16_Type                    = nullptr;\n+const TypeFunc* OptoRuntime::_kyberBarrettReduce_Type             = nullptr;\n@@ -251,1 +257,0 @@\n-\n@@ -345,1 +350,1 @@\n-    current->set_vm_result(result);\n+    current->set_vm_result_oop(result);\n@@ -391,1 +396,1 @@\n-  current->set_vm_result(result);\n+  current->set_vm_result_oop(result);\n@@ -419,1 +424,1 @@\n-  current->set_vm_result(result);\n+  current->set_vm_result_oop(result);\n@@ -426,1 +431,1 @@\n-  oop result = current->vm_result();\n+  oop result = current->vm_result_oop();\n@@ -463,1 +468,1 @@\n-  current->set_vm_result(obj);\n+  current->set_vm_result_oop(obj);\n@@ -480,1 +485,1 @@\n-  current->set_vm_result(obj);\n+  current->set_vm_result_oop(obj);\n@@ -498,1 +503,1 @@\n-  current->set_vm_result(obj);\n+  current->set_vm_result_oop(obj);\n@@ -517,1 +522,1 @@\n-  current->set_vm_result(obj);\n+  current->set_vm_result_oop(obj);\n@@ -535,1 +540,1 @@\n-  current->set_vm_result(obj);\n+  current->set_vm_result_oop(obj);\n@@ -1412,0 +1417,140 @@\n+\/\/ Kyber NTT function\n+static const TypeFunc* make_kyberNtt_Type() {\n+    int argcnt = 2;\n+\n+    const Type** fields = TypeTuple::fields(argcnt);\n+    int argp = TypeFunc::Parms;\n+    fields[argp++] = TypePtr::NOTNULL;      \/\/ coeffs\n+    fields[argp++] = TypePtr::NOTNULL;      \/\/ NTT zetas\n+\n+    assert(argp == TypeFunc::Parms + argcnt, \"correct decoding\");\n+    const TypeTuple* domain = TypeTuple::make(TypeFunc::Parms + argcnt, fields);\n+\n+    \/\/ result type needed\n+    fields = TypeTuple::fields(1);\n+    fields[TypeFunc::Parms + 0] = TypeInt::INT;\n+    const TypeTuple* range = TypeTuple::make(TypeFunc::Parms + 1, fields);\n+    return TypeFunc::make(domain, range);\n+}\n+\n+\/\/ Kyber inverse NTT function\n+static const TypeFunc* make_kyberInverseNtt_Type() {\n+    int argcnt = 2;\n+\n+    const Type** fields = TypeTuple::fields(argcnt);\n+    int argp = TypeFunc::Parms;\n+    fields[argp++] = TypePtr::NOTNULL;      \/\/ coeffs\n+    fields[argp++] = TypePtr::NOTNULL;      \/\/ inverse NTT zetas\n+\n+    assert(argp == TypeFunc::Parms + argcnt, \"correct decoding\");\n+    const TypeTuple* domain = TypeTuple::make(TypeFunc::Parms + argcnt, fields);\n+\n+    \/\/ result type needed\n+    fields = TypeTuple::fields(1);\n+    fields[TypeFunc::Parms + 0] = TypeInt::INT;\n+    const TypeTuple* range = TypeTuple::make(TypeFunc::Parms + 1, fields);\n+    return TypeFunc::make(domain, range);\n+}\n+\n+\/\/ Kyber NTT multiply function\n+static const TypeFunc* make_kyberNttMult_Type() {\n+    int argcnt = 4;\n+\n+    const Type** fields = TypeTuple::fields(argcnt);\n+    int argp = TypeFunc::Parms;\n+    fields[argp++] = TypePtr::NOTNULL;      \/\/ result\n+    fields[argp++] = TypePtr::NOTNULL;      \/\/ ntta\n+    fields[argp++] = TypePtr::NOTNULL;      \/\/ nttb\n+    fields[argp++] = TypePtr::NOTNULL;      \/\/ NTT multiply zetas\n+\n+    assert(argp == TypeFunc::Parms + argcnt, \"correct decoding\");\n+    const TypeTuple* domain = TypeTuple::make(TypeFunc::Parms + argcnt, fields);\n+\n+    \/\/ result type needed\n+    fields = TypeTuple::fields(1);\n+    fields[TypeFunc::Parms + 0] = TypeInt::INT;\n+    const TypeTuple* range = TypeTuple::make(TypeFunc::Parms + 1, fields);\n+    return TypeFunc::make(domain, range);\n+}\n+\/\/ Kyber add 2 polynomials function\n+static const TypeFunc* make_kyberAddPoly_2_Type() {\n+    int argcnt = 3;\n+\n+    const Type** fields = TypeTuple::fields(argcnt);\n+    int argp = TypeFunc::Parms;\n+    fields[argp++] = TypePtr::NOTNULL;      \/\/ result\n+    fields[argp++] = TypePtr::NOTNULL;      \/\/ a\n+    fields[argp++] = TypePtr::NOTNULL;      \/\/ b\n+\n+    assert(argp == TypeFunc::Parms + argcnt, \"correct decoding\");\n+    const TypeTuple* domain = TypeTuple::make(TypeFunc::Parms + argcnt, fields);\n+\n+    \/\/ result type needed\n+    fields = TypeTuple::fields(1);\n+    fields[TypeFunc::Parms + 0] = TypeInt::INT;\n+    const TypeTuple* range = TypeTuple::make(TypeFunc::Parms + 1, fields);\n+    return TypeFunc::make(domain, range);\n+}\n+\n+\n+\/\/ Kyber add 3 polynomials function\n+static const TypeFunc* make_kyberAddPoly_3_Type() {\n+    int argcnt = 4;\n+\n+    const Type** fields = TypeTuple::fields(argcnt);\n+    int argp = TypeFunc::Parms;\n+    fields[argp++] = TypePtr::NOTNULL;      \/\/ result\n+    fields[argp++] = TypePtr::NOTNULL;      \/\/ a\n+    fields[argp++] = TypePtr::NOTNULL;      \/\/ b\n+    fields[argp++] = TypePtr::NOTNULL;      \/\/ c\n+\n+    assert(argp == TypeFunc::Parms + argcnt, \"correct decoding\");\n+    const TypeTuple* domain = TypeTuple::make(TypeFunc::Parms + argcnt, fields);\n+\n+    \/\/ result type needed\n+    fields = TypeTuple::fields(1);\n+    fields[TypeFunc::Parms + 0] = TypeInt::INT;\n+    const TypeTuple* range = TypeTuple::make(TypeFunc::Parms + 1, fields);\n+    return TypeFunc::make(domain, range);\n+}\n+\n+\n+\/\/ Kyber XOF output parsing into polynomial coefficients candidates\n+\/\/ or decompress(12,...) function\n+static const TypeFunc* make_kyber12To16_Type() {\n+    int argcnt = 4;\n+\n+    const Type** fields = TypeTuple::fields(argcnt);\n+    int argp = TypeFunc::Parms;\n+    fields[argp++] = TypePtr::NOTNULL;      \/\/ condensed\n+    fields[argp++] = TypeInt::INT;          \/\/ condensedOffs\n+    fields[argp++] = TypePtr::NOTNULL;      \/\/ parsed\n+    fields[argp++] = TypeInt::INT;          \/\/ parsedLength\n+\n+    assert(argp == TypeFunc::Parms + argcnt, \"correct decoding\");\n+    const TypeTuple* domain = TypeTuple::make(TypeFunc::Parms + argcnt, fields);\n+\n+    \/\/ result type needed\n+    fields = TypeTuple::fields(1);\n+    fields[TypeFunc::Parms + 0] = TypeInt::INT;\n+    const TypeTuple* range = TypeTuple::make(TypeFunc::Parms + 1, fields);\n+    return TypeFunc::make(domain, range);\n+}\n+\n+\/\/ Kyber Barrett reduce function\n+static const TypeFunc* make_kyberBarrettReduce_Type() {\n+    int argcnt = 1;\n+\n+    const Type** fields = TypeTuple::fields(argcnt);\n+    int argp = TypeFunc::Parms;\n+    fields[argp++] = TypePtr::NOTNULL;      \/\/ coeffs\n+    assert(argp == TypeFunc::Parms + argcnt, \"correct decoding\");\n+    const TypeTuple* domain = TypeTuple::make(TypeFunc::Parms + argcnt, fields);\n+\n+    \/\/ result type needed\n+    fields = TypeTuple::fields(1);\n+    fields[TypeFunc::Parms + 0] = TypeInt::INT;\n+    const TypeTuple* range = TypeTuple::make(TypeFunc::Parms + 1, fields);\n+    return TypeFunc::make(domain, range);\n+}\n+\n@@ -1802,1 +1947,1 @@\n-  debug_only(NoHandleMark __hm;)\n+  DEBUG_ONLY(NoHandleMark __hm;)\n@@ -1870,1 +2015,1 @@\n-  thread->set_vm_result(exception);\n+  thread->set_vm_result_oop(exception);\n@@ -2123,1 +2268,7 @@\n-\n+  _kyberNtt_Type                      = make_kyberNtt_Type();\n+  _kyberInverseNtt_Type               = make_kyberInverseNtt_Type();\n+  _kyberNttMult_Type                  = make_kyberNttMult_Type();\n+  _kyberAddPoly_2_Type                = make_kyberAddPoly_2_Type();\n+  _kyberAddPoly_3_Type                = make_kyberAddPoly_3_Type();\n+  _kyber12To16_Type                   = make_kyber12To16_Type();\n+  _kyberBarrettReduce_Type            = make_kyberBarrettReduce_Type();\n@@ -2129,1 +2280,0 @@\n-\n","filename":"src\/hotspot\/share\/opto\/runtime.cpp","additions":165,"deletions":15,"binary":false,"changes":180,"status":"modified"},{"patch":"@@ -406,1 +406,1 @@\n-      Node* neg_c0 = phase->longcon(-c0);\n+      Node* neg_c0 = phase->longcon(java_negate(c0));\n","filename":"src\/hotspot\/share\/opto\/subnode.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -759,1 +759,1 @@\n-  debug_only(base());           \/\/ Check the assertion in Type::base().\n+  DEBUG_ONLY(base());           \/\/ Check the assertion in Type::base().\n@@ -3688,0 +3688,6 @@\n+bool TypeInterfaces::has_non_array_interface() const {\n+  assert(TypeAryPtr::_array_interfaces != nullptr, \"How come Type::Initialize_shared wasn't called yet?\");\n+\n+  return !TypeAryPtr::_array_interfaces->contains(this);\n+}\n+\n@@ -6200,0 +6206,13 @@\n+bool TypeInstKlassPtr::might_be_an_array() const {\n+  if (!instance_klass()->is_java_lang_Object()) {\n+    \/\/ TypeInstKlassPtr can be an array only if it is java.lang.Object: the only supertype of array types.\n+    return false;\n+  }\n+  if (interfaces()->has_non_array_interface()) {\n+    \/\/ Arrays only implement Cloneable and Serializable. If we see any other interface, [this] cannot be an array.\n+    return false;\n+  }\n+  \/\/ Cannot prove it's not an array.\n+  return true;\n+}\n+\n","filename":"src\/hotspot\/share\/opto\/type.cpp","additions":20,"deletions":1,"binary":false,"changes":21,"status":"modified"},{"patch":"@@ -957,0 +957,1 @@\n+  bool has_non_array_interface() const;\n@@ -1434,0 +1435,1 @@\n+  friend class TypeInterfaces;\n@@ -1706,0 +1708,2 @@\n+  bool might_be_an_array() const;\n+\n","filename":"src\/hotspot\/share\/opto\/type.hpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -94,1 +94,1 @@\n-bool   Arguments::_sun_java_launcher_is_altjvm  = false;\n+bool   Arguments::_executing_unit_tests         = false;\n@@ -335,1 +335,0 @@\n-        matches_property_suffix(property_suffix, ADDOPENS, ADDOPENS_LEN) ||\n@@ -346,0 +345,1 @@\n+          matches_property_suffix(property_suffix, ADDOPENS, ADDOPENS_LEN) ||\n@@ -358,1 +358,1 @@\n-  \/\/ See if sun.java.launcher or sun.java.launcher.is_altjvm is defined.\n+  \/\/ See if sun.java.launcher is defined.\n@@ -369,4 +369,2 @@\n-    if (match_option(option, \"-Dsun.java.launcher.is_altjvm=\", &tail)) {\n-      if (strcmp(tail, \"true\") == 0) {\n-        _sun_java_launcher_is_altjvm = true;\n-      }\n+    if (match_option(option, \"-XX:+ExecutingUnitTests\")) {\n+      _executing_unit_tests = true;\n@@ -529,0 +527,3 @@\n+#ifdef _LP64\n+  { \"UseCompressedClassPointers\",   JDK_Version::jdk(25),  JDK_Version::jdk(26), JDK_Version::undefined() },\n+#endif\n@@ -1274,4 +1275,0 @@\n-  } else if (strcmp(key, \"sun.java.launcher.is_altjvm\") == 0) {\n-    \/\/ sun.java.launcher.is_altjvm property is\n-    \/\/ private and is processed in process_sun_java_launcher_properties();\n-    \/\/ the sun.java.launcher property is passed on to the java application\n@@ -1766,2 +1763,2 @@\n-bool Arguments::sun_java_launcher_is_altjvm() {\n-  return _sun_java_launcher_is_altjvm;\n+bool Arguments::executing_unit_tests() {\n+  return _executing_unit_tests;\n@@ -3714,1 +3711,1 @@\n-  CDSConfig::initialize();\n+  CDSConfig::ergo_initialize();\n@@ -3795,5 +3792,0 @@\n-\n-    if (!FLAG_IS_DEFAULT(UseVectorStubs) && UseVectorStubs) {\n-      warning(\"Disabling UseVectorStubs since EnableVectorSupport is turned off.\");\n-    }\n-    FLAG_SET_DEFAULT(UseVectorStubs, false);\n","filename":"src\/hotspot\/share\/runtime\/arguments.cpp","additions":11,"deletions":19,"binary":false,"changes":30,"status":"modified"},{"patch":"@@ -128,1 +128,1 @@\n-          \"Use 32-bit class pointers in 64-bit VM. \"                        \\\n+          \"(Deprecated) Use 32-bit class pointers in 64-bit VM. \"           \\\n@@ -328,0 +328,2 @@\n+  product(bool, UseKyberIntrinsics, false, DIAGNOSTIC,                      \\\n+          \"Use intrinsics for the vectorized version of Kyber\")             \\\n@@ -1168,3 +1170,0 @@\n-  develop(bool, VerifyFPU, false,                                           \\\n-          \"Verify FPU state (check for NaN's, etc.)\")                       \\\n-                                                                            \\\n","filename":"src\/hotspot\/share\/runtime\/globals.hpp","additions":3,"deletions":4,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -128,1 +128,1 @@\n-    Atomic::inc(&_items_count);\n+    Atomic::inc(&_items_count, memory_order_relaxed);\n@@ -132,1 +132,1 @@\n-    Atomic::dec(&_items_count);\n+    Atomic::dec(&_items_count, memory_order_relaxed);\n@@ -136,1 +136,2 @@\n-    return (double)_items_count \/ (double)_table_size;\n+    size_t count = Atomic::load(&_items_count);\n+    return (double)count \/ (double)_table_size;\n@@ -532,2 +533,6 @@\n-        _thread->om_set_monitor_cache(_monitor);\n-        _lock->set_object_monitor_cache(_monitor);\n+        \/\/ If the monitor is already in the BasicLock cache then it is most\n+        \/\/ likely in the thread cache, do not set it again to avoid reordering.\n+        if (_monitor != _lock->object_monitor_cache()) {\n+          _thread->om_set_monitor_cache(_monitor);\n+          _lock->set_object_monitor_cache(_monitor);\n+        }\n@@ -547,0 +552,10 @@\n+\/\/ Reads first from the BasicLock cache then from the OMCache in the current thread.\n+\/\/ C2 fast-path may have put the monitor in the cache in the BasicLock.\n+inline static ObjectMonitor* read_caches(JavaThread* current, BasicLock* lock, oop object) {\n+  ObjectMonitor* monitor = lock->object_monitor_cache();\n+  if (monitor == nullptr) {\n+    monitor = current->om_get_from_monitor_cache(object);\n+  }\n+  return monitor;\n+}\n+\n@@ -628,0 +643,1 @@\n+  assert(!UseObjectMonitorTable || lock->object_monitor_cache() == nullptr, \"must be cleared\");\n@@ -635,2 +651,0 @@\n-  CacheSetter cache_setter(locking_thread, lock);\n-\n@@ -647,1 +661,1 @@\n-      monitor = inflate_and_enter(obj(), ObjectSynchronizer::inflate_cause_monitor_enter, locking_thread, current);\n+      monitor = inflate_and_enter(obj(), lock, ObjectSynchronizer::inflate_cause_monitor_enter, locking_thread, current);\n@@ -653,1 +667,1 @@\n-  cache_setter.set_monitor(monitor);\n+  assert(!UseObjectMonitorTable || lock->object_monitor_cache() == nullptr, \"unused. already cleared\");\n@@ -703,1 +717,1 @@\n-    ObjectMonitor* monitor = inflate_and_enter(obj(), ObjectSynchronizer::inflate_cause_monitor_enter, current, current);\n+    ObjectMonitor* monitor = inflate_and_enter(obj(), lock, ObjectSynchronizer::inflate_cause_monitor_enter, current, current);\n@@ -717,1 +731,1 @@\n-void LightweightSynchronizer::exit(oop object, JavaThread* current) {\n+void LightweightSynchronizer::exit(oop object, BasicLock* lock, JavaThread* current) {\n@@ -752,1 +766,9 @@\n-  ObjectMonitor* monitor = ObjectSynchronizer::read_monitor(current, object, mark);\n+  ObjectMonitor* monitor;\n+  if (UseObjectMonitorTable) {\n+    monitor = read_caches(current, lock, object);\n+    if (monitor == nullptr) {\n+      monitor = get_monitor_from_table(current, object);\n+    }\n+  } else {\n+    monitor = ObjectSynchronizer::read_monitor(mark);\n+  }\n@@ -991,1 +1013,1 @@\n-ObjectMonitor* LightweightSynchronizer::inflate_and_enter(oop object, ObjectSynchronizer::InflateCause cause, JavaThread* locking_thread, JavaThread* current) {\n+ObjectMonitor* LightweightSynchronizer::inflate_and_enter(oop object, BasicLock* lock, ObjectSynchronizer::InflateCause cause, JavaThread* locking_thread, JavaThread* current) {\n@@ -1017,3 +1039,0 @@\n-  \/\/ Lightweight monitors require that hash codes are installed first\n-  ObjectSynchronizer::FastHashCode(locking_thread, object);\n-\n@@ -1024,1 +1043,1 @@\n-    monitor = current->om_get_from_monitor_cache(object);\n+    monitor = read_caches(current, lock, object);\n@@ -1029,0 +1048,2 @@\n+    \/\/ Lightweight monitors require that hash codes are installed first\n+    ObjectSynchronizer::FastHashCode(locking_thread, object);\n@@ -1044,0 +1065,3 @@\n+    \/\/ Clear the BasicLock cache as it may contain this monitor.\n+    lock->clear_object_monitor_cache();\n+\n@@ -1176,3 +1200,0 @@\n-  \/\/ If quick_enter succeeds with entering, the cache should be in a valid initialized state.\n-  CacheSetter cache_setter(current, lock);\n-\n@@ -1205,2 +1226,6 @@\n-    ObjectMonitor* const monitor = UseObjectMonitorTable ? current->om_get_from_monitor_cache(obj) :\n-                                                           ObjectSynchronizer::read_monitor(mark);\n+    ObjectMonitor* monitor;\n+    if (UseObjectMonitorTable) {\n+      monitor = read_caches(current, lock, obj);\n+    } else {\n+      monitor = ObjectSynchronizer::read_monitor(mark);\n+    }\n@@ -1213,3 +1238,10 @@\n-    if (monitor->try_enter(current)) {\n-      \/\/ ObjectMonitor enter successful.\n-      cache_setter.set_monitor(monitor);\n+    if (UseObjectMonitorTable) {\n+      \/\/ Set the monitor regardless of success.\n+      \/\/ Either we successfully lock on the monitor, or we retry with the\n+      \/\/ monitor in the slow path. If the monitor gets deflated, it will be\n+      \/\/ cleared, either by the CacheSetter if we fast lock in enter or in\n+      \/\/ inflate_and_enter when we see that the monitor is deflated.\n+      lock->set_object_monitor_cache(monitor);\n+    }\n+\n+    if (monitor->spin_enter(current)) {\n","filename":"src\/hotspot\/share\/runtime\/lightweightSynchronizer.cpp","additions":57,"deletions":25,"binary":false,"changes":82,"status":"modified"},{"patch":"@@ -64,1 +64,1 @@\n-  static void exit(oop object, JavaThread* current);\n+  static void exit(oop object, BasicLock* lock, JavaThread* current);\n@@ -69,1 +69,1 @@\n-  static ObjectMonitor* inflate_and_enter(oop object, ObjectSynchronizer::InflateCause cause, JavaThread* locking_thread, JavaThread* current);\n+  static ObjectMonitor* inflate_and_enter(oop object, BasicLock* lock, ObjectSynchronizer::InflateCause cause, JavaThread* locking_thread, JavaThread* current);\n","filename":"src\/hotspot\/share\/runtime\/lightweightSynchronizer.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -372,2 +372,1 @@\n-      \/\/ that we own, we can transfer one or more threads from the waitset\n-      \/\/ to the entry_list here and now, avoiding the slow-path.\n+      \/\/ that we own, we quickly notify them here and now, avoiding the slow-path.\n@@ -375,1 +374,1 @@\n-        DTRACE_MONITOR_PROBE(notifyAll, mon, obj, current);\n+        mon->quick_notifyAll(current);\n@@ -377,1 +376,1 @@\n-        DTRACE_MONITOR_PROBE(notify, mon, obj, current);\n+        mon->quick_notify(current);\n@@ -379,3 +378,0 @@\n-      do {\n-        mon->notify_internal(current);\n-      } while (mon->first_waiter() != nullptr && all);\n@@ -431,1 +427,1 @@\n-      m->_recursions++;\n+      m->increment_recursions(current);\n@@ -448,1 +444,1 @@\n-      assert(m->_recursions == 0, \"invariant\");\n+      assert(m->recursions() == 0, \"invariant\");\n@@ -682,1 +678,2 @@\n-      entered = LightweightSynchronizer::inflate_and_enter(obj(), inflate_cause_jni_enter, current, current) != nullptr;\n+      BasicLock lock;\n+      entered = LightweightSynchronizer::inflate_and_enter(obj(), &lock, inflate_cause_jni_enter, current, current) != nullptr;\n","filename":"src\/hotspot\/share\/runtime\/synchronizer.cpp","additions":7,"deletions":10,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -608,1 +608,1 @@\n-  nonstatic_field(JavaThread,                  _scopedValueCache,                              OopHandle)                             \\\n+  nonstatic_field(JavaThread,                  _scopedValueCache,                              OopHandle)                            \\\n@@ -610,2 +610,0 @@\n-  nonstatic_field(JavaThread,                  _vm_result,                                    oop)                                   \\\n-  nonstatic_field(JavaThread,                  _vm_result_2,                                  Metadata*)                             \\\n@@ -708,0 +706,1 @@\n+     static_field(Abstract_VM_Version,         _cpu_info_string,                              const char*)                           \\\n","filename":"src\/hotspot\/share\/runtime\/vmStructs.cpp","additions":2,"deletions":3,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2013, 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2013, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -77,0 +77,1 @@\n+                \"-XX:+UnlockExperimentalVMOptions\",\n@@ -78,0 +79,1 @@\n+                \"-XX:-UseCompactObjectHeaders\",\n","filename":"test\/hotspot\/jtreg\/runtime\/CompressedOops\/CompressedClassPointersEncodingScheme.java","additions":3,"deletions":1,"binary":false,"changes":4,"status":"modified"}]}